<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 90]
- [cs.CL](#cs.CL) [Total: 40]
- [cs.RO](#cs.RO) [Total: 16]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SkyCap: Bitemporal VHR Optical-SAR Quartets for Amplitude Change Detection and Foundation-Model Evaluation](https://arxiv.org/abs/2512.14755)
*Paul Weinmann,Ferdinand Schenck,Martin Šiklar*

Main category: cs.CV

TL;DR: 该文介绍了一个结合了光学和合成孔径雷达（SAR）影像的新数据集SkyCap，并评估了基础模型在SAR变化检测上的表现。结果显示，经过特定预处理的光学基础模型在SAR变化检测中也可优于专用的SAR模型。


<details>
  <summary>Details</summary>
Motivation: 线性基础设施监测对高分辨率和规律获取的数据依赖很强，但光学影像受云影响大，SAR影像虽能全天候获取但难以标注。为解决标注和数据获取难题，提出结合两者优势的新方案。

Method: 构建了SkyCap数据集，该数据集通过匹配和配准SkySat（光学）与Capella Space（SAR）影像，利用光学至SAR标签转移实现SAR幅度变化检测的无专家标注。随后将现有SAR基础模型（SARATR-X）在新数据集上预训练并基准评估，比较不同预处理和模型的效果。

Result: 在评估模型中，经过dB+Z-score预处理的光学基础模型（MTP(ViT-B+RVSA)）取得最佳效果（F1c=45.06），超过了专门为SAR数据进一步预训练的SAR基础模型。并且模型对预处理对齐与预训练统计高度敏感，光学模型在光学变化检测中的表现排名不能直接推广到SAR变化检测任务。

Conclusion: 光学基础模型经过合适预处理后在SAR变化检测中具有竞争力，基础模型的泛化能力依赖于数据分布和预处理方法，提供了新数据集和评估工具，为今后VHR SAR变化检测的基础模型研究提供了借鉴。

Abstract: Change detection for linear infrastructure monitoring requires reliable high-resolution data and regular acquisition cadence. Optical very-high-resolution (VHR) imagery is interpretable and straightforward to label, but clouds break this cadence. Synthetic Aperture Radar (SAR) enables all-weather acquisitions, yet is difficult to annotate. We introduce SkyCap, a bitemporal VHR optical-SAR dataset constructed by archive matching and co-registration of (optical) SkySat and Capella Space (SAR) scenes. We utilize optical-to-SAR label transfer to obtain SAR amplitude change detection (ACD) labels without requiring SAR-expert annotations. We perform continued pretraining of SARATR-X on our SAR data and benchmark the resulting SAR-specific foundation models (FMs) together with SARATR-X against optical FMs on SkyCap under different preprocessing choices. Among evaluated models, MTP(ViT-B+RVSA), an optical FM, with dB+Z-score preprocessing attains the best result (F1$_c$ = 45.06), outperforming SAR-specific FMs further pretrained directly on Capella data. We observe strong sensitivity to preprocessing alignment with pretraining statistics, and the ranking of optical models on optical change detection does not transfer one-to-one to SAR ACD. To our knowledge, this is the first evaluation of foundation models on VHR SAR ACD.

</details>


### [2] [SocialNav-MoE: A Mixture-of-Experts Vision Language Model for Socially Compliant Navigation with Reinforcement Fine-Tuning](https://arxiv.org/abs/2512.14757)
*Tomohito Kawabata,Xinyu Zhang,Ling Xiao*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的视觉-语言混合专家模型 SocialNav-MoE，通过强化学习微调，实现了在社区环境中兼顾安全性与社会合规性的机器人导航。模型在平衡导航准确率和效率的同时，显著减少了算力与能耗消耗。


<details>
  <summary>Details</summary>
Motivation: 传统机器人导航主要关注安全，忽视了与人互动时的社会合规性（如舒适度、社会规范等），导致在人口密集环境中的应用有限。大规模视觉语言模型虽具备解释社会规范能力，但推理延迟高、能耗大，不适用于受限资源的机器人平台。

Method: 作者研究了小型视觉语言模型的适用性，提出了 SocialNav-MoE（混合专家结构），结合了图像和语言双模态输入。模型通过强化学习（RFT）优化，并引入语义相似度奖励SSR来提升决策能力。此外，系统性对比了不同的小型语言模型（如 Phi、Qwen、StableLM）、路由策略和视觉编码器（CLIP与SigLIP，是否微调）的效果。

Result: 在 SNEI 数据集上，SocialNav-MoE 达到了优异的导航准确率与效率平衡，SSR 奖励相比于硬件级或字符级奖励有更好的表现。模型显著降低了计算资源占用及推理延迟。

Conclusion: SocialNav-MoE 能在保障安全的同时，显著提升机器人在群体环境下的社会合规导航能力，适用于资源受限的机器人平台，为未来社会型机器人导航提供了一条高效可行的新路径。

Abstract: For robots navigating in human-populated environments, safety and social compliance are equally critical, yet prior work has mostly emphasized safety. Socially compliant navigation that accounts for human comfort, social norms, and contextual appropriateness remains underexplored. Vision language models (VLMs) show promise for this task; however, large-scale models incur substantial computational overhead, leading to higher inference latency and energy consumption, which makes them unsuitable for real-time deployment on resource-constrained robotic platforms. To address this issue, we investigate the effectiveness of small VLM and propose SocialNav-MoE, an efficient Mixture-of-Experts vision language model for socially compliant navigation with reinforcement fine-tuning (RFT). We further introduce a semantic similarity reward (SSR) to effectively leverage RFT for enhancing the decision-making capabilities. Additionally, we study the effectiveness of different small language model types (Phi, Qwen, and StableLM), routing strategies, and vision encoders (CLIP vs. SigLIP, frozen vs. fine-tuned). Experiments on the SNEI dataset demonstrate that SocialNav-MoE achieves an excellent balance between navigation accuracy and efficiency. The proposed SSR function is more effective than hard-level and character-level rewards. Source code will be released upon acceptance.

</details>


### [3] [The Renaissance of Expert Systems: Optical Recognition of Printed Chinese Jianpu Musical Scores with Lyrics](https://arxiv.org/abs/2512.14758)
*Fan Bu,Rongfeng Li,Zijin Li,Ya Li,Linfeng Fan,Pei Huang*

Main category: cs.CV

TL;DR: 本论文提出了一种模块化专家系统流程，高效地将带歌词的中国简谱纸质乐谱转换为MusicXML和MIDI等机器可读格式，在准确率和可解释性之间实现了良好平衡。系统无需大量标注数据即可大规模高精度识别旋律和歌词。


<details>
  <summary>Details</summary>
Motivation: 主流的光学音乐识别(OMR)研究主要集中在西方五线谱，对于中国简谱及其丰富的歌词信息利用不足。现有方法受限于训练数据需求和对中文歌词支持不佳，推动开发专门适用于简谱、可高效大规模数字化的OMR解决方案。

Method: 方法采用专家系统式顶层设计，结合传统计算机视觉方法（如短语相关、骨架分析）利用先验知识，同时引入无监督深度学习模块做特征嵌入，兼顾系统的可解释性与准确性。整体流程模块化，能够处理旋律与歌词双重内容，生成MusicXML和MIDI格式输出。

Result: 在《中国民歌大系》数据集上，该系统实现了大规模数字化：旋律集覆盖5000余首歌曲（30万+音符），带歌词集1400余首（10万+音符）。在旋律和歌词对齐任务中达到了高精度，音符级F1为0.951，字符级F1为0.931。

Conclusion: 提出的混合专家系统方法无需大规模标注数据，在中国简谱及其歌词的数字化识别领域表现出很高性能，展现了在乐谱内容大规模数字化上的应用潜力和推广前景。

Abstract: Large-scale optical music recognition (OMR) research has focused mainly on Western staff notation, leaving Chinese Jianpu (numbered notation) and its rich lyric resources underexplored. We present a modular expert-system pipeline that converts printed Jianpu scores with lyrics into machine-readable MusicXML and MIDI, without requiring massive annotated training data. Our approach adopts a top-down expert-system design, leveraging traditional computer-vision techniques (e.g., phrase correlation, skeleton analysis) to capitalize on prior knowledge, while integrating unsupervised deep-learning modules for image feature embeddings. This hybrid strategy strikes a balance between interpretability and accuracy. Evaluated on The Anthology of Chinese Folk Songs, our system massively digitizes (i) a melody-only collection of more than 5,000 songs (> 300,000 notes) and (ii) a curated subset with lyrics comprising over 1,400 songs (> 100,000 notes). The system achieves high-precision recognition on both melody (note-wise F1 = 0.951) and aligned lyrics (character-wise F1 = 0.931).

</details>


### [4] [AquaDiff: Diffusion-Based Underwater Image Enhancement for Addressing Color Distortion](https://arxiv.org/abs/2512.14760)
*Afrah Shaahid,Muzammil Behzad*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的水下图像增强方法AquaDiff，有效提升了水下图像的颜色校准和结构保真度，在多个数据集上优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 水下图像因光的吸收和散射出现严重退化，造成颜色失真、对比度低及细节丢失，妨碍基于视觉的水下应用，因此需要更有效的增强方法。

Method: AquaDiff结合色度先验引导的颜色补偿策略与条件扩散过程，在每一步去噪中通过交叉注意力融合退化输入和潜在状态。采用含残差密集块和多尺度注意力的增强去噪主干，以捕获全局色彩与局部细节。同时引入跨域一致性损失，从像素、感知、结构和频域共同约束增强效果。

Result: 在多个具有挑战性的水下图像基准上，AquaDiff在颜色校正和图像整体质量上均优于或媲美现有传统、CNN、GAN及其他扩散方法。

Conclusion: AquaDiff能够有效校正水下图像色彩失真，提升结构和感知质量，是水下图像增强领域的一种先进方法。

Abstract: Underwater images are severely degraded by wavelength-dependent light absorption and scattering, resulting in color distortion, low contrast, and loss of fine details that hinder vision-based underwater applications. To address these challenges, we propose AquaDiff, a diffusion-based underwater image enhancement framework designed to correct chromatic distortions while preserving structural and perceptual fidelity. AquaDiff integrates a chromatic prior-guided color compensation strategy with a conditional diffusion process, where cross-attention dynamically fuses degraded inputs and noisy latent states at each denoising step. An enhanced denoising backbone with residual dense blocks and multi-resolution attention captures both global color context and local details. Furthermore, a novel cross-domain consistency loss jointly enforces pixel-level accuracy, perceptual similarity, structural integrity, and frequency-domain fidelity. Extensive experiments on multiple challenging underwater benchmarks demonstrate that AquaDiff provides good results as compared to the state-of-the-art traditional, CNN-, GAN-, and diffusion-based methods, achieving superior color correction and competitive overall image quality across diverse underwater conditions.

</details>


### [5] [Improving VQA Reliability: A Dual-Assessment Approach with Self-Reflection and Cross-Model Verification](https://arxiv.org/abs/2512.14770)
*Xixian Wu,Yang Ou,Pengchao Tian,Zian Yang,Jielei Zhang,Peiyi Li,Longwen Gao*

Main category: cs.CV

TL;DR: 本文提出了DAVR框架，通过自省和交叉模型验证两种方式，综合评估视觉-语言模型（VLM）在视觉问答任务中的回答可靠性，从而有效减少幻觉现象，提高答案信任度。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型虽然在视觉问答任务中表现优异，但对幻觉现象（模型自信地给出错误答案）的易感性严重影响了答案的可靠性。因此亟须设计方法提高VLM的稳健性和可解释性。

Method: DAVR采用双路径架构：（1）路径一结合VLM的潜在特征和问答嵌入，通过双选择器模块评估模型输出的可靠性；（2）路径二引入外部参考模型进行事实核查，辅助识别并减少幻觉输出。两路径结果结合，实现全面的不确定性评估。

Result: DAVR在ICCV-CLVL 2025举办的Reliable VQA Challenge中，取得了领先的$Φ_{100}$分数39.64和100-AUC分数97.22，排名第一，证明了其在提升VLM回答信任度上的有效性。

Conclusion: DAVR框架能够有效整合自身反思与外部校验机制，针对VLM在视觉问答中的幻觉问题，显著提升了模型的答案可靠性和可信度。

Abstract: Vision-language models (VLMs) have demonstrated significant potential in Visual Question Answering (VQA). However, the susceptibility of VLMs to hallucinations can lead to overconfident yet incorrect answers, severely undermining answer reliability. To address this, we propose Dual-Assessment for VLM Reliability (DAVR), a novel framework that integrates Self-Reflection and Cross-Model Verification for comprehensive uncertainty estimation. The DAVR framework features a dual-pathway architecture: one pathway leverages dual selector modules to assess response reliability by fusing VLM latent features with QA embeddings, while the other deploys external reference models for factual cross-checking to mitigate hallucinations. Evaluated in the Reliable VQA Challenge at ICCV-CLVL 2025, DAVR achieves a leading $Φ_{100}$ score of 39.64 and a 100-AUC of 97.22, securing first place and demonstrating its effectiveness in enhancing the trustworthiness of VLM responses.

</details>


### [6] [HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering](https://arxiv.org/abs/2512.14870)
*Dan Ben-Ami,Gabriele Serussi,Kobi Cohen,Chaim Baskin*

Main category: cs.CV

TL;DR: 本文提出了一个新的视频问答基准数据集HERBench，专门用于评测视频大语言模型对跨时间多证据整合推理的能力，并系统性地分析了现有模型在此任务上的主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答基准往往支持模型仅凭单一显著线索作答，无法全面检测模型跨时整合多条视觉证据的推理能力。因此，亟需新的基准来驱动更复杂、更真实的推理能力提升。

Method: 作者构建了HERBench数据集，包含26K个多选题，每道题需整合至少三个互不重叠的视频片段信息，涵盖身份绑定、实体关系、时序排序、共现判断、计数等12类任务。作者提出了最小需求帧集（MRFS）指标，量化每道题所需融合的最少帧数，并用该指标证明HERBench比以往数据集更具多证据信息需求。

Result: 对13种主流视频大语言模型在HERBench上的测试显示，模型准确率普遍很低（31-42%），仅略高于随机猜测（20%），其中关键瓶颈包括：1）检索不足，即关键画面没有被选取；2）融合不足，即即使给出了必要画面也无法有效整合信息。

Conclusion: HERBench通过强制并可度量的视频跨时多证据整合需求，为视频理解模型的多步推理和组合性理解能力提升提供了明确的未来发展方向和可验证的评测标准。

Abstract: Video Large Language Models (Video-LLMs) are rapidly improving, yet current Video Question Answering (VideoQA) benchmarks often allow questions to be answered from a single salient cue, under-testing reasoning that must aggregate multiple, temporally separated visual evidence. We present HERBench, a VideoQA benchmark purpose-built to assess multi-evidence integration across time. Each question requires aggregating at least three non-overlapping evidential cues across distinct video segments, so neither language priors nor a single snapshot can suffice. HERBench comprises 26K five-way multiple-choice questions organized into twelve compositional tasks that probe identity binding, cross-entity relations, temporal ordering, co-occurrence verification, and counting. To make evidential demand measurable, we introduce the Minimum Required Frame-Set (MRFS), the smallest number of frames a model must fuse to answer correctly, and show that HERBench imposes substantially higher demand than prior datasets (mean MRFS 5.5 vs. 2.6-4.2). Evaluating 13 state-of-the-art Video-LLMs on HERBench reveals pervasive failures: accuracies of 31-42% are only slightly above the 20% random-guess baseline. We disentangle this failure into two critical bottlenecks: (1) a retrieval deficit, where frame selectors overlook key evidence, and (2) a fusion deficit, where models fail to integrate information even when all necessary evidence is provided. By making cross-time evidence both unavoidable and quantifiable, HERBench establishes a principled target for advancing robust, compositional video understanding.

</details>


### [7] [Isolated Sign Language Recognition with Segmentation and Pose Estimation](https://arxiv.org/abs/2512.14876)
*Daniel Perkins,Davis Hunter,Dhrumil Patel,Galen Flanagan*

Main category: cs.CV

TL;DR: 本文提出了一种高效且对手语者变异具有鲁棒性的美式手语（ASL）孤立手语识别（ISLR）模型，通过动作捕捉与深度学习方法提升了识别性能并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型极大推动了语音与书面语言的自动翻译，但美式手语（ASL）用户受限于视觉沟通而无法公平受益。当前孤立手语识别（ISLR）存在数据稀缺、手语者差异大和计算成本高等挑战，需要高效且泛化能力强的新方法来实现普及。

Method: 作者提出的方法包括三大模块：(i) 利用姿态估计提取手部和面部关节坐标，(ii) 采用分割模块以过滤出最相关的动作信息，(iii) 结合ResNet和Transformer神经网络架构来联合建模空间和时间特征，从而实现高效识别。

Result: 所提模型在减少计算资源消耗的同时，能够更好地适应不同手语者的动作差异，并在孤立手语识别任务上取得了令人满意的效果。

Conclusion: 本文方法为解决ASL孤立手语识别中的效率、数据和泛化难题提供了有效途径，有助于让手语自动识别技术更加普及、可用。

Abstract: The recent surge in large language models has automated translations of spoken and written languages. However, these advances remain largely inaccessible to American Sign Language (ASL) users, whose language relies on complex visual cues. Isolated sign language recognition (ISLR) - the task of classifying videos of individual signs - can help bridge this gap but is currently limited by scarce per-sign data, high signer variability, and substantial computational costs. We propose a model for ISLR that reduces computational requirements while maintaining robustness to signer variation. Our approach integrates (i) a pose estimation pipeline to extract hand and face joint coordinates, (ii) a segmentation module that isolates relevant information, and (iii) a ResNet-Transformer backbone to jointly model spatial and temporal dependencies.

</details>


### [8] [Visual-textual Dermatoglyphic Animal Biometrics: A First Case Study on Panthera tigris](https://arxiv.org/abs/2512.14878)
*Wenshuo Li,Majid Mirmehdi,Tilo Burghardt*

Main category: cs.CV

TL;DR: 该论文提出结合皮纹学文本描述与视觉信息的新方法，提升动物Re-ID的准确率与可解释性，并通过生成“虚拟个体”进一步优化AI表现。


<details>
  <summary>Details</summary>
Motivation: 传统动物Re-ID侧重于图像，对文本信息利用有限，且容易受图像数据稀缺影响；而皮纹学文本特征在法医学中证明有效，但尚未应用于生态领域。该研究动机在于融合人类可解释的皮纹文本描述来补足现有AI Re-ID方法的不足。

Method: 研究团队手工标注了185只老虎（共3355张图像，84264个皮肤纹理细节），利用这些数据评估了结合视觉与皮纹语言标签的新型Re-ID方法。提出了文本-图像协同合成工具链，生成包含多张逼真图像和相应皮纹描述的“虚拟个体”，来增强AI的跨模态检索能力。

Result: 实验表明，所提方法在实际生态监测场景下表现优异，生成虚拟个体显著提升了跨模态检索(AI文本到图片)的准确率，有效缓解了数据稀缺问题。

Conclusion: 通过引入皮纹学语言标签实现可被人类验证的文本到视觉身份检索，突破了仅依赖图像的局限性，为生态监测和动物Re-ID提供了更具解释性的方式。助力描述模态统一，是生物监测领域的重要进步。

Abstract: Biologists have long combined visuals with textual field notes to re-identify (Re-ID) animals. Contemporary AI tools automate this for species with distinctive morphological features but remain largely image-based. Here, we extend Re-ID methodologies by incorporating precise dermatoglyphic textual descriptors-an approach used in forensics but new to ecology. We demonstrate that these specialist semantics abstract and encode animal coat topology using human-interpretable language tags. Drawing on 84,264 manually labelled minutiae across 3,355 images of 185 tigers (Panthera tigris), we evaluate this visual-textual methodology, revealing novel capabilities for cross-modal identity retrieval. To optimise performance, we developed a text-image co-synthesis pipeline to generate 'virtual individuals', each comprising dozens of life-like visuals paired with dermatoglyphic text. Benchmarking against real-world scenarios shows this augmentation significantly boosts AI accuracy in cross-modal retrieval while alleviating data scarcity. We conclude that dermatoglyphic language-guided biometrics can overcome vision-only limitations, enabling textual-to-visual identity recovery underpinned by human-verifiable matchings. This represents a significant advance towards explainability in Re-ID and a language-driven unification of descriptive modalities in ecological monitoring.

</details>


### [9] [Vibe Spaces for Creatively Connecting and Expressing Visual Concepts](https://arxiv.org/abs/2512.14884)
*Huzheng Yang,Katherine Xu,Andrew Lu,Michael D. Grossberg,Yutong Bai,Jianbo Shi*

Main category: cs.CV

TL;DR: 论文提出了一种名为Vibe Blending的新任务，旨在通过“氛围”关联生成有意义的图像混合。为此，作者设计了Vibe Space层级图流形，实现了特征空间中跨概念的平滑过渡。实验表明，该方法在创造性和连贯性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在潜在空间中识别并穿越距离较远的概念之间的非线性路径，因此很难基于“氛围”属性生成有意义的新视觉混合概念。作者希望解决这个问题，激发AI视觉创意能力。

Method: 提出Vibe Space，一种基于分层图流形的方法，能够在CLIP等特征空间中学习低维测地线，实现概念之间平滑且语义一致的过渡；并设计了结合人类评价、大模型推理及几何路径难度分数的创意评价体系。

Result: Vibe Space生成的图像混合在创造性和一致性上均被人类评价为优于现有方法。

Conclusion: Vibe Space能够更好地揭示和利用概念间的共享特征，促进了创意视觉内容的生成，突破了当前方法在概念过渡和创新性方面的局限。

Abstract: Creating new visual concepts often requires connecting distinct ideas through their most relevant shared attributes -- their vibe. We introduce Vibe Blending, a novel task for generating coherent and meaningful hybrids that reveals these shared attributes between images. Achieving such blends is challenging for current methods, which struggle to identify and traverse nonlinear paths linking distant concepts in latent space. We propose Vibe Space, a hierarchical graph manifold that learns low-dimensional geodesics in feature spaces like CLIP, enabling smooth and semantically consistent transitions between concepts. To evaluate creative quality, we design a cognitively inspired framework combining human judgments, LLM reasoning, and a geometric path-based difficulty score. We find that Vibe Space produces blends that humans consistently rate as more creative and coherent than current methods.

</details>


### [10] [PANDA-PLUS-Bench: A Clinical Benchmark for Evaluating Robustness of AI Foundation Models in Prostate Cancer Diagnosis](https://arxiv.org/abs/2512.14922)
*Joshua L. Ebbert,Dennis Della Corte*

Main category: cs.CV

TL;DR: 本文介绍了PANDA-PLUS-Bench基准数据集，用于评估人工智能基础模型在前列腺癌Gleason分级中的泛化能力，重点分辨模型是否依赖特定切片伪迹而非生物学特征。实验发现各大模型在区分生物信号和切片混杂因素上的鲁棒性差异显著。


<details>
  <summary>Details</summary>
Motivation: Gleason等级中GP3/GP4的判别直接影响前列腺癌患者的治疗决策。现有AI基础模型虽然在验证数据上表现优异，但可能更多依赖于样本特定的伪迹，而非真正具有临床意义的生物学特征，导致其在真实临床环境下的泛化能力受限。因此，需要一个可控的基准来定量评估并解决这一问题。

Method: 作者构建了PANDA-PLUS-Bench基准集合，精选了来自9位患者的9张全切片图像，包含多样化Gleason模式，并从中在两种分辨率及八种增强条件下采集了非重叠组织块。基于该基准，作者对7种基础模型进行评价，比较模型在去除切片混杂因素后识别生物信号的能力。

Result: 模型在区分生物信号与切片特异性伪迹上的表现有较大差异，如Virchow2模型的切片水平编码最低但跨切片准确率也较低；HistoEncoder模型（专为前列腺组织训练）表现出最高的跨切片准确率和切片水平编码，显示组织特异性训练的优势。所有模型在切片内与切片间准确率上均存在显著差距。

Conclusion: PANDA-PLUS-Bench为AI基础模型在临床相关Gleason分级中的泛化鲁棒性评估提供了新的公开资源，有助于推动模型更关注真正的生物学特征捕捉，提升临床可用性。

Abstract: Artificial intelligence foundation models are increasingly deployed for prostate cancer Gleason grading, where GP3/GP4 distinction directly impacts treatment decisions. However, these models may achieve high validation accuracy by learning specimen-specific artifacts rather than generalizable biological features, limiting real-world clinical utility. We introduce PANDA-PLUS-Bench, a curated benchmark dataset derived from expert-annotated prostate biopsies designed specifically to quantify this failure mode. The benchmark comprises nine carefully selected whole slide images from nine unique patients containing diverse Gleason patterns, with non-overlapping tissue patches extracted at both 512x512 and 224x224 pixel resolutions across eight augmentation conditions. Using this benchmark, we evaluate seven foundation models on their ability to separate biological signal from slide-level confounders. Our results reveal substantial variation in robustness across models: Virchow2 achieved the lowest slide-level encoding among large-scale models (81.0%) yet exhibited the second-lowest cross-slide accuracy (47.2%). HistoEncoder, trained specifically on prostate tissue, demonstrated the highest cross-slide accuracy (59.7%) and the strongest slide-level encoding (90.3%), suggesting tissue-specific training may enhance both biological feature capture and slide-specific signatures. All models exhibited measurable within-slide vs. cross-slide accuracy gaps, though the magnitude varied from 19.9 percentage points to 26.9 percentage points. We provide an open-source Google Colab notebook enabling researchers to evaluate additional foundation models against our benchmark using standardized metrics. PANDA-PLUS-Bench addresses a critical gap in foundation model evaluation by providing a purpose-built resource for robustness assessment in the clinically important context of Gleason grading.

</details>


### [11] [Improving Pre-trained Segmentation Models using Post-Processing](https://arxiv.org/abs/2512.14937)
*Abhijeet Parida,Daniel Capellán-Martín,Zhifan Jiang,Nishad Kulkarni,Krithika Iyer,Austin Tapp,Syed Muhammad Anwar,María J. Ledesma-Carbayo,Marius George Linguraru*

Main category: cs.CV

TL;DR: 本文针对颅脑恶性肿瘤——胶质瘤的MRI分割任务，提出了一套自适应后处理技术，有效提升了大规模预训练模型预测分割的精度与临床实用性。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习模型提升了胶质瘤自动分割的准确性，但其在迁移泛化能力、系统性错误（如假阳性、标签交换、切片间不连贯）和高算力需求等方面仍存在重大局限。同时，GPU资源的不均等获得和大规模模型训练带来的环境成本，进一步限制了这些方法的广泛推广。

Method: 作者提出了一种自适应后处理技术，用于修正大规模预训练模型（针对多类肿瘤）的分割结果。该技术不依赖于模型架构本身，而是针对模型输出结果，通过高效的后处理策略提升分割质量，并在BraTS 2025分割挑战的多个任务中进行实证。

Result: 采用该自适应后处理技术，在BraTS 2025的非洲撒哈拉以南分割挑战中，排名指标提升了14.9%；在成人胶质瘤挑战中提升了0.9%。

Conclusion: 文章表明，聚焦高效、精准且计算公平的临床对齐后处理策略，是提升肿瘤分割性能、减少系统性错误并推动可持续研究的一种有效替代路线。

Abstract: Gliomas are the most common malignant brain tumors in adults and are among the most lethal. Despite aggressive treatment, the median survival rate is less than 15 months. Accurate multiparametric MRI (mpMRI) tumor segmentation is critical for surgical planning, radiotherapy, and disease monitoring. While deep learning models have improved the accuracy of automated segmentation, large-scale pre-trained models generalize poorly and often underperform, producing systematic errors such as false positives, label swaps, and slice discontinuities in slices. These limitations are further compounded by unequal access to GPU resources and the growing environmental cost of large-scale model training. In this work, we propose adaptive post-processing techniques to refine the quality of glioma segmentations produced by large-scale pretrained models developed for various types of tumors. We demonstrated the techniques in multiple BraTS 2025 segmentation challenge tasks, with the ranking metric improving by 14.9 % for the sub-Saharan Africa challenge and 0.9% for the adult glioma challenge. This approach promotes a shift in brain tumor segmentation research from increasingly complex model architectures to efficient, clinically aligned post-processing strategies that are precise, computationally fair, and sustainable.

</details>


### [12] [TalkVerse: Democratizing Minute-Long Audio-Driven Video Generation](https://arxiv.org/abs/2512.14938)
*Zhenzhi Wang,Jian Wang,Ke Ma,Dahua Lin,Bing Zhou*

Main category: cs.CV

TL;DR: TalkVerse是一个开放的、规模巨大的单人、音频驱动口播视频生成语料库，并发布了基于该数据集的高效可复现模型及相关资源。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动的视频生成方法多依赖封闭数据集或计算资源消耗大的模型，导致方法间难以公平、可复现对比，且相关资源稀缺，限制了领域发展。

Method: 1) 构建TalkVerse数据集，从6万小时视频中筛选出2.3百万条高分辨率（720p/1080p）音视频同步片段，包含详尽标注（2D骨架、风格描述等）；2) 提出5B DiT基线模型，采用高下采样比的视频VAE及滑动窗机制，可生成分钟级且漂移低的口播视频，效果可比肩更大模型但推理成本大幅下降；3) 使用MLLM驱动的director提升长视频情节；4) 支持通过噪声注入的零样本视频配音。

Result: 所提基线模型在唇形同步与视觉质量上达到现有SOTA模型 Wan-S2V（14B参数型）的水平，但推理成本降低10倍；实现长时稳定视频生成和零样本配音。数据集、代码、模型等均已开源。

Conclusion: 提供了一个大规模、开放、高质量的音频驱动口播视频生成语料库及经济高效的基线模型，极大降低该领域研究门槛，有助于推动相关技术发展与方法公平对比。

Abstract: We introduce TalkVerse, a large-scale, open corpus for single-person, audio-driven talking video generation designed to enable fair, reproducible comparison across methods. While current state-of-the-art systems rely on closed data or compute-heavy models, TalkVerse offers 2.3 million high-resolution (720p/1080p) audio-video synchronized clips totaling 6.3k hours. These are curated from over 60k hours of video via a transparent pipeline that includes scene-cut detection, aesthetic assessment, strict audio-visual synchronization checks, and comprehensive annotations including 2D skeletons and structured visual/audio-style captions. Leveraging TalkVerse, we present a reproducible 5B DiT baseline built on Wan2.2-5B. By utilizing a video VAE with a high downsampling ratio and a sliding window mechanism with motion-frame context, our model achieves minute-long generation with low drift. It delivers comparable lip-sync and visual quality to the 14B Wan-S2V model but with 10$\times$ lower inference cost. To enhance storytelling in long videos, we integrate an MLLM director to rewrite prompts based on audio and visual cues. Furthermore, our model supports zero-shot video dubbing via controlled latent noise injection. We open-source the dataset, training recipes, and 5B checkpoints to lower barriers for research in audio-driven human video generation. Project Page: https://zhenzhiwang.github.io/talkverse/

</details>


### [13] [Puzzle Curriculum GRPO for Vision-Centric Reasoning](https://arxiv.org/abs/2512.14944)
*Ahmadreza Jeddi,Hakki Can Karaimer,Hue Nguyen,Zhongling Wang,Ke Zhao,Javad Rajabi,Ran Zhang,Raghav Goyal,Babak Taati,Radek Grzeszczuk*

Main category: cs.CV

TL;DR: 该论文提出PC-GRPO方法，无需标注或外部验证器，通过自监督puzzle环境与奖励机制，提升视觉语言模型（VLM）的推理能力和任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习（RL）方法在视觉语言模型推理中存在依赖人工标注、奖励稀疏/平坦、推理与答案逻辑不一致等问题，限制了模型实用性和扩展性。

Method: 提出无监督奖励的Puzzle Curriculum GRPO（PC-GRPO）方法，用PatchFit、Rotation、Jigsaw三种自监督任务替换标注奖励，并设计基于难度动态加权的curriculum机制，缓解奖励平坦和稀疏问题，并在训练后监控推理与答案一致性（RAC），引入一致性奖励进一步提升表现。

Result: PC-GRPO方法在Qwen-7B/3B等模型和多项基准上验证，有效提升推理质量、训练稳定性以及下游任务准确率，RAC与准确度显著相关，新的奖励与curriculum机制改善了原方法的多项不足。

Conclusion: PC-GRPO为视觉语言模型提供了一种无需标注、可验证、可扩展、可解释的RL训练新途径，对提升VLM实际应用价值具有积极意义。

Abstract: Recent reinforcement learning (RL) approaches like outcome-supervised GRPO have advanced chain-of-thought reasoning in Vision Language Models (VLMs), yet key issues linger: (i) reliance on costly and noisy hand-curated annotations or external verifiers; (ii) flat and sparse reward schemes in GRPO; and (iii) logical inconsistency between a chain's reasoning and its final answer. We present Puzzle Curriculum GRPO (PC-GRPO), a supervision-free recipe for RL with Verifiable Rewards (RLVR) that strengthens visual reasoning in VLMs without annotations or external verifiers. PC-GRPO replaces labels with three self-supervised puzzle environments: PatchFit, Rotation (with binary rewards) and Jigsaw (with graded partial credit mitigating reward sparsity). To counter flat rewards and vanishing group-relative advantages, we introduce a difficulty-aware curriculum that dynamically weights samples and peaks at medium difficulty. We further monitor Reasoning-Answer Consistency (RAC) during post-training: mirroring reports for vanilla GRPO in LLMs, RAC typically rises early then degrades; our curriculum delays this decline, and consistency-enforcing reward schemes further boost RAC. RAC correlates with downstream accuracy. Across diverse benchmarks and on Qwen-7B and Qwen-3B backbones, PC-GRPO improves reasoning quality, training stability, and end-task accuracy, offering a practical path to scalable, verifiable, and interpretable RL post-training for VLMs.

</details>


### [14] [Adaptive Multimodal Person Recognition: A Robust Framework for Handling Missing Modalities](https://arxiv.org/abs/2512.14961)
*Aref Farhadipour,Teodora Vukovic,Volker Dellwo,Petr Motlicek,Srikanth Madikeri*

Main category: cs.CV

TL;DR: 本论文提出了一种能处理人脸、声音与手势三模态并对模态缺失具鲁棒性的身份识别框架。其通过多任务学习和跨模态融合机制，能动态适应信息不完整的情况，测试成绩优异。


<details>
  <summary>Details</summary>
Motivation: 现实中，身份识别时常出现音频、视觉或行为数据缺失的问题，传统方法难以应对模态丢失，亟需一种对数据缺失具备鲁棒性的多模态融合新方案。

Method: 作者采用多任务学习独立处理每个模态，然后利用跨注意力和门控融合机制促进模态间信息交互，并引入置信度加权的融合策略应对数据缺失或质量下降的情况。

Result: 在新引入的数据集CANDOR上，三模态系统Top-1识别准确率达99.18%，超越常规单模态和晚期融合方法。在标准数据集VoxCeleb1的双模态识别下达到99.92%的准确率。

Conclusion: 该方法在真实环境下可在一至两个模态缺失时仍保持极高识别率，整体表现优于现有方法。据称相关代码和数据已开源，便于后续研究与应用。

Abstract: Person recognition systems often rely on audio, visual, or behavioral cues, but real-world conditions frequently result in missing or degraded modalities. To address this challenge, we propose a Trimodal person identification framework that integrates voice, face, and gesture modalities, while remaining robust to modality loss. Our approach leverages multi-task learning to process each modality independently, followed by a cross-attention and gated fusion mechanisms to facilitate interaction across modalities. Moreover, a confidence-weighted fusion strategy dynamically adapts to missing and low-quality data, ensuring optimal classification even in Unimodal or Bimodal scenarios. We evaluate our method on CANDOR, a newly introduced interview-based multimodal dataset, which we benchmark for the first time. Our results demonstrate that the proposed Trimodal system achieves 99.18% Top-1 accuracy on person identification tasks, outperforming conventional Unimodal and late-fusion approaches. In addition, we evaluate our model on the VoxCeleb1 dataset as a benchmark and reach 99.92% accuracy in Bimodal mode. Moreover, we show that our system maintains high accuracy even when one or two modalities are unavailable, making it a robust solution for real-world person recognition applications. The code and data for this work are publicly available.

</details>


### [15] [Criticality Metrics for Relevance Classification in Safety Evaluation of Object Detection in Automated Driving](https://arxiv.org/abs/2512.15181)
*Jörg Gamerdinger,Sven Teufel,Stephan Amann,Oliver Bringmann*

Main category: cs.CV

TL;DR: 该论文分析并提出了用于自动驾驶物体检测系统安全评估的criticality度量方法，通过新策略提升了关键性分类准确性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶的首要目标是安全，因此需要准确全面的环境感知。现有性能评估指标不足以体现安全性，如何区分相关和不相关物体是一个关键挑战。

Method: 作者综述现有文献中的criticality相关度量方法，并在DeepAccident数据集上对比验证；提出了双向criticality评级与多度量聚合两种新策略提升评估准确性。

Result: 新提出的评估策略在criticality分类准确性上实现了最高100%的提升。

Conclusion: 该方法能显著提升自动驾驶物体检测系统中的安全评估效果，有助于推进更安全的自动驾驶系统发展。

Abstract: Ensuring safety is the primary objective of automated driving, which necessitates a comprehensive and accurate perception of the environment. While numerous performance evaluation metrics exist for assessing perception capabilities, incorporating safety-specific metrics is essential to reliably evaluate object detection systems. A key component for safety evaluation is the ability to distinguish between relevant and non-relevant objects - a challenge addressed by criticality or relevance metrics. This paper presents the first in-depth analysis of criticality metrics for safety evaluation of object detection systems. Through a comprehensive review of existing literature, we identify and assess a range of applicable metrics. Their effectiveness is empirically validated using the DeepAccident dataset, which features a variety of safety-critical scenarios. To enhance evaluation accuracy, we propose two novel application strategies: bidirectional criticality rating and multi-metric aggregation. Our approach demonstrates up to a 100% improvement in terms of criticality classification accuracy, highlighting its potential to significantly advance the safety evaluation of object detection systems in automated vehicles.

</details>


### [16] [Where is the Watermark? Interpretable Watermark Detection at the Block Level](https://arxiv.org/abs/2512.14994)
*Maria Bulychev,Neil G. Marchant,Benjamin I. P. Rubinstein*

Main category: cs.CV

TL;DR: 本文提出了一种结合局部嵌入与区域级可解释性的后置图像水印方法，能实现鲁棒且可解释的水印检测。


<details>
  <summary>Details</summary>
Motivation: 现有的图像水印方案大多是黑盒操作，仅生成整体检测分数，难以解释水印具体嵌入和篡改位置，这影响了用户信任和对水印完整性的理解。因此，亟需一种既鲁棒又可解释的水印检测方法。

Method: 作者在离散小波变换域内采用基于统计的分块策略嵌入水印，实现了水印信号的局部化嵌入，并可生成区域级的检测图，展示图像哪些部分被水印覆盖或被篡改。

Result: 该方法在面对常见图像变换时具有很强的鲁棒性，对语义操控也较为敏感。同时，水印对视觉几乎不可察觉。与现有方法相比，新方案在检出可解释性和鲁棒性上都表现优异，例如能经受住裁剪至一半图像面积的操作。

Conclusion: 本文方法兼具可解释性和鲁棒性，在提高水印可检测性的同时仍保持高不可见性，为数字内容保护和溯源提供了更可信的技术路径。

Abstract: Recent advances in generative AI have enabled the creation of highly realistic digital content, raising concerns around authenticity, ownership, and misuse. While watermarking has become an increasingly important mechanism to trace and protect digital media, most existing image watermarking schemes operate as black boxes, producing global detection scores without offering any insight into how or where the watermark is present. This lack of transparency impacts user trust and makes it difficult to interpret the impact of tampering. In this paper, we present a post-hoc image watermarking method that combines localised embedding with region-level interpretability. Our approach embeds watermark signals in the discrete wavelet transform domain using a statistical block-wise strategy. This allows us to generate detection maps that reveal which regions of an image are likely watermarked or altered. We show that our method achieves strong robustness against common image transformations while remaining sensitive to semantic manipulations. At the same time, the watermark remains highly imperceptible. Compared to prior post-hoc methods, our approach offers more interpretable detection while retaining competitive robustness. For example, our watermarks are robust to cropping up to half the image.

</details>


### [17] [Photorealistic Phantom Roads in Real Scenes: Disentangling 3D Hallucinations from Physical Geometry](https://arxiv.org/abs/2512.15423)
*Hoang Nguyen,Xiaohao Xu,Xiaonan Huang*

Main category: cs.CV

TL;DR: 本论文揭示了单目深度基础模型在处理感知模糊但几何平面的输入时，容易“幻觉”出虚假三维结构，并提出首个系统性框架用于检测、量化和缓解这种“3D幻影”现象。


<details>
  <summary>Details</summary>
Motivation: 虽然单目深度模型能通过大规模语义先验实现强泛化能力，但面对视觉错觉（如街头艺术等平面但易引发深度误判的场景）时，模型会出现“虚幻的三维结构”，这带来安全风险，急需可量化的衡量和改进手段。

Method: 作者建立了包括真实世界错觉（如街头艺术）的新数据集3D-Mirage；提出拉普拉斯评价框架和两项度量：偏离复合分数（DCS）和混淆复合分数（CCS）；并引入“锚定自蒸馏”方法，通过冻结教师模型、仅调整目标区域参数来强化结构正确性且防止遗忘。

Result: 该框架能显著检测和抑制单目深度模型对平面幻觉的误判问题，并以新度量展示了模型在结构鲁棒性和上下文稳定性方面的改进。

Conclusion: 工作为诊断和改善3D幻影提供了实用工具，建议MDE评价需从像素精度转向结构与语境鲁棒性，并将公开代码和数据集促进后续研究。

Abstract: Monocular depth foundation models achieve remarkable generalization by learning large-scale semantic priors, but this creates a critical vulnerability: they hallucinate illusory 3D structures from geometrically planar but perceptually ambiguous inputs. We term this failure the 3D Mirage. This paper introduces the first end-to-end framework to probe, quantify, and tame this unquantified safety risk. To probe, we present 3D-Mirage, the first benchmark of real-world illusions (e.g., street art) with precise planar-region annotations and context-restricted crops. To quantify, we propose a Laplacian-based evaluation framework with two metrics: the Deviation Composite Score (DCS) for spurious non-planarity and the Confusion Composite Score (CCS) for contextual instability. To tame this failure, we introduce Grounded Self-Distillation, a parameter-efficient strategy that surgically enforces planarity on illusion ROIs while using a frozen teacher to preserve background knowledge, thus avoiding catastrophic forgetting. Our work provides the essential tools to diagnose and mitigate this phenomenon, urging a necessary shift in MDE evaluation from pixel-wise accuracy to structural and contextual robustness. Our code and benchmark will be publicly available to foster this exciting research direction.

</details>


### [18] [Beyond Proximity: A Keypoint-Trajectory Framework for Classifying Affiliative and Agonistic Social Networks in Dairy Cattle](https://arxiv.org/abs/2512.14998)
*Sibi Parivendan,Kashfia Sailunaz,Suresh Neethirajan*

Main category: cs.CV

TL;DR: 该文提出了一种基于姿态识别的计算框架，通过分析动物骨骼关键点的时空几何特征，实现了奶牛社会行为的自动分类，显著提升了亲社会行为识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有农场动物社会行为评估方法多基于静态接近阈值，难以区分友好与敌对行为，限制了社交网络分析在商业场景的实用性。因此，亟需更精细化、可解释的自动检测技术。

Method: 作者设计了一个端到端计算机视觉流程：利用YOLOv11检测个体目标，结合监督识别和ByteTrack实现多目标跟踪，采用ZebraPose提取27个解剖关键点，并通过支持向量机（SVM）对姿态动态进行社交行为分类。核心在于采用姿态关键点轨迹而非简单距离或像素特征，区分社交行为的情感倾向。

Result: 该系统在商业奶牛场环境下的标注视频片段测试中，基于姿态信息的分类器对友好与敌对行为的识别准确率达77.51%，大幅优于基于距离阈值的传统方法。各流程模块（检测、追踪等）表现优异，实现了近实时效果。

Conclusion: 研究验证了基于计算机视觉和姿态分析的方法可用于自动区分动物社会互动类型，促进更具解释性和实用性的群体行为监测，有助于精确畜牧业和动物福利评估实际应用。

Abstract: Precision livestock farming requires objective assessment of social behavior to support herd welfare monitoring, yet most existing approaches infer interactions using static proximity thresholds that cannot distinguish affiliative from agonistic behaviors in complex barn environments. This limitation constrains the interpretability of automated social network analysis in commercial settings. We present a pose-based computational framework for interaction classification that moves beyond proximity heuristics by modeling the spatiotemporal geometry of anatomical keypoints. Rather than relying on pixel-level appearance or simple distance measures, the proposed method encodes interaction-specific motion signatures from keypoint trajectories, enabling differentiation of social interaction valence. The framework is implemented as an end-to-end computer vision pipeline integrating YOLOv11 for object detection (mAP@0.50: 96.24%), supervised individual identification (98.24% accuracy), ByteTrack for multi-object tracking (81.96% accuracy), ZebraPose for 27-point anatomical keypoint estimation, and a support vector machine classifier trained on pose-derived distance dynamics. On annotated interaction clips collected from a commercial dairy barn, the classifier achieved 77.51% accuracy in distinguishing affiliative and agonistic behaviors using pose information alone. Comparative evaluation against a proximity-only baseline shows substantial gains in behavioral discrimination, particularly for affiliative interactions. The results establish a proof-of-concept for automated, vision-based inference of social interactions suitable for constructing interaction-aware social networks, with near-real-time performance on commodity hardware.

</details>


### [19] [Evaluating the Capability of Video Question Generation for Expert Knowledge Elicitation](https://arxiv.org/abs/2512.15006)
*Huaying Zhang,Atsushi Hashimoto,Tosho Hirasawa*

Main category: cs.CV

TL;DR: 本文提出了一种用于评估视频问答中自动生成问题质量的新协议，并引入了一个新的问答数据集EgoExoAsk，以用于模型评测。


<details>
  <summary>Details</summary>
Motivation: 当前视频问答领域主要关注于用生成的问题能否被正确回答，而很少关注问题本身能否有效引导专家提供未知新知识。该工作旨在提升问题生成模型能引出专家新信息的能力，以实现更高效的人机知识交流。

Method: 作者提出通过“问题-答案检索”模拟专家问答场景，从而量化评估问题生成模型引出新知识的能力。同时，构建了包含27,666组问答对的新数据集EgoExoAsk（基于Ego-Exo4D专家标注），用于训练和评测检索器，并在标准视频片段上进行基准测试。

Result: 实验证明，该协议评估的分数与模型生成问题的语境丰富程度相关，模型能访问更丰富上下文时，生成的问题被评为更优。

Conclusion: 提出的新评估协议和数据集有效用于衡量视频问答中问题生成质量，可促进该领域模型的持续进步。

Abstract: Skilled human interviewers can extract valuable information from experts. This raises a fundamental question: what makes some questions more effective than others? To address this, a quantitative evaluation of question-generation models is essential. Video question generation (VQG) is a topic for video question answering (VideoQA), where questions are generated for given answers. Their evaluation typically focuses on the ability to answer questions, rather than the quality of generated questions. In contrast, we focus on the question quality in eliciting unseen knowledge from human experts. For a continuous improvement of VQG models, we propose a protocol that evaluates the ability by simulating question-answering communication with experts using a question-to-answer retrieval. We obtain the retriever by constructing a novel dataset, EgoExoAsk, which comprises 27,666 QA pairs generated from Ego-Exo4D's expert commentary annotation. The EgoExoAsk training set is used to obtain the retriever, and the benchmark is constructed on the validation set with Ego-Exo4D video segments. Experimental results demonstrate our metric reasonably aligns with question generation settings: models accessing richer context are evaluated better, supporting that our protocol works as intended. The EgoExoAsk dataset is available in https://github.com/omron-sinicx/VQG4ExpertKnowledge .

</details>


### [20] [Model Agnostic Preference Optimization for Medical Image Segmentation](https://arxiv.org/abs/2512.15009)
*Yunseong Nam,Jiwon Jang,Dongkyu Won,Sang Hyun Park,Soopil Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种新的无模型偏好优化（MAPO）方法，通过Dropout产生多样化预测用于偏好学习，在无需直接真实标注的情况下提升医学图像分割效果。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割的偏好优化方法通常依赖于特定模型结构和低多样性采样，限制了其通用性和效果。作者希望设计一种适用于所有主流架构、能产生更高多样性预测的训练框架。

Method: 提出MAPO训练框架，通过Dropout机制生成多样化分割预测，并利用这些预测之间的相对偏好信号来优化模型参数，无需直接使用真实标注数据，且适用于2D/3D CNN与Transformer等不同架构。

Result: 在多个医学图像数据集上的实验证明，MAPO在增强分割边界精度、降低过拟合、优化训练稳定性方面均优于传统监督学习方法。

Conclusion: MAPO为医学图像分割引入了一种高效、通用、无需直接标注的新型训练范式，有望改善模型泛化和部署效果。

Abstract: Preference optimization offers a scalable supervision paradigm based on relative preference signals, yet prior attempts in medical image segmentation remain model-specific and rely on low-diversity prediction sampling. In this paper, we propose MAPO (Model-Agnostic Preference Optimization), a training framework that utilizes Dropout-driven stochastic segmentation hypotheses to construct preference-consistent gradients without direct ground-truth supervision. MAPO is fully architecture- and dimensionality-agnostic, supporting 2D/3D CNN and Transformer-based segmentation pipelines. Comprehensive evaluations across diverse medical datasets reveal that MAPO consistently enhances boundary adherence, reduces overfitting, and yields more stable optimization dynamics compared to conventional supervised training.

</details>


### [21] [MVGSR: Multi-View Consistent 3D Gaussian Super-Resolution via Epipolar Guidance](https://arxiv.org/abs/2512.15048)
*Kaizhe Zhang,Shinan Chen,Qian Zhao,Weizhan Zhang,Caixia Yan,Yudeng Xin*

Main category: cs.CV

TL;DR: 本文提出了一种多视图一致性的3D高斯泼溅超分辨率方法（MVGSR），通过多视角信息融合实现高分辨率渲染，取得了业界领先表现。


<details>
  <summary>Details</summary>
Motivation: 受限于训练图像分辨率，当前3D高斯泼溅（3DGS）只能生成低分辨率场景，无法实现高质量渲染，现有单图像或视频超分方法都存在跨视角一致性和多视融合不足的问题，特别是在多视图无序场景中。

Method: 提出了一种辅助视角选择机制，根据相机位姿适配任意组织的多视图集，无需时序连续；创新性地将带极线约束的多视图注意力机制引入3DGS超分网络，实现多视角信息有选择地融合，提升几何一致性和细节还原。

Result: 在物体级与场景级的3DGS超分辨率基准测试上，方法取得了目前最好的性能，验证了多视一致性与注意力机制的有效性。

Conclusion: MVGSR有效整合多视角信息，解决了3DGS超分中跨视一致性与细节缺失的问题，在实际无结构多视场景具有广泛适用性，并且推动了3D重建与渲染的精细化发展。

Abstract: Scenes reconstructed by 3D Gaussian Splatting (3DGS) trained on low-resolution (LR) images are unsuitable for high-resolution (HR) rendering. Consequently, a 3DGS super-resolution (SR) method is needed to bridge LR inputs and HR rendering. Early 3DGS SR methods rely on single-image SR networks, which lack cross-view consistency and fail to fuse complementary information across views. More recent video-based SR approaches attempt to address this limitation but require strictly sequential frames, limiting their applicability to unstructured multi-view datasets. In this work, we introduce Multi-View Consistent 3D Gaussian Splatting Super-Resolution (MVGSR), a framework that focuses on integrating multi-view information for 3DGS rendering with high-frequency details and enhanced consistency. We first propose an Auxiliary View Selection Method based on camera poses, making our method adaptable for arbitrarily organized multi-view datasets without the need of temporal continuity or data reordering. Furthermore, we introduce, for the first time, an epipolar-constrained multi-view attention mechanism into 3DGS SR, which serves as the core of our proposed multi-view SR network. This design enables the model to selectively aggregate consistent information from auxiliary views, enhancing the geometric consistency and detail fidelity of 3DGS representations. Extensive experiments demonstrate that our method achieves state-of-the-art performance on both object-centric and scene-level 3DGS SR benchmarks.

</details>


### [22] [Asynchronous Event Stream Noise Filtering for High-frequency Structure Deformation Measurement](https://arxiv.org/abs/2512.15055)
*Yifei Bian,Banglei Guan,Zibin Liu,Ang Su,Shiyao Zhu,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 本文提出了一种利用事件相机与LED标记物的新方法，准确测量大型结构的高频变形，克服传统测量手段的限制。


<details>
  <summary>Details</summary>
Motivation: 传统高速相机受限于照明条件和高昂的成本，不适合复杂环境下结构高频变形的测量，因此需要经济且适用性强的新方法。

Method: 采用事件相机配合LED标记物，通过事件流特征和时空相关去除噪声，区分运动引发事件与LED闪烁事件，提取高频移动的LED标记，从而以单目事件相机实现高频面内变形测量。

Result: 实验验证了该方法在高频面内变形测量中的准确性。

Conclusion: 基于事件相机和LED标记物的方法能低成本、稳健地实现大型结构高频变形的高精度测量，具有实际应用前景。

Abstract: Large-scale structures suffer high-frequency deformations due to complex loads. However, harsh lighting conditions and high equipment costs limit measurement methods based on traditional high-speed cameras. This paper proposes a method to measure high-frequency deformations by exploiting an event camera and LED markers. Firstly, observation noise is filtered based on the characteristics of the event stream generated by LED markers blinking and spatiotemporal correlation. Then, LED markers are extracted from the event stream after differentiating between motion-induced events and events from LED blinking, which enables the extraction of high-speed moving LED markers. Ultimately, high-frequency planar deformations are measured by a monocular event camera. Experimental results confirm the accuracy of our method in measuring high-frequency planar deformations.

</details>


### [23] [Tracking spatial temporal details in ultrasound long video via wavelet analysis and memory bank](https://arxiv.org/abs/2512.15066)
*Chenxiao Zhang,Runshi Zhang,Junchen Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于记忆库的多尺度小波滤波与融合网络（MWNet），显著提升了超声视频中小病灶及目标器官的分割效果，尤其在长视频处理和边界保持方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 超声视频因低对比度和噪声背景常导致器官边界分割错误及小目标丢失，而视频追踪中的目标跟踪和长序列处理同样是技术难点。现有方法难以兼顾高精度细粒度空间特征提取与序列记忆，亟需创新方法提升超声分割的准确率，特别是对小目标和边界处理。

Method: 提出一种融合记忆库、波段（小波）卷积与融合机制的编码器-解码器结构。1）通过记忆库型小波卷积同时捕获类别与细节信息，并利用邻域信息。2）级联小波压缩用于多尺度频域特征融合、扩大感受野。3）设计基于交叉注意力和记忆压缩机制的长短时记忆模块以实现长视频目标追踪。4）在解码器中通过自适应小波滤波实现高频边界敏感特征的自适应融合。

Result: 在4个公开超声视频数据集上，与最先进方法（SOTA）对比，本文方法在分割精度等指标上有明显提升，尤其在小目标（如小甲状腺结节）长序列分割中表现优异。

Conclusion: 新提出的MWNet方法有效缓解了超声视频分割的边界误分与目标丢失问题，特别是在小目标、长序列追踪应用场景下显示出强劲的性能和应用潜力，为计算机辅助外科临床场景带来更精准的解决方案。

Abstract: Medical ultrasound videos are widely used for medical inspections, disease diagnosis and surgical planning. High-fidelity lesion area and target organ segmentation constitutes a key component of the computer-assisted surgery workflow. The low contrast levels and noisy backgrounds of ultrasound videos cause missegmentation of organ boundary, which may lead to small object losses and increase boundary segmentation errors. Object tracking in long videos also remains a significant research challenge. To overcome these challenges, we propose a memory bank-based wavelet filtering and fusion network, which adopts an encoder-decoder structure to effectively extract fine-grained detailed spatial features and integrate high-frequency (HF) information. Specifically, memory-based wavelet convolution is presented to simultaneously capture category, detailed information and utilize adjacent information in the encoder. Cascaded wavelet compression is used to fuse multiscale frequency-domain features and expand the receptive field within each convolutional layer. A long short-term memory bank using cross-attention and memory compression mechanisms is designed to track objects in long video. To fully utilize the boundary-sensitive HF details of feature maps, an HF-aware feature fusion module is designed via adaptive wavelet filters in the decoder. In extensive benchmark tests conducted on four ultrasound video datasets (two thyroid nodule, the thyroid gland, the heart datasets) compared with the state-of-the-art methods, our method demonstrates marked improvements in segmentation metrics. In particular, our method can more accurately segment small thyroid nodules, demonstrating its effectiveness for cases involving small ultrasound objects in long video. The code is available at https://github.com/XiAooZ/MWNet.

</details>


### [24] [PMMD: A pose-guided multi-view multi-modal diffusion for person generation](https://arxiv.org/abs/2512.15069)
*Ziyu Shang,Haoran Liu,Rongchao Zhang,Zhiqian Wei,Tongtong Feng*

Main category: cs.CV

TL;DR: 本文提出了PMMD方法，通过多视角参考、姿态图与文本提示，结合扩散模型生成高一致性、易控制的人像图像，有效改善了遮挡、服饰失真与姿态不对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有可控人像生成方法在多视角一致性、局部细节及姿态准确性方面表现不理想，限制了虚拟试衣、图像编辑等应用效果。

Method: 提出了一种基于多模态扩散的PMMD框架，采用联合编码器整合视觉、姿态与文本语义信息，并设计ResCVA模块加强局部细节表现，通过跨模态融合模块提升全局结构及图文语义的一致性。

Result: 在DeepFashion MultiModal数据集实验表明，PMMD在一致性、细节还原及生成效果可控性方面均超过了现有主流方法。

Conclusion: PMMD为多模态、可控人像生成提供了新范式，有效提升了人像一致性与真实性，具有广泛实际应用潜力。

Abstract: Generating consistent human images with controllable pose and appearance is essential for applications in virtual try on, image editing, and digital human creation. Current methods often suffer from occlusions, garment style drift, and pose misalignment. We propose Pose-guided Multi-view Multimodal Diffusion (PMMD), a diffusion framework that synthesizes photorealistic person images conditioned on multi-view references, pose maps, and text prompts. A multimodal encoder jointly models visual views, pose features, and semantic descriptions, which reduces cross modal discrepancy and improves identity fidelity. We further design a ResCVA module to enhance local detail while preserving global structure, and a cross modal fusion module that integrates image semantics with text throughout the denoising pipeline. Experiments on the DeepFashion MultiModal dataset show that PMMD outperforms representative baselines in consistency, detail preservation, and controllability. Project page and code are available at https://github.com/ZANMANGLOOPYE/PMMD.

</details>


### [25] [Uni-Parser Technical Report](https://arxiv.org/abs/2512.15098)
*Xi Fang,Haoyi Tao,Shuwen Yang,Suyang Zhong,Haocheng Lu,Han Lyu,Chaozheng Huang,Xinyu Li,Linfeng Zhang,Guolin Ke*

Main category: cs.CV

TL;DR: 本文提出了Uni-Parser，一种针对科学文献和专利的高性能、强鲁棒性和高性价比的文档解析引擎，支持大规模分布式部署和多种下游应用。


<details>
  <summary>Details</summary>
Motivation: 当前文档解析方法多基于流水线，难以兼顾跨模态的细粒度对齐及灵活扩展，对大规模处理和新兴模态支持有限。该工作旨在解决高吞吐、高准确性并兼容多模态文档的解析需求。

Method: Uni-Parser采用模块化、多专家、松耦合架构，实现文本、公式、表格等多模态内容的高效对齐与解析，并引入自适应GPU负载均衡、分布式推理和动态模块编排；支持整体或特定模态解析模式，优化云端大规模部署。

Result: 在8块NVIDIA RTX 4090D GPU上，Uni-Parser可实现单秒处理20页PDF，支持数十亿页文档的高效推理，满足批量文献检索、摘要、化学结构和生物活性数据抽取等多场景需求。

Conclusion: Uni-Parser具备强大的扩展性与工业级性能，可推动大规模语料构建、AI4Science和下一代大模型的训练，为科学和技术文档解析领域提供了新的解决方案。

Abstract: This technical report introduces Uni-Parser, an industrial-grade document parsing engine tailored for scientific literature and patents, delivering high throughput, robust accuracy, and cost efficiency. Unlike pipeline-based document parsing methods, Uni-Parser employs a modular, loosely coupled multi-expert architecture that preserves fine-grained cross-modal alignments across text, equations, tables, figures, and chemical structures, while remaining easily extensible to emerging modalities. The system incorporates adaptive GPU load balancing, distributed inference, dynamic module orchestration, and configurable modes that support either holistic or modality-specific parsing. Optimized for large-scale cloud deployment, Uni-Parser achieves a processing rate of up to 20 PDF pages per second on 8 x NVIDIA RTX 4090D GPUs, enabling cost-efficient inference across billions of pages. This level of scalability facilitates a broad spectrum of downstream applications, ranging from literature retrieval and summarization to the extraction of chemical structures, reaction schemes, and bioactivity data, as well as the curation of large-scale corpora for training next-generation large language models and AI4Science models.

</details>


### [26] [Is Nano Banana Pro a Low-Level Vision All-Rounder? A Comprehensive Evaluation on 14 Tasks and 40 Datasets](https://arxiv.org/abs/2512.15110)
*Jialong Zuo,Haoyou Deng,Hanyu Zhou,Jiaxin Zhu,Yicheng Zhang,Yiwei Zhang,Yongxin Yan,Kaixing Huang,Weisen Chen,Yongtai Deng,Rui Jin,Nong Sang,Changxin Gao*

Main category: cs.CV

TL;DR: 本文评估了商用文本到图像生成器Nano Banana Pro在传统低级视觉任务上的表现，发现其在主观视觉质量上优于专家模型，但在定量指标上落后。


<details>
  <summary>Details</summary>
Motivation: 尽管像Nano Banana Pro这样的生成模型在视觉内容创作领域广受关注，但其在低层次视觉任务中的通用性能鲜有系统研究。研究动机在于系统探索这类生成模型能否成为低级视觉任务的全能工具。

Method: 作者设计了包含14项不同低级视觉任务、覆盖40个数据集的大规模零样本测试，仅用简单文本提示词，无需微调，对比了Nano Banana Pro和现有专家型模型。

Result: 分析显示，Nano Banana Pro在主观视觉质量（如细节虚构与视觉真实度）上超越专家模型，但在像素精度为主的传统定量指标上表现不佳。

Conclusion: Nano Banana Pro在低级视觉任务上表现出强大的零样本适应能力，但要达到专家水平的高保真度和像素一致性，仍面临明显挑战。

Abstract: The rapid evolution of text-to-image generation models has revolutionized visual content creation. While commercial products like Nano Banana Pro have garnered significant attention, their potential as generalist solvers for traditional low-level vision challenges remains largely underexplored. In this study, we investigate the critical question: Is Nano Banana Pro a Low-Level Vision All-Rounder? We conducted a comprehensive zero-shot evaluation across 14 distinct low-level tasks spanning 40 diverse datasets. By utilizing simple textual prompts without fine-tuning, we benchmarked Nano Banana Pro against state-of-the-art specialist models. Our extensive analysis reveals a distinct performance dichotomy: while \textbf{Nano Banana Pro demonstrates superior subjective visual quality}, often hallucinating plausible high-frequency details that surpass specialist models, it lags behind in traditional reference-based quantitative metrics. We attribute this discrepancy to the inherent stochasticity of generative models, which struggle to maintain the strict pixel-level consistency required by conventional metrics. This report identifies Nano Banana Pro as a capable zero-shot contender for low-level vision tasks, while highlighting that achieving the high fidelity of domain specialists remains a significant hurdle.

</details>


### [27] [3DProxyImg: Controllable 3D-Aware Animation Synthesis from Single Image via 2D-3D Aligned Proxy Embedding](https://arxiv.org/abs/2512.15126)
*Yupeng Zhu,Xiongzhen Zhang,Ye Chen,Bingbing Ni*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级3D动画生成框架，能够高效地从单张图片生成可控的3D动画，同时兼顾渲染质量与三维控制，并优于现有的视频生成方法。


<details>
  <summary>Details</summary>
Motivation: 当前3D动画制作流程复杂、耗时且成本高，尽管AIGC方法出现并自动化部分流程，但在3D可控性和高质量渲染之间存在权衡，难以两者兼得。

Method: 提出将几何控制与外观合成解耦，采用2D-3D对齐的代理表示形式：利用粗略的3D估算做结构承载，同时将高保真外观和视图合成交给图像空间的生成先验。这样既能实现传统3D动画的运动控制和交互性，又不需精确建模或大量优化计算。

Result: 实验证明该方法能在低功耗平台上高效生成动画，且在身份保持、几何一致性、纹理一致性以及用户交互性方面均优于基于视频的方法。

Conclusion: 该方法突破了单图3D动画中质量与控制的瓶颈，提供了一种高效、可控且低成本的制作途径，有望为3D动画生产带来变革。

Abstract: 3D animation is central to modern visual media, yet traditional production pipelines remain labor-intensive, expertise-demanding, and computationally expensive. Recent AIGC-based approaches partially automate asset creation and rigging, but they either inherit the heavy costs of full 3D pipelines or rely on video-synthesis paradigms that sacrifice 3D controllability and interactivity. We focus on single-image 3D animation generation and argue that progress is fundamentally constrained by a trade-off between rendering quality and 3D control.
  To address this limitation, we propose a lightweight 3D animation framework that decouples geometric control from appearance synthesis. The core idea is a 2D-3D aligned proxy representation that uses a coarse 3D estimate as a structural carrier, while delegating high-fidelity appearance and view synthesis to learned image-space generative priors. This proxy formulation enables 3D-aware motion control and interaction comparable to classical pipelines, without requiring accurate geometry or expensive optimization, and naturally extends to coherent background animation. Extensive experiments demonstrate that our method achieves efficient animation generation on low-power platforms and outperforms video-based 3D animation generation in identity preservation, geometric and textural consistency, and the level of precise, interactive control it offers to users.

</details>


### [28] [Borrowing from anything: A generalizable framework for reference-guided instance editing](https://arxiv.org/abs/2512.15138)
*Shengxiao Zhou,Chenghua Li,Jianhao Huang,Qinghao Hu,Yifan Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种叫做GENIE的新框架，用于实例级参考引导编辑，通过模块化方法有效地实现了语义解耦，并在高难度数据集上取得了最新最好结果。


<details>
  <summary>Details</summary>
Motivation: 参考引导的实例编辑常常受到语义纠缠问题的限制，即参考图像的内在属性与外在属性难以分离，导致编辑效果受损。作者希望突破这一瓶颈，实现对参考信息的精确选择与应用。

Method: 作者提出了GENIE框架。首先用空间对齐模块（SAM）矫正源与目标的空间错位；然后用自适应残差缩放模块（ARSM）自适应学习应借用的参考内部属性并抑制外部属性影响；最后用渐进注意力融合机制（PAF）控制参考外观对目标的融合应用，保持目标结构。

Result: 在AnyInsertion等高难度数据集上进行了大量实验，GENIE在保真度和鲁棒性上都优于现有方法，达到了业内最优表现。

Conclusion: GENIE能够实现显式语义解耦，在参考引导的实例编辑任务上树立了新标准，具备出色的泛化性及效果。

Abstract: Reference-guided instance editing is fundamentally limited by semantic entanglement, where a reference's intrinsic appearance is intertwined with its extrinsic attributes. The key challenge lies in disentangling what information should be borrowed from the reference, and determining how to apply it appropriately to the target. To tackle this challenge, we propose GENIE, a Generalizable Instance Editing framework capable of achieving explicit disentanglement. GENIE first corrects spatial misalignments with a Spatial Alignment Module (SAM). Then, an Adaptive Residual Scaling Module (ARSM) learns what to borrow by amplifying salient intrinsic cues while suppressing extrinsic attributes, while a Progressive Attention Fusion (PAF) mechanism learns how to render this appearance onto the target, preserving its structure. Extensive experiments on the challenging AnyInsertion dataset demonstrate that GENIE achieves state-of-the-art fidelity and robustness, setting a new standard for disentanglement-based instance editing.

</details>


### [29] [Explainable Action Form Assessment by Exploiting Multimodal Chain-of-Thoughts Reasoning](https://arxiv.org/abs/2512.15153)
*Mengshi Qi,Yeteng Wu,Xianlin Zhang,Huadong Ma*

Main category: cs.CV

TL;DR: 本文针对人类动作标准性评估与反馈提出新任务与新数据集，同时开发了一种可解释动作评估框架，有效提升动作分类、质量评估及解释能力。


<details>
  <summary>Details</summary>
Motivation: 当前视频理解方法侧重于识别动作类型与位置，无法对动作标准性进行有效评估和提供详细反馈，且现有数据集缺乏动作标准性层次标签与解释性反馈。因此，迫切需要一个面向复杂场景、具解释性反馈机制的动作评估系统。

Method: 作者定义了“人类动作形式评估（AFA）”任务，建设了包含健身与武术领域多层次标注的大型数据集CoT-AFA，并采用链式思维解释范式，提供从动作识别到分析与改进建议的全流程解释。同时，提出Explainable Fitness Assessor框架，通过双流处理与动态门控机制融合视觉与语义，提升分析与解释能力。

Result: 提出的方法在解释生成（CIDEr提升16%）、动作分类（准确率提升2.7%）、动作质量评估（准确率提升2.1%）等方面均取得明显提升，证明了方法和数据集的有效性及前景。

Conclusion: 该研究为动作标准性评估任务提出了更加系统且具解释能力的新方案，可为实际应用中的动作优化和指导提供理论与工具支持，具有广阔应用前景。

Abstract: Evaluating whether human action is standard or not and providing reasonable feedback to improve action standardization is very crucial but challenging in real-world scenarios. However, current video understanding methods are mainly concerned with what and where the action is, which is unable to meet the requirements. Meanwhile, most of the existing datasets lack the labels indicating the degree of action standardization, and the action quality assessment datasets lack explainability and detailed feedback. Therefore, we define a new Human Action Form Assessment (AFA) task, and introduce a new diverse dataset CoT-AFA, which contains a large scale of fitness and martial arts videos with multi-level annotations for comprehensive video analysis. We enrich the CoT-AFA dataset with a novel Chain-of-Thought explanation paradigm. Instead of offering isolated feedback, our explanations provide a complete reasoning process--from identifying an action step to analyzing its outcome and proposing a concrete solution. Furthermore, we propose a framework named Explainable Fitness Assessor, which can not only judge an action but also explain why and provide a solution. This framework employs two parallel processing streams and a dynamic gating mechanism to fuse visual and semantic information, thereby boosting its analytical capabilities. The experimental results demonstrate that our method has achieved improvements in explanation generation (e.g., +16.0% in CIDEr), action classification (+2.7% in accuracy) and quality assessment (+2.1% in accuracy), revealing great potential of CoT-AFA for future studies. Our dataset and source code is available at https://github.com/MICLAB-BUPT/EFA.

</details>


### [30] [EagleVision: A Dual-Stage Framework with BEV-grounding-based Chain-of-Thought for Spatial Intelligence](https://arxiv.org/abs/2512.15160)
*Jiaxu Wan,Xu Wang,Mengwei Xie,Hang Zhang,Mu Xu,Yang Han,Hong Zhang,Ding Yuan,Yifan Yang*

Main category: cs.CV

TL;DR: 本文提出了EagleVision框架，通过分阶段（宏观感知与微观验证）提升多模态空间推理能力，实现更强一致性和溯源能力，并在基准测试中取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有空间智能方法大多仅将3D信息附加于2D推理流程，或与黑盒重建模块耦合，导致空间一致性差、视角多样性受限且推理过程难以溯源。针对构建全球空间感知、3D假设与视频帧显式关联、空间奖励设计等关键挑战，有必要开发新方法提升空间推理的能力和可解释性。

Method: EagleVision采用双阶段流程，包括：1）宏观感知阶段，利用语义-视角融合行列式点过程（SPF-DPP）从长视频中选取兼顾几何和语义的关键帧，满足固定token预算；2）微观验证阶段，将空间推理形式化为基于鸟瞰图（BEV）坐标的姿态查询，智能体迭代地在BEV上预测位置、检索最近真实帧，并使用依据预测姿态与观测视图一致性的空间奖励，通过强化学习进行纯自主训练。

Result: 在VSI-Bench基准上，EagleVision在开源视觉-语言模型中取得了最优性能，表现出强大且可泛化的空间理解能力。

Conclusion: EagleVision极大提升了多模态空间推理框架的空间一致性、证据可追溯性和泛化能力。通过高效帧选取和空间自监督强化学习，为视觉-语言模型带来更好的空间认知能力。

Abstract: Recent spatial intelligence approaches typically attach 3D cues to 2D reasoning pipelines or couple MLLMs with black-box reconstruction modules, leading to weak spatial consistency, limited viewpoint diversity, and evidence chains that cannot be traced back to supporting views. Frameworks for "thinking with images" (e.g., ChatGPT-o3 and DeepEyes) show that stepwise multimodal reasoning can emerge by interleaving hypothesis formation with active acquisition of visual evidence, but they do not address three key challenges in spatial Chain-of-Thought (CoT): building global space perception under strict token budgets, explicitly associating 3D hypotheses with video frames for verification, and designing spatially grounded rewards for reinforcement learning. To address these issues, we present EagleVision, a dual-stage framework for progressive spatial cognition through macro perception and micro verification. In the macro perception stage, EagleVision employs a semantics-perspective-fusion determinantal point process (SPF-DPP) to select a compact set of geometry- and semantics-aware keyframes from long videos under a fixed token budget. In the micro verification stage, we formalize spatial CoT as BEV-grounded pose querying: the agent iteratively predicts poses on a BEV plane, retrieves the nearest real frames, and is trained purely by reinforcement learning with a spatial grounding reward that scores the consistency between predicted poses and observed views. On VSI-Bench, EagleVision achieves state-of-the-art performance among open-source vision-language models, demonstrating strong and generalizable spatial understanding.

</details>


### [31] [Cross-modal ultra-scale learning with tri-modalities of renal biopsy images for glomerular multi-disease auxiliary diagnosis](https://arxiv.org/abs/2512.15171)
*Kaixing Long,Danyi Weng,Yun Mi,Zhentai Zhang,Yanmeng Lu,Jian Geng,Zhitao Zhou,Liming Zhong,Qianjin Feng,Wei Yang,Lei Cao*

Main category: cs.CV

TL;DR: 该论文提出了一种跨模态超尺度学习网络（CMUS-Net），用于肾小球多疾病辅助诊断，通过融合三种肾活检图像（TEM、OM、IM）实现自动分类，在自有数据集上取得了较高的准确率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于TEM（纳米级）与OM、IM（微米级）图像在尺度上的巨大差异，导致现有多模态多尺度模型难以有效融合特征，从而影响多疾病分类的准确性。

Method: 提出了CMUS-Net网络，利用多种超微结构信息跨尺度弥合纳米和微米图像的尺度差异。具体方法包括引入稀疏多实例学习模块聚合TEM特征、设计跨模态尺度注意力模块促进特征交互、结合多重损失函数平衡不同模态的重要性，以提升分类精度。

Result: CMUS-Net在自有数据集上对IgA肾病（IgAN）、膜性肾病（MN）、狼疮性肾炎（LN）三类疾病的自动分类准确率ACC达到95.37±2.41%，AUC为99.05±0.53%，F1-score为95.32±2.41%。表现优于其他多模态、多尺度方法，并展示出良好的泛化能力（如MN分期）。

Conclusion: CMUS-Net能够有效融合跨尺度、多模态肾活检图像特征，准确自动分类多种肾小球疾病，有望成为病理医生辅助诊断的重要工具。

Abstract: Constructing a multi-modal automatic classification model based on three types of renal biopsy images can assist pathologists in glomerular multi-disease identification. However, the substantial scale difference between transmission electron microscopy (TEM) image features at the nanoscale and optical microscopy (OM) or immunofluorescence microscopy (IM) images at the microscale poses a challenge for existing multi-modal and multi-scale models in achieving effective feature fusion and improving classification accuracy. To address this issue, we propose a cross-modal ultra-scale learning network (CMUS-Net) for the auxiliary diagnosis of multiple glomerular diseases. CMUS-Net utilizes multiple ultrastructural information to bridge the scale difference between nanometer and micrometer images. Specifically, we introduce a sparse multi-instance learning module to aggregate features from TEM images. Furthermore, we design a cross-modal scale attention module to facilitate feature interaction, enhancing pathological semantic information. Finally, multiple loss functions are combined, allowing the model to weigh the importance among different modalities and achieve precise classification of glomerular diseases. Our method follows the conventional process of renal biopsy pathology diagnosis and, for the first time, performs automatic classification of multiple glomerular diseases including IgA nephropathy (IgAN), membranous nephropathy (MN), and lupus nephritis (LN) based on images from three modalities and two scales. On an in-house dataset, CMUS-Net achieves an ACC of 95.37+/-2.41%, an AUC of 99.05+/-0.53%, and an F1-score of 95.32+/-2.41%. Extensive experiments demonstrate that CMUS-Net outperforms other well-known multi-modal or multi-scale methods and show its generalization capability in staging MN. Code is available at https://github.com/SMU-GL-Group/MultiModal_lkx/tree/main.

</details>


### [32] [Robust and Calibrated Detection of Authentic Multimedia Content](https://arxiv.org/abs/2512.15182)
*Sarim Hashmi,Abdelrahman Elsayed,Mohammed Talha Alam,Samuele Poppi,Nils Lukas*

Main category: cs.CV

TL;DR: 本文提出一种基于再合成（resynthesis）框架的新型深度伪造检测方法，着重解决现有方法在后验检测和对抗鲁棒性上的不足。该方法在高精度、低召回率的设定下实现对真实数据的可靠认证，并对计算受限的对手具备鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型能够制造高度逼真的伪造内容，正被大规模滥用于数字媒体，威胁其真实性。现有检测手段存在两大问题：无法后验区分部分伪造内容，导致高假阳性率；对抗鲁棒性差，易被低算力的对抗者规避。因此需要新的解决方案。

Method: 作者提出了基于校准的再合成检测方法：对给定的样本进行再合成，然后利用再合成过程中的信息判定其真实性，并通过精心设计，保证低假阳性并能抵抗高效对手的攻击。该方法适用于多种模态，利用了先进的反演技术。

Result: 实验结果表明，该方法在真实样本验证中比现有方法更可靠，能够有效控制且降低假阳性率，对运算受限对手表现出更强的鲁棒性，而现有检测方法在相同算力条件下易被攻破。

Conclusion: 所提出的再合成检测框架在高精度低召回率设定下，对深度伪造检测展现出更优的可靠性和对抗鲁棒性，是应对当前深度伪造内容泛滥的有效新方向。

Abstract: Generative models can synthesize highly realistic content, so-called deepfakes, that are already being misused at scale to undermine digital media authenticity. Current deepfake detection methods are unreliable for two reasons: (i) distinguishing inauthentic content post-hoc is often impossible (e.g., with memorized samples), leading to an unbounded false positive rate (FPR); and (ii) detection lacks robustness, as adversaries can adapt to known detectors with near-perfect accuracy using minimal computational resources. To address these limitations, we propose a resynthesis framework to determine if a sample is authentic or if its authenticity can be plausibly denied. We make two key contributions focusing on the high-precision, low-recall setting against efficient (i.e., compute-restricted) adversaries. First, we demonstrate that our calibrated resynthesis method is the most reliable approach for verifying authentic samples while maintaining controllable, low FPRs. Second, we show that our method achieves adversarial robustness against efficient adversaries, whereas prior methods are easily evaded under identical compute budgets. Our approach supports multiple modalities and leverages state-of-the-art inversion techniques.

</details>


### [33] [ERIENet: An Efficient RAW Image Enhancement Network under Low-Light Environment](https://arxiv.org/abs/2512.15186)
*Jianan Wang,Yang Hong,Hesong Li,Tao Wang,Songrong Liu,Ying Fu*

Main category: cs.CV

TL;DR: 本文提出了一种高效的RAW图像增强网络（ERIENet），可在低照度图像增强任务中实现更优的速度和效果，尤其关注对绿色通道信息的利用。


<details>
  <summary>Details</summary>
Motivation: 现有低照度增强方法多序贯处理多尺度信息，难以同时兼顾高效轻量化与速度，并且忽略了RAW图像绿色通道的信息价值。

Method: 提出了ERIENet神经网络，采用高效多尺度全并行结构和新颖的通道感知残差密集块。同时引入绿色通道引导分支，充分利用绿色通道的信息来提升重建效果。

Result: 在常用低照度图像增强数据集上，ERIENet在质量和效率上均优于现有主流方法。4K分辨率下，单块RTX 3090显卡可达146 FPS。

Conclusion: ERIENet不仅提升了RAW图像低照度增强的重建质量，也显著提高了处理速度，有助于实际应用场景的高效部署。

Abstract: RAW images have shown superior performance than sRGB images in many image processing tasks, especially for low-light image enhancement. However, most existing methods for RAW-based low-light enhancement usually sequentially process multi-scale information, which makes it difficult to achieve lightweight models and high processing speeds. Besides, they usually ignore the green channel superiority of RAW images, and fail to achieve better reconstruction performance with good use of green channel information. In this work, we propose an efficient RAW Image Enhancement Network (ERIENet), which parallelly processes multi-scale information with efficient convolution modules, and takes advantage of rich information in green channels to guide the reconstruction of images. Firstly, we introduce an efficient multi-scale fully-parallel architecture with a novel channel-aware residual dense block to extract feature maps, which reduces computational costs and achieves real-time processing speed. Secondly, we introduce a green channel guidance branch to exploit the rich information within the green channels of the input RAW image. It increases the quality of reconstruction results with few parameters and computations. Experiments on commonly used low-light image enhancement datasets show that ERIENet outperforms state-of-the-art methods in enhancing low-light RAW images with higher effiency. It also achieves an optimal speed of over 146 frame-per-second (FPS) for 4K-resolution images on a single NVIDIA GeForce RTX 3090 with 24G memory.

</details>


### [34] [TBC: A Target-Background Contrast Metric for Low-Altitude Infrared and Visible Image Fusion](https://arxiv.org/abs/2512.15211)
*Yufeng Xie*

Main category: cs.CV

TL;DR: 该论文针对低空无人机红外与可见光图像融合任务，提出了一种新的无参考评价指标——目标-背景对比度（TBC），有效避免了传统评价中“噪声陷阱”问题，提高了指标与人眼感知的一致性。


<details>
  <summary>Details</summary>
Motivation: 针对复杂低照度环境下，传统无参考指标（如EN和AG）会将高频噪声误判为细节，导致对图像质量的错误评价，误导融合算法优化，亟需一种能准确反映目标可见性和忽略噪声的新指标。

Method: 提出了受韦伯定律（Weber's Law）启发的TBC指标，通过衡量显著目标与背景的相对对比度，重点评估目标区域显著性，同时对背景噪声进行惩罚。

Result: 在DroneVehicle数据集上的实验表明，TBC指标与人眼主观感知更加一致，并能有效作为低空场景下的图像融合质量评判标准。

Conclusion: TBC指标克服了传统方法在复杂、低照环境下的缺陷，为无人机红外-可见光图像融合任务提供了一种更可靠的评价方法，有助于指导下游的检测与跟踪等实际应用。

Abstract: Infrared and visible image fusion is a pivotal technology in low-altitude UAV reconnaissance missions, providing high-quality data support for downstream tasks such as target detection and tracking by integrating thermal saliency with background texture details.However, traditional no-reference metrics fail(Specifically,like Entropy (EN) and Average Gradient (AG)) in complex low-light environments. They often misinterpret high-frequency sensor noise as valid detail. This creates a "Noise Trap," paradoxically assigning higher scores to noisy images and misguiding fusion algorithms.To address this, we propose the Target-Background Contrast (TBC) metric. Inspired by Weber's Law, TBC focuses on the relative contrast of salient targets rather than global statistics. Unlike traditional metrics, TBC penalizes background noise and rewards target visibility. Experiments on the DroneVehicle dataset demonstrate that TBC aligns better with human perception and provides a reliable standard for low-altitude scenarios.

</details>


### [35] [From Camera to World: A Plug-and-Play Module for Human Mesh Transformation](https://arxiv.org/abs/2512.15212)
*Changhai Ma,Ziyu Wu,Yunkang Zhang,Qijun Ying,Boyan Liu,Xiaohui Cai*

Main category: cs.CV

TL;DR: 本文提出了Mesh-Plug模块，实现从真实环境图像中将人体三维网格精确从相机坐标系转换到世界坐标系，克服了以往由于缺乏相机旋转信息带来的精度瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设相机无旋转，在仅在相机坐标系下表现良好，但一旦变换到世界坐标系会产生较大误差，根本原因是缺乏准确的相机旋转参数。作者希望无需依赖环境线索，仅用人体信息实现准确转换。

Method: 提出Mesh-Plug模块：1）利用初始重建的人体网格，渲染RGB和深度图像，基于这些信息训练相机旋转预测模块，以人体姿态为中心估计相机俯仰角；2）用预测的相机参数和初始网格，设计网格调整模块，联合优化根关节朝向和肢体姿态，提升从相机系到世界系的转换精度。

Result: 在SPEC-SYN和SPEC-MTP两个人体三维重建基准数据集上，性能超过了当前主流方法，取得更高精度表现。

Conclusion: Mesh-Plug能够在不依赖环境信息的前提下，有效解决三维人体网格从相机系到世界系的转换难题，为野外场景下的人体三维重建应用提供了更好解决方案。

Abstract: Reconstructing accurate 3D human meshes in the world coordinate system from in-the-wild images remains challenging due to the lack of camera rotation information. While existing methods achieve promising results in the camera coordinate system by assuming zero camera rotation, this simplification leads to significant errors when transforming the reconstructed mesh to the world coordinate system. To address this challenge, we propose Mesh-Plug, a plug-and-play module that accurately transforms human meshes from camera coordinates to world coordinates. Our key innovation lies in a human-centered approach that leverages both RGB images and depth maps rendered from the initial mesh to estimate camera rotation parameters, eliminating the dependency on environmental cues. Specifically, we first train a camera rotation prediction module that focuses on the human body's spatial configuration to estimate camera pitch angle. Then, by integrating the predicted camera parameters with the initial mesh, we design a mesh adjustment module that simultaneously refines the root joint orientation and body pose. Extensive experiments demonstrate that our framework outperforms state-of-the-art methods on the benchmark datasets SPEC-SYN and SPEC-MTP.

</details>


### [36] [SLCFormer: Spectral-Local Context Transformer with Physics-Grounded Flare Synthesis for Nighttime Flare Removal](https://arxiv.org/abs/2512.15221)
*Xiyu Zhu,Wei Wang,Xin Yuan,Xiao Wang*

Main category: cs.CV

TL;DR: 本文提出了一种针对夜间镜头炫光去除的创新深度学习框架SLCFormer，显著提升了复杂炫光场景下的视觉质量和泛化表现。


<details>
  <summary>Details</summary>
Motivation: 现有夜间镜头炫光去除方法在应对不均匀、复杂光照条件下的炫光时效果不佳，严重限制了实际应用，因此需要一种更有效的方法来提升在多变场景中的表现。

Method: 提出了SLCFormer架构，包含频域特征建模模块（FFEM）和空间方向增强模块（DESM），分别用于捕捉炫光的全局特性和局部结构细节。此外，基于ZernikeVAE生成具有空间变化PSF的物理真实散射炫光数据，提升模型训练效果。

Result: 在Flare7K++数据集上，通过广泛实验，SLCFormer在各类定量指标和感知视觉质量上均优于现有方法，对实际复杂夜间炫光场景具有更强的泛化能力。

Conclusion: SLCFormer有效融合频域与空间域特征，通过物理驱动数据合成提升训练质量，为复杂场景下夜间镜头炫光去除提供了更优解决方案。

Abstract: Lens flare is a common nighttime artifact caused by strong light sources scattering within camera lenses, leading to hazy streaks, halos, and glare that degrade visual quality. However, existing methods usually fail to effectively address nonuniform scattered flares, which severely reduces their applicability to complex real-world scenarios with diverse lighting conditions. To address this issue, we propose SLCFormer, a novel spectral-local context transformer framework for effective nighttime lens flare removal. SLCFormer integrates two key modules: the Frequency Fourier and Excitation Module (FFEM), which captures efficient global contextual representations in the frequency domain to model flare characteristics, and the Directionally-Enhanced Spatial Module (DESM) for local structural enhancement and directional features in the spatial domain for precise flare removal. Furthermore, we introduce a ZernikeVAE-based scatter flare generation pipeline to synthesize physically realistic scatter flares with spatially varying PSFs, bridging optical physics and data-driven training. Extensive experiments on the Flare7K++ dataset demonstrate that our method achieves state-of-the-art performance, outperforming existing approaches in both quantitative metrics and perceptual visual quality, and generalizing robustly to real nighttime scenes with complex flare artifacts.

</details>


### [37] [Null-LoRA: Low-Rank Adaptation on Null Space](https://arxiv.org/abs/2512.15233)
*Yi Zhang,Yulei Kang,Haoxuan Chen,Jinxuan Li,ian-Fang Hu*

Main category: cs.CV

TL;DR: 本文提出了一种新的高效参数微调方法Null-LoRA，通过将低秩增量约束在预训练模型的零空间内，能够以更少参数实现更好的下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有主流微调方法如LoRA等虽然高效，但通常在整个参数空间内进行低秩调整，存在参数冗余的问题。观察到大型预训练模型存在结构化的零空间，作者希望利用这些特性设计更精简有效的微调方法。

Method: 提出Null-LoRA，通过锁定低秩矩阵的部分参数，并将所有的参数更新限制在零空间内，以减少冗余并提升增益效果，从而更高效地适应新任务。

Result: 在多个图文检索和视觉问答等任务实验证明，Null-LoRA用更少的参数超越了现有最优方法。

Conclusion: Null-LoRA能更好地利用参数，在提升微调效率与性能的同时大幅减少参数量，展示出优越的实际应用潜力。

Abstract: Parameter-efficient fine-tuning methods have gained considerable popularity for adapting large-scale models to downstream tasks, particularly LoRA and its variants. Existing methods perform low-rank adaptation over the full parameter space. However, fine-tuning within a subspace can achieve comparable effectiveness. Inspired by the observation that pre-trained models possess non-trivial null spaces, we propose Null-space based Low-Rank Adaptation (Null-LoRA). Null-LoRA effectively reduces redundancy and enhances effective rank by freezing portions of the low-rank matrices. To further improve parameter efficiency, Null-LoRA constrains the entire incremental update within the null space, maximizing the utilization of incremental updates to adapt to new task paradigms. Null-LoRA surpasses the state of the art with fewer parameters in extensive experiments across image-text retrieval and visual question answering tasks.

</details>


### [38] [Intersectional Fairness in Vision-Language Models for Medical Image Disease Classification](https://arxiv.org/abs/2512.15249)
*Yupeng Zhang,Adam G. Dunn,Usman Naseem,Jinman Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种新训练框架CMAC-MMD，旨在减少医学AI系统在边缘化患者群体中的诊断信心偏差，提高诊断的公平性与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有医学AI，尤其是多模态视觉语言模型，在诊断边缘群体时信心不足，导致诊断错误率高。此前的公平性策略往往牺牲整体性能，也无法消除信心分布的不均。急需既公平又高效的新方法。

Method: 提出Cross-Modal Alignment Consistency (CMAC-MMD)训练框架，实现不同群体间诊断信心标准化。特点是：不需要推理阶段输入患者敏感信息。实验使用HAM10000、BCN20000皮肤图像和Harvard-FairVLMed眼底图像，按年龄、性别、种族等交叉分组评估。

Result: 与传统方式相比，CMAC-MMD策略大幅减少了不同交叉群体间漏诊差距（TPR差值），如皮肤病数据集从0.50降至0.26，同时AUC提升至0.97。青光眼筛查同样减少群体间TPR差距，AUC小幅提升。

Conclusion: CMAC-MMD提供了一个可扩展、无需增加隐私风险的公平性诊断系统开发新框架，可适用于多类高风险临床场景，实现更精准和公平的AI诊断。

Abstract: Medical artificial intelligence (AI) systems, particularly multimodal vision-language models (VLM), often exhibit intersectional biases where models are systematically less confident in diagnosing marginalised patient subgroups. Such bias can lead to higher rates of inaccurate and missed diagnoses due to demographically skewed data and divergent distributions of diagnostic certainty. Current fairness interventions frequently fail to address these gaps or compromise overall diagnostic performance to achieve statistical parity among the subgroups. In this study, we developed Cross-Modal Alignment Consistency (CMAC-MMD), a training framework that standardises diagnostic certainty across intersectional patient subgroups. Unlike traditional debiasing methods, this approach equalises the model's decision confidence without requiring sensitive demographic data during clinical inference. We evaluated this approach using 10,015 skin lesion images (HAM10000) with external validation on 12,000 images (BCN20000), and 10,000 fundus images for glaucoma detection (Harvard-FairVLMed), stratifying performance by intersectional age, gender, and race attributes. In the dermatology cohort, the proposed method reduced the overall intersectional missed diagnosis gap (difference in True Positive Rate, $Δ$TPR) from 0.50 to 0.26 while improving the overall Area Under the Curve (AUC) from 0.94 to 0.97 compared to standard training. Similarly, for glaucoma screening, the method reduced $Δ$TPR from 0.41 to 0.31, achieving a better AUC of 0.72 (vs. 0.71 baseline). This establishes a scalable framework for developing high-stakes clinical decision support systems that are both accurate and can perform equitably across diverse patient subgroups, ensuring reliable performance without increasing privacy risks.

</details>


### [39] [Assessing the Visual Enumeration Abilities of Specialized Counting Architectures and Vision-Language Models](https://arxiv.org/abs/2512.15254)
*Kuinan Hou,Jing Mi,Marco Zorzi,Lamberto Ballan,Alberto Testolin*

Main category: cs.CV

TL;DR: 本文比较了视觉语言模型（VLMs）与专门计数模型在多个数据集上的计数性能，发现VLMs在部分场景下已能达到或超越传统方法，但在复杂场景下仍不够可靠。


<details>
  <summary>Details</summary>
Motivation: 传统视觉计数方法依赖于特定领域的架构与数据集，缺乏灵活性。近期多模态视觉-语言模型发展迅速，有潜力实现通用且开放类集的目标计数。本文动机在于探索和评估这些新兴VLM能否替代传统方法，并找出它们在现实计数任务中的限制。

Method: 系统性地比较了当前主流的专用视觉计数架构与各种VLMs，在两个常用计数数据集和一个新设计的、对视觉属性有细致控制的基准测试上测试性能。此外，研究了VLM通过生成中间表征（如物体位置和文字标签）增强计数能力的方法。

Result: 多数VLM能较好地估算视觉场景中物体数量，并在某些情况下已经达到或超过特定视觉计数架构的水平。当引导VLM输出物体中间表征时，计数准确率明显提升。

Conclusion: 当前VLM在标准和一定复杂度场景下具备较好计数能力，但在复杂现实视觉场景中的可靠性仍不足，未来仍需进一步研究改进其实际部署能力。

Abstract: Counting the number of items in a visual scene remains a fundamental yet challenging task in computer vision. Traditional approaches to solving this problem rely on domain-specific counting architectures, which are trained using datasets annotated with a predefined set of object categories. However, recent progress in creating large-scale multimodal vision-language models (VLMs) suggests that these domain-general architectures may offer a flexible alternative for open-set object counting. In this study, we therefore systematically compare the performance of state-of-the-art specialized counting architectures against VLMs on two popular counting datasets, as well as on a novel benchmark specifically created to have a finer-grained control over the visual properties of test images. Our findings show that most VLMs can approximately enumerate the number of items in a visual scene, matching or even surpassing the performance of specialized computer vision architectures. Notably, enumeration accuracy significantly improves when VLMs are prompted to generate intermediate representations (i.e., locations and verbal labels) of each object to be counted. Nevertheless, none of the models can reliably count the number of objects in complex visual scenes, showing that further research is still needed to create AI systems that can reliably deploy counting procedures in realistic environments.

</details>


### [40] [MMMamba: A Versatile Cross-Modal In Context Fusion Framework for Pan-Sharpening and Zero-Shot Image Enhancement](https://arxiv.org/abs/2512.15261)
*Yingying Wang,Xuanhua He,Chen Wu,Jialing Huang,Suiyun Zhang,Rui Liu,Xinghao Ding,Haoxuan Che*

Main category: cs.CV

TL;DR: 本文提出了一种名为MMMamba的高效跨模态融合框架，能有效融合高分辨率全色图像与低分辨率多光谱图像，实现高质量的高分辨率多光谱影像重建，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的CNN方法在融合全色与多光谱图像时，受限于通道拼接和固定卷积算子，难以适应多样的空间与光谱变化；基于交叉注意力的方法虽然可实现全局信息交互，但计算效率低且可能削弱细粒度对应关系。本工作旨在设计一种既高效又能充分利用跨模态信息的新方法。

Method: 本文提出MMMamba框架，基于Mamba架构，具备线性计算复杂度和强跨模态信息交互能力。引入了多模态交错扫描机制（MI scanning），促进全色与多光谱信息的深度混合与高效交换。该方法还可零样本迁移至图像超分辨任务。

Result: 在多个任务和数据集基准上，MMMamba方法优于现有最新（SOTA）方法，显示出更好的融合效果和性能指标。

Conclusion: MMMamba框架为全色-多光谱图像融合任务提供了高效、强大且具有良好推广性的解决方案，在遥感图像重建、融合和超分等领域具有广泛应用潜力。

Abstract: Pan-sharpening aims to generate high-resolution multispectral (HRMS) images by integrating a high-resolution panchromatic (PAN) image with its corresponding low-resolution multispectral (MS) image. To achieve effective fusion, it is crucial to fully exploit the complementary information between the two modalities. Traditional CNN-based methods typically rely on channel-wise concatenation with fixed convolutional operators, which limits their adaptability to diverse spatial and spectral variations. While cross-attention mechanisms enable global interactions, they are computationally inefficient and may dilute fine-grained correspondences, making it difficult to capture complex semantic relationships. Recent advances in the Multimodal Diffusion Transformer (MMDiT) architecture have demonstrated impressive success in image generation and editing tasks. Unlike cross-attention, MMDiT employs in-context conditioning to facilitate more direct and efficient cross-modal information exchange. In this paper, we propose MMMamba, a cross-modal in-context fusion framework for pan-sharpening, with the flexibility to support image super-resolution in a zero-shot manner. Built upon the Mamba architecture, our design ensures linear computational complexity while maintaining strong cross-modal interaction capacity. Furthermore, we introduce a novel multimodal interleaved (MI) scanning mechanism that facilitates effective information exchange between the PAN and MS modalities. Extensive experiments demonstrate the superior performance of our method compared to existing state-of-the-art (SOTA) techniques across multiple tasks and benchmarks.

</details>


### [41] [SynthSeg-Agents: Multi-Agent Synthetic Data Generation for Zero-Shot Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2512.15310)
*Wangyu Wu,Zhenhong Chen,Xiaowei Huang,Fei Ma,Jimin Xiao*

Main category: cs.CV

TL;DR: 本论文提出了一种全新的思路，实现无需真实图像也能进行像素级语义分割，构建了由大语言模型驱动的多代理框架SynthSeg Agents，完全依赖合成数据进行训练，并在公开数据集上取得了有竞争力的表现。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督语义分割方法虽减少了对像素级标注的需求，但依赖真实图像，真实数据获取依然昂贵且受限。为进一步降低数据成本，作者尝试脱离对真实图像的依赖，实现高质量分割。

Method: 提出了SynthSeg Agents多代理框架。包括（1）自我优化的Prompt生成代理，通过循环迭代、记忆和多样性筛选基于CLIP策略生成高质量提示词；（2）图像生成代理，利用视觉语言模型生成图像；（3）用CLIP筛选高质量合成图像，再用ViT分类器对数据集进行高精度语义重标注。该流程完全不使用真实图像。

Result: 在PASCAL VOC 2012和COCO 2014数据集上，用全部合成的训练数据就获得了与现有基于真实图像方法具有竞争力的表现。

Conclusion: SynthSeg Agents证实了LLM驱动的多代理系统能有效生成高质量合成数据，摆脱对真实图片监督依赖，为语义分割提供了高效可扩展的新方向。

Abstract: Weakly Supervised Semantic Segmentation (WSSS) with image level labels aims to produce pixel level predictions without requiring dense annotations. While recent approaches have leveraged generative models to augment existing data, they remain dependent on real world training samples. In this paper, we introduce a novel direction, Zero Shot Weakly Supervised Semantic Segmentation (ZSWSSS), and propose SynthSeg Agents, a multi agent framework driven by Large Language Models (LLMs) to generate synthetic training data entirely without real images. SynthSeg Agents comprises two key modules, a Self Refine Prompt Agent and an Image Generation Agent. The Self Refine Prompt Agent autonomously crafts diverse and semantically rich image prompts via iterative refinement, memory mechanisms, and prompt space exploration, guided by CLIP based similarity and nearest neighbor diversity filtering. These prompts are then passed to the Image Generation Agent, which leverages Vision Language Models (VLMs) to synthesize candidate images. A frozen CLIP scoring model is employed to select high quality samples, and a ViT based classifier is further trained to relabel the entire synthetic dataset with improved semantic precision. Our framework produces high quality training data without any real image supervision. Experiments on PASCAL VOC 2012 and COCO 2014 show that SynthSeg Agents achieves competitive performance without using real training images. This highlights the potential of LLM driven agents in enabling cost efficient and scalable semantic segmentation.

</details>


### [42] [KD360-VoxelBEV: LiDAR and 360-degree Camera Cross Modality Knowledge Distillation for Bird's-Eye-View Segmentation](https://arxiv.org/abs/2512.15311)
*Wenke E,Yixin Sun,Jiaxu Liu,Hubert P. H. Shum,Amir Atapour-Abarghouei,Toby P. Breckon*

Main category: cs.CV

TL;DR: 本文提出了一种专为单全景摄像头鸟瞰视角（BEV）分割设计的跨模态知识蒸馏框架，实现了成本低、效率高的BEV语义分割。通过利用融合的激光雷达图像表征作为教师网络，引导仅用全景相机的学生网络学习优秀特征，在实验中展现出显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前BEV语义分割普遍依赖多传感器（如LiDAR与摄像头），导致成本和复杂性高，而仅基于单摄像头的方案又面临精度低的问题。因此，作者试图借助强大传感器网络的知识优势，提升单全景摄像头系统的BEV分割能力，降低自动驾驶解决方案部署成本。

Method: 作者构建了一个以高容量的LiDAR和摄像头融合网络为教师端，使用融合的激光雷达多通道表征和体素对齐转换模块，提取丰富的空间与语义特征。借助跨模态知识蒸馏，将这些特征迁移给只用单360°全景相机的学生网络，在训练阶段完成特征指导，推理阶段学生网络仅需图像即可完成高效BEV分割。

Result: 在Dur360BEV数据集上，教师网络在IoU上获得25.6%的提升，学生网络也获得8.5%的IoU增益，并达到了31.2 FPS的推理速度。在KITTI-360（双鱼眼相机）上测试，展示了方法对不同摄像头设置的泛化性。

Conclusion: 本文证明了通过跨模态知识蒸馏，可以有效提升单全景摄像头BEV分割的性能，兼具高效性与低部署成本，为实际自动驾驶应用提供了具备实际价值的解决方案。

Abstract: We present the first cross-modality distillation framework specifically tailored for single-panoramic-camera Bird's-Eye-View (BEV) segmentation. Our approach leverages a novel LiDAR image representation fused from range, intensity and ambient channels, together with a voxel-aligned view transformer that preserves spatial fidelity while enabling efficient BEV processing. During training, a high-capacity LiDAR and camera fusion Teacher network extracts both rich spatial and semantic features for cross-modality knowledge distillation into a lightweight Student network that relies solely on a single 360-degree panoramic camera image. Extensive experiments on the Dur360BEV dataset demonstrate that our teacher model significantly outperforms existing camera-based BEV segmentation methods, achieving a 25.6\% IoU improvement. Meanwhile, the distilled Student network attains competitive performance with an 8.5\% IoU gain and state-of-the-art inference speed of 31.2 FPS. Moreover, evaluations on KITTI-360 (two fisheye cameras) confirm that our distillation framework generalises to diverse camera setups, underscoring its feasibility and robustness. This approach reduces sensor complexity and deployment costs while providing a practical solution for efficient, low-cost BEV segmentation in real-world autonomous driving.

</details>


### [43] [Emotion Recognition in Signers](https://arxiv.org/abs/2512.15376)
*Kotaro Funakoshi,Yaoxiong Zhu*

Main category: cs.CV

TL;DR: 本文提出并分析了情感识别在手语识别中的理论与实践难题，构建了日语手语情感识别新数据集（eJSL），并通过跨语言方法提升了识别效果。


<details>
  <summary>Details</summary>
Motivation: 目前手语者情感识别面临两个主要问题：一是语法与情感面部表情的重叠导致识别困难，二是用于训练模型的数据稀缺。作者希望通过新数据集和跨语言方法缓解这些问题。

Method: 作者构建了新的日语手语情感识别数据集eJSL（包含两名手语者，七种情感，共1092个视频），并结合英国手语数据集（BOBSL），通过文本情感识别、时间片段选择和手部动作特征等多种处理方法，提升了模型识别能力。

Result: 1）借助口语文本情感识别模型可有效缓解手语数据稀缺问题；2）情感识别的关键在于合理选择时间段；3）引入手部动作特征有助于提升手语者的情感识别；最终建立了比通用大语言模型更强的基线。

Conclusion: 跨语言、跨模态方法以及针对性特征工程可以显著提升手语情感识别效果，为今后该领域研究打下基础。

Abstract: Recognition of signers' emotions suffers from one theoretical challenge and one practical challenge, namely, the overlap between grammatical and affective facial expressions and the scarcity of data for model training. This paper addresses these two challenges in a cross-lingual setting using our eJSL dataset, a new benchmark dataset for emotion recognition in Japanese Sign Language signers, and BOBSL, a large British Sign Language dataset with subtitles. In eJSL, two signers expressed 78 distinct utterances with each of seven different emotional states, resulting in 1,092 video clips. We empirically demonstrate that 1) textual emotion recognition in spoken language mitigates data scarcity in sign language, 2) temporal segment selection has a significant impact, and 3) incorporating hand motion enhances emotion recognition in signers. Finally we establish a stronger baseline than spoken language LLMs.

</details>


### [44] [Automated Motion Artifact Check for MRI (AutoMAC-MRI): An Interpretable Framework for Motion Artifact Detection and Severity Assessment](https://arxiv.org/abs/2512.15315)
*Antony Jerald,Dattesh Shanbhag,Sudhanya Chatterjee*

Main category: cs.CV

TL;DR: 本文提出了AutoMAC-MRI框架，可对MRI运动伪影进行分级评估，并提高解释性，为提升影像质量控制与减少不必要的重复扫描提供了新方法。


<details>
  <summary>Details</summary>
Motivation: MRI成像中运动伪影严重影响图像质量，导致患者需多次扫描。现有自动化质量评估方法多为二分类决策，缺乏细致等级划分与解释性。由此亟需一种能细致量化和解释运动伪影的方法。

Method: 提出了AutoMAC-MRI框架，利用有监督对比学习训练特征空间，并设计了基于该空间的分级亲和评分系统。评分系统评估图像与不同运动分级的距离，使每个分级的判断更加透明、解释性更强。方法在覆盖多种脑MRI对比和视角的5000余张专家标注样本中进行了评估。

Result: 实验显示，AutoMAC-MRI的亲和评分与专家标签高度一致，并能准确检测伪影分级。该方法在多种MRI对比和取向下均表现优异。

Conclusion: AutoMAC-MRI不仅提高了MRI运动伪影检测及分级的准确性，还通过分级亲和评分增强了解释性，有潜力用于实时质量控制、减少重复扫描、提升工作效率。

Abstract: Motion artifacts degrade MRI image quality and increase patient recalls. Existing automated quality assessment methods are largely limited to binary decisions and provide little interpretability. We introduce AutoMAC-MRI, an explainable framework for grading motion artifacts across heterogeneous MR contrasts and orientations. The approach uses supervised contrastive learning to learn a discriminative representation of motion severity. Within this feature space, we compute grade-specific affinity scores that quantify an image's proximity to each motion grade, thereby making grade assignments transparent and interpretable. We evaluate AutoMAC-MRI on more than 5000 expert-annotated brain MRI slices spanning multiple contrasts and views. Experiments assessing affinity scores against expert labels show that the scores align well with expert judgment, supporting their use as an interpretable measure of motion severity. By coupling accurate grade detection with per-grade affinity scoring, AutoMAC-MRI enables inline MRI quality control, with the potential to reduce unnecessary rescans and improve workflow efficiency.

</details>


### [45] [VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?](https://arxiv.org/abs/2512.15649)
*Hongbo Zhao,Meng Wang,Fei Zhu,Wenzhuo Liu,Bolin Ni,Fanhu Zeng,Gaofeng Meng,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: 本论文提出了首个视觉-文本压缩（VTC）基准，系统性评测了视觉语言模型（VLMs）在长上下文理解任务中的能力。结果显示多数主流VLM在压缩后信息密度极高的场景下，尽管能准确识别文本，却在长距离关联与依赖的理解任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 目前扩大大语言模型（LLMs）上下文窗口时的算力与内存消耗急剧增加，限制其可扩展性。视觉-文本压缩（VTC）方法虽能实现高倍率压缩，提升效率，但压缩后VLM核心长上下文能力影响尚未被充分研究。

Method: 作者提出了第一个用于VTC的评测基准，包括三种主要任务：信息检索（VTC-Retrieval）、推理（VTC-Reasoning）及长时记忆问答（VTC-Memory），并构建VTCBench-Wild以模拟多样场景，对多种主流开源与闭源模型进行了系统测试。

Result: 测试结果发现，多数VLM即使能良好解码压缩文本内容，但在需要长距离推理与依赖理解时表现非常差，难以充分利用高信息密度带来的扩展能力。

Conclusion: 论文揭示了VTC面临的核心瓶颈，为未来设计高效、可扩展的新一代VLM提供了重要参考和理论基础。

Abstract: The computational and memory overheads associated with expanding the context window of LLMs severely limit their scalability. A noteworthy solution is vision-text compression (VTC), exemplified by frameworks like DeepSeek-OCR and Glyph, which convert long texts into dense 2D visual representations, thereby achieving token compression ratios of 3x-20x. However, the impact of this high information density on the core long-context capabilities of vision-language models (VLMs) remains under-investigated. To address this gap, we introduce the first benchmark for VTC and systematically assess the performance of VLMs across three long-context understanding settings: VTC-Retrieval, which evaluates the model's ability to retrieve and aggregate information; VTC-Reasoning, which requires models to infer latent associations to locate facts with minimal lexical overlap; and VTC-Memory, which measures comprehensive question answering within long-term dialogue memory. Furthermore, we establish the VTCBench-Wild to simulate diverse input scenarios.We comprehensively evaluate leading open-source and proprietary models on our benchmarks. The results indicate that, despite being able to decode textual information (e.g., OCR) well, most VLMs exhibit a surprisingly poor long-context understanding ability with VTC-compressed information, failing to capture long associations or dependencies in the context.This study provides a deep understanding of VTC and serves as a foundation for designing more efficient and scalable VLMs.

</details>


### [46] [Prototypical Learning Guided Context-Aware Segmentation Network for Few-Shot Anomaly Detection](https://arxiv.org/abs/2512.15319)
*Yuxin Jiang,Yunkang Cao,Weiming Shen*

Main category: cs.CV

TL;DR: 提出了一种新的少样本异常检测网络PCSNet，能够在只用极少的正常样本下实现高效的异常检测。


<details>
  <summary>Details</summary>
Motivation: 现有少样本异常检测方法依赖预训练特征，但忽视了与实际应用场景的领域差异，影响检测准确性。

Method: 设计了包含原型特征自适应（PFA）和上下文感知分割（CAS）两个子网络的PCSNet。其中PFA通过原型特征引导，提升正常样本特征的紧致性并使异常具有更强区分性，引入像素级判别损失增强对细微异常的分辨能力；CAS网络用于像素级定位异常，并通过伪异常样本辅助训练。

Result: 在MVTec和MPDD数据集上，PCSNet在8-shot场景下分别取得了94.9%和80.2%的图像级AUROC，优于现有方法。实际在汽车塑料零件检测任务中也验证了高效性。

Conclusion: PCSNet能够有效缩小领域差距并提升少样本异常检测能力，适用于实际工业检测任务，在数据有限时表现尤为突出。

Abstract: Few-shot anomaly detection (FSAD) denotes the identification of anomalies within a target category with a limited number of normal samples. Existing FSAD methods largely rely on pre-trained feature representations to detect anomalies, but the inherent domain gap between pre-trained representations and target FSAD scenarios is often overlooked. This study proposes a Prototypical Learning Guided Context-Aware Segmentation Network (PCSNet) to address the domain gap, thereby improving feature descriptiveness in target scenarios and enhancing FSAD performance. In particular, PCSNet comprises a prototypical feature adaption (PFA) sub-network and a context-aware segmentation (CAS) sub-network. PFA extracts prototypical features as guidance to ensure better feature compactness for normal data while distinct separation from anomalies. A pixel-level disparity classification loss is also designed to make subtle anomalies more distinguishable. Then a CAS sub-network is introduced for pixel-level anomaly localization, where pseudo anomalies are exploited to facilitate the training process. Experimental results on MVTec and MPDD demonstrate the superior FSAD performance of PCSNet, with 94.9% and 80.2% image-level AUROC in an 8-shot scenario, respectively. Real-world applications on automotive plastic part inspection further demonstrate that PCSNet can achieve promising results with limited training samples. Code is available at https://github.com/yuxin-jiang/PCSNet.

</details>


### [47] [MECAD: A multi-expert architecture for continual anomaly detection](https://arxiv.org/abs/2512.15323)
*Malihe Dahmardeh,Francesco Setti*

Main category: cs.CV

TL;DR: 提出了一种多专家架构（MECAD）用于持续异常检测，在MVTec AD数据集上表现优异，同时降低知识遗忘。


<details>
  <summary>Details</summary>
Motivation: 工业环境中产品类型不断变化，持续异常检测需兼顾新类别适应能力及原有知识保留，单一模型面临灾难性遗忘。

Method: 采用多专家系统，根据特征相似性动态分配专家类，结合高效内存管理、优化的核心集选择与专用回放缓存，支持增量学习和知识保留，无需整体模型重训。

Result: 在MVTec AD数据集上，五专家最优组合在15类对象上平均AUROC达到0.8259，相比单专家模型大幅降低知识遗忘。

Conclusion: 该框架兼顾计算效率、知识留存与适应性，适用于产品类型不断更新的工业场景。

Abstract: In this paper we propose MECAD, a novel approach for continual anomaly detection using a multi-expert architecture. Our system dynamically assigns experts to object classes based on feature similarity and employs efficient memory management to preserve the knowledge of previously seen classes. By leveraging an optimized coreset selection and a specialized replay buffer mechanism, we enable incremental learning without requiring full model retraining. Our experimental evaluation on the MVTec AD dataset demonstrates that the optimal 5-expert configuration achieves an average AUROC of 0.8259 across 15 diverse object categories while significantly reducing knowledge degradation compared to single-expert approaches. This framework balances computational efficiency, specialized knowledge retention, and adaptability, making it well-suited for industrial environments with evolving product types.

</details>


### [48] [A Masked Reverse Knowledge Distillation Method Incorporating Global and Local Information for Image Anomaly Detection](https://arxiv.org/abs/2512.15326)
*Yuxin Jiang,Yunkang Can,Weiming Shen*

Main category: cs.CV

TL;DR: 本文提出了一种改进的知识蒸馏方法——掩码反向知识蒸馏（MRKD），通过引入图像级和特征级掩码机制，有效缓解异常检测中过度泛化问题，并在MVTec数据集上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏在图像异常检测与定位中效果显著，但容易因为输入与监督信号过于相似而导致模型过度泛化，降低检测精度。因此需要新的机制抑制过度泛化，提升模型性能。

Method: 提出掩码反向知识蒸馏（MRKD），包含图像级掩码（ILM）与特征级掩码（FLM）。ILM通过区分输入与监督信号捕捉全局信息，FLM在特征层面引入合成异常以强化局部表征，使学习到的特征兼顾全局和局部信息。

Result: 在MVTec数据集上，MRKD达到了图像级98.9%、像素级98.4%的AU-ROC，以及95.3%的AU-PRO。同时通过消融实验验证了MRKD在缓解过度泛化方面的优越性。

Conclusion: MRKD方法能有效提升异常检测、定位能力，显著降低过度泛化，具有良好的实用价值。

Abstract: Knowledge distillation is an effective image anomaly detection and localization scheme. However, a major drawback of this scheme is its tendency to overly generalize, primarily due to the similarities between input and supervisory signals. In order to address this issue, this paper introduces a novel technique called masked reverse knowledge distillation (MRKD). By employing image-level masking (ILM) and feature-level masking (FLM), MRKD transforms the task of image reconstruction into image restoration. Specifically, ILM helps to capture global information by differentiating input signals from supervisory signals. On the other hand, FLM incorporates synthetic feature-level anomalies to ensure that the learned representations contain sufficient local information. With these two strategies, MRKD is endowed with stronger image context capture capacity and is less likely to be overgeneralized. Experiments on the widely-used MVTec anomaly detection dataset demonstrate that MRKD achieves impressive performance: image-level 98.9% AU-ROC, pixel-level 98.4% AU-ROC, and 95.3% AU-PRO. In addition, extensive ablation experiments have validated the superiority of MRKD in mitigating the overgeneralization problem.

</details>


### [49] [Vision-based module for accurately reading linear scales in a laboratory](https://arxiv.org/abs/2512.15327)
*Parvesh Saini,Soumyadipta Maiti,Beena Rai*

Main category: cs.CV

TL;DR: 该论文提出了一种模仿人类视觉的方法，实现自动、精准地从图像中读取线性刻度仪器（如注射器、量筒）上的数值测量，结果与人工读取高度吻合。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉模型在物体检测、图像分类等领域表现出色，但可以像人一样从图像中精确读数的模型依然稀缺。实验室自动化等场景中，机器人需要具备自主读取仪器测量值的能力，以实现更高水平的自动化和仿真人类操作。

Method: 作者以注射器和量筒为例，模仿人类从线性刻度读取数值的过程。该系统首先对任意朝向的注射器图像进行校正，然后聚焦于包含线性刻度的图像区域，提取主要刻度、对应数字以及液位指示位置等特征，最终得出数值读数。

Result: 系统自动读取的数据与人类读数进行了比对，结果显示自动系统的读数与人工高度一致，具有较高准确性。

Conclusion: 本文方法在无需人工辅助的情况下，能够高效、准确地自动读取实验室仪器刻度，为智能实验室或机器人自主操作提供关键技术支持。

Abstract: Capabilities and the number of vision-based models are increasing rapidly. And these vision models are now able to do more tasks like object detection, image classification, instance segmentation etc. with great accuracy. But models which can take accurate quantitative measurements form an image, as a human can do by just looking at it, are rare. For a robot to work with complete autonomy in a Laboratory environment, it needs to have some basic skills like navigation, handling objects, preparing samples etc. to match human-like capabilities in an unstructured environment. Another important capability is to read measurements from instruments and apparatus. Here, we tried to mimic a human inspired approach to read measurements from a linear scale. As a test case we have picked reading level from a syringe and a measuring cylinder. For a randomly oriented syringe we carry out transformations to correct the orientation. To make the system efficient and robust, the area of interest is reduced to just the linear scale containing part of the image. After that, a series of features were extracted like the major makers, the corresponding digits, and the level indicator location, from which the final reading was calculated. Readings obtained using this system were also compared against human read values of the same instances and an accurate correspondence was observed.

</details>


### [50] [Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics](https://arxiv.org/abs/2512.15340)
*Junjie Chen,Fei Wang,Zhihao Huang,Qing Zhou,Kun Li,Dan Guo,Linfeng Zhang,Xun Yang*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D对话头部生成框架TIMAR，有效提升了交互机器人的自然表达能力，实验结果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 人类对话不仅涉及语言，还包含点头、视线、表情等非言语交流。现有方法往往将说话和聆听当作互不相关的独立过程，无法保持对话的时序连贯性，因此需要一种能捕捉完整对话动态的新方法。

Method: 提出TIMAR框架，将对话建模为交错的音视频上下文，通过turn-level因果注意力机制整合多模态信息，并采用轻量级Diffusion机制预测连续3D头部动态。

Result: 在DualTalk基准上，TIMAR在Fréchet Distance和MSE指标上提升15-30%，在分布外数据上同样有可观提升。

Conclusion: TIMAR可有效捕捉对话中的协调与表达变化，为虚拟头像和交互机器人提供了更自然的表达方式。论文源码即将开源。

Abstract: Human conversation involves continuous exchanges of speech and nonverbal cues such as head nods, gaze shifts, and facial expressions that convey attention and emotion. Modeling these bidirectional dynamics in 3D is essential for building expressive avatars and interactive robots. However, existing frameworks often treat talking and listening as independent processes or rely on non-causal full-sequence modeling, hindering temporal coherence across turns. We present TIMAR (Turn-level Interleaved Masked AutoRegression), a causal framework for 3D conversational head generation that models dialogue as interleaved audio-visual contexts. It fuses multimodal information within each turn and applies turn-level causal attention to accumulate conversational history, while a lightweight diffusion head predicts continuous 3D head dynamics that captures both coordination and expressive variability. Experiments on the DualTalk benchmark show that TIMAR reduces Fréchet Distance and MSE by 15-30% on the test set, and achieves similar gains on out-of-distribution data. The source code will be released in the GitHub repository https://github.com/CoderChen01/towards-seamleass-interaction.

</details>


### [51] [Expand and Prune: Maximizing Trajectory Diversity for Effective GRPO in Generative Models](https://arxiv.org/abs/2512.15347)
*Shiran Ge,Chenyi Huang,Yuang Ai,Qihang Fan,Huaibo Huang,Ran He*

Main category: cs.CV

TL;DR: 提出了Pro-GRPO方法，在生成模型的训练中，通过主动筛选高优化价值的轨迹，显著提高效率并提升最终模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法在选择大样本组时面临计算量过大和优化效果有限的矛盾，急需提升效率和优化价值。

Method: 作者首先观察到奖励聚类现象，即许多样本获得相似奖励，实际优化贡献有限。为此提出OVF（Optimal Variance Filtering）筛选高方差、高价值的子集。随后进一步提出Pro-GRPO框架，将基于潜变量的轨迹剪枝提前至采样过程，实现动态、主动的高效筛选，采用“扩展-剪枝”策略，先扩展采样多样性，再多步筛除低价值轨迹。

Result: 实验证明在不同类型的生成模型（扩散模型、流模型）上，Pro-GRPO大幅降低计算量，并在保持甚至提升优化结果的同时，提升训练效率和策略表现。

Conclusion: Pro-GRPO通过主动轨迹筛选方法，有效解决了GRPO的计算瓶颈和奖励聚类问题，为高效生成模型对齐提供了新范式，在各类任务上展现出优越的通用性和效果。

Abstract: Group Relative Policy Optimization (GRPO) is a powerful technique for aligning generative models, but its effectiveness is bottlenecked by the conflict between large group sizes and prohibitive computational costs. In this work, we investigate the trade-off through empirical studies, yielding two key observations. First, we discover the reward clustering phenomenon in which many trajectories collapse toward the group-mean reward, offering limited optimization value. Second, we design a heuristic strategy named Optimal Variance Filtering (OVF), and verify that a high-variance subset of trajectories, selected by OVF can outperform the larger, unfiltered group. However, this static, post-sampling OVF approach still necessitates critical computational overhead, as it performs unnecessary sampling for trajectories that are ultimately discarded. To resolve this, we propose Pro-GRPO (Proactive GRPO), a novel dynamic framework that integrates latent feature-based trajectory pruning into the sampling process. Through the early termination of reward-clustered trajectories, Pro-GRPO reduces computational overhead. Leveraging its efficiency, Pro-GRPO employs an "Expand-and-Prune" strategy. This strategy first expands the size of initial sampling group to maximize trajectory diversity, then it applies multi-step OVF to the latents, avoiding prohibitive computational costs. Extensive experiments on both diffusion-based and flow-based models demonstrate the generality and effectiveness of our Pro-GRPO framework.

</details>


### [52] [SemanticBridge -- A Dataset for 3D Semantic Segmentation of Bridges and Domain Gap Analysis](https://arxiv.org/abs/2512.15369)
*Maximilian Kellner,Mariana Ferrandon Cervantes,Yuandong Pan,Ruodan Lu,Ioannis Brilakis,Alexander Reiterer*

Main category: cs.CV

TL;DR: 论文提出并发布了一个专为桥梁三维语义分割及传感器域差分析设计的新数据集，并分析了现有三维深度学习模型在该数据集上的表现及传感器差异对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 随着社会基础设施老化，自动化桥梁检测与维护日益重要。然而，缺乏专门针对桥梁三维语义分割的数据集，且实际应用中多样的传感器导致域间差异影响模型泛化能力。

Method: 作者采集和标注了来自不同国家、结构多样的桥梁高分辨率三维扫描数据，给出详细的语义标签，并通过多种传感器记录同一场景，构建数据集。然后选取了三种主流三维深度学习模型，在此数据集上进行了系统实验和域差分析。

Result: 三种模型在数据集上均表现出了较强鲁棒性。然而，由传感器差异引起的域间差距会导致模型性能（mIoU）下降，最大降幅可达11.4%。

Conclusion: 该工作填补了桥梁三维分割数据集的空白，对结构健康监测有促进作用。传感器差异带来的域间扰动不容忽视，需进一步研究提升模型应对能力。

Abstract: We propose a novel dataset that has been specifically designed for 3D semantic segmentation of bridges and the domain gap analysis caused by varying sensors. This addresses a critical need in the field of infrastructure inspection and maintenance, which is essential for modern society. The dataset comprises high-resolution 3D scans of a diverse range of bridge structures from various countries, with detailed semantic labels provided for each. Our initial objective is to facilitate accurate and automated segmentation of bridge components, thereby advancing the structural health monitoring practice. To evaluate the effectiveness of existing 3D deep learning models on this novel dataset, we conduct a comprehensive analysis of three distinct state-of-the-art architectures. Furthermore, we present data acquired through diverse sensors to quantify the domain gap resulting from sensor variations. Our findings indicate that all architectures demonstrate robust performance on the specified task. However, the domain gap can potentially lead to a decline in the performance of up to 11.4% mIoU.

</details>


### [53] [See It Before You Grab It: Deep Learning-based Action Anticipation in Basketball](https://arxiv.org/abs/2512.15386)
*Arnau Barrera Roy,Albert Clapés Sintes*

Main category: cs.CV

TL;DR: 本文提出了篮球视频中的动作预测新任务，专注于预测投篮后哪支球队将获得球权，并发布了一个包含10万段比赛片段的新数据集，首次系统性研究了篮板球预测问题。


<details>
  <summary>Details</summary>
Motivation: 尽管计算机视觉在运动分析中已取得诸多突破，但针对投篮后球队球权预判（如篮板球归属）的研究相对较少。研究者希望通过更早动作预测，为实时转播和赛后分析带来智能化改进。

Method: 作者自建了包含30万小时NBA比赛和2000个手工标注篮板球事件的新数据集，采用多种现有深度学习动作预测方法作基线，在预测投篮后球权等任务上进行评测，同时探索了篮板归属分类和检测等附加任务。

Result: 实验结果显示，该数据集能支持多种篮球视频理解应用，深度学习方法在篮板球预测任务上取得了可行但具有挑战的效果，也揭示了多智能体体育行为建模的困难与前景。

Conclusion: 首次系统提出和验证了篮球篮板球预测任务及数据集，实现了比赛片段中球权归属的提前预测，为实时智能转播与分析决策工具提供了基础数据和技术支持。

Abstract: Computer vision and video understanding have transformed sports analytics by enabling large-scale, automated analysis of game dynamics from broadcast footage. Despite significant advances in player and ball tracking, pose estimation, action localization, and automatic foul recognition, anticipating actions before they occur in sports videos has received comparatively little attention. This work introduces the task of action anticipation in basketball broadcast videos, focusing on predicting which team will gain possession of the ball following a shot attempt. To benchmark this task, a new self-curated dataset comprising 100,000 basketball video clips, over 300 hours of footage, and more than 2,000 manually annotated rebound events is presented. Comprehensive baseline results are reported using state-of-the-art action anticipation methods, representing the first application of deep learning techniques to basketball rebound prediction. Additionally, two complementary tasks, rebound classification and rebound spotting, are explored, demonstrating that this dataset supports a wide range of video understanding applications in basketball, for which no comparable datasets currently exist. Experimental results highlight both the feasibility and inherent challenges of anticipating rebounds, providing valuable insights into predictive modeling for dynamic multi-agent sports scenarios. By forecasting team possession before rebounds occur, this work enables applications in real-time automated broadcasting and post-game analysis tools to support decision-making.

</details>


### [54] [SMART: Semantic Matching Contrastive Learning for Partially View-Aligned Clustering](https://arxiv.org/abs/2512.15396)
*Liang Peng,Yixuan Ye,Cheng Liu,Hangjun Che,Fei Wang,Zhiwen Yu,Si Wu,Hau-San Wong*

Main category: cs.CV

TL;DR: 本文提出了一种新的部分视图对齐聚类（PVC）方法SMART，更好地利用了对齐与未对齐多视图数据间的语义信息，显著提升了聚类性能。


<details>
  <summary>Details</summary>
Motivation: 现实中的多视图数据很难完全对齐，如何有效利用包含未对齐样本在内的多视图数据，是提升聚类质量的关键。而现有方法普遍未能充分挖掘未对齐数据的共性语义，也难以缓解跨视图异质性导致的特征分布漂移问题。

Method: 作者提出SMART模型，通过语义匹配对比学习机制，缓解多视图数据分布漂移，并有效进行对齐与未对齐样本间的跨视图语义匹配，从而充分挖掘样本间的共性与互补信息。

Result: 在8个公开数据集上的实验表明，SMART方法在部分视图对齐聚类任务上显著优于现有主流方法。

Conclusion: SMART模型能有效克服未对齐多视图数据的分布差异，提升对齐与未对齐样本间的聚类效果，为实际多视图学习场景提供了更有效的解决方案。

Abstract: Multi-view clustering has been empirically shown to improve learning performance by leveraging the inherent complementary information across multiple views of data. However, in real-world scenarios, collecting strictly aligned views is challenging, and learning from both aligned and unaligned data becomes a more practical solution. Partially View-aligned Clustering aims to learn correspondences between misaligned view samples to better exploit the potential consistency and complementarity across views, including both aligned and unaligned data. However, most existing PVC methods fail to leverage unaligned data to capture the shared semantics among samples from the same cluster. Moreover, the inherent heterogeneity of multi-view data induces distributional shifts in representations, leading to inaccuracies in establishing meaningful correspondences between cross-view latent features and, consequently, impairing learning effectiveness. To address these challenges, we propose a Semantic MAtching contRasTive learning model (SMART) for PVC. The main idea of our approach is to alleviate the influence of cross-view distributional shifts, thereby facilitating semantic matching contrastive learning to fully exploit semantic relationships in both aligned and unaligned data. Extensive experiments on eight benchmark datasets demonstrate that our method consistently outperforms existing approaches on the PVC problem.

</details>


### [55] [Preserving Marker Specificity with Lightweight Channel-Independent Representation Learning](https://arxiv.org/abs/2512.15410)
*Simon Gutwein,Arthur Longuefosse,Jun Seita,Sabine Taschner-Mandl,Roxane Licandro*

Main category: cs.CV

TL;DR: 本文比较了多重组织成像中的深度学习架构，发现轻量级、通道分离的浅层模型在表征学习上优于常用的早期通道融合模型。


<details>
  <summary>Details</summary>
Motivation: 主流的深度学习方法普遍采用早期通道融合，对所有蛋白标记共享结构的假设未被充分检验，而每个标记的信息可能独立、异质，尤其在区分稀有细胞时这一假设受到挑战，因此有必要探索保持标记独立性的方法是否能提升多重成像数据中的表征能力。

Method: 作者在Hodgkin淋巴瘤CODEX数据集（含145,000个细胞、49种蛋白标记）上，比较了标准早期融合型CNN与多类通道分离架构，包括作者提出的浅层独立通道模型CIM-S（仅5.5K参数）。所有模型均接受对比自监督预训练与线性评估，并在不同的数据增强和标记数量下验证鲁棒性和重复性。

Result: 早期通道融合模型对标记特异性信息保持有限，区分稀有细胞效果不好；而通道独立架构，尤其是CIM-S，能学得更强的表示，模型规模更小。该结论横跨多种自监督框架和数据设定均得以验证。

Conclusion: 轻量级、通道独立的模型不仅能与深层早期融合CNN竞争甚至超越其性能，是多重组织成像表征学习的优选架构。

Abstract: Multiplexed tissue imaging measures dozens of protein markers per cell, yet most deep learning models still apply early channel fusion, assuming shared structure across markers. We investigate whether preserving marker independence, combined with deliberately shallow architectures, provides a more suitable inductive bias for self-supervised representation learning in multiplex data than increasing model scale. Using a Hodgkin lymphoma CODEX dataset with 145,000 cells and 49 markers, we compare standard early-fusion CNNs with channel-separated architectures, including a marker-aware baseline and our novel shallow Channel-Independent Model (CIM-S) with 5.5K parameters. After contrastive pretraining and linear evaluation, early-fusion models show limited ability to retain marker-specific information and struggle particularly with rare-cell discrimination. Channel-independent architectures, and CIM-S in particular, achieve substantially stronger representations despite their compact size. These findings are consistent across multiple self-supervised frameworks, remain stable across augmentation settings, and are reproducible across both the 49-marker and reduced 18-marker settings. These results show that lightweight, channel-independent architectures can match or surpass deep early-fusion CNNs and foundation models for multiplex representation learning. Code is available at https://github.com/SimonBon/CIM-S.

</details>


### [56] [Step-GUI Technical Report](https://arxiv.org/abs/2512.15431)
*Haolong Yan,Jia Wang,Xin Huang,Yeqing Shen,Ziyang Meng,Zhimin Fan,Kaijun Tan,Jin Gao,Lieyu Shi,Mi Yang,Shiliang Yang,Zhirui Wang,Brian Li,Kang An,Chenyang Li,Lei Lei,Mengmeng Duan,Danxun Liang,Guodong Liu,Hang Cheng,Hao Wu,Jie Dong,Junhao Huang,Mei Chen,Renjie Yu,Shunshan Li,Xu Zhou,Yiting Dai,Yineng Deng,Yingdan Liang,Zelin Chen,Wen Sun,Chengxu Yan,Chunqin Xu,Dong Li,Fengqiong Xiao,Guanghao Fan,Guopeng Li,Guozhen Peng,Hongbing Li,Hang Li,Hongming Chen,Jingjing Xie,Jianyong Li,Jingyang Zhang,Jiaju Ren,Jiayu Yuan,Jianpeng Yin,Kai Cao,Liang Zhao,Liguo Tan,Liying Shi,Mengqiang Ren,Min Xu,Manjiao Liu,Mao Luo,Mingxin Wan,Na Wang,Nan Wu,Ning Wang,Peiyao Ma,Qingzhou Zhang,Qiao Wang,Qinlin Zeng,Qiong Gao,Qiongyao Li,Shangwu Zhong,Shuli Gao,Shaofan Liu,Shisi Gao,Shuang Luo,Xingbin Liu,Xiaojia Liu,Xiaojie Hou,Xin Liu,Xuanti Feng,Xuedan Cai,Xuan Wen,Xianwei Zhu,Xin Liang,Xin Liu,Xin Zhou,Yingxiu Zhao,Yukang Shi,Yunfang Xu,Yuqing Zeng,Yixun Zhang,Zejia Weng,Zhonghao Yan,Zhiguo Huang,Zhuoyu Wang,Zheng Ge,Jing Li,Yibo Zhu,Binxing Jiao,Xiangyu Zhang,Daxin Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种高效、高质量地获取GUI自动化训练数据的新方法，并推出了Step-GUI多模态大模型和GUI-MCP协议，实现了高隐私、高通用性的GUI自动化，并建立了AndroidDaily基准评测，推动了实际应用部署。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态大模型为GUI自动化带来了新机遇，但获取高质量训练数据的成本高、可靠性难以保证，阻碍了模型性能提升和实际落地，因此需要新的高效标注方法和评测体系。

Method: 1. 提出自进化训练流程和校准奖励系统，通过对模型生成的操作轨迹校准，实现高准确率、低成本的数据标注。
2. 基于这一流程训练了Step-GUI系列大模型（4B/8B）。
3. 提出首个GUI自动化建模上下文协议GUI-MCP，结合分层架构（底层原子操作+高层本地专家模型任务分配），保证隐私。
4. 构建AndroidDaily评测集，以真实用户场景考察模型性能。

Result: Step-GUI 8B模型在多个基准上表现优异（AndroidWorld 80.2%、OSWorld 48.5%、ScreenShot-Pro 62.6%），在AndroidDaily测评上分别取得静态动作89.91%、端到端任务52.50%的成绩，数据标注准确率超过90%，且大幅降低人力成本。

Conclusion: 该研究实现了高效、低成本的GUI自动化数据获取、训练和隐私友好的实际部署，为日常数字交互中的多模态大模型应用提供了可行方案和强有力的实验依据，有助于推动智能GUI代理的大规模应用。

Abstract: Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.

</details>


### [57] [CLIP-FTI: Fine-Grained Face Template Inversion via CLIP-Driven Attribute Conditioning](https://arxiv.org/abs/2512.15433)
*Longchen Dai,Zixuan Shen,Zhiheng Zhou,Peipeng Yu,Zhihua Xia*

Main category: cs.CV

TL;DR: 该论文提出了一种名为CLIP-FTI的新方法，通过结合CLIP模型的语义嵌入和StyleGAN，显著提升了人脸模板反演的精细化和可攻击性。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸模板反演方法虽然能还原较真实的人脸图像，但在面部细节（如眼、鼻、嘴）的还原上仍然过于平滑，且生成图像在跨模型攻击中表现有限。这导致隐私风险和伪造攻击的威胁没有被充分揭示，因此需要开发能提升细节保真和攻击迁移性的反演方法。

Method: 作者提出CLIP-FTI框架，以CLIP模型提取面部属性的语义特征，将这些特征与泄露的人脸模板通过跨模态特征交互网络融合，再投射到预训练StyleGAN的中间隐空间，由StyleGAN生成具有更精细面部属性且与原身份匹配的人脸图像。

Result: 实验证明，CLIP-FTI在多个主流人脸识别模型和数据集上，生成的人脸不仅身份和细节属性还原度更高，还提升了跨模型攻击的迁移性，达到了当前最佳（SOTA）水平。

Conclusion: CLIP-FTI首次利用人脸模板以外的额外信息辅助反演，显著提升了模板反演图像的精细度和攻击效果，给人脸识别系统的安全带来更大挑战。

Abstract: Face recognition systems store face templates for efficient matching. Once leaked, these templates pose a threat: inverting them can yield photorealistic surrogates that compromise privacy and enable impersonation. Although existing research has achieved relatively realistic face template inversion, the reconstructed facial images exhibit over-smoothed facial-part attributes (eyes, nose, mouth) and limited transferability. To address this problem, we present CLIP-FTI, a CLIP-driven fine-grained attribute conditioning framework for face template inversion. Our core idea is to use the CLIP model to obtain the semantic embeddings of facial features, in order to realize the reconstruction of specific facial feature attributes. Specifically, facial feature attribute embeddings extracted from CLIP are fused with the leaked template via a cross-modal feature interaction network and projected into the intermediate latent space of a pretrained StyleGAN. The StyleGAN generator then synthesizes face images with the same identity as the templates but with more fine-grained facial feature attributes. Experiments across multiple face recognition backbones and datasets show that our reconstructions (i) achieve higher identification accuracy and attribute similarity, (ii) recover sharper component-level attribute semantics, and (iii) improve cross-model attack transferability compared to prior reconstruction attacks. To the best of our knowledge, ours is the first method to use additional information besides the face template attack to realize face template inversion and obtains SOTA results.

</details>


### [58] [ST-DETrack: Identity-Preserving Branch Tracking in Entangled Plant Canopies via Dual Spatiotemporal Evidence](https://arxiv.org/abs/2512.15445)
*Yueqianji Chen,Kevin Williams,John H. Doonan,Paolo Remagnino,Jo Hepworth*

Main category: cs.CV

TL;DR: 提出了一种新颖的ST-DETrack网络，有效追踪植物枝条全生长期内的身份，显著提升了追踪准确率。


<details>
  <summary>Details</summary>
Motivation: 植物高通量表型分析中，自动分割和追踪单个枝条对于研究植物生长等关键表型性状非常重要，但由于枝条生长非刚性和枝条交错，导致身份追踪极具挑战。

Method: 设计了时空融合的双解码网络（ST-DETrack）：空间解码器利用几何先验（如位置、角度）进行早期追踪，时间解码器利用运动一致性解决后期遮挡。结合自适应门控机制动态调整空间-时间特征权重，并结合负重力性生物约束以减少竖直生长不确定性。

Result: 在油菜（Brassica napus）数据集上验证，ST-DETrack的分支匹配准确率（BMA）达93.6%，较空间和时间方法提升28.9与3.3个百分点。

Conclusion: 方法能在复杂、动态的植物结构条件下，实现长期和高准确性的分支身份追踪，具有良好的鲁棒性。

Abstract: Automated extraction of individual plant branches from time-series imagery is essential for high-throughput phenotyping, yet it remains computationally challenging due to non-rigid growth dynamics and severe identity fragmentation within entangled canopies. To overcome these stage-dependent ambiguities, we propose ST-DETrack, a spatiotemporal-fusion dual-decoder network designed to preserve branch identity from budding to flowering. Our architecture integrates a spatial decoder, which leverages geometric priors such as position and angle for early-stage tracking, with a temporal decoder that exploits motion consistency to resolve late-stage occlusions. Crucially, an adaptive gating mechanism dynamically shifts reliance between these spatial and temporal cues, while a biological constraint based on negative gravitropism mitigates vertical growth ambiguities. Validated on a Brassica napus dataset, ST-DETrack achieves a Branch Matching Accuracy (BMA) of 93.6%, significantly outperforming spatial and temporal baselines by 28.9 and 3.3 percentage points, respectively. These results demonstrate the method's robustness in maintaining long-term identity consistency amidst complex, dynamic plant architectures.

</details>


### [59] [Evaluation of deep learning architectures for wildlife object detection: A comparative study of ResNet and Inception](https://arxiv.org/abs/2512.15480)
*Malach Obisa Amonga,Benard Osero,Edna Too*

Main category: cs.CV

TL;DR: 本文对比研究了ResNet-101和Inception v3两种深度学习模型在野生动植物检测场景下的有效性，结果均表现优异。


<details>
  <summary>Details</summary>
Motivation: 野生动物目标检测对于生物多样性保护、生态监测等非常重要，但由于环境变化大、物种间视觉相似性高、类别内差异大，检测任务面临很大挑战。

Method: 采用标准化预处理（图片缩放至最长800像素、转为RGB格式、转换为PyTorch张量）；将野生动物图像数据集以70:30划分为训练和验证集，分别训练并评估ResNet-101和Inception v3模型。

Result: ResNet-101模型分类精度94%，mAP达0.91；Inception v3稍优，分类精度95%，mAP为0.92。两种模型在特征提取方面表现强劲，但面对物种高度相似、低光照和遮挡时识别有难度。

Conclusion: ResNet-101和Inception v3均能有效完成野生动物目标检测，为保护相关的计算机视觉应用提供了可靠技术基础。

Abstract: Wildlife object detection plays a vital role in biodiversity conservation, ecological monitoring, and habitat protection. However, this task is often challenged by environmental variability, visual similarities among species, and intra-class diversity. This study investigates the effectiveness of two individual deep learning architectures ResNet-101 and Inception v3 for wildlife object detection under such complex conditions. The models were trained and evaluated on a wildlife image dataset using a standardized preprocessing approach, which included resizing images to a maximum dimension of 800 pixels, converting them to RGB format, and transforming them into PyTorch tensors. A ratio of 70:30 training and validation split was used for model development. The ResNet-101 model achieved a classification accuracy of 94% and a mean Average Precision (mAP) of 0.91, showing strong performance in extracting deep hierarchical features. The Inception v3 model performed slightly better, attaining a classification accuracy of 95% and a mAP of 0.92, attributed to its efficient multi-scale feature extraction through parallel convolutions. Despite the strong results, both models exhibited challenges when detecting species with similar visual characteristics or those captured under poor lighting and occlusion. Nonetheless, the findings confirm that both ResNet-101 and Inception v3 are effective models for wildlife object detection tasks and provide a reliable foundation for conservation-focused computer vision applications.

</details>


### [60] [RUMPL: Ray-Based Transformers for Universal Multi-View 2D to 3D Human Pose Lifting](https://arxiv.org/abs/2512.15488)
*Seyed Abolfazl Ghasemzadeh,Alexandre Alahi,Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: RUMPL是一种基于Transformer的新型3D姿态估计方法，使用3D射线对2D关键点进行建模，不依赖相机校准与视角数量，在多视图应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有多视图3D姿态估计方法因真实多视图3D标注数据稀缺，加上通常在受限环境采集，导致在野外真实场景中泛化较差。大部分近期方法结合2D姿态估计与2D到3D映射，但对相机设置有限制，泛用性不足。

Method: 提出基于Transformer的RUMPL框架，引入3D射线表示，模型设计使其无需相机校准或固定视角数量即可部署。设计了新的View Fusion Transformer，融合不同视角的射线信息，提升了多视图一致性。

Result: RUMPL在多个数据集上实现了显著性能提升。相较三角测量法，MPJPE误差降低最多53%，相较其它基于Transformer的方法降低超60%。在新提出的野外及多人场景基准上，验证了其鲁棒性与可扩展性。

Conclusion: RUMPL框架通过3D射线表示和Transformer模型，极大提升了3D姿态估计在多视图、不定相机设置、真实环境下的泛化能力和准确率，技术可广泛适用于实际应用。

Abstract: Estimating 3D human poses from 2D images remains challenging due to occlusions and projective ambiguity. Multi-view learning-based approaches mitigate these issues but often fail to generalize to real-world scenarios, as large-scale multi-view datasets with 3D ground truth are scarce and captured under constrained conditions. To overcome this limitation, recent methods rely on 2D pose estimation combined with 2D-to-3D pose lifting trained on synthetic data. Building on our previous MPL framework, we propose RUMPL, a transformer-based 3D pose lifter that introduces a 3D ray-based representation of 2D keypoints. This formulation makes the model independent of camera calibration and the number of views, enabling universal deployment across arbitrary multi-view configurations without retraining or fine-tuning. A new View Fusion Transformer leverages learned fused-ray tokens to aggregate information along rays, further improving multi-view consistency. Extensive experiments demonstrate that RUMPL reduces MPJPE by up to 53% compared to triangulation and over 60% compared to transformer-based image-representation baselines. Results on new benchmarks, including in-the-wild multi-view and multi-person datasets, confirm its robustness and scalability. The framework's source code is available at https://github.com/aghasemzadeh/OpenRUMPL

</details>


### [61] [The LUMirage: An independent evaluation of zero-shot performance in the LUMIR challenge](https://arxiv.org/abs/2512.15505)
*Rohit Jena,Pratik Chaudhari,James C. Gee*

Main category: cs.CV

TL;DR: 本文重新评估了LUMIR挑战中深度学习配准方法“零样本泛化”能力的主张，结果表明实际性能远没有宣称的那么优越，尤其是在未知对比度、高分辨率和不同预处理条件下。


<details>
  <summary>Details</summary>
Motivation: LUMIR挑战宣称深度学习方法在影像配准任务上对未见类型和高分辨率能够实现“零样本泛化”，这与深度学习领域关于领域转移的认知存在冲突，促使作者对这一主张进行独立审查。

Method: 作者采用严格的评估协议，独立验证了公开数据上的零样本泛化表现，并排除了可能的工具偏差，比较了深度学习方法与传统迭代优化方法在同分布和异分布（不同对比度、分辨率等）数据上的表现。

Result: 结果显示：（1）深度学习与迭代方法在分布内T1w和灵长类（如猕猴）数据上性能相当；（2）在分布外数据（如T2, T2*, FLAIR）上，深度学习方法性能大幅下降，对实际临床流程有显著负面影响；（3）在高分辨率数据（0.6mm）下深度方法无法运行，而迭代方法效果更优；（4）深度方法对预处理高度敏感。

Conclusion: 深度学习方法并不能如宣称那样实现“零样本泛化”，相关主张需谨慎对待。未来应制定更加贴合实际临床与科研流程的评价协议，避免过度偏向某类方法。

Abstract: The LUMIR challenge represents an important benchmark for evaluating deformable image registration methods on large-scale neuroimaging data. While the challenge demonstrates that modern deep learning methods achieve competitive accuracy on T1-weighted MRI, it also claims exceptional zero-shot generalization to unseen contrasts and resolutions, assertions that contradict established understanding of domain shift in deep learning. In this paper, we perform an independent re-evaluation of these zero-shot claims using rigorous evaluation protocols while addressing potential sources of instrumentation bias. Our findings reveal a more nuanced picture: (1) deep learning methods perform comparably to iterative optimization on in-distribution T1w images and even on human-adjacent species (macaque), demonstrating improved task understanding; (2) however, performance degrades significantly on out-of-distribution contrasts (T2, T2*, FLAIR), with Cohen's d scores ranging from 0.7-1.5, indicating substantial practical impact on downstream clinical workflows; (3) deep learning methods face scalability limitations on high-resolution data, failing to run on 0.6 mm isotropic images, while iterative methods benefit from increased resolution; and (4) deep methods exhibit high sensitivity to preprocessing choices. These results align with the well-established literature on domain shift and suggest that claims of universal zero-shot superiority require careful scrutiny. We advocate for evaluation protocols that reflect practical clinical and research workflows rather than conditions that may inadvertently favor particular method classes.

</details>


### [62] [Off The Grid: Detection of Primitives for Feed-Forward 3D Gaussian Splatting](https://arxiv.org/abs/2512.15508)
*Arthur Moreau,Richard Shaw,Michal Nazarczuk,Jisu Shin,Thomas Tanay,Zhensong Zhang,Songcen Xu,Eduardo Pérez-Pellitero*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D Gaussian Splatting方法，采用自适应、非网格化（Off The Grid）分布替换了传统的像素对齐刚性网格，在提升画质和效率的同时减少了原始数据量。模型无需相机位姿输入即可快速生成高质量的新视角图像，并在效率和细节捕捉上优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 传统基于Feed-forward的3D Gaussian Splatting模型依赖稠密且固定的像素网格进行原语（primitive）的分布，这种机制限制了模型在细节还原、算力消耗和质量效率方面的表现。为克服此局限，需要探索更灵活与高效的原语分布方式。

Method: 作者提出了一种基于关键点检测思想的多分辨率解码器，能够在图像块间自适应地分配3D高斯原语。该结构替代了像素级网格，采用自监督方式与3D重建骨干网络进行端到端联合训练，无需外部相机位姿。

Result: 新方法在无需相机姿态标签的前提下，实现了秒级、无姿态约束的真实感新视角生成。在使用更少原语的同时，模型不仅减少了伪影，也提升了细节的捕捉能力，性能优于当前的Feed-forward模型。

Conclusion: 本文提出的方法不仅提升了生成效率与画面质量，还促进了3D重建模型本身对相机姿态的估计能力，表明未来可以训练无标签的基础模型。

Abstract: Feed-forward 3D Gaussian Splatting (3DGS) models enable real-time scene generation but are hindered by suboptimal pixel-aligned primitive placement, which relies on a dense, rigid grid and limits both quality and efficiency. We introduce a new feed-forward architecture that detects 3D Gaussian primitives at a sub-pixel level, replacing the pixel grid with an adaptive, "Off The Grid" distribution. Inspired by keypoint detection, our multi-resolution decoder learns to distribute primitives across image patches. This module is trained end-to-end with a 3D reconstruction backbone using self-supervised learning. Our resulting pose-free model generates photorealistic scenes in seconds, achieving state-of-the-art novel view synthesis for feed-forward models. It outperforms competitors while using far fewer primitives, demonstrating a more accurate and efficient allocation that captures fine details and reduces artifacts. Moreover, we observe that by learning to render 3D Gaussians, our 3D reconstruction backbone improves camera pose estimation, suggesting opportunities to train these foundational models without labels.

</details>


### [63] [VAAS: Vision-Attention Anomaly Scoring for Image Manipulation Detection in Digital Forensics](https://arxiv.org/abs/2512.15512)
*Opeyemi Bamigbade,Mark Scanlon,John Sheppard*

Main category: cs.CV

TL;DR: 本文提出VAAS框架，结合视觉注意力机制与局部一致性评分，实现对图像伪造的高效检测和异常评分，并且提升了解释性。实验结果显示本方法在主流数据集上具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 生成式AI能够制造高度真实的伪造图像，现有检测方法难以应对。同时，缺乏能够量化检测出异常强度的方法，难以明确伪造的程度和区域。

Method: 该方法提出了结合两个核心模块的框架：一是基于Vision Transformer实现全局的异常强度估计，二是通过SegFormer特征获取局部区域自一致性评分，最终融合为连续、可解释的异常分数组合。

Result: 在DF2023和CASIA v2.0两个公开数据集上实验，VAAS获得了有竞争力的F1和IoU分数，同时其生成的注意力异常图提升了可视化解释能力。

Conclusion: VAAS框架不仅实现了高性能的图像篡改检测，也使异常检测结果易于人类理解，为图像取证的透明性和可靠性提供支撑。

Abstract: Recent advances in AI-driven image generation have introduced new challenges for verifying the authenticity of digital evidence in forensic investigations. Modern generative models can produce visually consistent forgeries that evade traditional detectors based on pixel or compression artefacts. Most existing approaches also lack an explicit measure of anomaly intensity, which limits their ability to quantify the severity of manipulation. This paper introduces Vision-Attention Anomaly Scoring (VAAS), a novel dual-module framework that integrates global attention-based anomaly estimation using Vision Transformers (ViT) with patch-level self-consistency scoring derived from SegFormer embeddings. The hybrid formulation provides a continuous and interpretable anomaly score that reflects both the location and degree of manipulation. Evaluations on the DF2023 and CASIA v2.0 datasets demonstrate that VAAS achieves competitive F1 and IoU performance, while enhancing visual explainability through attention-guided anomaly maps. The framework bridges quantitative detection with human-understandable reasoning, supporting transparent and reliable image integrity assessment. The source code for all experiments and corresponding materials for reproducing the results are available open source.

</details>


### [64] [DeX-Portrait: Disentangled and Expressive Portrait Animation via Explicit and Latent Motion Representations](https://arxiv.org/abs/2512.15524)
*Yuxiang Shi,Zhe Li,Yanwen Wang,Hao Zhu,Xun Cao,Ligang Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新方法DeX-Portrait，可以实现肖像动画中头部姿态与面部表情的高质量解耦控制，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 已有扩散模型虽然能生成真实且生动的肖像动画，但无法很好地独立控制头部姿态和面部表情，限制了表情编辑以及姿态单独动画等应用场景。

Method: 作者创新性地分别用显式全局变换表示头部姿态、隐式潜在编码表示表情，设计了新的运动训练器独立提取精确的驱动信号，再将姿态变换通过双分支条件机制注入扩散模型，表情潜码通过交叉注意力注入，最后使用渐进式混合无分类器引导提升身份一致性。

Result: 实验结果显示，DeX-Portrait在动画质量和解耦可控性方面均优于目前的主流方法。

Conclusion: DeX-Portrait为肖像动画提供了一种高保真、可控性强的解耦姿态和表情的方法，推动了单图像驱动动画领域的发展。

Abstract: Portrait animation from a single source image and a driving video is a long-standing problem. Recent approaches tend to adopt diffusion-based image/video generation models for realistic and expressive animation. However, none of these diffusion models realizes high-fidelity disentangled control between the head pose and facial expression, hindering applications like expression-only or pose-only editing and animation. To address this, we propose DeX-Portrait, a novel approach capable of generating expressive portrait animation driven by disentangled pose and expression signals. Specifically, we represent the pose as an explicit global transformation and the expression as an implicit latent code. First, we design a powerful motion trainer to learn both pose and expression encoders for extracting precise and decomposed driving signals. Then we propose to inject the pose transformation into the diffusion model through a dual-branch conditioning mechanism, and the expression latent through cross attention. Finally, we design a progressive hybrid classifier-free guidance for more faithful identity consistency. Experiments show that our method outperforms state-of-the-art baselines on both animation quality and disentangled controllability.

</details>


### [65] [EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration](https://arxiv.org/abs/2512.15528)
*Daiqing Wu,Dongbao Yang,Can Ma. Yu Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种为多模态大模型（MLLM）赋予情感预测置信度表达能力的新方法EmoCaliber，并在VECBench基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉情感理解（VEC）方法通常将任务视为单一标签的判别性任务，忽略了情感理解的主观性及多样性，缺乏对预测不确定性和多样解释性的考虑，影响模型实际可用性和可靠性。

Method: 作者提出了一个三阶段训练框架：（1）逐步赋予模型结构化推理能力；（2）教会模型表达置信度；（3）校准模型输出的置信表达，并据此实现了名为EmoCaliber的VEC专用置信感知型MLLM。

Result: 在统一基准VECBench上，EmoCaliber在情感预测准确性和置信度估计两个维度都优于现有方法，表现出更全面和可靠的能力。

Conclusion: 引入置信表达机制能够提高VEC系统的解释性和可靠性。EmoCaliber的实验结果证明了该方法的有效性，为建设更可信任的视觉情感理解系统提供了可行方案。

Abstract: Visual Emotion Comprehension (VEC) aims to infer sentiment polarities or emotion categories from affective cues embedded in images. In recent years, Multimodal Large Language Models (MLLMs) have established a popular paradigm in VEC, leveraging their generalizability to unify VEC tasks defined under diverse emotion taxonomies. While this paradigm achieves notable success, it typically formulates VEC as a deterministic task, requiring the model to output a single, definitive emotion label for each image. Such a formulation insufficiently accounts for the inherent subjectivity of emotion perception, overlooking alternative interpretations that may be equally plausible to different viewers. To address this limitation, we propose equipping MLLMs with capabilities to verbalize their confidence in emotion predictions. This additional signal provides users with an estimate of both the plausibility of alternative interpretations and the MLLMs' self-assessed competence, thereby enhancing reliability in practice. Building on this insight, we introduce a three-stage training framework that progressively endows with structured reasoning, teaches to verbalize confidence, and calibrates confidence expression, culminating in EmoCaliber, a confidence-aware MLLM for VEC. Through fair and comprehensive evaluations on the unified benchmark VECBench, EmoCaliber demonstrates overall superiority against existing methods in both emotion prediction and confidence estimation. These results validate the effectiveness of our approach and mark a feasible step toward more reliable VEC systems. Project page: https://github.com/wdqqdw/EmoCaliber.

</details>


### [66] [An Efficient and Effective Encoder Model for Vision and Language Tasks in the Remote Sensing Domain](https://arxiv.org/abs/2512.15531)
*João Daniel Silva,Joao Magalhaes,Devis Tuia,Bruno Martins*

Main category: cs.CV

TL;DR: 本论文提出了一种高效且紧凑的新型多任务学习模型GeoMELT，能够在遥感视觉与语言任务中实现生成和检索任务，且在现有基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉-语言模型（LVLM）在遥感领域的多任务应用效果突出，但由于参数量巨大，训练与推理成本极高，限制其大规模部署。急需探索参数量更小、效率更高但能兼顾多任务能力的模型。

Method: 本文利用encoder-only架构，设计了一种新型的紧凑型多任务学习模型GeoMELT，专为遥感领域融合文本生成（如图像描述）与跨模态检索等多任务设计，突破了以往模型仅覆盖单一任务的局限。

Result: GeoMELT模型在多个权威基准测试中展示出高效且有效的性能，验证了其在参数高效前提下，依旧兼具多任务学习能力和优异表现。

Conclusion: 本研究证明在遥感多任务学习场景中，参数高效的encoder-only模型（GeoMELT）能用于统一处理文本生成和跨模态检索等多任务，为相关领域带来低成本、高效率的解决方案。

Abstract: The remote sensing community has recently seen the emergence of methods based on Large Vision and Language Models (LVLMs) that can address multiple tasks at the intersection of computer vision and natural language processing. To fully exploit the potential of such models, a significant focus has been given to the collection of large amounts of training data that cover multiple remote sensing-specific tasks, such as image captioning or visual question answering. However, the cost of using and training LVLMs is high, due to the large number of parameters. While multiple parameter-efficient adaptation techniques have been explored, the computational costs of training and inference with these models can remain prohibitive for most institutions. In this work, we explore the use of encoder-only architectures and propose a model that can effectively address multi-task learning while remaining compact in terms of the number of parameters. In particular, our model tackles combinations of tasks that are not typically explored in a unified model: the generation of text from remote sensing images and cross-modal retrieval. The results of our GeoMELT model - named from Multi-task Efficient Learning Transformer - in established benchmarks confirm the efficacy and efficiency of the proposed approach.

</details>


### [67] [BLANKET: Anonymizing Faces in Infant Video Recordings](https://arxiv.org/abs/2512.15542)
*Ditmar Hadera,Jan Cech,Miroslav Purkrabek,Matej Hoffmann*

Main category: cs.CV

TL;DR: 本文提出BLANKET方法，实现对婴儿视频人脸的有效匿名处理，同时保留关键面部属性，优于现有方法DeepPrivacy2。


<details>
  <summary>Details</summary>
Motivation: 随着涉及人类，尤其是婴儿的视频数据大量产生，保护隐私、保障伦理迫切需要自动化且稳健的人脸匿名技术，以防止敏感身份信息泄露。

Method: 方法分两阶段：首先利用扩散模型进行修补，生成与原身份兼容的随机新面孔；其次采用时序一致的人脸交换技术，并实现表情转移，将新身份平滑融合进每帧视频中。

Result: 在婴儿视频数据集上，BLANKET与主流匿名方法DeepPrivacy2进行比较。评估指标包括去身份化程度、面部特征保留、人姿态估计影响及伪影等方面。结果表明BLANKET在除了身份改变外的所有方面均优于DeepPrivacy2。

Conclusion: BLANKET方法能有效实现婴儿视频数据的人脸匿名处理，并较好地保留关键面部信息及生成质量，有助于后续任务应用，是更优的自动化隐私保护工具。

Abstract: Ensuring the ethical use of video data involving human subjects, particularly infants, requires robust anonymization methods. We propose BLANKET (Baby-face Landmark-preserving ANonymization with Keypoint dEtection consisTency), a novel approach designed to anonymize infant faces in video recordings while preserving essential facial attributes. Our method comprises two stages. First, a new random face, compatible with the original identity, is generated via inpainting using a diffusion model. Second, the new identity is seamlessly incorporated into each video frame through temporally consistent face swapping with authentic expression transfer. The method is evaluated on a dataset of short video recordings of babies and is compared to the popular anonymization method, DeepPrivacy2. Key metrics assessed include the level of de-identification, preservation of facial attributes, impact on human pose estimation (as an example of a downstream task), and presence of artifacts. Both methods alter the identity, and our method outperforms DeepPrivacy2 in all other respects. The code is available as an easy-to-use anonymization demo at https://github.com/ctu-vras/blanket-infant-face-anonym.

</details>


### [68] [GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models](https://arxiv.org/abs/2512.15560)
*Bozhou Li,Sihan Yang,Yushuo Guan,Ruichuan An,Xinlong Chen,Yang Shi,Pengfei Wan,Wentao Zhang,Yuanxing zhang*

Main category: cs.CV

TL;DR: 提出了一种新型文本编码器GRAN-TED和TED-6K评测基准，用于提升文本到图像/视频扩散模型中的文本信息表征效率，并验证其实用性。


<details>
  <summary>Details</summary>
Motivation: 当前文本编码器的表现直接影响文本到图像/视频生成模型生成内容的语义精准度。然而，缺乏高效评测体系，以及难以将预训练语言模型适配到视觉生成任务，制约了文本编码器的发展。

Method: 提出TED-6K文本基准，不需训练端到端模型即可评测编码质量；还设计了一个两阶段训练流程：先在多模态大模型上微调，再用逐层加权方法提取更细致的文本特征，从而开发出新的GRAN-TED文本编码器。

Result: 实验表明GRAN-TED编码器在TED-6K基准和文本到图像/视频生成任务上都达到了当前最优性能，优于现有方法。

Conclusion: GRAN-TED结合TED-6K评测，有效提升了文本到图像/视频生成任务的文本表征能力，解决了现有编码器评估和适配效率低的问题，对产出高质量视觉生成内容具有现实意义。

Abstract: The text encoder is a critical component of text-to-image and text-to-video diffusion models, fundamentally determining the semantic fidelity of the generated content. However, its development has been hindered by two major challenges: the lack of an efficient evaluation framework that reliably predicts downstream generation performance, and the difficulty of effectively adapting pretrained language models for visual synthesis. To address these issues, we introduce GRAN-TED, a paradigm to Generate Robust, Aligned, and Nuanced Text Embeddings for Diffusion models. Our contribution is twofold. First, we propose TED-6K, a novel text-only benchmark that enables efficient and robust assessment of an encoder's representational quality without requiring costly end-to-end model training. We demonstrate that performance on TED-6K, standardized via a lightweight, unified adapter, strongly correlates with an encoder's effectiveness in downstream generation tasks. Second, guided by this validated framework, we develop a superior text encoder using a novel two-stage training paradigm. This process involves an initial fine-tuning stage on a Multimodal Large Language Model for better visual representation, followed by a layer-wise weighting method to extract more nuanced and potent text features. Our experiments show that the resulting GRAN-TED encoder not only achieves state-of-the-art performance on TED-6K but also leads to demonstrable performance gains in text-to-image and text-to-video generation. Our code is available at the following link: https://anonymous.4open.science/r/GRAN-TED-4FCC/.

</details>


### [69] [On the Effectiveness of Textual Prompting with Lightweight Fine-Tuning for SAM3 Remote Sensing Segmentation](https://arxiv.org/abs/2512.15564)
*Roni Blushtein-Livnon,Osher Rafaeli,David Ioffe,Amir Boger,Karen Sandberg Esquenazi,Tal Svoray*

Main category: cs.CV

TL;DR: 本文评估了SAM3框架在遥感图像分割中的表现，发现结合语义和几何提示能获得最佳效果，少量几何标注便可实现有效自适应，但边界分割仍存在挑战。


<details>
  <summary>Details</summary>
Motivation: 遥感图像缺乏大量标注数据，且与自然图像存在域差异，导致主流分割模型难以高效适应，因此需要高效的少监督适应方法。

Method: 采用SAM3概念驱动框架，通过文本、几何或混合提示生成分割掩码。在遥感图像的四类目标上，比较不同提示策略（文本、几何、混合）和监督规模（零样本、轻量微调），分析其分割性能。

Result: 组合语义和几何线索取得了最高分割效果；纯文本提示表现最差，特别是针对形状不规则目标，显示其表达与遥感影像之间的语义鸿沟。随着监督的增加，性能提升明显，但收益递减；少量几何标注已足够。精确度与IoU之间的差距表明边界分割（尤其是不规则和稀有目标）仍是主要难题。

Conclusion: SAM3框架可通过少量几何指导自适应遥感分割任务。混合提示优于单一文本提示，对规则目标效果好，但形状复杂目标边界分割仍待提升。

Abstract: Remote sensing (RS) image segmentation is constrained by the limited availability of annotated data and a gap between overhead imagery and natural images used to train foundational models. This motivates effective adaptation under limited supervision. SAM3 concept-driven framework generates masks from textual prompts without requiring task-specific modifications, which may enable this adaptation. We evaluate SAM3 for RS imagery across four target types, comparing textual, geometric, and hybrid prompting strategies, under lightweight fine-tuning scales with increasing supervision, alongside zero-shot inference. Results show that combining semantic and geometric cues yields the highest performance across targets and metrics. Text-only prompting exhibits the lowest performance, with marked score gaps for irregularly shaped targets, reflecting limited semantic alignment between SAM3 textual representations and their overhead appearances. Nevertheless, textual prompting with light fine-tuning offers a practical performance-effort trade-off for geometrically regular and visually salient targets. Across targets, performance improves between zero-shot inference and fine-tuning, followed by diminishing returns as the supervision scale increases. Namely, a modest geometric annotation effort is sufficient for effective adaptation. A persistent gap between Precision and IoU further indicates that under-segmentation and boundary inaccuracies remain prevalent error patterns in RS tasks, particularly for irregular and less prevalent targets.

</details>


### [70] [MoonSeg3R: Monocular Online Zero-Shot Segment Anything in 3D with Reconstructive Foundation Priors](https://arxiv.org/abs/2512.15577)
*Zhipeng Du,Duolikun Danier,Jan Eric Lenssen,Hakan Bilen*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的在线零样本单目3D实例分割方法MoonSeg3R，解决现有方法对姿态和深度信息的依赖，首次实现单目RGB流上的高性能3D分割，且达到了与现有RGB-D方法相当的效果。


<details>
  <summary>Details</summary>
Motivation: 现有3D实例分割方法普遍依赖于带姿态的RGB-D序列，而在实际场景中仅有单目RGB数据的情况较为常见，因此有必要开发无需姿态和深度信息也能进行精确3D分割的新方法。

Method: 作者基于最新的CUT3R重建基础模型，以单目RGB流获得可靠的几何先验，并提出了MoonSeg3R方法，包括：（1）自监督的查询优化模块，将2D视觉基础模型的分割掩码转化为三维判别性查询；（2）三维查询索引记忆机制，实现时序一致性；（3）利用CUT3R中的状态分布token作为掩码标识符，加强跨帧信息融合。

Result: 在ScanNet200和SceneNN数据集上的实验表明，MoonSeg3R为首个支持在线单目3D分割的方法，性能与现有最先进的RGB-D方法相当。

Conclusion: MoonSeg3R打破了3D分割对深度与姿态信息的依赖，推动了3D视觉任务向轻量级、实用化方向发展，具有良好的应用前景。

Abstract: In this paper, we focus on online zero-shot monocular 3D instance segmentation, a novel practical setting where existing approaches fail to perform because they rely on posed RGB-D sequences. To overcome this limitation, we leverage CUT3R, a recent Reconstructive Foundation Model (RFM), to provide reliable geometric priors from a single RGB stream. We propose MoonSeg3R, which introduces three key components: (1) a self-supervised query refinement module with spatial-semantic distillation that transforms segmentation masks from 2D visual foundation models (VFMs) into discriminative 3D queries; (2) a 3D query index memory that provides temporal consistency by retrieving contextual queries; and (3) a state-distribution token from CUT3R that acts as a mask identity descriptor to strengthen cross-frame fusion. Experiments on ScanNet200 and SceneNN show that MoonSeg3R is the first method to enable online monocular 3D segmentation and achieves performance competitive with state-of-the-art RGB-D-based systems. Code and models will be released.

</details>


### [71] [IMKD: Intensity-Aware Multi-Level Knowledge Distillation for Camera-Radar Fusion](https://arxiv.org/abs/2512.15581)
*Shashank Mishra,Karan Patil,Didier Stricker,Jason Rambach*

Main category: cs.CV

TL;DR: 本文提出了一种名为IMKD的雷达-相机3D目标检测融合框架，通过多级知识蒸馏实现高性能、无需推理时使用LiDAR。IMKD在保持传感器特性的同时，增强了各自优势并提升融合效果，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有雷达-相机融合知识蒸馏方法往往直接转移模态特征，导致各自独有信息被削弱，影响检测性能。为了解决这一问题，需要既能增进跨模态互补、又不损伤各自独立优势的新型蒸馏方法。

Method: IMKD采用三阶段、强度感知的知识蒸馏策略：（1）LiDAR到雷达的细粒度结构特征蒸馏，增强雷达特征表达；（2）LiDAR到融合特征的强度引导蒸馏，有选择地突出有用几何和深度信息，增强互补；（3）相机-雷达的强度引导融合机制，有效对齐和校正多模态特征。

Result: 在nuScenes数据集上，IMKD达到了67.0% NDS和61.0% mAP，全面超越了所有基于知识蒸馏的雷达-相机融合方法。

Conclusion: IMKD实现了不依赖推理时LiDAR数据的高性能雷达-相机3D检测，为多模态目标检测提供了新思路，推动了实际应用落地。

Abstract: High-performance Radar-Camera 3D object detection can be achieved by leveraging knowledge distillation without using LiDAR at inference time. However, existing distillation methods typically transfer modality-specific features directly to each sensor, which can distort their unique characteristics and degrade their individual strengths. To address this, we introduce IMKD, a radar-camera fusion framework based on multi-level knowledge distillation that preserves each sensor's intrinsic characteristics while amplifying their complementary strengths. IMKD applies a three-stage, intensity-aware distillation strategy to enrich the fused representation across the architecture: (1) LiDAR-to-Radar intensity-aware feature distillation to enhance radar representations with fine-grained structural cues, (2) LiDAR-to-Fused feature intensity-guided distillation to selectively highlight useful geometry and depth information at the fusion level, fostering complementarity between the modalities rather than forcing them to align, and (3) Camera-Radar intensity-guided fusion mechanism that facilitates effective feature alignment and calibration. Extensive experiments on the nuScenes benchmark show that IMKD reaches 67.0% NDS and 61.0% mAP, outperforming all prior distillation-based radar-camera fusion methods. Our code and models are available at https://github.com/dfki-av/IMKD/.

</details>


### [72] [FlexAvatar: Learning Complete 3D Head Avatars with Partial Supervision](https://arxiv.org/abs/2512.15599)
*Tobias Kirschstein,Simon Giebenhain,Matthias Nießner*

Main category: cs.CV

TL;DR: FlexAvatar是一种能够从单张图像生成高质量、完整3D头部头像的方法，融合了单目和多视角数据的优势，实现了更好的3D重建和动画表现。


<details>
  <summary>Details</summary>
Motivation: 现有3D头像重建方法受限于多视角数据不足以及单目训练时3D信息残缺。作者发现单目视频训练时，驱动信号和目标视角的纠缠是导致效果差的根本原因。

Method: 提出了一种基于transformer的3D头像动画模型，引入可学习的数据源token（bias sinks），使模型能够统一利用单目和多视角数据进行训练。该方法在推理时既能利用单目数据的泛化能力，也可结合多视角数据提高3D完整性。此外，训练过程获得了平滑的隐空间，便于身份插值和多输入灵活拟合。

Result: 在单视图、少样本和单目头像重建任务中，FlexAvatar表现出色，能克服现有方法无法有效进行视角外推的问题，生成完整、真实的3D头像动画。

Conclusion: FlexAvatar有效融合了单目和多视角训练优势，实现了高质量、完整3D头像的生成和动画，显著优于现有方法，具有广泛应用前景。

Abstract: We introduce FlexAvatar, a method for creating high-quality and complete 3D head avatars from a single image. A core challenge lies in the limited availability of multi-view data and the tendency of monocular training to yield incomplete 3D head reconstructions. We identify the root cause of this issue as the entanglement between driving signal and target viewpoint when learning from monocular videos. To address this, we propose a transformer-based 3D portrait animation model with learnable data source tokens, so-called bias sinks, which enables unified training across monocular and multi-view datasets. This design leverages the strengths of both data sources during inference: strong generalization from monocular data and full 3D completeness from multi-view supervision. Furthermore, our training procedure yields a smooth latent avatar space that facilitates identity interpolation and flexible fitting to an arbitrary number of input observations. In extensive evaluations on single-view, few-shot, and monocular avatar creation tasks, we verify the efficacy of FlexAvatar. Many existing methods struggle with view extrapolation while FlexAvatar generates complete 3D head avatars with realistic facial animations. Website: https://tobias-kirschstein.github.io/flexavatar/

</details>


### [73] [Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition](https://arxiv.org/abs/2512.15603)
*Shengming Yin,Zekai Zhang,Zecheng Tang,Kaiyuan Gao,Xiao Xu,Kun Yan,Jiahao Li,Yilei Chen,Yuxiang Chen,Heung-Yeung Shum,Lionel M. Ni,Jingren Zhou,Junyang Lin,Chenfei Wu*

Main category: cs.CV

TL;DR: 本文提出Qwen-Image-Layered，一个能够将单张RGB图片分解为多个可独立编辑的RGBA图层的扩散模型，极大提升了一致性与可编辑性，在图像分层分解上超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉生成模型在图像编辑时一致性较差，原因是像素图像所有内容融合在同一画布中，而专业设计工具采用分层表示，从而带来更好的编辑性和一致性。因此，希望通过分层分解方法提升生成图像的可编辑性与一致性。

Method: 提出了Qwen-Image-Layered，包括：(1)设计RGBA-VAE以统一RGB与RGBA图像的潜在表示；(2)提出VLD-MMDiT架构以支持变长图层分解；(3)采用多阶段训练策略，将预训练生成模型适配为多层分解模型。同时搭建了用于从PSD文件中提取带注释高质量分层图像的数据管线。

Result: 实验结果表明，Qwen-Image-Layered在图像分层分解的质量上显著优于现有方法，能够实现一致性更佳的可编辑分层图像输出。

Conclusion: Qwen-Image-Layered为一致性图像编辑提供了新的范式，可独立编辑每一层内容，极大提升了实际应用中的灵活性和可控性，对图像生成及编辑任务具有重要意义。

Abstract: Recent visual generative models often struggle with consistency during image editing due to the entangled nature of raster images, where all visual content is fused into a single canvas. In contrast, professional design tools employ layered representations, allowing isolated edits while preserving consistency. Motivated by this, we propose \textbf{Qwen-Image-Layered}, an end-to-end diffusion model that decomposes a single RGB image into multiple semantically disentangled RGBA layers, enabling \textbf{inherent editability}, where each RGBA layer can be independently manipulated without affecting other content. To support variable-length decomposition, we introduce three key components: (1) an RGBA-VAE to unify the latent representations of RGB and RGBA images; (2) a VLD-MMDiT (Variable Layers Decomposition MMDiT) architecture capable of decomposing a variable number of image layers; and (3) a Multi-stage Training strategy to adapt a pretrained image generation model into a multilayer image decomposer. Furthermore, to address the scarcity of high-quality multilayer training images, we build a pipeline to extract and annotate multilayer images from Photoshop documents (PSD). Experiments demonstrate that our method significantly surpasses existing approaches in decomposition quality and establishes a new paradigm for consistent image editing. Our code and models are released on \href{https://github.com/QwenLM/Qwen-Image-Layered}{https://github.com/QwenLM/Qwen-Image-Layered}

</details>


### [74] [Robust Multi-view Camera Calibration from Dense Matches](https://arxiv.org/abs/2512.15608)
*Johannes Hägerlind,Bao-Long Tran,Urs Waldmann,Per-Erik Forssén*

Main category: cs.CV

TL;DR: 本文提出了一种更加稳健的相机位姿估计与标定方法，通过对SfM（结构光恢复运动）的各个组件进行分析，改进了对应点采样与视角增量加入，显著提升了在强畸变相机下的精度，适用于动物行为与法医分析等多摄像机场景。


<details>
  <summary>Details</summary>
Motivation: 在动物行为研究与监控画面分析等应用场景中，常常需要用多个从不同视角拍摄的刚性相机。准确估计这些相机的内部与外部参数依旧存在挑战，尤其是在相机存在较强畸变的情况下，本工作旨在提升这类复杂相机系统下的位姿估计与标定准确性及实用性。

Method: 作者系统性地分析了SfM流程中的关键环节，提出了有效的特征点稀疏采样策略以及更优的视角增量加入准则。通过对Dense matcher生成的预测对应点进行合理子采样，提高了位姿估计过程的鲁棒性。同时，精心设计了选择新视角加入顺序的标准，优化了整体的重建效果。

Result: 基于严格的定量实验，所提方法在具有强径向畸变相机条件下展现出显著性能提升（准确率79.9%，相较于原始VGGT方法的40.4%）。此外，在全局SfM场景下，作者用VGGT初始化后结合其子采样策略，也达到了良好的重建效果。

Conclusion: 文章方法可适应多种复杂摄像机布控环境，具有较高泛化能力，为动物行为等领域以及法医监控分析提供了稳定高效的SfM解决方案。

Abstract: Estimating camera intrinsics and extrinsics is a fundamental problem in computer vision, and while advances in structure-from-motion (SfM) have improved accuracy and robustness, open challenges remain. In this paper, we introduce a robust method for pose estimation and calibration. We consider a set of rigid cameras, each observing the scene from a different perspective, which is a typical camera setup in animal behavior studies and forensic analysis of surveillance footage. Specifically, we analyse the individual components in a structure-from-motion (SfM) pipeline, and identify design choices that improve accuracy. Our main contributions are: (1) we investigate how to best subsample the predicted correspondences from a dense matcher to leverage them in the estimation process. (2) We investigate selection criteria for how to add the views incrementally. In a rigorous quantitative evaluation, we show the effectiveness of our changes, especially for cameras with strong radial distortion (79.9% ours vs. 40.4 vanilla VGGT). Finally, we demonstrate our correspondence subsampling in a global SfM setting where we initialize the poses using VGGT. The proposed pipeline generalizes across a wide range of camera setups, and could thus become a useful tool for animal behavior and forensic analysis.

</details>


### [75] [Persistent feature reconstruction of resident space objects (RSOs) within inverse synthetic aperture radar (ISAR) images](https://arxiv.org/abs/2512.15618)
*Morgan Coe,Gruffudd Jones,Leah-Nani Alconcel,Marina Gashinova*

Main category: cs.CV

TL;DR: 该论文提出利用亚太赫兹波段ISAR成像系统对近地空间目标进行高分辨率外部结构识别，并结合特征检测与跟踪方法提升空间域态势感知能力。


<details>
  <summary>Details</summary>
Motivation: 随着近地轨道空间目标日益增多，需要对其状态和能力有更详细的感知与认知，因此发展独立于大气影响的空间基观测及高分辨图像特征识别成为迫切需求。

Method: 研究采用亚太赫兹ISAR成像系统，在100km范围内实现了亚厘米级分辨率。针对卫星的ISAR序列图像，通过仿射变换实现序列配准，采用比值梯度法进行边缘检测，用加权Hough变换高精度提取与跟踪线性特征，从而分析特征在时序图像中的演变和关联。

Result: 仿真器生成不同部署场景下的ISAR图像，经序列配准与特征提取后，特征检测和分类的置信度显著提升。论文还以阴影特征为例展示了方法的鲁棒性。

Conclusion: 将特征检测与跟踪结合应用于高分辨ISAR卫星图像序列，能有效提升近地空间域目标结构识别的准确性和可靠性，对于提升空间域态势感知水平具有重要意义。

Abstract: With the rapidly growing population of resident space objects (RSOs) in the near-Earth space environment, detailed information about their condition and capabilities is needed to provide Space Domain Awareness (SDA). Space-based sensing will enable inspection of RSOs at shorter ranges, independent of atmospheric effects, and from all aspects. The use of a sub-THz inverse synthetic aperture radar (ISAR) imaging and sensing system for SDA has been proposed in previous work, demonstrating the achievement of sub-cm image resolution at ranges of up to 100 km. This work focuses on recognition of external structures by use of sequential feature detection and tracking throughout the aligned ISAR images of the satellites. The Hough transform is employed to detect linear features, which are tracked throughout the sequence. ISAR imagery is generated via a metaheuristic simulator capable of modelling encounters for a variety of deployment scenarios. Initial frame-to-frame alignment is achieved through a series of affine transformations to facilitate later association between image features. A gradient-by-ratio method is used for edge detection within individual ISAR images, and edge magnitude and direction are subsequently used to inform a double-weighted Hough transform to detect features with high accuracy. Feature evolution during sequences of frames is analysed. It is shown that the use of feature tracking within sequences with the proposed approach will increase confidence in feature detection and classification, and an example use-case of robust detection of shadowing as a feature is presented.

</details>


### [76] [OccSTeP: Benchmarking 4D Occupancy Spatio-Temporal Persistence](https://arxiv.org/abs/2512.15621)
*Yu Zheng,Jie Hu,Kailun Yang,Jiaming Zhang*

Main category: cs.CV

TL;DR: 提出了新的4D Occupancy Spatio-Temporal Persistence (OccSTeP) 概念和基准，用于自动驾驶场景下的时空稳定性理解，通过新模型实现了更强健的预测性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶需要对三维环境具备持续、鲁棒的理解，同时还能预测未来变化及对假设动作的反应，现有方法在面对语义错误或传感器异常时表现不佳。因此，急需一种既能时间维度建模，又对实时数据丢失或噪音具备鲁棒性的新方法。

Method: 提出了OccSTeP任务和基准数据集，场景包含错误标签、丢帧等挑战。为了解决该任务，设计了基于体素的、无需tokenizer的世界模型OccSTeP-WM，其采用线性复杂度注意力骨干与循环状态空间模块以捕捉长距离空间依赖，并结合自车运动补偿机制对场景记忆进行增量融合，使得即使历史输入丢失或噪声严重，也能进行在线鲁棒推理。

Result: 实验显示，OccSTeP-WM在新基准上取得了平均语义mIoU 23.70%（提升6.56%），占用率IoU 35.89%（提升9.26%），显著优于对比方法。

Conclusion: 系统验证了OccSTeP理念及其实现OccSTeP-WM在自动驾驶场景下对时空持续理解和未来预测的有效性和强鲁棒性，相关数据与代码均已开放。

Abstract: Autonomous driving requires a persistent understanding of 3D scenes that is robust to temporal disturbances and accounts for potential future actions. We introduce a new concept of 4D Occupancy Spatio-Temporal Persistence (OccSTeP), which aims to address two tasks: (1) reactive forecasting: ''what will happen next'' and (2) proactive forecasting: "what would happen given a specific future action". For the first time, we create a new OccSTeP benchmark with challenging scenarios (e.g., erroneous semantic labels and dropped frames). To address this task, we propose OccSTeP-WM, a tokenizer-free world model that maintains a dense voxel-based scene state and incrementally fuses spatio-temporal context over time. OccSTeP-WM leverages a linear-complexity attention backbone and a recurrent state-space module to capture long-range spatial dependencies while continually updating the scene memory with ego-motion compensation. This design enables online inference and robust performance even when historical sensor input is missing or noisy. Extensive experiments prove the effectiveness of the OccSTeP concept and our OccSTeP-WM, yielding an average semantic mIoU of 23.70% (+6.56% gain) and occupancy IoU of 35.89% (+9.26% gain). The data and code will be open source at https://github.com/FaterYU/OccSTeP.

</details>


### [77] [Towards Physically-Based Sky-Modeling For Image Based Lighting](https://arxiv.org/abs/2512.15632)
*Ian J. Maquignaz*

Main category: cs.CV

TL;DR: 该论文提出了一种名为AllSky的全新天空建模方法，旨在直接从物理捕获的高动态范围图像（HDRI）中学习，生成高保真度、全动态范围的环境光照图，弥补现有DNN方法在真实感和动态范围上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的DNN天空建模与HDR文献方法，虽然能提升视觉质量，但在复现自然天空、实现真实重光和支持全动态范围（FDR，22档曝光）方面均有明显不足，限制了其在逼真户外渲染等领域的应用。

Method: 研究提出AllSky模型，直接以物理捕获的HDRI作为学习对象，探索输入信号、色调映射、条件约束与评价方案，并允许用户交互式控制太阳和云层位置实现环境的直观调节。

Result: AllSky在用户可控性、环境图真实性和全动态范围（22 f-stops）等维度达到SOTA效果，针对既有DNN天空模型、物理HDRI和参数化天空模型开展系统评测，实证证明AllSky的优势及当前方法的不足。

Conclusion: AllSky不仅提升了环境图的光照准确性和真实感，还显著增强了可交互性和可扩展性，当前DNN模型与物理HDR图不可直接互换，这对真实渲染等应用具重要意义。

Abstract: Accurate environment maps are a key component for rendering photorealistic outdoor scenes with coherent illumination. They enable captivating visual arts, immersive virtual reality, and a wide range of engineering and scientific applications. Recent works have extended sky-models to be more comprehensive and inclusive of cloud formations but, as we demonstrate, existing methods fall short in faithfully recreating natural skies. Though in recent years the visual quality of DNN-generated High Dynamic Range Imagery (HDRI) has greatly improved, the environment maps generated by DNN sky-models do not re-light scenes with the same tones, shadows, and illumination as physically captured HDR imagery. In this work, we demonstrate progress in HDR literature to be tangential to sky-modelling as current works cannot support both photorealism and the 22 f-stops required for the Full Dynamic Range (FDR) of outdoor illumination. We achieve this by proposing AllSky, a flexible all-weather sky-model learned directly from physically captured HDRI which we leverage to study the input modalities, tonemapping, conditioning, and evaluation of sky-models. Per user-controlled positioning of the sun and cloud formations, AllSky expands on current functionality by allowing for intuitive user control over environment maps and achieves state-of-the-art sky-model performance. Through our proposed evaluation, we demonstrate existing DNN sky-models are not interchangeable with physically captured HDRI or parametric sky-models, with current limitations being prohibitive of scalability and accurate illumination in downstream applications

</details>


### [78] [IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning](https://arxiv.org/abs/2512.15635)
*Yuanhang Li,Yiren Song,Junzhe Bai,Xinran Liang,Hu Yang,Libiao Jin,Qi Mao*

Main category: cs.CV

TL;DR: IC-Effect是一个基于指令引导和DiT模型的视频特效编辑框架，能够在保证空间和时间一致性的前提下，实现高质量的复杂视频特效合成。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑模型难以高质量地在不改变背景的前提下，将复杂特效（如火焰、粒子、卡通人物等）无缝注入视频，且高效从有限配对数据中学习特效模式存在挑战。

Method: 提出IC-Effect框架，利用DiT模型的上下文学习能力，通过以源视频为上下文条件，实现精确的背景保持和自然的特效注入。采用“两阶段训练策略”：第一阶段为通用编辑能力适应，第二阶段通过Effect-LoRA进行特效类别的专属学习。进一步引入时空稀疏token化，显著降低计算成本，提高效果。同时发布一个包含15种高质量视觉风格的视频特效编辑数据集。

Result: IC-Effect在多个实验中展现了高质量、可控且时间一致的视频特效编辑性能，显著优于现有方法。

Conclusion: IC-Effect为视频创作带来新的可能性，能够高效地实现复杂视频特效的少量样本编辑，严密保持背景不变并生成自然特效。

Abstract: We propose \textbf{IC-Effect}, an instruction-guided, DiT-based framework for few-shot video VFX editing that synthesizes complex effects (\eg flames, particles and cartoon characters) while strictly preserving spatial and temporal consistency. Video VFX editing is highly challenging because injected effects must blend seamlessly with the background, the background must remain entirely unchanged, and effect patterns must be learned efficiently from limited paired data. However, existing video editing models fail to satisfy these requirements. IC-Effect leverages the source video as clean contextual conditions, exploiting the contextual learning capability of DiT models to achieve precise background preservation and natural effect injection. A two-stage training strategy, consisting of general editing adaptation followed by effect-specific learning via Effect-LoRA, ensures strong instruction following and robust effect modeling. To further improve efficiency, we introduce spatiotemporal sparse tokenization, enabling high fidelity with substantially reduced computation. We also release a paired VFX editing dataset spanning $15$ high-quality visual styles. Extensive experiments show that IC-Effect delivers high-quality, controllable, and temporally consistent VFX editing, opening new possibilities for video creation.

</details>


### [79] [InpaintDPO: Mitigating Spatial Relationship Hallucinations in Foreground-conditioned Inpainting via Diverse Preference Optimization](https://arxiv.org/abs/2512.15644)
*Qirui Li,Yizhe Tang,Ran Yi,Guangben Lu,Fangyuan Zou,Peng Shu,Huan Yu,Jie Jiang*

Main category: cs.CV

TL;DR: 本文提出了InpaintDPO，一个专注于提升前景-背景空间合理性的图像修复生成方法，解决了前景对象与生成背景之间空间关系失真问题。


<details>
  <summary>Details</summary>
Motivation: 传统前景条件修复容易出现前景与背景空间关系（如尺度、位置、视角）不合理的错误，且空间合理性难以量化，限制了现有基于奖励的RLHF方法的使用。

Method: 1）提出基于直接偏好优化（DPO）的InpaintDPO方法，首次针对空间合理性优化前景条件图像修复；2）为解决DPO梯度冲突问题，设计MaskDPO，仅对背景部分优化偏好，前景用常规损失保持；3）为增强前景-背景边界连贯性，提出有条件的非对称偏好优化，通过不同裁剪操作采样提升感知一致性；4）提出共享共性偏好优化，提升模型对优秀样本空间共性的理解。

Result: 实验结果表明，InpaintDPO在前景-背景空间合理性及整体图像一致性方面优于现有方法，能够生成更协调的合成图像。

Conclusion: InpaintDPO为前景条件修复提供了新思路，通过多层次偏好优化显著提升了空间关系真实感，对可控图像生成领域有重要意义。

Abstract: Foreground-conditioned inpainting, which aims at generating a harmonious background for a given foreground subject based on the text prompt, is an important subfield in controllable image generation. A common challenge in current methods, however, is the occurrence of Spatial Relationship Hallucinations between the foreground subject and the generated background, including inappropriate scale, positional relationships, and viewpoints. Critically, the subjective nature of spatial rationality makes it challenging to quantify, hindering the use of traditional reward-based RLHF methods. To address this issue, we propose InpaintDPO, the first Direct Preference Optimization (DPO) based framework dedicated to spatial rationality in foreground-conditioned inpainting, ensuring plausible spatial relationships between foreground and background elements. To resolve the gradient conflicts in standard DPO caused by identical foreground in win-lose pairs, we propose MaskDPO, which confines preference optimization exclusively to the background to enhance background spatial relationships, while retaining the inpainting loss in the foreground region for robust foreground preservation. To enhance coherence at the foreground-background boundary, we propose Conditional Asymmetric Preference Optimization, which samples pairs with differentiated cropping operations and applies global preference optimization to promote contextual awareness and enhance boundary coherence. Finally, based on the observation that winning samples share a commonality in plausible spatial relationships, we propose Shared Commonality Preference Optimization to enhance the model's understanding of spatial commonality across high-quality winning samples, further promoting shared spatial rationality.

</details>


### [80] [Hard Labels In! Rethinking the Role of Hard Labels in Mitigating Local Semantic Drift](https://arxiv.org/abs/2512.15647)
*Jiacheng Cui,Bingkui Tong,Xinyue Bi,Xiaohan Zhao,Jiacheng Liu,Zhiqiang shen*

Main category: cs.CV

TL;DR: 本论文针对当前知识蒸馏与大规模数据集精炼中软标签普遍使用的问题，提出软标签易受局部语义漂移影响，并用硬标签辅助校正的新范式（HALD），在多个任务上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 在仅用少量图片裁剪时，软标签常会出现局部语义漂移，导致其表达与原始图像的真实语义不符，带来系统性误差及分布失配，却很少有工作关注硬标签在此中的作用。

Method: 理论分析软标签在少量监督时的语义漂移，并提出Soft-Hard标签混合的HALD训练范式，将硬标签作为语义校正锚点，引导学习过程，兼顾软标签细粒度优势。

Result: 在ImageNet-1K等数据集精炼与分类任务上，HALD方法实现42.7%准确率，仅需285M软标签存储，较最好现有方法LPLD提升9.0%。多个基准实验展现了方法的广泛优越性。

Conclusion: 文章重新强调硬标签作为软标签补充的重要性。实验结果证明硬标签可显著缓解语义漂移，优化标签监督的分布一致性，为软标签主导训练带来新思考。

Abstract: Soft labels generated by teacher models have become a dominant paradigm for knowledge transfer and recent large-scale dataset distillation such as SRe2L, RDED, LPLD, offering richer supervision than conventional hard labels. However, we observe that when only a limited number of crops per image are used, soft labels are prone to local semantic drift: a crop may visually resemble another class, causing its soft embedding to deviate from the ground-truth semantics of the original image. This mismatch between local visual content and global semantic meaning introduces systematic errors and distribution misalignment between training and testing. In this work, we revisit the overlooked role of hard labels and show that, when appropriately integrated, they provide a powerful content-agnostic anchor to calibrate semantic drift. We theoretically characterize the emergence of drift under few soft-label supervision and demonstrate that hybridizing soft and hard labels restores alignment between visual content and semantic supervision. Building on this insight, we propose a new training paradigm, Hard Label for Alleviating Local Semantic Drift (HALD), which leverages hard labels as intermediate corrective signals while retaining the fine-grained advantages of soft labels. Extensive experiments on dataset distillation and large-scale conventional classification benchmarks validate our approach, showing consistent improvements in generalization. On ImageNet-1K, we achieve 42.7% with only 285M storage for soft labels, outperforming prior state-of-the-art LPLD by 9.0%. Our findings re-establish the importance of hard labels as a complementary tool, and call for a rethinking of their role in soft-label-dominated training.

</details>


### [81] [Stylized Synthetic Augmentation further improves Corruption Robustness](https://arxiv.org/abs/2512.15675)
*Georg Siedel,Rojan Regmi,Abhirami Anand,Weijia Shao,Silvia Vock,Andrey Morozov*

Main category: cs.CV

TL;DR: 本文提出了一种结合合成图像与神经风格迁移的数据增强方法，显著提升了深度视觉模型对常见扰动的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度视觉模型容易受到常见扰动（如噪音、模糊等）的影响，造成分类性能下降。为了解决这一问题，研究者关注如何增强训练数据以提升模型鲁棒性。

Method: 作者提出结合合成图像生成与神经风格迁移的训练数据增强流程，并系统分析了相关增强方法及参数对图像分类器性能的影响。同时，还将该方法与TrivialAugment等流行的规则型增强技术结合，并比较了效果。

Result: 尽管风格迁移会使合成图像的FID指标变差，但这些图像能提升训练效果。实验结果表明，合成数据和风格化增强具有互补性，并能提升模型对图像扰动的鲁棒性。结合TrivialAugment方法时，模型在CIFAR-10-C、CIFAR-100-C和TinyImageNet-C上分别取得93.54%、74.9%和50.86%的鲁棒精度，达到最新水平。

Conclusion: 合成数据与神经风格迁移结合可有效提升模型对常见扰动的鲁棒性，为训练更健壮的视觉模型提供了新思路。

Abstract: This paper proposes a training data augmentation pipeline that combines synthetic image data with neural style transfer in order to address the vulnerability of deep vision models to common corruptions. We show that although applying style transfer on synthetic images degrades their quality with respect to the common FID metric, these images are surprisingly beneficial for model training. We conduct a systematic empirical analysis of the effects of both augmentations and their key hyperparameters on the performance of image classifiers. Our results demonstrate that stylization and synthetic data complement each other well and can be combined with popular rule-based data augmentation techniques such as TrivialAugment, while not working with others. Our method achieves state-of-the-art corruption robustness on several small-scale image classification benchmarks, reaching 93.54%, 74.9% and 50.86% robust accuracy on CIFAR-10-C, CIFAR-100-C and TinyImageNet-C, respectively

</details>


### [82] [Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning](https://arxiv.org/abs/2512.15693)
*Yifei Li,Wenzhao Zheng,Yanran Zhang,Runze Sun,Yu Zheng,Lei Chen,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Skyra的多模态大语言模型(MLLM)，实现对AI生成视频中可被人类感知的伪造痕迹进行检测与解释。作者还构建了首个大规模、精细注释的视频伪造痕迹数据集ViF-CoT-4K，并提出了配套的评测基准ViF-Bench。实验表明，Skyra在多个基准上的表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成视频检测方法仅限于二分类，且缺乏可解释性，不能满足人类对检测原因的理解需求。

Method: 1. 提出了Skyra模型，能检测并解释视频中的视觉伪造痕迹；2. 构建了ViF-CoT-4K数据集，用于模型的有监督微调，包含大规模精细化人工标注；3. 采用两阶段训练策略提升模型对时空伪造特征的感知、解释和检测能力；4. 提出ViF-Bench基准，用于多方法全面评测。

Result: Skyra模型在多个相关基准数据集上性能超过了现有各类检测方法，增强了检测的准确性和可解释性。

Conclusion: Skyra为AI视频伪造检测和解释提供了新方向，相关基准和数据集有望推动该领域未来发展。

Abstract: The misuse of AI-driven video generation technologies has raised serious social concerns, highlighting the urgent need for reliable AI-generated video detectors. However, most existing methods are limited to binary classification and lack the necessary explanations for human interpretation. In this paper, we present Skyra, a specialized multimodal large language model (MLLM) that identifies human-perceivable visual artifacts in AI-generated videos and leverages them as grounded evidence for both detection and explanation. To support this objective, we construct ViF-CoT-4K for Supervised Fine-Tuning (SFT), which represents the first large-scale AI-generated video artifact dataset with fine-grained human annotations. We then develop a two-stage training strategy that systematically enhances our model's spatio-temporal artifact perception, explanation capability, and detection accuracy. To comprehensively evaluate Skyra, we introduce ViF-Bench, a benchmark comprising 3K high-quality samples generated by over ten state-of-the-art video generators. Extensive experiments demonstrate that Skyra surpasses existing methods across multiple benchmarks, while our evaluation yields valuable insights for advancing explainable AI-generated video detection.

</details>


### [83] [VLIC: Vision-Language Models As Perceptual Judges for Human-Aligned Image Compression](https://arxiv.org/abs/2512.15701)
*Kyle Sargent,Ruiqi Gao,Philipp Henzler,Charles Herrmann,Aleksander Holynski,Li Fei-Fei,Jiajun Wu,Jason Zhang*

Main category: cs.CV

TL;DR: 本文提出利用先进的视觉-语言模型（VLM）来辅助图像压缩，使压缩效果更贴合人类感知。通过将VLM的零样本视觉推理能力用于判断图片对的感知差异，提出了一种新型基于扩散模型（VLIC）的图像压缩方法，并展示了在多个数据集上的优越表现。


<details>
  <summary>Details</summary>
Motivation: 以往基于简单失真度量（如MSE）的图像压缩方法无法与人类感知一致，尽管已有感知损失函数改进方法，但人类主观评价与压缩结果之间仍存在差距。作者希望利用VLM强大的视觉-语言推理能力，提升图像压缩对人类感知的契合度。

Method: 作者提出了Vision-Language Models for Image Compression（VLIC）系统。该系统基于扩散模型，后训练过程中直接利用VLM对图片对的二选一偏好判断作为训练反馈，而非通过另一个感知损失网络间接蒸馏。系统通过VLM零样本推理能力来引导压缩优化方向，力求最大化主观感知质量。

Result: VLIC在多个数据集和用户感知测试中均取得了具有竞争力甚至达到最新水平的表现。通过感知指标和大规模用户调研，VLIC表现优于或媲美当前领先的感知对齐压缩方法。

Conclusion: 利用VLM进行感知对齐的压缩模型能够有效提升人类主观感知质量，VLIC系统在架构与训练过程中展示了新颖和有效的设计。同时，作者分享了VLM奖励设计与训练流程中的重要经验，可为后续相关研究提供宝贵参考。

Abstract: Evaluations of image compression performance which include human preferences have generally found that naive distortion functions such as MSE are insufficiently aligned to human perception. In order to align compression models to human perception, prior work has employed differentiable perceptual losses consisting of neural networks calibrated on large-scale datasets of human psycho-visual judgments. We show that, surprisingly, state-of-the-art vision-language models (VLMs) can replicate binary human two-alternative forced choice (2AFC) judgments zero-shot when asked to reason about the differences between pairs of images. Motivated to exploit the powerful zero-shot visual reasoning capabilities of VLMs, we propose Vision-Language Models for Image Compression (VLIC), a diffusion-based image compression system designed to be post-trained with binary VLM judgments. VLIC leverages existing techniques for diffusion model post-training with preferences, rather than distilling the VLM judgments into a separate perceptual loss network. We show that calibrating this system on VLM judgments produces competitive or state-of-the-art performance on human-aligned visual compression depending on the dataset, according to perceptual metrics and large-scale user studies. We additionally conduct an extensive analysis of the VLM-based reward design and training procedure and share important insights. More visuals are available at https://kylesargent.github.io/vlic

</details>


### [84] [End-to-End Training for Autoregressive Video Diffusion via Self-Resampling](https://arxiv.org/abs/2512.15702)
*Yuwei Guo,Ceyuan Yang,Hao He,Yang Zhao,Meng Wei,Zhenheng Yang,Weilin Huang,Dahua Lin*

Main category: cs.CV

TL;DR: 本论文提出了一种名为Resampling Forcing的端到端自回归视频扩散模型训练框架，不依赖教师模型或判别器，通过自重采样机制和历史帧路由，有效解决了训练时推理偏差问题，提升了长时序视频生成的一致性与性能。


<details>
  <summary>Details</summary>
Motivation: 现有自回归视频扩散模型受训练与测试阶段分布不一致（exposure bias）影响，特别是在生成长视频时会产生累积误差。以往的缓解方法多依赖教师模型或判别器，流程复杂且难以大规模端到端应用。

Method: 提出无教师的Resampling Forcing框架，在训练过程中对历史帧进行自重采样，模拟推理时的错误情境。采用稀疏因果掩码实现并行训练，并基于帧级扩散损失优化。为提升长视频生成效率，引入历史路由机制，动态选取最相关的k个历史帧，无需额外参数。

Result: 实验表明，该方法在多个数据集上表现与基于蒸馏的主流方法相当，同时在长时序视频生成上的时序一致性明显优于现有技术。

Conclusion: Resampling Forcing无需教师模型即可端到端高效训练自回归视频模型，尤其适合长视频时序任务，为世界模拟与视频生成领域带来更高性能与更简流程的解决方案。

Abstract: Autoregressive video diffusion models hold promise for world simulation but are vulnerable to exposure bias arising from the train-test mismatch. While recent works address this via post-training, they typically rely on a bidirectional teacher model or online discriminator. To achieve an end-to-end solution, we introduce Resampling Forcing, a teacher-free framework that enables training autoregressive video models from scratch and at scale. Central to our approach is a self-resampling scheme that simulates inference-time model errors on history frames during training. Conditioned on these degraded histories, a sparse causal mask enforces temporal causality while enabling parallel training with frame-level diffusion loss. To facilitate efficient long-horizon generation, we further introduce history routing, a parameter-free mechanism that dynamically retrieves the top-k most relevant history frames for each query. Experiments demonstrate that our approach achieves performance comparable to distillation-based baselines while exhibiting superior temporal consistency on longer videos owing to native-length training.

</details>


### [85] [GateFusion: Hierarchical Gated Cross-Modal Fusion for Active Speaker Detection](https://arxiv.org/abs/2512.15707)
*Yu Wang,Juhyung Ha,Frangil M. Ramirez,Yuchen Wang,David J. Crandall*

Main category: cs.CV

TL;DR: 本文提出了一种新的主动说话人检测（ASD）架构GateFusion，通过分层门控融合和多模态自适应特征注入，实现细粒度的音视频特征交互，并在多个数据集上取得了新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有ASD方法多依赖于视觉与音频特征的晚期融合，难以捕捉关键的细致跨模态交互，制约了其在复杂场景下的鲁棒性提升。

Method: 提出GateFusion架构，结合强大的单模态预训练编码器和分层门控融合解码器（HiGate），在Transformer各层通过可学习门控引导多层次的跨模态特征注入，并引入Masked Alignment Loss（MAL）和Over-Positive Penalty（OPP）两个辅助目标强化多模态学习。

Result: 在Ego4D-ASD、UniTalk和WASD等主流基准上取得了77.8%、86.1%、96.1%的mAP（分别领先现有方法9.4%、2.9%、0.5%），并在AVA-ActiveSpeaker上表现出色。模型在非同源数据上的泛化能力良好，消融实验验证了各组件的互补性。

Conclusion: GateFusion通过精细化跨模态门控融合，有效提升了ASD的准确性和泛化能力，为复杂真实场景的主动说话人检测提供了有力方法。

Abstract: Active Speaker Detection (ASD) aims to identify who is currently speaking in each frame of a video. Most state-of-the-art approaches rely on late fusion to combine visual and audio features, but late fusion often fails to capture fine-grained cross-modal interactions, which can be critical for robust performance in unconstrained scenarios. In this paper, we introduce GateFusion, a novel architecture that combines strong pretrained unimodal encoders with a Hierarchical Gated Fusion Decoder (HiGate). HiGate enables progressive, multi-depth fusion by adaptively injecting contextual features from one modality into the other at multiple layers of the Transformer backbone, guided by learnable, bimodally-conditioned gates. To further strengthen multimodal learning, we propose two auxiliary objectives: Masked Alignment Loss (MAL) to align unimodal outputs with multimodal predictions, and Over-Positive Penalty (OPP) to suppress spurious video-only activations. GateFusion establishes new state-of-the-art results on several challenging ASD benchmarks, achieving 77.8% mAP (+9.4%), 86.1% mAP (+2.9%), and 96.1% mAP (+0.5%) on Ego4D-ASD, UniTalk, and WASD benchmarks, respectively, and delivering competitive performance on AVA-ActiveSpeaker. Out-of-domain experiments demonstrate the generalization of our model, while comprehensive ablations show the complementary benefits of each component.

</details>


### [86] [Multi-View Foundation Models](https://arxiv.org/abs/2512.15708)
*Leo Segre,Or Hirschorn,Shai Avidan*

Main category: cs.CV

TL;DR: 本文提出了一种将基础模型扩展为多视角基础模型的方法，使其在处理多视角同一场景时，能生成特征一致性更高的表征，并在表面法线估计和多视角分割等任务上取得了性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基础视觉模型在多视角场景下处理图像时，对每一个视图独立编码，导致同一3D点在不同视图下获得的特征不一致，这限制了模型在三维场景理解和多视角任务中的表现。

Method: 提出通过在Transformer基础模型（如DINO、SAM、CLIP）中插入3D感知的中间注意力层，实现不同视角间特征点的一致性对齐。模型输入多张同一场景图片，输出每张图片的特征图，并通过特定机制促使同一3D点的特征在不同图片中更一致。

Result: 在表面法线估计与多视角分割任务上进行了定量实验，结果显示所提方法在多视角特征匹配上相比现有基础模型有显著提升。

Conclusion: 该方法无需重建3D特征的一致模型，实现了直接在图像空间的操控，提高了多视角任务中的特征一致性，并有效提升了相关应用任务的表现。

Abstract: Foundation models are vital tools in various Computer Vision applications. They take as input a single RGB image and output a deep feature representation that is useful for various applications. However, in case we have multiple views of the same 3D scene, they operate on each image independently and do not always produce consistent features for the same 3D point. We propose a way to convert a Foundation Model into a Multi-View Foundation Model. Such a model takes as input a set of images and outputs a feature map for each image such that the features of corresponding points are as consistent as possible. This approach bypasses the need to build a consistent 3D model of the features and allows direct manipulation in the image space. Specifically, we show how to augment Transformers-based foundation models (i.e., DINO, SAM, CLIP) with intermediate 3D-aware attention layers that help match features across different views. As leading examples, we show surface normal estimation and multi-view segmentation tasks. Quantitative experiments show that our method improves feature matching considerably compared to current foundation models.

</details>


### [87] [Gaussian Pixel Codec Avatars: A Hybrid Representation for Efficient Rendering](https://arxiv.org/abs/2512.15711)
*Divam Gupta,Anuj Pahuja,Nemanja Bartolovic,Tomas Simon,Forrest Iandola,Giljoo Nam*

Main category: cs.CV

TL;DR: 该论文提出了一种名为GPiCA的高真实感头像重建方法，结合了三角网格和各向异性的3D高斯体，兼顾了渲染效率和逼真度，适用于在移动设备上高效渲染。


<details>
  <summary>Details</summary>
Motivation: 现有头像重建方法要么注重真实感却效率低下（如纯高斯体方法），要么注重渲染效率但真实感一般（如纯网格方法），很难兼得二者。因此需要新的方法在移动设备上高效渲染高逼真头像。

Method: 采用混合表示方式：面部区域用三角网格高效表示，头发胡须等非表面区域用3D各向异性高斯体表示，结合为统一可微渲染框架。神经网络输入表情编码，输出3D网格、纹理和高斯体三者，由统一引擎同时渲染。训练时利用多视角图像监督。

Result: 实验表明，GPiCA在头像视觉真实感上达到基于纯高斯体方法的效果，同时渲染性能与基于网格的方法相当，能在移动端高效渲染。

Conclusion: GPiCA兼顾了高真实感和高渲染效率，为移动设备上的高质量虚拟头像生成和渲染提供了有效方案。

Abstract: We present Gaussian Pixel Codec Avatars (GPiCA), photorealistic head avatars that can be generated from multi-view images and efficiently rendered on mobile devices. GPiCA utilizes a unique hybrid representation that combines a triangle mesh and anisotropic 3D Gaussians. This combination maximizes memory and rendering efficiency while maintaining a photorealistic appearance. The triangle mesh is highly efficient in representing surface areas like facial skin, while the 3D Gaussians effectively handle non-surface areas such as hair and beard. To this end, we develop a unified differentiable rendering pipeline that treats the mesh as a semi-transparent layer within the volumetric rendering paradigm of 3D Gaussian Splatting. We train neural networks to decode a facial expression code into three components: a 3D face mesh, an RGBA texture, and a set of 3D Gaussians. These components are rendered simultaneously in a unified rendering engine. The networks are trained using multi-view image supervision. Our results demonstrate that GPiCA achieves the realism of purely Gaussian-based avatars while matching the rendering performance of mesh-based avatars.

</details>


### [88] [DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models](https://arxiv.org/abs/2512.15713)
*Lunbin Zeng,Jingfeng Yao,Bencheng Liao,Hongyuan Tao,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种将强大的自回归（AR）多模态模型转化为扩散式视觉语言模型（dVLM）的方法——DiffusionVL。通过简单微调，可以让AR模型高效迁移至扩散范式，并取得大幅度性能提升和推理速度提升。


<details>
  <summary>Details</summary>
Motivation: 当前扩散式多模态模型因基础语言模型能力有限，表现落后于主流AR模型。作者思考是否可以利用已有强大AR模型来弥补扩散范式的能力不足。

Method: 作者提出DiffusionVL，可以将任意强大的AR模型转化为dVLM，仅需简单微调。核心还包括引入block-decoding的设计，支持任意长度生成和KV cache复用，以大大提升推理速度。

Result: DiffusionVL在用不到5%训练数据的条件下，相较以往方法，在MMMU-Pro (vision)提升34.4%，在MME (认知)提升37.5%，同时推理速度翻倍。

Conclusion: DiffusionVL证明了将强大AR模型迁移为扩散视觉语言模型的有效性和可行性，为多模态模型提供了一条高效且性能优越的新途径。

Abstract: In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL.

</details>


### [89] [In Pursuit of Pixel Supervision for Visual Pre-training](https://arxiv.org/abs/2512.15715)
*Lihe Yang,Shang-Wen Li,Yang Li,Xinjie Lei,Dong Wang,Abdelrahman Mohamed,Hengshuang Zhao,Hu Xu*

Main category: cs.CV

TL;DR: 本文提出了一种增强版掩码自编码器（Pixio），在2B网络图像上进行自监督预训练，在多种视觉下游任务中表现出色，能够与当前主流的自监督方法竞争。


<details>
  <summary>Details</summary>
Motivation: 虽然自编码器是经典的视觉表征学习方法，但近年来以DINOv3等为代表的latent-space自监督方法更为流行。作者想探索pixel-space自监督方法是否仍有竞争力，并提升其性能。

Method: 提出Pixio方法，通过设计更具挑战的掩码自编码预训练任务以及采用更强大的架构，并用自我筛选策略减少人工干预，基于2B web图片进行训练。

Result: 在深度估计、三维重建、语义分割和机器人学习等任务上，Pixio取得了匹配或超过DINOv3（同规模训练）的效果。

Conclusion: 像素空间自监督学习不仅是latent-space方法的有力补充，也可成为强有力的替代方案。

Abstract: At the most basic level, pixels are the source of the visual information through which we perceive the world. Pixels contain information at all levels, ranging from low-level attributes to high-level concepts. Autoencoders represent a classical and long-standing paradigm for learning representations from pixels or other raw inputs. In this work, we demonstrate that autoencoder-based self-supervised learning remains competitive today and can produce strong representations for downstream tasks, while remaining simple, stable, and efficient. Our model, codenamed "Pixio", is an enhanced masked autoencoder (MAE) with more challenging pre-training tasks and more capable architectures. The model is trained on 2B web-crawled images with a self-curation strategy with minimal human curation. Pixio performs competitively across a wide range of downstream tasks in the wild, including monocular depth estimation (e.g., Depth Anything), feed-forward 3D reconstruction (i.e., MapAnything), semantic segmentation, and robot learning, outperforming or matching DINOv3 trained at similar scales. Our results suggest that pixel-space self-supervised learning can serve as a promising alternative and a complement to latent-space approaches.

</details>


### [90] [Spatia: Video Generation with Updatable Spatial Memory](https://arxiv.org/abs/2512.15716)
*Jinjing Zhao,Fangyun Wei,Zhening Liu,Hongyang Zhang,Chang Xu,Yan Lu*

Main category: cs.CV

TL;DR: 现有视频生成模型难以保证视频在空间和时间上的一致性，Spatia框架通过引入空间记忆点云和视觉SLAM，有效提升了长期一致性，并支持更多3D相关应用。


<details>
  <summary>Details</summary>
Motivation: 视频生成模型在处理长视频、动态场景时，难以保持空间和时间上的长期一致性。这主要由于视频信号本身的高维和稠密特性。作者希望通过新框架解决这一瓶颈，提升生成视频的空间一致性及真实感。

Method: 提出Spatia框架：在生成视频过程中，以3D场景点云作为空间记忆，视频生成时始终参考并动态更新这个点云记忆。空间记忆通过视觉SLAM（同步定位与地图构建）实时更新，实现动态静态内容的分离（disentanglement），从而提升空间一致性。

Result: Spatia显著提升了生成视频的空间和时间一致性，并且能够支持显式的摄像机视角控制和3D感知的交互式编辑等更丰富的应用场景。

Conclusion: Spatia以几何为基础的空间记忆机制，有效提升了视频生成的一致性与可控性，同时为3D可编辑和交互式视频生成应用打下基础，具有很好的扩展性和实用价值。

Abstract: Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [91] [Incentives or Ontology? A Structural Rebuttal to OpenAI's Hallucination Thesis](https://arxiv.org/abs/2512.14801)
*Richard Ackermann,Simeon Emanuilov*

Main category: cs.CL

TL;DR: 本文认为Transformer语言模型的幻觉（hallucination）并非简单的优化失败，也不是仅靠改进奖励机制或评测基准就能解决，而是其体系结构的必然产物。


<details>
  <summary>Details</summary>
Motivation: 尽管OpenAI等机构认为，只需通过调整奖励机制和评测方法，即可减少或消除大型语言模型的幻觉，但作者认为这种解释过于简单，忽视了模型架构本身的局限。因此，作者希望通过理论和实验证明幻觉是Transformer模型不可避免的结构性问题。

Method: 作者借鉴了结构性幻觉的相关理论，结合实证实验（如引入Licensing Oracle），分析了Transformer模型在数据稀疏或边界条件下生成信息时的行为。特别强调Transformer通过统计关联建构的伪本体空间在语义边界上必然插值虚构内容，以确保连贯性。

Result: 实验证明，无论如何调整激励结构、提示方式、甚至继续微调模型，都无法从根本上消除幻觉。只有引入如Licensing Oracle这样的外部真实验证或选择性输出模块，才能实现“完美的避免虚假输出”。

Conclusion: 幻觉是生成式模型（如Transformer）的自然结构属性。要实现真正可靠的AI系统，必须引入能区分语言流畅性与知识责任的混合系统。

Abstract: OpenAI has recently argued that hallucinations in large language models result primarily from misaligned evaluation incentives that reward confident guessing rather than epistemic humility. On this view, hallucination is a contingent behavioral artifact, remediable through improved benchmarks and reward structures. In this paper, we challenge that interpretation. Drawing on previous work on structural hallucination and empirical experiments using a Licensing Oracle, we argue that hallucination is not an optimization failure but an architectural inevitability of the transformer model.
  Transformers do not represent the world; they model statistical associations among tokens. Their embedding spaces form a pseudo-ontology derived from linguistic co-occurrence rather than world-referential structure. At ontological boundary conditions - regions where training data is sparse or incoherent - the model necessarily interpolates fictional continuations in order to preserve coherence. No incentive mechanism can modify this structural dependence on pattern completion.
  Our empirical results demonstrate that hallucination can only be eliminated through external truth-validation and abstention modules, not through changes to incentives, prompting, or fine-tuning. The Licensing Oracle achieves perfect abstention precision across domains precisely because it supplies grounding that the transformer lacks.
  We conclude that hallucination is a structural property of generative architectures and that reliable AI requires hybrid systems that distinguish linguistic fluency from epistemic responsibility.

</details>


### [92] [T5Gemma 2: Seeing, Reading, and Understanding Longer](https://arxiv.org/abs/2512.14856)
*Biao Zhang,Paul Suganthan,Gaël Liu,Ilya Philippov,Sahil Dua,Ben Hora,Kat Black,Gus Martins,Omar Sanseviero,Shreya Pathak,Cassidy Hardin,Francesco Visin,Jiageng Zhang,Kathleen Kenealy,Qin Yin,Olivier Lacombe,Armand Joulin,Tris Warkentin,Adam Roberts*

Main category: cs.CL

TL;DR: T5Gemma 2是一种新的轻量级多语种、多模态、长上下文编码器-解码器模型，具备更高效的新结构，效果优于同类模型，并已开源。


<details>
  <summary>Details</summary>
Motivation: 当前大模型主要采用解码器结构，T5Gemma 2希望通过利用编码器-解码器结构，增强模型多语种、多模态和长上下文处理能力，并提升效率与性能，实现更广泛的应用。

Method: T5Gemma 2基于UL2方法，将预训练的仅解码器模型转换为编码器-解码器模型，并结合Gemma 3的多模态能力。提出了两项提升效率的方法：一是共享编码器和解码器的词嵌入，二是合并解码器的自注意力和交叉注意力为单一模块。

Result: 实验证明上述适配策略对不同结构和模态具有普适性，T5Gemma 2在长上下文建模上展现出编码器-解码器架构独特的优势，在预训练和训练后表现上均超越同参数量的Gemma 3模型。

Conclusion: T5Gemma 2作为新一代轻量级开放式模型，兼具多语种、多模态、长上下文、高效率等优点，预训练和微调效果优异，并已开源推动后续研究。

Abstract: We introduce T5Gemma 2, the next generation of the T5Gemma family of lightweight open encoder-decoder models, featuring strong multilingual, multimodal and long-context capabilities. T5Gemma 2 follows the adaptation recipe (via UL2) in T5Gemma -- adapting a pretrained decoder-only model into an encoder-decoder model, and extends it from text-only regime to multimodal based on the Gemma 3 models. We further propose two methods to improve the efficiency: tied word embedding that shares all embeddings across encoder and decoder, and merged attention that unifies decoder self- and cross-attention into a single joint module. Experiments demonstrate the generality of the adaptation strategy over architectures and modalities as well as the unique strength of the encoder-decoder architecture on long context modeling. Similar to T5Gemma, T5Gemma 2 yields comparable or better pretraining performance and significantly improved post-training performance than its Gemma 3 counterpart. We release the pretrained models (270M-270M, 1B-1B and 4B-4B) to the community for future research.

</details>


### [93] [Integrating Large Language Models and Knowledge Graphs to Capture Political Viewpoints in News Media](https://arxiv.org/abs/2512.14887)
*Massimiliano Fadda,Enrico Motta,Francesco Osborne,Diego Reforgiato Recupero,Angelo Salatino*

Main category: cs.CL

TL;DR: 本文提出了一种改进的观点识别流程，通过结合大语言模型（LLMs）微调与利用Wikidata丰富语义信息，提升了新闻语料中的观点分类能力，并在英国移民议题数据集上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 新闻机构对公共话语有重大影响，了解它们如何表达不同观点对评估媒体的公正与平衡至关重要。此前方法存在观点识别与分类的改进空间。

Method: 作者优化了既有流程，采用大语言模型进行观点分类的微调；同时引入Wikidata中相关行为体的语义描述，增强主张表征。在数据集（以英国移民议题为例）上与替代方案进行效果对比。

Result: 单独应用LLM微调或语义增强均可提升观点分类表现，但两者结合时效果最佳，特别是在可处理长文本输入的LLM模型上。

Conclusion: 结合大语言模型与Wikidata语义信息的新方法可显著提升新闻中观点的自动识别和分类效果，为分析媒体多元性和公正性提供了更强工具。

Abstract: News sources play a central role in democratic societies by shaping political and social discourse through specific topics, viewpoints and voices. Understanding these dynamics is essential for assessing whether the media landscape offers a balanced and fair account of public debate. In earlier work, we introduced a pipeline that, given a news corpus, i) uses a hybrid human-machine approach to identify the range of viewpoints expressed about a given topic, and ii) classifies relevant claims with respect to the identified viewpoints, defined as sets of semantically and ideologically congruent claims (e.g., positions arguing that immigration positively impacts the UK economy). In this paper, we improve this pipeline by i) fine-tuning Large Language Models (LLMs) for viewpoint classification and ii) enriching claim representations with semantic descriptions of relevant actors drawn from Wikidata. We evaluate our approach against alternative solutions on a benchmark centred on the UK immigration debate. Results show that while both mechanisms independently improve classification performance, their integration yields the best results, particularly when using LLMs capable of processing long inputs.

</details>


### [94] [DrugRAG: Enhancing Pharmacy LLM Performance Through A Novel Retrieval-Augmented Generation Pipeline](https://arxiv.org/abs/2512.14896)
*Houman Kazemzadeh,Kiarash Mokhtari Dizaji,Seyed Reza Tavakoli,Farbod Davoodi,MohammadReza KarimiNejad,Parham Abed Azad,Ali Sabzi,Armin Khosravi,Siavash Ahmadi,Mohammad Hossein Rohban,Glolamali Aminian,Tahereh Javaheri*

Main category: cs.CL

TL;DR: 本文评估了多种大语言模型（LLM）在药学执业资格类问答任务中的表现，并提出DrugRAG外部知识集成方法显著提升其准确率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在专业药学问答场景下准确率不理想，尤其是参数较小的模型更为明显，因此急需提升其在实际药事应用中的能力。

Method: 作者对11种不同参数规模的LLM进行药学问答基线测试，然后设计了三步检索-增强生成（RAG）流程DrugRAG，从专业药学知识库检索信息并增强模型输入，无需更改模型结构或参数。

Result: LLM原生准确率为46%~92%，参数低于8B的模型表现较差。应用DrugRAG后，所有模型准确率显著提升，增幅7~21个百分点。

Conclusion: DrugRAG方法无需修改模型本体即可提高药学AI的可靠性和实用性，为临床和药学实践提供可行的AI增强管道。

Abstract: Objectives: To evaluate large language model (LLM) performance on pharmacy licensure-style question-answering (QA) tasks and develop an external knowledge integration method to improve their accuracy.
  Methods: We benchmarked eleven existing LLMs with varying parameter sizes (8 billion to 70+ billion) using a 141-question pharmacy dataset. We measured baseline accuracy for each model without modification. We then developed a three-step retrieval-augmented generation (RAG) pipeline, DrugRAG, that retrieves structured drug knowledge from validated sources and augments model prompts with evidence-based context. This pipeline operates externally to the models, requiring no changes to model architecture or parameters.
  Results: Baseline accuracy ranged from 46% to 92%, with GPT-5 (92%) and o3 (89%) achieving the highest scores. Models with fewer than 8 billion parameters scored below 50%. DrugRAG improved accuracy across all tested models, with gains ranging from 7 to 21 percentage points (e.g., Gemma 3 27B: 61% to 71%, Llama 3.1 8B: 46% to 67%) on the 141-item benchmark.
  Conclusion: We demonstrate that external structured drug knowledge integration through DrugRAG measurably improves LLM accuracy on pharmacy tasks without modifying the underlying models. This approach provides a practical pipeline for enhancing pharmacy-focused AI applications with evidence-based information.

</details>


### [95] [Multiscale Aggregated Hierarchical Attention (MAHA): A Game Theoretic and Optimization Driven Approach to Efficient Contextual Modeling in Large Language Models](https://arxiv.org/abs/2512.14925)
*Caner Erden*

Main category: cs.CL

TL;DR: 本文提出了一种新的注意力机制MAHA，能够大幅降低长上下文任务中多头自注意力的计算量，同时兼顾全局与局部信息建模。


<details>
  <summary>Details</summary>
Motivation: 现有的自注意力机制在长序列下计算复杂度高，限制了大语言模型的扩展性。线性化或稀疏化注意力虽然降低了计算量，但容易丢失全局依赖或多尺度语义。

Method: 提出Multiscale Aggregated Hierarchical Attention（MAHA）框架：先用可学习的下采样算子把序列分层，再通过聚合不同分辨率下的注意力矩阵来建模全局与局部关系。聚合过程建模为资源分配问题，通过凸优化或纳什均衡博弈方法求解，并集成到混合膨胀卷积-Transformer结构中，用可微分优化层实现端到端训练。

Result: 实验证明，MAHA相较于标准注意力在序列长度4096时计算量减少了81%；在模型扩展性和建模效果上表现更佳。

Conclusion: MAHA提供了理论与实验兼具的新型注意力架构，显著推动了大语言模型高效扩展，兼具全局与多尺度信息捕获能力。

Abstract: The quadratic computational complexity of MultiHead SelfAttention (MHSA) remains a fundamental bottleneck in scaling Large Language Models (LLMs) for longcontext tasks. While sparse and linearized attention mechanisms attempt to mitigate this, they often compromise the representation of global dependencies or fail to capture multiscale semantic granularity effectively. In this paper, we propose Multiscale Aggregated Hierarchical Attention (MAHA), a novel architectural framework that reformulates the attention mechanism through hierarchical decomposition and mathematically rigorous aggregation. Unlike conventional approaches that treat token interactions at a single resolution, MAHA dynamically partitions the input sequence into hierarchical scales via learnable downsampling operators. The core innovation lies in its aggregation strategy: we model the fusion of scalespecific attention matrices as a resource allocation problem, solved via a convex optimization framework or a Nash equilibriumbased gametheoretic approach. This ensures a theoretically optimal balance between local nuance and global context fidelity. Implemented within a hybrid dilatedconvolutional transformer backbone, MAHA utilizes differentiable optimization layers to enable endtoend training. Experimental evaluations demonstrate that MAHA achieves superior scalability; empirical FLOPs analysis confirms an 81% reduction in computational cost at a sequence length of 4096 compared to standard attention. This work bridges the gap between optimization theory and sequence modeling, offering a scalable solution for nextgeneration LLMs.

</details>


### [96] [Parameter Efficient Multimodal Instruction Tuning for Romanian Vision Language Models](https://arxiv.org/abs/2512.14926)
*George-Andrei Dima,Dumitru-Clementin Cercel*

Main category: cs.CL

TL;DR: 本文致力于提升罗马尼亚语多模态NLP资源，翻译并扩展了Flickr30k数据集，显著提升了开放源模型在罗马尼亚语视觉问答与描述任务的能力。


<details>
  <summary>Details</summary>
Motivation: 低资源语言在生成式AI发展中资源稀缺，局限了AI的普惠性。现有多模态NLP数据集多聚焦于高资源语言，亟需针对罗马尼亚语等低资源语言的数据与模型能力提升。

Method: 将Flickr30k数据集翻译为罗马尼亚语，并利用开源LLM自动扩展为视觉问答数据集；选择三类流行视觉语言模型（LLaMA 3.2、LLaVA 1.6、Qwen2）进行微调，应用参数高效的LoRA方法。

Result: 微调后的模型在罗马尼亚语视觉问答和图像描述任务上表现更优；其中Qwen2-VL-RoVQA在两项任务上BERTScore F1分别提升6.05%和2.61%，语法错误也有明显减少。

Conclusion: 通过数据扩展与参数高效微调，实现了罗马尼亚语视觉多模态任务的能力提升，并改善了模型的语言流利性与理解能力，为低资源语言AI应用做出了有益探索。

Abstract: Focusing on low-resource languages is an essential step toward democratizing generative AI. In this work, we contribute to reducing the multimodal NLP resource gap for Romanian. We translate the widely known Flickr30k dataset into Romanian and further extend it for visual question answering by leveraging open-source LLMs. We demonstrate the usefulness of our datasets by fine-tuning open-source VLMs on Romanian visual question answering. We select VLMs from three widely used model families: LLaMA 3.2, LLaVA 1.6, and Qwen2. For fine-tuning, we employ the parameter-efficient LoRA method. Our models show improved Romanian capabilities in visual QA, as well as on tasks they were not trained on, such as Romanian image description generation. The seven-billion-parameter Qwen2-VL-RoVQA obtains top scores on both tasks, with improvements of +6.05% and +2.61% in BERTScore F1 over its original version. Finally, the models show substantial reductions in grammatical errors compared to their original forms, indicating improvements not only in language understanding but also in Romanian fluency.

</details>


### [97] [Cross-Tokenizer Likelihood Scoring Algorithms for Language Model Distillation](https://arxiv.org/abs/2512.14954)
*Buu Phan,Ashish Khisti,Karen Ullrich*

Main category: cs.CL

TL;DR: 该论文提出了一种解决教师模型和学生模型使用不同分词器时，计算下一个 token 概率的方法，实现了跨分词器的似然评分，既能保持精度，也提升了内存和推理效率。


<details>
  <summary>Details</summary>
Motivation: 在模型蒸馏等任务中，需要比较两个语言模型对同一句话下一个词概率的预测，但当模型采用不同分词器时，概率空间不一致，带来比较和知识传递的困难，尤其是实际部署中常为了减小模型、降低资源消耗而调整分词器结构。

Method: 论文分析了常用的 BPE 分词器的递归结构，提出了新的概率框架：在学生词表为教师子集时，可精确高效计算任意 token 概率；在词表完全不同的一般情形下，结合无损递归推理和高效近似算法，兼顾准确性与大规模可用性。

Result: 在 Qwen2.5-1.5B 模型蒸馏实验中，方法带来最多 12% 的内存占用减少，且性能提升最高达 4%；在复杂的数学推理 GSM8K 任务上，也较现有最佳方案有 2% 的准确率提升。

Conclusion: 该方法突破了以往分词器不一致带来跨模型概率难对比的难题，在蒸馏与推理尤其资源敏感场景下有显著优势，兼具高效性和泛用性。

Abstract: Computing next-token likelihood ratios between two language models (LMs) is a standard task in training paradigms such as knowledge distillation. Since this requires both models to share the same probability space, it becomes challenging when the teacher and student LMs use different tokenizers, for instance, when edge-device deployment necessitates a smaller vocabulary size to lower memory overhead. In this work, we address this vocabulary misalignment problem by uncovering an implicit recursive structure in the commonly deployed Byte-Pair Encoding (BPE) algorithm and utilizing it to create a probabilistic framework for cross-tokenizer likelihood scoring. Our method enables sequence likelihood evaluation for vocabularies different from the teacher model native tokenizer, addressing two specific scenarios: when the student vocabulary is a subset of the teacher vocabulary, and the general case where it is arbitrary. In the subset regime, our framework computes exact likelihoods and provides next-token probabilities for sequential sampling with only O(1) model evaluations per token. When used for distillation, this yields up to a 12% reduction in memory footprint for the Qwen2.5-1.5B model while also improving baseline performance up to 4% on the evaluated tasks. For the general case, we introduce a rigorous lossless procedure that leverages BPE recursive structure, complemented by a fast approximation that keeps large-vocabulary settings practical. Applied to distillation for mathematical reasoning, our approach improves GSM8K accuracy by more than 2% over the current state of the art.

</details>


### [98] [Evaluating Large Language Models on Multimodal Chemistry Olympiad Exams](https://arxiv.org/abs/2512.14989)
*Yiming Cui,Xin Yao,Yuxuan Qin,Xin Li,Shijin Wang,Guoping Hu*

Main category: cs.CL

TL;DR: 本文评估了40种主流多模态大模型在化学奥林匹克题目上的科学推理能力，发现它们在视觉与文本信息融合方面存在明显短板。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在科学领域任务（尤其是化学）表现有限，而化学推理高度依赖图片、结构和符号等多重模态，故需系统评估其实际能力。

Method: 作者构建了涵盖视觉与文本信息、基于美国国家化学奥林匹克考试的基准题库，系统性测试了40种主流专有及开源多模态大模型，在多模态融合和链式思考提示等方面设计实验。

Result: 研究显示许多模型未能有效融合图文信息，有时移除图片反而提升准确率。采用链式思考提示后，模型的推理准确性及视觉结合能力有明显提升。

Conclusion: 当前多模态大模型在化学领域仍存在科学推理不足、视觉-语言对齐失衡等问题。本文基准为后续研究提供参考，并指明需要从模型结构和推理能力上进一步突破多模态科学推理。

Abstract: Multimodal scientific reasoning remains a significant challenge for large language models (LLMs), particularly in chemistry, where problem-solving relies on symbolic diagrams, molecular structures, and structured visual data. Here, we systematically evaluate 40 proprietary and open-source multimodal LLMs, including GPT-5, o3, Gemini-2.5-Pro, and Qwen2.5-VL, on a curated benchmark of Olympiad-style chemistry questions drawn from over two decades of U.S. National Chemistry Olympiad (USNCO) exams. These questions require integrated visual and textual reasoning across diverse modalities. We find that many models struggle with modality fusion, where in some cases, removing the image even improves accuracy, indicating misalignment in vision-language integration. Chain-of-Thought prompting consistently enhances both accuracy and visual grounding, as demonstrated through ablation studies and occlusion-based interpretability. Our results reveal critical limitations in the scientific reasoning abilities of current MLLMs, providing actionable strategies for developing more robust and interpretable multimodal systems in chemistry. This work provides a timely benchmark for measuring progress in domain-specific multimodal AI and underscores the need for further advances at the intersection of artificial intelligence and scientific reasoning.

</details>


### [99] [DASH: Dialogue-Aware Similarity and Handshake Recognition for Topic Segmentation in Public-Channel Conversations](https://arxiv.org/abs/2512.15042)
*Sijin Sun,Liangbin Zhao,Ming Deng,Xiuju Fu*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的基于大语言模型的对话主题分割(DTS)方法DASH-DTS，并发布了首个实际海事通话数据集VHF-Dial。该方法显著提升了VHF和标准数据集上的分割效果。


<details>
  <summary>Details</summary>
Motivation: 在实际工作如海事通信中，对话往往非正式且主题切换隐晦，传统的主题分割方法在此类场景下表现有限。为提升自动理解和管理任务型对话的能力，需要更可靠和智能的分割模型。

Method: 提出DASH-DTS框架：(1) 通过‘握手’识别对话主题转变；(2) 利用相似性引导的样本选择增强上下文理解；(3) 精选正负样本提升模型辨别能力和鲁棒性。此外，发布了真实的VHF通信数据集（VHF-Dial）。

Result: DASH-DTS在公开的新VHF-Dial数据集及常用基准数据集上均取得了多项最优分割准确率，模型能输出可解释推理与置信度。

Conclusion: DASH-DTS为实际运营中的稳定监控与决策支持奠定强有力基础，为相关领域数据开放与研究拓展作出重要贡献。

Abstract: Dialogue Topic Segmentation (DTS) is crucial for understanding task-oriented public-channel communications, such as maritime VHF dialogues, which feature informal speech and implicit transitions. To address the limitations of traditional methods, we propose DASH-DTS, a novel LLM-based framework. Its core contributions are: (1) topic shift detection via dialogue handshake recognition; (2) contextual enhancement through similarity-guided example selection; and (3) the generation of selective positive and negative samples to improve model discrimination and robustness. Additionally, we release VHF-Dial, the first public dataset of real-world maritime VHF communications, to advance research in this domain. DASH-DTS provides interpretable reasoning and confidence scores for each segment. Experimental results demonstrate that our framework achieves several sota segmentation trusted accuracy on both VHF-Dial and standard benchmarks, establishing a strong foundation for stable monitoring and decision support in operational dialogues.

</details>


### [100] [SGM: Safety Glasses for Multimodal Large Language Models via Neuron-Level Detoxification](https://arxiv.org/abs/2512.15052)
*Hongbo Wang,MaungMaung AprilPyone,Isao Echizen*

Main category: cs.CL

TL;DR: 该论文提出了一种新方法SGM，从神经元层面对多模态大模型中的有害信息进行干预，大幅降低生成内容的毒性，并保持流畅性和推理能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型容易学习到数据中的有害、偏见和不当内容，尤其在面对对抗性攻击时现有去毒方法难以有效应对。因此，迫切需要一种高效、安全且可解释的去毒工具。

Method: 作者提出SGM方法，通过白盒方式在神经元层面对被判定为“有毒”的专家神经元进行加权软抑制，实现无须参数更新的去毒效果。同时建立了多模态毒性评估测试集MM-TOXIC-QA，并与现有方法进行定量比较。

Result: 在开源多模态大模型上的实验结果显示，SGM能在标准和对抗场景下将有害率从48.2%降至2.5%，同时保持生成内容的流畅性和推理能力。

Conclusion: SGM方法扩展性强，可与其他去毒技术集成（SGM*），为多模态大模型安全生成提供了低成本、可解释的控制方案。

Abstract: Disclaimer: Samples in this paper may be harmful and cause discomfort.
  Multimodal large language models (MLLMs) enable multimodal generation but inherit toxic, biased, and NSFW signals from weakly curated pretraining corpora, causing safety risks, especially under adversarial triggers that late, opaque training-free detoxification methods struggle to handle. We propose SGM, a white-box neuron-level multimodal intervention that acts like safety glasses for toxic neurons: it selectively recalibrates a small set of toxic expert neurons via expertise-weighted soft suppression, neutralizing harmful cross-modal activations without any parameter updates. We establish MM-TOXIC-QA, a multimodal toxicity evaluation framework, and compare SGM with existing detoxification techniques. Experiments on open-source MLLMs show that SGM mitigates toxicity in standard and adversarial conditions, cutting harmful rates from 48.2\% to 2.5\% while preserving fluency and multimodal reasoning. SGM is extensible, and its combined defenses, denoted as SGM*, integrate with existing detoxification methods for stronger safety performance, providing an interpretable, low-cost solution for toxicity-controlled multimodal generation.

</details>


### [101] [The Meta-Prompting Protocol: Orchestrating LLMs via Adversarial Feedback Loops](https://arxiv.org/abs/2512.15053)
*Fanzhe Fu*

Main category: cs.CL

TL;DR: 论文提出了Meta-Prompting Protocol，通过生成器、审计器和优化器三者协作，把大模型 “提示” 流程形式化为可编程、可自优化的系统，以提升LLM在关键任务下的确定性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前大模型依赖启发式的提示工程，缺乏确定性保证，难以应用于关键和高可靠性场景。论文希望为LLM提供可预测、可验证的交互协议。

Method: 提出Meta-Prompting Protocol，核心是“对抗三元组”架构：生成器（P）、审计器（A）和优化器（O）协作。该框架将自然语言指令视为可微变量，并用文本批判作为梯度，通过声明式编程（DSPy）和文本自动微分（TextGrad）实现。

Result: 理论上展示了该协议的可行性，并通过结合DSPy和TextGrad等工具，形成可观测的软件工程基础，能减少大模型幻觉和模型崩溃问题。

Conclusion: 该协议为大模型作为可靠软件组件提供了理论基础，有助于推动大模型在需要高可靠性的领域落地。

Abstract: The transition of Large Language Models (LLMs) from stochastic chat interfaces to reliable software components necessitates a fundamental re-engineering of interaction paradigms. Current methodologies, predominantly heuristic-based "prompt engineering," fail to provide the deterministic guarantees required for mission-critical applications. We introduce the Meta-Prompting Protocol, a rigorous theoretical framework that formalizes the orchestration of LLMs as a programmable, self-optimizing system. Central to this protocol is the Adversarial Trinity, a tripartite topology comprising a Generator (P), an Auditor (A), and an Optimizer (O). By treating natural language instructions as differentiable variables within a semantic computation graph and utilizing textual critiques as gradients, this architecture mitigates hallucination and prevents model collapse. We demonstrate the theoretical viability of this approach using declarative programming paradigms (DSPy) and automatic textual differentiation (TextGrad), establishing a foundation for "Observable Software Engineering" in the era of probabilistic computing.

</details>


### [102] [Beyond Majority Voting: Towards Fine-grained and More Reliable Reward Signal for Test-Time Reinforcement Learning](https://arxiv.org/abs/2512.15146)
*Weiqin Wang,Yile Wang,Kehao Chen,Hui Huang*

Main category: cs.CL

TL;DR: SCOPE通过引入模型置信度和动态子群分组，有效提升了大语言模型的推理能力，显著优于以往同类方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于多数投票策略的伪标签方法在大语言模型推理增强中容易造成确认偏差，且奖励稀疏，导致表现受限，因此亟需新的机制提升伪标签质量和推理多样性。

Method: 提出了SCOPE框架，结合模型的逐步置信度和动态子群划分，对多个候选输出进行分组，并依据置信度高低优先选取高质量推理路径，通过重复采样获得各子群局部共识，形成多元伪标签，丰富监督信号。

Result: 在多种模型和数据集上实验，SCOPE方法的性能均优于近期主流基线。在AIME 2025取得了13.1%的相对提升，在AMC上提升8.1%。

Conclusion: SCOPE通过创新性地结合信心加权和动态分组，有效缓解了传统多数投票的确认偏差问题并提升奖励信号密度，为无监督推理增强提供了更优方法。

Abstract: Test-time reinforcement learning mitigates the reliance on annotated data by using majority voting results as pseudo-labels, emerging as a complementary direction to reinforcement learning with verifiable rewards (RLVR) for improving reasoning ability of large language models (LLMs). However, this voting strategy often induces confirmation bias and suffers from sparse rewards, limiting the overall performance. In this work, we propose subgroup-specific step-wise confidence-weighted pseudo-label estimation (SCOPE), a framework integrating model confidence and dynamic subgroup partitioning to address these issues. Specifically, SCOPE integrates the proposed step-wise confidence into pseudo label deduction, prioritizing high-quality reasoning paths over simple frequency count. Furthermore, it dynamically partitions the candidate outputs pool into independent subgroups by balancing reasoning quality against exploration diversity. By deriving local consensus via repeat sampling for each sub group, SCOPE provides diverse supervision targets to encourage broader exploration. We conduct experiments across various models and benchmarks, experimental results show that SCOPE consistently outperforms recent baselines. Notably, SCOPE achieving relative improvements of 13.1\% on challenging AIME 2025 and 8.1\% on AMC. The code is released at \href{https://github.com/szu-tera/SCOPE}{https://github.com/szu-tera/SCOPE}.

</details>


### [103] [Rakuten Data Release: A Large-Scale and Long-Term Reviews Corpus for Hotel Domain](https://arxiv.org/abs/2512.15151)
*Yuki Nakayama,Koki Hikichi,Yun Ching Liu,Yu Hirate*

Main category: cs.CL

TL;DR: 本文发布了一个包含730万条、跨越2009-2024年的Rakuten Travel评论的大规模数据集，并分析了2019-2024年期间的数据漂移因素。


<details>
  <summary>Details</summary>
Motivation: 缺乏公开、规模大、包含多元属性的酒店评论数据集，制约了相关自然语言处理和用户行为分析的研究。通过发布该数据集，为学术与产业界提供研究和应用基础。

Method: 收集、清洗并整理16年间的Rakuten Travel评论，构建包含详细元数据与多维度评分的大型语料库，对2019-2024年数据漂移现象进行统计分析。

Result: 获得了一个结构丰富、信息完整的酒店评论数据集，并以统计方法揭示了2019-2024年间造成评论数据变化的因素。

Conclusion: 该数据集为酒店服务评价、用户行为分析与相关NLP任务提供了重要资源，同时，数据分析结果有助于理解平台与用户行为的演变和其对评论数据的影响。

Abstract: This paper presents a large-scale corpus of Rakuten Travel Reviews. Our collection contains 7.3 million customer reviews for 16 years, ranging from 2009 to 2024. Each record in the dataset contains the review text, its response from an accommodation, an anonymized reviewer ID, review date, accommodation ID, plan ID, plan title, room type, room name, purpose, accompanying group, and user ratings from different aspect categories, as well as an overall score. We present statistical information about our corpus and provide insights into factors driving data drift between 2019 and 2024 using statistical approaches.

</details>


### [104] [MCP-SafetyBench: A Benchmark for Safety Evaluation of Large Language Models with Real-World MCP Servers](https://arxiv.org/abs/2512.15163)
*Xuanjun Zong,Zhiqi Shen,Lei Wang,Yunshi Lan,Chao Yang*

Main category: cs.CL

TL;DR: 本文提出了MCP-SafetyBench，这是一个面向大型语言模型(MCP协议)交互安全性的全面测试基准，以真实多服务器场景与多轮任务为基础，系统评测了多种LLM的安全表现。


<details>
  <summary>Details</summary>
Motivation: 随着LLM向具备智能体能力（推理、规划、调用外部工具）转变，MCP协议成为它们与异构工具和服务连接的重要标准，但其开放性和多服务器特性带来了现有基准未涵盖的新安全风险。

Method: 设计并实现了MCP-SafetyBench基准，在真实MCP服务器环境下覆盖浏览器自动化、金融分析、导航、代码仓库管理和网页搜索五大领域，定义了20类攻击类型，任务涵盖多轮推理及跨服务器协调。

Result: 用MCP-SafetyBench系统评测了主流开源和闭源LLM，发现它们在安全性上表现差异大，任务复杂度提升和服务器交互次数增加时漏洞迅速增多。

Conclusion: MCP-SafetyBench有效揭示了现实部署中MCP框架的安全短板，为未来的安全防护方法研究及风险诊断提供了重要基础。

Abstract: Large language models (LLMs) are evolving into agentic systems that reason, plan, and operate external tools. The Model Context Protocol (MCP) is a key enabler of this transition, offering a standardized interface for connecting LLMs with heterogeneous tools and services. Yet MCP's openness and multi-server workflows introduce new safety risks that existing benchmarks fail to capture, as they focus on isolated attacks or lack real-world coverage. We present MCP-SafetyBench, a comprehensive benchmark built on real MCP servers that supports realistic multi-turn evaluation across five domains: browser automation, financial analysis, location navigation, repository management, and web search. It incorporates a unified taxonomy of 20 MCP attack types spanning server, host, and user sides, and includes tasks requiring multi-step reasoning and cross-server coordination under uncertainty. Using MCP-SafetyBench, we systematically evaluate leading open- and closed-source LLMs, revealing large disparities in safety performance and escalating vulnerabilities as task horizons and server interactions grow. Our results highlight the urgent need for stronger defenses and establish MCP-SafetyBench as a foundation for diagnosing and mitigating safety risks in real-world MCP deployments.

</details>


### [105] [From NLG Evaluation to Modern Student Assessment in the Era of ChatGPT: The Great Misalignment Problem and Pedagogical Multi-Factor Assessment (P-MFA)](https://arxiv.org/abs/2512.15183)
*Mika Hämäläinen,Kimmo Leiviskä*

Main category: cs.CL

TL;DR: 本文指出，当前自然语言生成（NLG）评价与芬兰高校学生评分均存在“重大错位问题”：传统方法只重结果，忽视过程，已不适应新技术带来的挑战。作者提出基于过程、多证据的评估新模型——教育多因子评估（P-MFA），以更好应对学生使用AI写作工具后的评估难题。


<details>
  <summary>Details</summary>
Motivation: 随着ChatGPT等AI写作工具的普及，学生产出的文本越来越复杂，传统上只关注最终作品而忽略学习和写作过程的评估方法，逐渐失去公信力和有效性。因此，作者希望提出新的评估方式来解决当前评分和NLG评价之间的错位问题。

Method: 作者提出了一种名为Pedagogical Multi-Factor Assessment (P-MFA)的评估模型。该模型借鉴多因子认证逻辑，通过多证据、多环节，强调学习和创作过程，而不仅仅评判最终结果。这样能更全面、准确地反映学生的真实能力和学习过程。

Result: P-MFA模型可以有效解决目前传统评分方法在AI生成文本普及背景下的失效问题，让评估体系更加关注过程和真实能力。多证据和过程导向的评价方式带来了更具说服力和适应时代变化的评判标准。

Conclusion: 单纯依赖结果的评分或NLG评价方式已难以胜任AI时代需求。通过像P-MFA这种过程-多因子结合的框架，可以实现更公平、有效的评估，保障教学与评价体系的合理性。

Abstract: This paper explores the growing epistemic parallel between NLG evaluation and grading of students in a Finnish University. We argue that both domains are experiencing a Great Misalignment Problem. As students increasingly use tools like ChatGPT to produce sophisticated outputs, traditional assessment methods that focus on final products rather than learning processes have lost their validity. To address this, we introduce the Pedagogical Multi-Factor Assessment (P-MFA) model, a process-based, multi-evidence framework inspired by the logic of multi-factor authentication.

</details>


### [106] [RFKG-CoT: Relation-Driven Adaptive Hop-count Selection and Few-Shot Path Guidance for Knowledge-Aware QA](https://arxiv.org/abs/2512.15219)
*Chao Zhang,Minghan Li,Tianrui Lv,Guodong Zhou*

Main category: cs.CL

TL;DR: 本论文提出了一种改进大模型知识图谱问答方法RFKG-CoT，通过自适应关系驱动跳数选择器与路径指导提升答案的准确性和真实性。


<details>
  <summary>Details</summary>
Motivation: 现有方法结合知识图谱路径以提升大型语言模型知识问答的可靠性，但面临仅基于问题选择固定跳数（导致推理僵化）以及推理路径利用不足的问题。

Method: 方法有两点创新：(1) 使用关系驱动的自适应跳数选择器，根据激活的知识图谱关系动态调整推理步数，通过关系掩码实现；(2) 设计带CoT思考的路径指导机制，采用"问题-路径-答案"的范例格式，提升大模型理解推理路径的能力。

Result: 在四个知识图谱问答基准测试集上，RFKG-CoT对比KG-CoT提升了准确率，最大提升达14.7个百分点（以Llama2-7B在WebQSP为例）。消融实验表明自适应跳数选择与路径提示机制互补，共同提升了基于KG证据的回答忠实性。

Conclusion: RFKG-CoT方法能有效克服以往KG-CoT的推理僵化和路径引导不足问题，显著提升大型语言模型在知识密集型问答场景的准确性和答案可靠性。

Abstract: Large language models (LLMs) often generate hallucinations in knowledge-intensive QA due to parametric knowledge limitations. While existing methods like KG-CoT improve reliability by integrating knowledge graph (KG) paths, they suffer from rigid hop-count selection (solely question-driven) and underutilization of reasoning paths (lack of guidance). To address this, we propose RFKG-CoT: First, it replaces the rigid hop-count selector with a relation-driven adaptive hop-count selector that dynamically adjusts reasoning steps by activating KG relations (e.g., 1-hop for direct "brother" relations, 2-hop for indirect "father-son" chains), formalized via a relation mask. Second, it introduces a few-shot in-context learning path guidance mechanism with CoT (think) that constructs examples in a "question-paths-answer" format to enhance LLMs' ability to understand reasoning paths. Experiments on four KGQA benchmarks show RFKG-CoT improves accuracy by up to 14.7 pp (Llama2-7B on WebQSP) over KG-CoT. Ablations confirm the hop-count selector and the path prompt are complementary, jointly transforming KG evidence into more faithful answers.

</details>


### [107] [Yes-MT's Submission to the Low-Resource Indic Language Translation Shared Task in WMT 2024](https://arxiv.org/abs/2512.15226)
*Yash Bhaskar,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: 本文介绍了Yes-MT团队在WMT 2024低资源印度语言翻译任务中的系统，主要研究了英语与Assamese、Mizo、Khasi、Manipuri语言的互译，同时对多种模型和方法进行了探索。


<details>
  <summary>Details</summary>
Motivation: 低资源印度语言缺乏高质量的平行语料，导致翻译系统性能有限，本文旨在提升这些语言之间与英语的翻译水平。

Method: 作者尝试了多种方法，包括：1）在多语或单语环境下微调mT5和IndicBart，2）对IndicTrans2和Llama 3应用LoRA微调，3）在大型语言模型（如Llama 3、Mixtral 8x7b）上进行零样本/小样本提示，4）从零开始训练Transformer模型。

Result: 在WMT23的低资源印度语言测试集上，通过SacreBLEU和CHRF指标对方法进行了评估。结果显示低资源翻译存在较大挑战，但大型语言模型经过微调后表现出较强潜力。

Conclusion: 大型语言模型在低资源语言翻译场景下，经过适当微调可以显著提升翻译效果，未来具备进一步研究与应用价值。

Abstract: This paper presents the systems submitted by the Yes-MT team for the Low-Resource Indic Language Translation Shared Task at WMT 2024 (Pakray et al., 2024), focusing on translating between English and the Assamese, Mizo, Khasi, and Manipuri languages. The experiments explored various approaches, including fine-tuning pre-trained models like mT5 (Xue et al., 2020) and IndicBart (Dabre et al., 2021) in both multilingual and monolingual settings, LoRA (Hu et al., 2021) fine-tuning IndicTrans2 (Gala et al., 2023), zero-shot and few-shot prompting (Brown, 2020) with large language models (LLMs) like Llama 3 (Dubey et al., 2024) and Mixtral 8x7b (Jiang et al., 2024), LoRA supervised fine-tuning of Llama 3 (Mecklenburg et al., 2024), and training Transformer models (Vaswani, 2017) from scratch. The results were evaluated on the WMT23 Low-Resource Indic Language Translation Shared Task test data using SacreBLEU (Post, 2018) and CHRF (Popovic, 2015), highlighting the challenges of low-resource translation and the potential of LLMs for these tasks, particularly with fine-tuning.

</details>


### [108] [FAME: Fictional Actors for Multilingual Erasure](https://arxiv.org/abs/2512.15235)
*Claudio Savelli,Moreno La Quatra,Alkis Koudounas,Flavio Giobergia*

Main category: cs.CL

TL;DR: 该论文提出了FAME，这是一个用于评估大语言模型（LLMs）遗忘能力的多语言合成基准，覆盖五种语言和两个忘却层级，可以更全面、可控地比较遗忘技术。


<details>
  <summary>Details</summary>
Motivation: 当前机器遗忘测评主要局限于英语，并且只支持实体级遗忘（如完全忘掉某个人的信息），无法细粒度或多语言地评估遗忘技术，限制了相关研究和实际应用的广度和深度。

Method: 作者构建了FAME数据集，包含1,000个虚构演员传记，涉及英语、法语、德语、意大利语和西班牙语五种语言，涵盖20个主题类别，并生成2万个问答对。FAME支持实体级遗忘（全忘）和实例级遗忘（部分事实遗忘）。数据集完全由虚构信息组成，避免真实数据带来的伦理和实验干扰。

Result: FAME成功提供了多语言、多层级的遗忘评测方案，并通过结构化设置，便于系统性比较不同机器遗忘方法的性能。实验环境高度可控，确保被评测内容未出现在预训练阶段。

Conclusion: FAME作为全新多语言遗忘基准，能推动LLM遗忘技术的研究，提高评测标准的全面性，为未来更强大、更合规的模型提供测试支持。

Abstract: LLMs trained on web-scale data raise concerns about privacy and the right to be forgotten. To address these issues, Machine Unlearning provides techniques to remove specific information from trained models without retraining from scratch. However, existing benchmarks for evaluating unlearning in LLMs face two major limitations: they focus only on English and support only entity-level forgetting (removing all information about a person). We introduce FAME (Fictional Actors for Multilingual Erasure), a synthetic benchmark for evaluating Machine Unlearning across five languages: English, French, German, Italian, and Spanish. FAME contains 1,000 fictional actor biographies and 20,000 question-answer pairs. Each biography includes information on 20 topics organized into structured categories (biography, career, achievements, personal information). This design enables both entity-level unlearning (i.e., forgetting entire identities) and instance-level unlearning (i.e., forgetting specific facts while retaining others). We provide two dataset splits to support these two different unlearning scenarios and enable systematic comparison of unlearning techniques across languages. Since FAME uses entirely fictional data, it ensures that the information was never encountered during model pretraining, allowing for a controlled evaluation of unlearning methods.

</details>


### [109] [The Moralization Corpus: Frame-Based Annotation and Analysis of Moralizing Speech Acts across Diverse Text Genres](https://arxiv.org/abs/2512.15248)
*Maria Becker,Mirko Sommer,Lars Tapken,Yi Wan Teh,Bruno Brocai*

Main category: cs.CL

TL;DR: 本文介绍了Moralization Corpus语料库，专注于分析论述中道德化表达的使用，并评估了大型语言模型在道德化检测任务中的表现，提升了对道德话语在NLP中处理的理解。


<details>
  <summary>Details</summary>
Motivation: 道德化表达是一种通过诉诸道德价值观来证明立场或要求的说服性交流方式，但此前对此的研究相对较少，尤其是在自动化、精细化分析上的挑战未得到有效解决。

Method: 作者构建了一个多体裁、包含政治辩论、新闻、网络讨论等德语文本的道德化语料库，并开发了基于“框架”的注释方案，捕捉道德化的道德价值、诉求和论述主体，并用这一语料库评估大型语言模型在不同提示条件下的道德化检测及成分抽取能力，结合人工注释进行对比。

Result: 实验发现，详细的任务指令对模型表现影响大于少样本或解释型提示，道德化表达的检测在自动和人工分析中都表现为高度主观且依赖语境，现有方法对该类复杂表达依然面临诸多挑战。

Conclusion: 该语料库与研究方法为NLP领域道德化话语的跨学科研究提供了新资源和工具，未来研究可基于此推动对道德推理与话语处理的深入探索。

Abstract: Moralizations - arguments that invoke moral values to justify demands or positions - are a yet underexplored form of persuasive communication. We present the Moralization Corpus, a novel multi-genre dataset designed to analyze how moral values are strategically used in argumentative discourse. Moralizations are pragmatically complex and often implicit, posing significant challenges for both human annotators and NLP systems. We develop a frame-based annotation scheme that captures the constitutive elements of moralizations - moral values, demands, and discourse protagonists - and apply it to a diverse set of German texts, including political debates, news articles, and online discussions. The corpus enables fine-grained analysis of moralizing language across communicative formats and domains. We further evaluate several large language models (LLMs) under varied prompting conditions for the task of moralization detection and moralization component extraction and compare it to human annotations in order to investigate the challenges of automatic and manual analysis of moralizations. Results show that detailed prompt instructions has a greater effect than few-shot or explanation-based prompting, and that moralization remains a highly subjective and context-sensitive task. We release all data, annotation guidelines, and code to foster future interdisciplinary research on moral discourse and moral reasoning in NLP.

</details>


### [110] [SynGP500: A Clinically-Grounded Synthetic Dataset of Australian General Practice Medical Notes](https://arxiv.org/abs/2512.15259)
*Piyawoot Songsiritat*

Main category: cs.CL

TL;DR: 本文介绍了SynGP500，一个包含500条仿真澳大利亚全科医疗笔记的数据集，具有多样性、真实性，能更好支持临床NLP研究。


<details>
  <summary>Details</summary>
Motivation: 澳大利亚全科医疗场景中缺乏高质量、涵盖广泛且保护隐私的医疗文本数据集，影响了本地化临床NLP模型的研发与评估。

Method: 作者根据澳大利亚全科医师课程和流行病学数据，结合实际咨询场景，由临床专家综合生成并精心设计500条杂乱、真实的合成医疗笔记，并通过流行病学对齐、风格变化、语义多样性和下游医学概念抽取等多方面进行评估。

Result: SynGP500在与真实就诊数据保持流行病学一致性的同时，展现出很高的语言和语义多样性，且在概念抽取等下游任务中提升了表现（F1提升）。

Conclusion: SynGP500填补了澳大利亚全科领域用于研究和教育的高质量、隐私安全医疗文本数据集空白，可用于推动本地化临床NLP方法的发展和评估。

Abstract: We introduce SynGP500, a clinician-curated collection of 500 synthetic Australian general practice medical notes. The dataset integrates curriculum-based clinical breadth (RACGP 2022 Curriculum), epidemiologically-calibrated prevalence (BEACH study), and diverse consultation contexts. This approach systematically includes both common presentations and less-common curriculum-specified conditions that GPs must recognize but appear infrequently in single practice populations, potentially supporting more generalizable model training than datasets constrained by naturally occurring case distributions. SynGP500 is messy by design, reflecting the authentic complexity of healthcare delivery: telegraphic documentation, typos, patient non-adherence, socioeconomic barriers, and clinician-patient disagreements, unlike sanitized synthetic datasets that obscure clinical realities. Multi-faceted validation demonstrates dataset quality through epidemiological alignment with real Australian GP consultation patterns (BEACH study), stylometric analysis confirming high linguistic variation, semantic diversity analysis demonstrating broad coverage, and exploratory downstream evaluation using self-supervised medical concept extraction, showing F1 improvements. SynGP500 addresses a critical national gap, providing researchers and educators with a resource for developing and evaluating clinical NLP methods for Australian general practice while inherently protecting patient privacy.

</details>


### [111] [Well Begun, Half Done: Reinforcement Learning with Prefix Optimization for LLM Reasoning](https://arxiv.org/abs/2512.15274)
*Yiliu Sun,Zicheng Zhao,Yang Wei,Yanfang Zhang,Chen Gong*

Main category: cs.CL

TL;DR: 本文提出了Progressive Prefix-token Policy Optimization (PPPO)方法，针对当前RLVR训练中未能有效关注高价值前缀token的问题，通过专注于前缀推理过程显著提升了大模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在训练过程中未区分token的重要性，导致高回报token优化不足，整体训练效率受限。论文试图改进训练方法，聚焦于更能影响推理结果的prefix（前缀）token。

Method: 提出PPPO方法，核心思路是将优化重点放在生成序列的前缀部分，借鉴'路径依赖'理论，并发现LLM推理中存在'起始锁定效应'（Beginning Lock-in Effect, BLE）。PPPO包含两个训练创新：一是Progressive Prefix Retention，逐步增加训练时保留前缀token的比例；二是Continuation Accumulated Reward，针对同一前缀采样多个后续，并累积得分缓解奖励偏差。

Result: 实验证明，PPPO在多项推理任务上显著优于现有代表性RLVR方法，在仅占26.17%训练token下，准确率提升18.02%。

Conclusion: 有针对性地优化前缀token，有效促进后续推理和整体性能提升。PPPO方法不仅提升了训练效率，还大幅增强了大语言模型的推理能力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) significantly enhances the reasoning capability of Large Language Models (LLMs). Current RLVR approaches typically conduct training across all generated tokens, but neglect to explore which tokens (e.g., prefix tokens) actually contribute to reasoning. This uniform training strategy spends substantial effort on optimizing low-return tokens, which in turn impedes the potential improvement from high-return tokens and reduces overall training effectiveness. To address this issue, we propose a novel RLVR approach called Progressive Prefix-token Policy Optimization (PPPO), which highlights the significance of the prefix segment of generated outputs. Specifically, inspired by the well-established human thinking theory of Path Dependence, where early-stage thoughts substantially constrain subsequent thinking trajectory, we identify an analogous phenomenon in LLM reasoning termed Beginning Lock-in Effect (BLE). PPPO leverages this finding by focusing its optimization objective on the prefix reasoning process of LLMs. This targeted optimization strategy can positively influence subsequent reasoning processes, and ultimately improve final results. To improve the learning effectiveness of LLMs on how to start reasoning with high quality, PPPO introduces two training strategies: (a) Progressive Prefix Retention, which shapes a progressive learning process by increasing the proportion of retained prefix tokens during training; (b) Continuation Accumulated Reward, which mitigates reward bias by sampling multiple continuations for one prefix token sequence, and accumulating their scores as the reward signal. Extensive experimental results on various reasoning tasks demonstrate that our proposed PPPO outperforms representative RLVR methods, with the accuracy improvements of 18.02% on only 26.17% training tokens.

</details>


### [112] [Towards Proactive Personalization through Profile Customization for Individual Users in Dialogues](https://arxiv.org/abs/2512.15302)
*Xiaotian Zhang,Yuan Wang,Ruizhe Chen,Zeya Wang,Runchen Hou,Zuozhu Liu*

Main category: cs.CL

TL;DR: 本文提出了PersonalAgent，一种能够持续推断和适应用户偏好的新型大模型交互体，在实现个性化和首轮冷启动场景下表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有大模型对齐技术主要针对普世价值或静态、单轮偏好，无法满足长效个性化和用户冷启动等需求，因此亟需更细致的用户个性偏好建模方法。

Method: PersonalAgent将对话分解为单轮交互，并将偏好推断建模为序列决策问题，通过持续完善统一的用户画像，实现动态更新和适应。

Result: 实验证明，PersonalAgent在标准和噪声对话环境下均优于现有的提示工程和策略优化基线模型，同时能保持跨会话的用户偏好一致性。人工评测也显示其能更自然、连贯地捕捉用户偏好。

Conclusion: 长期个性化对于构建更具包容性和适应性的对话智能体至关重要，PersonalAgent为此提供了有效新方案。

Abstract: The deployment of Large Language Models (LLMs) in interactive systems necessitates a deep alignment with the nuanced and dynamic preferences of individual users. Current alignment techniques predominantly address universal human values or static, single-turn preferences, thereby failing to address the critical needs of long-term personalization and the initial user cold-start problem. To bridge this gap, we propose PersonalAgent, a novel user-centric lifelong agent designed to continuously infer and adapt to user preferences. PersonalAgent constructs and dynamically refines a unified user profile by decomposing dialogues into single-turn interactions, framing preference inference as a sequential decision-making task. Experiments show that PersonalAgent achieves superior performance over strong prompt-based and policy optimization baselines, not only in idealized but also in noisy conversational contexts, while preserving cross-session preference consistency. Furthermore, human evaluation confirms that PersonalAgent excels at capturing user preferences naturally and coherently. Our findings underscore the importance of lifelong personalization for developing more inclusive and adaptive conversational agents. Our code is available here.

</details>


### [113] [Evaluating LLMs for Zeolite Synthesis Event Extraction (ZSEE): A Systematic Analysis of Prompting Strategies](https://arxiv.org/abs/2512.15312)
*Charan Prakash Rathore,Saumi Ray,Dhruv Kumar*

Main category: cs.CL

TL;DR: 该论文系统评估了多种大模型（LLM）在沸石合成实验过程信息抽取中的表现，发现虽然模型泛化能力强，但精准提取实验参数存在明显局限。


<details>
  <summary>Details</summary>
Motivation: 沸石合成实验记录中包含结构化信息，是材料发现领域的重要资源，但现有工作很少系统比较LLM在科学文献信息抽取上的表现和有效提示策略。该研究旨在回答不同提示策略下LLM抽取科学信息的有效性。

Method: 作者关注四个子任务：事件类型分类、触发词识别、论元角色抽取、论元文本抽取。对六种前沿LLM和四种提示策略（零样本、少样本、事件特定、反思型）在ZSEE数据集上进行了评估。

Result: 主要发现包括：1）事件类型分类F1较高（80-90%），但精细的参数提取F1只有50-65%；2）不同提示策略提升有限，尤其复杂策略提升不大；3）某些模型如GPT-5-mini对提示极其敏感，性能波动大；4）误差分析显示，模型易出现幻觉、过度泛化，对实验细节掌握不足。

Conclusion: 大模型能较好理解科学流程，但对实验参数的精准抽取存在根本性局限。针对领域的模型适配和更专门的抽取方法仍是提升准确率的关键。

Abstract: Extracting structured information from zeolite synthesis experimental procedures is critical for materials discovery, yet existing methods have not systematically evaluated Large Language Models (LLMs) for this domain-specific task. This work addresses a fundamental question: what is the efficacy of different prompting strategies when applying LLMs to scientific information extraction? We focus on four key subtasks: event type classification (identifying synthesis steps), trigger text identification (locating event mentions), argument role extraction (recognizing parameter types), and argument text extraction (extracting parameter values). We evaluate four prompting strategies - zero-shot, few-shot, event-specific, and reflection-based - across six state-of-the-art LLMs (Gemma-3-12b-it, GPT-5-mini, O4-mini, Claude-Haiku-3.5, DeepSeek reasoning and non-reasoning) using the ZSEE dataset of 1,530 annotated sentences. Results demonstrate strong performance on event type classification (80-90\% F1) but modest performance on fine-grained extraction tasks, particularly argument role and argument text extraction (50-65\% F1). GPT-5-mini exhibits extreme prompt sensitivity with 11-79\% F1 variation. Notably, advanced prompting strategies provide minimal improvements over zero-shot approaches, revealing fundamental architectural limitations. Error analysis identifies systematic hallucination, over-generalization, and inability to capture synthesis-specific nuances. Our findings demonstrate that while LLMs achieve high-level understanding, precise extraction of experimental parameters requires domain-adapted models, providing quantitative benchmarks for scientific information extraction.

</details>


### [114] [Why Your Academic Field Is Everywhere at Once: A Case Study of Arabic Linguistics](https://arxiv.org/abs/2512.15328)
*Ayman Eddakrouri,Amani Ramadan*

Main category: cs.CL

TL;DR: 本文利用Brookes的分类离散度（Δ）对当代阿拉伯应用语言学研究的主题结构进行了分析，发现该领域主题高度分散，说明学科内异质性强。


<details>
  <summary>Details</summary>
Motivation: 当前阿拉伯应用语言学领域文献众多，但其研究主题的结构和分布尚不清楚，需一种科学方法揭示其学科结构及异质性。

Method: 研究收集了2019-2025年间1564篇相关出版物，按8个核心子学科分类，应用Brookes原始的离散度公式，计算主题分散指数Δ，并分析其结果。

Result: 得出Δ=0.194，数值极低，显示主题极度分散；计算语言学虽突出但未占主导，社会语言学、语言教学等子领域同样活跃。

Conclusion: Brookes的公式适合用来表征学科结构，该方法具有可复制性，可用于其它学科领域，揭示领域内高度异质性。

Abstract: This study applies Brookes' Measure of Categorical Dispersion (Δ) to analyze the thematic structure of contemporary Arabic Applied Linguistics research. Using a comprehensive, real-world dataset of 1,564 publications from 2019 to 2025, classified into eight core sub-disciplines, we calculate a dispersion index of Δ = 0.194. This remarkably low value indicates extreme thematic dispersion, revealing that the field is characterized by pronounced heterogeneity rather than concentration. The analysis identifies Computational Linguistics as a dominant but non-hegemonic force, coexisting with robust research in Sociolinguistics, Language Teaching, and other subfields. This study clarifies the correct application of Brookes' original formula, demonstrates its utility for field characterization, and provides a replicable bibliometric methodology for assessing disciplinary structure across domains.

</details>


### [115] [Adversarial versification in portuguese as a jailbreak operator in LLMs](https://arxiv.org/abs/2512.15353)
*Joao Queiroz*

Main category: cs.CL

TL;DR: 本研究发现，将提示（prompt）写成诗歌形式是一种强大的对齐大语言模型（LLM）攻击手段，能极大提升越狱（jailbreak）成功率。


<details>
  <summary>Details</summary>
Motivation: 目前主流的大模型对齐安全策略主要依赖对常规文本表面特征的识别，然而缺乏对结构性变体（如诗歌、韵文提示）的深入防御能力。因此，探索诗歌形式的提示能否绕开模型安全防护、揭示现有对齐机制的缺陷，成为本文的研究出发点。

Method: 作者对常见的对齐大模型进行了实验，将通常被模型拒绝执行的提示内容改写为诗歌体，通过手动和自动化方式生成不同的诗歌提示，并在MLCommons AILuminate基准框架下评估模型的响应和安全失效率。此外，作者还分析了不同训练机制（RLHF、宪法AI、混合方法）下的表现，并指出对葡萄牙语诗歌相关对齐漏洞的研究空白。

Result: 结果显示：诗歌提示能让本被拒绝的敏感请求顺利通过，涉及指令的越狱成功率可高达18倍提升。手动编写诗歌达到约62%的攻击成功率，自动化生成也有43%。某些模型在单轮互动下成功率甚至超过90%。同时，各类训练机制下模型都表现出一致的脆弱性。此外，目前对葡萄牙语高复杂度诗歌模式的越狱评测基本缺失。

Conclusion: 本文揭示了现有大模型安全和对齐策略对表面特征过度依赖，面对结构体变化（比如诗歌形式）时存在严重漏洞。研究建议未来的安全评测和对齐机制应将诗歌、押韵句式等多样语用结构纳入测试，尤其是针对如葡萄牙语这类许多未覆盖的复杂语言。

Abstract: Recent evidence shows that the versification of prompts constitutes a highly effective adversarial mechanism against aligned LLMs. The study 'Adversarial poetry as a universal single-turn jailbreak mechanism in large language models' demonstrates that instructions routinely refused in prose become executable when rewritten as verse, producing up to 18 x more safety failures in benchmarks derived from MLCommons AILuminate. Manually written poems reach approximately 62% ASR, and automated versions 43%, with some models surpassing 90% success in single-turn interactions. The effect is structural: systems trained with RLHF, constitutional AI, and hybrid pipelines exhibit consistent degradation under minimal semiotic formal variation. Versification displaces the prompt into sparsely supervised latent regions, revealing guardrails that are excessively dependent on surface patterns. This dissociation between apparent robustness and real vulnerability exposes deep limitations in current alignment regimes. The absence of evaluations in Portuguese, a language with high morphosyntactic complexity, a rich metric-prosodic tradition, and over 250 million speakers, constitutes a critical gap. Experimental protocols must parameterise scansion, metre, and prosodic variation to test vulnerabilities specific to Lusophone patterns, which are currently ignored.

</details>


### [116] [Dual-Density Inference for Efficient Language Model Reasoning](https://arxiv.org/abs/2512.15358)
*Zhengyi Zhao,Shubo Zhang,Yuxi Zhang,Huimin Wang,Binyang Li,Kam-Fai Wong*

Main category: cs.CL

TL;DR: 该论文提出了一种新框架Denser，在大型语言模型推理过程中分别优化推理和答题阶段的信息密度，实现减少消耗的token数量，并提升或保持推理准确率。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在推理和最终答题过程中采用统一的信息密度，导致计算效率低。推理阶段主要为模型自身的计算服务，而答题阶段则需为人类理解服务。二者功能不同，有信息密度分离优化的空间。

Method: 提出Denser框架，将推理和回答分为两个阶段，分别进行信息密度优化：推理阶段采用高密度、符号化语言以提升计算效率；回答阶段则将推理结果转为便于人类理解的自然语言。整个框架包括输入分析、符号化推理、答案生成三个模块。

Result: 在多个推理问答基准测试上，Denser相比传统的Chain-of-Thought方法，token消耗降低最多可达62%，同时保持或提升了准确率，效率提升在多步复杂推理问题中优势尤为明显。

Conclusion: 针对推理和答题的不同需求优化信息密度能大幅提升大语言模型在推理任务中的效率且不损失甚至提升准确性，为后续复杂推理任务的模型设计与计算优化提供了新思路。

Abstract: Large Language Models (LLMs) have shown impressive capabilities in complex reasoning tasks. However, current approaches employ uniform language density for both intermediate reasoning and final answers, leading to computational inefficiency. Our observation found that reasoning process serves a computational function for the model itself, while answering serves a communicative function for human understanding. This distinction enables the use of compressed, symbol-rich language for intermediate computations while maintaining human-readable final explanations. To address this inefficiency, we present Denser: \underline{D}ual-d\underline{ens}ity inf\underline{er}ence, a novel framework that optimizes information density separately for reasoning and answering phases. Our framework implements this through three components: a query processing module that analyzes input problems, a high-density compressed reasoning mechanism for efficient intermediate computations, and an answer generation component that translates compressed reasoning into human-readable solutions. Experimental evaluation across multiple reasoning question answering benchmarks demonstrates that Denser reduces token consumption by up to 62\% compared to standard Chain-of-Thought methods while preserving or improving accuracy. These efficiency gains are particularly significant for complex multi-step reasoning problems where traditional methods generate extensive explanations.

</details>


### [117] [ORACLE: Time-Dependent Recursive Summary Graphs for Foresight on News Data Using LLMs](https://arxiv.org/abs/2512.15397)
*Lev Kharlashkin,Eiaki Morooka,Yehor Tereshchenko,Mika Hämäläinen*

Main category: cs.CL

TL;DR: ORACLE系统能够自动分析每日新闻，生成简明、可用于决策的每周动态洞察，用于芬兰一所应用科技大学案例。


<details>
  <summary>Details</summary>
Motivation: 高校和决策者需从海量资讯中获得实时、与自身相关的深度见解，支持课程及策略调整。手动处理繁杂、耗时，因此需要自动化、个性化的新闻分析工具。

Method: 1. 平台自动抓取新闻并分版本；2. 针对高校需求进行相关性过滤；3. 使用嵌入模型处理文本内容；4. 将新闻分入PESTEL六大维度进行分类；5. 构建时序递归摘要图（TRSG），通过大语言模型进行两层聚类与摘要，每周更新；6. 变化检测算法高亮新增、变动、删除的内容，并按主题聚类，方便PESTEL专题分析。

Result: 系统实现了稳定的生产部署，能定期输出高质量、主题细分的新闻摘要。展示了与大学课程智能管理结合的实际用例，并规划了系统评估方法。

Conclusion: ORACLE系统为组织提供了可扩展的自动化资讯处理方案，支持基于PESTEL模型的决策及课程设计，并在实际高校场景验证了其实用性和生产级稳定性。

Abstract: ORACLE turns daily news into week-over-week, decision-ready insights for one of the Finnish University of Applied Sciences. The platform crawls and versions news, applies University-specific relevance filtering, embeds content, classifies items into PESTEL dimensions and builds a concise Time-Dependent Recursive Summary Graph (TRSG): two clustering layers summarized by an LLM and recomputed weekly. A lightweight change detector highlights what is new, removed or changed, then groups differences into themes for PESTEL-aware analysis. We detail the pipeline, discuss concrete design choices that make the system stable in production and present a curriculum-intelligence use case with an evaluation plan.

</details>


### [118] [Toward expert-level motivational interviewing for health behavior improvement with LLMs](https://arxiv.org/abs/2512.15446)
*Run-ze Hu,Yang Yang,Yi-hang Yang,Jing-qi Kong,Jia-hui Luo,Wen-yu Yang,Jing Chen,Jing-yao Liu,Hui-qun Zeng,Lei Zhang,Zheng Liu*

Main category: cs.CL

TL;DR: 本文开发并评估了面向动机式访谈（MI）的中文大模型（MI-LLMs），提升了AI辅助健康行为改变的能力。


<details>
  <summary>Details</summary>
Motivation: 动机式访谈（MI）有助于促进健康行为改变，但受限于对高技能咨询师的需求，难以大规模推广。作者希望通过大模型提供可扩展、有效的替代方案。

Method: 作者整理了五个中文心理咨询语料库，并使用GPT-4结合MI提示词将两个高质量数据集中的对话转录为2,040段MI风格咨询对话，以2,000条用于训练、40条用于测试。对三个主流中文开源大模型进行微调，形成MI-LLMs，通过自动指标与专家人工编码（MITI 4.2.1）进行评估。

Result: 微调模型在BLEU-4和ROUGE等自动指标上显著优于基线模型，人工评估显示MI-LLMs在技术、关系评分及MI一致性方面接近真实MI对话，但复杂反思和反思/提问比仍偏低。

Conclusion: MI风格微调能让通用语言模型具备核心的MI一致咨询行为，为AI辅助健康干预提供了可扩展路径，但在数据规模、复杂技能和真实世界应用上仍需进一步研究。

Abstract: Background: Motivational interviewing (MI) is an effective counseling approach for promoting health behavior change, but its impact is constrained by the need for highly trained human counselors. Objective: This study aimed to explore a scalable alternative by developing and evaluating Large Language Models for Motivational Interviewing (MI-LLMs). Methods: We first curated five Chinese psychological counseling corpora and, using GPT-4 with an MI-informed prompt, transcribed multi-turn dialogues from the two highest-quality datasets (CPsyCounD and PsyDTCorpus) into 2,040 MI-style counseling conversations, of which 2,000 were used for training and 40 for testing. Three Chinese-capable open-source LLMs (Baichuan2-7B-Chat, ChatGLM-4-9B-Chat and Llama-3-8B-Chinese-Chat-v2) were fine-tuned on this corpus and were named as MI-LLMs. We evaluated MI-LLMs using round-based automatic metrics and expert manual coding with the Motivational Interviewing Treatment Integrity (MITI) Coding Manual 4.2.1. Results: Across all three models, fine-tuning substantially improved BLEU-4 and ROUGE scores compared with the base models, and manual coding showed that MI-LLMs achieved technical and relational global scores, and MI-adherent ratios that approached those of real MI dialogues, although complex reflections and reflection-to-question ratios remained less frequent. Conclusions: These findings provide initial evidence that MI-oriented fine-tuning can endow general-purpose LLMs with core MI-consistent counseling behaviors, suggesting a scalable pathway toward AI-assisted health behavior change support while underscoring the need for further work on data scale, complex MI skills and real-world intervention trials.

</details>


### [119] [When a Nation Speaks: Machine Learning and NLP in People's Sentiment Analysis During Bangladesh's 2024 Mass Uprising](https://arxiv.org/abs/2512.15547)
*Md. Samiul Alim,Mahir Shahriar Tamim,Maisha Rahman,Tanvir Ahmed Khan,Md Mushfique Anwar*

Main category: cs.CL

TL;DR: 本文首次针对孟加拉语在全国危机期间的情绪分析展开研究，通过Facebook新闻门户收集并标注新闻标题，利用LDA分析情感主题，并开发了专用模型，准确识别舆情动向。


<details>
  <summary>Details</summary>
Motivation: 情绪分析多用于选举、社交媒体等领域，但对民事动荡特别是孟加拉语环境中公众情绪动态的研究稀缺。

Method: 收集了2024年孟加拉群众起义期间主要Facebook新闻门户的2028条新闻标题并进行情感标注（愤怒、希望、绝望），运用LDA提炼主题，比较不同模型的效果。

Result: 专门为孟加拉语开发的模型准确率超过mBERT（67%）、XLM-RoBERTa（71%）及传统机器学习模型（SVM和LR均为70%）。

Conclusion: 本研究证明了语言专属模型在动态社会危机中的有效性，为政治动荡时期舆情感知和社会分析提供了重要参考。

Abstract: Sentiment analysis, an emerging research area within natural language processing (NLP), has primarily been explored in contexts like elections and social media trends, but there remains a significant gap in understanding emotional dynamics during civil unrest, particularly in the Bangla language. Our study pioneers sentiment analysis in Bangla during a national crisis by examining public emotions amid Bangladesh's 2024 mass uprising. We curated a unique dataset of 2,028 annotated news headlines from major Facebook news portals, classifying them into Outrage, Hope, and Despair. Through Latent Dirichlet Allocation (LDA), we identified prevalent themes like political corruption and public protests, and analyzed how events such as internet blackouts shaped sentiment patterns. It outperformed multilingual transformers (mBERT: 67%, XLM-RoBERTa: 71%) and traditional machine learning methods (SVM and Logistic Regression: both 70%). These results highlight the effectiveness of language-specific models and offer valuable insights into public sentiment during political turmoil.

</details>


### [120] [CTkvr: KV Cache Retrieval for Long-Context LLMs via Centroid then Token Indexing](https://arxiv.org/abs/2512.15550)
*Kuan Lu,Shuhang Lin,Sai Wu,Yichen Yao,Junhan Yang,Huan Li,Wei Chu,Xu Yinghui,Yuan Qi,Gang Chen*

Main category: cs.CL

TL;DR: 本文提出了一种用于大语言模型（LLMs）长上下文场景下的高效KV Cache检索方法CTKVR，实现高效内存管理和推理加速，平均几乎不损失精度。


<details>
  <summary>Details</summary>
Motivation: 多轮对话等长上下文应用使LLMs的KV cache导致推理时内存消耗大和延迟高。现有方法在准确率和效率之间存在权衡。作者旨在同时提升KV检索效率和精度，解决现有动态KV选择的不足。

Method: 作者观察到经过旋转位置编码（RoPE）后，相邻Query向量高度相似，检索出的top-k KV条目大多重叠。基于此，提出CTKVR方法：第一步使用轻量级的质心（centroid）进行索引和粗筛，第二步针对具体token精细检索，实现分级加速。为进一步提升性能，作者在CPU-GPU协同架构下优化了索引构建和搜索流程。

Result: CTKVR在多个基准测试中，准确率损失小于1%，在Llama-3-8B和Yi-9B等模型、96K长上下文下，达到了3到4倍的推理吞吐率提升，且适用于多种GPU平台。

Conclusion: CTKVR兼顾了KV检索的高效率和准确性，大幅降低了长上下文推理的计算和内存成本，为长文本场景下大语言模型应用提供了实用方案。

Abstract: Large language models (LLMs) are increasingly applied in long-context scenarios such as multi-turn conversations. However, long contexts pose significant challenges for inference efficiency, including high memory overhead from Key-Value (KV) cache and increased latency due to excessive memory accesses. Recent methods for dynamic KV selection struggle with trade-offs: block-level indexing degrades accuracy by retrieving irrelevant KV entries, while token-level indexing incurs high latency from inefficient retrieval mechanisms. In this paper, we propose CTKVR, a novel centroid-then-token KV retrieval scheme that addresses these limitations. CTKVR leverages a key observation: query vectors adjacent in position exhibit high similarity after Rotary Position Embedding (RoPE) and share most of their top-k KV cache entries. Based on this insight, CTKVR employs a two-stage retrieval strategy: lightweight centroids are precomputed during prefilling for centroid-grained indexing, followed by token-level refinement for precise KV retrieval. This approach balances retrieval efficiency and accuracy. To further enhance performance, we implement an optimized system for indexing construction and search using CPU-GPU co-execution. Experimentally, CTKVR achieves superior performance across multiple benchmarks with less than 1% accuracy degradation. Meanwhile, CTKVR delivers 3 times and 4 times throughput speedups on Llama-3-8B and Yi-9B at 96K context length across diverse GPU hardware.

</details>


### [121] [Learning inflection classes using Adaptive Resonance Theory](https://arxiv.org/abs/2512.15551)
*Peter Dekker,Heikki Rasilo,Bart de Boer*

Main category: cs.CL

TL;DR: 本文利用自适应共振理论（ART）无监督聚类模型，对拉丁语、葡萄牙语和爱沙尼亚语动词屈折类的可学习性进行建模分析，结果显示模型能以相对高的准确度还原实际屈折类别，并揭示了参数调整对表现的影响。


<details>
  <summary>Details</summary>
Motivation: 屈折类别概念帮助语言学家描述并推断词形变化，了解个体如何内化和辨析未知屈折形式对理解形态习得至关重要，因此探究这些类别的可学习性和认知合理性很有意义。

Method: 采用自适应共振理论（ART）神经网络，通过控制泛化度参数（警戒度）对拉丁语、葡萄牙语和爱沙尼亚语的词汇进行无监督聚类实验，并与真实屈折类别进行比对。

Result: 聚类结果与真实屈折类别的一致性因语言屈折复杂性而异，在警戒度参数的最优范围内模型表现最佳。模型提取的特征与语言学已知屈折类别特征具有一定相似性。

Conclusion: 该ART模型能够以认知合理的方式模拟屈折类别的学习过程，对不同语言有效。未来可将其融入基于代理的建模中，进一步研究屈折类别的变化。

Abstract: The concept of inflection classes is an abstraction used by linguists, and provides a means to describe patterns in languages that give an analogical base for deducing previously unencountered forms. This ability is an important part of morphological acquisition and processing. We study the learnability of a system of verbal inflection classes by the individual language user by performing unsupervised clustering of lexemes into inflection classes. As a cognitively plausible and interpretable computational model, we use Adaptive Resonance Theory, a neural network with a parameter that determines the degree of generalisation (vigilance). The model is applied to Latin, Portuguese and Estonian. The similarity of clustering to attested inflection classes varies depending on the complexity of the inflectional system. We find the best performance in a narrow region of the generalisation parameter. The learned features extracted from the model show similarity with linguistic descriptions of the inflection classes. The proposed model could be used to study change in inflection classes in the future, by including it in an agent-based model.

</details>


### [122] [From Data to Dialogue: Unlocking Language for All](https://arxiv.org/abs/2512.15552)
*Dakota Ellis,Samy Bakikerali,Wanshan Chen,Bao Dinh,Uyen Le*

Main category: cs.CL

TL;DR: 本文提出一种全自动化、客观的方法来构建专业化词汇表（SWL），并证明该方法能以更少的词覆盖更多的语料，相比当前业界标准更高效地提升语言学习效率。


<details>
  <summary>Details</summary>
Motivation: 传统的通用词表（如GSL）的构建费时且主观性较强，满足不了不同需求下语言学习者的精准需求，所以需要一种既自动化又客观的方法提升词汇表构建效率和适用性。

Method: 作者通过开发模型，自动基于客观标准，从特定子语料库中生成专业化词汇表（SWL），并将其与业界标准（NGSL）进行对比，检验其在词汇覆盖率和学习效率方面的表现。

Result: 基于模型生成的SWL在词汇覆盖率上优于NGSL，即用更少的单词达到理解语料95%的标准。

Conclusion: SWL方法既高效、自动又可客观量化，能够适应全球不同语言学习者定制需求，为词汇学习优化提供了可推广的解决方案。

Abstract: Traditional linguists have proposed the use of a General Service List (GSL) to assist new language learners in identifying the most important words in English. This process requires linguistic expertise, subjective input, and a considerable amount of time. We attempt to create our own GSL and evaluate its practicality against the industry standard (The NGSL). We found creating a Specialized Word List (SWL), or a word list specific to a subset of the overall corpus, to be the most practical way for language-learners to optimize the process. The SWL's that we created using our model outperformed the industry standard, reaching the 95% coverage required for language comprehension with fewer words comparatively. By restricting the SWL process to objective criteria only, it can be automated, scaled, and tailored to the needs of language-learners across the globe.

</details>


### [123] [An Empirical Study on Chinese Character Decomposition in Multiword Expression-Aware Neural Machine Translation](https://arxiv.org/abs/2512.15556)
*Lifeng Han,Gareth J. F. Jones,Alan F. Smeaton*

Main category: cs.CL

TL;DR: 本论文主要关注中文多词表达（MWE）对自然语言任务造成的挑战，并系统研究了汉字分解技术在神经机器翻译中帮助理解和翻译MWE的方法与效果。


<details>
  <summary>Details</summary>
Motivation: 由于多词表达（MWE）会引入歧义、成语和罕见用法，严重影响自然语言理解和翻译，已经有大量针对西方语言（特别是英语）的研究，但中文和相关亚洲语言的研究相对滞后，且汉字的表意特性使西方有效的分词建模方法（如BPE）无法直接适用，因此亟需针对中文的有效处理方法。

Method: 论文系统性地研究了汉字分解技术，探讨其在面向MWE的神经机器翻译（NMT）中的应用。通过实验分析该技术对中文单词和汉字原始含义表示的贡献，并评估其在应对MWE翻译挑战中的有效性。

Result: 实验表明，汉字分解技术提升了对中文单词和字符原始含义的表示能力，有效帮助理解和翻译MWE，改善了神经机器翻译系统对MWE的处理效果。

Conclusion: 针对中文独特的语言特性，采用汉字分解技术能够提升NMT系统对MWE的理解与翻译能力，为解决表意文字语言家族的MWE翻译难题提供了有效思路。

Abstract: Word meaning, representation, and interpretation play fundamental roles in natural language understanding (NLU), natural language processing (NLP), and natural language generation (NLG) tasks. Many of the inherent difficulties in these tasks stem from Multi-word Expressions (MWEs), which complicate the tasks by introducing ambiguity, idiomatic expressions, infrequent usage, and a wide range of variations. Significant effort and substantial progress have been made in addressing the challenging nature of MWEs in Western languages, particularly English. This progress is attributed in part to the well-established research communities and the abundant availability of computational resources. However, the same level of progress is not true for language families such as Chinese and closely related Asian languages, which continue to lag behind in this regard. While sub-word modelling has been successfully applied to many Western languages to address rare words improving phrase comprehension, and enhancing machine translation (MT) through techniques like byte-pair encoding (BPE), it cannot be applied directly to ideograph language scripts like Chinese. In this work, we conduct a systematic study of the Chinese character decomposition technology in the context of MWE-aware neural machine translation (NMT). Furthermore, we report experiments to examine how Chinese character decomposition technology contributes to the representation of the original meanings of Chinese words and characters, and how it can effectively address the challenges of translating MWEs.

</details>


### [124] [Bolmo: Byteifying the Next Generation of Language Models](https://arxiv.org/abs/2512.15586)
*Benjamin Minixhofer,Tyler Murray,Tomasz Limisiewicz,Anna Korhonen,Luke Zettlemoyer,Noah A. Smith,Edoardo M. Ponti,Luca Soldaini,Valentin Hofmann*

Main category: cs.CL

TL;DR: 本文提出了Bolmo，这是一种新的全开源字节级语言模型，在参数规模为1B和7B时具有竞争力。通过将已有的子词级模型“字节化”转化为字节级模型，Bolmo既克服了子词分词的局限，也能达到领先的性能。Bolmo在字符理解和某些编程任务上超过了原有子词模型，且推理速度有竞争力，是字节级语言模型的重大进步。


<details>
  <summary>Details</summary>
Motivation: 子词分词存在字符理解差与词汇效率受限等问题，传统字节级语言模型虽然避免了这些问题，但性能仍不如主流子词级模型。因此，作者希望通过创新方法，使字节级模型在性能、效率和实用性上都能媲美甚至超越子词级模型。

Method: 作者提出“字节化”的方案，即将现有子词级预训练语言模型转化为字节级，且Bolmo模型结构专为此过程优化。过程中采用有效的蒸馏目标，使得只需消耗1%预训练数据量即可完成转换。同时，通过更高的token压缩率提高了推理速度，也能利用子词生态中的各类后训练方法。

Result: Bolmo相比同等规模的已有字节级模型性能显著提升。在字符理解、部分代码任务上超越原始子词模型，其它任务上表现也非常接近。同时，Bolmo在推理速度上也达到了与子词级模型相当的水平。

Conclusion: Bolmo证明了字节级语言模型也能在广泛场景中表现出与子词级模型相媲美的能力，并在某些方面具备优势，推动了字节级模型的实用落地。

Abstract: We introduce Bolmo, the first family of competitive fully open byte-level language models (LMs) at the 1B and 7B parameter scales. In contrast to prior research on byte-level LMs, which focuses predominantly on training from scratch, we train Bolmo by byteifying existing subword-level LMs. Byteification enables overcoming the limitations of subword tokenization - such as insufficient character understanding and efficiency constraints due to the fixed subword vocabulary - while performing at the level of leading subword-level LMs. Bolmo is specifically designed for byteification: our architecture resolves a mismatch between the expressivity of prior byte-level architectures and subword-level LMs, which makes it possible to employ an effective exact distillation objective between Bolmo and the source subword model. This allows for converting a subword-level LM to a byte-level LM by investing less than 1\% of a typical pretraining token budget. Bolmo substantially outperforms all prior byte-level LMs of comparable size, and outperforms the source subword-level LMs on character understanding and, in some cases, coding, while coming close to matching the original LMs' performance on other tasks. Furthermore, we show that Bolmo can achieve inference speeds competitive with subword-level LMs by training with higher token compression ratios, and can be cheaply and effectively post-trained by leveraging the existing ecosystem around the source subword-level LM. Our results finally make byte-level LMs a practical choice competitive with subword-level LMs across a wide set of use cases.

</details>


### [125] [You Never Know a Person, You Only Know Their Defenses: Detecting Levels of Psychological Defense Mechanisms in Supportive Conversations](https://arxiv.org/abs/2512.15601)
*Hongbin Na,Zimu Wang,Zhaoming Chen,Peilin Zhou,Yining Hua,Grace Ziqi Zhou,Haiyang Zhang,Tao Shen,Wei Wang,John Torous,Shaoxiong Ji,Ling Chen*

Main category: cs.CL

TL;DR: 本研究构建了一个用于测量心理防御机制的新对话语料库，并提出了辅助标注工具，有助于更好地在临床对话中分析防御反应。


<details>
  <summary>Details</summary>
Motivation: 心理防御机制在人类应对心理压力时起到重要作用，但其衡量尤其在真实临床对话中的标注非常复杂且难以标准化。当前缺乏高质量带标注的对话数据集和高效的标注工具，以支持心理防御机制在自然语言中的研究。

Method: 提出PsyDefConv对话语料库，标注了求助者语句的防御水平。同时设计了一个四阶段的标注辅助工具（DMRS Co-Pilot）提升标注效率，并通过专家盲评和自动化基线系统验证工具的表现。对众多当前强大的语言模型进行了基准测试，分析了它们在新任务中的表现和偏差。

Result: 建立了包含200个对话、4709句发言（其中求助者2336）的新型语料库，标注一致性达到Cohen's kappa 0.639。辅助工具能使标注时间缩短22.4%。专家评价显示工具在循证、临床可信度和洞见方面得分较高。语言模型基准测试发现宏平均F1约30%，且易高估成熟型防御。语料库分析证实成熟型防御最常见，情感类别存在特定偏差。

Conclusion: 该工作丰富了心理防御机制在自然语言中的研究资源，通过数据和工具的开源有助于推动相关研究的发展，同时为NLP和心理健康交叉领域提供了新基准和挑战。

Abstract: Psychological defenses are strategies, often automatic, that people use to manage distress. Rigid or overuse of defenses is negatively linked to mental health and shapes what speakers disclose and how they accept or resist help. However, defenses are complex and difficult to reliably measure, particularly in clinical dialogues. We introduce PsyDefConv, a dialogue corpus with help seeker utterances labeled for defense level, and DMRS Co-Pilot, a four-stage pipeline that provides evidence-based pre-annotations. The corpus contains 200 dialogues and 4709 utterances, including 2336 help seeker turns, with labeling and Cohen's kappa 0.639. In a counterbalanced study, the co-pilot reduced average annotation time by 22.4%. In expert review, it averaged 4.62 for evidence, 4.44 for clinical plausibility, and 4.40 for insight on a seven-point scale. Benchmarks with strong language models in zero-shot and fine-tuning settings demonstrate clear headroom, with the best macro F1-score around 30% and a tendency to overpredict mature defenses. Corpus analyses confirm that mature defenses are most common and reveal emotion-specific deviations. We will release the corpus, annotations, code, and prompts to support research on defensive functioning in language.

</details>


### [126] [Evaluating Metrics for Safety with LLM-as-Judges](https://arxiv.org/abs/2512.15617)
*Kester Clegg,Richard Hawkins,Ibrahim Habli,Tom Lawton*

Main category: cs.CL

TL;DR: 本文讨论了在关键任务中引入大型语言模型（LLMs）可能带来的安全性挑战，并提出通过多维度、加权评估指标提高LLM评估的可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM广泛应用于文本处理流程，许多原由人为把控的关键信息流（如医疗护理分流、核设施排班等）有望被自动化提升效率，但考虑到LLM可能出错，特别是在安全相关场景，因此需要方法提升其安全性和可靠性。

Method: 作者未采用现有的生成式或图论技术作为重点，而是建议在评估框架上发力，特别针对当下流行的LLM-as-Judges（LaJ）评估方式，提出采用多种加权指标体系，对评估过程中的风险进行量化，并针对不同上下文灵活定义错误严重性，以及设定置信度阈值以便在评估者间一致性不足时引入人工复核。

Result: 通过理论探讨，认为结合多样指标和灵活设计阈值，可为关键任务中LLM的应用建立更安全可靠的评估机制，有潜力降低关键任务中的自动化风险。

Conclusion: LLM在关键信息流中的应用需要注重评估证据类型，通过合理的加权多指标评估体系和人工介入机制，可以提高LLM评判的安全性和可信度，减少出错风险。

Abstract: LLMs (Large Language Models) are increasingly used in text processing pipelines to intelligently respond to a variety of inputs and generation tasks. This raises the possibility of replacing human roles that bottleneck existing information flows, either due to insufficient staff or process complexity. However, LLMs make mistakes and some processing roles are safety critical. For example, triaging post-operative care to patients based on hospital referral letters, or updating site access schedules in nuclear facilities for work crews. If we want to introduce LLMs into critical information flows that were previously performed by humans, how can we make them safe and reliable? Rather than make performative claims about augmented generation frameworks or graph-based techniques, this paper argues that the safety argument should focus on the type of evidence we get from evaluation points in LLM processes, particularly in frameworks that employ LLM-as-Judges (LaJ) evaluators. This paper argues that although we cannot get deterministic evaluations from many natural language processing tasks, by adopting a basket of weighted metrics it may be possible to lower the risk of errors within an evaluation, use context sensitivity to define error severity and design confidence thresholds that trigger human review of critical LaJ judgments when concordance across evaluators is low.

</details>


### [127] [How Much is Too Much? Exploring LoRA Rank Trade-offs for Retaining Knowledge and Domain Robustness](https://arxiv.org/abs/2512.15634)
*Darshita Rathore,Vineet Kumar,Chetna Bansal,Anindya Moitra*

Main category: cs.CL

TL;DR: 本文系统评估了全参数微调（SFT）和参数高效微调（PEFT，特别是LoRA）在下游问答与泛化任务中的表现，揭示不同设置下的性能权衡及内部结构变化。


<details>
  <summary>Details</summary>
Motivation: 尽管PEFT方法如LoRA因效率高而被广泛采用，但其参数配置（如秩）对下游任务表现和泛化能力的影响尚未被深入研究。

Method: 作者在多个推理和回忆数据集上，对SFT和PEFT（LoRA）进行横向评测，系统改变LoRA的秩参数（rank sweep），并考查在域内与跨域适应下的准确率及遗忘现象。同时，通过光谱特征和分层注意力结构分析内部表征的变化。

Result: 在特定秩值下，LoRA在推理任务中达到或优于SFT的表现，尤其在一些任务上表现突出。不同方法在域内与跨域的泛化和遗忘特性存在显著差异。

Conclusion: LoRA等PEFT方法不仅具备参数和计算高效的优势，还能在适当配置下在某些任务上超越传统SFT，对模型选择和调优具有重要参考价值。

Abstract: Large language models are increasingly adapted to downstream tasks through fine-tuning. Full supervised fine-tuning (SFT) and parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), are two dominant approaches. While PEFT methods are widely used for their computational efficiency, the implications of their configurations (e.g., rank) remain under-explored in downstream Q&A tasks and generalisation. In this work, we perform a comprehensive evaluation across multiple reasoning and recall datasets, conducting a rank sweep to quantify the trade-off between SFT and PEFT. We also compare the accuracy of PEFT and SFT models across in-domain and out-of-domain adaptation, highlighting distinct generalisation behaviour and task-specific forgetting. We demonstrate that LoRA achieves competitive and in some cases superior performance compared to SFT, particularly on reasoning tasks at specific rank values. Additionally, we analyze the internal representations via spectral features and layer-wise attention structures, offering insights into representational drift and structural changes in attention patterns.

</details>


### [128] [Characterizing Mamba's Selective Memory using Auto-Encoders](https://arxiv.org/abs/2512.15653)
*Tamanna Hossain,Robert L. Logan,Ganesh Jagadeesan,Sameer Singh,Joel Tetreault,Alejandro Jaimes*

Main category: cs.CL

TL;DR: 本文通过研究状态空间模型（SSM）在处理长序列时，容易遗忘哪些类型的信息，发现数学相关词汇、组织机构实体、以及标准美式英语以外的方言更易丢失。


<details>
  <summary>Details</summary>
Motivation: SSM作为Transformer的替代方案在推理时内存需求固定，但为此在处理长序列时信息会丢失。此前相关工作仅关注信息丢失发生的序列长度，尚未细致探究具体遗忘的内容类型。本文正是为填补这一知识空白，深入分析SSM在文本建模中遗忘内容的具体类型。

Method: 研究方法为：对SSM隐状态训练自编码器以重构原始序列，通过比较原输入与重构结果，定量测量信息损失。以Mamba系列SSM模型（130M到1.4B参数规模）在长度为4到256的序列上开展实验，并统计不同内容类型的遗忘规律。

Result: 实验证明，SSM在数字、变量等数学相关词、组织机构类命名实体、以及非标准美式英语方言上的信息丢失率显著高。进一步分析发现，训练数据中出现频率较低的token更容易被遗忘。

Conclusion: 本研究揭示了SSM遗忘内容的类型和原因，对未来提升SSM模型保持重要信息能力、优化预训练token分布具有明确启示。

Abstract: State space models (SSMs) are a promising alternative to transformers for language modeling because they use fixed memory during inference. However, this fixed memory usage requires some information loss in the hidden state when processing long sequences. While prior work has studied the sequence length at which this information loss occurs, it does not characterize the types of information SSM language models (LMs) tend to forget. In this paper, we address this knowledge gap by identifying the types of tokens (e.g., parts of speech, named entities) and sequences (e.g., code, math problems) that are more frequently forgotten by SSM LMs. We achieve this by training an auto-encoder to reconstruct sequences from the SSM's hidden state, and measure information loss by comparing inputs with their reconstructions. We perform experiments using the Mamba family of SSM LMs (130M--1.4B) on sequences ranging from 4--256 tokens. Our results show significantly higher rates of information loss on math-related tokens (e.g., numbers, variables), mentions of organization entities, and alternative dialects to Standard American English. We then examine the frequency that these tokens appear in Mamba's pretraining data and find that less prevalent tokens tend to be the ones Mamba is most likely to forget. By identifying these patterns, our work provides clear direction for future research to develop methods that better control Mamba's ability to retain important information.

</details>


### [129] [PPSEBM: An Energy-Based Model with Progressive Parameter Selection for Continual Learning](https://arxiv.org/abs/2512.15658)
*Xiaodi Li,Dingcheng Li,Rujun Gao,Mahmoud Zamani,Feng Mi,Latifur Khan*

Main category: cs.CL

TL;DR: 论文提出了一种名为PPSEBM的新框架，通过结合能量模型（EBM）与渐进式参数选择（PPS）来解决连续学习中的灾难性遗忘问题，实验结果优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 连续学习要求模型能在不断变化的任务流中学习而不丢失已有知识，但灾难性遗忘使得模型对早期任务的表现变差。现有方法难以有效平衡新旧知识的保留与适应，因此需要创新性的解决方案。

Method: PPSEBM由两部分组合：渐进式参数选择为每个新任务分配任务特定的参数，避免参数冲突；能量模型用于生成旧任务的代表性伪样本，这些样本反向指导新参数的选择，从而保护旧知识。

Result: 在多个NLP基准数据集上，PPSEBM取得了优于最新连续学习方法的表现，能更好地缓解灾难性遗忘。

Conclusion: PPSEBM框架为NLP中的连续学习任务提供了一种鲁棒、有效的解决思路，在减缓灾难性遗忘问题上具有显著优势。

Abstract: Continual learning remains a fundamental challenge in machine learning, requiring models to learn from a stream of tasks without forgetting previously acquired knowledge. A major obstacle in this setting is catastrophic forgetting, where performance on earlier tasks degrades as new tasks are learned. In this paper, we introduce PPSEBM, a novel framework that integrates an Energy-Based Model (EBM) with Progressive Parameter Selection (PPS) to effectively address catastrophic forgetting in continual learning for natural language processing tasks. In PPSEBM, progressive parameter selection allocates distinct, task-specific parameters for each new task, while the EBM generates representative pseudo-samples from prior tasks. These generated samples actively inform and guide the parameter selection process, enhancing the model's ability to retain past knowledge while adapting to new tasks. Experimental results on diverse NLP benchmarks demonstrate that PPSEBM outperforms state-of-the-art continual learning methods, offering a promising and robust solution to mitigate catastrophic forgetting.

</details>


### [130] [Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers](https://arxiv.org/abs/2512.15674)
*Adam Karvonen,James Chua,Clément Dumas,Kit Fraser-Taliente,Subhash Kantamneni,Julian Minder,Euan Ong,Arnab Sen Sharma,Daniel Wen,Owain Evans,Samuel Marks*

Main category: cs.CL

TL;DR: 本文提出并评估了一种通过自然语言查询 LLM 内部激活状态的新方法，并证明其在多任务下的通用性和优越性能。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型（LLM）内部激活机制非常困难，现有方法多为复杂、专用化手段，难以普适应用。本研究旨在探索更简单和通用的解释方法。

Method: 采用 LatentQA 的方法，训练 LLM 直接以激活作为输入，并用自然语言回答关于激活内容的问题。扩展训练数据类型和范围，检验模型在远离分布的数据上的泛化能力，并与现有白盒、黑盒方法在多任务上做对比评估。

Result: 发现这些被称为 Activation Oracles（AOs）的模型能够恢复模型内部信息（如微调融入的知识），即使这些信息未在输入文本或训练过程中出现。此外，增加多样化任务训练集后，性能进一步提升。在四项评测任务中，AOs 在三项中表现最佳，其余一项与最优方法持平。

Conclusion: 以自然语言查询经多样任务训练的 AOs，对 LLM 激活状态的解释具有很强通用性和效果，有望成为理解 LLM 内部机制的有效通用方法。

Abstract: Large language model (LLM) activations are notoriously difficult to understand, with most existing techniques using complex, specialized methods for interpreting them. Recent work has proposed a simpler approach known as LatentQA: training LLMs to directly accept LLM activations as inputs and answer arbitrary questions about them in natural language. However, prior work has focused on narrow task settings for both training and evaluation. In this paper, we instead take a generalist perspective. We evaluate LatentQA-trained models, which we call Activation Oracles (AOs), in far out-of-distribution settings and examine how performance scales with training data diversity. We find that AOs can recover information fine-tuned into a model (e.g., biographical knowledge or malign propensities) that does not appear in the input text, despite never being trained with activations from a fine-tuned model. Our main evaluations are four downstream tasks where we can compare to prior white- and black-box techniques. We find that even narrowly-trained LatentQA models can generalize well, and that adding additional training datasets (such as classification tasks and a self-supervised context prediction task) yields consistent further improvements. Overall, our best AOs match or exceed prior white-box baselines on all four tasks and are the best method on 3 out of 4. These results suggest that diversified training to answer natural-language queries imparts a general capability to verbalize information about LLM activations.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [131] [Breathe with Me: Synchronizing Biosignals for User Embodiment in Robots](https://arxiv.org/abs/2512.14952)
*Iddo Yehoshua Wald,Amber Maimon,Shiyao Zhang,Dennis Küster,Robert Porzel,Tanja Schultz,Rainer Malaka*

Main category: cs.RO

TL;DR: 研究考察了在机器人系统中将用户呼吸与机器人运动同步，提升用户对机器人的身体归属感和控制感，结果发现呼吸同步可有效增强用户体验。


<details>
  <summary>Details</summary>
Motivation: 以往的人机交互多以视觉、动作等外部感知为主，较少关注生理信号（如呼吸）对用户与机器人的结合体验。本研究动机是探究通过“embreathment”——实时体现用户呼吸——是否能增强机器人系统中的用户身体感知体验。

Method: 采用被试内实验设计，让参与者通过两种模式（与呼吸同步与非同步）操控机器人手臂，比较两种情况下的身体归属感和偏好。

Result: 结果发现呼吸同步显著提升了用户的身体归属感，大部分参与者更偏好呼吸同步的体验。

Conclusion: 生理信号（如呼吸）作为一种新的体内感受途径能显著增强人机互动的身体感和代理感，对远程呈现、义肢和人机协作等场景具有潜力和应用前景。

Abstract: Embodiment of users within robotic systems has been explored in human-robot interaction, most often in telepresence and teleoperation. In these applications, synchronized visuomotor feedback can evoke a sense of body ownership and agency, contributing to the experience of embodiment. We extend this work by employing embreathment, the representation of the user's own breath in real time, as a means for enhancing user embodiment experience in robots. In a within-subjects experiment, participants controlled a robotic arm, while its movements were either synchronized or non-synchronized with their own breath. Synchrony was shown to significantly increase body ownership, and was preferred by most participants. We propose the representation of physiological signals as a novel interoceptive pathway for human-robot interaction, and discuss implications for telepresence, prosthetics, collaboration with robots, and shared autonomy.

</details>


### [132] [ISS Policy : Scalable Diffusion Policy with Implicit Scene Supervision](https://arxiv.org/abs/2512.15020)
*Wenlong Xia,Jinhao Zhang,Ce Zhang,Yaojia Wang,Youmin Gong,Jie Mei*

Main category: cs.RO

TL;DR: 提出一种基于3D点云的视觉-动作政策（ISS Policy），通过隐式场景监督提升机器人操作学习的泛化性和效率，并在多项任务中取得最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模仿学习方法过度依赖物体表观，忽视三维结构，导致训练效率低和泛化能力差。作者希望解决这一问题。

Method: 提出Implicit Scene Supervision（ISS）Policy，它基于DiT扩展，加入了隐式场景监督模块。该模块使输出与场景几何演变一致，从点云观测直接预测连续动作序列。

Result: ISS Policy在单臂操作（MetaWorld）和灵巧手操作（Adroit）任务中取得了最先进的表现，在真实机器人实验中表现出较强的泛化和鲁棒性。消融实验表明方法可随数据和参数扩展。

Conclusion: 通过引入3D结构的隐式监督，有效提升了视觉模仿学习在机器人操作领域的效率、泛化能力和鲁棒性。

Abstract: Vision-based imitation learning has enabled impressive robotic manipulation skills, but its reliance on object appearance while ignoring the underlying 3D scene structure leads to low training efficiency and poor generalization. To address these challenges, we introduce \emph{Implicit Scene Supervision (ISS) Policy}, a 3D visuomotor DiT-based diffusion policy that predicts sequences of continuous actions from point cloud observations. We extend DiT with a novel implicit scene supervision module that encourages the model to produce outputs consistent with the scene's geometric evolution, thereby improving the performance and robustness of the policy. Notably, ISS Policy achieves state-of-the-art performance on both single-arm manipulation tasks (MetaWorld) and dexterous hand manipulation (Adroit). In real-world experiments, it also demonstrates strong generalization and robustness. Additional ablation studies show that our method scales effectively with both data and parameters. Code and videos will be released.

</details>


### [133] [HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles](https://arxiv.org/abs/2512.15047)
*Yunheng Wang,Yixiao Feng,Yuetong Fang,Shuning Zhang,Tan Jing,Jian Li,Xiangrui Jiang,Renjing Xu*

Main category: cs.RO

TL;DR: 本文提出了一种新的3D场景图(Hierarchical Traversable 3DSG，HERO)方法，将可操作障碍物建模为可通行路径，以克服传统静态可通行性假设的问题，大幅提升机器人在复杂环境中的导航效率与覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景图在导航中只根据静态空间定义可通行区域，无法处理实际环境中可操作障碍物的问题，导致机器人在复杂环境下的可达性和效率受限。

Method: 提出HERO框架，通过将可操作障碍物（如门等）建模为可通路径，综合其物理交互性、功能语义和场景结构层级，动态构建分层遍历的3D场景图，更好地反映真实环境中的可通行性。

Result: HERO在部分阻塞环境下将路径长度（PL）减少35.1%，在完全阻塞环境下将成功率（SR）提升79.4%，显著提高了导航效率与可达性。

Conclusion: HERO通过重新定义可通行性，增强了3D场景图的表达能力，使智能体能够更高效、灵活地在复杂动态环境中导航，具有更强的实际应用潜力。

Abstract: 3D Scene Graphs (3DSGs) constitute a powerful representation of the physical world, distinguished by their abilities to explicitly model the complex spatial, semantic, and functional relationships between entities, rendering a foundational understanding that enables agents to interact intelligently with their environment and execute versatile behaviors. Embodied navigation, as a crucial component of such capabilities, leverages the compact and expressive nature of 3DSGs to enable long-horizon reasoning and planning in complex, large-scale environments. However, prior works rely on a static-world assumption, defining traversable space solely based on static spatial layouts and thereby treating interactable obstacles as non-traversable. This fundamental limitation severely undermines their effectiveness in real-world scenarios, leading to limited reachability, low efficiency, and inferior extensibility. To address these issues, we propose HERO, a novel framework for constructing Hierarchical Traversable 3DSGs, that redefines traversability by modeling operable obstacles as pathways, capturing their physical interactivity, functional semantics, and the scene's relational hierarchy. The results show that, relative to its baseline, HERO reduces PL by 35.1% in partially obstructed environments and increases SR by 79.4% in fully obstructed ones, demonstrating substantially higher efficiency and reachability.

</details>


### [134] [NAP3D: NeRF Assisted 3D-3D Pose Alignment for Autonomous Vehicles](https://arxiv.org/abs/2512.15080)
*Gaurav Bansal*

Main category: cs.RO

TL;DR: 本文提出了一种新的姿态估计方法NAP3D，结合了深度图与预训练NeRF的三维点对齐，即使未回到已知场景也能修正定位误差，提升了三维空间中的对齐精度。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶等自主系统需要长期高精度定位，但传感器噪声和漂移会导致累积误差。现有基于视觉的闭环检测主要依赖回到已知位置，且通常需多传感器融合。如何在无法回访场景时持续校正位姿，是亟需解决的问题。

Method: 提出NeRF-Assisted 3D-3D Pose Alignment (NAP3D)，通过将当前深度图采样得到的3D点与预训练NeRF合成的3D点进行直接空间对齐，无需依赖场景重复观测或回环检测，改进了姿态估计流程。

Result: 在自建数据集上，NAP3D可将相机位姿误差修正至5厘米以内，显著优于基线的2D-3D PnP方法。在公共TUM RGB-D数据集上，在不同噪声条件下，NAP3D的三维空间对齐结果RMSE低约6厘米，表现出更好的几何一致性。

Conclusion: NAP3D无需传统闭环检测即可显著提升定位与三维对齐精度，是对现有SLAM/定位方案的轻量级、数据集无关补充工具，尤其适用于无法进行传统回环的场景。

Abstract: Accurate localization is essential for autonomous vehicles, yet sensor noise and drift over time can lead to significant pose estimation errors, particularly in long-horizon environments. A common strategy for correcting accumulated error is visual loop closure in SLAM, which adjusts the pose graph when the agent revisits previously mapped locations. These techniques typically rely on identifying visual mappings between the current view and previously observed scenes and often require fusing data from multiple sensors.
  In contrast, this work introduces NeRF-Assisted 3D-3D Pose Alignment (NAP3D), a complementary approach that leverages 3D-3D correspondences between the agent's current depth image and a pre-trained Neural Radiance Field (NeRF). By directly aligning 3D points from the observed scene with synthesized points from the NeRF, NAP3D refines the estimated pose even from novel viewpoints, without relying on revisiting previously observed locations.
  This robust 3D-3D formulation provides advantages over conventional 2D-3D localization methods while remaining comparable in accuracy and applicability. Experiments demonstrate that NAP3D achieves camera pose correction within 5 cm on a custom dataset, robustly outperforming a 2D-3D Perspective-N-Point baseline. On TUM RGB-D, NAP3D consistently improves 3D alignment RMSE by approximately 6 cm compared to this baseline given varying noise, despite PnP achieving lower raw rotation and translation parameter error in some regimes, highlighting NAP3D's improved geometric consistency in 3D space. By providing a lightweight, dataset-agnostic tool, NAP3D complements existing SLAM and localization pipelines when traditional loop closure is unavailable.

</details>


### [135] [BEV-Patch-PF: Particle Filtering with BEV-Aerial Feature Matching for Off-Road Geo-Localization](https://arxiv.org/abs/2512.15111)
*Dongmyeong Lee,Jesse Quattrociocchi,Christian Ellis,Rwik Rana,Amanda Adkins,Adam Uccello,Garrett Warnell,Joydeep Biswas*

Main category: cs.RO

TL;DR: 提出了一种新的GPS无依赖定位系统BEV-Patch-PF，通过结合粒子滤波方法和学得的鸟瞰视角（BEV）以及航拍特征图，从RGB和深度图像实现机器人高精度实时定位。


<details>
  <summary>Details</summary>
Motivation: 现有的地理定位系统大多依赖GPS信号，但在树荫稠密、阴影覆盖等环境下，GPS信号容易失效。因此，需要发展新的、无需GPS的定位方法来实现机器人在各种复杂环境下的高精度定位。

Method: 方法利用机器人本体的RGB与深度图像生成鸟瞰视野特征图，然后对每个粒子假设的三自由度位置，从本地航拍图上裁剪出对应的特征patch。通过比对BEV特征和航拍patch特征，为每个粒子计算对数似然，最后完成定位。该方法在NVIDIA Tesla T4上可实时运行。

Result: 在两个真实世界的复杂越野数据集上，BEV-Patch-PF在已知和未知路线上的绝对轨迹误差分别比检索基线方法低7.5倍和7倍，且即使在树荫和阴影覆盖条件下也能保持定位精度。

Conclusion: BEV-Patch-PF为机器人提供了一种无需GPS、兼具高精度和实时性的定位解决方案，特别适用于信号受限或极为复杂的环境，有望推动无人系统的实际部署和应用。

Abstract: We propose BEV-Patch-PF, a GPS-free sequential geo-localization system that integrates a particle filter with learned bird's-eye-view (BEV) and aerial feature maps. From onboard RGB and depth images, we construct a BEV feature map. For each 3-DoF particle pose hypothesis, we crop the corresponding patch from an aerial feature map computed from a local aerial image queried around the approximate location. BEV-Patch-PF computes a per-particle log-likelihood by matching the BEV feature to the aerial patch feature. On two real-world off-road datasets, our method achieves 7.5x lower absolute trajectory error (ATE) on seen routes and 7.0x lower ATE on unseen routes than a retrieval-based baseline, while maintaining accuracy under dense canopy and shadow. The system runs in real time at 10 Hz on an NVIDIA Tesla T4, enabling practical robot deployment.

</details>


### [136] [EPSM: A Novel Metric to Evaluate the Safety of Environmental Perception in Autonomous Driving](https://arxiv.org/abs/2512.15195)
*Jörg Gamerdinger,Sven Teufel,Stephan Amann,Lukas Marc Listl,Oliver Bringmann*

Main category: cs.RO

TL;DR: 本论文提出了一种新的安全性评估指标，能够综合评估智能驾驶车辆的感知系统在物体和车道检测中的安全表现，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的感知系统评估指标（如precision、recall、F1-score）只反映了总体检测准确性，未能关注安全相关的感知失误，这可能导致高分系统依然存在严重安全隐患。因此，迫切需要能够反映安全性的评估方法。

Method: 提出了一种统一的安全指标框架，包括一个用于物体检测误差的轻量级安全性指标、一个车道检测安全性指标，并考虑了两者任务间的相互依赖。综合两个指标形成单一、可解释的安全评估分数。方法在DeepAccident公开数据集上进行了验证。

Result: 实验表明，所提安全性指标能发现传统评估方法无法捕捉的重要安全性感知错误。

Conclusion: 安全导向的感知系统评估方法对于提升自动驾驶安全性至关重要，本文所提方法能够有效补充和完善现有的感知系统性能评估体系。

Abstract: Extensive evaluation of perception systems is crucial for ensuring the safety of intelligent vehicles in complex driving scenarios. Conventional performance metrics such as precision, recall and the F1-score assess the overall detection accuracy, but they do not consider the safety-relevant aspects of perception. Consequently, perception systems that achieve high scores in these metrics may still cause misdetections that could lead to severe accidents. Therefore, it is important to evaluate not only the overall performance of perception systems, but also their safety. We therefore introduce a novel safety metric for jointly evaluating the most critical perception tasks, object and lane detection. Our proposed framework integrates a new, lightweight object safety metric that quantifies the potential risk associated with object detection errors, as well as an lane safety metric including the interdependence between both tasks that can occur in safety evaluation. The resulting combined safety score provides a unified, interpretable measure of perception safety performance. Using the DeepAccident dataset, we demonstrate that our approach identifies safety critical perception errors that conventional performance metrics fail to capture. Our findings emphasize the importance of safety-centric evaluation methods for perception systems in autonomous driving.

</details>


### [137] [Infrastructure-based Autonomous Mobile Robots for Internal Logistics -- Challenges and Future Perspectives](https://arxiv.org/abs/2512.15215)
*Erik Brorsson,Kristian Ceder,Ze Zhang,Sabino Francesco Roselli,Endre Erős,Martin Dahl,Beatrice Alenljung,Jessica Lindblom,Thanh Bui,Emmanuel Dean,Lennart Svensson,Kristofer Bengtsson,Per-Lage Götvall,Knut Åkesson*

Main category: cs.RO

TL;DR: 论文综述了基于基础设施的自主移动机器人（AMR）系统，提出了结合基础设施传感、本地云计算和车载智能的参考架构，并通过实际工业部署和用户体验评估验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 目前AMR多依靠分散化、车载智能，但在工厂等室内环境下配合外部基础设施可带来优势，相关系统尚缺乏系统性探索。

Method: 提出了一种结合基础设施传感、本地云计算和车载自主能力的参考架构，综述了定位、感知和规划等核心技术，并在重型车辆制造环境中实际部署，辅以用户体验评估。

Result: 实际部署显示该架构在复杂工业环境中具有可扩展性与稳健性，用户体验评估结果支持其人机协同友好性。

Conclusion: 本文为未来可扩展、稳健且人性化的AMR系统开发提供了系统性基础，并明确了未来的研究方向和挑战。

Abstract: The adoption of Autonomous Mobile Robots (AMRs) for internal logistics is accelerating, with most solutions emphasizing decentralized, onboard intelligence. While AMRs in indoor environments like factories can be supported by infrastructure, involving external sensors and computational resources, such systems remain underexplored in the literature. This paper presents a comprehensive overview of infrastructure-based AMR systems, outlining key opportunities and challenges. To support this, we introduce a reference architecture combining infrastructure-based sensing, on-premise cloud computing, and onboard autonomy. Based on the architecture, we review core technologies for localization, perception, and planning. We demonstrate the approach in a real-world deployment in a heavy-vehicle manufacturing environment and summarize findings from a user experience (UX) evaluation. Our aim is to provide a holistic foundation for future development of scalable, robust, and human-compatible AMR systems in complex industrial environments.

</details>


### [138] [VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments](https://arxiv.org/abs/2512.15258)
*Yuze Wu,Mo Zhu,Xingxing Li,Yuheng Du,Yuxin Fan,Wenjun Li,Xin Zhou,Fei Gao*

Main category: cs.RO

TL;DR: 本文提出了VLA-AN框架，旨在实现高效的无人机自主导航，重点解决领域差距、推理能力不足、安全性及板载部署等问题。通过数据集构建、分阶段训练、轻量化安全模块设计及深度部署优化，显著提升了无人机的智能导航性能和实际应用的可行性。


<details>
  <summary>Details</summary>
Motivation: 现有的无人机视觉-语言-动作导航模型在复杂环境中应用时，常受到数据域差异、时间推理不足、动作生成策略安全性差和硬件资源有限等局限，严重影响智能无人机的落地应用。

Method: 1. 利用3D高斯光斑技术建立高保真数据集，缩小仿真与现实域的差距；2. 采用三阶段递进式训练框架，分步强化场景理解、基础飞行技能以及复杂导航能力；3. 设计轻量级实时动作模块，并结合几何矫正以保证指令安全、避免碰撞；4. 深度优化板载部署流程，显著提升模型在资源受限无人机上的实时推理效率。

Result: VLA-AN在多项实验中表现优越，大幅提升空间理解、场景推理以及长距离导航能力，单项任务最高成功率达98.1%；在资源有限的无人机平台上，推理吞吐量提升8.3倍。

Conclusion: VLA-AN实现了高效、全链路闭环自主导航，为轻量级空中机器人在复杂场景中的实际应用提供了切实可行的解决方案。

Abstract: This paper proposes VLA-AN, an efficient and onboard Vision-Language-Action (VLA) framework dedicated to autonomous drone navigation in complex environments. VLA-AN addresses four major limitations of existing large aerial navigation models: the data domain gap, insufficient temporal navigation with reasoning, safety issues with generative action policies, and onboard deployment constraints. First, we construct a high-fidelity dataset utilizing 3D Gaussian Splatting (3D-GS) to effectively bridge the domain gap. Second, we introduce a progressive three-stage training framework that sequentially reinforces scene comprehension, core flight skills, and complex navigation capabilities. Third, we design a lightweight, real-time action module coupled with geometric safety correction. This module ensures fast, collision-free, and stable command generation, mitigating the safety risks inherent in stochastic generative policies. Finally, through deep optimization of the onboard deployment pipeline, VLA-AN achieves a robust real-time 8.3x improvement in inference throughput on resource-constrained UAVs. Extensive experiments demonstrate that VLA-AN significantly improves spatial grounding, scene reasoning, and long-horizon navigation, achieving a maximum single-task success rate of 98.1%, and providing an efficient, practical solution for realizing full-chain closed-loop autonomy in lightweight aerial robots.

</details>


### [139] [A Network-Based Framework for Modeling and Analyzing Human-Robot Coordination Strategies](https://arxiv.org/abs/2512.15282)
*Martijn IJtsma,Salvatore Hargis*

Main category: cs.RO

TL;DR: 本文提出了一种新的计算框架，结合功能建模和图论方法，来分析和设计人机协作系统中的联合工作策略，尤其适用于动态和非结构化环境。通过在灾难机器人案例中演示，该框架有助于在设计初期分析协作需求和能力。


<details>
  <summary>Details</summary>
Motivation: 随着机器人的能力增强，人机协作系统对协作能力的需求不断增长。现有的人机交互框架要么关注实时计算支持，要么依赖静态的设计表示，难以在初期设计阶段支持对协作动态的推理，因此需要新的方法。

Method: 作者提出了一个融合功能建模和图论表示的计算框架。该框架以系统功能、物理和信息结构为基础，动态展现随时间变化的协作需求。

Result: 通过灾难救援机器人系统案例，演示了如何在设计初期利用该框架分析和探索人机协作策略，及时识别和管理协作所需的关键能力。

Conclusion: 该方法能显式描绘人机协作的需求和其随时间的演变，支持在系统实现前的早期设计阶段进行协作能力和需求推理，提升了人机协作系统的设计效率和灵活性。

Abstract: Studies of human-robot interaction in dynamic and unstructured environments show that as more advanced robotic capabilities are deployed, the need for cooperative competencies to support collaboration with human problem-holders increases. Designing human-robot systems to meet these demands requires an explicit understanding of the work functions and constraints that shape the feasibility of alternative joint work strategies. Yet existing human-robot interaction frameworks either emphasize computational support for real-time execution or rely on static representations for design, offering limited support for reasoning about coordination dynamics during early-stage conceptual design. To address this gap, this article presents a novel computational framework for analyzing joint work strategies in human-robot systems by integrating techniques from functional modeling with graph-theoretic representations. The framework characterizes collective work in terms of the relationships among system functions and the physical and informational structure of the work environment, while explicitly capturing how coordination demands evolve over time. Its use during conceptual design is demonstrated through a case study in disaster robotics, which shows how the framework can be used to support early trade-space exploration of human-robot coordination strategies and to identify cooperative competencies that support flexible management of coordination overhead. These results show how the framework makes coordination demands and their temporal evolution explicit, supporting design-time reasoning about cooperative competency requirements and work demands prior to implementation.

</details>


### [140] [GuangMing-Explorer: A Four-Legged Robot Platform for Autonomous Exploration in General Environments](https://arxiv.org/abs/2512.15309)
*Kai Zhang,Shoubin Chen,Dong Li,Baiyang Zhang,Tao Huang,Zehao Wu,Jiasheng Chen,Bo Zhang*

Main category: cs.RO

TL;DR: 本文提出了GuangMing-Explorer，一个完整集成的自主探索平台，涵盖硬件与软件，并在多种真实环境下进行了实证验证，展示了其在复杂环境中的实用性和效率。


<details>
  <summary>Details</summary>
Motivation: 虽然自主探索领域的感知、规划、控制等单独环节取得进展，但缺乏对硬件与软件一体化、可实际部署的完整自主探索系统的系统性描述。

Method: 设计并实现了GuangMing-Explorer平台，从系统架构、硬件设计、软件栈、算法部署到实验配置做了全方位介绍，并进行了丰富的实地实验。

Result: 实地实验显示该平台能高效、有效地完成自主探索任务，适应复杂和非结构化环境。

Conclusion: GuangMing-Explorer作为集成自主探索平台，在复杂环境下展现出强大实用性，为自主探索系统的实际应用提供了新方案。

Abstract: Autonomous exploration is a fundamental capability that tightly integrates perception, planning, control, and motion execution. It plays a critical role in a wide range of applications, including indoor target search, mapping of extreme environments, resource exploration, etc. Despite significant progress in individual components, a holistic and practical description of a completely autonomous exploration system, encompassing both hardware and software, remains scarce. In this paper, we present GuangMing-Explorer, a fully integrated autonomous exploration platform designed for robust operation across diverse environments. We provide a comprehensive overview of the system architecture, including hardware design, software stack, algorithm deployment, and experimental configuration. Extensive real-world experiments demonstrate the platform's effectiveness and efficiency in executing autonomous exploration tasks, highlighting its potential for practical deployment in complex and unstructured environments.

</details>


### [141] [Remotely Detectable Robot Policy Watermarking](https://arxiv.org/abs/2512.15379)
*Michael Amir,Manon Flageat,Amanda Prorok*

Main category: cs.RO

TL;DR: 本文提出了一种面向机器人策略远程验证的新型水印方法，可以通过遥远观测（如摄像头视频）检测训练策略的归属权和防止未授权使用。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在机器人系统中的成功，训练得到的策略成为新的知识产权，但传统水印方法难以在仅有外部观测的情况下检测策略归属，存在‘物理观测缺口’。因此，需要新的水印技术以支持遥测场合下的检测。

Method: 提出Colored Noise Coherency（CoNoCo）水印方法，把频谱信号嵌入到机器人的运动中，利用策略本身的随机性设计，不影响机器人行为的边缘分布，仅用外部观测的信号即可检测。正式定义了‘glimpse sequence’来理论描述这一问题与方法。

Result: 实验表明CoNoCo方法在多种遥测场景下（包括动作捕捉、侧视或俯视视频）都能稳健地检测到水印，在仿真和真实机器人实验中均获得良好表现，且不会削弱机器人原有能力。

Conclusion: 本文提供了首个仅基于远程感知、非侵入式的机器人策略知识产权验证方案，迈出了物理自主系统知识产权保护的重要一步。

Abstract: The success of machine learning for real-world robotic systems has created a new form of intellectual property: the trained policy. This raises a critical need for novel methods that verify ownership and detect unauthorized, possibly unsafe misuse. While watermarking is established in other domains, physical policies present a unique challenge: remote detection. Existing methods assume access to the robot's internal state, but auditors are often limited to external observations (e.g., video footage). This ``Physical Observation Gap'' means the watermark must be detected from signals that are noisy, asynchronous, and filtered by unknown system dynamics. We formalize this challenge using the concept of a \textit{glimpse sequence}, and introduce Colored Noise Coherency (CoNoCo), the first watermarking strategy designed for remote detection. CoNoCo embeds a spectral signal into the robot's motions by leveraging the policy's inherent stochasticity. To show it does not degrade performance, we prove CoNoCo preserves the marginal action distribution. Our experiments demonstrate strong, robust detection across various remote modalities, including motion capture and side-way/top-down video footage, in both simulated and real-world robot experiments. This work provides a necessary step toward protecting intellectual property in robotics, offering the first method for validating the provenance of physical policies non-invasively, using purely remote observations.

</details>


### [142] [MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training](https://arxiv.org/abs/2512.15411)
*Zhenhan Yin,Xuanhan Wang,Jiahao Jiang,Kaiyuan Deng,Pengqi Chen,Shuangle Li,Chong Liu,Xing Xu,ingkuan Song,Lianli Gao,Heng Tao Shen*

Main category: cs.RO

TL;DR: 提出了一种通过人-机互相模仿预训练提升泛化能力的视觉-语言-动作模型（MiVLA），能更好地融合人与机器人的行为知识，并显著提升了机器人任务泛化表现。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型（VLA）受限于人类与机器人镜头视角、外观和形态差异，造成利用人类视频数据和机器人仿真数据时泛化能力不足，导致难以很好地解决真实机器人任务中的数据稀缺问题。

Method: MiVLA利用人-机器人行为间的本质相似性，通过引入左右手坐标系的运动学规则，在人的动作空间与机器人的动作空间之间进行双向对齐。具体来说，模型在获得人类或仿真机器人演示后，能学习预测其中一种实体的行为轨迹，同时模仿另一种未见过的实体行为，通过相互模仿融合了真实人类和多样仿真数据的信息构建统一行为先验。

Result: 在模拟和真实环境下，分别用三种机器人（ARX、PiPer、LocoMan）进行了大量实验。结果显示，MiVLA在仿真场景下较现有SOTA提升25%，在真实控制任务上提升14%。

Conclusion: 通过创新的人-机互模仿预训练，MiVLA有效结合了人类数据的真实性和机器数据的多样性，在提升机器人泛化能力方面达到了新的水平，优于当前主流方法。

Abstract: While leveraging abundant human videos and simulated robot data poses a scalable solution to the scarcity of real-world robot data, the generalization capability of existing vision-language-action models (VLAs) remains limited by mismatches in camera views, visual appearance, and embodiment morphologies. To overcome this limitation, we propose MiVLA, a generalizable VLA empowered by human-robot mutual imitation pre-training, which leverages inherent behavioral similarity between human hands and robotic arms to build a foundation of strong behavioral priors for both human actions and robotic control. Specifically, our method utilizes kinematic rules with left/right hand coordinate systems for bidirectional alignment between human and robot action spaces. Given human or simulated robot demonstrations, MiVLA is trained to forecast behavior trajectories for one embodiment, and imitate behaviors for another one unseen in the demonstration. Based on this mutual imitation, it integrates the behavioral fidelity of real-world human data with the manipulative diversity of simulated robot data into a unified model, thereby enhancing the generalization capability for downstream tasks. Extensive experiments conducted on both simulation and real-world platforms with three robots (ARX, PiPer and LocoMan), demonstrate that MiVLA achieves strong improved generalization capability, outperforming state-of-the-art VLAs (e.g., $\boldsymbolπ_{0}$, $\boldsymbolπ_{0.5}$ and H-RDT) by 25% in simulation, and 14% in real-world robot control tasks.

</details>


### [143] [Load-Based Variable Transmission Mechanism for Robotic Applications](https://arxiv.org/abs/2512.15448)
*Sinan Emre,Victor Barasuol,Matteo Villa,Claudio Semini*

Main category: cs.RO

TL;DR: 本文提出了一种基于负载的可变传动机制（LBVT），能够根据外部力矩需求自动调整传动比，无需额外致动器，有效提升机器人关节执行效率。通过仿真分析验证了其在特定扭矩条件下可提高40%传动比，实现所需时的扭矩放大。特别适用于对动态力矩适应性要求高的机器人，例如足式机器人。


<details>
  <summary>Details</summary>
Motivation: 现有可变传动系统通常需要额外的致动器实现主动控制，结构复杂、重量大、能耗高。为降低系统复杂性并提升执行性能，亟需一种无需额外致动器即可智能适应负载变化的传动机制。

Method: 本方法基于预加张力弹簧与四连杆机构，利用被动结构响应外部负载，自动改变传动比。通过仿真方式考察了系统在不同负载与力矩条件下的响应和工作特性。

Result: 仿真结果显示，在达到一定力矩阈值后，传动比最高提升40%，关节可实现扭矩放大。具体在外载大于18N时，设备会自动启用扭矩放大机制，有效增强关节驱动力。

Conclusion: LBVT机制在无需额外致动器的条件下，实现了轻量化、高效、自适应的传动比调节，有助于提升足式机器人等对扭矩动态适应性要求高领域的执行能力，对机器人机构简化和性能提升具有重要意义。

Abstract: This paper presents a Load-Based Variable Transmission (LBVT) mechanism designed to enhance robotic actuation by dynamically adjusting the transmission ratio in response to external torque demands. Unlike existing variable transmission systems that require additional actuators for active control, the proposed LBVT mechanism leverages a pre-tensioned spring and a four-bar linkage to passively modify the transmission ratio, thereby reducing the complexity of robot joint actuation systems. The effectiveness of the LBVT mechanism is evaluated through simulation-based analyses. The results confirm that the system achieves up to a 40 percent increase in transmission ratio upon reaching a predefined torque threshold, effectively amplifying joint torque when required without additional actuation. Furthermore, the simulations demonstrate a torque amplification effect triggered when the applied force exceeds 18 N, highlighting the system ability to autonomously respond to varying load conditions. This research contributes to the development of lightweight, efficient, and adaptive transmission systems for robotic applications, particularly in legged robots where dynamic torque adaptation is critical.

</details>


### [144] [OMCL: Open-vocabulary Monte Carlo Localization](https://arxiv.org/abs/2512.15557)
*Evgenii Kruzhkov,Raphael Memmesheimer,Sven Behnke*

Main category: cs.RO

TL;DR: 本文提出了一种结合视觉-语言特征的蒙特卡洛定位方法，实现了不同传感器构建地图时，观测与地图元素的鲁棒关联，并通过自然语言描述进行全局初始化。


<details>
  <summary>Details</summary>
Motivation: 在机器人导航中，精确定位依赖于观测数据与地图的准确关联。若地图由不同类型的传感器生成（如RGB-D和点云），传统方法难以做到鲁棒关联。亟需克服多模态数据下地图与观测的匹配难题。

Method: 本文扩展了蒙特卡洛定位方法，引入视觉-语言开放词表特征，将自然语言描述、RGB-D图像或点云配准得到的三维地图与当前摄像头观测关联。利用这些抽象特征估算观测在不同位姿下的似然。

Result: 在Matterport3D和Replica等室内场景以及SemanticKITTI室外场景上验证，结果表明该方法具有较强的泛化能力和鲁棒性，能有效处理多源、多模态数据。

Conclusion: 结合视觉-语言特征后，机器人能够在复杂多源环境下进行可靠定位，并可利用自然语言初始化定位，为未来多模态感知下的自主导航提供了新方向。

Abstract: Robust robot localization is an important prerequisite for navigation planning. If the environment map was created from different sensors, robot measurements must be robustly associated with map features. In this work, we extend Monte Carlo Localization using vision-language features. These open-vocabulary features enable to robustly compute the likelihood of visual observations, given a camera pose and a 3D map created from posed RGB-D images or aligned point clouds. The abstract vision-language features enable to associate observations and map elements from different modalities. Global localization can be initialized by natural language descriptions of the objects present in the vicinity of locations. We evaluate our approach using Matterport3D and Replica for indoor scenes and demonstrate generalization on SemanticKITTI for outdoor scenes.

</details>


### [145] [An Open Toolkit for Underwater Field Robotics](https://arxiv.org/abs/2512.15597)
*Giacomo Picardi,Saverio Iacoponi,Matias Carandell,Jorge Aguirregomezcorta,Mrudul Chellapurath,Joaquin del Rio,Marcello Calisti,Iacopo Aguzzi*

Main category: cs.RO

TL;DR: 该论文提出了一套开源、低成本的水下操作与驱动系统工具包，并通过多项实验验证了其实用性，旨在助力水下机器人研究社区。


<details>
  <summary>Details</summary>
Motivation: 当前水下机器人的操作系统开发受高成本、专有设计和缺乏模块化硬件等因素限制，难以满足研究和实际应用需求。现有开源项目多聚焦于整车和控制软件，缺少适合机械手、抓取器等设备的开源驱动关节，导致开发周期长、可重复性差、原型难以应用于实地。

Method: 作者开发了一个包括深度防水机械关节（带早期渗漏检测）、紧凑的控制/电源管理电子模块，以及基于ROS2的传感与多模态驱动软件栈的工具包。所有设计文件和代码全部开源，支持本地制造和社区改进。工具包在实验室和实地多次测试，包括用于3自由度机械手、软体抓手和采样器等多种场景。

Result: 工具包在多种应用中实现了高达40米深度的可靠运行，并支持多种水下操作装置，验证其在真实环境下的鲁棒性和复用性。

Conclusion: 本工作通过开源、可现场应用的平台，极大降低了水下操作研究的门槛，有助于提升实验可重复性，并加速水下机器人领域的创新。

Abstract: Underwater robotics is becoming increasingly important for marine science, environmental monitoring, and subsea industrial operations, yet the development of underwater manipulation and actuation systems remains restricted by high costs, proprietary designs, and limited access to modular, research-oriented hardware. While open-source initiatives have democratized vehicle construction and control software, a substantial gap persists for joint-actuated systems-particularly those requiring waterproof, feedback-enabled actuation suitable for manipulators, grippers, and bioinspired devices. As a result, many research groups face lengthy development cycles, limited reproducibility, and difficulty transitioning laboratory prototypes to field-ready platforms.
  To address this gap, we introduce an open, cost-effective hardware and software toolkit for underwater manipulation research. The toolkit includes a depth-rated Underwater Robotic Joint (URJ) with early leakage detection, compact control and power management electronics, and a ROS2-based software stack for sensing and multi-mode actuation. All CAD models, fabrication files, PCB sources, firmware, and ROS2 packages are openly released, enabling local manufacturing, modification, and community-driven improvement.
  The toolkit has undergone extensive laboratory testing and multiple field deployments, demonstrating reliable operation up to 40 m depth across diverse applications, including a 3-DoF underwater manipulator, a tendon-driven soft gripper, and an underactuated sediment sampler. These results validate the robustness, versatility, and reusability of the toolkit for real marine environments.
  By providing a fully open, field-tested platform, this work aims to lower the barrier to entry for underwater manipulation research, improve reproducibility, and accelerate innovation in underwater field robotics.

</details>


### [146] [mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs](https://arxiv.org/abs/2512.15692)
*Jonas Pai,Liam Achenbach,Victoriano Montesinos,Benedek Forrai,Oier Mees,Elvis Nava*

Main category: cs.RO

TL;DR: 该论文提出了一种新的视频-动作模型（VAM），通过结合大规模视频模型和动作解码器，提高了机器人操作任务的样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型(VLA)主要依赖于静态网络数据预训练，缺乏物理因果推理能力，导致需要大量专家数据以弥补不足。作者认为VLA模型在物理理解上存在先天缺陷。

Method: 提出了一种VAM框架，将预训练视频模型与基于流匹配的动作解码器结合。动作解码器作为逆动力学模型，从视频的潜在表示生成机器人的低级动作。

Result: 在仿真和真实机器人操作任务中，所提出的方法不仅实现了业界领先的性能，还使样本效率提升了10倍，收敛速度提升了2倍。

Conclusion: 通过利用视频预训练模型获取语义和物理动态表示，该方法有效减少了对专家数据的依赖，是比现有VLA更优的范式。

Abstract: Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce \model, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.

</details>
