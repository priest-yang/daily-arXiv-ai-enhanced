<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 63]
- [cs.CL](#cs.CL) [Total: 44]
- [cs.RO](#cs.RO) [Total: 21]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [LoRA-Edge: Tensor-Train-Assisted LoRA for Practical CNN Fine-Tuning on Edge Devices](https://arxiv.org/abs/2511.03765)
*Hyunseok Kwak,Kyeongwon Lee,Jae-Jin Lee,Woojoo Lee*

Main category: cs.CV

TL;DR: 本文提出了LoRA-Edge，一种高效并适用于端侧设备的卷积神经网络(CNN)微调方法，显著减少参数数量但保持较高精度，实现了对领域偏移的适应，且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备（如可穿戴设备中的人体活动识别任务）上，因内存、算力和能耗受限，直接对完整CNN进行微调不可行。高效的微调方法对于适应新环境（领域偏移）非常必要。

Method: 提出了LoRA-Edge方法：基于低秩适应（LoRA），结合张量-列分解（Tensor-Train Singular Value Decomposition, TT-SVD）。具体做法是：对预训练卷积层应用TT-SVD，仅更新output侧核心（初始为全零），微调后将更新结果融合回稠密卷积核，不增加推理开销。这种方法极大减少了需训练参数量。

Result: 在多个人体活动识别数据集和不同CNN后端架构上，LoRA-Edge在只需最多1.49%参数更新的情况下，准确率仅比全微调低4.7%，在相同预算下持续超越以往高效微调方法。同时，在Jetson Orin Nano平台上，TT-SVD初始化和核心选择性训练令模型收敛速度提升1.4-3.8倍。

Conclusion: LoRA-Edge实现了结构对齐和参数高效的端侧CNN自适应，提高了低成本设备上的实际微调能力，兼顾精度和资源受限的实际需求。

Abstract: On-device fine-tuning of CNNs is essential to withstand domain shift in edge
applications such as Human Activity Recognition (HAR), yet full fine-tuning is
infeasible under strict memory, compute, and energy budgets. We present
LoRA-Edge, a parameter-efficient fine-tuning (PEFT) method that builds on
Low-Rank Adaptation (LoRA) with tensor-train assistance. LoRA-Edge (i) applies
Tensor-Train Singular Value Decomposition (TT-SVD) to pre-trained convolutional
layers, (ii) selectively updates only the output-side core with
zero-initialization to keep the auxiliary path inactive at the start, and (iii)
fuses the update back into dense kernels, leaving inference cost unchanged.
This design preserves convolutional structure and reduces the number of
trainable parameters by up to two orders of magnitude compared to full
fine-tuning. Across diverse HAR datasets and CNN backbones, LoRA-Edge achieves
accuracy within 4.7% of full fine-tuning while updating at most 1.49% of
parameters, consistently outperforming prior parameter-efficient baselines
under similar budgets. On a Jetson Orin Nano, TT-SVD initialization and
selective-core training yield 1.4-3.8x faster convergence to target F1.
LoRA-Edge thus makes structure-aligned, parameter-efficient on-device CNN
adaptation practical for edge platforms.

</details>


### [2] [SILVI: Simple Interface for Labeling Video Interactions](https://arxiv.org/abs/2511.03819)
*Ozan Kanbertay,Richard Vogg,Elif Karakoc,Peter M. Kappeler,Claudia Fichtel,Alexander S. Ecker*

Main category: cs.CV

TL;DR: 提出了一款新的开源标签软件SILVI，结合了行为标注和个体定位，专为动物行为互动的分析开发。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉方法多聚焦于个体行为检测，缺乏对行为互动的检测和标注工具，而理解互动对于动物社会行为的研究至关重要。现有开源工具不能同时标注个体定位与互动。

Method: 开发了SILVI软件，能在视频数据中直接标注行为和互动，同时生成结构化数据输出，适合计算机视觉模型的训练和验证。

Result: SILVI能够高效地支持动物行为学和计算机视觉交叉的细粒度行为分析，为自动化行为分析方法的发展提供了基础。

Conclusion: SILVI有效填补了行为互动标注工具的空白，促进了动物行为研究和相关自动化方法的发展，也有望用于更广泛的场景如人类互动标注。

Abstract: Computer vision methods are increasingly used for the automated analysis of
large volumes of video data collected through camera traps, drones, or direct
observations of animals in the wild. While recent advances have focused
primarily on detecting individual actions, much less work has addressed the
detection and annotation of interactions -- a crucial aspect for understanding
social and individualized animal behavior. Existing open-source annotation
tools support either behavioral labeling without localization of individuals,
or localization without the capacity to capture interactions. To bridge this
gap, we present SILVI, an open-source labeling software that integrates both
functionalities. SILVI enables researchers to annotate behaviors and
interactions directly within video data, generating structured outputs suitable
for training and validating computer vision models. By linking behavioral
ecology with computer vision, SILVI facilitates the development of automated
approaches for fine-grained behavioral analyses. Although developed primarily
in the context of animal behavior, SILVI could be useful more broadly to
annotate human interactions in other videos that require extracting dynamic
scene graphs. The software, along with documentation and download instructions,
is available at: https://gitlab.gwdg.de/kanbertay/interaction-labelling-app.

</details>


### [3] [Noise Injection: Improving Out-of-Distribution Generalization for Limited Size Datasets](https://arxiv.org/abs/2511.03855)
*Duong Mai,Lawrence Hall*

Main category: cs.CV

TL;DR: 本文提出在训练阶段向输入数据注入不同类型的噪声，以提升COVID-19胸部X光深度学习检测模型的跨分布泛化能力，并显著减少了内外分布间的性能差距。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学图像识别领域存在泛化不足的问题，尤其是在新冠肺炎检测中，当模型遇到来自新医院或设备的数据（即分布外数据）时，性能骤降。主要原因是模型倾向于利用与任务无关的“捷径”特征（如来源相关的伪迹），而不是可靠的生物标志物。本文试图解决模型分布外泛化能力不足问题。

Method: 作者在训练阶段对输入胸部X光图像分别添加高斯噪声、斑点噪声、泊松噪声以及椒盐噪声，通过噪声注入来提升模型对分布变化的鲁棒性，并与常规训练方式下的模型进行对比实验。

Result: 在10个随机种子、AUC、F1、准确率、召回率和特异性等多项关键指标下，基于噪声注入训练的模型显著缩小了分布内外的性能差距（由0.10-0.20降至0.01-0.06）。

Conclusion: 噪声注入是一种简单有效的方法，能显著提升医学影像深度学习模型在分布外数据上的泛化能力，对提升实际临床应用的可靠性具有重要意义。

Abstract: Deep learned (DL) models for image recognition have been shown to fail to
generalize to data from different devices, populations, etc. COVID-19 detection
from Chest X-rays (CXRs), in particular, has been shown to fail to generalize
to out-of-distribution (OOD) data from new clinical sources not covered in the
training set. This occurs because models learn to exploit shortcuts -
source-specific artifacts that do not translate to new distributions - rather
than reasonable biomarkers to maximize performance on in-distribution (ID)
data. Rendering the models more robust to distribution shifts, our study
investigates the use of fundamental noise injection techniques (Gaussian,
Speckle, Poisson, and Salt and Pepper) during training. Our empirical results
demonstrate that this technique can significantly reduce the performance gap
between ID and OOD evaluation from 0.10-0.20 to 0.01-0.06, based on results
averaged over ten random seeds across key metrics such as AUC, F1, accuracy,
recall and specificity. Our source code is publicly available at
https://github.com/Duongmai127/Noisy-ood

</details>


### [4] [Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures](https://arxiv.org/abs/2511.03882)
*Florence Klitzner,Blanca Inigo,Benjamin D. Killeen,Lalithkumar Seenivasan,Michelle Song,Axel Krieger,Mathias Unberath*

Main category: cs.CV

TL;DR: 本文探索了基于模仿学习的视频感知机器人控制方法是否适用于X射线引导的脊柱内手术（如套管针置入）。作者构建了高仿真的仿真平台，采集了正确轨迹及双平面X射线数据，通过视觉信息训练模仿学习控制策略。实验表明该策略在多样解剖结构下表现良好并有一定泛化能力，但在进针点精度方面仍有限制。


<details>
  <summary>Details</summary>
Motivation: 当前模仿学习在视频感知机器人控制中取得新进展，但该方法是否可以应用于复杂且需多视角解读的X射线引导脊柱手术尚不明确。作者希望评估模仿学习在手术机器人领域的潜力与局限。

Method: 开发高仿真仿真平台，自动模拟X射线引导的脊柱套管针置入。收集正确轨迹及相应的双平面X射线序列，基于这些数据训练模仿学习策略，进行视觉驱动的对准与控制。通过仿真实验和部分实际X射线数据对模型进行验证。

Result: 策略首次尝试成功率为68.5%，在多样椎体结构和不同起始条件下均能保持安全的椎弓根内轨迹。策略能泛化到骨折等复杂解剖结构，实际X射线测试中也能生成合理轨迹，但在进针点精度方面尚有不足。

Conclusion: 初步结果显示模仿学习对X射线引导机器人手术有前景，但精度和反馈频率等问题待改进。结合更优先验和领域知识，有望为轻量化、无需CT的术中导航奠定基础。

Abstract: Imitation learning-based robot control policies are enjoying renewed interest
in video-based robotics. However, it remains unclear whether this approach
applies to X-ray-guided procedures, such as spine instrumentation. This is
because interpretation of multi-view X-rays is complex. We examine
opportunities and challenges for imitation policy learning in bi-plane-guided
cannula insertion. We develop an in silico sandbox for scalable, automated
simulation of X-ray-guided spine procedures with a high degree of realism. We
curate a dataset of correct trajectories and corresponding bi-planar X-ray
sequences that emulate the stepwise alignment of providers. We then train
imitation learning policies for planning and open-loop control that iteratively
align a cannula solely based on visual information. This precisely controlled
setup offers insights into limitations and capabilities of this method. Our
policy succeeded on the first attempt in 68.5% of cases, maintaining safe
intra-pedicular trajectories across diverse vertebral levels. The policy
generalized to complex anatomy, including fractures, and remained robust to
varied initializations. Rollouts on real bi-planar X-rays further suggest that
the model can produce plausible trajectories, despite training exclusively in
simulation. While these preliminary results are promising, we also identify
limitations, especially in entry point precision. Full closed-look control will
require additional considerations around how to provide sufficiently frequent
feedback. With more robust priors and domain knowledge, such models may provide
a foundation for future efforts toward lightweight and CT-free robotic
intra-operative spinal navigation.

</details>


### [5] [Desert Waste Detection and Classification Using Data-Based and Model-Based Enhanced YOLOv12 DL Model](https://arxiv.org/abs/2511.03888)
*Abdulmumin Sa'ad,Sulaimon Oyeniyi Adebayo,Abdul Jabbar Siddiqui*

Main category: cs.CV

TL;DR: 该论文提出了一种适用于沙漠环境的轻量化YOLOv12目标检测框架，结合自对抗训练和特殊数据增强手段，实现了无人机端高效、精准的垃圾检测。


<details>
  <summary>Details</summary>
Motivation: 全球固体垃圾增速快，现有人工收集在沙漠等恶劣环境下效率低且危险。现有自动化垃圾检测多关注城市和可回收物，忽略有机、有害垃圾及沙漠地带。

Method: 作者提出对YOLOv12模型进行剪枝优化，并结合自对抗训练（SAT）及专门的数据增强策略，使用DroneTrashNet数据集训练模型，并与主流轻量化YOLO模型做对比。

Result: 新框架在精准率、召回率和mAP方面均有显著提升，同时延迟低，模型体积小，便于搭载无人机等资源受限平台。

Conclusion: 将数据和模型层面的优化联合应用，可有效提升沙漠环境中无人机实时垃圾检测的准确性与实用性。

Abstract: The global waste crisis is escalating, with solid waste generation expected
to increase by 70% by 2050. Traditional waste collection methods, particularly
in remote or harsh environments like deserts, are labor-intensive, inefficient,
and often hazardous. Recent advances in computer vision and deep learning have
opened the door to automated waste detection systems, yet most research focuses
on urban environments and recyclable materials, overlooking organic and
hazardous waste and underexplored terrains such as deserts. In this work, we
propose an enhanced real-time object detection framework based on a pruned,
lightweight version of YOLOv12 integrated with Self-Adversarial Training (SAT)
and specialized data augmentation strategies. Using the DroneTrashNet dataset,
we demonstrate significant improvements in precision, recall, and mean average
precision (mAP), while achieving low latency and compact model size suitable
for deployment on resource-constrained aerial drones. Benchmarking our model
against state-of-the-art lightweight YOLO variants further highlights its
optimal balance of accuracy and efficiency. Our results validate the
effectiveness of combining data-centric and model-centric enhancements for
robust, real-time waste detection in desert environments.

</details>


### [6] [Improving Diagnostic Performance on Small and Imbalanced Datasets Using Class-Based Input Image Composition](https://arxiv.org/abs/2511.03891)
*Hlali Azzeddine,Majid Ben Yakhlef,Soulaiman El Hazzat*

Main category: cs.CV

TL;DR: 本文提出了一种新的数据增强方法——基于类别的图像合成（Class-Based Image Composition），通过将同一类别的多张原始图片融合为复合输入图像（CoImg），提升了训练的有效信息量和模型区分细微病变模式的能力。实验表明该方法在小样本、不均衡的OCT眼底数据集上大幅提升了诊断准确率并显著下降了误判率。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学成像中，由于数据集样本数量少与类别不均衡，模型常出现高伪预测率。作者希望通过增强训练样本的丰富度和类别内变异性，有效缓解这一问题。

Method: 提出将同类多张原始图像通过特定方法融合为单张复合输入图像（CoImg），提升每个训练样本中的信息密度。对原始不均衡OCT眼底数据集（OCTDL）进行了重采样与合成，获得了均衡的新数据集（Co-OCTDL），并和原始数据集在VGG16模型下进行实验对比。

Result: 在保持相同的网络结构和超参数下，使用合成数据集训练的模型诊断准确率提升到99.6%，F1分数和AUC也有大幅提升，并显著降低了误判率，优于原始数据集训练结果。

Conclusion: 基于类别的图像合成能显著提升小规模且类别不均衡医学影像数据的深度学习诊断性能，提高预测质量，减小伪判风险，具有实际应用价值。

Abstract: Small, imbalanced datasets and poor input image quality can lead to high
false predictions rates with deep learning models. This paper introduces
Class-Based Image Composition, an approach that allows us to reformulate
training inputs through a fusion of multiple images of the same class into
combined visual composites, named Composite Input Images (CoImg). That enhances
the intra-class variance and improves the valuable information density per
training sample and increases the ability of the model to distinguish between
subtle disease patterns. Our method was evaluated on the Optical Coherence
Tomography Dataset for Image-Based Deep Learning Methods (OCTDL) (Kulyabin et
al., 2024), which contains 2,064 high-resolution optical coherence tomography
(OCT) scans of the human retina, representing seven distinct diseases with a
significant class imbalance. We constructed a perfectly class-balanced version
of this dataset, named Co-OCTDL, where each scan is resented as a 3x1 layout
composite image. To assess the effectiveness of this new representation, we
conducted a comparative analysis between the original dataset and its variant
using a VGG16 model. A fair comparison was ensured by utilizing the identical
model architecture and hyperparameters for all experiments. The proposed
approach markedly improved diagnostic results.The enhanced Dataset achieved
near-perfect accuracy (99.6%) with F1-score (0.995) and AUC (0.9996), compared
to a baseline model trained on raw dataset. The false prediction rate was also
significantly lower, this demonstrates that the method can producehigh-quality
predictions even for weak datasets affected by class imbalance or small sample
size.

</details>


### [7] [I Detect What I Don't Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging](https://arxiv.org/abs/2511.03912)
*Nand Kumar Yadav,Rodrigue Rizk,William CW Chen,KC Santosh*

Main category: cs.CV

TL;DR: 本论文提出了一种无需标签、基于不确定性门控的增量式医学影像异常检测方法，可以在极少监督、无先验异常标签的条件下，有效扩展正常样本集，提高异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像中异常样本标签稀缺，专家标注昂贵，因此需要在不依赖异常标签的情况下，提升未知异常检测能力。

Method: 方法从一小批已验证正常影像出发，采用冻结的大型视觉主干和微型卷积Adapter实现快速领域适应。系统利用k近邻评分管理异常检测，通过双重概率门限（z-score距离和基于SWAG的不确定性阈值），在每次增量扩展时决定是否将新样本纳入正常记忆库，防止漂移和假纳入，无需生成重建或回放缓冲。

Result: 在COVID-CXR、Pneumonia CXR及Brain MRI ND-5等公开医学影像数据集上，框架大幅提升了异常检测性能（如COVID-CXR的AUC从0.9489提升到0.9982，F1从0.8048提升到0.9746等），显著优于基线方法。

Conclusion: 提出的无监督、不依赖oracle的增量异常检测框架，能高效适应医学影像中极少标签场景，对实际应用具有很大潜力和推广价值。

Abstract: Unknown anomaly detection in medical imaging remains a fundamental challenge
due to the scarcity of labeled anomalies and the high cost of expert
supervision. We introduce an unsupervised, oracle-free framework that
incrementally expands a trusted set of normal samples without any anomaly
labels. Starting from a small, verified seed of normal images, our method
alternates between lightweight adapter updates and uncertainty-gated sample
admission. A frozen pretrained vision backbone is augmented with tiny
convolutional adapters, ensuring rapid domain adaptation with negligible
computational overhead. Extracted embeddings are stored in a compact coreset
enabling efficient k-nearest neighbor anomaly (k-NN) scoring. Safety during
incremental expansion is enforced by dual probabilistic gates, a sample is
admitted into the normal memory only if its distance to the existing coreset
lies within a calibrated z-score threshold, and its SWAG-based epistemic
uncertainty remains below a seed-calibrated bound. This mechanism prevents
drift and false inclusions without relying on generative reconstruction or
replay buffers. Empirically, our system steadily refines the notion of
normality as unlabeled data arrive, producing substantial gains over baselines.
On COVID-CXR, ROC-AUC improves from 0.9489 to 0.9982 (F1: 0.8048 to 0.9746); on
Pneumonia CXR, ROC-AUC rises from 0.6834 to 0.8968; and on Brain MRI ND-5,
ROC-AUC increases from 0.6041 to 0.7269 and PR-AUC from 0.7539 to 0.8211. These
results highlight the effectiveness and efficiency of the proposed framework
for real-world, label-scarce medical imaging applications.

</details>


### [8] [Adaptive Temporal Refinement: Continuous Depth Allocation and Distance Regression for Efficient Action Localization](https://arxiv.org/abs/2511.03943)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.CV

TL;DR: 该论文提出两项创新方法提升时序动作定位中的边界检测精度，通过边界距离回归（BDR）优化定位，提高边界锐度，并提出自适应时序细化（ATR）有效分配计算资源，最终在多个基准上减少计算量且提升精度。


<details>
  <summary>Details</summary>
Motivation: 现有时序动作定位方法对所有边界采用统一计算，未能针对不同边界难度有效分配资源，导致边界检测精度受限。作者希望通过更高效和精确的方法提升边界定位能力。

Method: 方法包含两部分：1）提出边界距离回归（BDR），用有符号距离回归代替分类，提高定位信息量和边界精度，仅通过少量代码即可集成到现有架构。2）提出自适应时序细化（ATR），采用连续深度选择分配计算，实现端到端可微优化并无需强化学习，同时结合知识蒸馏降低训练成本。

Result: BDR集成带来平均1.8-3.1%的mAP@0.7提升，边界更锐利。ATR在THUMOS14数据集上，以更低计算量（162G FLOPs vs 198G FLOPs）提升性能（mAP@0.7从53.6%提升到56.5%）；对短动作提升更显著。知识蒸馏下轻量模型可保持99%性能。

Conclusion: 两种方法协同提升了时序动作定位的精度和效率，减少了计算资源消耗，实验证明对不同架构和数据集具有稳定有效的提升作用。

Abstract: Temporal action localization requires precise boundary detection; however,
current methods apply uniform computation despite significant variations in
difficulty across boundaries. We present two complementary contributions.
First, Boundary Distance Regression (BDR) provides information-theoretically
optimal localization through signed-distance regression rather than
classification, achieving 43\% sharper boundary peaks. BDR retrofits to
existing methods with approximately 50 lines of code, yielding consistent 1.8
to 3.1\% mAP@0.7 improvements across diverse architectures. Second, Adaptive
Temporal Refinement (ATR) allocates computation via continuous depth selection
$\tau \in [0,1]$, enabling end-to-end differentiable optimization without
reinforcement learning. On THUMOS14, ATR achieves 56.5\% mAP@0.7 at 162G FLOPs,
compared to 53.6\% at 198G for uniform processing, providing a 2.9\%
improvement with 18\% less compute. Gains scale with boundary heterogeneity,
showing 4.2\% improvement on short actions. Training cost is mitigated via
knowledge distillation, with lightweight students retaining 99\% performance at
baseline cost. Results are validated across four benchmarks with rigorous
statistical testing.

</details>


### [9] [Improving Multi-View Reconstruction via Texture-Guided Gaussian-Mesh Joint Optimization](https://arxiv.org/abs/2511.03950)
*Zhejia Cai,Puhua Jiang,Shiwei Mao,Hongkun Cao,Ruqi Huang*

Main category: cs.CV

TL;DR: 本文提出了一种新的三维重建方法，实现了几何结构和外观颜色的联合优化，提升了编辑和渲染的效果。


<details>
  <summary>Details</summary>
Motivation: 现有多视图三维重建方法往往只注重几何精度或逼真外观，并且倾向于将几何与外观分开优化，导致难以高效实现后续的三维编辑。作者希望通过统一处理这两方面，提升三维重建结果的可编辑性和质量。

Method: 提出了一种无缝联合优化网格几何（顶点位置与面片）和顶点颜色的新框架。该方法利用高斯引导的可微分网格渲染，结合输入图像的光度一致性损失和法线、深度图的几何正则，优化获取高质量的三维模型。

Result: 实验结果表明，该方法能够生成高质量并具备良好外观的三维重建模型，且兼容后续如重光照、形变等编辑任务。

Conclusion: 作者的方法有效统一了三维几何和外观的优化，为后续三维内容编辑提供了更高质量、易用性的基础，推动了三维编辑领域的发展。

Abstract: Reconstructing real-world objects from multi-view images is essential for
applications in 3D editing, AR/VR, and digital content creation. Existing
methods typically prioritize either geometric accuracy (Multi-View Stereo) or
photorealistic rendering (Novel View Synthesis), often decoupling geometry and
appearance optimization, which hinders downstream editing tasks. This paper
advocates an unified treatment on geometry and appearance optimization for
seamless Gaussian-mesh joint optimization. More specifically, we propose a
novel framework that simultaneously optimizes mesh geometry (vertex positions
and faces) and vertex colors via Gaussian-guided mesh differentiable rendering,
leveraging photometric consistency from input images and geometric
regularization from normal and depth maps. The obtained high-quality 3D
reconstruction can be further exploit in down-stream editing tasks, such as
relighting and shape deformation. The code will be publicly available upon
acceptance.

</details>


### [10] [A Linear Fractional Transformation Model and Calibration Method for Light Field Camera](https://arxiv.org/abs/2511.03962)
*Zhong Chen,Changfeng Chen*

Main category: cs.CV

TL;DR: 本文提出了一种用于光场相机内参标定的新方法，引入了线性分式变换（LFT）参数α，实现主镜头与微透镜阵列的解耦，并通过理论分析和实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 光场相机的3D重建高度依赖于精确的内参标定。然而，主镜头与微透镜阵列的参数耦合导致标定过程复杂且不精确。因此需要新的模型和方法来提高标定的准确性和效率。

Method: 提出利用线性分式变换（LFT）参数α，实现主镜头与微透镜阵列的参数解耦。采用基于最小二乘的解析解作为初始估计，并辅以非线性优化迭代提高精度。同时，优化了原始图像的特征检测算法。

Result: 在真实和仿真数据集上进行了实验，结果表明新方法在准确度和鲁棒性上均优于现有方法。此外，基于该模型的光场原始图像仿真速度提升，便于后续的数据驱动深度学习研究。

Conclusion: 所提出的LFT标定方法能够有效提升光场相机的内参标定精度与效率，并加速光场数据仿真流程，对3D重建和相关深度学习应用具有重要推动作用。

Abstract: Accurate calibration of internal parameters is a crucial yet challenging
prerequisite for 3D reconstruction using light field cameras. In this paper, we
propose a linear fractional transformation(LFT) parameter $\alpha$ to decoupled
the main lens and micro lens array (MLA). The proposed method includes an
analytical solution based on least squares, followed by nonlinear refinement.
The method for detecting features from the raw images is also introduced.
Experimental results on both physical and simulated data have verified the
performance of proposed method. Based on proposed model, the simulation of raw
light field images becomes faster, which is crucial for data-driven deep
learning methods. The corresponding code can be obtained from the author's
website.

</details>


### [11] [Room Envelopes: A Synthetic Dataset for Indoor Layout Reconstruction from Images](https://arxiv.org/abs/2511.03970)
*Sam Bahrami,Dylan Campbell*

Main category: cs.CV

TL;DR: 该论文关注于场景重建中结构性元素（如墙面、地板和天花板）的推断，提出了新的合成数据集Room Envelopes，帮助提升从单张图片估计算法对房间结构布局的预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有三维重建方法往往只重建相机可见的部分，导致遮挡区域（如墙后地板、天花板）缺失。而这些结构性元素通常具有重复、平面等特性，理论上可用低成本方法预测，但相关研究相对较少，因此需要专门数据集和方法支持。

Method: 作者提出了Room Envelopes合成数据集。为每张RGB图像分别配套两个点图：一个为可见表面点图，另一个为移除家具之后的结构布局点图。这一设计允许直接监督单目几何网络同时预测可见表面与结构布局面。

Result: 实验表明，借助该数据集训练的前馈单目几何估计算法，能够较好地预测房间结构布局并区分可见表面与实际结构表面，提升了场景的完整感知能力。

Conclusion: 论文认为重建房间结构布局比复杂形体更容易，可用低成本方法实现。Room Envelopes数据集为单目场景结构布局估计任务提供了有力的工具，有助于推进相关研究。

Abstract: Modern scene reconstruction methods are able to accurately recover 3D
surfaces that are visible in one or more images. However, this leads to
incomplete reconstructions, missing all occluded surfaces. While much progress
has been made on reconstructing entire objects given partial observations using
generative models, the structural elements of a scene, like the walls, floors
and ceilings, have received less attention. We argue that these scene elements
should be relatively easy to predict, since they are typically planar,
repetitive and simple, and so less costly approaches may be suitable. In this
work, we present a synthetic dataset -- Room Envelopes -- that facilitates
progress on this task by providing a set of RGB images and two associated
pointmaps for each image: one capturing the visible surface and one capturing
the first surface once fittings and fixtures are removed, that is, the
structural layout. As we show, this enables direct supervision for feed-forward
monocular geometry estimators that predict both the first visible surface and
the first layout surface. This confers an understanding of the scene's extent,
as well as the shape and location of its objects.

</details>


### [12] [Simple 3D Pose Features Support Human and Machine Social Scene Understanding](https://arxiv.org/abs/2511.03988)
*Wenshuo Qin,Leyla Isik*

Main category: cs.CV

TL;DR: 本文通过研究表明，人类在理解社交场景时主要依赖明确的3D身体姿态信息，而大多数AI视觉模型缺乏对此类信息的准确建模。通过3D姿态和深度估计算法获得的关键信息大大超越了现有AI模型，突显了3D身体姿态在社会互动识别中的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管人类能轻松理解他人之间的社交互动，但支撑这一能力的计算机制仍不明确。更重要的是，即便是最先进的AI视觉系统，在社交互动识别方面仍然表现不佳。因此，作者希望探究人类社交判断的核心线索，并检验现有AI模型在这一任务中的差距。

Method: 作者结合最先进的3D姿态和深度估计算法，从日常人类动作短片中提取人体3D关节位置信息，并与当前主流AI视觉模型在预测人的社交判断能力上进行对比。此外，作者设计了仅包含头部朝向和面部位置的精简3D社会姿态特征，进一步探索哪些姿态线索对人类判断最为关键。

Result: 实验结果显示，全3D关节位置信息在预测人类对社交互动的判断上远优于多数AI视觉模型。仅用精简的3D社会姿态特征（如头部朝向和位置）即可达到与全3D关节模型相当的预测效果，并且这些特征结合AI模型输出时能显著提升AI性能。此外，AI模型对3D社会姿态特征的表征能力越强，其与人类判断的一致性越高。

Conclusion: 研究表明，人类对社交场景的理解高度依赖于3D身体姿态的显式表征，而这种能力可由简单且结构化的视觉空间原语支持。这为提升AI系统的社交感知能力提供了关键指引，即强调引入和加强3D姿态表征。

Abstract: Humans can quickly and effortlessly extract a variety of information about
others' social interactions from visual input, ranging from visuospatial cues
like whether two people are facing each other to higher-level information. Yet,
the computations supporting these abilities remain poorly understood, and
social interaction recognition continues to challenge even the most advanced AI
vision systems. Here, we hypothesized that humans rely on 3D visuospatial pose
information to make social interaction judgments, which is absent in most AI
vision models. To test this, we combined state-of-the-art pose and depth
estimation algorithms to extract 3D joint positions of people in short video
clips depicting everyday human actions and compared their ability to predict
human social interaction judgments with current AI vision models. Strikingly,
3D joint positions outperformed most current AI vision models, revealing that
key social information is available in explicit body position but not in the
learned features of most vision models, including even the layer-wise
embeddings of the pose models used to extract joint positions. To uncover the
critical pose features humans use to make social judgments, we derived a
compact set of 3D social pose features describing only the 3D position and
direction of faces in the videos. We found that these minimal descriptors
matched the predictive strength of the full set of 3D joints and significantly
improved the performance of off-the-shelf AI vision models when combined with
their embeddings. Moreover, the degree to which 3D social pose features were
represented in each off-the-shelf AI vision model predicted the model's ability
to match human social judgments. Together, our findings provide strong evidence
that human social scene understanding relies on explicit representations of 3D
pose and can be supported by simple, structured visuospatial primitives.

</details>


### [13] [CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation](https://arxiv.org/abs/2511.03992)
*Yuwen Tao,Kanglei Zhou,Xin Tan,Yuan Xie*

Main category: cs.CV

TL;DR: 本文提出了Camera Aware Referring Field (CaRF)，一种在3D高斯空间中实现多视图一致性、无需依赖2D投影监督的跨模态语言-3D分割方法，显著提升了分割精度。


<details>
  <summary>Details</summary>
Motivation: 当前将自然语言与3D几何对齐的方法多依赖2D渲染、伪监督和特定视角特征，导致3D区域定位存在视角不一致问题。作者希望在3D高斯场中直接实现端到端、多视角一致的语言引导场景分割。

Method: 提出了CaRF框架，核心包括两个创新点：1）GFCE（Gaussian Field Camera Encoding）将相机几何信息融入高斯文本交互，建模不同视角的变化，提升几何推理能力；2）ITPVS（In Training Paired View Supervision）多视角监督，在训练阶段对齐不同校准视角下每个高斯的分类结果，缓解单一视角过拟合，优化跨视角一致性。

Result: 在Ref LERF、LERF OVS和3D OVS三个代表性数据集上，CaRF实现了平均mIoU提升16.8%、4.3%、2.0%，表现超过现有最佳方法。

Conclusion: CaRF框架有效提升了基于语言的3D区域分割的多视角一致性与精度，推动了更可靠的3D场景理解，对具身智能、AR/VR交互以及自动感知等领域具有重要应用前景。

Abstract: Referring 3D Gaussian Splatting Segmentation (R3DGS) aims to interpret
free-form language expressions and localize the corresponding 3D regions in
Gaussian fields. While recent advances have introduced cross-modal alignment
between language and 3D geometry, existing pipelines still struggle with
cross-view consistency due to their reliance on 2D rendered pseudo supervision
and view specific feature learning. In this work, we present Camera Aware
Referring Field (CaRF), a fully differentiable framework that operates directly
in the 3D Gaussian space and achieves multi view consistency. Specifically,
CaRF introduces Gaussian Field Camera Encoding (GFCE), which incorporates
camera geometry into Gaussian text interactions to explicitly model view
dependent variations and enhance geometric reasoning. Building on this, In
Training Paired View Supervision (ITPVS) is proposed to align per Gaussian
logits across calibrated views during training, effectively mitigating single
view overfitting and exposing inter view discrepancies for optimization.
Extensive experiments on three representative benchmarks demonstrate that CaRF
achieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state of
the art methods on the Ref LERF, LERF OVS, and 3D OVS datasets, respectively.
Moreover, this work promotes more reliable and view consistent 3D scene
understanding, with potential benefits for embodied AI, AR/VR interaction, and
autonomous perception.

</details>


### [14] [PhysCorr: Dual-Reward DPO for Physics-Constrained Text-to-Video Generation with Automated Preference Selection](https://arxiv.org/abs/2511.03997)
*Peiyao Wang,Weining Wang,Qi Li*

Main category: cs.CV

TL;DR: 本文提出了一个名为PhysCorr的统一框架，用于提升文本生成视频的物理一致性，显著改善了生成视频中常见的物理不合理问题。


<details>
  <summary>Details</summary>
Motivation: 当前文本生成视频方法虽然在感知质量上有提升，但常出现物体运动不合理、交互不连贯等物理不一致现象，严重影响真实世界应用，如机器人、仿真等领域。作者希望通过评估和优化物理一致性，提升生成视频的可信度和应用价值。

Method: 提出PhysicsRM，这是一个首创的二维奖励模型，分别衡量视频中物体本身的稳定性及物体间的交互合理性；并基于此提出了PhyDPO，一个结合对比反馈和物理感知加权的优化流程，使视频生成过程更趋于物理合理。该方法不依赖具体模型架构，可广泛集成至主流视频生成框架中。

Result: 通过多个基准测试，PhysCorr有效提升了生成视频的物理真实度，同时保持了视觉质量和语义一致性。

Conclusion: PhysCorr框架为视频生成领域带来了物理一致性建模和优化方面的重要进步，推动了更加可信赖、物理合理的视频生成系统。

Abstract: Recent advances in text-to-video generation have achieved impressive
perceptual quality, yet generated content often violates fundamental principles
of physical plausibility - manifesting as implausible object dynamics,
incoherent interactions, and unrealistic motion patterns. Such failures hinder
the deployment of video generation models in embodied AI, robotics, and
simulation-intensive domains. To bridge this gap, we propose PhysCorr, a
unified framework for modeling, evaluating, and optimizing physical consistency
in video generation. Specifically, we introduce PhysicsRM, the first
dual-dimensional reward model that quantifies both intra-object stability and
inter-object interactions. On this foundation, we develop PhyDPO, a novel
direct preference optimization pipeline that leverages contrastive feedback and
physics-aware reweighting to guide generation toward physically coherent
outputs. Our approach is model-agnostic and scalable, enabling seamless
integration into a wide range of video diffusion and transformer-based
backbones. Extensive experiments across multiple benchmarks demonstrate that
PhysCorr achieves significant improvements in physical realism while preserving
visual fidelity and semantic alignment. This work takes a critical step toward
physically grounded and trustworthy video generation.

</details>


### [15] [GNN-MoE: Context-Aware Patch Routing using GNNs for Parameter-Efficient Domain Generalization](https://arxiv.org/abs/2511.04008)
*Mahmoud Soliman,Omar Abdelaziz,Ahmed Radwan,Anand,Mohamed Shehata*

Main category: cs.CV

TL;DR: 本文提出了一种名为GNN-MoE的方法，通过在Vision Transformer（ViT）中引入基于图神经网络的专家混合机制，实现了高效、参数节省的跨域泛化能力提升，并且在多个基准上取得了领先或有竞争力的表现。


<details>
  <summary>Details</summary>
Motivation: 主流的Vision Transformer在遇到未知域时，泛化能力有限；现有参数高效微调方法虽然减小了训练成本，却难以在各类新域上保持强鲁棒性。因此，亟需开发一种既能高效微调、又具备强泛化能力的DG方法。

Method: 该文提出GNN-MoE方法，在ViT模型中结合了高效的Kronecker适配器（adapter），并采用MoE结构（专家混合），核心创新点是用GNN（如GCN、GAT、SAGE）实现专家路由，通过分析图结构的patch间关系，有针对性地将patch送往最合适的专家，从而提升模型适应不同域的能力。

Result: GNN-MoE在多个Domain Generalization基准任务上达到了SOTA（最优）或接近SOTA的水平，而且显著减少了参数开销。

Conclusion: 基于GNN的上下文专家路由机制能够有效提升ViT在跨域任务中的表现，兼顾了参数效率与适应性，是面向实时和多域环境应用的有力方案。

Abstract: Domain generalization (DG) seeks robust Vision Transformer (ViT) performance
on unseen domains. Efficiently adapting pretrained ViTs for DG is challenging;
standard fine-tuning is costly and can impair generalization. We propose
GNN-MoE, enhancing Parameter-Efficient Fine-Tuning (PEFT) for DG with a
Mixture-of-Experts (MoE) framework using efficient Kronecker adapters. Instead
of token-based routing, a novel Graph Neural Network (GNN) router (GCN, GAT,
SAGE) operates on inter-patch graphs to dynamically assign patches to
specialized experts. This context-aware GNN routing leverages inter-patch
relationships for better adaptation to domain shifts. GNN-MoE achieves
state-of-the-art or competitive DG benchmark performance with high parameter
efficiency, highlighting the utility of graph-based contextual routing for
robust, lightweight DG.

</details>


### [16] [MedDChest: A Content-Aware Multimodal Foundational Vision Model for Thoracic Imaging](https://arxiv.org/abs/2511.04016)
*Mahmoud Soliman,Islam Osman,Mohamed S. Shehata,Rasika Rajapakshe*

Main category: cs.CV

TL;DR: 该论文提出了MedDChest，一种专为胸部医学影像设计的大规模视觉Transformer模型，通过在120万多模态胸部影像数据集上从头预训练，并结合新颖的内容感知数据增强方法，大幅提升了下游任务表现，优于ImageNet预训练模型。


<details>
  <summary>Details</summary>
Motivation: 当前医学影像视觉模型普遍依赖于在自然图像上预训练的骨干网络，导致领域不匹配并限制了性能。因此，亟需专为医学影像设计并在同领域大规模数据上预训练的基础模型。

Method: 作者从头训练了一个Vision Transformer模型（MedDChest），使用超过120万张胸部影像（涵盖X射线和CT），并融合了创新的“Guided Random Resized Crops”内容感知增强方法，有效聚焦重要解剖区域。

Result: 通过在多种下游诊断任务上微调，MedDChest在性能上显著优于使用ImageNet预训练的主流模型。大量实验证明其对医学任务的适用性和优越性。

Conclusion: MedDChest验证了同领域大规模预训练与专用数据增强的有效性，为胸部影像诊断任务提供了强大且鲁棒的特征提取基础，模型权重也将公开以促进医学影像领域的进一步研究和应用。

Abstract: The performance of vision models in medical imaging is often hindered by the
prevailing paradigm of fine-tuning backbones pre-trained on out-of-domain
natural images. To address this fundamental domain gap, we propose MedDChest, a
new foundational Vision Transformer (ViT) model optimized specifically for
thoracic imaging. We pre-trained MedDChest from scratch on a massive, curated,
multimodal dataset of over 1.2 million images, encompassing different
modalities including Chest X-ray and Computed Tomography (CT) compiled from 10
public sources. A core technical contribution of our work is Guided Random
Resized Crops, a novel content-aware data augmentation strategy that biases
sampling towards anatomically relevant regions, overcoming the inefficiency of
standard cropping techniques on medical scans. We validate our model's
effectiveness by fine-tuning it on a diverse set of downstream diagnostic
tasks. Comprehensive experiments empirically demonstrate that MedDChest
significantly outperforms strong, publicly available ImageNet-pretrained
models. By establishing the superiority of large-scale, in-domain pre-training
combined with domain-specific data augmentation, MedDChest provides a powerful
and robust feature extractor that serves as a significantly better starting
point for a wide array of thoracic diagnostic tasks. The model weights will be
made publicly available to foster future research and applications.

</details>


### [17] [Near-Lossless 3D Voxel Representation Free from Iso-surface](https://arxiv.org/abs/2511.04029)
*Yihao Luo,Xianglong He,Chuanyu Pan,Yiwen Chen,Jiaqi Wu,Yangguang Li,Wanli Ouyang,Yuanming Hu,Guang Yang,ChoonHwai Yap*

Main category: cs.CV

TL;DR: 提出一种高保真稀疏体素化3D网格表示方法Faithful Contouring，不需要水密化或提取等操作，支持高分辨率，对复杂几何结构具有近乎无损的表达能力，实验显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于等值面的3D体素化表示高度依赖重网格化（如水密化或渲染），导致几何精度损失，难以兼顾高分辨率和真实细节。为提升三维重建与生成的表示效果，亟需突破。

Method: 提出Faithful Contouring方法——无需将网格转换为场函数，也无需等值面提取，通过稀疏体素结构实现高分辨率和近无损的几何保真。还设计了兼容的双模自编码器，用于高效细节重建。

Result: 该方法在直接表示上，距离误差达到10^{-5}量级；在网格重建上，Chamfer Distance减少93%，F-score提升35%，明显优于主流基线方法，且对贴图、编辑等操作更灵活。

Conclusion: Faithful Contouring在表示和重建3D网格方面兼具效率和高保真度，突破了传统方法的局限，是3D学习任务中优越的表达选择。

Abstract: Accurate and efficient voxelized representations of 3D meshes are the
foundation of 3D reconstruction and generation. However, existing
representations based on iso-surface heavily rely on water-tightening or
rendering optimization, which inevitably compromise geometric fidelity. We
propose Faithful Contouring, a sparse voxelized representation that supports
2048+ resolutions for arbitrary meshes, requiring neither converting meshes to
field functions nor extracting the isosurface during remeshing. It achieves
near-lossless fidelity by preserving sharpness and internal structures, even
for challenging cases with complex geometry and topology. The proposed method
also shows flexibility for texturing, manipulation, and editing. Beyond
representation, we design a dual-mode autoencoder for Faithful Contouring,
enabling scalable and detail-preserving shape reconstruction. Extensive
experiments show that Faithful Contouring surpasses existing methods in
accuracy and efficiency for both representation and reconstruction. For direct
representation, it achieves distance errors at the $10^{-5}$ level; for mesh
reconstruction, it yields a 93\% reduction in Chamfer Distance and a 35\%
improvement in F-score over strong baselines, confirming superior fidelity as a
representation for 3D learning tasks.

</details>


### [18] [A Hybrid Deep Learning Model for Robust Biometric Authentication from Low-Frame-Rate PPG Signals](https://arxiv.org/abs/2511.04037)
*Arfina Rahman,Mahesh Banavar*

Main category: cs.CV

TL;DR: 该论文提出了一种基于低帧率指尖PPG信号的高效生物识别身份认证系统，并借助深度学习模型CVT-ConvMixer-LSTM取得了98%的高准确率。


<details>
  <summary>Details</summary>
Motivation: PPG信号由于采集非侵入性、具有生物活体检测能力且适用于可穿戴设备，逐渐成为生物认证的重要研究对象。但其品质常受运动伪影、光照变化和个体差异影响，导致特征提取和分类具有挑战性。因此亟需提出一种对干扰鲁棒且易于实际部署的便捷认证方法。

Method: 作者利用CFIHSR公开数据集（46名受试者，14Hz采样），首先通过预处理（包括去基线漂移、PCA伪影抑制、带通滤波、傅里叶重采样和幅度归一化）清洗信号。之后将单维PPG段转为连续小波变换（CWT）得到的二维时间-频率图（scalogram）。模型架构采用CVT-ConvMixer-LSTM混合深度学习方法，整合卷积视觉Transformer、ConvMixer和LSTM网络，以提取空间和时序特征。

Result: 该系统在46名受试者上的身份认证准确率达98%，展现了模型对噪声和个体生理差异的鲁棒性。

Conclusion: 该系统具有高效率、可扩展性、活体检测等优势，适用于移动和嵌入式设备中的现实生物认证应用。

Abstract: Photoplethysmography (PPG) signals, which measure changes in blood volume in
the skin using light, have recently gained attention in biometric
authentication because of their non-invasive acquisition, inherent liveness
detection, and suitability for low-cost wearable devices. However, PPG signal
quality is challenged by motion artifacts, illumination changes, and
inter-subject physiological variability, making robust feature extraction and
classification crucial. This study proposes a lightweight and cost-effective
biometric authentication framework based on PPG signals extracted from
low-frame-rate fingertip videos. The CFIHSR dataset, comprising PPG recordings
from 46 subjects at a sampling rate of 14 Hz, is employed for evaluation. The
raw PPG signals undergo a standard preprocessing pipeline involving baseline
drift removal, motion artifact suppression using Principal Component Analysis
(PCA), bandpass filtering, Fourier-based resampling, and amplitude
normalization. To generate robust representations, each one-dimensional PPG
segment is converted into a two-dimensional time-frequency scalogram via the
Continuous Wavelet Transform (CWT), effectively capturing transient
cardiovascular dynamics. We developed a hybrid deep learning model, termed
CVT-ConvMixer-LSTM, by combining spatial features from the Convolutional Vision
Transformer (CVT) and ConvMixer branches with temporal features from a Long
Short-Term Memory network (LSTM). The experimental results on 46 subjects
demonstrate an authentication accuracy of 98%, validating the robustness of the
model to noise and variability between subjects. Due to its efficiency,
scalability, and inherent liveness detection capability, the proposed system is
well-suited for real-world mobile and embedded biometric security applications.

</details>


### [19] [Unveiling Deep Semantic Uncertainty Perception for Language-Anchored Multi-modal Vision-Brain Alignment](https://arxiv.org/abs/2511.04078)
*Zehui Feng,Chenqi Zhang,Mingru Wang,Minuo Wei,Shiwei Cheng,Cuntai Guan,Ting Han*

Main category: cs.CV

TL;DR: 本文提出Bratrix，一个创新的多模态语言锚定视觉-脑对齐框架，通过引入不确定性感知与两阶段训练方法，显著提升了EEG、MEG、fMRI等神经信号与视觉语义的对齐与解读能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法将神经活动与视觉嵌入直接对齐，但只基于视觉的信息往往无法捕捉深层语义维度，导致解释性和鲁棒性有限，且受制于受试者差异和视觉特征的复杂交织关系。

Method: Bratrix 框架将视觉刺激分解为分层的视觉和语言语义成分，同时将视觉和脑部表征投影到共享潜在空间中，实现视觉-语言与脑-语言的嵌入对齐。为应对神经信号噪声，引入了不确定性感知模块，并采用视觉/语言单模态预训练和多模态微调的两阶段训练策略。

Result: 在EEG、MEG和fMRI基准上大幅提升了检索、重建和描述性能，EEG 200类检索任务中对比最先进方法提升了14.3%。

Conclusion: Bratrix突破了原有视觉-脑信号对齐的局限性，实现了更深层次的语义解读和跨模态相关性，对神经信号解码与解释具有重要意义，相关代码和模型已公开。

Abstract: Unveiling visual semantics from neural signals such as EEG, MEG, and fMRI
remains a fundamental challenge due to subject variability and the entangled
nature of visual features. Existing approaches primarily align neural activity
directly with visual embeddings, but visual-only representations often fail to
capture latent semantic dimensions, limiting interpretability and deep
robustness. To address these limitations, we propose Bratrix, the first
end-to-end framework to achieve multimodal Language-Anchored Vision-Brain
alignment. Bratrix decouples visual stimuli into hierarchical visual and
linguistic semantic components, and projects both visual and brain
representations into a shared latent space, enabling the formation of aligned
visual-language and brain-language embeddings. To emulate human-like perceptual
reliability and handle noisy neural signals, Bratrix incorporates a novel
uncertainty perception module that applies uncertainty-aware weighting during
alignment. By leveraging learnable language-anchored semantic matrices to
enhance cross-modal correlations and employing a two-stage training strategy of
single-modality pretraining followed by multimodal fine-tuning, Bratrix-M
improves alignment precision. Extensive experiments on EEG, MEG, and fMRI
benchmarks demonstrate that Bratrix improves retrieval, reconstruction, and
captioning performance compared to state-of-the-art methods, specifically
surpassing 14.3% in 200-way EEG retrieval task. Code and model are available.

</details>


### [20] [Adversarial and Score-Based CT Denoising: CycleGAN vs Noise2Score](https://arxiv.org/abs/2511.04083)
*Abu Hanif Muhammad Syarubany*

Main category: cs.CV

TL;DR: 本文研究了在未配对和自监督场景下CT图像去噪的两种高效方法：基于CycleGAN的残差转换器和Noise2Score（N2S）得分匹配去噪器。在统一评测协议下，发现CycleGAN在特定设置下效果最佳，并显著提升图像质量；Noise2Score在无干净配对数据时表现鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有CT图像去噪方法依赖干净-噪声配对数据，实际采集困难。亟需无需配对数据的高效去噪方法。

Method: 评估两种高效无配对训练方法：CycleGAN（提出并调优U-Net骨干参数）和Noise2Score（无需干净样本的自监督方法），并用统一协议全面对比其性能。

Result: 优化设置下的CycleGAN将输入的PSNR/SSIM从34.66 dB/0.9234提升至38.913 dB/0.971，在Kaggle测试集上也表现突出。Noise2Score对极高噪声输入提升显著，尤其在无配对数据时依然有竞争力。

Conclusion: CycleGAN在图像最终质量上最优，但Noise2Score在没有干净-噪声对的情况下是鲁棒且有效的备选方案。源码已公开。

Abstract: We study CT image denoising in the unpaired and self-supervised regimes by
evaluating two strong, training-data-efficient paradigms: a CycleGAN-based
residual translator and a Noise2Score (N2S) score-matching denoiser. Under a
common evaluation protocol, a configuration sweep identifies a simple standard
U-Net backbone within CycleGAN (lambda_cycle = 30, lambda_iden = 2, ngf = ndf =
64) as the most reliable setting; we then train it to convergence with a longer
schedule. The selected CycleGAN improves the noisy input from 34.66 dB / 0.9234
SSIM to 38.913 dB / 0.971 SSIM and attains an estimated score of 1.9441 and an
unseen-set (Kaggle leaderboard) score of 1.9343. Noise2Score, while slightly
behind in absolute PSNR / SSIM, achieves large gains over very noisy inputs,
highlighting its utility when clean pairs are unavailable. Overall, CycleGAN
offers the strongest final image quality, whereas Noise2Score provides a robust
pair-free alternative with competitive performance. Source code is available at
https://github.com/hanifsyarubany/CT-Scan-Image-Denoising-using-CycleGAN-and-Noise2Score.

</details>


### [21] [When Swin Transformer Meets KANs: An Improved Transformer Architecture for Medical Image Segmentation](https://arxiv.org/abs/2511.04084)
*Nishchal Sapkota,Haoyan Shi,Yejia Zhang,Xianshi Ma,Bofang Zheng,Danny Z. Chen*

Main category: cs.CV

TL;DR: 本文提出将 Kolmogorov-Arnold Networks（KANs）与 Swin Transformer 编码器结合，构建了一种高效且数据利用率高的医学图像分割新架构 UKAST。该方法在多个2D和3D医学图像分割基准上达到了最新的精度，尤其是在数据稀缺场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割因解剖结构复杂和标注样本有限而具有挑战性。传统 CNN 方法善于提取局部特征，但难以建模全局依赖性；Transformer 虽然能够捕捉全局上下文，但对数据量和算力需求高。作者希望结合二者优点，提升数据利用效率和分割性能，尤其适应数据稀缺情况。

Method: 作者提出 UKAST 架构，将基于有理函数的 KANs 融入 Swin Transformer 编码器，并采用 Group Rational KANs（GR-KANs）机制，解决传统 spline-based KANs 的低效问题，形成更具表现力且数据高效的模型。该方法在保有相对较低计算复杂度（FLOPs）和极小参数量增长前提下，提升了表达能力。

Result: UKAST 在四个主流的2D与3D医学图像分割基准测试中获得了目前最优性能，评分超过现有 CNN 和 Transformer 架构，特别是在训练数据较少时也能保持优越精度。

Conclusion: KAN增强的 Transformer 架构可大幅提升医学图像分割的准确率和数据利用率；UKAST 为有限标注数据条件下的分割任务带来重要突破，表明该方法有望推动医学图像处理的新进展。

Abstract: Medical image segmentation is critical for accurate diagnostics and treatment
planning, but remains challenging due to complex anatomical structures and
limited annotated training data. CNN-based segmentation methods excel at local
feature extraction, but struggle with modeling long-range dependencies.
Transformers, on the other hand, capture global context more effectively, but
are inherently data-hungry and computationally expensive. In this work, we
introduce UKAST, a U-Net like architecture that integrates rational-function
based Kolmogorov-Arnold Networks (KANs) into Swin Transformer encoders. By
leveraging rational base functions and Group Rational KANs (GR-KANs) from the
Kolmogorov-Arnold Transformer (KAT), our architecture addresses the
inefficiencies of vanilla spline-based KANs, yielding a more expressive and
data-efficient framework with reduced FLOPs and only a very small increase in
parameter count compared to SwinUNETR. UKAST achieves state-of-the-art
performance on four diverse 2D and 3D medical image segmentation benchmarks,
consistently surpassing both CNN- and Transformer-based baselines. Notably, it
attains superior accuracy in data-scarce settings, alleviating the data-hungry
limitations of standard Vision Transformers. These results show the potential
of KAN-enhanced Transformers to advance data-efficient medical image
segmentation. Code is available at: https://github.com/nsapkota417/UKAST

</details>


### [22] [SpatialLock: Precise Spatial Control in Text-to-Image Synthesis](https://arxiv.org/abs/2511.04112)
*Biao Liu,Yuanzhi Liang*

Main category: cs.CV

TL;DR: 提出SpatialLock框架，通过结合感知信号和锚定信息，实现文本生成图像中对物体空间位置的精确控制，IOU得分超过0.9，达到新SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 尽管文本生成图像取得进步，但生成物体在图像中的空间位置控制依然困难，现有方法无法充分利用位置信息，导致空间布局理解不足。

Method: 提出SpatialLock框架，包含Position-Engaged Injection（PoI）和Position-Guided Learning（PoG）两大模块。PoI通过注意力层直接注入空间信息，提升模型对锚定信息的学习能力；PoG则利用感知监督进一步优化物体定位表现。

Result: SpatialLock在多个数据集上实现了高于0.9的IOU得分，物体定位精准，图像视觉质量提升，且刷新了现有定位SOTA。

Conclusion: SpatialLock提升了文本生成图像中对物体空间位置的精确控制能力，证明了联合利用感知信息和定位监督的有效性，具备优越的应用前景。

Abstract: Text-to-Image (T2I) synthesis has made significant advancements in recent
years, driving applications such as generating datasets automatically. However,
precise control over object localization in generated images remains a
challenge. Existing methods fail to fully utilize positional information,
leading to an inadequate understanding of object spatial layouts. To address
this issue, we propose SpatialLock, a novel framework that leverages perception
signals and grounding information to jointly control the generation of spatial
locations. SpatialLock incorporates two components: Position-Engaged Injection
(PoI) and Position-Guided Learning (PoG). PoI directly integrates spatial
information through an attention layer, encouraging the model to learn the
grounding information effectively. PoG employs perception-based supervision to
further refine object localization. Together, these components enable the model
to generate objects with precise spatial arrangements and improve the visual
quality of the generated images. Experiments show that SpatialLock sets a new
state-of-the-art for precise object positioning, achieving IOU scores above 0.9
across multiple datasets.

</details>


### [23] [Tortoise and Hare Guidance: Accelerating Diffusion Model Inference with Multirate Integration](https://arxiv.org/abs/2511.04117)
*Yunghee Lee,Byeonghyun Pak,Junwha Hong,Hoseong Kim*

Main category: cs.CV

TL;DR: 本文提出一种名为Tortoise and Hare Guidance（THG）的无需训练的扩散采样加速方法，在保持高保真生成效果的同时显著减少计算量。该方法显著减少了额外引导计算，降低了函数评估次数（NFE），效率优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在高质量图像生成领域表现出色，但采样过程通常计算量大，影响实时生成应用。现有基于CFG的加速方法仍未完全优化多余计算，因此需要开发更高效的采样加速策略。

Method: 作者通过将CFG无分类器引导(ODE)重新表述为多速率系统，发现误差对噪声估计和引导项有不同影响。根据误差分析，提出THG方法：噪声估计在细粒度网格上计算（“乌龟”方程），引导项在粗粒度网格上计算（“兔子”方程），并引入自适应步长采样器和引导尺度调度器以进一步提升稳定性和效率。

Result: 在保持生成质量几乎无损的情况下（ImageReward变化≤0.032），THG方法将函数评估次数减少了多达30%，并在相同算力预算下优于当前最先进训练自由加速方法。

Conclusion: 多速率表述为扩散采样器带来巨大优化空间，无需模型重新训练即可实现实时高质量图像合成。这为扩散模型在实际应用中提供了更高效的采样方式。

Abstract: In this paper, we propose Tortoise and Hare Guidance (THG), a training-free
strategy that accelerates diffusion sampling while maintaining high-fidelity
generation. We demonstrate that the noise estimate and the additional guidance
term exhibit markedly different sensitivity to numerical error by reformulating
the classifier-free guidance (CFG) ODE as a multirate system of ODEs. Our
error-bound analysis shows that the additional guidance branch is more robust
to approximation, revealing substantial redundancy that conventional solvers
fail to exploit. Building on this insight, THG significantly reduces the
computation of the additional guidance: the noise estimate is integrated with
the tortoise equation on the original, fine-grained timestep grid, while the
additional guidance is integrated with the hare equation only on a coarse grid.
We also introduce (i) an error-bound-aware timestep sampler that adaptively
selects step sizes and (ii) a guidance-scale scheduler that stabilizes large
extrapolation spans. THG reduces the number of function evaluations (NFE) by up
to 30% with virtually no loss in generation fidelity ($\Delta$ImageReward
$\leq$ 0.032) and outperforms state-of-the-art CFG-based training-free
accelerators under identical computation budgets. Our findings highlight the
potential of multirate formulations for diffusion solvers, paving the way for
real-time high-quality image synthesis without any model retraining. The source
code is available at https://github.com/yhlee-add/THG.

</details>


### [24] [BoRe-Depth: Self-supervised Monocular Depth Estimation with Boundary Refinement for Embedded Systems](https://arxiv.org/abs/2511.04388)
*Chang Liu,Juan Li,Sheng Zhang,Chang Liu,Jie Li,Xu Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种只包含8.7M参数的新型单目深度估计模型BoRe-Depth，在嵌入式设备上能够高效高质量地估算深度图，并极大提升了边界细节表现。


<details>
  <summary>Details</summary>
Motivation: 单目深度估计因其低成本受到广泛关注，但在嵌入式系统上，现有方法深度估计精度不高、物体边界模糊。为了解决这些问题，作者提出了新的模型和方法。

Method: 1) 提出增强型特征自适应融合模块（EFAF），自适应融合深度特征以加强边界细节表现；2) 在编码器中融合语义知识，以提升物体识别与边界感知能力；3) 将BoRe-Depth部署在NVIDIA Jetson Orin并在多数据集上测试。

Result: 模型参数仅8.7M，在Jetson Orin上推理速度达到50.7 FPS，显著优于已有轻量级模型，并在多个具有挑战性的数据集上取得了更好的性能表现，同时提供了详细的消融实验。

Conclusion: BoRe-Depth在保证嵌入式设备推理效率的前提下，实现了更精确、更清晰的深度估计与边界刻画，优于当前轻量化模型，推广性和实用性强。

Abstract: Depth estimation is one of the key technologies for realizing 3D perception
in unmanned systems. Monocular depth estimation has been widely researched
because of its low-cost advantage, but the existing methods face the challenges
of poor depth estimation performance and blurred object boundaries on embedded
systems. In this paper, we propose a novel monocular depth estimation model,
BoRe-Depth, which contains only 8.7M parameters. It can accurately estimate
depth maps on embedded systems and significantly improves boundary quality.
Firstly, we design an Enhanced Feature Adaptive Fusion Module (EFAF) which
adaptively fuses depth features to enhance boundary detail representation.
Secondly, we integrate semantic knowledge into the encoder to improve the
object recognition and boundary perception capabilities. Finally, BoRe-Depth is
deployed on NVIDIA Jetson Orin, and runs efficiently at 50.7 FPS. We
demonstrate that the proposed model significantly outperforms previous
lightweight models on multiple challenging datasets, and we provide detailed
ablation studies for the proposed methods. The code is available at
https://github.com/liangxiansheng093/BoRe-Depth.

</details>


### [25] [Text to Sketch Generation with Multi-Styles](https://arxiv.org/abs/2511.04123)
*Tengjie Li,Shikui Tu,Lei Xu*

Main category: cs.CV

TL;DR: 本文提出了一种无需重新训练的新型基于扩散模型的草图生成框架M3S，实现了对草图风格的精确文本和参考图引导控制，支持多风格组合，提升了风格对齐和生成灵活度。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型虽推动了草图生成的发展，但专门方法多侧重于通用生成，缺乏对于草图风格的精细可控能力，同时多风格融合和内容泄漏等问题也未得到有效解决。

Method: 该方法基于扩散模型，无需额外训练。通过文本提示词和参考草图双重方式显式控制风格，引入线性平滑和风格-内容引导机制，将参考草图特征作为辅助信息整合，有效减少内容泄漏。多风格场景下，以联合AdaIN模块融合多参考草图特征，实现可控多风格生成。

Result: 大量实验显示该方法生成的草图不仅风格精准对齐，且在风格控制灵活性和生成质量上均优于现有方法，尤其是在参考图与目标草图结构差异较大时效果突出。

Conclusion: M3S框架无需训练即可实现高质量、可控的草图风格生成，为视觉-语言领域草图合成提供了一种有效的新途径。

Abstract: Recent advances in vision-language models have facilitated progress in sketch
generation. However, existing specialized methods primarily focus on generic
synthesis and lack mechanisms for precise control over sketch styles. In this
work, we propose a training-free framework based on diffusion models that
enables explicit style guidance via textual prompts and referenced style
sketches. Unlike previous style transfer methods that overwrite key and value
matrices in self-attention, we incorporate the reference features as auxiliary
information with linear smoothing and leverage a style-content guidance
mechanism. This design effectively reduces content leakage from reference
sketches and enhances synthesis quality, especially in cases with low
structural similarity between reference and target sketches. Furthermore, we
extend our framework to support controllable multi-style generation by
integrating features from multiple reference sketches, coordinated via a joint
AdaIN module. Extensive experiments demonstrate that our approach achieves
high-quality sketch generation with accurate style alignment and improved
flexibility in style control. The official implementation of M3S is available
at https://github.com/CMACH508/M3S.

</details>


### [26] [Automated Tennis Player and Ball Tracking with Court Keypoints Detection (Hawk Eye System)](https://arxiv.org/abs/2511.04126)
*Venkata Manikanta Desu,Syed Fawaz Ali*

Main category: cs.CV

TL;DR: 本文提出了一个自动化网球比赛分析的完整流水线系统，集成了多种深度学习模型，实现了对球员和网球的实时检测与追踪，并进行球场空间关键点定位，最终输出标注视频和详细比赛分析数据。


<details>
  <summary>Details</summary>
Motivation: 传统网球比赛分析依赖大量人工工作，实时性差，细节度有限，无法满足教练、选手及媒体对高效、精准分析的需求。随着深度学习发展，有必要开发自动化、智能化的分析系统。

Method: 本研究集成了多个深度学习模型：采用YOLOv8用于球员检测，自定义训练的YOLOv5用于网球追踪，ResNet50架构用于网球场关键点检测。系统能实时检测并追踪球员及球，对球场进行空间参考定位，综合分析玩家行动轨迹、球速、击球准确性及反应时间等。

Result: 实验结果显示，该系统在多种场地条件和比赛场景下，均表现出强健的性能。可输出包含标记和详细分析指标的视频，具备较高实用性和准确性。

Conclusion: 该流水线系统为网球比赛分析提供了高效、自动、可扩展的新方法，为教练、运动员和转播方带来具体可操作的数据和洞见，具有广泛推广应用前景。

Abstract: This study presents a complete pipeline for automated tennis match analysis.
Our framework integrates multiple deep learning models to detect and track
players and the tennis ball in real time, while also identifying court
keypoints for spatial reference. Using YOLOv8 for player detection, a
custom-trained YOLOv5 model for ball tracking, and a ResNet50-based
architecture for court keypoint detection, our system provides detailed
analytics including player movement patterns, ball speed, shot accuracy, and
player reaction times. The experimental results demonstrate robust performance
in varying court conditions and match scenarios. The model outputs an annotated
video along with detailed performance metrics, enabling coaches, broadcasters,
and players to gain actionable insights into the dynamics of the game.

</details>


### [27] [DMSORT: An efficient parallel maritime multi-object tracking architecture for unmanned vessel platforms](https://arxiv.org/abs/2511.04128)
*Shengyu Tang,Zeyuan Lu,Jiazhi Dong,Changdong Yu,Xiaoyu Wang,Yaohui Lyu,Weihao Xia*

Main category: cs.CV

TL;DR: 本文提出了一种高效的海事多目标跟踪方法DMSORT，能够在复杂海洋环境下有效应对相机抖动、噪声和遮挡问题，显著提升了跟踪的鲁棒性与准确性。


<details>
  <summary>Details</summary>
Motivation: 海洋环境复杂，经常发生相机运动和视觉退化，现有多目标跟踪系统在这些情况下效果不佳。因此亟需提升跟踪方法对于动态相机、遮挡和抖动等难题的鲁棒性。

Method: DMSORT框架中，设置了双分支结构：一条分支利用集成了可逆柱状检测网络（RCDN）的检测与重识别模块，实现多层次特征提取和全局外观信息表达；另一分支通过投影变换与Kalman滤波，对平台运动与目标自有运动进行解耦，实现运动补偿和轨迹稳定。最终，采用聚类优化的特征融合模块将运动与外观特征结合，确保在噪声和遮挡下的身份一致性。

Result: 在Singapore Maritime Dataset上，DMSORT实现了目前最优的多目标跟踪表现，尤其在ReID基础的MOT框架中运行速度最快，在身份一致性、抗抖动和抗遮挡方面表现优异。

Conclusion: DMSORT不仅大幅提升了海事多目标跟踪的准确性与稳定性，还兼具实时高效运行能力，可为智能航行与海上监测等应用提供有力技术支撑。

Abstract: Accurate perception of the marine environment through robust multi-object
tracking (MOT) is essential for ensuring safe vessel navigation and effective
maritime surveillance. However, the complicated maritime environment often
causes camera motion and subsequent visual degradation, posing significant
challenges to MOT. To address this challenge, we propose an efficient
Dual-branch Maritime SORT (DMSORT) method for maritime MOT. The core of the
framework is a parallel tracker with affine compensation, which incorporates an
object detection and re-identification (ReID) branch, along with a dedicated
branch for dynamic camera motion estimation. Specifically, a Reversible
Columnar Detection Network (RCDN) is integrated into the detection module to
leverage multi-level visual features for robust object detection. Furthermore,
a lightweight Transformer-based appearance extractor (Li-TAE) is designed to
capture global contextual information and generate robust appearance features.
Another branch decouples platform-induced and target-intrinsic motion by
constructing a projective transformation, applying platform-motion compensation
within the Kalman filter, and thereby stabilizing true object trajectories.
Finally, a clustering-optimized feature fusion module effectively combines
motion and appearance cues to ensure identity consistency under noise,
occlusion, and drift. Extensive evaluations on the Singapore Maritime Dataset
demonstrate that DMSORT achieves state-of-the-art performance. Notably, DMSORT
attains the fastest runtime among existing ReID-based MOT frameworks while
maintaining high identity consistency and robustness to jitter and occlusion.
Code is available at:
https://github.com/BiscuitsLzy/DMSORT-An-efficient-parallel-maritime-multi-object-tracking-architecture-.

</details>


### [28] [Learning from Online Videos at Inference Time for Computer-Use Agents](https://arxiv.org/abs/2511.04137)
*Yujian Liu,Ze Wang,Hao Chen,Ximeng Sun,Xiaodong Yu,Jialian Wu,Jiang Liu,Emad Barsoum,Zicheng Liu,Shiyu Chang*

Main category: cs.CV

TL;DR: 本文提出了一种新的系统，让计算机代理能够在执行任务时学习在线视频教程，将其转化为结构化的演示轨迹，并动态在推理时作为上下文指导，实现比只能用文本教程的代理更优的效能。


<details>
  <summary>Details</summary>
Motivation: 尽管计算机代理在自动化繁琐任务方面已有较大进展，但在需要特定领域知识和多步骤操作时，仍然不如人类。因为人类能通过观看视频教程，快速学习关键步骤。因此亟需让代理像人类一样，从丰富的视频资源中有效汲取操作知识。

Method: 提出了一个完整框架：自动检索、筛选教程视频，将其转化为结构化的演示轨迹（包括动作分段、目标描述），并在推理时通过两阶段机制动态选择最有用的操作轨迹加入agent的上下文，实现有效引导每一步操作。核心技术包括视觉语言模型（VLM）推断UI动作、视频分割与目标分配，以及动态轨迹挑选。

Result: 在两个广泛使用的基准测试上，该框架始终优于强大的基础代理和仅使用文本教程的变体。实验和分析显示，轨迹分割与选择、动作筛选和视觉信息对效果有重要提升作用。

Conclusion: 丰富的在线视频可系统化转化为操作性指导，显著优化计算机代理推理表现。所提出方法为代理学习和泛化提供了新思路，并证明视觉-语言相关知识的有效融合是提升自动化智能的重要途径。

Abstract: Computer-use agents can operate computers and automate laborious tasks, but
despite recent rapid progress, they still lag behind human users, especially
when tasks require domain-specific procedural knowledge about particular
applications, platforms, and multi-step workflows. Humans can bridge this gap
by watching video tutorials: we search, skim, and selectively imitate short
segments that match our current subgoal. In this paper, we study how to enable
computer-use agents to learn from online videos at inference time effectively.
We propose a framework that retrieves and filters tutorial videos, converts
them into structured demonstration trajectories, and dynamically selects
trajectories as in-context guidance during execution. Particularly, using a
VLM, we infer UI actions, segment videos into short subsequences of actions,
and assign each subsequence a textual objective. At inference time, a two-stage
selection mechanism dynamically chooses a single trajectory to add in context
at each step, focusing the agent on the most helpful local guidance for its
next decision. Experiments on two widely used benchmarks show that our
framework consistently outperforms strong base agents and variants that use
only textual tutorials or transcripts. Analyses highlight the importance of
trajectory segmentation and selection, action filtering, and visual
information, suggesting that abundant online videos can be systematically
distilled into actionable guidance that improves computer-use agents at
inference time. Our code is available at
https://github.com/UCSB-NLP-Chang/video_demo.

</details>


### [29] [Seeing Straight: Document Orientation Detection for Efficient OCR](https://arxiv.org/abs/2511.04161)
*Suranjan Goswami,Abhinav Ravi,Raja Kolla,Ali Faraz,Shaharukh Khan,Akash,Chandra Khatri,Shubham Agarwal*

Main category: cs.CV

TL;DR: 本文提出了一种高效、轻量的文档图像旋转分类方法，基于Phi-3.5-Vision模型，并构建了新的OCR鲁棒性基准ORC-Rotation-Bench，对旋转校正和OCR性能提升进行了系统评估。


<details>
  <summary>Details</summary>
Motivation: 现实中，扫描或拍照片的文档常因相机基向不正确导致旋转，影响OCR等后续任务的准确性，所以急需高效、可靠的文档图像旋转校正方法。

Method: 提出OCR-Rotation-Bench (ORB)基准，包含ORB-En（英文）和ORB-Indic（11种印地语低资源语言）。并设计了一个基于Phi-3.5-Vision视觉编码器的旋转分类器，通过动态裁剪图像并针对四类旋转（0/90/180/270度）任务微调。

Result: 该方法在两个数据集上分别达到了96%和92%的旋转识别准确率。在“真实世界”模拟环境下，模块还能显著提升OCR系统性能：闭源模型提升最多14%，开源模型提升达4倍。

Conclusion: 所提出的旋转校正方案不仅在多语言、多场景下效果优异，还对下游OCR有显著提升作用，为实际文档处理提供了强有力支持。

Abstract: Despite significant advances in document understanding, determining the
correct orientation of scanned or photographed documents remains a critical
pre-processing step in the real world settings. Accurate rotation correction is
essential for enhancing the performance of downstream tasks such as Optical
Character Recognition (OCR) where misalignment commonly arises due to user
errors, particularly incorrect base orientations of the camera during capture.
In this study, we first introduce OCR-Rotation-Bench (ORB), a new benchmark for
evaluating OCR robustness to image rotations, comprising (i) ORB-En, built from
rotation-transformed structured and free-form English OCR datasets, and (ii)
ORB-Indic, a novel multilingual set spanning 11 Indic mid to low-resource
languages. We also present a fast, robust and lightweight rotation
classification pipeline built on the vision encoder of Phi-3.5-Vision model
with dynamic image cropping, fine-tuned specifically for 4-class rotation task
in a standalone fashion. Our method achieves near-perfect 96% and 92% accuracy
on identifying the rotations respectively on both the datasets. Beyond
classification, we demonstrate the critical role of our module in boosting OCR
performance: closed-source (up to 14%) and open-weights models (up to 4x) in
the simulated real-world setting.

</details>


### [30] [Systematic Evaluation of Preprocessing Techniques for Accurate Image Registration in Digital Pathology](https://arxiv.org/abs/2511.04171)
*Fatemehzahra Darzi,Rodrigo Escobar Diaz Guerrero,Thomas Bocklitz*

Main category: cs.CV

TL;DR: 本文研究了多种颜色变换技术对不同染色方式下病理图像配准效果的影响，发现CycleGAN颜色变换在配准误差方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 在数字病理学中，不同染色或成像模态的图像对比分析很重要，但由于成像方式差异，直接配准难度大。研究者想找出哪种颜色变换能最大程度提升不同模态图像配准的准确性，以支持后续病理分析。

Method: 作者使用了20对不同模态（H&E染色与非线性多模态）组织切片图像。每对图像先经过若干预处理步骤，包括多种颜色变换（CycleGAN, Macenko, Reinhard, Vahadane）、反色、对比度调整、强度归一化和去噪。所有图像用VALIS方法配准（先刚性，后多步非刚性，在低/高分辨率下进行）。评估采用rTRE（相对目标配准误差），并对每种方法报告MMrTRE和AMrTRE，还用10个人工标注关键点进行了自定义评估。

Result: 实验分为原始和反色两种场景，CycleGAN色彩变换在配准精度（rTRE）上表现最好，其他方法误差较高。

Conclusion: 在数字病理多模态图像配准前进行颜色变换（尤其是CycleGAN方法）能显著提升配准准确性，有助于更可靠的跨模态病理分析。

Abstract: Image registration refers to the process of spatially aligning two or more
images by mapping them into a common coordinate system, so that corresponding
anatomical or tissue structures are matched across images. In digital
pathology, registration enables direct comparison and integration of
information from different stains or imaging modalities, sup-porting
applications such as biomarker analysis and tissue reconstruction. Accurate
registration of images from different modalities is an essential step in
digital pathology. In this study, we investigated how various color
transformation techniques affect image registration between hematoxylin and
eosin (H&E) stained images and non-linear multimodal images. We used a dataset
of 20 tissue sample pairs, with each pair undergoing several preprocessing
steps, including different color transformation (CycleGAN, Macenko, Reinhard,
Vahadane), inversion, contrast adjustment, intensity normalization, and
denoising. All images were registered using the VALIS registration method,
which first applies rigid registration and then performs non-rigid registration
in two steps on both low and high-resolution images. Registration performance
was evaluated using the relative Target Registration Error (rTRE). We reported
the median of median rTRE values (MMrTRE) and the average of median rTRE values
(AMrTRE) for each method. In addition, we performed a custom point-based
evaluation using ten manually selected key points. Registration was done
separately for two scenarios, using either the original or inverted multimodal
images. In both scenarios, CycleGAN color transformation achieved the lowest
registration errors, while the other methods showed higher errors. These
findings show that applying color transformation before registration improves
alignment between images from different modalities and supports more reliable
analysis in digital pathology.

</details>


### [31] [Covariance Descriptors Meet General Vision Encoders: Riemannian Deep Learning for Medical Image Classification](https://arxiv.org/abs/2511.04190)
*Josef Mayr,Anna Reithmeir,Maxime Di Folco,Julia A. Schnabel*

Main category: cs.CV

TL;DR: 本文探讨协方差描述符在医学图像分类中的性能，特别关注结合预训练视觉编码器和SPDNet，结果显示该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在医学图像领域，协方差描述符虽具有潜力，但未被充分研究。作者旨在评估其在医学图像分类中的有效性，并与当前最优方法比较，寻找更优解决方案。

Method: 作者提出用两种预训练通用视觉编码器（DINOv2和MedSAM）提取特征，再基于这些特征构建协方差描述符，并与手工特征的协方差描述符对比，最终将这些特征输入SPDNet网络。在MedMNIST基准的11个子数据集上进行二分类和多分类实验。

Result: 实验结果显示，用GVE特征（尤其DINOv2）构建的协方差描述符在各类任务中性能优于手工特征；同时，将DINOv2特征与SPDNet结合，整体优于同类最佳方法。

Conclusion: 预训练视觉编码器生成特征与协方差描述符结合，对医学图像分析有显著提升潜力，相关方法值得进一步探索。

Abstract: Covariance descriptors capture second-order statistics of image features.
They have shown strong performance in general computer vision tasks, but remain
underexplored in medical imaging. We investigate their effectiveness for both
conventional and learning-based medical image classification, with a particular
focus on SPDNet, a classification network specifically designed for symmetric
positive definite (SPD) matrices. We propose constructing covariance
descriptors from features extracted by pre-trained general vision encoders
(GVEs) and comparing them with handcrafted descriptors. Two GVEs - DINOv2 and
MedSAM - are evaluated across eleven binary and multi-class datasets from the
MedMNSIT benchmark. Our results show that covariance descriptors derived from
GVE features consistently outperform those derived from handcrafted features.
Moreover, SPDNet yields superior performance to state-of-the-art methods when
combined with DINOv2 features. Our findings highlight the potential of
combining covariance descriptors with powerful pretrained vision encoders for
medical image analysis.

</details>


### [32] [AStF: Motion Style Transfer via Adaptive Statistics Fusor](https://arxiv.org/abs/2511.04192)
*Hanmo Chen,Chenghao Xu,Jiexi Yan,Cheng Deng*

Main category: cs.CV

TL;DR: 作者提出了一种用于人类动作风格迁移的新方法AStF，通过引入偏度(skewness)和峰度(kurtosis)等高阶统计量，提升了风格转移的表现，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像和动作风格迁移大多基于均值和方差，无法充分捕捉动作数据复杂的动态模式与时空相关性，因此需要更丰富的统计特征来改善风格转移效果。

Method: 作者提出了自适应统计融合器（AStF），包含风格解耦模块（SDM）和高阶多统计注意力模块（HOS-Attn），并结合动作一致性正则化鉴别器（MCR）进行训练。该方法在风格分析中引入了偏度和峰度两种高阶统计特征。

Result: 实验表明，AStF能够更全面地建模动态风格的时空统计模式，在多项动作风格迁移任务上超越了现有技术。

Conclusion: 通过整合高阶统计特征，AStF方法有效提升了动作风格迁移的真实感和表现力，证明了其在动作数据领域的创新性和有效性。

Abstract: Human motion style transfer allows characters to appear less rigidity and
more realism with specific style. Traditional arbitrary image style transfer
typically process mean and variance which is proved effective. Meanwhile,
similar methods have been adapted for motion style transfer. However, due to
the fundamental differences between images and motion, relying on mean and
variance is insufficient to fully capture the complex dynamic patterns and
spatiotemporal coherence properties of motion data. Building upon this, our key
insight is to bring two more coefficient, skewness and kurtosis, into the
analysis of motion style. Specifically, we propose a novel Adaptive Statistics
Fusor (AStF) which consists of Style Disentanglement Module (SDM) and
High-Order Multi-Statistics Attention (HOS-Attn). We trained our AStF in
conjunction with a Motion Consistency Regularization (MCR) discriminator.
Experimental results show that, by providing a more comprehensive model of the
spatiotemporal statistical patterns inherent in dynamic styles, our proposed
AStF shows proficiency superiority in motion style transfers over
state-of-the-arts. Our code and model are available at
https://github.com/CHMimilanlan/AStF.

</details>


### [33] [MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection](https://arxiv.org/abs/2511.04255)
*Marawan Elbatel,Anbang Wang,Keyuan Liu,Kaouther Mouheb,Enrique Almar-Munoz,Lizhuo Lin,Yanqi Yang,Karim Lekadir,Xiaomeng Li*

Main category: cs.CV

TL;DR: 该论文提出并验证了一种将以人体为中心的大型基础模型（Sapiens）迁移用于医学影像解剖学标注检测的新方法，显著优于现有通用和专用模型。


<details>
  <summary>Details</summary>
Motivation: 传统的解剖学标注检测依赖于医学领域专用模型，而视觉基础模型的兴起为这一任务带来新机遇。作者认为人体姿态估计模型中的空间定位能力未被充分利用于医学影像领域。

Method: 将以人体为中心的基础模型Sapiens通过多数据集预训练方式迁移至医学影像领域，提出名为MedSapiens的新模型，并在多个数据集上对比通用和专用的现有解剖标注检测方法。

Result: MedSapiens在平均成功检测率（SDR）上相比通用模型最高提升5.26%，相比专用模型最高提升21.81%；在少样本设定下也优于现有少样本方法2.69%。

Conclusion: 人体姿态基础模型能为医学影像解剖学标注检测带来强先验，明显提高检测性能。融合通用基础模型能促进医学标注检测的发展，这一潜力尚未被充分发掘。

Abstract: This paper does not introduce a novel architecture; instead, it revisits a
fundamental yet overlooked baseline: adapting human-centric foundation models
for anatomical landmark detection in medical imaging. While landmark detection
has traditionally relied on domain-specific models, the emergence of
large-scale pre-trained vision models presents new opportunities. In this
study, we investigate the adaptation of Sapiens, a human-centric foundation
model designed for pose estimation, to medical imaging through multi-dataset
pretraining, establishing a new state of the art across multiple datasets. Our
proposed model, MedSapiens, demonstrates that human-centric foundation models,
inherently optimized for spatial pose localization, provide strong priors for
anatomical landmark detection, yet this potential has remained largely
untapped. We benchmark MedSapiens against existing state-of-the-art models,
achieving up to 5.26% improvement over generalist models and up to 21.81%
improvement over specialist models in the average success detection rate (SDR).
To further assess MedSapiens adaptability to novel downstream tasks with few
annotations, we evaluate its performance in limited-data settings, achieving
2.69% improvement over the few-shot state of the art in SDR. Code and model
weights are available at https://github.com/xmed-lab/MedSapiens .

</details>


### [34] [Proto-LeakNet: Towards Signal-Leak Aware Attribution in Synthetic Human Face Imagery](https://arxiv.org/abs/2511.04260)
*Claudio Giusti,Luca Guarnera,Sebastiano Battiato*

Main category: cs.CV

TL;DR: 本文提出Proto-LeakNet方法，通过利用扩散模型输出中的信号泄露，在特征嵌入空间实现了对合成图像来源的可解释鉴别和开集检测，有效提升了深度伪造检验的准确率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 合成图像和深度伪造技术日益成熟，给图像溯源和真伪验证带来了巨大挑战。现有方法对未知生成器和后处理的鲁棒性较弱，急需更精准、可解释的溯源与检测方法，尤其要能泛化到未见过的生成模型。

Method: 提出Proto-LeakNet框架，在扩散模型的隐空间域进行操作。该方法首先通过部分正向扩散重现残存的生成器特征信号，引入时序注意力编码器以聚合多步隐特征，并采用加权原型头，结构化特征嵌入，实现闭集分类和基于密度的开集识别。该网络只需在关闭集数据上训练，无需对新生成器重新训练。

Result: Proto-LeakNet在仅用闭集数据训练情况下，Macro AUC达到98.13%，优于现有最先进方法。该方法在已知和未知生成器之间实现了良好区分，且对常见后处理具有鲁棒性，体现了优越的泛化能力与解释性。

Conclusion: 在隐空间建模信号泄露，能实现高效、可解释的合成图像及深度伪造取证。Proto-LeakNet不仅提升了检测精度，还兼具开集识别与泛化能力，是AI图像溯源与伪造检测领域的有力工具。

Abstract: The growing sophistication of synthetic image and deepfake generation models
has turned source attribution and authenticity verification into a critical
challenge for modern computer vision systems. Recent studies suggest that
diffusion pipelines unintentionally imprint persistent statistical traces,
known as signal leaks, within their outputs, particularly in latent
representations. Building on this observation, we propose Proto-LeakNet, a
signal-leak-aware and interpretable attribution framework that integrates
closed-set classification with a density-based open-set evaluation on the
learned embeddings, enabling analysis of unseen generators without retraining.
Operating in the latent domain of diffusion models, our method re-simulates
partial forward diffusion to expose residual generator-specific cues. A
temporal attention encoder aggregates multi-step latent features, while a
feature-weighted prototype head structures the embedding space and enables
transparent attribution. Trained solely on closed data and achieving a Macro
AUC of 98.13%, Proto-LeakNet learns a latent geometry that remains robust under
post-processing, surpassing state-of-the-art methods, and achieves strong
separability between known and unseen generators. These results demonstrate
that modeling signal-leak bias in latent space enables reliable and
interpretable AI-image and deepfake forensics. The code for the whole work will
be available upon submission.

</details>


### [35] [DINOv2 Driven Gait Representation Learning for Video-Based Visible-Infrared Person Re-identification](https://arxiv.org/abs/2511.04281)
*Yujie Yang,Shuang Li,Jun Ye,Neng Dong,Fan Li,Huafeng Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于DINOv2驱动的人体步态学习方法（DinoGRL），用于提升视频可见-红外跨模态行人再识别任务的表现。该方法通过结合外观和步态特征，实现更鲁棒的序列级行人识别，实验证明其优于现有同类技术。


<details>
  <summary>Details</summary>
Motivation: 现有的视频可见-红外行人再识别方法主要关注模态无关的视觉特征，忽略了跨模态鲁棒且富含时序动态信息的步态特征，导致在建模时空一致性方面存在局限，影响跨模态匹配性能。

Method: 提出DinoGRL框架，利用DINOv2的视觉先验学习补充于外观的信息的步态特征，引入语义感知剪影和步态学习（SASGL）模块，通过DINOv2丰富的语义信息优化步态特征，同时与主干ReID任务联合训练。此外设计渐进式双向多粒度增强（PBMGE）模块，使步态与外观特征跨多个空间尺度进行交互和细化，从而提升最终的全局辨别能力。

Result: 在HITSZ-VCM和BUPT常用行人再识别数据集上进行大量实验，结果显示所提方法在各项指标上均显著超过当前最优技术。

Conclusion: 结合DINOv2等强大视觉先验，跨模态地集成外观与步态特征，是提升视频跨模态行人再识别效果的有效途径。所提DinoGRL框架具备强大泛化能力和辨别力，对跨模态视频检索领域具有实际应用价值。

Abstract: Video-based Visible-Infrared person re-identification (VVI-ReID) aims to
retrieve the same pedestrian across visible and infrared modalities from video
sequences. Existing methods tend to exploit modality-invariant visual features
but largely overlook gait features, which are not only modality-invariant but
also rich in temporal dynamics, thus limiting their ability to model the
spatiotemporal consistency essential for cross-modal video matching. To address
these challenges, we propose a DINOv2-Driven Gait Representation Learning
(DinoGRL) framework that leverages the rich visual priors of DINOv2 to learn
gait features complementary to appearance cues, facilitating robust
sequence-level representations for cross-modal retrieval. Specifically, we
introduce a Semantic-Aware Silhouette and Gait Learning (SASGL) model, which
generates and enhances silhouette representations with general-purpose semantic
priors from DINOv2 and jointly optimizes them with the ReID objective to
achieve semantically enriched and task-adaptive gait feature learning.
Furthermore, we develop a Progressive Bidirectional Multi-Granularity
Enhancement (PBMGE) module, which progressively refines feature representations
by enabling bidirectional interactions between gait and appearance streams
across multiple spatial granularities, fully leveraging their complementarity
to enhance global representations with rich local details and produce highly
discriminative features. Extensive experiments on HITSZ-VCM and BUPT datasets
demonstrate the superiority of our approach, significantly outperforming
existing state-of-the-art methods.

</details>


### [36] [FastGS: Training 3D Gaussian Splatting in 100 Seconds](https://arxiv.org/abs/2511.04283)
*Shiwei Ren,Tianci Wen,Yongchun Fang,Biao Lu*

Main category: cs.CV

TL;DR: FastGS是一种高效且通用的3D高斯点云加速框架，通过基于多视角一致性的稠密化与剪枝策略，显著提升了训练速度并减少了冗余高斯点，同时保持了高质量的渲染效果。


<details>
  <summary>Details</summary>
Motivation: 现有的3DGS加速方法在训练过程中无法有效调节高斯点的数量，导致浪费了计算资源和训练时间。因此，作者希望能够在保证渲染质量的前提下，提高训练效率，开发更实用的3DGS加速方案。

Method: 本文提出了FastGS框架，采用创新的基于多视角一致性的高斯点稠密化和剪枝策略，评估每个高斯点的重要性以动态调整点的数量，完全摆脱人工设置预算机制。该策略在训练过程中持续优化高斯点的分布与数量。

Result: 实验表明，FastGS在Mip-NeRF 360数据集上实现了3.32倍的训练加速且保留了与DashGaussian相当的渲染效果，在Deep Blending数据集上比普通3DGS方法快了15.45倍。多个数据集和任务上的表现都优于现有技术，训练速度提升2-7倍。

Conclusion: FastGS能高效且通用地适用于各种3D重建任务，包括动态场景、表面重建、稀疏视图、超大场景重建与SLAM，平衡了训练速度和渲染效果，有望推动3DGS技术的实际应用。

Abstract: The dominant 3D Gaussian splatting (3DGS) acceleration methods fail to
properly regulate the number of Gaussians during training, causing redundant
computational time overhead. In this paper, we propose FastGS, a novel, simple,
and general acceleration framework that fully considers the importance of each
Gaussian based on multi-view consistency, efficiently solving the trade-off
between training time and rendering quality. We innovatively design a
densification and pruning strategy based on multi-view consistency, dispensing
with the budgeting mechanism. Extensive experiments on Mip-NeRF 360, Tanks &
Temples, and Deep Blending datasets demonstrate that our method significantly
outperforms the state-of-the-art methods in training speed, achieving a
3.32$\times$ training acceleration and comparable rendering quality compared
with DashGaussian on the Mip-NeRF 360 dataset and a 15.45$\times$ acceleration
compared with vanilla 3DGS on the Deep Blending dataset. We demonstrate that
FastGS exhibits strong generality, delivering 2-7$\times$ training acceleration
across various tasks, including dynamic scene reconstruction, surface
reconstruction, sparse-view reconstruction, large-scale reconstruction, and
simultaneous localization and mapping. The project page is available at
https://fastgs.github.io/

</details>


### [37] [Vision Foundation Models in Agriculture: Toward Domain-Specific Adaptation for Weed Herbicide Trials Assessment](https://arxiv.org/abs/2511.04288)
*Leire Benito-Del-Valle,Artzai Picón,Daniel Mugica,Manuel Ramos,Eva Portillo,Javier Romero,Carlos Javier Jimenez,Ramón Navarra-Mestre*

Main category: cs.CV

TL;DR: 本文提出了一种针对除草剂试验场景的领域专用视觉基础模型，能够更准确地识别植物种类并评估除草剂损伤类型，在多种环境条件下表现优异，减少人工标注需求。


<details>
  <summary>Details</summary>
Motivation: 除草剂田间试验需要在复杂自然环境中精确区分植物种类及损伤程度，而通用视觉基础模型在此类农业精细化任务中的表现有限。因此，亟需开发专门优化用于农业领域、特别是除草剂试验的视觉模型，以提升精度和泛化能力。

Method: 作者采用自监督学习方法，基于大规模精心挑选的农业数据集对通用视觉基础模型进行领域适应预训练，得到可迁移的特征表示，并在除草剂试验相关图像任务上进行了性能评估。

Result: 领域专用模型在植物种类识别和损伤分类任务中的F1分数均显著优于通用模型，在新地点、不同时间等未见环境下提升更大，在遥感等领域转移任务中同样保持较高水平；且在标注效率上，领域专用模型以少量标注样本获得更高表现。

Conclusion: 面向特定农业场景进行预训练的领域专用视觉基础模型能兼具准确性与泛化性，极大减少人工标注负担，为大规模自动化除草剂试验分析提供了可行方案。

Abstract: Herbicide field trials require accurate identification of plant species and
assessment of herbicide-induced damage across diverse environments. While
general-purpose vision foundation models have shown promising results in
complex visual domains, their performance can be limited in agriculture, where
fine-grained distinctions between species and damage types are critical.
  In this work, we adapt a general-purpose vision foundation model to herbicide
trial characterization. Trained using a self-supervised learning approach on a
large, curated agricultural dataset, the model learns rich and transferable
representations optimized for herbicide trials images.
  Our domain-specific model significantly outperforms the best general-purpose
foundation model in both species identification (F1 score improvement from 0.91
to 0.94) and damage classification (from 0.26 to 0.33). Under unseen conditions
(new locations and other time), it achieves even greater gains (species
identification from 0.56 to 0.66; damage classification from 0.17 to 0.27). In
domain-shift scenarios, such as drone imagery, it maintains strong performance
(species classification from 0.49 to 0.60).
  Additionally, we show that domain-specific pretraining enhances segmentation
accuracy, particularly in low-annotation regimes. An annotation-efficiency
analysis reveals that, under unseen conditions, the domain-specific model
achieves 5.4% higher F1 score than the general-purpose model, while using 80%
fewer labeled samples.
  These results demonstrate the generalization capabilities of domain-specific
foundation models and their potential to significantly reduce manual annotation
efforts, offering a scalable and automated solution for herbicide trial
analysis.

</details>


### [38] [Deep learning-based object detection of offshore platforms on Sentinel-1 Imagery and the impact of synthetic training data](https://arxiv.org/abs/2511.04304)
*Robin Spanier,Thorsten Hoeser,Claudia Kuenzer*

Main category: cs.CV

TL;DR: 该论文提出使用深度学习和合成数据提升海上基础设施遥感监测模型的泛化能力，显著提升了模型检测精度。


<details>
  <summary>Details</summary>
Motivation: 随着海上基础设施快速扩展，如海上风电场和油气平台等，对高效监测系统的需求日益增加。然而，训练稳健的检测模型依赖于全面且平衡的数据集，而现实中部分类别的数据稀缺，影响了模型的性能。

Method: 作者采用深度学习YOLOv10方法，结合真实和合成的Sentinel-1卫星影像，训练检测海上平台的模型。训练数据涵盖四个区域，测试在三个未见过的区域，评估模型的地理泛化能力。特别分析了合成数据对模型表现和类别平衡的影响。

Result: 模型在未见区域共检测到3,529个位于不同区域的海上平台。基础模型F1值为0.85，加入合成数据后提升至0.90，展现了合成数据对提升模型性能的显著作用。

Conclusion: 平衡的数据集对遥感检测模型尤为重要，合成数据生成能够有效弥补数据不平衡问题。基于深度学习的方法展现了实现全球海上基础设施可扩展监测的潜力。

Abstract: The recent and ongoing expansion of marine infrastructure, including offshore
wind farms, oil and gas platforms, artificial islands, and aquaculture
facilities, highlights the need for effective monitoring systems. The
development of robust models for offshore infrastructure detection relies on
comprehensive, balanced datasets, but falls short when samples are scarce,
particularly for underrepresented object classes, shapes, and sizes. By
training deep learning-based YOLOv10 object detection models with a combination
of synthetic and real Sentinel-1 satellite imagery acquired in the fourth
quarter of 2023 from four regions (Caspian Sea, South China Sea, Gulf of
Guinea, and Coast of Brazil), this study investigates the use of synthetic
training data to enhance model performance. We evaluated this approach by
applying the model to detect offshore platforms in three unseen regions (Gulf
of Mexico, North Sea, Persian Gulf) and thereby assess geographic
transferability. This region-holdout evaluation demonstrated that the model
generalises beyond the training areas. In total, 3,529 offshore platforms were
detected, including 411 in the North Sea, 1,519 in the Gulf of Mexico, and
1,593 in the Persian Gulf. The model achieved an F1 score of 0.85, which
improved to 0.90 upon incorporating synthetic data. We analysed how synthetic
data enhances the representation of unbalanced classes and overall model
performance, taking a first step toward globally transferable detection of
offshore infrastructure. This study underscores the importance of balanced
datasets and highlights synthetic data generation as an effective strategy to
address common challenges in remote sensing, demonstrating the potential of
deep learning for scalable, global offshore infrastructure monitoring.

</details>


### [39] [RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive Text-to-Video Generation](https://arxiv.org/abs/2511.04317)
*Xiangjun Zhang,Litong Gong,Yinglin Zheng,Yansong Liu,Wentao Jiang,Mingyi Xu,Biao Wang,Tiezheng Ge,Ming Zeng*

Main category: cs.CV

TL;DR: RISE-T2V提出了一种能自动重述用户文本提示并提升生成视频质量的通用型提升模块。


<details>
  <summary>Details</summary>
Motivation: 目前大多数文本转视频（T2V）扩散模型依赖预训练文本编码器对文本与视频进行语义对齐。但当用户只给出简短提示（而非精心设计的详细提示）时，模型往往难以生成高质量、符合语义的视频。这主要因为当前文本编码器对文本语义理解有限且无法对用户输入做在线重述，影响了模型的适应性和可用性。

Method: RISE-T2V将文本提示重述和语义特征提取两个过程融合为一步。通过提出Rephrasing Adapter模块，让T2V扩散模型可以利用大语言模型（LLM）下一个token预测时的文本隐藏态，用作视频生成的条件，从而在不分离流程的情况下，自动将用户的基础提示隐式重述为更丰富、贴合意图的表示。该方法能兼容各种预训练LLM与视频扩散模型，将LLM的强大语义理解能力引入视频生成任务。

Result: 实验结果表明，该方法能有效提升不同T2V扩散模型在高质量、贴合用户意图的视频生成上的能力，验证了框架的通用性和有效性。

Conclusion: RISE-T2V是一种通用、灵活且能大幅提升T2V输出质量和语义对齐的框架，通过自动重述文本提示，大幅增强T2V模型的表现力和易用性。

Abstract: Most text-to-video(T2V) diffusion models depend on pre-trained text encoders
for semantic alignment, yet they often fail to maintain video quality when
provided with concise prompts rather than well-designed ones. The primary issue
lies in their limited textual semantics understanding. Moreover, these text
encoders cannot rephrase prompts online to better align with user intentions,
which limits both the scalability and usability of the models, To address these
challenges, we introduce RISE-T2V, which uniquely integrates the processes of
prompt rephrasing and semantic feature extraction into a single and seamless
step instead of two separate steps. RISE-T2V is universal and can be applied to
various pre-trained LLMs and video diffusion models(VDMs), significantly
enhancing their capabilities for T2V tasks. We propose an innovative module
called the Rephrasing Adapter, enabling diffusion models to utilize text hidden
states during the next token prediction of the LLM as a condition for video
generation. By employing a Rephrasing Adapter, the video generation model can
implicitly rephrase basic prompts into more comprehensive representations that
better match the user's intent. Furthermore, we leverage the powerful
capabilities of LLMs to enable video generation models to accomplish a broader
range of T2V tasks. Extensive experiments demonstrate that RISE-T2V is a
versatile framework applicable to different video diffusion model
architectures, significantly enhancing the ability of T2V models to generate
high-quality videos that align with user intent. Visual results are available
on the webpage at https://rise-t2v.github.io.

</details>


### [40] [Submanifold Sparse Convolutional Networks for Automated 3D Segmentation of Kidneys and Kidney Tumours in Computed Tomography](https://arxiv.org/abs/2511.04334)
*Saúl Alonso-Monsalve,Leigh H. Whitehead,Adam Aurisano,Lorena Escudero Sanchez*

Main category: cs.CV

TL;DR: 本文提出了一种结合体素稀疏化与亚子流形稀疏卷积网络的新方法，实现了高分辨率CT肿瘤分割精度与显著的计算资源节省。


<details>
  <summary>Details</summary>
Motivation: 高精度肿瘤分割在医学图像分析中的人工成本高、耗时久，且3D卷积处理整幅高分辨率医学影像受限于巨大的计算与内存消耗。需要更高效的自动分割方法以利于临床应用。

Method: 方法分为两步：首先将体素信息稀疏化，随后使用子流形稀疏卷积网络结构，对CT扫描3D高分辨率输入进行分割。

Result: 在肾癌CT图像KiTS23挑战数据集上，方法获得肾脏+肿块Dice 95.8%、肿瘤+囊肿Dice 85.7%、肿瘤单独Dice 80.3%。推理时间减少高达60%，显存占用降低达75%。

Conclusion: 新方法在保证分割精度的同时大幅降低硬件资源消耗，有潜力推广到实际临床和其他大体积医学图像分割任务。

Abstract: The accurate delineation of tumours in radiological images like Computed
Tomography is a very specialised and time-consuming task, and currently a
bottleneck preventing quantitative analyses to be performed routinely in the
clinical setting. For this reason, developing methods for the automated
segmentation of tumours in medical imaging is of the utmost importance and has
driven significant efforts in recent years. However, challenges regarding the
impracticality of 3D scans, given the large amount of voxels to be analysed,
usually requires the downsampling of such images or using patches thereof when
applying traditional convolutional neural networks. To overcome this problem,
in this paper we propose a new methodology that uses, divided into two stages,
voxel sparsification and submanifold sparse convolutional networks. This method
allows segmentations to be performed with high-resolution inputs and a native
3D model architecture, obtaining state-of-the-art accuracies while
significantly reducing the computational resources needed in terms of GPU
memory and time. We studied the deployment of this methodology in the context
of Computed Tomography images of renal cancer patients from the KiTS23
challenge, and our method achieved results competitive with the challenge
winners, with Dice similarity coefficients of 95.8% for kidneys + masses, 85.7%
for tumours + cysts, and 80.3% for tumours alone. Crucially, our method also
offers significant computational improvements, achieving up to a 60% reduction
in inference time and up to a 75\% reduction in VRAM usage compared to an
equivalent dense architecture, across both CPU and various GPU cards tested.

</details>


### [41] [Comparative Study of CNN Architectures for Binary Classification of Horses and Motorcycles in the VOC 2008 Dataset](https://arxiv.org/abs/2511.04344)
*Muhammad Annas Shaikh,Hamza Zaman,Arbaz Asif*

Main category: cs.CV

TL;DR: 本文系统评估了九种主流CNN架构在VOC 2008数据集上进行马与摩托车二分类的表现，并且着重探讨了类别不平衡问题及其缓解方法。


<details>
  <summary>Details</summary>
Motivation: 在现实数据集中，二分类任务常常存在类别不平衡问题，导致模型对少数类检测性能较差。本文旨在评估流行CNN架构在该情景下的表现，并量化少数类数据增强对不平衡问题的缓解效果。

Method: 选取九种流行卷积神经网络架构（包括ResNet-50、ConvNeXt-Tiny、DenseNet-121、Vision Transformer等）在VOC 2008马与摩托车二分类任务上进行对比实验。采用对少数类的数据增强策略，并通过多项性能指标评价各模型表现。

Result: 实验结果显示，不同架构在性能上有显著差异。其中ConvNeXt-Tiny取得了检测马的平均精度（AP）95.53%，检测摩托车的AP为89.12%。数据增强对少数类检测有显著提升，尤其对深层网络架构效果明显。

Conclusion: 本文为选择适用于类别不平衡对象检测任务的神经网络架构提供了参考，并定量分析了少数类数据增强对提升检测性能，尤其在深层架构中的积极作用。

Abstract: This paper presents a comprehensive evaluation of nine convolutional neural
network architectures for binary classification of horses and motorcycles in
the VOC 2008 dataset. We address the significant class imbalance problem by
implementing minority-class augmentation techniques. Our experiments compare
modern architectures including ResNet-50, ConvNeXt-Tiny, DenseNet-121, and
Vision Transformer across multiple performance metrics. Results demonstrate
substantial performance variations, with ConvNeXt-Tiny achieving the highest
Average Precision (AP) of 95.53% for horse detection and 89.12% for motorcycle
detection. We observe that data augmentation significantly improves minority
class detection, particularly benefiting deeper architectures. This study
provides insights into architecture selection for imbalanced binary
classification tasks and quantifies the impact of data augmentation strategies
in mitigating class imbalance issues in object detection.

</details>


### [42] [Evaluating the Impact of Weather-Induced Sensor Occlusion on BEVFusion for 3D Object Detection](https://arxiv.org/abs/2511.04347)
*Sanjay Kumar,Tim Brophy,Eoin Martino Grua,Ganesh Sistu,Valentina Donzella,Ciaran Eising*

Main category: cs.CV

TL;DR: 本文研究了环境遮挡（如雾、障碍物等）对自动驾驶3D目标检测中传感器（摄像头与激光雷达）性能的影响，发现BEV融合架构对激光雷达的依赖更大，且遮挡严重影响检测精度，强调未来需关注遮挡环境下的融合方法与评价指标。


<details>
  <summary>Details</summary>
Motivation: 虽然BEV（俯视图）数据融合在多传感器3D目标检测中表现强劲，但在现实复杂环境（如雾气、遮挡物）中，传感器遮挡对检测性能的具体影响尚未被充分研究，因此有必要系统性地探究不同传感器遮挡对检测效果的影响。

Method: 本文基于BEVFusion架构，在nuScenes数据集上系统地测试了摄像头和激光雷达在不同遮挡条件下的检测表现，并采用mAP和NDS作为度量指标，分别分析了摄像头、激光雷达单独、以及融合时的性能变化。

Result: 实验证明，在仅用摄像头时，中等遮挡情况下mAP下降41.3%（由35.6%降至20.9%）；激光雷达在重度遮挡下mAP下降47.3%（由64.7%降至34.1%），尤其长距离目标影响更大。融合时，如果摄像头被遮挡，mAP略降4.1%，而激光雷达被遮挡时mAP下降26.8%，显示对激光雷达依赖更强。

Conclusion: BEV融合方法下，激光雷达对整体3D检测性能影响更大，环境遮挡对不同传感器影响各异。未来需开发更健壮的融合技术和遮挡感知的评估方法，以提升恶劣环境下自动驾驶安全性。

Abstract: Accurate 3D object detection is essential for automated vehicles to navigate
safely in complex real-world environments. Bird's Eye View (BEV)
representations, which project multi-sensor data into a top-down spatial
format, have emerged as a powerful approach for robust perception. Although
BEV-based fusion architectures have demonstrated strong performance through
multimodal integration, the effects of sensor occlusions, caused by
environmental conditions such as fog, haze, or physical obstructions, on 3D
detection accuracy remain underexplored. In this work, we investigate the
impact of occlusions on both camera and Light Detection and Ranging (LiDAR)
outputs using the BEVFusion architecture, evaluated on the nuScenes dataset.
Detection performance is measured using mean Average Precision (mAP) and the
nuScenes Detection Score (NDS). Our results show that moderate camera
occlusions lead to a 41.3% drop in mAP (from 35.6% to 20.9%) when detection is
based only on the camera. On the other hand, LiDAR sharply drops in performance
only under heavy occlusion, with mAP falling by 47.3% (from 64.7% to 34.1%),
with a severe impact on long-range detection. In fused settings, the effect
depends on which sensor is occluded: occluding the camera leads to a minor 4.1%
drop (from 68.5% to 65.7%), while occluding LiDAR results in a larger 26.8%
drop (to 50.1%), revealing the model's stronger reliance on LiDAR for the task
of 3D object detection. Our results highlight the need for future research into
occlusion-aware evaluation methods and improved sensor fusion techniques that
can maintain detection accuracy in the presence of partial sensor failure or
degradation due to adverse environmental conditions.

</details>


### [43] [A MATLAB tutorial on deep feature extraction combined with chemometrics for analytical applications](https://arxiv.org/abs/2511.04349)
*Puneet Mishra,Martijntje Vollebregt,Yizhou Ma,Maria Font-i-Furnols*

Main category: cs.CV

TL;DR: 这篇论文介绍了如何应用开源深度学习模型对分析化学中的成像数据进行深层特征提取，并通过教程案例提供了MATLAB代码，帮助读者在自己的数据集上实践。


<details>
  <summary>Details</summary>
Motivation: 分析化学中成像技术广泛应用，但传统化学计量学方法在有效提取与分析空间信息方面存在挑战。尽管深度学习在图像处理上取得显著进步，但缺乏结构化的操作指导限制了其在分析化学领域的应用。

Method: 论文提供了一个详细的分步教程，指导如何利用现有的开源深度学习模型（而非从零训练模型）来自动提取成像数据中的多尺度深层特征，并将其与其他数据源（如光谱信息）整合。教程包括MATLAB代码，并鼓励读者在自己的成像数据上实践操作。

Result: 教程演示了如何用开源深度学习模型处理分析化学中多种成像数据类型，推广了自动化空间特征提取的实用流程，使研究者可便捷操作其自身数据。

Conclusion: 该论文助力分析化学领域研究人员使用先进的深度学习方法提取与分析成像数据空间特征，有助于提升该领域图像分析与多源数据融合的效率和准确性。

Abstract: Background In analytical chemistry, spatial information about materials is
commonly captured through imaging techniques, such as traditional color cameras
or with advanced hyperspectral cameras and microscopes. However, efficiently
extracting and analyzing this spatial information for exploratory and
predictive purposes remains a challenge, especially when using traditional
chemometric methods. Recent advances in deep learning and artificial
intelligence have significantly enhanced image processing capabilities,
enabling the extraction of multiscale deep features that are otherwise
challenging to capture with conventional image processing techniques. Despite
the wide availability of open-source deep learning models, adoption in
analytical chemistry remains limited because of the absence of structured,
step-by-step guidance for implementing these models.
  Results This tutorial aims to bridge this gap by providing a step-by-step
guide for applying deep learning approaches to extract spatial information from
imaging data and integrating it with other data sources, such as spectral
information. Importantly, the focus of this work is not on training deep
learning models for image processing but on using existing open source models
to extract deep features from imaging data.
  Significance The tutorial provides MATLAB code tutorial demonstrations,
showcasing the processing of imaging data from various imaging modalities
commonly encountered in analytical chemistry. Readers must run the tutorial
steps on their own datasets using the codes presented in this tutorial.

</details>


### [44] [Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA](https://arxiv.org/abs/2511.04384)
*Itbaan Safwan,Muhammad Annas Shaikh,Muhammad Haaris,Ramail Khan,Muhammad Atif Tahir*

Main category: cs.CV

TL;DR: 本论文提出了一个多任务学习框架，利用LoRA微调的Florence-2模型，实现医学视觉问答(VQA)、解释生成和可视化定位三项任务联合优化，显著提升了答案准确性和定位能力。


<details>
  <summary>Details</summary>
Motivation: 医学视觉问答系统面临不仅要给出准确答案，还需要解释和定位相关区域的挑战，单一任务通常难以兼顾。为促进更全面的医学AI应用，需要一个能够整合多种认知能力的多任务框架。

Method: 方法上，基于Florence-2大模型进行LoRA微调，同时训练三个任务：1）利用Kvasir-VQA-x1数据集进行问答学习，2）合成丰富的解释数据以习得结构化医学推理，3）文本-区域对数据以学习视觉特征与分割掩码的关联，实现视觉定位。

Result: 实验表明，该多任务框架在答案准确度和可视化定位方面，均优于单任务基线方法，模型输出既准确又有解释性。

Conclusion: 多任务联合学习能够显著提升医学视觉问答的实际效果，特别是在需要推理和区域定位的应用场景中具有重要价值，并为后续医学AI应用奠定基础。

Abstract: We present a multi-task framework for the MediaEval Medico 2025 challenge,
leveraging a LoRA-tuned Florence-2 model for simultaneous visual question
answering (VQA), explanation generation, and visual grounding. The proposed
system integrates three curated datasets: (1) Kvasir-VQA-x1 for question-answer
learning, (2) a synthetically enriched explanation dataset offering structured
medical reasoning, and (3) text-to-region pairs linking visual features with
segmentation masks. This multi-task setup enables the model to jointly learn
visual grounding, reasoning, and interpretation, producing responses that are
both accurate and interpretable. Extensive evaluation demonstrates that our
approach substantially improves over single-task baselines in both answer
accuracy and visual localization, highlighting the effectiveness of grounded
multi-task learning for medical VQA applications.

</details>


### [45] [DORAEMON: A Unified Library for Visual Object Modeling and Representation Learning at Scale](https://arxiv.org/abs/2511.04394)
*Ke Du,Yimin Peng,Chao Gao,Fan Zhou,Siqiao Xue*

Main category: cs.CV

TL;DR: DORAEMON 是一个开源 PyTorch 库，整合了视觉对象建模与表示学习，支持多种任务与大规模模型，便于模型训练、评估及部署。


<details>
  <summary>Details</summary>
Motivation: 目前视觉对象识别和表征学习领域中，相关工具分散、模型接口不统一，难以进行高效的实验复现和算法迁移。该论文旨在解决视觉任务中工具链碎片化问题，提供一个统一的平台便于各种视觉任务的模型开发和部署。

Method: DORAEMON 基于 PyTorch 实现，采用 YAML 文件驱动的统一工作流，支持分类、检索和度量学习等任务。库内集成了超过1000个预训练 backbone，以 timm 兼容格式统一接口，提供了丰富的损失函数、增强方式及分布式训练工具；支持一键导出为 ONNX 或 HuggingFace 格式，方便模型部署。

Result: 在 ImageNet-1K、MS-Celeb-1M 以及 Stanford online products 等数据集上，DORAEMON 提供的实验模板可以达到或超过已有工作基线的结果，显示出平台的性能和可靠性。

Conclusion: DORAEMON 通过整合数据集、模型和训练技术，为视觉识别和表征学习提供了可扩展、高效的实验平台，加速了科研成果向实际应用的转移。

Abstract: DORAEMON is an open-source PyTorch library that unifies visual object
modeling and representation learning across diverse scales. A single
YAML-driven workflow covers classification, retrieval and metric learning; more
than 1000 pretrained backbones are exposed through a timm-compatible interface,
together with modular losses, augmentations and distributed-training utilities.
Reproducible recipes match or exceed reference results on ImageNet-1K,
MS-Celeb-1M and Stanford online products, while one-command export to ONNX or
HuggingFace bridges research and deployment. By consolidating datasets, models,
and training techniques into one platform, DORAEMON offers a scalable
foundation for rapid experimentation in visual recognition and representation
learning, enabling efficient transfer of research advances to real-world
applications. The repository is available at https://github.com/wuji3/DORAEMON.

</details>


### [46] [HideAndSeg: an AI-based tool with automated prompting for octopus segmentation in natural habitats](https://arxiv.org/abs/2511.04426)
*Alan de Aguiar,Michaella Pereira Andrade,Charles Morphy D. Santos,João Paulo Gois*

Main category: cs.CV

TL;DR: 本文提出了一种名为HideAndSeg的AI工具，以最小人工标注实现章鱼视频的自动分割，有效应对了章鱼在自然环境下的伪装、快速形态变化及遮挡等难题，并引入无监督评估指标，显著提升了分割的自动化和准确度。


<details>
  <summary>Details</summary>
Motivation: 章鱼在自然环境中的复杂形态变化、伪装及光照变化等因素，使得传统手动分割方法效率低下且难以获得大规模标注数据，限制了其行为学研究的发展。因此，迫切需要一种高效、自动化的分割工具来辅助章鱼行为的科学研究。

Method: 作者提出HideAndSeg方法，将SAM2分割模型与定制训练的YOLOv11检测器结合。初期用户只需提供点坐标辅助SAM2生成初始分割掩码，后续掩码作为YOLO训练数据，实现全自动流程，只需用YOLO输出为SAM2提供bounding box prompt。为解决无真实标签难题，本文提出了两个无监督评价指标（$DICE_t$和$NC_t$）用于分割质量度量与优化。

Result: 实验表明，HideAndSeg能在无需持续人工干预的情况下，有效减少分割噪音，并能在章鱼完全遮挡后实现重新识别与分割，优于纯人工提示方法。两项无监督指标帮助验证了分割的连贯性与准确性。

Conclusion: HideAndSeg工具大幅降低了野外章鱼视频分析所需的人工成本，提高了分割效率与准确性，为野生头足类动物的行为学研究提供了有力工具，有助于推动相关研究的发展。

Abstract: Analyzing octopuses in their natural habitats is challenging due to their
camouflage capability, rapid changes in skin texture and color, non-rigid body
deformations, and frequent occlusions, all of which are compounded by variable
underwater lighting and turbidity. Addressing the lack of large-scale annotated
datasets, this paper introduces HideAndSeg, a novel, minimally supervised
AI-based tool for segmenting videos of octopuses. It establishes a quantitative
baseline for this task. HideAndSeg integrates SAM2 with a custom-trained
YOLOv11 object detector. First, the user provides point coordinates to generate
the initial segmentation masks with SAM2. These masks serve as training data
for the YOLO model. After that, our approach fully automates the pipeline by
providing a bounding box prompt to SAM2, eliminating the need for further
manual intervention. We introduce two unsupervised metrics - temporal
consistency $DICE_t$ and new component count $NC_t$ - to quantitatively
evaluate segmentation quality and guide mask refinement in the absence of
ground-truth data, i.e., real-world information that serves to train, validate,
and test AI models. Results show that HideAndSeg achieves satisfactory
performance, reducing segmentation noise compared to the manually prompted
approach. Our method can re-identify and segment the octopus even after periods
of complete occlusion in natural environments, a scenario in which the manually
prompted model fails. By reducing the need for manual analysis in real-world
scenarios, this work provides a practical tool that paves the way for more
efficient behavioral studies of wild cephalopods.

</details>


### [47] [Solving Convex Partition Visual Jigsaw Puzzles](https://arxiv.org/abs/2511.04450)
*Yaniv Ohayon,Ofir Itzhak Shahar,Ohad Ben-Shahar*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，能够高效处理更广泛类型的非方形拼图（凸分割拼图），并建立了首个相关基准数据集。


<details>
  <summary>Details</summary>
Motivation: 绝大多数自动拼图求解方法仅限于处理方形拼图，导致实际应用受限。为此，作者希望拓展算法对多样化拼图（特别是多边形凸块拼图）的处理能力。

Method: 作者结合几何和图像特征，设计了一种贪心算法来求解凸分割拼图。同时，制作并公开了第一个此类拼图的基准数据集，用于算法性能评测。

Result: 新方法能够有效处理包含凸形拼块的多边形拼图，并在自建基准集上给出了多项性能评测指标。

Conclusion: 本文极大扩展了计算机自动拼图领域的能力，对实际多样化拼图场景具有更好的适应性，并为今后研究提供了数据集基础。

Abstract: Jigsaw puzzle solving requires the rearrangement of unordered pieces into
their original pose in order to reconstruct a coherent whole, often an image,
and is known to be an intractable problem. While the possible impact of
automatic puzzle solvers can be disruptive in various application domains, most
of the literature has focused on developing solvers for square jigsaw puzzles,
severely limiting their practical use. In this work, we significantly expand
the types of puzzles handled computationally, focusing on what is known as
Convex Partitions, a major subset of polygonal puzzles whose pieces are convex.
We utilize both geometrical and pictorial compatibilities, introduce a greedy
solver, and report several performance measures next to the first benchmark
dataset of such puzzles.

</details>


### [48] [V-Thinker: Interactive Thinking with Images](https://arxiv.org/abs/2511.04460)
*Runqi Qiao,Qiuna Tan,Minghan Yang,Guanting Dong,Peiqing Yang,Shiqiang Lang,Enhui Wan,Xiaowan Wang,Yida Xu,Lan Yang,Chong Sun,Chen Li,Honggang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为V-Thinker的通用多模态推理助手，能够通过端到端强化学习实现高度交互、以视觉为中心的推理，并显著超越现有大多模态模型基线。


<details>
  <summary>Details</summary>
Motivation: 目前大多模态模型在图像与长程推理深度融合上仍面临挑战，现有方法大多依赖有限的工具或针对特定任务的设计，难以推广。该文旨在推动LMMs实现更通用、深入的多模态交互推理。

Method: V-Thinker包含两大核心方法：(1)“数据进化飞轮”自动合成、进化和验证多维的交互式推理数据集；(2)“视觉递进训练课程”，首先通过点状监督对视觉感知进行校准，再用两阶段强化学习融入交互式推理。

Result: 提出了新基准VTBench进行权威评测。实验结果显示，V-Thinker在常规及交互推理任务中均优于强力LMM基线。

Conclusion: V-Thinker为提升图像互动式推理能力提供了有效新方向，对多模态推理发展具有参考价值。

Abstract: Empowering Large Multimodal Models (LMMs) to deeply integrate image
interaction with long-horizon reasoning capabilities remains a long-standing
challenge in this field. Recent advances in vision-centric reasoning explore a
promising "Thinking with Images" paradigm for LMMs, marking a shift from
image-assisted reasoning to image-interactive thinking. While this milestone
enables models to focus on fine-grained image regions, progress remains
constrained by limited visual tool spaces and task-specific workflow designs.
To bridge this gap, we present V-Thinker, a general-purpose multimodal
reasoning assistant that enables interactive, vision-centric thinking through
end-to-end reinforcement learning. V-Thinker comprises two key components: (1)
a Data Evolution Flywheel that automatically synthesizes, evolves, and verifies
interactive reasoning datasets across three dimensions-diversity, quality, and
difficulty; and (2) a Visual Progressive Training Curriculum that first aligns
perception via point-level supervision, then integrates interactive reasoning
through a two-stage reinforcement learning framework. Furthermore, we introduce
VTBench, an expert-verified benchmark targeting vision-centric interactive
reasoning tasks. Extensive experiments demonstrate that V-Thinker consistently
outperforms strong LMM-based baselines in both general and interactive
reasoning scenarios, providing valuable insights for advancing
image-interactive reasoning applications.

</details>


### [49] [Landslide Hazard Mapping with Geospatial Foundation Models: Geographical Generalizability, Data Scarcity, and Band Adaptability](https://arxiv.org/abs/2511.04474)
*Wenwen Li,Sizhe Wang,Hyunho Lee,Chenyan Lu,Sujit Roy,Rahul Ramachandran,Chia-Yu Hsu*

Main category: cs.CV

TL;DR: 本论文提出了一种三轴分析框架，对地理基础模型（GeoFMs）进行适应，显著提升了滑坡制图的准确性和泛化能力，尤其是在数据有限和多样化传感器情况下。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在跨传感器、跨区域或缺乏训练数据时，滑坡识别能力下降。如何实现高鲁棒性和泛化能力的地理空间模型，是滑坡风险应对中亟需解决的问题。

Method: 论文设计了一个针对传感器、标签和域的三轴分析框架，重点使用GeoFMs（以Prithvi-EO-2.0为代表），结合全球预训练、自监督学习和可调优微调技术。通过与现有CNN、视觉Transformer及其他GeoFMs对比实验，验证模型优势。

Result: 实验结果显示，Prithvi-EO-2.0在滑坡制图任务中，整体优于U-Net、Segformer等主流方法和其它GeoFMs，具有更强的抗光谱变化能力、在标签稀缺时保持准确性，并在不同数据集和地域间更好泛化。

Conclusion: GeoFMs为滑坡风险减缓和环境监测提供了更强大的技术基础，但当前依旧存在计算成本高和训练数据匮乏等挑战。

Abstract: Landslides cause severe damage to lives, infrastructure, and the environment,
making accurate and timely mapping essential for disaster preparedness and
response. However, conventional deep learning models often struggle when
applied across different sensors, regions, or under conditions of limited
training data. To address these challenges, we present a three-axis analytical
framework of sensor, label, and domain for adapting geospatial foundation
models (GeoFMs), focusing on Prithvi-EO-2.0 for landslide mapping. Through a
series of experiments, we show that it consistently outperforms task-specific
CNNs (U-Net, U-Net++), vision transformers (Segformer, SwinV2-B), and other
GeoFMs (TerraMind, SatMAE). The model, built on global pretraining,
self-supervision, and adaptable fine-tuning, proved resilient to spectral
variation, maintained accuracy under label scarcity, and generalized more
reliably across diverse datasets and geographic settings. Alongside these
strengths, we also highlight remaining challenges such as computational cost
and the limited availability of reusable AI-ready training data for landslide
research. Overall, our study positions GeoFMs as a step toward more robust and
scalable approaches for landslide risk reduction and environmental monitoring.

</details>


### [50] [Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm](https://arxiv.org/abs/2511.04570)
*Jingqi Tong,Yurong Mou,Hangcheng Li,Mingzhe Li,Yongzhuo Yang,Ming Zhang,Qiguang Chen,Tianyi Liang,Xiaomeng Hu,Yining Zheng,Xinchi Chen,Jun Zhao,Xuanjing Huang,Xipeng Qiu*

Main category: cs.CV

TL;DR: 本文提出“用视频思考”的新范式，通过视频生成模型（如Sora-2）提升多模态统一推理能力，并开发了相应基准进行评测。Sora-2在多项视觉和文本任务中表现优异，验证了该范式的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的“用文本思考”和“用图像思考”范式存在局限性：图像无法表达动态变化，文本与视觉的分割限制了统一的多模态理解与生成。为突破这些瓶颈，作者尝试引入视频作为新的推理载体。

Method: 提出“用视频思考”范式，并基于Sora-2等视频生成模型，将视觉推理和文本推理统一到具时序的视频框架中。设计VideoThinkBench，包括视觉中心和文本中心两类任务，系统评测Sora-2模型能力，并分析其推理来源。

Result: Sora-2在视觉中心任务上整体表现可比肩最新视觉语言模型（VLMs），在部分任务如Eyeballing Games上甚至超越VLMs；在文本中心任务上也达到高准确率（MATH为92%，MMMU为75.53%）。同时自洽性和上下文学习进一步提升了其性能。

Conclusion: 视频生成模型有望作为统一的多模态理解与生成模型，“用视频思考”可作为新一代多模态推理范式，能弥合文本与视觉的界限，实现更强的推理与表达能力。

Abstract: "Thinking with Text" and "Thinking with Images" paradigm significantly
improve the reasoning ability of large language models (LLMs) and Vision
Language Models (VLMs). However, these paradigms have inherent limitations. (1)
Images capture only single moments and fail to represent dynamic processes or
continuous changes, and (2) The separation of text and vision as distinct
modalities, hindering unified multimodal understanding and generation. To
overcome these limitations, we introduce "Thinking with Video", a new paradigm
that leverages video generation models, such as Sora-2, to bridge visual and
textual reasoning in a unified temporal framework. To support this exploration,
we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench
encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing
Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our
evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,
Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even
surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric
tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.
Furthermore, we systematically analyse the source of these abilities. We also
find that self-consistency and in-context learning can improve Sora-2's
performance. In summary, our findings demonstrate that the video generation
model is the potential unified multimodal understanding and generation model,
positions "thinking with video" as a unified multimodal reasoning paradigm.

</details>


### [51] [THEval. Evaluation Framework for Talking Head Video Generation](https://arxiv.org/abs/2511.04520)
*Nabyl Quignon,Baptiste Chopin,Yaohui Wang,Antitza Dantcheva*

Main category: cs.CV

TL;DR: 本论文提出了一个新的面向说话人视频生成的评测框架，包括8项针对视频质量、自然度和同步性的指标，可更细粒度地分析头部、嘴部、眉毛动态和人脸质量。作者在大规模数据集上评测了17个主流模型，发现唇同步方面表现良好，但在表情丰富性与无伪影细节上仍有提升空间。相关代码、数据集和排行榜将公开，为该领域评测提供标准。


<details>
  <summary>Details</summary>
Motivation: 现有说话人生成视频的评测标准有限，主要集中于视频整体质量、唇同步及用户调研，缺乏细致、系统和高效的评测方法。生成视频技术进展迅速，但评测手段已无法有效区分不同算法间的细微差异与不足，需要新的评测框架来推动真实、自然和高质量视频生成的发展。

Method: 作者设计了涵盖8个指标的评测框架，从视频质量、自然度、同步性三个维度，细粒度地分析头部、嘴部、眉毛及面部质量。这些指标兼顾评测效率和与人类主观偏好的契合度。为减少评测偏差，作者基于新建的大型真实说话人视频数据集，评测了17种先进模型的85,000段生成视频，系统比较各种算法，梳理各自优势与短板。

Result: 实验结果显示，当前主流模型在唇同步方面普遍表现优秀，但在面部表情丰富性、细致动态和无伪影细节生成方面仍存在明显挑战。作者建立的数据集和评测体系，有效揭示了不同模型的优劣和未来提升空间。

Conclusion: 论文提出的新评测框架和数据集，为说话人视频生成算法提供了系统、精细和高效的评测标准，有助于推动未来生成模型向更自然、真实、细腻及高质量方向发展。相关资源将开放，持续更新，以促进社区进步。

Abstract: Video generation has achieved remarkable progress, with generated videos
increasingly resembling real ones. However, the rapid advance in generation has
outpaced the development of adequate evaluation metrics. Currently, the
assessment of talking head generation primarily relies on limited metrics,
evaluating general video quality, lip synchronization, and on conducting user
studies. Motivated by this, we propose a new evaluation framework comprising 8
metrics related to three dimensions (i) quality, (ii) naturalness, and (iii)
synchronization. In selecting the metrics, we place emphasis on efficiency, as
well as alignment with human preferences. Based on this considerations, we
streamline to analyze fine-grained dynamics of head, mouth, and eyebrows, as
well as face quality. Our extensive experiments on 85,000 videos generated by
17 state-of-the-art models suggest that while many algorithms excel in lip
synchronization, they face challenges with generating expressiveness and
artifact-free details. These videos were generated based on a novel real
dataset, that we have curated, in order to mitigate bias of training data. Our
proposed benchmark framework is aimed at evaluating the improvement of
generative methods. Original code, dataset and leaderboards will be publicly
released and regularly updated with new methods, in order to reflect progress
in the field.

</details>


### [52] [Learning from Single Timestamps: Complexity Estimation in Laparoscopic Cholecystectomy](https://arxiv.org/abs/2511.04525)
*Dimitrios Anastasiou,Santiago Barbarisi,Lucy Culshaw,Jayna Patel,Evangelos B. Mazomenos,Imanol Luengo,Danail Stoyanov*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法STC-Net，用于全手术视频中胆囊切除术复杂性的自动评估，准确率和F1分数均提升10%以上。


<details>
  <summary>Details</summary>
Motivation: 在腹腔镜胆囊切除术（LC）中，炎症严重程度影响手术复杂性和并发症风险，现有自动化评估方法多只针对静态图片或人工剪辑片段，缺乏对全手术视频的自动复杂度分析。

Method: 作者提出STC-Net框架，在单一时间戳基础上，实现对完整手术视频中复杂性的自动估算。该方法具备弱时序监督，通过时序定位、片段提议和分级模块联合处理，创新性地采用结合硬软定位目标和背景感知分级的损失函数设计。

Result: 在1859例腹腔镜胆囊切除术私有视频数据集上，STC-Net准确率达62.11%，F1分数达61.42%，较无定位基线提升超过10%。

Conclusion: STC-Net在全手术视频中的复杂度自动评估展现出良好的可扩展性和有效性，有望应用于术后分析与外科培训领域。

Abstract: Purpose: Accurate assessment of surgical complexity is essential in
Laparoscopic Cholecystectomy (LC), where severe inflammation is associated with
longer operative times and increased risk of postoperative complications. The
Parkland Grading Scale (PGS) provides a clinically validated framework for
stratifying inflammation severity; however, its automation in surgical videos
remains largely unexplored, particularly in realistic scenarios where complete
videos must be analyzed without prior manual curation. Methods: In this work,
we introduce STC-Net, a novel framework for SingleTimestamp-based Complexity
estimation in LC via the PGS, designed to operate under weak temporal
supervision. Unlike prior methods limited to static images or manually trimmed
clips, STC-Net operates directly on full videos. It jointly performs temporal
localization and grading through a localization, window proposal, and grading
module. We introduce a novel loss formulation combining hard and soft
localization objectives and background-aware grading supervision. Results:
Evaluated on a private dataset of 1,859 LC videos, STC-Net achieves an accuracy
of 62.11% and an F1-score of 61.42%, outperforming non-localized baselines by
over 10% in both metrics and highlighting the effectiveness of weak supervision
for surgical complexity assessment. Conclusion: STC-Net demonstrates a scalable
and effective approach for automated PGS-based surgical complexity estimation
from full LC videos, making it promising for post-operative analysis and
surgical training.

</details>


### [53] [UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction](https://arxiv.org/abs/2511.04595)
*Chen Shi,Shaoshuai Shi,Xiaoyang Lyu,Chunyang Liu,Kehua Sheng,Bo Zhang,Li Jiang*

Main category: cs.CV

TL;DR: UniSplat是一种面向自动驾驶的统一3D动态场景重建框架，能处理相机视角稀疏、场景动态复杂等挑战，对视野外也能高质量重建。


<details>
  <summary>Details</summary>
Motivation: 现有3D重建方法无法很好处理稀疏、非重叠相机视角与复杂动态场景，导致重建结果不完整且不够细致，影响自动驾驶的安全和鲁棒性。

Method: UniSplat通过引入统一的三维潜在骨架，通过时空融合机制集成多视角与多时刻信息，利用先验基础模型感知结构与语义，再采用双分支解码器结合点锚优化和体素生成实现动态高斯的生成，并维护静态高斯持久内存，支持视野外流式补全。

Result: 在真实数据集上实验，UniSplat在新视角合成任务上达到SOTA（最好）效果，并能对原有相机视角覆盖不到的区域进行高质量鲁棒渲染。

Conclusion: UniSplat为自动驾驶提供了更强大的三维动态场景重建能力，不仅提升了视角外渲染质量，也增强了场景理解的完整性与鲁棒性。

Abstract: Feed-forward 3D reconstruction for autonomous driving has advanced rapidly,
yet existing methods struggle with the joint challenges of sparse,
non-overlapping camera views and complex scene dynamics. We present UniSplat, a
general feed-forward framework that learns robust dynamic scene reconstruction
through unified latent spatio-temporal fusion. UniSplat constructs a 3D latent
scaffold, a structured representation that captures geometric and semantic
scene context by leveraging pretrained foundation models. To effectively
integrate information across spatial views and temporal frames, we introduce an
efficient fusion mechanism that operates directly within the 3D scaffold,
enabling consistent spatio-temporal alignment. To ensure complete and detailed
reconstructions, we design a dual-branch decoder that generates dynamic-aware
Gaussians from the fused scaffold by combining point-anchored refinement with
voxel-based generation, and maintain a persistent memory of static Gaussians to
enable streaming scene completion beyond current camera coverage. Extensive
experiments on real-world datasets demonstrate that UniSplat achieves
state-of-the-art performance in novel view synthesis, while providing robust
and high-quality renderings even for viewpoints outside the original camera
coverage.

</details>


### [54] [PixCLIP: Achieving Fine-grained Visual Language Understanding via Any-granularity Pixel-Text Alignment Learning](https://arxiv.org/abs/2511.04601)
*Yicheng Xiao,Yu Chen,Haoxuan Ma,Jiale Hong,Caorui Li,Lingxiang Wu,Haiyun Guo,Jinqiao Wang*

Main category: cs.CV

TL;DR: 本文提出了PixCLIP，一种能处理长文本描述和像素级视觉提示的视觉-语言预训练新框架，提升了CLIP模型在细粒度图文对齐方面的能力，并取得了业界最佳表现。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP模型虽然在多种视觉-语言任务中表现优异，但其文本编码长度有限，难以利用长文本中的细粒度信息。同时，如何更精细地对齐图像和文本内容仍然是难题。

Method: 1. 提出PixCLIP框架，同时支持视觉提示（如像素级区域）和长文本输入；2. 建立自动注释流程，生成像素级本地化的长文本描述，并构建约150万样本的新数据集LongGRIT；3. 用多模态大语言模型取代CLIP原有文本编码器，设计了三分支像素-文本对齐学习结构。

Result: 实验证明，PixCLIP在像素级图像文本交互及长文本处理上取得突破，在多个细粒度对齐任务上达到了最新最优性能。

Conclusion: PixCLIP框架能有效弥补CLIP在精细化图文对齐和长文本处理方面的不足，推进了多模态预训练模型精细内容理解的发展。

Abstract: While the Contrastive Language-Image Pretraining(CLIP) model has achieved
remarkable success in a variety of downstream vison language understanding
tasks, enhancing its capability for fine-grained image-text alignment remains
an active research focus. To this end, most existing works adopt the strategy
of explicitly increasing the granularity of visual information processing,
e.g., incorporating visual prompts to guide the model focus on specific local
regions within the image. Meanwhile, researches on Multimodal Large Language
Models(MLLMs) have demonstrated that training with long and detailed textual
descriptions can effectively improve the model's fine-grained vision-language
alignment. However, the inherent token length limitation of CLIP's text encoder
fundamentally limits CLIP to process more granular textual information embedded
in long text sequences. To synergistically leverage the advantages of enhancing
both visual and textual content processing granularity, we propose PixCLIP, a
novel framework designed to concurrently accommodate visual prompt inputs and
process lengthy textual descriptions. Specifically, we first establish an
automated annotation pipeline capable of generating pixel-level localized,
long-form textual descriptions for images. Utilizing this pipeline, we
construct LongGRIT, a high-quality dataset comprising nearly 1.5 million
samples. Secondly, we replace CLIP's original text encoder with the LLM and
propose a three-branch pixel-text alignment learning framework, facilitating
fine-grained alignment between image regions and corresponding textual
descriptions at arbitrary granularity. Experiments demonstrate that PixCLIP
showcases breakthroughs in pixel-level interaction and handling long-form
texts, achieving state-of-the-art performance.

</details>


### [55] [Building Trust in Virtual Immunohistochemistry: Automated Assessment of Image Quality](https://arxiv.org/abs/2511.04615)
*Tushar Kataria,Shikha Dubey,Mary Bronner,Jolanta Jedrzkiewicz,Ben J. Brintz,Shireen Y. Elhabian,Beatrice S. Knudsen*

Main category: cs.CV

TL;DR: 本文提出了一套自动化评估虚拟免疫组化（IHC）染色图像质量的新方法，比传统的图像保真度指标更能准确反映像素级染色准确性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习虚拟IHC染色虽然成本低、可扩展，但主流的图像质量评价指标（如FID、PSNR、SSIM）只能反映视觉保真度，无法真正体现IHC染色的像素级准确性，这对临床落地造成阻碍。

Method: 作者在16种配对及非配对虚拟IHC图像转换模型上，利用色彩去卷积技术自动分割棕色（IHC阳性）像素掩码，然后采用Dice、IoU、Hausdorff距离等指标，对比真实与虚拟IHC的分割掩码，无需专家手动标注。

Result: 实验显示，常用的图像保真度指标与染色准确性及病理学家评估相关性低，且配对模型（如PyramidPix2Pix、AdaptiveNCE）表现较好，非配对扩散模型和GAN模型准确性较差。此外，仅用局部Patch评测会掩盖整体切片效果下的性能下降，强调了WSI评测的重要性。

Conclusion: 该框架为虚拟IHC模型提供了可复现、自动化的评价方法，推动其向临床实际应用转化。

Abstract: Deep learning models can generate virtual immunohistochemistry (IHC) stains
from hematoxylin and eosin (H&E) images, offering a scalable and low-cost
alternative to laboratory IHC. However, reliable evaluation of image quality
remains a challenge as current texture- and distribution-based metrics quantify
image fidelity rather than the accuracy of IHC staining. Here, we introduce an
automated and accuracy grounded framework to determine image quality across
sixteen paired or unpaired image translation models. Using color deconvolution,
we generate masks of pixels stained brown (i.e., IHC-positive) as predicted by
each virtual IHC model. We use the segmented masks of real and virtual IHC to
compute stain accuracy metrics (Dice, IoU, Hausdorff distance) that directly
quantify correct pixel - level labeling without needing expert manual
annotations. Our results demonstrate that conventional image fidelity metrics,
including Frechet Inception Distance (FID), peak signal-to-noise ratio (PSNR),
and structural similarity (SSIM), correlate poorly with stain accuracy and
pathologist assessment. Paired models such as PyramidPix2Pix and AdaptiveNCE
achieve the highest stain accuracy, whereas unpaired diffusion- and GAN-based
models are less reliable in providing accurate IHC positive pixel labels.
Moreover, whole-slide images (WSI) reveal performance declines that are
invisible in patch-based evaluations, emphasizing the need for WSI-level
benchmarks. Together, this framework defines a reproducible approach for
assessing the quality of virtual IHC models, a critical step to accelerate
translation towards routine use by pathologists.

</details>


### [56] [NovisVQ: A Streaming Convolutional Neural Network for No-Reference Opinion-Unaware Frame Quality Assessment](https://arxiv.org/abs/2511.04628)
*Kylie Cancilla,Alexander Moore,Amar Saini,Carmen Carrano*

Main category: cs.CV

TL;DR: 本文提出一种无需参考视频且无需人工主观评分的视频质量评价（VQA）方法，利用时间卷积网络和合成退化数据训练，无需推理阶段的参考视频，效果优于传统基于图像的方法。


<details>
  <summary>Details</summary>
Motivation: 当前主流视频质量评价要么依赖于参考视频（影响实际应用），要么需要大量昂贵的人工主观标注，而且大多数无主观分数的方法忽视了视频时间信息，无法适应复杂真实场景。

Method: 作者应用DAVIS数据集对原始视频施加合成退化，并用这些数据训练一个时序感知的卷积网络，模型输入受损视频，输出预测的全参考指标得分（如LPIPS、PSNR、SSIM），在推理阶段无需参考视频，也不依赖人工标签。

Result: 该模型在各种视频退化情形下较图像级基线方法表现更优，尤其体现了时序建模能力的重要性。同时，模型与传统主观感知NR基线（BRISQUE）结果对比，显示其与全参考指标的相关性更高。

Conclusion: 该方法证明了无需主观评分和参考视频的时序建模对视频质量评测具有很大潜力，为实际视觉系统可扩展、高效的视频质量评价提供新思路。

Abstract: Video quality assessment (VQA) is vital for computer vision tasks, but
existing approaches face major limitations: full-reference (FR) metrics require
clean reference videos, and most no-reference (NR) models depend on training on
costly human opinion labels. Moreover, most opinion-unaware NR methods are
image-based, ignoring temporal context critical for video object detection. In
this work, we present a scalable, streaming-based VQA model that is both
no-reference and opinion-unaware. Our model leverages synthetic degradations of
the DAVIS dataset, training a temporal-aware convolutional architecture to
predict FR metrics (LPIPS , PSNR, SSIM) directly from degraded video, without
references at inference. We show that our streaming approach outperforms our
own image-based baseline by generalizing across diverse degradations,
underscoring the value of temporal modeling for scalable VQA in real-world
vision systems. Additionally, we demonstrate that our model achieves higher
correlation with full-reference metrics compared to BRISQUE, a widely-used
opinion-aware image quality assessment baseline, validating the effectiveness
of our temporal, opinion-unaware approach.

</details>


### [57] [Polarization-resolved imaging improves eye tracking](https://arxiv.org/abs/2511.04652)
*Mantas Žurauskas,Tom Bu,Sanaz Alali,Beyza Kalkanli,Derek Shi,Fernando Alamos,Gauresh Pandit,Christopher Mei,Ali Behrooz,Ramin Mirjalili,Dave Stronks,Alexander Fix,Dmitri Model*

Main category: cs.CV

TL;DR: 该论文提出了一种基于偏振成像的眼动追踪方法，通过测量眼部组织反射光的偏振信息增强了追踪精度。结果显示，相较于传统方法，该方案显著降低了凝视误差，在各种干扰条件下表现稳定。


<details>
  <summary>Details</summary>
Motivation: 传统眼动追踪技术通常只利用反射光的强度信息，容易受到眼睑遮挡、眼防变化等干扰，准确率有限。论文的动机是通过引入偏振成像这一新型光学对比机制，挖掘更多能反映眼球状态的信息，提升眼动追踪的鲁棒性和准确率。

Method: 构建了一套偏振增强型眼动追踪系统（PET），采用带有偏振滤波阵列的相机和线偏振近红外光源。通过收集346名参与者的眼部偏振反射图像，利用卷积神经网络进行训练和测试。将基于偏振信息的模型与同等配置的传统强度信息模型进行对比。

Result: 在各种条件下（包括正常、眼睑遮挡、眼防变化和瞳孔大小变化），PET系统将95百分位凝视绝对误差的中位数降低了10-16%，性能优于只用强度信息的方法。偏振效应带来可追踪特征和凝视相关图案，大大增强了追踪能力。

Conclusion: 眼组织的偏振反射效应能够转化为实际的眼动追踪性能提升，为人机交互领域带来应用价值。PET方案操作简单、鲁棒性强，有望作为未来可穿戴设备中的新型感知手段。

Abstract: Polarization-resolved near-infrared imaging adds a useful optical contrast
mechanism to eye tracking by measuring the polarization state of light
reflected by ocular tissues in addition to its intensity. In this paper we
demonstrate how this contrast can be used to enable eye tracking. Specifically,
we demonstrate that a polarization-enabled eye tracking (PET) system composed
of a polarization--filter--array camera paired with a linearly polarized
near-infrared illuminator can reveal trackable features across the sclera and
gaze-informative patterns on the cornea, largely absent in intensity-only
images. Across a cohort of 346 participants, convolutional neural network based
machine learning models trained on data from PET reduced the median
95th-percentile absolute gaze error by 10--16\% relative to capacity-matched
intensity baselines under nominal conditions and in the presence of eyelid
occlusions, eye-relief changes, and pupil-size variation. These results link
light--tissue polarization effects to practical gains in human--computer
interaction and position PET as a simple, robust sensing modality for future
wearable devices.

</details>


### [58] [Benchmark Designers Should "Train on the Test Set" to Expose Exploitable Non-Visual Shortcuts](https://arxiv.org/abs/2511.04655)
*Ellis Brown,Jihan Yang,Shusheng Yang,Rob Fergus,Saining Xie*

Main category: cs.CV

TL;DR: 该论文提出并实践了一种诊断和消除多模态大模型（MLLMs）视觉基准测试中非视觉偏差的方法，以防止模型单靠语言和表面模式 '作弊' 获高分。


<details>
  <summary>Details</summary>
Motivation: 多模态基准常被用于评价MLLMs视觉理解能力，但现有基准易被模型利用非视觉信息（如语言偏见、表面规律）“破解”，从而无法真实衡量模型视觉理解水平。

Method: 1. 提出测试集压力测试（TsT）方法：用强大的语言模型，仅利用测试集中的文本输入作k折交叉验证，量化每个样本的偏差得分。2. 辅以基于随机森林和特征工程的快速诊断方法。3. 提出迭代偏差修剪（IBP）：基于偏差信息，逐步剔除偏差较高的样本，以去除非视觉可利用模式。

Result: 在四个主流多模态基准上应用该框架，证实均存在大量非视觉偏差。对VSI-Bench实验，构建了去偏版本VSI-Bench-Debiased，显著降低了通过非视觉信息高分的可能性，模型视觉盲区更清晰可见。

Conclusion: 本文提出可诊断并修剪多模态基准中非视觉偏差的系统方法，有助于设计更鲁棒、更能评价真实视觉理解能力的多模态基准。

Abstract: Robust benchmarks are crucial for evaluating Multimodal Large Language Models
(MLLMs). Yet we find that models can ace many multimodal benchmarks without
strong visual understanding, instead exploiting biases, linguistic priors, and
superficial patterns. This is especially problematic for vision-centric
benchmarks that are meant to require visual inputs. We adopt a diagnostic
principle for benchmark design: if a benchmark can be gamed, it will be.
Designers should therefore try to ``game'' their own benchmarks first, using
diagnostic and debiasing procedures to systematically identify and mitigate
non-visual biases. Effective diagnosis requires directly ``training on the test
set'' -- probing the released test set for its intrinsic, exploitable patterns.
  We operationalize this standard with two components. First, we diagnose
benchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology.
Our primary diagnostic tool involves fine-tuning a powerful Large Language
Model via $k$-fold cross-validation on exclusively the non-visual, textual
inputs of the test set to reveal shortcut performance and assign each sample a
bias score $s(x)$. We complement this with a lightweight Random Forest-based
diagnostic operating on hand-crafted features for fast, interpretable auditing.
Second, we debias benchmarks by filtering high-bias samples using an
``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to four
benchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasive
non-visual biases. As a case study, we apply our full framework to create
VSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider
vision-blind performance gap than the original.

</details>


### [59] [SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding](https://arxiv.org/abs/2511.04668)
*Ellis Brown,Arijit Ray,Ranjay Krishna,Ross Girshick,Rob Fergus,Saining Xie*

Main category: cs.CV

TL;DR: 该论文提出用3D模拟器生成带有丰富空间信息的视频数据，为多模态大语言模型提供训练，以提升时空推理能力，并用少量高效问题集在空间任务上超过了大模型。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型虽擅长视频理解，但在时空推理上的能力受限。由于获得真实世界高质量空间标注视频具有挑战性，数据获取成为发展瓶颈。论文旨在突破数据瓶颈，为空间推理训练提供更优方案。

Method: 提出SIMS-V框架，通过3D模拟器系统性生成包含空间注释的数据集。通过分析不同问题类别、组合和规模，找出最有效的训练方式，强化模型空间智能的迁移能力，仅采用三类高效问题（度量测量、依赖视角的推理、时序跟踪）。

Result: 用仅2.5万条模拟样本对7B参数的视频LLM微调，在空间推理任务上超过了72B参数基线模型，在真实世界空间推理基准上取得与私有模型媲美的表现。同时，模型在常规视频理解任务上保持稳健性能。

Conclusion: 利用3D模拟器合成空间数据，并精简高效的问题集，可以极大提升多模态模型的空间智能及其在真实世界的泛化能力，为空间视频推理数据生成和模型训练提供新范式。

Abstract: Despite impressive high-level video comprehension, multimodal language models
struggle with spatial reasoning across time and space. While current spatial
training approaches rely on real-world video data, obtaining diverse footage
with precise spatial annotations remains a bottleneck. To alleviate this
bottleneck, we present SIMS-V -- a systematic data-generation framework that
leverages the privileged information of 3D simulators to create spatially-rich
video training data for multimodal language models. Using this framework, we
investigate which properties of simulated data drive effective real-world
transfer through systematic ablations of question types, mixes, and scales. We
identify a minimal set of three question categories (metric measurement,
perspective-dependent reasoning, and temporal tracking) that prove most
effective for developing transferable spatial intelligence, outperforming
comprehensive coverage despite using fewer question types. These insights
enable highly efficient training: our 7B-parameter video LLM fine-tuned on just
25K simulated examples outperforms the larger 72B baseline and achieves
competitive performance with proprietary models on rigorous real-world spatial
reasoning benchmarks. Our approach demonstrates robust generalization,
maintaining performance on general video understanding while showing
substantial improvements on embodied and real-world spatial tasks.

</details>


### [60] [Cambrian-S: Towards Spatial Supersensing in Video](https://arxiv.org/abs/2511.04670)
*Shusheng Yang,Jihan Yang,Pinzhi Huang,Ellis Brown,Zihao Yang,Yue Yu,Shengbang Tong,Zihan Zheng,Yifan Xu,Muhan Wang,Daohan Lu,Rob Fergus,Yann LeCun,Li Fei-Fei,Saining Xie*

Main category: cs.CV

TL;DR: 本论文认为实现真正的多模态智能，需要超越简单的反应式、任务驱动系统和长上下文处理，提出了“空间超感知”范式，并设计了新的评测基准和方法对现有模型进行挑战。


<details>
  <summary>Details</summary>
Motivation: 当前多模态系统主要关注感知和任务回答，缺乏对空间认知、时间延续性、空间推理以及预测建模的评测，因此难以推动真正意义上的多模态智能发展。

Method: 作者定义了空间超感知的四个阶段：语义感知、持续性事件认知、隐含3D空间认知、预测性世界建模。针对现有基准的局限，提出了VSI-SUPER评测体系，包括VSR（长时视觉空间回忆）和VSC（持续视觉空间计数）两个新任务，专为考验模型空间认知及记忆能力而设计，并测试了模型数据规模扩展的上限。还提出了基于预测误差进行记忆与事件分段的方法。

Result: 通过训练Cambrian-S模型，并利用大量新数据，现有空间任务性能提升30%；但在更难的VSI-SUPER基准上，单靠数据扩大效果有限。而采用预测驱动的自监督方法，在VSI-SUPER任务上超过了市面领先的专有模型。

Conclusion: 单纯扩展模型规模和数据体量不足以达到更高级的空间超感知；需要发展能主动预测和组织体验的智能模型。提出的预测驱动方法为实现空间超感知提供了有效方向。

Abstract: We argue that progress in true multimodal intelligence calls for a shift from
reactive, task-driven systems and brute-force long context towards a broader
paradigm of supersensing. We frame spatial supersensing as four stages beyond
linguistic-only understanding: semantic perception (naming what is seen),
streaming event cognition (maintaining memory across continuous experiences),
implicit 3D spatial cognition (inferring the world behind pixels), and
predictive world modeling (creating internal models that filter and organize
information). Current benchmarks largely test only the early stages, offering
narrow coverage of spatial cognition and rarely challenging models in ways that
require true world modeling. To drive progress in spatial supersensing, we
present VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial
recall) and VSC (continual visual spatial counting). These tasks require
arbitrarily long video inputs yet are resistant to brute-force context
expansion. We then test data scaling limits by curating VSI-590K and training
Cambrian-S, achieving +30% absolute improvement on VSI-Bench without
sacrificing general capabilities. Yet performance on VSI-SUPER remains limited,
indicating that scale alone is insufficient for spatial supersensing. We
propose predictive sensing as a path forward, presenting a proof-of-concept in
which a self-supervised next-latent-frame predictor leverages surprise
(prediction error) to drive memory and event segmentation. On VSI-SUPER, this
approach substantially outperforms leading proprietary baselines, showing that
spatial supersensing requires models that not only see but also anticipate,
select, and organize experience.

</details>


### [61] [InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual Generation](https://arxiv.org/abs/2511.04675)
*Jinlai Liu,Jian Han,Bin Yan,Hui Wu,Fengda Zhu,Xing Wang,Yi Jiang,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

TL;DR: InfinityStar提出了一种统一的时空自回归方法，可生成高分辨率图像和动态视频，并在多个生成任务上表现优异，速度大幅快于传统扩散方法。


<details>
  <summary>Details</summary>
Motivation: 当前高分辨率图像和视频生成普遍依赖扩散模型，不仅速度慢、计算资源消耗大，且在时空建模统一性上存在不足。因此，迫切需要更高效、统一的生成框架，满足工业级视频生成需求。

Method: 提出InfinityStar：纯离散的时空自回归架构，能同时捕捉空间和时间上的依赖关系。该设计可直接应用于文生图、文生视频、图生视频等多种视频生成任务，采用简明的自回归时序策略。

Result: 实验显示InfinityStar在VBench测试集上得分83.74，显著领先其他自回归模型，甚至超过部分扩散模型（如HunyuanVideo）；且在无额外优化下，能够比扩散模型快10倍生成5秒720p高清视频。

Conclusion: InfinityStar是首个能生成工业级720p视频的离散自回归生成模型，在高效、高质量视频生成领域具有开创意义，将推动产业与学界进一步发展。

Abstract: We introduce InfinityStar, a unified spacetime autoregressive framework for
high-resolution image and dynamic video synthesis. Building on the recent
success of autoregressive modeling in both vision and language, our purely
discrete approach jointly captures spatial and temporal dependencies within a
single architecture. This unified design naturally supports a variety of
generation tasks such as text-to-image, text-to-video, image-to-video, and long
interactive video synthesis via straightforward temporal autoregression.
Extensive experiments demonstrate that InfinityStar scores 83.74 on VBench,
outperforming all autoregressive models by large margins, even surpassing some
diffusion competitors like HunyuanVideo. Without extra optimizations, our model
generates a 5s, 720p video approximately 10x faster than leading
diffusion-based methods. To our knowledge, InfinityStar is the first discrete
autoregressive video generator capable of producing industrial level 720p
videos. We release all code and models to foster further research in efficient,
high-quality video generation.

</details>


### [62] [Tracking and Understanding Object Transformations](https://arxiv.org/abs/2511.04678)
*Yihong Sun,Xinyu Yang,Jennifer J. Sun,Bharath Hariharan*

Main category: cs.CV

TL;DR: 该论文提出了Track Any State任务及其数据集VOST-TAS，旨在跟踪在状态变化（如苹果被切开、毛毛虫变蝴蝶）过程中物体的轨迹和状态变化，并介绍了TubeletGraph系统，有效提升了复杂变化下的目标跟踪和状态理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在物体发生明显外观变化（如切割、成长等）后，常常无法继续跟踪目标，导致跟踪中断，无法全面理解物体状态的时序演变。因此，需要一种能处理状态变化且能描述状态过程的新方法及评价基准。

Method: 提出了Track Any State任务以及VOST-TAS数据集，要求能跟踪物体的连续状态变化并描述这些变化。方法上提出TubeletGraph系统，以零样本（zero-shot）方式识别丢失的轨迹，通过语义和时空邻近性综合判断、整合新的轨迹，并通过推理建立状态图来描述物体状态随时间演化的过程。

Result: TubeletGraph在应对物体状态变化时，实现了领先的跟踪性能。其还表现出在时间定位与复杂语义推理方面的潜力，能够更深入理解目标的转换过程，对复杂动态场景具备优势。

Conclusion: 该研究推动了目标跟踪领域向更复杂、更真实世界状态变化的理解发展，并为后续相关研究提供了新的基准数据集和有效基础方法，展现了强拓展性和应用前景。

Abstract: Real-world objects frequently undergo state transformations. From an apple
being cut into pieces to a butterfly emerging from its cocoon, tracking through
these changes is important for understanding real-world objects and dynamics.
However, existing methods often lose track of the target object after
transformation, due to significant changes in object appearance. To address
this limitation, we introduce the task of Track Any State: tracking objects
through transformations while detecting and describing state changes,
accompanied by a new benchmark dataset, VOST-TAS. To tackle this problem, we
present TubeletGraph, a zero-shot system that recovers missing objects after
transformation and maps out how object states are evolving over time.
TubeletGraph first identifies potentially overlooked tracks, and determines
whether they should be integrated based on semantic and proximity priors. Then,
it reasons about the added tracks and generates a state graph describing each
observed transformation. TubeletGraph achieves state-of-the-art tracking
performance under transformations, while demonstrating deeper understanding of
object transformations and promising capabilities in temporal grounding and
semantic reasoning for complex object transformations. Code, additional
results, and the benchmark dataset are available at
https://tubelet-graph.github.io.

</details>


### [63] [Carousel: A High-Resolution Dataset for Multi-Target Automatic Image Cropping](https://arxiv.org/abs/2511.04680)
*Rafe Loya,Andrew Hamara,Benjamin Estell,Benjamin Kilpatrick,Andrew C. Freeman*

Main category: cs.CV

TL;DR: 本文关注于自动图像裁剪特别是生成多个具有美学吸引力且彼此不同的裁剪区域。


<details>
  <summary>Details</summary>
Motivation: 当前大部分自动裁剪方法只关注生成单一裁剪，而在实际如社交媒体应用中，往往需要多个不同的裁剪以适应多图展示场景，因此亟需研究多裁剪的美学图像自动裁剪方法。

Method: 作者引入了包含277张图片及人工标签的数据集，评估了多种单裁剪模型，通过图像分割算法作为预处理，对比分析其在多裁剪任务中的效果。

Result: 实验表明，现有单裁剪模型搭配图像分割算法作为预处理，能够提高多裁剪任务中图像美学裁剪的表现。

Conclusion: 多裁剪美学图像裁剪是当前自动图像裁剪研究的空白，作者提出数据集和方法，验证了现有效技术在该领域的应用潜力。

Abstract: Automatic image cropping is a method for maximizing the human-perceived
quality of cropped regions in photographs. Although several works have proposed
techniques for producing singular crops, little work has addressed the problem
of producing multiple, distinct crops with aesthetic appeal. In this paper, we
motivate the problem with a discussion on modern social media applications,
introduce a dataset of 277 relevant images and human labels, and evaluate the
efficacy of several single-crop models with an image partitioning algorithm as
a pre-processing step. The dataset is available at
https://github.com/RafeLoya/carousel.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [64] [Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs](https://arxiv.org/abs/2511.03738)
*Pranav Bhandari,Nicolas Fay,Sanjeevan Selvaganapathy,Amitava Datta,Usman Naseem,Mehwish Nasim*

Main category: cs.CL

TL;DR: 本文提出了一种通过大模型内部状态实现稳健、可控地调整和表达大五人格特质（开放性、责任心、外向性、宜人性、神经质）的方法，从而更好地操控模型的行为风格。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型具有隐含的人格，但如何精确地控制或对齐这些人格特质仍是未解决的问题。有效的行为操控方法对任务和应用十分关键，但相关机制尚不完善。

Method: 研究提出了一套新流程：基于大五人格理论，抽取transformer层的隐藏状态，用低秩子空间方法发现特征，并跨不同模型寻找人格注入的最优层。最终通过动态层选择，将人格方向转变为可操作的行为引导机制。

Result: 发现大五人格特征在模型内部表现为低秩共享子空间，可以通过对这些隐空间结构的微小调整，有效改变模型输出的人格表达，同时不会影响流畅性、多样性及模型通用能力。

Conclusion: 该方法促进了心理人格理论与大模型行为操控的结合，推动了可控、人格化大模型的发展，对应用和理论都有重要意义。

Abstract: Large Language Models exhibit implicit personalities in their generation, but
reliably controlling or aligning these traits to meet specific needs remains an
open challenge. The need for effective mechanisms for behavioural manipulation
of the model during generation is a critical gap in the literature that needs
to be fulfilled. Personality-aware LLMs hold a promising direction towards this
objective. However, the relationship between these psychological constructs and
their representations within LLMs remains underexplored and requires further
investigation. Moreover, it is intriguing to understand and study the use of
these representations to steer the models' behaviour. We propose a novel
pipeline that extracts hidden state activations from transformer layers using
the Big Five Personality Traits (Openness, Conscientiousness, Extraversion,
Agreeableness and Neuroticism), which is a comprehensive and empirically
validated framework to model human personality applies low-rank subspace
discovery methods, and identifies trait-specific optimal layers across
different model architectures for robust injection. The resulting
personality-aligned directions are then operationalised through a flexible
steering framework with dynamic layer selection, enabling precise control of
trait expression in LLM outputs. Our findings reveal that personality traits
occupy a low-rank shared subspace, and that these latent structures can be
transformed into actionable mechanisms for effective steering through careful
perturbations without impacting the fluency, variance and general capabilities,
helping to bridge the gap between psychological theory and practical model
alignment.

</details>


### [65] [TextualVerifier: Verify TextGrad Step-by-Step](https://arxiv.org/abs/2511.03739)
*Eugenius Mario Situmorang,Adila Alfa Krisnadhi,Ari Wibisono*

Main category: cs.CL

TL;DR: 本文提出了TextualVerifier，一种用于文本自动微分(TextGrad)的自我验证框架，通过链式思维和大模型多数投票提升文本推理的正确性，实现了无需数值梯度的高可靠性文本优化。


<details>
  <summary>Details</summary>
Motivation: TextGrad在文本自动微分和文本优化中具有创新性，但缺乏自我验证机制，容易导致推理结果错误，亟需一种有效的方法确保系统推理的有效性与可靠性。

Method: TextualVerifier结合链式思维分解、变体生成、多数投票和共识聚合四个阶段流程，通过大模型实现推理校验，并在TextGrad的损失函数和结果验证阶段无侵入集成。实验分两步，第一步是单独评估，第二步将其集成到TextGrad中并在标准数据集上进行测试。

Result: 实验表明，TextualVerifier显著提升了推理有效性，独立评估提升29%；集成到TextGrad损失函数后，准确率提升2.2个百分点（68.2%到70.4%），平均仅需5.9次LLM调用；在不同基准测试中提升幅度为3.92至10.71个百分点。

Conclusion: TextualVerifier为TextGrad提供了首个基于大模型的自我验证方案，不依赖数值梯度，显著提升了文本优化系统的推理可靠性，对文本化优化的验证技术开辟了新的方向。

Abstract: TextGrad is a novel approach to text-based automatic differentiation that
enables composite AI systems to perform optimization without explicit numerical
equations. However, it currently lacks self-verification mechanisms that ensure
reasoning validity in text-based decision making. This research introduces
TextualVerifier, a verification framework that leverages chain-of-thought
reasoning and majority voting with large language models to address this
verification gap. TextualVerifier implements a four-stage workflow:
chain-of-thought decomposition, variant generation, majority voting, and
consensus aggregation. It integrates non-invasively with TextGrad at both the
loss function and optimization result verification stages. Experimental
evaluation using the Gemini 1.5 Pro model is conducted in two phases: (1)
standalone evaluation on PRM800K, and (2) integrated evaluation with TextGrad
on GPQA-Diamond, MMLU-ML, and MMLU-CP benchmarks. Results show statistically
significant improvements (p < 0.001). In phase one, TextualVerifier improves
the validity of reasoning steps by 29 percent. In phase two, integration into
TextGrad loss function yields a 2.2 percentage point gain from 68.2 to 70.4
percent with a moderate overhead of 5.9 LLM calls on average. Further
evaluations of TextualVerifier versioning yield 8.08, 10.71, and 3.92
percentage point improvements on GPQA, MMLU-ML, and MMLU-CP respectively.
TextualVerifier thus presents the first self-verification framework for
TextGrad through LLM-based techniques without requiring numerical gradients,
enabling more reliable reasoning and opening new directions for verification in
text-based optimization.

</details>


### [66] [GRDD+: An Extended Greek Dialectal Dataset with Cross-Architecture Fine-tuning Evaluation](https://arxiv.org/abs/2511.03772)
*Stergios Chatzikyriakidis,Dimitris Papadakis,Sevasti-Ioanna Papaioannou,Erofili Psaltaki*

Main category: cs.CL

TL;DR: 本论文扩展了希腊方言数据集，涵盖10种方言，总词数达到6374939，是迄今为止最大、最丰富的希腊方言数据集。作者还通过微调多种大型语言模型，验证高质量方言数据对模型表现的影响，并与主流前沿模型对比。


<details>
  <summary>Details</summary>
Motivation: 现有希腊方言数据集GRDD数据量和方言覆盖度有限，难以满足大语言模型对多样化高质量希腊方言数据的需求，因此作者有动力构建更大、更全面的希腊方言数据集。

Method: 作者在原有GRDD基础上大规模采集和整理Cretan、Cypriot、Pontic和Northern Greek等方言的新数据，新增收集6种方言，总计覆盖10种希腊方言，并对Llama-3-8B、Llama-3.1-8B、Krikri-8B等三种架构进行微调，最后与Claude-3.7-Sonnet、Gemini-2.5、ChatGPT-5这些前沿模型进行效果对比。

Result: 建立了迄今为止最多样且体量最大的希腊方言数据集（6374939词，10种方言），并在微调模型实验中，显示高质量、多样性的方言数据有助于提升大语言模型对于希腊方言的处理能力。

Conclusion: 丰富的希腊方言数据集可以显著提升大模型的方言理解及生成表现，为希腊方言的自动处理和保护提供了数据和方法基础，对相关方向的研究具有积极推动作用。

Abstract: We present an extended Greek Dialectal Dataset (GRDD+) 1that complements the
existing GRDD dataset with more data from Cretan, Cypriot, Pontic and Northern
Greek, while we add six new varieties: Greco-Corsican, Griko (Southern Italian
Greek), Maniot, Heptanesian, Tsakonian, and Katharevusa Greek. The result is a
dataset with total size 6,374,939 words and 10 varieties. This is the first
dataset with such variation and size to date. We conduct a number of
fine-tuning experiments to see the effect of good quality dialectal data on a
number of LLMs. We fine-tune three model architectures (Llama-3-8B,
Llama-3.1-8B, Krikri-8B) and compare the results to frontier models
(Claude-3.7-Sonnet, Gemini-2.5, ChatGPT-5).

</details>


### [67] [PLLuM: A Family of Polish Large Language Models](https://arxiv.org/abs/2511.03823)
*Jan Kocoń,Maciej Piasecki,Arkadiusz Janz,Teddy Ferdinan,Łukasz Radliński,Bartłomiej Koptyra,Marcin Oleksy,Stanisław Woźniak,Paweł Walkowiak,Konrad Wojtasik,Julia Moska,Tomasz Naskręt,Bartosz Walkowiak,Mateusz Gniewkowski,Kamil Szyc,Dawid Motyka,Dawid Banach,Jonatan Dalasiński,Ewa Rudnicka,Bartłomiej Alberski,Tomasz Walkowiak,Aleksander Szczęsny,Maciej Markiewicz,Tomasz Bernaś,Hubert Mazur,Kamil Żyta,Mateusz Tykierko,Grzegorz Chodak,Tomasz Kajdanowicz,Przemysław Kazienko,Agnieszka Karlińska,Karolina Seweryn,Anna Kołos,Maciej Chrabąszcz,Katarzyna Lorenc,Aleksandra Krasnodębska,Artur Wilczek,Katarzyna Dziewulska,Paula Betscher,Zofia Cieślińska,Katarzyna Kowol,Daria Mikoś,Maciej Trzciński,Dawid Krutul,Marek Kozłowski,Sławomir Dadas,Rafał Poświata,Michał Perełkiewicz,Małgorzata Grębowiec,Maciej Kazuła,Marcin Białas,Roman Roszko,Danuta Roszko,Jurgita Vaičenonienė,Andrius Utka,Paweł Levchuk,Paweł Kowalski,Irena Prawdzic-Jankowska,Maciej Ogrodniczuk,Monika Borys,Anna Bulińska,Wiktoria Gumienna,Witold Kieraś,Dorota Komosińska,Katarzyna Krasnowska-Kieraś,Łukasz Kobyliński,Martyna Lewandowska,Marek Łaziński,Mikołaj Łątkowski,Dawid Mastalerz,Beata Milewicz,Agnieszka Anna Mykowiecka,Angelika Peljak-Łapińska,Sandra Penno,Zuzanna Przybysz,Michał Rudolf,Piotr Rybak,Karolina Saputa,Aleksandra Tomaszewska,Aleksander Wawer,Marcin Woliński,Joanna Wołoszyn,Alina Wróblewska,Bartosz Żuk,Filip Żarnecki,Konrad Kaczyński,Anna Cichosz,Zuzanna Deckert,Monika Garnys,Izabela Grabarczyk,Wojciech Janowski,Sylwia Karasińska,Aleksandra Kujawiak,Piotr Misztela,Maria Szymańska,Karolina Walkusz,Igor Siek,Jakub Kwiatkowski,Piotr Pęzik*

Main category: cs.CL

TL;DR: 本论文介绍了PLLuM，这是首个专门为波兰语开发的大规模开源基础语言模型，涵盖了模型的构建、训练和应用，并展示其在特定应用中的有效性。


<details>
  <summary>Details</summary>
Motivation: 目前主流的大语言模型主要以英语为中心，对其他语言（如波兰语）的支持十分有限，商业产品也缺乏透明性和本地文化适应性，因此亟需有高质量、开放且本地化的波兰语语言模型。

Method: 由多个波兰研究机构联合开发，构建了一个包含1400亿词的波兰语文本语料库，定制了77k条指令数据集及10万条偏好优化数据集，并嵌入了负责治理与安全的AI框架，包含混合式输出校正和安全过滤模块。详细介绍了模型架构、训练流程以及base模型和指令微调模型的对齐技术。

Result: PLLuM在相关下游任务中（如公共行政领域）展现了良好的效果，说明了其在实际应用场景中的潜力和有效性。

Conclusion: PLLuM模型开源发布，旨在促进开放研究、增强波兰AI主权，并为本地和国际语境下的语言模型开发提供参考。

Abstract: Large Language Models (LLMs) play a central role in modern artificial
intelligence, yet their development has been primarily focused on English,
resulting in limited support for other languages. We present PLLuM (Polish
Large Language Model), the largest open-source family of foundation models
tailored specifically for the Polish language. Developed by a consortium of
major Polish research institutions, PLLuM addresses the need for high-quality,
transparent, and culturally relevant language models beyond the English-centric
commercial landscape. We describe the development process, including the
construction of a new 140-billion-token Polish text corpus for pre-training, a
77k custom instructions dataset, and a 100k preference optimization dataset. A
key component is a Responsible AI framework that incorporates strict data
governance and a hybrid module for output correction and safety filtering. We
detail the models' architecture, training procedures, and alignment techniques
for both base and instruction-tuned variants, and demonstrate their utility in
a downstream task within public administration. By releasing these models
publicly, PLLuM aims to foster open research and strengthen sovereign AI
technologies in Poland.

</details>


### [68] [STARS: Segment-level Token Alignment with Rejection Sampling in Large Language Models](https://arxiv.org/abs/2511.03827)
*Mohammad Atif Quamar,Mohammad Areeb,Mikhail Kuznetsov,Muslum Ozgur Ozmen,Z. Berkay Celik*

Main category: cs.CL

TL;DR: 本文提出了一种新的大语言模型对齐方法STARS，通过在生成时对短token片段进行采样和拒绝，提高效率和对齐质量，优于主流微调和部分采样方法。


<details>
  <summary>Details</summary>
Motivation: 现有大模型对齐方法如微调计算资源消耗大、不够高效，而推理时采样方法（如Best-of-N）虽然对齐好，但计算量更不可接受。因此，亟需一种高效且能有效对齐模型生成结果的方法。

Method: 作者提出STARS方法：在推理（解码）时，将输出序列分割为定长的小片段，对每个片段进行采样和评分，若评分不高则拒绝，并重新采样，从而在生成过程中动态地纠正输出方向。该方法可以结合任意奖励模型，并避免传统微调和Best-of-N等方法的高昂成本。

Result: 在六种大型语言模型上实验，STARS方法在win rate指标上相较于SFT提高了最高14.9个百分点，相较于DPO提升了最高4.3个百分点，同时在效果上与强力的Best-of-N采样接近或相当，但计算效率更高。

Conclusion: STARS方法证明了基于奖励引导的细粒度采样对齐不仅通用、健壮且高效，有望替代传统微调或全序列样本重排序方法，应用前景广阔。

Abstract: Aligning large language models with human values is crucial for their safe
deployment; however, existing methods, such as fine-tuning, are computationally
expensive and suboptimal. In contrast, inference-time approaches like Best-of-N
sampling require practically infeasible computation to achieve optimal
alignment. We propose STARS: Segment-level Token Alignment with Rejection
Sampling, a decoding-time algorithm that steers model generation by iteratively
sampling, scoring, and rejecting/accepting short, fixed-size token segments.
This allows for early correction of the generation path, significantly
improving computational efficiency and boosting alignment quality. Across a
suite of six LLMs, we show that STARS outperforms Supervised Fine-Tuning (SFT)
by up to 14.9 percentage points and Direct Preference Optimization (DPO) by up
to 4.3 percentage points on win-rates, while remaining highly competitive with
strong Best-of-N baselines. Our work establishes granular, reward-guided
sampling as a generalizable, robust, and efficient alternative to traditional
fine-tuning and full-sequence ranking methods for aligning LLMs.

</details>


### [69] [Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification](https://arxiv.org/abs/2511.03830)
*Mikołaj Langner,Jan Eliasz,Ewa Rudnicka,Jan Kocoń*

Main category: cs.CL

TL;DR: 该论文提出了一种高效的多标签文本分类方法，将多标签任务分解为一系列独立的二分类（是/否）判断，并结合前缀缓存机制，提升了短文本推理的效率且准确率不下降。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）在多标签分类任务中，生成全部标签通常效率较低，且推理成本高，尤其是在需处理多个维度或标签时。作者希望通过任务重构和模型优化，提高LLM在多标签分类上的实用性和效率。

Method: 将多标签分类任务重构为对每一个分类维度分别进行独立的二分类（是/否）查询，每次只判断某一标签是否成立。引入前缀缓存机制来复用推理部分过程，提升效率。同时，采用LLM至SLM（小型语言模型）蒸馏，将高性能LLM生成的多重注释用于微调小模型，提高小模型表现。

Result: 在涵盖情感和情绪等24维度的情感文本分析任务中，经过蒸馏与微调的小模型（如HerBERT-Large、CLARIN-1B等）在训练维度上大幅超过了zero-shot基线性能。方法在保证准确性的同时，有效提升了推理效率。

Conclusion: 将多标签分类转化为一系列二分类任务，结合蒸馏和缓存优化，是一种高效、可扩展的LLM分类方案。方法适用于多领域，不仅限于情感分析。

Abstract: We introduce a method for efficient multi-label text classification with
large language models (LLMs), built on reformulating classification tasks as
sequences of dichotomic (yes/no) decisions. Instead of generating all labels in
a single structured response, each target dimension is queried independently,
which, combined with a prefix caching mechanism, yields substantial efficiency
gains for short-text inference without loss of accuracy. To demonstrate the
approach, we focus on affective text analysis, covering 24 dimensions including
emotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator
model (DeepSeek-V3) provides multiple annotations per text, which are
aggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B,
Gemma3-1B). The fine-tuned models show significant improvements over zero-shot
baselines, particularly on the dimensions seen during training. Our findings
suggest that decomposing multi-label classification into dichotomic queries,
combined with distillation and cache-aware inference, offers a scalable and
effective framework for LLM-based classification. While we validate the method
on affective states, the approach is general and applicable across domains.

</details>


### [70] [Evaluating Machine Translation Datasets for Low-Web Data Languages: A Gendered Lens](https://arxiv.org/abs/2511.03880)
*Hellina Hailu Nigatu,Bethelhem Yemane Mamo,Bontu Fufa Balcha,Debora Taye Tesfaye,Elbethel Daniel Zewdie,Ikram Behiru Nesiru,Jitu Ewnetu Hailu,Senait Mengesha Yayo*

Main category: cs.CL

TL;DR: 本文分析了三种低资源语言的机器翻译数据集，强调数据质量和性别代表问题，发现数据集存在性别偏见和有害内容，数据量大并不代表高质量。


<details>
  <summary>Details</summary>
Motivation: 当前NLP领域对低资源语言越来越重视，但在大规模收集数据集时，往往侧重数量而忽略了质量问题，由此可能导致技术性能下降和助长社会偏见。作者旨在揭示这一问题，并提醒业界关注低资源语言数据集中的内容质量及其潜在有害影响。

Method: 作者选择了三种低资源语言（Afan Oromo、阿姆哈拉语和提格利尼亚语）的机器翻译数据集，通过分析数据集中政治、宗教、健康、新闻、体育等领域的文本比例，重点关注性别相关内容，如人物姓名、动词语法性别和刻板印象等，进一步考察了数据中针对女性的有害及不当表述。

Result: 研究发现：1）训练数据大量涵盖政治和宗教领域，基准测试集侧重新闻、健康和体育领域；2）数据集呈现出男性性别过度代表，无论是在名字、语法性别还是刻板印象中；3）存在针对女性的有害和毒性描述，且数据量较大的语言问题更突出。

Conclusion: 作者强调，提高数据集数量不能替代对质量的把控，数据偏见和有害内容可能影响模型表现和加剧社会歧视，呼吁NLP社区更早更积极地审查和修正低资源语言数据集中的有害内容。

Abstract: As low-resourced languages are increasingly incorporated into NLP research,
there is an emphasis on collecting large-scale datasets. But in prioritizing
quantity over quality, we risk 1) building language technologies that perform
poorly for these languages and 2) producing harmful content that perpetuates
societal biases. In this paper, we investigate the quality of Machine
Translation (MT) datasets for three low-resourced languages--Afan Oromo,
Amharic, and Tigrinya, with a focus on the gender representation in the
datasets. Our findings demonstrate that while training data has a large
representation of political and religious domain text, benchmark datasets are
focused on news, health, and sports. We also found a large skew towards the
male gender--in names of persons, the grammatical gender of verbs, and in
stereotypical depictions in the datasets. Further, we found harmful and toxic
depictions against women, which were more prominent for the language with the
largest amount of data, underscoring that quantity does not guarantee quality.
We hope that our work inspires further inquiry into the datasets collected for
low-resourced languages and prompts early mitigation of harmful content.
WARNING: This paper contains discussion of NSFW content that some may find
disturbing.

</details>


### [71] [GRAD: Graph-Retrieved Adaptive Decoding for Hallucination Mitigation](https://arxiv.org/abs/2511.03900)
*Manh Nguyen,Sunil Gupta,Dai Do,Hung Le*

Main category: cs.CL

TL;DR: 本文提出了一种新方法GRAD，通过在解码时引用语料库中的证据，有效减少大语言模型产生幻觉的情况，无需额外训练并取得优异效果。


<details>
  <summary>Details</summary>
Motivation: 随着大模型规模增长，幻觉问题依然突出。现有缓解方法要么依赖外部知识库，复杂且成本高，要么是基于prompt的方法，易受领域影响且鲁棒性不足。研究者希望开发一种轻量、高效且通用的幻觉缓解方式。

Method: 作者提出GRAD（Graph-Retrieved Adaptive Decoding），该方法在解码阶段，从检索到的小型语料库中积累token转移信息，构建稀疏的token转移图。然后在生成过程中将该图得到的概率与原模型的概率自适应融合，从而推动模型输出基于高证据支持的内容，无需重新训练模型。

Result: 在三个大语言模型及多个问答基准测试上，GRAD方法比现有基线取得更高的准确性（最高提高9.7%）、更低幻觉率（降低8.6%）和更好的正确性（提升6.9%），且truth-informativeness产品分数也达到各方法最高。

Conclusion: GRAD是一种轻量、即插即用、无需模型重训练的解码增强方法，通过引入语料级证据显著提升模型输出的真实性和可验证性，优于传统对比解码和知识图增强方法，为缓解大模型幻觉问题提供了有效途径。

Abstract: Hallucination mitigation remains a persistent challenge for large language
models (LLMs), even as model scales grow. Existing approaches often rely on
external knowledge sources, such as structured databases or knowledge graphs,
accessed through prompting or retrieval. However, prompt-based grounding is
fragile and domain-sensitive, while symbolic knowledge integration incurs heavy
retrieval and formatting costs. Motivated by knowledge graphs, we introduce
Graph-Retrieved Adaptive Decoding (GRAD), a decoding-time method that grounds
generation in corpus-derived evidence without retraining. GRAD constructs a
sparse token transition graph by accumulating next-token logits across a small
retrieved corpus in a single forward pass. During decoding, graph-retrieved
logits are max-normalized and adaptively fused with model logits to favor
high-evidence continuations while preserving fluency. Across three models and a
range of question-answering benchmarks spanning intrinsic, extrinsic
hallucination, and factuality tasks, GRAD consistently surpasses baselines,
achieving up to 9.7$\%$ higher intrinsic accuracy, 8.6$\%$ lower hallucination
rates, and 6.9$\%$ greater correctness compared to greedy decoding, while
attaining the highest truth--informativeness product score among all methods.
GRAD offers a lightweight, plug-and-play alternative to contrastive decoding
and knowledge graph augmentation, demonstrating that statistical evidence from
corpus-level token transitions can effectively steer generation toward more
truthful and verifiable outputs.

</details>


### [72] [Context informs pragmatic interpretation in vision-language models](https://arxiv.org/abs/2511.03908)
*Alvin Wei Ming Tan,Ben Prystawski,Veronica Boyce,Michael C. Frank*

Main category: cs.CL

TL;DR: 本文研究了迭代指称游戏中，人类与视觉-语言模型在多轮次、上下文变化环境下的表现，并发现模型逐步逼近人类，但在抽象指称任务上依然存在难度。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在自然语言处理和多模态理解领域的进步，模型是否能像人类一样在复杂、变化的上下文中进行语用推理成为重要问题，迭代指称游戏为此提供了实验平台。

Method: 作者设计了多轮迭代指称游戏实验，比较人在不同上下文量、顺序和相关性条件下的表现，并让视觉-语言模型在相同条件下进行测试，以评估模型对上下文的敏感性和语用推理能力。

Result: 没有相关上下文时，模型表现略优于随机，但明显劣于人类；引入相关上下文后，模型表现显著提升。面对少量示例、涉及抽象指称的任务时，现有机器学习模型仍表现不佳。

Conclusion: 虽然视觉-语言模型在获取更多上下文后表现提升，但在高难度、抽象指称的场景下仍无法媲美人类，凸显出当前技术在复杂语用推理上的局限性。

Abstract: Iterated reference games - in which players repeatedly pick out novel
referents using language - present a test case for agents' ability to perform
context-sensitive pragmatic reasoning in multi-turn linguistic environments. We
tested humans and vision-language models on trials from iterated reference
games, varying the given context in terms of amount, order, and relevance.
Without relevant context, models were above chance but substantially worse than
humans. However, with relevant context, model performance increased
dramatically over trials. Few-shot reference games with abstract referents
remain a difficult task for machine learning models.

</details>


### [73] [The Human Flourishing Geographic Index: A County-Level Dataset for the United States, 2013--2023](https://arxiv.org/abs/2511.03915)
*Stefano M. Iacus,Devika Jain,Andrea Nasuto,Giuseppe Porro,Marcello Carammia,Andrea Vezzulli*

Main category: cs.CL

TL;DR: 本文提出了“人类繁荣地理指数”（HFGI），用以高时空分辨率量化美国各地的人类繁荣状态，数据来源为大规模推特文本分析。


<details>
  <summary>Details</summary>
Motivation: 现有衡量“人类繁荣”的方法通常时空分辨率低，无法细致反映地区、时间上的变化，且多依赖传统经济指标，不能全面描述幸福、健康、目的感、美德、人际关系和财务稳定等多维度。

Method: 作者收集了2013-2023年间约26亿条美国推文，利用微调大型语言模型，对推文中的内容在哈佛全球繁荣研究框架下的48个具体指标（包括迁移态度和腐败感知）进行自动分类，从而生成月度和年度的县/州级繁荣相关数据。

Result: 经验证，这些推文推导出的繁荣指标能准确反映实际概念，并与公认指标存在合理相关性。数据集可用于多学科分析社会福祉、不平等与社会变迁，时空分辨率前所未有。

Conclusion: 通过社会媒体大数据与AI模型，HFGI为理解美国社会繁荣的动态及其地区差异提供了创新且精细化的新工具，有望推动相关政策和学术研究。

Abstract: Quantifying human flourishing, a multidimensional construct including
happiness, health, purpose, virtue, relationships, and financial stability, is
critical for understanding societal well-being beyond economic indicators.
Existing measures often lack fine spatial and temporal resolution. Here we
introduce the Human Flourishing Geographic Index (HFGI), derived from analyzing
approximately 2.6 billion geolocated U.S. tweets (2013-2023) using fine-tuned
large language models to classify expressions across 48 indicators aligned with
Harvard's Global Flourishing Study framework plus attitudes towards migration
and perception of corruption. The dataset offers monthly and yearly county- and
state-level indicators of flourishing-related discourse, validated to confirm
that the measures accurately represent the underlying constructs and show
expected correlations with established indicators. This resource enables
multidisciplinary analyses of well-being, inequality, and social change at
unprecedented resolution, offering insights into the dynamics of human
flourishing as reflected in social media discourse across the United States
over the past decade.

</details>


### [74] [Direct Semantic Communication Between Large Language Models via Vector Translation](https://arxiv.org/abs/2511.03945)
*Fu-Chun Yang,Jason Eshraghian*

Main category: cs.CL

TL;DR: 本文提出了一种多大语言模型间在潜在语义层面的交流方式，通过向量转换（vector translations）实现高效信息传递，而不是传统的明文token交流。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型之间在多智能体协作（如辩论、反思、工具调用）中仅使用文本token交流，这种方式丢失了丰富的潜在语义信息，导致信息传递受限且计算效率低。作者希望通过语义层面的“桥接”提升信息交换效率和系统协同能力。

Method: 作者提出用学习得到的向量映射（latent bridge），实现不同模型语义空间之间的向量“翻译”。具体用dual-encoder训练Llama-2-7B与Mistral-7B-Instruct的表示互相翻译，并探索如何注入这些翻译后的向量（如30% blending）到目标模型中影响其生成。

Result: Llama-2-7B和Mistral-7B-Instruct间的向量翻译平均余弦对齐度达到0.538。注入30% blending的翻译向量能有效引导目标模型生成但不会造成logits不稳定。双向评估中发现普通模型的表征更容易转移，迁移不对称比为2.01:1。

Conclusion: （1）跨模型的潜在向量交流是可行的，并能确保计算稳定；（2）语义层级的信息共享提升协同AI系统的效率，有望推动AI系统协作从“令牌传递”转向“意义共享”。

Abstract: In multi-agent settings, such as debate, reflection, or tool-calling, large
language models (LLMs) pass messages as plain tokens, discarding most latent
semantics. This constrains information transfer and adds unnecessary
computational overhead. We form a latent bridge via vector translations, which
use learned mappings that enable direct semantic exchange between
representation spaces. A dual-encoder translator trained between Llama-2-7B and
Mistral-7B-Instruct attains an average cosine alignment of 0.538. Injecting the
translated vectors at 30 percent blending strength steers the target model's
generation without destabilizing logits. Bidirectional evaluation shows a
2.01:1 transfer asymmetry, indicating that general-purpose models yield more
transferable representations than instruction-tuned variants. This conservative
injection preserves computational stability while demonstrating that
cross-model latent communication is feasible, enabling collaborative AI systems
that share meaning rather than tokens.

</details>


### [75] [Abductive Inference in Retrieval-Augmented Language Models: Generating and Validating Missing Premises](https://arxiv.org/abs/2511.04020)
*Shiyin Lin*

Main category: cs.CL

TL;DR: 本文提出了将归纳推理（abductive inference）整合进检索增强型大语言模型（RAG）的新框架，以填补检索证据不完整导致推理断裂的问题，经实验证明本方法显著提升了答案的准确性与推理的可信度。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在检索证据不完整时常因推理链断裂而导致错误或不完整的答案，缺乏对推理空白的自动修补能力，因此需要机制来自动生成并验证缺失前提，提高系统稳健性和解释性。

Method: 本文提出的方法包括：检测当前检索证据是否充足；在证据不足时自动生成可能填补推理空白的缺失前提；并结合一致性和合理性判断机制对生成前提进行验证，以形成更完整的推理链。

Result: 在归纳推理和多跳问答任务上进行实验，结果显示，该方法在答案准确率和推理可信度（faithfulness）方面均有提升。

Conclusion: 将归纳推理机制引入RAG模型能有效补充推理所需的缺失证据，增强系统鲁棒性和可解释性，是提升RAG性能的有前景方向。

Abstract: Large Language Models (LLMs) enhanced with retrieval -- commonly referred to
as Retrieval-Augmented Generation (RAG) -- have demonstrated strong performance
in knowledge-intensive tasks. However, RAG pipelines often fail when retrieved
evidence is incomplete, leaving gaps in the reasoning process. In such cases,
\emph{abductive inference} -- the process of generating plausible missing
premises to explain observations -- offers a principled approach to bridge
these gaps. In this paper, we propose a framework that integrates abductive
inference into retrieval-augmented LLMs. Our method detects insufficient
evidence, generates candidate missing premises, and validates them through
consistency and plausibility checks. Experimental results on abductive
reasoning and multi-hop QA benchmarks show that our approach improves both
answer accuracy and reasoning faithfulness. This work highlights abductive
inference as a promising direction for enhancing the robustness and
explainability of RAG systems.

</details>


### [76] [WST: Weakly Supervised Transducer for Automatic Speech Recognition](https://arxiv.org/abs/2511.04035)
*Dongji Gao,Chenda Liao,Changliang Liu,Matthew Wiesner,Leibny Paola Garcia,Daniel Povey,Sanjeev Khudanpur,Jian Wu*

Main category: cs.CL

TL;DR: 本文提出了一种名为Weakly Supervised Transducer（WST）的新型端到端语音识别模型，能够容忍高达70%的转录错误，超越了现有的CTC类弱监督方法。


<details>
  <summary>Details</summary>
Motivation: 当前RNN-T模型在语音识别中表现优异，但依赖大量高质量标注数据，而此类数据昂贵且难以获取。因此，降低对高质量标注数据的依赖成为实际应用的重要诉求。

Method: 本文设计了WST模型，通过引入灵活的训练图，无需置信度估计或预训练辅助模型，即可自动适应和纠正转录中的错误，从而实现弱监督学习。

Result: 在合成和实际工业数据集上，WST即使面对高达70%的转录错误，仍能保持良好表现，且显著优于BTC和OTC等弱监督CTC方法。

Conclusion: WST方法在实际ASR应用中展现出较强的实用性和鲁棒性。相关实现也将开源，便于社区进一步应用与推广。

Abstract: The Recurrent Neural Network-Transducer (RNN-T) is widely adopted in
end-to-end (E2E) automatic speech recognition (ASR) tasks but depends heavily
on large-scale, high-quality annotated data, which are often costly and
difficult to obtain. To mitigate this reliance, we propose a Weakly Supervised
Transducer (WST), which integrates a flexible training graph designed to
robustly handle errors in the transcripts without requiring additional
confidence estimation or auxiliary pre-trained models. Empirical evaluations on
synthetic and industrial datasets reveal that WST effectively maintains
performance even with transcription error rates of up to 70%, consistently
outperforming existing Connectionist Temporal Classification (CTC)-based weakly
supervised approaches, such as Bypass Temporal Classification (BTC) and
Omni-Temporal Classification (OTC). These results demonstrate the practical
utility and robustness of WST in realistic ASR settings. The implementation
will be publicly available.

</details>


### [77] [T-FIX: Text-Based Explanations with Features Interpretable to eXperts](https://arxiv.org/abs/2511.04070)
*Shreya Havaldar,Helen Jin,Chaehyeon Kim,Anton Xue,Weiqiu You,Marco Gatti,Bhuvnesh Jain,Helen Qu,Daniel A Hashimoto,Amin Madani,Rajat Deo,Sameed Ahmed M. Khatana,Gary E. Weissman,Lyle Ungar,Eric Wong*

Main category: cs.CL

TL;DR: 本文关注于在知识密集型领域中，大语言模型（LLM）解释性输出与专家实际思考模式的契合程度，并提出了新的评测方法和数据集。


<details>
  <summary>Details</summary>
Motivation: 现有对LLM解释输出的评测方法（如合理性、一致性）无法有效评估其与专家直觉和专有知识的符合程度。但越来越多的知识密集领域（外科、天文、心理治疗等）用户需具备可信且高质量的解释，因此亟需新的评估标准。

Method: 作者提出并形式化了“专家一致性（expert alignment）”作为LLM解释评估新标准。基于该标准，与真实领域专家合作，开发了一套跨越7个知识密集领域的测试集（T-FIX），并设计新指标来定量衡量LLM解释与专家判断的一致性。

Result: 研究提供了一个覆盖多领域的专家一致性评测基准（T-FIX）及一组用于量化分析的创新指标，为评估LLM输出质量提供了更精细、一致且与实际专家需求相关的工具。

Conclusion: T-FIX及其衍生指标有效弥补了以往评测方法的不足，为今后LLM在知识密集型场景下的可解释性和专业应用提供了重要方向和评估手段。

Abstract: As LLMs are deployed in knowledge-intensive settings (e.g., surgery,
astronomy, therapy), users expect not just answers, but also meaningful
explanations for those answers. In these settings, users are often domain
experts (e.g., doctors, astrophysicists, psychologists) who require
explanations that reflect expert-level reasoning. However, current evaluation
schemes primarily emphasize plausibility or internal faithfulness of the
explanation, which fail to capture whether the content of the explanation truly
aligns with expert intuition. We formalize expert alignment as a criterion for
evaluating explanations with T-FIX, a benchmark spanning seven
knowledge-intensive domains. In collaboration with domain experts, we develop
novel metrics to measure the alignment of LLM explanations with expert
judgment.

</details>


### [78] [Plan of Knowledge: Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering](https://arxiv.org/abs/2511.04072)
*Xinying Qian,Ying Zhang,Yu Zhao,Baohang Zhou,Xuhui Sui,Xiaojie Yuan*

Main category: cs.CL

TL;DR: 本文针对时序知识图谱问答（TKGQA）任务，提出了一种结合大语言模型（LLM）与时序知识检索的新方法（PoK），显著提升了模型的推理准确率和知识检索精度。


<details>
  <summary>Details</summary>
Motivation: 当前TKGQA的主流方法未能充分理解时间约束中的复杂语义信息，大型语言模型虽具有强大的语义理解能力，但在时序推理和事实准确性方面仍存在短板。本文旨在克服LLM时序推理弱和幻觉等问题。

Method: 作者提出Plan of Knowledge（PoK）框架，包括：（1）将复杂时序问题分解为可执行的子目标以引导推理；（2）构建对比时序检索器，通过对比学习在时序知识库中检索语义、时间对齐的事实。（3）结合结构化计划与时序知识检索来强化LLM。

Result: 在四个TKGQA基准数据集上的大量实验表明，PoK在检索精度和推理准确率方面大幅提升，最大超越现有方法56.0%。

Conclusion: 结合知识规划与对比时序检索能显著增强LLM处理时序问答的能力，使模型具备更好的解释性和事实一致性，为TKGQA任务提供了更可靠的解决方案。

Abstract: Temporal Knowledge Graph Question Answering (TKGQA) aims to answer
time-sensitive questions by leveraging factual information from Temporal
Knowledge Graphs (TKGs). While previous studies have employed pre-trained TKG
embeddings or graph neural networks to inject temporal knowledge, they fail to
fully understand the complex semantic information of time constraints.
Recently, Large Language Models (LLMs) have shown remarkable progress,
benefiting from their strong semantic understanding and reasoning
generalization capabilities. However, their temporal reasoning ability remains
limited. LLMs frequently suffer from hallucination and a lack of knowledge. To
address these limitations, we propose the Plan of Knowledge framework with a
contrastive temporal retriever, which is named PoK. Specifically, the proposed
Plan of Knowledge module decomposes a complex temporal question into a sequence
of sub-objectives from the pre-defined tools, serving as intermediate guidance
for reasoning exploration. In parallel, we construct a Temporal Knowledge Store
(TKS) with a contrastive retrieval framework, enabling the model to selectively
retrieve semantically and temporally aligned facts from TKGs. By combining
structured planning with temporal knowledge retrieval, PoK effectively enhances
the interpretability and factual consistency of temporal reasoning. Extensive
experiments on four benchmark TKGQA datasets demonstrate that PoK significantly
improves the retrieval precision and reasoning accuracy of LLMs, surpassing the
performance of the state-of-the-art TKGQA methods by 56.0% at most.

</details>


### [79] [The truth is no diaper: Human and AI-generated associations to emotional words](https://arxiv.org/abs/2511.04077)
*Špela Vintar,Jan Jona Javoršek*

Main category: cs.CL

TL;DR: 本论文比较了人类与大型语言模型（LLM）在词语联想任务中的表现，特别关注于对带有情感色彩词语的联想，分析两者的异同。


<details>
  <summary>Details</summary>
Motivation: 词语联想是探究人类心理词汇结构的经典方法，但人与人之间的联想差异较大，受情感、经验和认知风格影响。了解人类与AI之间联想的异同，对于理解AI模型创造力和情感处理能力有重要意义。

Method: 作者对比了人类和LLM（如ChatGPT）对情感色彩词汇的联想，分析了两者的联想重合度、创造性以及情感表达等特征。

Result: 实验结果表明，LLM与人类在联想的重合度中等，但LLM更倾向于加强刺激词的情感色彩；此外，LLM的联想更可预测、创造性较低。

Conclusion: LLM在词语联想任务中的行为与人类部分相似，但在创造性和情感处理方式上存在明显差异，显示出LLM在模拟人类复杂心理词汇方面仍有局限性。

Abstract: Human word associations are a well-known method of gaining insight into the
internal mental lexicon, but the responses spontaneously offered by human
participants to word cues are not always predictable as they may be influenced
by personal experience, emotions or individual cognitive styles. The ability to
form associative links between seemingly unrelated concepts can be the driving
mechanisms of creativity. We perform a comparison of the associative behaviour
of humans compared to large language models. More specifically, we explore
associations to emotionally loaded words and try to determine whether large
language models generate associations in a similar way to humans. We find that
the overlap between humans and LLMs is moderate, but also that the associations
of LLMs tend to amplify the underlying emotional load of the stimulus, and that
they tend to be more predictable and less creative than human ones.

</details>


### [80] [Improving the Performance of Radiology Report De-identification with Large-Scale Training and Benchmarking Against Cloud Vendor Methods](https://arxiv.org/abs/2511.04079)
*Eva Prakash,Maayane Attias,Pierre Chambon,Justin Xu,Steven Truong,Jean-Benoit Delbrouck,Tessa Cook,Curtis Langlotz*

Main category: cs.CL

TL;DR: 本文提出并验证了一种基于transformer的放射科报告自动去标识化系统，在大规模标注数据集上训练后性能优越，优于现有学术和商业系统。


<details>
  <summary>Details</summary>
Motivation: 随着医疗数据共享需求增长，自动和高效去除受保护健康信息（PHI）变得尤为重要，尤其在放射科报告中。现有方法在跨机构、不同影像类型报告上表现有限，且尚有提升空间。作者希望通过大规模多机构、多类型数据集训练提升模型的通用性和准确性。

Method: 本研究基于最新的transformer模型，利用斯坦福大学两个大型标注放射科语料库（涵盖多种影像类型），引入新增AGE类别，实施微调训练。模型在斯坦福及宾大测试集上验证，并与商业云服务进行纵向性能对比，同时评估合成PHI（用hide-in-plain-sight方法生成）的识别稳定性。主要评价指标为precision、recall、F1分数。

Result: 模型在Penn数据集获得0.973、在Stanford数据集获得0.996的F1分数，优于或达到先前最好学术水平。合成PHI检测F1分数为0.959，50次独立去标识处理间一致性良好，在合成Penn报告上超越全部商业系统（本模型F1 0.960，对比商用系统0.632-0.754）。

Conclusion: 大规模、多模态训练的数据及方法提升了模型在不同机构及报告类型中的泛化和鲁棒性。合成PHI方法既保障隐私又保留数据可用性。综合来看，该方法为医学文本安全处理树立了新标杆。

Abstract: Objective: To enhance automated de-identification of radiology reports by
scaling transformer-based models through extensive training datasets and
benchmarking performance against commercial cloud vendor systems for protected
health information (PHI) detection. Materials and Methods: In this
retrospective study, we built upon a state-of-the-art, transformer-based, PHI
de-identification pipeline by fine-tuning on two large annotated radiology
corpora from Stanford University, encompassing chest X-ray, chest CT,
abdomen/pelvis CT, and brain MR reports and introducing an additional PHI
category (AGE) into the architecture. Model performance was evaluated on test
sets from Stanford and the University of Pennsylvania (Penn) for token-level
PHI detection. We further assessed (1) the stability of synthetic PHI
generation using a "hide-in-plain-sight" method and (2) performance against
commercial systems. Precision, recall, and F1 scores were computed across all
PHI categories. Results: Our model achieved overall F1 scores of 0.973 on the
Penn dataset and 0.996 on the Stanford dataset, outperforming or maintaining
the previous state-of-the-art model performance. Synthetic PHI evaluation
showed consistent detectability (overall F1: 0.959 [0.958-0.960]) across 50
independently de-identified Penn datasets. Our model outperformed all vendor
systems on synthetic Penn reports (overall F1: 0.960 vs. 0.632-0.754).
Discussion: Large-scale, multimodal training improved cross-institutional
generalization and robustness. Synthetic PHI generation preserved data utility
while ensuring privacy. Conclusion: A transformer-based de-identification model
trained on diverse radiology datasets outperforms prior academic and commercial
systems in PHI detection and establishes a new benchmark for secure clinical
text processing.

</details>


### [81] [A Characterization of List Language Identification in the Limit](https://arxiv.org/abs/2511.04103)
*Moses Charikar,Chirag Pabbaraju,Ambuj Tewari*

Main category: cs.CL

TL;DR: 本文探讨了语言极限识别问题，通过允许学习者每步给出一个$k$个猜测的列表，扩展了Gold和Angluin的经典理论，给出了$k$-list极限识别集合的精确刻画，并讨论了统计情形下的极限速率。


<details>
  <summary>Details</summary>
Motivation: 经典研究显示，在没有额外帮助下，除极少数情况外，针对一个输入样本队列确定语言的极限识别是不可能的。随着生成型学习取得进展，作者希望借助于“每次多猜几个（即列表猜测）”的权力，突破经典极限识别模型的限制，发掘更大范围语言可被正确识别的条件。

Method: 作者将学习者扩展为每次输出$k$个语言集合猜测（列表识别），通过递归一般化Angluin的可识别集合的刻画方法，形式化给出了什么样的语言集合能够被$k$-list极限识别，并进一步用在输入是某语言分布的i.i.d.流的统计情形下，分析识别的速度。

Result: 作者证明并严格刻画了：某个语言集合能被$k$-list极限识别，当且仅当该集合可被划分为$k$个极限可被1-list极限识别的子集合。此外，若集合可被$k$-list极限识别，则在统计分布下识别速率能达指数级，且不能更快；若不可以列表极限识别，则速率无法收敛。

Conclusion: 列表猜测极大扩展了可极限识别的语言集合，突破了经典Gold极限识别理论的局限。递归刻画结果对理解学习理论有重要参考意义，同时在统计和实际算法设计中具有应用价值。

Abstract: We study the problem of language identification in the limit, where given a
sequence of examples from a target language, the goal of the learner is to
output a sequence of guesses for the target language such that all the guesses
beyond some finite time are correct. Classical results of Gold showed that
language identification in the limit is impossible for essentially any
interesting collection of languages. Later, Angluin gave a precise
characterization of language collections for which this task is possible.
Motivated by recent positive results for the related problem of language
generation, we revisit the classic language identification problem in the
setting where the learner is given the additional power of producing a list of
$k$ guesses at each time step. The goal is to ensure that beyond some finite
time, one of the guesses is correct at each time step.
  We give an exact characterization of collections of languages that can be
$k$-list identified in the limit, based on a recursive version of Angluin's
characterization (for language identification with a list of size $1$). This
further leads to a conceptually appealing characterization: A language
collection can be $k$-list identified in the limit if and only if the
collection can be decomposed into $k$ collections of languages, each of which
can be identified in the limit (with a list of size $1$). We also use our
characterization to establish rates for list identification in the statistical
setting where the input is drawn as an i.i.d. stream from a distribution
supported on some language in the collection. Our results show that if a
collection is $k$-list identifiable in the limit, then the collection can be
$k$-list identified at an exponential rate, and this is best possible. On the
other hand, if a collection is not $k$-list identifiable in the limit, then it
cannot be $k$-list identified at any rate that goes to zero.

</details>


### [82] [Batch Prompting Suppresses Overthinking Reasoning Under Constraint: How Batch Prompting Suppresses Overthinking in Reasoning Models](https://arxiv.org/abs/2511.04108)
*Wenmo Qiu,Saurabh Srivastava*

Main category: cs.CL

TL;DR: 这篇论文发现，将多个推理任务批量处理（batch prompting）不仅能够节省大模型的推理成本，还能显著提升多步推理的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 之前的研究主要关注batch prompting的效率优势（即推理成本的节省），本论文想探索batch prompting是否还能改善模型推理的质量和行为方式。

Method: 作者在13个多样化的基准测试上，系统对比了单独推理与批量推理的性能差异，同时详细分析了模型生成过程中的行为特征，如自我修正频率、决定性、以及集体行为模式。

Result: 批量推理不仅在准确率上优于单独推理，还能显著减少生成的推理token（通常可减少3-5倍）。此外，批量推理能抑制过度思考、减少反复修正，并促进更果断的回答，还出现了批处理中的集体泛化现象。

Conclusion: 批量推理不只是提升推理速度的工具，更是能够增强大模型推理效果、提升效率和可靠性的重要正则化方法。

Abstract: Recent work has explored batch prompting as a strategy to amortize inference
cost in large language models (LLMs). In this paper, we show that batching
offers an additional, underappreciated benefit: it regularizes model behavior
during multi-step reasoning for Large Reasoning Models (LRMs). We conduct a
comprehensive study across 13 diverse benchmarks and observe that batching
improves accuracy while substantially reducing reasoning token usage, often by
3x-5x. Through detailed behavioral analysis, we find that batching suppresses
overthinking, reduces hedging language (e.g., repetitive self-corrections), and
encourages more decisive answers. Surprisingly, we also observe emergent
collective effects in batched inference: models often generalize patterns from
earlier examples to solve harder ones in the same batch. These findings
position batching not just as a throughput optimization, but as a powerful
inference-time regularizer for more efficient and reliable LLM reasoning.

</details>


### [83] [RIDE: Difficulty Evolving Perturbation with Item Response Theory for Mathematical Reasoning](https://arxiv.org/abs/2511.04120)
*Xinyuan Li,Murong Xu,Wenbiao Tao,Hanlun Zhu,Yike Zhao,Jipeng Zhang,Yunshi Lan*

Main category: cs.CL

TL;DR: 该论文提出了一种名为RIDE的基于对抗扰动的问题重写框架，用于更系统地评估大语言模型在数学推理上的真实能力。通过生成难度更高且良构的问题，有效揭示了现有大模型的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有的数学推理能力评测结果容易被训练数据泄漏或表层模式识别所夸大，且目前基于规则的扰动方法往往生成不合理的问题，阻碍了评测体系的发展。因此，需要更可靠的评测和问题生成方法。

Method: 提出RIDE框架，结合项目反应理论（IRT）定量衡量问题难度。运用35个大语言模型模拟“学生”作答，以此搭建难度排序器，并通过强化学习引导问题重写模型生成具有不同难度、良构的新问题，实现对数学基准题目的高质量对抗扰动。

Result: 将RIDE应用于竞赛级数学基准问题，生成的扰动新题显著降低了26个主流模型在数学推理上的表现，平均降幅达21.73%，充分暴露了模型在数学推理方面的鲁棒性不足。

Conclusion: RIDE框架能够系统、有效地评估大语言模型的数学推理能力，生成高难度且良构的问题，有助于推动数学推理评测基准的进化和模型能力的真实评判。

Abstract: Large language models (LLMs) achieve high performance on mathematical
reasoning, but these results can be inflated by training data leakage or
superficial pattern matching rather than genuine reasoning. To this end, an
adversarial perturbation-based evaluation is needed to measure true
mathematical reasoning ability. Current rule-based perturbation methods often
generate ill-posed questions and impede the systematic evaluation of question
difficulty and the evolution of benchmarks. To bridge this gap, we propose
RIDE, a novel adversarial question-rewriting framework that leverages Item
Response Theory (IRT) to rigorously measure question difficulty and to generate
intrinsically more challenging, well-posed variations of mathematical problems.
We employ 35 LLMs to simulate students and build a difficulty ranker from their
responses. This ranker provides a reward signal during reinforcement learning
and guides a question-rewriting model to reformulate existing questions across
difficulty levels. Applying RIDE to competition-level mathematical benchmarks
yields perturbed versions that degrade advanced LLM performance, with
experiments showing an average 21.73% drop across 26 models, thereby exposing
limited robustness in mathematical reasoning and confirming the validity of our
evaluation approach.

</details>


### [84] [CantoASR: Prosody-Aware ASR-LALM Collaboration for Low-Resource Cantonese](https://arxiv.org/abs/2511.04139)
*Dazhong Chen,Yi-Cheng Lin,Yuchen Huang,Ziwei Gong,Di Jiang,Zeying Xie,Yi R.,Fung*

Main category: cs.CL

TL;DR: 本文提出了一种名为CantoASR的新型自动语音识别（ASR）框架，有效提升了粤语等低资源声调语言的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 粤语等低资源语言由于数据匮乏、丰富声调及口音变化，ASR模型很难实现高准确率。现有主流大模型如Whisper在粤语语音识别中出现较高词错误率，亟需新方法提升低资源声调方言的ASR效果。

Method: 1. 引入CantoASR框架，将ASR和大音频-语言模型(LALM)协同纠错。2. 结合受控对齐（forced alignment）提取声学特征。3. 使用LoRA方法微调Whisper模块增强声调判别能力。4. 用instruction-tuned Qwen-Audio模型做基于韵律的误差纠正。

Result: 在真实自然粤语数据集上的测试显示，CantoASR明显优于Whisper-Large-V3，在字符错误率（CER）方面取得了大幅提升。

Conclusion: 融合声学特征和大语言模型推理能力，可以成为低资源声调和方言ASR场景下具有扩展性的新策略，有效提升识别性能。

Abstract: Automatic speech recognition (ASR) is critical for language accessibility,
yet low-resource Cantonese remains challenging due to limited annotated data,
six lexical tones, tone sandhi, and accent variation. Existing ASR models, such
as Whisper, often suffer from high word error rates. Large audio-language
models (LALMs), in contrast, can leverage broader contextual reasoning but
still require explicit tonal and prosodic acoustic cues. We introduce CantoASR,
a collaborative ASR-LALM error correction framework that integrates forced
alignment for acoustic feature extraction, a LoRA-finetuned Whisper for
improved tone discrimination, and an instruction-tuned Qwen-Audio for
prosody-aware correction. Evaluations on spontaneous Cantonese data show
substantial CER gains over Whisper-Large-V3. These findings suggest that
integrating acoustic cues with LALM reasoning provides a scalable strategy for
low-resource tonal and dialectal ASR.

</details>


### [85] [BAPPA: Benchmarking Agents, Plans, and Pipelines for Automated Text-to-SQL Generation](https://arxiv.org/abs/2511.04153)
*Fahim Ahmed,Md Mubtasim Ahasan,Jahir Sadik Monon,Muntasir Wahed,M Ashraful Amin,A K M Mahbubur Rahman,Amin Ahsan Ali*

Main category: cs.CL

TL;DR: 本文提出了三种多智能体多模型的Text-to-SQL管道方法，并系统评估其在不同规模开源大模型上的表现，发现这些方法能明显提升小模型的SQL生成准确率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理复杂数据库模式和推理时，Text-to-SQL生成能力有限，而当前研究多集中于大模型和复杂、效率低的方案，忽视了小模型的提升空间。

Method: 提出三种多智能体管道：(1) 多智能体讨论法，多个模型批判并修正SQL，最终由裁判选出答案；(2) 规划-编码法，一个模型给出分步实现规划，另一个模型生成SQL；(3) 编码选择法，多个模型独立生成SQL，由推理模型选最优结果；对不同规模的开源模型进行系统性性能基准测试。

Result: 在Bird-Bench Mini-Dev集上，三轮多智能体讨论能使小模型如Qwen2.5-7b-Instruct的执行准确率提升10.6%；多管道中，Reasoner-Coder管道效果最佳，Gemma 3 27B IT准确率由52.4%提升至56.4%。

Conclusion: 多智能体协作能显著提升小型LLM的Text-to-SQL能力，部分管道比单一模型方案有明显优势，为实用高效的Text-to-SQL系统提供新方案。

Abstract: Text-to-SQL systems provide a natural language interface that can enable even
laymen to access information stored in databases. However, existing Large
Language Models (LLM) struggle with SQL generation from natural instructions
due to large schema sizes and complex reasoning. Prior work often focuses on
complex, somewhat impractical pipelines using flagship models, while smaller,
efficient models remain overlooked. In this work, we explore three multi-agent
LLM pipelines, with systematic performance benchmarking across a range of small
to large open-source models: (1) Multi-agent discussion pipeline, where agents
iteratively critique and refine SQL queries, and a judge synthesizes the final
answer; (2) Planner-Coder pipeline, where a thinking model planner generates
stepwise SQL generation plans and a coder synthesizes queries; and (3)
Coder-Aggregator pipeline, where multiple coders independently generate SQL
queries, and a reasoning agent selects the best query. Experiments on the
Bird-Bench Mini-Dev set reveal that Multi-Agent discussion can improve small
model performance, with up to 10.6% increase in Execution Accuracy for
Qwen2.5-7b-Instruct seen after three rounds of discussion. Among the pipelines,
the LLM Reasoner-Coder pipeline yields the best results, with DeepSeek-R1-32B
and QwQ-32B planners boosting Gemma 3 27B IT accuracy from 52.4% to the highest
score of 56.4%. Codes are available at
https://github.com/treeDweller98/bappa-sql.

</details>


### [86] [Trustworthy LLM-Mediated Communication: Evaluating Information Fidelity in LLM as a Communicator (LAAC) Framework in Multiple Application Domains](https://arxiv.org/abs/2511.04184)
*Mohammed Musthafa Rafi,Adarsh Krishnamurthy,Aditya Balu*

Main category: cs.CL

TL;DR: 本论文针对AI生成内容导致的交流失真现象，提出以LLM为中介的全新沟通范式LAAC，并系统性评估其在真实通信中可用性的信任度问题。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI普及，信息发送者常用LLM将简单内容膨胀成冗长文本，接收者又借助LLM摘要回原意，造成沟通双方始终未接触真实内容，沟通变得失真且低效。作者希望解决AI生成内容的循环失真，引入更加可信和高效的交流模式。

Method: 提出LAAC（LLM as a Communicator）范式，用结构化对话捕捉发送者意图，作为智能中介实现规范、真实的信息沟通。论文从信息捕捉准确性、知识复现一致性、查询回应完整性三大信任维度，通过多场景、多实例的受控实验，测试LAAC在不同类型沟通中的性能。

Result: 实验表明，虽然LAAC能够改善沟通真实性，但在意图捕获准确性、一致性和回应可靠性等方面仍存在可测量的信任缺口，尤其是在高风险沟通场景中更需关注。

Conclusion: LAAC为AI辅助沟通提供了新思路，突出强调LLM作为沟通中介的潜力，但要实现实际部署，还需解决信任性相关的关键挑战。

Abstract: The proliferation of AI-generated content has created an absurd communication
theater where senders use LLMs to inflate simple ideas into verbose content,
recipients use LLMs to compress them back into summaries, and as a consequence
neither party engage with authentic content. LAAC (LLM as a Communicator)
proposes a paradigm shift - positioning LLMs as intelligent communication
intermediaries that capture the sender's intent through structured dialogue and
facilitate genuine knowledge exchange with recipients. Rather than perpetuating
cycles of AI-generated inflation and compression, LAAC enables authentic
communication across diverse contexts including academic papers, proposals,
professional emails, and cross-platform content generation. However, deploying
LLMs as trusted communication intermediaries raises critical questions about
information fidelity, consistency, and reliability. This position paper
systematically evaluates the trustworthiness requirements for LAAC's deployment
across multiple communication domains. We investigate three fundamental
dimensions: (1) Information Capture Fidelity - accuracy of intent extraction
during sender interviews across different communication types, (2)
Reproducibility - consistency of structured knowledge across multiple
interaction instances, and (3) Query Response Integrity - reliability of
recipient-facing responses without hallucination, source conflation, or
fabrication. Through controlled experiments spanning multiple LAAC use cases,
we assess these trust dimensions using LAAC's multi-agent architecture.
Preliminary findings reveal measurable trust gaps that must be addressed before
LAAC can be reliably deployed in high-stakes communication scenarios.

</details>


### [87] [Computational Turing Test Reveals Systematic Differences Between Human and AI Language](https://arxiv.org/abs/2511.04195)
*Nicolò Pagan,Petter Törnberg,Christopher A. Bail,Anikó Hannák,Christopher Barrie*

Main category: cs.CL

TL;DR: 本文提出并验证了一种自动化评估大语言模型生成文本与人类文本相似度的计算性“图灵测试”，并系统地比较多种模型及校准方式在人类仿真上的表现，发现模型文本与人类文本在情感表达等方面显著不同，并揭示了提升“人类性”与语义准确性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在社会科学领域被广泛用于模拟人类行为，但关于其生成的文本是否真正具备“人类性”的系统检验仍然不足，现有研究多依赖主观判断，可靠性有限，缺乏可扩展和客观的验证框架。

Method: 作者提出了计算性“图灵测试”框架，结合BERT可检测性得分、语义相似性、句法风格等指标量化模型文本与人类文本的相似度，并对9个开源模型在5种校准策略（如微调、风格化提示、上下文检索等）下模拟社交平台（X、Bluesky、Reddit）用户交互能力进行系统对比。

Result: 结果显示，即使经过校准调整，LLM生成文本在情感语气和情绪表达等方面仍与人类文本有明显差异；指令微调模型表现反而逊于基础模型，扩大参数规模亦未提升“类人性”；优化“人类性”通常会降低语义忠实度，两者难以兼得。

Conclusion: 本文为LLM模拟的验证和校准提供了客观可扩展的方法框架，并提示当前模型距离真实人类语言在核心情感表达等维度仍有显著差距，对相关研究提出了警示。

Abstract: Large language models (LLMs) are increasingly used in the social sciences to
simulate human behavior, based on the assumption that they can generate
realistic, human-like text. Yet this assumption remains largely untested.
Existing validation efforts rely heavily on human-judgment-based evaluations --
testing whether humans can distinguish AI from human output -- despite evidence
that such judgments are blunt and unreliable. As a result, the field lacks
robust tools for assessing the realism of LLM-generated text or for calibrating
models to real-world data. This paper makes two contributions. First, we
introduce a computational Turing test: a validation framework that integrates
aggregate metrics (BERT-based detectability and semantic similarity) with
interpretable linguistic features (stylistic markers and topical patterns) to
assess how closely LLMs approximate human language within a given dataset.
Second, we systematically compare nine open-weight LLMs across five calibration
strategies -- including fine-tuning, stylistic prompting, and context retrieval
-- benchmarking their ability to reproduce user interactions on X (formerly
Twitter), Bluesky, and Reddit. Our findings challenge core assumptions in the
literature. Even after calibration, LLM outputs remain clearly distinguishable
from human text, particularly in affective tone and emotional expression.
Instruction-tuned models underperform their base counterparts, and scaling up
model size does not enhance human-likeness. Crucially, we identify a trade-off:
optimizing for human-likeness often comes at the cost of semantic fidelity, and
vice versa. These results provide a much-needed scalable framework for
validation and calibration in LLM simulations -- and offer a cautionary note
about their current limitations in capturing human communication.

</details>


### [88] [LLM-as-a-Judge is Bad, Based on AI Attempting the Exam Qualifying for the Member of the Polish National Board of Appeal](https://arxiv.org/abs/2511.04205)
*Michał Karp,Anna Kubaszewska,Magdalena Król,Robert Król,Aleksander Smywiński-Pohl,Mateusz Szymański,Witold Wydmański*

Main category: cs.CL

TL;DR: 本论文实证测评了当前大语言模型（LLMs）能否通过波兰国家申诉院的官方资格考试。结果显示，LLMs在知识测验中表现尚可，但在写作判决部分未能及格，且自动评判与官方评判常有分歧。


<details>
  <summary>Details</summary>
Motivation: 评估现代LLM能否胜任专业法律考试，探究其在实际司法流程（如公采裁判）中替代人类的潜力和局限。

Method: 模拟LLMs作为考试候选人及评委，全面测试其在封闭书本和检索增强情境下的表现。考查内容包括法律知识多项选择题和实际书面判决撰写，并采用模型自动评估与专家委员会结果对比。

Result: 多款LLMs（包括GPT-4.1等）在法律知识测试中分数令人满意，但在实际写作题未达及格线。“模型评判模型答案”与官方评判结果经常出现偏差。

Conclusion: 当前LLMs难以胜任波兰公采法判例考试，主要局限在幻觉、引证错误、论证薄弱等方面，尚无法替代专业法官和考官。开发高水平法律AI需紧密合作技术与法律专家。

Abstract: This study provides an empirical assessment of whether current large language
models (LLMs) can pass the official qualifying examination for membership in
Poland's National Appeal Chamber (Krajowa Izba Odwo{\l}awcza). The authors
examine two related ideas: using LLM as actual exam candidates and applying the
'LLM-as-a-judge' approach, in which model-generated answers are automatically
evaluated by other models. The paper describes the structure of the exam, which
includes a multiple-choice knowledge test on public procurement law and a
written judgment, and presents the hybrid information recovery and extraction
pipeline built to support the models. Several LLMs (including GPT-4.1, Claude 4
Sonnet and Bielik-11B-v2.6) were tested in closed-book and various
Retrieval-Augmented Generation settings. The results show that although the
models achieved satisfactory scores in the knowledge test, none met the passing
threshold in the practical written part, and the evaluations of the
'LLM-as-a-judge' often diverged from the judgments of the official examining
committee. The authors highlight key limitations: susceptibility to
hallucinations, incorrect citation of legal provisions, weaknesses in logical
argumentation, and the need for close collaboration between legal experts and
technical teams. The findings indicate that, despite rapid technological
progress, current LLMs cannot yet replace human judges or independent examiners
in Polish public procurement adjudication.

</details>


### [89] [REMIND: Input Loss Landscapes Reveal Residual Memorization in Post-Unlearning LLMs](https://arxiv.org/abs/2511.04228)
*Liran Cohen,Yaniv Nemcovesky,Avi Mendelson*

Main category: cs.CL

TL;DR: 本文提出REMIND方法，通过分析机器学习模型对输入微小变化的损失动态，更敏锐地检测模型是否真正忘记了指定数据。


<details>
  <summary>Details</summary>
Motivation: 现有机器unlearning效果评估大多只关注单一输入，忽略了被删除数据在语义相似例子上的残余影响。这可能导致隐私泄露、无法真正满足法规要求，因此需要更敏感、全面的unlearning评估方法。

Method: 提出REMIND（Residual Memorization In Neighborhood Dynamics）方法，通过在数据邻域内（即对数据做微小扰动）的损失曲线变化来区分模型是否遗忘了目标数据。具体来看，被成功unlearn的数据的损失呈现更平缓走势，而未被遗忘的数据则波动较大。REMIND只需查询模型输出，无需模型内部访问，可适用于各种模型和数据。

Result: REMIND比现有只用单点评价的方法更能发现未完全遗忘的数据，表现更佳。方法已在多种模型、数据集及同义改写输入上验证了鲁棒性和泛化能力。

Conclusion: REMIND是一种敏感、解释性强的unlearning评估工具，为模型合规和隐私评估提供了更可靠的检测手段，推动了机器学习可控性和信任度的发展。

Abstract: Machine unlearning aims to remove the influence of specific training data
from a model without requiring full retraining. This capability is crucial for
ensuring privacy, safety, and regulatory compliance. Therefore, verifying
whether a model has truly forgotten target data is essential for maintaining
reliability and trustworthiness. However, existing evaluation methods often
assess forgetting at the level of individual inputs. This approach may overlook
residual influence present in semantically similar examples. Such influence can
compromise privacy and lead to indirect information leakage. We propose REMIND
(Residual Memorization In Neighborhood Dynamics), a novel evaluation method
aiming to detect the subtle remaining influence of unlearned data and classify
whether the data has been effectively forgotten. REMIND analyzes the model's
loss over small input variations and reveals patterns unnoticed by single-point
evaluations. We show that unlearned data yield flatter, less steep loss
landscapes, while retained or unrelated data exhibit sharper, more volatile
patterns. REMIND requires only query-based access, outperforms existing methods
under similar constraints, and demonstrates robustness across different models,
datasets, and paraphrased inputs, making it practical for real-world
deployment. By providing a more sensitive and interpretable measure of
unlearning effectiveness, REMIND provides a reliable framework to assess
unlearning in language models. As a result, REMIND offers a novel perspective
on memorization and unlearning.

</details>


### [90] [Reusing Pre-Training Data at Test Time is a Compute Multiplier](https://arxiv.org/abs/2511.04234)
*Alex Fang,Thomas Voice,Ruoming Pang,Ludwig Schmidt,Tom Gunter*

Main category: cs.CL

TL;DR: 本文分析了大模型在预训练过程中对数据集信息的利用率，发现通过检索增强生成（RAG）和增加测试时计算量，可以进一步提升模型在多个任务上的表现，说明当前预训练机制未能充分挖掘数据价值。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型依靠大规模预训练数据来学习并提升多种任务能力，但学界对于预训练到底能从数据“榨取”出多少知识与信息却研究较少。因此，作者希望量化现有预训练流程对数据有效信息的利用程度，并探索提升空间。

Method: 作者使用检索增强生成（RAG）技术，在测试时将开放数据集内容与模型输出结合。同时，通过增加测试时的计算资源，评估模型在多任务（如MMLU、Math-500和SimpleQA）上的准确率变化，对比预训练单独使用与结合RAG的效果。

Result: 实验显示，结合检索增强生成后，模型在上述任务中的准确率显著提升，并且即使经过去污染处理，这些提升依然存在。以MMLU为例，检索相当于让模型的计算力提升近5倍。此外，测试时加大计算也带来了更多提升，例如LLaMA 3.1 8B模型在MMLU上准确率提升了10个百分点。

Conclusion: 现有大模型预训练方式并未充分利用现有语料中的信息，通过检索技术及推理阶段增加计算量，可显著提升大模型性能，预示着数据利用效率仍有很大改进空间。

Abstract: Large language models learn from their vast pre-training corpora, gaining the
ability to solve an ever increasing variety of tasks; yet although researchers
work to improve these datasets, there is little effort to understand how
efficient the pre-training apparatus is at extracting ideas and knowledge from
the data. In this work, we use retrieval augmented generation along with
test-time compute as a way to quantify how much dataset value was left behind
by the process of pre-training, and how this changes across scale. We
demonstrate that pre-training then retrieving from standard and largely
open-sourced datasets results in significant accuracy gains in MMLU, Math-500,
and SimpleQA, which persist through decontamination. For MMLU we observe that
retrieval acts as a ~5x compute multiplier versus pre-training alone. We show
that these results can be further improved by leveraging additional compute at
test time to parse the retrieved context, demonstrating a 10 percentage point
improvement on MMLU for the public LLaMA 3.1 8B model. Overall, our results
suggest that today's pre-training methods do not make full use of the
information in existing pre-training datasets, leaving significant room for
progress.

</details>


### [91] [Efficient Topic Extraction via Graph-Based Labeling: A Lightweight Alternative to Deep Models](https://arxiv.org/abs/2511.04248)
*Salma Mekaooui,Hiba Sofyan,Imane Amaaz,Imane Benchrif,Arsalane Zarghili,Ilham Chaker,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: 本文提出了一种基于图的主题标注方法，旨在为主题建模生成的词集赋予更具意义的标签，并在保证计算效率的同时提升理解性。该方法与主流基线和ChatGPT-3.5在两个数据集上对比，取得了较优或相当的效果。


<details>
  <summary>Details</summary>
Motivation: 随着文本数据剧增，主题提取需求增加，但现有方法大多计算量大且主题通常仅为一组可代表词，不便于理解。本文希望开发一种低计算成本且可解释性强的主题标注方法。

Method: 提出一种基于图的主题标注方法：用图结构扩展并捕捉主题词间语义联系，通过分析这些关系自动生成主题标签，避免依赖高计算成本的深度模型。

Result: 在两类数据集上，本文方法在BERTScore和余弦相似度指标下均优于传统方法，对比ChatGPT-3.5则在保持良好计算效率下取得了相当效果。

Conclusion: 基于图的主题标注方法在标签准确性和计算效率上表现优异，具备实际应用价值。未来可进一步提升解释性与标注过程自动化。

Abstract: Extracting topics from text has become an essential task, especially with the
rapid growth of unstructured textual data. Most existing works rely on highly
computational methods to address this challenge. In this paper, we argue that
probabilistic and statistical approaches, such as topic modeling (TM), can
offer effective alternatives that require fewer computational resources. TM is
a statistical method that automatically discovers topics in large collections
of unlabeled text; however, it produces topics as distributions of
representative words, which often lack clear interpretability. Our objective is
to perform topic labeling by assigning meaningful labels to these sets of
words. To achieve this without relying on computationally expensive models, we
propose a graph-based approach that not only enriches topic words with
semantically related terms but also explores the relationships among them. By
analyzing these connections within the graph, we derive suitable labels that
accurately capture each topic's meaning. We present a comparative study between
our proposed method and several benchmarks, including ChatGPT-3.5, across two
different datasets. Our method achieved consistently better results than
traditional benchmarks in terms of BERTScore and cosine similarity and produced
results comparable to ChatGPT-3.5, while remaining computationally efficient.
Finally, we discuss future directions for topic labeling and highlight
potential research avenues for enhancing interpretability and automation.

</details>


### [92] [SSPO: Subsentence-level Policy Optimization](https://arxiv.org/abs/2511.04256)
*Kun Yang,Zikang chen,Yanmeng Wang,Zhigen Li*

Main category: cs.CL

TL;DR: 本文提出了一种新的RLVR算法SSPO，利用句子级重要性比率，兼顾了现有方法的优缺点，实现了更稳定的训练和更高效的数据利用。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR算法（如GRPO和GSPO）分别存在训练不稳定和数据利用率低等问题，亟需一种方法权衡两者，提升Large Language Models推理能力。

Method: 作者提出SSPO，采用句子级别重要性比率介于GRPO（token级）和GSPO（响应级）之间；同时引入句子熵，动态调整PPO-CLIP裁剪界限，提升高熵token探索能力，防止低熵token裁剪范围过大。

Result: SSPO在五个数据集上取得了46.57的平均分，优于GRPO的43.01和GSPO的44.42，并在其中三个数据集上达到最优结果。

Conclusion: SSPO有效结合了GSPO的优势，克服了其响应级重要性比率带来的弊端，实现了训练的稳定以及采样数据的高效利用，对LLMs推理型任务有显著提升效果。

Abstract: As a significant part of post-training of the Large Language Models (LLMs),
Reinforcement Learning from Verifiable Reward (RLVR) has greatly improved LLMs'
reasoning skills. However, some RLVR algorithms, such as GRPO (Group Relative
Policy Optimization) and GSPO (Group Sequence Policy Optimization), are
observed to suffer from unstable policy updates and low usage of sampling data,
respectively. The importance ratio of GRPO is calculated at the token level,
which focuses more on optimizing a single token. This will be easily affected
by outliers, leading to model training collapse. GSPO proposed the calculation
of the response level importance ratio, which solves the problem of high
variance and training noise accumulation in the calculation of the GRPO
importance ratio. However, since all the response tokens share a common
importance ratio, extreme values can easily raise or lower the overall mean,
leading to the entire response being mistakenly discarded, resulting in a
decrease in the utilization of sampled data. This paper introduces SSPO, which
applies sentence-level importance ratio, taking the balance between GRPO and
GSPO. SSPO not only avoids training collapse and high variance, but also
prevents the whole response tokens from being abandoned by the clipping
mechanism. Furthermore, we apply sentence entropy to PPO-CLIP to steadily
adjust the clipping bounds, encouraging high-entropy tokens to explore and
narrow the clipping range of low-entropy tokens. In particular, SSPO achieves
an average score of 46.57 across five datasets, surpassing GRPO (43.01) and
GSPO (44.42), and wins state-of-the-art performance on three datasets. These
results highlight SSPO's effectiveness in leveraging generated data by taking
the essence of GSPO but rejecting its shortcomings.

</details>


### [93] [Dynamic Jointly Batch Selection for Data Efficient Machine Translation Fine-Tuning](https://arxiv.org/abs/2511.04406)
*Mohammad Amin Ghanizadeh,Mohammad Javad Dousti*

Main category: cs.CL

TL;DR: 本文提出了一种针对机器翻译模型微调的数据选择方法，通过学习模型与预训练参考模型的协作，提升了数据筛选效率和模型性能，实现了更高效的训练过程。


<details>
  <summary>Details</summary>
Motivation: 高质量数据对于提升机器翻译模型的性能至关重要，如何有效选取训练数据成为获得强健、可靠翻译系统的核心问题。

Method: 提出了基于'可学习性分数'的数据筛选方案，利用学习者模型与预训练参考模型的交互评估每个训练样本的价值。方法还采用了批次选择策略，综合考虑数据间的相关性以优化训练效率和数据相关性。

Result: 在mBART模型和CCMatrix数据集上的英-波斯语及其他语言实验中，相较iid基线，数据利用率提升了5倍，使用缓存嵌入加速后，计算效率提升24倍，且泛化能力也优于随机选择。

Conclusion: 提出的方法能更高效、有效地选择用于微调的数据，提升翻译模型性能，并显著减少所需训练数据量和算力成本。

Abstract: Data quality and its effective selection are fundamental to improving the
performance of machine translation models, serving as cornerstones for
achieving robust and reliable translation systems. This paper presents a data
selection methodology specifically designed for fine-tuning machine translation
systems, which leverages the synergy between a learner model and a pre-trained
reference model to enhance overall training effectiveness. By defining a
learnability score, our approach systematically evaluates the utility of data
points for training, ensuring that only the most relevant and impactful
examples contribute to the fine-tuning process. Furthermore, our method employs
a batch selection strategy which considers interdependencies among data points,
optimizing the efficiency of the training process while maintaining a focus on
data relevance. Experiments on English to Persian and several other language
pairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate that
our method can achieve up to a fivefold improvement in data efficiency compared
to an iid baseline. Experimental results indicate that our approach improves
computational efficiency by 24 when utilizing cached embeddings, as it requires
fewer training data points. Additionally, it enhances generalization, resulting
in superior translation performance compared to random selection method.

</details>


### [94] [If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning Task for LLMs](https://arxiv.org/abs/2511.04432)
*Lars Bungum,Charles Yijia Huang,Abeer Kashar*

Main category: cs.CL

TL;DR: 本文测试了大语言模型（LLM）的时间推理能力，使用1940年挪威题库，分别用挪威语和英语提问并评估不同模型的表现。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的兴起，理解其在历史和特定时期知识推理方面的局限性和能力非常重要，特别是在小语种（如挪威语）以及多语言环境下。

Method: 作者利用一本1940年出版的挪威语问答书，要求模型模拟1940年时点来回答问题，分别用挪威语和英文提问，并用LLM及母语者抽查方式评分。同时对比了不同LLM家族与专为挪威语设计的大模型。

Result: 英文提问得到的答案质量显著优于挪威语，有悖作者预期；模型规模越大，效果越好。

Conclusion: 当前LLM即使面向小语种优化，也会受语言主流性影响，英文表现依然优越，大模型在时间推理和多语言环境下有更好潜力。

Abstract: In this study, we experiment with the ability of LLMs to do temporal
reasoning. Using a Norwegian book from 1940 containing trivia questions, we
prompt the LLMs to answer the questions as if it were 1940. We also pose the
questions in both English and Norwegian. Correct answers are often presented as
sentences, and grading is done by means of LLM-as-judge, with sampled checks by
a native speaker. Prompting in English consistently gave better results than in
Norwegian, an unexpected result. In contrast, using larger LLMs improved
results. We tested the DeepSeek-R1, Gemma3, Qwen3, and Llama3.1 model families,
and also the largest available LLM especially crafted for Norwegian.

</details>


### [95] [Probabilistic Textual Time Series Depression Detection](https://arxiv.org/abs/2511.04476)
*Fabian Schmidt,Seyedehmoniba Ravan,Vladimir Vlassov*

Main category: cs.CL

TL;DR: 本论文提出PTTSD模型，用于对抑郁症严重程度进行不确定性建模和时序预测，突破了现有模型在临床可解释性和时序建模上的不足，并在多个数据集上达成了领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的抑郁症预测模型往往缺乏不确定性估计和针对时间序列的建模能力，这降低了其在临床辅助决策中的实际应用和可解释性。

Method: 提出了PTTSD框架，包括seq2seq和seq2one两种结构，利用双向LSTM、自注意力机制和残差连接，输出采用高斯分布或学生t分布头，并通过负对数似然损失进行训练，实现了对PHQ-8分数的不确定性预测和时间序列建模。

Result: 在E-DAIC和DAIC-WOZ两个数据集上，PTTSD在文本模型中表现出最优性能（如MAE分别为3.85和3.55），同时生成了良好的预测区间。消融实验表明注意力和概率建模设计具备重要作用，与MentalBERT的对比显示方法具备较强通用性。

Conclusion: PTTSD不仅提升了预测准确性，还增强了预测区间的可解释性和临床相关性，对抑郁症严重程度的不确定性感知预测具有实际价值和科学意义。

Abstract: Accurate and interpretable predictions of depression severity are essential
for clinical decision support, yet existing models often lack uncertainty
estimates and temporal modeling. We propose PTTSD, a Probabilistic Textual Time
Series Depression Detection framework that predicts PHQ-8 scores from
utterance-level clinical interviews while modeling uncertainty over time. PTTSD
includes sequence-to-sequence and sequence-to-one variants, both combining
bidirectional LSTMs, self-attention, and residual connections with Gaussian or
Student-t output heads trained via negative log-likelihood. Evaluated on E-DAIC
and DAIC-WOZ, PTTSD achieves state-of-the-art performance among text-only
systems (e.g., MAE = 3.85 on E-DAIC, 3.55 on DAIC) and produces well-calibrated
prediction intervals. Ablations confirm the value of attention and
probabilistic modeling, while comparisons with MentalBERT establish generality.
A three-part calibration analysis and qualitative case studies further
highlight the interpretability and clinical relevance of uncertainty-aware
forecasting.

</details>


### [96] [ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding in Thai](https://arxiv.org/abs/2511.04479)
*Surapon Nonesung,Teetouch Jaknamon,Sirinya Chaiophat,Natapong Nitarach,Chanakan Wittayasakpan,Warit Sirichotedumrong,Adisai Na-Thalang,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: 本文提出了泰语OCR评测基准ThaiOCRBench，用于系统性评测视觉-语言模型在泰语文本视觉理解任务上的能力，揭示了专有模型与开源模型之间的显著性能差距，为低资源语言的文档理解评测提供了标准。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型评测基准多集中于高资源语言，对泰语等复杂脚本的低资源语言支持严重不足，尤其在文档结构理解任务上缺乏评测资源，因此亟需针对泰语的系统化评测框架。

Method: 本文构建了ThaiOCRBench数据集，包含13类任务、共2,808个人工标注样本，对当前主流专有和开源视觉-语言模型进行了零样本评测，并引入细致的误差分析探究模型在泰语视觉理解中的表现及失败原因。

Result: 实验发现：专有模型（如Gemini 2.5 Pro）在泰语文档视觉理解任务上明显优于开源模型。开源模型在细粒度文本识别和手写内容抽取方面性能下降最严重。

Conclusion: ThaiOCRBench为低资源、复杂脚本文档的多模态模型评测提供了统一标准，通过揭示现有模型差距及挑战，为未来泰语文档理解模型的进一步改进提供了参考方向和实证依据。

Abstract: We present ThaiOCRBench, the first comprehensive benchmark for evaluating
vision-language models (VLMs) on Thai text-rich visual understanding tasks.
Despite recent progress in multimodal modeling, existing benchmarks
predominantly focus on high-resource languages, leaving Thai underrepresented,
especially in tasks requiring document structure understanding. ThaiOCRBench
addresses this gap by offering a diverse, human-annotated dataset comprising
2,808 samples across 13 task categories. We evaluate a wide range of
state-of-the-art VLMs in a zero-shot setting, spanning both proprietary and
open-source systems. Results show a significant performance gap, with
proprietary models (e.g., Gemini 2.5 Pro) outperforming open-source
counterparts. Notably, fine-grained text recognition and handwritten content
extraction exhibit the steepest performance drops among open-source models.
Through detailed error analysis, we identify key challenges such as language
bias, structural mismatch, and hallucinated content. ThaiOCRBench provides a
standardized framework for assessing VLMs in low-resource, script-complex
settings, and provides actionable insights for improving Thai-language document
understanding.

</details>


### [97] [RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables](https://arxiv.org/abs/2511.04491)
*Nikhil Abhyankar,Purvi Chaurasia,Sanchit Kabra,Ananya Srivastava,Vivek Gupta,Chandan K. Reddy*

Main category: cs.CL

TL;DR: 本文提出了新的表格推理基准RUST-BENCH，以更真实复杂的数据验证大语言模型在表格推理上的能力，发现现有模型在异构结构和复杂推理上表现有限。


<details>
  <summary>Details</summary>
Motivation: 目前常用的表格推理基准多基于小型且结构单一的表格，无法反映实际应用场景中真实表格的多样性和复杂性，这导致我们对大语言模型推理能力的认知不完整。

Method: 作者构建了RUST-BENCH基准，涵盖来自两个领域（科学NSF基金记录和体育NBA统计）的2031个真实世界表格，提出7966个问题，重点考察模型在大规模、异构、特定领域、多跳推理等方面的能力。对开源和商业大语言模型进行了系统测试。

Result: 实验结果显示，无论是开源还是专有的大语言模型，在处理异构结构和需要多跳推理的问题时表现不佳，暴露出当前模型架构与推理策略的持续性弱点。

Conclusion: RUST-BENCH为表格推理研究提供了更具挑战性的新标准和测试平台，有助于推动该领域向处理更复杂真实数据的能力提升。

Abstract: Existing tabular reasoning benchmarks mostly test models on small, uniform
tables, underrepresenting the complexity of real-world data and giving an
incomplete view of Large Language Models' (LLMs) reasoning abilities. Real
tables are long, heterogeneous, and domain-specific, mixing structured fields
with free text and requiring multi-hop reasoning across thousands of tokens. To
address this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from
2031 real-world tables spanning two domains: i) RB-Science (NSF grant records)
and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates
LLMs jointly across scale, heterogeneity, domain specificity, and reasoning
complexity. Experiments with open-source and proprietary models show that LLMs
struggle with heterogeneous schemas and complex multi-hop inference, revealing
persistent weaknesses in current architectures and prompting strategies.
RUST-BENCH establishes a challenging new testbed for advancing tabular
reasoning research.

</details>


### [98] [OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code Generation](https://arxiv.org/abs/2511.04495)
*Cuong Huynh,Jie Cao*

Main category: cs.CL

TL;DR: 本文介绍了OUNLP系统，该系统通过基于LLM提示的生成进行可控易读性文本简化，并提出了多轮简化方法以提升简化效果。


<details>
  <summary>Details</summary>
Motivation: 研究发现，文本简化效果与源CEFR等级和目标CEFR等级之间的差距高度相关。为了提升文本简化系统表现，作者探索了该差距对简化成效的影响。

Method: 作者提出了两种多轮简化方法：基于规则的简化（MRS-Rule）和规则与LLM结合的简化方法（MRS-Joint），并均通过GPT-4o实现。同时分析了不同CEFR等级间切换对简化效果的影响。

Result: 提交的系统在20个参赛队伍中排名第7。后续实验发现以LLM简化候选作为多轮简化起点，能进一步提升简化性能。

Conclusion: 分阶段、多轮、结合规则与LLM的简化策略可有效提升文本简化效果，尤其在控制目标易读性等级上表现突出。

Abstract: This paper describes the OUNLP system submitted to the TSAR-2025 Shared Task
(Alva-Manchego et al., 2025), designed for readability-controlled text
simplification using LLM-prompting-based generation. Based on the analysis of
prompt-based text simplification methods, we discovered an interesting finding
that text simplification performance is highly related to the gap between the
source CEFR (Arase et al., 2022) level and the target CEFR level. Inspired by
this finding, we propose two multi-round simplification methods and generate
them via GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-based
LLM simplification (MRS-Joint). Our submitted systems ranked 7 out of 20 teams.
Later improvements with MRS-Joint show that taking the LLM simplified
candidates as the starting point could further boost the multi-round
simplification performance.

</details>


### [99] [Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering](https://arxiv.org/abs/2511.04499)
*Christos-Nikolaos Zacharopoulos,Revekka Kyriakoglou*

Main category: cs.CL

TL;DR: 本研究系统性评估了六种大型语言模型（LLM）的“人格”表现，发现不同模型在大五人格特质上存在明显差异，且神经质和外向性对采样温度尤为敏感。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在以人为中心的应用中越来越重要，了解其类人格行为对于模型的负责任开发和部署非常关键。研究试图揭示模型的人格特质如何随参数变化，以及其背后的机制及影响。

Method: 作者利用Big Five Inventory-2（BFI-2）人格评估框架，系统地测试了六个主流LLM，并分析它们在不同采样温度下表现出的五大人格特质分布。同时，通过层次聚类分析模型之间的人格特征相似性。

Result: 结果表明，不同LLM在人格五维中有显著分布差异。特别是“神经质”和“外向性”两个维度对采样温度变化较为敏感。此外，模型间形成了不同的聚类，暗示模型架构可能影响其稳定的人格特质倾向。

Conclusion: LLM展现出可测量的人格特质模式，采样参数和模型架构共同作用于这些表现。本研究为理解和管理AI人格带来了新视角，有助于模型调优和AI伦理治理。

Abstract: As Large Language Models (LLMs) become integral to human-centered
applications, understanding their personality-like behaviors is increasingly
important for responsible development and deployment. This paper systematically
evaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to
assess trait expressions under varying sampling temperatures. We find
significant differences across four of the five personality dimensions, with
Neuroticism and Extraversion susceptible to temperature adjustments. Further,
hierarchical clustering reveals distinct model clusters, suggesting that
architectural features may predispose certain models toward stable trait
profiles. Taken together, these results offer new insights into the emergence
of personality-like patterns in LLMs and provide a new perspective on model
tuning, selection, and the ethical governance of AI systems. We share the data
and code for this analysis here:
https://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1

</details>


### [100] [RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific RAG](https://arxiv.org/abs/2511.04502)
*Joshua Gao,Quoc Huy Pham,Subin Varghese,Silwal Saurav,Vedhus Hoskere*

Main category: cs.CL

TL;DR: 本文提出RAGalyst框架，实现对专用领域RAG系统的严谨自动评估，并使其结果与人类判断高度一致。框架可生成高质量合成QA数据集，并优化评估指标，在多个领域验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统在特定、高风险领域评估困难，现有方法无法有效反映领域细节或缺乏与人类判断的一致性。因此需要一个自动化且与人类高度一致的评估框架。

Method: 提出RAGalyst自动化评估框架：通过agentic流程自动从源文档生成高质量的合成问答数据集，包括数据过滤环节确保数据质量；并优化了LLM-as-a-Judge的答案正确性和可答性指标，使其与人工标注高度相关。

Result: 在军事、网络安全和桥梁工程三个不同领域内评测多个RAG组件，发现RAG性能高度上下文相关，无单一模型或参数配置适用于所有领域。同时分析了导致低答案正确性的主要原因。

Conclusion: 系统的RAGalyst评估框架对于发现领域专属权衡、优化系统设计至关重要，使RAG应用于高风险专业领域更加可靠有效。

Abstract: Retrieval-Augmented Generation (RAG) is a critical technique for grounding
Large Language Models (LLMs) in factual evidence, yet evaluating RAG systems in
specialized, safety-critical domains remains a significant challenge. Existing
evaluation frameworks often rely on heuristic-based metrics that fail to
capture domain-specific nuances and other works utilize LLM-as-a-Judge
approaches that lack validated alignment with human judgment. This paper
introduces RAGalyst, an automated, human-aligned agentic framework designed for
the rigorous evaluation of domain-specific RAG systems. RAGalyst features an
agentic pipeline that generates high-quality, synthetic question-answering (QA)
datasets from source documents, incorporating an agentic filtering step to
ensure data fidelity. The framework refines two key LLM-as-a-Judge
metrics-Answer Correctness and Answerability-using prompt optimization to
achieve a strong correlation with human annotations. Applying this framework to
evaluate various RAG components across three distinct domains (military
operations, cybersecurity, and bridge engineering), we find that performance is
highly context-dependent. No single embedding model, LLM, or hyperparameter
configuration proves universally optimal. Additionally, we provide an analysis
on the most common low Answer Correctness reasons in RAG. These findings
highlight the necessity of a systematic evaluation framework like RAGalyst,
which empowers practitioners to uncover domain-specific trade-offs and make
informed design choices for building reliable and effective RAG systems.
RAGalyst is available on our Github.

</details>


### [101] [Modeling Clinical Uncertainty in Radiology Reports: from Explicit Uncertainty Markers to Implicit Reasoning Pathways](https://arxiv.org/abs/2511.04506)
*Paloma Rabaey,Jong Hak Moon,Jung-Oh Lee,Min Gwan Kim,Hangyul Yoon,Thomas Demeester,Edward Choi*

Main category: cs.CL

TL;DR: 本文提出了一套对放射学报告中显式和隐式不确定性进行量化与建模的方法，并发布了Lunguage++数据集以促进不确定性感知的自动分析。


<details>
  <summary>Details</summary>
Motivation: 放射学报告在临床决策中十分重要，但其自动化分析长期面临报告中不确定性表达（显性与隐性两类）的巨大挑战。现有基于规则的方法难以准确量化和解释这些不确定性。

Method: （1）针对显式不确定性，作者采用LLM生成常见模糊表述的概率排名，并专家验证，进而将报告中具体医学发现与概率值对应起来。（2）针对隐式不确定性，基于专家知识路径，系统化地对常见的14类诊断添加典型子发现，实现报告内容的扩展。

Result: 通过上述方法，作者发布了Lunguage++，即对原有Lunguage基准的扩展版，能够表达和量化临床诊断报告中的不确定性信息。

Conclusion: Lunguage++成为一个丰富的不确定性感知的放射学报告资源，为图像分类、推理与诊断不确定性的临床影响等研究提供了新工具。

Abstract: Radiology reports are invaluable for clinical decision-making and hold great
potential for automated analysis when structured into machine-readable formats.
These reports often contain uncertainty, which we categorize into two distinct
types: (i) Explicit uncertainty reflects doubt about the presence or absence of
findings, conveyed through hedging phrases. These vary in meaning depending on
the context, making rule-based systems insufficient to quantify the level of
uncertainty for specific findings; (ii) Implicit uncertainty arises when
radiologists omit parts of their reasoning, recording only key findings or
diagnoses. Here, it is often unclear whether omitted findings are truly absent
or simply unmentioned for brevity. We address these challenges with a two-part
framework. We quantify explicit uncertainty by creating an expert-validated,
LLM-based reference ranking of common hedging phrases, and mapping each finding
to a probability value based on this reference. In addition, we model implicit
uncertainty through an expansion framework that systematically adds
characteristic sub-findings derived from expert-defined diagnostic pathways for
14 common diagnoses. Using these methods, we release Lunguage++, an expanded,
uncertainty-aware version of the Lunguage benchmark of fine-grained structured
radiology reports. This enriched resource enables uncertainty-aware image
classification, faithful diagnostic reasoning, and new investigations into the
clinical impact of diagnostic uncertainty.

</details>


### [102] [Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics](https://arxiv.org/abs/2511.04527)
*Amir Zur,Atticus Geiger,Ekdeep Singh Lubana,Eric Bigelow*

Main category: cs.CL

TL;DR: 本论文研究了语言模型在产生文本时，对不确定性和推理路径的内部表征和干预方法。作者发现，通过调整隐层激活，可以预测和影响模型在链式思维推理过程中的不确定性。


<details>
  <summary>Details</summary>
Motivation: 语言模型在生成文本过程中，由于每一步的选择都可能导致不同的推理路径，因此量化不确定性成为一个难题。本文作者希望理解：在链式思维推理中，模型是否、以及如何在内部表征这些可能的推理分支。

Method: 作者通过操控模型的隐层激活（hidden activations），在生成过程中人为地引导模型，并观察模型对激活干预的敏感性。此外，作者还分析隐层激活对未来输出分布的预测能力。整个实验围绕链式思维推理（chain-of-thought reasoning）任务展开。

Result: 实验表明，模型对不同token的不确定性与其激活受控程度有明显相关性。当模型存在多个可选路径、尚未确定最终答案时，隐层激活干预最为有效。同时，隐层激活还能预测模型后续输出的可能分布。

Conclusion: 语言模型在内部隐含地表征了多种可能的推理路径，通过对隐层激活的分析和干预不仅能够揭示模型的不确定性，还可以影响其推理过程和最终选择。该发现为理解和提升模型可控性提供了新的思路。

Abstract: When a language model generates text, the selection of individual tokens
might lead it down very different reasoning paths, making uncertainty difficult
to quantify. In this work, we consider whether reasoning language models
represent the alternate paths that they could take during generation. To test
this hypothesis, we use hidden activations to control and predict a language
model's uncertainty during chain-of-thought reasoning. In our experiments, we
find a clear correlation between how uncertain a model is at different tokens,
and how easily the model can be steered by controlling its activations. This
suggests that activation interventions are most effective when there are
alternate paths available to the model -- in other words, when it has not yet
committed to a particular final answer. We also find that hidden activations
can predict a model's future outcome distribution, demonstrating that models
implicitly represent the space of possible paths.

</details>


### [103] [IntelliProof: An Argumentation Network-based Conversational Helper for Organized Reflection](https://arxiv.org/abs/2511.04528)
*Kaveh Eskandari Miandoab,Katharine Kowalyshyn,Kabir Pamnani,Anesu Gavhera,Vasanth Sarathy,Matthias Scheutz*

Main category: cs.CL

TL;DR: IntelliProof 是一个基于大语言模型（LLM）的互动式系统，通过结构化和可视化分析，有效提升了议论文写作分析的用户体验和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的自动评分系统通常只关注分数，缺乏对论证结构和用户理解的关注。为了让用户更好地理解议论文的论证过程及其结构，需要开发一种能直观展示和分析文章结构的工具。

Method: IntelliProof 将议论文转化为论证图，将主张作为节点、证据作为节点属性、支持或反对关系作为边，使用LLM对各关系进行分类和评分，并可视化展示。系统还为每一分类关系提供理由，并给出文章连贯性的量化分析指标，允许用户快速探查论证质量。

Result: 系统实现了互动式论证分析，不仅能以图形方式呈现论证结构，还可以通过自然语言工具进一步理解文本和论证图之间的联系。用户能够方便地探索和评估议论文的论证质量。

Conclusion: IntelliProof 不仅提高了议论文分析的透明度和用户体验，还促进了用户对论证结构语义的深刻理解，为自动化与人工协作提供了有力支持。

Abstract: We present IntelliProof, an interactive system for analyzing argumentative
essays through LLMs. IntelliProof structures an essay as an argumentation
graph, where claims are represented as nodes, supporting evidence is attached
as node properties, and edges encode supporting or attacking relations. Unlike
existing automated essay scoring systems, IntelliProof emphasizes the user
experience: each relation is initially classified and scored by an LLM, then
visualized for enhanced understanding. The system provides justifications for
classifications and produces quantitative measures for essay coherence. It
enables rapid exploration of argumentative quality while retaining human
oversight. In addition, IntelliProof provides a set of tools for a better
understanding of an argumentative essay and its corresponding graph in natural
language, bridging the gap between the structural semantics of argumentative
essays and the user's understanding of a given text. A live demo and the system
are available here to try: \textbf{https://intelliproof.vercel.app}

</details>


### [104] [From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities Reporting](https://arxiv.org/abs/2511.04538)
*Cyril Vallez,Alexander Sternfeld,Andrei Kucharavy,Ljiljana Dolamic*

Main category: cs.CL

TL;DR: 本文发现目前主流的开源大型语言模型（LLM）代码辅助系统在已知的早期漏洞场景下依然容易生成存在安全漏洞的代码，并提出了一种新的衡量漏洞严重性的指标和模型暴露分数。


<details>
  <summary>Details</summary>
Motivation: 尽管有多种针对LLM生成代码安全性的评测标准和加固方法，但尚不清楚这些努力在实际广泛使用的代码LLM中产生了多大影响，仍有安全风险。

Method: 作者测试了最新的开源LLM在现实代码生成场景下的表现，判断其是否会出现早期已知的安全漏洞。同时，提出Prompt Exposure（PE）指标，综合漏洞严重程度、生成概率及诱发漏洞的提示词因素。以此为基础，引入Model Exposure（ME）分数，用于衡量整个模型易生成严重及高发漏洞的水平。

Result: 实验显示，即便是最新的开源大语言模型，在真实使用情境下仍容易犯下以往已报告过的安全漏洞。现有的安全加固方法因功能与安全的权衡，未能有效消除这些漏洞。

Conclusion: 现有主流LLM代码助手在实际开发中面临安全挑战，新引入的PE和ME指标可帮助更有效地评估和缓解严重且常见的生成型安全漏洞。

Abstract: As the role of Large Language Models (LLM)-based coding assistants in
software development becomes more critical, so does the role of the bugs they
generate in the overall cybersecurity landscape. While a number of LLM code
security benchmarks have been proposed alongside approaches to improve the
security of generated code, it remains unclear to what extent they have
impacted widely used coding LLMs. Here, we show that even the latest
open-weight models are vulnerable in the earliest reported vulnerability
scenarios in a realistic use setting, suggesting that the safety-functionality
trade-off has until now prevented effective patching of vulnerabilities. To
help address this issue, we introduce a new severity metric that reflects the
risk posed by an LLM-generated vulnerability, accounting for vulnerability
severity, generation chance, and the formulation of the prompt that induces
vulnerable code generation - Prompt Exposure (PE). To encourage the mitigation
of the most serious and prevalent vulnerabilities, we use PE to define the
Model Exposure (ME) score, which indicates the severity and prevalence of
vulnerabilities a model generates.

</details>


### [105] [BanglaMedQA and BanglaMMedBench: Evaluating Retrieval-Augmented Generation Strategies for Bangla Biomedical Question Answering](https://arxiv.org/abs/2511.04560)
*Sadia Sultana,Saiyma Sittul Muna,Mosammat Zannatul Samarukh,Ajwad Abrar,Tareque Mohmud Chowdhury*

Main category: cs.CL

TL;DR: 论文提出了首个大规模孟加拉语医学多项选择题数据集，并利用多种增强检索-生成（RAG）方法显著提升了孟加拉语医学问答系统的准确性。


<details>
  <summary>Details</summary>
Motivation: 在低资源语言（如孟加拉语）上缺乏准确的医学问答系统，限制了公平获取医学知识，因此亟需开发高质量数据集和有效方法提升问答性能。

Method: 构建了两个大规模孟加拉语医学MCQ数据集（BanglaMedQA和BanglaMMedBench），利用OCR技术集成孟加拉医学教材，设计并比较了多种RAG策略，如传统、零样本回退、智能代理、迭代反馈和聚合RAG，通过结合教材和网络检索结果提升AI的推理与检索能力。

Result: 实验表明，Agentic RAG策略结合openai/gpt-oss-120b模型可达最高89.54%的准确率，优于其他方法，并且推理质量最优。

Conclusion: RAG方法能有效提升孟加拉语医学问答系统的可靠性和信息获取能力，推动多语种医学AI研究与应用发展。

Abstract: Developing accurate biomedical Question Answering (QA) systems in
low-resource languages remains a major challenge, limiting equitable access to
reliable medical knowledge. This paper introduces BanglaMedQA and
BanglaMMedBench, the first large-scale Bangla biomedical Multiple Choice
Question (MCQ) datasets designed to evaluate reasoning and retrieval in medical
artificial intelligence (AI). The study applies and benchmarks several
Retrieval-Augmented Generation (RAG) strategies, including Traditional,
Zero-Shot Fallback, Agentic, Iterative Feedback, and Aggregate RAG, combining
textbook-based and web retrieval with generative reasoning to improve factual
accuracy. A key novelty lies in integrating a Bangla medical textbook corpus
through Optical Character Recognition (OCR) and implementing an Agentic RAG
pipeline that dynamically selects between retrieval and reasoning strategies.
Experimental results show that the Agentic RAG achieved the highest accuracy
89.54% with openai/gpt-oss-120b, outperforming other configurations and
demonstrating superior rationale quality. These findings highlight the
potential of RAG-based methods to enhance the reliability and accessibility of
Bangla medical QA, establishing a foundation for future research in
multilingual medical artificial intelligence.

</details>


### [106] [When retrieval outperforms generation: Dense evidence retrieval for scalable fake news detection](https://arxiv.org/abs/2511.04643)
*Alamgir Munir Qazi,John P. McCrae,Jamal Abdul Nasir*

Main category: cs.CL

TL;DR: 本文提出了一种新的事实核查方法DeReC，通过密集检索结合分类模块，用通用文本嵌入替代大规模语言模型(LLM)生成解释性理由的方法，显著提升了效率且保持甚至超过了准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于大模型生成解释的事实核查方法在实际应用中面临高计算复杂度和幻觉问题，需要更高效且可靠的解决方案。

Method: 设计了DeReC框架：先用密集检索获取与待核查文本最相关的信息，再采用专门的分类器进行事实判别，整体流程避免了自回归LLM推理环节，仅用通用文本嵌入实现高效处理。

Result: 在RAWFC和LIAR-RAW两个数据集上，DeReC比LLM-based方法加速了92%-95%，在RAWFC数据集F1提升至65.58%，超过了SOTA方法L-Defense的61.20%。

Conclusion: 以检索结合分类为核心的高效系统，在事实核查任务中可达甚至超越大模型表现，并大幅削减运行时间，适用于实际部署。

Abstract: The proliferation of misinformation necessitates robust yet computationally
efficient fact verification systems. While current state-of-the-art approaches
leverage Large Language Models (LLMs) for generating explanatory rationales,
these methods face significant computational barriers and hallucination risks
in real-world deployments. We present DeReC (Dense Retrieval Classification), a
lightweight framework that demonstrates how general-purpose text embeddings can
effectively replace autoregressive LLM-based approaches in fact verification
tasks. By combining dense retrieval with specialized classification, our system
achieves better accuracy while being significantly more efficient. DeReC
outperforms explanation-generating LLMs in efficiency, reducing runtime by 95%
on RAWFC (23 minutes 36 seconds compared to 454 minutes 12 seconds) and by 92%
on LIAR-RAW (134 minutes 14 seconds compared to 1692 minutes 23 seconds),
showcasing its effectiveness across varying dataset sizes. On the RAWFC
dataset, DeReC achieves an F1 score of 65.58%, surpassing the state-of-the-art
method L-Defense (61.20%). Our results demonstrate that carefully engineered
retrieval-based systems can match or exceed LLM performance in specialized
tasks while being significantly more practical for real-world deployment.

</details>


### [107] [Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.04654)
*Mohammad Atif Quamar,Mohammad Areeb*

Main category: cs.CL

TL;DR: LEASH是一种无须训练的新型自适应推理生成截断算法，可大幅减少推理字数和延迟，仅带来较小准确率损失。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型中的链式思维（CoT）方法需要生成大量推理文本，导致计算资源浪费和响应延迟增加，因此需要更高效的推理截断机制。

Method: 提出LEASH（Logit-Entropy Adaptive Stopping Heuristic），通过监测生成过程中Token级熵的斜率和最高logit间隙的改善情况，当这两个信号均稳定时自动终止推理生成，无需额外训练和监督，且适用于各种模型。

Result: 在GSM8K和AQuA-RAT等数据集上，LEASH可使平均生成Token数减少约30-35%，延迟降低27%，准确率损失仅为10百分点。

Conclusion: LEASH方法无需重新训练且简单高效，为复杂推理生成提供了一种可行的节省计算和提高响应速度的新途径。

Abstract: Chain-of-Thought (CoT) prompting is a key technique for enabling complex
reasoning in large language models. However, generating full, fixed-length
rationales is computationally wasteful, inflating both token usage and latency.
We introduce LEASH: Logit-Entropy Adaptive Stopping Heuristic, a training-free
decoding algorithm that adaptively halts rationale generation. LEASH monitors
two intrinsic signals: the slope of token-level entropy and the improvement in
the top-logit margin. It terminates the generation once both signals plateau,
indicating the model has reached a stable reasoning state. Across four
instruction-tuned models on the GSM8K and AQuA-RAT benchmarks, LEASH reduces
average token generation by 30--35% and latency by 27%, while incurring a 10
p.p. accuracy drop relative to CoT. LEASH is model-agnostic and requires no
additional training or supervision, offering a simple and efficient alternative
to CoT decoding.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [108] [Dynamic Shape Control of Soft Robots Enabled by Data-Driven Model Reduction](https://arxiv.org/abs/2511.03931)
*Iman Adibnazari,Harsh Sharma,Myungsun Park,Jacobo Cervera-Torralba,Boris Kramer,Michael T. Tolley*

Main category: cs.RO

TL;DR: 该论文对数据驱动的模型降维技术进行比较，用于软体机器人的动态形状控制。结果表明，Lagrangian算子推断（LOpInf）方法在所有实验中都优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 软体机器人具有强大的动态形状控制能力，但受限于目前缺乏有效的高维动力学建模工具。这限制了软体机器人在实际中的高效控制，因此急需比较各种模型降维方法，以生成可用于控制的线性模型。

Method: 作者比较了三种数据驱动的模型降维方法：本征系统实现算法（ERA）、带控制的动态模态分解（DMDc）以及Lagrangian算子推断（LOpInf）。并利用这三类模型分别用于预测控制，实现对仿真鳗鱼形软体机器人的动态形状控制。

Result: 在三项实验任务中（跟踪可行的参考轨迹、生物学鳗鱼运动轨迹、缩尺物理模型生成的轨迹），LOpInf方法的模型控制策略均显示出较低的跟踪误差，效果优于ERA和DMDc模型。

Conclusion: Lagrangian算子推断方法在为软体机器人动态形状控制生成线性模型方面效果最佳，为今后软体机器人高效控制方法的研发提供了有力技术路径。

Abstract: Soft robots have shown immense promise in settings where they can leverage
dynamic control of their entire bodies. However, effective dynamic shape
control requires a controller that accounts for the robot's high-dimensional
dynamics--a challenge exacerbated by a lack of general-purpose tools for
modeling soft robots amenably for control. In this work, we conduct a
comparative study of data-driven model reduction techniques for generating
linear models amendable to dynamic shape control. We focus on three
methods--the eigensystem realization algorithm, dynamic mode decomposition with
control, and the Lagrangian operator inference (LOpInf) method. Using each
class of model, we explored their efficacy in model predictive control policies
for the dynamic shape control of a simulated eel-inspired soft robot in three
experiments: 1) tracking simulated reference trajectories guaranteed to be
feasible, 2) tracking reference trajectories generated from a biological model
of eel kinematics, and 3) tracking reference trajectories generated by a
reduced-scale physical analog. In all experiments, the LOpInf-based policies
generated lower tracking errors than policies based on other models.

</details>


### [109] [Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots](https://arxiv.org/abs/2511.03996)
*Yushi Wang,Changsheng Luo,Penghui Chen,Jianran Liu,Weijian Sun,Tong Guo,Kechang Yang,Biao Hu,Yangang Zhang,Mingguo Zhao*

Main category: cs.RO

TL;DR: 本论文提出一种基于强化学习的统一控制器，使类人机器人能够直接整合视觉感知和运动控制，从而高效应对动态的足球比赛环境。


<details>
  <summary>Details</summary>
Motivation: 现有的类人机器人足球系统通常采用分离的感知和动作模块，导致响应延迟和行为不连贯，且实际环境下的感知局限性加剧了这些问题。需要一种能高效整合感知与动作，提升机器人反应能力的控制方法。

Method: 作者提出了一种扩展了对抗性动作先验(Adversarial Motion Priors)的方法到真实视觉环境，设计了一种编码器-解码器架构及虚拟感知系统，能够从不完美的视觉观测中恢复机器人内部状态，实现感知与动作的主动协调。

Result: 所提出的控制器展示了极强的反应能力，能够持续稳定地执行连贯且鲁棒的足球动作，且在包括RoboCup真实比赛等多种场景下表现优异。

Conclusion: 通过感知与控制的深度整合，论文方案显著提升了类人机器人在动态实际环境中的足球表现，为机器人体智能提供了有力的新路径。

Abstract: Humanoid soccer poses a representative challenge for embodied intelligence,
requiring robots to operate within a tightly coupled perception-action loop.
However, existing systems typically rely on decoupled modules, resulting in
delayed responses and incoherent behaviors in dynamic environments, while
real-world perceptual limitations further exacerbate these issues. In this
work, we present a unified reinforcement learning-based controller that enables
humanoid robots to acquire reactive soccer skills through the direct
integration of visual perception and motion control. Our approach extends
Adversarial Motion Priors to perceptual settings in real-world dynamic
environments, bridging motion imitation and visually grounded dynamic control.
We introduce an encoder-decoder architecture combined with a virtual perception
system that models real-world visual characteristics, allowing the policy to
recover privileged states from imperfect observations and establish active
coordination between perception and action. The resulting controller
demonstrates strong reactivity, consistently executing coherent and robust
soccer behaviors across various scenarios, including real RoboCup matches.

</details>


### [110] [Integrating Ergonomics and Manipulability for Upper Limb Postural Optimization in Bimanual Human-Robot Collaboration](https://arxiv.org/abs/2511.04009)
*Chenzui Li,Yiming Chen,Xi Wu,Giacinto Barresi,Fei Chen*

Main category: cs.RO

TL;DR: 本文提出了一种用于双人协作搬运任务的上肢姿态优化方法，兼顾人体力学安全性和操作能力，并通过实验验证了优化效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常只关注人体安全或操控效率，缺乏将二者结合的方法。该论文希望提升人-机器人协作时的安全性和效率，适应不同把持姿势与物体形状。

Method: 1. 建立简化人体骨骼模型，设置关节角度优化问题。2. 以安全与可操作性为优化目标，设计损失函数进行求解。3. 通过转换模块生成机器人末端位姿指导人类向优化姿态靠拢。4. 针对人形机器人CURI提出双手模型预测阻抗控制器，对轨迹进行实时调整。5. 通过多被试、不同物体，分别在双人协作和人-机协作场景下实验验证。

Result: 实验对比优化前后目标肌肉的激活度，结果显示采用该方法显著改善了肌肉负荷条件。

Conclusion: 该方法能在不同协作和物体场景下有效提升身体的人体工学状况与操作性，有利于推广到更广泛的人-机协作应用。

Abstract: This paper introduces an upper limb postural optimization method for
enhancing physical ergonomics and force manipulability during bimanual
human-robot co-carrying tasks. Existing research typically emphasizes human
safety or manipulative efficiency, whereas our proposed method uniquely
integrates both aspects to strengthen collaboration across diverse conditions
(e.g., different grasping postures of humans, and different shapes of objects).
Specifically, the joint angles of a simplified human skeleton model are
optimized by minimizing the cost function to prioritize safety and manipulative
capability. To guide humans towards the optimized posture, the reference
end-effector poses of the robot are generated through a transformation module.
A bimanual model predictive impedance controller (MPIC) is proposed for our
human-like robot, CURI, to recalibrate the end effector poses through planned
trajectories. The proposed method has been validated through various subjects
and objects during human-human collaboration (HHC) and human-robot
collaboration (HRC). The experimental results demonstrate significant
improvement in muscle conditions by comparing the activation of target muscles
before and after optimization.

</details>


### [111] [An LLM-based Framework for Human-Swarm Teaming Cognition in Disaster Search and Rescue](https://arxiv.org/abs/2511.04042)
*Kailun Ji,Xiaoyu Hu,Xinyu Zhang,Jun Chen*

Main category: cs.RO

TL;DR: 本文提出了一种结合大语言模型（LLM）驱动的UAV群体搜索救援系统，通过更自然的交互方式，显著提升了救援效率并降低了操作人员的认知负担。


<details>
  <summary>Details</summary>
Motivation: 在大规模灾害救援行动中，复杂环境和通信中断问题常常使搜索和救援任务极具挑战。现有无人机群体协作虽然有潜力，但其指挥与协同对人类操作员的认知负担十分沉重，主要瓶颈在于如何高效准确地将高层级意图转化为具体的无人机指令。

Method: 本文提出了一种结合大语言模型（LLM）和认知推理框架（CRF）的系统（LLM-CRF），通过语音或图形标注等多模态交互捕捉人类操作员的意图，利用LLM进行意图理解、任务分解与无人机群任务规划，实现了闭环推理和主动反馈。

Result: 在SAR模拟场景下，LLM-CRF系统相比传统基于命令和操作的界面，任务完成时间缩短约64.2%，任务成功率提高7%，主观认知负荷（NASA-TLX）降低42.9%。

Conclusion: 该方法验证了大语言模型在提升高风险场景下人-群体机器协作的直观性和有效性上的巨大潜力。

Abstract: Large-scale disaster Search And Rescue (SAR) operations are persistently
challenged by complex terrain and disrupted communications. While Unmanned
Aerial Vehicle (UAV) swarms offer a promising solution for tasks like wide-area
search and supply delivery, yet their effective coordination places a
significant cognitive burden on human operators. The core human-machine
collaboration bottleneck lies in the ``intention-to-action gap'', which is an
error-prone process of translating a high-level rescue objective into a
low-level swarm command under high intensity and pressure. To bridge this gap,
this study proposes a novel LLM-CRF system that leverages Large Language Models
(LLMs) to model and augment human-swarm teaming cognition. The proposed
framework initially captures the operator's intention through natural and
multi-modal interactions with the device via voice or graphical annotations. It
then employs the LLM as a cognitive engine to perform intention comprehension,
hierarchical task decomposition, and mission planning for the UAV swarm. This
closed-loop framework enables the swarm to act as a proactive partner,
providing active feedback in real-time while reducing the need for manual
monitoring and control, which considerably advances the efficacy of the SAR
task. We evaluate the proposed framework in a simulated SAR scenario.
Experimental results demonstrate that, compared to traditional order and
command-based interfaces, the proposed LLM-driven approach reduced task
completion time by approximately $64.2\%$ and improved task success rate by
$7\%$. It also leads to a considerable reduction in subjective cognitive
workload, with NASA-TLX scores dropping by $42.9\%$. This work establishes the
potential of LLMs to create more intuitive and effective human-swarm
collaborations in high-stakes scenarios.

</details>


### [112] [Enhancing Fault-Tolerant Space Computing: Guidance Navigation and Control (GNC) and Landing Vision System (LVS) Implementations on Next-Gen Multi-Core Processors](https://arxiv.org/abs/2511.04052)
*Kyongsik Yun,David Bayard,Gerik Kubiak,Austin Owens,Andrew Johnson,Ryan Johnson,Dan Scharf,Thomas Lu*

Main category: cs.RO

TL;DR: 本文评估了高性能多核处理器在未来行星探测任务中的关键角色，突出其在着陆导航和视觉系统上的显著加速，同时提出了一种新型多核冗余容错机制以增强任务可靠性。


<details>
  <summary>Details</summary>
Motivation: 未来的行星探测任务需要更高性能且具备容错能力的计算平台，用于支持自主的引导、导航与控制（GNC）以及着陆视觉系统（LVS）。传统航天硬件在计算能力和效率上难以满足这些高要求。

Method: 作者评估了多核处理器（HPSC、Snapdragon VOXL2、AMD Xilinx Versal）在GNC和LVS算法中的部署效果，并提出了ARBITER（异步冗余行为检查）多核投票机制，用于实时故障检测和修正。通过故障注入研究识别了GFOLD轨迹优化过程中最敏感的阶段，并采用选择性防护和向量输出仲裁策略。

Result: 与传统硬件相比，新一代多核平台在LVS图像处理上实现了高达15倍加速，在GFOLD轨迹优化上获得了250倍以上加速。ARBITER容错机制在静态优化任务和动态闭环控制中均得到了验证。故障注入实验指出梯度计算阶段对比特错误最为敏感。

Conclusion: 该工作为未来自主、低时延和具备高容错能力的行星探测任务提供了一种可扩展且能效高的计算架构，支持如火星样品返回等高自主性任务的需求。

Abstract: Future planetary exploration missions demand high-performance, fault-tolerant
computing to enable autonomous Guidance, Navigation, and Control (GNC) and
Lander Vision System (LVS) operations during Entry, Descent, and Landing (EDL).
This paper evaluates the deployment of GNC and LVS algorithms on
next-generation multi-core processors--HPSC, Snapdragon VOXL2, and AMD Xilinx
Versal--demonstrating up to 15x speedup for LVS image processing and over 250x
speedup for Guidance for Fuel-Optimal Large Divert (GFOLD) trajectory
optimization compared to legacy spaceflight hardware. To ensure computational
reliability, we present ARBITER (Asynchronous Redundant Behavior Inspection for
Trusted Execution and Recovery), a Multi-Core Voting (MV) mechanism that
performs real-time fault detection and correction across redundant cores.
ARBITER is validated in both static optimization tasks (GFOLD) and dynamic
closed-loop control (Attitude Control System). A fault injection study further
identifies the gradient computation stage in GFOLD as the most sensitive to
bit-level errors, motivating selective protection strategies and vector-based
output arbitration. This work establishes a scalable and energy-efficient
architecture for future missions, including Mars Sample Return, Enceladus
Orbilander, and Ceres Sample Return, where onboard autonomy, low latency, and
fault resilience are critical.

</details>


### [113] [CBMC-V3: A CNS-inspired Control Framework Towards Manipulation Agility with SNN](https://arxiv.org/abs/2511.04109)
*Yanbo Pang,Qingkai Li,Mingguo Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种仿生脉冲神经网络（SNN）控制框架，模仿人类中枢神经系统，实现了机械臂在复杂环境下的灵活控制，经仿真和真实平台验证优于工业级位置控制。


<details>
  <summary>Details</summary>
Motivation: 随着机械臂应用拓展到医疗、服务和生活等更复杂且动态的场景，现有控制算法在轨迹多变、交互不可预测和物体多样性上表现不足，需要新的高敏捷性控制方法。

Method: 受人类中枢神经系统启发，提出了基于SNN的分层控制框架，包括五个模块（大脑皮层、小脑、丘脑、脑干、脊髓）、三级控制（高阶、中阶、低阶）以及上下行两种信息通路。各模块充分利用SNN特点，如尖峰编码、LIF神经元、循环SNN及强化学习等，分别实现反馈和前馈控制。

Result: 该系统在仿真和真实机械臂平台上，在不同负载和运动轨迹下测试，整体在操作敏捷度上超越了常规工业级位置控制方法。

Conclusion: 该仿生SNN控制框架显著提升了机械臂在动态复杂环境中的敏捷性，展示了神经科学机制在机器人控制中的潜力。

Abstract: As robotic arm applications extend beyond industrial settings into
healthcare, service, and daily life, existing control algorithms struggle to
achieve the agile manipulation required for complex environments with dynamic
trajectories, unpredictable interactions, and diverse objects. This paper
presents a biomimetic control framework based on Spiking Neural Networks (SNN),
inspired by the human Central Nervous System (CNS), to achieve agile control in
such environments. The proposed framework features five control modules
(cerebral cortex, cerebellum, thalamus, brainstem, spinal cord), three
hierarchical control levels (first-order, second-order, third-order), and two
information pathways (ascending, descending). Each module is fully implemented
using SNN. The spinal cord module uses spike encoding and Leaky
Integrate-and-Fire (LIF) neurons for feedback control. The brainstem module
employs a network of LIF and non-spiking LIF neurons to dynamically adjust
spinal cord parameters via reinforcement learning. The thalamus module
similarly adjusts the cerebellum's torque outputs. The cerebellum module uses a
recurrent SNN to learn the robotic arm's dynamics through regression, providing
feedforward gravity compensation torques. The framework is validated both in
simulation and on real-world robotic arm platform under various loads and
trajectories. Results demonstrate that our method outperforms the
industrial-grade position control in manipulation agility.

</details>


### [114] [BFM-Zero: A Promptable Behavioral Foundation Model for Humanoid Control Using Unsupervised Reinforcement Learning](https://arxiv.org/abs/2511.04131)
*Yitang Li,Zhengyi Luo,Tonghe Zhang,Cunxi Dai,Anssi Kanervisto,Andrea Tirinzoni,Haoyang Weng,Kris Kitani,Mateusz Guzek,Ahmed Touati,Alessandro Lazaric,Matteo Pirotta,Guanya Shi*

Main category: cs.RO

TL;DR: 论文提出了BFM-Zero框架，实现了可扩展、可提示的人形机器人行为基础模型，支持多种控制任务并在实际硬件上验证。


<details>
  <summary>Details</summary>
Motivation: 现有人形机器人通用控制模型要么仅在仿真中有效，要么局限于单一任务，缺乏统一支持多任务、可迁移的通用策略。

Method: BFM-Zero通过学习包含动作、目标与奖励的共享潜在空间，使一个策略模型可实现多下游任务（如零样本运动追踪、目标到达、奖励优化等），无需每次都重新训练。核心创新包括结合无监督强化学习及Forward-Backward模型， reward shaping、域随机化与历史相关的非对称学习技术，以弥合仿真与现实的差距并提升泛化能力。

Result: BFM-Zero在Unitree G1实体人形机器人上实现了稳定多样的全身技能，支持不同推理方式，相关设计细节在仿真中做了消融分析验证其有效性。

Conclusion: 首次提出可扩展、可提示的人形机器人行为基础模型，为实现统一、多任务、可迁移的人形机器人控制迈出重要一步。

Abstract: Building Behavioral Foundation Models (BFMs) for humanoid robots has the
potential to unify diverse control tasks under a single, promptable generalist
policy. However, existing approaches are either exclusively deployed on
simulated humanoid characters, or specialized to specific tasks such as
tracking. We propose BFM-Zero, a framework that learns an effective shared
latent representation that embeds motions, goals, and rewards into a common
space, enabling a single policy to be prompted for multiple downstream tasks
without retraining. This well-structured latent space in BFM-Zero enables
versatile and robust whole-body skills on a Unitree G1 humanoid in the real
world, via diverse inference methods, including zero-shot motion tracking, goal
reaching, and reward optimization, and few-shot optimization-based adaptation.
Unlike prior on-policy reinforcement learning (RL) frameworks, BFM-Zero builds
upon recent advancements in unsupervised RL and Forward-Backward (FB) models,
which offer an objective-centric, explainable, and smooth latent representation
of whole-body motions. We further extend BFM-Zero with critical reward shaping,
domain randomization, and history-dependent asymmetric learning to bridge the
sim-to-real gap. Those key design choices are quantitatively ablated in
simulation. A first-of-its-kind model, BFM-Zero establishes a step toward
scalable, promptable behavioral foundation models for whole-body humanoid
control.

</details>


### [115] [PUL-SLAM: Path-Uncertainty Co-Optimization with Lightweight Stagnation Detection for Efficient Robotic Exploration](https://arxiv.org/abs/2511.04180)
*Yizhen Yin,Dapeng Feng,Hongbo Chen,Yuhua Qi*

Main category: cs.RO

TL;DR: 提出了一种结合路径-不确定性协同优化深度强化学习和轻量级停滞检测的新型主动SLAM框架，实现更快、更高效的探索。


<details>
  <summary>Details</summary>
Motivation: 现有主动SLAM方法存在探索速度慢、路径非最优等问题，影响机器人的环境自主感知与地图构建效率。

Method: 方法采用双目标奖励函数联合优化移动路径和地图不确定性（路径-不确定性协同优化深度强化学习），同时通过激光雷达静态异常检测和地图更新停滞检测实现轻量级探索停滞检测，低拓展率时提前结束探索。

Result: 相比frontier-based和RRT方法，新方法探索时间缩短多达65%，路径距离减少多达42%，提升了效率和地图完整性。在实体机器人实验中，实现了从仿真到现实的可迁移性。消融实验显示协同机制显著加速训练收敛速度。

Conclusion: 该方法在复杂环境下可显著提升主动SLAM的探索效率和地图质量，并能够适配真实机器人系统，具备良好的实用性和推广价值。

Abstract: Existing Active SLAM methodologies face issues such as slow exploration speed
and suboptimal paths. To address these limitations, we propose a hybrid
framework combining a Path-Uncertainty Co-Optimization Deep Reinforcement
Learning framework and a Lightweight Stagnation Detection mechanism. The
Path-Uncertainty Co-Optimization framework jointly optimizes travel distance
and map uncertainty through a dual-objective reward function, balancing
exploration and exploitation. The Lightweight Stagnation Detection reduces
redundant exploration through Lidar Static Anomaly Detection and Map Update
Stagnation Detection, terminating episodes on low expansion rates. Experimental
results show that compared with the frontier-based method and RRT method, our
approach shortens exploration time by up to 65% and reduces path distance by up
to 42%, significantly improving exploration efficiency in complex environments
while maintaining reliable map completeness. Ablation studies confirm that the
collaborative mechanism accelerates training convergence. Empirical validation
on a physical robotic platform demonstrates the algorithm's practical
applicability and its successful transferability from simulation to real-world
environments.

</details>


### [116] [GraspView: Active Perception Scoring and Best-View Optimization for Robotic Grasping in Cluttered Environments](https://arxiv.org/abs/2511.04199)
*Shenglin Wang,Mingtong Dai,Jingxuan Su,Lingbo Liu,Chunjie Chen,Xinyu Wu,Liang Lin*

Main category: cs.RO

TL;DR: 本文提出了一种仅基于RGB图像的机器人抓取系统GraspView，不依赖深度相机，能在复杂遮挡、透明物体等困难环境下实现高效抓取。


<details>
  <summary>Details</summary>
Motivation: 深度相机通常对透明或高反光物体无效，且在近距离时感知效果变差，导致机器人难以在混乱环境下稳定抓取目标物体。因此需探索无需深度信息、对环境鲁棒的抓取方法。

Method: GraspView包括三大模块：1）全局感知场景重建，从单张图片估算局部一致且有尺度的几何信息，并融合多视角生成全局3D场景；2）基于渲染-评分的新视角主动视觉策略，动态选择展示被遮挡区域的最佳视角；3）在线度量对齐模块，保证估计结果与机械臂运动一致，实现物理尺度的可靠抓取。最终融合重建信息和GraspNet实现稳健抓取。

Result: 在多种桌面目标实验证明，GraspView在遮挡严重、近距离感知以及抓取透明物体等情形下均显著优于基于RGB-D和单视图RGB的基线方法。

Conclusion: GraspView无需深度相机即可在现实复杂环境下实现高精度抓取，是现有RGB-D抓取管线的实用且通用的替代方案。

Abstract: Robotic grasping is a fundamental capability for autonomous manipulation, yet
remains highly challenging in cluttered environments where occlusion, poor
perception quality, and inconsistent 3D reconstructions often lead to unstable
or failed grasps. Conventional pipelines have widely relied on RGB-D cameras to
provide geometric information, which fail on transparent or glossy objects and
degrade at close range. We present GraspView, an RGB-only robotic grasping
pipeline that achieves accurate manipulation in cluttered environments without
depth sensors. Our framework integrates three key components: (i) global
perception scene reconstruction, which provides locally consistent, up-to-scale
geometry from a single RGB view and fuses multi-view projections into a
coherent global 3D scene; (ii) a render-and-score active perception strategy,
which dynamically selects next-best-views to reveal occluded regions; and (iii)
an online metric alignment module that calibrates VGGT predictions against
robot kinematics to ensure physical scale consistency. Building on these
tailor-designed modules, GraspView performs best-view global grasping, fusing
multi-view reconstructions and leveraging GraspNet for robust execution.
Experiments on diverse tabletop objects demonstrate that GraspView
significantly outperforms both RGB-D and single-view RGB baselines, especially
under heavy occlusion, near-field sensing, and with transparent objects. These
results highlight GraspView as a practical and versatile alternative to RGB-D
pipelines, enabling reliable grasping in unstructured real-world environments.

</details>


### [117] [Can Context Bridge the Reality Gap? Sim-to-Real Transfer of Context-Aware Policies](https://arxiv.org/abs/2511.04249)
*Marco Iannotta,Yuxuan Yang,Johannes A. Stork,Erik Schaffernicht,Todor Stoyanov*

Main category: cs.RO

TL;DR: 本文针对强化学习中仿真到现实的迁移问题，提出引入环境动态参数（context）估计模块，使学习的策略具备动态适应性，从而提升迁移表现。实验显示，基于context的策略优于传统基线。


<details>
  <summary>Details</summary>
Motivation: 受限于仿真环境与现实环境之间的差异，强化学习中训练得到的策略往往无法直接应用于真实机器人。现有的Domain Randomization方法通过增加训练时的环境多样性部分缓解此问题，但会导致性能下降。因此，探索更精细、更适应性的策略成为必要。

Method: 在基于Domain Randomization的强化学习框架中，作者引入了一个context（动态参数）估计模块，并对当前主流的监督策略进行了系统对比。通过让策略在决策时可感知环境动态参数，提高其泛化和适应未知环境的能力。

Result: 在标准控制基准环境以及真实的机器人推物任务中，基于context的策略在所有场景下均显著优于不考虑context的基线策略，不同的监督方式在不同任务下效果有差异。

Conclusion: 带有环境动态参数感知能力的策略可以有效提升从仿真到现实的迁移效果，是Domain Randomization方法的重要增强方向。

Abstract: Sim-to-real transfer remains a major challenge in reinforcement learning (RL)
for robotics, as policies trained in simulation often fail to generalize to the
real world due to discrepancies in environment dynamics. Domain Randomization
(DR) mitigates this issue by exposing the policy to a wide range of randomized
dynamics during training, yet leading to a reduction in performance. While
standard approaches typically train policies agnostic to these variations, we
investigate whether sim-to-real transfer can be improved by conditioning the
policy on an estimate of the dynamics parameters -- referred to as context. To
this end, we integrate a context estimation module into a DR-based RL framework
and systematically compare SOTA supervision strategies. We evaluate the
resulting context-aware policies in both a canonical control benchmark and a
real-world pushing task using a Franka Emika Panda robot. Results show that
context-aware policies outperform the context-agnostic baseline across all
settings, although the best supervision strategy depends on the task.

</details>


### [118] [Design and Control of a Coaxial Dual-rotor Reconfigurable Tailsitter UAV Based on Swashplateless Mechanism](https://arxiv.org/abs/2511.04251)
*Jinfeng Liang,Haocheng Guo,Ximin Lyu*

Main category: cs.RO

TL;DR: 本论文提出并验证了一种采用可重构机翼和创新控制机构的新型垂直起降（VTOL）尾座无人机（UAV）设计，实现了低重量、高抗风、低能耗和优良飞行性能。


<details>
  <summary>Details</summary>
Motivation: 尾座VTOL UAV 虽然重量轻、结构简单，但在多旋翼模式下易受风干扰，影响飞行稳定性，亟需提升抗风与能效能力。

Method: 设计了可重构机翼，能在多旋翼模式下收起、固定翼模式伸展，并采用同轴异构双旋翼以降低能耗；引入免球型盘控制机构并改进结构，通过添加拍击铰链降低振动。此外，进行了全飞行包线的过渡飞行实验验证。

Result: 新型无人机在各模式下表现出良好的力效，结构复杂度和重量有所减少，动力消耗显著降低，并通过试验实现了从垂直起降到水平巡航的稳定转换和抗风测试。

Conclusion: 创新的可重构机翼与简化控制机构显著提升了尾座VTOL UAV的能效、抗风与飞行性能，为该类型无人机的实用化提供了有效工程方案。

Abstract: The tailsitter vertical takeoff and landing (VTOL) UAV is widely used due to
its lower dead weight, which eliminates the actuators and mechanisms for
tilting. However, the tailsitter UAV is susceptible to wind disturbances in
multi-rotor mode, as it exposes a large frontal fuselage area. To address this
issue, our tailsitter UAV features a reconfigurable wing design, allowing wings
to retract in multi-rotor mode and extend in fixed- wing mode. Considering
power efficiency, we design a coaxial heterogeneous dual-rotor configuration,
which significantly re- duces the total power consumption. To reduce structural
weight and simplify structural complexity, we employ a swashplateless mechanism
with an improved design to control pitch and roll in multi-rotor mode. We
optimize the structure of the swashplateless mechanism by adding flapping
hinges, which reduces vibration during cyclic acceleration and deceleration.
Finally, we perform comprehensive transition flight tests to validate stable
flight performance across the entire flight envelope of the tailsitter UAV.

</details>


### [119] [MacroNav: Multi-Task Context Representation Learning Enables Efficient Navigation in Unknown Environments](https://arxiv.org/abs/2511.04320)
*Kuankuan Sima,Longbin Tang,Haozhe Ma,Lin Zhao*

Main category: cs.RO

TL;DR: 该论文提出了一种新型自主导航框架MacroNav，有效提升了在未知环境下的导航成功率与效率。


<details>
  <summary>Details</summary>
Motivation: 在未知环境中自主导航需要对部分可见空间有高效且信息丰富的表达，但现有方法难以在表达能力与导航效率之间取得平衡。

Method: MacroNav框架包含两个关键模块：(1) 通过多任务自监督学习训练的轻量级上下文编码器，用于提取多尺度、导航为中心的空间表达；(2) 基于强化学习的策略模块，将上述空间表达与图神经网络推理结合，实现高效的动作选择。

Result: 实验显示，提出的上下文编码器对环境有高效且鲁棒的表征能力，在现实环境测试中，MacroNav在SR和SPL等指标上显著优于主流导航方法，且保持低计算量。

Conclusion: MacroNav为未知环境自主导航提供了高效的空间理解与决策方案，兼具精度与效率，有望在实际机器人应用中推广。

Abstract: Autonomous navigation in unknown environments requires compact yet expressive
spatial understanding under partial observability to support high-level
decision making. Existing approaches struggle to balance rich contextual
representation with navigation efficiency. We present MacroNav, a
learning-based navigation framework featuring two key components: (1) a
lightweight context encoder trained via multi-task self-supervised learning to
capture multi-scale, navigation-centric spatial representations; and (2) a
reinforcement learning policy that seamlessly integrates these representations
with graph-based reasoning for efficient action selection. Extensive
experiments demonstrate the context encoder's efficient and robust
environmental understanding. Real-world deployments further validate MacroNav's
effectiveness, yielding significant gains over state-of-the-art navigation
methods in both Success Rate (SR) and Success weighted by Path Length (SPL),
while maintaining low computational cost. Code will be released upon
acceptance.

</details>


### [120] [GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon Planning with VLA Policies](https://arxiv.org/abs/2511.04357)
*Maëlic Neau,Zoe Falomir,Paulo E. Santos,Anne-Gwenn Bosser,Cédric Buche*

Main category: cs.RO

TL;DR: 本文提出了一种结合神经网络与符号方法的新型机器人技能学习框架GraSP-VLA，能够在保持高层规划能力的同时，实现长时任务中的动作泛化与序列执行。


<details>
  <summary>Details</summary>
Motivation: 现有机器人技能学习方案存在两大局限：VLA端到端方法缺乏符号高层规划能力，难以完成长时任务；而AML符号方法泛化与扩展性不足。作者为解决上述二者的不足，提出一种神经-符号结合的新方法。

Method: GraSP-VLA框架借助连续场景图（Continuous Scene Graph）从人类演示中自动生成场景的符号表示，并在推理阶段由该表示自动生成规划域，引导并协调低层VLA行为策略，从而扩展能够连续学习和执行的动作数目。

Result: 实验显示，GraSP-VLA能够有效从观测中自动建构符号表示并生成规划域，并能在真实世界实验中有效编排和控制更多低层动作，实现长时任务。

Conclusion: GraSP-VLA框架在自动化符号表示构建和长时动作学习执行方面均具备优势，为机器人自适应与泛化性技能学习提供了有效的新方向。

Abstract: Deploying autonomous robots that can learn new skills from demonstrations is
an important challenge of modern robotics. Existing solutions often apply
end-to-end imitation learning with Vision-Language Action (VLA) models or
symbolic approaches with Action Model Learning (AML). On the one hand, current
VLA models are limited by the lack of high-level symbolic planning, which
hinders their abilities in long-horizon tasks. On the other hand, symbolic
approaches in AML lack generalization and scalability perspectives. In this
paper we present a new neuro-symbolic approach, GraSP-VLA, a framework that
uses a Continuous Scene Graph representation to generate a symbolic
representation of human demonstrations. This representation is used to generate
new planning domains during inference and serves as an orchestrator for
low-level VLA policies, scaling up the number of actions that can be reproduced
in a row. Our results show that GraSP-VLA is effective for modeling symbolic
representations on the task of automatic planning domain generation from
observations. In addition, results on real-world experiments show the potential
of our Continuous Scene Graph representation to orchestrate low-level VLA
policies in long-horizon tasks.

</details>


### [121] [Studying the Effect of Explicit Interaction Representations on Learning Scene-level Distributions of Human Trajectories](https://arxiv.org/abs/2511.04375)
*Anna Mészáros,Javier Alonso-Mora,Jens Kober*

Main category: cs.RO

TL;DR: 本文探讨了在多智能体场景中如何表示和建模交互关系，以提升联合分布预测的准确性。结果显示，明确建模交互关系优于仅依赖神经网络隐式学习。


<details>
  <summary>Details</summary>
Motivation: 多智能体场景下，准确捕捉所有智能体的联合分布对于自动驾驶等决策过程至关重要，但目前尚无一致方法来最佳描述智能体间交互，尤其在人为定义和神经网络隐式学习之间存在争议。

Method: 作者在相同的网络结构下，比较了不同的交互表示方式，包括允许网络从数据中自动学习交互和基于空间、时间等先验明确建模交互，并分析它们对最后学习到的联合分布的影响。

Result: 实验证明，仅依赖网络自动学习交互关系通常会降低预测性能；而手工明确建模如先后通行等交互方式，则能显著提升性能。

Conclusion: 明确定义和建模智能体交互（而非完全依赖数据驱动的隐式学习）对于提升多智能体联合分布建模的准确性具有积极作用。

Abstract: Effectively capturing the joint distribution of all agents in a scene is
relevant for predicting the true evolution of the scene and in turn providing
more accurate information to the decision processes of autonomous vehicles.
While new models have been developed for this purpose in recent years, it
remains unclear how to best represent the joint distributions particularly from
the perspective of the interactions between agents. Thus far there is no clear
consensus on how best to represent interactions between agents; whether they
should be learned implicitly from data by neural networks, or explicitly
modeled using the spatial and temporal relations that are more grounded in
human decision-making. This paper aims to study various means of describing
interactions within the same network structure and their effect on the final
learned joint distributions. Our findings show that more often than not, simply
allowing a network to establish interactive connections between agents based on
data has a detrimental effect on performance. Instead, having well defined
interactions (such as which agent of an agent pair passes first at an
intersection) can often bring about a clear boost in performance.

</details>


### [122] [ForeRobo: Unlocking Infinite Simulation Data for 3D Goal-driven Robotic Manipulation](https://arxiv.org/abs/2511.04381)
*Dexin wang,Faliang Chang,Chunsheng Liu*

Main category: cs.RO

TL;DR: 该论文提出了ForeRobo，一种结合生成模拟和经典控制的机器人自主学习操作技能的框架，并在多任务上取得了优异的迁移和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在机器人操作技能学习中，如何高效利用仿真系统和提升技能泛化能力一直是难题。传统端到端策略学习存在可解释性低、效率不高的问题，因此需要探索解释性更强、执行效率更高的方法。

Method: ForeRobo采用了生成范式与经典控制结合的新策略。流程包括：1）机器人提出所需技能并构建模拟环境；2）通过ForeGen生成与技能一致的目标状态（对象排列）；3）使用生成的数据训练ForeFormer模型，基于场景状态和任务指令，预测每个点的3D目标位置，从而建立点对点对应关系；4）将目标状态输入经典控制算法，驱动机器人在真实环境中执行。

Result: ForeFormer在各类刚体和关节物体操作任务中，相较于最新的状态生成模型平均提升56.32%。实物评测中，涉及20余种机器人任务，ForeRobo实现零样本仿真到现实迁移，平均成功率达79.28%。

Conclusion: 基于生成模拟与经典控制的组合，ForeRobo能高效自学多种操作技能，并在泛化和仿真到现实迁移上都表现出色，具备极高的实际应用潜力。

Abstract: Efficiently leveraging simulation to acquire advanced manipulation skills is
both challenging and highly significant. We introduce \textit{ForeRobo}, a
generative robotic agent that utilizes generative simulations to autonomously
acquire manipulation skills driven by envisioned goal states. Instead of
directly learning low-level policies, we advocate integrating generative
paradigms with classical control. Our approach equips a robotic agent with a
self-guided \textit{propose-generate-learn-actuate} cycle. The agent first
proposes the skills to be acquired and constructs the corresponding simulation
environments; it then configures objects into appropriate arrangements to
generate skill-consistent goal states (\textit{ForeGen}). Subsequently, the
virtually infinite data produced by ForeGen are used to train the proposed
state generation model (\textit{ForeFormer}), which establishes point-wise
correspondences by predicting the 3D goal position of every point in the
current state, based on the scene state and task instructions. Finally,
classical control algorithms are employed to drive the robot in real-world
environments to execute actions based on the envisioned goal states. Compared
with end-to-end policy learning methods, ForeFormer offers superior
interpretability and execution efficiency. We train and benchmark ForeFormer
across a variety of rigid-body and articulated-object manipulation tasks, and
observe an average improvement of 56.32\% over the state-of-the-art state
generation models, demonstrating strong generality across different
manipulation patterns. Moreover, in real-world evaluations involving more than
20 robotic tasks, ForeRobo achieves zero-shot sim-to-real transfer and exhibits
remarkable generalization capabilities, attaining an average success rate of
79.28\%.

</details>


### [123] [Temporal Action Selection for Action Chunking](https://arxiv.org/abs/2511.04421)
*Yueyang Weng,Xiaopeng Zhang,Yongjin Mu,Yingcong Zhu,Yanjie Li,Qi Liu*

Main category: cs.RO

TL;DR: 该论文提出了一种新的动作选择算法（TAS），在提升复杂任务成功率的同时，实现了反应性、决策一致性和动作连贯性的平衡。


<details>
  <summary>Details</summary>
Motivation: 目前通过动作分块提升LfD建模能力，但牺牲了对新观测的快速反应性，导致系统难以适应动态环境和传感噪声。现有方法无法同时兼顾反应性和决策一致性，因此亟需新方法突破限制。

Method: 提出了Temporal Action Selector（TAS）算法。TAS会缓存多个时间步的预测动作分块，并利用一个轻量的选择网络，动态从中选取最优动作。该方法同时优化反应性、决策一致性和动作连贯性。

Result: 在多种任务和不同基线策略下，TAS显著提升了任务成功率，最高绝对提升达73.3%。将TAS作为基本策略再结合残差强化学习，还能进一步提升训练效率和最终性能。实验覆盖仿真和真实机器人，充分验证了算法有效性。

Conclusion: TAS方法有效突破了传统动作分块方案中反应性和一致性无法兼顾的瓶颈，实现了性能与效率双提升，对复杂实时机器人控制具有实际应用价值。

Abstract: Action chunking is a widely adopted approach in Learning from Demonstration
(LfD). By modeling multi-step action chunks rather than single-step actions,
action chunking significantly enhances modeling capabilities for human expert
policies. However, the reduced decision frequency restricts the utilization of
recent observations, degrading reactivity - particularly evident in the
inadequate adaptation to sensor noise and dynamic environmental changes.
Existing efforts to address this issue have primarily resorted to trading off
reactivity against decision consistency, without achieving both. To address
this limitation, we propose a novel algorithm, Temporal Action Selector (TAS),
which caches predicted action chunks from multiple timesteps and dynamically
selects the optimal action through a lightweight selector network. TAS achieves
balanced optimization across three critical dimensions: reactivity, decision
consistency, and motion coherence. Experiments across multiple tasks with
diverse base policies show that TAS significantly improves success rates -
yielding an absolute gain of up to 73.3%. Furthermore, integrating TAS as a
base policy with residual reinforcement learning (RL) substantially enhances
training efficiency and elevates the performance plateau. Experiments in both
simulation and physical robots confirm the method's efficacy.

</details>


### [124] [Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment](https://arxiv.org/abs/2511.04555)
*Tao Lin,Yilei Zhong,Yuxin Du,Jingjing Zhang,Jiting Liu,Yinxinyu Chen,Encheng Gu,Ziyan Liu,Hongyi Cai,Yanwen Zou,Lixing Zou,Zhaoye Zhou,Gen Li,Bo Zhao*

Main category: cs.RO

TL;DR: Evo-1是一款轻量级视觉-语言-行动（VLA）模型，能够在不依赖大规模机器人数据预训练的情况下，实现高效部署和强大性能。该模型在多个主流任务集上取得了领先成果，并公开资源以促进该领域研究。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型参数庞大、训练成本高、部署实时性差，且常常因训练范式原因导致感知表达退化、过拟合和泛化能力不足。需要能够兼顾性能、效率和泛化能力的VLA模型，以推动机器人多模态智能的发展和落地。

Method: Evo-1基于原生多模态视觉-语言模型（VLM），创新性地引入交叉调制扩散Transformer及优化融合模块，形成高效架构。训练上采用两阶段范式，分步将动作与感知对齐，同时保护VLM的表征能力，避免过拟合和表达能力损失。

Result: Evo-1仅有7.7亿参数，无需机器人数据预训练，在Meta-World和RoboTwin评测集上超过此前最佳模型12.4%和6.9%。在LIBERO任务上达到94.8%。实地评测中，Evo-1以高推理频率和低内存消耗实现78%成功率，大幅优于现有方法。

Conclusion: Evo-1证明了轻量级VLA模型可在不依赖大数据预训练条件下高效、准确执行多样任务，具备实际部署和应用价值。其方法和公开的资源对VLA模型的研究与应用有重要推动作用。

Abstract: Vision-Language-Action (VLA) models have emerged as a powerful framework that
unifies perception, language, and control, enabling robots to perform diverse
tasks through multimodal understanding. However, current VLA models typically
contain massive parameters and rely heavily on large-scale robot data
pretraining, leading to high computational costs during training, as well as
limited deployability for real-time inference. Moreover, most training
paradigms often degrade the perceptual representations of the vision-language
backbone, resulting in overfitting and poor generalization to downstream tasks.
In this work, we present Evo-1, a lightweight VLA model that reduces
computation and improves deployment efficiency, while maintaining strong
performance without pretraining on robot data. Evo-1 builds on a native
multimodal Vision-Language model (VLM), incorporating a novel cross-modulated
diffusion transformer along with an optimized integration module, together
forming an effective architecture. We further introduce a two-stage training
paradigm that progressively aligns action with perception, preserving the
representations of the VLM. Notably, with only 0.77 billion parameters, Evo-1
achieves state-of-the-art results on the Meta-World and RoboTwin suite,
surpassing the previous best models by 12.4% and 6.9%, respectively, and also
attains a competitive result of 94.8% on LIBERO. In real-world evaluations,
Evo-1 attains a 78% success rate with high inference frequency and low memory
overhead, outperforming all baseline methods. We release code, data, and model
weights to facilitate future research on lightweight and efficient VLA models.

</details>


### [125] [SAFe-Copilot: Unified Shared Autonomy Framework](https://arxiv.org/abs/2511.04664)
*Phat Nguyen,Erfan Aasi,Shiva Sreeram,Guy Rosman,Andrew Silva,Sertac Karaman,Daniela Rus*

Main category: cs.RO

TL;DR: 本论文提出了一种结合视觉语言模型（VLMs）的高级自动驾驶共享自治框架，能更好融合人类与自动控制，显著提升系统在复杂场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统在罕见、模糊、超分布场景下易失效，而人类能通过语境推理做出更好决策。共享自主有助于弥补这一短板，但目前主流方法大多仅在低层轨迹层面进行人机协作，缺乏对驾驶意图的保留和理解。

Method: 该方法使用视觉语言模型（VLMs），结合多模态线索（如驾驶员动作和环境上下文）推断驾驶意图，并在高层抽象层面调和人类输入与自动驾驶规划器间的协作。首先在模拟人类场景下测试该框架，其后通过问卷调查人类参与者，再用Bench2Drive基准测试系统性能。

Result: 在模拟人类设置下，框架召回率、准确率和精度均表现优异；问卷调查中，92%参与者同意系统仲裁决策；Bench2Drive基准显示与纯自动控制相比，该方法显著减少碰撞率，提升整体性能。

Conclusion: 基于语义和语言级表征的人机仲裁机制能有效捕捉驾驶意图，显著提升共享自治系统在复杂情境下的推理能力和安全表现，是未来自动驾驶共享自治的重要设计原则。

Abstract: Autonomous driving systems remain brittle in rare, ambiguous, and
out-of-distribution scenarios, where human driver succeed through contextual
reasoning. Shared autonomy has emerged as a promising approach to mitigate such
failures by incorporating human input when autonomy is uncertain. However, most
existing methods restrict arbitration to low-level trajectories, which
represent only geometric paths and therefore fail to preserve the underlying
driving intent. We propose a unified shared autonomy framework that integrates
human input and autonomous planners at a higher level of abstraction. Our
method leverages Vision Language Models (VLMs) to infer driver intent from
multi-modal cues -- such as driver actions and environmental context -- and to
synthesize coherent strategies that mediate between human and autonomous
control. We first study the framework in a mock-human setting, where it
achieves perfect recall alongside high accuracy and precision. A human-subject
survey further shows strong alignment, with participants agreeing with
arbitration outcomes in 92% of cases. Finally, evaluation on the Bench2Drive
benchmark demonstrates a substantial reduction in collision rate and
improvement in overall performance compared to pure autonomy. Arbitration at
the level of semantic, language-based representations emerges as a design
principle for shared autonomy, enabling systems to exercise common-sense
reasoning and maintain continuity with human intent.

</details>


### [126] [Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions](https://arxiv.org/abs/2511.04665)
*Kaifeng Zhang,Shuo Sha,Hanxiao Jiang,Matthew Loper,Hyunjong Song,Guangyan Cai,Zhuo Xu,Xiaochen Hu,Changxi Zheng,Yunzhu Li*

Main category: cs.RO

TL;DR: 提出一种从真实视频构建软体物体数字孪生的仿真评估框架，使用3D高斯着色实现高度逼真的渲染，有效将现实场景的机器人操控任务迁移到仿真中进行系统评估。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操控策略（尤其是涉及可变形物体）在现实世界评估成本高、难以复现，仿真方法虽具规模化优势，但视觉与物理复杂度难以真实还原。该研究旨在构建一种新的仿真平台，实现高保真、可复现的策略验证。

Method: 提出“real-to-sim”评估框架，从真实世界视频重建软体物体的数字孪生，并结合3D高斯着色技术实现机器人、物体和环境的照片级渲染，进而在仿真中对策略进行评估。

Result: 在娃娃打包、绳索布线、T型推块等软体操控任务上验证了方法的可行性，仿真评估结果与真实执行表现高度相关。同时，揭示了策略的关键行为模式。

Conclusion: 将物理感知重建与高质量渲染结合，可实现可复现、可扩展且准确的机器人操控策略评估，对实际机器人系统的开发有重要推动作用。

Abstract: Robotic manipulation policies are advancing rapidly, but their direct
evaluation in the real world remains costly, time-consuming, and difficult to
reproduce, particularly for tasks involving deformable objects. Simulation
provides a scalable and systematic alternative, yet existing simulators often
fail to capture the coupled visual and physical complexity of soft-body
interactions. We present a real-to-sim policy evaluation framework that
constructs soft-body digital twins from real-world videos and renders robots,
objects, and environments with photorealistic fidelity using 3D Gaussian
Splatting. We validate our approach on representative deformable manipulation
tasks, including plush toy packing, rope routing, and T-block pushing,
demonstrating that simulated rollouts correlate strongly with real-world
execution performance and reveal key behavioral patterns of learned policies.
Our results suggest that combining physics-informed reconstruction with
high-quality rendering enables reproducible, scalable, and accurate evaluation
of robotic manipulation policies. Website: https://real2sim-eval.github.io/

</details>


### [127] [X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations](https://arxiv.org/abs/2511.04671)
*Maximus A. Pace,Prithwish Dan,Chuanruo Ning,Atiksh Bhardwaj,Audrey Du,Edward W. Duan,Wei-Chiu Ma,Kushal Kedia*

Main category: cs.RO

TL;DR: 本论文提出了X-Diffusion框架，通过在训练扩散策略时合理利用人类与机器人视频数据的异同，提高了机器人操作任务的成功率。


<details>
  <summary>Details</summary>
Motivation: 人类视频可快速大规模采集，是机器人学习的数据宝库，但人机体态不同，直接映射导致动作物理不可行。作者希望利用人类示范的高层指导，规避底层动作差异带来的负面影响。

Method: 设计X-Diffusion方法，先训练判别器区分噪声动作来源（人类/机器人），若添加足够噪声后难分辨其来源，则用于策略训练。低噪声时以机器人动作精细监督，高噪声时以人类动作粗监督，区别利用二者。在训练过程中，逐步融合人类示范和机器人数据。

Result: 在五个操作任务上，直接混合训练会降低机器人策略性能，而X-Diffusion明显提升，平均成功率比最优基线高16%。

Conclusion: X-Diffusion通过扩散过程和判别筛选，有效融合了人类和机器人数据，在存在执行差异时显著提升了机器人操作策略的表现。

Abstract: Human videos can be recorded quickly and at scale, making them an appealing
source of training data for robot learning. However, humans and robots differ
fundamentally in embodiment, resulting in mismatched action execution. Direct
kinematic retargeting of human hand motion can therefore produce actions that
are physically infeasible for robots. Despite these low-level differences,
human demonstrations provide valuable motion cues about how to manipulate and
interact with objects. Our key idea is to exploit the forward diffusion
process: as noise is added to actions, low-level execution differences fade
while high-level task guidance is preserved. We present X-Diffusion, a
principled framework for training diffusion policies that maximally leverages
human data without learning dynamically infeasible motions. X-Diffusion first
trains a classifier to predict whether a noisy action is executed by a human or
robot. Then, a human action is incorporated into policy training only after
adding sufficient noise such that the classifier cannot discern its embodiment.
Actions consistent with robot execution supervise fine-grained denoising at low
noise levels, while mismatched human actions provide only coarse guidance at
higher noise levels. Our experiments show that naive co-training under
execution mismatches degrades policy performance, while X-Diffusion
consistently improves it. Across five manipulation tasks, X-Diffusion achieves
a 16% higher average success rate than the best baseline. The project website
is available at https://portal-cornell.github.io/X-Diffusion/.

</details>


### [128] [GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human and Object Interaction](https://arxiv.org/abs/2511.04679)
*Qingzhou Lu,Yao Feng,Baiyu Shi,Michael Piseno,Zhenan Bao,C. Karen Liu*

Main category: cs.RO

TL;DR: 本文提出了GentleHumanoid框架，将阻抗控制整合到仿人机器人全身运动追踪策略中，实现了上半身的柔顺性。实验结果表明，该方法可减少接触冲击力，提升人机与物互动的安全性和自然度。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习策略多强调刚性跟踪，抑制外力，阻碍了仿人机器人在需要安全、自然身体互动的环境下工作。以往带有阻抗的控制多局限于机器人底座或末端执行器，且更侧重抵抗强力、而非实现柔顺协作，因此亟需全身层面、灵活的柔顺控制机制。

Method: 提出了GentleHumanoid——一个整合阻抗控制的全身运动追踪框架。核心方法是基于弹簧模型统一处理两种力的接触：一是对表面有恢复力的抵抗性接触，二是模拟人类动作推拉的引导性接触。该方法保证了肩、肘、腕等关节间力的运动学一致性，并支持任务自适应的力阈值调节。算法在多种模拟与实际任务中验证。

Result: 在模拟和Unitree G1仿人机器人真实场景测试（如温柔拥抱、辅助坐立、物体安全操控）中，提出的方法在保持任务成功率的同时，显著降低了瞬时接触力峰值，带来了更平顺、更自然的交互。

Conclusion: GentleHumanoid展现了使仿人机器人具备全身柔顺性的可行性，有助于仿人机器人安全、有效地与人协作及操作物体，推动其在真实环境中的应用。

Abstract: Humanoid robots are expected to operate in human-centered environments where
safe and natural physical interaction is essential. However, most recent
reinforcement learning (RL) policies emphasize rigid tracking and suppress
external forces. Existing impedance-augmented approaches are typically
restricted to base or end-effector control and focus on resisting extreme
forces rather than enabling compliance. We introduce GentleHumanoid, a
framework that integrates impedance control into a whole-body motion tracking
policy to achieve upper-body compliance. At its core is a unified spring-based
formulation that models both resistive contacts (restoring forces when pressing
against surfaces) and guiding contacts (pushes or pulls sampled from human
motion data). This formulation ensures kinematically consistent forces across
the shoulder, elbow, and wrist, while exposing the policy to diverse
interaction scenarios. Safety is further supported through task-adjustable
force thresholds. We evaluate our approach in both simulation and on the
Unitree G1 humanoid across tasks requiring different levels of compliance,
including gentle hugging, sit-to-stand assistance, and safe object
manipulation. Compared to baselines, our policy consistently reduces peak
contact forces while maintaining task success, resulting in smoother and more
natural interactions. These results highlight a step toward humanoid robots
that can safely and effectively collaborate with humans and handle objects in
real-world environments.

</details>
