<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 72]
- [cs.CL](#cs.CL) [Total: 40]
- [cs.RO](#cs.RO) [Total: 18]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Preventing Shortcuts in Adapter Training via Providing the Shortcuts](https://arxiv.org/abs/2510.20887)
*Anujraaj Argo Goyal,Guocheng Gordon Qian,Huseyin Coskun,Aarush Gupta,Himmy Tam,Daniil Ostashev,Ju Hu,Dhritiman Sagar,Sergey Tulyakov,Kfir Aberman,Kuan-Chieh Jackson Wang*

Main category: cs.CV

TL;DR: 提出了一种改进的适配器训练方法，通过引入辅助模块分流无关的视觉因子，提升了用于个性化文本到图像生成的泛化能力和文本遵循度。


<details>
  <summary>Details</summary>
Motivation: 现有的适配器经常会将目标属性与杂散属性（如姿势、表情、光照）纠缠在一起，导致模型泛化能力差以及对文本提示的响应能力不足。如何减轻这种属性纠缠，提高个性化图像生成的效果，是该研究关注的动力。

Method: 在适配器训练时，引入辅助模块（如 ControlNet 或 LoRA）专门处理杂散视觉因子，将原本适配器可能学习的非目标属性“分流”出去。适配器仅专注于目标属性（如身份），训练结束后移除辅助模块，推理阶段只用适配器。

Result: 在面部及全身身份注入等任务上，新方法带来了更高的生成质量、更丰富的图像多样性以及更好的文本提示遵循度。

Conclusion: 为大模型时代的特征解耦提供了新的思路——通过有意设计“捷径”引导模型不学习无关属性，从而获得更好的解耦表达和生成能力。

Abstract: Adapter-based training has emerged as a key mechanism for extending the
capabilities of powerful foundation image generators, enabling personalized and
stylized text-to-image synthesis. These adapters are typically trained to
capture a specific target attribute, such as subject identity, using
single-image reconstruction objectives. However, because the input image
inevitably contains a mixture of visual factors, adapters are prone to entangle
the target attribute with incidental ones, such as pose, expression, and
lighting. This spurious correlation problem limits generalization and obstructs
the model's ability to adhere to the input text prompt. In this work, we
uncover a simple yet effective solution: provide the very shortcuts we wish to
eliminate during adapter training. In Shortcut-Rerouted Adapter Training,
confounding factors are routed through auxiliary modules, such as ControlNet or
LoRA, eliminating the incentive for the adapter to internalize them. The
auxiliary modules are then removed during inference. When applied to tasks like
facial and full-body identity injection, our approach improves generation
quality, diversity, and prompt adherence. These results point to a general
design principle in the era of large models: when seeking disentangled
representations, the most effective path may be to establish shortcuts for what
should NOT be learned.

</details>


### [2] [Video-As-Prompt: Unified Semantic Control for Video Generation](https://arxiv.org/abs/2510.20888)
*Yuxuan Bian,Xin Chen,Zenan Li,Tiancheng Zhi,Shen Sang,Linjie Luo,Qiang Xu*

Main category: cs.CV

TL;DR: 本文提出了一种新范式Video-As-Prompt (VAP)用于语义可控视频生成，通过使用参考视频作为语义提示，结合冻结的扩散变换器与可插拔专家，实现强泛化能力和最新的开源表现。


<details>
  <summary>Details</summary>
Motivation: 现有的语义可控视频生成方法要么引入像素级先验导致伪影，要么依赖于特定条件的微调或定制结构，缺乏统一性与泛化能力，急需一种通用、可推广的高质量控制方案。

Method: 作者提出VAP范式，将语义控制视为上下文生成问题，利用参考视频作为语义提示，结合冻结的Video Diffusion Transformer和可插拔Mixture-of-Transformers专家，辅以时序偏置的位置嵌入避免错误匹配；并建立了业内最大规模的VAP-Data数据集支持该方法。

Result: VAP作为统一单模型，在100种语义条件、10万对视频上训练，开源方法中实现了38.7%的用户偏好率，效果媲美最佳商用特定模型，并展示了强零样本泛化能力和多下游应用支持。

Conclusion: VAP为实现通用、高度可控的视频生成迈出重要一步，其强泛化能力和对各类语义条件的支持意味着视频生成领域发展的重大进展。

Abstract: Unified, generalizable semantic control in video generation remains a
critical open challenge. Existing methods either introduce artifacts by
enforcing inappropriate pixel-wise priors from structure-based controls, or
rely on non-generalizable, condition-specific finetuning or task-specific
architectures. We introduce Video-As-Prompt (VAP), a new paradigm that reframes
this problem as in-context generation. VAP leverages a reference video as a
direct semantic prompt, guiding a frozen Video Diffusion Transformer (DiT) via
a plug-and-play Mixture-of-Transformers (MoT) expert. This architecture
prevents catastrophic forgetting and is guided by a temporally biased position
embedding that eliminates spurious mapping priors for robust context retrieval.
To power this approach and catalyze future research, we built VAP-Data, the
largest dataset for semantic-controlled video generation with over 100K paired
videos across 100 semantic conditions. As a single unified model, VAP sets a
new state-of-the-art for open-source methods, achieving a 38.7% user preference
rate that rivals leading condition-specific commercial models. VAP's strong
zero-shot generalization and support for various downstream applications mark a
significant advance toward general-purpose, controllable video generation.

</details>


### [3] [Focal Modulation and Bidirectional Feature Fusion Network for Medical Image Segmentation](https://arxiv.org/abs/2510.20933)
*Moin Safdar,Shahzaib Iqbal,Mehwish Mehmood,Mubeen Ghafoor,Tariq M. Khan,Imran Razzak*

Main category: cs.CV

TL;DR: 本文提出了一种名为FM-BFF-Net的新型医学影像分割网络，结合了卷积神经网络（CNN）与Transformer结构，采用焦点调制注意力机制和双向特征融合模块，在多个公开数据集上超过了当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有的卷积神经网络虽然在医学影像分割中表现突出，但由于局部感受野的限制，难以捕捉全局上下文信息和长距离依赖，从而影响了复杂结构的准确分割。为了解决这一局限性，作者将Transformer的自注意力机制与CNN结合，希望提升分割的全局感知能力和边界精度。

Method: 提出了一种融合卷积和Transformer架构的FM-BFF-Net。该方法包括焦点调制注意力机制，用以增强对全局和局部上下文的感知能力，并引入双向特征融合模块，实现编码器与解码器在不同尺度间的高效信息交互。网络结构针对医学影像在病变边界、形状及大小变化等方面的特殊需求进行了优化。

Result: 在包括息肉检测、皮肤病变分割及超声影像等八个公开数据集上，FM-BFF-Net在Jaccard指数和Dice系数等指标上均取得了优于当前主流方法的效果，展现了更高的边界精度和对不同病变类型的适应性。

Conclusion: FM-BFF-Net能够显著提升医学影像分割的精度和鲁棒性，特别适合应对病灶形状、大小、对比度变化大的场景，具有较强的通用性和应用前景。

Abstract: Medical image segmentation is essential for clinical applications such as
disease diagnosis, treatment planning, and disease development monitoring
because it provides precise morphological and spatial information on anatomical
structures that directly influence treatment decisions. Convolutional neural
networks significantly impact image segmentation; however, since convolution
operations are local, capturing global contextual information and long-range
dependencies is still challenging. Their capacity to precisely segment
structures with complicated borders and a variety of sizes is impacted by this
restriction. Since transformers use self-attention methods to capture global
context and long-range dependencies efficiently, integrating transformer-based
architecture with CNNs is a feasible approach to overcoming these challenges.
To address these challenges, we propose the Focal Modulation and Bidirectional
Feature Fusion Network for Medical Image Segmentation, referred to as
FM-BFF-Net in the remainder of this paper. The network combines convolutional
and transformer components, employs a focal modulation attention mechanism to
refine context awareness, and introduces a bidirectional feature fusion module
that enables efficient interaction between encoder and decoder representations
across scales. Through this design, FM-BFF-Net enhances boundary precision and
robustness to variations in lesion size, shape, and contrast. Extensive
experiments on eight publicly available datasets, including polyp detection,
skin lesion segmentation, and ultrasound imaging, show that FM-BFF-Net
consistently surpasses recent state-of-the-art methods in Jaccard index and
Dice coefficient, confirming its effectiveness and adaptability for diverse
medical imaging scenarios.

</details>


### [4] [Generative Point Tracking with Flow Matching](https://arxiv.org/abs/2510.20951)
*Mattie Tesfaldet,Adam W. Harley,Konstantinos G. Derpanis,Derek Nowrouzezahrai,Christopher Pal*

Main category: cs.CV

TL;DR: 提出了一种新的生成式点追踪器GenPT，能够建模多模态轨迹，在标准点追踪数据集及加入更多遮挡的新数据集上获得了领先表现，尤其提升了对遮挡点的追踪能力。


<details>
  <summary>Details</summary>
Motivation: 传统判别式点追踪方法在存在不确定性（如遮挡或外观变化）时，只能输出平均轨迹，无法区分多种可能性，即难以捕捉多模态。这种局限性在复杂场景、遮挡情况下尤为突出。为解决这一问题，作者提出一种能描述多种轨迹可能性的生成式模型。

Method: 作者提出了GenPT生成式点追踪框架，使用创新的flow matching训练方法，融合了判别式追踪器的迭代优化、窗口依赖先验以及为点坐标专门设计的方差调度。推理阶段，采用模型自信度引导的最佳优先搜索策略在生成样本中选择最优轨迹。

Result: GenPT在PointOdyssey、Dynamic Replica和TAP-Vid等基准测试上与当前最优判别式方法进行了对比。作者还提出了更具遮挡挑战的TAP-Vid变体，实验显示GenPT在遮挡点的多模态轨迹捕捉和追踪准确率上取得了最优，同时对可见点的追踪能力也与现有方法持平。

Conclusion: GenPT首次实现了点轨迹的多模态建模，极大增强了复杂遮挡场景下的点追踪表现，提升了整体点追踪鲁棒性，并且对可见点无明显劣化，适合更具挑战性的实际任务。

Abstract: Tracking a point through a video can be a challenging task due to uncertainty
arising from visual obfuscations, such as appearance changes and occlusions.
Although current state-of-the-art discriminative models excel in regressing
long-term point trajectory estimates -- even through occlusions -- they are
limited to regressing to a mean (or mode) in the presence of uncertainty, and
fail to capture multi-modality. To overcome this limitation, we introduce
Generative Point Tracker (GenPT), a generative framework for modelling
multi-modal trajectories. GenPT is trained with a novel flow matching
formulation that combines the iterative refinement of discriminative trackers,
a window-dependent prior for cross-window consistency, and a variance schedule
tuned specifically for point coordinates. We show how our model's generative
capabilities can be leveraged to improve point trajectory estimates by
utilizing a best-first search strategy on generated samples during inference,
guided by the model's own confidence of its predictions. Empirically, we
evaluate GenPT against the current state of the art on the standard
PointOdyssey, Dynamic Replica, and TAP-Vid benchmarks. Further, we introduce a
TAP-Vid variant with additional occlusions to assess occluded point tracking
performance and highlight our model's ability to capture multi-modality. GenPT
is capable of capturing the multi-modality in point trajectories, which
translates to state-of-the-art tracking accuracy on occluded points, while
maintaining competitive tracking accuracy on visible points compared to extant
discriminative point trackers.

</details>


### [5] [3DReasonKnee: Advancing Grounded Reasoning in Medical Vision Language Models](https://arxiv.org/abs/2510.20967)
*Sraavya Sambara,Sung Eun Kim,Xiaoman Zhang,Luyang Luo,Shreya Johri,Mohammed Baharoon,Du Hyun Ro,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: 本文提出了3DReasonKnee，这是首个针对医学影像3D定位与推理的基准与数据集，专门用于提升和评估视觉-语言模型在3D医学影像中的定位与逐步诊断推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型虽然在医学影像分析有进展，但在3D医学影像中对具体解剖结构的定位与连贯推理依然表现不佳，难以满足临床真实场景下的诊断流程对AI的需求，而目前也缺乏相应的高质量基准与数据支撑该能力的发展。

Method: 作者构建了3DReasonKnee数据集，包含494k高质量配对数据，覆盖7970例膝关节3D MRI。每条数据包含3D影像、一条特定解剖区的诊断问题、空间定位框、临床专家逐步推理链和结构化的严重程度评估等。数据集由专家耗时450小时手动分割与注释，确保临床相关性和准确性。同时提出ReasonKnee-Bench标准用以评价模型在定位和诊断推理方面的能力，并对五种主流VLM模型进行基准测试。

Result: 构建的3DReasonKnee数据集为业界首个具备3D定位推理特性的医学影像数据集，实现了高质量、多维度的信息标注，并通过ReasonKnee-Bench为VLM评估提供了详细基线参考。各SOTA模型在此任务上被系统评测，为进一步模型改进提供方向。

Conclusion: 3DReasonKnee为推动多模态医学AI在3D场景下的真实临床决策能力提供了关键数据资源与测试基准，为AI与医生之间建立可信赖的协作方案奠定基础，有望加速临床智能辅助诊断的发展。

Abstract: Current Vision-Language Models (VLMs) struggle to ground anatomical regions
in 3D medical images and reason about them in a step-by-step manner, a key
requirement of real-world diagnostic assessment. This ability is essential for
aligning model outputs with the diagnostic workflows clinicians use in
practice, enabling trustworthy clinician-AI collaboration. Existing 3D datasets
provide localization labels, but none support this "grounded reasoning"
ability. To address this gap, we introduce 3DReasonKnee, the first 3D grounded
reasoning dataset for medical images, which provides 494k high-quality
quintuples derived from 7,970 3D knee MRI volumes. Each quintuple includes: (1)
the 3D MRI volume, (2) a diagnostic question targeting a specific anatomical
region (3) a 3D bounding box localizing the relevant anatomical structures, (4)
clinician-generated diagnostic reasoning steps that explicitly detail the 3D
reasoning process, and (5) structured severity assessments for the relevant
anatomical region. The creation and validation of 3DReasonKnee, involving over
450 hours of expert clinician time for manually segmenting MRIs and generating
reasoning chains, ensures its superior quality and clinical relevance. We
establish ReasonKnee-Bench to evaluate localization and diagnostic accuracy,
providing insight into VLM ability to perform grounding and severity assessment
across anatomical regions and diagnostic inquiries. We benchmark five
state-of-the-art VLMs, providing baseline performance for ReasonKnee-Bench. By
providing this unique resource of expert-annotated 3D reasoning pathways,
3DReasonKnee serves as a repository of orthopedic surgeons' diagnostic
expertise and offers a vital testbed for advancing multimodal medical AI
systems towards 3D, clinically aligned, localized decision-making capabilities.
The dataset can be found in:
https://huggingface.co/datasets/rajpurkarlab/3DReasonKnee

</details>


### [6] [Thermal Polarimetric Multi-view Stereo](https://arxiv.org/abs/2510.20972)
*Takahiro Kushida,Kenichiro Tanaka*

Main category: cs.CV

TL;DR: 本文提出了一种基于热极化信息的详细3D形状重建新方法，不依赖照明和材料特性，能够精准还原复杂物体细节。


<details>
  <summary>Details</summary>
Motivation: 现有3D重建方法往往依赖可见光、受照明条件和材料属性影响，且对透明、半透明与异质材料物体重建效果不佳。作者旨在解决这些局限。

Method: 推导了极化成像的一般理论，发现长波红外（LWIR）极化成像不受可见光极化分析中固有歧义影响，并提出融合多视角热极化图像进行3D形状恢复的新算法。

Result: 实验表明，该方法能有效重建透明、半透明以及异质物体的精细结构，重建效果优于当前已有的技术。

Conclusion: 基于热极化的3D重建方法克服了传统手段的局限，适用于复杂材料和微细结构的高精度3D建模，有较强的实际应用潜力。

Abstract: This paper introduces a novel method for detailed 3D shape reconstruction
utilizing thermal polarization cues. Unlike state-of-the-art methods, the
proposed approach is independent of illumination and material properties. In
this paper, we formulate a general theory of polarization observation and show
that long-wave infrared (LWIR) polarimetric imaging is free from the
ambiguities that affect visible polarization analyses. Subsequently, we propose
a method for recovering detailed 3D shapes using multi-view thermal
polarimetric images. Experimental results demonstrate that our approach
effectively reconstructs fine details in transparent, translucent, and
heterogeneous objects, outperforming existing techniques.

</details>


### [7] [VESSA: Video-based objEct-centric Self-Supervised Adaptation for Visual Foundation Models](https://arxiv.org/abs/2510.20994)
*Jesimon Barreto,Carlos Caetano,André Araujo,William Robson Schwartz*

Main category: cs.CV

TL;DR: 该论文提出了一种自监督微调方法VESSA，用于无标签情况下适应视觉基础模型到新领域，表现出对比现有方法更优的下游分类任务表现。


<details>
  <summary>Details</summary>
Motivation: 基础视觉模型在大规模预训练和有监督下表现优异，但在分布偏移和标注稀缺情况下效果不佳，传统的自监督适应策略对视觉模型无效，因而需探索新的自监督适应方法。

Method: 提出VESSA方法，实现对视觉基础模型的无监督领域适应。该方法利用多视角物体中心视频，通过自蒸馏范式和参数高效适应技术调整预测头，使模型在保留预训练知识的同时具备对新领域视频多样性和鲁棒性，无需人工注释。

Result: 在3个视觉基础模型和2个数据集上进行实验，VESSA在下游分类任务上相比基础模型和以往适应方法均取得持续性能提升。

Conclusion: VESSA证明了多视角视频自监督微调是有效的视觉基础模型适应策略，无需标注即可增强模型在新域的表现，且优于现有主流方法。

Abstract: Foundation models have advanced computer vision by enabling strong
performance across diverse tasks through large-scale pretraining and supervised
fine-tuning. However, they may underperform in domains with distribution shifts
and scarce labels, where supervised fine-tuning may be infeasible. While
continued self-supervised learning for model adaptation is common for
generative language models, this strategy has not proven effective for
vision-centric encoder models. To address this challenge, we introduce a novel
formulation of self-supervised fine-tuning for vision foundation models, where
the model is adapted to a new domain without requiring annotations, leveraging
only short multi-view object-centric videos. Our method is referred to as
VESSA: Video-based objEct-centric Self-Supervised Adaptation for visual
foundation models. VESSA's training technique is based on a self-distillation
paradigm, where it is critical to carefully tune prediction heads and deploy
parameter-efficient adaptation techniques - otherwise, the model may quickly
forget its pretrained knowledge and reach a degraded state. VESSA benefits
significantly from multi-view object observations sourced from different frames
in an object-centric video, efficiently learning robustness to varied capture
conditions, without the need of annotations. Through comprehensive experiments
with 3 vision foundation models on 2 datasets, VESSA demonstrates consistent
improvements in downstream classification tasks, compared to the base models
and previous adaptation methods. Code is publicly available at
https://github.com/jesimonbarreto/VESSA.

</details>


### [8] [BioDet: Boosting Industrial Object Detection with Image Preprocessing Strategies](https://arxiv.org/abs/2510.21000)
*Jiaqi Hu,Hongli Xu,Junwen Huang,Peter KT Yu,Slobodan Ilic,Benjamin Busam*

Main category: cs.CV

TL;DR: 本文提出了一种针对工业环境中未见物体的标准化2D检测流程，通过图像增强和背景去除，有效提升了下游位姿估计算法的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有6D位姿估计流程依赖于现成的物体检测器，其性能在复杂环境（如遮挡、光照差、背景复杂等）下严重退化，尤其检测阶段成为瓶颈。为解决此问题，作者提出提升未见物体检测稳定性的通用方法。

Method: 基于当前最优检测基线，提出通过低光照图像增强与由基础模型引导的开放词汇背景去除，抑制错误检测和背景干扰。该流程可模块化接入任意2D检测任务，有效减少领域偏移和背景伪影，提升最终检测结果可靠性。

Result: 在BOP工业抓取基准的真实场景测试中，该方法在检测准确率上有显著提升，同时推理开销极小。

Conclusion: 所提方案能有效提升复杂工业环境中未见物体的检测精度，且具备良好的实用性和易用性。

Abstract: Accurate 6D pose estimation is essential for robotic manipulation in
industrial environments. Existing pipelines typically rely on off-the-shelf
object detectors followed by cropping and pose refinement, but their
performance degrades under challenging conditions such as clutter, poor
lighting, and complex backgrounds, making detection the critical bottleneck. In
this work, we introduce a standardized and plug-in pipeline for 2D detection of
unseen objects in industrial settings. Based on current SOTA baselines, our
approach reduces domain shift and background artifacts through low-light image
enhancement and background removal guided by open-vocabulary detection with
foundation models. This design suppresses the false positives prevalent in raw
SAM outputs, yielding more reliable detections for downstream pose estimation.
Extensive experiments on real-world industrial bin-picking benchmarks from BOP
demonstrate that our method significantly boosts detection accuracy while
incurring negligible inference overhead, showing the effectiveness and
practicality of the proposed method.

</details>


### [9] [Deep learning-based automated damage detection in concrete structures using images from earthquake events](https://arxiv.org/abs/2510.21063)
*Abdullah Turer,Yongsheng Bai,Halil Sezen,Alper Yilmaz*

Main category: cs.CV

TL;DR: 本文提出利用深度学习方法，基于地震后图片自动检测钢筋外露和结构损伤，实现建筑与桥梁地震损伤的快速评估。


<details>
  <summary>Details</summary>
Motivation: 地震发生后，结构损伤的及时评估对公共安全和应急响应至关重要。钢筋外露常见于混凝土剥落或严重开裂，是结构损伤退化的关键标志，因此开发一种快速自动检测钢筋外露的工具十分必要。

Method: 收集2023年土耳其地震后多样化损伤结构图片，人工标注，采用深度学习（YOLOv11）模型，结合数据增强、微调，在公共数据集上测试，实现对室内外及结构部件、开裂、剥落和钢筋暴露的自动识别，构建混合自动化分级框架。

Result: 训练的深度学习模型可以准确自动地检测图片中的钢筋外露和不同类型损伤，并能够自动辨别损伤等级，实现了地震后结构自动化损伤评估。

Conclusion: 结合图像采集与深度学习的新方法能够高效、自动地在不同场景下实现灾后损伤检测，提高了地震后的损伤识别速度和准确率，有助于应急响应。

Abstract: Timely assessment of integrity of structures after seismic events is crucial
for public safety and emergency response. This study focuses on assessing the
structural damage conditions using deep learning methods to detect exposed
steel reinforcement in concrete buildings and bridges after large earthquakes.
Steel bars are typically exposed after concrete spalling or large flexural or
shear cracks. The amount and distribution of exposed steel reinforcement is an
indication of structural damage and degradation. To automatically detect
exposed steel bars, new datasets of images collected after the 2023 Turkey
Earthquakes were labeled to represent a wide variety of damaged concrete
structures. The proposed method builds upon a deep learning framework, enhanced
with fine-tuning, data augmentation, and testing on public datasets. An
automated classification framework is developed that can be used to identify
inside/outside buildings and structural components. Then, a YOLOv11 (You Only
Look Once) model is trained to detect cracking and spalling damage and exposed
bars. Another YOLO model is finetuned to distinguish different categories of
structural damage levels. All these trained models are used to create a hybrid
framework to automatically and reliably determine the damage levels from input
images. This research demonstrates that rapid and automated damage detection
following disasters is achievable across diverse damage contexts by utilizing
image data collection, annotation, and deep learning approaches.

</details>


### [10] [ZING-3D: Zero-shot Incremental 3D Scene Graphs via Vision-Language Models](https://arxiv.org/abs/2510.21069)
*Pranav Saxena,Jimmy Chiun*

Main category: cs.CV

TL;DR: 本文提出了ZING-3D框架，利用预训练模型零样本地识别和抽象复杂3D场景，同时支持逐步更新和3D几何定位，适合机器人应用。


<details>
  <summary>Details</summary>
Motivation: 当前3D场景图生成方法主要局限于单视角，无法增量更新，缺乏明确的3D几何结合，难以满足机器人等具身场景的实际需求。

Method: ZING-3D利用预训练视觉-语言模型（VLM），在2D视角下生成场景语义图，并结合深度信息将其锚定到3D空间，节点包含开放词汇对象的特征、3D位置和语义上下文，边包含空间和语义关系及对象间距离，可零样本识别和逐步更新。

Result: 在Replica和HM3D数据集上的实验表明，无需任务定制训练的ZING-3D能有效捕捉场景的空间和关系知识。

Conclusion: ZING-3D为复杂3D场景的语义感知提供了灵活、高效、可扩展的方案，适用于机器人等需要连续场景理解和增量知识更新的应用场景。

Abstract: Understanding and reasoning about complex 3D environments requires structured
scene representations that capture not only objects but also their semantic and
spatial relationships. While recent works on 3D scene graph generation have
leveraged pretrained VLMs without task-specific fine-tuning, they are largely
confined to single-view settings, fail to support incremental updates as new
observations arrive and lack explicit geometric grounding in 3D space, all of
which are essential for embodied scenarios. In this paper, we propose, ZING-3D,
a framework that leverages the vast knowledge of pretrained foundation models
to enable open-vocabulary recognition and generate a rich semantic
representation of the scene in a zero-shot manner while also enabling
incremental updates and geometric grounding in 3D space, making it suitable for
downstream robotics applications. Our approach leverages VLM reasoning to
generate a rich 2D scene graph, which is grounded in 3D using depth
information. Nodes represent open-vocabulary objects with features, 3D
locations, and semantic context, while edges capture spatial and semantic
relations with inter-object distances. Our experiments on scenes from the
Replica and HM3D dataset show that ZING-3D is effective at capturing spatial
and relational knowledge without the need of task-specific training.

</details>


### [11] [WaveSeg: Enhancing Segmentation Precision via High-Frequency Prior and Mamba-Driven Spectrum Decomposition](https://arxiv.org/abs/2510.21079)
*Guoan Xu,Yang Xiao,Wenjing Jia,Guangwei Gao,Guo-Jun Qi,Chia-Wen Lin*

Main category: cs.CV

TL;DR: 提出了一种新型语义分割解码器WaveSeg，在空间和小波域中协同优化特征，显著提升像素精细度和整体分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有语义分割网络多依赖强力的预训练编码器，而采用的解码器较为简单，导致语义信息与细节保持之间难以平衡。需要设计能更好提取细节同时兼具语义理解能力的新型解码结构。

Method: 提出WaveSeg解码器。核心创新包括：1）输入阶段直接学习图像高频分量，增强边界细节；2）提出多尺度双域融合机制DDO，结合频域与空间域特征；3）设计了利用Mamba高效长程建模的光谱分解注意力（SDA）模块，进一步优化高频结构表征；4）在小波域用重参数卷积保持低频语义信息；5）最后采用残差引导的多尺度特征融合，实现分辨率保持与边界感知的高质量输出。

Result: 在标准数据集上进行大量实验，WaveSeg在定量和定性分析上均超过现有主流方法，展现出更高效及精确的分割能力。

Conclusion: WaveSeg首次把小波域频率先验和Mamba注意力机制结合在解码器中，有效提升了分割任务中的细节和语义信息保持能力，为高效精准的语义分割带来全新框架。

Abstract: While recent semantic segmentation networks heavily rely on powerful
pretrained encoders, most employ simplistic decoders, leading to suboptimal
trade-offs between semantic context and fine-grained detail preservation. To
address this, we propose a novel decoder architecture, WaveSeg, which jointly
optimizes feature refinement in spatial and wavelet domains. Specifically,
high-frequency components are first learned from input images as explicit
priors to reinforce boundary details at early stages. A multi-scale fusion
mechanism, Dual Domain Operation (DDO), is then applied, and the novel Spectrum
Decomposition Attention (SDA) block is proposed, which is developed to leverage
Mamba's linear-complexity long-range modeling to enhance high-frequency
structural details. Meanwhile, reparameterized convolutions are applied to
preserve low-frequency semantic integrity in the wavelet domain. Finally, a
residual-guided fusion integrates multi-scale features with boundary-aware
representations at native resolution, producing semantically and structurally
rich feature maps. Extensive experiments on standard benchmarks demonstrate
that WaveSeg, leveraging wavelet-domain frequency prior with Mamba-based
attention, consistently outperforms state-of-the-art approaches both
quantitatively and qualitatively, achieving efficient and precise segmentation.

</details>


### [12] [Knowledge-Driven Vision-Language Model for Plexus Detection in Hirschsprung's Disease](https://arxiv.org/abs/2510.21083)
*Youssef Megahed,Atallah Madi,Dina El Demellawy,Adrian D. C. Chan*

Main category: cs.CV

TL;DR: 本研究提出了一种结合专家知识和视觉-语言模型的新方法，用于提升Hirschsprung病的组织切片判别能力，模型准确率、精确率和特异性均优于传统CNN方法。


<details>
  <summary>Details</summary>
Motivation: 目前Hirschsprung病的诊断依赖显微镜下对神经节细胞区域的识别，深度学习虽有效但常被视为黑盒，缺乏可解释性且与医生的决策过程不同。因此，需要更透明并结合医学知识的模型。

Method: 作者提出将专家文本概念融入基于对比语言-图像预训练（CLIP）类视觉-语言模型，利用医学文献与教科书生成的提示词，引入QuiltNet，将这些语义线索与视觉特征对齐以指导神经丛区域分类。

Result: 该方法在各项分类指标上均优于VGG-19、ResNet-18和ResNet-50等常用CNN模型，准确率83.9%、精确率86.6%、特异性87.6%。

Conclusion: 融合专家知识的多模态学习方法能够提升组织病理图像的判别能力，使模型输出更贴近临床，具有很高的应用前景。

Abstract: Hirschsprung's disease is defined as the congenital absence of ganglion cells
in some segment(s) of the colon. The muscle cannot make coordinated movements
to propel stool in that section, most commonly leading to obstruction. The
diagnosis and treatment for this disease require a clear identification of
different region(s) of the myenteric plexus, where ganglion cells should be
present, on the microscopic view of the tissue slide. While deep learning
approaches, such as Convolutional Neural Networks, have performed very well in
this task, they are often treated as black boxes, with minimal understanding
gained from them, and may not conform to how a physician makes decisions. In
this study, we propose a novel framework that integrates expert-derived textual
concepts into a Contrastive Language-Image Pre-training-based vision-language
model to guide plexus classification. Using prompts derived from expert sources
(e.g., medical textbooks and papers) generated by large language models and
reviewed by our team before being encoded with QuiltNet, our approach aligns
clinically relevant semantic cues with visual features. Experimental results
show that the proposed model demonstrated superior discriminative capability
across different classification metrics as it outperformed CNN-based models,
including VGG-19, ResNet-18, and ResNet-50; achieving an accuracy of 83.9%, a
precision of 86.6%, and a specificity of 87.6%. These findings highlight the
potential of multi-modal learning in histopathology and underscore the value of
incorporating expert knowledge for more clinically relevant model outputs.

</details>


### [13] [HistRetinex: Optimizing Retinex model in Histogram Domain for Efficient Low-Light Image Enhancement](https://arxiv.org/abs/2510.21100)
*Jingtian Zhao,Xueli Xie,Jianxiang Xi,Xiaogang Yang,Haoxuan Sun*

Main category: cs.CV

TL;DR: 本文提出了一种基于直方图域的Retinex低照度图像增强方法（HistRetinex），在提升图像可见性与性能指标的同时显著提高了处理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Retinex的低照度图像增强方法在处理大尺寸图像时耗时较长，亟需一种兼顾效果与效率的新方法。

Method: 将传统的空间域Retinex模型扩展到直方图域，通过定义直方图位置矩阵和计数矩阵，建立照明、反射和低照度图像之间的直方图关系，结合先验信息，设计了一个两层优化模型，并给出了解的迭代公式，最后通过直方图匹配实现图像增强。

Result: HistRetinex在可见性和各项性能指标上优于现有方法，且在分辨率1000*664的图像上仅需1.86秒处理，至少节省6.67秒。

Conclusion: HistRetinex方法兼顾了增强效果和处理速度，为低照度图像增强提供了高效实用的新途径。

Abstract: Retinex-based low-light image enhancement methods are widely used due to
their excellent performance. However, most of them are time-consuming for
large-sized images. This paper extends the Retinex model from the spatial
domain to the histogram domain, and proposes a novel histogram-based Retinex
model for fast low-light image enhancement, named HistRetinex. Firstly, we
define the histogram location matrix and the histogram count matrix, which
establish the relationship among histograms of the illumination, reflectance
and the low-light image. Secondly, based on the prior information and the
histogram-based Retinex model, we construct a novel two-level optimization
model. Through solving the optimization model, we give the iterative formulas
of the illumination histogram and the reflectance histogram, respectively.
Finally, we enhance the low-light image through matching its histogram with the
one provided by HistRetinex. Experimental results demonstrate that the
HistRetinex outperforms existing enhancement methods in both visibility and
performance metrics, while executing 1.86 seconds on 1000*664 resolution
images, achieving a minimum time saving of 6.67 seconds.

</details>


### [14] [PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments](https://arxiv.org/abs/2510.21111)
*Weijie Zhou,Xuantang Xiong,Yi Peng,Manli Tao,Chaoyang Zhao,Honghui Dong,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: 本文提出并研究了主动视觉推理（AVR）任务，让多模态大模型在可交互、部分可观测的环境中，通过主动探索和行动完成视觉推理，从而更贴近真实世界应用。提出了CLEVR-AVR基准和AVR-152k数据集，并开发了新模型PhysVLM-AVR，显著提升了相关任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大模型（MLLMs）视觉推理能力主要局限于静态、可完全观测的情景，这与现实环境中信息不完全的情况（如遮挡或视野限制）不符，人类则能主动探索环境收集信息。为弥补MLLM在主动推理上的不足，提出AVR任务。

Method: 提出了AVR任务，要求智能体通过连续物理行动主动采集信息，多步整合观测并动态调整决策。同时设计了CLEVR-AVR基准环境和包含丰富链式推理标注的AVR-152k数据集，涵盖了不确定性识别、基于行动的信息增益预测和最有效行动选择。在此基础上，开发了PhysVLM-AVR模型。

Result: PhysVLM-AVR在CLEVR-AVR、OpenEQA、RoboVQA等主动和被动视觉推理任务中都取得了最新最强结果。分析也显示当前主流的具身MLLM虽然能感知信息不完整，但难以通过主动交互行动获取和整合新信息，暴露了主动推理方面的短板。

Conclusion: 本文首次系统性提出并实验验证了主动视觉推理的任务和基准，为MLLM在现实世界交互推理应用打下基础。未来需进一步提升模型的主动感知和推理能力，促进人工智能在复杂现实环境中的应用。

Abstract: Visual reasoning in multimodal large language models (MLLMs) has primarily
been studied in static, fully observable settings, limiting their effectiveness
in real-world environments where information is often incomplete due to
occlusion or limited field of view. Humans, in contrast, actively explore and
interact with their environment-moving, examining, and manipulating objects-to
gather information through a closed-loop process integrating perception,
reasoning, and action. Inspired by this human capability, we introduce the
Active Visual Reasoning (AVR) task, extending visual reasoning to partially
observable, interactive environments. AVR necessitates agents to: (1) actively
acquire information via sequential physical actions, (2) integrate observations
across multiple steps for coherent reasoning, and (3) dynamically adjust
decisions based on evolving visual feedback. To rigorously evaluate AVR, we
introduce CLEVR-AVR, a simulation benchmark featuring multi-round interactive
environments designed to assess both reasoning correctness and
information-gathering efficiency. We present AVR-152k, a large-scale dataset
that offers rich Chain-of-Thought (CoT) annotations detailing iterative
reasoning for uncertainty identification, action-conditioned information gain
prediction, and information-maximizing action selection, crucial for training
agents in a higher-order Markov Decision Process. Building on this, we develop
PhysVLM-AVR, an MLLM achieving state-of-the-art performance on CLEVR-AVR,
embodied reasoning (OpenEQA, RoboVQA), and passive visual reasoning (GeoMath,
Geometry30K). Our analysis also reveals that current embodied MLLMs, despite
detecting information incompleteness, struggle to actively acquire and
integrate new information through interaction, highlighting a fundamental gap
in active reasoning capabilities.

</details>


### [15] [Urban 3D Change Detection Using LiDAR Sensor for HD Map Maintenance and Smart Mobility](https://arxiv.org/abs/2510.21112)
*Hezam Albagami,Haitian Wang,Xinyu Wang,Muhammad Ibrahim,Zainy M. Malakan,Abdullah M. Alqamdi,Mohammed H. Alghamdi,Ajmal Mian*

Main category: cs.CV

TL;DR: 该论文提出了一种面向对象、具备不确定性意识的高精度城市级LiDAR变化检测方法，提升了城市3D地图的自动维护和变更检测的准确性，对地图更新、建筑监控和自动驾驶至关重要。


<details>
  <summary>Details</summary>
Motivation: 传统基于DSM和影像的方法在复杂地形与视角不匹配场景下表现不佳，且无法准确关联或处理目标分裂与合并问题。点云神经模型和体素方法则存在对齐精度、内存消耗大及目标分裂/合并处理不足的问题，因此需要更精确且具备不确定性处理的新方法。

Method: 方法包括多分辨率NDT与点到面ICP对不同时期点云精确配准，通过高度归一化和注册协方差、表面粗糙度推算检测水平，自适应剔除虚假变化。利用几何先验进行跨时期对象关联，通过语义和实例分割及类约束二分匹配，处理复杂目标分裂与合并。分块处理平衡内存并保护细节变化，融合重叠度、法线、体积等多特征判别变化，决策结果根据局部检测水平动态调整。

Result: 在15个城市街区的实验中，所提方法达到了95.2%准确率、90.4% mF1和82.6% mIoU，优于当前代表性方法Triplet KPConv，尤其在检测目标减少（Decreased）类提升显著。

Conclusion: 该方法适用于城市大规模LiDAR数据的高精度变化检测，兼具不确定性量化、目标身份保持与复杂情况处理优势，为HD地图构建与智能交通提供了坚实基础。

Abstract: High-definition 3D city maps underpin smart transportation, digital twins,
and autonomous driving, where object level change detection across bi temporal
LiDAR enables HD map maintenance, construction monitoring, and reliable
localization. Classical DSM differencing and image based methods are sensitive
to small vertical bias, ground slope, and viewpoint mismatch and yield cellwise
outputs without object identity. Point based neural models and voxel encodings
demand large memory, assume near perfect pre alignment, degrade thin
structures, and seldom enforce class consistent association, which leaves split
or merge cases unresolved and ignores uncertainty. We propose an object
centric, uncertainty aware pipeline for city scale LiDAR that aligns epochs
with multi resolution NDT followed by point to plane ICP, normalizes height,
and derives a per location level of detection from registration covariance and
surface roughness to calibrate decisions and suppress spurious changes.
Geometry only proxies seed cross epoch associations that are refined by
semantic and instance segmentation and a class constrained bipartite assignment
with augmented dummies to handle splits and merges while preserving per class
counts. Tiled processing bounds memory without eroding narrow ground changes,
and instance level decisions combine 3D overlap, normal direction displacement,
and height and volume differences with a histogram distance, all gated by the
local level of detection to remain stable under partial overlap and sampling
variation. On 15 representative Subiaco blocks the method attains 95.2%
accuracy, 90.4% mF1, and 82.6% mIoU, exceeding Triplet KPConv by 0.2 percentage
points in accuracy, 0.2 in mF1, and 0.8 in mIoU, with the largest gain on
Decreased where IoU reaches 74.8% and improves by 7.6 points.

</details>


### [16] [Controllable-LPMoE: Adapting to Challenging Object Segmentation via Dynamic Local Priors from Mixture-of-Experts](https://arxiv.org/abs/2510.21114)
*Yanguang Sun,Jiawei Lian,Jian Yang,Lei Luo*

Main category: cs.CV

TL;DR: 本文提出了一种名为 Controllable-LPMoE 的动态先验微调范式，能高效适配大规模基础模型用于分割任务，参数更少且性能优越。


<details>
  <summary>Details</summary>
Motivation: 传统基础模型全参数微调需大量计算资源，形成训练效率瓶颈，而现有可训练提示缺乏语义先验，导致适应性有限。

Method: 提出一种‘动态先验’微调范式Controllable-LPMoE：通过轻量级、多样卷积提取输入图像的本地先验，并用门控网络动态选择专家先验；还设计了双向交互适配器，用于在冻结与可训练特征间高效重组和增强。

Result: 大量实验表明，Controllable-LPMoE在多个二值目标分割任务上优于31种SOTA方法，表现出优秀的分割性能和广泛适配性。

Conclusion: 所提方法能以更少参数高效微调大模型，提升下游分割任务表现，有望推广到更多场景。

Abstract: Large-scale foundation models provide powerful feature representations for
downstream object segmentation tasks. However, when adapted to specific tasks
through the full-parameter fine-tuning, the enormous parameters being updated
often results in significant computational overhead, creating a bottleneck in
training efficiency. Although existing methods attempt to fine-tune frozen
models by directly embedding trainable prompts, these prompts lack inherent
semantic priors, limiting the adaptability of large-scale models. In this
paper, we propose a novel dynamic priors-based fine-tuning paradigm with fewer
trainable parameters, dubbed Controllable-LPMoE, which adaptively modulates
frozen foundation models by dynamically controlling local priors to enhance
fine-grained perception for specific segmentation tasks. More specifically, we
construct a lightweight dynamic mixed local priors extractor that captures
diverse local priors from input images through heterogeneous convolutions while
employing a gating network to dynamically output expert priors required for the
subsequent fine-tuning. Furthermore, we design a bi-directional interaction
adapter that employs cosine-aligned deformable attention and channel-oriented
adaptive scale enhancement to interact and restructure between frozen and
trainable features, achieving efficient fine-tuning. Extensive experiments
validate the superiority of our
\href{https://github.com/CSYSI/Controllable-LPMoE} {Controllable-LPMoE}
approach, demonstrating excellent segmentation performance compared to 31
state-of-the-art (SOTA) methods and adaptability to multiple binary object
segmentation tasks.

</details>


### [17] [PhysWorld: From Real Videos to World Models of Deformable Objects via Physics-Aware Demonstration Synthesis](https://arxiv.org/abs/2510.21447)
*Yu Yang,Zhilu Zhang,Xiang Zhang,Yihan Zeng,Hui Li,Wangmeng Zuo*

Main category: cs.CV

TL;DR: PhysWorld结合物理仿真与图神经网络，通过合成多样的物理演示数据，高效学习变形物体的世界动力学模型，实现准确且快速的预测与泛化。


<details>
  <summary>Details</summary>
Motivation: 现实视频数据有限，尤其是变形物体（如软体或弹性体）空间物理属性各异，导致基于数据训练的动力学模型难以实现物理一致、泛化能力强的预测。

Method: 1. 在MPM物理仿真器中，通过本构模型选择和物理属性全局到局部优化，构建与真实物理一致的数字孪生体。
2. 针对数字孪生体进行分部物理属性扰动，生成多样化的运动模式，合成大规模多样物理演示。
3. 利用这些合成数据，训练嵌入物理属性的轻量级图神经网络世界模型。4. 可用真实视频进一步细化物理参数。

Result: PhysWorld可对多种变形物体实现准确、快速的运动预测，在新型交互情境下依然表现出良好的泛化能力。推理速度比最新版PhysTwin快47倍，性能优越。

Conclusion: PhysWorld突破了现实数据稀缺的瓶颈，实现了兼具精度、速度和泛化能力的世界模型，对于机器人、VR/AR等物理交互应用具有重要意义。

Abstract: Interactive world models that simulate object dynamics are crucial for
robotics, VR, and AR. However, it remains a significant challenge to learn
physics-consistent dynamics models from limited real-world video data,
especially for deformable objects with spatially-varying physical properties.
To overcome the challenge of data scarcity, we propose PhysWorld, a novel
framework that utilizes a simulator to synthesize physically plausible and
diverse demonstrations to learn efficient world models. Specifically, we first
construct a physics-consistent digital twin within MPM simulator via
constitutive model selection and global-to-local optimization of physical
properties. Subsequently, we apply part-aware perturbations to the physical
properties and generate various motion patterns for the digital twin,
synthesizing extensive and diverse demonstrations. Finally, using these
demonstrations, we train a lightweight GNN-based world model that is embedded
with physical properties. The real video can be used to further refine the
physical properties. PhysWorld achieves accurate and fast future predictions
for various deformable objects, and also generalizes well to novel
interactions. Experiments show that PhysWorld has competitive performance while
enabling inference speeds 47 times faster than the recent state-of-the-art
method, i.e., PhysTwin.

</details>


### [18] [SafetyPairs: Isolating Safety Critical Image Features with Counterfactual Image Generation](https://arxiv.org/abs/2510.21120)
*Alec Helbling,Shruti Palaskar,Kundan Krishna,Polo Chau,Leon Gatys,Joseph Yitan Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种名为SafetyPairs的新型方法，用于生成仅在安全性相关特征上有差异的图像对，以便更细致地分析图像安全性的判别标准，并发布了一个覆盖9个安全类别的新基准数据集。


<details>
  <summary>Details</summary>
Motivation: 目前的图像安全性数据集标签过于粗略，无法准确定位哪些细节会导致安全性标签的变更。为了更好地评估和提升模型对图像细粒度安全差异的识别能力，需要更加系统和针对性的标注数据。

Method: 作者设计了一套可扩展的框架SafetyPairs，通过图像编辑模型对图像进行有针对性的修改，仅改变与安全政策相关的特征，从而生成一对安全标签相反但其它细节不变的对比图像。利用这种方法，作者建立了一个包括3,020个图像对、涵盖9个安全类别的基准数据集。

Result: SafetyPairs数据集能有效暴露视觉-语言模型在细微安全性差异识别中的不足。另外，将该流程用于训练样本增强，也提升了轻量化守卫模型的样本使用效率。

Conclusion: SafetyPairs为细粒度图像安全性的研究和模型评测，提供了首个系统性资源，并证明了对比式生成和基准能提升模型性能与评估可靠性。

Abstract: What exactly makes a particular image unsafe? Systematically differentiating
between benign and problematic images is a challenging problem, as subtle
changes to an image, such as an insulting gesture or symbol, can drastically
alter its safety implications. However, existing image safety datasets are
coarse and ambiguous, offering only broad safety labels without isolating the
specific features that drive these differences. We introduce SafetyPairs, a
scalable framework for generating counterfactual pairs of images, that differ
only in the features relevant to the given safety policy, thus flipping their
safety label. By leveraging image editing models, we make targeted changes to
images that alter their safety labels while leaving safety-irrelevant details
unchanged. Using SafetyPairs, we construct a new safety benchmark, which serves
as a powerful source of evaluation data that highlights weaknesses in
vision-language models' abilities to distinguish between subtly different
images. Beyond evaluation, we find our pipeline serves as an effective data
augmentation strategy that improves the sample efficiency of training
lightweight guard models. We release a benchmark containing over 3,020
SafetyPair images spanning a diverse taxonomy of 9 safety categories, providing
the first systematic resource for studying fine-grained image safety
distinctions.

</details>


### [19] [MATrack: Efficient Multiscale Adaptive Tracker for Real-Time Nighttime UAV Operations](https://arxiv.org/abs/2510.21586)
*Xuzhao Li,Xuchen Li,Shiyu Hu*

Main category: cs.CV

TL;DR: 本文针对夜间无人机跟踪难题，提出了高效多尺度自适应系统MATrack，在精度和速度上均超越现有方法，并已在实际无人机平台上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现实无人机夜间操作面临低照度、背景杂乱、视角变动等挑战，现有提升或迁移方法存在效率低、伪影多等不足，难以满足实际需求，因此亟需更适用于夜间环境的跟踪系统。

Method: 提出MATrack系统，整合了三大核心模块：多尺度分层融合（MHB）提升静动态模板特征一致性，自适应关键令牌门控模块提升复杂背景中的目标判别能力，夜间模板校准器（NTC）保障长序列下的稳定跟踪。

Result: 在UAVDark135基准集上，MATrack分别在精度、归一化精度和AUC指标上超越SOTA方法5.9%、5.4%和4.2%，且速度高达81FPS。实机平台测试亦证明了其实用性和稳定性。

Conclusion: MATrack系统有效提升了无人机夜间跟踪精度与可靠性，能为夜间搜救、边境巡逻等关键机器人应用提供坚实的技术支撑。

Abstract: Nighttime UAV tracking faces significant challenges in real-world robotics
operations. Low-light conditions not only limit visual perception capabilities,
but cluttered backgrounds and frequent viewpoint changes also cause existing
trackers to drift or fail during deployment. To address these difficulties,
researchers have proposed solutions based on low-light enhancement and domain
adaptation. However, these methods still have notable shortcomings in actual
UAV systems: low-light enhancement often introduces visual artifacts, domain
adaptation methods are computationally expensive and existing lightweight
designs struggle to fully leverage dynamic object information. Based on an
in-depth analysis of these key issues, we propose MATrack-a multiscale adaptive
system designed specifically for nighttime UAV tracking. MATrack tackles the
main technical challenges of nighttime tracking through the collaborative work
of three core modules: Multiscale Hierarchy Blende (MHB) enhances feature
consistency between static and dynamic templates. Adaptive Key Token Gate
accurately identifies object information within complex backgrounds. Nighttime
Template Calibrator (NTC) ensures stable tracking performance over long
sequences. Extensive experiments show that MATrack achieves a significant
performance improvement. On the UAVDark135 benchmark, its precision, normalized
precision and AUC surpass state-of-the-art (SOTA) methods by 5.9%, 5.4% and
4.2% respectively, while maintaining a real-time processing speed of 81 FPS.
Further tests on a real-world UAV platform validate the system's reliability,
demonstrating that MATrack can provide stable and effective nighttime UAV
tracking support for critical robotics applications such as nighttime search
and rescue and border patrol.

</details>


### [20] [NoisyGRPO: Incentivizing Multimodal CoT Reasoning via Noise Injection and Bayesian Estimation](https://arxiv.org/abs/2510.21122)
*Longtian Qiu,Shan Ning,Jiaxuan Sun,Xuming He*

Main category: cs.CV

TL;DR: 提出了一种名为NoisyGRPO的多模态强化学习（RL）框架，通过向视觉输入注入可控噪声并以贝叶斯方法估算优势，显著提升了多模态大模型的链式思维（CoT）泛化和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法难以实现多模态大模型CoT推理的广泛泛化，特别是在超出训练分布时效果不佳，亟需改进以增强模型在多样视觉场景下的推理表现和泛化能力。

Method: NoisyGRPO包括两大创新：1）利用高斯噪声扰动视觉输入以促进模型探索更多视觉场景（噪声注入探索策略）；2）将优势估算问题建模为贝叶斯推断任务，噪声水平作为先验，轨迹奖励作为似然，融合两者形成稳健的优势后验分布，从而增强RL训练。

Result: 在标准链式思维质量、泛化能力、幻觉等基准测试上，尤其是在规模较小的多模态大模型（如Qwen2.5-VL 3B）上，NoisyGRPO明显提升了模型的泛化与鲁棒性。

Conclusion: NoisyGRPO能有效提高多模态大模型的RL训练效果，增强模型应对不同视觉情境的泛化能力和抗噪声的鲁棒性，对小规模多模态大模型尤为有效。

Abstract: Reinforcement learning (RL) has shown promise in enhancing the general
Chain-of-Thought (CoT) reasoning capabilities of multimodal large language
models (MLLMs). However, when applied to improve general CoT reasoning,
existing RL frameworks often struggle to generalize beyond the training
distribution. To address this, we propose NoisyGRPO, a systematic multimodal RL
framework that introduces controllable noise into visual inputs for enhanced
exploration and explicitly models the advantage estimation process via a
Bayesian framework. Specifically, NoisyGRPO improves RL training by: (1)
\textbf{Noise-Injected Exploration Policy}: Perturbing visual inputs with
Gaussian noise to encourage exploration across a wider range of visual
scenarios; and (2) \textbf{Bayesian Advantage Estimation}: Formulating
advantage estimation as a principled Bayesian inference problem, where the
injected noise level serves as a prior and the observed trajectory reward as
the likelihood. This Bayesian modeling fuses both sources of information to
compute a robust posterior estimate of trajectory advantage, effectively
guiding MLLMs to prefer visually grounded trajectories over noisy ones.
Experiments on standard CoT quality, general capability, and hallucination
benchmarks demonstrate that NoisyGRPO substantially improves generalization and
robustness, especially in RL settings with small-scale MLLMs such as Qwen2.5-VL
3B. The project page is available at
\href{https://artanic30.github.io/project_pages/NoisyGRPO/}{\texttt{https://artanic30.github.io/project\_pages/NoisyGRPO}}.

</details>


### [21] [Digital Contrast CT Pulmonary Angiography Synthesis from Non-contrast CT for Pulmonary Vascular Disease](https://arxiv.org/abs/2510.21140)
*Ying Ming,Yue Lin,Longfei Zhao,Gengwan Li,Zuopeng Tan,Bing Li,Sheng Xie,Wei Song,Qiqi Xu*

Main category: cs.CV

TL;DR: 该论文提出了一种利用CycleGAN生成对比增强CTPA影像的新方法，能在无需造影剂的情况下由非增强CT（NCCT）生成数字对比CTPA（DCCTPA），有效避免造影剂带来的风险。


<details>
  <summary>Details</summary>
Motivation: CTPA虽然是确诊肺血管疾病的金标准，但对造影剂的依赖会增加肾毒性和过敏风险，尤其对高危患者不友好。因此，需要能在无需造影剂的情况下得到等效诊断价值的影像方法。

Method: 该研究采用Cycle-Consistent Generative Adversarial Networks（CycleGAN）级联合成器，将410对NCCT与CTPA影像分为训练、验证与测试集，训练模型从NCCT合成DCCTPA影像，并对比最先进方法和原始CTPA在多项定量指标与下游分割任务上的表现。

Result: 在定量评估中，所提方法MAE、PSNR和SSIM均优于现有SOTA方法。质性评估亦显示合成图像血管增强和结构还原效果好。分割任务中，动静脉分割的各种准确率指标均大幅优于直接用NCCT。血管容积ICC明显提升，特别是对小血管增强明显。

Conclusion: 该方法能生成高质量、结构保持良好的虚拟对比CT影像，有望取代传统CTPA用于高危或禁忌造影剂患者，并显著提升下游肺血管诊断与量化任务效果。

Abstract: Computed Tomography Pulmonary Angiography (CTPA) is the reference standard
for diagnosing pulmonary vascular diseases such as Pulmonary Embolism (PE) and
Chronic Thromboembolic Pulmonary Hypertension (CTEPH). However, its reliance on
iodinated contrast agents poses risks including nephrotoxicity and allergic
reactions, particularly in high-risk patients. This study proposes a method to
generate Digital Contrast CTPA (DCCTPA) from Non-Contrast CT (NCCT) scans using
a cascaded synthesizer based on Cycle-Consistent Generative Adversarial
Networks (CycleGAN). Totally retrospective 410 paired CTPA and NCCT scans were
obtained from three centers. The model was trained and validated internally on
249 paired images. Extra dataset that comprising 161 paired images was as test
set for model generalization evaluation and downstream clinical tasks
validation. Compared with state-of-the-art (SOTA) methods, the proposed method
achieved the best comprehensive performance by evaluating quantitative metrics
(For validation, MAE: 156.28, PSNR: 20.71 and SSIM: 0.98; For test, MAE:
165.12, PSNR: 20.27 and SSIM: 0.98) and qualitative visualization,
demonstrating valid vessel enhancement, superior image fidelity and structural
preservation. The approach was further applied to downstream tasks of pulmonary
vessel segmentation and vascular quantification. On the test set, the average
Dice, clDice, and clRecall of artery and vein pulmonary segmentation was 0.70,
0.71, 0.73 and 0.70, 0.72, 0.75 respectively, all markedly improved compared
with NCCT inputs.\@ Inter-class Correlation Coefficient (ICC) for vessel volume
between DCCTPA and CTPA was significantly better than that between NCCT and
CTPA (Average ICC : 0.81 vs 0.70), indicating effective vascular enhancement in
DCCTPA, especially for small vessels.

</details>


### [22] [Towards Physics-informed Spatial Intelligence with Human Priors: An Autonomous Driving Pilot Study](https://arxiv.org/abs/2510.21160)
*Guanlin Wu,Boyan Su,Yang Zhao,Pu Wang,Yichen Lin,Hao Frank Yang*

Main category: cs.CV

TL;DR: 本文提出了一种新的空间智能表示和评测方法——Spatial Intelligence Grid（SIG），用于提升和量化基础模型中的视觉-空间智能（VSI），并发布了相关基准数据集SIGBench。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型对于空间智能的集成和验证尚不完善，通常依靠文字提示和VQA式评分，这种方式难以准确反映空间理解，容易被语言技巧影响，无法有效分离空间能力与语言先验。

Method: 提出网格化的SIG结构，显式编码物体布局、相互关系及物理先验，用作文本外的补充信息通道，并据此设计SIG驱动的VSI评估指标，从而独立量化模型的空间理解能力。

Result: 在使用SIG训练和评测主流多模态LLM（如GPT、Gemini系列）后，模型在VSI各项指标上获得了比仅用VQA方法更大、更稳定和更全面的提升。

Conclusion: SIG为基础大模型空间智能的学习与评测提供了有效工具，能促进更真实且可分离空间能力的提升，并有潜力作为数据标注和训练新标准。

Abstract: How to integrate and verify spatial intelligence in foundation models remains
an open challenge. Current practice often proxies Visual-Spatial Intelligence
(VSI) with purely textual prompts and VQA-style scoring, which obscures
geometry, invites linguistic shortcuts, and weakens attribution to genuinely
spatial skills. We introduce Spatial Intelligence Grid (SIG): a structured,
grid-based schema that explicitly encodes object layouts, inter-object
relations, and physically grounded priors. As a complementary channel to text,
SIG provides a faithful, compositional representation of scene structure for
foundation-model reasoning. Building on SIG, we derive SIG-informed evaluation
metrics that quantify a model's intrinsic VSI, which separates spatial
capability from language priors. In few-shot in-context learning with
state-of-the-art multimodal LLMs (e.g. GPT- and Gemini-family models), SIG
yields consistently larger, more stable, and more comprehensive gains across
all VSI metrics compared to VQA-only representations, indicating its promise as
a data-labeling and training schema for learning VSI. We also release SIGBench,
a benchmark of 1.4K driving frames annotated with ground-truth SIG labels and
human gaze traces, supporting both grid-based machine VSI tasks and
attention-driven, human-like VSI tasks in autonomous-driving scenarios.

</details>


### [23] [Blockwise Flow Matching: Improving Flow Matching Models For Efficient High-Quality Generation](https://arxiv.org/abs/2510.21167)
*Dogyun Park,Taehoon Lee,Minseok Joo,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: 本文提出Blockwise Flow Matching (BFM)方法，通过分块专用的网络结构和语义特征引导，大幅提升流匹配模型的生成效率和质量，并降低推理计算成本，在ImageNet 256x256数据集上实现了2.1至4.9倍推理加速，生成表现与原方法相当。


<details>
  <summary>Details</summary>
Motivation: 现有Flow Matching模型虽然生成高质量数据，但普遍采用单一大网络，难以同时捕获各时刻信号特征，且推理过程计算开销大。作者旨在提升该类模型的效率和精度。

Method: 作者设计了BFM框架，将生成过程按时间片段分段，每段由小型但专精的velocity block建模。引入语义特征引导模块，让每个block能结合丰富的预训练语义信息。提出特征残差近似策略，有效降低计算负担又能保持语义信息。

Result: 在ImageNet 256x256数据集实验显示，BFM相比现有Flow Matching方法，在保持同等生成效果下，实现了2.1-4.9倍推理加速，并显著改善效率-质量折中曲线。

Conclusion: BFM通过结构分块和语义引导创新，使流匹配模型兼顾效率与质量，在生成任务上具有广阔应用前景并推动相关方法实用化。

Abstract: Recently, Flow Matching models have pushed the boundaries of high-fidelity
data generation across a wide range of domains. It typically employs a single
large network to learn the entire generative trajectory from noise to data.
Despite their effectiveness, this design struggles to capture distinct signal
characteristics across timesteps simultaneously and incurs substantial
inference costs due to the iterative evaluation of the entire model. To address
these limitations, we propose Blockwise Flow Matching (BFM), a novel framework
that partitions the generative trajectory into multiple temporal segments, each
modeled by smaller but specialized velocity blocks. This blockwise design
enables each block to specialize effectively in its designated interval,
improving inference efficiency and sample quality. To further enhance
generation fidelity, we introduce a Semantic Feature Guidance module that
explicitly conditions velocity blocks on semantically rich features aligned
with pretrained representations. Additionally, we propose a lightweight Feature
Residual Approximation strategy that preserves semantic quality while
significantly reducing inference cost. Extensive experiments on ImageNet
256x256 demonstrate that BFM establishes a substantially improved Pareto
frontier over existing Flow Matching methods, achieving 2.1x to 4.9x
accelerations in inference complexity at comparable generation performance.
Code is available at https://github.com/mlvlab/BFM.

</details>


### [24] [TokenCLIP: Token-wise Prompt Learning for Zero-shot Anomaly Detection](https://arxiv.org/abs/2510.21171)
*Qihang Zhou,Binbin Gao,Guansong Pang,Xin Wang,Jiming Chen,Shibo He*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法TokenCLIP，用于在零样本条件下提升CLIP对未见对象异常检测的能力。通过引入token级别的动态视觉-文本子空间对齐，模型能够更细致地捕捉异常语义，相比传统方法展现出更优性能。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP方案通过单一文本空间与视觉语义对齐，会导致对各种对象和领域的语义捕捉不够精准，限制了其对异常检测的泛化能力。因此需要实现更细粒度的语义对齐方法，以提升异常检测的准确性。

Method: TokenCLIP采用token级别的动态视觉-可学习文本空间对齐方法。具体实现为：将单一文本空间扩展为一组正交的文本子空间，并通过语义亲和性动态地把每个视觉token分配到最相关的子空间组合。这一分配被建模为最优传输（OT）问题，通过OT保证了子空间优化的充分性与多样性。最后，运用top-k masking进一步稀疏化分配方案，让不同子空间专注于不同视觉区域。

Result: 在大量实验（类别未详述）中，TokenCLIP在未见对象的异常检测任务上，性能优于现有基线方法，展现出更好的泛化和细粒度异常识别能力。

Conclusion: TokenCLIP通过token级动态对齐和子空间组合，大幅提升了CLIP在零样本异常检测中的表现。该框架为未来多领域、多对象的异常检测任务提供了更有效、更通用的解决思路。

Abstract: Adapting CLIP for anomaly detection on unseen objects has shown strong
potential in a zero-shot manner. However, existing methods typically rely on a
single textual space to align with visual semantics across diverse objects and
domains. The indiscriminate alignment hinders the model from accurately
capturing varied anomaly semantics. We propose TokenCLIP, a token-wise
adaptation framework that enables dynamic alignment between visual and
learnable textual spaces for fine-grained anomaly learning. Rather than mapping
all visual tokens to a single, token-agnostic textual space, TokenCLIP aligns
each token with a customized textual subspace that represents its visual
characteristics. Explicitly assigning a unique learnable textual space to each
token is computationally intractable and prone to insufficient optimization. We
instead expand the token-agnostic textual space into a set of orthogonal
subspaces, and then dynamically assign each token to a subspace combination
guided by semantic affinity, which jointly supports customized and efficient
token-wise adaptation. To this end, we formulate dynamic alignment as an
optimal transport problem, where all visual tokens in an image are transported
to textual subspaces based on semantic similarity. The transport constraints of
OT ensure sufficient optimization across subspaces and encourage them to focus
on different semantics. Solving the problem yields a transport plan that
adaptively assigns each token to semantically relevant subspaces. A top-k
masking is then applied to sparsify the plan and specialize subspaces for
distinct visual regions. Extensive experiments demonstrate the superiority of
TokenCLIP.

</details>


### [25] [KBE-DME: Dynamic Multimodal Evaluation via Knowledge Enhanced Benchmark Evolution](https://arxiv.org/abs/2510.21182)
*Junzhe Zhang,Huixuan Zhang,Xiaojun Wan*

Main category: cs.CV

TL;DR: 该论文提出了一种动态多模态评测框架KBE，用以解决传统静态基准测试面临的数据污染和饱和问题，提高多模态大模型评测的可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型（MLLMs）的迅速发展，如何可靠地评估其能力成为挑战。现有静态基准测试易受数据污染与饱和影响，导致评测结果失真，因此亟需更智能、动态的评测标准。

Method: 作者首先将视觉问答（VQA）样本用图结构表示，再提出知识增强的基准演化（KBE）方法，能够根据多模态知识动态扩展原有基准测试，包括：通过重新选择原图视觉信息重构问题、利用外部文本知识扩张问题，并能调控问题难度。

Result: 广泛实验表明，KBE有效降低了数据污染与饱和的风险，并能更全面、细致地评估多模态大模型的能力。

Conclusion: KBE为多模态大模型评测带来了全新思路，提升了评测的科学性和适应性，为今后MLLM能力的合理比对提供了有力工具。

Abstract: The rapid progress of multimodal large language models (MLLMs) calls for more
reliable evaluation protocols. Existing static benchmarks suffer from the
potential risk of data contamination and saturation, leading to inflated or
misleading performance evaluations. To address these issues, we first apply
Graph formulation to represent a static or dynamic VQA sample. With the
formulation, we propose Knowledge-enhanced Benchmark Evolution(KBE), a dynamic
multimodal evaluation framework. KBE first analyzes the original static
benchmark, then expands it by integrating multimodal knowledge, transforming
the static benchmark into a controllable, dynamic evolving version. Crucially,
KBE can both reconstruct questions by Re-selecting visual information in the
original image and expand existing questions with external textual knowledge.
It enables difficulty-controllable evaluation by adjusting the degree of
question exploration. Extensive experiments demonstrate that KBE alleviates the
risk of data contamination, data saturation, and provides a more comprehensive
assessment of MLLM capabilities.

</details>


### [26] [3rd Place Solution to ICCV LargeFineFoodAI Retrieval](https://arxiv.org/abs/2510.21198)
*Yang Zhong,Zhiming Wang,Zhaoyang Li,Jinyu Ma,Xiang Li*

Main category: cs.CV

TL;DR: 本文介绍了在ICCV LargeFineFoodAI 检索竞赛中获得第三名的解决方案。核心是在多模型集成与新重排序方法下实现特征提取与检索表现的提升，最终取得了较高成绩。


<details>
  <summary>Details</summary>
Motivation: 该竞赛旨在推动高效的食品图像检索方法发展。作者希望通过优化损失函数与检索后处理，进一步提升检索精度。

Method: 首先独立训练了四个基本模型，采用ArcFace与Circle loss的加权和作为损失函数优化特征表征。接着运用测试时增强（TTA）及集成（Ensemble）方法提升系统性能。最后，提出结合扩散（diffusion）与k-reciprocal reranking的新型重排序策略优化最终检索结果。

Result: 提出的方法在Kaggle比赛的公开测试集和私有测试集上分别取得了0.81219和0.81191的mAP@100成绩。

Conclusion: 通过模型集成、多样化损失函数训练和创新的重排序算法，系统显著提升了检索准确率，在竞赛中表现出色，为食品检索领域方法提供了参考。

Abstract: This paper introduces the 3rd place solution to the ICCV LargeFineFoodAI
Retrieval Competition on Kaggle. Four basic models are independently trained
with the weighted sum of ArcFace and Circle loss, then TTA and Ensemble are
successively applied to improve feature representation ability. In addition, a
new reranking method for retrieval is proposed based on diffusion and
k-reciprocal reranking. Finally, our method scored 0.81219 and 0.81191 mAP@100
on the public and private leaderboard, respectively.

</details>


### [27] [3rd Place Solution to Large-scale Fine-grained Food Recognition](https://arxiv.org/abs/2510.21199)
*Yang Zhong,Yifan Yao,Tong Luo,Youcai Zhang,Yaqian Li*

Main category: cs.CV

TL;DR: 本文提出了一种细粒度食物识别方法，通过将Arcface loss和Circle loss结合，并对模型训练进行精细调优及集成，取得了LargeFineFoodAI挑战赛第三名。


<details>
  <summary>Details</summary>
Motivation: 随着健康领域对食物分析的关注增加，细粒度的食物识别成为关键任务。作者旨在提升食物识别的准确率，以应对实际应用中的需求，并挑战国际竞赛以测试方案的有效性。

Method: 作者采用Arcface和Circle loss的组合策略，通过调参优化模型训练过程，并对多个模型进行集成以提升整体表现。

Result: 所提出的方法在LargeFineFoodAI-ICCV Workshop-Recognition竞赛中获得第3名，显示出优越性能。

Conclusion: 结合Arcface和Circle loss，并经过细致的训练与模型集成，可以有效提升细粒度食物识别的准确性，在公开竞赛中获得了验证。

Abstract: Food analysis is becoming a hot topic in health area, in which fine-grained
food recognition task plays an important role. In this paper, we describe the
details of our solution to the LargeFineFoodAI-ICCV Workshop-Recognition
challenge held on Kaggle. We find a proper combination of Arcface loss[1] and
Circle loss[9] can bring improvement to the performance. With Arcface and the
combined loss, model was trained with carefully tuned configurations and
ensembled to get the final results. Our solution won the 3rd place in the
competition.

</details>


### [28] [Improved Training Technique for Shortcut Models](https://arxiv.org/abs/2510.21250)
*Anh Nguyen,Viet Nguyen,Duc Vu,Trung Dao,Chi Tran,Toan Tran,Anh Tran*

Main category: cs.CV

TL;DR: 本论文提出了一种新框架iSM，系统性地解决了捷径模型（shortcut models）面临的五大核心性能瓶颈，使其在生成任务中大幅提升表现。通过引入四项新技术，极大地改善了模型的灵活性、稳定性和生成质量。


<details>
  <summary>Details</summary>
Motivation: 捷径模型能够以单一网络实现一步、少步和多步采样，具备很大潜力，但实际应用受限于五大技术瓶颈（指导累积产生伪影、推理控制不灵活、频率偏置、自一致性冲突、生成路径不稳定）。为推动捷径模型发展，有必要一一突破这些障碍。

Method: 作者提出iSM框架，通过：1）Intrinsic Guidance动态调节指导强度，解决指导累积和控制不灵活；2）Multi-Level Wavelet Loss，多层波レット损失缓解低频偏置，保留高频细节；3）Scaling Optimal Transport（sOT）减少训练方差，使生成路径更平直和稳定；4）Twin EMA策略增强训练稳定性与自一致性。

Result: 在ImageNet 256x256数据集上的大量实验表明，iSM显著改善了一步、少步、多步生成时的FID分数，超越了现有捷径模型的基线表现。

Conclusion: 论文提出的iSM框架有效克服了捷径生成模型的五大瓶颈，使其成为一种可行且具备竞争力的生成模型选择。

Abstract: Shortcut models represent a promising, non-adversarial paradigm for
generative modeling, uniquely supporting one-step, few-step, and multi-step
sampling from a single trained network. However, their widespread adoption has
been stymied by critical performance bottlenecks. This paper tackles the five
core issues that held shortcut models back: (1) the hidden flaw of compounding
guidance, which we are the first to formalize, causing severe image artifacts;
(2) inflexible fixed guidance that restricts inference-time control; (3) a
pervasive frequency bias driven by a reliance on low-level distances in the
direct domain, which biases reconstructions toward low frequencies; (4)
divergent self-consistency arising from a conflict with EMA training; and (5)
curvy flow trajectories that impede convergence. To address these challenges,
we introduce iSM, a unified training framework that systematically resolves
each limitation. Our framework is built on four key improvements: Intrinsic
Guidance provides explicit, dynamic control over guidance strength, resolving
both compounding guidance and inflexibility. A Multi-Level Wavelet Loss
mitigates frequency bias to restore high-frequency details. Scaling Optimal
Transport (sOT) reduces training variance and learns straighter, more stable
generative paths. Finally, a Twin EMA strategy reconciles training stability
with self-consistency. Extensive experiments on ImageNet 256 x 256 demonstrate
that our approach yields substantial FID improvements over baseline shortcut
models across one-step, few-step, and multi-step generation, making shortcut
models a viable and competitive class of generative models.

</details>


### [29] [Topology Sculptor, Shape Refiner: Discrete Diffusion Model for High-Fidelity 3D Meshes Generation](https://arxiv.org/abs/2510.21264)
*Kaiyu Song,Hanjiang Lai,Yaqing Zhang,Chuangjian Cai,Yan Pan Kun Yue,Jian Yin*

Main category: cs.CV

TL;DR: 本论文提出了一种基于离散扩散模型（DDM）的高质量3D网格生成方法TSSR，能高效并行生成艺术风格3D模型，在复杂数据集上实现极高分辨率表现。


<details>
  <summary>Details</summary>
Motivation: 现有3D网格生成方法多为自回归模式，生成速度慢且难以精确控制网格结构。作者希望能提升网格生成的效率和质量，特别是实现并行化和对复杂艺术风格的高保真还原。

Method: 作者提出TSSR方法，包含三个创新：（1）将生成过程分为拓扑雕刻和形状细化两个阶段，分别处理局部拓扑和全局形状，通过解耦训练和混合推理提升生成效果；（2）改进的沙漏网络架构，结合双向注意力和RoPE位置编码，增强对网格结构的上下文感知能力；（3）新颖的连接损失函数，作为拓扑约束提升网格连接的真实感。

Result: 在复杂数据集上实验表明，TSSR能生成高达10,000个面、分辨率为1024^3的高质量3D艺术风格网格，模型效率和表现均优于以往方法。

Conclusion: TSSR在3D网格的并行生成、结构控制和细节保真上表现优越，为3D艺术内容生成提供了新工具，并具备良好的实际应用前景。代码已开源。

Abstract: In this paper, we introduce Topology Sculptor, Shape Refiner (TSSR), a novel
method for generating high-quality, artist-style 3D meshes based on Discrete
Diffusion Models (DDMs). Our primary motivation for TSSR is to achieve highly
accurate token prediction while enabling parallel generation, a significant
advantage over sequential autoregressive methods. By allowing TSSR to "see" all
mesh tokens concurrently, we unlock a new level of efficiency and control. We
leverage this parallel generation capability through three key innovations: 1)
Decoupled Training and Hybrid Inference, which distinctly separates the
DDM-based generation into a topology sculpting stage and a subsequent shape
refinement stage. This strategic decoupling enables TSSR to effectively capture
both intricate local topology and overarching global shape. 2) An Improved
Hourglass Architecture, featuring bidirectional attention enriched by
face-vertex-sequence level Rotational Positional Embeddings (RoPE), thereby
capturing richer contextual information across the mesh structure. 3) A novel
Connection Loss, which acts as a topological constraint to further enhance the
realism and fidelity of the generated meshes. Extensive experiments on complex
datasets demonstrate that TSSR generates high-quality 3D artist-style meshes,
capable of achieving up to 10,000 faces at a remarkable spatial resolution of
$1024^3$. The code will be released at:
https://github.com/psky1111/Tencent-TSSR.

</details>


### [30] [Towards Physically Executable 3D Gaussian for Embodied Navigation](https://arxiv.org/abs/2510.21307)
*Bingchen Miao,Rong Wei,Zhiqi Ge,Xiaoquan sun,Shiqi Gao,Jingzhe Zhu,Renhan Wang,Siliang Tang,Jun Xiao,Rui Tang,Juncheng Li*

Main category: cs.CV

TL;DR: 本文提出了SAGE-3D，一种针对3D高斯渲染（3DGS）环境的语义和物理对齐升级方法，使3DGS既具备可执行性，又拥有细粒度语义和物理交互能力，旨在提升视觉-语言导航（VLN）的实用性和表现。


<details>
  <summary>Details</summary>
Motivation: 传统3DGS在实现照片级实时渲染的同时，缺乏对象级语义和物理可执行性，难以满足视觉-语言导航（VLN）等带有实际行为需求的任务。因此，亟需一种提升3DGS语义和物理能力的新方法。

Method: 作者提出SAGE-3D，包括（1）对象级语义绑定，为3DGS添加细粒度目标标注；（2）物理感知执行拼接，将可碰撞对象和物理接口嵌入3DGS环境。此外，发布了包含1K对象注释场景的InteriorGS数据集和2M数据的SAGE-Bench基准。

Result: 实验证明3DGS场景数据在训练时更难收敛，但具有很强泛化能力，在VLN-CE Unseen任务上的表现提升了31%。

Conclusion: SAGE-3D实现了3DGS环境的语义和物理对齐，极大提升了其在视觉-语言导航任务中的实用性和表现，推动了3D渲染技术到实际智能体任务的转化。

Abstract: 3D Gaussian Splatting (3DGS), a 3D representation method with photorealistic
real-time rendering capabilities, is regarded as an effective tool for
narrowing the sim-to-real gap. However, it lacks fine-grained semantics and
physical executability for Visual-Language Navigation (VLN). To address this,
we propose SAGE-3D (Semantically and Physically Aligned Gaussian Environments
for 3D Navigation), a new paradigm that upgrades 3DGS into an executable,
semantically and physically aligned environment. It comprises two components:
(1) Object-Centric Semantic Grounding, which adds object-level fine-grained
annotations to 3DGS; and (2) Physics-Aware Execution Jointing, which embeds
collision objects into 3DGS and constructs rich physical interfaces. We release
InteriorGS, containing 1K object-annotated 3DGS indoor scene data, and
introduce SAGE-Bench, the first 3DGS-based VLN benchmark with 2M VLN data.
Experiments show that 3DGS scene data is more difficult to converge, while
exhibiting strong generalizability, improving baseline performance by 31% on
the VLN-CE Unseen task. The data and code will be available soon.

</details>


### [31] [FineRS: Fine-grained Reasoning and Segmentation of Small Objects with Reinforcement Learning](https://arxiv.org/abs/2510.21311)
*Lu Zhang,Jiazuo Yu,Haomiao Xiong,Ping Hu,Yunzhi Zhuge,Huchuan Lu,You He*

Main category: cs.CV

TL;DR: 本文提出了一种名为FineRS的多模态大模型强化学习框架，有效提升了高分辨率场景中极小目标的理解与分割能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型受限于输入分辨率，难以在复杂高分辨率场景下准确理解和定位微小目标，严重影响实际应用。

Method: 作者提出了一种由粗到细的两阶段方法（FineRS）：第一阶段通过全局语义探索（GSE）进行指令引导的粗粒度定位；第二阶段进行局部细致感知优化（LPR），输出更精准的边界框与分割掩码。同时引入回顾奖励机制，利用LPR结果反向优化GSE。并发布了FineRS-4k数据集，支持复杂高分辨率场景下的属性推理与像素级分割评测。

Result: 在FineRS-4k及公开数据集上的实验显示，FineRS相较现有多模态大模型在指令引导分割和视觉推理任务上均取得了更优结果。

Conclusion: FineRS方法显著提升了多模态大模型在处理高分辨率复杂场景、极小目标时的细致理解与分割能力，对未来相关应用及研究具有积极意义。

Abstract: Multi-modal Large Language Models (MLLMs) have shown remarkable capabilities
across a wide range of vision-language tasks. However, due to the restricted
input resolutions, MLLMs face significant challenges in precisely understanding
and localizing visual details in high-resolution images -- particularly when
dealing with extra-small objects embedded in cluttered contexts. To address
this issue, we propose \textsc{FineRS}, a two-stage MLLM-based reinforcement
learning framework for jointly reasoning and segmenting extremely small objects
within high-resolution scenes. \textsc{FineRS} adopts a coarse-to-fine pipeline
comprising Global Semantic Exploration (GSE) and Localized Perceptual
Refinement (LPR). Specifically, GSE performs instruction-guided reasoning to
generate a textural response and a coarse target region, while LPR refines this
region to produce an accurate bounding box and segmentation mask. To couple the
two stages, we introduce a locate-informed retrospective reward, where LPR's
outputs are used to optimize GSE for more robust coarse region exploration. %
Additionally, we present \textsc{FineRS}-4k, a new dataset for evaluating MLLMs
on attribute-level reasoning and pixel-level segmentation on subtle,
small-scale targets in complex high-resolution scenes. Experimental results on
\textsc{FineRS}-4k and public datasets demonstrate that our method consistently
outperforms state-of-the-art MLLM-based approaches on both instruction-guided
segmentation and visual reasoning tasks.

</details>


### [32] [VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a Unified Concept Set](https://arxiv.org/abs/2510.21323)
*Shufan Shen,Junshu Sun,Qingming Huang,Shuhui Wang*

Main category: cs.CV

TL;DR: 本文提出了一种稀疏自编码器（VL-SAE），能够将视觉-语言模型中的表征映射为可解释的概念神经元，以提升多模态对齐的可解释性和性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型（VLMs）多模态对齐能力强，但其对齐机制缺乏可解释性，原因在于难以将多模态表征统一映射到一致的概念集。提升对齐可解释性有助于模型理解和下游应用表现。

Method: 1）提出VL-SAE稀疏自编码器，将视觉-语言表征编码为稀疏隐层，每个神经元对应一个语义概念。2）利用自监督训练，促使语义相似的表征具有一致激活。3）采用基于余弦相似度的显式对齐方式度量多模态表征的语义相似性。4）设计距离编码器和特定模态解码器，确保概念级别的对齐和激活一致性。

Result: 在多个主流VLMs（如CLIP、LLaVA）上实验证明，VL-SAE能够提升对齐解释性，同时在零样本分类、幻觉消除等下游任务中性能提升。

Conclusion: 将多模态表征解释为统一概念级别的神经元，有助于理解、加强视觉-语言对齐，并提升实际任务表现。

Abstract: The alignment of vision-language representations endows current
Vision-Language Models (VLMs) with strong multi-modal reasoning capabilities.
However, the interpretability of the alignment component remains uninvestigated
due to the difficulty in mapping the semantics of multi-modal representations
into a unified concept set. To address this problem, we propose VL-SAE, a
sparse autoencoder that encodes vision-language representations into its hidden
activations. Each neuron in its hidden layer correlates to a concept
represented by semantically similar images and texts, thereby interpreting
these representations with a unified concept set. To establish the
neuron-concept correlation, we encourage semantically similar representations
to exhibit consistent neuron activations during self-supervised training.
First, to measure the semantic similarity of multi-modal representations, we
perform their alignment in an explicit form based on cosine similarity. Second,
we construct the VL-SAE with a distance-based encoder and two modality-specific
decoders to ensure the activation consistency of semantically similar
representations. Experiments across multiple VLMs (e.g., CLIP, LLaVA)
demonstrate the superior capability of VL-SAE in interpreting and enhancing the
vision-language alignment. For interpretation, the alignment between vision and
language representations can be understood by comparing their semantics with
concepts. For enhancement, the alignment can be strengthened by aligning
vision-language representations at the concept level, contributing to
performance improvements in downstream tasks, including zero-shot image
classification and hallucination elimination. Codes are available at
https://github.com/ssfgunner/VL-SAE.

</details>


### [33] [Morphologically Intelligent Perturbation Prediction with FORM](https://arxiv.org/abs/2510.21337)
*Reed Naidoo,Matt De Vries,Olga Fourkioti,Vicky Bousgouni,Mar Arias-Garcia,Maria Portillo-Malumbres,Chris Bakal*

Main category: cs.CV

TL;DR: 作者提出了一套名为FORM的机器学习框架，能够预测和生成细胞在不同扰动条件下的三维结构变化，较好地突破了此前仅限二维建模的局限性。


<details>
  <summary>Details</summary>
Motivation: 大部分细胞形态建模只停留在二维层面，无法捕捉复杂的三维细胞结构变化，从而影响对细胞响应外界刺激机制的深入理解和虚拟细胞模型的准确构建。

Method: FORM包括两个核心模块：一是基于多通道VQGAN的形态编码器，能端到端学习细胞三维结构紧凑表征；二是扩散式扰动轨迹模块，建模细胞形态在各种扰动下的动态演变。此外，研究中还建立了大规模（6.5万例）带多通道荧光标记的3D细胞图像数据集，提出了多维形态评估工具MorphoEval，用以评估模型表现。

Result: FORM不仅能无条件生成拟真的细胞三维形态，还能在给定扰动条件下模拟特定细胞状态。同时，FORM可用于预测信号通路活性、模拟组合扰动效应和未见扰动下的形态动态变化。MorphoEval则提供了从结构、统计及生物学等多维度量化扰动带来的形态变化。

Conclusion: FORM显著提升了三维虚拟细胞建模的精度和适用性，实现了形态-扰动-功能的高精度关联预测，有望成为生物医学和药物开发领域的重要工具。

Abstract: Understanding how cells respond to external stimuli is a central challenge in
biomedical research and drug development. Current computational frameworks for
modelling cellular responses remain restricted to two-dimensional
representations, limiting their capacity to capture the complexity of cell
morphology under perturbation. This dimensional constraint poses a critical
bottleneck for the development of accurate virtual cell models. Here, we
present FORM, a machine learning framework for predicting perturbation-induced
changes in three-dimensional cellular structure. FORM consists of two
components: a morphology encoder, trained end-to-end via a novel multi-channel
VQGAN to learn compact 3D representations of cell shape, and a diffusion-based
perturbation trajectory module that captures how morphology evolves across
perturbation conditions. Trained on a large-scale dataset of over 65,000
multi-fluorescence 3D cell volumes spanning diverse chemical and genetic
perturbations, FORM supports both unconditional morphology synthesis and
conditional simulation of perturbed cell states. Beyond generation, FORM can
predict downstream signalling activity, simulate combinatorial perturbation
effects, and model morphodynamic transitions between states of unseen
perturbations. To evaluate performance, we introduce MorphoEval, a benchmarking
suite that quantifies perturbation-induced morphological changes in structural,
statistical, and biological dimensions. Together, FORM and MorphoEval work
toward the realisation of the 3D virtual cell by linking morphology,
perturbation, and function through high-resolution predictive simulation.

</details>


### [34] [CT-CLIP: A Multi-modal Fusion Framework for Robust Apple Leaf Disease Recognition in Complex Environments](https://arxiv.org/abs/2510.21346)
*Lemin Liu,Fangchao Hu,Honghua Jiang,Yaru Chen,Limin Liu,Yongliang Qiao*

Main category: cs.CV

TL;DR: 该论文提出了一种名为CT-CLIP的多分支识别框架，通过结合CNN与Transformer，并引入CLIP实现图文多模态对齐，有效提升了复杂果园环境下苹果叶疾病识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 目前多尺度特征融合方法难以充分融合局部和全局特征，面对果园中不同病斑形态和分布多样性表现不佳，且复杂背景和小样本条件下识别准确率低。

Method: 提出CNN-Transformer-CLIP（CT-CLIP）框架：利用CNN提取局部特征，Vision Transformer捕捉全局结构关系，Adaptive Feature Fusion Module动态融合两者特征；同时采用多模态图文学习方法，利用预训练CLIP权重实现视觉特征与疾病语义描述深度对齐。

Result: 在公开和自建苹果病害数据集上，CT-CLIP识别准确率分别达到97.38%和96.12%，超越多种基线方法。

Conclusion: CT-CLIP在农业病害识别任务中具备强大能力，在复杂环境下显著提升识别准确率，为自动化病害识别提供创新实用的技术方案。

Abstract: In complex orchard environments, the phenotypic heterogeneity of different
apple leaf diseases, characterized by significant variation among lesions,
poses a challenge to traditional multi-scale feature fusion methods. These
methods only integrate multi-layer features extracted by convolutional neural
networks (CNNs) and fail to adequately account for the relationships between
local and global features. Therefore, this study proposes a multi-branch
recognition framework named CNN-Transformer-CLIP (CT-CLIP). The framework
synergistically employs a CNN to extract local lesion detail features and a
Vision Transformer to capture global structural relationships. An Adaptive
Feature Fusion Module (AFFM) then dynamically fuses these features, achieving
optimal coupling of local and global information and effectively addressing the
diversity in lesion morphology and distribution. Additionally, to mitigate
interference from complex backgrounds and significantly enhance recognition
accuracy under few-shot conditions, this study proposes a multimodal image-text
learning approach. By leveraging pre-trained CLIP weights, it achieves deep
alignment between visual features and disease semantic descriptions.
Experimental results show that CT-CLIP achieves accuracies of 97.38% and 96.12%
on a publicly available apple disease and a self-built dataset, outperforming
several baseline methods. The proposed CT-CLIP demonstrates strong capabilities
in recognizing agricultural diseases, significantly enhances identification
accuracy under complex environmental conditions, provides an innovative and
practical solution for automated disease recognition in agricultural
applications.

</details>


### [35] [Dynamic Semantic-Aware Correlation Modeling for UAV Tracking](https://arxiv.org/abs/2510.21351)
*Xinyu Zhou,Tongxin Pan,Lingyi Hong,Pinxue Guo,Haijing Guo,Zhaoyu Chen,Kaixun Jiang,Wenqiang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种动态语义相关性建模的无人机跟踪框架，有效提升了跟踪的准确性和鲁棒性，并兼顾速度和资源消耗，在多个无人机跟踪数据集上取得了有竞争力的效果。


<details>
  <summary>Details</summary>
Motivation: 现有无人机（UAV）跟踪方法偏重速度，缺乏对语义信息的深入利用，导致模板难以为搜索区域提供准确的定位信息，面对相机运动、快速运动、低分辨率等典型难题时效果不佳。

Method: 核心方法是设计了动态语义相关生成器（Dynamic Semantic Relevance Generator），结合Transformer产生的相关性图，更好地挖掘和利用模板与搜索区域之间的语义相关性。同时，通过框架剪枝提升跟踪速度，提出了速度与精度权衡的多种模型变体以适应不同算力需求。

Result: 所提方法在多个无人机跟踪数据集上进行了实验，结果显示该方法在准确性和鲁棒性上都取得了具有竞争力的性能表现。

Conclusion: 该框架能有效兼顾无人机跟踪的准确性与速度，提升了在复杂环境下的表现，并具备灵活部署能力，为无人机目标跟踪领域带来新的解决思路。

Abstract: UAV tracking can be widely applied in scenarios such as disaster rescue,
environmental monitoring, and logistics transportation. However, existing UAV
tracking methods predominantly emphasize speed and lack exploration in semantic
awareness, which hinders the search region from extracting accurate
localization information from the template. The limitation results in
suboptimal performance under typical UAV tracking challenges such as camera
motion, fast motion, and low resolution, etc. To address this issue, we propose
a dynamic semantic aware correlation modeling tracking framework. The core of
our framework is a Dynamic Semantic Relevance Generator, which, in combination
with the correlation map from the Transformer, explore semantic relevance. The
approach enhances the search region's ability to extract important information
from the template, improving accuracy and robustness under the aforementioned
challenges. Additionally, to enhance the tracking speed, we design a pruning
method for the proposed framework. Therefore, we present multiple model
variants that achieve trade-offs between speed and accuracy, enabling flexible
deployment according to the available computational resources. Experimental
results validate the effectiveness of our method, achieving competitive
performance on multiple UAV tracking datasets. The code is available at
https://github.com/zxyyxzz/DSATrack.

</details>


### [36] [Gaze-VLM:Bridging Gaze and VLMs through Attention Regularization for Egocentric Understanding](https://arxiv.org/abs/2510.21356)
*Anupam Pani,Yanchao Yang*

Main category: cs.CV

TL;DR: 本文提出了一种基于凝视正则化的新框架，仅在训练阶段引入人类眼动数据，提升视觉-语言模型（VLM）在第一视角行为理解上的表现，包含未来事件预测和当前活动识别。


<details>
  <summary>Details</summary>
Motivation: 人在第一视角任务中通过眼动表现出对注意力和意图的线索，但现有方法要么忽略眼动信息，要么只将其作为辅助特征，尚未充分利用。作者希望通过更好地利用眼动数据提升VLM对未来行为和当前活动的建模能力。

Method: 该方法在训练阶段引入 gaze-regularized attention mechanism，使模型的注意力机制与人类凝视区域对齐。该机制作为正则项插入，可应用于多种基于注意力的VLM架构，并在推理时不依赖眼动数据。

Result: 实验证明，该方法相较于无凝视正则化的基线模型，在未来事件预测任务中语义预测分数提升最高11个点，在当前活动理解提升约7个点。

Conclusion: 通过训练阶段引入凝视正则化显著提升了第一视角VLM的准确性和鲁棒性，为在真实场景（如辅助机器人和人机协作）中应用人类凝视信息提供了基础。

Abstract: Eye gaze offers valuable cues about attention, short-term intent, and future
actions, making it a powerful signal for modeling egocentric behavior. In this
work, we propose a gaze-regularized framework that enhances VLMs for two key
egocentric understanding tasks: fine-grained future event prediction and
current activity understanding. Unlike prior approaches that rely solely on
visual inputs or use gaze as an auxiliary input signal , our method uses gaze
only during training. We introduce a gaze-regularized attention mechanism that
aligns model focus with human visual gaze. This design is flexible and modular,
allowing it to generalize across multiple VLM architectures that utilize
attention. Experimental results show that our approach improves semantic
prediction scores by up to 11 for future event prediction and around 7 for
current activity understanding, compared to the corresponding baseline models
trained without gaze regularization. These results highlight the value of
gaze-guided training in improving the accuracy and robustness of egocentric
VLMs. Overall, this work establishes a foundation for using human gaze to
enhance the predictive capabilities of VLMs in real-world scenarios like
assistive robots and human-machine collaboration. Code and additional
information is available at: https://github.com/anupampani/Gaze-VLM

</details>


### [37] [Why Registration Quality Matters: Enhancing sCT Synthesis with IMPACT-Based Registration](https://arxiv.org/abs/2510.21358)
*Valentin Boussot,Cédric Hémon,Jean-Claude Nunes,Jean-Louis Dillenseger*

Main category: cs.CV

TL;DR: 本论文提出了一个用于MRI和CBCT到合成CT（sCT）生成的统一处理流程，基于2.5D U-Net++结构，结合了像素级和结构感知损失，重点评估了不同配准方法对结果的影响。


<details>
  <summary>Details</summary>
Motivation: 当前sCT生成任务受到配准误差影响，尤其是在训练和评测过程中，常用的基于互信息的配准方法可能带来评价偏差和结构失真。作者希望通过引入结构感知的配准方法来提高解剖一致性和模型泛化能力。

Method: 方法上采用2.5D U-Net++模型（ResNet-34编码器），跨解剖区域训练并微调，结合像素L1损失与结构感知损失（IMPACT-Synth。基于SAM与TotalSegmentator）以提升结构还原。使用AdamW优化器，基于输入mask和归一化patch的方式训练。比较了基于互信息（Elastix）与结构特征（IMPACT）两种配准策略的影响。

Result: IMPACT配准在本地测试集中实现了更精确和解剖结构一致的对齐，使得sCT合成表现更优（MAE更低、结构更真实）；但在公开验证集上，Elastix配准的模型表现分数更高，反映评测流程对对齐方式的偏向。

Conclusion: 本文指出配准误差会影响深度学习的训练和评估结果，甚至夸大性能。结构感知的IMPACT配准有助于减少此类偏差，推动sCT模型更好泛化和稳健发展。

Abstract: We participated in the SynthRAD2025 challenge (Tasks 1 and 2) with a unified
pipeline for synthetic CT (sCT) generation from MRI and CBCT, implemented using
the KonfAI framework. Our model is a 2.5D U-Net++ with a ResNet-34 encoder,
trained jointly across anatomical regions and fine-tuned per region. The loss
function combined pixel-wise L1 loss with IMPACT-Synth, a perceptual loss
derived from SAM and TotalSegmentator to enhance structural fidelity. Training
was performed using AdamW (initial learning rate = 0.001, halved every 25k
steps) on patch-based, normalized, body-masked inputs (320x320 for MRI, 256x256
for CBCT), with random flipping as the only augmentation. No post-processing
was applied. Final predictions leveraged test-time augmentation and five-fold
ensembling. The best model was selected based on validation MAE. Two
registration strategies were evaluated: (i) Elastix with mutual information,
consistent with the challenge pipeline, and (ii) IMPACT, a feature-based
similarity metric leveraging pretrained segmentation networks. On the local
test sets, IMPACT-based registration achieved more accurate and anatomically
consistent alignments than mutual-information-based registration, resulting in
improved sCT synthesis with lower MAE and more realistic anatomical structures.
On the public validation set, however, models trained with Elastix-aligned data
achieved higher scores, reflecting a registration bias favoring alignment
strategies consistent with the evaluation pipeline. This highlights how
registration errors can propagate into supervised learning, influencing both
training and evaluation, and potentially inflating performance metrics at the
expense of anatomical fidelity. By promoting anatomically consistent alignment,
IMPACT helps mitigate this bias and supports the development of more robust and
generalizable sCT synthesis models.

</details>


### [38] [BADiff: Bandwidth Adaptive Diffusion Model](https://arxiv.org/abs/2510.21366)
*Xi Zhang,Hanwei Zhu,Yan Zhong,Jiamang Wang,Weisi Lin*

Main category: cs.CV

TL;DR: 本文提出了一种面向带宽受限环境的自适应扩散模型框架，实现根据网络带宽动态调整生成图像质量，在保证视觉质量的同时提升传输效率。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型在生成高质量图像时，通常采用固定次数去噪步骤，但在实际云到设备（cloud-to-device）场景下，有限带宽会导致图片需压缩传输，从而造成质量损失及计算资源浪费。

Method: 本文提出端到端联合训练策略，通过将目标质量等级（根据可用带宽推导）作为条件传入扩散模型。模型在训练时自适应调整去噪过程，实现可自行早停的采样机制。具体方法是在去噪轨迹中引入轻量级质量嵌入，无需大幅修改原始结构。

Result: 实验表明，该方法在不同带宽约束下，生成图片的视觉质量明显优于传统早停方法，实现了更高效的带宽自适应图像生成。

Conclusion: 本方法能有效提升带宽受限环境下图像传输效率与质量，为云到设备的高效图像交付提供了可行解决方案。

Abstract: In this work, we propose a novel framework to enable diffusion models to
adapt their generation quality based on real-time network bandwidth
constraints. Traditional diffusion models produce high-fidelity images by
performing a fixed number of denoising steps, regardless of downstream
transmission limitations. However, in practical cloud-to-device scenarios,
limited bandwidth often necessitates heavy compression, leading to loss of fine
textures and wasted computation. To address this, we introduce a joint
end-to-end training strategy where the diffusion model is conditioned on a
target quality level derived from the available bandwidth. During training, the
model learns to adaptively modulate the denoising process, enabling early-stop
sampling that maintains perceptual quality appropriate to the target
transmission condition. Our method requires minimal architectural changes and
leverages a lightweight quality embedding to guide the denoising trajectory.
Experimental results demonstrate that our approach significantly improves the
visual fidelity of bandwidth-adapted generations compared to naive
early-stopping, offering a promising solution for efficient image delivery in
bandwidth-constrained environments. Code is available at:
https://github.com/xzhang9308/BADiff.

</details>


### [39] [TerraGen: A Unified Multi-Task Layout Generation Framework for Remote Sensing Data Augmentation](https://arxiv.org/abs/2510.21391)
*Datao Tang,Hao Wang,Yudeng Xin,Hui Qiao,Dongsheng Jiang,Yin Li,Zhiheng Yu,Xiangyong Cao*

Main category: cs.CV

TL;DR: TerraGen提出了一种统一的遥感影像生成框架，通过对地理空间布局进行编码，实现多任务和灵活的空间可控数据增强，提升了下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 目前的生成式数据增强方法通常仅能服务于单一视觉任务，并且忽略了遥感影像中的地理与空间约束，无法满足多任务与空间一致性需求。针对这些痛点，需要开发一种既能跨多任务，又能考虑地理空间信息的统一生成框架。

Method: 提出了TerraGen框架，引入了地理空间布局编码器，将检测、分割等多种输入方式统一处理，并设计了多尺度注入和掩码加权损失，实现了空间约束的编码。同时，作者构建了首个大规模多任务遥感布局生成数据集（4.5万张图像），并建立了标准评测协议。

Result: TerraGen在多项遥感视觉任务上生成的图像质量优于现有方法。作为通用数据增强生成器时，显著提升了下游检测、分割等任务的性能，并在全数据和小样本场景下显示出良好的跨任务泛化能力。

Conclusion: TerraGen有效弥补了现有方法在多任务和空间可控生成上的不足，可大幅提升遥感数据增广质量和下游任务表现，有望成为遥感视觉领域的数据合成标准方案。

Abstract: Remote sensing vision tasks require extensive labeled data across multiple,
interconnected domains. However, current generative data augmentation
frameworks are task-isolated, i.e., each vision task requires training an
independent generative model, and ignores the modeling of geographical
information and spatial constraints. To address these issues, we propose
\textbf{TerraGen}, a unified layout-to-image generation framework that enables
flexible, spatially controllable synthesis of remote sensing imagery for
various high-level vision tasks, e.g., detection, segmentation, and extraction.
Specifically, TerraGen introduces a geographic-spatial layout encoder that
unifies bounding box and segmentation mask inputs, combined with a multi-scale
injection scheme and mask-weighted loss to explicitly encode spatial
constraints, from global structures to fine details. Also, we construct the
first large-scale multi-task remote sensing layout generation dataset
containing 45k images and establish a standardized evaluation protocol for this
task. Experimental results show that our TerraGen can achieve the best
generation image quality across diverse tasks. Additionally, TerraGen can be
used as a universal data-augmentation generator, enhancing downstream task
performance significantly and demonstrating robust cross-task generalisation in
both full-data and few-shot scenarios.

</details>


### [40] [Depth-Supervised Fusion Network for Seamless-Free Image Stitching](https://arxiv.org/abs/2510.21396)
*Zhiying Jiang,Ruhao Yan,Zengxi Zhang,Bowei Zhang,Jinyuan Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度一致性约束的无缝图像拼接方法，通过引入多阶段机制和全局深度正则化提升多视角图像中的对齐精度，有效缓解由视差带来的重影和错位问题。


<details>
  <summary>Details</summary>
Motivation: 传统图像拼接方法在面对物体深度变化较大的场景时，容易出现重影和错位，这是由于视差导致多视角图像难以精确对齐。为解决这一问题，研究人员亟需提出新的拼接策略，实现自然且无缝的拼接效果。

Method: 方法上，作者提出多阶段对齐机制并结合全局深度正则化，对不同深度范围的目标进行精确对齐；同时采用基于图的低代价最优拼接缝计算，并通过软缝区域扩散精准定位过渡区域，显著减少视差引发的错位；另外，引入重参数化策略优化结构设计，在保证性能的基础上大幅提升算法效率。

Result: 大量实验表明，该方法在对齐精度、无缝性以及算法效率方面均优于现有主流方法，取得了更自然的拼接效果。

Conclusion: 本文方法有效解决了大视差条件下多视角图像拼接中的对齐与无缝难题，为实际复杂场景的拼接应用提供了高效且准确的新方案。

Abstract: Image stitching synthesizes images captured from multiple perspectives into a
single image with a broader field of view. The significant variations in object
depth often lead to large parallax, resulting in ghosting and misalignment in
the stitched results. To address this, we propose a
depth-consistency-constrained seamless-free image stitching method. First, to
tackle the multi-view alignment difficulties caused by parallax, a multi-stage
mechanism combined with global depth regularization constraints is developed to
enhance the alignment accuracy of the same apparent target across different
depth ranges. Second, during the multi-view image fusion process, an optimal
stitching seam is determined through graph-based low-cost computation, and a
soft-seam region is diffused to precisely locate transition areas, thereby
effectively mitigating alignment errors induced by parallax and achieving
natural and seamless stitching results. Furthermore, considering the
computational overhead in the shift regression process, a reparameterization
strategy is incorporated to optimize the structural design, significantly
improving algorithm efficiency while maintaining optimal performance. Extensive
experiments demonstrate the superior performance of the proposed method against
the existing methods. Code is available at https://github.com/DLUT-YRH/DSFN.

</details>


### [41] [MUVR: A Multi-Modal Untrimmed Video Retrieval Benchmark with Multi-Level Visual Correspondence](https://arxiv.org/abs/2510.21406)
*Yue Feng,Jinwei Hu,Qijia Lu,Jiawei Niu,Li Tan,Shuo Yuan,Ziyi Yan,Yizhen Jia,Qingzhi He,Shiping Ge,Ethan Q. Chen,Wentong Li,Limin Wang,Jie Qin*

Main category: cs.CV

TL;DR: 本文提出了多模态未裁剪视频检索任务和新的基准数据集MUVR，用于提升长视频平台上的检索能力。


<details>
  <summary>Details</summary>
Motivation: 长视频平台内容丰富，传统检索方法难以满足对未裁剪视频及多模态复杂需求的检索，实际应用亟需更实用和细致的检索机制。

Method: 1) 提出MUVR任务和基准，支持多模态（文本、标签、掩码）视频查询，采用一对多检索方式针对未裁剪视频；2) 构建六层次视觉对应标准，精准划分视频类别及检索匹配标准；3) 设计Base、Filter、QA三个评测版本，综合评估模型检索与复排序能力。数据集涵盖53K条B站长视频，1050个多模态查询，84K条配对。

Result: 针对3种前沿视频检索模型、6种基于图像的VLMs和10种MLLMs进行了系统性评测，发现目前方法在处理未裁剪视频和多模态查询、多视频复排序等方面仍有明显不足。

Conclusion: MUVR为多模态、未裁剪视频检索领域提供了新的研究和评测标准，有助于推动相关方法的技术发展。

Abstract: We propose the Multi-modal Untrimmed Video Retrieval task, along with a new
benchmark (MUVR) to advance video retrieval for long-video platforms. MUVR aims
to retrieve untrimmed videos containing relevant segments using multi-modal
queries. It has the following features: 1) Practical retrieval paradigm: MUVR
supports video-centric multi-modal queries, expressing fine-grained retrieval
needs through long text descriptions, video tag prompts, and mask prompts. It
adopts a one-to-many retrieval paradigm and focuses on untrimmed videos,
tailored for long-video platform applications. 2) Multi-level visual
correspondence: To cover common video categories (e.g., news, travel, dance)
and precisely define retrieval matching criteria, we construct multi-level
visual correspondence based on core video content (e.g., news events, travel
locations, dance moves) which users are interested in and want to retrieve. It
covers six levels: copy, event, scene, instance, action, and others. 3)
Comprehensive evaluation criteria: We develop 3 versions of MUVR (i.e., Base,
Filter, QA). MUVR-Base/Filter evaluates retrieval models, while MUVR-QA
assesses MLLMs in a question-answering format. We also propose a Reranking
Score to evaluate the reranking ability of MLLMs. MUVR consists of 53K
untrimmed videos from the video platform Bilibili, with 1,050 multi-modal
queries and 84K matches. Extensive evaluations of 3 state-of-the-art video
retrieval models, 6 image-based VLMs, and 10 MLLMs are conducted. MUVR reveals
the limitations of retrieval methods in processing untrimmed videos and
multi-modal queries, as well as MLLMs in multi-video understanding and
reranking. Our code and benchmark is available at
https://github.com/debby-0527/MUVR.

</details>


### [42] [Bridging the gap to real-world language-grounded visual concept learning](https://arxiv.org/abs/2510.21412)
*Whie Jung,Semin Kim,Junee Kim,Seunghoon Hong*

Main category: cs.CV

TL;DR: 本文提出了一种可扩展的视觉概念学习框架，能从真实场景自适应发现和绑定多样化的视觉语义维度，有效地实现复杂视觉编辑和概念泛化。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言结合的概念学习方法大多只支持少量预定义维度（如颜色、形状），局限于合成数据，难以涵盖真实世界中丰富多变的视觉语义，因此亟需一种能自动适应和扩展到更多语义轴的通用方法。

Method: 提出利用预训练视觉-语言模型和通用提示策略，无需先验知识自适应发现图像相关语义轴；设计通用概念编码器，高效绑定视觉特征且每个新概念无需添加额外参数；引入组合锚定优化目标，实现每个语义轴的独立操控，避免相互干扰。

Result: 在ImageNet、CelebA-HQ和AFHQ等真实数据集上验证框架有效性，实现了针对多样、复杂视觉概念的高质量编辑，表现优于现有视觉概念学习和文本编辑方法，并展现出较强的组合泛化能力。

Conclusion: 该框架能自动从真实数据自适应发现并操控丰富视觉语义维度，具备优越的泛化和编辑能力，为视觉-语言结合的概念学习带来了新的可能。

Abstract: Human intelligence effortlessly interprets visual scenes along a rich
spectrum of semantic dimensions. However, existing approaches to
language-grounded visual concept learning are limited to a few predefined
primitive axes, such as color and shape, and are typically explored in
synthetic datasets. In this work, we propose a scalable framework that
adaptively identifies image-related concept axes and grounds visual concepts
along these axes in real-world scenes. Leveraging a pretrained vision-language
model and our universal prompting strategy, our framework identifies a diverse
image-related axes without any prior knowledge. Our universal concept encoder
adaptively binds visual features to the discovered axes without introducing
additional model parameters for each concept. To ground visual concepts along
the discovered axes, we optimize a compositional anchoring objective, which
ensures that each axis can be independently manipulated without affecting
others. We demonstrate the effectiveness of our framework on subsets of
ImageNet, CelebA-HQ, and AFHQ, showcasing superior editing capabilities across
diverse real-world concepts that are too varied to be manually predefined. Our
method also exhibits strong compositional generalization, outperforming
existing visual concept learning and text-based editing methods. The code is
available at https://github.com/whieya/Language-grounded-VCL.

</details>


### [43] [ArtiLatent: Realistic Articulated 3D Object Generation via Structured Latents](https://arxiv.org/abs/2510.21432)
*Honghua Chen,Yushi Lan,Yongwei Chen,Xingang Pan*

Main category: cs.CV

TL;DR: ArtiLatent提出了一种新的生成式框架，可生成具有精细几何、准确关节动作和真实外观的人造3D物体。该方法在多个数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 当前3D物体生成方法难以同时保证关键部件的精细建模、关节动作的准确描述和视觉外观的真实性。尤其是在处理可动物体（如抽屉、门等）时，动态产生的可见性变化和被遮挡区域的纹理合成尤为困难。

Method: 该方法利用变分自编码器将稀疏体素表示和关节属性（如关节类型、轴、原点、范围、部件类别）嵌入到统一潜空间中，并通过潜空间扩散模型实现多样化且物理合理的采样。为实现逼真的3D重建，引入了关节感知的高斯解码器，能根据关节状态在可见或被遮挡区域分配合适纹理特征。

Result: 在PartNet-Mobility和ACD等家具类数据集上，ArtiLatent在几何一致性和外观保真度方面均优于现有方法，能更真实地再现物体在不同关节动作下的外观变化。

Conclusion: ArtiLatent为可动3D物体的高质量生成与编辑提供了可扩展的解决方案，对推动相关领域的发展具有重要意义。

Abstract: We propose ArtiLatent, a generative framework that synthesizes human-made 3D
objects with fine-grained geometry, accurate articulation, and realistic
appearance. Our approach jointly models part geometry and articulation dynamics
by embedding sparse voxel representations and associated articulation
properties, including joint type, axis, origin, range, and part category, into
a unified latent space via a variational autoencoder. A latent diffusion model
is then trained over this space to enable diverse yet physically plausible
sampling. To reconstruct photorealistic 3D shapes, we introduce an
articulation-aware Gaussian decoder that accounts for articulation-dependent
visibility changes (e.g., revealing the interior of a drawer when opened). By
conditioning appearance decoding on articulation state, our method assigns
plausible texture features to regions that are typically occluded in static
poses, significantly improving visual realism across articulation
configurations. Extensive experiments on furniture-like objects from
PartNet-Mobility and ACD datasets demonstrate that ArtiLatent outperforms
existing approaches in geometric consistency and appearance fidelity. Our
framework provides a scalable solution for articulated 3D object synthesis and
manipulation.

</details>


### [44] [Head Pursuit: Probing Attention Specialization in Multimodal Transformers](https://arxiv.org/abs/2510.21518)
*Lorenzo Basile,Valentino Maiorca,Diego Doimo,Francesco Locatello,Alberto Cazzaniga*

Main category: cs.CV

TL;DR: 本文探讨了文本生成模型（包括语言模型和视觉-语言多模态模型）中单个注意力头的语义或视觉属性专精情况，提出了一种可解释的方法对注意力头进行排序，并可通过编辑极少部分头来有针对性地控制模型输出，实现了在多任务下模型行为的抑制与增强。


<details>
  <summary>Details</summary>
Motivation: 当前大模型尤其是注意力机制的内部工作机制尚未被完全理解，研究单个注意力头的专精性和可编辑性对于模型解释性和可控性提升具有重要意义。

Method: 基于已有的可解释性方法，作者将用最终解码层探查中间激活的做法用信号处理的视角重释，发展出一种原则化分析多个样本并对注意力头与目标概念相关性进行打分的方法，从而选择关键注意力头实施编辑。

Result: 实验发现，同一细粒度目标概念常由特定注意力头表达，并且通过只编辑1%左右、经排序选出的注意力头，即可有效抑制或增强指定概念的输出。这一方法在文本问答、有害内容缓解、图片分类与描述等多模态任务上均得以验证。

Conclusion: 注意力层内蕴含可解释且可控的结构，通过少量注意力头编辑即可高效操控模型输出，为理解和编辑大规模生成式模型提供了简单实用的工具和理论支撑。

Abstract: Language and vision-language models have shown impressive performance across
a wide range of tasks, but their internal mechanisms remain only partly
understood. In this work, we study how individual attention heads in
text-generative models specialize in specific semantic or visual attributes.
Building on an established interpretability method, we reinterpret the practice
of probing intermediate activations with the final decoding layer through the
lens of signal processing. This lets us analyze multiple samples in a
principled way and rank attention heads based on their relevance to target
concepts. Our results show consistent patterns of specialization at the head
level across both unimodal and multimodal transformers. Remarkably, we find
that editing as few as 1% of the heads, selected using our method, can reliably
suppress or enhance targeted concepts in the model output. We validate our
approach on language tasks such as question answering and toxicity mitigation,
as well as vision-language tasks including image classification and captioning.
Our findings highlight an interpretable and controllable structure within
attention layers, offering simple tools for understanding and editing
large-scale generative models.

</details>


### [45] [Anisotropic Pooling for LUT-realizable CNN Image Restoration](https://arxiv.org/abs/2510.21437)
*Xi Zhang,Xiaolin Wu*

Main category: cs.CV

TL;DR: 本文提出了一种改进的表查找（LUT）实现的图像复原CNN方法，通过采用各向异性池化策略，有效提升了恢复效果。


<details>
  <summary>Details</summary>
Motivation: 传统LUT实现的CNN在实现高效和高质量图像复原时面临查找表规模与感受野受限的矛盾，当前主流做法是通过平均池化融合不同方向的查找结果，但平均池化对于各向异性信号结构效果不好，因此需要更灵活的融合策略。

Method: 首先引入广义中值池化以替代平均池化，提升各向异性信息的融合效果；随后进一步通过对每个方向学习数据相关的池化系数，实现自适应的方向加权融合。

Result: 在多个图像复原基准上实验，提出的各向异性池化方法在主观和客观指标上均明显优于现有的LUT型CNN方法。

Conclusion: 各向异性池化能够有效解决LUT-CNN中平均池化不足的问题，提高了图像复原效果，对资源受限应用中的神经网络有较大意义。

Abstract: Table look-up realization of image restoration CNNs has the potential of
achieving competitive image quality while being much faster and resource frugal
than the straightforward CNN implementation. The main technical challenge
facing the LUT-based CNN algorithm designers is to manage the table size
without overly restricting the receptive field. The prevailing strategy is to
reuse the table for small pixel patches of different orientations (apparently
assuming a degree of isotropy) and then fuse the look-up results. The fusion is
currently done by average pooling, which we find being ill suited to
anisotropic signal structures. To alleviate the problem, we investigate and
discuss anisotropic pooling methods to replace naive averaging for improving
the performance of the current LUT-realizable CNN restoration methods. First,
we introduce the method of generalized median pooling which leads to measurable
gains over average pooling. We then extend this idea by learning data-dependent
pooling coefficients for each orientation, so that they can adaptively weigh
the contributions of differently oriented pixel patches. Experimental results
on various restoration benchmarks show that our anisotropic pooling strategy
yields both perceptually and numerically superior results compared to existing
LUT-realizable CNN methods.

</details>


### [46] [OpenHype: Hyperbolic Embeddings for Hierarchical Open-Vocabulary Radiance Fields](https://arxiv.org/abs/2510.21441)
*Lisa Weijler,Sebastian Koch,Fabio Poiesi,Timo Ropinski,Pedro Hermosilla*

Main category: cs.CV

TL;DR: OpenHype提出了一种利用连续超曲面空间来建模3D场景层次结构的方法，无需多次推理或依赖固定的离散层次，提升了3D场景理解的效率与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景层次结构建模方法往往需要多次推理，导致推理时间增加，或者依赖于事先定义的、固定的离散层次结构，缺乏灵活性，难以适应真实世界的复杂多样场景。

Method: 提出OpenHype方法，核心是利用超球几何的连续潜在空间来表达场景的多层次关系，通过在该空间中的测地路径实现对层次结构的平滑遍历，避免了多次渲染或对固定结构的依赖。

Result: OpenHype在标准3D场景理解基准上优于现有方法，表现出更好的效率与自适应能力。

Conclusion: OpenHype能够高效且自适应地建模3D对象和场景的层次结构，相比主流方法同时兼顾推理效率和泛化能力，对场景理解任务有重要意义。

Abstract: Modeling the inherent hierarchical structure of 3D objects and 3D scenes is
highly desirable, as it enables a more holistic understanding of environments
for autonomous agents. Accomplishing this with implicit representations, such
as Neural Radiance Fields, remains an unexplored challenge. Existing methods
that explicitly model hierarchical structures often face significant
limitations: they either require multiple rendering passes to capture
embeddings at different levels of granularity, significantly increasing
inference time, or rely on predefined, closed-set discrete hierarchies that
generalize poorly to the diverse and nuanced structures encountered by agents
in the real world. To address these challenges, we propose OpenHype, a novel
approach that represents scene hierarchies using a continuous hyperbolic latent
space. By leveraging the properties of hyperbolic geometry, OpenHype naturally
encodes multi-scale relationships and enables smooth traversal of hierarchies
through geodesic paths in latent space. Our method outperforms state-of-the-art
approaches on standard benchmarks, demonstrating superior efficiency and
adaptability in 3D scene understanding.

</details>


### [47] [MoniTor: Exploiting Large Language Models with Instruction for Online Video Anomaly Detection](https://arxiv.org/abs/2510.21449)
*Shengtian Yang,Yue Feng,Yingshi Liu,Jingrou Zhang,Jie Qin*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的新颖在线视频异常检测框架MoniTor，通过记忆式在线评分队列提升实时异常检测效果，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大模型推动了离线视频异常检测（VAD）的进步，在线VAD因为实时性与计算复杂度的挑战鲜有突破。该领域缺乏利用大模型、无需训练、能实时高效检测异常的方法。

Method: MoniTor框架在无需训练的情况下，使用预训练视觉-语言大模型（VLM）解析视频流，结合创新的LSTM启发预测机制捕捉时间依赖特征。同时，设计在线记忆评分队列和异常先验，实现对异常分数的动态存储和全局覆盖，引导模型区分正常与异常行为。

Result: 在UCF-Crime与XD-Violence两个大规模数据集上评测，MoniTor显著优于同类方法，并在无需训练的设定下达到了弱监督方法的性能水平。

Conclusion: MoniTor框架有效解决了在线视频异常检测中的实时性与有效性难题，可作为未来实时视频异常检测领域的重要基线。

Abstract: Video Anomaly Detection (VAD) aims to locate unusual activities or behaviors
within videos. Recently, offline VAD has garnered substantial research
attention, which has been invigorated by the progress in large language models
(LLMs) and vision-language models (VLMs), offering the potential for a more
nuanced understanding of anomalies. However, online VAD has seldom received
attention due to real-time constraints and computational intensity. In this
paper, we introduce a novel Memory-based online scoring queue scheme for
Training-free VAD (MoniTor), to address the inherent complexities in online
VAD. Specifically, MoniTor applies a streaming input to VLMs, leveraging the
capabilities of pre-trained large-scale models. To capture temporal
dependencies more effectively, we incorporate a novel prediction mechanism
inspired by Long Short-Term Memory (LSTM) networks. This ensures the model can
effectively model past states and leverage previous predictions to identify
anomalous behaviors. Thereby, it better understands the current frame.
Moreover, we design a scoring queue and an anomaly prior to dynamically store
recent scores and cover all anomalies in the monitoring scenario, providing
guidance for LLMs to distinguish between normal and abnormal behaviors over
time. We evaluate MoniTor on two large datasets (i.e., UCF-Crime and
XD-Violence) containing various surveillance and real-world scenarios. The
results demonstrate that MoniTor outperforms state-of-the-art methods and is
competitive with weakly supervised methods without training. Code is available
at https://github.com/YsTvT/MoniTor.

</details>


### [48] [VidSplice: Towards Coherent Video Inpainting via Explicit Spaced Frame Guidance](https://arxiv.org/abs/2510.21461)
*Ming Xie,Junqiu Yu,Qiaole Dong,Xiangyang Xue,Yanwei Fu*

Main category: cs.CV

TL;DR: 针对当前视频修复方法时常出现时序一致性差、后续帧内容难以控制等问题，提出了VidSplice框架，通过引入间隔帧先验和创新模块，显著提升了空间和时间上的修复质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频修复方法即使在中等遮挡下能取得不错效果，但在严重内容缺失时易出现时空稳定性差、视频后半段难以控制等缺陷，因此需要寻求新的方法来提升修复质量及一致性。

Method: 将视频修复任务拆分为多帧一致的图像修复和遮挡区域动作传播两子任务。提出VidSplice框架，引入间隔帧先验作为时空线索。具体方法包括：(1)设计CoSpliced Module利用首帧传播策略将首帧内容扩散到后续参考帧，提高空间统一性；(2)提出context controller模块，在帧复制后编码连贯先验，并在I2V生成主干中注入修复视频，减小内容失真。

Result: 在多种视频修复场景下进行大量实验，VidSplice方法在前景对齐和动作稳定性方面均优于现有方法，表现出色。

Conclusion: VidSplice不仅提升了视频修复的整体性能，在解决时空一致性和复杂场景恢复上具有优势，为视频修复领域带来更鲁棒的新思路。

Abstract: Recent video inpainting methods often employ image-to-video (I2V) priors to
model temporal consistency across masked frames. While effective in moderate
cases, these methods struggle under severe content degradation and tend to
overlook spatiotemporal stability, resulting in insufficient control over the
latter parts of the video. To address these limitations, we decouple video
inpainting into two sub-tasks: multi-frame consistent image inpainting and
masked area motion propagation. We propose VidSplice, a novel framework that
introduces spaced-frame priors to guide the inpainting process with
spatiotemporal cues. To enhance spatial coherence, we design a CoSpliced Module
to perform first-frame propagation strategy that diffuses the initial frame
content into subsequent reference frames through a splicing mechanism.
Additionally, we introduce a delicate context controller module that encodes
coherent priors after frame duplication and injects the spliced video into the
I2V generative backbone, effectively constraining content distortion during
generation. Extensive evaluations demonstrate that VidSplice achieves
competitive performance across diverse video inpainting scenarios. Moreover,
its design significantly improves both foreground alignment and motion
stability, outperforming existing approaches.

</details>


### [49] [CXR-LanIC: Language-Grounded Interpretable Classifier for Chest X-Ray Diagnosis](https://arxiv.org/abs/2510.21464)
*Yiming Tang,Wenjia Zhong,Rushi Shah,Dianbo Liu*

Main category: cs.CV

TL;DR: 本文提出了CXR-LanIC框架，通过可解释的视觉模式提升深度学习模型在胸部X光诊断中的可解释性，使模型输出能够被临床医生理解和验证，同时保持较高的诊断准确率。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在胸部X光诊断上表现出色，但由于模型为“黑箱”模式，医生难以信任和理解其诊断结果，限制了其临床应用。因此，亟需一种既准确又可解释的AI诊断方法，以增加医生信心并发现潜在模型问题。

Method: 作者开发了一种新的CXR-LanIC框架：先用BiomedCLIP诊断分类器获得医学影像表示，再训练一组稀疏自编码器（transcoder-based sparse autoencoders）对其进行分解，得到大约5000个具有单一含义的可解释视觉模式。这些模式涵盖心脏、肺部等多个类别，且在含有相应放射特征的图像间具有稳定的激活表现，从而为每一次诊断分解出20-50个可验证的模式，并能够生成自然语言解释。

Result: CXR-LanIC在包含五类重要病变的胸片诊断上获得有竞争力的准确度。同时，实现了对诊断预测的透明分解，支持有根据的可视化和语言化解释。

Conclusion: 通过紧密对齐临床任务目标训练、提取直接相关的解释特征，CXR-LanIC证明AI医疗系统可兼具高准确和强可解释性，为安全、可信的临床应用提供了基础。

Abstract: Deep learning models have achieved remarkable accuracy in chest X-ray
diagnosis, yet their widespread clinical adoption remains limited by the
black-box nature of their predictions. Clinicians require transparent,
verifiable explanations to trust automated diagnoses and identify potential
failure modes. We introduce CXR-LanIC (Language-Grounded Interpretable
Classifier for Chest X-rays), a novel framework that addresses this
interpretability challenge through task-aligned pattern discovery. Our approach
trains transcoder-based sparse autoencoders on a BiomedCLIP diagnostic
classifier to decompose medical image representations into interpretable visual
patterns. By training an ensemble of 100 transcoders on multimodal embeddings
from the MIMIC-CXR dataset, we discover approximately 5,000 monosemantic
patterns spanning cardiac, pulmonary, pleural, structural, device, and artifact
categories. Each pattern exhibits consistent activation behavior across images
sharing specific radiological features, enabling transparent attribution where
predictions decompose into 20-50 interpretable patterns with verifiable
activation galleries. CXR-LanIC achieves competitive diagnostic accuracy on
five key findings while providing the foundation for natural language
explanations through planned large multimodal model annotation. Our key
innovation lies in extracting interpretable features from a classifier trained
on specific diagnostic objectives rather than general-purpose embeddings,
ensuring discovered patterns are directly relevant to clinical decision-making,
demonstrating that medical AI systems can be both accurate and interpretable,
supporting safer clinical deployment through transparent, clinically grounded
explanations.

</details>


### [50] [ITC-RWKV: Interactive Tissue-Cell Modeling with Recurrent Key-Value Aggregation for Histopathological Subtyping](https://arxiv.org/abs/2510.21479)
*Yating Huang,Qijun Yang,Lintao Xiang,Hujun Yin*

Main category: cs.CV

TL;DR: 该论文提出了一种新的双流（dual-stream）架构，用于在组织病理图像中融合宏观组织特征和细胞级特征，有效提升了癌症亚型等细粒度分类任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有病理学基础模型虽然能很好地处理宏观组织信息，但对于需要精细分类的任务，如癌症亚型分类，缺乏细胞级信息建模。为了解决这一关键不足，作者希望在模型中引入细胞层面的特征融合和交互机制。

Method: 作者设计了双流架构：一条通路处理宏观组织特征，另一条处理聚合后的细胞级特征。为高效聚合大量细胞信息，提出了“receptance-weighted key-value aggregation”模型（受控加权的键值聚合模型），使用递归式transformer以线性复杂度捕捉细胞间关系。同时，提出了双向组织-细胞交互模块，实现细胞线索与其周围组织环境的相互注意力机制。

Result: 在四个组织病理亚型分类任务基准测试中，该方法显著优于现有模型，证明了聚合细胞级特征和组织-细胞交互对于细粒度病理学计算任务的重要性。

Conclusion: 细胞级特征的有效聚合及其与组织级信息的深度交互，是提升病理图像分类等细粒度任务模型性能的关键。本文提出的新架构为临床病理图像分析提供了更精细、有效的计算方法。

Abstract: Accurate interpretation of histopathological images demands integration of
information across spatial and semantic scales, from nuclear morphology and
cellular textures to global tissue organization and disease-specific patterns.
Although recent foundation models in pathology have shown strong capabilities
in capturing global tissue context, their omission of cell-level feature
modeling remains a key limitation for fine-grained tasks such as cancer subtype
classification. To address this, we propose a dual-stream architecture that
models the interplay between macroscale tissue features and aggregated cellular
representations. To efficiently aggregate information from large cell sets, we
propose a receptance-weighted key-value aggregation model, a recurrent
transformer that captures inter-cell dependencies with linear complexity.
Furthermore, we introduce a bidirectional tissue-cell interaction module to
enable mutual attention between localized cellular cues and their surrounding
tissue environment. Experiments on four histopathological subtype
classification benchmarks show that the proposed method outperforms existing
models, demonstrating the critical role of cell-level aggregation and
tissue-cell interaction in fine-grained computational pathology.

</details>


### [51] [GRAP-MOT: Unsupervised Graph-based Position Weighted Person Multi-camera Multi-object Tracking in a Highly Congested Space](https://arxiv.org/abs/2510.21482)
*Marek Socha,Michał Marczyk,Aleksander Kempski,Michał Cogiel,Paweł Foszner,Radosław Zawiski,Michał Staniszewski*

Main category: cs.CV

TL;DR: GRAP-MOT是一种面向闭环区域多摄像头视角下人员多目标跟踪（MOT）问题的新方法，通过引入图权重和位置估计模块，显著提升了在高密集区域下的多目标跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 在封闭空间多人重叠、遮挡频繁的多摄像头视频监控环境中，现有MOT方法在身份追踪准确性和鲁棒性上存在不足。作者希望解决人员遮挡与辨识难题，提高追踪性能。

Method: 提出GRAP-MOT方法，结合在线身份标签更新、特征提取、轨迹跟踪和社区检测等多元要素，并引入人员位置估计模块，利用图加权方式更好整合多维信息。

Result: GRAP-MOT在自建闭环空间场景和公开高密度数据集上测试，性能优于无位置数据的相关方法。同时通过实验分析认为IDF1优于MOTA作为MOT评价指标。

Conclusion: GRAP-MOT在处理高密集密封环境下的多摄像头人员跟踪任务上显示出较强优势，并验证位置估计和合适评价指标的重要性。代码和数据集已公开，有助于后续相关研究。

Abstract: GRAP-MOT is a new approach for solving the person MOT problem dedicated to
videos of closed areas with overlapping multi-camera views, where person
occlusion frequently occurs. Our novel graph-weighted solution updates a
person's identification label online based on tracks and the person's
characteristic features. To find the best solution, we deeply investigated all
elements of the MOT process, including feature extraction, tracking, and
community search. Furthermore, GRAP-MOT is equipped with a person's position
estimation module, which gives additional key information to the MOT method,
ensuring better results than methods without position data. We tested GRAP-MOT
on recordings acquired in a closed-area model and on publicly available real
datasets that fulfil the requirement of a highly congested space, showing the
superiority of our proposition. Finally, we analyzed existing metrics used to
compare MOT algorithms and concluded that IDF1 is more adequate than MOTA in
such comparisons. We made our code, along with the acquired dataset, publicly
available.

</details>


### [52] [An Automatic Detection Method for Hematoma Features in Placental Abruption Ultrasound Images Based on Few-Shot Learning](https://arxiv.org/abs/2510.21495)
*Xiaoqing Liu,Jitai Han,Hua Yan,Peng Li,Sida Tang,Ying Li,Kaiwen Zhang,Min Yu*

Main category: cs.CV

TL;DR: 本文提出了一种基于小样本学习的改进YOLO模型（EH-YOLOv11n），实现对胎盘超声图像中血肿特征的自动检测，检测准确率达到78%，优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 胎盘早剥诊断依赖医师经验，主观性强且存在一致性差问题，亟需自动化、高准确性的计算机辅助诊断方法以保障母婴安全。

Method: 提出EH-YOLOv11n模型：结合小样本学习、引入小波卷积和坐标卷积增强频率及空间特征提取，融合级联组注意力机制抑制超声伪影和遮挡干扰，提高定位精度。

Result: EH-YOLOv11n在胎盘超声血肿检测任务上的准确率为78%，较YOLOv11n提升2.5%，较YOLOv8提升13.7%；精确-召回曲线、置信度、遮挡场景等方面均有显著优势。

Conclusion: 该模型兼具高准确率与实时性，为胎盘早剥的计算机辅助诊断提供了可靠解决方案，具备重要的临床应用价值。

Abstract: Placental abruption is a severe complication during pregnancy, and its early
accurate diagnosis is crucial for ensuring maternal and fetal safety.
Traditional ultrasound diagnostic methods heavily rely on physician experience,
leading to issues such as subjective bias and diagnostic inconsistencies. This
paper proposes an improved model, EH-YOLOv11n (Enhanced Hemorrhage-YOLOv11n),
based on small-sample learning, aiming to achieve automatic detection of
hematoma features in placental ultrasound images. The model enhances
performance through multidimensional optimization: it integrates wavelet
convolution and coordinate convolution to strengthen frequency and spatial
feature extraction; incorporates a cascaded group attention mechanism to
suppress ultrasound artifacts and occlusion interference, thereby improving
bounding box localization accuracy. Experimental results demonstrate a
detection accuracy of 78%, representing a 2.5% improvement over YOLOv11n and a
13.7% increase over YOLOv8. The model exhibits significant superiority in
precision-recall curves, confidence scores, and occlusion scenarios. Combining
high accuracy with real-time processing, this model provides a reliable
solution for computer-aided diagnosis of placental abruption, holding
significant clinical application value.

</details>


### [53] [GranViT: A Fine-Grained Vision Model With Autoregressive Perception For MLLMs](https://arxiv.org/abs/2510.21501)
*Guanghao Zheng,Bowen Shi,Mingxing Xu,Ruoyu Sun,Peisen Zhao,Zhibo Zhang,Wenrui Dai,Junni Zou,Hongkai Xiong,Xiaopeng Zhang,Qi Tian*

Main category: cs.CV

TL;DR: GranViT提出了一种新型视觉Transformer，通过大规模精细标注数据集提升视觉编码器的细粒度感知能力，实现了在多模态任务（如视觉问答、细粒度识别、OCR等）上的领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉编码器在多模态大模型中多数关注全局图像特征，对细粒度区域分析不足，主要受限于缺乏高质量细粒度标注数据和有效的预训练方法。本文旨在解决细粒度感知能力弱的瓶颈。

Method: 作者构建了Gran-29M大规模细粒度数据集，并提出GranViT视觉Transformer。通过区域级自回归训练，将细粒度视觉特征与大语言模型对齐，采用bounding-box-to-caption和caption-to-bounding-box的回归方法进行预训练和适配，并引入自蒸馏机制，强化区域定位和推理能力。

Result: GranViT在细粒度识别、多模态视觉问答、OCR理解等多个任务和标准数据集上取得了比现有视觉编码器更优的表现，具备更强跨模型迁移能力。

Conclusion: GranViT通过创新的结构与训练范式，有效提升了视觉编码器的细粒度区域感知与表达能力，为多模态大模型视觉理解提供了新基线。

Abstract: Vision encoders are indispensable for allowing impressive performance of
Multi-modal Large Language Models (MLLMs) in vision language tasks such as
visual question answering and reasoning. However, existing vision encoders
focus on global image representations but overlook fine-grained regional
analysis. They are limited in fine grained perception due to the scarcity of
fine grained annotated data and the lack of a fine grained pre-training
paradigm. In this paper, we propose GranViT, a novel Vision Transformer that
integrates fine-grained feature extraction with semantic alignment to Large
Language Models (LLMs) via region level autoregressive training. We first
construct Gran-29M, a dataset comprising 2million natural and OCR images paired
with over 180 million high-quality region-level annotations, to enable large
scale fine grained pretraining. Consequently, we develop a
pretraining-adaptation framework along with a self distillation mechanism to
train fine-grained GranViT on Gran-29M. We sufficiently exploit the
fine-grained annotations from Gran-29M to resort to bounding-box-to-caption
regression to enhance localized visual representation of the vision encoder in
the pretraining and caption-to-bounding-box regression to improve vision
feature utilization and localization for LLM in the adaptation. We further
incorporate a self distillation mechanism that imposes explicit localization
constraints on the vision encoder to strengthen its regional reasoning
capability. Extensive experiments show that GranViT surpasses existing vision
encoders and attains strong transferability to varying LLMs. Remarkably, it
achieves state-of-the-art results on fine-grained recognition, multimodal VQA,
and OCR understanding.

</details>


### [54] [Towards a Golden Classifier-Free Guidance Path via Foresight Fixed Point Iterations](https://arxiv.org/abs/2510.21512)
*Kaibo Wang,Jianda Mao,Tong Wu,Yang Xiang*

Main category: cs.CV

TL;DR: 本论文提出了一种统一的视角，将条件引导（如Classifier-Free Guidance，CFG）视为定点迭代过程，并基于此提出了新的引导方法Foresight Guidance（FSG），在多个数据集和模型上证明了其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的CFG方法理论解释分歧，缺乏统一理解，限制了设计空间，也模糊了关键的设计选择，因此需要一种全新的统一理论视角来推动性能与效率提升。

Method: 作者将条件引导建模为定点迭代问题，指出现有CFG属于一步短区间迭代，效率较低。为此，作者提出FSG方法，在扩散早期采用更多步长更大的迭代，从而解决长区间子问题，提高生成一致性和效率。

Result: 在多个数据集与多种架构下实验，FSG在图像质量和计算效率上均优于最先进的方法。

Conclusion: FSG不仅提升了条件生成的质量和效率，还为扩散模型条件引导设计提供了新的理论视角和自适应设计空间。

Abstract: Classifier-Free Guidance (CFG) is an essential component of text-to-image
diffusion models, and understanding and advancing its operational mechanisms
remains a central focus of research. Existing approaches stem from divergent
theoretical interpretations, thereby limiting the design space and obscuring
key design choices. To address this, we propose a unified perspective that
reframes conditional guidance as fixed point iterations, seeking to identify a
golden path where latents produce consistent outputs under both conditional and
unconditional generation. We demonstrate that CFG and its variants constitute a
special case of single-step short-interval iteration, which is theoretically
proven to exhibit inefficiency. To this end, we introduce Foresight Guidance
(FSG), which prioritizes solving longer-interval subproblems in early diffusion
stages with increased iterations. Extensive experiments across diverse datasets
and model architectures validate the superiority of FSG over state-of-the-art
methods in both image quality and computational efficiency. Our work offers
novel perspectives for conditional guidance and unlocks the potential of
adaptive design.

</details>


### [55] [Foley Control: Aligning a Frozen Latent Text-to-Audio Model to Video](https://arxiv.org/abs/2510.21581)
*Ciara Rowles,Varun Jampani,Simon Donné,Shimon Vainer,Julian Parker,Zach Evans*

Main category: cs.CV

TL;DR: Foley Control方法通过在视频和音频预训练模型间插入精简的跨注意力桥接，实现了高效、可控的视频驱动音效生成，并显著减少了可训练参数。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态视频-音频合成方法需要训练参数庞大，灵活性不足，且难以升级或替换模型。如何以最小代价打通预训练的单模态模型，实现高效、可控的视频引导Foley音效生成是一个亟需解决的问题。

Method: 该方法保持预训练视频（V-JEPA2）和音频（Stable Audio Open DiT T2A）模型的参数冻结，仅在二者间插入一个小型的视频跨注意力桥接模块。该桥接在音频模型的文本跨注意力后插入，使文本提示控制全局语义，视频细化局部时序。并通过池化视频token，降低内存消耗，提升训练稳定性。

Result: 在多个精心设计的视频-音频基准上，Foley Control以远少于现有多模态系统的可训练参数，实现了有竞争力的时序和语义对齐，并兼具提示可控性和模块化便于生产的优点。

Conclusion: Foley Control有效连接了预训练的视频和音频生成模型，兼顾性能、效率和灵活性，为视频到音效的合成任务提供了新范式；桥接设计也有望推广到其他音频模态。

Abstract: Foley Control is a lightweight approach to video-guided Foley that keeps
pretrained single-modality models frozen and learns only a small
cross-attention bridge between them. We connect V-JEPA2 video embeddings to a
frozen Stable Audio Open DiT text-to-audio (T2A) model by inserting compact
video cross-attention after the model's existing text cross-attention, so
prompts set global semantics while video refines timing and local dynamics. The
frozen backbones retain strong marginals (video; audio given text) and the
bridge learns the audio-video dependency needed for synchronization -- without
retraining the audio prior. To cut memory and stabilize training, we pool video
tokens before conditioning. On curated video-audio benchmarks, Foley Control
delivers competitive temporal and semantic alignment with far fewer trainable
parameters than recent multi-modal systems, while preserving prompt-driven
controllability and production-friendly modularity (swap/upgrade encoders or
the T2A backbone without end-to-end retraining). Although we focus on
Video-to-Foley, the same bridge design can potentially extend to other audio
modalities (e.g., speech).

</details>


### [56] [Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image Generation](https://arxiv.org/abs/2510.21583)
*Yifu Luo,Penghui Du,Bo Li,Sinan Du,Tiantian Zhang,Yongzhe Chang,Kai Wu,Kun Gai,Xueqian Wang*

Main category: cs.CV

TL;DR: 本文提出了Chunk-GRPO，一种基于块级别优化的文本到图像生成方法，通过将连续生成步骤分组优化，显著提升了生成质量与偏好对齐。


<details>
  <summary>Details</summary>
Motivation: 现有基于GRPO的文本到图像生成方法存在收益归因不准确以及忽略生成过程中时间动态的问题，因而优化效果受限。

Method: 作者提出将优化粒度从单步提升到“块”级别（即将连续多个步骤作为整体进行优化），并引入可选加权采样策略以进一步提升表现。

Result: 大量实验表明，Chunk-GRPO在偏好对齐和图像质量上均优于以往方法。

Conclusion: 块级别的优化策略显著改善了GRPO方法的生成效果，展示了该策略在T2I任务中的应用前景。

Abstract: Group Relative Policy Optimization (GRPO) has shown strong potential for
flow-matching-based text-to-image (T2I) generation, but it faces two key
limitations: inaccurate advantage attribution, and the neglect of temporal
dynamics of generation. In this work, we argue that shifting the optimization
paradigm from the step level to the chunk level can effectively alleviate these
issues. Building on this idea, we propose Chunk-GRPO, the first chunk-level
GRPO-based approach for T2I generation. The insight is to group consecutive
steps into coherent 'chunk's that capture the intrinsic temporal dynamics of
flow matching, and to optimize policies at the chunk level. In addition, we
introduce an optional weighted sampling strategy to further enhance
performance. Extensive experiments show that ChunkGRPO achieves superior
results in both preference alignment and image quality, highlighting the
promise of chunk-level optimization for GRPO-based methods.

</details>


### [57] [Restore Text First, Enhance Image Later: Two-Stage Scene Text Image Super-Resolution with Glyph Structure Guidance](https://arxiv.org/abs/2510.21590)
*Minxing Luo,Linlong Fan,Wang Qiushi,Ge Wu,Yiyan Luo,Yuhang Yu,Jinwei Chen,Yaxing Wang,Qingnan Fan,Jian Yang*

Main category: cs.CV

TL;DR: TIGER方法通过先恢复文本结构、再提升整体图像质量，打破了生成式超分辨率中图像质量与文本可读性的矛盾，效果优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式超分辨率方法在自然图像提升中表现优异，但往往会损坏图片中的文本，导致图像质量和文本可读性之间存在根本性的权衡问题。

Method: 提出了TIGER（Text-Image Guided supEr-Resolution）两阶段超分辨率框架，采用“先文本、后图像”的思路，先精确还原图片中的文本字形，再以还原的文本信息为引导，对整幅图像进行超分辨率重建，分离了文本结构恢复与图像增强两项任务。并构建了UltraZoom-ST极高倍率场景文本数据集供训练与评测。

Result: 大量实验证明，TIGER在提升文本可读性的同时，也能很好地保持整体图像质量，达到了目前最佳水平。

Conclusion: 通过“先文本、后图像”路线和字形到图像的引导机制，TIGER兼顾了文本与图片质量，为包含文本的图像超分辨率提供了更优解决方案。

Abstract: Current generative super-resolution methods show strong performance on
natural images but distort text, creating a fundamental trade-off between image
quality and textual readability. To address this, we introduce \textbf{TIGER}
(\textbf{T}ext-\textbf{I}mage \textbf{G}uided
sup\textbf{E}r-\textbf{R}esolution), a novel two-stage framework that breaks
this trade-off through a \textit{"text-first, image-later"} paradigm.
\textbf{TIGER} explicitly decouples glyph restoration from image enhancement:
it first reconstructs precise text structures and then uses them to guide
subsequent full-image super-resolution. This glyph-to-image guidance ensures
both high fidelity and visual consistency. To support comprehensive training
and evaluation, we also contribute the \textbf{UltraZoom-ST} (UltraZoom-Scene
Text), the first scene text dataset with extreme zoom (\textbf{$\times$14.29}).
Extensive experiments show that \textbf{TIGER} achieves
\textbf{state-of-the-art} performance, enhancing readability while preserving
overall image quality.

</details>


### [58] [Automated interictal epileptic spike detection from simple and noisy annotations in MEG data](https://arxiv.org/abs/2510.21596)
*Pauline Mouches,Julien Jung,Armand Demasson,Agnès Guinard,Romain Bouet,Rosalie Marchal,Romain Quentin*

Main category: cs.CV

TL;DR: 该论文提出用深度学习方法自动检测MEG（脑磁图）记录中的脑癫痫放电波，减少人工标注的负担并提升检测准确性。


<details>
  <summary>Details</summary>
Motivation: 人工检测MEG癫痫相关生物标志物耗时、易错且一致性仅中等，而现有自动方法在临床中应用有限。因此亟需更加高效、稳健、适应真实临床数据的新方法。

Method: 作者提出两种深度学习模型：基于特征的ANN和卷积神经网络（CNN），在59例患者数据上训练并与SOTA模型对比。此外，采用交互式机器学习方法，通过模型输出辅助提升数据标注质量。

Result: 两种模型在10名留出测试患者上的F1分数分别为0.46（CNN）和0.44（ANN），均优于SOTA模型。交互式机器学习策略显示模型对标注噪声具有鲁棒性。

Conclusion: 简单结构的深度学习模型在处理复杂且标注不完美的医学数据时依然表现出色，交互式机器学习提升了数据标注效率，这些模型为癫痫临床MEG数据自动分析提供了有力工具。

Abstract: In drug-resistant epilepsy, presurgical evaluation of epilepsy can be
considered. Magnetoencephalography (MEG) has been shown to be an effective exam
to inform the localization of the epileptogenic zone through the localization
of interictal epileptic spikes. Manual detection of these pathological
biomarkers remains a fastidious and error-prone task due to the high
dimensionality of MEG recordings, and interrater agreement has been reported to
be only moderate. Current automated methods are unsuitable for clinical
practice, either requiring extensively annotated data or lacking robustness on
non-typical data. In this work, we demonstrate that deep learning models can be
used for detecting interictal spikes in MEG recordings, even when only temporal
and single-expert annotations are available, which represents real-world
clinical practice. We propose two model architectures: a feature-based
artificial neural network (ANN) and a convolutional neural network (CNN),
trained on a database of 59 patients, and evaluated against a state-of-the-art
model to classify short time windows of signal. In addition, we employ an
interactive machine learning strategy to iteratively improve our data
annotation quality using intermediary model outputs. Both proposed models
outperform the state-of-the-art model (F1-scores: CNN=0.46, ANN=0.44) when
tested on 10 holdout test patients. The interactive machine learning strategy
demonstrates that our models are robust to noisy annotations. Overall, results
highlight the robustness of models with simple architectures when analyzing
complex and imperfectly annotated data. Our method of interactive machine
learning offers great potential for faster data annotation, while our models
represent useful and efficient tools for automated interictal spikes detection.

</details>


### [59] [S3OD: Towards Generalizable Salient Object Detection with Synthetic Data](https://arxiv.org/abs/2510.21605)
*Orest Kupyn,Hirokatsu Kataoka,Christian Rupprecht*

Main category: cs.CV

TL;DR: 该论文提出通过大规模合成数据生成和支持模糊性的架构，大幅提升显著性目标检测在不同子任务间的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的显著性目标检测因像素级标注代价高，导致针对不同子任务（如DIS、HR-SOD）需分别训练模型，泛化能力不足。

Method: 提出了一种基于多模态扩散的合成数据生成管道，利用diffusion和DINO-v3特征提取标签，生成S3OD数据集（包含13.9万高分辨率图像）；同时，设计了多掩码解码器以处理预测中的多种合理解释。数据生成过程根据模型表现优先生成困难类别。

Result: 仅用合成数据训练的模型在跨数据集泛化上，错误率减少20-50%；微调后，在DIS和HR-SOD基准上达到最新最优性能。

Conclusion: 合成大规模高质数据和处理多解释性的方法能显著提升显著性检测任务的泛化和整体性能。

Abstract: Salient object detection exemplifies data-bounded tasks where expensive
pixel-precise annotations force separate model training for related subtasks
like DIS and HR-SOD. We present a method that dramatically improves
generalization through large-scale synthetic data generation and
ambiguity-aware architecture. We introduce S3OD, a dataset of over 139,000
high-resolution images created through our multi-modal diffusion pipeline that
extracts labels from diffusion and DINO-v3 features. The iterative generation
framework prioritizes challenging categories based on model performance. We
propose a streamlined multi-mask decoder that naturally handles the inherent
ambiguity in salient object detection by predicting multiple valid
interpretations. Models trained solely on synthetic data achieve 20-50% error
reduction in cross-dataset generalization, while fine-tuned versions reach
state-of-the-art performance across DIS and HR-SOD benchmarks.

</details>


### [60] [Modest-Align: Data-Efficient Alignment for Vision-Language Models](https://arxiv.org/abs/2510.21606)
*Jiaxiang Liu,Yuan Wang,Jiawei Du,Joey Tianyi Zhou,Mingkun Xu,Zuozhu Liu*

Main category: cs.CV

TL;DR: 提出了Modest-Align，一种用于跨模态对齐的轻量级框架，通过引入随机扰动和嵌入平滑，提升低资源环境下的鲁棒性和效率，显著减少对大规模数据和计算资源的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态对齐方法，如CLIP，依赖于大规模高质量图文数据。但在数据稀缺或数据质量不佳时，模型容易过度自信且表现变差，尤其在处理图文配对不明确或相关性弱的场景下更加明显。单对比学习方法（只用单一正样本）会加剧这种过度自信的问题。因此，急需一种既高效又鲁棒的新方法。

Method: 提出Modest-Align框架，包括随机扰动（Random Perturbation）和嵌入平滑（Embedding Smoothing）两种策略。前者通过人为加入噪声模拟不确定性，后者则平滑嵌入空间内的相似性分布，两者相辅相成，共同减少模型对不确定样本的过度自信。

Result: 在多个基准数据集上，Modest-Align在检索任务中优于现有方法。实验显示使用的数据量比CLIP少100倍，GPU时间少600倍，依然能取得有竞争力的效果。

Conclusion: Modest-Align为实际低资源场景的跨模态对齐问题提供了高效、可靠且易于扩展的解决方案，显著降低了对大规模数据和计算资源的依赖。

Abstract: Cross-modal alignment aims to map heterogeneous modalities into a shared
latent space, as exemplified by models like CLIP, which benefit from
large-scale image-text pretraining for strong recognition capabilities.
However, when operating in resource-constrained settings with limited or
low-quality data, these models often suffer from overconfidence and degraded
performance due to the prevalence of ambiguous or weakly correlated image-text
pairs. Current contrastive learning approaches, which rely on single positive
pairs, further exacerbate this issue by reinforcing overconfidence on uncertain
samples. To address these challenges, we propose Modest-Align, a lightweight
alignment framework designed for robustness and efficiency. Our approach
leverages two complementary strategies -- Random Perturbation, which introduces
controlled noise to simulate uncertainty, and Embedding Smoothing, which
calibrates similarity distributions in the embedding space. These mechanisms
collectively reduce overconfidence and improve performance on noisy or weakly
aligned samples. Extensive experiments across multiple benchmark datasets
demonstrate that Modest-Align outperforms state-of-the-art methods in retrieval
tasks, achieving competitive results with over 100x less training data and 600x
less GPU time than CLIP. Our method offers a practical and scalable solution
for cross-modal alignment in real-world, low-resource scenarios.

</details>


### [61] [Epipolar Geometry Improves Video Generation Models](https://arxiv.org/abs/2510.21615)
*Orest Kupyn,Fabian Manhardt,Federico Tombari,Christian Rupprecht*

Main category: cs.CV

TL;DR: 本文提出利用极线约束优化视频扩散模型，以提升视频生成的三维一致性，解决现有方法中几何不稳定、运动不连贯及视觉伪影等问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成模型尽管取得了显著进展，但在真实感3D场景下仍存在几何一致性差、运动不稳定、视觉伪影等挑战。提升3D一致性对于内容生成与重建等下游任务有重要意义。

Method: 作者将极线几何约束引入视频扩散模型，通过基于偏好的优化方法对模型进行对齐，直接以数学原理约束相机轨迹与几何伪影，实现高效的几何一致性优化，无需端到端可微分操作。

Result: 实验证明，传统几何约束能提供比现代学习指标更稳定的优化信号，提升了对齐质量。同时模型在训练于静态场景后，对动态内容也具有良好泛化能力。

Conclusion: 本文方法结合了数据驱动的深度学习与传统几何计算机视觉，可高效生成空间一致性强且视觉质量高的视频，具备实际应用价值。

Abstract: Video generation models have progressed tremendously through large latent
diffusion transformers trained with rectified flow techniques. Yet these models
still struggle with geometric inconsistencies, unstable motion, and visual
artifacts that break the illusion of realistic 3D scenes. 3D-consistent video
generation could significantly impact numerous downstream applications in
generation and reconstruction tasks. We explore how epipolar geometry
constraints improve modern video diffusion models. Despite massive training
data, these models fail to capture fundamental geometric principles underlying
visual content. We align diffusion models using pairwise epipolar geometry
constraints via preference-based optimization, directly addressing unstable
camera trajectories and geometric artifacts through mathematically principled
geometric enforcement. Our approach efficiently enforces geometric principles
without requiring end-to-end differentiability. Evaluation demonstrates that
classical geometric constraints provide more stable optimization signals than
modern learned metrics, which produce noisy targets that compromise alignment
quality. Training on static scenes with dynamic cameras ensures high-quality
measurements while the model generalizes effectively to diverse dynamic
content. By bridging data-driven deep learning with classical geometric
computer vision, we present a practical method for generating spatially
consistent videos without compromising visual quality.

</details>


### [62] [DAP-MAE: Domain-Adaptive Point Cloud Masked Autoencoder for Effective Cross-Domain Learning](https://arxiv.org/abs/2510.21635)
*Ziqi Gao,Qiufu Li,Linlin Shen*

Main category: cs.CV

TL;DR: 本文提出了一种适用于多领域点云数据的自监督预训练方法DAP-MAE，能够更好地融合不同领域知识，提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 点云数据比2D数据更稀缺，尤其在不同领域之间，数据分布存在较大差异，混合预训练常导致下游任务表现退化，因此需要一种方法有效融合多领域点云数据知识，用于通用点云分析。

Method: 提出DAP-MAE方法，在预训练阶段引入异构域适配器（adaptation mode）整合跨领域信息，在微调阶段切换为融合模式（fusion mode）强化点云特征，同时通过域特征生成器引导特征适应不同下游任务。

Result: DAP-MAE在ScanObjectNN数据集上实现了95.18%的物体分类准确率，在Bosphorus数据集上实现了88.45%的面部表情识别准确率，并在四个不同任务上表现优秀。

Conclusion: DAP-MAE通过自适应整合多领域点云知识，能够显著提升点云分析任务的性能，是一种高效且具有通用性的预训练方法。

Abstract: Compared to 2D data, the scale of point cloud data in different domains
available for training, is quite limited. Researchers have been trying to
combine these data of different domains for masked autoencoder (MAE)
pre-training to leverage such a data scarcity issue. However, the prior
knowledge learned from mixed domains may not align well with the downstream 3D
point cloud analysis tasks, leading to degraded performance. To address such an
issue, we propose the Domain-Adaptive Point Cloud Masked Autoencoder (DAP-MAE),
an MAE pre-training method, to adaptively integrate the knowledge of
cross-domain datasets for general point cloud analysis. In DAP-MAE, we design a
heterogeneous domain adapter that utilizes an adaptation mode during
pre-training, enabling the model to comprehensively learn information from
point clouds across different domains, while employing a fusion mode in the
fine-tuning to enhance point cloud features. Meanwhile, DAP-MAE incorporates a
domain feature generator to guide the adaptation of point cloud features to
various downstream tasks. With only one pre-training, DAP-MAE achieves
excellent performance across four different point cloud analysis tasks,
reaching 95.18% in object classification on ScanObjectNN and 88.45% in facial
expression recognition on Bosphorus.

</details>


### [63] [A Dynamic Knowledge Distillation Method Based on the Gompertz Curve](https://arxiv.org/abs/2510.21649)
*Han Yang,Guangjun Qin*

Main category: cs.CV

TL;DR: 本文提出了Gompertz-CNN，一种融合Gompertz增长模型的动态知识蒸馏框架，实现了更适应学生模型学习阶段的蒸馏效果。在CIFAR-10和CIFAR-100上，取得了比传统方法最高多8%和4%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法无法捕捉学生模型认知能力的变化，导致知识迁移效果不理想。

Method: 提出基于Gompertz增长曲线的动态蒸馏策略，自动调节蒸馏损失权重；用Wasserstein距离衡量特征层差异，并采用梯度匹配对齐师生模型反向传播行为，统一到多损失目标下。

Result: 在CIFAR-10和CIFAR-100数据集上，Gompertz-CNN在多种师生网络组合下准确率分别提升约8%和4%。

Conclusion: 动态蒸馏方案（Gompertz-CNN）能更好适应学生模型的学习过程并提升知识迁移效果，优于传统方法。

Abstract: This paper introduces a novel dynamic knowledge distillation framework,
Gompertz-CNN, which integrates the Gompertz growth model into the training
process to address the limitations of traditional knowledge distillation.
Conventional methods often fail to capture the evolving cognitive capacity of
student models, leading to suboptimal knowledge transfer. To overcome this, we
propose a stage-aware distillation strategy that dynamically adjusts the weight
of distillation loss based on the Gompertz curve, reflecting the student's
learning progression: slow initial growth, rapid mid-phase improvement, and
late-stage saturation. Our framework incorporates Wasserstein distance to
measure feature-level discrepancies and gradient matching to align backward
propagation behaviors between teacher and student models. These components are
unified under a multi-loss objective, where the Gompertz curve modulates the
influence of distillation losses over time. Extensive experiments on CIFAR-10
and CIFAR-100 using various teacher-student architectures (e.g., ResNet50 and
MobileNet_v2) demonstrate that Gompertz-CNN consistently outperforms
traditional distillation methods, achieving up to 8% and 4% accuracy gains on
CIFAR-10 and CIFAR-100, respectively.

</details>


### [64] [Group Inertial Poser: Multi-Person Pose and Global Translation from Sparse Inertial Sensors and Ultra-Wideband Ranging](https://arxiv.org/abs/2510.21654)
*Ying Xue,Jiaxi Jiang,Rayan Armani,Dominik Hollidt,Yi-Chi Liao,Christian Holz*

Main category: cs.CV

TL;DR: 本文提出了一种结合IMU（惯性测量单元）与UWB（超宽带测距）的多人体三维动作捕捉方法，实现了更高精度和鲁棒性的身体姿态与全局轨迹估计。


<details>
  <summary>Details</summary>
Motivation: 仅依赖IMU技术进行动作捕捉，无法准确获得人体之间的相对位置和全局位移，尤其在多人场景下更为突出。视觉方法存在遮挡与环境依赖问题，该研究旨在克服这些限制，实现精准稳定的多人运动捕捉。

Method: 提出Group Inertial Poser方法，利用UWB测距获取传感器间（包括跨不同人体的）的绝对距离，并与IMU数据进行融合，输入结构化状态空间模型，结合时序优化流程，实现3D姿态及位移估计。此外，构建了首个IMU+UWB两人跟踪公开数据集GIP-DB。

Result: 在合成及真实数据上，Group Inertial Poser在精度和鲁棒性方面均优于现有主流方法，尤其在多人运动捕捉应用中表现突出。

Conclusion: IMU与UWB融合是提升多人运动捕捉精度的有效方式，本文方法为实际场景中的高性能动作捕捉提供了技术可能，并推动了该领域公开数据和工具的发展。

Abstract: Tracking human full-body motion using sparse wearable inertial measurement
units (IMUs) overcomes the limitations of occlusion and instrumentation of the
environment inherent in vision-based approaches. However, purely IMU-based
tracking compromises translation estimates and accurate relative positioning
between individuals, as inertial cues are inherently self-referential and
provide no direct spatial reference for others. In this paper, we present a
novel approach for robustly estimating body poses and global translation for
multiple individuals by leveraging the distances between sparse wearable
sensors - both on each individual and across multiple individuals. Our method
Group Inertial Poser estimates these absolute distances between pairs of
sensors from ultra-wideband ranging (UWB) and fuses them with inertial
observations as input into structured state-space models to integrate temporal
motion patterns for precise 3D pose estimation. Our novel two-step optimization
further leverages the estimated distances for accurately tracking people's
global trajectories through the world. We also introduce GIP-DB, the first
IMU+UWB dataset for two-person tracking, which comprises 200 minutes of motion
recordings from 14 participants. In our evaluation, Group Inertial Poser
outperforms previous state-of-the-art methods in accuracy and robustness across
synthetic and real-world data, showing the promise of IMU+UWB-based multi-human
motion capture in the wild. Code, models, dataset:
https://github.com/eth-siplab/GroupInertialPoser

</details>


### [65] [Long-tailed Species Recognition in the NACTI Wildlife Dataset](https://arxiv.org/abs/2510.21657)
*Zehua Liu,Tilo Burghardt*

Main category: cs.CV

TL;DR: 本文系统研究了长尾类别识别方法在野生动物图像数据集上的效果，并提出增强模型，可大幅提升分类准确率，尤以稀有类别表现明显。


<details>
  <summary>Details</summary>
Motivation: NACTI野生动物图像数据集存在典型的长尾类别分布，主类别占比极高，导致现有方法难以识别稀有类别。现实领域识别任务普遍面临此挑战，因此亟需专门的长尾识别方法。

Method: 基于PyTorch Wildlife模型，评估并改进了多种长尾识别（LTR）损失函数与LTR敏感正则化方法，系统实验对比不同LTR调度策略，并在标准与跨域测试集上验证泛化性。

Result: 改进配置在NACTI测试集Top-1准确率从基线95.51%提升至99.40%，优于以往MLWIC2方法（96.8%）；在减少偏差的跨域测试集上准确率达52.55%，优于WCE损失函数（51.20%），有效提升模型的长尾识别与泛化能力。

Conclusion: LTR方法大幅提升长尾类别识别和跨域泛化表现，但在极端稀有类别和严重分布移位时仍有明显短板。作者公开全部实验资源，便于复现和后续研究。

Abstract: As most ''in the wild'' data collections of the natural world, the North
America Camera Trap Images (NACTI) dataset shows severe long-tailed class
imbalance, noting that the largest 'Head' class alone covers >50% of the 3.7M
images in the corpus. Building on the PyTorch Wildlife model, we present a
systematic study of Long-Tail Recognition methodologies for species recognition
on the NACTI dataset covering experiments on various LTR loss functions plus
LTR-sensitive regularisation. Our best configuration achieves 99.40% Top-1
accuracy on our NACTI test data split, substantially improving over a 95.51%
baseline using standard cross-entropy with Adam. This also improves on
previously reported top performance in MLWIC2 at 96.8% albeit using partly
unpublished (potentially different) partitioning, optimiser, and evaluation
protocols. To evaluate domain shifts (e.g. night-time captures, occlusion,
motion-blur) towards other datasets we construct a Reduced-Bias Test set from
the ENA-Detection dataset where our experimentally optimised long-tail enhanced
model achieves leading 52.55% accuracy (up from 51.20% with WCE loss),
demonstrating stronger generalisation capabilities under distribution shift. We
document the consistent improvements of LTR-enhancing scheduler choices in this
NACTI wildlife domain, particularly when in tandem with state-of-the-art LTR
losses. We finally discuss qualitative and quantitative shortcomings that LTR
methods cannot sufficiently address, including catastrophic breakdown for
'Tail' classes under severe domain shift. For maximum reproducibility we
publish all dataset splits, key code, and full network weights.

</details>


### [66] [Self-Supervised Learning of Synapse Types from EM Images](https://arxiv.org/abs/2510.21663)
*Aarav Shetty,Gary B Huang*

Main category: cs.CV

TL;DR: 本文提出了一种无监督方法，根据神经元内相邻突触在EM图像中的相似性对突触进行分类，而不依赖已知的类别标签。


<details>
  <summary>Details</summary>
Motivation: 传统突触分类依赖于有监督学习，需要大量人工标注。为减少对已知类别的依赖，并更贴合生物神经元内突触的本质相似性，提出新的分类方法。

Method: 方法假设同一神经元内的相邻突触在EM图像上更为相似，利用这一假设，无需预先设定类别数量即可对突触进行无监督聚类。

Result: 将本方法应用于果蝇（Drosophila）数据，实现了无需提前定义类别数量的突触自动分类。

Conclusion: 本方法为突触类型的无监督区分提供新思路，并能指导后续数据标注及神经结构分析。

Abstract: Separating synapses into different classes based on their appearance in EM
images has many applications in biology. Examples may include assigning a
neurotransmitter to a particular class, or separating synapses whose strength
can be modulated from those whose strength is fixed. Traditionally, this has
been done in a supervised manner, giving the classification algorithm examples
of the different classes. Here we instead separate synapses into classes based
only on the observation that nearby synapses in the same neuron are likely more
similar than synapses chosen randomly from different cells. We apply our
methodology to data from {\it Drosophila}. Our approach has the advantage that
the number of synapse types does not need to be known in advance. It may also
provide a principled way to select ground-truth that spans the range of synapse
structure.

</details>


### [67] [Foundation Models in Dermatopathology: Skin Tissue Classification](https://arxiv.org/abs/2510.21664)
*Riya Gupta,Yiwei Zong,Dennis H. Murphree*

Main category: cs.CV

TL;DR: 本文评估了两种基础模型（UNI和Virchow2）在皮肤病理全切片图像（WSI）分类任务中的表现，发现Virchow2提取的特征优于UNI，并探讨了模型增强与归一化等方法。


<details>
  <summary>Details</summary>
Motivation: 随着皮肤病理全切片图像（WSI）生成速度的提升，急需高效、自动化的处理和分类方式，以提升诊断效率和准确性。

Method: 对WSI进行切块，分别用UNI和Virchow2模型提取Patch级特征，经均值聚合生成切片级表示，利用多种机器学习方法（逻辑回归、梯度提升树、随机森林）进行分类，并通过多种评价指标对模型进行评估。同时探索数据增强和图像归一化方法提升模型泛化能力。

Result: Virchow2模型提取的特征在大多数分类器下优于UNI，使用Virchow2+逻辑回归获得最高准确率（90%），但与UNI的差异无统计学意义。均值聚合能够为切片级分类构建稳定特征表示。

Conclusion: 基础模型（尤其是Virchow2）在WSI自动分类任务中展现出良好潜力，该方法具备可扩展性和较高诊断效力，为后续切片级表示学习与自动化病理诊断提供了新思路和有力支撑。

Abstract: The rapid generation of whole-slide images (WSIs) in dermatopathology
necessitates automated methods for efficient processing and accurate
classification. This study evaluates the performance of two foundation models,
UNI and Virchow2, as feature extractors for classifying WSIs into three
diagnostic categories: melanocytic, basaloid, and squamous lesions. Patch-level
embeddings were aggregated into slide-level features using a mean-aggregation
strategy and subsequently used to train multiple machine learning classifiers,
including logistic regression, gradient-boosted trees, and random forest
models. Performance was assessed using precision, recall, true positive rate,
false positive rate, and the area under the receiver operating characteristic
curve (AUROC) on the test set. Results demonstrate that patch-level features
extracted using Virchow2 outperformed those extracted via UNI across most
slide-level classifiers, with logistic regression achieving the highest
accuracy (90%) for Virchow2, though the difference was not statistically
significant. The study also explored data augmentation techniques and image
normalization to enhance model robustness and generalizability. The
mean-aggregation approach provided reliable slide-level feature
representations. All experimental results and metrics were tracked and
visualized using WandB.ai, facilitating reproducibility and interpretability.
This research highlights the potential of foundation models for automated WSI
classification, providing a scalable and effective approach for
dermatopathological diagnosis while paving the way for future advancements in
slide-level representation learning.

</details>


### [68] [WorldGrow: Generating Infinite 3D World](https://arxiv.org/abs/2510.21682)
*Sikuang Li,Chen Yang,Jiemin Fang,Taoran Yi,Jia Lu,Jiazhong Cen,Lingxi Xie,Wei Shen,Qi Tian*

Main category: cs.CV

TL;DR: 本文提出了一种名为WorldGrow的分层级3D场景生成方法，能够实现无限扩展、连贯且真实外观的大型3D世界生成，并在几何重建和生成质量上达到当前最佳水平。


<details>
  <summary>Details</summary>
Motivation: 现有3D内容生成方法存在局限：2D提升方法存在跨视角不一致、3D隐式表示难以扩展、基础模型多以对象为中心，难以应用于场景级别的生成。因此，亟需一种既能保证连贯性，又方便扩展的3D场景生成方法。

Method: WorldGrow方法有三大核心：1）通过数据管道选取高质量场景块作为训练材料，获得适合场景生成的3D结构化潜表示；2）设计了3D场景块补全机制，实现基于上下文的场景扩展；3）采用由粗到细的生成策略，兼顾全局布局的合理性和局部几何纹理的真实感。

Result: 在大规模3D-FRONT数据集上，WorldGrow在几何重建任务中取得了当前最优性能，同时独特地支持无限制、逼真且结构一致的场景生成。

Conclusion: WorldGrow不仅在3D场景生成方面表现卓越，还展现了构建大规模虚拟环境和未来世界建模的潜力。

Abstract: We tackle the challenge of generating the infinitely extendable 3D world --
large, continuous environments with coherent geometry and realistic appearance.
Existing methods face key challenges: 2D-lifting approaches suffer from
geometric and appearance inconsistencies across views, 3D implicit
representations are hard to scale up, and current 3D foundation models are
mostly object-centric, limiting their applicability to scene-level generation.
Our key insight is leveraging strong generation priors from pre-trained 3D
models for structured scene block generation. To this end, we propose
WorldGrow, a hierarchical framework for unbounded 3D scene synthesis. Our
method features three core components: (1) a data curation pipeline that
extracts high-quality scene blocks for training, making the 3D structured
latent representations suitable for scene generation; (2) a 3D block inpainting
mechanism that enables context-aware scene extension; and (3) a coarse-to-fine
generation strategy that ensures both global layout plausibility and local
geometric/textural fidelity. Evaluated on the large-scale 3D-FRONT dataset,
WorldGrow achieves SOTA performance in geometry reconstruction, while uniquely
supporting infinite scene generation with photorealistic and structurally
consistent outputs. These results highlight its capability for constructing
large-scale virtual environments and potential for building future world
models.

</details>


### [69] [On Thin Ice: Towards Explainable Conservation Monitoring via Attribution and Perturbations](https://arxiv.org/abs/2510.21689)
*Jiayi Zhou,Günel Aghakishiyeva,Saagar Arya,Julian Dale,James David Poling,Holly R. Houliston,Jamie N. Womble,Gregory D. Larsen,David W. Johnston,Brinnae Bent*

Main category: cs.CV

TL;DR: 本研究利用后置可解释性方法（如HiResCAM、LayerCAM、LIME等）为深度学习物体检测在生态监测中的预测提供解释，提高生态学领域对计算机视觉技术的信任度，并通过实证测试证明解释性能及模型存在的误差来源。


<details>
  <summary>Details</summary>
Motivation: 目前深度学习等计算机视觉方法在生态学中的应用受限，部分原因是模型预测过程不透明，生态学家难以信任其结果，因此需要提升模型的可解释性。

Method: 以美国冰川湾国家公园的航拍图像为例，训练Faster R-CNN模型检测海豹，并采用多种后置解释方法（HiResCAM、LayerCAM、LIME、扰动法）对结果解释，从定位准确性、忠实性和诊断效用三方面评估解释效果。

Result: 解释结果显示模型主要关注海豹躯干及轮廓区域，移除被检测海豹后检测置信度显著下降，支持模型检测为真阳性。同时还发现模型容易将海豹与黑冰或岩石混淆，揭示了系统性误差来源。

Conclusion: 将目标检测与可解释性方法结合，有助于提升深度学习模型在生态监测中的透明度和可审计性，建议通过数据集优化和样本扩充进一步提升模型性能，为生态保护提供可追溯的决策支持工具。

Abstract: Computer vision can accelerate ecological research and conservation
monitoring, yet adoption in ecology lags in part because of a lack of trust in
black-box neural-network-based models. We seek to address this challenge by
applying post-hoc explanations to provide evidence for predictions and document
limitations that are important to field deployment. Using aerial imagery from
Glacier Bay National Park, we train a Faster R-CNN to detect pinnipeds (harbor
seals) and generate explanations via gradient-based class activation mapping
(HiResCAM, LayerCAM), local interpretable model-agnostic explanations (LIME),
and perturbation-based explanations. We assess explanations along three axes
relevant to field use: (i) localization fidelity: whether high-attribution
regions coincide with the animal rather than background context; (ii)
faithfulness: whether deletion/insertion tests produce changes in detector
confidence; and (iii) diagnostic utility: whether explanations reveal
systematic failure modes. Explanations concentrate on seal torsos and contours
rather than surrounding ice/rock, and removal of the seals reduces detection
confidence, providing model-evidence for true positives. The analysis also
uncovers recurrent error sources, including confusion between seals and black
ice and rocks. We translate these findings into actionable next steps for model
development, including more targeted data curation and augmentation. By pairing
object detection with post-hoc explainability, we can move beyond "black-box"
predictions toward auditable, decision-supporting tools for conservation
monitoring.

</details>


### [70] [BachVid: Training-Free Video Generation with Consistent Background and Character](https://arxiv.org/abs/2510.21696)
*Han Yan,Xibin Song,Yifu Wang,Hongdong Li,Pan Ji,Chao Ma*

Main category: cs.CV

TL;DR: BachVid是一种无需训练和参考图像即可实现多视频角色与背景一致性生成的新方法。


<details>
  <summary>Details</summary>
Motivation: 在文本生成视频的任务中，尤其是生成多个视频时，确保角色与背景在各视频间的一致性十分困难。之前的方法通常需借助参考图像或额外训练，且多只关注角色一致性，忽视了背景一致性问题。因此，研究能同时保证前景和背景一致性的简便方法具有重要意义。

Method: 作者分析了扩散Transformer（DiT）的注意力机制和中间特征，发现其在去噪过程中可用于提取前景掩码和匹配点。基于此，方法流程为：首先生成一个身份视频并缓存中间变量，再将这些变量注入新生成的视频的对应位置，实现多个视频在前景和背景上的一致性。整个过程无需任何参考图像和额外训练。

Result: 实验表明，BachVid能在不需要额外训练和参考图像的情况下，稳定生成具有高一致性的多视频结果。

Conclusion: BachVid提出了一种创新且高效的无训练、多视频一致性生成机制，突破了此前方法对参考图像或大量训练的依赖，为consistent T2V生成提供了新途径。

Abstract: Diffusion Transformers (DiTs) have recently driven significant progress in
text-to-video (T2V) generation. However, generating multiple videos with
consistent characters and backgrounds remains a significant challenge. Existing
methods typically rely on reference images or extensive training, and often
only address character consistency, leaving background consistency to
image-to-video models. We introduce BachVid, the first training-free method
that achieves consistent video generation without needing any reference images.
Our approach is based on a systematic analysis of DiT's attention mechanism and
intermediate features, revealing its ability to extract foreground masks and
identify matching points during the denoising process. Our method leverages
this finding by first generating an identity video and caching the intermediate
variables, and then inject these cached variables into corresponding positions
in newly generated videos, ensuring both foreground and background consistency
across multiple videos. Experimental results demonstrate that BachVid achieves
robust consistency in generated videos without requiring additional training,
offering a novel and efficient solution for consistent video generation without
relying on reference images or additional training.

</details>


### [71] [Visual Diffusion Models are Geometric Solvers](https://arxiv.org/abs/2510.21697)
*Nir Goren,Shai Yehezkel,Omer Dahary,Andrey Voynov,Or Patashnik,Daniel Cohen-Or*

Main category: cs.CV

TL;DR: 该论文证明了视觉扩散模型可以作为高效的几何问题求解器，通过像素空间的操作解决几何难题，包括内切正方形问题、Steiner树问题和简单多边形问题。作者用标准视觉扩散模型将高斯噪声逐步转化为有效的、近似精确的几何图像解法。


<details>
  <summary>Details</summary>
Motivation: 动机在于当前利用扩散模型解决几何问题需要专门的网络结构或针对参数化几何的定制调整，缺乏通用性。作者想探索能否用通用的视觉扩散模型，直接处理视觉表现下的几何问题，简化流程并提升泛化能力。

Method: 方法上，将每个几何问题实例转化为图像，使用标准视觉扩散模型训练，实现将噪声变为有效近似解。无需专属几何参数或自定义网络，仅用图像表示就可完成几何推理。

Result: 结果显示该方法在内切正方形问题、Steiner树问题和简单多边形问题上均能生成与精确解高度匹配的近似图像解，显示了该方法的有效性和普适性。

Conclusion: 结论认为，视觉空间中的生成式建模可为求解复杂几何问题提供通用、实用框架，不仅涵盖了论文所研究的问题，也为更广泛的几何难题开启了新型解决思路。

Abstract: In this paper we show that visual diffusion models can serve as effective
geometric solvers: they can directly reason about geometric problems by working
in pixel space. We first demonstrate this on the Inscribed Square Problem, a
long-standing problem in geometry that asks whether every Jordan curve contains
four points forming a square. We then extend the approach to two other
well-known hard geometric problems: the Steiner Tree Problem and the Simple
Polygon Problem.
  Our method treats each problem instance as an image and trains a standard
visual diffusion model that transforms Gaussian noise into an image
representing a valid approximate solution that closely matches the exact one.
The model learns to transform noisy geometric structures into correct
configurations, effectively recasting geometric reasoning as image generation.
  Unlike prior work that necessitates specialized architectures and
domain-specific adaptations when applying diffusion to parametric geometric
representations, we employ a standard visual diffusion model that operates on
the visual representation of the problem. This simplicity highlights a
surprising bridge between generative modeling and geometric problem solving.
Beyond the specific problems studied here, our results point toward a broader
paradigm: operating in image space provides a general and practical framework
for approximating notoriously hard problems, and opens the door to tackling a
far wider class of challenging geometric tasks.

</details>


### [72] [Automated Detection of Visual Attribute Reliance with a Self-Reflective Agent](https://arxiv.org/abs/2510.21704)
*Christy Li,Josep Lopez Camuñas,Jake Thomas Touchet,Jacob Andreas,Agata Lapedriza,Antonio Torralba,Tamar Rott Shaham*

Main category: cs.CV

TL;DR: 本文提出了一种自动化分析视觉模型依赖哪些视觉属性的新方法，通过自反式代理(agent)自动假设并测试模型在识别时依赖哪些特征，并不断自我反思优化假设，最终更准确识别模型潜在的偏差和依赖。


<details>
  <summary>Details</summary>
Motivation: 识别视觉模型预测过程中依赖的视觉属性对于提升模型健壮性、防止过拟合以及避免虚假相关性至关重要。现有方法往往无法自动检测并解释模型的特征依赖关系，缺乏系统化分析工具，因此需要一种新机制来剖析和诊断视觉模型的属性依赖。

Method: 作者构建了一个自反式代理(agent)框架，框架可以自动提出视觉属性依赖性假设、设计实验进行验证，并基于实验结果不断自我反思、调整和优化假设形成与自我评价流程。这一迭代式流程能有效识别视觉模型潜在的特征依赖。

Result: 在设计的涵盖18类别，共130个拥有多样视觉属性依赖的模型基准上评估了方法，结果显示该自反代理在多轮自我反思后性能显著优于无反思的基线方法。此外，方法还可解析真实主流模型（如CLIP、YOLOv8）中的视觉属性依赖。

Conclusion: 该研究展示了自反式自动假设和验证框架能准确检测视觉模型的属性依赖，有助于提升模型可解释性与鲁棒性。该方法为理解和改进复杂视觉系统提供了新思路。

Abstract: When a vision model performs image recognition, which visual attributes drive
its predictions? Detecting unintended reliance on specific visual features is
critical for ensuring model robustness, preventing overfitting, and avoiding
spurious correlations. We introduce an automated framework for detecting such
dependencies in trained vision models. At the core of our method is a
self-reflective agent that systematically generates and tests hypotheses about
visual attributes that a model may rely on. This process is iterative: the
agent refines its hypotheses based on experimental outcomes and uses a
self-evaluation protocol to assess whether its findings accurately explain
model behavior. When inconsistencies arise, the agent self-reflects over its
findings and triggers a new cycle of experimentation. We evaluate our approach
on a novel benchmark of 130 models designed to exhibit diverse visual attribute
dependencies across 18 categories. Our results show that the agent's
performance consistently improves with self-reflection, with a significant
performance increase over non-reflective baselines. We further demonstrate that
the agent identifies real-world visual attribute dependencies in
state-of-the-art models, including CLIP's vision encoder and the YOLOv8 object
detector.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [73] [Shoot First, Ask Questions Later? Building Rational Agents that Explore and Act Like People](https://arxiv.org/abs/2510.20886)
*Gabriel Grand,Valerio Pepe,Jacob Andreas,Joshua B. Tenenbaum*

Main category: cs.CL

TL;DR: 本文针对AI在有限资源条件下进行数据驱动假设和精准猜测时的理性程度展开分析，并提出方法提升大语言模型（LM）代理的信息探索能力。研究通过新设计的人机对话任务和信息论方法，显著提升了模型的表现和效费比。


<details>
  <summary>Details</summary>
Motivation: 许多高风险AI应用需在资源有限下做出高效、理性的信息探索与决策（如科学和诊断）。当前语言模型驱动的代理在此类场景中表现如何，如何显著提升其理性决策能力，这是本文探索的重点。

Method: 1.设计了“协作军舰”任务：一名部分知情者（Captain）需权衡探索（提问）与行动（试探）；一名全知者（Spotter）在信息受限下答题。通过对比人类（N=42）与语言模型代理的表现，分析其不足。2.提出基于贝叶斯实验设计（BED）的蒙特卡洛推断策略，用于指导LM的信息探索行为；分别提升Spotter（答题）和Captain（提问与行动）的策略。

Result: 与人类相比，初始LM代理在语境理解、提问策略和决策效益上有明显不足。BED策略大幅提升Spotter准确率（最高提升14.7个百分点），Captain的信息增益（最高提升0.227 bits，接近理论极限）。综合改进后，F1值提升0.303-0.374，甚至效费比高出高级模型和人类，Llama-4-Scout胜率从8%飞升至82%（对人类）、0%到67%（对GPT-5）。另外该方法在Guess Who?任务上复制了优越表现，准确率提高28.3-42.4个百分点。

Conclusion: 将贝叶斯实验设计等理性信息探索策略引入大模型代理任务，可显著提高其决策效益和通用性，使相对弱小的模型在特定任务上甚至超越人类和前沿模型，具有广泛推广前景。

Abstract: Many high-stakes applications of AI require forming data-driven hypotheses
and making targeted guesses; e.g., in scientific and diagnostic settings. Given
limited resources, to what extent do agents based on language models (LMs) act
rationally? We develop methods to benchmark and enhance agentic
information-seeking, drawing on insights from human behavior. First, we
introduce a strategic decision-oriented dialogue task called Collaborative
Battleship, in which a partially-informed Captain must balance exploration
(asking questions) and action (taking shots), while a fully-informed Spotter
must provide accurate answers under an information bottleneck. Compared to
human players (N=42), we find that LM agents struggle to ground answers in
context, generate informative questions, and select high-value actions. Next,
to address these gaps, we develop novel Monte Carlo inference strategies for
LMs based on principles from Bayesian Experimental Design (BED). For Spotter
agents, our approach boosts accuracy by up to 14.7% absolute over LM-only
baselines; for Captain agents, it raises expected information gain (EIG) by up
to 0.227 bits (94.2% of the achievable noise ceiling). Combined, these
components yield sharper targeting (+0.303-0.374 F1), and enable weaker LMs,
such as Llama-4-Scout, to outperform both humans (8% -> 82% win rate) and
frontier models (0% -> 67% win rate vs. GPT-5) at ~1% of GPT-5's cost. We
replicate these findings on Guess Who? where our methods significantly boost
accuracy (+28.3-42.4 p.p.), demonstrating their general applicability for
building rational information-seeking agents.

</details>


### [74] [Code-enabled language models can outperform reasoning models on diverse tasks](https://arxiv.org/abs/2510.20909)
*Cedegao E. Zhang,Cédric Colas,Gabriel Poesia,Joshua B. Tenenbaum,Jacob Andreas*

Main category: cs.CL

TL;DR: 作者提出了CodeAdapt方法，让现有的大型语言模型（无需再特殊finetune）就能在多个推理任务中达到或超越特定训练的推理模型（RMs），同时更高效。


<details>
  <summary>Details</summary>
Motivation: 尽管用强化学习训练的推理模型表现优异，但其训练和推理的计算和成本很高。作者想探索能否只用标准的指令微调LM，通过新方法发挥同等甚至更强的推理能力。

Method: 提出CodeAdapt方法，将CodeAct框架（在自然语言推理中插入代码执行）与few-shot bootstrap in-context learning相结合，仅用极少量样例即可提升模型能力。通过对四对LM与RM的对比分析，衡量了多项推理任务上的表现和token效率。

Result: 在八项任务中，有三对LM利用CodeAdapt后整体性能超过对应RM（最高提升22.9%），token效率提升达10-81%。在四个模型的六项任务平均表现上也有最高35.7%的提升，并展现了多样化的推理策略。

Conclusion: CodeAdapt方法使LM无需详细finetune即可获得优秀推理能力，且表现优良、训练推理更高效。证明了代码辅助推理和CodeAdapt学习方式的广泛适用性，为今后“模型体内”强化学习奠定了基础。

Abstract: Reasoning models (RMs), language models (LMs) trained with reinforcement
learning to produce long-form natural language reasoning, have been remarkably
successful, but they still require large amounts of computation and data to
train, and can be slow and expensive to run. In this paper, we show that
standard instruct LMs can already be elicited to be strong reasoners at a level
comparable to or even surpassing their corresponding RMs (e.g., DeepSeek V3 vs
R1) without finetuning, across diverse domains from instruction following and
creative generation to mathematical reasoning. This is achieved by CodeAdapt,
our simple recipe that combines the CodeAct framework, where LMs interleave
natural language reasoning with code execution in a multi-step fashion, with
few-shot bootstrap in-context learning from as few as five training problems.
Analyzing four matched pairs of LMs and RMs, we find that CodeAdapt enables
three LMs to outperform the corresponding RMs on average over eight tasks (up
to 22.9%) while being 10-81% more token efficient, and delivers superior
performance on six tasks when averaged over the four models (up to 35.7%).
Furthermore, the code-augmented reasoning traces display rich and varied
problem-solving strategies. Our findings support that (1) CodeAdapt-style
learning and reasoning may be robust and domain general and (2) code-enabled
LMs are cognitively grounded and powerful systems, potentially providing a
strong foundation for in-weight reinforcement learning.

</details>


### [75] [FicSim: A Dataset for Multi-Faceted Semantic Similarity in Long-Form Fiction](https://arxiv.org/abs/2510.20926)
*Natasha Johnson,Amanda Bertsch,Maria-Emil Deal,Emma Strubell*

Main category: cs.CL

TL;DR: 本文提出了FICSIM数据集，专注于评估长篇小说 embedding 相似度，揭示现有模型偏重表层特征。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型逐渐能够处理更长、更复杂的文本，人们开始关注其在计算文学研究中的应用。但目前缺乏适用于文学领域且覆盖长文本细粒度相似性的评测数据集。

Method: 作者构建了FICSIM数据集，包括长篇、近期创作的小说，并在12个相似性维度上依据作者元数据与人文学者验证进行打分。同时，注重作者参与与持续知情同意。作者用该数据集对多种嵌入模型进行评估。

Result: 评测结果显示，现有嵌入模型普遍更关注表层特征，而难以捕捉到对文学分析更有价值的深层语义类别。

Conclusion: FICSIM为长篇文学文本的embedding相似度评价填补空白，但现有模型在文学研究任务上存在明显短板。

Abstract: As language models become capable of processing increasingly long and complex
texts, there has been growing interest in their application within
computational literary studies. However, evaluating the usefulness of these
models for such tasks remains challenging due to the cost of fine-grained
annotation for long-form texts and the data contamination concerns inherent in
using public-domain literature. Current embedding similarity datasets are not
suitable for evaluating literary-domain tasks because of a focus on
coarse-grained similarity and primarily on very short text. We assemble and
release FICSIM, a dataset of long-form, recently written fiction, including
scores along 12 axes of similarity informed by author-produced metadata and
validated by digital humanities scholars. We evaluate a suite of embedding
models on this task, demonstrating a tendency across models to focus on
surface-level features over semantic categories that would be useful for
computational literary studies tasks. Throughout our data-collection process,
we prioritize author agency and rely on continual, informed author consent.

</details>


### [76] [Do LLMs Truly Understand When a Precedent Is Overruled?](https://arxiv.org/abs/2510.20941)
*Li Zhang,Jaromir Savelka,Kevin Ashley*

Main category: cs.CL

TL;DR: 该论文评估当前主流大语言模型在理解长篇法律文档、识别美国最高法院案例推翻关系方面存在时代敏感性、浅层推理和上下文推理失败等局限性，并提出了一个更贴近真实法律任务的长上下文基准。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型虽然具备一定的法律推理能力，但在复杂长篇的真实法律文档理解方面评价不充分。现有评测多为简化合成任务，难以反映真实法律场景中高风险任务的复杂性，因此需要开发更贴合实际的基准来检验模型能力。

Method: 作者以美国最高法院案例的236对推翻关系为数据集，评估主流大语言模型在识别案例推翻关系上的表现，考察模型对长文档的理解和推理能力。

Result: 评估发现当前大模型存在三大不足：（1）对历史案例处理时表现明显下降，存在时代敏感性和时序偏差；（2）模型多依赖浅层逻辑和启发式方法，缺乏深层法律理解；（3）在复杂任务中出现时序关系错误，尽管在简单场景下可以维护基本的时序意识。

Conclusion: 该研究提出并发布了一个反映现实法律任务复杂性的长上下文评测基准，有效填补了现有大模型长文档理解能力评测中的空白，对推动模型在法律领域的实际应用具有重要意义。

Abstract: Large language models (LLMs) with extended context windows show promise for
complex legal reasoning tasks, yet their ability to understand long legal
documents remains insufficiently evaluated. Developing long-context benchmarks
that capture realistic, high-stakes tasks remains a significant challenge in
the field, as most existing evaluations rely on simplified synthetic tasks that
fail to represent the complexity of real-world document understanding.
Overruling relationships are foundational to common-law doctrine and commonly
found in judicial opinions. They provide a focused and important testbed for
long-document legal understanding that closely resembles what legal
professionals actually do. We present an assessment of state-of-the-art LLMs on
identifying overruling relationships from U.S. Supreme Court cases using a
dataset of 236 case pairs. Our evaluation reveals three critical limitations:
(1) era sensitivity -- the models show degraded performance on historical cases
compared to modern ones, revealing fundamental temporal bias in their training;
(2) shallow reasoning -- models rely on shallow logical heuristics rather than
deep legal comprehension; and (3) context-dependent reasoning failures --
models produce temporally impossible relationships in complex open-ended tasks
despite maintaining basic temporal awareness in simple contexts. Our work
contributes a benchmark that addresses the critical gap in realistic
long-context evaluation, providing an environment that mirrors the complexity
and stakes of actual legal reasoning tasks.

</details>


### [77] [Irish-BLiMP: A Linguistic Benchmark for Evaluating Human and Language Model Performance in a Low-Resource Setting](https://arxiv.org/abs/2510.20957)
*Josh McGiff,Khanh-Tung Tran,William Mulcahy,Dáibhidh Ó Luinín,Jake Dalzell,Róisín Ní Bhroin,Adam Burke,Barry O'Sullivan,Hoang D. Nguyen,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: 本文推出了首个用于评估罕见语言爱尔兰语中LLM语言能力的基准数据集Irish-BLiMP，对现有大模型和人类表现进行了对比。人类整体表现优于模型，揭示了模型在爱尔兰语语法上存在明显差距。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型在英语等资源丰富语言上的表现已大幅提升，但在资源稀缺、濒危语言如爱尔兰语的能力评估及提升极为有限。缺乏相关系统测试集导致研究进展缓慢，因此需要构建爱尔兰语的评测框架和数据集。

Method: 团队通过查阅大量语言学文献和语法参考手册，招募流利的爱尔兰语使用者，手工标注和审查了跨11项语言特征、总计1020对最小对立的句子。将这些最小对立对用于测试大模型和人类的爱尔兰语句法知识表现。

Result: 人类参与者在所有语言特征上的平均正确率比模型高16.6%；开源与闭源模型之间差距18.1%。目前表现最好的大模型（gpt-5）准确率为73.5%，远低于人类的90.1%。同时，模型和人类在爱尔兰语语法的不同方面存在各自障碍。

Conclusion: Irish-BLiMP为评估大模型在爱尔兰语等低资源语言的语法能力提供了首个系统基准，对今后推进低资源语言理解和模型改进具有重要研究意义。

Abstract: We present Irish-BLiMP (Irish Benchmark of Linguistic Minimal Pairs), the
first dataset and framework designed for fine-grained evaluation of linguistic
competence in the Irish language, an endangered language. Drawing on a variety
of linguistic literature and grammar reference works, we manually constructed
and reviewed 1020 minimal pairs across a taxonomy of 11 linguistic features,
through a team of fluent Irish speakers. We evaluate both existing Large
Language Models (LLMs) and fluent human participants on their syntactic
knowledge of Irish. Our findings show that humans outperform all models across
all linguistic features, achieving 16.6% higher accuracy on average. Moreover,
a substantial performance gap of 18.1% persists between open- and closed-source
LLMs, with even the strongest model (gpt-5) reaching only 73.5% accuracy
compared to 90.1% by human. Interestingly, human participants and models
struggle on different aspects of Irish grammar, thus highlighting a difference
in representation learned by the models. Overall, Irish-BLiMP provides the
first systematic framework for evaluating the grammatical competence of LLMs in
Irish and offers a valuable benchmark for advancing research on linguistic
understanding in low-resource languages.

</details>


### [78] [Can Confidence Estimates Decide When Chain-of-thought is Necessary for Llms?](https://arxiv.org/abs/2510.21007)
*Samuel Lewis-Lim,Xingwei Tan,Zhixue Zhao,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 本文系统研究了如何通过对模型信心估计来自适应地调用Chain-of-Thought（CoT）推理，从而在提升模型推理准确性的同时降低不必要的token消耗。


<details>
  <summary>Details</summary>
Motivation: 尽管CoT推理能够提升大模型在复杂任务上的表现，但其带来的冗余计算和token消耗在实际应用中是个障碍。同时，现有模型虽然提供了CoT开关或调整手段，但在何时、何种情况下真正需要CoT尚不明确。因此，作者希望研究如何自动判断何时需要CoT推理。

Method: 提出了一种基于信心门控（confidence-gated）的CoT方法。核心做法是：当模型对直接答案信心不足时，才调用CoT推理。作者系统性评估了四种无需额外训练的信心估计方法，用于触发CoT，并与随机触发及理想触发（oracle）做对比。

Result: 实验表明，无需训练的信心估计方法能够减少冗余CoT调用，比随机触发方式更优。但这些信心估计方法在不同数据集和模型上的效果不一致，有时表现优秀，有时则不理想。

Conclusion: 作者指出当前基于信心的CoT触发方法在实用性上仍有挑战，但也展示了该方向的潜力和局限性，为未来更可靠CoT自适应门控方案的研究指明了方向。

Abstract: Chain-of-thought (CoT) prompting has emerged as a common technique for
enhancing the reasoning abilities of large language models (LLMs). While
extended reasoning can boost accuracy on complex tasks, it is often unnecessary
and substantially increases token usage, limiting the practicality of reasoning
models in many scenarios. Recent models, such as GPT-OSS and Qwen3, expose
controls that enable users to adjust the length of CoT or determine whether it
is used at all. Yet, it remains unclear when CoT should be used: on some tasks
it improves performance, while on others it provides little benefit or even
harms performance. We address this challenge with confidence-gated CoT, where a
model invokes reasoning only when confidence in its direct answer is low. To
this end, we present the first systematic study of training-free confidence
estimation methods for CoT gating. Specifically, we evaluate four training-free
confidence estimation methods and compare them to a random baseline and an
oracle that always knows when CoT is needed. Through extensive experiments, we
show that existing training-free confidence measures can reduce redundant CoT
and outperform randomly invoked CoT. However, the utility of individual
confidence measures is inconsistent, varying with both the dataset and the
model, underscoring the difficulty of deploying confidence-gated CoT in
practice. By analysing both strengths and failure modes, our study highlights
the potential and limitations of current methods and paves the way toward more
reliable adaptive gating of CoT.

</details>


### [79] [Input Matters: Evaluating Input Structure's Impact on LLM Summaries of Sports Play-by-Play](https://arxiv.org/abs/2510.21034)
*Barkavi Sundararajan,Somayajulu Sripada,Ehud Reiter*

Main category: cs.CL

TL;DR: 研究了结构化输入对LLM生成NBA比赛摘要时事实错误和幻觉的影响，发现结构化输入能显著降低模型的错误率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在如体育报道等对准确性要求极高的领域应用时，生成内容可能不忠实于输入数据，产生事实错误或幻觉。论文意在系统性量化和分析输入数据结构对模型生成事实错误的影响。

Method: 论文以NBA赛事实时数据（play-by-play）为对象，设置三种输入格式（行结构化、JSON和非结构化），用Llama-3.1-70B和Qwen2.5-72B两个大模型生成摘要，并对180场比赛的摘要共计3,312处事实错误进行人工标注和统计分析。采用双因素重复测量方差分析（ANOVA）和Tukey HSD事后检验对不同输入结构对错误率的影响进行统计学比较。

Result: JSON输入格式使Llama和Qwen模型的错误率分别下降69%和65%；行结构化输入可分别降低54%和51%。输入数据的结构化对错误率的影响占总方差的80%以上，各输入格式间差异显著。

Conclusion: 在精确性要求高的任务中，结构化输入形式（尤其是JSON格式）能大幅降低大模型生成文本的事实错误和幻觉率。

Abstract: A major concern when deploying LLMs in accuracy-critical domains such as
sports reporting is that the generated text may not faithfully reflect the
input data. We quantify how input structure affects hallucinations and other
factual errors in LLM-generated summaries of NBA play-by-play data, across
three formats: row-structured, JSON and unstructured. We manually annotated
3,312 factual errors across 180 game summaries produced by two models,
Llama-3.1-70B and Qwen2.5-72B. Input structure has a strong effect: JSON input
reduces error rates by 69% for Llama and 65% for Qwen compared to unstructured
input, while row-structured input reduces errors by 54% for Llama and 51% for
Qwen. A two-way repeated measures ANOVA shows that input structure accounts for
over 80% of the variance in error rates, with Tukey HSD post hoc tests
confirming statistically significant differences between all input formats.

</details>


### [80] [Reasoning's Razor: Reasoning Improves Accuracy but Can Hurt Recall at Critical Operating Points in Safety and Hallucination Detection](https://arxiv.org/abs/2510.21049)
*Atoosa Chegini,Hamid Kazemi,Garrett Souza,Maria Safi,Yang Song,Samy Bengio,Sinead Williamson,Mehrdad Farajtabar*

Main category: cs.CL

TL;DR: 本文系统研究了大型语言模型（LLM）在需要低误报率（低FPR）精度要求场景下，推理增强（reasoning-augmented）方法的实际表现，发现“推理模式”提升了整体准确率但不适合高精度场景，而传统推理关闭模式则在低FPR下表现更优。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM的推理能力在多项任务中提升了准确率，但这一能力是否适用于以低误报率为目标的高精度分类场景还未明确，尤其在安全性和幻觉检测等实际部署中非常关键。

Method: 作者分别在安全检测和幻觉检测两大任务上，采用模型微调和零样本（zero-shot）设置，比较了有推理增强（Think On）和无推理增强（Think Off）两种模式下，标准LLM和大型推理模型（LRM）的表现，并考察了基于token的评分与自信心表述的优劣。

Result: 推理增强模式在平均准确率上优于无推理模式，但在低误报率（低FPR）严格阈值下反而表现不佳，而无推理模式则在高精度场景主导优势。基于token的评分优于自信心表述。将两种模式进行简单集成可以综合各自优点。

Conclusion: 推理增强对整体精度有益，但不适合精度敏感、容错率低的实际应用场景。选择推理模式应根据实际需求权衡。

Abstract: Reasoning has become a central paradigm for large language models (LLMs),
consistently boosting accuracy across diverse benchmarks. Yet its suitability
for precision-sensitive tasks remains unclear. We present the first systematic
study of reasoning for classification tasks under strict low false positive
rate (FPR) regimes. Our analysis covers two tasks--safety detection and
hallucination detection--evaluated in both fine-tuned and zero-shot settings,
using standard LLMs and Large Reasoning Models (LRMs). Our results reveal a
clear trade-off: Think On (reasoning-augmented) generation improves overall
accuracy, but underperforms at the low-FPR thresholds essential for practical
use. In contrast, Think Off (no reasoning during inference) dominates in these
precision-sensitive regimes, with Think On surpassing only when higher FPRs are
acceptable. In addition, we find token-based scoring substantially outperforms
self-verbalized confidence for precision-sensitive deployments. Finally, a
simple ensemble of the two modes recovers the strengths of each. Taken
together, our findings position reasoning as a double-edged tool: beneficial
for average accuracy, but often ill-suited for applications requiring strict
precision.

</details>


### [81] [Dynamic Retriever for In-Context Knowledge Editing via Policy Optimization](https://arxiv.org/abs/2510.21059)
*Mahmud Wasif Nafee,Maiqi Jiang,Haipeng Chen,Yanfu Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种动态检索器（DR-IKE），用于改进大语言模型的上下文知识编辑，通过动态筛选示例以提升编辑效率和质量。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在事实回溯方面表现良好，但存在知识过时或错误的问题。现有的上下文知识编辑方法依赖静态示例集，存在数量与质量权衡及缺乏自适应问题。本研究旨在解决这些不足，提升知识编辑的灵活性和效果。

Method: 提出DR-IKE框架，主要包括：1）用BERT和REINFORCE训练检索器，根据编辑奖励动态排序示例；2）采用可学习阈值，剪除低价值示例，实现针对不同任务难度的自适应示例集扩缩。该方法无需修改模型权重，兼容黑盒大模型。

Result: 在COUNTERFACT基准上，DR-IKE提升了编辑成功率（最多17.1%），显著降低了延迟（41.6%），同时保持了对不相关查询的准确率。

Conclusion: DR-IKE实现了高效、可扩展且自适应的知识编辑，为黑盒大语言模型的知识修正提供了实用方案。

Abstract: Large language models (LLMs) excel at factual recall yet still propagate
stale or incorrect knowledge. In-context knowledge editing offers a
gradient-free remedy suitable for black-box APIs, but current editors rely on
static demonstration sets chosen by surface-level similarity, leading to two
persistent obstacles: (i) a quantity-quality trade-off, and (ii) lack of
adaptivity to task difficulty. We address these issues by dynamically selecting
supporting demonstrations according to their utility for the edit. We propose
Dynamic Retriever for In-Context Knowledge Editing (DR-IKE), a lightweight
framework that (1) trains a BERT retriever with REINFORCE to rank
demonstrations by editing reward, and (2) employs a learnable threshold to
prune low-value examples, shortening the prompt when the edit is easy and
expanding it when the task is hard. DR-IKE performs editing without modifying
model weights, relying solely on forward passes for compatibility with
black-box LLMs. On the COUNTERFACT benchmark, it improves edit success by up to
17.1%, reduces latency by 41.6%, and preserves accuracy on unrelated queries,
demonstrating scalable and adaptive knowledge editing. The code is available at
https://github.com/mwnafee/DR-IKE .

</details>


### [82] [Bridging Language Gaps with Adaptive RAG: Improving Indonesian Language Question Answering](https://arxiv.org/abs/2510.21068)
*William Christian,Daniel Adamlu,Adrian Yu,Derwin Suhartono*

Main category: cs.CL

TL;DR: 本文提出了一种适应印度尼西亚语的检索增强生成（RAG）问答系统，并利用机器翻译扩充数据，但多检索策略存在显著不一致性，影响了整体评价。


<details>
  <summary>Details</summary>
Motivation: 当前RAG问答系统在英文领域表现卓越，然而对于资源稀缺语言（如印尼语）支持不足。为缩小这一差距，作者尝试将RAG扩展到印尼语问答任务。

Method: 提出自适应RAG系统，通过引入一个分类器判断问题复杂度，据此选择最合适的作答策略。此外，利用机器翻译扩充印尼语数据集，增强模型训练。

Result: 实验结果表明问题复杂度分类器表现可靠，但多重检索作答策略存在较大不一致性，导致整体问答效果下降。

Conclusion: 该研究展现了RAG在资源稀缺语言上的潜力及面临的挑战，为未来改进低资源语言问答系统指明了方向。

Abstract: Question Answering (QA) has seen significant improvements with the
advancement of machine learning models, further studies enhanced this question
answering system by retrieving external information, called Retrieval-Augmented
Generation (RAG) to produce more accurate and informative answers. However,
these state-of-the-art-performance is predominantly in English language. To
address this gap we made an effort of bridging language gaps by incorporating
Adaptive RAG system to Indonesian language. Adaptive RAG system integrates a
classifier whose task is to distinguish the question complexity, which in turn
determines the strategy for answering the question. To overcome the limited
availability of Indonesian language dataset, our study employs machine
translation as data augmentation approach. Experiments show reliable question
complexity classifier; however, we observed significant inconsistencies in
multi-retrieval answering strategy which negatively impacted the overall
evaluation when this strategy was applied. These findings highlight both the
promise and challenges of question answering in low-resource language
suggesting directions for future improvement.

</details>


### [83] [CDrugRed: A Chinese Drug Recommendation Dataset for Discharge Medications in Metabolic Diseases](https://arxiv.org/abs/2510.21084)
*Juntao Li,Haobin Yuan,Ling Luo,Yan Jiang,Fan Wang,Ping Zhang,Huiyi Lv,Jian Wang,Yuanyuan Sun,Hongfei Lin*

Main category: cs.CL

TL;DR: 本研究开发并公开了首个面向代谢疾病出院用药的中文药物推荐数据集CDrugRed，并用多种主流大语言模型进行基准测试，验证其在提升药物推荐系统发展中的作用。


<details>
  <summary>Details</summary>
Motivation: 临床决策中的药物推荐可通过智能系统提升质量和效率，但目前缺乏真实、公开、尤其是非英文的EHR数据集，阻碍了药物推荐系统尤其在中文领域的发展。

Method: 作者构建了包含5894份、3190名患者的去标识化出院用药记录（含人口学、病史、临床过程和出院诊断信息）的中文数据集CDrugRed。并以此数据集对多种当前主流大语言模型进行出院药物推荐任务基准实验，评估其效能。

Result: 基准实验显示，经过有监督微调后模型表现提升，但即使最优模型的F1分数仅为0.5648，Jaccard分数为0.4477，说明任务具相当难度，仍有很大提升空间。

Conclusion: CDrugRed作为首个公开的中文出院用药数据集，揭示临床药物推荐任务的复杂性，为相关系统的迭代发展提供了宝贵、富有挑战的数据资源，促进该领域进步。

Abstract: Intelligent drug recommendation based on Electronic Health Records (EHRs) is
critical for improving for improving the quality and efficiency of clinical
decision-making. By leveraging large-scale patient data, drug recommendation
systems can assist physicians in selecting the most appropriate medications
according to a patient's medical history, diagnoses, laboratory results, and
comorbidities. However, the advancement of such systems is significantly
hampered by the scarcity of publicly available, real-world EHR datasets,
particularly in languages other than English. In this work, we present
CDrugRed, a first publicly available Chinese drug recommendation dataset
focused on discharge medications for metabolic diseases. The dataset includes
5,894 de-identified records from 3,190 patients, containing comprehensive
information such as patient demographics, medical history, clinical course, and
discharge diagnoses. We assess the utility of CDrugRed by benchmarking several
state-of-the-art large language models (LLMs) on the discharge medication
recommendation task. Experimental results show that while supervised
fine-tuning improves model performance, there remains substantial room for
improvement, with the best model achieving the F1 score of 0.5648 and Jaccard
score of 0.4477. This result highlights the complexity of the clinical drug
recommendation task and establishes CDrugRed as a challenging and valuable
resource for developing more robust and accurate drug recommendation systems.
The dataset is publicly available to the research community under the data
usage agreements at https://github.com/DUTIR-BioNLP/CDrugRed.

</details>


### [84] [Self-Rewarding PPO: Aligning Large Language Models with Demonstrations Only](https://arxiv.org/abs/2510.21090)
*Qingru Zhang,Liang Qiu,Ilgee Hong,Zhenghao Xu,Tianyi Liu,Shiyang Li,Rongzhi Zhang,Zheng Li,Lihong Li,Bing Yin,Chao Zhang,Jianshu Chen,Haoming Jiang,Tuo Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种基于自奖励PPO的新型大语言模型（LLM）微调方法，有效提升模型泛化能力，尤其适用于标注数据有限的场景。


<details>
  <summary>Details</summary>
Motivation: 受限于数据时，SFT（有监督微调）容易过拟合且泛化能力差，尤其难以应对领域外任务。需要一种更适合小样本和泛化场景的LLM对齐方法。

Method: 提出了一种将SFT与PPO（近端策略优化）结合的新方法——Self-Rewarding PPO。其核心是利用SFT模型与预训练模型log概率比值作为奖励信号，实现基于策略的自监督微调，无需人工偏好标注。用PPO算法优化该奖励结构，从而得到更好的策略更新。

Result: 实验表明，Self-Rewarding PPO在多项自然语言任务中较传统SFT方法有更强的泛化能力、数据利用效率和鲁棒性，尤其当高质量标注数据稀缺时提升更为明显。

Conclusion: Self-Rewarding PPO可大幅改善LLM的对齐效果，对数据有限或领域迁移场景尤为有效，是对传统SFT的重要提升。

Abstract: Supervised fine-tuning (SFT) has emerged as a crucial method for aligning
large language models (LLMs) with human-annotated demonstrations. However, SFT,
being an off-policy approach similar to behavior cloning, often struggles with
overfitting and poor out-of-domain generalization, especially in limited-data
scenarios. To address these limitations, we propose Self-Rewarding PPO, a novel
fine-tuning method that leverages on-policy techniques to enhance
generalization performance. Our approach combines the strengths of SFT and
proximal policy optimization (PPO) to achieve more effective alignment from
demonstration data. At its core is a reward function designed as the log policy
ratio between the SFT model and the pretrained base model. This function serves
as an implicit reward signal, using the pretrained policy as a baseline and the
SFT policy as a target. By doing so, it enables on-policy fine-tuning without
relying on human preference annotations. The integration of this self-rewarding
mechanism with PPO addresses key limitations of SFT, improving generalization,
data efficiency, and robustness. Our empirical evaluation across a range of
natural language processing tasks demonstrates that Self-Rewarding PPO
consistently outperforms traditional SFT methods. The results highlight the
effectiveness of our approach in aligning LLMs using demonstration data,
particularly in scenarios where high-quality annotated data is scarce.

</details>


### [85] [The Gray Zone of Faithfulness: Taming Ambiguity in Unfaithfulness Detection](https://arxiv.org/abs/2510.21118)
*Qiang Ding,Lvzhou Luo,Yixuan Cao,Ping Luo*

Main category: cs.CL

TL;DR: 本文提出了一个新的用于评测大语言模型生成摘要时忠实性的基准和注释框架，解决现有注释歧义的问题。


<details>
  <summary>Details</summary>
Motivation: 现有针对LLM摘要忠实性的评测存在注释不一致，尤其是涉及可接受外部知识边界模糊，如常识性内容是否属于忠实。缺乏明确的标准影响基准的可靠性。

Method: 提出了一种新的忠实性注释框架，并加入中间类别Out-Dependent，专门用来标注需要外部知识验证的情况。据此构建了新的评测数据集VeriGray，并对主流LLM进行了实验评测。

Result: 实验显示，即使是最强的LLM如GPT-5，在摘要生成中仍有约6%的句子出现幻觉（即内容不真实），约8%的句子属于需要借助外部知识验证的Out-Dependent类别。同时，多种基线方法在该基准下表现一般。

Conclusion: 新的注释框架和VeriGray基准有效揭示了现有模型在忠实性上的不足，外部知识带来的注释歧义问题亟需解决，未来尚有很大提升空间。

Abstract: Ensuring that Large Language Models (LLMs) generate summaries faithful to a
given source document is essential for real-world applications. While prior
research has explored LLM faithfulness, existing benchmarks suffer from
annotation ambiguity, primarily due to the ill-defined boundary of permissible
external knowledge in generated outputs. For instance, common sense is often
incorporated into responses and labeled as "faithful", yet the acceptable
extent of such knowledge remains unspecified, leading to inconsistent
annotations. To address this issue, we propose a novel faithfulness annotation
framework, which introduces an intermediate category, Out-Dependent, to
classify cases where external knowledge is required for verification. Using
this framework, we construct VeriGray (Verification with the Gray Zone) -- a
new unfaithfulness detection benchmark in summarization. Statistics reveal that
even SOTA LLMs, such as GPT-5, exhibit hallucinations ($\sim 6\%$ of sentences)
in summarization tasks. Moreover, a substantial proportion ($\sim 8\%$ on
average of models) of generated sentences fall into the Out-Dependent category,
underscoring the importance of resolving annotation ambiguity in unfaithfulness
detection benchmarks. Experiments demonstrate that our benchmark poses
significant challenges to multiple baseline methods, indicating considerable
room for future improvement.

</details>


### [86] [Large Language Models Meet Text-Attributed Graphs: A Survey of Integration Frameworks and Applications](https://arxiv.org/abs/2510.21131)
*Guangxin Su,Hanchen Wang,Jianwei Wang,Wenjie Zhang,Ying Zhang,Jian Pei*

Main category: cs.CL

TL;DR: 本文综述了大语言模型（LLM）与文本属性图（TAG）结合的最新研究进展，并提出了系统的整合策略和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型虽具备强大的语义理解与生成能力，但其黑盒特性限制了结构化和多跳推理；而文本属性图虽有结构关系和上下文，但语义深度不足。将LLM与TAG结合有助于弥补双方不足。

Method: 本文提出一个新颖的分类体系，从LLM助力TAG和TAG反哺LLM的双向出发，对集成技术进行归纳，包括串行、并行和多模块框架，并总结了TAG预训练、提示生成和高效微调等具体方法。

Result: 通过系统梳理相关方法，整理了多领域应用案例（如推荐系统、生物医学、问答等），对比了不同策略在实际任务中的表现，并汇总了可用数据集和经验性发现。

Conclusion: 本文为LLM与TAG融合的研究提供了系统综述，明确了现有技术瓶颈、开放挑战和未来研究方向。为推进语言-图学习交叉领域发展提供了理论基础和实践指引。

Abstract: Large Language Models (LLMs) have achieved remarkable success in natural
language processing through strong semantic understanding and generation.
However, their black-box nature limits structured and multi-hop reasoning. In
contrast, Text-Attributed Graphs (TAGs) provide explicit relational structures
enriched with textual context, yet often lack semantic depth. Recent research
shows that combining LLMs and TAGs yields complementary benefits: enhancing TAG
representation learning and improving the reasoning and interpretability of
LLMs. This survey provides the first systematic review of LLM--TAG integration
from an orchestration perspective. We introduce a novel taxonomy covering two
fundamental directions: LLM for TAG, where LLMs enrich graph-based tasks, and
TAG for LLM, where structured graphs improve LLM reasoning. We categorize
orchestration strategies into sequential, parallel, and multi-module
frameworks, and discuss advances in TAG-specific pretraining, prompting, and
parameter-efficient fine-tuning. Beyond methodology, we summarize empirical
insights, curate available datasets, and highlight diverse applications across
recommendation systems, biomedical analysis, and knowledge-intensive question
answering. Finally, we outline open challenges and promising research
directions, aiming to guide future work at the intersection of language and
graph learning.

</details>


### [87] [Social Simulations with Large Language Model Risk Utopian Illusion](https://arxiv.org/abs/2510.21180)
*Ning Bian,Xianpei Han,Hongyu Lin,Baolei Wu,Jun Wang*

Main category: cs.CL

TL;DR: 论文系统评估LLMs在社会行为仿真中的表现，发现其行为理想化，不完全反映真实人类社会复杂性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs能模拟人类行为，但其与真实人类社会行为的差异尚不明确，这可能导致社会科学研究误判与应用风险。

Method: 作者提出了一个系统化框架，通过聊天室风格的多智能体对话，结合五种语言维度分析LLMs的社会仿真行为，并对八个主流模型进行实验。

Result: 结果显示LLMs并不能忠实再现人类行为，往往因迎合社会期望而产生过于理想化（乌托邦）的社会互动，表现出社会角色偏见、首因效应和正向偏见。

Conclusion: 应开发更加贴近真实、多样化人类社会行为的LLMs，以改进现有模型在社会科学和实际应用中的可靠性。

Abstract: Reliable simulation of human behavior is essential for explaining,
predicting, and intervening in our society. Recent advances in large language
models (LLMs) have shown promise in emulating human behaviors, interactions,
and decision-making, offering a powerful new lens for social science studies.
However, the extent to which LLMs diverge from authentic human behavior in
social contexts remains underexplored, posing risks of misinterpretation in
scientific studies and unintended consequences in real-world applications.
Here, we introduce a systematic framework for analyzing LLMs' behavior in
social simulation. Our approach simulates multi-agent interactions through
chatroom-style conversations and analyzes them across five linguistic
dimensions, providing a simple yet effective method to examine emergent social
cognitive biases. We conduct extensive experiments involving eight
representative LLMs across three families. Our findings reveal that LLMs do not
faithfully reproduce genuine human behavior but instead reflect overly
idealized versions of it, shaped by the social desirability bias. In
particular, LLMs show social role bias, primacy effect, and positivity bias,
resulting in "Utopian" societies that lack the complexity and variability of
real human interactions. These findings call for more socially grounded LLMs
that capture the diversity of human social behavior.

</details>


### [88] [Estonian Native Large Language Model Benchmark](https://arxiv.org/abs/2510.21193)
*Helena Grete Lillepalu,Tanel Alumäe*

Main category: cs.CL

TL;DR: 本文介绍了一个专门用于评测爱沙尼亚语大型语言模型（LLM）的新基准，涵盖多个领域和能力，并首次对多种不同类型的LLM进行了系统比较。


<details>
  <summary>Details</summary>
Motivation: 当前针对爱沙尼亚语的LLM评测数据集稀缺，业界尚未有全面评估不同LLM在爱沙尼亚语任务性能的工作，因此需要开发相应基准以推动相关研究进展。

Method: 作者基于7个多样化的本地语料数据集（涵盖知识、语法、词汇、摘要、理解等），采用人工与LLM评审双重方式，评价了6个基础模型与26个指令微调模型的表现，并对开源与商用模型进行了对比。

Result: 人类评价分数与基准表现之间在不同任务上表现出中高相关性。以Claude 3.7 Sonnet为代表的先进LLM评审结果与人类高度一致，显示头部LLM可辅助爱沙尼亚语模型演示评测。

Conclusion: 提出的基准覆盖面广且来源权威，验证了LLM可胜任爱沙尼亚语评测任务，促进了在低资源语言下LLM性能测评研究的发展。

Abstract: The availability of LLM benchmarks for the Estonian language is limited, and
a comprehensive evaluation comparing the performance of different LLMs on
Estonian tasks has yet to be conducted. We introduce a new benchmark for
evaluating LLMs in Estonian, based on seven diverse datasets. These datasets
assess general and domain-specific knowledge, understanding of Estonian grammar
and vocabulary, summarization abilities, contextual comprehension, and more.
The datasets are all generated from native Estonian sources without using
machine translation. We compare the performance of base models,
instruction-tuned open-source models, and commercial models. Our evaluation
includes 6 base models and 26 instruction-tuned models. To assess the results,
we employ both human evaluation and LLM-as-a-judge methods. Human evaluation
scores showed moderate to high correlation with benchmark evaluations,
depending on the dataset. Claude 3.7 Sonnet, used as an LLM judge, demonstrated
strong alignment with human ratings, indicating that top-performing LLMs can
effectively support the evaluation of Estonian-language models.

</details>


### [89] [The "Right" Discourse on Migration: Analysing Migration-Related Tweets in Right and Far-Right Political Movements](https://arxiv.org/abs/2510.21220)
*Nishan Chatterjee,Veronika Bajt,Ana Zwitter Vitez,Senja Pollak*

Main category: cs.CL

TL;DR: 本文结合自然语言处理和社会学方法，分析极右推特上的移民和仇恨言论，揭示了极右翼如何在社交平台传播极端思想及其政治影响。


<details>
  <summary>Details</summary>
Motivation: 极右翼民粹主义在欧洲崛起，社交媒体成为其传播极端意识形态的重要渠道。分析这些言论有助于理解其如何影响政治结果。

Method: 研究提出结合最前沿的自然语言处理技术和社会学分析，对MIGR-TWIT极右翼英语和法语推文语料库进行多维度分析，重点考察移民、仇恨言论和说服技巧。

Result: 通过跨学科方法，揭示了极右及极右翼在社交平台上交流移民、散布仇恨、运用说服策略的具体模式。

Conclusion: 整合语言、社会学和计算方法有助于深入理解社交媒体上极右翼极端主义的传播机制，对应对当代社会挑战具有重要参考价值。

Abstract: The rise of right-wing populism in Europe has brought to the forefront the
significance of analysing social media discourse to understand the
dissemination of extremist ideologies and their impact on political outcomes.
Twitter, as a platform for interaction and mobilisation, provides a unique
window into the everyday communication of far-right supporters. In this paper,
we propose a methodology that uses state-of-the-art natural language processing
techniques with sociological insights to analyse the MIGR-TWIT corpus of
far-right tweets in English and French. We aim to uncover patterns of discourse
surrounding migration, hate speech, and persuasion techniques employed by right
and far-right actors. By integrating linguistic, sociological, and
computational approaches, we seek to offer cross-disciplinary insights into
societal dynamics and contribute to a better understanding of contemporary
challenges posed by right-wing extremism on social media platforms.

</details>


### [90] [DispatchMAS: Fusing taxonomy and artificial intelligence agents for emergency medical services](https://arxiv.org/abs/2510.21228)
*Xiang Li,Huizi Yu,Wenkong Wang,Yiran Wu,Jiayan Zhou,Wenyue Hua,Xinxin Lin,Wenjia Tan,Lexuan Zhu,Bingyi Chen,Guang Chen,Ming-Li Chen,Yang Zhou,Zhao Li,Themistocles L. Assimes,Yongfeng Zhang,Qingyun Wu,Xin Ma,Lingyao Li,Lizhou Fan*

Main category: cs.CL

TL;DR: 本论文提出并评估了一种基于大语言模型（LLM）和多智能体系统（MAS）的急救医疗接警仿真系统，可用于培训和决策支持。


<details>
  <summary>Details</summary>
Motivation: 急救医疗接警过程中，由于来电者压力、交流模糊和认知负荷高，接警员极易出现误判。利用LLM和MAS的新技术，有望为接警员提供支持，但需要确保仿真系统具备临床可信度和多样性。

Method: 作者基于MIMIC-III数据库，构建了包含32种主诉和6种来电者身份的临床分类体系，并设计六阶段接警流程，用于开发基于AutoGen的多智能体系统，实现接警员和来电者模拟对话。系统设有共享事实库确保医疗合理性，并通过四名医生对100个模拟案例进行人工评估，同时辅以自动化语言分析（情感、可读性、礼貌度等）。

Result: 该系统在人工评价中表现优异，医生间一致性良好（AC1>0.70）。系统在模拟场景中的接警有效性（94%准确联系其他相关方）和指导效能（91%成功提供建议）均获高度认可。算法评价也显示系统语言风格中性、可读性高（Flesch 80.9）、礼貌度高且无不礼貌案例。

Conclusion: 基于分类体系的MAS能够高保真地模拟多样且合理的急救接警场景，有助于培训、规范评估及实时决策支持，为高级AI代理安全集成进应急流程奠定了基础。

Abstract: Objective: Emergency medical dispatch (EMD) is a high-stakes process
challenged by caller distress, ambiguity, and cognitive load. Large Language
Models (LLMs) and Multi-Agent Systems (MAS) offer opportunities to augment
dispatchers. This study aimed to develop and evaluate a taxonomy-grounded,
LLM-powered multi-agent system for simulating realistic EMD scenarios. Methods:
We constructed a clinical taxonomy (32 chief complaints, 6 caller identities
from MIMIC-III) and a six-phase call protocol. Using this framework, we
developed an AutoGen-based MAS with Caller and Dispatcher Agents. The system
grounds interactions in a fact commons to ensure clinical plausibility and
mitigate misinformation. We used a hybrid evaluation framework: four physicians
assessed 100 simulated cases for "Guidance Efficacy" and "Dispatch
Effectiveness," supplemented by automated linguistic analysis (sentiment,
readability, politeness). Results: Human evaluation, with substantial
inter-rater agreement (Gwe's AC1 > 0.70), confirmed the system's high
performance. It demonstrated excellent Dispatch Effectiveness (e.g., 94 %
contacting the correct potential other agents) and Guidance Efficacy (advice
provided in 91 % of cases), both rated highly by physicians. Algorithmic
metrics corroborated these findings, indicating a predominantly neutral
affective profile (73.7 % neutral sentiment; 90.4 % neutral emotion), high
readability (Flesch 80.9), and a consistently polite style (60.0 % polite; 0 %
impolite). Conclusion: Our taxonomy-grounded MAS simulates diverse, clinically
plausible dispatch scenarios with high fidelity. Findings support its use for
dispatcher training, protocol evaluation, and as a foundation for real-time
decision support. This work outlines a pathway for safely integrating advanced
AI agents into emergency response workflows.

</details>


### [91] [Correlation Dimension of Auto-Regressive Large Language Models](https://arxiv.org/abs/2510.21258)
*Xin Du,Kumiko Tanaka-Ishii*

Main category: cs.CL

TL;DR: 本文提出用相关维数（一种分形几何度量）来量化大型语言模型生成文本的复杂性，并展示该方法能有效检测文本生成退化等多种现象。


<details>
  <summary>Details</summary>
Motivation: 传统的评价指标（如困惑度）只重视局部预测准确率，无法有效评估语言模型生成文本的结构复杂性，导致模型即使困惑度很低也可能出现重复、无序等问题。

Method: 作者引入了相关维数这一分形几何指标，能评估语言模型生成文本的自相似性和层次性递归结构。通过一套实验，衡量不同训练阶段、上下文复杂度、幻觉倾向和退化文本的复杂性。

Result: 实验表明，相关维数可以揭示预训练中的三个阶段，反映上下文相关的复杂度，判断模型产生幻觉的倾向，并能检测生成文本的多种退化。

Conclusion: 相关维数是一种高效、鲁棒、通用的新型评估方法，可用于不同架构下的大模型生成机制分析，并为理解和优化生成式模型提供了新视角。

Abstract: Large language models (LLMs) have achieved remarkable progress in natural
language generation, yet they continue to display puzzling behaviors -- such as
repetition and incoherence -- even when exhibiting low perplexity. This
highlights a key limitation of conventional evaluation metrics, which emphasize
local prediction accuracy while overlooking long-range structural complexity.
We introduce correlation dimension, a fractal-geometric measure of
self-similarity, to quantify the epistemological complexity of text as
perceived by a language model. This measure captures the hierarchical
recurrence structure of language, bridging local and global properties in a
unified framework. Through extensive experiments, we show that correlation
dimension (1) reveals three distinct phases during pretraining, (2) reflects
context-dependent complexity, (3) indicates a model's tendency toward
hallucination, and (4) reliably detects multiple forms of degeneration in
generated text. The method is computationally efficient, robust to model
quantization (down to 4-bit precision), broadly applicable across
autoregressive architectures (e.g., Transformer and Mamba), and provides fresh
insight into the generative dynamics of LLMs.

</details>


### [92] [Sparser Block-Sparse Attention via Token Permutation](https://arxiv.org/abs/2510.21270)
*Xinghao Wang,Pengyu Wang,Dong Zhang,Chenkun Tan,Shaojun Zhou,Zhaoxiang Liu,Shiguo Lian,Fangxu Liu,Kai Song,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文提出了一种新的块稀疏注意力方法Permuted Block-Sparse Attention（PBS-Attn），在保证精度的基础上大幅提升长上下文预填充的计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在扩展上下文长度时，计算复杂度因自注意力机制的$O(N^2)$规模大幅增加，成为效率瓶颈。虽然块稀疏注意力可缓解计算压力，但现有方法的效果受块划分和注意力分布影响，往往导致计算冗余，降低了优化效果。

Method: 提出PBS-Attn方法，通过排列（permutation）优化块之间的分布，增强块级别的稀疏性，从而减少无效计算。该方法可无缝集成到现有块稀疏注意力流程，配合自研的permuted-FlashAttention加速核进行实验。

Result: 在多个真实长上下文数据集上，PBS-Attn在准确率上超越现有块稀疏注意力方法，并能逼近全注意力（full attention）基线。速度方面，在长上下文预填充任务中可实现最高2.75倍的加速效果。

Conclusion: PBS-Attn作为一种插件式注意力加速方法，能够显著提升大模型高效处理长文本的能力，在实际应用场景中具有较高的实用性和推广价值。

Abstract: Scaling the context length of large language models (LLMs) offers significant
benefits but is computationally expensive. This expense stems primarily from
the self-attention mechanism, whose $O(N^2)$ complexity with respect to
sequence length presents a major bottleneck for both memory and latency.
Fortunately, the attention matrix is often sparse, particularly for long
sequences, suggesting an opportunity for optimization. Block-sparse attention
has emerged as a promising solution that partitions sequences into blocks and
skips computation for a subset of these blocks. However, the effectiveness of
this method is highly dependent on the underlying attention patterns, which can
lead to sub-optimal block-level sparsity. For instance, important key tokens
for queries within a single block may be scattered across numerous other
blocks, leading to computational redundancy. In this work, we propose Permuted
Block-Sparse Attention (\textbf{PBS-Attn}), a plug-and-play method that
leverages the permutation properties of attention to increase block-level
sparsity and enhance the computational efficiency of LLM prefilling. We conduct
comprehensive experiments on challenging real-world long-context datasets,
demonstrating that PBS-Attn consistently outperforms existing block-sparse
attention methods in model accuracy and closely matches the full attention
baseline. Powered by our custom permuted-FlashAttention kernels, PBS-Attn
achieves an end-to-end speedup of up to $2.75\times$ in long-context
prefilling, confirming its practical viability. Code available at
https://github.com/xinghaow99/pbs-attn

</details>


### [93] [PARL: Prompt-based Agents for Reinforcement Learning](https://arxiv.org/abs/2510.21306)
*Yarik Menchaca Resendiz,Roman Klinger*

Main category: cs.CL

TL;DR: 本文提出了一种利用大语言模型（LLMs）作为强化学习（RL）智能体的新方法PARL，能够在无需微调的情况下通过提示学习任务，并在部分标准RL任务中取得与传统RL方法相当或更好的表现，但在需要复杂数学运算或状态/动作解码的任务中表现有限。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注LLMs在自然语言相关的监督或无监督任务中的表现，很少评估其作为强化学习智能体在RL环境中的能力，特别是面对非语言结构化推理（如网格世界任务）时的效果，因此希望探索通过提示让LLMs直接参与RL任务的有效性及局限性。

Method: 提出PARL方法，通过提示（prompting）方式将动作、状态和奖励信息编码进提示语中，让预训练的大语言模型依据试错交互完成学习，无需进行任何微调。

Result: 在三个标准RL任务中（非完全依赖自然语言），PARL利用预训练知识在简单环境下能达到甚至超过传统RL智能体的表现。但在要求复杂数学运算或者状态、动作解码能力的复杂任务中，表现存在明显局限。

Conclusion: LLMs作为RL智能体在部分任务中表现优异，充分显示其预训练知识的价值，但面对需要非语言化推理和计算能力的复杂RL任务时，性能存在瓶颈，说明该方法虽有潜力但尚不适用于全部RL场景。

Abstract: Large language models (LLMs) have demonstrated high performance on tasks
expressed in natural language, particularly in zero- or few-shot settings.
These are typically framed as supervised (e.g., classification) or unsupervised
(e.g., clustering) problems. However, limited work evaluates LLMs as agents in
reinforcement learning (RL) tasks (e.g., playing games), where learning occurs
through interaction with an environment and a reward system. While prior work
focused on representing tasks that rely on a language representation, we study
structured, non-linguistic reasoning - such as interpreting positions in a grid
world. We therefore introduce PARL (Prompt-based Agent for Reinforcement
Learning), a method that uses LLMs as RL agents through prompting, without any
fine-tuning. PARL encodes actions, states, and rewards in the prompt, enabling
the model to learn through trial-and-error interaction. We evaluate PARL on
three standard RL tasks that do not entirely rely on natural language. We show
that it can match or outperform traditional RL agents in simple environments by
leveraging pretrained knowledge. However, we identify performance limitations
in tasks that require complex mathematical operations or decoding states and
actions.

</details>


### [94] [Efficient semantic uncertainty quantification in language models via diversity-steered sampling](https://arxiv.org/abs/2510.21310)
*Ji Won Park,Kyunghyun Cho*

Main category: cs.CL

TL;DR: 本文提出一种多样性引导的采样器，在大语言模型进行自由问答时，更高效准确地估计语义不确定性，无需接入底层梯度，并可显著提升样本利用率。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型的开放式问答中，精确估计语义层面的不确定性极具挑战，通常需要产生大量昂贵的样本，该问题亟需提升采样效率和估算稳定性。

Method: 作者提出了一种在采样分布中注入连续语义相似性惩罚的新型采样器。该方法利用轻微微调后的自然语言推断（NLI）模型，对自动回归及掩码扩散两种范式均适用，并借助重要性重加权及控制变量降低下游不确定性估计的偏差和方差。

Result: 在四个QA基准上，所提方法在生成相同数量样本的情况下可涵盖更多语义簇，其不确定性估计表现等于或优于主流基线模型，且样本效率显著提升。

Conclusion: 该方法无需访问底层LLM梯度，模块化、易于集成，可作为提升风险敏感场景中LLM不确定性估算效果的即插即用型增强组件。

Abstract: Accurately estimating semantic aleatoric and epistemic uncertainties in large
language models (LLMs) is particularly challenging in free-form question
answering (QA), where obtaining stable estimates often requires many expensive
generations. We introduce a diversity-steered sampler that discourages
semantically redundant outputs during decoding, covers both autoregressive and
masked diffusion paradigms, and yields substantial sample-efficiency gains. The
key idea is to inject a continuous semantic-similarity penalty into the model's
proposal distribution using a natural language inference (NLI) model lightly
finetuned on partial prefixes or intermediate diffusion states. We debias
downstream uncertainty estimates with importance reweighting and shrink their
variance with control variates. Across four QA benchmarks, our method matches
or surpasses baselines while covering more semantic clusters with the same
number of samples. Being modular and requiring no gradient access to the base
LLM, the framework promises to serve as a drop-in enhancement for uncertainty
estimation in risk-sensitive model deployments.

</details>


### [95] [Typoglycemia under the Hood: Investigating Language Models' Understanding of Scrambled Words](https://arxiv.org/abs/2510.21326)
*Gianluca Sperduti,Alejandro Moreo*

Main category: cs.CL

TL;DR: 该论文探讨了NLP模型如何能够在词内字母顺序被打乱的情况下（typoglycemia现象）依然能保持较好表现，并通过系列实验分析其背后原因。


<details>
  <summary>Details</summary>
Motivation: 人类能够读懂词内字母顺序打乱的单词（如form与from），近期出现一些NLP模型同样对于此类扰动具有鲁棒性。文章希望理解模型面对大量单词“坍缩”为相同表示时，为何依旧表现良好，这关系到模型的语义区分能力与泛化机制。

Method: （1）利用British National Corpus统计分析typoglycemia下的单词坍缩与歧义；（2）评估BERT模型对坍缩词形的辨别能力；（3）通过对比实验，分析在正常和typoglycemia扰动过的语料上训练的BERT，考查其表现变化。

Result: 结果表明：在typoglycemia条件下，真正坍缩为同一词形的单词数量很少，而且这些词一般出现在语境截然不同的情况下，模型的辨析难度不大。同时，BERT在scramble条件下的性能下降幅度低于预期。

Conclusion: NLP模型对typoglycemia表现出较强鲁棒性主要归因于真正歧义性低以及丰富语境助力歧义消解，为理解NLP模型处理扰动输入的能力提供了实证依据。

Abstract: Research in linguistics has shown that humans can read words with internally
scrambled letters, a phenomenon recently dubbed typoglycemia. Some specific NLP
models have recently been proposed that similarly demonstrate robustness to
such distortions by ignoring the internal order of characters by design. This
raises a fundamental question: how can models perform well when many distinct
words (e.g., form and from) collapse into identical representations under
typoglycemia? Our work, focusing exclusively on the English language, seeks to
shed light on the underlying aspects responsible for this robustness. We
hypothesize that the main reasons have to do with the fact that (i) relatively
few English words collapse under typoglycemia, and that (ii) collapsed words
tend to occur in contexts so distinct that disambiguation becomes trivial. In
our analysis, we (i) analyze the British National Corpus to quantify word
collapse and ambiguity under typoglycemia, (ii) evaluate BERT's ability to
disambiguate collapsing forms, and (iii) conduct a probing experiment by
comparing variants of BERT trained from scratch on clean versus typoglycemic
Wikipedia text; our results reveal that the performance degradation caused by
scrambling is smaller than expected.

</details>


### [96] [TripTide: A Benchmark for Adaptive Travel Planning under Disruptions](https://arxiv.org/abs/2510.21329)
*Priyanshu Karmakar,Soumyabrata Chaudhuri,Shubhojit Mallick,Manish Gupta,Abhik Jana,Shreya Ghosh*

Main category: cs.CL

TL;DR: 本文提出了TripTide，这是首个用于评估大语言模型（LLM）在实际旅行中应对行程扰动、修订规划能力的基准。通过多个自动和人工指标，系统性评估LLM的适应性、保持计划目标的能力和行程调整效果。实验发现LLM在保持计划语义和顺序方面表现优异，但在较长行程下处理扰动的能力下降，显示其鲁棒性有限。


<details>
  <summary>Details</summary>
Motivation: 尽管已有方法能基于LLM个性化生成受约束的旅行计划，但现实旅行常遇到航班取消、天气关闭等突发状况，现有研究尚未系统评测LLM应对此类扰动的能力。因此，研究动机是在实际情境下，建立评测LLM应对旅行扰动的基准和方法。

Method: 作者提出TripTide基准系统，模拟不同扰动严重程度和旅客容忍度，对LLM进行三重评测：1）自动化指标，包括意图保持、响应性、适应性等；2）由LLM自动评分修订质量；3）专家人工评估，考察各方面plan修订效果。

Result: 实验表明，LLM在保持行程顺序与语义一致性方面表现稳定，短途旅行空间偏离较大但长途则趋于一致；然而，随着行程变长或复杂，其对突发事件的应对能力下降。

Conclusion: TripTide为实用环境下评测LLM在旅行规划的适应性、个性化和韧性奠定了基准，也揭示了其在实际应用中尚存的局限，为后续研究指明方向。

Abstract: Recent efforts like TripCraft and TravelPlanner have advanced the use of
Large Language Models ( LLMs) for personalized, constraint aware travel
itinerary generation. Yet, real travel often faces disruptions. To address
this, we present TripTide, the first benchmark evaluating LLM's ability to
revise itineraries under realistic disruptions. TripTide models key dimensions
such as disruption severity and traveler tolerance, enabling nuanced assessment
of LLM adaptability to events like flight cancellations, weather closures, or
overbooked attractions. We conduct a threefold evaluation. First, we introduce
automatic metrics including Preservation of Intent (how well the revised plan
maintains feasibility and goals), Responsiveness (promptness and
appropriateness of disruption handling), and Adaptability (semantic, spatial,
and sequential divergence between original and revised plans). Second, we apply
an LLM-as-a-judge approach to automatically assess revision quality. Third, we
perform manual expert evaluation to verify whether revisions preserve semantic,
spatial, sequential, and responsive aspects. Our experiments show that LLMs
maintain strong sequential consistency and semantic stability, while spatial
deviations are larger for shorter trips but decrease with longer ones,
indicating that extended plans encourage better geographic coherence. However,
disruption-handling ability declines as plan length increases, highlighting
limits in LLM robustness. TripTide establishes a benchmark for evaluating
adaptability, personalization, and resilience in LLM-based travel planning
under real-world uncertainty.

</details>


### [97] [Multi-turn Training with Basic Human Feedback Helps Little on LLM Reasoning](https://arxiv.org/abs/2510.21339)
*Qiang Liu,Wuganjing Song,Zhenzhou Lin,Feifan Chen,Qiaolong Cai,Chen Li,Yongduo Sui*

Main category: cs.CL

TL;DR: 单轮训练大模型也能很好解决多轮推理任务，多轮训练反而可能降低推理能力。


<details>
  <summary>Details</summary>
Motivation: 实际应用中多轮互动常见，但目前的大模型推理训练方式主要用单轮强化学习，因此需探究多轮训练是否有必要。

Method: 设计对比实验，将单轮训练与三种多轮训练策略进行对比，评测各自对推理任务性能的影响。

Result: 单轮训练模型不仅能泛化到多轮，还在单轮推理上表现更佳；多轮训练模型反而在单轮推理上退化。

Conclusion: 在信息充分的任务上，单轮训练比多轮训练更有效和可靠，多轮训练带来的反馈提升有限，甚至有负面影响。

Abstract: The reasoning capabilities of Large Language Models (LLMs) are typically
developed through the single-turn reinforcement learning, whereas real-world
applications often involve multi-turn interactions with human feedback, leading
to a potential mismatch between training and deployment conditions. In this
work, we study whether multi-turn training with human feedback is necessary for
reasoning tasks. We compare conventional single-turn training with three
multi-turn strategies and reach contrary conclusions to previous research. We
find that models trained in a single-turn setting generalize effectively to
both single- and multi-turn evaluations, while models trained with multi-turn
strategies exhibit a significant degradation in single-turn reasoning
performance. These results suggest that for tasks with complete information,
robust single-turn training remains more effective and reliable, as multi-turn
training with basic feedback provides limited benefits and can even degrade
reasoning capabilities.

</details>


### [98] [A Diagnostic Benchmark for Sweden-Related Factual Knowledge](https://arxiv.org/abs/2510.21360)
*Jenny Kunz*

Main category: cs.CL

TL;DR: 本文提出了一个专为瑞典知识设计的问答基准数据集，用于测试语言模型对瑞典特有事实的记忆效果。


<details>
  <summary>Details</summary>
Motivation: 目前用于瑞典语的大多数基准数据集是翻译自以美国为中心的英文基准，不适合评估瑞典本地知识，尤其是诸如瑞典公众人物和事件等在国际上曝光较少的内容。

Method: 研究者手动编写了一个关注瑞典相关人物及事件的问答数据集，并引入英文译文以便跨语言一致性测试。灵感来源包括瑞典流行广播节目和重要体育赛事。随后，作者用不同规模且瑞典语能力不同的模型进行实验对比。

Result: 实验发现，瑞典语能力较强的小模型在瑞典事实回忆任务上，表现可与规模三倍大的多语种模型媲美。持续用瑞典语进行继续预训练虽能提升相关知识，但会导致部分已知信息遗忘。

Conclusion: 提出的数据集有助于诊断多语言模型在瑞典语适应过程中的知识保留及语言迁移效果，是研究模型知识保持与遗忘的重要工具。

Abstract: Many Swedish benchmarks are translated US-centric benchmarks, and therefore
not suitable for testing knowledge that is particularly relevant, or even
specific, to Sweden. We therefore introduce a manually written
question-answering benchmark specifically targeted to Sweden-related
personalities and events, many of which receive very limited coverage in
international media. Our annotators drew inspiration from a popular radio
program featuring public figures from culture and media, as well as major
sports events in Sweden. The dataset can be used to measure factual recall
across models of varying sizes and degrees of Swedish coverage, and allows to
probe cross-lingual factual consistency as to contains English translations.
Using the dataset, we find that smaller models with stronger Swedish coverage
perform comparably to a three times larger multilingual model in recalling
Sweden-related facts. We also observe that continued pre-training on Swedish
generally improves factual knowledge but also leads to forgetting of a part of
the previously known information. These results demonstrate the dataset's
potential as a diagnostic tool for studying language adaptation and knowledge
retention in multilingual models and during language adaptation.

</details>


### [99] [SindBERT, the Sailor: Charting the Seas of Turkish NLP](https://arxiv.org/abs/2510.21364)
*Raphael Scheible-Schmitt,Stefan Schweter*

Main category: cs.CL

TL;DR: 文章介绍了SindBERT，这是一个为土耳其语打造的大规模RoBERTa编码器模型，在多个NLP下游任务中展现了与现有模型有竞争力的性能，并探讨了模型增大规模效果的平稳趋势。


<details>
  <summary>Details</summary>
Motivation: 目前主流的NLP预训练模型对于形态丰富的土耳其语等语言覆盖不足，缺乏专门的大规模编码器模型支持土耳其语的自然语言处理研究与应用。本文旨在填补这一空白。

Method: 研究者从零开始训练了SindBERT，基于RoBERTa架构，分别提供base与large两个版本。训练语料覆盖mC4、OSCAR23和Wikipedia，共计312GB土耳其语数据。模型性能在词性标注、命名实体识别、攻击性言论检测和TurBLiMP语言可接受性等基准任务上进行了评测。

Result: SindBERT在多项任务上与土耳其语及多语言模型表现相当，大型版本在四项任务中的两项取得最佳，但整体性能随模型规模扩大未出现明显提升。并且，与精挑细选的小规模模型BERTurk的对比中，语料的质量和多样性被证明比数据量更关键。

Conclusion: SindBERT为土耳其语NLP开源提供了新的重要资源，同时作为大规模化和语料组成在形态丰富语言上作用的实证案例。当前土耳其语NLP基准可能已经趋于饱和，推动模型性能的关键可能在于优化语料质量而非单纯扩展数据量。

Abstract: Transformer models have revolutionized NLP, yet many morphologically rich
languages remain underrepresented in large-scale pre-training efforts. With
SindBERT, we set out to chart the seas of Turkish NLP, providing the first
large-scale RoBERTa-based encoder for Turkish. Trained from scratch on 312 GB
of Turkish text (mC4, OSCAR23, Wikipedia), SindBERT is released in both base
and large configurations, representing the first large-scale encoder-only
language model available for Turkish. We evaluate SindBERT on part-of-speech
tagging, named entity recognition, offensive language detection, and the
TurBLiMP linguistic acceptability benchmark. Our results show that SindBERT
performs competitively with existing Turkish and multilingual models, with the
large variant achieving the best scores in two of four tasks but showing no
consistent scaling advantage overall. This flat scaling trend, also observed
for XLM-R and EuroBERT, suggests that current Turkish benchmarks may already be
saturated. At the same time, comparisons with smaller but more curated models
such as BERTurk highlight that corpus quality and diversity can outweigh sheer
data volume. Taken together, SindBERT contributes both as an openly released
resource for Turkish NLP and as an empirical case study on the limits of
scaling and the central role of corpus composition in morphologically rich
languages. The SindBERT models are released under the MIT license and made
available in both fairseq and Huggingface formats.

</details>


### [100] [HalleluBERT: Let every token that has meaning bear its weight](https://arxiv.org/abs/2510.21372)
*Raphael Scheible-Schmitt*

Main category: cs.CL

TL;DR: 本文提出了HalleluBERT，这是针对希伯来语、基于RoBERTa架构的新型语言模型，经过大规模希伯来语语料库训练，并在多项任务上取得了最好成绩。


<details>
  <summary>Details</summary>
Motivation: 当前希伯来语缺乏经过大规模训练的RoBERTa语言模型，现有模型在语料、词表或训练深度上均有限制，导致在多种NLP任务上的表现不佳。

Method: 作者从零开始训练了HalleluBERT，包括base和large两个版本，使用了49.1GB希伯来语网页和维基百科语料，并构建了希伯来语特定的byte-level BPE词汇表，采用RoBERTa的预训练框架。

Result: 在命名实体识别（NER）和情感分类等任务上，HalleluBERT显著优于已有的希伯来语和多语言模型，取得了新的最优性能。

Conclusion: 完全基于大规模希伯来语语料进行单语预训练可以极大提升希伯来语NLP任务表现，HalleluBERT为今后相关研究和应用打下基础。

Abstract: Transformer-based models have advanced NLP, yet Hebrew still lacks a
large-scale RoBERTa encoder which is extensively trained. Existing models such
as HeBERT, AlephBERT, and HeRo are limited by corpus size, vocabulary, or
training depth. We present HalleluBERT, a RoBERTa-based encoder family (base
and large) trained from scratch on 49.1~GB of deduplicated Hebrew web text and
Wikipedia with a Hebrew-specific byte-level BPE vocabulary. Evaluated on NER
and sentiment classification benchmarks, HalleluBERT outperforms both
monolingual and multilingual baselines. HalleluBERT sets a new state of the art
for Hebrew and highlights the benefits of fully converged monolingual
pretraining.

</details>


### [101] [Vision Language Models for Dynamic Human Activity Recognition in Healthcare Settings](https://arxiv.org/abs/2510.21424)
*Abderrazek Abid,Thanh-Cong Ho,Fakhri Karray*

Main category: cs.CL

TL;DR: 本论文探讨了视觉语言模型（VLMs）在远程健康监测中的人体活动识别（HAR）应用，并提出新的评估方法及数据集。结果显示，VLMs在准确率上可与传统模型媲美甚至超越。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在医疗健康领域应用广泛，但其在人体活动识别（HAR）上的利用尚不充分。受限于传统深度学习模型的灵活性问题，以及VLMs输出的动态性和非确定性，本文旨在推动该领域的发展，并探索VLMs的优势与挑战。

Method: 作者提出了新的描述性字幕数据集，并设计了用于评估VLMs在HAR任务中的综合评估方法。通过与最新的深度学习模型进行对比实验，全面评估VLMs的表现。

Result: 实验证明，VLMs在人体活动识别中能达到甚至超过主流深度学习模型的准确率，显示出其在该任务中的潜力。

Conclusion: 本研究为VLMs在智能健康系统的集成提供了有力的基准和方法，表明该模型有望突破传统方法局限，推动远程健康监测领域的发展。

Abstract: As generative AI continues to evolve, Vision Language Models (VLMs) have
emerged as promising tools in various healthcare applications. One area that
remains relatively underexplored is their use in human activity recognition
(HAR) for remote health monitoring. VLMs offer notable strengths, including
greater flexibility and the ability to overcome some of the constraints of
traditional deep learning models. However, a key challenge in applying VLMs to
HAR lies in the difficulty of evaluating their dynamic and often
non-deterministic outputs. To address this gap, we introduce a descriptive
caption data set and propose comprehensive evaluation methods to evaluate VLMs
in HAR. Through comparative experiments with state-of-the-art deep learning
models, our findings demonstrate that VLMs achieve comparable performance and,
in some cases, even surpass conventional approaches in terms of accuracy. This
work contributes a strong benchmark and opens new possibilities for the
integration of VLMs into intelligent healthcare systems.

</details>


### [102] [Redefining Retrieval Evaluation in the Era of LLMs](https://arxiv.org/abs/2510.21440)
*Giovanni Trappolini,Florin Cuconasu,Simone Filice,Yoelle Maarek,Fabrizio Silvestri*

Main category: cs.CL

TL;DR: 传统的信息检索评价指标无法准确衡量面向大型语言模型（LLM）的RAG系统的检索效果，作者提出了新的UDCG指标，更好地和端到端生成效果相关联。


<details>
  <summary>Details</summary>
Motivation: 现有的IR指标假设用户是按顺序阅读文档并对位次逐渐失去关注，但是RAG系统中的LLM会整体处理全部检索结果，且无关但相关的文档会对生成质量产生负面影响，传统指标对此并未考虑。

Method: 作者提出了基于效用的标注框架，既量化了有用信息的正向贡献，也衡量了无关干扰信息的负面影响；基于此提出了UDCG指标，采用LLM特有的折扣方式优化与最终答案准确性的相关性。

Result: 在五个数据集和六种LLM上的实验显示，UDCG指标与传统指标相比在相关性上最多提高了36%。

Conclusion: UDCG指标更贴合RAG场景下LLM的实际信息处理方式，为IR检索质量评价和RAG系统性能分析提供了更可靠的工具。

Abstract: Traditional Information Retrieval (IR) metrics, such as nDCG, MAP, and MRR,
assume that human users sequentially examine documents with diminishing
attention to lower ranks. This assumption breaks down in Retrieval Augmented
Generation (RAG) systems, where search results are consumed by Large Language
Models (LLMs), which, unlike humans, process all retrieved documents as a whole
rather than sequentially. Additionally, traditional IR metrics do not account
for related but irrelevant documents that actively degrade generation quality,
rather than merely being ignored. Due to these two major misalignments, namely
human vs. machine position discount and human relevance vs. machine utility,
classical IR metrics do not accurately predict RAG performance. We introduce a
utility-based annotation schema that quantifies both the positive contribution
of relevant passages and the negative impact of distracting ones. Building on
this foundation, we propose UDCG (Utility and Distraction-aware Cumulative
Gain), a metric using an LLM-oriented positional discount to directly optimize
the correlation with the end-to-end answer accuracy. Experiments on five
datasets and six LLMs demonstrate that UDCG improves correlation by up to 36%
compared to traditional metrics. Our work provides a critical step toward
aligning IR evaluation with LLM consumers and enables more reliable assessment
of RAG components

</details>


### [103] [REMONI: An Autonomous System Integrating Wearables and Multimodal Large Language Models for Enhanced Remote Health Monitoring](https://arxiv.org/abs/2510.21445)
*Thanh Cong Ho,Farah Kharrat,Abderrazek Abid,Fakhri Karray*

Main category: cs.CL

TL;DR: 该论文提出了一套名为REMONI的远程健康监测系统，结合多模态大语言模型、物联网与可穿戴设备，实现自动、持续收集患者多种健康数据并智能分析，提升远程医疗交互性。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴设备普及，远程患者监测需求提升，但当前研究主要聚焦在数据采集和异常检测，缺少人机交互和对患者情绪活动的理解。

Method: 系统融合多模态大语言模型（MLLMs）、IoT与可穿戴设备，收集生命体征、加速度及视频数据，并通过异常检测模块实现跌倒检测等。系统采用自然语言处理，能理解并响应医护人员的查询，利用prompt工程整合患者多源数据。

Result: 实验表明该系统可在现实环境中实施，具备扩展性，能够减轻医护工作负担及医疗成本。并已开发全功能原型进行测试，验证其多项能力的稳健性。

Conclusion: REMONI系统提升了远程健康监护的人机交互水平，丰富了医疗工作者对患者状态的理解，对推进智能远程医疗系统的落地具有积极作用。

Abstract: With the widespread adoption of wearable devices in our daily lives, the
demand and appeal for remote patient monitoring have significantly increased.
Most research in this field has concentrated on collecting sensor data,
visualizing it, and analyzing it to detect anomalies in specific diseases such
as diabetes, heart disease and depression. However, this domain has a notable
gap in the aspect of human-machine interaction. This paper proposes REMONI, an
autonomous REmote health MONItoring system that integrates multimodal large
language models (MLLMs), the Internet of Things (IoT), and wearable devices.
The system automatically and continuously collects vital signs, accelerometer
data from a special wearable (such as a smartwatch), and visual data in patient
video clips collected from cameras. This data is processed by an anomaly
detection module, which includes a fall detection model and algorithms to
identify and alert caregivers of the patient's emergency conditions. A
distinctive feature of our proposed system is the natural language processing
component, developed with MLLMs capable of detecting and recognizing a
patient's activity and emotion while responding to healthcare worker's
inquiries. Additionally, prompt engineering is employed to integrate all
patient information seamlessly. As a result, doctors and nurses can access
real-time vital signs and the patient's current state and mood by interacting
with an intelligent agent through a user-friendly web application. Our
experiments demonstrate that our system is implementable and scalable for
real-life scenarios, potentially reducing the workload of medical professionals
and healthcare costs. A full-fledged prototype illustrating the functionalities
of the system has been developed and being tested to demonstrate the robustness
of its various capabilities.

</details>


### [104] [MRO: Enhancing Reasoning in Diffusion Language Models via Multi-Reward Optimization](https://arxiv.org/abs/2510.21473)
*Chenglong Wang,Yang Gan,Hang Zhou,Chi Hu,Yongyu Mu,Kai Song,Murun Yang,Bei Li,Chunliang Zhang,Tongran Liu,Jingbo Zhu,Zhengtao Yu,Tong Xiao*

Main category: cs.CL

TL;DR: 本文提出了一种多奖励优化方法（MRO），提升扩散语言模型（DLMs）推理能力，通过加强token相关性且加速采样。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型在推理能力上仍落后于传统自回归大语言模型，特别是在降噪步数较少时。根本原因是各降噪步中掩码token独立生成，未建模token相关性。

Method: 作者提出通过增强token相关性来提升模型推理力，并细分为序列内和序列间两类。具体方法为多奖励优化（MRO），联合采用测试时缩放、拒绝采样和强化学习，通过精心设计的多个奖励直接优化token相关性。同时引入分组步骤和重要性采样以降低奖励方差和提升采样效率。

Result: 在大量实验证明下，MRO方法显著提升了扩散语言模型在推理任务上的表现，同时大幅加快采样速度，并且在推理基准测试中保持高性能。

Conclusion: 多奖励优化不仅补足了扩散语言模型token相关性不足的短板，也为推理任务提供了高效且高性能的新范式。

Abstract: Recent advances in diffusion language models (DLMs) have presented a
promising alternative to traditional autoregressive large language models
(LLMs). However, DLMs still lag behind LLMs in reasoning performance,
especially as the number of denoising steps decreases. Our analysis reveals
that this shortcoming arises primarily from the independent generation of
masked tokens across denoising steps, which fails to capture the token
correlation. In this paper, we define two types of token correlation:
intra-sequence correlation and inter-sequence correlation, and demonstrate that
enhancing these correlations improves reasoning performance. To this end, we
propose a Multi-Reward Optimization (MRO) approach, which encourages DLMs to
consider the token correlation during the denoising process. More specifically,
our MRO approach leverages test-time scaling, reject sampling, and
reinforcement learning to directly optimize the token correlation with multiple
elaborate rewards. Additionally, we introduce group step and importance
sampling strategies to mitigate reward variance and enhance sampling
efficiency. Through extensive experiments, we demonstrate that MRO not only
improves reasoning performance but also achieves significant sampling speedups
while maintaining high performance on reasoning benchmarks.

</details>


### [105] [Brain-tuning Improves Generalizability and Efficiency of Brain Alignment in Speech Models](https://arxiv.org/abs/2510.21520)
*Omer Moussa,Mariya Toneva*

Main category: cs.CL

TL;DR: 提出了一种可扩展、泛化性强的大脑调优方法，将预训练语言模型微调以联合预测多位参与者的fMRI反应，实现了更好的大脑对齐并能泛化到新参与者和新数据集。


<details>
  <summary>Details</summary>
Motivation: 以往将语言模型与大脑反应对齐的方法依赖单一参与者且数据需求大，限制了对新参与者和群体水平的泛化分析，亟需一种数据利用率高、适用性强的方法。

Method: 提出了多参与者大脑调优方法，通过微调预训练语音语言模型，使其能同时预测多位参与者的fMRI反应，从而实现跨参与者泛化。

Result: 1）新参与者fMRI数据需求减少5倍；2）整体大脑对齐度提升50%；3）可泛化到新数据集，并提升语义任务下游表现。

Conclusion: 多参与者大脑调优不仅促进了大脑-模型对齐和群体分析，也提升了模型语义泛化能力，有助于推动神经科学和人工智能领域的交叉融合。

Abstract: Pretrained language models are remarkably effective in aligning with human
brain responses elicited by natural language stimuli, positioning them as
promising model organisms for studying language processing in the brain.
However, existing approaches for both estimating and improving this brain
alignment are participant-dependent and highly affected by the amount of data
available per participant, hindering both generalization to new participants
and population-level analyses. In this work, we address these limitations by
introducing a scalable, generalizable brain-tuning method, in which we
fine-tune pretrained speech language models to jointly predict fMRI responses
from multiple participants. We demonstrate that the resulting brain-tuned
models exhibit strong individual brain alignment while generalizing across
participants. Specifically, our method leads to 1) a 5-fold decrease in the
amount of fMRI data needed to predict brain data from new participants, 2) up
to a 50% increase in the overall brain alignment, and 3) strong generalization
to new unseen datasets. Furthermore, this multi-participant brain-tuning
additionally improves downstream performance on semantic tasks, suggesting that
training using brain data from multiple participants leads to more
generalizable semantic representations. Taken together, these findings
demonstrate a bidirectional benefit between neuroscience and AI, helping bridge
the gap between the two fields. We make our code and models publicly available
at https://github.com/bridge-ai-neuro/multi-brain-tuning.

</details>


### [106] [InterpDetect: Interpretable Signals for Detecting Hallucinations in Retrieval-Augmented Generation](https://arxiv.org/abs/2510.21538)
*Likun Tan,Kuan-Wei Huang,Joy Shi,Kevin Wu*

Main category: cs.CL

TL;DR: 本文分析了RAG（检索增强生成）模型中幻觉（hallucination）发生的机制，提出用区分外部知识与参数知识信号的方法高效检测幻觉，并验证了方法的有效性与普适性。


<details>
  <summary>Details</summary>
Motivation: RAG模型用外部知识缓解幻觉问题，但模型输出往往与检索内容不符。现有幻觉检测方法无法区分外部知识和模型内在知识的作用，因此难以精确检测幻觉。作者希望深入理解RAG幻觉发生机制，并开发出更有效的检测方法。

Method: 作者分析了RAG模型后层FFN模块对残差流的影响，提出利用外部上下文得分和参数知识得分进行幻觉检测。具体做法是在Qwen3-0.6b等模型中，计算各层及注意力头的这两种分数，并用回归分类器预测幻觉，并与GPT-5、GPT-4.1等SOTA模型及主流检测方法对比。

Result: 新方法与当前主流幻觉检测方法（如RAGAS、TruLens、RefChecker）相比表现更好。此外，用较小模型（Qwen3-0.6b）训练出的分类器对GPT-4.1-mini的幻觉检测也有良好泛化能力，验证了代理模型评测的有效性。

Conclusion: 通过区分外部知识和参数知识信号，作者揭示了RAG幻觉发生机制，提出了一种高效且具良好泛化性的检测方法，为RAG系统幻觉检测提供了新思路。

Abstract: Retrieval-Augmented Generation (RAG) integrates external knowledge to
mitigate hallucinations, yet models often generate outputs inconsistent with
retrieved content. Accurate hallucination detection requires disentangling the
contributions of external context and parametric knowledge, which prior methods
typically conflate. We investigate the mechanisms underlying RAG hallucinations
and find they arise when later-layer FFN modules disproportionately inject
parametric knowledge into the residual stream. To address this, we explore a
mechanistic detection approach based on external context scores and parametric
knowledge scores. Using Qwen3-0.6b, we compute these scores across layers and
attention heads and train regression-based classifiers to predict
hallucinations. Our method is evaluated against state-of-the-art LLMs (GPT-5,
GPT-4.1) and detection baselines (RAGAS, TruLens, RefChecker). Furthermore,
classifiers trained on Qwen3-0.6b signals generalize to GPT-4.1-mini responses,
demonstrating the potential of proxy-model evaluation. Our results highlight
mechanistic signals as efficient, generalizable predictors for hallucination
detection in RAG systems.

</details>


### [107] [Document Understanding, Measurement, and Manipulation Using Category Theory](https://arxiv.org/abs/2510.21553)
*Jared Claypoole,Yunye Gong,Noson S. Yanofsky,Ajay Divakaran*

Main category: cs.CL

TL;DR: 本文利用范畴论对多模态文档结构进行抽取，提出信息度量、内容总结和扩展方法，并提出自监督方法提升大模型。


<details>
  <summary>Details</summary>
Motivation: 面对多模态、大规模文档，缺乏数学化结构化表达和自动化分析评估框架，现有文档处理与模型改进路径受限。

Method: 1）用范畴论将文档表示为问答对的范畴；2）开发信息正交分解，将文档信息无重叠划分；3）发展信息度量和枚举方法，并基于此构建新型摘要与文档扩展（释义/exegesis）技术；4）通过问答对形式进行新颖的摘要失真率分析；5）利用大预训练模型实现上述方法；6）拓展为多模态框架；7）提出RLVR自监督方法，利用数学结构自带的组合与封闭性一致约束，提升预训练模型。

Result: 成功建立了基于范畴论的多模态文档结构提取与度量方法，提出了新颖的内容总结与扩展方式，开发了自监督模型增强机制，所有方案均以大模型为实现基础。

Conclusion: 范畴论为文档信息的结构化、度量、归纳与模型自我提升提供了统一且可扩展的理论与算法基础，有望推动多模态信息处理和大模型自监督训练的发展。

Abstract: We apply category theory to extract multimodal document structure which leads
us to develop information theoretic measures, content summarization and
extension, and self-supervised improvement of large pretrained models. We first
develop a mathematical representation of a document as a category of
question-answer pairs. Second, we develop an orthogonalization procedure to
divide the information contained in one or more documents into non-overlapping
pieces. The structures extracted in the first and second steps lead us to
develop methods to measure and enumerate the information contained in a
document. We also build on those steps to develop new summarization techniques,
as well as to develop a solution to a new problem viz. exegesis resulting in an
extension of the original document. Our question-answer pair methodology
enables a novel rate distortion analysis of summarization techniques. We
implement our techniques using large pretrained models, and we propose a
multimodal extension of our overall mathematical framework. Finally, we develop
a novel self-supervised method using RLVR to improve large pretrained models
using consistency constraints such as composability and closure under certain
operations that stem naturally from our category theoretic framework.

</details>


### [108] [Are the LLMs Capable of Maintaining at Least the Language Genus?](https://arxiv.org/abs/2510.21561)
*Sandra Mitrović,David Kletz,Ljiljana Dolamic,Fabio Rinaldi*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLMs）在多语言任务中的行为表现，关注其是否受到语言世系结构（genera）的影响及其机制。


<details>
  <summary>Details</summary>
Motivation: 多语言大语言模型在处理不同语言时表现差异明显，而语言的家族（genealogy）结构可能影响这种表现，但此前对此缺少深入研究。作者希望澄清LLMs是否能够识别或利用语言之间的世系关系。

Method: 作者基于MultiQ数据集扩展分析：1）考察模型在没能保持提示语言一致性时，是否更倾向于切换到具有世系关系的语言；2）检验知识的一致性在同一世系语言内是否更好于跨世系语言。

Result: 分析结果显示，世系结构确实影响了LLMs的表现，但这种效应与训练数据的丰富程度强相关。不同模型家族呈现出各自独特的多语言策略。

Conclusion: LLMs部分编码了语言世系结构，但多语言表现的主要影响因素仍是训练数据的覆盖和均衡性。

Abstract: Large Language Models (LLMs) display notable variation in multilingual
behavior, yet the role of genealogical language structure in shaping this
variation remains underexplored. In this paper, we investigate whether LLMs
exhibit sensitivity to linguistic genera by extending prior analyses on the
MultiQ dataset. We first check if models prefer to switch to genealogically
related languages when prompt language fidelity is not maintained. Next, we
investigate whether knowledge consistency is better preserved within than
across genera. We show that genus-level effects are present but strongly
conditioned by training resource availability. We further observe distinct
multilingual strategies across LLMs families. Our findings suggest that LLMs
encode aspects of genus-level structure, but training data imbalances remain
the primary factor shaping their multilingual performance.

</details>


### [109] [From Polyester Girlfriends to Blind Mice: Creating the First Pragmatics Understanding Benchmarks for Slovene](https://arxiv.org/abs/2510.21575)
*Mojca Brglez,Špela Vintar*

Main category: cs.CL

TL;DR: 本文提出了首个斯洛文尼亚语语用学理解评测数据集，并利用大语言模型进行了初步实验。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力提升，现有基准不再能全面评估其语言理解，尤其是更深层次的语用学能力（即超越字面和结构层面，涉及上下文、隐喻等）。

Method: 构建了SloPragEval和SloPragMega两个斯洛文尼亚语语用学多项选择题集（共405题），介绍了翻译挑战、人类基线建立过程，并用大模型进行了测试。

Result: 现有模型在理解细微语义方面能力已显著提升，但在推断隐含意义（特别是文化相关性强的非字面表达）上仍有不足。闭源模型明显优于开源模型。

Conclusion: 高质量、涵盖文化和语用细节的评测应基于本地数据且需人类校验。

Abstract: Large language models are demonstrating increasing capabilities, excelling at
benchmarks once considered very difficult. As their capabilities grow, there is
a need for more challenging evaluations that go beyond surface-level linguistic
competence. Namely, language competence involves not only syntax and semantics
but also pragmatics, i.e., understanding situational meaning as shaped by
context as well as linguistic and cultural norms. To contribute to this line of
research, we introduce SloPragEval and SloPragMega, the first pragmatics
understanding benchmarks for Slovene that contain altogether 405
multiple-choice questions. We discuss the difficulties of translation, describe
the campaign to establish a human baseline, and report pilot evaluations with
LLMs. Our results indicate that current models have greatly improved in
understanding nuanced language but may still fail to infer implied speaker
meaning in non-literal utterances, especially those that are culture-specific.
We also observe a significant gap between proprietary and open-source models.
Finally, we argue that benchmarks targeting nuanced language understanding and
knowledge of the target culture must be designed with care, preferably
constructed from native data, and validated with human responses.

</details>


### [110] [Automated Quality Control for Language Documentation: Detecting Phonotactic Inconsistencies in a Kokborok Wordlist](https://arxiv.org/abs/2510.21584)
*Kellen Parker van Dam,Abishek Stephen*

Main category: cs.CL

TL;DR: 该论文提出无监督异常检测方法，通过音系特征来识别语言文献词表中的转写错误与借词，提升数据质量。方法在Kokborok方言和孟加拉语多语数据集上实验，发现以音节为单位的特征优于字符级方法。


<details>
  <summary>Details</summary>
Motivation: 语言文献中的词汇数据常包含转写错误和未记录的借词，这影响语言分析的准确性，很难被人工排查。为确保低资源语言档案数据的可靠性，需自动检测和标注这类异常。

Method: 作者提出了基于音系规律（音节级和字符级）的无监督异常检测算法，自动寻找词表中的音系异常项，并在多语Kokborok和孟加拉语数据集上进行实验对比。

Result: 检测效果精度与召回率有限，但音节级特征性能明显好于字符级；高召回方案有助于系统性地找出需进一步核查的数据条目。

Conclusion: 该方法为低资源语言田野调查者提供了一种自动化、系统化的数据异常条目标注手段，有助于提升语言档案的数据质量。

Abstract: Lexical data collection in language documentation often contains
transcription errors and undocumented borrowings that can mislead linguistic
analysis. We present unsupervised anomaly detection methods to identify
phonotactic inconsistencies in wordlists, applying them to a multilingual
dataset of Kokborok varieties with Bangla. Using character-level and
syllable-level phonotactic features, our algorithms identify potential
transcription errors and borrowings. While precision and recall remain modest
due to the subtle nature of these anomalies, syllable-aware features
significantly outperform character-level baselines. The high-recall approach
provides fieldworkers with a systematic method to flag entries requiring
verification, supporting data quality improvement in low-resourced language
documentation.

</details>


### [111] [RETuning: Upgrading Inference-Time Scaling for Stock Movement Prediction with Large Language Models](https://arxiv.org/abs/2510.21604)
*Xueyuan Lin,Cehao Yang,Ye Ma,Ming Li,Rongjunchen Zhang,Yang Ni,Xiaojun Wu,Chengjin Xu,Jian Guo,Hui Xiong*

Main category: cs.CL

TL;DR: 本文关注大语言模型（LLM）在股票涨跌预测任务中的应用，提出了一种名为Reflective Evidence Tuning（RETuning）的新方法，通过动态构建分析框架提升LLM在金融领域的推理与预测能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在数学与编程任务上表现出色，但其在股票涨跌预测等金融核心任务上的应用研究有限。作者发现现有模型在推理过程中过度依赖分析师观点，缺乏独立逻辑推理与对反证据的有效评估，亟需方法改进以真正激发其推理能力。

Method: 提出RETuning方法，在生成推理链（CoT）时引导模型主动从多信息源动态构建分析框架，并基于框架对不同证据打分、综合，最终反思得到预测结论。方法在预训练阶段使用，无需强化学习先验。并构建了涵盖2024年全部A股（5123只），包含长上下文和丰富信息源的大规模数据集。

Result: 实验表明，RETuning能显著提升模型在金融领域的独立推理与预测能力。即便随着推断时间推移或应用在分布外股票上，模型仍然保持良好泛化和推理表现。

Conclusion: RETuning有效释放了LLM在金融中的推理能力，提高了股票涨跌预测的可靠性和独立性，为金融智能推理提供了新范式。

Abstract: Recently, large language models (LLMs) have demonstrated outstanding
reasoning capabilities on mathematical and coding tasks. However, their
application to financial tasks-especially the most fundamental task of stock
movement prediction-remains underexplored. We study a three-class
classification problem (up, hold, down) and, by analyzing existing reasoning
responses, observe that: (1) LLMs follow analysts' opinions rather than exhibit
a systematic, independent analytical logic (CoTs). (2) LLMs list summaries from
different sources without weighing adversarial evidence, yet such
counterevidence is crucial for reliable prediction. It shows that the model
does not make good use of its reasoning ability to complete the task. To
address this, we propose Reflective Evidence Tuning (RETuning), a cold-start
method prior to reinforcement learning, to enhance prediction ability. While
generating CoT, RETuning encourages dynamically constructing an analytical
framework from diverse information sources, organizing and scoring evidence for
price up or down based on that framework-rather than on contextual
viewpoints-and finally reflecting to derive the prediction. This approach
maximally aligns the model with its learned analytical framework, ensuring
independent logical reasoning and reducing undue influence from context. We
also build a large-scale dataset spanning all of 2024 for 5,123 A-share stocks,
with long contexts (32K tokens) and over 200K samples. In addition to price and
news, it incorporates analysts' opinions, quantitative reports, fundamental
data, macroeconomic indicators, and similar stocks. Experiments show that
RETuning successfully unlocks the model's reasoning ability in the financial
domain. Inference-time scaling still works even after 6 months or on
out-of-distribution stocks, since the models gain valuable insights about stock
movement prediction.

</details>


### [112] [The Universal Landscape of Human Reasoning](https://arxiv.org/abs/2510.21623)
*Qiguang Chen,Jinhao Liu,Libo Qin,Yimeng Zhang,Yihao Liang,Shangxu Ren,Chengyu Luan,Dengyun Peng,Hanjing Li,Jiannan Guan,Zheng Yan,Jiaqi Wang,Mengkang Hu,Yantao Du,Zhi Chen,Xie Chen,Wanxiang Che*

Main category: cs.CL

TL;DR: 该论文提出了一种名为信息流跟踪（IF-Track）的方法，利用大语言模型（LLM）量化人类推理中的信息熵和信息增益，首次在单一度量空间内刻画人类推理行为。


<details>
  <summary>Details</summary>
Motivation: 现有关于人类推理的经典逻辑和概率模型只能解释部分输出或个体建模，缺乏统一、可量化的推理动态描述。因此，迫切需要一种方法定量描述和理解一般性的人类推理动态。

Method: 作者提出了信息流跟踪（IF-Track）方法，将大语言模型作为概率编码器，在多样任务中细化分析推理过程中每一步的信息熵和信息增益变化，从而实现对推理行为的连续定量跟踪。

Result: IF-Track能够准确捕捉推理的核心特征，发现系统性错误模式，描绘个体差异，并成功统一了人类推理行为于一个度量空间。此外，方法验证了单过程和双过程心理理论间的关系，并揭示了人工智能模型如何影响和重塑人类推理过程。

Conclusion: IF-Track为理论与测量之间建立了量化桥梁，为推理机制的结构和认知本质提供了新见解，对心理学和人工智能研究都具有积极意义。

Abstract: Understanding how information is dynamically accumulated and transformed in
human reasoning has long challenged cognitive psychology, philosophy, and
artificial intelligence. Existing accounts, from classical logic to
probabilistic models, illuminate aspects of output or individual modelling, but
do not offer a unified, quantitative description of general human reasoning
dynamics. To solve this, we introduce Information Flow Tracking (IF-Track),
that uses large language models (LLMs) as probabilistic encoder to quantify
information entropy and gain at each reasoning step. Through fine-grained
analyses across diverse tasks, our method is the first successfully models the
universal landscape of human reasoning behaviors within a single metric space.
We show that IF-Track captures essential reasoning features, identifies
systematic error patterns, and characterizes individual differences. Applied to
discussion of advanced psychological theory, we first reconcile single- versus
dual-process theories in IF-Track and discover the alignment of artificial and
human cognition and how LLMs reshaping human reasoning process. This approach
establishes a quantitative bridge between theory and measurement, offering
mechanistic insights into the architecture of reasoning.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [113] [ROPES: Robotic Pose Estimation via Score-Based Causal Representation Learning](https://arxiv.org/abs/2510.20884)
*Pranamya Kulkarni,Puranjay Datta,Burak Varıcı,Emre Acartürk,Karthikeyan Shanmugam,Ali Tajer*

Main category: cs.RO

TL;DR: 本文提出了一种无监督因果表示学习方法（ROPES），用于机器人位姿估计。该方法能自动分离影响图像生成的潜在因子（如关节角度、机械臂形状、光照等），并准确恢复可控变量，仅依赖分布变化，无需标注数据。实验结果优于现有半监督方法。


<details>
  <summary>Details</summary>
Motivation: 虽然因果表示学习理论进展迅速，但在实际机器人任务中的应用还不成熟，理论与实际存在差距。机器人位姿估计作为典型任务，既驱动了该领域的发展，也具备天然的因果结构，因此成为验证无监督CRL方法高效性的重要实践场景。

Method: 作者提出ROPES（Robotic Pose Estimation via Score-Based CRL）方法：通过对机器人关节驱动、自动干预，并采集不同控制下的图像，利用因果理论自动分辨可控和不可控的潜在生成因子，并以分布变化为线索实现变量解耦，无需人工标注。

Result: 在半合成机械臂实验中，ROPES能高保真地分离出与机器人控制直接相关的潜在生成因子，恢复效果接近真实值，且完全不依赖标签，且在对比最新半监督方法时表现更优。

Conclusion: 本文将机器人位姿估计实践与因果表示学习理论结合，展示了CRL在机器人领域的实用性，且仅凭无监督设定即可有效实现。作者建议该任务可作为CRL方法实际应用的标准测试平台。

Abstract: Causal representation learning (CRL) has emerged as a powerful unsupervised
framework that (i) disentangles the latent generative factors underlying
high-dimensional data, and (ii) learns the cause-and-effect interactions among
the disentangled variables. Despite extensive recent advances in
identifiability and some practical progress, a substantial gap remains between
theory and real-world practice. This paper takes a step toward closing that gap
by bringing CRL to robotics, a domain that has motivated CRL. Specifically,
this paper addresses the well-defined robot pose estimation -- the recovery of
position and orientation from raw images -- by introducing Robotic Pose
Estimation via Score-Based CRL (ROPES). Being an unsupervised framework, ROPES
embodies the essence of interventional CRL by identifying those generative
factors that are actuated: images are generated by intrinsic and extrinsic
latent factors (e.g., joint angles, arm/limb geometry, lighting, background,
and camera configuration) and the objective is to disentangle and recover the
controllable latent variables, i.e., those that can be directly manipulated
(intervened upon) through actuation. Interventional CRL theory shows that
variables that undergo variations via interventions can be identified. In
robotics, such interventions arise naturally by commanding actuators of various
joints and recording images under varied controls. Empirical evaluations in
semi-synthetic manipulator experiments demonstrate that ROPES successfully
disentangles latent generative factors with high fidelity with respect to the
ground truth. Crucially, this is achieved by leveraging only distributional
changes, without using any labeled data. The paper also includes a comparison
with a baseline based on a recently proposed semi-supervised framework. This
paper concludes by positioning robot pose estimation as a near-practical
testbed for CRL.

</details>


### [114] [Aircraft Collision Avoidance Systems: Technological Challenges and Solutions on the Path to Regulatory Acceptance](https://arxiv.org/abs/2510.20916)
*Sydney M. Katz,Robert J. Moss,Dylan M. Asmar,Wesley A. Olson,James K. Kuchar,Mykel J. Kochenderfer*

Main category: cs.RO

TL;DR: 本文综述了航空器防撞系统中的技术挑战及其解决方案，重点关注经过严格验证并获得监管机构认可的方法。


<details>
  <summary>Details</summary>
Motivation: 随着航空业发展，空中交通密度增加，防撞系统变得尤为重要。如何高效预测潜在碰撞并推荐有效规避措施是亟需解决的关键难题。

Method: 通过文献梳理，作者总结了防撞系统面临的监视、决策和验证等技术难题，并系统回顾了为应对这些问题所提出的多种技术方案，特别强调了经过严格验证和认证的系统。

Result: 文中详细介绍了主流防撞系统及其技术实现，展示了相关系统在监测、决策与验证环节取得的进展及其在航空监管中的实际采纳情况。

Conclusion: 航空防撞系统的研究成果已为其他安全关键领域提供了有益借鉴。持续优化防撞系统对提升整体民航安全意义重大，同时相关方法也具有跨领域应用潜力。

Abstract: Aircraft collision avoidance systems is critical to modern aviation. These
systems are designed to predict potential collisions between aircraft and
recommend appropriate avoidance actions. Creating effective collision avoidance
systems requires solutions to a variety of technical challenges related to
surveillance, decision making, and validation. These challenges have sparked
significant research and development efforts over the past several decades that
have resulted in a variety of proposed solutions. This article provides an
overview of these challenges and solutions with an emphasis on those that have
been put through a rigorous validation process and accepted by regulatory
bodies. The challenges posed by the collision avoidance problem are often
present in other domains, and aircraft collision avoidance systems can serve as
case studies that provide valuable insights for a wide range of safety-critical
systems.

</details>


### [115] [SutureBot: A Precision Framework & Benchmark For Autonomous End-to-End Suturing](https://arxiv.org/abs/2510.20965)
*Jesse Haworth,Juo-Tung Chen,Nigel Nelson,Ji Woong Kim,Masoud Moghani,Chelsea Finn,Axel Krieger*

Main category: cs.RO

TL;DR: 本文提出了SutureBot——一个基于da Vinci Research Kit（dVRK）的自主缝合基准，涵盖针的拾取、组织穿刺和打结等环节，并发布了高质量缝合演示数据集。提出了显式优化插入点精度的目标条件框架，并对多种主流视觉-语言-行动模型进行评估，以推动外科手术机器人的自主化进程。


<details>
  <summary>Details</summary>
Motivation: 目前尚未在物理硬件上实现完全自主的机器人缝合流程。机器人缝合涉及多个连续高难操作，对精度和灵巧度要求极高。社区缺乏代表性的评测标准和高质量数据集，这阻碍了模仿学习算法和精细操作技术的发展。

Method: 1）在dVRK系统上设计自主缝合全流程基准；2）采集并公开1890组高仿真缝合演示数据；3）提出目标条件框架，聚焦提升穿刺插入点精度；4）用该基准评测多种先进VLA（视觉-语言-行动）模型，并结合高阶任务预测策略。

Result: 引入目标条件框架后，穿刺目标精度提升59%-74%；发布了标准化可复现的数据集和完整评测流程；评测了多种最新主流VLA模型，展示了各自优劣与发展空间。

Conclusion: SutureBot推动了外科机器人自主缝合的发展，提供了高质量基准和数据集，为长时序灵巧操作任务的算法研究和精度提升提供了重要支持，是实现手术机器人全流程自主化的重要里程碑。

Abstract: Robotic suturing is a prototypical long-horizon dexterous manipulation task,
requiring coordinated needle grasping, precise tissue penetration, and secure
knot tying. Despite numerous efforts toward end-to-end autonomy, a fully
autonomous suturing pipeline has yet to be demonstrated on physical hardware.
We introduce SutureBot: an autonomous suturing benchmark on the da Vinci
Research Kit (dVRK), spanning needle pickup, tissue insertion, and knot tying.
To ensure repeatability, we release a high-fidelity dataset comprising 1,890
suturing demonstrations. Furthermore, we propose a goal-conditioned framework
that explicitly optimizes insertion-point precision, improving targeting
accuracy by 59\%-74\% over a task-only baseline. To establish this task as a
benchmark for dexterous imitation learning, we evaluate state-of-the-art
vision-language-action (VLA) models, including $\pi_0$, GR00T N1, OpenVLA-OFT,
and multitask ACT, each augmented with a high-level task-prediction policy.
Autonomous suturing is a key milestone toward achieving robotic autonomy in
surgery. These contributions support reproducible evaluation and development of
precision-focused, long-horizon dexterous manipulation policies necessary for
end-to-end suturing. Dataset is available at:
https://huggingface.co/datasets/jchen396/suturebot

</details>


### [116] [Robust Point Cloud Reinforcement Learning via PCA-Based Canonicalization](https://arxiv.org/abs/2510.20974)
*Michael Bezick,Vittorio Giammarino,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: 本论文提出了一种用于强化学习的PCA点云规范化框架PPC，有效提升了点云强化学习在不同摄像头视角下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的强化学习方法对环境外分布变化（如光照、颜色、视角变化）非常敏感。虽然点云强化学习能够减少对外观的敏感性，但仍然容易受到摄像头位姿不匹配的影响，限制了其在实际应用中的可靠性。因此，提升点云强化学习在不同摄像头视角下的鲁棒性具有重要意义。

Method: 提出了一种PCA点云规范化（PPC）方法，通过将任意刚体变换下的点云数据规范化到唯一标准的姿态，实现了观测数据的一致对齐，从而大幅度降低了因视角变化带来的不一致性。该方法专为下游机器人控制任务设计。

Result: 实验证明，在各种具有挑战性的机器人任务中，PPC能够显著提升点云强化学习模型对未知摄像头视角的鲁棒性。

Conclusion: PPC作为一种规范化方法，为机器人控制中的点云强化学习提供了一种比域随机化更为系统、有效的提升鲁棒性的思路。

Abstract: Reinforcement Learning (RL) from raw visual input has achieved impressive
successes in recent years, yet it remains fragile to out-of-distribution
variations such as changes in lighting, color, and viewpoint. Point Cloud
Reinforcement Learning (PC-RL) offers a promising alternative by mitigating
appearance-based brittleness, but its sensitivity to camera pose mismatches
continues to undermine reliability in realistic settings. To address this
challenge, we propose PCA Point Cloud (PPC), a canonicalization framework
specifically tailored for downstream robotic control. PPC maps point clouds
under arbitrary rigid-body transformations to a unique canonical pose, aligning
observations to a consistent frame, thereby substantially decreasing
viewpoint-induced inconsistencies. In our experiments, we show that PPC
improves robustness to unseen camera poses across challenging robotic tasks,
providing a principled alternative to domain randomization.

</details>


### [117] [HRT1: One-Shot Human-to-Robot Trajectory Transfer for Mobile Manipulation](https://arxiv.org/abs/2510.21026)
*Sai Haneesh Allu,Jishnu Jaykumar P,Ninad Khargonkar,Tyler Summers,Jian Yao,Yu Xiang*

Main category: cs.RO

TL;DR: 本文提出了一个新颖的人机轨迹迁移系统，使机器人能够通过学习人类演示视频来操作物体。该系统可让机器人模仿人类，只看一次演示视频，就能在不同环境中复现任务。实验验证了系统的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的机器人编程和学习方法通常要求繁琐的手动设定，或者对人类与机器人的差距处理不足。本研究动机在于创造一种更自然、高效的机器人学习框架，使机器人能直接从人类演示（尤其是视频）中学习复杂操作技能，提高移动操作任务的泛化能力。

Method: 系统由四个模块组成：1）数据采集模块，利用AR头盔从机器人的视角收集演示视频；2）视频理解模块，检测物体并提取3D人手轨迹；3）轨迹转移模块，将人手轨迹转为机器人末端执行器的参考轨迹；4）轨迹优化模块，计算机器人在自身配置空间内可执行的轨迹，使其跟踪转移自人类演示的数据。

Result: 该系统能让机器人仅通过观看一次人类演示视频，就能在不同物体摆放位置下重复执行移动操作任务。实际在多项机器人操作实验中证明了系统的有效性。

Conclusion: 该系统为机器人学习和迁移人类操作技能提供高效可靠的方法，极大提升了机器人对复杂任务的适应力和实际应用能力。

Abstract: We introduce a novel system for human-to-robot trajectory transfer that
enables robots to manipulate objects by learning from human demonstration
videos. The system consists of four modules. The first module is a data
collection module that is designed to collect human demonstration videos from
the point of view of a robot using an AR headset. The second module is a video
understanding module that detects objects and extracts 3D human-hand
trajectories from demonstration videos. The third module transfers a human-hand
trajectory into a reference trajectory of a robot end-effector in 3D space. The
last module utilizes a trajectory optimization algorithm to solve a trajectory
in the robot configuration space that can follow the end-effector trajectory
transferred from the human demonstration. Consequently, these modules enable a
robot to watch a human demonstration video once and then repeat the same mobile
manipulation task in different environments, even when objects are placed
differently from the demonstrations. Experiments of different manipulation
tasks are conducted on a mobile manipulator to verify the effectiveness of our
system

</details>


### [118] [Sequentially Teaching Sequential Tasks $(ST)^2$: Teaching Robots Long-horizon Manipulation Skills](https://arxiv.org/abs/2510.21046)
*Zlatan Ajanović,Ravi Prakash,Leandro de Souza Rosa,Jens Kober*

Main category: cs.RO

TL;DR: 对比了两种机器人长时序任务教学方法（整体式/依次分步），引入了新的顺序学习框架$(ST)^2$。用户实验发现两种方法在效果上相近，但用户偏好有所不同。


<details>
  <summary>Details</summary>
Motivation: 长时序多技能任务的机器人教学中，人类演示面临偏差累积和分布漂移，且传统整体示教方式容易让人类教师疲劳、失败率上升，因此需要优化教学流程。

Method: 提出了$(ST)^2$方法，允许用户对任务分段、控制示教流程，通过逐步提供关键节点演示，进行结构化、增量示教。并在16人零售环境补货任务中，比较了整体式和顺序式教导方法的实际效果和用户反馈。

Result: 两种方法在示教轨迹质量和任务成功率上无显著差别。参与者在主观体验上有分歧：部分人喜欢顺序式（更易控），部分人倾向整体式（更简单直观）。

Conclusion: 顺序式和整体式示教法都能达成相当效果。顺序法为复杂任务示教提供了结构与灵活性，用户偏好多样，建议依据实际需求选择合适方法。

Abstract: Learning from demonstration is effective for teaching robots complex skills
with high sample efficiency. However, teaching long-horizon tasks with multiple
skills is difficult, as deviations accumulate, distributional shift increases,
and human teachers become fatigued, raising the chance of failure. In this
work, we study user responses to two teaching frameworks: (i) a traditional
monolithic approach, where users demonstrate the entire trajectory of a
long-horizon task; and (ii) a sequential approach, where the task is segmented
by the user and demonstrations are provided step by step. To support this
study, we introduce $(ST)^2$, a sequential method for learning long-horizon
manipulation tasks that allows users to control the teaching flow by defining
key points, enabling incremental and structured demonstrations. We conducted a
user study on a restocking task with 16 participants in a realistic retail
environment to evaluate both user preference and method effectiveness. Our
objective and subjective results show that both methods achieve similar
trajectory quality and success rates. Some participants preferred the
sequential approach for its iterative control, while others favored the
monolithic approach for its simplicity.

</details>


### [119] [Revisiting Replanning from Scratch: Real-Time Incremental Planning with Fast Almost-Surely Asymptotically Optimal Planners](https://arxiv.org/abs/2510.21074)
*Mitchell E. C. Sabbadini,Andrew H. Liu,Joseph Ruan,Tyler S. Wilson,Zachary Kingston,Jonathan D. Gammell*

Main category: cs.RO

TL;DR: 本文提出了一种新的针对动态环境中机器人路径规划的方法，即不再依赖对旧有路径的显式更新，而是将重规划问题视为一系列独立问题，通过速度快且近似最优的（ASAO）规划算法解决。实验证明该法在仿真和现实任务上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 在动态或未知环境中，机器人路径规划面临障碍物不断变化的挑战。传统的方法分为预测型和反应型，但各有局限。反应型方法在环境发生变化时需要迅速且高效地产生新路径，但现有技术在更新复杂规划图时计算量大，且检测环境变化本身也难以实现。因此，亟需更加高效和实用的动态路径重规划方法。

Method: 该论文不再采用“更新旧有路径”的方式进行重规划，而是将每次路径规划视为独立任务，并采用近似最优且收敛迅速的ASAO（Almost-Surely Asymptotically Optimal）规划算法，如Effort Informed Trees（EIT*）和Asymptotically Optimal RRT-Connect（AORRTC）。通过这种方式，能够有效应对障碍物的动态变化，无需复杂的计划重用与更新。

Result: 实验证明，EIT*方法在仿真环境下生成的路径比现有的反应式规划算法更短，效率更高；AORRTC在实际机器人臂路径规划任务中也获得了有效验证。

Conclusion: 无需对原有路径显式修改，利用ASAO系列算法把动态路径规划拆分为独立子问题，可以实现高效、鲁棒的全局路径搜索，优于传统反应式重规划方法。

Abstract: Robots operating in changing environments either predict obstacle changes
and/or plan quickly enough to react to them. Predictive approaches require a
strong prior about the position and motion of obstacles. Reactive approaches
require no assumptions about their environment but must replan quickly and find
high-quality paths to navigate effectively.
  Reactive approaches often reuse information between queries to reduce
planning cost. These techniques are conceptually sound but updating dense
planning graphs when information changes can be computationally prohibitive. It
can also require significant effort to detect the changes in some applications.
  This paper revisits the long-held assumption that reactive replanning
requires updating existing plans. It shows that the incremental planning
problem can alternatively be solved more efficiently as a series of independent
problems using fast almost-surely asymptotically optimal (ASAO) planning
algorithms. These ASAO algorithms quickly find an initial solution and converge
towards an optimal solution which allows them to find consistent global plans
in the presence of changing obstacles without requiring explicit plan reuse.
This is demonstrated with simulated experiments where Effort Informed Trees
(EIT*) finds shorter median solution paths than the tested reactive planning
algorithms and is further validated using Asymptotically Optimal RRT-Connect
(AORRTC) on a real-world planning problem on a robot arm.

</details>


### [120] [Generalizable Hierarchical Skill Learning via Object-Centric Representation](https://arxiv.org/abs/2510.21121)
*Haibo Zhao,Yu Qi,Boce Hu,Yizhe Zhu,Ziyan Chen,Heng Tian,Xupeng Zhu,Owen Howell,Haojie Huang,Robin Walters,Dian Wang,Robert Platt*

Main category: cs.RO

TL;DR: 提出了一种新的分层策略学习框架GSL（Generalizable Hierarchical Skill Learning），显著提升了机器人操作任务中的泛化能力和样本效率。


<details>
  <summary>Details</summary>
Motivation: 机器人操作任务通常面临泛化困难和样本效率低的问题，现有方法往往在新场景、新物体或新任务组合下表现不佳，急需一种能够提升泛化能力且样本利用高效的学习方法。

Method: GSL框架将高层视觉-语言模型与低层视觉-运动策略通过以物体为中心的技能（object-centric skills）接口连接起来。它利用基础模型将演示分解为可转移且物体规范化的技能原语，使低层技能学习能在物体参考系内高效进行。在测试时，高层预测的技能-物体对输入到低层模块，低层推理出的规范动作再映射回世界坐标执行。

Result: 在仿真中，GSL每个任务仅用3条演示就性能超越了用30倍样本量训练的基线，在未见过的任务上提升15.5%。现实世界实验中，GSL同样超过了用10倍数据训练的基线。

Conclusion: GSL通过结构化且灵活的框架，显著提升了机器人操作任务的泛化能力及样本效率，在仿真和现实中均获得了优异的实验结果。

Abstract: We present Generalizable Hierarchical Skill Learning (GSL), a novel framework
for hierarchical policy learning that significantly improves policy
generalization and sample efficiency in robot manipulation. One core idea of
GSL is to use object-centric skills as an interface that bridges the high-level
vision-language model and the low-level visual-motor policy. Specifically, GSL
decomposes demonstrations into transferable and object-canonicalized skill
primitives using foundation models, ensuring efficient low-level skill learning
in the object frame. At test time, the skill-object pairs predicted by the
high-level agent are fed to the low-level module, where the inferred canonical
actions are mapped back to the world frame for execution. This structured yet
flexible design leads to substantial improvements in sample efficiency and
generalization of our method across unseen spatial arrangements, object
appearances, and task compositions. In simulation, GSL trained with only 3
demonstrations per task outperforms baselines trained with 30 times more data
by 15.5 percent on unseen tasks. In real-world experiments, GSL also surpasses
the baseline trained with 10 times more data.

</details>


### [121] [An Agnostic End-Effector Alignment Controller for Robust Assembly of Modular Space Robots](https://arxiv.org/abs/2510.21164)
*Shamistan Karimov,Elian Neppel,Shreya Santra,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 该论文提出了一种适用于登月任务的模块化机器人新型控制器，通过动态高维球体限制自适应速度，实现平稳且高精度的末端执行器轨迹跟踪。


<details>
  <summary>Details</summary>
Motivation: 模块化机器人需要在恶劣和未知的月球环境中灵活重构，自主适应各类扰动和故障，因此迫切需要一种无需依赖详细硬件建模，且能在物理不完美和传感器噪声影响下依然鲁棒的控制方法。

Method: 开发了一种利用动态高维球体限制速度的控制算法，仅依据实时末端执行器和目标位姿测量，自适应地调整机器人各关节的速度上下界。实现了离散步进式和连续速度式两个算法版本，并在JAXA月球环境模拟器中的MoonBot机器人上进行了对比实验。

Result: 离散步进式控制器带来可高度预测、低抖动的移动效果，连续速度式收敛更快，并能保持毫米级的准确度。两种方法都在面对不同机械结构缺陷（如间隙和柔顺性）及传感器噪声时保持鲁棒性。

Conclusion: 提出的硬件无关、自适应同步控制方法在月球环境等极端条件下展现出良好的柔性和鲁棒性，适用于模块化机器人的自主自组装和重构任务。

Abstract: Modular robots offer reconfigurability and fault tolerance essential for
lunar missions, but require controllers that adapt safely to real-world
disturbances. We build on our previous hardware-agnostic actuator
synchronization in Motion Stack to develop a new controller enforcing adaptive
velocity bounds via a dynamic hypersphere clamp. Using only real-time
end-effector and target pose measurements, the controller adjusts its
translational and rotational speed limits to ensure smooth, stable alignment
without abrupt motions. We implemented two variants, a discrete, step-based
version and a continuous, velocity-based version, and tested them on two
MoonBot limbs in JAXA's lunar environment simulator. Field trials demonstrate
that the step-based variant produces highly predictable, low-wobble motions,
while the continuous variant converges more quickly and maintains
millimeter-level positional accuracy, and both remain robust across limbs with
differing mechanical imperfections and sensing noise (e.g., backlash and flex).
These results highlight the flexibility and robustness of our robot-agnostic
framework for autonomous self-assembly and reconfiguration under harsh
conditions.

</details>


### [122] [Underwater Visual-Inertial-Acoustic-Depth SLAM with DVL Preintegration for Degraded Environments](https://arxiv.org/abs/2510.21215)
*Shuoshuo Ding,Tiedong Zhang,Dapeng Jiang,Ming Lei*

Main category: cs.RO

TL;DR: 本文提出一种结合视觉、惯性、声学和深度信息的多传感器水下SLAM系统，在恶劣视觉条件下表现优异，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 水下视觉环境受能见度、光照和特征稀缺等因素影响，限制了基于视觉惯性的SLAM系统性能。亟需融合多种传感器以提升定位与建图的鲁棒性与准确性。

Method: 系统整合立体相机、IMU、多普勒速度计(DVL)和压力传感器，通过图优化实现紧密融合。创新点包括基于速度偏置的DVL预积分策略、混合前端跟踪与声学-惯性-深度联合优化，并将多源残差纳入图优化框架。

Result: 在仿真与真实水下场景中进行了全面定量和定性分析，所提方法在定位稳定性和精度上明显优于现有立体视觉惯性SLAM系统。

Conclusion: 该多模态紧耦合SLAM系统在恶劣水下视觉环境下展现出极高的鲁棒性与优越的定位表现，具有广阔的实际应用前景。

Abstract: Visual degradation caused by limited visibility, insufficient lighting, and
feature scarcity in underwater environments presents significant challenges to
visual-inertial simultaneous localization and mapping (SLAM) systems. To
address these challenges, this paper proposes a graph-based
visual-inertial-acoustic-depth SLAM system that integrates a stereo camera, an
inertial measurement unit (IMU), the Doppler velocity log (DVL), and a pressure
sensor. The key innovation lies in the tight integration of four distinct
sensor modalities to ensure reliable operation, even under degraded visual
conditions. To mitigate DVL drift and improve measurement efficiency, we
propose a novel velocity-bias-based DVL preintegration strategy. At the
frontend, hybrid tracking strategies and acoustic-inertial-depth joint
optimization enhance system stability. Additionally, multi-source hybrid
residuals are incorporated into a graph optimization framework. Extensive
quantitative and qualitative analyses of the proposed system are conducted in
both simulated and real-world underwater scenarios. The results demonstrate
that our approach outperforms current state-of-the-art stereo visual-inertial
SLAM systems in both stability and localization accuracy, exhibiting
exceptional robustness, particularly in visually challenging environments.

</details>


### [123] [Remote Autonomy for Multiple Small Lowcost UAVs in GNSS-denied Search and Rescue Operations](https://arxiv.org/abs/2510.21357)
*Daniel Schleich,Jan Quenzel,Sven Behnke*

Main category: cs.RO

TL;DR: 本文提出了一种利用轻量级消费级DJI无人机的自主飞行系统，依赖安卓App实现状态估计和避障，可支持单人操作多个异构无人机，并将多机观测数据融合生成联合三维环境模型。


<details>
  <summary>Details</summary>
Motivation: 现有消费级无人机主要依赖手动操作，尤其在GNSS失效和复杂环境下需要经验丰富的飞手，增加了操作难度与负担。自主飞行虽可简化操作，但现有方案往往需要定制硬件和高算力设备，限制了应用推广。该研究旨在降低消费级无人机自主飞行的门槛，扩展其应用场景。

Method: 系统基于现有轻量级DJI无人机，通过安卓应用在遥控器本地实现状态估计和障碍物规避功能。地面控制站允许单人同时配置和监控多台异构无人机，并可将各无人机采集数据整合，生成联合的三维环境感知模型。

Result: 实验系统实现了对多台消费级DJI无人机的自主控制和协作感知，能有效支持单人对多机的操作与监控，完成了复杂场景下的三维环境建模，提升了态势感知能力。

Conclusion: 提出的系统简化了无人机自主飞行的部署与应用要求，不依赖专用硬件，降低了操作门槛，有望推动消费级无人机在应急、救援等多种GNSS受限场景中的广泛应用。

Abstract: In recent years, consumer-grade UAVs have been widely adopted by first
responders. In general, they are operated manually, which requires trained
pilots, especially in unknown GNSS-denied environments and in the vicinity of
structures. Autonomous flight can facilitate the application of UAVs and reduce
operator strain. However, autonomous systems usually require special
programming interfaces, custom sensor setups, and strong onboard computers,
which limits a broader deployment.
  We present a system for autonomous flight using lightweight consumer-grade
DJI drones. They are controlled by an Android app for state estimation and
obstacle avoidance directly running on the UAV's remote control. Our ground
control station enables a single operator to configure and supervise multiple
heterogeneous UAVs at once. Furthermore, it combines the observations of all
UAVs into a joint 3D environment model for improved situational awareness.

</details>


### [124] [Load-bearing Assessment for Safe Locomotion of Quadruped Robots on Collapsing Terrain](https://arxiv.org/abs/2510.21369)
*Vivian S. Medeiros,Giovanni B. Dessy,Thiago Boaventura,Marcelo Becker,Claudio Semini,Victor Barasuol*

Main category: cs.RO

TL;DR: 该论文提出了一种适用于塌陷地形的四足机器人鲁棒行走框架，通过结合地形探测、承重分析、运动规划与控制，使机器人能安全穿越不稳定表面。无需额外硬件，仅通过关节测量评估地形稳定性，并能实时调整落足点。实验证明该方法在模拟塌陷与岩石地形中效果良好。


<details>
  <summary>Details</summary>
Motivation: 塌陷地形频繁出现在救援、行星探测等场景，极大挑战四足机器人移动的安全与稳定性。现有方法多依赖特殊传感器或外部地形建图，成本高、通用性差，因此亟需无需额外硬件的高适应性解决方案。

Method: 本方法通过关节测量数据判断地形承重能力；引入MPC（模型预测控制）以权衡稳定性与探测动作；设计状态机统筹探地与步态调整，使机器人能主动探测并避开塌陷区域，并实时动态调整落足点。

Result: 在自制塌陷平台和岩石堆等代表性典型地形上开展实验，机器人能成功检测不稳定区域、动态变更行走路径，并有效避免塌陷失稳，实现安全高效通过。

Conclusion: 框架无需专用传感器或地貌重建，仅凭关节传感器即可实现塌陷地形下的鲁棒行走，具备广泛的实际应用潜力和强适应性。

Abstract: Collapsing terrains, often present in search and rescue missions or planetary
exploration, pose significant challenges for quadruped robots. This paper
introduces a robust locomotion framework for safe navigation over unstable
surfaces by integrating terrain probing, load-bearing analysis, motion
planning, and control strategies. Unlike traditional methods that rely on
specialized sensors or external terrain mapping alone, our approach leverages
joint measurements to assess terrain stability without hardware modifications.
A Model Predictive Control (MPC) system optimizes robot motion, balancing
stability and probing constraints, while a state machine coordinates terrain
probing actions, enabling the robot to detect collapsible regions and
dynamically adjust its footholds. Experimental results on custom-made
collapsing platforms and rocky terrains demonstrate the framework's ability to
traverse collapsing terrain while maintaining stability and prioritizing
safety.

</details>


### [125] [PREVENT: Proactive Risk Evaluation and Vigilant Execution of Tasks for Mobile Robotic Chemists using Multi-Modal Behavior Trees](https://arxiv.org/abs/2510.21438)
*Satheeshkumar Veeramani,Zhengxue Zhou,Francisco Munguia-Galeano,Hatem Fakhruldeen,Thomas Roddelkopf,Mohammed Faeik Ruzaij Al-Okby,Kerstin Thurow,Andrew Ian Cooper*

Main category: cs.RO

TL;DR: 提出了PREVENT系统，结合多模态行为树方法，实现移动化学机器人更高效、可靠的实验流程感知，极大降低异常的误报和漏报，实验验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前移动化学机器人在实验中缺乏流程感知，轻微异常如样品瓶封盖不当，可能中断整个流程，浪费资源、影响安全。现有感知方法易造成误报，影响自动化价值。

Method: 提出PREVENT系统，基于多模态行为树（BT）架构，整合导航与操作技能，通过分层感知结合AI与传感反馈（Dexterous Vision、导航摄像头及IoT气体传感器），可与现有软件架构低成本集成。

Result: 在仿真高风险化学实验流程中，PREVENT系统完全避免了误报与漏报，比单一感知方式具备更高的部署准确率，且适用于导航和操作环节。

Conclusion: PREVENT系统提升了移动化学机器人对异常的感知和自适应能力，保障实验流程连续性与研究者安全，在实际部署上显示出更强的可靠性与高效性。

Abstract: Mobile robotic chemists are a fast growing trend in the field of chemistry
and materials research. However, so far these mobile robots lack workflow
awareness skills. This poses the risk that even a small anomaly, such as an
improperly capped sample vial could disrupt the entire workflow. This wastes
time, and resources, and could pose risks to human researchers, such as
exposure to toxic materials. Existing perception mechanisms can be used to
predict anomalies but they often generate excessive false positives. This may
halt workflow execution unnecessarily, requiring researchers to intervene and
to resume the workflow when no problem actually exists, negating the benefits
of autonomous operation. To address this problem, we propose PREVENT a system
comprising navigation and manipulation skills based on a multimodal Behavior
Tree (BT) approach that can be integrated into existing software architectures
with minimal modifications. Our approach involves a hierarchical perception
mechanism that exploits AI techniques and sensory feedback through Dexterous
Vision and Navigational Vision cameras and an IoT gas sensor module for
execution-related decision-making. Experimental evaluations show that the
proposed approach is comparatively efficient and completely avoids both false
negatives and false positives when tested in simulated risk scenarios within
our robotic chemistry workflow. The results also show that the proposed
multi-modal perception skills achieved deployment accuracies that were higher
than the average of the corresponding uni-modal skills, both for navigation and
for manipulation.

</details>


### [126] [Enhancing Social Robots through Resilient AI](https://arxiv.org/abs/2510.21469)
*Domenico Palmisano,Giuseppe Palestra,Berardina Nadja De Carolis*

Main category: cs.RO

TL;DR: 本文强调社会机器人在关键场景中需具备韧性，以提升用户信任，特别针对老年用户对AI系统的信任难题。


<details>
  <summary>Details</summary>
Motivation: 人工智能广泛应用于医疗、教育等敏感领域，要求系统具备更高的稳定性与可靠性，尤其面对用户信任度较低的老年群体，如何确保社会机器人被信赖，是亟需解决的问题。

Method: 通过分析社会机器人的韧性特征，探究其在逆境或受限条件下保持基本功能的能力，从而提升用户信任。

Result: 研究发现，社会机器人展现出的韧性行为有助于在压力和受损状态下维持关键操作，这种能力能够明显增强老年用户对机器人的信任。

Conclusion: 社会机器人的韧性设计对于在老年人等特殊用户群体中建立信任至关重要，是AI系统可靠性的核心组成部分。

Abstract: As artificial intelligence continues to advance and becomes more integrated
into sensitive areas like healthcare, education, and everyday life, it's
crucial for these systems to be both resilient and robust. This paper shows how
resilience is a fundamental characteristic of social robots, which, through it,
ensure trust in the robot itself-an essential element especially when operating
in contexts with elderly people, who often have low trust in these systems.
Resilience is therefore the ability to operate under adverse or stressful
conditions, even when degraded or weakened, while maintaining essential
operational capabilities.

</details>


### [127] [AURASeg: Attention Guided Upsampling with Residual Boundary-Assistive Refinement for Drivable-Area Segmentation](https://arxiv.org/abs/2510.21536)
*Narendhiran Vijayakumar,Sridevi. M*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的地面语义分割模型AURASeg，解决了现有模型在室内和结构化环境下难以准确分割细粒度特征和边界的问题。


<details>
  <summary>Details</summary>
Motivation: 现有地面分割模型在精准提取多尺度特征、精细化边界与提升特征表达方面存在不足，限制了机器人室内外导航与通行的准确性和效率。

Method: 提出AURASeg模型，采用CSP-Darknet主干网络，辅以残差边界细化模块（RBRM）增强边缘信息，并通过注意力引导的逐步上采样解码器（APUD）实现多尺度特征集成，同时引入轻量级空洞空间金字塔池化模块（ASPP-Lite）以保证多尺度上下文提取与实时性。

Result: AURASeg在GMRP数据集和自定义Gazebo室内数据集上，相较于主流分割架构mIoU提升1.26%，分割精度提升1.65%。

Conclusion: 该方法在保证推理速度的前提下，在室内外环境均能实现更高精度的边界分割，为自主机器人感知任务提供了有力支撑。

Abstract: Free space ground segmentation is essential to navigate robots and autonomous
vehicles, recognize drivable zones, and traverse efficiently. Fine-grained
features remain challenging for existing segmentation models, particularly for
robots in indoor and structured environments. These difficulties arise from
ineffective multi-scale processing, suboptimal boundary refinement, and limited
feature representation. In order to overcome these limitations, we propose
Attention-Guided Upsampling with Residual Boundary-Assistive Refinement
(AURASeg), a ground-plane semantic segmentation model that maintains high
segmentation accuracy while improving border precision. Our method uses
CSP-Darknet backbone by adding a Residual Border Refinement Module (RBRM) for
accurate edge delineation and an Attention Progressive Upsampling Decoder
(APUD) for strong feature integration. We also incorporate a lightweight Atrous
Spatial Pyramid Pooling (ASPP-Lite) module to ensure multi-scale context
extraction without compromising real-time performance. The proposed model beats
benchmark segmentation architectures in mIoU and F1 metrics when tested on the
Ground Mobile Robot Perception (GMRP) Dataset and a custom Gazebo indoor
dataset. Our approach achieves an improvement in mean Intersection-over-Union
(mIoU) of +1.26% and segmentation precision of +1.65% compared to
state-of-the-art models. These results show that our technique is feasible for
autonomous perception in both indoor and outdoor environments, enabling precise
border refinement with minimal effect on inference speed.

</details>


### [128] [Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos](https://arxiv.org/abs/2510.21571)
*Qixiu Li,Yu Deng,Yaobo Liang,Lin Luo,Lei Zhou,Chengtang Yao,Lingqi Zeng,Zhiyuan Feng,Huizhi Liang,Sicheng Xu,Yizhong Zhang,Xi Chen,Hao Chen,Lily Sun,Dong Chen,Jiaolong Yang,Baining Guo*

Main category: cs.RO

TL;DR: 提出了一种创新的方法，利用真实人类手部活动的视频无监督预训练机器人操控视觉-语言-动作（VLA）模型，并取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有机器人VLA模型缺乏大规模、真实、多样化的训练数据，难以泛化到现实复杂任务。本研究希望通过利用无标注的人类第一人称手部视频，生成高质量的机器人操控预训练数据，提升模型通用性和能力。

Method: 作者开发了一套全自动人类手部活动分析框架，将“野外”真人第一视角视频转化为与机器人VLA预训练格式一致的数据，包括动作原子分割、语言描述、帧级三维手部与摄像机运动。处理生成百万级段落、千万级帧数据，涵盖丰富场景与任务。设计新颖的灵巧手VLA模型并在该数据集上预训练。

Result: 预训练后模型在零样本识别实际场景中表现出色，微调少量机器人动作数据后，任务成功率和迁移泛化能力进一步提升。实验还显示，模型性能随预训练数据规模增长显著增强。

Conclusion: 此项工作提出了可扩展的VLA预训练新范式，为实现通用、具身智能的机器人奠定了坚实基础。

Abstract: This paper presents a novel approach for pretraining robotic manipulation
Vision-Language-Action (VLA) models using a large corpus of unscripted
real-life video recordings of human hand activities. Treating human hand as
dexterous robot end-effector, we show that "in-the-wild" egocentric human
videos without any annotations can be transformed into data formats fully
aligned with existing robotic V-L-A training data in terms of task granularity
and labels. This is achieved by the development of a fully-automated holistic
human activity analysis approach for arbitrary human hand videos. This approach
can generate atomic-level hand activity segments and their language
descriptions, each accompanied with framewise 3D hand motion and camera motion.
We process a large volume of egocentric videos and create a hand-VLA training
dataset containing 1M episodes and 26M frames. This training data covers a wide
range of objects and concepts, dexterous manipulation tasks, and environment
variations in real life, vastly exceeding the coverage of existing robot data.
We design a dexterous hand VLA model architecture and pretrain the model on
this dataset. The model exhibits strong zero-shot capabilities on completely
unseen real-world observations. Additionally, fine-tuning it on a small amount
of real robot action data significantly improves task success rates and
generalization to novel objects in real robotic experiments. We also
demonstrate the appealing scaling behavior of the model's task performance with
respect to pretraining data scale. We believe this work lays a solid foundation
for scalable VLA pretraining, advancing robots toward truly generalizable
embodied intelligence.

</details>


### [129] [Enhancing Tactile-based Reinforcement Learning for Robotic Control](https://arxiv.org/abs/2510.21609)
*Elle Miller,Trevor McInroe,David Abel,Oisin Mac Aodha,Sethu Vijayakumar*

Main category: cs.RO

TL;DR: 本文提出通过自监督学习方法高效利用机器人触觉传感数据，以提升强化学习在实际操作任务中的表现，并开发了新的基准测试套件。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作主要依赖视觉和理想状态信息，缺乏对触觉的有效利用，导致在复杂任务下表现有限，亟需突破触觉利用这一瓶颈。

Method: 作者提出了结合本体感受和稀疏二值触觉信号的自监督学习方法，同时探索将自监督学习记忆与在线策略记忆分开的方案，并构建了标准化基准（Robot Tactile Olympiad, RoTO）。

Result: 实验证明稀疏二值触觉信号对于提升机器人灵巧性至关重要，尤其是在本体感受无法察觉的场景中。智能体在复杂任务中表现出超越人类的灵巧性，并且分离记忆结构提升了训练效果。

Conclusion: 触觉信号尤其是稀疏二值信号，对于强化学习中的机器人操控任务至关重要，所提方法与基准规范化了相关研究并促进该领域发展。

Abstract: Achieving safe, reliable real-world robotic manipulation requires agents to
evolve beyond vision and incorporate tactile sensing to overcome sensory
deficits and reliance on idealised state information. Despite its potential,
the efficacy of tactile sensing in reinforcement learning (RL) remains
inconsistent. We address this by developing self-supervised learning (SSL)
methodologies to more effectively harness tactile observations, focusing on a
scalable setup of proprioception and sparse binary contacts. We empirically
demonstrate that sparse binary tactile signals are critical for dexterity,
particularly for interactions that proprioceptive control errors do not
register, such as decoupled robot-object motions. Our agents achieve superhuman
dexterity in complex contact tasks (ball bouncing and Baoding ball rotation).
Furthermore, we find that decoupling the SSL memory from the on-policy memory
can improve performance. We release the Robot Tactile Olympiad (RoTO) benchmark
to standardise and promote future research in tactile-based manipulation.
Project page: https://elle-miller.github.io/tactile_rl

</details>


### [130] [Design and Structural Validation of a Micro-UAV with On-Board Dynamic Route Planning](https://arxiv.org/abs/2510.21648)
*Inbazhagan Ravikumar,Ram Sundhar,Narendhiran Vijayakumar*

Main category: cs.RO

TL;DR: 本文提出了一种低成本、易组装、结构耐用并具备自主导航能力的微型无人机，用于现实中的搜救任务。


<details>
  <summary>Details</summary>
Motivation: 现有低价微型无人机在恶劣环境飞行时缺乏足够的结构耐久性，且动态路径规划能力不足，限制了其在搜救任务中的实际应用。

Method: 从零开始自制无人机，全部采用常见部件和材料，并注重模块化、低成本及易装配。机体结构使用轻质耐用材料加固，控制系统全部基于免费开源软件，无需昂贵硬件加速器实现实时感知与自适应导航。

Result: 实验表明所提出系统能在无额外高价硬件帮助下，完成实时感知与动态路径规划，在粗糙地形和新障碍、目标出现时也能有效执行搜救任务。

Conclusion: 提出的无人机方案为现实世界搜救任务提供了兼具成本效益与实用性的技术选择，显著提升了低价无人机的耐久性和自主性。

Abstract: Micro aerial vehicles are becoming increasingly important in search and
rescue operations due to their agility, speed, and ability to access confined
spaces or hazardous areas. However, designing lightweight aerial systems
presents significant structural, aerodynamic, and computational challenges.
This work addresses two key limitations in many low-cost aerial systems under
two kilograms: their lack of structural durability during flight through rough
terrains and inability to replan paths dynamically when new victims or
obstacles are detected. We present a fully customised drone built from scratch
using only commonly available components and materials, emphasising modularity,
low cost, and ease of assembly. The structural frame is reinforced with
lightweight yet durable materials to withstand impact, while the onboard
control system is powered entirely by free, open-source software solutions. The
proposed system demonstrates real-time perception and adaptive navigation
capabilities without relying on expensive hardware accelerators, offering an
affordable and practical solution for real-world search and rescue missions.

</details>
