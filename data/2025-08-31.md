<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 98]
- [cs.CL](#cs.CL) [Total: 47]
- [cs.RO](#cs.RO) [Total: 23]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization](https://arxiv.org/abs/2508.20181)
*Alberto Compagnoni,Davide Caffagni,Nicholas Moratelli,Lorenzo Baraldi,Marcella Cornia,Rita Cucchiara*

Main category: cs.CV

TL;DR: 该论文提出了一种基于CHAIR指标的直接偏好优化（CHAIR-DPO）方法，有效减少多模态大模型（MLLM）在生成视觉相关内容时出现的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型虽然在多个任务上表现优异，但普遍存在幻觉问题，即生成内容中包含与视觉输入不符的信息。如何减少这种幻觉现象仍是亟需解决的问题。

Method: 作者提出将CHAIR指标引入至偏好学习中，对比成对生成结果，用CHAIR区分幻觉与非幻觉答案，利用Direct Preference Optimization（DPO）对现有MLLM微调，无需构建复杂的合成数据集或依赖专有模型。

Result: 在多个幻觉评测基准上，CHAIR-DPO方法能有效降低MLLM生成幻觉内容的比例。

Conclusion: 基于CHAIR指标的细粒度偏好优化能显著改善MLLM的幻觉问题，是一种高效且可复现的模型微调方案。

Abstract: Multimodal Large Language Models (MLLMs) emerge as a unified interface to
address a multitude of tasks, ranging from NLP to computer vision. Despite
showcasing state-of-the-art results in many benchmarks, a long-standing issue
is the tendency of MLLMs to hallucinate, that is to generate answers to the
user's query that are not reflected in the visual input. In this paper, we
address the problem of hallucinations as an alignment problem, seeking to steer
the MLLM so that it prefers generating content without hallucinations. In
contrast to recent approaches that require complicated pipelines to build
synthetic preference data for alignment training, often relying on proprietary
models, we capitalize on the well-known CHAIR metric, originally proposed to
gauge the degree of hallucinations in image captioning. Given a pair of
generated answers, we leverage CHAIR to distinguish winner and loser options
(i.e., non-hallucinated and hallucinated samples) and fine-tune off-the-shelf
MLLMs via Direct Preference Optimization (DPO). The resulting method, which we
refer to as CHAIR-DPO, effectively diminishes the amount of hallucinated
answers on several hallucination benchmarks, demonstrating the effectiveness of
fine-tuning the MLLM with a CHAIR-based reward. Source code and trained models
are publicly available at https://github.com/aimagelab/CHAIR-DPO.

</details>


### [2] [SDiFL: Stable Diffusion-Driven Framework for Image Forgery Localization](https://arxiv.org/abs/2508.20182)
*Yang Su,Shunquan Tan,Jiwu Huang*

Main category: cs.CV

TL;DR: 本文提出了一种基于Stable Diffusion（SD）多模态大模型的新型图像伪造定位方法，通过集成SD的图像生成与感知能力，实现更高效、准确的图像伪造检测。通过在潜空间中引入高频残差信号作为新的模态提升检测性能，实验结果表明该方法相较现有技术有显著提升。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大模型（如Stable Diffusion）推动图像生成与编辑技术快速发展，传统高度依赖人工标注数据的图像取证方法难以应对新兴的、复杂的图像伪造技术。因此亟需开发创新的、无需大量标注数据且能适应新型伪造技术的伪造定位方法。

Method: 作者首次将Stable Diffusion模型的生成与感知能力引入图像伪造定位框架。理论上证明了SD可通过伪造相关条件输出定位结果。具体做法是以SD3为基础，结合高频残余（通过高通滤波器提取）作为显式新模态，并在训练时将其融合进潜空间，从而提升伪造定位精度，同时保持输入图像的语义信息。

Result: 在广泛使用的基准数据集上，该方法相较于当前最先进的图像伪造定位模型有高达12%的性能提升。在处理真实文档伪造和自然场景伪造任务时（这些数据未出现在训练集中），模型依然表现优异。

Conclusion: 该方法不仅提升了伪造图像定位的准确性和普适性，还能适应多种真实世界伪造场景，是图像取证领域的重要技术进步。

Abstract: Driven by the new generation of multi-modal large models, such as Stable
Diffusion (SD), image manipulation technologies have advanced rapidly, posing
significant challenges to image forensics. However, existing image forgery
localization methods, which heavily rely on labor-intensive and costly
annotated data, are struggling to keep pace with these emerging image
manipulation technologies. To address these challenges, we are the first to
integrate both image generation and powerful perceptual capabilities of SD into
an image forensic framework, enabling more efficient and accurate forgery
localization. First, we theoretically show that the multi-modal architecture of
SD can be conditioned on forgery-related information, enabling the model to
inherently output forgery localization results. Then, building on this
foundation, we specifically leverage the multimodal framework of Stable
DiffusionV3 (SD3) to enhance forgery localization performance.We leverage the
multi-modal processing capabilities of SD3 in the latent space by treating
image forgery residuals -- high-frequency signals extracted using specific
highpass filters -- as an explicit modality. This modality is fused into the
latent space during training to enhance forgery localization performance.
Notably, our method fully preserves the latent features extracted by SD3,
thereby retaining the rich semantic information of the input image.
Experimental results show that our framework achieves up to 12% improvements in
performance on widely used benchmarking datasets compared to current
state-of-the-art image forgery localization models. Encouragingly, the model
demonstrates strong performance on forensic tasks involving real-world document
forgery images and natural scene forging images, even when such data were
entirely unseen during training.

</details>


### [3] [Grounding Multimodal Large Language Models with Quantitative Skin Attributes: A Retrieval Study](https://arxiv.org/abs/2508.20188)
*Max Torop,Masih Eskandar,Nicholas Kurtansky,Jinyang Liu,Jochen Weber,Octavia Camps,Veronica Rotemberg,Jennifer Dy,Kivanc Kose*

Main category: cs.CV

TL;DR: 本论文融合多模态大语言模型（MLLMs）与定量属性，通过引入定量皮损特征并结合自然语言解释，提升AI皮肤疾病诊断模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管AI模型已在皮肤病诊断中取得重大成果，但模型的可解释性不足，限制其在临床实际中的应用。提升模型对诊断逻辑的自然语言解释能力及其与可量化医学特征的关联，有助于增强医生对AI决策的信任和接受度。

Method: 本研究结合MLLMs与皮损定量属性（如患处面积等），通过对MLLM进行微调，使其embedding空间能够表达并预测与皮损相关的重要定量属性。实验采用SLICE-3D数据集，通过属性导向的内容检索任务来评估embedding空间与真实属性的关联与解释能力。

Result: 实验证明，MLLM的embedding空间经微调后能够准确地表达与皮损定量属性相关的信息，实现属性驱动的图像检索，验证了其与定量医学概念的可对齐性。

Conclusion: 将MLLM与医学定量属性结合可提升皮肤病AI诊断系统的可解释性，为后续将AI模型更安全、透明地应用于临床实践提供了切实可行的技术路径。

Abstract: Artificial Intelligence models have demonstrated significant success in
diagnosing skin diseases, including cancer, showing the potential to assist
clinicians in their analysis. However, the interpretability of model
predictions must be significantly improved before they can be used in practice.
To this end, we explore the combination of two promising approaches: Multimodal
Large Language Models (MLLMs) and quantitative attribute usage. MLLMs offer a
potential avenue for increased interpretability, providing reasoning for
diagnosis in natural language through an interactive format. Separately, a
number of quantitative attributes that are related to lesion appearance (e.g.,
lesion area) have recently been found predictive of malignancy with high
accuracy. Predictions grounded as a function of such concepts have the
potential for improved interpretability. We provide evidence that MLLM
embedding spaces can be grounded in such attributes, through fine-tuning to
predict their values from images. Concretely, we evaluate this grounding in the
embedding space through an attribute-specific content-based image retrieval
case study using the SLICE-3D dataset.

</details>


### [4] [Enhancing Automatic Modulation Recognition With a Reconstruction-Driven Vision Transformer Under Limited Labels](https://arxiv.org/abs/2508.20193)
*Hossein Ahmadi,Banafsheh Saffari*

Main category: cs.CV

TL;DR: 本文提出了一种统一的Vision Transformer（ViT）框架，结合监督、自监督和重构目标，用于自动调制识别（AMR），在标签稀缺情况下优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有AMR方法依赖大量标注数据或复杂的多阶段训练，导致实际应用中的可扩展性和泛化性受到限制。

Method: 该方法采用ViT编码器、轻量级卷积解码器和线性分类器，将重构分支作为约束，以保持对原始I/Q信号结构的敏感性。预训练阶段采用自监督特征学习，微调阶段则结合部分标签，提升标签利用效率。

Result: 在RML2018.01A数据集上，该方法在少标签场景下优于传统的监督CNN与ViT基线，仅需15-20%标签即可接近ResNet准确率，并在不同信噪比下表现稳健。

Conclusion: 该框架为AMR任务提供了一种简单、易迁移且高效利用标签的数据驱动方案，解决了标签稀缺和实用性方面的挑战。

Abstract: Automatic modulation recognition (AMR) is critical for cognitive radio,
spectrum monitoring, and secure wireless communication. However, existing
solutions often rely on large labeled datasets or multi-stage training
pipelines, which limit scalability and generalization in practice. We propose a
unified Vision Transformer (ViT) framework that integrates supervised,
self-supervised, and reconstruction objectives. The model combines a ViT
encoder, a lightweight convolutional decoder, and a linear classifier; the
reconstruction branch maps augmented signals back to their originals, anchoring
the encoder to fine-grained I/Q structure. This strategy promotes robust,
discriminative feature learning during pretraining, while partial label
supervision in fine-tuning enables effective classification with limited
labels. On the RML2018.01A dataset, our approach outperforms supervised CNN and
ViT baselines in low-label regimes, approaches ResNet-level accuracy with only
15-20% labeled data, and maintains strong performance across varying SNR
levels. Overall, the framework provides a simple, generalizable, and
label-efficient solution for AMR.

</details>


### [5] [InfinityHuman: Towards Long-Term Audio-Driven Human](https://arxiv.org/abs/2508.20210)
*Xiaodi Li,Pan Xie,Yi Ren,Qijun Gan,Chen Zhang,Fangyuan Kong,Xiang Yin,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种新方法InfinityHuman，可以根据音频驱动生成高分辨率、长时间且外观一致、手部动作自然的真人动画视频，并在多个指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前音频驱动的真人动画生成在生成高分辨率、持续时间长的视频时存在身份漂移、色彩漂移和场景不稳定等问题，且手部动作建模不足，难以自然对齐音频，需要更有效的方法。

Method: 提出InfinityHuman：1）先粗略生成与音频同步的中间表示；2）用姿态引导的细化器逐步生成高分辨率、长时长视频。姿态序列独立于外观，不会时序退化；细化器用初始帧做锚点，减少身份漂移并提升唇形同步；3）提出手部奖励机制，通过高质量手部动作数据训练模型，提高手部动作的语义和真实感。

Result: 在EMTD和HDTF数据集上，InfinityHuman在视频质量、身份保持、手部准确性和唇形同步性等方面均取得了最先进表现。消融实验验证了各模块的有效性。

Conclusion: InfinityHuman显著提升了音频驱动的真人动画视频生成效果，有效解决了现有方法的缺陷，为实际应用和后续研究奠定了基础。代码将公开。

Abstract: Audio-driven human animation has attracted wide attention thanks to its
practical applications. However, critical challenges remain in generating
high-resolution, long-duration videos with consistent appearance and natural
hand motions. Existing methods extend videos using overlapping motion frames
but suffer from error accumulation, leading to identity drift, color shifts,
and scene instability. Additionally, hand movements are poorly modeled,
resulting in noticeable distortions and misalignment with the audio. In this
work, we propose InfinityHuman, a coarse-to-fine framework that first generates
audio-synchronized representations, then progressively refines them into
high-resolution, long-duration videos using a pose-guided refiner. Since pose
sequences are decoupled from appearance and resist temporal degradation, our
pose-guided refiner employs stable poses and the initial frame as a visual
anchor to reduce drift and improve lip synchronization. Moreover, to enhance
semantic accuracy and gesture realism, we introduce a hand-specific reward
mechanism trained with high-quality hand motion data. Experiments on the EMTD
and HDTF datasets show that InfinityHuman achieves state-of-the-art performance
in video quality, identity preservation, hand accuracy, and lip-sync. Ablation
studies further confirm the effectiveness of each module. Code will be made
public.

</details>


### [6] [Spherical Vision Transformers for Audio-Visual Saliency Prediction in 360-Degree Videos](https://arxiv.org/abs/2508.20221)
*Mert Cokelek,Halit Ozsoy,Nevrez Imamoglu,Cagri Ozcinar,Inci Ayhan,Erkut Erdem,Aykut Erdem*

Main category: cs.CV

TL;DR: 本研究提出了两个用于360度全景视频显著性预测的新模型，并构建了新的音视频数据集，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏能够综合音频和视频信息，实现360度全景环境下视觉显著性预测的完整数据集和有效方法，特别是在考虑球面畸变和空间音频条件下的研究存在空白。

Method: 研究构建了YT360-EyeTracking数据集，涵盖81段360度全景视频及多种音视频条件。提出了基于视觉Transformer的SalViT360模型，利用球面几何感知的时空注意力层；并进一步扩展为SalViT360-AV模型，引入基于音频的Transformer适配器，实现音视频融合预测。

Result: 在多个基准测试集上（包括新构建的YT360-EyeTracking），SalViT360与SalViT360-AV模型在360度场景视觉显著性预测方面显著优于主流现有方法。

Conclusion: 模型中整合空间音频线索对于精确预测全景视频中的关注区域至关重要。所提出的数据集和方法为后续360度视频关注预测提供了基础与新思路。

Abstract: Omnidirectional videos (ODVs) are redefining viewer experiences in virtual
reality (VR) by offering an unprecedented full field-of-view (FOV). This study
extends the domain of saliency prediction to 360-degree environments,
addressing the complexities of spherical distortion and the integration of
spatial audio. Contextually, ODVs have transformed user experience by adding a
spatial audio dimension that aligns sound direction with the viewer's
perspective in spherical scenes. Motivated by the lack of comprehensive
datasets for 360-degree audio-visual saliency prediction, our study curates
YT360-EyeTracking, a new dataset of 81 ODVs, each observed under varying
audio-visual conditions. Our goal is to explore how to utilize audio-visual
cues to effectively predict visual saliency in 360-degree videos. Towards this
aim, we propose two novel saliency prediction models: SalViT360, a
vision-transformer-based framework for ODVs equipped with spherical
geometry-aware spatio-temporal attention layers, and SalViT360-AV, which
further incorporates transformer adapters conditioned on audio input. Our
results on a number of benchmark datasets, including our YT360-EyeTracking,
demonstrate that SalViT360 and SalViT360-AV significantly outperform existing
methods in predicting viewer attention in 360-degree scenes. Interpreting these
results, we suggest that integrating spatial audio cues in the model
architecture is crucial for accurate saliency prediction in omnidirectional
videos. Code and dataset will be available at
https://cyberiada.github.io/SalViT360.

</details>


### [7] [A Novel Framework for Automated Explain Vision Model Using Vision-Language Models](https://arxiv.org/abs/2508.20227)
*Phu-Vinh Nguyen,Tan-Hanh Pham,Chris Ngo,Truong Son Hy*

Main category: cs.CV

TL;DR: 本文关注于视觉模型的可解释性，提出了一种新的分析流程，可以在样本级和数据集级解释视觉模型的行为。


<details>
  <summary>Details</summary>
Motivation: 当前视觉模型多侧重追求性能指标提升，对于模型行为的可解释性关注较少。现有xAI方法多针对单个样本解释，缺乏对模型整体行为的解释方法，而这对于防止偏见判断及洞察模型趋势尤为重要。

Method: 提出了一套结合视觉-语言模型的新管道，能够实现对视觉模型在单个样本以及整个数据集水平上的解释分析。

Result: 该流程能够较为容易地发现模型失效案例，并帮助研究者理解模型决策过程、特征提取等行为模式。

Conclusion: 将新的解释流程集成到视觉模型研发中，有助于推动可解释AI在图像分析领域的发展，实现性能与可解释性的有效结合。

Abstract: The development of many vision models mainly focuses on improving their
performance using metrics such as accuracy, IoU, and mAP, with less attention
to explainability due to the complexity of applying xAI methods to provide a
meaningful explanation of trained models. Although many existing xAI methods
aim to explain vision models sample-by-sample, methods explaining the general
behavior of vision models, which can only be captured after running on a large
dataset, are still underexplored. Furthermore, understanding the behavior of
vision models on general images can be very important to prevent biased
judgments and help identify the model's trends and patterns. With the
application of Vision-Language Models, this paper proposes a pipeline to
explain vision models at both the sample and dataset levels. The proposed
pipeline can be used to discover failure cases and gain insights into vision
models with minimal effort, thereby integrating vision model development with
xAI analysis to advance image analysis.

</details>


### [8] [ATMS-KD: Adaptive Temperature and Mixed Sample Knowledge Distillation for a Lightweight Residual CNN in Agricultural Embedded Systems](https://arxiv.org/abs/2508.20232)
*Mohamed Ohamouddou,Said Ohamouddou,Abdellatif El Afia,Rafik Lasri*

Main category: cs.CV

TL;DR: 提出了一种新型轻量级CNN知识蒸馏框架ATMS-KD，在资源有限的农业场景下对玫瑰花图像成熟度分类效果显著优于现有方法，模型参数量小且推理速度快。


<details>
  <summary>Details</summary>
Motivation: 农业领域存在计算资源受限的问题，需要能够高效且准确的轻量级视觉识别模型。本研究希望通过更高效的知识蒸馏方法，增强小型学生模型在农业复杂环境下的表现。

Method: 提出ATMS-KD框架，将自适应温度调度和混合样本增强方法结合，用于知识蒸馏。以MobileNetV3 Large为教师模型，不同规模残差CNN为学生模型。验证方法是在摩洛哥农业田地采集的玫瑰花成熟度图片集上进行实验并与多种现有知识蒸馏方法对比。

Result: 所有学生模型验证集准确率均超96.7%，ATMS-KD明显优于直接训练（95-96%）及11种主流知识蒸馏方法。紧凑型模型在保持最低推理延迟（72.19ms）下，准确率达97.11%，比次优方法高出1.60个百分点，模型知识保留率超99%。

Conclusion: ATMS-KD能在复杂农业应用中实现高效、准确且低延迟的轻量级模型训练，是面向资源受限场景下视觉识别任务的有效解决方案。

Abstract: This study proposes ATMS-KD (Adaptive Temperature and Mixed-Sample Knowledge
Distillation), a novel framework for developing lightweight CNN models suitable
for resource-constrained agricultural environments. The framework combines
adaptive temperature scheduling with mixed-sample augmentation to transfer
knowledge from a MobileNetV3 Large teacher model (5.7\,M parameters) to
lightweight residual CNN students. Three student configurations were evaluated:
Compact (1.3\,M parameters), Standard (2.4\,M parameters), and Enhanced (3.8\,M
parameters). The dataset used in this study consists of images of \textit{Rosa
damascena} (Damask rose) collected from agricultural fields in the Dades Oasis,
southeastern Morocco, providing a realistic benchmark for agricultural computer
vision applications under diverse environmental conditions. Experimental
evaluation on the Damascena rose maturity classification dataset demonstrated
significant improvements over direct training methods. All student models
achieved validation accuracies exceeding 96.7\% with ATMS-KD compared to
95--96\% with direct training. The framework outperformed eleven established
knowledge distillation methods, achieving 97.11\% accuracy with the compact
model -- a 1.60 percentage point improvement over the second-best approach
while maintaining the lowest inference latency of 72.19\,ms. Knowledge
retention rates exceeded 99\% for all configurations, demonstrating effective
knowledge transfer regardless of student model capacity.

</details>


### [9] [Linking heterogeneous microstructure informatics with expert characterization knowledge through customized and hybrid vision-language representations for industrial qualification](https://arxiv.org/abs/2508.20243)
*Mutahar Safdar,Gentry Wood,Max Zimmermann,Guy Lamouche,Priti Wanjara,Yaoyao Fiona Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种结合图像和文本的多模态深度学习方法，用于自动化、可扩展地评估增材制造材料的微观结构质量，不需专门训练新模型即可实现零样本分类。


<details>
  <summary>Details</summary>
Motivation: 在工业制造中，先进材料的快速可靠鉴定是难点，尤其是面对非传统工艺（如增材制造）生成的复杂异质结构，传统方法难以高效处理并解释多模态数据。

Method: 作者建立了一个融合深度语义分割与多模态预训练模型（CLIP与FLAVA）的知识表征框架，将图像数据与专家文本评估共同编码，同时引入定制的基于相似度的表示方法，对专家标注包含正负参考，支持零样本分类并用Z-score归一化提升分数可比性。

Result: 在金属基复合材料的实验中，框架能有效区分合格/缺陷样本，FLAVA在视觉敏感性上更优，CLIP在文本一致性上表现更好；方法无需针对每个新任务再训练，验证了可扩展性与可靠性。

Conclusion: 该方法增强了制造流程中数据与知识的语义互操作性，实现了解释性强、可追溯的人机协同决策，有助于大规模自动化材料鉴定和领域适应推广。

Abstract: Rapid and reliable qualification of advanced materials remains a bottleneck
in industrial manufacturing, particularly for heterogeneous structures produced
via non-conventional additive manufacturing processes. This study introduces a
novel framework that links microstructure informatics with a range of expert
characterization knowledge using customized and hybrid vision-language
representations (VLRs). By integrating deep semantic segmentation with
pre-trained multi-modal models (CLIP and FLAVA), we encode both visual
microstructural data and textual expert assessments into shared
representations. To overcome limitations in general-purpose embeddings, we
develop a customized similarity-based representation that incorporates both
positive and negative references from expert-annotated images and their
associated textual descriptions. This allows zero-shot classification of
previously unseen microstructures through a net similarity scoring approach.
Validation on an additively manufactured metal matrix composite dataset
demonstrates the framework's ability to distinguish between acceptable and
defective samples across a range of characterization criteria. Comparative
analysis reveals that FLAVA model offers higher visual sensitivity, while the
CLIP model provides consistent alignment with the textual criteria. Z-score
normalization adjusts raw unimodal and cross-modal similarity scores based on
their local dataset-driven distributions, enabling more effective alignment and
classification in the hybrid vision-language framework. The proposed method
enhances traceability and interpretability in qualification pipelines by
enabling human-in-the-loop decision-making without task-specific model
retraining. By advancing semantic interoperability between raw data and expert
knowledge, this work contributes toward scalable and domain-adaptable
qualification strategies in engineering informatics.

</details>


### [10] [MedNet-PVS: A MedNeXt-Based Deep Learning Model for Automated Segmentation of Perivascular Spaces](https://arxiv.org/abs/2508.20256)
*Zhen Xuen Brandon Low,Rory Zhang,Hang Min,William Pham,Lucy Vivash,Jasmine Moses,Miranda Lynch,Karina Dorfman,Cassandra Marotta,Shaun Koh,Jacob Bunyamin,Ella Rowsthorn,Alex Jarema,Himashi Peiris,Zhaolin Chen,Sandy R. Shultz,David K. Wright,Dexiao Kong,Sharon L. Naismith,Terence J. O'Brien,Ying Xia,Meng Law,Benjamin Sinclair*

Main category: cs.CV

TL;DR: 本文提出了一种基于Transformer的3D深度学习模型MedNeXt-L-k5，用于自动分割脑部MRI图像中的扩张型血管周围间隙（PVS），在T2加权MRI样本中取得了当前最高的分割准确度，但在不同数据集之间的泛化表现一般，且未优于现有的nnU-Net模型。


<details>
  <summary>Details</summary>
Motivation: 扩张型血管周围间隙是多种脑疾病及认知衰老的重要影像生物标志物，手工分割费时且一致性有限，而现有自动分割方法泛化性和准确性均不理想，因此亟需更高效且泛化能力强的自动分割模型。

Method: 采用Transformer风格的3D编码-解码卷积网络MedNeXt-L-k5，对两组MRI数据分别训练模型：一组为同质T2加权MRI（200份HCP-Aging数据），一组为异质T1加权MRI（40份，来自7个研究、6台扫描仪）。评估方法包括内部五折交叉验证和留一站点交叉验证，并与已有模型如nnU-Net进行对比。

Result: 在T2加权同质数据集上，MedNeXt-L-k5模型白质区分割的Dice得分达0.88±0.06，为目前文献中最高，且与人工标注一致性相当。在T1加权数据集及跨站点泛化测试中，Dice得分明显下降（最低为0.35-0.38），在全局泛化能力上表现一般。同时，该模型并未超过现有的nnU-Net模型。

Conclusion: MedNeXt-L-k5在T2加权MRI数据上实现了高效而准确的PVS自动分割，但在异构数据和泛化能力方面仍有提升空间。Transformer风格的全局注意力机制在PVS分割场景下非必需，nnU-Net等传统方法依旧具备较强的竞争力。

Abstract: Enlarged perivascular spaces (PVS) are increasingly recognized as biomarkers
of cerebral small vessel disease, Alzheimer's disease, stroke, and
aging-related neurodegeneration. However, manual segmentation of PVS is
time-consuming and subject to moderate inter-rater reliability, while existing
automated deep learning models have moderate performance and typically fail to
generalize across diverse clinical and research MRI datasets. We adapted
MedNeXt-L-k5, a Transformer-inspired 3D encoder-decoder convolutional network,
for automated PVS segmentation. Two models were trained: one using a
homogeneous dataset of 200 T2-weighted (T2w) MRI scans from the Human
Connectome Project-Aging (HCP-Aging) dataset and another using 40 heterogeneous
T1-weighted (T1w) MRI volumes from seven studies across six scanners. Model
performance was evaluated using internal 5-fold cross validation (5FCV) and
leave-one-site-out cross validation (LOSOCV). MedNeXt-L-k5 models trained on
the T2w images of the HCP-Aging dataset achieved voxel-level Dice scores of
0.88+/-0.06 (white matter, WM), comparable to the reported inter-rater
reliability of that dataset, and the highest yet reported in the literature.
The same models trained on the T1w images of the HCP-Aging dataset achieved a
substantially lower Dice score of 0.58+/-0.09 (WM). Under LOSOCV, the model had
voxel-level Dice scores of 0.38+/-0.16 (WM) and 0.35+/-0.12 (BG), and
cluster-level Dice scores of 0.61+/-0.19 (WM) and 0.62+/-0.21 (BG).
MedNeXt-L-k5 provides an efficient solution for automated PVS segmentation
across diverse T1w and T2w MRI datasets. MedNeXt-L-k5 did not outperform the
nnU-Net, indicating that the attention-based mechanisms present in
transformer-inspired models to provide global context are not required for high
accuracy in PVS segmentation.

</details>


### [11] [Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation](https://arxiv.org/abs/2508.20265)
*Zhixiang Chi,Yanan Wu,Li Gu,Huan Liu,Ziqiang Wang,Yang Zhang,Yang Wang,Konstantinos N. Plataniotis*

Main category: cs.CV

TL;DR: CLIP虽有优秀的图文对齐能力，但在开放词汇分割任务中定位能力较差。本文提出一种训练无关、基于反馈的自适应框架，通过输出结果反馈加强中间注意力的空间一致性，无需训练且能直接提升多种现有方法与主干上的分割表现。


<details>
  <summary>Details</summary>
Motivation: CLIP在开放词汇分割任务中效果欠佳，主要因中间注意力与最终输出语义之间缺乏一致性，难以充分利用图文语义。以往方法尝试提升空间一致性，但空间信息难以传递到最终输出，且中间注意力与文本特征耦合不足。

Method: 提出了一种无需训练的新框架，通过对最终输出的patch级别对应关系进行反馈，指导中间注意力与输出保持一致，增强空间与语义一致性。设计了包括注意力隔离、基于置信度的稀疏筛选、自适应集成在内的关键模块，使输出空间一致性线索能高效反馈到中间层。该方法可作为插件，兼容多种主干与主流方法。

Result: 本方法无缝集成至4种领先方法、3种ViT主干，并在Q-K、self-self等多类型注意力机制及MAE、SAM、DINO等框架下测试，在8项基准上均有一致性提升。结果显示可广泛提升开放词汇分割性能。

Conclusion: 提出的自适应反馈框架有效弥补了CLIP分割中的空间与语义一致性问题，无需训练即可提升现有方法表现，具有通用性与实用价值。

Abstract: CLIP exhibits strong visual-textual alignment but struggle with
open-vocabulary segmentation due to poor localization. Prior methods enhance
spatial coherence by modifying intermediate attention. But, this coherence
isn't consistently propagated to the final output due to subsequent operations
such as projections. Additionally, intermediate attention lacks direct
interaction with text representations, such semantic discrepancy limits the
full potential of CLIP.
  In this work, we propose a training-free, feedback-driven self-adaptive
framework that adapts output-based patch-level correspondences back to the
intermediate attention. The output predictions, being the culmination of the
model's processing, encapsulate the most comprehensive visual and textual
semantics about each patch. Our approach enhances semantic consistency between
internal representations and final predictions by leveraging the model's
outputs as a stronger spatial coherence prior. We design key modules, including
attention isolation, confidence-based pruning for sparse adaptation, and
adaptation ensemble, to effectively feedback the output coherence cues. Our
method functions as a plug-in module, seamlessly integrating into four
state-of-the-art approaches with three backbones (ViT-B, ViT-L, ViT-H). We
further validate our framework across multiple attention types (Q-K, self-self,
and Proxy augmented with MAE, SAM, and DINO). Our approach consistently
improves their performance across eight benchmarks.

</details>


### [12] [How Multimodal LLMs Solve Image Tasks: A Lens on Visual Grounding, Task Reasoning, and Answer Decoding](https://arxiv.org/abs/2508.20279)
*Zhuoran Yu,Yong Jae Lee*

Main category: cs.CV

TL;DR: 本文提出了一种探查多模态大语言模型（MLLMs）内部处理过程的分析框架，并揭示其层级间的功能分工。


<details>
  <summary>Details</summary>
Motivation: 虽然MLLMs在视觉-语言任务上表现优异，但其内部如何处理视觉和文本信息尚不清楚。理解MLLMs每一层如何处理和整合视觉与文本输入，有助于模型优化和透明化。

Method: 作者提出了一套探查框架，通过从各层提取的token embedding训练线性分类器，预测视觉类别。使用三种控制提示词变化（词汇、语义否定、输出格式）测试不同层的功能，并应用于LLaVA-1.5、LLaVA-Next-LLaMA-3、Qwen2-VL等模型。

Result: 实验发现MLLMs内部呈现阶段式结构：早期层进行视觉锚定，中间层融合词汇与语义推理，后期层专注于特定任务输出。该阶段结构在不同模型和数据下都较为稳定，但具体分配到哪几层会因底层架构不同有所变化。

Conclusion: 该框架为理解MLLMs层级结构提供了统一视角，并提出了一种轻量、模型无关的方法用于分析多模态表征动态。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated strong performance
across a wide range of vision-language tasks, yet their internal processing
dynamics remain underexplored. In this work, we introduce a probing framework
to systematically analyze how MLLMs process visual and textual inputs across
layers. We train linear classifiers to predict fine-grained visual categories
(e.g., dog breeds) from token embeddings extracted at each layer, using a
standardized anchor question. To uncover the functional roles of different
layers, we evaluate these probes under three types of controlled prompt
variations: (1) lexical variants that test sensitivity to surface-level
changes, (2) semantic negation variants that flip the expected answer by
modifying the visual concept in the prompt, and (3) output format variants that
preserve reasoning but alter the answer format. Applying our framework to
LLaVA-1.5, LLaVA-Next-LLaMA-3, and Qwen2-VL, we identify a consistent
stage-wise structure in which early layers perform visual grounding, middle
layers support lexical integration and semantic reasoning, and final layers
prepare task-specific outputs. We further show that while the overall
stage-wise structure remains stable across variations in visual tokenization,
instruction tuning data, and pretraining corpus, the specific layer allocation
to each stage shifts notably with changes in the base LLM architecture. Our
findings provide a unified perspective on the layer-wise organization of MLLMs
and offer a lightweight, model-agnostic approach for analyzing multimodal
representation dynamics.

</details>


### [13] [Disentangling Latent Embeddings with Sparse Linear Concept Subspaces (SLiCS)](https://arxiv.org/abs/2508.20322)
*Zhi Li,Hau Phan,Matthew Emigh,Austin J. Brockmeier*

Main category: cs.CV

TL;DR: 本文提出了将视觉-语言嵌入（如CLIP）中的信息，通过稀疏线性分解成面向概念的子空间，实现内容解耦，并用于更高精度的多标签检索和条件图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言嵌入虽包含丰富语义，但缺乏对复杂场景中不同概念的可解释解耦表征，难以对多标签和概念条件任务进行精细控制。

Method: 提出了一种有监督的字典学习方法，将嵌入表示分解为多个稀疏、非负、分组结构的原子向量组合；每组原子对应一个语义标签，通过交替优化算法保证收敛。同时，利用文本嵌入，采用无监督方案为训练集自动生成多标签。

Result: 无论是在CLIP/TiTok编码还是自监督DINOv2嵌入上，实验展示了方法在概念筛选图像检索及条件图像生成的精度优于现有方法。

Conclusion: SLiCS能实现视觉-语言嵌入的信息解耦，提升多标签检索和生成效果，在各种嵌入空间均表现出优越性。

Abstract: Vision-language co-embedding networks, such as CLIP, provide a latent
embedding space with semantic information that is useful for downstream tasks.
We hypothesize that the embedding space can be disentangled to separate the
information on the content of complex scenes by decomposing the embedding into
multiple concept-specific component vectors that lie in different subspaces. We
propose a supervised dictionary learning approach to estimate a linear
synthesis model consisting of sparse, non-negative combinations of groups of
vectors in the dictionary (atoms), whose group-wise activity matches the
multi-label information. Each concept-specific component is a non-negative
combination of atoms associated to a label. The group-structured dictionary is
optimized through a novel alternating optimization with guaranteed convergence.
Exploiting the text co-embeddings, we detail how semantically meaningful
descriptions can be found based on text embeddings of words best approximated
by a concept's group of atoms, and unsupervised dictionary learning can exploit
zero-shot classification of training set images using the text embeddings of
concept labels to provide instance-wise multi-labels. We show that the
disentangled embeddings provided by our sparse linear concept subspaces (SLiCS)
enable concept-filtered image retrieval (and conditional generation using
image-to-prompt) that is more precise. We also apply SLiCS to highly-compressed
autoencoder embeddings from TiTok and the latent embedding from self-supervised
DINOv2. Quantitative and qualitative results highlight the improved precision
of the concept-filtered image retrieval for all embeddings.

</details>


### [14] [MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models](https://arxiv.org/abs/2508.20345)
*Xiao Li,Yanfan Zhu,Ruining Deng,Wei-Qi Wei,Yu Wang,Shilin Zhao,Yaohong Wang,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: 本论文提出了MedFoundationHub，一个专为医疗视觉语言模型（VLMs）设计的安全GUI工具包，可提升模型在医疗环境中的实际应用安全与便利性。


<details>
  <summary>Details</summary>
Motivation: 尽管医疗视觉语言模型在自动化报告生成、医生助手等领域展示巨大潜力，但其在医院环境中的使用涉及PHI泄漏、数据泄露及网络安全等重要风险。因此，亟需开发既能便利使用又能保障数据安全的工具。

Method: 设计并实现了MedFoundationHub，一个图形界面工具包，使医生无需编程即可切换和使用不同的VLM模型，并支持工程师高效、即插即用地部署各类Hugging Face开源模型，通过Docker实现隐私保护、系统无关的本地部署。实验采用离线单台A6000 GPU，邀请认证病理学家对多种主流VLM进行真实评测。

Result: 评测过程中，专家在结肠和肾脏病例任务上与五个VLM模型进行1015次打分，发现模型在回答准确性、推理充分性及专业术语一致性方面存在反复性不足。

Conclusion: MedFoundationHub提升了医疗VLM的可用性与安全性，但当前模型在精准性和医学专业性方面仍需进一步提升。

Abstract: Recent advances in medical vision-language models (VLMs) open up remarkable
opportunities for clinical applications such as automated report generation,
copilots for physicians, and uncertainty quantification. However, despite their
promise, medical VLMs introduce serious security concerns, most notably risks
of Protected Health Information (PHI) exposure, data leakage, and vulnerability
to cyberthreats - which are especially critical in hospital environments. Even
when adopted for research or non-clinical purposes, healthcare organizations
must exercise caution and implement safeguards. To address these challenges, we
present MedFoundationHub, a graphical user interface (GUI) toolkit that: (1)
enables physicians to manually select and use different models without
programming expertise, (2) supports engineers in efficiently deploying medical
VLMs in a plug-and-play fashion, with seamless integration of Hugging Face
open-source models, and (3) ensures privacy-preserving inference through
Docker-orchestrated, operating system agnostic deployment. MedFoundationHub
requires only an offline local workstation equipped with a single NVIDIA A6000
GPU, making it both secure and accessible within the typical resources of
academic research labs. To evaluate current capabilities, we engaged
board-certified pathologists to deploy and assess five state-of-the-art VLMs
(Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and
LLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases,
yielding 1015 clinician-model scoring events. These assessments revealed
recurring limitations, including off-target answers, vague reasoning, and
inconsistent pathology terminology.

</details>


### [15] [Enhancing Mamba Decoder with Bidirectional Interaction in Multi-Task Dense Prediction](https://arxiv.org/abs/2508.20376)
*Mang Cao,Sanping Zhou,Yizhe Li,Ye Deng,Wenli Huang,Le Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为BIM（Bidirectional Interaction Mamba）的新型方法，通过高效的双向扫描机制强化多任务密集预测中的任务间互动，同时提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 多任务密集预测需要不同任务间有充足的信息互动，但充分互动会导致计算复杂度过高，现有方法无法兼顾高质量互动与计算效率。

Method: 提出了双向互动扫描（BI-Scan）机制，将多任务特征以任务优先和位置优先两种方式进行高效双向序列互动，同时设计了多尺度扫描（MS-Scan）机制，实现多粒度场景建模，增强任务间特征融合。整个框架保持线性复杂度，有效提升效率。

Result: 在NYUD-V2和PASCAL-Context两个有挑战性的基准数据集上进行大量实验，BIM显著优于当前最先进方法。

Conclusion: BIM方法兼顾了任务间充分互动和高效计算，在多任务密集预测场景下具有明显优势。

Abstract: Sufficient cross-task interaction is crucial for success in multi-task dense
prediction. However, sufficient interaction often results in high computational
complexity, forcing existing methods to face the trade-off between interaction
completeness and computational efficiency. To address this limitation, this
work proposes a Bidirectional Interaction Mamba (BIM), which incorporates novel
scanning mechanisms to adapt the Mamba modeling approach for multi-task dense
prediction. On the one hand, we introduce a novel Bidirectional Interaction
Scan (BI-Scan) mechanism, which constructs task-specific representations as
bidirectional sequences during interaction. By integrating task-first and
position-first scanning modes within a unified linear complexity architecture,
BI-Scan efficiently preserves critical cross-task information. On the other
hand, we employ a Multi-Scale Scan~(MS-Scan) mechanism to achieve
multi-granularity scene modeling. This design not only meets the diverse
granularity requirements of various tasks but also enhances nuanced cross-task
feature interactions. Extensive experiments on two challenging benchmarks,
\emph{i.e.}, NYUD-V2 and PASCAL-Context, show the superiority of our BIM vs its
state-of-the-art competitors.

</details>


### [16] [Audio-Guided Visual Editing with Complex Multi-Modal Prompts](https://arxiv.org/abs/2508.20379)
*Hyeonyu Kim,Seokhoon Jeong,Seonghee Han,Chanhyuk Choi,Taehwan Kim*

Main category: cs.CV

TL;DR: 本文提出了一种创新的音频引导视觉编辑框架，通过结合多模态（文本和音频）提示，在无需额外训练的情况下，高效处理复杂编辑任务，实验表明其在复杂场景下优于仅用文本的方法。


<details>
  <summary>Details</summary>
Motivation: 已有扩散模型的视觉编辑手段主要依赖文本指导，但文本难以精确描述复杂的编辑需求，因此有引入非文本编辑提示（如音频）的必要。现有音频引导方法又依赖专门的数据集训练，泛化性差，难以适应真实场景。

Method: 采用预训练的强零样本能力多模态编码器，并设计机制缓解音频编码器与扩散模型提示编码空间的不一致问题。为应对多/多模态组合编辑，提出分支噪声与自适应patch选择方法，实现多文本与音频提示的协同编辑。

Result: 在丰富的视觉编辑实验任务中，该方法能通过整合音频信息，出色应对仅文本方法表现不佳的复杂编辑场景，验证了框架的有效性和优越性。

Conclusion: 融合文本与音频等多模态提示可极大提升复杂视觉编辑的能力，所提框架具有通用性和无需额外训练的优势，为应对实际复杂编辑需求提供了新思路。

Abstract: Visual editing with diffusion models has made significant progress but often
struggles with complex scenarios that textual guidance alone could not
adequately describe, highlighting the need for additional non-text editing
prompts. In this work, we introduce a novel audio-guided visual editing
framework that can handle complex editing tasks with multiple text and audio
prompts without requiring additional training. Existing audio-guided visual
editing methods often necessitate training on specific datasets to align audio
with text, limiting their generalization to real-world situations. We leverage
a pre-trained multi-modal encoder with strong zero-shot capabilities and
integrate diverse audio into visual editing tasks, by alleviating the
discrepancy between the audio encoder space and the diffusion model's prompt
encoder space. Additionally, we propose a novel approach to handle complex
scenarios with multiple and multi-modal editing prompts through our separate
noise branching and adaptive patch selection. Our comprehensive experiments on
diverse editing tasks demonstrate that our framework excels in handling
complicated editing scenarios by incorporating rich information from audio,
where text-only approaches fail.

</details>


### [17] [More Reliable Pseudo-labels, Better Performance: A Generalized Approach to Single Positive Multi-label Learning](https://arxiv.org/abs/2508.20381)
*Luong Tran,Thieu Vo,Anh Nguyen,Sang Dinh,Van Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种新的多标签学习框架，尤其针对每张图片仅有一个正标签（SPML）的极端情况，通过创新损失函数和伪标签处理策略，显著提升了多标签分类性能。


<details>
  <summary>Details</summary>
Motivation: 多标签学习在计算机视觉领域很重要，但由于标注成本高，大型数据集常常无法获得完整标注。SPML极端场景下，每张图片仅有一个正标签，其余标签均未知，现有方法易出现误分类和引入噪声，需要新方法提升学习效果。

Method: 提出了Generalized Pseudo-Label Robust Loss(GPR Loss)新损失函数，有效利用多样化伪标签并抗噪声；同时设计了Dynamic Augmented Multi-focus Pseudo-labeling(DAMP)技术，增强伪标签的有效性。两者共同组成AEVLP视觉-语言伪标签框架，提升数据利用率和模型表现。

Result: 在四个主流基准数据集上，所提框架实现了显著优于现有方法的分类性能，达到了最新最优结果。

Conclusion: AEVLP框架通过创新的损失和伪标签策略，有效缓解了多标签稀疏标注带来的挑战，大幅提升了多标签分类准确率，对弱标注学习场景具有广泛应用前景。

Abstract: Multi-label learning is a challenging computer vision task that requires
assigning multiple categories to each image. However, fully annotating
large-scale datasets is often impractical due to high costs and effort,
motivating the study of learning from partially annotated data. In the extreme
case of Single Positive Multi-Label Learning (SPML), each image is provided
with only one positive label, while all other labels remain unannotated.
Traditional SPML methods that treat missing labels as unknown or negative tend
to yield inaccuracies and false negatives, and integrating various
pseudo-labeling strategies can introduce additional noise. To address these
challenges, we propose the Generalized Pseudo-Label Robust Loss (GPR Loss), a
novel loss function that effectively learns from diverse pseudo-labels while
mitigating noise. Complementing this, we introduce a simple yet effective
Dynamic Augmented Multi-focus Pseudo-labeling (DAMP) technique. Together, these
contributions form the Adaptive and Efficient Vision-Language Pseudo-Labeling
(AEVLP) framework. Extensive experiments on four benchmark datasets demonstrate
that our framework significantly advances multi-label classification, achieving
state-of-the-art results.

</details>


### [18] [Ultra-Low-Latency Spiking Neural Networks with Temporal-Dependent Integrate-and-Fire Neuron Model for Objects Detection](https://arxiv.org/abs/2508.20392)
*Chengjun Zhang,Yuhao Zhang,Jie Yang,Mohamad Sawan*

Main category: cs.CV

TL;DR: 该论文提出了一种改进脉冲神经网络（SNN）在视觉检测任务表现的方法，通过新型时序依赖神经元结构显著提升性能，并实现极低时延和能耗。


<details>
  <summary>Details</summary>
Motivation: 尽管SNN在分类任务中效果突出，但在视觉检测任务中表现不佳。主要瓶颈在于脉冲发放模式异质性导致的残余膜电位问题。

Method: 提出delay-spike方法缓解残余膜电位，并设计了一种时序依赖的Integrate-and-Fire（tdIF）神经元结构，使神经元能根据时序动态调整发放行为，提升了脉冲的时序表达能力。

Result: 采用该方法在目标检测和车道线检测两大任务上进行了评估，结果显示在超低时延（≤5步）下，性能超过现有最佳ANN-SNN转换方法。

Conclusion: 提出的方法在不增加能耗的同时，提升了SNN在视觉检测任务中的表现，为低能耗、低延迟的实时视觉应用提供了新思路。

Abstract: Spiking Neural Networks (SNNs), inspired by the brain, are characterized by
minimal power consumption and swift inference capabilities on neuromorphic
hardware, and have been widely applied to various visual perception tasks.
Current ANN-SNN conversion methods have achieved excellent results in
classification tasks with ultra-low time-steps, but their performance in visual
detection tasks remains suboptimal. In this paper, we propose a delay-spike
approach to mitigate the issue of residual membrane potential caused by
heterogeneous spiking patterns. Furthermore, we propose a novel
temporal-dependent Integrate-and-Fire (tdIF) neuron architecture for SNNs. This
enables Integrate-and-fire (IF) neurons to dynamically adjust their
accumulation and firing behaviors based on the temporal order of time-steps.
Our method enables spikes to exhibit distinct temporal properties, rather than
relying solely on frequency-based representations. Moreover, the tdIF neuron
maintains energy consumption on par with traditional IF neuron. We demonstrate
that our method achieves more precise feature representation with lower
time-steps, enabling high performance and ultra-low latency in visual detection
tasks. In this study, we conduct extensive evaluation of the tdIF method across
two critical vision tasks: object detection and lane line detection. The
results demonstrate that the proposed method surpasses current ANN-SNN
conversion approaches, achieving state-of-the-art performance with ultra-low
latency (within 5 time-steps).

</details>


### [19] [Graph-Based Uncertainty Modeling and Multimodal Fusion for Salient Object Detection](https://arxiv.org/abs/2508.20415)
*Yuqi Xiong,Wuzhen Shi,Yang Wen,Ruhan Liu*

Main category: cs.CV

TL;DR: 本文提出了一种显著性目标检测新方法DUP-MCRNet，通过动态不确定性传播和多模态协同推理，有效提升了小结构细节和边缘检测能力，并增强了对复杂场景的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的显著性目标检测方法在处理复杂场景时，常出现细节丢失、边缘模糊及单一模态信息融合不足等问题，影响检测精度和鲁棒性，因此需要新的方法提升这些方面的能力。

Method: 1）设计动态不确定性图卷积模块（DUGC），通过基于空间语义距离构建的稀疏图，在层间传播不确定性，并结合通道自适应交互，提升小结构和边缘区域检测精度；2）提出多模态协同融合策略（MCF），利用可学习的模态门控权重，权重融合RGB、深度和边缘特征的注意力图，动态调整各模态重要性，增强跨模态互补性和一致性，抑制冗余或干扰信息；3）结合多尺度BCE和IoU损失、跨尺度一致性约束及不确定性引导监督机制，优化像素和区域级的检测性能。

Result: 大量实验表明，DUP-MCRNet在多个主流显著性目标检测数据集上，特别是在边缘清晰度和复杂背景鲁棒性方面，均优于现有多种方法。

Conclusion: DUP-MCRNet有效结合动态不确定性传播和多模态协同推理，增强了复杂场景下的显著性检测能力，尤其在细节和边缘检测、抗干扰性等方面表现突出，具有良好的应用前景。

Abstract: In view of the problems that existing salient object detection (SOD) methods
are prone to losing details, blurring edges, and insufficient fusion of
single-modal information in complex scenes, this paper proposes a dynamic
uncertainty propagation and multimodal collaborative reasoning network
(DUP-MCRNet). Firstly, a dynamic uncertainty graph convolution module (DUGC) is
designed to propagate uncertainty between layers through a sparse graph
constructed based on spatial semantic distance, and combined with channel
adaptive interaction, it effectively improves the detection accuracy of small
structures and edge regions. Secondly, a multimodal collaborative fusion
strategy (MCF) is proposed, which uses learnable modality gating weights to
weightedly fuse the attention maps of RGB, depth, and edge features. It can
dynamically adjust the importance of each modality according to different
scenes, effectively suppress redundant or interfering information, and
strengthen the semantic complementarity and consistency between
cross-modalities, thereby improving the ability to identify salient regions
under occlusion, weak texture or background interference. Finally, the
detection performance at the pixel level and region level is optimized through
multi-scale BCE and IoU loss, cross-scale consistency constraints, and
uncertainty-guided supervision mechanisms. Extensive experiments show that
DUP-MCRNet outperforms various SOD methods on most common benchmark datasets,
especially in terms of edge clarity and robustness to complex backgrounds. Our
code is publicly available at https://github.com/YukiBear426/DUP-MCRNet.

</details>


### [20] [MSMVD: Exploiting Multi-scale Image Features via Multi-scale BEV Features for Multi-view Pedestrian Detection](https://arxiv.org/abs/2508.20447)
*Taiga Yamane,Satoshi Suzuki,Ryo Masumura,Shota Orihashi,Tomohiro Tanaka,Mana Ihori,Naoki Makishima,Naotaka Kawata*

Main category: cs.CV

TL;DR: 提出了一种新的多尺度多视角行人检测方法（MSMVD），通过利用多尺度BEV特征大幅提升了检测性能，超过现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 现有多视角行人检测方法难以同时处理视图内同尺度的极大/极小目标及视图间尺度变化较大的行人，关键原因是未能充分利用多尺度图像特征。

Method: 提出以多尺度图像特征为基础，将其逐尺度投影至鸟瞰视角的BEV空间，获得多尺度BEV特征，然后通过特征金字塔网络（FPN）融合不同尺度信息，实现精确检测不同尺度行人。

Result: 大量实验表明，MSMVD方法显著提升检测准确度，在GMVD数据集上MODA指标超越以往最好结果4.5个百分点。

Conclusion: 合理利用多尺度图像特征生成多尺度BEV特征，能够有效提升多视角行人检测性能，MSMVD方法优于现有主流方案。

Abstract: Multi-View Pedestrian Detection (MVPD) aims to detect pedestrians in the form
of a bird's eye view (BEV) from multi-view images. In MVPD, end-to-end
trainable deep learning methods have progressed greatly. However, they often
struggle to detect pedestrians with consistently small or large scales in views
or with vastly different scales between views. This is because they do not
exploit multi-scale image features to generate the BEV feature and detect
pedestrians. To overcome this problem, we propose a novel MVPD method, called
Multi-Scale Multi-View Detection (MSMVD). MSMVD generates multi-scale BEV
features by projecting multi-scale image features extracted from individual
views into the BEV space, scale-by-scale. Each of these BEV features inherits
the properties of its corresponding scale image features from multiple views.
Therefore, these BEV features help the precise detection of pedestrians with
consistently small or large scales in views. Then, MSMVD combines information
at different scales of multiple views by processing the multi-scale BEV
features using a feature pyramid network. This improves the detection of
pedestrians with vastly different scales between views. Extensive experiments
demonstrate that exploiting multi-scale image features via multi-scale BEV
features greatly improves the detection performance, and MSMVD outperforms the
previous highest MODA by $4.5$ points on the GMVD dataset.

</details>


### [21] [A Spatial-Frequency Aware Multi-Scale Fusion Network for Real-Time Deepfake Detection](https://arxiv.org/abs/2508.20449)
*Libo Lv,Tianyi Wang,Mengxiao Huang,Ruixia Liu,Yinglong Wang*

Main category: cs.CV

TL;DR: 该文提出了一种名为SFMFNet的轻量级且高效的实时deepfake检测方法，在保证准确率的同时提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 目前deepfake生成技术飞速发展，伪造内容愈发逼真且广泛应用于视频会议和社交媒体。尽管先进的检测方法在标准评测中表现良好，但高昂的计算成本限制了其实时应用。因此，亟需高效且可部署的检测模型。

Method: SFMFNet架构包含空间-频率混合感知模块（联合利用空间纹理和频率伪影，通过门控机制增强对细微篡改的敏感性），令牌选择性交叉注意力（高效实现多层特征交互），以及残差增强模糊池化结构（保留下采样过程中的关键信息）。

Result: 在多个基准数据集上的实验表明，SFMFNet在准确率和效率之间取得了理想平衡，并具备很强的泛化能力和现实应用价值。

Conclusion: SFMFNet为实时deepfake检测提供了一种兼顾轻量与准确的解决方案，有望应用于实际场景。

Abstract: With the rapid advancement of real-time deepfake generation techniques,
forged content is becoming increasingly realistic and widespread across
applications like video conferencing and social media. Although
state-of-the-art detectors achieve high accuracy on standard benchmarks, their
heavy computational cost hinders real-time deployment in practical
applications. To address this, we propose the Spatial-Frequency Aware
Multi-Scale Fusion Network (SFMFNet), a lightweight yet effective architecture
for real-time deepfake detection. We design a spatial-frequency hybrid aware
module that jointly leverages spatial textures and frequency artifacts through
a gated mechanism, enhancing sensitivity to subtle manipulations. A
token-selective cross attention mechanism enables efficient multi-level feature
interaction, while a residual-enhanced blur pooling structure helps retain key
semantic cues during downsampling. Experiments on several benchmark datasets
show that SFMFNet achieves a favorable balance between accuracy and efficiency,
with strong generalization and practical value for real-time applications.

</details>


### [22] [Dual-Model Weight Selection and Self-Knowledge Distillation for Medical Image Classification](https://arxiv.org/abs/2508.20461)
*Ayaka Tsutsumi,Guang Li,Ren Togo,Takahiro Ogawa,Satoshi Kondo,Miki Haseyama*

Main category: cs.CV

TL;DR: 本文提出了一种结合双模型权重选择和自蒸馏的新型医学图像分类方法，有效提升了轻量级模型的性能，并在多个公开医学影像数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在实际医疗场景中，由于算力受限，难以部署大规模模型。急需在保持计算高效的前提下，开发性能接近大模型的轻量级模型。

Method: 提出双模型权重选择策略：用大规模预训练模型权重初始化两个轻量级模型，实现知识迁移。随后对挑选的模型实施自知识蒸馏，并在目标分类任务上进行微调，实现高效训练。

Result: 在胸部X光、肺部CT、脑部MRI等公开数据集上进行大量实验，证明本方法在性能和鲁棒性上均优于现有方法。

Conclusion: 结合双模型权重选择与自知识蒸馏，有效克服了轻量模型在信息保留上的传统缺陷，兼顾了性能和计算效率，适合实际医疗环境部署。

Abstract: We propose a novel medical image classification method that integrates
dual-model weight selection with self-knowledge distillation (SKD). In
real-world medical settings, deploying large-scale models is often limited by
computational resource constraints, which pose significant challenges for their
practical implementation. Thus, developing lightweight models that achieve
comparable performance to large-scale models while maintaining computational
efficiency is crucial. To address this, we employ a dual-model weight selection
strategy that initializes two lightweight models with weights derived from a
large pretrained model, enabling effective knowledge transfer. Next, SKD is
applied to these selected models, allowing the use of a broad range of initial
weight configurations without imposing additional excessive computational cost,
followed by fine-tuning for the target classification tasks. By combining
dual-model weight selection with self-knowledge distillation, our method
overcomes the limitations of conventional approaches, which often fail to
retain critical information in compact models. Extensive experiments on
publicly available datasets-chest X-ray images, lung computed tomography scans,
and brain magnetic resonance imaging scans-demonstrate the superior performance
and robustness of our approach compared to existing methods.

</details>


### [23] [Re-Densification Meets Cross-Scale Propagation: Real-Time Compression of LiDAR Point Clouds](https://arxiv.org/abs/2508.20466)
*Pengpeng Yu,Haoran Li,Dingquan Li,Runqing Jiang,Jing Wang,Liang Lin,Yulan Guo*

Main category: cs.CV

TL;DR: 本文提出了一种高效预测编码的点云压缩方法，实现了更高的压缩率和实时速度。


<details>
  <summary>Details</summary>
Motivation: 高精度LiDAR点云扫描带来了大量存储和传输开销，而现有点云压缩方法在极度稀疏的细节处理和上下文建模效率方面存在瓶颈，限制了性能和速度。

Method: 方法包括两大轻量模块：1) 几何重密化模块（Geometry Re-Densification Module）在编码稀疏几何信息后，生成密集特征再稀疏化，用于后续高效预测编码；2) 跨尺度特征传播模块（Cross-scale Feature Propagation Module）利用多分辨率层级的占据信息，促进特征传播和信息共享，从而提升特征质量并减少重复计算。两模块协同构建紧凑的特征表达，实现高效的上下文建模和加速编码过程。

Result: 在KITTI数据集上，方法达到了业界领先的压缩比，同时编码与解码均可实时运行，12位量化下能实现26帧每秒。

Conclusion: 提出的方法增强了点云压缩的上下文建模效率，兼顾压缩性能和实时性，有望为自动驾驶等领域带来实际应用价值。

Abstract: LiDAR point clouds are fundamental to various applications, yet
high-precision scans incur substantial storage and transmission overhead.
Existing methods typically convert unordered points into hierarchical octree or
voxel structures for dense-to-sparse predictive coding. However, the extreme
sparsity of geometric details hinders efficient context modeling, thereby
limiting their compression performance and speed. To address this challenge, we
propose to generate compact features for efficient predictive coding. Our
framework comprises two lightweight modules. First, the Geometry
Re-Densification Module re-densifies encoded sparse geometry, extracts features
at denser scale, and then re-sparsifies the features for predictive coding.
This module avoids costly computation on highly sparse details while
maintaining a lightweight prediction head. Second, the Cross-scale Feature
Propagation Module leverages occupancy cues from multiple resolution levels to
guide hierarchical feature propagation. This design facilitates information
sharing across scales, thereby reducing redundant feature extraction and
providing enriched features for the Geometry Re-Densification Module. By
integrating these two modules, our method yields a compact feature
representation that provides efficient context modeling and accelerates the
coding process. Experiments on the KITTI dataset demonstrate state-of-the-art
compression ratios and real-time performance, achieving 26 FPS for both
encoding and decoding at 12-bit quantization. Code is available at
https://github.com/pengpeng-yu/FastPCC.

</details>


### [24] [SKGE-SWIN: End-To-End Autonomous Vehicle Waypoint Prediction and Navigation Using Skip Stage Swin Transformer](https://arxiv.org/abs/2508.20762)
*Fachri Najm Noer Kartiman,Rasim,Yaya Wihardi,Nurul Hasanah,Oskar Natan,Bambang Wahono,Taufik Ibnu Salim*

Main category: cs.CV

TL;DR: 该论文提出了一种基于Swin Transformer的新型端到端自动驾驶模型SKGE-Swin，突出像素级上下文感知能力，在CARLA平台模拟真实环境下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶模型在复杂环境理解及远距离像素信息提取方面存在不足，亟需更高效的全局特征表征方法提高模型安全性和准确性。

Method: 提出SKGE-Swin架构，将Swin Transformer与跳跃连接机制结合，利用SW-MSA机制提取全局及多层次特征，保留关键信息贯穿特征提取全过程，并通过CARLA仿真平台的对抗性场景进行性能验证。

Result: 实验表明，SKGE-Swin在CARLA平台下获得了比先前方法更优的驾驶评分。消融实验进一步验证了跳跃连接与Swin Transformer对性能提升的贡献。

Conclusion: SKGE-Swin架构显著提升了端到端自动驾驶模型对复杂环境的理解和决策能力，对后续自动驾驶感知-决策体系研究有积极推动作用。

Abstract: Focusing on the development of an end-to-end autonomous vehicle model with
pixel-to-pixel context awareness, this research proposes the SKGE-Swin
architecture. This architecture utilizes the Swin Transformer with a skip-stage
mechanism to broaden feature representation globally and at various network
levels. This approach enables the model to extract information from distant
pixels by leveraging the Swin Transformer's Shifted Window-based Multi-head
Self-Attention (SW-MSA) mechanism and to retain critical information from the
initial to the final stages of feature extraction, thereby enhancing its
capability to comprehend complex patterns in the vehicle's surroundings. The
model is evaluated on the CARLA platform using adversarial scenarios to
simulate real-world conditions. Experimental results demonstrate that the
SKGE-Swin architecture achieves a superior Driving Score compared to previous
methods. Furthermore, an ablation study will be conducted to evaluate the
contribution of each architectural component, including the influence of skip
connections and the use of the Swin Transformer, in improving model
performance.

</details>


### [25] [Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation](https://arxiv.org/abs/2508.20470)
*Xiaochuan Li,Guoguang Du,Runze Zhang,Liang Jin,Qi Jia,Lihua Lu,Zhenhua Guo,Yaqian Zhao,Haiyang Liu,Tianqi Wang,Changsheng Li,Xiaoli Gong,Rengang Li,Baoyu Fan*

Main category: cs.CV

TL;DR: 本文提出使用视频数据作为新的监督信号以缓解3D资产生成领域的数据稀缺问题，推出了大规模多视角视频数据集Droplet3D-4M，并训练了支持图像与文本输入的3D生成模型Droplet3D，实验证明该方法在空间一致性和语义合理性上均有提升，对场景级应用也具有扩展潜力。


<details>
  <summary>Details</summary>
Motivation: 在文本、图像和视频领域，大规模数据集推动了生成模型的发展，但3D领域因可用数据匮乏受到限制。作者发现，视频中暗含常识先验，能够弥补3D原生数据不足，进而提升3D生成模型的泛化能力和表现。

Method: 1）构建了Droplet3D-4M数据集，为首个标有多视角注释的大规模视频数据集；2）提出Droplet3D，一种可接收图像与高密度文本输入的3D生成模型；3）利用多视角视频蕴含的空间一致性与丰富语义，实现更加准确、合理的3D资产生成。

Result: 实验表明，使用视频先验训练的Droplet3D模型在空间一致性和语义合理性方面优于现有方法，并展现出良好的场景级扩展能力。

Conclusion: 将视频作为带有常识先验的监督信号为3D内容生成领域提供了新的方向。该方法显著缓解了数据稀缺带来的泛化瓶颈，有望推动3D生成模型在多个实际场景的应用。所有数据集与代码均已开源，促进后续研究。

Abstract: Scaling laws have validated the success and promise of large-data-trained
models in creative generation across text, image, and video domains. However,
this paradigm faces data scarcity in the 3D domain, as there is far less of it
available on the internet compared to the aforementioned modalities.
Fortunately, there exist adequate videos that inherently contain commonsense
priors, offering an alternative supervisory signal to mitigate the
generalization bottleneck caused by limited native 3D data. On the one hand,
videos capturing multiple views of an object or scene provide a spatial
consistency prior for 3D generation. On the other hand, the rich semantic
information contained within the videos enables the generated content to be
more faithful to the text prompts and semantically plausible. This paper
explores how to apply the video modality in 3D asset generation, spanning
datasets to models. We introduce Droplet3D-4M, the first large-scale video
dataset with multi-view level annotations, and train Droplet3D, a generative
model supporting both image and dense text input. Extensive experiments
validate the effectiveness of our approach, demonstrating its ability to
produce spatially consistent and semantically plausible content. Moreover, in
contrast to the prevailing 3D solutions, our approach exhibits the potential
for extension to scene-level applications. This indicates that the commonsense
priors from the videos significantly facilitate 3D creation. We have
open-sourced all resources including the dataset, code, technical framework,
and model weights: https://dropletx.github.io/.

</details>


### [26] [To New Beginnings: A Survey of Unified Perception in Autonomous Vehicle Software](https://arxiv.org/abs/2508.20892)
*Loïc Stratil,Felix Fent,Esteban Rivera,Markus Lienkamp*

Main category: cs.CV

TL;DR: 本文综述了自动驾驶感知中的统一感知方法，系统梳理了不同集成范式并提出新的分类与框架，为后续研究指明方向。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶感知依赖于分模块的流水线（如检测、跟踪和预测），虽然可解释性好，但容易产生误差累计且各任务之间协同有限，导致鲁棒性和效率受限。因此需要一种将多任务整合在一起的统一感知框架来提升整体系统性能。

Method: 论文采用文献综述方法，对现有统一感知相关研究进行系统梳理，并按照任务集成方式、跟踪表述、特征表示流动等角度建立新的分类体系。提出了三种统一感知范式（早期、晚期、完全统一），并评述现有方法的架构、训练策略、数据集和开源情况。

Result: 文章回顾并系统化了不同的统一感知方案，总结了各自特点、优势及不足，并通过详细分类揭示了领域发展现状与趋势。同时，为统一区感知领域构建了首个系统性知识框架。

Conclusion: 本综述首次建立了统一感知方法的全面框架，梳理整合了过去零散的研究工作，有助于推动更鲁棒、泛化和可解释的自动驾驶感知系统发展，并为未来研究提供指导。

Abstract: Autonomous vehicle perception typically relies on modular pipelines that
decompose the task into detection, tracking, and prediction. While
interpretable, these pipelines suffer from error accumulation and limited
inter-task synergy. Unified perception has emerged as a promising paradigm that
integrates these sub-tasks within a shared architecture, potentially improving
robustness, contextual reasoning, and efficiency while retaining interpretable
outputs. In this survey, we provide a comprehensive overview of unified
perception, introducing a holistic and systemic taxonomy that categorizes
methods along task integration, tracking formulation, and representation flow.
We define three paradigms -Early, Late, and Full Unified Perception- and
systematically review existing methods, their architectures, training
strategies, datasets used, and open-source availability, while highlighting
future research directions. This work establishes the first comprehensive
framework for understanding and advancing unified perception, consolidates
fragmented efforts, and guides future research toward more robust,
generalizable, and interpretable perception.

</details>


### [27] [Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation](https://arxiv.org/abs/2508.20471)
*Jiusi Li,Jackson Jiang,Jinyu Miao,Miao Long,Tuopu Wen,Peijin Jia,Shengxiang Liu,Chunlei Yu,Maolin Liu,Yuzhan Cai,Kun Jiang,Mengmeng Yang,Diange Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为G^2Editor的新方法，可在自动驾驶视频中实现高保真且精确的物体编辑，包括位置调整、插入和删除，并在Waymo数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中收集自动驾驶系统“极端/边角案例”非常昂贵且危险，因此需要一种有效的方法生成多样化场景，来全面训练与验证自动驾驶系统。现有方法如3D高斯渲染及图像生成模型难以兼顾高画质和精确位姿控制。

Method: G^2Editor方法基于3D高斯表征，将编辑对象以稠密先验融入去噪过程，实现精确的位姿控制与空间一致性。对非编辑目标区域采用场景级3D包围盒重建遮挡区域，并结合分层细粒度特征指导外观生成，提升视觉细节。

Result: 在Waymo Open Dataset实验中，G^2Editor能统一完成物体位置微调、插入和删除，比现有方法在姿态控制和视觉质量上更优，并能助力自动驾驶数据驱动任务。

Conclusion: G^2Editor为编辑真实驾驶视频中的物体提供了更高质量和更大灵活性，有效提升了仿真场景编辑的能力，对推动自动驾驶系统的训练与评测具有重要应用价值。

Abstract: Corner cases are crucial for training and validating autonomous driving
systems, yet collecting them from the real world is often costly and hazardous.
Editing objects within captured sensor data offers an effective alternative for
generating diverse scenarios, commonly achieved through 3D Gaussian Splatting
or image generative models. However, these approaches often suffer from limited
visual fidelity or imprecise pose control. To address these issues, we propose
G^2Editor, a framework designed for photorealistic and precise object editing
in driving videos. Our method leverages a 3D Gaussian representation of the
edited object as a dense prior, injected into the denoising process to ensure
accurate pose control and spatial consistency. A scene-level 3D bounding box
layout is employed to reconstruct occluded areas of non-target objects.
Furthermore, to guide the appearance details of the edited object, we
incorporate hierarchical fine-grained features as additional conditions during
generation. Experiments on the Waymo Open Dataset demonstrate that G^2Editor
effectively supports object repositioning, insertion, and deletion within a
unified framework, outperforming existing methods in both pose controllability
and visual quality, while also benefiting downstream data-driven tasks.

</details>


### [28] [COMETH: Convex Optimization for Multiview Estimation and Tracking of Humans](https://arxiv.org/abs/2508.20920)
*Enrico Martini,Ho Jin Choi,Nadia Figueroa,Nicola Bombieri*

Main category: cs.CV

TL;DR: 本文提出COMETH算法，通过在边缘设备上进行多视角融合并结合凸优化、运动学和生物力学约束，实现了低资源下的人体姿势追踪，兼顾精度和实时性，适合工业和安全应用。


<details>
  <summary>Details</summary>
Motivation: 工业5.0时代需要对人体活动进行高效监测以保障安全与健康，但现有多摄像头中心化方法带来高算力和带宽需求，不利于实际部署。将计算分布到边缘设备虽降低了负载，但也容易引发精度下降与时空一致性问题。

Method: 作者提出COMETH算法，该方法包括三大亮点：1）融合运动学与生物力学约束以提升关节定位准确性；2）通过基于凸优化的逆运动学实现多视角空间融合；3）利用状态观测器提升时间一致性。整个算法轻量级，适合边缘设备部署。

Result: 在公开和工业数据集上，COMETH在人体定位、检测和跟踪的精度上均优于现有方法。

Conclusion: COMETH方法兼顾了准确性与可扩展性，能够实现高效实时的人体动作跟踪，非常适合工业及安全关键场景使用，并已开源。

Abstract: In the era of Industry 5.0, monitoring human activity is essential for
ensuring both ergonomic safety and overall well-being. While multi-camera
centralized setups improve pose estimation accuracy, they often suffer from
high computational costs and bandwidth requirements, limiting scalability and
real-time applicability. Distributing processing across edge devices can reduce
network bandwidth and computational load. On the other hand, the constrained
resources of edge devices lead to accuracy degradation, and the distribution of
computation leads to temporal and spatial inconsistencies. We address this
challenge by proposing COMETH (Convex Optimization for Multiview Estimation and
Tracking of Humans), a lightweight algorithm for real-time multi-view human
pose fusion that relies on three concepts: it integrates kinematic and
biomechanical constraints to increase the joint positioning accuracy; it
employs convex optimization-based inverse kinematics for spatial fusion; and it
implements a state observer to improve temporal consistency. We evaluate COMETH
on both public and industrial datasets, where it outperforms state-of-the-art
methods in localization, detection, and tracking accuracy. The proposed fusion
pipeline enables accurate and scalable human motion tracking, making it
well-suited for industrial and safety-critical applications. The code is
publicly available at https://github.com/PARCO-LAB/COMETH.

</details>


### [29] [Enhancing Corpus Callosum Segmentation in Fetal MRI via Pathology-Informed Domain Randomization](https://arxiv.org/abs/2508.20475)
*Marina Grifell i Plana,Vladyslav Zalevskyi,Léa Schmidt,Yvan Gomez,Thomas Sanchez,Vincent Dunet,Mériam Koob,Vanessa Siffredi,Meritxell Bach Cuadra*

Main category: cs.CV

TL;DR: 本文提出了一种基于病理信息的领域随机化策略，将CCD（胼胝体发育异常）相关先验知识融入合成数据生成流程，有效提升了胎儿大脑分割表现，实现了从健康数据推断其他病变类型的能力。


<details>
  <summary>Details</summary>
Motivation: CCD等罕见脑部疾病会导致显著的结构变异，然而临床标注数据极其稀缺，阻碍了深度学习模型的泛化和应用。亟需一种方法能克服有标签病理数据不足的问题，实现对病理结构的准确分割和分析。

Method: 通过将CCD的先验解剖知识以领域随机化的方式，嵌入到健康大脑影像的合成数据生成流程中，模拟了多样的病理变异，无需真实病理分割标签即可训练模型。

Result: 在248例健康、26例CCD、47例其他病变胎儿上验证，CCD分割显著提升，同时健康及其他病例分割性能未受影响。应用所得分割图可精准推断LCC等生物标志物，其误差在健康组从1.89mm降至0.80mm，CCD组从10.9mm降至0.7mm，并在拓扑结构一致性方面优于现有方法。

Conclusion: 将特定病理结构的领域知识融合进合成数据生成流程，可弥补稀有病种样本短缺的问题，提升自动分割和表型分析的准确性与可靠性，对罕见但重要的神经发育异常有重要指导意义。

Abstract: Accurate fetal brain segmentation is crucial for extracting biomarkers and
assessing neurodevelopment, especially in conditions such as corpus callosum
dysgenesis (CCD), which can induce drastic anatomical changes. However, the
rarity of CCD severely limits annotated data, hindering the generalization of
deep learning models. To address this, we propose a pathology-informed domain
randomization strategy that embeds prior knowledge of CCD manifestations into a
synthetic data generation pipeline. By simulating diverse brain alterations
from healthy data alone, our approach enables robust segmentation without
requiring pathological annotations.
  We validate our method on a cohort comprising 248 healthy fetuses, 26 with
CCD, and 47 with other brain pathologies, achieving substantial improvements on
CCD cases while maintaining performance on both healthy fetuses and those with
other pathologies. From the predicted segmentations, we derive clinically
relevant biomarkers, such as corpus callosum length (LCC) and volume, and show
their utility in distinguishing CCD subtypes. Our pathology-informed
augmentation reduces the LCC estimation error from 1.89 mm to 0.80 mm in
healthy cases and from 10.9 mm to 0.7 mm in CCD cases. Beyond these
quantitative gains, our approach yields segmentations with improved topological
consistency relative to available ground truth, enabling more reliable
shape-based analyses. Overall, this work demonstrates that incorporating
domain-specific anatomical priors into synthetic data pipelines can effectively
mitigate data scarcity and enhance analysis of rare but clinically significant
malformations.

</details>


### [30] [CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification](https://arxiv.org/abs/2508.21046)
*Wei Li,Renshan Zhang,Rui Shao,Jie He,Liqiang Nie*

Main category: cs.CV

TL;DR: 本文提出了一种高效的认知对齐视觉-语言-动作框架CogVLA，通过多阶段稀疏化和路由机制，提升了视觉-语言-动作模型的表现与效率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作（VLA）模型需要大量训练，计算开销极大，影响实际部署和扩展，因此作者希望通过新的模型架构减小训练和推理成本，同时保证或提升性能。

Method: CogVLA 模型受人类多模态协调启发，采用三阶段渐进结构：
1）EFA-Routing将指令信息注入视觉编码器，对视觉特征进行有选择的聚合与压缩；
2）LFP-Routing在语言模型中引入动作意图，对与指令无关的视觉token进行修剪，实现稀疏化；
3）V-L-A Coupled Attention（CAtten）模块结合因果视觉-语言注意力与动作并行解码，确保经过压缩后的感知输入依然支持准确连贯的动作生成。

Result: 在LIBERO基准和真实机器人任务上，CogVLA取得了97.4%与70.0%的成功率。相比OpenVLA，训练成本降低2.5倍，推理延迟下降2.8倍，且达到SOTA水平。

Conclusion: CogVLA在确保或提升VLA系统性能的同时，极大地优化了训练和推理效率，为大规模和实际部署提供了可行方案。代码已经开源。

Abstract: Recent Vision-Language-Action (VLA) models built on pre-trained
Vision-Language Models (VLMs) require extensive post-training, resulting in
high computational overhead that limits scalability and deployment.We propose
CogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages
instruction-driven routing and sparsification to improve both efficiency and
performance. CogVLA draws inspiration from human multimodal coordination and
introduces a 3-stage progressive architecture. 1) Encoder-FiLM based
Aggregation Routing (EFA-Routing) injects instruction information into the
vision encoder to selectively aggregate and compress dual-stream visual tokens,
forming a instruction-aware latent representation. 2) Building upon this
compact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing)
introduces action intent into the language model by pruning
instruction-irrelevant visually grounded tokens, thereby achieving token-level
sparsity. 3) To ensure that compressed perception inputs can still support
accurate and coherent action generation, we introduce V-L-A Coupled Attention
(CAtten), which combines causal vision-language attention with bidirectional
action parallel decoding. Extensive experiments on the LIBERO benchmark and
real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art
performance with success rates of 97.4% and 70.0%, respectively, while reducing
training costs by 2.5-fold and decreasing inference latency by 2.8-fold
compared to OpenVLA. CogVLA is open-sourced and publicly available at
https://github.com/JiuTian-VL/CogVLA.

</details>


### [31] [Towards Inclusive Communication: A Unified LLM-Based Framework for Sign Language, Lip Movements, and Audio Understanding](https://arxiv.org/abs/2508.20476)
*Jeong Hun Yeo,Hyeongseop Rha,Sungjune Park,Junil Won,Yong Man Ro*

Main category: cs.CV

TL;DR: 本论文提出了第一个能够同时处理手语、唇动和音频等多模态输入的统一架构，大幅提升了不同任务的表现，特别是在手语翻译任务中。


<details>
  <summary>Details</summary>
Motivation: 现有的听觉识别系统对聋哑或听力障碍人士不友好，虽然手语翻译和唇读近年来有进步，但通常被孤立研究，缺少统一的多模态处理框架。

Method: 设计了一种多模态统一、与模态无关的架构，能处理手语、唇动、音频的各种组合，并探索了多模态的协同作用，特别是将唇动作为手语理解中的非手部提示进行建模。

Result: 提出的统一框架在手语翻译、视觉语音识别、语音识别，以及音视语音识别等任务上，均达到了与当前最优单任务模型相当或更优的表现。分析还发现，显式加入唇动模态能显著提升手语翻译效果。

Conclusion: 多模态融合、特别是唇动特征的显式建模，对提升无听觉环境下的人机通信能力及聋人可及性具有重要意义，统一框架为相关领域技术进步提供了新方向。

Abstract: Audio is the primary modality for human communication and has driven the
success of Automatic Speech Recognition (ASR) technologies. However, such
systems remain inherently inaccessible to individuals who are deaf or hard of
hearing. Visual alternatives such as sign language and lip reading offer
effective substitutes, and recent advances in Sign Language Translation (SLT)
and Visual Speech Recognition (VSR) have improved audio-less communication.
Yet, these modalities have largely been studied in isolation, and their
integration within a unified framework remains underexplored. In this paper, we
introduce the first unified framework capable of handling diverse combinations
of sign language, lip movements, and audio for spoken-language text generation.
We focus on three main objectives: (i) designing a unified, modality-agnostic
architecture capable of effectively processing heterogeneous inputs; (ii)
exploring the underexamined synergy among modalities, particularly the role of
lip movements as non-manual cues in sign language comprehension; and (iii)
achieving performance on par with or superior to state-of-the-art models
specialized for individual tasks. Building on this framework, we achieve
performance on par with or better than task-specific state-of-the-art models
across SLT, VSR, ASR, and AVSR. Furthermore, our analysis reveals that
explicitly modeling lip movements as a separate modality significantly improves
SLT performance.

</details>


### [32] [Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding](https://arxiv.org/abs/2508.20478)
*Yuan Xie,Tianshui Chen,Zheng Ge,Lionel Ni*

Main category: cs.CV

TL;DR: 提出了一种多轮推理的视频理解框架Video-MTR，通过逐步选择关键片段和理解问题，有效提升长视频分析的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有长视频理解方法依赖静态推理或外部视觉-语言模型，存在复杂性高、优化不足、难以端到端训练等问题，难以应对长时依赖和多事件特性。

Method: 提出Video-MTR，通过强化学习实现多轮推理，利用逐步分析方法按需选择视频片段。引入了新的门控双层奖励机制，分别对答案正确性（轨迹级）和帧-查询相关性（回合级）进行优化，无需外部视觉-语言模型，实现端到端训练。

Result: 在VideoMME、MLVU和EgoSchema等基准上进行了大量实验，Video-MTR在准确率和效率上均超越现有方法，表现出色。

Conclusion: Video-MTR有效提升了长视频理解的性能，实现了更精细和高效的推理，有望成为该领域的新主流方法。

Abstract: Long-form video understanding, characterized by long-range temporal
dependencies and multiple events, remains a challenge. Existing methods often
rely on static reasoning or external visual-language models (VLMs), which face
issues like complexity and sub-optimal performance due to the lack of
end-to-end training. In this paper, we propose Video-MTR, a reinforced
multi-turn reasoning framework designed to enable iterative key video segment
selection and question comprehension. Unlike traditional video reasoning
pipeline, which generate predictions in a single turn, Video-MTR performs
reasoning in multiple turns, selecting video segments progressively based on
the evolving understanding of previously processed segments and the current
question. This iterative process allows for a more refined and contextually
aware analysis of the video. To ensure intermediate reasoning process, we
introduce a novel gated bi-level reward system, combining trajectory-level
rewards based on answer correctness and turn-level rewards emphasizing
frame-query relevance. This system optimizes both video segment selection and
question comprehension, eliminating the need for external VLMs and allowing
end-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU,
and EgoSchema demonstrate that Video-MTR outperforms existing methods in both
accuracy and efficiency, advancing the state-of-the-art in long video
understanding.

</details>


### [33] [Adaptive Dual Uncertainty Optimization: Boosting Monocular 3D Object Detection under Test-Time Shifts](https://arxiv.org/abs/2508.20488)
*Zixuan Hu,Dongxiao Li,Xinzhu Ma,Shixiang Tang,Xiaotong Li,Wenhan Yang,Ling-Yu Duan*

Main category: cs.CV

TL;DR: 本文针对单目3D目标检测(M3OD)在真实场景域偏移下性能下降的问题，提出了首个可同时优化语义和几何不确定性的测试时自适应(TTA)框架——DUO，通过凸优化方法与新型约束，有效提升了模型在多种数据集和域偏移类型下的鲁棒性与检测性能。


<details>
  <summary>Details</summary>
Motivation: 实际应用如自动驾驶高度依赖单目3D目标检测的准确性，但模型面对环境与传感器变化，泛化能力弱，检测精度下降。现有TTA方法未能协同解决M3OD中的语义不确定性（类别预测模糊）和几何不确定性（空间定位不稳）双重问题，因此急需新方法兼顾两类不确定性以提升检测鲁棒性。

Method: 提出Dual Uncertainty Optimization (DUO) 框架，创新性地利用凸结构的focal loss及其无监督版本实现不确定性加权和高不确定性目标的均衡学习。同时，设计了基于语义感知的法向量场约束，在语义线索清晰区域保持几何一致性，形成“空间感知-语义预测”互补机制。

Result: 通过多个数据集和多种域偏移条件下的广泛实验，DUO框架在鲁棒性和检测精度方面均明显优于现有TTA方法。

Conclusion: DUO首次实现了语义与几何不确定性联合优化，有效提升了单目3D目标检测在不同域条件下的鲁棒性和实用性，为后续安全关键应用如自动驾驶提供了理论和方法支持。

Abstract: Accurate monocular 3D object detection (M3OD) is pivotal for safety-critical
applications like autonomous driving, yet its reliability deteriorates
significantly under real-world domain shifts caused by environmental or sensor
variations. To address these shifts, Test-Time Adaptation (TTA) methods have
emerged, enabling models to adapt to target distributions during inference.
While prior TTA approaches recognize the positive correlation between low
uncertainty and high generalization ability, they fail to address the dual
uncertainty inherent to M3OD: semantic uncertainty (ambiguous class
predictions) and geometric uncertainty (unstable spatial localization). To
bridge this gap, we propose Dual Uncertainty Optimization (DUO), the first TTA
framework designed to jointly minimize both uncertainties for robust M3OD.
Through a convex optimization lens, we introduce an innovative convex structure
of the focal loss and further derive a novel unsupervised version, enabling
label-agnostic uncertainty weighting and balanced learning for high-uncertainty
objects. In parallel, we design a semantic-aware normal field constraint that
preserves geometric coherence in regions with clear semantic cues, reducing
uncertainty from the unstable 3D representation. This dual-branch mechanism
forms a complementary loop: enhanced spatial perception improves semantic
classification, and robust semantic predictions further refine spatial
understanding. Extensive experiments demonstrate the superiority of DUO over
existing methods across various datasets and domain shift types.

</details>


### [34] [CaddieSet: A Golf Swing Dataset with Human Joint Features and Ball Information](https://arxiv.org/abs/2508.20491)
*Seunghyeon Jung,Seoyoung Hong,Jiwoo Jeong,Seungwon Jeong,Jaerim Choi,Hoki Kim,Woojin Lee*

Main category: cs.CV

TL;DR: 本文提出了CaddieSet数据集，结合高尔夫挥杆过程中的关节及球的相关信息，能够更好地分析挥杆姿势与球轨迹间的关系，并通过基于计算机视觉的方法分割挥杆阶段，实现了精准的训练与反馈。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法无法定量建立高尔夫挥杆姿势和球轨迹之间的联系，限制了对球手提供有效的挥杆优化建议，因此需要新的数据集和分析方法。

Method: 作者构建了CaddieSet数据集，收集单次挥杆中的关节和球体信息，并利用计算机视觉方法将挥杆分割为八个阶段。此外，结合专家知识定义了15个关键影响指标，用于解释挥杆结果。

Result: 实验证明，CaddieSet在多种基准测试中可有效预测球的轨迹，并且基于关节特征的挥杆反馈与高尔夫领域专业知识吻合。

Conclusion: CaddieSet及相关方法能够为高尔夫挥杆分析提供新见解，对学术界和体育业具有广泛的应用前景。

Abstract: Recent advances in deep learning have led to more studies to enhance golfers'
shot precision. However, these existing studies have not quantitatively
established the relationship between swing posture and ball trajectory,
limiting their ability to provide golfers with the necessary insights for swing
improvement. In this paper, we propose a new dataset called CaddieSet, which
includes joint information and various ball information from a single shot.
CaddieSet extracts joint information from a single swing video by segmenting it
into eight swing phases using a computer vision-based approach. Furthermore,
based on expert golf domain knowledge, we define 15 key metrics that influence
a golf swing, enabling the interpretation of swing outcomes through
swing-related features. Through experiments, we demonstrated the feasibility of
CaddieSet for predicting ball trajectories using various benchmarks. In
particular, we focus on interpretable models among several benchmarks and
verify that swing feedback using our joint features is quantitatively
consistent with established domain knowledge. This work is expected to offer
new insight into golf swing analysis for both academia and the sports industry.

</details>


### [35] [IAENet: An Importance-Aware Ensemble Model for 3D Point Cloud-Based Anomaly Detection](https://arxiv.org/abs/2508.20492)
*Xuanming Cao,Chengyu Tao,Yifeng Cheng,Juan Du*

Main category: cs.CV

TL;DR: 本文提出了一种结合2D与3D专家模型的表面异常检测新方法，提升了3D点云检测性能，达到了业界领先水平。


<details>
  <summary>Details</summary>
Motivation: 近年来2D图像在表面异常检测上表现优异，而可以利用更多几何信息的3D点云检测却因缺乏强大预训练骨干模型而发展缓慢，成为工业实际场景中的检测瓶颈。

Method: 提出Importance-Aware Ensemble Network (IAENet) 框架，融合2D和3D专家模型的检测结果，通过全新的Importance-Aware Fusion (IAF)模块动态评估各自贡献并加权整合异常分数，同时设计关键损失函数以引导IAF优化兼容所有专家优势。

Result: 在MVTec 3D-AD数据集上，IAENet取得了新的SOTA，显著降低了误报率，优于现有方法。

Conclusion: 该方法有效结合2D和3D检测的互补信息，突破了3D异常检测依赖单一模型效果不佳的难题，具有良好的工业应用前景。

Abstract: Surface anomaly detection is pivotal for ensuring product quality in
industrial manufacturing. While 2D image-based methods have achieved remarkable
success, 3D point cloud-based detection remains underexplored despite its
richer geometric cues. We argue that the key bottleneck is the absence of
powerful pretrained foundation backbones in 3D comparable to those in 2D. To
bridge this gap, we propose Importance-Aware Ensemble Network (IAENet), an
ensemble framework that synergizes 2D pretrained expert with 3D expert models.
However, naively fusing predictions from disparate sources is non-trivial:
existing strategies can be affected by a poorly performing modality and thus
degrade overall accuracy. To address this challenge, We introduce an novel
Importance-Aware Fusion (IAF) module that dynamically assesses the contribution
of each source and reweights their anomaly scores. Furthermore, we devise
critical loss functions that explicitly guide the optimization of IAF, enabling
it to combine the collective knowledge of the source experts but also preserve
their unique strengths, thereby enhancing the overall performance of anomaly
detection. Extensive experiments on MVTec 3D-AD demonstrate that our IAENet
achieves a new state-of-the-art with a markedly lower false positive rate,
underscoring its practical value for industrial deployment.

</details>


### [36] [Describe, Don't Dictate: Semantic Image Editing with Natural Language Intent](https://arxiv.org/abs/2508.20505)
*En Ci,Shanyan Guan,Yanhao Ge,Yilin Zhang,Wei Li,Zhenyu Zhang,Jian Yang,Ying Tai*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于描述性提示词的图像编辑框架DescriptiveEdit，通过将图像编辑任务转化为参考图像引导的文生图任务，提升了编辑的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 已有的基于反演的方法会引入重建误差，而基于指令的方法受限于数据集的质量与规模。需要一种既能充分利用现有大规模文生图模型能力、又能减少重建误差的方法提升图像编辑效果。

Method: DescriptiveEdit框架提出以参考图像和Prompt为输入，通过引入Cross-Attentive UNet网络结构，在图像生成过程中将参考图像特征注入文本到图像的生成流程，无需架构改动或反演操作，增强编辑效果。

Result: 在Emu Edit基准测试上，该方法在图像编辑的准确性和一致性方面均有提升，并能无缝集成到ControlNet、IP-Adapter等扩展模块中，显示良好的可扩展性。

Conclusion: DescriptiveEdit通过创新思路重构图像编辑流程，有效克服现有方法的局限，未来在高质量、高一致性图像编辑领域具有良好应用前景。

Abstract: Despite the progress in text-to-image generation, semantic image editing
remains a challenge. Inversion-based algorithms unavoidably introduce
reconstruction errors, while instruction-based models mainly suffer from
limited dataset quality and scale. To address these problems, we propose a
descriptive-prompt-based editing framework, named DescriptiveEdit. The core
idea is to re-frame `instruction-based image editing' as `reference-image-based
text-to-image generation', which preserves the generative power of well-trained
Text-to-Image models without architectural modifications or inversion.
Specifically, taking the reference image and a prompt as input, we introduce a
Cross-Attentive UNet, which newly adds attention bridges to inject reference
image features into the prompt-to-edit-image generation process. Owing to its
text-to-image nature, DescriptiveEdit overcomes limitations in instruction
dataset quality, integrates seamlessly with ControlNet, IP-Adapter, and other
extensions, and is more scalable. Experiments on the Emu Edit benchmark show it
improves editing accuracy and consistency.

</details>


### [37] [DCFS: Continual Test-Time Adaptation via Dual Consistency of Feature and Sample](https://arxiv.org/abs/2508.20516)
*Wenting Yin,Han Sun,Xinru Meng,Ningzhong Liu,Huiyu Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新的持续测试时自适应（CTTA）框架DCFS，通过双路径特征一致性和置信感知样本学习，有效解决伪标签噪声和误差累积问题，在多个数据集上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 在持续测试时自适应中，难点在于无源域数据情况下模型只能依赖目标域特征，导致伪标签质量难以保证以及误差累积问题，现有方法对此处理不足。

Method: 提出DCFS框架：1）利用双分类器将目标数据的整体特征向量分解为语义相关和域相关子特征，并保持这些子特征与整体特征的一致性，实现多角度特征捕捉；2）为每个样本计算置信分数并设自适应阈值，通过置信感知的加权损失进行自监督学习，降低伪标签噪声与误差积累。

Result: 在CIFAR10-C、CIFAR100-C和ImageNet-C等多个数据集进行了实验，结果表明所提方法在持续测试时自适应场景下获得了稳定且优越的性能。

Conclusion: DCFS框架有效提升了CTTA的自适应性能，同时抑制了伪标签噪声及误差累积问题，是应对无源域数据条件下持续自适应的有效解决方案。

Abstract: Continual test-time adaptation aims to continuously adapt a pre-trained model
to a stream of target domain data without accessing source data. Without access
to source domain data, the model focuses solely on the feature characteristics
of the target data. Relying exclusively on these features can lead to confusion
and introduce learning biases. Currently, many existing methods generate
pseudo-labels via model predictions. However, the quality of pseudo-labels
cannot be guaranteed and the problem of error accumulation must be solved. To
address these challenges, we propose DCFS, a novel CTTA framework that
introduces dual-path feature consistency and confidence-aware sample learning.
This framework disentangles the whole feature representation of the target data
into semantic-related feature and domain-related feature using dual classifiers
to learn distinct feature representations. By maintaining consistency between
the sub-features and the whole feature, the model can comprehensively capture
data features from multiple perspectives. Additionally, to ensure that the
whole feature information of the target domain samples is not overlooked, we
set a adaptive threshold and calculate a confidence score for each sample to
carry out loss weighted self-supervised learning, effectively reducing the
noise of pseudo-labels and alleviating the problem of error accumulation. The
efficacy of our proposed method is validated through extensive experimentation
across various datasets, including CIFAR10-C, CIFAR100-C, and ImageNet-C,
demonstrating consistent performance in continual test-time adaptation
scenarios.

</details>


### [38] [Adam SLAM - the last mile of camera calibration with 3DGS](https://arxiv.org/abs/2508.20526)
*Matthieu Gendrin,Stéphane Pateux,Xiaoran Jiang,Théo Ladune,Luce Morin*

Main category: cs.CV

TL;DR: 该论文提出通过3DGS模型和基于新视角颜色损失的反向传播，优化相机标定参数，从而提升新视角合成质量。实验证明，相机标定的微调平均可提升0.4 dB的PSNR。


<details>
  <summary>Details</summary>
Motivation: 相机标定误差对新视角合成质量影响很大，尤其是在缺乏真实场景的标定真值时，如何提升标定质量成为亟需解决的问题。新视角合成的质量直接反映了标定的优劣。

Method: 利用3DGS（3D Gaussian Splatting）模型，通过对新视角合成图像的颜色损失关于相机参数进行反向传播，实现对相机标定参数的微调。

Result: 在3DGS参考数据集上，采用该方法微调后的标定，平均提升了0.4 dB的PSNR，合成质量显著提高。

Conclusion: 该方法能显著提升重要参考场景的相机标定质量，尤其适用于对新视角质量要求极高的场景。尽管微调过程耗时较长，但对需要高精度合成的任务具有很高价值。

Abstract: The quality of the camera calibration is of major importance for evaluating
progresses in novel view synthesis, as a 1-pixel error on the calibration has a
significant impact on the reconstruction quality. While there is no ground
truth for real scenes, the quality of the calibration is assessed by the
quality of the novel view synthesis. This paper proposes to use a 3DGS model to
fine tune calibration by backpropagation of novel view color loss with respect
to the cameras parameters. The new calibration alone brings an average
improvement of 0.4 dB PSNR on the dataset used as reference by 3DGS. The fine
tuning may be long and its suitability depends on the criticity of training
time, but for calibration of reference scenes, such as Mip-NeRF 360, the stake
of novel view quality is the most important.

</details>


### [39] [Learning What is Worth Learning: Active and Sequential Domain Adaptation for Multi-modal Gross Tumor Volume Segmentation](https://arxiv.org/abs/2508.20528)
*Jingyun Yang,Guoqing Zhang,Jingge Wang,Yang Li*

Main category: cs.CV

TL;DR: 本文提出了一种主动与序列域自适应框架，实现多模态医学数据中动态样本选择，有效减少肿瘤分割中的标注成本，并在若干任务上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 多模态医学图像肿瘤分割对放疗计划至关重要，但高质量标注数据稀缺且标注耗时，这促使研究者寻求主动学习和自适应迁移技术以最大化有限标注数据的价值。

Method: 作者提出了一种主动且序列的域自适应方法，针对多模态数据，设计算法根据样本的信息量和代表性动态优先选择标注和训练样本，从而提升分割模型性能并降低负迁移风险。

Result: 实验覆盖了不同的肿瘤体积分割任务，所提方法在分割表现上显著优于现有主动域自适应方法，展现出更强效果。

Conclusion: 该研究为主动领域自适应在多模态医学图像分割领域提供了新策略，显著提升分割性能并有效降低标注成本，有望促进相关医学影像智能应用。

Abstract: Accurate gross tumor volume segmentation on multi-modal medical data is
critical for radiotherapy planning in nasopharyngeal carcinoma and
glioblastoma. Recent advances in deep neural networks have brought promising
results in medical image segmentation, leading to an increasing demand for
labeled data. Since labeling medical images is time-consuming and
labor-intensive, active learning has emerged as a solution to reduce annotation
costs by selecting the most informative samples to label and adapting
high-performance models with as few labeled samples as possible. Previous
active domain adaptation (ADA) methods seek to minimize sample redundancy by
selecting samples that are farthest from the source domain. However, such
one-off selection can easily cause negative transfer, and access to source
medical data is often limited. Moreover, the query strategy for multi-modal
medical data remains unexplored. In this work, we propose an active and
sequential domain adaptation framework for dynamic multi-modal sample selection
in ADA. We derive a query strategy to prioritize labeling and training on the
most valuable samples based on their informativeness and representativeness.
Empirical validation on diverse gross tumor volume segmentation tasks
demonstrates that our method achieves favorable segmentation performance,
significantly outperforming state-of-the-art ADA methods. Code is available at
the git repository: \href{https://github.com/Hiyoochan/mmActS}{mmActS}.

</details>


### [40] [Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for Unsupervised 3D Object Detection](https://arxiv.org/abs/2508.20530)
*Mingqian Ji,Jian Yang,Shanshan Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的数据级融合框架，通过在早期集成RGB图像与LiDAR数据，实现无需人工标注的高性能3D目标检测。


<details>
  <summary>Details</summary>
Motivation: 目前主流的LiDAR点云3D目标检测方法对人工标注依赖很大，而高质量3D标签获取成本高、效率低。现有无监督方案通常只在标签层面对点云与RGB伪标签做融合，无法充分利用两种模态的互补信息，提升有限。

Method: 作者提出在数据级融合阶段结合RGB图像和LiDAR点云。通过视觉基础模型对图像做实例分割与深度估计，并提出双向融合方法：一方面把2D中的类别标签赋予真实点云，另一方面将2D像素映射到3D补充点云密度。同时设计了局部半径滤波抑制深度噪声和全局统计滤波去除分割异常点。此外，引入动态自进化策略，在稠密表示下迭代优化伪框。

Result: 在nuScenes数据集上实验证明，该方法远优于现有主流无监督方法，在验证集上mAP提升至28.4%。

Conclusion: 所提方法充分挖掘RGB图像和LiDAR点云互补性，通过数据级融合与动态自进化策略，极大提升了无监督3D目标检测精度，有望推动行业应用发展。

Abstract: Existing LiDAR-based 3D object detectors typically rely on manually annotated
labels for training to achieve good performance. However, obtaining
high-quality 3D labels is time-consuming and labor-intensive. To address this
issue, recent works explore unsupervised 3D object detection by introducing RGB
images as an auxiliary modal to assist pseudo-box generation. However, these
methods simply integrate pseudo-boxes generated by LiDAR point clouds and RGB
images. Yet, such a label-level fusion strategy brings limited improvements to
the quality of pseudo-boxes, as it overlooks the complementary nature in terms
of LiDAR and RGB image data. To overcome the above limitations, we propose a
novel data-level fusion framework that integrates RGB images and LiDAR data at
an early stage. Specifically, we utilize vision foundation models for instance
segmentation and depth estimation on images and introduce a bi-directional
fusion method, where real points acquire category labels from the 2D space,
while 2D pixels are projected onto 3D to enhance real point density. To
mitigate noise from depth and segmentation estimations, we propose a local and
global filtering method, which applies local radius filtering to suppress depth
estimation errors and global statistical filtering to remove
segmentation-induced outliers. Furthermore, we propose a data-level fusion
based dynamic self-evolution strategy, which iteratively refines pseudo-boxes
under a dense representation, significantly improving localization accuracy.
Extensive experiments on the nuScenes dataset demonstrate that the detector
trained by our method significantly outperforms that trained by previous
state-of-the-art methods with 28.4$\%$ mAP on the nuScenes validation
benchmark.

</details>


### [41] [Digital Scale: Open-Source On-Device BMI Estimation from Smartphone Camera Images Trained on a Large-Scale Real-World Dataset](https://arxiv.org/abs/2508.20534)
*Frederik Rajiv Manichand,Robin Deuber,Robert Jakob,Steve Swerling,Jamie Rosen,Elgar Fleisch,Patrick Langer*

Main category: cs.CV

TL;DR: 本研究利用深度学习方法，从智能手机图像中准确估算身体质量指数（BMI），并在大规模数据集上取得了当前最低的误差率。


<details>
  <summary>Details</summary>
Motivation: 当前用图像计算BMI的方法受限于数据集规模，且在远程医疗或紧急情况下传统测量手段难以实施，因此需要更高效、规模化且准确的BMI估算方法。

Method: 作者开发了WayBED数据集（包含84,963张图像），并提出自动图像过滤方法（姿态聚类和人体检测）以剔除低质量样本，最后采用深度学习模型进行BMI估算，并应用在移动端。

Result: 在WayBED测试集上，模型MAPE为7.9%，为已知最低值；在未见过的VisualBodyToBMI数据集上MAPE为13%；微调后在该数据集上MAPE降至8.56%。

Conclusion: 该方法显著提高了BMI视觉估算的准确性与泛化能力，并可在智能手机等设备端部署，有助于实际远程健康场景应用，相关代码已开源。

Abstract: Estimating Body Mass Index (BMI) from camera images with machine learning
models enables rapid weight assessment when traditional methods are unavailable
or impractical, such as in telehealth or emergency scenarios. Existing computer
vision approaches have been limited to datasets of up to 14,500 images. In this
study, we present a deep learning-based BMI estimation method trained on our
WayBED dataset, a large proprietary collection of 84,963 smartphone images from
25,353 individuals. We introduce an automatic filtering method that uses
posture clustering and person detection to curate the dataset by removing
low-quality images, such as those with atypical postures or incomplete views.
This process retained 71,322 high-quality images suitable for training. We
achieve a Mean Absolute Percentage Error (MAPE) of 7.9% on our hold-out test
set (WayBED data) using full-body images, the lowest value in the published
literature to the best of our knowledge. Further, we achieve a MAPE of 13% on
the completely unseen~(during training) VisualBodyToBMI dataset, comparable
with state-of-the-art approaches trained on it, demonstrating robust
generalization. Lastly, we fine-tune our model on VisualBodyToBMI and achieve a
MAPE of 8.56%, the lowest reported value on this dataset so far. We deploy the
full pipeline, including image filtering and BMI estimation, on Android devices
using the CLAID framework. We release our complete code for model training,
filtering, and the CLAID package for mobile deployment as open-source
contributions.

</details>


### [42] [Domain Adaptation Techniques for Natural and Medical Image Classification](https://arxiv.org/abs/2508.20537)
*Ahmad Chaddad,Yihang Wu,Reem Kateb,Christian Desrosiers*

Main category: cs.CV

TL;DR: 本文对七种主流领域自适应（DA）方法在五个自然图像和八个医学图像数据集进行大规模对比实验，发现DSAN方法在医学图像上表现突出，在COVID-19等数据集上表现优异，并具备较高可解释性。


<details>
  <summary>Details</summary>
Motivation: 领域自适应虽在自然图像分类领域取得诸多进展，但在医学图像中应用较少，且主流数据集可能导致性能偏差。作者希望系统比较不同DA方法在自然及医学图像上的效果，为医学图像机器学习提供参考。

Method: 作者在五个自然、八个医学图像数据集上，使用七种常见DA方法，覆盖分布外、动态数据流及训练样本有限等多种情境，进行了557组仿真实验，对比了方法的分类精度及可解释性。

Result: 实验证明，Deep Subdomain Adaptation Network（DSAN）算法在COVID-19数据集上使用Resnet50获得了91.2%的准确率，在动态数据流场景下对比基线提升了6.7%。此外，DSAN在COVID-19和皮肤癌等医学数据集上解释性优秀。

Conclusion: 本研究为领域自适应方法在医学图像上的应用提供了系统性分析和洞见，验证了DSAN等方法的有效性和实用性，对推动医学图像自动化分析有重要意义。

Abstract: Domain adaptation (DA) techniques have the potential in machine learning to
alleviate distribution differences between training and test sets by leveraging
information from source domains. In image classification, most advances in DA
have been made using natural images rather than medical data, which are harder
to work with. Moreover, even for natural images, the use of mainstream datasets
can lead to performance bias. {With the aim of better understanding the
benefits of DA for both natural and medical images, this study performs 557
simulation studies using seven widely-used DA techniques for image
classification in five natural and eight medical datasets that cover various
scenarios, such as out-of-distribution, dynamic data streams, and limited
training samples.} Our experiments yield detailed results and insightful
observations highlighting the performance and medical applicability of these
techniques. Notably, our results have shown the outstanding performance of the
Deep Subdomain Adaptation Network (DSAN) algorithm. This algorithm achieved
feasible classification accuracy (91.2\%) in the COVID-19 dataset using
Resnet50 and showed an important accuracy improvement in the dynamic data
stream DA scenario (+6.7\%) compared to the baseline. Our results also
demonstrate that DSAN exhibits remarkable level of explainability when
evaluated on COVID-19 and skin cancer datasets. These results contribute to the
understanding of DA techniques and offer valuable insight into the effective
adaptation of models to medical data.

</details>


### [43] [Contrastive Learning through Auxiliary Branch for Video Object Detection](https://arxiv.org/abs/2508.20551)
*Lucas Rakotoarivony*

Main category: cs.CV

TL;DR: 本文提出一种简单但有效的辅助分支对比学习方法（CLAB），在不增加推理时计算量的前提下，显著提升了视频目标检测的精度，在ImageNet VID数据集上取得了当前CNN模型的最优结果。


<details>
  <summary>Details</summary>
Motivation: 视频目标检测受运动模糊、遮挡及物体形变等图像劣化影响，难度高于静态图像检测。以往方法通过特征聚合和复杂后处理提升性能，但增加了计算开销。作者希望提出一种在不增加推理复杂度下提升模型鲁棒性的方案。

Method: 作者设计了对比学习的辅助分支，通过引入对比损失，增强主干网络的特征表达能力。提出动态损失加权策略：训练初期更关注辅助分支学习，训练后期则更重主任务检测。整个方法在推理时不会带来额外计算负担。

Result: 在ImageNet VID基准上，采用ResNet-101和ResNeXt-101主干网络，CLAB分别达到了84.0%和85.2%的mAP。实验与消融分析表明该方法在不使用额外后处理的情况下能稳定提升性能。

Conclusion: CLAB方法能以低成本提升视频目标检测中CNN模型的鲁棒性和准确率，为实际复杂场景下的目标检测落地提供了有价值的技术方案。

Abstract: Video object detection is a challenging task because videos often suffer from
image deterioration such as motion blur, occlusion, and deformable shapes,
making it significantly more difficult than detecting objects in still images.
Prior approaches have improved video object detection performance by employing
feature aggregation and complex post-processing techniques, though at the cost
of increased computational demands. To improve robustness to image degradation
without additional computational load during inference, we introduce a
straightforward yet effective Contrastive Learning through Auxiliary Branch
(CLAB) method. First, we implement a constrastive auxiliary branch using a
contrastive loss to enhance the feature representation capability of the video
object detector's backbone. Next, we propose a dynamic loss weighting strategy
that emphasizes auxiliary feature learning early in training while gradually
prioritizing the detection task as training converges. We validate our approach
through comprehensive experiments and ablation studies, demonstrating
consistent performance gains. Without bells and whistles, CLAB reaches a
performance of 84.0% mAP and 85.2% mAP with ResNet-101 and ResNeXt-101,
respectively, on the ImageNet VID dataset, thus achieving state-of-the-art
performance for CNN-based models without requiring additional post-processing
methods.

</details>


### [44] [Towards Mechanistic Defenses Against Typographic Attacks in CLIP](https://arxiv.org/abs/2508.20570)
*Lorenz Hufe,Constantin Venhoff,Maximilian Dreyer,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.CV

TL;DR: 本论文分析了CLIP视觉编码器在排版攻击下的表现，并提出了一种无需微调的防御方法，通过屏蔽特定注意力头预防排版攻击，显著提升了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 排版攻击可以让多模态系统（如CLIP）因图像中的字体注入而误判或被越狱，威胁系统安全。因此，理解和提升模型抵抗排版攻击的能力具有重要意义。

Method: 作者通过分析CLIP模型内部，定位到专门负责提取和传递排版信息的注意力头。基于此，提出了一种通过有选择性屏蔽这些注意力通路的防御方法，无需重新训练模型。

Result: 在ImageNet-100的排版攻击版本下，防御方法将模型表现提升了19.6%，而在标准ImageNet-100测试下性能仅下降不到1%。

Conclusion: 提出的训练免疫方法在抵抗排版攻击方面与现有的微调方法效果接近，同时更易部署。论文还开源了更抗排版攻击的“dyslexic CLIP”模型，适合在安全敏感的场景下作为替代品。

Abstract: Typographic attacks exploit multi-modal systems by injecting text into
images, leading to targeted misclassifications, malicious content generation
and even Vision-Language Model jailbreaks. In this work, we analyze how CLIP
vision encoders behave under typographic attacks, locating specialized
attention heads in the latter half of the model's layers that causally extract
and transmit typographic information to the cls token. Building on these
insights, we introduce a method to defend CLIP models against typographic
attacks by selectively ablating a typographic circuit, consisting of attention
heads. Without requiring finetuning, our method improves performance by up to
19.6% on a typographic variant of ImageNet-100, while reducing standard
ImageNet-100 accuracy by less than 1%. Notably, our training-free approach
remains competitive with current state-of-the-art typographic defenses that
rely on finetuning. To this end, we release a family of dyslexic CLIP models
which are significantly more robust against typographic attacks. These models
serve as suitable drop-in replacements for a broad range of safety-critical
applications, where the risks of text-based manipulation outweigh the utility
of text recognition.

</details>


### [45] [GLaRE: A Graph-based Landmark Region Embedding Network for Emotion Recognition](https://arxiv.org/abs/2508.20579)
*Debasis Maji,Debaditya Barman*

Main category: cs.CV

TL;DR: 该论文提出了一种基于图神经网络（GNN）的面部表情识别新方法GLaRE，有效提升了表情识别的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统面部表情识别系统在遮挡、表情变化及可解释性等方面存在挑战，因而有必要探索更有效且可解释的识别方法。

Method: 利用3D面部对齐提取面部关键点，通过层次化粗化构建商图，保留空间结构，减少复杂度，并基于区域嵌入进行情感分类。

Result: 在AffectNet上取得了64.89%的准确率，在FERG数据集上达到了94.24%，均优于多种现有基线方法。消融实验表明区域级嵌入对提升效果有明显贡献。

Conclusion: GLaRE方法在提升准确率、可解释性及处理复杂表情变化方面具备显著优势，对FER领域具有应用潜力。

Abstract: Facial expression recognition (FER) is a crucial task in computer vision with
wide range of applications including human computer interaction, surveillance,
and assistive technologies. However, challenges such as occlusion, expression
variability, and lack of interpretability hinder the performance of traditional
FER systems. Graph Neural Networks (GNNs) offer a powerful alternative by
modeling relational dependencies between facial landmarks, enabling structured
and interpretable learning. In this paper, we propose GLaRE, a novel
Graph-based Landmark Region Embedding network for emotion recognition. Facial
landmarks are extracted using 3D facial alignment, and a quotient graph is
constructed via hierarchical coarsening to preserve spatial structure while
reducing complexity. Our method achieves 64.89 percentage accuracy on AffectNet
and 94.24 percentage on FERG, outperforming several existing baselines.
Additionally, ablation studies have demonstrated that region-level embeddings
from quotient graphs have contributed to improved prediction performance.

</details>


### [46] [FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models](https://arxiv.org/abs/2508.20586)
*Zheng Chong,Yanwei Lei,Shiyue Zhang,Zhuandi He,Zhen Wang,Xujie Zhang,Xiao Dong,Yiling Wu,Dongmei Jiang,Xiaodan Liang*

Main category: cs.CV

TL;DR: 本文提出了一种名为FastFit的新型高效虚拟试穿系统，实现了多参考服饰（含配件）的高效穿搭和可复用特征编码，在速度和精度上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿技术难以支持多参考服饰组合（如同时搭配多种衣物和配件），且在每一步去噪时都需重复提取特征，导致效率极低，无法满足实际应用需求。

Method: 作者提出基于可缓存扩散架构的FastFit框架，通过Semi-Attention机制和用类别嵌入代替传统时间步嵌入，彻底解耦了参考特征的编码与去噪过程，使得参考特征只需计算一次，能在各步高效复用，显著提升推理速度。

Result: FastFit在VITON-HD、DressCode和作者新提出的DressCode-MR（含五大类服饰配件的大型数据集）上进行实验，相比现有方法，在关键保真度指标上均取得更好表现，推理速度平均提升3.5倍。

Conclusion: FastFit突破了多服饰高效虚拟试穿的瓶颈，可大幅提升实际应用可行性，对相关研究有重要推动作用。

Abstract: Despite its great potential, virtual try-on technology is hindered from
real-world application by two major challenges: the inability of current
methods to support multi-reference outfit compositions (including garments and
accessories), and their significant inefficiency caused by the redundant
re-computation of reference features in each denoising step. To address these
challenges, we propose FastFit, a high-speed multi-reference virtual try-on
framework based on a novel cacheable diffusion architecture. By employing a
Semi-Attention mechanism and substituting traditional timestep embeddings with
class embeddings for reference items, our model fully decouples reference
feature encoding from the denoising process with negligible parameter overhead.
This allows reference features to be computed only once and losslessly reused
across all steps, fundamentally breaking the efficiency bottleneck and
achieving an average 3.5x speedup over comparable methods. Furthermore, to
facilitate research on complex, multi-reference virtual try-on, we introduce
DressCode-MR, a new large-scale dataset. It comprises 28,179 sets of
high-quality, paired images covering five key categories (tops, bottoms,
dresses, shoes, and bags), constructed through a pipeline of expert models and
human feedback refinement. Extensive experiments on the VITON-HD, DressCode,
and our DressCode-MR datasets show that FastFit surpasses state-of-the-art
methods on key fidelity metrics while offering its significant advantage in
inference efficiency.

</details>


### [47] [UTA-Sign: Unsupervised Thermal Video Augmentation via Event-Assisted Traffic Signage Sketching](https://arxiv.org/abs/2508.20594)
*Yuqi Han,Songqian Zhang,Weijian Su,Ke Li,Jiayu Yang,Jinli Suo,Qiang Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种结合热成像和神经形态相机（事件相机）的无监督交通标志视频增强方法，有效提升了低光环境下标志检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 热成像相机在低光条件下能够很好地感知环境，但难以区分材质相同的交通标志，影响自动驾驶的语义理解。而事件相机可在高速、低光下检测光强变化，但存在采样非均匀的问题。二者优缺点互补，因此需要融合两种传感器的数据，解决单一传感器存在的盲点和采样问题。

Method: 提出UTA-Sign方法，通过无监督方式对低照度下的交通标志（如车牌、路障指示）进行热成像-事件视频增强。设计了双提升机制：一方面用热成像帧提供准确运动线索，作为对齐事件信号的时序参考；另一方面让事件信号增强热成像帧中标志的细节表达，实现多模态信息融合。

Result: 在真实场景采集的数据集上进行了实验，结果显示该方法在交通标志草图提取质量和检测准确率方面均优于现有方法。

Conclusion: 多模态融合显著改善了低光环境下交通标志的检测与识别，为自动驾驶等实际应用提供了更可靠的感知能力。

Abstract: The thermal camera excels at perceiving outdoor environments under low-light
conditions, making it ideal for applications such as nighttime autonomous
driving and unmanned navigation. However, thermal cameras encounter challenges
when capturing signage from objects made of similar materials, which can pose
safety risks for accurately understanding semantics in autonomous driving
systems. In contrast, the neuromorphic vision camera, also known as an event
camera, detects changes in light intensity asynchronously and has proven
effective in high-speed, low-light traffic environments. Recognizing the
complementary characteristics of these two modalities, this paper proposes
UTA-Sign, an unsupervised thermal-event video augmentation for traffic signage
in low-illumination environments, targeting elements such as license plates and
roadblock indicators. To address the signage blind spots of thermal imaging and
the non-uniform sampling of event cameras, we developed a dual-boosting
mechanism that fuses thermal frames and event signals for consistent signage
representation over time. The proposed method utilizes thermal frames to
provide accurate motion cues as temporal references for aligning the uneven
event signals. At the same time, event signals contribute subtle signage
content to the raw thermal frames, enhancing the overall understanding of the
environment. The proposed method is validated on datasets collected from
real-world scenarios, demonstrating superior quality in traffic signage
sketching and improved detection accuracy at the perceptual level.

</details>


### [48] [Disruptive Attacks on Face Swapping via Low-Frequency Perceptual Perturbations](https://arxiv.org/abs/2508.20595)
*Mengxiao Huang,Minglei Shu,Shuwang Zhou,Zhaoyang Liu*

Main category: cs.CV

TL;DR: 本文提出一种主动防御Deepfake攻击的新方法，通过低频感知扰动，显著降低深度换脸技术的效果，同时保持图像自然感。


<details>
  <summary>Details</summary>
Motivation: 当前Deepfake伪造技术对隐私和社会安全构成威胁，现有检测方法大多是被动检测，难以在事前或过程中防御，存在明显局限性。

Method: 作者提出了一种基于低频域感知扰动的主动防御Deepfake方法。具体方法为：1）结合频域与空域特征，通过将离散小波变换（DWT）提取的低频分量引入扰动，直接干扰深度换脸生成过程；2）设计包含编码器、扰动生成器和解码器的完整架构，对输入图像生成带有低频伪影但保留高频细节的图像，以破坏伪造的自然性并降低有效性。

Result: 在CelebA-HQ和LFW数据集上实验表明，该方法明显削弱了换脸伪造的效果，提高了防御成功率，同时较好地保持了视觉质量。

Conclusion: 这种基于低频扰动的主动防御方法能够显著遏制Deepfake换脸技术，保护图像真实性，并在实际应用中具有很大潜力。

Abstract: Deepfake technology, driven by Generative Adversarial Networks (GANs), poses
significant risks to privacy and societal security. Existing detection methods
are predominantly passive, focusing on post-event analysis without preventing
attacks. To address this, we propose an active defense method based on
low-frequency perceptual perturbations to disrupt face swapping manipulation,
reducing the performance and naturalness of generated content. Unlike prior
approaches that used low-frequency perturbations to impact classification
accuracy,our method directly targets the generative process of deepfake
techniques. We combine frequency and spatial domain features to strengthen
defenses. By introducing artifacts through low-frequency perturbations while
preserving high-frequency details, we ensure the output remains visually
plausible. Additionally, we design a complete architecture featuring an
encoder, a perturbation generator, and a decoder, leveraging discrete wavelet
transform (DWT) to extract low-frequency components and generate perturbations
that disrupt facial manipulation models. Experiments on CelebA-HQ and LFW
demonstrate significant reductions in face-swapping effectiveness, improved
defense success rates, and preservation of visual quality.

</details>


### [49] [Embracing Aleatoric Uncertainty: Generating Diverse 3D Human Motion](https://arxiv.org/abs/2508.20604)
*Zheng Qin,Yabing Wang,Minghui Yang,Sanping Zhou,Ming Yang,Le Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Diverse-T2M的文本到3D人体动作生成方法，能够在保证文本与动作一致性的前提下，明显提升生成动作的多样性。


<details>
  <summary>Details</summary>
Motivation: 当前文本到动作生成领域已能生成高质量且精准的人体动作，但生成动作的多样性依然是主要难题，影响到实际应用价值。

Method: 方法上，作者提出在transformer结构中引入噪声信号来显式建模生成过程的不确定性。通过将文本映射到连续的潜在空间，并利用采样机制引入随机性，增强输出动作的多样性。

Result: 实验在HumanML3D和KIT-ML等基准数据集上进行，结果显示该方法大幅提升生成动作的多样性，同时保持与文本一致的性能达到了最新水平。

Conclusion: 该方法有效缓解了文本驱动的3D动作生成多样性不足的问题，为相关应用提供了更具表现力和多样性的动作生成工具。

Abstract: Generating 3D human motions from text is a challenging yet valuable task. The
key aspects of this task are ensuring text-motion consistency and achieving
generation diversity. Although recent advancements have enabled the generation
of precise and high-quality human motions from text, achieving diversity in the
generated motions remains a significant challenge. In this paper, we aim to
overcome the above challenge by designing a simple yet effective text-to-motion
generation method, \textit{i.e.}, Diverse-T2M. Our method introduces
uncertainty into the generation process, enabling the generation of highly
diverse motions while preserving the semantic consistency of the text.
Specifically, we propose a novel perspective that utilizes noise signals as
carriers of diversity information in transformer-based methods, facilitating a
explicit modeling of uncertainty. Moreover, we construct a latent space where
text is projected into a continuous representation, instead of a rigid
one-to-one mapping, and integrate a latent space sampler to introduce
stochastic sampling into the generation process, thereby enhancing the
diversity and uncertainty of the outputs. Our results on text-to-motion
generation benchmark datasets~(HumanML3D and KIT-ML) demonstrate that our
method significantly enhances diversity while maintaining state-of-the-art
performance in text consistency.

</details>


### [50] [Optimization-Based Calibration for Intravascular Ultrasound Volume Reconstruction](https://arxiv.org/abs/2508.20605)
*Karl-Philippe Beaudet,Sidaty El Hadramy,Philippe C Cattin,Juan Verde,Stéphane Cotin*

Main category: cs.CV

TL;DR: 本文提出了一种基于优化的3D血管内超声(IVUS)标定方法，通过3D打印模型对IVUS体积重建进行精确校准，实现术中IVUS与术前CT的准确配准，显著提升肝脏手术中的术中导航能力。


<details>
  <summary>Details</summary>
Motivation: 在肝脏手术中，术中超声图像受视野限制和结构复杂性影响，难以准确解释。如何有效地将术前CT信息与术中超声图像结合，是提高手术导航精度的关键。

Method: 作者提出利用3D打印的物理仿体，对3D血管内超声系统进行基于优化的精确标定。通过将IVUS数据与CT图像配准，实现术中图像与术前数据的精准对齐。实验选用猪肝活体图像进行验证。

Result: 该方法实现的标定误差为0.88到1.80毫米，IVUS与CT配准误差为3.40到5.71毫米，显示出良好的精度和可靠性。

Conclusion: 提出的方法可提供准确可靠的IVUS体积重建与配准技术，为肝脏手术中术中超声与术前CT的结合及导航提供了有效工具，有助于提升手术安全性和效果。

Abstract: Intraoperative ultrasound images are inherently challenging to interpret in
liver surgery due to the limited field of view and complex anatomical
structures. Bridging the gap between preoperative and intraoperative data is
crucial for effective surgical guidance. 3D IntraVascular UltraSound (IVUS)
offers a potential solution by enabling the reconstruction of the entire organ,
which facilitates registration between preoperative computed tomography (CT)
scans and intraoperative IVUS images. In this work, we propose an
optimization-based calibration method using a 3D-printed phantom for accurate
3D Intravascular Ultrasound volume reconstruction. Our approach ensures precise
alignment of tracked IVUS data with preoperative CT images, improving
intraoperative navigation. We validated our method using in vivo swine liver
images, achieving a calibration error from 0.88 to 1.80 mm and a registration
error from 3.40 to 5.71 mm between the 3D IVUS data and the corresponding CT
scan. Our method provides a reliable and accurate means of calibration and
volume reconstruction. It can be used to register intraoperative ultrasound
images with preoperative CT images in the context of liver surgery, and enhance
intraoperative guidance.

</details>


### [51] [Physics Informed Generative Models for Magnetic Field Images](https://arxiv.org/abs/2508.20612)
*Aye Phyu Phyu Aung,Lucas Lum,Zhansen Shi,Wen Qiu,Bernice Zee,JM Chin,Yeow Kheng Lim,J. Senthilnath*

Main category: cs.CV

TL;DR: 本文提出了一种物理约束下的生成式扩散模型（PI-GenMFI），用于生成半导体制造领域的磁场成像（MFI）数据，以提升缺陷检测和定位的效率。


<details>
  <summary>Details</summary>
Motivation: 由于磁场成像（MFI）数据因专有性而稀缺，导致基于MFI的机器学习模型训练受限，亟需生成高质量的合成MFI数据以提高缺陷检测效率。

Method: 提出结合物理约束的生成式扩散模型（PI-GenMFI），借助扩散模型及领域物理知识，合成常见缺陷类型（如电源短路）的MFI图像，并与VAE及其他扩散模型等SOTA模型进行对比评估，通过专家评审和多种定量指标对生成效果进行验证。

Result: PI-GenMFI能生成高质量的合成MFI图像，定量和定性评测均优于SOTA生成模型，专家评审也认可生成数据的有效性。

Conclusion: PI-GenMFI为解决MFI数据稀缺提供了有效方法，用合成数据优化了半导体缺陷定位机器学习流程，显示其在实际生产中的应用潜力。

Abstract: In semiconductor manufacturing, defect detection and localization are
critical to ensuring product quality and yield. While X-ray imaging is a
reliable non-destructive testing method, it is memory-intensive and
time-consuming for large-scale scanning, Magnetic Field Imaging (MFI) offers a
more efficient means to localize regions of interest (ROI) for targeted X-ray
scanning. However, the limited availability of MFI datasets due to proprietary
concerns presents a significant bottleneck for training machine learning (ML)
models using MFI. To address this challenge, we consider an ML-driven approach
leveraging diffusion models with two physical constraints. We propose Physics
Informed Generative Models for Magnetic Field Images (PI-GenMFI) to generate
synthetic MFI samples by integrating specific physical information. We generate
MFI images for the most common defect types: power shorts. These synthetic
images will serve as training data for ML algorithms designed to localize
defect areas efficiently. To evaluate generated MFIs, we compare our model to
SOTA generative models from both variational autoencoder (VAE) and diffusion
methods. We present a domain expert evaluation to assess the generated samples.
In addition, we present qualitative and quantitative evaluation using various
metrics used for image generation and signal processing, showing promising
results to optimize the defect localization process.

</details>


### [52] [Improving Alignment in LVLMs with Debiased Self-Judgment](https://arxiv.org/abs/2508.20655)
*Sihan Yang,Chenhang Cui,Zihao Zhao,Yiyang Zhou,Weilong Yan,Ying Wei,Huaxiu Yao*

Main category: cs.CV

TL;DR: 本文提出了一种通过模型内部自评而非依赖外部数据来提升大模型视觉-语言对齐及安全性的创新方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型（如LLMs和LVLMs）在视觉与语言对齐时，普遍会出现“幻觉”问题，即输出内容与视觉输入脱离，带来安全隐患。而现有对齐方法（如指令微调、偏好微调）高度依赖外部资源和人工标注，难以扩展且成本高。

Method: 提出了一种新的自评分数机制，使模型能够根据自己内部的评判标准进行去偏自评，而不需要外部数据或人工干预。该机制被集成进解码和偏好微调过程中，用于指导模型自我提升。

Result: 实验证明，该方法有效降低了幻觉现象，提高了模型安全性和综合能力，在LVLM对齐上明显优于传统方法。

Conclusion: 通过引入内部自评机制，无需外部依赖即可推动视觉-语言模型的自我优化，对提升多模态大模型的可靠性和实际应用价值具有积极意义。

Abstract: The rapid advancements in Large Language Models (LLMs) and Large
Visual-Language Models (LVLMs) have opened up new opportunities for integrating
visual and linguistic modalities. However, effectively aligning these
modalities remains challenging, often leading to hallucinations--where
generated outputs are not grounded in the visual input--and raising safety
concerns across various domains. Existing alignment methods, such as
instruction tuning and preference tuning, often rely on external datasets,
human annotations, or complex post-processing, which limit scalability and
increase costs. To address these challenges, we propose a novel approach that
generates the debiased self-judgment score, a self-evaluation metric created
internally by the model without relying on external resources. This enables the
model to autonomously improve alignment. Our method enhances both decoding
strategies and preference tuning processes, resulting in reduced
hallucinations, enhanced safety, and improved overall capability. Empirical
results show that our approach significantly outperforms traditional methods,
offering a more effective solution for aligning LVLMs.

</details>


### [53] [Revisiting the Privacy Risks of Split Inference: A GAN-Based Data Reconstruction Attack via Progressive Feature Optimization](https://arxiv.org/abs/2508.20613)
*Yixiang Qiu,Yanhan Liu,Hongyao Yu,Hao Fang,Bin Chen,Shu-Tao Xia,Ke Xu*

Main category: cs.CV

TL;DR: 本文提出了一种基于GAN的新型数据重建攻击框架，并通过逐步特征优化，能更有效地从分割推理中的中间特征重建高保真原始数据，对更深层、更复杂的神经网络和更高分辨率场景同样有效，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络变得越来越复杂，分割推理（Split Inference，SI）成为在端设备与云之间分配计算负载、降低延迟与保护隐私的主流方法。然而，最近的数据重建攻击表明，SI传输的中间特征可被用来还原敏感数据，现有攻击方法多数仅在浅层模型下有效，重建质量和泛化能力有限。本文旨在解决这些安全隐患。

Method: 提出了一种基于生成对抗网络（GAN）的数据重建攻击框架，引入逐步特征优化（Progressive Feature Optimization，PFO），将生成器分为多个分层模块，逐步细化中间特征以增强所重建图像的语义真实性。同时采用L1球约束以稳定优化过程并提升图像现实感。

Result: 通过大量实验验证，提出的方法在高分辨率、数据分布外与更复杂、更深层的神经网络场景下，重建效果显著优于以往攻击手段。

Conclusion: 本文的方法极大提升了对分割推理系统的重建攻击能力，凸显了其在更复杂与现实场景中的隐私风险，揭示了SI在实际隐私保护方面的不足。

Abstract: The growing complexity of Deep Neural Networks (DNNs) has led to the adoption
of Split Inference (SI), a collaborative paradigm that partitions computation
between edge devices and the cloud to reduce latency and protect user privacy.
However, recent advances in Data Reconstruction Attacks (DRAs) reveal that
intermediate features exchanged in SI can be exploited to recover sensitive
input data, posing significant privacy risks. Existing DRAs are typically
effective only on shallow models and fail to fully leverage semantic priors,
limiting their reconstruction quality and generalizability across datasets and
model architectures. In this paper, we propose a novel GAN-based DRA framework
with Progressive Feature Optimization (PFO), which decomposes the generator
into hierarchical blocks and incrementally refines intermediate representations
to enhance the semantic fidelity of reconstructed images. To stabilize the
optimization and improve image realism, we introduce an L1-ball constraint
during reconstruction. Extensive experiments show that our method outperforms
prior attacks by a large margin, especially in high-resolution scenarios,
out-of-distribution settings, and against deeper and more complex DNNs.

</details>


### [54] [MobileCLIP2: Improving Multi-Modal Reinforced Training](https://arxiv.org/abs/2508.20691)
*Fartash Faghri,Pavan Kumar Anasosalu Vasu,Cem Koc,Vaishaal Shankar,Alexander Toshev,Oncel Tuzel,Hadi Pouransari*

Main category: cs.CV

TL;DR: 本文提出了MobileCLIP2，有效提升了移动端图文基础模型的零样本识别准确率，且具有低延迟和小参数量。


<details>
  <summary>Details</summary>
Motivation: 现有的图文基础模型如CLIP，虽然零样本能力强，但在移动端的低延迟和高效率部署面临挑战。MobileCLIP已在效率和准确率之间取得平衡，但仍有进一步提升空间。

Method: 提出了改进的多模态强化训练流程：1）采用在DFN数据集上训练的更优CLIP集成教师；2）改进的生成式caption教师，在DFN和其它多样化数据集上微调。进一步通过消融实验探索对比蒸馏的参数影响、多模型合成caption等对性能的贡献。

Result: MobileCLIP2系列模型在ImageNet-1k上实现了最新的零样本准确率，MobileCLIP2-B比前作提升2.2%。MobileCLIP2-S4用一半参数达到了SigLIP-SO400M/14的零样本准确率，并以2.5倍更低延迟超越DFN ViT-L/14。

Conclusion: 改进的多模态强化训练方法有力提升了移动端图文模型在准确率和推理速度上的表现，为高效部署和后续研究提供了基础。预训练模型和数据生成代码已开源，推动了社区的发展。

Abstract: Foundation image-text models such as CLIP with zero-shot capabilities enable
a wide array of applications. MobileCLIP is a recent family of image-text
models at 3-15ms latency and 50-150M parameters with state-of-the-art zero-shot
accuracy. The main ingredients in MobileCLIP were its low-latency and light
architectures and a novel multi-modal reinforced training that made knowledge
distillation from multiple caption-generators and CLIP teachers efficient,
scalable, and reproducible. In this paper, we improve the multi-modal
reinforced training of MobileCLIP through: 1) better CLIP teacher ensembles
trained on the DFN dataset, 2) improved captioner teachers trained on the DFN
dataset and fine-tuned on a diverse selection of high-quality image-caption
datasets. We discover new insights through ablations such as the importance of
temperature tuning in contrastive knowledge distillation, the effectiveness of
caption-generator fine-tuning for caption diversity, and the additive
improvement from combining synthetic captions generated by multiple models. We
train a new family of models called MobileCLIP2 and achieve state-of-the-art
ImageNet-1k zero-shot accuracies at low latencies. In particular, we observe
2.2% improvement in ImageNet-1k accuracy for MobileCLIP2-B compared with
MobileCLIP-B architecture. Notably, MobileCLIP2-S4 matches the zero-shot
accuracy of SigLIP-SO400M/14 on ImageNet-1k while being 2$\times$ smaller and
improves on DFN ViT-L/14 at 2.5$\times$ lower latency. We release our
pretrained models (https://github.com/apple/ml-mobileclip) and the data
generation code (https://github.com/apple/ml-mobileclip-dr). The data
generation code makes it easy to create new reinforced datasets with arbitrary
teachers using distributed scalable processing.

</details>


### [55] [EmoCAST: Emotional Talking Portrait via Emotive Text Description](https://arxiv.org/abs/2508.20615)
*Yiguo Jiang,Xiaodong Cun,Yong Zhang,Yudian Zheng,Fan Tang,Chi-Man Pun*

Main category: cs.CV

TL;DR: EmoCAST提出了一种新的生成情感说话头像视频的方法，既提升了情感控制与自然度，也优化了表情与音频的关联，取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 现有情感头像生成方法在灵活控制、表情自然度、情感质量及数据集多样性等方面存在明显不足，限制了实际应用。

Method: 提出了基于扩散模型的EmoCAST，包括：1) 文本引导的情感模块，增强情感理解能力；2) 情感音频注意力模块，加强音频与情感的结合；3) 构建了带详细文本标签的新情感人脸数据集，并提出情感相关的采样和渐进训练策略，进一步提高表情细腻度与唇形同步性。

Result: EmoCAST在生成真实、富有情感表现力和音频同步的说话人头像视频任务上取得了当前最优表现。

Conclusion: EmoCAST显著提升了情感说话头像生成的可控性、自然性和表现力，为实际应用奠定了基础，并推动了该领域研究进展。

Abstract: Emotional talking head synthesis aims to generate talking portrait videos
with vivid expressions. Existing methods still exhibit limitations in control
flexibility, motion naturalness, and expression quality. Moreover, currently
available datasets are primarily collected in lab settings, further
exacerbating these shortcomings. Consequently, these limitations substantially
hinder practical applications in real-world scenarios. To address these
challenges, we propose EmoCAST, a diffusion-based framework with two key
modules for precise text-driven emotional synthesis. In appearance modeling,
emotional prompts are integrated through a text-guided decoupled emotive
module, enhancing the spatial knowledge to improve emotion comprehension. To
improve the relationship between audio and emotion, we introduce an emotive
audio attention module to capture the interplay between controlled emotion and
driving audio, generating emotion-aware features to guide more precise facial
motion synthesis. Additionally, we construct an emotional talking head dataset
with comprehensive emotive text descriptions to optimize the framework's
performance. Based on the proposed dataset, we propose an emotion-aware
sampling training strategy and a progressive functional training strategy that
further improve the model's ability to capture nuanced expressive features and
achieve accurate lip-synchronization. Overall, EmoCAST achieves
state-of-the-art performance in generating realistic, emotionally expressive,
and audio-synchronized talking-head videos. Project Page:
https://github.com/GVCLab/EmoCAST

</details>


### [56] [ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering](https://arxiv.org/abs/2508.21010)
*Paritosh Parmar,Eric Peh,Basura Fernando*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的模块化框架，将因果推理与答案生成显式解耦，并引入自然语言因果链作为可解释的中间表示，用于提升视频问答系统的推理能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有因果-为什么类视频问答模型在高阶推理上表现不佳，采用黑箱一体化方案，难以解释推理过程且依赖浅层启发式方法，因此亟需提升模型推理透明性和实用性。

Method: 方法上，框架分为两个阶段：1）因果链提取器（CCE）从视频-问题对生成因果链；2）基于因果链的答案生成器（CCDA）利用这些链生成回答。此外，结合大语言模型自动从数据集生成高质量因果链，并提出新的以因果为导向的评测指标CauCo，进行效果评估。

Result: 实验表明，该方法在三个大规模基准上优于现有最先进模型，且在可解释性、用户信任和泛化能力上均有显著提升。CCE模块还能作为通用因果推理引擎，广泛适用于不同领域。

Conclusion: 新框架不仅提升了推理效果，也增强了解释性和跨领域通用性，为因果推理视频问答提供了新的范式与评测工具，并推动了该领域的发展。

Abstract: Existing Causal-Why Video Question Answering (VideoQA) models often struggle
with higher-order reasoning, relying on opaque, monolithic pipelines that
entangle video understanding, causal inference, and answer generation. These
black-box approaches offer limited interpretability and tend to depend on
shallow heuristics. We propose a novel, modular framework that explicitly
decouples causal reasoning from answer generation, introducing natural language
causal chains as interpretable intermediate representations. Inspired by human
cognitive models, these structured cause-effect sequences bridge low-level
video content with high-level causal reasoning, enabling transparent and
logically coherent inference. Our two-stage architecture comprises a Causal
Chain Extractor (CCE) that generates causal chains from video-question pairs,
and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in
these chains. To address the lack of annotated reasoning traces, we introduce a
scalable method for generating high-quality causal chains from existing
datasets using large language models. We also propose CauCo, a new evaluation
metric for causality-oriented captioning. Experiments on three large-scale
benchmarks demonstrate that our approach not only outperforms state-of-the-art
models, but also yields substantial gains in explainability, user trust, and
generalization -- positioning the CCE as a reusable causal reasoning engine
across diverse domains. Project page:
https://paritoshparmar.github.io/chainreaction/

</details>


### [57] [Mask-Guided Multi-Channel SwinUNETR Framework for Robust MRI Classification](https://arxiv.org/abs/2508.20621)
*Smriti Joshi,Lidia Garrucho,Richard Osuala,Oliver Diaz,Karim Lekadir*

Main category: cs.CV

TL;DR: 本文提出了一种基于SwinUNETR的深度学习框架，用于乳腺MRI中良性及恶性病变的自动检测与分类，在国际挑战赛中取得第二名。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌死亡率高，早期检测对改善预后至关重要，且当前乳腺X线摄影对高风险或高密度乳腺女性效果有限，亟需开发更加敏感可靠的辅助诊断工具。

Method: 采用SwinUNETR深度学习网络，结合乳腺区域掩膜、丰富的数据增强和模型集成技术，训练并评估MRI扫描数据集（来自6家欧洲机构、多品牌1.5T/3T设备），数据标注包含良性、恶性及无病变三类。

Result: 该方法在ODELIA多中心乳腺MRI AI挑战赛中获得第二名，表现出了较强的泛化能力和鲁棒性。

Conclusion: 提出的AI模型有望辅助临床乳腺MRI影像判读，提升乳腺癌早期检测的准确性和效率，相关代码已公开。

Abstract: Breast cancer is one of the leading causes of cancer-related mortality in
women, and early detection is essential for improving outcomes. Magnetic
resonance imaging (MRI) is a highly sensitive tool for breast cancer detection,
particularly in women at high risk or with dense breast tissue, where
mammography is less effective. The ODELIA consortium organized a multi-center
challenge to foster AI-based solutions for breast cancer diagnosis and
classification. The dataset included 511 studies from six European centers,
acquired on scanners from multiple vendors at both 1.5 T and 3 T. Each study
was labeled for the left and right breast as no lesion, benign lesion, or
malignant lesion. We developed a SwinUNETR-based deep learning framework that
incorporates breast region masking, extensive data augmentation, and ensemble
learning to improve robustness and generalizability. Our method achieved second
place on the challenge leaderboard, highlighting its potential to support
clinical breast MRI interpretation. We publicly share our codebase at
https://github.com/smriti-joshi/bcnaim-odelia-challenge.git.

</details>


### [58] [AvatarBack: Back-Head Generation for Complete 3D Avatars from Front-View Images](https://arxiv.org/abs/2508.20623)
*Shiqi Xin,Xiaolin Zhang,Yanbin Liu,Peng Zhang,Caifeng Shan*

Main category: cs.CV

TL;DR: 本文提出了AvatarBack框架，通过生成拟合缺失的后脑区域，实现了完整一致且高质量的3D头像重建，解决了现有3D高斯头像方法后脑区域建模不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于Gaussian Splatting的3D头像重建依赖于正面图像，导致头像后脑区域重建质量低下，产生几何不一致、结构模糊、真实性下降等问题，限制了虚拟头像的质量和应用。

Method: 提出了AvatarBack框架，包括两个核心创新：1）面向个体的生成器（SSG），利用生成先验自正面输入合成一致且合理的后脑伪视图，提供多视角监督；2）自适应空间对齐策略（ASA），通过可学习的变换矩阵在训练中优化，用于精确对齐合成视图与3D高斯表示，解决姿态及坐标不一致问题。该框架为主流高斯头像方法的无缝插件。

Result: 在NeRSemble和K-hairstyle数据集上，通过几何、光度和GPT-4o感知指标进行评估，AvatarBack显著提升了后脑区域建模的质量，同时保持了正面区域的真实性和结构一致性。

Conclusion: AvatarBack有效完善了3D高斯头像的后脑建模，可生成完整、视觉真实且可动画化的3D头像，为虚拟人、动画等场景提供了更高质量的基础。

Abstract: Recent advances in Gaussian Splatting have significantly boosted the
reconstruction of head avatars, enabling high-quality facial modeling by
representing an 3D avatar as a collection of 3D Gaussians. However, existing
methods predominantly rely on frontal-view images, leaving the back-head poorly
constructed. This leads to geometric inconsistencies, structural blurring, and
reduced realism in the rear regions, ultimately limiting the fidelity of
reconstructed avatars. To address this challenge, we propose AvatarBack, a
novel plug-and-play framework specifically designed to reconstruct complete and
consistent 3D Gaussian avatars by explicitly modeling the missing back-head
regions. AvatarBack integrates two core technical innovations,i.e., the
Subject-specific Generator (SSG) and the Adaptive Spatial Alignment Strategy
(ASA). The former leverages a generative prior to synthesize
identity-consistent, plausible back-view pseudo-images from sparse frontal
inputs, providing robust multi-view supervision. To achieve precise geometric
alignment between these synthetic views and the 3D Gaussian representation, the
later employs learnable transformation matrices optimized during training,
effectively resolving inherent pose and coordinate discrepancies. Extensive
experiments on NeRSemble and K-hairstyle datasets, evaluated using geometric,
photometric, and GPT-4o-based perceptual metrics, demonstrate that AvatarBack
significantly enhances back-head reconstruction quality while preserving
frontal fidelity. Moreover, the reconstructed avatars maintain consistent
visual realism under diverse motions and remain fully animatable.

</details>


### [59] [ArtFace: Towards Historical Portrait Face Identification via Model Adaptation](https://arxiv.org/abs/2508.20626)
*Francois Poh,Anjith George,Sébastien Marcel*

Main category: cs.CV

TL;DR: 本文提出利用基础模型提升历史绘画中的人像识别准确性，通过微调基础模型，并将其嵌入表示与传统人脸识别网络结合，实现对传统方法的显著改进。


<details>
  <summary>Details</summary>
Motivation: 历史绘画人像身份识别对艺术史研究具有重要意义，但由于资料有限、画风多样和风格迁移，识别过程主观性强且困难重重，传统人脸识别模型在绘画领域效果欠佳，亟需新方法提升识别能力。

Method: 作者对基础模型进行微调，并将其特征嵌入与传统人脸识别网络的特征嵌入进行融合，旨在发挥基础模型的泛化和表示能力，弥合照片与绘画间的领域差距。

Result: 实验证明，该方法在历史绘画人像识别任务中，相比当前最新方法取得了显著性能提升。

Conclusion: 基础模型通过适当微调和特征融合，可大大提升艺术作品中人像识别性能，为无法用传统方法处理的场景提供新解决思路。

Abstract: Identifying sitters in historical paintings is a key task for art historians,
offering insight into their lives and how they chose to be seen. However, the
process is often subjective and limited by the lack of data and stylistic
variations. Automated facial recognition is capable of handling challenging
conditions and can assist, but while traditional facial recognition models
perform well on photographs, they struggle with paintings due to domain shift
and high intra-class variation. Artistic factors such as style, skill, intent,
and influence from other works further complicate recognition. In this work, we
investigate the potential of foundation models to improve facial recognition in
artworks. By fine-tuning foundation models and integrating their embeddings
with those from conventional facial recognition networks, we demonstrate
notable improvements over current state-of-the-art methods. Our results show
that foundation models can bridge the gap where traditional methods are
ineffective. Paper page at https://www.idiap.ch/paper/artface/

</details>


### [60] [CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models](https://arxiv.org/abs/2508.20640)
*Ayan Banerjee,Fernando Vilariño,Josep Lladós*

Main category: cs.CV

TL;DR: 本文提出了一种名为CraftGraffiti的文本引导式涂鸦生成框架，实现了在极端风格化条件下的人脸特征保留。该方法在风格转换后，采用身份保持机制，有效减少了人脸属性漂移，提升了识别度和美学评价。


<details>
  <summary>Details</summary>
Motivation: 在高度风格化（如涂鸦）的人像艺术中，容易因细节变形导致人物难以辨认，这影响了创作的个性表达及文化意义。缺乏对身份特征的有效保护手段，制约了创意AI的应用。

Method: CraftGraffiti框架依次执行风格迁移和身份保持：1）通过LoRA微调扩散转换器先实现涂鸦风格转化；2）采用加入身份嵌入的自注意力机制，强化面部特征一致性；3）利用CLIP指导的提示词扩展，实现不依赖关键点的姿态调整，确保面部连贯。提出并验证了“先风格后身份”顺序优于反序的归因。

Result: 该方法在面部特征一致性、风格美学和用户偏好测试中取得了行业领先结果，显著降低了面部属性漂移。真实场景（Cruilla Festival现场）应用证明其创意和社会影响力。

Conclusion: CraftGraffiti为创意AI在人脸涂鸦生成中提供了兼顾身份识别与风格表达的系统方案，推动了身份尊重型AI艺术的发展，在实际应用中展现了广泛价值。

Abstract: Preserving facial identity under extreme stylistic transformation remains a
major challenge in generative art. In graffiti, a high-contrast, abstract
medium, subtle distortions to the eyes, nose, or mouth can erase the subject's
recognizability, undermining both personal and cultural authenticity. We
present CraftGraffiti, an end-to-end text-guided graffiti generation framework
designed with facial feature preservation as a primary objective. Given an
input image and a style and pose descriptive prompt, CraftGraffiti first
applies graffiti style transfer via LoRA-fine-tuned pretrained diffusion
transformer, then enforces identity fidelity through a face-consistent
self-attention mechanism that augments attention layers with explicit identity
embeddings. Pose customization is achieved without keypoints, using CLIP-guided
prompt extension to enable dynamic re-posing while retaining facial coherence.
We formally justify and empirically validate the "style-first, identity-after"
paradigm, showing it reduces attribute drift compared to the reverse order.
Quantitative results demonstrate competitive facial feature consistency and
state-of-the-art aesthetic and human preference scores, while qualitative
analyses and a live deployment at the Cruilla Festival highlight the system's
real-world creative impact. CraftGraffiti advances the goal of
identity-respectful AI-assisted artistry, offering a principled approach for
blending stylistic freedom with recognizability in creative AI applications.

</details>


### [61] ["Humor, Art, or Misinformation?": A Multimodal Dataset for Intent-Aware Synthetic Image Detection](https://arxiv.org/abs/2508.20670)
*Anastasios Skoularikis,Stefanos-Iordanis Papadopoulos,Symeon Papadopoulos,Panagiotis C. Petrantonakis*

Main category: cs.CV

TL;DR: 本文提出S-HArM多模态数据集，专注于图像内容背后意图的识别，并系统评测多种模型方法，但结果显示意图推断仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态AI虽能检测合成或离境内容，但对理解AI生成图像背后意图关注不足。该研究旨在填补这一空白，推动意图识别相关研究。

Method: 构建包含9,576组真实社交媒体图片-文本对的数据集，标注为幽默/讽刺、艺术或虚假信息。同时，利用三种提示策略结合Stable Diffusion生成大规模合成训练数据，并对多种模型（融合、对比学习、重构网络、注意力机制及大规模视觉-语言模型）进行了系统测试与比较。

Result: 实验发现，使用图像和多模态引导生成的数据训练的模型对真实场景的泛化能力较强，因为视觉上下文得以保留。但整体性能依然有限，反映了推断意图任务的复杂性。

Conclusion: 尽管部分方法提升了泛化性，但当前模型对意图识别能力不足，表明需要更具针对性和创新性的方法来解决多模态意图识别的难题。

Abstract: Recent advances in multimodal AI have enabled progress in detecting synthetic
and out-of-context content. However, existing efforts largely overlook the
intent behind AI-generated images. To fill this gap, we introduce S-HArM, a
multimodal dataset for intent-aware classification, comprising 9,576 "in the
wild" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art,
or Misinformation. Additionally, we explore three prompting strategies
(image-guided, description-guided, and multimodally-guided) to construct a
large-scale synthetic training dataset with Stable Diffusion. We conduct an
extensive comparative study including modality fusion, contrastive learning,
reconstruction networks, attention mechanisms, and large vision-language
models. Our results show that models trained on image- and multimodally-guided
data generalize better to "in the wild" content, due to preserved visual
context. However, overall performance remains limited, highlighting the
complexity of inferring intent and the need for specialized architectures.

</details>


### [62] [Learned Rate Control for Frame-Level Adaptive Neural Video Compression via Dynamic Neural Network](https://arxiv.org/abs/2508.20709)
*Chenhao Zhang,Wei Gao*

Main category: cs.CV

TL;DR: 本文提出了一种可变比特率下的新型神经视频压缩方法，能更精确地控制码率，并兼顾压缩率、画质和计算复杂度，实验效果优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的神经视频压缩（NVC）虽有优异表现，但由于学习型编解码器的局限，难以实现精确的码率控制，这在实际应用中是个难题。

Method: 1. 提出动态路径自编码器（Dynamic-Route Autoencoder），其不同编码路径对应不同的码率和计算复杂度，实现变码率需求下的灵活压缩。2. 设置码率控制代理（Rate Control Agent），可实时估算并调整每条编码路径的码率，从而精确匹配目标码率。3. 采用多路径联合优化（Joint-Routes Optimization），联合训练多条路径，确保在大范围码率覆盖下维持良好性能。

Result: 在HEVC和UVG数据集上，本文方法相较主流模型平均BD-Rate降低14.8%、BD-PSNR提升0.47dB，且平均码率误差仅为1.66%。

Conclusion: 该框架在多种比特率及码率约束应用场景下，实现了码率-失真-复杂度（RDCO）上的多目标优化并超越现有方法，具有实际推广价值。

Abstract: Neural Video Compression (NVC) has achieved remarkable performance in recent
years. However, precise rate control remains a challenge due to the inherent
limitations of learning-based codecs. To solve this issue, we propose a dynamic
video compression framework designed for variable bitrate scenarios. First, to
achieve variable bitrate implementation, we propose the Dynamic-Route
Autoencoder with variable coding routes, each occupying partial computational
complexity of the whole network and navigating to a distinct RD trade-off.
Second, to approach the target bitrate, the Rate Control Agent estimates the
bitrate of each route and adjusts the coding route of DRA at run time. To
encompass a broad spectrum of variable bitrates while preserving overall RD
performance, we employ the Joint-Routes Optimization strategy, achieving
collaborative training of various routes. Extensive experiments on the HEVC and
UVG datasets show that the proposed method achieves an average BD-Rate
reduction of 14.8% and BD-PSNR gain of 0.47dB over state-of-the-art methods
while maintaining an average bitrate error of 1.66%, achieving
Rate-Distortion-Complexity Optimization (RDCO) for various bitrate and
bitrate-constrained applications. Our code is available at
https://git.openi.org.cn/OpenAICoding/DynamicDVC.

</details>


### [63] [CardioMorphNet: Cardiac Motion Prediction Using a Shape-Guided Bayesian Recurrent Deep Network](https://arxiv.org/abs/2508.20734)
*Reza Akbari Movahed,Abuzar Rezaee,Arezoo Zakeri,Colin Berry,Edmond S. L. Ho,Ali Gooya*

Main category: cs.CV

TL;DR: CardioMorphNet是一种结合了递归贝叶斯深度学习与3D心脏形状引导的医学图像配准方法，在心脏磁共振影像（CMR）中实现了更准确的心脏运动估计，并提供了更低不确定性的运动场预测。


<details>
  <summary>Details</summary>
Motivation: 现有心脏运动估计方法依赖基于图像强度的配准损失，容易忽略心脏解剖结构区域，导致对关键功能区域运动捕捉不准确。这限制了心脏功能评估和异常检测的精度。针对该问题，作者提出融合形状信息与概率建模的新方法。

Method: 提出CardioMorphNet，包括递归变分自编码器（VAE）建模心脏周期的时空依赖、两个后验模型分别用于双心室分割和运动估计，并基于贝叶斯原理设计新损失函数，使网络专注于解剖区域，无需传统基于强度的损失。该框架还可输出运动场的不确定性评估。

Result: 在UK Biobank数据集上，与当前主流方法比较，CardioMorphNet在心脏运动估计准确性上表现更优，变形后分割掩码与真实掩码更一致。不确定性量化显示其在心脏区域的运动场预测具有更低的不确定性。

Conclusion: CardioMorphNet能够更准确地估计心脏运动，并有效提升预测置信度，为心脏疾病评估和监测提供了有力工具，优于现有概率型配准技术。

Abstract: Accurate cardiac motion estimation from cine cardiac magnetic resonance (CMR)
images is vital for assessing cardiac function and detecting its abnormalities.
Existing methods often struggle to capture heart motion accurately because they
rely on intensity-based image registration similarity losses that may overlook
cardiac anatomical regions. To address this, we propose CardioMorphNet, a
recurrent Bayesian deep learning framework for 3D cardiac shape-guided
deformable registration using short-axis (SAX) CMR images. It employs a
recurrent variational autoencoder to model spatio-temporal dependencies over
the cardiac cycle and two posterior models for bi-ventricular segmentation and
motion estimation. The derived loss function from the Bayesian formulation
guides the framework to focus on anatomical regions by recursively registering
segmentation maps without using intensity-based image registration similarity
loss, while leveraging sequential SAX volumes and spatio-temporal features. The
Bayesian modelling also enables computation of uncertainty maps for the
estimated motion fields. Validated on the UK Biobank dataset by comparing
warped mask shapes with ground truth masks, CardioMorphNet demonstrates
superior performance in cardiac motion estimation, outperforming
state-of-the-art methods. Uncertainty assessment shows that it also yields
lower uncertainty values for estimated motion fields in the cardiac region
compared with other probabilistic-based cardiac registration methods,
indicating higher confidence in its predictions.

</details>


### [64] [Mix, Align, Distil: Reliable Cross-Domain Atypical Mitosis Classification](https://arxiv.org/abs/2508.20745)
*Kaustubh Atey,Sameer Anand Jha,Gouranga Bala,Amit Sethi*

Main category: cs.CV

TL;DR: 本文提出了一种在不同域（如扫描仪、染色、获取方式）下具有鲁棒性的非典型有丝分裂图像（AMF）分类方法，在MIDOG 2025大赛上展示了优异性能。


<details>
  <summary>Details</summary>
Motivation: 非典型有丝分裂被认为是重要的组织病理学标志，但由于领域转移（如设备、染色体、物种等差异），自动识别存在一致性难题，需要开发鲁棒的方法来提升自动识别的可靠性。

Method: 方法包括：1）在骨干网络早期和中期引入风格扰动以增加特征多样性；2）利用弱域标签（如扫描仪类型、来源、物种、肿瘤等）通过辅助对齐损失函数对跨域注意力特征进行对齐；3）采用带有温度缩放的KL散度蒸馏损失从EMA教师模型稳定训练。推理时几乎无额外耗时，仅依赖粗粒度的域标签。

Result: 在MIDOG 2025非典型有丝分裂分类初步排行榜上取得了平衡准确率0.8762，敏感性0.8873，特异性0.8651，ROC AUC为0.9499，表现出强大且均衡的性能。

Conclusion: 该方法无需复杂的额外域信息，实现了强鲁棒性和均衡性，推理高效，是MIDOG 2025比赛中具有竞争力的方案。

Abstract: Atypical mitotic figures (AMFs) are important histopathological markers yet
remain challenging to identify consistently, particularly under domain shift
stemming from scanner, stain, and acquisition differences. We present a simple
training-time recipe for domain-robust AMF classification in MIDOG 2025 Task 2.
The approach (i) increases feature diversity via style perturbations inserted
at early and mid backbone stages, (ii) aligns attention-refined features across
sites using weak domain labels (Scanner, Origin, Species, Tumor) through an
auxiliary alignment loss, and (iii) stabilizes predictions by distilling from
an exponential moving average (EMA) teacher with temperature-scaled KL
divergence. On the organizer-run preliminary leaderboard for atypical mitosis
classification, our submission attains balanced accuracy of 0.8762, sensitivity
of 0.8873, specificity of 0.8651, and ROC AUC of 0.9499. The method incurs
negligible inference-time overhead, relies only on coarse domain metadata, and
delivers strong, balanced performance, positioning it as a competitive
submission for the MIDOG 2025 challenge.

</details>


### [65] [Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning](https://arxiv.org/abs/2508.20751)
*Yibin Wang,Zhimin Li,Yuhang Zang,Yujie Zhou,Jiazi Bu,Chunyu Wang,Qinglin Lu,Cheng Jin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于首选对比奖励的GRPO方法（Pref-GRPO），以提升文本到图像（T2I）生成的稳定性，同时还引入了更细致多元的评价基准UniGenBench。


<details>
  <summary>Details</summary>
Motivation: 当前T2I生成的强化学习方法常用点式奖励模型，但容易出现奖励欺骗（reward hacking）的问题，导致生成模型因过度优化微小分数差异而不稳定。同时，现有基准测试评价标准粗糙，无法全面评估模型优劣。

Method: 1）Pref-GRPO方法采用成对偏好奖励模型，将优化目标从分数最大化转为拟合偏好，通过比较图片对之间的相对胜率作为奖励信号，提升训练稳定性并抑制奖励欺骗。2）UniGenBench基准涵盖600个多样化提示，采用主从标准及多维指标，结合多模态大模型（MLLM）进行评价。

Result: 实验表明，Pref-GRPO能更精细地区分图像质量差异并保持优势，显著减轻奖励欺骗问题。UniGenBench揭示了开源与闭源T2I模型的优劣，并验证了Pref-GRPO方法的有效性。

Conclusion: Pref-GRPO提升了T2I生成的训练稳定性，UniGenBench为行业提供了更全面的评价体系，为未来T2I模型的发展打下基础。

Abstract: Recent advancements highlight the importance of GRPO-based reinforcement
learning methods and benchmarking in enhancing text-to-image (T2I) generation.
However, current methods using pointwise reward models (RM) for scoring
generated images are susceptible to reward hacking. We reveal that this happens
when minimal score differences between images are amplified after
normalization, creating illusory advantages that drive the model to
over-optimize for trivial gains, ultimately destabilizing the image generation
process. To address this, we propose Pref-GRPO, a pairwise preference
reward-based GRPO method that shifts the optimization objective from score
maximization to preference fitting, ensuring more stable training. In
Pref-GRPO, images are pairwise compared within each group using preference RM,
and the win rate is used as the reward signal. Extensive experiments
demonstrate that PREF-GRPO differentiates subtle image quality differences,
providing more stable advantages and mitigating reward hacking. Additionally,
existing T2I benchmarks are limited by coarse evaluation criteria, hindering
comprehensive model assessment. To solve this, we introduce UniGenBench, a
unified T2I benchmark comprising 600 prompts across 5 main themes and 20
subthemes. It evaluates semantic consistency through 10 primary and 27
sub-criteria, leveraging MLLM for benchmark construction and evaluation. Our
benchmarks uncover the strengths and weaknesses of both open and closed-source
T2I models and validate the effectiveness of Pref-GRPO.

</details>


### [66] [${C}^{3}$-GS: Learning Context-aware, Cross-dimension, Cross-scale Feature for Generalizable Gaussian Splatting](https://arxiv.org/abs/2508.20754)
*Yuxi Hu,Jun Zhang,Kuangyi Chen,Zhe Zhang,Friedrich Fraundorfer*

Main category: cs.CV

TL;DR: 本文提出了C3-GS框架，通过引入上下文感知、跨维度和跨尺度约束，提升了Gaussian Splatting在稀疏多视图条件下对新场景的合成能力，实现了无需单独场景优化的高质量三维重建。


<details>
  <summary>Details</summary>
Motivation: 现有的泛化型Gaussian Splatting方法在处理稀疏视图时，难以对高辨别力且多视图一致的特征进行编码，导致几何构建不准确，影响新视角的渲染质量。该问题限制了其在实际应用中的表现。

Method: 作者提出C3-GS架构，通过集成三个轻量级模块至统一的渲染流程中，分别引入上下文感知（context-aware）、跨维度（cross-dimension）和跨尺度（cross-scale）的特征约束，从而增强特征融合效果，无需额外监督即可提升合成质量。

Result: 在多个基准数据集上进行了大量实验，结果显示C3-GS在渲染质量和泛化能力方面均达到或超过了当前最好水平。

Conclusion: C3-GS框架能够在稀疏输入视图下高效、逼真地合成新场景视角，为泛化型3D重建提供了新思路，具有实际应用价值。

Abstract: Generalizable Gaussian Splatting aims to synthesize novel views for unseen
scenes without per-scene optimization. In particular, recent advancements
utilize feed-forward networks to predict per-pixel Gaussian parameters,
enabling high-quality synthesis from sparse input views. However, existing
approaches fall short in encoding discriminative, multi-view consistent
features for Gaussian predictions, which struggle to construct accurate
geometry with sparse views. To address this, we propose $\mathbf{C}^{3}$-GS, a
framework that enhances feature learning by incorporating context-aware,
cross-dimension, and cross-scale constraints. Our architecture integrates three
lightweight modules into a unified rendering pipeline, improving feature fusion
and enabling photorealistic synthesis without requiring additional supervision.
Extensive experiments on benchmark datasets validate that $\mathbf{C}^{3}$-GS
achieves state-of-the-art rendering quality and generalization ability. Code is
available at: https://github.com/YuhsiHu/C3-GS.

</details>


### [67] [SeqVLM: Proposal-Guided Multi-View Sequences Reasoning via VLM for Zero-Shot 3D Visual Grounding](https://arxiv.org/abs/2508.20758)
*Jiawen Lin,Shiran Bian,Yihang Zhu,Wenbin Tan,Yachao Zhang,Yuan Xie,Yanyun Qu*

Main category: cs.CV

TL;DR: 该论文提出SeqVLM框架，在无需针对特定场景训练的前提下，实现了在3D场景中根据自然语言精确定位目标物体，并在主流数据集上大幅提升了零样本方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽然在有监督条件下表现较好，但实际应用场景更需要零样本（zero-shot）能力。当前零样本方法在空间推理和细节保持上存在不足，难以广泛应用于复杂真实场景。

Method: 提出SeqVLM：先利用3D语义分割生成候选物体，通过语义筛选保留相关目标；然后采用多视角投影，将3D点云候选转为多视角图像，最大程度覆盖空间关系和上下文细节；最后设计动态调度机制，分步调用视觉-语言模型进行跨模态推理，保证效率与效果。

Result: 在ScanRefer和Nr3D两大3D视觉定位主流数据集上，SeqVLM在Acc@0.25指标上达到55.6%和53.2%，分别超越现有零样本方法4.0%和5.2%。

Conclusion: SeqVLM有效提升了3D视觉定位的泛化能力，在无需场景特定训练的前提下实现了更优的性能，使零样本3DVG更接近实际应用。

Abstract: 3D Visual Grounding (3DVG) aims to localize objects in 3D scenes using
natural language descriptions. Although supervised methods achieve higher
accuracy in constrained settings, zero-shot 3DVG holds greater promise for
real-world applications since eliminating scene-specific training requirements.
However, existing zero-shot methods face challenges of spatial-limited
reasoning due to reliance on single-view localization, and contextual omissions
or detail degradation. To address these issues, we propose SeqVLM, a novel
zero-shot 3DVG framework that leverages multi-view real-world scene images with
spatial information for target object reasoning. Specifically, SeqVLM first
generates 3D instance proposals via a 3D semantic segmentation network and
refines them through semantic filtering, retaining only semantic-relevant
candidates. A proposal-guided multi-view projection strategy then projects
these candidate proposals onto real scene image sequences, preserving spatial
relationships and contextual details in the conversion process of 3D point
cloud to images. Furthermore, to mitigate VLM computational overload, we
implement a dynamic scheduling mechanism that iteratively processes
sequances-query prompts, leveraging VLM's cross-modal reasoning capabilities to
identify textually specified objects. Experiments on the ScanRefer and Nr3D
benchmarks demonstrate state-of-the-art performance, achieving Acc@0.25 scores
of 55.6% and 53.2%, surpassing previous zero-shot methods by 4.0% and 5.2%,
respectively, which advance 3DVG toward greater generalization and real-world
applicability. The code is available at https://github.com/JiawLin/SeqVLM.

</details>


### [68] [Occlusion Robustness of CLIP for Military Vehicle Classification](https://arxiv.org/abs/2508.20760)
*Jan Erik van Woerden,Gertjan Burghouts,Lotte Nijskens,Alma M. Liezenga,Sabina van Rooij,Frank Ruis,Hugo J. Kuijf*

Main category: cs.CV

TL;DR: 本文分析了CLIP视觉-语言模型在军事环境下遮挡和信噪比降级场景中的鲁棒性，表明Transformer结构优于CNN，改进和微调模型可提升在高遮挡下的表现。


<details>
  <summary>Details</summary>
Motivation: 传统VLM如CLIP主要在标注资源稀缺的防御场景下具优势，但其在复杂军事环境（如部分遮挡、低信噪比）下的鲁棒性还未被深入研究，因此需测试其在这类真实挑战下的可靠性。

Method: 作者基于18类军事车辆自建数据集，对CLIP及其变体分别在不同遮挡百分比（包括细粒度分散和大片连续遮挡）下进行性能评测，采用归一化曲线下面积（NAUC）等指标进行量化分析，并对比Transformer与CNN结构，探讨线性探针与骨干微调对抗遮挡能力。

Result: 1. Transformer结构的CLIP模型优于CNN模型。2. 分散的遮挡比大的连续遮挡对性能影响更大。3. 线性探针模型在约35%遮挡时性能大幅下降。4. 微调骨干网络后，性能显著提升至60%以上遮挡仍稳定。

Conclusion: 研究强调了在训练过程中引入面向遮挡的增强技术的重要性，同时指出需进一步探索模型对补丁（patch）级遮挡的敏感性及架构层面的鲁棒性设计，为CLIP在真实军事场景中的应用提供改进方向。

Abstract: Vision-language models (VLMs) like CLIP enable zero-shot classification by
aligning images and text in a shared embedding space, offering advantages for
defense applications with scarce labeled data. However, CLIP's robustness in
challenging military environments, with partial occlusion and degraded
signal-to-noise ratio (SNR), remains underexplored. We investigate CLIP
variants' robustness to occlusion using a custom dataset of 18 military vehicle
classes and evaluate using Normalized Area Under the Curve (NAUC) across
occlusion percentages. Four key insights emerge: (1) Transformer-based CLIP
models consistently outperform CNNs, (2) fine-grained, dispersed occlusions
degrade performance more than larger contiguous occlusions, (3) despite
improved accuracy, performance of linear-probed models sharply drops at around
35% occlusion, (4) by finetuning the model's backbone, this performance drop
occurs at more than 60% occlusion. These results underscore the importance of
occlusion-specific augmentations during training and the need for further
exploration into patch-level sensitivity and architectural resilience for
real-world deployment of CLIP.

</details>


### [69] [Looking Beyond the Obvious: A Survey on Abstract Concept Recognition for Video Understanding](https://arxiv.org/abs/2508.20765)
*Gowreesh Mago,Pascal Mettes,Stevan Rudinac*

Main category: cs.CV

TL;DR: 本文综述了视频内容抽象概念理解的挑战与进展，强调在多模态基础模型时代融合过往成果的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管机器对视频中具体对象和事件已能高效识别，但在理解公正、自由、团结等高度抽象概念方面，仍远落后于人类，而这对于实现更贴合人类价值观的视频理解系统至关重要。

Method: 本文主要采取综述方法，回顾了相关任务和数据集，分析了现有技术和方法，并探讨了基础大模型（如多模态模型）在理解视频抽象概念方面的应用潜力。

Result: 研究发现，学界长期以来不间断地关注抽象概念理解，持续利用当时最先进的工具攻关，并取得了一些阶段性进展和经验积累。

Conclusion: 作者主张，在多模态基础模型等新时代背景下，充分吸收前人经验和成果对于解决这一重大挑战至关重要，可以避免重复造轮子，更有效地推动领域发展。

Abstract: The automatic understanding of video content is advancing rapidly. Empowered
by deeper neural networks and large datasets, machines are increasingly capable
of understanding what is concretely visible in video frames, whether it be
objects, actions, events, or scenes. In comparison, humans retain a unique
ability to also look beyond concrete entities and recognize abstract concepts
like justice, freedom, and togetherness. Abstract concept recognition forms a
crucial open challenge in video understanding, where reasoning on multiple
semantic levels based on contextual information is key. In this paper, we argue
that the recent advances in foundation models make for an ideal setting to
address abstract concept understanding in videos. Automated understanding of
high-level abstract concepts is imperative as it enables models to be more
aligned with human reasoning and values. In this survey, we study different
tasks and datasets used to understand abstract concepts in video content. We
observe that, periodically and over a long period, researchers have attempted
to solve these tasks, making the best use of the tools available at their
disposal. We advocate that drawing on decades of community experience will help
us shed light on this important open grand challenge and avoid ``re-inventing
the wheel'' as we start revisiting it in the era of multi-modal foundation
models.

</details>


### [70] [Safer Skin Lesion Classification with Global Class Activation Probability Map Evaluation and SafeML](https://arxiv.org/abs/2508.20776)
*Kuniko Paxton,Koorosh Aslansefat,Amila Akagić,Dhavalkumar Thakker,Yiannis Papadopoulos*

Main category: cs.CV

TL;DR: 本文提出了一种新的皮肤病变分类可解释性方法，提升AI诊断的可信度，并增强了误诊检测能力。


<details>
  <summary>Details</summary>
Motivation: 尽管皮肤病变AI分类模型精度已超越部分皮肤科医生，但临床中仍存在对AI模型的不信任。当前可解释性方法（如LIME和CAM）存在可靠性问题，难以满足医学诊断的高可信要求。

Method: 提出了Global Class Activation Probabilistic Map Evaluation方法，对所有类别的激活概率图在像素级别进行概率分析与可视化，同时结合SafeML技术实现误诊检测和预警。

Result: 在ISIC公开数据集上，结合MobileNetV2和Vision Transformers模型验证了方法有效性。

Conclusion: 新方法不仅提升了AI诊断结果的可解释性和可信度，也能实时发现并警示潜在误诊，提高临床安全性。

Abstract: Recent advancements in skin lesion classification models have significantly
improved accuracy, with some models even surpassing dermatologists' diagnostic
performance. However, in medical practice, distrust in AI models remains a
challenge. Beyond high accuracy, trustworthy, explainable diagnoses are
essential. Existing explainability methods have reliability issues, with
LIME-based methods suffering from inconsistency, while CAM-based methods
failing to consider all classes. To address these limitations, we propose
Global Class Activation Probabilistic Map Evaluation, a method that analyses
all classes' activation probability maps probabilistically and at a pixel
level. By visualizing the diagnostic process in a unified manner, it helps
reduce the risk of misdiagnosis. Furthermore, the application of SafeML
enhances the detection of false diagnoses and issues warnings to doctors and
patients as needed, improving diagnostic reliability and ultimately patient
safety. We evaluated our method using the ISIC datasets with MobileNetV2 and
Vision Transformers.

</details>


### [71] [Evaluating Compositional Generalisation in VLMs and Diffusion Models](https://arxiv.org/abs/2508.20783)
*Beth Pearson,Bilal Boulbarss,Michael Wray,Martha Lewis*

Main category: cs.CV

TL;DR: 本文探讨了主流视觉-语言模型（如CLIP）在组合新意义能力上的局限，发现它们难以正确结合图像中的属性和关系，而基于扩散模型的分类器在某些组合任务中表现更好。


<details>
  <summary>Details</summary>
Motivation: 虽然VLMs（视觉-语言模型）近年来取得了显著进展，但它们在从已知部分组合出新意义时表现出不足，尤其是在对属性和对象关系的理解上，这限制了其在更复杂语义场景下的应用。

Method: 论文对扩散模型分类器（Diffusion Classifier）、CLIP和ViLT三种模型进行评估，通过零样本学习（ZSL）和广义零样本学习（GZSL）任务，测试它们在对象属性和关系绑定方面的泛化能力。

Result: 实验结果表明，Diffusion Classifier和ViLT在概念绑定任务上表现良好，但三个模型在关系型GZSL任务中均表现不佳，暴露了VLMs在关系推理上的不足。分析显示，CLIP对诸如“左”和“右”这类关系性词语的表征过于相似。

Conclusion: 当前VLMs，尽管在部分组合语义识别上取得进展，但在关系推理领域仍有显著挑战。扩散模型带来一定提升，但要实现更强的组合泛化能力，还需更进一步的研究。

Abstract: A fundamental aspect of the semantics of natural language is that novel
meanings can be formed from the composition of previously known parts.
Vision-language models (VLMs) have made significant progress in recent years,
however, there is evidence that they are unable to perform this kind of
composition. For example, given an image of a red cube and a blue cylinder, a
VLM such as CLIP is likely to incorrectly label the image as a red cylinder or
a blue cube, indicating it represents the image as a `bag-of-words' and fails
to capture compositional semantics. Diffusion models have recently gained
significant attention for their impressive generative abilities, and zero-shot
classifiers based on diffusion models have been shown to perform competitively
with CLIP in certain compositional tasks. In this work we explore whether the
generative Diffusion Classifier has improved compositional generalisation
abilities compared to discriminative models. We assess three models --
Diffusion Classifier, CLIP, and ViLT -- on their ability to bind objects with
attributes and relations in both zero-shot learning (ZSL) and generalised
zero-shot learning (GZSL) settings. Our results show that the Diffusion
Classifier and ViLT perform well at concept binding tasks, but that all models
struggle significantly with the relational GZSL task, underscoring the broader
challenges VLMs face with relational reasoning. Analysis of CLIP embeddings
suggests that the difficulty may stem from overly similar representations of
relational concepts such as left and right. Code and dataset are available at:
https://github.com/otmive/diffusion_classifier_clip

</details>


### [72] [Surfel-based 3D Registration with Equivariant SE(3) Features](https://arxiv.org/abs/2508.20789)
*Xueyang Kang,Hang Zhao,Kourosh Khoshelham,Patrick Vandewalle*

Main category: cs.CV

TL;DR: 该论文提出了一种基于surfel的姿态学习回归方法，专注解决现有点云配准方法对点的方向和不确定性忽略所带来的鲁棒性不足问题，通过引入SE(3)等变特征卷积提升对噪声和强旋转的抗干扰能力，实验表明性能优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有点云配准方法普遍忽视点的方向性和不确定性，导致模型易受噪声和大范围旋转影响，需要大量带变换的数据扩增才能鲁棒。这一问题在实际三维重建（如遥感、数字遗产）中极为关键，亟需更鲁棒的配准方法。

Method: 论文提出用surfel（表面元素）作为特征载体，从激光雷达点云中用虚拟相机参数初始化surfels，通过SE(3)等变卷积核显式学习位置与旋转的等变特征。模型结构包括等变卷积编码器、跨注意力机制计算相似度、全连接解码器和非线性Huber损失。最终回归源与目标点云间的相对变换。

Result: 在室内和室外数据集上的实验证明，本文方法在真实点云扫描的配准任务中，相比当前最先进方法展现出更优越和更强鲁棒性的性能表现。

Conclusion: 该方法有效提升了点云配准在面对噪声与强旋转情况时的鲁棒性，减少对大规模带变换训练数据的需求，具有很好的实际应用价值。

Abstract: Point cloud registration is crucial for ensuring 3D alignment consistency of
multiple local point clouds in 3D reconstruction for remote sensing or digital
heritage. While various point cloud-based registration methods exist, both
non-learning and learning-based, they ignore point orientations and point
uncertainties, making the model susceptible to noisy input and aggressive
rotations of the input point cloud like orthogonal transformation; thus, it
necessitates extensive training point clouds with transformation augmentations.
To address these issues, we propose a novel surfel-based pose learning
regression approach. Our method can initialize surfels from Lidar point cloud
using virtual perspective camera parameters, and learns explicit
$\mathbf{SE(3)}$ equivariant features, including both position and rotation
through $\mathbf{SE(3)}$ equivariant convolutional kernels to predict relative
transformation between source and target scans. The model comprises an
equivariant convolutional encoder, a cross-attention mechanism for similarity
computation, a fully-connected decoder, and a non-linear Huber loss.
Experimental results on indoor and outdoor datasets demonstrate our model
superiority and robust performance on real point-cloud scans compared to
state-of-the-art methods.

</details>


### [73] [Adapting Foundation Model for Dental Caries Detection with Dual-View Co-Training](https://arxiv.org/abs/2508.20813)
*Tao Luo,Han Wu,Tong Yang,Dinggang Shen,Zhiming Cui*

Main category: cs.CV

TL;DR: 该论文提出一种新的深度学习网络DVCTNet，通过结合全景口腔X光和单颗牙齿图像进行联合训练，提高了龋齿检测的准确率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的龋齿检测方法在面对影像对比度细微变化及病变多样性时准确率不足。临床上，牙医通常会结合全景筛查与局部详细检查。该论文受此启发，旨在弥补现有技术的不足。

Method: 提出DVCTNet，包含自动牙齿检测、全景与局部双视图信息通路。分别预训练全局与局部的视觉基础模型，通过区域建议匹配对应牙齿局部，再用门控跨视图注意力模块（GCV-Atten）动态融合两种特征，提升最终检测效果。

Result: 在公开数据集和新构建的高精度龋齿检测数据集上，DVCTNet在准确率上均超过了现有SOTA方法，显示出更强的临床应用潜力。

Conclusion: DVCTNet能有效融合全景与局部信息，极大提升了牙齿龋齿检测的准确性，为实际临床应用提供了新工具。源码和数据集已开源，便于后续研究和应用。

Abstract: Accurate dental caries detection from panoramic X-rays plays a pivotal role
in preventing lesion progression. However, current detection methods often
yield suboptimal accuracy due to subtle contrast variations and diverse lesion
morphology of dental caries. In this work, inspired by the clinical workflow
where dentists systematically combine whole-image screening with detailed
tooth-level inspection, we present DVCTNet, a novel Dual-View Co-Training
network for accurate dental caries detection. Our DVCTNet starts with employing
automated tooth detection to establish two complementary views: a global view
from panoramic X-ray images and a local view from cropped tooth images. We then
pretrain two vision foundation models separately on the two views. The
global-view foundation model serves as the detection backbone, generating
region proposals and global features, while the local-view model extracts
detailed features from corresponding cropped tooth patches matched by the
region proposals. To effectively integrate information from both views, we
introduce a Gated Cross-View Attention (GCV-Atten) module that dynamically
fuses dual-view features, enhancing the detection pipeline by integrating the
fused features back into the detection model for final caries detection. To
rigorously evaluate our DVCTNet, we test it on a public dataset and further
validate its performance on a newly curated, high-precision dental caries
detection dataset, annotated using both intra-oral images and panoramic X-rays
for double verification. Experimental results demonstrate DVCTNet's superior
performance against existing state-of-the-art (SOTA) methods on both datasets,
indicating the clinical applicability of our method. Our code and labeled
dataset are available at https://github.com/ShanghaiTech-IMPACT/DVCTNet.

</details>


### [74] [FusionCounting: Robust visible-infrared image fusion guided by crowd counting via multi-task learning](https://arxiv.org/abs/2508.20817)
*He Li,Xinyu Liu,Weihang Kong,Xingchen Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为FusionCounting的新型多任务学习框架，将人群计数任务与可见光-红外图像融合（VIF）过程结合，提升融合图像质量与人群计数效果。


<details>
  <summary>Details</summary>
Motivation: 现有VIF方法多关注融合图像的视觉质量，并尝试引入语义分割或目标检测等下游任务作为语义指导，但语义分割需要大量标注，目标检测在拥挤场景下存在重叠和遮挡问题。同时，尽管RGB-T人群计数受到关注，目前尚无人将VIF与人群计数统一到一个框架。

Method: 提出FusionCounting框架，将人群计数作为VIF的辅助任务，通过多任务学习联合优化融合图像和人群密度估计。设计了动态损失权重分配方法以平衡不同任务贡献，并引入对抗训练提升系统鲁棒性和抗攻击能力。

Result: 在公开数据集上的实验表明，FusionCounting不仅提升了融合图像的质量，还在人群计数任务上取得了优异表现。

Conclusion: 将人群计数和VIF有机结合，通过多任务互助推动两项任务性能提升，尤其适用于密集场景，具有更高稳定性和鲁棒性。

Abstract: Most visible and infrared image fusion (VIF) methods focus primarily on
optimizing fused image quality. Recent studies have begun incorporating
downstream tasks, such as semantic segmentation and object detection, to
provide semantic guidance for VIF. However, semantic segmentation requires
extensive annotations, while object detection, despite reducing annotation
efforts compared with segmentation, faces challenges in highly crowded scenes
due to overlapping bounding boxes and occlusion. Moreover, although RGB-T crowd
counting has gained increasing attention in recent years, no studies have
integrated VIF and crowd counting into a unified framework. To address these
challenges, we propose FusionCounting, a novel multi-task learning framework
that integrates crowd counting into the VIF process. Crowd counting provides a
direct quantitative measure of population density with minimal annotation,
making it particularly suitable for dense scenes. Our framework leverages both
input images and population density information in a mutually beneficial
multi-task design. To accelerate convergence and balance tasks contributions,
we introduce a dynamic loss function weighting strategy. Furthermore, we
incorporate adversarial training to enhance the robustness of both VIF and
crowd counting, improving the model's stability and resilience to adversarial
attacks. Experimental results on public datasets demonstrate that
FusionCounting not only enhances image fusion quality but also achieves
superior crowd counting performance.

</details>


### [75] [Estimating 2D Keypoints of Surgical Tools Using Vision-Language Models with Low-Rank Adaptation](https://arxiv.org/abs/2508.20830)
*Krit Duangprom,Tryphon Lambrou,Binod Bhattarai*

Main category: cs.CV

TL;DR: 本论文提出了一种新颖的2D手术工具关键点检测方法，通过微调视觉语言模型(VLM)并结合LoRA技术，有效提升小样本数据集上的表现，优于传统CNN或Transformer方法。


<details>
  <summary>Details</summary>
Motivation: 以往CNN或Transformer方法在医学小数据集上易出现过拟合，难以泛化，因此需寻求更具泛化能力的方法提升关键点检测准确性。

Method: 采用预训练VLM，并用LoRA低秩微调技术进行高效微调，通过精心设计提示(prompts)构建指令微调数据集，将视觉特征与语义关键点描述对齐。

Result: 在仅2个epoch微调后，所提出的VLM模型效果优于多种基线模型，验证了方法在低资源场景下的有效性。

Conclusion: 本方法不仅提升了2D关键点检测性能，也为未来拓展到3D手术手部与工具姿态估计提供了前景。

Abstract: This paper presents a novel pipeline for 2D keypoint estima- tion of surgical
tools by leveraging Vision Language Models (VLMs) fine- tuned using a low rank
adjusting (LoRA) technique. Unlike traditional Convolutional Neural Network
(CNN) or Transformer-based approaches, which often suffer from overfitting in
small-scale medical datasets, our method harnesses the generalization
capabilities of pre-trained VLMs. We carefully design prompts to create an
instruction-tuning dataset and use them to align visual features with semantic
keypoint descriptions. Experimental results show that with only two epochs of
fine tuning, the adapted VLM outperforms the baseline models, demonstrating the
ef- fectiveness of LoRA in low-resource scenarios. This approach not only
improves keypoint detection performance, but also paves the way for future work
in 3D surgical hands and tools pose estimation.

</details>


### [76] [PointDGRWKV: Generalizing RWKV-like Architecture to Unseen Domains for Point Cloud Classification](https://arxiv.org/abs/2508.20835)
*Hao Yang,Qianyu Zhou,Haijia Sun,Xiangtai Li,Xuequan Lu,Lizhuang Ma,Shuicheng Yan*

Main category: cs.CV

TL;DR: 提出了PointDGRWKV，这是首个基于RWKV架构、专为三维点云分类领域泛化（DG PCC）设计的模型，在多个基准上取得了最先进表现。


<details>
  <summary>Details</summary>
Motivation: 现有点云分类泛化方法（如卷积、Transformer、Mamba等）在建模接收域、计算复杂度或长距离依赖方面存在不足。RWKV具备线性复杂度、全局感受野及长距离建模能力，但直接应用时存在跨域泛化问题。

Method: 分析了直接将RWKV用于点云分类时遭遇的空间失真和关注漂移问题。提出Adaptive Geometric Token Shift（自适应几何Token迁移）以提升局部几何建模能力，以及Cross-Domain key feature Distribution Alignment（跨域关键特征分布对齐）解决特征分布漂移。整体实现了更佳空间建模与跨域鲁棒性，同时保留了RWKV高效性。

Result: 在多个泛化基准上，PointDGRWKV模型的点云分类性能达到了业界领先水平。

Conclusion: 本工作首次证实了RWKV架构在点云分类泛化领域的潜力。提出的新模块有效解决了原始RWKV面临的空间与跨域建模挑战，为后续相关研究提供了新的思路。

Abstract: Domain Generalization (DG) has been recently explored to enhance the
generalizability of Point Cloud Classification (PCC) models toward unseen
domains. Prior works are based on convolutional networks, Transformer or Mamba
architectures, either suffering from limited receptive fields or high
computational cost, or insufficient long-range dependency modeling. RWKV, as an
emerging architecture, possesses superior linear complexity, global receptive
fields, and long-range dependency. In this paper, we present the first work
that studies the generalizability of RWKV models in DG PCC. We find that
directly applying RWKV to DG PCC encounters two significant challenges: RWKV's
fixed direction token shift methods, like Q-Shift, introduce spatial
distortions when applied to unstructured point clouds, weakening local
geometric modeling and reducing robustness. In addition, the Bi-WKV attention
in RWKV amplifies slight cross-domain differences in key distributions through
exponential weighting, leading to attention shifts and degraded generalization.
To this end, we propose PointDGRWKV, the first RWKV-based framework tailored
for DG PCC. It introduces two key modules to enhance spatial modeling and
cross-domain robustness, while maintaining RWKV's linear efficiency. In
particular, we present Adaptive Geometric Token Shift to model local
neighborhood structures to improve geometric context awareness. In addition,
Cross-Domain key feature Distribution Alignment is designed to mitigate
attention drift by aligning key feature distributions across domains. Extensive
experiments on multiple benchmarks demonstrate that PointDGRWKV achieves
state-of-the-art performance on DG PCC.

</details>


### [77] [PathMR: Multimodal Visual Reasoning for Interpretable Pathology Diagnosis](https://arxiv.org/abs/2508.20851)
*Ye Zhang,Yu Zhou,Jingwen Qi,Yongbing Zhang,Simon Puettmann,Finn Wichmann,Larissa Pereira Ferreira,Lara Sichward,Julius Keyl,Sylvia Hartmann,Shuo Zhao,Hongxiao Wang,Xiaowei Xu,Jianxu Chen*

Main category: cs.CV

TL;DR: 提出PathMR，一种针对病理图像分析的细胞级多模态视觉推理框架，实现了专家级诊断解释和细胞分布预测，并在多个数据集上显著优于现有方法，提升AI病理诊断的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习病理自动诊断效率高但难以解释，导致临床应用受限，亟需兼具视觉定位和自然语言解释的可解释AI模型。

Method: 提出PathMR框架，结合病理图像和文本查询，实现细胞级分割和专家风格诊断文本生成，并在PathGen和新建GADVR数据集上进行评估。

Result: PathMR在文本生成质量、分割精度及跨模态对齐度上均显著优于现有主流视觉推理模型。

Conclusion: PathMR有效提升了病理AI诊断的透明性和可解释性，有望促进临床实际应用，代码开源以推动领域发展。

Abstract: Deep learning based automated pathological diagnosis has markedly improved
diagnostic efficiency and reduced variability between observers, yet its
clinical adoption remains limited by opaque model decisions and a lack of
traceable rationale. To address this, recent multimodal visual reasoning
architectures provide a unified framework that generates segmentation masks at
the pixel level alongside semantically aligned textual explanations. By
localizing lesion regions and producing expert style diagnostic narratives,
these models deliver the transparent and interpretable insights necessary for
dependable AI assisted pathology. Building on these advancements, we propose
PathMR, a cell-level Multimodal visual Reasoning framework for Pathological
image analysis. Given a pathological image and a textual query, PathMR
generates expert-level diagnostic explanations while simultaneously predicting
cell distribution patterns. To benchmark its performance, we evaluated our
approach on the publicly available PathGen dataset as well as on our newly
developed GADVR dataset. Extensive experiments on these two datasets
demonstrate that PathMR consistently outperforms state-of-the-art visual
reasoning methods in text generation quality, segmentation accuracy, and
cross-modal alignment. These results highlight the potential of PathMR for
improving interpretability in AI-driven pathological diagnosis. The code will
be publicly available in https://github.com/zhangye-zoe/PathMR.

</details>


### [78] [Deep Learning Framework for Early Detection of Pancreatic Cancer Using Multi-Modal Medical Imaging Analysis](https://arxiv.org/abs/2508.20877)
*Dennis Slobodzian,Karissa Tilbury,Amir Kordijazi*

Main category: cs.CV

TL;DR: 本文提出并验证了一套基于深度学习的双模成像（自体荧光与二次谐波生成）早期胰腺导管腺癌（PDAC）检测方法，在有限的患者样本上显著提升了癌症识别准确率。


<details>
  <summary>Details</summary>
Motivation: PDAC的早期发现率低，导致五年生存率低于10%。目前的人工分析方法效率和准确率有限，临床急需高效、自动化的辅助诊断工具。

Method: 分析40例双模成像患者样本，搭建并对比六种深度学习网络，包括传统CNN和ViT，最终通过优化（如冻结预训练层与类别加权）选定改进版ResNet，解决小样本和类别不平衡等医学图像常见难题。

Result: 最终模型（改进ResNet）在癌症检测任务上取得超过90%的准确率，性能大幅超过人工分析。

Conclusion: 该深度学习框架为自动化PDAC检测奠定了基础，可提升病理学家诊断能力，并有望推广到其他癌症及小样本医学影像任务，对实际临床应用具有现实意义。

Abstract: Pacreatic ductal adenocarcinoma (PDAC) remains one of the most lethal forms
of cancer, with a five-year survival rate below 10% primarily due to late
detection. This research develops and validates a deep learning framework for
early PDAC detection through analysis of dual-modality imaging:
autofluorescence and second harmonic generation (SHG). We analyzed 40 unique
patient samples to create a specialized neural network capable of
distinguishing between normal, fibrotic, and cancerous tissue. Our methodology
evaluated six distinct deep learning architectures, comparing traditional
Convolutional Neural Networks (CNNs) with modern Vision Transformers (ViTs).
Through systematic experimentation, we identified and overcome significant
challenges in medical image analysis, including limited dataset size and class
imbalance. The final optimized framework, based on a modified ResNet
architecture with frozen pre-trained layers and class-weighted training,
achieved over 90% accuracy in cancer detection. This represents a significant
improvement over current manual analysis methods an demonstrates potential for
clinical deployment. This work establishes a robust pipeline for automated PDAC
detection that can augment pathologists' capabilities while providing a
foundation for future expansion to other cancer types. The developed
methodology also offers valuable insights for applying deep learning to
limited-size medical imaging datasets, a common challenge in clinical
applications.

</details>


### [79] [Understanding and evaluating computer vision models through the lens of counterfactuals](https://arxiv.org/abs/2508.20881)
*Pushkar Shukla*

Main category: cs.CV

TL;DR: 本文通过反事实推理，提出了一系列用于解释、审计和缓解视觉分类和生成模型偏见的框架。这些方法揭示了模型中的因果关系、错误相关性，并提供了公平性与可解释性的新工具。


<details>
  <summary>Details</summary>
Motivation: 随着AI被广泛应用于决策领域，模型解释性和公平性变得至关重要。当前模型易因数据偏见做出不公正、不可解释的决策，尤其在视觉相关任务中更为突出。因此，作者希望通过反事实方法系统性地发现和消除模型偏见。

Method: 针对判别式（分类）模型：提出CAVLI，将LIME和TCAV结合，以量化模型对人类可解释概念的依赖，通过热力图和概念依赖分数揭示不相关特征现象。提出ASAC，利用对抗性反事实扰动受保护属性，通过课程式学习提升模型公平性和准确率。针对生成式（文本到图像）模型：提出TIBET，构建可扩展评测管道，系统性变化身份相关词，进行偏见因果审计。BiasConnect建立因果图诊断交互式偏见。InterMit则是无训练、模块化的算法，通过因果敏感度和用户定义的公平标准缓解交互性偏见。

Result: 上述方法系统性揭示并度量了视觉模型中的偏见和因果关系。CAVLI和ASAC证明可提升分类器的公平性与鲁棒性。TIBET和BiasConnect成功识别了生成模型在性别、年龄、种族等维度的系统性偏见，InterMit有效实现了无需重训练的偏见缓解。

Conclusion: 作者证明了反事实推理在提高AI模型可解释性、公平性和因果分析中的统一核心作用，并提出了一套可扩展、原理性强且实际有效的框架，为社会责任型AI系统的偏见评估和消除提供了重要工具。

Abstract: Counterfactual reasoning -- the practice of asking ``what if'' by varying
inputs and observing changes in model behavior -- has become central to
interpretable and fair AI. This thesis develops frameworks that use
counterfactuals to explain, audit, and mitigate bias in vision classifiers and
generative models. By systematically altering semantically meaningful
attributes while holding others fixed, these methods uncover spurious
correlations, probe causal dependencies, and help build more robust systems.
  The first part addresses vision classifiers. CAVLI integrates attribution
(LIME) with concept-level analysis (TCAV) to quantify how strongly decisions
rely on human-interpretable concepts. With localized heatmaps and a Concept
Dependency Score, CAVLI shows when models depend on irrelevant cues like
backgrounds. Extending this, ASAC introduces adversarial counterfactuals that
perturb protected attributes while preserving semantics. Through curriculum
learning, ASAC fine-tunes biased models for improved fairness and accuracy
while avoiding stereotype-laden artifacts.
  The second part targets generative Text-to-Image (TTI) models. TIBET provides
a scalable pipeline for evaluating prompt-sensitive biases by varying
identity-related terms, enabling causal auditing of how race, gender, and age
affect image generation. To capture interactions, BiasConnect builds causal
graphs diagnosing intersectional biases. Finally, InterMit offers a modular,
training-free algorithm that mitigates intersectional bias via causal
sensitivity scores and user-defined fairness goals.
  Together, these contributions show counterfactuals as a unifying lens for
interpretability, fairness, and causality in both discriminative and generative
models, establishing principled, scalable methods for socially responsible bias
evaluation and mitigation.

</details>


### [80] [Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation](https://arxiv.org/abs/2508.20909)
*Yifan Gao,Haoyue Li,Feng Yuan,Xiaosong Wang,Xin Gao*

Main category: cs.CV

TL;DR: 本文提出Dino U-Net模型，利用DINOv3视觉基础模型的高质量特征显著提升医学图像分割性能，在多个数据集上取得了最优结果。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模自然图像预训练的基础模型在医学图像分割中表现出较好潜力，但如何高效迁移其丰富表征到具体临床应用仍然具有挑战。

Method: 提出了Dino U-Net架构，采用冻结的DINOv3主干网络作为编码器，通过专用适配器融合高层语义与低层空间特征，配合新设计的FAPM模块提升特征降维过程的表达能力，编码结果再输入解码器进行分割。

Result: 在七个多样化的公开医学图像分割数据集上，Dino U-Net均取得了优于主流方法的最先进性能，并且随着主干模型参数规模扩大，分割准确率持续提升。

Conclusion: Dino U-Net展现了基础模型密集预训练特征在医学图像分割中的高效与强大迁移能力，提供了一种参数高效且精度领先的新型分割解决方案。

Abstract: Foundation models pre-trained on large-scale natural image datasets offer a
powerful paradigm for medical image segmentation. However, effectively
transferring their learned representations for precise clinical applications
remains a challenge. In this work, we propose Dino U-Net, a novel
encoder-decoder architecture designed to exploit the high-fidelity dense
features of the DINOv3 vision foundation model. Our architecture introduces an
encoder built upon a frozen DINOv3 backbone, which employs a specialized
adapter to fuse the model's rich semantic features with low-level spatial
details. To preserve the quality of these representations during dimensionality
reduction, we design a new fidelity-aware projection module (FAPM) that
effectively refines and projects the features for the decoder. We conducted
extensive experiments on seven diverse public medical image segmentation
datasets. Our results show that Dino U-Net achieves state-of-the-art
performance, consistently outperforming previous methods across various imaging
modalities. Our framework proves to be highly scalable, with segmentation
accuracy consistently improving as the backbone model size increases up to the
7-billion-parameter variant. The findings demonstrate that leveraging the
superior, dense-pretrained features from a general-purpose foundation model
provides a highly effective and parameter-efficient approach to advance the
accuracy of medical image segmentation. The code is available at
https://github.com/yifangao112/DinoUNet.

</details>


### [81] [Classifying Mitotic Figures in the MIDOG25 Challenge with Deep Ensemble Learning and Rule Based Refinement](https://arxiv.org/abs/2508.20919)
*Sara Krauss,Ellena Spieß,Daniel Hieber,Frank Kramer,Johannes Schobel,Dominik Müller*

Main category: cs.CV

TL;DR: 本文提出了一种结合深度卷积神经网络（ConvNeXtBase）集成和基于规则的方法对肿瘤切片中的异常和正常有丝分裂体进行分类，在MIDOG25测试集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 手动区分异常有丝分裂体（AMFs）和正常有丝分裂体（NMFs）效率低且主观，影响肿瘤分级的准确性。自动化区分方法亟需发展。

Method: 训练ConvNeXtBase模型集成（利用AUCMEDI框架），并引入基于规则细化（RBR）模块对初步结果进行后处理。

Result: 在MIDOG25数据集测试中，模型集成取得平衡准确率84.02%。引入RBR后，特异性提高但敏感性和整体性能下降。

Conclusion: 深度模型集成对AMF分类效果较好，规则模式可提升某些指标，但整体应用有待进一步优化。

Abstract: Mitotic figures (MFs) are relevant biomarkers in tumor grading.
Differentiating atypical MFs (AMFs) from normal MFs (NMFs) remains difficult,
as manual annotation is time-consuming and subjective. In this work an ensemble
of ConvNeXtBase models was trained with AUCMEDI and extend with a rule-based
refinement (RBR) module. On the MIDOG25 preliminary test set, the ensemble
achieved a balanced accuracy of 84.02%. While the RBR increased specificity, it
reduced sensitivity and overall performance. The results show that deep
ensembles perform well for AMF classification. RBR can increase specific
metrics but requires further research.

</details>


### [82] [Olive Tree Satellite Image Segmentation Based On SAM and Multi-Phase Refinement](https://arxiv.org/abs/2508.20954)
*Amir Jmal,Chaima Chtourou,Mahdi Louati,Abdelaziz Kallel,Houda Khmila*

Main category: cs.CV

TL;DR: 本论文提出了一种创新方法，通过深度学习模型对卫星图像中的橄榄树进行高精度分割，有效提升了检测准确率。


<details>
  <summary>Details</summary>
Motivation: 在气候变化背景下，为了保持橄榄树的生物多样性，需要通过早期异常检测和处理来实现有效管理，遥感技术是关键工具。

Method: 提出结合Segment Anything Model（SAM）和高级分割技术，利用橄榄树在田间排列的几何对齐、形状和大小可学习约束，对卫星影像中橄榄树进行分割和修正。

Result: 该方法分割准确率达到98%，大幅超越原始SAM的82%。

Conclusion: 本文方法可实现橄榄树的自动、精确检测与管理，对维持农业生物多样性具有重要意义。

Abstract: In the context of proven climate change, maintaining olive biodiversity
through early anomaly detection and treatment using remote sensing technology
is crucial, offering effective management solutions. This paper presents an
innovative approach to olive tree segmentation from satellite images. By
leveraging foundational models and advanced segmentation techniques, the study
integrates the Segment Anything Model (SAM) to accurately identify and segment
olive trees in agricultural plots. The methodology includes SAM segmentation
and corrections based on trees alignement in the field and a learanble
constraint about the shape and the size. Our approach achieved a 98\% accuracy
rate, significantly surpassing the initial SAM performance of 82\%.

</details>


### [83] [E-ConvNeXt: A Lightweight and Efficient ConvNeXt Variant with Cross-Stage Partial Connections](https://arxiv.org/abs/2508.20955)
*Fang Wang,Huitao Li,Wenhan Chao,Zheng Zhuo,Yiran Ji,Chang Peng,Yupeng Sun*

Main category: cs.CV

TL;DR: 本文提出了一种适用于轻量级应用场景的新型卷积神经网络E-ConvNeXt，在参数量和复杂度大幅削减的同时保持了高性能。


<details>
  <summary>Details</summary>
Motivation: 许多高性能网络并未针对轻量级应用场景设计，导致其适用范围受限。因此，需要新的架构在减少模型复杂度的同时保持准确率。

Method: 以ConvNeXt为基础，(1) 整合Cross Stage Partial Network（CSPNet）机制并调整网络结构，显著降低复杂度；(2) 优化Stem和Block结构，提升特征表达与效率；(3) 用通道注意力代替Layer Scale。

Result: E-ConvNeXt在ImageNet分类上表现优异：E-ConvNeXt-mini在0.9GFLOPs达到78.3%准确率，E-ConvNeXt-small在3.1GFLOPs下准确率为81.9%。在目标检测等迁移学习任务中也展现了良好的泛化能力。

Conclusion: E-ConvNeXt有效兼顾了模型效率与准确率，适用于对资源敏感的轻量级应用，具有良好的推广前景。

Abstract: Many high-performance networks were not designed with lightweight application
scenarios in mind from the outset, which has greatly restricted their scope of
application. This paper takes ConvNeXt as the research object and significantly
reduces the parameter scale and network complexity of ConvNeXt by integrating
the Cross Stage Partial Connections mechanism and a series of optimized
designs. The new network is named E-ConvNeXt, which can maintain high accuracy
performance under different complexity configurations. The three core
innovations of E-ConvNeXt are : (1) integrating the Cross Stage Partial Network
(CSPNet) with ConvNeXt and adjusting the network structure, which reduces the
model's network complexity by up to 80%; (2) Optimizing the Stem and Block
structures to enhance the model's feature expression capability and operational
efficiency; (3) Replacing Layer Scale with channel attention. Experimental
validation on ImageNet classification demonstrates E-ConvNeXt's superior
accuracy-efficiency balance: E-ConvNeXt-mini reaches 78.3% Top-1 accuracy at
0.9GFLOPs. E-ConvNeXt-small reaches 81.9% Top-1 accuracy at 3.1GFLOPs. Transfer
learning tests on object detection tasks further confirm its generalization
capability.

</details>


### [84] [DrivingGaussian++: Towards Realistic Reconstruction and Editable Simulation for Surrounding Dynamic Driving Scenes](https://arxiv.org/abs/2508.20965)
*Yajiao Xiong,Xiaoyu Zhou,Yongtao Wan,Deqing Sun,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: DrivingGaussian++是一种高效、真实的自动驾驶动态场景重建与可控编辑框架，通过3D高斯建模与融合激光雷达、图像等多模态信息，实现无训练的场景重建、编辑及自动生成动态轨迹，提升了场景真实感和多样性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要对周边动态环境进行高保真建模和灵活编辑，以满足仿真、测试和感知训练等需求，现有方法在动态场景的真实性、细节一致性和编辑能力方面存在不足。

Method: 该方法采用增量式3D高斯对静态背景建模，动态物体通过复合动态高斯图进行精准重建，结合激光雷达先验和多视图图像、深度信息，提高了重建的细节和一致性。同时，融合大语言模型，能够自动生成动态物体轨迹并进行真实感增强，实现无需训练的可控编辑（如纹理修改、天气模拟、物体操作等）。

Result: 实验表明，本方法在动态场景重建和照片级真实感多视图合成上优于现有方法，支持训练自由的可控编辑，能够一致、真实地生成多样化的自动驾驶动态场景。

Conclusion: DrivingGaussian++显著提升了自动驾驶动态场景的重建质量、编辑灵活性和仿真多样性，具有较强的行业应用潜力。

Abstract: We present DrivingGaussian++, an efficient and effective framework for
realistic reconstructing and controllable editing of surrounding dynamic
autonomous driving scenes. DrivingGaussian++ models the static background using
incremental 3D Gaussians and reconstructs moving objects with a composite
dynamic Gaussian graph, ensuring accurate positions and occlusions. By
integrating a LiDAR prior, it achieves detailed and consistent scene
reconstruction, outperforming existing methods in dynamic scene reconstruction
and photorealistic surround-view synthesis. DrivingGaussian++ supports
training-free controllable editing for dynamic driving scenes, including
texture modification, weather simulation, and object manipulation, leveraging
multi-view images and depth priors. By integrating large language models (LLMs)
and controllable editing, our method can automatically generate dynamic object
motion trajectories and enhance their realism during the optimization process.
DrivingGaussian++ demonstrates consistent and realistic editing results and
generates dynamic multi-view driving scenarios, while significantly enhancing
scene diversity. More results and code can be found at the project site:
https://xiong-creator.github.io/DrivingGaussian_plus.github.io

</details>


### [85] [Webly-Supervised Image Manipulation Localization via Category-Aware Auto-Annotation](https://arxiv.org/abs/2508.20987)
*Chenfan Qu,Yiwu Zhong,Bin Li,Lianwen Jin*

Main category: cs.CV

TL;DR: 本论文提出一种新方法，利用来自网络的数据和自动标注技术，显著扩展了图像篡改区域定位的数据集规模，并通过构建高质量数据集及创新训练方式，大幅提升了定位模型的能力。


<details>
  <summary>Details</summary>
Motivation: 现有图像篡改区域定位任务面临标注数据稀缺和采集成本过高的问题，严重制约了模型的精度和应用推广。

Method: 作者提出以网络收集的伪造图像为基础，通过新范式CAAAv2自动生成像素级标注，并引入QES指标自动剔除低质量标注，最终构建了大规模高质量MIMLv2数据集。此外，Object Jitter技术进一步增加了伪造特效用于训练，最终开发了Web-IML模型，高效利用网络数据进行训练。

Result: 新构建的MIMLv2数据集比以往数据集如IMD20大120倍，包含246,212张伪造图像。Web-IML模型在多个伪造检测基线和真实世界测试集上性能显著提升，平均IoU提升24.1点，总体性能提升31%。

Conclusion: 本文方法极大缓解了图像篡改定位领域训练数据稀缺的问题，并通过创新方法提高了检测模型的实际效果，对今后相关研究和落地应用具有重要意义。

Abstract: Images manipulated using image editing tools can mislead viewers and pose
significant risks to social security. However, accurately localizing the
manipulated regions within an image remains a challenging problem. One of the
main barriers in this area is the high cost of data acquisition and the severe
lack of high-quality annotated datasets. To address this challenge, we
introduce novel methods that mitigate data scarcity by leveraging readily
available web data. We utilize a large collection of manually forged images
from the web, as well as automatically generated annotations derived from a
simpler auxiliary task, constrained image manipulation localization.
Specifically, we introduce a new paradigm CAAAv2, which automatically and
accurately annotates manipulated regions at the pixel level. To further improve
annotation quality, we propose a novel metric, QES, which filters out
unreliable annotations. Through CAAA v2 and QES, we construct MIMLv2, a
large-scale, diverse, and high-quality dataset containing 246,212 manually
forged images with pixel-level mask annotations. This is over 120x larger than
existing handcrafted datasets like IMD20. Additionally, we introduce Object
Jitter, a technique that further enhances model training by generating
high-quality manipulation artifacts. Building on these advances, we develop a
new model, Web-IML, designed to effectively leverage web-scale supervision for
the image manipulation localization task. Extensive experiments demonstrate
that our approach substantially alleviates the data scarcity problem and
significantly improves the performance of various models on multiple real-world
forgery benchmarks. With the proposed web supervision, Web-IML achieves a
striking performance gain of 31% and surpasses previous SOTA TruFor by 24.1
average IoU points. The dataset and code will be made publicly available at
https://github.com/qcf-568/MIML.

</details>


### [86] [ExpertSim: Fast Particle Detector Simulation Using Mixture-of-Generative-Experts](https://arxiv.org/abs/2508.20991)
*Patryk Będkowski,Jan Dubiński,Filip Szatkowski,Kamil Deja,Przemysław Rokita,Tomasz Trzciński*

Main category: cs.CV

TL;DR: 本论文提出了一种名为ExpertSim的深度学习仿真方法，能更高效精准地模拟粒子物理实验中的探测器响应，并大幅提升仿真速度。


<details>
  <summary>Details</summary>
Motivation: 在CERN大型强子对撞机（LHC）中，探测器响应仿真依赖于统计蒙特卡洛方法，但计算消耗巨大，给计算资源带来压力。为了提升效率，需要寻求更快且精度高的替代方案。

Method: 提出Mixture-of-Generative-Experts架构，每个生成专家（expert）专注于模拟数据不同的子集，从而提升对数据分布多样性的捕捉能力。以ALICE实验中的Zero Degree Calorimeter为例，定制了ExpertSim深度学习仿真方案。

Result: ExpertSim方法生产的数据不仅比传统蒙特卡洛方法更准确，而且生成速度大幅提升，并能高效应对数据分布异质性。

Conclusion: ExpertSim为高能物理探测器仿真提供了高效且准确的新方法，对CERN及类似实验的计算资源优化具有重要意义。

Abstract: Simulating detector responses is a crucial part of understanding the inner
workings of particle collisions in the Large Hadron Collider at CERN. Such
simulations are currently performed with statistical Monte Carlo methods, which
are computationally expensive and put a significant strain on CERN's
computational grid. Therefore, recent proposals advocate for generative machine
learning methods to enable more efficient simulations. However, the
distribution of the data varies significantly across the simulations, which is
hard to capture with out-of-the-box methods. In this study, we present
ExpertSim - a deep learning simulation approach tailored for the Zero Degree
Calorimeter in the ALICE experiment. Our method utilizes a
Mixture-of-Generative-Experts architecture, where each expert specializes in
simulating a different subset of the data. This allows for a more precise and
efficient generation process, as each expert focuses on a specific aspect of
the calorimeter response. ExpertSim not only improves accuracy, but also
provides a significant speedup compared to the traditional Monte-Carlo methods,
offering a promising solution for high-efficiency detector simulations in
particle physics experiments at CERN. We make the code available at
https://github.com/patrick-bedkowski/expertsim-mix-of-generative-experts.

</details>


### [87] [POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models](https://arxiv.org/abs/2508.21019)
*Jiaxiang Cheng,Bing Ma,Xuhua Ren,Hongyi Jin,Kai Yu,Peng Zhang,Wenyue Li,Yuan Zhou,Tianxiang Zheng,Qinglin Lu*

Main category: cs.CV

TL;DR: POSE是一种新的视频扩散模型蒸馏框架，大幅提升采样效率，实现优质视频的一步生成，并在各指标上优于现有加速方法。


<details>
  <summary>Details</summary>
Motivation: 当前大规模视频扩散模型采样效率低，难以快速生成高质量长视频，现有基于图像的方法无法有效捕捉时间一致性，也不适配大规模模型的单步蒸馏。

Method: POSE框架包括两个主要部分：1）稳定性启动，通过热身机制优化高质量一步生成器从高到低信噪比的过程，保证单步映射在流轨迹端点的稳定性和质量；2）统一自对抗均衡，通过灵活的自对抗机制促进单步对抗训练向纳什均衡收敛，在高斯噪声空间内实现更加真实的单步视频生成。此外，对于条件视频生成，引入了条件对抗一致性机制，增强语义一致性和帧间一致性。

Result: POSE在VBench-I2V基准上整体提升7.15%的语义对齐性、时序一致性和帧质量，将预训练模型推理延迟从1000秒降至10秒，提速100倍，同时维持了竞争性生成性能。

Conclusion: POSE为大规模视频扩散模型的高效蒸馏提供了有效方案，实现单步高质量生成、极大加速推理，在多个指标上均优于现有加速技术，有较强落地和拓展价值。

Abstract: The field of video diffusion generation faces critical bottlenecks in
sampling efficiency, especially for large-scale models and long sequences.
Existing video acceleration methods adopt image-based techniques but suffer
from fundamental limitations: they neither model the temporal coherence of
video frames nor provide single-step distillation for large-scale video models.
To bridge this gap, we propose POSE (Phased One-Step Equilibrium), a
distillation framework that reduces the sampling steps of large-scale video
diffusion models, enabling the generation of high-quality videos in a single
step. POSE employs a carefully designed two-phase process to distill video
models:(i) stability priming: a warm-up mechanism to stabilize adversarial
distillation that adapts the high-quality trajectory of the one-step generator
from high to low signal-to-noise ratio regimes, optimizing the video quality of
single-step mappings near the endpoints of flow trajectories. (ii) unified
adversarial equilibrium: a flexible self-adversarial distillation mechanism
that promotes stable single-step adversarial training towards a Nash
equilibrium within the Gaussian noise space, generating realistic single-step
videos close to real videos. For conditional video generation, we propose (iii)
conditional adversarial consistency, a method to improve both semantic
consistency and frame consistency between conditional frames and generated
frames. Comprehensive experiments demonstrate that POSE outperforms other
acceleration methods on VBench-I2V by average 7.15% in semantic alignment,
temporal conference and frame quality, reducing the latency of the pre-trained
model by 100$\times$, from 1000 seconds to 10 seconds, while maintaining
competitive performance.

</details>


### [88] [Reusing Computation in Text-to-Image Diffusion for Efficient Generation of Image Sets](https://arxiv.org/abs/2508.21032)
*Dale Decatur,Thibault Groueix,Wang Yifan,Rana Hanocka,Vladimir Kim,Matheus Gadelha*

Main category: cs.CV

TL;DR: 本文提出了一种减少文本到图像扩散模型计算成本的新方法：通过聚类相似文本提示，并在早期扩散步骤共享计算，从而提升效率且提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型虽然生成效果好，但计算资源消耗极大。以往的工作多致力于单次推理的优化，本文则关注于如何在多个相关提示的生成过程中，通过减少冗余计算以提升整体效率。

Method: 提出了一种训练无关的新方法，利用扩散模型由粗到细建模特性，把语义相似的提示进行聚类，并在扩散模型的早期去噪步骤中共享计算结果。具体来说，在模型基于图像嵌入条件生成的情况下，相似提示在早期共享生成特征，后续再分别细化。方法还结合了UnClip的text-to-image先验，以优化扩散步骤分配，提高效率。

Result: 实验证明，该方法显著降低了计算成本，同时还能提升最终生成图像的质量。对于大规模提示集，该方法具备良好的可扩展性，并能直接兼容已有的生成管线。

Conclusion: 本文提出的方法为应对大规模文本到图像生成的高计算消耗提供了有效解决方案，显著降低了环境与经济负担，并提升了生成效率与图像质量。

Abstract: Text-to-image diffusion models enable high-quality image generation but are
computationally expensive. While prior work optimizes per-inference efficiency,
we explore an orthogonal approach: reducing redundancy across correlated
prompts. Our method leverages the coarse-to-fine nature of diffusion models,
where early denoising steps capture shared structures among similar prompts. We
propose a training-free approach that clusters prompts based on semantic
similarity and shares computation in early diffusion steps. Experiments show
that for models trained conditioned on image embeddings, our approach
significantly reduces compute cost while improving image quality. By leveraging
UnClip's text-to-image prior, we enhance diffusion step allocation for greater
efficiency. Our method seamlessly integrates with existing pipelines, scales
with prompt sets, and reduces the environmental and financial burden of
large-scale text-to-image generation. Project page:
https://ddecatur.github.io/hierarchical-diffusion/

</details>


### [89] [Mitosis detection in domain shift scenarios: a Mamba-based approach](https://arxiv.org/abs/2508.21033)
*Gennaro Percannella,Mattia Sarno,Francesco Tortorella,Mario Vento*

Main category: cs.CV

TL;DR: 本文提出了一种基于Mamba模型的有丝分裂检测方法，通过引入VM-UNet架构和染色增强技术，旨在应对领域漂移问题，在MIDOG++数据集上的初步实验表明，方法尚有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 有丝分裂检测对肿瘤评估至关重要，然而现有的机器学习方法在遇到与训练域不同的数据时，性能会显著下降。为了解决领域漂移带来的挑战，作者力图开发更具泛化能力的方法。

Method: 作者提出采用Mamba模型结合VM-UNet架构，并引入染色增强操作以提升模型对不同领域（域）数据的鲁棒性，整体方案用于解决有丝分裂检测在领域漂移情境下的难题。

Result: 方法提交到了MIDOG挑战赛的track 1，在MIDOG++数据集上的初步实验结果显示，当前方案还有很大的改进空间，表现尚不理想。

Conclusion: 虽然该方法在领域泛化的有丝分裂检测方向上进行了探索，但目前效果并不突出，提示有待进一步优化和完善。

Abstract: Mitosis detection in histopathology images plays a key role in tumor
assessment. Although machine learning algorithms could be exploited for aiding
physicians in accurately performing such a task, these algorithms suffer from
significative performance drop when evaluated on images coming from domains
that are different from the training ones. In this work, we propose a
Mamba-based approach for mitosis detection under domain shift, inspired by the
promising performance demonstrated by Mamba in medical imaging segmentation
tasks. Specifically, our approach exploits a VM-UNet architecture for carrying
out the addressed task, as well as stain augmentation operations for further
improving model robustness against domain shift. Our approach has been
submitted to the track 1 of the MItosis DOmain Generalization (MIDOG)
challenge. Preliminary experiments, conducted on the MIDOG++ dataset, show
large room for improvement for the proposed method.

</details>


### [90] [A multi-task neural network for atypical mitosis recognition under domain shift](https://arxiv.org/abs/2508.21035)
*Gennaro Percannella,Mattia Sarno,Francesco Tortorella,Mario Vento*

Main category: cs.CV

TL;DR: 本文提出一种基于多任务学习的方法，以应对病理图像中非典型有丝分裂识别任务中由于领域迁移导致的机器学习模型性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 在病理图像分析中，识别非典型有丝分裂对于准确评估肿瘤侵袭性非常重要，但现有机器学习模型在跨领域应用时性能大幅下降，亟需提升模型的泛化能力。

Method: 作者提出了一种多任务学习方法，通过引入与主要分类任务相关的辅助任务，引导模型关注分类目标本身，减少图像中因领域变化带来的背景干扰。该方法被用于参加MIDOG挑战赛，并在多个数据集上进行了测试。

Result: 初步实验结果显示，该方法在MIDOG 2025 Atypical Training Set、Ami-Br数据集以及MIDOG25挑战的初步测试集上都取得了有前景的性能表现。

Conclusion: 基于多任务学习的模型能够在不同领域的数据上减少性能下降，提升病理图像中非典型有丝分裂识别的鲁棒性和泛化能力。

Abstract: Recognizing atypical mitotic figures in histopathology images allows
physicians to correctly assess tumor aggressiveness. Although machine learning
models could be exploited for automatically performing such a task, under
domain shift these models suffer from significative performance drops. In this
work, an approach based on multi-task learning is proposed for addressing this
problem. By exploiting auxiliary tasks, correlated to the main classification
task, the proposed approach, submitted to the track 2 of the MItosis DOmain
Generalization (MIDOG) challenge, aims to aid the model to focus only on the
object to classify, ignoring the domain varying background of the image. The
proposed approach shows promising performance in a preliminary evaluation
conducted on three distinct datasets, i.e., the MIDOG 2025 Atypical Training
Set, the Ami-Br dataset, as well as the preliminary test set of the MIDOG25
challenge.

</details>


### [91] [FW-GAN: Frequency-Driven Handwriting Synthesis with Wave-Modulated MLP Generator](https://arxiv.org/abs/2508.21040)
*Huynh Tong Dang Khoa,Dang Hoai Nam,Vo Nguyen Le Duy*

Main category: cs.CV

TL;DR: 本文提出了FW-GAN，一种针对手写数据稀缺问题的单样本手写体合成方法，引入频域信息和新颖的生成对抗架构，大幅提升了生成效果。


<details>
  <summary>Details</summary>
Motivation: 手写识别系统受限于标注手写数据的稀缺，现有合成方法在捕捉复杂笔画与风格细节上效果有限，因此亟需更高质量、风格一致的手写数据增强方案。

Method: 提出FW-GAN框架，包括带有相位感知的Wave-MLP生成器（捕捉空间与风格关系）、频域引导判别器（利用高频信息提升真实性检测能力）、以及新颖的频率分布损失（保证合成图像与真实手写在频域上一致）。支持单样本风格迁移。

Result: 在越南语和英语手写体数据集上，FW-GAN能生成高质量、风格一致的手写文本，显著提升数据增强效果。

Conclusion: FW-GAN可有效合成真实、风格一致的手写样本，为低资源手写识别任务提供了有力的数据增强手段。

Abstract: Labeled handwriting data is often scarce, limiting the effectiveness of
recognition systems that require diverse, style-consistent training samples.
Handwriting synthesis offers a promising solution by generating artificial data
to augment training. However, current methods face two major limitations.
First, most are built on conventional convolutional architectures, which
struggle to model long-range dependencies and complex stroke patterns. Second,
they largely ignore the crucial role of frequency information, which is
essential for capturing fine-grained stylistic and structural details in
handwriting. To address these challenges, we propose FW-GAN, a one-shot
handwriting synthesis framework that generates realistic, writer-consistent
text from a single example. Our generator integrates a phase-aware Wave-MLP to
better capture spatial relationships while preserving subtle stylistic cues. We
further introduce a frequency-guided discriminator that leverages
high-frequency components to enhance the authenticity detection of generated
samples. Additionally, we introduce a novel Frequency Distribution Loss that
aligns the frequency characteristics of synthetic and real handwriting, thereby
enhancing visual fidelity. Experiments on Vietnamese and English handwriting
datasets demonstrate that FW-GAN generates high-quality, style-consistent
handwriting, making it a valuable tool for augmenting data in low-resource
handwriting recognition (HTR) pipelines. Official implementation is available
at https://github.com/DAIR-Group/FW-GAN

</details>


### [92] [MMG-Vid: Maximizing Marginal Gains at Segment-level and Token-level for Efficient Video LLMs](https://arxiv.org/abs/2508.21044)
*Junpeng Ma,Qizhe Zhang,Ming Lu,Zhibin Wang,Qiang Zhou,Jun Song,Shanghang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一个高效的视频大模型可用的视觉Token剪枝框架MMG-Vid，在几乎不损失性能的情况下显著减少计算量和加速推理。


<details>
  <summary>Details</summary>
Motivation: 视频大语言模型（VLLMs）在理解视频时需要处理大量视觉token，这会带来巨大的计算压力，限制了其在实际应用中的可用性。为了解决推理效率低下的问题，需要有效地剪枝和减少冗余的视觉token。

Method: 作者提出了MMG-Vid框架，包括两个阶段：首先将视频按帧相似性划分为多个片段，并为每个片段动态分配token预算以最大化每段的边际增益；之后提出时序引导的DPC算法，共同建模帧间独特性和帧内多样性，进一步选取token，从而最大化有限预算下的整体利用率。该框架完全训练无关。

Result: MMG-Vid能在保持原始性能99.5%以上的同时，削减75%的视觉token，将LLaVA-OneVision-7B模型的prefilling阶段加速3.9倍。

Conclusion: MMG-Vid框架在不需训练、性能几乎无损的前提下，可大幅减少视觉token使用和显著提高推理效率，对于真实场景下VLLMs的部署非常有意义。

Abstract: Video Large Language Models (VLLMs) excel in video understanding, but their
excessive visual tokens pose a significant computational challenge for
real-world applications. Current methods aim to enhance inference efficiency by
visual token pruning. However, they do not consider the dynamic characteristics
and temporal dependencies of video frames, as they perceive video understanding
as a multi-frame task. To address these challenges, we propose MMG-Vid, a novel
training-free visual token pruning framework that removes redundancy by
Maximizing Marginal Gains at both segment-level and token-level. Specifically,
we first divide the video into segments based on frame similarity, and then
dynamically allocate the token budget for each segment to maximize the marginal
gain of each segment. Subsequently, we propose a temporal-guided DPC algorithm
that jointly models inter-frame uniqueness and intra-frame diversity, thereby
maximizing the marginal gain of each token. By combining both stages, MMG-Vid
can maximize the utilization of the limited token budget, significantly
improving efficiency while maintaining strong performance. Extensive
experiments demonstrate that MMG-Vid can maintain over 99.5% of the original
performance, while effectively reducing 75% visual tokens and accelerating the
prefilling stage by 3.9x on LLaVA-OneVision-7B. Code will be released soon.

</details>


### [93] [Veritas: Generalizable Deepfake Detection via Pattern-Aware Reasoning](https://arxiv.org/abs/2508.21048)
*Hao Tan,Jun Lan,Zichang Tan,Ajian Liu,Chuanbiao Song,Senyuan Shi,Huijia Zhu,Weiqiang Wang,Jun Wan,Zhen Lei*

Main category: cs.CV

TL;DR: 本文提出了更贴近真实应用场景的新数据集HydraFake，并基于此开发了多模态大语言模型（MLLM）驱动的深度伪造检测器Veritas，在实际未知伪造类型和数据域下效果显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测研究中的数据集与评测方式与工业界实际应用差距较大，现有方法在实际难题下泛化能力弱，难以实用。作者欲通过新的数据集和检测机制弥补实验和应用脱节的问题。

Method: 1. 构建HydraFake数据集，包含多样化伪造技术、真实场景下的数据，并设计严谨的训练和测试协议。2. 提出基于多模态大语言模型（MLLM）的检测器Veritas，引入模式感知推理机制，如“计划”“自省”等，模拟人类取证过程。3. 采用两阶段训练管线，将深假推理能力融入现有MLLM。

Result: 在HydraFake上实验发现，现有检测器虽然可在模型跨域（cross-model）泛化良好，但在未见过的伪造方法与数据域上效果欠佳。Veritas在多个OOD场景下均获得明显性能提升，并能输出透明可信的检测解释。

Conclusion: 新数据集HydraFake极大提高了检测研究与现实需求的贴合度；Veritas方法通过模式感知推理和多模态建模，极大增强了深伪检测在实际复杂场景中的泛化能力与可解释性。

Abstract: Deepfake detection remains a formidable challenge due to the complex and
evolving nature of fake content in real-world scenarios. However, existing
academic benchmarks suffer from severe discrepancies from industrial practice,
typically featuring homogeneous training sources and low-quality testing
images, which hinder the practical deployments of current detectors. To
mitigate this gap, we introduce HydraFake, a dataset that simulates real-world
challenges with hierarchical generalization testing. Specifically, HydraFake
involves diversified deepfake techniques and in-the-wild forgeries, along with
rigorous training and evaluation protocol, covering unseen model architectures,
emerging forgery techniques and novel data domains. Building on this resource,
we propose Veritas, a multi-modal large language model (MLLM) based deepfake
detector. Different from vanilla chain-of-thought (CoT), we introduce
pattern-aware reasoning that involves critical reasoning patterns such as
"planning" and "self-reflection" to emulate human forensic process. We further
propose a two-stage training pipeline to seamlessly internalize such deepfake
reasoning capacities into current MLLMs. Experiments on HydraFake dataset
reveal that although previous detectors show great generalization on
cross-model scenarios, they fall short on unseen forgeries and data domains.
Our Veritas achieves significant gains across different OOD scenarios, and is
capable of delivering transparent and faithful detection outputs.

</details>


### [94] [FakeParts: a New Family of AI-Generated DeepFakes](https://arxiv.org/abs/2508.21052)
*Gaetan Brison,Soobash Daiboo,Samy Aimeur,Awais Hussain Sani,Xi Wang,Gianni Franchi,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: 本论文介绍了FakeParts，一种针对视频中局部区域或时间段进行细微篡改的新型深度伪造（deepfake）技术，并提出了专门用于评估这类局部深度伪造的新基准数据集FakePartsBench。


<details>
  <summary>Details</summary>
Motivation: 现有的深度伪造内容大多聚焦于完整内容的操控，而局部、微妙的视频篡改更具迷惑性且难以检测，现有方法和数据集对此适应性不足，存在明显检测缺口。

Method: 提出了FakeParts，专注于视频中特定空间区域或时间段的小范围篡改，如面部表情变化、物体替换、背景修改等。建立了FakePartsBench大规模数据集，含有超过25,000条带有像素级和帧级标注的局部深度伪造视频，用于全面评测检测方法。

Result: 用户研究显示，FakeParts使人类检测准确率相比传统deepfake降低了30%以上，且主流检测模型在此类局部篡改下也表现大幅下降。

Conclusion: 针对局部深度伪造存在重要检测漏洞。FakePartsBench数据集为提升相关检测技术提供了基础资源，有助于推动更健壮的局部视频篡改检测方法的发展。

Abstract: We introduce FakeParts, a new class of deepfakes characterized by subtle,
localized manipulations to specific spatial regions or temporal segments of
otherwise authentic videos. Unlike fully synthetic content, these partial
manipulations, ranging from altered facial expressions to object substitutions
and background modifications, blend seamlessly with real elements, making them
particularly deceptive and difficult to detect. To address the critical gap in
detection capabilities, we present FakePartsBench, the first large-scale
benchmark dataset specifically designed to capture the full spectrum of partial
deepfakes. Comprising over 25K videos with pixel-level and frame-level
manipulation annotations, our dataset enables comprehensive evaluation of
detection methods. Our user studies demonstrate that FakeParts reduces human
detection accuracy by over 30% compared to traditional deepfakes, with similar
performance degradation observed in state-of-the-art detection models. This
work identifies an urgent vulnerability in current deepfake detection
approaches and provides the necessary resources to develop more robust methods
for partial video manipulations.

</details>


### [95] [Multi-View 3D Point Tracking](https://arxiv.org/abs/2508.21060)
*Frano Rajič,Haofei Xu,Marko Mihajlovic,Siyuan Li,Irem Demir,Emircan Gündoğdu,Lei Ke,Sergey Prokudin,Marc Pollefeys,Siyu Tang*

Main category: cs.CV

TL;DR: 本文提出了首个数据驱动的多视角3D点追踪器，可在动态场景中利用多个摄像头高效追踪任意点，具有鲁棒性强且精准度高的优点。


<details>
  <summary>Details</summary>
Motivation: 现有单目追踪方法存在深度歧义和遮挡问题，传统多摄像头方法又需大量摄像头和繁琐的针对每段视频的优化。因此，迫切需要一种既实用、又无需复杂配置的多摄像头3D点追踪方案。

Method: 本方法在已知摄像头姿态和多视角深度（传感器或估算）的条件下，集成多视角特征为统一点云，并通过k近邻相关性和基于Transformer的更新机制，实现对长距离3D对应关系的可靠估算，且对遮挡具有鲁棒性。方法前馈推理，仅需4个摄像头即可高效追踪。

Result: 在5K合成序列上训练，在真实数据集Panoptic Studio和DexYCB上评估，分别取得了3.1cm和2.0cm的中值轨迹误差。对1-8个视角、不同时间长度（24-150帧）的视频均表现良好，泛化能力强。

Conclusion: 本文的方法有效降低了3D点追踪的门槛，实现少摄像头下的高精度、实时多视角3D追踪，推动了该领域的发展，并为实际场景应用提供了实用工具。

Abstract: We introduce the first data-driven multi-view 3D point tracker, designed to
track arbitrary points in dynamic scenes using multiple camera views. Unlike
existing monocular trackers, which struggle with depth ambiguities and
occlusion, or prior multi-camera methods that require over 20 cameras and
tedious per-sequence optimization, our feed-forward model directly predicts 3D
correspondences using a practical number of cameras (e.g., four), enabling
robust and accurate online tracking. Given known camera poses and either
sensor-based or estimated multi-view depth, our tracker fuses multi-view
features into a unified point cloud and applies k-nearest-neighbors correlation
alongside a transformer-based update to reliably estimate long-range 3D
correspondences, even under occlusion. We train on 5K synthetic multi-view
Kubric sequences and evaluate on two real-world benchmarks: Panoptic Studio and
DexYCB, achieving median trajectory errors of 3.1 cm and 2.0 cm, respectively.
Our method generalizes well to diverse camera setups of 1-8 views with varying
vantage points and video lengths of 24-150 frames. By releasing our tracker
alongside training and evaluation datasets, we aim to set a new standard for
multi-view 3D tracking research and provide a practical tool for real-world
applications. Project page available at https://ethz-vlg.github.io/mvtracker.

</details>


### [96] [OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning](https://arxiv.org/abs/2508.21066)
*Yuan Gong,Xionghui Wang,Jie Wu,Shiyin Wang,Yitong Wang,Xinglong Wu*

Main category: cs.CV

TL;DR: 提出OneReward，一个统一的强化学习框架，实现使用单一奖励模型跨任务提升生成式模型表现，并发布了Seedream 3.0 Fill，实现无须任务特定微调的多任务图像编辑能力，效果优于主流竞品。


<details>
  <summary>Details</summary>
Motivation: 现有跨任务生成模型需要针对每个具体任务进行监督微调，造成泛化性不足与训练低效，难以应对多样的编辑需求和评测标准。因此，作者希望提出一个统一的解决方案，提高模型通用性与训练效率。

Method: 本文提出OneReward框架，采用单一视觉-语言模型（VLM）作为奖励模型，能够根据任务及评测标准判定生成结果优劣。以此为基础，实现统一的多任务强化学习，用于各种基于mask的图像编辑（如图像填充、扩展、对象移除、文本渲染）。并开发Seedream 3.0 Fill，直接在预训练模型上多任务训练，无需针对每个任务单独微调。

Result: 实验结果显示，该方法在多项评测标准上优于包括Ideogram、Adobe Photoshop和FLUX Fill等主流商用及开源系统。

Conclusion: OneReward通过统一奖励机制和多任务强化学习，提升了跨多任务、跨评测标准的生成模型能力，无需任务特定微调；Seedream 3.0 Fill在各种图像编辑任务上表现突出，具备更优泛化性和实用价值。

Abstract: In this paper, we introduce OneReward, a unified reinforcement learning
framework that enhances the model's generative capabilities across multiple
tasks under different evaluation criteria using only \textit{One Reward} model.
By employing a single vision-language model (VLM) as the generative reward
model, which can distinguish the winner and loser for a given task and a given
evaluation criterion, it can be effectively applied to multi-task generation
models, particularly in contexts with varied data and diverse task objectives.
We utilize OneReward for mask-guided image generation, which can be further
divided into several sub-tasks such as image fill, image extend, object
removal, and text rendering, involving a binary mask as the edit area. Although
these domain-specific tasks share same conditioning paradigm, they differ
significantly in underlying data distributions and evaluation metrics. Existing
methods often rely on task-specific supervised fine-tuning (SFT), which limits
generalization and training efficiency. Building on OneReward, we develop
Seedream 3.0 Fill, a mask-guided generation model trained via multi-task
reinforcement learning directly on a pre-trained base model, eliminating the
need for task-specific SFT. Experimental results demonstrate that our unified
edit model consistently outperforms both commercial and open-source
competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across
multiple evaluation dimensions. Code and model are available at:
https://one-reward.github.io

</details>


### [97] [Dress&Dance: Dress up and Dance as You Like It - Technical Preview](https://arxiv.org/abs/2508.21070)
*Jun-Kun Chen,Aayush Bansal,Minh Phuoc Vo,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: Dress&Dance 是一个能生成高质量虚拟试衣视频的新方法。只需一张用户照片，就能让用户在视频中穿上不同衣服，并模仿目标视频的动作，分辨率高达1152x720，每秒24帧。核心创新是CondNet网络，通过多模态输入（文本、图片、视频）的关注机制，显著提升衣物合成和动作拟合质量。方法在多阶段、混合视频和大量图片的构成数据下训练，性能优于现有的开源及商用产品。


<details>
  <summary>Details</summary>
Motivation: 当前虚拟试衣技术要么生成图像分辨率低、动作僵硬的视频，要么还无法同时实现高质量衣物合成与动作拟合。用户也希望体验更灵活、真实的试衣效果。因此急需开发能处理多模态输入、分辨率高、衣物动作拟合度高的虚拟试衣视频生成方法。

Method: 提出 Dress&Dance 框架，只需一张用户照片及参考视频，即可生成穿着目标衣物并做指定动作的高分辨率试衣视频。核心为CondNet条件网络，利用注意力机制整合文本、图片和视频等多模态输入，提升衣物注册和动作真实度。训练采用分阶段、图片和视频混合数据的策略，有效缓解了视频标注数据稀缺的问题。

Result: Dress&Dance 能够以1152x720分辨率和24FPS速度，在5秒钟视频内展现用户穿着不同上衣、下装或连体衣物并随参考视频动作运动。多实验显示其视觉效果、合成质量、动作还原度均优于现有公开及商用系统。

Conclusion: Dress&Dance 创新地实现了高保真、灵活的虚拟试衣视频生成，仅需一张照片和一段参考视频，极大提升了衣物合成和动作还原质量。为线上虚拟试衣和数字服装展示提供更优体验，具备应用推广潜力。

Abstract: We present Dress&Dance, a video diffusion framework that generates high
quality 5-second-long 24 FPS virtual try-on videos at 1152x720 resolution of a
user wearing desired garments while moving in accordance with a given reference
video. Our approach requires a single user image and supports a range of tops,
bottoms, and one-piece garments, as well as simultaneous tops and bottoms
try-on in a single pass. Key to our framework is CondNet, a novel conditioning
network that leverages attention to unify multi-modal inputs (text, images, and
videos), thereby enhancing garment registration and motion fidelity. CondNet is
trained on heterogeneous training data, combining limited video data and a
larger, more readily available image dataset, in a multistage progressive
manner. Dress&Dance outperforms existing open source and commercial solutions
and enables a high quality and flexible try-on experience.

</details>


### [98] [First-Place Solution to NeurIPS 2024 Invisible Watermark Removal Challenge](https://arxiv.org/abs/2508.21072)
*Fahad Shamshad,Tameem Bakr,Yahia Shaaban,Noor Hussein,Karthik Nandakumar,Nils Lukas*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，成功破解了当前内容水印在不同攻击者知识下的鲁棒性，攻破了NeurIPS 2024挑战赛的两个任务。通过结合不同攻击策略，实现了几乎完美的水印去除效果，对图片质量影响极小。


<details>
  <summary>Details</summary>
Motivation: 随着数字媒体广泛传播，水印被广泛应用于验证和版权保护。但现有数字水印技术是否能抵御针对性攻击尚不明确，驱动作者针对当前水印鲁棒性开展实证研究并提出攻防测试方法。

Method: 针对NeurIPS挑战赛的黑盒和beige-box两种场景。对于beige-box，提出基于VAE的自适应规避攻击，并结合测试优化和CIELAB色彩空间恢复以保持图像质量。对于黑盒，先基于空间或频域特征对图片聚类，再对每一组图片应用结合了扩散模型、受控噪声注入和ChatGPT生成语义先验的去水印方案。

Result: 实证评估表明，该方法能在几乎不影响图片质量的情况下，实现平均95.7%的水印去除率，效果显著优于传统方法。

Conclusion: 现有水印技术在面对智能、有针对性的攻击手段时存在较大脆弱性，本文提出的攻击方案挑战了行业现有水印保护能力。作者希望本项工作能促进更强健水印技术的发展。

Abstract: Content watermarking is an important tool for the authentication and
copyright protection of digital media. However, it is unclear whether existing
watermarks are robust against adversarial attacks. We present the winning
solution to the NeurIPS 2024 Erasing the Invisible challenge, which
stress-tests watermark robustness under varying degrees of adversary knowledge.
The challenge consisted of two tracks: a black-box and beige-box track,
depending on whether the adversary knows which watermarking method was used by
the provider. For the beige-box track, we leverage an adaptive VAE-based
evasion attack, with a test-time optimization and color-contrast restoration in
CIELAB space to preserve the image's quality. For the black-box track, we first
cluster images based on their artifacts in the spatial or frequency-domain.
Then, we apply image-to-image diffusion models with controlled noise injection
and semantic priors from ChatGPT-generated captions to each cluster with
optimized parameter settings. Empirical evaluations demonstrate that our method
successfully achieves near-perfect watermark removal (95.7%) with negligible
impact on the residual image's quality. We hope that our attacks inspire the
development of more robust image watermarking methods.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [99] [Social Bias in Multilingual Language Models: A Survey](https://arxiv.org/abs/2508.20201)
*Lance Calvin Lim Gamboa,Yue Feng,Mark Lee*

Main category: cs.CL

TL;DR: 本文综述了多语言预训练模型中的社会偏见问题，分析了现有研究在多语言和非英语环境下的偏见评估与消除方法，总结了研究现状、存在的问题及未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 尽管多语言预训练模型被广泛使用，但其社会偏见问题鲜有在多语言及非英语语境下系统地研究。评估与减缓英语文本中的偏见已较成熟，而新兴的多语言环境面临语言多样性与文化差异的独特挑战。作者旨在梳理这些工作，推动领域对不同语言和文化中的偏见问题更加重视。

Method: 采用系统性文献综述方法，分析了有关多语言偏见评估与缓解的最新研究，重点关注研究在语言多样性、文化意识以及评估指标和消除技术方面的选择。作者还整理了跨语言适配偏见基准时常遇到的问题与解决方法。

Result: 综述发现，目前研究倾向于某些语言，缺乏多语言下的系统性偏见缓解实验，且普遍存在文化敏感度不足等问题。作者归纳了适配多语言与文化差异时的常见挑战与应对措施，并指出现有方法在包容性、交叉文化合理性及与前沿NLP进展对齐方面存在不足。

Conclusion: 作者呼吁未来要加强多语言偏见消除方法的包容性和交叉文化适用性，鼓励采用更全面、多样的评估与实验设计，以适应日新月异的NLP技术发展。

Abstract: Pretrained multilingual models exhibit the same social bias as models
processing English texts. This systematic review analyzes emerging research
that extends bias evaluation and mitigation approaches into multilingual and
non-English contexts. We examine these studies with respect to linguistic
diversity, cultural awareness, and their choice of evaluation metrics and
mitigation techniques. Our survey illuminates gaps in the field's dominant
methodological design choices (e.g., preference for certain languages, scarcity
of multilingual mitigation experiments) while cataloging common issues
encountered and solutions implemented in adapting bias benchmarks across
languages and cultures. Drawing from the implications of our findings, we chart
directions for future research that can reinforce the multilingual bias
literature's inclusivity, cross-cultural appropriateness, and alignment with
state-of-the-art NLP advancements.

</details>


### [100] [Prompting Strategies for Language Model-Based Item Generation in K-12 Education: Bridging the Gap Between Small and Large Language Models](https://arxiv.org/abs/2508.20217)
*Mohammad Amini,Babak Ahmadi,Xiaomeng Xiong,Yilin Zhang,Christopher Qiao*

Main category: cs.CL

TL;DR: 本研究利用语言模型自动生成选项题（MCQ）用于形态学评估，比较不同模型和提示策略，提出一种实用高效的题目开发流程。


<details>
  <summary>Details</summary>
Motivation: 手工编写试题成本高且一致性不足，亟需自动化、可扩展且可靠的题目生成方法，特别是针对K-12语言评估。

Method: 采用两步法：1.比较微调后的中型模型（Gemma, 2B）与大型未调优模型（GPT-3.5, 175B）；2.测试七种结构化提示策略，包括零样、少样、本链思维、角色、依次、及其组合。使用自动化指标和专家多维评分，用GPT-4.1模拟大规模人类评分。

Result: 结构化提示显著提升了Gemma的题目质量。Gemma结合链式思维和顺序设计策略后，生成的试题在契合度和教学适宜性上优于GPT-3.5零样表现。

Conclusion: 结构化提示加高效微调可提升中型模型AIG表现，即使在数据有限情况下也有效。多重评价法提高题目开发的可靠性和一致性，流程适用于大规模K-12语言评估题库开发。

Abstract: This study explores automatic generation (AIG) using language models to
create multiple choice questions (MCQs) for morphological assessment, aiming to
reduce the cost and inconsistency of manual test development. The study used a
two-fold approach. First, we compared a fine-tuned medium model (Gemma, 2B)
with a larger untuned one (GPT-3.5, 175B). Second, we evaluated seven
structured prompting strategies, including zero-shot, few-shot,
chain-of-thought, role-based, sequential, and combinations. Generated items
were assessed using automated metrics and expert scoring across five
dimensions. We also used GPT-4.1, trained on expert-rated samples, to simulate
human scoring at scale. Results show that structured prompting, especially
strategies combining chain-of-thought and sequential design, significantly
improved Gemma's outputs. Gemma generally produced more construct-aligned and
instructionally appropriate items than GPT-3.5's zero-shot responses, with
prompt design playing a key role in mid-size model performance. This study
demonstrates that structured prompting and efficient fine-tuning can enhance
midsized models for AIG under limited data conditions. We highlight the value
of combining automated metrics, expert judgment, and large-model simulation to
ensure alignment with assessment goals. The proposed workflow offers a
practical and scalable way to develop and validate language assessment items
for K-12.

</details>


### [101] [Integrating SystemC TLM into FMI 3.0 Co-Simulations with an Open-Source Approach](https://arxiv.org/abs/2508.20223)
*Andrei Mihai Albu,Giovanni Pollo,Alessio Burrello,Daniele Jahier Pagliari,Cristian Tesconi,Alessandra Neri,Dario Soldi,Fabio Autieri,Sara Vinco*

Main category: cs.CL

TL;DR: 本文提出了一种完全开源的方法，将SystemC TLM模型封装为FMI 3.0 Co-Simulation FMUs，从而实现不同仿真环境间的高效集成。通过开源工具链解决了时间同步与数据交换等技术难题，并通过案例验证了方法的可行性。


<details>
  <summary>Details</summary>
Motivation: 随着汽车等领域的网络物理系统复杂性提升，对高效建模和跨域联合仿真技术的需求日益增长。而SystemC TLM虽适合软硬协同设计，但与其他工程领域模型的互操作性有限，集成难。

Method: 作者提出将SystemC TLM组件封装为FMI 3.0联合仿真功能单元（FMU），并开发了轻量级开源工具链，实现SystemC TLM与FMI标准的集成，解决了时间同步与数据交互等关键技术挑战。

Result: 提出的方法实现了SystemC TLM和FMI模型的无缝对接，在不同领域的仿真环境中进行了代表性案例测试，验证了方法的可行性与高效性。

Conclusion: 开源方法和工具链能够标准化地集成异构模型，显著提升了复杂网络物理系统（如汽车应用）的建模与仿真效率。

Abstract: The growing complexity of cyber-physical systems, particularly in automotive
applications, has increased the demand for efficient modeling and cross-domain
co-simulation techniques. While SystemC Transaction-Level Modeling (TLM)
enables effective hardware/software co-design, its limited interoperability
with models from other engineering domains poses integration challenges. This
paper presents a fully open-source methodology for integrating SystemC TLM
models into Functional Mock-up Interface (FMI)-based co-simulation workflows.
By encapsulating SystemC TLM components as FMI 3.0 Co Simulation Functional
Mock-up Units (FMUs), the proposed approach facilitates seamless, standardized
integration across heterogeneous simulation environments. We introduce a
lightweight open-source toolchain, address key technical challenges such as
time synchronization and data exchange, and demonstrate the feasibility and
effectiveness of the integration through representative case studies.

</details>


### [102] [Can Compact Language Models Search Like Agents? Distillation-Guided Policy Optimization for Preserving Agentic RAG Capabilities](https://arxiv.org/abs/2508.20324)
*Rikuto Kotoge,Mai Nishimura,Jiaxin Ma*

Main category: cs.CL

TL;DR: 本文提出了Distillation-Guided Policy Optimization (DGPO)方法，提升小型语言模型在基于RAG的推理、搜索和规划能力，实现紧凑模型的高效智能体行为。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型因推理能力弱，导致强化学习训练回报稀疏且不稳定，难以实现复杂智能体行为；为解决这一问题，需要新方法提升小模型在有限计算资源下的表现。

Method: 提出DGPO方法，通过教师模型示范进行冷启动初始化，并在策略优化过程中持续接受教师引导，引导小模型学习复杂任务。提出ARC指标，细粒度测评推理、搜索协调及回应生成能力。

Result: 实验证明，在ARC评测上，DGPO显著提升了小型模型的智能体搜索行为表现，部分任务甚至超越了大型教师模型。

Conclusion: DGPO方法使得小模型也能实现高效的智能体型RAG行为，降低了算力需求，适用于资源受限的环境。

Abstract: Reinforcement Learning has emerged as a post-training approach to elicit
agentic RAG behaviors such as search and planning from language models.
However, compact language models (e.g., 0.5B parameters) struggle due to poor
reasoning ability, resulting in sparse rewards and unstable training. To
overcome these difficulties, we propose Distillation-Guided Policy Optimization
(DGPO), which addresses the challenges through cold-start initialization from
teacher demonstrations and continuous teacher guidance during policy
optimization. To systematically evaluate our approach, we introduce Agentic RAG
Capabilities (ARC), a fine-grained metric analyzing reasoning, search
coordination, and response synthesis. Comprehensive experiments demonstrate
that DGPO enables compact models to achieve sophisticated agentic search
behaviors, even outperforming the larger teacher model in some cases. DGPO
makes agentic RAG feasible in computing resource-constrained environments.

</details>


### [103] [GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs](https://arxiv.org/abs/2508.20325)
*Haibo Jin,Ruoxi Chen,Peiyan Zhang,Andy Zhou,Yang Zhang,Haohan Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为GUARD的自动化测试方法，用于将政府AI伦理指南转化为可操作的测试问题，以检测大语言模型对伦理规范的遵循情况。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型应用的普及，其生成有害响应的潜力引发了社会和监管层面的担忧。虽然各国政府发布了AI伦理指南，但其内容多为宏观指导，缺乏可直接操作性，实际开发与测试中难以落实。因此亟需将这些高层次指南转化为具体可测的问题或场景，以便体系化评估大模型合规性。

Method: 作者提出了GUARD方法：通过自动化生成针对政府指南的违规性问题，对大模型进行合规性测试。对于直接违规的回答直接报告不一致性，对于表面上未违规的回答，引入“越狱（jailbreak）”诊断，通过创造诱发不道德或违规回应的情境，检测模型是否存在被绕过安全机制的风险。最终形成详细合规报告。该方法还可拓展到视觉-语言模型。

Result: 作者在七个主流大模型（包括Vicuna-13B、Llama2-7B、GPT-4等）和三份政府指南上进行了实证测试，验证了GUARD方法在合规检测和越狱诊断中的有效性。结果显示，GUARD及其越狱诊断能高效捕捉模型潜在的合规风险。

Conclusion: GUARD有效填补了政府伦理指南与模型合规测试之间的操作性缺口，为大模型合规性检验提供了一种可扩展、系统化的解决方案，对推动安全、可靠的AI应用有重要意义。

Abstract: As Large Language Models become increasingly integral to various domains,
their potential to generate harmful responses has prompted significant societal
and regulatory concerns. In response, governments have issued ethics guidelines
to promote the development of trustworthy AI. However, these guidelines are
typically high-level demands for developers and testers, leaving a gap in
translating them into actionable testing questions to verify LLM compliance.
  To address this challenge, we introduce GUARD (\textbf{G}uideline
\textbf{U}pholding Test through \textbf{A}daptive \textbf{R}ole-play and
Jailbreak \textbf{D}iagnostics), a testing method designed to operationalize
guidelines into specific guideline-violating questions that assess LLM
adherence. To implement this, GUARD uses automated generation of
guideline-violating questions based on government-issued guidelines, thereby
testing whether responses comply with these guidelines. When responses directly
violate guidelines, GUARD reports inconsistencies. Furthermore, for responses
that do not directly violate guidelines, GUARD integrates the concept of
``jailbreaks'' to diagnostics, named GUARD-JD, which creates scenarios that
provoke unethical or guideline-violating responses, effectively identifying
potential scenarios that could bypass built-in safety mechanisms. Our method
finally culminates in a compliance report, delineating the extent of adherence
and highlighting any violations.
  We have empirically validated the effectiveness of GUARD on seven LLMs,
including Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4,
GPT-4o, and Claude-3.7, by testing compliance under three government-issued
guidelines and conducting jailbreak diagnostics. Additionally, GUARD-JD can
transfer jailbreak diagnostics to vision-language models, demonstrating its
usage in promoting reliable LLM-based applications.

</details>


### [104] [Joint Enhancement of Relational Reasoning for Long-Context LLMs](https://arxiv.org/abs/2508.20351)
*Zhirui Chen,Wei Shen,Jiashui Huang,Ling Shao*

Main category: cs.CL

TL;DR: 本文提出了一种名为JERR的新型框架，通过引入图结构推理，提升大语言模型在长文本理解和复杂任务推理中的准确性与透明度。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在处理长文本和复杂推理任务时受限于内存和推理能力，且常常输出缺乏解释性和产生幻觉。

Method: JERR框架包括三个核心模块：1）通过策略性分块提取文本概要，提升信息处理效率；2）构建有向无环图（DAG）消除冗余并保证逻辑一致；3）采用蒙特卡洛树搜索（MCTS）辅助模型高效推理，确保输出的可解释性和准确性。

Result: 实验结果显示，JERR在ROUGE和F1指标及LLM-Rater评测中均优于所有基线方法，表现最优。

Conclusion: JERR显著提高了大语言模型在长文本和复杂推理任务中的表现及输出的可靠性和透明度，为相关领域提供了一种有效解决方案。

Abstract: Despite significant progress, large language models (LLMs) still struggle
with long contexts due to memory limitations and their inability to tackle
complex and long-context tasks. Additionally, LLMs often suffer from a lack of
transparency and are prone to producing hallucinations. To address these
challenges, we propose \textbf{JERR}, a novel framework designed to enhance
long-context comprehension via graph-based reasoning in LLMs. JERR integrates
three key components: synopsis extraction, graph construction, and relational
reasoning. First, synopsis is extracted by chunking text strategically,
allowing the model to summarize and understand information more efficiently.
Second, we build a directed acyclic graph (DAG) to resolve redundancy, ensuring
logical consistency and clarity. Finally, we incorporate Monte Carlo Tree
Search (MCTS) to help the model navigate complex reasoning paths, ensuring more
accurate and interpretable outputs. This framework provides a novel solution
that enables LLMs to handle extended contexts and complex reasoning tasks with
improved reliability and transparency. Experimental results show that JERR
consistently outperforms all baselines on the ROUGE and F1 metrics, achieving
the highest scores on the LLM-Rater evaluation.

</details>


### [105] [Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems](https://arxiv.org/abs/2508.20373)
*Yuyao Wang,Bowen Liu,Jianheng Tang,Nuo Chen,Yuhan Li,Qifan Zhang,Jia Li*

Main category: cs.CL

TL;DR: 本文提出以NP-hard图论问题作为大型语言模型（LLM）长链式思维(Chain-of-Thought, CoT)推理能力后训练的全新且可扩展的合成语料，首次实现了高效、深入的推理泛化效果。新模型Graph-R1-7B在数学、编程、STEM及逻辑领域表现优异，在NP-hard图问题上超越QwQ-32B。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在复杂推理任务上取得进展主要依赖高质量的人工精选数据集进行后训练，如数学与代码数据，这些数据集昂贵且难以扩展。该问题限制了LLM长链式推理能力的发展，因此亟需一种可扩展的、能有效提升LLM推理能力的新型数据源和训练方案。

Method: 作者提出将NP-hard类型的图论问题作为训练数据，通过两阶段后训练框架：(1)基于拒绝采样的NPH图问题进行长链式推理监督微调(Long CoT SFT)，提升推理深度；(2)结合具细粒度奖励机制的强化学习(RL)，提升推理效率。

Result: 新模型Graph-R1-7B在数学、编码、STEM和逻辑领域展现出强泛化能力，在NP-hard图问题上精度和推理效率均优于QwQ-32B。

Conclusion: 以NP-hard图问题为语料能有效大规模提升LLM的长链式推理能力，为LLM后训练开辟新方向，具有广泛应用前景。

Abstract: Reasoning Large Language Models (RLLMs) have recently achieved remarkable
progress on complex reasoning tasks, largely enabled by their long
chain-of-thought (Long CoT) capabilities. However, developing these Long CoT
behaviors relies heavily on post-training with high-quality datasets, which are
typically costly and human-curated (e.g., mathematics and code), leaving
scalable alternatives unexplored. In this work, we introduce NP-hard (NPH)
graph problems as a novel synthetic training corpus, as they inherently require
deep reasoning, extensive exploration, and reflective strategies, which are
core characteristics of Long CoT reasoning. Building on this insight, we
develop a two-stage post-training framework: (i) Long CoT Supervised
Fine-Tuning (SFT) on rejection-sampled NPH graph instances, which substantially
enhances reasoning depth, and (ii) Reinforcement Learning (RL) with a
fine-grained reward design, which sharpens reasoning efficiency. Our flagship
model, Graph-R1-7B, demonstrates strong generalization across mathematics,
coding, STEM, and logic, and surpasses QwQ-32B on NPH graph problems in both
accuracy and reasoning efficiency. These results position NPH graph problems as
an effective and scalable resource for advancing Long CoT reasoning in LLMs,
opening a new frontier for LLM post-training. Our implementation is available
at https://github.com/Graph-Reasoner/Graph-R1, with models and datasets hosted
in our Hugging Face collection HKUST-DSAIL/Graph-R1.

</details>


### [106] [CAPE: Context-Aware Personality Evaluation Framework for Large Language Models](https://arxiv.org/abs/2508.20385)
*Jivnesh Sandhan,Fei Cheng,Tushar Sandhan,Yugo Murawaki*

Main category: cs.CL

TL;DR: 本论文提出了一种新的针对大模型（LLM）性格特质测试框架——CAPE（Context-Aware Personality Evaluation），强调在对话上下文下测试模型的行为一致性与个性变化。通过对7个主流模型的大量实验证明，上下文不仅提升了模型的一致性，也会引发个性漂移，各模型对上下文敏感度不同。相关代码和数据集已开源。


<details>
  <summary>Details</summary>
Motivation: 现有对大模型人格测试的方法忽视了真实应用中对话历史的影响，仅采用孤立无上下文的“Disney World”测试，这与日常使用场景严重脱节。论文旨在填补该空白，提出一种考虑对话上下文影响的更真实的测试框架。

Method: 提出CAPE框架，将过往对话内容纳入人格测试流程。为此设计了新的一致性度量指标，系统分析上下文对模型回答一致性及个性的影响，并选取7种主流模型（包括GPT-4-Turbo、Gemini-1.5-Flash、Llama-8B等）进行实验评估。

Result: 实验发现，加入对话历史能增强模型回答一致性，但部分模型（如GPT-3.5-Turbo、GPT-4-Turbo）会发生显著个性漂移。不同模型对题目顺序和上下文的响应差别较大，GPT系模型有内在个性，但Gemini和Llama则更依赖之前的对话。此外，将此框架用于角色扮演代理时，也能提升一致性，更贴近真人判断。

Conclusion: CAPE框架让大模型人格测试更贴合实际应用，有助于理解模型行为随上下文变化的机制。论文工具和数据公开，有助于后续相关研究和应用发展。

Abstract: Psychometric tests, traditionally used to assess humans, are now being
applied to Large Language Models (LLMs) to evaluate their behavioral traits.
However, existing studies follow a context-free approach, answering each
question in isolation to avoid contextual influence. We term this the Disney
World test, an artificial setting that ignores real-world applications, where
conversational history shapes responses. To bridge this gap, we propose the
first Context-Aware Personality Evaluation (CAPE) framework for LLMs,
incorporating prior conversational interactions. To thoroughly analyze the
influence of context, we introduce novel metrics to quantify the consistency of
LLM responses, a fundamental trait in human behavior.
  Our exhaustive experiments on 7 LLMs reveal that conversational history
enhances response consistency via in-context learning but also induces
personality shifts, with GPT-3.5-Turbo and GPT-4-Turbo exhibiting extreme
deviations. While GPT models are robust to question ordering, Gemini-1.5-Flash
and Llama-8B display significant sensitivity. Moreover, GPT models response
stem from their intrinsic personality traits as well as prior interactions,
whereas Gemini-1.5-Flash and Llama--8B heavily depend on prior interactions.
Finally, applying our framework to Role Playing Agents (RPAs) shows
context-dependent personality shifts improve response consistency and better
align with human judgments. Our code and datasets are publicly available at:
https://github.com/jivnesh/CAPE

</details>


### [107] [Measuring Reasoning Utility in LLMs via Conditional Entropy Reduction](https://arxiv.org/abs/2508.20395)
*Xu Guo*

Main category: cs.CL

TL;DR: 本文通过在MATH数据集上的实验，揭示了推理步骤中模型不确定性（条件熵）与答案正确性的关联。发现随着推理步骤条件熵下降时，模型往往给出正确答案，反之则错误。推理链越长反而更可能出错，为后续高效推理路径设计提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型依赖生成中间推理步骤以提高准确率，但尚缺乏对推理步骤“有效性”对最终正确率贡献的细致研究。作者希望揭示什么样的推理路径能够带来更高的正确率，以及能否在生成过程中预测步骤是否有效，从而提高推理效率。

Method: 作者在MATH数据集上，利用Qwen2.5-32B和GPT-4o生成推理链，并用Qwen3-8B来量化这些推理链对最终准确率的贡献。通过逐步扩展上下文，在每个推理步骤上测量答案片段的条件熵（即对答案词汇的预期负对数似然），分析熵随推理进展的变化规律。

Result: 结果发现：当推理链进行过程中条件熵逐步减小时，模型通常能得到正确答案；若条件熵持平或上升，则多给出错误答案。同时，错误推理路径显著更长，说明“多推理”并不一定带来更好结果。

Conclusion: 该工作揭示了衡量推理路径有效性的关键指标——条件熵，并证明其与模型最终输出准确性高度相关。这为未来开发能主动检测并终止无效推理的高效智能推理算法提供新的理论依据。

Abstract: Recent advancements in large language models (LLMs) often rely on generating
intermediate reasoning steps to enhance accuracy. However, little work has
examined how reasoning utility contributes to the final answer's correctness.
Due to the stochastic nature of autoregressive generation, generating more
context does not guarantee increased confidence in the answer. If we could
predict, during generation, whether a reasoning step will be useful, we could
stop early or prune ineffective steps, avoiding distractions in the final
decision.
  We present an oracle study on MATH dataset, using Qwen2.5-32B and GPT-4o to
generate reasoning chains, and then employing a separate model (Qwen3-8B) to
quantify the utility of these chains for final accuracy. Specifically, we
measure the model's uncertainty on the answer span Y at each reasoning step
using conditional entropy (expected negative log-likelihood over the
vocabulary) with context expanding step by step. Our results show a clear
pattern: conditional entropy that decreases over steps is strongly associated
with correct answers, whereas flat or increasing entropy often results in wrong
answers. We also corroborate that incorrect reasoning paths tend to be longer
than correct ones, suggesting that longer reasoning does not necessarily yield
better outcomes. These findings serve as a foundation to inspire future work on
designing efficient reasoning pipelines that detect and avoid unproductive
reasoning early.

</details>


### [108] [UI-Bench: A Benchmark for Evaluating Design Capabilities of AI Text-to-App Tools](https://arxiv.org/abs/2508.20410)
*Sam Jung,Agustin Garcinuno,Spencer Mateega*

Main category: cs.CL

TL;DR: 本论文提出了UI-Bench，这是首个用于比较AI文本生成应用工具视觉效果的大规模基准，通过专家评审对比，形成公开、可复现的标准和排行榜。


<details>
  <summary>Details</summary>
Motivation: 当前AI文本生成应用和网站的工具承诺能够在短时间内生成高质量产品，但缺乏公开且严格的基准来验证这些主张。建立可靠的评测体系对于推动AI在网页设计领域的进步尤为重要。

Method: UI-Bench基准覆盖10个AI文本生成应用工具，使用30个不同的提示词，生成了300个网站，并获得了4000多份专家两两对比评判。评测使用TrueSkill模型计算并校准系统排名的置信区间。

Result: 通过专家评审和TrueSkill推断，UI-Bench对当前主流AI文本生成工具进行了可量化排名，并对结果的不确定性进行了置信区间校准。此外，研究团队对评测流程和工具集进行了开源。

Conclusion: UI-Bench建立了AI驱动网站设计的公开、可复现的评测标准，有助于推动该领域的研究和产品进步。作者开放了完整的提示词集、评测框架和排行榜，未来将发布全部生成结果。

Abstract: AI text-to-app tools promise high quality applications and websites in
minutes, yet no public benchmark rigorously verifies those claims. We introduce
UI-Bench, the first large-scale benchmark that evaluates visual excellence
across competing AI text-to-app tools through expert pairwise comparison.
Spanning 10 tools, 30 prompts, 300 generated sites, and \textit{4000+} expert
judgments, UI-Bench ranks systems with a TrueSkill-derived model that yields
calibrated confidence intervals. UI-Bench establishes a reproducible standard
for advancing AI-driven web design. We release (i) the complete prompt set,
(ii) an open-source evaluation framework, and (iii) a public leaderboard. The
generated sites rated by participants will be released soon. View the UI-Bench
leaderboard at https://uibench.ai/leaderboard.

</details>


### [109] [DentalBench: Benchmarking and Advancing LLMs Capability for Bilingual Dentistry Understanding](https://arxiv.org/abs/2508.20416)
*Hengchuan Zhu,Yihuan Xu,Yichen Li,Zijie Meng,Zuozhu Liu*

Main category: cs.CL

TL;DR: 本文提出了DentalBench，这是首个用于评估和提升大语言模型（LLM）在牙科领域能力的双语综合评测基准。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在医学领域已有较好表现，但针对牙科等需要深度专业知识的细分领域，其能力尚未被充分探究，主要因缺乏专门的评测资源。

Method: 构建了DentalBench，包括英文-中文问答数据集DentalQA（涵盖4大任务和16个牙科子领域，共36597个问题）和规模达3.37亿词的大型高质量牙科文本库DentalCorpus，支持SFT和RAG。用14种LLM进行了评测，并以Qwen-2.5-3B为例做了领域适应实验。

Result: 评估揭示了不同模型在多任务、多语言下的显著性能差异。领域适应显著提升了模型在知识密集型和术语相关任务上的表现。

Conclusion: 该研究强调了领域专用评测基准对医疗健康等专业场景下LLM能力提升和可信应用开发的重要性。

Abstract: Recent advances in large language models (LLMs) and medical LLMs (Med-LLMs)
have demonstrated strong performance on general medical benchmarks. However,
their capabilities in specialized medical fields, such as dentistry which
require deeper domain-specific knowledge, remain underexplored due to the lack
of targeted evaluation resources. In this paper, we introduce DentalBench, the
first comprehensive bilingual benchmark designed to evaluate and advance LLMs
in the dental domain. DentalBench consists of two main components: DentalQA, an
English-Chinese question-answering (QA) benchmark with 36,597 questions
spanning 4 tasks and 16 dental subfields; and DentalCorpus, a large-scale,
high-quality corpus with 337.35 million tokens curated for dental domain
adaptation, supporting both supervised fine-tuning (SFT) and
retrieval-augmented generation (RAG). We evaluate 14 LLMs, covering
proprietary, open-source, and medical-specific models, and reveal significant
performance gaps across task types and languages. Further experiments with
Qwen-2.5-3B demonstrate that domain adaptation substantially improves model
performance, particularly on knowledge-intensive and terminology-focused tasks,
and highlight the importance of domain-specific benchmarks for developing
trustworthy and effective LLMs tailored to healthcare applications.

</details>


### [110] [KG-CQR: Leveraging Structured Relation Representations in Knowledge Graphs for Contextual Query Retrieval](https://arxiv.org/abs/2508.20417)
*Chi Minh Bui,Ngoc Mai Thieu,Van Vinh Nguyen,Json J. Jung,Khac-Hoai Nam Bui*

Main category: cs.CL

TL;DR: 本文提出了KG-CQR框架，通过结合知识图谱和大语言模型，提升检索增强生成（RAG）系统中检索环节的效果。


<details>
  <summary>Details</summary>
Motivation: 目前RAG系统在处理复杂查询时，通常存在上下文信息丢失的问题，尤其是在丰富和理解查询语境方面不足。传统方法多关注于语料库级别的上下文，忽视了针对查询本身的结构化知识增强。

Method: KG-CQR利用语料中心的知识图谱对复杂查询进行上下文增强。其流程包括子图提取、补全和上下文生成等模块，可以无缝对接不同大小的大语言模型，无需额外训练。

Result: 在RAGBench和MultiHop-RAG数据集上，KG-CQR相较于强基线模型，平均mAP提升4-6%，Recall@25提升2-3%。多跳问答等高难度RAG任务也有明显突破。

Conclusion: KG-CQR能够有效增强RAG系统的检索性能，展现出良好的泛化性和可扩展性，在面向复杂查询的检索与生成任务中表现卓越。

Abstract: The integration of knowledge graphs (KGs) with large language models (LLMs)
offers significant potential to improve the retrieval phase of
retrieval-augmented generation (RAG) systems. In this study, we propose KG-CQR,
a novel framework for Contextual Query Retrieval (CQR) that enhances the
retrieval phase by enriching the contextual representation of complex input
queries using a corpus-centric KG. Unlike existing methods that primarily
address corpus-level context loss, KG-CQR focuses on query enrichment through
structured relation representations, extracting and completing relevant KG
subgraphs to generate semantically rich query contexts. Comprising subgraph
extraction, completion, and contextual generation modules, KG-CQR operates as a
model-agnostic pipeline, ensuring scalability across LLMs of varying sizes
without additional training. Experimental results on RAGBench and MultiHop-RAG
datasets demonstrate KG-CQR's superior performance, achieving a 4-6%
improvement in mAP and a 2-3% improvement in Recall@25 over strong baseline
models. Furthermore, evaluations on challenging RAG tasks such as multi-hop
question answering show that, by incorporating KG-CQR, the performance
consistently outperforms the existing baseline in terms of retrieval
effectiveness

</details>


### [111] [CAMB: A comprehensive industrial LLM benchmark on civil aviation maintenance](https://arxiv.org/abs/2508.20420)
*Feng Zhang,Chengjie Pang,Yuehan Zhang,Chenyu Luo*

Main category: cs.CL

TL;DR: 本文提出了一个专为民航维护设计的大型语言模型评测基准，用于衡量和提升模型在该领域的专业知识和复杂推理能力，并开源基准和代码供社区使用。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型评估工具大多集中在数学、编程推理等通用任务，缺乏针对民航维护等高门槛垂直领域的标准化测试，导致模型在该类场景的局限性无法被系统发现和优化。

Method: 作者设计了一个民航维护专用的工业级评测基准，内容涵盖维护程序和故障排查等领域知识与复杂推理要求，并利用该基准对主流嵌入模型和LLMs的RAG系统进行了实验性评估分析。

Result: 实验表明，该基准能够有效发现模型在民航维护场景下的能力差距和短板，具有实际指导意义。评测结果为后续针对性改进（如微调、RAG优化、提示工程）提供方向。

Conclusion: 该工作弥补了LLM评估在民航维护等垂直行业的空白，为推动领域智能化应用提供了基础工具。开源基准和代码有助于整个社区持续迭代和深入研究。

Abstract: Civil aviation maintenance is a domain characterized by stringent industry
standards. Within this field, maintenance procedures and troubleshooting
represent critical, knowledge-intensive tasks that require sophisticated
reasoning. To address the lack of specialized evaluation tools for large
language models (LLMs) in this vertical, we propose and develop an
industrial-grade benchmark specifically designed for civil aviation
maintenance. This benchmark serves a dual purpose: It provides a standardized
tool to measure LLM capabilities within civil aviation maintenance, identifying
specific gaps in domain knowledge and complex reasoning. By pinpointing these
deficiencies, the benchmark establishes a foundation for targeted improvement
efforts (e.g., domain-specific fine-tuning, RAG optimization, or specialized
prompt engineering), ultimately facilitating progress toward more intelligent
solutions within civil aviation maintenance. Our work addresses a significant
gap in the current LLM evaluation, which primarily focuses on mathematical and
coding reasoning tasks. In addition, given that Retrieval-Augmented Generation
(RAG) systems are currently the dominant solutions in practical applications ,
we leverage this benchmark to evaluate existing well-known vector embedding
models and LLMs for civil aviation maintenance scenarios. Through experimental
exploration and analysis, we demonstrate the effectiveness of our benchmark in
assessing model performance within this domain, and we open-source this
evaluation benchmark and code to foster further research and
development:https://github.com/CamBenchmark/cambenchmark

</details>


### [112] [Searching the Title of Practical Work of the Informatics Engineering Bachelor Program with the Case Base Reasoning Method](https://arxiv.org/abs/2508.20442)
*Agung Sukrisna Jaya,Osvari Arsalan,Danny Matthew Saputra*

Main category: cs.CL

TL;DR: 该论文提出了一种基于案例推理（CBR）的实习课题检索系统，结合TF-IDF和余弦相似度来实现课题标题的匹配与推荐。


<details>
  <summary>Details</summary>
Motivation: 随着实习课题数量的增加，如何高效、准确地查找到与需求最匹配的课题标题成为一项重要挑战。该研究旨在利用已有经验案例，实现实习课题的高效检索与推荐，提高选题的合理性和便利性。

Method: 本研究采用CBR（案例推理）方法，利用历史实习课题标题的相似性进行匹配。文本向量化采用TF-IDF算法，相似度计算采用余弦相似度。系统支持输入标题或关键词两种检索方式。通过检索返回匹配的实习课题标题及其相似度分值。实验部分基于705个实习课题标题，分两步进行：第一步以已有标题检索，第二步对标题顺序随机打乱后检索进行对比分析。

Result: 两轮实验的检索结果在找到的课题数量上保持一致，且第二阶段的平均相似度得分最高。

Conclusion: 结合CBR、TF-IDF及余弦相似度的方法能有效提升实习课题检索的准确性与相关度，系统能为用户提供高质量的课题推荐。

Abstract: Case Base Reasoning (CBR) is a case solving technique based on experience in
cases that have occurred before with the highest similarity. CBR is used to
search for practical work titles. TF-IDF is applied to process the
vectorization of each practical work title word and Cosine Similarity for the
calculation of similarity values. This system can search either in the form of
titles or keywords. The output of the system is the title of practical work and
the match value of each title. Based on the test results using 705 practical
work titles, testing was carried out with five titles and carried out in two
stages. The first stage searches with existing titles and the second stage
randomizes the title from the first stage. And the results obtained in the
second stage are the same number of titles found and the highest average match
score.

</details>


### [113] [MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers](https://arxiv.org/abs/2508.20453)
*Zhenting Wang,Qi Chang,Hemani Patel,Shashank Biju,Cheng-En Wu,Quan Liu,Aolin Ding,Alireza Rezazadeh,Ankit Shah,Yujia Bao,Eugene Siow*

Main category: cs.CL

TL;DR: 本文提出了MCP-Bench，这是一个用于评测大语言模型在多步骤、需要工具协作和复杂推理任务中的能力的新基准。它促进了真实情境下工具查找、参数控制、规划及跨领域任务执行能力的评估。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型基准往往只关注单一工具的API调用或浅层次的简单任务，缺乏对复杂、多工具协作和跨领域任务的真实评估。因此，开发能够更好模拟真实应用场景、要求多步骤推理与工具协调的新型评测基准尤为重要。

Method: 作者基于Model Context Protocol (MCP)构建了MCP-Bench，将LLM与28个活跃的MCP服务器连接起来，涵盖250种跨领域工具。设计的任务不直接给出工具名称，而要求模型在模糊指令下自主检索和规划使用多个工具执行复杂的多步骤任务。同时，提出了多维度的评价框架，包括工具层级理解与使用、执行轨迹规划和任务完成度。

Result: 作者在20个先进的大语言模型上进行了实验。实验表明，当前的LLM在MCP-Bench这个多步骤、工具需求高的环境下依然面临显著挑战，说明现有模型难以很好地胜任复杂协作和推理场景。

Conclusion: MCP-Bench为评估LLM在实际复杂任务中的工具协调、规划、推理等核心能力提供了新的有效基准，有助于未来推动相关技术进步。

Abstract: We introduce MCP-Bench, a benchmark for evaluating large language models
(LLMs) on realistic, multi-step tasks that demand tool use, cross-tool
coordination, precise parameter control, and planning/reasoning for solving
tasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28
representative live MCP servers spanning 250 tools across domains such as
finance, traveling, scientific computing, and academic search. Unlike prior
API-based benchmarks, each MCP server provides a set of complementary tools
designed to work together, enabling the construction of authentic, multi-step
tasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability
to retrieve relevant tools from fuzzy instructions without explicit tool names,
plan multi-hop execution trajectories for complex objectives, ground responses
in intermediate tool outputs, and orchestrate cross-domain workflows -
capabilities not adequately evaluated by existing benchmarks that rely on
explicit tool specifications, shallow few-step workflows, and isolated domain
operations. We propose a multi-faceted evaluation framework covering tool-level
schema understanding and usage, trajectory-level planning, and task completion.
Experiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code
and data: https://github.com/Accenture/mcp-bench.

</details>


### [114] [Prediction of mortality and resource utilization in critical care: a deep learning approach using multimodal electronic health records with natural language processing techniques](https://arxiv.org/abs/2508.20460)
*Yucheng Ruan,Xiang Lan,Daniel J. Tan,Hairil Rizal Abdullah,Mengling Feng*

Main category: cs.CL

TL;DR: 本研究提出并验证了一种结合自然语言处理和深度学习的新框架，可融合结构化和非结构化EHR数据，有效提升了ICU环境下死亡率和资源利用预测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统方法侧重于结构化电子健康记录（EHR），忽略了自由文本中的临床信息，且未充分挖掘结构化数据中的文本信息。因此，需要一种能整合多模态信息的新方法，以提升预测效果。

Method: 本研究开发了一种基于深度学习和自然语言处理的多模态EHR集成模型。模型结合医疗提示、自由文本和预训练句子编码器。通过在两个真实EHR数据集上，模型完成了死亡率预测、住院时间预测和手术时长估计，并与主流方法做对比，进行了组分消融实验及结构化数据破坏下的鲁棒性测试。

Result: 模型在两大数据集和三项临床任务上均显著优于现有最佳方法。例如，死亡率预测BACC/AUROC提升1.6%/0.8%，住院时长RMSE/MAE提升0.5%/2.2%，手术时长RMSE/MAE提升10.9%/11.0%。即使在高比例结构化数据损坏下依然保持领先。

Conclusion: 该框架为重症监护资源预测和病人死亡率评估提供了一种有效、准确的深度学习工具。结合提示学习和Transformer有效挖掘了多模态EHR信息，并具备很强的数据鲁棒性。

Abstract: Background Predicting mortality and resource utilization from electronic
health records (EHRs) is challenging yet crucial for optimizing patient
outcomes and managing costs in intensive care unit (ICU). Existing approaches
predominantly focus on structured EHRs, often ignoring the valuable clinical
insights in free-text notes. Additionally, the potential of textual information
within structured data is not fully leveraged. This study aimed to introduce
and assess a deep learning framework using natural language processing
techniques that integrates multimodal EHRs to predict mortality and resource
utilization in critical care settings. Methods Utilizing two real-world EHR
datasets, we developed and evaluated our model on three clinical tasks with
leading existing methods. We also performed an ablation study on three key
components in our framework: medical prompts, free-texts, and pre-trained
sentence encoder. Furthermore, we assessed the model's robustness against the
corruption in structured EHRs. Results Our experiments on two real-world
datasets across three clinical tasks showed that our proposed model improved
performance metrics by 1.6\%/0.8\% on BACC/AUROC for mortality prediction,
0.5%/2.2% on RMSE/MAE for LOS prediction, 10.9%/11.0% on RMSE/MAE for surgical
duration estimation compared to the best existing methods. It consistently
demonstrated superior performance compared to other baselines across three
tasks at different corruption rates. Conclusions The proposed framework is an
effective and accurate deep learning approach for predicting mortality and
resource utilization in critical care. The study also highlights the success of
using prompt learning with a transformer encoder in analyzing multimodal EHRs.
Importantly, the model showed strong resilience to data corruption within
structured data, especially at high corruption levels.

</details>


### [115] [ConspirED: A Dataset for Cognitive Traits of Conspiracy Theories and Large Language Model Safety](https://arxiv.org/abs/2508.20468)
*Luke Bates,Max Glockner,Preslav Nakov,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本论文提出了ConspirED数据集，用于研究阴谋论内容中的认知特征，并探索大模型在面对阴谋论输入时的鲁棒性及其潜在风险。


<details>
  <summary>Details</summary>
Motivation: 阴谋论削弱了公众对科学和机构的信任，并且具有强大的自我进化和反驳机制，难以被纠正。随着AI生成虚假信息能力提升，识别和干预阴谋论内容的重要性大大增加。

Method: 作者构建了ConspirED数据集，收集来自网络的阴谋论文章片段，并依据CONSPIR认知框架进行认知特征注释。基于该数据集，开发用于识别和判断文本中主导阴谋特征的计算模型，同时评估大语言模型和推理模型在阴谋输入下的表现。

Result: 实验证明，无论是识别模型还是大语言/推理模型，在阴谋论输入下都容易被其推理模式误导，甚至在可以识别或抵御经过事实核查的虚假信息时也会模仿阴谋论的推理结构输出。

Conclusion: 当前模型在应对阴谋论内容时仍显脆弱，需要进一步研究和开发更具鲁棒性的干预与辨识手段，防止AI被阴谋内容误导。

Abstract: Conspiracy theories erode public trust in science and institutions while
resisting debunking by evolving and absorbing counter-evidence. As AI-generated
misinformation becomes increasingly sophisticated, understanding rhetorical
patterns in conspiratorial content is important for developing interventions
such as targeted prebunking and assessing AI vulnerabilities. We introduce
ConspirED (CONSPIR Evaluation Dataset), which captures the cognitive traits of
conspiratorial ideation in multi-sentence excerpts (80--120 words) from online
conspiracy articles, annotated using the CONSPIR cognitive framework
(Lewandowsky and Cook, 2020). ConspirED is the first dataset of conspiratorial
content annotated for general cognitive traits. Using ConspirED, we (i) develop
computational models that identify conspiratorial traits and determine dominant
traits in text excerpts, and (ii) evaluate large language/reasoning model
(LLM/LRM) robustness to conspiratorial inputs. We find that both are misaligned
by conspiratorial content, producing output that mirrors input reasoning
patterns, even when successfully deflecting comparable fact-checked
misinformation.

</details>


### [116] [Languages Still Left Behind: Toward a Better Multilingual Machine Translation Benchmark](https://arxiv.org/abs/2508.20511)
*Chihiro Taguchi,Seng Mai,Keita Kurabe,Yusuke Sakai,Georgina Agyei,Soudabeh Eslami,David Chiang*

Main category: cs.CL

TL;DR: 这篇文章对现有的多语言机器翻译评测集FLORES+提出了质疑，发现其多语言和评价体系存在显著缺陷。通过人工评估和实验揭示出数据集的局限性，并建议未来应采用更通用、中立的文本构建多语言翻译基准。


<details>
  <summary>Details</summary>
Motivation: 虽然FLORES+作为多语言机器翻译基准被广泛应用，但作者认为其数据来源和评价体系并未真正代表多语言和现实世界的需求，因此希望揭示其中的问题，推动更有效的评测体系发展。

Method: 作者选取了四种语言（Asante Twi、日语、景颇语和南阿塞拜疆语），对FLORES+中的翻译质量进行了人工评估，并设计了简单的评测启发式（如复制专名）以测试当前BLEU评分体系的漏洞，同时比较了在高质量数据上训练的模型在FLORES+和自建测试集上的表现。

Result: 人工评估显示，FLORES+中的许多翻译未达到所称的90%质量标准，且文本内容常常带有明显的英语世界文化偏向。同时，仅通过简单操作即可获得较高BLEU分数，暴露了指标漏洞。高质量模型在FLORES+数据上表现不佳，但在自建语料上表现优异，显示当前评测基准难以反映真实翻译能力。

Conclusion: 作者建议未来多语言机器翻译基准应采用领域通用、文化中立的素材，并减少依赖专名，以更好反映实际翻译挑战和能力。

Abstract: Multilingual machine translation (MT) benchmarks play a central role in
evaluating the capabilities of modern MT systems. Among them, the FLORES+
benchmark is widely used, offering English-to-many translation data for over
200 languages, curated with strict quality control protocols. However, we study
data in four languages (Asante Twi, Japanese, Jinghpaw, and South Azerbaijani)
and uncover critical shortcomings in the benchmark's suitability for truly
multilingual evaluation. Human assessments reveal that many translations fall
below the claimed 90% quality standard, and the annotators report that source
sentences are often too domain-specific and culturally biased toward the
English-speaking world. We further demonstrate that simple heuristics, such as
copying named entities, can yield non-trivial BLEU scores, suggesting
vulnerabilities in the evaluation protocol. Notably, we show that MT models
trained on high-quality, naturalistic data perform poorly on FLORES+ while
achieving significant gains on our domain-relevant evaluation set. Based on
these findings, we advocate for multilingual MT benchmarks that use
domain-general and culturally neutral source texts rely less on named entities,
in order to better reflect real-world translation challenges.

</details>


### [117] [SciTopic: Enhancing Topic Discovery in Scientific Literature through Advanced LLM](https://arxiv.org/abs/2508.20514)
*Pengjiang Li,Zaitian Wang,Xinhao Zhang,Ran Zhang,Lu Jiang,Pengfei Wang,Yuanchun Zhou*

Main category: cs.CL

TL;DR: 该论文提出了一种利用大语言模型（LLM）增强的科学主题发现方法——SciTopic，能更有效识别科学文献的研究主题，并在多个真实数据集上超过了当前最好的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的科学文献主题发现主要依赖词向量嵌入，难以全面理解高维复杂文本关系，导致主题辨识有限。考虑到大语言模型在文本理解方面的突出表现，作者希望利用LLM来提升主题发现能力，帮助科研人员更快掌握最新研究趋势。

Method: 作者提出了名为SciTopic的方法，具体包括：（1）搭建一个文本编码器，综合提取文献元数据、题目和摘要信息；（2）设计一个空间优化模块，把基于熵的采样和LLM指导的三元组任务结合，提高对主题相关性和上下文细节的判别力；（3）借助LLM指导微调文本编码器，通过优化三元组的对比损失，使编码器区分不同主题的文本能力更强。

Result: 在三个大型真实科学文献数据集上的大量实验证明，SciTopic显著超过当前最优的主题发现方法，无论在主题识别的准确性还是深度和速度方面均表现优异。

Conclusion: SciTopic能够有效增强科学文献主题的识别和区分，为科研人员提供更高质量、更便捷的主题发现工具，有助于推动前沿科学发现和学科演进。

Abstract: Topic discovery in scientific literature provides valuable insights for
researchers to identify emerging trends and explore new avenues for
investigation, facilitating easier scientific information retrieval. Many
machine learning methods, particularly deep embedding techniques, have been
applied to discover research topics. However, most existing topic discovery
methods rely on word embedding to capture the semantics and lack a
comprehensive understanding of scientific publications, struggling with
complex, high-dimensional text relationships. Inspired by the exceptional
comprehension of textual information by large language models (LLMs), we
propose an advanced topic discovery method enhanced by LLMs to improve
scientific topic identification, namely SciTopic. Specifically, we first build
a textual encoder to capture the content from scientific publications,
including metadata, title, and abstract. Next, we construct a space
optimization module that integrates entropy-based sampling and triplet tasks
guided by LLMs, enhancing the focus on thematic relevance and contextual
intricacies between ambiguous instances. Then, we propose to fine-tune the
textual encoder based on the guidance from the LLMs by optimizing the
contrastive loss of the triplets, forcing the text encoder to better
discriminate instances of different topics. Finally, extensive experiments
conducted on three real-world datasets of scientific publications demonstrate
that SciTopic outperforms the state-of-the-art (SOTA) scientific topic
discovery methods, enabling researchers to gain deeper and faster insights.

</details>


### [118] [Overview of BioASQ 2024: The twelfth BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering](https://arxiv.org/abs/2508.20532)
*Anastasios Nentidis,Georgios Katsimpras,Anastasia Krithara,Salvador Lima-López,Eulàlia Farré-Maduell,Martin Krallinger,Natalia Loukachevitch,Vera Davydova,Elena Tutubalina,Georgios Paliouras*

Main category: cs.CL

TL;DR: 本文回顾了2024年CLEF BioASQ第十二届挑战赛的总体情况，介绍了本届比赛的主要任务和参与情况，体现了生物医学语义检索和问答领域持续进步。


<details>
  <summary>Details</summary>
Motivation: BioASQ旨在推动生物医学领域大规模语义索引和问答系统的技术进步，促进社区交流与系统性能提升。每年通过设计新任务迎合实际应用需求，测试最新方法。

Method: 本届BioASQ设有两个经典任务（task b和Synergy）与两个新任务：MultiCardioNER（跨语种心脏病学临床实体识别）和BIONNE（英俄双语嵌套实体识别任务）。共有37支队伍参与，700余份系统提交。

Result: 大多数参赛系统在各项任务取得了有竞争力的表现，显示出当前方法在生物医学文本处理等领域的先进性和有效性。

Conclusion: 本届BioASQ成功吸引了众多团队参与，推动了生物医学信息检索、问答和实体识别技术的发展，体现出该领域持续的研究活力和技术进步。

Abstract: This is an overview of the twelfth edition of the BioASQ challenge in the
context of the Conference and Labs of the Evaluation Forum (CLEF) 2024. BioASQ
is a series of international challenges promoting advances in large-scale
biomedical semantic indexing and question answering. This year, BioASQ
consisted of new editions of the two established tasks b and Synergy, and two
new tasks: a) MultiCardioNER on the adaptation of clinical entity detection to
the cardiology domain in a multilingual setting, and b) BIONNE on nested NER in
Russian and English. In this edition of BioASQ, 37 competing teams participated
with more than 700 distinct submissions in total for the four different shared
tasks of the challenge. Similarly to previous editions, most of the
participating systems achieved competitive performance, suggesting the
continuous advancement of the state-of-the-art in the field.

</details>


### [119] [Overview of BioASQ 2025: The Thirteenth BioASQ Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering](https://arxiv.org/abs/2508.20554)
*Anastasios Nentidis,Georgios Katsimpras,Anastasia Krithara,Martin Krallinger,Miguel Rodríguez-Ortega,Eduard Rodriguez-López,Natalia Loukachevitch,Andrey Sakhovskiy,Elena Tutubalina,Dimitris Dimitriadis,Grigorios Tsoumakas,George Giannakoulas,Alexandra Bekiaridou,Athanasios Samaras,Giorgio Maria Di Nunzio,Nicola Ferro,Stefano Marchesin,Marco Martinelli,Gianmaria Silvello,Georgios Paliouras*

Main category: cs.CL

TL;DR: 本文介绍了第十三届BioASQ挑战赛的整体情况，包括新增和传统任务，以及参赛队伍表现，强调了生物医学领域语义索引和问答技术的持续进步。


<details>
  <summary>Details</summary>
Motivation: 推动生物医学大规模语义索引和问答系统的最新技术进步，为研究者搭建一个国际交流和评测平台。

Method: 通过六项不同的共享任务，包括传统的b和Synergy任务，以及四个全新任务（多语言临床摘要、多语言嵌套命名实体链接、心脏病临床编码、肠脑互作信息抽取），邀请参赛队伍提交与任务相关的系统，评估其水平。

Result: 共有83支参赛队、超过1000次有效提交参与到六大任务中。多项参赛系统取得了有竞争力的性能，体现领域技术持续进展。

Conclusion: BioASQ挑战赛持续推动生物医学领域语义索引和问答相关技术的发展，吸引大量团队高水平参与，验证了现有系统的进步和社区的活跃性。

Abstract: This is an overview of the thirteenth edition of the BioASQ challenge in the
context of the Conference and Labs of the Evaluation Forum (CLEF) 2025. BioASQ
is a series of international challenges promoting advances in large-scale
biomedical semantic indexing and question answering. This year, BioASQ
consisted of new editions of the two established tasks, b and Synergy, and four
new tasks: a) Task MultiClinSum on multilingual clinical summarization. b) Task
BioNNE-L on nested named entity linking in Russian and English. c) Task
ELCardioCC on clinical coding in cardiology. d) Task GutBrainIE on gut-brain
interplay information extraction. In this edition of BioASQ, 83 competing teams
participated with more than 1000 distinct submissions in total for the six
different shared tasks of the challenge. Similar to previous editions, several
participating systems achieved competitive performance, indicating the
continuous advancement of the state-of-the-art in the field.

</details>


### [120] [Adaptive Federated Distillation for Multi-Domain Non-IID Textual Data](https://arxiv.org/abs/2508.20557)
*Jiahao Xiao,Jiangming Liu*

Main category: cs.CL

TL;DR: 本论文提出了应对多领域非IID（非独立同分布）环境的联邦蒸馏框架，为联邦学习在真实世界中的应用搭建了更全面的评测基准，并展示了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习多聚焦于输出标签多样性的非IID场景，忽视了输入端语言领域的多样性，这与自然语言处理实际应用存在差异。因此需要更全面的非IID场景和适用于多领域数据的联邦学习方法。

Method: 作者构建了一套包含多领域数据的非IID评测基准框架，用于模拟真实世界中的多样性。在此基础上，提出了AdaFD（Adaptive Federated Distillation）自适应联邦蒸馏框架，可在同构和异构条件下适应多领域差异。

Result: 实验结果显示，所提方法能够有效捕捉本地客户端的多样性，整体性能超过现有主流联邦蒸馏方法。

Conclusion: AdaFD与全新的评测基准共同推动了联邦学习在多领域、非IID现实环境中的发展与应用。

Abstract: The widespread success of pre-trained language models has established a new
training paradigm, where a global PLM is fine-tuned using task-specific data
from local clients. The local data are highly different from each other and can
not capture the global distribution of the whole data in real world. To address
the challenges of non-IID data in real environments, privacy-preserving
federated distillation has been proposed and highly investigated. However,
previous experimental non-IID scenarios are primarily identified with the label
(output) diversity, without considering the diversity of language domains
(input) that is crucial in natural language processing. In this paper, we
introduce a comprehensive set of multi-domain non-IID scenarios and propose a
unified benchmarking framework that includes diverse data. The benchmark can be
used to evaluate the federated learning framework in a real environment. To
this end, we propose an Adaptive Federated Distillation (AdaFD) framework
designed to address multi-domain non-IID challenges in both homogeneous and
heterogeneous settings. Experimental results demonstrate that our models
capture the diversity of local clients and achieve better performance compared
to the existing works. The code for this paper is available at:
https://github.com/jiahaoxiao1228/AdaFD.

</details>


### [121] [Leveraging Generative Models for Real-Time Query-Driven Text Summarization in Large-Scale Web Search](https://arxiv.org/abs/2508.20559)
*Zeyu Xiong,Yixuan Nan,Li Gao,Hengzhu Tang,Shuaiqiang Wang,Junfeng Wang,Dawei Yin*

Main category: cs.CL

TL;DR: 本文提出了一种创新性方法，应用生成式模型于大规模网络搜索中的查询驱动文本摘要任务，实现更高效和语义理解更深的摘要生成，并在实际工业环境中超越了现有系统。


<details>
  <summary>Details</summary>
Motivation: 现有工业级提取式摘要方法在模型架构和语义理解能力上存在显著限制，无法充分应对复杂的用户查询和文档理解需求，且多阶段流程带来信息损失和瓶颈。

Method: 提出了一个结合大模型蒸馏、监督微调、直接偏好优化和“前瞻式”解码的框架，将仅有0.1B参数的轻量级模型转化为领域专用的查询驱动文本摘要（QDTS）专家。

Result: 该方法在多个行业相关评测指标上优于现有生产基线，达到了新的业界最佳，并在部署效率方面表现优异，仅用334张NVIDIA L20 GPU即可支持每秒约5万次查询且平均延迟低于55毫秒。

Conclusion: 生成式方法在工业QTDTS上的应用具备显著优势，不仅在准确性上超越了现有主流方法，也能高效部署，具备实际商业落地价值。

Abstract: In the dynamic landscape of large-scale web search, Query-Driven Text
Summarization (QDTS) aims to generate concise and informative summaries from
textual documents based on a given query, which is essential for improving user
engagement and facilitating rapid decision-making. Traditional extractive
summarization models, based primarily on ranking candidate summary segments,
have been the dominant approach in industrial applications. However, these
approaches suffer from two key limitations: 1) The multi-stage pipeline often
introduces cumulative information loss and architectural bottlenecks due to its
weakest component; 2) Traditional models lack sufficient semantic understanding
of both user queries and documents, particularly when dealing with complex
search intents. In this study, we propose a novel framework to pioneer the
application of generative models to address real-time QDTS in industrial web
search. Our approach integrates large model distillation, supervised
fine-tuning, direct preference optimization, and lookahead decoding to
transform a lightweight model with only 0.1B parameters into a
domain-specialized QDTS expert. Evaluated on multiple industry-relevant
metrics, our model outperforms the production baseline and achieves a new state
of the art. Furthermore, it demonstrates excellent deployment efficiency,
requiring only 334 NVIDIA L20 GPUs to handle \textasciitilde50,000 queries per
second under 55~ms average latency per query.

</details>


### [122] [KCS: Diversify Multi-hop Question Generation with Knowledge Composition Sampling](https://arxiv.org/abs/2508.20567)
*Yangfan Wang,Jie Liu,Chen Tang,Lian Yan,Jingchi Jiang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Knowledge Composition Sampling（KCS）的新框架，通过在上下文内采样不同知识组合，提升了多跳问答中问题生成的多样性，有效缓解了数据稀疏和虚假模式学习的问题。相较于现有方法，KCS在知识组合选择准确率和下游任务上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 多跳问答需要整合多步推理和关联知识，但数据稀疏导致预训练语言模型易习得虚假模式，且以往生成式多样性方法缺乏对关键知识的融合，限制了问题质量和复杂度。作者为增强多跳问答生成能力，提出更系统处理相关知识整合方式。

Method: KCS将知识组合建模为句级条件预测任务，利用概率对比损失预测下一个最相关的知识片段。在生成中引入随机解码策略，实现准确性与多样性的平衡。方法框架不仅促进命题多样化，还显式考虑知识片段之间的组合方式。

Result: 与主流基线模型相比，KCS框架在知识组合选择准确率上提升3.9%，并在HotpotQA与2WikiMultihopQA数据集的数据增强实验中均取得性能提升。

Conclusion: KCS有效解决了多跳问答生成中知识整合不足和多样性欠缺的问题，证明了以知识组合采样为核心的数据增强策略能够提升推理系统的表现。

Abstract: Multi-hop question answering faces substantial challenges due to data
sparsity, which increases the likelihood of language models learning spurious
patterns. To address this issue, prior research has focused on diversifying
question generation through content planning and varied expression. However,
these approaches often emphasize generating simple questions and neglect the
integration of essential knowledge, such as relevant sentences within
documents. This paper introduces the Knowledge Composition Sampling (KCS), an
innovative framework designed to expand the diversity of generated multi-hop
questions by sampling varied knowledge compositions within a given context. KCS
models the knowledge composition selection as a sentence-level conditional
prediction task and utilizes a probabilistic contrastive loss to predict the
next most relevant piece of knowledge. During inference, we employ a stochastic
decoding strategy to effectively balance accuracy and diversity. Compared to
competitive baselines, our KCS improves the overall accuracy of knowledge
composition selection by 3.9%, and its application for data augmentation yields
improvements on HotpotQA and 2WikiMultihopQA datasets. Our code is available
at: https://github.com/yangfanww/kcs.

</details>


### [123] [A Graph Talks, But Who's Listening? Rethinking Evaluations for Graph-Language Models](https://arxiv.org/abs/2508.20583)
*Soham Petkar,Hari Aakash K,Anirudh Vempati,Akshit Sinha,Ponnurangam Kumarauguru,Chirag Agarwal*

Main category: cs.CL

TL;DR: 现有的图-语言模型（GLM）评测基准无法有效衡量多模态推理。作者提出了CLEGR新基准，并发现当前GLM在需要结构推理的任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 当前GLM评价多用节点分类数据集，主要考查单一模态能力，不能充分反映结构与语义结合带来的多模态推理效果。

Method: 作者提出CLEGR基准，结合合成图生成和需要结构及文本联合推理的问题，对各类GLM架构和LLM基线模型进行了系统性比较。

Result: 实验证明soft-prompt LLM基线与全GNN GLM效果相当，同时GLM在需结构推理的任务上表现明显下降。

Conclusion: 目前GLM在图结构与语言的结合推理上存在明显瓶颈，未来需关注促进显式多模态推理的研究。

Abstract: Developments in Graph-Language Models (GLMs) aim to integrate the structural
reasoning capabilities of Graph Neural Networks (GNNs) with the semantic
understanding of Large Language Models (LLMs). However, we demonstrate that
current evaluation benchmarks for GLMs, which are primarily repurposed
node-level classification datasets, are insufficient to assess multimodal
reasoning. Our analysis reveals that strong performance on these benchmarks is
achievable using unimodal information alone, suggesting that they do not
necessitate graph-language integration. To address this evaluation gap, we
introduce the CLEGR(Compositional Language-Graph Reasoning) benchmark, designed
to evaluate multimodal reasoning at various complexity levels. Our benchmark
employs a synthetic graph generation pipeline paired with questions that
require joint reasoning over structure and textual semantics. We perform a
thorough evaluation of representative GLM architectures and find that
soft-prompted LLM baselines perform on par with GLMs that incorporate a full
GNN backbone. This result calls into question the architectural necessity of
incorporating graph structure into LLMs. We further show that GLMs exhibit
significant performance degradation in tasks that require structural reasoning.
These findings highlight limitations in the graph reasoning capabilities of
current GLMs and provide a foundation for advancing the community toward
explicit multimodal reasoning involving graph structure and language.

</details>


### [124] [Generative Annotation for ASR Named Entity Correction](https://arxiv.org/abs/2508.20700)
*Yuanchang Luo,Daimeng Wei,Shaojun Li,Hengchao Shang,Jiaxin Guo,Zongyao Li,Zhanglin Wu,Xiaoyu Chen,Zhiqiang Rao,Jinlong Yang,Hao Yang*

Main category: cs.CL

TL;DR: 针对端到端语音识别对专有名词识别不佳的问题，提出了一种结合语音特征的生成式命名实体修正方法，有效提升了实体准确率。


<details>
  <summary>Details</summary>
Motivation: 现有语音识别系统往往难以准确转写领域内的专有名词，对下游任务造成灾难性影响，而已有的高效命名实体修正模型在实体与错误转写差异较大时表现有限。

Method: 提出利用语音声学特征检索候选实体，并结合生成式方法，自动标注ASR文本中的实体错误，并替换为正确实体。该方法针对字形差异较大的错误能够有效定位和修正。

Result: 在公开和自建的数据集上验证了方法的有效性，取得了实体准确率的大幅提升。

Conclusion: 所提方法可显著提高ASR中文本专有名词识别的准确性，并将在社区开源其测试和训练数据。

Abstract: End-to-end automatic speech recognition systems often fail to transcribe
domain-specific named entities, causing catastrophic failures in downstream
tasks. Numerous fast and lightweight named entity correction (NEC) models have
been proposed in recent years. These models, mainly leveraging phonetic-level
edit distance algorithms, have shown impressive performances. However, when the
forms of the wrongly-transcribed words(s) and the ground-truth entity are
significantly different, these methods often fail to locate the wrongly
transcribed words in hypothesis, thus limiting their usage. We propose a novel
NEC method that utilizes speech sound features to retrieve candidate entities.
With speech sound features and candidate entities, we inovatively design a
generative method to annotate entity errors in ASR transcripts and replace the
text with correct entities. This method is effective in scenarios of word form
difference. We test our method using open-source and self-constructed test
sets. The results demonstrate that our NEC method can bring significant
improvement to entity accuracy. We will open source our self-constructed test
set and training data.

</details>


### [125] [Multi-Lingual Implicit Discourse Relation Recognition with Multi-Label Hierarchical Learning](https://arxiv.org/abs/2508.20712)
*Nelson Filipe Costa,Leila Kosseim*

Main category: cs.CL

TL;DR: 该论文提出了第一个用于隐式话语关系识别（IDRR）的多语种、多标签分类模型HArch，并在DiscoGeM 2.0语料库和PDTB 3.0框架下获得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 隐式话语关系识别是自然语言处理中的一个难题，尤其是在多语种和多标签场景下缺乏有效模型。因此，作者致力于构建能够跨语言、跨标签层次进行识别的先进模型，以填补现有研究的空白。

Method: 作者提出了HArch模型，通过利用话语感觉的层级依赖关系，支持在PDTB 3.0三个层级上输出概率分布。作者对多种预训练编码器骨干（如RoBERTa、XLM-RoBERTa）进行了实验，并与GPT-4o和Llama-4-Maverick等大模型的few-shot学习能力进行了对比。

Result: 实验证明，RoBERTa-HArch在英文任务上表现最佳，XLM-RoBERTa-HArch在多语种环境下最优。微调后的专用模型在所有语言配置下都显著优于GPT-4o和Llama-4-Maverick等大模型，并在DiscoGeM 1.0语料库上取得了SOTA新成绩。

Conclusion: 作者提出的HArch层次化方法显著提升了多语种IDRR任务的表现，证明了针对特定任务微调的模型优于通用大语言模型。

Abstract: This paper introduces the first multi-lingual and multi-label classification
model for implicit discourse relation recognition (IDRR). Our model, HArch, is
evaluated on the recently released DiscoGeM 2.0 corpus and leverages
hierarchical dependencies between discourse senses to predict probability
distributions across all three sense levels in the PDTB 3.0 framework. We
compare several pre-trained encoder backbones and find that RoBERTa-HArch
achieves the best performance in English, while XLM-RoBERTa-HArch performs best
in the multi-lingual setting. In addition, we compare our fine-tuned models
against GPT-4o and Llama-4-Maverick using few-shot prompting across all
language configurations. Our results show that our fine-tuned models
consistently outperform these LLMs, highlighting the advantages of
task-specific fine-tuning over prompting in IDRR. Finally, we report SOTA
results on the DiscoGeM 1.0 corpus, further validating the effectiveness of our
hierarchical approach.

</details>


### [126] [Addressing Tokenization Inconsistency in Steganography and Watermarking Based on Large Language Models](https://arxiv.org/abs/2508.20718)
*Ruiyi Yan,Yugo Murawaki*

Main category: cs.CL

TL;DR: 大型语言模型提升了文本生成的能力，但也暴露了在文本隐写和水印应用中因分词不一致（TI）带来的鲁棒性问题。本文分析了TI发生的原因，并提出了消除TI的具体方法，提升了隐写和水印的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的发展，文本隐写技术变得更难检测，同时恶意用途也在增加，因此对可靠性更高的水印技术需求变强。分词不一致（TI）成为影响隐写和水印鲁棒性的关键问题，需要深入研究和解决。

Method: 作者分析了导致分词不一致的根本原因——问题token的稀有性和临时性，并针对隐写和水印分别提出了消除TI的方法：隐写中采用逐步验证法，水印中采用后退恢复法。

Result: 实验结果表明，直接针对TI的问题进行处理，相比传统消歧方法，在隐写任务上提升了文本流畅性、隐蔽性和抗分析能力；在水印任务上提升了检测准确率和对攻击的鲁棒性。

Conclusion: 针对分词不一致导致的鲁棒性下降，提出的专用方法能有效提升文本隐写和水印的综合性能，对实用场景中的安全通信有重要意义。

Abstract: Large language models have significantly enhanced the capacities and
efficiency of text generation. On the one hand, they have improved the quality
of text-based steganography. On the other hand, they have also underscored the
importance of watermarking as a safeguard against malicious misuse. In this
study, we focus on tokenization inconsistency (TI) between Alice and Bob in
steganography and watermarking, where TI can undermine robustness. Our
investigation reveals that the problematic tokens responsible for TI exhibit
two key characteristics: infrequency and temporariness. Based on these
findings, we propose two tailored solutions for TI elimination: a stepwise
verification method for steganography and a post-hoc rollback method for
watermarking. Experiments show that (1) compared to traditional disambiguation
methods in steganography, directly addressing TI leads to improvements in
fluency, imperceptibility, and anti-steganalysis capacity; (2) for
watermarking, addressing TI enhances detectability and robustness against
attacks.

</details>


### [127] [rStar2-Agent: Agentic Reasoning Technical Report](https://arxiv.org/abs/2508.20722)
*Ning Shang,Yifei Liu,Yi Zhu,Li Lyna Zhang,Weijiang Xu,Xinyu Guan,Buze Zhang,Bingcheng Dong,Xudong Zhou,Bowen Zhang,Ying Xin,Ziming Miao,Scarlett Li,Fan Yang,Mao Yang*

Main category: cs.CL

TL;DR: rStar2-Agent 是一个 140 亿参数的数学推理模型，采用高效的自主强化学习训练方法，达到了前沿的性能水平。


<details>
  <summary>Details</summary>
Motivation: 当前大模型长链式思维（CoT）虽然提升了数学推理能力，但模型在实用的代码推理和工具使用上仍有局限，希望通过更智能的训练方法，赋予模型高级的认知与自主探索能力，提高模型实用性和推理精度。

Method: 1. 搭建高效 RL 基础设施，优化 Python 代码执行环境，降低 rollout 成本；2. 提出 GRPO-RoC 强化学习算法，通过 Resample-on-Correct 策略减少代码工具环境噪声，提升推理效果；3. 设计分阶段、高效的 agent 训练流程，从简单 SFT 到多阶段 RL，循序渐进提升模型能力。

Result: rStar2-Agent 仅用 64 张 MI300X GPU、510 个 RL 步、1 周时间，将 14B 基础模型提升至 SOTA，AIME24 达 80.6%、AIME25 达 69.8%（均为 pass@1），超越了参数量更大的 DeepSeek-R1（671B），且生成答案更简洁。模型还表现出优异的泛化能力。

Conclusion: 高效 agentic 强化学习与合理训练流程可显著提升大模型的数学推理、工具使用与泛化能力，在计算资源有限的情况下取得了超越更大模型的前沿表现。

Abstract: We introduce rStar2-Agent, a 14B math reasoning model trained with agentic
reinforcement learning to achieve frontier-level performance. Beyond current
long CoT, the model demonstrates advanced cognitive behaviors, such as thinking
carefully before using Python coding tools and reflecting on code execution
feedback to autonomously explore, verify, and refine intermediate steps in
complex problem-solving. This capability is enabled through three key
innovations that makes agentic RL effective at scale: (i) an efficient RL
infrastructure with a reliable Python code environment that supports
high-throughput execution and mitigates the high rollout costs, enabling
training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic
RL algorithm with a Resample-on-Correct rollout strategy that addresses the
inherent environment noises from coding tools, allowing the model to reason
more effectively in a code environment; (iii) An efficient agent training
recipe that starts with non-reasoning SFT and progresses through multi-RL
stages, yielding advanced cognitive abilities with minimal compute cost. To
this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in
only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on
AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly
shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates
strong generalization to alignment, scientific reasoning, and agentic tool-use
tasks. Code and training recipes are available at
https://github.com/microsoft/rStar.

</details>


### [128] [Leveraging Semantic Triples for Private Document Generation with Local Differential Privacy Guarantees](https://arxiv.org/abs/2508.20736)
*Stephen Meisenbacher,Maulik Chevli,Florian Matthes*

Main category: cs.CL

TL;DR: 本文提出了一种结合语义三元组和大模型后处理的新方法（DP-ST），在满足本地差分隐私保证的情况下实现更有用且连贯的隐私文本生成。


<details>
  <summary>Details</summary>
Motivation: 现有利用差分隐私（DP）保护自然语言文本的方法，尤其是在本地DP设定下，往往需要极高的隐私参数才能保证合理文本质量，这导致隐私和文本可用性难以平衡。

Method: 作者提出DP-ST方法，利用语义三元组对输入文本进行分解，结合必邻感知来限定DP保护的邻域范围，并进一步使用大语言模型（LLM）进行后处理，从而生成更连贯、有效的私密文本。

Result: 实验表明，DP-ST方法采用分而治之策略，并通过控制DP邻域，可在更低的隐私参数（ε）下实现较好的文本连贯性和可用性。

Conclusion: 本工作强调了文本连贯性在实现隐私与可用性平衡中的重要性，提出的方法显著提升了本地DP条件下文本私有化质量，在更实用的ε参数范围内达到了更优的隐私保护与实用性平衡。

Abstract: Many works at the intersection of Differential Privacy (DP) in Natural
Language Processing aim to protect privacy by transforming texts under DP
guarantees. This can be performed in a variety of ways, from word perturbations
to full document rewriting, and most often under local DP. Here, an input text
must be made indistinguishable from any other potential text, within some bound
governed by the privacy parameter $\varepsilon$. Such a guarantee is quite
demanding, and recent works show that privatizing texts under local DP can only
be done reasonably under very high $\varepsilon$ values. Addressing this
challenge, we introduce DP-ST, which leverages semantic triples for
neighborhood-aware private document generation under local DP guarantees.
Through the evaluation of our method, we demonstrate the effectiveness of the
divide-and-conquer paradigm, particularly when limiting the DP notion (and
privacy guarantees) to that of a privatization neighborhood. When combined with
LLM post-processing, our method allows for coherent text generation even at
lower $\varepsilon$ values, while still balancing privacy and utility. These
findings highlight the importance of coherence in achieving balanced
privatization outputs at reasonable $\varepsilon$ levels.

</details>


### [129] [Specializing General-purpose LLM Embeddings for Implicit Hate Speech Detection across Datasets](https://arxiv.org/abs/2508.20750)
*Vassiliy Cheremetiev,Quang Long Ho Ngo,Chau Ying Kot,Alina Elena Baia,Andrea Cavallaro*

Main category: cs.CL

TL;DR: 本文提出直接微调最新的大型语言模型（LLMs）嵌入模型，无需额外外部知识，能有效检测隐式仇恨言论(IHS)，并在多数据集上实现了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 隐式仇恨言论难以检测，因其通常没有明显的侮辱或煽动性词汇，传统方法常需外部知识或上下文支持，现有自动化检测手段有局限。

Method: 作者通过微调通用的大型语言模型嵌入（如Stella、Jasper、NV-Embed和E5）用于IHS任务，未引入额外上下文或情感知识，仅利用模型本身的语义表示能力。

Result: 在多个IHS数据集上，这一方法F1-macro分数in-dataset最高提升1.10个百分点，跨数据集最高提升20.35个百分点，超过现有SOTA方法。

Conclusion: 仅凭微调通用LLM嵌入模型即可实现优越的IHS检测效果，表明LLMs具备捕捉隐式仇恨表达的潜力，无需复杂特征工程或补充信息，具有很强的实用价值。

Abstract: Implicit hate speech (IHS) is indirect language that conveys prejudice or
hatred through subtle cues, sarcasm or coded terminology. IHS is challenging to
detect as it does not include explicit derogatory or inflammatory words. To
address this challenge, task-specific pipelines can be complemented with
external knowledge or additional information such as context, emotions and
sentiment data. In this paper, we show that, by solely fine-tuning recent
general-purpose embedding models based on large language models (LLMs), such as
Stella, Jasper, NV-Embed and E5, we achieve state-of-the-art performance.
Experiments on multiple IHS datasets show up to 1.10 percentage points
improvements for in-dataset, and up to 20.35 percentage points improvements in
cross-dataset evaluation, in terms of F1-macro score.

</details>


### [130] [GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and Efficient Open-Ended Text Generation](https://arxiv.org/abs/2508.20757)
*Yuanhao Ding,Esteban Garces Arias,Meimingwei Li,Julian Rodemann,Matthias Aßenmacher,Danlu Chen,Gaojuan Fan,Christian Heumann,Chongsheng Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为GUARD的自适应解码方法，通过结合全局与局部不确定性，平衡了大语言模型生成文本时的多样性与连贯性，同时提高了生成速度，实验验证其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有的开放式文本生成在追求输出多样性和连贯性时表现出明显的权衡，且现有对比搜索解码方法高度依赖超参数、计算开销大，限制了实际应用。作者希望设计一种更优且高效的解码策略。

Method: GUARD方法在解码阶段结合了全局熵估计和局部熵偏差，从而利用长期与短期的不确定性信号，理论上保证无偏性和一致性。同时引入基于token计数的惩罚项，旨在降低计算复杂度。

Result: 实验结果表明，GUARD在文本多样性和连贯性之间取得了良好的平衡，显著提升了生成速度。在多个文本质量维度上的细致对比测试中，无论人类还是LLM评价者均证明了其卓越性能。

Conclusion: GUARD为开放式文本生成提供了一种理论扎实且实践高效的通用解码框架，有效解决了多样性-连贯性权衡和效率瓶颈，适用于实际应用场景。

Abstract: Open-ended text generation faces a critical challenge: balancing coherence
with diversity in LLM outputs. While contrastive search-based decoding
strategies have emerged to address this trade-off, their practical utility is
often limited by hyperparameter dependence and high computational costs. We
introduce GUARD, a self-adaptive decoding method that effectively balances
these competing objectives through a novel "Glocal" uncertainty-driven
framework. GUARD combines global entropy estimates with local entropy
deviations to integrate both long-term and short-term uncertainty signals. We
demonstrate that our proposed global entropy formulation effectively mitigates
abrupt variations in uncertainty, such as sudden overconfidence or high entropy
spikes, and provides theoretical guarantees of unbiasedness and consistency. To
reduce computational overhead, we incorporate a simple yet effective
token-count-based penalty into GUARD. Experimental results demonstrate that
GUARD achieves a good balance between text diversity and coherence, while
exhibiting substantial improvements in generation speed. In a more nuanced
comparison study across different dimensions of text quality, both human and
LLM evaluators validated its remarkable performance. Our code is available at
https://github.com/YecanLee/GUARD.

</details>


### [131] [Feel the Difference? A Comparative Analysis of Emotional Arcs in Real and LLM-Generated CBT Sessions](https://arxiv.org/abs/2508.20764)
*Xiaoyi Wang,Jiwei Zhang,Guangtao Zhang,Honglei Guo*

Main category: cs.CL

TL;DR: 本文比较了真实和大模型生成的认知行为疗法对话在情感动态上的差异，发现合成对话在情感表达真实性和多样性上显著不足。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型合成心理咨询对话在模型训练、场景模拟等方面的广泛应用，人们担心这些对话是否真实反映了心理咨询中的细腻情感变化。该研究想要系统评估合成对话与真实咨询对话在情绪特征上的异同。

Method: 作者首次将 Utterance Emotion Dynamics 框架应用于情绪弧线分析，通过量化情绪价度、唤醒度和支配度，比较真实（公开视频转录）与 LLM 合成（CACTUS数据集）认知行为疗法对话的整体与分角色情绪轨迹。

Result: 研究发现，尽管大模型生成的对话流畅且结构完整，但在真实对话中出现的情感多样性、丰富的情绪词汇，以及角色之间真实的情绪反应与调节模式，在合成对话中均显著不足。尤其是“来访者”角色的情绪动态与真实数据相差最远。

Conclusion: 当前大模型生成的咨询对话情感表现有限，难以完全替代真实数据，在实际心理健康应用中需重视情感真实性。作者还公开了 RealCBT 真实数据集以促进后续研究。

Abstract: Synthetic therapy dialogues generated by large language models (LLMs) are
increasingly used in mental health NLP to simulate counseling scenarios, train
models, and supplement limited real-world data. However, it remains unclear
whether these synthetic conversations capture the nuanced emotional dynamics of
real therapy. In this work, we conduct the first comparative analysis of
emotional arcs between real and LLM-generated Cognitive Behavioral Therapy
dialogues. We adapt the Utterance Emotion Dynamics framework to analyze
fine-grained affective trajectories across valence, arousal, and dominance
dimensions. Our analysis spans both full dialogues and individual speaker roles
(counselor and client), using real sessions transcribed from public videos and
synthetic dialogues from the CACTUS dataset. We find that while synthetic
dialogues are fluent and structurally coherent, they diverge from real
conversations in key emotional properties: real sessions exhibit greater
emotional variability,more emotion-laden language, and more authentic patterns
of reactivity and regulation. Moreover, emotional arc similarity between real
and synthetic speakers is low, especially for clients. These findings
underscore the limitations of current LLM-generated therapy data and highlight
the importance of emotional fidelity in mental health applications. We
introduce RealCBT, a curated dataset of real CBT sessions, to support future
research in this space.

</details>


### [132] [Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection](https://arxiv.org/abs/2508.20766)
*Harethah Abu Shairah,Hasan Abed Al Kader Hammoud,George Turkiyyah,Bernard Ghanem*

Main category: cs.CL

TL;DR: 该论文提出了一种提升大语言模型（LLM）安全性的简单方法ROSI，通过对权重进行低秩（秩1）调整，使模型在激活上更好地拒绝有害请求，并验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 目前为止，LLM的安全对齐通常通过调整内部表征来让模型拒绝有害请求，但已有研究表明这些机制可以通过移除特定方向来被绕过。因此，作者希望提出一种直接增强模型在关键安全表征方向上的能力的新方法，从而更有效且廉价地提升安全性。

Method: 作者提出“Rank-One Safety Injection（ROSI）”方法，无需微调，通过一次性对所有残差流矩阵做秩1权重修改，将激活引向模型已知的拒绝有害请求的子空间。安全方向可通过少量有害/无害指令对计算得到。

Result: 实验证明，ROSI能持续提升模型（以Llama Guard 3评测）的安全拒绝率，同时对MMLU、HellaSwag、Arc等常用基准测试的常规任务性能影响极小。此外，对于缺乏安全性的“未审查”模型，ROSI也能通过放大其潜在安全方向使其重新对齐。

Conclusion: ROSI是一种廉价、有效、可解释的权重操纵方法，可提升LLM的安全性，并可作为资源消耗较大的微调方案的有力补充。

Abstract: Safety alignment in Large Language Models (LLMs) often involves mediating
internal representations to refuse harmful requests. Recent research has
demonstrated that these safety mechanisms can be bypassed by ablating or
removing specific representational directions within the model. In this paper,
we propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box
method that amplifies a model's safety alignment by permanently steering its
activations toward the refusal-mediating subspace. ROSI operates as a simple,
fine-tuning-free rank-one weight modification applied to all residual stream
write matrices. The required safety direction can be computed from a small set
of harmful and harmless instruction pairs. We show that ROSI consistently
increases safety refusal rates - as evaluated by Llama Guard 3 - while
preserving the utility of the model on standard benchmarks such as MMLU,
HellaSwag, and Arc. Furthermore, we show that ROSI can also re-align
'uncensored' models by amplifying their own latent safety directions,
demonstrating its utility as an effective last-mile safety procedure. Our
results suggest that targeted, interpretable weight steering is a cheap and
potent mechanism to improve LLM safety, complementing more resource-intensive
fine-tuning paradigms.

</details>


### [133] [Signs of Struggle: Spotting Cognitive Distortions across Language and Register](https://arxiv.org/abs/2508.20771)
*Abhishek Kuber,Enrico Liscio,Ruixuan Zhang,Caroline Figueroa,Pradeep K. Murukannaiah*

Main category: cs.CL

TL;DR: 本论文研究了认知扭曲自动检测在不同语言和文体下的泛化能力，尤其是分析荷兰青少年论坛的帖子，发现领域自适应方法最有效。


<details>
  <summary>Details</summary>
Motivation: 随着青少年心理健康问题上升，利用自动化方法尽早在数字文本中检测心理困扰征兆成为研究热点。认知扭曲是心理压力加重的重要表征，早期发现有助于及时、低成本干预。

Method: 本研究首次对认知扭曲检测模型在跨语言（英语到荷兰语）和跨文体（临床数据到论坛帖文）上的泛化能力进行了深入分析，对荷兰青少年论坛帖文进行了实验，并测试了不同领域适应方法。

Result: 结果显示，语言和写作风格的变化会显著影响检测模型的表现，而领域适应技术在跨语言和跨文体泛化上最有效。

Conclusion: 跨语言、跨文体自动检测认知扭曲虽具挑战性，但领域适应方法带来了良好的提升，未来在应用于不同群体时应优先考虑相关适应策略。

Abstract: Rising mental health issues among youth have increased interest in automated
approaches for detecting early signs of psychological distress in digital text.
One key focus is the identification of cognitive distortions, irrational
thought patterns that have a role in aggravating mental distress. Early
detection of these distortions may enable timely, low-cost interventions. While
prior work has focused on English clinical data, we present the first in-depth
study of cross-lingual and cross-register generalization of cognitive
distortion detection, analyzing forum posts written by Dutch adolescents. Our
findings show that while changes in language and writing style can
significantly affect model performance, domain adaptation methods show the most
promise.

</details>


### [134] [Exploring Machine Learning and Language Models for Multimodal Depression Detection](https://arxiv.org/abs/2508.20805)
*Javier Si Zhao Hong,Timothy Zoe Delaya,Sherwyn Chan Yin Kit,Pai Chet Ng,Xiaoxiao Miao*

Main category: cs.CL

TL;DR: 本文研究了多模态抑郁检测任务，对比了XGBoost、transformer类模型和大语言模型在音频、视频、文本特征上的表现，并分析其优缺点。


<details>
  <summary>Details</summary>
Motivation: 多模态数据可辅助更准确地检测抑郁，但尚不清楚不同模型在各种模态上的表现及代表策略效果如何。本文希望探索最有效的模型和多模态融合策略，以提升心理健康自动检测能力。

Method: 作者利用音频、视频和文本三类特征，分别采用XGBoost、基于transformer的架构和大语言模型进行建模，并对这些模型的检测表现进行对比实验分析。

Result: 不同模型在捕捉抑郁相关信号上表现各异。实验结果明确了各模型针对不同模态在抑郁检测中的长处与局限。

Conclusion: 探索多模态特征融合和模型选择对于提升抑郁检测效果至关重要，本文结果为心理健康领域的多模态预测策略提供了有价值的参考。

Abstract: This paper presents our approach to the first Multimodal Personality-Aware
Depression Detection Challenge, focusing on multimodal depression detection
using machine learning and deep learning models. We explore and compare the
performance of XGBoost, transformer-based architectures, and large language
models (LLMs) on audio, video, and text features. Our results highlight the
strengths and limitations of each type of model in capturing depression-related
signals across modalities, offering insights into effective multimodal
representation strategies for mental health prediction.

</details>


### [135] [GDLLM: A Global Distance-aware Modeling Approach Based on Large Language Models for Event Temporal Relation Extraction](https://arxiv.org/abs/2508.20828)
*Jie Zhao,Wanting Ning,Yuxiao Fei,Yubo Feng,Lishuang Li*

Main category: cs.CL

TL;DR: 该论文提出了一种结合大型语言模型（LLM）与图注意力网络（GAT）的全局距离感知方法（GDLLM），用于事件时序关系抽取任务，在提升少数类关系及整体性能方面取得了SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有小型语言模型（SLM）在处理不平衡数据集中的少数类别关系时能力有限；而LLM常用的人工提示或指令方式可能引入噪声，干扰模型对事件间长距离依赖的判断。因此，亟需提高少数类关系的识别能力，并增强长距离依赖建模能力。

Method: 提出GDLLM方法：1）利用基于GAT的距离感知图结构帮助LLM建模长距离依赖；2）设计基于软推理的时序特征学习范式，增强短距离关系的识别，并把LLM输出的概率信息融合至多头注意力机制，实现全局特征的更有效捕获。

Result: 在TB-Dense和MATRES两个公开数据集上进行了实验，结果显示GDLLM在事件时序关系抽取任务中取得了当前最佳（SOTA）表现，特别提升了少数类关系的效果。

Conclusion: GDLLM能有效结合LLM和图结构优势，显著提升事件时序关系抽取的整体及少数类性能，为相关任务提供了一种更强的建模框架。

Abstract: In Natural Language Processing(NLP), Event Temporal Relation Extraction
(ETRE) is to recognize the temporal relations of two events. Prior studies have
noted the importance of language models for ETRE. However, the restricted
pre-trained knowledge of Small Language Models(SLMs) limits their capability to
handle minority class relations in imbalanced classification datasets. For
Large Language Models(LLMs), researchers adopt manually designed prompts or
instructions, which may introduce extra noise, leading to interference with the
model's judgment of the long-distance dependencies between events. To address
these issues, we propose GDLLM, a Global Distance-aware modeling approach based
on LLMs. We first present a distance-aware graph structure utilizing Graph
Attention Network(GAT) to assist the LLMs in capturing long-distance dependency
features. Additionally, we design a temporal feature learning paradigm based on
soft inference to augment the identification of relations with a short-distance
proximity band, which supplements the probabilistic information generated by
LLMs into the multi-head attention mechanism. Since the global feature can be
captured effectively, our framework substantially enhances the performance of
minority relation classes and improves the overall learning ability.
Experiments on two publicly available datasets, TB-Dense and MATRES,
demonstrate that our approach achieves state-of-the-art (SOTA) performance.

</details>


### [136] [MSRS: Evaluating Multi-Source Retrieval-Augmented Generation](https://arxiv.org/abs/2508.20867)
*Rohan Phanse,Yijie Zhou,Kejian Shi,Wencai Zhang,Yixin Liu,Yilun Zhao,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出了一个专门用于评估RAG（Retrieval Augmented Generation）系统综合多来源信息并生成长文本答案的可扩展评测框架，并基于此构建了两个新基准数据集：MSRS-Story和MSRS-Meet。实验发现检索有效性对生成质量影响极大，推理模型在多源信息整合任务上表现明显优于常规大模型。


<details>
  <summary>Details</summary>
Motivation: 现实中的许多任务需要从多个信息源综合信息并生成长文本答案，然而现有检索增强生成系统（RAG）的评测往往局限于答短题、事实型回答或单一信息源。为推动RAG系统在真实复杂任务场景下的发展，亟需新的评价方法和数据集。

Method: 作者提出了一套可扩展的评测框架，专门挑战RAG系统在多源检索和长文本生成任务中的综合能力。基于该框架创建了两个新基准数据集：MSRS-Story（侧重叙事整合任务）和MSRS-Meet（侧重摘要场景任务），并在这两项任务中系统评测了不同组合（稀疏/密集检索器+前沿LLMs）下的RAG系统表现。

Result: 实验显示，生成质量强烈依赖于检索环节的有效性，但检索效果受具体任务影响差异很大。即便在“理想检索”（oracle retrieval）设置中，实现高质量多源信息整合依然极具挑战，但推理增强型模型在这方面的表现远超普通大模型。

Conclusion: 多源信息检索与整合的生成任务非常困难，对检索与生成两个环节都提出了更高要求。新的评测框架和基准能够更好推动RAG系统在复杂真实任务中的能力提升，而提升推理与整合能力仍是未来的重要着力点。

Abstract: Retrieval-augmented systems are typically evaluated in settings where
information required to answer the query can be found within a single source or
the answer is short-form or factoid-based. However, many real-world
applications demand the ability to integrate and summarize information
scattered across multiple sources, where no single source is sufficient to
respond to the user's question. In such settings, the retrieval component of a
RAG pipeline must recognize a variety of relevance signals, and the generation
component must connect and synthesize information across multiple sources. We
present a scalable framework for constructing evaluation benchmarks that
challenge RAG systems to integrate information across distinct sources and
generate long-form responses. Using our framework, we build two new benchmarks
on Multi-Source Retrieval and Synthesis: MSRS-Story and MSRS-Meet, representing
narrative synthesis and summarization tasks, respectively, that require
retrieval from large collections. Our extensive experiments with various RAG
pipelines -- including sparse and dense retrievers combined with frontier LLMs
-- reveal that generation quality is highly dependent on retrieval
effectiveness, which varies greatly by task. While multi-source synthesis
proves challenging even in an oracle retrieval setting, we find that reasoning
models significantly outperform standard LLMs at this distinct step.

</details>


### [137] [The Uneven Impact of Post-Training Quantization in Machine Translation](https://arxiv.org/abs/2508.20893)
*Benjamin Marie,Atsushi Fujita*

Main category: cs.CL

TL;DR: 本研究系统评估了大语言模型在多语言机器翻译任务上的量化策略，发现低位量化在低资源语言上表现下降严重，不同算法及校准方法对结果影响显著。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在现实中的广泛应用，资源受限的硬件环境（如移动端、嵌入式设备）越来越常见。通过量化缩小模型体积已成主流做法，但其对多语种任务的影响尚不明确，因此本研究填补了这一空白。

Method: 作者基于5种不同参数规模（1.7B-70B）的LLM，用4种主流量化算法（AWQ、BitsAndBytes、GGUF、AutoRound）对55种语言的机器翻译结果进行了大规模后训练量化评测，并系统对比各算法和模型规模的表现。此外，还分析了量化、解码超参数及校准语言间的相互影响。

Result: 实验发现：1) 对高资源语言和大模型，4-bit量化通常能较好保留翻译质量；2) 对低资源、结构多样语言/低bit（如2-bit），性能大幅下降；3) GGUF算法在2-bit条件下亦最稳健；4) 语言匹配的校准对于低bit更有效。

Conclusion: 量化策略要针对任务和语言资源状况谨慎选择。低资源语言在低bit量化下受损严重，推荐选择稳健的量化算法（如GGUF）并结合校准优化。研究为多语种LLM在受限硬件条件下落地提供了实用指导。

Abstract: Quantization is essential for deploying large language models (LLMs) on
resource-constrained hardware, but its implications for multilingual tasks
remain underexplored. We conduct the first large-scale evaluation of
post-training quantization (PTQ) on machine translation across 55 languages
using five LLMs ranging from 1.7B to 70B parameters. Our analysis reveals that
while 4-bit quantization often preserves translation quality for high-resource
languages and large models, significant degradation occurs for low-resource and
typologically diverse languages, particularly in 2-bit settings. We compare
four quantization techniques (AWQ, BitsAndBytes, GGUF, and AutoRound), showing
that algorithm choice and model size jointly determine robustness. GGUF
variants provide the most consistent performance, even at 2-bit precision.
Additionally, we quantify the interactions between quantization, decoding
hyperparameters, and calibration languages, finding that language-matched
calibration offers benefits primarily in low-bit scenarios. Our findings offer
actionable insights for deploying multilingual LLMs for machine translation
under quantization constraints, especially in low-resource settings.

</details>


### [138] [SageLM: A Multi-aspect and Explainable Large Language Model for Speech Judgement](https://arxiv.org/abs/2508.20916)
*Yuan Ge,Junxiang Zhang,Xiaoqian Liu,Bei Li,Xiangnan Ma,Chenglong Wang,Kaiyang Ye,Yangfan Du,Linfeng Zhang,Yuxin Huang,Tong Xiao,Zhengtao Yu,JingBo Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种名为SageLM的端到端、多维度、可解释的语音大模型评估方法，显著提升S2S大模型评测的效果。


<details>
  <summary>Details</summary>
Motivation: 当前语音到语音（S2S）大模型在人机语音交互中具有重要应用，但对这些模型的综合评估存在挑战，尤其是现有方法往往忽略音频特征、解释性不足且评价数据稀缺。

Method: SageLM不仅联合评测语义和音频两个维度，还引入基于推理（rationale-based）的监督策略提升解释性和评价一致性。作者还构建合成偏好数据集SpeechFeedback，采用两阶段训练范式缓解数据不足问题。对比传统级联方法和基于规则的强化学习，有更高的人类评价一致性。

Result: SageLM在语义和音频两个维度的训练下，与人工评测结果达成82.79%的高度一致性，分别比级联模型和SLM基础线高出至少7.42%和26.20%。

Conclusion: SageLM能够更全面、准确、可解释地评估S2S语音大模型，为语音交互系统的可靠性评估提供了有力工具。

Abstract: Speech-to-Speech (S2S) Large Language Models (LLMs) are foundational to
natural human-computer interaction, enabling end-to-end spoken dialogue
systems. However, evaluating these models remains a fundamental challenge. We
propose \texttt{SageLM}, an end-to-end, multi-aspect, and explainable speech
LLM for comprehensive S2S LLMs evaluation. First, unlike cascaded approaches
that disregard acoustic features, SageLM jointly assesses both semantic and
acoustic dimensions. Second, it leverages rationale-based supervision to
enhance explainability and guide model learning, achieving superior alignment
with evaluation outcomes compared to rule-based reinforcement learning methods.
Third, we introduce \textit{SpeechFeedback}, a synthetic preference dataset,
and employ a two-stage training paradigm to mitigate the scarcity of speech
preference data. Trained on both semantic and acoustic dimensions, SageLM
achieves an 82.79\% agreement rate with human evaluators, outperforming
cascaded and SLM-based baselines by at least 7.42\% and 26.20\%, respectively.

</details>


### [139] [How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on $τ$-bench](https://arxiv.org/abs/2508.20931)
*Venkatesh Mishra,Amir Saeidi,Satyam Raj,Mutsumi Nakamura,Jayanth Srinivasa,Gaowen Liu,Ali Payani,Chitta Baral*

Main category: cs.CL

TL;DR: 本文提出了一种名为Input-Reformulation Multi-Agent (IRMA)的新框架，通过自动重写用户查询、结合领域规则及工具建议，提升大模型在多轮对话和工具调用中的稳定性和决策表现，实验证明IRMA优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 伴随大语言模型推理与规划能力提升，其作为自主代理在动态环境下的工具调用应用前景广阔。然而在如τ-bench的多轮对话环境中，这类代理常出现推理不连贯、不符领域规范及信息提取不准确等问题。本文动机在于识别并改善这些常见错误，提高模型在复杂对话任务中的可靠性和表现。

Method: 作者首先对现有多轮对话任务中大模型代理常见错误进行手动细致分析，挖掘其失败原因。随后，作者探索针对工具调用代理的输入重构方法，以协助其更好地决策。最终提出IRMA框架，该框架自动将用户问题与相关领域规则、工具建议结合，构建针对性的输入，使代理更聚焦任务需求，提升工具调用表现。

Result: IRMA框架在总体pass^5指标上，相比ReAct、Function Calling和Self-Reflection方法分别提升了16.1%、12.7%、19.1%。实验表明IRMA在动态环境下的鲁棒性和一致性均显著优于现有方案。

Conclusion: IRMA显著提升了大模型在多轮会话和多工具调用情境下的可靠性与一致性，是面向动态环境的复杂对话与推理任务中的更优选择。

Abstract: Recent advances in reasoning and planning capabilities of large language
models (LLMs) have enabled their potential as autonomous agents capable of tool
use in dynamic environments. However, in multi-turn conversational environments
like $\tau$-bench, these agents often struggle with consistent reasoning,
adherence to domain-specific policies, and extracting correct information over
a long horizon of tool-calls and conversation. To capture and mitigate these
failures, we conduct a comprehensive manual analysis of the common errors
occurring in the conversation trajectories. We then experiment with
reformulations of inputs to the tool-calling agent for improvement in agent
decision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA)
framework, which automatically reformulates user queries augmented with
relevant domain rules and tool suggestions for the tool-calling agent to focus
on. The results show that IRMA significantly outperforms ReAct, Function
Calling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in
overall pass^5 scores. These findings highlight the superior reliability and
consistency of IRMA compared to other methods in dynamic environments.

</details>


### [140] [STARE at the Structure: Steering ICL Exemplar Selection with Structural Alignment](https://arxiv.org/abs/2508.20944)
*Jiaqian Li,Qisheng Hu,Jing Li,Wenya Wang*

Main category: cs.CL

TL;DR: 本文针对结构化预测任务中，ICL示例选择策略未充分考虑结构对齐问题，导致效果不佳。作者提出了一种结构感知的两阶段示例选择策略，结合BERT检索器和增强插件，提升了泛化性与性能，在多个基准数据集及大模型上效果优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 当前ICL在结构化预测类任务（如语义解析）上，拿来即用的示例选择往往只重视语义相关性，忽视了任务中的结构对齐，导致泛化和性能都不理想。因此需要更好地示例筛选方式。

Method: 方法分两阶段：第一步微调一个基于BERT的检索器，加入结构感知监督，让其检索到既语义相关又结构对齐的示例；第二步为检索器加入轻量级模块，强化语法结构信息，该插件可无缝集成到各类pipline中，且模型无关。

Result: 在三个语义解析任务下的四个基准数据集上，用多种大语言模型作为推理模型，实验结果表明本文方法在所有设置下均显著优于现有主流基线。

Conclusion: 结构感知的两阶段示例选择策略能够有效提升ICL在结构化任务中的表现，并具有较好的通用性与集成灵活性，对后续相关研究及工程实践具有重要价值。

Abstract: In-Context Learning (ICL) has become a powerful paradigm that enables LLMs to
perform a wide range of tasks without task-specific fine-tuning. However, the
effectiveness of ICL heavily depends on the quality of exemplar selection. In
particular, for structured prediction tasks such as semantic parsing, existing
ICL selection strategies often overlook structural alignment, leading to
suboptimal performance and poor generalization. To address this issue, we
propose a novel two-stage exemplar selection strategy that achieves a strong
balance between efficiency, generalizability, and performance. First, we
fine-tune a BERT-based retriever using structure-aware supervision, guiding it
to select exemplars that are both semantically relevant and structurally
aligned. Then, we enhance the retriever with a plug-in module, which amplifies
syntactically meaningful information in the hidden representations. This
plug-in is model-agnostic, requires minimal overhead, and can be seamlessly
integrated into existing pipelines. Experiments on four benchmarks spanning
three semantic parsing tasks demonstrate that our method consistently
outperforms existing baselines with multiple recent LLMs as inference-time
models.

</details>


### [141] [ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue Agents](https://arxiv.org/abs/2508.20973)
*Tianjian Liu,Fanqi Wan,Jiajian Guo,Xiaojun Quan*

Main category: cs.CL

TL;DR: 本文提出了ProactiveEval，一个用于评估大语言模型（LLM）主动式对话能力的统一框架，从目标规划和对话引导两方面量化不同模型的主动对话表现，并在多领域进行了系统评测。


<details>
  <summary>Details</summary>
Motivation: 当前主动对话研究多局限于任务型或特定领域，导致评估碎片化，难以全面衡量和提升LLM的主动对话能力，因此需要统一和通用的评测框架。

Method: 作者设计了ProactiveEval评测框架，将主动对话拆解为目标规划和对话引导两部分，并设定跨多领域的评测指标。该框架还可以自动生成多样化、高难度的评测数据。在该框架下，作者构建了6个领域共328个评测环境，并测试了22种不同类型的LLM。

Result: 实验显示，DeepSeek-R1模型在目标规划方面表现突出，Claude-3.7-Sonnet模型在对话引导任务上表现优异。此外，作者还探讨了推理能力对主动行为的影响。

Conclusion: ProactiveEval实现了LLM主动对话能力的系统性评估，为未来模型开发提供了依据，并强调了推理能力在提升主动对话水平中的作用。

Abstract: Proactive dialogue has emerged as a critical and challenging research problem
in advancing large language models (LLMs). Existing works predominantly focus
on domain-specific or task-oriented scenarios, which leads to fragmented
evaluations and limits the comprehensive exploration of models' proactive
conversation abilities. In this work, we propose ProactiveEval, a unified
framework designed for evaluating proactive dialogue capabilities of LLMs. This
framework decomposes proactive dialogue into target planning and dialogue
guidance, establishing evaluation metrics across various domains. Moreover, it
also enables the automatic generation of diverse and challenging evaluation
data. Based on the proposed framework, we develop 328 evaluation environments
spanning 6 distinct domains. Through experiments with 22 different types of
LLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional
performance on target planning and dialogue guidance tasks, respectively.
Finally, we investigate how reasoning capabilities influence proactive
behaviors and discuss their implications for future model development.

</details>


### [142] [Lethe: Purifying Backdoored Large Language Models with Knowledge Dilution](https://arxiv.org/abs/2508.21004)
*Chen Chen,Yuchen Sun,Jiaxin Gao,Xueluan Gong,Qian Wang,Ziyao Wang,Yongsen Zheng,Kwok-Yan Lam*

Main category: cs.CL

TL;DR: 本文提出了LETHE方法，通过知识稀释结合内部与外部机制，有效清除大语言模型中的后门攻击，并在多项实验中优于现有的八种主流防御方法，攻击成功率最高可降低98%。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型虽然在NLP任务中表现优异，但依然容易受到后门攻击，现有防御手段局限性大，难以应对复杂和进阶的后门场景。例如仅支持特定类型触发器防范，或者只是检测而无法消除后门，缺乏鲁棒性。

Method: LETHE方法包括两大机制：（1）内部机制：用一个轻量级的数据集训练干净的模型，并与存在后门的模型进行权重合并，稀释掉模型中后门相关参数记忆，降低后门效果；（2）外部机制：在提示中引入良性且语义相关的证据信息，分散模型对后门特征的注意力。

Result: 在5种主流大语言模型、多个分类和生成任务及8种后门攻击场景下，LETHE防御成功率显著优于8个主流防御基线。对复杂后门攻击，攻击成功率最高下降98%，同时保持了原有模型效能。

Conclusion: LETHE能高效、低成本并且鲁棒地清除复杂后门攻击行为，是比当前主流防御机制更有效的解决方案。

Abstract: Large language models (LLMs) have seen significant advancements, achieving
superior performance in various Natural Language Processing (NLP) tasks.
However, they remain vulnerable to backdoor attacks, where models behave
normally for standard queries but generate harmful responses or unintended
output when specific triggers are activated. Existing backdoor defenses either
lack comprehensiveness, focusing on narrow trigger settings, detection-only
mechanisms, and limited domains, or fail to withstand advanced scenarios like
model-editing-based, multi-trigger, and triggerless attacks. In this paper, we
present LETHE, a novel method to eliminate backdoor behaviors from LLMs through
knowledge dilution using both internal and external mechanisms. Internally,
LETHE leverages a lightweight dataset to train a clean model, which is then
merged with the backdoored model to neutralize malicious behaviors by diluting
the backdoor impact within the model's parametric memory. Externally, LETHE
incorporates benign and semantically relevant evidence into the prompt to
distract LLM's attention from backdoor features. Experimental results on
classification and generation domains across 5 widely used LLMs demonstrate
that LETHE outperforms 8 state-of-the-art defense baselines against 8 backdoor
attacks. LETHE reduces the attack success rate of advanced backdoor attacks by
up to 98% while maintaining model utility. Furthermore, LETHE has proven to be
cost-efficient and robust against adaptive backdoor attacks.

</details>


### [143] [An Agile Method for Implementing Retrieval Augmented Generation Tools in Industrial SMEs](https://arxiv.org/abs/2508.21024)
*Mathieu Bourdin,Anas Neumann,Thomas Paviot,Robert Pellerin,Samir Lamouri*

Main category: cs.CL

TL;DR: 本文提出EASI-RAG方法，帮中小企业快速部署RAG检索增强生成系统，提升数据问答的准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 虽然RAG能解决大模型幻觉与知识滞后的问题，但中小企业因缺乏资源与NLP经验，难以实际部署。

Method: 作者以方法工程为基础，提出EASI-RAG，包括明确的角色分工、步骤活动和实施技术。在一个环境测试实验室进行了案例验证，从无经验到部署成功不到一个月，并通过用户反馈持续优化。

Result: EASI-RAG实现了快速落地、高用户采用率、准确的QA能力，并提升了数据可靠性。

Conclusion: EASI-RAG为工业中小企业使用RAG提供了可行路径，有望推广到更多场景并融合定制化模型。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful solution to
mitigate the limitations of Large Language Models (LLMs), such as
hallucinations and outdated knowledge. However, deploying RAG-based tools in
Small and Medium Enterprises (SMEs) remains a challenge due to their limited
resources and lack of expertise in natural language processing (NLP). This
paper introduces EASI-RAG, Enterprise Application Support for Industrial RAG, a
structured, agile method designed to facilitate the deployment of RAG systems
in industrial SME contexts. EASI-RAG is based on method engineering principles
and comprises well-defined roles, activities, and techniques. The method was
validated through a real-world case study in an environmental testing
laboratory, where a RAG tool was implemented to answer operators queries using
data extracted from operational procedures. The system was deployed in under a
month by a team with no prior RAG experience and was later iteratively improved
based on user feedback. Results demonstrate that EASI-RAG supports fast
implementation, high user adoption, delivers accurate answers, and enhances the
reliability of underlying data. This work highlights the potential of RAG
deployment in industrial SMEs. Future works include the need for generalization
across diverse use cases and further integration with fine-tuned models.

</details>


### [144] [Re-Representation in Sentential Relation Extraction with Sequence Routing Algorithm](https://arxiv.org/abs/2508.21049)
*Ramazan Ali Bahrami,Ramin Yahyapour*

Main category: cs.CL

TL;DR: 该论文提出了一种基于动态路由的胶囊网络方法用于句子级关系抽取，并在多个数据集上取得了领先效果。作者分析了该方法在不同数据集表现差异的原因，并探讨了再表征能力对关系抽取的影响。


<details>
  <summary>Details</summary>
Motivation: 句子级关系抽取是NLP中的关键任务，但当前方法在不同数据集间表现波动较大，特别是在标签噪音较多的大型数据集上。作者旨在提高模型泛化能力，并理解导致性能差异的本质原因。

Method: 作者提出了基于动态路由的胶囊网络用于句子级关系抽取，并在四个主流数据集（Tacred、Tacredrev、Retacred、Conll04）上进行了实验。通过实验分析噪音标签和再表征机制对性能的影响。

Result: 所提方法在Tacred、Tacredrev、Retacred、Conll04等主流数据集上均超过当前最优模型，但在Wikidata（较大且带有较多噪音标签的数据集）上表现不佳。模型具备比传统模型更强的再表征能力。

Conclusion: 标签噪音会显著影响关系抽取效果，同时再表征能力是提高句子级关系抽取质量的关键。作者建议未来在远程监督数据集上不仅应关注噪音处理，还应关注模型的再表征能力提升。

Abstract: Sentential relation extraction (RE) is an important task in natural language
processing (NLP). In this paper we propose to do sentential RE with dynamic
routing in capsules. We first show that the proposed approach outperform state
of the art on common sentential relation extraction datasets Tacred, Tacredrev,
Retacred, and Conll04. We then investigate potential reasons for its good
performance on the mentioned datasets, and yet low performance on another
similar, yet larger sentential RE dataset, Wikidata. As such, we identify noise
in Wikidata labels as one of the reasons that can hinder performance.
Additionally, we show associativity of better performance with better
re-representation, a term from neuroscience referred to change of
representation in human brain to improve the match at comparison time. As
example, in the given analogous terms King:Queen::Man:Woman, at comparison
time, and as a result of re-representation, the similarity between related head
terms (King,Man), and tail terms (Queen,Woman) increases. As such, our
observation show that our proposed model can do re-representation better than
the vanilla model compared with. To that end, beside noise in the labels of the
distantly supervised RE datasets, we propose re-representation as a challenge
in sentential RE.

</details>


### [145] [Enabling Equitable Access to Trustworthy Financial Reasoning](https://arxiv.org/abs/2508.21051)
*William Jurayj,Nils Holzenberger,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 本论文提出了一种结合大型语言模型（LLM）与符号求解器的系统，用于自动化计算税务义务，并在真实世界成本和准确率方面进行了评估。


<details>
  <summary>Details</summary>
Motivation: 当前税务申报在全球范围内都非常复杂，需要结合规则理解和数值计算。自动化系统必须兼具高准确率和可审计性，而现有LLM难以满足这些要求。

Method: 将LLM与符号推理系统集成，通过将文本化的税法规则转译为形式逻辑程序，并结合智能检索的案例进行推理和计算，应用于SARA数据集，并提出实际部署成本估算方法。

Result: 系统在SARA数据集表现优异，通过形式化规则与案例检索大幅提升了性能，成本也显著低于人工办理的平均水平。

Conclusion: 神经-符号架构有潜力为用户提供高效、经济且可靠的税务自动化服务，有助于提升普惠性的合规与税务辅助水平。

Abstract: According to the United States Internal Revenue Service, ''the average
American spends $\$270$ and 13 hours filing their taxes''. Even beyond the
U.S., tax filing requires complex reasoning, combining application of
overlapping rules with numerical calculations. Because errors can incur costly
penalties, any automated system must deliver high accuracy and auditability,
making modern large language models (LLMs) poorly suited for this task. We
propose an approach that integrates LLMs with a symbolic solver to calculate
tax obligations. We evaluate variants of this system on the challenging
StAtutory Reasoning Assessment (SARA) dataset, and include a novel method for
estimating the cost of deploying such a system based on real-world penalties
for tax errors. We further show how combining up-front translation of
plain-text rules into formal logic programs, combined with intelligently
retrieved exemplars for formal case representations, can dramatically improve
performance on this task and reduce costs to well below real-world averages.
Our results demonstrate the promise and economic feasibility of neuro-symbolic
architectures for increasing equitable access to reliable tax assistance.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [146] [Learning Fast, Tool aware Collision Avoidance for Collaborative Robots](https://arxiv.org/abs/2508.20457)
*Joonho Lee,Yunho Kim,Seokjoon Kim,Quan Nguyen,Youngjin Heo*

Main category: cs.RO

TL;DR: 提出了一种新型协作机器人避障系统，能根据工具变化实时调整，在动态环境下实现高效安全避障，同时计算成本低于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有协作机器人在动态环境（障碍和任务不断变化）下难以兼顾安全与效率，且往往假设视野完整、工具固定，导致避障过度保守或发生碰撞。

Method: 本工作设计了一个工具感知的碰撞规避系统。利用学习的感知模型从点云中过滤机器人和工具组件，推理遮挡区域，在部分可见情况下预测碰撞。控制策略通过受约束强化学习训练，实现<10毫秒的平滑避让操作。

Result: 在模拟和实际测试中，该方法在动态环境下超越主流方法（APF、MPPI），保持亚毫米级精度，同时计算成本减少约60%（对比先进的基于GPU规划器）。

Conclusion: 该系统提供了模块化、高效且有效的避障能力，已成功集成到协作机器人应用中，实现了安全、灵敏的实际操作。

Abstract: Ensuring safe and efficient operation of collaborative robots in human
environments is challenging, especially in dynamic settings where both obstacle
motion and tasks change over time. Current robot controllers typically assume
full visibility and fixed tools, which can lead to collisions or overly
conservative behavior. In our work, we introduce a tool-aware collision
avoidance system that adjusts in real time to different tool sizes and modes of
tool-environment interaction. Using a learned perception model, our system
filters out robot and tool components from the point cloud, reasons about
occluded area, and predicts collision under partial observability. We then use
a control policy trained via constrained reinforcement learning to produce
smooth avoidance maneuvers in under 10 milliseconds. In simulated and
real-world tests, our approach outperforms traditional approaches (APF, MPPI)
in dynamic environments, while maintaining sub-millimeter accuracy. Moreover,
our system operates with approximately 60% lower computational cost compared to
a state-of-the-art GPU-based planner. Our approach provides modular, efficient,
and effective collision avoidance for robots operating in dynamic environments.
We integrate our method into a collaborative robot application and demonstrate
its practical use for safe and responsive operation.

</details>


### [147] [SPGrasp: Spatiotemporal Prompt-driven Grasp Synthesis in Dynamic Scenes](https://arxiv.org/abs/2508.20547)
*Yunpeng Mei,Hongjie Cao,Yinqiu Xia,Wei Xiao,Zhaohan Feng,Gang Wang,Jie Chen*

Main category: cs.RO

TL;DR: SPGrasp提出了一种面向动态物体实时交互抓取的新方法，极大提升了抓取速度和准确性，有效解决了实时性与可交互性的矛盾。


<details>
  <summary>Details</summary>
Motivation: 现有的抓取合成方法在动态物体场景下难以实现低延迟推理和及时响应用户指令，影响了工业与机器人等领域的实际应用。

Method: SPGrasp创新性地在视频流抓取估计中融合了用户提示与时空上下文，将Segment Anything Model v2（SAMv2）进行拓展，实现了低至59毫秒的端到端延迟并保证时序一致性。

Result: 在OCID和Jacquard基准上，SPGrasp的抓取准确率分别达到90.6%和93.8%；在GraspNet-1Billion数据集上以73.1毫秒的单帧延迟实现92.0%准确率，比现有可提示方法RoG-SAM的延迟降低58.5%；13个运动物体的真实实验中交互式抓取成功率为94.8%。

Conclusion: SPGrasp在动态抓取任务中有效解决了延迟与可交互性的权衡，在准确率、速度和交互体验上均具备领先优势，为实际机器人抓取应用提供了有力支撑。

Abstract: Real-time interactive grasp synthesis for dynamic objects remains challenging
as existing methods fail to achieve low-latency inference while maintaining
promptability. To bridge this gap, we propose SPGrasp (spatiotemporal
prompt-driven dynamic grasp synthesis), a novel framework extending segment
anything model v2 (SAMv2) for video stream grasp estimation. Our core
innovation integrates user prompts with spatiotemporal context, enabling
real-time interaction with end-to-end latency as low as 59 ms while ensuring
temporal consistency for dynamic objects. In benchmark evaluations, SPGrasp
achieves instance-level grasp accuracies of 90.6% on OCID and 93.8% on
Jacquard. On the challenging GraspNet-1Billion dataset under continuous
tracking, SPGrasp achieves 92.0% accuracy with 73.1 ms per-frame latency,
representing a 58.5% reduction compared to the prior state-of-the-art
promptable method RoG-SAM while maintaining competitive accuracy. Real-world
experiments involving 13 moving objects demonstrate a 94.8% success rate in
interactive grasping scenarios. These results confirm SPGrasp effectively
resolves the latency-interactivity trade-off in dynamic grasp synthesis. Code
is available at https://github.com/sejmoonwei/SPGrasp.

</details>


### [148] [SimShear: Sim-to-Real Shear-based Tactile Servoing](https://arxiv.org/abs/2508.20561)
*Kipp McAdam Freud,Yijiong Lin,Nathan F. Lepora*

Main category: cs.RO

TL;DR: SimShear是一种针对机器人触觉控制的sim-to-real（从仿真到真实）管道，创新地在无需显式建模剪切动态的前提下，实现了剪切信息的引入和还原，提高了涉及剪切感知的机器人控制任务表现。


<details>
  <summary>Details</summary>
Motivation: 剪切（shear）感知对机器人操作动态交互任务至关重要，但剪切动力学难以建模和仿真，限制了触觉机器人领域的发展。因此，研究者希望找到无需复杂建模也能引入剪切感知的sim-to-real解决方案。

Method: 本文提出了一种shear-conditioned U-Net GAN（shPix2pix），能够将无剪切的仿真触觉图像和剪切矢量信息结合，生成包含真实剪切变形的仿真图像。然后将SimShear应用于两个机器人触觉控制任务：1）机器人手臂跟踪任务；2）机器人协作搬运任务。

Result: shPix2pix方法优于pix2pix基线方法，在仿真触觉图像生成和位姿/剪切预测任务中表现更佳。在两个机器人控制任务中，方法能将触觉跟踪误差控制在1-2毫米以内，有效支撑了剪切感知任务的实现。

Conclusion: SimShear无须在仿真中复杂建模剪切动力学，即能较好实现剪切感知的 sim-to-real 迁移，为触觉机器人仿真研究提供了新的方向。

Abstract: We present SimShear, a sim-to-real pipeline for tactile control that enables
the use of shear information without explicitly modeling shear dynamics in
simulation. Shear, arising from lateral movements across contact surfaces, is
critical for tasks involving dynamic object interactions but remains
challenging to simulate. To address this, we introduce shPix2pix, a
shear-conditioned U-Net GAN that transforms simulated tactile images absent of
shear, together with a vector encoding shear information, into realistic
equivalents with shear deformations. This method outperforms baseline pix2pix
approaches in simulating tactile images and in pose/shear prediction. We apply
SimShear to two control tasks using a pair of low-cost desktop robotic arms
equipped with a vision-based tactile sensor: (i) a tactile tracking task, where
a follower arm tracks a surface moved by a leader arm, and (ii) a collaborative
co-lifting task, where both arms jointly hold an object while the leader
follows a prescribed trajectory. Our method maintains contact errors within 1
to 2 mm across varied trajectories where shear sensing is essential, validating
the feasibility of sim-to-real shear modeling with rigid-body simulators and
opening new directions for simulation in tactile robotics.

</details>


### [149] [Traversing the Narrow Path: A Two-Stage Reinforcement Learning Framework for Humanoid Beam Walking](https://arxiv.org/abs/2508.20661)
*TianChen Huang,Wei Gao,Runchen Xu,Shiwu Zhang*

Main category: cs.RO

TL;DR: 提出了一个两阶段的步态规划框架，结合模板与残差修正，让人形机器人能更稳定安全地跨越窄梁，并在仿真和真实环境中取得比传统方法更好的效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法在窄梁行走等高风险任务上鲁棒性不足，且安全可控性有待提升。该论文旨在设计一种兼具物理解释性和安全性的步态生成方案。

Method: 提出物理基础的两阶段架构，第一阶段训练低层追踪器在平地上鲁棒地跟随步伐目标；第二阶段在仿真中对高层规划器进行训练，让其基于脚步模板，在狭窄梁面精炼步态，实现更安全精准的落脚。使用有限感知输入以便现实部署。

Result: 在Unitree G1机器人和仿真环境均实现0.2米宽、3米长窄梁稳定行走。与模板法和纯深度学习法相比，该方法在成功率、中心线保持及安全边际等指标更优。

Conclusion: 结构化模板+残差微调方法既保证了安全性和可解释性，也实现了从仿真到真实机器人的顺利迁移，是窄梁步态生成的有效策略。

Abstract: Traversing narrow beams is challenging for humanoids due to sparse,
safety-critical contacts and the fragility of purely learned policies. We
propose a physically grounded, two-stage framework that couples an XCoM/LIPM
footstep template with a lightweight residual planner and a simple low-level
tracker. Stage-1 is trained on flat ground: the tracker learns to robustly
follow footstep targets by adding small random perturbations to heuristic
footsteps, without any hand-crafted centerline locking, so it acquires stable
contact scheduling and strong target-tracking robustness. Stage-2 is trained in
simulation on a beam: a high-level planner predicts a body-frame residual
(Delta x, Delta y, Delta psi) for the swing foot only, refining the template
step to prioritize safe, precise placement under narrow support while
preserving interpretability. To ease deployment, sensing is kept minimal and
consistent between simulation and hardware: the planner consumes compact,
forward-facing elevation cues together with onboard IMU and joint signals. On a
Unitree G1, our system reliably traverses a 0.2 m-wide, 3 m-long beam. Across
simulation and real-world studies, residual refinement consistently outperforms
template-only and monolithic baselines in success rate, centerline adherence,
and safety margins, while the structured footstep interface enables transparent
analysis and low-friction sim-to-real transfer.

</details>


### [150] [Task-Oriented Edge-Assisted Cross-System Design for Real-Time Human-Robot Interaction in Industrial Metaverse](https://arxiv.org/abs/2508.20664)
*Kan Chen,Zhen Meng,Xiangmin Xu,Jiaming Yang,Emma Li,Philip G. Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种面向任务的边缘辅助跨系统框架，结合数字孪生（DTs）和预测性人机交互，有效提升工业元宇宙中实时人-设备互动的响应性和精准度。


<details>
  <summary>Details</summary>
Motivation: 工业元宇宙中的实时人-设备交互面临高算力需求、带宽有限和严格延迟等挑战，现有方案难以兼顾系统性能和响应速度。

Method: 作者设计了一种借助数字孪生、分离为视觉与控制的虚拟功能模块的框架，通过预测操作员动作，实现元宇宙渲染和远程设备预控，并提出HITL-MAML算法动态优化预测时域，提升框架通用性。

Result: 在两个实际任务中，框架显著优化表现：绘图轨迹控制任务中加权RMSE降至0.0101m，核退役3D场景任务中，PSNR为22.11，SSIM为0.8729，LPIPS为0.1298，表现出高精度与高视觉保真度。

Conclusion: 该框架能在高风险工业环境中保障实时空间精度和视觉质量，为工业元宇宙中复杂任务的可靠人机协作提供了新的解决思路。

Abstract: Real-time human-device interaction in industrial Metaverse faces challenges
such as high computational load, limited bandwidth, and strict latency. This
paper proposes a task-oriented edge-assisted cross-system framework using
digital twins (DTs) to enable responsive interactions. By predicting operator
motions, the system supports: 1) proactive Metaverse rendering for visual
feedback, and 2) preemptive control of remote devices. The DTs are decoupled
into two virtual functions-visual display and robotic control-optimizing both
performance and adaptability. To enhance generalizability, we introduce the
Human-In-The-Loop Model-Agnostic Meta-Learning (HITL-MAML) algorithm, which
dynamically adjusts prediction horizons. Evaluation on two tasks demonstrates
the framework's effectiveness: in a Trajectory-Based Drawing Control task, it
reduces weighted RMSE from 0.0712 m to 0.0101 m; in a real-time 3D scene
representation task for nuclear decommissioning, it achieves a PSNR of 22.11,
SSIM of 0.8729, and LPIPS of 0.1298. These results show the framework's
capability to ensure spatial precision and visual fidelity in real-time,
high-risk industrial environments.

</details>


### [151] [Task Allocation for Autonomous Machines using Computational Intelligence and Deep Reinforcement Learning](https://arxiv.org/abs/2508.20688)
*Thanh Thi Nguyen,Quoc Viet Hung Nguyen,Jonathan Kua,Imran Razzak,Dung Nguyen,Saeid Nahavandi*

Main category: cs.RO

TL;DR: 本文综述了多自主机器在复杂环境下协同控制的算法，着重分析了基于计算智能和深度强化学习的任务分配方法，评估其优劣并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 多自主机器在现实环境中广泛应用，但其高效、可靠的协同控制仍面临诸多挑战，需要发展更加先进的任务分配与控制协调算法。

Method: 对现有用于多自主机器控制与协调的主要算法开展文献综述，系统讨论了采用计算智能与深度强化学习进行任务分配的方法，并详细分析这些方法的应用情况、优点与不足。

Result: 计算智能与深度强化学习方法在解决复杂任务分配与不确定、动态环境中的难题方面取得了良好效果，近年来深度强化学习已成为控制与协调领域的重要发展方向。

Conclusion: CI和深度RL为多自主机器的任务分配与协同提供了有效途径，未来需进一步研究新方法和提升现有方法的实用性，探究未充分挖掘的研究领域，为相关研究提供全面参考。

Abstract: Enabling multiple autonomous machines to perform reliably requires the
development of efficient cooperative control algorithms. This paper presents a
survey of algorithms that have been developed for controlling and coordinating
autonomous machines in complex environments. We especially focus on task
allocation methods using computational intelligence (CI) and deep reinforcement
learning (RL). The advantages and disadvantages of the surveyed methods are
analysed thoroughly. We also propose and discuss in detail various future
research directions that shed light on how to improve existing algorithms or
create new methods to enhance the employability and performance of autonomous
machines in real-world applications. The findings indicate that CI and deep RL
methods provide viable approaches to addressing complex task allocation
problems in dynamic and uncertain environments. The recent development of deep
RL has greatly contributed to the literature on controlling and coordinating
autonomous machines, and it has become a growing trend in this area. It is
envisaged that this paper will provide researchers and engineers with a
comprehensive overview of progress in machine learning research related to
autonomous machines. It also highlights underexplored areas, identifies
emerging methodologies, and suggests new avenues for exploration in future
research within this domain.

</details>


### [152] [Non-expert to Expert Motion Translation Using Generative Adversarial Networks](https://arxiv.org/abs/2508.20740)
*Yuki Tanaka,Seiichiro Katsura*

Main category: cs.RO

TL;DR: 本文针对熟练工人数量减少的问题，提出一种基于生成对抗网络（GAN）的灵活运动迁移方法，实现专家技能向机器人传授，突破了传统模仿学习在任务可变性与标签局限上的不足。


<details>
  <summary>Details</summary>
Motivation: 全球熟练工人减少，导致对机器人技能学习的需求增加。现有模仿学习主要记录位置和力的数据，但在适应新任务和满足人类意图方面仍有限。部分方法通过条件训练能适应变更任务，但标签局限，灵活性不足。

Method: 作者提出了一种利用生成对抗网络（GAN）的灵活运动转译方法，不仅支持通过数据教机器人任务，还能通过训练模型迁移人类技能。该系统通过输入专家的数据，实现运动的保存和灵活复现。

Result: 该方法在一台3自由度书法机器人上进行了评估，结果表明系统能够有效将专家技能灵活迁移并应用于机器人任务中。

Conclusion: 基于GAN的运动迁移提高了机器人学习专家技能的灵活性和适应新任务的能力，相较于现有方法具有更高的通用性和实用性，有助于解决熟练工人稀缺带来的产业挑战。

Abstract: Decreasing skilled workers is a very serious problem in the world. To deal
with this problem, the skill transfer from experts to robots has been
researched. These methods which teach robots by human motion are called
imitation learning. Experts' skills generally appear in not only position data,
but also force data. Thus, position and force data need to be saved and
reproduced. To realize this, a lot of research has been conducted in the
framework of a motion-copying system. Recent research uses machine learning
methods to generate motion commands. However, most of them could not change
tasks by following human intention. Some of them can change tasks by
conditional training, but the labels are limited. Thus, we propose the flexible
motion translation method by using Generative Adversarial Networks. The
proposed method enables users to teach robots tasks by inputting data, and
skills by a trained model. We evaluated the proposed system with a 3-DOF
calligraphy robot.

</details>


### [153] [Uncertainty Aware-Predictive Control Barrier Functions: Safer Human Robot Interaction through Probabilistic Motion Forecasting](https://arxiv.org/abs/2508.20812)
*Lorenzo Busellato,Federico Cunico,Diego Dall'Alba,Marco Emporio,Andrea Giachetti,Riccardo Muradore,Marco Cristani*

Main category: cs.RO

TL;DR: 本文提出了一种结合概率人类动作预测与控制障碍函数（CBFs）的不确定性感知预测控制障碍函数（UA-PCBFs）框架，实现了人机协作场景下更灵活、高效且安全的机器人行为。


<details>
  <summary>Details</summary>
Motivation: 当前协作机器人系统面临在保证严格安全性的同时，实现流畅、高效人机交互的难题。传统方法往往采用过度保守的安全包络，导致机器人频繁刹车、停工，影响任务进度与交互流畅性。现有学习型人类动作预测方法大多数未能有效处理预测不确定性，导致机器人规划仍偏保守，缺乏灵活性。

Method: 提出UA-PCBFs框架，首次将概率化的人手动作预测与控制障碍函数（CBF）结合，能根据人类动作预测的不确定性动态调整安全边界。通过引入预测模块，框架为机器人提供对未来人类状态的概率理解，用于更智能的动作规划。

Result: 在包括自动化和真实人机交互的多种实验证明下，UA-PCBFs框架在及时性、可用性和用户信心方面均表现优异。在任务关键的安全性指标上，相比最新方法，UA-PCBFs显著减少了机器人安全空间的违规次数。

Conclusion: UA-PCBFs能够有效提升协作机器人在动态人机交互场景下的安全性与交互流畅度，为实际部署更智能、灵活的机器人系统奠定基础。

Abstract: To enable flexible, high-throughput automation in settings where people and
robots share workspaces, collaborative robotic cells must reconcile stringent
safety guarantees with the need for responsive and effective behavior. A
dynamic obstacle is the stochastic, task-dependent variability of human motion:
when robots fall back on purely reactive or worst-case envelopes, they brake
unnecessarily, stall task progress, and tamper with the fluidity that true
Human-Robot Interaction demands. In recent years, learning-based human-motion
prediction has rapidly advanced, although most approaches produce worst-case
scenario forecasts that often do not treat prediction uncertainty in a
well-structured way, resulting in over-conservative planning algorithms,
limiting their flexibility. We introduce Uncertainty-Aware Predictive Control
Barrier Functions (UA-PCBFs), a unified framework that fuses probabilistic
human hand motion forecasting with the formal safety guarantees of Control
Barrier Functions. In contrast to other variants, our framework allows for
dynamic adjustment of the safety margin thanks to the human motion uncertainty
estimation provided by a forecasting module. Thanks to uncertainty estimation,
UA-PCBFs empower collaborative robots with a deeper understanding of future
human states, facilitating more fluid and intelligent interactions through
informed motion planning. We validate UA-PCBFs through comprehensive real-world
experiments with an increasing level of realism, including automated setups (to
perform exactly repeatable motions) with a robotic hand and direct human-robot
interactions (to validate promptness, usability, and human confidence).
Relative to state-of-the-art HRI architectures, UA-PCBFs show better
performance in task-critical metrics, significantly reducing the number of
violations of the robot's safe space during interaction with respect to the
state-of-the-art.

</details>


### [154] [A Soft Fabric-Based Thermal Haptic Device for VR and Teleoperation](https://arxiv.org/abs/2508.20831)
*Rui Chen,Domenico Chiaradia,Antonio Frisoli,Daniele Leonardis*

Main category: cs.RO

TL;DR: 本论文提出了一种用于虚拟现实和遥操作的新型织物基热触觉界面，结合了气动驱动和导电织物，实现了超轻量化（每个手指单元仅2克），可为手指垫提供动态压力和热刺激。经实验验证，该系统可实现快速热调节、高力输出并提升虚拟任务操作表现。


<details>
  <summary>Details</summary>
Motivation: 当前虚拟现实和遥操作系统对沉浸式和真实触觉反馈的需求日益增长，传统触觉界面普遍存在重量大、舒适性差、反馈类型单一等问题。因此需要开发一种既轻便、舒适，又可同时实现热与压力反馈的可穿戴触觉设备。

Method: 设计并制造了一种将加热元件嵌入织物气动腔体的新型热触觉设备，结构完全柔软。通过对热和气动子系统做综合表征，评估了加热速率、输出力、冷却效率等关键性能，并通过两轮用户实验验证其在人机交互任务中的有效性。

Result: 系统表现出高达3°C/s的快速加热能力，气动系统最大输出力为8.93N。指垫—驱动器间隙优化后，冷却效率提升且力损失极小。用户实验表明，三档热反馈的辨识准确率达到0.98。在虚拟抓取任务中，成功率由88.5%提升至96.4%（p=0.029），力控制精度显著提升（p=0.013）。

Conclusion: 该集成化热-触觉设备重量轻、柔软且性能优异，能有效提升虚拟现实和遥操作中的沉浸感与操作表现，显示出其在人机交互领域的重要应用潜力。

Abstract: This paper presents a novel fabric-based thermal-haptic interface for virtual
reality and teleoperation. It integrates pneumatic actuation and conductive
fabric with an innovative ultra-lightweight design, achieving only 2~g for each
finger unit. By embedding heating elements within textile pneumatic chambers,
the system delivers modulated pressure and thermal stimuli to fingerpads
through a fully soft, wearable interface.
  Comprehensive characterization demonstrates rapid thermal modulation with
heating rates up to 3$^{\circ}$C/s, enabling dynamic thermal feedback for
virtual or teleoperation interactions. The pneumatic subsystem generates forces
up to 8.93~N at 50~kPa, while optimization of fingerpad-actuator clearance
enhances cooling efficiency with minimal force reduction. Experimental
validation conducted with two different user studies shows high temperature
identification accuracy (0.98 overall) across three thermal levels, and
significant manipulation improvements in a virtual pick-and-place tasks.
Results show enhanced success rates (88.5\% to 96.4\%, p = 0.029) and improved
force control precision (p = 0.013) when haptic feedback is enabled, validating
the effectiveness of the integrated thermal-haptic approach for advanced
human-machine interaction applications.

</details>


### [155] [Model-Free Hovering and Source Seeking via Extremum Seeking Control: Experimental Demonstration](https://arxiv.org/abs/2508.20836)
*Ahmed A. Elgohary,Rohan Palanikumar,Sameh A. Eisa*

Main category: cs.RO

TL;DR: 本文通过实验首次验证了一种基于极值寻优控制（ESC）的新方法，可实现扑翼机器人的无模型、实时悬停和寻源。结果证实ESC为扑翼飞行控制和仿生学提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 当前对扑翼系统（如昆虫、蜂鸟等）实现悬停和目标寻求的控制方法大多依赖准确建模，然而实际系统复杂且难以获得精确模型。因而亟须一种无需建模、能在真实环境中即时起效的新型仿生控制方法。

Method: 基于极值寻优控制（ESC），提出一种无模型、实时的扑翼机器人控制方法，可模拟昆虫及蜂鸟的悬停和寻源行为，本文首次通过实验验证其效果（1维场景内）。

Result: 实验结果显示在1维空间内，该ESC方法可实现扑翼机器人的悬停与目标寻找，验证了无模型、实时控制的可行性。

Conclusion: ESC是扑翼飞行和仿生机器人领域的一种自然的新型控制方案，能够在无需物理建模的前提下实现仿生悬停和寻源控制，为该领域开辟新的思路和方法。

Abstract: In a recent effort, we successfully proposed a categorically novel approach
to mimic the phenomenoa of hovering and source seeking by flapping insects and
hummingbirds using a new extremum seeking control (ESC) approach. Said ESC
approach was shown capable of characterizing the physics of hovering and source
seeking by flapping systems, providing at the same time uniquely novel
opportunity for a model-free, real-time biomimicry control design. In this
paper, we experimentally test and verify, for the first time in the literature,
the potential of ESC in flapping robots to achieve model-free, real-time
controlled hovering and source seeking. The results of this paper, while being
restricted to 1D, confirm the premise of introducing ESC as a natural control
method and biomimicry mechanism to the field of flapping flight and robotics.

</details>


### [156] [Learning Primitive Embodied World Models: Towards Scalable Robotic Learning](https://arxiv.org/abs/2508.20840)
*Qiao Sun,Liujia Yang,Wei Tang,Wei Huang,Kaixin Xu,Yongchao Chen,Mingyu Liu,Jiange Yang,Haoyi Zhu,Yating Wang,Tong He,Yilun Chen,Xili Dai,Nanyang Ye,Qinying Gu*

Main category: cs.RO

TL;DR: 本文提出了一种面向原始动作的世界模型（PEWM），通过将视频生成限制在短时段，实现了更高效的动作-语言对齐、降低学习难度，并提高数据效率。方法还整合了视觉-语言模型和起止点热力图引导机制，提升了任务泛化和推理速度，有望推进通用化、可解释的智能体发展。


<details>
  <summary>Details</summary>
Motivation: 现有基于视频生成的智能体世界模型需要大量高维度的数据，收集困难且动作与语言之间的对齐粒度有限，限制了长时序生成和大规模提升。作者观察到，尽管互动数据多样，但基本动作空间有限，提出专注原始动作建模以提升效率和精度。

Method: 提出Primitive Embodied World Models (PEWM) 框架，将视频生成限制为固定短时段，实现精细的动作-语言-视觉对齐。方法中集成了模块化的视觉-语言模型（VLM）规划器与起止点热力图引导机制（SGG），支持在复杂、长任务中对原始动作策略进行灵活组合和闭环控制。

Result: PEWM在数据利用率、推理时延、动作-语言对齐和策略泛化等方面优于传统方法。通过大量实验，证明了在有限数据下也能高效学习并完成复杂任务，表现出更好的可扩展性和可解释性。

Conclusion: 提出的PEWM方法显著提升了数据效率和任务泛化能力，有望成为可扩展、可解释、通用型智能体世界模型发展的新方向。

Abstract: While video-generation-based embodied world models have gained increasing
attention, their reliance on large-scale embodied interaction data remains a
key bottleneck. The scarcity, difficulty of collection, and high dimensionality
of embodied data fundamentally limit the alignment granularity between language
and actions and exacerbate the challenge of long-horizon video
generation--hindering generative models from achieving a "GPT moment" in the
embodied domain. There is a naive observation: the diversity of embodied data
far exceeds the relatively small space of possible primitive motions. Based on
this insight, we propose a novel paradigm for world modeling--Primitive
Embodied World Models (PEWM). By restricting video generation to fixed short
horizons, our approach 1) enables fine-grained alignment between linguistic
concepts and visual representations of robotic actions, 2) reduces learning
complexity, 3) improves data efficiency in embodied data collection, and 4)
decreases inference latency. By equipping with a modular Vision-Language Model
(VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further
enables flexible closed-loop control and supports compositional generalization
of primitive-level policies over extended, complex tasks. Our framework
leverages the spatiotemporal vision priors in video models and the semantic
awareness of VLMs to bridge the gap between fine-grained physical interaction
and high-level reasoning, paving the way toward scalable, interpretable, and
general-purpose embodied intelligence.

</details>


### [157] [Genetic Informed Trees (GIT*): Path Planning via Reinforced Genetic Programming Heuristics](https://arxiv.org/abs/2508.20871)
*Liding Zhang,Kuanqi Cai,Zhenshan Bing,Chaoqun Wang,Alois Knoll*

Main category: cs.RO

TL;DR: 本文提出了一种新型路径规划方法Genetic Informed Trees（GIT*），通过引入复杂环境信息和奖励系统反馈优化启发函数，进一步提升了路径规划效率和解的质量。


<details>
  <summary>Details</summary>
Motivation: 现有最优路径规划依赖启发函数，但往往简化其结构，未能充分利用环境数据，导致效率和效果受限。本文旨在克服这一局限，提升算法性能。

Method: 提出GIT*算法，在前作Effort Informed Trees（EIT*）的基础上，集成障碍物排斥力、节点动态重要性等多种环境信息以优化启发函数；融合强化遗传编程（RGP），结合奖励反馈机制自动进化生成更优的启发函数。

Result: 与传统单次查询、采样式规划器相比，GIT*在R^4到R^16问题及真实移动操作实验中，表现出更高的计算效率和更优的解质量。

Conclusion: 综合环境因素和奖励反馈的GIT*方法能显著提升路径规划的效率与结果质量，优于现有同类算法，适用于更复杂的实际任务。

Abstract: Optimal path planning involves finding a feasible state sequence between a
start and a goal that optimizes an objective. This process relies on heuristic
functions to guide the search direction. While a robust function can improve
search efficiency and solution quality, current methods often overlook
available environmental data and simplify the function structure due to the
complexity of information relationships. This study introduces Genetic Informed
Trees (GIT*), which improves upon Effort Informed Trees (EIT*) by integrating a
wider array of environmental data, such as repulsive forces from obstacles and
the dynamic importance of vertices, to refine heuristic functions for better
guidance. Furthermore, we integrated reinforced genetic programming (RGP),
which combines genetic programming with reward system feedback to mutate
genotype-generative heuristic functions for GIT*. RGP leverages a multitude of
data types, thereby improving computational efficiency and solution quality
within a set timeframe. Comparative analyses demonstrate that GIT* surpasses
existing single-query, sampling-based planners in problems ranging from R^4 to
R^16 and was tested on a real-world mobile manipulation task. A video
showcasing our experimental results is available at
https://youtu.be/URjXbc_BiYg

</details>


### [158] [Deep Fuzzy Optimization for Batch-Size and Nearest Neighbors in Optimal Robot Motion Planning](https://arxiv.org/abs/2508.20884)
*Liding Zhang,Qiyang Zong,Yu Zhang,Zhenshan Bing,Alois Knoll*

Main category: cs.RO

TL;DR: 本文提出了一种名为LIT*的新型采样规划器，通过深度模糊神经网络动态优化运动规划参数，在高维复杂环境中有效提升路径规划效率和质量。


<details>
  <summary>Details</summary>
Motivation: 目前采样型运动规划算法在参数设置上对环境的自适应能力低，难以在障碍物分布复杂或高维空间中兼顾效率与结果质量。

Method: 文中借鉴深度模糊神经网络思想，提出LIT*，可根据配置空间中障碍物分布动态调整batch size和最近邻参数，通过全局与局部状态编码区分稀疏与密集区域，实现智能参数优化。

Result: 在高维空间（R^8-R^14）实验表明，LIT*较先进单次查询采样型规划器收敛更快、路径质量更高，并在双臂机器人任务中验证了其实用性。

Conclusion: LIT*通过学习与动态自适应优化采样策略提升了采样型规划效率与结果质量，适用于实际高维复杂机器人任务，具有良好的应用前景。

Abstract: Efficient motion planning algorithms are essential in robotics. Optimizing
essential parameters, such as batch size and nearest neighbor selection in
sampling-based methods, can enhance performance in the planning process.
However, existing approaches often lack environmental adaptability. Inspired by
the method of the deep fuzzy neural networks, this work introduces
Learning-based Informed Trees (LIT*), a sampling-based deep fuzzy
learning-based planner that dynamically adjusts batch size and nearest neighbor
parameters to obstacle distributions in the configuration spaces. By encoding
both global and local ratios via valid and invalid states, LIT* differentiates
between obstacle-sparse and obstacle-dense regions, leading to lower-cost paths
and reduced computation time. Experimental results in high-dimensional spaces
demonstrate that LIT* achieves faster convergence and improved solution
quality. It outperforms state-of-the-art single-query, sampling-based planners
in environments ranging from R^8 to R^14 and is successfully validated on a
dual-arm robot manipulation task. A video showcasing our experimental results
is available at: https://youtu.be/NrNs9zebWWk

</details>


### [159] [CoCoL: A Communication Efficient Decentralized Collaborative Method for Multi-Robot Systems](https://arxiv.org/abs/2508.20898)
*Jiaxi Huang,Yan Huang,Yixian Zhao,Wenchao Meng,Jinming Xu*

Main category: cs.RO

TL;DR: 提出了一种高效通信的去中心化多机器人协同学习方法CoCoL，在保持精度的同时，极大减少了通信开销，适用于异构数据、流数据和动态网络环境。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统在协同学习复杂任务中表现出色，但高通信开销和数据异质性导致实际应用受限。因此需要开发通信高效且能适应异构数据分布的协同学习方法。

Method: 提出CoCoL方法，基于镜像下降框架，近似采用牛顿型更新捕捉多机器人目标函数间相似性，通过不精确子问题求解减少计算负担，并结合梯度追踪机制增强对数据异质性的鲁棒性。

Result: 在三个典型多机器人协同学习任务中，CoCoL能显著减少通信轮数和总带宽消耗，同时保持最先进的精度，特别适用于非独立同分布、流数据及动态网络拓扑环境。

Conclusion: CoCoL显著提升了多机器人协同学习任务的通信效率与适应性，为实际部署大规模多机器人系统提供了有效解决方案。

Abstract: Collaborative learning enhances the performance and adaptability of
multi-robot systems in complex tasks but faces significant challenges due to
high communication overhead and data heterogeneity inherent in multi-robot
tasks. To this end, we propose CoCoL, a Communication efficient decentralized
Collaborative Learning method tailored for multi-robot systems with
heterogeneous local datasets. Leveraging a mirror descent framework, CoCoL
achieves remarkable communication efficiency with approximate Newton-type
updates by capturing the similarity between objective functions of robots, and
reduces computational costs through inexact sub-problem solutions. Furthermore,
the integration of a gradient tracking scheme ensures its robustness against
data heterogeneity. Experimental results on three representative multi robot
collaborative learning tasks show the superiority of the proposed CoCoL in
significantly reducing both the number of communication rounds and total
bandwidth consumption while maintaining state-of-the-art accuracy. These
benefits are particularly evident in challenging scenarios involving non-IID
(non-independent and identically distributed) data distribution, streaming
data, and time-varying network topologies.

</details>


### [160] [Language-Enhanced Mobile Manipulation for Efficient Object Search in Indoor Environments](https://arxiv.org/abs/2508.20899)
*Liding Zhang,Zeqi Li,Kuanqi Cai,Qian Huang,Zhenshan Bing,Alois Knoll*

Main category: cs.RO

TL;DR: 本文提出了一种利用语言模型增强的分层导航框架（GODHS），提升机器人在复杂环境中搜索和定位目标物体的效率。


<details>
  <summary>Details</summary>
Motivation: 当前场景表示大多只捕捉静态语义，缺乏可解释的上下文推理能力，致使机器人难以在完全陌生的环境中高效搜寻物体。

Method: 提出Goal-Oriented Dynamically Heuristic-Guided Hierarchical Search（GODHS），通过紧密结合语义感知和空间推理，采用大语言模型分析场景语义，并利用多级决策结构与逻辑约束指导搜索。同时引入结合极角排序和距离优先的启发式运动规划器规划探索路径。

Result: 在Isaac Sim仿真平台上进行全面评估显示，GODHS在定位目标物体时较传统非语义搜索策略具有更高搜索效率。

Conclusion: GODHS框架有效提升了机器人在复杂环境下的目标搜索效率，验证了基于语义和启发式引导的分层搜索策略的可行性和优越性。

Abstract: Enabling robots to efficiently search for and identify objects in complex,
unstructured environments is critical for diverse applications ranging from
household assistance to industrial automation. However, traditional scene
representations typically capture only static semantics and lack interpretable
contextual reasoning, limiting their ability to guide object search in
completely unfamiliar settings. To address this challenge, we propose a
language-enhanced hierarchical navigation framework that tightly integrates
semantic perception and spatial reasoning. Our method, Goal-Oriented
Dynamically Heuristic-Guided Hierarchical Search (GODHS), leverages large
language models (LLMs) to infer scene semantics and guide the search process
through a multi-level decision hierarchy. Reliability in reasoning is achieved
through the use of structured prompts and logical constraints applied at each
stage of the hierarchy. For the specific challenges of mobile manipulation, we
introduce a heuristic-based motion planner that combines polar angle sorting
with distance prioritization to efficiently generate exploration paths.
Comprehensive evaluations in Isaac Sim demonstrate the feasibility of our
framework, showing that GODHS can locate target objects with higher search
efficiency compared to conventional, non-semantic search strategies. Website
and Video are available at: https://drapandiger.github.io/GODHS

</details>


### [161] [PLUME: Procedural Layer Underground Modeling Engine](https://arxiv.org/abs/2508.20926)
*Gabriel Manuel Garcia,Antoine Richard,Miguel Olivares-Mendez*

Main category: cs.RO

TL;DR: 本文提出了PLUME，一个用于生成三维地下环境的过程化生成框架，可为太空探索、AI训练和机器人算法评估等多种应用提供真实多样的模拟地下场景。


<details>
  <summary>Details</summary>
Motivation: 随着太空探索的发展，地下环境因其在提供庇护、资源获取和科学研究方面的优势而变得越来越有吸引力。地球上的地下环境不易获取且多样性有限，无法代表太阳系中丰富的地下环境，需要新的工具来低成本、高效地创建这些环境以助力技术研发。

Method: 作者提出了PLUME框架，通过过程化生成技术，支持灵活定制和持续增强多种地下特征，能够生成多样化的三维地下场景。这些场景可用于AI训练、机器人算法评估、三维渲染等，还能加快探索算法的测试迭代。PLUME已集成进机器人模拟器，并开源发布。

Result: 实验表明，PLUME已成功与机器人模拟器结合使用，能够高效且多样地生成地下三维环境，满足AI训练及各类算法开发需求。开源特性方便了社区共同完善和扩展。

Conclusion: PLUME为太空地下环境探索和仿真提供了一种高效、灵活且可扩展的工具，有助于推动AI和机器人在相关领域的研究和应用。

Abstract: As space exploration advances, underground environments are becoming
increasingly attractive due to their potential to provide shelter, easier
access to resources, and enhanced scientific opportunities. Although such
environments exist on Earth, they are often not easily accessible and do not
accurately represent the diversity of underground environments found throughout
the solar system. This paper presents PLUME, a procedural generation framework
aimed at easily creating 3D underground environments. Its flexible structure
allows for the continuous enhancement of various underground features, aligning
with our expanding understanding of the solar system. The environments
generated using PLUME can be used for AI training, evaluating robotics
algorithms, 3D rendering, and facilitating rapid iteration on developed
exploration algorithms. In this paper, it is demonstrated that PLUME has been
used along with a robotic simulator. PLUME is open source and has been released
on Github. https://github.com/Gabryss/P.L.U.M.E

</details>


### [162] [Scaling Fabric-Based Piezoresistive Sensor Arrays for Whole-Body Tactile Sensing](https://arxiv.org/abs/2508.20959)
*Curtis C. Johnson,Daniel Webb,David Hill,Marc D. Killpack*

Main category: cs.RO

TL;DR: 本论文提出了一种新型、可扩展的全身触觉传感系统架构，利用织物传感器和定制电子设备，实现了高效、低串扰和高同步的数据采集，适用于实时机器人控制，并经验证能改善机器人整体操作任务。


<details>
  <summary>Details</summary>
Motivation: 目前全身触觉传感在机器人领域受制于布线复杂、数据吞吐量不足及系统可靠性差等问题，限制了其应用规模和实际效能。

Method: 作者采用开源织物基传感器与自研低串扰的读出电子设备，结合创新的串联SPI总线拓扑，避免了无线协议和USB集线器的限制，实现大面积（1平方米）高分辨率（8000+触点）同步数据流，并超过50FPS的刷新率。

Result: 系统可流畅采集8000余个触点的同步数据，支持实时机器人控制。在实验证明中，机器人在无反馈时会挤压易变形物体，有损伤风险，有触觉反馈后能柔和、稳定地抓取目标，避免损坏。

Conclusion: 提出的系统为高级全身控制和人机物理交互研究提供了一种高鲁棒、可扩展的平台，显著提高了大型触觉感知的实用性和效能。

Abstract: Scaling tactile sensing for robust whole-body manipulation is a significant
challenge, often limited by wiring complexity, data throughput, and system
reliability. This paper presents a complete architecture designed to overcome
these barriers. Our approach pairs open-source, fabric-based sensors with
custom readout electronics that reduce signal crosstalk to less than 3.3%
through hardware-based mitigation. Critically, we introduce a novel,
daisy-chained SPI bus topology that avoids the practical limitations of common
wireless protocols and the prohibitive wiring complexity of USB hub-based
systems. This architecture streams synchronized data from over 8,000 taxels
across 1 square meter of sensing area at update rates exceeding 50 FPS,
confirming its suitability for real-time control. We validate the system's
efficacy in a whole-body grasping task where, without feedback, the robot's
open-loop trajectory results in an uncontrolled application of force that
slowly crushes a deformable cardboard box. With real-time tactile feedback, the
robot transforms this motion into a gentle, stable grasp, successfully
manipulating the object without causing structural damage. This work provides a
robust and well-characterized platform to enable future research in advanced
whole-body control and physical human-robot interaction.

</details>


### [163] [ActLoc: Learning to Localize on the Move via Active Viewpoint Selection](https://arxiv.org/abs/2508.20981)
*Jiajie Li,Boyang Sun,Luca Di Giammarino,Hermann Blum,Marc Pollefeys*

Main category: cs.RO

TL;DR: 本文提出了一种名为ActLoc的主动视角感知规划框架，通过智能选择相机朝向提升机器人导航中的定位准确性，实现了比以往更稳健的自主定位。


<details>
  <summary>Details</summary>
Motivation: 现有机器人定位系统通常假设同一位置的各个视角都具有同等的信息量，实际场景中由于未映射区域或模糊视角可能导致定位失败，因此需要一种能够动态选择最优视角的方法以提升定位的可靠性。

Method: 提出ActLoc框架，核心为大规模训练的attention机制模型，对输入的度量地图和相机位姿进行编码，预测任意三维位置在不同偏航与俯仰方向上的定位准确率分布。将该分布整合进路径规划器，引导机器人主动选择最优化的相机朝向，兼顾任务和运动约束。

Result: ActLoc在单视点选择任务上取得了最优成绩，同时能有效扩展至整条轨迹的规划，表现出较好的泛化能力。

Conclusion: ActLoc设计模块化，适用于多种机器人导航与巡检任务，能够显著提升定位准确性与鲁棒性。

Abstract: Reliable localization is critical for robot navigation, yet most existing
systems implicitly assume that all viewing directions at a location are equally
informative. In practice, localization becomes unreliable when the robot
observes unmapped, ambiguous, or uninformative regions. To address this, we
present ActLoc, an active viewpoint-aware planning framework for enhancing
localization accuracy for general robot navigation tasks. At its core, ActLoc
employs a largescale trained attention-based model for viewpoint selection. The
model encodes a metric map and the camera poses used during map construction,
and predicts localization accuracy across yaw and pitch directions at arbitrary
3D locations. These per-point accuracy distributions are incorporated into a
path planner, enabling the robot to actively select camera orientations that
maximize localization robustness while respecting task and motion constraints.
ActLoc achieves stateof-the-art results on single-viewpoint selection and
generalizes effectively to fulltrajectory planning. Its modular design makes it
readily applicable to diverse robot navigation and inspection tasks.

</details>


### [164] [UltraTac: Integrated Ultrasound-Augmented Visuotactile Sensor for Enhanced Robotic Perception](https://arxiv.org/abs/2508.20982)
*Junhao Gong,Kit-Wa Sou,Shoujie Li,Changqing Guo,Yan Huang,Chuqiao Lyu,Ziwu Song,Wenbo Ding*

Main category: cs.RO

TL;DR: 本论文提出了一种新型的集成传感器UltraTac，将视觉-触觉成像与超声波传感相结合，能够实现材料分类和多功能感知，提升机器人操作能力。


<details>
  <summary>Details</summary>
Motivation: 传统视觉-触觉传感器虽然具备高分辨率的触觉感知能力，但无法识别物体的材料特性。为拓展其感知能力，作者设计了能集成超声波检测的传感系统。

Method: 提出共轴光声架构，将超声波传感与视觉-触觉传感集成于同一结构中，并通过声学匹配设计，保证两种传感模式协同工作。系统通过触觉反馈动态调整超声波模块的工作状态，实现柔性协调。通过一系列系统实验评估了其性能。

Result: 系统可实现3-8cm内的接近感知（$R^2=0.90$）、高精度材料分类（准确率99.20%），并在15类物体上实现92.11%的纹理-材料双模识别。同时，该传感器集成到机器人系统，可同时检测容器表面图案和内部内容物。

Conclusion: UltraTac传感器兼有视觉-触觉成像与超声波检测能力，显著提升了多模态物体识别和机器人操作的精确性与灵活性，展示了其在先进人机交互和精细操作中的应用前景。

Abstract: Visuotactile sensors provide high-resolution tactile information but are
incapable of perceiving the material features of objects. We present UltraTac,
an integrated sensor that combines visuotactile imaging with ultrasound sensing
through a coaxial optoacoustic architecture. The design shares structural
components and achieves consistent sensing regions for both modalities.
Additionally, we incorporate acoustic matching into the traditional
visuotactile sensor structure, enabling integration of the ultrasound sensing
modality without compromising visuotactile performance. Through tactile
feedback, we dynamically adjust the operating state of the ultrasound module to
achieve flexible functional coordination. Systematic experiments demonstrate
three key capabilities: proximity sensing in the 3-8 cm range ($R^2=0.90$),
material classification (average accuracy: 99.20%), and texture-material
dual-mode object recognition achieving 92.11% accuracy on a 15-class task.
Finally, we integrate the sensor into a robotic manipulation system to
concurrently detect container surface patterns and internal content, which
verifies its potential for advanced human-machine interaction and precise
robotic manipulation.

</details>


### [165] [Rapid Mismatch Estimation via Neural Network Informed Variational Inference](https://arxiv.org/abs/2508.21007)
*Mateusz Jaszczuk,Nadia Figueroa*

Main category: cs.RO

TL;DR: 本文提出了一种称为RME（快速不匹配估计）的新方法，能够让机器人在未知动态变化下安全、迅速地适应物体质量和质心的变化，无需外部传感器。


<details>
  <summary>Details</summary>
Motivation: 现有基于阻抗的控制器对动力学模型的准确性要求高，模型不准确时会导致任务失败和不安全行为。因此需要一种能够实时估计和适应动力学变化的方法，尤其是在与人或环境交互时。

Method: 提出了RME框架，该方法无需外部力/力矩传感器，仅通过机器人本体反馈，利用神经网络生成先验，再结合变分推断方法在线估计末端执行器的动力学不匹配参数，并能同时输出不确定性评估。实验在带有先进阻抗控制器的7自由度机器人上实现。

Result: RME方法在实际机器人上测试，能够在大约400毫秒内适应末端质量和质心突变（包括静止和运动情景）。还演示了在人机协作场景下，能够在操作过程中对附件和负载变化做出快速安全自适应。

Conclusion: RME框架实现了无需额外传感器的、不依赖具体控制器的动力学不匹配估计，使得机器人能更安全、灵活地应对动态环境和人机协作，为机器人在实际复杂场景中的应用提供了重要支持。

Abstract: With robots increasingly operating in human-centric environments, ensuring
soft and safe physical interactions, whether with humans, surroundings, or
other machines, is essential. While compliant hardware can facilitate such
interactions, this work focuses on impedance controllers that allow
torque-controlled robots to safely and passively respond to contact while
accurately executing tasks. From inverse dynamics to quadratic
programming-based controllers, the effectiveness of these methods relies on
accurate dynamics models of the robot and the object it manipulates. Any model
mismatch results in task failures and unsafe behaviors. Thus, we introduce
Rapid Mismatch Estimation (RME), an adaptive, controller-agnostic,
probabilistic framework that estimates end-effector dynamics mismatches online,
without relying on external force-torque sensors. From the robot's
proprioceptive feedback, a Neural Network Model Mismatch Estimator generates a
prior for a Variational Inference solver, which rapidly converges to the
unknown parameters while quantifying uncertainty. With a real 7-DoF manipulator
driven by a state-of-the-art passive impedance controller, RME adapts to sudden
changes in mass and center of mass at the end-effector in $\sim400$ ms, in
static and dynamic settings. We demonstrate RME in a collaborative scenario
where a human attaches an unknown basket to the robot's end-effector and
dynamically adds/removes heavy items, showcasing fast and safe adaptation to
changing dynamics during physical interaction without any external sensory
system.

</details>


### [166] [HITTER: A HumanoId Table TEnnis Robot via Hierarchical Planning and Learning](https://arxiv.org/abs/2508.21043)
*Zhi Su,Bike Zhang,Nima Rahmanian,Yuman Gao,Qiayuan Liao,Caitlin Regan,Koushil Sreenath,S. Shankar Sastry*

Main category: cs.RO

TL;DR: 本文提出一种结合模型规划和强化学习控制的分层框架，使类人机器人能够以高敏捷性和精度参与乒乓球对打，在真人与机器人以及机器人之间实现连续多板往返，刷新现有水平。


<details>
  <summary>Details</summary>
Motivation: 尽管类人机器人在行走和全身协调控制上已经取得了进展，但它们在需要快速动态交互的操作任务（如乒乓球）中仍然受限，迫切需要提升感知、预测和反应速度以实现高自由度操作。

Method: 方法上，作者设计了一个分层架构：上层为基于建模的规划器，负责球的轨迹预测及球拍的目标规划（包括击球位置、速度和时机）；下层为基于强化学习的全身控制器，能够模仿人类击球动作并协调四肢运动，训练中引入了真人运动数据作为参考，提高动作自然度。

Result: 该系统在通用类人机器人上进行了实测，机器人与真人对打实现了最高106板的连续回合，并与另一个机器人实现了持续多板往返，展示了高效的亚秒级反应能力和稳定的控制效果。

Conclusion: 结果表明，分层结合模型与强化学习的方法让类人机器人在真实世界乒乓球中实现了灵敏的实时交互控制，为今后机器人敏捷交互行为发展迈出了重要一步。

Abstract: Humanoid robots have recently achieved impressive progress in locomotion and
whole-body control, yet they remain constrained in tasks that demand rapid
interaction with dynamic environments through manipulation. Table tennis
exemplifies such a challenge: with ball speeds exceeding 5 m/s, players must
perceive, predict, and act within sub-second reaction times, requiring both
agility and precision. To address this, we present a hierarchical framework for
humanoid table tennis that integrates a model-based planner for ball trajectory
prediction and racket target planning with a reinforcement learning-based
whole-body controller. The planner determines striking position, velocity and
timing, while the controller generates coordinated arm and leg motions that
mimic human strikes and maintain stability and agility across consecutive
rallies. Moreover, to encourage natural movements, human motion references are
incorporated during training. We validate our system on a general-purpose
humanoid robot, achieving up to 106 consecutive shots with a human opponent and
sustained exchanges against another humanoid. These results demonstrate
real-world humanoid table tennis with sub-second reactive control, marking a
step toward agile and interactive humanoid behaviors.

</details>


### [167] [Prompt-to-Product: Generative Assembly via Bimanual Manipulation](https://arxiv.org/abs/2508.21063)
*Ruixuan Liu,Philip Huang,Ava Pun,Kangle Deng,Shobhit Aggarwal,Kevin Tang,Michelle Liu,Deva Ramanan,Jun-Yan Zhu,Jiaoyang Li,Changliu Liu*

Main category: cs.RO

TL;DR: 本文提出了一种从自然语言提示自动生成实体拼装产品的系统——Prompt-to-Product，降低了手动拼装产品的门槛和难度。


<details>
  <summary>Details</summary>
Motivation: 传统的拼装产品（如乐高）设计与构建依赖大量人工和专业知识，限制了非专家用户将创意变为现实的能力。该研究旨在简化和自动化这一流程。

Method: 提出了一套端到端自动化流程，包括（1）接受自然语言输入描述用户所需拼装产品，（2）自动生成可实际搭建的乐高积木拼装设计，（3）利用双臂机器人系统自动完成产品组装。

Result: 通过用户研究验证，该系统显著降低了用户从创意到实体产品设计与组装的难度和所需手动劳动量。

Conclusion: Prompt-to-Product系统有效地使得非专业用户能够以更低门槛将想象变为现实，为拼装类产品创作与用户体验带来了重要提升。

Abstract: Creating assembly products demands significant manual effort and expert
knowledge in 1) designing the assembly and 2) constructing the product. This
paper introduces Prompt-to-Product, an automated pipeline that generates
real-world assembly products from natural language prompts. Specifically, we
leverage LEGO bricks as the assembly platform and automate the process of
creating brick assembly structures. Given the user design requirements,
Prompt-to-Product generates physically buildable brick designs, and then
leverages a bimanual robotic system to construct the real assembly products,
bringing user imaginations into the real world. We conduct a comprehensive user
study, and the results demonstrate that Prompt-to-Product significantly lowers
the barrier and reduces manual effort in creating assembly products from
imaginative ideas.

</details>


### [168] [Learning on the Fly: Rapid Policy Adaptation via Differentiable Simulation](https://arxiv.org/abs/2508.21065)
*Jiahe Pan,Jiaxu Xing,Rudolf Reiter,Yifan Zhai,Elie Aljalbout,Davide Scaramuzza*

Main category: cs.RO

TL;DR: 本文提出了一种新的在线自适应学习框架，通过在可微分仿真中结合残差动力学学习与实时策略自适应，显著提升了从仿真到现实的控制策略迁移能力。


<details>
  <summary>Details</summary>
Motivation: 传统的仿真到现实迁移方法（如域随机化、Real2Sim2Real）在面对未曾见过的扰动或分布外情况时表现不佳，且往往需要耗时费力的离线重训练。因此，如何让策略在真实环境中能快速适应新的动力学变化与环境干扰，成为一个亟需解决的问题。

Method: 作者提出在简单动力学模型的基础上，借助真实世界数据不断微调模型以捕捉未建模效应（如载荷变化、风等），并将该过程嵌入可微分仿真框架，使策略能够通过梯度反向传播在很短时间内高效更新。该方法超越了传统强化学习，如PPO的样本效率和适应速度。

Result: 在仿真和现实中对四旋翼无人机进行了多种干扰下的敏捷控制实验。该框架能在5秒内实现对未知扰动的快速适应，悬停误差相比L1-MPC减少81%，相比DATT减少55%，且在无显式状态估计的视觉控制情境下亦表现出良好鲁棒性。

Conclusion: 本文方法突破了以往策略仅依赖多样训练或离线适应的局限，实现了面向真实环境的、几乎实时的策略自适应，对提升机器人在复杂环境下的自主能力和落地能力具有重要意义。

Abstract: Learning control policies in simulation enables rapid, safe, and
cost-effective development of advanced robotic capabilities. However,
transferring these policies to the real world remains difficult due to the
sim-to-real gap, where unmodeled dynamics and environmental disturbances can
degrade policy performance. Existing approaches, such as domain randomization
and Real2Sim2Real pipelines, can improve policy robustness, but either struggle
under out-of-distribution conditions or require costly offline retraining. In
this work, we approach these problems from a different perspective. Instead of
relying on diverse training conditions before deployment, we focus on rapidly
adapting the learned policy in the real world in an online fashion. To achieve
this, we propose a novel online adaptive learning framework that unifies
residual dynamics learning with real-time policy adaptation inside a
differentiable simulation. Starting from a simple dynamics model, our framework
refines the model continuously with real-world data to capture unmodeled
effects and disturbances such as payload changes and wind. The refined dynamics
model is embedded in a differentiable simulation framework, enabling gradient
backpropagation through the dynamics and thus rapid, sample-efficient policy
updates beyond the reach of classical RL methods like PPO. All components of
our system are designed for rapid adaptation, enabling the policy to adjust to
unseen disturbances within 5 seconds of training. We validate the approach on
agile quadrotor control under various disturbances in both simulation and the
real world. Our framework reduces hovering error by up to 81% compared to
L1-MPC and 55% compared to DATT, while also demonstrating robustness in
vision-based control without explicit state estimation.

</details>
