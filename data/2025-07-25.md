<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 90]
- [cs.CL](#cs.CL) [Total: 49]
- [cs.RO](#cs.RO) [Total: 16]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Lumina-mGPT 2.0: Stand-Alone AutoRegressive Image Modeling](https://arxiv.org/abs/2507.17801)
*Yi Xin,Juncheng Yan,Qi Qin,Zhen Li,Dongyang Liu,Shicheng Li,Victor Shea-Jay Huang,Yupeng Zhou,Renrui Zhang,Le Zhuo,Tiancheng Han,Xiaoqing Sun,Siqi Luo,Mengmeng Wang,Bin Fu,Yuewen Cao,Hongsheng Li,Guangtao Zhai,Xiaohong Liu,Yu Qiao,Peng Gao*

Main category: cs.CV

TL;DR: 本文提出Lumina-mGPT 2.0，一种完全从零开始训练的自回归生成模型，实现了媲美最先进扩散模型的高质量图像生成，在多任务和多模态生成任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前主流高质量图像生成模型大多依赖预训练组件或混合架构，限制了模型设计和授权的自由度。此外，自回归模型的灵活性和组合性尚未被充分利用，因此作者希望开发一个结构完全自由、具备强大多任务能力的自回归生成模型。

Method: Lumina-mGPT 2.0为解码器结构的自回归模型，从零开始训练，无需预训练组件。提出统一的token化方案，使模型可处理多种任务（如文本生成、图像编辑、可控合成、密集预测等）。引入高效的解码策略（如推理时缩放和speculative Jacobi采样），提升推理速度和质量。

Result: 在标准文本到图像基准测试（如GenEval和DPG）上，Lumina-mGPT 2.0表现与SOTA扩散模型（如DALL-E 3、SANA）相当，有时甚至超越这些模型。在Graph200K基准上显示了强大的多任务能力。

Conclusion: Lumina-mGPT 2.0是一种灵活且具备统一多模态生成能力的基础大模型，展现出与甚至超越当前SOTA扩散模型的生成质量，可作为多任务生成领域的重要基础方案。

Abstract: We present Lumina-mGPT 2.0, a stand-alone, decoder-only autoregressive model
that revisits and revitalizes the autoregressive paradigm for high-quality
image generation and beyond. Unlike existing approaches that rely on pretrained
components or hybrid architectures, Lumina-mGPT 2.0 is trained entirely from
scratch, enabling unrestricted architectural design and licensing freedom. It
achieves generation quality on par with state-of-the-art diffusion models such
as DALL-E 3 and SANA, while preserving the inherent flexibility and
compositionality of autoregressive modeling. Our unified tokenization scheme
allows the model to seamlessly handle a wide spectrum of tasks-including
subject-driven generation, image editing, controllable synthesis, and dense
prediction-within a single generative framework. To further boost usability, we
incorporate efficient decoding strategies like inference-time scaling and
speculative Jacobi sampling to improve quality and speed, respectively.
Extensive evaluations on standard text-to-image benchmarks (e.g., GenEval, DPG)
demonstrate that Lumina-mGPT 2.0 not only matches but in some cases surpasses
diffusion-based models. Moreover, we confirm its multi-task capabilities on the
Graph200K benchmark, with the native Lumina-mGPT 2.0 performing exceptionally
well. These results position Lumina-mGPT 2.0 as a strong, flexible foundation
model for unified multimodal generation. We have released our training details,
code, and models at https://github.com/Alpha-VLLM/Lumina-mGPT-2.0.

</details>


### [2] [SV3.3B: A Sports Video Understanding Model for Action Recognition](https://arxiv.org/abs/2507.17844)
*Sai Varun Kodathala,Yashwanth Reddy Vutukoori,Rakesh Vunnam*

Main category: cs.CV

TL;DR: 本文提出了一种高效轻量级的智能体育视频分析模型SV3.3B，能够在设备端高质量地理解和描述运动动作，性能超越大型闭源模型且计算需求更低。


<details>
  <summary>Details</summary>
Motivation: 以往体育视频分析受限于计算密集型模型，需服务器支持且难以精确理解运动员的细致动作和阶段，如准备、执行、收尾等，错失关键动作信息，难以实现广泛和实用的场景落地。

Method: SV3.3B利用创新的时序运动差分采样和自监督学习，与DWT-VGG16-LDA融合实现关键帧智能提取（每段截取16张代表帧），再借助V-DWT-JEPA2编码器（通过掩码去噪目标预训练）和大型语言模型（LLM）解码器（针对体育动作描述微调）进行端到端分析和文字生成。

Result: 在NSVA篮球数据集子集上，SV3.3B无论传统文本生成指标还是体育专项评价标准均优于GPT-4o等更大闭源模型，尤其是在信息密度、动作复杂性、测量精度等体育分析所需的技术细节指标上提升显著（ground truth 验证指标领先29.2%）。

Conclusion: SV3.3B为体育视频分析领域提供了高效智能的新范式，可低算力设备本地部署，兼具高分析准确率与丰富细节输出，对体育技术分析和应用具有重要价值。

Abstract: This paper addresses the challenge of automated sports video analysis, which
has traditionally been limited by computationally intensive models requiring
server-side processing and lacking fine-grained understanding of athletic
movements. Current approaches struggle to capture the nuanced biomechanical
transitions essential for meaningful sports analysis, often missing critical
phases like preparation, execution, and follow-through that occur within
seconds. To address these limitations, we introduce SV3.3B, a lightweight 3.3B
parameter video understanding model that combines novel temporal motion
difference sampling with self-supervised learning for efficient on-device
deployment. Our approach employs a DWT-VGG16-LDA based keyframe extraction
mechanism that intelligently identifies the 16 most representative frames from
sports sequences, followed by a V-DWT-JEPA2 encoder pretrained through
mask-denoising objectives and an LLM decoder fine-tuned for sports action
description generation. Evaluated on a subset of the NSVA basketball dataset,
SV3.3B achieves superior performance across both traditional text generation
metrics and sports-specific evaluation criteria, outperforming larger
closed-source models including GPT-4o variants while maintaining significantly
lower computational requirements. Our model demonstrates exceptional capability
in generating technically detailed and analytically rich sports descriptions,
achieving 29.2% improvement over GPT-4o in ground truth validation metrics,
with substantial improvements in information density, action complexity, and
measurement precision metrics essential for comprehensive athletic analysis.
Model Available at https://huggingface.co/sportsvision/SV3.3B.

</details>


### [3] [Detail++: Training-Free Detail Enhancer for Text-to-Image Diffusion Models](https://arxiv.org/abs/2507.17853)
*Lifeng Chen,Jiner Wang,Zihao Pan,Beier Zhu,Xiaofeng Yang,Chi Zhang*

Main category: cs.CV

TL;DR: 提出了一种无需重新训练的Detail++框架，通过分阶段细化注入策略显著提升多主体与复杂属性的文本生成图像能力。


<details>
  <summary>Details</summary>
Motivation: 当前文本生成图像模型在处理多主题和复杂属性描述时，容易出现主体混淆、细节失真等问题。该研究希望仿照人类先整体后细节的绘画过程，解决复杂提示下各主体及属性绑定准确性不足的问题。

Method: 提出Progressive Detail Injection（PDI）分阶段生成策略，将复杂提示分解为多个简化子提示，逐步引导图像生成过程。利用自注意力机制保证全局布局，结合交叉注意力和测试时引入Centroid Alignment Loss，优化属性与主体的绑定。全流程无需重新训练。

Result: 在T2I-CompBench和新提出的风格组合基准上，Detail++在生成多物体、复杂风格条件下的表现明显优于现有方法，属性绑定更准确，细节表达更优。

Conclusion: Detail++通过分阶段细化策略及属性绑定优化，显著提升了复杂文本提示的生成图像质量，对多主体复杂描述场景具有实际应用潜力。

Abstract: Recent advances in text-to-image (T2I) generation have led to impressive
visual results. However, these models still face significant challenges when
handling complex prompt, particularly those involving multiple subjects with
distinct attributes. Inspired by the human drawing process, which first
outlines the composition and then incrementally adds details, we propose
Detail++, a training-free framework that introduces a novel Progressive Detail
Injection (PDI) strategy to address this limitation. Specifically, we decompose
a complex prompt into a sequence of simplified sub-prompts, guiding the
generation process in stages. This staged generation leverages the inherent
layout-controlling capacity of self-attention to first ensure global
composition, followed by precise refinement. To achieve accurate binding
between attributes and corresponding subjects, we exploit cross-attention
mechanisms and further introduce a Centroid Alignment Loss at test time to
reduce binding noise and enhance attribute consistency. Extensive experiments
on T2I-CompBench and a newly constructed style composition benchmark
demonstrate that Detail++ significantly outperforms existing methods,
particularly in scenarios involving multiple objects and complex stylistic
conditions.

</details>


### [4] [FishDet-M: A Unified Large-Scale Benchmark for Robust Fish Detection and CLIP-Guided Model Selection in Diverse Aquatic Visual Domains](https://arxiv.org/abs/2507.17859)
*Muayad Abujabal,Lyes Saad Saoud,Irfan Hussain*

Main category: cs.CV

TL;DR: 该论文提出了FishDet-M，这是迄今为止最大的统一鱼类检测基准数据集，整合了13个公开水下数据集，并进行了详细的模型评测和创新性模型选择方法的探索。


<details>
  <summary>Details</summary>
Motivation: 水下鱼类检测对于生态监测、水产养殖自动化和机器人感知非常关键，但目前受限于数据集分散、成像条件不一和评估标准不统一等问题，导致实际应用困难。

Method: 作者收集并统一了13个多样化水域环境下的公开鱼类数据集，全部采用COCO格式注释。系统性地评测了28种主流检测算法（包括YOLOv8–v12、R-CNN系列、DETR等），采用标准检测指标和效率分析。此外，提出了一种基于CLIP的模型选择框架，可根据输入图片动态匹配最合适的检测器，实现零样本条件下的高效模型选取。

Result: 详细基准测试展示了不同模型在FishDet-M上的性能分布、精度与效率之间的权衡，验证了CLIP模型选择机制在无需集成条件下依然获得优异检测效果。

Conclusion: FishDet-M为复杂水下场景的鱼类目标检测研究提供了统一、可复现的评测平台，有望推动水下计算机视觉和智能海洋系统的发展。所有相关数据、模型和工具均已公开，为社区后续研究提供便利。

Abstract: Accurate fish detection in underwater imagery is essential for ecological
monitoring, aquaculture automation, and robotic perception. However, practical
deployment remains limited by fragmented datasets, heterogeneous imaging
conditions, and inconsistent evaluation protocols. To address these gaps, we
present \textit{FishDet-M}, the largest unified benchmark for fish detection,
comprising 13 publicly available datasets spanning diverse aquatic environments
including marine, brackish, occluded, and aquarium scenes. All data are
harmonized using COCO-style annotations with both bounding boxes and
segmentation masks, enabling consistent and scalable cross-domain evaluation.
We systematically benchmark 28 contemporary object detection models, covering
the YOLOv8 to YOLOv12 series, R-CNN based detectors, and DETR based models.
Evaluations are conducted using standard metrics including mAP, mAP@50, and
mAP@75, along with scale-specific analyses (AP$_S$, AP$_M$, AP$_L$) and
inference profiling in terms of latency and parameter count. The results
highlight the varying detection performance across models trained on FishDet-M,
as well as the trade-off between accuracy and efficiency across models of
different architectures. To support adaptive deployment, we introduce a
CLIP-based model selection framework that leverages vision-language alignment
to dynamically identify the most semantically appropriate detector for each
input image. This zero-shot selection strategy achieves high performance
without requiring ensemble computation, offering a scalable solution for
real-time applications. FishDet-M establishes a standardized and reproducible
platform for evaluating object detection in complex aquatic scenes. All
datasets, pretrained models, and evaluation tools are publicly available to
facilitate future research in underwater computer vision and intelligent marine
systems.

</details>


### [5] [Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis](https://arxiv.org/abs/2507.17860)
*Ko Watanabe. Stanislav Frolov. Adriano Lucieri. Andreas Dengel*

Main category: cs.CV

TL;DR: 本文研究了使用生成式AI（GenAI）模型，通过高质量合成数据来评估皮肤癌检测深度学习系统的公平性。结果显示，该方法有价值，但在评价模型与合成数据差异较大时，验证公平性会变得困难。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习应用于边缘设备推动皮肤癌筛查的发展，其固有的及不可预见的算法偏见问题也日益突出。为保障此类系统对各类人群（如性别、年龄、种族等）的公平性，对其评估体系的公平性进行检测和改进迫在眉睫。

Method: 本文利用先进的生成式AI模型LightningDiT，合成高保真度的皮肤影像数据，用来评估公开可用的黑色素瘤（皮肤癌）自动分类器在不同个人信息（性别、年龄、种族等）和少数群体上的公平性表现。

Result: 研究结果表明，基于高真实性合成数据进行公平性评估是非常有前景的方法。但如果公平性评价所用检测模型的训练数据与合成影像存在较大差异，则结果的可信度会下降。

Conclusion: 本研究提出的方法为使用合成数据评估和提升医学影像生成AI系统的公平性，开辟了新思路。尽管仍面临实际的挑战，但这一方法具有很高的应用潜力和研究意义。

Abstract: Recent advancements in Deep Learning and its application on the edge hold
great potential for the revolution of routine screenings for skin cancers like
Melanoma. Along with the anticipated benefits of this technology, potential
dangers arise from unforseen and inherent biases. Thus, assessing and improving
the fairness of such systems is of utmost importance. A key challenge in
fairness assessment is to ensure that the evaluation dataset is sufficiently
representative of different Personal Identifiable Information (PII) (sex, age,
and race) and other minority groups. Against the backdrop of this challenge,
this study leverages the state-of-the-art Generative AI (GenAI) LightningDiT
model to assess the fairness of publicly available melanoma classifiers. The
results suggest that fairness assessment using highly realistic synthetic data
is a promising direction. Yet, our findings indicate that verifying fairness
becomes difficult when the melanoma-detection model used for evaluation is
trained on data that differ from the dataset underpinning the synthetic images.
Nonetheless, we propose that our approach offers a valuable new avenue for
employing synthetic data to gauge and enhance fairness in medical-imaging GenAI
systems.

</details>


### [6] [DiNAT-IR: Exploring Dilated Neighborhood Attention for High-Quality Image Restoration](https://arxiv.org/abs/2507.17892)
*Hanzhou Liu,Binghan Li,Chengkai Liu,Mi Lu*

Main category: cs.CV

TL;DR: 该论文提出DiNAT-IR，一种结合全局与局部关注的高效图像复原Transformer架构，有效提升高分辨率下的性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型自注意力机制虽能捕捉远距离依赖，但计算量巨大，在高分辨率图像复原中扩展性差。虽然Restormer采用通道自注意力，可提升效率，但对局部细节捕捉不足，容易漏检局部伪影，影响高质量复原。为此，研究聚焦于兼顾效率与复原质量。

Method: 提出膨胀邻域注意力（DiNA），结合滑动窗口与多种膨胀因子，扩展感受野，融合全局与局部信息。同时针对仅用局部注意力导致的全局上下文建模受限，设计通道感知模块，提升全局信息编码能力。最终构建DiNAT-IR架构用于图像复原。

Result: 所提DiNAT-IR Transformer模型在多项公开图像复原基准任务上取得有竞争力的结果，兼顾高分辨率下的效率和复原质量。

Conclusion: DiNAT-IR通过将膨胀邻域注意力与通道感知模块结合，实现高效高质量的低级视觉任务图像复原，在全局与局部信息融合策略上为该领域提供新思路。

Abstract: Transformers, with their self-attention mechanisms for modeling long-range
dependencies, have become a dominant paradigm in image restoration tasks.
However, the high computational cost of self-attention limits scalability to
high-resolution images, making efficiency-quality trade-offs a key research
focus. To address this, Restormer employs channel-wise self-attention, which
computes attention across channels instead of spatial dimensions. While
effective, this approach may overlook localized artifacts that are crucial for
high-quality image restoration. To bridge this gap, we explore Dilated
Neighborhood Attention (DiNA) as a promising alternative, inspired by its
success in high-level vision tasks. DiNA balances global context and local
precision by integrating sliding-window attention with mixed dilation factors,
effectively expanding the receptive field without excessive overhead. However,
our preliminary experiments indicate that directly applying this global-local
design to the classic deblurring task hinders accurate visual restoration,
primarily due to the constrained global context understanding within local
attention. To address this, we introduce a channel-aware module that
complements local attention, effectively integrating global context without
sacrificing pixel-level precision. The proposed DiNAT-IR, a Transformer-based
architecture specifically designed for image restoration, achieves competitive
results across multiple benchmarks, offering a high-quality solution for
diverse low-level computer vision problems.

</details>


### [7] [AFRDA: Attentive Feature Refinement for Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2507.17957)
*Md. Al-Masrur Khan,Durgakant Pushp,Lantao Liu*

Main category: cs.CV

TL;DR: 本论文提出了自适应特征细化（AFR）模块，提升了无监督域自适应语义分割（UDA-SS）在复杂区域的表现，通过高分辨率与低分辨率特征融合实现更好的分割精度，取得了新的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有UDA-SS方法难以同时顾及局部细节和全局语境，导致复杂区域分割误差。本论文旨在解决细粒度结构与整体语义信息平衡不足的问题，提升域迁移分割性能。

Method: 提出AFR模块：用低分辨率语义先验细化高分辨率特征，融合高频分量提升边界检测，通过不确定性引导的注意力机制动态平衡局部和全局。该模块结构轻量，能无缝集成至HRDA等UDA框架。

Result: 实验在GTA V到Cityscapes和Synthia到Cityscapes任务上，分别提升了1.05%和1.04%的mIoU，达到当前最优性能。

Conclusion: AFR模块能有效增强UDA-SS模型的分割能力，尤其在复杂及边界区域，且适用性强，为无监督域自适应分割带来新的性能提升方案。

Abstract: In Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS), a model is
trained on labeled source domain data (e.g., synthetic images) and adapted to
an unlabeled target domain (e.g., real-world images) without access to target
annotations. Existing UDA-SS methods often struggle to balance fine-grained
local details with global contextual information, leading to segmentation
errors in complex regions. To address this, we introduce the Adaptive Feature
Refinement (AFR) module, which enhances segmentation accuracy by refining
highresolution features using semantic priors from low-resolution logits. AFR
also integrates high-frequency components, which capture fine-grained
structures and provide crucial boundary information, improving object
delineation. Additionally, AFR adaptively balances local and global information
through uncertaintydriven attention, reducing misclassifications. Its
lightweight design allows seamless integration into HRDA-based UDA methods,
leading to state-of-the-art segmentation performance. Our approach improves
existing UDA-SS methods by 1.05% mIoU on GTA V --> Cityscapes and 1.04% mIoU on
Synthia-->Cityscapes. The implementation of our framework is available at:
https://github.com/Masrur02/AFRDA

</details>


### [8] [OPEN: A Benchmark Dataset and Baseline for Older Adult Patient Engagement Recognition in Virtual Rehabilitation Learning Environments](https://arxiv.org/abs/2507.17959)
*Ali Abedi,Sadaf Safa,Tracey J. F. Colella,Shehroz S. Khan*

Main category: cs.CV

TL;DR: 该论文提出并发布了开放的OPEN数据集，用于老年人在虚拟学习和远程康复中的AI驱动的参与度识别，是目前同类中规模最大的公开数据集。


<details>
  <summary>Details</summary>
Motivation: 在虚拟学习和远程康复等场景中，参与度对结果影响显著，然而目前缺乏针对老年群体的真实、长期、上下文相关的数据集，限制了自动化参与度识别的发展和效果。

Method: 收集了11位老年人在为期六周的虚拟心脏康复课程中参与远程小组学习的35小时数据，仅发布了提取的关键信息（如面部、手部和身体关键点、情感/行为特征），并配有参与标签、情感与行为标注及场域相关信息。作者还用多种机器学习和深度学习模型在该数据集上进行了参与度识别实验。

Result: 通过多种机器学习与深度学习模型，最高可实现81%的参与度识别准确率。

Conclusion: OPEN数据集为面向老年群体的个性化虚拟学习参与度建模和人工智能识别提供了可扩展基础，对虚拟学习与康复领域的研究具有重要推动作用。

Abstract: Engagement in virtual learning is essential for participant satisfaction,
performance, and adherence, particularly in online education and virtual
rehabilitation, where interactive communication plays a key role. Yet,
accurately measuring engagement in virtual group settings remains a challenge.
There is increasing interest in using artificial intelligence (AI) for
large-scale, real-world, automated engagement recognition. While engagement has
been widely studied in younger academic populations, research and datasets
focused on older adults in virtual and telehealth learning settings remain
limited. Existing methods often neglect contextual relevance and the
longitudinal nature of engagement across sessions. This paper introduces OPEN
(Older adult Patient ENgagement), a novel dataset supporting AI-driven
engagement recognition. It was collected from eleven older adults participating
in weekly virtual group learning sessions over six weeks as part of cardiac
rehabilitation, producing over 35 hours of data, making it the largest dataset
of its kind. To protect privacy, raw video is withheld; instead, the released
data include facial, hand, and body joint landmarks, along with affective and
behavioral features extracted from video. Annotations include binary engagement
states, affective and behavioral labels, and context-type indicators, such as
whether the instructor addressed the group or an individual. The dataset offers
versions with 5-, 10-, 30-second, and variable-length samples. To demonstrate
utility, multiple machine learning and deep learning models were trained,
achieving engagement recognition accuracy of up to 81 percent. OPEN provides a
scalable foundation for personalized engagement modeling in aging populations
and contributes to broader engagement recognition research.

</details>


### [9] [Bearded Dragon Activity Recognition Pipeline: An AI-Based Approach to Behavioural Monitoring](https://arxiv.org/abs/2507.17987)
*Arsen Yermukan,Pedro Machado,Feliciano Domingos,Isibor Kennedy Ihianle,Jordan J. Bird,Stefano S. K. Kaburu,Samantha J. Ward*

Main category: cs.CV

TL;DR: 本项目提出了一套自动化系统，利用YOLO目标检测模型实时分析视频，识别胡须龙的晒背和狩猎行为，取得较高的准确率，但狩猎检测还需改进。


<details>
  <summary>Details</summary>
Motivation: 传统人工监测胡须龙行为方法耗时且易出错，因此亟需高效、自动化的智能监测手段提升研究效率和数据准确性。

Method: 作者基于YOLO五种变体（v5、v7、v8、v11、v12），在包含胡须龙、加热灯和蟋蟀的定制数据集（共1200张图像）上训练模型。经比较，选取YOLOv8s作为性能最佳的模型。系统通过逐帧获取目标坐标、时间插值保持连续性、再结合基于规则的逻辑判别动物行为。

Result: YOLOv8s在对象检测上获得mAP@0.5:0.95=0.855的准确率。对晒背行为检测可靠，但狩猎（主要依赖蟋蟀检测）准确率较低，蟋蟀检测表现不佳，mAP@0.5=0.392。

Conclusion: 该自动化监测系统能大幅提升爬行动物行为研究的效率和数据质量，适用于受控环境下的规模化应用。后续可通过丰富小对象（如蟋蟀）检测相关数据集或引入专用的小目标检测方法进一步提升系统表现。

Abstract: Traditional monitoring of bearded dragon (Pogona Viticeps) behaviour is
time-consuming and prone to errors. This project introduces an automated system
for real-time video analysis, using You Only Look Once (YOLO) object detection
models to identify two key behaviours: basking and hunting. We trained five
YOLO variants (v5, v7, v8, v11, v12) on a custom, publicly available dataset of
1200 images, encompassing bearded dragons (600), heating lamps (500), and
crickets (100). YOLOv8s was selected as the optimal model due to its superior
balance of accuracy (mAP@0.5:0.95 = 0.855) and speed. The system processes
video footage by extracting per-frame object coordinates, applying temporal
interpolation for continuity, and using rule-based logic to classify specific
behaviours. Basking detection proved reliable. However, hunting detection was
less accurate, primarily due to weak cricket detection (mAP@0.5 = 0.392).
Future improvements will focus on enhancing cricket detection through expanded
datasets or specialised small-object detectors. This automated system offers a
scalable solution for monitoring reptile behaviour in controlled environments,
significantly improving research efficiency and data quality.

</details>


### [10] [AG-VPReID.VIR: Bridging Aerial and Ground Platforms for Video-based Visible-Infrared Person Re-ID](https://arxiv.org/abs/2507.17995)
*Huy Nguyen,Kien Nguyen,Akila Pemasiri,Akmal Jahan,Clinton Fookes,Sridha Sridharan*

Main category: cs.CV

TL;DR: 本文提出了首个涵盖空中与地面、可见光与红外双模态的视频行人再识别数据集AG-VPReID.VIR，并提出一种三分支架构TCC-VPReID显著提升复杂场景下识别精度。


<details>
  <summary>Details</summary>
Motivation: 传统的跨模态行人再识别多集中于地面视角，面临视角局限、遮挡和监控覆盖不足等问题。空中（无人机）视角能够弥补地面监控的缺陷，但缺乏相应的跨模态数据集和方法推动研究进展。

Method: 1）构建AG-VPReID.VIR数据集，涵盖无人机与地面摄像头、RGB与红外四类组合，具有丰富身份数和跟踪数据；2）提出TCC-VPReID三分支网络，分别聚焦风格鲁棒特征、基于记忆的跨视角适应和时序建模，从而缓解多平台、多模态的特征鸿沟。

Result: 实验表明AG-VPReID.VIR数据集具有比现有数据集更显著的挑战性。所提出的TCC-VPReID方法在多种协议下均显著超越现有方法。

Conclusion: 面向全天候和多场景实际监控需求，空地双视角跨模态行人再识别问题值得关注。AG-VPReID.VIR为该研究方向提供了关键数据支持，TCC-VPReID为多域深度融合提供高效解决方案。

Abstract: Person re-identification (Re-ID) across visible and infrared modalities is
crucial for 24-hour surveillance systems, but existing datasets primarily focus
on ground-level perspectives. While ground-based IR systems offer nighttime
capabilities, they suffer from occlusions, limited coverage, and vulnerability
to obstructions--problems that aerial perspectives uniquely solve. To address
these limitations, we introduce AG-VPReID.VIR, the first aerial-ground
cross-modality video-based person Re-ID dataset. This dataset captures 1,837
identities across 4,861 tracklets (124,855 frames) using both UAV-mounted and
fixed CCTV cameras in RGB and infrared modalities. AG-VPReID.VIR presents
unique challenges including cross-viewpoint variations, modality discrepancies,
and temporal dynamics. Additionally, we propose TCC-VPReID, a novel
three-stream architecture designed to address the joint challenges of
cross-platform and cross-modality person Re-ID. Our approach bridges the domain
gaps between aerial-ground perspectives and RGB-IR modalities, through
style-robust feature learning, memory-based cross-view adaptation, and
intermediary-guided temporal modeling. Experiments show that AG-VPReID.VIR
presents distinctive challenges compared to existing datasets, with our
TCC-VPReID framework achieving significant performance gains across multiple
evaluation protocols. Dataset and code are available at
https://github.com/agvpreid25/AG-VPReID.VIR.

</details>


### [11] [Exploring the interplay of label bias with subgroup size and separability: A case study in mammographic density classification](https://arxiv.org/abs/2507.17996)
*Emma A. M. Stanley,Raghav Mehta,Mélanie Roschewitz,Nils D. Forkert,Ben Glocker*

Main category: cs.CV

TL;DR: 本文研究了医学影像数据集中针对特定亚群体（如成像设备不同）存在标签偏差（label bias）时，对深度学习模型学习特征与性能的影响。结果显示，标签偏差会导致模型特征表征发生显著偏移，并且这种影响与亚群体的规模和可分性密切相关。


<details>
  <summary>Details</summary>
Motivation: 当前医学AI公平性研究多关注整体数据不平衡、模型偏见等，但针对数据标签偏差如何影响特定亚群体（比如某台成像设备下的病人是否被系统性误标），相关研究较少。本文动机在于深入理解标签偏差对医学AI系统公平性的影响机制。

Method: 作者使用EMBED数据库，设定二分类任务（组织密度），通过人为模拟亚群体标签偏差（针对不同厂商的成像设备组成的可分与不可分亚群体），训练深度学习模型，并分析模型学习到的特征空间分布及分类性能，且对比了使用干净与有偏验证集设定阈值时不同情形。

Result: 标签偏差会导致模型学习到的特征表征发生明显漂移，且漂移程度依赖于受影响亚群体的比例和可分性。当标签偏差影响可分的多数群体时，如果验证集也存在偏差，亚群体的真正率显著降低（如0.898降至0.518）。

Conclusion: 标签偏差对医学影像AI中特定亚群体的公平性具有显著影响，尤其在受影响亚群体较大且易分情况下更严重。合理的验证集和阈值选择至关重要。该研究为理解并缓解标签偏差带来的公平性问题提供了理论基础。

Abstract: Systematic mislabelling affecting specific subgroups (i.e., label bias) in
medical imaging datasets represents an understudied issue concerning the
fairness of medical AI systems. In this work, we investigated how size and
separability of subgroups affected by label bias influence the learned features
and performance of a deep learning model. Therefore, we trained deep learning
models for binary tissue density classification using the EMory BrEast imaging
Dataset (EMBED), where label bias affected separable subgroups (based on
imaging manufacturer) or non-separable "pseudo-subgroups". We found that
simulated subgroup label bias led to prominent shifts in the learned feature
representations of the models. Importantly, these shifts within the feature
space were dependent on both the relative size and the separability of the
subgroup affected by label bias. We also observed notable differences in
subgroup performance depending on whether a validation set with clean labels
was used to define the classification threshold for the model. For instance,
with label bias affecting the majority separable subgroup, the true positive
rate for that subgroup fell from 0.898, when the validation set had clean
labels, to 0.518, when the validation set had biased labels. Our work
represents a key contribution toward understanding the consequences of label
bias on subgroup fairness in medical imaging AI.

</details>


### [12] [Registration beyond Points: General Affine Subspace Alignment via Geodesic Distance on Grassmann Manifold](https://arxiv.org/abs/2507.17998)
*Jaeho Shin,Hyeonjae Gil,Junwoo Jang,Maani Ghaffari,Ayoung Kim*

Main category: cs.CV

TL;DR: 本文针对仿射Grassmann流形上特征（如直线、平面）之间的距离优化问题，首次推导了一个可优化的距离函数，能够直接用于刚体变换下的配准任务。


<details>
  <summary>Details</summary>
Motivation: 传统仿射Grassmann流形方法虽可准确衡量特征之间的距离，但未能将其距离表述为刚体变换的显式函数，从而难以应用于以优化为核心的配准算法。缺乏可优化代价函数严重制约了其在实际计算机视觉任务中的应用。

Method: 作者通过严格数学推导，证明了高维线性子空间的基可以作为距离函数的显式表达，进一步提出一个基于变换后基的可优化代价函数，适用于任意仿射子空间的配准问题。此外，该函数被扩展应用于优化内点集的BnB（Branch-and-Bound）框架下。

Result: 实验表明，所提出的新代价函数不仅优化收敛性更好，还在多项计算机视觉任务中优于基于向量参数的方法。此外，代码已开源，便于社区复现。

Conclusion: 本文提出了首个针对仿射Grassmann流形下刚体变换的可优化距离函数，有效促进了其在配准等实际任务中的应用，并为今后相关特征匹配和最优化方法开发打下理论与实践基础。

Abstract: Affine Grassmannian has been favored for expressing proximity between lines
and planes due to its theoretical exactness in measuring distances among
features. Despite this advantage, the existing method can only measure the
proximity without yielding the distance as an explicit function of rigid body
transformation. Thus, an optimizable distance function on the manifold has
remained underdeveloped, stifling its application in registration problems.
This paper is the first to explicitly derive an optimizable cost function
between two Grassmannian features with respect to rigid body transformation
($\mathbf{R}$ and $\mathbf{t}$). Specifically, we present a rigorous
mathematical proof demonstrating that the bases of high-dimensional linear
subspaces can serve as an explicit representation of the cost. Finally, we
propose an optimizable cost function based on the transformed bases that can be
applied to the registration problem of any affine subspace. Compared to vector
parameter-based approaches, our method is able to find a globally optimal
solution by directly minimizing the geodesic distance which is agnostic to
representation ambiguity. The resulting cost function and its extension to the
inlier-set maximizing \ac{BnB} solver have been demonstrated to improve the
convergence of existing solutions or outperform them in various computer vision
tasks. The code is available on
https://github.com/joomeok/GrassmannRegistration.

</details>


### [13] [GRR-CoCa: Leveraging LLM Mechanisms in Multimodal Model Architectures](https://arxiv.org/abs/2507.18009)
*Jake R. Patock,Nicole Catherine Lewis,Kevin McCoy,Christina Gomez,Canling Chen,Lorenzo Luzi*

Main category: cs.CV

TL;DR: 本文提出了一种改进的多模态模型GRR-CoCa，通过在文本解码器和ViT编码器中引入高斯误差门控线性单元、均方根归一化和旋转位置编码，从而提升了模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态生成模型在结构上落后于先进的大语言模型（LLMs），本文旨在引入LLM中有效的结构创新以提升多模态模型的表现。

Method: 在CoCa模型的文本解码器和ViT编码器中，分别加入了高斯误差门控线性单元、均方根归一化和旋转位置编码，并将该改进模型GRR-CoCa与仅修改文本解码器的Baseline CoCa进行对比，按标准流程进行预训练和微调，并在多项任务上评估。

Result: GRR-CoCa在预训练和微调数据集上均显著优于Baseline CoCa。在预训练时，对比损失降低27.25%、困惑度降低3.71%、CoCa损失降低7.15%；微调时平均对比损失降低13.66%、困惑度降低5.18%、CoCa损失降低5.55%。

Conclusion: 结构性改进显著提升了GRR-CoCa的性能，证明新结构能增强视觉-语言领域的泛化能力和表现。

Abstract: State-of-the-art (SOTA) image and text generation models are multimodal
models that have many similarities to large language models (LLMs). Despite
achieving strong performances, leading foundational multimodal model
architectures frequently lag behind the architectural sophistication of
contemporary LLMs. We propose GRR-CoCa, an improved SOTA Contrastive Captioner
(CoCa) model that incorporates Gaussian error gated linear units, root mean
squared normalization, and rotary positional embedding into the textual
decoders and the vision transformer (ViT) encoder. Each architectural
modification has been shown to improve model performance in LLMs, but has yet
to be adopted in CoCa. We benchmarked GRR-CoCa against Baseline CoCa, a model
with the same modified textual decoders but with CoCa's original ViT encoder.
We used standard pretraining and fine-tuning workflows to benchmark the models
on contrastive and generative tasks. Our GRR-CoCa significantly outperformed
Baseline CoCa on the pretraining dataset and three diverse fine-tuning
datasets. Pretraining improvements were 27.25% in contrastive loss, 3.71% in
perplexity, and 7.15% in CoCa loss. The average fine-tuning improvements were
13.66% in contrastive loss, 5.18% in perplexity, and 5.55% in CoCa loss. We
show that GRR-CoCa's modified architecture improves performance and
generalization across vision-language domains.

</details>


### [14] [Celeb-DF++: A Large-scale Challenging Video DeepFake Benchmark for Generalizable Forensics](https://arxiv.org/abs/2507.18015)
*Yuezun Li,Delong Zhu,Xinjie Cui,Siwei Lyu*

Main category: cs.CV

TL;DR: 本文介绍了新的视频DeepFake基准数据集Celeb-DF++，其涵盖三类常见伪造场景和22种主流DeepFake方法，专为提升伪造检测模型的泛化能力设计，同时对现有24种检测方法的泛化能力进行了评估。


<details>
  <summary>Details</summary>
Motivation: 现有DeepFake数据集虽然规模大，但伪造手法单一，难以支撑对新型、未知DeepFake类型的检测模型研发，制约了模型的泛化能力提升。因此，亟需构建包含多种伪造类型和方法的大规模数据集。

Method: 在原有Celeb-DF数据集基础上，作者构建了新的Celeb-DF++数据集，包含三类主流伪造场景（人脸替换、表情重演、虚拟开口说话）及22种流行DeepFake生成方法，涵盖不同架构、生成流程和目标区域。此外，设定了针对泛化能力的评测协议，系统评估24种近期检测方法。

Result: 实验表明，现有主流DeepFake检测方法在Celeb-DF++这个多样性更高的数据集上普遍存在泛化能力不足的问题，揭示了当前方法难以应对真实环境中多种多样伪造视频的挑战。

Conclusion: Celeb-DF++为提升DeepFake检测方法的泛化能力提供了标准和挑战，其高多样性和高质量特性使其成为评估和推动新一代伪造检测模型发展的重要基准。

Abstract: The rapid advancement of AI technologies has significantly increased the
diversity of DeepFake videos circulating online, posing a pressing challenge
for \textit{generalizable forensics}, \ie, detecting a wide range of unseen
DeepFake types using a single model. Addressing this challenge requires
datasets that are not only large-scale but also rich in forgery diversity.
However, most existing datasets, despite their scale, include only a limited
variety of forgery types, making them insufficient for developing generalizable
detection methods. Therefore, we build upon our earlier Celeb-DF dataset and
introduce {Celeb-DF++}, a new large-scale and challenging video DeepFake
benchmark dedicated to the generalizable forensics challenge. Celeb-DF++ covers
three commonly encountered forgery scenarios: Face-swap (FS), Face-reenactment
(FR), and Talking-face (TF). Each scenario contains a substantial number of
high-quality forged videos, generated using a total of 22 various recent
DeepFake methods. These methods differ in terms of architectures, generation
pipelines, and targeted facial regions, covering the most prevalent DeepFake
cases witnessed in the wild. We also introduce evaluation protocols for
measuring the generalizability of 24 recent detection methods, highlighting the
limitations of existing detection methods and the difficulty of our new
dataset.

</details>


### [15] [DSFormer: A Dual-Scale Cross-Learning Transformer for Visual Place Recognition](https://arxiv.org/abs/2507.18444)
*Haiyang Jiang,Songhao Piao,Chao Gao,Lei Yu,Liguo Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种结合Transformer的双尺度特征融合模块DSFormer与创新分块聚类策略，用于提升视觉定位系统在变化环境中的鲁棒性和效率，实验优于主流方法。


<details>
  <summary>Details</summary>
Motivation: VPR在环境与视角变化下很难保持稳定识别性能，当前主流方法对小数据集和通用性面临限制，因此迫切需要新机制提升鲁棒性和效率。

Method: 1. 提出DSFormer模块：基于Transformer框架，在CNN后两层分别提取语义和空间特征，并通过双向信息流和自/交叉注意力机制高效融合；2. 创新分块聚类策略：对大规模训练集（如SF-XL）进行多视角分割重组，优化数据组织，提高对视点变化的鲁棒性；二者结合以获得优质全局嵌入表示。

Result: 新方法能在减少约30%的训练数据量前提下，生成更健壮的全局特征，在大多数VPR基准数据集上表现优于DELG、Patch-NetVLAD、TransVPR等方法，且512维特征下计算效率大幅提升。

Conclusion: 该框架在保证或提升定位识别性能的同时，显著减少所需训练数据和计算量，为视觉定位任务中的泛化与实际应用带来新突破。

Abstract: Visual Place Recognition (VPR) is crucial for robust mobile robot
localization, yet it faces significant challenges in maintaining reliable
performance under varying environmental conditions and viewpoints. To address
this, we propose a novel framework that integrates Dual-Scale-Former
(DSFormer), a Transformer-based cross-learning module, with an innovative block
clustering strategy. DSFormer enhances feature representation by enabling
bidirectional information transfer between dual-scale features extracted from
the final two CNN layers, capturing both semantic richness and spatial details
through self-attention for long-range dependencies within each scale and shared
cross-attention for cross-scale learning. Complementing this, our block
clustering strategy repartitions the widely used San Francisco eXtra Large
(SF-XL) training dataset from multiple distinct perspectives, optimizing data
organization to further bolster robustness against viewpoint variations.
Together, these innovations not only yield a robust global embedding adaptable
to environmental changes but also reduce the required training data volume by
approximately 30\% compared to previous partitioning methods. Comprehensive
experiments demonstrate that our approach achieves state-of-the-art performance
across most benchmark datasets, surpassing advanced reranking methods like
DELG, Patch-NetVLAD, TransVPR, and R2Former as a global retrieval solution
using 512-dim global descriptors, while significantly improving computational
efficiency.

</details>


### [16] [High-fidelity 3D Gaussian Inpainting: preserving multi-view consistency and photorealistic details](https://arxiv.org/abs/2507.18023)
*Jun Zhou,Dinghao Li,Nannan Li,Mingjie Wang*

Main category: cs.CV

TL;DR: 本文提出了一种用于3D场景修复的新颖高斯点云3D修复框架，大幅提升了修补后场景的真实感和多视角一致性。


<details>
  <summary>Details</summary>
Motivation: 虽然NeRF和3D Gaussian Splatting已在多视角3D重建和新视图合成领域取得进展，但3D场景修补因其结构不规则性及需保证多视角一致性，依然非常困难。

Method: 提出了一种3D高斯点修补方法，通过稀疏修补视角重建完整3D场景。方法包含自动遮罩优化流程（如高斯场景滤波和反投影），以精细定位遮挡区域并恢复真实边界；同时，提出了基于不确定性的区域优化策略，训练时动态衡量各区域在多视角图片中的重要性，提升细节和一致性。

Result: 在多种数据集上的实验表明，该方法在视觉质量和多视图一致性上均优于现有最新方法。

Conclusion: 本文的方法不仅提升了3D场景修补的真实感，还改善了多视角一致性，为3D内容自动修补提供了强有力的技术支持。

Abstract: Recent advancements in multi-view 3D reconstruction and novel-view synthesis,
particularly through Neural Radiance Fields (NeRF) and 3D Gaussian Splatting
(3DGS), have greatly enhanced the fidelity and efficiency of 3D content
creation. However, inpainting 3D scenes remains a challenging task due to the
inherent irregularity of 3D structures and the critical need for maintaining
multi-view consistency. In this work, we propose a novel 3D Gaussian inpainting
framework that reconstructs complete 3D scenes by leveraging sparse inpainted
views. Our framework incorporates an automatic Mask Refinement Process and
region-wise Uncertainty-guided Optimization. Specifically, we refine the
inpainting mask using a series of operations, including Gaussian scene
filtering and back-projection, enabling more accurate localization of occluded
regions and realistic boundary restoration. Furthermore, our Uncertainty-guided
Fine-grained Optimization strategy, which estimates the importance of each
region across multi-view images during training, alleviates multi-view
inconsistencies and enhances the fidelity of fine details in the inpainted
results. Comprehensive experiments conducted on diverse datasets demonstrate
that our approach outperforms existing state-of-the-art methods in both visual
quality and view consistency.

</details>


### [17] [Emotion Recognition from Skeleton Data: A Comprehensive Survey](https://arxiv.org/abs/2507.18026)
*Haifeng Lu,Jiuyi Chen,Zhen Zhang,Ruida Liu,Runhao Zeng,Xiping Hu*

Main category: cs.CV

TL;DR: 本综述系统梳理了基于骨架的情感识别技术，在介绍情感心理模型和相关公开数据集的基础上，全面分析了现有方法与应用前景。


<details>
  <summary>Details</summary>
Motivation: 现有情感识别方法多依赖面部表情或生理信号，存在隐私和环境适应性问题。随着3D骨架采集与姿态估计算法的发展，基于身体动作的情感识别变得可行且更具隐私保护意义，因此需要对骨架感知领域的技术现状进行系统性梳理。

Method: 论文首先介绍情感心理模型及情感与动作的关系，接着系统总结了相关公开数据集，并根据数据采集和情感标注方法进行对比。随后，将骨架情感识别方法分为基于姿态与步态两大类，并从数据驱动和技术实现两角度进行分析。提出统一技术分类体系，细分为传统方法、Feat2Net、FeatFusionNet、End2EndNet四类，并对每类代表性工作进行评述及数据集benchmark结果对比。

Result: 论文梳理了各类骨架情感识别方法的优缺点与适用场景，对比了常用benchmark上的性能表现。同时探讨了该技术在心理健康评估（如抑郁、自闭症检测）等领域的应用前景。

Conclusion: 基于骨架的情感识别技术在隐私保护与实际应用方面具有广阔前景，未来应关注情感标签准确性、跨域通用性和多模态融合等方向，以推动其在心理健康等实际场景的落地。

Abstract: Emotion recognition through body movements has emerged as a compelling and
privacy-preserving alternative to traditional methods that rely on facial
expressions or physiological signals. Recent advancements in 3D skeleton
acquisition technologies and pose estimation algorithms have significantly
enhanced the feasibility of emotion recognition based on full-body motion. This
survey provides a comprehensive and systematic review of skeleton-based emotion
recognition techniques. First, we introduce psychological models of emotion and
examine the relationship between bodily movements and emotional expression.
Next, we summarize publicly available datasets, highlighting the differences in
data acquisition methods and emotion labeling strategies. We then categorize
existing methods into posture-based and gait-based approaches, analyzing them
from both data-driven and technical perspectives. In particular, we propose a
unified taxonomy that encompasses four primary technical paradigms: Traditional
approaches, Feat2Net, FeatFusionNet, and End2EndNet. Representative works
within each category are reviewed and compared, with benchmarking results
across commonly used datasets. Finally, we explore the extended applications of
emotion recognition in mental health assessment, such as detecting depression
and autism, and discuss the open challenges and future research directions in
this rapidly evolving field.

</details>


### [18] [ViGText: Deepfake Image Detection with Vision-Language Model Explanations and Graph Neural Networks](https://arxiv.org/abs/2507.18031)
*Ahmad ALBarqawi,Mahmoud Nazzal,Issa Khalil,Abdallah Khreishah,NhatHai Phan*

Main category: cs.CV

TL;DR: ViGText提出了一种结合视觉大模型文本解释与图神经网络的方法，有效提升了深度伪造检测的泛化性与鲁棒性，特别针对复杂、定制化的deepfake具有显著优势。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术的飞速发展对媒体真实性构成威胁，而传统deepfake检测方法对复杂、定制化的伪造内容在泛化能力和对抗鲁棒性方面表现不佳。

Method: ViGText方法将VLLM（视觉大语言模型）生成的图像详细文本解释与图像自身通过图结构整合，利用Graph Neural Networks（GNNs）进行分析。具体包括：将图像分割为patches，分别构建图像和文本图，通过多层次空间-频域特征提取，并融合两者信息进行深度伪造识别。

Result: ViGText方法在广泛实验中表现优异：泛化能力指标（F1分数）由72.45%显著提升至98.32%；在检测自定义deepfake时表现突出；对抗攻击下召回率提升11.1%；面向针对性攻击时分类性能下降不超过4%。

Conclusion: ViGText利用融合视觉与文本细节的创新架构，有效提升了深度伪造检测的新标准，对保障媒体真实性和信息安全具有重要意义。

Abstract: The rapid rise of deepfake technology, which produces realistic but
fraudulent digital content, threatens the authenticity of media. Traditional
deepfake detection approaches often struggle with sophisticated, customized
deepfakes, especially in terms of generalization and robustness against
malicious attacks. This paper introduces ViGText, a novel approach that
integrates images with Vision Large Language Model (VLLM) Text explanations
within a Graph-based framework to improve deepfake detection. The novelty of
ViGText lies in its integration of detailed explanations with visual data, as
it provides a more context-aware analysis than captions, which often lack
specificity and fail to reveal subtle inconsistencies. ViGText systematically
divides images into patches, constructs image and text graphs, and integrates
them for analysis using Graph Neural Networks (GNNs) to identify deepfakes.
Through the use of multi-level feature extraction across spatial and frequency
domains, ViGText captures details that enhance its robustness and accuracy to
detect sophisticated deepfakes. Extensive experiments demonstrate that ViGText
significantly enhances generalization and achieves a notable performance boost
when it detects user-customized deepfakes. Specifically, average F1 scores rise
from 72.45% to 98.32% under generalization evaluation, and reflects the model's
superior ability to generalize to unseen, fine-tuned variations of stable
diffusion models. As for robustness, ViGText achieves an increase of 11.1% in
recall compared to other deepfake detection approaches. When facing targeted
attacks that exploit its graph-based architecture, ViGText limits
classification performance degradation to less than 4%. ViGText uses detailed
visual and textual analysis to set a new standard for detecting deepfakes,
helping ensure media authenticity and information integrity.

</details>


### [19] [Enhancing Scene Transition Awareness in Video Generation via Post-Training](https://arxiv.org/abs/2507.18046)
*Hanwen Shen,Jiajie Lu,Yupeng Cao,Xiaonan Yang*

Main category: cs.CV

TL;DR: 本文提出了Transition-Aware Video (TAV)数据集，通过预处理多场景转场的视频，提升了AI视频生成模型对多场景转场的理解和生成能力。实验证明，使用TAV数据集进行后训练可以更好地响应涉及多场景变化的提示词，提升转场准确度，并能维持画质。


<details>
  <summary>Details</summary>
Motivation: 当前AI文本生成视频模型多用于单场景短视频的生成，难以处理多场景转场。这是因为训练数据大多为单场景视频，模型缺乏对多场景和转场的学习，难以根据文本提示灵活判断和实现视频转场。为满足多场景视频生成的需求，亟需提升模型的场景转场感知能力。

Method: 作者提出并构建了一个带有多个场景转场的预处理视频数据集，即TAV（Transition-Aware Video）数据集，并用该数据集对现有模型进行后训练。通过这种方式，模型能学习到场景转场的判别与实现方式，有效提升生成多场景视频的能力。

Result: 实验证明，使用TAV数据集后训练的模型，在解析涉及多场景和转场的文本提示方面表现优异。模型生成的视频中场景转场与提示要求更匹配，转场准确性提高，同时画面质量未受损失。

Conclusion: TAV数据集有效提升了模型对文本提示中场景转场的识别和生成能力，在保持画质的同时，实现了多场景视频的自然转场，为AI视频生成向多场景、长视频方向发展奠定了基础。

Abstract: Recent advances in AI-generated video have shown strong performance on
\emph{text-to-video} tasks, particularly for short clips depicting a single
scene. However, current models struggle to generate longer videos with coherent
scene transitions, primarily because they cannot infer when a transition is
needed from the prompt. Most open-source models are trained on datasets
consisting of single-scene video clips, which limits their capacity to learn
and respond to prompts requiring multiple scenes. Developing scene transition
awareness is essential for multi-scene generation, as it allows models to
identify and segment videos into distinct clips by accurately detecting
transitions.
  To address this, we propose the \textbf{Transition-Aware Video} (TAV)
dataset, which consists of preprocessed video clips with multiple scene
transitions. Our experiment shows that post-training on the \textbf{TAV}
dataset improves prompt-based scene transition understanding, narrows the gap
between required and generated scenes, and maintains image quality.

</details>


### [20] [BokehDiff: Neural Lens Blur with One-Step Diffusion](https://arxiv.org/abs/2507.18060)
*Chengxuan Zhu,Qingnan Fan,Qi Zhang,Jinwei Chen,Huaqi Zhang,Chao Xu,Boxin Shi*

Main category: cs.CV

TL;DR: 本文提出了BokehDiff，一种结合生成扩散先验的新型镜头虚化渲染方法，实现了更高的物理精度和视觉质量，尤其在深度边界表现优异。


<details>
  <summary>Details</summary>
Motivation: 以往的虚化渲染受限于深度估计准确性，常在深度突变处产生伪影，难以实现既物理真实又美观的镜头虚化效果。因此亟需突破这一技术瓶颈。

Method: 方法上，BokehDiff采用了物理启发的自注意力模块，符合实际成像流程，同时引入了基于景深的散焦圈约束及自遮挡效应。此外，扩散模型被调整为一步推理，无需额外引入噪声。针对真实有标注数据稀缺，作者利用扩散模型合成带透明度的高质量前景，实现数据多样性与真实性的平衡。

Result: 实验表明，BokehDiff在虚化视觉效果、物理真实性和高深度结构区域均优于传统方法，实现了高品质高保真渲染。

Conclusion: BokehDiff极大地提升了镜头虚化渲染的物理与视觉质量，为相关应用（如摄影后期、虚拟现实等）提供了新的技术路线。

Abstract: We introduce BokehDiff, a novel lens blur rendering method that achieves
physically accurate and visually appealing outcomes, with the help of
generative diffusion prior. Previous methods are bounded by the accuracy of
depth estimation, generating artifacts in depth discontinuities. Our method
employs a physics-inspired self-attention module that aligns with the image
formation process, incorporating depth-dependent circle of confusion constraint
and self-occlusion effects. We adapt the diffusion model to the one-step
inference scheme without introducing additional noise, and achieve results of
high quality and fidelity. To address the lack of scalable paired data, we
propose to synthesize photorealistic foregrounds with transparency with
diffusion models, balancing authenticity and scene diversity.

</details>


### [21] [Adapting Large VLMs with Iterative and Manual Instructions for Generative Low-light Enhancement](https://arxiv.org/abs/2507.18064)
*Xiaoran Sun,Liyan Wang,Cong Wang,Yeying Jin,Kin-man Lam,Zhixun Su,Yang Yang,Jinshan Pan*

Main category: cs.CV

TL;DR: 本文提出了一种利用视觉-语言大模型(VLM)结合迭代及人工指令(IMI)进行低光照图像增强的新框架VLM-IMI，实现了更好的语义引导与修复。


<details>
  <summary>Details</summary>
Motivation: 现有低光照图像增强方法多依赖先验或低光照输入，忽视了正常光照图像中的语义信息，导致在复杂场景下表现受限。

Method: VLM-IMI框架将文本描述(正常光照意图)作为增强线索，利用大规模视觉-语言模型辅助，设计了指令先验融合模块动态对齐并融合图像和文本特征。推理时采取迭代和人工修正指令策略，持续提升增强效果。

Result: 大量实验表明，VLM-IMI在多种场景下的定量指标和感知质量均优于最新方法。

Conclusion: VLM-IMI通过引入语义指导和跨模态融合，有效提升了极低照度下图像的结构与细节恢复，具备实际应用潜力。

Abstract: Most existing low-light image enhancement (LLIE) methods rely on pre-trained
model priors, low-light inputs, or both, while neglecting the semantic guidance
available from normal-light images. This limitation hinders their effectiveness
in complex lighting conditions. In this paper, we propose VLM-IMI, a novel
framework that leverages large vision-language models (VLMs) with iterative and
manual instructions (IMIs) for LLIE. VLM-IMI incorporates textual descriptions
of the desired normal-light content as enhancement cues, enabling semantically
informed restoration. To effectively integrate cross-modal priors, we introduce
an instruction prior fusion module, which dynamically aligns and fuses image
and text features, promoting the generation of detailed and semantically
coherent outputs. During inference, we adopt an iterative and manual
instruction strategy to refine textual instructions, progressively improving
visual quality. This refinement enhances structural fidelity, semantic
alignment, and the recovery of fine details under extremely low-light
conditions. Extensive experiments across diverse scenarios demonstrate that
VLM-IMI outperforms state-of-the-art methods in both quantitative metrics and
perceptual quality. The source code is available at
https://github.com/sunxiaoran01/VLM-IMI.

</details>


### [22] [TextSAM-EUS: Text Prompt Learning for SAM to Accurately Segment Pancreatic Tumor in Endoscopic Ultrasound](https://arxiv.org/abs/2507.18082)
*Pascal Spiegler,Taha Koleilat,Arash Harirpoush,Corey S. Miller,Hassan Rivaz,Marta Kersten-Oertel,Yiming Xiao*

Main category: cs.CV

TL;DR: 本文提出了一种针对胰腺癌超声内镜（EUS）影像的新型自动分割方法TextSAM-EUS，在无需人工几何提示的情况下，通过文本提示学习实现高精度自动胰腺肿瘤分割，有效超越了现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 胰腺癌预后较差，需依靠EUS进行靶向活检和放疗。然而，EUS图像的斑点噪声、对比度低和外观不直观等问题，使得传统深度学习分割模型易出错且严重依赖大量专家标注数据。现有分割方法在实际应用中面临显著挑战。

Method: 作者提出了TextSAM-EUS，一种基于SAM（Segment Anything Model）的轻量级文本驱动自适应方法，采用BiomedCLIP文本编码器和LoRA架构，为EUS胰腺肿瘤分割引入文本提示学习（context optimization），推理时不需要手动几何提示，仅微调SAM模型0.86%的参数。

Result: 在公开的Endoscopic Ultrasound Database of the Pancreas上，TextSAM-EUS自动提示下的Dice系数为82.69%，NSD为85.28%；人工提示下Dice为83.10%，NSD为85.70%；均优于现有SOTA及其他基础模型。

Conclusion: TextSAM-EUS首次将提示学习引入基于SAM的医学图像分割，具备高效且稳健的EUS自动分割能力，为胰腺肿瘤影像分析提供了实用方案，有望推动智能诊断临床应用。

Abstract: Pancreatic cancer carries a poor prognosis and relies on endoscopic
ultrasound (EUS) for targeted biopsy and radiotherapy. However, the speckle
noise, low contrast, and unintuitive appearance of EUS make segmentation of
pancreatic tumors with fully supervised deep learning (DL) models both
error-prone and dependent on large, expert-curated annotation datasets. To
address these challenges, we present TextSAM-EUS, a novel, lightweight,
text-driven adaptation of the Segment Anything Model (SAM) that requires no
manual geometric prompts at inference. Our approach leverages text prompt
learning (context optimization) through the BiomedCLIP text encoder in
conjunction with a LoRA-based adaptation of SAM's architecture to enable
automatic pancreatic tumor segmentation in EUS, tuning only 0.86% of the total
parameters. On the public Endoscopic Ultrasound Database of the Pancreas,
TextSAM-EUS with automatic prompts attains 82.69% Dice and 85.28% normalized
surface distance (NSD), and with manual geometric prompts reaches 83.10% Dice
and 85.70% NSD, outperforming both existing state-of-the-art (SOTA) supervised
DL models and foundation models (e.g., SAM and its variants). As the first
attempt to incorporate prompt learning in SAM-based medical image segmentation,
TextSAM-EUS offers a practical option for efficient and robust automatic EUS
segmentation. Our code will be publicly available upon acceptance.

</details>


### [23] [Comparison of Segmentation Methods in Remote Sensing for Land Use Land Cover](https://arxiv.org/abs/2507.18099)
*Naman Srivastava,Joel D Joy,Yash Dixit,Swarup E,Rakshit Ramesh*

Main category: cs.CV

TL;DR: 本文结合大气校正、深度学习等先进技术，对Cartosat多光谱遥感影像进行土地利用/覆盖（LULC）高精度分类，有效服务城市规划。以印度海得拉巴为例，揭示了城市化导致的用地变化。


<details>
  <summary>Details</summary>
Motivation: 城市与资源规划需要精准的土地利用/覆被（LULC）信息。传统方法精度及实用性有限，尤其是在快速城市化背景下，亟需更高效、智能的LULC制图技术，为可持续城市发展提供支撑。

Method: 本文首先使用基于查找表（LUT）的Cartosat MX图像大气校正技术，然后分别采用DeepLabV3+、交叉伪标签监督（CPS）等深度学习模型进行LULC图像分类，并对CPS进行动态权重优化，提高伪标签训练可靠性。再通过时序多光谱遥感数据，定量分析用地变化。

Result: 实验发现，结合大气校正与先进深度学习模型（尤其是动态加权CPS）能显著提升LULC制图精度。以海得拉巴为例，监测到近年来城市扩张、绿地缩减、工业用地增长等显著土地利用变化。

Conclusion: 本文提出的LULC映射方法兼具高精度与实用性，为城市规划与政策制定提供了科学数据依据。该方法有助于动态监测城市发展、指导资源配置，实现智慧与可持续城市目标。

Abstract: Land Use Land Cover (LULC) mapping is essential for urban and resource
planning, and is one of the key elements in developing smart and sustainable
cities.This study evaluates advanced LULC mapping techniques, focusing on
Look-Up Table (LUT)-based Atmospheric Correction applied to Cartosat
Multispectral (MX) sensor images, followed by supervised and semi-supervised
learning models for LULC prediction. We explore DeeplabV3+ and Cross-Pseudo
Supervision (CPS). The CPS model is further refined with dynamic weighting,
enhancing pseudo-label reliability during training. This comprehensive approach
analyses the accuracy and utility of LULC mapping techniques for various urban
planning applications. A case study of Hyderabad, India, illustrates
significant land use changes due to rapid urbanization. By analyzing Cartosat
MX images over time, we highlight shifts such as urban sprawl, shrinking green
spaces, and expanding industrial areas. This demonstrates the practical utility
of these techniques for urban planners and policymakers.

</details>


### [24] [Datasets and Recipes for Video Temporal Grounding via Reinforcement Learning](https://arxiv.org/abs/2507.18100)
*Ruizhe Chen,Zhiting Fan,Tianze Luo,Heqing Zou,Zhaopeng Feng,Guiyang Xie,Hansheng Zhang,Zhuochen Wang,Zuozhu Liu,Huaijian Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种结合监督微调和强化学习的双阶段训练框架，以提升视频时序定位(VTG)任务中模型的时序感知和泛化能力，并在多个基准数据集上取得了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的视频时序定位方法在利用大视觉语言模型(LVLMs)和指令微调后，仍面临时序感知有限及泛化性弱的问题。现有模型在面对不同难度和开放域场景时，准确性和鲁棒性不足，限制了实际应用和性能提升。

Method: 提出双阶段训练框架：第一阶段用高质量的冷启动数据进行监督微调(SFT)以初始化模型；第二阶段采用难度可控的强化学习(RL)训练，进一步提升时序定位和推理能力。对比实验分析了不同训练策略与数据策划方式的影响。

Result: 实验显示，该方法在多个视频时序定位基准上表现优异，在高难度和开放域任务中明显优于其他现有模型。论文还深入分析了高质量冷启动数据和难度可控RL的重要作用。

Conclusion: 本文工作有效提升了VTG模型的精准度和鲁棒性，并为相关研究和产业落地提供了可用的模型、数据和代码资源，推动了该领域的进一步发展。

Abstract: Video Temporal Grounding (VTG) aims to localize relevant temporal segments in
videos given natural language queries. Despite recent progress with large
vision-language models (LVLMs) and instruction-tuning, existing approaches
often suffer from limited temporal awareness and poor generalization. In this
work, we introduce a two-stage training framework that integrates supervised
fine-tuning with reinforcement learning (RL) to improve both the accuracy and
robustness of VTG models. Our approach first leverages high-quality curated
cold start data for SFT initialization, followed by difficulty-controlled RL to
further enhance temporal localization and reasoning abilities. Comprehensive
experiments on multiple VTG benchmarks demonstrate that our method consistently
outperforms existing models, particularly in challenging and open-domain
scenarios. We conduct an in-depth analysis of training strategies and dataset
curation, highlighting the importance of both high-quality cold start data and
difficulty-controlled RL. To facilitate further research and industrial
adoption, we release all intermediate datasets, models, and code to the
community.

</details>


### [25] [A Multimodal Seq2Seq Transformer for Predicting Brain Responses to Naturalistic Stimuli](https://arxiv.org/abs/2507.18104)
*Qianyi He,Yuan Chang Leong*

Main category: cs.CV

TL;DR: 该论文提出了一种基于序列到序列Transformer的模型，能够根据视觉、听觉和语言输入自回归地预测全脑fMRI响应，有效提升了对自然场景多模态电影刺激下大脑活动的预测能力。


<details>
  <summary>Details</summary>
Motivation: 当前对于多模态自然刺激下全脑fMRI响应的预测精度不高，尤其对于长时间、复杂的序列和多主体个体差异难以建模。该研究旨在为Algonauts 2025挑战开发更强的编码模型，提升对复杂自然刺激下脑活动的理解和预测水平。

Method: 论文方法采用了序列到序列Transformer结构，输入为由视觉（VideoMAE）、听觉（HuBERT）、语言（Qwen、BridgeTower）预训练模型提取的多模态特征。模型解码器通过双重跨注意力机制整合了前期脑状态、当前刺激和剧集级摘要信息。创新点包括（1）利用长序列多模态上下文捕捉长时间依赖性，（2）采用共享编码器和部分主体特定解码器以兼顾共性与个体差异。

Result: 该模型在分布内和分布外数据集上均表现出很强的预测性能，优于以往方法，有效验证了多模态、时序感知的序列建模框架对脑活动预测的提升作用。

Conclusion: 时序感知的多模态序列建模方法显著提升了对自然刺激下大脑多区响应的预测水平，同时兼顾了个体差异，对推动脑与人工智能交叉研究具有重要意义。

Abstract: The Algonauts 2025 Challenge called on the community to develop encoding
models that predict whole-brain fMRI responses to naturalistic multimodal
movies. In this submission, we propose a sequence-to-sequence Transformer that
autoregressively predicts fMRI activity from visual, auditory, and language
inputs. Stimulus features were extracted using pretrained models including
VideoMAE, HuBERT, Qwen, and BridgeTower. The decoder integrates information
from prior brain states, current stimuli, and episode-level summaries via dual
cross-attention mechanisms that attend to both perceptual information extracted
from the stimulus as well as narrative information provided by high-level
summaries of narrative content. One core innovation of our approach is the use
of sequences of multimodal context to predict sequences of brain activity,
enabling the model to capture long-range temporal structure in both stimuli and
neural responses. Another is the combination of a shared encoder with partial
subject-specific decoder, which leverages common structure across subjects
while accounting for individual variability. Our model achieves strong
performance on both in-distribution and out-of-distribution data, demonstrating
the effectiveness of temporally-aware, multimodal sequence modeling for brain
activity prediction. The code is available at
https://github.com/Angelneer926/Algonauts_challenge.

</details>


### [26] [Distributional Uncertainty for Out-of-Distribution Detection](https://arxiv.org/abs/2507.18106)
*JinYoung Kim,DaeUng Jo,Kimin Yun,Jeonghyo Song,Youngjoon Yoo*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于自由能的后验网络（Free-Energy Posterior Network），旨在更精确地估计深度神经网络的不确定性，并提升异常检测（OoD）和误分类检测的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的不确定性估计方法（如MC Dropout）往往只关注模型或数据不确定性中的单一方面，无法与实际异常检测需求完全对齐。因此，需要一种能更全面捕捉分布偏移及异常区域的新方法。

Method: 该方法提出引入基于自由能的密度估计器（由Beta分布参数化），实现对难以区分或新颖区域的细粒度不确定性估计。并将自由能损失集成进后验网络，可以直接通过网络参数推断不确定性，无需采样。同时结合RPL框架，通过Beta分布方差进行有语义的异常检测。

Result: 该方法在Fishyscapes、RoadAnomaly和Segment-Me-If-You-Can等挑战性实际数据集上进行了验证，表现优于传统方法，实现了更精确的异常区域检测。

Conclusion: 本文提出的方案能更有效捕捉真实场景中的模型不确定性和分布外样本，结合RPL实现高效且有解释性的分割，具备计算效率和实际应用价值。

Abstract: Estimating uncertainty from deep neural networks is a widely used approach
for detecting out-of-distribution (OoD) samples, which typically exhibit high
predictive uncertainty. However, conventional methods such as Monte Carlo (MC)
Dropout often focus solely on either model or data uncertainty, failing to
align with the semantic objective of OoD detection. To address this, we propose
the Free-Energy Posterior Network, a novel framework that jointly models
distributional uncertainty and identifying OoD and misclassified regions using
free energy. Our method introduces two key contributions: (1) a
free-energy-based density estimator parameterized by a Beta distribution, which
enables fine-grained uncertainty estimation near ambiguous or unseen regions;
and (2) a loss integrated within a posterior network, allowing direct
uncertainty estimation from learned parameters without requiring stochastic
sampling. By integrating our approach with the residual prediction branch (RPL)
framework, the proposed method goes beyond post-hoc energy thresholding and
enables the network to learn OoD regions by leveraging the variance of the Beta
distribution, resulting in a semantically meaningful and computationally
efficient solution for uncertainty-aware segmentation. We validate the
effectiveness of our method on challenging real-world benchmarks, including
Fishyscapes, RoadAnomaly, and Segment-Me-If-You-Can.

</details>


### [27] [T2VWorldBench: A Benchmark for Evaluating World Knowledge in Text-to-Video Generation](https://arxiv.org/abs/2507.18107)
*Yubin Chen,Xuyang Guo,Zhenmei Shi,Zhao Song,Jiahao Zhang*

Main category: cs.CV

TL;DR: 本文提出了T2VWorldBench，一个系统性评测文本生成视频模型在世界知识生成能力方面的基准，并分析当前主流模型的表现和不足。


<details>
  <summary>Details</summary>
Motivation: 虽然文本生成视频（T2V）模型在生成视觉上合理的场景方面表现突出，但其在利用世界知识以保证语义一致性和事实准确性方面的能力尚未充分研究。为填补这一评价空白，设计专门的评测体系显得尤为必要。

Method: 作者构建了T2VWorldBench评测框架，涵盖6大类、60个子类和1200个跨物理、自然、活动、文化、因果和物体等领域的提示。评测结合了人工偏好评价和基于视觉-语言模型的自动评价，系统性地测试现有10个先进T2V模型（含开源与商用）的世界知识生成能力。

Result: 评测发现，目前大多数T2V模型无法准确理解世界知识，生成的视频在常识性与事实性方面存在显著不足。无论是开源还是商用模型，都未能很好地处理涉及世界常识的复杂场景。

Conclusion: 当前T2V模型在利用世界知识方面存在明显短板。本文提出的T2VWorldBench为未来具备更强常识推理与事实生成能力的模型研究提供了有效的评测工具和研究方向。

Abstract: Text-to-video (T2V) models have shown remarkable performance in generating
visually reasonable scenes, while their capability to leverage world knowledge
for ensuring semantic consistency and factual accuracy remains largely
understudied. In response to this challenge, we propose T2VWorldBench, the
first systematic evaluation framework for evaluating the world knowledge
generation abilities of text-to-video models, covering 6 major categories, 60
subcategories, and 1,200 prompts across a wide range of domains, including
physics, nature, activity, culture, causality, and object. To address both
human preference and scalable evaluation, our benchmark incorporates both human
evaluation and automated evaluation using vision-language models (VLMs). We
evaluated the 10 most advanced text-to-video models currently available,
ranging from open source to commercial models, and found that most models are
unable to understand world knowledge and generate truly correct videos. These
findings point out a critical gap in the capability of current text-to-video
models to leverage world knowledge, providing valuable research opportunities
and entry points for constructing models with robust capabilities for
commonsense reasoning and factual generation.

</details>


### [28] [Information Entropy-Based Framework for Quantifying Tortuosity in Meibomian Gland Uneven Atrophy](https://arxiv.org/abs/2507.18135)
*Kesheng Wang,Xiaoyu Chen,Chunlei He,Fenfen Li,Xinxin Yu,Dexing Kong,Shoujun Huang,Qi Dai*

Main category: cs.CV

TL;DR: 提出一种基于信息熵的新颖曲线弯曲度量框架，并验证其在医疗图像中评估睑板腺萎缩一致性上的有效性。结果显示该方法具有较强区分能力，具有临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有医疗图像分析中曲线弯曲度的定量方法存在不足，传统的弯曲度或弧弦比法受理想化直线比较影响，缺乏鲁棒性和客观性。本研究旨在开发更可靠、适用于有生理参考曲线数据的医学场景的新方法。

Method: 提出信息熵为核心、结合概率建模和域变换的曲线弯曲度定量框架，通过与已知参考曲线对比评估目标曲线的弯曲度，首先用仿真实验评估后，再应用于睑板腺萎缩一致性分析，比较Demodex阴性与阳性患者组差异。

Result: 新框架在睑板腺萎缩一致性区分中表现显著，阴阳性组弯曲度差异具有统计学意义，AUC达0.8768，灵敏度0.75，特异度0.93。

Conclusion: 新框架在医疗数据曲线弯曲度评价中表现出强鲁棒性与临床实用价值，有望作为通用的形态学量化工具扩展至更多医学诊断场景。

Abstract: In the medical image analysis field, precise quantification of curve
tortuosity plays a critical role in the auxiliary diagnosis and pathological
assessment of various diseases. In this study, we propose a novel framework for
tortuosity quantification and demonstrate its effectiveness through the
evaluation of meibomian gland atrophy uniformity,serving as a representative
application scenario.
  We introduce an information entropy-based tortuosity quantification framework
that integrates probability modeling with entropy theory and incorporates
domain transformation of curve data. Unlike traditional methods such as
curvature or arc-chord ratio, this approach evaluates the tortuosity of a
target curve by comparing it to a designated reference curve. Consequently, it
is more suitable for tortuosity assessment tasks in medical data where
biologically plausible reference curves are available, providing a more robust
and objective evaluation metric without relying on idealized straight-line
comparisons.
  First, we conducted numerical simulation experiments to preliminarily assess
the stability and validity of the method. Subsequently, the framework was
applied to quantify the spatial uniformity of meibomian gland atrophy and to
analyze the difference in this uniformity between \textit{Demodex}-negative and
\textit{Demodex}-positive patient groups. The results demonstrated a
significant difference in tortuosity-based uniformity between the two groups,
with an area under the curve of 0.8768, sensitivity of 0.75, and specificity of
0.93. These findings highlight the clinical utility of the proposed framework
in curve tortuosity analysis and its potential as a generalizable tool for
quantitative morphological evaluation in medical diagnostics.

</details>


### [29] [Degradation-Consistent Learning via Bidirectional Diffusion for Low-Light Image Enhancement](https://arxiv.org/abs/2507.18144)
*Jinhong He,Minglong Xue,Zhipu Liu,Mingliang Zhou,Aoxiang Ning,Palaiahnakote Shivakumara*

Main category: cs.CV

TL;DR: 本文提出了一种基于双向扩散优化机制的低照度图像增强方法，有效提升了降解复杂场景下的图像质量和生成一致性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的方法仅对降解建模为单向过程，难以有效捕捉真实复杂的降解模式，导致结构不一致和像素错位，难以满足人眼视觉需求。

Method: 作者提出双向扩散机制，训练阶段联合建模低照度与正常照度的相互降解路径，并引入自适应特征交互块（AFI）优化特征表达，结合反射感知校正模块（RACM）辅助颜色恢复和防止过曝，整体通过对称性约束增强模型对光照和细节退化的感知能力。

Result: 在多个基准数据集上的定量与定性实验均显示，该方法在图像增强效果上超越主流方法，且更好适应多样化的真实退化场景。

Conclusion: 双向扩散联合建模及特征优化机制可显著提升低照度图像增强质量，改善内容一致性和细节恢复，对实际应用具有更广泛的适应能力。

Abstract: Low-light image enhancement aims to improve the visibility of degraded images
to better align with human visual perception. While diffusion-based methods
have shown promising performance due to their strong generative capabilities.
However, their unidirectional modelling of degradation often struggles to
capture the complexity of real-world degradation patterns, leading to
structural inconsistencies and pixel misalignments. To address these
challenges, we propose a bidirectional diffusion optimization mechanism that
jointly models the degradation processes of both low-light and normal-light
images, enabling more precise degradation parameter matching and enhancing
generation quality. Specifically, we perform bidirectional diffusion-from
low-to-normal light and from normal-to-low light during training and introduce
an adaptive feature interaction block (AFI) to refine feature representation.
By leveraging the complementarity between these two paths, our approach imposes
an implicit symmetry constraint on illumination attenuation and noise
distribution, facilitating consistent degradation learning and improving the
models ability to perceive illumination and detail degradation. Additionally,
we design a reflection-aware correction module (RACM) to guide color
restoration post-denoising and suppress overexposed regions, ensuring content
consistency and generating high-quality images that align with human visual
perception. Extensive experiments on multiple benchmark datasets demonstrate
that our method outperforms state-of-the-art methods in both quantitative and
qualitative evaluations while generalizing effectively to diverse degradation
scenarios. Code at https://github.com/hejh8/BidDiff

</details>


### [30] [WaveMamba: Wavelet-Driven Mamba Fusion for RGB-Infrared Object Detection](https://arxiv.org/abs/2507.18173)
*Haodong Zhu,Wenhao Dong,Linlin Yang,Hong Li,Yuguang Yang,Yangyang Ren,Qingcheng Zhu,Zichao Feng,Changbai Li,Shaohui Lin,Runqi Wang,Xiaoyan Luo,Baochang Zhang*

Main category: cs.CV

TL;DR: 本文提出了WaveMamba，一种创新的RGB与红外图像融合方法，通过离散小波变换（DWT）分解并有效融合不同频率特征，并在检测头中引入逆小波变换减少信息损失，从而显著提升目标检测性能，在四个基准测试平均mAP提升4.5%。


<details>
  <summary>Details</summary>
Motivation: RGB与红外图像具有互补特性，融合两者可以优化目标检测，但不同频率特征融合与信息损失等难题仍待解决。

Method: 方法包括：1）用离散小波变换（DWT）分解RGB和IR图像频率特征；2）设计WaveMamba Fusion Block (WMFB)集成低/高频段特征；其中低频融合块（LMFB）基于Mamba框架，结合通道交换和门控注意力机制，增强融合能力；高频采用绝对最大值策略融合。3）改进检测头，引入逆DWT减少特征损失。

Result: 方法在4个标准数据集的目标检测任务上，平均mAP提升4.5%，优于现有方法。

Conclusion: WaveMamba通过创新的多频段融合策略和高效特征整合设计，有效提升了跨模态目标检测精度，为RGB与红外图像融合提供了新思路。

Abstract: Leveraging the complementary characteristics of visible (RGB) and infrared
(IR) imagery offers significant potential for improving object detection. In
this paper, we propose WaveMamba, a cross-modality fusion method that
efficiently integrates the unique and complementary frequency features of RGB
and IR decomposed by Discrete Wavelet Transform (DWT). An improved detection
head incorporating the Inverse Discrete Wavelet Transform (IDWT) is also
proposed to reduce information loss and produce the final detection results.
The core of our approach is the introduction of WaveMamba Fusion Block (WMFB),
which facilitates comprehensive fusion across low-/high-frequency sub-bands.
Within WMFB, the Low-frequency Mamba Fusion Block (LMFB), built upon the Mamba
framework, first performs initial low-frequency feature fusion with channel
swapping, followed by deep fusion with an advanced gated attention mechanism
for enhanced integration. High-frequency features are enhanced using a strategy
that applies an ``absolute maximum" fusion approach. These advancements lead to
significant performance gains, with our method surpassing state-of-the-art
approaches and achieving average mAP improvements of 4.5% on four benchmarks.

</details>


### [31] [Real-Time Object Detection and Classification using YOLO for Edge FPGAs](https://arxiv.org/abs/2507.18174)
*Rashed Al Amin,Roman Obermaisser*

Main category: cs.CV

TL;DR: 本文提出了一种针对FPGA边缘计算平台优化的YOLOv5目标检测与分类系统，实现了高准确率和低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 当前主流的深度学习目标检测方法（如YOLO等）虽然准确率高，运行速度快，但在FPGA等资源受限的边缘设备上仍存在资源消耗过大的问题，限制了其广泛应用。

Method: 作者对YOLOv5进行了资源优化，提升其在FPGA上的效率，并在COCO和GTSRD数据集上进行训练，最终部署于Xilinx Kria KV260 FPGA开发板进行实际测试。

Result: 实验结果显示，系统在分类准确率达到99%的同时，功耗仅为3.5W，处理速度可达9帧/秒。

Conclusion: 该系统能够在保障实时性能与高准确率的前提下，有效降低资源消耗，适合在边缘计算场景下部署，实现高效的目标检测与分类。

Abstract: Object detection and classification are crucial tasks across various
application domains, particularly in the development of safe and reliable
Advanced Driver Assistance Systems (ADAS). Existing deep learning-based methods
such as Convolutional Neural Networks (CNNs), Single Shot Detectors (SSDs), and
You Only Look Once (YOLO) have demonstrated high performance in terms of
accuracy and computational speed when deployed on Field-Programmable Gate
Arrays (FPGAs). However, despite these advances, state-of-the-art YOLO-based
object detection and classification systems continue to face challenges in
achieving resource efficiency suitable for edge FPGA platforms. To address this
limitation, this paper presents a resource-efficient real-time object detection
and classification system based on YOLOv5 optimized for FPGA deployment. The
proposed system is trained on the COCO and GTSRD datasets and implemented on
the Xilinx Kria KV260 FPGA board. Experimental results demonstrate a
classification accuracy of 99%, with a power consumption of 3.5W and a
processing speed of 9 frames per second (FPS). These findings highlight the
effectiveness of the proposed approach in enabling real-time,
resource-efficient object detection and classification for edge computing
applications.

</details>


### [32] [Unsupervised Domain Adaptation for 3D LiDAR Semantic Segmentation Using Contrastive Learning and Multi-Model Pseudo Labeling](https://arxiv.org/abs/2507.18176)
*Abhishek Kaushik,Norbert Haala,Uwe Soergel*

Main category: cs.CV

TL;DR: 本文提出了一种用于3D LiDAR语义分割领域无监督领域自适应（UDA）的两阶段新方法，通过对比学习和多模型伪标签策略，有效缓解域偏移问题，提升了目标域分割准确率。


<details>
  <summary>Details</summary>
Motivation: 3D激光雷达语义分割在不同域（如传感器类型、地理位置）下容易因域偏移而性能下降，而为目标域手动标注数据成本高昂，因此需要有效的无监督自适应方法来解决实际应用中的泛化问题。

Method: 方法分为两阶段：第一阶段在分割级别使用无监督对比学习，训练骨干网络提取鲁棒且具有域不变性的特征；第二阶段采用多模型伪标签策略，融合多种主流架构（投影、体素、混合、圆柱体等）的预测结果，通过硬投票获得高质量伪标签，并用其对对比学习预训练的网络进行微调。

Result: 在SemanticKITTI到未标注目标数据集（SemanticPOSS、SemanticSlamantic）的迁移实验中，该方法在分割准确率上优于直接迁移和单模型UDA的方法。

Conclusion: 结合分割级对比预训练和多模型融合伪标签，可在无需目标域标注的情况下，有效弥合复杂域间差异，显著提升3D LiDAR语义分割域自适应性能。

Abstract: Addressing performance degradation in 3D LiDAR semantic segmentation due to
domain shifts (e.g., sensor type, geographical location) is crucial for
autonomous systems, yet manual annotation of target data is prohibitive. This
study addresses the challenge using Unsupervised Domain Adaptation (UDA) and
introduces a novel two-stage framework to tackle it. Initially, unsupervised
contrastive learning at the segment level is used to pre-train a backbone
network, enabling it to learn robust, domain-invariant features without labels.
Subsequently, a multi-model pseudo-labeling strategy is introduced, utilizing
an ensemble of diverse state-of-the-art architectures (including projection,
voxel, hybrid, and cylinder-based methods). Predictions from these models are
aggregated via hard voting to generate high-quality, refined pseudo-labels for
the unlabeled target domain, mitigating single-model biases. The contrastively
pre-trained network is then fine-tuned using these robust pseudo-labels.
Experiments adapting from SemanticKITTI to unlabeled target datasets
(SemanticPOSS, SemanticSlamantic) demonstrate significant improvements in
segmentation accuracy compared to direct transfer and single-model UDA
approaches. These results highlight the effectiveness of combining contrastive
pre-training with refined ensemble pseudo-labeling for bridging complex domain
gaps without requiring target domain annotations.

</details>


### [33] [Differential-UMamba: Rethinking Tumor Segmentation Under Limited Data Scenarios](https://arxiv.org/abs/2507.18177)
*Dhruv Jain,Romain Modzelewski,Romain Hérault,Clement Chatelain,Eva Torfeh,Sebastien Thureau*

Main category: cs.CV

TL;DR: 该论文提出了一种针对医疗图像分割的创新架构Diff-UMamba，在样本数据稀缺时能有效减少模型对噪声和无关模式的过拟合，并提升分割准确率。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医疗图像分割任务中的泛化能力受限于样本稀缺，容易过拟合噪声和不相关特征。为了提升模型在小数据场景下的表现，有必要开发抗噪性强、能捕捉长程依赖的新模型结构。

Method: 提出Diff-UMamba架构，将UNet与mamba机制结合，增强对长距离依赖的建模能力；引入噪声抑制模块（NRM），通过信号差分策略过滤编码器中的噪声或无关激活，突出任务相关特征。

Result: 在公开数据集MSD（肺、胰腺）、AIIB23上测试，Diff-UMamba在各类分割任务中较基线方法提升1-3%；在BraTS-21数据集上，通过变更训练样本比例评估其在小数据条件下的适应性；在内部小规模NSCLC数据集(CBCT-GTV分割)上，提升4-5%。

Conclusion: Diff-UMamba能在低数据场景下有效提升医学图像分割的鲁棒性和准确性，对滤噪和识别临床有意义的区域具有优势，适合推广到实际医学场景。

Abstract: In data-scarce scenarios, deep learning models often overfit to noise and
irrelevant patterns, which limits their ability to generalize to unseen
samples. To address these challenges in medical image segmentation, we
introduce Diff-UMamba, a novel architecture that combines the UNet framework
with the mamba mechanism for modeling long-range dependencies. At the heart of
Diff-UMamba is a Noise Reduction Module (NRM), which employs a signal
differencing strategy to suppress noisy or irrelevant activations within the
encoder. This encourages the model to filter out spurious features and enhance
task-relevant representations, thereby improving its focus on clinically
meaningful regions. As a result, the architecture achieves improved
segmentation accuracy and robustness, particularly in low-data settings.
Diff-UMamba is evaluated on multiple public datasets, including MSD (lung and
pancreas) and AIIB23, demonstrating consistent performance gains of 1-3% over
baseline methods across diverse segmentation tasks. To further assess
performance under limited-data conditions, additional experiments are conducted
on the BraTS-21 dataset by varying the proportion of available training
samples. The approach is also validated on a small internal non-small cell lung
cancer (NSCLC) dataset for gross tumor volume (GTV) segmentation in cone beam
CT (CBCT), where it achieves a 4-5% improvement over the baseline.

</details>


### [34] [MatSSL: Robust Self-Supervised Representation Learning for Metallographic Image Segmentation](https://arxiv.org/abs/2507.18184)
*Hoang Hai Nam Nguyen,Phan Nguyen Duc Hieu,Ho Won Lee*

Main category: cs.CV

TL;DR: MatSSL是一种高效的自监督学习结构，通过门控特征融合提升金属材料显微图像分割的性能，且对小规模无标签数据十分有效。


<details>
  <summary>Details</summary>
Motivation: 当前金属材料显微图像分析主要依赖有监督方法，但这些方法需要对每个新数据集重新训练，并且在样本较少时表现不佳。虽然自监督学习可以利用无标签数据，但多数方法依然依赖大规模数据集。为此，研究者希望开发出能在小规模无标签数据上表现良好的自监督学习模型。

Method: 提出MatSSL架构，在主干网络各层采用门控特征融合机制，有效结合多层级特征。方法包括先在小规模无标签数据集上自监督预训练，再在公开基准数据集上微调。

Result: 在MetalDAM数据集上实现了69.13%的mIoU，优于ImageNet预训练编码器的66.73%；在EBC数据集上，相比MicroNet取得了将近40%的mIoU提升。

Conclusion: MatSSL使金属显微分析模型能够仅靠少量无标签数据实现对领域的有效适配，同时兼具大规模自然图像预训练模型的强泛化和特征迁移能力。

Abstract: MatSSL is a streamlined self-supervised learning (SSL) architecture that
employs Gated Feature Fusion at each stage of the backbone to integrate
multi-level representations effectively. Current micrograph analysis of
metallic materials relies on supervised methods, which require retraining for
each new dataset and often perform inconsistently with only a few labeled
samples. While SSL offers a promising alternative by leveraging unlabeled data,
most existing methods still depend on large-scale datasets to be effective.
MatSSL is designed to overcome this limitation. We first perform
self-supervised pretraining on a small-scale, unlabeled dataset and then
fine-tune the model on multiple benchmark datasets. The resulting segmentation
models achieve 69.13% mIoU on MetalDAM, outperforming the 66.73% achieved by an
ImageNet-pretrained encoder, and delivers consistently up to nearly 40%
improvement in average mIoU on the Environmental Barrier Coating benchmark
dataset (EBC) compared to models pretrained with MicroNet. This suggests that
MatSSL enables effective adaptation to the metallographic domain using only a
small amount of unlabeled data, while preserving the rich and transferable
features learned from large-scale pretraining on natural images.

</details>


### [35] [TeEFusion: Blending Text Embeddings to Distill Classifier-Free Guidance](https://arxiv.org/abs/2507.18192)
*Minghao Fu,Guo-Hua Wang,Xiaohao Chen,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种高效的text-to-image模型蒸馏方法TeEFusion，通过对文本嵌入进行简单线性融合，极大提升了推理速度，几乎不损失生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有text-to-image生成模型（如CFG和复杂采样算法）虽然能生成高质量图片，但推理计算量大，速度慢，限制了实际应用。作者希望降低生成成本、提升推理效率，同时保持生成质量。

Method: 提出TeEFusion方法，将CFG的引导强度直接融入文本嵌入，通过简单线性操作融合有条件和无条件的嵌入。学生模型借助蒸馏学习自教师模型复杂采样策略下的输出，无需引入额外参数即可复现复杂采样的引导效果。

Result: 在以SD3等先进模型为基线的大量实验中，学生模型能以更简单高效的采样策略，接近复制教师模型的生成表现。推理速度提升最高可达6倍，而生成图片质量基本不变。

Conclusion: TeEFusion能显著加速text-to-image生成模型的推理，且生成质量接近教师模型，适合实际高效部署。代码已开源。

Abstract: Recent advances in text-to-image synthesis largely benefit from sophisticated
sampling strategies and classifier-free guidance (CFG) to ensure high-quality
generation. However, CFG's reliance on two forward passes, especially when
combined with intricate sampling algorithms, results in prohibitively high
inference costs. To address this, we introduce TeEFusion (\textbf{Te}xt
\textbf{E}mbeddings \textbf{Fusion}), a novel and efficient distillation method
that directly incorporates the guidance magnitude into the text embeddings and
distills the teacher model's complex sampling strategy. By simply fusing
conditional and unconditional text embeddings using linear operations,
TeEFusion reconstructs the desired guidance without adding extra parameters,
simultaneously enabling the student model to learn from the teacher's output
produced via its sophisticated sampling approach. Extensive experiments on
state-of-the-art models such as SD3 demonstrate that our method allows the
student to closely mimic the teacher's performance with a far simpler and more
efficient sampling strategy. Consequently, the student model achieves inference
speeds up to 6$\times$ faster than the teacher model, while maintaining image
quality at levels comparable to those obtained through the teacher's complex
sampling approach. The code is publicly available at
\href{https://github.com/AIDC-AI/TeEFusion}{github.com/AIDC-AI/TeEFusion}.

</details>


### [36] [LEAF: Latent Diffusion with Efficient Encoder Distillation for Aligned Features in Medical Image Segmentation](https://arxiv.org/abs/2507.18214)
*Qilin Huang,Tianyu Lin,Zhiguang Chen,Fudan Zheng*

Main category: cs.CV

TL;DR: 作者提出了LEAF模型，通过优化扩散模型的使用方式，提升了医学图像分割的表现，并且在不增加推理消耗的前提下取得更好效果。


<details>
  <summary>Details</summary>
Motivation: 传统的扩散模型在医学图像分割任务中取得了一定成效，但其训练流程未针对此任务进行优化，预训练模型的特征提取能力也存在不足。因此，迫切需要更适合分割任务的扩散模型方法。

Method: 1. 提出LEAF模型，基于潜变量扩散模型开发；2. 在微调过程中，将噪声预测任务替换为直接预测分割图，从而降低结果方差；3. 利用特征蒸馏，将卷积层的隐藏状态与基于变换器视觉编码器的特征对齐。

Result: 在多个疾病类型的分割数据集上，LEAF显著提升了原始扩散模型的分割效果。

Conclusion: LEAF方法无需改变模型结构，也不增加推理时的参数量和计算量，效率突出，适合应用于高效医学图像分割场景。

Abstract: Leveraging the powerful capabilities of diffusion models has yielded quite
effective results in medical image segmentation tasks. However, existing
methods typically transfer the original training process directly without
specific adjustments for segmentation tasks. Furthermore, the commonly used
pre-trained diffusion models still have deficiencies in feature extraction.
Based on these considerations, we propose LEAF, a medical image segmentation
model grounded in latent diffusion models. During the fine-tuning process, we
replace the original noise prediction pattern with a direct prediction of the
segmentation map, thereby reducing the variance of segmentation results. We
also employ a feature distillation method to align the hidden states of the
convolutional layers with the features from a transformer-based vision encoder.
Experimental results demonstrate that our method enhances the performance of
the original diffusion model across multiple segmentation datasets for
different disease types. Notably, our approach does not alter the model
architecture, nor does it increase the number of parameters or computation
during the inference phase, making it highly efficient.

</details>


### [37] [3D Test-time Adaptation via Graph Spectral Driven Point Shift](https://arxiv.org/abs/2507.18225)
*Xin Wei,Qin Yang,Yijie Fang,Mingrui Zhu,Nannan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D点云分类测试时自适应方法GSDTTA，在图谱谱域中对特征进行高效自适应，并在多个基准数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 针对现有测试时自适应（TTA）方法在3D点云领域中计算开销大和结构适应不便的问题，尤其是在点云数据的不规则和无序特性下，亟需一种高效且适应性强的3D TTA方法。

Method: 提出了图谱谱域测试时自适应（GSDTTA）方法，将目标域点云表示为异常感知图，在图谱谱域中利用图傅里叶变换（GFT）只优化点云能量最多的最低10%频率分量，通过逆GFT实现点云自适应并结合特征引导自训练对模型和谱调整进行迭代优化。

Result: 在公开的3D点云分类基准数据集上，GSDTTA在性能和效率上均优于现有的测试时自适应方法，消融实验也验证了各步骤的有效性。

Conclusion: GSDTTA在3D点云场景下实现了高效、有效的测试时自适应，展示了图谱谱域方法在高维无序数据上的巨大潜力。

Abstract: While test-time adaptation (TTA) methods effectively address domain shifts by
dynamically adapting pre-trained models to target domain data during online
inference, their application to 3D point clouds is hindered by their irregular
and unordered structure. Current 3D TTA methods often rely on computationally
expensive spatial-domain optimizations and may require additional training
data. In contrast, we propose Graph Spectral Domain Test-Time Adaptation
(GSDTTA), a novel approach for 3D point cloud classification that shifts
adaptation to the graph spectral domain, enabling more efficient adaptation by
capturing global structural properties with fewer parameters. Point clouds in
target domain are represented as outlier-aware graphs and transformed into
graph spectral domain by Graph Fourier Transform (GFT). For efficiency,
adaptation is performed by optimizing only the lowest 10% of frequency
components, which capture the majority of the point cloud's energy. An inverse
GFT (IGFT) is then applied to reconstruct the adapted point cloud with the
graph spectral-driven point shift. This process is enhanced by an
eigenmap-guided self-training strategy that iteratively refines both the
spectral adjustments and the model parameters. Experimental results and
ablation studies on benchmark datasets demonstrate the effectiveness of GSDTTA,
outperforming existing TTA methods for 3D point cloud classification.

</details>


### [38] [DATA: Domain-And-Time Alignment for High-Quality Feature Fusion in Collaborative Perception](https://arxiv.org/abs/2507.18237)
*Chengchang Tian,Jianwei Ma,Yan Huang,Zhanye Chen,Honghao Wei,Hui Zhang,Wei Hong*

Main category: cs.CV

TL;DR: 该论文提出了DATA网络，通过系统性地对特征进行域与时间对齐，提高协同感知中基于特征融合的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 在协同感知中，特征级融合可兼顾性能与通信带宽，但其效果严重依赖输入特征质量。来自硬件差异、部署条件的域间差异及由于传输延迟导致的时间错位，会严重削弱特征质量，进而层层影响整个网络性能，急需系统化解决方案提升融合前输入特征的有效性。

Method: 提出的DATA网络包括三个核心模块：1）一致性保持域对齐模块（CDAM）以层次化降采样和可观测性约束判别器减小域间差异；2）渐进式时间对齐模块（PTAM）通过多尺度运动建模和两阶段补偿机制处理传输延迟；3）实例关注特征聚合模块（IFAM）提升特征的语义表达能力。

Result: 实验显示，DATA网络在三个典型数据集上均取得了当前最佳成绩，在面临严峻的通讯延迟及位姿误差情况下，依然保持较强鲁棒性。

Conclusion: DATA网络通过系统性对齐特征的域和时间因素，大幅提升协同感知中特征融合的有效性和鲁棒性，为实际部署中的跨域、时延等难题提供了有效解决方案。

Abstract: Feature-level fusion shows promise in collaborative perception (CP) through
balanced performance and communication bandwidth trade-off. However, its
effectiveness critically relies on input feature quality. The acquisition of
high-quality features faces domain gaps from hardware diversity and deployment
conditions, alongside temporal misalignment from transmission delays. These
challenges degrade feature quality with cumulative effects throughout the
collaborative network. In this paper, we present the Domain-And-Time Alignment
(DATA) network, designed to systematically align features while maximizing
their semantic representations for fusion. Specifically, we propose a
Consistency-preserving Domain Alignment Module (CDAM) that reduces domain gaps
through proximal-region hierarchical downsampling and observability-constrained
discriminator. We further propose a Progressive Temporal Alignment Module
(PTAM) to handle transmission delays via multi-scale motion modeling and
two-stage compensation. Building upon the aligned features, an Instance-focused
Feature Aggregation Module (IFAM) is developed to enhance semantic
representations. Extensive experiments demonstrate that DATA achieves
state-of-the-art performance on three typical datasets, maintaining robustness
with severe communication delays and pose errors. The code will be released at
https://github.com/ChengchangTian/DATA.

</details>


### [39] [DepthDark: Robust Monocular Depth Estimation for Low-Light Environments](https://arxiv.org/abs/2507.18243)
*Longjian Zeng,Zunjie Zhu,Rongfeng Lu,Ming Lu,Bolun Zheng,Chenggang Yan,Anke Xue*

Main category: cs.CV

TL;DR: 本文提出了DepthDark，一个专为低光照场景设计的单目深度估计基础模型，通过高质量数据合成和高效微调策略，有效提升了弱光下的深度估计性能，并在多个权威数据集上取得了最优结果。


<details>
  <summary>Details</summary>
Motivation: 当前的单目深度估计算法主要针对白天和良好光照环境，弱光环境下效果显著下降，且缺乏大规模高质量的低光条件深度配对数据及高效微调方法，限制了基础模型的开发与应用。

Method: 作者提出DepthDark，包含两个核心创新：1）通过反射模拟模块和噪声模拟模块仿真夜间成像过程，合成高质量低光配对深度数据集；2）设计了基于光照引导和多尺度特征融合的高效低光PEFT微调策略，提升模型在低光环境下的泛化与表现能力。

Result: DepthDark在nuScenes-Night和RobotCar-Night等具挑战性的低光数据集上取得了最优的深度估计表现，同时在训练数据和计算资源有限的条件下依然展现了有效性。

Conclusion: DepthDark通过高质量数据合成与高效微调策略，显著提升了低光照场景下的单目深度估计能力，推动了相关基础模型在实际弱光环境下的应用前景。

Abstract: In recent years, foundation models for monocular depth estimation have
received increasing attention. Current methods mainly address typical daylight
conditions, but their effectiveness notably decreases in low-light
environments. There is a lack of robust foundational models for monocular depth
estimation specifically designed for low-light scenarios. This largely stems
from the absence of large-scale, high-quality paired depth datasets for
low-light conditions and the effective parameter-efficient fine-tuning (PEFT)
strategy. To address these challenges, we propose DepthDark, a robust
foundation model for low-light monocular depth estimation. We first introduce a
flare-simulation module and a noise-simulation module to accurately simulate
the imaging process under nighttime conditions, producing high-quality paired
depth datasets for low-light conditions. Additionally, we present an effective
low-light PEFT strategy that utilizes illumination guidance and multiscale
feature fusion to enhance the model's capability in low-light environments. Our
method achieves state-of-the-art depth estimation performance on the
challenging nuScenes-Night and RobotCar-Night datasets, validating its
effectiveness using limited training data and computing resources.

</details>


### [40] [LONG3R: Long Sequence Streaming 3D Reconstruction](https://arxiv.org/abs/2507.18255)
*Zhuoguang Chen,Minghui Qin,Tianyuan Yuan,Zhe Liu,Hang Zhao*

Main category: cs.CV

TL;DR: 本文提出了LONG3R模型，实现了支持长序列输入的流式多视角3D场景重建，突破现有实时性和序列长度的限制，在保证实时速度的同时性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前多视角场景重建方法无法高效处理长序列输入，主要受限于离线优化耗时或只适合短序列，缺乏适用于实时与长序列场景的重建方案。

Method: 1. 提出递归式模型结构，每帧输入后持续更新记忆，实现流式处理；2. 采用记忆门控机制筛选与新观测相关的信息，并与新观测一起输送到双源细化解码器；3. 引入3D时空记忆机制，动态裁剪冗余空间信息并自适应调整长场景的分辨率；4. 用两阶段课程式训练策略提升长序列学习能力并保证训练效率。

Result: 实验结果表明，LONG3R在长序列输入条件下显著优于现有流式重建方法，并能保持实时推理速度。

Conclusion: LONG3R突破了流式3D重建的应用瓶颈，实现了高效、实时和高质量的长序列多视角场景重建，对实际应用具有很大推动作用。

Abstract: Recent advancements in multi-view scene reconstruction have been significant,
yet existing methods face limitations when processing streams of input images.
These methods either rely on time-consuming offline optimization or are
restricted to shorter sequences, hindering their applicability in real-time
scenarios. In this work, we propose LONG3R (LOng sequence streaming 3D
Reconstruction), a novel model designed for streaming multi-view 3D scene
reconstruction over longer sequences. Our model achieves real-time processing
by operating recurrently, maintaining and updating memory with each new
observation. We first employ a memory gating mechanism to filter relevant
memory, which, together with a new observation, is fed into a dual-source
refined decoder for coarse-to-fine interaction. To effectively capture
long-sequence memory, we propose a 3D spatio-temporal memory that dynamically
prunes redundant spatial information while adaptively adjusting resolution
along the scene. To enhance our model's performance on long sequences while
maintaining training efficiency, we employ a two-stage curriculum training
strategy, each stage targeting specific capabilities. Experiments demonstrate
that LONG3R outperforms state-of-the-art streaming methods, particularly for
longer sequences, while maintaining real-time inference speed. Project page:
https://zgchen33.github.io/LONG3R/.

</details>


### [41] [Exploiting Gaussian Agnostic Representation Learning with Diffusion Priors for Enhanced Infrared Small Target Detection](https://arxiv.org/abs/2507.18260)
*Junyao Li,Yahao Lu,Xingyuan Guo,Xiaoyu Xian,Tiantian Wang,Yukai Shi*

Main category: cs.CV

TL;DR: 本文提出了一种新的红外小目标检测方法，能够在高质量红外数据稀缺的情况下提升检测鲁棒性，并通过两阶段扩散模型大幅提升合成样本质量。


<details>
  <summary>Details</summary>
Motivation: 现有红外小目标检测方法依赖大量高质量人工标注数据，导致其在真实环境中表现脆弱。本文关注在高质量红外数据缺失的情况下，检测性能如何变化及改进方法。

Method: 提出高斯不可知表示学习方法：1）引入Gaussian Group Squeezer，利用高斯采样与压缩进行非均匀量化，提升模型在训练样本多样性下的鲁棒性；2）利用两阶段扩散模型，增强量化信号与真实分布对齐，从而提升合成样本的质量和保真度。

Result: 在数据稀缺的多种情景下，与现有主流方法对比，本文方法在红外小目标检测任务中展现出了更强效能和抗干扰能力。

Conclusion: 本文提出的方法不仅缓解了对高质量标注数据的依赖，在真实环境中的适应性与稳定性也有显著提升，对实际红外检测应用具有重要意义。

Abstract: Infrared small target detection (ISTD) plays a vital role in numerous
practical applications. In pursuit of determining the performance boundaries,
researchers employ large and expensive manual-labeling data for representation
learning. Nevertheless, this approach renders the state-of-the-art ISTD methods
highly fragile in real-world challenges. In this paper, we first study the
variation in detection performance across several mainstream methods under
various scarcity -- namely, the absence of high-quality infrared data -- that
challenge the prevailing theories about practical ISTD. To address this
concern, we introduce the Gaussian Agnostic Representation Learning.
Specifically, we propose the Gaussian Group Squeezer, leveraging Gaussian
sampling and compression for non-uniform quantization. By exploiting a diverse
array of training samples, we enhance the resilience of ISTD models against
various challenges. Then, we introduce two-stage diffusion models for
real-world reconstruction. By aligning quantized signals closely with
real-world distributions, we significantly elevate the quality and fidelity of
the synthetic samples. Comparative evaluations against state-of-the-art
detection methods in various scarcity scenarios demonstrate the efficacy of the
proposed approach.

</details>


### [42] [Dissecting the Dental Lung Cancer Axis via Mendelian Randomization and Mediation Analysis](https://arxiv.org/abs/2507.18287)
*Wenran Zhang,Huihuan Luo,Linda Wei,Ping Nie,Yiqun Wu,Dedong Yu*

Main category: cs.CV

TL;DR: 该研究发现蛀牙与肺癌，尤其是鳞状细胞肺癌之间具有显著的因果关系，而牙周炎则没有发现与肺癌的因果关系。


<details>
  <summary>Details</summary>
Motivation: 先前观察性研究提示口腔疾病与肺癌可能有关，但这种关系是否具有因果性尚不明确。本研究旨在澄清牙周炎和龋齿（蛀牙）与肺癌之间的因果联系，并探索肺功能是否在其中起到中介作用。

Method: 采用两样本孟德尔随机化（MR）分析方法，通过遗传工具变量（源自最大规模的全基因组关联研究），评估牙周炎、蛀牙与肺癌及其各亚型之间的因果关系，同时用delta法评估肺功能的中介作用。

Result: 结果显示，蛀牙与总的肺癌及其亚型（特别是鳞状细胞癌）有显著正向因果关系。蛀牙每增加一个标准差，罹患鳞癌的风险增加188.0%。肺功能下降（FVC和FEV1）在该因果链中起到了部分中介作用，各自占总效应的5%左右。牙周炎与肺癌未发现因果联系。

Conclusion: 该研究强调了蛀牙在肺癌发病中的因果作用，建议将牙科保健与肺功能监测纳入癌症预防策略。

Abstract: Periodontitis and dental caries are common oral diseases affecting billions
globally. While observational studies suggest links between these conditions
and lung cancer, causality remains uncertain. This study used two sample
Mendelian randomization (MR) to explore causal relationships between dental
traits (periodontitis, dental caries) and lung cancer subtypes, and to assess
mediation by pulmonary function. Genetic instruments were derived from the
largest available genome wide association studies, including data from 487,823
dental caries and 506,594 periodontitis cases, as well as lung cancer data from
the Transdisciplinary Research of Cancer in Lung consortium. Inverse variance
weighting was the main analytical method; lung function mediation was assessed
using the delta method. The results showed a significant positive causal effect
of dental caries on overall lung cancer and its subtypes. Specifically, a one
standard deviation increase in dental caries incidence was associated with a
188.0% higher risk of squamous cell lung carcinoma (OR = 2.880, 95% CI =
1.236--6.713, p = 0.014), partially mediated by declines in forced vital
capacity (FVC) and forced expiratory volume in one second (FEV1), accounting
for 5.124% and 5.890% of the total effect. No causal effect was found for
periodontitis. These findings highlight a causal role of dental caries in lung
cancer risk and support integrating dental care and pulmonary function
monitoring into cancer prevention strategies.

</details>


### [43] [LMM-Det: Make Large Multimodal Models Excel in Object Detection](https://arxiv.org/abs/2507.18300)
*Jincheng Li,Chunyu Xie,Ji Ao,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: 本文提出了一种基于大规模多模态模型（LMM）的通用目标检测方法LMM-Det，无需专门的检测模块，并通过数据分布调整和推理优化提升召回率。实验展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前的大模型在多模态任务（如图像描述、视觉问答等）中表现优异，但在目标检测方面仍落后于专业目标检测器。作者希望消除这一差距，实现通用LMM对目标检测的直接适应。

Method: 作者分析了LMM在目标检测任务上的表现，发现召回率明显低于专业检测器。为此，提出通过数据分布调整和针对检测任务的推理优化来提升LMM的目标检测召回能力，同时重新组织指令型对话以增强检测能力。

Result: LMM-Det无需额外检测模块，仅用LMM本身即可完成目标检测。实验结果表明，所提方法提升了LMM在目标检测上的表现，验证了思路的有效性。

Conclusion: 通用大模型具备目标检测潜力，通过设计合适的训练与推理策略，无需专业检测头也能取得可观的检测效果。LMM-Det为多模态模型扩展了更多实际应用可能。

Abstract: Large multimodal models (LMMs) have garnered wide-spread attention and
interest within the artificial intelligence research and industrial
communities, owing to their remarkable capability in multimodal understanding,
reasoning, and in-context learning, among others. While LMMs have demonstrated
promising results in tackling multimodal tasks like image captioning, visual
question answering, and visual grounding, the object detection capabilities of
LMMs exhibit a significant gap compared to specialist detectors. To bridge the
gap, we depart from the conventional methods of integrating heavy detectors
with LMMs and propose LMM-Det, a simple yet effective approach that leverages a
Large Multimodal Model for vanilla object Detection without relying on
specialized detection modules. Specifically, we conduct a comprehensive
exploratory analysis when a large multimodal model meets with object detection,
revealing that the recall rate degrades significantly compared with specialist
detection models. To mitigate this, we propose to increase the recall rate by
introducing data distribution adjustment and inference optimization tailored
for object detection. We re-organize the instruction conversations to enhance
the object detection capabilities of large multimodal models. We claim that a
large multimodal model possesses detection capability without any extra
detection modules. Extensive experiments support our claim and show the
effectiveness of the versatile LMM-Det. The datasets, models, and codes are
available at https://github.com/360CVGroup/LMM-Det.

</details>


### [44] [Improving Large Vision-Language Models' Understanding for Field Data](https://arxiv.org/abs/2507.18311)
*Xiaomei Zhang,Hanyu Zheng,Xiangyu Zhu,Jinghuan Wei,Junhong Zou,Zhen Lei,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: 作者提出了FieldLVLM框架，通过领域感知和数据压缩，提升大规模视觉-语言模型对科学领域数据的理解能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LVLMs在视觉与文本结合任务上表现突出，但对自然科学中复杂现场数据的理解能力还未充分开发。作者旨在弥补这一应用空白。

Method: 提出FieldLVLM，包括两个核心部分：1）领域感知的语言生成策略，通过机器学习管道提取物理特征并生成结构化文本描述；2）数据压缩的多模态模型调优，仅保留最具信息量的数据，精简输入，以提高模型学习效率和兼容性。

Result: 在新提出的基准数据集上，FieldLVLM在科学场数据相关任务上明显优于已有方法，表现出更强的理解和推理能力。

Conclusion: FieldLVLM为LVLMs进入科学研究领域打开了新局面，有助于缩小通用大模型与专业领域发现之间的差距。

Abstract: Large Vision-Language Models (LVLMs) have shown impressive capabilities
across a range of tasks that integrate visual and textual understanding, such
as image captioning and visual question answering. These models are trained on
large-scale image and video datasets paired with text, enabling them to bridge
visual perception and natural language processing. However, their application
to scientific domains, especially in interpreting complex field data commonly
used in the natural sciences, remains underexplored. In this work, we introduce
FieldLVLM, a novel framework designed to improve large vision-language models'
understanding of field data. FieldLVLM consists of two main components: a
field-aware language generation strategy and a data-compressed multimodal model
tuning. The field-aware language generation strategy leverages a
special-purpose machine learning pipeline to extract key physical features from
field data, such as flow classification, Reynolds number, and vortex patterns.
This information is then converted into structured textual descriptions that
serve as a dataset. The data-compressed multimodal model tuning focuses on
LVLMs with these generated datasets, using a data compression strategy to
reduce the complexity of field inputs and retain only the most informative
values. This ensures compatibility with the models language decoder and guides
its learning more effectively. Experimental results on newly proposed benchmark
datasets demonstrate that FieldLVLM significantly outperforms existing methods
in tasks involving scientific field data. Our findings suggest that this
approach opens up new possibilities for applying large vision-language models
to scientific research, helping bridge the gap between large models and
domain-specific discovery.

</details>


### [45] [A Multi-Dataset Benchmark for Semi-Supervised Semantic Segmentation in ECG Delineation](https://arxiv.org/abs/2507.18323)
*Minje Park,Jeonghwa Lim,Taehyung Yu,Sunghoon Joo*

Main category: cs.CV

TL;DR: 本文提出了首个系统性的半监督ECG描记分割基准，比较了主流方法和模型，并发现transformer在此任务上优于卷积网络。


<details>
  <summary>Details</summary>
Motivation: 由于公开标注数据稀缺，现有基于深度学习的ECG波形特征分割进展受限。半监督学习能利用大量未标注数据，是一个提升分割性能的有前景方法。

Method: 作者整合并统一了多个ECG公开数据集，涵盖了先前较少使用的数据源。选取了五种代表性半监督分割算法，在卷积网络和transformer两种结构上实现，并在同域和跨域两种设置下评估。此外还提出了针对ECG的训练和增强策略，并构建了标准化评价框架。

Result: 实验证明，transformer结构在半监督ECG分割任务中表现优于卷积神经网络。

Conclusion: 该研究所建立的基准为进一步开发和评估半监督ECG分割方法提供了基础平台，有望推动该领域的研究发展。

Abstract: Electrocardiogram (ECG) delineation, the segmentation of meaningful waveform
features, is critical for clinical diagnosis. Despite recent advances using
deep learning, progress has been limited by the scarcity of publicly available
annotated datasets. Semi-supervised learning presents a promising solution by
leveraging abundant unlabeled ECG data. In this study, we present the first
systematic benchmark for semi-supervised semantic segmentation (SemiSeg) in ECG
delineation. We curated and unified multiple public datasets, including
previously underused sources, to support robust and diverse evaluation. We
adopted five representative SemiSeg algorithms from computer vision,
implemented them on two different architectures: the convolutional network and
the transformer, and evaluated them in two different settings: in-domain and
cross-domain. Additionally, we propose ECG-specific training configurations and
augmentation strategies and introduce a standardized evaluation framework. Our
results show that the transformer outperforms the convolutional network in
semi-supervised ECG delineation. We anticipate that our benchmark will serve as
a foundation for advancing semi-supervised ECG delineation methods and will
facilitate further research in this domain.

</details>


### [46] [Beyond Low-rankness: Guaranteed Matrix Recovery via Modified Nuclear Norm](https://arxiv.org/abs/2507.18327)
*Jiangjun Peng,Yisi Luo,Xiangyong Cao,Shuang Xu,Deyu Meng*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的矩阵恢复方法：修改的核范数（MNN）框架。该方法通过对变换后的矩阵计算核范数，兼顾局部信息和全局低秩特性，并无需调整权衡参数。实验和理论分析都证明了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统核范数方法虽然在鲁棒主成分分析（PCA）和矩阵补全等问题上表现出色，但只能利用全局低秩结构，难以同时挖掘局部信息。已有尝试将局部和全局信息结合，但通常需要复杂的参数调整且理论保障不足。

Method: 作者提出通过合适的线性变换将矩阵转换，再对变换后的矩阵应用核范数（即MNN范数）。这种设计允许灵活选择多种有效的变换方式，实现信息的统一表达和优化。无须人工设定局部与全局权重参数。

Result: 理论上，作者在较温和的变换假设下，给出了MNN框架在鲁棒PCA和矩阵补全任务上的准确恢复保障。实验结果也显示该方法优于其它结合局部与全局信息的模型。

Conclusion: MNN是一种通用、高效的结构化低秩恢复方法，不但在实践上提升了性能，也填补了现有理论空白。该方法适应性强，对实际低秩数据建模具有广阔应用前景。

Abstract: The nuclear norm (NN) has been widely explored in matrix recovery problems,
such as Robust PCA and matrix completion, leveraging the inherent global
low-rank structure of the data. In this study, we introduce a new modified
nuclear norm (MNN) framework, where the MNN family norms are defined by
adopting suitable transformations and performing the NN on the transformed
matrix. The MNN framework offers two main advantages: (1) it jointly captures
both local information and global low-rankness without requiring trade-off
parameter tuning; (2) Under mild assumptions on the transformation, we provided
exact theoretical recovery guarantees for both Robust PCA and MC tasks-an
achievement not shared by existing methods that combine local and global
information. Thanks to its general and flexible design, MNN can accommodate
various proven transformations, enabling a unified and effective approach to
structured low-rank recovery. Extensive experiments demonstrate the
effectiveness of our method. Code and supplementary material are available at
https://github.com/andrew-pengjj/modified_nuclear_norm.

</details>


### [47] [GVCCS: A Dataset for Contrail Identification and Tracking on Visible Whole Sky Camera Sequences](https://arxiv.org/abs/2507.18330)
*Gabriel Jarry,Ramon Dalmau,Philippe Very,Franck Ballerini,Stephania-Denisa Bocu*

Main category: cs.CV

TL;DR: 本文提出了一个新的地面可见光相机卷云（contrail）视频数据集，并通过深度学习方法实现了对卷云的精确分析，为航空非CO2气候影响研究与模型标定提供了重要数据支持。


<details>
  <summary>Details</summary>
Motivation: 航空活动对气候的影响不仅仅来自CO2排放，还包括卷云等非CO2效应，卷云对地球辐射平衡有显著影响。目前基于物理的模拟依赖于高质量数据，但现有遥感数据在卷云追踪、归因等方面存在不足，难以满足研究需求。

Method: 作者构建了一个新的开源地面可见光相机卷云序列数据集（GVCCS），每个卷云被单独标注并实现时序追踪。数据集含122段视频（共24,228帧），并包含卷云归属航班的信息。同时，提出了基于全景分割的深度学习模型框架，实现了对卷云的语义分割、实例分割和时序追踪。

Result: 数据集为每个卷云提供高质量的时序标注和航班识别信息，深度学习框架实现了卷云的自动检测、分割和追踪。该数据集和模型为后续物理模型的标定和评测提供了统一基准。

Conclusion: 本文的数据集和方法填补了卷云相关数据的空白，为航空非CO2气候效应建模与研究提供了新的工具和数据支撑，有助于提升物理模型的准确性，为气候影响评估打下基础。

Abstract: Aviation's climate impact includes not only CO2 emissions but also
significant non-CO2 effects, especially from contrails. These ice clouds can
alter Earth's radiative balance, potentially rivaling the warming effect of
aviation CO2. Physics-based models provide useful estimates of contrail
formation and climate impact, but their accuracy depends heavily on the quality
of atmospheric input data and on assumptions used to represent complex
processes like ice particle formation and humidity-driven persistence.
Observational data from remote sensors, such as satellites and ground cameras,
could be used to validate and calibrate these models. However, existing
datasets don't explore all aspect of contrail dynamics and formation: they
typically lack temporal tracking, and do not attribute contrails to their
source flights. To address these limitations, we present the Ground Visible
Camera Contrail Sequences (GVCCS), a new open data set of contrails recorded
with a ground-based all-sky camera in the visible range. Each contrail is
individually labeled and tracked over time, allowing a detailed analysis of its
lifecycle. The dataset contains 122 video sequences (24,228 frames) and
includes flight identifiers for contrails that form above the camera. As
reference, we also propose a unified deep learning framework for contrail
analysis using a panoptic segmentation model that performs semantic
segmentation (contrail pixel identification), instance segmentation (individual
contrail separation), and temporal tracking in a single architecture. By
providing high-quality, temporally resolved annotations and a benchmark for
model evaluation, our work supports improved contrail monitoring and will
facilitate better calibration of physical models. This sets the groundwork for
more accurate climate impact understanding and assessments.

</details>


### [48] [Boosting Multi-View Indoor 3D Object Detection via Adaptive 3D Volume Construction](https://arxiv.org/abs/2507.18331)
*Runmin Zhang,Zhu Yu,Si-Yuan Cao,Lingyu Zhu,Guangyi Zhang,Xiaokai Bai,Hui-Liang Shen*

Main category: cs.CV

TL;DR: SGCDet是一种新型多视角室内3D目标检测框架，通过自适应方式构建3D体素体积，结合几何和上下文信息，提升了检测性能和效率，并只需3D框监督。


<details>
  <summary>Details</summary>
Motivation: 当前多视角3D目标检测方法在体素构建时通常局限于固定位置，导致感受野受限，对图像间信息整合能力不足，且存在冗余计算。

Method: SGCDet引入了几何与上下文感知的聚合模块，在每幅图像中自适应地整合几何和上下文信息，并对多视角的贡献实行动态调整。此外，通过稀疏体素选择策略，仅保留高占据概率体素用于特征精炼，减少无用计算。

Result: SGCDet在ScanNet、ScanNet200以及ARKitScenes等数据集上取得了当前最佳的检测结果。

Conclusion: SGCDet实现了更高效且自适应的3D特征体积构建方法，无需真实的场景几何，仅需3D包围框标注即可进行训练，性能优异。

Abstract: This work presents SGCDet, a novel multi-view indoor 3D object detection
framework based on adaptive 3D volume construction. Unlike previous approaches
that restrict the receptive field of voxels to fixed locations on images, we
introduce a geometry and context aware aggregation module to integrate
geometric and contextual information within adaptive regions in each image and
dynamically adjust the contributions from different views, enhancing the
representation capability of voxel features. Furthermore, we propose a sparse
volume construction strategy that adaptively identifies and selects voxels with
high occupancy probabilities for feature refinement, minimizing redundant
computation in free space. Benefiting from the above designs, our framework
achieves effective and efficient volume construction in an adaptive way. Better
still, our network can be supervised using only 3D bounding boxes, eliminating
the dependence on ground-truth scene geometry. Experimental results demonstrate
that SGCDet achieves state-of-the-art performance on the ScanNet, ScanNet200
and ARKitScenes datasets. The source code is available at
https://github.com/RM-Zhang/SGCDet.

</details>


### [49] [Improving Bird Classification with Primary Color Additives](https://arxiv.org/abs/2507.18334)
*Ezhini Rasendiran R,Chandresh Kumar Maurya*

Main category: cs.CV

TL;DR: 该论文提出通过给频谱图添加颜色，将频率信息嵌入图像，以提升鸟鸣分类模型在多物种、低信噪比等复杂场景下的判别能力，并在BirdCLEF 2024任务上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 鸟鸣分类面临环境噪音、多物种同时鸣叫和标签缺失等挑战，目前深度学习方法难以区分具有相似音型（motif）的不同鸟类。因此，作者希望通过在频谱图中引入更加丰富的频率信息，增强模型的区分能力。

Method: 方法是将频率信息用主色彩叠加的方式嵌入到频谱图中，生成色彩化的图像，再用深度学习模型进行分类，以帮助模型更好辨别不同鸟类的音型细节。

Result: 所提方法在与未做色彩化处理的基线模型对比下，F1提升7.3%，ROC-AUC提升6.2%，CMAP提升6.6%，且超过BirdCLEF 2024冠军方案，提升具有统计学意义。

Conclusion: 通过将频率信息以色彩形式编码进频谱图，显著提升了鸟鸣分类准确度，为今后相关音频分类任务提供了有效手段。

Abstract: We address the problem of classifying bird species using their song
recordings, a challenging task due to environmental noise, overlapping
vocalizations, and missing labels. Existing models struggle with low-SNR or
multi-species recordings. We hypothesize that birds can be classified by
visualizing their pitch pattern, speed, and repetition, collectively called
motifs. Deep learning models applied to spectrogram images help, but similar
motifs across species cause confusion. To mitigate this, we embed frequency
information into spectrograms using primary color additives. This enhances
species distinction and improves classification accuracy. Our experiments show
that the proposed approach achieves statistically significant gains over models
without colorization and surpasses the BirdCLEF 2024 winner, improving F1 by
7.3%, ROC-AUC by 6.2%, and CMAP by 6.6%. These results demonstrate the
effectiveness of incorporating frequency information via colorization.

</details>


### [50] [EgoExoBench: A Benchmark for First- and Third-person View Video Understanding in MLLMs](https://arxiv.org/abs/2507.18342)
*Yuping He,Yifei Huang,Guo Chen,Baoqi Pei,Jilan Xu,Tong Lu,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 本文提出了EgoExoBench，这是首个用于测试模型在主观（第一人称）和客观（第三人称）视角下视频理解与推理能力的基准，发现现有大模型在跨视角推理任务上表现有限。


<details>
  <summary>Details</summary>
Motivation: 人类能够自如地在主观与客观视角间传递并融合知识，这是智慧的重要特征。目前多模态大模型虽然进步显著，但其跨视角推理能力尚未深入研究。

Method: 作者设计并构建了EgoExoBench基准，涵盖7300多个问答对，涉及11个子任务，分三大类核心挑战：语义对齐、视角关联及时序推理。该基准整合自公开数据集，并用来评估13种最新多模态大模型。

Result: 实验结果显示，虽然主流多模态语言模型在单一视角任务上表现优异，但在主客观视角间的语义匹配、视角匹配及动态时序推理等跨视角任务上显著不足。

Conclusion: EgoExoBench为发展具有人类类跨视角智能的体感智能体与智能助手研究提供了标准测试资源，有望推动相关领域进步。

Abstract: Transferring and integrating knowledge across first-person (egocentric) and
third-person (exocentric) viewpoints is intrinsic to human intelligence,
enabling humans to learn from others and convey insights from their own
experiences. Despite rapid progress in multimodal large language models
(MLLMs), their ability to perform such cross-view reasoning remains unexplored.
To address this, we introduce EgoExoBench, the first benchmark for
egocentric-exocentric video understanding and reasoning. Built from publicly
available datasets, EgoExoBench comprises over 7,300 question-answer pairs
spanning eleven sub-tasks organized into three core challenges: semantic
alignment, viewpoint association, and temporal reasoning. We evaluate 13
state-of-the-art MLLMs and find that while these models excel on single-view
tasks, they struggle to align semantics across perspectives, accurately
associate views, and infer temporal dynamics in the ego-exo context. We hope
EgoExoBench can serve as a valuable resource for research on embodied agents
and intelligent assistants seeking human-like cross-view intelligence.

</details>


### [51] [VB-Mitigator: An Open-source Framework for Evaluating and Advancing Visual Bias Mitigation](https://arxiv.org/abs/2507.18348)
*Ioannis Sarridis,Christos Koutlis,Symeon Papadopoulos,Christos Diou*

Main category: cs.CV

TL;DR: 本文提出了VB-Mitigator，这是一个开源框架，用于规范与促进视觉偏见消除方法的开发、评估和对比分析。


<details>
  <summary>Details</summary>
Motivation: 当前计算机视觉模型中的偏见问题严重，导致模型结果不公平和难以泛化。尽管偏见缓解方法研究活跃，但由于实现分散、评估标准不统一，不同研究难以公平比较和复现。

Method: 作者开发了VB-Mitigator框架，包含12种现有偏见缓解方法和7个基准数据集，集成了多种主流指标，并且设计为可扩展，便于研究者集成新方法、数据集、指标和模型。

Result: 基于VB-Mitigator，作者对多种主流方法和数据集进行了系统的性能对比，并总结了最佳评估实践建议。

Conclusion: VB-Mitigator为社区提供了标准、可扩展的平台，有望推动公正、可靠的计算机视觉模型研究，并促进基于公平性的视觉偏见缓解技术进步。

Abstract: Bias in computer vision models remains a significant challenge, often
resulting in unfair, unreliable, and non-generalizable AI systems. Although
research into bias mitigation has intensified, progress continues to be
hindered by fragmented implementations and inconsistent evaluation practices.
Disparate datasets and metrics used across studies complicate reproducibility,
making it difficult to fairly assess and compare the effectiveness of various
approaches. To overcome these limitations, we introduce the Visual Bias
Mitigator (VB-Mitigator), an open-source framework designed to streamline the
development, evaluation, and comparative analysis of visual bias mitigation
techniques. VB-Mitigator offers a unified research environment encompassing 12
established mitigation methods, 7 diverse benchmark datasets. A key strength of
VB-Mitigator is its extensibility, allowing for seamless integration of
additional methods, datasets, metrics, and models. VB-Mitigator aims to
accelerate research toward fairness-aware computer vision models by serving as
a foundational codebase for the research community to develop and assess their
approaches. To this end, we also recommend best evaluation practices and
provide a comprehensive performance comparison among state-of-the-art
methodologies.

</details>


### [52] [Deformable Convolution Module with Globally Learned Relative Offsets for Fundus Vessel Segmentation](https://arxiv.org/abs/2507.18354)
*Lexuan Zhu,Yuxuan Li,Yuning Ren*

Main category: cs.CV

TL;DR: 本文提出了一种新的可插拔可变形卷积模块，通过注意力机制和前馈网络自适应学习偏移，实现对复杂全局特征的抓取，并在眼底血管分割任务取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 传统可变形卷积受限于核的局部性，难以充分捕捉全局性的复杂形状特征，尤其是在如眼底血管这类具有全球自相似复杂边缘的语义分割任务上。为此，作者希望设计一种既能处理全局特征又便于集成的模块。

Method: 作者提出了基于注意力和前馈网络的可变形卷积模块，可以在所有通道上自适应地学习亚像素级的位移场并改变特征图的采样点，实现核采样网格的全局相对形变。同时，设计了GDCUnet网络用于眼底血管分割任务，并进行了多组对比实验和消融实验。

Result: GDCUnet在相同配置和统一框架下，在公开数据集的眼底血管分割任务上取得了目前最优的性能。消融实验表明，本文提出的可变形卷积模块显著提升了复杂特征的学习能力和模型泛化能力。

Conclusion: 所提出的可变形卷积模块既具备传统卷积的接口兼容性，又能更好地捕捉全局自相似特征，非常适合眼底血管等具有复杂全局结构的任务，并有潜力推广到其他复杂机器视觉任务。

Abstract: Deformable convolution can adaptively change the shape of convolution kernel
by learning offsets to deal with complex shape features. We propose a novel
plug and play deformable convolutional module that uses attention and
feedforward networks to learn offsets, so that the deformable patterns can
capture long-distance global features. Compared with previously existing
deformable convolutions, the proposed module learns the sub pixel displacement
field and adaptively warps the feature maps across all channels rather than
directly deforms the convolution kernel , which is equivalent to a relative
deformation of the kernel sampling grids, achieving global feature deformation
and the decoupling of kernel size and learning network. Considering that the
fundus blood vessels have globally self similar complex edges, we design a deep
learning model for fundus blood vessel segmentation, GDCUnet, based on the
proposed convolutional module. Empirical evaluations under the same
configuration and unified framework show that GDCUnet has achieved state of the
art performance on public datasets. Further ablation experiments demonstrated
that the proposed deformable convolutional module could more significantly
learn the complex features of fundus blood vessels, enhancing the model
representation and generalization capabilities.The proposed module is similar
to the interface of conventional convolution, we suggest applying it to more
machine vision tasks with complex global self similar features.

</details>


### [53] [MVG4D: Image Matrix-Based Multi-View and Motion Generation for 4D Content Creation from a Single Image](https://arxiv.org/abs/2507.18371)
*Xiaotian Chen,DongFu Yin,Fei Richard Yu,Xuanchen Li,Xinhao Zhang*

Main category: cs.CV

TL;DR: 本文提出MVG4D框架，可仅凭单张静态图像生成高保真、时序一致的动态4D内容，在视觉保真度和效率上均优于现有方法，有助于提升AR/VR体验。


<details>
  <summary>Details</summary>
Motivation: 当前生成式建模已能生成高质量2D、3D及4D数字内容，但要实现高保真、时序一致的动态4D场景（例如用于AR/VR）仍具挑战，尤其是从极少输入（如单张图像）出发时，现有4D高斯雪花(4D GS)方法易出现结构模糊、动态不连贯及背景退化等问题。

Method: 提出MVG4D框架，主要包括：1）图像矩阵模块，输入单张图片后合成多视角、多时刻的图像序列，确保时序和空间多样性；2）使用这些多视角图像优化出3D高斯点云；3）再通过轻量级变形网络，将3D点云扩展至时间域，生成动态4D内容。

Result: 在Objaverse数据集上，MVG4D在CLIP-I、PSNR、FVD等评价指标和生成效率方面均优于现有主流4D内容生成方法，有效减少闪烁、强化细节结构、动态连贯性更佳。

Conclusion: MVG4D能以极少输入高效生成高质量4D动态场景，显著提升生成内容的结构细节和时序一致性，为面向AR/VR的可控4D内容生成奠定基础。

Abstract: Advances in generative modeling have significantly enhanced digital content
creation, extending from 2D images to complex 3D and 4D scenes. Despite
substantial progress, producing high-fidelity and temporally consistent dynamic
4D content remains a challenge. In this paper, we propose MVG4D, a novel
framework that generates dynamic 4D content from a single still image by
combining multi-view synthesis with 4D Gaussian Splatting (4D GS). At its core,
MVG4D employs an image matrix module that synthesizes temporally coherent and
spatially diverse multi-view images, providing rich supervisory signals for
downstream 3D and 4D reconstruction. These multi-view images are used to
optimize a 3D Gaussian point cloud, which is further extended into the temporal
domain via a lightweight deformation network. Our method effectively enhances
temporal consistency, geometric fidelity, and visual realism, addressing key
challenges in motion discontinuity and background degradation that affect prior
4D GS-based methods. Extensive experiments on the Objaverse dataset demonstrate
that MVG4D outperforms state-of-the-art baselines in CLIP-I, PSNR, FVD, and
time efficiency. Notably, it reduces flickering artifacts and sharpens
structural details across views and time, enabling more immersive AR/VR
experiences. MVG4D sets a new direction for efficient and controllable 4D
generation from minimal inputs.

</details>


### [54] [Towards Effective Human-in-the-Loop Assistive AI Agents](https://arxiv.org/abs/2507.18374)
*Filippos Bellos,Yayuan Li,Cary Shu,Ruey Day,Jeffrey M. Siskind,Jason J. Corso*

Main category: cs.CV

TL;DR: 本文提出一种评估人机协作完成物理任务的新框架和多模态数据集，展示了AI指导对提升任务表现的积极作用。


<details>
  <summary>Details</summary>
Motivation: 物理任务中的人机协作在人们日常生活及专业领域具有巨大潜力，但目前人机协作的评估面临挑战，尤其是人类参与交互的复杂性。

Method: 作者提出了一种评估框架，并构建了一个多模态的人机交互数据集，用于分析AI指导对程序化任务表现、错误减少和学习效果的影响。此外，开发了基于增强现实（AR）的AI代理，为烹饪、战地医疗等现实任务提供互动式指导，并通过真人实验进行评估。

Result: 通过人类实验，研究获得了关于AI辅助下人类任务表现的实证数据，显示AI的指导能有效提升任务完成率。

Conclusion: AI辅助的人机协作能提升任务完成质量，表明提高信息化指导和混合现实技术将有效改善物理任务中的人机合作。

Abstract: Effective human-AI collaboration for physical task completion has significant
potential in both everyday activities and professional domains. AI agents
equipped with informative guidance can enhance human performance, but
evaluating such collaboration remains challenging due to the complexity of
human-in-the-loop interactions. In this work, we introduce an evaluation
framework and a multimodal dataset of human-AI interactions designed to assess
how AI guidance affects procedural task performance, error reduction and
learning outcomes. Besides, we develop an augmented reality (AR)-equipped AI
agent that provides interactive guidance in real-world tasks, from cooking to
battlefield medicine. Through human studies, we share empirical insights into
AI-assisted human performance and demonstrate that AI-assisted collaboration
improves task completion.

</details>


### [55] [Towards Consistent Long-Term Pose Generation](https://arxiv.org/abs/2507.18382)
*Yayuan Li,Filippos Bellos,Jason Corso*

Main category: cs.CV

TL;DR: 本文提出了一种无需中间表征且可端到端生成动作姿势的新方法，在长期动作生成任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有动作姿态生成方法普遍依赖中间表征或自回归机制，这些方法在推理时易积累误差，致使长期生成时时序一致性变差、性能下降。

Method: 提出一种单阶段(one-stage)架构，直接从一张RGB图像和文本描述出发，无需中间表征或离散化，将生成目标对准在连续空间下的姿势坐标，并通过相对运动预测来保持空间关系，采用统一的占位符token机制保证训练和推理行为一致。

Result: 在Penn Action和F-PHAB两个公开数据集上进行大量实验验证，结果显示相比于基于量化或自回归机制的主流方法，所提方法在长期动作生成任务尤其明显地提升了性能。

Conclusion: 所提出的直接坐标生成方法，不仅简化了动作生成流程，还在实际效果上取得领先，证明该路线在解决长期动作生成上的潜力。

Abstract: Current approaches to pose generation rely heavily on intermediate
representations, either through two-stage pipelines with quantization or
autoregressive models that accumulate errors during inference. This fundamental
limitation leads to degraded performance, particularly in long-term pose
generation where maintaining temporal coherence is crucial. We propose a novel
one-stage architecture that directly generates poses in continuous coordinate
space from minimal context - a single RGB image and text description - while
maintaining consistent distributions between training and inference. Our key
innovation is eliminating the need for intermediate representations or
token-based generation by operating directly on pose coordinates through a
relative movement prediction mechanism that preserves spatial relationships,
and a unified placeholder token approach that enables single-forward generation
with identical behavior during training and inference. Through extensive
experiments on Penn Action and First-Person Hand Action Benchmark (F-PHAB)
datasets, we demonstrate that our approach significantly outperforms existing
quantization-based and autoregressive methods, especially in long-term
generation scenarios.

</details>


### [56] [HumanMaterial: Human Material Estimation from a Single Image via Progressive Training](https://arxiv.org/abs/2507.18385)
*Yu Jiang,Jiahao Xia,Jiongming Qin,Yusen Wang,Tuo Cao,Chunxia Xiao*

Main category: cs.CV

TL;DR: 本文提出了基于物理渲染的全身人体逆渲染方法，解决了高质量材质估计的难题，并通过新数据集和模型设计显著提升了渲染真实感。


<details>
  <summary>Details</summary>
Motivation: 现有全身人体逆渲染方法受到材质数据和渲染方程简化的限制，导致渲染效果（尤其在皮肤细节上）不够真实。本研究旨在解决高质量、多材质估计中的数据和模型瓶颈，提升逆渲染结果的现实感。

Method: 1）构建高质量的OpenHumanBRDF数据集，融合真实扫描数据和统计材质数据，包含法线、漫反射反照率、粗糙度、高光反照率、位移图和次表面散射等多种材质特征。2）提出HumanMaterial模型，采用渐进式训练策略，先用三个先验模型分别估计不同材质，再通过微调模型整体优化。3）引入CPR损失函数，在训练中动态加权待优化材质，提高材质映射精度。

Result: 在OpenHumanBRDF数据集及真实数据上，所提方法在人脸及全身材质重建、真实感渲染方面均优于现有技术，展现了更高的材质细节还原和整体渲染逼真度。

Conclusion: 通过高质量数据集、针对性模型结构与损失函数设计，本文大幅提升了全身人体逆渲染的真实感和精度，推动了三维人体建模与虚拟现实等领域的发展。

Abstract: Full-body Human inverse rendering based on physically-based rendering aims to
acquire high-quality materials, which helps achieve photo-realistic rendering
under arbitrary illuminations. This task requires estimating multiple material
maps and usually relies on the constraint of rendering result. The absence of
constraints on the material maps makes inverse rendering an ill-posed task.
Previous works alleviated this problem by building material dataset for
training, but their simplified material data and rendering equation lead to
rendering results with limited realism, especially that of skin. To further
alleviate this problem, we construct a higher-quality dataset (OpenHumanBRDF)
based on scanned real data and statistical material data. In addition to the
normal, diffuse albedo, roughness, specular albedo, we produce displacement and
subsurface scattering to enhance the realism of rendering results, especially
for the skin. With the increase in prediction tasks for more materials, using
an end-to-end model as in the previous work struggles to balance the importance
among various material maps, and leads to model underfitting. Therefore, we
design a model (HumanMaterial) with progressive training strategy to make full
use of the supervision information of the material maps and improve the
performance of material estimation. HumanMaterial first obtain the initial
material results via three prior models, and then refine the results by a
finetuning model. Prior models estimate different material maps, and each map
has different significance for rendering results. Thus, we design a Controlled
PBR Rendering (CPR) loss, which enhances the importance of the materials to be
optimized during the training of prior models. Extensive experiments on
OpenHumanBRDF dataset and real data demonstrate that our method achieves
state-of-the-art performance.

</details>


### [57] [Iwin Transformer: Hierarchical Vision Transformer using Interleaved Windows](https://arxiv.org/abs/2507.18405)
*Simin Huo,Ning Li*

Main category: cs.CV

TL;DR: Iwin Transformer是一种新型的分层视觉Transformer，无需位置编码，可灵活支持低到高分辨率的微调，结合了交错窗口注意力和深度可分离卷积，在主流视觉任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: Swin Transformer虽有效，但需两层才能近似全局注意力，存在信息交流受限问题。此外，传统Transformer多依赖位置嵌入，灵活性不足。作者旨在解决这些瓶颈，实现更高效、灵活的信息整合。

Method: 提出Iwin Transformer，移除了位置编码，采用交错窗口注意力（可连接远距离Token）和深度可分离卷积（连接邻近Token），使全局信息仅需单模块即可高效交互，并支持直接从低到高分辨率无缝微调。同时，将核心模块独立应用于类别条件图像生成任务。

Result: Iwin Transformer在ImageNet-1K上图像分类达到87.4%的Top-1准确率，并在语义分割、视频动作识别等任务中表现强劲。核心模块可替换为条件生成任务的自注意模块，表现依然优越。

Conclusion: Iwin Transformer不仅在多项视觉任务中展现竞争力，其无位置编码和高效的全局信息交互方案为未来视觉Transformer和视频生成（如Iwin 3D Attention）研究提供了新思路。

Abstract: We introduce Iwin Transformer, a novel position-embedding-free hierarchical
vision transformer, which can be fine-tuned directly from low to high
resolution, through the collaboration of innovative interleaved window
attention and depthwise separable convolution. This approach uses attention to
connect distant tokens and applies convolution to link neighboring tokens,
enabling global information exchange within a single module, overcoming Swin
Transformer's limitation of requiring two consecutive blocks to approximate
global attention. Extensive experiments on visual benchmarks demonstrate that
Iwin Transformer exhibits strong competitiveness in tasks such as image
classification (87.4 top-1 accuracy on ImageNet-1K), semantic segmentation and
video action recognition. We also validate the effectiveness of the core
component in Iwin as a standalone module that can seamlessly replace the
self-attention module in class-conditional image generation. The concepts and
methods introduced by the Iwin Transformer have the potential to inspire future
research, like Iwin 3D Attention in video generation. The code and models are
available at https://github.com/cominder/Iwin-Transformer.

</details>


### [58] [DCFFSNet: Deep Connectivity Feature Fusion Separation Network for Medical Image Segmentation](https://arxiv.org/abs/2507.18407)
*Xun Ye,Ruixiang Tang,Mingda Zhang,Jianglong Qin*

Main category: cs.CV

TL;DR: 本文提出了DCFFSNet模型，有效提升医学图像分割精度，特别在边缘平滑性和区域一致性方面效果显著，并在多个公开数据集上优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有将拓扑连通性用于医学图像分割的深度网络，普遍存在特征融合粗暴、连通性特征与其他特征耦合、无法定量不同特征间强度等问题。这导致分割精度受限，尤其体现在边缘与区域一致性上。

Method: 提出DCFFSNet（双连通性特征融合-分离网络），引入特征空间解耦策略，可量化连通性特征与其他特征的相对强度。网络架构具备深度连通性特征融合与分离能力，动态平衡多尺度特征表达。方法在ISIC2018、DSB2018和MoNuSeg数据集上进行了测试。

Result: DCFFSNet在ISIC2018上Dice提升1.3%、IoU提升1.2%；在DSB2018上Dice提升0.7%、IoU提升0.9%；在MoNuSeg上Dice提升0.8%、IoU提升0.9%。各项指标均优于目前主流方法。

Conclusion: DCFFSNet有效突破了现有分割网络在特征融合与表达上的限制，显著提升了分割连续性与边缘平滑性，对临床应用具有较高价值。

Abstract: Medical image segmentation leverages topological connectivity theory to
enhance edge precision and regional consistency. However, existing deep
networks integrating connectivity often forcibly inject it as an additional
feature module, resulting in coupled feature spaces with no standardized
mechanism to quantify different feature strengths. To address these issues, we
propose DCFFSNet (Dual-Connectivity Feature Fusion-Separation Network). It
introduces an innovative feature space decoupling strategy. This strategy
quantifies the relative strength between connectivity features and other
features. It then builds a deep connectivity feature fusion-separation
architecture. This architecture dynamically balances multi-scale feature
expression. Experiments were conducted on the ISIC2018, DSB2018, and MoNuSeg
datasets. On ISIC2018, DCFFSNet outperformed the next best model (CMUNet) by
1.3% (Dice) and 1.2% (IoU). On DSB2018, it surpassed TransUNet by 0.7% (Dice)
and 0.9% (IoU). On MoNuSeg, it exceeded CSCAUNet by 0.8% (Dice) and 0.9% (IoU).
The results demonstrate that DCFFSNet exceeds existing mainstream methods
across all metrics. It effectively resolves segmentation fragmentation and
achieves smooth edge transitions. This significantly enhances clinical
usability.

</details>


### [59] [SynC: Synthetic Image Caption Dataset Refinement with One-to-many Mapping for Zero-shot Image Captioning](https://arxiv.org/abs/2507.18616)
*Si-Woo Kim,MinJu Jeon,Ye-Chan Kim,Soeun Lee,Taewhan Kim,Dong-Jin Kim*

Main category: cs.CV

TL;DR: 这篇论文提出了一种用于零样本图像描述（ZIC）的新框架SynC，能够在无需人工标注的条件下优化由文本生成图像（T2I）模型合成的数据集，通过重新分配语义更对齐的图像与描述配对，显著提升了模型效果。


<details>
  <summary>Details</summary>
Motivation: 当前零样本图像描述依赖T2I合成数据减少人工标注成本，但T2I合成的图像常与原始描述存在语义不符（例如遗漏物体、属性错误），导致训练数据噪声大。以往的数据筛选方法主要针对网页抓取文本噪声，不适用合成数据。作者因此旨在解决合成图像描述数据中的语义错配问题。

Method: 作者提出的SynC方法不采用常规的数据筛选或数据再生成，而是主张将每个描述重新分配给最语义对齐的已有合成图像。它首先为每个描述检索多个相关候选图像，然后通过一种受循环一致性启发的对齐评分方法，最终选择能最佳反向检索回原始描述的图像。

Result: 在MS-COCO、Flickr30k、NoCaps等标准基准下，实验表明SynC能持续并大幅提升各种零样本图像描述模型的表现，并且在多个场景下达到最新最佳水平。

Conclusion: SynC为优化合成图像-描述数据、提升零样本图像描述任务提供了一种有效且通用的处理策略，能显著提高数据质量和模型效果。

Abstract: Zero-shot Image Captioning (ZIC) increasingly utilizes synthetic datasets
generated by text-to-image (T2I) models to mitigate the need for costly manual
annotation. However, these T2I models often produce images that exhibit
semantic misalignments with their corresponding input captions (e.g., missing
objects, incorrect attributes), resulting in noisy synthetic image-caption
pairs that can hinder model training. Existing dataset pruning techniques are
largely designed for removing noisy text in web-crawled data. However, these
methods are ill-suited for the distinct challenges of synthetic data, where
captions are typically well-formed, but images may be inaccurate
representations. To address this gap, we introduce SynC, a novel framework
specifically designed to refine synthetic image-caption datasets for ZIC.
Instead of conventional filtering or regeneration, SynC focuses on reassigning
captions to the most semantically aligned images already present within the
synthetic image pool. Our approach employs a one-to-many mapping strategy by
initially retrieving multiple relevant candidate images for each caption. We
then apply a cycle-consistency-inspired alignment scorer that selects the best
image by verifying its ability to retrieve the original caption via
image-to-text retrieval. Extensive evaluations demonstrate that SynC
consistently and significantly improves performance across various ZIC models
on standard benchmarks (MS-COCO, Flickr30k, NoCaps), achieving state-of-the-art
results in several scenarios. SynC offers an effective strategy for curating
refined synthetic data to enhance ZIC.

</details>


### [60] [Self-Supervised Ultrasound-Video Segmentation with Feature Prediction and 3D Localised Loss](https://arxiv.org/abs/2507.18424)
*Edward Ellis,Robert Mendel,Andrew Bulpitt,Nasim Parsa,Michael F Byrne,Sharib Ali*

Main category: cs.CV

TL;DR: 本论文提出将V-JEPA自监督学习框架应用于超声视频分割，通过新的3D辅助任务优化模型表现，在标注数据有限时显著提升分割准确率。


<details>
  <summary>Details</summary>
Motivation: 超声医学影像数据获取和标注非常困难，特别是由于本身噪声大、对比度低且容易受伪影影响，因而需要探索可以利用未标注数据的方法来提升分割效果。

Method: 将V-JEPA（一种基于特征预测的视频自监督学习方法）首次应用于超声视频数据，并针对ViT架构在医学小数据集上表现不足的问题，设计了新的3D定位辅助任务强化局部性学习，提升表征能力。

Result: 实验结果表明，采用辅助任务的V-JEPA方法在不同冻结编码器配置下均显著提升分割性能：全部训练数据提升最高达3.4%，仅用10%标注数据时提升高达8.35%。

Conclusion: 该研究首次将V-JEPA应用于超声视频并结合3D定位任务，有效提升了有限标注下的分割表现，为超声医学影像分析提供了更优的自监督学习方案。

Abstract: Acquiring and annotating large datasets in ultrasound imaging is challenging
due to low contrast, high noise, and susceptibility to artefacts. This process
requires significant time and clinical expertise. Self-supervised learning
(SSL) offers a promising solution by leveraging unlabelled data to learn useful
representations, enabling improved segmentation performance when annotated data
is limited. Recent state-of-the-art developments in SSL for video data include
V-JEPA, a framework solely based on feature prediction, avoiding pixel level
reconstruction or negative samples. We hypothesise that V-JEPA is well-suited
to ultrasound imaging, as it is less sensitive to noisy pixel-level detail
while effectively leveraging temporal information. To the best of our
knowledge, this is the first study to adopt V-JEPA for ultrasound video data.
Similar to other patch-based masking SSL techniques such as VideoMAE, V-JEPA is
well-suited to ViT-based models. However, ViTs can underperform on small
medical datasets due to lack of inductive biases, limited spatial locality and
absence of hierarchical feature learning. To improve locality understanding, we
propose a novel 3D localisation auxiliary task to improve locality in ViT
representations during V-JEPA pre-training. Our results show V-JEPA with our
auxiliary task improves segmentation performance significantly across various
frozen encoder configurations, with gains up to 3.4\% using 100\% and up to
8.35\% using only 10\% of the training data.

</details>


### [61] [NLML-HPE: Head Pose Estimation with Limited Data via Manifold Learning](https://arxiv.org/abs/2507.18429)
*Mahdi Ghafourian,Federico M. Sukno*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的深度学习方法NLML-HPE，用于在有限训练数据下进行头部姿态估计，通过非线性流形学习实现。该方法结合了张量分解和前馈神经网络，并能实时、高效地从面部特征点回归预测连续姿态角度。


<details>
  <summary>Details</summary>
Motivation: 现有头部姿态估计面临训练数据标注不准确和数据有限的问题，且传统方法多依赖分类而非回归，从而限制了精度和泛化能力。

Method: 该方法将头部姿态估计建模为回归任务，引入张量(Tucker)分解将欧拉角（yaw, pitch, roll）拆分到独立子空间，并使用余弦曲线建模旋转流形。为训练集构建了精确2D头部姿态数据集，并基于面部标记点学习非线性姿态流形。

Result: 提出的NLML-HPE方法能够在有限训练数据下，准确快速地识别头部姿态，实现了实时性能，并提升了姿态预测精度。

Conclusion: NLML-HPE方法证明了其在小样本条件下的高效性和优越性，提升了对头部姿态的精准估计，为相关应用提供了更可靠的基础工具。

Abstract: Head pose estimation (HPE) plays a critical role in various computer vision
applications such as human-computer interaction and facial recognition. In this
paper, we propose a novel deep learning approach for head pose estimation with
limited training data via non-linear manifold learning called NLML-HPE. This
method is based on the combination of tensor decomposition (i.e., Tucker
decomposition) and feed forward neural networks. Unlike traditional
classification-based approaches, our method formulates head pose estimation as
a regression problem, mapping input landmarks into a continuous representation
of pose angles. To this end, our method uses tensor decomposition to split each
Euler angle (yaw, pitch, roll) to separate subspaces and models each dimension
of the underlying manifold as a cosine curve. We address two key challenges: 1.
Almost all HPE datasets suffer from incorrect and inaccurate pose annotations.
Hence, we generated a precise and consistent 2D head pose dataset for our
training set by rotating 3D head models for a fixed set of poses and rendering
the corresponding 2D images. 2. We achieved real-time performance with limited
training data as our method accurately captures the nature of rotation of an
object from facial landmarks. Once the underlying manifold for rotation around
each axis is learned, the model is very fast in predicting unseen data. Our
training and testing code is available online along with our trained models:
https: //github.com/MahdiGhafoorian/NLML_HPE.

</details>


### [62] [PDB-Eval: An Evaluation of Large Multimodal Models for Description and Explanation of Personalized Driving Behavior](https://arxiv.org/abs/2507.18447)
*Junda Wu,Jessica Echterhoff,Kyungtae Han,Amr Abdelraouf,Rohit Gupta,Julian McAuley*

Main category: cs.CV

TL;DR: 本文提出了PDB-Eval基准，用于更深入理解驾驶员个性化行为，并评估多模态大模型（MLLMs）在驾驶场景下的理解与推理能力，同时为驾驶领域多模态任务提供新的数据集和基线，显著提升了相关任务表现。


<details>
  <summary>Details</summary>
Motivation: 驾驶员行为和意图的理解对于风险评估和事故预防至关重要，且个性化的安全与辅助系统可提高驾驶安全性。然而，现有数据集在基于视觉证据解释车辆行为方面存在不足，亟需更细致、能从外部视觉信息解释内部驾驶员行为的数据集及衡量基准来推动多模态模型在该领域的发展。

Method: 本文构建了PDB-Eval基准，包含两部分：PDB-X用于评估MLLMs对驾驶场景的时序理解，PDB-QA为视觉解释问答任务，支持MLLMs指令微调。PDB-Eval的设计旨在通过外部视觉信息解释驾驶行为，并通过细粒度描述和解释数据缩小MLLMs与驾驶领域的距离。

Result: 实验显示，MLLMs在使用细致驾驶行为数据微调后，零样本问答任务性能最高提升73.2%。进一步，MLLMs在Brain4Cars转向意图预测提升最多12.5%，在AIDE所有任务中最高提升达11%。

Conclusion: 通过PDB-Eval基准和数据集，可以显著提升MLLMs对驾驶场景的理解与推理能力，并有效推动多模态模型在智能驾驶领域的泛化与应用发展。

Abstract: Understanding a driver's behavior and intentions is important for potential
risk assessment and early accident prevention. Safety and driver assistance
systems can be tailored to individual drivers' behavior, significantly
enhancing their effectiveness. However, existing datasets are limited in
describing and explaining general vehicle movements based on external visual
evidence. This paper introduces a benchmark, PDB-Eval, for a detailed
understanding of Personalized Driver Behavior, and aligning Large Multimodal
Models (MLLMs) with driving comprehension and reasoning. Our benchmark consists
of two main components, PDB-X and PDB-QA. PDB-X can evaluate MLLMs'
understanding of temporal driving scenes. Our dataset is designed to find valid
visual evidence from the external view to explain the driver's behavior from
the internal view. To align MLLMs' reasoning abilities with driving tasks, we
propose PDB-QA as a visual explanation question-answering task for MLLM
instruction fine-tuning. As a generic learning task for generative models like
MLLMs, PDB-QA can bridge the domain gap without harming MLLMs'
generalizability. Our evaluation indicates that fine-tuning MLLMs on
fine-grained descriptions and explanations can effectively bridge the gap
between MLLMs and the driving domain, which improves zero-shot performance on
question-answering tasks by up to 73.2%. We further evaluate the MLLMs
fine-tuned on PDB-X in Brain4Cars' intention prediction and AIDE's recognition
tasks. We observe up to 12.5% performance improvements on the turn intention
prediction task in Brain4Cars, and consistent performance improvements up to
11.0% on all tasks in AIDE.

</details>


### [63] [Revisiting Physically Realizable Adversarial Object Attack against LiDAR-based Detection: Clarifying Problem Formulation and Experimental Protocols](https://arxiv.org/abs/2507.18457)
*Luo Cheng,Hanwei Zhang,Lijun Zhang,Holger Hermanns*

Main category: cs.CV

TL;DR: 本文提出了一个与设备无关的标准化框架，用于物理对抗性攻击在LiDAR 3D物体检测场景下的研究，并通过开源代码和基准测试协议促进公平对比及加速相关研究。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR对抗性攻击方法多数局限于数字空间，缺少物理可实现性，且物理攻击实验难以复现，受设备与设置影响较大。

Method: 设计了一个抽象化关键要素、支持多种物理对抗性攻击方法的统一框架，并提供仿真与真实硬件环境下的开源代码和评测协议。

Result: 该框架被成功验证——仿真攻击能够迁移到真实LiDAR系统，提升了成果复现性和攻击分析的系统性。

Conclusion: 本框架为物理对抗性攻击提供了实验标准与比较基础，促进了实际LiDAR感知系统对抗鲁棒性的深入理解和快速进步。

Abstract: Adversarial robustness in LiDAR-based 3D object detection is a critical
research area due to its widespread application in real-world scenarios. While
many digital attacks manipulate point clouds or meshes, they often lack
physical realizability, limiting their practical impact. Physical adversarial
object attacks remain underexplored and suffer from poor reproducibility due to
inconsistent setups and hardware differences. To address this, we propose a
device-agnostic, standardized framework that abstracts key elements of physical
adversarial object attacks, supports diverse methods, and provides open-source
code with benchmarking protocols in simulation and real-world settings. Our
framework enables fair comparison, accelerates research, and is validated by
successfully transferring simulated attacks to a physical LiDAR system. Beyond
the framework, we offer insights into factors influencing attack success and
advance understanding of adversarial robustness in real-world LiDAR perception.

</details>


### [64] [CRUISE: Cooperative Reconstruction and Editing in V2X Scenarios using Gaussian Splatting](https://arxiv.org/abs/2507.18473)
*Haoran Xu,Saining Zhang,Peishuo Li,Baijun Ye,Xiaoxue Chen,Huan-ang Gao,Jv Zheng,Xiaowei Song,Ziqiao Peng,Run Miao,Jinrang Jia,Yifeng Shi,Guangqi Yi,Hang Zhao,Hao Tang,Hongyang Li,Kaicheng Yu,Hao Zhao*

Main category: cs.CV

TL;DR: 提出了一种针对V2X自动驾驶场景的重建与合成系统CRUISE，可高保真重现和灵活编辑真实交通场景，有效提升数据增强和任务性能。


<details>
  <summary>Details</summary>
Motivation: 虽然仿真手段促进了自动驾驶的发展，但在V2X领域内，用于数据生成和增强的能力尚未充分挖掘，因此亟需高效灵活的新方法，以丰富V2X训练与评测数据集并覆盖更多场景和极端情况。

Method: 提出的CRUISE框架基于分解的高斯斑点（decomposed Gaussian Splatting），能逼真重建真实世界V2X驾驶环境。动态交通参与体被分解为可编辑的高斯表示，实现场景灵活合成与增强。该系统支持从自车（ego-vehicle）和基础设施视角渲染图像，扩展增强训练及评测数据。

Result: 实验结果表明：1）CRUISE高保真重建V2X驾驶场景；2）利用CRUISE增强的数据能显著提升自车、基础设施及协同视角下的3D检测和协同3D跟踪能力（在V2X-Seq基准上验证）；3）能有效生成具有挑战性的极端交通场景。

Conclusion: CRUISE不仅显著提高了V2X场景重建与数据增强能力，还助力下游3D感知任务性能提升，为大规模V2X协同自动驾驶数据生成和评测提供了有力工具。

Abstract: Vehicle-to-everything (V2X) communication plays a crucial role in autonomous
driving, enabling cooperation between vehicles and infrastructure. While
simulation has significantly contributed to various autonomous driving tasks,
its potential for data generation and augmentation in V2X scenarios remains
underexplored. In this paper, we introduce CRUISE, a comprehensive
reconstruction-and-synthesis framework designed for V2X driving environments.
CRUISE employs decomposed Gaussian Splatting to accurately reconstruct
real-world scenes while supporting flexible editing. By decomposing dynamic
traffic participants into editable Gaussian representations, CRUISE allows for
seamless modification and augmentation of driving scenes. Furthermore, the
framework renders images from both ego-vehicle and infrastructure views,
enabling large-scale V2X dataset augmentation for training and evaluation. Our
experimental results demonstrate that: 1) CRUISE reconstructs real-world V2X
driving scenes with high fidelity; 2) using CRUISE improves 3D detection across
ego-vehicle, infrastructure, and cooperative views, as well as cooperative 3D
tracking on the V2X-Seq benchmark; and 3) CRUISE effectively generates
challenging corner cases.

</details>


### [65] [Q-Former Autoencoder: A Modern Framework for Medical Anomaly Detection](https://arxiv.org/abs/2507.18481)
*Francesco Dalmonte,Emirhan Bayar,Emre Akbas,Mariana-Iuliana Georgescu*

Main category: cs.CV

TL;DR: 本文提出了一种基于现代自编码器框架（Q-Former Autoencoder，QFAE）的无监督医学图像异常检测方法，有效提升了异常检测的准确率，并在多个医学图像数据集上取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 医学图像的异常检测因异常类型多样且难以获取充分标注数据而极具挑战性。需要一种能在无监督情况下提升检测效果的方法。

Method: 方法采用冻结的视觉基础模型（如DINO、DINOv2、Masked Autoencoder）作为特征提取器，获得丰富的多尺度高层特征，并引入Q-Former结构作为瓶颈，实现对重构序列长度的控制和多尺度特征高效聚合。此外，加入基于预训练Masked Autoencoder特征的感知损失，引导重构更具语义相关性。

Result: 在BraTS2021、RESC和RSNA等四个医学异常检测基准上取得了SOTA（最新最优）结果，证明该框架的有效性和通用性。

Conclusion: 预训练视觉基础模型，无需针对医学领域微调即可很好迁移至医学图像异常检测；所提出的QFAE架构在无监督场景下效果突出，有望应用于多种医学图像分析任务。

Abstract: Anomaly detection in medical images is an important yet challenging task due
to the diversity of possible anomalies and the practical impossibility of
collecting comprehensively annotated data sets. In this work, we tackle
unsupervised medical anomaly detection proposing a modernized autoencoder-based
framework, the Q-Former Autoencoder, that leverages state-of-the-art pretrained
vision foundation models, such as DINO, DINOv2 and Masked Autoencoder. Instead
of training encoders from scratch, we directly utilize frozen vision foundation
models as feature extractors, enabling rich, multi-stage, high-level
representations without domain-specific fine-tuning. We propose the usage of
the Q-Former architecture as the bottleneck, which enables the control of the
length of the reconstruction sequence, while efficiently aggregating multiscale
features. Additionally, we incorporate a perceptual loss computed using
features from a pretrained Masked Autoencoder, guiding the reconstruction
towards semantically meaningful structures. Our framework is evaluated on four
diverse medical anomaly detection benchmarks, achieving state-of-the-art
results on BraTS2021, RESC, and RSNA. Our results highlight the potential of
vision foundation model encoders, pretrained on natural images, to generalize
effectively to medical image analysis tasks without further fine-tuning. We
release the code and models at https://github.com/emirhanbayar/QFAE.

</details>


### [66] [A COCO-Formatted Instance-Level Dataset for Plasmodium Falciparum Detection in Giemsa-Stained Blood Smears](https://arxiv.org/abs/2507.18483)
*Frauke Wilm,Luis Carlos Rivera Monroy,Mathias Öttl,Lukas Mürdter,Leonid Mill,Andreas Maier*

Main category: cs.CV

TL;DR: 本论文改进了NIH疟疾公开数据集，新增了COCO格式的详细标注，用于提升深度学习模型自动检测疟原虫的能力，并取得了优秀检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度学习的疟疾自动检测方法受限于缺乏详细实例级标注的数据集，导致其推广受阻。本研究旨在解决准确标注不足这一瓶颈。

Method: 作者对NIH公开疟疾数据集进行了人工与自动化修正，增加了每个红细胞和白细胞的边框标注，采用COCO格式，并用Faster R-CNN模型进行训练和验证。

Result: 在原始数据集上的交叉验证，感染细胞检测的F1分数最高达0.88，展示了新高质量标注对模型性能的显著提升。

Conclusion: 数据标注的数量和一致性对于模型检测效果至关重要，结合自动化和人工标注能够产生高质量训练数据，新数据集为疟疾自动检测研究提供了重要资源。

Abstract: Accurate detection of Plasmodium falciparum in Giemsa-stained blood smears is
an essential component of reliable malaria diagnosis, especially in developing
countries. Deep learning-based object detection methods have demonstrated
strong potential for automated Malaria diagnosis, but their adoption is limited
by the scarcity of datasets with detailed instance-level annotations. In this
work, we present an enhanced version of the publicly available NIH malaria
dataset, with detailed bounding box annotations in COCO format to support
object detection training. We validated the revised annotations by training a
Faster R-CNN model to detect infected and non-infected red blood cells, as well
as white blood cells. Cross-validation on the original dataset yielded F1
scores of up to 0.88 for infected cell detection. These results underscore the
importance of annotation volume and consistency, and demonstrate that automated
annotation refinement combined with targeted manual correction can produce
training data of sufficient quality for robust detection performance. The
updated annotations set is publicly available via GitHub:
https://github.com/MIRA-Vision-Microscopy/malaria-thin-smear-coco.

</details>


### [67] [Reinforced Embodied Active Defense: Exploiting Adaptive Interaction for Robust Visual Perception in Adversarial 3D Environments](https://arxiv.org/abs/2507.18484)
*Xiao Yang,Lingxuan Wu,Lizhong Wang,Chengyang Ying,Hang Su,Jun Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种主动防御框架 Rein-EAD，能够在3D环境下提升视觉感知系统对抗对抗攻击的鲁棒性，通过实验验证其有效性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统3D对抗攻击防御方法多为被动策略，且依赖对攻击手段的预设假设，导致在动态3D场景下适应性不足。论文旨在提出更加主动且自适应的防御机制，提升视觉系统在真实复杂环境中的安全性和鲁棒性。

Method: 提出 Rein-EAD 框架，通过主动探索与环境交互，结合多步目标（兼顾即时准确率与预测熵最小化）优化防御策略。同时引入面向不确定性的奖励塑形机制，有效促进策略更新并降低计算开销，无需可微分环境。

Result: 大量实验结果显示 Rein-EAD 能显著降低对抗攻击成功率，同时保持正常准确率，在3D物体分类、人脸识别和自动驾驶等多任务上表现优异，并能泛化到未见过和自适应攻击。

Conclusion: Rein-EAD 能够有效提升3D感知系统对抗对抗攻击的鲁棒性，适合实际复杂应用场景，具有较强的推广潜力。

Abstract: Adversarial attacks in 3D environments have emerged as a critical threat to
the reliability of visual perception systems, particularly in safety-sensitive
applications such as identity verification and autonomous driving. These
attacks employ adversarial patches and 3D objects to manipulate deep neural
network (DNN) predictions by exploiting vulnerabilities within complex scenes.
Existing defense mechanisms, such as adversarial training and purification,
primarily employ passive strategies to enhance robustness. However, these
approaches often rely on pre-defined assumptions about adversarial tactics,
limiting their adaptability in dynamic 3D settings. To address these
challenges, we introduce Reinforced Embodied Active Defense (Rein-EAD), a
proactive defense framework that leverages adaptive exploration and interaction
with the environment to improve perception robustness in 3D adversarial
contexts. By implementing a multi-step objective that balances immediate
prediction accuracy with predictive entropy minimization, Rein-EAD optimizes
defense strategies over a multi-step horizon. Additionally, Rein-EAD involves
an uncertainty-oriented reward-shaping mechanism that facilitates efficient
policy updates, thereby reducing computational overhead and supporting
real-world applicability without the need for differentiable environments.
Comprehensive experiments validate the effectiveness of Rein-EAD, demonstrating
a substantial reduction in attack success rates while preserving standard
accuracy across diverse tasks. Notably, Rein-EAD exhibits robust generalization
to unseen and adaptive attacks, making it suitable for real-world complex
tasks, including 3D object classification, face recognition and autonomous
driving.

</details>


### [68] [Delving into Mapping Uncertainty for Mapless Trajectory Prediction](https://arxiv.org/abs/2507.18498)
*Zongzheng Zhang,Xuchong Qiu,Boran Zhang,Guantian Zheng,Xunjiang Gu,Guoxuan Chi,Huan-ang Gao,Leichen Wang,Ziming Liu,Xinrun Li,Igor Gilitschenski,Hongyang Li,Hang Zhao,Hao Zhao*

Main category: cs.CV

TL;DR: 本论文提出了一种新方法，根据车辆自身未来运动状态，动态调整地图不确定性在轨迹预测中的作用，从而大幅提升了基于实时生成地图的轨迹预测效果。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶的发展，依赖高精地图的传统方法面临高昂的标注与维护成本，转而向无需预先地图、只用传感器数据在线生成地图的方式转变。但是这类实时生成的地图精度和可靠性存在疑问，尤其体现在下游的轨迹预测任务中。因此，研究如何有效利用这些不确定的地图信息提升预测性能成为关键。

Method: 作者首先分析了地图不确定性对轨迹预测有重要正面作用的场景，发现车辆自身动力学状态是关键因素。在此基础上，提出 'Proprioceptive Scenario Gating' 方法，基于自监督预测的车辆未来运动状态，自适应地引入地图不确定性信息。此外，又提出了基于协方差的地图不确定性表征方式，更加贴合地图本身的几何属性。

Result: 该方法通过大量消融实验，在真实世界nuScenes驾驶数据集上，轨迹预测性能较最新主流方法提升高达23.6%。

Conclusion: 通过动态、可解释地融合地图不确定性，显著提升了无高精地图下自动驾驶轨迹预测的准确性和鲁棒性，为未来无人驾驶感知预测系统提供了更优解。

Abstract: Recent advances in autonomous driving are moving towards mapless approaches,
where High-Definition (HD) maps are generated online directly from sensor data,
reducing the need for expensive labeling and maintenance. However, the
reliability of these online-generated maps remains uncertain. While
incorporating map uncertainty into downstream trajectory prediction tasks has
shown potential for performance improvements, current strategies provide
limited insights into the specific scenarios where this uncertainty is
beneficial. In this work, we first analyze the driving scenarios in which
mapping uncertainty has the greatest positive impact on trajectory prediction
and identify a critical, previously overlooked factor: the agent's kinematic
state. Building on these insights, we propose a novel Proprioceptive Scenario
Gating that adaptively integrates map uncertainty into trajectory prediction
based on forecasts of the ego vehicle's future kinematics. This lightweight,
self-supervised approach enhances the synergy between online mapping and
trajectory prediction, providing interpretability around where uncertainty is
advantageous and outperforming previous integration methods. Additionally, we
introduce a Covariance-based Map Uncertainty approach that better aligns with
map geometry, further improving trajectory prediction. Extensive ablation
studies confirm the effectiveness of our approach, achieving up to 23.6%
improvement in mapless trajectory prediction performance over the
state-of-the-art method using the real-world nuScenes driving dataset. Our
code, data, and models are publicly available at
https://github.com/Ethan-Zheng136/Map-Uncertainty-for-Trajectory-Prediction.

</details>


### [69] [Human Scanpath Prediction in Target-Present Visual Search with Semantic-Foveal Bayesian Attention](https://arxiv.org/abs/2507.18503)
*João Luzio,Alexandre Bernardino,Plinio Moreno*

Main category: cs.CV

TL;DR: 本文提出了一种新的人眼视觉注意力预测模型SemBA-FAST，并在视觉搜索任务中取得了比现有基线和其他顶层方法更优异的表现，部分情况下甚至可与使用注视数据的模型媲美。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习的发展，利用真人扫视路径数据的仿生计算注意力模型性能提升显著。但在基于目标的视觉搜索任务中，如何更准确地预测人类视觉注意力分布和注视序列仍是挑战，尤其是将语义、概率融合与人工中心视集成起来提升模拟人类视觉的能力。

Method: SemBA-FAST结合了深度物体检测网络、概率语义融合机制，并实行人工中心视，动态生成并更新顶层注意力信息。该方法利用预训练的物体检测器和人工中心视，不断修正和增强对目标区域的关注，同时在COCO-Search18数据集上与其他主流模型进行对比评估。

Result: SemBA-FAST在COCO-Search18基准集上实现了与真人地面事实注视路径高度相符的注视序列预测。总体来看，该方法超过了基线模型和其他顶层方法，部分指标上可与直接利用人类扫视数据的模型相竞争。

Conclusion: 语义-中心视概率框架（如SemBA-FAST）能够有效模拟类人视觉注意力，对实时认知计算与机器人领域的人类视觉建模有实际意义。

Abstract: In goal-directed visual tasks, human perception is guided by both top-down
and bottom-up cues. At the same time, foveal vision plays a crucial role in
directing attention efficiently. Modern research on bio-inspired computational
attention models has taken advantage of advancements in deep learning by
utilizing human scanpath data to achieve new state-of-the-art performance. In
this work, we assess the performance of SemBA-FAST, i.e. Semantic-based
Bayesian Attention for Foveal Active visual Search Tasks, a top-down framework
designed for predicting human visual attention in target-present visual search.
SemBA-FAST integrates deep object detection with a probabilistic semantic
fusion mechanism to generate attention maps dynamically, leveraging pre-trained
detectors and artificial foveation to update top-down knowledge and improve
fixation prediction sequentially. We evaluate SemBA-FAST on the COCO-Search18
benchmark dataset, comparing its performance against other scanpath prediction
models. Our methodology achieves fixation sequences that closely match human
ground-truth scanpaths. Notably, it surpasses baseline and other top-down
approaches and competes, in some cases, with scanpath-informed models. These
findings provide valuable insights into the capabilities of semantic-foveal
probabilistic frameworks for human-like attention modelling, with implications
for real-time cognitive computing and robotics.

</details>


### [70] [Explaining How Visual, Textual and Multimodal Encoders Share Concepts](https://arxiv.org/abs/2507.18512)
*Clément Cornet,Romaric Besançon,Hervé Le Borgne*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的方法，基于稀疏自动编码器（SAE）来量化并比较视觉、文本和多模态神经网络编码器的特征共享性，实现跨模态模型的定量分析。


<details>
  <summary>Details</summary>
Motivation: 现有基于SAE特征对模型进行比较的工作大多局限于同一模态内部，缺乏有效工具对不同模态（视觉、文本、多模态）编码器进行人可解释特征的定量比较。该文为了解决跨模态特征解释和对比的难题，提出了新的量化指标。

Method: 提出了两种新工具：（1）基于SAE特征对跨模态模型进行定量比较的指标；（2）比较不同类别模型间单个特征共享性的量化方法，并在21种编码器（视觉、文本和多模态，包含不同规模与领域）上进行了实验分析。

Result: 结果显示：1）多模态训练背景下的编码器体现出与以往分析中不同的特征共享模式；2）视觉-语言模型（VLMs）中的视觉独有特征竟被文本编码器所共享，突显了文本预训练对特征表达的影响。

Conclusion: 作者的新指标和工具为理解不同类型神经网络编码器间的特征共享性、可解释性及表征能力提供了新的视角，有助于提升多模态模型的分析和设计。

Abstract: Sparse autoencoders (SAEs) have emerged as a powerful technique for
extracting human-interpretable features from neural networks activations.
Previous works compared different models based on SAE-derived features but
those comparisons have been restricted to models within the same modality. We
propose a novel indicator allowing quantitative comparison of models across SAE
features, and use it to conduct a comparative study of visual, textual and
multimodal encoders. We also propose to quantify the Comparative Sharedness of
individual features between different classes of models. With these two new
tools, we conduct several studies on 21 encoders of the three types, with two
significantly different sizes, and considering generalist and domain specific
datasets. The results allow to revisit previous studies at the light of
encoders trained in a multimodal context and to quantify to which extent all
these models share some representations or features. They also suggest that
visual features that are specific to VLMs among vision encoders are shared with
text encoders, highlighting the impact of text pretraining. The code is
available at https://github.com/CEA-LIST/SAEshareConcepts

</details>


### [71] [Towards Large Scale Geostatistical Methane Monitoring with Part-based Object Detection](https://arxiv.org/abs/2507.18513)
*Adhemar de Senneville,Xavier Bou,Thibaud Ehret,Rafael Grompone,Jean Louis Bonne,Nicolas Dumelie,Thomas Lauvaux,Gabriele Facciolo*

Main category: cs.CV

TL;DR: 本文提出一种针对遥感图像中罕见目标检测的部件级方法，并以法国生物沼气池为例，建立新数据集，提升检测效果，最终结合地统计方法估算甲烷产量。


<details>
  <summary>Details</summary>
Motivation: 遥感数据体量巨大，而罕见目标（如生物沼气池）检测对环境评估等应用至关重要，但传统方法往往受限于样本不平衡与稀缺，急需更有效的检测方法。

Method: 作者首先构建了一个包含生物沼气池的全新数据集，训练与验证样本较少，测试集样本高度偏向于无目标区域。针对对象稀少的问题，提出基于目标关键子部件检测的部件级方法，以提升检测准确率，并将模型应用到新的地理区域。随后结合检测结果与地统计方法，对区域和时间范围内的甲烷产量进行估算。

Result: 采用所提出方法后，初步检测效果得到提升（具体提升未在摘要披露），通过实际应用在新区域，成功建立了生物沼气池清单，并实现了区域甲烷产量的估算。

Conclusion: 部件级目标检测方法能在遥感图像中有效提升稀有基础设施检测的性能，结合地统计分析，有助于大型环境影响量化，为相关领域应用提供技术支撑。

Abstract: Object detection is one of the main applications of computer vision in remote
sensing imagery. Despite its increasing availability, the sheer volume of
remote sensing data poses a challenge when detecting rare objects across large
geographic areas. Paradoxically, this common challenge is crucial to many
applications, such as estimating environmental impact of certain human
activities at scale. In this paper, we propose to address the problem by
investigating the methane production and emissions of bio-digesters in France.
We first introduce a novel dataset containing bio-digesters, with small
training and validation sets, and a large test set with a high imbalance
towards observations without objects since such sites are rare. We develop a
part-based method that considers essential bio-digester sub-elements to boost
initial detections. To this end, we apply our method to new, unseen regions to
build an inventory of bio-digesters. We then compute geostatistical estimates
of the quantity of methane produced that can be attributed to these
infrastructures in a given area at a given time.

</details>


### [72] [Object segmentation in the wild with foundation models: application to vision assisted neuro-prostheses for upper limbs](https://arxiv.org/abs/2507.18517)
*Bolutife Atoki,Jenny Benois-Pineau,Renaud Péteri,Fabien Baldacci,Aymar de Rugy*

Main category: cs.CV

TL;DR: 本文提出了一种基于注视点生成提示的方法，引导Segment Anything Model（SAM）在真实混杂场景下实现语义目标分割，并在Grasping-in-the-Wild数据集上取得了性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型能否在无需针对特定物体进行微调的情况下，实现高混杂现实场景中的语义分割，特别是在辅助神经假体等实际应用中的有效性，是一个有待探索的重要问题。

Method: 作者提出利用凝视点（gaze fixation）信息作为提示，结合 Segment Anything Model（SAM），并在第一视角数据上进行微调，以提升模型在现实混杂视觉场景下的分割能力。

Result: 在Grasping-in-the-Wild真实场景数据集上的评测结果显示，所提出方法的IoU分割质量提升最高达0.51点。

Conclusion: 通过注视点引导和适当微调，基础模型在无须针对特定图片精调的情况下，可提升复杂场景中语义分割的表现，适用于如神经义肢等实际视觉引导应用。

Abstract: In this work, we address the problem of semantic object segmentation using
foundation models. We investigate whether foundation models, trained on a large
number and variety of objects, can perform object segmentation without
fine-tuning on specific images containing everyday objects, but in highly
cluttered visual scenes. The ''in the wild'' context is driven by the target
application of vision guided upper limb neuroprostheses. We propose a method
for generating prompts based on gaze fixations to guide the Segment Anything
Model (SAM) in our segmentation scenario, and fine-tune it on egocentric visual
data. Evaluation results of our approach show an improvement of the IoU
segmentation quality metric by up to 0.51 points on real-world challenging data
of Grasping-in-the-Wild corpus which is made available on the RoboFlow Platform
(https://universe.roboflow.com/iwrist/grasping-in-the-wild)

</details>


### [73] [GaussianFusionOcc: A Seamless Sensor Fusion Approach for 3D Occupancy Prediction Using 3D Gaussians](https://arxiv.org/abs/2507.18522)
*Tomislav Pavković,Mohammad-Ali Nikouei Mahani,Johannes Niedermayer,Johannes Betz*

Main category: cs.CV

TL;DR: 本论文提出了一种名为GaussianFusionOcc的3D语义占用预测方法，通过创新的3D高斯表达与多模态传感器融合机制，实现了高效、精准并可扩展的环境感知能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要在复杂环境中进行高精度的语义占用预测，多模态融合可提升感知可靠性，但现有方法对存储和推理速度要求高且适应性有限，因此有必要研发更加高效且泛化能力强的新方法。

Method: 提出以3D高斯分布作为稀疏表达单位，实现对环境空间的精确建模，并融合来自摄像头、激光雷达、毫米波雷达等多种传感器的信息。方法采用与模态无关的可变形注意力机制，以提取和融合不同模态的关键特征，优化每个高斯点的属性表达。

Result: 在多种传感器组合设置下，GaussianFusionOcc显著提高了占用预测的精度和效率，并且在存储利用和推理速度两方面均优于基于稠密体素方法的主流方案。

Conclusion: 通过高效的高斯表示和鲁棒的多模态融合机制，GaussianFusionOcc为3D语义占用预测提供了更高精度和更强适应性的解决方案，在自动驾驶感知任务中优于当前主流模型。

Abstract: 3D semantic occupancy prediction is one of the crucial tasks of autonomous
driving. It enables precise and safe interpretation and navigation in complex
environments. Reliable predictions rely on effective sensor fusion, as
different modalities can contain complementary information. Unlike conventional
methods that depend on dense grid representations, our approach,
GaussianFusionOcc, uses semantic 3D Gaussians alongside an innovative sensor
fusion mechanism. Seamless integration of data from camera, LiDAR, and radar
sensors enables more precise and scalable occupancy prediction, while 3D
Gaussian representation significantly improves memory efficiency and inference
speed. GaussianFusionOcc employs modality-agnostic deformable attention to
extract essential features from each sensor type, which are then used to refine
Gaussian properties, resulting in a more accurate representation of the
environment. Extensive testing with various sensor combinations demonstrates
the versatility of our approach. By leveraging the robustness of multi-modal
fusion and the efficiency of Gaussian representation, GaussianFusionOcc
outperforms current state-of-the-art models.

</details>


### [74] [IntentVCNet: Bridging Spatio-Temporal Gaps for Intention-Oriented Controllable Video Captioning](https://arxiv.org/abs/2507.18531)
*Tianheng Qiu,Jingchun Gao,Jingyu Li,Huiyi Leong,Xuan Huang,Xi Wang,Xiaocheng Zhang,Kele Xu,Lan Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的意图导向型可控视频描述方法IntentVCNet，通过统一LVLMs的时空理解能力，实现了对视频中特定目标的细粒度控制性描述，并在多个公开数据集与竞赛中取得了优秀成绩。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型（LVLMs）在理解和描述视觉内容方面已经展现强大能力，但难以实现对视频序列中特定物体的时空细粒度控制描述，限制了其在个性化视频内容生成方面的应用。

Method: 作者提出了IntentVCNet，主要包括两个创新点：1）提出了prompt组合策略，使语言模型能够关联用户意图提示与视频序列内容；2）设计了高效的box adapter，通过增强全局视觉上下文中的目标语义信息，让视觉标记具备用户意图相关先验，提高对细粒度空间细节的建模能力。

Result: 通过上述方法，IntentVCNet显著提升了LVLMs在视频序列中对空间细节的建模能力，实现了更精准的意图导向型描述，并在多个开源LVLMs上达到SOTA，获得IntentVC挑战赛亚军。

Conclusion: IntentVCNet成功解决了LVLMs在时空细粒度控制描述上的瓶颈，为实现高质量、可控化的视频字幕生成提供了一条有效途径，推动了视觉语言模型在视频理解和生成方面的进一步发展。

Abstract: Intent-oriented controlled video captioning aims to generate targeted
descriptions for specific targets in a video based on customized user intent.
Current Large Visual Language Models (LVLMs) have gained strong instruction
following and visual comprehension capabilities. Although the LVLMs
demonstrated proficiency in spatial and temporal understanding respectively, it
was not able to perform fine-grained spatial control in time sequences in
direct response to instructions. This substantial spatio-temporal gap
complicates efforts to achieve fine-grained intention-oriented control in
video. Towards this end, we propose a novel IntentVCNet that unifies the
temporal and spatial understanding knowledge inherent in LVLMs to bridge the
spatio-temporal gap from both prompting and model perspectives. Specifically,
we first propose a prompt combination strategy designed to enable LLM to model
the implicit relationship between prompts that characterize user intent and
video sequences. We then propose a parameter efficient box adapter that
augments the object semantic information in the global visual context so that
the visual token has a priori information about the user intent. The final
experiment proves that the combination of the two strategies can further
enhance the LVLM's ability to model spatial details in video sequences, and
facilitate the LVLMs to accurately generate controlled intent-oriented
captions. Our proposed method achieved state-of-the-art results in several open
source LVLMs and was the runner-up in the IntentVC challenge. Our code is
available on https://github.com/thqiu0419/IntentVCNet.

</details>


### [75] [COT-AD: Cotton Analysis Dataset](https://arxiv.org/abs/2507.18532)
*Akbar Ali,Mahek Vyas,Soumyaratna Debnath,Chanda Grover Kamra,Jaidev Sanjay Khalane,Reuben Shibu Devanesan,Indra Deep Mastan,Subramanian Sankaranarayanan,Pankaj Khanna,Shanmuganathan Raman*

Main category: cs.CV

TL;DR: 本文提出了COT-AD，一个专为棉花作物分析设计的大规模数据集，包含2.5万余张图像及详尽标注，支持多种视觉任务，填补了棉花特定农业数据集的空白。


<details>
  <summary>Details</summary>
Motivation: 当前棉花农业缺乏高质量、专门针对病虫害识别和田间管理的计算机视觉数据集，限制了数据驱动的智能农业发展。作者旨在解决这一数据缺口，支持更高效的棉花作物管理和早期病害检测。

Method: 作者采集了超过2.5万张棉花生长周期内的图像，其中包括空中影像（田块级检测与分割）、高分辨率单反相机图像（记录关键病害），并对5,000张图像进行了标注，涵盖病虫害识别、植被和杂草分析。

Result: COT-AD 数据集能够支持分类、分割、图像恢复与增强、深度生成模型的棉花作物综合合成以及早期病害管理等多项任务，同时弥补了行业内针对棉花专用视觉数据集的不足。

Conclusion: COT-AD为棉花作物数据驱动管理与分析提供了高质量的数据基础，有望促进智能农业、病虫害早防等多领域的研究和实践进步。

Abstract: This paper presents COT-AD, a comprehensive Dataset designed to enhance
cotton crop analysis through computer vision. Comprising over 25,000 images
captured throughout the cotton growth cycle, with 5,000 annotated images,
COT-AD includes aerial imagery for field-scale detection and segmentation and
high-resolution DSLR images documenting key diseases. The annotations cover
pest and disease recognition, vegetation, and weed analysis, addressing a
critical gap in cotton-specific agricultural datasets. COT-AD supports tasks
such as classification, segmentation, image restoration, enhancement, deep
generative model-based cotton crop synthesis, and early disease management,
advancing data-driven crop management

</details>


### [76] [Elucidating the Design Space of Arbitrary-Noise-Based Diffusion Models](https://arxiv.org/abs/2507.18534)
*Xingyu Qiu,Mengying Yang,Xinghua Ma,Dong Liang,Yuzhen Li,Fanding Li,Gongning Luo,Wei Wang,Kuanquan Wang,Shuo Li*

Main category: cs.CV

TL;DR: 本文提出EDA，一个允许任意噪声模式的扩散模型框架，有效地拓展了传统EDM仅限高斯噪声的局限性，并显著提升了图像恢复的效果。


<details>
  <summary>Details</summary>
Motivation: 传统EDM扩散模型只能使用高斯噪声，在图像恢复任务中会带来额外损伤和恢复难度，因此需要扩展噪声模式的灵活性，提高恢复质量和效率。

Method: 提出了一种名为EDA的新框架，允许扩散过程采用任意噪声分布（任意噪声模式），并在理论上证明即使噪声复杂性增加，也不会导致图像恢复的计算开销增加。EDA保留了EDM的灵活性和模块化。

Result: EDA在三个任务（MRI伪影校正、CT金属伪影去除、自然图像阴影去除）上进行了实验证明，只需5步采样即可超越大部分任务特定方法，并在偏场校正和阴影去除上达到了最新SOTA水平。

Conclusion: EDA为扩散模型的设计和实际应用提供了更大的噪声自由度，能够提升图像恢复任务的性能，其理论和实践具有广阔应用前景。

Abstract: EDM elucidates the unified design space of diffusion models, yet its fixed
noise patterns restricted to pure Gaussian noise, limit advancements in image
restoration. Our study indicates that forcibly injecting Gaussian noise
corrupts the degraded images, overextends the image transformation distance,
and increases restoration complexity. To address this problem, our proposed EDA
Elucidates the Design space of Arbitrary-noise-based diffusion models.
Theoretically, EDA expands the freedom of noise pattern while preserving the
original module flexibility of EDM, with rigorous proof that increased noise
complexity incurs no additional computational overhead during restoration. EDA
is validated on three typical tasks: MRI bias field correction (global smooth
noise), CT metal artifact reduction (global sharp noise), and natural image
shadow removal (local boundary-aware noise). With only 5 sampling steps, EDA
outperforms most task-specific methods and achieves state-of-the-art
performance in bias field correction and shadow removal.

</details>


### [77] [TTS-VAR: A Test-Time Scaling Framework for Visual Auto-Regressive Generation](https://arxiv.org/abs/2507.18537)
*Zhekai Chen,Ruihang Chu,Yukang Chen,Shiwei Zhang,Yujie Wei,Yingya Zhang,Xihui Liu*

Main category: cs.CV

TL;DR: 论文提出了一种名为TTS-VAR的视觉自回归(VAR)模型测试时扩展框架，针对提升生成模型测试效率和质量。该方法结合自适应批大小调整、结构多样性搜索与潜势评分筛选，实现高效且性能提升的视觉内容生成。实验证明在Infinity模型上GenEval得分提升明显。


<details>
  <summary>Details</summary>
Motivation: 视觉生成模型大规模训练与推理成本高昂，实际应用受限。为缓解资源压力，提高生成效果，需在测试阶段动态扩展模型生成能力，因此作者研究如何在不增加训练负担的情况下提升生成性能。

Method: 提出TTS-VAR，建模为路径搜索问题。一方面采用自适应递减批大小策略动态平衡效率与探索性。另一方面，引入两大核心机制：(1) 在粗略尺度上，根据结构信息聚类进行多样性搜索，保留样本结构多样性；(2) 在精细尺度上，通过重采样和定义潜势评分从历史生成中优选有潜力的样本。

Result: 实验基于Infinity VAR模型，TTS-VAR框架在GenEval分数上提升8.7%（由0.69到0.75）。分析发现，早期结构特征对最终生成质量有关键影响，且各尺度重采样效果不同。

Conclusion: TTS-VAR框架显著提升了VAR模型的测试时生成能力，为资源受限、无需额外训练情况下的视觉生成任务提供有效方案。该方法普适性强，有良好的性能与效率平衡。

Abstract: Scaling visual generation models is essential for real-world content
creation, yet requires substantial training and computational expenses.
Alternatively, test-time scaling has garnered growing attention due to resource
efficiency and promising performance. In this work, we present TTS-VAR, the
first general test-time scaling framework for visual auto-regressive (VAR)
models, modeling the generation process as a path searching problem. To
dynamically balance computational efficiency with exploration capacity, we
first introduce an adaptive descending batch size schedule throughout the
causal generation process. Besides, inspired by VAR's hierarchical
coarse-to-fine multi-scale generation, our framework integrates two key
components: (i) At coarse scales, we observe that generated tokens are hard for
evaluation, possibly leading to erroneous acceptance of inferior samples or
rejection of superior samples. Noticing that the coarse scales contain
sufficient structural information, we propose clustering-based diversity
search. It preserves structural variety through semantic feature clustering,
enabling later selection on samples with higher potential. (ii) In fine scales,
resampling-based potential selection prioritizes promising candidates using
potential scores, which are defined as reward functions incorporating
multi-scale generation history. Experiments on the powerful VAR model Infinity
show a notable 8.7% GenEval score improvement (from 0.69 to 0.75). Key insights
reveal that early-stage structural features effectively influence final
quality, and resampling efficacy varies across generation scales. Code is
available at https://github.com/ali-vilab/TTS-VAR.

</details>


### [78] [Unposed 3DGS Reconstruction with Probabilistic Procrustes Mapping](https://arxiv.org/abs/2507.18541)
*Chong Cheng,Zijian Wang,Sicheng Yu,Yu Hu,Nanjie Yao,Hao Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的针对3D高斯展布(3DGS)在无已知相机位姿条件下重建户外场景的方法，通过结合预训练MVS先验和概率Procrustes映射，实现了大规模图像数据的高效且精准的三维重建。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法高度依赖准确的相机位姿和点云初始化，但处理大规模、无已知相机位姿的户外图像组时，MVS模型存在内存和准确性瓶颈，因此需要一种解决大规模无位姿数据下三维重建的有效框架。

Method: 方法包括：1）将输入图像分块处理，每部分生成子图后映射到全局坐标系；2）将点云对齐问题建模为概率Procrustes问题，并利用软回收机制剔除不确定对应关系，从而全局对齐点云和相机位姿；3）结合3DGS可微渲染和解析雅可比矩阵，联合优化三维场景和相机位姿。

Result: 该方法在Waymo和KITTI数据集上进行了实验证明，能够从大量无位姿图像中重建高精度的三维场景，且优化和对齐过程在分钟级内完成，显著优于现有无位姿三维重建方法。

Conclusion: 所提方案突破了无位姿大规模三维重建的技术瓶颈，实现了高效精准的3DGS重建，推动了自动化户外场景建模的发展，成为无位姿3DGS领域的新标杆。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a core technique for 3D
representation. Its effectiveness largely depends on precise camera poses and
accurate point cloud initialization, which are often derived from pretrained
Multi-View Stereo (MVS) models. However, in unposed reconstruction task from
hundreds of outdoor images, existing MVS models may struggle with memory limits
and lose accuracy as the number of input images grows. To address this
limitation, we propose a novel unposed 3DGS reconstruction framework that
integrates pretrained MVS priors with the probabilistic Procrustes mapping
strategy. The method partitions input images into subsets, maps submaps into a
global space, and jointly optimizes geometry and poses with 3DGS. Technically,
we formulate the mapping of tens of millions of point clouds as a probabilistic
Procrustes problem and solve a closed-form alignment. By employing
probabilistic coupling along with a soft dustbin mechanism to reject uncertain
correspondences, our method globally aligns point clouds and poses within
minutes across hundreds of images. Moreover, we propose a joint optimization
framework for 3DGS and camera poses. It constructs Gaussians from
confidence-aware anchor points and integrates 3DGS differentiable rendering
with an analytical Jacobian to jointly refine scene and poses, enabling
accurate reconstruction and pose estimation. Experiments on Waymo and KITTI
datasets show that our method achieves accurate reconstruction from unposed
image sequences, setting a new state of the art for unposed 3DGS
reconstruction.

</details>


### [79] [A 3D Cross-modal Keypoint Descriptor for MR-US Matching and Registration](https://arxiv.org/abs/2507.18551)
*Daniil Morozov,Reuben Dorent,Nazim Haouchine*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的3D跨模态关键点描述符，用于术中超声（iUS）和术前磁共振（MRI）的匹配与配准，显著提升了两种影像间的自动配准精度。


<details>
  <summary>Details</summary>
Motivation: 术中超声与术前磁共振影像在分辨率、显像特性及视野范围上存在显著不同，导致两者配准困难，缺乏鲁棒且自动化程度高的方法。

Method: 作者开发了一个患者特异性的“匹配-生成”策略：首先由MRI影像合成超声影像，通过对比学习训练共享的3D关键点描述符，并采用概率性关键点检测与动态难负样本筛选的triplet loss实现鲁棒、旋转不变的描述符学习。推理时，自动检测关键点并实现稀疏匹配，进一步用于刚性配准。

Result: 在ReMIND数据集上的实验表明，所提方法在11位患者数据上，平均匹配精度达到69.8%，影像配准的目标误差为2.39mm（ReMIND2Reg 基准），均优于现有关键点匹配与配准方法。

Conclusion: 该方法无需人工初始化，对超声视野变化具有鲁棒性，且具有良好的可解释性，在高精度MRI-iUS 自动配准领域具有潜力。

Abstract: Intraoperative registration of real-time ultrasound (iUS) to preoperative
Magnetic Resonance Imaging (MRI) remains an unsolved problem due to severe
modality-specific differences in appearance, resolution, and field-of-view. To
address this, we propose a novel 3D cross-modal keypoint descriptor for MRI-iUS
matching and registration. Our approach employs a patient-specific
matching-by-synthesis approach, generating synthetic iUS volumes from
preoperative MRI. This enables supervised contrastive training to learn a
shared descriptor space.
  A probabilistic keypoint detection strategy is then employed to identify
anatomically salient and modality-consistent locations. During training, a
curriculum-based triplet loss with dynamic hard negative mining is used to
learn descriptors that are i) robust to iUS artifacts such as speckle noise and
limited coverage, and ii) rotation-invariant . At inference, the method detects
keypoints in MR and real iUS images and identifies sparse matches, which are
then used to perform rigid registration. Our approach is evaluated using 3D
MRI-iUS pairs from the ReMIND dataset. Experiments show that our approach
outperforms state-of-the-art keypoint matching methods across 11 patients, with
an average precision of $69.8\%$. For image registration, our method achieves a
competitive mean Target Registration Error of 2.39 mm on the ReMIND2Reg
benchmark.
  Compared to existing iUS-MR registration approach, our framework is
interpretable, requires no manual initialization, and shows robustness to iUS
field-of-view variation. Code is available at
https://github.com/morozovdd/CrossKEY.

</details>


### [80] [VideoMind: An Omni-Modal Video Dataset with Intent Grounding for Deep-Cognitive Video Understanding](https://arxiv.org/abs/2507.18552)
*Baoyao Yang,Wanyun Li,Dixin Chen,Junxiang Chen,Wenbin Yao,Haifeng Lin*

Main category: cs.CV

TL;DR: 本文提出了VideoMind，这是一个专注于视频内容认知的全模态（omni-modal）数据集，旨在提升多模态特征表示和深层视频理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态数据集对视频深层语义理解的支持有限，尤其是在主观意图表达等认知层面缺乏高质量数据。VideoMind通过提供细致分层的描述和意图表述，弥补了这一空白。

Method: 构建了包含10.3万条带有音频和丰富文本描述的视频样本数据集，每条视频进行三层次描述（事实、抽象、意图），并引入Chain-of-Thought方法驱动mLLM逐步生成深层认知表达。同时，对每条数据标注了主体、地点、时间、事件、行为和意图等要素。建立了3000条人工校验的金标用于评测，并设计多级检索实验和指标来验证模型视频理解能力。

Result: 基于VideoMind数据集，对InternVideo、VAST、UMT-L等模型进行了评测，相关结果已发布。混合认知检索实验表明，VideoMind有效支持了多级、深层的视频理解评测需求。

Conclusion: VideoMind是支持细粒度跨模态对齐与深层视频认知的权威基准，为情感、意图识别等领域的发展提供了坚实的数据基础。数据集现已开放获取。

Abstract: This paper introduces VideoMind, a video-centric omni-modal dataset designed
for deep video content cognition and enhanced multi-modal feature
representation. The dataset comprises 103K video samples (3K reserved for
testing), each paired with audio and systematically detailed textual
descriptions. Specifically, every video and its audio is described across three
hierarchical layers (factual, abstract, and intent), progressing from surface
to depth. It contains over 22 million words, averaging ~225 words per sample.
VideoMind's key distinction from existing datasets is its provision of intent
expressions, which require contextual integration across the entire video and
are not directly observable. These deep-cognitive expressions are generated
using a Chain-of-Thought (COT) approach, prompting the mLLM through
step-by-step reasoning. Each description includes annotations for subject,
place, time, event, action, and intent, supporting downstream recognition
tasks. Crucially, we establish a gold-standard benchmark with 3,000 manually
validated samples for evaluating deep-cognitive video understanding. We design
hybrid-cognitive retrieval experiments, scored by multi-level retrieval
metrics, to appropriately assess deep video comprehension. Evaluation results
for models (e.g., InternVideo, VAST, UMT-L) are released. VideoMind serves as a
powerful benchmark for fine-grained cross-modal alignment and advances fields
requiring in-depth video understanding, such as emotion and intent recognition.
The data is publicly available on GitHub, HuggingFace, and OpenDataLab,
https://github.com/cdx-cindy/VideoMind.

</details>


### [81] [Synthetic Data Augmentation for Enhanced Chicken Carcass Instance Segmentation](https://arxiv.org/abs/2507.18558)
*Yihong Feng,Chaitanya Pallerla,Xiaomin Lin,Pouya Sohrabipour Sr,Philip Crandall,Wan Shou,Yu She,Dongyi Wang*

Main category: cs.CV

TL;DR: 本研究提出了一套生成鸡胴体照片级真实感合成图像并自动标注的流水线，并发布了一个用于家禽分割的真实数据集。通过用合成数据增强实例分割模型，提升了在数据稀缺环境下的识别效果。


<details>
  <summary>Details</summary>
Motivation: 在家禽屠宰加工过程中，自动化检测鸡胴体对于质量控制和食品安全至关重要。但训练深度学习模型通常依赖大量人工标注的真实图像，获取和标注这些数据既耗时又昂贵，因此需要更高效的数据获取手段。

Method: 作者开发了一个能够自动生成带标签的高仿真鸡胴体合成图像的流水线，并建立了300张已标注真实图片的数据集。通过合成数据与少量真实数据相结合，利用主流实例分割模型进行训练和评估。

Result: 实验结果表明，合成数据的加入显著提升了不同实例分割模型对鸡胴体的分割性能，尤其在真实标注数据有限时效果更为明显。

Conclusion: 合成数据增强是一种可行且有效的解决数据稀缺、降低人工标注成本的方法，有助于推动家禽屠宰行业中基于AI的自动检测系统的发展。

Abstract: The poultry industry has been driven by broiler chicken production and has
grown into the world's largest animal protein sector. Automated detection of
chicken carcasses on processing lines is vital for quality control, food
safety, and operational efficiency in slaughterhouses and poultry processing
plants. However, developing robust deep learning models for tasks like instance
segmentation in these fast-paced industrial environments is often hampered by
the need for laborious acquisition and annotation of large-scale real-world
image datasets. We present the first pipeline generating photo-realistic,
automatically labeled synthetic images of chicken carcasses. We also introduce
a new benchmark dataset containing 300 annotated real-world images, curated
specifically for poultry segmentation research. Using these datasets, this
study investigates the efficacy of synthetic data and automatic data annotation
to enhance the instance segmentation of chicken carcasses, particularly when
real annotated data from the processing line is scarce. A small real dataset
with varying proportions of synthetic images was evaluated in prominent
instance segmentation models. Results show that synthetic data significantly
boosts segmentation performance for chicken carcasses across all models. This
research underscores the value of synthetic data augmentation as a viable and
effective strategy to mitigate data scarcity, reduce manual annotation efforts,
and advance the development of robust AI-driven automated detection systems for
chicken carcasses in the poultry processing industry.

</details>


### [82] [Deep Learning-Based Age Estimation and Gender Deep Learning-Based Age Estimation and Gender Classification for Targeted Advertisement](https://arxiv.org/abs/2507.18565)
*Muhammad Imran Zaman,Nisar Ahmed*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的新方法，实现了人脸图像的年龄和性别同时分类，并显著提升了分类准确率。


<details>
  <summary>Details</summary>
Motivation: 针对广告投放等应用中对人脸年龄和性别精准识别的需求，克服以往方法分别处理两任务而忽视二者相关性的不足。

Method: 设计了一种自定义CNN架构，同时学习年龄和性别的共享表示，在大规模多样化人脸数据集上训练，优化了预处理以提升对光照、姿态及图像质量变化的鲁棒性。

Result: 性别分类准确率达到95%，年龄估计的平均绝对误差为5.77年，对不同年龄段的表现进行了详细分析，发现年轻人年龄估计存在挑战。

Conclusion: 联合建模能提升任务表现，不同年龄段模型存在偏差，需要针对性增强数据和改进模型，实验还对不同架构和参数设置的影响进行了分析，可为后续研究提供参考。

Abstract: This paper presents a novel deep learning-based approach for simultaneous age
and gender classification from facial images, designed to enhance the
effectiveness of targeted advertising campaigns. We propose a custom
Convolutional Neural Network (CNN) architecture, optimized for both tasks,
which leverages the inherent correlation between age and gender information
present in facial features. Unlike existing methods that often treat these
tasks independently, our model learns shared representations, leading to
improved performance. The network is trained on a large, diverse dataset of
facial images, carefully pre-processed to ensure robustness against variations
in lighting, pose, and image quality. Our experimental results demonstrate a
significant improvement in gender classification accuracy, achieving 95%, and a
competitive mean absolute error of 5.77 years for age estimation. Critically,
we analyze the performance across different age groups, identifying specific
challenges in accurately estimating the age of younger individuals. This
analysis reveals the need for targeted data augmentation and model refinement
to address these biases. Furthermore, we explore the impact of different CNN
architectures and hyperparameter settings on the overall performance, providing
valuable insights for future research.

</details>


### [83] [Facial Demorphing from a Single Morph Using a Latent Conditional GAN](https://arxiv.org/abs/2507.18566)
*Nitish Shukla,Arun Ross*

Main category: cs.CV

TL;DR: 该论文提出了一种新的解混合（demorphing）方法，能有效从复杂的融合人脸图像中恢复出原始人脸，解决了现有方法复现混合图像及过度依赖训练生成技术的问题，并在真实人脸融合图像上获得显著提升。


<details>
  <summary>Details</summary>
Motivation: 人脸融合攻击通过将多个人脸图像合成为一个复合图，使得该伪造图像可与多个人的生物特征匹配，这给安全系统带来隐患。虽然现有的融合攻击检测（MAD）方法能检测出伪造图像，却无法复原出混合前的原始人脸，限制了证据收集和取证能力。因此，开发可靠的解混合技术以揭示原始身份变得至关重要。

Method: 论文提出了一种在潜在空间中对人脸混合图像进行分解的解混合方法。该方法通过在合成脸上训练模型，使其具备对未见过的融合技术和不同人脸风格的泛化能力，并能在测试阶段应用于真实人脸和任意融合技术。

Result: 实验表明，该方法在真实人脸的融合攻击下，比现有解混合方法具有明显更好的分辨力和复原质量，成功还原了高保真度的原始人脸图像。

Conclusion: 本方法突破了当前解混合手段的局限，可适用于未知融合手法和多样人脸风格的场景，极大提升了人脸鉴伪和取证的效果，对生物识别安全有重要意义。

Abstract: A morph is created by combining two (or more) face images from two (or more)
identities to create a composite image that is highly similar to both
constituent identities, allowing the forged morph to be biometrically
associated with more than one individual. Morph Attack Detection (MAD) can be
used to detect a morph, but does not reveal the constituent images. Demorphing
- the process of deducing the constituent images - is thus vital to provide
additional evidence about a morph. Existing demorphing methods suffer from the
morph replication problem, where the outputs tend to look very similar to the
morph itself, or assume that train and test morphs are generated using the same
morph technique. The proposed method overcomes these issues. The method
decomposes a morph in latent space allowing it to demorph images created from
unseen morph techniques and face styles. We train our method on morphs created
from synthetic faces and test on morphs created from real faces using arbitrary
morph techniques. Our method outperforms existing methods by a considerable
margin and produces high fidelity demorphed face images.

</details>


### [84] [Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis](https://arxiv.org/abs/2507.18569)
*Yanzuo Lu,Yuxi Ren,Xin Xia,Shanchuan Lin,Xing Wang,Xuefeng Xiao,Andy J. Ma,Xiaohua Xie,Jian-Huang Lai*

Main category: cs.CV

TL;DR: 本文提出了一种改进的分数蒸馏方法，通过对抗分布匹配（ADM）替代传统的分布匹配蒸馏（DMD），在不牺牲性能的前提下，显著提升了生成模型在一步生成任务上的效果，并具备更高的效率。


<details>
  <summary>Details</summary>
Motivation: 现有的分布匹配蒸馏技术（如DMD）虽能将大型扩散模型高效压缩为一步或多步生成器，但其基于反向KL散度的优化方法在某些场景下容易导致模式崩溃（mode collapse），限制了模型表现。

Method: 作者提出ADM框架，利用扩散判别器在对抗训练下对齐真实与伪分数估计器的潜在预测。在极端的一步蒸馏任务上，引入同时作用于潜在空间与像素空间的混合判别器，结合分布损失预训练与对抗分蒸细调，形成统一的DMDX流程。

Result: 所提方法在SDXL上的一步生成任务优于DMD2方法，且更节省GPU资源。同时，在SD3-Medium、SD3.5-Large、CogVideoX等数据集上的多步ADM蒸馏也刷新了图像与视频生成的效率基准。

Conclusion: ADM通过对抗训练有效缓解了DMD方法的模式坍缩问题，提升了一步与多步扩散模型蒸馏的效果，并以更低算力实现更优性能，推动了高效图像与视频生成的发展。

Abstract: Distribution Matching Distillation (DMD) is a promising score distillation
technique that compresses pre-trained teacher diffusion models into efficient
one-step or multi-step student generators. Nevertheless, its reliance on the
reverse Kullback-Leibler (KL) divergence minimization potentially induces mode
collapse (or mode-seeking) in certain applications. To circumvent this inherent
drawback, we propose Adversarial Distribution Matching (ADM), a novel framework
that leverages diffusion-based discriminators to align the latent predictions
between real and fake score estimators for score distillation in an adversarial
manner. In the context of extremely challenging one-step distillation, we
further improve the pre-trained generator by adversarial distillation with
hybrid discriminators in both latent and pixel spaces. Different from the mean
squared error used in DMD2 pre-training, our method incorporates the
distributional loss on ODE pairs collected from the teacher model, and thus
providing a better initialization for score distillation fine-tuning in the
next stage. By combining the adversarial distillation pre-training with ADM
fine-tuning into a unified pipeline termed DMDX, our proposed method achieves
superior one-step performance on SDXL compared to DMD2 while consuming less GPU
time. Additional experiments that apply multi-step ADM distillation on
SD3-Medium, SD3.5-Large, and CogVideoX set a new benchmark towards efficient
image and video synthesis.

</details>


### [85] [HybridTM: Combining Transformer and Mamba for 3D Semantic Segmentation](https://arxiv.org/abs/2507.18575)
*Xinyu Wang,Jinghua Hou,Zhe Liu,Yingying Zhu*

Main category: cs.CV

TL;DR: 本文提出了HybridTM，一种结合了Transformer和Mamba的混合架构，实现了在3D语义分割任务上的高效且高性能表现，并在多个主流数据集上取得了最新的效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的方法在3D语义分割中表现出色，但由于注意力机制的二次复杂度，在大规模点云建模长距离依赖时受到限制。而Mamba虽然具有线性复杂度，处理效率高，但3D特征表达能力有限。因此，亟需方法能有效整合两者优点。

Method: 提出首个Transformer与Mamba结合的混合框架HybridTM，并创新性地设计了Inner Layer Hybrid Strategy，在每一层内部细粒度地融合了注意力机制和Mamba结构，实现了同时捕获长距离依赖与局部细节。

Result: 在ScanNet、ScanNet200、nuScenes等主流室内外3D数据集上，HybridTM表现优异，取得了最新的性能，并在不同数据集上展现出良好泛化性。

Conclusion: HybridTM有效结合了Transformer和Mamba的优势，填补了现有3D语义分割方法在计算效率与特征表达能力方面的空白，对3D点云语义分割研究具有重要推动作用。

Abstract: Transformer-based methods have demonstrated remarkable capabilities in 3D
semantic segmentation through their powerful attention mechanisms, but the
quadratic complexity limits their modeling of long-range dependencies in
large-scale point clouds. While recent Mamba-based approaches offer efficient
processing with linear complexity, they struggle with feature representation
when extracting 3D features. However, effectively combining these complementary
strengths remains an open challenge in this field. In this paper, we propose
HybridTM, the first hybrid architecture that integrates Transformer and Mamba
for 3D semantic segmentation. In addition, we propose the Inner Layer Hybrid
Strategy, which combines attention and Mamba at a finer granularity, enabling
simultaneous capture of long-range dependencies and fine-grained local
features. Extensive experiments demonstrate the effectiveness and
generalization of our HybridTM on diverse indoor and outdoor datasets.
Furthermore, our HybridTM achieves state-of-the-art performance on ScanNet,
ScanNet200, and nuScenes benchmarks. The code will be made available at
https://github.com/deepinact/HybridTM.

</details>


### [86] [DRWKV: Focusing on Object Edges for Low-Light Image Enhancement](https://arxiv.org/abs/2507.18594)
*Xuecheng Bai,Yuxiang Wang,Boyu Hu,Qinyuan Jie,Chuanzhi Xu,Hongru Xiao,Kechen Li,Vera Chung*

Main category: cs.CV

TL;DR: 本文提出了一种新的低光照图像增强模型DRWKV，有效提升了图像边缘连续性和细节，同时保持高性能和较低复杂度。


<details>
  <summary>Details</summary>
Motivation: 在极端低光照环境下提升图像质量十分困难，尤其是要兼顾边缘和结构细节的保留。现有方法在处理极端照明退化时，常常导致边缘模糊或细节损失。

Method: 1) 提出基于Global Edge Retinex理论的DRWKV模型，实现照明与边缘结构的有效解耦；2) 设计Evolving WKV Attention机制，通过螺旋扫描方式，更好地捕捉空间边缘连续性和不规则结构；3) 引入Bilateral Spectrum Aligner和专门设计的MS2-Loss，实现亮度与色度特征的对齐，提升视觉自然性、减少伪影。

Result: 在五个低光照图像增强基准测试集上，DRWKV在PSNR、SSIM和NIQE三个主流评测指标上均取得了领先性能，并保持了较低的计算复杂度。此外，在低光照多目标跟踪任务中也能提升下游表现，验证了模型的泛化能力。

Conclusion: DRWKV不仅提升了低光照图像增强的性能和边缘细节保持能力，同时其低复杂度和较强的泛化能力使其在实际应用中具有很大潜力。

Abstract: Low-light image enhancement remains a challenging task, particularly in
preserving object edge continuity and fine structural details under extreme
illumination degradation. In this paper, we propose a novel model, DRWKV
(Detailed Receptance Weighted Key Value), which integrates our proposed Global
Edge Retinex (GER) theory, enabling effective decoupling of illumination and
edge structures for enhanced edge fidelity. Secondly, we introduce Evolving WKV
Attention, a spiral-scanning mechanism that captures spatial edge continuity
and models irregular structures more effectively. Thirdly, we design the
Bilateral Spectrum Aligner (Bi-SAB) and a tailored MS2-Loss to jointly align
luminance and chrominance features, improving visual naturalness and mitigating
artifacts. Extensive experiments on five LLIE benchmarks demonstrate that DRWKV
achieves leading performance in PSNR, SSIM, and NIQE while maintaining low
computational complexity. Furthermore, DRWKV enhances downstream performance in
low-light multi-object tracking tasks, validating its generalization
capabilities.

</details>


### [87] [3D Software Synthesis Guided by Constraint-Expressive Intermediate Representation](https://arxiv.org/abs/2507.18625)
*Shuqing Li,Anson Y. Lam,Yun Peng,Wenxuan Wang,Michael R. Lyu*

Main category: cs.CV

TL;DR: 本文提出了一种新方法Scenethesis，实现了根据用户需求生成可控且可追踪的3D界面软件代码，在需求捕获和约束满足方面有显著技术突破。


<details>
  <summary>Details</summary>
Motivation: 现有二维（2D）界面软件自动生成已较成熟，但三维（3D）界面软件生成依然缺乏研究。当前方法不便于对3D软件中具体元素进行精细修改，且难以应对复杂空间和语义约束。

Method: 本文设计了Scenethesis体系结构，提出了专用中间表示语言ScenethesisLang，能够细粒度表达空间与约束，同时作为自然语言需求到可执行3D软件的桥梁。3D软件合成流程分为多个阶段，围绕ScenethesisLang进行独立验证、定向修改及系统化约束满足。

Result: Scenethesis在捕获用户需求和约束满足方面分别达到80%与90%以上，可并行处理100多个约束。BLIP-2视觉评测得分较现有最优方法提升42.8%。

Conclusion: Scenethesis为3D界面软件的自动生成提供了可追踪、可控制、约束表达力强的解决方案，优于现有技术，推动了3D软件生成自动化发展。

Abstract: Graphical user interface (UI) software has undergone a fundamental
transformation from traditional two-dimensional (2D) desktop/web/mobile
interfaces to spatial three-dimensional (3D) environments. While existing work
has made remarkable success in automated 2D software generation, such as
HTML/CSS and mobile app interface code synthesis, the generation of 3D software
still remains under-explored. Current methods for 3D software generation
usually generate the 3D environments as a whole and cannot modify or control
specific elements in the software. Furthermore, these methods struggle to
handle the complex spatial and semantic constraints inherent in the real world.
To address the challenges, we present Scenethesis, a novel
requirement-sensitive 3D software synthesis approach that maintains formal
traceability between user specifications and generated 3D software. Scenethesis
is built upon ScenethesisLang, a domain-specific language that serves as a
granular constraint-aware intermediate representation (IR) to bridge natural
language requirements and executable 3D software. It serves both as a
comprehensive scene description language enabling fine-grained modification of
3D software elements and as a formal constraint-expressive specification
language capable of expressing complex spatial constraints. By decomposing 3D
software synthesis into stages operating on ScenethesisLang, Scenethesis
enables independent verification, targeted modification, and systematic
constraint satisfaction. Our evaluation demonstrates that Scenethesis
accurately captures over 80% of user requirements and satisfies more than 90%
of hard constraints while handling over 100 constraints simultaneously.
Furthermore, Scenethesis achieves a 42.8% improvement in BLIP-2 visual
evaluation scores compared to the state-of-the-art method.

</details>


### [88] [SIDA: Synthetic Image Driven Zero-shot Domain Adaptation](https://arxiv.org/abs/2507.18632)
*Ye-Chan Kim,SeungJu Cha,Si-Woo Kim,Taewhan Kim,Dong-Jin Kim*

Main category: cs.CV

TL;DR: 本文提出了一种基于合成图像的高效零样本领域自适应方法SIDA，提升了对复杂领域变化的建模能力，并缩短了适应时间。


<details>
  <summary>Details</summary>
Motivation: 现有零样本领域自适应方法多依赖CLIP嵌入与文本描述模拟目标风格，但难以捕捉真实世界复杂变化，适应耗时较长。因此，作者尝试利用更多细致真实的图像数据线索，提升适应效果与效率。

Method: SIDA方法通过生成带有详细风格的合成源图像，并采用图像翻译技术将其转化为目标域风格，随后利用这些合成图像的风格特征建立对目标域的代理。核心方法包括Domain Mix和Patch Style Transfer两个模块，前者通过混合多种风格扩展域内表示，后者为不同图像块赋予不同风格，增强对复杂真实变化的建模能力。

Result: 在多种零样本领域自适应场景和特别具有挑战性的领域上，SIDA展示了优于现有方法的性能，且适应时间大幅缩短。

Conclusion: 本文所提SIDA方法无需目标域图像，利用合成图像多样风格进行高效适应，兼具性能与效率，具有实际应用价值。

Abstract: Zero-shot domain adaptation is a method for adapting a model to a target
domain without utilizing target domain image data. To enable adaptation without
target images, existing studies utilize CLIP's embedding space and text
description to simulate target-like style features. Despite the previous
achievements in zero-shot domain adaptation, we observe that these text-driven
methods struggle to capture complex real-world variations and significantly
increase adaptation time due to their alignment process. Instead of relying on
text descriptions, we explore solutions leveraging image data, which provides
diverse and more fine-grained style cues. In this work, we propose SIDA, a
novel and efficient zero-shot domain adaptation method leveraging synthetic
images. To generate synthetic images, we first create detailed, source-like
images and apply image translation to reflect the style of the target domain.
We then utilize the style features of these synthetic images as a proxy for the
target domain. Based on these features, we introduce Domain Mix and Patch Style
Transfer modules, which enable effective modeling of real-world variations. In
particular, Domain Mix blends multiple styles to expand the intra-domain
representations, and Patch Style Transfer assigns different styles to
individual patches. We demonstrate the effectiveness of our method by showing
state-of-the-art performance in diverse zero-shot adaptation scenarios,
particularly in challenging domains. Moreover, our approach achieves high
efficiency by significantly reducing the overall adaptation time.

</details>


### [89] [Identifying Prompted Artist Names from Generated Images](https://arxiv.org/abs/2507.18633)
*Grace Su,Sheng-Yu Wang,Aaron Hertzmann,Eli Shechtman,Jun-Yan Zhu,Richard Zhang*

Main category: cs.CV

TL;DR: 本文引入了一个用于识别文本到图像模型生成图片中被点名艺术家的基准数据集，并评估了不同识别方法的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型常被用于通过提示中明确指定艺术家（如“in the style of Greg Rutkowski”）来生成图片，这引发了关于版权与归属的争议。当前缺乏对模型生成图片中艺术家风格被识别能力的系统评估工具，因此亟需建立相关基准，促进模型的合理使用与治理。

Method: 作者构建了一个含195万张图片、涵盖110名艺术家的数据集，制定了四种泛化评测环境（如未见艺术家、高复杂度提示、多艺术家提示及不同生成器）。在此基础上系统评测了特征相似度、对比式风格描述、数据归属方法、监督式分类器及小样本原型网络的识别效果。

Result: 监督和小样本模型在已见艺术家与复杂提示场景下表现最好；风格描述方法在艺术家风格突出时泛化更好；但多艺术家提示下任务仍极具挑战。整体结果显示该领域仍有巨大提升空间。

Conclusion: 本文提出的基准和数据集为文本到图像模型应用中的艺术家识别和合理治理提供了统一测试平台，推动了相关研究进展。数据集和基准已公开发布以促进社区后续研究。

Abstract: A common and controversial use of text-to-image models is to generate
pictures by explicitly naming artists, such as "in the style of Greg
Rutkowski". We introduce a benchmark for prompted-artist recognition:
predicting which artist names were invoked in the prompt from the image alone.
The dataset contains 1.95M images covering 110 artists and spans four
generalization settings: held-out artists, increasing prompt complexity,
multiple-artist prompts, and different text-to-image models. We evaluate
feature similarity baselines, contrastive style descriptors, data attribution
methods, supervised classifiers, and few-shot prototypical networks.
Generalization patterns vary: supervised and few-shot models excel on seen
artists and complex prompts, whereas style descriptors transfer better when the
artist's style is pronounced; multi-artist prompts remain the most challenging.
Our benchmark reveals substantial headroom and provides a public testbed to
advance the responsible moderation of text-to-image models. We release the
dataset and benchmark to foster further research:
https://graceduansu.github.io/IdentifyingPromptedArtists/

</details>


### [90] [Captain Cinema: Towards Short Movie Generation](https://arxiv.org/abs/2507.18634)
*Junfei Xiao,Ceyuan Yang,Lvmin Zhang,Shengqu Cai,Yang Zhao,Yuwei Guo,Gordon Wetzstein,Maneesh Agrawala,Alan Yuille,Lu Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Captain Cinema的短片电影生成框架，通过文本剧情描述生成高质量、叙事连贯的短片。主要分为关键帧规划与视频合成两步，并采用MM-DiT与专用数据集训练，实现了多场景、长叙事的电影自动生成。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的发展，自动生成高质量、有完整故事线的电影视频在内容创作、娱乐等领域具备极大潜力。但现有方法在长剧情一致性、多场景切换以及高视觉质量同步实现上仍面临挑战。本文旨在提升短片电影自动生成的故事连贯性与画面质量。

Method: 方法分为两步：首先，基于输入的详细文本剧情描述，进行“自顶向下”的关键帧规划，生成代表完整叙事的关键场景帧；其次，这些关键帧作为条件，输入到支持长时依赖学习的视频生成模型（Multimodal Diffusion Transformers, MM-DiT）中，进行“自底向上”的视频动态片段生成。为提升多场景长剧情表现力，引入了交错训练策略，并收集/构建了专用电影级数据集。

Result: 实验表明，Captain Cinema在自动短片生成中，在视觉一致性与叙事连贯性方面表现优异，能够高效生成多场景、富有电影感的高质量短片，超越了现有基线方法。

Conclusion: Captain Cinema框架有效提升了短片自动生成的叙事连贯性与视觉质量，为自动化电影内容制作提供了新的技术路线与工具，推动了AI驱动视觉内容生成的发展。

Abstract: We present Captain Cinema, a generation framework for short movie generation.
Given a detailed textual description of a movie storyline, our approach firstly
generates a sequence of keyframes that outline the entire narrative, which
ensures long-range coherence in both the storyline and visual appearance (e.g.,
scenes and characters). We refer to this step as top-down keyframe planning.
These keyframes then serve as conditioning signals for a video synthesis model,
which supports long context learning, to produce the spatio-temporal dynamics
between them. This step is referred to as bottom-up video synthesis. To support
stable and efficient generation of multi-scene long narrative cinematic works,
we introduce an interleaved training strategy for Multimodal Diffusion
Transformers (MM-DiT), specifically adapted for long-context video data. Our
model is trained on a specially curated cinematic dataset consisting of
interleaved data pairs. Our experiments demonstrate that Captain Cinema
performs favorably in the automated creation of visually coherent and narrative
consistent short movies in high quality and efficiency. Project page:
https://thecinema.ai

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [91] [Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning](https://arxiv.org/abs/2507.17842)
*Yimeng Zhang,Tian Wang,Jiri Gesi,Ziyi Wang,Yuxuan Lu,Jiacheng Lin,Sinong Zhan,Vianne Gao,Ruochen Jiao,Junze Liu,Kun Qian,Yuxin Tang,Ran Xue,Houyu Zhang,Qingjun Cui,Yufan Guo,Dakuo Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Shop-R1的强化学习框架，用于提升大语言模型在模拟真实人类网络购物行为中的推理能力，实现了比现有方法高65%以上的相对提升。


<details>
  <summary>Details</summary>
Motivation: 现有利用LLM推理增强的行为预测方法，性能受限于模型原有的推理能力，难以进一步提升人类行为模拟的真实度。本文旨在突破这一性能瓶颈。

Method: Shop-R1将人类行为模拟分为理由生成和行为预测两个阶段，分别用不同奖励机制引导。理由生成利用模型内部信号进行自监督优化，行为预测则采用层次化奖励，并根据任务难度动态缩放，细致考察行为类型及其属性、数值等细粒度正确性。

Result: 实验结果表明，Shop-R1方法在下游任务上相较于基线获得了超过65%的性能提升。

Conclusion: 通过引入细致的任务分解与奖励设计，Shop-R1显著提升了大模型对复杂人类行为的模拟与推理能力，有效突破了现有SFT方法的性能瓶颈。

Abstract: Large Language Models (LLMs) have recently demonstrated strong potential in
generating 'believable human-like' behavior in web environments. Prior work has
explored augmenting training data with LLM-synthesized rationales and applying
supervised fine-tuning (SFT) to enhance reasoning ability, which in turn can
improve downstream action prediction. However, the performance of such
approaches remains inherently bounded by the reasoning capabilities of the
model used to generate the rationales. In this paper, we introduce Shop-R1, a
novel reinforcement learning (RL) framework aimed at enhancing the reasoning
ability of LLMs for simulation of real human behavior in online shopping
environments Specifically, Shop-R1 decomposes the human behavior simulation
task into two stages: rationale generation and action prediction, each guided
by distinct reward signals. For rationale generation, we leverage internal
model signals (e.g., logit distributions) to guide the reasoning process in a
self-supervised manner. For action prediction, we propose a hierarchical reward
structure with difficulty-aware scaling to prevent reward hacking and enable
fine-grained reward assignment. This design evaluates both high-level action
types and the correctness of fine-grained sub-action details (attributes and
values), rewarding outputs proportionally to their difficulty. Experimental
results show that our method achieves a relative improvement of over 65%
compared to the baseline.

</details>


### [92] [Dynamic and Generalizable Process Reward Modeling](https://arxiv.org/abs/2507.17849)
*Zhangyue Yin,Qiushi Sun,Zhiyuan Zeng,Qinyuan Cheng,Xipeng Qiu,Xuanjing Huang*

Main category: cs.CL

TL;DR: 本文提出了一种新的Process Reward Model（PRM）方法——DG-PRM，能够动态、细致且泛化地为大模型提供奖励信号。实验显示其在多个基准任务上均取得了显著提升，并具备良好的跨领域适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有的PRM方法主要依赖启发式方式，难以在不同领域间泛化。而LLM作为判定者的新方法虽然更具泛用性，但对奖励的利用过于粗糙，忽略了文本中深层次的指导意义。同时，静态、粗粒度的评价标准难以适应复杂的过程监督场景。

Method: 提出Dynamic and Generalizable Process Reward Modeling（DG-PRM）方法。核心包括建立‘奖励树’以存储和捕获细粒度、多维度的奖励标准，动态选择奖励信息实现逐步评分，并首次引入帕累托最优估计方法，识别区分性的正负样本对。

Result: 在主流基准数据集上，DG-PRM性能优异，在多种任务的密集奖励设定下均显著提升了模型表现。分析还发现，该方法在分布外场景下同样保持了很好的泛化能力。

Conclusion: DG-PRM能有效应对复杂、变化多样的任务奖励建模问题，展现了在泛化性、细粒度和适应性方面的明显优势。

Abstract: Process Reward Models (PRMs) are crucial for guiding Large Language Models
(LLMs) in complex scenarios by providing dense reward signals. However,
existing PRMs primarily rely on heuristic approaches, which struggle with
cross-domain generalization. While LLM-as-judge has been proposed to provide
generalized rewards, current research has focused mainly on feedback results,
overlooking the meaningful guidance embedded within the text. Additionally,
static and coarse-grained evaluation criteria struggle to adapt to complex
process supervision. To tackle these challenges, we propose Dynamic and
Generalizable Process Reward Modeling (DG-PRM), which features a reward tree to
capture and store fine-grained, multi-dimensional reward criteria. DG-PRM
dynamically selects reward signals for step-wise reward scoring. To handle
multifaceted reward signals, we pioneeringly adopt Pareto dominance estimation
to identify discriminative positive and negative pairs. Experimental results
show that DG-PRM achieves stunning performance on prevailing benchmarks,
significantly boosting model performance across tasks with dense rewards.
Further analysis reveals that DG-PRM adapts well to out-of-distribution
scenarios, demonstrating exceptional generalizability.

</details>


### [93] [VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL](https://arxiv.org/abs/2507.17896)
*Shubham Mohole,Sainyam Galhotra*

Main category: cs.CL

TL;DR: 本文介绍了VeriMinder系统，这是一个用于检测和缓解NLIDB（自然语言数据库接口）中分析性认知偏见的互动平台，并通过用户测试证实其显著提升了数据分析质量。


<details>
  <summary>Details</summary>
Motivation: 尽管自然语言数据库接口极大拓展了数据分析的受众，但使用者缺乏统计分析经验，容易在提出分析问题时带入认知偏见，因此亟需辅助工具以帮助普通用户规避分析中的'错误问题'陷阱。

Method: VeriMinder系统引入三大创新：（1）背景相关的语义偏见映射框架，根据具体分析场景识别偏见；（2）将Hard-to-Vary原则操作化，引导用户系统性分析；（3）基于优化的大型语言模型，结构化生成高质量领域特定提示，通过多轮候选生成、批判反馈和自我反思提升输出质量。

Result: 用户测试表明，82.5%的参与者认为VeriMinder对分析质量有正面影响。与其他方法比较，该系统在分析具体性、全面性与准确性指标上至少高出20%。

Conclusion: VeriMinder能够有效帮助用户规避提出有认知偏见或方向性错误的分析问题，提升数据分析的可靠性和科学性。同时其开放源代码便于社区进一步研究和应用。

Abstract: Application systems using natural language interfaces to databases (NLIDBs)
have democratized data analysis. This positive development has also brought
forth an urgent challenge to help users who might use these systems without a
background in statistical analysis to formulate bias-free analytical questions.
Although significant research has focused on text-to-SQL generation accuracy,
addressing cognitive biases in analytical questions remains underexplored. We
present VeriMinder, https://veriminder.ai, an interactive system for detecting
and mitigating such analytical vulnerabilities. Our approach introduces three
key innovations: (1) a contextual semantic mapping framework for biases
relevant to specific analysis contexts (2) an analytical framework that
operationalizes the Hard-to-Vary principle and guides users in systematic data
analysis (3) an optimized LLM-powered system that generates high-quality,
task-specific prompts using a structured process involving multiple candidates,
critic feedback, and self-reflection.
  User testing confirms the merits of our approach. In direct user experience
evaluation, 82.5% participants reported positively impacting the quality of the
analysis. In comparative evaluation, VeriMinder scored significantly higher
than alternative approaches, at least 20% better when considered for metrics of
the analysis's concreteness, comprehensiveness, and accuracy. Our system,
implemented as a web application, is set to help users avoid "wrong question"
vulnerability during data analysis. VeriMinder code base with prompts,
https://reproducibility.link/veriminder, is available as an MIT-licensed
open-source software to facilitate further research and adoption within the
community.

</details>


### [94] [One Whisper to Grade Them All](https://arxiv.org/abs/2507.17918)
*Nhan Phan,Anusha Porwal,Yaroslav Getman,Ekaterina Voskoboinik,Tamás Grósz,Mikko Kurimo*

Main category: cs.CL

TL;DR: 本文提出了一种高效的端到端整体口语自动评分系统，可同时评估多部分二语口语测试，用于2025 Speak & Improve Challenge。系统无须文本转录和分项模型，推理速度快，适合大规模语言学习应用。


<details>
  <summary>Details</summary>
Motivation: 现有自动口语评估系统通常需要对每一题单独建模，并依赖语音转录，效率低、成本高，不适合大规模应用。该研究寻求开发一种高效、简易且能直接整体评分的新系统。

Method: 采用单一Whisper-small编码器处理全部四段口语回答，经轻量聚合器汇总后直接输出最终得分，全过程无需转录文本或对每一部分建模。同时引入有效数据采样策略以提升数据利用效率和处理类别不平衡问题。

Result: 系统在评测中取得RMSE 0.384，优于基线的文本系统（0.44），且模型参数不超过168M（约为Whisper-small的70%）。通过数据采样策略，模型仅用44.8%说话人训练就能取得0.383 RMSE，表现更优，显示数据效率强。

Conclusion: 新提出的系统结构显著提升了整体口语评分效率和准确性，降低了参数和推理成本，使ASA系统更实用且易于大规模部署，对数据不平衡和小样本下亦表现良好。

Abstract: We present an efficient end-to-end approach for holistic Automatic Speaking
Assessment (ASA) of multi-part second-language tests, developed for the 2025
Speak & Improve Challenge. Our system's main novelty is the ability to process
all four spoken responses with a single Whisper-small encoder, combine all
information via a lightweight aggregator, and predict the final score. This
architecture removes the need for transcription and per-part models, cuts
inference time, and makes ASA practical for large-scale Computer-Assisted
Language Learning systems.
  Our system achieved a Root Mean Squared Error (RMSE) of 0.384, outperforming
the text-based baseline (0.44) while using at most 168M parameters (about 70%
of Whisper-small). Furthermore, we propose a data sampling strategy, allowing
the model to train on only 44.8% of the speakers in the corpus and still reach
0.383 RMSE, demonstrating improved performance on imbalanced classes and strong
data efficiency.

</details>


### [95] [Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text](https://arxiv.org/abs/2507.17944)
*Hulayyil Alshammari,Praveen Rao*

Main category: cs.CL

TL;DR: 本文评估了六种主流AI文本检测工具在识别DeepSeek生成文本（包括经过敌意扰动处理后）的能力，发现部分工具在原始检测中效果优秀，但在“人性化”伪装后准确率明显下降。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型（LLM）如DeepSeek广泛应用于文本创作，引发了对写作诚信与AI检测能力的担忧，并推动了检测工具的发展。以往研究主要聚焦于ChatGPT等知名LLM，DeepSeek的检测研究存在空白。

Method: 研究挑选6款主流检测工具，采用49个人工编写问答及其由DeepSeek-v3生成的AI回应，并利用同义改写和“人性化”等方法进行敌意扰动，最终共采集245个测试样本。检测工具接受原文和敌意处理文本测试；此外，还用few-shot和链式思考（CoT）提示DeepSeek对文本进行AI/人工判别。

Result: QuillBot和Copyleaks在原始及同义改写文本上检测准确率高，但AI Text Classifier与GPT-2效果不稳定。“人性化”扰动是最有效攻击，Copyleaks、QuillBot、GPTZero准确率分别降至71%、58%、52%。用few-shot和CoT提示DeepSeek自身进行检测时，五次提示下只错判一例（AI召回96%，人工召回100%）。

Conclusion: 主流AI检测工具对DeepSeek生成文本存在显著识别差异，对部分敌意扰动特别是“人性化”处理后的检测效果均大幅下降。few-shot与链式思考提示下DeepSeek检测准确率远超通用工具，显示模型自判能力值得进一步研究。

Abstract: Large language models (LLMs) have rapidly transformed the creation of written
materials. LLMs have led to questions about writing integrity, thereby driving
the creation of artificial intelligence (AI) detection technologies.
Adversarial attacks, such as standard and humanized paraphrasing, inhibit
detectors' ability to detect machine-generated text. Previous studies have
mainly focused on ChatGPT and other well-known LLMs and have shown varying
accuracy across detectors. However, there is a clear gap in the literature
about DeepSeek, a recently published LLM. Therefore, in this work, we
investigate whether six generally accessible AI detection tools -- AI Text
Classifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, and GPTZero -- can
consistently recognize text generated by DeepSeek. The detectors were exposed
to the aforementioned adversarial attacks. We also considered DeepSeek as a
detector by performing few-shot prompting and chain-of-thought reasoning (CoT)
for classifying AI and human-written text. We collected 49 human-authored
question-answer pairs from before the LLM era and generated matching responses
using DeepSeek-v3, producing 49 AI-generated samples. Then, we applied
adversarial techniques such as paraphrasing and humanizing to add 196 more
samples. These were used to challenge detector robustness and assess accuracy
impact. While QuillBot and Copyleaks showed near-perfect performance on
original and paraphrased DeepSeek text, others -- particularly AI Text
Classifier and GPT-2 -- showed inconsistent results. The most effective attack
was humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and
52% for GPTZero. Few-shot and CoT prompting showed high accuracy, with the best
five-shot result misclassifying only one of 49 samples (AI recall 96%, human
recall 100%).

</details>


### [96] [Are LLM Belief Updates Consistent with Bayes' Theorem?](https://arxiv.org/abs/2507.17951)
*Sohaib Imran,Ihor Kendiukhov,Matthew Broerman,Aditya Thomas,Riccardo Campanella,Rob Lamb,Peter M. Atkinson*

Main category: cs.CL

TL;DR: 本文通过提出新的Bayesian Coherence Coefficient（BCC）指标，并构建相关数据集，量化评估不同规模和能力的预训练语言模型依据贝叶斯定理更新信念的一致性。实验发现，大模型的信念一致性更符合贝叶斯原理。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解更大更强的语言模型是否能像贝叶斯定理那样更一致地更新其信念，从而深入洞察大模型的推理“可信度”以及对其治理和安全性的启示。

Method: 提出并定义了Bayesian Coherence Coefficient（BCC）作为量化语言模型信念一致性的新指标，同时构建数据集以测量BCC，并对五个模型系列的多个预训练模型进行了测试。进一步比较了模型参数量、训练数据和基准测试成绩对BCC的影响。

Result: 结果表明，参数量更大、能力更强的模型，依据贝叶斯定理分配概率的表现更加一致。即，大型语言模型的信念在面对新证据时，更新得更为贝叶斯合理。

Conclusion: 论文结论为：模型规模和能力的提升，确实增强了模型信念与贝叶斯一致性。这对我们理解大型语言模型的认知能力和制定相关治理策略具有重要意义。

Abstract: Do larger and more capable language models learn to update their "beliefs"
about propositions more consistently with Bayes' theorem when presented with
evidence in-context? To test this, we formulate a Bayesian Coherence
Coefficient (BCC) metric and generate a dataset with which to measure the BCC.
We measure BCC for multiple pre-trained-only language models across five model
families, comparing against the number of model parameters, the amount of
training data, and model scores on common benchmarks. Our results provide
evidence for our hypothesis that larger and more capable pre-trained language
models assign credences that are more coherent with Bayes' theorem. These
results have important implications for our understanding and governance of
LLMs.

</details>


### [97] [Natural Language Processing for Tigrinya: Current State and Future Directions](https://arxiv.org/abs/2507.17974)
*Fitsum Gaim,Jong C. Park*

Main category: cs.CL

TL;DR: 本文综述了自2011年至2025年关于提格利尼亚语（Tigrinya）自然语言处理的40余项研究，涵盖计算资源、模型和应用进展，并公开相关元数据资源。


<details>
  <summary>Details</summary>
Motivation: 尽管提格利尼亚语有数百万使用者，但在NLP领域几乎未受到重视。因此，系统梳理现有研究进展与挑战，为未来研究提供指导和资源，是极为必要的。

Method: 作者对2011-2025年40+篇Tigrinya相关NLP论文、资源和模型进行了系统综述，涵盖形态学处理、机器翻译、语音识别、问答系统等十个任务，并对资源与模型发展进行了分析对比。

Result: 1. NLP方法由规则系统逐步演化至神经模型，取得持续进步；2. 资源建设与模型进步密切相关；3. 主要挑战为形态复杂性和资源匮乏。

Conclusion: 提格利尼亚NLP的发展受限于资源和语言特性。未来应加强形态感知建模、跨语言迁移以及社区资源共建，论文还为学者提供了研究路线图与资源导航。

Abstract: Despite being spoken by millions of people, Tigrinya remains severely
underrepresented in Natural Language Processing (NLP) research. This work
presents a comprehensive survey of NLP research for Tigrinya, analyzing over 40
studies spanning more than a decade of work from 2011 to 2025. We
systematically review the current state of computational resources, models, and
applications across ten distinct downstream tasks, including morphological
processing, machine translation, speech recognition, and question-answering.
Our analysis reveals a clear trajectory from foundational, rule-based systems
to modern neural architectures, with progress consistently unlocked by resource
creation milestones. We identify key challenges rooted in Tigrinya's
morphological complexity and resource scarcity, while highlighting promising
research directions, including morphology-aware modeling, cross-lingual
transfer, and community-centered resource development. This work serves as both
a comprehensive reference for researchers and a roadmap for advancing Tigrinya
NLP. A curated metadata of the surveyed studies and resources is made publicly
available.\footnote{Tigrinya NLP Anthology:
https://github.com/fgaim/tigrinya-nlp-anthology.

</details>


### [98] [Technical Report of TeleChat2, TeleChat2.5 and T1](https://arxiv.org/abs/2507.18013)
*Zihan Wang,Xinzhang Liu,Yitong Yao,Chao Wang,Yu Zhao,Zhihao Yang,Wenmin Deng,Kaipeng Jia,Jiaxin Peng,Yuyao Huang,Sishi Xiong,Zhuo Jiang,Kaidong Yu,Xiaohui Hu,Fubei Yao,Ruiyu Fang,Zhuoru Jiang,Ruiting Song,Qiyi Xie,Rui Xue,Xuewei He,Yanlei Xue,Zhu Yuan,Zhaoxi Zhang,Zilu Huang,Shiquan Wang,Xin Wang,Hanming Wu,Mingyuan Wang,Xufeng Zhan,Yuhan Sun,Zhaohu Xing,Yuhao Jiang,Bingkai Yang,Shuangyong Song,Yongxiang Li,Zhongjiang He,Xuelong Li*

Main category: cs.CL

TL;DR: 本文介绍了新一代TeleChat系列模型（TeleChat2、TeleChat2.5和T1），通过改进训练策略，在保持架构基本不变的情况下，在任务表现上实现了重大突破，并公开发布供开发者和研究者使用。


<details>
  <summary>Details</summary>
Motivation: 为了提升大语言模型在推理、代码生成和数学推理等任务上的性能，同时兼顾速度与多样应用需求，对原有TeleChat模型进行升级。

Method: 以10万亿高质量、多样化token预训练TeleChat2，并通过有监督微调(SFT)和直接偏好优化(DPO)增强能力。TeleChat2.5和T1进一步引入领域数据持续预训练和强化学习，用于强化代码和数学推理。T1专攻复杂长链推理和数学、代码任务，TeleChat2.5则优化推理速度。所有主力模型采用115B参数的密集Transformer架构。

Result: T1-115B在推理与通用任务上表现优于前代TeleChat，并超过OpenAI的o1-mini和GPT-4o等专有模型。

Conclusion: 新系列模型在性能和多样性方面实现突破，为开发者和研究者提供了具备前沿水平的大语言模型解决方案，推动相关应用发展。

Abstract: We introduce the latest series of TeleChat models: \textbf{TeleChat2},
\textbf{TeleChat2.5}, and \textbf{T1}, offering a significant upgrade over
their predecessor, TeleChat. Despite minimal changes to the model architecture,
the new series achieves substantial performance gains through enhanced training
strategies in both pre-training and post-training stages. The series begins
with \textbf{TeleChat2}, which undergoes pretraining on 10 trillion
high-quality and diverse tokens. This is followed by Supervised Fine-Tuning
(SFT) and Direct Preference Optimization (DPO) to further enhance its
capabilities. \textbf{TeleChat2.5} and \textbf{T1} expand the pipeline by
incorporating a continual pretraining phase with domain-specific datasets,
combined with reinforcement learning (RL) to improve performance in code
generation and mathematical reasoning tasks. The \textbf{T1} variant is
designed for complex reasoning, supporting long Chain-of-Thought (CoT)
reasoning and demonstrating substantial improvements in mathematics and coding.
In contrast, \textbf{TeleChat2.5} prioritizes speed, delivering rapid
inference. Both flagship models of \textbf{T1} and \textbf{TeleChat2.5} are
dense Transformer-based architectures with 115B parameters, showcasing
significant advancements in reasoning and general task performance compared to
the original TeleChat. Notably, \textbf{T1-115B} outperform proprietary models
such as OpenAI's o1-mini and GPT-4o. We publicly release \textbf{TeleChat2},
\textbf{TeleChat2.5} and \textbf{T1}, including post-trained versions with 35B
and 115B parameters, to empower developers and researchers with
state-of-the-art language models tailored for diverse applications.

</details>


### [99] [NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database](https://arxiv.org/abs/2507.18028)
*Weizhi Fei,Hao Shi,Jing Xu,Jingchen Peng,Jiazheng Li,Jingzhao Zhang,Bo Bai,Wei Han,Zhenyuan Chen,Xueyan Niu*

Main category: cs.CL

TL;DR: 本论文提出了一种名为NeuralDB的新框架，通过将被编辑的知识以神经键值数据库的方式显式存储，实现了对大型语言模型高效且大规模的知识编辑，并有效保持了原有模型的泛化能力和性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）被广泛应用，如何高效地在不经过大规模训练的前提下更新模型中的知识成为一个重要课题。目前主流的Locate-and-Edit（L&E）方法虽然能一次性修改大量事实，但易损害模型原有通用能力，并且在大规模编辑下容易遗忘已编辑的知识。因此，亟需一种既能大规模编辑又能保持模型能力的高效方法。

Method: 作者将现有的线性L&E方法建模为键值数据库查询，并提出NeuralDB框架：将被编辑事实以神经键值数据库方式显式存储，配备一个非线性门控检索模块。该模块仅在推理涉及编辑事实时工作，从而保护了模型的通用能力。

Result: 在ZsRE和CounterFacts数据集上，通过对GPT2-XL、GPT-J (6B)、Llama-3 (8B)进行最高10,000条事实编辑的实验显示，NeuralDB在编辑效率、泛化性、特异性、流畅性和一致性等方面表现优异，且在6个代表性NLP任务上均可保持整体性能。进一步实验表明，当扩展到10万条事实编辑时，NeuralDB依然保持有效性，比以往工作上限提升50倍。

Conclusion: NeuralDB不仅显著提升了LLMs大规模知识编辑的能力，同时很好地维持了模型的通用理解与生成能力，为知识可编辑的LLMs设计和应用提供了新范式。

Abstract: Efficiently editing knowledge stored in large language models (LLMs) enables
model updates without large-scale training. One possible solution is
Locate-and-Edit (L\&E), allowing simultaneous modifications of a massive number
of facts. However, such editing may compromise the general abilities of LLMs
and even result in forgetting edited facts when scaling up to thousands of
edits. In this paper, we model existing linear L\&E methods as querying a
Key-Value (KV) database. From this perspective, we then propose NeuralDB, an
editing framework that explicitly represents the edited facts as a neural KV
database equipped with a non-linear gated retrieval module, % In particular,
our gated module only operates when inference involves the edited facts,
effectively preserving the general abilities of LLMs. Comprehensive experiments
involving the editing of 10,000 facts were conducted on the ZsRE and
CounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results
demonstrate that NeuralDB not only excels in editing efficacy, generalization,
specificity, fluency, and consistency, but also preserves overall performance
across six representative text understanding and generation tasks. Further
experiments indicate that NeuralDB maintains its effectiveness even when scaled
to 100,000 facts (\textbf{50x} more than in prior work).

</details>


### [100] [GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs](https://arxiv.org/abs/2507.18043)
*Duy Nguyen,Archiki Prasad,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: 论文提出了一种新的推理时干预方法GrAInS，无需微调，通过调整模型内部激活对模型行为进行高效、细粒度和可解释的引导，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多数现有推理时干预方法仅使用全局固定向量，忽视了输入的具体token影响，也难以利用模型输出logit的梯度信息，尤其在多模态场景下无法有效区分视觉与文本输入的不同贡献。因而，亟需一种既能细致分析token影响，又能动态调整模型行为的高效方法。

Method: GrAInS方法基于Integrated Gradients获取对模型输出最有影响力的正、负token，通过这些token构建语义定向“引导向量”。在推理时，根据token归因对模型transformer层的激活进行调整，并对激活做归一，确保表示规模稳定。方法支持语言模型和视觉-语言模型，无需权重更新和辅助监督。

Result: 在多个任务上，GrAInS均取得显著提升，包括用Llama-3.1-8B在TruthfulQA上提升13.22%准确率，在LLaVA-1.6-7B上MMHal-Bench降低幻觉率（从0.624降至0.514），SPA-VL任务上alignment提升8.11%，同时保持输出流畅和通用能力。

Conclusion: GrAInS实现了无需额外训练或监督的、可解释的、模块化的推理时模型行为调控，性能超过微调与现有干预基线，展现推进大模型可控性的广泛前景。

Abstract: Inference-time steering methods offer a lightweight alternative to
fine-tuning large language models (LLMs) and vision-language models (VLMs) by
modifying internal activations at test time without updating model weights.
However, most existing approaches rely on fixed, global intervention vectors,
overlook the causal influence of individual input tokens, and fail to leverage
informative gradients from the model's logits, particularly in multimodal
settings where visual and textual inputs contribute unevenly. To address these
limitations, we introduce GrAInS, an inference-time steering approach that
operates across both language-only and vision-language models and tasks. GrAInS
uses contrastive, gradient-based attribution via Integrated Gradients to
identify the top-k most influential tokens, both positively and negatively
attributed based on their contribution to preferred versus dispreferred
outputs. These tokens are then used to construct directional steering vectors
that capture semantic shifts from undesirable to desirable behavior. During
inference, GrAInS adjusts hidden activations at transformer layers guided by
token-level attribution signals, and normalizes activations to preserve
representational scale. This enables fine-grained, interpretable, and modular
control over model behavior, without retraining or auxiliary supervision.
Empirically, GrAInS consistently outperforms both fine-tuning and existing
steering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using
Llama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514
with LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all
while preserving the model's fluency and general capabilities.

</details>


### [101] [Synthetic Data Generation for Phrase Break Prediction with Large Language Model](https://arxiv.org/abs/2507.18044)
*Hoyeon Lee,Sejung Son,Ye-Eun Kang,Jong-Hwan Kim*

Main category: cs.CL

TL;DR: 本论文提出利用大语言模型（LLM）生成短语断点的合成标注数据，有效降低人工标注成本，并在多语言场景下验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统短语断点预测方法高度依赖人工标注，既费时费力，又难以保证一致性和高质量数据。语音自身的变异性也导致高质量数据难以获取。急需新的自动化方法来缓解数据瓶颈。

Method: 该研究利用大语言模型（LLM）生成合成的短语断点标注数据，并将其与传统的人类标注数据进行比较。实验跨多种语言进行，评估LLM生成数据在短语断点预测任务中的可用性和有效性。

Result: 实验表明，基于LLM生成的合成数据在短语断点预测中表现良好，具有降低人工标注需求、适应多语种的优势。

Conclusion: LLM合成数据为短语断点预测提供了高效、低成本的新途径，展现了LLM应用于语音领域的广阔前景。

Abstract: Current approaches to phrase break prediction address crucial prosodic
aspects of text-to-speech systems but heavily rely on vast human annotations
from audio or text, incurring significant manual effort and cost. Inherent
variability in the speech domain, driven by phonetic factors, further
complicates acquiring consistent, high-quality data. Recently, large language
models (LLMs) have shown success in addressing data challenges in NLP by
generating tailored synthetic data while reducing manual annotation needs.
Motivated by this, we explore leveraging LLM to generate synthetic phrase break
annotations, addressing the challenges of both manual annotation and
speech-related tasks by comparing with traditional annotations and assessing
effectiveness across multiple languages. Our findings suggest that LLM-based
synthetic data generation effectively mitigates data challenges in phrase break
prediction and highlights the potential of LLMs as a viable solution for the
speech domain.

</details>


### [102] [Privacy-Preserving Synthetic Review Generation with Diverse Writing Styles Using LLMs](https://arxiv.org/abs/2507.18055)
*Tevin Atwal,Chan Nam Tieu,Yefeng Yuan,Zhan Shi,Yuhong Liu,Liang Cheng*

Main category: cs.CL

TL;DR: 本文评估了LLMs生成的文本型合成数据在多样性和隐私方面的表现，发现存在显著不足，并提出基于提示改进的方法。


<details>
  <summary>Details</summary>
Motivation: 合成数据能降低成本并支持规模化数据训练，但现有LLMs生成的数据在多样性与隐私保护方面缺乏系统评估，其风险和有效性尚不明确。

Method: 作者提出一套综合量化指标，针对多样性（语言表达、情感、用户视角）和隐私（可再识别风险、文体异常）评测主流LLMs生成的文本数据，并提出基于提示的多样性增强策略。

Result: 实验显示，主流LLMs难以生成既多样又注重隐私的高质量合成文本，存在语言表达单一、情感倾向偏窄及可识别风险等问题。

Conclusion: 当前LLMs生成的合成文本数据在多样性和隐私性上存在较大短板，需通过专门设计的提示或机制加以提升，这对数据驱动应用的安全和有效性至关重要。

Abstract: The increasing use of synthetic data generated by Large Language Models
(LLMs) presents both opportunities and challenges in data-driven applications.
While synthetic data provides a cost-effective, scalable alternative to
real-world data to facilitate model training, its diversity and privacy risks
remain underexplored. Focusing on text-based synthetic data, we propose a
comprehensive set of metrics to quantitatively assess the diversity (i.e.,
linguistic expression, sentiment, and user perspective), and privacy (i.e.,
re-identification risk and stylistic outliers) of synthetic datasets generated
by several state-of-the-art LLMs. Experiment results reveal significant
limitations in LLMs' capabilities in generating diverse and privacy-preserving
synthetic data. Guided by the evaluation results, a prompt-based approach is
proposed to enhance the diversity of synthetic reviews while preserving
reviewer privacy.

</details>


### [103] [TELEVAL: A Dynamic Benchmark Designed for Spoken Language Models in Chinese Interactive Scenarios](https://arxiv.org/abs/2507.18061)
*Zehan Li,Hongjie Chen,Yuxin Zhang,Jing Zhou,Xuening Wang,Hang Lv,Mengjie Du,Yaodong Song,Jie Lian,Jian Kang,Jie Li,Yongxiang Li,Zhongjiang He,Xuelong Li*

Main category: cs.CL

TL;DR: 该论文提出了一个名为 TELEVAL 的新型基准，用于更真实地评估口语语言模型（SLM）在中文场景下作为对话代理的表现，强调自然互动而非传统复杂任务。


<details>
  <summary>Details</summary>
Motivation: 现有的口语语言模型评测往往不符合真实用户的交互需求，过度关注模型是否能完成像大型语言模型那样的复杂任务，忽视了真实对话中的语用和交互体验，因此需要一个更贴近实际应用的评测方法。

Method: 提出了 TELEVAL 基准，从显性语义、语用/隐性语义以及系统能力三个维度进行评测，采用与真实对话一致的对话格式，分别评价文本和音频输出，特别考察模型能否无提示地从用户语音中提取隐含信息并做出合适回复。

Result: 实验表明，尽管SLM取得了快速进步，但在自然对话任务中仍有很大的提升空间。现有模型在理解和回应用户隐性语义及真实对话情境下的表现不尽理想。

Conclusion: TELEVAL 可作为以用户为中心的评测框架，更好地反映用户体验，并促进未来更强对话型口语语言模型的研发。

Abstract: Spoken language models (SLMs) have seen rapid progress in recent years, along
with the development of numerous benchmarks for evaluating their performance.
However, most existing benchmarks primarily focus on evaluating whether SLMs
can perform complex tasks comparable to those tackled by large language models
(LLMs), often failing to align with how users naturally interact in real-world
conversational scenarios. In this paper, we propose TELEVAL, a dynamic
benchmark specifically designed to evaluate SLMs' effectiveness as
conversational agents in realistic Chinese interactive settings. TELEVAL
defines three evaluation dimensions: Explicit Semantics, Paralinguistic and
Implicit Semantics, and System Abilities. It adopts a dialogue format
consistent with real-world usage and evaluates text and audio outputs
separately. TELEVAL particularly focuses on the model's ability to extract
implicit cues from user speech and respond appropriately without additional
instructions. Our experiments demonstrate that despite recent progress,
existing SLMs still have considerable room for improvement in natural
conversational tasks. We hope that TELEVAL can serve as a user-centered
evaluation framework that directly reflects the user experience and contributes
to the development of more capable dialogue-oriented SLMs.

</details>


### [104] [Hybrid and Unitary Fine-Tuning of Large Language Models: Methods and Benchmarking under Resource Constraints](https://arxiv.org/abs/2507.18076)
*Haomin Qi,Zihan Dai,Chengbo Huang*

Main category: cs.CL

TL;DR: 论文提出了一种混合PEFT方法，通过结合BOFT和LoRA-GA，并引入梯度归一自适应更新，提高了大语言模型参数高效微调的效果，兼顾收敛速度和泛化能力，显著减少了训练资源消耗。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的微调成本高，尤其在计算和内存方面，对实际应用造成很大限制。本文旨在寻找一种更加节省资源、同时效果不输全微调的方法。

Method: 系统评估了多种主流参数高效微调（PEFT）技术（如LoRA, BOFT, LoRA-GA 和 uRNN），并创新性提出一种动态融合BOFT正交稳定性和LoRA-GA梯度对齐快速收敛的混合策略。该方法按层自适应调整更新步长，部分参考每层的梯度范数。首次探索了unitary RNN理念在transformer LLM中的应用，增加了梯度稳定性。

Result: 在GLUE、GSM8K、MT-Bench和HumanEval四个基准上，用7B到405B的模型测试，混合方法在收敛速度、泛化能力上均优于单一PEFT方案。性能接近全参数微调，但训练时间缩短最高2.1倍，内存消耗减少多达50%。

Conclusion: 混合PEFT方法具有极佳的可扩展性和实用性，在各种资源受限场景下能高效微调大语言模型，成为实际部署时的优选方案。

Abstract: Fine-tuning large language models (LLMs) remains a computational bottleneck
due to their scale and memory demands. This paper presents a comprehensive
evaluation of parameter-efficient fine-tuning (PEFT) techniques, including
LoRA, BOFT, LoRA-GA, and uRNN, and introduces a novel hybrid strategy that
dynamically integrates BOFT's orthogonal stability with LoRA-GA's
gradient-aligned rapid convergence. By computing per-layer adaptive updates
guided by gradient norms, the hybrid method achieves superior convergence
efficiency and generalization across diverse tasks. We also explore, for the
first time, the adaptation of unitary RNN (uRNN) principles to
transformer-based LLMs, enhancing gradient stability through structured unitary
constraints. Empirical evaluations on four benchmarks -- GLUE, GSM8K, MT-Bench,
and HumanEval -- using models ranging from 7B to 405B parameters demonstrate
that our hybrid method consistently outperforms individual PEFT baselines,
approaching full fine-tuning accuracy while reducing resource consumption by up
to 2.1 times in training time and 50 percent in memory usage. These findings
establish the hybrid approach as a practical and scalable fine-tuning solution
for real-world deployment of LLMs under resource constraints.

</details>


### [105] [A New Pair of GloVes](https://arxiv.org/abs/2507.18103)
*Riley Carlson,John Bauer,Christopher D. Manning*

Main category: cs.CL

TL;DR: 本文介绍并评估了2024年新版英文GloVe词向量模型。新版模型在数据、文档和性能上相较2014年旧版有重要更新。


<details>
  <summary>Details</summary>
Motivation: 语言和世界在不断变化，原有2014版GloVe映射模型虽影响广泛，但缺乏对新版语料、未记录详细数据处理流程，也未能及时反映新出现的词汇和用法。为此，作者推出了更能反映当前实际语言环境的GloVe新模型，并详细记录了模型数据和预处理方法。

Method: 作者利用2024年版的Wikipedia、Gigaword和Dolma子集重新训练了两组词向量模型。全过程对数据来源与预处理操作进行详细记录，然后通过词汇对比、结构性任务（类似词/类比）、以及命名实体识别（NER）任务进行系统评估。

Result: 新模型相比2014版，能够包含更多反映当前文化和语言的新词汇。在词类比和相似度等结构性任务上表现相当，在针对时效性强、内容含非西方新闻语料的NER任务中，表现有明显提升。

Conclusion: 新版2024 GloVe英文词向量模型不仅性能达标，而且优于旧版在反映新语料和词汇能力，适合当前自然语言处理任务。

Abstract: This report documents, describes, and evaluates new 2024 English GloVe
(Global Vectors for Word Representation) models. While the original GloVe
models built in 2014 have been widely used and found useful, languages and the
world continue to evolve and we thought that current usage could benefit from
updated models. Moreover, the 2014 models were not carefully documented as to
the exact data versions and preprocessing that were used, and we rectify this
by documenting these new models. We trained two sets of word embeddings using
Wikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary
comparison, direct testing, and NER tasks shows that the 2024 vectors
incorporate new culturally and linguistically relevant words, perform
comparably on structural tasks like analogy and similarity, and demonstrate
improved performance on recent, temporally dependent NER datasets such as
non-Western newswire data.

</details>


### [106] [GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness](https://arxiv.org/abs/2507.18119)
*Hongjie Chen,Zehan Li,Yaodong Song,Wenming Deng,Yitong Yao,Yuxin Zhang,Hang Lv,Xuechao Zhu,Jian Kang,Jie Lian,Jie Li,Chao Wang,Shuangyong Song,Yongxiang Li,Zhongjiang He*

Main category: cs.CL

TL;DR: 本文提出了一种新型的语音语言模型GOAT-SLM，首次关注除了语义以外的说话人特征与副语言信息，在多个维度和任务中性能优于现有开源模型。


<details>
  <summary>Details</summary>
Motivation: 尽管当前端到端语音语言模型已大幅提高AI的语音交互能力，但它们大多只关注语音中携带的语言内容，忽略了方言、情感、年龄等丰富的副语言和说话人特征。现实语音交互中，这些信息对于更自然自适应的人机对话至关重要。作者旨在补足这一短板，推动语音模型更好地捕捉和利用人类语音的多维度信息。

Method: GOAT-SLM采用双模态头结构，将语义建模与声学实现解耦，保证了语义理解和丰富的表达能力。此外，提出模块化、分阶段的训练策略，逐步对齐语义、副语言及说话人信息，利用大规模语音-文本语料实现高效训练。

Result: 在TELEVAL等多维评测基准上，GOAT-SLM在语义和非语义任务（如情感、方言、年龄等）上表现均衡，并在多项任务上超越现有开源模型。

Conclusion: 本文工作强调了副语言信息建模的重要性，并推进了更自然、自适应和具备社会意识的语音对话系统的发展。

Abstract: Recent advances in end-to-end spoken language models (SLMs) have
significantly improved the ability of AI systems to engage in natural spoken
interactions. However, most existing models treat speech merely as a vehicle
for linguistic content, often overlooking the rich paralinguistic and speaker
characteristic cues embedded in human speech, such as dialect, age, emotion,
and non-speech vocalizations. In this work, we introduce GOAT-SLM, a novel
spoken language model with paralinguistic and speaker characteristic awareness,
designed to extend spoken language modeling beyond text semantics. GOAT-SLM
adopts a dual-modality head architecture that decouples linguistic modeling
from acoustic realization, enabling robust language understanding while
supporting expressive and adaptive speech generation. To enhance model
efficiency and versatility, we propose a modular, staged training strategy that
progressively aligns linguistic, paralinguistic, and speaker characteristic
information using large-scale speech-text corpora. Experimental results on
TELEVAL, a multi-dimensional evaluation benchmark, demonstrate that GOAT-SLM
achieves well-balanced performance across both semantic and non-semantic tasks,
and outperforms existing open-source models in handling emotion, dialectal
variation, and age-sensitive interactions. This work highlights the importance
of modeling beyond linguistic content and advances the development of more
natural, adaptive, and socially aware spoken language systems.

</details>


### [107] [MathOPEval: A Fine-grained Evaluation Benchmark for Visual Operations of MLLMs in Mathematical Reasoning](https://arxiv.org/abs/2507.18140)
*Xiaoyuan Li,Moxin Li,Wenjie Wang,Rui Men,Yichang Zhang,Fuli Feng,Dayiheng Liu,Junyang Lin*

Main category: cs.CL

TL;DR: 本论文提出了评估多模态大语言模型（MLLM）在数学推理中，基于代码实现视觉操作能力的新方法，并发现当前主流模型在细致视觉操作上距离人类水平还有较大差距。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在数学推理中，虽然能结合图像与文本，但评测主要集中在文本输出，模型通过代码实现精准视觉操作的能力鲜有系统评估。因此，亟需填补这一评测空白。

Method: 作者提出了多模态代码生成（MCG）和多模态代码编辑（MCE）两个评测任务，分别用于评测模型自主生成可视化图形及精细编辑图形的能力，同时构建了涵盖五类主流数学图形的数据集，并针对九个主流MLLM进行了实验评估。

Result: 实验发现，当前MLLM虽然在宏观图形生成上可用，但在删除、修改和注释等细粒度视觉操作任务上表现远逊于人类，存在明显短板。

Conclusion: 现有多模态大语言模型在基于代码的细致视觉操作上仍有很大提升空间，针对这一能力的系统评测框架能够推动模型在多模态数学推理的应用升级。

Abstract: Recent progress in Multi-modal Large Language Models (MLLMs) has enabled
step-by-step multi-modal mathematical reasoning by performing visual operations
based on the textual instructions. A promising approach uses code as an
intermediate representation to precisely express and manipulate the images in
the reasoning steps. However, existing evaluations focus mainly on text-only
reasoning outputs, leaving the MLLM's ability to perform accurate visual
operations via code largely unexplored. This work takes a first step toward
addressing that gap by evaluating MLLM's code-based capabilities in multi-modal
mathematical reasoning.Specifically, our framework focuses on two key
evaluation aspects: (1) Multi-modal Code Generation (MCG) evaluates the model's
ability to accurately understand and construct visualizations from scratch. (2)
Multi-modal Code Editing (MCE) assesses the model's capacity for fine-grained
operations, which include three types: Deletion, Modification and Annotation.
To evaluate the above tasks, we incorporate a dataset that covers the five most
popular types of mathematical figures, including geometric diagrams, function
plots, and three types of statistical charts, to provide a comprehensive and
effective measurement of existing MLLMs. Our experimental evaluation involves
nine mainstream MLLMs, and the results reveal that existing models still lag
significantly behind human performance in performing fine-grained visual
operations.

</details>


### [108] [HIVMedQA: Benchmarking large language models for HIV medical decision support](https://arxiv.org/abs/2507.18143)
*Gonzalo Cardenal Antolin,Jacques Fellay,Bashkim Jaha,Roger Kouyos,Niko Beerenwinkel,Diane Duroux*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型在HIV管理中的表现，并推出了专门的基准数据集HIVMedQA。结果显示通用LLM与医学专用LLM表现各有优劣，准确性与安全性仍需关注。


<details>
  <summary>Details</summary>
Motivation: HIV治疗复杂，涉及多种选择及并发症，传统临床决策存在挑战。大型语言模型或可成为辅助工具，但其精确度和安全性尚存疑。该领域的AI应用和基准研究稀缺，需要系统评估。

Method: 研究提出HIVMedQA问答基准，由感染科医生参与问题编写。基于该数据集，评测七个通用LLM和三个医学专用LLM，通过Prompt工程优化模型表现，采用文本相似度与LLM评审相结合的方式，从理解、推理、知识、偏见、伤害和准确性六个维度综合评测。

Result: Gemini 2.5 Pro在多数维度表现最佳，前三甲有两个是闭源模型。题目复杂度上升时模型表现下降；医学专用LLM并不总优于通用LLM，模型体量亦非性能可靠指标。理解与推理难度高于事实回忆，且存在近期性、现状偏见等认知偏差。

Conclusion: 当前LLM在HIV管理中表现尚有不足，需进一步针对医疗场景进行定向开发和评估，确保其安全有效地进入临床实践。

Abstract: Large language models (LLMs) are emerging as valuable tools to support
clinicians in routine decision-making. HIV management is a compelling use case
due to its complexity, including diverse treatment options, comorbidities, and
adherence challenges. However, integrating LLMs into clinical practice raises
concerns about accuracy, potential harm, and clinician acceptance. Despite
their promise, AI applications in HIV care remain underexplored, and LLM
benchmarking studies are scarce. This study evaluates the current capabilities
of LLMs in HIV management, highlighting their strengths and limitations. We
introduce HIVMedQA, a benchmark designed to assess open-ended medical question
answering in HIV care. The dataset consists of curated, clinically relevant
questions developed with input from an infectious disease physician. We
evaluated seven general-purpose and three medically specialized LLMs, applying
prompt engineering to enhance performance. Our evaluation framework
incorporates both lexical similarity and an LLM-as-a-judge approach, extended
to better reflect clinical relevance. We assessed performance across key
dimensions: question comprehension, reasoning, knowledge recall, bias,
potential harm, and factual accuracy. Results show that Gemini 2.5 Pro
consistently outperformed other models across most dimensions. Notably, two of
the top three models were proprietary. Performance declined as question
complexity increased. Medically fine-tuned models did not always outperform
general-purpose ones, and larger model size was not a reliable predictor of
performance. Reasoning and comprehension were more challenging than factual
recall, and cognitive biases such as recency and status quo were observed.
These findings underscore the need for targeted development and evaluation to
ensure safe, effective LLM integration in clinical care.

</details>


### [109] [Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models](https://arxiv.org/abs/2507.18171)
*Kexin Chen,Dongxia Wang,Yi Liu,Haonan Zhang,Wenhai Wang*

Main category: cs.CL

TL;DR: 论文发现Transformer文本嵌入模型中存在“sticky tokens”异常问题，这些特殊token严重影响模型嵌入效果和下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 在NLP任务中，Transformer文本嵌入广泛应用，但有时会出现距离分布异常，嵌入不稳健，影响后续任务，作者怀疑源于某些特殊token。

Method: 作者对sticky tokens进行正式定义，并提出了高效检测方法STD（Sticky Token Detector），通过句子与token过滤，从14个模型家族的40个checkpoint检测出共868个sticky tokens。同时，分析其来源、影响及机制。

Result: 发现sticky tokens多来自词表中特殊/未用项及多语料带来的碎片子词，对模型size无严格相关。sticky tokens会导致聚类、检索等下游性能最高下降50%。通过注意力层分析发现这些token极大主导模型内部表示。

Conclusion: sticky tokens对文本嵌入稳健性危害较大，应改进tokenization策略及模型设计，以提升未来文本嵌入的可靠性。

Abstract: Despite the widespread use of Transformer-based text embedding models in NLP
tasks, surprising 'sticky tokens' can undermine the reliability of embeddings.
These tokens, when repeatedly inserted into sentences, pull sentence similarity
toward a certain value, disrupting the normal distribution of embedding
distances and degrading downstream performance. In this paper, we
systematically investigate such anomalous tokens, formally defining them and
introducing an efficient detection method, Sticky Token Detector (STD), based
on sentence and token filtering. Applying STD to 40 checkpoints across 14 model
families, we discover a total of 868 sticky tokens. Our analysis reveals that
these tokens often originate from special or unused entries in the vocabulary,
as well as fragmented subwords from multilingual corpora. Notably, their
presence does not strictly correlate with model size or vocabulary size. We
further evaluate how sticky tokens affect downstream tasks like clustering and
retrieval, observing significant performance drops of up to 50%. Through
attention-layer analysis, we show that sticky tokens disproportionately
dominate the model's internal representations, raising concerns about
tokenization robustness. Our findings show the need for better tokenization
strategies and model design to mitigate the impact of sticky tokens in future
text embedding applications.

</details>


### [110] [SCOPE: Stochastic and Counterbiased Option Placement for Evaluating Large Language Models](https://arxiv.org/abs/2507.18182)
*Wonjun Jeong,Dongseok Kim,Taegkeun Whangbo*

Main category: cs.CL

TL;DR: 该论文指出大语言模型在选择题评测中可通过利用选项位置或标签的偏差刷高分数，而非真正理解题意。作者提出SCOPE评测框架，用于测量和缓解模型的选择偏差，并在多项基准测试中展现出比现有去偏方法更稳健的提升。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在多选题测试中，常因选项排列或标签的固有偏差获取虚高分数，影响对模型真实理解能力的评估，缺乏有效且通用的去偏评价框架。

Method: SCOPE框架通过多次输入“无语义内容”的空提示，估算模型自身的选项位次偏好分布，然后用其逆分布重新分配正确答案槽，均衡模型‘走运猜中’的概率。此外，SCOPE还避免将语义相近的干扰选项与正确答案放在一起，以阻断模型通过表面接近性进行投机猜测。

Result: SCOPE在多个基准任务中，相较现有去偏技术，展现了更稳定的性能提升，并能给出更明确的正确选项置信分布。

Conclusion: SCOPE为LLM的评估提供了新的公平性与可靠性标准，能有效识别和缓解模型在多选任务中的选择偏差。

Abstract: Large Language Models (LLMs) can achieve inflated scores on multiple-choice
tasks by exploiting inherent biases in option positions or labels, rather than
demonstrating genuine understanding. This study introduces SCOPE, an evaluation
framework designed to measure and mitigate such selection bias in a
dataset-independent manner. By repeatedly invoking a null prompt that lacks
semantic content, SCOPE estimates each model's unique position-bias
distribution. It then redistributes the answer slot according to the
inverse-bias distribution, thereby equalizing the lucky-rate, the probability
of selecting the correct answer by chance. Furthermore, it prevents
semantically similar distractors from being placed adjacent to the answer,
thereby blocking near-miss guesses based on superficial proximity cues. Across
multiple benchmark experiments, SCOPE consistently outperformed existing
debiasing methods in terms of stable performance improvements and showed
clearer confidence distributions over correct options. This framework thus
offers a new standard for enhancing the fairness and reliability of LLM
evaluations.

</details>


### [111] [TN-AutoRCA: Benchmark Construction and Agentic Framework for Self-Improving Alarm-Based Root Cause Analysis in Telecommunication Networks](https://arxiv.org/abs/2507.18190)
*Keyu Wu,Qianjin Yu,Manlin Mei,Ruiting Liu,Jun Wang,Kailai Zhang,Yelun Bao*

Main category: cs.CL

TL;DR: 本文关注电信网络中的根本原因分析（RCA）问题，指出该任务对AI来说具有挑战性，主要原因在于其需要复杂的图结构推理和缺乏真实基准数据。


<details>
  <summary>Details</summary>
Motivation: 电信网络发生故障时，需快速定位根本原因以保证业务连续性。然而，现有AI系统难以处理复杂的网络拓扑和相互依赖的问题，且缺乏可用于模型开发与评估的真实数据集。

Method: 论文主要分析了RCA任务的特殊性，探讨AI在处理此类图结构推理时面临的主要障碍，并强调了缺乏高质量基准的重要影响。可能还提出构建新型基准的初步构想。

Result: 指出了当前AI在RCA任务上普遍遇到的困难，包括推理能力不足和现实数据稀缺。对比现有研究，总结存在的主要不足。

Conclusion: RCA作为电信AI关键应用领域，未来需创新推理方法和完善真实数据基准，才能推动AI在网络故障分析中的实际落地。

Abstract: Root Cause Analysis (RCA) in telecommunication networks is a critical task,
yet it presents a formidable challenge for Artificial Intelligence (AI) due to
its complex, graph-based reasoning requirements and the scarcity of realistic
benchmarks.

</details>


### [112] [Integrating an ISO30401-compliant Knowledge management system with existing business processes of an organization](https://arxiv.org/abs/2507.18197)
*Aline Belloni,Patrick Prieur*

Main category: cs.CL

TL;DR: 本文探讨了知识管理系统（KMS）如何与组织现有的业务管理流程（尤其是ISO9001）集成，提出可借助SECI模型和PDCA循环来实现ISO30401标准合规的KMS。


<details>
  <summary>Details</summary>
Motivation: 随着ISO30401知识管理标准的推出，越来越多组织面临一个实际问题：如何将知识管理活动与现有的运营流程、质量管理体系（如ISO9001）有机结合，以提升整体业务成效并实现合规。

Method: 作者基于实践经验，回顾了业务流程建模原则，并分析了一个合规的知识管理系统在综合管理体系中的嵌入方法。文章提出将SECI模型（社会化、外化、结合、内化）融合进PDCA（计划-执行-检查-行动）循环，作为实现知识管理与业务流程融合的操作路径。

Result: 分析展示了在ISO9001流程基础上，如何利用SECI模型与PDCA步骤结合，将知识开发、转化和传播活动嵌入组织各项运营流程，实现知识管理与其它管理体系的有机联动。

Conclusion: 构建符合ISO30401标准的KMS不是孤立活动，而应与组织现有管理系统深度结合。通过流程建模、SECI模型与PDCA循环相结合的方式，可以有效推动知识管理系统在组织中的落地实施和流程联动。

Abstract: Business process modeling is used by most organizations as an essential
framework for ensuring efficiency and effectiveness of the work and workflow
performed by its employees and for ensuring the alignment of such work with its
strategic goals. For organizations that are compliant or near-compliant with
ISO 9001, this approach involves the detailed mapping of processes,
sub-processes, activities, and tasks. ISO30401 is a Management System Standard,
introduced in 2018, establishing universal requirements for the set up of a
Knowledge Management System in an organization. As ``ISO30401 implementers'' we
regularly face the challenge of explaining our clients how the knowledge
development, transformation and conveyances activities depicted in ISO30401 do
integrate with existing operational processes. This article recaps process
modelling principles in the context of ISO9001 and explores, based on our
experience, how an ISO30401-compliant Knowledge Management System (KMS)
entwines with all other processes of an Integrated Management System and in
particular how it can be implemented by deploying the mechanisms of the SECI
model through the steps of PDCA cycles.

</details>


### [113] [Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token Probability Method for Poisoned Document Detection](https://arxiv.org/abs/2507.18202)
*San Kim,Jonghwi Kim,Yejin Jeon,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 本文提出了一种新的检测并过滤RAG知识库中恶意文档的方法，能高效拦截90%以上的投毒内容，保障大模型问答安全。


<details>
  <summary>Details</summary>
Motivation: RAG技术虽能增强大模型的知识能力，但由于依赖外部知识库，容易被攻击者注入恶意（poisoned）文档，从而引导模型生成有害或误导性的回答，因此需要有效的安全防护机制。

Method: 作者提出了Gradient-based Masked Token Probability（GMTP）方法：首先分析检索器相似度函数的梯度，找出影响最大的关键token；然后将这些token掩码，并通过Masked Language Model（MLM）计算其被掩码的概率。恶意注入的token通常概率异常低，GMTP据此检测并过滤被攻击的文档。

Result: 实验显示，GMTP能在不同数据集及攻击场景中，识别并剔除90%以上的投毒内容，同时保留有效文档，保持高检索与生成质量。

Conclusion: GMTP是一种有效、实用的RAG知识库安全方案，能大幅提升大模型面对投毒攻击时的稳健性，对实际应用具有显著意义。

Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
providing external knowledge for accurate and up-to-date responses. However,
this reliance on external sources exposes a security risk, attackers can inject
poisoned documents into the knowledge base to steer the generation process
toward harmful or misleading outputs. In this paper, we propose Gradient-based
Masked Token Probability (GMTP), a novel defense method to detect and filter
out adversarially crafted documents. Specifically, GMTP identifies high-impact
tokens by examining gradients of the retriever's similarity function. These key
tokens are then masked, and their probabilities are checked via a Masked
Language Model (MLM). Since injected tokens typically exhibit markedly low
masked-token probabilities, this enables GMTP to easily detect malicious
documents and achieve high-precision filtering. Experiments demonstrate that
GMTP is able to eliminate over 90% of poisoned content while retaining relevant
documents, thus maintaining robust retrieval and generation performance across
diverse datasets and adversarial settings.

</details>


### [114] [Exploring the Impact of Instruction-Tuning on LLM's Susceptibility to Misinformation](https://arxiv.org/abs/2507.18203)
*Kyubeen Han,Junseo Jang,Hongjin Kim,Geunyeong Jeong,Harksoo Kim*

Main category: cs.CL

TL;DR: 论文研究了指令微调如何影响大模型对用户提供虚假信息的接受度，发现经过指令微调的模型更容易无条件接受用户输入的谣言。


<details>
  <summary>Details</summary>
Motivation: 虽然指令微调提高了大语言模型执行指令的效果，但也可能导致模型对用户输入的高度依赖，从而无过滤地接受并生成虚假信息。然而，目前针对指令微调与虚假信息易感性的直接关系研究较少。

Method: 作者对比分析了经过指令微调和未经过指令微调的大型语言模型，测试它们面对由用户提供的虚假信息时的反应。同时，探讨了如用户在提示中的角色、虚假信息长度、系统提示中警告的有无等相关影响因素。

Result: 结果表明，经过指令微调的模型在面对用户输入的虚假信息时，更倾向于直接接受和生成误导性内容。同时，指令微调增加了模型对用户输入的依赖，使得原本应承担过滤角色的助手模型变得更容易受到用户输入影响。

Conclusion: 作者指出需要系统性的方法来减轻指令微调带来的意外后果，提升真实世界中大模型的可靠性，尤其是在面对不实指令时。

Abstract: Instruction-tuning enhances the ability of large language models (LLMs) to
follow user instructions more accurately, improving usability while reducing
harmful outputs. However, this process may increase the model's dependence on
user input, potentially leading to the unfiltered acceptance of misinformation
and the generation of hallucinations. Existing studies primarily highlight that
LLMs are receptive to external information that contradict their parametric
knowledge, but little research has been conducted on the direct impact of
instruction-tuning on this phenomenon. In our study, we investigate the impact
of instruction-tuning on LLM's susceptibility to misinformation. Our analysis
reveals that instruction-tuned LLMs are significantly more likely to accept
misinformation when it is presented by the user. A comparison with base models
shows that instruction-tuning increases reliance on user-provided information,
shifting susceptibility from the assistant role to the user role. Furthermore,
we explore additional factors influencing misinformation susceptibility, such
as the role of the user in prompt structure, misinformation length, and the
presence of warnings in the system prompt. Our findings underscore the need for
systematic approaches to mitigate unintended consequences of instruction-tuning
and enhance the reliability of LLMs in real-world applications.

</details>


### [115] [Prune&Comp: Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation](https://arxiv.org/abs/2507.18212)
*Xinrui Chen,Hongxing Zhang,Fanyi Zeng,Yongxian Wei,Yizhi Wang,Xitong Ling,Guanghao Li,Chun Yuan*

Main category: cs.CL

TL;DR: 本文提出了一种名为Prune&Comp的层剪枝方法，通过对剪枝带来的隐状态幅度差距进行补偿，从而提高大语言模型（LLMs）剪枝后的性能表现。该方法无需训练，且没有运行时开销。


<details>
  <summary>Details</summary>
Motivation: 层剪枝可以有效压缩大语言模型，并按比例加速推理。然而，直接移除层会导致隐状态幅度出现较大差距，从而显著降低模型性能。动机是解决层剪枝导致的性能劣化问题。

Method: 提出Prune&Comp算法。首先，离线预估每移除一层后带来的隐状态幅度差距，然后通过缩放（rescale）剩余权重来补偿该差距，实现无训练、无推理时额外开销的网络剪枝。还提出将其用于迭代式剪枝，在每次剪枝后都进行补偿调整。

Result: 在以LLaMA-3-8B为例进行实验时，剪掉5层后，采用Prune&Comp方案能使困惑度下降近一半，问答性能保留了93.19%，比基线提升了4.01%。各类层剪枝评价指标均有提升。

Conclusion: Prune&Comp能显著缓解来自层剪枝的性能下降，效果优于现有方法，且实现简便、无需训练和运行时开销，为大模型压缩提供了一条高效可行的途径。

Abstract: Layer pruning has emerged as a promising technique for compressing large
language models (LLMs) while achieving acceleration proportional to the pruning
ratio. In this work, we identify that removing any layer induces a significant
magnitude gap in hidden states, resulting in substantial performance
degradation. To address this issue, we propose Prune&Comp, a novel
plug-and-play layer pruning scheme that leverages magnitude compensation to
mitigate such gaps in a training-free manner. Specifically, we first estimate
the magnitude gap caused by layer removal and then eliminate this gap by
rescaling the remaining weights offline, with zero runtime overhead incurred.
We further demonstrate the advantages of Prune&Comp through an iterative
pruning strategy. When integrated with an iterative prune-and-compensate loop,
Prune&Comp consistently enhances existing layer pruning metrics. For instance,
when 5 layers of LLaMA-3-8B are pruned using the prevalent block influence
metric, Prune&Comp nearly halves the perplexity and retains 93.19\% of the
original model's question-answering performance, outperforming the baseline by
4.01%.

</details>


### [116] [Locate-and-Focus: Enhancing Terminology Translation in Speech Language Models](https://arxiv.org/abs/2507.18263)
*Suhang Wu,Jialong Tang,Chengyi Yang,Pei Zhang,Baosong Yang,Junhui Li,Junfeng Yao,Min Zhang,Jinsong Su*

Main category: cs.CL

TL;DR: 本文针对直接语音翻译（ST）中术语翻译难题，提出了一种新颖的方法，能够更有效地定位并利用术语相关知识，从而显著提升术语翻译的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管直接语音翻译技术获得了越来越多关注，但其中术语翻译仍存在很大挑战。现有方法利用翻译知识改善ST模型，但常常受无关噪声干扰，且翻译知识利用不充分。因此，亟需一种更精准有效的术语翻译方法。

Method: 作者提出Locate-and-Focus方法。首先，准确定位包含关键术语的语音片段，构建术语相关的翻译知识，以减少无关信息影响。然后，将术语翻译知识与原始语音和文本假设进行多模态关联，帮助ST模型更专注地利用术语知识进行翻译。

Result: 实验表明，该方法在多个数据集上能够有效定位术语，提升了术语翻译的成功率，并且保持了整体翻译性能的稳健性。

Conclusion: 该新方法能精准筛选和利用术语相关翻译知识，显著改善了语音翻译中术语的准确转译，并不会影响一般性的翻译任务效果。

Abstract: Direct speech translation (ST) has garnered increasing attention nowadays,
yet the accurate translation of terminology within utterances remains a great
challenge. In this regard, current studies mainly concentrate on leveraging
various translation knowledge into ST models. However, these methods often
struggle with interference from irrelevant noise and can not fully utilize the
translation knowledge. To address these issues, in this paper, we propose a
novel Locate-and-Focus method for terminology translation. It first effectively
locates the speech clips containing terminologies within the utterance to
construct translation knowledge, minimizing irrelevant information for the ST
model. Subsequently, it associates the translation knowledge with the utterance
and hypothesis from both audio and textual modalities, allowing the ST model to
better focus on translation knowledge during translation. Experimental results
across various datasets demonstrate that our method effectively locates
terminologies within utterances and enhances the success rate of terminology
translation, while maintaining robust general translation performance.

</details>


### [117] [Zero-shot OCR Accuracy of Low-Resourced Languages: A Comparative Analysis on Sinhala and Tamil](https://arxiv.org/abs/2507.18264)
*Nevidu Jayatilleke,Nisansa de Silva*

Main category: cs.CL

TL;DR: 该论文比较了六种不同OCR引擎在低资源语言（LRL）僧伽罗语和泰米尔语上的零样本性能，发现不同引擎在不同语言上表现最佳，并引入了一个新的合成泰米尔语OCR基准数据集。


<details>
  <summary>Details</summary>
Motivation: 尽管高资源语言（如英语）的OCR技术已非常成熟，但许多使用独特书写系统的低资源语言的OCR问题尚未得到解决。本文关注于评估现有主流OCR引擎在低资源语言上的适用性和性能。

Method: 作者选择了六款包括商用和开源的OCR引擎（Cloud Vision API、Surya、Document AI、Tesseract、Subasa OCR和EasyOCR），針对僧伽罗语与泰米尔语进行零样本测试。采用五种评测手段，从字符和单词两个层面系统评估其准确性。部分OCR工具因本身限制，仅支持一种语言。

Result: 实验证明，Surya在僧伽罗语所有指标上表现最佳（WER仅2.61%），而Document AI在泰米尔语上的各项指标均居首（CER仅0.78%）。

Conclusion: 不同的OCR引擎在特定低资源语言上的表现存在差异，未有一款引擎在所有语种上稳居第一。研究还贡献了新的泰米尔语合成OCR基准数据集，为后续研究奠定数据基础。

Abstract: Solving the problem of Optical Character Recognition (OCR) on printed text
for Latin and its derivative scripts can now be considered settled due to the
volumes of research done on English and other High-Resourced Languages (HRL).
However, for Low-Resourced Languages (LRL) that use unique scripts, it remains
an open problem. This study presents a comparative analysis of the zero-shot
performance of six distinct OCR engines on two LRLs: Sinhala and Tamil. The
selected engines include both commercial and open-source systems, aiming to
evaluate the strengths of each category. The Cloud Vision API, Surya, Document
AI, and Tesseract were evaluated for both Sinhala and Tamil, while Subasa OCR
and EasyOCR were examined for only one language due to their limitations. The
performance of these systems was rigorously analysed using five measurement
techniques to assess accuracy at both the character and word levels. According
to the findings, Surya delivered the best performance for Sinhala across all
metrics, with a WER of 2.61%. Conversely, Document AI excelled across all
metrics for Tamil, highlighted by a very low CER of 0.78%. In addition to the
above analysis, we also introduce a novel synthetic Tamil OCR benchmarking
dataset.

</details>


### [118] [StyleAdaptedLM: Enhancing Instruction Following Models with Efficient Stylistic Transfer](https://arxiv.org/abs/2507.18294)
*Pritika Ramu,Apoorv Saxena,Meghanath M Y,Varsha Sankar,Debraj Basu*

Main category: cs.CL

TL;DR: 本文提出StyleAdaptedLM，一种通过LoRA方法将特定风格特征有效迁移到大语言模型（LLM）中的新框架，实现了无需成对数据的高效风格定制，同时保持了指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在企业对外传播、品牌沟通等场景下，需适应个性化风格（如品牌声音或作者语气），但缺乏配对指令-响应格式的数据使得实现高质量风格迁移变得困难。

Method: 采用LoRA低秩适配方法，先用无结构风格语料对基座模型进行训练，得到包含风格特征的LoRA适配器，再与已具备指令遵循能力的模型融合，实现风格和指令能力兼备。

Result: 实验证明，StyleAdaptedLM在多个数据集和模型上均提升了风格一致性，同时保持原有的指令执行能力。通过人工评测，模型能很好地吸收并体现品牌特定风格。

Conclusion: StyleAdaptedLM为大模型风格个性化提供了高效、实用的解决方案，无需成对数据、不牺牲模型性能，适合实际企业应用。

Abstract: Adapting LLMs to specific stylistic characteristics, like brand voice or
authorial tones, is crucial for enterprise communication but challenging to
achieve from corpora which lacks instruction-response formatting without
compromising instruction adherence. We introduce StyleAdaptedLM, a framework
that efficiently transfers stylistic traits to instruction-following models
using Low-Rank Adaptation (LoRA). LoRA adapters are first trained on a base
model with diverse unstructured stylistic corpora, then merged with a separate
instruction-following model. This enables robust stylistic customization
without paired data or sacrificing task performance. Experiments across
multiple datasets and models demonstrate improved stylistic consistency while
preserving instruction adherence, with human evaluations confirming
brand-specific convention uptake. StyleAdaptedLM offers an efficient path for
stylistic personalization in LLMs.

</details>


### [119] [BadReasoner: Planting Tunable Overthinking Backdoors into Large Reasoning Models for Fun or Profit](https://arxiv.org/abs/2507.18305)
*Biao Yi,Zekun Fei,Jianing Geng,Tong Li,Lihai Nie,Zheli Liu,Yiming Li*

Main category: cs.CL

TL;DR: 本文提出了一种针对大型推理模型（LRM）的新型攻击方式——“过度思考后门”，该攻击可精确控制模型推理过程的冗长程度，诱使模型在保证答案正确性的前提下生成极为冗长的推理链，造成计算资源浪费。作者通过创新的数据投毒方法实现该攻击，并验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRM）以其强大的推理能力广泛应用于解决复杂任务，但随之而来的安全隐患尚未被充分探索。现有的后门攻击多为简单的开关式触发，缺乏对模型输出复杂性的精细控制。因此，研究者动机在于揭示和验证一种更隐蔽、可调控、以资源消耗为目标的新攻击面。

Method: 作者提出了一种新颖的数据投毒技术：为训练数据注入特定的可调节触发器（通过触发器重复次数决定推理的冗长程度），并借助教师LLM自动生成包含冗余推理步骤的链式思考（CoT）答案，确保最终输出的正确性，同时悄无声息地显著增加推理过程的长度。

Result: 实验结果显示，该方法能在多种类型的LRM上稳健地实现对推理长度的精细、可控提升（多倍增长），且不影响最终答案的正确性。

Conclusion: 本文证明了“过度思考后门”作为LRM新的安全风险，攻击可显著增加模型计算资源消耗且难以被察觉。未来需关注LRM的资源消耗型后门风险及防御措施。

Abstract: Large reasoning models (LRMs) have emerged as a significant advancement in
artificial intelligence, representing a specialized class of large language
models (LLMs) designed to tackle complex reasoning tasks. The defining
characteristic of LRMs lies in their extensive chain-of-thought (CoT) reasoning
capabilities. In this paper, we identify a previously unexplored attack vector
against LRMs, which we term "overthinking backdoors". We advance this concept
by proposing a novel tunable backdoor, which moves beyond simple on/off attacks
to one where an attacker can precisely control the extent of the model's
reasoning verbosity. Our attack is implemented through a novel data poisoning
methodology. It pairs a tunable trigger-where the number of repetitions signals
the desired intensity-with a correspondingly verbose CoT response. These
responses are programmatically generated by instructing a teacher LLM to inject
a controlled number of redundant refinement steps into a correct reasoning
process. The approach preserves output correctness, which ensures stealth and
establishes the attack as a pure resource-consumption vector. Extensive
empirical results on various LRMs demonstrate that our method can reliably
trigger a controllable, multi-fold increase in the length of the reasoning
process, without degrading the final answer's correctness. Our source code is
available at https://github.com/FZaKK/BadReasoner.

</details>


### [120] [Uncertainty Quantification for Evaluating Machine Translation Bias](https://arxiv.org/abs/2507.18338)
*Ieva Raminta Staliūnaitė,Julius Cheng,Andreas Vlachos*

Main category: cs.CL

TL;DR: 机器翻译模型在涉及性别词时，即使上下文不明确，仍倾向于根据刻板印象给出确定性翻译，而非保持合理的不确定性。高准确性的模型在面对性别歧义时未必表现出应有的不确定性，去偏方法对不同情形影响各异。


<details>
  <summary>Details</summary>
Motivation: 源语言性别未显式标注，但目标语言需要指定性别时，机器翻译模型常需推断正确性别。既有研究发现模型易受刻板印象影响，忽视实际上下文，导致性别偏差。作者关注模型在面对性别歧义时是否能够适度表达不确定性。

Method: 利用最新的语义不确定性度量方法，对机器翻译模型在性别清晰与歧义情况下的翻译表现进行比较，同时研究去偏技术对两种情形的独立效果。

Result: 即使模型在性别明确时翻译准确，在歧义情形下也未必表现出对应的不确定性。去偏处理对歧义和非歧义实例的表现有独立影响。

Conclusion: 机器翻译模型既要在性别清晰时给出自信预测，也要在性别歧义时合理保持不确定性。当前模型与期望存在差距，相关去偏方法需更细致区分不同情景。

Abstract: In machine translation (MT), when the source sentence includes a lexeme whose
gender is not overtly marked, but whose target-language equivalent requires
gender specification, the model must infer the appropriate gender from the
context and/or external knowledge. Studies have shown that MT models exhibit
biased behaviour, relying on stereotypes even when they clash with contextual
information. We posit that apart from confidently translating using the correct
gender when it is evident from the input, models should also maintain
uncertainty about the gender when it is ambiguous. Using recently proposed
metrics of semantic uncertainty, we find that models with high translation and
gender accuracy on unambiguous instances do not necessarily exhibit the
expected level of uncertainty in ambiguous ones. Similarly, debiasing has
independent effects on ambiguous and unambiguous translation instances.

</details>


### [121] [TDR: Task-Decoupled Retrieval with Fine-Grained LLM Feedback for In-Context Learning](https://arxiv.org/abs/2507.18340)
*Yifu Chen,Bingchen Huang,Zhiling Wang,Yuanchao Du,Junfeng Luo,Lei Shen,Zhineng chen*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架TDR，旨在提升大型语言模型在in-context learning（ICL）中的样本检索能力，解决以往方法在多任务环境下存在的样本分布区分困难和检索与LLM反馈连接不细致的问题。TDR能在多任务数据集中为目标任务检索专用示例，并利用LLM的细粒度反馈指导检索模块训练，在30项NLP任务上实现SOTA表现。


<details>
  <summary>Details</summary>
Motivation: ICL对于大模型性能有很大提升，但检索高质量in-context示例一直是难点，尤其是在多任务或跨任务数据分布下。过去的检索方法在区分不同任务样本与反馈作用上仍有限，需要创新方法更精准、高效地检索示例以提升ICL效果。

Method: 提出TDR框架，具体包括：1）对ICL示例进行任务级解耦，使检索模块在多任务数据中针对性地为当前任务检索示例；2）引入LLM细粒度反馈信号，对检索模块训练进行监督和优化，提升最终检索质量。该方法具有“即插即用”特性，可灵活结合多种大模型使用。

Result: 在30个NLP任务上进行全面实验，与现有方法相比，TDR在所有数据集上均获得更优表现，刷新了SOTA记录。

Conclusion: TDR有效提升了多任务环境下ICL示例检索的准确性和大模型下游任务性能，且易于集成扩展，为LLM的实际应用提供了实用工具。

Abstract: In-context learning (ICL) has become a classic approach for enabling LLMs to
handle various tasks based on a few input-output examples. The effectiveness of
ICL heavily relies on the quality of these examples, and previous works which
focused on enhancing example retrieval capabilities have achieved impressive
performances. However, two challenges remain in retrieving high-quality
examples: (1) Difficulty in distinguishing cross-task data distributions, (2)
Difficulty in making the fine-grained connection between retriever output and
feedback from LLMs. In this paper, we propose a novel framework called TDR. TDR
decouples the ICL examples from different tasks, which enables the retrieval
module to retrieve examples specific to the target task within a multi-task
dataset. Furthermore, TDR models fine-grained feedback from LLMs to supervise
and guide the training of the retrieval module, which helps to retrieve
high-quality examples. We conducted extensive experiments on a suite of 30 NLP
tasks, the results demonstrate that TDR consistently improved results across
all datasets and achieves state-of-the-art performance. Meanwhile, our approach
is a plug-and-play method, which can be easily combined with various LLMs to
improve example retrieval abilities for ICL. The code is available at
https://github.com/Nnn-s/TDR.

</details>


### [122] [Hybrid Annotation for Propaganda Detection: Integrating LLM Pre-Annotations with Human Intelligence](https://arxiv.org/abs/2507.18343)
*Ariana Sahitaj,Premtim Sahitaj,Veronika Solopova,Jiaao Li,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 提出结合人类专家与大语言模型（LLM）的新方法，提升社交媒体宣传检测的标注一致性与可扩展性，并提出细粒度、分层的宣传技术分类，通过LLM辅助标注显著提升了一致性与效率，最终通过知识蒸馏让小模型也能实现高质量自动标注。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的宣传检测具有任务复杂与高质量标注数据稀缺的挑战，现有方法在细粒度标注上一致性低，难以扩展，因此需要寻找新方法提升标注质量和规模。

Method: 1. 制定分层的细粒度宣传技术分类体系；2. 通过人工标注分析标注一致性问题；3. 构建LLM辅助的自动预标注流程，提取相关文本片段、生成解释、分配细致和全局标签；4. 二次人工验证提升一致性和标注效率；5. 利用LLM高质量标注数据进行小模型的知识蒸馏和微调。

Result: LLM辅助标注流程在一致性和效率上均显著优于单纯人工方法，知识蒸馏后的小模型亦可实现结构化、高质量批量标注。

Conclusion: 结合人工与LLM的标注流程不仅提升了宣传检测系统的可用性和可扩展性，还促进了透明和负责的媒体环境建设，为实现联合国可持续发展目标（SDG 16）提供技术支持。

Abstract: Propaganda detection on social media remains challenging due to task
complexity and limited high-quality labeled data. This paper introduces a novel
framework that combines human expertise with Large Language Model (LLM)
assistance to improve both annotation consistency and scalability. We propose a
hierarchical taxonomy that organizes 14 fine-grained propaganda techniques into
three broader categories, conduct a human annotation study on the HQP dataset
that reveals low inter-annotator agreement for fine-grained labels, and
implement an LLM-assisted pre-annotation pipeline that extracts propagandistic
spans, generates concise explanations, and assigns local labels as well as a
global label. A secondary human verification study shows significant
improvements in both agreement and time-efficiency. Building on this, we
fine-tune smaller language models (SLMs) to perform structured annotation.
Instead of fine-tuning on human annotations, we train on high-quality
LLM-generated data, allowing a large model to produce these annotations and a
smaller model to learn to generate them via knowledge distillation. Our work
contributes towards the development of scalable and robust propaganda detection
systems, supporting the idea of transparent and accountable media ecosystems in
line with SDG 16. The code is publicly available at our GitHub repository.

</details>


### [123] [CLEAR: Error Analysis via LLM-as-a-Judge Made Easy](https://arxiv.org/abs/2507.18392)
*Asaf Yehudai,Lilach Eden,Yotam Perlitz,Roy Bar-Haim,Michal Shmueli-Scheuer*

Main category: cs.CL

TL;DR: 本文提出了CLEAR工具，能够对大语言模型（LLMs）进行细致的错误分析，实现‘不仅知道哪个好，还知道为什么好/差’。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM评测多数只输出简单排名或得分，无法揭示模型表现背后的具体原因，这不利于改进与优化。

Method: CLEAR包括以下流程：1) 对每个样本生成文本反馈；2) 聚类并总结为系统级错误类型；3) 统计每类错误出现频率；4) 提供交互式仪表盘支持多维度分析和可视化。

Result: 在RAG（检索增强生成）和数学基准任务上的实验验证了CLEAR工具的有效性和实用性，同时通过用户案例展示了工具帮助分析模型问题的能力。

Conclusion: CLEAR为LLM评测带来了透明化和可操作的错误分析，有助于开发者定位并解决模型的具体问题，提升模型改进效率。

Abstract: The evaluation of Large Language Models (LLMs) increasingly relies on other
LLMs acting as judges. However, current evaluation paradigms typically yield a
single score or ranking, answering which model is better but not why. While
essential for benchmarking, these top-level scores obscure the specific,
actionable reasons behind a model's performance. To bridge this gap, we
introduce CLEAR, an interactive, open-source package for LLM-based error
analysis. CLEAR first generates per-instance textual feedback, then it creates
a set of system-level error issues, and quantifies the prevalence of each
identified issue. Our package also provides users with an interactive dashboard
that allows for a comprehensive error analysis through aggregate
visualizations, applies interactive filters to isolate specific issues or score
ranges, and drills down to the individual instances that exemplify a particular
behavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks,
and showcase its utility through a user case study.

</details>


### [124] [Factual Inconsistencies in Multilingual Wikipedia Tables](https://arxiv.org/abs/2507.18406)
*Silvia Cappa,Lingxiao Kong,Pille-Riin Peet,Fanfu Wei,Yuchen Zhou,Jan-Christoph Kalo*

Main category: cs.CL

TL;DR: 本文分析了多语言维基百科结构化内容（特别是表格数据）在不同语言版本间的不一致性，并提出了一套收集、比对和解析多语言表格数据的方法，为知识核查和可信AI构建提供了参考。


<details>
  <summary>Details</summary>
Motivation: 维基百科虽覆盖全球300多种语言，但各版本均独立编辑与更新，导致同一主题内容在不同语言间出现事实不一致，这影响了百科的中立性、可靠性，也影响了依赖其训练的AI系统。

Method: 研究者提出了一种系统性方法，针对多语言维基百科条目中的表格内容进行收集、对齐与分析，并定义了表格数据不一致的多种类型，然后利用定量和定性指标对多语言表格一致性进行深入评估。

Result: 通过样本数据集实验，文章展示了多语言表格数据的不一致现象及其类别，同时评估了这些不一致对知识交互和AI训练的潜在影响。

Conclusion: 本文工作有助于提升事实核查的效率，促进多语言知识的相互理解，并为开发更可靠的多语言AI系统提供了数据基础和设计建议。

Abstract: Wikipedia serves as a globally accessible knowledge source with content in
over 300 languages. Despite covering the same topics, the different versions of
Wikipedia are written and updated independently. This leads to factual
inconsistencies that can impact the neutrality and reliability of the
encyclopedia and AI systems, which often rely on Wikipedia as a main training
source. This study investigates cross-lingual inconsistencies in Wikipedia's
structured content, with a focus on tabular data. We developed a methodology to
collect, align, and analyze tables from Wikipedia multilingual articles,
defining categories of inconsistency. We apply various quantitative and
qualitative metrics to assess multilingual alignment using a sample dataset.
These insights have implications for factual verification, multilingual
knowledge interaction, and design for reliable AI systems leveraging Wikipedia
content.

</details>


### [125] [FinDPO: Financial Sentiment Analysis for Algorithmic Trading through Preference Optimization of LLMs](https://arxiv.org/abs/2507.18417)
*Giorgos Iacovides,Wuyang Zhou,Danilo Mandic*

Main category: cs.CL

TL;DR: 本文提出了一种面向金融领域的新型大语言模型FinDPO，通过人类偏好对齐后训练，实现了对金融文本情感的更准确分析。FinDPO在标准情感分类任务上超越已有的模型，并首次在真实背景下，基于情感分析推动投资组合实现高额年化收益和优秀风险收益比。


<details>
  <summary>Details</summary>
Motivation: 传统的监督微调大语言模型在金融情感分析中容易出现过拟合训练数据，对新事件和金融领域复杂表述的泛化能力有限。而金融市场需要模型能够应对未见过的新型信息和更细粒度的情感识别。

Method: 作者提出基于Direct Preference Optimization（DPO）人类偏好对齐的金融专用大语言模型框架FinDPO，通过后训练阶段让模型学习人类对文本的实际偏好反馈，并创新性地提出了‘logit-to-score’方法，将情感分析输出转化为可用于投资策略的连续分值。

Result: FinDPO在金融情感分析标准数据集上取得了平均11%的精度提升。通过将情感分值用于投资组合建模，年化收益达到67%，Sharpe比率为2.0，即使考虑5个基点的手续费，也明显超过现有方法。

Conclusion: FinDPO框架显著提升了金融情感分析和实际量化投资的有效性，为金融领域情感驱动投资策略提供了新的方法论参考，表明结合人类偏好和大模型在实际金融应用上具有巨大潜力。

Abstract: Opinions expressed in online finance-related textual data are having an
increasingly profound impact on trading decisions and market movements. This
trend highlights the vital role of sentiment analysis as a tool for quantifying
the nature and strength of such opinions. With the rapid development of
Generative AI (GenAI), supervised fine-tuned (SFT) large language models (LLMs)
have become the de facto standard for financial sentiment analysis. However,
the SFT paradigm can lead to memorization of the training data and often fails
to generalize to unseen samples. This is a critical limitation in financial
domains, where models must adapt to previously unobserved events and the
nuanced, domain-specific language of finance. To this end, we introduce FinDPO,
the first finance-specific LLM framework based on post-training human
preference alignment via Direct Preference Optimization (DPO). The proposed
FinDPO achieves state-of-the-art performance on standard sentiment
classification benchmarks, outperforming existing supervised fine-tuned models
by 11% on the average. Uniquely, the FinDPO framework enables the integration
of a fine-tuned causal LLM into realistic portfolio strategies through a novel
'logit-to-score' conversion, which transforms discrete sentiment predictions
into continuous, rankable sentiment scores (probabilities). In this way,
simulations demonstrate that FinDPO is the first sentiment-based approach to
maintain substantial positive returns of 67% annually and strong risk-adjusted
performance, as indicated by a Sharpe ratio of 2.0, even under realistic
transaction costs of 5 basis points (bps).

</details>


### [126] [AraTable: Benchmarking LLMs' Reasoning and Understanding of Arabic Tabular Data](https://arxiv.org/abs/2507.18442)
*Rana Alshaikh,Israa Alghanmi,Shelan Jeawak*

Main category: cs.CL

TL;DR: 本文提出了AraTable，这是一个专门用于评估大型语言模型（LLMs）在阿拉伯语表格数据理解和推理能力的基准数据集。AraTable包含多种任务，并采用人工与自动结合的方法生成高质量数据。初步分析发现LLMs在简单任务表现尚可，但在复杂推理上仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 尽管自然语言处理领域中的大型语言模型取得了突出进展，但用于阿拉伯语表格数据的基准和资源极为有限，阻碍相关技术发展。本文旨在填补这一领域的空白，推动阿拉伯语结构化数据的处理能力提升。

Method: 提出AraTable基准，涵盖直接问答、事实验证和复杂推理等表格任务。数据集采用混合流程，先由LLMs生成内容，再由人类专家筛选和校验。此外，构建了一个自动化评测框架，利用自我推理机制接近人工评判标准。

Result: 初步分析表明，LLMs在直接问答等简单表格任务上表现尚可，但在事实验证和复杂推理时面临显著挑战。自动化评测框架在准确性上几乎可媲美人工评审。

Conclusion: AraTable丰富了阿拉伯语结构化数据理解相关的资源，并为未来提升LLMs在复杂表格推理上的表现提供了基准和方法。其公开资源和评测框架将促进该领域模型的持续进步。

Abstract: The cognitive and reasoning abilities of large language models (LLMs) have
enabled remarkable progress in natural language processing. However, their
performance in interpreting structured data, especially in tabular formats,
remains limited. Although benchmarks for English tabular data are widely
available, Arabic is still underrepresented because of the limited availability
of public resources and its unique language features. To address this gap, we
present AraTable, a novel and comprehensive benchmark designed to evaluate the
reasoning and understanding capabilities of LLMs when applied to Arabic tabular
data. AraTable consists of various evaluation tasks, such as direct question
answering, fact verification, and complex reasoning, involving a wide range of
Arabic tabular sources. Our methodology follows a hybrid pipeline, where
initial content is generated by LLMs and subsequently filtered and verified by
human experts to ensure high dataset quality. Initial analyses using AraTable
show that, while LLMs perform adequately on simpler tabular tasks such as
direct question answering, they continue to face significant cognitive
challenges when tasks require deeper reasoning and fact verification. This
indicates that there are substantial opportunities for future work to improve
performance on complex tabular reasoning tasks. We also propose a fully
automated evaluation framework that uses a self-deliberation mechanism and
achieves performance nearly identical to that of human judges. This research
provides a valuable, publicly available resource and evaluation framework that
can help accelerate the development of foundational models for processing and
analysing Arabic structured data.

</details>


### [127] [Restoring Rhythm: Punctuation Restoration Using Transformer Models for Bangla, a Low-Resource Language](https://arxiv.org/abs/2507.18448)
*Md Obyedullahil Mamun,Md Adyelullahil Mamun,Arif Ahmad,Md. Imran Hossain Emu*

Main category: cs.CL

TL;DR: 本文提出利用XLM-RoBERTa-large模型对孟加拉语无标点文本进行自动标点恢复，并通过构建多样化语料和数据增强技术提升模型准确率，显著提升了ASR后处理效果。


<details>
  <summary>Details</summary>
Motivation: 自动语音识别（ASR）后文本缺少标点，降低了可读性，尤其对像孟加拉语这样资源稀缺的语言影响更大。因此亟需有效的自动标点还原技术，提升文本后处理质量。

Method: 作者采用Transformer架构的XLM-RoBERTa-large模型对无标点的孟加拉语文本进行训练，针对句点、逗号、问号、感叹号四种标点进行预测。为解决数据稀缺问题，作者构建了大型、多样化训练语料，并采用数据增强（增强因子为0.20%）提升模型泛化能力。

Result: 最佳模型在新闻测试集上取得97.1%准确率，在参考集和ASR集分别为91.2%和90.2%。实验显示模型对现实噪声数据的良好泛化能力。

Conclusion: 本文工作为孟加拉语标点恢复树立了强有力的基线，同时公开了数据集与代码，为后续低资源NLP研究提供支持。

Abstract: Punctuation restoration enhances the readability of text and is critical for
post-processing tasks in Automatic Speech Recognition (ASR), especially for
low-resource languages like Bangla. In this study, we explore the application
of transformer-based models, specifically XLM-RoBERTa-large, to automatically
restore punctuation in unpunctuated Bangla text. We focus on predicting four
punctuation marks: period, comma, question mark, and exclamation mark across
diverse text domains. To address the scarcity of annotated resources, we
constructed a large, varied training corpus and applied data augmentation
techniques. Our best-performing model, trained with an augmentation factor of
alpha = 0.20%, achieves an accuracy of 97.1% on the News test set, 91.2% on the
Reference set, and 90.2% on the ASR set.
  Results show strong generalization to reference and ASR transcripts,
demonstrating the model's effectiveness in real-world, noisy scenarios. This
work establishes a strong baseline for Bangla punctuation restoration and
contributes publicly available datasets and code to support future research in
low-resource NLP.

</details>


### [128] [Generation of Synthetic Clinical Text: A Systematic Review](https://arxiv.org/abs/2507.18451)
*Basel Alshaikhdeeb,Ahmed Abdelmonem Hemedan,Soumyabrata Ghosh,Irina Balaur,Venkata Satagopam*

Main category: cs.CL

TL;DR: 本文系统性回顾了医学自由文本的合成生成方法，分析了生成目的、技术手段及评估方法，梳理了该领域从2018年起的研究进展与挑战。


<details>
  <summary>Details</summary>
Motivation: 医学领域的自然语言处理（NLP）面临数据稀疏和隐私问题，生成合成医学文本成为应对这些挑战的有效方法。因此，需要对现有生成方法、应用目的及有效性进行全面梳理和分析。

Method: 作者通过检索多个数据库（如PubMed、IEEE、arXiv等），筛选出94篇与医学自由文本生成相关的文章，并围绕生成目的、技术方案（如transformer和GPT模型）、评估方法等方面进行量化分析。

Result: 分析发现，合成医学文本研究自2018年起受到广泛关注，主要应用于文本增强、辅助写作、语料库建设、隐私保护等，技术上以transformer及GPT为主。评估方法主要涉及相似性、隐私、结构和实用性，其中实用性评价最常采用。生成文本可有效作为下游任务的补充，弥补数据不足，但隐私泄露问题仍需更多人工评估和改进。

Conclusion: 合成医学文本在提升数据利用率、加速医学NLP应用落地等方面作用明显，但隐私保护依然是亟待解决的核心难题。未来应加强敏感信息检测与人类评估，以促进合成文本安全可靠地应用。

Abstract: Generating clinical synthetic text represents an effective solution for
common clinical NLP issues like sparsity and privacy. This paper aims to
conduct a systematic review on generating synthetic medical free-text by
formulating quantitative analysis to three research questions concerning (i)
the purpose of generation, (ii) the techniques, and (iii) the evaluation
methods. We searched PubMed, ScienceDirect, Web of Science, Scopus, IEEE,
Google Scholar, and arXiv databases for publications associated with generating
synthetic medical unstructured free-text. We have identified 94 relevant
articles out of 1,398 collected ones. A great deal of attention has been given
to the generation of synthetic medical text from 2018 onwards, where the main
purpose of such a generation is towards text augmentation, assistive writing,
corpus building, privacy-preserving, annotation, and usefulness. Transformer
architectures were the main predominant technique used to generate the text,
especially the GPTs. On the other hand, there were four main aspects of
evaluation, including similarity, privacy, structure, and utility, where
utility was the most frequent method used to assess the generated synthetic
medical text. Although the generated synthetic medical text demonstrated a
moderate possibility to act as real medical documents in different downstream
NLP tasks, it has proven to be a great asset as augmented, complementary to the
real documents, towards improving the accuracy and overcoming
sparsity/undersampling issues. Yet, privacy is still a major issue behind
generating synthetic medical text, where more human assessments are needed to
check for the existence of any sensitive information. Despite that, advances in
generating synthetic medical text will considerably accelerate the adoption of
workflows and pipeline development, discarding the time-consuming legalities of
data transfer.

</details>


### [129] [Not All Features Deserve Attention: Graph-Guided Dependency Learning for Tabular Data Generation with Language Models](https://arxiv.org/abs/2507.18504)
*Zheyu Zhang,Shuo Yang,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 本文提出了一种新方法GraDe，将稀疏依赖图引入大语言模型（LLMs）注意力机制，提升表格数据生成效果，特别适用于复杂或依赖关系稀疏的数据。


<details>
  <summary>Details</summary>
Motivation: 传统LLMs在表格数据建模时，因自注意力机制平均关注所有特征对，导致关键特征依赖关系被稀释，尤其在结构复杂或语义不清的数据中表现不佳。

Method: 提出GraDe方法，结合外部提取的函数依赖关系，通过轻量级动态图学习模块，将稀疏依赖结构融入LLMs的注意力，增强关键特征之间的建模能力，抑制无关特征影响。

Result: 在多个真实世界数据集上，GraDe相比现有LLM方法在复杂数据集上提升达12%，于合成数据集实现与SOTA方法有竞争力的结果。

Conclusion: GraDe方法在不显著增加模型复杂度的基础上，有效提升了LLMs对结构性表格数据的建模能力，尤其适用于依赖关系复杂的场景。

Abstract: Large Language Models (LLMs) have shown strong potential for tabular data
generation by modeling textualized feature-value pairs. However, tabular data
inherently exhibits sparse feature-level dependencies, where many feature
interactions are structurally insignificant. This creates a fundamental
mismatch as LLMs' self-attention mechanism inevitably distributes focus across
all pairs, diluting attention on critical relationships, particularly in
datasets with complex dependencies or semantically ambiguous features. To
address this limitation, we propose GraDe (Graph-Guided Dependency Learning), a
novel method that explicitly integrates sparse dependency graphs into LLMs'
attention mechanism. GraDe employs a lightweight dynamic graph learning module
guided by externally extracted functional dependencies, prioritizing key
feature interactions while suppressing irrelevant ones. Our experiments across
diverse real-world datasets demonstrate that GraDe outperforms existing
LLM-based approaches by up to 12% on complex datasets while achieving
competitive results with state-of-the-art approaches in synthetic data quality.
Our method is minimally intrusive yet effective, offering a practical solution
for structure-aware tabular data modeling with LLMs.

</details>


### [130] [The Moral Gap of Large Language Models](https://arxiv.org/abs/2507.18523)
*Maciej Skorski,Alina Landowska*

Main category: cs.CL

TL;DR: 本文比较了大语言模型（LLMs）和专门微调的Transformer模型在道德基础检测任务上的表现，发现LLMs在该任务上表现不如通过微调的Transformers，尤其存在较高的漏检率。


<details>
  <summary>Details</summary>
Motivation: 道德基准检测对于分析社会话语和开发符合伦理的AI系统非常重要。然而，虽然大语言模型在广泛任务上表现突出，其在专业道德推理任务上的具体能力尚不清楚。

Method: 作者首次系统性地对比了最先进的LLM与经过特定任务微调的Transformer模型，在Twitter和Reddit数据集上，使用ROC、PR和DET曲线进行性能分析。

Result: 结果显示，LLM存在较高的假阴性率，甚至通过优化提示（prompt engineering）后，对道德内容的检测仍然系统性不足；相比之下，微调模型表现更优。

Conclusion: 研究表明，在道德推理类应用中，针对特定任务进行模型微调依然比仅依赖泛化型LLM和提示工程更加有效。

Abstract: Moral foundation detection is crucial for analyzing social discourse and
developing ethically-aligned AI systems. While large language models excel
across diverse tasks, their performance on specialized moral reasoning remains
unclear.
  This study provides the first comprehensive comparison between
state-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit
datasets using ROC, PR, and DET curve analysis.
  Results reveal substantial performance gaps, with LLMs exhibiting high false
negative rates and systematic under-detection of moral content despite prompt
engineering efforts. These findings demonstrate that task-specific fine-tuning
remains superior to prompting for moral reasoning applications.

</details>


### [131] [Effective Multi-Task Learning for Biomedical Named Entity Recognition](https://arxiv.org/abs/2507.18542)
*João Ruano,Gonçalo M. Correia,Leonor Barreiros,Afonso Mendes*

Main category: cs.CL

TL;DR: 这篇论文提出了一种新型的医学命名实体识别方法SRU-NER，可以处理嵌套实体并通过多任务学习整合多个数据集，有效提升了跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 医学领域的命名实体识别面临术语复杂、不同数据集标注不一致等挑战。目前方法难以兼顾嵌套实体识别和多数据集整合。本文旨在解决上述问题，提高跨域适应能力。

Method: 提出了SRU-NER模型：使用Slot-based Recurrent Unit结构，以多任务学习方式整合多数据集。在损失函数中，针对某数据集不存在的实体类型，动态调整损失计算，避免错误惩罚，从而缓解数据集间标注差异。

Result: 大量实验（包括跨语料评测和人工评审）表明，SRU-NER在医学和通用领域的命名实体识别任务上表现出较强竞争力，并有效提升了跨领域泛化性能。

Conclusion: SRU-NER能够有效处理嵌套实体与标注不一致问题，在不同域内外均取得优异成绩，特别增强了模型的跨领域泛化能力。

Abstract: Biomedical Named Entity Recognition presents significant challenges due to
the complexity of biomedical terminology and inconsistencies in annotation
across datasets. This paper introduces SRU-NER (Slot-based Recurrent Unit NER),
a novel approach designed to handle nested named entities while integrating
multiple datasets through an effective multi-task learning strategy. SRU-NER
mitigates annotation gaps by dynamically adjusting loss computation to avoid
penalizing predictions of entity types absent in a given dataset. Through
extensive experiments, including a cross-corpus evaluation and human assessment
of the model's predictions, SRU-NER achieves competitive performance in
biomedical and general-domain NER tasks, while improving cross-domain
generalization.

</details>


### [132] [GLiNER2: An Efficient Multi-Task Information Extraction System with Schema-Driven Interface](https://arxiv.org/abs/2507.18546)
*Urchade Zaratiana,Gil Pasternak,Oliver Boyd,George Hurn-Maloney,Ash Lewis*

Main category: cs.CL

TL;DR: GLiNER2提出了一个统一的信息抽取框架，支持多种NLP任务且高效易用，并且已开源。


<details>
  <summary>Details</summary>
Motivation: 当前信息抽取任务通常需要为不同任务设计不同的专用模型，或依赖于计算成本高的大型语言模型，限制了应用的普适性和部署效率。因此，作者希望提出一个高效统一的框架，简化不同任务的信息抽取，实现更低资源下的广泛应用。

Method: GLiNER2基于预训练Transformer编码器架构，扩展了原有的GLiNER，使其能同时支持命名实体识别、文本分类和分层结构化数据抽取。通过引入基于schema的接口，实现多任务组合，并保持模型体积小、CPU推理效率高。

Result: 实验结果显示，GLiNER2在抽取和分类任务上表现竞争力，并且相比依赖LLM的方法，显著提升了易于部署性。

Conclusion: GLiNER2框架为信息抽取与分类任务提供了统一的、效率高且易于部署的解决方案，有望加速NLP应用的普及与落地。

Abstract: Information extraction (IE) is fundamental to numerous NLP applications, yet
existing solutions often require specialized models for different tasks or rely
on computationally expensive large language models. We present GLiNER2, a
unified framework that enhances the original GLiNER architecture to support
named entity recognition, text classification, and hierarchical structured data
extraction within a single efficient model. Built pretrained transformer
encoder architecture, GLiNER2 maintains CPU efficiency and compact size while
introducing multi-task composition through an intuitive schema-based interface.
Our experiments demonstrate competitive performance across extraction and
classification tasks with substantial improvements in deployment accessibility
compared to LLM-based alternatives. We release GLiNER2 as an open-source
pip-installable library with pre-trained models and documentation at
https://github.com/fastino-ai/GLiNER2.

</details>


### [133] [GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation](https://arxiv.org/abs/2507.18562)
*Jiafeng Xiong,Yuting Zhao*

Main category: cs.CL

TL;DR: 提出了一种新的MMT方法GIIFT，利用跨模态图注意力网络，实现了在没有图像的情况下也能提升翻译效果，超越了当前方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态机器翻译（MMT）方法难以有效融合视觉与语言信息，且推理时受限于训练的多模态域，缺乏泛化能力。

Method: 构建创新性的多模态场景图，一方面保留和整合模态特有的信息；另一方面提出了GIIFT框架，该方法分为两个阶段，采用跨模态图注意力网络适配器，在统一融合空间中学习多模态知识，并归纳泛化到无图像的翻译场景。

Result: 在Multi30K数据集英法、英德翻译任务上，GIIFT在没有图像参与推理的情况下表现优于当前所有方法，达到了新的最优水平；在WMT基准集上对比无图像基线显著提升。

Conclusion: GIIFT框架不仅在传统多模态任务中具备优势，还能成功将多模态知识泛化应用到纯文本翻译，突出其无图像情况下的迁移泛化能力。

Abstract: Multimodal Machine Translation (MMT) has demonstrated the significant help of
visual information in machine translation. However, existing MMT methods face
challenges in leveraging the modality gap by enforcing rigid visual-linguistic
alignment whilst being confined to inference within their trained multimodal
domains. In this work, we construct novel multimodal scene graphs to preserve
and integrate modality-specific information and introduce GIIFT, a two-stage
Graph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph
Attention Network adapter to learn multimodal knowledge in a unified fused
space and inductively generalize it to broader image-free translation domains.
Experimental results on the Multi30K dataset of English-to-French and
English-to-German tasks demonstrate that our GIIFT surpasses existing
approaches and achieves the state-of-the-art, even without images during
inference. Results on the WMT benchmark show significant improvements over the
image-free translation baselines, demonstrating the strength of GIIFT towards
inductive image-free inference.

</details>


### [134] [Hybrid Tokenization Strategy for DNA Language Model using Byte Pair Encoding and K-MER Methods](https://arxiv.org/abs/2507.18570)
*Ganesh Sapkota,Md Hasibur Rahman*

Main category: cs.CL

TL;DR: 该论文提出了一种结合6-mer与BPE（Byte Pair Encoding-600）的新型混合分词策略，用于提升DNA语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的k-mer分词法能有效捕捉DNA序列的局部结构信息，但存在词分布不均、无法充分理解全局上下文的不足。为解决这些问题，需要设计新的分词方法兼顾局部和全局特征。

Method: 作者提出将独特的6mer分词与经过600轮BPE生成的最优BPE分词合并，构建混合词表，用于DNA语言模型训练，以同时捕捉DNA序列的短程与长程规律。

Result: 采用混合分词策略训练的DNA基础模型在next-k-mer预测任务中表现优异，3-mer、4-mer、5-mer的预测准确率分别为10.78%、10.1%、4.12%，优于NT、DNABERT2和GROVER等现有模型。

Conclusion: 混合分词能够兼顾局部序列结构和全局上下文信息，为以后DNA序列分析和生物学研究中采用先进分词技术奠定了坚实基础。

Abstract: This paper presents a novel hybrid tokenization strategy that enhances the
performance of DNA Language Models (DLMs) by combining 6-mer tokenization with
Byte Pair Encoding (BPE-600). Traditional k-mer tokenization is effective at
capturing local DNA sequence structures but often faces challenges, including
uneven token distribution and a limited understanding of global sequence
context. To address these limitations, we propose merging unique 6mer tokens
with optimally selected BPE tokens generated through 600 BPE cycles. This
hybrid approach ensures a balanced and context-aware vocabulary, enabling the
model to capture both short and long patterns within DNA sequences
simultaneously. A foundational DLM trained on this hybrid vocabulary was
evaluated using next-k-mer prediction as a fine-tuning task, demonstrating
significantly improved performance. The model achieved prediction accuracies of
10.78% for 3-mers, 10.1% for 4-mers, and 4.12% for 5-mers, outperforming
state-of-the-art models such as NT, DNABERT2, and GROVER. These results
highlight the ability of the hybrid tokenization strategy to preserve both the
local sequence structure and global contextual information in DNA modeling.
This work underscores the importance of advanced tokenization methods in
genomic language modeling and lays a robust foundation for future applications
in downstream DNA sequence analysis and biological research.

</details>


### [135] [Wide-In, Narrow-Out: Revokable Decoding for Efficient and Effective DLLMs](https://arxiv.org/abs/2507.18578)
*Feng Hong,Geng Yu,Yushi Ye,Haicheng Huang,Huangjie Zheng,Ya Zhang,Yanfeng Wang,Jiangchao Yao*

Main category: cs.CL

TL;DR: 本文提出了一种新型的DLLM解码算法WINO，能够大幅提升生成质量与速度，缓解以往DLLM速度与质量难以兼顾的问题。


<details>
  <summary>Details</summary>
Motivation: 传统DLLMs存在质量-速度权衡：快速并行解码会显著降低性能，主要原因是不可逆的标准解码易造成错误累积和极端解码路径。作者希望打破这种瓶颈，提升DLLM并行解码既快又准。

Method: 提出WINO（Wide-In, Narrow-Out）算法，实现可撤销的并行解码。该方法无需重新训练，通过并行预测多token并利用模型双向上下文进行校验，将可疑token重新遮盖后细致修正，既快又能反复纠错。

Result: 在开源DLLM（如LLaDA、MMaDA）上验证WINO，显著改善质量-速度权衡：如GSM8K数学数据集上推理速度提升6倍，准确率提高2.58%；Flickr30K图像描述上速度提升10倍并提升性能。

Conclusion: WINO有效解决DLLM的质量-速度困境，实现解码速度和效果同步提升，为并行生成开拓了新思路，并通过多项实验验证了其全面优越性。

Abstract: Diffusion Large Language Models (DLLMs) have emerged as a compelling
alternative to Autoregressive models, designed for fast parallel generation.
However, existing DLLMs are plagued by a severe quality-speed trade-off, where
faster parallel decoding leads to significant performance degradation. We
attribute this to the irreversibility of standard decoding in DLLMs, which is
easily polarized into the wrong decoding direction along with early error
context accumulation. To resolve this, we introduce Wide-In, Narrow-Out (WINO),
a training-free decoding algorithm that enables revokable decoding in DLLMs.
WINO employs a parallel draft-and-verify mechanism, aggressively drafting
multiple tokens while simultaneously using the model's bidirectional context to
verify and re-mask suspicious ones for refinement. Verified in open-source
DLLMs like LLaDA and MMaDA, WINO is shown to decisively improve the
quality-speed trade-off. For instance, on the GSM8K math benchmark, it
accelerates inference by 6$\times$ while improving accuracy by 2.58%; on
Flickr30K captioning, it achieves a 10$\times$ speedup with higher performance.
More comprehensive experiments are conducted to demonstrate the superiority and
provide an in-depth understanding of WINO.

</details>


### [136] [System Report for CCL25-Eval Task 10: SRAG-MAV for Fine-Grained Chinese Hate Speech Recognition](https://arxiv.org/abs/2507.18580)
*Jiahao Wang,Ramen Liu,Longhui Zhang,Jing Li*

Main category: cs.CL

TL;DR: 该论文提出了一种用于细粒度中文仇恨言论识别的新框架SRAG-MAV，并在CCL25-Eval Task 10任务上取得了显著领先的结果。


<details>
  <summary>Details</summary>
Motivation: 随着网络上中文仇恨言论的不断增长，传统识别方法在处理细粒度任务时存在性能瓶颈。因此，作者希望提升模型对于复杂仇恨言论的理解和识别能力。

Method: 作者提出了SRAG-MAV框架，结合了任务重构（将四元组抽取转化为三元组抽取）、自检索增强生成（动态从训练集中检索上下文提示）以及多轮累积投票推理（通过多轮生成及投票提升输出稳定性和性能）。该系统基于Qwen2.5-7B大模型开发实现。

Result: 在STATE ToxiCN数据集上，SRAG-MAV系统Hard Score为26.66，Soft Score为48.35，平均得分37.505，明显优于GPT-4o（15.63）及微调后Qwen2.5-7B模型（35.365）。

Conclusion: 所提SRAG-MAV框架可显著提升细粒度中文仇恨言论识别性能，为相关领域研究和实际应用提供了有效方法。

Abstract: This paper presents our system for CCL25-Eval Task 10, addressing
Fine-Grained Chinese Hate Speech Recognition (FGCHSR). We propose a novel
SRAG-MAV framework that synergistically integrates task reformulation(TR),
Self-Retrieval-Augmented Generation (SRAG), and Multi-Round Accumulative Voting
(MAV). Our method reformulates the quadruplet extraction task into triplet
extraction, uses dynamic retrieval from the training set to create contextual
prompts, and applies multi-round inference with voting to improve output
stability and performance. Our system, based on the Qwen2.5-7B model, achieves
a Hard Score of 26.66, a Soft Score of 48.35, and an Average Score of 37.505 on
the STATE ToxiCN dataset, significantly outperforming baselines such as GPT-4o
(Average Score 15.63) and fine-tuned Qwen2.5-7B (Average Score 35.365). The
code is available at https://github.com/king-wang123/CCL25-SRAG-MAV.

</details>


### [137] [AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance Data Synthesis for Specialist LLMs](https://arxiv.org/abs/2507.18584)
*Xiaopeng Ke,Hexuan Deng,Xuebo Liu,Jun Rao,Zhenxi Song,Jun Yu,Min Zhang*

Main category: cs.CL

TL;DR: 提出AQuilt框架，能从无标注数据自动构建专业领域的指令微调数据，提升大模型在专用领域的表现，成本仅为同类方案的17%。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在通用领域表现优异，但在专业领域表现不佳。传统专业领域数据构建方法要么开销大，要么泛化能力有限，难以高效生成高质量任务相关数据。

Method: AQuilt框架利用专业领域的无标注数据，通过引入包含问答、逻辑推理、自我检查等机制，以及可定制任务指令的设计，生成高相关性、高质量的专业领域微调数据，并最终构建含70多万样本的数据集以训练数据合成模型。

Result: AQuilt模型在实验中与先进的DeepSeek-V3效果相当，但生产成本仅为其17%。分析显示AQuilt生成的数据与下游专业任务更相关。

Conclusion: AQuilt可大幅降低专业领域LLM数据构建成本，同时提升任务相关性和泛化能力，为专用领域大模型微调提供了高效方案。

Abstract: Despite the impressive performance of large language models (LLMs) in general
domains, they often underperform in specialized domains. Existing approaches
typically rely on data synthesis methods and yield promising results by using
unlabeled data to capture domain-specific features. However, these methods
either incur high computational costs or suffer from performance limitations,
while also demonstrating insufficient generalization across different tasks. To
address these challenges, we propose AQuilt, a framework for constructing
instruction-tuning data for any specialized domains from corresponding
unlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic,
and Task type. By incorporating logic and inspection, we encourage reasoning
processes and self-inspection to enhance model performance. Moreover,
customizable task instructions enable high-quality data generation for any
task. As a result, we construct a dataset of 703k examples to train a powerful
data synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3
while utilizing just 17% of the production cost. Further analysis demonstrates
that our generated data exhibits higher relevance to downstream tasks. Source
code, models, and scripts are available at https://github.com/Krueske/AQuilt.

</details>


### [138] [TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual Rewards](https://arxiv.org/abs/2507.18618)
*Andreea Nica,Ivan Zakazov,Nicolas Mario Baldwin,Saibo Geng,Robert West*

Main category: cs.CL

TL;DR: 本文提出了一种新的提示优化方法，通过将文本反馈直接融入提示模型训练，显著提升大语言模型在复杂数学任务上的推理能力，无需更新模型参数或提前收集数据，达到了最新的效果。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法要么依靠文本反馈、要么用数值奖励训练专用提示模型，两者各有缺陷，缺乏能够充分利用文本反馈并动态优化提示的统一框架。

Method: 提出了Textual Reward Prompt（TRPrompt）框架，将文本反馈作为奖励信号，直接用于训练提示模型，无需提前收集数据，通过迭代优化提升提示质量。

Result: 该框架在无需更新目标大模型参数的情况下，在GSMHard和MATH等高难度数学数据集上，生成的针对性提示达到了最优的推理表现。

Conclusion: TRPrompt方法证明了文本奖励信号的高分辨率优势，在提升大语言模型推理能力方面相较以往方法更优且无需复杂的数据准备，对提示优化领域具有重要意义。

Abstract: Prompt optimization improves the reasoning abilities of large language models
(LLMs) without requiring parameter updates to the target model. Following
heuristic-based "Think step by step" approaches, the field has evolved in two
main directions: while one group of methods uses textual feedback to elicit
improved prompts from general-purpose LLMs in a training-free way, a concurrent
line of research relies on numerical rewards to train a special prompt model,
tailored for providing optimal prompts to the target model. In this paper, we
introduce the Textual Reward Prompt framework (TRPrompt), which unifies these
approaches by directly incorporating textual feedback into training of the
prompt model. Our framework does not require prior dataset collection and is
being iteratively improved with the feedback on the generated prompts. When
coupled with the capacity of an LLM to internalize the notion of what a "good"
prompt is, the high-resolution signal provided by the textual rewards allows us
to train a prompt model yielding state-of-the-art query-specific prompts for
the problems from the challenging math datasets GSMHard and MATH.

</details>


### [139] [Checklists Are Better Than Reward Models For Aligning Language Models](https://arxiv.org/abs/2507.18624)
*Vijay Viswanathan,Yanchao Sun,Shuang Ma,Xiang Kong,Meng Cao,Graham Neubig,Tongshuang Wu*

Main category: cs.CL

TL;DR: 本文提出了一种名为“基于清单反馈的强化学习（RLCF）”的新方法，用以提升大语言模型对用户指令的理解与响应能力，并通过检查项细化模型训练标准，在多个基准任务中均有提升。


<details>
  <summary>Details</summary>
Motivation: 目前主流大语言模型通常通过针对“有帮助性”“有害性”等固定标准进行强化学习，但现实用户需求复杂多样，固定标准显然不足，缺乏灵活性，因此亟需更能适应多样需求的评估和训练方法。

Method: 作者提出RLCF方法：首先从用户指令中自动提取清单式细分检查项，然后用AI判决器及专门校验程序分别评估每项完成度，对模型输出基于此计算奖励值以进行强化学习训练。最后，将RLCF与现有主流对齐方法在Qwen2.5-7B-Instruct模型和五项公开指令基准上进行对比测试。

Result: 实验结果显示，RLCF是唯一在全部五项基准任务上均有性能提升的方法。例如，在FollowBench上的硬满意度提升4个百分点，InFoBench上提升6个百分点，Arena-Hard上胜率提升3个百分点。

Conclusion: 结果证明，基于清单化反馈的训练方法能够显著提升大语言模型对多元复杂指令的理解和执行效果，是推动模型能力扩展和贴合用户需求的有效手段。

Abstract: Language models must be adapted to understand and follow user instructions.
Reinforcement learning is widely used to facilitate this -- typically using
fixed criteria such as "helpfulness" and "harmfulness". In our work, we instead
propose using flexible, instruction-specific criteria as a means of broadening
the impact that reinforcement learning can have in eliciting instruction
following. We propose "Reinforcement Learning from Checklist Feedback" (RLCF).
From instructions, we extract checklists and evaluate how well responses
satisfy each item - using both AI judges and specialized verifier programs -
then combine these scores to compute rewards for RL. We compare RLCF with other
alignment methods applied to a strong instruction following model
(Qwen2.5-7B-Instruct) on five widely-studied benchmarks -- RLCF is the only
method to improve performance on every benchmark, including a 4-point boost in
hard satisfaction rate on FollowBench, a 6-point increase on InFoBench, and a
3-point rise in win rate on Arena-Hard. These results establish checklist
feedback as a key tool for improving language models' support of queries that
express a multitude of needs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [140] [PinchBot: Long-Horizon Deformable Manipulation with Guided Diffusion Policy](https://arxiv.org/abs/2507.17846)
*Alison Bartsch,Arvind Car,Amir Barati Farimani*

Main category: cs.RO

TL;DR: 本文提出了一种机器人系统PinchBot，利用基于扩散的策略模型和3D点云等技术，仅通过捏合动作就能制作简单陶器形状，展示了其在复杂可变形物体操作中的有效性。


<details>
  <summary>Details</summary>
Motivation: 陶艺创作对动作的灵巧性和精确性要求极高，如何让机器人仅用简单的“捏”动作完成陶器制作，是多模态、长时序可变形物体操作中的一大挑战。

Method: 提出了PinchBot系统，核心为基于目标条件的扩散策略模型，结合预训练3D点云嵌入、任务进度预测及避免碰撞的动作投影，实现复杂陶器形态的生成。

Result: PinchBot能够成功利用捏合作用制造出多种简单陶器目标，实验视频与演示数据集均已公开。

Conclusion: 该方法验证了基于扩散模型和多模态感知的机器人在陶器类可变形物体操控上的潜力与实用性。

Abstract: Pottery creation is a complicated art form that requires dexterous, precise
and delicate actions to slowly morph a block of clay to a meaningful, and often
useful 3D goal shape. In this work, we aim to create a robotic system that can
create simple pottery goals with only pinch-based actions. This pinch pottery
task allows us to explore the challenges of a highly multi-modal and
long-horizon deformable manipulation task. To this end, we present PinchBot, a
goal-conditioned diffusion policy model that when combined with pre-trained 3D
point cloud embeddings, task progress prediction and collision-constrained
action projection, is able to successfully create a variety of simple pottery
goals. For experimental videos and access to the demonstration dataset, please
visit our project website:
https://sites.google.com/andrew.cmu.edu/pinchbot/home.

</details>


### [141] [A Step-by-step Guide on Nonlinear Model Predictive Control for Safe Mobile Robot Navigation](https://arxiv.org/abs/2507.17856)
*Dennis Benders,Laura Ferranti,Johannes Köhler*

Main category: cs.RO

TL;DR: 本文介绍了一种基于非线性模型预测控制（NMPC）的安全移动机器人导航方案，兼顾理论与实践实现，聚焦如何保障机器人在受扰动和噪声影响下安全穿越障碍物环境。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在动态、充满障碍的环境中实现安全导航存在很大挑战，MPC（特别是NMPC）作为理论上可行但在实际中实现复杂的方法，需要有实用性的落地路线指导研究人员和工程师实现从理论到应用的转化。

Method: 报告采用非线性模型预测控制（NMPC），强调保证机器人在状态、输入约束和避障等方面的安全。内容以理论基础为起点，逐步推导到数学证明，并提供实现建议，重视面向实际的问题和性能保障。

Result: 报告为读者提供了系统且易用的NMPC导航方案，从理论出发到实际实现流程，填补了已有文献中缺乏应用细节的不足。

Conclusion: 本文面向机器人领域的研究和工程人员，作为理论与现实之间的桥梁，为安全NMPC导航方法提供了实践操作路径。同时，文章欢迎反馈与更新，保持技术文档的时效性。

Abstract: Designing a Model Predictive Control (MPC) scheme that enables a mobile robot
to safely navigate through an obstacle-filled environment is a complicated yet
essential task in robotics. In this technical report, safety refers to ensuring
that the robot respects state and input constraints while avoiding collisions
with obstacles despite the presence of disturbances and measurement noise. This
report offers a step-by-step approach to implementing Nonlinear Model
Predictive Control (NMPC) schemes addressing these safety requirements.
Numerous books and survey papers provide comprehensive overviews of linear MPC
(LMPC) \cite{bemporad2007robust,kouvaritakis2016model}, NMPC
\cite{rawlings2017model,allgower2004nonlinear,mayne2014model,grune2017nonlinear,saltik2018outlook},
and their applications in various domains, including robotics
\cite{nascimento2018nonholonomic,nguyen2021model,shi2021advanced,wei2022mpc}.
This report does not aim to replicate those exhaustive reviews. Instead, it
focuses specifically on NMPC as a foundation for safe mobile robot navigation.
The goal is to provide a practical and accessible path from theoretical
concepts to mathematical proofs and implementation, emphasizing safety and
performance guarantees. It is intended for researchers, robotics engineers, and
practitioners seeking to bridge the gap between theoretical NMPC formulations
and real-world robotic applications.
  This report is not necessarily meant to remain fixed over time. If someone
finds an error in the presented theory, please reach out via the given email
addresses. We are happy to update the document if necessary.

</details>


### [142] [OpenNav: Open-World Navigation with Multimodal Large Language Models](https://arxiv.org/abs/2507.18033)
*Mingfeng Yuan,Letian Wang,Steven L. Waslander*

Main category: cs.RO

TL;DR: 本文提出利用多模态大语言模型（MLLMs）结合视觉语言感知，实现机器人对自然语言导航指令的理解和任务执行，提升机器人在室内外复杂环境下的自主导航能力。


<details>
  <summary>Details</summary>
Motivation: 虽然现有大语言模型在常识推理上表现优异，但它们在将自然语言描述转换为实际机器人操作、特别是在开放环境下执行复杂任务方面，仍存在难题，如无法仅通过调用有限动作原语应对开放世界的多样导航和规划。

Method: 作者利用MLLMs强大的跨模态理解和代码生成能力，将自由形式的自然语言指令与场景地图结合，通过生成组合式2D鸟瞰值图（value map），融合语义知识与空间信息，从而指导机器人路径规划。方法在大规模无人车数据集上进行零样本室外导航测试，并在Husky机器人上于室内外环境进行了实地验证。

Result: 实验结果显示，该方法能够执行多样化的自然语言导航任务，对物体检测错误及语言歧义具有较强鲁棒性，并且在真实机器人平台上表现出良好的适应性和可靠性。

Conclusion: MLLMs结合视觉感知和代码生成，有效拓展了机器人处理自然语言指令的能力，使机器人能够理解并可靠完成复杂、多样的导航任务，在实际室内外场景中表现出强健的适用性。

Abstract: Pre-trained large language models (LLMs) have demonstrated strong
common-sense reasoning abilities, making them promising for robotic navigation
and planning tasks. However, despite recent progress, bridging the gap between
language descriptions and actual robot actions in the open-world, beyond merely
invoking limited predefined motion primitives, remains an open challenge. In
this work, we aim to enable robots to interpret and decompose complex language
instructions, ultimately synthesizing a sequence of trajectory points to
complete diverse navigation tasks given open-set instructions and open-set
objects. We observe that multi-modal large language models (MLLMs) exhibit
strong cross-modal understanding when processing free-form language
instructions, demonstrating robust scene comprehension. More importantly,
leveraging their code-generation capability, MLLMs can interact with
vision-language perception models to generate compositional 2D bird-eye-view
value maps, effectively integrating semantic knowledge from MLLMs with spatial
information from maps to reinforce the robot's spatial understanding. To
further validate our approach, we effectively leverage large-scale autonomous
vehicle datasets (AVDs) to validate our proposed zero-shot vision-language
navigation framework in outdoor navigation tasks, demonstrating its capability
to execute a diverse range of free-form natural language navigation
instructions while maintaining robustness against object detection errors and
linguistic ambiguities. Furthermore, we validate our system on a Husky robot in
both indoor and outdoor scenes, demonstrating its real-world robustness and
applicability. Supplementary videos are available at
https://trailab.github.io/OpenNav-website/

</details>


### [143] [Modular Robot and Landmark Localisation Using Relative Bearing Measurements](https://arxiv.org/abs/2507.18070)
*Behzad Zamani,Jochen Trumpf,Chris Manzie*

Main category: cs.RO

TL;DR: 提出了一种模块化的非线性最小二乘滤波方法，能独立估计各子系统状态，同时可处理子系统间有关联的观测，并通过协方差交汇算法防止信息重复。该方法在机器人-地标定位问题中与联合状态滤波方法进行了对比分析。


<details>
  <summary>Details</summary>
Motivation: 当系统由多个独立子系统组成且子系统间存在信息交流时，传统联合滤波既增加了计算复杂度，也容易重复计算信息。因此需要一种既能保证估计准确性，又能模块化、低耦合的滤波方法。

Method: 方法基于非线性最小二乘滤波思想，将每个子系统独立更新状态和误差协方差，遇到涉及多个子系统的观测时，通过CI（Covariance Intersection）算法合成各子系统的估计结果。作者还提出了CI的新推导方法，使其能自然集成到最小二乘框架下。在机器人-地标定位实例中实现与对比。

Result: 通过随机仿真实验，表明所提模块化滤波方法在准确性和通信/带宽等资源消耗上与传统联合状态滤波方法各有优劣，且其简化版能在降低带宽下保持合理性能。

Conclusion: 提出的模块化滤波方法可有效解决多子系统情况下的状态估计问题，在保证一定性能的情况下有更好的灵活性和可扩展性，适用于通信受限或分布式系统。

Abstract: In this paper we propose a modular nonlinear least squares filtering approach
for systems composed of independent subsystems. The state and error covariance
estimate of each subsystem is updated independently, even when a relative
measurement simultaneously depends on the states of multiple subsystems. We
integrate the Covariance Intersection (CI) algorithm as part of our solution in
order to prevent double counting of information when subsystems share estimates
with each other. An alternative derivation of the CI algorithm based on least
squares estimation makes this integration possible. We particularise the
proposed approach to the robot-landmark localization problem. In this problem,
noisy measurements of the bearing angle to a stationary landmark position
measured relative to the SE(2) pose of a moving robot couple the estimation
problems for the robot pose and the landmark position. In a randomized
simulation study, we benchmark the proposed modular method against a monolithic
joint state filter to elucidate their respective trade-offs. In this study we
also include variants of the proposed method that achieve a graceful
degradation of performance with reduced communication and bandwidth
requirements.

</details>


### [144] [A Modular Residual Learning Framework to Enhance Model-Based Approach for Robust Locomotion](https://arxiv.org/abs/2507.18138)
*Min-Gyu Kim,Dongyun Kang,Hajun Kim,Hae-Won Park*

Main category: cs.RO

TL;DR: 本论文提出了一种结合基于模型和基于学习方法的行走控制框架，在高不确定性环境下实现了更强的鲁棒性和更高的学习效率，并在真实四足机器人上验证了效果。


<details>
  <summary>Details</summary>
Motivation: 传统基于模型的方法对模型不匹配和不确定性较敏感，而纯粹基于学习的方法则学习效率较低且泛化性有限。因此需要结合两者优势，提升机器人运动控制表现。

Method: 将残差模块分别和基于模型的框架（足步规划器和动力学模型）集成，通过针对不同部分选择合适的基于学习的方法以补偿模型不精确带来的性能下降。并以模块化结构实现灵活组合。

Result: 该方法在高不确定性环境下展现出优于基线方法的控制性能和学习效率，同时对参数调整更鲁棒。在真实四足机器人结合模型预测控制进行实验，机器人能应对超出仿真的不确定性，保持平衡并跟踪指令速度。

Conclusion: 该框架有效促进了机器人在不确定环境中的鲁棒运动控制，提升了控制性能和实践中的适用性，对参数敏感性也有所降低。

Abstract: This paper presents a novel approach that combines the advantages of both
model-based and learning-based frameworks to achieve robust locomotion. The
residual modules are integrated with each corresponding part of the model-based
framework, a footstep planner and dynamic model designed using heuristics, to
complement performance degradation caused by a model mismatch. By utilizing a
modular structure and selecting the appropriate learning-based method for each
residual module, our framework demonstrates improved control performance in
environments with high uncertainty, while also achieving higher learning
efficiency compared to baseline methods. Moreover, we observed that our
proposed methodology not only enhances control performance but also provides
additional benefits, such as making nominal controllers more robust to
parameter tuning. To investigate the feasibility of our framework, we
demonstrated residual modules combined with model predictive control in a real
quadrupedal robot. Despite uncertainties beyond the simulation, the robot
successfully maintains balance and tracks the commanded velocity.

</details>


### [145] [Autonomous UAV Navigation for Search and Rescue Missions Using Computer Vision and Convolutional Neural Networks](https://arxiv.org/abs/2507.18160)
*Luka Šiktar,Branimir Ćaran,Bojan Šekoranja,Marko Švaco*

Main category: cs.RO

TL;DR: 本论文提出了一套基于无人机与深度学习的系统，用于搜救任务中对人员的检测、识别和追踪，初步实验已在14名受试者上取得成功。


<details>
  <summary>Details</summary>
Motivation: 搜索与救援任务中，快速准确地发现、识别及追踪失踪人员至关重要。传统巡查方式效率低、危险性大，亟需智能化无人机系统提升搜救效率和安全性。

Method: 系统采用搭载ROS2框架的无人机，集成YOLOv11与YOLOv11-pose卷积神经网络进行人体检测与追踪，dlib的CNN进行人脸识别。通过IMU数据进行系统辨识，设计PD控制器，基于关键点实现人物追踪并保持安全距离。若首次出现人员，可人工标注后立即启动追踪。

Result: 在14名已知个体中，系统实现了实时人物检测、识别与追踪，验证了该系统的可行性与实用性。

Conclusion: 该无人机子系统能有效用于搜救中人物检测与追踪，下一步将扩展到大型无人机与GPS导航，实现更大规模下的自主救援部署。

Abstract: In this paper, we present a subsystem, using Unmanned Aerial Vehicles (UAV),
for search and rescue missions, focusing on people detection, face recognition
and tracking of identified individuals. The proposed solution integrates a UAV
with ROS2 framework, that utilizes multiple convolutional neural networks (CNN)
for search missions. System identification and PD controller deployment are
performed for autonomous UAV navigation. The ROS2 environment utilizes the
YOLOv11 and YOLOv11-pose CNNs for tracking purposes, and the dlib library CNN
for face recognition. The system detects a specific individual, performs face
recognition and starts tracking. If the individual is not yet known, the UAV
operator can manually locate the person, save their facial image and
immediately initiate the tracking process. The tracking process relies on
specific keypoints identified on the human body using the YOLOv11-pose CNN
model. These keypoints are used to track a specific individual and maintain a
safe distance. To enhance accurate tracking, system identification is
performed, based on measurement data from the UAVs IMU. The identified system
parameters are used to design PD controllers that utilize YOLOv11-pose to
estimate the distance between the UAVs camera and the identified individual.
The initial experiments, conducted on 14 known individuals, demonstrated that
the proposed subsystem can be successfully used in real time. The next step
involves implementing the system on a large experimental UAV for field use and
integrating autonomous navigation with GPS-guided control for rescue operations
planning.

</details>


### [146] [MoRPI-PINN: A Physics-Informed Framework for Mobile Robot Pure Inertial Navigation](https://arxiv.org/abs/2507.18206)
*Arup Kumar Sahoo,Itzik Klein*

Main category: cs.RO

TL;DR: 该论文提出了一种基于物理信息神经网络（PINN）的惯性导航方法MoRPI-PINN，实现了在无卫星信号或摄像头不可用的场景下，移动机器人高精度惯性导航，精度提升超过85%。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，移动机器人常面临卫星导航或摄像头不可用的环境，而单靠惯性传感器会因噪声和误差导致定位漂移。现有解决方法存在准确性不足或实现复杂的问题，因此急需一种高准确度、具备实际部署能力的惯性导航方案。

Method: 作者提出MoRPI-PINN，将物理定律与约束嵌入神经网络训练过程，结合蛇形运动以提升惯性信号质量，通过数据驱动与物理模型融合的方式抑制误差扩散，提升导航精度。

Result: 实验证明，MoRPI-PINN在惯性导航任务中，性能优于传统或现有方法，准确度提升超过85%，且该方法轻量，可部署到边缘设备满足实际应用需求。

Conclusion: MoRPI-PINN为无外部定位信息环境下的移动机器人提供了一种高精度、可行的导航解决方案，对提升机器人自主移动能力具有重要意义。

Abstract: A fundamental requirement for full autonomy in mobile robots is accurate
navigation even in situations where satellite navigation or cameras are
unavailable. In such practical situations, relying only on inertial sensors
will result in navigation solution drift due to the sensors' inherent noise and
error terms. One of the emerging solutions to mitigate drift is to maneuver the
robot in a snake-like slithering motion to increase the inertial
signal-to-noise ratio, allowing the regression of the mobile robot position. In
this work, we propose MoRPI-PINN as a physics-informed neural network framework
for accurate inertial-based mobile robot navigation. By embedding physical laws
and constraints into the training process, MoRPI-PINN is capable of providing
an accurate and robust navigation solution. Using real-world experiments, we
show accuracy improvements of over 85% compared to other approaches. MoRPI-PINN
is a lightweight approach that can be implemented even on edge devices and used
in any typical mobile robot application.

</details>


### [147] [Evaluation of facial landmark localization performance in a surgical setting](https://arxiv.org/abs/2507.18248)
*Ines Frajtag,Marko Švaco,Filip Šuligoj*

Main category: cs.RO

TL;DR: 本文研究了MediaPipe算法在手术照明下检测面部标志点的表现，并探讨了其在医疗中的潜在应用。


<details>
  <summary>Details</summary>
Motivation: 医学影像和机器人视觉在神经外科、眼科和整形外科等领域应用广泛，但面部检测算法在多变光照和检测位置灵活性方面依然面临挑战，亟需改进。

Method: 实验采用机械臂控制，自动调整检测位置，固定手术灯和面部模型（phantom），测试MediaPipe算法在不同角度下对面部标志点的检测准确性。

Result: 结果显示，在手术灯光环境下，MediaPipe算法对大角度（偏航、俯仰）下的面部标志点检测准确性明显提升，但部分标志点检测的标准差上升，指示个别点检测精度下降。

Conclusion: 改进后的MediaPipe算法能显著提高手术场景下面部检测能力，具备向医疗程序集成的潜力，但仍需优化标志点精度。

Abstract: The use of robotics, computer vision, and their applications is becoming
increasingly widespread in various fields, including medicine. Many face
detection algorithms have found applications in neurosurgery, ophthalmology,
and plastic surgery. A common challenge in using these algorithms is variable
lighting conditions and the flexibility of detection positions to identify and
precisely localize patients. The proposed experiment tests the MediaPipe
algorithm for detecting facial landmarks in a controlled setting, using a
robotic arm that automatically adjusts positions while the surgical light and
the phantom remain in a fixed position. The results of this study demonstrate
that the improved accuracy of facial landmark detection under surgical lighting
significantly enhances the detection performance at larger yaw and pitch
angles. The increase in standard deviation/dispersion occurs due to imprecise
detection of selected facial landmarks. This analysis allows for a discussion
on the potential integration of the MediaPipe algorithm into medical
procedures.

</details>


### [148] [ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic Grounding for Generalizable Robotic Manipulation](https://arxiv.org/abs/2507.18262)
*Chenyu Su,Weiwei Shang,Chen Qian,Fei Zhang,Shuang Cong*

Main category: cs.RO

TL;DR: 本文提出了ReSem3D框架，通过融合视觉基础模型（VFMs）和多模态大语言模型（MLLMs），实现复杂语义环境下的3D空间约束精细建模与实时操作，有效提升机器人在多样环境下的适应性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于语义驱动的3D空间约束方法存在三个主要问题：1）语义粒度较粗，难以表达细致任务差异，2）缺乏实时闭环规划能力，响应动态变化不足，3）面对多样化语义环境时鲁棒性受限。因此，亟需一种兼具高语义分辨率、实时性及鲁棒性的统一操作框架。

Method: 提出ReSem3D框架，通过MLLMs的层次递归推理，与VFMs交互，从自然语言命令和RGB-D观测中，分两阶段自动构建3D空间约束：先进行部件级提取，再细化到区域级。在操作过程中，这些层次空间约束被编码为关节空间内的实时最优化目标，实现对动态扰动的快速响应。

Result: 在多样语义的家庭和实验室等环境下，进行了大量仿真与真实机器人实验。结果显示，ReSem3D即便在零样本（zero-shot）条件下也能完成多种操作任务，展现出强大的环境适应性和泛化能力。

Conclusion: ReSem3D充分利用VFMs和MLLMs的协同推理，实现了对不同环境与任务的高级语义理解与动作执行统一，从而显著提升了机器人操作在复杂语义环境下的能力和稳定性。

Abstract: Semantics-driven 3D spatial constraints align highlevel semantic
representations with low-level action spaces, facilitating the unification of
task understanding and execution in robotic manipulation. The synergistic
reasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation
Models (VFMs) enables cross-modal 3D spatial constraint construction.
Nevertheless, existing methods have three key limitations: (1) coarse semantic
granularity in constraint modeling, (2) lack of real-time closed-loop planning,
(3) compromised robustness in semantically diverse environments. To address
these challenges, we propose ReSem3D, a unified manipulation framework for
semantically diverse environments, leveraging the synergy between VFMs and
MLLMs to achieve fine-grained visual grounding and dynamically constructs
hierarchical 3D spatial constraints for real-time manipulation. Specifically,
the framework is driven by hierarchical recursive reasoning in MLLMs, which
interact with VFMs to automatically construct 3D spatial constraints from
natural language instructions and RGB-D observations in two stages: part-level
extraction and region-level refinement. Subsequently, these constraints are
encoded as real-time optimization objectives in joint space, enabling reactive
behavior to dynamic disturbances. Extensive simulation and real-world
experiments are conducted in semantically rich household and sparse chemical
lab environments. The results demonstrate that ReSem3D performs diverse
manipulation tasks under zero-shot conditions, exhibiting strong adaptability
and generalization. Code and videos at https://resem3d.github.io.

</details>


### [149] [Adaptive Articulated Object Manipulation On The Fly with Foundation Model Reasoning and Part Grounding](https://arxiv.org/abs/2507.18276)
*Xiaojie Zhang,Yuanfei Wang,Ruihai Wu,Kunqi Xu,Yu Li,Liuyu Xiang,Hao Dong,Zhaofeng He*

Main category: cs.RO

TL;DR: 提出了AdaRPG新框架，基于基础模型按部件推理，提升机器人对多种类关节对象的操作泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前机器人操控多种类关节物体时，因物体结构和功能多样，难以统一推理和泛化操作策略。部分研究已尝试跨类别，但几何复杂性和功能多样性仍是难题。

Method: AdaRPG框架采用基础模型抽取物体部件而非整体，训练部件层级的可供性模型，推理部件功能，由此生成高阶控制代码调用基础技能。还构建了部件可供性注释数据集以支持训练。

Result: 在仿真和真实世界实验中，AdaRPG展现了较强的新类关节物体操控泛化能力。

Conclusion: AdaRPG可以有效提升机器人对新颖关节物体的视觉理解与适应性操控能力，在复杂多样物体场景中显示了良好的泛化表现。

Abstract: Articulated objects pose diverse manipulation challenges for robots. Since
their internal structures are not directly observable, robots must adaptively
explore and refine actions to generate successful manipulation trajectories.
While existing works have attempted cross-category generalization in adaptive
articulated object manipulation, two major challenges persist: (1) the
geometric diversity of real-world articulated objects complicates visual
perception and understanding, and (2) variations in object functions and
mechanisms hinder the development of a unified adaptive manipulation strategy.
To address these challenges, we propose AdaRPG, a novel framework that
leverages foundation models to extract object parts, which exhibit greater
local geometric similarity than entire objects, thereby enhancing visual
affordance generalization for functional primitive skills. To support this, we
construct a part-level affordance annotation dataset to train the affordance
model. Additionally, AdaRPG utilizes the common knowledge embedded in
foundation models to reason about complex mechanisms and generate high-level
control codes that invoke primitive skill functions based on part affordance
inference. Simulation and real-world experiments demonstrate AdaRPG's strong
generalization ability across novel articulated object categories.

</details>


### [150] [AF-RLIO: Adaptive Fusion of Radar-LiDAR-Inertial Information for Robust Odometry in Challenging Environments](https://arxiv.org/abs/2507.18317)
*Chenglong Qian,Yang Xu,Xiufang Shi,Jiming Chen,Liang Li*

Main category: cs.RO

TL;DR: 本文提出了一种名为AF-RLIO的自适应多传感器融合方法，通过融合4D毫米波雷达、激光雷达、IMU和GPS，实现了在复杂和动态环境中更加稳健的里程计估计，显著优于单一传感器系统，特别适用于烟雾、隧道等极端场景。


<details>
  <summary>Details</summary>
Motivation: 常规的单一传感器（如LiDAR或GPS）在复杂或恶劣的环境下（如烟雾、隧道、恶劣天气）容易失效，导致机器人定位和导航不准确，从而影响安全和稳定性，因此需要开发鲁棒的多传感器融合方法。

Method: 方法包括三个关键模块：（1）预处理模块利用毫米波雷达协助激光雷达识别和去除动态点，判断激光雷达性能下降的情形；（2）动态感知的多模态里程计模块，选取合适点云进行扫描匹配，并与IMU采用迭代误差状态卡尔曼滤波器进行紧耦合；（3）因子图优化模块在里程计和GPS数据之间权衡权重，构建位姿图进行全局优化。

Result: 该方法在公开数据集和实际机器人环境中进行了验证，与现有方法对比，在烟雾、隧道等极端环境下展现出更优的定位精度和鲁棒性。

Conclusion: AF-RLIO多传感器融合方法能有效提升自动机器人在复杂动态环境中的定位和导航能力，优于传统方法，具有广泛的实际应用前景。

Abstract: In robotic navigation, maintaining precise pose estimation and navigation in
complex and dynamic environments is crucial. However, environmental challenges
such as smoke, tunnels, and adverse weather can significantly degrade the
performance of single-sensor systems like LiDAR or GPS, compromising the
overall stability and safety of autonomous robots. To address these challenges,
we propose AF-RLIO: an adaptive fusion approach that integrates 4D
millimeter-wave radar, LiDAR, inertial measurement unit (IMU), and GPS to
leverage the complementary strengths of these sensors for robust odometry
estimation in complex environments. Our method consists of three key modules.
Firstly, the pre-processing module utilizes radar data to assist LiDAR in
removing dynamic points and determining when environmental conditions are
degraded for LiDAR. Secondly, the dynamic-aware multimodal odometry selects
appropriate point cloud data for scan-to-map matching and tightly couples it
with the IMU using the Iterative Error State Kalman Filter. Lastly, the factor
graph optimization module balances weights between odometry and GPS data,
constructing a pose graph for optimization. The proposed approach has been
evaluated on datasets and tested in real-world robotic environments,
demonstrating its effectiveness and advantages over existing methods in
challenging conditions such as smoke and tunnels.

</details>


### [151] [G2S-ICP SLAM: Geometry-aware Gaussian Splatting ICP SLAM](https://arxiv.org/abs/2507.18344)
*Gyuhyeon Pak,Hae Min Cho,Euntai Kim*

Main category: cs.RO

TL;DR: 本文提出了一种新的基于几何感知的RGB-D高斯斑点SLAM系统G2S-ICP SLAM，实现了高保真3D重建和鲁棒的实时相机位姿跟踪。该系统结合局部切平面上的二维高斯分布用于表面建模，相比传统的等方差三维椭球体方法，在多视角下获得了更一致的深度解释。引入了各向异性协方差先验至ICP配准框架，无需更改配准基础，并提出了结合几何信息的损失函数以提升整体匹配和重建质量。实验显示其在多个数据集上超越现有SLAM系统，兼顾定位精度与重建完整性。


<details>
  <summary>Details</summary>
Motivation: 实时SLAM系统在三维重建和相机位姿跟踪方面受限于表面表示方式的准确性。常规方法使用三维椭球体且等方差，难以充分捕捉表面几何细节，导致深度与重建不一致，因此亟需更符合几何结构的表示来提升SLAM整体性能。

Method: 提出用约束于切平面的二维高斯分布对场景进行建模，将该表面对齐的高斯斑点嵌入到通用ICP配准框架，并引入各向异性协方差先验，无需改变配准流程。同时设计了结合色彩、深度和法向一致性的几何感知损失函数提升系统表现。

Result: 在Replica和TUM-RGBD数据集上的实验结果表明，G2S-ICP SLAM实现了更高的定位精度和更完整的三维重建，同时保持较高的渲染质量，优于现有主流SLAM系统。

Conclusion: 通过新颖的表面对齐高斯斑点建模与改进的ICP配准，提出的G2S-ICP SLAM系统能在实时下提升三维重建和相机跟踪表现，为高质量、鲁棒的RGB-D SLAM提供了新方法。

Abstract: In this paper, we present a novel geometry-aware RGB-D Gaussian Splatting
SLAM system, named G2S-ICP SLAM. The proposed method performs high-fidelity 3D
reconstruction and robust camera pose tracking in real-time by representing
each scene element using a Gaussian distribution constrained to the local
tangent plane. This effectively models the local surface as a 2D Gaussian disk
aligned with the underlying geometry, leading to more consistent depth
interpretation across multiple viewpoints compared to conventional 3D
ellipsoid-based representations with isotropic uncertainty. To integrate this
representation into the SLAM pipeline, we embed the surface-aligned Gaussian
disks into a Generalized ICP framework by introducing anisotropic covariance
prior without altering the underlying registration formulation. Furthermore we
propose a geometry-aware loss that supervises photometric, depth, and normal
consistency. Our system achieves real-time operation while preserving both
visual and geometric fidelity. Extensive experiments on the Replica and
TUM-RGBD datasets demonstrate that G2S-ICP SLAM outperforms prior SLAM systems
in terms of localization accuracy, reconstruction completeness, while
maintaining the rendering quality.

</details>


### [152] [Residual Koopman Model Predictive Control for Enhanced Vehicle Dynamics with Small On-Track Data Input](https://arxiv.org/abs/2507.18396)
*Yonghao Fu,Cheng Hu,Haokun Xiong,Zhangpeng Bao,Wenyuan Du,Edoardo Ghignone,Michele Magno,Lei Xie,Hongye Su*

Main category: cs.RO

TL;DR: 提出一种结合传统线性模型与神经网络残差补偿的车辆轨迹跟踪方法（RKMPC），在多种仿真与实车实验中验证，显著提升控制性能并减少训练数据需求。


<details>
  <summary>Details</summary>
Motivation: 传统轨迹跟踪方法如Pure Pursuit忽略车辆模型约束，存在安全隐患；而模型预测控制（MPC）性能依赖于高精度建模，非线性与计算效率难以兼顾，影响实际控制效果。

Method: 搭建双层控制架构：一层为基于车辆运动学模型的线性MPC生成基线控制量，另一层为神经网络学习残差的RKMPC生成补偿控制量，两者相加形成最终输出。通过残差学习兼顾可解释性与高性能。

Result: RKMPC在Carsim-Matlab仿真和1:10 F1TENTH实体赛车实验均优于传统LMPC，所需训练数据仅为传统Koopman MPC的20%。横向误差降低11.7%~22.1%，航向误差降低8.9%~15.8%，前轮转向稳定性提升27.6%。

Conclusion: RKMPC兼顾模型可解释性与跟踪性能，大幅减小训练数据需求，提高了轨迹跟踪控制的安全性和实用性。

Abstract: In vehicle trajectory tracking tasks, the simplest approach is the Pure
Pursuit (PP) Control. However, this single-point preview tracking strategy
fails to consider vehicle model constraints, compromising driving safety. Model
Predictive Control (MPC) as a widely adopted control method, optimizes control
actions by incorporating mechanistic models and physical constraints. While its
control performance critically depends on the accuracy of vehicle modeling.
Traditional vehicle modeling approaches face inherent trade-offs between
capturing nonlinear dynamics and maintaining computational efficiency, often
resulting in reduced control performance. To address these challenges, this
paper proposes Residual Koopman Model Predictive Control (RKMPC) framework.
This method uses two linear MPC architecture to calculate control inputs: a
Linear Model Predictive Control (LMPC) computes the baseline control input
based on the vehicle kinematic model, and a neural network-based RKMPC
calculates the compensation input. The final control command is obtained by
adding these two components. This design preserves the reliability and
interpretability of traditional mechanistic model while achieving performance
optimization through residual modeling. This method has been validated on the
Carsim-Matlab joint simulation platform and a physical 1:10 scale F1TENTH
racing car. Experimental results show that RKMPC requires only 20% of the
training data needed by traditional Koopman Model Predictive Control (KMPC)
while delivering superior tracking performance. Compared to traditional LMPC,
RKMPC reduces lateral error by 11.7%-22.1%, decreases heading error by
8.9%-15.8%, and improves front-wheel steering stability by up to 27.6%. The
implementation code is available at: https://github.com/ZJU-DDRX/Residual
Koopman.

</details>


### [153] [Evaluating the Pre-Dressing Step: Unfolding Medical Garments Via Imitation Learning](https://arxiv.org/abs/2507.18436)
*David Blanco-Mulero,Júlia Borràs,Carme Torras*

Main category: cs.RO

TL;DR: 本文提出在机器人辅助穿衣中增设“预穿衣”步骤，即在正式穿衣前先展开叠好的医疗服装。通过模仿学习训练了三种展开动作，并用视觉分类器判断衣服展开状态。实验证明，不同动作组合能够提升展开效率，而单一大动态动作效果不佳。


<details>
  <summary>Details</summary>
Motivation: 以往的机器人辅助穿衣研究默认衣服已展开，忽略了医疗场景中衣物通常处于叠放状态。因此，需要解决衣服自动展开的问题，以便实现更实用的机器人辅助穿衣。

Method: 采用模仿学习训练机器人三种展开动作（包括高速和低速），并利用视觉分类器识别衣物状态（闭合、部分打开、完全打开）。对不同动作及其组合在展开中的表现进行实验对比分析。

Result: 实验表明，单纯依赖高速、大幅度动作难以有效展开新拆封叠好的衣服。组合多种动作方式更能高效地将衣物展开到适合穿衣的状态。

Conclusion: 在机器人辅助穿衣前增加自动展开衣物的“预穿衣”环节非常重要，通过组合多种操作动作能更有效地实现衣物充分展开，从而提升机器人辅助穿衣的实用性和效率。

Abstract: Robotic-assisted dressing has the potential to significantly aid both
patients as well as healthcare personnel, reducing the workload and improving
the efficiency in clinical settings. While substantial progress has been made
in robotic dressing assistance, prior works typically assume that garments are
already unfolded and ready for use. However, in medical applications gowns and
aprons are often stored in a folded configuration, requiring an additional
unfolding step. In this paper, we introduce the pre-dressing step, the process
of unfolding garments prior to assisted dressing. We leverage imitation
learning for learning three manipulation primitives, including both high and
low acceleration motions. In addition, we employ a visual classifier to
categorise the garment state as closed, partly opened, and fully opened. We
conduct an empirical evaluation of the learned manipulation primitives as well
as their combinations. Our results show that highly dynamic motions are not
effective for unfolding freshly unpacked garments, where the combination of
motions can efficiently enhance the opening configuration.

</details>


### [154] [A Novel Monte-Carlo Compressed Sensing and Dictionary Learning Method for the Efficient Path Planning of Remote Sensing Robots](https://arxiv.org/abs/2507.18462)
*Alghalya Al-Hajri,Ejmen Al-Ubejdij,Aiman Erbad,Ali Safa*

Main category: cs.RO

TL;DR: 本文提出了一种结合压缩感知测量矩阵结构与机器人路径优化的新方法，通过数据驱动的字典学习提升环境数据采集效率，显著缩短机器人行驶距离并提高数据重建精度。


<details>
  <summary>Details</summary>
Motivation: 随着压缩感知在高分辨率数据采集领域的应用增加，以及移动机器人在遥感和环境监测中的普及，急需一种高效采样路径和信号重建方法以提升采集效率和降低成本。以往方法未充分利用测量矩阵结构来优化机器人采样路径。

Method: 提出通过Monte Carlo优化框架，联合压缩感知测量矩阵结构和机器人遍历路径设计，结合字典学习自适应获取稀疏变换，协同优化路径长度和信号重建误差。

Result: 在NO2污染地图重建实验中，所提方法相比传统基于DCT与多项式字典的压缩感知方法，重建精度提高五倍以上，机器人行驶距离可降至全覆盖的10%以内，相对信息路径规划方法精度提升也达两倍。

Conclusion: 该方法显著提升了基于机器人环境感知的采样效率和重建精度，表明在移动感知任务中利用压缩感知测量矩阵结构进行路径优化具有巨大潜力。

Abstract: In recent years, Compressed Sensing (CS) has gained significant interest as a
technique for acquiring high-resolution sensory data using fewer measurements
than traditional Nyquist sampling requires. At the same time, autonomous
robotic platforms such as drones and rovers have become increasingly popular
tools for remote sensing and environmental monitoring tasks, including
measurements of temperature, humidity, and air quality. Within this context,
this paper presents, to the best of our knowledge, the first investigation into
how the structure of CS measurement matrices can be exploited to design
optimized sampling trajectories for robotic environmental data collection. We
propose a novel Monte Carlo optimization framework that generates measurement
matrices designed to minimize both the robot's traversal path length and the
signal reconstruction error within the CS framework. Central to our approach is
the application of Dictionary Learning (DL) to obtain a data-driven sparsifying
transform, which enhances reconstruction accuracy while further reducing the
number of samples that the robot needs to collect. We demonstrate the
effectiveness of our method through experiments reconstructing $NO_2$ pollution
maps over the Gulf region. The results indicate that our approach can reduce
robot travel distance to less than $10\%$ of a full-coverage path, while
improving reconstruction accuracy by over a factor of five compared to
traditional CS methods based on DCT and polynomial dictionaries, as well as by
a factor of two compared to previously-proposed Informative Path Planning (IPP)
methods.

</details>


### [155] [Experimental Comparison of Whole-Body Control Formulations for Humanoid Robots in Task Acceleration and Task Force Spaces](https://arxiv.org/abs/2507.18502)
*Sait Sovukluk,Grazia Zambella,Tobias Egle,Christian Ott*

Main category: cs.RO

TL;DR: 本文比较了两种不同的人形机器人全身控制方法：逆动力学控制（ID-WBC）与基于无源性的全身控制（PB-WBC），通过一系列实际实验测试其实际性能差异。


<details>
  <summary>Details</summary>
Motivation: 两种控制方法在理论上都能保证理想情况下的稳定性，但在实际中面对摩擦、传感器噪声、未知外部干扰和接触不完美等问题时，性能如何缺乏对比与研究，因此需要通过实验来实际检验其稳健性与优缺点。

Method: 在一个人形机器人平台上，分别用ID-WBC与PB-WBC控制器进行对比实验，包括挥动脚的位姿控制、负重和不负重的深蹲，以及跳跃动作，记录和分析控制性能，以及与理论分析相结合解释不同控制器的表现差异。

Result: 实验对比展示了在不同工况和任务下，两种控制器在性能上的优势和不足，以及各自在实际环境下的鲁棒性表现。

Conclusion: 两类控制方法各有优缺点，本文实验验证了理论分析结果，为实际人形机器人控制器选择与应用提供了参考依据。

Abstract: This paper studies the experimental comparison of two different whole-body
control formulations for humanoid robots: inverse dynamics whole-body control
(ID-WBC) and passivity-based whole-body control (PB-WBC). The two controllers
fundamentally differ from each other as the first is formulated in task
acceleration space and the latter is in task force space with passivity
considerations. Even though both control methods predict stability under ideal
conditions in closed-loop dynamics, their robustness against joint friction,
sensor noise, unmodeled external disturbances, and non-perfect contact
conditions is not evident. Therefore, we analyze and experimentally compare the
two controllers on a humanoid robot platform through swing foot position and
orientation control, squatting with and without unmodeled additional weights,
and jumping. We also relate the observed performance and characteristic
differences with the controller formulations and highlight each controller's
advantages and disadvantages.

</details>
