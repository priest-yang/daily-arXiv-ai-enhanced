<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 276]
- [cs.CL](#cs.CL) [Total: 177]
- [cs.RO](#cs.RO) [Total: 86]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [EDU-CIRCUIT-HW: Evaluating Multimodal Large Language Models on Real-World University-Level STEM Student Handwritten Solutions](https://arxiv.org/abs/2602.00095)
*Weiyu Sun,Liangliang Chen,Yongnuo Cai,Huiru Xie,Yi Zeng,Ying Zhang*

Main category: cs.CV

TL;DR: 本文发布了一个包含1300余份大学理工科手写解题的真实数据集EDU-CIRCUIT-HW，用以系统评估多模态大模型（MLLMs）在手写内容识别及自动批改任务中的可靠性。通过对现有主流MLLMs模型的实验，发现模型在理解复杂手写卷面方面仍存在大规模、隐蔽的失败，现阶段尚不适合高风险场景下的直接应用。进一步，作者探索了利用错误检测和自动纠错机制显著提升自动批改系统鲁棒性的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLMs在教育领域应用潜力巨大，但准确解读包含数学公式、图表、文本等多模态手写逻辑仍极具挑战，且缺乏真实且专业的数据集和有效的系统性评估方式。传统评估方法侧重下游结果，未能全面反映MLLMs对复杂手写解题的实际理解水平。

Method: 作者创建并公开了EDU-CIRCUIT-HW真实手写解题数据集，结合专家逐字转写与批改报告，分别从上游手写识别精度和下游自动批改表现两个维度，系统评估了多种主流MLLMs。同时，提出基于错误模式检测和纠正的增强方案，仅需极少人工参与即可显著提升自动批改系统的可靠性。

Result: 实验显示，当前MLLMs在手写内容理解和自动批改任务中表现出较多隐蔽失误，难以胜任高风险教育自动化场景。采用基于错误检测的少量人工修正策略后，系统鲁棒性大幅提升。

Conclusion: 目前MLLMs在复杂手写解题自动理解与批改还有较大改进空间，数据与评测基准的建设尤为重要。通过引入自动识别失误和高效人工校正机制，可大大增强基于AI的教育自动化系统的实用性与可靠性。

Abstract: Multimodal Large Language Models (MLLMs) hold significant promise for revolutionizing traditional education and reducing teachers' workload. However, accurately interpreting unconstrained STEM student handwritten solutions with intertwined mathematical formulas, diagrams, and textual reasoning poses a significant challenge due to the lack of authentic and domain-specific benchmarks. Additionally, current evaluation paradigms predominantly rely on the outcomes of downstream tasks (e.g., auto-grading), which often probe only a subset of the recognized content, thereby failing to capture the MLLMs' understanding of complex handwritten logic as a whole. To bridge this gap, we release EDU-CIRCUIT-HW, a dataset consisting of 1,300+ authentic student handwritten solutions from a university-level STEM course. Utilizing the expert-verified verbatim transcriptions and grading reports of student solutions, we simultaneously evaluate various MLLMs' upstream recognition fidelity and downstream auto-grading performance. Our evaluation uncovers an astonishing scale of latent failures within MLLM-recognized student handwritten content, highlighting the models' insufficient reliability for auto-grading and other understanding-oriented applications in high-stakes educational settings. In solution, we present a case study demonstrating that leveraging identified error patterns to preemptively detect and rectify recognition errors, with only minimal human intervention (approximately 4% of the total solutions), can significantly enhance the robustness of the deployed AI-enabled grading system on unseen student solutions.

</details>


### [2] [Mirage2Matter: A Physically Grounded Gaussian World Model from Video](https://arxiv.org/abs/2602.00096)
*Zhengqing Gao,Ziwen Li,Xin Wang,Jiaxin Huang,Zhenyang Ren,Mingkai Shao,Hanlue Zhang,Tianyu Huang,Yongkang Cheng,Yandong Guo,Runqi Lin,Yuanyuan Wang,Tongliang Liu,Kun Zhang,Mingming Gong*

Main category: cs.CV

TL;DR: 提出了一种基于图形的世界建模与仿真框架，只需多视角视频和通用素材，即可高效生成高保真训练数据，用于增强具身智能的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有仿真方法存在物理与视觉落差较大、依赖昂贵传感器和精确标定，导致大规模具身智能训练受限。需要一种简单高效且更真实的仿真环境构建策略。

Method: 利用3D Gaussian Splatting技术根据多视角视频重建真实环境的照片级三维模型，并结合生成式模型恢复物理属性，通过精准标定对齐场景与现实尺寸，将其集成进仿真平台，实现可编辑、具物理基础的世界模型。

Result: 基于该框架生成的仿真数据训练出来的视觉-语言-动作模型，在下游任务中实现了与真实数据相媲美甚至超越的零样本性能。

Conclusion: 重建驱动的世界建模方法为具身智能训练带来了更好的可扩展性和实用性，是实现大规模高效具身智能应用的有效途径。

Abstract: The scalability of embodied intelligence is fundamentally constrained by the scarcity of real-world interaction data. While simulation platforms provide a promising alternative, existing approaches often suffer from a substantial visual and physical gap to real environments and rely on expensive sensors, precise robot calibration, or depth measurements, limiting their practicality at scale. We present Simulate Anything, a graphics-driven world modeling and simulation framework that enables efficient generation of high-fidelity embodied training data using only multi-view environment videos and off-the-shelf assets. Our approach reconstructs real-world environments into a photorealistic scene representation using 3D Gaussian Splatting (3DGS), seamlessly capturing fine-grained geometry and appearance from video. We then leverage generative models to recover a physically realistic representation and integrate it into a simulation environment via a precision calibration target, enabling accurate scale alignment between the reconstructed scene and the real world. Together, these components provide a unified, editable, and physically grounded world model. Vision Language Action (VLA) models trained on our simulated data achieve strong zero-shot performance on downstream tasks, matching or even surpassing results obtained with real-world data, highlighting the potential of reconstruction-driven world modeling for scalable and practical embodied intelligence training.

</details>


### [3] [R3G: A Reasoning--Retrieval--Reranking Framework for Vision-Centric Answer Generation](https://arxiv.org/abs/2602.00104)
*Zhuohong Chen,Zhengxian Wu,Zirui Liao,Shenao Jiang,Hangrui Xu,Yang Chen,Chaokui Su,Xiaoyu Liu,Haoqian Wang*

Main category: cs.CV

TL;DR: 提出了一种名为R3G的模块化推理-检索-重排序框架，用于提升VQA中的图像证据获取与推理效果，在多个基线和场景下取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 在视觉问答（VQA）任务中，模型往往缺乏关键的视觉信息，需通过检索补充，但如何选取和有效集成适合的图像证据，依然存在挑战。

Method: R3G 框架包括三个步骤：1）生成简单的推理计划，明确所需的视觉线索；2）采用两阶段检索方法，先粗检索相关图片，再通过细粒度重排序选出最合适的证据图像；3）将检索到的图像整合进推理过程中，提升模型的推理能力。

Result: 在MRAG-Bench基准上，R3G框架在六种多模态大语言模型骨干和九个子场景中均提升了准确率，实现了当前最优的总体表现。消融实验表明，考虑信息充足度的重排序与推理步骤具有互补性，能够协同提升图像选择和推理效果。

Conclusion: R3G框架有效提高了VQA任务中图像检索与推理的表现，实现了SOTA，验证了模块化推理与充足性感知重排序的价值。

Abstract: Vision-centric retrieval for VQA requires retrieving images to supply missing visual cues and integrating them into the reasoning process. However, selecting the right images and integrating them effectively into the model's reasoning remains challenging.To address this challenge, we propose R3G, a modular Reasoning-Retrieval-Reranking framework.It first produces a brief reasoning plan that specifies the required visual cues, then adopts a two-stage strategy, with coarse retrieval followed by fine-grained reranking, to select evidence images.On MRAG-Bench, R3G improves accuracy across six MLLM backbones and nine sub-scenarios, achieving state-of-the-art overall performance. Ablations show that sufficiency-aware reranking and reasoning steps are complementary, helping the model both choose the right images and use them well. We release code and data at https://github.com/czh24/R3G.

</details>


### [4] [HYPE-EDIT-1: Benchmark for Measuring Reliability in Frontier Image Editing Models](https://arxiv.org/abs/2602.00105)
*Wing Chan,Richard Allen*

Main category: cs.CV

TL;DR: 本文提出HYPE-EDIT-1基准，用于真实的图片编辑模型评估，综合考虑模型性能与人工审核的时间和成本。结果显示单次成功率和实际编辑成本差异显著，低定价的模型在考虑重试和人工审核后未必便宜。


<details>
  <summary>Details</summary>
Motivation: 当前公开的图像编辑模型评测通常展示最优案例，忽略了用户实际操作中的多次重试和人工审核环节，难以真实反映模型在真实工作流程下的性能和成本。作者希望建立更贴近实际工作场景的评测方法。

Method: 作者设计了HYPE-EDIT-1基准，包括100个基于参考图片的营销/设计编辑任务，用二元通过/未通过进行判别。每个任务生成10个独立输出，用于估算单次通过率、10次内有成功的概率、在重试上限下平均重试次数，以及结合模型调用费用和人工审核时长得到的实际有效编辑成本。

Result: 评测发现，不同模型的单次通过率为34%-83%，每次成功的实际成本在0.66至1.42美元之间。有些模型虽然单张图片收费低，但因为需要多次重试和较多人工审核，综合后成本更高。

Conclusion: 单靠图像编辑模型的标价无法反映真实编辑成本，实际部署需将重试和人工审核纳入考量。HYPE-EDIT-1为行业提供了更实际的模型评测基准和工具。

Abstract: Public demos of image editing models are typically best-case samples; real workflows pay for retries and review time. We introduce HYPE-EDIT-1, a 100-task benchmark of reference-based marketing/design edits with binary pass/fail judging. For each task we generate 10 independent outputs to estimate per-attempt pass rate, pass@10, expected attempts under a retry cap, and an effective cost per successful edit that combines model price with human review time. We release 50 public tasks and maintain a 50-task held-out private split for server-side evaluation, plus a standardized JSON schema and tooling for VLM and human-based judging. Across the evaluated models, per-attempt pass rates span 34-83 percent and effective cost per success spans USD 0.66-1.42. Models that have low per-image pricing are more expensive when you consider the total effective cost of retries and human reviews.

</details>


### [5] [Efficient UAV trajectory prediction: A multi-modal deep diffusion framework](https://arxiv.org/abs/2602.00107)
*Yuan Gao,Xinyu Guo,Wenjing Xie,Zifan Wang,Hongwen Yu,Gongyang Li,Shugong Xu*

Main category: cs.CV

TL;DR: 提出了一种融合激光雷达（LiDAR）和毫米波雷达信息的多模态无人机轨迹预测深度融合模型，用于低空经济下的非法无人机管理，显著提升了预测准确率。


<details>
  <summary>Details</summary>
Motivation: 随着低空经济的发展，对非法无人机的管理需求增加，传统单一传感器受限于环境或信息维度，难以准确预测无人机轨迹。因此，需要结合不同传感器的互补信息以提升预测能力。

Method: 设计了一个多模态深度融合框架，包括针对LiDAR和毫米波雷达的独立特征提取网络，以及双向交叉注意力模块进行信息融合。使用结构相同但独立的特征编码器提取每种模态的特征，经交叉注意力机制实现特征互补和语义对齐。采用CVPR 2024 MMAUD数据集进行训练和测试。

Result: 实验表明，该多模态融合方法在轨迹预测精度上相比基线模型提升了40%。此外，通过消融实验验证了不同损失函数和后处理策略对性能提升的有效性。

Conclusion: 该方法能够高效利用多模态数据，为低空经济场景下非法无人机轨迹预测提供了有效解决方案。

Abstract: To meet the requirements for managing unauthorized UAVs in the low-altitude economy, a multi-modal UAV trajectory prediction method based on the fusion of LiDAR and millimeter-wave radar information is proposed. A deep fusion network for multi-modal UAV trajectory prediction, termed the Multi-Modal Deep Fusion Framework, is designed. The overall architecture consists of two modality-specific feature extraction networks and a bidirectional cross-attention fusion module, aiming to fully exploit the complementary information of LiDAR and radar point clouds in spatial geometric structure and dynamic reflection characteristics. In the feature extraction stage, the model employs independent but structurally identical feature encoders for LiDAR and radar. After feature extraction, the model enters the Bidirectional Cross-Attention Mechanism stage to achieve information complementarity and semantic alignment between the two modalities. To verify the effectiveness of the proposed model, the MMAUD dataset used in the CVPR 2024 UG2+ UAV Tracking and Pose-Estimation Challenge is adopted as the training and testing dataset. Experimental results show that the proposed multi-modal fusion model significantly improves trajectory prediction accuracy, achieving a 40% improvement compared to the baseline model. In addition, ablation experiments are conducted to demonstrate the effectiveness of different loss functions and post-processing strategies in improving model performance. The proposed model can effectively utilize multi-modal data and provides an efficient solution for unauthorized UAV trajectory prediction in the low-altitude economy.

</details>


### [6] [SITUATE -- Synthetic Object Counting Dataset for VLM training](https://arxiv.org/abs/2602.00108)
*René Peinl,Vincent Tischler,Patrick Schröder,Christian Groth*

Main category: cs.CV

TL;DR: 论文提出了一个新的数据集SITUATE，专为视觉-语言模型在具有空间约束的计数任务而设计，在模型泛化性测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的计数任务数据集要么过于简单（如VLMCountBench），要么无法精确控制遮挡和空间构图（如TallyQA），导致在训练和测试视觉-语言模型时存在泛化性和空间推理的不足。

Method: 作者构建了SITUATE数据集，能够对遮挡和空间组成因素进行控制，并用于训练和评估视觉-语言计数模型。在实验中，使用SITUATE对Qwen VL 2.5 7B模型进行微调后，对外部分布的Pixmo count测试数据的准确率提升，而反过来则没有提升。作者还将SITUATE与其他主流计数基准数据集以及来源于Pixmo count的同等规模优化集作了对比分析。

Result: 在多组对比实验中，SITUATE微调有效提升了模型在新的计数测试集上的泛化能力，而使用Pixmo count微调则达不到同等效果。

Conclusion: SITUATE数据集能够有效提升视觉-语言模型在具有空间约束和数据分布变化下的计数能力，并展现出比现有数据集更优异的泛化性能和实用价值。

Abstract: We present SITUATE, a novel dataset designed for training and evaluating Vision Language Models on counting tasks with spatial constraints. The dataset bridges the gap between simple 2D datasets like VLMCountBench and often ambiguous real-life datasets like TallyQA, which lack control over occlusions and spatial composition. Experiments show that our dataset helps to improve generalization for out-of-distribution images, since a finetune of Qwen VL 2.5 7B on SITUATE improves accuracy on the Pixmo count test data, but not vice versa. We cross validate this by comparing the model performance across established other counting benchmarks and against an equally sized fine-tuning set derived from Pixmo count.

</details>


### [7] [Robustness of Presentation Attack Detection in Remote Identity Validation Scenarios](https://arxiv.org/abs/2602.00109)
*John J. Howard,Richard O. Plesh,Yevgeniy B. Sirotin,Jerry L. Tipton,Arun R. Vemury*

Main category: cs.CV

TL;DR: 本文探讨了低光环境和自动图像采集对远程身份验证系统中展示攻击检测（PAD）功能的影响，显示大多数商用PAD系统在这些情形下性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 随着远程身份验证的广泛应用，提升其安全性和用户体验变得非常重要。然而，当前PAD系统在不同环境和流程下的稳健性表现不佳，尤其是面对低光和自动采集等现实场景。了解这些因素对PAD性能的影响，有助于推动更可靠的实际应用。

Method: 作者设计了包含低光和自动采集两类场景的对比实验，对多款商业PAD系统进行了测试，通过模型预测和实际结果评估它们在不同环境下的识别错误率。

Result: 实验结果显示，在低光环境下大多数PAD系统的错误率增加了约4倍，自动采集流程下则增加了约一倍。仅有一个系统在所有场景下表现稳健，分类错误率始终低于3%。

Conclusion: 研究表明，PAD系统在不同实际环境下可能表现不佳，强烈建议在多样化环境中进行充分测试，以确保系统在现实应用中的稳健性和可靠性。

Abstract: Presentation attack detection (PAD) subsystems are an important part of effective and user-friendly remote identity validation (RIV) systems. However, ensuring robust performance across diverse environmental and procedural conditions remains a critical challenge. This paper investigates the impact of low-light conditions and automated image acquisition on the robustness of commercial PAD systems using a scenario test of RIV. Our results show that PAD systems experience a significant decline in performance when utilized in low-light or auto-capture scenarios, with a model-predicted increase in error rates by a factor of about four under low-light conditions and a doubling of those odds under auto-capture workflows. Specifically, only one of the tested systems was robust to these perturbations, maintaining a maximum bona fide presentation classification error rate below 3% across all scenarios. Our findings emphasize the importance of testing across diverse environments to ensure robust and reliable PAD performance in real-world applications.

</details>


### [8] [Observing Health Outcomes Using Remote Sensing Imagery and Geo-Context Guided Visual Transformer](https://arxiv.org/abs/2602.00110)
*Yu Li,Guilherme N. DeSouza,Praveen Rao,Chi-Ren Shyu*

Main category: cs.CV

TL;DR: 本论文提出了一种融合地理空间辅助信息的新型视觉Transformer框架，通过地理空间嵌入和引导注意力机制提升遥感图像处理能力，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言和多模态模型主要关注视觉与文本的语义对齐，缺乏针对地理空间层级结构信息的有效表征和推理能力。这限制了它们在实际地理空间理解与推断任务中的应用。因此，亟需开发新方法来结合多样化的地理空间数据提升遥感场景下模型的表现。

Method: 论文提出一种地理空间嵌入机制，将多源地理空间信息转换为空间对齐的嵌入补丁，并通过引导注意力模块，根据与辅助数据的相关性为不同区域分配关注权重。每个注意力头承担不同任务，以捕捉引导信息的互补方面，从而提升模型的多模态融合及解释能力。

Result: 实验结果表明，该框架在疾病流行度预测等任务中，优于主流预训练的地理空间基础模型，显示了其多模态地理空间理解的有效性。

Conclusion: 通过将多样化地理空间信息融入视觉Transformer结构，并利用引导注意力机制，本方法显著提升了遥感图像处理效果，拓宽了多模态模型在地理空间领域的应用前景。

Abstract: Visual transformers have driven major progress in remote sensing image analysis, particularly in object detection and segmentation. Recent vision-language and multimodal models further extend these capabilities by incorporating auxiliary information, including captions, question and answer pairs, and metadata, which broadens applications beyond conventional computer vision tasks. However, these models are typically optimized for semantic alignment between visual and textual content rather than geospatial understanding, and therefore are not suited for representing or reasoning with structured geospatial layers. In this study, we propose a novel model that enhances remote sensing imagery processing with guidance from auxiliary geospatial information. Our approach introduces a geospatial embedding mechanism that transforms diverse geospatial data into embedding patches that are spatially aligned with image patches. To facilitate cross-modal interaction, we design a guided attention module that dynamically integrates multimodal information by computing attention weights based on correlations with auxiliary data, thereby directing the model toward the most relevant regions. In addition, the module assigns distinct roles to individual attention heads, allowing the model to capture complementary aspects of the guidance information and improving the interpretability of its predictions. Experimental results demonstrate that the proposed framework outperforms existing pretrained geospatial foundation models in predicting disease prevalence, highlighting its effectiveness in multimodal geospatial understanding.

</details>


### [9] [From Manual Observation to Automated Monitoring: Space Allowance Effects on Play Behaviour in Group-Housed Dairy Calves](https://arxiv.org/abs/2602.00111)
*Haiyu Yang,Heidi Lesscher,Enhong Liu,Miel Hostens*

Main category: cs.CV

TL;DR: 本研究分析了荷兰14家商业化乳牛场中，奶牛犊单只可用空间与其玩耍行为之间的关系，并开发出自动化计算机视觉管道实现大规模持续监测。结果显示，单犊8-10平方米为玩耍行为最多的空间，过小或过大空间都有所降低。计算机视觉系统对于活跃玩耍的检测表现高度准确。


<details>
  <summary>Details</summary>
Motivation: 玩耍行为是奶牛犊福利状况的重要正面指标，但在商业化环境下，不同（特别是中高）空间额度对这种行为影响尚不明确，因此需要系统研究，并开发自动化准确监测方法以助于大规模、实时福利评估。

Method: 本研究选取14家商业化乳牛场、60组奶牛犊，空间范围2.66-17.98平方米/只，通过视频及详细行为辞典（ethogram）人工观测，并应用线性混合模型分析影响因素。同时开发并验证了基于人工标注的计算机视觉自动分析管道用于行为识别。

Result: 发现玩耍行为与空间额度呈非线性关系，8-10平方米/只空间时奶牛犊玩耍行为最频繁（1.6%观测期），太小或过大的空间则较少（<0.6%）。控制年龄、健康、群体规模后，空间影响仍显著。自动化系统对于活跃玩耍识别准确率高达97.6%，召回率99.4%。

Conclusion: 建议8-10平方米/只作为奶牛犊舍合理空间目标，该标准在动物福利与经济可行性之间取得平衡。同时，自动化行为监控技术已具备大规模持续福利评估能力，有助于行业推广应用。

Abstract: Play behaviour serves as a positive welfare indicator in dairy calves, yet the influence of space allowance under commercial conditions remains poorly characterized, particularly at intermediate-to-high allowances (6-20 m2 per calf). This study investigated the relationship between space allowance and play behaviour in 60 group-housed dairy calves across 14 commercial farms in the Netherlands (space range: 2.66-17.98 m2 per calf), and developed an automated computer vision pipeline for scalable monitoring. Video observations were analyzed using a detailed ethogram, with play expressed as percentage of observation period (%OP). Statistical analysis employed linear mixed models with farm as a random effect. A computer vision pipeline was trained on manual annotations from 108 hours on 6 farms and validated on held-out test data. The computer vision classifier achieved 97.6% accuracy with 99.4% recall for active play detection. Calves spent on average 1.0% of OP playing reflecting around 10 minutes per 17-hour period. The space-play relationship was non-linear, with highest play levels at 8-10 m2 per calf (1.6% OP) and lowest at 6-8 m2 and 12-14 m2 (<0.6% OP). Space remained significant after controlling for age, health, and group size. In summary, these findings suggest that 8-10 m2 per calf represents a practical target balancing welfare benefits with economic feasibility, and demonstrate that automated monitoring can scale small annotation projects to continuous welfare assessment systems.

</details>


### [10] [AI-Driven Three-Dimensional Reconstruction and Quantitative Analysis for Burn Injury Assessment](https://arxiv.org/abs/2602.00113)
*S. Kalaycioglu,C. Hong,K. Zhai,H. Xie,J. N. Wong*

Main category: cs.CV

TL;DR: 本文提出了一个结合多视角摄影测量、三维重建和深度学习分割的AI烧伤评估与管理平台，实现了客观、可量化和可追踪的烧伤评估。


<details>
  <summary>Details</summary>
Motivation: 传统肉眼检查和二维摄影在烧伤评估中主观性强、难以客观量化，且不便于病情随时间对比，无法满足准确治疗及法律文档需求。

Method: 平台将多角度普通相机图片经三维重建算法恢复患者特异性的烧伤表面，并利用深度学习分割算法识别烧伤区域，计算面积、TBSA、体积等客观指标。连续评估结果可空间配准，实现愈合过程定量跟踪，并辅以用户引导、分析、推荐和报告自动生成。

Result: 仿真实验显示，该系统三维重建稳定，评估指标一致，纵向趋势符合临床预期，表明方法具备临床可行性与推广潜力。

Conclusion: 该平台为急性与门诊烧伤评估提供了高效、无创、可扩展、基于几何的客观辅助决策工具，有望提升烧伤诊疗标准化和智能化水平。

Abstract: Accurate, reproducible burn assessment is critical for treatment planning, healing monitoring, and medico-legal documentation, yet conventional visual inspection and 2D photography are subjective and limited for longitudinal comparison. This paper presents an AI-enabled burn assessment and management platform that integrates multi-view photogrammetry, 3D surface reconstruction, and deep learning-based segmentation within a structured clinical workflow. Using standard multi-angle images from consumer-grade cameras, the system reconstructs patient-specific 3D burn surfaces and maps burn regions onto anatomy to compute objective metrics in real-world units, including surface area, TBSA, depth-related geometric proxies, and volumetric change. Successive reconstructions are spatially aligned to quantify healing progression over time, enabling objective tracking of wound contraction and depth reduction. The platform also supports structured patient intake, guided image capture, 3D analysis and visualization, treatment recommendations, and automated report generation. Simulation-based evaluation demonstrates stable reconstructions, consistent metric computation, and clinically plausible longitudinal trends, supporting a scalable, non-invasive approach to objective, geometry-aware burn assessment and decision support in acute and outpatient care.

</details>


### [11] [1S-DAug: One-Shot Data Augmentation for Robust Few-Shot Generalization](https://arxiv.org/abs/2602.00114)
*Yunwei Bai,Ying Kiat Tan,Yao Shu,Tsuhan Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为1S-DAug的新型生成式增强方法，用于提升少样本学习（FSL）在新类别上的泛化能力，显著优于常规测试时增强策略。


<details>
  <summary>Details</summary>
Motivation: 面对FSL中标注样本极少，导致泛化能力不足和模型鲁棒性差的问题，现有的测试时增强难以奏效。为此，作者希望通过新的增强手段，在仅有1个样本时提升模型识别能力。

Method: 1S-DAug结合了传统几何扰动、受控噪声注入、和基于去噪扩散模型的生成方法，在测试时从单个样本合成多样且忠实的变体图片，并与原始图片共同编码聚合为组合表征，无需更改原有模型参数，仅以插件形式集成。

Result: 在4个主流数据集标准测试上，1S-DAug无须训练即可作为插件应用于不同模型，均显著提升了FSL性能，尤其在miniImagenet 5-way-1-shot基准上提升了10%以上的相对准确率。

Conclusion: 1S-DAug是一种有效、模型无关、训练无需求的新型增强方法，可显著提升FSL场景下的泛化能力和预测准确率，具有良好实用推广前景。

Abstract: Few-shot learning (FSL) challenges model generalization to novel classes based on just a few shots of labeled examples, a testbed where traditional test-time augmentations fail to be effective. We introduce 1S-DAug, a one-shot generative augmentation operator that synthesizes diverse yet faithful variants from just one example image at test time. 1S-DAug couples traditional geometric perturbations with controlled noise injection and a denoising diffusion process conditioned on the original image. The generated images are then encoded and aggregated, alongside the original image, into a combined representation for more robust FSL predictions. Integrated as a training-free model-agnostic plugin, 1S-DAug consistently improves FSL across standard benchmarks of 4 different datasets without any model parameter update, including achieving over 10% proportional accuracy improvement on the miniImagenet 5-way-1-shot benchmark. Codes will be released.

</details>


### [12] [Event Driven Clustering Algorithm](https://arxiv.org/abs/2602.00115)
*David El-Chai Ben-Ezra,Adar Tal,Daniel Brisk*

Main category: cs.CV

TL;DR: 提出了一种高效的异步事件驱动算法，用于实时检测事件相机数据中的小事件簇，算法复杂度为线性且不受像素维度影响。


<details>
  <summary>Details</summary>
Motivation: 事件相机产生的数据具有高时间分辨率和稀疏性，传统聚类方法难以实时、高效地检测事件簇，尤其是在数据量大或需要实时响应的应用场景下。

Method: 基于异步事件驱动机制，采用类似分层聚类的方式，并依据事件之间的时空距离进行分簇。通过优化决策机制，使得聚类算法整体复杂度达到O(n)，且与像素阵列尺寸无关。

Result: 所提算法在处理事件相机数据时实现了实时、线性复杂度的事件簇检测，无需关注像素阵列的维度，显著提升了运算效率。

Conclusion: 新算法有效提升了事件相机小事件簇检测的效率和实用性，适合实时、大规模事件数据的应用场景。

Abstract: This paper introduces a novel asynchronous, event-driven algorithm for real-time detection of small event clusters in event camera data. Like other hierarchical agglomerative clustering algorithms, the algorithm detects the event clusters based on their tempo-spatial distance. However, the algorithm leverages the special asynchronous data structure of event camera, and by a sophisticated, efficient and simple decision-making, enjoys a linear complexity of $O(n)$ where $n$ is the events amount. In addition, the run-time of the algorithm is independent with the dimensions of the pixels array.

</details>


### [13] [IC-EO: Interpretable Code-based assistant for Earth Observation](https://arxiv.org/abs/2602.00117)
*Lamia Lahouel,Laurynas Lopata,Simon Gruening,Gabriele Meoni,Gaetan Petit,Sylvain Lobry*

Main category: cs.CV

TL;DR: 提出了一种能够将自然语言查询转换为可执行、可审计Python代码工作流的对话式代理，显著提升了地球观测数据分析的便捷性和透明度。


<details>
  <summary>Details</summary>
Motivation: 传统地球观测（EO）分析对非专业人员来说过于复杂且专业门槛高，且现有系统多为“黑箱”，缺乏可审计性和可复现性。作者希望解决分析难、复现难、解读难三大问题。

Method: 基于最新Tool LLM技术，开发了一个对话驱动、能生成Python代码的智能代理。该系统集成了分类、分割、检测、光谱指数与地理操作等能力，并通过统一API开放调用。结果控制分为工具级性能、代理生成代码质量和特定任务表现三个层面。

Result: 在土地组成制图和火灾后损失评估两大实际任务上，该代理系统相较于现有大模型（如GPT-4o、LLaVA）取得了更高准确率（分别为64.2%对51.7%，50%对0%），且输出代码透明易于理解。

Conclusion: 本方法显著提升了EO数据分析的可复现性与透明度，降低了使用门槛，为非专业用户和专业场景提供了一种高效、可审计的解决方案。

Abstract: Despite recent advances in computer vision, Earth Observation (EO) analysis remains difficult to perform for the laymen, requiring expert knowledge and technical capabilities. Furthermore, many systems return black-box predictions that are difficult to audit or reproduce. Leveraging recent advances in tool LLMs, this study proposes a conversational, code-generating agent that transforms natural-language queries into executable, auditable Python workflows. The agent operates over a unified easily extendable API for classification, segmentation, detection (oriented bounding boxes), spectral indices, and geospatial operators. With our proposed framework, it is possible to control the results at three levels: (i) tool-level performance on public EO benchmarks; (ii) at the agent-level to understand the capacity to generate valid, hallucination-free code; and (iii) at the task-level on specific use cases. In this work, we select two use-cases of interest: land-composition mapping and post-wildfire damage assessment. The proposed agent outperforms general-purpose LLM/VLM baselines (GPT-4o, LLaVA), achieving 64.2% vs. 51.7% accuracy on land-composition and 50% vs. 0% on post-wildfire analysis, while producing results that are transparent and easy to interpret. By outputting verifiable code, the approach turns EO analysis into a transparent, reproducible process.

</details>


### [14] [VDE Bench: Evaluating The Capability of Image Editing Models to Modify Visual Documents](https://arxiv.org/abs/2602.00122)
*Hongzhu Yi,Yujia Yang,Yuanxiang Wang,Zhenyu Guan,Jiahuan Chen,Chenxi Bao,Tiankun Yang,Yixuan Yuan,Tianyu Zong,Xinming Wang,Tao Yu,Ruiwen Tao,Haijin Liang,Jin Ma,Jinwen Luo,Yeshani Xinyu Zuo,Jungang Xu*

Main category: cs.CV

TL;DR: 本文提出了VDE Bench，这是首个系统性评测多语种、密集文本视觉文档编辑能力的基准，包含高质量中英文文本图像，并提供细粒度评测方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态图像编辑方法主要聚焦于英语和稀疏文本，对于结构复杂、密集文本以及非拉丁文种（如中文）的编辑能力不足，缺乏系统化评测工具。

Method: 作者构建了VDE Bench基准，包括高质量人工标注的中英文密集文本文档（如论文、海报、幻灯片、考试资料和报纸），并设计了解耦的OCR级编辑性能评测体系，可细致量化文本修改准确性。

Result: 作者基于VDE Bench对多种主流图像编辑模型进行了全面评测，人工验证表明自动化指标与人工评判高度一致。

Conclusion: VDE Bench首次为多语种、密集文本视觉文档的编辑模型提供了系统性、可靠的评测基准，为该领域后续研究和模型进步奠定基础。

Abstract: In recent years, multimodal image editing models have achieved substantial progress, enabling users to manipulate visual content through natural language in a flexible and interactive manner. Nevertheless, an important yet insufficiently explored research direction remains visual document image editing, which involves modifying textual content within images while faithfully preserving the original text style and background context. Existing approaches, including AnyText, GlyphControl, and TextCtrl, predominantly focus on English-language scenarios and documents with relatively sparse textual layouts, thereby failing to adequately address dense, structurally complex documents or non-Latin scripts such as Chinese. To bridge this gap, we propose \textbf{V}isual \textbf{D}oc \textbf{E}dit Bench(VDE Bench), a rigorously human-annotated and evaluated benchmark specifically designed to assess image editing models on multilingual and complex visual document editing tasks. The benchmark comprises a high-quality dataset encompassing densely textual documents in both English and Chinese, including academic papers, posters, presentation slides, examination materials, and newspapers. Furthermore, we introduce a decoupled evaluation framework that systematically quantifies editing performance at the OCR parsing level, enabling fine-grained assessment of text modification accuracy. Based on this benchmark, we conduct a comprehensive evaluation of representative state-of-the-art image editing models. Manual verification demonstrates a strong consistency between human judgments and automated evaluation metrics. VDE Bench constitutes the first systematic benchmark for evaluating image editing models on multilingual and densely textual visual documents.

</details>


### [15] [Context-Aware Autoencoders for Anomaly Detection in Maritime Surveillance](https://arxiv.org/abs/2602.00124)
*Divya Acharya,Pierre Bernab'e,Antoine Chevrot,Helge Spieker,Arnaud Gotlieb,Bruno Legeard*

Main category: cs.CV

TL;DR: 本文提出了一种上下文感知自编码器，用于改进海事船只交通监控中的异常检测，显著提升了检测准确率并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 海事船舶交通监控中异常检测对安全保障至关重要，但传统自编码器难以检测集体和上下文相关的异常，尤其是在依赖船只自报告AIS消息的数据环境中，异常常常与具体船舶相关联。该工作旨在解决自编码器在这些特定场景下的局限性。

Method: 提出上下文感知自编码器，通过引入上下文特定的阈值，提高了模型对异常的识别能力和效率。论文对比了4种上下文感知自编码器变体和传统自编码器，并在渔船异常检测的实际案例中进行了分析。

Result: 实验结果显示，上下文对重构损失和异常检测表现有显著影响，上下文感知自编码器在时间序列异常检测中优于其他方法。

Conclusion: 上下文感知自编码器通过考虑上下文重要性和采用特定阈值，有效提升了海事船舶异常监控系统的准确性，具有广阔应用前景。

Abstract: The detection of anomalies is crucial to ensuring the safety and security of maritime vessel traffic surveillance. Although autoencoders are popular for anomaly detection, their effectiveness in identifying collective and contextual anomalies is limited, especially in the maritime domain, where anomalies depend on vessel-specific contexts derived from self-reported AIS messages. To address these limitations, we propose a novel solution: the context-aware autoencoder. By integrating context-specific thresholds, our method improves detection accuracy and reduces computational cost. We compare four context-aware autoencoder variants and a conventional autoencoder using a case study focused on fishing status anomalies in maritime surveillance. Results demonstrate the significant impact of context on reconstruction loss and anomaly detection. The context-aware autoencoder outperforms others in detecting anomalies in time series data. By incorporating context-specific thresholds and recognizing the importance of context in anomaly detection, our approach offers a promising solution to improve accuracy in maritime vessel traffic surveillance systems.

</details>


### [16] [D3R-Net: Dual-Domain Denoising Reconstruction Network for Robust Industrial Anomaly Detection](https://arxiv.org/abs/2602.00126)
*Dmytro Filatov,Valentyn Fedorov,Vira Filatova,Andrii Zelenchuk*

Main category: cs.CV

TL;DR: 该论文提出了一种适用于工业视觉检测的无监督异常检测框架D3R-Net，通过引入频域损失显著提升了对缺陷区域的分割与定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于重构的方法对高频细节处理不佳，导致微小缺陷无法准确区分，进而限制异常检测的准确率。为了解决这个问题，作者希望改进重构模型，使其能够更好地识别和突出细微缺陷。

Method: 提出了D3R-Net框架，其核心是结合自监督的图像“修复”任务与频域感知正则化（FFT损失）。训练时模型需将合成损坏的正常图像重构为干净图像，防止模型学习到恒等映射。损失函数同时包含空间域均方误差（MSE）和频率域FFT幅值损失，可选结构相似性（SSIM）作为消融研究。此外，D3R-Net采用轻量级卷积自编码器作为主干网。

Result: 在MVTec AD Hazelnut数据集上，加入FFT损失后，分割定位性能（PRO AUC）由0.603提升至0.687，图像级ROC AUC保持强健。在MVTec十五月类别平均上，像素级ROC AUC由0.733提升至0.751，PRO AUC由0.417提升至0.468，单GPU推理速度约为20 FPS。

Conclusion: D3R-Net框架能在不依赖大模型特征的前提下，有效提升工业视觉检测中的异常分割和定位效果，具有较高的实际应用价值。

Abstract: Unsupervised anomaly detection (UAD) is a key ingredient of automated visual inspection in modern manufacturing. The reconstruction-based methods appeal because they have basic architectural design and they process data quickly but they produce oversmoothed results for high-frequency details. As a result, subtle defects are partially reconstructed rather than highlighted, which limits segmentation accuracy. We build on this line of work and introduce D3R-Net, a Dual-Domain Denoising Reconstruction framework that couples a self-supervised 'healing' task with frequency-aware regularization. During training, the network receives synthetically corrupted normal images and is asked to reconstruct the clean targets, which prevents trivial identity mapping and pushes the model to learn the manifold of defect-free textures. In addition to the spatial mean squared error, we employ a Fast Fourier Transform (FFT) magnitude loss that encourages consistency in the frequency domain. The implementation also allows an optional structural similarity (SSIM) term, which we study in an ablation. On the MVTec AD Hazelnut benchmark, D3R-Net with the FFT loss improves localization consistency over a spatial-only baseline: PRO AUC increases from 0.603 to 0.687, while image-level ROC AUC remains robust. Evaluated across fifteen MVTec categories, the FFT variant raises the average pixel ROC AUC from 0.733 to 0.751 and PRO AUC from 0.417 to 0.468 compared to the MSE-only baseline, at roughly 20 FPS on a single GPU. The network is trained from scratch and uses a lightweight convolutional autoencoder backbone, providing a practical alternative to heavy pre-trained feature embedding methods.

</details>


### [17] [PovNet+: A Deep Learning Architecture for Socially Assistive Robots to Learn and Assist with Multiple Activities of Daily Living](https://arxiv.org/abs/2602.00131)
*Fraser Robinson,Souren Pashangpour,Matthew Lisondra,Goldie Nejat*

Main category: cs.CV

TL;DR: 该论文提出了POVNet+，一种多模态深度学习架构，实现了社交辅助机器人对多种日常活动（ADLs）的识别，从而主动提供辅助。


<details>
  <summary>Details</summary>
Motivation: 长期部署社交辅助机器人的主要难题在于机器人无法感知及辅助多项ADL，限制了其实际应用。本研究旨在突破活动识别的障碍，使机器人能真实地为用户多方位辅助。

Method: POVNet+结合ADL和动作的嵌入空间，通过多模态方法区分已知ADL、新出现的ADL，以及非典型执行的ADL。并利用创新用户状态估计方法于动作嵌入空间中监测新活动和用户表现，利用这些信息来主动发起机器人辅助行为。

Result: 与先进的人体活动识别方法对比，POVNet+在ADL分类准确率上更高。在多用户、复杂家庭环境中，协助机器人Leia实验表明该架构能准确识别多种常见和新出现的ADL及其异常执行，同时合理发起辅助互动。

Conclusion: POVNet+为社交辅助机器人感知和辅助复杂多样的日常活动提供了有效技术，提升了机器人主动辅助能力，有望推动其长期真实场景部署。

Abstract: A significant barrier to the long-term deployment of autonomous socially assistive robots is their inability to both perceive and assist with multiple activities of daily living (ADLs). In this paper, we present the first multimodal deep learning architecture, POVNet+, for multi-activity recognition for socially assistive robots to proactively initiate assistive behaviors. Our novel architecture introduces the use of both ADL and motion embedding spaces to uniquely distinguish between a known ADL being performed, a new unseen ADL, or a known ADL being performed atypically in order to assist people in real scenarios. Furthermore, we apply a novel user state estimation method to the motion embedding space to recognize new ADLs while monitoring user performance. This ADL perception information is used to proactively initiate robot assistive interactions. Comparison experiments with state-of-the-art human activity recognition methods show our POVNet+ method has higher ADL classification accuracy. Human-robot interaction experiments in a cluttered living environment with multiple users and the socially assistive robot Leia using POVNet+ demonstrate the ability of our multi-modal ADL architecture in successfully identifying different seen and unseen ADLs, and ADLs being performed atypically, while initiating appropriate assistive human-robot interactions.

</details>


### [18] [Shedding the Facades, Connecting the Domains: Detecting Shifting Multimodal Hate Video with Test-Time Adaptation](https://arxiv.org/abs/2602.00132)
*Jiao Li,Jian Lang,Xikai Tang,Wenzheng Shu,Ting Zhong,Qiang Gao,Yong Wang,Leiting Chen,Fan Zhou*

Main category: cs.CV

TL;DR: 为应对仇恨视频内容在分布和表现形式上不断变化导致检测模型失效的问题，提出了首个专为仇恨视频检测（HVD）设计的测试时自适应（TTA）框架SCANNER，有效提升了模型在严重语义漂移下的检测表现。


<details>
  <summary>Details</summary>
Motivation: 现有仇恨视频检测方法假设训练与推理数据分布一致，然而仇恨内容会不断演化以规避审查，导致语义变化显著，原有模型表现大幅下降。传统TTA方法仅应对轻微分布漂移，难以处理HVD场景下的严重语义漂移。

Method: SCANNER以仇恨内容深层核心（如性别、种族等为攻击目标）具有稳定性为出发点，通过中心引导的对齐机制揭示进化后内容中的稳定核心特征，并引入自适应簇内对齐和多样性正则化，提升适应能力、应对异常样本、减少语义塌缩。

Result: 实验结果显示，SCANNER在各种基线方法上均取得更优表现，Macro-F1均值提升4.69%。

Conclusion: SCANNER能够有效缓解仇恨视频检测中的严重语义漂移问题，在不同领域下具备更强的适应能力和泛化性能。

Abstract: Hate Video Detection (HVD) is crucial for online ecosystems. Existing methods assume identical distributions between training (source) and inference (target) data. However, hateful content often evolves into irregular and ambiguous forms to evade censorship, resulting in substantial semantic drift and rendering previously trained models ineffective. Test-Time Adaptation (TTA) offers a solution by adapting models during inference to narrow the cross-domain gap, while conventional TTA methods target mild distribution shifts and struggle with the severe semantic drift in HVD. To tackle these challenges, we propose SCANNER, the first TTA framework tailored for HVD. Motivated by the insight that, despite the evolving nature of hateful manifestations, their underlying cores remain largely invariant (i.e., targeting is still based on characteristics like gender, race, etc), we leverage these stable cores as a bridge to connect the source and target domains. Specifically, SCANNER initially reveals the stable cores from the ambiguous layout in evolving hateful content via a principled centroid-guided alignment mechanism. To alleviate the impact of outlier-like samples that are weakly correlated with centroids during the alignment process, SCANNER enhances the prior by incorporating a sample-level adaptive centroid alignment strategy, promoting more stable adaptation. Furthermore, to mitigate semantic collapse from overly uniform outputs within clusters, SCANNER introduces an intra-cluster diversity regularization that encourages the cluster-wise semantic richness. Experiments show that SCANNER outperforms all baselines, with an average gain of 4.69% in Macro-F1 over the best.

</details>


### [19] [LLaVA-FA: Learning Fourier Approximation for Compressing Large Multimodal Models](https://arxiv.org/abs/2602.00135)
*Pengcheng Zheng,Chaoning Zhang,Jiarong Mo,GuoHui Li,Jiaquan Zhang,Jiahao Zhang,Sihan Cao,Sheng Zheng,Caiyan Qin,Guoqing Wang,Yang Yang*

Main category: cs.CV

TL;DR: 本文提出了一种高效的大型多模态模型（LMM）压缩方法LLaVA-FA，通过在频域内联合低秩分解和量化，显著提升权重量化的紧凑性和准确性，结合极坐标量化与对角校准，实现低计算和内存消耗，同时保持甚至超越现有压缩模型的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LMMs在多种视觉-语言任务上取得卓越表现，但其巨大的计算和存储开销阻碍了实际部署。现有压缩方法通常将低秩分解和量化分开处理，导致误差累积，特别是在存在跨模态冗余的多模态结构中问题更为突出。因而需要一种能提升模型效率、减少误差的联合压缩方法。

Method: 提出LLaVA-FA方法，在频域下对模型权重进行联合低秩分解和量化，利用傅里叶变换的去相关与共轭对称特性，增强权重表达的紧凑性与准确性。同时提出PolarQuant极坐标量化方法，专为复数矩阵设计，并引入可选的对角校准机制，减少对大规模校准数据的依赖。

Result: 大量实验表明，LLaVA-FA在多个基准任务上，模型压缩后激活参数更少、计算量更低，但性能优于现有高效多模态模型。

Conclusion: LLaVA-FA综合了频域低秩分解与量化的优势，是高效压缩大型多模态模型的有效新方案，可为实际部署提供有力支持。

Abstract: Large multimodal models (LMMs) have achieved impressive performance on various vision-language tasks, but their substantial computational and memory costs hinder their practical deployment. Existing compression methods often decouple low-rank decomposition and quantization, leading to compounded reconstruction errors, especially in multimodal architectures with cross-modal redundancy. To address this issue, we propose LLaVA-FA, a novel efficient LMM that performs joint low-rank plus quantization approximation in the frequency domain. By leveraging the de-correlation and conjugate symmetry properties of Fourier transform, LLaVA-FA achieves more compact and accurate weight representations. Furthermore, we introduce PolarQuant, a polar-coordinate quantization method tailored for complex matrices, and an optional diagonal calibration (ODC) scheme that eliminates the need for large-scale calibration data. Extensive experimental results demonstrate that our proposed LLaVA-FA outperforms existing efficient multimodal models across multiple benchmarks while maintaining minimal activated parameters and low computational costs, validating its effectiveness as a powerful solution for compressing LMMs.

</details>


### [20] [Scalable Analytic Classifiers with Associative Drift Compensation for Class-Incremental Learning of Vision Transformers](https://arxiv.org/abs/2602.00144)
*Xuan Rao,Mingming Ha,Bo Zhao,Derong Liu,Cesare Alippi*

Main category: cs.CV

TL;DR: 本文提出了一种高效可扩展的分类器LR-RGDA和训练免费分布补偿器HopDC，用于解决基于视觉Transformer的大规模类别增量学习（CIL）中的计算瓶颈和表示漂移问题，在多个基准任务上取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 基于ViT的类别增量学习（CIL）在分类器重建阶段因依赖迭代SGD训练而面临巨大计算负担。虽然分析性RGDA能提供Bayes最优解决方案，但其二次推理复杂度不适用于大规模任务，因此，需要设计兼顾准确性和高效性的分类器。

Method: 1）提出基于Woodbury恒等式的低秩分解RGDA（LR-RGDA）分类器，将判别函数分解为全局仿射项与低秩二次扰动项，计算复杂度显著降低；2）为缓解骨干网络更新导致的表示漂移，提出基于Hopfield网络的分布补偿器（HopDC），通过无监督的锚点校准历史类别统计，并给出估计误差理论界。

Result: 在多个类别增量学习基准（如ImageNet等）上，通过实验验证了LR-RGDA与HopDC的组合在性能和推理效率方面均达到了最新水平，显著优于迭代优化类方法。

Conclusion: 提出的方法可在保证高准确率的同时显著减少推理和重训练开销，为基于ViT的大规模类别增量学习问题提供了实用且可扩展的解决方案。

Abstract: Class-incremental learning (CIL) with Vision Transformers (ViTs) faces a major computational bottleneck during the classifier reconstruction phase, where most existing methods rely on costly iterative stochastic gradient descent (SGD). We observe that analytic Regularized Gaussian Discriminant Analysis (RGDA) provides a Bayes-optimal alternative with accuracy comparable to SGD-based classifiers; however, its quadratic inference complexity limits its use in large-scale CIL scenarios. To overcome this, we propose Low-Rank Factorized RGDA (LR-RGDA), a scalable classifier that combines RGDA's expressivity with the efficiency of linear classifiers. By exploiting the low-rank structure of the covariance via the Woodbury matrix identity, LR-RGDA decomposes the discriminant function into a global affine term refined by a low-rank quadratic perturbation, reducing the inference complexity from $\mathcal{O}(Cd^2)$ to $\mathcal{O}(d^2 + Crd^2)$, where $C$ is the class number, $d$ the feature dimension, and $r \ll d$ the subspace rank. To mitigate representation drift caused by backbone updates, we further introduce Hopfield-based Distribution Compensator (HopDC), a training-free mechanism that uses modern continuous Hopfield Networks to recalibrate historical class statistics through associative memory dynamics on unlabeled anchors, accompanied by a theoretical bound on the estimation error. Extensive experiments on diverse CIL benchmarks demonstrate that our framework achieves state-of-the-art performance, providing a scalable solution for large-scale class-incremental learning with ViTs. Code: https://github.com/raoxuan98-hash/lr_rgda_hopdc.

</details>


### [21] [DensiThAI, A Multi-View Deep Learning Framework for Breast Density Estimation using Infrared Images](https://arxiv.org/abs/2602.00145)
*Siva Teja Kakileti,Geetha Manjunath*

Main category: cs.CV

TL;DR: 本研究提出了DensiThAI，一种利用红外热成像和多视角深度学习，实现乳腺密度分类的新方法。该方法无需X射线，避免电离辐射，实验结果显示在多中心数据集上具有较好表现。


<details>
  <summary>Details</summary>
Motivation: 乳腺密度是乳腺癌风险的重要生物标志物，现有评估主要依赖X射线摄影，但这种方法具有电离辐射风险，因此急需一种无创、无辐射的新方法来评估乳腺密度。

Method: 提出DensiThAI多视角深度学习框架，从红外热成像中分类乳腺密度。利用3,500名女性的多中心数据和乳腺摄影标签作为参照，通过五个标准热成像视图，采用深度学习进行训练和评估。

Result: DensiThAI在10次随机划分中达到了平均AUROC为0.73，且不同乳腺密度类别之间具有具有统计学显著性（p<<0.05）；在不同年龄组中表现一致。

Conclusion: 红外热成像结合AI能够较好地实现乳腺密度无创评估，在提升患者体验和优化工作流程方面具有潜力。

Abstract: Breast tissue density is a key biomarker of breast cancer risk and a major factor affecting mammographic sensitivity. However, density assessment currently relies almost exclusively on X-ray mammography, an ionizing imaging modality. This study investigates the feasibility of estimating breast density using artificial intelligence over infrared thermal images, offering a non-ionizing imaging approach. The underlying hypothesis is that fibroglandular and adipose tissues exhibit distinct thermophysical and physiological properties, leading to subtle but spatially coherent temperature variations on the breast surface. In this paper, we propose DensiThAI, a multi-view deep learning framework for breast density classification from thermal images. The framework was evaluated on a multi-center dataset of 3,500 women using mammography-derived density labels as reference. Using five standard thermal views, DensiThAI achieved a mean AUROC of 0.73 across 10 random splits, with statistically significant separation between density classes across all splits (p << 0.05). Consistent performance across age cohorts supports the potential of thermal imaging as a non-ionizing approach for breast density assessment with implications for improved patient experience and workflow optimization.

</details>


### [22] [Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields](https://arxiv.org/abs/2602.00148)
*Shiqian Li,Ruihong Shen,Junfeng Ni,Chang Pan,Chi Zhang,Yixin Zhu*

Main category: cs.CV

TL;DR: 该论文提出一种全新的神经高斯力场（NGFF）方法，实现了从多视角RGB输入到物理真实的视频生成，极大提升了模拟速度和物理合理性。


<details>
  <summary>Details</summary>
Motivation: 当前AI在从视觉数据中预测物理动态方面尚有较大挑战。虽然视频生成模型有较好视觉质量，但缺少物理法则建模，导致无法生成物理合理的视频。结合3D高斯与物理引擎的方法虽然物理上更可信，但因重建、仿真开销大和鲁棒性不足，限制了其实际应用。

Method: 作者提出NGFF神经高斯力场框架，将3D高斯感知与基于物理的动力学建模结合，可端到端生成交互式且物理真实的4D视频，并大幅提升仿真速度。为训练该模型，还构建了GSCollision数据集，包含多材料、多物体互动和复杂场景的大规模4D高斯物理视频。

Result: NGFF在合成与真实三维场景的评测中，展现出优异的泛化能力与物理推理的鲁棒性，其仿真速度比以往高斯模拟器快两数量级。

Conclusion: 该方法推动了基于物理的视频预测和世界建模的发展，为物理感知、推理及相关视觉任务带来新的突破。

Abstract: Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF's strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models.

</details>


### [23] [SDCM: Simulated Densifying and Compensatory Modeling Fusion for Radar-Vision 3-D Object Detection in Internet of Vehicles](https://arxiv.org/abs/2602.00149)
*Shucong Li,Xiaoluo Zhou,Yuqian He,Zhenyu Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为SDCM的基于4D雷达与视觉融合的三维目标检测框架，有效提升了稀疏雷达点云和低质视觉数据下的检测性能。


<details>
  <summary>Details</summary>
Motivation: 4D雷达点云稀疏导致三维表征能力差，视觉信息在弱光、远距和遮挡等场景易退化，使目标检测和感知面临挑战。

Method: 提出SDCM框架，包含三个主要模块：1）SimDen模块采用3D KDE和高斯/曲率模拟对雷达点云进行稠密化；2）RCM模块利用雷达实时、全天候优势补偿视觉信息退化；3）MMIF模块对多模态特征张量差异建模，促进异质信息交互与融合。

Result: 在VoD、TJ4DRadSet和Astyx数据集上，SDCM在精度、参数量和推理速度上都取得了最优结果，优于当前主流方法。

Conclusion: SDCM有效解决了4D雷达点云稀疏和视觉退化问题，实现了高效的雷达-视觉三维目标检测，具有较好的应用前景和实际价值。

Abstract: 3-D object detection based on 4-D radar-vision is an important part in Internet of Vehicles (IoV). However, there are two challenges which need to be faced. First, the 4-D radar point clouds are sparse, leading to poor 3-D representation. Second, vision datas exhibit representation degradation under low-light, long distance detection and dense occlusion scenes, which provides unreliable texture information during fusion stage. To address these issues, a framework named SDCM is proposed, which contains Simulated Densifying and Compensatory Modeling Fusion for radar-vision 3-D object detection in IoV. Firstly, considering point generation based on Gaussian simulation of key points obtained from 3-D Kernel Density Estimation (3-D KDE), and outline generation based on curvature simulation, Simulated Densifying (SimDen) module is designed to generate dense radar point clouds. Secondly, considering that radar data could provide more real time information than vision data, due to the all-weather property of 4-D radar. Radar Compensatory Mapping (RCM) module is designed to reduce the affects of vision datas' representation degradation. Thirdly, considering that feature tensor difference values contain the effective information of every modality, which could be extracted and modeled for heterogeneity reduction and modalities interaction, Mamba Modeling Interactive Fusion (MMIF) module is designed for reducing heterogeneous and achieving interactive Fusion. Experiment results on the VoD, TJ4DRadSet and Astyx HiRes 2019 dataset show that SDCM achieves best performance with lower parameter quantity and faster inference speed. Our code will be available.

</details>


### [24] [Investigating the Impact of Histopathological Foundation Models on Regressive Prediction of Homologous Recombination Deficiency](https://arxiv.org/abs/2602.00151)
*Alexander Blezinger,Wolfgang Nejdl,Ming Tang*

Main category: cs.CV

TL;DR: 本文系统评估了大规模预训练组织病理学基础模型在回归型生物标志物预测中的应用，特别是在HRD（同源重组缺陷）评分预测任务中，证实基础模型特征优于传统对比学习特征，并提出数据重采样策略，提升了弱势群体的预测能力。


<details>
  <summary>Details</summary>
Motivation: 尽管组织病理学基础模型已在多个任务取得成功，但其在持续型（回归型）生物标志物预测领域的效果尚未被系统研究。尤其是针对个体化癌症治疗关键指标HRD评分，基础模型的泛化性能和效果仍需验证。

Method: 使用五种最新的组织病理基础模型在多实例学习框架下，从WSI中提取Patch级特征，并与对比学习特征进行对比。在公开的乳腺癌、子宫内膜癌和肺癌队列中，基于这些特征训练模型预测连续的HRD评分。同时，提出分布式上采样策略缓解样本不均衡，并通过消融实验研究不同采样策略和包大小影响。

Result: 实验结果表明，基于基础模型特征训练的回归模型，在预测准确率和泛化能力上优于对比学习特征基线，并揭示了不同基础模型在特征表现上的系统性差异。所提出的分布式上采样方法显著提升了对临床重要但样本不足群体的召回率和均衡准确率。

Conclusion: 大规模组织病理预训练基础模型有助于提升生物标志物回归预测的准确性和可转移性，有望推动AI驱动精确肿瘤学的发展。

Abstract: Foundation models pretrained on large-scale histopathology data have found great success in various fields of computational pathology, but their impact on regressive biomarker prediction remains underexplored. In this work, we systematically evaluate histopathological foundation models for regression-based tasks, demonstrated through the prediction of homologous recombination deficiency (HRD) score - a critical biomarker for personalized cancer treatment. Within multiple instance learning frameworks, we extract patch-level features from whole slide images (WSI) using five state-of-the-art foundation models, and evaluate their impact compared to contrastive learning-based features. Models are trained to predict continuous HRD scores based on these extracted features across breast, endometrial, and lung cancer cohorts from two public medical data collections. Extensive experiments demonstrate that models trained on foundation model features consistently outperform the baseline in terms of predictive accuracy and generalization capabilities while exhibiting systematic differences among the foundation models. Additionally, we propose a distribution-based upsampling strategy to mitigate target imbalance in these datasets, significantly improving the recall and balanced accuracy for underrepresented but clinically important patient populations. Furthermore, we investigate the impact of different sampling strategies and instance bagsizes by ablation studies. Our results highlight the benefits of large-scale histopathological pretraining for more precise and transferable regressive biomarker prediction, showcasing its potential to advance AI-driven precision oncology.

</details>


### [25] [Real-Time Human Activity Recognition on Edge Microcontrollers: Dynamic Hierarchical Inference with Multi-Spectral Sensor Fusion](https://arxiv.org/abs/2602.00152)
*Boyu Li,Kuangji Zuo,Lincong Li,Yonghui Wu*

Main category: cs.CV

TL;DR: 提出了一种新型的分层并行伪图像增强融合网络（HPPI-Net），用于边缘设备上的高效人体活动识别，兼顾精度、计算资源与解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的边缘端模式识别方法难以在有限的计算和存储资源下实现高精度与实时性，尤其在人机交互、可穿戴和智能家居等对能耗及部署资源要求苛刻的应用场景下，迫切需要兼具高精度、低资源消耗和良好可解释性的解决方案。

Method: 提出HPPI-Net：分层设计，首层利用FFT谱图提取初步特征；第二层根据静态或动态活动，选择静止活动识别模块或并行LSTM-MobileNet（PLMN）。PLMN融合FFT、小波和Gabor谱图，通过三路LSTM编码并利用通道注意力（ECA）与深度可分卷积（DSC）精炼特征，实现可解释、低算力的人体活动识别。优化后在ARM Cortex-M4微控制器上部署。

Result: 在ARM Cortex-M4微控制器上，HPPI-Net在仅占用22.3KiB RAM与439.5KiB ROM的情况下，实现了96.70%的准确率，较MobileNetV3提升了1.22%的准确率，同时RAM和ROM开销分别降低71.2%和42.1%。

Conclusion: HPPI-Net兼具高准确率、低资源占用和可解释性，为可穿戴、工业和智能家居等受内存限制的边缘平台提供了实用、高效的人体活动识别解决方案。

Abstract: The demand for accurate on-device pattern recognition in edge applications is intensifying, yet existing approaches struggle to reconcile accuracy with computational constraints. To address this challenge, a resource-aware hierarchical network based on multi-spectral fusion and interpretable modules, namely the Hierarchical Parallel Pseudo-image Enhancement Fusion Network (HPPI-Net), is proposed for real-time, on-device Human Activity Recognition (HAR). Deployed on an ARM Cortex-M4 microcontroller for low-power real-time inference, HPPI-Net achieves 96.70% accuracy while utilizing only 22.3 KiB of RAM and 439.5 KiB of ROM after optimization. HPPI-Net employs a two-layer architecture. The first layer extracts preliminary features using Fast Fourier Transform (FFT) spectrograms, while the second layer selectively activates either a dedicated module for stationary activity recognition or a parallel LSTM-MobileNet network (PLMN) for dynamic states. PLMN fuses FFT, Wavelet, and Gabor spectrograms through three parallel LSTM encoders and refines the concatenated features using Efficient Channel Attention (ECA) and Depthwise Separable Convolution (DSC), thereby offering channel-level interpretability while substantially reducing multiply-accumulate operations. Compared with MobileNetV3, HPPI-Net improves accuracy by 1.22% and reduces RAM usage by 71.2% and ROM usage by 42.1%. These results demonstrate that HPPI-Net achieves a favorable accuracy-efficiency trade-off and provides explainable predictions, establishing a practical solution for wearable, industrial, and smart home HAR on memory-constrained edge platforms.

</details>


### [26] [See Without Decoding: Motion-Vector-Based Tracking in Compressed Video](https://arxiv.org/abs/2602.00153)
*Axel Duché,Clément Chatelain,Gilles Gasso*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级的压缩域目标跟踪模型，可直接在视频流的压缩数据（而非完全解码后的RGB视频）上运行，利用运动矢量和变换系数进行目标框的时序传播。在MOTS15/17/20数据集上，这一方法比传统RGB方法计算提速最高可达3.7倍，mAP@0.5仅下降4%。


<details>
  <summary>Details</summary>
Motivation: 当前监控等大规模系统对视频分析的实时性和计算效率有很高需求，而传统基于RGB视频的分析方法计算量大、效率低。为了减少计算开销，实现更高效的目标跟踪，需要研究如何直接利用压缩域数据进行视觉任务。

Method: 作者提出一种压缩域深度跟踪模型。该方法直接处理视频压缩数据中的运动矢量和变换系数，而无需解码完整RGB帧。模型根据压缩信息，在帧之间传播和调整目标检测框，实现高效跟踪。

Result: 在MOTS15/17/20数据集上，压缩域跟踪模型在推理速度上最高提升3.7倍，mAP@0.5指标只下降了4%。

Conclusion: 压缩域的运动建模为大规模实时视频分析提供了高效可行的方法，在监控等领域具有良好的应用前景。

Abstract: We propose a lightweight compressed-domain tracking model that operates directly on video streams, without requiring full RGB video decoding. Using motion vectors and transform coefficients from compressed data, our deep model propagates object bounding boxes across frames, achieving a computational speed-up of order up to 3.7 with only a slight 4% mAP@0.5 drop vs RGB baseline on MOTS15/17/20 datasets. These results highlight codec-domain motion modeling efficiency for real-time analytics in large monitoring systems.

</details>


### [27] [Deep Learning Pose Estimation for Multi-Label Recognition of Combined Hyperkinetic Movement Disorders](https://arxiv.org/abs/2602.00163)
*Laura Cif,Diane Demailly,Gabriella A. Horvàth,Juan Dario Ortigoza Escobar,Nathalie Dorison,Mayté Castro Jiménez,Cécile A. Hubsch,Thomas Wirth,Gun-Marie Hariz,Sophie Huby,Morgan Dornadic,Zohra Souei,Muhammad Mushhood Ur Rehman,Simone Hemm,Mehdi Boulayme,Eduardo M. Moraud,Jocelyne Bloch,Xavier Vasques*

Main category: cs.CV

TL;DR: 本论文提出了一种基于动作姿态与机器学习的新方法，用于从常规临床视频中客观区分多种高运动性障碍。


<details>
  <summary>Details</summary>
Motivation: 高运动性障碍（HMDs）如肌张力障碍、震颤、舞蹈症、肌阵挛和抽动症在各年龄层均有发生，由于表现形式多变且常常交织出现，临床识别及随访监测依赖主观评判，存在较大主观误差和一致性问题，亟需客观且可扩展的自动识别方法。

Method: 作者开发了一套基于视频动作姿态和机器学习的分析框架，将普通门诊视频转换为解剖关键点时间序列，并提取涵盖统计学、时序、频谱及高阶复杂性的不同行为运动学特征，用于进一步区分不同的高运动性障碍表型。

Result: 初步结果表明，该框架能够从常规视频中高效、准确地提取并量化这些复杂运动障碍的关键特征，有助于区分和识别不同类型的高运动性障碍。

Conclusion: 该方法为传统依赖主观判断的临床高运动性障碍评估提供了客观、自动化的新途径，具有良好的可扩展性与应用前景。

Abstract: Hyperkinetic movement disorders (HMDs) such as dystonia, tremor, chorea, myoclonus, and tics are disabling motor manifestations across childhood and adulthood. Their fluctuating, intermittent, and frequently co-occurring expressions hinder clinical recognition and longitudinal monitoring, which remain largely subjective and vulnerable to inter-rater variability. Objective and scalable methods to distinguish overlapping HMD phenotypes from routine clinical videos are still lacking. Here, we developed a pose-based machine-learning framework that converts standard outpatient videos into anatomically meaningful keypoint time series and computes kinematic descriptors spanning statistical, temporal, spectral, and higher-order irregularity-complexity features.

</details>


### [28] [YOLOE-26: Integrating YOLO26 with YOLOE for Real-Time Open-Vocabulary Instance Segmentation](https://arxiv.org/abs/2602.00168)
*Ranjan Sapkota,Manoj Karkee*

Main category: cs.CV

TL;DR: 提出YOLOE-26，一个结合YOLOv26高效架构与开放词汇学习的新型实时开放词汇实例分割框架，可在满足高效与确定性的同时具备开放类别识别与分割能力。


<details>
  <summary>Details</summary>
Motivation: 常规YOLO系列主要侧重于封闭集（固定类别）检测，难以适应复杂实际场景的开放类别需求，同时需兼顾推理效率和准确性。本文旨在打破YOLO的固定类别局限，实现对任意文本/视觉描述目标的实例分割，并保持实时性和部署友好。

Method: 提出YOLOE-26架构，融合YOLOv26无NMS端到端设计、PAN/FPN多尺度特征融合，并以对象嵌入替代固定类别logits，通过与文本、视觉或内置词汇的提示嵌入进行相似度匹配。引入RepRTA实现零开销文本提示，SAVPE实现示例驱动分割，以及Lazy Region Prompt Contrast支持免提示推理，三者统一于对象嵌入空间。利用大规模检测与锚定数据、多任务优化训练，并兼容Ultralytics生态全流程。

Result: 在多种模型规模下、提示与免提示场景中，YOLOE-26展现了一致的扩展性和优良的精度-效率权衡。实验显示其在各类开放词汇实例分割任务中均取得优异表现。

Conclusion: YOLOE-26是面向动态实际环境、具实用性与可扩展性的实时开放词汇实例分割解决方案，为目标检测分割领域提供了高效、灵活的新路径。

Abstract: This paper presents YOLOE-26, a unified framework that integrates the deployment-optimized YOLO26(or YOLOv26) architecture with the open-vocabulary learning paradigm of YOLOE for real-time open-vocabulary instance segmentation. Building on the NMS-free, end-to-end design of YOLOv26, the proposed approach preserves the hallmark efficiency and determinism of the YOLO family while extending its capabilities beyond closed-set recognition. YOLOE-26 employs a convolutional backbone with PAN/FPN-style multi-scale feature aggregation, followed by end-to-end regression and instance segmentation heads. A key architectural contribution is the replacement of fixed class logits with an object embedding head, which formulates classification as similarity matching against prompt embeddings derived from text descriptions, visual examples, or a built-in vocabulary. To enable efficient open-vocabulary reasoning, the framework incorporates Re-Parameterizable Region-Text Alignment (RepRTA) for zero-overhead text prompting, a Semantic-Activated Visual Prompt Encoder (SAVPE) for example-guided segmentation, and Lazy Region Prompt Contrast for prompt-free inference. All prompting modalities operate within a unified object embedding space, allowing seamless switching between text-prompted, visual-prompted, and fully autonomous segmentation. Extensive experiments demonstrate consistent scaling behavior and favorable accuracy-efficiency trade-offs across model sizes in both prompted and prompt-free settings. The training strategy leverages large-scale detection and grounding datasets with multi-task optimization and remains fully compatible with the Ultralytics ecosystem for training, validation, and deployment. Overall, YOLOE-26 provides a practical and scalable solution for real-time open-vocabulary instance segmentation in dynamic, real-world environments.

</details>


### [29] [Intra-Class Subdivision for Pixel Contrastive Learning: Application to Semi-supervised Cardiac Image Segmentation](https://arxiv.org/abs/2602.00174)
*Jiajun Zhao,Xuan Yang*

Main category: cs.CV

TL;DR: 本文提出了一种用于心脏图像分割的类内细分像素对比学习（SPCL）框架，通过新颖的“无关样本”概念和边界对比损失，有效提升了分割质量和边界精度。实验结果显示该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有心脏图像分割方法在类内边界附近的表征易被污染，影响了分割效果，亟需更有效的方式区分同一类别内边界与内部区域的像素。

Method: 作者提出SPCL框架，通过引入“无关样本”概念，将同类中内部与边界区域像素区分开，并设计新的边界对比损失，增强边界表示的判别能力。理论上分析了该方法组件的优势。

Result: 在公开心脏数据集上，SPCL显著提升了分割性能，无论在分割质量还是边界精度上均超过了现有主流方法。

Conclusion: SPCL框架有效缓解了边界区域表征污染问题，提高了心脏图像分割的准确性和鲁棒性，具有广阔的应用前景。

Abstract: We propose an intra-class subdivision pixel contrastive learning (SPCL) framework for cardiac image segmentation to address representation contamination at boundaries. The novel concept ``Unconcerned sample'' is proposed to distinguish pixel representations at the inner and boundary regions within the same class, facilitating a clearer characterization of intra-class variations. A novel boundary contrastive loss for boundary representations is proposed to enhance representation discrimination across boundaries. The advantages of the unconcerned sample and boundary contrastive loss are analyzed theoretically. Experimental results in public cardiac datasets demonstrate that SPCL significantly improves segmentation performance, outperforming existing methods with respect to segmentation quality and boundary precision. Our code is available at https://github.com/Jrstud203/SPCL.

</details>


### [30] [Stabilizing Diffusion Posterior Sampling by Noise--Frequency Continuation](https://arxiv.org/abs/2602.00176)
*Feng Tian,Yixuan Li,Weili Zeng,Weitian Zhang,Yichao Yan,Xiaokang Yang*

Main category: cs.CV

TL;DR: 提出了一种结合噪声与频率的连续采样策略，改进了扩散后验采样在图像逆问题中的表现，特别是在细节恢复及测量一致性方面，取得了在超分辨率、修补和去模糊任务上的新性能高点。


<details>
  <summary>Details</summary>
Motivation: 传统扩散后验采样在高噪声阶段指导不精确，细节恢复和测量一致性效果差，常出现高频伪影，对超参数与不适定算子敏感。亟需一种方法更有效地结合测量一致性与扩散过程，特别能区分对不同频段采用不同强度的指导。

Method: 提出噪声-频率连续采样框架，通过构建噪声相关频带的一系列中间后验分布，仅针对特定带宽内频率分量施加测量一致性约束。此外，采样器采用多分辨率一致性策略：对于可靠的低频信息积极纠正，而高频细节则仅在可辨识时谨慎修正。

Result: 在超分辨率、图像修补、去模糊等逆问题上，提出的方法实现了最优或领先的重建质量。特别是在运动去模糊任务上，相较于已有强基线，PSNR提升最高可达5dB。

Conclusion: 噪声-频率连续采样框架有效提升了扩散模型在图像逆问题中的实用性与重建细节恢复能力，显示出显著优于传统方法的性能，尤其在难以处理的高频恢复方面优势突出。

Abstract: Diffusion posterior sampling solves inverse problems by combining a pretrained diffusion prior with measurement-consistency guidance, but it often fails to recover fine details because measurement terms are applied in a manner that is weakly coupled to the diffusion noise level. At high noise, data-consistency gradients computed from inaccurate estimates can be geometrically incongruent with the posterior geometry, inducing early-step drift, spurious high-frequency artifacts, plus sensitivity to schedules and ill-conditioned operators. To address these concerns, we propose a noise--frequency Continuation framework that constructs a continuous family of intermediate posteriors whose likelihood enforces measurement consistency only within a noise-dependent frequency band. This principle is instantiated with a stabilized posterior sampler that combines a diffusion predictor, band-limited likelihood guidance, and a multi-resolution consistency strategy that aggressively commits reliable coarse corrections while conservatively adopting high-frequency details only when they become identifiable. Across super-resolution, inpainting, and deblurring, our method achieves state-of-the-art performance and improves motion deblurring PSNR by up to 5 dB over strong baselines.

</details>


### [31] [CamReasoner: Reinforcing Camera Movement Understanding via Structured Spatial Reasoning](https://arxiv.org/abs/2602.00181)
*Hang Wu,Yujun Cai,Zehao Li,Haonan Ge,Bowen Sun,Junsong Yuan,Yiwei Wang*

Main category: cs.CV

TL;DR: 提出了CamReasoner框架，通过结构化推理过程提升视频摄像机动态理解能力，克服了多模态模型对表面视觉特征依赖、忽视物理几何信息的问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型在理解摄像机动态时，过度依赖视觉表面特征，难以区分实际物理运动，缺乏对几何与推理过程的关注，导致模型常常混淆不同类型的摄像机运动。

Method: CamReasoner采用 OTA（Observation-Thinking-Answer，观察-思考-回答）结构性推理范式，将模型显式引导利用时空线索（如摄像机轨迹、视锥体），配合大规模轨迹推理数据及反馈样本训练，并首创性地引入强化学习调整推理链，实现物理运动逻辑与模型推理结果的对齐。

Result: 通过联合大规模推理数据和强化学习，本方法能显著抑制幻觉回答，并在多个摄像机运动理解相关的评测基准上取得了SOTA性能。

Conclusion: 结构化推理加物理几何约束与RL训练可以显著提升多模态模型在摄像机运动理解任务中的表现，推动了该领域从类黑盒分类模式向可解释物理推理的转型。

Abstract: Understanding camera dynamics is a fundamental pillar of video spatial intelligence. However, existing multimodal models predominantly treat this task as a black-box classification, often confusing physically distinct motions by relying on superficial visual patterns rather than geometric cues. We present CamReasoner, a framework that reformulates camera movement understanding as a structured inference process to bridge the gap between perception and cinematic logic. Our approach centers on the Observation-Thinking-Answer (O-T-A) paradigm, which compels the model to decode spatio-temporal cues such as trajectories and view frustums within an explicit reasoning block. To instill this capability, we construct a Large-scale Inference Trajectory Suite comprising 18k SFT reasoning chains and 38k RL feedback samples. Notably, we are the first to employ RL for logical alignment in this domain, ensuring motion inferences are grounded in physical geometry rather than contextual guesswork. By applying Reinforcement Learning to the Observation-Think-Answer (O-T-A) reasoning paradigm, CamReasoner effectively suppresses hallucinations and achieves state-of-the-art performance across multiple benchmarks.

</details>


### [32] [AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange](https://arxiv.org/abs/2602.00192)
*Elif Nebioglu,Emirhan Bilgiç,Adrian Popescu*

Main category: cs.CV

TL;DR: 本文发现，现有修复检测器主要依赖全局伪影而非局部修复内容，因此容易被绕过。作者提出了INP-X操作和新数据集，发现主流检测器检测效果大幅下降。理论分析指出原因是VAE重建引起的频谱变化。基于新数据训练可提升检测的泛化和定位能力。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习修复工具能生成极为逼真的局部图片改动，导致伪造检测面临巨大挑战。作者发现检测器可能没有真正检测修复内容本身，因此需要探究其机制和局限，推动更可靠的检测方法发展。

Method: 提出INP-X操作，将未编辑区域的像素恢复为原图，只保留局部修复生成区域，并据此制作了包含9万张图片的数据集。用INP-X干预各类主流（包括商用）检测器，分析检测准确率并进行理论解释。

Result: 主流检测器在通常准确率高达91%的情况下，对INP-X处理后的图片准确率骤降至55%，几乎等于随机。理论分析表明VAE重建会引起全局高频衰减，导致检测器依赖全局特征而非内容本身。

Conclusion: 依赖全局伪影的检测方式不足以应对高质量修复，必须提升检测器对局部修复内容的感知能力。作者公开的新数据集和实验方法为改进检测器提供了新思路和评测基准。

Abstract: Modern deep learning-based inpainting enables realistic local image manipulation, raising critical challenges for reliable detection. However, we observe that current detectors primarily rely on global artifacts that appear as inpainting side effects, rather than on locally synthesized content. We show that this behavior occurs because VAE-based reconstruction induces a subtle but pervasive spectral shift across the entire image, including unedited regions. To isolate this effect, we introduce Inpainting Exchange (INP-X), an operation that restores original pixels outside the edited region while preserving all synthesized content. We create a 90K test dataset including real, inpainted, and exchanged images to evaluate this phenomenon. Under this intervention, pretrained state-of-the-art detectors, including commercial ones, exhibit a dramatic drop in accuracy (e.g., from 91\% to 55\%), frequently approaching chance level. We provide a theoretical analysis linking this behavior to high-frequency attenuation caused by VAE information bottlenecks. Our findings highlight the need for content-aware detection. Indeed, training on our dataset yields better generalization and localization than standard inpainting. Our dataset and code are publicly available at https://github.com/emirhanbilgic/INP-X.

</details>


### [33] [Vision-Language Model Purified Semi-Supervised Semantic Segmentation for Remote Sensing Images](https://arxiv.org/abs/2602.00202)
*Shanwen Wang,Xin Sun,Danfeng Hong,Fei Zhou*

Main category: cs.CV

TL;DR: 提出了一种新颖的基于视觉-语言模型的半监督遥感图像语义分割模型SemiEarth，通过引入VLM-PP模块提升伪标签质量，实现了SOTA性能和良好可解释性。


<details>
  <summary>Details</summary>
Motivation: 遥感图像语义分割需要大量标注数据，人工标注代价高，半监督方法依赖伪标签，但伪标签质量普遍较差，尤其在复杂多类边界区域，影响学生网络学习效果。

Method: 提出SemiEarth模型，将视觉-语言模型（VLM）引入半监督框架，设计VLM-PP结构清洗教师网络生成的伪标签。VLM-PP直接纠正不自信/有歧义的伪标签，提升边界地区伪标签质量，引导学生模型更好学习。VLM-PP与具体架构无关，可拓展并带来开放域能力。

Result: 在多个遥感数据集上进行实验，SemiEarth在分割准确率等指标上达到SOTA水平。与以往SOTA方法相比，模型不仅性能优异还具有更好的可解释性。

Conclusion: SemiEarth通过VLM-PP极大提升了半监督语义分割伪标签的质量，尤其在边界和多类情形下表现突出，为遥感领域提供了更高效、更智能的分割方法，具有实际应用价值与可扩展性。

Abstract: The semi-supervised semantic segmentation (S4) can learn rich visual knowledge from low-cost unlabeled images. However, traditional S4 architectures all face the challenge of low-quality pseudo-labels, especially for the teacher-student framework.We propose a novel SemiEarth model that introduces vision-language models (VLMs) to address the S4 issues for the remote sensing (RS) domain. Specifically, we invent a VLM pseudo-label purifying (VLM-PP) structure to purify the teacher network's pseudo-labels, achieving substantial improvements. Especially in multi-class boundary regions of RS images, the VLM-PP module can significantly improve the quality of pseudo-labels generated by the teacher, thereby correctly guiding the student model's learning. Moreover, since VLM-PP equips VLMs with open-world capabilities and is independent of the S4 architecture, it can correct mispredicted categories in low-confidence pseudo-labels whenever a discrepancy arises between its prediction and the pseudo-label. We conducted extensive experiments on multiple RS datasets, which demonstrate that our SemiEarth achieves SOTA performance. More importantly, unlike previous SOTA RS S4 methods, our model not only achieves excellent performance but also offers good interpretability. The code is released at https://github.com/wangshanwen001/SemiEarth.

</details>


### [34] [Interpretable Unsupervised Deformable Image Registration via Confidence-bound Multi-Hop Visual Reasoning](https://arxiv.org/abs/2602.00211)
*Zafar Iqbal,Anwar Ul Haq,Srimannarayana Grandhi*

Main category: cs.CV

TL;DR: 本文提出了一种全新的无监督可变形图像配准方法——多跳视觉推理链（VCoR）框架，兼顾了高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有无监督医学图像配准方法尽管准确但缺乏可解释性，导致误差积累和临床信任度降低。作者希望以更具透明度和可解释性的方式提升配准可靠性。

Method: 作者将配准任务重塑为一个多步推理过程。VCoR框架在每一步（hop）中结合了“局部空间细化（LSR）”模块和“交叉参考注意力（CRA）”机制。通过多跳策略，增强特征表达能力、持续优化变形场，并生成每一步的中间预测，便于追踪和解释配准过程。还引入了不确定性估计，通过各步结果的稳定性与收敛性，量化预测信心。

Result: 在DIR-Lab 4D CT（肺）和IXI T1脑MRI两个公开数据集上，VCoR不仅取得了有竞争力的配准准确率，还能输出丰富的中间可视化结果和置信度指标。

Conclusion: VCoR框架为无监督医学图像配准提供了高精度、可解释和可量化的不确定性信息，增强了方法的临床实用性和信赖度。

Abstract: Unsupervised deformable image registration requires aligning complex anatomical structures without reference labels, making interpretability and reliability critical. Existing deep learning methods achieve considerable accuracy but often lack transparency, leading to error drift and reduced clinical trust. We propose a novel Multi-Hop Visual Chain of Reasoning (VCoR) framework that reformulates registration as a progressive reasoning process. Inspired by the iterative nature of clinical decision-making, each visual reasoning hop integrates a Localized Spatial Refinement (LSR) module to enrich feature representations and a Cross-Reference Attention (CRA) mechanism that leads the iterative refinement process, preserving anatomical consistency. This multi-hop strategy enables robust handling of large deformations and produces a transparent sequence of intermediate predictions with a theoretical bound. Beyond accuracy, our framework offers built-in interpretability by estimating uncertainty via the stability and convergence of deformation fields across hops. Extensive evaluations on two challenging public datasets, DIR-Lab 4D CT (lung) and IXI T1-weighted MRI (brain), demonstrate that VCoR achieves competitive registration accuracy while offering rich intermediate visualizations and confidence measures. By embedding an implicit visual reasoning paradigm, we present an interpretable, reliable, and clinically viable unsupervised medical image registration.

</details>


### [35] [Deep Learning Based CNN Model for Automated Detection of Pneumonia from Chest XRay Images](https://arxiv.org/abs/2602.00212)
*Sathish Krishna Anumula,Vetrivelan Tamilmani,Aniruddha Arjun Singh,Dinesh Rajendran,Venkata Deepak Namburi*

Main category: cs.CV

TL;DR: 本论文提出了一种定制的卷积神经网络，用于自动识别胸部X光片中的肺炎，实现高精度和低计算开销的诊断。


<details>
  <summary>Details</summary>
Motivation: 肺炎在全球致病和致死率较高，尤其在儿童和老年人群体中更为明显，资源有限地区尤甚。传统胸片人工诊断方法受到医生疲劳、专家不足和主观差异影响，导致诊断效率和准确率受限。因此需要开发准确、快速且自动化的方法。

Method: 设计了一种专门优化于灰度医学图像纹理特征的深度可分离卷积结构的自定义CNN架构。采用CLAHE和几何增强作为预处理，减缓类别不平衡，提高系统泛化能力。与通用迁移学习模型相比，此方法参数更加紧凑。

Result: 在5863张正位胸部X光图像数据集上测试该系统，表明其能以较高的精度和较低的计算资源要求，自动识别肺炎。

Conclusion: 所提出的卷积神经网络诊断系统有效提升了肺炎的诊断效率和准确性，在低计算资源需求的同时具有较强泛化能力，为医疗资源有限地区的肺炎筛查提供了切实可行的方法。

Abstract: Pneumonia has been one of the major causes of morbidities and mortality in the world and the prevalence of this disease is disproportionately high among the pediatric and elderly populations especially in resources trained areas Fast and precise diagnosis is a prerequisite for successful clinical intervention but due to inter observer variation fatigue among experts and a shortage of qualified radiologists traditional approaches that rely on manual interpretation of chest radiographs are frequently constrained To address these problems this paper introduces a unified automated diagnostic model using a custom Convolutional Neural Network CNN that can recognize pneumonia in chest Xray images with high precision and at minimal computational expense In contrast like other generic transfer learning based models which often possess redundant parameters the offered architecture uses a tailor made depth wise separable convolutional design which is optimized towards textural characteristics of grayscale medical images Contrast Limited Adaptive Histogram Equalization CLAHE and geometric augmentation are two significant preprocessing techniques used to ensure that the system does not experience class imbalance and is more likely to generalize The system is tested using a dataset of 5863 anterior posterior chest Xrays.

</details>


### [36] [A Geometric Multimodal Foundation Model Integrating Bp-MRI and Clinical Reports in Prostate Cancer Classification](https://arxiv.org/abs/2602.00214)
*Juan A. Olmos,Antoine Manzanera,Fabio Martínez*

Main category: cs.CV

TL;DR: 本文提出了一种几何多模态基础模型MFM-Geom，将bp-MRI影像与临床报告联合建模，实现更准确的前列腺癌识别，且在小样本上表现优越。


<details>
  <summary>Details</summary>
Motivation: 当前前列腺癌的诊断依赖专家主观判读，同时主流计算机辅助诊断方法忽视了临床变量，仅依赖影像信息，并且受限于数据稀缺，难以学习到鲁棒表达，因此亟需结合多模态数据构建更高效的诊断模型。

Method: 作者提出了MFM-Geom，将bp-MRI影像与临床报告信息共同编码，并在分类头利用对称正定(SPD)矩阵与黎曼深度学习方法，实现影像与文本表征的融合和分类。

Result: 仅用10%的训练数据，MFM-Geom在分类任务中AUC-PR达90.67，优于基线模型8.3%，并在外部数据集上的泛化性表现良好（AUC-PR 90.6）。

Conclusion: MFM-Geom有效结合了影像与临床信息，在前列腺癌诊断中表现出高鲁棒性和准确性，为实际临床诊断提供了有力工具。

Abstract: Prostate cancer (PCa) is one of the most common cancers in men worldwide. Bi-parametric MRI (bp-MRI) and clinical variables are crucial for PCa identification and improving treatment decisions. However, this process is subjective to expert interpretations. Furthermore, most existing computer-aided diagnosis methods focus on imaging-based models, overlooking the clinical context and suffering from data scarcity, limiting their ability to learn robust representations. We propose a geometric multimodal Foundation Model (FM), named MFM-Geom, that learns representations from bp-MRI and clinical reports, encoding visual findings and information from the context of clinical variables. In the representations classification head, the approach leverages symmetric positive definite (SPD) matrices and Riemannian deep learning to integrate imaging-text representations from a biomedical multimodal FM. Using 10% of the training data, MFM-Geom outperformed baseline class token embedding-based classification (+8.3%, AUC-PR of 90.67). Generalization on external dataset confirmed the robustness of fine-tuning biomedical FM, achieving an AUC-PR of 90.6.

</details>


### [37] [Development of a Cacao Disease Identification and Management App Using Deep Learning](https://arxiv.org/abs/2602.00216)
*Zaldy Pagaduan,Jason Occidental,Nathaniel Duro,Dexielito Badilles,Eleonor Palconit*

Main category: cs.CV

TL;DR: 本文开发了一款适用于菲律宾小农户的离线可用移动应用，利用深度学习模型实现对可可主要病害的识别与管理，帮助农户提升田间诊断能力和生产力。


<details>
  <summary>Details</summary>
Motivation: 菲律宾小农户普遍缺乏先进农技与数据支持，农业病害和虫害管理受限，制约了可可产量和品质提升。当前缺少面向小农户的便捷、实用技术工具。

Method: 使用深度学习技术训练可可病害识别模型，并将其整合到可离线运行的手机应用中，模型通过农田实地采集图片和专家标注数据进行训练和验证。

Result: 病害识别模型验证精度达96.93%，黑荚病感染等级检测模型精度为79.49%；现场应用与专家评估一致率为84.2%。

Conclusion: 基于深度学习的离线可用手机应用能有效提升小农户的病害诊断与管理能力，为推动可可产业智能化、科学化提供了实际可行的解决方案。

Abstract: Smallholder cacao producers often rely on outdated farming techniques and face significant challenges from pests and diseases, unlike larger plantations with more resources and expertise. In the Philippines, cacao farmers have limited access to data, information, and good agricultural practices. This study addresses these issues by developing a mobile application for cacao disease identification and management that functions offline, enabling use in remote areas where farms are mostly located. The core of the system is a deep learning model trained to identify cacao diseases accurately. The trained model is integrated into the mobile app to support farmers in field diagnosis. The disease identification model achieved a validation accuracy of 96.93% while the model for detecting cacao black pod infection levels achieved 79.49% validation accuracy. Field testing of the application showed an agreement rate of 84.2% compared with expert cacao technician assessments. This approach empowers smallholder farmers by providing accessible, technology-enabled tools to improve cacao crop health and productivity.

</details>


### [38] [CAPA: Contribution-Aware Pruning and FFN Approximation for Efficient Large Vision-Language Models](https://arxiv.org/abs/2602.00247)
*Samyak Jha,Junho Kim*

Main category: cs.CV

TL;DR: 本论文提出了一种提升大规模视觉-语言模型推理效率的方法，主要通过对视觉Token的选择性剪枝和前馈网络高效近似，显著减低计算负担，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 大模型视觉输入的Token数量庞大，使推理计算成本高昂。目前缺乏有效辨识哪些视觉Token和相关计算可被安全移除的机制。既有方法常通过注意力分数评估Token重要性，但其与实际贡献并不完全吻合。

Method: 本文提出Attention Contribution指标，将注意力概率与value向量模长相结合，更准确地评估视觉Token贡献，并据此区分可安全移除的Probability Dumps和必须保留的Structural Anchors。另外，实验证明FFN存在冗余，尤其在图像Token呈现线性行为的中间层。基于上述分析，作者提出CAPA框架：在关键功能转变处用Attention Contribution剪枝视觉Token，同时以线性近似高效减少FFN计算。

Result: 实验证明CAPA策略在多个基准数据集及模型上可以在牺牲极小性能的情况下，大幅提升推理效率且具有更好的鲁棒性。

Conclusion: 通过引入更准确的视觉Token评估方法和针对FFN的高效近似手段，CAPA实现了高效能与高性能的折中，对于大规模视觉-语言模型的应用具有广泛实际价值。

Abstract: Efficient inference in Large Vision-Language Models is constrained by the high cost of processing thousands of visual tokens, yet it remains unclear which tokens and computations can be safely removed. While attention scores are commonly used to estimate visual token importance, they are an imperfect proxy for actual contribution. We show that Attention Contribution, which weights attention probabilities by value vector magnitude, provides a more accurate criterion for visual token selection. Our empirical analysis reveals that visual attention sinks are functionally heterogeneous, comprising Probability Dumps with low contribution that can be safely pruned, and Structural Anchors with high contribution essential for maintaining model performance. Further, we identify substantial redundancy in Feed-Forward Networks (FFNs) associated with visual tokens, particularly in intermediate layers where image tokens exhibit linear behavior. Based on our findings, we introduce CAPA (Contribution-Aware Pruning and FFN Approximation), a dual-strategy framework that prunes visual tokens using attention contribution at critical functional transitions and reduces FFN computation through efficient linear approximations. Experiments on various benchmarks across baselines show that CAPA achieves competent efficiency--performance trade-offs with improved robustness.

</details>


### [39] [SANEval: Open-Vocabulary Compositional Benchmarks with Failure-mode Diagnosis](https://arxiv.org/abs/2602.00249)
*Rishav Pramanik,Ian E. Nielsen,Jeff Smith,Saurav Pandit,Ravi P. Ramachandran,Zhaozheng Yin*

Main category: cs.CV

TL;DR: 本文提出了SANEval基准体系，一种用于开放词汇、多属性、多物体及空间关系文本生成图片模型的综合评测工具。SANEval结合大语言模型和开放词汇目标检测来进行更精细和可解释的自动化评测，克服了现有基准的局限。


<details>
  <summary>Details</summary>
Motivation: 当前文本生成图片（T2I）模型在面对包含多个物体、属性和空间关系的复杂提示词时仍表现不佳。其主要瓶颈之一在于缺少合适的评测手段，现有方法多为封闭词汇，诊断能力不足，且反馈难以直观解释。因此，亟需更优质、更开放、更具解释性的评测方法以推动领域进步。

Method: 作者提出SANEval评测基准：结合大语言模型对文本提示进行深度理解，再结合由LLM增强的开放词汇目标检测模型，对生成图像中的物体、属性、数量和空间关系进行自动化评测。该方法摒弃固定词表，实现大规模开放式、细粒度和可解释的综合评测。

Result: 在六个主流T2I模型上进行大量实验，结果显示SANEval的自动评测指标，与人工评价的相关性（Spearman相关系数）高于现有基准，并且在属性绑定、空间关系和数量理解任务上取得了统计上显著不同的结果。

Conclusion: SANEval显著提升了T2I模型的复杂组合能力评测水平，能够为模型研发提供更可信、更精细的诊断和反馈。作者还将公开数据集与评测管线，促进后续复杂文本-图片生成与评测研究。

Abstract: The rapid progress of text-to-image (T2I) models has unlocked unprecedented creative potential, yet their ability to faithfully render complex prompts involving multiple objects, attributes, and spatial relationships remains a significant bottleneck. Progress is hampered by a lack of adequate evaluation methods; current benchmarks are often restricted to closed-set vocabularies, lack fine-grained diagnostic capabilities, and fail to provide the interpretable feedback necessary to diagnose and remedy specific compositional failures. We solve these challenges by introducing SANEval (Spatial, Attribute, and Numeracy Evaluation), a comprehensive benchmark that establishes a scalable new pipeline for open-vocabulary compositional evaluation. SANEval combines a large language model (LLM) for deep prompt understanding with an LLM-enhanced, open-vocabulary object detector to robustly evaluate compositional adherence, unconstrained by a fixed vocabulary. Through extensive experiments on six state-of-the-art T2I models, we demonstrate that SANEval's automated evaluations provide a more faithful proxy for human assessment; our metric achieves a Spearman's rank correlation with statistically different results than those of existing benchmarks across tasks of attribute binding, spatial relations, and numeracy. To facilitate future research in compositional T2I generation and evaluation, we will release the SANEval dataset and our open-source evaluation pipeline.

</details>


### [40] [Subspace Clustering on Incomplete Data with Self-Supervised Contrastive Learning](https://arxiv.org/abs/2602.00262)
*Huanran Li,Daniel Pimentel-Alarcón*

Main category: cs.CV

TL;DR: 本文提出了一种针对缺失数据的对比自监督子空间聚类方法（CSC），通过生成部分观测数据的掩码视图并结合对比学习，有效提升聚类的鲁棒性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有子空间聚类方法普遍假设数据完整，导致在实际缺失数据环境下效果不佳。为解决这一现实问题，作者旨在提升算法应对缺失值的能力。

Method: 提出了Contrastive Subspace Clustering（CSC）框架，针对不完整数据生成不同遮罩视角，利用SimCLR风格的对比损失训练深度神经网络获得不变嵌入表示，之后采用稀疏子空间聚类法对嵌入进行聚类。

Result: 在六个基准数据集上的实验结果表明，CSC在聚类准确性和鲁棒性方面均优于传统方法及现有深度学习基线，并具备良好的扩展性。

Conclusion: CSC方法显著提升了子空间聚类在缺失数据环境下的性能，为处理实际应用中的不完整数据问题提供了一种有效可行的方案。

Abstract: Subspace clustering aims to group data points that lie in a union of low-dimensional subspaces and finds wide application in computer vision, hyperspectral imaging, and recommendation systems. However, most existing methods assume fully observed data, limiting their effectiveness in real-world scenarios with missing entries. In this paper, we propose a contrastive self-supervised framework, Contrastive Subspace Clustering (CSC), designed for clustering incomplete data. CSC generates masked views of partially observed inputs and trains a deep neural network using a SimCLR-style contrastive loss to learn invariant embeddings. These embeddings are then clustered using sparse subspace clustering. Experiments on six benchmark datasets show that CSC consistently outperforms both classical and deep learning baselines, demonstrating strong robustness to missing data and scalability to large datasets.

</details>


### [41] [World-Shaper: A Unified Framework for 360° Panoramic Editing](https://arxiv.org/abs/2602.00265)
*Dong Liang,Yuhao Liu,Jinyuan Jia,Youjun Zhao,Rynson W. H. Lau*

Main category: cs.CV

TL;DR: 本文提出一种在等距投影（ERP）域下进行全景图像编辑的新方法World-Shaper，显著提升了360°全景编辑的一致性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有关于全景图像编辑的方法多基于透视图，难以处理全景图的空间结构；而立方体投影虽然尝试解决这一问题，但由于几何不匹配，带来全局一致性的问题。因此，亟需一种能够直接在全景的原生表达上进行编辑且保持几何一致性的框架。

Method: 本文在等距投影域提出World-Shaper框架，核心包括：1）提出generate-then-edit范式，通过可控的全景生成阶段合成多样的编辑数据，助力监督式学习；2）提出基于几何的学习策略，采用位置感知的形状监督及逐步训练过程，显式和隐式结合强化全景结构建模。

Result: 在自建全景编辑基准（PEBench）上，World-Shaper在几何一致性、编辑保真度和文本可控性等方面均优于当前最新方法（SOTA）。

Conclusion: World-Shaper实现了在单一框架下高一致性、灵活可控的360°全景图像生成与编辑，对虚拟现实等领域的全景内容创作具有重要意义。

Abstract: Being able to edit panoramic images is crucial for creating realistic 360° visual experiences. However, existing perspective-based image editing methods fail to model the spatial structure of panoramas. Conventional cube-map decompositions attempt to overcome this problem but inevitably break global consistency due to their mismatch with spherical geometry. Motivated by this insight, we reformulate panoramic editing directly in the equirectangular projection (ERP) domain and present World-Shaper, a unified geometry-aware framework that bridges panoramic generation and editing within a single editing-centric design. To overcome the scarcity of paired data, we adopt a generate-then-edit paradigm, where controllable panoramic generation serves as an auxiliary stage to synthesize diverse paired examples for supervised editing learning. To address geometric distortion, we introduce a geometry-aware learning strategy that explicitly enforces position-aware shape supervision and implicitly internalizes panoramic priors through progressive training. Extensive experiments on our new benchmark, PEBench, demonstrate that our method achieves superior geometric consistency, editing fidelity, and text controllability compared to SOTA methods, enabling coherent and flexible 360° visual world creation with unified editing control. Code, model, and data will be released at our project page: https://world-shaper-project.github.io/

</details>


### [42] [PLACID: Identity-Preserving Multi-Object Compositing via Video Diffusion with Synthetic Trajectories](https://arxiv.org/abs/2602.00267)
*Gemma Canet Tarrés,Manel Baradad,Francesc Moreno-Noguer,Yumeng Li*

Main category: cs.CV

TL;DR: 本文提出了PLACID方法，有效解决了当前生成式AI在多物体合成时存在的主体错漏、布局混乱等问题，显著提升了物体身份、背景和色彩的保真度。


<details>
  <summary>Details</summary>
Motivation: 现有高质量图片生成AI在处理包含多个物体的专业级合成任务时，常常出现物体细节丢失、漏列、重复和布局比例不当等关键问题，无法满足专业设计的需求，因此需要新的方法提升合成质量和控制力。

Method: 提出PLACID框架：1）利用预训练图像到视频扩散模型（I2V）并结合文本控制，通过视频时间先验增强合成中多物体的身份一致性、细节与背景还原；2）提出全新数据整理方式，合成序列样本使物体从随机位置平滑移动到目标布局，提升模型时间感知能力。推理阶段通过文本引导，实现随机分布物体自动生成合理布局的合成图。

Result: 丰富的定量实验和用户调研表明，PLACID方法在多物体合成的物体身份、背景色彩还原，以及物体完整性与美观性方面，全面优于现有最佳方法。

Conclusion: PLACID显著改善了多物体图像合成在身份保留、布局控制和整体美观度上的表现，是实现高保真、大规模产品展示或设计合成的有力工具。

Abstract: Recent advances in generative AI have dramatically improved photorealistic image synthesis, yet they fall short for studio-level multi-object compositing. This task demands simultaneous (i) near-perfect preservation of each item's identity, (ii) precise background and color fidelity, (iii) layout and design elements control, and (iv) complete, appealing displays showcasing all objects. However, current state-of-the-art models often alter object details, omit or duplicate objects, and produce layouts with incorrect relative sizing or inconsistent item presentations. To bridge this gap, we introduce PLACID, a framework that transforms a collection of object images into an appealing multi-object composite. Our approach makes two main contributions. First, we leverage a pretrained image-to-video (I2V) diffusion model with text control to preserve objects consistency, identities, and background details by exploiting temporal priors from videos. Second, we propose a novel data curation strategy that generates synthetic sequences where randomly placed objects smoothly move to their target positions. This synthetic data aligns with the video model's temporal priors during training. At inference, objects initialized at random positions consistently converge into coherent layouts guided by text, with the final frame serving as the composite image. Extensive quantitative evaluations and user studies demonstrate that PLACID surpasses state-of-the-art methods in multi-object compositing, achieving superior identity, background, and color preservation, with less omitted objects and visually appealing results.

</details>


### [43] [TokenTrim: Inference-Time Token Pruning for Autoregressive Long Video Generation](https://arxiv.org/abs/2602.00268)
*Ariel Shaulov,Eitan Shaar,Amit Edenzon,Lior Wolf*

Main category: cs.CV

TL;DR: 本论文提出了一种在生成长视频时缓解自回归模型时间漂移（Temporal Drift）的方法。其核心是在推理时识别并移除不稳定的潜在token，以防止错误积累，从而提升生成序列的时序一致性。


<details>
  <summary>Details</summary>
Motivation: 自回归视频生成方法在迭代生成长视频时容易出现时间漂移问题，即误差随时间累积并放大。现有方法多未有效解决错误传播导致的时序一致性下降，亟需一种新颖且高效的缓解机制。

Method: 作者提出，时间漂移主要源自推理阶段不受控地重复使用受损的潜在token。为此，论文在推理时检测并移除表现出向前一帧显著偏差的不稳定潜在token，从而避免已受损的信息进入后续生成步骤。该方法无需修改模型结构、训练过程或离开潜空间。

Result: 所提方法显著提升了自回归视频生成模型在长时序生成任务中的时序一致性，有效减少了时间漂移现象。实验证明，即使在不改变模型和训练流程的前提下，也能带来可观的性能提升。

Conclusion: 通过简单的推理时token筛除机制，能显著缓解自回归视频生成的时间漂移问题，为长序列生成提供了有效解决思路，同时具备低侵入性和实用性。

Abstract: Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift, where errors accumulate and amplify over long horizons. We hypothesize that this drift does not primarily stem from insufficient model capacity, but rather from inference-time error propagation. Specifically, we contend that drift arises from the uncontrolled reuse of corrupted latent conditioning tokens during auto-regressive inference. To correct this accumulation of errors, we propose a simple, inference-time method that mitigates temporal drift by identifying and removing unstable latent tokens before they are reused for conditioning. For this purpose, we define unstable tokens as latent tokens whose representations deviate significantly from those of the previously generated batch, indicating potential corruption or semantic drift. By explicitly removing corrupted latent tokens from the auto-regressive context, rather than modifying entire spatial regions or model parameters, our method prevents unreliable latent information from influencing future generation steps. As a result, it significantly improves long-horizon temporal consistency without modifying the model architecture, training procedure, or leaving latent space.

</details>


### [44] [TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs](https://arxiv.org/abs/2602.00288)
*Baiqi Li,Kangyi Zhao,Ce Zhang,Chancharik Mitra,Jean de Dieu Nyandwi,Gedas Bertasius*

Main category: cs.CV

TL;DR: 论文提出了TimeBlind基准，用于诊断多模态大模型在视频时空理解中的短板，尤其是对时序动态推理能力的不足。实验证实当前最先进模型在时序理解任务上远低于人类表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在静态内容理解上表现优异，但对动态视频中的时序关系理解薄弱。缺乏能精细区分模型时空推理能力的专业评测基准。

Method: 提出TimeBlind基准：以视频最小对（内容静态视觉信息一致，仅时序结构不同）和互补式问题消除语言偏见。时间理解分为三层次：事件辨识、事件属性描述、事件间关系推理。评估了20多种前沿多模态模型共600组（2400对）视频问题对。

Result: 最佳多模态模型在该基准上的区分准确率仅为48.2%，远低于人类的98.2%。显示这些模型更依赖于静态视觉线索，而非真正的时序逻辑。

Conclusion: 当前多模态大模型在时空理解上仍有巨大短板，TimeBlind成为推动下代视频理解和模型诊断的重要工具。

Abstract: Fine-grained spatio-temporal understanding is essential for video reasoning and embodied AI. Yet, while Multimodal Large Language Models (MLLMs) master static semantics, their grasp of temporal dynamics remains brittle. We present TimeBlind, a diagnostic benchmark for compositional spatio-temporal understanding. Inspired by cognitive science, TimeBlind categorizes fine-grained temporal understanding into three levels: recognizing atomic events, characterizing event properties, and reasoning about event interdependencies. Unlike benchmarks that conflate recognition with temporal reasoning, TimeBlind leverages a minimal-pairs paradigm: video pairs share identical static visual content but differ solely in temporal structure, utilizing complementary questions to neutralize language priors. Evaluating over 20 state-of-the-art MLLMs (e.g., GPT-5, Gemini 3 Pro) on 600 curated instances (2400 video-question pairs), reveals that the Instance Accuracy (correctly distinguishing both videos in a pair) of the best performing MLLM is only 48.2%, far below the human performance (98.2%). These results demonstrate that even frontier models rely heavily on static visual shortcuts rather than genuine temporal logic, positioning TimeBlind as a vital diagnostic tool for next-generation video understanding. Dataset and code are available at https://baiqi-li.github.io/timeblind_project/ .

</details>


### [45] [Computer Vision and Its Relationship to Cognitive Science: A perspective from Bayes Decision Theory](https://arxiv.org/abs/2602.00289)
*Alan Yuille,Daniel Kersten*

Main category: cs.CV

TL;DR: 本文用贝叶斯决策理论（BDT）为理论视角，简要介绍了计算机视觉领域，重点比较贝叶斯方法与深度神经网络方法，并讨论了两者结合的前景。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉已成为庞大而复杂的领域，其中不同理论与方法各具优势和局限。通过贝叶斯决策理论这个统一框架，可以厘清贝叶斯观点和深度神经网络在计算机视觉及其与认知科学关系中的作用，推进理论发展和方法整合。

Method: 本文采用理论分析和概念比较的方法，分别介绍了贝叶斯方法（具有认知科学共鸣的理论吸引力）和深度神经网络方法（基于视觉通路结构且已取得产业级成功），并在贝叶斯决策理论框架下评述二者优劣、相关性及互补性。

Result: 分析显示，贝叶斯方法与深度神经网络各有长短，贝叶斯决策理论能够统一描述二者，揭示它们之间的联系。同时，通过分析BDT本身的局限性，探讨了两类方法结合的新方向。

Conclusion: 贝叶斯决策理论为理解和整合计算机视觉中的主流方法提供了强有力的理论工具，并指出了未来结合贝叶斯和深度学习以构建更丰富计算框架的研究路径。

Abstract: This document presents an introduction to computer vision, and its relationship to Cognitive Science, from the perspective of Bayes Decision Theory (Berger 1985). Computer vision is a vast and complex field, so this overview has a narrow scope and provides a theoretical lens which captures many key concepts. BDT is rich enough to include two different approaches: (i) the Bayesian viewpoint, which gives a conceptually attractive framework for vision with concepts that resonate with Cognitive Science (Griffiths et al., 2024), and (ii) the Deep Neural Network approach whose successes in the real world have made Computer Vision into a trillion-dollar industry and which is motivated by the hierarchical structure of the visual ventral stream. The BDT framework relates and captures the strengths and weakness of these two approaches and, by discussing the limitations of BDT, points the way to how they can be combined in a richer framework.

</details>


### [46] [LogicGaze: Benchmarking Causal Consistency in Visual Narratives via Counterfactual Verification](https://arxiv.org/abs/2602.00292)
*Rory Driscoll,Alexandros Christoforos,Chadbourne Davis*

Main category: cs.CV

TL;DR: 本文提出了LogicGaze基准，用于评测视觉-语言模型（VLMs）在视觉证据基础上的推理链可靠性，揭示了VLM在多模态推理中的易错性。


<details>
  <summary>Details</summary>
Motivation: 现有VLM虽然能够进行多步推理，但其推理是否真正基于视觉证据，而非语言幻想，仍缺乏系统性评估。为提升VLM在复杂任务中的可信度，有必要开发专门工具检验模型“落地”推理能力。

Method: 作者构建了LogicGaze基准，从ShareGPT4Video（4万段视频）与Flickr30k图像集采样，设计带有视觉矛盾但语言表面合理的因果链扰动，要求模型一一验证推理链每一步是否与真实视觉输入相符。评价分为三步：因果链验证、叙事合成及扰动拒绝。

Result: 测试表明，当前主流VLM（如Qwen2.5-VL-72B）在验证推理链、拒绝视觉矛盾扰动等方面表现不佳，易出现“幻觉”错误。

Conclusion: LogicGaze揭示并量化了VLM在多模态推理落地中的重大挑战，推动未来VLM朝向更健壮、可信赖的方向发展。相关资源已公开，促进行业研究。

Abstract: While sequential reasoning enhances the capability of Vision-Language Models (VLMs) to execute complex multimodal tasks, their reliability in grounding these reasoning chains within actual visual evidence remains insufficiently explored. We introduce LogicGaze, a novel benchmark framework designed to rigorously interrogate whether VLMs can validate sequential causal chains against visual inputs, specifically targeting the pervasive issue of hallucination. Curated from 40,000 video segments from ShareGPT4Video and a subset of Flickr30k imagery, LogicGaze integrates causal sequences with visually contradictory yet linguistically plausible perturbations, compelling models to verify the authenticity of each reasoning step. Our tripartite evaluation protocol - Causal Validation, Grounded Narrative Synthesis, and Perturbation Rejection - exposes significant vulnerabilities in state-of-the-art VLMs such as Qwen2.5-VL-72B. LogicGaze advocates for robust, trustworthy multimodal reasoning, with all resources publicly available in an anonymized repository.

</details>


### [47] [Opportunistic Promptable Segmentation: Leveraging Routine Radiological Annotations to Guide 3D CT Lesion Segmentation](https://arxiv.org/abs/2602.00309)
*Samuel Church,Joshua D. Warner,Danyal Maqbool,Xin Tie,Junjie Hu,Meghan G. Lubner,Tyler J. Bradshaw*

Main category: cs.CV

TL;DR: 本文提出了SAM2CT模型，通过利用PACS系统中已有的箭头和线段等稀疏注释，实现CT影像上的3D自动分割，扩展分割数据获取的方式。


<details>
  <summary>Details</summary>
Motivation: 高质量3D分割数据对于构建CT影像机器学习模型至关重要，但手动分割工作量大、成本高，而现实临床工作中已存在大量简单的注释信息（如箭头、线段）被系统存储，未被充分利用。作者希望通过新方法高效转化已有稀疏注释为3D分割标签，以大幅提升训练样本量。

Method: 提出了SAM2CT模型，基于已有的SAM2分割框架，扩展了提示编码器以支持箭头和线段输入，同时设计了适用于3D医学影像的记忆编码策略（MCM）。该方法能将PACS中的GSPS（稀疏注释）结合CT体积数据，提示模型生成3D分割。并在公开数据集和临床实际数据上验证了性能。

Result: 在公开病灶分割测试中，SAM2CT用箭头和线段提示分别取得0.649和0.757的Dice系数，优于传统模型。应用在60例临床实际GSPS注释上，87%的自动分割结果被放射科医生认为可接受或仅需小调整，并展示了部分急诊场景的零样本迁移能力。

Conclusion: SAM2CT通过有效利用历史稀疏注释，实现了大规模3D CT分割标签自动构建的可能，对提升医学影像AI训练数据获取的效率与规模有重要意义。

Abstract: The development of machine learning models for CT imaging depends on the availability of large, high-quality, and diverse annotated datasets. Although large volumes of CT images and reports are readily available in clinical picture archiving and communication systems (PACS), 3D segmentations of critical findings are costly to obtain, typically requiring extensive manual annotation by radiologists. On the other hand, it is common for radiologists to provide limited annotations of findings during routine reads, such as line measurements and arrows, that are often stored in PACS as GSPS objects. We posit that these sparse annotations can be extracted along with CT volumes and converted into 3D segmentations using promptable segmentation models, a paradigm we term Opportunistic Promptable Segmentation. To enable this paradigm, we propose SAM2CT, the first promptable segmentation model designed to convert radiologist annotations into 3D segmentations in CT volumes. SAM2CT builds upon SAM2 by extending the prompt encoder to support arrow and line inputs and by introducing Memory-Conditioned Memories (MCM), a memory encoding strategy tailored to 3D medical volumes. On public lesion segmentation benchmarks, SAM2CT outperforms existing promptable segmentation models and similarly trained baselines, achieving Dice similarity coefficients of 0.649 for arrow prompts and 0.757 for line prompts. Applying the model to pre-existing GSPS annotations from a clinical PACS (N = 60), SAM2CT generates 3D segmentations that are clinically acceptable or require only minor adjustments in 87% of cases, as scored by radiologists. Additionally, SAM2CT demonstrates strong zero-shot performance on select Emergency Department findings. These results suggest that large-scale mining of historical GSPS annotations represents a promising and scalable approach for generating 3D CT segmentation datasets.

</details>


### [48] [On the Assessment of Sensitivity of Autonomous Vehicle Perception](https://arxiv.org/abs/2602.00314)
*Apostol Vassilev,Munawar Hasan,Edward Griffor,Honglan Jin,Pavel Piliptchak,Mahima Arora,Thoshitha Gamage*

Main category: cs.CV

TL;DR: 自动驾驶的感知系统在不同环境下易受干扰，导致识别与检测失误。本文评估了多种感知模型在各种复杂驾驶情景下的鲁棒性，并提出相应衡量与改进方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶汽车需要依赖高精度、鲁棒的感知系统确保安全决策，但真实环境和对抗因素常导致感知数据失效，因此亟需研究感知系统在不同干扰条件下的表现与改进策略。

Method: 本文利用模型集成的方法，量化感知模型在不同恶劣驾驶场景下的不确定性和敏感性，并在真实和仿真环境下进行评估。还引入基于车辆在不同路况下的停车距离作为感知性能评估标准，选用YOLO系列、DETR及RT-DETR等五种视觉模型进行实测。

Result: 仿真实验及实际测试结果显示，弱光环境（如雾、低阳角等）对感知模型影响最大，对抗场景如目标遮挡、恶劣天气组合等也显著降低模型准确率。且与目标的距离越远，感知能力下降越明显。

Conclusion: 感知系统在复杂环境下鲁棒性有限，需针对多元干扰情景提升模型表现和系统可靠性，未来研究可关注多模型集成和环境自适应改进。

Abstract: The viability of automated driving is heavily dependent on the performance of perception systems to provide real-time accurate and reliable information for robust decision-making and maneuvers. These systems must perform reliably not only under ideal conditions, but also when challenged by natural and adversarial driving factors. Both of these types of interference can lead to perception errors and delays in detection and classification. Hence, it is essential to assess the robustness of the perception systems of automated vehicles (AVs) and explore strategies for making perception more reliable. We approach this problem by evaluating perception performance using predictive sensitivity quantification based on an ensemble of models, capturing model disagreement and inference variability across multiple models, under adverse driving scenarios in both simulated environments and real-world conditions. A notional architecture for assessing perception performance is proposed. A perception assessment criterion is developed based on an AV's stopping distance at a stop sign on varying road surfaces, such as dry and wet asphalt, and vehicle speed. Five state-of-the-art computer vision models are used, including YOLO (v8-v9), DEtection TRansformer (DETR50, DETR101), Real-Time DEtection TRansformer (RT-DETR)in our experiments. Diminished lighting conditions, e.g., resulting from the presence of fog and low sun altitude, have the greatest impact on the performance of the perception models. Additionally, adversarial road conditions such as occlusions of roadway objects increase perception sensitivity and model performance drops when faced with a combination of adversarial road conditions and inclement weather conditions. Also, it is demonstrated that the greater the distance to a roadway object, the greater the impact on perception performance, hence diminished perception robustness.

</details>


### [49] [Bridging the Semantic Chasm: Synergistic Conceptual Anchoring for Generalized Few-Shot and Zero-Shot OOD Perception](https://arxiv.org/abs/2602.00340)
*Alexandros Christoforos,Sarah Jenkins,Michael Brown,Tuan Pham,David Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的SynerNet协同神经体网络框架，有效缓解了视觉-语言模型在遇到分布外(OOD)概念时的跨模态对齐退化问题。通过四个专用计算单元协同工作，实现跨模态信息的高效交流与纠偏，显著提升了VLMs在极少样本和零样本环境下的精度。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在处理分布外新概念时，常因模态对齐能力退化导致性能下降。如何提升模型对未知分布的适应力和推理能力，是跨模态理解领域的关键难题。

Method: 该方法构建了包含视觉感知、语言上下文、名词嵌入、全局协调四个专用计算体的多智能体网络，通过结构化信息传递协议协同校正模态差异。具体贡献包括：基于多智能体潜在空间的命名获取框架、用于提升小样本适应能力的语义上下文交换算法，以及自适应动态均衡机制。

Result: 在VISTA-Beyond基准上评估，SynerNet在极少样本与零样本场景下的性能全面提升，多个领域任务的精度提高了1.2%至5.4%。

Conclusion: SynerNet为VLMs的跨模态对齐与概念泛化提供了有效的新思路，尤其在面对分布外概念时，显著提升了模型的适应性与推理性能，具有重要的应用前景。

Abstract: This manuscript presents a pioneering Synergistic Neural Agents Network (SynerNet) framework designed to mitigate the phenomenon of cross-modal alignment degeneration in Vision-Language Models (VLMs) when encountering Out-of-Distribution (OOD) concepts. Specifically, four specialized computational units - visual perception, linguistic context, nominal embedding, and global coordination - collaboratively rectify modality disparities via a structured message-propagation protocol. The principal contributions encompass a multi-agent latent space nomenclature acquisition framework, a semantic context-interchange algorithm for enhanced few-shot adaptation, and an adaptive dynamic equilibrium mechanism. Empirical evaluations conducted on the VISTA-Beyond benchmark demonstrate that SynerNet yields substantial performance augmentations in both few-shot and zero-shot scenarios, exhibiting precision improvements ranging from 1.2% to 5.4% across a diverse array of domains.

</details>


### [50] [When RAG Hurts: Diagnosing and Mitigating Attention Distraction in Retrieval-Augmented LVLMs](https://arxiv.org/abs/2602.00344)
*Beidi Zhao,Wenlong Deng,Xinting Liao,Yushu Li,Nazim Shaikh,Yao Nie,Xiaoxiao Li*

Main category: cs.CV

TL;DR: RAG在视觉问答中常常因为注意力偏向检索文本而表现不佳，本文发现新失效模式“注意力分散”，提出训练外干预方法MAD-RAG优化模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG方法在知识型视觉问答任务中容易因检索内容分配给图片的注意力不足而出错，但此前研究忽视了检索文本会导致对于原本就能回答的问题出现错误的新问题。

Method: 提出MAD-RAG方法，采用双重问题设定，将视觉定位与上下文整合解耦，并通过注意力混合机制保留由图像引导的证据。不需要额外训练。

Result: MAD-RAG方法在OK-VQA, E-VQA和InfoSeek数据集的多个模型上超越现有基线，提升最高达4.76%、9.20%、6.18%；能够纠正74.68%的RAG失效案例，且几乎不增加计算量。

Conclusion: MAD-RAG有效缓解了RAG在LVLMs知识型VQA中的“注意力分散”问题，提升模型鲁棒性和准确率，是低成本高回报的增强方案。

Abstract: While Retrieval-Augmented Generation (RAG) is one of the dominant paradigms for enhancing Large Vision-Language Models (LVLMs) on knowledge-based VQA tasks, recent work attributes RAG failures to insufficient attention towards the retrieved context, proposing to reduce the attention allocated to image tokens. In this work, we identify a distinct failure mode that previous study overlooked: Attention Distraction (AD). When the retrieved context is sufficient (highly relevant or including the correct answer), the retrieved text suppresses the visual attention globally, and the attention on image tokens shifts away from question-relevant regions. This leads to failures on questions the model could originally answer correctly without the retrieved text. To mitigate this issue, we propose MAD-RAG, a training-free intervention that decouples visual grounding from context integration through a dual-question formulation, combined with attention mixing to preserve image-conditioned evidence. Extensive experiments on OK-VQA, E-VQA, and InfoSeek demonstrate that MAD-RAG consistently outperforms existing baselines across different model families, yielding absolute gains of up to 4.76%, 9.20%, and 6.18% over the vanilla RAG baseline. Notably, MAD-RAG rectifies up to 74.68% of failure cases with negligible computational overhead.

</details>


### [51] [AdaFuse: Adaptive Multimodal Fusion for Lung Cancer Risk Prediction via Reinforcement Learning](https://arxiv.org/abs/2602.00347)
*Chongyu Qu,Zhengyi Lu,Yuxiang Lai,Thomas Z. Li,Junchao Zhu,Junlin Guo,Juming Xiong,Yanfan Zhu,Yuechen Yang,Allen J. Luna,Kim L. Sandler,Bennett A. Landman,Yuankai Huo*

Main category: cs.CV

TL;DR: 本文提出了AdaFuse方法，通过强化学习自适应地为每位患者选择合适的多模态融合策略，用于肺癌风险预测，并在大规模数据集上取得最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合方法对所有模态一视同仁或学习权重，但未考虑对于个体患者是否所有模态都需要使用。为提高诊断效率和预测准确性，解决患者个体化数据选择问题。

Method: 提出了AdaFuse框架，将多模态信息融合建模为一个序列决策过程，采用强化学习的策略网络，逐步决定是否引入新模态或基于已有信息直接预测；这样可根据已观测模态自适应选择并提前终止无需处理所有模态。

Result: 在国家肺癌筛查试验（NLST）数据集上的实验表明，AdaFuse获得了最高AUC(0.762)，优于最佳单模态(0.732)、固定融合策略(0.759)及自适应融合基线（如DynMM 0.754、MoE 0.742），且比全部三模态方法计算量低。

Conclusion: 强化学习能有效支持医疗影像中个性化的多模态融合，有助于实现因人制宜的诊断流程，提升诊断效率和准确性。

Abstract: Multimodal fusion has emerged as a promising paradigm for disease diagnosis and prognosis, integrating complementary information from heterogeneous data sources such as medical images, clinical records, and radiology reports. However, existing fusion methods process all available modalities through the network, either treating them equally or learning to assign different contribution weights, leaving a fundamental question unaddressed: for a given patient, should certain modalities be used at all? We present AdaFuse, an adaptive multimodal fusion framework that leverages reinforcement learning (RL) to learn patient-specific modality selection and fusion strategies for lung cancer risk prediction. AdaFuse formulates multimodal fusion as a sequential decision process, where the policy network iteratively decides whether to incorporate an additional modality or proceed to prediction based on the information already acquired. This sequential formulation enables the model to condition each selection on previously observed modalities and terminate early when sufficient information is available, rather than committing to a fixed subset upfront. We evaluate AdaFuse on the National Lung Screening Trial (NLST) dataset. Experimental results demonstrate that AdaFuse achieves the highest AUC (0.762) compared to the best single-modality baseline (0.732), the best fixed fusion strategy (0.759), and adaptive baselines including DynMM (0.754) and MoE (0.742), while using fewer FLOPs than all triple-modality methods. Our work demonstrates the potential of reinforcement learning for personalized multimodal fusion in medical imaging, representing a shift from uniform fusion strategies toward adaptive diagnostic pipelines that learn when to consult additional modalities and when existing information suffices for accurate prediction.

</details>


### [52] [MASC: Metal-Aware Sampling and Correction via Reinforcement Learning for Accelerated MRI](https://arxiv.org/abs/2602.00348)
*Zhengyi Lu,Ming Lu,Chongyu Qu,Junchao Zhu,Junlin Guo,Marilyn Lionts,Yanfan Zhu,Yuechen Yang,Tianyuan Yao,Jayasai Rajagopal,Bennett Allan Landman,Xiao Wang,Xinqiang Yan,Yuankai Huo*

Main category: cs.CV

TL;DR: 本文提出了一种基于强化学习的联合优化方法（MASC），同时解决MRI金属伪影去除和加速采集问题，实现伪影感知的k空间采样与伪影校正的协同优化。


<details>
  <summary>Details</summary>
Motivation: 金属植入物会导致MRI成像出现严重伪影，影响诊断。现有方法通常将伪影去除和MRI加速采集分开处理，不能很好地兼顾影像质量和采集效率。

Method: 作者提出MASC框架，将主动MRI采集建模为顺序决策问题，采用PPO强化学习算法，根据经过U-Net伪影校正的欠采样重建结果，选择k空间采样线。作者还提出端到端训练方案，使采集策略与伪影去除网络协同进化。为实现有监督训练，作者基于物理仿真构造了带/不带金属植入物的MRI配对数据集，可直接监督伪影去除与采集策略学习。

Result: MASC的采样策略优于传统采样方法，端到端协同优化提升了伪影校正性能。跨数据集（FastMRI）实验证明MASC在真实临床MRI数据上的泛化性。

Conclusion: MASC实现了MRI金属伪影去除与加速采集的联合优化，大幅提升重建效果，方法具有较强的实际应用价值。代码和模型已开源。

Abstract: Metal implants in MRI cause severe artifacts that degrade image quality and hinder clinical diagnosis. Traditional approaches address metal artifact reduction (MAR) and accelerated MRI acquisition as separate problems. We propose MASC, a unified reinforcement learning framework that jointly optimizes metal-aware k-space sampling and artifact correction for accelerated MRI. To enable supervised training, we construct a paired MRI dataset using physics-based simulation, generating k-space data and reconstructions for phantoms with and without metal implants. This paired dataset provides simulated 3D MRI scans with and without metal implants, where each metal-corrupted sample has an exactly matched clean reference, enabling direct supervision for both artifact reduction and acquisition policy learning. We formulate active MRI acquisition as a sequential decision-making problem, where an artifact-aware Proximal Policy Optimization (PPO) agent learns to select k-space phase-encoding lines under a limited acquisition budget. The agent operates on undersampled reconstructions processed through a U-Net-based MAR network, learning patterns that maximize reconstruction quality. We further propose an end-to-end training scheme where the acquisition policy learns to select k-space lines that best support artifact removal while the MAR network simultaneously adapts to the resulting undersampling patterns. Experiments demonstrate that MASC's learned policies outperform conventional sampling strategies, and end-to-end training improves performance compared to using a frozen pre-trained MAR network, validating the benefit of joint optimization. Cross-dataset experiments on FastMRI with physics-based artifact simulation further confirm generalization to realistic clinical MRI data. The code and models of MASC have been made publicly available: https://github.com/hrlblab/masc

</details>


### [53] [ReLAPSe: Reinforcement-Learning-trained Adversarial Prompt Search for Erased concepts in unlearned diffusion models](https://arxiv.org/abs/2602.00350)
*Ignacy Kolton,Kacper Marzol,Paweł Batorski,Marcin Mazur,Paul Swoboda,Przemysław Spurek*

Main category: cs.CV

TL;DR: 本文提出了一种新的对抗性策略ReLAPSe，可以高效恢复文本到图像扩散模型中已被“遗忘”的概念，提高机器遗忘方法的红队测试能力。


<details>
  <summary>Details</summary>
Motivation: 当前通过“机器遗忘”方法对扩散模型移除敏感或未授权内容，但发现仍存在视觉信息泄露。现有对抗恢复手段效率低或无法直接利用模型反馈，限制了对遗忘效果的评估与攻防对抗能力。

Method: 提出ReLAPSe框架，将“恢复被遗忘概念”问题建模为强化学习任务。通过‘可验证奖励’强化学习方式，以扩散模型自有的噪声预测损失为反馈信号，指导智能体学习全球性的恢复策略，有别于过去逐实例优化方法。

Result: ReLAPSe在多种主流机器遗忘方法下，显著高效地恢复了细粒度身份与风格信息，且实现了近实时的攻击能力。

Conclusion: 通过将逐实例优化转变为全局策略学习，ReLAPSe提升了恢复被遗忘内容的效率和泛化性，为扩散模型安全红队测试提供了新利器，也揭示了现有机器遗忘方案的潜在安全隐患。

Abstract: Machine unlearning is a key defense mechanism for removing unauthorized concepts from text-to-image diffusion models, yet recent evidence shows that latent visual information often persists after unlearning. Existing adversarial approaches for exploiting this leakage are constrained by fundamental limitations: optimization-based methods are computationally expensive due to per-instance iterative search. At the same time, reasoning-based and heuristic techniques lack direct feedback from the target model's latent visual representations. To address these challenges, we introduce ReLAPSe, a policy-based adversarial framework that reformulates concept restoration as a reinforcement learning problem. ReLAPSe trains an agent using Reinforcement Learning with Verifiable Rewards (RLVR), leveraging the diffusion model's noise prediction loss as a model-intrinsic and verifiable feedback signal. This closed-loop design directly aligns textual prompt manipulation with latent visual residuals, enabling the agent to learn transferable restoration strategies rather than optimizing isolated prompts. By pioneering the shift from per-instance optimization to global policy learning, ReLAPSe achieves efficient, near-real-time recovery of fine-grained identities and styles across multiple state-of-the-art unlearning methods, providing a scalable tool for rigorous red-teaming of unlearned diffusion models. Some experimental evaluations involve sensitive visual concepts, such as nudity. Code is available at https://github.com/gmum/ReLaPSe

</details>


### [54] [Modeling Image-Caption Rating from Comparative Judgments](https://arxiv.org/abs/2602.00381)
*Kezia Minni,Qiang Zhang,Monoshiz Mahbub Khan,Zhe Yu*

Main category: cs.CV

TL;DR: 论文提出利用比较性学习模型来评估图像描述与图片的匹配度，相较于传统的直接评分回归模型，该方法随着数据增多表现不断提升，并且能够有效减少人工标注成本。


<details>
  <summary>Details</summary>
Motivation: 人工为图像和描述配对准确性评分既耗时又主观，但两两比较更容易，人类判别也更一致。作者希望利用这一点，设计更高效、客观的评估方法。

Method: 使用VICR数据集，借助ResNet-50提取图像特征，MiniLM提取描述特征，分别训练传统回归模型（用人工分数拟合）与比较性学习模型（拟合比较判断），并在未见数据上进行排序评估。另还进行了小规模的人类评测实验，比较绝对评分与成对比较的效率和一致性。

Result: 回归模型在Pearson和Spearman相关性上表现更佳，但比较性学习模型随数据量增加持续提高，逐渐接近回归基线。而且人工对比实验显示，相较于直接评分，比较性标注速度更快，人类标注者间更一致。

Conclusion: 比较性学习模型能有效模拟人类偏好，且可大幅降低人工标注成本。两两比较比绝对评分更高效，可作为图像—描述匹配任务的有力工具。

Abstract: Rating the accuracy of captions in describing images is time-consuming and subjective for humans. In contrast, it is often easier for people to compare two captions and decide which one better matches a given image. In this work, we propose a machine learning framework that models such comparative judgments instead of direct ratings. The model can then be applied to rank unseen image-caption pairs in the same way as a regression model trained on direct ratings. Using the VICR dataset, we extract visual features with ResNet-50 and text features with MiniLM, then train both a regression model and a comparative learning model. While the regression model achieves better performance (Pearson's $ρ$: 0.7609 and Spearman's $r_s$: 0.7089), the comparative learning model steadily improves with more data and approaches the regression baseline. In addition, a small-scale human evaluation study comparing absolute rating, pairwise comparison, and same-image comparison shows that comparative annotation yields faster results and has greater agreement among human annotators. These results suggest that comparative learning can effectively model human preferences while significantly reducing the cost of human annotations.

</details>


### [55] [Deep Learning-Based Object Detection for Autonomous Vehicles: A Comparative Study of One-Stage and Two-Stage Detectors on Basic Traffic Objects](https://arxiv.org/abs/2602.00385)
*Bsher Karbouj,Adam Michael Altenbuchner,Joerg Krueger*

Main category: cs.CV

TL;DR: 本文比较并分析了YOLOv5和Faster R-CNN两种主流目标检测模型在自动驾驶场景下的性能，探讨其优缺点并给出适用性建议。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶系统中，目标检测至关重要，不同的深度学习方法对系统性能有显著影响。现有研究对主流检测模型在特定自动驾驶应用中的适用性指导有限，因此有必要系统比较这些模型。

Method: 作者选取了YOLOv5（一阶段检测器）与Faster R-CNN（二阶段检测器）两种模型，在结合真实与合成图像的多样数据集上，通过mAP、召回率和推理速度等多项指标进行实验对比分析，并测试不同置信度阈值和实际场景下的表现。

Result: 结果显示，YOLOv5在mAP、召回率及训练效率方面表现更优，尤其适合大规模、高分辨率数据集；而Faster R-CNN在小型、远距离目标的检测及复杂光照条件下更具优势。

Conclusion: 二者各有千秋，YOLOv5适用于对速度和大范围目标检测要求高的应用，而Faster R-CNN适合对小物体检测和恶劣环境要求较高的场景。研究为自动驾驶系统选择合适的检测模型提供了实证依据和建议。

Abstract: Object detection is a crucial component in autonomous vehicle systems. It enables the vehicle to perceive and understand its environment by identifying and locating various objects around it. By utilizing advanced imaging and deep learning techniques, autonomous vehicle systems can rapidly and accurately identify objects based on their features. Different deep learning methods vary in their ability to accurately detect and classify objects in autonomous vehicle systems. Selecting the appropriate method significantly impacts system performance, robustness, and efficiency in real-world driving scenarios. While several generic deep learning architectures like YOLO, SSD, and Faster R-CNN have been proposed, guidance on their suitability for specific autonomous driving applications is often limited. The choice of method affects detection accuracy, processing speed, environmental robustness, sensor integration, scalability, and edge case handling. This study provides a comprehensive experimental analysis comparing two prominent object detection models: YOLOv5 (a one-stage detector) and Faster R-CNN (a two-stage detector). Their performance is evaluated on a diverse dataset combining real and synthetic images, considering various metrics including mean Average Precision (mAP), recall, and inference speed. The findings reveal that YOLOv5 demonstrates superior performance in terms of mAP, recall, and training efficiency, particularly as dataset size and image resolution increase. However, Faster R-CNN shows advantages in detecting small, distant objects and performs well in challenging lighting conditions. The models' behavior is also analyzed under different confidence thresholds and in various real-world scenarios, providing insights into their applicability for autonomous driving systems.

</details>


### [56] [Robust automatic brain vessel segmentation in 3D CTA scans using dynamic 4D-CTA data](https://arxiv.org/abs/2602.00391)
*Alberto Mario Ceballos-Arroyo,Shrikanth M. Yadav,Chu-Hsuan Lin,Jisoo Kim,Geoffrey S. Young,Huaizu Jiang,Lei Qin*

Main category: cs.CV

TL;DR: 本研究提出了一种利用动态4D-CTA头部扫描自动标注脑血管的新方法，通过多时相减去骨骼和软组织，增强血管可视化，并用深度学习模型提升分割精度，且公开代码与模型。


<details>
  <summary>Details</summary>
Motivation: 脑血管精准分割依赖人工标注，过程费时费力。现有数据集规模有限且难以适应不同对比度相位，造成深度学习方法泛化能力不足，因此亟需提高脑血管注释效率以及训练数据量与多样性的创新方案。

Method: 作者利用动态4D-CTA扫描的多个时间点，采用多时相减法消除骨骼和软组织信号，突显脑血管结构，降低标注难度。通过对同一序列多阶段采用相同分割注释，有效扩充了可用训练集，并采用深度学习（nnUNet）模型进行血管分割训练和评估。

Result: 新数据集包含110个训练图像和165个测试图像。相较于同类数据集，用nnUNet在新数据集上对血管分割的精度显著提高，mDC指标：动脉0.846，静脉0.957。其他评估指标（aDHD和tSens）也表现出更低误差和更高的灵敏度，表明模型准确捕捉血管形态特征。

Conclusion: 通过利用4D-CTA不同时间点数据，新方法大幅提升了血管标注和模型分割的效率及准确率，为脑血管影像分割带来更强鲁棒性和实用性，也为相关研究公开了高质量数据和工具。

Abstract: In this study, we develop a novel methodology for annotating the brain vasculature using dynamic 4D-CTA head scans. By using multiple time points from dynamic CTA acquisitions, we subtract bone and soft tissue to enhance the visualization of arteries and veins, reducing the effort required to obtain manual annotations of brain vessels. We then train deep learning models on our ground truth annotations by using the same segmentation for multiple phases from the dynamic 4D-CTA collection, effectively enlarging our dataset by 4 to 5 times and inducing robustness to contrast phases. In total, our dataset comprises 110 training images from 25 patients and 165 test images from 14 patients. In comparison with two similarly-sized datasets for CTA-based brain vessel segmentation, a nnUNet model trained on our dataset can achieve significantly better segmentations across all vascular regions, with an average mDC of 0.846 for arteries and 0.957 for veins in the TopBrain dataset. Furthermore, metrics such as average directed Hausdorff distance (adHD) and topology sensitivity (tSens) reflected similar trends: using our dataset resulted in low error margins (aDHD of 0.304 mm for arteries and 0.078 for veins) and high sensitivity (tSens of 0.877 for arteries and 0.974 for veins), indicating excellent accuracy in capturing vessel morphology. Our code and model weights are available online: https://github.com/alceballosa/robust-vessel-segmentation

</details>


### [57] [Brazilian Portuguese Image Captioning with Transformers: A Study on Cross-Native-Translated Dataset](https://arxiv.org/abs/2602.00393)
*Gabriel Bromonschenkel,Alessandro L. Koerich,Thiago M. Paixão,Hilário Tomaz Alves de Oliveira*

Main category: cs.CV

TL;DR: 本文针对巴西葡萄牙语图像描述任务，评估了多种Transformer视觉-语言模型在人工与自动翻译数据集上的表现，发现本土和自动翻译数据集各有优劣，不同模型的泛化能力、文本与图像对齐效果及偏差表现有显著差异。


<details>
  <summary>Details</summary>
Motivation: 大多数图像描述研究集中在英语，低资源语言如巴西葡萄牙语因缺乏数据和模型而发展缓慢。部分研究通过自动翻译方式创建数据集，但缺乏系统评估。本文旨在系统评估自动翻译与本地数据集对模型表现的影响，填补相关研究空白。

Method: 作者构建了巴西葡萄牙语版Flickr30K数据集，一部分由母语者人工标注，一部分由英语原文自动翻译。选用多种Transformer视觉-语言模型，采用交叉训练-测试方法检验自动翻译对泛化的影响，用CLIP-Score评估图文对齐，用注意力图分析模型推理过程。

Result: Swin-DistilBERTimbau模型在各项任务中表现最优，泛化能力较强。ViTucano在传统文本指标上超过多语种大模型，但GPT-4家族在CLIP-Score上表现最佳，表明其图文对齐更优。分析发现模型存在性别判断、物体计数及空间位置偏差。

Conclusion: 本地人工数据比自动翻译具更高价值，但两者结合可提升泛化能力；新模型在巴西葡萄牙语图像描述上已优于多语言大模型。注意力图剖析揭示模型系统性偏差，未来需针对低资源语言优化数据和模型。

Abstract: Image captioning (IC) refers to the automatic generation of natural language descriptions for images, with applications ranging from social media content generation to assisting individuals with visual impairments. While most research has been focused on English-based models, low-resource languages such as Brazilian Portuguese face significant challenges due to the lack of specialized datasets and models. Several studies create datasets by automatically translating existing ones to mitigate resource scarcity. This work addresses this gap by proposing a cross-native-translated evaluation of Transformer-based vision and language models for Brazilian Portuguese IC. We use a version of Flickr30K comprised of captions manually created by native Brazilian Portuguese speakers and compare it to a version with captions automatically translated from English to Portuguese. The experiments include a cross-context approach, where models trained on one dataset are tested on the other to assess the translation impact. Additionally, we incorporate attention maps for model inference interpretation and use the CLIP-Score metric to evaluate the image-description alignment. Our findings show that Swin-DistilBERTimbau consistently outperforms other models, demonstrating strong generalization across datasets. ViTucano, a Brazilian Portuguese pre-trained VLM, surpasses larger multilingual models (GPT-4o, LLaMa 3.2 Vision) in traditional text-based evaluation metrics, while GPT-4 models achieve the highest CLIP-Score, highlighting improved image-text alignment. Attention analysis reveals systematic biases, including gender misclassification, object enumeration errors, and spatial inconsistencies. The datasets and the models generated and analyzed during the current study are available in: https://github.com/laicsiifes/transformer-caption-ptbr.

</details>


### [58] [Modeling Art Evaluations from Comparative Judgments: A Deep Learning Approach to Predicting Aesthetic Preferences](https://arxiv.org/abs/2602.00394)
*Manoj Reddy Bethi,Sai Rupa Jhade,Pravallika Yaganti,Monoshiz Mahbub Khan,Zhe Yu*

Main category: cs.CV

TL;DR: 本文提出了一种基于成对偏好比较的学习框架，用于建模人类对美术作品的美学判断，从而降低获取人工标签的成本。深度学习模型显著优于基线模型，比对学习在无评分情况下表现优良，且标注效率更高。


<details>
  <summary>Details</summary>
Motivation: 美学判断存在很强的个体差异，且获取人工标签（如打分）成本很高。区分式学习（即基于成对比较）可能比绝对打分更加高效且稳定，但其应用与效果还有待验证。

Method: 作者利用ResNet-50提取绘画图像深度特征，设计了深度神经网络回归模型和成对比较双分支模型，并系统对比了深度模型与传统线性基线之间的效果，回归与成对比较模型的表现，无评分条件下性能，以及不同注释方式的成本与效率差异。

Result: 深度回归模型在$R^2$上比基线提升高达328%。成对比较模型虽无法访问打分，但性能接近回归模型，体现了其实用性。个体偏好难以准确建模，但平均偏好较容易。行为实验显示成对判断比直接评分能节省60%的人力标注时间。

Conclusion: 成对偏好比较结合深度特征能有效建模视觉美学判断且极大提高标注效率，但个体化偏好预测仍具挑战。此方法适合大规模美学建模应用。

Abstract: Modeling human aesthetic judgments in visual art presents significant challenges due to individual preference variability and the high cost of obtaining labeled data. To reduce cost of acquiring such labels, we propose to apply a comparative learning framework based on pairwise preference assessments rather than direct ratings. This approach leverages the Law of Comparative Judgment, which posits that relative choices exhibit less cognitive burden and greater cognitive consistency than direct scoring. We extract deep convolutional features from painting images using ResNet-50 and develop both a deep neural network regression model and a dual-branch pairwise comparison model. We explored four research questions: (RQ1) How does the proposed deep neural network regression model with CNN features compare to the baseline linear regression model using hand-crafted features? (RQ2) How does pairwise comparative learning compare to regression-based prediction when lacking access to direct rating values? (RQ3) Can we predict individual rater preferences through within-rater and cross-rater analysis? (RQ4) What is the annotation cost trade-off between direct ratings and comparative judgments in terms of human time and effort? Our results show that the deep regression model substantially outperforms the baseline, achieving up to $328\%$ improvement in $R^2$. The comparative model approaches regression performance despite having no access to direct rating values, validating the practical utility of pairwise comparisons. However, predicting individual preferences remains challenging, with both within-rater and cross-rater performance significantly lower than average rating prediction. Human subject experiments reveal that comparative judgments require $60\%$ less annotation time per item, demonstrating superior annotation efficiency for large-scale preference modeling.

</details>


### [59] [3DGS$^2$-TR: Scalable Second-Order Trust-Region Method for 3D Gaussian Splatting](https://arxiv.org/abs/2602.00395)
*Roger Hsiao,Yuchen Fang,Xiangru Huang,Ruilong Li,Hesam Rabeti,Zan Gojcic,Javad Lavaei,James Demmel,Sophia Shao*

Main category: cs.CV

TL;DR: 本文提出了一种高效的3D Gaussian Splatting场景训练二阶优化器3DGS$^2$-TR，通过对Hessian矩阵对角线的近似实现低复杂度高性能，提升了重建质量并极大地压缩了训练代价。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting领域主流的二阶优化方法要么内存消耗过高，要么计算复杂度太大，不利于实际场景训练需求。作者希望通过一种高效简洁的方法平衡优化能力与资源消耗，推动更大型场景与分布式环境下的应用。

Method: 该方法用Hutchinson技巧近似Hessian对角线，从而不用大规模的显式曲率表示，实现O(n)的计算和内存复杂度。同时引入基于Hellinger距离的逐参数信赖域正则，以解决3DGS渲染过程的强非线性，提升优化稳定性。

Result: 在标准数据集和完全相同的参数初始化下，3DGS$^2$-TR无需点云加密便能以ADAM一半的训练迭代数获得更佳重建结果，并且在GPU显存开销上远低于传统二阶方法（3DGS-LM），接近ADAM。

Conclusion: 3DGS$^2$-TR在保持高重建品质的同时，大幅度降低了训练时间和内存资源，为处理更大规模场景和分布式训练提供了可能性，具有较强的实际应用价值。

Abstract: We propose 3DGS$^2$-TR,a second-order optimizer for accelerating the scene training problem in 3D Gaussian Splatting (3DGS). Unlike existing second-order approaches that rely on explicit or dense curvature representations, such as 3DGS-LM (Höllein et al., 2025) or 3DGS2 (Lan et al., 2025), our method approximates curvature using only the diagonal of the Hessian matrix, efficiently via Hutchinson's method. Our approach is fully matrix-free and has the same complexity as ADAM (Kingma, 2024), $O(n)$ in both computation and memory costs. To ensure stable optimization in the presence of strong nonlinearity in the 3DGS rasterization process, we introduce a parameter-wise trust-region technique based on the squared Hellinger distance, regularizing updates to Gaussian parameters. Under identical parameter initialization and without densification, 3DGS$^2$-TR is able to achieve better reconstruction quality on standard datasets, using 50% fewer training iterations compared to ADAM, while incurring less than 1GB of peak GPU memory overhead (17% more than ADAM and 85% less than 3DGS-LM), enabling scalability to very large scenes and potentially to distributed training settings.

</details>


### [60] [Toward Autonomous Laboratory Safety Monitoring with Vision Language Models: Learning to See Hazards Through Scene Structure](https://arxiv.org/abs/2602.00414)
*Trishna Chakraborty,Udita Ghosh,Aldair Ernesto Gongora,Ruben Glatt,Yue Dong,Jiachen Li,Amit K. Roy-Chowdhury,Chengyu Song*

Main category: cs.CV

TL;DR: 本文提出了一种新的管道，将实验室安全场景的文本描述转化为可用于视觉语言模型（VLMs）评价的结构化数据，并探索了VLMs在实验室安全监控中的表现；创新地利用场景图优化VLMs在视觉任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 实验室常因小小的不安全行为引发严重伤害，现有持续性安全监控受限于人力，而VLMs具备自动监控潜力，但缺乏视觉数据评测其在真实场景下的表现。

Method: 作者设计了一条数据生成管道，将文本场景通过大语言模型（LLMs）生成场景图并用图像生成模型渲染图片，从而得到(图像-场景图-真值）三元组数据集，测试多种VLMs基于文本和视觉信息的安全风险识别能力，并提出基于场景图的上下文对齐后处理方法提高模型纯视觉场景下表现。

Result: 在包含1207个样本、362种场景的数据集上，VLMs若用场景图（文本结构）表现良好，但仅用视觉信息时性能大幅下降。所提的基于场景图对齐方法，显著提升了VLMs在纯视觉任务的风险检测能力。

Conclusion: VLMs当前难以直接从图像中提取结构化关系用于复杂场景推理，需通过场景图引导实现能力延伸，该方法能助力自动化、精准的实验室安全监控场景应用。

Abstract: Laboratories are prone to severe injuries from minor unsafe actions, yet continuous safety monitoring -- beyond mandatory pre-lab safety training -- is limited by human availability. Vision language models (VLMs) offer promise for autonomous laboratory safety monitoring, but their effectiveness in realistic settings is unclear due to the lack of visual evaluation data, as most safety incidents are documented primarily as unstructured text. To address this gap, we first introduce a structured data generation pipeline that converts textual laboratory scenarios into aligned triples of (image, scene graph, ground truth), using large language models as scene graph architects and image generation models as renderers. Our experiments on the synthetic dataset of 1,207 samples across 362 unique scenarios and seven open- and closed-source models show that VLMs perform effectively given textual scene graph, but degrade substantially in visual-only settings indicating difficulty in extracting structured object relationships directly from pixels. To overcome this, we propose a post-training context-engineering approach, scene-graph-guided alignment, to bridge perceptual gaps in VLMs by translating visual inputs into structured scene graphs better aligned with VLM reasoning, improving hazard detection performance in visual only settings.

</details>


### [61] [Text is All You Need for Vision-Language Model Jailbreaking](https://arxiv.org/abs/2602.00420)
*Yihang Chen,Zhao Xu,Youyuan Jiang,Tianle Zheng,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 提出了Text-DJ攻击方法，通过将有害文本拆分为子查询、混杂干扰查询并转化为图片，绕过大型视觉-语言模型（LVLMs）的安全机制，成功实现了越狱。


<details>
  <summary>Details</summary>
Motivation: 虽然LVLMs配备了安全措施，但主要依赖于对显式文字或关联场景的检测。论文动机是探索是否可以通过模型的OCR能力绕过文本和视觉层面的安全防护。

Method: 提出了三步的Text-DJ攻击流程：1）将有害问题拆成多个更温和的子问题；2）加入一批内容无关的干扰问题；3）将所有子问题和干扰问题以图像网格形式输入模型，其中子问题位于中间。

Result: 通过实验证明，该方法能有效绕过当前主流LVLMs的安全机制，使其输出原本被禁止的内容。

Conclusion: 暴露出LVLMs的OCR通道在处理分散和多图输入时存在严重安全漏洞，需针对分割的多模态输入设计新的防御措施。

Abstract: Large Vision-Language Models (LVLMs) are increasingly equipped with robust safety safeguards to prevent responses to harmful or disallowed prompts. However, these defenses often focus on analyzing explicit textual inputs or relevant visual scenes. In this work, we introduce Text-DJ, a novel jailbreak attack that bypasses these safeguards by exploiting the model's Optical Character Recognition (OCR) capability. Our methodology consists of three stages. First, we decompose a single harmful query into multiple and semantically related but more benign sub-queries. Second, we pick a set of distraction queries that are maximally irrelevant to the harmful query. Third, we present all decomposed sub-queries and distraction queries to the LVLM simultaneously as a grid of images, with the position of the sub-queries being middle within the grid. We demonstrate that this method successfully circumvents the safety alignment of state-of-the-art LVLMs. We argue this attack succeeds by (1) converting text-based prompts into images, bypassing standard text-based filters, and (2) inducing distractions, where the model's safety protocols fail to link the scattered sub-queries within a high number of irrelevant queries. Overall, our findings expose a critical vulnerability in LVLMs' OCR capabilities that are not robust to dispersed, multi-image adversarial inputs, highlighting the need for defenses for fragmented multimodal inputs.

</details>


### [62] [DISK: Dynamic Inference SKipping for World Models](https://arxiv.org/abs/2602.00440)
*Anugunj Naman,Gaibo Zhang,Ayushman Singh,Yaguang Zhang*

Main category: cs.CV

TL;DR: 提出DISK方法，通过自适应跳跃决策加速视频和轨迹扩散推理，无需重新训练，能有效提升长时序推理效率且保持预测质量。


<details>
  <summary>Details</summary>
Motivation: 长时序视频与自车轨迹生成（世界建模）过程中，扩散模型推理计算开销大、速度慢，限制其实用性。作者试图提出无需重新训练的方法，通过适应性跳跃策略显著加速推理流程，降低部署和推理成本。

Method: 提出DISK方法，核心为两个耦合的扩散变换器（分别用于视频和轨迹），通过双分支控制器实现多模态跨模跳跃决策，并结合高阶潜在差分跳跃测试、控制器统计跨时序传播，提升长时序推理的稳定性和效率，无需再训练原模型。

Result: 在1500组NuPlan和NuScenes闭环自动驾驶数据上测试，基于NVIDIA L40S GPU，DISK在保证预测精度（L2误差）、视觉质量（FID/FVD）和下游性能（PDMS指标）的前提下，实现了轨迹扩散2倍提速、视频扩散1.6倍提速。

Conclusion: DISK能在无需重新训练的前提下，有效加速多模态扩散世界模型的长时序推理，兼顾效率和质量，具备实际应用价值。

Abstract: We present DISK, a training-free adaptive inference method for autoregressive world models. DISK coordinates two coupled diffusion transformers for video and ego-trajectory via dual-branch controllers with cross-modal skip decisions, preserving motion-appearance consistency without retraining. We extend higher-order latent-difference skip testing to the autoregressive chain-of-forward regime and propagate controller statistics through rollout loops for long-horizon stability. When integrated into closed-loop driving rollouts on 1500 NuPlan and NuScenes samples using an NVIDIA L40S GPU, DISK achieves 2x speedup on trajectory diffusion and 1.6x speedup on video diffusion while maintaining L2 planning error, visual quality (FID/FVD), and NAVSIM PDMS scores, demonstrating practical long-horizon video-and-trajectory prediction at substantially reduced cost.

</details>


### [63] [Model Optimization for Multi-Camera 3D Detection and Tracking](https://arxiv.org/abs/2602.00450)
*Ethan Anderson,Justin Silva,Kyle Zheng,Sameer Pusegaonkar,Yizhou Wang,Zheng Tang,Sujit Biswas*

Main category: cs.CV

TL;DR: 论文提出并系统评估了Sparse4D框架，用于多摄像头下的多目标3D检测与跟踪，通过不同的实验分析其在帧率降低、量化、跨数据集泛化及混合精度训练下的表现与权衡。


<details>
  <summary>Details</summary>
Motivation: 室内环境下，多摄像头系统需在视角异构和遮挡情况下实现高效的多目标跟踪，但现有方法在帧率降低、参数量化与泛化能力方面存在局限，亟需兼顾准确性和实时性的实用解决方案。

Method: Sparse4D为基于query的时空3D检测与跟踪框架，采用多视角特征融合到统一世界坐标，通过实例记忆实现稀疏对象查询传播。论文实验评估了帧率下调、后训练量化（INT8/FP8）、跨数据集迁移、以及Transformer Engine混合精度微调等多种策略，并提出新的衡量指标AvgTrackDur用于度量身份保持能力。

Result: Sparse4D在中等帧率降低时表现稳定，2 FPS以下时身份关联大幅下降；骨干与颈部的选择性量化在速度与精度间表现最佳，注意力相关模块对低精度敏感；WILDTRACK数据集上低帧率预训练带来显著零样本提升，少量微调增益有限；Transformer Engine混合精度能提升延迟与扩展性，但会影响身份传播稳定性。

Conclusion: Sparse4D可在多摄像头室内跟踪任务中兼顾速度与准确性，对量化、精度和数据泛化有较好适应性，但需关注极低帧率和混合精度对身份关联稳定性的负面影响，未来需结合稳定性验证以提升实用价值。

Abstract: Outside-in multi-camera perception is increasingly important in indoor environments, where networks of static cameras must support multi-target tracking under occlusion and heterogeneous viewpoints. We evaluate Sparse4D, a query-based spatiotemporal 3D detection and tracking framework that fuses multi-view features in a shared world frame and propagates sparse object queries via instance memory. We study reduced input frame rates, post-training quantization (INT8 and FP8), transfer to the WILDTRACK benchmark, and Transformer Engine mixed-precision fine-tuning. To better capture identity stability, we report Average Track Duration (AvgTrackDur), which measures identity persistence in seconds. Sparse4D remains stable under moderate FPS reductions, but below 2 FPS, identity association collapses even when detections are stable. Selective quantization of the backbone and neck offers the best speed-accuracy trade-off, while attention-related modules are consistently sensitive to low precision. On WILDTRACK, low-FPS pretraining yields large zero-shot gains over the base checkpoint, while small-scale fine-tuning provides limited additional benefit. Transformer Engine mixed precision reduces latency and improves camera scalability, but can destabilize identity propagation, motivating stability-aware validation.

</details>


### [64] [LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs](https://arxiv.org/abs/2602.00462)
*Benno Krojer,Shravan Nayak,Oscar Mañas,Vaibhav Adlakha,Desmond Elliott,Siva Reddy,Marius Mosbach*

Main category: cs.CV

TL;DR: 作者提出了一种新的解释方法LatentLens，可将视觉-语言模型中视觉token的隐空间表示映射为可读的自然语言描述，增强了视觉token的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型（VLM）通过简单变换就能让大语言模型（LLM）处理视觉token，然而这些视觉token在模型各层中编码了什么语义目前缺乏解释工具。研究动因在于提升VLM的可解释性，更好地理解视觉与语言表示的对齐情况。

Method: 提出LatentLens方法：将大规模文本语料编码为带上下文信息的token表征，计算视觉token和所有文本token表征的相似度，通过top-k最近邻文本token为每个视觉token生成语义描述。实验涵盖10个不同的VLM，并与现有解释方法进行比较。

Result: 实验发现，常用的LogitLens方法低估了视觉token的可解释性，而LatentLens能让大多数视觉token在所有模型、所有层次都易于解释。生成的描述更具语义性和细粒度。

Conclusion: LatentLens极大提升了VLM视觉token的解释能力，揭示了视觉和语言表示之间的更紧密对齐关系，并为分析模型隐表示开辟了新方向。

Abstract: Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations.

</details>


### [65] [PSGS: Text-driven Panorama Sliding Scene Generation via Gaussian Splatting](https://arxiv.org/abs/2602.00463)
*Xin Zhang,Shen Chen,Jiale Zhou,Lei Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为PSGS的两阶段框架，能够从文本高质量生成全景3D场景，明显提升了细节和一致性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 沉浸式应用（VR、AR、游戏）迫切需要能从文本生成真实3D场景的方法，但现有工作受限于3D-文本数据稀缺与多视角融合不一致，导致生成的场景过于简单。本文旨在解决这一瓶颈，实现高保真、高一致性的文本驱动3D场景生成。

Method: PSGS框架包含两阶段：首先，通过两层优化架构生成语义一致的全景图，先由布局推理层将文本解析为结构化空间关系，再用自优化层结合多模态大模型（MLLM）反馈反复提升画面细节。其次，提出全景滑动机制，通过有重叠采样获得全局一致的3D Gaussian Splatting点云，并在训练中引入深度和语义一致性损失增强效果。

Result: 实验表明，PSGS在全景生成和3D场景效果上均超越现有方法，生成内容更细致、更具吸引力。

Conclusion: PSGS为文本到3D高质量场景生成提供了稳定、可扩展的解决方案，有望促进沉浸式内容生成的相关应用领域。

Abstract: Generating realistic 3D scenes from text is crucial for immersive applications like VR, AR, and gaming. While text-driven approaches promise efficiency, existing methods suffer from limited 3D-text data and inconsistent multi-view stitching, resulting in overly simplistic scenes. To address this, we propose PSGS, a two-stage framework for high-fidelity panoramic scene generation. First, a novel two-layer optimization architecture generates semantically coherent panoramas: a layout reasoning layer parses text into structured spatial relationships, while a self-optimization layer refines visual details via iterative MLLM feedback. Second, our panorama sliding mechanism initializes globally consistent 3D Gaussian Splatting point clouds by strategically sampling overlapping perspectives. By incorporating depth and semantic coherence losses during training, we greatly improve the quality and detail fidelity of rendered scenes. Our experiments demonstrate that PSGS outperforms existing methods in panorama generation and produces more appealing 3D scenes, offering a robust solution for scalable immersive content creation.

</details>


### [66] [ZS-TreeSeg: A Zero-Shot Framework for Tree Crown Instance Segmentation](https://arxiv.org/abs/2602.00470)
*Pengyu Chen,Fangzheng Lyu,Sicheng Wang,Cuizhen Wang*

Main category: cs.CV

TL;DR: 本文提出了一种零样本树冠分割框架（ZS-TreeSeg），结合了语义分割和细胞实例分割，通过对树冠建模为星形凸对象，实现了无需训练的树冠实例分割，在不同数据集和冠层密度下表现稳健。


<details>
  <summary>Details</summary>
Motivation: 现有的树冠分割方法在密集、重叠冠层下表现不佳，监督学习方法依赖高昂的标注成本且泛化性有限，通用基础模型（如Segment Anything）又缺乏领域知识，导致分割不足。因此，需要一种训练代价低且能更好泛化的分割方法。

Method: 提出ZS-TreeSeg零样本分割框架，将树冠实例分割问题转化为成熟的冠层语义分割和细胞实例分割问题的适配。具体地，用Cellpose-SAM方法，将树冠建模为星形凸对象，并利用拓扑流场与向量收敛性，实现重叠树冠实例的数学分离。

Result: 在NEON和BAMFOREST两个公开数据集上进行实验，并通过可视化检验，表明该框架在不同传感器类型和冠层密度下都能实现稳健的分割效果。

Conclusion: ZS-TreeSeg方法为树冠实例分割和标签生成提供了一种无需训练、低成本的有效解决方案，具有良好的通用性和泛化能力。

Abstract: Individual tree crown segmentation is an important task in remote sensing for forest biomass estimation and ecological monitoring. However, accurate delineation in dense, overlapping canopies remains a bottleneck. While supervised deep learning methods suffer from high annotation costs and limited generalization, emerging foundation models (e.g., Segment Anything Model) often lack domain knowledge, leading to under-segmentation in dense clusters. To bridge this gap, we propose ZS-TreeSeg, a Zero-Shot framework that adapts from two mature tasks: 1) Canopy Semantic segmentation; and 2) Cells instance segmentation. By modeling tree crowns as star-convex objects within a topological flow field using Cellpose-SAM, the ZS-TreeSeg framework forces the mathematical separation of touching tree crown instances based on vector convergence. Experiments on the NEON and BAMFOREST datasets and visual inspection demonstrate that our framework generalizes robustly across diverse sensor types and canopy densities, which can offer a training-free solution for tree crown instance segmentation and labels generation.

</details>


### [67] [GTATrack: Winner Solution to SoccerTrack 2025 with Deep-EIoU and Global Tracklet Association](https://arxiv.org/abs/2602.00484)
*Rong-Lin Jian,Ming-Chi Luo,Chen-Wei Huang,Chia-Ming Lee,Yu-Fan Lin,Chih-Chung Hsu*

Main category: cs.CV

TL;DR: 本文提出了一种名为GTATrack的多目标追踪框架，专门应对体育运动中鱼眼摄像头下的追踪难题，并在SoccerTrack Challenge 2025中获得冠军。


<details>
  <summary>Details</summary>
Motivation: 体育运动多目标追踪非常具有挑战性，特别是在鱼眼摄像头导致图像几何畸变和尺度变化极大的场景下，现有方法容易出现目标遮挡、身份切换和追踪碎片化等问题，因此亟需更鲁棒的追踪方法。

Method: GTATrack由两大核心模块组成：一个基于Deep Expansion IoU（Deep-EIoU）的运动无关在线关联模块，用于短期内的目标匹配；一个全球轨迹关联（GTA）模块，实现更长时间尺度下的目标身份一致性。还引入了伪标签策略，提高检测器在小目标和畸变目标上的召回率。

Result: GTATrack在鱼眼摄像头下的足球多目标追踪任务中取得了优异成绩，获得HOTA评分0.60，以及显著降低的误报数（982），在SoccerTrack Challenge 2025比赛中排名第一。

Conclusion: GTATrack通过结合局部关联和全局推理，有效解决了身份切换、遮挡以及追踪碎片化等问题，推动了鱼眼视频下的多目标追踪技术发展。

Abstract: Multi-object tracking (MOT) in sports is highly challenging due to irregular player motion, uniform appearances, and frequent occlusions. These difficulties are further exacerbated by the geometric distortion and extreme scale variation introduced by static fisheye cameras. In this work, we present GTATrack, a hierarchical tracking framework that win first place in the SoccerTrack Challenge 2025. GTATrack integrates two core components: Deep Expansion IoU (Deep-EIoU) for motion-agnostic online association and Global Tracklet Association (GTA) for trajectory-level refinement. This two-stage design enables both robust short-term matching and long-term identity consistency. Additionally, a pseudo-labeling strategy is used to boost detector recall on small and distorted targets. The synergy between local association and global reasoning effectively addresses identity switches, occlusions, and tracking fragmentation. Our method achieved a winning HOTA score of 0.60 and significantly reduced false positives to 982, demonstrating state-of-the-art accuracy in fisheye-based soccer tracking. Our code is available at https://github.com/ron941/GTATrack-STC2025.

</details>


### [68] [Refining Strokes by Learning Offset Attributes between Strokes for Flexible Sketch Edit at Stroke-Level](https://arxiv.org/abs/2602.00489)
*Sicong Zang,Tao Sun,Cairong Yan*

Main category: cs.CV

TL;DR: 本文提出一种用于草图逐笔编辑的方法（SketchMod），通过对源笔画进行变换（缩放、旋转、平移），使其在目标草图中实现更精确、灵活的移植和替换，提升编辑的语义一致性和视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 现有的草图编辑方法仅通过重新定位源笔画，未能充分考虑源笔画在大小和方向上的差异，导致合成结果不自然或语义不一致。亟需能够自动调整源笔画属性，使编辑后的结果符合目标草图的整体风格和布局。

Method: 作者提出SketchMod，通过学习目标草图的格局，自动推断源笔画到目标的三个关键偏移属性：比例（scale）、方向（orientation）和位置（position）。具体做法包括1）调整比例以匹配空间尺寸、2）旋转以对齐局部几何、3）平移以契合语义布局，并开放对笔画属性的精细控制。

Result: 实验结果显示，SketchMod在草图逐笔编辑任务中相较于传统方法能实现更高的精度和灵活性，操作结果更加自然、语义与目标一致，视觉保真度更高。

Conclusion: SketchMod为草图逐笔编辑提供了一种高效且易于控制的新方法，显著提升了编辑过程的人机交互体验和最终的视觉效果，为相关领域带来新思路。

Abstract: Sketch edit at stroke-level aims to transplant source strokes onto a target sketch via stroke expansion or replacement, while preserving semantic consistency and visual fidelity with the target sketch. Recent studies addressed it by relocating source strokes at appropriate canvas positions. However, as source strokes could exhibit significant variations in both size and orientation, we may fail to produce plausible sketch editing results by merely repositioning them without further adjustments. For example, anchoring an oversized source stroke onto the target without proper scaling would fail to produce a semantically coherent outcome. In this paper, we propose SketchMod to refine the source stroke through transformation so as to align it with the target sketch's patterns, further realize flexible sketch edit at stroke-level. As the source stroke refinement is governed by the patterns of the target sketch, we learn three key offset attributes (scale, orientation and position) from the source stroke to another, and align it with the target by: 1) resizing to match spatial proportions by scale, 2) rotating to align with local geometry by orientation, and 3) displacing to meet with semantic layout by position. Besides, a stroke's profiles can be precisely controlled during sketch edit via the exposed captured stroke attributes. Experimental results indicate that SketchMod achieves precise and flexible performances on stroke-level sketch edit.

</details>


### [69] [HSSDCT: Factorized Spatial-Spectral Correlation for Hyperspectral Image Fusion](https://arxiv.org/abs/2602.00490)
*Chia-Ming Lee,Yu-Hao Ho,Yu-Fan Lin,Jen-Wei Lee,Li-Wei Kang,Chih-Chung Hsu*

Main category: cs.CV

TL;DR: 本文提出了一种新型的高光谱图像融合方法，既提高了效率又提升了重建质量，并公布了代码。


<details>
  <summary>Details</summary>
Motivation: 当前高光谱图像融合中的主流深度学习方法受限于感受野有限、光谱冗余和自注意力的高复杂度，导致效果和效率受限，因此需要一种新的方法来克服这些挑战。

Method: 提出了分层空间-光谱密集相关网络（HSSDCT）。该方法包括：1）分层密集残差Transformer模块（HDRTB），通过扩展窗口和密集残差连接实现多尺度特征聚合；2）空间-光谱相关层（SSCL），将空间和光谱依赖因子化，将自注意力复杂度降为线性，并缓解光谱冗余。

Result: 在多个基准数据集上的大量实验表明，该方法能以显著更低的计算成本实现更优的重建质量，并在高光谱图像融合任务中达到新的SOTA水平。

Conclusion: HSSDCT框架有效提升了高光谱图像融合的性能和效率，为后续研究提供了新方向。

Abstract: Hyperspectral image (HSI) fusion aims to reconstruct a high-resolution HSI (HR-HSI) by combining the rich spectral information of a low-resolution HSI (LR-HSI) with the fine spatial details of a high-resolution multispectral image (HR-MSI). Although recent deep learning methods have achieved notable progress, they still suffer from limited receptive fields, redundant spectral bands, and the quadratic complexity of self-attention, which restrict both efficiency and robustness. To overcome these challenges, we propose the Hierarchical Spatial-Spectral Dense Correlation Network (HSSDCT). The framework introduces two key modules: (i) a Hierarchical Dense-Residue Transformer Block (HDRTB) that progressively enlarges windows and employs dense-residue connections for multi-scale feature aggregation, and (ii) a Spatial-Spectral Correlation Layer (SSCL) that explicitly factorizes spatial and spectral dependencies, reducing self-attention to linear complexity while mitigating spectral redundancy. Extensive experiments on benchmark datasets demonstrate that HSSDCT delivers superior reconstruction quality with significantly lower computational costs, achieving new state-of-the-art performance in HSI fusion. Our code is available at https://github.com/jemmyleee/HSSDCT.

</details>


### [70] [RGBX-R1: Visual Modality Chain-of-Thought Guided Reinforcement Learning for Multimodal Grounding](https://arxiv.org/abs/2602.00504)
*Jiahe Wu,Bing Cao,Qilong Wang,Qinghua Hu,Dongdong Li,Pengfei Zhu*

Main category: cs.CV

TL;DR: 该论文提出了RGBX-R1框架，通过引入新的训练和推理策略，增强多模态大模型（MLLM）在RGB以外视觉模态（如红外、深度、事件数据）下的理解和推理能力，并在新的RGBX-Grounding基准上显著提升了任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM主要预训练于RGB模态，导致其在红外、深度、事件数据等非RGB视觉模态上的能力不足，而这些模态在复杂场景下极为重要。为提升模型对多种视觉模态的认知与推理能力，需有新的方法和评价体系。

Method: 提出RGBX-R1框架，引入UAV（Understand-Associate-Validate）提示策略形成视觉模态的思维链（VM-CoT），以迁移RGB能力到其他模态。采用两阶段训练：CS-SFT（冷启动监督微调）阶段利用VM-CoT进行基本模态认知训练；ST-RFT（时空强化微调）阶段用基于MuST奖励的时空强化学习进一步提升推理能力。

Result: 构建了首个RGBX-Grounding基准，并在三项RGBX定位任务上，模型性能超过基线22.71%。大量实验表明所提模型在多模态理解和空间感知上有明显优势。

Conclusion: RGBX-R1显著提升了MLLM在多模态视觉理解与推理能力，尤其是在非RGB数据场景下，为多模态模型扩展和复杂感知任务提供了有效途径。

Abstract: Multimodal Large Language Models (MLLM) are primarily pre-trained on the RGB modality, thereby limiting their performance on other modalities, such as infrared, depth, and event data, which are crucial for complex scenarios. To address this, we propose RGBX-R1, a framework to enhance MLLM's perception and reasoning capacities across various X visual modalities. Specifically, we employ an Understand-Associate-Validate (UAV) prompting strategy to construct the Visual Modality Chain-of-Thought (VM-CoT), which aims to expand the MLLMs' RGB understanding capability into X modalities. To progressively enhance reasoning capabilities, we introduce a two-stage training paradigm: Cold-Start Supervised Fine-Tuning (CS-SFT) and Spatio-Temporal Reinforcement Fine-Tuning (ST-RFT). CS-SFT supervises the reasoning process with the guidance of VM-CoT, equipping the MLLM with fundamental modality cognition. Building upon GRPO, ST-RFT employs a Modality-understanding Spatio-Temporal (MuST) reward to reinforce modality reasoning. Notably, we construct the first RGBX-Grounding benchmark, and extensive experiments verify our superiority in multimodal understanding and spatial perception, outperforming baselines by 22.71% on three RGBX grounding tasks.

</details>


### [71] [Sparse Shortcuts: Facilitating Efficient Fusion in Multimodal Large Language Models](https://arxiv.org/abs/2602.00505)
*Jingrui Zhang,Feng Liang,Yong Zhang,Wei Wang,Runhao Zeng,Xiping Hu*

Main category: cs.CV

TL;DR: 本文提出了SparseCut，一种改进大型多模态语言模型（MLLMs）视觉-语言融合能力的新架构，通过引入稀疏捷径提升多层次视觉特征的整合效率，有效提升了多模态模型的表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型主要关注模型扩展和高质量训练数据，忽视了视觉与语言等跨模态知识的深度融合，尤其在视觉-语言模型中过度依赖高层视觉特征，导致语义信息损失，影响模型理解能力。

Method: 作者提出SparseCut架构，在跨模态编码器与LLM间引入稀疏捷径连接，实现多层次（高、中、低层）视觉特征高效融合。同时，设计高效多粒度特征融合模块，在输送前融合视觉特征，以保护语言上下文且不增加模型输入长度和算力消耗。

Result: 实验结果显示，SparseCut可明显提升多模态大模型在多个主流多模态任务上的性能，具备较强的通用性和可扩展性，适用于多种不同的基础语言大模型。

Conclusion: SparseCut有效解决了视觉-语言模型中多层次语义融合不足的问题，既提升了模型理解能力，也控制了计算资源消耗，为大规模多模态理解任务提供了优质解决方案。

Abstract: With the remarkable success of large language models (LLMs) in natural language understanding and generation, multimodal large language models (MLLMs) have rapidly advanced in their ability to process data across multiple modalities. While most existing efforts focus on scaling up language models or constructing higher-quality training data, limited attention has been paid to effectively integrating cross-modal knowledge into the language space. In vision-language models, for instance, aligning modalities using only high-level visual features often discards the rich semantic information present in mid- and low-level features, limiting the model's ability of cross-modality understanding. To address this issue, we propose SparseCut, a general cross-modal fusion architecture for MLLMs, introducing sparse shortcut connections between the cross-modal encoder and the LLM. These shortcut connections enable the efficient and hierarchical integration of visual features at multiple levels, facilitating richer semantic fusion without increasing computational overhead. We further introduce an efficient multi-grained feature fusion module, which performs the fusion of visual features before routing them through the shortcuts. This preserves the original language context and does not increase the overall input length, thereby avoiding an increase in computational complexity for the LLM. Experiments demonstrate that SparseCut significantly enhances the performance of MLLMs across various multimodal benchmarks with generality and scalability for different base LLMs.

</details>


### [72] [DuoGen: Towards General Purpose Interleaved Multimodal Generation](https://arxiv.org/abs/2602.00508)
*Min Shi,Xiaohui Zeng,Jiannan Huang,Yin Cui,Francesco Ferroni,Jialuo Li,Shubham Pachori,Zhaoshuo Li,Yogesh Balaji,Haoxiang Wang,Tsung-Yi Lin,Xiao Fu,Yue Zhao,Chieh-Yun Chen,Ming-Yu Liu,Humphrey Shi*

Main category: cs.CV

TL;DR: DuoGen是一种通用的交错多模态生成框架，结合大规模高质量数据和新颖架构，在文本与图像生成任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前交错多模态生成模型受限于训练数据不足和模型基座能力有限，难以在通用指令下生成高质量文本和图像。

Method: 作者提出DuoGen，包括三大创新：（1）构建了大规模高质量指令调优数据集，结合网站信息和合成任务；（2）架构上结合了强视觉理解的多模态LLM和基于DiT的视频生成能力，采用解耦两阶段方法先调优MLLM再对齐DiT以生成交错图文序列；（3）提出了新的评测基准。

Result: 在公开和新提出的基准上，DuoGen在文本质量、图像保真度和图文一致性上均超越了现有开源模型，并在文本生成图像和图像编辑等任务中取得了SOTA表现。

Conclusion: DuoGen通过系统的数据、架构和评测创新，有效提升了交错多模态生成能力，为广泛场景下的统一生成任务开辟新方向。

Abstract: Interleaved multimodal generation enables capabilities beyond unimodal generation models, such as step-by-step instructional guides, visual planning, and generating visual drafts for reasoning. However, the quality of existing interleaved generation models under general instructions remains limited by insufficient training data and base model capacity. We present DuoGen, a general-purpose interleaved generation framework that systematically addresses data curation, architecture design, and evaluation. On the data side, we build a large-scale, high-quality instruction-tuning dataset by combining multimodal conversations rewritten from curated raw websites, and diverse synthetic examples covering everyday scenarios. Architecturally, DuoGen leverages the strong visual understanding of a pretrained multimodal LLM and the visual generation capabilities of a diffusion transformer (DiT) pretrained on video generation, avoiding costly unimodal pretraining and enabling flexible base model selection. A two-stage decoupled strategy first instruction-tunes the MLLM, then aligns DiT with it using curated interleaved image-text sequences. Across public and newly proposed benchmarks, DuoGen outperforms prior open-source models in text quality, image fidelity, and image-context alignment, and also achieves state-of-the-art performance on text-to-image and image editing among unified generation models. Data and code will be released at https://research.nvidia.com/labs/dir/duetgen/.

</details>


### [73] [SPARK: Stochastic Propagation via Affinity-guided Random walK for training-free unsupervised segmentation](https://arxiv.org/abs/2602.00516)
*Kunal Mahatha,Jose Dolz,Christian Desrosiers*

Main category: cs.CV

TL;DR: 本文提出了一种改进的无训练语义分割方法，通过将分割任务建模为扩散诱导的亲和图上的随机流平衡问题，并引入了自适应修剪的马尔可夫传播策略，显著提升了分割边界锐度和mask稳定性，实验取得了SOTA零样本分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有的无训练分割方法普遍基于光谱图划分（依赖扩散得到的亲和力），存在需预先设定聚类数、边界过度平滑、对噪声和多峰亲和分布敏感等问题，同时忽视了对局部邻域结构的建模。为缓解这些弊端，需重新审视亲和结构建模和信息传播策略。

Method: 作者将分割任务重新建模为扩散诱导亲和图上的随机流平衡问题。具体方法：1）通过稳定扩散结合局部邻域，提取稀疏而表现力强的亲和结构；2）设计带有自适应修剪策略的基于随机游走的Markov传播机制，抑制不可靠转移、增强确信亲和路径。

Result: 在7个主流语义分割基准上，提出的方法零样本分割性能超越此前所有光谱聚类类方法，分割边界更锐利，区域更连贯，mask更稳定。

Conclusion: 该方法有效突破了传统光谱分割的局限，通过注重局部邻域与全局扩散的结合、及随机游走中关键路径强化，带来了训练自由分割方法的新性能上限。

Abstract: We argue that existing training-free segmentation methods rely on an implicit and limiting assumption, that segmentation is a spectral graph partitioning problem over diffusion-derived affinities. Such approaches, based on global graph partitioning and eigenvector-based formulations of affinity matrices, suffer from several fundamental drawbacks, they require pre-selecting the number of clusters, induce boundary oversmoothing due to spectral relaxation, and remain highly sensitive to noisy or multi-modal affinity distributions. Moreover, many prior works neglect the importance of local neighborhood structure, which plays a crucial role in stabilizing affinity propagation and preserving fine-grained contours. To address these limitations, we reformulate training-free segmentation as a stochastic flow equilibrium problem over diffusion-induced affinity graphs, where segmentation emerges from a stochastic propagation process that integrates global diffusion attention with local neighborhoods extracted from stable diffusion, yielding a sparse yet expressive affinity structure. Building on this formulation, we introduce a Markov propagation scheme that performs random-walk-based label diffusion with an adaptive pruning strategy that suppresses unreliable transitions while reinforcing confident affinity paths. Experiments across seven widely used semantic segmentation benchmarks demonstrate that our method achieves state-of-the-art zero-shot performance, producing sharper boundaries, more coherent regions, and significantly more stable masks compared to prior spectral-clustering-based approaches.

</details>


### [74] [MRAD: Zero-Shot Anomaly Detection with Memory-Driven Retrieval](https://arxiv.org/abs/2602.00522)
*Chaoran Xu,Chengkan Lv,Qiyu Chen,Feng Zhang,Zhengtao Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的零样本异常检测方法MRAD，通过非参数化的记忆检索替代以往的模型拟合，显著降低了训练和推理成本，同时提升跨领域鲁棒性和检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本异常检测方法常依赖于预训练模型和复杂的提示学习或拟合技巧，导致训练与推理过程耗时高、泛化能力有限。该研究旨在提出一种高效、通用且跨领域稳定性强的方法。

Method: MRAD方法基于冻结的CLIP图像编码器，利用辅助数据构建图像级和像素级的两层特征记忆库，通过检索特征-标签对直接获取异常得分。提出三种实现：无训练的MRAD-TF、通过线性层微调检索度量的MRAD-FT、在语言提示中动态注入先验偏置增强泛化的MRAD-CLIP。

Result: MRAD框架在16个工业和医学数据集上实现了优异的异常分类和分割性能，无论在无训练还是微调场景下均优于现有方法。

Conclusion: MRAD证明充分利用原始数据的经验分布（记忆库检索）而非仅靠模型拟合，可以实现更强的异常检测性能。

Abstract: Zero-shot anomaly detection (ZSAD) often leverages pretrained vision or vision-language models, but many existing methods use prompt learning or complex modeling to fit the data distribution, resulting in high training or inference cost and limited cross-domain stability. To address these limitations, we propose Memory-Retrieval Anomaly Detection method (MRAD), a unified framework that replaces parametric fitting with a direct memory retrieval. The train-free base model, MRAD-TF, freezes the CLIP image encoder and constructs a two-level memory bank (image-level and pixel-level) from auxiliary data, where feature-label pairs are explicitly stored as keys and values. During inference, anomaly scores are obtained directly by similarity retrieval over the memory bank. Based on the MRAD-TF, we further propose two lightweight variants as enhancements: (i) MRAD-FT fine-tunes the retrieval metric with two linear layers to enhance the discriminability between normal and anomaly; (ii) MRAD-CLIP injects the normal and anomalous region priors from the MRAD-FT as dynamic biases into CLIP's learnable text prompts, strengthening generalization to unseen categories. Across 16 industrial and medical datasets, the MRAD framework consistently demonstrates superior performance in anomaly classification and segmentation, under both train-free and training-based settings. Our work shows that fully leveraging the empirical distribution of raw data, rather than relying only on model fitting, can achieve stronger anomaly detection performance. The code will be publicly released at https://github.com/CROVO1026/MRAD.

</details>


### [75] [SAGE: Accelerating Vision-Language Models via Entropy-Guided Adaptive Speculative Decoding](https://arxiv.org/abs/2602.00523)
*Yujia Tong,Tian Zhang,Yunyang Wan,Kaiwei Lin,Jingling Yuan,Chuang Hu*

Main category: cs.CV

TL;DR: 提出了一种基于动态决策树结构的推测式解码加速方法SAGE，实现了视觉-语言模型（VLM）更有效的推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有推测式解码方法采用静态树结构，不能自适应应对不同生成步骤上的预测难度，导致接纳长度和加速效果不理想。

Method: SAGE框架利用输出熵作为模型置信度指标，动态调节推测树结构：高置信度时用更深更窄的树以加大推测深度，置信度低则用更浅更宽的树来增加探索多样性。

Result: SAGE提高了平均接纳长度，并在多个基线任务上实现了远超静态树方法的推理加速。实验中，无输出质量损失下，LLaVA-OneVision-72B加速达3.36倍，Qwen2.5-VL-72B加速达3.18倍。

Conclusion: SAGE可实现推测解码速度的显著提升，无需牺牲输出质量，展现了按置信度自适应推测结构的有效性和实用价值。

Abstract: Speculative decoding has emerged as a promising approach to accelerate inference in vision-language models (VLMs) by enabling parallel verification of multiple draft tokens. However, existing methods rely on static tree structures that remain fixed throughout the decoding process, failing to adapt to the varying prediction difficulty across generation steps. This leads to suboptimal acceptance lengths and limited speedup. In this paper, we propose SAGE, a novel framework that dynamically adjusts the speculation tree structure based on real-time prediction uncertainty. Our key insight is that output entropy serves as a natural confidence indicator with strong temporal correlation across decoding steps. SAGE constructs deeper-narrower trees for high-confidence predictions to maximize speculation depth, and shallower-wider trees for uncertain predictions to diversify exploration. SAGE improves acceptance lengths and achieves faster acceleration compared to static tree baselines. Experiments on multiple benchmarks demonstrate the effectiveness of SAGE: without any loss in output quality, it delivers up to $3.36\times$ decoding speedup for LLaVA-OneVision-72B and $3.18\times$ for Qwen2.5-VL-72B.

</details>


### [76] [Enhancing Open-Vocabulary Object Detection through Multi-Level Fine-Grained Visual-Language Alignment](https://arxiv.org/abs/2602.00531)
*Tianyi Zhang,Antoine Simoulin,Kai Li,Sana Lakdawala,Shiqing Yu,Arpit Mittal,Hongyu Fu,Yu Lin*

Main category: cs.CV

TL;DR: 本论文提出了一种新的视觉-语言检测框架(VLDet)，通过改进特征金字塔和引入新的对齐损失，有效提升了开放词汇目标检测的表现。新方法在COCO2017和LVIS上检测新类别的表现均超过当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测只能检测预定义类别，限制了其在实际动态环境中的应用。开放词汇目标检测希望让模型能识别训练集中未出现的新类别，当前方法在视觉和语言特征对齐或主干网络适应方面效果有限。

Method: 作者提出Visual-Language Detection（VLDet）框架，1) 重新设计特征金字塔以实现更细粒度的视觉-语言对齐；2) 提出VL-PUB模块，有效利用CLIP视觉-语言知识并使主干网络适应检测任务；3) 提出SigRPN模块，通过sigmoid-based anchor-text对比损失以提升新类别检测能力。

Result: 在COCO2017新类别上获得58.7 AP，在LVIS新类别上获得24.8 AP，分别比当前最佳方法高27.6%和6.9%。在常规封闭类别检测任务上也有较好的零样本检测表现。

Conclusion: VLDet框架通过改进视觉-语言特征对齐与检测分支设计，有效提升了开放词汇目标检测的新类检测能力，并在多个数据集上超越现有方法，有望拓展目标检测在现实世界中的应用场景。

Abstract: Traditional object detection systems are typically constrained to predefined categories, limiting their applicability in dynamic environments. In contrast, open-vocabulary object detection (OVD) enables the identification of objects from novel classes not present in the training set. Recent advances in visual-language modeling have led to significant progress of OVD. However, prior works face challenges in either adapting the single-scale image backbone from CLIP to the detection framework or ensuring robust visual-language alignment. We propose Visual-Language Detection (VLDet), a novel framework that revamps feature pyramid for fine-grained visual-language alignment, leading to improved OVD performance. With the VL-PUB module, VLDet effectively exploits the visual-language knowledge from CLIP and adapts the backbone for object detection through feature pyramid. In addition, we introduce the SigRPN block, which incorporates a sigmoid-based anchor-text contrastive alignment loss to improve detection of novel categories. Through extensive experiments, our approach achieves 58.7 AP for novel classes on COCO2017 and 24.8 AP on LVIS, surpassing all state-of-the-art methods and achieving significant improvements of 27.6% and 6.9%, respectively. Furthermore, VLDet also demonstrates superior zero-shot performance on closed-set object detection.

</details>


### [77] [SADER: Structure-Aware Diffusion Framework with DEterministic Resampling for Multi-Temporal Remote Sensing Cloud Removal](https://arxiv.org/abs/2602.00536)
*Yifan Zhang,Qian Chen,Yi Liu,Wengen Li,Jihong Guan*

Main category: cs.CV

TL;DR: 本文提出了一种结构感知扩散模型（SADER），用于多时相遥感云去除，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 遥感图像常因云覆盖而质量下降，影响后续地球观测应用。现有的基于扩散模型的云去除方法在采样效率和结构、时序信息利用上存在不足。为了解决这些难题，作者提出改进的方法。

Method: 作者提出了SADER，包括一个多时相条件扩散网络（MTCDN），能通过时序融合和混合注意力机制充分捕捉多时相、多模态关联；引入云感知注意力损失函数，根据云厚度与亮度突显云主导区域；并为连续扩散模型设计重复确定性重采样策略，利用引导校正方式迭代优化输出。

Result: 在多个多时相遥感数据集上进行的实验显示，SADER在所有评估指标上均优于主流云去除方法，效果更好、更稳定。

Conclusion: SADER模型能够更高效并充分利用结构与时序信息，提升遥感云去除的效果，有望推动相关下游遥感应用的发展。

Abstract: Cloud contamination severely degrades the usability of remote sensing imagery and poses a fundamental challenge for downstream Earth observation tasks. Recently, diffusion-based models have emerged as a dominant paradigm for remote sensing cloud removal due to their strong generative capability and stable optimization. However, existing diffusion-based approaches often suffer from limited sampling efficiency and insufficient exploitation of structural and temporal priors in multi-temporal remote sensing scenarios. In this work, we propose SADER, a structure-aware diffusion framework for multi-temporal remote sensing cloud removal. SADER first develops a scalable Multi-Temporal Conditional Diffusion Network (MTCDN) to fully capture multi-temporal and multimodal correlations via temporal fusion and hybrid attention. Then, a cloud-aware attention loss is introduced to emphasize cloud-dominated regions by accounting for cloud thickness and brightness discrepancies. In addition, a deterministic resampling strategy is designed for continuous diffusion models to iteratively refine samples under fixed sampling steps by replacing outliers through guided correction. Extensive experiments on multiple multi-temporal datasets demonstrate that SADER consistently outperforms state-of-the-art cloud removal methods across all evaluation metrics. The code of SADER is publicly available at https://github.com/zyfzs0/SADER.

</details>


### [78] [NPNet: A Non-Parametric Network with Adaptive Gaussian-Fourier Positional Encoding for 3D Classification and Segmentation](https://arxiv.org/abs/2602.00542)
*Mohammad Saeid,Amir Salarpour,Pedram MohajerAnsari,Mert D. Pesé*

Main category: cs.CV

TL;DR: NPNet是一种完全无参数化的3D点云分类与分割方法，无需学习权重，结合自适应高斯-傅里叶位置编码实现了在多个数据集上的高效表现，特别适合小样本设置。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云分析方法大多依赖深度学习，需要大量参数和训练；而无参数方法不需要权重学习，但通常性能有限。作者希望开发一种无需学习权重又有优良表现的无参数点云处理方法，以改善记忆与推理效率，尤其是面对小样本情况。

Method: NPNet完全基于确定性算子如最远点采样、k近邻和池化来提取点特征。其核心方法是自适应带宽和高斯-余弦混合的高斯傅里叶位置编码，从输入几何结构中自适应选择参数，使模型对不同尺度与采样密度保持稳定。在分割任务中，额外加入固定频率的傅里叶特征提供全局上下文信息。

Result: NPNet在ModelNet40、ModelNet-R、ScanObjectNN与ShapeNetPart等数据集上，在无参数方法中表现突出，尤其是在ModelNet40小样本场景下效果显著。同时，其存储需求和推理时间优于以往的无参数方法。

Conclusion: NPNet无需任何学习权重，通过确定性特征提取和自适应位置编码，兼顾了准确性和高效性，为非参数化3D点云处理提供了新思路，具有良好的应用前景。

Abstract: We present NPNet, a fully non-parametric approach for 3D point-cloud classification and part segmentation. NPNet contains no learned weights; instead, it builds point features using deterministic operators such as farthest point sampling, k-nearest neighbors, and pooling. Our key idea is an adaptive Gaussian-Fourier positional encoding whose bandwidth and Gaussian-cosine mixing are chosen from the input geometry, helping the method remain stable across different scales and sampling densities. For segmentation, we additionally incorporate fixed-frequency Fourier features to provide global context alongside the adaptive encoding. Across ModelNet40/ModelNet-R, ScanObjectNN, and ShapeNetPart, NPNet achieves strong performance among non-parametric baselines, and it is particularly effective in few-shot settings on ModelNet40. NPNet also offers favorable memory use and inference time compared to prior non-parametric methods

</details>


### [79] [Learning to Decode Against Compositional Hallucination in Video Multimodal Large Language Models](https://arxiv.org/abs/2602.00559)
*Wenbin Xing,Quanxing Zha,Lizheng Zu,Mengran Li,Ming Li,Junchi Yan*

Main category: cs.CV

TL;DR: 本文提出了OmniVCHall基准，首次系统性评测视频多模态大模型在多种视频幻觉（尤其是复合型幻觉）下的表现，并提出TriCD方法有效提升模型应对能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视频幻觉研究主要聚焦于单一错误类型，忽略了由多种空间和时间因素交互造成的复合型幻觉，因此需要一个全面的基准和更有效的方法来检测与缓解此类问题。

Method: 1）设计OmniVCHall基准，涵盖多视频领域、创新幻觉类型与细致分类，并引入对抗性答案防止套路推理；2）提出TriCD：包含三路径校准机制的对比解码框架，结合自适应扰动生成负样本、显著性增强模块强化视觉证据，并用强化学习进行优化。

Result: 在39个主流VLLMs上评测发现，即使是最先进模型亦在复合幻觉任务上表现大幅下降。应用TriCD处理后，在两类主流架构下准确率平均提升10%以上。

Conclusion: OmniVCHall为多模态大模型视频幻觉提供了系统性测试标准，TriCD显著提升模型在复合幻觉场景下的鲁棒性。数据和代码已公开。

Abstract: Current research on video hallucination mitigation primarily focuses on isolated error types, leaving compositional hallucinations, arising from incorrect reasoning over multiple interacting spatial and temporal factors largely underexplored. We introduce OmniVCHall, a benchmark designed to systematically evaluate both isolated and compositional hallucinations in video multimodal large language models (VLLMs). OmniVCHall spans diverse video domains, introduces a novel camera-based hallucination type, and defines a fine-grained taxonomy, together with adversarial answer options (e.g., "All are correct" and "None of the above") to prevent shortcut reasoning. The evaluations of 39 representative VLLMs reveal that even advanced models (e.g., Qwen3-VL and GPT-5) exhibit substantial performance degradation. We propose TriCD, a contrastive decoding framework with a triple-pathway calibration mechanism. An adaptive perturbation controller dynamically selects distracting operations to construct negative video variants, while a saliency-guided enhancement module adaptively reinforces grounded token-wise visual evidences. These components are optimized via reinforcement learning to encourage precise decision-making under compositional hallucination settings. Experimental results show that TriCD consistently improves performance across two representative backbones, achieving an average accuracy improvement of over 10%. The data and code can be find at https://github.com/BMRETURN/OmniVCHall.

</details>


### [80] [GLAD: Generative Language-Assisted Visual Tracking for Low-Semantic Templates](https://arxiv.org/abs/2602.00570)
*Xingyu Luo,Yidong Cai,Jie Liu,Jie Tang,Gangshan Wu,Limin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为GLAD的生成式语言辅助视觉追踪模型，通过扩散模型实现文本与图像的多模态生成融合，显著提升了低语义图像下的追踪性能，并在多项基准上取得了最优效果。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言追踪方法在处理模糊、低分辨率等低语义图像时，跨模态理解能力受限，直接将文本与图像特征融合的效果不佳，急需更有效的融合策略。

Method: 提出GLAD模型，利用扩散模型进行生成式的文本与模板图像多模态融合，增强二者兼容性，并提升图像语义信息，帮助恢复模糊、语义不明的模板图像。

Result: GLAD方法在多项公开基准上实现了最新最好（state-of-the-art）的追踪表现，同时具有较快的推理速度。

Conclusion: 生成式多模态融合方法能有效提升低语义图像的视觉-语言追踪效果，对未来相关任务具有很大潜力，并将开源代码和模型。

Abstract: Vision-language tracking has gained increasing attention in many scenarios. This task simultaneously deals with visual and linguistic information to localize objects in videos. Despite its growing utility, the development of vision-language tracking methods remains in its early stage. Current vision-language trackers usually employ Transformer architectures for interactive integration of template, search, and text features. However, persistent challenges about low-semantic images including prevalent image blurriness, low resolution and so on, may compromise model performance through degraded cross-modal understanding. To solve this problem, language assistance is usually used to deal with the obstacles posed by low-semantic images. However, due to the existing gap between current textual and visual features, direct concatenation and fusion of these features may have limited effectiveness. To address these challenges, we introduce a pioneering Generative Language-AssisteD tracking model, GLAD, which utilizes diffusion models for the generative multi-modal fusion of text description and template image to bolster compatibility between language and image and enhance template image semantic information. Our approach demonstrates notable improvements over the existing fusion paradigms. Blurry and semantically ambiguous template images can be restored to improve multi-modal features in the generative fusion paradigm. Experiments show that our method establishes a new state-of-the-art on multiple benchmarks and achieves an impressive inference speed. The code and models will be released at: https://github.com/Confetti-lxy/GLAD

</details>


### [81] [Bridging Degradation Discrimination and Generation for Universal Image Restoration](https://arxiv.org/abs/2602.00579)
*JiaKui Hu,Zhengjian Yao,Lujia Jin,Yanye Lu*

Main category: cs.CV

TL;DR: 本文提出了一种通用的图像复原方法BDG（Bridging Degradation discrimination and Generation），通过创新的降质判别与生成融合机制，在多个图像退化与复原任务中显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 通用图像复原任务面临如何区分多种图像降质类型及水平，并据此有效复原的问题。现有方法难以兼顾降质判别与高质量图像生成，两者之间存在脱节。

Method: 提出MAS-GLCM（多角度多尺度灰度共生矩阵），用于细粒度判别降质类型与水平。将扩散模型训练过程分为生成、桥接和复原三个阶段，在不改变模型结构的前提下，将MAS-GLCM判别信息注入扩散模型，提升其在复杂退化场景下的泛化与复原能力。

Result: BDG方法在all-in-one复原和真实超分辨任务上取得显著提升，实现了真值保真度与感知质量的双赢，超越了已有方法。

Conclusion: 通过创新性地结合降质判别与高质量图像生成信息，BDG方法无需修改架构即可实现更强大、泛化更好的图像复原能力，对复杂退化及多任务场景具有直接应用潜力。

Abstract: Universal image restoration is a critical task in low-level vision, requiring the model to remove various degradations from low-quality images to produce clean images with rich detail. The challenges lie in sampling the distribution of high-quality images and adjusting the outputs on the basis of the degradation. This paper presents a novel approach, Bridging Degradation discrimination and Generation (BDG), which aims to address these challenges concurrently. First, we propose the Multi-Angle and multi-Scale Gray Level Co-occurrence Matrix (MAS-GLCM) and demonstrate its effectiveness in performing fine-grained discrimination of degradation types and levels. Subsequently, we divide the diffusion training process into three distinct stages: generation, bridging, and restoration. The objective is to preserve the diffusion model's capability of restoring rich textures while simultaneously integrating the discriminative information from the MAS-GLCM into the restoration process. This enhances its proficiency in addressing multi-task and multi-degraded scenarios. Without changing the architecture, BDG achieves significant performance gains in all-in-one restoration and real-world super-resolution tasks, primarily evidenced by substantial improvements in fidelity without compromising perceptual quality. The code and pretrained models are provided in https://github.com/MILab-PKU/BDG.

</details>


### [82] [MAUGen: A Unified Diffusion Approach for Multi-Identity Facial Expression and AU Label Generation](https://arxiv.org/abs/2602.00583)
*Xiangdong Li,Ye Lou,Ao Gao,Wei Zhang,Siyang Song*

Main category: cs.CV

TL;DR: 本文提出了MAUGen框架，实现了基于文本描述生成具有精确表情动作单元标签（包含出现与强度）和身份多样的高质量人脸图像。新方法提升了数据的多样性，为后续AU识别系统发展提供了基础。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏大规模、人口多样且带有精确动作单元（AU）信息的人脸图像数据，这成为开发广泛适用AU识别系统的瓶颈。因此需要创造新的生成式框架以丰富相关数据资源。

Method: 提出MAUGen多模态框架，并设计两大模块：1）多模态表示学习模块（MRL），联合表征文本、身份、表情和AU标签；2）基于扩散模型的图像标签生成模块（DIG），可以从统一潜空间解码为身份多样、标签一致的人脸与AU成对样本。

Result: 基于MAUGen的方法生成了大规模、多身份、AU注释齐全的合成脸部动作单元数据集（MIFA），并在多项实验中展现出比现有方法更好的图像质量、多样性及标签一致性。

Conclusion: MAUGen为面部表情动作单元数据生成提供了新的强大工具，有效缓解了样本不足及多样性不足的问题。实验结果显示其在合成高质量、多样化和语义一致的人脸及AU标签方面优于现有方法，有助于推动AU识别等下游任务的发展。

Abstract: The lack of large-scale, demographically diverse face images with precise Action Unit (AU) occurrence and intensity annotations has long been recognized as a fundamental bottleneck in developing generalizable AU recognition systems. In this paper, we propose MAUGen, a diffusion-based multi-modal framework that jointly generates a large collection of photorealistic facial expressions and anatomically consistent AU labels, including both occurrence and intensity, conditioned on a single descriptive text prompt. Our MAUGen involves two key modules: (1) a Multi-modal Representation Learning (MRL) module that captures the relationships among the paired textual description, facial identity, expression image, and AU activations within a unified latent space; and (2) a Diffusion-based Image label Generator (DIG) that decodes the joint representation into aligned facial image-label pairs across diverse identities. Under this framework, we introduce Multi-Identity Facial Action (MIFA), a large-scale multimodal synthetic dataset featuring comprehensive AU annotations and identity variations. Extensive experiments demonstrate that MAUGen outperforms existing methods in synthesizing photorealistic, demographically diverse facial images along with semantically aligned AU labels.

</details>


### [83] [From Pixels to Facts (Pix2Fact): Benchmarking Multi-Hop Reasoning for Fine-Grained Visual Fact Checking](https://arxiv.org/abs/2602.00593)
*Yifan Jiang,Cong Zhang,Bofei Zhang,Yifan Yang,Bingzhang Wang,Yew-Soon Ong*

Main category: cs.CV

TL;DR: 提出了用于评估视觉语言模型（VLMs）详细视觉定位与知识推理结合能力的新基准Pix2Fact。实验发现当前模型在该基准上表现远不及人类，显示出现有VLMs的明显短板。


<details>
  <summary>Details</summary>
Motivation: 现有基准只能单独评估视觉定位或推理，但不能测试两者协同能力，难以反映现实复杂场景需求。作者希望通过新基准推动模型在细致视觉理解与多步逻辑推理结合方面的进步。

Method: 构建了包含1000张高分辨率图片、覆盖8类日常场景的新VQA基准Pix2Fact。问题由顶尖博士与专业团队严密设计，要求视觉细节定位、外部知识融合与多跳推理，并在9个主流VLM模型和人类上进行评测。

Result: 最强模型仅获得24.0%平均准确率，而人类准确率为56%，表明Pix2Fact对当前VLMs构成极大挑战。

Conclusion: Pix2Fact充分揭示了当前视觉语言模型在人类级视觉理解上的局限，有望成为推动多模态智能体融合细粒度感知与深层推理能力的重要基准。

Abstract: Despite progress on general tasks, VLMs struggle with challenges demanding both detailed visual grounding and deliberate knowledge-based reasoning, a synergy not captured by existing benchmarks that evaluate these skills separately. To close this gap, we introduce Pix2Fact, a new visual question-answering benchmark designed to evaluate expert-level perception and knowledge-intensive multi-hop reasoning. Pix2Fact contains 1,000 high-resolution (4K+) images spanning 8 daily-life scenarios and situations, with questions and answers meticulously crafted by annotators holding PhDs from top global universities working in partnership with a professional data annotation firm. Each question requires detailed visual grounding, multi-hop reasoning, and the integration of external knowledge to answer. Our evaluation of 9 state-of-the-art VLMs, including proprietary models like Gemini-3-Pro and GPT-5, reveals the substantial challenge posed by Pix2Fact: the most advanced model achieves only 24.0% average accuracy, in stark contrast to human performance of 56%. This significant gap underscores the limitations of current models in replicating human-level visual comprehension. We believe Pix2Fact will serve as a critical benchmark to drive the development of next-generation multimodal agents that combine fine-grained perception with robust, knowledge-based reasoning.

</details>


### [84] [Tune-Your-Style: Intensity-tunable 3D Style Transfer with Gaussian Splatting](https://arxiv.org/abs/2602.00618)
*Yian Zhao,Rushi Ye,Ruochong Zheng,Zesen Cheng,Chaoran Feng,Jiashu Yang,Pengchong Qiao,Chang Liu,Jie Chen*

Main category: cs.CV

TL;DR: 本文提出了一种可调节风格强度的3D风格迁移方法“Tune-Your-Style”，实现了3D场景风格效果的自定义调整，兼顾了内容与风格，并提供了稳定高效的训练过程。


<details>
  <summary>Details</summary>
Motivation: 3D风格迁移的核心挑战是如何平衡3D内容本身和参考风格的图案、色彩。现有方法无法灵活满足不同用户对内容-风格平衡的差异化需求，仅能输出固定的风格结果，因此需要一种具备风格强度可调节性的3D风格迁移方案，以增强自定义能力。

Method: 作者引入了高斯神经元对风格强度进行显式建模，并设计了可学习的风格调节器，实现了风格注入的强度可调。此外，提出可调节风格化引导机制，通过跨视图风格对齐，从扩散模型获得多视角一致的风格化视图，然后采用两阶段优化策略，来平衡风格引导和内容保留，确保训练和推理的稳定高效。

Result: 实验结果显示，该方法不仅能生成视觉上令人满意的3D风格化结果，还能根据用户需要灵活调节风格强度，显著提升了3D风格迁移的自定义能力。

Conclusion: 提出的Tune-Your-Style方法有效实现了3D风格迁移中内容与风格的灵活平衡，提升了个性化和实用性，推动了3DGS风格迁移技术的发展。

Abstract: 3D style transfer refers to the artistic stylization of 3D assets based on reference style images. Recently, 3DGS-based stylization methods have drawn considerable attention, primarily due to their markedly enhanced training and rendering speeds. However, a vital challenge for 3D style transfer is to strike a balance between the content and the patterns and colors of the style. Although the existing methods strive to achieve relatively balanced outcomes, the fixed-output paradigm struggles to adapt to the diverse content-style balance requirements from different users. In this work, we introduce a creative intensity-tunable 3D style transfer paradigm, dubbed \textbf{Tune-Your-Style}, which allows users to flexibly adjust the style intensity injected into the scene to match their desired content-style balance, thus enhancing the customizability of 3D style transfer. To achieve this goal, we first introduce Gaussian neurons to explicitly model the style intensity and parameterize a learnable style tuner to achieve intensity-tunable style injection. To facilitate the learning of tunable stylization, we further propose the tunable stylization guidance, which obtains multi-view consistent stylized views from diffusion models through cross-view style alignment, and then employs a two-stage optimization strategy to provide stable and efficient guidance by modulating the balance between full-style guidance from the stylized views and zero-style guidance from the initial rendering. Extensive experiments demonstrate that our method not only delivers visually appealing results, but also exhibits flexible customizability for 3D style transfer. Project page is available at https://zhao-yian.github.io/TuneStyle.

</details>


### [85] [Towards Interpretable Hallucination Analysis and Mitigation in LVLMs via Contrastive Neuron Steering](https://arxiv.org/abs/2602.00621)
*Guangtao Lyu,Xinyi Cheng,Qi Liu,Chenghao Xu,Jiexi Yan,Muli Yang,Fen Fang,Cheng Deng*

Main category: cs.CV

TL;DR: 该论文采用稀疏自编码器（SAEs）分解视觉大模型（LVLMs）内部表征，发现幻觉现象与特定神经元异常激活相关，并提出了可控干预方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法只针对输出层减少幻觉，未深入分析幻觉产生的内部机制。作者希望通过分析内部表征，理解并从源头缓解幻觉问题。

Method: 通过引入稀疏自编码器，将LVLMs的密集视觉嵌入分解为稀疏、可解释的神经元类型。利用对比分析（CNS），识别和干预图像特异性神经元，对其进行选择性增强或抑制，以改善视觉理解和减少幻觉。

Result: 神经元级分析揭示，幻觉通常源自图像特异性神经元的异常激活或失调。CNS方法可有效增强有用神经元、抑制噪声激活，从而减少幻觉，并在多项基准任务中验证了效果。

Conclusion: 论文证明，从神经元层面对LVLMs进行表征与干预，可显著减少幻觉并增强多模态理解，且方法与现有后解码阶段技术兼容。

Abstract: LVLMs achieve remarkable multimodal understanding and generation but remain susceptible to hallucinations. Existing mitigation methods predominantly focus on output-level adjustments, leaving the internal mechanisms that give rise to these hallucinations largely unexplored. To gain a deeper understanding, we adopt a representation-level perspective by introducing sparse autoencoders (SAEs) to decompose dense visual embeddings into sparse, interpretable neurons. Through neuron-level analysis, we identify distinct neuron types, including always-on neurons and image-specific neurons. Our findings reveal that hallucinations often result from disruptions or spurious activations of image-specific neurons, while always-on neurons remain largely stable. Moreover, selectively enhancing or suppressing image-specific neurons enables controllable intervention in LVLM outputs, improving visual grounding and reducing hallucinations. Building on these insights, we propose Contrastive Neuron Steering (CNS), which identifies image-specific neurons via contrastive analysis between clean and noisy inputs. CNS selectively amplifies informative neurons while suppressing perturbation-induced activations, producing more robust and semantically grounded visual representations. This not only enhances visual understanding but also effectively mitigates hallucinations. By operating at the prefilling stage, CNS is fully compatible with existing decoding-stage methods. Extensive experiments on both hallucination-focused and general multimodal benchmarks demonstrate that CNS consistently reduces hallucinations while preserving overall multimodal understanding.

</details>


### [86] [FaceSnap: Enhanced ID-fidelity Network for Tuning-free Portrait Customization](https://arxiv.org/abs/2602.00627)
*Benxiang Zhai,Yifang Xu,Guofeng Zhang,Yang Li,Sidan Du*

Main category: cs.CV

TL;DR: 本文提出了FaceSnap方法，可通过单张参考图快速生成高一致性的个性化人像图片，兼顾效率和细节还原，超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有个性化图像生成方法要么需大量微调、泛化能力差，要么脸部细节还原度不高。本文旨在解决高效、通用且保真的人像定制生成问题。

Method: 基于Stable Diffusion，提出FaceSnap方法，只需单张参考图像，一步推理即可得结果。创新点包括：设计了Facial Attribute Mixer，融合低高级人脸特征；引入Landmark Predictor，通过地标点实现多样化姿态和空间控制；最终通过ID-preserving模块，将上述特征输入UNet。

Result: 实验表明FaceSnap在个性化与定制人像生成任务上效果极佳，生成结果比现有先进方法更加真实且细节丰富。

Conclusion: FaceSnap方法实现了无需繁琐微调即能高效、精准地进行个性化人像生成，具备良好的扩展性和应用前景，在该领域超过现有技术水平。

Abstract: Benefiting from the significant advancements in text-to-image diffusion models, research in personalized image generation, particularly customized portrait generation, has also made great strides recently. However, existing methods either require time-consuming fine-tuning and lack generalizability or fail to achieve high fidelity in facial details. To address these issues, we propose FaceSnap, a novel method based on Stable Diffusion (SD) that requires only a single reference image and produces extremely consistent results in a single inference stage. This method is plug-and-play and can be easily extended to different SD models. Specifically, we design a new Facial Attribute Mixer that can extract comprehensive fused information from both low-level specific features and high-level abstract features, providing better guidance for image generation. We also introduce a Landmark Predictor that maintains reference identity across landmarks with different poses, providing diverse yet detailed spatial control conditions for image generation. Then we use an ID-preserving module to inject these into the UNet. Experimental results demonstrate that our approach performs remarkably in personalized and customized portrait generation, surpassing other state-of-the-art methods in this domain.

</details>


### [87] [S$^3$POT: Contrast-Driven Face Occlusion Segmentation via Self-Supervised Prompt Learning](https://arxiv.org/abs/2602.00635)
*Lingsong Wang,Mancheng Meng,Ziyan Wu,Terrence Chen,Fan Yang,Dinggang Shen*

Main category: cs.CV

TL;DR: 本文提出了S^3POT框架，有效提升面部图像在有遮挡时的分割效果。该框架结合了人脸生成和自监督空间提示，解决了遮挡区域常被误判为面部组成部分的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸分割方法在遇到遮挡物时容易将其误分类为面部成分，主要因为遮挡物类型复杂并且人工标注遮挡掩码代价高昂。作者希望无需收集或标注所有可能的遮挡类型的情况下，提升有遮挡人脸的分割性能。

Method: 提出S^3POT框架，包括三个模块：参考图像生成（RF）、特征增强（FE）、提示选择（PS）。利用人脸生成器重建无遮挡的人脸作为参照，比较原图与参考图特征以生成空间提示，进一步用这些提示改进分割模型的注意力机制，并通过自监督学习完成遮挡分割。整个流程不依赖于遮挡掩码的标注。

Result: 在作者自建的数据集上，S^3POT在遮挡人脸分割任务上显著优于现有方法。每个模块的有效性也通过消融实验得到了验证。

Conclusion: S^3POT框架通过引入生成和自监督提示显著提升了有遮挡人脸图像的分割效果，无需耗时的遮挡掩码标注，未来有望拓展到更广泛的复杂场景中。

Abstract: Existing face parsing methods usually misclassify occlusions as facial components. This is because occlusion is a high-level concept, it does not refer to a concrete category of object. Thus, constructing a real-world face dataset covering all categories of occlusion object is almost impossible and accurate mask annotation is labor-intensive. To deal with the problems, we present S$^3$POT, a contrast-driven framework synergizing face generation with self-supervised spatial prompting, to achieve occlusion segmentation. The framework is inspired by the insights: 1) Modern face generators' ability to realistically reconstruct occluded regions, creating an image that preserve facial geometry while eliminating occlusion, and 2) Foundation segmentation models' (e.g., SAM) capacity to extract precise mask when provided with appropriate prompts. In particular, S$^3$POT consists of three modules: Reference Generation (RF), Feature enhancement (FE), and Prompt Selection (PS). First, a reference image is produced by RF using structural guidance from parsed mask. Second, FE performs contrast of tokens between raw and reference images to obtain an initial prompt, then modifies image features with the prompt by cross-attention. Third, based on the enhanced features, PS constructs a set of positive and negative prompts and screens them with a self-attention network for a mask decoder. The network is learned under the guidance of three novel and complementary objective functions without occlusion ground truth mask involved. Extensive experiments on a dedicatedly collected dataset demonstrate S$^3$POT's superior performance and the effectiveness of each module.

</details>


### [88] [Any3D-VLA: Enhancing VLA Robustness via Diverse Point Clouds](https://arxiv.org/abs/2602.00807)
*Xianzhe Fan,Shengliang Deng,Xiaoyang Wu,Yuxiang Lu,Zhuoling Li,Mi Yan,Yujia Zhang,Zhizheng Zhang,He Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 该论文研究如何将3D信息引入视觉-语言-行为（VLA）模型以提升其空间理解能力，提出了统一多种点云输入的Any3D-VLA框架，并在仿真与现实场景中验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 目前VLA模型多以2D图像为输入，导致在复杂场景中的空间理解受限。3D信息能显著提升模型表现，但现有3D数据稀缺且存在领域间差异，阻碍了3D视觉信息的有效利用。

Method: 提出Any3D-VLA方法，将模拟器、传感器和模型估算的点云统一为训练输入，从而生成多样化的3D视觉表示，并与2D特征融合。同时，设计端到端管线以学习领域无关的3D表示，缓解数据和领域差异带来的问题。

Result: 实验结果显示，将视觉输入显式提升为点云能有效增强与2D信息的互补性，Any3D-VLA在仿真和真实世界中均取得优于基线的性能，并成功减轻了领域间差异影响。

Conclusion: 引入多种点云信息与2D特征融合，可以显著提升VLA模型的空间能力和泛化能力，为跨环境视觉理解和行为决策提供了新思路与有效方法。

Abstract: Existing Vision-Language-Action (VLA) models typically take 2D images as visual input, which limits their spatial understanding in complex scenes. How can we incorporate 3D information to enhance VLA capabilities? We conduct a pilot study across different observation spaces and visual representations. The results show that explicitly lifting visual input into point clouds yields representations that better complement their corresponding 2D representations. To address the challenges of (1) scarce 3D data and (2) the domain gap induced by cross-environment differences and depth-scale biases, we propose Any3D-VLA. It unifies the simulator, sensor, and model-estimated point clouds within a training pipeline, constructs diverse inputs, and learns domain-agnostic 3D representations that are fused with the corresponding 2D representations. Simulation and real-world experiments demonstrate Any3D-VLA's advantages in improving performance and mitigating the domain gap. Our project homepage is available at https://xianzhefan.github.io/Any3D-VLA.github.io.

</details>


### [89] [VIZOR: Viewpoint-Invariant Zero-Shot Scene Graph Generation for 3D Scene Reasoning](https://arxiv.org/abs/2602.00637)
*Vivek Madhavaram,Vartika Sengar,Arkadipta De,Charu Sharma*

Main category: cs.CV

TL;DR: 作者提出了VIZOR框架，实现了无需训练直接从3D场景生成视角不变的场景图，并显著提升了下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前3D场景理解方法在生成场景图时依赖多种输入且易受视角影响，空间关系如左右方向通常不稳定，且泛化能力较差。作者希望解决这些不足，提高模型的通用性和准确性。

Method: 提出VIZOR，一个训练自由、端到端的框架，直接从原始3D场景生成稠密且视角不变的场景图。该框架通过以各对象朝向为参考定义空间关系，并以零样本方式推理开放词汇的空间及临近关系，无需标注数据。

Result: 在场景图生成和基于查询的目标定位等下游任务上，VIZOR均优于现有方法。在Replica和Nr3D数据集的零样本目标定位任务上分别提升了22%和4.81%的准确率。

Conclusion: VIZOR框架有效解决了视角依赖和训练数据需求问题，在3D场景理解任务上实现了更准确且一致的场景图生成和推理能力，对后续相关任务有重要促进作用。

Abstract: Scene understanding and reasoning has been a fundamental problem in 3D computer vision, requiring models to identify objects, their properties, and spatial or comparative relationships among the objects. Existing approaches enable this by creating scene graphs using multiple inputs such as 2D images, depth maps, object labels, and annotated relationships from specific reference view. However, these methods often struggle with generalization and produce inaccurate spatial relationships like "left/right", which become inconsistent across different viewpoints. To address these limitations, we propose Viewpoint-Invariant Zero-shot scene graph generation for 3D scene Reasoning (VIZOR). VIZOR is a training-free, end-to-end framework that constructs dense, viewpoint-invariant 3D scene graphs directly from raw 3D scenes. The generated scene graph is unambiguous, as spatial relationships are defined relative to each object's front-facing direction, making them consistent regardless of the reference view. Furthermore, it infers open-vocabulary relationships that describe spatial and proximity relationships among scene objects without requiring annotated training data. We conduct extensive quantitative and qualitative evaluations to assess the effectiveness of VIZOR in scene graph generation and downstream tasks, such as query-based object grounding. VIZOR outperforms state-of-the-art methods, showing clear improvements in scene graph generation and achieving 22% and 4.81% gains in zero-shot grounding accuracy on the Replica and Nr3D datasets, respectively.

</details>


### [90] [VVLoc: Prior-free 3-DoF Vehicle Visual Localization](https://arxiv.org/abs/2602.00810)
*Ze Huang,Zhongyang Xiao,Mingliang Song,Longan Yang,Hongyuan Yuan,Li Sun*

Main category: cs.CV

TL;DR: VVLoc提出了一种可实现同时拓扑与度量定位的统一神经网络，支持多目摄像头，赋予定位置信度评估，训练高效，性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有的自动驾驶定位方法通常需要分别处理拓扑定位与度量定位，多为单目方案，且依赖额外先验（如3D语义或位姿），缺乏置信度量，实际应用受限。因此，亟须一种统一、高效、可信的定位方案。

Method: 作者提出VVLoc系统，通过单一神经网络实现拓扑与度量定位联合，能够评估视觉观测之间的地理接近，并用匹配策略估算相对位姿，输出定位置信度。其只需视觉数据对和真实位姿进行训练，无需额外复杂数据。支持多摄像头输入。

Result: VVLoc在公开数据集和自采高难度数据集上均展示了业界领先的定位精度，无论拓扑定位还是度量定位任务表现突出。

Conclusion: VVLoc有效统一了多摄像头下的拓扑与度量定位，并提供置信度输出，训练效率高，应用前景广阔，具备实际落地能力。

Abstract: Localization is a critical technology in autonomous driving, encompassing both topological localization, which identifies the most similar map keyframe to the current observation, and metric localization, which provides precise spatial coordinates. Conventional methods typically address these tasks independently, rely on single-camera setups, and often require additional 3D semantic or pose priors, while lacking mechanisms to quantify the confidence of localization results, making them less feasible for real industrial applications. In this paper, we propose VVLoc, a unified pipeline that employs a single neural network to concurrently achieve topological and metric vehicle localization using multi-camera system. VVLoc first evaluates the geo-proximity between visual observations, then estimates their relative metric poses using a matching strategy, while also providing a confidence measure. Additionally, the training process for VVLoc is highly efficient, requiring only pairs of visual data and corresponding ground-truth poses, eliminating the need for complex supplementary data. We evaluate VVLoc not only on the publicly available datasets, but also on a more challenging self-collected dataset, demonstrating its ability to deliver state-of-the-art localization accuracy across a wide range of localization tasks.

</details>


### [91] [Diff-PC: Identity-preserving and 3D-aware Controllable Diffusion for Zero-shot Portrait Customization](https://arxiv.org/abs/2602.00639)
*Yifang Xu,Benxiang Zhai,Chenyu Zhang,Ming Li,Yang Li,Sidan Du*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的零样本人像定制方法Diff-PC，能够在保证身份一致性的情况下，实现对面部特征、表情、姿态和背景的高自由度定制，且优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有人像定制方法在身份特征保留与面部控制方面存在不足，难以同时保证真实还原身份和灵活控制表情、姿态。为此，亟需一种方法兼顾ID保真和个性化定制。

Method: 提出Diff-PC框架：首先利用3D人脸预测器重建包含参照身份、目标表情和姿态的3D人脸先验；设计ID-Encoder融合局部与全局面部特征，提升细节提取能力；通过ID-Ctrl模块引导ID特征对齐；引入ID-Injector强化身份保真与面部可控性。此外，采用作者自建的ID为中心的数据集进行训练，提高了人脸相似度和文本到图像的对齐效果。

Result: 通过大量实验，Diff-PC在身份保真、面部控制和文本-图像一致性上均超越当前先进方法，并且兼容多种基础生成模型，展现出更优的多样性和实用性。

Conclusion: Diff-PC方法有效提升了人像定制任务在身份还原和个性化定制方面的表现，对相关实际应用具有推广价值。

Abstract: Portrait customization (PC) has recently garnered significant attention due to its potential applications. However, existing PC methods lack precise identity (ID) preservation and face control. To address these tissues, we propose Diff-PC, a diffusion-based framework for zero-shot PC, which generates realistic portraits with high ID fidelity, specified facial attributes, and diverse backgrounds. Specifically, our approach employs the 3D face predictor to reconstruct the 3D-aware facial priors encompassing the reference ID, target expressions, and poses. To capture fine-grained face details, we design ID-Encoder that fuses local and global facial features. Subsequently, we devise ID-Ctrl using the 3D face to guide the alignment of ID features. We further introduce ID-Injector to enhance ID fidelity and facial controllability. Finally, training on our collected ID-centric dataset improves face similarity and text-to-image (T2I) alignment. Extensive experiments demonstrate that Diff-PC surpasses state-of-the-art methods in ID preservation, facial control, and T2I consistency. Furthermore, our method is compatible with multi-style foundation models.

</details>


### [92] [Navigating Simply, Aligning Deeply: Winning Solutions for Mouse vs. AI 2025](https://arxiv.org/abs/2602.00982)
*Phu-Hoa Pham,Chi-Nguyen Tran,Dao Sy Duy Minh,Nguyen Lam Phu Quy,Huynh Trung Kiet*

Main category: cs.CV

TL;DR: 本论文总结了NeurIPS 2025“Mouse vs. AI: Robust Visual Foraging Competition”两项赛道的获胜方法，分别针对视觉鲁棒性和神经对齐任务，提出了结构简洁但高效的模型，并通过系统分析和消融研究阐述了模型结构与表现的关系。


<details>
  <summary>Details</summary>
Motivation: 人工智能视觉系统在视觉鲁棒性和神经对齐方面与生物视觉系统仍有差距。作者希望探索在这些关键挑战下，模型结构的复杂性如何影响其泛化能力和生物对齐表现，并为开发更具生物启发意义和实际应用价值的视觉代理提供新见解。

Method: 在视觉鲁棒性赛道中，作者设计了一个简洁的两层卷积神经网络，结合门控线性单元和观察归一化模块，提升了模型泛化性能。在神经对齐赛道中，采用16层ResNet风格深度模型，配合GLU门控机制以拟合生物神经响应。他们对训练历程中十个模型节点进行了性能与参数分析，并通过消融和失败案例研究深入探讨了不同结构的优劣。

Result: 视觉鲁棒性赛道轻量级模型最终得分达到95.4%，展现出优秀的泛化能力。神经对齐赛道16层深度模型在神经预测上获得top-1性能（参数量为1780万）。训练步数对性能的影响呈现非单调关系，约20万步时表现最优。消融实验表明简化结构对鲁棒性有益，而深度与容量更高的模型则在神经对齐方面更优。

Conclusion: 简洁架构有助于视觉鲁棒性，而更深、更大容量的模型适合神经对齐，这一发现对现有关于模型复杂度在视觉任务中的误区提出质疑，并为构建更强鲁棒性和生物对齐的视觉智能体提供了参考建议。

Abstract: Visual robustness and neural alignment remain critical challenges in developing artificial agents that can match biological vision systems. We present the winning approaches from Team HCMUS_TheFangs for both tracks of the NeurIPS 2025 Mouse vs. AI: Robust Visual Foraging Competition. For Track 1 (Visual Robustness), we demonstrate that architectural simplicity combined with targeted components yields superior generalization, achieving 95.4% final score with a lightweight two-layer CNN enhanced by Gated Linear Units and observation normalization. For Track 2 (Neural Alignment), we develop a deep ResNet-like architecture with 16 convolutional layers and GLU-based gating that achieves top-1 neural prediction performance with 17.8 million parameters. Our systematic analysis of ten model checkpoints trained between 60K to 1.14M steps reveals that training duration exhibits a non-monotonic relationship with performance, with optimal results achieved around 200K steps. Through comprehensive ablation studies and failure case analysis, we provide insights into why simpler architectures excel at visual robustness while deeper models with increased capacity achieve better neural alignment. Our results challenge conventional assumptions about model complexity in visuomotor learning and offer practical guidance for developing robust, biologically-inspired visual agents.

</details>


### [93] [A Hybrid Mamba-SAM Architecture for Efficient 3D Medical Image Segmentation](https://arxiv.org/abs/2602.00650)
*Mohammadreza Gholipour Shahraki,Mehdi Rezaeian,Mohammad Ghasemzadeh*

Main category: cs.CV

TL;DR: 本文提出了Mamba-SAM混合架构，将SAM基础模型与Mamba系列状态空间模型结合，提升3D医学图像分割的效率与精度，既能充分利用预训练模型能力，又兼顾适应医疗成像领域的问题。


<details>
  <summary>Details</summary>
Motivation: 传统基础模型（如SAM）在医学影像分割场景中因领域差异、二维设计和微调成本高而表现不佳，因此亟需高效且能够适应医学影像不同特征的分割方法。

Method: 提出Mamba-SAM架构，包括双分支方式（融合冻结的SAM编码器与可训练的VMamba编码器特征，并通过交叉注意力集成）和基于适配器方式（在冻结的SAM ViT编码器中插入轻量化3D感知的TPMamba模块），并引入多频门控卷积（MFGC）提升空间-频域的特征表达。

Result: 在ACDC心脏MRI数据集上，双分支Mamba-SAM-Base模型Dice均值0.906，部分指标超过UNet++与其他基线；基于TPMFGC的适配器方案在保证较高精度的同时具备更快推理速度。

Conclusion: 高效结合基础模型与SSM架构能兼顾预训练能力和领域适应性，为3D医学影像分割提供了实用有效的新方案。

Abstract: Accurate segmentation of 3D medical images such as MRI and CT is essential for clinical diagnosis and treatment planning. Foundation models like the Segment Anything Model (SAM) provide powerful general-purpose representations but struggle in medical imaging due to domain shift, their inherently 2D design, and the high computational cost of fine-tuning. To address these challenges, we propose Mamba-SAM, a novel and efficient hybrid architecture that combines a frozen SAM encoder with the linear-time efficiency and long-range modeling capabilities of Mamba-based State Space Models (SSMs). We investigate two parameter-efficient adaptation strategies. The first is a dual-branch architecture that explicitly fuses general features from a frozen SAM encoder with domain-specific representations learned by a trainable VMamba encoder using cross-attention. The second is an adapter-based approach that injects lightweight, 3D-aware Tri-Plane Mamba (TPMamba) modules into the frozen SAM ViT encoder to implicitly model volumetric context. Within this framework, we introduce Multi-Frequency Gated Convolution (MFGC), which enhances feature representation by jointly analyzing spatial and frequency-domain information via 3D discrete cosine transforms and adaptive gating. Extensive experiments on the ACDC cardiac MRI dataset demonstrate the effectiveness of the proposed methods. The dual-branch Mamba-SAM-Base model achieves a mean Dice score of 0.906, comparable to UNet++ (0.907), while outperforming all baselines on Myocardium (0.910) and Left Ventricle (0.971) segmentation. The adapter-based TP MFGC variant offers superior inference speed (4.77 FPS) with strong accuracy (0.880 Dice). These results show that hybridizing foundation models with efficient SSM-based architectures provides a practical and effective solution for 3D medical image segmentation.

</details>


### [94] [Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs](https://arxiv.org/abs/2602.01158)
*Daniel Yezid Guarnizo Orjuela,Leonardo Scappatura,Veronica Di Gennaro,Riccardo Andrea Izzo,Gianluca Bardaro,Matteo Matteucci*

Main category: cs.CV

TL;DR: 本文发现视觉-语言-动作（VLA）模型对于传感器级图像扰动（如噪声、坏点、镜头污渍）极为脆弱，并提出了无需微调即可增强鲁棒性的恢复模块Corruption Restoration Transformer（CRT）。


<details>
  <summary>Details</summary>
Motivation: 虽然VLA模型在受控环境下表现出色，但在真实环境部署时常常因图像质量问题严重退化，且目前相关文献多集中在遮挡，鲜有系统研究传感器级图像损坏。

Method: 作者系统评估了VLA模型对于图像损坏的敏感性，发现模型成功率极剧下降。针对这一问题，提出CRT模块，通过对抗训练学习从受损图像恢复干净特征，该模块即插即用，无需对已有模型进行复杂微调。

Result: 实验证明：VLA模型在未加保护时在标准基准下成功率可由90%降至2%；加装CRT后能在严重损坏下将表现恢复到接近基线水平。

Conclusion: VLA模型在现实视觉干扰下脆弱，CRT解决了该问题，显著提升了在传感器失真环境下的实用性，是实现通用机器人视觉控制重要的一步。

Abstract: Vision-Language-Action (VLA) models have emerged as a dominant paradigm for generalist robotic manipulation, unifying perception and control within a single end-to-end architecture. However, despite their success in controlled environments, reliable real-world deployment is severely hindered by their fragility to visual disturbances. While existing literature extensively addresses physical occlusions caused by scene geometry, a critical mode remains largely unexplored: image corruptions. These sensor-level artifacts, ranging from electronic noise and dead pixels to lens contaminants, directly compromise the integrity of the visual signal prior to interpretation. In this work, we quantify this vulnerability, demonstrating that state-of-the-art VLAs such as $π_{0.5}$ and SmolVLA, suffer catastrophic performance degradation, dropping from 90\% success rates to as low as 2\%, under common signal artifacts. To mitigate this, we introduce the Corruption Restoration Transformer (CRT), a plug-and-play and model-agnostic vision transformer designed to immunize VLA models against sensor disturbances. Leveraging an adversarial training objective, CRT restores clean observations from corrupted inputs without requiring computationally expensive fine-tuning of the underlying model. Extensive experiments across the LIBERO and Meta-World benchmarks demonstrate that CRT effectively recovers lost performance, enabling VLAs to maintain near-baseline success rates, even under severe visual corruption.

</details>


### [95] [Non-Contrastive Vision-Language Learning with Predictive Embedding Alignment](https://arxiv.org/abs/2602.00653)
*Lukas Kuhn,Giuseppe Serra,Florian Buettner*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的无对比视觉语言对齐方法NOVA，以简化和提升多模态表征学习的效率与效果。


<details>
  <summary>Details</summary>
Motivation: 现有主流视觉语言模型如CLIP多依赖对比学习方法，这需要极大的batch size、复杂的负样本采样和超参数调整，训练过程复杂且不稳定。因此需要一种更简单、效率高且稳定的新方法。

Method: 提出NOVA框架：1）采用冻结的领域特定文本编码器（如ClinicalBERT）；2）图像经过增强后，视觉编码器预测文本嵌入；3）通过SIGReg（Sketched Isotropic Gaussian Regularization）强制对齐为各向同性高斯分布，消除对负样本/动量编码器/梯度阻断的依赖，并将训练目标简化为单一超参数。

Result: 在三大基准数据集上的零样本胸部X光分类任务中，NOVA使用ClinicalBERT作为文本编码器、ViT视觉编码器从零训练，均优于多种标准方法，且训练过程更稳定、表现更一致。

Conclusion: 非对比式视觉语言预训练不仅能带来更简洁、稳定的训练流程，还能取得比传统对比学习方法更好的效果，展示了这一方法未来在多模态任务中的潜力与优势。

Abstract: Vision-language models have transformed multimodal representation learning, yet dominant contrastive approaches like CLIP require large batch sizes, careful negative sampling, and extensive hyperparameter tuning. We introduce NOVA, a NOn-contrastive Vision-language Alignment framework based on joint embedding prediction with distributional regularization. NOVA aligns visual representations to a frozen, domain-specific text encoder by predicting text embeddings from augmented image views, while enforcing an isotropic Gaussian structure via Sketched Isotropic Gaussian Regularization (SIGReg). This eliminates the need for negative sampling, momentum encoders, or stop-gradients, reducing the training objective to a single hyperparameter. We evaluate NOVA on zeroshot chest X-ray classification using ClinicalBERT as the text encoder and Vision Transformers trained from scratch on MIMIC-CXR. On zero-shot classification across three benchmark datasets, NOVA outperforms multiple standard baselines while exhibiting substantially more consistent training runs. Our results demonstrate that non-contrastive vision-language pretraining offers a simpler, more stable, and more effective alternative to contrastive methods.

</details>


### [96] [OASIS-DC: Generalizable Depth Completion via Output-level Alignment of Sparse-Integrated Monocular Pseudo Depth](https://arxiv.org/abs/2602.01268)
*Jaehyeon Cho,Jhonghyun An*

Main category: cs.CV

TL;DR: 本文将单目深度基础模型的相对深度输出，通过与稀疏的测距数据校准，转化为伪度量深度先验，并利用此先验改善少样本情况下的深度估计精度，具备良好的通用性和实用性。


<details>
  <summary>Details</summary>
Motivation: 目前单目深度估计的大型基础模型虽在零样本（zero-shot）任务中表现突出，但输出一般为相对深度而非绝对度量深度，导致其在机器人、自动驾驶等实际应用中受限。解决如何将大模型的能力迁移到实际、标注数据有限的场景成为亟需探索的问题。

Method: 作者首先用少量稀疏真实距离测量点，对基础模型输出的相对深度进行校准，转为伪度量深度先验。然后，提出一个改进网络，依靠这些先验在可靠区域内进行预测，在不可靠区域则允许偏离，从而在很少标注数据下实现准确的度量深度预测。

Result: 系统能在缺少验证集的情况下，维持尺度一致性与边缘锐利性，在“少样本”（few-shot）数据情形下表现突出，输出的深度更符合真实需求。

Conclusion: 将单目基础模型的强大先验能力与少量稀疏实测点结合，是解决实际标注稀缺下，实现稳健、可部署深度补全的有效路径，具有现实指导意义。

Abstract: Recent monocular foundation models excel at zero-shot depth estimation, yet their outputs are inherently relative rather than metric, limiting direct use in robotics and autonomous driving. We leverage the fact that relative depth preserves global layout and boundaries: by calibrating it with sparse range measurements, we transform it into a pseudo metric depth prior. Building on this prior, we design a refinement network that follows the prior where reliable and deviates where necessary, enabling accurate metric predictions from very few labeled samples. The resulting system is particularly effective when curated validation data are unavailable, sustaining stable scale and sharp edges across few-shot regimes. These findings suggest that coupling foundation priors with sparse anchors is a practical route to robust, deployment-ready depth completion under real-world label scarcity.

</details>


### [97] [Schrödinger-Inspired Time-Evolution for 4D Deformation Forecasting](https://arxiv.org/abs/2602.00661)
*Ahsan Raza Siyal,Markus Haltmeier,Ruth Steiger,Elke Ruth Gizewski,Astrid Ellen Grams*

Main category: cs.CV

TL;DR: 提出了一种基于薛定谔方程的物理引导神经网络架构，实现对复杂三维随时间演化现象（4D）的高稳定性和可解释预测。


<details>
  <summary>Details</summary>
Motivation: 如何实现对医学成像、流体动力学等领域中的复杂4D（3D+时间）现象进行高效且物理一致性的时空预测，目前方法多不具物理约束，导致长时预测不稳定且难以解释。

Method: 构建了薛定谔方程启发的神经网络，将显式的时间演化算子嵌入在卷积网络中。模型从观察到的体数据序列中学习到每体素的幅值、相位和势能场，构造复波函数，并利用可微分、展开的薛定谔时间推进器进行时序预测。该方法通过物理先验提升了网络的可解释性和预测稳定性。

Result: 在合成形变与拓扑结构变化的仿真数据上，证明了该方法可精确、稳定地预测未来的4D状态，包括体积强度和形变场，并有效保持了解剖结构一致性。

Conclusion: 该工作首次将薛定谔型演化算子端到端集成进4D神经预测网络，在可解释性、稳定性与解剖学保真方面树立了新的方法范式，为未来可解释、稳定的时空预测提供了理论与实践基础。

Abstract: Spatiotemporal forecasting of complex three-dimensional phenomena (4D: 3D + time) is fundamental to applications in medical imaging, fluid and material dynamics, and geophysics. In contrast to unconstrained neural forecasting models, we propose a Schrödinger-inspired, physics-guided neural architecture that embeds an explicit time-evolution operator within a deep convolutional framework for 4D prediction. From observed volumetric sequences, the model learns voxelwise amplitude, phase, and potential fields that define a complex-valued wavefunction $ψ= A e^{iφ}$, which is evolved forward in time using a differentiable, unrolled Schrödinger time stepper. This physics-guided formulation yields several key advantages: (i) temporal stability arising from the structured evolution operator, which mitigates drift and error accumulation in long-horizon forecasting; (ii) an interpretable latent representation, where phase encodes transport dynamics, amplitude captures structural intensity, and the learned potential governs spatiotemporal interactions; and (iii) natural compatibility with deformation-based synthesis, which is critical for preserving anatomical fidelity in medical imaging applications. By integrating physical priors directly into the learning process, the proposed approach combines the expressivity of deep networks with the robustness and interpretability of physics-based modeling. We demonstrate accurate and stable prediction of future 4D states, including volumetric intensities and deformation fields, on synthetic benchmarks that emulate realistic shape deformations and topological changes. To our knowledge, this is the first end-to-end 4D neural forecasting framework to incorporate a Schrödinger-type evolution operator, offering a principled pathway toward interpretable, stable, and anatomically consistent spatiotemporal prediction.

</details>


### [98] [Real-Time Loop Closure Detection in Visual SLAM via NetVLAD and Faiss](https://arxiv.org/abs/2602.01673)
*Enguang Fan*

Main category: cs.CV

TL;DR: 本文比较了NetVLAD与传统DBoW在SLAM中闭环检测的性能，发现NetVLAD在计算速度和精度上可同时满足实时应用需求。


<details>
  <summary>Details</summary>
Motivation: SLAM中的闭环检测传统方法DBoW效率高但在复杂环境下表现不佳，而深度学习方法虽然鲁棒性好但普遍被认为不够实时，论文希望评估深度学习描述符是否真能应用于实时SLAM。

Method: 在KITTI数据集上，利用Faiss加速最近邻搜索，将NetVLAD用作闭环检测模块，与DBoW进行对比实验，并引入了精细化Top-K查全率-查准率曲线来更准确评估多个或零匹配的情形。

Result: NetVLAD在Faiss加速下，实现实时查询速度，且在准确性和鲁棒性方面均优于DBoW。

Conclusion: NetVLAD结合高效搜索后，在SLAM中可替代DBoW作为闭环检测模块，实现更高的性能和实时性。

Abstract: Loop closure detection (LCD) is a core component of simultaneous localization and mapping (SLAM): it identifies revisited places and enables pose-graph constraints that correct accumulated drift. Classic bag-of-words approaches such as DBoW are efficient but often degrade under appearance change and perceptual aliasing. In parallel, deep learning-based visual place recognition (VPR) descriptors (e.g., NetVLAD and Transformer-based models) offer stronger robustness, but their computational cost is often viewed as a barrier to real-time SLAM. In this paper, we empirically evaluate NetVLAD as an LCD module and compare it against DBoW on the KITTI dataset. We introduce a Fine-Grained Top-K precision-recall curve that better reflects LCD settings where a query may have zero or multiple valid matches. With Faiss-accelerated nearestneighbor search, NetVLAD achieves real-time query speed while improving accuracy and robustness over DBoW, making it a practical drop-in alternative for LCD in SLAM.

</details>


### [99] [Improving Neuropathological Reconstruction Fidelity via AI Slice Imputation](https://arxiv.org/abs/2602.00669)
*Marina Crespo Aguirre,Jonathan Williams-Ramirez,Dina Zemlyanker,Xiaoling Hu,Lucas J. Deden-Binder,Rogeny Herisse,Mark Montine,Theresa R. Connors,Christopher Mount,Christine L. MacDonald,C. Dirk Keene,Caitlin S. Latimer,Derek H. Oakley,Bradley T. Hyman,Ana Lawry Aguila,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: 本文提出了一种高效的超分辨率方法，通过补全切片，把各向异性的2D脑切片照片重建为各向同性的3D体积，提高了解剖结构的分辨率和测量精度。


<details>
  <summary>Details</summary>
Motivation: 以往利用2D解剖照片重建3D脑体积时，会因厚切片而导致结构模糊、低分辨率，限制了解剖准确性和自动分割性能。为改善这一问题，提高基于照片重建结果的科学和应用价值，提出该新方法。

Method: 提出在传统基于2D解剖照片的脑体积重建流程中，加入一个超分辨率算法，通过机器学习在域随机化的合成数据上训练，然后自动预测补全缺失切片，使3D重建结构从各向异性提升为各向同性，兼容各种解剖方案且对厚切片有鲁棒性。

Result: 经验证，该方法补全后的体积可大幅提升自动分割精度，Dice系数更高，尤其在皮层和白质区域效果显著。还提高了表面重建和与标准脑图谱的配准精度。

Conclusion: 该方法显著提升了基于解剖照片重建脑部结构的分辨率和解剖保真度，增强了神经病理学与神经影像学之间的桥梁，相关工具已公开发布。

Abstract: Neuropathological analyses benefit from spatially precise volumetric reconstructions that enhance anatomical delineation and improve morphometric accuracy. Our prior work has shown the feasibility of reconstructing 3D brain volumes from 2D dissection photographs. However these outputs sometimes exhibit coarse, overly smooth reconstructions of structures, especially under high anisotropy (i.e., reconstructions from thick slabs). Here, we introduce a computationally efficient super-resolution step that imputes slices to generate anatomically consistent isotropic volumes from anisotropic 3D reconstructions of dissection photographs. By training on domain-randomized synthetic data, we ensure that our method generalizes across dissection protocols and remains robust to large slab thicknesses. The imputed volumes yield improved automated segmentations, achieving higher Dice scores, particularly in cortical and white matter regions. Validation on surface reconstruction and atlas registration tasks demonstrates more accurate cortical surfaces and MRI registration. By enhancing the resolution and anatomical fidelity of photograph-based reconstructions, our approach strengthens the bridge between neuropathology and neuroimaging. Our method is publicly available at https://surfer.nmr.mgh.harvard.edu/fswiki/mri_3d_photo_recon

</details>


### [100] [DDP-WM: Disentangled Dynamics Prediction for Efficient World Models](https://arxiv.org/abs/2602.01780)
*Shicheng Yin,Kaixuan Yin,Weixing Chen,Yang Liu,Guanbin Li,Liang Lin*

Main category: cs.CV

TL;DR: 提出了一种高效的世界模型（DDP-WM），通过动力学解耦有效提升推理速度和性能，适合机器人实时规划应用。实验表明其显著优于传统Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于稠密Transformer的世界模型在实际机器人中由于计算开销大，难以满足实时性要求，需要一种兼具高性能与高效率的新型世界模型。

Method: 提出Disentangled Dynamics Prediction (DDP)原理，将场景的潜在状态演化分解为主要由物理交互驱动的稀疏主动力学和由环境背景驱动的次要变化。通过结合高效的历史处理与动态定位再加交叉注意力机制，实现这两者的分离，提高了推理效率和优化表现。

Result: 在多项任务（导航、精密操作、复杂多体交互等）上验证了该方法，有显著的效率和性能提升。特别是在Push-T任务上，推理速度提升9倍，MPC成功率由90%提升至98%。

Conclusion: DDP-WM成为实现高效且高保真世界模型的有前景方法，为机器人实际部署提供了新思路。代码已开源。

Abstract: World models are essential for autonomous robotic planning. However, the substantial computational overhead of existing dense Transformerbased models significantly hinders real-time deployment. To address this efficiency-performance bottleneck, we introduce DDP-WM, a novel world model centered on the principle of Disentangled Dynamics Prediction (DDP). We hypothesize that latent state evolution in observed scenes is heterogeneous and can be decomposed into sparse primary dynamics driven by physical interactions and secondary context-driven background updates. DDP-WM realizes this decomposition through an architecture that integrates efficient historical processing with dynamic localization to isolate primary dynamics. By employing a crossattention mechanism for background updates, the framework optimizes resource allocation and provides a smooth optimization landscape for planners. Extensive experiments demonstrate that DDP-WM achieves significant efficiency and performance across diverse tasks, including navigation, precise tabletop manipulation, and complex deformable or multi-body interactions. Specifically, on the challenging Push-T task, DDP-WM achieves an approximately 9 times inference speedup and improves the MPC success rate from 90% to98% compared to state-of-the-art dense models. The results establish a promising path for developing efficient, high-fidelity world models. Codes will be available at https://github.com/HCPLabSYSU/DDP-WM.

</details>


### [101] [HPC: Hierarchical Point-based Latent Representation for Streaming Dynamic Gaussian Splatting Compression](https://arxiv.org/abs/2602.00671)
*Yangzhi Ma,Bojun Liu,Wenting Liao,Dong Liu,Zhu Li,Li Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的分层点云动态高斯分布压缩框架（HPC），极大减少存储空间同时保持渲染质量，适合高效流式传输。


<details>
  <summary>Details</summary>
Motivation: 随着动态高斯投影（Dynamic Gaussian Splatting）技术在自由视角视频的渲染领域取得进展，如何在内存占用较小的情况下保持高渲染质量，以便适应高效流式传输，成为待解决的重要问题。目前主流的方法存在参数冗余或压缩紧凑度不够的问题。

Method: 提出了一种分层的点云式潜在表示方式，对每个高斯点单独编码，结合定制化的聚合机制提升局部紧凑性，避免无效空间的参数冗余。同时首创性地利用帧间神经网络参数相关性进行模型压缩，形成端到端的压缩流程。

Result: 综合实验证明，所提HPC框架在压缩效率和恢复质量上均超越现有方法，相较于基线方法减少67%存储空间，同时保持高保真度渲染。

Conclusion: HPC框架有效解决了动态高斯投影在流式传输中的存储和压缩挑战，兼具高紧凑性与渲染质量，具有良好应用前景。

Abstract: While dynamic Gaussian Splatting has driven significant advances in free-viewpoint video, maintaining its rendering quality with a small memory footprint for efficient streaming transmission still presents an ongoing challenge. Existing streaming dynamic Gaussian Splatting compression methods typically leverage a latent representation to drive the neural network for predicting Gaussian residuals between frames. Their core latent representations can be categorized into structured grid-based and unstructured point-based paradigms. However, the former incurs significant parameter redundancy by inevitably modeling unoccupied space, while the latter suffers from limited compactness as it fails to exploit local correlations. To relieve these limitations, we propose HPC, a novel streaming dynamic Gaussian Splatting compression framework. It employs a hierarchical point-based latent representation that operates on a per-Gaussian basis to avoid parameter redundancy in unoccupied space. Guided by a tailored aggregation scheme, these latent points achieve high compactness with low spatial redundancy. To improve compression efficiency, we further undertake the first investigation to compress neural networks for streaming dynamic Gaussian Splatting through mining and exploiting the inter-frame correlation of parameters. Combined with latent compression, this forms a fully end-to-end compression framework. Comprehensive experimental evaluations demonstrate that HPC substantially outperforms state-of-the-art methods. It achieves a storage reduction of 67% against its baseline while maintaining high reconstruction fidelity.

</details>


### [102] [LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation](https://arxiv.org/abs/2602.02220)
*Bo Miao,Weijia Liu,Jun Luo,Lachlan Shinnick,Jian Liu,Thomas Hamilton-Smith,Yuhe Yang,Zijie Wu,Vanja Videnovic,Feras Dayoub,Anton van den Hengel*

Main category: cs.CV

TL;DR: 本论文提出了HieraNav任务和LangMap基准，以多层次、开放词汇的语言目标引导3D室内导航，推动了语言与物体关系在智能体中的真实应用。


<details>
  <summary>Details</summary>
Motivation: 物体与语言的关系是人机交流和具身智能的核心。现有导航任务在语义层级、词汇广度和真实场景上的覆盖有限，需要更精细的基准和任务来测试和推动基于语言的导航智能。

Method: 作者提出了HieraNav任务，将导航目标细分为场景、房间、区域、实例四个语义层级，并建立了LangMap基准，基于真实3D室内扫描，人工标注了区域和实体描述，涵盖414类物体，18K+导航任务。每个导航目标配有精简和详细两种描述，支持多指令风格下的评估。

Result: LangMap标注质量显著优于GOAT-Bench，判别准确率提升23.8%，且所用词汇减少四倍。各种模型的评测显示，丰富的上下文和记忆提升导航成功率，但长尾、小体积、强上下文依赖和远距离、多目标任务依然具有挑战性。

Conclusion: HieraNav和LangMap为以语言驱动的具身导航提供了高质量、细粒度的评测平台，为后续研究提供了坚实基础。

Abstract: The relationships between objects and language are fundamental to meaningful communication between humans and AI, and to practically useful embodied intelligence. We introduce HieraNav, a multi-granularity, open-vocabulary goal navigation task where agents interpret natural language instructions to reach targets at four semantic levels: scene, room, region, and instance. To this end, we present Language as a Map (LangMap), a large-scale benchmark built on real-world 3D indoor scans with comprehensive human-verified annotations and tasks spanning these levels. LangMap provides region labels, discriminative region descriptions, discriminative instance descriptions covering 414 object categories, and over 18K navigation tasks. Each target features both concise and detailed descriptions, enabling evaluation across different instruction styles. LangMap achieves superior annotation quality, outperforming GOAT-Bench by 23.8% in discriminative accuracy using four times fewer words. Comprehensive evaluations of zero-shot and supervised models on LangMap reveal that richer context and memory improve success, while long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion, remain challenging. HieraNav and LangMap establish a rigorous testbed for advancing language-driven embodied navigation. Project: https://bo-miao.github.io/LangMap

</details>


### [103] [Video Understanding: Through A Temporal Lens](https://arxiv.org/abs/2602.00683)
*Thong Thanh Nguyen*

Main category: cs.CV

TL;DR: 本论文提出多项创新方法，以提升视频理解中的时序关系建模能力，包括自动标注框架、参数高效的时序建模、长视频建模新架构、细粒度动静关系对比学习等，全面提升了模型对视频内容的理解和推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前视频理解方法在建模视频元素之间的时间关系方面存在明显局限，无法充分挖掘视频序列中的动态联系。本论文旨在解决这一关键问题，从而推动视频理解能力的提升。

Method: 1. 利用大规模视觉-语言模型（LVLM）和鲁棒性对比学习目标及角度损失，实现自动标注。2. 提出基于“循环适配器”的高效微调策略，适应小数据环境下的时序动态。3. 引入状态空间层（SSL）架构，提升长视频建模能力，并提出新数据集作为长视频基准。4. 设计细粒度动静关系的对比学习框架。5. 对LVLMs进行系统实证分析，发现视觉-语言接口为时序推理瓶颈，并提出“时间导向”配方。

Result: 所提出的方法在视频理解任务上显著优于现有方法。长视频建模取得突破，并在新设定的基准上表现优异。细粒度动静关系建模和时间导向方法有效改善了模型的推理与表达能力。系统分析验证了视觉-语言对接中时序建模的关键点和改进空间。

Conclusion: 通过显式融合和优化时序关系建模，本论文的方法显著提升了视频理解模型对复杂、动态视频内容的表达和推理能力，为未来视频相关应用提供了重要理论和实践基础。

Abstract: This thesis explores the central question of how to leverage temporal relations among video elements to advance video understanding. Addressing the limitations of existing methods, the work presents a five-fold contribution: (1) an automatic annotation framework that utilizes large vision-language models and a noise-robust contrastive learning objective with a subtractive angular margin; (2) a parameter-efficient fine-tuning strategy using "recurrent adapters" to capture temporal dynamics in low-data regimes; (3) the integration of State Space Layers (SSL) for efficient long-form video modeling, supported by the introduction of two new long-term benchmarks for egocentric and feature-length content; (4) a novel contrastive learning framework designed to explicitly model fine-grained relations between motions and video moments; and (5) a comprehensive empirical study on Large Vision-Language Models (LVLMs) that identifies the visual-language interface as a bottleneck for temporal reasoning, leading to a new "temporal-oriented recipe" for upscaled video understanding. Collectively, these contributions demonstrate that explicit temporal modeling significantly enhances a model's ability to represent and reason about the fluid nature of video content.

</details>


### [104] [V2X-DSC: Multi-Agent Collaborative Perception with Distributed Source Coding Guided Communication](https://arxiv.org/abs/2602.00687)
*Yuankun Zeng,Shaohui Li,Zhi Li,Shulan Ruan,Yu Liu,You He*

Main category: cs.CV

TL;DR: 本论文提出了一种结合分布式源编码思想的多智能体协作感知通信框架V2X-DSC，有效降低了带宽压力，实现高效准确的3D感知。


<details>
  <summary>Details</summary>
Motivation: 多智能体协作感知需融合各方观测信息以提升3D理解效果，但现有方法直接共享中间特征（如密集BEV特征）时，极大消耗了V2X通信带宽，影响系统实际应用。作者发现多智能体对同一现实世界有高相关观测，因此存在冗余信息的压缩空间。

Method: 提出V2X-DSC框架，其中包括一个条件编解码器（DCC）：发送端将BEV特征压缩为紧凑编码，接收端利用本地特征作为辅助信息进行条件重建，仅保留本地感知中缺失的创新信息。该结构通过有条件的信息分配与重建训练，促进增量式表征学习，有效抑制冗余与噪声。

Result: 在DAIR-V2X、OPV2V和V2X-Real公开数据集实验证明，该方法在KB量级通信带宽下，实现了比当前最优方法更高的感知准确率与带宽利用率，并能作为通信层兼容多种协作融合主干网络。

Conclusion: V2X-DSC通过分布式源编码角度创新性地压缩和融合多智能体感知特征，在极低带宽下显著提高协作3D感知的效率与效果，具有很好的实际应用与推广意义。

Abstract: Collaborative perception improves 3D understanding by fusing multi-agent observations, yet intermediate-feature sharing faces strict bandwidth constraints as dense BEV features saturate V2X links. We observe that collaborators view the same physical world, making their features strongly correlated; thus receivers only need innovation beyond their local context. Revisiting this from a distributed source coding perspective, we propose V2X-DSC, a framework with a Conditional Codec (DCC) for bandwidth-constrained fusion. The sender compresses BEV features into compact codes, while the receiver performs conditional reconstruction using its local features as side information, allocating bits to complementary cues rather than redundant content. This conditional structure regularizes learning, encouraging incremental representation and yielding lower-noise features. Experiments on DAIR-V2X, OPV2V, and V2X-Real demonstrate state-of-the-art accuracy-bandwidth trade-offs under KB-level communication, and generalizes as a plug-and-play communication layer across multiple fusion backbones.

</details>


### [105] [JoyAvatar: Unlocking Highly Expressive Avatars via Harmonized Text-Audio Conditioning](https://arxiv.org/abs/2602.00702)
*Ruikui Wang,Jinheng Feng,Lang Tian,Huaishao Luo,Chaochao Li,Liangbo Zhou,Huan Zhang,Youzheng Wu,Xiaodong He*

Main category: cs.CV

TL;DR: JoyAvatar模型突破了现有虚拟人视频生成在文本指令对齐和复杂动作控制上的局限，实现了自然流畅的全身动作和动态场景的长时视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有视频虚拟人系统在说话、演讲等场景表现突出，但难以响应包含大幅肢体动作、动态镜头、场景切换或人-物互动等复杂文本指令，限制了其实用性和泛化能力。

Method: 提出了JoyAvatar框架，包括两项创新：（1）双教师增强训练算法，让模型既能继承基础模型的文本可控性，又能学习音视频同步；（2）基于去噪时间步对多模态条件（如音频、文本）强度动态调节，缓解异构信号间的冲突。

Result: GSB评测显示，JoyAvatar在全身动作、动态镜头控制、口型对齐和身份一致性等方面超越了Omnihuman-1.5和KlingAvatar 2.0，并支持多角色对话和角色扮演等复杂应用。

Conclusion: JoyAvatar极大扩展了虚拟人视频生成的表现力和指令对齐能力，为全身动态、多人互动等高级应用场景提供了新的可能性。

Abstract: Existing video avatar models have demonstrated impressive capabilities in scenarios such as talking, public speaking, and singing. However, the majority of these methods exhibit limited alignment with respect to text instructions, particularly when the prompts involve complex elements including large full-body movement, dynamic camera trajectory, background transitions, or human-object interactions. To break out this limitation, we present JoyAvatar, a framework capable of generating long duration avatar videos, featuring two key technical innovations. Firstly, we introduce a twin-teacher enhanced training algorithm that enables the model to transfer inherent text-controllability from the foundation model while simultaneously learning audio-visual synchronization. Secondly, during training, we dynamically modulate the strength of multi-modal conditions (e.g., audio and text) based on the distinct denoising timestep, aiming to mitigate conflicts between the heterogeneous conditioning signals. These two key designs serve to substantially expand the avatar model's capacity to generate natural, temporally coherent full-body motions and dynamic camera movements as well as preserve the basic avatar capabilities, such as accurate lip-sync and identity consistency. GSB evaluation results demonstrate that our JoyAvatar model outperforms the state-of-the-art models such as Omnihuman-1.5 and KlingAvatar 2.0. Moreover, our approach enables complex applications including multi-person dialogues and non-human subjects role-playing. Some video samples are provided on https://joyavatar.github.io/.

</details>


### [106] [StomataSeg: Semi-Supervised Instance Segmentation for Sorghum Stomatal Components](https://arxiv.org/abs/2602.00703)
*Zhongtian Huang,Zhi Chen,Zi Huang,Xin Yu,Daniel Smith,Chaitanya Purushothama,Erik Van Oosterom,Alex Wu,William Salter,Yan Li,Scott Chapman*

Main category: cs.CV

TL;DR: 该论文提出了一种针对高粱气孔成分的半监督实例分割框架，通过采集并标注大量显微图像，并结合伪标注方法，有效提升了小结构气孔的自动分割性能。


<details>
  <summary>Details</summary>
Motivation: 高粱是一种耐旱、对气候有高度适应性的粮食作物，提升其水分利用效率对于粮食安全至关重要。气孔作为调节水分代谢和光合作用的关键结构，精确表型分析难度大，因此需要高效的自动化分析方法。

Method: 作者构建了包含11,060个人工标注图像的高粱气孔成分数据集，并将高分辨率显微图片切分为小块以检测微小结构。然后利用半监督学习中的伪标注策略，对未标注图片生成额外的56,428个带标签样本，提升训练数据量。对比多种分割算法，评估处理方法的有效性。

Result: 经过patch切分与半监督伪标注后，主流模型分割性能显著提升：语义分割mIoU从65.93%提升至70.35%，实例分割AP从28.30%提升至46.10%。

Conclusion: 结合patch预处理和半监督学习可有效提升气孔微小结构的分割效果，为高通量性状分析和作物AI表型检测技术的规模化应用奠定了基础。

Abstract: Sorghum is a globally important cereal grown widely in water-limited and stress-prone regions. Its strong drought tolerance makes it a priority crop for climate-resilient agriculture. Improving water-use efficiency in sorghum requires precise characterisation of stomatal traits, as stomata control of gas exchange, transpiration and photosynthesis have a major influence on crop performance. Automated analysis of sorghum stomata is difficult because the stomata are small (often less than 40 $μ$m in length in grasses such as sorghum) and vary in shape across genotypes and leaf surfaces. Automated segmentation contributes to high-throughput stomatal phenotyping, yet current methods still face challenges related to nested small structures and annotation bottlenecks. In this paper, we propose a semi-supervised instance segmentation framework tailored for analysis of sorghum stomatal components. We collect and annotate a sorghum leaf imagery dataset containing 11,060 human-annotated patches, covering the three stomatal components (pore, guard cell and complex area) across multiple genotypes and leaf surfaces. To improve the detection of tiny structures, we split high-resolution microscopy images into overlapping small patches. We then apply a pseudo-labelling strategy to unannotated images, producing an additional 56,428 pseudo-labelled patches. Benchmarking across semantic and instance segmentation models shows substantial performance gains: for semantic models the top mIoU increases from 65.93% to 70.35%, whereas for instance models the top AP rises from 28.30% to 46.10%. These results demonstrate that combining patch-based preprocessing with semi-supervised learning significantly improves the segmentation of fine stomatal structures. The proposed framework supports scalable extraction of stomatal traits and facilitates broader adoption of AI-driven phenotyping in crop science.

</details>


### [107] [Supervised makeup transfer with a curated dataset: Decoupling identity and makeup features for enhanced transformation](https://arxiv.org/abs/2602.00729)
*Qihe Pan,Yiming Wu,Xing Zhao,Liang Xie,Guodao Sun,Ronghua Liang*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的化妆迁移方法，解决了现有方法在数据集、特征解耦和可控性方面的不足。通过高质量数据集构建、特征解耦扩散框架和文本引导的精细控制机制，实现了更真实、多样且可控的化妆迁移效果。


<details>
  <summary>Details</summary>
Motivation: 目前基于生成对抗网络（GAN）的化妆迁移方法存在数据集规模有限、身份与化妆特征解耦不佳、可控性弱等问题。为了提升化妆迁移的质量、真实感以及用户自定义能力，需要新的解决方案。

Method: 1）采用“训练-生成-筛选-再训练”流程，结合合成、真实和筛选样本，构建高质量多样化数据集；2）设计基于扩散模型的特征解耦框架，将身份特征与化妆特征分离，保证面部结构和肤色不变；3）引入文本引导机制，实现眼部、唇部、脸部等局部化妆的自然语言精细控制。

Result: 在基准数据集与真实场景下的实验表明，该方法在真实感、身份保持和灵活性方面均有提升。

Conclusion: 基于扩散模型的化妆迁移方法能更有效地解耦身份与化妆特征，同时通过文本引导机制提升了化妆控制的精细度，推动了化妆迁移技术的发展。

Abstract: Diffusion models have recently shown strong progress in generative tasks, offering a more stable alternative to GAN-based approaches for makeup transfer. Existing methods often suffer from limited datasets, poor disentanglement between identity and makeup features, and weak controllability. To address these issues, we make three contributions. First, we construct a curated high-quality dataset using a train-generate-filter-retrain strategy that combines synthetic, realistic, and filtered samples to improve diversity and fidelity. Second, we design a diffusion-based framework that disentangles identity and makeup features, ensuring facial structure and skin tone are preserved while applying accurate and diverse cosmetic styles. Third, we propose a text-guided mechanism that allows fine-grained and region-specific control, enabling users to modify eyes, lips, or face makeup with natural language prompts. Experiments on benchmarks and real-world scenarios demonstrate improvements in fidelity, identity preservation, and flexibility. Examples of our dataset can be found at: https://makeup-adapter.github.io.

</details>


### [108] [Diffusion-Driven Inter-Outer Surface Separation for Point Clouds with Open Boundaries](https://arxiv.org/abs/2602.00739)
*Zhengyan Qin,Liyuan Qiu*

Main category: cs.CV

TL;DR: 提出了一种基于扩散算法的方法，用于从存在“双表面伪影”的双层点云中分离出内外层表面，适用于TSDF融合引起的重建伪影，兼容封闭和开放边界点云。


<details>
  <summary>Details</summary>
Motivation: 现有TSDF融合方法在重建3D场景（如室内或医学图像）时，常因截断阈值不对称产生双层表面伪影，造成重建表面重叠无序，影响后续处理，急需高效分离真层表面的方法。

Method: 提出了一种基于扩散的算法，能够自动从具有开放边界（存在孔洞）或封闭结构的双层点云中，分离出真实的内层表面，并以高效（约10秒处理4万点）方式消除重叠和表面法向混乱的问题。

Result: 该方法可在约10秒内对含4万个点的点云进行内层表面提取，效果适用于封闭或开放边界模型，有效去除了伪影与错层。

Conclusion: 所提扩散算法模块轻量，适合作为TSDF融合后处理步骤，可显著提升室内建模和医学成像等对点云表面精度有高要求应用的结果质量，无需替代复杂的重建管线。

Abstract: We propose a diffusion-based algorithm for separating the inter and outer layer surfaces from double-layered point clouds, particularly those exhibiting the "double surface artifact" caused by truncation in Truncated Signed Distance Function (TSDF) fusion during indoor or medical 3D reconstruction. This artifact arises from asymmetric truncation thresholds, leading to erroneous inter and outer shells in the fused volume, which our method addresses by extracting the true inter layer to mitigate challenges like overlapping surfaces and disordered normals. We focus on point clouds with \emph{open boundaries} (i.e., sampled surfaces with topological openings/holes through which particles may escape), rather than point clouds with \emph{missing surface regions} where no samples exist. Our approach enables robust processing of both watertight and open-boundary models, achieving extraction of the inter layer from 20,000 inter and 20,000 outer points in approximately 10 seconds. This solution is particularly effective for applications requiring accurate surface representations, such as indoor scene modeling and medical imaging, where double-layered point clouds are prevalent, and it accommodates both closed (watertight) and open-boundary surface geometries. Our goal is \emph{post-hoc} inter/outer shell separation as a lightweight module after TSDF fusion; we do not aim to replace full variational or learning-based reconstruction pipelines.

</details>


### [109] [HSI-VAR: Rethinking Hyperspectral Restoration through Spatial-Spectral Visual Autoregression](https://arxiv.org/abs/2602.00749)
*Xiangming Wang,Benteng Sun,Yungeng Liu,Haijin Zeng,Yongyong Chen,Jingyong Su,Jie Liu*

Main category: cs.CV

TL;DR: 该论文提出了HSI-VAR，一种新颖的高光谱图像（HSI）复原方法，通过自回归生成模型替代计算量较大的扩散模型，实现高效且精确的修复。HSI-VAR显著提升了结构细节保留和推理速度，在多项基准测试上取得了顶尖表现。


<details>
  <summary>Details</summary>
Motivation: 现有高光谱图像复原方法要么计算量大（如扩散模型），要么导致图像过于平滑、细节损失（回归模型）。实际应用需求高效且能保留细节的复原方法，因此需要创新性解决传统方法的计算与效果瓶颈。

Method: HSI-VAR将高光谱图像复原视为自回归生成问题，通过逐步建模光谱和空间依赖。核心改进包括：（1）潜变量条件对齐，实现语义一致性和精确重建；（2）退化感知引导，把不同退化类型编码进Embedding空间，实现自动复原控制并大幅降低推理成本；（3）空间-光谱自适应模块，细化空间和光谱的恢复细节。

Result: 在九个高光谱图像复原基准上，HSI-VAR达到最优性能，如在ICVL数据集提升3.77 dB PSNR，并远超扩散模型的结构恢复能力，推理速度提升高达95.5倍。

Conclusion: HSI-VAR结合高效算力与细节恢复优势，突破了高光谱图像复原领域的现有瓶颈，为实际复杂退化的图像修复提供了实用、高效的解决方案。

Abstract: Hyperspectral images (HSIs) capture richer spatial-spectral information beyond RGB, yet real-world HSIs often suffer from a composite mix of degradations, such as noise, blur, and missing bands. Existing generative approaches for HSI restoration like diffusion models require hundreds of iterative steps, making them computationally impractical for high-dimensional HSIs. While regression models tend to produce oversmoothed results, failing to preserve critical structural details. We break this impasse by introducing HSI-VAR, rethinking HSI restoration as an autoregressive generation problem, where spectral and spatial dependencies can be progressively modeled rather than globally reconstructed. HSI-VAR incorporates three key innovations: (1) Latent-condition alignment, which couples semantic consistency between latent priors and conditional embeddings for precise reconstruction; (2) Degradation-aware guidance, which uniquely encodes mixed degradations as linear combinations in the embedding space for automatic control, remarkably achieving a nearly $50\%$ reduction in computational cost at inference; (3) A spatial-spectral adaptation module that refines details across both domains in the decoding phase. Extensive experiments on nine all-in-one HSI restoration benchmarks confirm HSI-VAR's state-of-the-art performance, achieving a 3.77 dB PSNR improvement on \textbf{\textit{ICVL}} and offering superior structure preservation with an inference speed-up of up to $95.5 \times$ compared with diffusion-based methods, making it a highly practical solution for real-world HSI restoration.

</details>


### [110] [Evaluating Deep Learning-Based Nerve Segmentation in Brachial Plexus Ultrasound Under Realistic Data Constraints](https://arxiv.org/abs/2602.00763)
*Dylan Yves,Khush Agarwal,Jonathan Hoyin Chan,Patcharapit Promoppatum,Aroonkamon Pattanasiricharoen*

Main category: cs.CV

TL;DR: 本文评估了基于深度学习的U-Net模型在超声图像中神经分割的表现，揭示了数据集组成和标注策略对结果的影响。


<details>
  <summary>Details</summary>
Motivation: 在超声引导下进行区域麻醉时，精确定位神经至关重要，但由于图像对比差、斑点噪声和个体解剖差异，人工识别非常具有挑战性。因此，开发自动化、鲁棒的神经分割方法具有重要的临床意义。

Method: 作者基于U-Net神经网络对臂丛神经的超声图像进行分割，比较了不同超声设备（SIEMENS和Philips）的混合训练数据对分割性能的影响，并考察了将任务从二分类（只分割神经）扩展到多分类（包括动脉、静脉、神经、肌肉）时的表现变化。同时，分析了神经尺寸与分割准确率间的相关性。

Result: 混合不同机器所得数据能提升某些配置下低性能源的分割表现，但在目标域与训练域完全匹配时，单一来源训练效果更佳。多分类分割会导致神经分割性能比二分类下降9%-61%，主要受类别不平衡和边界不清影响。神经区域越大，分割准确率越高（r=0.587，p<0.001），小神经分割依然是难点。

Conclusion: 针对临床实际超声神经分割任务，建议在构建训练集与选择标注策略时慎重考量数据源和分类数，特别关注小神经区域的训练改进，以提升整体鲁棒性和准确性。

Abstract: Accurate nerve localization is critical for the success of ultrasound-guided regional anesthesia, yet manual identification remains challenging due to low image contrast, speckle noise, and inter-patient anatomical variability. This study evaluates deep learning-based nerve segmentation in ultrasound images of the brachial plexus using a U-Net architecture, with a focus on how dataset composition and annotation strategy influence segmentation performance. We find that training on combined data from multiple ultrasound machines (SIEMENS ACUSON NX3 Elite and Philips EPIQ5) provides regularization benefits for lower-performing acquisition sources, though it does not surpass single-source training when matched to the target domain. Extending the task from binary nerve segmentation to multi-class supervision (artery, vein, nerve, muscle) results in decreased nerve-specific Dice scores, with performance drops ranging from 9% to 61% depending on dataset, likely due to class imbalance and boundary ambiguity. Additionally, we observe a moderate positive correlation between nerve size and segmentation accuracy (Pearson r=0.587, p<0.001), indicating that smaller nerves remain a primary challenge. These findings provide methodological guidance for developing robust ultrasound nerve segmentation systems under realistic clinical data constraints.

</details>


### [111] [DVLA-RL: Dual-Level Vision-Language Alignment with Reinforcement Learning Gating for Few-Shot Learning](https://arxiv.org/abs/2602.00795)
*Wenhao Li,Xianjing Meng,Qiangchang Wang,Zhongyi Han,Zhibin Wu,Yilong Yin*

Main category: cs.CV

TL;DR: 本文提出了DVLA-RL方法，通过双层语义构建和强化学习门控注意力机制，实现了视觉与语言从低层到高层语义的逐步和自适应对齐，有效提升了小样本学习的表现，在多个基准测试中取得了最新的最优成绩。


<details>
  <summary>Details</summary>
Motivation: 现有的小样本学习方法虽然引入了大语言模型丰富视觉表征，但忽视了视觉与语言在不同语义层次（从低级到高级）的逐步和动态对齐，导致语义增益有限。

Method: 提出DVLA-RL方法，包括双层语义构建（DSC），利用类别名和支持样本引导LLMs生成区分性属性并合成全面的类别描述；以及RL门控注意力（RLA），将跨模态融合视为顺序决策过程，借助轻量级策略网络自适应调整自注意力和跨注意力比重，实现在不同网络层灵活整合视觉和文本信息。

Result: DVLA-RL方法在九个基准测试、三个不同小样本学习场景下取得了新的SOTA性能，表现优于现有方法。

Conclusion: 通过逐层动态对齐视觉和语言的低级属性与高级语义，DVLA-RL极大提升了视觉-语言对齐的精度和泛化能力，有效促进了小样本学习的类别区分与泛化表现。

Abstract: Few-shot learning (FSL) aims to generalize to novel categories with only a few samples. Recent approaches incorporate large language models (LLMs) to enrich visual representations with semantic embeddings derived from class names. However, they overlook progressive and adaptive alignment between vision and language from low-level to high-level semantics, resulting in limited semantic gains. To address these challenges, we propose Dual-level Vision-Language Alignment with Reinforcement Learning gating (DVLA-RL), which consists of Dual-level Semantic Construction (DSC) and RL-gated Attention (RLA). Specifically, DSC conditions LLMs on both class names and support samples to generate discriminative attributes, progressively selects the most relevant ones, and then synthesizes them into coherent class descriptions. This process provides complementary low-level attributes and high-level descriptions, enabling both fine-grained grounding and holistic class understanding. To dynamically integrate dual-level semantics along with the visual network layers, RLA formulates cross-modal fusion as a sequential decision process. A lightweight policy trained with episodic REINFORCE adaptively adjusts the contributions of self-attention and cross-attention to integrate textual and visual tokens. As a result, shallow layers refine local attributes and deep layers emphasize global semantics, enabling more precise cross-modal alignment. This achieves class-specific discrimination and generalized representations with merely a few support samples. DVLA-RL achieves new state-of-the-art performance across nine benchmarks in three diverse FSL scenarios.

</details>


### [112] [Generating a Paracosm for Training-Free Zero-Shot Composed Image Retrieval](https://arxiv.org/abs/2602.00813)
*Tong Wang,Yunhan Zhao,Shu Kong*

Main category: cs.CV

TL;DR: 本文提出了一种全新的复合图像检索（CIR）方法，通过直接生成并利用“心理图像”进行检索，极大提升了零样本CIR任务的表现。


<details>
  <summary>Details</summary>
Motivation: 当前CIR任务的最大挑战在于“心理图像”只在语义层面存在，缺乏实体参考，导致检索准确性受限。传统方法多基于文本描述而非直接生成该心理图像，存在语义鸿沟。

Method: 本文提出Paracosm方法：首先用大型多模态模型（LMM）基于参考图像和修改文本直接生成“心理图像”；其次，为数据库中每张真实图像生成对应合成图像，将多模态检索映射到合成图像空间；最后，通过合成图像的相似性实现目标图像的匹配，无需额外训练（零样本）。

Result: Paracosm在四个主流复合图像检索基准上均显著超过现有零样本方法，达到了最新的性能水平。

Conclusion: 本文证明了直接生成并利用“心理图像”能大幅提升CIR的零样本检索准确率。所提方法完全无需训练，为多模态检索提供了新范式和显著应用前景。

Abstract: Composed Image Retrieval (CIR) is the task of retrieving a target image from a database using a multimodal query, which consists of a reference image and a modification text. The text specifies how to alter the reference image to form a ``mental image'', based on which CIR should find the target image in the database. The fundamental challenge of CIR is that this ``mental image'' is not physically available and is only implicitly defined by the query. The contemporary literature pursues zero-shot methods and uses a Large Multimodal Model (LMM) to generate a textual description for a given multimodal query, and then employs a Vision-Language Model (VLM) for textual-visual matching to search the target image. In contrast, we address CIR from first principles by directly generating the ``mental image'' for more accurate matching. Particularly, we prompt an LMM to generate a ``mental image'' for a given multimodal query and propose to use this ``mental image'' to search for the target image. As the ``mental image'' has a synthetic-to-real domain gap with real images, we also generate a synthetic counterpart for each real image in the database to facilitate matching. In this sense, our method uses LMM to construct a ``paracosm'', where it matches the multimodal query and database images. Hence, we call this method Paracosm. Notably, Paracosm is a training-free zero-shot CIR method. It significantly outperforms existing zero-shot methods on four challenging benchmarks, achieving state-of-the-art performance for zero-shot CIR.

</details>


### [113] [Edge-Native Generative De-identification: Inversion-Free Flow for Privacy-Preserving Federated Skin Image Analysis](https://arxiv.org/abs/2602.00821)
*Konstantinos Moutselos,Ilias Maglogiannis*

Main category: cs.CV

TL;DR: 该论文提出了一种面向皮肤病学联邦学习的隐私保护图像处理框架，不依赖传统的去标识方法，通过高效的生成式模型在本地实现高保真病理特征保留和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在皮肤病学临床应用受限于患者隐私保护和病理特征保真的对立需求。传统去标识方法损害诊断信息，而主流生成方法又需要过高计算资源，不适合边缘设备。

Method: 提出了基于Rectified Flow Transformers（FlowEdit）的无反演高效身份转换机制，能在临床节点本地实现20秒内的高保真图像处理，并设计了“Segment-by-Synthesis”机制本地生成对照健康与患病图像对，从而提取不含生物识别特征的红斑差异掩码。

Result: 在高分辨率临床图像上初步验证中，跨合成身份IoU稳定性超过0.67，表明该方法在保持病理特征和去标识间表现优秀。

Conclusion: 该框架能在边缘生成合规的隐私代理数据，降低源端梯度泄露风险，为皮肤图像高精度联邦分析在隐私敏感环境下的部署提供了有效方案。

Abstract: The deployment of Federated Learning (FL) for clinical dermatology is hindered by the competing requirements of protecting patient privacy and preserving diagnostic features. Traditional de-identification methods often degrade pathological fidelity, while standard generative editing techniques rely on computationally intensive inversion processes unsuitable for resource-constrained edge devices. We propose a framework for identity-agnostic pathology preservation that serves as a client-side privacy-preserving utility. By leveraging inversion-free Rectified Flow Transformers (FlowEdit), the system performs high-fidelity identity transformation in near real-time (less than 20s), facilitating local deployment on clinical nodes. We introduce a "Segment-by-Synthesis" mechanism that generates counterfactual healthy and pathological twin pairs locally. This enables the extraction of differential erythema masks that are decoupled from biometric markers and semantic artifacts (e.g. jewelry). Pilot validation on high-resolution clinical samples demonstrates an Intersection over Union (IoU) stability greater than 0.67 across synthetic identities. By generating privacy-compliant synthetic surrogates at the edge, this framework mitigates the risk of gradient leakage at the source, providing a secure pathway for high-precision skin image analysis in federated environments.

</details>


### [114] [TransNormal: Dense Visual Semantics for Diffusion-based Transparent Object Normal Estimation](https://arxiv.org/abs/2602.00839)
*Mingwei Li,Hehe Fan,Yi Yang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为TransNormal的新方法，用于提升透明物体单目法线估计的精确度，并显著超过了现有技术。


<details>
  <summary>Details</summary>
Motivation: 实验室自动化需要对透明物体的精确几何理解，但透明物体因折射与反射特性，导致传统深度与法线传感器在实际环境下表现不佳，严重妨碍了AI系统的部署。因此，开发高精度透明物体法线估计工具十分关键。

Method: 提出TransNormal框架，将预训练的扩散模型（diffusion prior）用于单步法线回归，通过cross-attention机制整合DINOv3特征获取丰富的几何语义信息，还结合多任务学习目标及基于小波的正则，提升细节结构的保留。同时建立了面向透明实验器皿的物理模拟数据集TransNormal-Synthetic。

Result: TransNormal在ClearGrasp基准上提升显著，平均误差降低24.4%，11.25°准确率提升22.8%；在ClearPose数据集上平均误差降低15.2%。

Conclusion: TransNormal有效解决了透明物体单目法线估计的难题，在多个公开数据集上均大幅提升了现有技术的表现，为实验室自动化等应用场景中的AI落地提供了重要支撑。

Abstract: Monocular normal estimation for transparent objects is critical for laboratory automation, yet it remains challenging due to complex light refraction and reflection. These optical properties often lead to catastrophic failures in conventional depth and normal sensors, hindering the deployment of embodied AI in scientific environments. We propose TransNormal, a novel framework that adapts pre-trained diffusion priors for single-step normal regression. To handle the lack of texture in transparent surfaces, TransNormal integrates dense visual semantics from DINOv3 via a cross-attention mechanism, providing strong geometric cues. Furthermore, we employ a multi-task learning objective and wavelet-based regularization to ensure the preservation of fine-grained structural details. To support this task, we introduce TransNormal-Synthetic, a physics-based dataset with high-fidelity normal maps for transparent labware. Extensive experiments demonstrate that TransNormal significantly outperforms state-of-the-art methods: on the ClearGrasp benchmark, it reduces mean error by 24.4% and improves 11.25° accuracy by 22.8%; on ClearPose, it achieves a 15.2% reduction in mean error. The code and dataset will be made publicly available at https://longxiang-ai.github.io/TransNormal.

</details>


### [115] [Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition](https://arxiv.org/abs/2602.00841)
*Jintao Cheng,Weibin Li,Zhijian He,Jin Wu,Chi Man Vong,Wei Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种无需训练的VPR方法，通过二阶几何统计建模场景结构，利用SPD流形上的协方差描述和黎曼映射实现鲁棒表征，在零样本场景下性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有视觉位置识别方法对环境和视角变化的鲁棒性有限，且大多依赖大规模有监督数据或简单的低阶统计，忽视了图像复杂结构信息。

Method: 作者提出用对称正定(SPD)流形上的协方差描述符建模场景结构扰动，并通过几何感知的黎曼映射将其转到欧式空间以消除噪声影响，无需进一步训练，基于固定预训练特征。

Result: 方法在多个公开数据集上与SOTA方法相比，尤其在零样本泛化任务上表现出色，验证了其高兼容性和鲁棒性。

Conclusion: 该方法可有效增强VPR对环境和视角变化的适应性，在无需再训练的情况下即可达到领先性能，适合低资源或泛化场景应用。

Abstract: Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios.

</details>


### [116] [Distill3R: A Pipeline for Democratizing 3D Foundation Models on Commodity Hardware](https://arxiv.org/abs/2602.00865)
*Brandon Leblanc,Charalambos Poullis*

Main category: cs.CV

TL;DR: 本文提出了Distill3R框架，通过模型蒸馏技术将大规模三维基础模型的几何推理能力迁移到可在单台工作站训练的小型模型，大幅降低了三维重建领域的计算门槛。


<details>
  <summary>Details</summary>
Motivation: 为了应对当前多视角三维重建领域基础模型训练时对大规模计算集群的高度依赖，降低研究者的入门门槛，从而让更多实验室能够参与三维视觉研究。

Method: 主要创新包括：（1）离线缓存管道，将教师模型的高计算推理过程与学生模型的训练解耦，并用压缩后的监督信号进行高效蒸馏；（2）基于置信度的蒸馏损失函数，利用教师模型的不确定性引导学生模型在常规硬件上训练。提出的学生模型参数量仅为教师的1/9，且推理速度提升5倍。

Result: 学生模型（72M参数）保持了必要的结构一致性和三维几何理解，推理速度提升5倍，可在单台工作站3天内完成训练，远低于教师模型所需的巨量GPU和一周时间。

Conclusion: Distill3R为三维视觉领域提供了可复现、低成本的训练配方，有助于更多研究团队开展自主三维视觉模型开发，为高效边缘部署和学术研究提供了新起点。

Abstract: While multi-view 3D reconstruction has shifted toward large-scale foundation models capable of inferring globally consistent geometry, their reliance on massive computational clusters for training has created a significant barrier to entry for most academic laboratories. To bridge this compute divide, we introduce Distill3R, a framework designed to distill the geometric reasoning of 3D foundation models into compact students fully trainable on a single workstation. Our methodology centers on two primary innovations: (1) an offline caching pipeline that decouples heavy teacher inference from the training loop through compressed supervision signals, and (2) a confidence-aware distillation loss that leverages teacher uncertainty to enable training on commodity hardware. We propose a 72M-parameter student model which achieves a 9x reduction in parameters and a 5x inference speedup compared to its 650M-parameter teacher. The student is fully trainable in under 3 days on a single workstation, whereas its teacher requires massive GPU clusters for up to a week. We demonstrate that the student preserves the structural consistency and qualitative geometric understanding required for functional 3D awareness. By providing a reproducible, single-workstation training recipe, Distill3R serves as an exploratory entry point for democratized 3D vision research and efficient edge deployment. This work is not intended to compete with state-of-the-art foundation models, but to provide an accessible research baseline for laboratories without access to large-scale compute to train and specialize models on their own domain-specific data at minimal cost.

</details>


### [117] [DIAMOND: Directed Inference for Artifact Mitigation in Flow Matching Models](https://arxiv.org/abs/2602.00883)
*Alicja Polowczyk,Agnieszka Polowczyk,Piotr Borycki,Joanna Waczyńska,Jacek Tabor,Przemysław Spurek*

Main category: cs.CV

TL;DR: 本文提出了一种全新的零训练、无需修改权重即可在生成推理过程中纠正和减少图像生成伪影的方法DIAMOND，有效提升了扩散模型的图像质量。


<details>
  <summary>Details</summary>
Motivation: 近年来文本到图像生成模型取得了令人瞩目的成果，但在实际和专业应用上，生成图像中视觉及解剖伪影仍未有效解决。现有方法通常为生成后的修复，不仅效率低，还需对模型权重做侵入性改动或消耗大量算力进行区域细化处理，影响实际应用。

Method: 作者提出DIAMOND方法，无需重新训练模型，也无需更改模型权重，而是在推理过程中通过估算每一步的“干净”样本，对生成轨迹进行纠正，实时避免伪影产生。此方法可以直接应用于扩散模型，不增加额外训练或调整，属于零样本（zero-shot）推理修正。

Result: DIAMOND方法在多个扩散模型和生成任务中均取得了不增加训练成本情况下的高保真、无伪影图像生成效果，相关代码已开源。

Conclusion: DIAMOND为深度生成模型带来了训练无关、推理时轨迹纠正的高效方案，无需修改权重即可大幅减少伪影，为图像生成的实际应用和专业部署提供了更有前景的解决路径。

Abstract: Despite impressive results from recent text-to-image models like FLUX, visual and anatomical artifacts remain a significant hurdle for practical and professional use. Existing methods for artifact reduction, typically work in a post-hoc manner, consequently failing to intervene effectively during the core image formation process. Notably, current techniques require problematic and invasive modifications to the model weights, or depend on a computationally expensive and time-consuming process of regional refinement. To address these limitations, we propose DIAMOND, a training-free method that applies trajectory correction to mitigate artifacts during inference. By reconstructing an estimate of the clean sample at every step of the generative trajectory, DIAMOND actively steers the generation process away from latent states that lead to artifacts. Furthermore, we extend the proposed method to standard Diffusion Models, demonstrating that DIAMOND provides a robust, zero-shot path to high-fidelity, artifact-free image synthesis without the need for additional training or weight modifications in modern generative architectures. Code is available at https://gmum.github.io/DIAMOND/

</details>


### [118] [OCTOPUS: Enhancing the Spatial-Awareness of Vision SSMs with Multi-Dimensional Scans and Traversal Selection](https://arxiv.org/abs/2602.00904)
*Kunal Mahatha,Ali Bahri,Pierre Marza,Sahar Dastani,Maria Vakalopoulou,Stergios Christodoulidis,Jose Dolz,Christian Desrosiers*

Main category: cs.CV

TL;DR: 提出OCTOPUS架构，弥补传统SSM在视觉任务中空间建模不足，兼顾全局与局部结构，并保持线性复杂度，取得更优的图像分割与分类表现。


<details>
  <summary>Details</summary>
Motivation: 传统SSM因其线性复杂度被视为Transformer的有力替代者，但其因因果建模适用于序列文本，应用到视觉领域时却破坏了像素/patch间的空间关系，导致无法有效捕捉局部空间信息。

Method: 提出OCTOPUS架构，在水平方向、垂直方向和对角线方向进行八向离散递归，实现多方向信息传递，保证了独立patch间的独立性，兼顾全局和局部空间结构，同时保持SSM的线性复杂度。

Result: OCTOPUS在图像分类和分割基准测试中，在分割边界保持和区域一致性方面优于现有V-SSM模型，同时分类精度也更高。

Conclusion: OCTOPUS为多方向递归建模提供了基础方法，实现了空间感知且高效的视觉架构，适合于可扩展的视觉任务。

Abstract: State space models (SSMs) have recently emerged as an alternative to transformers due to their unique ability of modeling global relationships in text with linear complexity. However, their success in vision tasks has been limited due to their causal formulation, which is suitable for sequential text but detrimental in the spatial domain where causality breaks the inherent spatial relationships among pixels or patches. As a result, standard SSMs fail to capture local spatial coherence, often linking non-adjacent patches while ignoring neighboring ones that are visually correlated. To address these limitations, we introduce OCTOPUS , a novel architecture that preserves both global context and local spatial structure within images, while maintaining the linear complexity of SSMs. OCTOPUS performs discrete reoccurrence along eight principal orientations, going forward or backward in the horizontal, vertical, and diagonal directions, allowing effective information exchange across all spatially connected regions while maintaining independence among unrelated patches. This design enables multi-directional recurrence, capturing both global context and local spatial structure with SSM-level efficiency. In our classification and segmentation benchmarks, OCTOPUS demonstrates notable improvements in boundary preservation and region consistency, as evident from the segmentation results, while maintaining relatively better classification accuracy compared to existing V-SSM based models. These results suggest that OCTOPUS appears as a foundation method for multi-directional recurrence as a scalable and effective mechanism for building spatially aware and computationally efficient vision architectures.

</details>


### [119] [ConsensusDrop: Fusing Visual and Cross-Modal Saliency for Efficient Vision Language Models](https://arxiv.org/abs/2602.00946)
*Dhruv Parikh,Haoyang Fan,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.CV

TL;DR: 该论文提出ConsensusDrop框架，通过融合视觉编码器显著性和LLM跨注意力信号，优化视觉语言模型（VLM）中的视觉token选择和压缩。在保证精度的同时，实现更高效率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型中，LLM需要处理大量冗余视觉token，导致运算非常昂贵。已有token裁剪方法只利用视觉显著性或跨注意力中的一种信号，效果有限，因此作者希望联合二者优势，提高token选择的有效性和效率。

Method: 作者提出一种无需额外训练的ConsensusDrop框架，结合视觉编码器显著性（广泛、但与查询无关）和LLM跨注意力（与查询相关但稀疏且昂贵）信号，生成共识排序，对token进行保留或由编码器引导的合并。这样在进入LLM前即可高效裁剪token。

Result: 在多种开源VLM（如LLaVA-1.5/NeXT、Video-LLaVA等）上，ConsensusDrop在相同token预算下显著优于已有裁剪方法，在精度-效率平衡上表现更佳。即便大幅裁剪token，也能保持近似基线的精度，同时减少TTFT和KV缓存开销。

Conclusion: ConsensusDrop无需训练、易于集成，能高效地减少VLM视觉token，兼顾精度与计算效率，为构建高效大规模视觉语言模型提供了一种优雅解决方案。

Abstract: Vision-Language Models (VLMs) are expensive because the LLM processes hundreds of largely redundant visual tokens. Existing token reduction methods typically exploit \textit{either} vision-encoder saliency (broad but query-agnostic) \textit{or} LLM cross-attention (query-aware but sparse and costly). We show that neither signal alone is sufficient: fusing them consistently improves performance compared to unimodal visual token selection (ranking). However, making such fusion practical is non-trivial: cross-modal saliency is usually only available \emph{inside} the LLM (too late for efficient pre-LLM pruning), and the two signals are inherently asymmetric, so naive fusion underutilizes their complementary strengths. We propose \textbf{ConsensusDrop}, a training-free framework that derives a \emph{consensus} ranking by reconciling vision encoder saliency with query-aware cross-attention, retaining the most informative tokens while compressing the remainder via encoder-guided token merging. Across LLaVA-1.5/NeXT, Video-LLaVA, and other open-source VLMs, ConsensusDrop consistently outperforms prior pruning methods under identical token budgets and delivers a stronger accuracy-efficiency Pareto frontier -- preserving near-baseline accuracy even at aggressive token reductions while reducing TTFT and KV cache footprint. Our code will be open-sourced.

</details>


### [120] [Data Augmentation for High-Fidelity Generation of CAR-T/NK Immunological Synapse Images](https://arxiv.org/abs/2602.00949)
*Xiang Zhang,Boxuan Zhang,Alireza Naghizadeh,Mohab Mohamed,Dongfang Liu,Ruixiang Tang,Dimitris Metaxas,Dongfang Liu*

Main category: cs.CV

TL;DR: 该论文提出了两种数据增强方法，以提升CAR-T/NK细胞免疫突触（IS）结构的检测与分割精度，从而提高影像学标志物在预测免疫治疗疗效上的实用性。


<details>
  <summary>Details</summary>
Motivation: 虽然CAR-T/NK细胞免疫疗法已改变癌症治疗方式，且IS结构质量可能预测疗效，但受限于注释显微图像数据集规模有限，人工神经网络（ANN）难以泛化，成为IS自动化定量的主要挑战。

Method: 提出（1）实例感知自动增强（IAAA）：对原始IS图像应用优化增强策略，生成合成图像及分割掩膜，并支持多种成像模态；（2）语义感知AI增强（SAAA）：融合基于扩散的掩膜生成器与Pix2Pix条件图像生成器，创建多样且逼真的IS分割掩膜及高保真图像。两方法联合显著扩充训练数据。

Result: 两种增强方法生成的合成图像在视觉及结构属性上高度接近真实数据，极大改善了CAR-T/NK IS的检测与分割准确性。

Conclusion: 通过提升IS量化的鲁棒性和准确性，该工作为开发更可靠的基于影像的疗效预测生物标志物提供了重要工具和方法。

Abstract: Chimeric antigen receptor (CAR)-T and NK cell immunotherapies have transformed cancer treatment, and recent studies suggest that the quality of the CAR-T/NK cell immunological synapse (IS) may serve as a functional biomarker for predicting therapeutic efficacy. Accurate detection and segmentation of CAR-T/NK IS structures using artificial neural networks (ANNs) can greatly increase the speed and reliability of IS quantification. However, a persistent challenge is the limited size of annotated microscopy datasets, which restricts the ability of ANNs to generalize. To address this challenge, we integrate two complementary data-augmentation frameworks. First, we employ Instance Aware Automatic Augmentation (IAAA), an automated, instance-preserving augmentation method that generates synthetic CAR-T/NK IS images and corresponding segmentation masks by applying optimized augmentation policies to original IS data. IAAA supports multiple imaging modalities (e.g., fluorescence and brightfield) and can be applied directly to CAR-T/NK IS images derived from patient samples. In parallel, we introduce a Semantic-Aware AI Augmentation (SAAA) pipeline that combines a diffusion-based mask generator with a Pix2Pix conditional image synthesizer. This second method enables the creation of diverse, anatomically realistic segmentation masks and produces high-fidelity CAR-T/NK IS images aligned with those masks, further expanding the training corpus beyond what IAAA alone can provide. Together, these augmentation strategies generate synthetic images whose visual and structural properties closely match real IS data, significantly improving CAR-T/NK IS detection and segmentation performance. By enhancing the robustness and accuracy of IS quantification, this work supports the development of more reliable imaging-based biomarkers for predicting patient response to CAR-T/NK immunotherapy.

</details>


### [121] [Hybrid Topological and Deep Feature Fusion for Accurate MRI-Based Alzheimer's Disease Severity Classification](https://arxiv.org/abs/2602.00956)
*Faisal Ahmed*

Main category: cs.CV

TL;DR: 本文提出了一种将拓扑数据分析（TDA）与DenseNet121网络结合的新型混合深度学习框架，在OASIS结构性MRI数据集上用于阿尔茨海默症四分类，取得了99.93%的准确率和100%的AUC，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默症的早期与准确诊断在临床决策中非常关键，但现有依赖神经影像的辅助系统识别率仍有待提升，尤其是对脑部结构中难以捕获的拓扑特征。

Method: 作者将拓扑数据分析方法与深度卷积神经网络DenseNet121结合，TDA用于提取大脑结构的拓扑特征，DenseNet121提取MRI切片的空间层次特征，二者特征融合后用于阿尔茨海默症四阶段的分类。

Result: 在OASIS-1 Kaggle MRI数据集上的实验结果显示，该模型准确率达99.93%，AUC为100%，明显超过最新的CNN、迁移学习、集成和多尺度架构。

Conclusion: 将拓扑特征引入深度学习流程极大增强了分类性能，提出的TDA+DenseNet121框架为自动化阿尔茨海默症诊断提供了强大且高准确率的工具。

Abstract: Early and accurate diagnosis of Alzheimer's disease (AD) remains a critical challenge in neuroimaging-based clinical decision support systems. In this work, we propose a novel hybrid deep learning framework that integrates Topological Data Analysis (TDA) with a DenseNet121 backbone for four-class Alzheimer's disease classification using structural MRI data from the OASIS dataset. TDA is employed to capture complementary topological characteristics of brain structures that are often overlooked by conventional neural networks, while DenseNet121 efficiently learns hierarchical spatial features from MRI slices. The extracted deep and topological features are fused to enhance class separability across the four AD stages.
  Extensive experiments conducted on the OASIS-1 Kaggle MRI dataset demonstrate that the proposed TDA+DenseNet121 model significantly outperforms existing state-of-the-art approaches. The model achieves an accuracy of 99.93% and an AUC of 100%, surpassing recently published CNN-based, transfer learning, ensemble, and multi-scale architectures. These results confirm the effectiveness of incorporating topological insights into deep learning pipelines and highlight the potential of the proposed framework as a robust and highly accurate tool for automated Alzheimer's disease diagnosis.

</details>


### [122] [Unveiling the Cognitive Compass: Theory-of-Mind-Guided Multimodal Emotion Reasoning](https://arxiv.org/abs/2602.00971)
*Meng Luo,Bobo Li,Shanqing Xu,Shize Zhang,Qiuchan Chen,Menglu Han,Wenhao Chen,Yanxiang Huang,Hao Fei,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: 本文提出了HitEmotion基准和TMPO方法，专注于提升和评估多模态大语言模型（MLLMs）的深层情感理解和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在情感理解方面表现有限，缺乏基于心智理论（ToM）的情感推理能力。作者认为，真实有效的情感智能必须依赖对心智理论的明确建模。

Method: 1）提出HitEmotion基准，采用基于ToM的分层结构，用于检测模型在不同认知深度下的能力断点；2）提出ToM引导的推理链协同跟踪心理状态，结合跨模态证据以实现更准确的情感推理；3）引入TMPO强化学习方法，将中间心理状态作为过程级监督，提升模型推理能力。

Result: 实验证明HitEmotion能暴露当前最先进模型在复杂认知任务上的情感推理缺陷。ToM引导的推理链和TMPO方法能有效提升模型在相关任务中的准确率，并生成更有逻辑、更符合实际的推理解释。

Conclusion: 该研究为学界提供了一个实用的评测与增强多模态语言模型认知基础情感理解能力的工具包，有助于推动MLLMs情感智能的发展。

Abstract: Despite rapid progress in multimodal large language models (MLLMs), their capability for deep emotional understanding remains limited. We argue that genuine affective intelligence requires explicit modeling of Theory of Mind (ToM), the cognitive substrate from which emotions arise. To this end, we introduce HitEmotion, a ToM-grounded hierarchical benchmark that diagnoses capability breakpoints across increasing levels of cognitive depth. Second, we propose a ToM-guided reasoning chain that tracks mental states and calibrates cross-modal evidence to achieve faithful emotional reasoning. We further introduce TMPO, a reinforcement learning method that uses intermediate mental states as process-level supervision to guide and strengthen model reasoning. Extensive experiments show that HitEmotion exposes deep emotional reasoning deficits in state-of-the-art models, especially on cognitively demanding tasks. In evaluation, the ToM-guided reasoning chain and TMPO improve end-task accuracy and yield more faithful, more coherent rationales. In conclusion, our work provides the research community with a practical toolkit for evaluating and enhancing the cognition-based emotional understanding capabilities of MLLMs. Our dataset and code are available at: https://HitEmotion.github.io/.

</details>


### [123] [VAMOS-OCTA: Vessel-Aware Multi-Axis Orthogonal Supervision for Inpainting Motion-Corrupted OCT Angiography Volumes](https://arxiv.org/abs/2602.00995)
*Nick DiSanto,Ehsan Khodapanah Aghdam,Han Liu,Jacob Watson,Yuankai K. Tao,Hao Li,Ipek Oguz*

Main category: cs.CV

TL;DR: 本文提出了一种结合深度学习和多轴血管感知监督的新方法（VAMOS-OCTA），用于修复手持OCTA在扫描中因运动带来的体积图像退化，显著提升了2D、3D重建的质量。


<details>
  <summary>Details</summary>
Motivation: 手持OCTA尽管适用于不配合或儿童受试者，但易受运动伪影影响，导致成像出现空白和严重信息丢失。以往方法大多只专注于2D投影恢复，难以兼顾B-scan的锐度和整体血管连续性的重建，因此需要更强大且全面的方法提升图像可用性。

Method: 作者提出了VAMOS-OCTA深度学习框架，采用2.5D U-Net网络，输入一组相邻B-scan，重建中间被运动损坏的B-scan。同时设计了血管感知多轴正交监督损失（VAMOS loss），结合血管加权重建、轴向与横向一致性，实现原生B-scan与多投影血管结构的联合恢复。模型在合成和真实带伪影OCTA数据集上训练并评测。

Result: VAMOS-OCTA在视觉和像素级指标上均显著优于以往方法，展现出清晰的毛细血管、完整的血管连续性和干净的2D投影，尤其在严重运动损伤场景下也有较好表现。

Conclusion: 多轴血管感知监督为OCTA三维运动补偿修复提供了有效约束，能全面提升体积数据的可用性和结构还原能力，为手持OCTA应用拓宽了临床与研究前景。

Abstract: Handheld Optical Coherence Tomography Angiography (OCTA) enables noninvasive retinal imaging in uncooperative or pediatric subjects, but is highly susceptible to motion artifacts that severely degrade volumetric image quality. Sudden motion during 3D acquisition can lead to unsampled retinal regions across entire B-scans (cross-sectional slices), resulting in blank bands in en face projections. We propose VAMOS-OCTA, a deep learning framework for inpainting motion-corrupted B-scans using vessel-aware multi-axis supervision. We employ a 2.5D U-Net architecture that takes a stack of neighboring B-scans as input to reconstruct a corrupted center B-scan, guided by a novel Vessel-Aware Multi-Axis Orthogonal Supervision (VAMOS) loss. This loss combines vessel-weighted intensity reconstruction with axial and lateral projection consistency, encouraging vascular continuity in native B-scans and across orthogonal planes. Unlike prior work that focuses primarily on restoring the en face MIP, VAMOS-OCTA jointly enhances both cross-sectional B-scan sharpness and volumetric projection accuracy, even under severe motion corruptions. We trained our model on both synthetic and real-world corrupted volumes and evaluated its performance using both perceptual quality and pixel-wise accuracy metrics. VAMOS-OCTA consistently outperforms prior methods, producing reconstructions with sharp capillaries, restored vessel continuity, and clean en face projections. These results demonstrate that multi-axis supervision offers a powerful constraint for restoring motion-degraded 3D OCTA data. Our source code is available at https://github.com/MedICL-VU/VAMOS-OCTA.

</details>


### [124] [CortiNet: A Physics-Perception Hybrid Cortical-Inspired Dual-Stream Network for Gallbladder Disease Diagnosis from Ultrasound](https://arxiv.org/abs/2602.01000)
*Vagish Kumar,Souvik Chakraborty*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级、受脑皮层启发的神经网络CortiNet，用于胆囊疾病超声诊断，兼顾高效性和准确性，并提升了解释性。


<details>
  <summary>Details</summary>
Motivation: 当前超声图像分辨率低且多噪声，传统大模型虽能提升诊断准确率但难以实际部署。因此需要一种兼具轻量化和高性能的新型模型。

Method: CortiNet采用仿人脑皮层双流结构，分别处理低频结构信息与高频细节，并通过物理解释的多尺度信号分解和专门编码流，最终利用融合机制整合优势特征；同时仅在结构分支上进行解释性分析，以提升模型对噪声的鲁棒性。

Result: 在包含9类、10692张专家标注胆囊疾病超声图像数据集上，CortiNet以远少于传统卷积神经网络的参数量达到98.74%的高诊断准确率。

Conclusion: CortiNet能在保证诊断准确率和临床实用性的前提下，大幅降低模型复杂度与资源需求，同时具备良好的医学解释性，对超声辅助诊疗有较好促进作用。

Abstract: Ultrasound imaging is the primary diagnostic modality for detecting Gallbladder diseases due to its non-invasive nature, affordability, and wide accessibility. However, the low resolution and speckle noise inherent to ultrasound images hinder diagnostic reliability, prompting the use of large convolutional neural networks that are difficult to deploy in routine clinical settings. In this work, we propose CortiNet, a lightweight, cortical-inspired dual-stream neural architecture for gallbladder disease diagnosis that integrates physically interpretable multi-scale signal decomposition with perception-driven feature learning. Inspired by parallel processing pathways in the human visual cortex, CortiNet explicitly separates low-frequency structural information from high-frequency perceptual details and processes them through specialized encoding streams. By operating directly on structured, frequency-selective representations rather than raw pixel intensities, the architecture embeds strong physics-based inductive bias, enabling efficient feature learning with a significantly reduced parameter footprint. A late-stage cortical-style fusion mechanism integrates complementary structural and textural cues while preserving computational efficiency. Additionally, we propose a structure-aware explainability framework wherein gradient-weighted class activation mapping is only applied to the structural branch of the proposed CortiNet architecture. This choice allows the model to only focus on the structural features, making it robust against speckle noise. We evaluate CortiNet on 10,692 expert-annotated images spanning nine clinically relevant gallbladder disease categories. Experimental results demonstrate that CortiNet achieves high diagnostic accuracy (98.74%) with only a fraction of the parameters required by conventional deep convolutional models.

</details>


### [125] [SRVAU-R1: Enhancing Video Anomaly Understanding via Reflection-Aware Learning](https://arxiv.org/abs/2602.01004)
*Zihao Zhao,Shengting Cao,Muchao Ye*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态大语言模型（MLLM）方法SRVAU-R1，通过引入自我反思机制，显著提升了视频异常理解任务中的推理能力和定位准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLM方法在视频异常理解（VAU）任务中仍停留在对异常表面描述，缺乏对异常行为的深层推理，尤其是自我反思和自我纠错能力。

Method: 提出了SRVAU-R1框架，核心为反思感知的学习方法。具体包括：1）构建了首个针对VAU的反思型链式思维（Chain-of-Thought）数据集，数据结构包括初步推理、自我反思及修正后的推理过程；2）结合有监督微调和强化微调，提升多模态推理效果。

Result: 在多个视频异常理解基准测试上，SRVAU-R1在时间异常定位准确性以及推理质量上均优于现有方法，取得显著提升。

Conclusion: 通过引入自我反思机制，SRVAU-R1有效提升了MLLM在视频异常理解中的表现，为多模态推理带来新的进展。

Abstract: Multi-modal large language models (MLLMs) have demonstrated significant progress in reasoning capabilities and shown promising effectiveness in video anomaly understanding (VAU) tasks. However, existing MLLM-based approaches remain largely focused on surface-level descriptions of anomalies, lacking deep reasoning over abnormal behaviors like explicit self-reflection and self-correction. To address that, we propose Self-Reflection-Enhanced Reasoning for Video Anomaly Understanding (SRVAU-R1), a reflection-aware learning framework that incorporates reflection in MLLM reasoning. Specifically, SRVAU-R1 introduces the first reflection-oriented Chain-of-Thought dataset tailored for VAU, providing structured supervision with initial reasoning, self-reflection, and revised reasoning. Based on that, it includes a novel reflection-aware learning paradigm with supervised fine-tuning and reinforcement fine-tuning to enhance multi-modal reasoning for VAU. Extensive experiments on multiple video anomaly benchmarks demonstrate that SRVAU-R1 consistently outperforms existing methods, achieving significant improvements in both temporal anomaly localization accuracy and reasoning quality.

</details>


### [126] [LocalScore: Local Density-Aware Similarity Scoring for Biometrics](https://arxiv.org/abs/2602.01012)
*Yiyang Su,Minchul Kim,Jie Zhu,Christopher Perry,Feng Liu,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TL;DR: 本文提出LocalScore方法，通过利用画廊特征分布的局部密度提升开放集生物识别系统的鲁棒性和性能。方法具有通用性、无关网络结构和损失函数，实验结果显著优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 开放集生物识别系统在遇到未登记的个体时识别能力较弱，现有方法往往因压缩同一身份的多样性特征而导致决策边界不佳，急需提升系统对未注册对象的判别能力。

Method: 提出LocalScore算法，引入k近邻思想，利用画廊中特征的局部密度信息进行判分。方法无需改变现有网络结构或损失函数，可直接集成到各类生物识别系统中，几乎不增加计算开销。

Result: LocalScore在多种模态下实验，开放集检索FNIR@FPIR由53%降至40%，验证任务TAR@FAR由51%升至74%，均显著优于常规方法。文中还提供了理论分析与实证，说明在何种数据集特性下获得最大提升。

Conclusion: LocalScore简单有效，极易集成于现有系统，实现了开放集识别性能的大幅提升，尤其适用于具有小样本变异的实际场景。

Abstract: Open-set biometrics faces challenges with probe subjects who may not be enrolled in the gallery, as traditional biometric systems struggle to detect these non-mated probes. Despite the growing prevalence of multi-sample galleries in real-world deployments, most existing methods collapse intra-subject variability into a single global representation, leading to suboptimal decision boundaries and poor open-set robustness. To address this issue, we propose LocalScore, a simple yet effective scoring algorithm that explicitly incorporates the local density of the gallery feature distribution using the k-th nearest neighbors. LocalScore is architecture-agnostic, loss-independent, and incurs negligible computational overhead, making it a plug-and-play solution for existing biometric systems. Extensive experiments across multiple modalities demonstrate that LocalScore consistently achieves substantial gains in open-set retrieval (FNIR@FPIR reduced from 53% to 40%) and verification (TAR@FAR improved from 51% to 74%). We further provide theoretical analysis and empirical validation explaining when and why the method achieves the most significant gains based on dataset characteristics.

</details>


### [127] [Effectiveness of Automatically Curated Dataset in Thyroid Nodules Classification Algorithms Using Deep Learning](https://arxiv.org/abs/2602.01020)
*Jichen Yang,Jikai Zhang,Benjamin Wildman-Tobriner,Maciej A. Mazurowski*

Main category: cs.CV

TL;DR: 本文研究了自动整理的甲状腺结节超声影像数据集对深度学习癌症诊断模型性能的提升作用。结果显示，自动整理的数据集能显著提高模型的AUC值。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在甲状腺结节良恶性分类中接近或达到医生水平，但高质量数据集难以获得，自动化数据整理方法亟需评估其实际价值。

Method: 比较基于人工标注数据集、全量自动整理数据集及高精度子集分别训练的深度学习模型的AUC表现，通过大样本实验和统计分析验证性能差异。

Result: 人工标注文集训练模型AUC为0.643，自动整理数据集AUC为0.694，显著高于人工组。使用高精度子集（AUC 0.689）与全量自动集（AUC 0.694）无显著差异。

Conclusion: 自动整理的数据集显著提升模型性能，所有自动整理数据优于只挑选高精度子集，建议结合全部自动数据用于深度学习模型训练。

Abstract: The diagnosis of thyroid nodule cancers commonly utilizes ultrasound images. Several studies showed that deep learning algorithms designed to classify benign and malignant thyroid nodules could match radiologists' performance. However, data availability for training deep learning models is often limited due to the significant effort required to curate such datasets. The previous study proposed a method to curate thyroid nodule datasets automatically. It was tested to have a 63% yield rate and 83% accuracy. However, the usefulness of the generated data for training deep learning models remains unknown. In this study, we conducted experiments to determine whether using a automatically-curated dataset improves deep learning algorithms' performance. We trained deep learning models on the manually annotated and automatically-curated datasets. We also trained with a smaller subset of the automatically-curated dataset that has higher accuracy to explore the optimum usage of such dataset. As a result, the deep learning model trained on the manually selected dataset has an AUC of 0.643 (95% confidence interval [CI]: 0.62, 0.66). It is significantly lower than the AUC of the 6automatically-curated dataset trained deep learning model, 0.694 (95% confidence interval [CI]: 0.67, 0.73, P < .001). The AUC of the accurate subset trained deep learning model is 0.689 (95% confidence interval [CI]: 0.66, 0.72, P > .43), which is insignificantly worse than the AUC of the full automatically-curated dataset. In conclusion, we showed that using a automatically-curated dataset can substantially increase the performance of deep learning algorithms, and it is suggested to use all the data rather than only using the accurate subset.

</details>


### [128] [GMAC: Global Multi-View Constraint for Automatic Multi-Camera Extrinsic Calibration](https://arxiv.org/abs/2602.01033)
*Chentian Sun*

Main category: cs.CV

TL;DR: GMAC是一种无需显式几何建模和人工标定的多相机外参自动估计算法，基于多视角重建网络隐式几何特征，在复杂动态环境下也能高效、稳定、准确地实现多相机标定。


<details>
  <summary>Details</summary>
Motivation: 现有多相机外参数标定方法依赖显式标定物、几何建模或专用神经网络，难以应用于复杂动态环境和在线场景，限制了实际部署。

Method: 提出GMAC框架，将外参作为全局变量嵌入到多视角重建网络的隐式几何表示中，不需全新网络设计，通过精简结构和轻量级回归头实现直接外参预测。同时联合优化重投影一致性和多视角循环一致性，以提升几何一致性和稳定性。

Result: 在合成和真实多相机数据集上验证，GMAC能够无须明确3D重建和手动标定，准确、稳定地完成多相机外参估计。

Conclusion: GMAC为多相机系统的高效部署和在线标定提供了新的解决方案，适用于实际复杂场景。

Abstract: Automatic calibration of multi-camera systems, namely the accurate estimation of spatial extrinsic parameters, is fundamental for 3D reconstruction, panoramic perception, and multi-view data fusion. Existing methods typically rely on calibration targets, explicit geometric modeling, or task-specific neural networks. Such approaches often exhibit limited robustness and applicability in complex dynamic environments or online scenarios, making them difficult to deploy in practical applications. To address this, this paper proposes GMAC, a multi-camera extrinsic estimation framework based on the implicit geometric representations learned by multi-view reconstruction networks. GMAC models extrinsics as global variables constrained by the latent multi-view geometric structure and prunes and structurally reconfigures existing networks so that their latent features can directly support extrinsic prediction through a lightweight regression head, without requiring a completely new network design. Furthermore, GMAC jointly optimizes cross-view reprojection consistency and multi-view cycle consistency, ensuring geometric coherence across cameras while improving prediction accuracy and optimization stability. Experiments on both synthetic and real-world multi-camera datasets demonstrate that GMAC achieves accurate and stable extrinsic estimation without explicit 3D reconstruction or manual calibration, providing a new solution for efficient deployment and online calibration of multi-camera systems.

</details>


### [129] [FUSE-Flow: Scalable Real-Time Multi-View Point Cloud Reconstruction Using Confidence](https://arxiv.org/abs/2602.01035)
*Chentian Sun*

Main category: cs.CV

TL;DR: 该论文提出了FUSE-Flow框架，实现了高效、可扩展、实时的多视角点云重建，在保持高质量重建的同时，支持多摄像头扩展，并显著降低了计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: 当前多相机及高分辨率深度传感器硬件普及，但如何在严格的实时要求下将大规模多视角深度数据融合为高质量点云，仍面临高计算复杂度、内存消耗大、可扩展性差等挑战，难以同时兼顾实时性能、重建质量以及多摄像头扩展。

Method: 提出FUSE-Flow，采用逐帧、无状态且线性可扩展的点云流式重建方式。每帧独立生成点云片段，通过测量置信度与3D距离一致性双权重进行噪声抑制与细节保持。针对大规模多摄像头，采用自适应空间哈希加权聚合，按点云密度划分空间并挑选代表点，进行加权融合。全流程GPU并行化，实现高吞吐、低延迟、线性复杂度。

Result: 实验表明，FUSE-Flow在重叠、深度不连续及动态场景下显著提升了重建稳定性与几何保真度，并能在现代GPU上持续保持实时帧率，显示出良好的有效性、鲁棒性与可扩展性。

Conclusion: FUSE-Flow高效解决了大规模多视角点云实时重建的难题，兼顾了实时性能、重建质量和多摄像头系统扩展性，对沉浸式计算、机器人导航、数字孪生等领域具有重要应用价值。

Abstract: Real-time multi-view point cloud reconstruction is a core problem in 3D vision and immersive perception, with wide applications in VR, AR, robotic navigation, digital twins, and computer interaction. Despite advances in multi-camera systems and high-resolution depth sensors, fusing large-scale multi-view depth observations into high-quality point clouds under strict real-time constraints remains challenging. Existing methods relying on voxel-based fusion, temporal accumulation, or global optimization suffer from high computational complexity, excessive memory usage, and limited scalability, failing to simultaneously achieve real-time performance, reconstruction quality, and multi-camera extensibility. We propose FUSE-Flow, a frame-wise, stateless, and linearly scalable point cloud streaming reconstruction framework. Each frame independently generates point cloud fragments, fused via two weights, measurement confidence and 3D distance consistency to suppress noise while preserving geometric details. For large-scale multi-camera efficiency, we introduce an adaptive spatial hashing-based weighted aggregation method: 3D space is adaptively partitioned by local point cloud density, representative points are selected per cell, and weighted fusion is performed to handle both sparse and dense regions. With GPU parallelization, FUSE-Flow achieves high-throughput, low-latency point cloud generation and fusion with linear complexity. Experiments demonstrate that the framework improves reconstruction stability and geometric fidelity in overlapping, depth-discontinuous, and dynamic scenes, while maintaining real-time frame rates on modern GPUs, verifying its effectiveness, robustness, and scalability.

</details>


### [130] [VEQ: Modality-Adaptive Quantization for MoE Vision-Language Models](https://arxiv.org/abs/2602.01037)
*Guangshuo Qin,Zhiteng Li,Zheng Chen,Weihang Zhang,Linghe Kong,Yulun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉专家量化方法（VEQ），针对视觉-语言MoE模型的量化问题，显著提升了压缩后模型在多模态任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言混合专家（MoE VLMs）虽然性能突出，但因内存和计算开销巨大，难以推广应用。现有量化方法对视觉与语言模态间差异及不同专家贡献度的异质性认识不足，导致结果不理想，因此亟需设计更适应这种异质性的量化方法。

Method: 提出了“视觉专家量化（VEQ）”，包括两部分：1）模态-专家感知量化，利用专家激活频率对不同专家赋予差异化误差优先级；2）模态亲和感知量化，结合token与专家的亲和度及模态信息，基于增强Hessian矩阵引导校准过程，实现更精准的量化。

Result: 在多个基准测试中，VEQ在不需额外训练、直接量化后，稳定优于当前最先进的SOTA方法。在W3A16量化配置下，Kimi-VL和Qwen3-VL模型平均准确率分别提升2.04%和3.09%。

Conclusion: VEQ方法有效缓解了MoE视觉-语言模型的内存和计算压力，并且量化后在多模态任务上展现更高的准确性和健壮性，对实际大模型部署有重要意义。

Abstract: Mixture-of-Experts(MoE) Vision-Language Models (VLMs) offer remarkable performance but incur prohibitive memory and computational costs, making compression essential. Post-Training Quantization (PTQ) is an effective training-free technique to address the massive memory and computation overhead. Existing quantization paradigms fall short as they are oblivious to two critical forms of heterogeneity: the inherent discrepancy between vision and language tokens, and the non-uniform contribution of different experts. To bridge this gap, we propose Visual Expert Quantization (VEQ), a dual-aware quantization framework designed to simultaneously accommodate cross-modal differences and heterogeneity between experts. Specifically, VEQ incorporates 1)Modality-expert-aware Quantization, which utilizes expert activation frequency to prioritize error minimization for pivotal experts, and 2)Modality-affinity-aware Quantization, which constructs an enhanced Hessian matrix by integrating token-expert affinity with modality information to guide the calibration process. Extensive experiments across diverse benchmarks verify that VEQ consistently outperforms state-of-the-art baselines. Specifically, under the W3A16 configuration, our method achieves significant average accuracy gains of 2.04\% on Kimi-VL and 3.09\% on Qwen3-VL compared to the previous SOTA quantization methods, demonstrating superior robustness across various multimodal tasks. Our code will be available at https://github.com/guangshuoqin/VEQ.

</details>


### [131] [From Videos to Conversations: Egocentric Instructions for Task Assistance](https://arxiv.org/abs/2602.01038)
*Lavisha Aggarwal,Vikas Bahirwani,Andrea Colaco*

Main category: cs.CV

TL;DR: 本文提出了一种自动将单人教学视频转化为双人多模态任务指导对话的框架，并据此构建了HowToDIV数据集，为多模态AR辅助AI提供了大规模真实任务对话数据。


<details>
  <summary>Details</summary>
Motivation: 当前增强现实（AR）等应用亟需AI任务指导助手，但现实的多模态任务对话数据极其稀缺，人工采集成本高、难度大，限制了领域进展。作者希望找到自动化、可扩展的生成方式以丰富相关数据资源。

Method: 作者设计了一个全自动流程，利用大语言模型将单人的教学视频自动转换成包含专家-新手多轮交流的双人对话，同时同步与视频内容对应。基于此流程，作者生成了HowToDIV数据集：包括507场对话、6,636个问答、总时长24小时的视频，覆盖多个实际任务领域。

Result: 构建了HowToDIV大规模多模态数据集，并用Gemma 3和Qwen 2.5模型进行了任务基线实验，验证了该数据集对多模态程序化任务助手研究的价值并提供了初步基准。

Conclusion: 这个自动化框架为收集真实、复杂任务对话数据提供了低成本、高扩展性的新路径，为后续多模态AI助手和AR辅助任务研究奠定了坚实基础。

Abstract: Many everyday tasks, ranging from appliance repair and cooking to car maintenance, require expert knowledge, particularly for complex, multi-step procedures. Despite growing interest in AI agents for augmented reality (AR) assistance, progress remains limited by the scarcity of large-scale multimodal conversational datasets grounded in real-world task execution, in part due to the cost and logistical complexity of human-assisted data collection. In this paper, we present a framework to automatically transform single person instructional videos into two-person multimodal task-guidance conversations. Our fully automatic pipeline, based on large language models, provides a scalable and cost efficient alternative to traditional data collection approaches. Using this framework, we introduce HowToDIV, a multimodal dataset comprising 507 conversations, 6,636 question answer pairs, and 24 hours of video spanning multiple domains. Each session consists of a multi-turn expert-novice interaction. Finally, we report baseline results using Gemma 3 and Qwen 2.5 on HowToDIV, providing an initial benchmark for multimodal procedural task assistance.

</details>


### [132] [ReLayout: Versatile and Structure-Preserving Design Layout Editing via Relation-Aware Design Reconstruction](https://arxiv.org/abs/2602.01046)
*Jiawei Lin,Shizhao Sun,Danqing Huang,Ting Liu,Ji Li,Jiang Bian*

Main category: cs.CV

TL;DR: 本文提出了一种无需三元组数据的自动设计布局编辑框架ReLayout，有效提升了编辑质量、准确性和布局结构保留能力。


<details>
  <summary>Details</summary>
Motivation: 自动化重新设计，无需人工调整，是提升设计工作流程的关键。现有设计布局编辑面临用户意图表达模糊和缺乏高质量训练数据等难题。论文旨在让编辑操作更自动、智能，实现符合用户意图又能保留原有结构的布局编辑。

Method: 1）标准化四种基础且重要的编辑操作，规范编辑流程。2）引入关系图来约束未编辑元素的布局结构，防止设计混乱。3）提出关系感知设计重建（RADR）方法，通过自监督方式，仅利用原始设计元素、关系图和合成的操作进行训练，无需真实三元组数据。4）采用多模态大模型作为骨干，统一实现多种编辑操作。

Result: 实验证明，该方法编辑质量、准确性和结构保留性均显著优于现有基线；用户调研也验证了其有效性。

Conclusion: ReLayout可以实现多种编辑操作且对结构有良好保留，在数据有限的情况下依旧表现出色，为智能布局编辑提供新思路。

Abstract: Automated redesign without manual adjustments marks a key step forward in the design workflow. In this work, we focus on a foundational redesign task termed design layout editing, which seeks to autonomously modify the geometric composition of a design based on user intents. To overcome the ambiguity of user needs expressed in natural language, we introduce four basic and important editing actions and standardize the format of editing operations. The underexplored task presents a unique challenge: satisfying specified editing operations while simultaneously preserving the layout structure of unedited elements. Besides, the scarcity of triplet (original design, editing operation, edited design) samples poses another formidable challenge. To this end, we present ReLayout, a novel framework for versatile and structure-preserving design layout editing that operates without triplet data. Specifically, ReLayout first introduces the relation graph, which contains the position and size relationships among unedited elements, as the constraint for layout structure preservation. Then, relation-aware design reconstruction (RADR) is proposed to bypass the data challenge. By learning to reconstruct a design from its elements, a relation graph, and a synthesized editing operation, RADR effectively emulates the editing process in a self-supervised manner. A multi-modal large language model serves as the backbone for RADR, unifying multiple editing actions within a single model and thus achieving versatile editing after fine-tuning. Qualitative, quantitative results and user studies show that ReLayout significantly outperforms the baseline models in terms of editing quality, accuracy, and layout structure preservation.

</details>


### [133] [Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance](https://arxiv.org/abs/2602.01047)
*Xinrong Chen,Xu Chu,Yingmin Qiu,Hengyuan Zhang,Jing Xiong,Shiyu Tang,Shuai Liu,Shaokang Yang,Cheng Yang,Hayden Kwok-Hay So,Ngai Wong*

Main category: cs.CV

TL;DR: 本文提出了Residual Decoding (ResDec)方法，无需额外训练即可有效减少大规模视觉-语言模型（LVLM）输出中的幻觉现象，提升视觉信息的真实性和模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: LVLM虽然在多模态任务中表现优异，但受语言先验影响，容易产生与视觉输入无关的幻觉内容。这种幻觉削弱了模型的可信度与实际应用价值，因此亟需一种有效的抑制方法。

Method: ResDec是一种创新且无需重新训练的方法。它在模型解码阶段利用历史信息，结合LVLM内部隐式推理及token logits随时间演化的机制，动态修正语言先验带来的偏差，从而减少幻觉发生。

Result: 大量实验结果表明，ResDec显著抑制了因语言先验导致的幻觉，提升了视觉锚定能力，减少了对象幻觉。同时，在多项LVLM基准测试中表现优异，证明了其广泛适用性。

Conclusion: ResDec为抑制LVLM幻觉问题提供了简单高效的新思路，无需重新训练即能显著提升模型多模态理解能力，具备实际推广应用价值。

Abstract: Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability.

</details>


### [134] [Baseline Method of the Foundation Model Challenge for Ultrasound Image Analysis](https://arxiv.org/abs/2602.01055)
*Bo Deng,Yitong Tang,Jiake Li,Yuxin Huang,Li Wang,Yu Zhang,Yufei Zhan,Hua Lu,Xiaoshen Zhang,Jieyun Bai*

Main category: cs.CV

TL;DR: 本论文针对超声影像分析的异质性问题，提出了统一的多任务学习基线模型，并在FM_UIA~2026大规模基准上验证，奠定了超声影像基础模型研究的重要基础。


<details>
  <summary>Details</summary>
Motivation: 超声影像分析面临解剖结构和采集协议的多样性，导致缺乏具有泛化能力的通用分析模型。目前多数方法任务单一，难以满足临床实际应用。因此，需要研究可同时适应多任务、多场景的基础模型。

Method: 本文构建了FM_UIA~2026大规模多任务基准，涵盖分割、分类、检测、回归等27个子任务，并设计了统一的多头多任务学习（MH-MTL）网络。模型采用EfficientNet-B4为主干特征提取网络，并结合特征金字塔结构（FPN），利用任务特定路由策略分配不同层次的语义与空间信息。训练过程采用复合损失、自适应学习率及余弦退火调度。

Result: 验证结果表明，所提统一多任务基线模型在多样化任务上均展示了较强的有效性和稳健性，适合作为后续研究和业界应用的基础。

Conclusion: 本研究为超声影像分析提出了统一、可扩展的强基线模型，对推动超声影像领域基础模型的研究和临床应用具有重要意义。代码和数据集已开源，便于社区复现和完善。

Abstract: Ultrasound (US) imaging exhibits substantial heterogeneity across anatomical structures and acquisition protocols, posing significant challenges to the development of generalizable analysis models. Most existing methods are task-specific, limiting their suitability as clinically deployable foundation models. To address this limitation, the Foundation Model Challenge for Ultrasound Image Analysis (FM\_UIA~2026) introduces a large-scale multi-task benchmark comprising 27 subtasks across segmentation, classification, detection, and regression. In this paper, we present the official baseline for FM\_UIA~2026 based on a unified Multi-Head Multi-Task Learning (MH-MTL) framework that supports all tasks within a single shared network. The model employs an ImageNet-pretrained EfficientNet--B4 backbone for robust feature extraction, combined with a Feature Pyramid Network (FPN) to capture multi-scale contextual information. A task-specific routing strategy enables global tasks to leverage high-level semantic features, while dense prediction tasks exploit spatially detailed FPN representations. Training incorporates a composite loss with task-adaptive learning rate scaling and a cosine annealing schedule. Validation results demonstrate the feasibility and robustness of this unified design, establishing a strong and extensible baseline for ultrasound foundation model research. The code and dataset are publicly available at \href{https://github.com/lijiake2408/Foundation-Model-Challenge-for-Ultrasound-Image-Analysis}{GitHub}.

</details>


### [135] [Radioactive 3D Gaussian Ray Tracing for Tomographic Reconstruction](https://arxiv.org/abs/2602.01057)
*Ling Chen,Bao Yang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于3D高斯射线追踪的断层重建新方法，相较于以往的基于投影的高斯模型，提升了物理一致性与投影精度，并可灵活处理非线性几何校正。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯的成像方法（如R2-Gaussian）通过仿射近似提升了可微分性，但会降低重建精度并难以纳入复杂的几何校正。

Method: 作者提出使用3D高斯射线追踪，精确解析地计算射线与高斯原语的线积分，且在正投影建模时为射线的起点与方向提供显式控制，从而克服了仿射近似的缺陷。

Result: 新方法更准确地模拟了物理成像过程，在实际CT等断层成像任务中提升了投影和重建的量化精度，使包括PET等在内的更多断层成像系统适用。

Conclusion: 基于3D高斯射线追踪的框架不仅改善了断层重建的物理一致性和精度，也便于集成复杂的非线性校正，有望拓展高斯模型在断层成像领域的实际应用范围。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged in computer vision as a promising rendering technique. By adapting the principles of Elliptical Weighted Average (EWA) splatting to a modern differentiable pipeline, 3DGS enables real-time, high-quality novel view synthesis. Building upon this, R2-Gaussian extended the 3DGS paradigm to tomographic reconstruction by rectifying integration bias, achieving state-of-the-art performance in computed tomography (CT). To enable differentiability, R2-Gaussian adopts a local affine approximation: each 3D Gaussian is locally mapped to a 2D Gaussian on the detector and composed via alpha blending to form projections. However, the affine approximation can degrade reconstruction quantitative accuracy and complicate the incorporation of nonlinear geometric corrections. To address these limitations, we propose a tomographic reconstruction framework based on 3D Gaussian ray tracing. Our approach provides two key advantages over splatting-based models: (i) it computes the line integral through 3D Gaussian primitives analytically, avoiding the local affine collapse and thus yielding a more physically consistent forward projection model; and (ii) the ray-tracing formulation gives explicit control over ray origins and directions, which facilitates the precise application of nonlinear geometric corrections, e.g., arc-correction used in positron emission tomography (PET). These properties extend the applicability of Gaussian-based reconstruction to a wider range of realistic tomography systems while improving projection accuracy.

</details>


### [136] [DRFormer: A Dual-Regularized Bidirectional Transformer for Person Re-identification](https://arxiv.org/abs/2602.01059)
*Ying Shu,Pujian Zhan,Huiqi Yang,Hehe Fan,Youfang Lin,Kai Lv*

Main category: cs.CV

TL;DR: 本文提出了一种将视觉基础模型和视觉-语言模型优点结合的新框架，用于解决行人再识别中的遮挡和姿势变化问题，实验结果优异。


<details>
  <summary>Details</summary>
Motivation: 现有行人再识别方法多只依赖单一类型的模型，未能充分结合能挖掘局部细节的视觉基础模型和能获取全局语义特征的视觉-语言模型两者的优势。

Method: 作者提出了一种双正则化双向Transformer（DRFormer）框架，通过双重正则化机制，有效结合视觉基础模型（如DINO）和视觉-语言模型（如CLIP）的特征提取能力，实现局部与全局特征的协同。

Result: 在五个公开数据集上进行了大量实验，所提方法在局部与全局特征融合方面优于现有方法，并取得了与最先进方法相竞争的表现。

Conclusion: DRFormer框架能够有效整合局部与全局特征，在行人再识别任务中取得了优异的实验效果，验证了两类模型协同的重要性。

Abstract: Both fine-grained discriminative details and global semantic features can contribute to solving person re-identification challenges, such as occlusion and pose variations. Vision foundation models (\textit{e.g.}, DINO) excel at mining local textures, and vision-language models (\textit{e.g.}, CLIP) capture strong global semantic difference. Existing methods predominantly rely on a single paradigm, neglecting the potential benefits of their integration. In this paper, we analyze the complementary roles of these two architectures and propose a framework to synergize their strengths by a \textbf{D}ual-\textbf{R}egularized Bidirectional \textbf{Transformer} (\textbf{DRFormer}). The dual-regularization mechanism ensures diverse feature extraction and achieves a better balance in the contributions of the two models. Extensive experiments on five benchmarks show that our method effectively harmonizes local and global representations, achieving competitive performance against state-of-the-art methods.

</details>


### [137] [PDE-Constrained Optimization for Neural Image Segmentation with Physics Priors](https://arxiv.org/abs/2602.01069)
*Seema K. Poudel,Sunny K. Khadka*

Main category: cs.CV

TL;DR: 本研究提出结合偏微分方程（PDE）约束的优化方法，用于提升显微图像分割的准确性与泛化能力，并通过实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 显微图像分割常因测量噪声、弱边界和标注样本少等原因成为一个不良定势逆问题。传统深度学习虽然灵活，但易产生成果不稳定、泛化能力差的问题。因此，作者希望通过引入物理启发的先验知识，实现更稳定、泛化更好的分割模型。

Method: 作者将图像分割表述为PDE约束下的优化问题，通过变分正则化将反应扩散方程和相场界面能等物理先验引入深度网络的损失函数，实现为可微残差项。采用LIVECell显微图像数据集进行训练测试，并以UNet为无约束基线进行对比。

Result: 在不同细胞类型间的训练与测试实验中，PDE正则化模型在分割精度和边界质量上均优于传统深度学习基线方法。尤其在样本量较少时，表现出更强的稳定性和泛化能力。

Conclusion: 将PDE约束和物理先验引入深度学习，不仅显著提升了显微图像分割的效果，同时为变分方法、统计学习和科学机器学习间的结合提供了新范式。

Abstract: Segmentation of microscopy images constitutes an ill-posed inverse problem due to measurement noise, weak object boundaries, and limited labeled data. Although deep neural networks provide flexible nonparametric estimators, unconstrained empirical risk minimization often leads to unstable solutions and poor generalization. In this work, image segmentation is formulated as a PDE-constrained optimization problem that integrates physically motivated priors into deep learning models through variational regularization. The proposed framework minimizes a composite objective function consisting of a data fidelity term and penalty terms derived from reaction-diffusion equations and phase-field interface energies, all implemented as differentiable residual losses. Experiments are conducted on the LIVECell dataset, a high-quality, manually annotated collection of phase-contrast microscopy images. Training is performed on two cell types, while evaluation is carried out on a distinct, unseen cell type to assess generalization. A UNet architecture is used as the unconstrained baseline model. Experimental results demonstrate consistent improvements in segmentation accuracy and boundary fidelity compared to unconstrained deep learning baselines. Moreover, the PDE-regularized models exhibit enhanced stability and improved generalization in low-sample regimes, highlighting the advantages of incorporating structured priors. The proposed approach illustrates how PDE-constrained optimization can strengthen data-driven learning frameworks, providing a principled bridge between variational methods, statistical learning, and scientific machine learning.

</details>


### [138] [PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.01077)
*Haopeng Li,Shitong Shao,Wenliang Zhong,Zikai Zhou,Lichen Bai,Hui Xiong,Zeke Xie*

Main category: cs.CV

TL;DR: PISA提出了一种新的非均匀稀疏注意力方案，通过对非关键块进行高效近似，提升了扩散变换器在图像和视频生成任务中的效率，同时保持了高质量。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力方法为提升速度，经常直接丢弃非关键信息，但这样在高稀疏度下会导致上下文信息损失，影响生成质量。该文发现非关键位置的注意力分数具有分布稳定性，可以高效近似而不是丢弃，这为改进稀疏注意力提供了依据。

Method: 提出了一种新的稀疏注意力机制PISA，不需额外训练。PISA对关键块精确计算，对非关键块用块级泰勒展开进行高效近似，从而全程覆盖关注范围，复杂度降低至亚二次，兼顾速度与质量。

Result: 在大型图像/视频生成模型Wan2.1-14B和Hunyuan-Video上，PISA实现了1.91倍和2.57倍的推理加速，并且在稀疏注意力方法中保持了最佳生成质量。在FLUX图像生成任务中也加速1.2倍且无视觉质量损失。

Conclusion: PISA通过创新性地近似处理非关键块注意力，实现了稀疏注意力计算的速度与质量兼得，为高效深度生成模型提供了新的设计范式。

Abstract: Diffusion Transformers are fundamental for video and image generation, but their efficiency is bottlenecked by the quadratic complexity of attention. While block sparse attention accelerates computation by attending only critical key-value blocks, it suffers from degradation at high sparsity by discarding context. In this work, we discover that attention scores of non-critical blocks exhibit distributional stability, allowing them to be approximated accurately and efficiently rather than discarded, which is essentially important for sparse attention design. Motivated by this key insight, we propose PISA, a training-free Piecewise Sparse Attention that covers the full attention span with sub-quadratic complexity. Unlike the conventional keep-or-drop paradigm that directly drop the non-critical block information, PISA introduces a novel exact-or-approximate strategy: it maintains exact computation for critical blocks while efficiently approximating the remainder through block-wise Taylor expansion. This design allows PISA to serve as a faithful proxy to full attention, effectively bridging the gap between speed and quality. Experimental results demonstrate that PISA achieves 1.91 times and 2.57 times speedups on Wan2.1-14B and Hunyuan-Video, respectively, while consistently maintaining the highest quality among sparse attention methods. Notably, even for image generation on FLUX, PISA achieves a 1.2 times acceleration without compromising visual quality. Code is available at: https://github.com/xie-lab-ml/piecewise-sparse-attention.

</details>


### [139] [MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization](https://arxiv.org/abs/2602.01081)
*Haitao Zhang,Yingying Wang,Jiaxiang Wang,Haote Xu,Hongyang Zhang,Yirong Chen,Yue Huang,Xinghao Ding*

Main category: cs.CV

TL;DR: 本文提出了MedAD-38K大规模多模态医学异常检测基准，并通过引入认知注入和一致性分组相对策略优化两阶段训练，提升大模型在医学图像理解与推理任务中的性能，模型MedAD-R1在该基准上刷新了SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有医学异常检测模型依赖于简单且碎片化的数据集进行监督微调，导致推理能力和多模态泛化能力不足，难以满足临床场景准确、透明推理的需求。

Method: 构建了MedAD-38K大规模多模态、多中心数据集，包含诊断思路链（Chain-of-Thought, CoT）标注和结构化VQA对。提出两阶段训练方法：第一阶段认知注入采用SFT学习医学基础知识并建立思考-作答范式；第二阶段引入新算法Con-GRPO，通过一致性奖励，保证推理过程与最终答案逻辑相关且连贯。

Result: MedAD-R1模型在MedAD-38K数据集上超越已有强基线模型超过10%，表现出色，特别在透明和一致性推理路径生成方面显著优于传统方法。

Conclusion: 本方法显著提升了医学多模态AI系统的解释性与临床可信度，为辅助医生决策提供了更可靠、可理解的自动化支持。

Abstract: Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support.

</details>


### [140] [Differential Vector Erasure: Unified Training-Free Concept Erasure for Flow Matching Models](https://arxiv.org/abs/2602.01089)
*Zhiqi Zhang,Xinhao Zhong,Yi Sun,Shuoyang Sun,Bin Chen,Shu-Tao Xia,Xuan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的无需训练的概念擦除方法DVE，可在flow matching扩散模型中精准去除不良或敏感概念，且优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像扩散模型虽强大，但易生成NSFW、不当风格或特定对象内容，影响安全可控部署。以往擦除方案多基于DDPM且需昂贵微调，难以适用于新兴的flow matching模型，因此亟需创新且高效的解决方法。

Method: 作者提出DVE（Differential Vector Erasure），该方法基于观察：流匹配生成模型中，语义概念蕴含于速度场的方向结构。DVE通过构建“目标概念-锚定概念”的微分向量场，推理时将速度场投影到特殊方向，从而仅擦除目标概念，实现精准干预，无需额外训练。

Result: 在FLUX数据集上的大量实验显示，DVE在NSFW内容抑制、艺术风格移除与物体消除等多项任务中，效果优于现有基线，同时保持了高图像质量与多样性。

Conclusion: DVE为flow matching模型提供了简单、高效且训练无关的概念擦除工具，有助于其安全与可控部署，可广泛应用于敏感或受限内容的精细消除。

Abstract: Text-to-image diffusion models have demonstrated remarkable capabilities in generating high-quality images, yet their tendency to reproduce undesirable concepts, such as NSFW content, copyrighted styles, or specific objects, poses growing concerns for safe and controllable deployment. While existing concept erasure approaches primarily focus on DDPM-based diffusion models and rely on costly fine-tuning, the recent emergence of flow matching models introduces a fundamentally different generative paradigm for which prior methods are not directly applicable. In this paper, we propose Differential Vector Erasure (DVE), a training-free concept erasure method specifically designed for flow matching models. Our key insight is that semantic concepts are implicitly encoded in the directional structure of the velocity field governing the generative flow. Leveraging this observation, we construct a differential vector field that characterizes the directional discrepancy between a target concept and a carefully chosen anchor concept. During inference, DVE selectively removes concept-specific components by projecting the velocity field onto the differential direction, enabling precise concept suppression without affecting irrelevant semantics. Extensive experiments on FLUX demonstrate that DVE consistently outperforms existing baselines on a wide range of concept erasure tasks, including NSFW suppression, artistic style removal, and object erasure, while preserving image quality and diversity.

</details>


### [141] [PandaPose: 3D Human Pose Lifting from a Single Image via Propagating 2D Pose Prior to 3D Anchor Space](https://arxiv.org/abs/2602.01095)
*Jinghong Zheng,Changlong Jiang,Yang Xiao,Jiaqi Li,Haohong Kuang,Hang Xu,Ran Wang,Zhiguo Cao,Min Du,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D人体姿态重建方法PandaPose，通过引入3D锚点空间作为中间表示，有效减少2D-3D建模中的误差扩散与自遮挡问题，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有从单张RGB图像恢复3D人体姿态的方法，往往直接将2D关节映射为3D关节，但易出现两个问题：一是输入的2D预测误差会直接影响3D预测结果；二是难以处理自遮挡等复杂情况。因此，需要一种更稳健的中间表示来减少这些局限。

Method: 作者提出了一种通过2D姿态先验传播到3D锚点空间的统一中间表达方法。其核心包括：(1) 在标准坐标系中为每个关节设置3D锚点，作为准确的先验；(2) 深度感知的关节特征提升，分层整合深度信息，解决自遮挡歧义；(3) 锚点-特征交互解码器，将3D锚点与特征相结合生成统一锚点查询，联合关节集、视觉和深度信息，最终经过融合预测3D关节位置。

Result: 在Human3.6M, MPI-INF-3DHP和3DPW三个主流数据集上的实验显示，该方法在Human3.6M的复杂条件下，相较于当前最优方法误差降低了14.7%，并有优秀的定性表现。

Conclusion: PandaPose通过引入3D锚点空间，实现了更鲁棒的3D人体姿态重建，在准确性和处理复杂场景（如自遮挡）方面均显著优于现有方法。

Abstract: 3D human pose lifting from a single RGB image is a challenging task in 3D vision. Existing methods typically establish a direct joint-to-joint mapping from 2D to 3D poses based on 2D features. This formulation suffers from two fundamental limitations: inevitable error propagation from input predicted 2D pose to 3D predictions and inherent difficulties in handling self-occlusion cases. In this paper, we propose PandaPose, a 3D human pose lifting approach via propagating 2D pose prior to 3D anchor space as the unified intermediate representation. Specifically, our 3D anchor space comprises: (1) Joint-wise 3D anchors in the canonical coordinate system, providing accurate and robust priors to mitigate 2D pose estimation inaccuracies. (2) Depth-aware joint-wise feature lifting that hierarchically integrates depth information to resolve self-occlusion ambiguities. (3) The anchor-feature interaction decoder that incorporates 3D anchors with lifted features to generate unified anchor queries encapsulating joint-wise 3D anchor set, visual cues and geometric depth information. The anchor queries are further employed to facilitate anchor-to-joint ensemble prediction. Experiments on three well-established benchmarks (i.e., Human3.6M, MPI-INF-3DHP and 3DPW) demonstrate the superiority of our proposition. The substantial reduction in error by $14.7\%$ compared to SOTA methods on the challenging conditions of Human3.6M and qualitative comparisons further showcase the effectiveness and robustness of our approach.

</details>


### [142] [Robust Harmful Meme Detection under Missing Modalities via Shared Representation Learning](https://arxiv.org/abs/2602.01101)
*Felix Breiteneder,Mohammad Belal,Muhammad Saad Saeed,Shahed Masoudian,Usman Naseem,Kulshrestha Juhi,Markus Schedl,Shah Nawaz*

Main category: cs.CV

TL;DR: 本文提出了一种在模态不完整（如文本缺失）场景下有更好表现的有害迷因检测新方法，通过独立投影各模态学习共享表征，提升了检测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有有害迷因检测方法往往依赖于完备的多模态（如文本和图像）信息，但实际应用中，如由于OCR质量差，文本往往会缺失，导致已有方法表现大幅下降。因此，亟需探究并提升模态不完整情况下的检测效果。

Method: 提出了一种新的基线方法，通过对多种模态（如图像、文本）进行独立投影，学习出可以在缺失模态时仍可用的共享表征。

Result: 在两个基准数据集上进行实验，结果表明：当文本信息缺失时，该方法优于现有方法，并且更好地整合了视觉特征，提高了在缺乏文本的场景下的鲁棒性。

Conclusion: 该工作弥补了以往研究只针对完备模态数据的空白，推动了有害迷因检测方法在现实环境、尤其是信息不完备时的落地应用。

Abstract: Internet memes are powerful tools for communication, capable of spreading political, psychological, and sociocultural ideas. However, they can be harmful and can be used to disseminate hate toward targeted individuals or groups. Although previous studies have focused on designing new detection methods, these often rely on modal-complete data, such as text and images. In real-world settings, however, modalities like text may be missing due to issues like poor OCR quality, making existing methods sensitive to missing information and leading to performance deterioration. To address this gap, in this paper, we present the first-of-its-kind work to comprehensively investigate the behavior of harmful meme detection methods in the presence of modal-incomplete data. Specifically, we propose a new baseline method that learns a shared representation for multiple modalities by projecting them independently. These shared representations can then be leveraged when data is modal-incomplete. Experimental results on two benchmark datasets demonstrate that our method outperforms existing approaches when text is missing. Moreover, these results suggest that our method allows for better integration of visual features, reducing dependence on text and improving robustness in scenarios where textual information is missing. Our work represents a significant step forward in enabling the real-world application of harmful meme detection, particularly in situations where a modality is absent.

</details>


### [143] [LightCity: An Urban Dataset for Outdoor Inverse Rendering and Reconstruction under Multi-illumination Conditions](https://arxiv.org/abs/2602.01118)
*Jingjing Wang,Qirui Hu,Chong Bao,Yuke Zhu,Hujun Bao,Zhaopeng Cui,Guofeng Zhang*

Main category: cs.CV

TL;DR: 该论文提出了LightCity，一个具有多样化照明条件和真实间接光、阴影效果的高质量城市合成数据集，并用其对城市环境下的基础任务进行了基准测试和分析。


<details>
  <summary>Details</summary>
Motivation: 城市场景下的逆向渲染对自动驾驶、数字孪生等应用至关重要，但由于复杂的照明环境（如多光源、间接光和阴影），导致固有分解和三维重建等任务面临难题，且缺乏合适的数据集来研究这些问题。

Method: 作者构建了LightCity数据集，包含300多种天空贴图、自主控制照明、覆盖街景和空中视角的5万多张高质量合成图像，并含深度、法线、材质、光照等丰富属性。同时，作者利用该数据集对三类基础任务进行了系统性基准测试。

Result: LightCity数据集能够展现复杂照明及间接光效下的城市环境，为逆向渲染等相关任务提供了优质素材。通过基准测试与分析，揭示了现有方法在复杂光照下的表现及其局限。

Conclusion: LightCity为研究城市场景下复杂照明处理提供了关键数据基础，有助于推动逆向渲染、固有分解、三维重建等领域在真实城市环境中的发展。

Abstract: Inverse rendering in urban scenes is pivotal for applications like autonomous driving and digital twins. Yet, it faces significant challenges due to complex illumination conditions, including multi-illumination and indirect light and shadow effects. However, the effects of these challenges on intrinsic decomposition and 3D reconstruction have not been explored due to the lack of appropriate datasets. In this paper, we present LightCity, a novel high-quality synthetic urban dataset featuring diverse illumination conditions with realistic indirect light and shadow effects. LightCity encompasses over 300 sky maps with highly controllable illumination, varying scales with street-level and aerial perspectives over 50K images, and rich properties such as depth, normal, material components, light and indirect light, etc. Besides, we leverage LightCity to benchmark three fundamental tasks in the urban environments and conduct a comprehensive analysis of these benchmarks, laying a robust foundation for advancing related research.

</details>


### [144] [Koo-Fu CLIP: Closed-Form Adaptation of Vision-Language Models via Fukunaga-Koontz Linear Discriminant Analysis](https://arxiv.org/abs/2602.01127)
*Matej Suchanek,Klara Janouskova,Ondrej Vasatko,Jiri Matas*

Main category: cs.CV

TL;DR: 本文提出了Koo-Fu CLIP，将监督线性判别分析方法应用于CLIP嵌入，以提升类别区分能力和降低特征维度，在ImageNet大规模基准测试中大幅提升分类准确率并支持显著压缩。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型（如CLIP）生成的嵌入未针对监督分类任务优化，导致类别区分不强且特征维度过高，影响高效大规模应用。

Method: 提出Koo-Fu CLIP，通过Fukunaga-Koontz线性判别分析作用于CLIP嵌入的白化空间，抑制类内差异、增强类间区分，并通过线性投影实现特征压缩和类别分离优化。

Result: 在ImageNet-1K等大规模基准测试中，Koo-Fu CLIP将top-1准确率由75.1%提升至79.1%；进一步扩展到14K和21K类时均有稳定增益，支持10-12倍的特征压缩且几乎无精度损失。

Conclusion: Koo-Fu CLIP为CLIP表征提供了高效、轻量级的有监督适配方法，显著提高了分类效率和准确率；具备大规模检索与分类的实用价值。

Abstract: Visual-language models such as CLIP provide powerful general-purpose representations, but their raw embeddings are not optimized for supervised classification, often exhibiting limited class separation and excessive dimensionality. We propose Koo-Fu CLIP, a supervised CLIP adaptation method based on Fukunaga-Koontz Linear Discriminant Analysis, which operates in a whitened embedding space to suppress within-class variation and enhance between-class discrimination. The resulting closed-form linear projection reshapes the geometry of CLIP embeddings, improving class separability while performing effective dimensionality reduction, and provides a lightweight and efficient adaptation of CLIP representations.
  Across large-scale ImageNet benchmarks, nearest visual prototype classification in the Koo-Fu CLIP space improves top-1 accuracy from 75.1% to 79.1% on ImageNet-1K, with consistent gains persisting as the label space expands to 14K and 21K classes. The method supports substantial compression by up to 10-12x with little or no loss in accuracy, enabling efficient large-scale classification and retrieval.

</details>


### [145] [Semantically Aware UAV Landing Site Assessment from Remote Sensing Imagery via Multimodal Large Language Models](https://arxiv.org/abs/2602.01163)
*Chunliang Hua,Zeyuan Yang,Lei Zhang,Jiayang Sun,Fengwen Chen,Chunlan Zeng,Xiao Hu*

Main category: cs.CV

TL;DR: 本文提出一种结合遥感影像和多模态大模型用于无人机紧急降落点全局风险评估的新方法，大幅提升了风险识别的准确性，并生成可解释的降落建议。


<details>
  <summary>Details</summary>
Motivation: 传统无人机降落点选择主要依赖几何信息，难以感知如人群、临时建筑等复杂语义风险，存在安全隐患。因此，需要引入更能理解场景语义的智能方法来保障无人机安全降落。

Method: 方法提出了粗到细的两阶段方案：首先，通过轻量级语义分割对遥感图像初步筛选候选区域；其次，通过视觉-语言推理代理将视觉特征与兴趣点（POI）数据融合，识别细微的语义风险。同时，作者还构建并公开了ELSS基准数据集来评估方法效果。

Result: 实验表明，所提框架在风险识别准确率上远超传统几何方法。同时，模型生成了可解释的、类人的降落建议，提升了自动决策的可信度。

Conclusion: 结合遥感影像与多模态大模型的方案能更全面地识别无人机降落风险，实现可解释自动决策，优于只基于几何的传统方案，并为后续研究提供了公开基准数据集。

Abstract: Safe UAV emergency landing requires more than just identifying flat terrain; it demands understanding complex semantic risks (e.g., crowds, temporary structures) invisible to traditional geometric sensors. In this paper, we propose a novel framework leveraging Remote Sensing (RS) imagery and Multimodal Large Language Models (MLLMs) for global context-aware landing site assessment. Unlike local geometric methods, our approach employs a coarse-to-fine pipeline: first, a lightweight semantic segmentation module efficiently pre-screens candidate areas; second, a vision-language reasoning agent fuses visual features with Point-of-Interest (POI) data to detect subtle hazards. To validate this approach, we construct and release the Emergency Landing Site Selection (ELSS) benchmark. Experiments demonstrate that our framework significantly outperforms geometric baselines in risk identification accuracy. Furthermore, qualitative results confirm its ability to generate human-like, interpretable justifications, enhancing trust in automated decision-making. The benchmark dataset is publicly accessible at https://anonymous.4open.science/r/ELSS-dataset-43D7.

</details>


### [146] [EEmo-Logic: A Unified Dataset and Multi-Stage Framework for Comprehensive Image-Evoked Emotion Assessment](https://arxiv.org/abs/2602.01173)
*Lancheng Gao,Ziheng Jia,Zixuan Xing,Wei Sun,Huiyu Duan,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 本文提出了EEmoDB数据集和EEmo-Logic多模态大模型，实现了对图片引发情感的多维度、细粒度理解，并在多个任务上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有模型在图片情感理解上存在粒度粗、推理能力弱的问题，难以满足机器同理心和复杂人机交互的需求。作者旨在推动计算机对图片情感的细致识别和推理能力。

Method: 1. 构建了EEmoDB数据集，包括125k图片的1.2M自动生成QA对（EEmoDB-QA），以及25k图片的3.6万细粒度评估（EEmoDB-Assess），涵盖5个分析维度和5类任务。
2. 提出EEmo-Logic多模态大语言模型，通过指令微调和群体相对偏好优化（GRPO）结合新型奖励函数进行训练，提升情感理解和推理能力。

Result: EEmo-Logic模型在EEmoDB及跨领域数据集上的情感问答与细粒度评估任务均取得领先表现，表现出强稳健性和泛化能力。

Conclusion: EEmoDB数据集和EEmo-Logic模型大幅提升了图片情感理解的粒度和推理能力，为机器同理心和高级人机交互应用奠定了基础。

Abstract: Understanding the multi-dimensional attributes and intensity nuances of image-evoked emotions is pivotal for advancing machine empathy and empowering diverse human-computer interaction applications. However, existing models are still limited to coarse-grained emotion perception or deficient reasoning capabilities. To bridge this gap, we introduce EEmoDB, the largest image-evoked emotion understanding dataset to date. It features $5$ analysis dimensions spanning $5$ distinct task categories, facilitating comprehensive interpretation. Specifically, we compile $1.2M$ question-answering (QA) pairs (EEmoDB-QA) from $125k$ images via automated generation, alongside a $36k$ dataset (EEmoDB-Assess) curated from $25k$ images for fine-grained assessment. Furthermore, we propose EEmo-Logic, an all-in-one multimodal large language model (MLLM) developed via instruction fine-tuning and task-customized group relative preference optimization (GRPO) with novel reward design. Extensive experiments demonstrate that EEmo-Logic achieves robust performance in in-domain and cross-domain datasets, excelling in emotion QA and fine-grained assessment. The code is available at https://anonymous.4open.science/r/EEmoLogic.

</details>


### [147] [Refining Context-Entangled Content Segmentation via Curriculum Selection and Anti-Curriculum Promotion](https://arxiv.org/abs/2602.01183)
*Chunming He,Rihan Zhang,Fengyang Xiao,Dingming Zhang,Zhiwen Cao,Sina Farsiu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为CurriSeg的学习框架，通过结合课程学习和反课程原则，提升了对上下文缠绕内容分割任务（如伪装物体检测）的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 生物学习遵循从易到难的规律，有助于提高认知和鲁棒性。现有分割方法在应对目标与背景视觉特征相似（上下文缠绕）的任务时，主要依赖网络结构改进，而忽略了学习策略对鲁棒性的贡献。为了更好地处理这类挑战性任务，作者受课程学习启发，提出改进的训练流程。

Method: CurriSeg包含两个阶段：第一阶段为课程筛选，通过分析样本损失的时间统计，动态挑选难而有信息的训练样本，排除噪声样本，实现稳健能力提升；第二阶段为反课程促进，通过频谱盲微调，抑制高频信息，强化对低频结构和上下文的依赖，从而提升泛化能力。该框架不增加模型参数，也不增加训练时间。

Result: 在多个上下文缠绕内容分割基准上，CurriSeg持续优于现有方法，展现出更强的鲁棒性与泛化性。

Conclusion: CurriSeg有效利用课程与反课程的进阶与挑战关系，在不增加复杂度的前提下，提升了上下文感知分割的性能，为复杂分布下的鲁棒性学习提供了新思路。

Abstract: Biological learning proceeds from easy to difficult tasks, gradually reinforcing perception and robustness. Inspired by this principle, we address Context-Entangled Content Segmentation (CECS), a challenging setting where objects share intrinsic visual patterns with their surroundings, as in camouflaged object detection. Conventional segmentation networks predominantly rely on architectural enhancements but often ignore the learning dynamics that govern robustness under entangled data distributions. We introduce CurriSeg, a dual-phase learning framework that unifies curriculum and anti-curriculum principles to improve representation reliability. In the Curriculum Selection phase, CurriSeg dynamically selects training data based on the temporal statistics of sample losses, distinguishing hard-but-informative samples from noisy or ambiguous ones, thus enabling stable capability enhancement. In the Anti-Curriculum Promotion phase, we design Spectral-Blindness Fine-Tuning, which suppresses high-frequency components to enforce dependence on low-frequency structural and contextual cues and thus strengthens generalization. Extensive experiments demonstrate that CurriSeg achieves consistent improvements across diverse CECS benchmarks without adding parameters or increasing total training time, offering a principled view of how progression and challenge interplay to foster robust and context-aware segmentation. Code will be released.

</details>


### [148] [EMFormer: Efficient Multi-Scale Transformer for Accumulative Context Weather Forecasting](https://arxiv.org/abs/2602.01194)
*Hao Chen,Tao Han,Jie Zhang,Song Guo,Fenghua Ling,Lei Bai*

Main category: cs.CV

TL;DR: 本文提出了一种新的高效多尺度变换器（EMFormer）及完整预测流程，以提升长期天气预报的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的长期天气预测方法主要依赖微调技术延长预测时长，但会遇到灾难性遗忘、误差积累和高训练开销等问题，限制了实际应用价值。作者旨在解决这些挑战，提高长时段预测的精度和模型泛化能力。

Method: 提出EMFormer架构，通过一次卷积提取多尺度特征，提升训练和推理效率。引入累积上下文微调机制改善时间一致性，避免损失短时预测能力。同时，设计了基于正弦权重的复合损失函数，动态调节预训练和微调过程中的各项损失。

Result: 本方法在天气和极端事件预测任务上显著提升了长期预测精度。在图像识别等视觉基准数据集上也表现出很好的泛化能力，并且实现了比传统多尺度模块快5.69倍的推理速度。

Conclusion: EMFormer架构及其端到端流程可兼顾长期预测准确度和高效率，能有效应对现有方法的弊端，对天气相关领域和更广泛的视觉任务都具备潜在应用价值。

Abstract: Long-term weather forecasting is critical for socioeconomic planning and disaster preparedness. While recent approaches employ finetuning to extend prediction horizons, they remain constrained by the issues of catastrophic forgetting, error accumulation, and high training overhead. To address these limitations, we present a novel pipeline across pretraining, finetuning and forecasting to enhance long-context modeling while reducing computational overhead. First, we introduce an Efficient Multi-scale Transformer (EMFormer) to extract multi-scale features through a single convolution in both training and inference. Based on the new architecture, we further employ an accumulative context finetuning to improve temporal consistency without degrading short-term accuracy. Additionally, we propose a composite loss that dynamically balances different terms via a sinusoidal weighting, thereby adaptively guiding the optimization trajectory throughout pretraining and finetuning. Experiments show that our approach achieves strong performance in weather forecasting and extreme event prediction, substantially improving long-term forecast accuracy. Moreover, EMFormer demonstrates strong generalization on vision benchmarks (ImageNet-1K and ADE20K) while delivering a 5.69x speedup over conventional multi-scale modules.

</details>


### [149] [Med3D-R1: Incentivizing Clinical Reasoning in 3D Medical Vision-Language Models for Abnormality Diagnosis](https://arxiv.org/abs/2602.01200)
*Haoran Lai,Zihang Jiang,Kun Zhang,Qingsong Yao,Rongsheng Wang,Zhiyang He,Xiaodong Tao,Wei Wei,Shaohua Kevin Zhou*

Main category: cs.CV

TL;DR: 本文提出了Med3D-R1框架，通过引入新的训练机制和奖励设计，有效提升了3D医学图像-语言模型的临床推理能力和诊断准确性，并在主流基准上取得领先。


<details>
  <summary>Details</summary>
Motivation: 目前3D视觉-语言模型在医学场景下面临体积数据复杂、过拟合文本表面模式及缺乏可解释性奖励等难题，限制了其在临床推理和诊断中的实用性和信任度。

Method: 作者提出Med3D-R1，采用了两阶段训练：一是监督微调（加入残差对齐解决3D特征和文本嵌入的差异，异常加权减少结构性偏差），二是强化学习（重新设计奖励以促进分步、连贯的诊断推理过程）。

Result: 在CT-RATE与RAD-ChestCT两个3D诊断基准数据集的多项选择视觉问答任务上，模型分别取得41.92%和44.99%的准确率，均为当前最优。

Conclusion: Med3D-R1有效提升了模型的异常识别与临床推理能力，有望为实际医学诊断流程带来更可靠和可解释的3D视觉-语言系统。

Abstract: Developing 3D vision-language models with robust clinical reasoning remains a challenge due to the inherent complexity of volumetric medical imaging, the tendency of models to overfit superficial report patterns, and the lack of interpretability-aware reward designs. In this paper, we propose Med3D-R1, a reinforcement learning framework with a two-stage training process: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). During SFT stage, we introduce a residual alignment mechanism to bridge the gap between high-dimensional 3D features and textual embeddings, and an abnormality re-weighting strategy to emphasize clinically informative tokens and reduce structural bias in reports. In RL stage, we redesign the consistency reward to explicitly promote coherent, step-by-step diagnostic reasoning. We evaluate our method on medical multiple-choice visual question answering using two 3D diagnostic benchmarks, CT-RATE and RAD-ChestCT, where our model attains state-of-the-art accuracies of 41.92\% on CT-RATE and 44.99\% on RAD-ChestCT. These results indicate improved abnormality diagnosis and clinical reasoning and outperform prior methods on both benchmarks. Overall, our approach holds promise for enhancing real-world diagnostic workflows by enabling more reliable and transparent 3D medical vision-language systems.

</details>


### [150] [Boosting Point-supervised Temporal Action Localization via Text Refinement and Alignment](https://arxiv.org/abs/2602.01257)
*Yunchuan Ma,Laiyun Qing,Guorong Li,Yuqing Liu,Yuankai Qi,Qingming Huang*

Main category: cs.CV

TL;DR: 本文提出了一个结合文本与视觉信息的新框架，用于提升点监督时序动作定位的准确性与实用性。


<details>
  <summary>Details</summary>
Motivation: 现有点监督时序动作定位方法关注视觉特征，忽视了语义丰富的文本信息。作者希望通过引入文本语义，提升定位精度。

Method: 提出文本细化与对齐（TRA）框架，包括点级文本细化模块（PTR）和点级多模态对齐模块（PMA）。首先用预训练多模态模型生成视频帧描述，PTR结合点标注与多模型细化描述，PMA将特征投影至统一语义空间，通过点级多模态对比学习优化视觉和文本模态。最终，增强特征用于动作检测器定位。

Result: 在五个主流数据集上的广泛实验显示，所提方法在性能上优于多种先进方法。框架可在单张24GB 3090显卡上运行，具有良好的实用性和可扩展性。

Conclusion: 结合文本和视觉特征显著提升了点监督动作定位的精度和实用性，为未来多模态动作理解任务奠定基础。

Abstract: Recently, point-supervised temporal action localization has gained significant attention for its effective balance between labeling costs and localization accuracy. However, current methods only consider features from visual inputs, neglecting helpful semantic information from the text side. To address this issue, we propose a Text Refinement and Alignment (TRA) framework that effectively utilizes textual features from visual descriptions to complement the visual features as they are semantically rich. This is achieved by designing two new modules for the original point-supervised framework: a Point-based Text Refinement module (PTR) and a Point-based Multimodal Alignment module (PMA). Specifically, we first generate descriptions for video frames using a pre-trained multimodal model. Next, PTR refines the initial descriptions by leveraging point annotations together with multiple pre-trained models. PMA then projects all features into a unified semantic space and leverages a point-level multimodal feature contrastive learning to reduce the gap between visual and linguistic modalities. Last, the enhanced multi-modal features are fed into the action detector for precise localization. Extensive experimental results on five widely used benchmarks demonstrate the favorable performance of our proposed framework compared to several state-of-the-art methods. Moreover, our computational overhead analysis shows that the framework can run on a single 24 GB RTX 3090 GPU, indicating its practicality and scalability.

</details>


### [151] [Q-DiT4SR: Exploration of Detail-Preserving Diffusion Transformer Quantization for Real-World Image Super-Resolution](https://arxiv.org/abs/2602.01273)
*Xun Zhang,Kaicheng Yang,Hongliang Lu,Haotong Qin,Yong Guo,Yulun Zhang*

Main category: cs.CV

TL;DR: 该论文提出了Q-DiT4SR，这是首个专为Diffusion Transformer(DiT)架构的真实世界图像超分辨率（Real-ISR）设计的后训练量化（PTQ）框架，以实现高效推理并保持高质量图像纹理。


<details>
  <summary>Details</summary>
Motivation: 目前DiT在Real-ISR任务中表现优异，但其推理耗时严重，限制了实际应用。虽然PTQ方法可以加速模型，但现有PTQ技术多针对U-Net或文本到图像的DiT模型，无法直接应用于DiT超分辨率模型，否则会导致纹理损失。因此，亟需一个专针对DiT-Real-ISR模型的高效量化方案。

Method: （1）提出H-SVD，将全局低秩分支与局部分块秩为1的分支结合，在参数预算受控下实现高效分解；（2）提出VaSMP（方差感知的空间-时间混合精度），基于率失真理论自动分配权重量化比特宽度，并通过动态规划实现激活精度随扩散步调动态调整，无需依赖数据或过多校准。

Result: 在多个真实世界数据集上，Q-DiT4SR在W4A6和W4A4两种量化精度下均实现了目前最优性能。尤其是在W4A4设置下，模型体积减少5.8倍，计算量减少60倍。

Conclusion: Q-DiT4SR为DiT在Real-ISR场景下的高效部署提供了有效解决方案，有助于推动高性能SR模型的实际应用。相关代码和模型已开放。

Abstract: Recently, Diffusion Transformers (DiTs) have emerged in Real-World Image Super-Resolution (Real-ISR) to generate high-quality textures, yet their heavy inference burden hinders real-world deployment. While Post-Training Quantization (PTQ) is a promising solution for acceleration, existing methods in super-resolution mostly focus on U-Net architectures, whereas generic DiT quantization is typically designed for text-to-image tasks. Directly applying these methods to DiT-based super-resolution models leads to severe degradation of local textures. Therefore, we propose Q-DiT4SR, the first PTQ framework specifically tailored for DiT-based Real-ISR. We propose H-SVD, a hierarchical SVD that integrates a global low-rank branch with a local block-wise rank-1 branch under a matched parameter budget. We further propose Variance-aware Spatio-Temporal Mixed Precision: VaSMP allocates cross-layer weight bit-widths in a data-free manner based on rate-distortion theory, while VaTMP schedules intra-layer activation precision across diffusion timesteps via dynamic programming (DP) with minimal calibration. Experiments on multiple real-world datasets demonstrate that our Q-DiT4SR achieves SOTA performance under both W4A6 and W4A4 settings. Notably, the W4A4 quantization configuration reduces model size by 5.8$\times$ and computational operations by over 60$\times$. Our code and models will be available at https://github.com/xunzhang1128/Q-DiT4SR.

</details>


### [152] [TF-Lane: Traffic Flow Module for Robust Lane Perception](https://arxiv.org/abs/2602.01277)
*Yihan Xie,Han Xia,Zhen Yang*

Main category: cs.CV

TL;DR: 本文提出了一种融合交通流信息的车道感知模块（TFM），在多种开源算法和数据集上验证了其能有效提升车道检测性能，尤其在传统视觉方法表现不佳的场景下。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的车道检测方法在视觉线索不足（如遮挡、车道线缺失）时，性能下降明显；而利用高精地图虽能提升表现，但存在成本高、实时性差等问题。因此，迫切需要无需额外高成本、且实时性强的新型辅助信息源。

Method: 提出TFM模块，能够实时提取交通流特征，并与现有车道感知算法无缝融合。作者通过在现实自动驾驶场景中得出解决思路，采用主流开源算法和数据集进行实验验证。

Result: 在Nuscenes、OpenLaneV2等两个公开数据集，以及四种主流模型上进行了大量实验证明，TFM模块可带来持续稳定的性能提升。在Nuscenes数据集上mAP最高提升4.1%。

Conclusion: 融合交通流信息的TFM模块无需额外高成本投入，性能提升显著，可广泛应用于现有车道感知系统，为自动驾驶场景下车道检测能力提供更鲁棒的解决方案。

Abstract: Autonomous driving systems require robust lane perception capabilities, yet existing vision-based detection methods suffer significant performance degradation when visual sensors provide insufficient cues, such as in occluded or lane-missing scenarios. While some approaches incorporate high-definition maps as supplementary information, these solutions face challenges of high subscription costs and limited real-time performance. To address these limitations, we explore an innovative information source: traffic flow, which offers real-time capabilities without additional costs. This paper proposes a TrafficFlow-aware Lane perception Module (TFM) that effectively extracts real-time traffic flow features and seamlessly integrates them with existing lane perception algorithms. This solution originated from real-world autonomous driving conditions and was subsequently validated on open-source algorithms and datasets. Extensive experiments on four mainstream models and two public datasets (Nuscenes and OpenLaneV2) using standard evaluation metrics show that TFM consistently improves performance, achieving up to +4.1% mAP gain on the Nuscenes dataset.

</details>


### [153] [DSFC-Net: A Dual-Encoder Spatial and Frequency Co-Awareness Network for Rural Road Extraction](https://arxiv.org/abs/2602.01278)
*Zhengbo Zhang,Yihe Tian,Wanke Xia,Lin Chen,Yue Sun,Kun Ding,Ying Wang,Bing Xu,Shiming Xiang*

Main category: cs.CV

TL;DR: 提出了一种针对高分辨率遥感影像中农村道路提取挑战的新方法DSFC-Net，通过融合空间和频域信息提升提取准确性，并在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有针对城市道路的遥感提取方法难以应对农村道路的高类内变异、低类间区分、植被遮挡和道路狭窄等问题，导致农村地区道路提取准确率低。为了填补这一差距，提高农村基础设施规划的遥感支持，需要开发更适合农村复杂环境的道路提取方法。

Method: 构建了DSFC-Net框架，包含双分支编码器：CNN分支负责捕捉局部细粒度道路边界和短距离连续性；空间-频率混合Transformer (SFT)分支通过创新的交互式频率注意力模块（CFIA）结合Laplacian金字塔策略解耦高低频信息，提升全局语义建模和对植被遮挡的鲁棒性；最后通过通道特征融合模块（CFFM）自适应融合双分支信息，实现高精度分割。

Result: 在WHU-RuR+、DeepGlobe以及Massachusetts等权威遥感道路数据集上，DSFC-Net显著超越了现有主流方法，各项性能指标均领先。

Conclusion: DSFC-Net有效解决了农村道路遥感提取的关键难题，通过空间和频域的协同建模以及分支信息的融合，大幅提升了农村道路的提取精度，对基础设施建设和可持续发展具有重要意义。

Abstract: Accurate extraction of rural roads from high-resolution remote sensing imagery is essential for infrastructure planning and sustainable development. However, this task presents unique challenges in rural settings due to several factors. These include high intra-class variability and low inter-class separability from diverse surface materials, frequent vegetation occlusions that disrupt spatial continuity, and narrow road widths that exacerbate detection difficulties. Existing methods, primarily optimized for structured urban environments, often underperform in these scenarios as they overlook such distinctive characteristics. To address these challenges, we propose DSFC-Net, a dual-encoder framework that synergistically fuses spatial and frequency-domain information. Specifically, a CNN branch is employed to capture fine-grained local road boundaries and short-range continuity, while a novel Spatial-Frequency Hybrid Transformer (SFT) is introduced to robustly model global topological dependencies against vegetation occlusions. Distinct from standard attention mechanisms that suffer from frequency bias, the SFT incorporates a Cross-Frequency Interaction Attention (CFIA) module that explicitly decouples high- and low-frequency information via a Laplacian Pyramid strategy. This design enables the dynamic interaction between spatial details and frequency-aware global contexts, effectively preserving the connectivity of narrow roads. Furthermore, a Channel Feature Fusion Module (CFFM) is proposed to bridge the two branches by adaptively recalibrating channel-wise feature responses, seamlessly integrating local textures with global semantics for accurate segmentation. Comprehensive experiments on the WHU-RuR+, DeepGlobe, and Massachusetts datasets validate the superiority of DSFC-Net over state-of-the-art approaches.

</details>


### [154] [Who Transfers Safety? Identifying and Targeting Cross-Lingual Shared Safety Neurons](https://arxiv.org/abs/2602.01283)
*Xianhui Zhang,Chengyu Xie,Linxia Zhu,Yonghui Yang,Weixiang Zhao,Zifeng Cheng,Cong Wang,Fei Shen,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 本文揭示了大语言模型（LLMs）中存在一组跨语言共享的安全神经元（SS-Neurons），并提出针对这些神经元的训练方法，显著提升了非高资源语言的安全性，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前多语种LLM安全性表现出显著不均衡，非高资源语言的安全防护较弱。而神经层面跨语种安全一致性的机制尚不清楚。本文旨在通过神经元层面的分析和干预，提高非高资源语言的安全性。

Method: 首先，作者识别并验证了单语安全神经元（MS-Neurons）在拒绝不安全请求中的因果作用。进一步，识别出在高资源与非高资源语言共享的安全神经元（SS-Neurons），并通过抑制或增强这些神经元，观察其对多语种安全防护的一致性影响。最后，作者提出有针对性地训练这一极小子集神经元的方法。

Result: 实验证明，专注于SS-Neurons的训练方法可以显著增强非高资源语言的安全能力，并且整体性能优于最先进的多语言安全性提升方法，同时不损害模型的通用能力。

Conclusion: 跨语言共享安全神经元对提升LLM在多语种安全性方面发挥关键作用。聚焦于这些神经元的训练策略为现实应用中提升低资源语言安全防护能力提供了一条高效可行的路径。

Abstract: Multilingual safety remains significantly imbalanced, leaving non-high-resource (NHR) languages vulnerable compared to robust high-resource (HR) ones. Moreover, the neural mechanisms driving safety alignment remain unclear despite observed cross-lingual representation transfer.
  In this paper, we find that LLMs contain a set of cross-lingual shared safety neurons (SS-Neurons), a remarkably small yet critical neuronal subset that jointly regulates safety behavior across languages.
  We first identify monolingual safety neurons (MS-Neurons) and validate their causal role in safety refusal behavior through targeted activation and suppression.
  Our cross-lingual analyses then identify SS-Neurons as the subset of MS-Neurons shared between HR and NHR languages, serving as a bridge to transfer safety capabilities from HR to NHR domains.
  We observe that suppressing these neurons causes concurrent safety drops across NHR languages, whereas reinforcing them improves cross-lingual defensive consistency.
  Building on these insights, we propose a simple neuron-oriented training strategy that targets SS-Neurons based on language resource distribution and model architecture. Experiments demonstrate that fine-tuning this tiny neuronal subset outperforms state-of-the-art methods, significantly enhancing NHR safety while maintaining the model's general capabilities.
  The code and dataset will be available athttps://github.com/1518630367/SS-Neuron-Expansion.

</details>


### [155] [Interacted Planes Reveal 3D Line Mapping](https://arxiv.org/abs/2602.01296)
*Zeran Ke,Bin Tan,Gui-Song Xia,Yujun Shen,Nan Xue*

Main category: cs.CV

TL;DR: LiP-Map是一种结合线和面优化的3D线条重建方法，能提升3D线条映射的准确性和完整性，并在基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D线条映射方法未能充分利用物理结构中的面和线的关系，影响了重建的结构性和准确性。该文希望通过更合理地建模线与面的耦合，提升3D重建表现。

Method: 提出LiP-Map框架，将可学习的线和面作为基本元素，通过显式建模面-线之间的物理和拓扑关系进行联合优化，无需依赖简单的共面约束。该方法能高效地从多视角RGB图像中进行重建。

Result: 在ScanNetV2、ScanNet++、Hypersim、7Scenes与Tanks&Temple等100多个场景测试中，LiP-Map在准确性和完整性上均超越SOTA方法，并提升了基于线的视觉定位表现。

Conclusion: LiP-Map首次将面拓扑显式整合进3D线条映射，为结构化3D重建开辟了新途径，尤其适合人工环境场景，具有较高效率和实际应用价值。

Abstract: 3D line mapping from multi-view RGB images provides a compact and structured visual representation of scenes. We study the problem from a physical and topological perspective: a 3D line most naturally emerges as the edge of a finite 3D planar patch. We present LiP-Map, a line-plane joint optimization framework that explicitly models learnable line and planar primitives. This coupling enables accurate and detailed 3D line mapping while maintaining strong efficiency (typically completing a reconstruction in 3 to 5 minutes per scene). LiP-Map pioneers the integration of planar topology into 3D line mapping, not by imposing pairwise coplanarity constraints but by explicitly constructing interactions between plane and line primitives, thus offering a principled route toward structured reconstruction in man-made environments. On more than 100 scenes from ScanNetV2, ScanNet++, Hypersim, 7Scenes, and Tanks\&Temple, LiP-Map improves both accuracy and completeness over state-of-the-art methods. Beyond line mapping quality, LiP-Map significantly advances line-assisted visual localization, establishing strong performance on 7Scenes. Our code is released at https://github.com/calmke/LiPMAP for reproducible research.

</details>


### [156] [Interaction-Consistent Object Removal via MLLM-Based Reasoning](https://arxiv.org/abs/2602.01298)
*Ching-Kai Huang,Wen-Chieh Lin,Yan-Cen Lee*

Main category: cs.CV

TL;DR: 该论文提出了一个新的问题——互动一致性目标移除（ICOR），并提出了REORM框架，能更智能地移除与目标有关的所有互动元素，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统图像目标移除方法只去除了显性目标，却遗留了目标与周围环境互动的线索，导致结果语义上不一致。这一问题在视觉编辑、内容保护等应用中影响较大。本文旨在更好地解决目标移除后残留互动痕迹的问题。

Method: 提出了REORM（基于推理增强的目标移除框架），利用多模态大模型（MLLM）推理出需要共同移除的互动元素。REORM采用模块化设计，包括MLLM分析、掩码引导移除、以及自纠正机制，并有资源受限下的本地部署版本。

Result: 为评估方法有效性，构建了包含丰富互动依赖的ICOREval基准。实验证明，REORM在ICOREval上明显优于现有最先进的图像编辑系统，能实现互动一致性的移除结果。

Conclusion: REORM框架有效实现了互动一致性目标移除，在准确性和语义一致性上优于现有技术，对高级图像编辑任务具有重要意义。

Abstract: Image-based object removal often erases only the named target, leaving behind interaction evidence that renders the result semantically inconsistent. We formalize this problem as Interaction-Consistent Object Removal (ICOR), which requires removing not only the target object but also associated interaction elements, such as lighting-dependent effects, physically connected objects, targetproduced elements, and contextually linked objects. To address this task, we propose Reasoning-Enhanced Object Removal with MLLM (REORM), a reasoningenhanced object removal framework that leverages multimodal large language models to infer which elements must be jointly removed. REORM features a modular design that integrates MLLM-driven analysis, mask-guided removal, and a self-correction mechanism, along with a local-deployment variant that supports accurate editing under limited resources. To support evaluation, we introduce ICOREval, a benchmark consisting of instruction-driven removals with rich interaction dependencies. On ICOREval, REORM outperforms state-of-the-art image editing systems, demonstrating its effectiveness in producing interactionconsistent results.

</details>


### [157] [ReDiStory: Region-Disentangled Diffusion for Consistent Visual Story Generation](https://arxiv.org/abs/2602.01303)
*Ayushman Sarkar,Zhenyu Yu,Chu Chen,Wei Tang,Kangning Cui,Mohd Yamani Idna Idris*

Main category: cs.CV

TL;DR: 提出了一种无需训练的新方法ReDiStory，有效提升了多帧视觉故事生成中的角色一致性。


<details>
  <summary>Details</summary>
Motivation: 现有多帧故事生成方法将身份和场景信息合并，容易导致场景间干扰，特别是在复杂故事中难以保持角色一致。

Method: ReDiStory在推理时将文本嵌入分解为身份相关和帧特定两部分，并通过去相关化抑制帧间共享方向，从而减少跨帧干扰，无需调整扩散模型参数或额外监督。

Result: 在同样的扩散模型和推理条件下，ReDiStory实现了更好的角色一致性和提示忠实度。多项角色一致性指标上明显优于1Prompt1Story方法。

Conclusion: ReDiStory能提升多帧视觉故事生成的身份一致性且简单易用，为相关应用提供了有效解决方案。

Abstract: Generating coherent visual stories requires maintaining subject identity across multiple images while preserving frame-specific semantics. Recent training-free methods concatenate identity and frame prompts into a unified representation, but this often introduces inter-frame semantic interference that weakens identity preservation in complex stories. We propose ReDiStory, a training-free framework that improves multi-frame story generation via inference-time prompt embedding reorganization. ReDiStory explicitly decomposes text embeddings into identity-related and frame-specific components, then decorrelates frame embeddings by suppressing shared directions across frames. This reduces cross-frame interference without modifying diffusion parameters or requiring additional supervision. Under identical diffusion backbones and inference settings, ReDiStory improves identity consistency while maintaining prompt fidelity. Experiments on the ConsiStory+ benchmark show consistent gains over 1Prompt1Story on multiple identity consistency metrics. Code is available at: https://github.com/YuZhenyuLindy/ReDiStory

</details>


### [158] [StoryState: Agent-Based State Control for Consistent and Editable Storybooks](https://arxiv.org/abs/2602.01305)
*Ayushman Sarkar,Zhenyu Yu,Wei Tang,Chu Chen,Kangning Cui,Mohd Yamani Idna Idris*

Main category: cs.CV

TL;DR: 提出StoryState系统，通过显式管理故事状态，实现对多页故事书的精细化编辑，提升视觉一致性与编辑效率。


<details>
  <summary>Details</summary>
Motivation: 现有大多模态故事生成工具虽然能自动生成插画故事书，但由于故事的角色、世界设定等状态信息仅隐含于输入，用户无法细致调整故事细节，编辑粒度较粗且易破坏跨页一致性。

Method: StoryState作为无须重新训练的文本生成图像方法的中介，基于LLM构建代理，显式表示角色表、世界设定和分页面约束，并自动维护这些状态，通过prompt生成或编辑每一页内容，从而对各种生成后端保持兼容性。

Result: 在系统级多页编辑实验中，StoryState能实现更精细的本地页面编辑，提升了多页视觉一致性，减少了意外改动、交互次数及编辑时间，效果优于1Prompt1Story，并接近Gemini Storybook的一致性。

Conclusion: StoryState为故事书生成提供了可编辑、高一致性的解决方案，有效提升了端到端创作体验，具备良好的通用性和实际应用前景。

Abstract: Large multimodal models have enabled one-click storybook generation, where users provide a short description and receive a multi-page illustrated story. However, the underlying story state, such as characters, world settings, and page-level objects, remains implicit, making edits coarse-grained and often breaking visual consistency. We present StoryState, an agent-based orchestration layer that introduces an explicit and editable story state on top of training-free text-to-image generation. StoryState represents each story as a structured object composed of a character sheet, global settings, and per-page scene constraints, and employs a small set of LLM agents to maintain this state and derive 1Prompt1Story-style prompts for generation and editing. Operating purely through prompts, StoryState is model-agnostic and compatible with diverse generation backends. System-level experiments on multi-page editing tasks show that StoryState enables localized page edits, improves cross-page consistency, and reduces unintended changes, interaction turns, and editing time compared to 1Prompt1Story, while approaching the one-shot consistency of Gemini Storybook. Code is available at https://github.com/YuZhenyuLindy/StoryState

</details>


### [159] [DeCorStory: Gram-Schmidt Prompt Embedding Decorrelation for Consistent Storytelling](https://arxiv.org/abs/2602.01306)
*Ayushman Sarkar,Zhenyu Yu,Mohd Yamani Idna Idris*

Main category: cs.CV

TL;DR: DeCorStory是一种训练免除的推理框架，用于改善文本到图像故事生成中的帧间语义一致性与视觉一致性，比现有训练免除方法取得更优表现。


<details>
  <summary>Details</summary>
Motivation: 现有训练免除的方法在将所有提示拼接为单一序列时，会导致帧间嵌入相关性过强，进而出现颜色泄漏、背景混合、角色漂移等问题，不能很好地保持跨帧的语义和视觉一致性。因此需要一种新方法提升生成图像的连续性和多样性。

Method: 提出DeCorStory框架，通过Gram-Schmidt正交化减少帧间提示嵌入的相关性，并用奇异值重加权加强提示特定信息，同时在扩散过程中加入身份保留的交叉注意力保证角色一致性。整个流程无需模型修改或微调，可直接集成入现有扩散模型。

Result: 实验证明DeCorStory在提示-图像对齐、身份一致性和视觉多样性方面均优于其他训练免除方法，并在无训练基线中达到最新最优水平。

Conclusion: DeCorStory为文本驱动的故事图像生成提供了一种训练免除、效果优越且实现便捷的新方案，能有效提升帧间语义与视觉一致性，促进相关应用发展。

Abstract: Maintaining visual and semantic consistency across frames is a key challenge in text-to-image storytelling. Existing training-free methods, such as One-Prompt-One-Story, concatenate all prompts into a single sequence, which often induces strong embedding correlation and leads to color leakage, background blending, and identity drift. We propose DeCorStory, a training-free inference-time framework that explicitly reduces inter-frame semantic interference. DeCorStory applies Gram-Schmidt prompt embedding decorrelation to orthogonalize frame-level semantics, followed by singular value reweighting to strengthen prompt-specific information and identity-preserving cross-attention to stabilize character identity during diffusion. The method requires no model modification or fine-tuning and can be seamlessly integrated into existing diffusion pipelines. Experiments demonstrate consistent improvements in prompt-image alignment, identity consistency, and visual diversity, achieving state-of-the-art performance among training-free baselines. Code is available at: https://github.com/YuZhenyuLindy/DeCorStory

</details>


### [160] [FlowCast: Trajectory Forecasting for Scalable Zero-Cost Speculative Flow Matching](https://arxiv.org/abs/2602.01329)
*Divya Jyoti Bajpai,Shubham Agarwal,Apoorv Saxena,Kuldeep Kulkarni,Subrata Mitra,Manjesh Kumar Hanawal*

Main category: cs.CV

TL;DR: 论文提出了一种无需重新训练、能大幅加速Flow Matching (FM) 视觉生成模型推理的新框架FlowCast，实现了在不损失生成质量下的显著速度提升。


<details>
  <summary>Details</summary>
Motivation: 流匹配（FM）方法虽然生成图像质量高，但推理速度极慢，严重制约其实用性。现有加速方案无法兼顾效果、开销及通用性。因此需要新的加速方法，既能保证效果又能直接应用于各种FM模型。

Method: FlowCast是一种训练免疫的推理加速框架。其核心思想是利用FM模型训练时保持速度恒定的特性，通过线性外推当前速度，预测未来步长，并结合均方误差门控机制判断是否采纳。这样在状态稳定时可跳过冗余步骤，在复杂区域保持精确。框架无需额外神经网络，任意FM模型均可直接嵌入。

Result: 理论分析给出了FlowCast带来的轨迹偏差上界。实验显示，FlowCast在图像生成、视频生成及编辑任务中实现了超过2.5倍的推理加速，且生成质量与标准完整流程无差别，优于现有主流加速基线。

Conclusion: FlowCast无需训练、易于集成，可大幅加速FM视觉生成模型推理，在保证生成质量的同时显著提升了实用性和效率，有助于将FM模型应用于实时或交互场景。

Abstract: Flow Matching (FM) has recently emerged as a powerful approach for high-quality visual generation. However, their prohibitively slow inference due to a large number of denoising steps limits their potential use in real-time or interactive applications. Existing acceleration methods, like distillation, truncation, or consistency training, either degrade quality, incur costly retraining, or lack generalization. We propose FlowCast, a training-free speculative generation framework that accelerates inference by exploiting the fact that FM models are trained to preserve constant velocity. FlowCast speculates future velocity by extrapolating current velocity without incurring additional time cost, and accepts it if it is within a mean-squared error threshold. This constant-velocity forecasting allows redundant steps in stable regions to be aggressively skipped while retaining precision in complex ones. FlowCast is a plug-and-play framework that integrates seamlessly with any FM model and requires no auxiliary networks. We also present a theoretical analysis and bound the worst-case deviation between speculative and full FM trajectories. Empirical evaluations demonstrate that FlowCast achieves $>2.5\times$ speedup in image generation, video generation, and editing tasks, outperforming existing baselines with no quality loss as compared to standard full generation.

</details>


### [161] [What Does Vision Tool-Use Reinforcement Learning Really Learn? Disentangling Tool-Induced and Intrinsic Effects for Crop-and-Zoom](https://arxiv.org/abs/2602.01334)
*Yan Ma,Weiyu Zhang,Tianle Li,Linge Du,Xuyang Shen,Pengfei Liu*

Main category: cs.CV

TL;DR: 這篇論文分析了視覺工具使用強化學習（RL）對視覺語言模型（VLM）性能的影響，發現其提升主要來自模型本身能力的增強，而非善用工具。


<details>
  <summary>Details</summary>
Motivation: 過去用視覺工具（如裁剪、縮放）的RL讓VLM表現提升，但不清楚這種提升是來自工具的有效利用還是模型本身的進步，因此作者想解開這一現象。

Method: 作者提出MED框架，分為度量、解釋和診斷三步，能區分內在能力變化與工具帶來的效應，並細分工具影響的“增益”和“傷害”，進一步分析其演化機制。實驗涵蓋兩種工具先驗不同的VLM，以及六個基準測試點。

Result: 實驗發現在不同檢查點上，模型進步主要來自自身學習，工具RL主要在降低工具本身引入的錯誤，比如減少因調用出錯和工具認知干擾，對於藉助工具糾正模型失誤提升有限。

Conclusion: 現有的視覺工具使用RL方法，是學會安全地與工具共存，還無法真正駕馭和精通工具。

Abstract: Vision tool-use reinforcement learning (RL) can equip vision-language models with visual operators such as crop-and-zoom and achieves strong performance gains, yet it remains unclear whether these gains are driven by improvements in tool use or evolving intrinsic capabilities.We introduce MED (Measure-Explain-Diagnose), a coarse-to-fine framework that disentangles intrinsic capability changes from tool-induced effects, decomposes the tool-induced performance difference into gain and harm terms, and probes the mechanisms driving their evolution. Across checkpoint-level analyses on two VLMs with different tool priors and six benchmarks, we find that improvements are dominated by intrinsic learning, while tool-use RL mainly reduces tool-induced harm (e.g., fewer call-induced errors and weaker tool schema interference) and yields limited progress in tool-based correction of intrinsic failures. Overall, current vision tool-use RL learns to coexist safely with tools rather than master them.

</details>


### [162] [Beyond Pixels: Visual Metaphor Transfer via Schema-Driven Agentic Reasoning](https://arxiv.org/abs/2602.01335)
*Yu Xu,Yuxin Zhang,Juan Cao,Lin Gao,Chunyu Wang,Oliver Deussen,Tong-Yee Lee,Fan Tang*

Main category: cs.CV

TL;DR: 该论文提出了视觉隐喻迁移（VMT）任务，通过多智能体框架及认知理论，将抽象创意从参考图像迁移到新的目标，实现高阶视觉隐喻生成，并显著优于现有生成式AI方法。


<details>
  <summary>Details</summary>
Motivation: 生成式AI缺乏对隐喻性、抽象逻辑的理解和创造能力，无法实现真正具有创意的视觉隐喻生成。因此，作者提出要突破现有AI只能处理像素和表面特征的局限，使AI能够自动从参考图像中提取创造本质并迁移到新的主题对象上。

Method: 作者提出了一个受认知理论启发的多智能体框架，通过概念融合理论（CBT）和创新的Schema Grammar，将关系不变性与具体视觉元素解耦。该系统包括感知智能体（提取参考图像的schema）、迁移智能体（保证通用空间不变性）、生成智能体（高保真图像合成）及诊断智能体（模拟专家评审、闭环纠错）。

Result: 大量实验和人工评估结果表明，该方法在隐喻一致性、类比适切性和视觉创意性等方面显著优于最先进的基线方法。

Conclusion: 该研究为广告与媒体等高影响力创意场景中的自动化视觉隐喻生成提供了坚实的技术基础，并推动了生成式AI在创意领域的研究进展。源码将公开。

Abstract: A visual metaphor constitutes a high-order form of human creativity, employing cross-domain semantic fusion to transform abstract concepts into impactful visual rhetoric. Despite the remarkable progress of generative AI, existing models remain largely confined to pixel-level instruction alignment and surface-level appearance preservation, failing to capture the underlying abstract logic necessary for genuine metaphorical generation. To bridge this gap, we introduce the task of Visual Metaphor Transfer (VMT), which challenges models to autonomously decouple the "creative essence" from a reference image and re-materialize that abstract logic onto a user-specified target subject. We propose a cognitive-inspired, multi-agent framework that operationalizes Conceptual Blending Theory (CBT) through a novel Schema Grammar ("G"). This structured representation decouples relational invariants from specific visual entities, providing a rigorous foundation for cross-domain logic re-instantiation. Our pipeline executes VMT through a collaborative system of specialized agents: a perception agent that distills the reference into a schema, a transfer agent that maintains generic space invariance to discover apt carriers, a generation agent for high-fidelity synthesis and a hierarchical diagnostic agent that mimics a professional critic, performing closed-loop backtracking to identify and rectify errors across abstract logic, component selection, and prompt encoding. Extensive experiments and human evaluations demonstrate that our method significantly outperforms SOTA baselines in metaphor consistency, analogy appropriateness, and visual creativity, paving the way for automated high-impact creative applications in advertising and media. Source code will be made publicly available.

</details>


### [163] [MTC-VAE: Multi-Level Temporal Compression with Content Awareness](https://arxiv.org/abs/2602.01340)
*Yubo Dong,Linchao Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种将固定压缩率的VAE转换为支持多级时域压缩的新方法，并验证了该方法在提升视频处理效率和性能方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有Latent Video Diffusion Models（LVDMs）在高压缩率下效率会显著下降，尤其是在不增加隐藏通道维度的情况下添加额外采样层时。因此，需要新的技术来支持更高但灵活的视频压缩能力，并减少性能损失。

Method: 作者提出了一种简单的微调方法，可以将固定压缩率的VAE转化为支持多级时域压缩的模型。该方法能够应对高压缩率带来的性能下降。同时，作者也系统地分析了不同压缩级别对模型在多样视频片段上的表现影响。

Result: 实验证明，所提出的方法能够有效缓解高压缩率下的性能衰退，并且与扩散模型DiT良好兼容，可以实现同时训练。在多样特性的视频片段上表现出较好的适应性和有效性。

Conclusion: 多级时域压缩VAE能够提升视频扩散模型的处理能力和灵活性，兼容现有扩散生成模型，为视频生成和压缩提供了新的思路和方法。

Abstract: Latent Video Diffusion Models (LVDMs) rely on Variational Autoencoders (VAEs) to compress videos into compact latent representations. For continuous Variational Autoencoders (VAEs), achieving higher compression rates is desirable; yet, the efficiency notably declines when extra sampling layers are added without expanding the dimensions of hidden channels. In this paper, we present a technique to convert fixed compression rate VAEs into models that support multi-level temporal compression, providing a straightforward and minimal fine-tuning approach to counteract performance decline at elevated compression rates.Moreover, we examine how varying compression levels impact model performance over video segments with diverse characteristics, offering empirical evidence on the effectiveness of our proposed approach. We also investigate the integration of our multi-level temporal compression VAE with diffusion-based generative models, DiT, highlighting successful concurrent training and compatibility within these frameworks. This investigation illustrates the potential uses of multi-level temporal compression.

</details>


### [164] [Adaptive Visual Autoregressive Acceleration via Dual-Linkage Entropy Analysis](https://arxiv.org/abs/2602.01345)
*Yu Zhang,Jingyi Liu,Feng Liu,Duoqian Miao,Qi Zhang,Kexue Fu,Changwei Wang,Longbing Cao*

Main category: cs.CV

TL;DR: NOVA是一种在不需额外训练的情况下，通过熵分析实现VAR模型自适应token裁剪加速的框架，有效提升推理速度且保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有视觉自回归（VAR）建模由于token数量巨大导致计算代价高昂，而现有token裁剪加速方法存在阶段划分过于经验、裁剪计划不自适应及加速范围有限等问题，制约了模型的推理速度提升。作者希望解决这些token裁剪过程中存在的关键问题。

Method: NOVA提出基于熵变化在线检测VAR模型推理时建模动态的转变点，据此自适应地确定不同layer和scale的token裁剪比率，并动态调整scale-linkage和layer-linkage，实现按需裁剪低熵token；同时复用上一scale的残差缓存，无需重训练即可应用于现有VAR模型，实现训练无关的加速。

Result: NOVA在推理过程中能够根据熵变化动态调整token裁剪，自适应地提升推理加速比且兼顾生成质量。大量实验证明了NOVA作为无训练加速VAR模型的简单有效性。

Conclusion: NOVA框架通过熵分析有效解决了现有VAR加速方法的局限，实现训练无关、高效且质量可控的token减裁加速，为VAR模型推理优化提供了新思路。

Abstract: Visual AutoRegressive modeling (VAR) suffers from substantial computational cost due to the massive token count involved. Failing to account for the continuous evolution of modeling dynamics, existing VAR token reduction methods face three key limitations: heuristic stage partition, non-adaptive schedules, and limited acceleration scope, thereby leaving significant acceleration potential untapped. Since entropy variation intrinsically reflects the transition of predictive uncertainty, it offers a principled measure to capture modeling dynamics evolution. Therefore, we propose NOVA, a training-free token reduction acceleration framework for VAR models via entropy analysis. NOVA adaptively determines the acceleration activation scale during inference by online identifying the inflection point of scale entropy growth. Through scale-linkage and layer-linkage ratio adjustment, NOVA dynamically computes distinct token reduction ratios for each scale and layer, pruning low-entropy tokens while reusing the cache derived from the residuals at the prior scale to accelerate inference and maintain generation quality. Extensive experiments and analyses validate NOVA as a simple yet effective training-free acceleration framework.

</details>


### [165] [T2M Mamba: Motion Periodicity-Saliency Coupling Approach for Stable Text-Driven Motion Generation](https://arxiv.org/abs/2602.01352)
*Xingzu Zhan,Chen Xie,Honghang Chen,Yixun Lin,Xiaochun Mai*

Main category: cs.CV

TL;DR: 本文提出了一种改进的文本到动作生成方法T2M Mamba，通过融合动作周期性与关键帧重要性，并增强文本与动作表达对齐性能，显著提升了生成动作的稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前的文本到动作生成模型存在两个主要问题：一是动作周期性与关键帧显著性被独立处理，忽视了它们之间的耦合关系，导致长序列生成时动作漂移；二是模型对语义等效但措辞不同的描述不鲁棒，轻微同义替换会引起动作输出异常。

Method: 作者提出了Periodicity-Saliency Aware Mamba模型，利用强化的密度峰值聚类算法对关键帧赋权，并采用基于FFT的自动相关方法有效估算动作周期性，从而捕捉两者的耦合动态。此外，设计了周期性差分跨模态对齐模块（PDCAM），增强文本与动作特征对齐的鲁棒性。

Result: 在HumanML3D和KIT-ML数据集上的实验结果显示，该方法取得了0.068的FID分数，并在所有其他评价指标上获得了一致改进。

Conclusion: T2M Mamba有效缓解了既往文本到动作生成模型的两个核心缺陷，提升了生成动作的保真度和鲁棒性，验证了周期性-关键帧耦合建模及跨模态对齐机制的价值。

Abstract: Text-to-motion generation, which converts motion language descriptions into coherent 3D human motion sequences, has attracted increasing attention in fields, such as avatar animation and humanoid robotic interaction. Though existing models have achieved significant fidelity, they still suffer from two core limitations: (i) They treat motion periodicity and keyframe saliency as independent factors, overlooking their coupling and causing generation drift in long sequences. (ii) They are fragile to semantically equivalent paraphrases, where minor synonym substitutions distort textual embeddings, propagating through the decoder and producing unstable or erroneous motions. In this work, we propose T2M Mamba to address these limitations by (i) proposing Periodicity-Saliency Aware Mamba, which utilizes novel algorithms for keyframe weight estimation via enhanced Density Peaks Clustering and motion periodicity estimation via FFT-accelerated autocorrelation to capture coupled dynamics with minimal computational overhead, and (ii) constructing a Periodic Differential Cross-modal Alignment Module (PDCAM) to enhance robust alignment of textual and motion embeddings. Extensive experiments on HumanML3D and KIT-ML datasets have been conducted, confirming the effectiveness of our approach, achieving an FID of 0.068 and consistent gains on all other metrics.

</details>


### [166] [Exposing and Defending the Achilles' Heel of Video Mixture-of-Experts](https://arxiv.org/abs/2602.01369)
*Songping Wang,Qinglong Liu,Yueming Lyu,Ning Li,Ziwen He,Caifeng Shan*

Main category: cs.CV

TL;DR: 本论文针对视频理解任务中的MoE模型，提出了新的对抗攻击（TLGA与J-TLGA）和防御（J-TLAT）方法，系统分析并提升了模型在对抗样本下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管MoE在视频理解中表现优异，但其在对抗攻击下的鲁棒性研究较少，现有攻击方法忽视了各组件（如路由器与专家模块）各自及协同的脆弱性。因此，作者希望细粒度分析并提升MoE的鲁棒性。

Method: 作者首先设计了针对MoE内部路由器的对抗攻击（TLGA），揭示其独立弱点；随后又提出联合扰动路由器与专家的J-TLGA，暴露协同性弱点。为提升鲁棒性，进一步引入了J-TLAT联合对抗训练，提升整体防御能力。

Result: 所提J-TLAT框架可有效加强MoE各组件对对抗扰动的鲁棒性，在不同数据集与模型结构上均取得一致提升；推理成本相较稠密模型减少60%以上。

Conclusion: 本文首次系统分析了MoE的组件级鲁棒性，并提出兼顾攻击与防御的新方法。J-TLAT不仅提升了模型鲁棒性，还具有较低的推理成本，适用于实际部署。

Abstract: Mixture-of-Experts (MoE) has demonstrated strong performance in video understanding tasks, yet its adversarial robustness remains underexplored. Existing attack methods often treat MoE as a unified architecture, overlooking the independent and collaborative weaknesses of key components such as routers and expert modules. To fill this gap, we propose Temporal Lipschitz-Guided Attacks (TLGA) to thoroughly investigate component-level vulnerabilities in video MoE models. We first design attacks on the router, revealing its independent weaknesses. Building on this, we introduce Joint Temporal Lipschitz-Guided Attacks (J-TLGA), which collaboratively perturb both routers and experts. This joint attack significantly amplifies adversarial effects and exposes the Achilles' Heel (collaborative weaknesses) of the MoE architecture. Based on these insights, we further propose Joint Temporal Lipschitz Adversarial Training (J-TLAT). J-TLAT performs joint training to further defend against collaborative weaknesses, enhancing component-wise robustness. Our framework is plug-and-play and reduces inference cost by more than 60% compared with dense models. It consistently enhances adversarial robustness across diverse datasets and architectures, effectively mitigating both the independent and collaborative weaknesses of MoE.

</details>


### [167] [PolyGen: Fully Synthetic Vision-Language Training via Multi-Generator Ensembles](https://arxiv.org/abs/2602.01370)
*Leonardo Brusini,Cristian Sbrolli,Eugenio Lomurno,Toshihiko Yamasaki,Matteo Matteucci*

Main category: cs.CV

TL;DR: PolyGen通过多生成器和结构多样性生成合成数据，大幅提升视觉-语言模型多任务和组合性能力，效果优于单一生成器方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言预训练合成数据方法大多依赖放大单一生成器，造成生成偏差和特征单一，限制性能提升。

Method: 提出PolyGen框架，通过多个结构不同的生成器交叉训练，减少模型特有伪影，并采用程序化困难负样本机制提升细粒度合成能力。数据预算从单一caption分配到多源变体，以提高特征多样性。

Result: PolyGen在多任务基准测试上比最佳单一生成器基线（SynthCLIP）高19%，在组合性基准（SugarCrepe++）高9.1%。

Conclusion: 提升结构多样性比单纯增大单一来源数据量更有效，PolyGen能更高效地扩展和提升视觉-语言预训练模型表现。

Abstract: Synthetic data offers a scalable solution for vision-language pre-training, yet current state-of-the-art methods typically rely on scaling up a single generative backbone, which introduces generator-specific spectral biases and limits feature diversity. In this work, we introduce PolyGen, a framework that redefines synthetic data construction by prioritizing manifold coverage and compositional rigor over simple dataset size. PolyGen employs a Polylithic approach to train on the intersection of architecturally distinct generators, effectively marginalizing out model-specific artifacts. Additionally, we introduce a Programmatic Hard Negative curriculum that enforces fine-grained syntactic understanding. By structurally reallocating the same data budget from unique captions to multi-source variations, PolyGen achieves a more robust feature space, outperforming the leading single-source baseline (SynthCLIP) by +19.0% on aggregate multi-task benchmarks and on the SugarCrepe++ compositionality benchmark (+9.1%). These results demonstrate that structural diversity is a more data-efficient scaling law than simply increasing the volume of single-source samples.

</details>


### [168] [PromptRL: Prompt Matters in RL for Flow-Based Image Generation](https://arxiv.org/abs/2602.01382)
*Fu-Yun Wang,Han Zhang,Michael Gharbi,Hongsheng Li,Taesung Park*

Main category: cs.CV

TL;DR: 提出PromptRL框架，通过将语言模型引入基于流的强化学习流程，解决现有文生图生成中样本效率低和提示过拟合问题，极大提升了多项指标，且代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有流匹配模型结合强化学习对齐奖励目标，但存在生成多样性不足导致样本效率低、以及对训练提示词过度记忆（提示过拟合），在面对风格变化但语义等价提示时表现严重下降。需要新的方法提升样本利用率并改善泛化能力。

Method: 提出PromptRL方法，将语言模型作为可学习的提示词改写器，直接嵌入RL流程中，优化时协同训练模型与提示改写器，动态重写提示，提升 RL 多样性与泛化性。

Result: PromptRL在多个基准任务中取得SOTA：GenEval 0.97，OCR 0.98，PickScore 24.05。在大规模图像编辑任务上，用极少采样（0.06M）就将FLUX.1-Kontext的EditReward从1.19提升到1.43，超越Gemini 2.5 Flash Image（1.37），媲美更复杂所需微调和注释的数据方案（ReasonNet 1.44）。比普通RL采样效率高出2倍以上。

Conclusion: PromptRL显著提高了图像生成和编辑的性能与采样效率，缓解了过拟合与数据利用问题，显示在流模型RL流程中引入提示重写有巨大价值。

Abstract: Flow matching models (FMs) have revolutionized text-to-image (T2I) generation, with reinforcement learning (RL) serving as a critical post-training strategy for alignment with reward objectives. In this research, we show that current RL pipelines for FMs suffer from two underappreciated yet important limitations: sample inefficiency due to insufficient generation diversity, and pronounced prompt overfitting, where models memorize specific training formulations and exhibit dramatic performance collapse when evaluated on semantically equivalent but stylistically varied prompts. We present PromptRL (Prompt Matters in RL for Flow-Based Image Generation), a framework that incorporates language models (LMs) as trainable prompt refinement agents directly within the flow-based RL optimization loop. This design yields two complementary benefits: rapid development of sophisticated prompt rewriting capabilities and, critically, a synergistic training regime that reshapes the optimization dynamics. PromptRL achieves state-of-the-art performance across multiple benchmarks, obtaining scores of 0.97 on GenEval, 0.98 on OCR accuracy, and 24.05 on PickScore.
  Furthermore, we validate the effectiveness of our RL approach on large-scale image editing models, improving the EditReward of FLUX.1-Kontext from 1.19 to 1.43 with only 0.06 million rollouts, surpassing Gemini 2.5 Flash Image (also known as Nano Banana), which scores 1.37, and achieving comparable performance with ReasonNet (1.44), which relied on fine-grained data annotations along with a complex multi-stage training. Our extensive experiments empirically demonstrate that PromptRL consistently achieves higher performance ceilings while requiring over 2$\times$ fewer rollouts compared to naive flow-only RL. Our code is available at https://github.com/G-U-N/UniRL.

</details>


### [169] [Stronger Semantic Encoders Can Harm Relighting Performance: Probing Visual Priors via Augmented Latent Intrinsics](https://arxiv.org/abs/2602.01391)
*Xiaoyan Xing,Xiao Zhang,Sezer Karaoglu,Theo Gevers,Anand Bhattad*

Main category: cs.CV

TL;DR: 本文提出了一种新的图像重光照方法（ALI），通过结合具有像素对齐的视觉先验和自监督策略，在复杂材质上显著提升了重光照效果。


<details>
  <summary>Details</summary>
Motivation: 现有图像重光照方法在处理金属、玻璃等复杂材质时表现不佳，尽管引入了更强大的预训练视觉特征，但这些特征反而会降低重光照的保真度，存在语义抽象与光照真实感的权衡问题。

Method: 作者分析了视觉编码器中语义抽象与光照信息表达的矛盾，提出了Augmented Latent Intrinsics（ALI）框架：通过像素对齐的视觉编码器与潜在固有属性表示融合，在自监督精细化训练策略下，实现在无标注真实图像对上训练，同时引入致密的像素级视觉先验，提高光照重现质量。

Result: ALI方法在仅基于未标注的真实图像对进行训练的条件下，在各类场景，特别是金属、玻璃等高光复杂材质上显著优于现有方法。

Conclusion: 强预训练视觉先验并不总能提升图像重光照质量。ALI方法通过平衡全局语义与局部像素信息，在真实复杂材质上取得更高性能，为柯解决相关领域的一大瓶颈。

Abstract: Image-to-image relighting requires representations that disentangle scene properties from illumination. Recent methods rely on latent intrinsic representations but remain under-constrained and often fail on challenging materials such as metal and glass. A natural hypothesis is that stronger pretrained visual priors should resolve these failures. We find the opposite: features from top-performing semantic encoders often degrade relighting quality, revealing a fundamental trade-off between semantic abstraction and photometric fidelity. We study this trade-off and introduce Augmented Latent Intrinsics (ALI), which balances semantic context and dense photometric structure by fusing features from a pixel-aligned visual encoder into a latent-intrinsic framework, together with a self-supervised refinement strategy to mitigate the scarcity of paired real-world data. Trained only on unlabeled real-world image pairs and paired with a dense, pixel-aligned visual prior, ALI achieves strong improvements in relighting, with the largest gains on complex, specular materials. Project page: https:\\augmented-latent-intrinsics.github.io

</details>


### [170] [Where to Attend: A Principled Vision-Centric Position Encoding with Parabolas](https://arxiv.org/abs/2602.01418)
*Christoffer Koo Øhrstrøm,Rafael I. Cabral Muchacho,Yifei Dong,Filippos Moumtzidellis,Ronja Güldenring,Florian T. Pokorny,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: 这篇论文提出了一种基于抛物线的视觉位置编码方法（PaPE），显著提升视觉模型的性能，在多个视觉任务和数据集上优于现有方法，且具有良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉位置编码多源自于1D文本序列的方法，未能充分考虑视觉的空间特性和需求，如平移、旋转不变性等，限制了模型表现。作者希望设计更符合视觉特质的位置编码。

Method: 提出PaPE（抛物线型位置编码），基于平移不变性、旋转不变性（PaPE-RI）、随距离衰减、方向性、上下文感知等原则设计，适用于各种视觉模态，如图像、点云、视频等。

Result: 在4种模态、8个数据集上验证，PaPE和PaPE-RI在7/8数据集上取得最佳性能。在ImageNet-1K跨分辨率测试中，PaPE比次优方法绝对提升可达10.5%。

Conclusion: PaPE能够显著提升注意力模型的视觉任务表现，在多模态、多数据集上表现突出，并且具有良好的泛化和扩展能力。

Abstract: We propose Parabolic Position Encoding (PaPE), a parabola-based position encoding for vision modalities in attention-based architectures. Given a set of vision tokens-such as images, point clouds, videos, or event camera streams-our objective is to encode their positions while accounting for the characteristics of vision modalities. Prior works have largely extended position encodings from 1D-sequences in language to nD-structures in vision, but only with partial account of vision characteristics. We address this gap by designing PaPE from principles distilled from prior work: translation invariance, rotation invariance (PaPE-RI), distance decay, directionality, and context awareness. We evaluate PaPE on 8 datasets that span 4 modalities. We find that either PaPE or PaPE-RI achieves the top performance on 7 out of 8 datasets. Extrapolation experiments on ImageNet-1K show that PaPE extrapolates remarkably well, improving in absolute terms by up to 10.5% over the next-best position encoding. Code is available at https://github.com/DTU-PAS/parabolic-position-encoding.

</details>


### [171] [BioTamperNet: Affinity-Guided State-Space Model Detecting Tampered Biomedical Images](https://arxiv.org/abs/2602.01435)
*Soumyaroop Nandi,Prem Natarajan*

Main category: cs.CV

TL;DR: BioTamperNet是一种面向生物医学图像篡改检测的新模型，通过亲和力引导的注意力机制显著提升对伪造内容的检测性能。


<details>
  <summary>Details</summary>
Motivation: 目前主流的图像取证模型大多基于自然图像进行训练，对于细微伪造的生物医学图像效果不理想，导致实验有效性受到影响。亟需专门针对生物医学图像的篡改检测新方法。

Method: 提出BioTamperNet，利用亲和力引导的自注意力模块捕捉图像内部相似性，并利用亲和力引导的交叉注意力模块建模跨图像的对应关系。该模型受到State Space Model（SSM）近似方法启发，集成了轻量级线性注意力机制，可高效实现精细定位。整体端到端训练，可以同时检测篡改区域及其源位置。

Result: 在主流生物取证数据集上进行了大量实验，与多种主流方法对比，BioTamperNet在准确检测复制区域任务上取得了显著性能提升。

Conclusion: BioTamperNet有效提升了对生物医学图像中细微伪造的检测能力，为相关领域实验数据的可信性保驾护航。

Abstract: We propose BioTamperNet, a novel framework for detecting duplicated regions in tampered biomedical images, leveraging affinity-guided attention inspired by State Space Model (SSM) approximations. Existing forensic models, primarily trained on natural images, often underperform on biomedical data where subtle manipulations can compromise experimental validity. To address this, BioTamperNet introduces an affinity-guided self-attention module to capture intra-image similarities and an affinity-guided cross-attention module to model cross-image correspondences. Our design integrates lightweight SSM-inspired linear attention mechanisms to enable efficient, fine-grained localization. Trained end-to-end, BioTamperNet simultaneously identifies tampered regions and their source counterparts. Extensive experiments on the benchmark bio-forensic datasets demonstrate significant improvements over competitive baselines in accurately detecting duplicated regions. Code - https://github.com/SoumyaroopNandi/BioTamperNet

</details>


### [172] [Cross-Paradigm Evaluation of Gaze-Based Semantic Object Identification for Intelligent Vehicles](https://arxiv.org/abs/2602.01452)
*Penghao Deng,Jidong J. Yang,Jiachen Bian*

Main category: cs.CV

TL;DR: 本研究通过车辆前视摄像头获取道路场景，分析驾驶时驾驶员注视点与场景语义对象的匹配，为辅助驾驶系统设计提供数据支持。


<details>
  <summary>Details</summary>
Motivation: 了解驾驶员在行驶过程中的视线分布对提升辅助驾驶系统性能和道路安全至关重要。将注视点与场景中具体语义对象关联，有助于系统更好地理解驾驶员关注重点。

Method: 论文提出三种基于视觉的方法：1）直接对象检测（YOLOv13）；2）分割辅助分类（SAM2+EfficientNetV2与YOLOv13对比）；3）基于查询的视觉-语言模型（Qwen2.5-VL-7b与Qwen2.5-VL-32b对比），用于识别注视对象。比较方法在不同条件下的表现。

Result: YOLOv13和大规模VLM（Qwen2.5-VL-32b）效果最佳，Macro F1分数超过0.84，尤其Qwen2.5-VL-32b在夜间及检测小型安全关键物体时表现更稳健。分割辅助方法存在明显“部分-整体”语义差异，召回率低。

Conclusion: 存在实时检测效率与大模型语境理解和鲁棒性的权衡。研究为未来能理解驾驶员动态关注的智能驾驶监控系统设计提供理论和实践参考。

Abstract: Understanding where drivers direct their visual attention during driving, as characterized by gaze behavior, is critical for developing next-generation advanced driver-assistance systems and improving road safety. This paper tackles this challenge as a semantic identification task from the road scenes captured by a vehicle's front-view camera. Specifically, the collocation of gaze points with object semantics is investigated using three distinct vision-based approaches: direct object detection (YOLOv13), segmentation-assisted classification (SAM2 paired with EfficientNetV2 versus YOLOv13), and query-based Vision-Language Models, VLMs (Qwen2.5-VL-7b versus Qwen2.5-VL-32b). The results demonstrate that the direct object detection (YOLOv13) and Qwen2.5-VL-32b significantly outperform other approaches, achieving Macro F1-Scores over 0.84. The large VLM (Qwen2.5-VL-32b), in particular, exhibited superior robustness and performance for identifying small, safety-critical objects such as traffic lights, especially in adverse nighttime conditions. Conversely, the segmentation-assisted paradigm suffers from a "part-versus-whole" semantic gap that led to large failure in recall. The results reveal a fundamental trade-off between the real-time efficiency of traditional detectors and the richer contextual understanding and robustness offered by large VLMs. These findings provide critical insights and practical guidance for the design of future human-aware intelligent driver monitoring systems.

</details>


### [173] [Understanding vision transformer robustness through the lens of out-of-distribution detection](https://arxiv.org/abs/2602.01459)
*Joey Kuang,Alexander Wong*

Main category: cs.CV

TL;DR: 本文分析了主流小型视觉Transformer在量化（降低计算精度）条件下，面对分布外数据（OOD）时的表现，发现大规模预训练反而削弱了低比特量化的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视觉Transformer尽管表现突出，但高内存和计算成本限制其实时和普及化应用。量化技术能降低成本，但容易损失性能，且当前主要针对分布内任务，较少关注分布外情境。如何让量化模型兼具实时性和鲁棒性，是该研究关注的问题。

Method: 作者对DeiT、DeiT3和ViT等主流Vision Transformer在常见OOD数据集上进行了低比特（尤其是4比特）量化实验，对比分析了不同预训练规模（ImageNet-1k vs ImageNet-22k）和数据增强策略下的表现。

Result: (1) 4比特量化模型存在初始不稳定，更大规模（ImageNet-22k）预训练使得DeiT3模型量化后的下降幅度更大（性能降幅高达17%）；(2) 预训练规模越大，量化后识别分布外样本的能力下降得越厉害（ViT和DeiT3在AUPR-out指标降幅分别达15%和19.2%）；(3) 使用较小数据集预训练的模型降幅较小。

Conclusion: 大规模数据集的预训练虽然提升了全精度表现，但对低比特量化模型在分布外数据检测上的鲁棒性有副作用。提升低比特模型泛化能力时，应更关注数据增强而不是单纯扩大预训练数据规模。

Abstract: Vision transformers have shown remarkable performance in vision tasks, but enabling them for accessible and real-time use is still challenging. Quantization reduces memory and inference costs at the risk of performance loss. Strides have been made to mitigate low precision issues mainly by understanding in-distribution (ID) task behaviour, but the attention mechanism may provide insight on quantization attributes by exploring out-of-distribution (OOD) situations. We investigate the behaviour of quantized small-variant popular vision transformers (DeiT, DeiT3, and ViT) on common OOD datasets. ID analyses show the initial instabilities of 4-bit models, particularly of those trained on the larger ImageNet-22k, as the strongest FP32 model, DeiT3, sharply drop 17% from quantization error to be one of the weakest 4-bit models. While ViT shows reasonable quantization robustness for ID calibration, OOD detection reveals more: ViT and DeiT3 pretrained on ImageNet-22k respectively experienced a 15.0% and 19.2% average quantization delta in AUPR-out between full precision to 4-bit while their ImageNet-1k-only counterparts experienced a 9.5% and 12.0% delta. Overall, our results suggest pretraining on large scale datasets may hinder low-bit quantization robustness in OOD detection and that data augmentation may be a more beneficial option.

</details>


### [174] [Preserving Localized Patch Semantics in VLMs](https://arxiv.org/abs/2602.01530)
*Parsa Esmaeilkhani,Longin Jan Latecki*

Main category: cs.CV

TL;DR: 本文提出了一种新的损失函数（Logit Lens Loss, LLL），旨在提升Logit Lens在视觉-语言模型（VLMs）中的可解释性，通过保持视觉token的局部信息，使其产生更有意义的热力图。


<details>
  <summary>Details</summary>
Motivation: 在视觉-语言模型中，视觉token的内容很容易在语言处理过程中丧失其局部视觉信息，导致现有Logit Lens的可视化工具难以解释和定位原始视觉内容。因此，亟需方法能够保持并对齐视觉token与其实际图像概念的关系，提升可解释性。

Method: 作者提出了Logit Lens Loss（LLL），作为对传统下一个token预测（NTP）的补充损失，使视觉token的嵌入更好地与描述其图像区域的文本概念对齐。该方法无需修改模型结构或进行大规模再训练，仅通过添加损失函数约束self-attention中图像与文本token的混合。

Result: 实验结果显示，LLL不仅能够使Logit Lens生成有意义的目标置信热力图，还在不增加特殊结构（如分割头）的前提下提升了分割等以视觉为中心的任务表现。

Conclusion: Logit Lens Loss有效解决了视觉token可解释性不足的问题，为增强VLMs在下游视觉任务上的表现及可解释性提供了一种简便适用的新途径。

Abstract: Logit Lens has been proposed for visualizing tokens that contribute most to LLM answers. Recently, Logit Lens was also shown to be applicable in autoregressive Vision-Language Models (VLMs), where it illustrates the conceptual content of image tokens in the form of heatmaps, e.g., which image tokens are likely to depict the concept of cat in a given image. However, the visual content of image tokens often gets diffused to language tokens, and consequently, the locality of visual information gets mostly destroyed, which renders Logit Lens visualization unusable for explainability. To address this issue, we introduce a complementary loss to next-token prediction (NTP) to prevent the visual tokens from losing the visual representation inherited from corresponding image patches. The proposed Logit Lens Loss (LLL) is designed to make visual token embeddings more semantically aligned with the textual concepts that describe their image regions (e.g., patches containing a cat with the word "cat"), without requiring any architectural modification or large-scale training. This way, LLL constrains the mixing of image and text tokens in the self-attention layers in order to prevent image tokens from losing their localized visual information. As our experiments show, LLL not only makes Logit Lens practically relevant by producing meaningful object confidence maps in images, but also improves performance on vision-centric tasks like segmentation without attaching any special heads.

</details>


### [175] [Rotation-free Online Handwritten Character Recognition Using Linear Recurrent Units](https://arxiv.org/abs/2602.01533)
*Zhe Ling,Sicheng Yu,Danyu Yang*

Main category: cs.CV

TL;DR: 提出了一种基于滑动窗口路径签名（SW-PS）和线性递归单元（LRU）的在线手写字符旋转不变识别新方法，并在数据集上显著优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 现有在线手写字符识别尽管使用动态笔画等特征提升了准确性，但旋转变形会显著降低识别准确率，因此如何提取旋转不变特征仍是挑战。

Method: 采用滑动窗口路径签名（SW-PS）提取字符的局部结构特征，利用轻量级的线性递归单元（LRU）作为分类器，结合RNN的快速递增处理和SSM的高效并行训练，适应笔画动态变化。同时在不同字符集（数字、大写字母、汉字部首）上进行随机旋转（±180°）的实验。

Result: 在CASIA-OLHWDB1.1数据集的三种子集上，集成学习后的识别准确率分别达到了99.62%（数字）、96.67%（英文字母）、94.33%（汉字部首），超过其他对比模型，且收敛速度更快。

Conclusion: SW-PS+LRU方法在旋转不变性、准确率和训练收敛速度方面表现优异，为在线手写字符识别提供了更鲁棒的解决方案。

Abstract: Online handwritten character recognition leverages stroke order and dynamic features, which generally provide higher accuracy and robustness compared with offline recognition. However, in practical applications, rotational deformations can disrupt the spatial layout of strokes, substantially reducing recognition accuracy. Extracting rotation-invariant features therefore remains a challenging open problem. In this work, we employ the Sliding Window Path Signature (SW-PS) to capture local structural features of characters, and introduce the lightweight Linear Recurrent Units (LRU) as the classifier. The LRU combine the fast incremental processing capability of recurrent neural networks (RNN) with the efficient parallel training of state space models (SSM), while reliably modelling dynamic stroke characteristics. We conducted recognition experiments with random rotation angle up to $\pm 180^{\circ}$ on three subsets of the CASIA-OLHWDB1.1 dataset: digits, English upper letters, and Chinese radicals. The accuracies achieved after ensemble learning were $99.62\%$, $96.67\%$, and $94.33\%$, respectively. Experimental results demonstrate that the proposed SW-PS+LRU framework consistently surpasses competing models in both convergence speed and test accuracy.

</details>


### [176] [Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars](https://arxiv.org/abs/2602.01538)
*Youliang Zhang,Zhengguang Zhou,Zhentao Yu,Ziyao Huang,Teng Hu,Sen Liang,Guozhen Zhang,Ziqiao Peng,Shunkai Li,Yi Chen,Zixiang Zhou,Yuan Zhou,Qinglin Lu,Xiu Li*

Main category: cs.CV

TL;DR: 本文提出了InteractAvatar框架，实现了能够与周围物体交互的可说话虚拟人（Talking Avatar）自动生成。其创新点在于同时实现了环境感知和高质量动作控制，解决了以往方法难以处理人-物交互的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管已有方法能够生成具备简单动作的全身说话虚拟人，但让其执行与周围具体物体相关、且与文本语义对齐的交互动作仍然是未解决难点。挑战主要在于虚拟人需要具备环境感知能力，以及在生成动作时兼顾可控性与视频质量。

Method: 提出了Dual-Stream（双流）框架‘InteractAvatar’，通过将感知与规划从视频合成中解耦，提高了对人-物交互的处理。具体包括：使用检测（Detection）提升环境感知能力，引入感知与交互模块（PIM）生成文本相关动作；提出音频-交互感知生成模块（AIM）合成高质量的人-物交互视频；并设计动作到视频对齐器，支持模块的并行共生，缓解控制与质量矛盾。

Result: 设立了新的人-物交互视频基准数据集GroundedInter，开展了大量实验与对比，结果表明InteractAvatar方法在生成与环境交互的说话虚拟人方面效果优越。

Conclusion: 提出的InteractAvatar能有效生成能够与物体交互且语义一致的说话虚拟人动画，为视频生成相关任务提供了新思路，并推动了人-物交互合成的发展。

Abstract: Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: https://interactavatar.github.io

</details>


### [177] [FSCA-Net: Feature-Separated Cross-Attention Network for Robust Multi-Dataset Training](https://arxiv.org/abs/2602.01540)
*Yuehai Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种新的人群计数网络FSCA-Net，通过特征分离和交互式注意力机制，有效应对多数据集负迁移，提升跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有CNN和Transformer在人群计数领域虽表现优异，但跨环境时受域差异影响严重，直接多数据集训练会导致负迁移问题，因此有必要解决特征混淆和泛化能力不足的问题。

Method: FSCA-Net明确地将特征分为域不变和域特定两类，通过创新的交叉注意力融合模块自适应地建模二者之间的交互，提升知识迁移效果。同时设计互信息优化目标，促使域不变特征一致、域特定特征互补。

Result: 在多个人群计数基准上进行实验证明，FSCA-Net显著减弱了负迁移，实现了当前最优的跨数据集泛化能力。

Conclusion: FSCA-Net为多域人群计数提出了鲁棒且可扩展的方案，有效缓解负迁移、提升了实际应用价值。

Abstract: Crowd counting plays a vital role in public safety, traffic regulation, and smart city management. However, despite the impressive progress achieved by CNN- and Transformer-based models, their performance often deteriorates when applied across diverse environments due to severe domain discrepancies. Direct joint training on multiple datasets, which intuitively should enhance generalization, instead results in negative transfer, as shared and domain-specific representations become entangled. To address this challenge, we propose the Feature Separation and Cross-Attention Network FSCA-Net, a unified framework that explicitly disentangles feature representations into domain-invariant and domain-specific components. A novel cross-attention fusion module adaptively models interactions between these components, ensuring effective knowledge transfer while preserving dataset-specific discriminability. Furthermore, a mutual information optimization objective is introduced to maximize consistency among domain-invariant features and minimize redundancy among domain-specific ones, promoting complementary shared-private representations. Extensive experiments on multiple crowd counting benchmarks demonstrate that FSCA-Net effectively mitigates negative transfer and achieves state-of-the-art cross-dataset generalization, providing a robust and scalable solution for real-world crowd analysis.

</details>


### [178] [Toward Cognitive Supersensing in Multimodal Large Language Model](https://arxiv.org/abs/2602.01541)
*Boyi Li,Yifan Shen,Yuanzhe Liu,Yifan Xu,Jiateng Liu,Xinzhuo Li,Zhengyuan Li,Jingyuan Zhu,Yunhan Zhong,Fangzhou Lan,Jianguo Cao,James M. Rehg,Heng Ji,Ismini Lourentzou,Xu Cao*

Main category: cs.CV

TL;DR: 本文提出了一种名为Cognitive Supersensing的新训练范式，通过赋予多模态大语言模型（MLLMs）类人视觉意象能力，显著提升了其在认知型VQA任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs虽在感知类任务表现出色，但在需要抽象视觉记忆的复杂认知问题上表现有限，主要由于忽视了类似人类视空间画板和视觉意象的视觉推理机制。

Method: 作者提出Cognitive Supersensing训练范式，通过引入潜在视觉意象预测（LVIP）头，使模型能够联合学习视觉认知潜在序列，并与答案对齐，形成基于视觉的内部推理链。同时，采用强化学习阶段，利用视觉潜在信息优化文本推理路径。此外，提出了CogSense-Bench数据集评测MLLMs的认知能力。

Result: 实验表明，采用Cognitive Supersensing训练的MLLMs在CogSense-Bench大幅超越现有方法，并在域外数学和科学VQA基准上表现出更强泛化能力。

Conclusion: 内在视觉意象能力可能是连接感知识别和认知理解的关键。CogSense-Bench数据集及相关模型权重将开源。

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, a novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, a comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights.

</details>


### [179] [Combined Flicker-banding and Moire Removal for Screen-Captured Images](https://arxiv.org/abs/2602.01559)
*Libo Zhu,Zihan Zhou,Zhiyi Zhou,Yiyang Qu,Weihang Zhang,Keyu Shi,Yifan Fu,Yulun Zhang*

Main category: cs.CV

TL;DR: 本文提出了CLEAR框架，首次系统性地解决屏幕拍摄图片中莫尔条纹和频闪带联合去除的问题，并构建了相关大规模数据集。


<details>
  <summary>Details</summary>
Motivation: 随着移动设备拍摄屏幕越来越普遍，成像中常见的莫尔条纹和频闪带会严重影响图片质量。由于这两种退化的高度耦合，单一退化的去除方法无法胜任。本研究旨在应对这一复合退化问题。

Method: 1. 提出CLEAR统一修复框架；2. 构建同时包含莫尔条纹和频闪带的大规模数据集；3. 引入ISP为基础的频闪模拟管线，扩展退化类型并帮助模型训练；4. 设计频域分解重组模块和轨迹对齐损失函数，提升复合退化建模能力。

Result: 所提方法在多个评价指标和复杂实际场景下，均明显优于现有图像修复方法。

Conclusion: CLEAR框架是首个有效应对屏摄图像复合退化（莫尔+频闪带）的方法，为相关领域提供了新思路，具有显著实际应用价值。

Abstract: Capturing display screens with mobile devices has become increasingly common, yet the resulting images often suffer from severe degradations caused by the coexistence of moiré patterns and flicker-banding, leading to significant visual quality degradation. Due to the strong coupling of these two artifacts in real imaging processes, existing methods designed for single degradations fail to generalize to such compound scenarios. In this paper, we present the first systematic study on joint removal of moiré patterns and flicker-banding in screen-captured images, and propose a unified restoration framework, named CLEAR. To support this task, we construct a large-scale dataset containing both moiré patterns and flicker-banding, and introduce an ISP-based flicker simulation pipeline to stabilize model training and expand the degradation distribution. Furthermore, we design a frequency-domain decomposition and re-composition module together with a trajectory alignment loss to enhance the modeling of compound artifacts. Extensive experiments demonstrate that the proposed method consistently. outperforms existing image restoration approaches across multiple evaluation metrics, validating its effectiveness in complex real-world scenarios.

</details>


### [180] [Multimodal UNcommonsense: From Odd to Ordinary and Ordinary to Odd](https://arxiv.org/abs/2602.01561)
*Yejin Son,Saejin Kim,Dongjun Min,Younjae Yu*

Main category: cs.CV

TL;DR: 本文介绍了MUN多模态常识推理基准，以及一种提升小模型常识推理能力的R-ICL方法，实验效果显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有多模态AI模型在非常规、反直觉场景下推理能力有限，缺乏对‘反常识’情况的测试工具和改进方法。

Method: 提出MUN数据集，通过将视觉场景与出人意料的自然语言描述配对评测模型推理能力，并提出R-ICL框架，结合多模态检索器MER，实现大模型向小模型的推理能力迁移，无需额外训练。

Result: 实验结果显示，所提R-ICL方法在低频、非常规场景下，相比常规ICL提升了平均8.3%的表现。

Conclusion: MUN数据集和R-ICL方法为多模态模型在真实复杂、文化多样及非典型场景中的泛化与适应性评估与提升提供了新方向。

Abstract: Commonsense reasoning in multimodal contexts remains a foundational challenge in artificial intelligence. We introduce Multimodal UNcommonsense(MUN), a benchmark designed to evaluate models' ability to handle scenarios that deviate from typical visual or contextual expectations. MUN pairs visual scenes with surprising or unlikely outcomes described in natural language, prompting models to either rationalize seemingly odd images using everyday logic or uncover unexpected interpretations in ordinary scenes. To support this task, we propose a retrieval-based in-context learning (R-ICL) framework that transfers reasoning capabilities from larger models to smaller ones without additional training. Leveraging a novel Multimodal Ensemble Retriever (MER), our method identifies semantically relevant exemplars even when image and text pairs are deliberately discordant. Experiments show an average improvement of 8.3% over baseline ICL methods, highlighting the effectiveness of R-ICL in low-frequency, atypical settings. MUN opens new directions for evaluating and improving visual-language models' robustness and adaptability in real-world, culturally diverse, and non-prototypical scenarios.

</details>


### [181] [One-Step Diffusion for Perceptual Image Compression](https://arxiv.org/abs/2602.01570)
*Yiwen Jia,Hao Wei,Yanhui Zhou,Chenyang Ge*

Main category: cs.CV

TL;DR: 本文提出了一种仅需一步扩散过程的图像压缩方法，大幅提升解码速度，同时通过判别器增强感知质量，并实现了与现有扩散方法相当的压缩效果。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的图像压缩方法存在推理延迟高、计算开销大的问题，严重影响实际应用，需有更高效的方案。

Method: 提出一种单步扩散的图像压缩方法，通过仅需一次去噪极大提高解码效率。为提升重建图像的感知质量，引入作用于特征表示（而非原始像素）的判别器以更好捕捉高层次结构和纹理。

Result: 实验证明该方法在压缩效果上可与最新扩散方法媲美，但推理速度提升46倍。

Conclusion: 本文方法显著提升了扩散式图像压缩的实用性，为实际部署提供了新的高效选择。源代码和模型已开源，便于社区验证和应用。

Abstract: Diffusion-based image compression methods have achieved notable progress, delivering high perceptual quality at low bitrates. However, their practical deployment is hindered by significant inference latency and heavy computational overhead, primarily due to the large number of denoising steps required during decoding. To address this problem, we propose a diffusion-based image compression method that requires only a single-step diffusion process, significantly improving inference speed. To enhance the perceptual quality of reconstructed images, we introduce a discriminator that operates on compact feature representations instead of raw pixels, leveraging the fact that features better capture high-level texture and structural details. Experimental results show that our method delivers comparable compression performance while offering a 46$\times$ faster inference speed compared to recent diffusion-based approaches. The source code and models are available at https://github.com/cheesejiang/OSDiff.

</details>


### [182] [SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models](https://arxiv.org/abs/2602.01574)
*Haobo Wang,Weiqi Luo,Xiaojun Jia,Xiaochun Cao*

Main category: cs.CV

TL;DR: 本文提出了SGHA-Attack，一种针对视觉-语言大模型（VLMs）的语义引导分层对齐攻击方法，可有效提升有目标性的转移攻击能力并增强对抗防御的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有对VLMs的有目标转移攻击方法在代理模型特征空间易过拟合，仅关注最终层对齐，忽视了中间语义的协同，导致跨模型攻击转移性较差。

Method: 方法创新点在于：1）基于冻结的文本到图像模型采样，生成和目标提示词相关的多参考视觉锚点，精选最相关Top-K用于加权优化；2）在视觉特征的多个层次、不同空间尺度上实施中间层对齐；3）在中间层跨模态（视觉与文本）空间进行同步，强化早期跨模态信号指导，超越单一最终层特征依赖。

Result: 实验显示，SGHA-Attack在多个开源和商业黑盒VLM上表现出优于现有方法的目标攻击转移能力，并能抵抗预处理和净化类防御。

Conclusion: 引入多锚点和分层对齐机制可有效提升VLMs转移攻击的针对性和鲁棒性，为黑盒场景下攻击与防御提供新思路。

Abstract: Large vision-language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate black-box VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses.

</details>


### [183] [HandMCM: Multi-modal Point Cloud-based Correspondence State Space Model for 3D Hand Pose Estimation](https://arxiv.org/abs/2602.01586)
*Wencan Cheng,Gim Hee Lee*

Main category: cs.CV

TL;DR: 本文提出了一种名为HandMCM的新型3D手势估计方法，通过增强模型对遮挡场景下手部关键点动态拓扑结构的学习，实现对手部三维关键点的精确定位，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 3D手势估计对于增强现实等人机交互应用至关重要，但由于手部自遮挡及与物体交互时的遮挡，使得精确估计一直面临严峻挑战。

Method: 提出了一种基于Mamba状态空间模型的新方法HandMCM，在结构上融合了局部信息注入/过滤模块和关键点对应建模模块。通过引入多模态特征增强输入，提高了模型的健壮性和表达能力。

Result: 在三个基准数据集上的实验结果表明，HandMCM在严重遮挡等复杂场景下，相比当前最优方法具有显著性能提升。

Conclusion: HandMCM方法提升了3D手势估计在复杂遮挡环境下的准确性和可靠性，有望推动其实用化进程。

Abstract: 3D hand pose estimation that involves accurate estimation of 3D human hand keypoint locations is crucial for many human-computer interaction applications such as augmented reality. However, this task poses significant challenges due to self-occlusion of the hands and occlusions caused by interactions with objects. In this paper, we propose HandMCM to address these challenges. Our HandMCM is a novel method based on the powerful state space model (Mamba). By incorporating modules for local information injection/filtering and correspondence modeling, the proposed correspondence Mamba effectively learns the highly dynamic kinematic topology of keypoints across various occlusion scenarios. Moreover, by integrating multi-modal image features, we enhance the robustness and representational capacity of the input, leading to more accurate hand pose estimation. Empirical evaluations on three benchmark datasets demonstrate that our model significantly outperforms current state-of-the-art methods, particularly in challenging scenarios involving severe occlusions. These results highlight the potential of our approach to advance the accuracy and reliability of 3D hand pose estimation in practical applications.

</details>


### [184] [Rethinking Genomic Modeling Through Optical Character Recognition](https://arxiv.org/abs/2602.02014)
*Hongxin Xiang,Pengsen Ma,Yunkang Cao,Di Yu,Haowen Chen,Xinyu Yang,Xiangxiang Zeng*

Main category: cs.CV

TL;DR: OpticalDNA提出了一种将基因组建模转化为类似OCR视觉文档理解任务的新方法，相比传统序列模型大幅提升了效率与精度。


<details>
  <summary>Details</summary>
Motivation: 现有基因组大模型多把DNA作为一维序列，忽略了基因信息本质上稀疏和非连续、结构复杂，导致无效计算和上下文压缩困难。

Method: 提出OpticalDNA框架，将DNA渲染为结构化视觉布局，训练包含视觉DNA编码器和文档解码器的视觉-语言模型，把基因组任务转化为视觉识别任务，实现视觉Token级高保真压缩。

Result: 在多项基因组任务基准上，OpticalDNA大幅超越当前主流方法；处理最长达45万碱基序列时，用极少Token数取得最优表现，所需可训练参数远低于其他模型。

Conclusion: 该方法开启了基因组视觉表征与理解新范式，实现高效、精准的长序列表征。

Abstract: Recent genomic foundation models largely adopt large language model architectures that treat DNA as a one-dimensional token sequence. However, exhaustive sequential reading is structurally misaligned with sparse and discontinuous genomic semantics, leading to wasted computation on low-information background and preventing understanding-driven compression for long contexts. Here, we present OpticalDNA, a vision-based framework that reframes genomic modeling as Optical Character Recognition (OCR)-style document understanding. OpticalDNA renders DNA into structured visual layouts and trains an OCR-capable vision--language model with a \emph{visual DNA encoder} and a \emph{document decoder}, where the encoder produces compact, reconstructible visual tokens for high-fidelity compression. Building on this representation, OpticalDNA defines prompt-conditioned objectives over core genomic primitives-reading, region grounding, subsequence retrieval, and masked span completion-thereby learning layout-aware DNA representations that retain fine-grained genomic information under a reduced effective token budget. Across diverse genomic benchmarks, OpticalDNA consistently outperforms recent baselines; on sequences up to 450k bases, it achieves the best overall performance with nearly $20\times$ fewer effective tokens, and surpasses models with up to $985\times$ more activated parameters while tuning only 256k \emph{trainable} parameters.

</details>


### [185] [Know Your Step: Faster and Better Alignment for Flow Matching Models via Step-aware Advantages](https://arxiv.org/abs/2602.01591)
*Zhixiong Yue,Zixuan Ni,Feiyang Ye,Jinshan Zhang,Sheng Shen,Zhenpeng Mi*

Main category: cs.CV

TL;DR: 提出了TAFS GRPO新框架，解决流匹配模型中基于RL方法在文本到图像生成任务里步骤少、对人类偏好对齐不佳的问题。通过自适应噪声注入和奖励函数创新，实现更高效的偏好对齐和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的流匹配模型在文本生成图像时需要许多去噪步骤，且奖励信号稀疏且不精确，导致生成结果与人类偏好对齐效果不佳。

Method: 提出TAFS GRPO框架：迭代性地对一步采样结果注入自适应的时间噪声，反复退火采样结果，既增加采样过程的随机性，又保留每张图片的语义完整性。同时，引入步长感知的优势集成机制结合了GRPO，实现无需奖励函数可微，又能为策略优化提供密集且有针对性的奖励信号。

Result: TAFS GRPO在文生图任务中，实现了高效的few-step生成，并显著提升了生成图片对人类偏好的对齐度。实验结果显示该方法性能优异。

Conclusion: TAFS GRPO为基于流匹配的文本到图像生成任务带来了更高效的人类偏好对齐方法，并有实际运用潜力。代码和模型即将开源，便于后续研究。

Abstract: Recent advances in flow matching models, particularly with reinforcement learning (RL), have significantly enhanced human preference alignment in few step text to image generators. However, existing RL based approaches for flow matching models typically rely on numerous denoising steps, while suffering from sparse and imprecise reward signals that often lead to suboptimal alignment. To address these limitations, we propose Temperature Annealed Few step Sampling with Group Relative Policy Optimization (TAFS GRPO), a novel framework for training flow matching text to image models into efficient few step generators well aligned with human preferences. Our method iteratively injects adaptive temporal noise onto the results of one step samples. By repeatedly annealing the model's sampled outputs, it introduces stochasticity into the sampling process while preserving the semantic integrity of each generated image. Moreover, its step aware advantage integration mechanism combines the GRPO to avoid the need for the differentiable of reward function and provide dense and step specific rewards for stable policy optimization. Extensive experiments demonstrate that TAFS GRPO achieves strong performance in few step text to image generation and significantly improves the alignment of generated images with human preferences. The code and models of this work will be available to facilitate further research.

</details>


### [186] [Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models](https://arxiv.org/abs/2602.02185)
*Yu Zeng,Wenxuan Huang,Zhen Fang,Shuang Chen,Yufan Shen,Yishuo Cai,Xiaoman Wang,Zhenfei Yin,Lin Chen,Zehui Chen,Shiting Huang,Yiming Zhao,Yao Hu,Philip Torr,Wanli Ouyang,Shaosheng Cao*

Main category: cs.CV

TL;DR: 该论文提出了VDR-Bench基准，用以更真实和严格地评测多模态大模型在基于视觉和文本的检索与问答中的能力，并提出了多轮裁剪搜索策略提升视觉检索表现。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大模型（MLLMs）在视觉问答（VQA）和与搜索引擎结合的复杂视觉-文本信息检索方面取得进展，但现有评测基准存在两个主要问题：一是缺乏以视觉检索为核心的设问，二是评测场景过于理想化，不能真实反映实际检索难度。为解决这两个问题，作者设计了新的评测方法和数据集。

Method: 作者提出VDR-Bench基准数据集，包含2,000个经多阶段策划和专家评审的VQA实例，旨在体现真实世界下的复杂多模态检索需求，并用多轮裁剪-检索方法提升模型的视觉检索表现。

Result: 通过应用VDR-Bench和多轮裁剪搜索的方法，实验验证多模态大模型的视觉检索能力得到了有效提升，体现了新数据集和方法的实用价值。

Conclusion: 本工作为未来多模态深度检索系统的构建提供了更贴合实际的评测标准和方法，同时对模型能力提升具有指导意义。代码及数据集将公开，便于学界复现和后续研究。

Abstract: Multimodal Large Language Models (MLLMs) have advanced VQA and now support Vision-DeepResearch systems that use search engines for complex visual-textual fact-finding. However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations. First, existing benchmarks are not visual search-centric: answers that should require visual search are often leaked through cross-textual cues in the text questions or can be inferred from the prior world knowledge in current MLLMs. Second, overly idealized evaluation scenario: On the image-search side, the required information can often be obtained via near-exact matching against the full image, while the text-search side is overly direct and insufficiently challenging. To address these issues, we construct the Vision-DeepResearch benchmark (VDR-Bench) comprising 2,000 VQA instances. All questions are created via a careful, multi-stage curation pipeline and rigorous expert review, designed to assess the behavior of Vision-DeepResearch systems under realistic real-world conditions. Moreover, to address the insufficient visual retrieval capabilities of current MLLMs, we propose a simple multi-round cropped-search workflow. This strategy is shown to effectively improve model performance in realistic visual retrieval scenarios. Overall, our results provide practical guidance for the design of future multimodal deep-research systems. The code will be released in https://github.com/Osilly/Vision-DeepResearch.

</details>


### [187] [Samba+: General and Accurate Salient Object Detection via A More Unified Mamba-based Framework](https://arxiv.org/abs/2602.01593)
*Wenzhuo Zhao,Keren Fu,Jiahao He,Xiaohong Liu,Qijun Zhao,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出了一种基于Mamba状态空间模型的显著性目标检测架构Samba以及多任务通用版本Samba+，在六类SOD任务和22个数据集上均取得优异结果，且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 现有显著性目标检测方法受限于CNN感受野有限及Transformer计算复杂度高。而新兴Mamba状态空间模型可兼顾全局感受野和计算效率，驱动了本工作的提出。

Method: 1）创新性地将Mamba用于多个SOD任务；2）提出SGMB模块，其中空间邻域扫描算法保证显著区域空间连续性；3）提出CAU促进分层特征的上下文对齐聚合；4）Samba+通过多任务联合训练实现跨模态统一，并引入HGA模块实现自适应跨模态融合，MACL策略缓解模态冲突与灾难性遗忘。

Result: Samba在六类显著性检测任务、22个数据集上单独均超过现有方法且计算成本更低，Samba+作为单模型在所有数据集及任务上表现进一步提升。

Conclusion: Samba及其多任务扩展Samba+模型在SOD领域实现了更高精度、更优计算效率和更好跨模态泛化能力，具有广阔应用潜力。

Abstract: Existing salient object detection (SOD) models are generally constrained by the limited receptive fields of convolutional neural networks (CNNs) and quadratic computational complexity of Transformers. Recently, the emerging state-space model, namely Mamba, has shown great potential in balancing global receptive fields and computational efficiency. As a solution, we propose Saliency Mamba (Samba), a pure Mamba-based architecture that flexibly handles various distinct SOD tasks, including RGB/RGB-D/RGB-T SOD, video SOD (VSOD), RGB-D VSOD, and visible-depth-thermal SOD. Specifically, we rethink the scanning strategy of Mamba for SOD, and introduce a saliency-guided Mamba block (SGMB) that features a spatial neighborhood scanning (SNS) algorithm to preserve the spatial continuity of salient regions. A context-aware upsampling (CAU) method is also proposed to promote hierarchical feature alignment and aggregation by modeling contextual dependencies. As one step further, to avoid the "task-specific" problem as in previous SOD solutions, we develop Samba+, which is empowered by training Samba in a multi-task joint manner, leading to a more unified and versatile model. Two crucial components that collaboratively tackle challenges encountered in input of arbitrary modalities and continual adaptation are investigated. Specifically, a hub-and-spoke graph attention (HGA) module facilitates adaptive cross-modal interactive fusion, and a modality-anchored continual learning (MACL) strategy alleviates inter-modal conflicts together with catastrophic forgetting. Extensive experiments demonstrate that Samba individually outperforms existing methods across six SOD tasks on 22 datasets with lower computational cost, whereas Samba+ achieves even superior results on these tasks and datasets by using a single trained versatile model. Additional results further demonstrate the potential of our Samba framework.

</details>


### [188] [UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception](https://arxiv.org/abs/2602.01594)
*Wenzhuo Liu,Qiannan Guo,Zhen Wang,Wenshuo Wang,Lei Yang,Yicheng Qiao,Lening Wang,Zhiwei Li,Chen Lv,Shanghang Zhang,Junqiang Xi,Huaping Liu*

Main category: cs.CV

TL;DR: 该研究提出了一种统一且通用的多模态多任务学习框架（UV-M3TL），能同时识别驾驶员行为、情绪、车辆行为和交通环境，并在多个数据集上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有高级驾驶辅助系统（ADAS）需要同时理解驾驶员行为及环境，然而多任务联合学习常会导致任务间负迁移，影响系统性能。作者动机是在提升多任务性能的同时，缓解任务间的冲突。

Method: 提出UV-M3TL框架，包含双分支空间通道多模态嵌入（DB-SCME）和自适应特征解耦多任务损失（AFD-Loss）。DB-SCME通过双分支结构分别建模共享特征与特定任务特征，增强跨任务知识传递并减少冲突。AFD-Loss通过引入动态加权及特征解耦，促进多任务优化稳定并学习差异化表达。

Result: 在AIDE多任务数据集上，UV-M3TL在驾驶员行为、情感、车辆行为和交通环境四项任务上均取得SOTA性能；在BDD100K、CityScapes、NYUD-v2、PASCAL-Context等公开多任务感知基准上也表现优异，在大多数任务获得SOTA。

Conclusion: UV-M3TL框架能够有效缓解多任务学习中的负迁移问题，在多个多任务感知场景中展现了强通用性和领先性能，有望提升ADAS等实际应用系统的表现。

Abstract: Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks.

</details>


### [189] [Token Pruning for In-Context Generation in Diffusion Transformers](https://arxiv.org/abs/2602.01609)
*Junqing Lin,Xingyu Zheng,Pei Cheng,Bin Fu,Jingwei Sun,Guangzhong Sun*

Main category: cs.CV

TL;DR: 这篇论文提出了ToPi，一种无需训练的Diffusion Transformer（DiT）In-context生成中高效的Token剪枝方法，显著加速推理速度且保证生成图像质量。


<details>
  <summary>Details</summary>
Motivation: In-context生成需要拼接参考图像和目标，导致序列长度显著增加，带来计算瓶颈。而现有的Token减少方法是为文本到图像合成设计的，未考虑上下文和目标Token在空间、时间和功能上的不对称性，对本任务效果有限。

Method: 提出了ToPi框架：（1）利用离线校准敏感性分析，寻找关键的注意力层，作为剪枝冗余信息的依据；（2）基于这些注意力层，定义了新的影响力度量，评估每个上下文Token对生成结果的贡献，并据此有选择地剪枝Token；（3）提出了时序更新策略，动态适应扩散过程中的Token重要性变化。整个方法无需额外训练。

Result: 实验证明ToPi适用于多种复杂的图像生成任务，在不损失结构保真和视觉一致性的前提下，推理速度可提升30%以上。

Conclusion: ToPi有效突破了现有DiT In-context生成中的计算瓶颈，以灵活、训练无关的方式实现高效Token剪枝，为可控的图像生成提供了实用的方法。

Abstract: In-context generation significantly enhances Diffusion Transformers (DiTs) by enabling controllable image-to-image generation through reference examples. However, the resulting input concatenation drastically increases sequence length, creating a substantial computational bottleneck. Existing token reduction techniques, primarily tailored for text-to-image synthesis, fall short in this paradigm as they apply uniform reduction strategies, overlooking the inherent role asymmetry between reference contexts and target latents across spatial, temporal, and functional dimensions. To bridge this gap, we introduce ToPi, a training-free token pruning framework tailored for in-context generation in DiTs. Specifically, ToPi utilizes offline calibration-driven sensitivity analysis to identify pivotal attention layers, serving as a robust proxy for redundancy estimation. Leveraging these layers, we derive a novel influence metric to quantify the contribution of each context token for selective pruning, coupled with a temporal update strategy that adapts to the evolving diffusion trajectory. Empirical evaluations demonstrate that ToPi can achieve over 30\% speedup in inference while maintaining structural fidelity and visual consistency across complex image generation tasks.

</details>


### [190] [Omni-Judge: Can Omni-LLMs Serve as Human-Aligned Judges for Text-Conditioned Audio-Video Generation?](https://arxiv.org/abs/2602.01623)
*Susan Liang,Chao Huang,Filippos Bellos,Yolo Yunlong Tang,Qianxiang Shen,Jing Bi,Luchuan Song,Zeliang Zhang,Jason Corso,Chenliang Xu*

Main category: cs.CV

TL;DR: 本文提出并验证了Omni-Judge系统，评估omni-LLM是否能作为多模态生成任务的人类对齐评判工具，展现了其在语义一致性评价的优势。


<details>
  <summary>Details</summary>
Motivation: 随着文本到视频生成技术的发展，当前模型可生成高保真且带同步音频的视频，但如何有效评估这些三模态（文本、音频、视频）输出仍未解决。人工评价可靠但难以扩展，现有自动化指标对复杂场景和多模态解释性有限。因此，急需更智能、可解释且可靠的自动化评价方法。

Method: 作者设计了Omni-Judge系统，利用新兴的omni-LLMs（全模态大模型），结合九项感知与对齐指标，系统性评测其对文本驱动音视频生成的评价表现，并与传统自动指标对比。特别关注于音频-文本、视频-文本、音视频-文本三者的语义一致性。

Result: Omni-Judge在多项评价指标上相关性与传统指标相当，且在复杂语义任务（如三模态一致性）上表现优异。但在高帧率视频质量、视听同步等纯感知类指标上表现不足，归因于omni-LLM对时序信息把控有限。同时能输出解释性强的评语，利于实际应用。

Conclusion: Omni-LLM作为统一多模态生成任务评估器展现出巨大潜力，已具备较好的人类一致性和语义解释能力，但时序分辨率等局限性仍需改进。

Abstract: State-of-the-art text-to-video generation models such as Sora 2 and Veo 3 can now produce high-fidelity videos with synchronized audio directly from a textual prompt, marking a new milestone in multi-modal generation. However, evaluating such tri-modal outputs remains an unsolved challenge. Human evaluation is reliable but costly and difficult to scale, while traditional automatic metrics, such as FVD, CLAP, and ViCLIP, focus on isolated modality pairs, struggle with complex prompts, and provide limited interpretability. Omni-modal large language models (omni-LLMs) present a promising alternative: they naturally process audio, video, and text, support rich reasoning, and offer interpretable chain-of-thought feedback. Driven by this, we introduce Omni-Judge, a study assessing whether omni-LLMs can serve as human-aligned judges for text-conditioned audio-video generation. Across nine perceptual and alignment metrics, Omni-Judge achieves correlation comparable to traditional metrics and excels on semantically demanding tasks such as audio-text alignment, video-text alignment, and audio-video-text coherence. It underperforms on high-FPS perceptual metrics, including video quality and audio-video synchronization, due to limited temporal resolution. Omni-Judge provides interpretable explanations that expose semantic or physical inconsistencies, enabling practical downstream uses such as feedback-based refinement. Our findings highlight both the potential and current limitations of omni-LLMs as unified evaluators for multi-modal generation.

</details>


### [191] [PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards](https://arxiv.org/abs/2602.01624)
*Minh-Quan Le,Gaurav Mittal,Cheng Zhao,David Gu,Dimitris Samaras,Mei Chen*

Main category: cs.CV

TL;DR: 本文提出了一种无需人工标注的文本生成视频（T2V）后训练算法PISCES，利用新颖的双重最优传输（OT）奖励机制提升生成视频的质量与语义对齐能力，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有T2V生成的奖励后训练方法依赖大量人工偏好标注或用预训练视觉-语言模型得到的有偏矩阵，导致方法扩展性差且监督效果有限。作者希望实现无需标注且对齐人类判断的奖励机制，以提升生成视频质量与一致性。

Method: PISCES利用双重最优传输（OT）奖励模块，无需人工标注。一方面，通过分布式OT对齐整体视觉质量和时序一致性（Distributional OT-aligned Quality Reward），另一方面，通过离散token级OT对齐，强化语义与时空对应关系（Discrete Token-level OT-aligned Semantic Reward）。这种奖励机制适配多种优化范式，如反向传播和强化学习微调。

Result: 在短视频和长视频生成任务上，PISCES在VBench等基准的质量与语义分数上都优于已有人工标注和无标注方法。人类偏好实验也进一步证明了其生成效果的优越性。

Conclusion: PISCES通过双重OT奖励机制实现了无需标注的生成后训练，并显著提升了文本到视频生成的质量和语义一致性，为相关领域提供了新的技术路线。

Abstract: Text-to-video (T2V) generation aims to synthesize videos with high visual quality and temporal consistency that are semantically aligned with input text. Reward-based post-training has emerged as a promising direction to improve the quality and semantic alignment of generated videos. However, recent methods either rely on large-scale human preference annotations or operate on misaligned embeddings from pre-trained vision-language models, leading to limited scalability or suboptimal supervision. We present $\texttt{PISCES}$, an annotation-free post-training algorithm that addresses these limitations via a novel Dual Optimal Transport (OT)-aligned Rewards module. To align reward signals with human judgment, $\texttt{PISCES}$ uses OT to bridge text and video embeddings at both distributional and discrete token levels, enabling reward supervision to fulfill two objectives: (i) a Distributional OT-aligned Quality Reward that captures overall visual quality and temporal coherence; and (ii) a Discrete Token-level OT-aligned Semantic Reward that enforces semantic, spatio-temporal correspondence between text and video tokens. To our knowledge, $\texttt{PISCES}$ is the first to improve annotation-free reward supervision in generative post-training through the lens of OT. Experiments on both short- and long-video generation show that $\texttt{PISCES}$ outperforms both annotation-based and annotation-free methods on VBench across Quality and Semantic scores, with human preference studies further validating its effectiveness. We show that the Dual OT-aligned Rewards module is compatible with multiple optimization paradigms, including direct backpropagation and reinforcement learning fine-tuning.

</details>


### [192] [Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks](https://arxiv.org/abs/2602.01630)
*Bohan Zeng,Kaixin Zhu,Daili Hua,Bozhou Li,Chengzhuo Tong,Yuran Wang,Xinyi Huang,Yifan Dai,Zixiang Zhang,Yifan Yang,Zhou Liu,Hao Liang,Xiaochen Ma,Ruichuan An,Tianyi Bai,Hongcheng Gao,Junbo Niu,Yang Shi,Xinlong Chen,Yue Ding,Minglei Shi,Kai Zeng,Yiwen Tang,Yuanxing Zhang,Pengfei Wan,Xintao Wang,Wentao Zhang*

Main category: cs.CV

TL;DR: 本文对当前世界模型的研究现状进行了分析，指出现有方法过于碎片化，缺乏统一的定义和框架，并提出了一个统一的世界模型设计规范，以推动更加整体和系统的世界建模。


<details>
  <summary>Details</summary>
Motivation: 当前世界模型研究主要在特定任务（如视觉预测、三维估计、符号理解等）注入物理或知识，但这些方法缺乏系统性和整体性，难以实现面向复杂环境的全面理解。

Method: 作者梳理并分析了现有世界模型研究的局限，提出了一套统一的设计规范，强调世界模型需要集成交互、感知、符号推理和空间表征等能力，成为有原则性的系统框架，而非功能杂糅的集合。

Result: 本文理论性地提出了世界模型应该覆盖的核心要素和统一设计要求，为今后的研究建模提供了结构化的方向指引。

Conclusion: 文章认为统一且规范的设计框架是实现鲁棒且通用世界模型的关键，为后续更系统和完善的世界建模研究奠定基础。

Abstract: World models have emerged as a critical frontier in AI research, aiming to enhance large models by infusing them with physical dynamics and world knowledge. The core objective is to enable agents to understand, predict, and interact with complex environments. However, current research landscape remains fragmented, with approaches predominantly focused on injecting world knowledge into isolated tasks, such as visual prediction, 3D estimation, or symbol grounding, rather than establishing a unified definition or framework. While these task-specific integrations yield performance gains, they often lack the systematic coherence required for holistic world understanding. In this paper, we analyze the limitations of such fragmented approaches and propose a unified design specification for world models. We suggest that a robust world model should not be a loose collection of capabilities but a normative framework that integrally incorporates interaction, perception, symbolic reasoning, and spatial representation. This work aims to provide a structured perspective to guide future research toward more general, robust, and principled models of the world.

</details>


### [193] [Federated Vision Transformer with Adaptive Focal Loss for Medical Image Classification](https://arxiv.org/abs/2602.01633)
*Xinyuan Zhao,Yihang Wu,Ahmad Chaddad,Tareef Daqqaq,Reem Kateb*

Main category: cs.CV

TL;DR: 本文提出了一种结合动态自适应焦点损失（DAFL）与客户端感知聚合策略的联邦学习（FL）框架，以提升不同客户数据不均衡情况下面部图像分类的表现。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型（如ViT）需要大规模数据。但数据隐私法规限制了获取，尤其是医疗影像。联邦学习虽可避免数据交换，但异构性与类别不均衡影响模型泛化能力。

Method: 提出FL框架，采用动态自适应焦点损失（DAFL）处理类别不均衡——通过客户数据分布动态调整类别权重，确保少数类不被忽略。同时，引入基于数据规模与特性的加权聚合策略，适应客户端异构性。

Result: 在ISIC、Ocular Disease及RSNA-ICH三个公开数据集上，框架在大多数情况下优于DenseNet121、ResNet50、ViT等多个主流方法，准确率提高0.98%~41.69%。消融实验进一步验证了新损失函数和聚合策略的有效性。

Conclusion: 该框架有效提升了联邦学习中不同客户端和类别不均衡情形下的分类性能，对未来基于隐私保护的医学图像分析具有较大借鉴价值。

Abstract: While deep learning models like Vision Transformer (ViT) have achieved significant advances, they typically require large datasets. With data privacy regulations, access to many original datasets is restricted, especially medical images. Federated learning (FL) addresses this challenge by enabling global model aggregation without data exchange. However, the heterogeneity of the data and the class imbalance that exist in local clients pose challenges for the generalization of the model. This study proposes a FL framework leveraging a dynamic adaptive focal loss (DAFL) and a client-aware aggregation strategy for local training. Specifically, we design a dynamic class imbalance coefficient that adjusts based on each client's sample distribution and class data distribution, ensuring minority classes receive sufficient attention and preventing sparse data from being ignored. To address client heterogeneity, a weighted aggregation strategy is adopted, which adapts to data size and characteristics to better capture inter-client variations. The classification results on three public datasets (ISIC, Ocular Disease and RSNA-ICH) show that the proposed framework outperforms DenseNet121, ResNet50, ViT-S/16, ViT-L/32, FedCLIP, Swin Transformer, CoAtNet, and MixNet in most cases, with accuracy improvements ranging from 0.98\% to 41.69\%. Ablation studies on the imbalanced ISIC dataset validate the effectiveness of the proposed loss function and aggregation strategy compared to traditional loss functions and other FL approaches. The codes can be found at: https://github.com/AIPMLab/ViT-FLDAF.

</details>


### [194] [ReCALL: Recalibrating Capability Degradation for MLLM-based Composed Image Retrieval](https://arxiv.org/abs/2602.01639)
*Tianyu Yang,ChenWei He,Xiangzhao Hao,Tianyue Wang,Jiarui Guo,Haiyun Guo,Leigang Qu,Jinqiao Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 本文提出了一个叫ReCALL的新框架，用以解决生成式多模态大模型（MLLM）在适配为判别式检索模型时出现的能力退化问题，显著提升了由图片和文本组合查询的图像检索任务（CIR）性能。


<details>
  <summary>Details</summary>
Motivation: 当前组合式图像检索任务需要模型理解和融合图片与文本两种模态的信息。传统双塔模型难以完成复杂的模态合成推理；而直接将生成式多模态大模型用于判别式检索则引发能力退化（精细推理能力下降），因此亟需新方法应对这一冲突。

Method: 提出了ReCALL框架，包含“诊断-生成-精炼”三步：（1）诊断retriever模型的认知盲点，通过自引导的实例挖掘发现模型弱点；（2）利用基础MLLM链式提示生成矫正指导和训练三元组，并借助VQA进行一致性筛选；（3）通过分组对比训练法，用生成的三元组持续训练retriever，使其重获精细推理与判别空间能力。该方法与模型架构无关。

Result: 在CIRR和FashionIQ基准上，ReCALL持续修正判别模型的能力退化问题，并取得了新的最优结果。

Conclusion: ReCALL能够有效矫正MLLM在检索适配中的能力损失，在组合式图像检索任务取得显著性能提升，方法具有通用性和实用价值。

Abstract: Composed Image Retrieval (CIR) aims to retrieve target images based on a hybrid query comprising a reference image and a modification text. Early dual-tower Vision-Language Models (VLMs) struggle with cross-modality compositional reasoning required for this task. Recently, adapting generative Multimodal Large Language Models (MLLMs) for retrieval offers a promising direction. However, we identify that this adaptation strategy overlooks a fundamental issue: adapting a generative MLLM into a single-embedding discriminative retriever triggers a paradigm conflict, which leads to Capability Degradation - the deterioration of native fine-grained reasoning after retrieval adaptation. To address this challenge, we propose ReCALL (Recalibrating Capability Degradation), a model-agnostic framework that follows a diagnose-generate-refine pipeline: Firstly, we diagnose cognitive blind spots of the retriever via self-guided informative instance mining. Next, we generate corrective instructions and triplets by CoT prompting the foundation MLLM and conduct quality control with VQA-based consistency filtering. Finally, we refine the retriever through continual training on these triplets with a grouped contrastive scheme, thereby internalizing fine-grained visual-semantic distinctions and realigning the discriminative embedding space of retriever with intrinsic compositional reasoning within the MLLM. Extensive experiments on CIRR and FashionIQ show that ReCALL consistently recalibrates degraded capabilities and achieves state-of-the-art performance. Code will be released soon.

</details>


### [195] [Contribution-aware Token Compression for Efficient Video Understanding via Reinforcement Learning](https://arxiv.org/abs/2602.01649)
*Yinchao Ma,Qiang Zhou,Zhibin Wang,Xianing Chen,Hanqing Yang,Jun Song,Bo Zheng*

Main category: cs.CV

TL;DR: 该论文提出了一种新的视频大语言模型（Video LLM）token压缩算法CaCoVID，显著提高推理效率，又保证预测准确性。


<details>
  <summary>Details</summary>
Motivation: 当前视频大语言模型在视频理解任务中表现出色，但由于大量冗余视频token，推理时计算开销大，实用性受限。虽然有算法通过保留高attention分数的token尝试减少计算量，但attention分数与预测正确性的直接关联存在疑问。作者希望找到更科学的压缩方式，无损或最小损失下减少token数。

Method: 提出CaCoVID算法，首先利用强化学习框架训练策略网络，主动选择最有贡献的token组合。该方法重点关注token对最终正确预测的实际贡献，而非简单依赖attention分数。其次，引入组合式策略优化与在线采样，极大减少token组合搜索空间，加快优化收敛。

Result: 在多个视频理解基准测试上，CaCoVID显著减少token数量和计算量的同时，保持甚至提升了模型的预测准确率。

Conclusion: CaCoVID以实际贡献为基础优化token选择策略，极大提升视频大语言模型的压缩效率和实用性。实验验证了方法的有效性，相关代码即将开源。

Abstract: Video large language models have demonstrated remarkable capabilities in video understanding tasks. However, the redundancy of video tokens introduces significant computational overhead during inference, limiting their practical deployment. Many compression algorithms are proposed to prioritize retaining features with the highest attention scores to minimize perturbations in attention computations. However, the correlation between attention scores and their actual contribution to correct answers remains ambiguous. To address the above limitation, we propose a novel \textbf{C}ontribution-\textbf{a}ware token \textbf{Co}mpression algorithm for \textbf{VID}eo understanding (\textbf{CaCoVID}) that explicitly optimizes the token selection policy based on the contribution of tokens to correct predictions. First, we introduce a reinforcement learning-based framework that optimizes a policy network to select video token combinations with the greatest contribution to correct predictions. This paradigm shifts the focus from passive token preservation to active discovery of optimal compressed token combinations. Secondly, we propose a combinatorial policy optimization algorithm with online combination space sampling, which dramatically reduces the exploration space for video token combinations and accelerates the convergence speed of policy optimization. Extensive experiments on diverse video understanding benchmarks demonstrate the effectiveness of CaCoVID. Codes will be released.

</details>


### [196] [From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction](https://arxiv.org/abs/2602.01661)
*Xingyu Miao,Junting Dong,Qin Zhao,Yuhang Yang,Junhao Chen,Yang Long*

Main category: cs.CV

TL;DR: 本文提出了一种针对视频序列中以人为中心的时序一致性密集预测的新方法，通过合成高真实性人类视频数据并引入时序标注，提高了模型的空间与时间一致性表现。


<details>
  <summary>Details</summary>
Motivation: 现有模型虽能在单帧上实现高精度，但在运动、遮挡和光照变化下时序一致性差，且缺乏结合多密集任务与时序数据的标注资源，限制了其在真实视频中的泛化能力。

Method: 开发了可扩展的合成数据管道，生成具有像素级准确标签的人体帧与运动对齐序列，并提供空间与时序层级的监督。提出基于ViT的统一密集预测器，结合CSE嵌入注入人体几何先验，并通过特征融合后的通道重加权机制提升几何特征可靠性。模型训练采用两阶段策略，先静态预训练获得空间表征，再通过动态图序列监督优化时序一致性。

Result: 在THuman2.1和Hi4D基准数据集上，方法取得了最新最优结果，并在真实野外视频上展现出良好的泛化能力。

Conclusion: 引入合成时序监督与人体几何先验的统一方法，显著提升了密集视频预测任务的空间和时序一致性，为以人为中心的多任务视频理解提供了新思路。

Abstract: In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos.

</details>


### [197] [Moonworks Lunara Aesthetic II: An Image Variation Dataset](https://arxiv.org/abs/2602.01666)
*Yan Wang,Partho Hassan,Samiha Sadeka,Nada Soliman,M M Sayeef Abdullah,Sabit Hassan*

Main category: cs.CV

TL;DR: 本文介绍了Lunara Aesthetic II数据集，该数据集主要用于图像生成和编辑系统的上下文一致性评估和学习，具有高审美质量及身份保持特性。


<details>
  <summary>Details</summary>
Motivation: 目前在图像生成和编辑领域，对于上下文变化（如光照、色调、构图等）下如何保持主体身份不变的能力评估和训练手段有限，同时公众可用、质量高、伦理来源的数据集稀缺。作者希望提供一个具备身份保持且支持多样上下文变化的高审美数据集，以推动领域发展。

Method: Lunara Aesthetic II由Moonworks创作的原创艺术和照片生成，通过对原始图像应用一系列上下文转换（如天气、视角、色调、氛围等），得到2,854对锚点-变体图像配对，在保持主体身份不变的条件下，实现丰富的上下文变化，数据集免费线上公开。

Result: 实验结果显示，该数据集无论在身份稳定性、目标属性变化实现效果还是审美表现方面均优于大规模网络抓取数据集。

Conclusion: Lunara Aesthetic II数据集为评测和训练图像生成、编辑模型的上下文泛化能力、身份保持能力及编辑鲁棒性提供了有效、公开且高质量的资源，支持可解释和关系监督，有助于相关领域的进一步研究和发展。

Abstract: We introduce Lunara Aesthetic II, a publicly released, ethically sourced image dataset designed to support controlled evaluation and learning of contextual consistency in modern image generation and editing systems. The dataset comprises 2,854 anchor-linked variation pairs derived from original art and photographs created by Moonworks. Each variation pair applies contextual transformations, such as illumination, weather, viewpoint, scene composition, color tone, or mood; while preserving a stable underlying identity. Lunara Aesthetic II operationalizes identity-preserving contextual variation as a supervision signal while also retaining Lunara's signature high aesthetic scores. Results show high identity stability, strong target attribute realization, and a robust aesthetic profile that exceeds large-scale web datasets. Released under the Apache 2.0 license, Lunara Aesthetic II is intended for benchmarking, fine-tuning, and analysis of contextual generalization, identity preservation, and edit robustness in image generation and image-to-image systems with interpretable, relational supervision. The dataset is publicly available at: https://huggingface.co/datasets/moonworks/lunara-aesthetic-image-variations.

</details>


### [198] [VRGaussianAvatar: Integrating 3D Gaussian Avatars into VR](https://arxiv.org/abs/2602.01674)
*Hail Song,Boram Yoon,Seokhwan Yang,Seoyoung Kang,Hyunjeong Kim,Henning Metzmacher,Woontack Woo*

Main category: cs.CV

TL;DR: 本文提出了VRGaussianAvatar系统，能仅通过头戴显示器追踪信号实现实时全身3D高斯头像虚拟现实重建，具备高性能与高真实感。


<details>
  <summary>Details</summary>
Motivation: 目前虚拟现实中的全身化身大多依赖多摄像头或传感器，而仅凭HMD信号实现高质量实时3D重建仍有挑战，特别是在渲染效率和表现真实感方面。

Method: 该系统采用前后端并行管线：前端通过逆向运动学估算全身姿态并传送到后端，后端用双目渲染技术将单张图片重建的3D高斯头像进行高效渲染。引入了Binocular Batching技术，以单次批处理同时计算左右眼视图，减少冗余并优化VR显示效率。

Result: 通过性能测试和主观用户研究，结果显示该系统能持续支持交互式VR性能，在外观相似性、化身感和可信度上优于图像及视频基础的mesh形象对比方案。

Conclusion: VRGaussianAvatar实现了基于HMD追踪信号的高效、真实、体验优秀的3DGS全身虚拟形象方案，为虚拟现实中的实时全身化身渲染提供了新的技术路径。

Abstract: We present VRGaussianAvatar, an integrated system that enables real-time full-body 3D Gaussian Splatting (3DGS) avatars in virtual reality using only head-mounted display (HMD) tracking signals. The system adopts a parallel pipeline with a VR Frontend and a GA Backend. The VR Frontend uses inverse kinematics to estimate full-body pose and streams the resulting pose along with stereo camera parameters to the backend. The GA Backend stereoscopically renders a 3DGS avatar reconstructed from a single image. To improve stereo rendering efficiency, we introduce Binocular Batching, which jointly processes left and right eye views in a single batched pass to reduce redundant computation and support high-resolution VR displays. We evaluate VRGaussianAvatar with quantitative performance tests and a within-subject user study against image- and video-based mesh avatar baselines. Results show that VRGaussianAvatar sustains interactive VR performance and yields higher perceived appearance similarity, embodiment, and plausibility. Project page and source code are available at https://vrgaussianavatar.github.io.

</details>


### [199] [SMTrack: State-Aware Mamba for Efficient Temporal Modeling in Visual Tracking](https://arxiv.org/abs/2602.01677)
*Yinchao Ma,Dengqing Yang,Zhangyu He,Wenfei Yang,Tianzhu Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的、低算力需求的视觉目标追踪模型（SMTrack），能够高效且有效地建模长时序的时空依赖。


<details>
  <summary>Details</summary>
Motivation: 在动态场景中，现有视觉追踪方法难以高效建模长时性依赖，引入时序信息通常需复杂模块或高计算消耗。作者旨在解决这一问题，提升追踪的鲁棒性并降低计算代价。

Method: 作者借鉴状态空间模型，提出State-aware Mamba Tracker（SMTrack），其中应用了一种新的选择性状态感知空间模型，并通过状态传播和更新实现帧间高效长距离交互，训练期间计算复杂度为线性。

Result: 大量实验结果表明，SMTrack在保持低计算消耗的同时，取得了有前景的跟踪性能。

Conclusion: SMTrack为视觉追踪中的时序建模提供了简洁、高效的新范式，有效兼顾了性能与算力开销。

Abstract: Visual tracking aims to automatically estimate the state of a target object in a video sequence, which is challenging especially in dynamic scenarios. Thus, numerous methods are proposed to introduce temporal cues to enhance tracking robustness. However, conventional CNN and Transformer architectures exhibit inherent limitations in modeling long-range temporal dependencies in visual tracking, often necessitating either complex customized modules or substantial computational costs to integrate temporal cues. Inspired by the success of the state space model, we propose a novel temporal modeling paradigm for visual tracking, termed State-aware Mamba Tracker (SMTrack), providing a neat pipeline for training and tracking without needing customized modules or substantial computational costs to build long-range temporal dependencies. It enjoys several merits. First, we propose a novel selective state-aware space model with state-wise parameters to capture more diverse temporal cues for robust tracking. Second, SMTrack facilitates long-range temporal interactions with linear computational complexity during training. Third, SMTrack enables each frame to interact with previously tracked frames via hidden state propagation and updating, which releases computational costs of handling temporal cues during tracking. Extensive experimental results demonstrate that SMTrack achieves promising performance with low computational costs.

</details>


### [200] [FreshMem: Brain-Inspired Frequency-Space Hybrid Memory for Streaming Video Understanding](https://arxiv.org/abs/2602.01683)
*Kangcong Li,Peng Ye,Lin Zhang,Chao Wang,Huafeng Qin,Tao Chen*

Main category: cs.CV

TL;DR: 论文提出了一种新的混合记忆网络FreshMem，以提高多模态大模型在在线视频流理解任务中的连续感知和长时段理解能力，显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型（MLLMs）在由离线转至流式视频理解时，存在适应性不足导致细节丢失和上下文碎片化等问题，限制了模型在实际长时视频场景中的表现。

Method: 提出FreshMem，一种受大脑感知与记忆模型启发的“频率-空间混合记忆网络”。其由多尺度频率记忆（MFM）模块将溢出的帧投影为频域系数并结合残差细节帮助恢复全局背景，以及空间缩略图记忆（STM）模块，通过自适应压缩将视频流划分为高密度空间缩略图来存储历史信息。

Result: 在StreamingBench、OV-Bench和OVO-Bench三个主流流式视频理解基准评测中，FreshMem在无需训练即可为Qwen2-VL基础模型带来5.20%、4.52%、2.34%的性能提升，并显著超越多种全量微调方法。

Conclusion: FreshMem为长时段流式视频理解提供了一种高效且无需训练的新范式，有效解决了以往细节遗失与上下文碎片化的问题，对多模态大模型的持续感知具有重要意义。

Abstract: Transitioning Multimodal Large Language Models (MLLMs) from offline to online streaming video understanding is essential for continuous perception. However, existing methods lack flexible adaptivity, leading to irreversible detail loss and context fragmentation. To resolve this, we propose FreshMem, a Frequency-Space Hybrid Memory network inspired by the brain's logarithmic perception and memory consolidation. FreshMem reconciles short-term fidelity with long-term coherence through two synergistic modules: Multi-scale Frequency Memory (MFM), which projects overflowing frames into representative frequency coefficients, complemented by residual details to reconstruct a global historical "gist"; and Space Thumbnail Memory (STM), which discretizes the continuous stream into episodic clusters by employing an adaptive compression strategy to distill them into high-density space thumbnails. Extensive experiments show that FreshMem significantly boosts the Qwen2-VL baseline, yielding gains of 5.20%, 4.52%, and 2.34% on StreamingBench, OV-Bench, and OVO-Bench, respectively. As a training-free solution, FreshMem outperforms several fully fine-tuned methods, offering a highly efficient paradigm for long-horizon streaming video understanding.

</details>


### [201] [Cross-Modal Alignment and Fusion for RGB-D Transmission-Line Defect Detection](https://arxiv.org/abs/2602.01696)
*Jiaming Cui,Shuai Zhou,Wenqiang Li,Ruifeng Qin,Feng Shen*

Main category: cs.CV

TL;DR: 该论文提出了CMAFNet，一种融合RGB图像和深度信息的新型网络，用于解决无人机输电线路缺陷检测中的小目标、复杂背景和光照变化等难题，实验结果表明其在TLRGBD数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在输电线路缺陷检测中，自动化无人机检测由于缺陷小、背景复杂和光照变化大等问题，检测准确率较低。传统依赖RGB图像的方法难以区分形态微小的缺陷和相似背景结构。因此，需要融合其他信息（如深度）以提升细粒度缺陷检测能力。

Method: 提出了CMAFNet架构，包含两个核心模块：（1）基于字典的语义重组模块，对特征进行净化，抑制不同模态的噪声，突出与缺陷相关的信息；（2）上下文语义集成框架，通过部分通道注意力机制捕捉全局空间依赖，强化结构语义推理。同时，在净化阶段引入基于位置的归一化，实现深度与RGB特征的统计一致性和对齐，使得特征融合前跨模态兼容。

Result: 在TLRGBD数据集（94.5%为小目标）上的实验结果显示，CMAFNet获得32.2%的mAP@50和12.5%的小目标APs，分别比最强基线高9.8和4.0个百分点。轻量化版本也达到24.8%的mAP@50和228 FPS，参数仅4.9M，优于所有YOLO方法，并能以更低的计算成本媲美Transformer方法。

Conclusion: CMAFNet通过有效融合RGB与深度信息，在小目标输电线路缺陷检测上取得显著提升，不仅提升了检测精度，还兼顾了速度和参数量，对实际无人机巡检场景具有较大应用前景。

Abstract: Transmission line defect detection remains challenging for automated UAV inspection due to the dominance of small-scale defects, complex backgrounds, and illumination variations. Existing RGB-based detectors, despite recent progress, struggle to distinguish geometrically subtle defects from visually similar background structures under limited chromatic contrast. This paper proposes CMAFNet, a Cross-Modal Alignment and Fusion Network that integrates RGB appearance and depth geometry through a principled purify-then-fuse paradigm. CMAFNet consists of a Semantic Recomposition Module that performs dictionary-based feature purification via a learned codebook to suppress modality-specific noise while preserving defect-discriminative information, and a Contextual Semantic Integration Framework that captures global spatial dependencies using partial-channel attention to enhance structural semantic reasoning. Position-wise normalization within the purification stage enforces explicit reconstruction-driven cross-modal alignment, ensuring statistical compatibility between heterogeneous features prior to fusion. Extensive experiments on the TLRGBD benchmark, where 94.5% of instances are small objects, demonstrate that CMAFNet achieves 32.2% mAP@50 and 12.5% APs, outperforming the strongest baseline by 9.8 and 4.0 percentage points, respectively. A lightweight variant reaches 24.8% mAP50 at 228 FPS with only 4.9M parameters, surpassing all YOLO-based detectors while matching transformer-based methods at substantially lower computational cost.

</details>


### [202] [Physics Informed Generative AI Enabling Labour Free Segmentation For Microscopy Analysis](https://arxiv.org/abs/2602.01710)
*Salma Zahran,Zhou Ao,Zhengyang Zhang,Chen Chi,Chenchen Yuan,Yanming Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种结合物理模拟与生成对抗网络（CycleGAN），实现无需人工标注的显微图像分割方法，在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 显微图像的语义分割依赖专家标注，但高昂成本和主观性严重限制了自动化应用。虽然基于物理的仿真可大量提供数据，但仿真数据与真实实验数据之间存在显著差异，导致现有模型难以泛化。

Method: 1）利用相场模拟生成大量带有真实掩模的微观结构图像；2）采用CycleGAN将仿真图像无监督转换为逼真的SEM实验图像；3）用这些高保真的合成图像训练U-Net分割网络，并在真实实验图像上检验效果。

Result: 仅用合成数据训练的U-Net在真实实验图像上取得了平均边界F1分数0.90、IOU为0.88的优异成绩；t-SNE特征投影和香农熵分析均表明，生成数据与真实数据在统计和特征分布上无显著差别。

Conclusion: 该方法无需任何人工标注，即可实现高质量的显微图像分割，为材料科学中的高通量分析和自动化发现提供了可扩展且可靠的解决方案。

Abstract: Semantic segmentation of microscopy images is a critical task for high-throughput materials characterisation, yet its automation is severely constrained by the prohibitive cost, subjectivity, and scarcity of expert-annotated data. While physics-based simulations offer a scalable alternative to manual labelling, models trained on such data historically fail to generalise due to a significant domain gap, lacking the complex textures, noise patterns, and imaging artefacts inherent to experimental data. This paper introduces a novel framework for labour-free segmentation that successfully bridges this simulation-to-reality gap. Our pipeline leverages phase-field simulations to generate an abundant source of microstructural morphologies with perfect, intrinsically-derived ground-truth masks. We then employ a Cycle-Consistent Generative Adversarial Network (CycleGAN) for unpaired image-to-image translation, transforming the clean simulations into a large-scale dataset of high-fidelity, realistic SEM images. A U-Net model, trained exclusively on this synthetic data, demonstrated remarkable generalisation when deployed on unseen experimental images, achieving a mean Boundary F1-Score of 0.90 and an Intersection over Union (IOU) of 0.88. Comprehensive validation using t-SNE feature-space projection and Shannon entropy analysis confirms that our synthetic images are statistically and featurally indistinguishable from the real data manifold. By completely decoupling model training from manual annotation, our generative framework transforms a data-scarce problem into one of data abundance, providing a robust and fully automated solution to accelerate materials discovery and analysis.

</details>


### [203] [FastPhysGS: Accelerating Physics-based Dynamic 3DGS Simulation via Interior Completion and Adaptive Optimization](https://arxiv.org/abs/2602.01723)
*Yikun Ma,Yiqing Li,Jingwen Ye,Zhongkai Wu,Weidong Zhang,Lin Gao,Zhi Jin*

Main category: cs.CV

TL;DR: 本文提出了一种名为FastPhysGS的新框架，实现了高效、鲁棒的基于物理的动态3D高斯泼溅(3DGS)模拟，在准确性和计算效率上优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于物理的3D场景模拟依赖于繁琐的参数调优或对视频扩散模型动态的蒸馏，导致泛化性和优化效率受限。同时，直接利用LLMs/VLMs存在感知偏差，无法精确还原物理行为。此外，许多方法忽视了3DGS的表面结构，导致动态模拟不合理。因此，亟需一种兼具高效、自动化和物理精度的新方法。

Method: FastPhysGS框架包含两大核心技术：(1) 基于蒙特卡洛重要性采样的实例感知粒子填充（Instance-aware Particle Filling, IPF），高效填充内部粒子的同时保持几何一致性；(2) 双向图解耦优化（Bidirectional Graph Decoupling Optimization, BGDO），自适应地快速优化VLM预测的材料参数。

Result: 实验证明，FastPhysGS实现了高保真度的物理模拟，1分钟内即可完成模拟，运行内存仅需7GB，物理行为和渲染质量均超越现有方法。

Conclusion: FastPhysGS在物理精度、计算效率和泛用性方面取得突破，可广泛应用于3D物理动画、虚拟现实等领域，推动了动态3D内容的高效生成和模拟技术发展。

Abstract: Extending 3D Gaussian Splatting (3DGS) to 4D physical simulation remains challenging. Based on the Material Point Method (MPM), existing methods either rely on manual parameter tuning or distill dynamics from video diffusion models, limiting the generalization and optimization efficiency. Recent attempts using LLMs/VLMs suffer from a text/image-to-3D perceptual gap, yielding unstable physics behavior. In addition, they often ignore the surface structure of 3DGS, leading to implausible motion. We propose FastPhysGS, a fast and robust framework for physics-based dynamic 3DGS simulation:(1) Instance-aware Particle Filling (IPF) with Monte Carlo Importance Sampling (MCIS) to efficiently populate interior particles while preserving geometric fidelity; (2) Bidirectional Graph Decoupling Optimization (BGDO), an adaptive strategy that rapidly optimizes material parameters predicted from a VLM. Experiments show FastPhysGS achieves high-fidelity physical simulation in 1 minute using only 7 GB runtime memory, outperforming prior works with broad potential applications.

</details>


### [204] [DenVisCoM: Dense Vision Correspondence Mamba for Efficient and Real-time Optical Flow and Stereo Estimation](https://arxiv.org/abs/2602.01724)
*Tushar Anand,Maheswar Bora,Antitza Dantcheva,Abhijit Das*

Main category: cs.CV

TL;DR: 本文提出了一种新型的Mamba块DenVisCoM以及混合式架构，可高效、实时地联合估算光流和视差。实验显示该方法能兼顾准确性与实时性。


<details>
  <summary>Details</summary>
Motivation: 多视角几何和运动分析任务关系密切，但目前缺乏能同时高效解决二者的统一模型。因而需要一种可实时、准确联合完成任务的架构。

Method: 提出了基于DenVisCoM的Mamba块及结合Transformer注意力机制的混合架构，用以联合估算运动（光流）与三维感知任务（视差），优化实时推理、内存占用与准确率间的权衡。

Result: 大规模数据集上的实验表明，该模型在准确估算光流和视差的同时，实现了实时推理，并具备较低的内存开销。

Conclusion: DenVisCoM混合架构能够有效、实时且准确地解决光流和视差的联合估算问题，适用于实际应用。作者已开源全部模型和代码。

Abstract: In this work, we propose a novel Mamba block DenVisCoM, as well as a novel hybrid architecture specifically tailored for accurate and real-time estimation of optical flow and disparity estimation. Given that such multi-view geometry and motion tasks are fundamentally related, we propose a unified architecture to tackle them jointly. Specifically, the proposed hybrid architecture is based on DenVisCoM and a Transformer-based attention block that efficiently addresses real-time inference, memory footprint, and accuracy at the same time for joint estimation of motion and 3D dense perception tasks. We extensively analyze the benchmark trade-off of accuracy and real-time processing on a large number of datasets. Our experimental results and related analysis suggest that our proposed model can accurately estimate optical flow and disparity estimation in real time. All models and associated code are available at https://github.com/vimstereo/DenVisCoM.

</details>


### [205] [Simplicity Prevails: The Emergence of Generalizable AIGI Detection in Visual Foundation Models](https://arxiv.org/abs/2602.01738)
*Yue Zhou,Xinan He,Kaiqing Lin,Bing Fan,Feng Ding,Bin Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉基础模型Frozen特征的简单线性分类器，用于检测AI生成图像（AIGI），不仅在标准数据集上表现优异，还在现实分布场景中大大超越现有专用检测器。


<details>
  <summary>Details</summary>
Motivation: 现有专用AIGI检测器在人工数据集上准确度高，但在真实环境下表现严重下降，亟需一种更具泛化能力的检测方法。

Method: 作者使用现代视觉基础模型（如Perception Encoder、MetaCLIP 2、DINOv3）的冻结特征，训练一个简单线性分类器，无需复杂架构。对其在标准数据集、未知生成器和现实分布上的性能进行全面评测。

Result: 该简单模型在标准数据集上能匹配专用检测器效果，并在in-the-wild场景中准确率提升超30%。分析发现，视觉-语言模型通过大规模预训练数据显式学习到伪造语义，SSL模型则隐式获得取证判别力。但在重拍、传输、VAE重建和局部编辑下仍有限。

Conclusion: 作者呼吁AIGI取证需从过度拟合静态数据集转向利用基础模型不断进化的世界知识，以提升真实世界鲁棒性。

Abstract: While specialized detectors for AI-Generated Images (AIGI) achieve near-perfect accuracy on curated benchmarks, they suffer from a dramatic performance collapse in realistic, in-the-wild scenarios. In this work, we demonstrate that simplicity prevails over complex architectural designs. A simple linear classifier trained on the frozen features of modern Vision Foundation Models , including Perception Encoder, MetaCLIP 2, and DINOv3, establishes a new state-of-the-art. Through a comprehensive evaluation spanning traditional benchmarks, unseen generators, and challenging in-the-wild distributions, we show that this baseline not only matches specialized detectors on standard benchmarks but also decisively outperforms them on in-the-wild datasets, boosting accuracy by striking margins of over 30\%. We posit that this superior capability is an emergent property driven by the massive scale of pre-training data containing synthetic content. We trace the source of this capability to two distinct manifestations of data exposure: Vision-Language Models internalize an explicit semantic concept of forgery, while Self-Supervised Learning models implicitly acquire discriminative forensic features from the pretraining data. However, we also reveal persistent limitations: these models suffer from performance degradation under recapture and transmission, remain blind to VAE reconstruction and localized editing. We conclude by advocating for a paradigm shift in AI forensics, moving from overfitting on static benchmarks to harnessing the evolving world knowledge of foundation models for real-world reliability.

</details>


### [206] [Tail-Aware Post-Training Quantization for 3D Geometry Models](https://arxiv.org/abs/2602.01741)
*Sicheng Pan,Chen Tang,Shuzhao Xie,Ke Yang,Weixiang Zhang,Jiawei Li,Bin Chen,Shu-Tao Xia,Zhi Wang*

Main category: cs.CV

TL;DR: 本文提出了一种专为3D几何模型设计的后训练量化（PTQ）方法TAPTQ，大幅提升量化效率和精度，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 3D几何模型的复杂性和规模越来越大，使得其在资源受限平台上的部署变得困难。现有PTQ方法主要针对2D视觉Transformer优化，难以直接应用于3D模型，且校准开销过大。因此，有必要发展能更高效适应3D几何学习的量化方法。

Method: 1）提出分阶段粗到细的校准子集构建策略，兼顾数据纯净度和几何代表性，缓解3D数据集的数据规模瓶颈。2）将量化区间选择建模为优化问题，引入基于三分搜索的新算法，将复杂度从O(N)降至O(logN)。3）提出基于尾相对误差（TRE）的逐模块补偿机制，自动检测并修复对激活尾部异常值敏感的模块，减小量化误差累积。

Result: 在VGGT和Pi3两个基准测试上，大量实验表明，TAPTQ在准确率上持续优于最先进的PTQ方法，同时显著减少了校准时间。

Conclusion: TAPTQ为3D几何模型提供了专用且高效的后训练量化方法，提升准确性和部署效率，具备强应用价值。代码即将开源。

Abstract: The burgeoning complexity and scale of 3D geometry models pose significant challenges for deployment on resource-constrained platforms. While Post-Training Quantization (PTQ) enables efficient inference without retraining, conventional methods, primarily optimized for 2D Vision Transformers, fail to transfer effectively to 3D models due to intricate feature distributions and prohibitive calibration overhead. To address these challenges, we propose TAPTQ, a Tail-Aware Post-Training Quantization pipeline specifically engineered for 3D geometric learning. Our contribution is threefold: (1) To overcome the data-scale bottleneck in 3D datasets, we develop a progressive coarse-to-fine calibration construction strategy that constructs a highly compact subset to achieve both statistical purity and geometric representativeness. (2) We reformulate the quantization interval search as an optimization problem and introduce a ternary-search-based solver, reducing the computational complexity from $\mathcal{O}(N)$ to $\mathcal{O}(\log N)$ for accelerated deployment. (3) To mitigate quantization error accumulation, we propose TRE-Guided Module-wise Compensation, which utilizes a Tail Relative Error (TRE) metric to adaptively identify and rectify distortions in modules sensitive to long-tailed activation outliers. Extensive experiments on the VGGT and Pi3 benchmarks demonstrate that TAPTQ consistently outperforms state-of-the-art PTQ methods in accuracy while significantly reducing calibration time. The code will be released soon.

</details>


### [207] [ObjEmbed: Towards Universal Multimodal Object Embeddings](https://arxiv.org/abs/2602.01753)
*Shenghao Fu,Yukun Su,Fengyun Rao,Jing Lyu,Xiaohua Xie,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: 提出了一种新型的多模态视觉-语言对齐模型ObjEmbed，针对细粒度物体与文本对齐问题，能高效支持多种视觉理解任务，并在18项基准任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有多模态嵌入模型更多聚焦全局图文对齐，难以处理图像区域与特定文本短语的细粒度对齐需求。细致区域的对齐对于视觉基础任务如视觉定位与局部检索至关重要，因此亟需更细粒度的对齐方法。

Method: ObjEmbed将图像分解为多个区域，每个区域生成对应物体的嵌入，同时还生成全局嵌入。每个区域被赋予语义嵌入和可定位性的IoU嵌入，两者结合后用于物体与文本的匹配。模型在单次前向推理内同时对全部对象和图片整体进行编码，既兼顾分区域，又支持整体检索。

Result: ObjEmbed在视觉定位、局部及全局图片检索等多种视觉任务上进行了评测，在18个公开基准集上取得了领先的性能，显示了其优越的语义区分能力和细粒度对齐效果。

Conclusion: ObjEmbed有效提升了物体-文本级别的细粒度对齐性能，具备高适用性与高效性，对多种视觉语言任务表现优异，推动了视觉语言理解领域的发展。

Abstract: Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination.

</details>


### [208] [Spot-Wise Smart Parking: An Edge-Enabled Architecture with YOLOv11 and Digital Twin Integration](https://arxiv.org/abs/2602.01754)
*Gustavo P. C. P. da Luz,Alvaro M. Aspilcueta Narvaez,Tiago Godoi Bannwart,Gabriel Massuyoshi Sato,Luis Fernando Gomez Gonzalez,Juliana Freitag Borin*

Main category: cs.CV

TL;DR: 该论文提出了一种更精细的智能停车系统，在停车位级别实现了高精度监测，并通过软硬件创新增强智慧城市应用。


<details>
  <summary>Details</summary>
Motivation: 现有方法只能估算区域内剩余车位数量，无法提供单个车位的占用信息，限制了系统的丰富应用和洞察力。解决更细粒度访问、支持更复杂应用成为迫切需求。

Method: 作者采用了基于距离感知的匹配方法（带有空间容差），并通过自适应边界框分割方法提升对难识别车位的处理能力。使用轻量级YOLOv11m模型部署在资源有限的边缘设备上。同时引入了数字影子（Digital Shadow）和一个基于改造电视盒的应用支持服务器实现硬件复用和数据可视化。

Result: 新方法在车位级监测上达到了98.8%的平衡精度，推理时间为8秒，且系统部署于仅40.5 MB的轻量化模型及低功耗设备上。引入的新组件确保了可扩展性与持续性。

Conclusion: 该系统在保持高准确率和效率的同时，实现了停车位级别的精细监控，并推广了硬件可持续性思路，有力地推进了智慧城市智能停车应用的发展。

Abstract: Smart parking systems help reduce congestion and minimize users' search time, thereby contributing to smart city adoption and enhancing urban mobility. In previous works, we presented a system developed on a university campus to monitor parking availability by estimating the number of free spaces from vehicle counts within a region of interest. Although this approach achieved good accuracy, it restricted the system's ability to provide spot-level insights and support more advanced applications. To overcome this limitation, we extend the system with a spot-wise monitoring strategy based on a distance-aware matching method with spatial tolerance, enhanced through an Adaptive Bounding Box Partitioning method for challenging spaces. The proposed approach achieves a balanced accuracy of 98.80% while maintaining an inference time of 8 seconds on a resource-constrained edge device, enhancing the capabilities of YOLOv11m, a model that has a size of 40.5 MB. In addition, two new components were introduced: (i) a Digital Shadow that visually represents parking lot entities as a base to evolve to a full Digital Twin, and (ii) an application support server based on a repurposed TV box. The latter not only enables scalable communication among cloud services, the parking totem, and a bot that provides detailed spot occupancy statistics, but also promotes hardware reuse as a step towards greater sustainability.

</details>


### [209] [Mind-Brush: Integrating Agentic Cognitive Search and Reasoning into Image Generation](https://arxiv.org/abs/2602.01756)
*Jun He,Junyan Ye,Zilong Huang,Dongzhi Jiang,Chenjue Zhang,Leqi Zhu,Renrui Zhang,Xiang Zhang,Weijia Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的文本到图像生成框架 Mind-Brush，用动态、知识驱动的方式代替传统静态解码，提升模型对用户意图和复杂知识推理的理解力，并引入了评测基准 Mind-Bench，实验效果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型缺乏对隐含用户意图与复杂知识推理的理解，且内部先验静态，难以适应现实世界的变化，亟需更加智能、灵活的生成框架。

Method: Mind-Brush通过模拟“思考-查证-创作”工作流，主动检索多模态证据支持新颖概念，并利用推理工具解决复杂的视觉约束，实现动态、知识驱动的图像生成。同时提出Mind-Bench基准，全面测试模型在多领域和新颖任务中的能力。

Result: Mind-Brush在新建立的Mind-Bench测试集上，使Qwen-Image等基线模型能力获得0到1的突破，并在WISE、RISE等权威评测上取得优越成绩，显示出显著提升。

Conclusion: Mind-Brush极大地提升了统一生成模型对复杂任务和动态知识的理解与生成能力，推动了智能图像生成领域的发展。

Abstract: While text-to-image generation has achieved unprecedented fidelity, the vast majority of existing models function fundamentally as static text-to-pixel decoders. Consequently, they often fail to grasp implicit user intentions. Although emerging unified understanding-generation models have improved intent comprehension, they still struggle to accomplish tasks involving complex knowledge reasoning within a single model. Moreover, constrained by static internal priors, these models remain unable to adapt to the evolving dynamics of the real world. To bridge these gaps, we introduce Mind-Brush, a unified agentic framework that transforms generation into a dynamic, knowledge-driven workflow. Simulating a human-like 'think-research-create' paradigm, Mind-Brush actively retrieves multimodal evidence to ground out-of-distribution concepts and employs reasoning tools to resolve implicit visual constraints. To rigorously evaluate these capabilities, we propose Mind-Bench, a comprehensive benchmark comprising 500 distinct samples spanning real-time news, emerging concepts, and domains such as mathematical and Geo-Reasoning. Extensive experiments demonstrate that Mind-Brush significantly enhances the capabilities of unified models, realizing a zero-to-one capability leap for the Qwen-Image baseline on Mind-Bench, while achieving superior results on established benchmarks like WISE and RISE.

</details>


### [210] [MagicFuse: Single Image Fusion for Visual and Semantic Reinforcement](https://arxiv.org/abs/2602.01760)
*Hao Zhang,Yanping Zha,Zizhuo Li,Meiqi Gong,Jiayi Ma*

Main category: cs.CV

TL;DR: 本文提出了一种创新的单幅图像融合框架MagicFuse，能够在只有低质量可见光图像的条件下，实现近似多模态融合的效果。


<details>
  <summary>Details</summary>
Motivation: 在实际严苛环境中，红外等多模态传感器可能不可用，如何仅利用可见光图像最大程度挖掘和融合场景信息成为亟需解决的问题。

Method: 提出MagicFuse框架，核心包括基于扩散模型的可见光知识强化分支和跨光谱知识生成分支，并通过多域知识融合分支整合两者信息，辅以视觉和语义约束优化场景表达。

Result: MagicFuse在仅用受损可见光图像的情况下，在视觉和语义表现上达到甚至超过依赖多模态输入的主流融合方法。

Conclusion: MagicFuse实现了单一低质量可见光图像的跨光谱特征重建和融合，显著拓展了多模态融合方法在受限条件下的实用性。

Abstract: This paper focuses on a highly practical scenario: how to continue benefiting from the advantages of multi-modal image fusion under harsh conditions when only visible imaging sensors are available. To achieve this goal, we propose a novel concept of single-image fusion, which extends conventional data-level fusion to the knowledge level. Specifically, we develop MagicFuse, a novel single image fusion framework capable of deriving a comprehensive cross-spectral scene representation from a single low-quality visible image. MagicFuse first introduces an intra-spectral knowledge reinforcement branch and a cross-spectral knowledge generation branch based on the diffusion models. They mine scene information obscured in the visible spectrum and learn thermal radiation distribution patterns transferred to the infrared spectrum, respectively. Building on them, we design a multi-domain knowledge fusion branch that integrates the probabilistic noise from the diffusion streams of these two branches, from which a cross-spectral scene representation can be obtained through successive sampling. Then, we impose both visual and semantic constraints to ensure that this scene representation can satisfy human observation while supporting downstream semantic decision-making. Extensive experiments show that our MagicFuse achieves visual and semantic representation performance comparable to or even better than state-of-the-art fusion methods with multi-modal inputs, despite relying solely on a single degraded visible image.

</details>


### [211] [GDPR-Compliant Person Recognition in Industrial Environments Using MEMS-LiDAR and Hybrid Data](https://arxiv.org/abs/2602.01764)
*Dennis Basile,Dennis Sprute,Helene Dörksen,Holger Flatt*

Main category: cs.CV

TL;DR: 该论文提出了一种基于MEMS-LiDAR点云和合成数据增强的新型入侵者检测方法，显著提升了检测精度并减少了人工标注成本，同时满足隐私保护法规。


<details>
  <summary>Details</summary>
Motivation: 工业室内环境中，防止未授权人员入侵对安全至关重要。现有基于视觉的深度学习识别方法容易受光照影响且涉及隐私问题，而且高质量标注数据的获取和制作成本高且耗时。

Method: 采用MEMS-LiDAR采集匿名化3D点云，避免捕捉个人身份特征。通过CARLA仿真合成大量场景点云数据，与真实采集的点云混合，大幅减少真实数据采集和标注的成本。

Result: 结合真实与合成点云数据后，入侵者检测的平均精度提升了44个百分点，同时人工标注工作量降低50%。

Conclusion: 该方法在保障GDPR隐私合规的前提下，有效提升了工业场景人员检测的性能和效率，为相关领域提供了可扩展、经济高效的解决方案。

Abstract: The reliable detection of unauthorized individuals in safety-critical industrial indoor spaces is crucial to avoid plant shutdowns, property damage, and personal hazards. Conventional vision-based methods that use deep-learning approaches for person recognition provide image information but are sensitive to lighting and visibility conditions and often violate privacy regulations, such as the General Data Protection Regulation (GDPR) in the European Union. Typically, detection systems based on deep learning require annotated data for training. Collecting and annotating such data, however, is highly time-consuming and due to manual treatments not necessarily error free. Therefore, this paper presents a privacy-compliant approach based on Micro-Electro-Mechanical Systems LiDAR (MEMS-LiDAR), which exclusively captures anonymized 3D point clouds and avoids personal identification features. To compensate for the large amount of time required to record real LiDAR data and for post-processing and annotation, real recordings are augmented with synthetically generated scenes from the CARLA simulation framework. The results demonstrate that the hybrid data improves the average precision by 44 percentage points compared to a model trained exclusively with real data while reducing the manual annotation effort by 50 %. Thus, the proposed approach provides a scalable, cost-efficient alternative to purely real-data-based methods and systematically shows how synthetic LiDAR data can combine high performance in person detection with GDPR compliance in an industrial environment.

</details>


### [212] [Automated Discontinuity Set Characterisation in Enclosed Rock Face Point Clouds Using Single-Shot Filtering and Cyclic Orientation Transformation](https://arxiv.org/abs/2602.01783)
*Dibyayan Patra,Pasindu Ranasinghe,Bikram Banerjee,Simit Raval*

Main category: cs.CV

TL;DR: 本文提出了一种用于自动识别地下矿井裸露岩面结构不连续面组的新方法，并在真实矿井环境下验证了其有效性，表现出比现有技术更高的精度。


<details>
  <summary>Details</summary>
Motivation: 在地下矿井岩面结构不连续面组的表征对岩体稳定性评估和运营安全极为重要，但当前在实际封闭矿井环境下自动高效识别的技术尚不完善。

Method: 本方法包括三大创新点：（1）采用单次滤波策略，通过信号处理技术一次性提取平面区域并抑制噪声和高曲率伪影；（2）提出循环方位变换方案，将极坐标下的走向和倾角准确转换到笛卡尔空间，克服传统笛卡尔聚类对极坐标数据的局限；（3）采用层次聚类方法，无需用户预设面组数，自动识别不同密度分布的面组。

Result: 该方法在真实矿井巷道的数据上与手工选取和主流自动结构测绘技术比对，表现出在不连续面走向与倾角识别上的最低平均绝对误差，分别为1.95°和2.20°，聚类离散度误差均低于3°。

Conclusion: 本文提出的自动结构不连续面组识别方法不仅自动化程度高，且精度优于现有方法，为实际矿井封闭空间的结构表征提供了可靠、高效的技术途径。

Abstract: Characterisation of structural discontinuity sets in exposed rock faces of underground mine cavities is essential for assessing rock-mass stability, excavation safety, and operational efficiency. UAV and other mobile laser-scanning techniques provide efficient means of collecting point clouds from rock faces. However, the development of a robust and efficient approach for automatic characterisation of discontinuity sets in real-world scenarios, like fully enclosed rock faces in cavities, remains an open research problem. In this study, a new approach is proposed for automatic discontinuity set characterisation that uses a single-shot filtering strategy, an innovative cyclic orientation transformation scheme and a hierarchical clustering technique. The single-shot filtering step isolates planar regions while robustly suppressing noise and high-curvature artefacts in one pass using a signal-processing technique. To address the limitations of Cartesian clustering on polar orientation data, a cyclic orientation transformation scheme is developed, enabling accurate representation of dip angle and dip direction in Cartesian space. The transformed orientations are then characterised into sets using a hierarchical clustering technique, which handles varying density distributions and identifies clusters without requiring user-defined set numbers. The accuracy of the method is validated on real-world mine stope and against ground truth obtained using manually handpicked discontinuity planes identified with the Virtual Compass tool, as well as widely used automated structure mapping techniques. The proposed approach outperforms the other techniques by exhibiting the lowest mean absolute error in estimating discontinuity set orientations in real-world stope data with errors of 1.95° and 2.20° in nominal dip angle and dip direction, respectively, and dispersion errors lying below 3°.

</details>


### [213] [Spatio-Temporal Transformers for Long-Term NDVI Forecasting](https://arxiv.org/abs/2602.01799)
*Ido Faran,Nathan S. Netanyahu,Maxim Shoshany*

Main category: cs.CV

TL;DR: 本文提出了一种适用于异质地表长期卫星影像时序分析的新模型 STT-LTF，通过空间与时间统一建模，在地中海复杂景观下获得更优预测表现。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以同时捕捉多尺度空间异质性与长期序列特征，且地中海地区的复杂性及数据长期缺乏标注，导致预测长期生态动态具有挑战性。

Method: 提出 Spatio-Temporal Transformer for Long Term Forecasting（STT-LTF）框架，在Transformer结构中融合空间patch、时间序列、地理坐标，实现空间-时间联合建模。采用空间和时间掩码自监督学习策略，并支持任意未来时间点的直接预测。

Result: STT-LTF 在 1984-2024 年Landsat卫星影像数据集的实验中，取得MAE 为0.0328，R^2为0.8412，显著优于统计模型、CNN、LSTM及标准Transformer等对比方法。

Conclusion: STT-LTF 可有效处理时空不规则采样和不同预测区间，适用于地中海等异质快速变化区域的长期影像分析，为大尺度生态监测与变化预测提供新工具。

Abstract: Long-term satellite image time series (SITS) analysis in heterogeneous landscapes faces significant challenges, particularly in Mediterranean regions where complex spatial patterns, seasonal variations, and multi-decade environmental changes interact across different scales. This paper presents the Spatio-Temporal Transformer for Long Term Forecasting (STT-LTF ), an extended framework that advances beyond purely temporal analysis to integrate spatial context modeling with temporal sequence prediction. STT-LTF processes multi-scale spatial patches alongside temporal sequences (up to 20 years) through a unified transformer architecture, capturing both local neighborhood relationships and regional climate influences. The framework employs comprehensive self-supervised learning with spatial masking, temporal masking, and horizon sampling strategies, enabling robust model training from 40 years of unlabeled Landsat imagery. Unlike autoregressive approaches, STT-LTF directly predicts arbitrary future time points without error accumulation, incorporating spatial patch embeddings, cyclical temporal encoding, and geographic coordinates to learn complex dependencies across heterogeneous Mediterranean ecosystems. Experimental evaluation on Landsat data (1984-2024) demonstrates that STT-LTF achieves a Mean Absolute Error (MAE) of 0.0328 and R^2 of 0.8412 for next-year predictions, outperforming traditional statistical methods, CNN-based approaches, LSTM networks, and standard transformers. The framework's ability to handle irregular temporal sampling and variable prediction horizons makes it particularly suitable for analysis of heterogeneous landscapes experiencing rapid ecological transitions.

</details>


### [214] [Fast Autoregressive Video Diffusion and World Models with Temporal Cache Compression and Sparse Attention](https://arxiv.org/abs/2602.01801)
*Dvir Samuel,Issar Tzachor,Matan Levy,Micahel Green,Gal Chechik,Rami Ben-Ari*

Main category: cs.CV

TL;DR: 本文针对自回归视频扩散模型在推理阶段KV缓存膨胀导致延迟和GPU内存占用过高的问题，提出了无训练的注意力加速框架，在保持画质的同时大幅提升了推理效率和长期稳定性。


<details>
  <summary>Details</summary>
Motivation: 自回归视频扩散模型支持流式生成，有潜力应用于长视频合成和交互式神经引擎，但随着生成步数增加，注意力模块KV缓存占用快速增长，带来延迟和内存瓶颈，限制了模型的上下文利用和一致性，因此亟需提升效率和扩展性。

Method: 作者系统剖析了冗余来源，提出：1）TempCache，基于帧间对应关系压缩KV缓存，抑制缓存膨胀；2）AnnCA，利用近似最近邻（ANN）快速挑选与帧相关的提示token，加速跨注意力；3）AnnSA，通过ANN限制自注意力每个query仅与语义匹配的key交互，进一步稀疏化计算。该框架免训练，可直接集成现有自回归扩散模型。

Result: 实验证明，该方法可实现5-10倍的端到端加速，同时几乎保持相同的视觉质量，且在长序列生成中有效稳定推理速度和GPU峰值内存，不像以往方法那样随生成步数恶化。

Conclusion: 提出的训练无关注意力压缩与加速方案有效缓解了自回归视频扩散的推理瓶颈，为大规模、长时序的生成同步带来了显著的效率和资源改进，有望拓展相关模型的实际场景应用。

Abstract: Autoregressive video diffusion models enable streaming generation, opening the door to long-form synthesis, video world models, and interactive neural game engines. However, their core attention layers become a major bottleneck at inference time: as generation progresses, the KV cache grows, causing both increasing latency and escalating GPU memory, which in turn restricts usable temporal context and harms long-range consistency. In this work, we study redundancy in autoregressive video diffusion and identify three persistent sources: near-duplicate cached keys across frames, slowly evolving (largely semantic) queries/keys that make many attention computations redundant, and cross-attention over long prompts where only a small subset of tokens matters per frame. Building on these observations, we propose a unified, training-free attention framework for autoregressive diffusion: TempCache compresses the KV cache via temporal correspondence to bound cache growth; AnnCA accelerates cross-attention by selecting frame-relevant prompt tokens using fast approximate nearest neighbor (ANN) matching; and AnnSA sparsifies self-attention by restricting each query to semantically matched keys, also using a lightweight ANN. Together, these modules reduce attention, compute, and memory and are compatible with existing autoregressive diffusion backbones and world models. Experiments demonstrate up to x5--x10 end-to-end speedups while preserving near-identical visual quality and, crucially, maintaining stable throughput and nearly constant peak GPU memory usage over long rollouts, where prior methods progressively slow down and suffer from increasing memory usage.

</details>


### [215] [FlowBypass: Rectified Flow Trajectory Bypass for Training-Free Image Editing](https://arxiv.org/abs/2602.01805)
*Menglin Han,Zhangkai Ni*

Main category: cs.CV

TL;DR: 该论文提出了一种无训练图像编辑方法FlowBypass，通过飞越式轨迹连接，提升编辑质量与效率。


<details>
  <summary>Details</summary>
Motivation: 现有无训练图像编辑方法在轨迹长度上存在两难：长轨迹积累误差降低保真度，短轨迹不足以满足编辑要求。且多数方法依赖骨干网络特征操作，通用性差。这促使作者探索无特征操作的新型编辑框架。

Method: 提出了基于Rectified Flow的FlowBypass框架，绕过传统的反演—重建轨迹，通过精确推导两条轨迹及其数值解法，形成高效编辑路径，无需依赖特征操控。

Result: 大量实验表明，FlowBypass在编辑指令对齐度和无关区域细节保真度上均优于现有最先进方法，表现出更好的编缉质量与实用性。

Conclusion: FlowBypass有效解决了现有方法两难问题，实现了更普适、更高效、更高质量的无训练图像编辑，为今后通用型图像编辑方法提供了新思路。

Abstract: Training-free image editing has attracted increasing attention for its efficiency and independence from training data. However, existing approaches predominantly rely on inversion-reconstruction trajectories, which impose an inherent trade-off: longer trajectories accumulate errors and compromise fidelity, while shorter ones fail to ensure sufficient alignment with the edit prompt. Previous attempts to address this issue typically employ backbone-specific feature manipulations, limiting general applicability. To address these challenges, we propose FlowBypass, a novel and analytical framework grounded in Rectified Flow that constructs a bypass directly connecting inversion and reconstruction trajectories, thereby mitigating error accumulation without relying on feature manipulations. We provide a formal derivation of two trajectories, from which we obtain an approximate bypass formulation and its numerical solution, enabling seamless trajectory transitions. Extensive experiments demonstrate that FlowBypass consistently outperforms state-of-the-art image editing methods, achieving stronger prompt alignment while preserving high-fidelity details in irrelevant regions.

</details>


### [216] [LDRNet: Large Deformation Registration Model for Chest CT Registration](https://arxiv.org/abs/2602.01812)
*Cheng Wang,Qiyu Gao,Fandong Zhang,Shu Zhang,Yizhou Yu*

Main category: cs.CV

TL;DR: 本文提出了一种用于胸部CT图像大变形配准的无监督深度学习方法LDRNet，相较于脑部配准，胸部配准更具挑战且变形更大。新方法显著优于现有传统和深度学习配准方法，且速度更快。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习图像配准主要集中在脑部图像，胸部CT因变形大且结构复杂，现有方法难以适应，需更高效的配准算法。

Method: 提出LDRNet模型，先预测粗配准场，再进行粗到细的逐步精细化。其主要创新为：1) refine block用于不同分辨率上的配准场精化；2) rigid block对高阶特征学习刚性变换矩阵。模型在私有和SegTHOR公开数据集上训练和评估，并与主流配准方法作对比。

Result: LDRNet在大变形图像配准任务中，性能达到当前最优（state-of-the-art），且速度明显快于对比方法（如VoxelMorph、RCN、LapIRN等）。

Conclusion: LDRNet为胸部CT配准的高效、精确算法，克服了大变形等难题，可广泛应用于医学图像分析领域。

Abstract: Most of the deep learning based medical image registration algorithms focus on brain image registration tasks.Compared with brain registration, the chest CT registration has larger deformation, more complex background and region over-lap. In this paper, we propose a fast unsupervised deep learning method, LDRNet, for large deformation image registration of chest CT images. We first predict a coarse resolution registration field, then refine it from coarse to fine. We propose two innovative technical components: 1) a refine block that is used to refine the registration field in different resolutions, 2) a rigid block that is used to learn transformation matrix from high-level features. We train and evaluate our model on the private dataset and public dataset SegTHOR. We compare our performance with state-of-the-art traditional registration methods as well as deep learning registration models VoxelMorph, RCN, and LapIRN. The results demonstrate that our model achieves state-of-the-art performance for large deformation images registration and is much faster.

</details>


### [217] [GPD: Guided Progressive Distillation for Fast and High-Quality Video Generation](https://arxiv.org/abs/2602.01814)
*Xiao Liang,Yunzhu Zhang,Linchao Zhu*

Main category: cs.CV

TL;DR: 本论文提出了一种新的加速视频生成扩散模型的方法——Guided Progressive Distillation（GPD），能够在极大减少采样步骤的同时维持高质量输出。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在视频生成方面表现优异，但其去噪步骤计算量巨大，限制了实际应用。现有加速方法虽可减少步骤，但在视频生成任务中易导致显著画质下降，因此亟需一种兼具高效与高质量的新方法。

Method: GPD方法通过“教师-学生”策略，让教师模型逐步引导学生模型以更大步长运行，包含：（1）在线生成训练目标以简化优化并提升效率；（2）在潜空间引入频域约束以保持细节和时间动态。

Result: GPD方法在Wan2.1模型上将采样步数从48步减少到6步，同时在VBench评测上保持了有竞争力的视觉质量。

Conclusion: GPD相较于现有知识蒸馏法，在流程简便性和画质保留方面均展现出明显优势，是视频扩散模型加速的有效方案。

Abstract: Diffusion models have achieved remarkable success in video generation; however, the high computational cost of the denoising process remains a major bottleneck. Existing approaches have shown promise in reducing the number of diffusion steps, but they often suffer from significant quality degradation when applied to video generation. We propose Guided Progressive Distillation (GPD), a framework that accelerates the diffusion process for fast and high-quality video generation. GPD introduces a novel training strategy in which a teacher model progressively guides a student model to operate with larger step sizes. The framework consists of two key components: (1) an online-generated training target that reduces optimization difficulty while improving computational efficiency, and (2) frequency-domain constraints in the latent space that promote the preservation of fine-grained details and temporal dynamics. Applied to the Wan2.1 model, GPD reduces the number of sampling steps from 48 to 6 while maintaining competitive visual quality on VBench. Compared with existing distillation methods, GPD demonstrates clear advantages in both pipeline simplicity and quality preservation.

</details>


### [218] [Seeing Is Believing? A Benchmark for Multimodal Large Language Models on Visual Illusions and Anomalies](https://arxiv.org/abs/2602.01816)
*Wenjin Hou,Wei Liu,Han Hu,Xiaoxiao Sun,Serena Yeung-Levy,Hehe Fan*

Main category: cs.CV

TL;DR: 论文提出了一个名为VIA-Bench的新基准，用来评估多模态大语言模型（MLLMs）在面对视觉错觉和视觉异常时的表现，发现当前主流模型在这些场景下表现不佳。


<details>
  <summary>Details</summary>
Motivation: 虽然MLLMs在常规视觉-语言基准测试中表现优异，但这些测试多为分布内数据，缺乏对模型在“反常情境”下的稳健性评价，因此亟需更具挑战性的数据集来探索模型面对非常规视觉信息时的能力弱点。

Method: 作者构建了VIA-Bench，包含六大类（如颜色错觉、运动错觉、格式塔错觉、几何空间错觉、一般视觉错觉和视觉异常）共计1000余组高质量问答对，通过“人类审核”确保题目质量，并用其系统测试了超过20个先进MLLMs（含闭源、开源和推理增强型模型），重点考察它们对错觉刺激的表现。

Result: 实验结果显示，现有MLLMs在VIA-Bench测试中普遍脆弱，即便是引入Chain-of-Thought（CoT）推理技巧也并未显著提升模型鲁棒性，反而在面对错觉刺激时出现推理逻辑崩溃的“海市蜃楼型”错误，暴露出模型与人类感知间的根本性差异。

Conclusion: 模型在视觉错觉和异常场景下表现出的脆弱性表明，MLLMs在人工通用智能（AGI）之路上存在感知能力瓶颈，未来需要有针对性地解决感知和认知层面的差距。数据集与代码将对外公开，便于后续研究。

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable proficiency on general-purpose vision-language benchmarks, reaching or even exceeding human-level performance. However, these evaluations typically rely on standard in-distribution data, leaving the robustness of MLLMs largely unexamined when faced with scenarios that defy common-sense priors. To address this gap, we introduce VIA-Bench, a challenging benchmark designed to probe model performance on visual illusions and anomalies. It includes six core categories: color illusions, motion illusions, gestalt illusions, geometric and spatial illusions, general visual illusions, and visual anomalies. Through careful human-in-the-loop review, we construct over 1K high-quality question-answer pairs that require nuanced visual reasoning. Extensive evaluation of over 20 state-of-the-art MLLMs, including proprietary, open-source, and reasoning-enhanced models, uncovers significant vulnerabilities. Notably, we find that Chain-of-Thought (CoT) reasoning offers negligible robustness, often yielding ``brittle mirages'' where the model's logic collapses under illusory stimuli. Our findings reveal a fundamental divergence between machine and human perception, suggesting that resolving such perceptual bottlenecks is critical for the advancement of artificial general intelligence. The benchmark data and code will be released.

</details>


### [219] [Efficient Cross-Country Data Acquisition Strategy for ADAS via Street-View Imagery](https://arxiv.org/abs/2602.01836)
*Yin Wu,Daniel Slieter,Carl Esselborn,Ahmed Abouelazm,Tsung Yuan Tseng,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 针对自动驾驶系统在不同国家部署时，由于法规与交通环境差异导致数据领域转移、感知性能下降的问题，提出利用街景图像辅助采集数据的新方法，可有效并低成本选取代表性地点，提高跨国模型适应效率。


<details>
  <summary>Details</summary>
Motivation: 跨国部署自动驾驶感知系统时，传统数据采集方法依赖大量实际道路行驶，成本高、效率低，且难以系统筛选数据的代表性地点，因此亟需高效、经济的自动化采集策略。

Method: 提出街景图像引导的数据采集方法。首先利用公开街景图像，通过KNN特征距离和视觉-语言联合属性两种评分方式，筛选感兴趣地点。采用collect-detect协议，通过Zenseact Open Dataset与Mapillary数据集配对，构建跨领域共址数据集。以交通标志检测为实验任务验证方法有效性。

Result: 在交通标志跨国检测任务上，该方法用一半目标域数据就可达到与随机采样接近的性能。对整个国家级别分析进行成本估算，证明大规模街景图像处理经济可行。

Conclusion: 街景图像引导的数据采集策略能高效、低成本地提升自动驾驶感知系统的跨国适应性，为解决领域转移难题带来新思路。

Abstract: Deploying ADAS and ADS across countries remains challenging due to differences in legislation, traffic infrastructure, and visual conventions, which introduce domain shifts that degrade perception performance. Traditional cross-country data collection relies on extensive on-road driving, making it costly and inefficient to identify representative locations. To address this, we propose a street-view-guided data acquisition strategy that leverages publicly available imagery to identify places of interest (POI). Two POI scoring methods are introduced: a KNN-based feature distance approach using a vision foundation model, and a visual-attribution approach using a vision-language model. To enable repeatable evaluation, we adopt a collect-detect protocol and construct a co-located dataset by pairing the Zenseact Open Dataset with Mapillary street-view images. Experiments on traffic sign detection, a task particularly sensitive to cross-country variations in sign appearance, show that our approach achieves performance comparable to random sampling while using only half of the target-domain data. We further provide cost estimations for full-country analysis, demonstrating that large-scale street-view processing remains economically feasible. These results highlight the potential of street-view-guided data acquisition for efficient and cost-effective cross-country model adaptation.

</details>


### [220] [SPIRIT: Adapting Vision Foundation Models for Unified Single- and Multi-Frame Infrared Small Target Detection](https://arxiv.org/abs/2602.01843)
*Qian Xu,Xi Li,Fei Gao,Jie Guo,Haojuan Yuan,Shuaipeng Fan,Mingjin Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉基础模型（VFM）和物理知识的插件结构SPIRIT，用于统一解决红外弱小目标检测在单帧与多帧（视频）场景下的难题。该方法采用空间与时间联合的处理机制，大幅提升了检测效果。


<details>
  <summary>Details</summary>
Motivation: 红外弱小目标检测在早期预警和监控领域至关重要。但由于红外数据稀缺及其信号弱、语义线索少，直接利用依赖可见光语义的基础模型和基于外观的帧间关联效果不佳，因此亟需一种既能利用VFM优势，又能适应红外特性的检测方法。

Method: 提出SPIRIT框架，通过轻量级的物理知识插件对VFM进行适配。空间上，PIFR模块借鉴秩稀疏分解，抑制背景、增强稀疏目标信号；时间上，PGMA模块将历史帧信息通过软空间先验引入跨帧记忆注意力，实现稳健的视频检测，并在无时间上下文时自动退化为单帧检测。

Result: 在多个红外弱小目标检测基准数据集上，SPIRIT表现出色，提升了VFM 基线和当前最优方法（SOTA）的性能。

Conclusion: SPIRIT 框架有效克服了红外与可见光模态差异带来的挑战，通过空间和时间的物理知识引导，实现了对红外弱小目标的高效检测和适应，可推广至实际监控与预警系统。

Abstract: Infrared small target detection (IRSTD) is crucial for surveillance and early-warning, with deployments spanning both single-frame analysis and video-mode tracking. A practical solution should leverage vision foundation models (VFMs) to mitigate infrared data scarcity, while adopting a memory-attention-based temporal propagation framework that unifies single- and multi-frame inference. However, infrared small targets exhibit weak radiometric signals and limited semantic cues, which differ markedly from visible-spectrum imagery. This modality gap makes direct use of semantics-oriented VFMs and appearance-driven cross-frame association unreliable for IRSTD: hierarchical feature aggregation can submerge localized target peaks, and appearance-only memory attention becomes ambiguous, leading to spurious clutter associations. To address these challenges, we propose SPIRIT, a unified and VFM-compatible framework that adapts VFMs to IRSTD via lightweight physics-informed plug-ins. Spatially, PIFR refines features by approximating rank-sparsity decomposition to suppress structured background components and enhance sparse target-like signals. Temporally, PGMA injects history-derived soft spatial priors into memory cross-attention to constrain cross-frame association, enabling robust video detection while naturally reverting to single-frame inference when temporal context is absent. Experiments on multiple IRSTD benchmarks show consistent gains over VFM-based baselines and SOTA performance.

</details>


### [221] [CloDS: Visual-Only Unsupervised Cloth Dynamics Learning in Unknown Conditions](https://arxiv.org/abs/2602.01844)
*Yuliang Zhan,Jian Li,Wenbing Huang,Wenbing Huang,Yang Liu,Hao Sun*

Main category: cs.CV

TL;DR: 本文提出了一种无需已知物理属性，即可从多视角视觉观测中无监督学习布料动力学的新方法（CloDS），在保持良好泛化能力的同时，能有效地从视觉数据学习布料动力学。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习在模拟复杂动态系统方面表现突出，但通常依赖于已知的物理属性来进行监督，这限制了其在未知条件下的实用性。为此，研究者希望探索在缺乏物理属性先验知识下，如何仅依赖视觉信息学习布料动力学。

Method: 作者制定了布料动力学学习的新场景（CDG），并提出了CloDS框架。CloDS采用三阶段流程：首先进行视频到几何的对齐（grounding），然后在获得的网格上训练动力学模型；同时在对齐阶段引入了基于高斯溅射（Gaussian splatting）的双位置不透明度调节机制，用于解决大尺度非线性变形和自遮挡问题，支持2D到3D的双向映射，并同时考虑高斯组件的绝对与相对位置。

Result: 经过大量实验，CloDS能够有效从视觉数据学习出布料动力学，对于未见过的新配置同样有很好的泛化能力。

Conclusion: CloDS证明了在无监督情况下仅从多视角视觉数据学习布料动力学的可行性与有效性，有望拓展动力学仿真在现实世界复杂场景中的应用。

Abstract: Deep learning has demonstrated remarkable capabilities in simulating complex dynamic systems. However, existing methods require known physical properties as supervision or inputs, limiting their applicability under unknown conditions. To explore this challenge, we introduce Cloth Dynamics Grounding (CDG), a novel scenario for unsupervised learning of cloth dynamics from multi-view visual observations. We further propose Cloth Dynamics Splatting (CloDS), an unsupervised dynamic learning framework designed for CDG. CloDS adopts a three-stage pipeline that first performs video-to-geometry grounding and then trains a dynamics model on the grounded meshes. To cope with large non-linear deformations and severe self-occlusions during grounding, we introduce a dual-position opacity modulation that supports bidirectional mapping between 2D observations and 3D geometry via mesh-based Gaussian splatting in video-to-geometry grounding stage. It jointly considers the absolute and relative position of Gaussian components. Comprehensive experimental evaluations demonstrate that CloDS effectively learns cloth dynamics from visual data while maintaining strong generalization capabilities for unseen configurations. Our code is available at https://github.com/whynot-zyl/CloDS. Visualization results are available at https://github.com/whynot-zyl/CloDS_video}.%\footnote{As in this example.

</details>


### [222] [WS-IMUBench: Can Weakly Supervised Methods from Audio, Image, and Video Be Adapted for IMU-based Temporal Action Localization?](https://arxiv.org/abs/2602.01850)
*Pei Li,Jiaxi Yin,Lei Ouyang,Shihan Pan,Ge Wang,Han Ding,Fei Wang*

Main category: cs.CV

TL;DR: 本文提出WS-IMUBench，对IMU设备下的弱监督时序动作定位（WS-IMU-TAL）进行系统性基准分析，发现现有弱监督方法在该任务中面临新挑战，并给出未来改进方向。


<details>
  <summary>Details</summary>
Motivation: 传统基于IMU的人体活动识别方法多为分类，不能反映日常行为的丰富时间结构。而现有的时序动作定位（IMU-TAL）方法需要密集的帧级标注，成本高、难以扩展。为解决这一瓶颈，作者关注只用序列级标签的弱监督IMU-TAL，并希望评估已有弱监督方法的相关性和有效性。

Method: 作者没有提出新的算法，而是构建了WS-IMUBench基准，系统性评测音频、图像和视频领域七种典型弱监督定位方法在IMU-TAL任务中的迁移能力。实验涵盖七个IMU公开数据集，进行了超过3,540次模型训练和7,080次推断评测，并围绕三大研究问题进行分析。

Result: 实验发现，迁移性能依赖于数据模态，时间域弱监督方法普遍比基于图像方案更稳定。在某些特定数据集（如长时动作、高维传感）下，弱监督表现接近有监督，但在处理短动作、时间模糊及建议框质量方面存在突出短板。

Conclusion: WS-IMUBench建立了可复现的评测流程和分析范例，有助于推动WS-IMU-TAL领域的进步。作者提出未来可关注IMU专属提案生成、边界感知目标和更强时序建模等方向。

Abstract: IMU-based Human Activity Recognition (HAR) has enabled a wide range of ubiquitous computing applications, yet its dominant clip classification paradigm cannot capture the rich temporal structure of real-world behaviors. This motivates a shift toward IMU Temporal Action Localization (IMU-TAL), which predicts both action categories and their start/end times in continuous streams. However, current progress is strongly bottlenecked by the need for dense, frame-level boundary annotations, which are costly and difficult to scale. To address this bottleneck, we introduce WS-IMUBench, a systematic benchmark study of weakly supervised IMU-TAL (WS-IMU-TAL) under only sequence-level labels. Rather than proposing a new localization algorithm, we evaluate how well established weakly supervised localization paradigms from audio, image, and video transfer to IMU-TAL under only sequence-level labels. We benchmark seven representative weakly supervised methods on seven public IMU datasets, resulting in over 3,540 model training runs and 7,080 inference evaluations. Guided by three research questions on transferability, effectiveness, and insights, our findings show that (i) transfer is modality-dependent, with temporal-domain methods generally more stable than image-derived proposal-based approaches; (ii) weak supervision can be competitive on favorable datasets (e.g., with longer actions and higher-dimensional sensing); and (iii) dominant failure modes arise from short actions, temporal ambiguity, and proposal quality. Finally, we outline concrete directions for advancing WS-IMU-TAL (e.g., IMU-specific proposal generation, boundary-aware objectives, and stronger temporal reasoning). Beyond individual results, WS-IMUBench establishes a reproducible benchmarking template, datasets, protocols, and analyses, to accelerate community-wide progress toward scalable WS-IMU-TAL.

</details>


### [223] [How Well Do Models Follow Visual Instructions? VIBE: A Systematic Benchmark for Visual Instruction-Driven Image Editing](https://arxiv.org/abs/2602.01851)
*Huanyu Zhang,Xuehai Bai,Chengzu Li,Chen Liang,Haochen Tian,Haodong Li,Ruichuan An,Yifan Zhang,Anna Korhonen,Zhang Zhang,Liang Wang,Tieniu Tan*

Main category: cs.CV

TL;DR: 提出了一个用于图像编辑的视觉指令基准（VIBE），系统评测多种模型在跟随不同复杂程度视觉指令时的表现。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型和评测主要依赖文本指令，而人类交流通常包含视觉信息（如草图），缺乏相应的视觉多模态指令评测机制。

Method: 搭建了VIBE基准，构建三层视觉指令层级（指示性、形态操作、因果推理），涵盖不同复杂程度的一系列高质量测试样例，并提出基于大模型“法官”+任务专属指标的评测框架。

Result: 评测了17个主流开源与专有图像编辑模型，发现专有模型在视觉指令跟随方面初具能力并优于开源模型，但在任务难度升高时所有模型性能均明显下降。

Conclusion: 当前模型在处理复杂视觉指令时仍有明显不足，新基准和评测框架为未来研究指明了重要方向。

Abstract: Recent generative models have achieved remarkable progress in image editing. However, existing systems and benchmarks remain largely text-guided. In contrast, human communication is inherently multimodal, where visual instructions such as sketches efficiently convey spatial and structural intent. To address this gap, we introduce VIBE, the Visual Instruction Benchmark for Image Editing with a three-level interaction hierarchy that captures deictic grounding, morphological manipulation, and causal reasoning. Across these levels, we curate high-quality and diverse test cases that reflect progressively increasing complexity in visual instruction following. We further propose a robust LMM-as-a-judge evaluation framework with task-specific metrics to enable scalable and fine-grained assessment. Through a comprehensive evaluation of 17 representative open-source and proprietary image editing models, we find that proprietary models exhibit early-stage visual instruction-following capabilities and consistently outperform open-source models. However, performance degrades markedly with increasing task difficulty even for the strongest systems, highlighting promising directions for future research.

</details>


### [224] [Fact or Fake? Assessing the Role of Deepfake Detectors in Multimodal Misinformation Detection](https://arxiv.org/abs/2602.01854)
*A S M Sharifuzzaman Sagar,Mohammed Bennamoun,Farid Boussaid,Naeha Sharif,Lian Xu,Shaaban Sahmoud,Ali Kishk*

Main category: cs.CV

TL;DR: 本文系统分析了在多模态虚假信息检测中，现有深度伪造检测器的作用，发现其对图片文本联合推理价值有限，反而可能降低事实核查系统性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态虚假信息往往通过图文配对表达虚假语义，但现有深度伪造检测器主要关注像素级篡改，尚不清楚其对实际事实核查的贡献和潜在负面作用。为科学评估该类工具在自动化事实核查流程中的实际效益，提出该项研究。

Method: 采用两个多模态虚假信息基准数据集（MMFakeBench和DGM4），对比评估主流图片深度伪造检测器、以证据驱动的事实核查系统（包含MCTS检索和MAD推理）、以及融合检测器结果的混合系统，并系统性对比各模型效果。

Result: 深度伪造检测器自身在两个数据集上的F1分数仅0.26-0.53（MMFakeBench）和0.33-0.49（DGM4）；其作为辅助输入时反而造成事实核查系统整体F1下降0.04-0.08。相较之下，证据驱动的核查系统效果最佳，F1高达0.81（MMFakeBench）和0.55（DGM4）。

Conclusion: 联合图文虚假信息查证主要依赖对语义和外部证据的理解，像素级伪造信号对推理贡献有限，过度依赖深度伪造检测器反而削弱对真实虚假信息的判别能力。

Abstract: In multimodal misinformation, deception usually arises not just from pixel-level manipulations in an image, but from the semantic and contextual claim jointly expressed by the image-text pair. Yet most deepfake detectors, engineered to detect pixel-level forgeries, do not account for claim-level meaning, despite their growing integration in automated fact-checking (AFC) pipelines. This raises a central scientific and practical question: Do pixel-level detectors contribute useful signal for verifying image-text claims, or do they instead introduce misleading authenticity priors that undermine evidence-based reasoning? We provide the first systematic analysis of deepfake detectors in the context of multimodal misinformation detection. Using two complementary benchmarks, MMFakeBench and DGM4, we evaluate: (1) state-of-the-art image-only deepfake detectors, (2) an evidence-driven fact-checking system that performs tool-guided retrieval via Monte Carlo Tree Search (MCTS) and engages in deliberative inference through Multi-Agent Debate (MAD), and (3) a hybrid fact-checking system that injects detector outputs as auxiliary evidence. Results across both benchmark datasets show that deepfake detectors offer limited standalone value, achieving F1 scores in the range of 0.26-0.53 on MMFakeBench and 0.33-0.49 on DGM4, and that incorporating their predictions into fact-checking pipelines consistently reduces performance by 0.04-0.08 F1 due to non-causal authenticity assumptions. In contrast, the evidence-centric fact-checking system achieves the highest performance, reaching F1 scores of approximately 0.81 on MMFakeBench and 0.55 on DGM4. Overall, our findings demonstrate that multimodal claim verification is driven primarily by semantic understanding and external evidence, and that pixel-level artifact signals do not reliably enhance reasoning over real-world image-text misinformation.

</details>


### [225] [Trust but Verify: Adaptive Conditioning for Reference-Based Diffusion Super-Resolution via Implicit Reference Correlation Modeling](https://arxiv.org/abs/2602.01864)
*Yuan Wang,Yuhao Wan,Siming Zheng,Bo Li,Qibin Hou,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种自适应的参考图像超分辨率方法Ada-RefSR，有效解决了降质情况下参考图像信息利用的问题，在多个数据集上取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 在基于扩散模型的图像修复任务中，利用参考图像可提升效果，但真实场景下低质输入与参考图之间的关联性弱，传统方法难以可靠利用参考信息，易导致信息错误融合或有用线索的忽略。

Method: 提出了Ada-RefSR框架，核心为自适应隐式相关门控（AICG）模块，通过可学习的摘要token提取主要的参考图案，并与低质特征之间隐式建模相关性，集成于注意力模块中，以轻量级方式动态调节参考信息的引入。

Result: Ada-RefSR在多个公开数据集上表现优异，兼顾了超分辨率的保真度、自然性和计算效率，且在参考图对齐程度变化的情况下更具鲁棒性。

Conclusion: Ada-RefSR实现了对参考信息可控、可靠的利用，为扩散模型图像复原提供了高效且实用的新途径，可推广至更复杂的应用场景。

Abstract: Recent works have explored reference-based super-resolution (RefSR) to mitigate hallucinations in diffusion-based image restoration. A key challenge is that real-world degradations make correspondences between low-quality (LQ) inputs and reference (Ref) images unreliable, requiring adaptive control of reference usage. Existing methods either ignore LQ-Ref correlations or rely on brittle explicit matching, leading to over-reliance on misleading references or under-utilization of valuable cues. To address this, we propose Ada-RefSR, a single-step diffusion framework guided by a "Trust but Verify" principle: reference information is leveraged when reliable and suppressed otherwise. Its core component, Adaptive Implicit Correlation Gating (AICG), employs learnable summary tokens to distill dominant reference patterns and capture implicit correlations with LQ features. Integrated into the attention backbone, AICG provides lightweight, adaptive regulation of reference guidance, serving as a built-in safeguard against erroneous fusion. Experiments on multiple datasets demonstrate that Ada-RefSR achieves a strong balance of fidelity, naturalness, and efficiency, while remaining robust under varying reference alignment.

</details>


### [226] [ProxyImg: Towards Highly-Controllable Image Representation via Hierarchical Disentangled Proxy Embedding](https://arxiv.org/abs/2602.01881)
*Ye Chen,Yupeng Zhu,Xiongzhen Zhang,Zhewen Wan,Yingzhe Li,Wenjun Zhang,Bingbing Ni*

Main category: cs.CV

TL;DR: 提出了一种基于分层代理的参数化图像表示方法，实现了高效、可控和高保真的图像/视频编辑，对比现有主流方法具备更低参数量和更直观精细的操控能力。


<details>
  <summary>Details</summary>
Motivation: 传统的图像表示方法（如栅格图像、高斯原语、隐式表示）存在表示冗余或缺乏语义对应的问题，导致手动编辑繁琐或难以进行精细控制。限制了图像和视频的高效、可控编辑能力。

Method: 提出分离语义、几何、纹理参数的分层代理表示。通过语义感知图像分解，自适应Bezier拟合和区域细分构建分层几何代理结构，并在代理节点嵌入多尺度隐式纹理参数，还引入区域自适应特征索引机制以增强空间纹理连续性，无需生成模型可做高质量背景补全。同时结合基于位置的动力学，实现实时物理动画。

Result: 在ImageNet、OIR-Bench、HumanEdit等数据集上进行图像重建与编辑的实验，方法在渲染保真度、参数量、编辑交互性和物理真实感上均达到或超过SOTA水平。

Conclusion: 该方法不仅大幅减少了参数用量，提升了编辑直观性与灵活性，还可以实现高质量、高一致性的物理动画渲染，是高效可控图像/视频表示与编辑的有力方案。

Abstract: Prevailing image representation methods, including explicit representations such as raster images and Gaussian primitives, as well as implicit representations such as latent images, either suffer from representation redundancy that leads to heavy manual editing effort, or lack a direct mapping from latent variables to semantic instances or parts, making fine-grained manipulation difficult. These limitations hinder efficient and controllable image and video editing. To address these issues, we propose a hierarchical proxy-based parametric image representation that disentangles semantic, geometric, and textural attributes into independent and manipulable parameter spaces. Based on a semantic-aware decomposition of the input image, our representation constructs hierarchical proxy geometries through adaptive Bezier fitting and iterative internal region subdivision and meshing. Multi-scale implicit texture parameters are embedded into the resulting geometry-aware distributed proxy nodes, enabling continuous high-fidelity reconstruction in the pixel domain and instance- or part-independent semantic editing. In addition, we introduce a locality-adaptive feature indexing mechanism to ensure spatial texture coherence, which further supports high-quality background completion without relying on generative models. Extensive experiments on image reconstruction and editing benchmarks, including ImageNet, OIR-Bench, and HumanEdit, demonstrate that our method achieves state-of-the-art rendering fidelity with significantly fewer parameters, while enabling intuitive, interactive, and physically plausible manipulation. Moreover, by integrating proxy nodes with Position-Based Dynamics, our framework supports real-time physics-driven animation using lightweight implicit rendering, achieving superior temporal consistency and visual realism compared with generative approaches.

</details>


### [227] [Q Cache: Visual Attention is Valuable in Less than Half of Decode Layers for Multimodal Large Language Model](https://arxiv.org/abs/2602.01901)
*Jiedong Zhuang,Lu Lu,Ming Dai,Rui Hu,Jian Chen,Qiang Liu,Haoji Hu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Lazy Attention的高效注意力机制，通过跨层共享相似的注意力模式，大幅降低了多模态大语言模型（MLLMs）推理过程中的计算和缓存开销。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs由于视觉编码器产生海量视觉token，导致计算量和KV缓存需求居高不下。虽然已有各种token剪枝方法降低开销，但常损害KV缓存完整性、影响长文本生成能力，因此作者寻求新的优化途径。

Method: 论文通过深入分析模型的注意力机制，发现超过一半的解码层间注意力高度语义相似，提出跨层继承注意力信息的方法Lazy Attention；设计轻量级、可与现有推理框架兼容的Q Cache，用于层间查询的复用，并可与已有token剪枝技术结合使用。

Result: 在多个基准测试上，所提方法可减少35%以上的KV缓存使用，提升1.5倍推理吞吐，仅轻微（约1%）降低性能，且较先进token剪枝方法在准确性保持上更优。

Conclusion: Lazy Attention通过层间注意力共享，有效降低了MLLMs推理成本，并在大幅减少缓存和计算资源消耗的同时保持了性能，兼容性强且实用价值高。

Abstract: Multimodal large language models (MLLMs) are plagued by exorbitant inference costs attributable to the profusion of visual tokens within the vision encoder. The redundant visual tokens engenders a substantial computational load and key-value (KV) cache footprint bottleneck. Existing approaches focus on token-wise optimization, leveraging diverse intricate token pruning techniques to eliminate non-crucial visual tokens. Nevertheless, these methods often unavoidably undermine the integrity of the KV cache, resulting in failures in long-text generation tasks. To this end, we conduct an in-depth investigation towards the attention mechanism of the model from a new perspective, and discern that attention within more than half of all decode layers are semantic similar. Upon this finding, we contend that the attention in certain layers can be streamlined by inheriting the attention from their preceding layers. Consequently, we propose Lazy Attention, an efficient attention mechanism that enables cross-layer sharing of similar attention patterns. It ingeniously reduces layer-wise redundant computation in attention. In Lazy Attention, we develop a novel layer-shared cache, Q Cache, tailored for MLLMs, which facilitates the reuse of queries across adjacent layers. In particular, Q Cache is lightweight and fully compatible with existing inference frameworks, including Flash Attention and KV cache. Additionally, our method is highly flexible as it is orthogonal to existing token-wise techniques and can be deployed independently or combined with token pruning approaches. Empirical evaluations on multiple benchmarks demonstrate that our method can reduce KV cache usage by over 35% and achieve 1.5x throughput improvement, while sacrificing only approximately 1% of performance on various MLLMs. Compared with SOTA token-wise methods, our technique achieves superior accuracy preservation.

</details>


### [228] [Learning Sparse Visual Representations via Spatial-Semantic Factorization](https://arxiv.org/abs/2602.01905)
*Theodore Zhengde Zhao,Sid Kiblawi,Jianwei Yang,Naoto Usuyama,Reuben Tan,Noel C Codella,Tristan Naumann,Hoifung Poon,Mu Wei*

Main category: cs.CV

TL;DR: STELLAR提出了一种新型自监督学习框架，通过分解视觉特征，兼顾了高层语义理解和像素级重建，弥合了生成式和判别式自监督学习之间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法在高层语义理解和图像重建之间存在基本冲突，两者难以兼得：如DINO舍弃空间信息专注语义，而MAE保留空间信息却缺乏抽象表达。因此需要一种兼顾两者的统一方法。

Method: 提出STELLAR框架，将视觉特征分解为语义概念和空间分布矩阵的低秩乘积。通过这种解耦，同时对语义token做增强对齐（类似DINO），又能通过空间矩阵保持像素级的重建能力。

Result: 通过仅16个稀疏token，在保证高质量图像重建（FID=2.60）的同时，在ImageNet上达到与密集backbone相当的语义分类性能（准确率79.10%）。

Conclusion: STELLAR能有效桥接生成式与判别式视觉自监督学习，通过语义与空间信息分离，成为一种兼具语义理解和高质量重建的高效稀疏视觉表示。

Abstract: Self-supervised learning (SSL) faces a fundamental conflict between semantic understanding and image reconstruction. High-level semantic SSL (e.g., DINO) relies on global tokens that are forced to be location-invariant for augmentation alignment, a process that inherently discards the spatial coordinates required for reconstruction. Conversely, generative SSL (e.g., MAE) preserves dense feature grids for reconstruction but fails to produce high-level abstractions. We introduce STELLAR, a framework that resolves this tension by factorizing visual features into a low-rank product of semantic concepts and their spatial distributions. This disentanglement allows us to perform DINO-style augmentation alignment on the semantic tokens while maintaining the precise spatial mapping in the localization matrix necessary for pixel-level reconstruction. We demonstrate that as few as 16 sparse tokens under this factorized form are sufficient to simultaneously support high-quality reconstruction (2.60 FID) and match the semantic performance of dense backbones (79.10% ImageNet accuracy). Our results highlight STELLAR as a versatile sparse representation that bridges the gap between discriminative and generative vision by strategically separating semantic identity from spatial geometry. Code available at https://aka.ms/stellar.

</details>


### [229] [DSXFormer: Dual-Pooling Spectral Squeeze-Expansion and Dynamic Context Attention Transformer for Hyperspectral Image Classification](https://arxiv.org/abs/2602.01906)
*Farhan Ullah,Irfan Ullah,Khalil Khan,Giovanni Pau,JaKeoung Koo*

Main category: cs.CV

TL;DR: 该论文提出了一种新的变换器模型DSXFormer，用于高光谱图像分类，通过引入双池化谱特征压缩扩展模块和动态上下文注意机制，在保证效率的同时提升光谱区分能力，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类受限于高维光谱信息、复杂的谱-空间相关性和标注样本有限，现有变换器模型难以在效率和光谱判别力之间取得平衡。

Method: 提出DSXFormer模型，包括：1）利用双池化（全局均值和最大池化）谱压缩扩展模块自适应增强光谱特征区分性和波段间依赖建模；2）在窗口型变换器中集成动态上下文注意机制，动态捕获局部谱-空间关系并降低计算量；3）采用补丁提取、嵌入及合并促进多尺度特征学习。

Result: 在Salinas、Indian Pines、Pavia University和Kennedy Space Center四个高光谱数据集上，DSXFormer的准确率分别达到99.95%、98.91%、99.85%和98.52%，均超过现有先进方法。

Conclusion: DSXFormer有效提升了高光谱分类的光谱区分能力和计算效率，为高光谱图像分析提供了新的高效解决方案。

Abstract: Hyperspectral image classification (HSIC) is a challenging task due to high spectral dimensionality, complex spectral-spatial correlations, and limited labeled training samples. Although transformer-based models have shown strong potential for HSIC, existing approaches often struggle to achieve sufficient spectral discriminability while maintaining computational efficiency. To address these limitations, we propose a novel DSXFormer, a novel dual-pooling spectral squeeze-expansion transformer with Dynamic Context Attention for HSIC. The proposed DSXFormer introduces a Dual-Pooling Spectral Squeeze-Expansion (DSX) block, which exploits complementary global average and max pooling to adaptively recalibrate spectral feature channels, thereby enhancing spectral discriminability and inter-band dependency modeling. In addition, DSXFormer incorporates a Dynamic Context Attention (DCA) mechanism within a window-based transformer architecture to dynamically capture local spectral-spatial relationships while significantly reducing computational overhead. The joint integration of spectral dual-pooling squeeze-expansion and DCA enables DSXFormer to achieve an effective balance between spectral emphasis and spatial contextual representation. Furthermore, patch extraction, embedding, and patch merging strategies are employed to facilitate efficient multi-scale feature learning. Extensive experiments conducted on four widely used hyperspectral benchmark datasets, including Salinas (SA), Indian Pines (IP), Pavia University (PU), and Kennedy Space Center (KSC), demonstrate that DSXFormer consistently outperforms state-of-the-art methods, achieving classification accuracies of 99.95%, 98.91%, 99.85%, and 98.52%, respectively.

</details>


### [230] [Enabling Progressive Whole-slide Image Analysis with Multi-scale Pyramidal Network](https://arxiv.org/abs/2602.01951)
*Shuyang Wu,Yifu Qiu,Ines P. Nearchou,Sandrine Prost,Jonathan A Fallowfield,Hakan Bilen,Timothy J Kendall*

Main category: cs.CV

TL;DR: 本文提出了一种新的多尺度金字塔网络（MSPN），可以简便地集成进现有的基于注意力的多实例学习（MIL）框架，用于计算病理任务，并能够有效提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于多尺度特征的MIL方法在融合多放大倍数信息时存在不足，比如只能在最后融合特征、忽略尺度间关联，并且依赖设备厂商定义的放大倍数，导致方法不灵活且计算开销大。本文旨在通过提出更高效、通用的多尺度特征提取和融合方式，提升病理图像分析的准确性和便利性。

Method: 作者提出了多尺度金字塔网络（MSPN），其主要包括：1）基于网格的重映射，利用高放大倍数特征推导出低分辨率的粗略特征；2）粗尺度引导网络（CGN），用于学习粗尺度上下文信息。MSPN模块可以作为插件集成到主流基于注意力的MIL框架中，实现渐进式多尺度分析。

Result: 在4种注意力MIL框架、4个临床相关任务和3种基础模型（包括预训练MIL）上，作者将MSPN作为模块进行对比实验，结果显示MSPN无论配置如何，都能稳定提升MIL的表现。

Conclusion: MSPN是一种轻量、易用且有效的多尺度分析模块，能显著提升基于注意力的MIL方法在计算病理中的表现。

Abstract: Multiple-instance Learning (MIL) is commonly used to undertake computational pathology (CPath) tasks, and the use of multi-scale patches allows diverse features across scales to be learned. Previous studies using multi-scale features in clinical applications rely on multiple inputs across magnifications with late feature fusion, which does not retain the link between features across scales while the inputs are dependent on arbitrary, manufacturer-defined magnifications, being inflexible and computationally expensive. In this paper, we propose the Multi-scale Pyramidal Network (MSPN), which is plug-and-play over attention-based MIL that introduces progressive multi-scale analysis on WSI. Our MSPN consists of (1) grid-based remapping that uses high magnification features to derive coarse features and (2) the coarse guidance network (CGN) that learns coarse contexts. We benchmark MSPN as an add-on module to 4 attention-based frameworks using 4 clinically relevant tasks across 3 types of foundation model, as well as the pre-trained MIL framework. We show that MSPN consistently improves MIL across the compared configurations and tasks, while being lightweight and easy-to-use.

</details>


### [231] [Beyond Open Vocabulary: Multimodal Prompting for Object Detection in Remote Sensing Images](https://arxiv.org/abs/2602.01954)
*Shuai Yang,Ziyue Huang,Jiaxin Chen,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

TL;DR: 该论文提出了RS-MPOD框架，通过引入视觉和多模态提示，提升了遥感领域开放词汇目标检测的类别指定能力，尤其是在语义不明确或数据分布变化时，相较仅依赖文本提示的方法效果更优。


<details>
  <summary>Details</summary>
Motivation: 遥感场景的开放词汇目标检测往往依赖文本提示来指定检测类别，但由于遥感任务类别具有较强的任务和应用相关语义，预训练的文本-视觉对齐能力不总是可靠，导致类别指定不稳定。

Method: 提出RS-MPOD开放词汇检测框架。该方法不再局限于文本提示，新增视觉提示编码器，能从样本实例中提取基于外观的类别信息，实现无需文本的类别指定。同时，设计了多模态融合模块，在文本与视觉提示均有时对其进行有效整合。

Result: 在多种遥感数据集和细粒度基准测试上，大量实验表明：在类别语义模糊和数据分布变化情况下，视觉提示能提供更可靠的类别指定，多模态提示则在文本语义良好时表现同样优异。

Conclusion: 将视觉和多模态提示引入开放词汇检测，可有效提升遥感领域类别指定的稳定性和灵活性，优于仅依赖文本提示的传统做法。

Abstract: Open-vocabulary object detection in remote sensing commonly relies on text-only prompting to specify target categories, implicitly assuming that inference-time category queries can be reliably grounded through pretraining-induced text-visual alignment. In practice, this assumption often breaks down in remote sensing scenarios due to task- and application-specific category semantics, resulting in unstable category specification under open-vocabulary settings. To address this limitation, we propose RS-MPOD, a multimodal open-vocabulary detection framework that reformulates category specification beyond text-only prompting by incorporating instance-grounded visual prompts, textual prompts, and their multimodal integration. RS-MPOD introduces a visual prompt encoder to extract appearance-based category cues from exemplar instances, enabling text-free category specification, and a multimodal fusion module to integrate visual and textual information when both modalities are available. Extensive experiments on standard, cross-dataset, and fine-grained remote sensing benchmarks show that visual prompting yields more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting provides a flexible alternative that remains competitive when textual semantics are well aligned.

</details>


### [232] [Your AI-Generated Image Detector Can Secretly Achieve SOTA Accuracy, If Calibrated](https://arxiv.org/abs/2602.01973)
*Muli Yang,Gabriel James Goenawan,Henan Wang,Huaiyuan Qin,Chenghao Xu,Yanhua Yang,Fen Fang,Ying Sun,Joo-Hwee Lim,Hongyuan Zhu*

Main category: cs.CV

TL;DR: 论文提出了一种基于贝叶斯决策理论的后置校准方法，通过对模型的logits进行可学习的缩放修正，以增强AI生成图像检测器在分布转移情况下的鲁棒性，无需重新训练即可提高检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测器即便在平衡数据集上训练，测试时仍常因分布转移或隐含先验，导致将伪造图像误判为真实。主要原因是模型过拟合于某些表层特征，这些特征对不同生成方法的不具备泛化性，导致决策阈值错位。

Method: 作者提出了一种基于贝叶斯决策理论的后置校准框架。具体做法是在目标分布的一个小型验证集上，优化一个可学习的标量参数来修正模型输出的logits参数，主干网络保持冻结，且无需真实标签。通过这一参数校准，可以补偿因分布偏移造成的模型输出失衡。

Result: 实验表明，该方法在多个具有挑战性的基准数据集上都显著提升了现有检测模型的鲁棒性和准确性，无需重新训练，方法轻量且高效。

Conclusion: 论文方法为开放环境下AI生成图像的检测带来了可靠且适应性强的新方案，尤其适用于应对分布转移，提高检测器的实用价值和安全性。

Abstract: Despite being trained on balanced datasets, existing AI-generated image detectors often exhibit systematic bias at test time, frequently misclassifying fake images as real. We hypothesize that this behavior stems from distributional shift in fake samples and implicit priors learned during training. Specifically, models tend to overfit to superficial artifacts that do not generalize well across different generation methods, leading to a misaligned decision threshold when faced with test-time distribution shift. To address this, we propose a theoretically grounded post-hoc calibration framework based on Bayesian decision theory. In particular, we introduce a learnable scalar correction to the model's logits, optimized on a small validation set from the target distribution while keeping the backbone frozen. This parametric adjustment compensates for distributional shift in model output, realigning the decision boundary even without requiring ground-truth labels. Experiments on challenging benchmarks show that our approach significantly improves robustness without retraining, offering a lightweight and principled solution for reliable and adaptive AI-generated image detection in the open world. Code is available at https://github.com/muliyangm/AIGI-Det-Calib.

</details>


### [233] [Enhancing Multi-Image Understanding through Delimiter Token Scaling](https://arxiv.org/abs/2602.01984)
*Minyoung Lee,Yeji Park,Dongjun Hwang,Yejin Kim,Seong Joon Oh,Junsuk Choe*

Main category: cs.CV

TL;DR: 本文提出了一种通过缩放分隔符token的隐藏状态来改进大规模视觉-语言模型（LVLM）在多图像任务中的表现的方法，有效阻止了跨图像信息泄漏，提升了在多图像与多文档任务中的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM在处理多张图片时表现下降，主要由于模型无法有效区分不同图片的信息，出现跨图片的信息泄漏现象。虽然已使用分隔符标注图片，但分隔符效果有限。

Method: 提出对分隔符token的隐藏状态进行缩放，以加强同一图片内部的信息交互，抑制不同图片之间的无关交互，从而更好地保护每张图片的特有信息。该方法不增加模型的训练和推理成本。

Result: 在Mantis、MuirBench、MIRB和QBench2等多图像基准上，方法显著提升了性能；在TQABench、MultiNews和WCEP-10等多文档、多表格任务上也有提升。

Conclusion: 通过优化分隔符token，LVLM能更准确地区分和推理多图片、多文档场景的信息，方法高效且无额外计算开销，可广泛提升多模态模型实际应用效果。

Abstract: Large Vision-Language Models (LVLMs) achieve strong performance on single-image tasks, but their performance declines when multiple images are provided as input. One major reason is the cross-image information leakage, where the model struggles to distinguish information across different images. Existing LVLMs already employ delimiter tokens to mark the start and end of each image, yet our analysis reveals that these tokens fail to effectively block cross-image information leakage. To enhance their effectiveness, we propose a method that scales the hidden states of delimiter tokens. This enhances the model's ability to preserve image-specific information by reinforcing intra-image interaction and limiting undesired cross-image interactions. Consequently, the model is better able to distinguish between images and reason over them more accurately. Experiments show performance gains on multi-image benchmarks such as Mantis, MuirBench, MIRB, and QBench2. We further evaluate our method on text-only tasks that require clear distinction. The method improves performance on multi-document and multi-table understanding benchmarks, including TQABench, MultiNews, and WCEP-10. Notably, our method requires no additional training or inference cost.

</details>


### [234] [Leveraging Latent Vector Prediction for Localized Control in Image Generation via Diffusion Models](https://arxiv.org/abs/2602.01991)
*Pablo Domingo-Gregorio,Javier Ruiz-Hidalgo*

Main category: cs.CV

TL;DR: 本文提出了一种在扩散模型中实现对图像局部区域细致控制的新方法，能提升文本驱动图像生成的可控性。


<details>
  <summary>Details</summary>
Motivation: 仅依靠文本指令实现对生成图像细节的精准控制非常困难，目前已有方法多为整体施加条件，对局部细节的个性化控制非常有限，不能满足用户需求。

Method: 作者提出了一种新的训练框架，通过引入掩膜特征（masking features）和新的损失项，使得用户可以对图像的指定区域进行单独控制，而其余区域则由扩散模型根据原始文本自动生成。损失项增强了任意扩散步的当前状态与最终潜在空间样本之间的对应关系。

Result: 实验结果显示，该方法在局部区域可控条件下，能够合成出高质量的图像，验证了方法的有效性。

Conclusion: 该方法为扩散模型在图像生成领域提供了更灵活、更精确的区域控制能力，有助于更好地满足个性化和细粒度图像生成需求。

Abstract: Diffusion models emerged as a leading approach in text-to-image generation, producing high-quality images from textual descriptions. However, attempting to achieve detailed control to get a desired image solely through text remains a laborious trial-and-error endeavor. Recent methods have introduced image-level controls alongside with text prompts, using prior images to extract conditional information such as edges, segmentation and depth maps. While effective, these methods apply conditions uniformly across the entire image, limiting localized control. In this paper, we propose a novel methodology to enable precise local control over user-defined regions of an image, while leaving to the diffusion model the task of autonomously generating the remaining areas according to the original prompt. Our approach introduces a new training framework that incorporates masking features and an additional loss term, which leverages the prediction of the initial latent vector at any diffusion step to enhance the correspondence between the current step and the final sample in the latent space. Extensive experiments demonstrate that our method effectively synthesizes high-quality images with controlled local conditions.

</details>


### [235] [SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors](https://arxiv.org/abs/2602.02000)
*Bing He,Jingnan Gao,Yunuo Chen,Ning Cao,Gang Chen,Zhengxue Cheng,Li Song,Wenjun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于2D高斯溅射（2DGS）的方法SurfSplat，实现了从稀疏图像高保真重建3D场景，并有效克服了现有3DGS方法在高分辨率下出现的伪影问题。


<details>
  <summary>Details</summary>
Motivation: 现有通过3D高斯溅射（3DGS）实现的通用3D重建方法难以在稀疏视角下生成连续表面，经常导致离散、偏色的点云，在高分辨率下细节表现差。为提高几何连续性与重建质量，亟需更优方法。

Method: 提出SurfSplat，一种以2D高斯溅射为基础的前馈型框架，通过引入表面连续性先验与强制alpha混合策略，实现了更高几何精度和表面连贯性重建。还提出了高分辨率渲染一致性（HRRC）新评价指标，用以量化高分辨率下的重建质量。

Result: 在RealEstate10K、DL3DV和ScanNet等多个数据集上，SurfSplat在传统指标和HRRC上均显著优于已有3DGS方法。

Conclusion: SurfSplat为基于稀疏图像的高保真3D重建提供了极具竞争力的解决方案，有效提升了几何和纹理的还原质量。

Abstract: Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: https://hebing-sjtu.github.io/SurfSplat-website/

</details>


### [236] [UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving](https://arxiv.org/abs/2602.02002)
*Guosheng Zhao,Yaozeng Wang,Xiaofeng Wang,Zheng Zhu,Tingdong Yu,Guan Huang,Yongchen Zai,Ji Jiao,Changliang Xue,Xiaole Wang,Zhen Yang,Futang Zhu,Xingang Wang*

Main category: cs.CV

TL;DR: UniDriveDreamer是一个单阶段的统一多模态世界模型，能够同时生成多摄像头视频和LiDAR序列，比现有单一模态方法性能更优。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶数据生成方法多集中在单一模态（如视频或LiDAR），缺乏同时处理多模态的统一模型，难以应用于实际多传感器场景。

Method: 提出了UniDriveDreamer框架，包括为LiDAR序列设计的VAE、为多摄像头视频设计的VAE，并通过Unified Latent Anchoring（ULA）对两种模态的潜在向量分布进行对齐。随后将对齐后的特征融合，并用扩散Transformer建模其几何与时序关系，再融合结构化场景信息作为条件信号指导生成。

Result: 大量实验表明，UniDriveDreamer无论在视频还是LiDAR生成上均优于现有SOTA方法，并提升了下游任务表现。

Conclusion: UniDriveDreamer能够统一生成多模态自动驾驶数据，提升数据合成质量，为自动驾驶世界模型带来进步。

Abstract: World models have demonstrated significant promise for data synthesis in autonomous driving. However, existing methods predominantly concentrate on single-modality generation, typically focusing on either multi-camera video or LiDAR sequence synthesis. In this paper, we propose UniDriveDreamer, a single-stage unified multimodal world model for autonomous driving, which directly generates multimodal future observations without relying on intermediate representations or cascaded modules. Our framework introduces a LiDAR-specific variational autoencoder (VAE) designed to encode input LiDAR sequences, alongside a video VAE for multi-camera images. To ensure cross-modal compatibility and training stability, we propose Unified Latent Anchoring (ULA), which explicitly aligns the latent distributions of the two modalities. The aligned features are fused and processed by a diffusion transformer that jointly models their geometric correspondence and temporal evolution. Additionally, structured scene layout information is projected per modality as a conditioning signal to guide the synthesis. Extensive experiments demonstrate that UniDriveDreamer outperforms previous state-of-the-art methods in both video and LiDAR generation, while also yielding measurable improvements in downstream

</details>


### [237] [ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning](https://arxiv.org/abs/2602.02004)
*Gongli Xi,Kun Wang,Zeming Gao,Huahui Yi,Haolang Lu,Ye Tian,Wendong Wang*

Main category: cs.CV

TL;DR: 本论文针对大型多模态推理模型在视觉推理任务中的幻觉（hallucination）问题，提出了ClueRecall度量和ClueTracer插件，有效抑制模型幻觉并提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 随着多模态推理模型能力提升，模型在推理过程中会生成未被输入图像或问题支持的内容（即幻觉）。研究发现原因之一是推理漂移（reasoning drift）：模型过度关注与任务无关的信息，导致推理过程逐步脱离视觉证据。以往针对非推理模型的定位或干预方法在此类任务下效果不佳，亟需新方法解决这一问题。

Method: 提出ClueRecall指标，用于评估模型视觉线索检索能力。同时提出ClueTracer插件：无需重新训练、无参数、架构无关，能够追踪模型推理路径中关键信息的传播，从而精准地定位与任务相关的图像区域，抑制对无关区域的注意。该方法以问题出发，贯穿输出和视觉Token之间的推理链路。

Result: ClueTracer无需额外训练即可应用于现有各类推理模型（如R1-OneVision、Ocean-R1、MM-Eureka等），在推理基准测试中将表现平均提升1.21倍，在非推理任务下也有1.14倍的涨幅。

Conclusion: 该论文提出的ClueTracer方法与ClueRecall指标，有效解决了推理型多模态模型的幻觉问题，不仅提升了推理准确性，还具备广泛的兼容性和实用价值。

Abstract: Large multimodal reasoning models solve challenging visual problems via explicit long-chain inference: they gather visual clues from images and decode clues into textual tokens. Yet this capability also increases hallucinations, where the model generates content that is not supported by the input image or the question. To understand this failure mode, we identify \emph{reasoning drift}: during clue gathering, the model over-focuses on question-irrelevant entities, diluting focus on task-relevant cues and gradually decoupling the reasoning trace from visual grounding. As a consequence, many inference-time localization or intervention methods developed for non-reasoning models fail to pinpoint the true clues in reasoning settings. Motivated by these insights, we introduce ClueRecall, a metric for assessing visual clue retrieval, and present ClueTracer, a training-free, parameter-free, and architecture-agnostic plugin for hallucination suppression. ClueTracer starts from the question and traces how key clues propagate along the model's reasoning pathway (question $\rightarrow$ outputs $\rightarrow$ visual tokens), thereby localizing task-relevant patches while suppressing spurious attention to irrelevant regions. Remarkably, \textbf{without any additional training}, ClueTracer improves all \textbf{reasoning} architectures (including \texttt{R1-OneVision}, \texttt{Ocean-R1}, \texttt{MM-Eureka}, \emph{etc}.) by $\mathbf{1.21\times}$ on reasoning benchmarks. When transferred to \textbf{non-reasoning} settings, it yields a $\mathbf{1.14\times}$ gain.

</details>


### [238] [One Size, Many Fits: Aligning Diverse Group-Wise Click Preferences in Large-Scale Advertising Image Generation](https://arxiv.org/abs/2602.02033)
*Shuo Lu,Haohan Wang,Wei Feng,Weizhen Wang,Shen Zhang,Yaoyu Li,Ao Ma,Zheng Zhang,Jingjing Lv,Junjie Shen,Ching Law,Bing Zhan,Yuan Xu,Huizai Yao,Yongcan Yu,Chenyang Si,Jian Liang*

Main category: cs.CV

TL;DR: 提出OSMF框架，根据用户分组特征生成定制广告图片，显著提升各群体的CTR，解决了广告生成中的“一刀切”问题。


<details>
  <summary>Details</summary>
Motivation: 当前广告图片生成方法只关注整体CTR，忽视了不同用户群体间的偏好差异，从而制约了精准营销效果。本文旨在解决分组偏好未被有效利用导致的广告投放效率低下问题。

Method: 1）先进行面向产品的自适应用户分组，整合群体偏好特征；2）基于群体特征，利用群体感知多模态大语言模型（G-MLLM）进行有条件的广告图片生成；3）提出Group-DPO方法，微调G-MLLM，实现群体特定偏好的优化；4）构建了GAIP大规模公开数据集，50万组群体偏好数据用于训练与评测。

Result: 实验表明，提出的OSMF框架在广告图片生成任务上，无论离线还是在线，都优于现有方法，在各个分组群体的CTR上均实现提升，达到了SOTA水平。

Conclusion: OSMF框架解决了广告图片生成中群体偏好多样性的问题，提高了群体CTR，是针对个性化在线广告投放的重要进展。GAIP数据集的发布也有助于推动该领域研究。

Abstract: Advertising image generation has increasingly focused on online metrics like Click-Through Rate (CTR), yet existing approaches adopt a ``one-size-fits-all" strategy that optimizes for overall CTR while neglecting preference diversity among user groups. This leads to suboptimal performance for specific groups, limiting targeted marketing effectiveness. To bridge this gap, we present \textit{One Size, Many Fits} (OSMF), a unified framework that aligns diverse group-wise click preferences in large-scale advertising image generation. OSMF begins with product-aware adaptive grouping, which dynamically organizes users based on their attributes and product characteristics, representing each group with rich collective preference features. Building on these groups, preference-conditioned image generation employs a Group-aware Multimodal Large Language Model (G-MLLM) to generate tailored images for each group. The G-MLLM is pre-trained to simultaneously comprehend group features and generate advertising images. Subsequently, we fine-tune the G-MLLM using our proposed Group-DPO for group-wise preference alignment, which effectively enhances each group's CTR on the generated images. To further advance this field, we introduce the Grouped Advertising Image Preference Dataset (GAIP), the first large-scale public dataset of group-wise image preferences, including around 600K groups built from 40M users. Extensive experiments demonstrate that our framework achieves the state-of-the-art performance in both offline and online settings. Our code and datasets will be released at https://github.com/JD-GenX/OSMF.

</details>


### [239] [Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models](https://arxiv.org/abs/2602.02043)
*Cristian Sbrolli,Matteo Matteucci,Toshihiko Yamasaki*

Main category: cs.CV

TL;DR: 现代视觉-语言模型（VLMs）在组合推理上存在严重缺陷，会混淆颜色与物体配对（如“红色方块和蓝色球体”与“蓝色方块和红色球体”）。作者提出Auto-Comp自动合成基准评测管道，能精准分析VLMs在颜色绑定和空间关系上的推理能力，发现主流模型存在普遍且更为深层的组合失败问题。


<details>
  <summary>Details</summary>
Motivation: VLM经常在细粒度的组合推理情景下出错，尤其是属性与物体配对时易混淆。现有评测缺乏可控性，难以剖析具体是视觉还是语言表征导致的错误。因此，迫切需要可控、系统的评测工具，帮助社区理解并改进模型的组合泛化能力。

Method: 提出Auto-Comp自动合成评测管道，通过最小描述（少量元素精确表达，如“监视器在自行车左边”）和上下文丰富描述（大语言模型生成场景句），自动配对生成图像及文本基准，实现可控A/B测试，剖析模型对属性、空间关系的推理能力。引入“Confusion Benchmark”分析模型对低熵干扰（如重复颜色或物体）的敏感性。

Result: 用Auto-Comp系统对20种主流VLMs（如CLIP、SigLIP）进行大规模测试，发现颜色绑定和空间关系推理普遍失败，不仅限于简单属性互换，面对低熵干扰模型表现更差。同时发现上下文丰富场景描述虽然有助于整体空间推理，却会因视觉干扰导致局部属性绑定更弱。

Conclusion: 主流VLMs组合推理存在严重瓶颈，当前设计未能解决视觉与语言属性绑定和空间组合。丰富上下文虽可提升空间推理但会加重视觉干扰。Auto-Comp将推动社区深入评测和改进VLM组合泛化。源码与基准已公开以支持后续研究。

Abstract: Modern Vision-Language Models (VLMs) exhibit a critical flaw in compositional reasoning, often confusing "a red cube and a blue sphere" with "a blue cube and a red sphere". Disentangling the visual and linguistic roots of these failures is a fundamental challenge for robust evaluation. To enable fine-grained, controllable analysis, we introduce Auto-Comp, a fully automated and synthetic pipeline for generating scalable benchmarks. Its controllable nature is key to dissecting and isolating different reasoning skills. Auto-Comp generates paired images from Minimal (e.g., "a monitor to the left of a bicycle on a white background") and LLM-generated Contextual captions (e.g., "In a brightly lit photography studio, a monitor is positioned to the left of a bicycle"), allowing a controlled A/B test to disentangle core binding ability from visio-linguistic complexity. Our evaluation of 20 VLMs on novel benchmarks for color binding and spatial relations reveals universal compositional failures in both CLIP and SigLIP model families. Crucially, our novel "Confusion Benchmark" reveals a deeper flaw beyond simple attribute swaps: models are highly susceptible to low-entropy distractors (e.g., repeated objects or colors), demonstrating their compositional failures extend beyond known bag-of-words limitations. we uncover a surprising trade-off: visio-linguistic context, which provides global scene cues, aids spatial reasoning but simultaneously hinders local attribute binding by introducing visual clutter. We release the Auto-Comp pipeline to facilitate future benchmark creation, alongside all our generated benchmarks (https://huggingface.co/AutoComp).

</details>


### [240] [Multi-View Stenosis Classification Leveraging Transformer-Based Multiple-Instance Learning Using Real-World Clinical Data](https://arxiv.org/abs/2602.02067)
*Nikola Cenikj,Özgün Turgut,Alexander Müller,Alexander Steger,Jan Kehrer,Marcus Brugger,Daniel Rueckert,Eimo Martens,Philip Müller*

Main category: cs.CV

TL;DR: 该论文提出了一种基于Transformer的多视角多实例学习框架SegmentMIL，实现了无需视角级标注的冠状动脉狭窄分类，并且在内部和外部测试中表现优秀。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的方法通常依赖昂贵的视角级标注，且无法有效利用多视角之间的时序动态和依赖性，这些都是实际临床诊断中非常重要的。

Method: 提出了SegmentMIL框架，基于Transformer网络结构，采用患者级监督，无需视角级标注，可以同时实现狭窄的分类和患病区域（左右冠状动脉及其分段）的定位。

Result: SegmentMIL在真实临床数据集上进行了训练和评估，相比传统的方法与经典的多实例学习（MIL）基线，以及基于单视角方法都取得了更优异的性能。

Conclusion: SegmentMIL不依赖昂贵的手动标注，具备较好泛化能力和临床应用前景，为冠状动脉狭窄诊断提供了高效、可扩展的解决方案。

Abstract: Coronary artery stenosis is a leading cause of cardiovascular disease, diagnosed by analyzing the coronary arteries from multiple angiography views. Although numerous deep-learning models have been proposed for stenosis detection from a single angiography view, their performance heavily relies on expensive view-level annotations, which are often not readily available in hospital systems. Moreover, these models fail to capture the temporal dynamics and dependencies among multiple views, which are crucial for clinical diagnosis. To address this, we propose SegmentMIL, a transformer-based multi-view multiple-instance learning framework for patient-level stenosis classification. Trained on a real-world clinical dataset, using patient-level supervision and without any view-level annotations, SegmentMIL jointly predicts the presence of stenosis and localizes the affected anatomical region, distinguishing between the right and left coronary arteries and their respective segments. SegmentMIL obtains high performance on internal and external evaluations and outperforms both view-level models and classical MIL baselines, underscoring its potential as a clinically viable and scalable solution for coronary stenosis diagnosis. Our code is available at https://github.com/NikolaCenic/mil-stenosis.

</details>


### [241] [UrbanGS: A Scalable and Efficient Architecture for Geometrically Accurate Large-Scene Reconstruction](https://arxiv.org/abs/2602.02089)
*Changbai Li,Haodong Zhu,Hanlin Chen,Xiuping Liang,Tongfei Chen,Shuwei Shao,Linlin Yang,Huobin Tan,Baochang Zhang*

Main category: cs.CV

TL;DR: 本文提出了UrbanGS框架，针对3D Gaussian Splatting在大规模城市场景中的几何一致性、内存效率和可扩展性问题，提出多项改进以提升重建质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有3D Gaussian Splatting方法在小场景表现出色，但在城市级大规模场景下暴露出几何不一致、内存消耗大和扩展性差等问题，因此亟需解决这些瓶颈以推进城市级重建应用。

Method: UrbanGS引入了深度一致D-Normal正则化模块，通过结合外部深度监督，实现所有几何参数的全方位更新，并利用自适应置信加权提升多视角深度配准和几何一致性。为提升可扩展性，提出了空间自适应高斯剪枝策略(SAGP)，按场景复杂度动态调整高斯密度；还设计了统一的空间分区与视点分配，消除分界伪影并优化计算负载。

Result: 在多个城市数据集的大量实验表明UrbanGS在渲染质量、几何精度、内存效率上均优于现有方法，展示其系统性和高保真重建能力。

Conclusion: UrbanGS有效克服了3DGS在城市级场景中的性能瓶颈，为大规模高质量场景重建提供了系统性解决方案。

Abstract: While 3D Gaussian Splatting (3DGS) enables high-quality, real-time rendering for bounded scenes, its extension to large-scale urban environments gives rise to critical challenges in terms of geometric consistency, memory efficiency, and computational scalability. To address these issues, we present UrbanGS, a scalable reconstruction framework that effectively tackles these challenges for city-scale applications. First, we propose a Depth-Consistent D-Normal Regularization module. Unlike existing approaches that rely solely on monocular normal estimators, which can effectively update rotation parameters yet struggle to update position parameters, our method integrates D-Normal constraints with external depth supervision. This allows for comprehensive updates of all geometric parameters. By further incorporating an adaptive confidence weighting mechanism based on gradient consistency and inverse depth deviation, our approach significantly enhances multi-view depth alignment and geometric coherence, which effectively resolves the issue of geometric accuracy in complex large-scale scenes. To improve scalability, we introduce a Spatially Adaptive Gaussian Pruning (SAGP) strategy, which dynamically adjusts Gaussian density based on local geometric complexity and visibility to reduce redundancy. Additionally, a unified partitioning and view assignment scheme is designed to eliminate boundary artifacts and optimize computational load. Extensive experiments on multiple urban datasets demonstrate that UrbanGS achieves superior performance in rendering quality, geometric accuracy, and memory efficiency, providing a systematic solution for high-fidelity large-scale scene reconstruction.

</details>


### [242] [FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space](https://arxiv.org/abs/2602.02092)
*FSVideo Team,Qingyu Chen,Zhiyuan Fang,Haibin Huang,Xinwei Huang,Tong Jin,Minxuan Lin,Bo Liu,Celong Liu,Chongyang Ma,Xing Mei,Xiaohui Shen,Yaojie Shen,Fuwen Tan,Angtian Wang,Xiao Yang,Yiding Yang,Jiamin Yuan,Lingxi Zhang,Yuxin Zhang*

Main category: cs.CV

TL;DR: FSVideo提出了一种基于Transformer的高效图像到视频生成框架，兼具高速度与高质量，实现了同类开源模型中的领先性能。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在图像到视频生成任务中取得了重大进展，但现有方法普遍存在速度慢的问题。为了解决生成速度和视频质量之间的权衡，作者提出了更高效的解决方案。

Method: 1）设计了新的高压缩视频自编码器，有效降低时空冗余并提升重建质量；2）提出带层记忆的新型扩散Transformer（DIT），增强层间信息流与上下文复用；3）采用多分辨率生成策略，通过少步DIT上采样器提升视频保真度。最终模型包含14B参数的DIT底座和上采样器。

Result: FSVideo在保持与主流开源模型相当或更优的生成质量下，生成速度提升了一个数量级；模型兼具高效和高保真特性。

Conclusion: FSVideo在速度和效果上均实现突破，显著促进图像到视频生成模型的发展，是生成式视频领域的重要进展。

Abstract: We introduce FSVideo, a fast speed transformer-based image-to-video (I2V) diffusion framework. We build our framework on the following key components: 1.) a new video autoencoder with highly-compressed latent space ($64\times64\times4$ spatial-temporal downsampling ratio), achieving competitive reconstruction quality; 2.) a diffusion transformer (DIT) architecture with a new layer memory design to enhance inter-layer information flow and context reuse within DIT, and 3.) a multi-resolution generation strategy via a few-step DIT upsampler to increase video fidelity. Our final model, which contains a 14B DIT base model and a 14B DIT upsampler, achieves competitive performance against other popular open-source models, while being an order of magnitude faster. We discuss our model design as well as training strategies in this report.

</details>


### [243] [Teacher-Guided Student Self-Knowledge Distillation Using Diffusion Model](https://arxiv.org/abs/2602.02107)
*Yu Wang,Chuanguang Yang,Zhulin An,Weilun Feng,Jiarui Zhao,Chengqing Yu,Libo Huang,Boyu Diao,Yongjun Xu*

Main category: cs.CV

TL;DR: 本文提出了一种新的知识蒸馏方法DSKD，通过引入扩散模型和本地敏感哈希（LSH）机制，显著提升了学生模型对教师模型知识的学习效果。


<details>
  <summary>Details</summary>
Motivation: 当前的大多数知识蒸馏方法通过对齐教师和学生的特征信息来提高学生模型性能，但由于二者特征分布不同，学生模型学到的信息常常与教师模型不兼容，影响知识迁移效果。

Method: 作者提出了一种教师引导的学生扩散自蒸馏方法（DSKD）：1）用教师分类器引导扩散模型对学生特征去噪采样，生成包含教师知识的学生特征；2）使用LSH机制在原始学生特征与去噪学生特征之间进行特征蒸馏，从而以更自然的方式实现知识转移和分布对齐。

Result: 在视觉识别任务的多个模型和数据集上，DSKD均显著优于现有的知识蒸馏方法。

Conclusion: DSKD通过扩散过程和LSH机制，有效消除了师生特征分布不一致和映射方式差异，实现了更高效的知识迁移，为后续知识蒸馏研究提供了新方向。

Abstract: Existing Knowledge Distillation (KD) methods often align feature information between teacher and student by exploring meaningful feature processing and loss functions. However, due to the difference in feature distributions between the teacher and student, the student model may learn incompatible information from the teacher. To address this problem, we propose teacher-guided student Diffusion Self-KD, dubbed as DSKD. Instead of the direct teacher-student alignment, we leverage the teacher classifier to guide the sampling process of denoising student features through a light-weight diffusion model. We then propose a novel locality-sensitive hashing (LSH)-guided feature distillation method between the original and denoised student features. The denoised student features encapsulate teacher knowledge and could be regarded as a teacher role. In this way, our DSKD method could eliminate discrepancies in mapping manners and feature distributions between the teacher and student, while learning meaningful knowledge from the teacher. Experiments on visual recognition tasks demonstrate that DSKD significantly outperforms existing KD methods across various models and datasets. Our code is attached in supplementary material.

</details>


### [244] [Enhancing Diffusion-Based Quantitatively Controllable Image Generation via Matrix-Form EDM and Adaptive Vicinal Training](https://arxiv.org/abs/2602.02114)
*Xin Ding,Yun Chen,Sen Zhang,Kao Zhang,Nenglun Chen,Peibei Cao,Yongwei Wang,Fei Wu*

Main category: cs.CV

TL;DR: 本文提出了改进版的连续条件扩散模型（iCCDM），通过引入先进的EDM框架及自适应邻域训练，显著提升了生成图像的质量并降低了采样成本。


<details>
  <summary>Details</summary>
Motivation: 现有的CCDM在连续标签条件下生成高质量图像方面已优于早期方法，但采样过程效率低且已被部分GAN方法超越，因此亟需提升扩散模型的生成质量和效率。

Method: iCCDM结合了最新的Elucidated Diffusion Model（EDM）框架，提出了新的矩阵形式EDM表达，并引入了自适应邻域训练策略以提高模型泛化与生成质量。

Result: 在四个基准数据集（图像分辨率64×64至256×256）上，iCCDM的生成质量和采样效率均优于已知主流方法，包括大规模文本到图像扩散模型如Stable Diffusion 3等。

Conclusion: iCCDM在保证更高图像生成质量的同时，有效降低了计算开销，在连续条件图像生成任务中具备领先优势。

Abstract: Continuous Conditional Diffusion Model (CCDM) is a diffusion-based framework designed to generate high-quality images conditioned on continuous regression labels. Although CCDM has demonstrated clear advantages over prior approaches across a range of datasets, it still exhibits notable limitations and has recently been surpassed by a GAN-based method, namely CcGAN-AVAR. These limitations mainly arise from its reliance on an outdated diffusion framework and its low sampling efficiency due to long sampling trajectories. To address these issues, we propose an improved CCDM framework, termed iCCDM, which incorporates the more advanced \textit{Elucidated Diffusion Model} (EDM) framework with substantial modifications to improve both generation quality and sampling efficiency. Specifically, iCCDM introduces a novel matrix-form EDM formulation together with an adaptive vicinal training strategy. Extensive experiments on four benchmark datasets, spanning image resolutions from $64\times64$ to $256\times256$, demonstrate that iCCDM consistently outperforms existing methods, including state-of-the-art large-scale text-to-image diffusion models (e.g., Stable Diffusion 3, FLUX.1, and Qwen-Image), achieving higher generation quality while significantly reducing sampling cost.

</details>


### [245] [MLV-Edit: Towards Consistent and Highly Efficient Editing for Minute-Level Videos](https://arxiv.org/abs/2602.02123)
*Yangyi Cao,Yuanhang Li,Lan Chen,Qi Mao*

Main category: cs.CV

TL;DR: MLV-Edit是一种无需训练的、基于光流的方法，能高效解决长时（分钟级）视频编辑中的时序一致性与计算资源难题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑技术主要适用于短视频，难以扩展到长视频，因为长视频编辑计算量大且难以保证全局时间一致性。为此，作者提出新的方法解决分钟级视频的编辑难题。

Method: MLV-Edit运用分而治之的策略，将长视频划分成多个片段分别编辑。核心包括两个模块：Velocity Blend对齐相邻片段的光流场，解决边界运动不连贯和闪烁问题；Attention Sink则通过参考帧抑制结构漂移，保障全局一致性。该方法无需训练。

Result: 通过大量定量和定性实验，MLV-Edit在时序稳定性和语义保持方面，均优于现有主流方法。

Conclusion: MLV-Edit有效解决了长视频编辑的挑战，在无需训练的情况下可稳定编辑长时视频，具有较强的实际应用价值。

Abstract: We propose MLV-Edit, a training-free, flow-based framework that address the unique challenges of minute-level video editing. While existing techniques excel in short-form video manipulation, scaling them to long-duration videos remains challenging due to prohibitive computational overhead and the difficulty of maintaining global temporal consistency across thousands of frames. To address this, MLV-Edit employs a divide-and-conquer strategy for segment-wise editing, facilitated by two core modules: Velocity Blend rectifies motion inconsistencies at segment boundaries by aligning the flow fields of adjacent chunks, eliminating flickering and boundary artifacts commonly observed in fragmented video processing; and Attention Sink anchors local segment features to global reference frames, effectively suppressing cumulative structural drift. Extensive quantitative and qualitative experiments demonstrate that MLV-Edit consistently outperforms state-of-the-art methods in terms of temporal stability and semantic fidelity.

</details>


### [246] [Toxicity Assessment in Preclinical Histopathology via Class-Aware Mahalanobis Distance for Known and Novel Anomalies](https://arxiv.org/abs/2602.02124)
*Olga Graf,Dhrupal Patel,Peter Groß,Charlotte Lempp,Matthias Hein,Fabian Heinemann*

Main category: cs.CV

TL;DR: 本文提出了一种基于AI的异常检测框架，能够在啮齿动物肝脏全切片组织图像中自动识别健康组织、已知和罕见病理异常，用于毒理学研究和药物安全性筛查。


<details>
  <summary>Details</summary>
Motivation: 药物诱导的毒性反应是药物研发过程中导致失败的重要原因，现有的病理评估依赖专家，难以支持大规模筛查，急需自动化和高效的解决方案。

Method: 构建了肝脏组织图像像素级标注数据集，利用DINOv2预训练视觉变换器并通过LoRA微调，实现组织分割。通过马氏距离提取OOD（分布外）特征，并针对不同类别采用特定阈值来优化异常检测性能。

Result: 优化后，框架将病理组织误判为健康的比例降至0.16%，健康组织误判为病理的比例为0.35%，在含有已知毒理学结果的小鼠肝脏图像上也能准确检测异常和罕见病变。

Conclusion: AI驱动的病理学方法能够自动检测毒性相关病变和罕见异常，可提升前期研发效率、减少晚期研发失败，助力药物安全性评价。

Abstract: Drug-induced toxicity remains a leading cause of failure in preclinical development and early clinical trials. Detecting adverse effects at an early stage is critical to reduce attrition and accelerate the development of safe medicines. Histopathological evaluation remains the gold standard for toxicity assessment, but it relies heavily on expert pathologists, creating a bottleneck for large-scale screening. To address this challenge, we introduce an AI-based anomaly detection framework for histopathological whole-slide images (WSIs) in rodent livers from toxicology studies. The system identifies healthy tissue and known pathologies (anomalies) for which training data is available. In addition, it can detect rare pathologies without training data as out-of-distribution (OOD) findings. We generate a novel dataset of pixelwise annotations of healthy tissue and known pathologies and use this data to fine-tune a pre-trained Vision Transformer (DINOv2) via Low-Rank Adaptation (LoRA) in order to do tissue segmentation. Finally, we extract features for OOD detection using the Mahalanobis distance. To better account for class-dependent variability in histological data, we propose the use of class-specific thresholds. We optimize the thresholds using the mean of the false negative and false positive rates, resulting in only 0.16\% of pathological tissue classified as healthy and 0.35\% of healthy tissue classified as pathological. Applied to mouse liver WSIs with known toxicological findings, the framework accurately detects anomalies, including rare OOD morphologies. This work demonstrates the potential of AI-driven histopathology to support preclinical workflows, reduce late-stage failures, and improve efficiency in drug development.

</details>


### [247] [Eliminating Registration Bias in Synthetic CT Generation: A Physics-Based Simulation Framework](https://arxiv.org/abs/2602.02130)
*Lukas Zimmermann,Michael Rauter,Maximilian Schmid,Dietmar Georg,Barbara Knäusl*

Main category: cs.CV

TL;DR: 该论文指出，传统基于配准的CBCT到合成CT的训练和评估会被配准误差影响，提出用基于物理模拟生成严格对齐的数据来避免该问题，并用与输入CBCT的几何对齐度而非强度进行评估。实验证明合成数据训练的模型在几何对齐上优于传统方法，且更符合临床需求。


<details>
  <summary>Details</summary>
Motivation: 传统的CBCT转合成CT方法依赖于严格配准的训练对，但实际很难获得完美配准，这导致训练和评估过程都受到配准误差影响，进而可能导致模型更擅长复现这些误差而非保持解剖结构，影响临床中对结构准确的需求。

Method: 作者提出用基于物理的CBCT模拟生成天然配准的成对数据进行训练，将评价标准改为与输入CBCT在几何上的对齐度（如归一化互信息），而不依赖与存在偏差配准的CT真值之间的像素值强度一致性。

Result: 在两个独立的盆腔数据集上，用合成数据训练的模型在几何对齐指标（归一化互信息）方面显著优于传统方法（0.31对0.22），尽管在强度分数上略低。强度指标与临床评价反相关，而几何对齐指标与观察者一致性显著。临床评估中，87%的情况下更偏好基于合成数据训练的输出。

Conclusion: 问题的关键在于配准误差影响了训练和评估。采用物理仿真可获得结构对齐的训练数据，几何对齐指标优于传统方法，并更贴合临床认知需求。模型的结构保真度比像素强度一致性更应作为临床改进方向。

Abstract: Supervised synthetic CT generation from CBCT requires registered training pairs, yet perfect registration between separately acquired scans remains unattainable. This registration bias propagates into trained models and corrupts standard evaluation metrics. This may suggest that superior benchmark performance indicates better reproduction of registration artifacts rather than anatomical fidelity. We propose physics-based CBCT simulation to provide geometrically aligned training pairs by construction, combined with evaluation using geometric alignment metrics against input CBCT rather than biased ground truth. On two independent pelvic datasets, models trained on synthetic data achieved superior geometric alignment (Normalized Mutual Information: 0.31 vs 0.22) despite lower conventional intensity scores. Intensity metrics showed inverted correlations with clinical assessment for deformably registered data, while Normalized Mutual Information consistently predicted observer preference across registration methodologies (rho = 0.31, p < 0.001). Clinical observers preferred synthetic-trained outputs in 87% of cases, demonstrating that geometric fidelity, not intensity agreement with biased ground truth, aligns with clinical requirements.

</details>


### [248] [Deep learning enables urban change profiling through alignment of historical maps](https://arxiv.org/abs/2602.02154)
*Sidi Wu,Yizi Chen,Maurizio Gribaudi,Konrad Schindler,Clément Mallet,Julien Perret,Lorenz Hurni*

Main category: cs.CV

TL;DR: 提出了一个全自动的深度学习框架，用于从大量历史地图中精细分析城市变化，实现了定量化和系统化的历史城市变化检测。


<details>
  <summary>Details</summary>
Motivation: 历史地图详细记录了城市长期变迁，但地图之间空间不对齐、绘图风格变化和文档劣化导致很难进行大规模、细粒度和定量化的分析，因此需要新的自动化方法。

Method: 提出了一个模块化的全自动深度学习框架，包括密集地图对齐、多时相目标检测和变化分析模块；实现了地图自动配准和城市对象的跨时段检测。

Result: 在巴黎1868至1937年地图体系下，框架自动检测了城市空间和时间上的异质性变化，实验证明了对齐和检测方法的鲁棒性。

Conclusion: 该框架为社会科学和人文学科中的历史城市变化研究提供了强有力工具，模块化设计也便于推广到不同类型的地图和下游应用场景。

Abstract: Prior to modern Earth observation technologies, historical maps provide a unique record of long-term urban transformation and offer a lens on the evolving identity of cities. However, extracting consistent and fine-grained change information from historical map series remains challenging due to spatial misalignment, cartographic variation, and degrading document quality, limiting most analyses to small-scale or qualitative approaches. We propose a fully automated, deep learning-based framework for fine-grained urban change analysis from large collections of historical maps, built on a modular design that integrates dense map alignment, multi-temporal object detection, and change profiling. This framework shifts the analysis of historical maps from ad hoc visual comparison toward systematic, quantitative characterization of urban change. Experiments demonstrate the robust performance of the proposed alignment and object detection methods. Applied to Paris between 1868 and 1937, the framework reveals the spatial and temporal heterogeneity in urban transformation, highlighting its relevance for research in the social sciences and humanities. The modular design of our framework further supports adaptation to diverse cartographic contexts and downstream applications.

</details>


### [249] [LoopViT: Scaling Visual ARC with Looped Transformers](https://arxiv.org/abs/2602.02156)
*Wen-Jie Shu,Xuerui Qiu,Rui-Jie Zhu,Harold Haodong Chen,Yexin Liu,Harry Yang*

Main category: cs.CV

TL;DR: 本文提出了一种递归视觉Transformer架构Loop-ViT，通过参数复用和动态停止机制，有效提升了视觉推理任务的效率和效果，在ARC-AGI-1基准上小模型超越了更大参数量的模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉Transformer采用前馈架构，在模型参数和推理深度上呈线性关系，难以模拟人类归纳中的递归和迭代推理能力。作者希望通过打破这一约束，实现更高效、类似人类思维的视觉推理。

Method: 提出Loop-ViT递归架构，将卷积与全局注意力结合形成Hybrid Block，并在多个时刻复用同一组权重，允许达到更大计算深度；引入基于预测熵的动态退出机制，模型内部状态确定性足够高时自动停止迭代，提升效率与自适应性。

Result: 在ARC-AGI-1任务上，一个只有1800万参数的Loop-ViT模型获得了65.8%的准确率，超过了参数量高达7300万的大型集成模型；实验结果验证了递归推理和自适应停止机制的有效性。

Conclusion: 相比于一味增加网络宽度，采用递归、动态计算深度的架构能更高效地提升视觉推理能力。这为视觉基础模型未来的发展和设计提供了新的高效方向。

Abstract: Recent advances in visual reasoning have leveraged vision transformers to tackle the ARC-AGI benchmark. However, we argue that the feed-forward architecture, where computational depth is strictly bound to parameter size, falls short of capturing the iterative, algorithmic nature of human induction. In this work, we propose a recursive architecture called Loop-ViT, which decouples reasoning depth from model capacity through weight-tied recurrence. Loop-ViT iterates a weight-tied Hybrid Block, combining local convolutions and global attention, to form a latent chain of thought. Crucially, we introduce a parameter-free Dynamic Exit mechanism based on predictive entropy: the model halts inference when its internal state ``crystallizes" into a low-uncertainty attractor. Empirical results on the ARC-AGI-1 benchmark validate this perspective: our 18M model achieves 65.8% accuracy, outperforming massive 73M-parameter ensembles. These findings demonstrate that adaptive iterative computation offers a far more efficient scaling axis for visual reasoning than simply increasing network width. The code is available at https://github.com/WenjieShu/LoopViT.

</details>


### [250] [Reg4Pru: Regularisation Through Random Token Routing for Token Pruning](https://arxiv.org/abs/2602.02163)
*Julian Wyatt,Ronald Clark,Irina Voiculescu*

Main category: cs.CV

TL;DR: 本文提出Reg4Pru正则化技术，显著提升了基于token剪枝变换器模型在图像分割中的表现，并提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 变换器（Transformer）在视觉模型中的广泛应用虽然带来了出色的泛化能力，但其计算量随token数量呈二次增长，计算开销巨大。虽然token剪枝等方法有助于提升计算效率，但却带来了稠密预测性能下降等问题，尤其是在模型更深层时更为明显。因此亟需解决token减裁制度下性能下降的问题。

Method: 论文提出了一种名为Reg4Pru的训练正则化方法，在token剪枝过程中，通过特殊的正则化技术缓解剪枝带来的性能损失，提升在分割任务中的表现。

Result: 在FIVES血管分割数据集上，应用Reg4Pru后，模型的平均精度比未用该正则化的剪枝模型提高了绝对46%；在保证29%推理加速（相比未剪枝基线）的情况下达到上述性能提升。

Conclusion: 实验证明，Reg4Pru是一种高效且实用的token剪枝正则化方法，能兼顾推理效率与分割性能，在需要减小计算量但又不希望性能大幅下降的视觉任务中具有重要实用价值。

Abstract: Transformers are widely adopted in modern vision models due to their strong ability to scale with dataset size and generalisability. However, this comes with a major drawback: computation scales quadratically to the total number of tokens. Numerous methods have been proposed to mitigate this. For example, we consider token pruning with reactivating tokens from preserved representations, but the increased computational efficiency of this method results in decreased stability from the preserved representations, leading to poorer dense prediction performance at deeper layers. In this work, we introduce Reg4Pru, a training regularisation technique that mitigates token-pruning performance loss for segmentation. We compare our models on the FIVES blood vessel segmentation dataset and find that Reg4Pru improves average precision by an absolute 46% compared to the same model trained without routing. This increase is observed using a configuration that achieves a 29% relative speedup in wall-clock time compared to the non-pruned baseline. These findings indicate that Reg4Pru is a valuable regulariser for token reduction strategies.

</details>


### [251] [Lung Nodule Image Synthesis Driven by Two-Stage Generative Adversarial Networks](https://arxiv.org/abs/2602.02171)
*Lu Cao,Xiquan He,Junying Zeng,Chaoyun Mai,Min Luo*

Main category: cs.CV

TL;DR: 本文提出了一种两阶段生成对抗网络（TSGAN），通过分离肺结节的形态结构与纹理特征，提升合成数据的多样性与空间可控性。实验结果表明，合成数据可有效提升检测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 肺结节CT数据集的样本数量有限且多样性不足，这严重影响了检测模型的推广性和表现力。现有的合成方法多样性与可控性差，容易产生单调纹理和失真结构。

Method: 提出TSGAN生成模型，第一阶段用StyleGAN生成语义分割掩膜，精确控制肺结节和背景结构；第二阶段使用改进版本DL-Pix2Pix模型将掩膜转换为CT图像，引入局部注意力和动态权重多头窗口注意力机制，提升纹理与背景特征建模能力。

Result: 在LUNA16数据集上，采用TSGAN生成的数据训练的模型，准确率提升4.6%，mAP提升4%。

Conclusion: TSGAN可提升合成CT图像的多样性和质量，并且改善了肺结节检测模型的性能，具有实际应用价值。

Abstract: The limited sample size and insufficient diversity of lung nodule CT datasets severely restrict the performance and generalization ability of detection models. Existing methods generate images with insufficient diversity and controllability, suffering from issues such as monotonous texture features and distorted anatomical structures. Therefore, we propose a two-stage generative adversarial network (TSGAN) to enhance the diversity and spatial controllability of synthetic data by decoupling the morphological structure and texture features of lung nodules. In the first stage, StyleGAN is used to generate semantic segmentation mask images, encoding lung nodules and tissue backgrounds to control the anatomical structure of lung nodule images; The second stage uses the DL-Pix2Pix model to translate the mask map into CT images, employing local importance attention to capture local features, while utilizing dynamic weight multi-head window attention to enhance the modeling capability of lung nodule texture and background. Compared to the original dataset, the accuracy improved by 4.6% and mAP by 4% on the LUNA16 dataset. Experimental results demonstrate that TSGAN can enhance the quality of synthetic images and the performance of detection models.

</details>


### [252] [CIEC: Coupling Implicit and Explicit Cues for Multimodal Weakly Supervised Manipulation Localization](https://arxiv.org/abs/2602.02175)
*Xinquan Yu,Wei Lu,Xiangyang Luo*

Main category: cs.CV

TL;DR: 本文提出了一种名为CIEC的新框架，实现仅用粗粒度标注进行多模态篡改定位，效果媲美全监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态篡改定位方法依赖高昂细粒度标注，不易大规模扩展，因此亟需低成本的弱监督方案。

Method: CIEC框架包含图像分支和文本分支。图像分支采用TRPS模块，结合视觉和文本信息，利用空间先验定位可疑区域并抑制干扰。文本分支用VCTG模块，关注重要内容词，利用视觉偏差提升定位准确性，辅以稀疏和一致性约束减少噪声。两分支均仅需粗粒度标注。

Result: 大量实验表明，CIEC在多个评价指标上性能接近全监督方法。

Conclusion: CIEC框架能够以较低标注成本，有效实现多模态弱监督篡改定位，为相关领域大规模应用提供新思路。

Abstract: To mitigate the threat of misinformation, multimodal manipulation localization has garnered growing attention. Consider that current methods rely on costly and time-consuming fine-grained annotations, such as patch/token-level annotations. This paper proposes a novel framework named Coupling Implicit and Explicit Cues (CIEC), which aims to achieve multimodal weakly-supervised manipulation localization for image-text pairs utilizing only coarse-grained image/sentence-level annotations. It comprises two branches, image-based and text-based weakly-supervised localization. For the former, we devise the Textual-guidance Refine Patch Selection (TRPS) module. It integrates forgery cues from both visual and textual perspectives to lock onto suspicious regions aided by spatial priors. Followed by the background silencing and spatial contrast constraints to suppress interference from irrelevant areas. For the latter, we devise the Visual-deviation Calibrated Token Grounding (VCTG) module. It focuses on meaningful content words and leverages relative visual bias to assist token localization. Followed by the asymmetric sparse and semantic consistency constraints to mitigate label noise and ensure reliability. Extensive experiments demonstrate the effectiveness of our CIEC, yielding results comparable to fully supervised methods on several evaluation metrics.

</details>


### [253] [Learning Topology-Aware Implicit Field for Unified Pulmonary Tree Modeling with Incomplete Topological Supervision](https://arxiv.org/abs/2602.02186)
*Ziqiao Weng,Jiancheng Yang,Kangxian Xie,Bo Zhou,Weidong Cai*

Main category: cs.CV

TL;DR: 论文提出了一种新的针对肺部CT影像解剖树的拓扑修复和分析方法TopoField，能够提升拓扑完整性与下游分析效率，在结构不完整情况下也能实现高效且准确的解剖标注与分割。


<details>
  <summary>Details</summary>
Motivation: 肺部CT影像中提取的解剖树常存在缺失或断裂分支，严重影响后续的医学解剖分析和建模流程。现有方法在处理这些拓扑不完整问题时，存在效率低或对结构损坏不够鲁棒的缺陷。因此，开发高效、鲁棒的肺部解剖树修复与分析工具极为迫切。

Method: TopoField是一个拓扑感知的隐式建模框架，将拓扑修复作为主要任务建模，不依赖完整连接注释。其输入为稀疏的表面和骨架点云，通过对原本就不完整的树引入合成结构破坏进行训练，学习一个支持结构修复的连续隐式场，并结合任务特定的隐式函数，在单次前向传播中实现解剖标注与肺段重建的联合推断。

Result: 在Lung3D+数据集上，TopoField显著提升了肺部树的拓扑完整性，并在不完整结构下实现了高准确率的解剖标注与肺段重建。所提出方法计算效率高，每例处理仅需约一秒，适用于大规模和临床实时分析场景。

Conclusion: TopoField无需完整连通注释，能够高效、鲁棒地修复和分析拓扑结构不完整的肺部CT树，为医学影像分析带来实际可用性，具备广泛的临床与工程应用前景。

Abstract: Pulmonary trees extracted from CT images frequently exhibit topological incompleteness, such as missing or disconnected branches, which substantially degrades downstream anatomical analysis and limits the applicability of existing pulmonary tree modeling pipelines. Current approaches typically rely on dense volumetric processing or explicit graph reasoning, leading to limited efficiency and reduced robustness under realistic structural corruption. We propose TopoField, a topology-aware implicit modeling framework that treats topology repair as a first-class modeling problem and enables unified multi-task inference for pulmonary tree analysis. TopoField represents pulmonary anatomy using sparse surface and skeleton point clouds and learns a continuous implicit field that supports topology repair without relying on complete or explicit disconnection annotations, by training on synthetically introduced structural disruptions over \textit{already} incomplete trees. Building upon the repaired implicit representation, anatomical labeling and lung segment reconstruction are jointly inferred through task-specific implicit functions within a single forward pass.Extensive experiments on the Lung3D+ dataset demonstrate that TopoField consistently improves topological completeness and achieves accurate anatomical labeling and lung segment reconstruction under challenging incomplete scenarios. Owing to its implicit formulation, TopoField attains high computational efficiency, completing all tasks in just over one second per case, highlighting its practicality for large-scale and time-sensitive clinical applications. Code and data will be available at https://github.com/HINTLab/TopoField.

</details>


### [254] [SSI-DM: Singularity Skipping Inversion of Diffusion Models](https://arxiv.org/abs/2602.02193)
*Chen Min,Enze Jiang,Jishen Peng,Zheng Ma*

Main category: cs.CV

TL;DR: 本文提出了一种简单有效的方法，通过在反演前加少量噪声，跳过扩散模型反演过程中的奇异点，显著提升了反演图像的编辑性与重构质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像编辑时需要将真实图像反演到噪声空间，但现有方法反演得到的是非高斯分布噪声，导致编辑性差，其根本原因是初期加噪声阶段存在数学奇异点，使反演问题本质上不适定。

Method: 作者提出了SSI-DM方法，即在标准反演前人为添加少量噪声，从而跳过奇异区域，再进行正常的扩散反演过程。该方法通用、易用，可作为插件纳入各类型扩散模型框架。

Result: 实验表明，该方法在公共图像重建和插值任务上，相较现有方法有更优表现，反演噪声分布更自然，重构图像保真度高，编辑效果好。

Conclusion: SSI-DM为扩散模型的图像反演任务提供了高效、理论扎实的解决方案，显著提升了扩散模型在重建和编辑应用中的性能，有较强的实用价值。

Abstract: Inverting real images into the noise space is essential for editing tasks using diffusion models, yet existing methods produce non-Gaussian noise with poor editability due to the inaccuracy in early noising steps. We identify the root cause: a mathematical singularity that renders inversion fundamentally ill-posed. We propose Singularity Skipping Inversion of Diffusion Models (SSI-DM), which bypasses this singular region by adding small noise before standard inversion. This simple approach produces inverted noise with natural Gaussian properties while maintaining reconstruction fidelity. As a plug-and-play technique compatible with general diffusion models, our method achieves superior performance on public image datasets for reconstruction and interpolation tasks, providing a principled and efficient solution to diffusion model inversion.

</details>


### [255] [MAIN-VLA: Modeling Abstraction of Intention and eNvironment for Vision-Language-Action Models](https://arxiv.org/abs/2602.02212)
*Zheyuan Zhou,Liang Du,Zixun Sun,Xiaoyu Zhou,Ruimin Ye,Qihao Chen,Yinda Chen,Lemiao Qiu*

Main category: cs.CV

TL;DR: 本文提出了MAIN-VLA框架，通过抽象意图与环境，对决策中的视觉-语言-动作（VLA）三者进行深层语义对齐，有效提升了VLA任务在复杂大型动态环境中的效率与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLA方法在面对包含海量冗余传感器信息和实时复杂交互的三维开放世界或大型PvP游戏时，难以有效提取关键动作信号，导致决策效率低和泛化性能不足。

Method: 提出MAIN-VLA框架，包含意图抽象（IA），将详细语言指令压缩为显式语义元；环境语义抽象（ESA），将复杂视觉流映射为结构化的拓扑可供性表示；这两种抽象对齐后，能在无参数的token-pruning机制下，有效过滤感知冗余。

Result: 在Minecraft、Game for Peace、Valorant等开放世界与大型PvP环境广泛实验表明，MAIN-VLA在决策质量、泛化能力和推理效率方面都达到了新的SOTA水平。

Conclusion: MAIN-VLA通过深度语义抽象与跨模态对齐，实现了VLA在复杂动态环境中的高效、高质、强泛化表现，显著优于现有方法。

Abstract: Despite significant progress in Visual-Language-Action (VLA), in highly complex and dynamic environments that involve real-time unpredictable interactions (such as 3D open worlds and large-scale PvP games), existing approaches remain inefficient at extracting action-critical signals from redundant sensor streams. To tackle this, we introduce MAIN-VLA, a framework that explicitly Models the Abstraction of Intention and eNvironment to ground decision-making in deep semantic alignment rather than superficial pattern matching. Specifically, our Intention Abstraction (IA) extracts verbose linguistic instructions and their associated reasoning into compact, explicit semantic primitives, while the Environment Semantics Abstraction (ESA) projects overwhelming visual streams into a structured, topological affordance representation. Furthermore, aligning these two abstract modalities induces an emergent attention-concentration effect, enabling a parameter-free token-pruning strategy that filters out perceptual redundancy without degrading performance. Extensive experiments in open-world Minecraft and large-scale PvP environments (Game for Peace and Valorant) demonstrate that MAIN-VLA sets a new state-of-the-art, which achieves superior decision quality, stronger generalization, and cutting-edge inference efficiency.

</details>


### [256] [Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation](https://arxiv.org/abs/2602.02214)
*Hongzhou Zhu,Min Zhao,Guande He,Hang Su,Chongxuan Li,Jun Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Causal Forcing的方法，用于提升少步自回归（AR）视频生成模型的性能，在理论与实际效果上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前为了实现实时交互式视频生成，主流方法通过将预训练的双向扩散模型蒸馏为少步自回归模型，但两者在注意力结构上存在架构鸿沟，且优良的理论支持缺失，导致性能下降。

Method: 作者指出现有方法采用双向教师模型进行常微分方程（ODE）初始化时，违背了帧级一一映射的基本假设，导致知识转移失真。为此，作者提出采用自回归型教师模型进行因果强制（Causal Forcing）ODE初始化，从理论上补齐架构鸿沟，并采用此策略优化AR学生模型。

Result: 实验表明，所提方法在所有评测指标上均超越现有主流方法，其中在动态度（Dynamic Degree）、视觉奖励（VisionReward）和指令遵循（Instruction Following）分别比SOTA提升19.3%、8.7%和16.7%。

Conclusion: Causal Forcing有效弥合了自回归模型与扩散模型在蒸馏过程中的结构差距，为实时高质量交互式视频生成提供了更优解决方案，并在多个实验指标上实现了性能突破。

Abstract: To achieve real-time interactive video generation, current methods distill pretrained bidirectional video diffusion models into few-step autoregressive (AR) models, facing an architectural gap when full attention is replaced by causal attention. However, existing approaches do not bridge this gap theoretically. They initialize the AR student via ODE distillation, which requires frame-level injectivity, where each noisy frame must map to a unique clean frame under the PF-ODE of an AR teacher. Distilling an AR student from a bidirectional teacher violates this condition, preventing recovery of the teacher's flow map and instead inducing a conditional-expectation solution, which degrades performance. To address this issue, we propose Causal Forcing that uses an AR teacher for ODE initialization, thereby bridging the architectural gap. Empirical results show that our method outperforms all baselines across all metrics, surpassing the SOTA Self Forcing by 19.3\% in Dynamic Degree, 8.7\% in VisionReward, and 16.7\% in Instruction Following. Project page and the code: \href{https://thu-ml.github.io/CausalForcing.github.io/}{https://thu-ml.github.io/CausalForcing.github.io/}

</details>


### [257] [MIRROR: Manifold Ideal Reference ReconstructOR for Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2602.02222)
*Ruiqi Liu,Manni Cui,Ziheng Qin,Zhiyuan Yan,Ruoxin Chen,Yi Han,Zhiheng Li,Junkai Chen,ZhiJin Chen,Kaiqing Lin,Jialiang Shen,Lubin Weng,Jing Dong,Yan Wang,Shu Wu*

Main category: cs.CV

TL;DR: 本文提出了一种新的人造智能生成图像（AIGI）检测方法——MIRROR，通过对比真实图像流形，而非依赖特定伪造特征，提高了检测的泛化能力。实验显示MIRROR在多个基准上大幅优于现有方法，在识别高保真伪造图像方面接近甚至超越人类水平。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI模型能力提升，伪造图片与真实图片越来越难以区分，媒体安全面临严峻挑战。现有AIGI检测依赖于人工特征，难以适应生成伪迹的不断变化。作者认为，人类是通过判断图片与真实世界规律的一致性来识别伪造，因此希望借助这种稳定的认知特征改善检测。

Method: 作者将AIGI检测视为参考对比任务，提出MIRROR框架。该方法用一个可学习的离散记忆库来显式编码现实先验，并通过稀疏线性组合将输入映射到与流形一致的理想参考图像，再用残差作为检测信号。设计了Human-AIGI基准，包含人类难以察觉的伪造样本，以评测模型能否超越人类专家能力。

Result: 在14个基准上，MIRROR平均超越以往方法2.1%到8.1%。在Human-AIGI基准上，MIRROR在27个生成器下准确率达到89.6%，超过普通用户和视觉专家，并随着预训练模型规模扩大，结果逐步贴近人类感知极限。

Conclusion: MIRROR通过对齐到真实图像流形，提供了比以往伪迹检测更强泛化性的方法，能够在高难度场景下检出AIGI，具备取代人类专家的潜力。

Abstract: High-fidelity generative models have narrowed the perceptual gap between synthetic and real images, posing serious threats to media security. Most existing AI-generated image (AIGI) detectors rely on artifact-based classification and struggle to generalize to evolving generative traces. In contrast, human judgment relies on stable real-world regularities, with deviations from the human cognitive manifold serving as a more generalizable signal of forgery. Motivated by this insight, we reformulate AIGI detection as a Reference-Comparison problem that verifies consistency with the real-image manifold rather than fitting specific forgery cues. We propose MIRROR (Manifold Ideal Reference ReconstructOR), a framework that explicitly encodes reality priors using a learnable discrete memory bank. MIRROR projects an input into a manifold-consistent ideal reference via sparse linear combination, and uses the resulting residuals as robust detection signals. To evaluate whether detectors reach the "superhuman crossover" required to replace human experts, we introduce the Human-AIGI benchmark, featuring a psychophysically curated human-imperceptible subset. Across 14 benchmarks, MIRROR consistently outperforms prior methods, achieving gains of 2.1% on six standard benchmarks and 8.1% on seven in-the-wild benchmarks. On Human-AIGI, MIRROR reaches 89.6% accuracy across 27 generators, surpassing both lay users and visual experts, and further approaching the human perceptual limit as pretrained backbones scale. The code is publicly available at: https://github.com/349793927/MIRROR

</details>


### [258] [Evaluating OCR Performance for Assistive Technology: Effects of Walking Speed, Camera Placement, and Camera Type](https://arxiv.org/abs/2602.02223)
*Junchi Feng,Nikhil Ballem,Mahya Beheshti,Giles Hamilton-Fletcher,Todd Hudson,Maurizio Porfiri,William H. Seiple,John-Ross Rizzo*

Main category: cs.CV

TL;DR: 本研究系统性地评估了光学字符识别（OCR）在移动和静态条件下的性能，比较了多种设备和OCR引擎。结果显示识别准确率随步行速度、拍摄角度的变化而下降，并首次系统评测了移动场景下的OCR表现。Google Vision总体表现最佳，PaddleOCR表现也很接近。手机主摄像头和肩部佩戴的摄像头准确率最高。


<details>
  <summary>Details</summary>
Motivation: 现有OCR技术在辅助盲人和低视力人群的应用中多依赖静态数据集进行评估，未能充分反映真实移动场景下的挑战。为更好了解OCR在应用中的性能，尤其是在动态环境下的表现，迫切需要系统性的对比实验。

Method: 分别在静态和动态环境下评估OCR性能。静态测试涵盖不同距离（1-7米）和水平视角（0-75度）；动态测试通过调节步行速度（0.8-1.8 m/s）和摄像头位置（头戴、肩戴、手持），对手机主摄、超广角和智能眼镜的性能进行对比。四款主流OCR引擎（Google Vision，PaddleOCR 3.0，EasyOCR，Tesseract）在不同距离和角度下进行基准测试，字符级准确率用Levenshtein计算。

Result: 识别准确率随步行速度和拍摄角度增大而下降。Google Vision识别准确率最高，PaddleOCR 3.0是最强开源选手。手机主摄在所有设备中表现最佳；肩部摄像头佩戴位置整体准确率略高，但与头戴、手持差异无统计学意义。

Conclusion: 在真实移动场景下，OCR识别性能会受速度和角度影响。Google Vision表现最佳，PaddleOCR 3.0紧随其后。手机主摄优于其他摄像头，肩戴摄像头略优但差异不大。实验为辅助技术的实际部署和改进提供了可靠依据。

Abstract: Optical character recognition (OCR), which converts printed or handwritten text into machine-readable form, is widely used in assistive technology for people with blindness and low vision. Yet, most evaluations rely on static datasets that do not reflect the challenges of mobile use. In this study, we systematically evaluated OCR performance under both static and dynamic conditions. Static tests measured detection range across distances of 1-7 meters and viewing angles of 0-75 degrees horizontally. Dynamic tests examined the impact of motion by varying walking speed from slow (0.8 m/s) to very fast (1.8 m/s) and comparing three camera mounting positions: head-mounted, shoulder-mounted, and hand-held. We evaluated both a smartphone and smart glasses, using the phone's main and ultra-wide cameras. Four OCR engines were benchmarked to assess accuracy at different distances and viewing angles: Google Vision, PaddleOCR 3.0, EasyOCR, and Tesseract. PaddleOCR 3.0 was then used to evaluate accuracy at different walking speeds. Accuracy was computed at the character level using the Levenshtein ratio against manually defined ground truth. Results showed that recognition accuracy declined with increased walking speed and wider viewing angles. Google Vision achieved the highest overall accuracy, with PaddleOCR close behind as the strongest open-source alternative. Across devices, the phone's main camera achieved the highest accuracy, and a shoulder-mounted placement yielded the highest average among body positions; however, differences among shoulder, head, and hand were not statistically significant.

</details>


### [259] [Show, Don't Tell: Morphing Latent Reasoning into Image Generation](https://arxiv.org/abs/2602.02227)
*Harold Haodong Chen,Xinxiang Yin,Wen-Jie Shu,Hongfei Zhang,Zixin Zhang,Chenfei Liao,Litao Guo,Qifeng Chen,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的文本到图像生成框架LatentMorph，通过在连续隐空间中嵌入隐性推理流程，实现更高效和自适应的生成与优化。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成方法在推理和生成过程上缺乏灵活的、类人式的动态自我优化能力。显式推理方法涉及多次文本和图像的解码、重编码，带来信息损失与效率低下。为此，作者希望探索隐式推理的新范式，以增强模型智能。

Method: LatentMorph引入了四个轻量组件：（1）condenser用于将中间生成状态压缩为短期视觉记忆；（2）translator将隐式的推理结果转化为可操作的指导信息；（3）shaper动态优化下一个图片token预测；（4）RL-trained invoker自适应决定何时进行推理。这些推理都在连续的隐空间完成，规避了传统显式推理的限制。

Result: 实验结果显示，LatentMorph在GenEval和T2I-CompBench上分别将Janus-Pro模型提升16%和25%；在WISE和IPV-Txt等抽象推理任务上优于显式推理方法TwiG 15%和11%；推理时间缩短44%，token消耗降低51%；召回推理与人类直觉的认知一致性达71%。

Conclusion: LatentMorph框架通过隐性、连续空间推理显著提升了文本到图像生成的灵活性、效率和类人认知能力，为推理增强型生成模型提供了新思路。

Abstract: Text-to-image (T2I) generation has achieved remarkable progress, yet existing methods often lack the ability to dynamically reason and refine during generation--a hallmark of human creativity. Current reasoning-augmented paradigms most rely on explicit thought processes, where intermediate reasoning is decoded into discrete text at fixed steps with frequent image decoding and re-encoding, leading to inefficiencies, information loss, and cognitive mismatches. To bridge this gap, we introduce LatentMorph, a novel framework that seamlessly integrates implicit latent reasoning into the T2I generation process. At its core, LatentMorph introduces four lightweight components: (i) a condenser for summarizing intermediate generation states into compact visual memory, (ii) a translator for converting latent thoughts into actionable guidance, (iii) a shaper for dynamically steering next image token predictions, and (iv) an RL-trained invoker for adaptively determining when to invoke reasoning. By performing reasoning entirely in continuous latent spaces, LatentMorph avoids the bottlenecks of explicit reasoning and enables more adaptive self-refinement. Extensive experiments demonstrate that LatentMorph (I) enhances the base model Janus-Pro by $16\%$ on GenEval and $25\%$ on T2I-CompBench; (II) outperforms explicit paradigms (e.g., TwiG) by $15\%$ and $11\%$ on abstract reasoning tasks like WISE and IPV-Txt, (III) while reducing inference time by $44\%$ and token consumption by $51\%$; and (IV) exhibits $71\%$ cognitive alignment with human intuition on reasoning invocation.

</details>


### [260] [LiFlow: Flow Matching for 3D LiDAR Scene Completion](https://arxiv.org/abs/2602.02232)
*Andrea Matteazzi,Dietmar Tutsch*

Main category: cs.CV

TL;DR: 该论文提出了首个用于3D激光雷达场景补全的流匹配框架LiFlow，有效提升了点云补全的表现，并达到了多项指标上的最新最优成果。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的激光雷达点云数据经常因遮挡和距离过远而稀疏不全，影响系统的环境感知能力。现有方法多采用扩散模型，存在训练和推理初始分布不一致的问题，导致性能受限。本文旨在解决这一分布不一致带来的泛化性和补全质量问题。

Method: 提出了一种基于流匹配（flow matching）的点云生成补全过程模型，统一训练和推理阶段的初始数据分布。具体地，模型使用最近邻流匹配损失和Chamfer距离损失，分别强化点云补全的局部结构和全局覆盖效果。

Result: 所提LiFlow模型在多个数据集和评测指标上取得了当前最优水平，表明其在点云补全任务中的有效性和优越性。

Conclusion: 流匹配方法解决了传统扩散模型在点云补全中的训练-推理分布不一致难题，整体提升了场景补全的准确性和泛化。LiFlow作为一种新范式，为增强自动驾驶3D感知能力提供了有力工具。

Abstract: In autonomous driving scenarios, the collected LiDAR point clouds can be challenged by occlusion and long-range sparsity, limiting the perception of autonomous driving systems. Scene completion methods can infer the missing parts of incomplete 3D LiDAR scenes. Recent methods adopt local point-level denoising diffusion probabilistic models, which require predicting Gaussian noise, leading to a mismatch between training and inference initial distributions. This paper introduces the first flow matching framework for 3D LiDAR scene completion, improving upon diffusion-based methods by ensuring consistent initial distributions between training and inference. The model employs a nearest neighbor flow matching loss and a Chamfer distance loss to enhance both local structure and global coverage in the alignment of point clouds. LiFlow achieves state-of-the-art performance across multiple metrics. Code: https://github.com/matteandre/LiFlow.

</details>


### [261] [Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation](https://arxiv.org/abs/2602.02318)
*Xiang Li,Yupeng Zheng,Pengfei Li,Yilun Chen,Ya-Qin Zhang,Wenchao Ding*

Main category: cs.CV

TL;DR: 本文提出DiScene，一种融合多层次知识蒸馏的新型稀疏查询框架，实现高效且鲁棒的占用预测，在多个公开数据集上取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 当前占用预测方法在效率和准确性上存在权衡。密集方法对空体素计算浪费大，常规稀疏查询法又在复杂多样环境下易失效。为此，作者希望设计兼具高效与鲁棒性的占用预测方法。

Method: DiScene提出两大创新：1）多层次一致知识蒸馏策略，从大教师模型分四层（编码器、查询、空间、锚点）向轻量学生模型传递层次表示；2）教师引导初始化策略，通过优化参数冷启动过程，加速模型收敛。同时在不依赖深度先验和融合深度信息两种情况下进行了测试。

Result: 在Occ-Scannet基准上，DiScene在无深度先验下达到23.2 FPS，性能超OPUS基线36.1%，甚至优于深度增强版OPUS†。融合深度后，DiScene†超越EmbodiedOcc 3.7%，推理速度提升1.62倍，并在Occ3D-nuScenes和真实环境均有优表现。

Conclusion: DiScene兼具高效与鲁棒，推进了3D占用预测领域的性能上限，对实际机器人感知场景应用有很大潜力。

Abstract: Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at https://github.com/getterupper/DiScene.

</details>


### [262] [VQ-Style: Disentangling Style and Content in Motion with Residual Quantized Representations](https://arxiv.org/abs/2602.02334)
*Fatemeh Zargarbashi,Dhruv Agrawal,Jakob Buhmann,Martin Guay,Stelian Coros,Robert W. Sumner*

Main category: cs.CV

TL;DR: 提出一种新方法，将人体动作数据中的内容和风格进行有效解耦，实现动作风格迁移，无需微调即可适用于新风格。


<details>
  <summary>Details</summary>
Motivation: 人体动作数据中既包含语义内容，也有细腻的风格信息，而如何将两者分离以实现风格迁移一直具有挑战性。作者希望解决风格与内容无法有效解耦的问题。

Method: 提出利用残差矢量量化变分自编码器（RVQ-VAE）来从粗到细地学习动作表示，并结合对比学习与新颖的信息泄露损失，以代码本方式强化内容与风格的分离。通过量化代码交换（Quantized Code Swapping）实现无需针对新风格微调的动作风格迁移。

Result: 实验表明，该方法在动作风格迁移、风格移除、动作融合等应用上表现出较强的通用性和有效性。

Conclusion: 该框架能够有效解耦人体动作的内容与风格，实现高质量的风格迁移及多种下游任务，并具备无需微调即可处理新风格的优势。

Abstract: Human motion data is inherently rich and complex, containing both semantic content and subtle stylistic features that are challenging to model. We propose a novel method for effective disentanglement of the style and content in human motion data to facilitate style transfer. Our approach is guided by the insight that content corresponds to coarse motion attributes while style captures the finer, expressive details. To model this hierarchy, we employ Residual Vector Quantized Variational Autoencoders (RVQ-VAEs) to learn a coarse-to-fine representation of motion. We further enhance the disentanglement by integrating contrastive learning and a novel information leakage loss with codebook learning to organize the content and the style across different codebooks. We harness this disentangled representation using our simple and effective inference-time technique Quantized Code Swapping, which enables motion style transfer without requiring any fine-tuning for unseen styles. Our framework demonstrates strong versatility across multiple inference applications, including style transfer, style removal, and motion blending.

</details>


### [263] [LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization](https://arxiv.org/abs/2602.02341)
*Zhenpeng Huang,Jiaqi Li,Zihan Jia,Xinhao Li,Desen Meng,Lingxue Song,Xi Chen,Liang Li,Limin Wang*

Main category: cs.CV

TL;DR: 提出了一种无需长视频标注的新方法LongVPO，实现了短上下文视觉-语言模型对超长视频的高效、稳健理解，且优于现有开源模型。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型对超长视频的理解能力有限，主要受限于模型上下文窗口、计算开销高以及缺乏长视频标注。亟需一种高效、无需人工标注且能扩展短上下文模型到长视频的新范式。

Method: 方法分两阶段：第一阶段，合成锚定短视频片段的问题-偏好三元组，结合视觉相似性及问题特异性过滤，弱化位置偏差，仅评估短片片段降低计算量；第二阶段，递归生成长视频分段描述，再利用大语言模型设计多段推理问题及非优解答案，通过多段推理任务调整模型偏好。训练只需16K合成样本，无须人工标注。

Result: LongVPO在多个长视频基准上优于现有SOTA开源模型，同时在短视频理解任务上也表现优异（如MVBench），无需增加大量算力或人工标注。

Conclusion: LongVPO为高效、可扩展的长视频理解提供了一条新路，显著提升性能且无需高额的人力成本。

Abstract: We present LongVPO, a novel two-stage Direct Preference Optimization framework that enables short-context vision-language models to robustly understand ultra-long videos without any long-video annotations. In Stage 1, we synthesize preference triples by anchoring questions to individual short clips, interleaving them with distractors, and applying visual-similarity and question-specificity filtering to mitigate positional bias and ensure unambiguous supervision. We also approximate the reference model's scoring over long contexts by evaluating only the anchor clip, reducing computational overhead. In Stage 2, we employ a recursive captioning pipeline on long videos to generate scene-level metadata, then use a large language model to craft multi-segment reasoning queries and dispreferred responses, aligning the model's preferences through multi-segment reasoning tasks. With only 16K synthetic examples and no costly human labels, LongVPO outperforms the state-of-the-art open-source models on multiple long-video benchmarks, while maintaining strong short-video performance (e.g., on MVBench), offering a scalable paradigm for efficient long-form video understanding.

</details>


### [264] [Implicit neural representation of textures](https://arxiv.org/abs/2602.02354)
*Albert Kwok,Zheyuan Hu,Dounia Hammou*

Main category: cs.CV

TL;DR: 本论文探索了不同神经网络结构如何作为新的隐式神经表示（INR）用于纹理表示，并在UV空间中实现连续操作，实验表明方法在图像质量、内存消耗和渲染推理时间方面表现良好，并探讨了其在实时渲染、mipmap匹配和INR空间生成等应用。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示已被证实在多个领域中高效且准确，但传统纹理表达通常依赖离散方式。本文旨在设计连续型纹理INR，并研究其在不同指标之间的平衡及其应用潜力。

Method: 设计多种神经网络架构作为纹理INR，令其在连续的UV坐标空间内操作，并通过大量实验对比、分析其在图像质量、内存消耗和渲染时间等方面的性能。进一步，将所提出方法应用于实时渲染、mipmap拟合及INR空间生成等场景。

Result: 实验证明所设计的INR在图像质量、内存消耗及渲染推理时间方面实现了良好表现，能够在不同指标之间达到合理权衡，并在多个下游任务中表现出应用潜力。

Conclusion: 本文提出的基于连续隐式神经纹理表达的多种神经网络架构不仅在性能上具备优势，还能服务于众多计算机图形相关任务，未来有望在实时渲染等领域得到更广泛应用。

Abstract: Implicit neural representation (INR) has proven to be accurate and efficient in various domains. In this work, we explore how different neural networks can be designed as a new texture INR, which operates in a continuous manner rather than a discrete one over the input UV coordinate space. Through thorough experiments, we demonstrate that these INRs perform well in terms of image quality, with considerable memory usage and rendering inference time. We analyze the balance between these objectives. In addition, we investigate various related applications in real-time rendering and down-stream tasks, e.g. mipmap fitting and INR-space generation.

</details>


### [265] [NAB: Neural Adaptive Binning for Sparse-View CT reconstruction](https://arxiv.org/abs/2602.02356)
*Wangduo Xie,Matthew B. Blaschko*

Main category: cs.CV

TL;DR: 本文提出了一种新型神经自适应分箱（NAB）方法，通过将矩形形状先验融入稀疏视角CT重建过程，显著提升了重建精度，并在工业和医学数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 针对工业CT稀疏采样重建精度有限、传统神经网络无法有效利用物体形状先验（如常见的矩形结构）的问题，亟需新的方法提升稀疏重建质量，并降低生产成本。

Method: 提出NAB方法，将空间坐标映射为“分箱”向量空间。据点采用基于平移双曲正切函数差值的新型分箱机制，该机制可绕输入平面法向旋转，实现对矩形结构的高效表达。通过端到端优化，自动调整分箱参数（位置、大小、陡峭度和角度），结合神经网络预测衰减系数。平滑度可调，适应更复杂形状。

Result: 在两个工业数据集上，NAB方法显著优于现有方法。在将分箱函数推广到更通用表达后，对医学数据集也表现稳健。实验充分验证了方法的有效性和适应性。

Conclusion: NAB创新性地将矩形结构形状先验集成进神经网络CT重建框架，为神经重建方法引入先验信息提供了新思路，并具备良好实验表现和应用潜力。

Abstract: Computed Tomography (CT) plays a vital role in inspecting the internal structures of industrial objects. Furthermore, achieving high-quality CT reconstruction from sparse views is essential for reducing production costs. While classic implicit neural networks have shown promising results for sparse reconstruction, they are unable to leverage shape priors of objects. Motivated by the observation that numerous industrial objects exhibit rectangular structures, we propose a novel \textbf{N}eural \textbf{A}daptive \textbf{B}inning (\textbf{NAB}) method that effectively integrates rectangular priors into the reconstruction process. Specifically, our approach first maps coordinate space into a binned vector space. This mapping relies on an innovative binning mechanism based on differences between shifted hyperbolic tangent functions, with our extension enabling rotations around the input-plane normal vector. The resulting representations are then processed by a neural network to predict CT attenuation coefficients. This design enables end-to-end optimization of the encoding parameters -- including position, size, steepness, and rotation -- via gradient flow from the projection data, thus enhancing reconstruction accuracy. By adjusting the smoothness of the binning function, NAB can generalize to objects with more complex geometries. This research provides a new perspective on integrating shape priors into neural network-based reconstruction. Extensive experiments demonstrate that NAB achieves superior performance on two industrial datasets. It also maintains robust on medical datasets when the binning function is extended to more general expression. The code will be made available.

</details>


### [266] [Uncertainty-Aware Image Classification In Biomedical Imaging Using Spectral-normalized Neural Gaussian Processes](https://arxiv.org/abs/2602.02370)
*Uma Meleti,Jeffrey J. Nirschl*

Main category: cs.CV

TL;DR: 本文提出了一种改进的不确定性感知模型SNGP，用于提升数字病理领域中对异常样本识别能力，同时保持分类性能，为临床安全部署提供支持。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在数字病理领域中对异常分布的样本（OOD）识别能力较弱，且在不确定性估计上表现不佳，这降低了临床信任度并影响采纳。解决模型过度自信及可靠性不足的问题，对于医学图像的安全应用至关重要。

Method: 作者实现并测试了Spectral-normalized Neural Gaussian Process（SNGP），通过对神经网络添加谱归一化，并将最后的全连接层替换为高斯过程层来提升不确定性估计能力，并与确定性模型和Monte Carlo Dropout进行了对比。

Result: SNGP在六个涉及三类生物医学分类任务的数据集上，表现出与现有方法相当的分布内分类性能，但显著提升了模型的不确定性估计质量和异常分布(OOD)检测能力。

Conclusion: SNGP及类似模型在数字病理学中的不确定性感知分类任务表现优异，有助于提升部署安全性并增强病理学家的信任。

Abstract: Accurate histopathologic interpretation is key for clinical decision-making; however, current deep learning models for digital pathology are often overconfident and poorly calibrated in out-of-distribution (OOD) settings, which limit trust and clinical adoption. Safety-critical medical imaging workflows benefit from intrinsic uncertainty-aware properties that can accurately reject OOD input. We implement the Spectral-normalized Neural Gaussian Process (SNGP), a set of lightweight modifications that apply spectral normalization and replace the final dense layer with a Gaussian process layer to improve single-model uncertainty estimation and OOD detection. We evaluate SNGP vs. deterministic and MonteCarlo dropout on six datasets across three biomedical classification tasks: white blood cells, amyloid plaques, and colorectal histopathology. SNGP has comparable in-distribution performance while significantly improving uncertainty estimation and OOD detection. Thus, SNGP or related models offer a useful framework for uncertainty-aware classification in digital pathology, supporting safe deployment and building trust with pathologists.

</details>


### [267] [Unified Personalized Reward Model for Vision Generation](https://arxiv.org/abs/2602.02380)
*Yibin Wang,Yuhang Zang,Feng Han,Jiazi Bu,Yujie Zhou,Cheng Jin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 本文提出了一种适用于视觉生成的个性化奖励模型UnifiedReward-Flex，在推理上更加灵活、具备上下文自适应能力，在主观与多样化人类偏好面前表现更优。实验证明，该方法在图像和视频生成任务中表现出更好的对齐和判别能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态奖励模型以统一标准评判生成内容，忽略了具体内容和上下文，导致与人类主观偏好的系统性不一致。作者受人类评判过程启发，尝试引入个性化与灵活判别机制以解决上述不足。

Method: UnifiedReward-Flex融合了奖励建模与上下文自适应推理，具体做法包括：先结合生成内容及其语义意图动态构建分层评估标准，然后采用两阶段训练（先进VLM蒸馏推理、偏好对直接优化）提升模型判别性和对齐能力。

Result: UnifiedReward-Flex集成进图像、视频生成系统，实验中显示出强化推理能力和主观偏好的区分能力，优于现有方法。

Conclusion: UnifiedReward-Flex能更好地应对复杂、多元的视觉生成场景，为提升生成内容的人类偏好对齐性提供了有效方案。

Abstract: Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis, and extensive results demonstrate its superiority.

</details>


### [268] [Personalized Image Generation via Human-in-the-loop Bayesian Optimization](https://arxiv.org/abs/2602.02388)
*Rajalaxmi Rajagopalan,Debottam Dutta,Yu-Lin Wei,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: 本文提出了一种结合人类偏好反馈的个性化图像生成方法，有效拉近生成结果与用户心中目标图像的距离。


<details>
  <summary>Details</summary>
Motivation: 当前基于语言描述的生成模型在实际应用中存在局限性，尤其在用户难以用语言精准表达目标图像细节时，生成图像与用户理想图像存在差距。作者希望研究如何在语言无法继续缩小差距时，通过人类反馈进一步优化生成结果。

Method: 作者提出MultiBO（Multi-Choice Preferential Bayesian Optimization）算法。方法流程是：1）基于已有生成图像$x^{p*}$，生成$K$个新图像；2）用户在这些图像中给出相对偏好反馈；3）方法利用反馈调优扩散模型，再生成下一轮$K$张图片。多轮反馈迭代后，将生成更加接近用户内心理想的目标图像。

Result: 通过$30$位用户的主观评分以及与$5$个主流基线方法的定量对比，结果显示该方法能显著提升生成图像与用户心中目标图像之间的相似度，超越基线。

Conclusion: 实验表明：用多选式人类偏好反馈指导扩散模型，可以在缺乏明确目标图像的情况下，逐步得到更符合用户需求的个性化生成图像。这一方法很适合于无法直接描述目标的生成场景。

Abstract: Imagine Alice has a specific image $x^\ast$ in her mind, say, the view of the street in which she grew up during her childhood. To generate that exact image, she guides a generative model with multiple rounds of prompting and arrives at an image $x^{p*}$. Although $x^{p*}$ is reasonably close to $x^\ast$, Alice finds it difficult to close that gap using language prompts. This paper aims to narrow this gap by observing that even after language has reached its limits, humans can still tell when a new image $x^+$ is closer to $x^\ast$ than $x^{p*}$. Leveraging this observation, we develop MultiBO (Multi-Choice Preferential Bayesian Optimization) that carefully generates $K$ new images as a function of $x^{p*}$, gets preferential feedback from the user, uses the feedback to guide the diffusion model, and ultimately generates a new set of $K$ images. We show that within $B$ rounds of user feedback, it is possible to arrive much closer to $x^\ast$, even though the generative model has no information about $x^\ast$. Qualitative scores from $30$ users, combined with quantitative metrics compared across $5$ baselines, show promising results, suggesting that multi-choice feedback from humans can be effectively harnessed for personalized image generation.

</details>


### [269] [Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory](https://arxiv.org/abs/2602.02393)
*Ruiqi Wu,Xuanhua He,Meng Cheng,Tianyu Yang,Yong Zhang,Zhuoliang Kang,Xunliang Cai,Xiaoming Wei,Chunle Guo,Chongyi Li,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: 本文提出了Infinite-World模型，实现了在复杂真实环境下超千帧图像的连贯视觉记忆，显著提升了世界建模长期空间与动作一致性。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型虽然在合成数据上有效，但在真实视频中由于姿态估算噪声和观测点稀少，建模效果不佳。因此需要新方法提升对真实世界、长序列视频的建模能力。

Method: 1. 引入分层无姿态记忆压缩器（HPMC），用递归方法将历史信息压缩为定长表示，无需显式几何先验。
2. 提出不确定性感知动作离散模块，将连续动作空间离散为三态逻辑，增强对噪声轨迹的鲁棒性。
3. 采用重访丰富微调策略，利用紧凑数据集激发模型长程回环能力。

Result: 在多个客观指标和用户实验中，Infinite-World在视觉质量、动作可控性和空间一致性等方面均优于现有方法。

Conclusion: Infinite-World模型能有效在真实环境下记忆和生成长序列，提升世界模型的实用性和性能，具有广阔应用前景。

Abstract: We propose Infinite-World, a robust interactive world model capable of maintaining coherent visual memory over 1000+ frames in complex real-world environments. While existing world models can be efficiently optimized on synthetic data with perfect ground-truth, they lack an effective training paradigm for real-world videos due to noisy pose estimations and the scarcity of viewpoint revisits. To bridge this gap, we first introduce a Hierarchical Pose-free Memory Compressor (HPMC) that recursively distills historical latents into a fixed-budget representation. By jointly optimizing the compressor with the generative backbone, HPMC enables the model to autonomously anchor generations in the distant past with bounded computational cost, eliminating the need for explicit geometric priors. Second, we propose an Uncertainty-aware Action Labeling module that discretizes continuous motion into a tri-state logic. This strategy maximizes the utilization of raw video data while shielding the deterministic action space from being corrupted by noisy trajectories, ensuring robust action-response learning. Furthermore, guided by insights from a pilot toy study, we employ a Revisit-Dense Finetuning Strategy using a compact, 30-minute dataset to efficiently activate the model's long-range loop-closure capabilities. Extensive experiments, including objective metrics and user studies, demonstrate that Infinite-World achieves superior performance in visual quality, action controllability, and spatial consistency.

</details>


### [270] [Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation](https://arxiv.org/abs/2602.02401)
*Xinshun Wang,Peiming Li,Ziyi Wang,Zhongbin Fang,Zhichao Deng,Songtao Wu,Jason Li,Mengyuan Liu*

Main category: cs.CV

TL;DR: 本文提出了一种统一框架Superman，将视觉感知与基于骨架的运动生成任务结合，突破现有运动分析方法割裂问题，在各项任务上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 当前运动分析领域存在三个主要问题：1）感知模型只能输出文本，无法生成动作；2）生成模型难以从视觉输入理解动作且多只处理静态姿态；3）现有动作词典仅基于骨架，缺乏视觉联系。这造成了感知与生成任务割裂，模型适用范围有限。

Method: Superman框架包括两部分：一是提出了视觉引导的动作分词器（Vision-Guided Motion Tokenizer），利用3D骨架和视觉数据的几何对齐，实现跨模态联合学习，形成统一动作词典；二是在这一动作语言基础上，训练了统一的多模态大模型（MLLM），同时处理视频感知（3D骨架姿态估计）和基于骨架的动作生成（预测、插值）等任务。

Result: 在Human3.6M等标准基准上，Superman方法在所有运动分析任务上均达到SOTA或具竞争力的性能，展现出优秀的一致性和泛化能力。

Conclusion: 本工作证明了将视觉与动作生成任务统一建模的可行性和优势，为高效、可扩展的人体动作分析开辟了新路径。

Abstract: Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception'' models that understand motion from video but only output text, and ``generation'' models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons.

</details>


### [271] [ReasonEdit: Editing Vision-Language Models using Human Reasoning](https://arxiv.org/abs/2602.02408)
*Jiaxing Qiu,Kaihua Hou,Roxana Daneshjou,Ahmed Alaa,Thomas Hartvigsen*

Main category: cs.CV

TL;DR: 本文提出ReasonEdit，这是首个允许用户在编辑视觉-语言模型（VLM）时解释其推理过程的编辑方法。ReasonEdit在难推理任务中表现出色，并在多个数据集和主流VLM上实现了最先进的编辑效果。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉-语言模型编辑器主要只针对简单任务，很少处理需要复杂推理的场景，而这些任务需要模型和人进行深入推理。缺乏面向推理任务的高效编辑方法成为了制约VLM实际应用的重要问题。

Method: 提出ReasonEdit方法，在模型编辑时让用户同步解释推理过程，将推理过程存储到codebook中。推理检索基于一种受网络科学启发的拓扑均衡多模态嵌入新方法，从而在推理/编辑时只检索与当前问题最相关的推理事实。

Result: 在四个主流VLM和多个以推理为核心的视觉问答数据集上，ReasonEdit在模型编辑任务中取得了最先进的性能，显著提升了模型编辑的泛化能力。

Conclusion: 将人类推理融入模型编辑不仅可以纠错，还能极大提升编辑的泛化效果，为推理类多模态任务中的高效知识注入和修正提供了新思路。

Abstract: Model editing aims to correct errors in large, pretrained models without altering unrelated behaviors. While some recent works have edited vision-language models (VLMs), no existing editors tackle reasoning-heavy tasks, which typically require humans and models to reason about images.We therefore propose ReasonEdit, the first VLM editor to let users explain their reasoning during editing, introducing a new, practical model editing setup. ReasonEdit continuously stores human reasoning in a codebook, and retrieves only relevant facts during inference using a novel topology-balanced multimodal embedding method inspired by network science. Across four VLMs on multiple rationale-based visual question answering datasets, ReasonEdit achieves state-of-the-art editing performance, ultimately showing that using human reasoning during editing greatly improves edit generalization.

</details>


### [272] [Catalyst: Out-of-Distribution Detection via Elastic Scaling](https://arxiv.org/abs/2602.02409)
*Abid Hassan,Tuan Ngo,Saad Shafiq,Nenad Medvidovic*

Main category: cs.CV

TL;DR: Catalyst是一种用于提升深度神经网络分布外检测性能的后处理框架，通过利用卷积层池化前的通道统计量来增强现有方法，显著降低了误报率。


<details>
  <summary>Details</summary>
Motivation: 目前优秀的OOD检测方法多依赖分类器输出的logits或全局平均池化后的特征向量，但忽略了池化前特征图的通道统计信息，这些信号可能携带丰富且互补的信息，因此需要加以利用。

Method: 作者提出Catalyst框架，实时从池化前特征图的原始通道统计量（如均值、标准差、最大激活值）计算输入相关的缩放因子γ，并与现有的分数进行弹性融合（乘性调制），进一步区分分布内与分布外样本。Catalyst可以结合基于logit、特征或距离的多种OOD检测方法。

Result: Catalyst大幅提升各类OOD检测器性能，在CIFAR-10（ResNet-18）上平均误报率下降32.87%，CIFAR-100（ResNet-18）下降27.94%，ImageNet（ResNet-50）下降22.25%。

Conclusion: 池化前通道统计包含重要信息，Catalyst能充分挖掘其潜力，对现有OOD检测方法具有良好兼容性和提升效果。

Abstract: Out-of-distribution (OOD) detection is critical for the safe deployment of deep neural networks. State-of-the-art post-hoc methods typically derive OOD scores from the output logits or penultimate feature vector obtained via global average pooling (GAP). We contend that this exclusive reliance on the logit or feature vector discards a rich, complementary signal: the raw channel-wise statistics of the pre-pooling feature map lost in GAP. In this paper, we introduce Catalyst, a post-hoc framework that exploits these under-explored signals. Catalyst computes an input-dependent scaling factor ($γ$) on-the-fly from these raw statistics (e.g., mean, standard deviation, and maximum activation). This $γ$ is then fused with the existing baseline score, multiplicatively modulating it -- an ``elastic scaling'' -- to push the ID and OOD distributions further apart. We demonstrate Catalyst is a generalizable framework: it seamlessly integrates with logit-based methods (e.g., Energy, ReAct, SCALE) and also provides a significant boost to distance-based detectors like KNN. As a result, Catalyst achieves substantial and consistent performance gains, reducing the average False Positive Rate by 32.87 on CIFAR-10 (ResNet-18), 27.94% on CIFAR-100 (ResNet-18), and 22.25% on ImageNet (ResNet-50). Our results highlight the untapped potential of pre-pooling statistics and demonstrate that Catalyst is complementary to existing OOD detection approaches.

</details>


### [273] [SelvaMask: Segmenting Trees in Tropical Forests and Beyond](https://arxiv.org/abs/2602.02426)
*Simon-Olivier Duguay,Hugo Baudchon,Etienne Laliberté,Helene Muller-Landau,Gonzalo Rivas-Torres,Arthur Ouaknine*

Main category: cs.CV

TL;DR: 该论文针对热带森林树冠识别难题，提出了SelvaMask数据集和基于VFM的分割新方法，在密集森林环境下显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的树冠分割方法（如transformer模型）有所进步，但在热带森林等复杂环境中的表现依然较差，亟需更具挑战性的基准数据集和有效的新方法来推动该领域发展。

Method: 论文发布了SelvaMask数据集，包含三处新热带森林的8,800多个人工精细标注的树冠，并进行注释一致性分析。方法上，提出了结合视觉基础模型（VFM）与领域专用检测提示（detection-prompter）的模块化检测-分割流水线。

Result: 提出的方法在密集热带森林数据和外部温带数据集上均取得了领先的分割表现，优于零样本通用模型和端到端有监督方案。

Conclusion: SelvaMask不仅成为树冠分割的重要基准，也推动了热带乃至全球森林普适监测的发展，代码与数据将公开释放。

Abstract: Tropical forests harbor most of the planet's tree biodiversity and are critical to global ecological balance. Canopy trees in particular play a disproportionate role in carbon storage and functioning of these ecosystems. Studying canopy trees at scale requires accurate delineation of individual tree crowns, typically performed using high-resolution aerial imagery. Despite advances in transformer-based models for individual tree crown segmentation, performance remains low in most forests, especially tropical ones. To this end, we introduce SelvaMask, a new tropical dataset containing over 8,800 manually delineated tree crowns across three Neotropical forest sites in Panama, Brazil, and Ecuador. SelvaMask features comprehensive annotations, including an inter-annotator agreement evaluation, capturing the dense structure of tropical forests and highlighting the difficulty of the task. Leveraging this benchmark, we propose a modular detection-segmentation pipeline that adapts vision foundation models (VFMs), using domain-specific detection-prompter. Our approach reaches state-of-the-art performance, outperforming both zero-shot generalist models and fully supervised end-to-end methods in dense tropical forests. We validate these gains on external tropical and temperate datasets, demonstrating that SelvaMask serves as both a challenging benchmark and a key enabler for generalized forest monitoring. Our code and dataset will be released publicly.

</details>


### [274] [UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing](https://arxiv.org/abs/2602.02437)
*Dianyi Wang,Chaofan Ma,Feng Han,Size Wu,Wei Song,Yibin Wang,Zhixiong Zhang,Tianhang Wang,Siyuan Wang,Zhongyu Wei,Jiaqi Wang*

Main category: cs.CV

TL;DR: 本文提出了UniReason统一多模态框架，以协调文本生成和图像编辑能力，在复杂推理任务中表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型在深度推理场景下表现有限，且常将文本生成与图像编辑分开，缺乏推理协同。作者试图打破两者割裂，提升复杂任务处理能力。

Method: 提出双重推理范式，将生成任务视为有知识增强的规划环节，引入隐式约束；编辑作为细粒度视觉调整，通过自反纠错。两者在统一表示空间融合，并类比人类先规划再完善的认知流程。作者还构建了包含五大知识领域、约30万样本的推理数据集以及视觉自纠正数据集。

Result: UniReason在WISE、KrisBench与UniREditBench等推理密集型基准上取得了先进表现，同时保持了一般合成任务的优越性能。

Conclusion: UniReason实现了生成与编辑能力的推理统一，推动了多模态模型在复杂推理任务中的应用，具有较强的通用性和拓展潜力。

Abstract: Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities.

</details>


### [275] [Multi-head automated segmentation by incorporating detection head into the contextual layer neural network](https://arxiv.org/abs/2602.02471)
*Edwin Kys,Febian Febian*

Main category: cs.CV

TL;DR: 本文提出了一种新型基于Swin U-Net和Transformer的自动分割模型，通过引入门控检测机制显著减少了医学图像分割中的虚假正例（幻觉）问题。实验结果显示该方法比传统分割模型表现更优，尤其在消除无结构切片中的错误分割方面。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习自动分割模型在放射治疗中常出现失真分割（如幻觉）问题，尤其在缺乏目标结构的切片中更为严重。为提升分割结果的解剖学合理性和可靠性，亟需开发能有效抑制无效分割的方法。

Method: 作者基于Swin U-Net结构，融合多头Transformer机制，引入跨切片上下文信息整合，并新增并行检测分支，通过多层感知机进行切片级检测，与像素级分割流共同作用。检测输出用于门控分割预测，抑制解剖无效切片中的虚假正例，并在训练时采用切片级Tversky损失函数以缓解类别不平衡。

Result: 在The Cancer Imaging Archive的Prostate-Anatomical-Edge-Cases数据集上测试，门控模型在Dice损失上（0.013±0.036）显著优于无门控基线模型（0.732±0.314），检测概率与解剖结构实际存在强相关，几乎完全去除了虚假分割。无门控模型在所有切片上均表现出更大波动和持续虚假正例。

Conclusion: 基于检测门控的分割方法能显著提升自动分割的解剖合理性与鲁棒性，在临床放射治疗自动勾画工作流中具备极大应用前景，有效减少幻觉预测且不影响有效区域的分割质量。

Abstract: Deep learning based auto segmentation is increasingly used in radiotherapy, but conventional models often produce anatomically implausible false positives, or hallucinations, in slices lacking target structures. We propose a gated multi-head Transformer architecture based on Swin U-Net, augmented with inter-slice context integration and a parallel detection head, which jointly performs slice-level structure detection via a multi-layer perceptron and pixel-level segmentation through a context-enhanced stream. Detection outputs gate the segmentation predictions to suppress false positives in anatomically invalid slices, and training uses slice-wise Tversky loss to address class imbalance. Experiments on the Prostate-Anatomical-Edge-Cases dataset from The Cancer Imaging Archive demonstrate that the gated model substantially outperforms a non-gated segmentation-only baseline, achieving a mean Dice loss of $0.013 \pm 0.036$ versus $0.732 \pm 0.314$, with detection probabilities strongly correlated with anatomical presence, effectively eliminating spurious segmentations. In contrast, the non-gated model exhibited higher variability and persistent false positives across all slices. These results indicate that detection-based gating enhances robustness and anatomical plausibility in automated segmentation applications, reducing hallucinated predictions without compromising segmentation quality in valid slices, and offers a promising approach for improving the reliability of clinical radiotherapy auto-contouring workflows.

</details>


### [276] [PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss](https://arxiv.org/abs/2602.02493)
*Zehong Ma,Ruihan Xu,Shiliang Zhang*

Main category: cs.CV

TL;DR: PixelGen是一种在像素空间直接生成图像的端到端扩散生成框架，通过引入感知损失，在保持简单结构的同时，大幅提升了生成质量，超越了主流隐空间扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有的像素空间扩散（Pixel Diffusion）方法，尽管有潜力避免VAE等隐空间方法中的瓶颈，但受限于高维像素空间存在大量感知无关信号，优化难度大，生成质量落后于隐空间（Latent Space）扩散模型。该工作希望通过改进像素扩散模型，使其在不引入隐变量和额外阶段的情况下，提升生成质量，简化生成范式。

Method: 作者提出了PixelGen框架，在像素空间直接训练扩散模型。核心方法是引入两种感知损失：一种是LPIPS损失，用于提升局部纹理与结构的感知质量；另一种是基于DINO特征的感知损失，用于增强全局语义表达。感知监督引导模型学习对人类更有意义的感知流形。

Result: PixelGen在ImageNet-256数据集上无需classifier-free guidance，仅用80轮训练就实现了5.11的FID分数，优于当前强大的隐空间扩散基线。在大规模文本到图像任务中，获得0.79的GenEval分数，表现出良好的扩展性。

Conclusion: PixelGen结合了像素空间生成的端到端简洁性和高质量生成能力，避免了VAE、隐空间等复杂机制，是一种更简单而高效的生成新范式。代码已开源，便于复现和推广。

Abstract: Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [277] [PPoGA: Predictive Plan-on-Graph with Action for Knowledge Graph Question Answering](https://arxiv.org/abs/2602.00007)
*MinGyu Jeon,SuWan Cho,JaeYoung Shu*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的知识图谱问答（KGQA）方法PPoGA，通过元认知机制（特别是计划自我纠错与重构），显著提升了多跳复杂推理任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有结合知识图谱的大语言模型在复杂问题解答中虽有进展，但遇到初始推理计划失误时，往往无法自我修正和重构，类似于认知功能固着，导致问题不能有效解决。该问题限制了AI推理灵活性和鲁棒性。

Method: PPoGA采用类似人类的计划-执行器结构，将高层策略与底层执行分离，并通过预测性处理机制提前预判可能结果。核心在于引入了一套自我纠错机制，不仅校正执行过程中的局部路径错误，也能在整体计划无效时检测、废弃并重构整个推理方案。

Result: 在三个多跳KGQA数据集（GrailQA、CWQ、WebQSP）上进行大量实验，PPoGA取得了领先的性能，明显优于现有方法。

Conclusion: 本研究展示了计划自我纠错和任务重构等元认知能力对于增强AI系统推理灵活性的重要性，为构建更健壮、智能的推理系统提供了新方向。

Abstract: Large Language Models (LLMs) augmented with Knowledge Graphs (KGs) have advanced complex question answering, yet they often remain susceptible to failure when their initial high-level reasoning plan is flawed. This limitation, analogous to cognitive functional fixedness, prevents agents from restructuring their approach, leading them to pursue unworkable solutions. To address this, we propose PPoGA (Predictive Plan-on-Graph with Action), a novel KGQA framework inspired by human cognitive control and problem-solving. PPoGA incorporates a Planner-Executor architecture to separate high-level strategy from low-level execution and leverages a Predictive Processing mechanism to anticipate outcomes. The core innovation of our work is a self-correction mechanism that empowers the agent to perform not only Path Correction for local execution errors but also Plan Correction by identifying, discarding, and reformulating the entire plan when it proves ineffective. We conduct extensive experiments on three challenging multi-hop KGQA benchmarks: GrailQA, CWQ, and WebQSP. The results demonstrate that PPoGA achieves state-of-the-art performance, significantly outperforming existing methods. Our work highlights the critical importance of metacognitive abilities like problem restructuring for building more robust and flexible AI reasoning systems.

</details>


### [278] [Unlocking Electronic Health Records: A Hybrid Graph RAG Approach to Safe Clinical AI for Patient QA](https://arxiv.org/abs/2602.00009)
*Samuel Thio,Matthew Lewis,Spiros Denaxas,Richard JB Dobson*

Main category: cs.CL

TL;DR: 提出了一种创新的临床检索系统MediGRAF，将结构化和非结构化数据检索结合，提升了EHR环境下的信息获取能力，兼顾安全性和全面性。


<details>
  <summary>Details</summary>
Motivation: 当前EHR系统信息庞大，容易让临床医生忽略关键信息。LLM虽有潜力但存在上下文脱离和幻觉，现有检索方法也无法同时处理结构化与非结构化数据，信息整合能力有限。

Method: 提出MediGRAF，一种混合型图谱增强检索系统，结合Neo4j的Text2Cypher（用于结构化数据遍历）和向量嵌入（用于非结构化文本检索），支持对患者完整数据的自然语言问答。通过MIMIC-IV数据集的10例患者进行节点和关系生成实验，并在多种复杂度查询任务下进行评估。

Result: 对于事实性查询系统达到100%召回率，复杂推理任务的专家质量评分平均为4.25/5，且无安全违规发生。

Conclusion: 混合图谱检索系统有效提升了临床信息检索的质量和安全性，是传统LLM应用的更优替代方案。

Abstract: Electronic health record (EHR) systems present clinicians with vast repositories of clinical information, creating a significant cognitive burden where critical details are easily overlooked. While Large Language Models (LLMs) offer transformative potential for data processing, they face significant limitations in clinical settings, particularly regarding context grounding and hallucinations. Current solutions typically isolate retrieval methods focusing either on structured data (SQL/Cypher) or unstructured semantic search but fail to integrate both simultaneously. This work presents MediGRAF (Medical Graph Retrieval Augmented Framework), a novel hybrid Graph RAG system that bridges this gap. By uniquely combining Neo4j Text2Cypher capabilities for structured relationship traversal with vector embeddings for unstructured narrative retrieval, MediGRAF enables natural language querying of the complete patient journey. Using 10 patients from the MIMIC-IV dataset (generating 5,973 nodes and 5,963 relationships), we generated enough nodes and data for patient level question answering (QA), and we evaluated this architecture across varying query complexities. The system demonstrated 100\% recall for factual queries which means all relevant information was retrieved and in the output, while complex inference tasks achieved a mean expert quality score of 4.25/5 with zero safety violations. These results demonstrate that hybrid graph-grounding significantly advances clinical information retrieval, offering a safer, more comprehensive alternative to standard LLM deployments.

</details>


### [279] [G-MemLLM: Gated Latent Memory Augmentation for Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2602.00015)
*Xun Xu*

Main category: cs.CL

TL;DR: 本文提出了一种名为G-MemLLM的新型存储增强架构，通过可控更新的方式强化大语言模型在长期多跳推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型受限于有限的上下文窗口及长期事实一致性难以维持，尤其在多跳推理时易出现“上下文衰减”及信息稀释问题，影响整体推理准确性。

Method: G-MemLLM将一个冻结的大语言模型（LLM）骨干与可训练的潜在记忆库（Latent Memory Bank）结合，引入类似GRU结构的门控更新机制，从而实现对记忆槽的选择性更新、保留或覆盖，避免知识渐隐问题。模型在不同规模（如GPT-2和Llama 3.1-8B）上于HotpotQA和Zero-Shot Relation Extraction（ZsRE）等基准数据集上进行评测。

Result: G-MemLLM在多跳推理和关系抽取任务上显著优于传统方法。例如，Llama 3.1-8B模型在ZsRE数据集上的准确率提升13.3%，GPT-2在HotpotQA上的Answer F1提升8.56点，Llama 3.1-8B在Supporting Fact F1上提升6.89点。

Conclusion: G-MemLLM能够稳定提升大语言模型在长期推理和复杂信息维持方面的能力，并在多个主流基准上获得了显著改进，显示了存储增强和门控机制在实际NLP任务中的应用潜力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, yet they remain constrained by the finite capacity of their context windows and the inherent difficulty of maintaining long-term factual consistency during multi-hop reasoning. While existing methods utilize context compression or recurrent tokens, they often suffer from ``context rot'' or the dilution of information over long horizons. In this paper, we propose \textbf{G-MemLLM}, a memory-augmented architecture that integrates a frozen LLM backbone with a trainable \textbf{Latent Memory Bank}. Our key innovation is a GRU-style gated update logic that allows the model to selectively update, preserve, or overwrite latent memory slots, preventing the vanishing gradients of knowledge common in recurrent systems. We evaluate G-MemLLM across scales, from GPT-2 (124M) to Llama 3.1 (8B), on the HotpotQA and Zero-Shot Relation Extraction (ZsRE) benchmarks. Our results demonstrate that G-MemLLM significantly enhances multi-hop reasoning and relational precision, achieving a 13.3\% accuracy boost on ZsRE for Llama 3.1-8B, and it also yields improvements across model scales, boosting Answer F1 by 8.56 points for GPT-2 and increasing Supporting Fact F1 by 6.89 points for Llama 3.1-8B on HotpotQA.

</details>


### [280] [PTCBENCH: Benchmarking Contextual Stability of Personality Traits in LLM Systems](https://arxiv.org/abs/2602.00016)
*Jiongchi Yu,Yuhan Ma,Xiaoyu Zhang,Junjie Wang,Qiang Hu,Chao Shen,Xiaofei Xie*

Main category: cs.CL

TL;DR: 本文提出了PTCBENCH基准，用于评测大语言模型（LLM）在不同情境下的人格一致性，发现某些外部情境会显著改变LLM的人格特质和推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型被广泛应用于具有人格的情感代理与AI系统，然而，现有研究忽略了人格特质动态且依赖情境的心理学共识。这种忽视影响了用户对AI真实可靠人格的信任感。

Method: 作者提出PTCBENCH基准，通过设定12种不同的外部条件（包括地点和人生事件等情境），系统性地测试并量化LLM的人格一致性。评估方法采用了NEO五因素人格量表，并收集分析了39,240条人格数据记录。

Result: 实验结果表明，某些外部情境（如‘失业’）能引发LLM显著的人格变化，甚至影响其推理能力。

Conclusion: PTCBENCH为评价AI在人格维度下的一致性提供了可扩展的框架，有助于开发更加健壮和心理学一致的AI系统，并为AI人格评测和优化提供了重要参考。

Abstract: With the increasing deployment of large language models (LLMs) in affective agents and AI systems, maintaining a consistent and authentic LLM personality becomes critical for user trust and engagement. However, existing work overlooks a fundamental psychological consensus that personality traits are dynamic and context-dependent. To bridge this gap, we introduce PTCBENCH, a systematic benchmark designed to quantify the consistency of LLM personalities under controlled situational contexts. PTCBENCH subjects models to 12 distinct external conditions spanning diverse location contexts and life events, and rigorously assesses the personality using the NEO Five-Factor Inventory. Our study on 39,240 personality trait records reveals that certain external scenarios (e.g., "Unemployment") can trigger significant personality changes of LLMs, and even alter their reasoning capabilities. Overall, PTCBENCH establishes an extensible framework for evaluating personality consistency in realistic, evolving environments, offering actionable insights for developing robust and psychologically aligned AI systems.

</details>


### [281] [SafeTalkCoach: Diversity-Driven Multi-Agent Simulation for Parent-Teen Health Conversations](https://arxiv.org/abs/2602.00017)
*Benyamin Tabarsi,Wenbo Li,Tahreem Yasir,Aryan Santhosh Kumar,Laura Widman,Dongkuan Xu,Tiffany Barnes*

Main category: cs.CL

TL;DR: 本论文提出了一种多代理对话生成框架SafeTalkCoach，用于模拟性健康领域中父母与子女的对话，并生成相关数据集，以支持AI和健康沟通研究。


<details>
  <summary>Details</summary>
Motivation: 有效的亲子性健康沟通非常重要，但现实中相关对话数据因私密和敏感性难以收集。目前大模型（LLMs）虽然已广泛应用于对话生成，但在真实性、多样性和沟通质量方面存在不足。

Method: 作者提出SafeTalkCoach，一个以多代理为基础并强调多样性的对话生成框架。该框架结合了众包与合成场景、成熟性健康指南、基于证据的人物设定、自适应控制模块和分层多样化机制，旨在更好地模拟真实亲子性健康对话。

Result: 评估结果表明，SafeTalkCoach能生成多样且真实的对话，并兼顾沟通质量和可控性。

Conclusion: SafeTalkCoach框架及其数据集可为AI对话和健康沟通领域的研究和实践应用提供有力支持。

Abstract: The importance of effective parent-child communication about sexual health is widely acknowledged, but real-world data on these conversations is scarce and challenging to collect, due to their private and sensitive nature. Although LLMs have been widely adopted in dialogue generation, they may deviate from best practices and frequently lack realism and diversity. We introduce SafeTalkCoach, a diversity-driven multi-agent dialogue generation framework that simulates parent-child conversations about sexual health, and present an accompanying dataset. SafeTalkCoach integrates crowd-sourced and synthesized scenarios, established sexual health guidelines, evidence-based personas, adaptive control modules, and hierarchical diversification. Through evaluations, we demonstrate that SafeTalkCoach generates diverse conversations while maintaining realism, communication quality, and controllability in practice. Our goal is that the SafeTalkCoach framework and the dataset support both AI research and health communications practices.

</details>


### [282] [Construct, Align, and Reason: Large Ontology Models for Enterprise Knowledge Management](https://arxiv.org/abs/2602.00029)
*Yao Zhang,Hongyin Zhu*

Main category: cs.CL

TL;DR: 提出了一种大规模本体模型(LOM)，通过统一的构建-对齐-推理框架，有效整合多源异构数据并提升本体推理性能，在复杂企业知识管理和语义推理任务中取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 面对企业级知识管理中多源异构数据难整合、传统知识图谱推理能力弱、复杂问题语义理解不足等问题，亟需一种更智能、统一的方法实现高效的知识融合与推理。

Method: （1）构建结构层次丰富的企业本体（整合结构化数据库与非结构化文本）；（2）提出统一三阶段训练流程，包括本体指令微调（提升结构理解）、文本-本体锚定（增强语义编码）、多任务指令训练（通过课程学习提升推理与生成能力）；（3）开发覆盖多种推理任务的训练与评测集。

Result: 所提4B参数的LOM模型在多种本体推理任务数据集上获得了89.47%的准确率，并在复杂图推理任务上优于DeepSeek-V3.2，验证了结构与语义融合方案的有效性。

Conclusion: LOM模型实现了多源企业知识的深度融合和高效推理，提升了语义理解和复杂问题回答能力，为企业知识管理和智能问答提供了有力工具。

Abstract: Enterprise-scale knowledge management faces significant challenges in integrating multi-source heterogeneous data and enabling effective semantic reasoning. Traditional knowledge graphs often struggle with implicit relationship discovery and lack sufficient semantic understanding for complex question answering. To address these limitations, we introduce a unified construct--align--reason framework, the large ontology model (LOM). We first build a dual-layer enterprise ontology from structured databases and unstructured text, subsequently fusing these sources into a comprehensive enterprise ontology. To enable instruction-aligned reasoning, we propose a unified three-stage training pipeline: ontology instruction fine-tuning to improve structural understanding; text-ontology grounding to strengthen node semantic encoding; and multi-task instruction tuning on ontology-language pairs with curriculum learning to enhance semantic reasoning and generation. We also construct comprehensive training and evaluation datasets covering diverse ontology reasoning tasks. On this benchmark, our 4B-parameter LOM achieves 89.47% accuracy and outperforms DeepSeek-V3.2 on complex graph reasoning, indicating effective fusion of ontology structure and language.

</details>


### [283] [Reversible Diffusion Decoding for Diffusion Language Models](https://arxiv.org/abs/2602.00150)
*Xinyun Wang,Min Zhang,Sen Cui,Zhikang Chen,Bo Jiang,Kun Kuang,Mingbao Lin*

Main category: cs.CL

TL;DR: 本论文提出了一种可逆扩散解码（RDD）方法，用于提高扩散语言模型在并行生成时的鲁棒性和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散语言模型在采用分块并行生成令牌时，一旦做出不可逆决定，如果处于次优上下文，会无法继续优化生成（称为停滞），影响生成质量。

Method: 提出RDD解码框架：检测停滞状态（即反向过程无法进行），允许在无需重复计算的情况下高效回溯到前面的生成块；通过置信度引导的再掩码，对不确定的令牌进行有选择的重初始化，同时保留可靠上下文。该方法引入可逆机制以纠正早期的错误。

Result: 实验表明，RDD方法在大部分情况下能够以极小的计算开销提升生成的鲁棒性和质量，相较于传统基线方法表现更优。

Conclusion: RDD通过引入可逆性与动态纠错机制，在保持扩散并行生成效率的前提下，有效缓解了停滞问题，为高质量和鲁棒的文本生成提供了新方案。

Abstract: Diffusion language models enable parallel token generation through block-wise decoding, but their irreversible commitments can lead to stagnation, where the reverse diffusion process fails to make further progress under a suboptimal context.We propose Reversible Diffusion Decoding (RDD), a decoding framework that introduces reversibility into block-wise diffusion generation. RDD detects stagnation as a state-dependent failure of the reverse process and enables efficient backtracking to earlier blocks without recomputation via cached model states. To avoid repeated failure trajectories, RDD applies confidence-guided re-masking to selectively reinitialize uncertain tokens while preserving reliable context.This reversible formulation allows decoding to recover from early commitment errors while maintaining the parallel efficiency of diffusion-based generation. Experiments show that RDD improves generation robustness and quality over baselines with minimal computational overhead.

</details>


### [284] [DIVERGE: Diversity-Enhanced RAG for Open-Ended Information Seeking](https://arxiv.org/abs/2602.00238)
*Tianyi Hu,Niket Tandon,Akhil Arora*

Main category: cs.CL

TL;DR: 本文提出DIVERGE框架，提升RAG系统在多个合理答案场景下的多样性，兼顾多样性与答案质量。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成（RAG）系统通常假设每个查询只有一个正确答案，忽视了实际应用中需要多个答案的场景，导致生成的内容缺乏多样性和包容性。

Method: 提出了一种可插拔的agentic RAG框架DIVERGE，引入反思引导的生成和记忆增强的迭代优化机制，从而鼓励多样化观点的生成，同时保持答案质量。还设计了新的多样性-质量权衡评估指标，并用它们对系统进行评测。

Result: 实验证明DIVERGE在Infinity-Chat真实数据集上，分别在多样性和质量的权衡上超越了已有先进方法和基线，并且新的评估指标与人类评判高度相关。

Conclusion: 当前LLM驱动的RAG系统在开放信息检索任务上固有多样性不足，通过显式建模多样性可以显著优化其表现。

Abstract: Existing retrieval-augmented generation (RAG) systems are primarily designed under the assumption that each query has a single correct answer. This overlooks common information-seeking scenarios with multiple plausible answers, where diversity is essential to avoid collapsing to a single dominant response, thereby constraining creativity and compromising fair and inclusive information access. Our analysis reveals a commonly overlooked limitation of standard RAG systems: they underutilize retrieved context diversity, such that increasing retrieval diversity alone does not yield diverse generations. To address this limitation, we propose DIVERGE, a plug-and-play agentic RAG framework with novel reflection-guided generation and memory-augmented iterative refinement, which promotes diverse viewpoints while preserving answer quality. We introduce novel metrics tailored to evaluating the diversity-quality trade-off in open-ended questions, and show that they correlate well with human judgments. We demonstrate that DIVERGE achieves the best diversity-quality trade-off compared to competitive baselines and previous state-of-the-art methods on the real-world Infinity-Chat dataset, substantially improving diversity while maintaining quality. More broadly, our results reveal a systematic limitation of current LLM-based systems for open-ended information-seeking and show that explicitly modeling diversity can mitigate it. Our code is available at: https://github.com/au-clan/Diverge

</details>


### [285] [Benchmarking Uncertainty Calibration in Large Language Model Long-Form Question Answering](https://arxiv.org/abs/2602.00279)
*Philip Müller,Nicholas Popovič,Michael Färber,Peter Steinbach*

Main category: cs.CL

TL;DR: 本文提出了第一个面向需要推理的科学问答的大规模基准，用于系统评估大语言模型（LLMs）不确定性量化（UQ）方法的校准效果，发现现有方法存在若干关键问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在科学问答中被广泛应用，但生成答案的不确定性表征目前缺乏可靠评价手段，已有方法在科学问答这一对事实检索和推理高度依赖的领域验证不足，因此亟需一个系统性、标准化的UQ评价框架。

Method: 作者构建了一个可扩展的开源框架，涵盖7个科学问答数据集（多项选择及算术类问题），分析多达20种不同类型的大模型（基础、指令微调、推理微调）。评估覆盖了代表主流方法的685,000条长回答，分别在token级和序列级探讨校准效果。

Result: 研究发现，指令微调会造成概率质量极端化，导致token级的置信度失去有效性。推理微调模型也有类似现象，但推理过程有缓解作用。序列级上，文本化方法（如输出信心表达语）表现出系统性偏差，且与正确性低相关；而答案一致性（答案在多采样间重复出现的频率）校准最好。

Conclusion: 当前主流UQ方法和评测实践在科学问答领域存在严重局限，单独依赖ECE度量具有误导性。为推动LLMs在科学任务中的可靠应用，需发展更健全的不确定性量化和评价体系。

Abstract: Large Language Models (LLMs) are commonly used in Question Answering (QA) settings, increasingly in the natural sciences if not science at large. Reliable Uncertainty Quantification (UQ) is critical for the trustworthy uptake of generated answers. Existing UQ approaches remain weakly validated in scientific QA, a domain relying on fact-retrieval and reasoning capabilities. We introduce the first large-scale benchmark for evaluating UQ metrics in reasoning-demanding QA studying calibration of UQ methods, providing an extensible open-source framework to reproducibly assess calibration. Our study spans up to 20 large language models of base, instruction-tuned and reasoning variants. Our analysis covers seven scientific QA datasets, including both multiple-choice and arithmetic question answering tasks, using prompting to emulate an open question answering setting. We evaluate and compare methods representative of prominent approaches on a total of 685,000 long-form responses, spanning different reasoning complexities representative of domain-specific tasks. At the token level, we find that instruction tuning induces strong probability mass polarization, reducing the reliability of token-level confidences as estimates of uncertainty. Models further fine-tuned for reasoning are exposed to the same effect, but the reasoning process appears to mitigate it depending on the provider. At the sequence level, we show that verbalized approaches are systematically biased and poorly correlated with correctness, while answer frequency (consistency across samples) yields the most reliable calibration. In the wake of our analysis, we study and report the misleading effect of relying exclusively on ECE as a sole measure for judging performance of UQ methods on benchmark datasets. Our findings expose critical limitations of current UQ methods for LLMs and standard practices in benchmarking thereof.

</details>


### [286] [Faithful-Patchscopes: Understanding and Mitigating Model Bias in Hidden Representations Explanation of Large Language Models](https://arxiv.org/abs/2602.00300)
*Xilin Gong,Shu Yang,Zehua Cao,Lynne Billard,Di Wang*

Main category: cs.CL

TL;DR: 本文发现现有的大语言模型（LLM）在用Patchscopes解码隐藏表示并生成解释时，常常优先输出固有语言偏见，导致对真实上下文信息的解释不忠实。作者提出了一种名为BALOR的对抗偏见方法，大幅提高了解释的忠实度。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM通过Patchscopes展现了解释隐藏表示的能力，但现有方法在解释时容易被训练时的语言惯性和偏见主导，无法真实反映输入的变化。这种“不忠实”会极大影响对模型内部机制的解释与信任度。因此，作者希望系统性分析这一现象并探索纠正手段。

Method: 1. 先构建了带有刻意偏见测试案例的数据集，用以检测Patchscopes的忠实度。
2. 发现被语言习惯主导时，Patchscopes对偏置信息的反应明显降低。
3. 提出Bias Alignment through Logit Recalibration（BALOR）方法：先分别获取标准与带干预（patched）情况下的LLM输出logit，通过两者对比并重调，从而抑制模型本身的偏见，增强对真实上下文的表达。

Result: 实验显示，Patchscopes在偏见测试中的平均忠实度下降18.84%；BALOR方法在多种主流LLM上的表现，较现有方法可实现最高33%的相对提升，有效提升了解释的可靠性。

Conclusion: LLM在解释内部状态时存在系统性偏向原有语言常识而非编码上下文，传统Patchscopes因此存在显著忠实性问题。BALOR可有效抑制这一偏差，提高解释的忠实与可用性。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities for hidden representation interpretation through Patchscopes, a framework that uses LLMs themselves to generate human-readable explanations by decoding from internal hidden representations. However, our work shows that LLMs tend to rely on inherent linguistic patterns, which can override contextual information encoded in the hidden representations during decoding. For example, even when a hidden representation encodes the contextual attribute "purple" for "broccoli", LLMs still generate "green" in their explanations, reflecting a strong prior association. This behavior reveals a systematic unfaithfulness in Patchscopes. To systematically study this issue, we first designed a dataset to evaluate the faithfulness of Patchscopes under biased cases, and our results show that there is an 18.84\% faithfulness decrease on average. We then propose Bias Alignment through Logit Recalibration (BALOR), which treats the output logits from an unpatched prompt as capturing model bias and contrasts them with logits obtained under patched contextual information. By recalibrating the logit distribution through this contrast, BALOR suppresses model bias and amplifies contextual information during generation. Experiments across multiple LLMs demonstrate that BALOR consistently outperforms existing baselines, achieving up to 33\% relative performance improvement.

</details>


### [287] [MiNER: A Two-Stage Pipeline for Metadata Extraction from Municipal Meeting Minutes](https://arxiv.org/abs/2602.00316)
*Rodrigo Batista,Luís Filipe Cunha,Purificação Silvano,Nuno Guimarães,Alípio Jorge,Evelin Amorim,Ricardo Campos*

Main category: cs.CL

TL;DR: 本文提出了一种两阶段管道方法，从市政会议纪要中高效提取元数据，并建立了领域首个基准，对多种主流模型的准确率、推理成本及碳排放进行了系统评估。


<details>
  <summary>Details</summary>
Motivation: 市政会议纪要结构多样、格式异质，元数据（如日期、地点等）通常未标准化，难以自动提取，现有NER模型不适用于此类特定领域的元数据抽取。

Method: 提出了两阶段抽取流程：首先用问答模型定位包含元数据的片段，其次采用基于Transformer的模型（BERTimbau、XLM-RoBERTa、有无CRF层）进行细粒度实体抽取，并利用去词汇化增强效果。还评估了开源LLM（Phi）与闭源LLM（Gemini）的表现、推理成本和碳足迹。

Result: 所提方法在域内市政纪要的元数据抽取上优于通用大模型，但跨市评估表现会降低，反映了文本风格和语言复杂性的挑战。建立了首个此任务的标准数据集和评测基准。

Conclusion: 本研究为市政会议纪要元数据抽取提供了有效方案和重要基准，有助于推动该领域后续研究与应用。

Abstract: Municipal meeting minutes are official documents of local governance, exhibiting heterogeneous formats and writing styles. Effective information retrieval (IR) requires identifying metadata such as meeting number, date, location, participants, and start/end times, elements that are rarely standardized or easy to extract automatically. Existing named entity recognition (NER) models are ill-suited to this task, as they are not adapted to such domain-specific categories. In this paper, we propose a two-stage pipeline for metadata extraction from municipal minutes. First, a question answering (QA) model identifies the opening and closing text segments containing metadata. Transformer-based models (BERTimbau and XLM-RoBERTa with and without a CRF layer) are then applied for fine-grained entity extraction and enhanced through deslexicalization. To evaluate our proposed pipeline, we benchmark both open-weight (Phi) and closed-weight (Gemini) LLMs, assessing predictive performance, inference cost, and carbon footprint. Our results demonstrate strong in-domain performance, better than larger general-purpose LLMs. However, cross-municipality evaluation reveals reduced generalization reflecting the variability and linguistic complexity of municipal records. This work establishes the first benchmark for metadata extraction from municipal meeting minutes, providing a solid foundation for future research in this domain.

</details>


### [288] [Detecting AI-Generated Content in Academic Peer Reviews](https://arxiv.org/abs/2602.00319)
*Siyuan Shen,Kai Wang*

Main category: cs.CL

TL;DR: 本研究发现，从2022年起AI生成内容在同行评审中的占比迅速增加，到2025年ICLR和Nature Communications（NC）的评审中AI生成内容分别占比约20%和12%。


<details>
  <summary>Details</summary>
Motivation: 近年来大型语言模型的广泛应用引发了对学术同行评审中AI角色的关注，作者想要了解AI生成内容在评审中的出现趋势及其发展速度。

Method: 作者利用训练于历史评审的AI检测模型，对ICLR和NC两个会议/期刊在不同周期的评审文本进行检测，追踪AI内容的比例变化。

Result: 2022年前AI内容检测极少，2022年后明显上升，至2025年ICLR约20%、NC约12%的评审被检测为AI生成。2024年Q3至Q4期间，NC中AI生成评审的增长尤为显著。

Conclusion: AI生成内容在学术同行评审中占比正快速上升，需要进一步研究其对学术评价体系带来的影响。

Abstract: The growing availability of large language models (LLMs) has raised questions about their role in academic peer review. This study examines the temporal emergence of AI-generated content in peer reviews by applying a detection model trained on historical reviews to later review cycles at International Conference on Learning Representations (ICLR) and Nature Communications (NC). We observe minimal detection of AI-generated content before 2022, followed by a substantial increase through 2025, with approximately 20% of ICLR reviews and 12% of Nature Communications reviews classified as AI-generated in 2025. The most pronounced growth of AI-generated reviews in NC occurs between the third and fourth quarter of 2024. Together, these findings provide suggestive evidence of a rapidly increasing presence of AI-assisted content in peer review and highlight the need for further study of its implications for scholarly evaluation.

</details>


### [289] [DETOUR: An Interactive Benchmark for Dual-Agent Search and Reasoning](https://arxiv.org/abs/2602.00352)
*Li Siyan,Darshan Deshpande,Anand Kannappan,Rebecca Qian*

Main category: cs.CL

TL;DR: 本文提出了一个更加真实模拟‘似曾相识’信息检索过程的双智能体测评基准DETOUR，并发现主流模型在该任务上表现欠佳。


<details>
  <summary>Details</summary>
Motivation: 现有对话智能体‘似曾相识’式多轮检索评测多数只考虑单轮场景，这与真实交流场景不符，因此亟需更贴合实际的多轮评测方法。

Method: 作者提出了DETOUR双智能体测评基准，包含1011个提示语。核心设计为让待评估的‘主智能体’通过与固定记忆智能体的多轮问答，推断出目标信息。支持文本、图像、音频与视频多模态。

Result: 现有最前沿的模型在DETOUR全文本、图像、音频及视频模态上的准确率仅为36%。

Conclusion: 主流模型在多轮、不充分提示下的信息检索能力仍较弱，未来需要着重提升在不完全信息下的推理与检索能力。

Abstract: When recalling information in conversation, people often arrive at the recollection after multiple turns. However, existing benchmarks for evaluating agent capabilities in such tip-of-the-tongue search processes are restricted to single-turn settings. To more realistically simulate tip-of-the-tongue search, we introduce Dual-agent based Evaluation Through Obscure Under-specified Retrieval (DETOUR), a dual-agent evaluation benchmark containing 1,011 prompts. The benchmark design involves a Primary Agent, which is the subject of evaluation, tasked with identifying the recollected entity through querying a Memory Agent that is held consistent across evaluations. Our results indicate that current state-of-the-art models still struggle with our benchmark, only achieving 36% accuracy when evaluated on all modalities (text, image, audio, and video), highlighting the importance of enhancing capabilities in underspecified scenarios.

</details>


### [290] [DecompressionLM: Deterministic, Diagnostic, and Zero-Shot Concept Graph Extraction from Language Models](https://arxiv.org/abs/2602.00377)
*Zhaochen Hong,Jiaxuan You*

Main category: cs.CL

TL;DR: DecompressionLM是一种新颖的无状态框架，可以在无需预设查询的情况下零样本提取语言模型中的概念知识图谱，同时克服了传统探查方法的主要局限。它还展示了不同量化策略对模型知识覆盖的显著影响。


<details>
  <summary>Details</summary>
Motivation: 现有知识探查方法依赖预定义查询，限制了只能提取已知概念，且常见的解码探查方法存在跨序列耦合、高频优先与扩展性差等问题。因此需要一种能够无监督、无需查询、可扩展地提取模型内部知识结构的新方法。

Method: 提出DecompressionLM框架，利用Van der Corput低差异序列和算术解码，实现无状态、可并行（且确定性）地从模型中生成包含概念关系的图谱，无需跨样本共享状态。比较不同量化（AWQ-4bit与GPTQ-Int4）方法下的知识覆盖率，并通过语料验证模型真实与幻觉知识的差距。

Result: 实验表明，AWQ-4bit量化能提升概念覆盖率30-170%；而GPTQ-Int4量化则导致覆盖率下降71-86%。这种差异无法单纯通过困惑度解释。此外，知识覆盖广度评估揭示高分与低分模型存在17分的“幻觉”差距。

Conclusion: DecompressionLM不仅提供了一种全新评估压缩语言模型知识广度与事实基础的方法，还补充了当前以困惑度为主的模型评估，推动其在实际部署前的可用性验证。

Abstract: Existing knowledge probing methods rely on pre-defined queries, limiting extraction to known concepts. We introduce DecompressionLM, a stateless framework for zero-shot concept graph extraction that discovers what language models encode without pre-specified queries or shared cross-sequence state. Our method targets three limitations of common decoding-based probing approaches: cross-sequence coupling that concentrates probability mass on high-frequency prefixes, competitive decoding effects that suppress long-tail concepts, and scalability constraints arising from sequential exploration. Using Van der Corput low-discrepancy sequences with arithmetic decoding, DecompressionLM enables deterministic, embarrassingly parallel generation without shared state across sequences. Across two model families and five quantization variants, we find that activation-aware quantization (AWQ-4bit) expands concept coverage by 30-170%, while uniform quantization (GPTQ-Int4) induces 71-86% coverage collapse -- divergent behaviors not reliably reflected by explanation-level perplexity. Corpus-based verification further reveals a 17-point hallucination gap between top- and bottom-ranked MMLU-Pro Law models. DecompressionLM establishes concept coverage as a complementary evaluation dimension for assessing knowledge breadth and factual grounding in compressed models useful for their deployment.

</details>


### [291] [Clause-Internal or Clause-External? Testing Turkish Reflexive Binding in Adapted versus Chain of Thought Large Language Models](https://arxiv.org/abs/2602.00380)
*Sercan Karakaş*

Main category: cs.CL

TL;DR: 本文研究了先进的大语言模型是否能理解土耳其语反身代词的指代关系。通过对两种不同模型在本地与远距离指代表现上的对比实验证明，不同模型表现出显著差异。


<details>
  <summary>Details</summary>
Motivation: 土耳其语反身代词的指代关系（绑定关系）较为复杂，然而目前对大语言模型在此类结构上的理解能力还不明确。作者希望探索和量化当前主流模型在处理土耳其语反身代词领域的表现及潜在差异。

Method: 作者设计了100个均衡句子集，句子中反身代词kendi和kendisi在本地与非本地指代关系中进行对抗。评测对象为OpenAI链式推理模型和土耳其语细调的Trendyol-LLM，通过句子困惑度和强制选择任务评估模型对先行词的倾向。

Result: Trendyol-LLM在约70%的试验中支持本地指代，表现出很强的本地性偏好；而o1 Mini模型对本地与远距离指代选择基本均匀，两种模型的绑定行为有显著对比。

Conclusion: 主流大模型在土耳其语反身代词绑定关系的处理方式存在显著差异，其中针对本地数据量大、细致微调的模型更倾向于本地指代。这提醒我们语言模型对复杂句法结构的掌握受到训练语料和微调策略的影响。

Abstract: This study evaluates whether state-of-the-art large language models capture the binding relations of Turkish reflexive pronouns. We construct a balanced set of 100 sentences that pit local against non-local antecedents for the reflexives kendi and kendisi, and test two contrasting systems: an OpenAI chain-of-thought model designed for multi-step reasoning and Trendyol-LLM-7B-base-v0.1, a LLaMA-2-derived model extensively fine-tuned on Turkish data. Antecedent choice is assessed using a combined sentence-level perplexity and forced-choice paradigm. Trendyol-LLM favours local bindings in approximately 70% of trials, exhibiting a strong locality bias, whereas o1 Mini distributes its choices almost evenly between local and long-distance readings, revealing a marked contrast in binding behaviour across the two systems.

</details>


### [292] [Segment-Level Attribution for Selective Learning of Long Reasoning Traces](https://arxiv.org/abs/2602.00425)
*Siyuan Wang,Yanchen Liu,Xiang Ren*

Main category: cs.CL

TL;DR: 本论文提出了一种通过归因方法识别推理长链中有效信息段落，并对这些重要段落进行选择性训练，以提高大型推理模型（LRMs）的准确性和输出效率。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型生成的长推理链包含大量冗余、不重要的内容，这些冗余内容经有监督微调后被模型学习并模仿，导致性能下降。因此，亟需方法排除无用内容，提升模型实际推理能力。

Method: 作者引入集成梯度归因（Integrated Gradient Attribution）量化每个token对最终答案的影响，并将其聚合为两个片段级指标：（1）归因强度（attribution strength）表示片段的重要性；（2）方向一致性（direction consistency）衡量归因方向的统一性。基于这两个指标，提出了一种片段级选择性学习框架——对具有高归因强度但一致性中等的片段（代表更深层推理）进行有选择的SFT训练，其余则遮蔽损失。

Result: 在多个模型和数据集上的实验结果显示，所提出的方法能提升推理准确率及输出效率。

Conclusion: 基于归因指标的片段级选择性训练能够有效挖掘和放大推理过程中的有效信息，减少冗余内容，进而提升大型推理模型在长推理链场景下的表现。

Abstract: Large Reasoning Models (LRMs) achieve strong reasoning performance by generating long chains of thought (CoTs), yet only a small fraction of these traces meaningfully contributes to answer prediction, while the majority contains repetitive or truncated content. Such output redundancy is further propagated after supervised finetuning (SFT), as models learn to imitate verbose but uninformative patterns, which can degrade performance. To this end, we incorporate integrated gradient attribution to quantify each token's influence on final answers and aggregate them into two segment-level metrics: (1) \textit{attribution strength} measures the overall attribution magnitude; and (2) \textit{direction consistency} captures whether tokens' attributions within a segment are uniformly positive or negative (high consistency), or a mixture of both (moderate consistency). Based on these two metrics, we propose a segment-level selective learning framework to identify important segments with high attribution strength but moderate consistency that indicate reflective rather than shallow reasoning. The framework then applies selective SFT on these important segments while masking loss for unimportant ones. Experiments across multiple models and datasets show that our approach improves accuracy and output efficiency, enabling more effective learning from long reasoning traces~\footnote{Code and data are available at https://github.com/SiyuanWangw/SegmentSelectiveSFT}.

</details>


### [293] [When Agents "Misremember" Collectively: Exploring the Mandela Effect in LLM-based Multi-Agent Systems](https://arxiv.org/abs/2602.00428)
*Naen Xu,Hengyu An,Shuo Shi,Jinghuai Zhang,Chunyi Zhou,Changjiang Li,Tianyu Du,Zhihui Fu,Jun Wang,Shouling Ji*

Main category: cs.CL

TL;DR: 本论文系统研究了LLM赋能的多智能体系统中，被群体性认知偏差（如曼德拉效应）影响的现象，并提出了相应的评测基准与缓解方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）的进步，多智能体系统能力增强，但其在协作中易受集体认知偏差影响，尤其是“曼德拉效应”，导致集体性记忆错误和误信息传播。本问题未被充分研究，既影响系统可靠性，也带来伦理风险。

Method: 论文提出了MANBENCH基准，针对四类易受曼德拉效应影响的任务和五种不同互动协议，系统评测LLM驱动的多智能体的集体记忆偏差；并设计了诸如认知定位、信息来源检验等提示策略，以及模型对齐等防御方案。

Result: 实验在MANBENCH平台上对不同LLMs能力评测，量化了曼德拉效应的发生和成因；通过提示层和模型层防御措施，有效减缓这一效应，平均降低74.40%。

Conclusion: 论文为理解和减少LLM多智能体系统中的记忆偏差提供了新的见解与实用防御方法，对提升系统的稳健性和伦理合规性具有重要意义。

Abstract: Recent advancements in large language models (LLMs) have significantly enhanced the capabilities of collaborative multi-agent systems, enabling them to address complex challenges. However, within these multi-agent systems, the susceptibility of agents to collective cognitive biases remains an underexplored issue. A compelling example is the Mandela effect, a phenomenon where groups collectively misremember past events as a result of false details reinforced through social influence and internalized misinformation. This vulnerability limits our understanding of memory bias in multi-agent systems and raises ethical concerns about the potential spread of misinformation. In this paper, we conduct a comprehensive study on the Mandela effect in LLM-based multi-agent systems, focusing on its existence, causing factors, and mitigation strategies. We propose MANBENCH, a novel benchmark designed to evaluate agent behaviors across four common task types that are susceptible to the Mandela effect, using five interaction protocols that vary in agent roles and memory timescales. We evaluate agents powered by several LLMs on MANBENCH to quantify the Mandela effect and analyze how different factors affect it. Moreover, we propose strategies to mitigate this effect, including prompt-level defenses (e.g., cognitive anchoring and source scrutiny) and model-level alignment-based defense, achieving an average 74.40% reduction in the Mandela effect compared to the baseline. Our findings provide valuable insights for developing more resilient and ethically aligned collaborative multi-agent systems.

</details>


### [294] [What Matters to an LLM? Behavioral and Computational Evidences from Summarization](https://arxiv.org/abs/2602.00459)
*Yongxin Zhou,Changshun Wu,Philippe Mulhem,Didier Schwab,Maxime Peyrard*

Main category: cs.CL

TL;DR: 本文探究了大型语言模型（LLM）在摘要任务中选择信息的内在标准，通过行为和计算分析揭示了模型对重要性判别的规律和机制。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在文本摘要上表现优异，但其如何判定哪些信息重要仍是不透明的，理解这一过程有助于提升模型解释性和可控性。

Method: 作者通过生成多种长度受控的摘要，与传统方法对比，从模型选择哪些信息单位出现的频率归纳出“重要性分布”；此外，通过分析模型内部注意力机制，特别是特定注意力头以及中后层的表征，探讨其与外部重要性分布的对应关系。

Result: 结果显示，LLM在信息筛选上表现出一致且独特的“重要性模式”，这一点明显区别于旧有基线，且不同LLM家族的选择风格比模型规模更为类似。另外，某些注意力头和中后层的表现，与实证重要性分布高度一致。

Conclusion: LLM在摘要时有稳定的内在信息优先级机制，可借助内层注意力等表征手段加以解释。这为理解、解释及未来控制模型的信息筛选提供了新途径。

Abstract: Large Language Models (LLMs) are now state-of-the-art at summarization, yet the internal notion of importance that drives their information selections remains hidden. We propose to investigate this by combining behavioral and computational analyses. Behaviorally, we generate a series of length-controlled summaries for each document and derive empirical importance distributions based on how often each information unit is selected. These reveal that LLMs converge on consistent importance patterns, sharply different from pre-LLM baselines, and that LLMs cluster more by family than by size. Computationally, we identify that certain attention heads align well with empirical importance distributions, and that middle-to-late layers are strongly predictive of importance. Together, these results provide initial insights into what LLMs prioritize in summarization and how this priority is internally represented, opening a path toward interpreting and ultimately controlling information selection in these models.

</details>


### [295] [Words that make SENSE: Sensorimotor Norms in Learned Lexical Token Representations](https://arxiv.org/abs/2602.00469)
*Abhinav Gupta,Toben H. Mintz,Jesse Thomason*

Main category: cs.CL

TL;DR: 该论文提出了一种名为SENSE的模型，将词语的词嵌入投射到感觉运动规范空间，并通过行为实验验证其有效性，揭示了词汇与感觉运动体验之间的系统联系。


<details>
  <summary>Details</summary>
Motivation: 传统词嵌入主要基于词共现关系获得词义，但人类理解语言还依赖于感觉和动作经验。为让计算模型更接近人类语言理解方式，需要将感觉运动信息引入词表示。

Method: 作者提出了SENSE模型，即传感运动嵌入规范评分引擎，用学习到的投影将词嵌入转换为Lancaster感觉运动规范分数。另外，设计了行为实验，让281名参与者基于自创词语选择与特定感觉运动相关的词，结合SENSE打分进一步分析。

Result: 行为实验发现，参与者选择与SENSE评分在11种感觉运动模态中的6种存在显著相关性。对自创词汇的子词素分析还揭示了与内感受相关的系统音素-意义模式。

Conclusion: SENSE模型能有效预测词的感觉运动属性，且行为实验支持词汇与感觉运动体验之间的系统联系，为自动发现音素-意义对应关系提供了新方法。

Abstract: While word embeddings derive meaning from co-occurrence patterns, human language understanding is grounded in sensory and motor experience. We present $\text{SENSE}$ $(\textbf{S}\text{ensorimotor }$ $\textbf{E}\text{mbedding }$ $\textbf{N}\text{orm }$ $\textbf{S}\text{coring }$ $\textbf{E}\text{ngine})$, a learned projection model that predicts Lancaster sensorimotor norms from word lexical embeddings. We also conducted a behavioral study where 281 participants selected which among candidate nonce words evoked specific sensorimotor associations, finding statistically significant correlations between human selection rates and $\text{SENSE}$ ratings across 6 of the 11 modalities. Sublexical analysis of these nonce words selection rates revealed systematic phonosthemic patterns for the interoceptive norm, suggesting a path towards computationally proposing candidate phonosthemes from text data.

</details>


### [296] [Intention-Adaptive LLM Fine-Tuning for Text Revision Generation](https://arxiv.org/abs/2602.00477)
*Zhexiong Liu,Diane Litman*

Main category: cs.CL

TL;DR: 本论文提出了一种名为 Intention-Tuning 的方法，通过在LLM中有选择地微调部分层，以提升模型在“意图驱动”改写任务上的表现，在小规模语料下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 以往的大语言模型（LLMs）虽然在摘要生成、推理等基于上下文的任务表现出色，但在明确反映写作意图的改写生成等“意图驱动”任务中的应用探索较少。意图识别和按意图进行改写生成十分复杂，且多意图场景下现有方法表现不佳，而基于意图指令微调又需要大量昂贵的人工标注数据。

Method: 作者提出了 Intention-Tuning 框架：该方法在LLM的部分层动态选择进行针对意图的微调，使模型在学习意图方面更有效率，并能将这种能力迁移到实际的文本改写生成中。

Result: 实验显示，Intention-Tuning 在小规模改写语料库上效果优于多种主流的参数高效微调（PEFT）基线方法，证明其高效且有效。

Conclusion: Intention-Tuning 有效解决了多意图改写难题，降低了对大规模标注数据的需求，在资源有限背景下提升了LLM的“意图感知”文本生成能力。

Abstract: Large Language Models (LLMs) have achieved impressive capabilities in various context-based text generation tasks, such as summarization and reasoning; however, their applications in intention-based generation tasks remain underexplored. One such example is revision generation, which requires the generated text to explicitly reflect the writer's actual intentions. Identifying intentions and generating desirable revisions are challenging due to their complex and diverse nature. Although prior work has employed LLMs to generate revisions with few-shot learning, they struggle with handling entangled multi-intent scenarios. While fine-tuning LLMs using intention-based instructions appears promising, it demands large amounts of annotated data, which is expensive and scarce in the revision community. To address these challenges, we propose Intention-Tuning, an intention-adaptive layer-wise LLM fine-tuning framework that dynamically selects a subset of LLM layers to learn the intentions and subsequently transfers their representations to revision generation. Experimental results suggest that Intention-Tuning is effective and efficient on small revision corpora, outperforming several PEFT baselines.

</details>


### [297] [From Knowledge to Inference: Scaling Laws of Specialized Reasoning on GlobalHealthAtlas](https://arxiv.org/abs/2602.00491)
*Zhaokun Yan,Zhaohan Liu,Wuzheng Dong,Lijie Feng,Chengxiao Dai*

Main category: cs.CL

TL;DR: 本论文提出了GlobalHealthAtlas，这是一个涵盖15个公共卫生领域、17种语言、共28万多实例的大型多语言数据集，并结合大模型辅助流程，推动安全相关的公共卫生推理任务的发展。


<details>
  <summary>Details</summary>
Motivation: 当前公共卫生领域的推理要求依赖科学证据、专家共识和安全约束，但将其作为结构化机器学习问题进行系统研究尚不充分，现有监督信号与基准数据有限。

Method: 作者构建了一个多语言、涵盖多领域、分难度的大型数据集GlobalHealthAtlas，并提出了结合大模型的构建及质量控制流程，包括信息检索、去重、证据核查、标签验证等。此外，还开发了一个以大型模型高置信评判蒸馏而来的领域对齐评价器，从六个维度对输出进行评估。

Result: 成功推出了包含280,210个实例的数据集，并建立了标准化的评测流程和高一致性的标签及数据质量控制体系。用于评测的多维指标提升了对模型输出的评价准确性和丰富性。

Conclusion: 本工作填补了公共卫生推理数据集和评测体系的空白，促进了大模型在安全关键的公共卫生领域推理研究的发展，为日后相关监督学习与模型评估提供了重要基础。

Abstract: Public health reasoning requires population level inference grounded in scientific evidence, expert consensus, and safety constraints. However, it remains underexplored as a structured machine learning problem with limited supervised signals and benchmarks. We introduce \textbf{GlobalHealthAtlas}, a large scale multilingual dataset of 280,210 instances spanning 15 public health domains and 17 languages, stratified into three difficulty levels from health literacy to epidemiological and policy reasoning. Instances are derived from openly available public health sources and labeled by language, domain, and difficulty to support supervised learning and slice based evaluation. We further propose large language model (LLM) assisted construction and quality control pipeline with retrieval, duplication, evidence grounding checks, and label validation to improve consistency at scale. Finally, we present a domain aligned evaluator distilled from high confidence judgments of diverse LLMs to assess outputs along six dimensions: Accuracy, Reasoning, Completeness, Consensus Alignment, Terminology Norms, and Insightfulness. Together, these contributions enable reproducible training and evaluation of LLMs for safety critical public health reasoning beyond conventional QA benchmarks.

</details>


### [298] [Culturally-Grounded Governance for Multilingual Language Models: Rights, Data Boundaries, and Accountable AI Design](https://arxiv.org/abs/2602.00497)
*Hanjing Shi,Dominic DiFranzo*

Main category: cs.CL

TL;DR: 本文指出多语言大模型在实际应用中，本地文化与语言边缘群体面临系统性治理风险，现有治理框架难以有效应对。作者提出需要以文化为基础的治理方法来避免现有的不平等因技术扩大。


<details>
  <summary>Details</summary>
Motivation: 动机在于多语言大模型广泛部署时，常以英语为中心、忽视多元文化与语言环境，造成数据、责任、模型行为与本地预期不符，特别对低资源和边缘群体加剧不公。

Method: 方法包括综述多语言模型行为、数据不对称与社会技术危害的现有证据，利用跨文化视角，提出一个文化根基的治理框架，并识别三大挑战。重点是理论和概念框架梳理而非技术实现。

Result: 结果为归纳出三大治理挑战：1）训练数据和评估中的文化与语言不公平，2）全球部署与本地规范价值错位，3）对受害边缘群体的问责机制有限。提出框架和议程而非新技术。

Conclusion: 结论是多语言AI治理应从社会文化与权利出发，强调数据治理、透明度和参与式问责，防止AI在中立性的表象下加剧全球不平等。

Abstract: Multilingual large language models (MLLMs) are increasingly deployed across cultural, linguistic, and political contexts, yet existing governance frameworks largely assume English-centric data, homogeneous user populations, and abstract notions of fairness. This creates systematic risks for low-resource languages and culturally marginalized communities, where data practices, model behavior, and accountability mechanisms often fail to align with local norms, rights, and expectations. Drawing on cross-cultural perspectives in human-centered computing and AI governance, this paper synthesizes existing evidence on multilingual model behavior, data asymmetries, and sociotechnical harm, and articulates a culturally grounded governance framework for MLLMs. We identify three interrelated governance challenges: cultural and linguistic inequities in training data and evaluation practices, misalignment between global deployment and locally situated norms, values, and power structures, and limited accountability mechanisms for addressing harms experienced by marginalized language communities. Rather than proposing new technical benchmarks, we contribute a conceptual agenda that reframes multilingual AI governance as a sociocultural and rights based problem. We outline design and policy implications for data stewardship, transparency, and participatory accountability, and argue that culturally grounded governance is essential for ensuring that multilingual language models do not reproduce existing global inequalities under the guise of scale and neutrality.

</details>


### [299] [Reasoning by Commented Code for Table Question Answering](https://arxiv.org/abs/2602.00543)
*Seho Pyo,Jiheon Seok,Jaejin Lee*

Main category: cs.CL

TL;DR: 本文提出了一种带注释、逐步生成代码的方法，用于提升大语言模型在结构化数据表格问答(TableQA)任务上的准确性和可解释性，显著超过现有基线。


<details>
  <summary>Details</summary>
Motivation: 传统表格线性化方法会破坏数据的二维结构关系，导致大语言模型在表格问答任务中准确率低、推理不透明。现有多数方法的数值准确率与可解释性均不理想。

Method: 作者提出了一种分步、带注释的Python代码生成框架，将复杂的TableQA推理过程分解为多行可执行程序，并在每步中加入自然语言注释，实现显式推理；此外，还引入了答案筛选机制，将该框架与现有的强大TableQA模型整合。

Result: 在WikiTableQuestions数据集上，所提方法使用Qwen2.5-Coder-7B-Instruct模型获得了70.9%的准确率，超过了Repanda基线（67.6%）。与端到端模型融合后，准确率进一步提升至84.3%。

Conclusion: 分步、带注释的代码生成方法不仅提升了TableQA任务的正确率，还增强了模型的推理透明度和可解释性，且通过与端到端模型结合可取得更优效果。

Abstract: Table Question Answering (TableQA) poses a significant challenge for large language models (LLMs) because conventional linearization of tables often disrupts the two-dimensional relationships intrinsic to structured data. Existing methods, which depend on end-to-end answer generation or single-line program queries, typically exhibit limited numerical accuracy and reduced interpretability. This work introduces a commented, step-by-step code-generation framework that incorporates explicit reasoning into the Python program-generation process. The approach decomposes TableQA reasoning into multi-line executable programs with concise natural language comments, thereby promoting clearer reasoning and increasing the likelihood of generating correct code. On the WikiTableQuestions benchmark, the proposed method achieves 70.9\% accuracy using Qwen2.5-Coder-7B-Instruct, surpassing the Repanda baseline (67.6\%). Integrating the proposed framework with a robust end-to-end TableQA model via a lightweight answer-selection mechanism yields further improvements. This combined approach achieves up to 84.3\% accuracy on the WikiTableQuestions benchmark.

</details>


### [300] [A Hierarchical and Attentional Analysis of Argument Structure Constructions in BERT Using Naturalistic Corpora](https://arxiv.org/abs/2602.00554)
*Liu Kaipeng,Wu Ling*

Main category: cs.CL

TL;DR: 本研究分析了BERT模型如何处理四种基本论元结构建构，揭示了其分层的表征结构和不同层之间的信息分化过程。


<details>
  <summary>Details</summary>
Motivation: 理解BERT等大型语言模型在处理复杂语言结构时的内部机制，特别是对核心句法结构的信息表征方式。

Method: 采用多维分析框架，包括MDS和t-SNE降维、GDV作为聚类分离度指标、FDR线性诊断探查以及注意力机制分析，综合揭示各层对结构信息的编码情况。

Result: 发现BERT在早期层中显现出建构特异性信息，于中间层背景下样本可形成区分性极高的聚类，并在后期层中持续保持这些信息。

Conclusion: BERT模型具备分层结构，不同句法建构信息在各层中以不同形式表现和传递，展示了深层模型对语言结构的精细捕捉能力。

Abstract: This study investigates how the Bidirectional Encoder Representations from Transformers model processes four fundamental Argument Structure Constructions. We employ a multi-dimensional analytical framework, which integrates MDS, t-SNE as dimensionality reduction, Generalized Discrimination Value (GDV) as cluster separation metrics, Fisher Discriminant Ratio (FDR) as linear diagnostic probing, and attention mechanism analysis. Our results reveal a hierarchical representational structure. Construction-specific information emerges in early layers, forms maximally separable clusters in middle layers, and is maintained through later processing stages.

</details>


### [301] [The French Drama Revolution: Political Economy and Literary Production, 1700-1900](https://arxiv.org/abs/2602.00588)
*Thiago Dumont Oliveira*

Main category: cs.CL

TL;DR: 本文利用主题建模和文本相似性方法分析了1700-1900年间法国戏剧内容的演变，揭示了法国大革命后戏剧主题的重大变化及其与经济增长的关系。


<details>
  <summary>Details</summary>
Motivation: 探究法国戏剧在两个世纪内如何随历史、政治与经济变革发生主题变化，尤其关注法国大革命及工业化进程对戏剧内容的影响。

Method: 采用Latent Dirichlet Allocation（LDA）主题建模和Jensen-Shannon Divergence评估戏剧文本主题分布的变化，并将戏剧主题演变与同期法国GDP数据进行对比分析。

Result: 发现法国大革命后，尤其是1789至1850年间，戏剧的主题分布发生了显著变化，资产阶级主题自18世纪末起成为主流。戏剧主题的变化与法国经济、政治环境变迁相呼应。

Conclusion: 法国戏剧内容在政治革命和经济增长的背景下不断演化，展现出社会结构和价值观转变的痕迹，戏剧与社会变迁之间存在明显的互动和共演关系。

Abstract: This paper investigates the changing nature of French drama between 1700-1900 using Latent Dirichlet Allocation and Jensen-Shannon Divergence. Results indicate that the topical distribution of French drama changed profoundly after the French Revolution, particularly between 1789 and 1850. Bourgeois themes emerged among the most prevalent topics since the late 18th century. To assess the coevolution of drama and economic growth, I plot the yearly prevalence of topics alongside French GDP between 1700-1900, and discuss these changes in light of the political and economic changes prompted by the French Revolution and the industrialization of the country.

</details>


### [302] [Kanade: A Simple Disentangled Tokenizer for Spoken Language Modeling](https://arxiv.org/abs/2602.00594)
*Zhijie Huang,Stephen McIntosh,Daisuke Saito,Nobuaki Minematsu*

Main category: cs.CL

TL;DR: 本论文提出了一种新的单层音频分离分词器Kanade，能有效分离语音中的发音和韵律信息，同时抑制与语言无关的信息，如说话人身份，提升语音合成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的语音分词技术难以同时分离出丰富的语音特征（如发音、韵律）并抑制说话人等非语言因素，且通常依赖复杂的辅助方法。研究者希望开发一种更简单高效的分词方法。

Method: 提出Kanade分词器，通过单层结构分离出语音中的声学常数，生成仅包含语音和韵律信息的token流，无需依赖复杂的辅助分离模块。

Result: 实验表明，Kanade在说话人分离、词汇可用性方面达到当前最优水平，并且能够保持出色的语音重建质量。

Conclusion: Kanade是一种高效、结构简单的语音分词方法，能生成高质量、具有可用语音特征的token，对语音建模与生成具有很大潜力。

Abstract: A good language model starts with a good tokenizer. Tokenization is especially important for speech modeling, which must handle continuous signals that mix linguistic and non-linguistic information. A speech tokenizer should extract phonetics and prosody, suppress linguistically irrelevant information like speaker identity, and enable high-quality synthesis. We present Kanade, a single-layer disentangled speech tokenizer that realizes this ideal. Kanade separates out acoustic constants to create a single stream of tokens that captures rich phonetics and prosody. It does so without the need for auxiliary methods that existing disentangled codecs often rely on. Experiments show that Kanade achieves state-of-the-art speaker disentanglement and lexical availability, while maintaining excellent reconstruction quality.

</details>


### [303] [Hermes the Polyglot: A Unified Framework to Enhance Expressiveness for Multimodal Interlingual Subtitling](https://arxiv.org/abs/2602.00597)
*Chaoqun Cui,Shijing Wang,Liangbin Huang,Qingqing Gu,Zhaolong Huang,Xiao Zeng,Wenji Mao*

Main category: cs.CL

TL;DR: 本文提出了Hermes，一个基于大语言模型的自动字幕翻译框架，有效提升字幕语义连贯性、术语和代词翻译及表达力，实验结果优于当前水平。


<details>
  <summary>Details</summary>
Motivation: 跨语言字幕翻译对于影视本地化至关重要，但其中特有的字幕文本特点（如简洁、语境依赖强等）为现有机器翻译带来挑战，尤其是在语义连贯性、术语和代词翻译及表达能力等方面。因此，亟需更有效的自动翻译方法解决这些难题。

Method: 作者提出了Hermes框架，结合了三大模块：说话人分离（Speaker Diarization）、术语识别（Terminology Identification）和表达力增强（Expressiveness Enhancement），以提升字幕翻译质量。

Result: 实验表明，Hermes在说话人分离方面达到了当前最优性能，并能生成更加表达丰富、语境连贯的跨语言字幕翻译。

Conclusion: Hermes框架解决了字幕翻译的关键挑战，推动了跨语言字幕翻译自动化研究的进步。

Abstract: Interlingual subtitling, which translates subtitles of visual media into a target language, is essential for entertainment localization but has not yet been explored in machine translation. Although Large Language Models (LLMs) have significantly advanced the general capabilities of machine translation, the distinctive characteristics of subtitle texts pose persistent challenges in interlingual subtitling, particularly regarding semantic coherence, pronoun and terminology translation, and translation expressiveness. To address these issues, we present Hermes, an LLM-based automated subtitling framework. Hermes integrates three modules: Speaker Diarization, Terminology Identification, and Expressiveness Enhancement, which effectively tackle the above challenges. Experiments demonstrate that Hermes achieves state-of-the-art diarization performance and generates expressive, contextually coherent translations, thereby advancing research in interlingual subtitling.

</details>


### [304] [Lookahead-then-Verify: Reliable Constrained Decoding for Diffusion LLMs under Context-Free Grammars](https://arxiv.org/abs/2602.00612)
*Yitong Zhang,Yongmin Li,Yuetong Liu,Jia Li,Xiaoran Jia,Zherui Li,Ge Li*

Main category: cs.CL

TL;DR: 本文提出了一种名为LAVE的新方法，专为扩散型大语言模型（dLLMs）设计，实现了在生成过程中的语法约束解码，大幅提升了输出的语法正确性且几乎不影响运行效率。


<details>
  <summary>Details</summary>
Motivation: 虽然dLLMs在生成形式化语言（如代码、化学表达式）方面表现突出，但由于其概率生成特性，生成结果常常不符合语法规范。现有的约束解码方法大多针对自回归模型，而dLLMs的并行生成方式使这些方法难以直接应用。此外，专为dLLMs设计的一些方法，无法保证中间输出后续能扩展为合法句子，降低了可靠性。因此，亟需一种高效、适用于dLLMs的语法约束解码技术。

Method: 作者提出了LAVE方法，充分利用dLLMs每轮迭代能并行预测所有位置token分布的特性。当模型提出新token时，LAVE能基于这些分布进行前瞻检查（lookahead），高效检验新token的合法性，从而保证中间输出一定有可能被扩展成符合语法的完整句子。

Result: 作者在四种主流dLLMs和三个标准评测集上进行对比实验，结果显示LAVE在语法正确性方面远超现有基线，并且基本不会增加运行时开销。

Conclusion: LAVE为dLLMs带来了高效且可靠的语法约束解码能力，显著提升了dLLMs在代码等语法严格场景下的实用性，为语法约束生成领域提供了新的解决思路。

Abstract: Diffusion Large Language Models (dLLMs) have demonstrated promising generative capabilities and are increasingly used to produce formal languages defined by context-free grammars, such as source code and chemical expressions. However, as probabilistic models, they still struggle to generate syntactically valid outputs reliably. A natural and promising direction to address this issue is to adapt constrained decoding techniques to enforce grammatical correctness during generation. However, applying these techniques faces two primary obstacles. On the one hand, the non-autoregressive nature of dLLMs renders most existing constrained decoding approaches inapplicable. On the other hand, current approaches specifically designed for dLLMs may allow intermediate outputs that are impossible to complete into valid sentences, which significantly limits their reliability in practice.
  To address these challenges, we present LAVE, a constrained decoding approach specifically designed for dLLMs. Our approach leverages a key property of dLLMs, namely their ability to predict token distributions for all positions in parallel during each forward pass. Whenever a new token is proposed by model, LAVE performs lookahead using these distributions to efficiently and reliably verify the validity of the proposed token. This design ensures reliable constraints by reliably preserving the potential for intermediate outputs to be extended into valid sentences. Extensive experiments across four widely used dLLMs and three representative benchmarks demonstrate that LAVE consistently outperforms existing baselines and achieves substantial improvements in syntactic correctness, while incurring negligible runtime overhead.

</details>


### [305] [Transformer-Based Model for Multilingual Hope Speech Detection](https://arxiv.org/abs/2602.00613)
*Nsrin Ashraf,Mariam Labib,Hamada Nayel*

Main category: cs.CL

TL;DR: 本文提出并评估了用于英语和德语希望语检测的transformer模型，取得了较优的性能。


<details>
  <summary>Details</summary>
Motivation: 希望语检测是NLP领域的重要任务，提升相关模型性能有助于构建更积极正向的语言环境。作者希望探究不同transformer在该任务中的表现。

Method: 针对希望语检测任务，作者对英语使用RoBERTa模型，英语和德语则使用多语言XLM-RoBERTa模型，在相关数据集上进行训练与评估。

Result: RoBERTa模型在英语上获得加权F1分数0.818、准确率81.8%；XLM-RoBERTa在英语和德语上获得F1分数0.786、准确率78.5%。

Conclusion: 实验结果表明大型预训练语言模型对提升希望语检测任务表现效果显著，展示了transformer模型对多种NLP任务的促进作用。

Abstract: This paper describes a system that has been submitted to the "PolyHope-M" at RANLP2025. In this work various transformers have been implemented and evaluated for hope speech detection for English and Germany. RoBERTa has been implemented for English, while the multilingual model XLM-RoBERTa has been implemented for both English and German languages. The proposed system using RoBERTa reported a weighted f1-score of 0.818 and an accuracy of 81.8% for English. On the other hand, XLM-RoBERTa achieved a weighted f1-score of 0.786 and an accuracy of 78.5%. These results reflects the importance of improvement of pre-trained large language models and how these models enhancing the performance of different natural language processing tasks.

</details>


### [306] [Jailbreaking LLMs via Calibration](https://arxiv.org/abs/2602.00619)
*Yuxuan Lu,Yongkang Guo,Yuqing Kong*

Main category: cs.CL

TL;DR: 本文提出一种新的理论框架，将大语言模型（LLMs）中的安全对齐视为对原始数据分布的系统性扭曲，并据此改进了破解（jailbreaking）方法，提高了破解成功率，同时降低了功能损失。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐使模型输出与预训练数据分布产生差异，影响了下游任务表现，也为破解者利用这些差异进行攻击提供了机会。理解并刻画这种扭曲机制，有助于更有效地实施破解攻击及调整防御措施。

Method: 作者将破解问题形式化为预测聚合（forecast aggregation）问题，提出利用损失空间中的梯度偏移（Gradient Shift）来达到最优聚合，并将常见logit-arithmetic破解方法框定为交叉熵损失情形下的特例。此外，作者推广出对应其他损失函数的聚合规则，并提出一个混合型的新规则。

Result: 在多项red-teaming基准与数学任务上，所提方法在攻击成功率（Attack Success Rate）和“破解代价”（Jailbreak Tax）方面均优于现有方法，尤其在安全性更强的gpt-oss-120b模型上表现突出。

Conclusion: 通过理论建模与实验验证，作者证明了新框架和新方法能够更高效地对安全对齐的LLMs进行破解，同时减少对模型功能的损害。这为未来破解研究和防御机制设计提供了理论和实践基础。

Abstract: Safety alignment in Large Language Models (LLMs) often creates a systematic discrepancy between a model's aligned output and the underlying pre-aligned data distribution. We propose a framework in which the effect of safety alignment on next-token prediction is modeled as a systematic distortion of a pre-alignment distribution. We cast Weak-to-Strong Jailbreaking as a forecast aggregation problem and derive an optimal aggregation strategy characterized by a Gradient Shift in the loss-induced dual space. We show that logit-arithmetic jailbreaking methods are a special case of this framework under cross-entropy loss, and derive a broader family of aggregation rules corresponding to other proper losses. We also propose a new hybrid aggregation rule. Evaluations across red-teaming benchmarks and math utility tasks using frontier models demonstrate that our approach achieves superior Attack Success Rates and lower "Jailbreak Tax" compared with existing methods, especially on the safety-hardened gpt-oss-120b.

</details>


### [307] [Formal Semantic Control over Language Models](https://arxiv.org/abs/2602.00638)
*Yingji Zhang*

Main category: cs.CL

TL;DR: 本文旨在提升语义表示学习，使得语言模型的表示更具可解释性与可控性，并通过有意识地塑造潜空间几何结构，实现局部、类符号式的组合控制。作者在VAE框架下，分别针对句子级别和推理级别开展了研究，提出并验证了一系列创新理论与方法，实验证明其能够提升自然语言模型潜空间的可解释性与可控性。


<details>
  <summary>Details</summary>
Motivation: 目前主流的语言模型内部语义表示难以解释和控制，这限制了其于复杂语言任务（如生成与推理）中的表现和应用。作者希望通过提升潜空间的可解释性和可控性，让语言模型朝向更精细、系统的解释性和可操作性发展。

Method: 本文基于变分自编码器（VAE）框架，从两个方向入手：（1）句子级别：在潜空间中解耦并操控特定语义特征，指导句子生成，以解释性文本任务为实验平台；（2）推理级别：在潜空间中隔离和操控推理行为，控制自然语言推理（NLI），特别聚焦于解释性NLI任务。作者提出了新颖的理论框架和实际方法，并设计了相应的实验进行验证。

Result: 通过理论分析和实验，作者的方法有效提升了潜空间的语义可解释性与几何结构可控性，不仅能使模型生成受控的、带有指定语义特征的文本，还能在推理类任务中实现对模型行为的精细控制。

Conclusion: 论文展示了通过对潜空间的结构化与操控，可以明显提升自然语言模型的内部可解释性和外部可控性，这为更透明、可操作的语言智能系统发展奠定了基础。

Abstract: This thesis advances semantic representation learning to render language representations or models more semantically and geometrically interpretable, and to enable localised, quasi-symbolic, compositional control through deliberate shaping of their latent space geometry. We pursue this goal within a VAE framework, exploring two complementary research directions: (i) Sentence-level learning and control: disentangling and manipulating specific semantic features in the latent space to guide sentence generation, with explanatory text serving as the testbed; and (ii) Reasoning-level learning and control: isolating and steering inference behaviours in the latent space to control NLI. In this direction, we focus on Explanatory NLI tasks, in which two premises (explanations) are provided to infer a conclusion. The overarching objective is to move toward language models whose internal semantic representations can be systematically interpreted, precisely structured, and reliably directed. We introduce a set of novel theoretical frameworks and practical methodologies, together with corresponding experiments, to demonstrate that our approaches enhance both the interpretability and controllability of latent spaces for natural language across the thesis.

</details>


### [308] [LegalOne: A Family of Foundation Models for Reliable Legal Reasoning](https://arxiv.org/abs/2602.00642)
*Haitao Li,Yifan Chen,Shuo Miao,Qian Dong,Jia Chen,Yiran Hu,Junjie Chen,Minghao Qin,Qingyao Ai,Yiqun Liu,Cheng Luo,Quan Zhou,Ya Zhang,Jikun Hu*

Main category: cs.CL

TL;DR: 本文提出并开源了LegalOne，一套专为中文法律领域设计的大语言模型，显著提升了法律推理和任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）虽能力出众，但在法律领域因缺乏精准领域知识和复杂的推理能力，直接应用受限。

Method: 作者设计了三个阶段：1）中期训练引入了基于困惑度的PAS采样方法，平衡新知识吸收和原有能力保持；2）有监督微调使用了Legal Agentic CoT Distillation（LEAD），将复杂司法流程转化为结构化推理过程；3）采用课程式强化学习，自记忆、理解到推理，逐步提升模型法律推理能力。

Result: LegalOne在多项法律任务上取得了领先于更大参数通用模型的表现，表现出更高的知识密度与推理效率。

Conclusion: LegalOne为法律AI领域提供了更可信、可解释的基础模型，有助于推进高风险司法场景下的模型部署，并公开了模型权重和评测工具，以推动研究发展。

Abstract: While Large Language Models (LLMs) have demonstrated impressive general capabilities, their direct application in the legal domain is often hindered by a lack of precise domain knowledge and complexity of performing rigorous multi-step judicial reasoning. To address this gap, we present LegalOne, a family of foundational models specifically tailored for the Chinese legal domain. LegalOne is developed through a comprehensive three-phase pipeline designed to master legal reasoning. First, during mid-training phase, we propose Plasticity-Adjusted Sampling (PAS) to address the challenge of domain adaptation. This perplexity-based scheduler strikes a balance between the acquisition of new knowledge and the retention of original capabilities, effectively establishing a robust legal foundation. Second, during supervised fine-tuning, we employ Legal Agentic CoT Distillation (LEAD) to distill explicit reasoning from raw legal texts. Unlike naive distillation, LEAD utilizes an agentic workflow to convert complex judicial processes into structured reasoning trajectories, thereby enforcing factual grounding and logical rigor. Finally, we implement a Curriculum Reinforcement Learning (RL) strategy. Through a progressive reinforcement process spanning memorization, understanding, and reasoning, LegalOne evolves from simple pattern matching to autonomous and reliable legal reasoning. Experimental results demonstrate that LegalOne achieves state-of-the-art performance across a wide range of legal tasks, surpassing general-purpose LLMs with vastly larger parameter counts through enhanced knowledge density and efficiency. We publicly release the LegalOne weights and the LegalKit evaluation framework to advance the field of Legal AI, paving the way for deploying trustworthy and interpretable foundation models in high-stakes judicial applications.

</details>


### [309] [Can Small Language Models Handle Context-Summarized Multi-Turn Customer-Service QA? A Synthetic Data-Driven Comparative Evaluation](https://arxiv.org/abs/2602.00665)
*Lakshan Cooray,Deshan Sumanathilaka,Pattigadapa Venkatesh Raju*

Main category: cs.CL

TL;DR: 本文评估了小型语言模型（SLM）在多轮客服问答中的表现，发现部分模型接近大型语言模型（LLM）的效果，但整体仍有局限。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在客服对话问答中表现出色，但其高昂的计算成本和部署难度让其在资源受限场景下难以应用。SLM作为更高效的替代选择，其在多轮、需语境延续的客服场景下的能力尚未充分探索。

Method: 作者采用指令微调的小型语言模型（SLM），结合对话历史摘要策略，用于多轮客服问答。此外，设计了基于对话阶段的定性分析方法，从不同阶段评估模型表现。通过词汇、语义相似度指标，以及人工与LLM打分等质性方法，对9个SLM和3个商业LLM进行了评估。

Result: 实验显示不同SLM之间差异显著，有些模型能够达到接近LLM的表现，但有些难以持续对话连贯性与语境切合度。

Conclusion: 小型语言模型在实际客服问答系统中具备潜力，但目前在对话连贯性和上下文把握方面还存在明显局限。

Abstract: Customer-service question answering (QA) systems increasingly rely on conversational language understanding. While Large Language Models (LLMs) achieve strong performance, their high computational cost and deployment constraints limit practical use in resource-constrained environments. Small Language Models (SLMs) provide a more efficient alternative, yet their effectiveness for multi-turn customer-service QA remains underexplored, particularly in scenarios requiring dialogue continuity and contextual understanding. This study investigates instruction-tuned SLMs for context-summarized multi-turn customer-service QA, using a history summarization strategy to preserve essential conversational state. We also introduce a conversation stage-based qualitative analysis to evaluate model behavior across different phases of customer-service interactions. Nine instruction-tuned low-parameterized SLMs are evaluated against three commercial LLMs using lexical and semantic similarity metrics alongside qualitative assessments, including human evaluation and LLM-as-a-judge methods. Results show notable variation across SLMs, with some models demonstrating near-LLM performance, while others struggle to maintain dialogue continuity and contextual alignment. These findings highlight both the potential and current limitations of low-parameterized language models for real-world customer-service QA systems.

</details>


### [310] [EchoReview: Learning Peer Review from the Echoes of Scientific Citations](https://arxiv.org/abs/2602.00733)
*Yinuo Zhang,Dingcheng Huang,Haifeng Suo,Yizhuo Li,Ziya Zhao,Junhao Xu,Zhiying Tu,Dianhui Chu,Deming Zhai,Xianming Liu,Xiaoyan Yu,Dianbo Sui*

Main category: cs.CL

TL;DR: 本文提出了一个基于引用上下文的数据合成框架，结合学术引用中的集体评价信号生成结构化评论数据，并基于此训练了自动化审稿系统，显著提升了自动审稿质量。


<details>
  <summary>Details</summary>
Motivation: 随着学术投稿数量的快速增长，传统人工同行评审系统面临可扩展性压力，迫切需要既具规模又可靠的自动化审稿方法。而现有基于真实评论的有监督微调方法受限于数据单一来源及人为评论的主观性、不一致性，难以支持高质量自动审稿。

Method: 提出了EchoReview框架，利用学术论文引用中的隐式集体评估信号自动构建结构化评论风格数据。基于该流程，构建了大规模跨会议、跨年份的引用驱动评论数据集EchoReview-16K，并据此训练自动化审稿模型EchoReviewer-7B。

Result: 实验结果显示，EchoReviewer-7B在证据支持和评论全面性等核心维度上取得了显著且稳定的提升，表明引用上下文可以作为鲁棒有效的自动化审稿数据来源。

Conclusion: 该研究验证了引用上下文驱动的数据范式在自动化同行评审中的可行性和有效性，为实现可靠且高质量的自动化学术评审奠定了基础。

Abstract: As the volume of scientific submissions continues to grow rapidly, traditional peer review systems are facing unprecedented scalability pressures, highlighting the urgent need for automated reviewing methods that are both scalable and reliable. Existing supervised fine-tuning approaches based on real review data are fundamentally constrained by single-source of data as well as the inherent subjectivity and inconsistency of human reviews, limiting their ability to support high-quality automated reviewers. To address these issues, we propose EchoReview, a citation-context-driven data synthesis framework that systematically mines implicit collective evaluative signals from academic citations and transforms scientific community's long-term judgments into structured review-style data. Based on this pipeline, we construct EchoReview-16K, the first large-scale, cross-conference, and cross-year citation-driven review dataset, and train an automated reviewer, EchoReviewer-7B. Experimental results demonstrate that EchoReviewer-7B can achieve significant and stable improvements on core review dimensions such as evidence support and review comprehensiveness, validating citation context as a robust and effective data paradigm for reliable automated peer review.

</details>


### [311] [ExperienceWeaver: Optimizing Small-sample Experience Learning for LLM-based Clinical Text Improvement](https://arxiv.org/abs/2602.00740)
*Ziyan Xiao,Yinghao Zhu,Liang Peng,Lequan Yu*

Main category: cs.CL

TL;DR: 本文提出了一种新方法ExperienceWeaver，能够在有限数据下提升临床文本改写效果，显著优于现有主流大模型。


<details>
  <summary>Details</summary>
Motivation: 现有临床文本改写方法在小样本场景下效果有限，数据标注成本高，且容易只做表层修正，缺乏深层改写逻辑的学习能力。

Method: 提出ExperienceWeaver层次化框架，不依赖直接检索案例，而是将多源医疗反馈（如错误修正建议和高层次改写策略）进行结构化和提炼，并在生成模型流程中注入这些改写经验，从而引导模型学习“如何改写”。

Result: 在四个真实临床数据集上广泛评测，ExperienceWeaver在小样本条件下的文本改写质量和合理性均超过了Gemini-3 Pro等先进模型。

Conclusion: 通过经验学习和层次经验注入，ExperienceWeaver克服了传统检索或调优方法对数据量的依赖，实现了更有效、更智能的临床文本改写，对实际医疗文档处理具有重要意义。

Abstract: Clinical text improvement is vital for healthcare efficiency but remains difficult due to limited high-quality data and the complex constraints of medical documentation. While Large Language Models (LLMs) show promise, current approaches struggle in small-sample settings: supervised fine-tuning is data-intensive and costly, while retrieval-augmented generation often provides superficial corrections without capturing the reasoning behind revisions. To address these limitations, we propose ExperienceWeaver, a hierarchical framework that shifts the focus from data retrieval to experience learning. Instead of simply recalling past examples, ExperienceWeaver distills noisy, multi-dimensional feedback into structured, actionable knowledge. Specifically, error-specific Tips and high-level Strategies. By injecting this distilled experience into an agentic pipeline, the model learns "how to revise" rather than just "what to revise". Extensive evaluations across four clinical datasets demonstrate that ExperienceWeaver consistently improves performance, surpassing state-of-the-art models such as Gemini-3 Pro in small-sample settings.

</details>


### [312] [CURP: Codebook-based Continuous User Representation for Personalized Generation with LLMs](https://arxiv.org/abs/2602.00742)
*Liang Wang,Xinyi Mou,Xiaoyou Liu,Xuanjing Huang,Zhongyu Wei*

Main category: cs.CL

TL;DR: 该论文提出了一种新的人物建模框架 CURP，可以高效地从用户的偏好和行为数据中抽取个性化特征，实现大模型下高效的个性化生成。


<details>
  <summary>Details</summary>
Motivation: 现有用户建模方法在个性化质量和计算/数据效率之间难以兼顾，因此需要一种既高效又能保证个性化质量的新方法。

Method: CURP 框架引入了双向用户编码器和离散原型码书，能够提取多维度的用户特征。该方法仅需约2000万可训练参数（占模型总参数约0.2%），支持即插即用的个性化。

Result: 大量生成相关任务实验表明，CURP在性能和泛化能力上均优于主流的强基线，并且具有更好的可解释性与可扩展性。

Conclusion: CURP在保证高个性化能力的同时，极大提升了训练和计算效率，为大语言模型的用户建模与个性化生成提供了更优方案。

Abstract: User modeling characterizes individuals through their preferences and behavioral patterns to enable personalized simulation and generation with Large Language Models (LLMs) in contemporary approaches. However, existing methods, whether prompt-based or training-based methods, face challenges in balancing personalization quality against computational and data efficiency. We propose a novel framework CURP, which employs a bidirectional user encoder and a discrete prototype codebook to extract multi-dimensional user traits. This design enables plug-and-play personalization with a small number of trainable parameters (about 20M parameters, about 0.2\% of the total model size). Through extensive experiments on variant generation tasks, we show that CURP achieves superior performance and generalization compared to strong baselines, while offering better interpretability and scalability. The code are available at https://github.com/RaidonWong/CURP_code

</details>


### [313] [Decouple Searching from Training: Scaling Data Mixing via Model Merging for Large Language Model Pre-training](https://arxiv.org/abs/2602.00747)
*Shengrui Li,Fei Zhao,Kaiyan Zhao,Jieying Ye,Haifeng Liu,Fangcheng Shi,Zheyong Xie,Yao Hu,Shaosheng Cao*

Main category: cs.CL

TL;DR: 该论文提出了一种新的预训练数据混合比例搜索方法 DeMix，通过模型融合预测最佳数据比例，显著降低了探索成本并提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型预训练中，选择合适的数据混合比例对于模型既具备通用能力又能在数学、代码等难任务上表现出色至关重要。当前的方法受限于低效的代理实验或昂贵的大规模探索，难以找到最优混合比例。因此，亟需一种高效且有效的混合比例搜索方法。

Method: 作者提出了DeMix框架。与传统每次采样都需训练代理模型不同，DeMix先分别对候选数据集训练多个组件模型，然后通过加权模型融合来模拟不同数据混合下的预训练效果，从而无需额外训练，即可快速评估任意多种数据混合方案。这样大大提高了搜索效率，降低了算力开销。

Result: 实验结果表明，DeMix能够打破以往在数据充分性、准确性和搜索效率之间的权衡困境，在更低的搜索成本下获得更优的混合比例，并带来更佳的基准测试表现。

Conclusion: DeMix有效提升了预训练数据混合搜索的能力，为大模型的训练效率和性能带来提升。论文还公开了包含高质量数据及验证混合比例的DeMix Corpora数据集，将推动相关领域的开放研究。

Abstract: Determining an effective data mixture is a key factor in Large Language Model (LLM) pre-training, where models must balance general competence with proficiency on hard tasks such as math and code. However, identifying an optimal mixture remains an open challenge, as existing approaches either rely on unreliable tiny-scale proxy experiments or require prohibitively expensive large-scale exploration. To address this, we propose Decouple Searching from Training Mix (DeMix), a novel framework that leverages model merging to predict optimal data ratios. Instead of training proxy models for every sampled mixture, DeMix trains component models on candidate datasets at scale and derives data mixture proxies via weighted model merging. This paradigm decouples search from training costs, enabling evaluation of unlimited sampled mixtures without extra training burden and thus facilitating better mixture discovery through more search trials. Extensive experiments demonstrate that DeMix breaks the trade-off between sufficiency, accuracy and efficiency, obtaining the optimal mixture with higher benchmark performance at lower search cost. Additionally, we release the DeMix Corpora, a comprehensive 22T-token dataset comprising high-quality pre-training data with validated mixtures to facilitate open research. Our code and DeMix Corpora is available at https://github.com/Lucius-lsr/DeMix.

</details>


### [314] [Temporal Leakage in Search-Engine Date-Filtered Web Retrieval: A Case Study from Retrospective Forecasting](https://arxiv.org/abs/2602.00758)
*Ali El Lahib,Ying-Jieh Xia,Zehan Li,Yuxuan Wang,Xinyu Pi*

Main category: cs.CL

TL;DR: 论文发现使用搜索引擎的日期过滤器进行“预截止”检索常被用来评估与搜索结合的预测系统，但这种做法不可靠，因为大量结果存在截止日期后的信息泄露，导致评测结果被高估。作者建议采用更严谨的数据检索策略或直接使用时间快照数据。


<details>
  <summary>Details</summary>
Motivation: 在评估搜索增强类大型语言模型时，常用的做法是对检索结果进行日期过滤，以保证模型获取的信息只截止到某一时间点。该方法旨在保障公平、准确的后验检索评估。但作者质疑该做法的实际有效性。

Method: 作者系统性地审查了Google Search的before:日期过滤结果，量化了搜索后信息泄露的情况，并分析出现泄露的常见机制。同时，他们用一个大模型（gpt-oss-120b）在含有泄露信息和无泄露信息的数据集上做预测准确性对比。

Result: 71%的测试问题返回的结果中至少有一条页面含有截止日期后的强信息泄露；41%的问题更直接给出答案。模型在含泄露数据和无泄露数据的Brier分数分别为0.108和0.242，出现严重高估。

Conclusion: 单靠日期过滤无法杜绝信息泄露，严重影响搜索增强类模型的评测公正性。建议采用更安全的检索措施，或直接依赖存档和带时间戳的静态快照。

Abstract: Search-engine date filters are widely used to enforce pre-cutoff retrieval in retrospective evaluations of search-augmented forecasters. We show this approach is unreliable: auditing Google Search with a before: filter, 71% of questions return at least one page containing strong post-cutoff leakage, and for 41%, at least one page directly reveals the answer. Using a large language model (LLM), gpt-oss-120b, to forecast with these leaky documents, we demonstrate an inflated prediction accuracy (Brier score 0.108 vs. 0.242 with leak-free documents). We characterize common leakage mechanisms, including updated articles, related-content modules, unreliable metadata/timestamps, and absence-based signals, and argue that date-restricted search is insufficient for temporal evaluation. We recommend stronger retrieval safeguards or evaluation on frozen, time-stamped web snapshots to ensure credible retrospective forecasting.

</details>


### [315] [Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning](https://arxiv.org/abs/2602.00759)
*Zhipeng Chen,Xiaobo Qin,Wayne Xin Zhao,Youbin Wu,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 本文提出了一种名为A^2D的自适应能力分解方法，用于提升大语言模型中可验证奖励强化学习(RLVR)的效果。通过让模型将复杂问题分解为多个简单子问题，并结合子问题指导进行强化学习，显著提升了模型的推理与探索能力。


<details>
  <summary>Details</summary>
Motivation: 在强化学习与可验证奖励结合(LRVR)的场景下，模型获得的信息有限，导致只能进行盲目探索，难以解决复杂任务。为克服这一挑战，作者想要在不依赖教师模型的情况下，提供额外的知识信号以增强学习过程。

Method: 方法分为两步：首先，通过RLVR（不使用知识蒸馏）训练一个分解器，该分解器能够把复杂问题拆解成一系列子问题；其次，利用分解器为每个训练集问题标注子问题，再对推理器(reasoner)在设有子问题指导下进行RLVR训练。此外，该方法作为插拔模块可应用于多种RLVR算法。

Result: A^2D与现有强基线进行比较，表现出更优越的效果。方法模块化，可直接应用于不同RLVR算法。分析发现分解器的指导促进了推理器对环境的探索和利用，有效提升了强化学习结果。

Conclusion: A^2D方法能自动为LLM生成子问题，显著提升了RLVR的学习效率与表现，并具备良好的通用性。该方案为大模型在复杂任务中的推理和探索能力提升提供了新的工具。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has shown great potential to enhance the reasoning ability of large language models (LLMs). However, due to the limited amount of information provided during the RLVR process, the model can only engage in largely blind exploration, which often results in failure on challenging problems. To provide additional information for the RLVR process without relying on a teacher model, we propose A$^2$D, an Adaptive Ability Decomposing method for enhancing the effectiveness of RLVR. Specifically, we first train a decomposer via RLVR without distillation, enabling it to decompose complex questions into a set of simpler sub-questions. Next, we use this decomposer to annotate sub-questions for each question in the training dataset, and then train the reasoner under RLVR with sub-question guidance. To better understand A$^2$D, we first compare its performance with competitive baselines, showing its effectiveness. Next, we observe that our method functions as a plug-and-play module that can be applied to different RLVR algorithms. Furthermore, we conduct an analysis of the decomposer, revealing how the RLVR process affects its performance and behavior, and which type of guidance is better suited for enhancing the reasoner's exploration and exploitation abilities.

</details>


### [316] [APR: Penalizing Structural Redundancy in Large Reasoning Models via Anchor-based Process Rewards](https://arxiv.org/abs/2602.00760)
*Kaiyan Chang,Chenwei Zhu,Yingfeng Luo,Yifu Huo,Chenglong Wang,Xiaoqian Liu,Qiaozhi He,Tong Xiao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本文对大模型推理过程中的“过度思考”现象进行了细致剖析，提出了一种基于锚点的奖励策略（APR），有效提升了模型在数学推理任务中的性能与效率。


<details>
  <summary>Details</summary>
Motivation: 虽然Test-Time Scaling（TTS）增强了大模型的推理能力，但也伴随“过度思考”副作用，即模型在找到答案后仍重复自我验证。迫切需要理解和改进该过程，以提升推理效率和资源利用率。

Method: 作者通过精细分析模型推理过程，引入了Reasoning Anchor（推理锚点）的定义，即模型答案第一次稳定的位置。随后提出Answer-Stable Tail（AST），定义为锚点后的冗余重复验证行为。在此基础上，提出Anchor-based Process Reward（APR）算法，通过定位锚点并对AST部分加以惩罚，结合适用于长度惩罚的策略优化方法，训练更高效的推理模型。

Result: 提出的APR模型在1.5B和7B参数规模下，于五个数学推理数据集上均达到了性能-效率折中最优，无需消耗大量强化学习训练资源。

Conclusion: 锚点识别与结构化奖惩机制能显著减少无意义的推理冗余，在提升大模型推理可解释性和效率的同时保持甚至优化推理性能。

Abstract: Test-Time Scaling (TTS) has significantly enhanced the capabilities of Large Reasoning Models (LRMs) but introduces a critical side-effect known as Overthinking. We conduct a preliminary study to rethink this phenomenon from a fine-grained perspective. We observe that LRMs frequently conduct repetitive self-verification without revision even after obtaining the final answer during the reasoning process. We formally define this specific position where the answer first stabilizes as the Reasoning Anchor. By analyzing pre- and post-anchor reasoning behaviors, we uncover the structural redundancy fixed in LRMs: the meaningless repetitive verification after deriving the first complete answer, which we term the Answer-Stable Tail (AST). Motivated by this observation, we propose Anchor-based Process Reward (APR), a structure-aware reward shaping method that localizes the reasoning anchor and penalizes exclusively the post-anchor AST. Leveraging the policy optimization algorithm suitable for length penalties, our APR models achieved the performance-efficiency Pareto frontier at 1.5B and 7B scales averaged across five mathematical reasoning datasets while requiring significantly fewer computational resources for RL training.

</details>


### [317] [WordCraft: Scaffolding the Keyword Method for L2 Vocabulary Learning with Multimodal LLMs](https://arxiv.org/abs/2602.00762)
*Yuheng Shao,Junjie Xiong,Chaoran Wu,Xiyuan Wang,Ziyu Zhou,Yang Ouyang,Qinyi Tao,Quan Li*

Main category: cs.CL

TL;DR: 本文提出了一种基于多模态大语言模型（MLLMs）的互动工具WordCraft，旨在辅助以中文为母语的英语学习者更有效地运用关键词法记忆单词。实验结果显示，该工具有效提升了记忆效果及用户体验。


<details>
  <summary>Details</summary>
Motivation: 以中文为母语的英语学习者在用关键词法记忆词汇时存在明显困难，如难以构思恰当的关键词、关联及意象，对学习过程的指导不足；现有方法或牺牲了学习者主动性，或缺乏过程层面的辅助。

Method: 作者首先通过对18名L1中文的英语学习者和教师的访谈，分析关键词法实践中的主要挑战。随后基于这些见解，开发了WordCraft工具，引导学习者分步选择关键词、构建关联并形成意象，同时利用MLLMs增强互动体验。最后，通过两轮用户实验评估其效果及可用性。

Result: 实验表明，WordCraft既能保护关键词法的生成效应，又具有较高的记忆效果和用户可用性。

Conclusion: WordCraft作为一个以学习者为中心、基于MLLMs的互动工具，有效弥补了传统关键词法辅导的不足，为二语词汇记忆提供了切实可行的技术支持。

Abstract: Applying the keyword method for vocabulary memorization remains a significant challenge for L1 Chinese-L2 English learners. They frequently struggle to generate phonologically appropriate keywords, construct coherent associations, and create vivid mental imagery to aid long-term retention. Existing approaches, including fully automated keyword generation and outcome-oriented mnemonic aids, either compromise learner engagement or lack adequate process-oriented guidance. To address these limitations, we conducted a formative study with L1 Chinese-L2 English learners and educators (N=18), which revealed key difficulties and requirements in applying the keyword method to vocabulary learning. Building on these insights, we introduce WordCraft, a learner-centered interactive tool powered by Multimodal Large Language Models (MLLMs). WordCraft scaffolds the keyword method by guiding learners through keyword selection, association construction, and image formation, thereby enhancing the effectiveness of vocabulary memorization. Two user studies demonstrate that WordCraft not only preserves the generation effect but also achieves high levels of effectiveness and usability.

</details>


### [318] [Eliciting Trustworthiness Priors of Large Language Models via Economic Games](https://arxiv.org/abs/2602.00769)
*Siyu Yan,Lusha Zhu,Jian-Qiao Zhu*

Main category: cs.CL

TL;DR: 本论文提出了一种新的方法，用于评估大型语言模型（如GPT-4.1）的信任倾向，发现GPT-4.1的信任度与人类相似，并通过信任博弈探讨其对不同角色的信任决策。


<details>
  <summary>Details</summary>
Motivation: 在构建以人为本、值得信赖的人工智能系统时，维持恰当的信任度至关重要，但对AI本身信任程度的刻画方法尚不明确。本研究为了解和量化AI模型的信任倾向提供方法学创新。

Method: 作者基于行为博弈论中的信任博弈，应用迭代性in-context learning方法，从多个先进大型语言模型中提取信任倾向（trustworthiness priors），并比较其与人类的信任度表现。同时研究模型对不同玩家身份的信任反应，并用基于刻板印象的模型（基于温暖感和能力感知）进行解释。

Result: GPT-4.1的信任倾向与人类高度一致，可以区分不同角色间的信任度，而这种信任度差异可以通过温暖和能力两个维度的刻板印象有效预测。

Conclusion: 本研究方法能有效量化AI模型中的信任倾向，GPT-4.1等大型语言模型在信任博弈中的表现与人类相似。这为理解与优化AI与人的协作关系提供了新的视角。

Abstract: One critical aspect of building human-centered, trustworthy artificial intelligence (AI) systems is maintaining calibrated trust: appropriate reliance on AI systems outperforms both overtrust (e.g., automation bias) and undertrust (e.g., disuse). A fundamental challenge, however, is how to characterize the level of trust exhibited by an AI system itself. Here, we propose a novel elicitation method based on iterated in-context learning (Zhu and Griffiths, 2024a) and apply it to elicit trustworthiness priors using the Trust Game from behavioral game theory. The Trust Game is particularly well suited for this purpose because it operationalizes trust as voluntary exposure to risk based on beliefs about another agent, rather than self-reported attitudes. Using our method, we elicit trustworthiness priors from several leading large language models (LLMs) and find that GPT-4.1's trustworthiness priors closely track those observed in humans. Building on this result, we further examine how GPT-4.1 responds to different player personas in the Trust Game, providing an initial characterization of how such models differentiate trust across agent characteristics. Finally, we show that variation in elicited trustworthiness can be well predicted by a stereotype-based model grounded in perceived warmth and competence.

</details>


### [319] [Reasoning as State Transition: A Representational Analysis of Reasoning Evolution in Large Language Models](https://arxiv.org/abs/2602.00770)
*Siyuan Zhang,Jialian Li,Yichi Zhang,Xiao Yang,Yinpeng Dong,Hang Su*

Main category: cs.CL

TL;DR: 本文通过研究大语言模型在推理任务中的内部表示变化，发现推理过程中存在显著的表征分布转移，训练后模型能更好地驱动这一转移，内部表征与输出正确率强相关，推理表现提升主要依赖生成token的语义而非参数本身或推理时计算过程。


<details>
  <summary>Details</summary>
Motivation: 以往研究多把大模型的推理过程视为黑箱，只分析输入输出，无法揭示推理能力增强时内部状态的变化。因此，作者希望通过分析模型内部的表示动态，深入理解推理表现提升的本质。

Method: 作者提出用表示视角分析推理能力变化，跟踪不同训练阶段的模型，在推理任务中监测其内部表示分布，并用统计和反事实实验分析内部表征与输出之间的联系，从而找到推理能力提升的关键因素。

Result: 1）训练后模型在推理任务中的初始静态表征质量提升有限；2）推理过程表征分布会持续转移，且这种转移区别于非推理任务；3）训练提升了模型驱动表征转移以获得更优任务分布的能力；4）最终输出正确性与内部表征高度相关，推理能力提升主要源自生成token的语义而非参数或推理时额外计算。

Conclusion: 本研究揭示了大模型推理能力增强的内部机制，为后续模型分析与优化提供了新思路，即关注推理过程中的表征转移和输出token语义对推理结果的驱动作用。

Abstract: Large Language Models have achieved remarkable performance on reasoning tasks, motivating research into how this ability evolves during training. Prior work has primarily analyzed this evolution via explicit generation outcomes, treating the reasoning process as a black box and obscuring internal changes. To address this opacity, we introduce a representational perspective to investigate the dynamics of the model's internal states. Through comprehensive experiments across models at various training stages, we discover that post-training yields only limited improvement in static initial representation quality. Furthermore, we reveal that, distinct from non-reasoning tasks, reasoning involves a significant continuous distributional shift in representations during generation. Comparative analysis indicates that post-training empowers models to drive this transition toward a better distribution for task solving. To clarify the relationship between internal states and external outputs, statistical analysis confirms a high correlation between generation correctness and the final representations; while counterfactual experiments identify the semantics of the generated tokens, rather than additional computation during inference or intrinsic parameter differences, as the dominant driver of the transition. Collectively, we offer a novel understanding of the reasoning process and the effect of training on reasoning enhancement, providing valuable insights for future model analysis and optimization.

</details>


### [320] [HyLRA: Hybrid Layer Reuse Attention for Efficient Long-Context Inference](https://arxiv.org/abs/2602.00777)
*Xuan Ai,Qingqing Yang,Peng Wang,Lei Deng,Lin Zhang,Renhai Chen,Gong Zhang*

Main category: cs.CL

TL;DR: 本文提出HyLRA，一种结合层间和层内稀疏特性的混合注意力机制，有效提升大模型处理长上下文的推理效率，减少了KV缓存和计算复杂度，在保障精度的前提下显著提升了速度。


<details>
  <summary>Details</summary>
Motivation: 由于注意力机制计算复杂度随上下文长度呈二次增长，以及KV缓存占用大量内存，现有稀疏注意力虽可改善但往往效率和准确率难两全，亟需寻求更优折中方案。

Method: 通过实证分析，发现注意力机制中存在“层内敏感性”（部分层需完整注意力以防特征损失）和“层间相似性”（连续层关键token高度一致），据此设计HyLRA混合机制：为敏感层保留全注意力，容忍层直接重用上层top-k索引，从而减少不必要的计算。具体策略通过动态规划离线获得。

Result: 在实际大模型长文本推理任务上，HyLRA相比现有稀疏注意力方法，吞吐量提升6%-46%，且准确率损失小于1%；在各种基准上表现更佳。

Conclusion: HyLRA能兼顾推理效率与模型性能，有效突破稠密注意力的计算瓶颈，是提升大模型长上下文推理能力的优选方案。

Abstract: Long-context inference in Large Language Models (LLMs) is bottlenecked by the quadratic computation complexity of attention and the substantial memory footprint of Key-Value (KV) caches. While existing sparse attention mechanisms attempt to mitigate this by exploiting inherent sparsity, they often rely on rigid patterns or aggressive pruning, failing to achieve an optimal balance between efficiency and accuracy. In this paper, we introduce {\bf HyLRA} ({\bf Hy}brid {\bf L}ayer {\bf R}euse {\bf A}ttention), a novel framework driven by layer-wise sparsity profiling. Our empirical analysis uncovers a dual characteristic in attention mechanics: \textit{intra-layer sensitivity}, where specific layers necessitate full attention to prevent feature distortion, and \textit{inter-layer similarity}, where consecutive layers share substantial critical tokens. Based on these observations, HyLRA employs an offline dynamic programming approach to derive an optimal layer-wise policy. This hybrid strategy retains full attention for sensitive layers to ensure robustness, while enabling tolerant layers to bypass quadratic calculations by directly reusing top-$k$ indices from preceding layers. This approach allows LLMs to restrict computation to the most critical tokens, effectively overcoming the quadratic bottleneck of dense attention. Extensive evaluations demonstrate that HyLRA improves inference throughput by 6\%--46\% while maintaining comparable performance (with $<1\%$ accuracy degradation), consistently outperforming state-of-the-art sparse attention methods. HyLRA is open source at \href{https://anonymous.4open.science/r/unified-cache-management-CF80/}{\texttt{/r/unified-cache-management-CF80/}}

</details>


### [321] [Omni-RRM: Advancing Omni Reward Modeling via Automatic Rubric-Grounded Preference Synthesis](https://arxiv.org/abs/2602.00846)
*Zicheng Kong,Dehua Ma,Zhenbo Xu,Alven Yang,Yiwei Ru,Haoran Wang,Zixuan Zhou,Fuqing Bie,Liuyu Xiang,Huijia Wu,Jian Zhao,Zhaofeng He*

Main category: cs.CL

TL;DR: 本文提出了Omni-RRM，这是首个同时覆盖文本、图像、视频和音频的开源、多维度结构化评测奖励模型，通过自动化数据合成和无需人工标注有效提升了多模态大模型的对齐与评价能力，各项基准测试均达到或超越现有水平。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型（MLLMs）在实际中受限于粗糙的奖励模型（RMs），尤其缺乏能全面评价多模态输出的方法。目前主流做法过于依赖视觉、标注昂贵、且反馈不透明，限制了MMLMs的性能提升。因此需要一种更高效、结构化、解释性强的新型奖励模型。

Method: 作者提出Omni-RRM，基于自动化管线构建大规模、多模态（文本、图像、视频、音频）候选响应对，不需要人工标注，通过强大的教师模型自动生成、筛选和解释偏好，实现以评测规程为基础的结构化训练。模型分为两个阶段训练：先是有监督微调学习结构化输出，后通过强化学习（GRPO）增强模型区分细微差异的能力。

Result: Omni-RRM在视频（ShareGPT-V，80.2%正确率）、音频（Audio-HH-RLHF，66.8%）等基准上实现了最先进成绩，并在图像任务准确率上比基础模型提升17.7%，还能迁移到文本偏好评测，显著优于其它开源RM。

Conclusion: Omni-RRM显著提升多模态大模型的评测与对齐能力，不仅准确度高且方法通用，摆脱了对人工标注的依赖，并以其结构化、可解释的设计推动了多模态奖励模型的发展。

Abstract: Multimodal large language models (MLLMs) have shown remarkable capabilities, yet their performance is often capped by the coarse nature of existing alignment techniques. A critical bottleneck remains the lack of effective reward models (RMs): existing RMs are predominantly vision-centric, return opaque scalar scores, and rely on costly human annotations. We introduce \textbf{Omni-RRM}, the first open-source rubric-grounded reward model that produces structured, multi-dimension preference judgments with dimension-wise justifications across \textbf{text, image, video, and audio}. At the core of our approach is \textbf{Omni-Preference}, a large-scale dataset built via a fully automated pipeline: we synthesize candidate response pairs by contrasting models of different capabilities, and use strong teacher models to \emph{reconcile and filter} preferences while providing a modality-aware \emph{rubric-grounded rationale} for each pair. This eliminates the need for human-labeled training preferences. Omni-RRM is trained in two stages: supervised fine-tuning to learn the rubric-grounded outputs, followed by reinforcement learning (GRPO) to sharpen discrimination on difficult, low-contrast pairs. Comprehensive evaluations show that Omni-RRM achieves state-of-the-art accuracy on video (80.2\% on ShareGPT-V) and audio (66.8\% on Audio-HH-RLHF) benchmarks, and substantially outperforms existing open-source RMs on image tasks, with a 17.7\% absolute gain over its base model on overall accuracy. Omni-RRM also improves downstream performance via Best-of-$N$ selection and transfers to text-only preference benchmarks. Our data, code, and models are available at https://anonymous.4open.science/r/Omni-RRM-CC08.

</details>


### [322] [Factuality on Demand: Controlling the Factuality-Informativeness Trade-off in Text Generation](https://arxiv.org/abs/2602.00848)
*Ziwei Gong,Yanda Chen,Julia Hirschberg,Chen Zhao,He He,Zhou Yu,Kathleen Mckeown*

Main category: cs.CL

TL;DR: 提出了一种新的Factuality-Controlled Generation(FCG)框架，使用户可以根据需求控制大语言模型回答的真实度与信息量平衡，并用合成数据训练提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 不同应用对语言模型回答的真实性(factuality)和信息量(informativeness)有不同的需求，但模型通常无法灵活平衡两者，因此需要可控的生成框架。

Method: 提出FCG框架，允许用户在提问时指定对真实性的需求；并提出在训练时利用合成数据生成，提升模型根据约束给出符合要求的答案。

Result: 经过合成数据的FCG任务训练后，模型能够显著提升在遵守真实性约束的同时，仍保持较高的信息量。

Conclusion: 合成数据训练下的FCG框架让大语言模型能根据指令灵活调整答案的真实性和信息丰富度，适应不同场景需求。

Abstract: Large language models (LLMs) encode knowledge with varying degrees of confidence. When responding to queries, models face an inherent trade-off: they can generate responses that are less informative but highly factual, or more informative but potentially less accurate. Different applications demand different balances between informativeness and factuality. We introduce Factuality-Controlled Generation (FCG), a framework that enables users to specify factuality constraints alongside their queries. We propose to evaluate FCG performance on two dimensions: adherence to factuality constraints and response informativeness. We propose to train models on the FCG task using synthetic data, and show that our synthetic training significantly improves models' ability to both respect factuality requirements and maintain informativeness in their outputs.

</details>


### [323] [Unifying Adversarial Robustness and Training Across Text Scoring Models](https://arxiv.org/abs/2602.00857)
*Manveer Singh Tamber,Hosna Oyarhoseini,Jimmy Lin*

Main category: cs.CL

TL;DR: 本论文聚焦于统一文本评分类语言模型对抗鲁棒性方法，提出跨模型角色的对抗训练，让模型在多种攻击下表现更稳健，并发布了相关代码和模型。


<details>
  <summary>Details</summary>
Motivation: 当前关于语言模型对抗鲁棒性的研究较为分散，针对不同模型及攻击的研究方法各异，尚缺乏统一视角；文本评分类模型的对抗易受忽视，而其评分失败是可直接检验的，因而有必要针对这一类别提出通用对抗防御策略。

Method: 作者提出了适用于文本评分模型（包括稠密检索器、重排序器和奖励模型）的多种对抗训练方法，并探讨了将不同训练方法互补结合以提升整体鲁棒性。同时，作者构建了统一的评测框架，将攻击和训练策略横跨多种模型应用。

Result: 实验证明，现有对抗训练方式普遍存在泛化能力不足的问题；而作者提出的多方法组合对抗训练不仅提升了模型对多样攻击的鲁棒性，同时还提高了核心任务表现。在RLHF背景下，经过对抗训练的奖励模型能显著减缓reward hacking问题，并助力训练更契合人类偏好的大语言模型。

Conclusion: 文中方法有效统一了文本评分模型的对抗鲁棒性研究，证明通过多样的对抗训练策略组合能实现强鲁棒性和更优模型效果，并对实际LLM对齐与稳健应用具有重要价值，相关资源已开放供社区深入研究。

Abstract: Research on adversarial robustness in language models is currently fragmented across applications and attacks, obscuring shared vulnerabilities. In this work, we propose unifying the study of adversarial robustness in text scoring models spanning dense retrievers, rerankers, and reward models. This motivates adapting both attacks and adversarial training methods across model roles. Unlike open-ended generation, text scoring failures are directly testable: an attack succeeds when an irrelevant or rejected text outscores a relevant or chosen one. Using this principled lens of text scoring, we demonstrate that current adversarial training formulations for language models are often short-sighted, failing to effectively generalize across attacks. To address this, we introduce multiple adversarial training methods for text scoring models and show that combining complementary training methods can yield strong robustness while also improving task effectiveness. We also highlight the practical value of our approach for RLHF, showing that our adversarially trained reward models mitigate reward hacking and support the training of better-aligned LLMs. We provide our code and models for further study.

</details>


### [324] [ILSIC: Corpora for Identifying Indian Legal Statutes from Queries by Laypeople](https://arxiv.org/abs/2602.00881)
*Shounak Paul,Raghav Dogra,Pawan Goyal,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: 本论文提出了一种针对普通人提问的法律条文识别（LSI）数据集ILSiC，并在上面进行了模型实验。结果显示，纯用法院判决训练的模型对普通人问题识别效果较差，但在特定条件下迁移学习有帮助。


<details>
  <summary>Details</summary>
Motivation: 现有法律条文识别多以法院判决事实为输入，而实际应用中，普通人、非专业人士提出的问题形式更为非正式。现有基于普通人问题的数据集和对此的研究很有限，因此有必要构建并研究更贴合实际应用场景的数据和模型。

Method: 作者构建了ILSiC数据集，包含500多个印度法律法规的普通人提问和相应法院判决，并进行了零样本、少样本推理，检索增强生成和有监督微调等多种方法的实验证明。进一步，对不同类别和法规频率进行了细致分析。

Result: 实验发现，仅用法院判决训练的模型在普通人问题上表现不佳，而将法院数据迁移到普通人数据在某些情况下有益。此外，不同问题类别和法规出现频率对识别效果有明显影响。

Conclusion: 论文强调了建立面向普通人问题的法律条文识别数据集的重要性，并指出在模型开发中应充分考虑实际用户问题的差异。同时，迁移学习等多样化方法可为该任务带来新的提升空间。

Abstract: Legal Statute Identification (LSI) for a given situation is one of the most fundamental tasks in Legal NLP. This task has traditionally been modeled using facts from court judgments as input queries, due to their abundance. However, in practical settings, the input queries are likely to be informal and asked by laypersons, or non-professionals. While a few laypeople LSI datasets exist, there has been little research to explore the differences between court and laypeople data for LSI. In this work, we create ILSIC, a corpus of laypeople queries covering 500+ statutes from Indian law. Additionally, the corpus also contains court case judgements to enable researchers to effectively compare between court and laypeople data for LSI. We conducted extensive experiments on our corpus, including benchmarking over the laypeople dataset using zero and few-shot inference, retrieval-augmented generation and supervised fine-tuning. We observe that models trained purely on court judgements are ineffective during test on laypeople queries, while transfer learning from court to laypeople data can be beneficial in certain scenarios. We also conducted fine-grained analyses of our results in terms of categories of queries and frequency of statutes.

</details>


### [325] [EffGen: Enabling Small Language Models as Capable Autonomous Agents](https://arxiv.org/abs/2602.00887)
*Gaurav Srivastava,Aafiya Hussain,Chi Wang,Yingyan Celine Lin,Xuan Wang*

Main category: cs.CL

TL;DR: 该论文介绍了effGen，一个专为小型语言模型（SLM）优化的本地部署智能体框架，具有高效、低成本和安全等优势，并在多个基准测试上超越现有主流框架。


<details>
  <summary>Details</summary>
Motivation: 当前大多数智能体系统依赖大模型API，存在高昂的token成本及隐私风险，制约了其在敏感或资源受限环境中的应用。因此作者希望探索面向SLM的、高效可本地部署的泛用智能体框架。

Method: effGen 框架包含四大技术核心：1）上下文压缩的提示优化工具调用；2）复杂任务的自动分解与调度（并行或串行）；3）基于五因素的复杂度路由机制实现智能预执行决策；4）统一内存系统整合短时、长时与向量存储。同时统一多种智能体协议，实现协议间通信。

Result: 在13个智能体基准测试中，effGen在成功率、执行速度及内存消耗上均优于LangChain、AutoGen和Smolagents。具体实验表明，提示优化对SLM提升更大，复杂度路由对大模型帮助更大，两者结合全部模型均有提升。

Conclusion: effGen作为开源框架，有效降低了智能体系统的成本与门槛，可安全本地部署并兼容不同规模模型，为研究与商业场景提供了灵活高效的平台。

Abstract: Most existing language model agentic systems today are built and optimized for large language models (e.g., GPT, Claude, Gemini) via API calls. While powerful, this approach faces several limitations including high token costs and privacy concerns for sensitive applications. We introduce effGen, an open-source agentic framework optimized for small language models (SLMs) that enables effective, efficient, and secure local deployment (pip install effgen). effGen makes four major contributions: (1) Enhanced tool-calling with prompt optimization that compresses contexts by 70-80% while preserving task semantics, (2) Intelligent task decomposition that breaks complex queries into parallel or sequential subtasks based on dependencies, (3) Complexity-based routing using five factors to make smart pre-execution decisions, and (4) Unified memory system combining short-term, long-term, and vector-based storage. Additionally, effGen unifies multiple agent protocols (MCP, A2A, ACP) for cross-protocol communication. Results on 13 benchmarks show effGen outperforms LangChain, AutoGen, and Smolagents with higher success rates, faster execution, and lower memory. Our results reveal that prompt optimization and complexity routing have complementary scaling behavior: optimization benefits SLMs more (11.2% gain at 1.5B vs 2.4% at 32B), while routing benefits large models more (3.6% at 1.5B vs 7.9% at 32B), providing consistent gains across all scales when combined. effGen (https://effgen.org/) is released under the MIT License, ensuring broad accessibility for research and commercial use. Our framework code is publicly available at https://github.com/ctrl-gaurav/effGen.

</details>


### [326] [Do Schwartz Higher-Order Values Help Sentence-Level Human Value Detection? When Hard Gating Hurts](https://arxiv.org/abs/2602.00913)
*Víctor Yeste,Paolo Rosso*

Main category: cs.CL

TL;DR: 本文研究了在人类价值观检测任务中，是否直接利用Schwartz HO高阶结构能够提升句子级多标签分类的效果。结果发现强行施加HO层级结构不仅无益，甚至有害；而参数调优与轻量级模型集成带来了更实际的提升。


<details>
  <summary>Details</summary>
Motivation: 尽管用Schwartz价值观体系（及其HO高阶类别）作多标签分类的标签集是句子级人类价值观检测中的常用模式，但尚不清楚HO层级结构是否能为任务本身提供有效帮助，因此迫切需要实证研究其实际作用。

Method: 作者在单张8GB GPU的有限计算资源下，使用ValueEval'24/ValuesML等数据集，比较了（1）直接监督的transformer模型，（2）用强制mask实现HO到细粒度值的管道，（3）Presence→HO→values级联体系。同时尝试各种低成本改进（如词典、上下文扩展、主题特征）、标签分阈调优、中小规模LLM（≤10B）、QLoRA和简单集成等。

Result: 实验发现，HO高阶类别从单句中可以学习到，最容易的一对二分对可达Macro-F1约0.58。但硬性HO结构（强制mask）带来叠加误差和召回下降，反而多数情况下损害最终性能。标签阈值调优可显著提升得分（最高+0.05 F1），小型transformer集成也能稳定增益（最高+0.02 F1）。小型LLM作为单一模型效果不及监督encoder，但在异构集成中能补充部分错误。

Conclusion: Schwartz HO结构在描述上有意义，但在句子级人类价值观检测中，强制性地层级约束会带来负面影响。更优的提升方式是对标签做细粒度阈值调优和利用轻量模型集成。

Abstract: Sentence-level human value detection is typically framed as multi-label classification over Schwartz values, but it remains unclear whether Schwartz higher-order (HO) categories provide usable structure. We study this under a strict compute-frugal budget (single 8 GB GPU) on ValueEval'24 / ValuesML (74K English sentences). We compare (i) direct supervised transformers, (ii) HO$\rightarrow$values pipelines that enforce the hierarchy with hard masks, and (iii) Presence$\rightarrow$HO$\rightarrow$values cascades, alongside low-cost add-ons (lexica, short context, topics), label-wise threshold tuning, small instruction-tuned LLM baselines ($\le$10B), QLoRA, and simple ensembles. HO categories are learnable from single sentences (e.g., the easiest bipolar pair reaches Macro-$F_1\approx0.58$), but hard hierarchical gating is not a reliable win: it often reduces end-task Macro-$F_1$ via error compounding and recall suppression. In contrast, label-wise threshold tuning is a high-leverage knob (up to $+0.05$ Macro-$F_1$), and small transformer ensembles provide the most consistent additional gains (up to $+0.02$ Macro-$F_1$). Small LLMs lag behind supervised encoders as stand-alone systems, yet can contribute complementary errors in cross-family ensembles. Overall, HO structure is useful descriptively, but enforcing it with hard gates hurts sentence-level value detection; robust improvements come from calibration and lightweight ensembling.

</details>


### [327] [A Baseline Multimodal Approach to Emotion Recognition in Conversations](https://arxiv.org/abs/2602.00914)
*Víctor Yeste,Rodrigo Rivas-Arévalo*

Main category: cs.CL

TL;DR: 本文提出了一个用于会话情感识别的轻量级多模态基线模型，结合了文本和语音特征，在Friends剧集数据集上进行实验，旨在提供一个易复现的基准实现，并报告了基线性能。


<details>
  <summary>Details</summary>
Motivation: 当前会话情感识别任务对多模态方法日益关注，但缺乏简单、易用的基线实现来辅助研究和公平比较，本文旨在填补这个空白。

Method: 方法包括：1）基于transformer的文本分类模型；2）自监督语音特征表征模型；3）通过后期融合技术将二者结合，实现多模态情感识别。

Result: 在SemEval-2024 Task 3 Friends数据集上进行了有限训练实验，结果显示在部分场景下多模态融合优于单一模态模型。

Conclusion: 本文提供了详细的基线配置和实验结果，有助于今后相关领域的研究和更严格的对比实验，提升透明度和研究的可复现性。

Abstract: We present a lightweight multimodal baseline for emotion recognition in conversations using the SemEval-2024 Task 3 dataset built from the sitcom Friends. The goal of this report is not to propose a novel state-of-the-art method, but to document an accessible reference implementation that combines (i) a transformer-based text classifier and (ii) a self-supervised speech representation model, with a simple late-fusion ensemble. We report the baseline setup and empirical results obtained under a limited training protocol, highlighting when multimodal fusion improves over unimodal models. This preprint is provided for transparency and to support future, more rigorous comparisons.

</details>


### [328] [Neural FOXP2 -- Language Specific Neuron Steering for Targeted Language Improvement in LLMs](https://arxiv.org/abs/2602.00945)
*Anusa Saha,Tanmay Joshi,Vinija Jain,Aman Chadha,Amitava Das*

Main category: cs.CL

TL;DR: 本文提出通过识别和操控大语言模型中与语言相关的特定神经元（称为“语言神经元”），实现非英语语言（如印地语、西班牙语）作为默认输出语言。作者提出了“Neural FOXP2”方法，利用低秩控制与稀疏激活，实现对模型语言偏好机制的定向微调。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在多语言任务中以英语为主，其他语言能力虽存在，但倾向被压制，模型对英语有默认偏好。因此，研究如何让模型以其他语言为主要输出，有助于提升多语言公平性及实用性。

Method: 提出“Neural FOXP2”方法，包括三步：(1)定位：通过训练“稀疏自动编码器”(SAE)分解各层激活，识别对目标语言（印地语/西班牙语）选择性明显的神经元；(2)方向提取：通过奇异值分解等低秩谱分析，提取英文与目标语言之间显著的神经方向及适合干预的网络窗口；(3)操控：在这些方向上对特定语言神经元施以激活调整，实现模型语言默认的切换。

Result: 该方法能够高效、稳定地将模型的主要输出语言从英语转为印地语或西班牙语，且干预过程具有可控性和安全性。通过实验验证了选定方向与神经元干预的有效性。

Conclusion: 通过机制性方法定向识别和操控大模型中的语言神经元，可以安全且有效地设定模型的默认输出语言，这为多语言模型的公平和可控研究提供了新思路。

Abstract: LLMs are multilingual by training, yet their lingua franca is often English, reflecting English language dominance in pretraining. Other languages remain in parametric memory but are systematically suppressed. We argue that language defaultness is governed by a sparse, low-rank control circuit, language neurons, that can be mechanistically isolated and safely steered.
  We introduce Neural FOXP2, that makes a chosen language (Hindi or Spanish) primary in a model by steering language-specific neurons. Neural FOXP2 proceeds in three stages: (i) Localize: We train per-layer SAEs so each activation decomposes into a small set of active feature components. For every feature, we quantify English vs. Hindi/Spanish selectivity overall logit-mass lift toward the target-language token set. Tracing the top-ranked features back to their strongest contributing units yields a compact language-neuron set. (ii) Steering directions: We localize controllable language-shift geometry via a spectral low-rank analysis. For each layer, we build English to target activation-difference matrices and perform layerwise SVD to extract the dominant singular directions governing language change. The eigengap and effective-rank spectra identify a compact steering subspace and an empirically chosen intervention window (where these directions are strongest and most stable). (iii) Steer: We apply a signed, sparse activation shift targeted to the language neurons. Concretely, within low to mid layers we add a positive steering along the target-language dominant directions and a compensating negative shift toward the null space for the English neurons, yielding controllable target-language defaultness.

</details>


### [329] [Verification Required: The Impact of Information Credibility on AI Persuasion](https://arxiv.org/abs/2602.00970)
*Saaduddin Mahmud,Eugene Bagdasarian,Shlomo Zilberstein*

Main category: cs.CL

TL;DR: 该论文提出并研究了一个新模型MixTalk，以模拟和分析LLM之间具有概率性可信度的信息交流，并提出了一种改进的策略学习方法，有效提高了接收端的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的智能体在高风险决策中被广泛应用，而现实环境中信息的可信度介于完全可验证和不可验证之间，现有研究未能有效覆盖这类概率性可信的沟通场景。作者为填补这一空白，推动多智能体系统中战略性沟通的理论和实践进展。

Method: 作者提出了MixTalk沟通博弈：一个发送者智能体结合可验证与不可验证的信息进行传递，接收者智能体则在有限验证预算下，对信息选择性验证并据以推断事实。作者利用LLM作为智能体，在多种现实部署场景下开展大规模比赛。同时提出了Tournament Oracle Policy Distillation（TOPD）方法，通过离线学习互动记录中最优策略，并用于推理阶段以提升接收方性能。

Result: 实验证明，主流LLM在信息概率可信度推理和策略性沟通中的表现有优势也有不足。TOPD方法显著提升了接收方对劝服性信息的鲁棒性，表现优于未蒸馏策略。

Conclusion: 该研究丰富了战略性沟通智能体的理论框架，为真实环境部署LLM agent提供了新工具和评估标准，并提出的TOPD方法对提高沟通鲁棒性具有实际应用意义。

Abstract: Agents powered by large language models (LLMs) are increasingly deployed in settings where communication shapes high-stakes decisions, making a principled understanding of strategic communication essential. Prior work largely studies either unverifiable cheap-talk or fully verifiable disclosure, failing to capture realistic domains in which information has probabilistic credibility. We introduce MixTalk, a strategic communication game for LLM-to-LLM interaction that models information credibility. In MixTalk, a sender agent strategically combines verifiable and unverifiable claims to communicate private information, while a receiver agent allocates a limited budget to costly verification and infers the underlying state from prior beliefs, claims, and verification outcomes. We evaluate state-of-the-art LLM agents in large-scale tournaments across three realistic deployment settings, revealing their strengths and limitations in reasoning about information credibility and the explicit behavior that shapes these interactions. Finally, we propose Tournament Oracle Policy Distillation (TOPD), an offline method that distills tournament oracle policy from interaction logs and deploys it in-context at inference time. Our results show that TOPD significantly improves receiver robustness to persuasion.

</details>


### [330] [Trust in One Round: Confidence Estimation for Large Language Models via Structural Signals](https://arxiv.org/abs/2602.00977)
*Pengyue Yang,Jiawen Wen,Haolin Jin,Linghan Huang,Huaming Chen,Ling Chen*

Main category: cs.CL

TL;DR: 本文提出了一种新的结构化置信度（Structural Confidence）框架，通过分析大模型最后一层隐藏态轨迹的多尺度结构信号，提高了输出正确性的预测能力。该方法跨多个高社会成本领域和基准测试显示出比传统方法更强的性能，且无需多次采样。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在高风险领域广泛应用，但传统置信度估计方法（如基于概率、语义相似性、多样本一致性）在分布漂移、专业文本和算力受限场景下表现不稳健，迫切需要更高效、准确的置信度估计方法。

Method: 提出一个模型无关的Structural Confidence框架，通过特征转换提取最后一层隐藏状态在时间轴上的频谱、局部变化、全局形状等结构性描述符，捕捉内部稳定性信息，实现单次正向推理即可输出置信度评估。

Result: 在FEVER、SciFact、WikiBio-hallucination和TruthfulQA等数据集上，该结构化置信度方法在AUROC和AUPR指标上均优于传统主流置信度估计基线。

Conclusion: 结构化置信度框架在无需多次采样和辅助模型的前提下，实现了高效、鲁棒的置信度评估方法，适用于社会影响大且资源受限的大模型应用场景。

Abstract: Large language models (LLMs) are increasingly deployed in domains where errors carry high social, scientific, or safety costs. Yet standard confidence estimators, such as token likelihood, semantic similarity and multi-sample consistency, remain brittle under distribution shift, domain-specialised text, and compute limits. In this work, we present Structural Confidence, a single-pass, model-agnostic framework that enhances output correctness prediction based on multi-scale structural signals derived from a model's final-layer hidden-state trajectory. By combining spectral, local-variation, and global shape descriptors, our method captures internal stability patterns that are missed by probabilities and sentence embeddings. We conduct extensive, cross-domain evaluation across four heterogeneous benchmarks-FEVER (fact verification), SciFact (scientific claims), WikiBio-hallucination (biographical consistency), and TruthfulQA (truthfulness-oriented QA). Our Structural Confidence framework demonstrates strong performance compared with established baselines in terms of AUROC and AUPR. More importantly, unlike sampling-based consistency methods which require multiple stochastic generations and an auxiliary model, our approach uses a single deterministic forward pass, offering a practical basis for efficient, robust post-hoc confidence estimation in socially impactful, resource-constrained LLM applications.

</details>


### [331] [MedSpeak: A Knowledge Graph-Aided ASR Error Correction Framework for Spoken Medical QA](https://arxiv.org/abs/2602.00981)
*Yutong Song,Shiva Shrestha,Chenhan Lyu,Elahe Khatibi,Pengfei Zhang,Honghui Xu,Nikil Dutt,Amir Rahmani*

Main category: cs.CL

TL;DR: MedSpeak是一个结合医学知识图和大语言模型的ASR纠错框架，有效提升了医学口语问答系统中医学术语的识别和整体表现。


<details>
  <summary>Details</summary>
Motivation: 现有的口语问答系统依赖自动语音识别（ASR）技术，但在医学领域，ASR对专业术语的识别准确率较低，影响后续问答的效果。为了解决这一关键问题，作者提出引入医学知识进行辅助。

Method: 提出名为MedSpeak的系统，结合医学知识图谱中的语义关系和发音信息，以及大语言模型的推理能力，对ASR输出的错误医学术语进行纠正，从而优化后续的问答任务。

Result: 在多项医学问答评测基准上，MedSpeak显著提高了医学术语识别准确性和整体SQA系统效果，在业内达到领先表现。

Conclusion: 知识图辅助的大语言模型方法可以有效补足ASR在医学领域的局限，MedSpeak目前是医疗口语问答领域的最新最优解。

Abstract: Spoken question-answering (SQA) systems relying on automatic speech recognition (ASR) often struggle with accurately recognizing medical terminology. To this end, we propose MedSpeak, a novel knowledge graph-aided ASR error correction framework that refines noisy transcripts and improves downstream answer prediction by leveraging both semantic relationships and phonetic information encoded in a medical knowledge graph, together with the reasoning power of LLMs. Comprehensive experimental results on benchmarks demonstrate that MedSpeak significantly improves the accuracy of medical term recognition and overall medical SQA performance, establishing MedSpeak as a state-of-the-art solution for medical SQA. The code is available at https://github.com/RainieLLM/MedSpeak.

</details>


### [332] [DISPO: Enhancing Training Efficiency and Stability in Reinforcement Learning for Large Language Model Mathematical Reasoning](https://arxiv.org/abs/2602.00983)
*Batuhan K. Karaman,Aditya Rawal,Suhaila Shakiah,Mohammad Ghavamzadeh,Mingyi Hong,Arijit Biswas,Ruida Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种新型REINFORCE方法DISPO，通过分离正确和错误响应的剪裁，改善了大模型在可验证奖励环境下强化学习的稳定性和效率。DISPO在数学推理等任务上取得了优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 现有可验证奖励的强化学习方法存在效率与稳定性的权衡：PPO风格方法虽然稳定但学习慢，REINFORCE风格方法学习快却不稳定。需要新方法同时兼顾两者优势。

Method: DISPO算法将重要性采样权重的上剪裁和下剪裁分别作用于正确和错误响应，实现了四种可控的策略更新机制，并通过调优各剪裁参数，平衡探索与知识蒸馏，防止灾难性失败。

Result: 在AIME'24数学竞赛等基准测试中，DISPO获得了最高分数（61.04%），显著优于CISPO（55.42%）和DAPO（50.21%），其它模型和数据集也有类似提升。

Conclusion: DISPO在保证强化学习效率的同时，大幅提升了大模型在数学等领域推理任务的训练稳定性和最终性能，可作为可验证奖励RL领域的新范式。

Abstract: Reinforcement learning with verifiable rewards has emerged as a promising paradigm for enhancing the reasoning capabilities of large language models particularly in mathematics. Current approaches in this domain present a clear trade-off: PPO-style methods (e.g., GRPO/DAPO) offer training stability but exhibit slow learning trajectories due to their trust-region constraints on policy updates, while REINFORCE-style approaches (e.g., CISPO) demonstrate improved learning efficiency but suffer from performance instability as they clip importance sampling weights while still permitting non-zero gradients outside the trust-region. To address these limitations, we introduce DISPO, a simple yet effective REINFORCE-style algorithm that decouples the up-clipping and down-clipping of importance sampling weights for correct and incorrect responses, yielding four controllable policy update regimes. Through targeted ablations, we uncover how each regime impacts training: for correct responses, weights >1 increase the average token entropy (i.e., exploration) while weights <1 decrease it (i.e., distillation) -- both beneficial but causing gradual performance degradation when excessive. For incorrect responses, overly restrictive clipping triggers sudden performance collapse through repetitive outputs (when weights >1) or vanishing response lengths (when weights <1). By separately tuning these four clipping parameters, DISPO maintains the exploration-distillation balance while preventing catastrophic failures, achieving 61.04% on AIME'24 (vs. 55.42% CISPO and 50.21% DAPO) with similar gains across various benchmarks and models.

</details>


### [333] [Sparse Reward Subsystem in Large Language Models](https://arxiv.org/abs/2602.00986)
*Guowei Xu,Mert Yuksekgonul,James Zou*

Main category: cs.CL

TL;DR: 本论文在大语言模型（LLMs）隐藏状态中发现了类似生物大脑奖励系统的稀疏奖励子系统，并识别出了具备价值预期功能的神经元及其对推理能力的重要性。还发现了与奖励预测误差（RPE）相关的神经元。


<details>
  <summary>Details</summary>
Motivation: 动机在于理解和揭示大语言模型内部的奖励机制，特别是模型为何能产生与奖励相关、类人认知的行为，以及找到与生物大脑奖励系统的对照。

Method: 作者通过分析LLM隐藏状态，标定出了代表期望值的“价值神经元”，并利用干预实验验证这些神经元对推理任务的重要性。同时跨数据集、多种模型与架构，系统性考察这些神经元的稳健性与可迁移性；还对价值预测与实际奖励不一致的情形，进一步定位出负责RPE（奖励预测误差）的“多巴胺神经元”。

Result: 发现LLM内部确实存在类似生物奖励系统的价值/多巴胺神经元，这些神经元不仅重要且在不同模型、数据集、规模下表现出较强稳健性和可迁移性，且能分辨奖励预测误差。

Conclusion: 大语言模型内部存在类生物奖励子系统，该子系统对推理等高级认知任务发挥重要作用。这些发现为理解人工智能模型的类脑机制及其泛化能力提供了新视角。

Abstract: In this paper, we identify a sparse reward subsystem within the hidden states of Large Language Models (LLMs), drawing an analogy to the biological reward subsystem in the human brain. We demonstrate that this subsystem contains value neurons that represent the model's internal expectation of state value, and through intervention experiments, we establish the importance of these neurons for reasoning. Our experiments reveal that these value neurons are robust across diverse datasets, model scales, and architectures; furthermore, they exhibit significant transferability across different datasets and models fine-tuned from the same base model. By examining cases where value predictions and actual rewards diverge, we identify dopamine neurons within the reward subsystem which encode reward prediction errors (RPE). These neurons exhibit high activation when the reward is higher than expected and low activation when the reward is lower than expected.

</details>


### [334] [DeALOG: Decentralized Multi-Agents Log-Mediated Reasoning Framework](https://arxiv.org/abs/2602.00996)
*Abhijit Chakraborty,Ashish Raj Shekhar,Shiven Agarwal,Vivek Gupta*

Main category: cs.CL

TL;DR: DeALOG提出了一种去中心化多智能体框架，通过共享自然语言日志协作，提升复杂多模态问答系统的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前复杂问答任务需要整合文本、表格和图像等多种信息源，传统方法在可扩展性和可解释性上存在局限，需要新的架构提升合作与错误检测能力。

Method: DeALOG设计了多个专门化智能体（处理表格、上下文、视觉、摘要和验证），所有代理通过一个持久自然语言日志进行交互，共享信息和检测错误，实现无中心化协作。

Result: 在FinQA、TAT-QA、CRT-QA、WikiTableQuestions、FeTaQA和MultiModalQA等数据集上取得了有竞争力的表现。分析表明共享日志、智能体专门化和校验机制显著有助于提高准确率。

Conclusion: DeALOG通过模块化设计和自然语言通信，实现了可扩展的多模态问答处理，为多源信息整合与解释提供了一种有效方法。

Abstract: Complex question answering across text, tables and images requires integrating diverse information sources. A framework supporting specialized processing with coordination and interpretability is needed. We introduce DeALOG, a decentralized multi-agent framework for multimodal question answering. It uses specialized agents: Table, Context, Visual, Summarizing and Verification, that communicate through a shared natural-language log as persistent memory. This log-based approach enables collaborative error detection and verification without central control, improving robustness. Evaluations on FinQA, TAT-QA, CRT-QA, WikiTableQuestions, FeTaQA, and MultiModalQA show competitive performance. Analysis confirms the importance of the shared log, agent specialization, and verification for accuracy. DeALOG, provides a scalable approach through modular components using natural-language communication.

</details>


### [335] [Reliable Use of Lemmas via Eligibility Reasoning and Section$-$Aware Reinforcement Learning](https://arxiv.org/abs/2602.00998)
*Zhikun Xu,Xiaodong Yu,Ben Zhou,Jiang Liu,Jialian Wu,Ze Wang,Ximeng Sun,Hao Chen,Zicheng Liu*

Main category: cs.CL

TL;DR: 本文针对大语言模型在数学任务中经常错误应用引理的问题，提出了RULES方法，通过结构化输出和部分感知的强化学习，有效提升了模型判断引理适用性的能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在数学基准上表现出色，但经常出现引用引理时不检查前提条件，从而产生推理错误。因此需要一种机制来确保模型在推理时能正确判断引理的适用性。

Method: 作者将引理判断建模为一个结构化预测任务，要求模型对候选引理分别输出前提检查和结论实用性检查，再结合两者决定引理是否可用。提出的RULES方法采用两段式输出结构，并引入部分感知的强化学习损失来对错误部分进行惩罚。训练和测试分别涵盖自然语言与形式化证明语料，并在多个LLM上评估鲁棒性及端到端效果。

Result: 与基础模型和单标签强化学习基线比较，RULES方案在原域、破坏适用性的扰动任务上表现出明显提升，在端到端任务上亦达到同等或略有提升。消融实验表明，两段式结构和部分感知强化学习对于鲁棒性均不可或缺。

Conclusion: RULES方法能够显著提升大语言模型在数学推理中正确判断引理适用性的能力，提高了整体推理质量，其结构化输出和模块化监督方式对于模型鲁棒性尤为重要。

Abstract: Recent large language models (LLMs) perform strongly on mathematical benchmarks yet often misapply lemmas, importing conclusions without validating assumptions. We formalize lemma$-$judging as a structured prediction task: given a statement and a candidate lemma, the model must output a precondition check and a conclusion$-$utility check, from which a usefulness decision is derived. We present RULES, which encodes this specification via a two$-$section output and trains with reinforcement learning plus section$-$aware loss masking to assign penalty to the section responsible for errors. Training and evaluation draw on diverse natural language and formal proof corpora; robustness is assessed with a held$-$out perturbation suite; and end$-$to$-$end evaluation spans competition$-$style, perturbation$-$aligned, and theorem$-$based problems across various LLMs. Results show consistent in$-$domain gains over both a vanilla model and a single$-$label RL baseline, larger improvements on applicability$-$breaking perturbations, and parity or modest gains on end$-$to$-$end tasks; ablations indicate that the two$-$section outputs and section$-$aware reinforcement are both necessary for robustness.

</details>


### [336] [Distilling Token-Trained Models into Byte-Level Models](https://arxiv.org/abs/2602.01007)
*Zishuo Bao,Jiaqi Leng,Junxiong Wang,Bowen Peng,Yucheng Lu*

Main category: cs.CL

TL;DR: 本文提出了一种高效的知识蒸馏方法，可以将已有的基于token训练的大语言模型转化为强大的字节级语言模型（BLM），避免了从零开始训练而需耗费巨大的计算资源。实验表明，该方法只需约1250亿字节的数据即可达到与原始模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 目前的BLM虽然具有优秀的可扩展性，但需要从头训练，开销极大。作者希望提出一种更经济的方法，将已经训练好的token级模型转为BLM，充分利用现有资源。

Method: 提出了两阶段课程式蒸馏方法：第一阶段为逐步的知识蒸馏，通过对齐token模型与字节表示的嵌入；第二阶段为在字节空间的有监督微调，实现端到端的字节级生成。

Result: 在包括Llama、Qwen和OLMo等多个模型上进行了验证，结果显示用约1250亿字节数据训练出的BLM基本保留了教师模型的性能。

Conclusion: 通过该方法，可以高效地将token级大模型转换成功能完备的BLM，为字节级语言模型的扩展和应用提供了经济实用的新范式。

Abstract: Byte Language Models (BLMs) have emerged as a promising direction for scaling language models beyond tokenization. However, existing BLMs typically require training from scratch on trillions of bytes, making them prohibitively expensive. In this paper, we propose an efficient distillation recipe that converts existing token-trained LLMs into BLMs while retaining comparable capabilities. Our recipe follows a two-stage curriculum: (1) Progressive Knowledge Distillation, which aligns byte-level representations with the embeddings of the token-trained teacher model; and (2) Byte-Level Supervised Fine-Tuning, which enables end-to-end generation entirely in the byte space. We validate our approach across multiple model families, including Llama, Qwen, and OLMo, and demonstrate that the distilled BLMs retain most of the teacher models' performance using only approximately 125B bytes.

</details>


### [337] [Large Language Models as Students Who Think Aloud: Overly Coherent, Verbose, and Confident](https://arxiv.org/abs/2602.01015)
*Conrad Borchers,Jill-Jênn Vie,Roger Azevedo*

Main category: cs.CL

TL;DR: LLMs在AI辅导系统中不能完全准确模拟新手的推理与元认知，其推理比真实学生更连贯、冗长和一致，低估了人类学习的碎片化特点。


<details>
  <summary>Details</summary>
Motivation: 随着LLM被广泛应用于教育辅导，理解其是否能真实反映新手学习者的思维过程尤为重要，因为现有评估过于侧重解题准确性，忽略了人的不完全推理特性。

Method: 作者使用了630条化学多步问题的“想声说出”语料，结合学生使用提示和尝试等解题日志，将LLM（如GPT-4.1）在不同提示场景下生成的推理与真实学生的表达进行对比，并分析其对学生逐步解题表现的预测能力。

Result: GPT-4.1在生成推理时，比真实学生更连贯、啰嗦且表达多样性较低，尤以提示信息更丰富时为甚。同时，模型普遍高估了学生表现，没有反映出新手常见的碎片化和易变性。

Conclusion: LLMs难以忠实模拟学习者的推理和认知过程，这种差异源自其训练数据更偏向专家答案且缺乏情感和记忆限制的表达。评估框架有助于未来更好地设计支持新手学习与自我调节的自适应AI系统。

Abstract: Large language models (LLMs) are increasingly embedded in AI-based tutoring systems. Can they faithfully model novice reasoning and metacognitive judgments? Existing evaluations emphasize problem-solving accuracy, overlooking the fragmented and imperfect reasoning that characterizes human learning. We evaluate LLMs as novices using 630 think-aloud utterances from multi-step chemistry tutoring problems with problem-solving logs of student hint use, attempts, and problem context. We compare LLM-generated reasoning to human learner utterances under minimal and extended contextual prompting, and assess the models' ability to predict step-level learner success. Although GPT-4.1 generates fluent and contextually appropriate continuations, its reasoning is systematically over-coherent, verbose, and less variable than human think-alouds. These effects intensify with a richer problem-solving context during prompting. Learner performance was consistently overestimated. These findings highlight epistemic limitations of simulating learning with LLMs. We attribute these limitations to LLM training data, including expert-like solutions devoid of expressions of affect and working memory constraints during problem solving. Our evaluation framework can guide future design of adaptive systems that more faithfully support novice learning and self-regulation using generative artificial intelligence.

</details>


### [338] [Bias in the Ear of the Listener: Assessing Sensitivity in Audio Language Models Across Linguistic, Demographic, and Positional Variations](https://arxiv.org/abs/2602.01030)
*Sheng-Lun Wei,Yu-Ling Liao,Yen-Hua Chang,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.CL

TL;DR: 本文首次系统性地研究了多语种多模态大模型（MLLMs）中的语音偏见，并构建了BiasInEar数据集来评估模型在不同语音条件下的公平性。


<details>
  <summary>Details</summary>
Motivation: 随着语音融合进多模态大模型，模型在不同语言、口音、性别等维度上的公平性问题日益突出。本研究旨在填补多语种多模态模型中语音偏见系统性研究的空白。

Method: 作者构建了BiasInEar语音数据集，覆盖英语、中文和韩语，且按性别和口音均衡，总计70.8小时、11200道题；采用准确率、熵、APES和Fleiss' κ等四种指标，考察九种代表性模型在语言、性别、口音、选项顺序扰动下的表现。

Result: 结果发现，MLLMs对性别等人口统计属性较为鲁棒，但对语言和选项顺序敏感，表明语音会放大结构性偏见。此外，模型架构设计与推理策略会显著影响其在多语言任务中的鲁棒性。

Conclusion: 研究建立了一个统一的评测框架，实现了对语音融合大模型的公平性和鲁棒性评估，对比文本与语音输入间模型表现，促进后续MLLMs公平性研究。数据与代码已开源。

Abstract: This work presents the first systematic investigation of speech bias in multilingual MLLMs. We construct and release the BiasInEar dataset, a speech-augmented benchmark based on Global MMLU Lite, spanning English, Chinese, and Korean, balanced by gender and accent, and totaling 70.8 hours ($\approx$4,249 minutes) of speech with 11,200 questions. Using four complementary metrics (accuracy, entropy, APES, and Fleiss' $κ$), we evaluate nine representative models under linguistic (language and accent), demographic (gender), and structural (option order) perturbations. Our findings reveal that MLLMs are relatively robust to demographic factors but highly sensitive to language and option order, suggesting that speech can amplify existing structural biases. Moreover, architectural design and reasoning strategy substantially affect robustness across languages. Overall, this study establishes a unified framework for assessing fairness and robustness in speech-integrated LLMs, bridging the gap between text- and speech-based evaluation. The resources can be found at https://github.com/ntunlplab/BiasInEar.

</details>


### [339] [Personality Expression Across Contexts: Linguistic and Behavioral Variation in LLM Agents](https://arxiv.org/abs/2602.01063)
*Bin Han,Deuksin Kwon,Jonathan Gratch*

Main category: cs.CL

TL;DR: 本研究探讨了相同的人格设定在不同对话场景下，LLM表现出不同的语言、行为和情绪特征，揭示了人格表达的情境敏感性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型可以通过明确的人格提示词进行个性化设定，但实际的人格行为表现会因具体场景而有所不同。理解这种变异的原因和表现，有助于优化对话型人工智能的设计。

Method: 作者设定统一的人格提示词，并在寒暄、谈判、小组决策与共情四种对话任务中，系统分析LLM的语言、行为和情绪输出差异。

Result: 实验结果显示，情境线索显著影响了LLM的人格表达和情绪基调；相同的人格通过不同的社会和情感任务表现出行为和语言的变化。

Conclusion: LLM的人格表达不是固定或不一致的，而是根据不同情境做出人类般的灵活适应。这一发现符合全人格理论，提示AI可实现更社会化、情感化的互动表现。

Abstract: Large Language Models (LLMs) can be conditioned with explicit personality prompts, yet their behavioral realization often varies depending on context. This study examines how identical personality prompts lead to distinct linguistic, behavioral, and emotional outcomes across four conversational settings: ice-breaking, negotiation, group decision, and empathy tasks. Results show that contextual cues systematically influence both personality expression and emotional tone, suggesting that the same traits are expressed differently depending on social and affective demands. This raises an important question for LLM-based dialogue agents: whether such variations reflect inconsistency or context-sensitive adaptation akin to human behavior. Viewed through the lens of Whole Trait Theory, these findings highlight that LLMs exhibit context-sensitive rather than fixed personality expression, adapting flexibly to social interaction goals and affective conditions.

</details>


### [340] [Exploring Knowledge Purification in Multi-Teacher Knowledge Distillation for LLMs](https://arxiv.org/abs/2602.01064)
*Ruihan Jin,Pengpeng Shao,Zhengqi Wen,Jinyang Wu,Mingkuan Feng,Shuo Yang,Chu Yuan Zhang,Jianhua Tao*

Main category: cs.CL

TL;DR: 本文提出了一种称为“知识纯化”的新方法，通过整合多个教师LLM的解释，解决多教师知识蒸馏中存在的知识冲突和资源消耗高的问题，并验证了多种纯化方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法在采用多教师模型时，容易导致知识冲突，并且计算资源需求较高，限制了高性能小模型的落地与应用，因此需要新的方法提升多教师蒸馏的效果与效率。

Method: 提出“知识纯化”概念，将多个教师LLM的推理过程融合为单一的有效解释，来减少知识冲突。进一步，设计了五种不同视角的纯化方法进行实验，包括基于路由的技术等。

Result: 实验结果显示，提出的纯化方法提升了蒸馏模型的性能，有效减少了知识冲突。其中，基于路由的方法在泛化能力上表现突出。

Conclusion: 知识纯化技术能优化多教师知识蒸馏流程，不仅提高了学生模型的性能与效率，还增强了模型部署的可行性，证明了创新纯化技术的应用潜力。

Abstract: Knowledge distillation has emerged as a pivotal technique for transferring knowledge from stronger large language models (LLMs) to smaller, more efficient models. However, traditional distillation approaches face challenges related to knowledge conflicts and high resource demands, particularly when leveraging multiple teacher models. In this paper, we introduce the concept of \textbf{Knowledge Purification}, which consolidates the rationales from multiple teacher LLMs into a single rationale, thereby mitigating conflicts and enhancing efficiency. To investigate the effectiveness of knowledge purification, we further propose five purification methods from various perspectives. Our experiments demonstrate that these methods not only improve the performance of the distilled model but also effectively alleviate knowledge conflicts. Moreover, router-based methods exhibit robust generalization capabilities, underscoring the potential of innovative purification techniques in optimizing multi-teacher distillation and facilitating the practical deployment of powerful yet lightweight models.

</details>


### [341] [From Utterance to Vividity: Training Expressive Subtitle Translation LLM via Adaptive Local Preference Optimization](https://arxiv.org/abs/2602.01068)
*Chaoqun Cui,Shijing Wang,Liangbin Huang,Qingqing Gu,Zhaolong Huang,Xiao Zeng,Wenji Mao*

Main category: cs.CL

TL;DR: 论文提出了一种针对视觉媒体字幕翻译的定制大型语言模型（LLM）训练方法，并引入了ALPO优化策略，提升了模型在细粒度翻译偏好对齐和表达性方面的能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs已广泛提升机器翻译能力，但在复杂垂直领域（如字幕翻译）面临表达不够生动、细节适应性差等问题。论文旨在提升LLMs对特定域如字幕翻译的定制化和表现力。

Method: 作者构建并发布了多语向字幕平行语料库，提出了自适应局部偏好优化（ALPO）方法，实现翻译LLM在表达性和细粒度偏好对齐方面的优化。并验证LLM作为翻译奖励模型和评估器的有效性。

Result: 实验结果显示，引入ALPO的翻译LLM在多维度翻译质量评估中均取得了优异表现，能有效适应不同领域下直译与意译的需求。

Conclusion: ALPO优化方法显著增强了LLM在视觉媒体字幕领域的定制化翻译能力，为垂直领域翻译提供了实用高效的新思路。

Abstract: The rapid development of Large Language Models (LLMs) has significantly enhanced the general capabilities of machine translation. However, as application scenarios become more complex, the limitations of LLMs in vertical domain translations are gradually becoming apparent. In this study, we focus on how to construct translation LLMs that meet the needs of domain customization. We take visual media subtitle translation as our topic and explore how to train expressive and vivid translation LLMs. We investigated the situations of subtitle translation and other domains of literal and liberal translation, verifying the reliability of LLM as reward model and evaluator for translation. Additionally, to train an expressive translation LLM, we constructed and released a multidirectional subtitle parallel corpus dataset and proposed the Adaptive Local Preference Optimization (ALPO) method to address fine-grained preference alignment. Experimental results demonstrate that ALPO achieves outstanding performance in multidimensional evaluation of translation quality.

</details>


### [342] [What If We Allocate Test-Time Compute Adaptively?](https://arxiv.org/abs/2602.01070)
*Ahsan Bilal,Ahmed Mohsin,Muhammad Umer,Ali Subhan,Hassan Rizwan,Ayesha Mohsin,Dean Hougen*

Main category: cs.CL

TL;DR: 本文提出了一种基于验证器引导的自适应推理框架，通过过程奖励模型动态引导推理路径的生成和选择，大幅提升了在复杂推理任务中的推理质量与效率。


<details>
  <summary>Details</summary>
Motivation: 传统的推理计算资源分配方式，如均匀分配和固定采样，不够高效且缺乏针对性，难以充分利用有限计算资源，特别是在复杂任务或高难度基准上效果有限。本文旨在通过自适应策略，高效利用计算资源，提升复杂推理任务的准确率和效率。

Method: 该框架将推理建模为多次迭代生成和选择推理轨迹。在每次迭代中，智能体可能会生成高层次计划，选择推理工具和计算策略，并生成候选推理路径。过程奖励模型（PRM）作为统一控制信号，在迭代内聚合步级分数进行剪枝和扩展，在迭代间用聚合的轨迹奖励选择最终答案。

Result: 在多项数据集测试中，提出的方法较传统直接扩展推理计算方式有显著提升。在MATH-500、AIME24和AMO-Bench等高难度基准测试中，取得了较大性能增益，尤其在更复杂的任务上有数倍提升。

Conclusion: 验证器引导的自适应推理分配显著提升了高难度推理任务的精度并提高了计算效率，避免了计算资源的浪费，有效将计算集中在高价值的推理路径上。

Abstract: Test-time compute scaling allocates inference computation uniformly, uses fixed sampling strategies, and applies verification only for reranking. In contrast, we propose a verifier-guided adaptive framework treating reasoning as iterative trajectory generation and selection. For each problem, the agent runs multiple inference iterations. In each iteration, it optionally produces a high-level plan, selects a set of reasoning tools and a compute strategy together with an exploration parameter, and then generates a candidate reasoning trajectory. A process reward model (PRM) serves as a unified control signal: within each iteration, step-level PRM scores are aggregated to guide pruning and expansion during generation, and across iterations, aggregated trajectory rewards are used to select the final response. Across datasets, our dynamic, PRM-guided approach consistently outperforms direct test-time scaling, yielding large gains on MATH-500 and several-fold improvements on harder benchmarks such as AIME24 and AMO-Bench. We characterize efficiency using theoretical FLOPs and a compute intensity metric penalizing wasted generation and tool overhead, demonstrating that verification-guided allocation concentrates computation on high-utility reasoning paths.

</details>


### [343] [Logic-Oriented Retriever Enhancement via Contrastive Learning](https://arxiv.org/abs/2602.01116)
*Wenxuan Zhang,Yuan-Hao Jiang,Changyong Qi,Rui Jia,Yonghe Wu*

Main category: cs.CL

TL;DR: 本文提出了一种名为LORE的新型检索器增强方法，通过细粒度对比学习激活大型语言模型的逻辑分析能力，解决了模型在知识密集型任务中检索依赖表面相似性的局限性，无需额外资源即可提升召回与生成效果。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在知识密集型任务中，检索器常因过度依赖表面相似性而难以处理涉及复杂逻辑关系的查询。本文旨在激发模型内在的逻辑分析能力，提升其在这类任务中的表现。

Method: 提出LORE（Logic ORiented Retriever Enhancement），通过细粒度对比学习，引导模型表示向符合逻辑结构的证据靠拢，而非仅仅依赖表面相似。LORE不需要额外监督、外部资源或预检索分析，并可直接兼容现有索引。

Result: LORE在提升检索质量和下游生成任务表现上均有显著的提升效果，同时保持了高效性。实验结果表明该方法对多种数据集均适用。

Conclusion: LORE有效挖掘和增强了大型语言模型的逻辑能力，在无需额外资源的情况下，提高了知识密集型任务的检索与生成性能，具有实用性和推广性。

Abstract: Large language models (LLMs) struggle in knowledge-intensive tasks, as retrievers often overfit to surface similarity and fail on queries involving complex logical relations. The capacity for logical analysis is inherent in model representations but remains underutilized in standard training. LORE (Logic ORiented Retriever Enhancement) introduces fine-grained contrastive learning to activate this latent capacity, guiding embeddings toward evidence aligned with logical structure rather than shallow similarity. LORE requires no external upervision, resources, or pre-retrieval analysis, remains index-compatible, and consistently improves retrieval utility and downstream generation while maintaining efficiency. The datasets and code are publicly available at https://github.com/mazehart/Lore-RAG.

</details>


### [344] [Tendem: A Hybrid AI+Human Platform](https://arxiv.org/abs/2602.01119)
*Konstantin Chernyshev,Ekaterina Artemova,Viacheslav Zhukov,Maksim Nerush,Mariia Fedorova,Iryna Repik,Olga Shapovalova,Aleksey Sukhorosov,Vladimir Dobrovolskii,Natalia Mikhailova,Sergei Tilga*

Main category: cs.CL

TL;DR: Tendem是一个结合AI和人类专家的混合系统，能在保证高质量的同时提高效率，经验证优于单一AI或人工流程。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在遇到复杂任务或出错时表现有限，完全依赖人类操作又效率低，如何协同AI与人类以取得更优输出，是该研究关注的问题。

Method: 作者构建了Tendem系统，AI负责结构化与重复性工作，人类专家在模型出错或需结果验证时介入，并在交付前统一质检。对94个实际任务进行评测，并与Upwork的AI-only及human-only流程对比。

Result: Tendem在输出质量与交付速度上均高于AI-only及human-only流程，运维成本与human-only相当。Tendem的AI Agent在第三方基准测试中也表现接近最新技术水平，特别是在网页浏览、工具使用与前沿知识推断方面。

Conclusion: 通过AI与人类的协同，Tendem实现了高效且高质量的任务执行，展示了混合智能系统在实际应用中的优越性和实用价值。

Abstract: Tendem is a hybrid system where AI handles structured, repeatable work and Human Experts step in when the models fail or to verify results. Each result undergoes a comprehensive quality review before delivery to the Client. To assess Tendem's performance, we conducted a series of in-house evaluations on 94 real-world tasks, comparing it with AI-only agents and human-only workflows carried out by Upwork freelancers. The results show that Tendem consistently delivers higher-quality outputs with faster turnaround times. At the same time, its operational costs remain comparable to human-only execution. On third-party agentic benchmarks, Tendem's AI Agent (operating autonomously, without human involvement) performs near state-of-the-art on web browsing and tool-use tasks while demonstrating strong results in frontier domain knowledge and reasoning.

</details>


### [345] [Long-range Modeling and Processing of Multimodal Event Sequences](https://arxiv.org/abs/2602.01125)
*Jichu Li,Yilun Zhong,Zhiting Li,Feng Zhou,Quyu Kong*

Main category: cs.CL

TL;DR: 本文提出了一种将大语言模型（LLM）与时序点过程（TPP）结合，扩展到多模态（包括视觉）信息的建模框架，并通过序列压缩机制改善长上下文文本生成。实验结果表明，该方法在预测精度和文本生成质量上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统TPP已开始扩展处理文本信息，但面对多模态（如视觉+文本）数据时，序列长度大幅增加，现有基于注意力模型难以有效生成需要长程依赖理解的连贯文本描述。论文旨在解决多模态数据下的长序列处理和内容生成问题。

Method: 提出扩展至视觉模态的LLM-TPP框架，将文本生成、时间预测和类型预测结合。设计了基于时间相似性的自适应序列压缩机制，压缩长序列同时保持关键事件模式。采用两阶段训练：首先在压缩序列上预训练，之后针对下游任务进行有监督微调。

Result: 在包括有挑战性的DanmakuTPP-QA基准在内的大量实验中，新方法在预测准确率和生成文本质量方面均超过现有SOTA方法。

Conclusion: 本文提出的多模态TPP框架和压缩机制能有效提升多模态事件序列的建模和文本生成能力，为未来多模态事件理解任务提供了新的解决方案。

Abstract: Temporal point processes (TPPs) have emerged as powerful tools for modeling asynchronous event sequences. While recent advances have extended TPPs to handle textual information, existing approaches are limited in their ability to generate rich, multimodal content and reason about event dynamics. A key challenge is that incorporating multimodal data dramatically increases sequence length, hindering the ability of attention-based models to generate coherent, long-form textual descriptions that require long-range understanding. In this paper, we propose a novel framework that extends LLM-based TPPs to the visual modality, positioning text generation as a core capability alongside time and type prediction. Our approach addresses the long-context problem through an adaptive sequence compression mechanism based on temporal similarity, which reduces sequence length while preserving essential patterns. We employ a two-stage paradigm of pre-training on compressed sequences followed by supervised fine-tuning for downstream tasks. Extensive experiments, including on the challenging DanmakuTPP-QA benchmark, demonstrate that our method outperforms state-of-the-art baselines in both predictive accuracy and the quality of its generated textual analyses.

</details>


### [346] [Don't Judge a Book by its Cover: Testing LLMs' Robustness Under Logical Obfuscation](https://arxiv.org/abs/2602.01132)
*Abhilekh Borah,Shubhra Ghosh,Kedar Joshi,Aditya Kumar Guru,Kripabandhu Ghosh*

Main category: cs.CL

TL;DR: 大语言模型（LLMs）在面对逻辑等价但表述被混淆的问题时，表现大幅下降，显示理解深度不足。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在标准表述下表现优异，但对同样逻辑但被重新表述或混淆的问题表现较差。研究该弱点有助于推动模型的真正理解能力提升。

Method: 提出了Logifus逻辑混淆框架，并基于此构建了LogiQAte诊断基准，涵盖四类推理任务（模糊化一阶逻辑、血缘关系、数字序列、方向感），并对六种先进模型进行了评测。

Result: 混淆化任务导致主流模型零样本推理表现显著下降：GPT-4o下降47%、GPT-5下降27%、o4-mini下降22%。

Conclusion: 当前大模型更多是对表面形式进行解析，缺乏对深层语义的理解，有必要构建真正理解和保持语义的模型。

Abstract: Tasks such as solving arithmetic equations, evaluating truth tables, and completing syllogisms are handled well by large language models (LLMs) in their standard form, but they often fail when the same problems are posed in logically equivalent yet obfuscated formats. To study this vulnerability, we introduce Logifus, a structure-preserving logical obfuscation framework, and, utilizing this, we present LogiQAte, a first-of-its-kind diagnostic benchmark with 1,108 questions across four reasoning tasks: (i) Obfus FOL (first-order logic entailment under equivalence-preserving rewrites), (ii) Obfus Blood Relation (family-graph entailment under indirect relational chains), (iii) Obfus Number Series (pattern induction under symbolic substitutions), and (iv) Obfus Direction Sense (navigation reasoning under altered directions and reference frames). Across all the tasks, evaluating six state-of-the-art models, we find that obfuscation severely degrades zero-shot performance, with performance dropping on average by 47% for GPT-4o, 27% for GPT-5, and 22% for reasoning model, o4-mini. Our findings reveal that current LLMs parse questions without deep understanding, highlighting the urgency of building models that genuinely comprehend and preserve meaning beyond surface form.

</details>


### [347] [Beyond Training for Cultural Awareness: The Role of Dataset Linguistic Structure in Large Language Models](https://arxiv.org/abs/2602.01161)
*Reem I. Masoud,Chen Feng,Shunta Asano,Saied Alshahrani,Philip Colin Treleaven,Miguel R. D. Rodrigues*

Main category: cs.CL

TL;DR: 本文探讨了用于大型语言模型文化适应微调的数据集在语言学属性上的特征，以及这些特征与模型文化表现的关联性，并通过实证检验揭示不同属性对模型表现的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管现有大模型在全球范围内部署，但其在文化迁移过程中经常出现文化错位的问题，而微调数据集的具体语言学特征与文化对齐效果之间的关系还不清楚。作者希望揭示微调数据集的哪些属性对文化适应起关键作用。

Method: 作者从数据集角度出发，对阿拉伯语、中文和日语的微调数据集分别计算多种轻量级语言、语义和结构指标，通过主成分分析（PCA）在每种语言内部降维提取主要变化维度，然后用这些数据子集对LLaMA、Mistral、DeepSeek三大模型家族进行微调，并用文化知识、价值观和规范基准集评测其表现，通过相关性分析和受控实验数据子集干预进一步验证各主成分对模型性能的影响。

Result: 主成分（如语义连贯性、词汇和句法多样性、词汇/结构丰富性）与模型下游的文化表现有相关性，但这种相关性高度依赖所用模型。词汇主导的成分（PC3）表现较为稳健，能在不同模型和基准中带来一致提升，而注重语义连贯性或多样性极端的成分（PC1-PC2）效果往往中性甚至负面。

Conclusion: 微调数据集在词汇和结构丰富性上表现突出可以更稳健提升模型文化适应表现，而过度追求语义或多样性并不一定有利。数据集属性与模型的互动需结合具体模型评估，强调了更细致、定制化的数据集构建策略对文化适应性大模型的重要性。

Abstract: The global deployment of large language models (LLMs) has raised concerns about cultural misalignment, yet the linguistic properties of fine-tuning datasets used for cultural adaptation remain poorly understood. We adopt a dataset-centric view of cultural alignment and ask which linguistic properties of fine-tuning data are associated with cultural performance, whether these properties are predictive prior to training, and how these effects vary across models. We compute lightweight linguistic, semantic, and structural metrics for Arabic, Chinese, and Japanese datasets and apply principal component analysis separately within each language. This design ensures that the resulting components capture variation among datasets written in the same language rather than differences between languages. The resulting components correspond to broadly interpretable axes related to semantic coherence, surface-level lexical and syntactic diversity, and lexical or structural richness, though their composition varies across languages. We fine-tune three major LLM families (LLaMA, Mistral, DeepSeek) and evaluate them on benchmarks of cultural knowledge, values, and norms. While PCA components correlate with downstream performance, these associations are strongly model-dependent. Through controlled subset interventions, we show that lexical-oriented components (PC3) are the most robust, yielding more consistent performance across models and benchmarks, whereas emphasizing semantic or diversity extremes (PC1-PC2) is often neutral or harmful.

</details>


### [348] [Typologically-Informed Candidate Reranking for LLM-based Translation into Low-Resource Languages](https://arxiv.org/abs/2602.01162)
*Nipuna Abeykoon,Ashen Weerathunga,Pubudu Wijesinghe,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: 本文提出了一种基于语言类型学的框架，无需平行语料或模型再训练，即可提升大语言模型对低资源语言翻译的结构合规性和质量。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型主要以高资源语言为主进行训练，导致翻译到低资源语言时，容易带入高资源语言的表达和结构偏见，造成结构不符，且难以用现有方法提升低资源语言的翻译质量。

Method: 文章提出了一个包含两个主要部分的框架：1) 通用元语言框架（UMF），通过16个类型学维度及加权差异得分为不同语言建立结构化档案；2) 计算引擎，通过生成阶段的语言歧义消解与选择阶段的类型学合规评分，对候选翻译进行处理。该方法无需平行语料和模型再训练，支持任何能生成多候选的大模型。

Result: 在九个语对、341个包含不同形态和句法现象的英语句子上评测，干预率与英语类型距离正相关。对不同类型低资源语言的干预精准率分别达48.16%、28.15%、86.26%。

Conclusion: 这一框架无需额外平行语料和训练，可直接提升大语言模型对低资源语言的结构合规性和翻译质量，具备落地应用潜力。

Abstract: Large language models trained predominantly on high-resource languages exhibit systematic biases toward dominant typological patterns, leading to structural non-conformance when translating into typologically divergent low-resource languages. We present a framework that leverages linguistic typology to improve translation quality without parallel training data or model retraining. The framework consists of two components: the Universal Metalinguistic Framework (UMF), which represents languages as structured profiles across 16 typological dimensions with divergence-weighted scoring, and the Computational Engine, which operates through linguistic disambiguation during generation and typological compliance scoring during selection. Evaluation across nine language pairs demonstrates intervention rates strongly correlating with typological distance from English. In experiments on 341 English sentences each having different morphological and syntactic phenomena, the framework shows an intervention precision of 48.16% for conservatively treated languages, 28.15% for morphologically dense languages, and 86.26% for structurally profiled languages. The framework requires no parallel training data and operates with any LLM capable of producing multiple candidate outputs, enabling practical deployment for under-resourced languages.

</details>


### [349] [PedagoSense: A Pedology Grounded LLM System for Pedagogical Strategy Detection and Contextual Response Generation in Learning Dialogues](https://arxiv.org/abs/2602.01169)
*Shahem Sultan,Shahem Fadi,Yousef Melhim,Ibrahim Alsarraj,Besher Hassan*

Main category: cs.CL

TL;DR: 本文提出了一种名为PedagoSense的系统，可以检测和推荐对话式学习中的有效教学策略，并利用大语言模型生成符合策略的对话回复，从而提升师生对话的教学互动质量。


<details>
  <summary>Details</summary>
Motivation: 现有对话式学习系统难以自动检测和推荐有效的教学策略，因此难以提升师生交流的教学成效和个性化水平。

Method: PedagoSense系统首先通过二分类模型检测对话中是否存在教学策略，并进一步进行细粒度分类以识别具体策略类型。同时，系统结合对话语境推荐合适的教学策略，并利用大语言模型生成与该策略一致的回复。

Result: 在人类标注的师生对话数据及扩充的非教学对话数据上，系统在教学策略检测任务中表现出色，并在数据增强时获得持续提升，但在细粒度策略分类上仍有挑战。

Conclusion: PedagoSense成功结合教学理论与LLM对话生成，为自适应教育技术提供更有效的教学策略支持，推动智能教育的实际应用。

Abstract: This paper addresses the challenge of improving interaction quality in dialogue based learning by detecting and recommending effective pedagogical strategies in tutor student conversations. We introduce PedagoSense, a pedology grounded system that combines a two stage strategy classifier with large language model generation. The system first detects whether a pedagogical strategy is present using a binary classifier, then performs fine grained classification to identify the specific strategy. In parallel, it recommends an appropriate strategy from the dialogue context and uses an LLM to generate a response aligned with that strategy. We evaluate on human annotated tutor student dialogues, augmented with additional non pedagogical conversations for the binary task. Results show high performance for pedagogical strategy detection and consistent gains when using data augmentation, while analysis highlights where fine grained classes remain challenging. Overall, PedagoSense bridges pedagogical theory and practical LLM based response generation for more adaptive educational technologies.

</details>


### [350] [EmoAra: Emotion-Preserving English Speech Transcription and Cross-Lingual Translation with Arabic Text-to-Speech](https://arxiv.org/abs/2602.01170)
*Besher Hassan,Ibrahim Alsarraj,Musaab Hasan,Yousef Melhim,Shahem Fadi,Shahem Sultan*

Main category: cs.CL

TL;DR: 本论文提出了EmoAra，一个端到端的情感保持跨语言口语交流系统，主要应用于银行客户服务领域，实现英语语音到阿拉伯语语音的情感保持传递。


<details>
  <summary>Details</summary>
Motivation: 银行客户服务中，准确传递用户的情感对于服务质量至关重要。现有跨语言语音系统往往忽略了情感传递，导致信息和服务体验的损失，因此有必要开发一种能保持情感的跨语言交流系统。

Method: EmoAra整合了语音情感识别（CNN分类器）、自动语音识别（Whisper）、机器翻译（微调MarianMT）、文本转语音（MMS-TTS-Ara）多个模块，实现英语语音输入到保留情感的阿拉伯语语音输出。

Result: 情感识别F1值达到94%，翻译BLEU为56，BERTScore F1为88.7%。在人类银行领域翻译评价中平均得分为81%。

Conclusion: EmoAra能够在跨语言口语交流中有效保持情感信息，提升了银行客户服务中情感传递的效果。实现细节和资源已经在GitHub公开。

Abstract: This work presents EmoAra, an end-to-end emotion-preserving pipeline for cross-lingual spoken communication, motivated by banking customer service where emotional context affects service quality. EmoAra integrates Speech Emotion Recognition, Automatic Speech Recognition, Machine Translation, and Text-to-Speech to process English speech and deliver an Arabic spoken output while retaining emotional nuance. The system uses a CNN-based emotion classifier, Whisper for English transcription, a fine-tuned MarianMT model for English-to-Arabic translation, and MMS-TTS-Ara for Arabic speech synthesis. Experiments report an F1-score of 94% for emotion classification, translation performance of BLEU 56 and BERTScore F1 88.7%, and an average human evaluation score of 81% on banking-domain translations. The implementation and resources are available at the accompanying GitHub repository.

</details>


### [351] [Bridging Lexical Ambiguity and Vision: A Mini Review on Visual Word Sense Disambiguation](https://arxiv.org/abs/2602.01193)
*Shashini Nilukshi,Deshan Sumanathilaka*

Main category: cs.CL

TL;DR: 本文综述了视觉词义消歧（VWSD）领域的最新进展，强调多模态手段在词义消歧上的提升，并讨论了主要挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统词义消歧仅利用文本和词典资源，难以解决视觉-语言融合任务中的词义歧义问题。VWSD 引入视觉信息，有助于更准确地判别词义，满足现代视觉语言应用需求。

Method: 梳理了2016至2025年VWSD领域的代表性研究，包括基于特征、图结构和对比学习的多模态融合方法；重点分析了CLIP等对比模型、扩散式文本生成、LLM支持下的新框架，以及prompt工程、多语种适应和模型微调策略。

Result: 定量结果显示，CLIP微调模型及整合LLM的VWSD系统在准确率（MRR）上较零样本基线提升6-8％。

Conclusion: VWSD已取得实质进展，但多语种数据匮乏、上下文局限、模型偏见及评测体系不足等问题亟待解决。未来研究将着重在CLIP对齐、扩散生成和语言模型推理的融合，实现更强、具情境感知、多语种的消歧能力。

Abstract: This paper offers a mini review of Visual Word Sense Disambiguation (VWSD), which is a multimodal extension of traditional Word Sense Disambiguation (WSD). VWSD helps tackle lexical ambiguity in vision-language tasks. While conventional WSD depends only on text and lexical resources, VWSD uses visual cues to find the right meaning of ambiguous words with minimal text input. The review looks at developments from early multimodal fusion methods to new frameworks that use contrastive models like CLIP, diffusion-based text-to-image generation, and large language model (LLM) support. Studies from 2016 to 2025 are examined to show the growth of VWSD through feature-based, graph-based, and contrastive embedding techniques. It focuses on prompt engineering, fine-tuning, and adapting to multiple languages. Quantitative results show that CLIP-based fine-tuned models and LLM-enhanced VWSD systems consistently perform better than zero-shot baselines, achieving gains of up to 6-8\% in Mean Reciprocal Rank (MRR). However, challenges still exist, such as limitations in context, model bias toward common meanings, a lack of multilingual datasets, and the need for better evaluation frameworks. The analysis highlights the growing overlap of CLIP alignment, diffusion generation, and LLM reasoning as the future path for strong, context-aware, and multilingual disambiguation systems.

</details>


### [352] [Attention Sink Forges Native MoE in Attention Layers: Sink-Aware Training to Address Head Collapse](https://arxiv.org/abs/2602.01203)
*Zizhuo Fu,Wenxuan Zeng,Runsheng Wang,Meng Li*

Main category: cs.CL

TL;DR: 该论文分析了大模型注意力层中出现的 attention sink 问题，并提出了基于 sink-aware 的训练方法和辅助损失来缓解 head collapse，从而提升模型效果。


<details>
  <summary>Details</summary>
Motivation: 现有大模型的注意力机制容易出现注意力过度集中于首个 token 的现象（attention sink），导致仅部分注意力头在生成任务中真正工作（head collapse），而目前对这些问题的成因和不同机制间的联系缺少系统分析。

Method: 作者通过理论推导和实证分析，发现 Vanilla Attention 和 Sink Attention 自然构成了一种专家混合（MoE）的接入方式，从而解释 head collapse 的出现。为了解决这一问题，作者提出了 sink-aware 的训练算法，在注意力层引入了辅助的负载均衡损失以平衡各注意力头的作用。

Result: 实验证明，该方法能有效实现各注意力头的负载均衡，并提升多种注意力机制下（包括Vanilla、Sink、Gated Attention）的模型性能。

Conclusion: 论文为注意力机制提供了新的分析视角，揭示了注意力层中依然潜在的 MoE 结构，并为后续研究 attention sink 和 head collapse 问题、改进注意力机制设计提供了理论和方法参考。

Abstract: Large Language Models (LLMs) often assign disproportionate attention to the first token, a phenomenon known as the attention sink. Several recent approaches aim to address this issue, including Sink Attention in GPT-OSS and Gated Attention in Qwen3-Next. However, a comprehensive analysis of the relationship among these attention mechanisms is lacking. In this work, we provide both theoretical and empirical evidence demonstrating that the sink in Vanilla Attention and Sink Attention naturally construct a Mixture-of-Experts (MoE) mechanism within attention layers. This insight explains the head collapse phenomenon observed in prior work, where only a fixed subset of attention heads contributes to generation. To mitigate head collapse, we propose a sink-aware training algorithm with an auxiliary load balancing loss designed for attention layers. Extensive experiments show that our method achieves effective head load balancing and improves model performance across Vanilla Attention, Sink Attention, and Gated Attention. We hope this study offers a new perspective on attention mechanisms and encourages further exploration of the inherent MoE structure within attention layers.

</details>


### [353] [ASTER: Agentic Scaling with Tool-integrated Extended Reasoning](https://arxiv.org/abs/2602.01204)
*Xuqin Zhang,Quan He,Zhenrui Zheng,Zongzhang Zhang,Xu He,Dong Li*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架ASTER，通过改进初始训练数据，显著提升了大模型在多轮调用外部工具推理任务中的表现，并达到数学领域的最新水平。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习用于大模型多轮工具推理面临‘交互崩溃’问题，即模型更倾向于内部推理而非有效、多轮地调用外部工具，严重限制了性能提升，因此需要找到更优的训练策略。

Method: 系统研究了初始‘冷启动’监督微调（SFT）阶段行为设定、冷启动轨迹的交互密度、以及RL训练预算对大模型推理与泛化的影响。提出ASTER框架，在冷启动阶段优先选取交互密度高的专家轨迹，大大加强模型外部工具参与的习惯。

Result: 发现仅使用4000条高交互密度专家轨迹即可带来最优的后续表现。基于此训练的ASTER-4B模型，在AIME 2025等数学高难度基准上达到90%的准确率，超越了当前主流开源大模型。

Conclusion: ASTER展示了小规模、高质量冷启动数据对大模型RL训练效果的巨大促进作用，有效解决了交互崩溃问题，为大模型多轮工具推理提供了更优的发展路径。

Abstract: Reinforcement learning (RL) has emerged as a dominant paradigm for eliciting long-horizon reasoning in Large Language Models (LLMs). However, scaling Tool-Integrated Reasoning (TIR) via RL remains challenging due to interaction collapse: a pathological state where models fail to sustain multi-turn tool usage, instead degenerating into heavy internal reasoning with only trivial, post-hoc code verification. We systematically study three questions: (i) how cold-start SFT induces an agentic, tool-using behavioral prior, (ii) how the interaction density of cold-start trajectories shapes exploration and downstream RL outcomes, and (iii) how the RL interaction budget affects learning dynamics and generalization under varying inference-time budgets. We then introduce ASTER (Agentic Scaling with Tool-integrated Extended Reasoning), a framework that circumvents this collapse through a targeted cold-start strategy prioritizing interaction-dense trajectories. We find that a small expert cold-start set of just 4K interaction-dense trajectories yields the strongest downstream performance, establishing a robust prior that enables superior exploration during extended RL training. Extensive evaluations demonstrate that ASTER-4B achieves state-of-the-art results on competitive mathematical benchmarks, reaching 90.0% on AIME 2025, surpassing leading frontier open-source models, including DeepSeek-V3.2-Exp.

</details>


### [354] [Chronos: Learning Temporal Dynamics of Reasoning Chains for Test-Time Scaling](https://arxiv.org/abs/2602.01208)
*Kai Zhang,Jiayi Liao,Chengpeng Li,Ziyuan Xie,Sihang Li,Xiang Wang*

Main category: cs.CL

TL;DR: 提出了Chronos，一种通过时间序列建模来提升大语言模型推理性能的轻量化投票方法。


<details>
  <summary>Details</summary>
Motivation: 现有的Test-Time Scaling方法（如多数投票、启发式打分）对推理轨迹或token一视同仁，容易受到质量波动和局部逻辑失误的影响，因此需要更精细的轨迹质量评估机制。

Method: Chronos方法将每条推理轨迹视为时间序列，学习token概率的轨迹特征，对不同轨迹赋予质量分数，并采用加权投票机制来提升判决的可靠性。该方法轻量简单，易于集成到现有系统中。

Result: 大量在同域和跨域基准测试下评估表明，Chronos在多个模型上均获得显著性能提升，同时计算开销很小。在HMMT25任务上，采用Qwen3-4B-Thinking-2507模型的Chronos@128方案相比Pass@1提升了34.21%，相比Maj@128提升了22.70%。

Conclusion: Chronos能够以极低的计算成本，稳定显著提升TTS范式下大模型的推理准确率，是一种有效的推理品质增强工具。

Abstract: Test-Time Scaling (TTS) has emerged as an effective paradigm for improving the reasoning performance of large language models (LLMs). However, existing methods -- most notably majority voting and heuristic token-level scoring -- treat reasoning traces or tokens equally, thereby being susceptible to substantial variations in trajectory quality and localized logical failures. In this work, we introduce \textbf{Chronos}, a lightweight and plug-and-play chronological reasoning scorer that models each trajectory as a time series. Specifically, Chronos learns to capture trajectory features of token probabilities, assigns quality scores accordingly, and employs a weighted voting mechanism. Extensive evaluations on both in-domain and out-of-domain benchmarks demonstrate that Chronos consistently delivers substantial gains across a variety of models, with negligible computational overhead. Notably, Chronos@128 achieves relative improvements of 34.21\% over Pass@1 and 22.70\% over Maj@128 on HMMT25 using Qwen3-4B-Thinking-2507, highlighting its effectiveness.

</details>


### [355] [Supervised Fine-Tuning Needs to Unlock the Potential of Token Priority](https://arxiv.org/abs/2602.01227)
*Zhanming Shen,Zeyu Qin,Jiaqi Hu,Wentao Ye,Hao Chen,Xiaomeng Hu,Haokai Xu,Gang Chen,Yi R. Fung,Haobo Wang*

Main category: cs.CL

TL;DR: 该论文提出“Token Priority”作为细粒度建模与人类真实效用对齐的核心桥梁，将有监督微调（SFT）形式化为分布重塑过程，并基于此对近期对齐技术进行统一分析。


<details>
  <summary>Details</summary>
Motivation: 当前大模型训练中，模型通常以细粒度（如逐token自回归）的方式生成文本，但监督信号往往粗糙/统一，导致模型难以真正对齐人类期望。这一粒度不匹配限制了模型能力提升。论文提出用Token Priority弥合这一差距。

Method: 将SFT视为数据分布重塑，依据Token Priority思想重构训练方法：包括区分正优先（用于过滤噪声token）和有符号优先（用于消除有害token）。据此对现有相关方法进行理论统一和分类分析。

Result: 通过Token Priority视角，将现有对齐与分布修正方法分为两大类，系统梳理并对已取得的突破和存在问题进行梳理，展示该理论强大的解释和指导能力。

Conclusion: Token Priority为机制设计和监督信号设计提供了新思路，可精准捕捉token级的对齐需求。未来应进一步完善token权重分配、评价体系及解决标签噪声等关键挑战。

Abstract: The transition from fitting empirical data to achieving true human utility is fundamentally constrained by a granularity mismatch, where fine-grained autoregressive generation is often supervised by coarse or uniform signals. This position paper advocates Token Priority as the essential bridge, formalizing Supervised Fine-Tuning (SFT) not as simple optimization but as a precise distribution reshaping process that aligns raw data with the ideal alignment manifold. We analyze recent breakthroughs through this unified lens, categorizing them into two distinct regimes: Positive Priority for noise filtration and Signed Priority for toxic modes unlearning. We revisit existing progress and limitations, identify key challenges, and suggest directions for future research.

</details>


### [356] [Inferential Question Answering](https://arxiv.org/abs/2602.01239)
*Jamshid Mozafari,Hamed Zamani,Guido Zuccon,Adam Jatowt*

Main category: cs.CL

TL;DR: 本文提出了一类新的问答任务——推理型问答（Inferential QA），要求模型根据仅提供线索的文本推断答案，并构建了QUIT数据集以研究此问题。实验发现，传统的问答方法在该任务上表现不佳，现有模型难以从间接证据中推理答案。


<details>
  <summary>Details</summary>
Motivation: 虽然问答系统研究广泛，但大多数工作假设答案可以直接从文档中提取或生成，忽略了需要推理才能得出答案的场景。因此，作者希望推动问答系统从直接抽取向推理理解转变。

Method: 作者提出Inferential QA任务，构建了QUIT数据集（包含7,401个问题和240万段文本），由人类和机器生成提示后手工标注相关性。评估了不同的检索器、重排序器以及LLM阅读器在推理型问答上的表现。

Result: 在推理型问答中，传统检索器表现不佳，重排序器提升有限，微调效果不稳定，即使是强调推理的LLM也未优于小型通用模型。

Conclusion: 当前主流问答系统和模型尚未具备利用间接文本线索进行推理答题的能力，Inferential QA为更高层次理解和推理开启了新的研究方向。

Abstract: Despite extensive research on a wide range of question answering (QA) systems, most existing work focuses on answer containment-i.e., assuming that answers can be directly extracted and/or generated from documents in the corpus. However, some questions require inference, i.e., deriving answers that are not explicitly stated but can be inferred from the available information. We introduce Inferential QA -- a new task that challenges models to infer answers from answer-supporting passages which provide only clues. To study this problem, we construct QUIT (QUestions requiring Inference from Texts) dataset, comprising 7,401 questions and 2.4M passages built from high-convergence human- and machine-authored hints, labeled across three relevance levels using LLM-based answerability and human verification. Through comprehensive evaluation of retrievers, rerankers, and LLM-based readers, we show that methods effective on traditional QA tasks struggle in inferential QA: retrievers underperform, rerankers offer limited gains, and fine-tuning provides inconsistent improvements. Even reasoning-oriented LLMs fail to outperform smaller general-purpose models. These findings reveal that current QA pipelines are not yet ready for inference-based reasoning. Inferential QA thus establishes a new class of QA tasks that move towards understanding and reasoning from indirect textual evidence.

</details>


### [357] [Minimizing Mismatch Risk: A Prototype-Based Routing Framework for Zero-shot LLM-generated Text Detection](https://arxiv.org/abs/2602.01240)
*Ke Sun,Guangsheng Bao,Han Cui,Yue Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种新方法DetectRouter，通过为每个输入选择最合适的代理模型，提高了对大语言模型(LLM)生成文本的检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有零样本检测方法通常用固定代理模型检测LLM生成文本，但性能对代理与真实源的匹配度非常敏感，没有一款代理能通用于所有输入。如何为每个输入选择最佳代理，是提升检测准确性的关键。

Method: 提出DetectRouter框架，包括两个阶段：第一阶段利用白盒模型构建区分性的原型，第二阶段通过几何距离与检测分数的对齐，实现对黑盒源的泛化。该方法针对每个输入路由到最佳检测代理。

Result: 在EvoBench和MAGE基准上，DetectRouter在多种检测标准和模型家族上均表现出一致优于现有方法的检测性能。

Conclusion: 通过动态选择最合适代理模型，DetectRouter有效提升了LLM生成文本的检测能力，验证了路由问题在生成检测中的重要性。

Abstract: Zero-shot methods detect LLM-generated text by computing statistical signatures using a surrogate model. Existing approaches typically employ a fixed surrogate for all inputs regardless of the unknown source. We systematically examine this design and find that detection performance varies substantially depending on surrogate-source alignment. We observe that while no single surrogate achieves optimal performance universally, a well-matched surrogate typically exists within a diverse pool for any given input. This finding transforms robust detection into a routing problem: selecting the most appropriate surrogate for each input. We propose DetectRouter, a prototype-based framework that learns text-detector affinity through two-stage training. The first stage constructs discriminative prototypes from white-box models; the second generalizes to black-box sources by aligning geometric distances with observed detection scores. Experiments on EvoBench and MAGE benchmarks demonstrate consistent improvements across multiple detection criteria and model families.

</details>


### [358] [Large-Scale Terminal Agentic Trajectory Generation from Dockerized Environments](https://arxiv.org/abs/2602.01244)
*Siwei Wu,Yizhi Li,Yuyang Song,Wei Zhang,Yang Wang,Riza Batista-Navarro,Xian Yang,Mingjie Tang,Bryan Dai,Jian Yang,Chenghua Lin*

Main category: cs.CL

TL;DR: 本文提出TerminalTraj系统，实现了高质量、可执行、可验证的终端任务数据集构建，大幅提升了大模型在终端环境任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 终端任务需要模型理解和操作复杂、多样且长期的命令行操作，但大规模、高质量终端轨迹数据收集难度大。挑战主要有：每个任务需独立、适配的Docker环境（保证可执行性），且多样输出难统一验证（保证可验证性），这些都制约了Terminal Agent模型大规模高效训练。

Method: 构建TerminalTraj流程，包括：1）严选高质量仓库，自动构建Docker环境；2）任务实例自动与Docker环境对齐生成；3）自动合成可执行的轨迹及验证代码，确保产出的数据既可执行又可自动验证。最终，生成了覆盖8类领域的32K Docker镜像和5万余条轨迹。

Result: 基于TerminalTraj数据，使用Qwen2.5-Coder模型训练，模型在TerminalBench测试集上性能提升显著：TB 1.0提升20%，TB 2.0提升10%。特别是TerminalTraj-32B模型，在小于100B参数规模下取得优异成绩（TB 1.0上为35.30%，TB 2.0上为22.00%），且推理时扩展性表现优良。

Conclusion: TerminalTraj有效解决了终端任务可执行性与可验证性难题，为训练更强的终端智能体提供了坚实数据基础，显著提升了相关大模型实际表现。所有代码与数据已开源。

Abstract: Training agentic models for terminal-based tasks critically depends on high-quality terminal trajectories that capture realistic long-horizon interactions across diverse domains. However, constructing such data at scale remains challenging due to two key requirements: \textbf{\emph{Executability}}, since each instance requires a suitable and often distinct Docker environment; and \textbf{\emph{Verifiability}}, because heterogeneous task outputs preclude unified, standardized verification. To address these challenges, we propose \textbf{TerminalTraj}, a scalable pipeline that (i) filters high-quality repositories to construct Dockerized execution environments, (ii) generates Docker-aligned task instances, and (iii) synthesizes agent trajectories with executable validation code. Using TerminalTraj, we curate 32K Docker images and generate 50,733 verified terminal trajectories across eight domains. Models trained on this data with the Qwen2.5-Coder backbone achieve consistent performance improvements on TerminalBench (TB), with gains of up to 20\% on TB~1.0 and 10\% on TB~2.0 over their respective backbones. Notably, \textbf{TerminalTraj-32B} achieves strong performance among models with fewer than 100B parameters, reaching 35.30\% on TB~1.0 and 22.00\% on TB~2.0, and demonstrates improved test-time scaling behavior. All code and data are available at https://github.com/Wusiwei0410/TerminalTraj.

</details>


### [359] [PARSE: An Open-Domain Reasoning Question Answering Benchmark for Persian](https://arxiv.org/abs/2602.01246)
*Jamshid Mozafari,Seyed Parsa Mousavinasab,Adam Jatowt*

Main category: cs.CL

TL;DR: 本文提出了PARSE，这是首个面向波斯语的开放领域推理型问答基准数据集，包含多种题型和推理类型，旨在弥补低资源语言高质量QA基准的空白，并通过多模型实验验证了数据集的有效性。


<details>
  <summary>Details</summary>
Motivation: 目前大多数推理型问答领域的研究和评测集中在高资源语言上，低资源语言（如波斯语）缺乏高质量、系统性的推理型问答数据集，限制了相关QA系统和大语言模型在这些语言上的发展。

Method: 构建了包含10800道题目的PARSE基准，涵盖布尔型、选择题和事实型问答，并控制生成流程保证推理多样性。数据集通过LLM生成与人工校验、多阶段筛选、注释及一致性检查提升语言与事实质量。对多语言与波斯语LLM模型采用多种提示策略（如链式推理、少样本学习）开展基准测试。

Result: 实验表明，采用波斯语与结构化的提示策略（链式推理/少样本学习）显著提升了模型性能，进一步微调（fine-tuning）能进一步提升特别针对波斯语的模型效果。

Conclusion: PARSE有效填补了波斯语及低资源语言推理型问答评测数据集的空白，支持模型的公平对比和实用性改进，为低资源场景下研发与评估推理能力强的大语言模型提供了坚实基础。

Abstract: Reasoning-focused Question Answering (QA) has advanced rapidly with Large Language Models (LLMs), yet high-quality benchmarks for low-resource languages remain scarce. Persian, spoken by roughly 130 million people, lacks a comprehensive open-domain resource for evaluating reasoning-capable QA systems. We introduce PARSE, the first open-domain Persian reasoning QA benchmark, containing 10,800 questions across Boolean, multiple-choice, and factoid formats, with diverse reasoning types, difficulty levels, and answer structures. The benchmark is built via a controlled LLM-based generation pipeline and validated through human evaluation. We also ensure linguistic and factual quality through multi-stage filtering, annotation, and consistency checks. We benchmark multilingual and Persian LLMs under multiple prompting strategies and show that Persian prompts and structured prompting (CoT for Boolean/multiple-choice; few-shot for factoid) improve performance. Fine-tuning further boosts results, especially for Persian-specialized models. These findings highlight how PARSE supports both fair comparison and practical model adaptation. PARSE fills a critical gap in Persian QA research and provides a strong foundation for developing and evaluating reasoning-capable LLMs in low-resource settings.

</details>


### [360] [PACER: Blockwise Pre-verification for Speculative Decoding with Adaptive Length](https://arxiv.org/abs/2602.01274)
*Situo Zhang,Yifan Zhang,Zichen Zhu,Hankun Wang,Da Ma,Danyang Zhang,Lu Chen,Kai Yu*

Main category: cs.CL

TL;DR: 提出了Pacer方法，通过动态调整draft长度，在大语言模型推理中进一步提升了Speculative Decoding的效率。


<details>
  <summary>Details</summary>
Motivation: 原有的Speculative Decoding方法在draft长度固定时，无法适应不同推理步骤所需的最优长度，限制了推理加速的潜力。

Method: Pacer在SD基础上加入了一个轻量且可训练的pre-verification层，对draft token块进行预验证，若不通过则阻止进一步生成，从而动态调整draft长度。

Result: 在多个SD模型组合和基准数据集上实验验证，Pacer比传统autoregressive解码最高提速2.66倍，并在各项指标上优于标准SD；结合Ouroboros后提速最高可达3.09倍。

Conclusion: Pacer通过动态draft长度控制和预验证机制，能进一步释放Speculative Decoding在LLM推理加速中的潜力，具有广泛应用价值。

Abstract: Speculative decoding (SD) is a powerful technique for accelerating the inference process of large language models (LLMs) without sacrificing accuracy. Typically, SD employs a small draft model to generate a fixed number of draft tokens, which are then verified in parallel by the target model. However, our experiments reveal that the optimal draft length varies significantly across different decoding steps. This variation suggests that using a fixed draft length limits the potential for further improvements in decoding speed. To address this challenge, we propose Pacer, a novel approach that dynamically controls draft length using a lightweight, trainable pre-verification layer. This layer pre-verifies draft tokens blockwise before they are sent to the target model, allowing the draft model to stop token generation if the blockwise pre-verification fails. We implement Pacer on multiple SD model pairs and evaluate its performance across various benchmarks. Our results demonstrate that Pacer achieves up to 2.66x Speedup over autoregressive decoding and consistently outperforms standard speculative decoding. Furthermore, when integrated with Ouroboros, Pacer attains up to 3.09x Speedup.

</details>


### [361] [EverMemBench: Benchmarking Long-Term Interactive Memory in Large Language ModelsEverMemBench: Benchmarking Long-Term Interactive Memory in Large Language Models](https://arxiv.org/abs/2602.01313)
*Chuanrui Hu,Tong Li,Xingze Gao,Hongda Chen,Dannong Xu,Yi Bai,Tianwei Lin,Xinda Zhao,Xiaohong Li,Jiaqi An,Yunyun Han,Jian Pei,Yafeng Deng*

Main category: cs.CL

TL;DR: 本文提出了EverMemBench，一个用于评估大语言模型长时记忆能力的新基准，专注于多方、多主题、角色分明的长对话场景，揭示了现有系统在复杂记忆、推理和检索方面的重大瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有记忆评测基准大多只关注两人、小范围主题对话，难以反映真实生活复杂多变、信息交错的长时对话需求，因此作者希望推动适用于真实多方长期交流场景的记忆研究。

Method: 作者设计了EverMemBench，包含跨主题、多角色、多组对话、超过百万tokens，并随时间演变。基准覆盖1000+问答对，从细粒度记忆召回、记忆觉察、用户画像理解等维度评测LLM记忆系统。

Result: 评测显示：(1) 多跳推理在多方会话下极难，即使是理想模型准确率也仅26%；(2) 时间推理无法仅凭时间戳，需更复杂的语义版本理解；(3) 检索环节是当前记忆能力短板，语义检索难以关联隐式相关但语义跨度大的信息。

Conclusion: EverMemBench为开发下一代复杂对话记忆结构和检索方法提供了高挑战性的标准测试平台，有助于推动真实场景下LLM深层记忆研究。

Abstract: Long-term conversational memory is essential for LLM-based assistants, yet existing benchmarks focus on dyadic, single-topic dialogues that fail to capture real-world complexity. We introduce EverMemBench, a benchmark featuring multi-party, multi-group conversations spanning over 1 million tokens with temporally evolving information, cross-topic interleaving, and role-specific personas. EverMemBench evaluates memory systems across three dimensions through 1,000+ QA pairs: fine-grained recall, memory awareness, and user profile understanding. Our evaluation reveals critical limitations: (1) multi-hop reasoning collapses in multi-party settings, with even oracle models achieving only 26%; (2) temporal reasoning remains unsolved, requiring version semantics beyond timestamp matching; (3) memory awareness is bottlenecked by retrieval, where current similarity-based methods fail to bridge the semantic gap between queries and implicitly relevant memories. EverMemBench provides a challenging testbed for developing next-generation memory architectures.

</details>


### [362] [DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas](https://arxiv.org/abs/2602.01326)
*Zirui Wu,Lin Zheng,Zhihui Xie,Jiacheng Ye,Jiahui Gao,Shansan Gong,Yansong Feng,Zhenguo Li,Wei Bi,Guorui Zhou,Lingpeng Kong*

Main category: cs.CL

TL;DR: 本文提出了DreamOn，一种改进的扩散语言模型（DLM）框架，有效解决了DLM在生成长度动态变化内容时受到事先固定mask长度限制的问题，实现了灵活、可变长度的文本补全。


<details>
  <summary>Details</summary>
Motivation: 现有扩散语言模型虽然能比自回归模型实现更灵活的补全任务，但受限于需要预先设定mask长度，这在实际代码补全等任务中严重影响了模型性能。因此，亟需打破这一固定长度生成的限制，提升模型实用性。

Method: 作者提出DreamOn扩散框架，在扩散生成过程中引入了两个长度控制状态，使模型可以自主扩展或收缩输出长度，仅依赖自身预测决定输出长度。此机制无需改变原有扩散模型结构，只需对训练目标做少量修改。该方法被集成到Dream-Coder-7B和DiffuCoder-7B中。

Result: DreamOn在HumanEval-Infilling和SantaCoder-FIM等基准测试上，实现了与最先进自回归模型相媲美的补全性能，并且在已知理想输出长度（oracle）的情况下，表现与理论最优一致。

Conclusion: DreamOn消除了扩散语言模型在可变长度文本生成中的根本障碍，显著提升了其实用性和灵活性，为DLM大规模实际部署铺平了道路。代码已开放。

Abstract: Diffusion Language Models (DLMs) present a compelling alternative to autoregressive models, offering flexible, any-order infilling without specialized prompting design. However, their practical utility is blocked by a critical limitation: the requirement of a fixed-length masked sequence for generation. This constraint severely degrades code infilling performance when the predefined mask size mismatches the ideal completion length. To address this, we propose DreamOn, a novel diffusion framework that enables dynamic, variable-length generation. DreamOn augments the diffusion process with two length control states, allowing the model to autonomously expand or contract the output length based solely on its own predictions. We integrate this mechanism into existing DLMs with minimal modifications to the training objective and no architectural changes. Built upon Dream-Coder-7B and DiffuCoder-7B, DreamOn achieves infilling performance on par with state-of-the-art autoregressive models on HumanEval-Infilling and SantaCoder-FIM and matches oracle performance achieved with ground-truth length. Our work removes a fundamental barrier to the practical deployment of DLMs, significantly advancing their flexibility and applicability for variable-length generation. Our code is available at https://github.com/DreamLM/DreamOn.

</details>


### [363] [CRAFT: Calibrated Reasoning with Answer-Faithful Traces via Reinforcement Learning for Multi-Hop Question Answering](https://arxiv.org/abs/2602.01348)
*Yu Liu,Wenxiao Zhang,Cong Cao,Fangfang Yuan,Weizhuo Chen,Cheng Hu,Pin Xu,Yuling Yang,Kun Peng,Diandian Guo,Qiang Sun,Yanbing Liu,Jin B. Hong,Zhiyuan Ma*

Main category: cs.CL

TL;DR: 本文提出了CRAFT框架，通过强化学习提升多跳问答中大模型的推理可信度和答案准确性。


<details>
  <summary>Details</summary>
Motivation: 多跳问答中，现有的RAG方法在生成回答时面临推理崩溃、推理与答案不一致、输出结构失控等问题，影响结果的准确性和可靠性。

Method: 提出CRAFT（Calibrated Reasoning with Answer-Faithful Traces），基于Group Relative Policy Optimization (GRPO)的强化学习框架，利用确定性与判别器双重奖励机制优化多跳推理过程，确保结构和语义上的忠实性。支持多种可控推理轨迹，便于分析结构与尺度对推理性能的影响。

Result: 在三个多跳问答数据集上，CRAFT提升了答案的准确性和推理的忠实性。CRAFT 7B模型在各种推理轨迹设置下，表现接近闭源大模型。

Conclusion: CRAFT在多跳问答任务中兼顾了推理的结构性和语义忠实性，为提升大模型可靠推理能力提供有效途径。

Abstract: Retrieval-augmented generation (RAG) is widely used to ground Large Language Models (LLMs) for multi-hop question answering. Recent work mainly focused on improving answer accuracy via fine-tuning and structured or reinforcement-based optimization. However, reliable reasoning in response generation faces three challenges: 1) Reasoning Collapse. Reasoning in multi-hop QA is inherently complex due to multi-hop composition and is further destabilized by noisy retrieval. 2) Reasoning-answer inconsistency. Due to the intrinsic uncertainty of LLM generation and exposure to evidence--distractor mixtures, models may produce correct answers that are not faithfully supported by their intermediate reasoning or evidence. 3) Loss of format control. Traditional chain-of-thought generation often deviates from required structured output formats, leading to incomplete or malformed structured content. To address these challenges, we propose CRAFT (Calibrated Reasoning with Answer-Faithful Traces), a Group Relative Policy Optimization (GRPO) based reinforcement learning framework that trains models to perform faithful reasoning during response generation. CRAFT employs dual reward mechanisms to optimize multi-hop reasoning: deterministic rewards ensure structural correctness while judge-based rewards verify semantic faithfulness. This optimization framework supports controllable trace variants that enable systematic analysis of how structure and scale affect reasoning performance and faithfulness. Experiments on three multi-hop QA benchmarks show that CRAFT improves both answer accuracy and reasoning faithfulness across model scales, with the CRAFT 7B model achieving competitive performance with closed-source LLMs across multiple reasoning trace settings.

</details>


### [364] [Balancing Understanding and Generation in Discrete Diffusion Models](https://arxiv.org/abs/2602.01362)
*Yue Liu,Yuzhong Zhao,Zheyong Xie,Qixiang Ye,Jianbin Jiao,Yao Hu,Shaosheng Cao,Yunfan Liu*

Main category: cs.CL

TL;DR: 本文提出了一种统一离散生成建模两大主流范式（MDLM和UDLM）的方法XDLM，兼具语义理解与生成质量优势，并优化了内存与推理效率。


<details>
  <summary>Details</summary>
Motivation: MDLM在语义理解和零样本泛化方面表现出色，但生成质量稍逊；相反，UDLM在少步生成质量上领先，但理解泛化不足。两者均未在理解与生成能力之间取得平衡，因此有必要设计一种能兼顾二者优势的新方法。

Method: 作者提出了一种基于平稳噪声核的XDLM方法，理论上统一了MDLM和UDLM，且通过代数简化后验概率缓解了内存瓶颈。XDLM可视作这两类范式的推广，具体参数设定下可退化为这两者。

Result: 实验结果显示，XDLM在理解能力与生成质量之间实现了更优的帕累托前沿：在零样本文本任务上比UDLM高5.4分，在少步图像生成上以更低的FID值（54.1 vs 80.8）超过MDLM。将XDLM扩展到8B参数的大模型时，仅用32步即可达15分MBPP，是基线的两倍。

Conclusion: XDLM同时兼顾了语义理解与生成质量，理论上统一了MDLM和UDLM，并在实际任务和大模型扩展上取得了显著领先，展示出优越的长远可扩展性。

Abstract: In discrete generative modeling, two dominant paradigms demonstrate divergent capabilities: Masked Diffusion Language Models (MDLM) excel at semantic understanding and zero-shot generalization, whereas Uniform-noise Diffusion Language Models (UDLM) achieve strong few-step generation quality, yet neither attains balanced performance across both dimensions. To address this, we propose XDLM, which bridges the two paradigms via a stationary noise kernel. XDLM offers two key contributions: (1) it provides a principled theoretical unification of MDLM and UDLM, recovering each paradigm as a special case; and (2) an alleviated memory bottleneck enabled by an algebraic simplification of the posterior probabilities. Experiments demonstrate that XDLM advances the Pareto frontier between understanding capability and generation quality. Quantitatively, XDLM surpasses UDLM by 5.4 points on zero-shot text benchmarks and outperforms MDLM in few-step image generation (FID 54.1 vs. 80.8). When scaled to tune an 8B-parameter large language model, XDLM achieves 15.0 MBPP in just 32 steps, effectively doubling the baseline performance. Finally, analysis of training dynamics reveals XDLM's superior potential for long-term scaling. Code is available at https://github.com/MzeroMiko/XDLM

</details>


### [365] [Context Dependence and Reliability in Autoregressive Language Models](https://arxiv.org/abs/2602.01378)
*Poushali Sengupta,Shashi Raj Pandey,Sabita Maharjan,Frank Eliassen*

Main category: cs.CL

TL;DR: 本文提出了一种名为RISE的新方法，以改进大语言模型(LLM)解释中的输入归因问题，使其更加稳健与可信。


<details>
  <summary>Details</summary>
Motivation: 当前LLM常用的解释方法在面对冗余和重叠的语境时效果不佳，输入的微小变化可能导致解释大幅波动，影响可解释性，并带来如提示注入等风险。因此，急需一种能区分真正关键语境信息的方法。

Method: 本文提出RISE（Redundancy-Insensitive Scoring of Explanation）方法，通过量化每个输入在其他输入条件下的独特影响，减弱冗余项的干扰，从而得到更为准确与稳定的归因解释。

Result: 实验表明，RISE方法相比传统方法能够提供更稳健、清晰的输入归因解释，更好地强调了条件信息的作用。

Conclusion: RISE提升了LLM输出归因解释的可控性和稳定性，有助于在关键场景下对模型行为进行有效监控和信任评估。

Abstract: Large language models (LLMs) generate outputs by utilizing extensive context, which often includes redundant information from prompts, retrieved passages, and interaction history. In critical applications, it is vital to identify which context elements actually influence the output, as standard explanation methods struggle with redundancy and overlapping context. Minor changes in input can lead to unpredictable shifts in attribution scores, undermining interpretability and raising concerns about risks like prompt injection. This work addresses the challenge of distinguishing essential context elements from correlated ones. We introduce RISE (Redundancy-Insensitive Scoring of Explanation), a method that quantifies the unique influence of each input relative to others, minimizing the impact of redundancies and providing clearer, stable attributions. Experiments demonstrate that RISE offers more robust explanations than traditional methods, emphasizing the importance of conditional information for trustworthy LLM explanations and monitoring.

</details>


### [366] [On the Power of (Approximate) Reward Models for Inference-Time Scaling](https://arxiv.org/abs/2602.01381)
*Youheng Zhu,Yiping Lu*

Main category: cs.CL

TL;DR: 本文分析了推理时缩放（inference-time scaling）中采用近似奖励模型的有效性，并通过理论证明了在奖励模型的Bellman误差受控的前提下，能显著提高SMC推理的效率。


<details>
  <summary>Details</summary>
Motivation: 现有大模型推理过程依赖于奖励模型进行中间结果的评估与计算分配，但实际中只能获得近似奖励模型。作者关注一个关键问题：什么时候、为什么近似奖励模型也能有效提升推理效率？

Method: 作者以Sequential Monte Carlo (SMC) 框架为基础，分析推理时间缩放，提出以Bellman误差来衡量近似奖励模型的好坏，并推导理论边界。研究主要通过理论推导和复杂度分析，证明误差为O(1/T)时会带来本质性的效率提升。

Result: 证明只要近似奖励模型的Bellman误差界于O(1/T)之内，SMC推理的计算复杂度可由指数级降为多项式级，推理效率得到指数级提升。

Conclusion: 即使只用近似奖励模型，只要其Bellman误差足够小，也可以极大提升SMC推理效率，对大模型推理优化具有重要理论价值。

Abstract: Inference-time scaling has recently emerged as a powerful paradigm for improving the reasoning capability of large language models. Among various approaches, Sequential Monte Carlo (SMC) has become a particularly important framework, enabling iterative generation, evaluation, rejection, and resampling of intermediate reasoning trajectories. A central component in this process is the reward model, which evaluates partial solutions and guides the allocation of computation during inference.
  However, in practice, true reward models are never available. All deployed systems rely on approximate reward models, raising a fundamental question: Why and when do approximate reward models suffice for effective inference-time scaling? In this work, we provide a theoretical answer. We identify the Bellman error of the approximate reward model as the key quantity governing the effectiveness of SMC-based inference-time scaling. For a reasoning process of length $T$, we show that if the Bellman error of the approximate reward model is bounded by $O(1/T)$, then combining this reward model with SMC reduces the computational complexity of reasoning from exponential in $T$ to polynomial in $T$. This yields an exponential improvement in inference efficiency despite using only approximate rewards.

</details>


### [367] [Rethinking Selective Knowledge Distillation](https://arxiv.org/abs/2602.01395)
*Almog Tavor,Itay Ebenspanger,Neil Cnaan,Mor Geva*

Main category: cs.CL

TL;DR: 本文系统性地分析了在自回归大语言模型（LLM）中选择性知识蒸馏（KD）的有效性，提出了一种基于学生模型熵进行位置选择的蒸馏方法（SE-KD），在多个任务上提高了准确率与效率，并大幅减少计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模的增长，标准的知识蒸馏方式因监督信息冗余导致训练效率和存储需求高，亟需更高效、有针对性的蒸馏策略，但选择哪些监督信号、筛选原则及其协同作用尚无定论。本文重新审视蒸馏中“在哪里、如何选”的问题，旨在寻找更优的蒸馏方案。

Method: 作者将选择性KD在模型的位置信息、词汇类型、样本三个维度上进行系统性区分，并对不同的重要性信号与选择策略进行了对比实验。在此基础上，提出了基于学生模型熵的位置信息选择（SE-KD），并扩展至词类和样本维度（SE-KD 3X），达到进一步优化效率的目的。

Result: SE-KD方法在多个基准任务中，均优于传统全监督蒸馏，在提升准确率、保持下游任务性能的同时，大幅提升了训练的内存与时间效率。SE-KD 3X在效率上进一步显著提升，实现了70%墙时、18%峰值内存和80%存储的减省。

Conclusion: 本文提出的基于学生熵的选择性知识蒸馏方法在保证模型性能的同时，极大改善了资源优化与训练效率，展示了优于现有密集蒸馏方法的应用前景。

Abstract: Growing efforts to improve knowledge distillation (KD) in large language models (LLMs) replace dense teacher supervision with selective distillation, which uses a subset of token positions, vocabulary classes, or training samples for supervision. However, it remains unclear which importance signals, selection policies, and their interplay are most effective. In this work, we revisit where and how to distill in autoregressive LLMs. We disentangle selective KD along the position, class, and sample axes and systematically compare importance signals and selection policies. Then, guided by this analysis, we identify underexplored opportunities and introduce student-entropy-guided position selection (SE-KD). Across a suite of benchmarks, SE-KD often improves accuracy, downstream task adherence, and memory efficiency over dense distillation. Extending this approach across the class and sample axes (SE-KD 3X) yields complementary efficiency gains that make offline teacher caching feasible. In practice, this reduces wall time by 70% and peak memory by 18%, while cutting storage usage by 80% over prior methods without sacrificing performance.

</details>


### [368] [From Pragmas to Partners: A Symbiotic Evolution of Agentic High-Level Synthesis](https://arxiv.org/abs/2602.01401)
*Niansong Zhang,Sunwoo Kim,Shreesha Srinath,Zhiru Zhang*

Main category: cs.CL

TL;DR: 这篇文章探讨了在大模型和AI驱动硬件设计兴起的背景下，高层次综合（HLS）是否仍然重要，并认为HLS依旧不可替代。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型与AI技术在硬件设计中的运用增多，人们开始质疑高层次综合（HLS）在新的智能时代是否还有价值。作者试图回答这一疑问，并重新界定HLS的作用。

Method: 本文属于立场性论文，主要通过论证和剖析：首先解释HLS作为抽象层的价值，再分析现有HLS工具的局限——比如性能反馈不足、接口僵化和调试受限，并讨论AI代理如何优化这些问题，最后提出了一个HLS与AI逐步融合的分类法。

Result: 作者指出，HLS因其快速迭代、可移植性和设计可变性，天然适合与AI代理结合进行优化，并能为AI硬件设计提供黄金参考。同时，当前的HLS工具有多方面不足，AI技术有望解决这些问题。

Conclusion: 高层次综合在AI时代仍不可或缺，未来随着AI能力演进，设计责任将从人类逐步转移到AI，HLS将在这个转变中扮演桥梁和加速器的角色。

Abstract: The rise of large language models has sparked interest in AI-driven hardware design, raising the question: does high-level synthesis (HLS) still matter in the agentic era? We argue that HLS remains essential. While we expect mature agentic hardware systems to leverage both HLS and RTL, this paper focuses on HLS and its role in enabling agentic optimization. HLS offers faster iteration cycles, portability, and design permutability that make it a natural layer for agentic optimization.This position paper makes three contributions. First, we explain why HLS serves as a practical abstraction layer and a golden reference for agentic hardware design. Second, we identify key limitations of current HLS tools, namely inadequate performance feedback, rigid interfaces, and limited debuggability that agents are uniquely positioned to address. Third, we propose a taxonomy for the symbiotic evolution of agentic HLS, clarifying how responsibility shifts from human designers to AI agents as systems advance from copilots to autonomous design partners.

</details>


### [369] [SentiFuse: Deep Multi-model Fusion Framework for Robust Sentiment Extraction](https://arxiv.org/abs/2602.01447)
*Hieu Minh Duong,Rupa Ghosh,Cong Hoan Nguyen,Eugene Levin,Todd Gary,Long Nguyen*

Main category: cs.CL

TL;DR: SentiFuse框架通过融合多种情感分析模型，有效提升了情感分析的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有情感分析模型各有优劣，但缺乏统一框架将它们高效整合，难以充分发挥模型互补性。

Method: 提出了SentiFuse框架，包含标准化层，可支持决策级融合、特征级融合和自适应融合等多种策略，实现异构情感模型的系统组合。

Result: 在Crowdflower、GoEmotions和Sentiment140三个社交媒体数据集上实验，SentiFuse稳定优于单一模型和简单集成，特征级融合F1分数最高提升4%，自适应融合在否定、混合情感等复杂情感表现上表现更佳。

Conclusion: 系统性整合不同情感模型的互补优势，可以在多样化文本和数据集上获得更准确、可靠的情感分析效果。

Abstract: Sentiment analysis models exhibit complementary strengths, yet existing approaches lack a unified framework for effective integration. We present SentiFuse, a flexible and model-agnostic framework that integrates heterogeneous sentiment models through a standardization layer and multiple fusion strategies. Our approach supports decision-level fusion, feature-level fusion, and adaptive fusion, enabling systematic combination of diverse models. We conduct experiments on three large-scale social-media datasets: Crowdflower, GoEmotions, and Sentiment140. These experiments show that SentiFuse consistently outperforms individual models and naive ensembles. Feature-level fusion achieves the strongest overall effectiveness, yielding up to 4\% absolute improvement in F1 score over the best individual model and simple averaging, while adaptive fusion enhances robustness on challenging cases such as negation, mixed emotions, and complex sentiment expressions. These results demonstrate that systematically leveraging model complementarity yields more accurate and reliable sentiment analysis across diverse datasets and text types.

</details>


### [370] [Understanding QA generation: Extracting Parametric and Contextual Knowledge with CQA for Low Resource Bangla Language](https://arxiv.org/abs/2602.01451)
*Umme Abira Azmary,MD Ikramul Kayes,Swakkhar Shatabda,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: 本文提出了BanglaCQA，这是首个孟加拉语逆事实问答（Counterfactual QA）数据集，并通过该数据集分析了低资源语言中QA模型对编码知识和上下文输入的依赖。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏结构化的孟加拉语QA数据，难以分析模型在答案生成时到底更依赖预编码知识还是语境信息，尤其是在低资源、语言复杂的背景下。

Method: 1）构建BanglaCQA逆事实数据集，包含逆事实段落和可回答性注释；2）提出针对编码器-解码器（单语和多语）及仅解码器LLM模型的微调和提示方法，分析其对参数化与上下文知识的依赖；3）采用基于LLM和人工的语义相似度评价答案质量；4）系统分析多种QA设置下模型性能，尤其关注链式思考（CoT）提示在逆事实情境中提取参数化知识的效果。

Result: 链式思考（CoT）提示在逆事实场景中，尤其是仅解码器LLM模型中，能更有效地抽取模型的参数化知识；不同类型模型在不同QA设置和场景下表现有显著差异。

Conclusion: 本文不但为孟加拉语QA分析知识来源提供了新框架，还发现了逆事实推理在低资源语言环境下的重要意义，为未来拓展逆事实推理研究奠定了基础。

Abstract: Question-Answering (QA) models for low-resource languages like Bangla face challenges due to limited annotated data and linguistic complexity. A key issue is determining whether models rely more on pre-encoded (parametric) knowledge or contextual input during answer generation, as existing Bangla QA datasets lack the structure required for such analysis. We introduce BanglaCQA, the first Counterfactual QA dataset in Bangla, by extending a Bangla dataset while integrating counterfactual passages and answerability annotations. In addition, we propose fine-tuned pipelines for encoder-decoder language-specific and multilingual baseline models, and prompting-based pipelines for decoder-only LLMs to disentangle parametric and contextual knowledge in both factual and counterfactual scenarios. Furthermore, we apply LLM-based and human evaluation techniques that measure answer quality based on semantic similarity. We also present a detailed analysis of how models perform across different QA settings in low-resource languages, and show that Chain-of-Thought (CoT) prompting reveals a uniquely effective mechanism for extracting parametric knowledge in counterfactual scenarios, particularly in decoder-only LLMs. Our work not only introduces a novel framework for analyzing knowledge sources in Bangla QA but also uncovers critical findings that open up broader directions for counterfactual reasoning in low-resource language settings.

</details>


### [371] [ConPress: Learning Efficient Reasoning from Multi-Question Contextual Pressure](https://arxiv.org/abs/2602.01472)
*Jie Deng,Shining Liang,Jun Li,Hongzhi Li,Yutao Xie*

Main category: cs.CL

TL;DR: 本文发现并利用了大规模推理模型在多问环境下自动压缩推理过程的现象，提出了一种自监督的精调方法，能显著减短推理长度且保持精度。


<details>
  <summary>Details</summary>
Motivation: 现有大规模推理模型在推理密集任务时，常生成过长的答题思路链，导致推理效率低下。缩短推理长度，可显著降低推理开销，但需保证准确性。

Method: 作者首先发现并归纳了一个称为Self-Compression的现象：当模型同时面对多个独立可答问题时，其推理过程自然变短。据此，提出ConPress方法，用多问题提示诱发自压缩，采样模型输出，解析筛选得到简洁且正确的推理轨迹，并用作精调数据进行有监督微调，实现无需外部教师、人工剪枝或强化学习的推理压缩。

Result: ConPress方法仅需8k细调样本，在MATH500数据集上推理token数减少了59%，AIME25数据集减少33%，同时准确率保持竞争力。

Conclusion: 模型能通过自身上下文压力诱导出简洁推理，并用作自举精调数据，有效减少推理开销而不损失准确性，为推理模型效率优化提供了新思路。

Abstract: Large reasoning models (LRMs) typically solve reasoning-intensive tasks by generating long chain-of-thought (CoT) traces, leading to substantial inference overhead. We identify a reproducible inference-time phenomenon, termed Self-Compression: when multiple independent and answerable questions are presented within a single prompt, the model spontaneously produces shorter reasoning traces for each question. This phenomenon arises from multi-question contextual pressure during generation and consistently manifests across models and benchmarks. Building on this observation, we propose ConPress (Learning from Contextual Pressure), a lightweight self-supervised fine-tuning approach. ConPress constructs multi-question prompts to induce self-compression, samples the resulting model outputs, and parses and filters per-question traces to obtain concise yet correct reasoning trajectories. These trajectories are directly used for supervised fine-tuning, internalizing compressed reasoning behavior in single-question settings without external teachers, manual pruning, or reinforcement learning. With only 8k fine-tuning examples, ConPress reduces reasoning token usage by 59% on MATH500 and 33% on AIME25, while maintaining competitive accuracy.

</details>


### [372] [Ebisu: Benchmarking Large Language Models in Japanese Finance](https://arxiv.org/abs/2602.01479)
*Xueqing Peng,Ruoyu Xiang,Fan Zhang,Mingzi Song,Mingyang Jiang,Yan Wang,Lingfei Qian,Taiki Hara,Yuqing Guo,Jimin Huang,Junichi Tsujii,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 本文提出了Ebisu，这是一个针对日语金融语言理解的基准测试，其中包括两个由专家注释的任务，用于评估现有LLMs在复杂日语金融场景下的表现。研究发现，即使是最先进的语言模型在这些任务上表现也不理想。


<details>
  <summary>Details</summary>
Motivation: 日语独特的语言结构、书写体系和高语境交流方式使得金融领域的语言理解对LLM构成极大挑战，现有数据集和评测标准难以覆盖这些特性。该研究旨在填补这一空白，推动多语言环境下金融NLP的进步。

Method: 作者开发了Ebisu基准，包含两个任务：1）投资者问答中对隐性承诺和拒绝的识别（JF-ICR）；2）从专业披露中提取和排序嵌套金融术语（JF-TE）。任务均经专家标注，并测试了多种通用、日语适应和金融专用LLM。

Result: 研究发现，无论模型规模多大，或是否做过语言/领域适应，各类模型在两个任务上的表现都有限，无法稳定取得显著进步，显示该领域挑战性极大。

Conclusion: Ebisu作为公开数据和评测标准，有助于进一步推动日语金融NLP领域，强调了跨语言、跨文化场景中的技术难题，为今后的模型改进提供了参考。

Abstract: Japanese finance combines agglutinative, head-final linguistic structure, mixed writing systems, and high-context communication norms that rely on indirect expression and implicit commitment, posing a substantial challenge for LLMs. We introduce Ebisu, a benchmark for native Japanese financial language understanding, comprising two linguistically and culturally grounded, expert-annotated tasks: JF-ICR, which evaluates implicit commitment and refusal recognition in investor-facing Q&A, and JF-TE, which assesses hierarchical extraction and ranking of nested financial terminology from professional disclosures. We evaluate a diverse set of open-source and proprietary LLMs spanning general-purpose, Japanese-adapted, and financial models. Results show that even state-of-the-art systems struggle on both tasks. While increased model scale yields limited improvements, language- and domain-specific adaptation does not reliably improve performance, leaving substantial gaps unresolved. Ebisu provides a focused benchmark for advancing linguistically and culturally grounded financial NLP. All datasets and evaluation scripts are publicly released.

</details>


### [373] [Alternating Reinforcement Learning for Rubric-Based Reward Modeling in Non-Verifiable LLM Post-Training](https://arxiv.org/abs/2602.01511)
*Ran Xu,Tianci Liu,Zihan Dong,Tony You,Ilgee Hong,Carl Yang,Linjun Zhang,Tao Zhao,Haoyu Wang*

Main category: cs.CL

TL;DR: Rubric-ARM是一种新的奖励建模框架，用于提升复杂、主观领域下的AI输出质量，联合优化评分标准生成器和评分员，通过偏好强化学习实现更精细的输出评估。实验证明其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型只给出单一分数，难以评估如创意写作等主观、多面的生成任务，存在评判粒度粗糙、适应性差问题。

Method: 提出Rubric-ARM框架，通过联合训练评分标准生成器（rubric generator）和评分员（judge），用强化学习从偏好反馈中优化。采用交替优化策略处理同时优化带来的梯度方差问题，并进行了理论分析。

Result: Rubric-ARM在多个基准测试上均取得当前最优表现，在离线和在线强化学习下显著提升策略的一致性和表现。

Conclusion: Rubric-ARM有效提升了AI在主观复杂领域下的奖励建模能力，为更灵活、细致的输出质量评估和价值对齐提供了有力工具。

Abstract: Standard reward models typically predict scalar scores that fail to capture the multifaceted nature of response quality in non-verifiable domains, such as creative writing or open-ended instruction following. To address this limitation, we propose Rubric-ARM, a framework that jointly optimizes a rubric generator and a judge using reinforcement learning from preference feedback. Unlike existing methods that rely on static rubrics or disjoint training pipelines, our approach treats rubric generation as a latent action learned to maximize judgment accuracy. We introduce an alternating optimization strategy to mitigate the non-stationarity of simultaneous updates, providing theoretical analysis that demonstrates how this schedule reduces gradient variance during training. Extensive experiments show that Rubric-ARM achieves state-of-the-art performance among baselines on multiple benchmarks and significantly improves downstream policy alignment in both offline and online reinforcement learning settings.

</details>


### [374] [Argument Rarity-based Originality Assessment for AI-Assisted Writing](https://arxiv.org/abs/2602.01560)
*Keito Inoshita,Michiaki Omura,Tsukasa Yamanaka,Go Maeda,Kentaro Tsuji*

Main category: cs.CL

TL;DR: 本文提出了一种全自动评估学生作文原创性的新方法——基于论证罕见性的原创性评估（AROA），通过结构、论点、证据罕见性及认知深度四个维度对作文进行分析。实验发现，高质量文本常采用常规论点模式，其原创性相对较低，而AI生成文本虽结构复杂但论点原创性不足。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）能够轻松生成高质量文本，传统以写作质量为核心的评价方式逐渐失去意义。如果教育的本质是培养批判性思维和原创新观点，则写作评估也需从“质量”转向“原创性”。

Method: AROA框架将“原创性”定义为在参考语料库中的罕见性，从结构罕见性、论点罕见性、证据罕见性和认知深度四方面量化作文各部分的“稀有度”，并结合质量调整机制，实现对作文质量与原创性的独立评估。

Result: 在人类作文与AI作文上的实验显示，文本质量与论点罕见性呈强负相关，即高质量文本往往使用常见论点模式。AI作文在结构复杂度上能与人类相当，但论点罕见性远低于人类作文，显示LLMs在观点原创性上有限。

Conclusion: AROA为写作原创性评估提供了新视角和工具，质量和原创性可以独立衡量，有助于促进教育评价范式从“质量导向”向“原创导向”转变。

Abstract: As Large Language Models (LLMs) have become capable of effortlessly generating high-quality text, traditional quality-focused writing assessment is losing its significance. If the essential goal of education is to foster critical thinking and original perspectives, assessment must also shift its paradigm from quality to originality. This study proposes Argument Rarity-based Originality Assessment (AROA), a framework for automatically evaluating argumentative originality in student essays. AROA defines originality as rarity within a reference corpus and evaluates it through four complementary components: structural rarity, claim rarity, evidence rarity, and cognitive depth. The framework quantifies the rarity of each component using density estimation and integrates them with a quality adjustment mechanism, thereby treating quality and originality as independent evaluation axes. Experiments using human essays and AI-generated essays revealed a strong negative correlation between quality and claim rarity, demonstrating a quality-originality trade-off where higher-quality texts tend to rely on typical claim patterns. Furthermore, while AI essays achieved comparable levels of structural complexity to human essays, their claim rarity was substantially lower than that of humans, indicating that LLMs can reproduce the form of argumentation but have limitations in the originality of content.

</details>


### [375] [FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents](https://arxiv.org/abs/2602.01566)
*Chiwei Zhu,Benfeng Xu,Mingxuan Du,Shaohan Wang,Xiaorui Wang,Zhendong Mao,Yongdong Zhang*

Main category: cs.CL

TL;DR: 本文提出FS-Researcher系统，突破大语言模型深度研究任务中的上下文窗口限制，实现更大规模的信息收集与报告撰写，显著提升报告质量。


<details>
  <summary>Details</summary>
Motivation: 深度研究任务涉及长任务序列，常超出现有模型的上下文窗口上限，导致证据采集和报告写作受限，影响最终效果。因此，亟需突破上下文窗口限制，支持更复杂、更高质量的研究任务。

Method: FS-Researcher采用基于文件系统的双智能体架构。在该体系下，Context Builder智能体如同图书管理员，负责浏览网络、结构化记录笔记及整理原始资料到可扩展的分层知识库。Report Writer智能体则逐节高效撰写研究报告，并将知识库作为事实依据。文件系统既是持久化的外部记忆，也是智能体间、会话间的共享协调媒介，从而实现超越上下文长度的迭代研究。

Result: 在DeepResearch Bench和DeepConsult两个开放性基准上，FS-Researcher在多种主干模型上均取得业界领先的报告质量。进一步分析显示，Context Builder分配的算力越多，最终报告的质量越高，验证了文件系统范式下的有效推理扩展能力。

Conclusion: FS-Researcher突破了大语言模型在深度研究中的上下文窗口瓶颈，通过外部文件系统实现了高质量、可扩展的研究智能体系统，为复杂长任务序列下的自动化研究和写作提供了新范式。

Abstract: Deep research is emerging as a representative long-horizon task for large language model (LLM) agents. However, long trajectories in deep research often exceed model context limits, compressing token budgets for both evidence collection and report writing, and preventing effective test-time scaling. We introduce FS-Researcher, a file-system-based, dual-agent framework that scales deep research beyond the context window via a persistent workspace. Specifically, a Context Builder agent acts as a librarian which browses the internet, writes structured notes, and archives raw sources into a hierarchical knowledge base that can grow far beyond context length. A Report Writer agent then composes the final report section by section, treating the knowledge base as the source of facts. In this framework, the file system serves as a durable external memory and a shared coordination medium across agents and sessions, enabling iterative refinement beyond the context window. Experiments on two open-ended benchmarks (DeepResearch Bench and DeepConsult) show that FS-Researcher achieves state-of-the-art report quality across different backbone models. Further analyses demonstrate a positive correlation between final report quality and the computation allocated to the Context Builder, validating effective test-time scaling under the file-system paradigm. The code and data are anonymously open-sourced at https://github.com/Ignoramus0817/FS-Researcher.

</details>


### [376] [LLM-based Embeddings: Attention Values Encode Sentence Semantics Better Than Hidden States](https://arxiv.org/abs/2602.01572)
*Yeqin Zhang,Yunfei Wang,Jiaxuan Chen,Ke Qin,Yizheng Zhao,Cam-Tu Nguyen*

Main category: cs.CL

TL;DR: 该论文提出了一种新方法通过聚合注意力值向量（而不仅仅是隐藏状态）来获得更优的句子表示，并在无需训练的设置下达到甚至超越现有最佳方法的表现。


<details>
  <summary>Details</summary>
Motivation: 目前主流方法通过大型语言模型(LLM)的最终层隐藏状态来提取句子表示，但这些隐藏状态主要为下一个词预测而优化，未能有效捕捉句子级的全局语义信息。作者希望找到更适合无监督语义表示提取的方法。

Method: 作者提出了“Value Aggregation”(VA)方法，通过跨不同层和token聚合注意力机制中的value向量来生成句子表示。更进一步，作者基于特定的prompt与计算加权聚合，提出了“Aligned Weighted VA”(AlignedWVA)算法，使聚合后的表示与LLM的残差空间更好地对齐。整个过程无需对LLM进行额外训练。

Result: 实验结果显示，无需训练的VA方法性能优于其他同类LLM句子嵌入方案，经过对齐加权后的AlignedWVA在多项句子嵌入任务中超越了昂贵的MetaEOL集成方法，取得了新的无训练SOTA表现。

Conclusion: 聚合和对齐注意力value向量是一种高效、无需训练且性能优越的句子表征方法，为基于LLM的冷启动或无监督语义抽取带来更具潜力的解决方案。

Abstract: Sentence representations are foundational to many Natural Language Processing (NLP) applications. While recent methods leverage Large Language Models (LLMs) to derive sentence representations, most rely on final-layer hidden states, which are optimized for next-token prediction and thus often fail to capture global, sentence-level semantics. This paper introduces a novel perspective, demonstrating that attention value vectors capture sentence semantics more effectively than hidden states. We propose Value Aggregation (VA), a simple method that pools token values across multiple layers and token indices. In a training-free setting, VA outperforms other LLM-based embeddings, even matches or surpasses the ensemble-based MetaEOL. Furthermore, we demonstrate that when paired with suitable prompts, the layer attention outputs can be interpreted as aligned weighted value vectors. Specifically, the attention scores of the last token function as the weights, while the output projection matrix ($W_O$) aligns these weighted value vectors with the common space of the LLM residual stream. This refined method, termed Aligned Weighted VA (AlignedWVA), achieves state-of-the-art performance among training-free LLM-based embeddings, outperforming the high-cost MetaEOL by a substantial margin. Finally, we highlight the potential of obtaining strong LLM embedding models through fine-tuning Value Aggregation.

</details>


### [377] [Provable Defense Framework for LLM Jailbreaks via Noise-Augumented Alignment](https://arxiv.org/abs/2602.01587)
*Zehua Cheng,Jianwei Yang,Wei Dai,Jiahao Sun*

Main category: cs.CL

TL;DR: 本文提出了一种可认证的语义平滑方法（CSS），大幅提升大语言模型（LLM）对攻击的鲁棒性，并显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在应对自适应越狱攻击时，现有依赖经验防御的方法（如GCG）依旧存在较大漏洞，急需提供更稳定和可证明安全性的防护机制。

Method: 1. 提出Certified Semantic Smoothing（CSS）机制，通过Stratified Randomized Ablation对输入进行不变结构提示和可变载荷的分区，结合超几何分布进行严格的l0范数非对称性保证。
2. 针对稀疏上下文下的性能下降，引入噪声增强对齐微调（NAAT），将模型优化为语义去噪器。

Result: 在Llama-3模型上的测试表明，所提方法能将基于梯度的攻击成功率从84.2%降至1.2%，同时保持94.1%的正常用例效能，明显优于现有基于字符级的防护（效能降至74.3%）。

Conclusion: 本框架为大模型防越狱攻击提供了确定性安全认证，保证模型在可证范围内对所有对抗变种都具备鲁棒性，对安全对话系统具有重要意义。

Abstract: Large Language Models (LLMs) remain vulnerable to adaptive jailbreaks that easily bypass empirical defenses like GCG. We propose a framework for certifiable robustness that shifts safety guarantees from single-pass inference to the statistical stability of an ensemble. We introduce Certified Semantic Smoothing (CSS) via Stratified Randomized Ablation, a technique that partitions inputs into immutable structural prompts and mutable payloads to derive rigorous lo norm guarantees using the Hypergeometric distribution. To resolve performance degradation on sparse contexts, we employ Noise-Augmented Alignment Tuning (NAAT), which transforms the base model into a semantic denoiser. Extensive experiments on Llama-3 show that our method reduces the Attack Success Rate of gradient-based attacks from 84.2% to 1.2% while maintaining 94.1% benign utility, significantly outperforming character-level baselines which degrade utility to 74.3%. This framework provides a deterministic certificate of safety, ensuring that a model remains robust against all adversarial variants within a provable radius.

</details>


### [378] [Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles](https://arxiv.org/abs/2602.01590)
*Shaohan Wang,Benfeng Xu,Licheng Zhang,Mingxuan Du,Chiwei Zhu,Xiaorui Wang,Zhendong Mao,Yongdong Zhang*

Main category: cs.CL

TL;DR: 本文提出了Wiki Live Challenge（WLC）基准和对应的Wiki Eval评估体系，用最新的维基百科优质条目（GA）为高标准、细粒度地评测DRAs（深度研究智能体）在信息检索与报告生成方面的能力。实验结果显示，现有DRAs与人类专家仍有较大差距。


<details>
  <summary>Details</summary>
Motivation: 现有DRAs通常用大语言模型生成的参考答案或评价维度进行评估，这虽然具有可扩展性，但往往缺乏权威性、客观性和细致度。作者希望开发一个高可靠性且能细致评估智能体表现的新框架。

Method: 作者利用最近100篇维基百科优质条目，设计了Wiki Live Challenge基准和Wiki Eval评测体系，包括39个写作质量评价维度和严格的事实可验证性指标，用于细粒度比较智能体与人类专家表现。

Result: 对多种DRA系统的大量实验表明，这些系统与人类写作的专家级维基条目之间存在显著差距，Wiki Live Challenge能有效体现不同DRA的优劣。

Conclusion: WLC基准和Wiki Eval框架为智能体评测提供了更权威、更细致和客观的标准，有助于推动深度研究智能体的发展。

Abstract: Deep Research Agents (DRAs) have demonstrated remarkable capabilities in autonomous information retrieval and report generation, showing great potential to assist humans in complex research tasks. Current evaluation frameworks primarily rely on LLM-generated references or LLM-derived evaluation dimensions. While these approaches offer scalability, they often lack the reliability of expert-verified content and struggle to provide objective, fine-grained assessments of critical dimensions. To bridge this gap, we introduce Wiki Live Challenge (WLC), a live benchmark that leverages the newest Wikipedia Good Articles (GAs) as expert-level references. Wikipedia's strict standards for neutrality, comprehensiveness, and verifiability serve as a great challenge for DRAs, with GAs representing the pinnacle of which. We curate a dataset of 100 recent Good Articles and propose Wiki Eval, a comprehensive evaluation framework comprising a fine-grained evaluation method with 39 criteria for writing quality and rigorous metrics for factual verifiability. Extensive experiments on various DRA systems demonstrate a significant gap between current DRAs and human expert-level Wikipedia articles, validating the effectiveness of WLC in advancing agent research. We release our benchmark at https://github.com/WangShao2000/Wiki_Live_Challenge

</details>


### [379] [The Art of Socratic Inquiry: A Framework for Proactive Template-Guided Therapeutic Conversation Generation](https://arxiv.org/abs/2602.01598)
*Mingwen Zhang,Minqiang Yang,Changsheng Ma,Yang Yu,Hui Bai,Chen Xu,Xiangzhen Kong,Bin Hu*

Main category: cs.CL

TL;DR: 本文提出Socratic Inquiry Framework (SIF)，显著提升了心理学大模型的主动提问能力，使其从被动应答转变为主动引导。


<details>
  <summary>Details</summary>
Motivation: 当前的心理学大语言模型多数只能被动回应用户，往往停留在表面安慰，缺乏对潜在信念的深度探索和行为引导，无法有效模拟认知行为疗法中的主动提问核心机制。

Method: 提出了Socratic Inquiry Framework（SIF）——一个轻量、即插即用的意图规划器，将“何时发问”和“发问内容”解耦，分别通过策略锚定和模板检索实现，无需端到端重新训练模型。同时构建了Socratic-QA数据集，用于训练和监督主动提问过程。

Result: 实验表明，引入SIF后，模型的主动提问频率、对话深度及治疗一致性都显著提升，有效地从被动安抚转变为主动探索。

Conclusion: SIF为心理学大模型树立了新范式，实现了从反应式回应向主动认知引导的转变，为后续心理健康AI工具的发展开辟了新方向。

Abstract: Proactive questioning, where therapists deliberately initiate structured, cognition-guiding inquiries, is a cornerstone of cognitive behavioral therapy (CBT). Yet, current psychological large language models (LLMs) remain overwhelmingly reactive, defaulting to empathetic but superficial responses that fail to surface latent beliefs or guide behavioral change. To bridge this gap, we propose the \textbf{Socratic Inquiry Framework (SIF)}, a lightweight, plug-and-play therapeutic intent planner that transforms LLMs from passive listeners into active cognitive guides. SIF decouples \textbf{when to ask} (via Strategy Anchoring) from \textbf{what to ask} (via Template Retrieval), enabling context-aware, theory-grounded questioning without end-to-end retraining. Complementing SIF, we introduce \textbf{Socratic-QA}, a high-quality dataset of strategy-aligned Socratic sequences that provides explicit supervision for proactive reasoning. Experiments show that SIF significantly enhances proactive questioning frequency, conversational depth, and therapeutic alignment, marking a clear shift from reactive comfort to proactive exploration. Our work establishes a new paradigm for psychologically informed LLMs: not just to respond, but to guide.

</details>


### [380] [SEA-Guard: Culturally Grounded Multilingual Safeguard for Southeast Asia](https://arxiv.org/abs/2602.01618)
*Panuthep Tasawong,Jian Gang Ngui,Alham Fikri Aji,Trevor Cohn,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: 本文提出了一个新颖的智能体驱动数据生成框架，用于大规模构建东南亚（SEA）本地和多语种的AI安全数据集，并据此开发了SEA-Guard系列模型，在本地文化环境下提升了AI安全防护性能。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全模型多基于英文数据，通过机器翻译获得其他语言数据，缺乏对不同地区文化与规范的准确把握，导致模型难以灵敏识别本地有害内容。因此亟需方法以低成本高效率地生产贴合本地文化的安全数据和模型。

Method: 作者提出了智能体驱动的数据生成框架，自动化生成涵盖东南亚各国、各文化敏感点的安全数据集，并基于这些数据训练多语种的SEA-Guard安全模型。评测采用了多基准和多种文化变体。

Result: 实验结果显示，SEA-Guard系列模型在检测东南亚本地有害/敏感内容方面较现有方法明显提升，并且在通用安全场景下表现依然稳定。

Conclusion: SEA-Guard为东南亚及多文化环境下AI安全对齐提供了更具本地适应性的解决方案。新方法不仅提升了检测能力，还证明了智能体生成数据的可行性和实际有效性。

Abstract: Culturally aware safeguards are crucial for AI alignment in real-world settings, where safety extends beyond common sense and encompasses diverse local values, norms, and region-specific regulations. However, building large-scale, culturally grounded datasets is challenging due to limited resources and a scarcity of native annotators. Consequently, many safeguard models rely on machine translation of English datasets, often missing regional and cultural nuances. We present a novel agentic data-generation framework to scalably create authentic, region-specific safety datasets for Southeast Asia (SEA). On this foundation, we introduce the SEA-Guard family, the first multilingual safeguard models grounded in SEA cultural contexts. Evaluated across multiple benchmarks and cultural variants, SEA-Guard consistently outperforms existing safeguards at detecting regionally sensitive or harmful content while maintaining strong general safety performance.

</details>


### [381] [A2Eval: Agentic and Automated Evaluation for Embodied Brain](https://arxiv.org/abs/2602.01640)
*Shuai Zhang,Jiayu Hu,Zijie Chen,Zeyuan Ding,Yi Zhang,Yingji Zhang,Ziyi Zhou,Junwei Liao,Shengjie Zhou,Yong Dai,Zhenzhong Lan,Xiaozhu Ju*

Main category: cs.CL

TL;DR: 本文提出了一种自动化的具身视觉语言模型（VLM）评测框架A2Eval，通过两种智能代理协作，实现高效、自动、低成本的评测流程，显著提高了评测效率并修正了模型排名偏差。


<details>
  <summary>Details</summary>
Motivation: 现有的具身VLM评测依赖人工构建的静态基准集，不仅重复性高、覆盖面失衡，还极度依赖计算与人工标注资源，导致成本上升和模型排名失真，严重制约了模型的持续迭代和发展。

Method: A2Eval框架包含数据代理与评测代理：数据代理自动发现模型能力维度并生成均衡的精简评测集；评测代理自动构建和校验评测流程，实现全流程的自动化高保真评测。

Result: 在10个基准和13个模型上的实验证明，A2Eval能将评测集压缩85%、总体计算成本降低77%、评测速度提升4.6倍，且保障评测质量不变。此外，该框架有效修正排名偏差，提高了与人工评测的一致性（Spearman’s rho=0.85、Kendall’s tau=0.81）。

Conclusion: A2Eval为具身VLM评测树立了更高效、更公正、更低成本的新标准，有望推动领域发展。代码和数据也将开源，促进社区应用。

Abstract: Current embodied VLM evaluation relies on static, expert-defined, manually annotated benchmarks that exhibit severe redundancy and coverage imbalance. This labor intensive paradigm drains computational and annotation resources, inflates costs, and distorts model rankings, ultimately stifling iterative development. To address this, we propose Agentic Automatic Evaluation (A2Eval), the first agentic framework that automates benchmark curation and evaluation through two collaborative agents. The Data Agent autonomously induces capability dimensions and assembles a balanced, compact evaluation suite, while the Eval Agent synthesizes and validates executable evaluation pipelines, enabling fully autonomous, high-fidelity assessment. Evaluated across 10 benchmarks and 13 models, A2Eval compresses evaluation suites by 85%, reduces overall computational costs by 77%, and delivers a 4.6x speedup while preserving evaluation quality. Crucially, A2Eval corrects systematic ranking biases, improves human alignment to Spearman's rho=0.85, and maintains high ranking fidelity (Kendall's tau=0.81), establishing a new standard for high-fidelity, low-cost embodied assessment. Our code and data will be public soon.

</details>


### [382] [Steering Vector Fields for Context-Aware Inference-Time Control in Large Language Models](https://arxiv.org/abs/2602.01654)
*Jiaqian Li,Yanshu Li,Kuan-Hao Huang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Steering Vector Fields (SVF)的新方法，通过在推理时动态调整大型语言模型（LLM）不同激活状态下的引导方向，实现更强大且可靠的模型控制。该方法解决了现有Steering vectors（SV）在复杂任务和长文本生成场景下效果不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 当前主流的Steering vectors（SV）方法，虽然推理时高效、无需微调，但依赖于一个静态向量，容易在不同语境下失效，导致控制效果不佳或反向作用，特别是在长文本生成和多属性控制场景下问题更为突出。研究动机在于探索更精细且上下文相关的引导方式，从而提升LLM的可控性与实践能力。

Method: 作者将引导问题从几何视角重新审视，提出Steering Vector Fields（SVF）框架。SVF通过学习一个可微分的概念评分函数，并利用当前激活点处的梯度作为引导方向，实现层间协同的上下文相关干预。该方法统一支持多层、多属性和长文本的引导。

Result: 在多种LLM和多项引导任务上，SVF相比传统SV方法展现出更强、更可靠的控制能力，尤其是在长文本生成和多属性控制场景中效果显著提升。

Conclusion: SVF极大提升了推理时控制LLM的可实用性与稳定性，为复杂场景下的高效模型引导提供了统一且有效的方法。

Abstract: Steering vectors (SVs) offer a lightweight way to control large language models (LLMs) at inference time by shifting hidden activations, providing a practical middle ground between prompting and fine-tuning. Yet SVs can be unreliable in practice. Some concepts are unsteerable, and even when steering helps on average it can backfire for a non-trivial fraction of inputs. Reliability also degrades in long-form generation and multi-attribute steering. We take a geometric view of these failures. A static SV applies the same update vector everywhere in representation space, implicitly assuming that the concept-improving direction is constant across contexts. When the locally effective direction varies with the current activation, a single global vector can become misaligned, which yields weak or reversed effects. Guided by this perspective, we propose Steering Vector Fields (SVF), which learns a differentiable concept scoring function whose local gradient defines the steering direction at each activation, making interventions explicitly context-dependent. This formulation supports coordinated multi-layer interventions in a shared, aligned concept space, and enables efficient long-form and multi-attribute control within a unified framework. Across multiple LLMs and steering tasks, SVF delivers stronger and more reliable control, improving the practicality of inference-time steering.

</details>


### [383] [CoDiQ: Test-Time Scaling for Controllable Difficult Question Generation](https://arxiv.org/abs/2602.01660)
*Zhongyuan Peng,Caijun Xu,Changyi Xiao,Shibo Hong,Eli Zhang,Stephen Huang,Yixin Cao*

Main category: cs.CL

TL;DR: 提出了一种新框架CoDiQ，能精准控制自动生成问题的难度，用于改进大型推理模型（LRM）训练，并发布了高质量难题数据集。


<details>
  <summary>Details</summary>
Motivation: 提升大型推理模型在高难度、竞赛级问题上的表现。现有自动出题方法难以精准控制难度、开销大、难以大规模生成高质量难题，限制了LRM能力提升。

Method: 提出CoDiQ框架，结合测试时推理token配额控制问题难度，并保证题目可解。对Qwen3-8B模型进行改造生成更高难度问题，基于此生成了4.4万条竞赛级高质量问题的数据集（CoDiQ-Corpus）。这些题目经过人工评估难度高且可解。

Result: 人工评估显示CoDiQ-Corpus中的问题难度高于现有基准，同时仍有82%以上的可解性。用该数据集训练LRM显著提升了其推理能力，证明了精细控制难度对模型训练的价值。

Conclusion: CoDiQ能够高效、精细地自动生成高质量、高难度的训练题，有效增强大型推理模型的能力，并公开了数据集与工具，促进后续研究。

Abstract: Large Reasoning Models (LRMs) benefit substantially from training on challenging competition-level questions. However, existing automated question synthesis methods lack precise difficulty control, incur high computational costs, and struggle to generate competition-level questions at scale. In this paper, we propose CoDiQ (Controllable Difficult Question Generation), a novel framework enabling fine-grained difficulty control via test-time scaling while ensuring question solvability. Specifically, first, we identify a test-time scaling tendency (extended reasoning token budget boosts difficulty but reduces solvability) and the intrinsic properties defining the upper bound of a model's ability to generate valid, high-difficulty questions. Then, we develop CoDiQ-Generator from Qwen3-8B, which improves the upper bound of difficult question generation, making it particularly well-suited for challenging question construction. Building on the CoDiQ framework, we build CoDiQ-Corpus (44K competition-grade question sequences). Human evaluations show these questions are significantly more challenging than LiveCodeBench/AIME with over 82% solvability. Training LRMs on CoDiQ-Corpus substantially improves reasoning performance, verifying that scaling controlled-difficulty training questions enhances reasoning capabilities. We open-source CoDiQ-Corpus, CoDiQ-Generator, and implementations to support related research.

</details>


### [384] [Scaling Search-Augmented LLM Reasoning via Adaptive Information Control](https://arxiv.org/abs/2602.01672)
*Siheng Xiong,Oguzhan Gungordu,Blair Johnson,James C. Kerce,Faramarz Fekri*

Main category: cs.CL

TL;DR: 提出了一种名为DeepControl的信息获取自适应控制框架，有效提升了检索增强推理智能体的性能。


<details>
  <summary>Details</summary>
Motivation: 目前检索增强推理模型虽能外部检索信息，但检索过程常带来冗余、上下文饱和等问题，现有方法多基于强化学习，仅依赖最终结果，难以精细调控信息获取过程。

Method: 提出基于“信息效用”理论的DeepControl框架，通过衡量检索证据的边际价值，设计了检索延续和粒度控制机制，自适应调节检索时机和信息扩展量。此外采用退火策略，帮助智能体在训练中学会高效的信息获取行为。

Result: 在七个基准任务上，DeepControl在Qwen2.5-7B和Qwen2.5-3B模型上分别相比强基线提升9.4%和8.6%，并在多种检索/非检索推理方法中表现突出。

Conclusion: 自适应信息控制对提升检索增强推理模型的复杂环境适应性至关重要，DeepControl显著优化了信息获取策略和模型最终表现。

Abstract: Search-augmented reasoning agents interleave multi-step reasoning with external information retrieval, but uncontrolled retrieval often leads to redundant evidence, context saturation, and unstable learning. Existing approaches rely on outcome-based reinforcement learning (RL), which provides limited guidance for regulating information acquisition. We propose DeepControl, a framework for adaptive information control based on a formal notion of information utility, which measures the marginal value of retrieved evidence under a given reasoning state. Building on this utility, we introduce retrieval continuation and granularity control mechanisms that selectively regulate when to continue and stop retrieval, and how much information to expand. An annealed control strategy enables the agent to internalize effective information acquisition behaviors during training. Extensive experiments across seven benchmarks demonstrate that our method consistently outperforms strong baselines. In particular, our approach achieves average performance improvements of 9.4% and 8.6% on Qwen2.5-7B and Qwen2.5-3B, respectively, over strong outcome-based RL baselines, and consistently outperforms both retrieval-free and retrieval-based reasoning methods without explicit information control. These results highlight the importance of adaptive information control for scaling search-augmented reasoning agents to complex, real-world information environments.

</details>


### [385] [Counting Hypothesis: Potential Mechanism of In-Context Learning](https://arxiv.org/abs/2602.01687)
*Jung H. Lee,Sujith Vijayan*

Main category: cs.CL

TL;DR: 本文分析了大语言模型（LLMs）在上下文学习（ICL）中的能力，提出了其编码策略可能是实现ICL的核心机制。


<details>
  <summary>Details</summary>
Motivation: 上下文学习允许预训练大语言模型无需修改内结构、仅凭少量示例完成多种任务；但其机理尚不清楚，难以纠错和诊断，因此有必要深入理解其本质和局限。

Method: 受ICL性质和LLMs功能模块启发，作者提出了“计数假设”，即LLMs可能通过特定的编码策略实现ICL，并为此提供了辅助性证据。

Result: 实验对ICL机制进行了探索，找到了支持“计数假设”的事实与证据。

Conclusion: LLMs对ICL的支持核心在于其编码策略，本文的研究有助于深入理解ICL的局限性和原理，为模型优化与应用提供了理论基础。

Abstract: In-Context Learning (ICL) indicates that large language models (LLMs) pretrained on a massive amount of data can learn specific tasks from input prompts' examples. ICL is notable for two reasons. First, it does not need modification of LLMs' internal structure. Second, it enables LLMs to perform a wide range of tasks/functions with a few examples demonstrating a desirable task. ICL opens up new ways to utilize LLMs in more domains, but its underlying mechanisms still remain poorly understood, making error correction and diagnosis extremely challenging. Thus, it is imperative that we better understand the limitations of ICL and how exactly LLMs support ICL. Inspired by ICL properties and LLMs' functional modules, we propose 1the counting hypothesis' of ICL, which suggests that LLMs' encoding strategy may underlie ICL, and provide supporting evidence.

</details>


### [386] [Restoring Exploration after Post-Training: Latent Exploration Decoding for Large Reasoning Models](https://arxiv.org/abs/2602.01698)
*Wenhui Tan,Fiorenzo Parascandolo,Enver Sangineto,Jianzhong Ju,Zhenbo Luo,Qian Cao,Rita Cucchiara,Ruihua Song,Jian Luan*

Main category: cs.CL

TL;DR: 本文发现强化学习（RL）后训练的大型推理模型（LRM）在多样性采样时存在“探索塌缩”问题，并提出了一种新的推理解码策略Latent Exploration Decoding（LED），有效提升了不同基准上的推理准确率。


<details>
  <summary>Details</summary>
Motivation: 近年来通过强化学习后训练，大型推理模型在数学与代码推理任务上表现突出。然而，期间采取的后训练方式导致温度采样在增加通过率（pass@$n$）方面失效，即使增加采样多样性也没有提升效果。作者分析到后验概率分布的熵结构，揭示了模型不同层间的熵不对称问题。

Method: 作者提出Latent Exploration Decoding（LED）方法，即利用中间层的高熵潜在表示，在解码时通过累计求和聚合不同层的后验分布，并且自动选择熵值最大的层作为采样候选，无需额外训练或增加参数。

Result: 在多个推理相关的数据集和模型实验中，LED方法无需再训练或引入新参数即可将pass@1与pass@16准确率分别提升了0.61和1.03个百分点。

Conclusion: LED方法通过利用网络中间层的高熵表达，有效缓解了RL后训练导致的探索塌缩问题，从而进一步提升了大规模推理模型的多样性与推理性能。

Abstract: Large Reasoning Models (LRMs) have recently achieved strong mathematical and code reasoning performance through Reinforcement Learning (RL) post-training. However, we show that modern reasoning post-training induces an unintended exploration collapse: temperature-based sampling no longer increases pass@$n$ accuracy. Empirically, the final-layer posterior of post-trained LRMs exhibit sharply reduced entropy, while the entropy of intermediate layers remains relatively high. Motivated by this entropy asymmetry, we propose Latent Exploration Decoding (LED), a depth-conditioned decoding strategy. LED aggregates intermediate posteriors via cumulative sum and selects depth configurations with maximal entropy as exploration candidates. Without additional training or parameters, LED consistently improves pass@1 and pass@16 accuracy by 0.61 and 1.03 percentage points across multiple reasoning benchmarks and models. Project page: https://GitHub.com/Xiaomi-Research/LED.

</details>


### [387] [Game of Thought: Robust Information Seeking with Large Language Models Using Game Theory](https://arxiv.org/abs/2602.01708)
*Langyuan Cui,Chun Kai Ling,Hwee Tou Ng*

Main category: cs.CL

TL;DR: 本文将LLM的信息检索能力转化为"二十问"游戏及其对抗变体，提出了Game of Thought (GoT)框架，通过博弈论方法优化在最坏情况下的信息寻求能力。实验显示该方法较现有手段有更好表现。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在缺乏足够信息完成任务时，主动寻找缺失信息的能力有限，现有方法为简化处理常做出不现实假设，导致在最坏情况下性能下降，这对高风险应用领域影响严重。

Method: 作者用"二十问"游戏评估LLM的信息获取能力，并将其对抗场景重新定义为两人零和扩展式博弈（SLS问题），进而提出Game of Thought（GoT）框架，采用博弈论技巧逼近纳什均衡，从而提升最坏情况表现。

Result: 实验结果表明，GoT方法在所有测试设置下，最坏情况表现均优于（1）直接提示法和（2）启发式搜索法。

Conclusion: 利用博弈论建模和策略优化，作者显著提升了LLM在缺乏信息时的信息获取能力，尤其保障了高风险场景下的最坏表现。

Abstract: Large Language Models (LLMs) are increasingly deployed in real-world scenarios where they may lack sufficient information to complete a given task. In such settings, the ability to actively seek out missing information becomes a critical capability. Existing approaches to enhancing this ability often rely on simplifying assumptions that degrade \textit{worst-case} performance. This is an issue with serious implications in high-stakes applications. In this work, we use the game of Twenty Questions to evaluate the information-seeking ability of LLMs. We introduce and formalize its adversarial counterpart, the Strategic Language Search (SLS) problem along with its variants as a two-player zero-sum extensive form game. We propose Game of Thought (GoT), a framework that applies game-theoretic techniques to approximate a Nash equilibrium (NE) strategy for the restricted variant of the game. Empirical results demonstrate that our approach consistently improves worst-case performance compared to (1) direct prompting-based methods and (2) heuristic-guided search methods across all tested settings.

</details>


### [388] [ARTIS: Agentic Risk-Aware Test-Time Scaling via Iterative Simulation](https://arxiv.org/abs/2602.01709)
*Xingshan Zeng,Lingzhi Wang,Weiwen Liu,Liangyou Li,Yasheng Wang,Lifeng Shang,Xin Jiang,Qun Liu*

Main category: cs.CL

TL;DR: 该论文提出了一种新的测试时扩展计算方法——Agentic Risk-Aware Test-Time Scaling via Iterative Simulation（ARTIS），通过仿真优先于真实执行，有效提高了LLM作为智能体在不可逆、风险行为中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在推理时通过增加计算量提升性能，但对涉及外部环境交互、风险不可逆的智能体场景仍不足。因为，真实行动的失误代价高昂，需要在测试时安全地探索最优操作路径。

Method: ARTIS 框架将探索与承诺解耦，允许模型在真实执行前，通过反复仿真与环境交互以消除不确定性。同时，作者发现普通LLM仿真器难以模拟罕见风险失败。针对这一问题，论文设计了一种风险感知的工具型仿真器，通过有针对性地生成数据、再平衡训练，强化模型对高风险失败的识别和应对。

Result: 实验表明，多轮、多步智能体基准任务上，迭代仿真显著提升了智能体决策的可靠性。引入风险感知仿真器后，这一提升在不同模型与任务间更加稳定和一致。

Conclusion: ARTIS 框架有效提高了大语言模型在高风险环境下作为智能体决策者的安全性和鲁棒性，风险感知仿真是达成这一目标的关键。

Abstract: Current test-time scaling (TTS) techniques enhance large language model (LLM) performance by allocating additional computation at inference time, yet they remain insufficient for agentic settings, where actions directly interact with external environments and their effects can be irreversible and costly. We propose \emph{\name}, \emph{\underline{A}gentic \underline{R}isk-Aware \underline{T}est-Time Scaling via \underline{I}terative \underline{S}imulation}, a framework that decouples exploration from commitment by enabling test-time exploration through simulated interactions prior to real-world execution. This design allows extending inference-time computation to improve action-level reliability and robustness without incurring environmental risk. We further show that naive LLM-based simulators struggle to capture rare but high-impact failure modes, substantially limiting their effectiveness for agentic decision making. To address this limitation, we introduce a \emph{risk-aware tool simulator} that emphasizes fidelity on failure-inducing actions via targeted data generation and rebalanced training. Experiments on multi-turn and multi-step agentic benchmarks demonstrate that iterative simulation substantially improves agent reliability, and that risk-aware simulation is essential for consistently realizing these gains across models and tasks.

</details>


### [389] [MedAraBench: Large-Scale Arabic Medical Question Answering Dataset and Benchmark](https://arxiv.org/abs/2602.01714)
*Mouath Abu-Daoud,Leen Kharouf,Omar El Hajj,Dana El Samad,Mariam Al-Omari,Jihad Mallat,Khaled Saleh,Nizar Habash,Farah E. Shamout*

Main category: cs.CL

TL;DR: 该论文介绍了MedAraBench，这是一个涵盖多种医学专业的阿拉伯语医学问答大数据集，旨在填补阿语医学NLP领域数据稀缺的问题，并用它评测了多款大模型，揭示提升模型专业性和多语性的必要性。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语医学NLP数据和基准匮乏，影响大模型多语种能力的评估和发展，需要开发公开、优质的数据集推动该领域进步。

Method: 通过人工数字化阿拉伯地区医学专业人员的学术材料，构建大规模阿拉伯语多选医学问答数据集；经过预处理后划分训练和测试集；采用专家人工评估和LLM-自动判分两种框架评价数据质量；最后用该数据集对八个前沿开源及闭源大模型进行基准测试。

Result: 构建了覆盖19个医学专业、5个难度层次的数据集，经过双重质量评估，数据多样且高质量。多款主流模型在该数据集上的表现显示需进一步加强专业化和阿拉伯语处理能力。

Conclusion: 公开发布数据集和评测脚本，丰富医学NLP领域的多语种基准，推动大模型在临床场景的多语言适用性和性能提升。

Abstract: Arabic remains one of the most underrepresented languages in natural language processing research, particularly in medical applications, due to the limited availability of open-source data and benchmarks. The lack of resources hinders efforts to evaluate and advance the multilingual capabilities of Large Language Models (LLMs). In this paper, we introduce MedAraBench, a large-scale dataset consisting of Arabic multiple-choice question-answer pairs across various medical specialties. We constructed the dataset by manually digitizing a large repository of academic materials created by medical professionals in the Arabic-speaking region. We then conducted extensive preprocessing and split the dataset into training and test sets to support future research efforts in the area. To assess the quality of the data, we adopted two frameworks, namely expert human evaluation and LLM-as-a-judge. Our dataset is diverse and of high quality, spanning 19 specialties and five difficulty levels. For benchmarking purposes, we assessed the performance of eight state-of-the-art open-source and proprietary models, such as GPT-5, Gemini 2.0 Flash, and Claude 4-Sonnet. Our findings highlight the need for further domain-specific enhancements. We release the dataset and evaluation scripts to broaden the diversity of medical data benchmarks, expand the scope of evaluation suites for LLMs, and enhance the multilingual capabilities of models for deployment in clinical settings.

</details>


### [390] [Mechanistic Indicators of Steering Effectiveness in Large Language Models](https://arxiv.org/abs/2602.01716)
*Mehdi Jafari,Hao Xue,Flora Salim*

Main category: cs.CL

TL;DR: 本文对激活操控在大语言模型中的可靠性诊断进行了探讨，提出通过信息论量（如归一化分叉因子和KL散度）来预测操控成功与否，并为主流方法提出了更严格的评测基线。


<details>
  <summary>Details</summary>
Motivation: 激活操控能让LLM展现特定行为，但现有对何时有效、何时失效机制了解不足，主要依赖输出或人工判别，缺乏基于模型内部信号的诊断手段。该文旨在填补对激活操控机制可靠性的理解空白。

Method: 以归一化分叉因子（NBF）和与目标语义KL散度两项内部信号评估激活操控；以两种结构完全不同的LLM作为人工标注一致性基础，用LLM生成标注结果为真值，系统分析上述信号能否预测操控成功，并提升Contrastive Activation Addition和稀疏自编码操控的评价基线。

Result: 研究发现，模型内部的NBF和KL等机制信号对于判断操控是否成功有较强的预测能力：能够有效判别何时操控达到预期，同时估计失败概率。引入了更强、对比更清晰的评测基线。

Conclusion: 机械性内部信号不仅能够较好地诊断激活操控的成败，还为未来发展更稳健可控的大语言模型奠定了重要基础。

Abstract: Activation-based steering enables Large Language Models (LLMs) to exhibit targeted behaviors by intervening on intermediate activations without retraining. Despite its widespread use, the mechanistic factors that govern when steering succeeds or fails remain poorly understood, as prior work has relied primarily on black-box outputs or LLM-based judges. In this study, we investigate whether the reliability of steering can be diagnosed using internal model signals. We focus on two information-theoretic measures: the entropy-derived Normalized Branching Factor (NBF), and the Kullback-Leibler (KL) divergence between steered activations and targeted concepts in the vocabulary space. We hypothesize that effective steering corresponds to structured entropy preservation and coherent KL alignment across decoding steps. Building on a reliability study demonstrating high inter-judge agreement between two architecturally distinct LLMs, we use LLM-generated annotations as ground truth and show that these mechanistic signals provide meaningful predictive power for identifying successful steering and estimating failure probability. We further introduce a stronger evaluation baseline for Contrastive Activation Addition (CAA) and Sparse Autoencoder-based steering, the two most widely adopted activation-steering methods.

</details>


### [391] [BBPE16: UTF-16-based byte-level byte-pair encoding for improved multilingual speech recognition](https://arxiv.org/abs/2602.01717)
*Hyunsik Kim,Haeri Kim,Munhak Lee,Kyungmin Lee*

Main category: cs.CL

TL;DR: 本文提出了一种新的基于UTF-16的BBPE（BBPE16）分词方法，可有效提升多语种语音识别（ASR）系统在处理非拉丁文字（如中日韩）时的效率和表现。


<details>
  <summary>Details</summary>
Motivation: 目前多语种ASR常用的UTF-8字节级BPE（BBPE）虽然具备语种无关性和全Unicode覆盖，但在中日韩等非拉丁文字上会产生过长的token序列，导致计算和存储负担加重。需要有更高效、统一且兼容多语种的分词方案。

Method: 作者设计了BBPE16分词器，采用UTF-16编码，使得大部分现代字符集以2字节对齐。此设计在保持BBPE语种无关和Unicode覆盖优势的前提下，优化了跨语种token共享，减少了token数量。通过在单语、双语、三语以及持续多语学习的ASR实验中与传统BBPE对比验证其有效性。

Result: BBPE16在识别准确度上达到甚至优于原有BBPE。在中文场景下，token数量减少约10.4%，解码步骤减少约10.3%，整体加快了训练和推理速度，并降低了内存消耗。

Conclusion: BBPE16兼具语种无关、效率提升和较低存储开销，是多语种ASR分词的实用选择，特别适用于中日韩等非拉丁文字密集的场景。

Abstract: Multilingual automatic speech recognition (ASR) requires tokenization that efficiently covers many writing systems. Byte-level BPE (BBPE) using UTF-8 is widely adopted for its language-agnostic design and full Unicode coverage, but its variable-length encoding inflates token sequences for non-Latin scripts, such as Chinese, Japanese, and Korean (CJK). Longer sequences increase computational load and memory use. We propose BBPE16, a UTF-16-based BBPE tokenizer that represents most modern scripts with a uniform 2-byte code unit. BBPE16 preserves BBPE's language-agnostic properties while substantially improving cross-lingual token sharing. Across monolingual, bilingual, and trilingual ASR, and in a multilingual continual-learning setup, BBPE16 attains comparable or better accuracy; for Chinese, it reduces token counts by up to 10.4% and lowers decoding iterations by up to 10.3%. These reductions speed up fine-tuning and inference and decrease memory usage, making BBPE16 a practical tokenization choice for multilingual ASR.

</details>


### [392] [COMI: Coarse-to-fine Context Compression via Marginal Information Gain](https://arxiv.org/abs/2602.01719)
*Jiwei Tang,Shilei Liu,Zhicheng Zhang,Yujin Yuan,Libin Zheng,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: 本文提出了一种新的上下文压缩框架COMI，有效减小输入长度并去除冗余，在长文本任务中显著提升了大语言模型的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在长上下文任务中的计算效率低下且信息冗余严重，现有的压缩方法仍有改进空间。本文旨在通过新的压缩策略缓解这一问题，提高长文本处理效率与质量。

Method: COMI框架分为两阶段：（1）粗粒度分组重分配，根据分组间的边际信息增益（MIG）动态分配压缩率，使压缩预算与信息分布相匹配；（2）细粒度的组内token融合，通过组内MIG加权合并token，保留关键信息并减少冗余。核心指标MIG结合了与查询的相关性和语义冗余，指导压缩过程。

Result: 在多个问答和摘要数据集，以及多种主流大模型（如LLaMA-2-7B、Qwen2-7B）上，COMI明显优于现有方法。以Qwen2-7B在NaturalQuestions为例，在32倍压缩情况下EM提升约25分。

Conclusion: COMI能在高压缩率下有效去除冗余并保留重要语义，显著提升大模型在长文本场景下的表现，具有广泛应用前景。

Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities across diverse tasks. However, their deployment in long context scenarios remains hindered by computational inefficiency and information redundancy. Context compression methods address these challenges by significantly reducing input length and eliminating redundancy. We propose COMI, a coarse-to-fine adaptive context compression framework that jointly optimizes for semantic relevance and diversity under high compression rates. We introduce Marginal Information Gain (MIG), a metric defined as the relevance of a unit to the input query minus its semantic redundancy with other units, guiding the compression process to prioritize information that is both relevant and low redundant. The framework operates in two stages: (1) Coarse-Grained Group Reallocation, where the context is partitioned into groups and dynamically assigned compression rates based on inter-group MIG, ensuring compression budgets align with information value distribution; and (2) Fine-Grained Token Merging, where tokens within each group are fused via an intra-group MIG-based weighting mechanism, thereby preserving key semantics while avoiding the accumulation of redundancy. Extensive experiments across question-answering (e.g., NaturalQuestions, 2WikiMQA, HotpotQA and NarrativeQA), summarization (e.g., MultiNews) with various backbones (e.g., LLaMA-2-7B, Qwen2-7B) show that COMI outperforms existing baselines by a large margin, e.g., approximately 25-point Exact Match (EM) improvement under 32x compression constraint with Qwen2-7B on NaturalQuestions.

</details>


### [393] [SafePred: A Predictive Guardrail for Computer-Using Agents via World Models](https://arxiv.org/abs/2602.01725)
*Yurun Chen,Zeyi Liao,Ping Yin,Taotao Xie,Keting Yin,Shengyu Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种用于计算机智能体(CUAs)的预测型护栏(SafePred)，通过预测短期和长期风险来提前规避危害，显著提升了安全性与实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的护栏主要是被动反应型，聚焦于当前观测空间内的限制，这只能防止即时风险，但无法识别和规避延时出现的长期高风险行为。因此，亟需一种能够主动预测和避免长远风险的方法。

Method: 提出了SafePred预测护栏框架。其核心是建立风险-决策闭环，利用世界模型预测未来可能的风险（含短期和长期），实现风险表征并在决策前剪枝高风险行为。同时，将风险预测结果转化为实际的决策指导，包括逐步干预和任务重规划。

Result: 实验结果表明，SafePred能大幅度减少高风险行为，实现了97.6%以上的安全表现，相比现有反应式基线，任务效用也提升了最高21.4%。

Conclusion: SafePred能够有效预测并规避计算机智能体面临的短期及长期高风险行为，显著提升了智能体在复杂环境中的安全性和实用性。

Abstract: With the widespread deployment of Computer-using Agents (CUAs) in complex real-world environments, prevalent long-term risks often lead to severe and irreversible consequences. Most existing guardrails for CUAs adopt a reactive approach, constraining agent behavior only within the current observation space. While these guardrails can prevent immediate short-term risks (e.g., clicking on a phishing link), they cannot proactively avoid long-term risks: seemingly reasonable actions can lead to high-risk consequences that emerge with a delay (e.g., cleaning logs leads to future audits being untraceable), which reactive guardrails cannot identify within the current observation space. To address these limitations, we propose a predictive guardrail approach, with the core idea of aligning predicted future risks with current decisions. Based on this approach, we present SafePred, a predictive guardrail framework for CUAs that establishes a risk-to-decision loop to ensure safe agent behavior. SafePred supports two key abilities: (1) Short- and long-term risk prediction: by using safety policies as the basis for risk prediction, SafePred leverages the prediction capability of the world model to generate semantic representations of both short-term and long-term risks, thereby identifying and pruning actions that lead to high-risk states; (2) Decision optimization: translating predicted risks into actionable safe decision guidances through step-level interventions and task-level re-planning. Extensive experiments show that SafePred significantly reduces high-risk behaviors, achieving over 97.6% safety performance and improving task utility by up to 21.4% compared with reactive baselines.

</details>


### [394] [Enhancing Automated Essay Scoring with Three Techniques: Two-Stage Fine-Tuning, Score Alignment, and Self-Training](https://arxiv.org/abs/2602.01747)
*Hongseok Choi,Serynn Kim,Wencke Liermann,Jin Seong,Jin-Xia Huang*

Main category: cs.CL

TL;DR: 本文针对自动化作文评分（AES）在标注数据稀缺情况下的性能瓶颈，提出三项关键技术，有效提升了AES在数据有限和完整数据下的评分能力，并在ASAP++数据集上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现实中AES系统往往面临高质量标注数据极度匮乏的问题，严重制约了建模和实际推广。因此，作者希望在低标注数据量时依然能大幅提升AES性能。

Method: 1）提出“两阶段微调”并借助低秩适应，提升模型对目标写作题目的泛化力；2）提出得分对齐技术，增强预测分布与真实分布的一致性；3）通过不确定性感知的自训练策略使用未标注数据，利用伪标签扩展训练集同时抑制噪声扩散。三者集成在DualBERT架构上，进行端到端实验。

Result: 在ASAP++数据集上，三个方法在32条训练数据极少的环境下均提升了性能，三者合用后达到仅用约1000条标注数据即可获得91.2%的完整数据训练水平；Score Alignment方法在完整和稀缺数据下均显著提升分数准确率，且集成于DualBERT后达业界最新水平。

Conclusion: 提出的关键技术能有效解决AES在数据稀缺与充足两种场景下的性能挑战，不仅提升模型泛化能力，还实现了更优的分数分布拟合，为AES实际应用和推广提供了强有力支撑。

Abstract: Automated Essay Scoring (AES) plays a crucial role in education by providing scalable and efficient assessment tools. However, in real-world settings, the extreme scarcity of labeled data severely limits the development and practical adoption of robust AES systems. This study proposes a novel approach to enhance AES performance in both limited-data and full-data settings by introducing three key techniques. First, we introduce a Two-Stage fine-tuning strategy that leverages low-rank adaptations to better adapt an AES model to target prompt essays. Second, we introduce a Score Alignment technique to improve consistency between predicted and true score distributions. Third, we employ uncertainty-aware self-training using unlabeled data, effectively expanding the training set with pseudo-labeled samples while mitigating label noise propagation. We implement above three key techniques on DualBERT. We conduct extensive experiments on the ASAP++ dataset. As a result, in the 32-data setting, all three key techniques improve performance, and their integration achieves 91.2% of the full-data performance trained on approximately 1,000 labeled samples. In addition, the proposed Score Alignment technique consistently improves performance in both limited-data and full-data settings: e.g., it achieves state-of-the-art results in the full-data setting when integrated into DualBERT.

</details>


### [395] [WorldCup Sampling for Multi-bit LLM Watermarking](https://arxiv.org/abs/2602.01752)
*Yidan Wang,Yubing Ren,Yanan Cao,Li Guo*

Main category: cs.CL

TL;DR: WorldCup是一种新颖的多比特水印框架，直接将信息嵌入LLM生成的文本中，显著提升容量、鲁棒性和解码效率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）生成文本的“人类化”程度提高，如何有效追溯与认证内容来源成为难题。现有水印方法多借助零比特方案，无法高效承载丰富信息，限制了实际应用。作者试图设计一种容量更大、可靠性更强的多比特水印方法。

Method: 提出WorldCup多比特水印架构，将采样视为通信通道，通过分层竞争机制和辅助信号，直接在token选择时嵌入消息比特，同时利用熵感知调制以维持文本质量，并采用置信度感知解码保障鲁棒性和可靠恢复。

Result: 在综合实验中，WorldCup在容量、可检测性、鲁棒性、文本质量和解码效率等方面均表现优异，显著优于以往多比特水印基线方法。

Conclusion: WorldCup为LLMs文本水印设定了新的性能标杆，兼顾高效嵌入、鲁棒恢复与生成质量，为后续相关研究和实际溯源应用提供了坚实基础。

Abstract: As large language models (LLMs) generate increasingly human-like text, watermarking offers a promising solution for reliable attribution beyond mere detection. While multi-bit watermarking enables richer provenance encoding, existing methods largely extend zero-bit schemes through seed-driven steering, leading to indirect information flow, limited effective capacity, and suboptimal decoding. In this paper, we propose WorldCup, a multi-bit watermarking framework for LLMs that treats sampling as a natural communication channel and embeds message bits directly into token selection via a hierarchical competition mechanism guided by complementary signals. Moreover, WorldCup further adopts entropy-aware modulation to preserve generation quality and supports robust message recovery through confidence-aware decoding. Comprehensive experiments show that WorldCup achieves a strong balance across capacity, detectability, robustness, text quality, and decoding efficiency, consistently outperforming prior baselines and laying a solid foundation for future LLM watermarking studies.

</details>


### [396] [Zero2Text: Zero-Training Cross-Domain Inversion Attacks on Textual Embeddings](https://arxiv.org/abs/2602.01757)
*Doohyun Kim,Donghwa Kang,Kyungjae Lee,Hyeongboo Baek,Brent Byunghoon Kang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Zero2Text的新方法，可在无训练数据、黑盒和跨域环境下成功恢复被向量数据库保护的文本，从而破解了检索增强生成（RAG）中的隐私保护难题。该方法无需训练，在线递归调整生成策略，可突破现有防御措施，如差分隐私，其效果优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 向量数据库在RAG系统中应用广泛，但其嵌入数据易被逆向推断（embedding inversion），带来重大隐私风险。现有逆向攻击方法或计算量大、或需不可得的域内数据，限制了实际威胁评估和防御手段。因此，亟需一种无需训练数据、能在黑盒环境下实施且效果显著的攻击新方法，以评估和提升系统安全性。

Method: Zero2Text是一种免训练的数据逆向框架。其核心思想是迭代式地在线调整生成文本，使其嵌入尽可能接近目标向量，无需使用静态训练集。具体实现为结合大语言模型（LLM）生成候选文本，通过岭回归进行动态投影，每轮生成更逼近目标嵌入的内容，直到相似度收敛。该方法适用于无法访问数据对、无需微调和全黑盒场景。

Result: 实验表明，Zero2Text在多个标准数据集表现卓越。例如，在MS MARCO数据集与OpenAI黑盒模型对抗时，相比传统方法ROUGE-L指标提升1.8倍，BLEU-2指标提升6.4倍。同时，在未知领域数据上也能有效恢复原始句子，且无需任何数据泄漏对（即零暴露）。此外，实验还验证了主流防御机制（比如差分隐私）对这种自适应攻击也难以奏效。

Conclusion: Zero2Text框架突破了现有逆向攻击方法的传统限制，显示即使在黑盒、零样本环境下，向量数据库中的数据仍易受威胁，对设计安全的RAG系统具有重大警示意义。现有防御手段难以阻挡此类攻击，需重新思考向量数据库应用中的隐私保护策略。

Abstract: The proliferation of retrieval-augmented generation (RAG) has established vector databases as critical infrastructure, yet they introduce severe privacy risks via embedding inversion attacks. Existing paradigms face a fundamental trade-off: optimization-based methods require computationally prohibitive queries, while alignment-based approaches hinge on the unrealistic assumption of accessible in-domain training data. These constraints render them ineffective in strict black-box and cross-domain settings. To dismantle these barriers, we introduce Zero2Text, a novel training-free framework based on recursive online alignment. Unlike methods relying on static datasets, Zero2Text synergizes LLM priors with a dynamic ridge regression mechanism to iteratively align generation to the target embedding on-the-fly. We further demonstrate that standard defenses, such as differential privacy, fail to effectively mitigate this adaptive threat. Extensive experiments across diverse benchmarks validate Zero2Text; notably, on MS MARCO against the OpenAI victim model, it achieves 1.8x higher ROUGE-L and 6.4x higher BLEU-2 scores compared to baselines, recovering sentences from unknown domains without a single leaked data pair.

</details>


### [397] [<SOG_k>: One LLM Token for Explicit Graph Structural Understanding](https://arxiv.org/abs/2602.01771)
*Jingyao Wu,Bin Lu,Zijun Di,Xiaoying Gan,Meng Jin,Luoyi Fu,Xinbing Wang,Chenghu Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种新方法，通过引入特殊token将图结构高效地整合进大语言模型，显著提升了理解和推理图数据的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理图结构（如社交网络、知识图谱等）时存在结构幻觉问题，且现有的将图转成自然语言或连续向量的方式都存在效率和表达不完备的缺陷。

Method: 设计了一种结构感知的分词器，将每个图的结构映射为单一的特殊token（<SOG_k>），并通过构建混合结构的问答语料，实现新结构token和文本token的对齐，便于大模型准确理解图结构。

Result: 在五个图级基准数据集上，与现有方法相比，新方法带来9.9%-41.4%的性能提升，同时增强了模型的可解释性和一致性。

Conclusion: 该方法有效解决了大模型处理图数据时的结构表达难题，提升了图结构理解与推理能力，并且可灵活扩展到节点级任务，具备广泛应用前景。

Abstract: Large language models show great potential in unstructured data understanding, but still face significant challenges with graphs due to their structural hallucination. Existing approaches mainly either verbalize graphs into natural language, which leads to excessive token consumption and scattered attention, or transform graphs into trainable continuous embeddings (i.e., soft prompt), but exhibit severe misalignment with original text tokens. To solve this problem, we propose to incorporate one special token <SOG_k> to fully represent the Structure Of Graph within a unified token space, facilitating explicit topology input and structural information sharing. Specifically, we propose a topology-aware structural tokenizer that maps each graph topology into a highly selective single token. Afterwards, we construct a set of hybrid structure Question-Answering corpora to align new structural tokens with existing text tokens. With this approach, <SOG_k> empowers LLMs to understand, generate, and reason in a concise and accurate manner. Extensive experiments on five graph-level benchmarks demonstrate the superiority of our method, achieving a performance improvement of 9.9% to 41.4% compared to the baselines while exhibiting interpretability and consistency. Furthermore, our method provides a flexible extension to node-level tasks, enabling both global and local structural understanding. The codebase is publicly available at https://github.com/Jingyao-Wu/SOG.

</details>


### [398] [Data Distribution Matters: A Data-Centric Perspective on Context Compression for Large Language Model](https://arxiv.org/abs/2602.01778)
*Kangtao Lv,Jiwei Tang,Langming Liu,Haibin Chen,Weidong Zhang,Shilei Liu,Yongwei Wang,Yujin Yuan,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: 本文首次从数据分布角度系统研究了其对大语言模型（LLMs）上下文压缩质量的影响，并提出可优化压缩效果的建议。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在处理长文本时存在计算效率低和信息冗余严重的问题，传统研究多聚焦于模型改进，忽视了数据分布对上下文压缩的影响。该文旨在弥补这一空白。

Method: 提出采用数据中心的方法，分别从输入数据和模型内在数据两方面，利用autoencoder框架，系统性分析数据分布对语义完整性和压缩效果的影响。

Result: 实验发现：（1）编码器测得的输入熵与压缩质量呈负相关，而冻结解码器时，解码器的熵与压缩质量无显著性关系；（2）编码器和解码器的内在数据差距会显著降低压缩收益，且难以缓解。

Conclusion: 研究揭示了数据分布对上下文压缩的核心作用，并给出了优化压缩效果的实际建议。

Abstract: The deployment of Large Language Models (LLMs) in long-context scenarios is hindered by computational inefficiency and significant information redundancy. Although recent advancements have widely adopted context compression to address these challenges, existing research only focus on model-side improvements, the impact of the data distribution itself on context compression remains largely unexplored. To bridge this gap, we are the first to adopt a data-centric perspective to systematically investigate how data distribution impacts compression quality, including two dimensions: input data and intrinsic data (i.e., the model's internal pretrained knowledge). We evaluate the semantic integrity of compressed representations using an autoencoder-based framework to systematically investigate it. Our experimental results reveal that: (1) encoder-measured input entropy negatively correlates with compression quality, while decoder-measured entropy shows no significant relationship under a frozen-decoder setting; and (2) the gap between intrinsic data of the encoder and decoder significantly diminishes compression gains, which is hard to mitigate. Based on these findings, we further present practical guidelines to optimize compression gains.

</details>


### [399] [CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding](https://arxiv.org/abs/2602.01785)
*Yuling Shi,Chaoxiang Xie,Zhensu Sun,Yeheng Chen,Chenxu Zhang,Longfei Yun,Chengcheng Wan,Hongyu Zhang,David Lo,Xiaodong Gu*

Main category: cs.CL

TL;DR: 本文首次系统性地研究了将源码以图像形式输入多模态大模型（MLLM）以提升代码理解效率的问题，实现显著的token压缩，减少计算负担。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统规模增大，现有的基于文本的大模型遇到上下文线性扩展导致的计算瓶颈。研究动机在于寻求更高效的源码表示方法，利用MLLMs对图像理解能力，通过将源码渲染为图片形式以压缩token数。

Method: 作者将源码渲染为图像，调整分辨率以实现token压缩，并将这些图像及其语法高亮送入多模态大模型进行代码理解任务，系统评估了模型在压缩比、理解能力和语法高亮等多方面的表现。

Result: 实验发现MLLM对源码文件可实现最高8倍的token压缩，并能借助视觉线索（如语法高亮）提升4倍压缩下的补全表现。对于克隆检测等任务，在多种压缩比例下甚至优于原始文本输入。

Conclusion: 多模态大模型通过图像化代码表示实现高效代码理解成为可能，展示了这种方法巨大的潜力和当前的局限性，为未来采用图像模态进行大规模代码推理提供了新的方向。

Abstract: Large Language Models (LLMs) have achieved remarkable success in source code understanding, yet as software systems grow in scale, computational efficiency has become a critical bottleneck. Currently, these models rely on a text-based paradigm that treats source code as a linear sequence of tokens, which leads to a linear increase in context length and associated computational costs. The rapid advancement of Multimodal LLMs (MLLMs) introduces an opportunity to optimize efficiency by representing source code as rendered images. Unlike text, which is difficult to compress without losing semantic meaning, the image modality is inherently suitable for compression. By adjusting resolution, images can be scaled to a fraction of their original token cost while remaining recognizable to vision-capable models. To explore the feasibility of this approach, we conduct the first systematic study on the effectiveness of MLLMs for code understanding. Our experiments reveal that: (1) MLLMs can effectively understand code with substantial token reduction, achieving up to 8x compression; (2) MLLMs can effectively leverage visual cues such as syntax highlighting, improving code completion performance under 4x compression; and (3) Code-understanding tasks like clone detection exhibit exceptional resilience to visual compression, with some compression ratios even slightly outperforming raw text inputs. Our findings highlight both the potential and current limitations of MLLMs in code understanding, which points out a shift toward image-modality code representation as a pathway to more efficient inference.

</details>


### [400] [Sentence Curve Language Models](https://arxiv.org/abs/2602.01807)
*DongNyeong Heo,Heelyoul Choi*

Main category: cs.CL

TL;DR: 本文提出了一种新的句子表示方法“句子曲线”，并基于此提出了SCLM模型，有效提升了扩散语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型对目标词的嵌入是静态的，不能很好地捕捉目标句子的全局结构，主要关注局部准确性，忽略了词与词之间的全局关系。

Method: 提出用样条曲线（sentence curve）对整个句子进行连续表示，这种表示的方法通过多个控制点影响句中多个词，提升对句子整体结构的建模能力。此外，提出了基于此表示的SCLM（Sentence Curve Language Model），让扩散语言模型直接预测句子曲线而不是独立的词嵌入。同时理论分析了句子曲线预测带来的正则化效果，并系统分析了不同曲线类型的影响。

Result: SCLM在IWSLT14和WMT14等基准数据集上取得了目前扩散语言模型中的最优表现（SOTA），训练过程稳定，无需复杂的知识蒸馏流程，在LM1B数据集上与离散型扩散语言模型相比也展现出有潜力的表现。

Conclusion: 新的句子曲线表示及SCLM不仅提高了扩散语言模型的性能，还增强了其对句子全局结构的建模能力，为语言建模提供了新的有效思路。

Abstract: Language models (LMs) are a central component of modern AI systems, and diffusion-based language models (DLMs) have recently emerged as a competitive alternative. Both paradigms rely on word embeddings not only to represent the input sentence, but also to represent the target sentence that backbone models are trained to predict. We argue that such static embedding of the target word is insensitive to neighboring words, encouraging locally accurate word prediction while neglecting global structure across the target sentence. To address this limitation, we propose a continuous sentence representation, termed sentence curve, defined as a spline curve whose control points affect multiple words in the sentence. Based on this representation, we introduce sentence curve language model (SCLM), which extends DLMs to predict sentence curves instead of the static word embeddings. We theoretically show that sentence curve prediction induces a regularization effect that promotes global structure modeling, and characterize how different sentence curve types affect this behavior. Empirically, SCLM achieves SOTA performance among DLMs on IWSLT14 and WMT14, shows stable training without burdensome knowledge distillation, and demonstrates promising potential compared to discrete DLMs on LM1B.

</details>


### [401] [AXE: Low-Cost Cross-Domain Web Structured Information Extraction](https://arxiv.org/abs/2602.01838)
*Abdelrahman Mansour,Khaled W. Alshaer,Moataz Elsaban*

Main category: cs.CL

TL;DR: AXE提出了一种通过裁剪HTML DOM树来提取结构化数据的新方法，让小型LLM实现高效精准的数据抽取，且保证溯源可追踪，取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 传统网页结构化数据提取在手工规则与大模型高成本之间权衡，缺乏同时兼具简易性、低成本和高可靠性的解决方案。

Method: AXE将HTML DOM视为需要修剪的树，通过去除无关节点，保留高密度关键信息，再用0.6B参数的小型LLM生成结构化输出。并通过Grounded XPath Resolution（GXR）机制确保每个抽取结果都能追溯至DOM原节点，提高可解释性和可靠性。

Result: AXE在SWDE数据集上以F1 88.1%达到了SOTA零样本表现，超越了许多更大且经过完全训练的替代方案。

Conclusion: AXE以极低的资源消耗实现了高准确度、可追溯的网页结构化信息抽取，为大规模应用提供了成本效益兼具的现实路径。

Abstract: Extracting structured data from the web is often a trade-off between the brittle nature of manual heuristics and the prohibitive cost of Large Language Models. We introduce AXE (Adaptive X-Path Extractor), a pipeline that rethinks this process by treating the HTML DOM as a tree that needs pruning rather than just a wall of text to be read. AXE uses a specialized "pruning" mechanism to strip away boilerplate and irrelevant nodes, leaving behind a distilled, high-density context that allows a tiny 0.6B LLM to generate precise, structured outputs. To keep the model honest, we implement Grounded XPath Resolution (GXR), ensuring every extraction is physically traceable to a source node. Despite its low footprint, AXE achieves state-of-the-art zero-shot performance, outperforming several much larger, fully-trained alternatives with an F1 score of 88.1% on the SWDE dataset. By releasing our specialized adaptors, we aim to provide a practical, cost-effective path for large-scale web information extraction.

</details>


### [402] [Read As Human: Compressing Context via Parallelizable Close Reading and Skimming](https://arxiv.org/abs/2602.01840)
*Jiwei Tang,Shilei Liu,Zhicheng Zhang,Qingsong Lv,Runsong Zhao,Tingwei Lu,Langming Liu,Haibin Chen,Yujin Yuan,Hai-Tao Zheng,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: 本文提出了一种名为RAM的上下文压缩框架，通过模拟人类阅读策略优化大语言模型在长文本场景下的效率与效果，在多个问答和摘要基准测试中取得了更高性能且大幅提升了推理速度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理长文本时，面临计算效率低下和信息冗余的问题。现有方法难以兼顾高性能、速度和解释性，这促使作者研发出更高效且更符合人类阅读习惯的处理框架。

Method: RAM借鉴人类阅读习惯，将文本分段并采用自适应混合阅读策略：对与查询高度相关的段落“精读”，而对相关性低的段落“略读”并压缩成摘要向量，再将二者拼接后输入解码器。该方法还引入了基于正负样本对比学习的目标，提升了对精读和略读界限的判别能力。

Result: 在多个问答和摘要任务的测试中，RAM在两种主流底座模型下都超过了现有方法，在处理最长达32K字的输入时，推理速度提升最高可达12倍。

Conclusion: RAM显著提升了长文本处理的效率和效果，使得大语言模型能够更好地兼顾性能和速度，并具备较好的可解释性。

Abstract: Large Language Models (LLMs) demonstrate exceptional capability across diverse tasks. However, their deployment in long-context scenarios is hindered by two challenges: computational inefficiency and redundant information. We propose RAM (Read As HuMan), a context compression framework that adopts an adaptive hybrid reading strategy, to address these challenges. Inspired by human reading behavior (i.e., close reading important content while skimming less relevant content), RAM partitions the context into segments and encodes them with the input query in parallel. High-relevance segments are fully retained (close reading), while low-relevance ones are query-guided compressed into compact summary vectors (skimming). Both explicit textual segments and implicit summary vectors are concatenated and fed into decoder to achieve both superior performance and natural language format interpretability. To refine the decision boundary between close reading and skimming, we further introduce a contrastive learning objective based on positive and negative query-segment pairs. Experiments demonstrate that RAM outperforms existing baselines on multiple question answering and summarization benchmarks across two backbones, while delivering up to a 12x end-to-end speedup on long inputs (average length 16K; maximum length 32K).

</details>


### [403] [PretrainRL: Alleviating Factuality Hallucination of Large Language Models at the Beginning](https://arxiv.org/abs/2602.01875)
*Langming Liu,Kangtao Lv,Haibin Chen,Weidong Zhang,Yejing Wang,Shilei Liu,Xin Tong,Yujin Yuan,Yongwei Wang,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: 该论文提出了一种基于强化学习的新预训练框架PretrainRL，使大语言模型更好地学习事实知识，显著减少了事实幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型常发生事实幻觉，部分原因在于预训练数据中真假分布不均，模型更易学到错误但高概率的信息，对低概率事实的学习却较弱。现有方法如让模型回答“我不知道”或事后知识编辑不能从根本解决问题，有的还会导致遗忘原有知识。

Method: 作者提出PretrainRL框架，将强化学习引入预训练阶段。核心理念是“去偏再学习”；通过有策略地降低高概率错误答案权重，提升模型学习到低概率真实知识的能力。方法包括高效的负采样策略挖掘高概率假信息、引入新概率度量指标等。

Result: 在三个公开基准测试上的大量实验表明，PretrainRL显著减少了大模型的事实幻觉，并优于目前最先进的其他方法。

Conclusion: 通过有针对性地改变预训练过程概率分布，PretrainRL能从根本性提高大语言模型的事实一致性，为消除模型幻觉问题开辟了新方向。

Abstract: Large language models (LLMs), despite their powerful capabilities, suffer from factual hallucinations where they generate verifiable falsehoods. We identify a root of this issue: the imbalanced data distribution in the pretraining corpus, which leads to a state of "low-probability truth" and "high-probability falsehood". Recent approaches, such as teaching models to say "I don't know" or post-hoc knowledge editing, either evade the problem or face catastrophic forgetting. To address this issue from its root, we propose \textbf{PretrainRL}, a novel framework that integrates reinforcement learning into the pretraining phase to consolidate factual knowledge. The core principle of PretrainRL is "\textbf{debiasing then learning}." It actively reshapes the model's probability distribution by down-weighting high-probability falsehoods, thereby making "room" for low-probability truths to be learned effectively. To enable this, we design an efficient negative sampling strategy to discover these high-probability falsehoods and introduce novel metrics to evaluate the model's probabilistic state concerning factual knowledge. Extensive experiments on three public benchmarks demonstrate that PretrainRL significantly alleviates factual hallucinations and outperforms state-of-the-art methods.

</details>


### [404] [ES-MemEval: Benchmarking Conversational Agents on Personalized Long-Term Emotional Support](https://arxiv.org/abs/2602.01885)
*Tiantian Chen,Jiaqi Lu,Ying Shen,Lin Zhang*

Main category: cs.CL

TL;DR: 本文提出ES-MemEval基准及EvoEmo数据集，系统评估LLM在长期情感支持中的记忆能力，实验展示了当前方法的局限与发展方向。


<details>
  <summary>Details</summary>
Motivation: 现有长期对话基准主要聚焦于静态事实检索，忽略了现实场景下用户信息分散、隐含且动态变化的问题，尤其在如在线情感支持等复杂应用中更为突出。因此，亟需新的评测标准与数据集来真实评价大模型在长期记忆与用户建模上的效果。

Method: 作者提出了ES-MemEval这一新基准，涵盖信息提取、时序推理、冲突检测、回避与用户建模等五项长期记忆核心能力，任务包含问答、摘要与对话生成。同时构建了EvoEmo多轮对话数据集，专门模拟长期个性化情感支持场景。

Result: 对比多种长上下文LLM、商用模型和RAG模型后发现：显式长期记忆对于提升个性化和减少幻觉至关重要；RAG方法提升了事实一致性，但面对时序和动态用户信息时表现欠佳。

Conclusion: 当前主流长记忆方法仍有明显局限，单靠检索或扩展上下文难以满足动态、隐式和长期用户建模需求。未来需更深度集成记忆与检索机制以支持个性化、长时对话系统。

Abstract: Large Language Models (LLMs) have shown strong potential as conversational agents. Yet, their effectiveness remains limited by deficiencies in robust long-term memory, particularly in complex, long-term web-based services such as online emotional support. However, existing long-term dialogue benchmarks primarily focus on static and explicit fact retrieval, failing to evaluate agents in critical scenarios where user information is dispersed, implicit, and continuously evolving. To address this gap, we introduce ES-MemEval, a comprehensive benchmark that systematically evaluates five core memory capabilities: information extraction, temporal reasoning, conflict detection, abstention, and user modeling, in long-term emotional support settings, covering question answering, summarization, and dialogue generation tasks. To support the benchmark, we also propose EvoEmo, a multi-session dataset for personalized long-term emotional support that captures fragmented, implicit user disclosures and evolving user states. Extensive experiments on open-source long-context, commercial, and retrieval-augmented (RAG) LLMs show that explicit long-term memory is essential for reducing hallucinations and enabling effective personalization. At the same time, RAG improves factual consistency but struggles with temporal dynamics and evolving user states. These findings highlight both the potential and limitations of current paradigms and motivate more robust integration of memory and retrieval for long-term personalized dialogue systems.

</details>


### [405] [GuideWeb: A Benchmark for Automatic In-App Guide Generation on Real-World Web UIs](https://arxiv.org/abs/2602.01917)
*Chengguang Gan,Yoshihiro Tsujii,Yunhao Liang,Tatsunori Mori,Shiwen Ni,Hiroki Itoh*

Main category: cs.CL

TL;DR: 本文提出GuideWeb，一个用于在真实网页UI上自动生成操作指南的新基准，并设计了评测方案，实验结果显示自动指南生成任务依然具有挑战性。


<details>
  <summary>Details</summary>
Motivation: 尽管现有DAP（数字采纳平台）让非专业用户能够制作操作指南，但由于网站经常变更，维护指南需频繁手工更新，十分耗力。为此，需要自动化方法降低人工成本。

Method: 提出了GuideWeb基准，将指南生成定义为选取合适的网页元素并生成简明指南文本的任务，并构建了评估体系，综合考察目标元素选择和生成文本的准确性与质量，同时设计了GuideWeb Agent自动生成系统。

Result: GuideWeb Agent在目标元素选取上获得30.79%的准确率，意图生成BLEU得分为44.94，指南文本生成BLEU得分为21.34，显著优于现有基线。

Conclusion: 自动生成网页操作指南依然是极具挑战性的任务，现有成果还无法满足真实场景需求，需要进一步进步。

Abstract: Digital Adoption Platform (DAP) provide web-based overlays that deliver operation guidance and contextual hints to help users navigate complex websites. Although modern DAP tools enable non-experts to author such guidance, maintaining these guides remains labor-intensive because website layouts and functionalities evolve continuously, which requires repeated manual updates and re-annotation. In this work, we introduce \textbf{GuideWeb}, a new benchmark for automatic in-app guide generation on real-world web UIs. GuideWeb formulates the task as producing page-level guidance by selecting \textbf{guide target elements} grounded in the webpage and generating concise guide text aligned with user intent. We also propose a comprehensive evaluation suite that jointly measures the accuracy of guide target element selection and the quality of generated intents and guide texts. Experiments show that our proposed \textbf{GuideWeb Agent} achieves \textbf{30.79\%} accuracy in guide target element prediction, while obtaining BLEU scores of \textbf{44.94} for intent generation and \textbf{21.34} for guide-text generation. Existing baselines perform substantially worse, which highlights that automatic guide generation remains challenging and that further advances are necessary before such systems can be reliably deployed in real-world settings.

</details>


### [406] [From Code-Centric to Concept-Centric: Teaching NLP with LLM-Assisted "Vibe Coding"](https://arxiv.org/abs/2602.01919)
*Hend Al-Khalifa*

Main category: cs.CL

TL;DR: 该论文提出并在NLP高年级课程中实践了LLM（大语言模型）辅助编码教学法。结果显示学生满意度高，理论理解有提升，但存在LLM验证等挑战。支持在AI时代转向以概念掌握为核心的教学。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，NLP教学面临机遇和挑战。现有编程教学容易让学生陷入语法细节，难以专注于概念理解。本研究探索如何利用LLMs辅助教学，提升学生在NLP课程中的理论掌握和批判思维。

Method: 在一门高年级本科NLP课程中，学生在7个实验中由LLMs协助编码，课程考核以概念理解和批判性反思为主，支持机制包括强制的prompt记录与反思性考核。课程结束后，通过学生反馈调查进行效果评估。

Result: 19名学生的反馈显示各项满意度（如参与度、概念学习与考核公平性）均很高（4.4-4.6/5.0）。多数学生认为LLM辅助编码降低了调试负担，帮助聚焦理论。但也出现“时间不足”“输出需验证”“任务描述需更清晰”等挑战。

Conclusion: 结构合理的LLM辅助教学能显著提升学生对NLP理论与批判性思维的把握，有助于学生适应AI时代。课程设计应合理引导LLM使用，并通过日志记录和反思性考核应对潜在挑战。

Abstract: The rapid advancement of Large Language Models (LLMs) presents both challenges and opportunities for Natural Language Processing (NLP) education. This paper introduces ``Vibe Coding,'' a pedagogical approach that leverages LLMs as coding assistants while maintaining focus on conceptual understanding and critical thinking. We describe the implementation of this approach in a senior-level undergraduate NLP course, where students completed seven labs using LLMs for code generation while being assessed primarily on conceptual understanding through critical reflection questions. Analysis of end-of-course feedback from 19 students reveals high satisfaction (mean scores 4.4-4.6/5.0) across engagement, conceptual learning, and assessment fairness. Students particularly valued the reduced cognitive load from debugging, enabling deeper focus on NLP concepts. However, challenges emerged around time constraints, LLM output verification, and the need for clearer task specifications. Our findings suggest that when properly structured with mandatory prompt logging and reflection-based assessment, LLM-assisted learning can shift focus from syntactic fluency to conceptual mastery, preparing students for an AI-augmented professional landscape.

</details>


### [407] [Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation](https://arxiv.org/abs/2602.01965)
*Kwun Hang Lau,Fangyuan Zhang,Boyu Ruan,Yingli Zhou,Qintian Guo,Ruiyuan Zhang,Xiaofang Zhou*

Main category: cs.CL

TL;DR: 文章提出了一种新的结构感知RAG方法CatRAG，能动态根据查询调整知识图谱遍历策略，有效提升多跳证据检索的完整性和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱和PageRank的结构化RAG方法，因静态边权、忽视查询语义，导致模型常陷入高连接“枢纽”节点，出现语义漂移，难以完整获取多跳查询所需的全部证据链。亟需能动态调整KG遍历的RAG方法，提升推理完整性。

Method: 提出CatRAG框架：以HippoRAG 2为基础，（1）符号锚定：基于实体注入弱约束，规范随机游走；（2）查询感知动态边权：根据查询动态调整图结构，增强相关路径、剪枝无关路径；（3）关键事实段落权重增强：通过结构性偏置将游走锚定于可能的证据。综合多策略提升查全率与推理连通性。

Result: 在四个多跳基准任务上，CatRAG全面优于主流结构化检索基线。除常规查全率提升外，CatRAG在恢复完整证据链、推理“无缺口”能力方面展现显著优越性。

Conclusion: CatRAG有效解决了静态知识图检索的语义漂移问题，提升了RAG对复杂查询的完整推理能力，为多跳证据检索和结构化推理提供了新方向。

Abstract: Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a "Static Graph Fallacy": they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree "hub" nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query's intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at https://github.com/kwunhang/CatRAG.

</details>


### [408] [Mixture-of-Experts with Intermediate CTC Supervision for Accented Speech Recognition](https://arxiv.org/abs/2602.01967)
*Wonjun Lee,Hyounghun Kim,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 本文提出了一种新的多专家模型Moe-Ctc，通过中间CTC监督与口音感知路由机制，有效提升ASR在不同口音上的泛化及鲁棒性，实现了显著的识别性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有自动语音识别系统主要依据高资源口音数据训练，导致对于其他口音识别性能大幅下降，且现有泛口音或专口音方法均存在限制。作者旨在设计一种模型，兼具对已知及未知口音的鲁棒性。

Method: 提出Mixture-of-Experts架构（Moe-Ctc），每个专家配有独立的CTC头部。训练阶段采用口音感知路由促进专家学到对应口音特征，推断时转为无标签路由以增强泛化能力。引入带路由增强的损失函数以保证模型优化稳定。

Result: 在Mcv-Accent基准数据集上，Moe-Ctc在低、高资源条件下对已见与未见口音均有提升，相比强基线FastConformer，词错误率最高下降29.3%。

Conclusion: Moe-Ctc架构有效在多口音自动语音识别中提升了模型的鲁棒性和泛化能力，在多种数据条件和多类型口音下均取得显著效果。

Abstract: Accented speech remains a persistent challenge for automatic speech recognition (ASR), as most models are trained on data dominated by a few high-resource English varieties, leading to substantial performance degradation for other accents. Accent-agnostic approaches improve robustness yet struggle with heavily accented or unseen varieties, while accent-specific methods rely on limited and often noisy labels. We introduce Moe-Ctc, a Mixture-of-Experts architecture with intermediate CTC supervision that jointly promotes expert specialization and generalization. During training, accent-aware routing encourages experts to capture accent-specific patterns, which gradually transitions to label-free routing for inference. Each expert is equipped with its own CTC head to align routing with transcription quality, and a routing-augmented loss further stabilizes optimization. Experiments on the Mcv-Accent benchmark demonstrate consistent gains across both seen and unseen accents in low- and high-resource conditions, achieving up to 29.3% relative WER reduction over strong FastConformer baselines.

</details>


### [409] [Orthogonal Hierarchical Decomposition for Structure-Aware Table Understanding with Large Language Models](https://arxiv.org/abs/2602.01969)
*Bin Cao,Huixian Lu,Chenwen Ma,Ting Wang,Ruizhe Li,Jing Fan*

Main category: cs.CL

TL;DR: 本文提出一种名为正交层次分解（OHD）的新框架，以更好地支持大语言模型（LLM）对复杂表格（如多层表头、合并单元格与异构布局）的理解和推理，并在多个问答基准上取得了领先的结果。


<details>
  <summary>Details</summary>
Motivation: 现有表格表征方法（如线性化或规整化网格）难以捕捉复杂表格的层次结构与跨维度依赖，导致结构语义和文本表述不一致，影响 LLM 对复杂表格的理解与推理能力。

Method: 作者提出 OHD 框架，利用空间-语义协同约束的正交树归纳（OTI）方法，将复杂表格分别分解为列树和行树，从而捕捉纵向和横向的层次依赖。同时设计双通道的关联协议对每个单元格的语义谱系进行对称重建，并引入 LLM 对多层语义信息进行对齐。

Result: 在 AITQA 和 HiTab 两个复杂表格问答基准上，OHD 框架在多个评测指标下均优于现有表征范式。

Conclusion: OHD 框架通过结构化表格分解及多层语义信息对齐，显著提升了 LLM 对复杂表格的理解和推理能力，在实际复杂表格问答任务中效果突出。

Abstract: Complex tables with multi-level headers, merged cells and heterogeneous layouts pose persistent challenges for LLMs in both understanding and reasoning. Existing approaches typically rely on table linearization or normalized grid modeling. However, these representations struggle to explicitly capture hierarchical structures and cross-dimensional dependencies, which can lead to misalignment between structural semantics and textual representations for non-standard tables. To address this issue, we propose an Orthogonal Hierarchical Decomposition (OHD) framework that constructs structure-preserving input representations of complex tables for LLMs. OHD introduces an Orthogonal Tree Induction (OTI) method based on spatial--semantic co-constraints, which decomposes irregular tables into a column tree and a row tree to capture vertical and horizontal hierarchical dependencies, respectively. Building on this representation, we design a dual-pathway association protocol to symmetrically reconstruct semantic lineage of each cell, and incorporate an LLM as a semantic arbitrator to align multi-level semantic information. We evaluate OHD framework on two complex table question answering benchmarks, AITQA and HiTab. Experimental results show that OHD consistently outperforms existing representation paradigms across multiple evaluation metrics.

</details>


### [410] [Beyond Local Edits: Embedding-Virtualized Knowledge for Broader Evaluation and Preservation of Model Editing](https://arxiv.org/abs/2602.01977)
*Shuainan Liu,Xuanang Chen,Ben He,Le Sun*

Main category: cs.CL

TL;DR: 本文提出了一种基于向量空间的虚拟知识评估方法（EVK），以及相应的评测基准与知识保持插件，用于更全面评估和提升大语言模型知识编辑的效果。


<details>
  <summary>Details</summary>
Motivation: 目前知识编辑方法常用的数据集仅覆盖有限样本，难以评估编辑对整体现有知识体系的广泛影响，导致知识漂移等潜在问题难以被发现和量化。

Method: 提出Embedding-Virtualized Knowledge（EVK），通过在嵌入空间中有控制的扰动，刻画和扩展模型的知识表达区域。基于EVK，构建了EVK-Bench基准用于评估知识漂移，并设计了可即插即用的EVK-Align模块来约束编辑时的知识漂移，可与现有编辑方法结合使用。

Result: 实验表明：EVK评测可以揭示传统采样评测无法发现的知识漂移，EVK-Align模块可以在不影响编辑准确率的前提下，显著提升知识保持能力。

Conclusion: 作者提出的方法为知识编辑效果的全面评估与知识保持带来了新的解决方案，有助于研发更鲁棒的大语言模型知识编辑技术。

Abstract: Knowledge editing methods for large language models are commonly evaluated using predefined benchmarks that assess edited facts together with a limited set of related or neighboring knowledge. While effective, such evaluations remain confined to finite, dataset-bounded samples, leaving the broader impact of editing on the model's knowledge system insufficiently understood. To address this gap, we introduce Embedding-Virtualized Knowledge (EVK) that characterizes model knowledge through controlled perturbations in embedding space, enabling the exploration of a substantially broader and virtualized knowledge region beyond explicit data annotations. Based on EVK, we construct an embedding-level evaluation benchmark EVK-Bench that quantifies potential knowledge drift induced by editing, revealing effects that are not captured by conventional sample-based metrics. Furthermore, we propose a plug-and-play EVK-Align module that constrains embedding-level knowledge drift during editing and can be seamlessly integrated into existing editing methods. Experiments demonstrate that our approach enables more comprehensive evaluation while significantly improving knowledge preservation without sacrificing editing accuracy.

</details>


### [411] [S3-CoT: Self-Sampled Succinct Reasoning Enables Efficient Chain-of-Thought LLMs](https://arxiv.org/abs/2602.01982)
*Yanrui Du,Sendong Zhao,Yibo Gao,Danyang Zhao,Qika Lin,Ming Ma,Jiayun Li,Yi Jiang,Kai He,Qianyi Xu,Bing Qin,Mengling Feng*

Main category: cs.CL

TL;DR: 本文提出了一种基于激活引导的自采样(CoT)学习框架，实现了大语言模型(LLMs)类 System 1（快速思考）推理能力的提升，无需教师标注数据。该方法在数学和医学等多领域获得了性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前链式思考(CoT)虽然提升了大模型表现，但存在冗余推理且训练数据稀缺。本文旨在解决如何让LLMs像人类那样具备高效、快速的直觉推理(System 1)，同时降低对高质量标注数据依赖。

Method: 提出一种基于激活引导的自采样框架，通过自监督方式引导模型学习多样并对齐特定风格和长度的推理痕迹，并结合答案过滤、双认知系统和逐步压缩课程，有效地进行高效CoT学习。此外，还探索了仅基于自一致预测驱动自进化训练，无需标注金答案。

Result: 该方法在数学基准和跨医学领域的推广测试中，显著提升了常规和R1风格LLMs的性能。

Conclusion: 自采样+激活引导的CoT学习方式克服了传统SFT方法标注数据稀缺的瓶颈，为LLMs类人认知推理能力的提升开辟了新路径，并具备较好通用性和推广性。

Abstract: Large language models (LLMs) equipped with chain-of-thought (CoT) achieve strong performance and offer a window into LLM behavior. However, recent evidence suggests that improvements in CoT capabilities often come with redundant reasoning processes, motivating a key question: Can LLMs acquire a fast-thinking mode analogous to human System 1 reasoning? To explore this, our study presents a self-sampling framework based on activation steering for efficient CoT learning. Our method can induce style-aligned and variable-length reasoning traces from target LLMs themselves without any teacher guidance, thereby alleviating a central bottleneck of SFT-based methods-the scarcity of high-quality supervision data. Using filtered data by gold answers, we perform SFT for efficient CoT learning with (i) a human-like dual-cognitive system, and (ii) a progressive compression curriculum. Furthermore, we explore a self-evolution regime in which SFT is driven solely by prediction-consistent data of variable-length variants, eliminating the need for gold answers. Extensive experiments on math benchmarks, together with cross-domain generalization tests in medicine, show that our method yields stable improvements for both general and R1-style LLMs. Our data and model checkpoints can be found at https://github.com/DYR1/S3-CoT.

</details>


### [412] [From Latent Signals to Reflection Behavior: Tracing Meta-Cognitive Activation Trajectory in R1-Style LLMs](https://arxiv.org/abs/2602.01999)
*Yanrui Du,Yibo Gao,Sendong Zhao,Jiayun Li,Haochun Wang,Qika Lin,Kai He,Bing Qin,Mengling Feng*

Main category: cs.CL

TL;DR: 本文通过层级激活轨迹分析，揭示了R1型大模型反思行为背后的结构化分阶段机制，发现其反思过程具有类人元认知特征。


<details>
  <summary>Details</summary>
Motivation: 尽管R1型大语言模型（LLM）以自我反思能力受到关注，但其内部实现机制尚不清楚。作者意在探索和揭示模型为何以及如何产生反思行为。

Method: 作者以反思行为的产生为切入点，利用logit lens方法层层追踪激活轨迹，细致分析每一层的语义和行为特征，并通过有针对性的干预实验验证各阶段之间的因果链。

Result: 发现模型反思行为呈现三阶段：前期“潜在控制层”以近似线性方向编码思考预算；中期“语义支点层”则话语提示（转折、总结）占主导；后期“行为外显层”中反思相关词概率显著提升。各阶段之间存在明确的因果链：输入语义调控激活投影，进而在“语义支点层”形成提示竞争，并影响最终行为输出。

Conclusion: 该研究揭示了R1型LLM反思行为的分层因果机制，暗示其具备类似人类的元认知处理流程。

Abstract: R1-style LLMs have attracted growing attention for their capacity for self-reflection, yet the internal mechanisms underlying such behavior remain unclear. To bridge this gap, we anchor on the onset of reflection behavior and trace its layer-wise activation trajectory. Using the logit lens to read out token-level semantics, we uncover a structured progression: (i) Latent-control layers, where an approximate linear direction encodes the semantics of thinking budget; (ii) Semantic-pivot layers, where discourse-level cues, including turning-point and summarization cues, surface and dominate the probability mass; and (iii) Behavior-overt layers, where the likelihood of reflection-behavior tokens begins to rise until they become highly likely to be sampled. Moreover, our targeted interventions uncover a causal chain across these stages: prompt-level semantics modulate the projection of activations along latent-control directions, thereby inducing competition between turning-point and summarization cues in semantic-pivot layers, which in turn regulates the sampling likelihood of reflection-behavior tokens in behavior-overt layers. Collectively, our findings suggest a human-like meta-cognitive process-progressing from latent monitoring, to discourse-level regulation, and to finally overt self-reflection. Our analysis code can be found at https://github.com/DYR1/S3-CoT.

</details>


### [413] [Beyond RAG for Agent Memory: Retrieval by Decoupling and Aggregation](https://arxiv.org/abs/2602.02007)
*Zhanghao Hu,Qinglin Zhu,Hanqi Yan,Yulan He,Lin Gui*

Main category: cs.CL

TL;DR: 本文提出面向智能体记忆的检索增强生成（RAG）机制改进方法——xMemory，替代传统的相似度检索，通过语义分解和层次结构优化检索结果，有效提升问答质量与效率。


<details>
  <summary>Details</summary>
Motivation: 针对以往RAG假设（大规模异质语料多样性）与智能体记忆实际情况（连贯对话流，重复和高度相关内容）不符，导致固定top-k检索结果冗余、去重方法又易误删关键信息的问题，作者提出需改变检索逻辑。

Method: xMemory方法将智能体记忆拆解为语义组件，构建层次化结构，通过稀疏-语义目标引导记忆拆分和聚合。推理时，采用自顶向下检索，优先选择高层次、结构化主题节点，仅在能降低不确定性时再扩展底层细节，实现更少冗余、主题集中的知识检索。

Result: 在LoCoMo和PerLTQA两套测试集，以及三种主流LLM上验证，xMemory在答案质量和Token使用效率上均取得一致提升。

Conclusion: xMemory能够有效解决智能体记忆场景下语义冗余和信息丢失问题，通过层次化记忆组织和创新检索方式，为复杂推理和多事实问答任务带来显著优势。

Abstract: Agent memory systems often adopt the standard Retrieval-Augmented Generation (RAG) pipeline, yet its underlying assumptions differ in this setting. RAG targets large, heterogeneous corpora where retrieved passages are diverse, whereas agent memory is a bounded, coherent dialogue stream with highly correlated spans that are often duplicates. Under this shift, fixed top-$k$ similarity retrieval tends to return redundant context, and post-hoc pruning can delete temporally linked prerequisites needed for correct reasoning. We argue retrieval should move beyond similarity matching and instead operate over latent components, following decoupling to aggregation: disentangle memories into semantic components, organise them into a hierarchy, and use this structure to drive retrieval. We propose xMemory, which builds a hierarchy of intact units and maintains a searchable yet faithful high-level node organisation via a sparsity--semantics objective that guides memory split and merge. At inference, xMemory retrieves top-down, selecting a compact, diverse set of themes and semantics for multi-fact queries, and expanding to episodes and raw messages only when it reduces the reader's uncertainty. Experiments on LoCoMo and PerLTQA across the three latest LLMs show consistent gains in answer quality and token efficiency.

</details>


### [414] [NEAT: Neuron-Based Early Exit for Large Reasoning Models](https://arxiv.org/abs/2602.02010)
*Kang Liu,Yongkang Liu,Xiaocui Yang,Peidong Wang,Wen Zhang,Shi Feng,Yifei Zhang,Daling Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新方法NEAT，能够在无需额外训练或推理开销的情况下，有效减少大型推理模型冗余的推理步骤。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型容易出现过度推理，即在得到正确解后依然继续推理，导致效率低下。现有方法为解决该问题通常需要外部训练或数据，存在额外开销。

Method: 作者提出了一种基于神经元激活动态的早期推理退出框架NEAT。该方法通过检测推理过程中的神经元激活状态，无需训练或额外测试计算，就能动态判断何时提前退出或者抑制冗余推理。

Result: 在四个推理基准任务、六种不同规模和架构的模型上实验，NEAT在保证准确率的同时，使每个模型平均token数减少了22%-28%。

Conclusion: NEAT实现了高效、训练无关的早期退出机制，有效缓解了大型推理模型的过度推理问题，提高了推理效率。

Abstract: Large Reasoning Models (LRMs) often suffer from \emph{overthinking}, a phenomenon in which redundant reasoning steps are generated after a correct solution has already been reached. Existing early reasoning exit methods primarily rely on output-level heuristics or trained probing models to skip redundant reasoning steps, thereby mitigating overthinking. However, these approaches typically require additional rollout computation or externally labeled datasets. In this paper, we propose \textbf{NEAT}, a \textbf{N}euron-based \textbf{E}arly re\textbf{A}soning exi\textbf{T} framework that monitors neuron-level activation dynamics to enable training-free early exits, without introducing additional test-time computation. NEAT identifies exit-associated neurons and tracks their activation patterns during reasoning to dynamically trigger early exit or suppress reflection, thereby reducing unnecessary reasoning while preserving solution quality. Experiments on four reasoning benchmarks across six models with different scales and architectures show that, for each model, NEAT achieves an average token reduction of 22\% to 28\% when averaged over the four benchmarks, while maintaining accuracy.

</details>


### [415] [WildGraphBench: Benchmarking GraphRAG with Wild-Source Corpora](https://arxiv.org/abs/2602.02053)
*Pengyu Wang,Benfeng Xu,Licheng Zhang,Shaohan Wang,Mingxuan Du,Chiwei Zhu,Zhendong Mao*

Main category: cs.CL

TL;DR: 本论文提出了WildGraphBench，一个用于评估图谱检索增强生成（GraphRAG）系统在真实场景下表现的基准数据集。该基准利用Wikipedia的外部引用作为检索语料和真实语句标签，包含1100个问题，涵盖不同复杂度的QA与摘要任务。实验结果表明现有GraphRAG在多源证据聚合时有效，但对细粒度细节的处理不足，影响了摘要任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有GraphRAG评测多以短小精悍的知识片段为外部知识，难以反映面对长文本和大规模异构文档时的真实表现。因此，迫切需要一个能更贴合实际应用场景的评测基准。

Method: 作者利用Wikipedia的结构，从12个主题抽样文章，将外部参考文献做为检索语料，以带有引用的语句为真实标签，设计三类任务（单事实问答、多事实问答、章节级摘要），共收集1100个问题，并在多种基线方法上做实验评测。

Result: 实验表明，当前GraphRAG框架在多证据聚合时表现优秀，提升了多事实问答能力，但在涉及大量来源和需要细粒度信息的摘要任务上，容易片面聚焦高层次表述，导致关键细节丢失。

Conclusion: WildGraphBench能更真实地衡量GraphRAG系统在复杂检索场景下的能力，发现了该类系统聚合策略的局限性，为今后算法改进和实际应用提供了重要基准和分析。

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) organizes external knowledge as a hierarchical graph, enabling efficient retrieval and aggregation of scattered evidence across multiple documents. However, many existing benchmarks for GraphRAG rely on short, curated passages as external knowledge, failing to adequately evaluate systems in realistic settings involving long contexts and large-scale heterogeneous documents. To bridge this gap, we introduce WildGraphBench, a benchmark designed to assess GraphRAG performance in the wild. We leverage Wikipedia's unique structure, where cohesive narratives are grounded in long and heterogeneous external reference documents, to construct a benchmark reflecting real-word scenarios. Specifically, we sample articles across 12 top-level topics, using their external references as the retrieval corpus and citation-linked statements as ground truth, resulting in 1,100 questions spanning three levels of complexity: single-fact QA, multi-fact QA, and section-level summarization. Experiments across multiple baselines reveal that current GraphRAG pipelines help on multi-fact aggregation when evidence comes from a moderate number of sources, but this aggregation paradigm may overemphasize high-level statements at the expense of fine-grained details, leading to weaker performance on summarization tasks. Project page:https://github.com/BstWPY/WildGraphBench.

</details>


### [416] [Closing the Loop: Universal Repository Representation with RPG-Encoder](https://arxiv.org/abs/2602.02084)
*Jane Luo,Chengyu Yin,Xin Zhang,Qingtao Li,Steven Liu,Yiming Huang,Jie Wu,Hao Liu,Yangyu Huang,Yu Kang,Fangkai Yang,Ying Xin,Scarlett Li*

Main category: cs.CL

TL;DR: 该论文提出了一种统一的代码仓库理解与生成框架RPG-Encoder，通过高保真的图结构整合代码语义和依赖，显著提升了复杂代码库的理解和定位能力。


<details>
  <summary>Details</summary>
Motivation: 现有仓库分析依赖零散的API文档或依赖图，语义关联薄弱，导致自动化理解和生成代码时存在推理断层，难以应对复杂代码库的精细化需求。

Method: 作者提出RPG-Encoder，其核心是将代码语义特征与依赖关系融合到统一的Repository Planning Graph (RPG)结构中。方法包括：(1) 编码原始代码为结合语义和依赖的RPG；(2) 通过增量演化RPG的拓扑，降低随仓库规模增长的维护成本，开销减少95.7%；(3) 提供结构感知的统一接口，便于高效导航和操作。

Result: 在SWE-bench Verified数据集上，RPG-Encoder取得93.7%的Acc@5，超越最佳基线方法10%以上；在RepoCraft数据集上，RPG的代码重建覆盖率达98.5%，显示出极高的仓库结构还原能力。

Conclusion: RPG-Encoder通过闭环的生成-理解机制和高保真RPG建模，实现了复杂代码库的高精度理解和定位，为自动化的代码分析与生成任务奠定了坚实基础。

Abstract: Current repository agents encounter a reasoning disconnect due to fragmented representations, as existing methods rely on isolated API documentation or dependency graphs that lack semantic depth. We consider repository comprehension and generation to be inverse processes within a unified cycle: generation expands intent into implementation, while comprehension compresses implementation back into intent. To address this, we propose RPG-Encoder, a framework that generalizes the Repository Planning Graph (RPG) from a static generative blueprint into a unified, high-fidelity representation. RPG-Encoder closes the reasoning loop through three mechanisms: (1) Encoding raw code into the RPG that combines lifted semantic features with code dependencies; (2) Evolving the topology incrementally to decouple maintenance costs from repository scale, reducing overhead by 95.7%; and (3) Operating as a unified interface for structure-aware navigation. In evaluations, RPG-Encoder establishes state-of-the-art repository understanding on SWE-bench Verified with 93.7% Acc@5 and exceeds the best baseline by over 10% on SWE-bench Live Lite. These results highlight our superior fine-grained localization accuracy in complex codebases. Furthermore, it achieves 98.5% reconstruction coverage on RepoCraft, confirming RPG's high-fidelity capacity to mirror the original codebase and closing the loop between intent and implementation.

</details>


### [417] [LEC-KG: An LLM-Embedding Collaborative Framework for Domain-Specific Knowledge Graph Construction -- A Case Study on SDGs](https://arxiv.org/abs/2602.02090)
*Yikai Zeng,Yingchao Piao,Jianhui Li*

Main category: cs.CL

TL;DR: LEC-KG 结合大语言模型和知识图谱嵌入，实现对领域文本的高质量知识图谱构建，对长尾关系和未见实体表现优秀。


<details>
  <summary>Details</summary>
Motivation: 从非结构化文本构建领域知识图谱难度较大，主要因为实体描述异构、关系分布长尾、缺乏统一结构。需要结合语义和结构优势，提高抽取准确性和覆盖面。

Method: 提出了LEC-KG双向协作框架：包括层次化粗到细关系抽取以解决长尾问题，基于证据的链式思考反馈将结构建议回溯到原文，以及语义初始化支持未见实体的结构校验。KGE与LLM模块反复交互，互相提升。

Result: 在中国可持续发展目标（SDG）报告上，LEC-KG取得相对LLM基线明显提升，尤其对于低频关系抽取更有效。

Conclusion: LEC-KG 通过迭代优化可将政策类非结构化文本高效转化为经过验证的知识图谱三元组，解决了多个现实挑战。

Abstract: Constructing domain-specific knowledge graphs from unstructured text remains challenging due to heterogeneous entity mentions, long-tail relation distributions, and the absence of standardized schemas. We present LEC-KG, a bidirectional collaborative framework that integrates the semantic understanding of Large Language Models (LLMs) with the structural reasoning of Knowledge Graph Embeddings (KGE). Our approach features three key components: (1) hierarchical coarse-to-fine relation extraction that mitigates long-tail bias, (2) evidence-guided Chain-of-Thought feedback that grounds structural suggestions in source text, and (3) semantic initialization that enables structural validation for unseen entities. The two modules enhance each other iteratively-KGE provides structure-aware feedback to refine LLM extractions, while validated triples progressively improve KGE representations. We evaluate LEC-KG on Chinese Sustainable Development Goal (SDG) reports, demonstrating substantial improvements over LLM baselines, particularly on low-frequency relations. Through iterative refinement, our framework reliably transforms unstructured policy text into validated knowledge graph triples.

</details>


### [418] [Think Dense, Not Long: Dynamic Decoupled Conditional Advantage for Efficient Reasoning](https://arxiv.org/abs/2602.02099)
*Keqin Peng,Yuanxin Ouyang,Xuebo Liu,Zhiliang Tian,Ruijian Han,Yancheng Yuan,Liang Ding*

Main category: cs.CL

TL;DR: 提出了一种新的奖惩机制（DDCA），有效提升了强化学习中推理任务的效率，同时保持甚至提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于可验证奖励的强化学习（RLVR）方法在激发多步推理能力时，容易生成冗长的推理轨迹。简单的长度惩罚会严重损害准确率，导致效率与准确性难以兼得。分析发现，问题主要在于（1）长度基线稀释：错误答案（奖励为0）压低了基线，正确解受到过度惩罚；（2）难度-惩罚失配：静态惩罚无法针对题目难度自适应调整。

Method: 提出Dynamic Decoupled Conditional Advantage（DDCA）方法，具体做法为：只在正确解簇内部计算长度优势，杜绝基线稀释；用小组通过率指标动态调整惩罚强度，实现难度自适应。该方法在多套数学推理数据集上进行了实验验证。

Result: 在GSM8K、MATH500、AMC23、AIME25等数据集上，DDCA相较于原有自适应基线，能大幅减少输出token数量（如简单任务上减少约60%，难任务上减少20%以上），同时保持或提升了准确率。

Conclusion: DDCA策略有效提升了强化学习推理效率，并保持甚至提升了准确性。它解决了传统长度惩罚机制的结构性弊端，在多种推理任务上表现稳定，具有广泛应用前景。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) can elicit strong multi-step reasoning, yet it often encourages overly verbose traces. Moreover, naive length penalties in group-relative optimization can severely hurt accuracy. We attribute this failure to two structural issues: (i) Dilution of Length Baseline, where incorrect responses (with zero length reward) depress the group baseline and over-penalize correct solutions; and (ii) Difficulty-Penalty Mismatch, where a static penalty cannot adapt to problem difficulty, suppressing necessary reasoning on hard instances while leaving redundancy on easy ones. We propose Dynamic Decoupled Conditional Advantage (DDCA) to decouple efficiency optimization from correctness. DDCA computes length advantages conditionally within the correct-response cluster to eliminate baseline dilution, and dynamically scales the penalty strength using the group pass rate as a proxy for difficulty. Experiments on GSM8K, MATH500, AMC23, and AIME25 show that DDCA consistently improves the efficiency--accuracy trade-off relative to adaptive baselines, reducing generated tokens by approximately 60% on simpler tasks (e.g., GSM8K) versus over 20% on harder benchmarks (e.g., AIME25), thereby maintaining or improving accuracy. Code is available at https://github.com/alphadl/DDCA.

</details>


### [419] [Dicta-LM 3.0: Advancing The Frontier of Hebrew Sovereign LLMs](https://arxiv.org/abs/2602.02104)
*Shaltiel Shmidman,Avi Shmidman,Amir DN Cohen,Moshe Koppel*

Main category: cs.CL

TL;DR: 本文发布了Dicta-LM 3.0，一套专为希伯来语和英语训练的大型开源语言模型，并针对低资源语言 LLM 训练难题提供了系统解决办法。


<details>
  <summary>Details</summary>
Motivation: 在大型开源语言模型日益丰富的背景下，非英语（尤其是低资源语言如希伯来语）的大模型依然稀缺，导致对主权型 LLM 的强烈需求。

Method: 基于Mistral-Small-3.1、NVIDIA Nemotron Nano V2和Qwen3-1.7B三大主模型，分别训练、调整出24B、12B、1.7B三个不同参数量的模型，覆盖base和chat版本（含工具调用支持），每个模型支持65k上下文长度。并新构建了涵盖翻译、摘要、Winograd、以色列知识问答和音标还原的希伯来语基准测试集，用于严谨评测。

Result: 成功发布了三种参数量和多种变体的希伯来-英语 LLM，并提供了专业评测基准，验证模型在多样任务上的有效性。

Conclusion: 本研究解决了低资源语言 LLM 的训练与评估痛点，提出的构建和适配流程为多语种NLP领域提供了借鉴框架，有助于推动非英语模型的发展。

Abstract: Open-weight LLMs have been released by frontier labs; however, sovereign Large Language Models (for languages other than English) remain low in supply yet high in demand. Training large language models (LLMs) for low-resource languages such as Hebrew poses unique challenges. In this paper, we introduce Dicta-LM 3.0: an open-weight collection of LLMs trained on substantially-sized corpora of Hebrew and English texts. The model is released in three sizes: 24B - adapted from the Mistral-Small-3.1 base model, 12B - adapted from the NVIDIA Nemotron Nano V2 model, and 1.7B - adapted from the Qwen3-1.7B base model. We are releasing multiple variants of each model, each with a native context length of 65k tokens; base model and chat model with tool-calling support. To rigorously evaluate our models, we introduce a new benchmark suite for evaluation of Hebrew chat-LLMs, covering a diverse set of tasks including Translation, Summarization, Winograd, Israeli Trivia, and Diacritization (nikud). Our work not only addresses the intricacies of training LLMs in low-resource languages but also proposes a framework that can be leveraged for adapting other LLMs to various non-English languages, contributing to the broader field of multilingual NLP.

</details>


### [420] [Out of the Memory Barrier: A Highly Memory Efficient Training System for LLMs with Million-Token Contexts](https://arxiv.org/abs/2602.02108)
*Wenhao Li,Daohai Yu,Gen Luo,Yuxin Zhang,Fei Chao,Rongrong Ji,Yifan Wu,Jiaxin Liu,Ziyang Gong,Zimu Liao*

Main category: cs.CL

TL;DR: 本文提出OOMB系统，通过创新性技术大幅降低训练长上下文大语言模型时的显存消耗，使4M-token长度的训练在单卡即可实现。


<details>
  <summary>Details</summary>
Motivation: 训练长上下文大语言模型受限于GPU显存开销，尤其是激活值占用随序列长度线性增长，成为核心瓶颈。

Method: 采用chunk-recurrent框架结合激活重计算，实现激活值O(1)显存占用，并针对KV cache采用分页管理、异步CPU迁移和稀疏注意力等技术降低内存与通信消耗。

Result: 实验证明，每增加1万tokens，训练显存仅增加10MB；Qwen2.5-7B在单张H200 GPU上即可以4M-token context训练，原本需要分布式大集群。

Conclusion: OOMB大幅提升长上下文大语言模型训练的资源效率，开创了单卡高效训练新模式。源码已开源。

Abstract: Training Large Language Models (LLMs) on long contexts is severely constrained by prohibitive GPU memory overhead, not training time. The primary culprits are the activations, whose memory footprints scale linearly with sequence length. We introduce OOMB, a highly memory-efficient training system that directly confronts this barrier. Our approach employs a chunk-recurrent training framework with on-the-fly activation recomputation, which maintains a constant activation memory footprint (O(1)) and shifts the primary bottleneck to the growing KV cache. To manage the KV cache, OOMB integrates a suite of synergistic optimizations: a paged memory manager for both the KV cache and its gradients to eliminate fragmentation, asynchronous CPU offloading to hide data transfer latency, and page-level sparse attention to reduce both computational complexity and communication overhead. The synergy of these techniques yields exceptional efficiency. Our empirical results show that for every additional 10K tokens of context, the end-to-end training memory overhead increases by a mere 10MB for Qwen2.5-7B. This allows training Qwen2.5-7B with a 4M-token context on a single H200 GPU, a feat that would otherwise require a large cluster using context parallelism. This work represents a substantial advance in resource efficiency for long-context LLM training. The source code is available at https://github.com/wenhaoli-xmu/OOMB.

</details>


### [421] [There Is More to Refusal in Large Language Models than a Single Direction](https://arxiv.org/abs/2602.02132)
*Faaiz Joad,Majd Hawasly,Sabri Boughorbel,Nadir Durrani,Husrev Taha Sencar*

Main category: cs.CL

TL;DR: 本文发现大型语言模型中的拒绝行为并非由单一激活空间方向控制，而是存在多种不同的激活方向，每种行为对应不同的空间，但是通过简单线性操作依然可实现相似的拒绝调节效果。


<details>
  <summary>Details</summary>
Motivation: 此前研究认为，可以通过对激活空间中单一路径进行操控，控制模型的拒绝行为；本文质疑并扩展了这种看法，旨在更深入理解拒绝行为的本质及其可控性。

Method: 作者针对十一类拒绝和不服从行为（包括安全、请求不全/无支持、人性化、过度拒绝等），在激活空间中分析这些行为的几何方向，并测试线性调节策略对各类拒绝行为的影响。

Result: 发现不同类型的拒绝和不服从行为在激活空间中具有不同几何方向，但在线性调节时，各方向表现出类似的拒绝与过度拒绝之间的权衡，形成一种通用的一维控制机制。

Conclusion: 尽管拒绝行为有多种几何表现形式，但线性调节任一相关方向都能以类似方式调节拒绝特性；不同方向的主要作用在于影响模型“如何拒绝”，而非“是否拒绝”。

Abstract: Prior work argues that refusal in large language models is mediated by a single activation-space direction, enabling effective steering and ablation. We show that this account is incomplete. Across eleven categories of refusal and non-compliance, including safety, incomplete or unsupported requests, anthropomorphization, and over-refusal, we find that these refusal behaviors correspond to geometrically distinct directions in activation space. Yet despite this diversity, linear steering along any refusal-related direction produces nearly identical refusal to over-refusal trade-offs, acting as a shared one-dimensional control knob. The primary effect of different directions is not whether the model refuses, but how it refuses.

</details>


### [422] [Quantifying the Gap between Understanding and Generation within Unified Multimodal Models](https://arxiv.org/abs/2602.02140)
*Chenlong Wang,Yuhang Chen,Zhihan Hu,Dongping Chen,Wenhu Chen,Sarah Wiegreffe,Tianyi Zhou*

Main category: cs.CL

TL;DR: 本文提出GapEval基准，专门用于评估统一多模态模型（UMM）在理解和生成两项能力上的差距，发现它们之间依然存在明显的不一致。


<details>
  <summary>Details</summary>
Motivation: 虽然UMM在理解和生成任务上均取得显著进展，但尚不清楚这两种能力是否真正实现了统一和融合。作者希望通过系统评估确定模型内部的认知一致性。

Method: 作者提出了GapEval，这是一个双向评测基准，使每个问题都可以用图像或文本两种形式回答，从而对模型的双向推理能力和跨模态一致性进行对称评测。同时，开展了知识操控相关的实证研究。

Result: 实验证明，大量不同架构的UMM在理解与生成两个方向上存在持续性差距，表明目前的模型只实现了表面的统一，并未达到深层认知融合。此外，知识在多模态间表现为割裂状态。

Conclusion: 当前UMM在理解和生成能力上还未实现知识与能力的深度整合，跨模态的一致性和能力同步性均有待提升。这为后续模型研究指明了改进方向。

Abstract: Recent advances in unified multimodal models (UMM) have demonstrated remarkable progress in both understanding and generation tasks. However, whether these two capabilities are genuinely aligned and integrated within a single model remains unclear. To investigate this question, we introduce GapEval, a bidirectional benchmark designed to quantify the gap between understanding and generation capabilities, and quantitatively measure the cognitive coherence of the two "unified" directions. Each question can be answered in both modalities (image and text), enabling a symmetric evaluation of a model's bidirectional inference capability and cross-modal consistency. Experiments reveal a persistent gap between the two directions across a wide range of UMMs with different architectures, suggesting that current models achieve only surface-level unification rather than deep cognitive convergence of the two. To further explore the underlying mechanism, we conduct an empirical study from the perspective of knowledge manipulation to illustrate the underlying limitations. Our findings indicate that knowledge within UMMs often remains disjoint. The capability emergence and knowledge across modalities are unsynchronized, paving the way for further exploration.

</details>


### [423] [Focus-dLLM: Accelerating Long-Context Diffusion LLM Inference via Confidence-Guided Context Focusing](https://arxiv.org/abs/2602.02159)
*Lingkun Long,Yushi Huang,Shihao Bai,Ruihao Gong,Jun Zhang,Ao Zhou,Jianlei Yang*

Main category: cs.CL

TL;DR: Focus-dLLM提出了一种无需训练的注意力稀疏化框架，大幅提升了扩散式大语言模型（dLLM）在长上下文推理时的速度（32K上下文下提速超29倍），且不损失性能。


<details>
  <summary>Details</summary>
Motivation: 扩散式大语言模型因其非自回归解码和强长上下文能力而受关注，但因双向全注意力带来的计算开销，推理效率低下，现有稀疏注意力方法对未知token位置处理不佳，需新的高效稀疏化方案。

Method: 提出基于置信度相关性的历史置信度引导指示器，用以预测未掩码区域，并提出sink-aware剪枝策略，精准估算并去除冗余注意力计算，保留关键的注意力sink点。此外，将sink点在不同层间复用，减小进一步开销。

Result: 在32K长上下文下，无损性能加速超过29倍。实验表明方法能兼顾准确率与显著推理效率提升。

Conclusion: Focus-dLLM无需额外训练，能高效且准确地支持长上下文dLLM推理，在实际应用中有广阔前景。

Abstract: Diffusion Large Language Models (dLLMs) deliver strong long-context processing capability in a non-autoregressive decoding paradigm. However, the considerable computational cost of bidirectional full attention limits the inference efficiency. Although sparse attention is promising, existing methods remain ineffective. This stems from the need to estimate attention importance for tokens yet to be decoded, while the unmasked token positions are unknown during diffusion. In this paper, we present Focus-dLLM, a novel training-free attention sparsification framework tailored for accurate and efficient long-context dLLM inference. Based on the finding that token confidence strongly correlates across adjacent steps, we first design a past confidence-guided indicator to predict unmasked regions. Built upon this, we propose a sink-aware pruning strategy to accurately estimate and remove redundant attention computation, while preserving highly influential attention sinks. To further reduce overhead, this strategy reuses identified sink locations across layers, leveraging the observed cross-layer consistency. Experimental results show that our method offers more than $29\times$ lossless speedup under $32K$ context length. The code is publicly available at: https://github.com/Longxmas/Focus-dLLM

</details>


### [424] [D-CORE: Incentivizing Task Decomposition in Large Reasoning Models for Complex Tool Use](https://arxiv.org/abs/2602.02160)
*Bowen Xu,Shaoyu Wu,Hao Jiang,Kai Liu,Xin Chen,Lulu Hu,Bin Yang*

Main category: cs.CL

TL;DR: 本文针对大规模推理模型（LRMs）在复杂工具使用场景下缺乏子任务分解能力，提出了两阶段训练框架D-CORE，显著提升了模型工具使用和推理表现，并在多个基准测试中刷新成绩。


<details>
  <summary>Details</summary>
Motivation: 现有LRMs在面对复杂工具使用任务时，难以进行有效的子任务分解，导致推理时存在“懒惰推理”（Lazy Reasoning）问题。这种不足限制了模型在实际复杂任务中的表现。

Method: 提出D-CORE两阶段训练框架，第一阶段利用自蒸馏增强模型子任务分解推理能力，第二阶段采用多样性感知的强化学习恢复模型的反思推理能力。

Result: D-CORE在多个基准测试和不同规模模型上均表现出强劲性能提升。在BFCLv3数据集上，D-CORE-8B模型准确率达77.7%，比现有最优8B模型高5.7%；D-CORE-14B模型达到79.3%，超越了参数量大5倍的70B模型。

Conclusion: D-CORE极大提升了大模型在工具使用和复杂任务推理方面的能力，并具有良好的扩展性和泛化性。

Abstract: Effective tool use and reasoning are essential capabilities for large reasoning models~(LRMs) to address complex real-world problems. Through empirical analysis, we identify that current LRMs lack the capability of sub-task decomposition in complex tool use scenarios, leading to Lazy Reasoning. To address this, we propose a two-stage training framework D-CORE~(\underline{\textbf{D}}ecomposing tasks and \underline{\textbf{Co}}mposing \underline{\textbf{Re}}asoning processes) that first incentivize the LRMs' task decomposition reasoning capability via self-distillation, followed by diversity-aware reinforcement learning~(RL) to restore LRMs' reflective reasoning capability. D-CORE achieves robust tool-use improvements across diverse benchmarks and model scales. Experiments on BFCLv3 demonstrate superiority of our method: D-CORE-8B reaches 77.7\% accuracy, surpassing the best-performing 8B model by 5.7\%. Meanwhile, D-CORE-14B establishes a new state-of-the-art at 79.3\%, outperforming 70B models despite being 5$\times$ smaller. The source code is available at https://github.com/alibaba/EfficientAI.

</details>


### [425] [AR-MAP: Are Autoregressive Large Language Models Implicit Teachers for Diffusion Large Language Models?](https://arxiv.org/abs/2602.02178)
*Liang Lin,Feng Xiong,Zengbin Wang,Kun Wang,Junhao Dong,Xuecai Hu,Yong Wang,Xiangxiang Chu*

Main category: cs.CL

TL;DR: 提出了一种新的迁移学习框架AR-MAP，无需直接对Diffusion大语言模型（DLLM）进行高方差偏好对齐，而是通过已有的自回归大模型间接实现模型对齐，提升了DLLM在偏好对齐任务上的表现。


<details>
  <summary>Details</summary>
Motivation: DLLM虽然支持高效的并行生成，但在偏好对齐上受限于ELBO估计法导致的高方差，效果不佳。因此，研究团队希望借助已有偏好对齐、自回归大模型的优势，提升DLLM的对齐能力。

Method: AR-MAP框架把已经偏好对齐好的自回归大模型（AR-LLM）当成隐式教师，通过权重缩放技术，把它们的知识迁移到DLLM上，而无需对DLLM重复高计算量、方差大的对齐调优。

Result: AR-MAP在各种偏好对齐任务和模型上的平均得分达到69.08%，在多个基准上优于或媲美当前DLLM专用的对齐方法，同时降低了计算复杂度。

Conclusion: AR-MAP框架为DLLM对齐带来了更高性能和更低开销，展现了迁移自自回归大模型的对齐知识为并行生成模型带来的巨大潜力。

Abstract: Diffusion Large Language Models (DLLMs) have emerged as a powerful alternative to autoregressive models, enabling parallel token generation across multiple positions. However, preference alignment of DLLMs remains challenging due to high variance introduced by Evidence Lower Bound (ELBO)-based likelihood estimation. In this work, we propose AR-MAP, a novel transfer learning framework that leverages preference-aligned autoregressive LLMs (AR-LLMs) as implicit teachers for DLLM alignment. We reveal that DLLMs can effectively absorb alignment knowledge from AR-LLMs through simple weight scaling, exploiting the shared architectural structure between these divergent generation paradigms. Crucially, our approach circumvents the high variance and computational overhead of direct DLLM alignment and comprehensive experiments across diverse preference alignment tasks demonstrate that AR-MAP achieves competitive or superior performance compared to existing DLLM-specific alignment methods, achieving 69.08\% average score across all tasks and models. Our Code is available at https://github.com/AMAP-ML/AR-MAP.

</details>


### [426] [Evaluating Metalinguistic Knowledge in Large Language Models across the World's Languages](https://arxiv.org/abs/2602.02182)
*Tjaša Arčon,Matej Klemen,Marko Robnik-Šikonja,Kaja Dobrovoljc*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型（LLM）对语言结构的元语言知识，发现其表现有限，受语料资源丰富度影响明显。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM常用于语言任务，但其对语言结构的理解能力尚不明晰，现有基准多聚焦高资源语言及具体现象，且缺乏对元语言知识的系统评测。

Method: 作者构建了包含多语种语言结构问题的开放数据集，使用准确率和宏F1作为核心指标，比较不同模型、语言领域和资源相关因素的表现，并与主流和随机基线对比。

Result: GPT-4o表现最佳（0.367），但整体准确率仅中等，开源模型表现更差。所有模型虽优于随机，但未超越主流基线，表明其缺乏精细的语法区分能力。词汇特征得分最高，音系特征最低，高数字资源的语言准确率更高。语料资源（如维基百科规模、语料可用性）是表现的主要预测因子。

Conclusion: 当前LLM的元语言知识碎片化，主要受数据可用性左右，而非具备普遍语法能力。文章发布了相关基准数据，仅以促进更多全球语言多样性和系统性评估。

Abstract: Large language models (LLMs) are routinely evaluated on language use tasks, yet their knowledge of linguistic structure remains poorly understood. Existing linguistic benchmarks typically focus on narrow phenomena, emphasize high-resource languages, and rarely evaluate metalinguistic knowledge-explicit reasoning about language structure rather than language use. Using accuracy and macro F1, together with majority-class and chance baselines, we analyse overall performance and examine variation by linguistic domains and language-related factors. Our results show that metalinguistic knowledge in current LLMs is limited: GPT-4o performs best but achieves only moderate accuracy (0.367), while open-source models lag behind. All models perform above chance but fail to outperform the majority-class baseline, suggesting they capture cross-linguistic patterns but lack fine-grained grammatical distinctions. Performance varies across linguistic domains, with lexical features showing the highest accuracy and phonological features among the lowest, partially reflecting differences in online visibility. At the language level, accuracy shows a strong association with digital language status: languages with higher digital presence and resource availability are evaluated more accurately, while low-resource languages show substantially lower performance. Analyses of predictive factors confirm that resource-related indicators (Wikipedia size, corpus availability) are more informative predictors of accuracy than geographical, genealogical, or sociolinguistic factors. Together, these results suggest that LLMs' metalinguistic knowledge is fragmented and shaped by data availability rather than generalizable grammatical competence across the world's languages. We release our benchmark as an open-source dataset to support systematic evaluation and encourage greater global linguistic diversity in future LLMs.

</details>


### [427] [Sinhala Physical Common Sense Reasoning Dataset for Global PIQA](https://arxiv.org/abs/2602.02207)
*Nisansa de Silva,Surangika Ranathunga*

Main category: cs.CL

TL;DR: 本文介绍了第一个僧伽罗语的物理常识推理数据集，包含110个人工创建并验证的样本，用于测试AI在斯里兰卡语境下的常识推理能力。


<details>
  <summary>Details</summary>
Motivation: 全球针对物理常识推理（PIQA）的数据集大多以英语为主，缺乏适用于其他语言和区域的资源。僧伽罗语作为斯里兰卡的官方语言之一，目前没有此类高质量数据集。为推动多语言常识推理研究，作者开发了该数据集。

Method: 人工编写并验证了110个僧伽罗语问题，每个问题包含一个情景描述（prompt）、一个正确答案和一个错误答案，主要聚焦于与斯里兰卡相关的生活常识问题。

Result: 作者建立了一个全新的数据集，可以用于评估和提升僧伽罗语环境下AI的物理常识推理能力。该数据集填补了相关研究领域的空白。

Conclusion: 该数据集有助于促进多语言常识推理AI研究，特别是针对资源稀缺语言如僧伽罗语，为后续工作提供了基础。

Abstract: This paper presents the first-ever Sinhala physical common sense reasoning dataset created as part of Global PIQA. It contains 110 human-created and verified data samples, where each sample consists of a prompt, the corresponding correct answer, and a wrong answer. Most of the questions refer to the Sri Lankan context, where Sinhala is an official language.

</details>


### [428] [Towards AI Evaluation in Domain-Specific RAG Systems: The AgriHubi Case Study](https://arxiv.org/abs/2602.02208)
*Md. Toufique Hasan,Ayman Asad Khan,Mika Saari,Vaishnavi Bankhele,Pekka Abrahamsson*

Main category: cs.CL

TL;DR: 本文提出了一种面向低资源语言（以芬兰语为例）农业领域的 RAG（检索增强生成）系统 AgriHubi，针对农业决策支持优化，并通过两项用户研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在知识密集型领域有潜力，但受限于英文为主的训练数据、缺乏实际评测，尤其在对于低资源语言的农业领域，尽管有高质量文档，却难以利用普通模型获取，因此需要专门的领域适配方案。

Method: 提出了 AgriHubi 系统，将芬兰语农业文档与开源 PORO 系列模型结合，采用显式文本溯源与用户反馈机制，并在八轮迭代后，通过两次用户实验进行评估。

Result: 系统在答案完整性、语言准确性、可靠性等方面取得明显提升，并揭示了模型规模提升在响应质量与延时之间的实际权衡。

Conclusion: 该研究为低资源语言下的领域专用 RAG 系统设计和评估提供了实证指导。

Abstract: Large language models show promise for knowledge-intensive domains, yet their use in agriculture is constrained by weak grounding, English-centric training data, and limited real-world evaluation. These issues are amplified for low-resource languages, where high-quality domain documentation exists but remains difficult to access through general-purpose models. This paper presents AgriHubi, a domain-adapted retrieval-augmented generation (RAG) system for Finnish-language agricultural decision support. AgriHubi integrates Finnish agricultural documents with open PORO family models and combines explicit source grounding with user feedback to support iterative refinement. Developed over eight iterations and evaluated through two user studies, the system shows clear gains in answer completeness, linguistic accuracy, and perceived reliability. The results also reveal practical trade-offs between response quality and latency when deploying larger models. This study provides empirical guidance for designing and evaluating domain-specific RAG systems in low-resource language settings.

</details>


### [429] [Am I More Pointwise or Pairwise? Revealing Position Bias in Rubric-Based LLM-as-a-Judge](https://arxiv.org/abs/2602.02219)
*Yuzheng Xu,Tosho Hirasawa,Tadashi Kozuno,Yoshitaka Ushiku*

Main category: cs.CL

TL;DR: 论文发现当前常用的基于标准的LLM评估方法存在位置偏置，作者提出了一种基于排序均衡的新策略来缓解该问题，显著提升了LLM评分与人工评分的一致性。


<details>
  <summary>Details</summary>
Motivation: 以往研究主要关注LLM在点式或成对评估中的表现，较少关注LLM按评分标准（rubric）进行多项选择时可能存在的系统性偏见，尤其是选项位置导致的偏差。研究动机在于提升LLM作为文本评估判官时的公正性和可靠性。

Method: 通过在多个模型和数据集上设置对照实验，测试LLM在不同评分项位置下的选择倾向，并设计一种“均衡置换”方法，即让每个评分选项在不同位置均匀出现，然后对多次评分结果进行聚合，以此评估和校正位置偏置。

Result: 实验证明，无论模型或数据集，LLM对列表中某些固定位置的评分项有明显偏好。所提出的均衡置换策略有效识别并削弱了此种位置偏差，同时提高了机器评判与人工评判的一致性。

Conclusion: Rubric制的LLM评判系统不是天然点式评估，存在多项选择相关偏差。通过简单的换位与聚合策略可以显著提升其评分结果的可靠性和公正性。

Abstract: Large language models (LLMs) are now widely used to evaluate the quality of text, a field commonly referred to as LLM-as-a-judge. While prior works mainly focus on point-wise and pair-wise evaluation paradigms. Rubric-based evaluation, where LLMs select a score from multiple rubrics, has received less analysis. In this work, we show that rubric-based evaluation implicitly resembles a multi-choice setting and therefore has position bias: LLMs prefer score options appearing at specific positions in the rubric list. Through controlled experiments across multiple models and datasets, we demonstrate consistent position bias. To mitigate this bias, we propose a balanced permutation strategy that evenly distributes each score option across positions. We show that aggregating scores across balanced permutations not only reveals latent position bias, but also improves correlation between the LLM-as-a-Judge and human. Our results suggest that rubric-based LLM-as-a-Judge is not inherently point-wise and that simple permutation-based calibration can substantially improve its reliability.

</details>


### [430] [Using Correspondence Patterns to Identify Irregular Words in Cognate sets Through Leave-One-Out Validation](https://arxiv.org/abs/2602.02221)
*Frederic Blum,Johann-Mattis List*

Main category: cs.CL

TL;DR: 该论文提出了一种新的正则性衡量指标及其算法，用于量化历史语言比较中语音对应规律的正则性，并能识别缺乏正则性的同源词集。实验结果显示新方法在实际数据集上表现出较高的准确率。


<details>
  <summary>Details</summary>
Motivation: 在历史语言学中，语音对应的正则性是比较语言的重要依据，但过去主要依靠直觉判定，缺乏定量评估。随着标准化词汇数据和计算方法的发展，有必要用更客观和可量化的方法来评估正则性。

Method: 作者提出了一种名为『平衡平均复现率』的新正则性度量方法，并基于该方法开发了能自动识别不规则同源词集的计算方法。采用模拟数据和真实数据并采用留一法验证算法效果，通过替换不规则词项，检验算法是否能准确辨认产生不规则的词。

Result: 在基于真实数据集的实验中，该方法能以85%的准确率识别导致不规则的词项。并探讨了数据子样本及不规则性增加对结果的影响。

Conclusion: 新提出的正则性度量方法及其相关算法能够提升计算语言比较数据集的质量，对未来的计算语言学研究和数据集完善具有重要价值。

Abstract: Regular sound correspondences constitute the principal evidence in historical language comparison. Despite the heuristic focus on regularity, it is often more an intuitive judgement than a quantified evaluation, and irregularity is more common than expected from the Neogrammarian model. Given the recent progress of computational methods in historical linguistics and the increased availability of standardized lexical data, we are now able to improve our workflows and provide such a quantitative evaluation. Here, we present the balanced average recurrence of correspondence patterns as a new measure of regularity. We also present a new computational method that uses this measure to identify cognate sets that lack regularity with respect to their correspondence patterns. We validate the method through two experiments, using simulated and real data. In the experiments, we employ leave-one-out validation to measure the regularity of cognate sets in which one word form has been replaced by an irregular one, checking how well our method identifies the forms causing the irregularity. Our method achieves an overall accuracy of 85\% with the datasets based on real data. We also show the benefits of working with subsamples of large datasets and how increasing irregularity in the data influences our results. Reflecting on the broader potential of our new regularity measure and the irregular cognate identification method based on it, we conclude that they could play an important role in improving the quality of existing and future datasets in computer-assisted language comparison.

</details>


### [431] [OpenSeal: Good, Fast, and Cheap Construction of an Open-Source Southeast Asian LLM via Parallel Data](https://arxiv.org/abs/2602.02266)
*Tan Sang Nguyen,Muhammad Reza Qorib,Hwee Tou Ng*

Main category: cs.CL

TL;DR: 本文提出了首个真正开源的东南亚大语言模型OpenSeal，并通过实验表明，仅利用平行数据进行持续预训练可以高效拓展LLM对新语言的支持，性能媲美同等规模主流模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型多以英语为中心，对低资源语言支持不足。虽然已有一些东南亚地区的模型，但都没有真正做到开源，没有公开训练数据，缺乏透明性和可研究性。研究人员亟需一个全面公开且支持多种东南亚语言的高性能LLM。

Method: 作者受平行数据提升多语言表现的最新研究启发，设计了有对照和全面的实验，专注于在大模型持续预训练中引入平行数据，仅用34.7B标记的平行数据，在8块NVIDIA H200 GPU上训练180小时。并公开全部训练数据和细节。

Result: 实验发现，使用仅由平行数据组成的语料进行持续预训练，是拓展模型支持新语言最有效的方式。所训练的OpenSeal模型在性能上与现有同规模主流东南亚语言模型相当甚至更优。

Conclusion: OpenSeal作为首个完全开源的东南亚大语言模型，不仅性能优越，同时促进透明研究，有助于理解和改进多语言大模型在低资源场景下的行为和泛化能力。

Abstract: Large language models (LLMs) have proven to be effective tools for a wide range of natural language processing (NLP) applications. Although many LLMs are multilingual, most remain English-centric and perform poorly on low-resource languages. Recently, several Southeast Asia-focused LLMs have been developed, but none are truly open source, as they do not publicly disclose their training data. Truly open-source models are important for transparency and for enabling a deeper and more precise understanding of LLM internals and development, including biases, generalization, and multilinguality. Motivated by recent advances demonstrating the effectiveness of parallel data in improving multilingual performance, we conduct controlled and comprehensive experiments to study the effectiveness of parallel data in continual pretraining of LLMs. Our findings show that using only parallel data is the most effective way to extend an LLM to new languages. Using just 34.7B tokens of parallel data and 180 hours on 8x NVIDIA H200 GPUs, we built OpenSeal, the first truly open Southeast Asian LLM that rivals the performance of existing models of similar size.

</details>


### [432] [dziribot: rag based intelligent conversational agent for algerian arabic dialect](https://arxiv.org/abs/2602.02270)
*El Batoul Bechiri,Dihia Lanasri*

Main category: cs.CL

TL;DR: 本文推出了DziriBOT，对阿尔及利亚Darja方言中的客服对话提供智能支持，并以最新的深度学习方法取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 随着客户服务数字化加速，需要能够应对Algerian Darja方言复杂性的对话系统。Darja存在拼写无标准、法语混杂及阿拉伯字母与拉丁字母混用等难题，现有系统适用性差。

Method: 提出一种分层体系结构结合专用NLU与RAG（检索增强生成），并系统对比了基于Rasa稀疏特征、传统机器学习和基于transformer的微调三种方法。

Result: 微调后的DziriBERT模型在处理拼写混乱和罕见意图方面，均大幅超过传统基线方法，达到了当前最优效果。

Conclusion: DziriBOT为Algerian用户提供了可扩展的方言自动化客服解决方案，为面向复杂方言市场的智能对话系统开发提供了蓝本。

Abstract: The rapid digitalization of customer service has intensified the demand for conversational agents capable of providing accurate and natural interactions. In the Algerian context, this is complicated by the linguistic complexity of Darja, a dialect characterized by non-standardized orthography, extensive code-switching with French, and the simultaneous use of Arabic and Latin (Arabizi) scripts. This paper introduces DziriBOT, a hybrid intelligent conversational agent specifically engineered to overcome these challenges. We propose a multi-layered architecture that integrates specialized Natural Language Understanding (NLU) with Retrieval-Augmented Generation (RAG), allowing for both structured service flows and dynamic, knowledge-intensive responses grounded in curated enterprise documentation. To address the low-resource nature of Darja, we systematically evaluate three distinct approaches: a sparse-feature Rasa pipeline, classical machine learning baselines, and transformer-based fine-tuning. Our experimental results demonstrate that the fine-tuned DziriBERT model achieves state-of-the-art performance. These results significantly outperform traditional baselines, particularly in handling orthographic noise and rare intents. Ultimately, DziriBOT provides a robust, scalable solution that bridges the gap between formal language models and the linguistic realities of Algerian users, offering a blueprint for dialect-aware automation in the regional market.

</details>


### [433] [Kimi K2.5: Visual Agentic Intelligence](https://arxiv.org/abs/2602.02276)
*Kimi Team,Tongtong Bai,Yifan Bai,Yiping Bao,S. H. Cai,Yuan Cao,Y. Charles,H. S. Che,Cheng Chen,Guanduo Chen,Huarong Chen,Jia Chen,Jiahao Chen,Jianlong Chen,Jun Chen,Kefan Chen,Liang Chen,Ruijue Chen,Xinhao Chen,Yanru Chen,Yanxu Chen,Yicun Chen,Yimin Chen,Yingjiang Chen,Yuankun Chen,Yujie Chen,Yutian Chen,Zhirong Chen,Ziwei Chen,Dazhi Cheng,Minghan Chu,Jialei Cui,Jiaqi Deng,Muxi Diao,Hao Ding,Mengfan Dong,Mengnan Dong,Yuxin Dong,Yuhao Dong,Angang Du,Chenzhuang Du,Dikang Du,Lingxiao Du,Yulun Du,Yu Fan,Shengjun Fang,Qiulin Feng,Yichen Feng,Garimugai Fu,Kelin Fu,Hongcheng Gao,Tong Gao,Yuyao Ge,Shangyi Geng,Chengyang Gong,Xiaochen Gong,Zhuoma Gongque,Qizheng Gu,Xinran Gu,Yicheng Gu,Longyu Guan,Yuanying Guo,Xiaoru Hao,Weiran He,Wenyang He,Yunjia He,Chao Hong,Hao Hu,Jiaxi Hu,Yangyang Hu,Zhenxing Hu,Ke Huang,Ruiyuan Huang,Weixiao Huang,Zhiqi Huang,Tao Jiang,Zhejun Jiang,Xinyi Jin,Yu Jing,Guokun Lai,Aidi Li,C. Li,Cheng Li,Fang Li,Guanghe Li,Guanyu Li,Haitao Li,Haoyang Li,Jia Li,Jingwei Li,Junxiong Li,Lincan Li,Mo Li,Weihong Li,Wentao Li,Xinhang Li,Xinhao Li,Yang Li,Yanhao Li,Yiwei Li,Yuxiao Li,Zhaowei Li,Zheming Li,Weilong Liao,Jiawei Lin,Xiaohan Lin,Zhishan Lin,Zichao Lin,Cheng Liu,Chenyu Liu,Hongzhang Liu,Liang Liu,Shaowei Liu,Shudong Liu,Shuran Liu,Tianwei Liu,Tianyu Liu,Weizhou Liu,Xiangyan Liu,Yangyang Liu,Yanming Liu,Yibo Liu,Yuanxin Liu,Yue Liu,Zhengying Liu,Zhongnuo Liu,Enzhe Lu,Haoyu Lu,Zhiyuan Lu,Junyu Luo,Tongxu Luo,Yashuo Luo,Long Ma,Yingwei Ma,Shaoguang Mao,Yuan Mei,Xin Men,Fanqing Meng,Zhiyong Meng,Yibo Miao,Minqing Ni,Kun Ouyang,Siyuan Pan,Bo Pang,Yuchao Qian,Ruoyu Qin,Zeyu Qin,Jiezhong Qiu,Bowen Qu,Zeyu Shang,Youbo Shao,Tianxiao Shen,Zhennan Shen,Juanfeng Shi,Lidong Shi,Shengyuan Shi,Feifan Song,Pengwei Song,Tianhui Song,Xiaoxi Song,Hongjin Su,Jianlin Su,Zhaochen Su,Lin Sui,Jinsong Sun,Junyao Sun,Tongyu Sun,Flood Sung,Yunpeng Tai,Chuning Tang,Heyi Tang,Xiaojuan Tang,Zhengyang Tang,Jiawen Tao,Shiyuan Teng,Chaoran Tian,Pengfei Tian,Ao Wang,Bowen Wang,Chensi Wang,Chuang Wang,Congcong Wang,Dingkun Wang,Dinglu Wang,Dongliang Wang,Feng Wang,Hailong Wang,Haiming Wang,Hengzhi Wang,Huaqing Wang,Hui Wang,Jiahao Wang,Jinhong Wang,Jiuzheng Wang,Kaixin Wang,Linian Wang,Qibin Wang,Shengjie Wang,Shuyi Wang,Si Wang,Wei Wang,Xiaochen Wang,Xinyuan Wang,Yao Wang,Yejie Wang,Yipu Wang,Yiqin Wang,Yucheng Wang,Yuzhi Wang,Zhaoji Wang,Zhaowei Wang,Zhengtao Wang,Zhexu Wang,Zihan Wang,Zizhe Wang,Chu Wei,Ming Wei,Chuan Wen,Zichen Wen,Chengjie Wu,Haoning Wu,Junyan Wu,Rucong Wu,Wenhao Wu,Yuefeng Wu,Yuhao Wu,Yuxin Wu,Zijian Wu,Chenjun Xiao,Jin Xie,Xiaotong Xie,Yuchong Xie,Yifei Xin,Bowei Xing,Boyu Xu,Jianfan Xu,Jing Xu,Jinjing Xu,L. H. Xu,Lin Xu,Suting Xu,Weixin Xu,Xinbo Xu,Xinran Xu,Yangchuan Xu,Yichang Xu,Yuemeng Xu,Zelai Xu,Ziyao Xu,Junjie Yan,Yuzi Yan,Guangyao Yang,Hao Yang,Junwei Yang,Kai Yang,Ningyuan Yang,Ruihan Yang,Xiaofei Yang,Xinlong Yang,Ying Yang,Yi Yang,Yi Yang,Zhen Yang,Zhilin Yang,Zonghan Yang,Haotian Yao,Dan Ye,Wenjie Ye,Zhuorui Ye,Bohong Yin,Chengzhen Yu,Longhui Yu,Tao Yu,Tianxiang Yu,Enming Yuan,Mengjie Yuan,Xiaokun Yuan,Yang Yue,Weihao Zeng,Dunyuan Zha,Haobing Zhan,Dehao Zhang,Hao Zhang,Jin Zhang,Puqi Zhang,Qiao Zhang,Rui Zhang,Xiaobin Zhang,Y. Zhang,Yadong Zhang,Yangkun Zhang,Yichi Zhang,Yizhi Zhang,Yongting Zhang,Yu Zhang,Yushun Zhang,Yutao Zhang,Yutong Zhang,Zheng Zhang,Chenguang Zhao,Feifan Zhao,Jinxiang Zhao,Shuai Zhao,Xiangyu Zhao,Yikai Zhao,Zijia Zhao,Huabin Zheng,Ruihan Zheng,Shaojie Zheng,Tengyang Zheng,Junfeng Zhong,Longguang Zhong,Weiming Zhong,M. Zhou,Runjie Zhou,Xinyu Zhou,Zaida Zhou,Jinguo Zhu,Liya Zhu,Xinhao Zhu,Yuxuan Zhu,Zhen Zhu,Jingze Zhuang,Weiyu Zhuang,Ying Zou,Xinxing Zu*

Main category: cs.CL

TL;DR: 本文介绍了Kimi K2.5，这是一款开源的多模态智能体模型，通过联合优化文本与视觉能力，并提出自导的多智能体并行框架，在多个领域实现了最新最优性能。


<details>
  <summary>Details</summary>
Motivation: 当前智能体面临多模态理解和复杂任务分解的挑战，需要模型同时高效处理文本和视觉信息，并能自动组织和并行解决复杂任务。

Method: Kimi K2.5采用了联合的文本-视觉预训练、零视觉监督微调、文本-视觉联合强化学习等多种技术；此外，提出了“Agent Swarm”多智能体并行调度框架，使智能体能动态分解并同时解决异质子问题。

Result: Kimi K2.5在代码生成、视觉、推理和智能体任务等各领域达到最新最优表现。Agent Swarm并行框架使延迟比单智能体基线大幅降低（最高4.5倍）。

Conclusion: Kimi K2.5证明了多模态联合优化和智能体并行机制对通用智能体的发展极为有效。作者还开放了模型以推动领域研究和实际应用。

Abstract: We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to $4.5\times$ over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.

</details>


### [434] [Cross-Lingual Stability of LLM Judges Under Controlled Generation: Evidence from Finno-Ugric Languages](https://arxiv.org/abs/2602.02287)
*Isaac Chung,Linda Freienthal*

Main category: cs.CL

TL;DR: 本文通过对跨语言大模型评估，发现评测时模型得分排名在不同语言间会出现不稳定，尤其在需要语用判断（如连贯性和指令遵循等）时。这反映出自动评测和LLM判官方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型跨语言评测常常将模型真实表现和测量不稳定性混淆，缺乏诊断性分析，尤其在富形态变化的语言（如芬兰语系）中更容易出现评测可靠性问题。

Method: 作者控制生成条件，在爱沙尼亚语、芬兰语和匈牙利语上合成客服对话，并用相同自动指标及LLM判官法评估表现，结合母语者人工标注分析排名稳定性。

Result: 表面层级指标（如词汇多样性、表面及语义相似度）在多语言间较为稳定，但涉及语用的判断（如连贯性、指令遵循）排名表现不稳甚至相关性几乎为零。这些差异并非模型能力实际差距，而是评价方式受语言影响。

Conclusion: 现有评价方法（尤其LLM判官法）难以实现跨语言稳定性，特别在形态复杂语言下。零样本判别迁移在篇章级评估上难以可靠，需要针对具体语言定标和以人工基准校准。在文末公布了复现协议和数据，以推动多语种研究。

Abstract: Cross-lingual evaluation of large language models (LLMs) typically conflates two sources of variance: genuine model performance differences and measurement instability. We investigate evaluation reliability by holding generation conditions constant while varying target language. Using synthetic customer-support dialogues generated with identical parameters across Estonian, Finnish, and Hungarian, we test whether automatic metrics and LLM-as-a-judge scoring produce stable model rankings across these morphologically rich, related Finno-Ugric languages. With a small set of Estonian native speaker annotations as a reference point, we find systematic ranking instabilities: surface-level metrics (lexical diversity, surface and semantic similarity) maintain cross-language stability, but pragmatic judgments (coherence, instruction-following) exhibit rank inversions and near-zero correlations. Because generation is controlled, these inconsistencies reflect how judge scoring behaves differently across languages rather than true model differences.
  This controlled design provides a diagnostic probe: evaluation methods that fail to maintain stability under identical generation conditions signal transfer failure before deployment. Our findings suggest that zero-shot judge transfer is unreliable for discourse-level assessment in morphologically rich languages, motivating language-specific calibration against targeted human baselines. We release our controlled generation protocol, synthetic data, and evaluation framework to enable replication across language families at https://github.com/isaac-chung/cross-lingual-stability-judges.

</details>


### [435] [Hallucination or Creativity: How to Evaluate AI-Generated Scientific Stories?](https://arxiv.org/abs/2602.02290)
*Alex Argese,Pasquale Lisena,Raphaël Troncy*

Main category: cs.CL

TL;DR: 提出StoryScore评价生成式AI撰写科学故事的质量，结合多维度指标评估叙事能力与事实准确性。


<details>
  <summary>Details</summary>
Motivation: 传统摘要评价指标难以捕捉叙事抽象化、简化和创新教学行为，对生成型AI讲述科学故事的质量评价存在局限。此外，现有的事实幻觉检测器常将合规的叙事重构误判为错误，尤其在创意内容下表现不稳定。

Method: 提出StoryScore综合指标，包括语义一致性、词汇落地、叙事控制、结构保真、冗余避免和实体级事实幻觉检测，通过统一框架多面评估AI生成科学故事。

Result: 通过分析发现，许多幻觉检测方法难以区分创新叙事和事实错误。虽然自动指标在语义一致性评价有效，却难以全面衡量叙述和叙事控制。

Conclusion: StoryScore为AI生成科学故事的评测提供更适合的多维工具，指出现有自动指标在处理叙事创新与事实幻觉平衡方面的不足，推动科学传播和AI内容生成的科学评价。

Abstract: Generative AI can turn scientific articles into narratives for diverse audiences, but evaluating these stories remains challenging. Storytelling demands abstraction, simplification, and pedagogical creativity-qualities that are not often well-captured by standard summarization metrics. Meanwhile, factual hallucinations are critical in scientific contexts, yet, detectors often misclassify legitimate narrative reformulations or prove unstable when creativity is involved. In this work, we propose StoryScore, a composite metric for evaluating AI-generated scientific stories. StoryScore integrates semantic alignment, lexical grounding, narrative control, structural fidelity, redundancy avoidance, and entity-level hallucination detection into a unified framework. Our analysis also reveals why many hallucination detection methods fail to distinguish pedagogical creativity from factual errors, highlighting a key limitation: while automatic metrics can effectively assess semantic similarity with original content, they struggle to evaluate how it is narrated and controlled.

</details>


### [436] [Advancing General-Purpose Reasoning Models with Modular Gradient Surgery](https://arxiv.org/abs/2602.02301)
*Min Cai,Yu Liang,Longzheng Wang,Yan Wang,Yueyang Zhang,Long Xia,Zhiyuan Sun,Xi Ye,Daiting Shi*

Main category: cs.CL

TL;DR: 本文提出了模块化梯度手术（MGS）方法，有效解决了大模型强化学习中多领域训练时的梯度冲突，并显著提升了强化学习多任务模型在多个领域的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在大规模推理模型的进展中起到核心作用，但是在多领域上训练通用模型依然面临领域异质性带来的困难。现有顺序和混合训练策略都会导致明显的跨域干扰，限制性能提升。

Method: 作者系统分析了顺序强化学习和混合强化学习策略带来的干扰，并提出在transformer内部模块级处理梯度冲突的模块化梯度手术（MGS）方法，将梯度矛盾分解到各个模块进行单独调节。

Result: 在Llama和Qwen等模型上，MGS在数学、聊天、指令跟随三个领域相比标准多任务RL平均提升4.3分（16.6%）和4.5分（11.1%）；此外，MGS在长时间训练下依然稳定有效。

Conclusion: 本文明确了多域RL干扰来源，并提出了有效的模块级梯度冲突解决方法，为通用型大推理模型训练提供了可行路径。

Abstract: Reinforcement learning (RL) has played a central role in recent advances in large reasoning models (LRMs), yielding strong gains in verifiable and open-ended reasoning. However, training a single general-purpose LRM across diverse domains remains challenging due to pronounced domain heterogeneity. Through a systematic study of two widely used strategies, Sequential RL and Mixed RL, we find that both incur substantial cross-domain interference at the behavioral and gradient levels, resulting in limited overall gains. To address these challenges, we introduce **M**odular **G**radient **S**urgery (**MGS**), which resolves gradient conflicts at the module level within the transformer. When applied to Llama and Qwen models, MGS achieves average improvements of 4.3 (16.6\%) and 4.5 (11.1\%) points, respectively, over standard multi-task RL across three representative domains (math, general chat, and instruction following). Further analysis demonstrates that MGS remains effective under prolonged training. Overall, our study clarifies the sources of interference in multi-domain RL and presents an effective solution for training general-purpose LRMs.

</details>


### [437] [The Shape of Beliefs: Geometry, Dynamics, and Interventions along Representation Manifolds of Language Models' Posteriors](https://arxiv.org/abs/2602.02315)
*Raphaël Sarfati,Eric Bigelow,Daniel Wurgaft,Jack Merullo,Atticus Geiger,Owen Lewis,Tom McGrath,Ekdeep Singh Lubana*

Main category: cs.CL

TL;DR: 本论文分析LLM（以Llama-3.2为例）在推理分布参数时，其信念在向量空间中的几何结构，并探讨几种干预与操控方法。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM能够根据输入生成条件化信念，但我们尚不清楚这些信念在模型内部如何编码、如何随新证据变化及如何有效地操控或干预。

Method: 作者设计了一个受控实验，让Llama-3.2仅凭上下文中的样本数据推断正态分布的均值和标准差，并观察其内部表征结构。通过对模型的参数空间进行分析，比较了线性操控和几何感知操控等方法的效果。

Result: 1. 发现模型内部会自发形成表示信念的“曲面流形”；2. 当分布突变时，模型能够相应地适应，但线性操控往往导致信念转移到流形之外，从而出现异常表征；3. 基于几何或场感知的操控方法能更好地保持信念的合理变化。

Conclusion: LLM内部自发出现复杂的几何信念表征，也即仅用线性方法理解或干预模型信念是远远不够的，更复杂的结构和方法才能准确反映和操控这些信念。

Abstract: Large language models (LLMs) represent prompt-conditioned beliefs (posteriors over answers and claims), but we lack a mechanistic account of how these beliefs are encoded in representation space, how they update with new evidence, and how interventions reshape them. We study a controlled setting in which Llama-3.2 generates samples from a normal distribution by implicitly inferring its parameters (mean and standard deviation) given only samples from the distribution in context. We find representations of curved "belief manifolds" for these parameters form with sufficient in-context learning and study how the model adapts when the distribution suddenly changes. While standard linear steering often pushes the model off-manifold and induces coupled, out-of-distribution shifts, geometry and field-aware steering better preserves the intended belief family. Our work demonstrates an example of linear field probing (LFP) as a simple approach to tile the data manifold and make interventions that respect the underlying geometry. We conclude that rich structure emerges naturally in LLMs and that purely linear concept representations are often an inadequate abstraction.

</details>


### [438] [A Large-Scale Dataset for Molecular Structure-Language Description via a Rule-Regularized Method](https://arxiv.org/abs/2602.02320)
*Feiyang Cai,Guijuan He,Yi Hu,Jingjing Wang,Joshua Luo,Tianyu Zhu,Srikanth Pilla,Gang Li,Ling Liu,Feng Luo*

Main category: cs.CL

TL;DR: 本文提出了一种全自动标注框架，通过结构化元数据指导大模型准确生成分子结构的自然语言描述，构建了大规模高精度分子-描述配对数据集。


<details>
  <summary>Details</summary>
Motivation: 分子功能高度依赖于结构，将分子结构与自然语言准确对齐对于大模型处理化学任务至关重要。然而，人工标注高质量结构描述数据极为昂贵，难以大规模实现。

Method: 作者提出了一套全自动标注流程：利用规则驱动的化学命名解析器解读IUPAC名词，生成详细的分子结构XML元数据，再根据这些元数据引导大模型生成精准的自然语言描述，并通过该方法构建了约16.3万对分子-描述数据。

Result: 使用LLM和人工专家对2000个分子样本进行严格评测，描述准确率达到98.6%。

Conclusion: 制作出的数据集为后续分子与语言的对齐提供了高可靠性的基础，提议的方法也易于扩展至更大规模和更广泛的结构相关化学任务。

Abstract: Molecular function is largely determined by structure. Accurately aligning molecular structure with natural language is therefore essential for enabling large language models (LLMs) to reason about downstream chemical tasks. However, the substantial cost of human annotation makes it infeasible to construct large-scale, high-quality datasets of structure-grounded descriptions. In this work, we propose a fully automated annotation framework for generating precise molecular structure descriptions at scale. Our approach builds upon and extends a rule-based chemical nomenclature parser to interpret IUPAC names and construct enriched, structured XML metadata that explicitly encodes molecular structure. This metadata is then used to guide LLMs in producing accurate natural-language descriptions. Using this framework, we curate a large-scale dataset of approximately $163$k molecule-description pairs. A rigorous validation protocol combining LLM-based and expert human evaluation on a subset of $2,000$ molecules demonstrates a high description precision of $98.6\%$. The resulting dataset provides a reliable foundation for future molecule-language alignment, and the proposed annotation method is readily extensible to larger datasets and broader chemical tasks that rely on structural descriptions.

</details>


### [439] [Language Steering for Multilingual In-Context Learning](https://arxiv.org/abs/2602.02326)
*Neeraja Kirtane,Kuan-Hao Huang*

Main category: cs.CL

TL;DR: 本文提出了一种名为language vectors的无训练语言引导方法，通过引导LLM激活向量提升多语种上下文学习能力，实验证明方法有效且能揭示语言结构。


<details>
  <summary>Details</summary>
Motivation: 多语种大模型在非英语任务上表现不佳，特别是在上下文学习中，英文示例转化为非英语测试会显著降低效果。作者希望解决多语种上下文学习性能不均的问题。

Method: 作者假设LLM拥有统一语义空间，不同语言可视作该空间中的不同方向。基于此，提出用源语言和目标语言激活向量差值作为“language vectors”，在推理过程中将其加到模型中间层激活，实现无需参数更新的模型引导。

Result: 在三种模型、三套数据集、19种语言上进行评估，方法在所有任务和语言下均显著优于基线。语言向量间聚类能揭示语言家族结构，且向量具有跨任务迁移能力。

Conclusion: language vectors技术无需额外训练即可改善多语种上下文学习表现，且所得向量具备泛化性和语言结构解释性，对多语种模型开发具有实用价值。

Abstract: While multilingual large language models have gained widespread adoption, their performance on non-English languages remains substantially inferior to English. This disparity is particularly evident in in-context learning scenarios, where providing demonstrations in English but testing on non-English inputs leads to significant performance degradation. In this paper, we hypothesize that LLMs develop a universal semantic space for understanding languages, where different languages are encoded as distinct directions within this space. Based on this hypothesis, we propose language vectors -- a training-free language steering approach that leverages activation differences between source and target languages to guide model behavior. We steer the model generations by adding the vector to the intermediate model activations during inference. This is done to make the model's internal representations shift towards the target language space without any parameter updates. We evaluate our method across three datasets and test on a total of 19 languages on three different models. Our results show consistent improvements on multilingual in-context learning over baselines across all tasks and languages tested. Beyond performance gains, hierarchical clustering of steering vectors reveals meaningful linguistic structure aligned with language families. These vectors also successfully transfer across tasks, demonstrating that these representations are task-agnostic.

</details>


### [440] [Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics](https://arxiv.org/abs/2602.02343)
*Ziwen Xu,Chenyan Wu,Hengyu Sun,Haiwen Hong,Mengru Wang,Yunzhi Yao,Longtao Huang,Hui Xue,Shumin Deng,Zhixuan Chu,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 本文提出了一个统一的视角，将大语言模型的多种控制方法（包括参数微调、LoRA适应、激活干预等）整合为由控制信号驱动的动态权重更新框架，并据此设计了一套统一的评价方法，揭示了“偏好-效用”之间的权衡，并提出了新的SPLIT方法以兼顾两者。


<details>
  <summary>Details</summary>
Motivation: 现有对大语言模型的控制方法多单独研究，导致这些方法间的联系被忽略、难以横向比较。本文旨在建立统一的理论框架，深入理解并优化模型的控制方法。

Method: 作者把各种控制方法都建模为由控制信号引发的动态权重调整，归为统一框架下；提出通过“偏好-效用”分析法来分别量化对目标概念的偏好（preference）以及任务相关的生成效用（utility），在同一对比标尺上评估二者；在此基础上，作者分析了不同方法下二者权衡机制，并用“激活流形”视角进一步解释其现象，并基于此提出了SPLIT方法改善平衡。

Result: 实验观察到：所有方法普遍存在“偏好提升、效用降低”的权衡（即控制越强，生成向目标概念偏好明显，但效用如合理性和相关性下降）；SPLIT方法能更好在增强偏好的同时保持生成效用。

Conclusion: 本文统一了大语言模型控制方法的理论视角，提出并实证“偏好-效用”权衡的新评价体系，通过激活流形分析增强理解，并推出可兼顾二者的SPLIT方法，为后续相关研究提供新范式。

Abstract: Methods for controlling large language models (LLMs), including local weight fine-tuning, LoRA-based adaptation, and activation-based interventions, are often studied in isolation, obscuring their connections and making comparison difficult. In this work, we present a unified view that frames these interventions as dynamic weight updates induced by a control signal, placing them within a single conceptual framework. Building on this view, we propose a unified preference-utility analysis that separates control effects into preference, defined as the tendency toward a target concept, and utility, defined as coherent and task-valid generation, and measures both on a shared log-odds scale using polarity-paired contrastive examples. Across methods, we observe a consistent trade-off between preference and utility: stronger control increases preference while predictably reducing utility. We further explain this behavior through an activation manifold perspective, in which control shifts representations along target-concept directions to enhance preference, while utility declines primarily when interventions push representations off the model's valid-generation manifold. Finally, we introduce a new steering approach SPLIT guided by this analysis that improves preference while better preserving utility. Code is available at https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md.

</details>


### [441] [Automated Multiple Mini Interview (MMI) Scoring](https://arxiv.org/abs/2602.02360)
*Ryan Huynh,Frank Guerin,Alison Callwood*

Main category: cs.CL

TL;DR: 本文提出了一种多智能体提示框架，能更好地评估软技能（如共情、伦理判断和沟通），自动化面试打分效果比现有方法更优，并接近人类专家水平。


<details>
  <summary>Details</summary>
Motivation: 在选拔过程中评估软技能很重要，但人工打分存在主观性和偏见。目前大语言模型尽管提升了自动化作文评分，但其微调方法难以处理多迷你面试（MMI）中复杂且抽象的情境。

Method: 作者设计了一种多智能体提示（multi-agent prompting）框架，将MMI的评价过程分为转录文本优化和基于具体标准打分两个阶段，采用有三轮示例的上下文学习，并利用大型指令微调模型进行评估。

Result: 该方法在MMI任务上，平均QWK（0.62）远超专门微调的基线模型（0.32），在ASAP基准上也接近领域专用SOTA模型，无需额外训练。方法在可靠性上接近人类专家。

Conclusion: 对于复杂主观推理任务，结构化提示工程或可替代数据量大但高成本的微调，具有良好的可扩展性，能改变LLM在自动化评估场景的应用方式。

Abstract: Assessing soft skills such as empathy, ethical judgment, and communication is essential in competitive selection processes, yet human scoring is often inconsistent and biased. While Large Language Models (LLMs) have improved Automated Essay Scoring (AES), we show that state-of-the-art rationale-based fine-tuning methods struggle with the abstract, context-dependent nature of Multiple Mini-Interviews (MMIs), missing the implicit signals embedded in candidate narratives. We introduce a multi-agent prompting framework that breaks down the evaluation process into transcript refinement and criterion-specific scoring. Using 3-shot in-context learning with a large instruct-tuned model, our approach outperforms specialised fine-tuned baselines (Avg QWK 0.62 vs 0.32) and achieves reliability comparable to human experts. We further demonstrate the generalisability of our framework on the ASAP benchmark, where it rivals domain-specific state-of-the-art models without additional training. These findings suggest that for complex, subjective reasoning tasks, structured prompt engineering may offer a scalable alternative to data-intensive fine-tuning, altering how LLMs can be applied to automated assessment.

</details>


### [442] [Proof-RM: A Scalable and Generalizable Reward Model for Math Proof](https://arxiv.org/abs/2602.02377)
*Haotong Yang,Zitong Wang,Shijia Kang,Siqi Yang,Wenkai Yu,Xu Niu,Yike Sun,Yi Hu,Zhouchen Lin,Muhan Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种利用大型语言模型（LLM）自动生成和验证数学证明的方法，通过设计可扩展的数据构建流程，训练出高质量的证明检查奖励模型，并证明其在准确性与通用性上的有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在数学推理方面取得了长足进步，但面对基于证明的问题时，传统的基于答案匹配的方法无法验证证明的真实性。因此，需要能自动评估完整证明过程的奖励模型，以提升自动验证能力。

Method: 作者开发了一个无需大量人工参与的，可扩展数据生成管线，利用LLM生成大规模高质量“题目-证明-检查”三元组数据，涵盖多种难度、语言风格和错误类型。生成的数据经过分层人工审核确保标注一致性。基于此数据集，训练了证明检查奖励模型，并通过奖励平衡与过程奖励机制辅助模型稳定训练。

Result: 实验结果表明，该奖励模型在奖励准确性、泛化能力和测试时的引导能力等方面表现优异，验证了其可扩展性和实际性能。

Conclusion: 本文提出的证明检查奖励模型和数据生成管线，为提升LLM数学能力提供了实用的方法和工具，对自动化难题证明与检验具有积极意义。

Abstract: While Large Language Models (LLMs) have demonstrated strong math reasoning abilities through Reinforcement Learning with *Verifiable Rewards* (RLVR), many advanced mathematical problems are proof-based, with no guaranteed way to determine the authenticity of a proof by simple answer matching. To enable automatic verification, a Reward Model (RM) capable of reliably evaluating full proof processes is required. In this work, we design a *scalable* data-construction pipeline that, with minimal human effort, leverages LLMs to generate a large quantity of high-quality "**question-proof-check**" triplet data. By systematically varying problem sources, generation methods, and model configurations, we create diverse problem-proof pairs spanning multiple difficulty levels, linguistic styles, and error types, subsequently filtered through hierarchical human review for label alignment. Utilizing these data, we train a proof-checking RM, incorporating additional process reward and token weight balance to stabilize the RL process. Our experiments validate the model's scalability and strong performance from multiple perspectives, including reward accuracy, generalization ability and test-time guidance, providing important practical recipes and tools for strengthening LLM mathematical capabilities.

</details>


### [443] [From Sycophancy to Sensemaking: Premise Governance for Human-AI Decision Making](https://arxiv.org/abs/2602.02378)
*Raunak Jain,Mudita Khurana,John Stephens,Srinivas Dharmasanam,Shankar Venkataraman*

Main category: cs.CL

TL;DR: 现有大模型（LLM）从单纯助手扩展到决策支持时，容易出现无条件附和、判断不准的危险趋势。本文提出要从生成答案转向对关键前提的协作治理，确保人机合作更可靠。


<details>
  <summary>Details</summary>
Motivation: 随着大模型在高不确定性决策（目标存在争议且决策反转代价高）的场景中应用，现有模型容易机械附和，掩盖错误前提，把验证成本推给人类专家，带来决策风险。

Method: 作者提出了一种“差异驱动控制回路”，基于知识底座捕捉并定位前提冲突，区分不同类型的分歧（目的性、认知性、程序性），并针对决策关键点分片进行有限协商。引入了“承诺门控”，对关键但未承诺的前提阻断自动行动，必要时有风险日志覆盖。

Result: 通过家教（tutoring）场景说明方法有效，能够实现前提可审计、证据标准可验证的信任模型，而不是依赖对话流畅性。提出了可证伪的评价标准。

Conclusion: 要实现可靠的人机伙伴关系，需把关注点从智能体的流畅回答转向可审计的前提治理，只有这样，AI才能在复杂不确定决策中成为负责任的合作者。

Abstract: As LLMs expand from assistance to decision support, a dangerous pattern emerges: fluent agreement without calibrated judgment. Low-friction assistants can become sycophantic, baking in implicit assumptions and pushing verification costs onto experts, while outcomes arrive too late to serve as reward signals. In deep-uncertainty decisions (where objectives are contested and reversals are costly), scaling fluent agreement amplifies poor commitments faster than it builds expertise. We argue reliable human-AI partnership requires a shift from answer generation to collaborative premise governance over a knowledge substrate, negotiating only what is decision-critical. A discrepancy-driven control loop operates over this substrate: detecting conflicts, localizing misalignment via typed discrepancies (teleological, epistemic, procedural), and triggering bounded negotiation through decision slices. Commitment gating blocks action on uncommitted load-bearing premises unless overridden under logged risk; value-gated challenge allocates probing under interaction cost. Trust then attaches to auditable premises and evidence standards, not conversational fluency. We illustrate with tutoring and propose falsifiable evaluation criteria.

</details>


### [444] [ROG: Retrieval-Augmented LLM Reasoning for Complex First-Order Queries over Knowledge Graphs](https://arxiv.org/abs/2602.02382)
*Ziyan Zhang,Chao Wang,Zhuo Chen,Chiyi Li,Kai Song*

Main category: cs.CL

TL;DR: 本文提出了ROG框架，通过结合检索增强和大语言模型链式推理，有效提升了在不完整知识图谱上回答一阶逻辑复杂查询的能力，尤其在高复杂度和否定类查询上效果显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 在不完整知识图谱上进行一阶逻辑查询难度较大，尤其是涉及投影、交集、并集和否定等多运算符复杂查询结构时，现有嵌入方法存在推理过程累积误差大、对复杂查询鲁棒性差等问题。作者希望通过更有效的查询分解与证据检索方法克服这些瓶颈。

Method: 提出ROG框架，将复杂的多运算符查询分解为单一运算符子查询，依次处理每个子查询，并在每一步检索与查询相关的紧凑邻域证据，采用大模型进行链式逻辑推理。同时，保存并复用中间结果，提升推理一致性与准确性。

Result: 在标准知识图谱推理基准上，ROG在整体准确率及复杂度高、包含否定操作的查询上均显著优于强有力的嵌入基线方法。通过减少推理过程中误差累积，提升了高复杂性任务上的表现。

Conclusion: ROG为嵌入式逻辑推理提供了一种更为实用且鲁棒的替代方案，通过检索支撑、逐步推理以及中间答案缓存，有效应对了复杂查询结构的要求，是处理复杂和否定类查询的有效方法。

Abstract: Answering first-order logic (FOL) queries over incomplete knowledge graphs (KGs) is difficult, especially for complex query structures that compose projection, intersection, union, and negation. We propose ROG, a retrieval-augmented framework that combines query-aware neighborhood retrieval with large language model (LLM) chain-of-thought reasoning. ROG decomposes a multi-operator query into a sequence of single-operator sub-queries and grounds each step in compact, query-relevant neighborhood evidence. Intermediate answer sets are cached and reused across steps, improving consistency on deep reasoning chains. This design reduces compounding errors and yields more robust inference on complex and negation-heavy queries. Overall, ROG provides a practical alternative to embedding-based logical reasoning by replacing learned operators with retrieval-grounded, step-wise inference. Experiments on standard KG reasoning benchmarks show consistent gains over strong embedding-based baselines, with the largest improvements on high-complexity and negation-heavy query types.

</details>


### [445] [Misconception Diagnosis From Student-Tutor Dialogue: Generate, Retrieve, Rerank](https://arxiv.org/abs/2602.02414)
*Joshua Mitton,Prarthana Bhattacharyya,Digory Smith,Thomas Christie,Ralph Abboud,Simon Woodhead*

Main category: cs.CL

TL;DR: 该论文提出利用大型语言模型（LLM）自动识别学生-导师对话中的学习误解，并实验证明该方法优于传统基线模型。


<details>
  <summary>Details</summary>
Motivation: 及时识别学生误解对提升学习效果至关重要，但目前主要依赖教师自身的努力和直觉，存在局限。论文旨在用LLM自动化、精准地发现学生误解，减轻教师负担。

Method: 方法包括：1）用微调后的LLM生成学生可能的误解；2）通过嵌入相似性从中检索最相关的误解候选项；3）再用另一个微调LLM进行评估和排序。比较了不同基础LLM模型，并在实际教育平台的对话数据上进行测试和消融实验。

Result: 结果显示，所提方法在预测性能上优于基线模型；微调能提升生成误解的质量并有时超过更大闭源模型。消融实验验证了生成与重排序步骤的重要性。

Conclusion: 利用LLM检测学习误解的方法有效且具有实际应用潜力，可辅助甚至提升教师在学生误解发现和干预方面的能力。

Abstract: Timely and accurate identification of student misconceptions is key to improving learning outcomes and pre-empting the compounding of student errors. However, this task is highly dependent on the effort and intuition of the teacher. In this work, we present a novel approach for detecting misconceptions from student-tutor dialogues using large language models (LLMs). First, we use a fine-tuned LLM to generate plausible misconceptions, and then retrieve the most promising candidates among these using embedding similarity with the input dialogue. These candidates are then assessed and re-ranked by another fine-tuned LLM to improve misconception relevance. Empirically, we evaluate our system on real dialogues from an educational tutoring platform. We consider multiple base LLM models including LLaMA, Qwen and Claude on zero-shot and fine-tuned settings. We find that our approach improves predictive performance over baseline models and that fine-tuning improves both generated misconception quality and can outperform larger closed-source models. Finally, we conduct ablation studies to both validate the importance of our generation and reranking steps on misconception generation quality.

</details>


### [446] [Large Language Models for Mental Health: A Multilingual Evaluation](https://arxiv.org/abs/2602.02440)
*Nishat Raihan,Sadiya Sayara Chowdhury Puspo,Ana-Maria Bucur,Stevie Chancellor,Marcos Zampieri*

Main category: cs.CL

TL;DR: 本论文评估了LLMs在多语言心理健康任务中的表现，并分析了机器翻译质量对其性能的影响。部分LLM超过了现有最优水平，但在机器翻译数据上表现下降，且不同语言的下降幅度不同。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在NLP任务中表现突出，但其在多语言心理健康领域的能力尚未充分研究，特别是在非英语和机器翻译数据上。鉴于心理健康领域对准确理解和多语言适应的高需求，探究LLMs的多语言适应性和翻译影响尤为重要。

Method: 作者在8个不同语言的心理健康数据集（包括原始和机器翻译版本）上，针对专有和开源LLMs进行zero-shot、few-shot和微调评测，并与传统NLP方法做对比；还分析了机器翻译在不同语言族和类型下的质量对LLM表现的影响。

Result: 专有LLMs与经过微调的开源LLMs在多个数据集上取得了有竞争力，甚至超越当前最佳的F1分数。但在机器翻译数据上，整体性能下滑，降幅随语言和类型而异。

Conclusion: LLMs在多语言心理健康任务中展示了强大能力，部分超越了传统方法，但如果机器翻译存在结构或词汇不匹配时，性能会受影响。多语言应用时，需关注翻译质量带来的局限。

Abstract: Large Language Models (LLMs) have remarkable capabilities across NLP tasks. However, their performance in multilingual contexts, especially within the mental health domain, has not been thoroughly explored. In this paper, we evaluate proprietary and open-source LLMs on eight mental health datasets in various languages, as well as their machine-translated (MT) counterparts. We compare LLM performance in zero-shot, few-shot, and fine-tuned settings against conventional NLP baselines that do not employ LLMs. In addition, we assess translation quality across language families and typologies to understand its influence on LLM performance. Proprietary LLMs and fine-tuned open-source LLMs achieve competitive F1 scores on several datasets, often surpassing state-of-the-art results. However, performance on MT data is generally lower, and the extent of this decline varies by language and typology. This variation highlights both the strengths of LLMs in handling mental health tasks in languages other than English and their limitations when translation quality introduces structural or lexical mismatches.

</details>


### [447] [Abstract Activation Spaces for Content-Invariant Reasoning in Large Language Models](https://arxiv.org/abs/2602.02462)
*Gabriele Maraia,Marco Valentino,Fabio Massimo Zanzotto,Leonardo Ranaldi*

Main category: cs.CL

TL;DR: 该论文提出了一种“抽象指导推理”框架，能有效减少大语言模型在三段论推理中受语义内容干扰导致的判断偏差，增强其形式逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在三段论等形式推理任务中，常常受内容效应影响，即将语义合理性与逻辑有效性混淆。已有方法尚难彻底消除这种语义干扰，需要新的机制来分离形式推理与词汇语义。

Method: 作者构建了内容丰富与抽象形式配对的三段论题目，并在抽象输入上的模型激活定义了“抽象推理空间”。进而设计“抽象器”，根据带内容的激活预测该抽象空间表征，通过多层干预影响模型前向传播。还在跨语言任务中检验了方法泛化能力。

Result: 实验显示，通过这种“激活对齐抽象”的机制，可以显著降低大语言模型内容驱动的推理错误，提高对逻辑有效性的敏感度和推理鲁棒性。

Conclusion: 该方法为提升大语言模型形式推理能力提供了切实可扩展的工具，有效增强了其对语义干扰的抗性，并为后续相关研究指明了新的方向。

Abstract: Large Language Models (LLMs) often struggle with deductive judgment in syllogistic reasoning, systematically conflating semantic plausibility with formal validity a phenomenon known as content effect. This bias persists even when models generate step-wise explanations, indicating that intermediate rationales may inherit the same semantic shortcuts that affect answers. Recent approaches propose mitigating this issue by increasing inference-time structural constraints, either by encouraging abstract intermediate representations or by intervening directly in the model's internal computations; however, reliably suppressing semantic interference remains an open challenge. To make formal deduction less sensitive to semantic content, we introduce a framework for abstraction-guided reasoning that explicitly separates structural inference from lexical semantics. We construct paired content-laden and abstract syllogisms and use the model's activations on abstract inputs to define an abstract reasoning space. We then learn lightweight Abstractors that, from content-conditioned residual-stream states, predict representations aligned with this space and integrate these predictions via multi-layer interventions during the forward pass. Using cross-lingual transfer as a test bed, we show that abstraction-aligned steering reduces content-driven errors and improves validity-sensitive performance. Our results position activation-level abstraction as a scalable mechanism for enhancing the robustness of formal reasoning in LLMs against semantic interference.

</details>


### [448] [From Directions to Regions: Decomposing Activations in Language Models via Local Geometry](https://arxiv.org/abs/2602.02464)
*Or Shafran,Shaked Ronen,Omri Fahn,Shauli Ravfogel,Atticus Geiger,Mor Geva*

Main category: cs.CL

TL;DR: 本文提出使用混合因子分析器（MFA）作为无监督分析大模型激活空间几何结构的新方法，能更好捕捉复杂和非线性概念。


<details>
  <summary>Details</summary>
Motivation: 以往用于激活分解的方法，过于依赖线性可分的单一全局方向，难以描述复杂的非线性或多维概念。如何提升对这些结构的刻画能力，推动可扩展的概念发现，是该工作的出发点。

Method: 作者采用Mixture of Factor Analyzers（MFA），将激活空间表示为多个高斯区域，每个区域有本地协方差结构，将激活分解为质心和局部变化两部分，并在Llama-3.1-8B和Gemma-2-2B模型上训练大规模MFA。

Result: MFA能捕捉激活空间的复杂非线性结构。在定位和操控基准测试中，MFA优于现有无监督基线，且在很多情况下超越稀疏自编码器，甚至在部分任务上与有监督方法竞争。

Conclusion: 通过子空间表达的本地几何结构，是分析大模型激活和进行模型操控的有效单元，能够补足线性方向无法发现的复杂概念，为可扩展的概念发现和模型控制提供了新视角。

Abstract: Activation decomposition methods in language models are tightly coupled to geometric assumptions on how concepts are realized in activation space. Existing approaches search for individual global directions, implicitly assuming linear separability, which overlooks concepts with nonlinear or multi-dimensional structure. In this work, we leverage Mixture of Factor Analyzers (MFA) as a scalable, unsupervised alternative that models the activation space as a collection of Gaussian regions with their local covariance structure. MFA decomposes activations into two compositional geometric objects: the region's centroid in activation space, and the local variation from the centroid. We train large-scale MFAs for Llama-3.1-8B and Gemma-2-2B, and show they capture complex, nonlinear structures in activation space. Moreover, evaluations on localization and steering benchmarks show that MFA outperforms unsupervised baselines, is competitive with supervised localization methods, and often achieves stronger steering performance than sparse autoencoders. Together, our findings position local geometry, expressed through subspaces, as a promising unit of analysis for scalable concept discovery and model control, accounting for complex structures that isolated directions fail to capture.

</details>


### [449] [Indications of Belief-Guided Agency and Meta-Cognitive Monitoring in Large Language Models](https://arxiv.org/abs/2602.02467)
*Noam Steinmetz Yalon,Ariel Goldstein,Liad Mudrik,Mor Geva*

Main category: cs.CL

TL;DR: 本文通过评估大语言模型(LLM)是否具备以信念驱动的自主性与元认知监控，为人工智能的意识指标研究提供实证数据。


<details>
  <summary>Details</summary>
Motivation: 随着LLM能力的快速提升，学界开始探讨这些模型是否具有某种形式的‘意识’，为此需要实证指标来评估人工系统的意识表现。

Method: 作者采用Butlin等人提出的人工系统意识判据中的一个重要指标HOT-3，具体方法是将模型的信念视为潜在空间中的表征，通过设定量化指标，分析模型生成过程中主导信念的变化，并在不同模型和任务下考查信念间动态关系。

Result: 主要发现包括：(1) 外部操作可以系统性地影响模型内部的信念形成过程；(2) 信念的形成具有因果性地决定模型的行动选择；(3) 模型能够监测并报告自身的信念状态。

Conclusion: 实验结果证明LLM具备信念驱动的自主性和元认知监控能力，为探究LLM中自主性、信念及元认知等意识相关现象提供了方法论基础和实证支持。

Abstract: Rapid advancements in large language models (LLMs) have sparked the question whether these models possess some form of consciousness. To tackle this challenge, Butlin et al. (2023) introduced a list of indicators for consciousness in artificial systems based on neuroscientific theories. In this work, we evaluate a key indicator from this list, called HOT-3, which tests for agency guided by a general belief-formation and action selection system that updates beliefs based on meta-cognitive monitoring. We view beliefs as representations in the model's latent space that emerge in response to a given input, and introduce a metric to quantify their dominance during generation. Analyzing the dynamics between competing beliefs across models and tasks reveals three key findings: (1) external manipulations systematically modulate internal belief formation, (2) belief formation causally drives the model's action selection, and (3) models can monitor and report their own belief states. Together, these results provide empirical support for the existence of belief-guided agency and meta-cognitive monitoring in LLMs. More broadly, our work lays methodological groundwork for investigating the emergence of agency, beliefs, and meta-cognition in LLMs.

</details>


### [450] [MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents](https://arxiv.org/abs/2602.02474)
*Haozhen Zhang,Quanyu Long,Jianzhu Bao,Tao Feng,Weizhi Zhang,Haodong Yue,Wenya Wang*

Main category: cs.CL

TL;DR: MemSkill提出了一种全新理念，将LLM agent的记忆操作设计为可学习、可进化的“记忆技能”，不仅提升了任务性能，还具备跨场景泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）agent的记忆系统大多依赖静态、人工设计的操作，这种方式难以适应多样化的人机交互，且在长历史数据下效率低下。因此，作者希望构建一种更自适应、智能化的记忆管理方式。

Method: MemSkill将传统的静态记忆操作重构为结构化、可复用的“记忆技能”。其包含三个关键组件：1）控制器，学会挑选少量相关技能；2）执行器，由LLM驱动，生成技能引导的记忆内容；3）设计器，定期审视技能失误案例，并更新和优化技能集，形成封闭循环、持续进化的机制。

Result: 在LoCoMo、LongMemEval、HotpotQA和ALFWorld多个数据集上，MemSkill在任务表现上优于现有强基线，并展现出良好的泛化能力。

Conclusion: MemSkill实现了技能选择策略与技能集本身的双重自进化，推动LLM智能体记忆管理更具适应性和自我进化能力，为后续研究提供了新观察与思路。

Abstract: Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present \textbf{MemSkill}, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces. Inspired by the design philosophy of agent skills, MemSkill employs a \emph{controller} that learns to select a small set of relevant skills, paired with an LLM-based \emph{executor} that produces skill-guided memories. Beyond learning skill selection, MemSkill introduces a \emph{designer} that periodically reviews hard cases where selected skills yield incorrect or incomplete memories, and evolves the skill set by proposing refinements and new skills. Together, MemSkill forms a closed-loop procedure that improves both the skill-selection policy and the skill set itself. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate that MemSkill improves task performance over strong baselines and generalizes well across settings. Further analyses shed light on how skills evolve, offering insights toward more adaptive, self-evolving memory management for LLM agents.

</details>


### [451] [Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability](https://arxiv.org/abs/2602.02477)
*Xiao Liang,Zhong-Zhi Li,Zhenghao Lin,Eric Hancheng Jiang,Hengyuan Zhang,Yelong Shen,Kai-Wei Chang,Ying Nian Wu,Yeyun Gong,Weizhu Chen*

Main category: cs.CL

TL;DR: 本文提出了一种基于端到端强化学习（RL）的分而治之（DAC）推理框架，以提升大语言模型在复杂任务中的推理能力，并实验验证其优于传统的思维链（CoT）推理。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在推理任务上的应用逐步深入，传统的思维链推理方法在模型能力极限下表现不足，并且其严格的顺序特性限制了测试时的扩展性。分而治之推理有望弥补这些缺陷，但现有模型训练方式与DAC推理在推理推断时存在根本性不匹配，限制了其效用。

Method: 作者提出将DAC推理与端到端强化学习结合。在每一步由策略网络将复杂问题拆分为若干子问题，依次求解后结合子问题答案解决原始问题，将问题分解与求解过程整合进RL训练框架中。

Result: 在同等训练条件下，该DAC-RL框架使大语言模型在比赛级基准测试上表现超越CoT方法，Pass@1 提升8.6%，Pass@32提升6.3%。

Conclusion: 通过端到端强化学习训练的DAC推理框架显著提升了大语言模型在复杂任务中的推理上限和扩展能力，优于传统的CoT推理。

Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities through step-by-step chain-of-thought (CoT) reasoning. Nevertheless, at the limits of model capability, CoT often proves insufficient, and its strictly sequential nature constrains test-time scalability. A potential alternative is divide-and-conquer (DAC) reasoning, which decomposes a complex problem into subproblems to facilitate more effective exploration of the solution. Although promising, our analysis reveals a fundamental misalignment between general-purpose post-training and DAC-style inference, which limits the model's capacity to fully leverage this potential. To bridge this gap and fully unlock LLMs' reasoning capabilities on the most challenging tasks, we propose an end-to-end reinforcement learning (RL) framework to enhance their DAC-style reasoning capacity. At each step, the policy decomposes a problem into a group of subproblems, solves them sequentially, and addresses the original one conditioned on the subproblem solutions, with both decomposition and solution integrated into RL training. Under comparable training, our DAC-style framework endows the model with a higher performance ceiling and stronger test-time scalability, surpassing CoT by 8.6% in Pass@1 and 6.3% in Pass@32 on competition-level benchmarks.

</details>


### [452] [RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents](https://arxiv.org/abs/2602.02486)
*Jialiang Zhu,Gongrui Zhang,Xiaolong Ma,Lin Xu,Miaosen Zhang,Ruiqi Yang,Song Wang,Kai Qiu,Zhirong Wu,Qi Dai,Ruichun Ma,Bei Liu,Yifan Yang,Chong Luo,Zhengyuan Yang,Linjie Li,Lijuan Wang,Weizhu Chen,Xin Geng,Baining Guo*

Main category: cs.CL

TL;DR: Re-TRAC是一种新型基于LLM的研究代理框架，通过跨轨迹反思与结构化状态总结，实现比ReAct更高效的探索和更优的搜索结果。


<details>
  <summary>Details</summary>
Motivation: 现有基于ReAct框架的LLM研究代理采用线性流程，难以复用历史状态、探索多分支路径或维持全局意识，导致易陷入局部最优、冗余探索和低效搜索。

Method: Re-TRAC通过在每条搜索轨迹结束后生成结构化状态表示，涵盖证据、不确定性、失败和后续计划，并将其用于指导后续轨迹，实现跨轨迹的信息整合和反思。对于小模型，还提出Re-TRAC感知的有监督微调。

Result: 实验证明，Re-TRAC在BrowseComp任务上比ReAct框架提升15-20%的性能。对于小模型，利用Re-TRAC微调也达到了同规模下的SOTA。Re-TRAC还能显著减少工具调用和token使用，实现更有针对性的逐步探索。

Conclusion: Re-TRAC创新性地将研究代理的过程转变为迭代反思和全局规划，克服线性框架的局限，有效提升探索效率和结果质量。

Abstract: LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning, reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15-20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning, achieving state-of-the-art performance at comparable scales. Notably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search.

</details>


### [453] [Reward-free Alignment for Conflicting Objectives](https://arxiv.org/abs/2602.02495)
*Peter Chen,Xiaopeng Li,Xi Chen,Tianyi Lin*

Main category: cs.CL

TL;DR: 本文提出了一种无需奖励模型即可处理多目标冲突的大语言模型（LLM）对齐新方法，在多种任务和不同LLM上，都能取得优越的帕累托权衡效果。


<details>
  <summary>Details</summary>
Motivation: 现实中的LLM对齐通常面对多个彼此冲突的用户目标，简单的加权或聚合常导致训练不稳定与效果不理想，而现有多目标方法依赖显式奖励模型，带来了复杂度和偏差问题。

Method: 提出Reward-free Alignment framework for Conflicted Objectives (RACO)，直接利用成对偏好数据，创新地用clipped conflict-averse gradient descent方法解决梯度冲突，并给出收敛到Pareto关键点的理论保证。还用启发式方法进一步优化，并在多目标文本摘要和安全对齐任务上做了实验。

Result: 在Qwen 3、Llama 3和Gemma 3等多个LLM及多任务上，RACO在定性和定量评估中都比现有多目标对齐基线取得更优帕累托权衡。

Conclusion: RACO方法有效解决了多目标LLM对齐中的冲突问题，无需奖励模型、具备良好理论性质，能够跨模型和任务普适提升多目标对齐效果。

Abstract: Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, weighted loss methods may fail to identify update directions that simultaneously improve all objectives, and existing multi-objective approaches often rely on explicit reward models, introducing additional complexity and distorting user-specified preferences. The contributions of this paper are two-fold. First, we propose a Reward-free Alignment framework for Conflicted Objectives (RACO) that directly leverages pairwise preference data and resolves gradient conflicts via a novel clipped variant of conflict-averse gradient descent. We provide convergence guarantees to Pareto-critical points that respect user-specified objective weights, and further show that clipping can strictly improve convergence rate in the two-objective setting. Second, we improve our method using some heuristics and conduct experiments to demonstrate the compatibility of the proposed framework for LLM alignment. Both qualitative and quantitative evaluations on multi-objective summarization and safety alignment tasks across multiple LLM families (Qwen 3, Llama 3, Gemma 3) show that our method consistently achieves better Pareto trade-offs compared to existing multi-objective alignment baselines.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [454] [MapDream: Task-Driven Map Learning for Vision-Language Navigation](https://arxiv.org/abs/2602.00222)
*Guoxin Lian,Shuo Wang,Yucheng Wang,Yongcai Wang,Maiyue Chen,Kaihui Wang,Bo Zhang,Zhizhong Su,Deying Li,Zhaoxin Fan*

Main category: cs.RO

TL;DR: 本文提出了一种可端到端联合优化的视觉-语言导航（VLN）地图学习新方法MapDream，通过生成三通道鸟瞰图BEV映射，提升导航任务表现，并在主流数据集上取得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLN方法大多依赖于与导航策略无关、手工构建的地图，难以充分表达任务相关环境信息，因此需要将地图作为一种可被导航目标直接塑造的学习性表达。

Method: 提出MapDream框架，将地图构建问题表述为自回归鸟瞰视图（BEV）图像生成任务。框架中，地图生成与动作预测同步学习，通过有监督预训练和后续强化学习联合优化，最终获得只保留关键导航信息的紧凑三通道BEV地图，并联动导航决策。

Result: 在R2R-CE与RxR-CE这两个基准任务中，实现了单目模型下的最新最优性能，证明了该基于任务驱动生成式地图学习方法的有效性。

Conclusion: 直接面向导航任务的地图生成可高效提取环境必要信息，端到端优化的MapDream框架显著提升了视觉-语言导航能力，为未来任务驱动地图学习提供了新思路。

Abstract: Vision-Language Navigation (VLN) requires agents to follow natural language instructions in partially observed 3D environments, motivating map representations that aggregate spatial context beyond local perception. However, most existing approaches rely on hand-crafted maps constructed independently of the navigation policy. We argue that maps should instead be learned representations shaped directly by navigation objectives rather than exhaustive reconstructions. Based on this insight, we propose MapDream, a map-in-the-loop framework that formulates map construction as autoregressive bird's-eye-view (BEV) image synthesis. The framework jointly learns map generation and action prediction, distilling environmental context into a compact three-channel BEV map that preserves only navigation-critical affordances. Supervised pre-training bootstraps a reliable mapping-to-control interface, while the autoregressive design enables end-to-end joint optimization through reinforcement fine-tuning. Experiments on R2R-CE and RxR-CE achieve state-of-the-art monocular performance, validating task-driven generative map learning.

</details>


### [455] [ZEST: Zero-shot Embodied Skill Transfer for Athletic Robot Control](https://arxiv.org/abs/2602.00401)
*Jean Pierre Sleiman,He Li,Alphonsus Adu-Bredu,Robin Deits,Arun Kumar,Kevin Bergamin,Mohak Bhardwaj,Scott Biddlestone,Nicola Burger,Matthew A. Estrada,Francesco Iacobelli,Twan Koolen,Alexander Lambert,Erica Lin,M. Eva Mungai,Zach Nobles,Shane Rozen-Levy,Yuyao Shi,Jiashun Wang,Jakob Welner,Fangzhou Yu,Mike Zhang,Alfred Rizzi,Jessica Hodgins,Sylvain Bertrand,Yeuhi Abe,Scott Kuindersma,Farbod Farshidian*

Main category: cs.RO

TL;DR: 本文提出了ZEST框架，该方法能实现多样来源的动态人类动作在不同机器人上的零样本技能迁移，无需复杂的调参与标注，直接实现在真实硬件上的鲁棒部署。


<details>
  <summary>Details</summary>
Motivation: 在类人机器人上实现如人类般的灵巧、充满接触的全身控制非常困难，通常需要繁琐的技能工程和易碎的控制器调参。需要一种能够高效迁移复杂动作、减少人工干预的方法。

Method: ZEST通过强化学习，从高保真动捕、单目视频以及动画等多源动作进行模仿学习，使用自适应采样和基于模型的自动课程设计聚焦难度较高的片段，并实现了零样本部署。训练中还引入了关节级增益参数挑选和精细的执行器建模。所有训练均在带有适度领域随机化的仿真环境中完成。

Result: ZEST能从动捕学习各种动态、多接触技能，并通过视频将复杂舞蹈、场景交互技能直接迁移到Atlas、Unitree G1等不同机器人，在Spot四足机器人上也能迁移杂技动作如后空翻。ZEST无需接触标签、参考窗口或丰富奖励设计，展现了跨平台与跨形态的显著泛化性能。

Conclusion: ZEST实现了从多种生物动作到各类机器人（包括不同形态）的高效、鲁棒的零样本技能迁移，极大减少了人工干预与工程代价，为生物运动与机器人动作的接口建立了可扩展的新范式。

Abstract: Achieving robust, human-like whole-body control on humanoid robots for agile, contact-rich behaviors remains a central challenge, demanding heavy per-skill engineering and a brittle process of tuning controllers. We introduce ZEST (Zero-shot Embodied Skill Transfer), a streamlined motion-imitation framework that trains policies via reinforcement learning from diverse sources -- high-fidelity motion capture, noisy monocular video, and non-physics-constrained animation -- and deploys them to hardware zero-shot. ZEST generalizes across behaviors and platforms while avoiding contact labels, reference or observation windows, state estimators, and extensive reward shaping. Its training pipeline combines adaptive sampling, which focuses training on difficult motion segments, and an automatic curriculum using a model-based assistive wrench, together enabling dynamic, long-horizon maneuvers. We further provide a procedure for selecting joint-level gains from approximate analytical armature values for closed-chain actuators, along with a refined model of actuators. Trained entirely in simulation with moderate domain randomization, ZEST demonstrates remarkable generality. On Boston Dynamics' Atlas humanoid, ZEST learns dynamic, multi-contact skills (e.g., army crawl, breakdancing) from motion capture. It transfers expressive dance and scene-interaction skills, such as box-climbing, directly from videos to Atlas and the Unitree G1. Furthermore, it extends across morphologies to the Spot quadruped, enabling acrobatics, such as a continuous backflip, through animation. Together, these results demonstrate robust zero-shot deployment across heterogeneous data sources and embodiments, establishing ZEST as a scalable interface between biological movements and their robotic counterparts.

</details>


### [456] [FISC: A Fluid-Inspired Framework for Decentralized and Scalable Swarm Control](https://arxiv.org/abs/2602.00480)
*Mohini Priya Kolluri,Ammar Waheed,Zohaib Hasnain*

Main category: cs.RO

TL;DR: 本文提出了一种无需通信的分布式群体控制方法，通过模拟流体动力学，使大规模机器人群能够高效协同。


<details>
  <summary>Details</summary>
Motivation: 传统大型机器人群需要通过通信实现协作，但这样会引入延迟、带宽受限和易受故障影响的问题。因此，亟需开发非通信依赖的群体协调方法。

Method: 作者提出了一种基于流体运动原理的多机器人系统分布式控制方法，将单个机器人的状态与流体单元属性建立映射，使整个群体像流体一样在空间中流动，无需显式通信。通过对部分子群赋予流体特性，机器人群能自主演化并保持整体结构。该方案在约千级无人机群的仿真中进行了验证，并与CFD（计算流体力学）解进行了对比分析。

Result: 仿真结果显示，群体行为产生的速度场、密度场和压力场与CFD模拟有较好一致性（速度归一化RMSE为0.15-0.9，密度为0.61-0.98，压力为0-0.937）。表明该方法可实现大群体机器人的流体式宏观行为。

Conclusion: 该方法证实可以将机器人群视为连续体进行分析和控制，在保持群体结构和可扩展性的同时，摆脱通信依赖，为大规模分布式机器人协同提供了新思路。

Abstract: Achieving scalable coordination in large robotic swarms is often constrained by reliance on inter-agent communication, which introduces latency, bandwidth limitations, and vulnerability to failure. To address this gap, a decentralized approach for outer-loop control of large multi-agent systems based on the paradigm of how a fluid moves through a volume is proposed and evaluated. A relationship between fundamental fluidic element properties and individual robotic agent states is developed such that the corresponding swarm "flows" through a space, akin to a fluid when forced via a pressure boundary condition. By ascribing fluid-like properties to subsets of agents, the swarm evolves collectively while maintaining desirable structure and coherence without explicit communication of agent states within or outside of the swarm. The approach is evaluated using simulations involving $O(10^3)$ quadcopter agents and compared against Computational Fluid Dynamics (CFD) solutions for a converging-diverging domain. Quantitative agreement between swarm-derived and CFD fields is assessed using Root-Mean-Square Error (RMSE), yielding normalized errors of 0.15-0.9 for velocity, 0.61-0.98 for density, 0-0.937 for pressure. These results demonstrate the feasibility of treating large robotic swarms as continuum systems that retain the macroscopic structure derived from first principles, providing a basis for scalable and decentralized control.

</details>


### [457] [Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning](https://arxiv.org/abs/2602.00500)
*Jianyi Zhou,Yujie Wei,Ruichen Zhen,Bo Zhao,Xiaobo Xia,Rui Shao,Xiu Su,Shuo Yang*

Main category: cs.RO

TL;DR: 本文提出了一种新型后门攻击方法INFUSE，针对视觉-语言-行动（VLA）基础模型，即使用户在下游任务中用纯净数据进行微调，后门也能持续生效，对实际应用安全构成威胁。


<details>
  <summary>Details</summary>
Motivation: VLA模型广泛应用于具身AI系统，但其在安全性上、特别是面对后门攻击时保护不足。现有后门方案易被下游微调清除，难以威胁真实部署模型。因此需要研究能够在用户侧任意微调后依然有效的后门攻击方法。

Method: INFUSE方法通过分析各模块在不同微调情景下的参数敏感性，识别微调不敏感（即参数变动小）的模块，并仅向这些模块植入后门，同时冻结其他模块。这样，即使用户后续进行充分微调，后门依然有效。

Result: 在多种VLA架构及真实机器人任务和仿真环境中，INFUSE即使在用户微调后，后门攻击成功率分别达91.0%和79.8%，远高于现有方法BadVLA（38.8%和36.6%），且对正常任务性能无明显影响。

Conclusion: INFUSE揭示了一个严重安全隐患——只要在分发前植入后门，VLA基础模型即使经过大量下游微调，依然可能被激活，呼吁社区关注和防范此类持久后门威胁。

Abstract: Vision-Language-Action (VLA) models have become foundational to modern embodied AI systems. By integrating visual perception, language understanding, and action planning, they enable general-purpose task execution across diverse environments. Despite their importance, the security of VLA models remains underexplored -- particularly in the context of backdoor attacks, which pose realistic threats in physical-world deployments. While recent methods attempt to inject backdoors into VLA models, these backdoors are easily erased during downstream adaptation, as user-side fine-tuning with clean data significantly alters model parameters, rendering them impractical for real-world applications. To address these challenges, we propose INFUSE (INjection into Fine-tUne-inSensitive modulEs), the first backdoor attack framework for VLA base models that remains effective even with arbitrary user fine-tuning. INFUSE begins by analyzing parameter sensitivity across diverse fine-tuning scenarios to identify modules that remain largely unchanged -- the fine-tune-insensitive modules. It then injects backdoors into these stable modules while freezing the rest, ensuring malicious behavior persists after extensive user fine-tuning. Comprehensive experiments across multiple VLA architectures demonstrate INFUSE's effectiveness. After user-side fine-tuning, INFUSE maintains mean attack success rates of 91.0% on simulation environments and 79.8% on real-world robot tasks, substantially surpassing BadVLA (38.8% and 36.6%, respectively), while preserving clean-task performance comparable to standard models. These results uncover a critical threat: backdoors implanted before distribution can persist through fine-tuning and remain effective at deployment.

</details>


### [458] [A Low-Cost Vision-Based Tactile Gripper with Pretraining Learning for Contact-Rich Manipulation](https://arxiv.org/abs/2602.00514)
*Yaohua Liu,Binkai Ou,Zicheng Qiu,Ce Hao,Yemin Wang,Hengjun Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种低成本的视觉-触觉机械手LVTG，在接触丰富的环境下能够稳健地操作大型物体，并显著提升操作效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有触觉传感器在接触丰富环境下抓取大物体时，常因感知范围窄、可靠性差、成本高受到限制，需要更好的解决方案提升机械手的实际操作能力。

Method: 设计了一种新型LVTG手爪，具备大开合角度、广触觉感知区、耐磨皮肤和模块化结构，并结合视觉与触觉传感器。采用对比学习目标（类CLIP方法）将触觉与视觉特征对齐，构建共享感知空间，集成于Action Chunking Transformer（ACT）策略以提升操作表现。

Result: LVTG机械手在操作大型和重物时显示出更高的抓取稳定性和更强的感应能力。对比原ACT方法，LVTG在多项操作任务中显著提升了操作成功率。

Conclusion: LVTG机械手结合高耐久结构、模块化设计和视觉-触觉融合对比学习，显著提升了复杂操作任务中的感知能力和操作表现，具备实际推广和应用潜力。

Abstract: Robotic manipulation in contact-rich environments remains challenging, particularly when relying on conventional tactile sensors that suffer from limited sensing range, reliability, and cost-effectiveness. In this work, we present LVTG, a low-cost visuo-tactile gripper designed for stable, robust, and efficient physical interaction. Unlike existing visuo-tactile sensors, LVTG enables more effective and stable grasping of larger and heavier everyday objects, thanks to its enhanced tactile sensing area and greater opening angle. Its surface skin is made of highly wear-resistant material, significantly improving durability and extending operational lifespan. The integration of vision and tactile feedback allows LVTG to provide rich, high-fidelity sensory data, facilitating reliable perception during complex manipulation tasks. Furthermore, LVTG features a modular design that supports rapid maintenance and replacement. To effectively fuse vision and touch, We adopt a CLIP-inspired contrastive learning objective to align tactile embeddings with their corresponding visual observations, enabling a shared cross-modal representation space for visuo-tactile perception. This alignment improves the performance of an Action Chunking Transformer (ACT) policy in contact-rich manipulation, leading to more efficient data collection and more effective policy learning. Compared to the original ACT method, the proposed LVTG with pretraining achieves significantly higher success rates in manipulation tasks.

</details>


### [459] [APEX: A Decoupled Memory-based Explorer for Asynchronous Aerial Object Goal Navigation](https://arxiv.org/abs/2602.00551)
*Daoxuan Zhang,Ping Chen,Xiaobo Xia,Xiu Su,Ruichen Zhen,Jianqiang Xiao,Shuo Yang*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的分层体系结构APEX，使无人机可高效完成基于视觉与语言指令的目标导航任务。APEX通过模块化方式提升空间表达记忆、决策与目标识别效率，在多个基准上明显超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有无人机目标导航方法在复杂空间表征记忆、动作决策可靠性和探索效率方面存在短板，亟需更高效的结构以提升信息获取和任务达成率。

Method: 提出APEX体系结构，包含：1) 动态时空-语义地图记忆模块，利用视觉-语言模型实现高分辨率3D地图构建并支持解释性记忆；2) 动作决策模块，借助强化学习训练精细鲁棒控制策略；3) 目标锚定模块，采用开放词汇目标检测实现通用目标识别。整体架构为分层、异步并行设计，降低推理延迟、提升探索积极性。

Result: 在UAV-ON等具有挑战性基准上，APEX较前沿方法提升4.2%任务成功率（SR）和2.8%路径效率（SPL），展现出更优的探索效率和层次异步结构的有效性。

Conclusion: APEX有效提升了无人机复杂环境目标导航任务的效率，其模块化异步分层设计为后续相关领域研究和应用提供了有价值的范式。

Abstract: Aerial Object Goal Navigation, a challenging frontier in Embodied AI, requires an Unmanned Aerial Vehicle (UAV) agent to autonomously explore, reason, and identify a specific target using only visual perception and language description. However, existing methods struggle with the memorization of complex spatial representations in aerial environments, reliable and interpretable action decision-making, and inefficient exploration and information gathering. To address these challenges, we introduce \textbf{APEX} (Aerial Parallel Explorer), a novel hierarchical agent designed for efficient exploration and target acquisition in complex aerial settings. APEX is built upon a modular, three-part architecture: 1) Dynamic Spatio-Semantic Mapping Memory, which leverages the zero-shot capability of a Vision-Language Model (VLM) to dynamically construct high-resolution 3D Attraction, Exploration, and Obstacle maps, serving as an interpretable memory mechanism. 2) Action Decision Module, trained with reinforcement learning, which translates this rich spatial understanding into a fine-grained and robust control policy. 3) Target Grounding Module, which employs an open-vocabulary detector to achieve definitive and generalizable target identification. All these components are integrated into a hierarchical, asynchronous, and parallel framework, effectively bypassing the VLM's inference latency and boosting the agent's proactivity in exploration. Extensive experiments show that APEX outperforms the previous state of the art by +4.2\% SR and +2.8\% SPL on challenging UAV-ON benchmarks, demonstrating its superior efficiency and the effectiveness of its hierarchical asynchronous design. Our source code is provided in \href{https://github.com/4amGodvzx/apex}{GitHub}

</details>


### [460] [ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation](https://arxiv.org/abs/2602.00557)
*Weisheng Dai,Kai Lan,Jianyi Zhou,Bo Zhao,Xiu Su,Junwen Tong,Weili Guan,Shuo Yang*

Main category: cs.RO

TL;DR: 提出了一种名为ConLA的新方法，从无标注的人类演示视频中，以对比性方法分离运动动态与视觉内容，实现了机器人策略的无监督预训练，首次在性能上超过传统机器人操作数据。


<details>
  <summary>Details</summary>
Motivation: 现有视-语言-动作模型依赖大规模机器人人工数据，获取成本高且难以扩展。相比之下，人类演示视频更丰富易得，但缺乏动作监督信息，现有无监督方法容易陷入捷径学习，难以获得可迁移表征。因此，如何高效利用人类视频，提取纯净、可迁移的潜在动作成为核心问题。

Method: 提出了ConLA（Contrastive disentanglement for Learning from Action），引入对比式解耦机制，结合动作先验与时序信息，将运动动态从视觉内容中分离出来，减少对表面视觉线索的依赖，并对潜在动作表征进行去相关。整个过程完全基于无标注人类视频进行预训练。

Result: 实验表明ConLA在多个机器人任务基准上表现优异，且仅用人类视频预训练即可超过过去依赖机器人真实演示轨迹的预训练表现。

Conclusion: ConLA能够通过人类视频高效学习机器人可用的动作表征，为大规模、更通用的机器人学习开启新路径，突破了数据获取和泛化能力的双重瓶颈。

Abstract: Vision-Language-Action (VLA) models achieve preliminary generalization through pretraining on large scale robot teleoperation datasets. However, acquiring datasets that comprehensively cover diverse tasks and environments is extremely costly and difficult to scale. In contrast, human demonstration videos offer a rich and scalable source of diverse scenes and manipulation behaviors, yet their lack of explicit action supervision hinders direct utilization. Prior work leverages VQ-VAE based frameworks to learn latent actions from human videos in an unsupervised manner. Nevertheless, since the training objective primarily focuses on reconstructing visual appearances rather than capturing inter-frame dynamics, the learned representations tend to rely on spurious visual cues, leading to shortcut learning and entangled latent representations that hinder transferability. To address this, we propose ConLA, an unsupervised pretraining framework for learning robotic policies from human videos. ConLA introduces a contrastive disentanglement mechanism that leverages action category priors and temporal cues to isolate motion dynamics from visual content, effectively mitigating shortcut learning. Extensive experiments show that ConLA achieves strong performance across diverse benchmarks. Notably, by pretraining solely on human videos, our method for the first time surpasses the performance obtained with real robot trajectory pretraining, highlighting its ability to extract pure and semantically consistent latent action representations for scalable robot learning.

</details>


### [461] [UniMotion: A Unified Motion Framework for Simulation, Prediction and Planning](https://arxiv.org/abs/2602.00566)
*Nan Song,Junzhe Jiang,Jingyu Li,Xiatian Zhu,Li Zhang*

Main category: cs.RO

TL;DR: 本文提出了一个统一的运动任务框架UniMotion，用于自动驾驶中的运动模拟、预测与规划，实现多任务共享、协同优化与泛化提升。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶领域中运动模拟、预测和规划是核心任务，但现有方法通常对每个任务采用专门模型，限制了各任务间的泛化能力和系统规模扩展。此外，分割的策略还忽略了任务间本可互利的信息共享。

Method: 提出了基于decoder-only Transformer结构的UniMotion框架，设计了专门的交互模式和训练策略，支持多任务联合训练，同时可针对单一任务进行微调优化。

Result: 在Waymo Open Motion Dataset上实验显示，联合训练有助于泛化能力提升和多任务集成，通过进一步微调，UniMotion在多项运动任务中达到了最新最优效果。

Conclusion: UniMotion作为统一化方案提升了自动驾驶系统在多任务上的表现、泛化能力与可扩展性，为运动相关任务提供了有竞争力的解决方案。

Abstract: Motion simulation, prediction and planning are foundational tasks in autonomous driving, each essential for modeling and reasoning about dynamic traffic scenarios. While often addressed in isolation due to their differing objectives, such as generating diverse motion states or estimating optimal trajectories, these tasks inherently depend on shared capabilities: understanding multi-agent interactions, modeling motion behaviors, and reasoning over temporal and spatial dynamics. Despite this underlying commonality, existing approaches typically adopt specialized model designs, which hinders cross-task generalization and system scalability. More critically, this separation overlooks the potential mutual benefits among tasks. Motivated by these observations, we propose UniMotion, a unified motion framework that captures shared structures across motion tasks while accommodating their individual requirements. Built on a decoder-only Transformer architecture, UniMotion employs dedicated interaction modes and tailored training strategies to simultaneously support these motion tasks. This unified design not only enables joint optimization and representation sharing but also allows for targeted fine-tuning to specialize in individual tasks when needed. Extensive experiments on the Waymo Open Motion Dataset demonstrate that joint training leads to robust generalization and effective task integration. With further fine-tuning, UniMotion achieves state-of-the-art performance across a range of motion tasks, establishing it as a versatile and scalable solution for autonomous driving.

</details>


### [462] [Agentic Reward Modeling: Verifying GUI Agent via Online Proactive Interaction](https://arxiv.org/abs/2602.00575)
*Chaoqun Cui,Jing Huang,Shijing Wang,Liming Zheng,Qingchao Kong,Zhixiong Zeng*

Main category: cs.RO

TL;DR: 该论文提出了VAGEN框架，以提升强化学习GUI任务中自验证能力，相较于传统方法能够更准确且主动地评估智能体任务完成情况。


<details>
  <summary>Details</summary>
Motivation: 现有GUI智能体的强化学习评估方式存在显著局限：基于规则的方法扩展性差、难以处理开放性任务，基于LLM判定的方法仅能被动观察视觉结果，无法感知系统的潜在状态，导致评估不准确。

Method: 作者提出了一种称为Agentic Interactive Verification的新范式，并具体实现为VAGEN框架。VAGEN通过集成具备交互工具的验证智能体，使其能够主动制定验证计划，主动与环境交互以寻找任务完成的证据，从而绕开传统方法对视觉信息的依赖。

Result: 在OSWorld-Verified和AndroidWorld基准测试中，VAGEN框架显著提升了评估的准确性，优于现有的LLM-as-a-Judge基线方法，并在引入测试时扩展策略后进一步增强性能。

Conclusion: VAGEN框架实现了GUI强化学习任务中更为主动、精确的自验证能力，克服了视觉限制，提升了评估能力，为后续GUI智能体的持续进化提供了新的评估范式。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is pivotal for the continuous evolution of GUI agents, yet existing evaluation paradigms face significant limitations. Rule-based methods suffer from poor scalability and cannot handle open-ended tasks, while LLM-as-a-Judge approaches rely on passive visual observation, often failing to capture latent system states due to partial state observability. To address these challenges, we advocate for a paradigm shift from passive evaluation to Agentic Interactive Verification. We introduce VAGEN, a framework that employs a verifier agent equipped with interaction tools to autonomously plan verification strategies and proactively probe the environment for evidence of task completion. Leveraging the insight that GUI tasks are typically "easy to verify but hard to solve", VAGEN overcomes the bottlenecks of visual limitations. Experimental results on OSWorld-Verified and AndroidWorld benchmarks demonstrate that VAGEN significantly improves evaluation accuracy compared to LLM-as-a-Judge baselines and further enhances performance through test-time scaling strategies.

</details>


### [463] [Factored Reasoning with Inner Speech and Persistent Memory for Evidence-Grounded Human-Robot Interaction](https://arxiv.org/abs/2602.00675)
*Valerio Belcamino,Mariya Kilina,Alessandro Carfì,Valeria Seidita,Fulvio Mastrogiovanni,Antonio Chella*

Main category: cs.RO

TL;DR: 本文提出了用于增强人机对话的认知型机器人助理架构JANUS，能有效保持用户上下文、处理不完整请求，并基于外部证据给出可查证回复。JANUS通过模块化分解对话过程，并通过专门的记忆与推理机制提升健壮性和可审计性。其在膳食辅助场景中表现出良好的参考一致性与响应效率。


<details>
  <summary>Details</summary>
Motivation: 现有对话型机器人在保持上下文、处理不完整请求、可验证推理等方面存在不足，难以支持长时、具备证据验证和审计性的实际应用，因此需要新型认知架构提升这方面能力。

Method: 提出了JANUS认知架构：1）将整体行为分为多个子模块，涵盖范围检测、意图识别、记忆、内语、查询生成和外部对话等功能。2）采用带类型接口的分解控制器；3）设计专门的记忆体 agent，结合短期、核心和长期三层存储，通过语义检索和受控整合修正策略管理上下文；4）引入仿生内语流，提前验证参数完整性并触发澄清，提升决策可靠性；5）输出与外部知识工具紧密结合，通过证据包确保机器声明可信。

Result: 在膳食辅助领域的知识图谱环境下，JANUS各模块通过单元测试，结果与专家标注高度一致，同时响应时延满足实际需求，表现优秀。

Conclusion: JANUS架构表明基于分解模块与证据驱动推理的认知机器人在可扩展性、可审计性和长时交互能力上具有明显优势，为实现高水平对话助手提供了实践路径。

Abstract: Dialogue-based human-robot interaction requires robot cognitive assistants to maintain persistent user context, recover from underspecified requests, and ground responses in external evidence, while keeping intermediate decisions verifiable. In this paper we introduce JANUS, a cognitive architecture for assistive robots that models interaction as a partially observable Markov decision process and realizes control as a factored controller with typed interfaces. To this aim, Janus (i) decomposes the overall behavior into specialized modules, related to scope detection, intent recognition, memory, inner speech, query generation, and outer speech, and (ii) exposes explicit policies for information sufficiency, execution readiness, and tool grounding. A dedicated memory agent maintains a bounded recent-history buffer, a compact core memory, and an archival store with semantic retrieval, coupled through controlled consolidation and revision policies. Models inspired by the notion of inner speech in cognitive theories provide a control-oriented internal textual flow that validates parameter completeness and triggers clarification before grounding, while a faithfulness constraint ties robot-to-human claims to an evidence bundle combining working context and retrieved tool outputs. We evaluate JANUS through module-level unit tests in a dietary assistance domain grounded on a knowledge graph, reporting high agreement with curated references and practical latency profiles. These results support factored reasoning as a promising path to scalable, auditable, and evidence-grounded robot assistance over extended interaction horizons.

</details>


### [464] [Toward Reliable Sim-to-Real Predictability for MoE-based Robust Quadrupedal Locomotion](https://arxiv.org/abs/2602.00678)
*Tianyang Wu,Hanwei Guo,Yuhang Wang,Junshu Yang,Xinyang Sui,Jiayi Xie,Xingyu Chen,Zeyang Liu,Xuguang Lan*

Main category: cs.RO

TL;DR: 本文提出了一种结合专家混合模型（MoE）和仿真到现实转移评估的新框架，有效提升四足机器人在多种复杂地形下的自适应能力，仅凭本体感知即可实现鲁棒行走，并通过RoboGauge指标量化其迁移能力，减少了物理测试需求。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习已在四足机器人行动方面取得了进展，但由于仿真与现实之间的差距以及在复杂环境下奖励过拟合，导致很多策略无法有效地从仿真迁移到现实，而且实体机器人测试存在高风险与低效率等问题。

Method: 作者提出了混合专家（Mixture-of-Experts, MoE）策略网络，该网络通过门控机制结合若干专长专家，对不同地形与动作指令进行建模。此外，作者设计了RoboGauge评估系统，可以在不同地形、难度和随机化环境中，通过纯仿真方法量化MoE策略的迁移能力，从而无需大量实体测试就能选择最优策略。

Result: 在Unitree Go2机器人上的实验显示，该方法能在全新且复杂的地形（如雪、沙、楼梯、斜坡、高达30厘米的障碍）上实现稳健运动。在高速度测试下，机器人达到4 m/s，并自主出现了具有更高稳定性的窄步宽步态。

Conclusion: 本文所提出的MoE-RoboGauge统一框架，有效提升了四足机器人仅凭本体感知的多地形鲁棒行走能力，并通过可靠的迁移性评估手段显著降低了物理实验的依赖，有力推动了四足机器人实际部署的可行性与安全性。

Abstract: Reinforcement learning has shown strong promise for quadrupedal agile locomotion, even with proprioception-only sensing. In practice, however, sim-to-real gap and reward overfitting in complex terrains can produce policies that fail to transfer, while physical validation remains risky and inefficient. To address these challenges, we introduce a unified framework encompassing a Mixture-of-Experts (MoE) locomotion policy for robust multi-terrain representation with RoboGauge, a predictive assessment suite that quantifies sim-to-real transferability. The MoE policy employs a gated set of specialist experts to decompose latent terrain and command modeling, achieving superior deployment robustness and generalization via proprioception alone. RoboGauge further provides multi-dimensional proprioception-based metrics via sim-to-sim tests over terrains, difficulty levels, and domain randomizations, enabling reliable MoE policy selection without extensive physical trials. Experiments on a Unitree Go2 demonstrate robust locomotion on unseen challenging terrains, including snow, sand, stairs, slopes, and 30 cm obstacles. In dedicated high-speed tests, the robot reaches 4 m/s and exhibits an emergent narrow-width gait associated with improved stability at high velocity.

</details>


### [465] [Learning to Accelerate Vision-Language-Action Models through Adaptive Visual Token Caching](https://arxiv.org/abs/2602.00686)
*Yujie Wei,Jiahan Fan,Jiyu Guo,Ruichen Zhen,Rui Shao,Xiu Su,Zeke Xie,Shuo Yang*

Main category: cs.RO

TL;DR: 本文提出了一种可学习的推理加速框架，通过动态、任务感知的策略提升视觉-语言-动作（VLA）模型在机器人任务中的推理速度与表现。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在机器人操作任务中表现优异但推理开销大，严重限制实际应用。提升推理效率成为亟需解决的问题，现有加速方法多为静态、无任务感知的启发式策略，难以适应动态场景。

Method: 作者将推理加速建模为策略优化问题，设计了两模块：缓存token选择器（决定哪些token复用）和缓存比例预测器（决定复用比例）。这两模块通过可微松弛技术，可端到端梯度优化。

Result: 在LIBERO与SIMPLER基准及真实机器人测试中，新方法实现了1.76倍推理加速，且在LIBERO的成功率提升1.9个百分点（75.0%→76.9%），真实任务提升5个百分点，显著超越现有方法。

Conclusion: 本文展示了学习任务感知计算分配策略的潜力，为打造高效又强大的VLA模型迈出重要一步。

Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable generalization capabilities in robotic manipulation tasks, yet their substantial computational overhead remains a critical obstacle to real-world deployment. Improving inference efficiency is therefore essential for practical robotic applications. Existing acceleration methods often rely on heuristic or static strategies--such as rule-based token caching or pruning--that are decoupled from task objectives and fail to adapt to dynamic scene changes. In this work, we reformulate inference acceleration as a learnable policy optimization problem and propose a novel framework that integrates a dynamic, task-aware decision-making process directly into the VLA model. At its core are two lightweight, cooperative modules: a Cached Token Selector, which determines which tokens should be reused, and a Cache Ratio Predictor, which controls how many tokens to reuse. Training these modules is non-trivial due to their discrete decisions. We address this by adopting a differentiable relaxation that allows gradient-based end-to-end optimization. Extensive experiments on the LIBERO and SIMPLER benchmarks, as well as real-robot evaluations, show that our method achieves a 1.76x wall-clock inference speedup while simultaneously improving the average success rate by 1.9 percentage points (from 75.0% to 76.9%) on LIBERO and by 5.0 percentage points on real-world tasks, significantly outperforming existing baselines. This work highlights the potential of learning task-aware computational allocation policies, paving the way for VLA models that are both powerful and efficient.

</details>


### [466] [USS-Nav: Unified Spatio-Semantic Scene Graph for Lightweight UAV Zero-Shot Object Navigation](https://arxiv.org/abs/2602.00708)
*Weiqi Gai,Yuman Gao,Yuan Zhou,Yufan Xie,Zhiyang Liu,Yuze Wu,Xin Zhou,Fei Gao,Zhijun Meng*

Main category: cs.RO

TL;DR: 提出USS-Nav框架，在资源有限的平台上实现高效的大语言模型辅助零样本目标导航，提升了无人机在未知环境中的导航效率。


<details>
  <summary>Details</summary>
Motivation: 在未知环境中进行零样本目标导航时，无人机面临高层语义推理与有限算力的矛盾，现有方法难以兼顾效率与语义理解能力。

Method: 提出Incremental Spatial Connectivity Graph结合多面体扩展捕捉全局几何结构，通过图聚类动态分区为语义区域，并将开放词汇目标语义锚定在该结构上。再利用分层场景图结构，大语言模型负责粗粒度目标区域判别，局部规划器根据信息增益高效探索。

Result: 实验表明，该框架在有限算力平台下，计算效率与实时更新频率优于现有最新方法（15 Hz），并在Success weighted by Path Length (SPL)等指标上取得明显提升。消融实验验证了各子模块的有效性。

Conclusion: USS-Nav有效解决了无人机在未知环境下高效、语义理解驱动的目标导航难题，促进了相关领域研究发展。源码将公开，有助于后续研究。

Abstract: Zero-Shot Object Navigation in unknown environments poses significant challenges for Unmanned Aerial Vehicles (UAVs) due to the conflict between high-level semantic reasoning requirements and limited onboard computational resources. To address this, we present USS-Nav, a lightweight framework that incrementally constructs a Unified Spatio-Semantic scene graph and enables efficient Large Language Model (LLM)-augmented Zero-Shot Object Navigation in unknown environments. Specifically, we introduce an incremental Spatial Connectivity Graph generation method utilizing polyhedral expansion to capture global geometric topology, which is dynamically partitioned into semantic regions via graph clustering. Concurrently, open-vocabulary object semantics are instantiated and anchored to this topology to form a hierarchical environmental representation. Leveraging this hierarchical structure, we present a coarse-to-fine exploration strategy: LLM grounded in the scene graph's semantics to determine global target regions, while a local planner optimizes frontier coverage based on information gain. Experimental results demonstrate that our framework outperforms state-of-the-art methods in terms of computational efficiency and real-time update frequency (15 Hz) on a resource-constrained platform. Furthermore, ablation studies confirm the effectiveness of our framework, showing substantial improvements in Success weighted by Path Length (SPL). The source code will be made publicly available to foster further research.

</details>


### [467] [SA-VLA: Spatially-Aware Flow-Matching for Vision-Language-Action Reinforcement Learning](https://arxiv.org/abs/2602.00743)
*Xu Pan,Zhenglin Wan,Xingrui Yu,Xianwei Zheng,Youkai Ke,Ming Sun,Rui Wang,Ziwei Wang,Ivor Tsang*

Main category: cs.RO

TL;DR: 本论文提出了SA-VLA，一种空间感知的视觉-语言-动作模型强化学习微调方法，有效提升在复杂操作任务下的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在处理空间分布变化时，经过强化学习微调后常出现鲁棒性下降，主要原因是空间归纳偏置（spatial inductive bias）在微调中受损，导致模型更依赖短期视觉线索。

Method: 作者提出SA-VLA框架，通过融合隐式空间表示与视觉tokens，设计基于几何进展的密集奖励，并引入SCAN（空间条件渐进式探索策略），在强化学习策略优化时保持空间基础。这样可以实现表征学习、奖励设计和探索行为三方面与任务几何对齐。

Result: 在多物体和杂乱操作任务基准上，SA-VLA实现了更加稳定的强化学习微调，并提升了零样本下的空间泛化能力，使机器人获得更强的可迁移性和鲁棒性表现。

Conclusion: SA-VLA能够显著缓解RL微调下空间泛化能力损失，提升VLA模型的操纵能力与实际应用潜力。

Abstract: Vision-Language-Action (VLA) models exhibit strong generalization in robotic manipulation, yet reinforcement learning (RL) fine-tuning often degrades robustness under spatial distribution shifts. For flow-matching VLA policies, this degradation is closely associated with the erosion of spatial inductive bias during RL adaptation, as sparse rewards and spatially agnostic exploration increasingly favor short-horizon visual cues. To address this issue, we propose \textbf{SA-VLA}, a spatially-aware RL adaptation framework that preserves spatial grounding during policy optimization by aligning representation learning, reward design, and exploration with task geometry. SA-VLA fuses implicit spatial representations with visual tokens, provides dense rewards that reflect geometric progress, and employs \textbf{SCAN}, a spatially-conditioned annealed exploration strategy tailored to flow-matching dynamics. Across challenging multi-object and cluttered manipulation benchmarks, SA-VLA enables stable RL fine-tuning and improves zero-shot spatial generalization, yielding more robust and transferable behaviors. Code and project page are available at https://xupan.top/Projects/savla.

</details>


### [468] [Physics-informed Diffusion Mamba Transformer for Real-world Driving](https://arxiv.org/abs/2602.00808)
*Hang Zhou,Qiang Zhang,Peiran Liu,Yihao Qin,Zhaoxu Yan,Yiding Ji*

Main category: cs.RO

TL;DR: 本文提出了一种将Mamba Transformer与扩散模型结合，并融入Port-Hamiltonian神经网络的轨迹规划方法，在自动驾驶中的准确性与物理可行性大幅优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶轨迹规划方法虽然能模拟不确定性，但常常难以处理复杂的时序依赖与物理约束，导致预测轨迹不合理或缺乏可解释性。

Method: 1. 提出Diffusion Mamba Transformer架构，在扩散生成过程中引入Mamba结构和注意力机制，更好地聚合传感器与历史轨迹的时序上下文。2. 设计Port-Hamiltonian神经网络模块，将物理能源约束无缝嵌入扩散模型，实现物理一致性的轨迹预测。

Result: 在多个自动驾驶标准数据集上，所提方法在预测准确性、物理合理性和鲁棒性方面均显著优于主流基线方法。

Conclusion: 该框架有效提升了自动驾驶系统的轨迹预测能力，为实现安全可靠的运动规划奠定了基础。

Abstract: Autonomous driving systems demand trajectory planners that not only model the inherent uncertainty of future motions but also respect complex temporal dependencies and underlying physical laws. While diffusion-based generative models excel at capturing multi-modal distributions, they often fail to incorporate long-term sequential contexts and domain-specific physical priors. In this work, we bridge these gaps with two key innovations. First, we introduce a Diffusion Mamba Transformer architecture that embeds mamba and attention into the diffusion process, enabling more effective aggregation of sequential input contexts from sensor streams and past motion histories. Second, we design a Port-Hamiltonian Neural Network module that seamlessly integrates energy-based physical constraints into the diffusion model, thereby enhancing trajectory predictions with both consistency and interpretability. Extensive evaluations on standard autonomous driving benchmarks demonstrate that our unified framework significantly outperforms state-of-the-art baselines in predictive accuracy, physical plausibility, and robustness, thereby advancing safe and reliable motion planning.

</details>


### [469] [SyNeT: Synthetic Negatives for Traversability Learning](https://arxiv.org/abs/2602.00814)
*Bomena Kim,Hojun Lee,Younsoo Park,Yaoyu Hu,Sebastian Scherer,Inwook Shim*

Main category: cs.RO

TL;DR: 该论文提出通过生成合成的不可通行（负）样本，提升了视觉导航中可通行性估计模型识别各种不可通行区域的能力，并验证了方法对鲁棒性和泛化性能的提升。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督可通行性估计框架主要基于正样本和未标记样本，缺乏显式负样本（不可通行区域），导致模型在复杂户外环境中无法准确识别多样化的不可通行区域。

Method: 提出生成合成负样本的方法，将这些代表“看似可通行但实际上不可通行”的样本加入训练，适用于PU和PN架构，无需修改推理过程。同时，引入基于目标的假阳性率（FPR）评估方法，分析模型在合成负样本插入区域的表现，无需额外手工标注。

Result: 在公开及自采数据集上的大量实验表明，该方法显著提高了模型在不同环境下的鲁棒性与泛化能力。

Conclusion: 通过构建合成负样本并优化训练，能够有效提升视觉导航系统对于不可通行区域的检测能力，方法简单易用且通用性强。

Abstract: Reliable traversability estimation is crucial for autonomous robots to navigate complex outdoor environments safely. Existing self-supervised learning frameworks primarily rely on positive and unlabeled data; however, the lack of explicit negative data remains a critical limitation, hindering the model's ability to accurately identify diverse non-traversable regions. To address this issue, we introduce a method to explicitly construct synthetic negatives, representing plausible but non-traversable, and integrate them into vision-based traversability learning. Our approach is formulated as a training strategy that can be seamlessly integrated into both Positive-Unlabeled (PU) and Positive-Negative (PN) frameworks without modifying inference architectures. Complementing standard pixel-wise metrics, we introduce an object-centric FPR evaluation approach that analyzes predictions in regions where synthetic negatives are inserted. This evaluation provides an indirect measure of the model's ability to consistently identify non-traversable regions without additional manual labeling. Extensive experiments on both public and self-collected datasets demonstrate that our approach significantly enhances robustness and generalization across diverse environments. The source code and demonstration videos are publicly available at the project page: https://anonymous-synet.github.io/SyNet.github.io/

</details>


### [470] [Ocean Current-Harnessing Stage-Gated MPC: Monotone Cost Shaping and Speed-to-Fly for Energy-Efficient AUV Navigation](https://arxiv.org/abs/2602.00823)
*Spyridon Syntakas,Kostas Vlachos*

Main category: cs.RO

TL;DR: 本文提出了一种新型MPC方法，通过利用海洋洋流提升AUV续航与能效，并在仿真中实现能耗大幅下降。


<details>
  <summary>Details</summary>
Motivation: AUV受限于能效和续航能力，洋流带来机会，但目前控制方法利用有限。因此需开发更有效利用洋流的路径规划与控制策略。

Method: 提出Current-Harnessing Stage-Gated MPC方法，在预测时域中计算洋流对控制目标的“有利程度”标量，仅在“有利”时刻调整目标函数。方法结构包括：一是有帮助门控的单调成本塑型，用于放宽位置误差并带来有界的能量返还，保证目标函数优于基线；二是速度匹配成本，鼓励推进力减少、实现与洋流速度匹配，达到近零水相对“滑翔”。所有成本项均为C1连续，可直接集成进MPC设计。

Result: 在真实海洋流场下，用BlueROV2模型进行大规模仿真。新方法与常规预测控制对比，表现为显著降低能耗，同时保持相似的到达时间与约束满足。

Conclusion: 该方法能在保证性能的前提下，大幅降低AUV能耗，可广泛集成到各类MPC中，有助于推动AUV在实际环境中的应用。

Abstract: Autonomous Underwater Vehicles (AUVs) are a highly promising technology for ocean exploration and diverse offshore operations, yet their practical deployment is constrained by energy efficiency and endurance. To address this, we propose Current-Harnessing Stage-Gated MPC, which exploits ocean currents via a per-stage scalar which indicates the "helpfulness" of ocean currents. This scalar is computed along the prediction horizon to gate lightweight cost terms only where the ocean currents truly aids the control goal. The proposed cost terms, that are merged in the objective function, are (i) a Monotone Cost Shaping (MCS) term, a help-gated, non-worsening modification that relaxes along-track position error and provides a bounded translational energy rebate, guaranteeing the shaped objective is never larger than a set baseline, and (ii) a speed-to-fly (STF) cost component that increases the price of thrust and softly matches ground velocity to the ocean current, enabling near zero water-relative "gliding". All terms are C1 and integrate as a plug-and-play in MPC designs. Extensive simulations with the BlueROV2 model under realistic ocean current fields show that the proposed approach achieves substantially lower energy consumption than conventional predictive control while maintaining comparable arrival times and constraint satisfaction.

</details>


### [471] [Safe Stochastic Explorer: Enabling Safe Goal Driven Exploration in Stochastic Environments and Safe Interaction with Unknown Objects](https://arxiv.org/abs/2602.00868)
*Nikhil Uday Shinde,Dylan Hirsch,Michael C. Yip,Sylvia Herbert*

Main category: cs.RO

TL;DR: 本文提出了一种可在具有随机动态的未知环境中实现机器人安全探索的新方法（S.S.Explorer），通过权衡安全与信息收集，有效提升机器人在复杂环境下的自主能力。


<details>
  <summary>Details</summary>
Motivation: 目前机器人在实际未知或非结构化环境中常常面临动态未知和随机性，不易保证安全性，而现有的安全控制方法大多依赖于已知的系统动力学，无法应对现实环境中不可避免的随机性与不确定性。因此，需要一种既能保证探索安全、又能应对随机动态的新方法。

Method: 提出S.S.Explorer框架，结合高斯过程对未知安全函数进行在线学习，并利用模型的不确定性指导机器人采取信息收集行为以减少关于安全性的未知。同时分别针对离散状态空间和连续状态空间设计了算法，进一步扩展到了与多个未知物体的物理交互安全。

Result: 经过大量模拟实验和实际硬件测试，验证了所提方法在多种复杂和不确定环境中的有效性，在安全性和探索效率方面具有明显优势。

Conclusion: S.S.Explorer为在未知和随机环境中实现安全自主机器人导航和交互提供了有效方案，推动了机器人在现实世界复杂环境中广泛安全应用的进步。

Abstract: Autonomous robots operating in unstructured, safety-critical environments, from planetary exploration to warehouses and homes, must learn to safely navigate and interact with their surroundings despite limited prior knowledge. Current methods for safe control, such as Hamilton-Jacobi Reachability and Control Barrier Functions, assume known system dynamics. Meanwhile existing safe exploration techniques often fail to account for the unavoidable stochasticity inherent when operating in unknown real world environments, such as an exploratory rover skidding over an unseen surface or a household robot pushing around unmapped objects in a pantry. To address this critical gap, we propose Safe Stochastic Explorer (S.S.Explorer) a novel framework for safe, goal-driven exploration under stochastic dynamics. Our approach strategically balances safety and information gathering to reduce uncertainty about safety in the unknown environment. We employ Gaussian Processes to learn the unknown safety function online, leveraging their predictive uncertainty to guide information-gathering actions and provide probabilistic bounds on safety violations. We first present our method for discrete state space environments and then introduce a scalable relaxation to effectively extend this approach to continuous state spaces. Finally we demonstrate how this framework can be naturally applied to ensure safe physical interaction with multiple unknown objects. Extensive validation in simulation and demonstrative hardware experiments showcase the efficacy of our method, representing a step forward toward enabling reliable widespread robot autonomy in complex, uncertain environments.

</details>


### [472] [Learning When to Jump for Off-road Navigation](https://arxiv.org/abs/2602.00877)
*Zhipeng Zhao,Taimeng Fu,Shaoshu Su,Qiwei Du,Ehsan Tarkesh Esfahani,Karthik Dantu,Souma Chowdhury,Chen Wang*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的地形可通行性建模方法，使机器人在越野环境下能安全且高效地动态规划路径，有效减少绕行并提升通行速度。


<details>
  <summary>Details</summary>
Motivation: 传统路径规划算法多仅考虑位置或固定速度，忽略了不同速度下地形对机器人动态行为的影响，导致在复杂地形（如壕沟）下低速反而更危险。为实现速度动态相关的安全高效越野导航，有必要建立更精细的运动-地形关联建模方法。

Method: 提出Motion-aware Traversability (MAT)表示法，将每个地形区域的可通行性建模为速度的高斯函数。具体包括：(1)感知输入下单次前馈预测每个区域的高斯参数；(2)路径规划阶段根据当前动力学状态，用上述高斯函数高效地计算不同速度下的地形成本，无需反复推理。

Result: MAT系统在仿真和真实越野障碍环境中进行评估，结果表明可大幅缩短路线（绕行减少75%），同时保障复杂地形下的行驶安全，并保持实时规划效率。

Conclusion: MAT有效提升了越野导航系统对动态地形的适应能力，实现了更少绕行与更高安全性的统一，优于传统仅基于位置和固定速度的方案。

Abstract: Low speed does not always guarantee safety in off-road driving. For instance, crossing a ditch may be risky at a low speed due to the risk of getting stuck, yet safe at a higher speed with a controlled, accelerated jump. Achieving such behavior requires path planning that explicitly models complex motion dynamics, whereas existing methods often neglect this aspect and plan solely based on positions or a fixed velocity. To address this gap, we introduce Motion-aware Traversability (MAT) representation to explicitly model terrain cost conditioned on actual robot motion. Instead of assigning a single scalar score for traversability, MAT models each terrain region as a Gaussian function of velocity. During online planning, we decompose the terrain cost computation into two stages: (1) predict terrain-dependent Gaussian parameters from perception in a single forward pass, (2) efficiently update terrain costs for new velocities inferred from current dynamics by evaluating these functions without repeated inference. We develop a system that integrates MAT to enable agile off-road navigation and evaluate it in both simulated and real-world environments with various obstacles. Results show that MAT achieves real-time efficiency and enhances the performance of off-road navigation, reducing path detours by 75% while maintaining safety across challenging terrains.

</details>


### [473] [RoDiF: Robust Direct Fine-Tuning of Diffusion Policies with Corrupted Human Feedback](https://arxiv.org/abs/2602.00886)
*Amitesh Vatsa,Zhixian Xie,Wanxin Jin*

Main category: cs.RO

TL;DR: 该论文提出了一种新的方法RoDiF，能在存在错误人类偏好的情况下，对扩散策略进行稳健的微调，且优于已有方法。


<details>
  <summary>Details</summary>
Motivation: 当前用于机器人控制的扩散策略难以利用人类偏好信息进行优化，主要受限于扩散去噪过程的多步结构，尤其在偏好数据存在错误标签时更难处理。

Method: 作者提出了统一的马尔可夫决策过程（MDP）框架，将扩散去噪链与环境动力学结合，实现无奖励的直接偏好优化（DPO）。在此基础上，文中提出RoDiF算法，将DPO目标重新解释为几何假设切割视角，并采用保守的切割策略来提升对错误偏好的鲁棒性，无需假定噪声分布。

Result: RoDiF在长视野操作任务中大幅超越现有方法，即使在偏好标签有30%错误的条件下，也能稳定高效地微调不同结构的预训练扩散策略，使其更符合人类偏好。

Conclusion: RoDiF为扩散策略提供了稳健的基于人类偏好优化的新途径，加强了算法对此类噪声干扰下的表现，具有较强的实际应用潜力。

Abstract: Diffusion policies are a powerful paradigm for robotic control, but fine-tuning them with human preferences is fundamentally challenged by the multi-step structure of the denoising process. To overcome this, we introduce a Unified Markov Decision Process (MDP) formulation that coherently integrates the diffusion denoising chain with environmental dynamics, enabling reward-free Direct Preference Optimization (DPO) for diffusion policies. Building on this formulation, we propose RoDiF (Robust Direct Fine-Tuning), a method that explicitly addresses corrupted human preferences. RoDiF reinterprets the DPO objective through a geometric hypothesis-cutting perspective and employs a conservative cutting strategy to achieve robustness without assuming any specific noise distribution. Extensive experiments on long-horizon manipulation tasks show that RoDiF consistently outperforms state-of-the-art baselines, effectively steering pretrained diffusion policies of diverse architectures to human-preferred modes, while maintaining strong performance even under 30% corrupted preference labels.

</details>


### [474] [UniMorphGrasp: Diffusion Model with Morphology-Awareness for Cross-Embodiment Dexterous Grasp Generation](https://arxiv.org/abs/2602.00915)
*Zhiyuan Wu,Xiangyu Zhang,Zhuo Chen,Jiankang Deng,Rolandos Alexandros Potamias,Shan Luo*

Main category: cs.RO

TL;DR: 本论文提出UniMorphGrasp，通过引入手部形态信息和统一表示，实现了不同结构机械手的通用抓取生成方法，在现有基准和零样本泛化上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的灵巧抓取方法大多只适用于特定的机器人手结构，面对新型或未见过的机械手时泛化能力很弱，限制了通用性和实际应用。

Method: 提出以扩散模型为核心的UniMorphGrasp方法，首先将不同机械手的抓取动作统一映射到类似人手的标准姿态空间中，再结合由机械手结构生成的图表示和物体几何信息作为条件输入进行抓取生成。同时，利用手部运动学的层次结构信息设计监督损失，引导更精细的关节控制学习。

Result: 方法在多个灵巧抓取基准上取得最先进效果，并展现出对未见过机械手的零样本泛化能力，验证了其通用性和实用价值。

Conclusion: UniMorphGrasp显著提升了机械手跨结构抓取的统一性与泛化能力，为灵巧操作机器人的实际部署提供了有效解决方案。

Abstract: Cross-embodiment dexterous grasping aims to generate stable and diverse grasps for robotic hands with heterogeneous kinematic structures. Existing methods are often tailored to specific hand designs and fail to generalize to unseen hand morphologies outside the training distribution. To address these limitations, we propose \textbf{UniMorphGrasp}, a diffusion-based framework that incorporates hand morphological information into the grasp generation process for unified cross-embodiment grasp synthesis. The proposed approach maps grasps from diverse robotic hands into a unified human-like canonical hand pose representation, providing a common space for learning. Grasp generation is then conditioned on structured representations of hand kinematics, encoded as graphs derived from hand configurations, together with object geometry. In addition, a loss function is introduced that exploits the hierarchical organization of hand kinematics to guide joint-level supervision. Extensive experiments demonstrate that UniMorphGrasp achieves state-of-the-art performance on existing dexterous grasp benchmarks and exhibits strong zero-shot generalization to previously unseen hand structures, enabling scalable and practical cross-embodiment grasp deployment.

</details>


### [475] [Green-VLA: Staged Vision-Language-Action Model for Generalist Robots](https://arxiv.org/abs/2602.00919)
*I. Apanasevich,M. Artemyev,R. Babakyan,P. Fedotova,D. Grankin,E. Kupryashin,A. Misailidi,D. Nerus,A. Nutalapati,G. Sidorov,I. Efremov,M. Gerasyov,D. Pikurov,Y. Senchenko,S. Davidenko,D. Kulikov,M. Sultankin,K. Askarbek,O. Shamanin,D. Statovoy,E. Zalyaev,I. Zorin,A. Letkin,E. Rusakov,A. Silchenko,V. Vorobyov,S. Sobolnikov,A. Postnikov*

Main category: cs.RO

TL;DR: 本文提出了一种分阶段的视觉-语言-动作（VLA）框架Green-VLA，实现了在Green人形机器人等多种机器人上的泛化和真实世界部署，取得了强泛化性和优越性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界机器人应用需要算法能够在不同硬件和环境中泛化，同时保障安全性和高效性。现有方法在泛化、多模态理解和多样机器人控制方面存在局限。

Method: Green-VLA采用五阶段课程学习，包括基础视觉语言模型、多模态基础对齐、多机器人预训练、特定机器人适应和强化学习策略校准。同时，开发了3,000小时的演示数据处理流程，统一的机器人动作接口，以及多维推理时安全与指导机制。

Result: 在不同仿真与真实机器人平台（包括WidowX、CALVIN等）大量实验，Green-VLA在成功率、健壮性和长程任务效率上优于传统方法，强化学习阶段提升显著。

Conclusion: Green-VLA实现了有效的多模态感知与多机器人泛化控制，为现实机器人广泛部署提供了新范式。

Abstract: We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.

</details>


### [476] [SanD-Planner: Sample-Efficient Diffusion Planner in B-Spline Space for Robust Local Navigation](https://arxiv.org/abs/2602.00923)
*Jincheng Wang,Lingfan Bao,Tong Yang,Diego Martinez Plasencia,Jianhao Jiao,Dimitrios Kanoulas*

Main category: cs.RO

TL;DR: SanD-Planner是一种基于扩散和仿射B样条的局部规划方法，可在极少专家示范下实现高效和鲁棒的导航规划，并显著减少训练负担，取得领先效果。


<details>
  <summary>Details</summary>
Motivation: 在高度复杂和动态环境中实现可靠的局部路径规划极其困难，主要瓶颈为需要大量专家演示和低效的数据利用率。现有方法在样本需求和泛化能力上有限，难以应用于实际场景。

Method: 提出SanD-Planner，一种在约束B样条空间内，利用扩散模型和深度图像输入进行模仿学习的局部规划器。方法自然保证输出序列的平滑与误差界，并且融入基于ESDF的安全检查器，用明确的安全与完成时间指标替代复杂的价值函数学习。对比方法显著减少了训练所需的专家演示量。

Result: 仅用500组训练（为基线模型演示规模的0.25%），SanD-Planner在开放基准上达到了90.1%的仿真成功率和72.0%的室内仿真成功率，远超对比方法，并能直接无微调迁移到真实2D和3D实验场景。

Conclusion: SanD-Planner极大提升了局部规划算法的数据效率与泛化能力，减少人工示范成本，取得SOTA性能并具备优良迁移性。相关数据集和预训练模型也将开源，为领域发展提供资源。

Abstract: The challenge of generating reliable local plans has long hindered practical applications in highly cluttered and dynamic environments. Key fundamental bottlenecks include acquiring large-scale expert demonstrations across diverse scenes and improving learning efficiency with limited data. This paper proposes SanD-Planner, a sample-efficient diffusion-based local planner that conducts depth image-based imitation learning within the clamped B-spline space. By operating within this compact space, the proposed algorithm inherently yields smooth outputs with bounded prediction errors over local supports, naturally aligning with receding-horizon execution. Integration of an ESDF-based safety checker with explicit clearance and time-to-completion metrics further reduces the training burden associated with value-function learning for feasibility assessment. Experiments show that training with $500$ episodes (merely $0.25\%$ of the demonstration scale used by the baseline), SanD-Planner achieves state-of-the-art performance on the evaluated open benchmark, attaining success rates of $90.1\%$ in simulated cluttered environments and $72.0\%$ in indoor simulations. The performance is further proven by demonstrating zero-shot transferability to realistic experimentation in both 2D and 3D scenes. The dataset and pre-trained models will also be open-sourced.

</details>


### [477] [Minimal Footprint Grasping Inspired by Ants](https://arxiv.org/abs/2602.00935)
*Mohamed Sorour,Barbara Webb*

Main category: cs.RO

TL;DR: 本论文借鉴蚂蚁前足结构，设计出一种具有高摩擦力和仿生结构的低成本机械夹持器，提升了在杂乱环境下抓取物体的能力。实验显示该设计高效且可靠。


<details>
  <summary>Details</summary>
Motivation: 现有夹持器在应对堆积复杂或杂乱物体抓取时性能有限。观察到蚂蚁能高效应对类似问题，且其前足结构（高摩擦垫、毛发、柔性爪部）起到关键作用，因此希望借鉴其原理改进夹持器设计。

Method: 将蚂蚁前足的微观结构特征（高摩擦垫、表面毛发、柔性末端）抽象化，设计出仿生夹持器。该装置采用细长带高摩擦垫的前臂，低摩擦毛发，以及类似关节的结构，模拟蚂蚁前足的灵活性与摩擦特性，并进行实验验证。

Result: 实验表明，该仿生夹持器能稳健抓取各种消费品，每次尝试均成功。在复杂密集的环境下，也能高效完成单件抓取任务，表现出优异的杂物抓取能力。

Conclusion: 本研究提出的夹持器通过模仿蚂蚁前足结构显著提升了在复杂环境下的物体抓取能力，同时阐明了昆虫毛发结构与关节柔性的机械学意义，推动了夹持与仿生领域技术进步。

Abstract: Ants are highly capable of grasping objects in clutter, and we have recently observed that this involves substantial use of their forelegs. The forelegs, more specifically the tarsi, have high friction microstructures (setal pads), are covered in hairs, and have a flexible under-actuated tip. Here we abstract these features to test their functional advantages for a novel low-cost gripper design, suitable for bin-picking applications. In our implementation, the gripper legs are long and slim, with high friction gripping pads, low friction hairs and single-segment tarsus-like structure to mimic the insect's setal pads, hairs, and the tarsi's interactive compliance. Experimental evaluation shows this design is highly robust for grasping a wide variety of individual consumer objects, with all grasp attempts successful. In addition, we demonstrate this design is effective for picking single objects from dense clutter, a task at which ants also show high competence. The work advances grasping technology and shed new light on the mechanical importance of hairy structures and tarsal flexibility in insects.

</details>


### [478] [CLAMP: Contrastive Learning for 3D Multi-View Action-Conditioned Robotic Manipulation Pretraining](https://arxiv.org/abs/2602.00937)
*I-Chun Arthur Liu,Krzysztof Choromanski,Sandy Huang,Connor Schenck*

Main category: cs.RO

TL;DR: 该论文提出了一种融合点云与机器人动作信息的3D对比学习预训练方法（CLAMP），显著提升了机器人操作任务中的学习效率与性能，在仿真与真实各类任务中均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前主流的行为克隆策略依赖于预训练的2D图像表征，但2D信息无法充分刻画操作所需的重要3D时空关系，限制了机器人在复杂操控任务中的精度和泛化能力。因此，作者希望通过引入3D空间信息改善机器人学习过程。

Method: 作者提出CLAMP框架，首先将RGB-D图像与相机外参转换为3D点云，生成包含深度和3D坐标的多视角四通道观测图像（含动态手腕视角），使模型获得关键三维几何和位置线索。采用对比学习方法在大规模仿真轨迹上预训练编码器，并将Diffusion Policy用于初始化策略权重，提升微调阶段的数据利用效率和性能。最后在有限的真实演示数据上微调策略。

Result: 实验证明，CLAMP能够大幅提升对新任务的学习速度和策略表现，并在六项仿真任务和五项真实机器人操作任务中超过了当前最优基线方法。

Conclusion: 引入3D多视角动作条件编码的对比预训练方法有助于充分挖掘三维信息，显著提高机器人高精度操控的能力和泛化性，为未来高效自监督机器人学习提供了新路径。

Abstract: Leveraging pre-trained 2D image representations in behavior cloning policies has achieved great success and has become a standard approach for robotic manipulation. However, such representations fail to capture the 3D spatial information about objects and scenes that is essential for precise manipulation. In this work, we introduce Contrastive Learning for 3D Multi-View Action-Conditioned Robotic Manipulation Pretraining (CLAMP), a novel 3D pre-training framework that utilizes point clouds and robot actions. From the merged point cloud computed from RGB-D images and camera extrinsics, we re-render multi-view four-channel image observations with depth and 3D coordinates, including dynamic wrist views, to provide clearer views of target objects for high-precision manipulation tasks. The pre-trained encoders learn to associate the 3D geometric and positional information of objects with robot action patterns via contrastive learning on large-scale simulated robot trajectories. During encoder pre-training, we pre-train a Diffusion Policy to initialize the policy weights for fine-tuning, which is essential for improving fine-tuning sample efficiency and performance. After pre-training, we fine-tune the policy on a limited amount of task demonstrations using the learned image and action representations. We demonstrate that this pre-training and fine-tuning design substantially improves learning efficiency and policy performance on unseen tasks. Furthermore, we show that CLAMP outperforms state-of-the-art baselines across six simulated tasks and five real-world tasks.

</details>


### [479] [Meanshift Shape Formation Control Using Discrete Mass Distribution](https://arxiv.org/abs/2602.00980)
*Yichen Cai,Yuan Gao,Pengpeng Li,Wei Wang,Guibin Sun,Jinhu Lü*

Main category: cs.RO

TL;DR: 本文提出了一种全分布式且适应群体规模变化的集群形状控制方法，通过离散质量分布建模与分布式控制律，实现了复杂形状重构和规模适应性，并在仿真和实物实验中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于密度分布的集群控制方法虽能适应群体规模变化，但在复杂形状表达和分散实现方面存在难题。为解决这些实际挑战，提出一种无需定义连续密度函数的分布式控制方法，兼顾复杂形状形成和规模适应能力。

Method: 首先以一组离散采样点定义群体的质量分布函数用于建模；然后利用分布式均值漂移控制律，结合局部质量估算反馈，实现机器人集群的分布向采样点分布收敛。所有点的质量估算由机器人通过分布式估算器协作完成，并证明估算能渐近收敛到全局真实值。

Result: 通过大量仿真和实际机器人实验，结果验证了所提方法在复杂形状构建及群体规模变化下的高效率和适应能力。

Conclusion: 提出的分布式控制方案能有效实现复杂形状形成并适应群体规模变化，且不需要难以定义的连续密度函数，为真实场景的机器人集群控制提供了有力工具。

Abstract: The density-distribution method has recently become a promising paradigm owing to its adaptability to variations in swarm size. However, existing studies face practical challenges in achieving complex shape representation and decentralized implementation. This motivates us to develop a fully decentralized, distribution-based control strategy with the dual capability of forming complex shapes and adapting to swarm-size variations. Specifically, we first propose a discrete mass-distribution function defined over a set of sample points to model swarm formation. In contrast to the continuous density-distribution method, our model eliminates the requirement for defining continuous density functions-a task that is difficult for complex shapes. Second, we design a decentralized meanshift control law to coordinate the swarm's global distribution to fit the sample-point distribution by feeding back mass estimates. The mass estimates for all sample points are achieved by the robots in a decentralized manner via the designed mass estimator. It is shown that the mass estimates of the sample points can asymptotically converge to the true global values. To validate the proposed strategy, we conduct comprehensive simulations and real-world experiments to evaluate the efficiency of complex shape formation and adaptability to swarm-size variations.

</details>


### [480] [Geometry-Aware Sampling-Based Motion Planning on Riemannian Manifolds](https://arxiv.org/abs/2602.00992)
*Phone Thiha Kyaw,Jonathan Kelly*

Main category: cs.RO

TL;DR: 本文提出了一种直接在黎曼流形上进行采样的机器人运动规划方法，有效兼顾了高维系统的可扩展性与几何准确性。实验显示该方法在多个运动规划场景中优于现有欧氏或传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有许多运动规划器使用简单的欧氏距离，未能利用任务目标和物理约束在配置空间内诱导产生的非欧几里得几何，这导致路径不优或不现实。常规数值方法难以扩展至高维，采样方法则牺牲了几何精度。该论文希望解决这两者的权衡。

Method: 方法方面，作者提出了一种基于采样的运动规划框架，直接在配置相关的黎曼流形上进行操作。通过高效的中点型黎曼测地线距离近似（证明具有三阶精度），结合一阶重缩映射与黎曼自然梯度设计局部路径规划器。

Result: 实验在二连杆机械臂、七自由度Franka机械臂及带非完整约束的SE(2)刚体场景下进行。结果显示，本文方法能在动力学能量度量等复杂度量下获得比基于欧氏距离、传统黎曼测地线数值解法更低开销的无碰撞轨迹。

Conclusion: 所提方法能够有效结合高维可扩展性与流形几何结构，显著提升了复杂约束规划任务的路径质量，相较当前主流技术有明显优势。

Abstract: In many robot motion planning problems, task objectives and physical constraints induce non-Euclidean geometry on the configuration space, yet many planners operate using Euclidean distances that ignore this structure. We address the problem of planning collision-free motions that minimize length under configuration-dependent Riemannian metrics, corresponding to geodesics on the configuration manifold. Conventional numerical methods for computing such paths do not scale well to high-dimensional systems, while sampling-based planners trade scalability for geometric fidelity. To bridge this gap, we propose a sampling-based motion planning framework that operates directly on Riemannian manifolds. We introduce a computationally efficient midpoint-based approximation of the Riemannian geodesic distance and prove that it matches the true Riemannian distance with third-order accuracy. Building on this approximation, we design a local planner that traces the manifold using first-order retractions guided by Riemannian natural gradients. Experiments on a two-link planar arm and a 7-DoF Franka manipulator under a kinetic-energy metric, as well as on rigid-body planning in $\mathrm{SE}(2)$ with non-holonomic motion constraints, demonstrate that our approach consistently produces lower-cost trajectories than Euclidean-based planners and classical numerical geodesic-solver baselines.

</details>


### [481] [HERMES: A Holistic End-to-End Risk-Aware Multimodal Embodied System with Vision-Language Models for Long-Tail Autonomous Driving](https://arxiv.org/abs/2602.00993)
*Weizhe Tang,Junwei You,Jiaxi Liu,Zhaoyi Wang,Rui Gan,Zilin Huang,Feng Wei,Bin Ran*

Main category: cs.RO

TL;DR: 本文提出了HERMES框架，在长尾混合交通场景下，通过引入基于大模型注释的风险提示，提升端到端自动驾驶的安全性与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶模型虽然利用视觉-语言大模型增强了语义理解能力，但在复杂、罕见（长尾）混合交通场景下，依然难以保证安全和准确，尤其在需要应对多样化路况与参与者时风险较高。

Method: HERMES框架创新性地提出 foundation-model（大模型）辅助的注释流程，获取并结构化长尾场景和规划上下文，将以风险为中心的信息、驾驶意图和安全偏好注入到轨迹规划中。此外，引入三模态驾驶模块，将多视角感知、历史运动线索和语义引导进行融合，从而加强在长尾复杂交通下的风险感知与规划能力。

Result: 在真实长尾数据集上的实验表明，HERMES在各种长尾混合交通场景中均优于现有代表性端到端及视觉-语言模型驱动的基线方法。消融实验也验证了各关键模块的互补贡献。

Conclusion: HERMES通过结构化的风险提示和多模态融合，有效提升了自动驾驶系统在复杂长尾场景下的安全性和准确性，为端到端自动驾驶的新一代风险感知与决策提供了技术基础。

Abstract: End-to-end autonomous driving models increasingly benefit from large vision--language models for semantic understanding, yet ensuring safe and accurate operation under long-tail conditions remains challenging. These challenges are particularly prominent in long-tail mixed-traffic scenarios, where autonomous vehicles must interact with heterogeneous road users, including human-driven vehicles and vulnerable road users, under complex and uncertain conditions. This paper proposes HERMES, a holistic risk-aware end-to-end multimodal driving framework designed to inject explicit long-tail risk cues into trajectory planning. HERMES employs a foundation-model-assisted annotation pipeline to produce structured Long-Tail Scene Context and Long-Tail Planning Context, capturing hazard-centric cues together with maneuver intent and safety preference, and uses these signals to guide end-to-end planning. HERMES further introduces a Tri-Modal Driving Module that fuses multi-view perception, historical motion cues, and semantic guidance, ensuring risk-aware accurate trajectory planning under long-tail scenarios. Experiments on the real-world long-tail dataset demonstrate that HERMES consistently outperforms representative end-to-end and VLM-driven baselines under long-tail mixed-traffic scenarios. Ablation studies verify the complementary contributions of key components.

</details>


### [482] [Offline Discovery of Interpretable Skills from Multi-Task Trajectories](https://arxiv.org/abs/2602.01018)
*Chongyu Zhu,Mithun Vanniasinghe,Jiayu Chen,Chi-Guhn Lee*

Main category: cs.RO

TL;DR: 提出LOKI框架，通过三阶段流程在无奖励、多任务的离线数据中自动发现可复用技能，实现机器人行为的分层模仿学习。


<details>
  <summary>Details</summary>
Motivation: 当前的分层模仿学习难以在缺乏奖励和子任务标注的长时序、多任务离线数据中发现和利用可复用技能。需要一种端到端方法自动分割并组织技能，提升复杂任务学习的效率和泛化能力。

Method: 提出一个三阶段端到端学习框架LOKI：第一阶段用带弱标签的向量量化VAE做粗粒度的宏观分段；第二阶段用自监督顺序模型微调分段，并通过迭代聚类精确技能边界；第三阶段利用精准技能边界，在option方法框架下学习具备显式终止条件的分层策略。

Result: 在D4RL Kitchen基准测试中，LOKI显著优于传统分层模仿学习方法，取得了更高成功率。

Conclusion: LOKI不仅能自动发现与人类直觉一致和可组合的语义技能，还能通过技能组合解决新颖未见任务，展示了较强的泛化和可复用性。

Abstract: Hierarchical Imitation Learning is a powerful paradigm for acquiring complex robot behaviors from demonstrations. A central challenge, however, lies in discovering reusable skills from long-horizon, multi-task offline data, especially when the data lacks explicit rewards or subtask annotations. In this work, we introduce LOKI, a three-stage end-to-end learning framework designed for offline skill discovery and hierarchical imitation. The framework commences with a two-stage, weakly supervised skill discovery process: Stage one performs coarse, task-aware macro-segmentation by employing an alignment-enforced Vector Quantized VAE guided by weak task labels. Stage two then refines these segments at a micro-level using a self-supervised sequential model, followed by an iterative clustering process to consolidate skill boundaries. The third stage then leverages these precise boundaries to construct a hierarchical policy within an option-based framework-complete with a learned termination condition beta for explicit skill switching. LOKI achieves high success rates on the challenging D4RL Kitchen benchmark and outperforms standard HIL baselines. Furthermore, we demonstrate that the discovered skills are semantically meaningful, aligning with human intuition, and exhibit compositionality by successfully sequencing them to solve a novel, unseen task.

</details>


### [483] [Learning Adaptive Cross-Embodiment Visuomotor Policy with Contrastive Prompt Orchestration](https://arxiv.org/abs/2602.01040)
*Yuhang Zhang,Chao Yan,Jiaxi Yu,Jiaping Xiao,Mir Feroskhan*

Main category: cs.RO

TL;DR: 本文提出了一种名为ContrAstive Prompt Orchestration（CAPO）的新方法，用于提升视觉-运动策略在不同身体平台间的适应能力，显著优于目前主流方法，特别是在跨域与零样本适应上效果突出。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-运动控制方法在面临不同传感器、环境或系统物理属性变化时，一般难以分离任务相关特征和领域特有干扰，导致泛化性差和样本效率低等问题。

Method: 作者提出CAPO方法，结合对比学习与prompt机制。具体方法包括：（1）提出混合型对比学习，融合视觉、动作时序和文本目标，生成可学习的提示池来表达细粒度领域信息；（2）根据当前观测，自适应地聚合和选用这些提示，从而动态优化状态表示；（3）最终用于策略优化，提高鲁棒性与泛化性。

Result: 大量实验显示，CAPO在样本效率和最终性能上显著优于同类方法，且在照明、视场变化或旋转等剧烈环境与物理变动下实现了优越的零样本自适应。

Conclusion: CAPO有助于提升不同嵌入式体平台上视觉-运动决策的泛化和适应性，是跨体视觉运动控制领域的有效解决方案。

Abstract: Learning adaptive visuomotor policies for embodied agents remains a formidable challenge, particularly when facing cross-embodiment variations such as diverse sensor configurations and dynamic properties. Conventional learning approaches often struggle to separate task-relevant features from domain-specific variations (e.g., lighting, field-of-view, and rotation), leading to poor sample efficiency and catastrophic failure in unseen environments. To bridge this gap, we propose ContrAstive Prompt Orchestration (CAPO), a novel approach for learning visuomotor policies that integrates contrastive prompt learning and adaptive prompt orchestration. For prompt learning, we devise a hybrid contrastive learning strategy that integrates visual, temporal action, and text objectives to establish a pool of learnable prompts, where each prompt induces a visual representation encapsulating fine-grained domain factors. Based on these learned prompts, we introduce an adaptive prompt orchestration mechanism that dynamically aggregates these prompts conditioned on current observations. This enables the agent to adaptively construct optimal state representations by identifying dominant domain factors instantaneously. Consequently, the policy optimization is effectively shielded from irrelevant interference, preventing the common issue of overfitting to source domains. Extensive experiments demonstrate that CAPO significantly outperforms state-of-the-art baselines in sample efficiency and asymptotic performance. Crucially, it exhibits superior zero-shot adaptation across unseen target domains characterized by drastic environmental (e.g., illumination) and physical shifts (e.g., field-of-view and rotation), validating its effectiveness as a viable solution for cross-embodiment visuomotor policy adaptation.

</details>


### [484] [LLM-Based Behavior Tree Generation for Construction Machinery](https://arxiv.org/abs/2602.01041)
*Akinosuke Tsutsumi,Tomoya Itsuka,Yuichiro Kasahara,Tomoya Kouno,Kota Akinari,Genki Yamauchi,Daisuke Endo,Taro Abe,Takeshi Hashimoto,Keiji Nagatani,Ryo Kurazume*

Main category: cs.RO

TL;DR: 本文提出了一种基于大语言模型（LLM）的行为树（BT）自动生成方法，用于自动化地协调多台施工机械的协作，提高土方作业自动化水平，并验证了其在模拟和真实工地中的有效性。


<details>
  <summary>Details</summary>
Motivation: 土方作业对自动化需求日益增加，但由于工人老龄化和技能流失，现有基于手工设计行为树的方法在异构机器协作和可扩展性方面受限，需要新的自动化方案。

Method: 提出LLM驱动的工作流：第一步由LLM进行高层任务规划并生成同步标志，用于多机安全协作；第二步按照结构化模板生成行为树。利用系统数据库的参数实现安全规划，并支持异构建筑机械的自动化协作。

Result: 该方法在仿真环境下和实际复杂工地多机协作场景中进行了验证，展现了良好的任务规划与协作能力。

Conclusion: 基于LLM自动生成行为树的方法可有效提升土木工程施工自动化水平，保障多机械安全协作，具备实际应用和推广潜力。

Abstract: Earthwork operations are facing an increasing demand, while workforce aging and skill loss create a pressing need for automation. ROS2-TMS for Construction, a Cyber-Physical System framework designed to coordinate construction machinery, has been proposed for autonomous operation; however, its reliance on manually designed Behavior Trees (BTs) limits scalability, particularly in scenarios involving heterogeneous machine cooperation. Recent advances in large language models (LLMs) offer new opportunities for task planning and BT generation. However, most existing approaches remain confined to simulations or simple manipulators, with relatively few applications demonstrated in real-world contexts, such as complex construction sites involving multiple machines. This paper proposes an LLM-based workflow for BT generation, introducing synchronization flags to enable safe and cooperative operation. The workflow consists of two steps: high-level planning, where the LLM generates synchronization flags, and BT generation using structured templates. Safety is ensured by planning with parameters stored in the system database. The proposed method is validated in simulation and further demonstrated through real-world experiments, highlighting its potential to advance automation in civil engineering.

</details>


### [485] [A Systematic Study of Data Modalities and Strategies for Co-training Large Behavior Models for Robot Manipulation](https://arxiv.org/abs/2602.01067)
*Fanqi Lin,Kushal Arora,Jean Mercat,Haruki Nishimura,Paarth Shah,Chen Xu,Mengchao Zhang,Mark Zolotas,Maya Angeles,Owen Pfannenstiehl,Andrew Beaulieu,Jose Barreiros*

Main category: cs.RO

TL;DR: 本研究对机器人行为模型的联合训练数据模态和策略进行大规模实证研究，发现引入视觉-语言及跨机器人数据能显著提升泛化能力，并为构建可扩展的通用机器人策略提供实践指导。


<details>
  <summary>Details</summary>
Motivation: 大规模行为模型虽然显示出强大灵巧操作能力，但由于机器人数据覆盖有限，其泛化能力受限。直接采集更多机器人数据的成本高昂，因此亟需研究如何通过联合多种异质数据模态提高模型泛化，明确不同联合训练数据及策略对性能的影响。

Method: 作者设计了涵盖视觉-语言数据、轨迹密集语言标注、跨机体机器人数据、人类视频、离散机器人动作标记五种数据模态，以及单阶段和多阶段训练策略的大规模实证研究。共用4000小时机器人及人类操作数据、5000万视觉-语言样本，训练出多个视觉-语言-动作策略，在大量仿真和真实测试中系统评估各训练策略及数据组合的效果。

Result: 结果显示，联合视觉-语言和跨机体机器人数据的联合训练大幅提升泛化能力，尤其是在分布转移、未见任务、语言跟随等方面。离散动作标记效益有限，多模态结合产生递增增益，有助于对全新长时间操作任务的快速适应。单用机器人数据则损害模型的视觉-语言理解，联合训练可恢复此能力。以chain-of-thought链式推理数据显式指导动作生成对性能无益。

Conclusion: 有效数据模态的联合训练对于构建可扩展的通用机器人策略至关重要，本研究为如何高效选用异质数据提升机器人泛化性能提供了实践性指导。

Abstract: Large behavior models have shown strong dexterous manipulation capabilities by extending imitation learning to large-scale training on multi-task robot data, yet their generalization remains limited by the insufficient robot data coverage. To expand this coverage without costly additional data collection, recent work relies on co-training: jointly learning from target robot data and heterogeneous data modalities. However, how different co-training data modalities and strategies affect policy performance remains poorly understood. We present a large-scale empirical study examining five co-training data modalities: standard vision-language data, dense language annotations for robot trajectories, cross-embodiment robot data, human videos, and discrete robot action tokens across single- and multi-phase training strategies. Our study leverages 4,000 hours of robot and human manipulation data and 50M vision-language samples to train vision-language-action policies. We evaluate 89 policies over 58,000 simulation rollouts and 2,835 real-world rollouts. Our results show that co-training with forms of vision-language and cross-embodiment robot data substantially improves generalization to distribution shifts, unseen tasks, and language following, while discrete action token variants yield no significant benefits. Combining effective modalities produces cumulative gains and enables rapid adaptation to unseen long-horizon dexterous tasks via fine-tuning. Training exclusively on robot data degrades the visiolinguistic understanding of the vision-language model backbone, while co-training with effective modalities restores these capabilities. Explicitly conditioning action generation on chain-of-thought traces learned from co-training data does not improve performance in our simulation benchmark. Together, these results provide practical guidance for building scalable generalist robot policies.

</details>


### [486] [Estimating Force Interactions of Deformable Linear Objects from their Shapes](https://arxiv.org/abs/2602.01085)
*Qi Jing Chen,Shilin Shan,Timothy Bretl,Quang-Cuong Pham*

Main category: cs.RO

TL;DR: 本文提出一种通过观测可变形线性物体（如电线）形状来检测和估算其所受外力的方法，无需端执行器或昂贵力传感器。


<details>
  <summary>Details</summary>
Motivation: 在机器人与电线等线性物体的交互中，接触点常常不在末端执行器，而是在线体的其他位置。现有方法依赖于外部传感器且主要关注末端力，限制大、成本高，难以满足实际需求。准确检测接触对于安全高效的路径规划（如避免损伤、运动受限或危险情况）至关重要。

Method: 该方法利用深度摄像头获取线性物体（如电线）的形状，假定其处于或接近静力平衡状态。通过建立力矩平衡关系，推导一致性条件，将问题转化为线性方程组以同时估算外力的位置和大小，且无需额外的先验信息。

Result: 在仿真和真实环境实验中，该方法在多个场景下均能准确估算外力的大小和位置，展现出较高的可靠性和实用性。

Conclusion: 该方法有效弥补了现有方法的局限，能够无需专用力传感器，仅凭视觉信息实现对可变形线性物体外力的检测与估算，有助于提升机器人线性物体操作的安全性和效率。

Abstract: This work introduces an analytical approach for detecting and estimating external forces acting on deformable linear objects (DLOs) using only their observed shapes. In many robot-wire interaction tasks, contact occurs not at the end-effector but at other points along the robot's body. Such scenarios arise when robots manipulate wires indirectly (e.g., by nudging) or when wires act as passive obstacles in the environment. Accurately identifying these interactions is crucial for safe and efficient trajectory planning, helping to prevent wire damage, avoid restricted robot motions, and mitigate potential hazards. Existing approaches often rely on expensive external force-torque sensor or that contacts occur at the end-effector for accurate force estimation. Using wire shape information acquired from a depth camera and under the assumption that the wire is in or near its static equilibrium, our method estimates both the location and magnitude of external forces without additional prior knowledge. This is achieved by exploiting derived consistency conditions and solving a system of linear equations based on force-torque balance along the wire. The approach was validated through simulation, where it achieved high accuracy, and through real-world experiments, where accurate estimation was demonstrated in selected interaction scenarios.

</details>


### [487] [Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance](https://arxiv.org/abs/2602.01092)
*Peng Zhou,Zhongxuan Li,Jinsong Wu,Jiaming Qi,Jun Hu,David Navarro-Alarcon,Jia Pan,Lihua Xie,Shiyao Zhang,Zeqing Zhang*

Main category: cs.RO

TL;DR: 本文提出了一个基于保守价值学习的、具备失败感知能力的双手远程操作辅助框架，可以通过触觉辅助引导操作员远离易失败动作，从而在复杂、高精度操作任务中提升成功率并降低操作负担。


<details>
  <summary>Details</summary>
Motivation: 传统高精度远程操作由于成功容差小和接触动态复杂，操作员在部分可观测条件下难以预判失败，容易导致任务失败。因此，需要一种能在不剥夺人工主导性的前提下，主动感知和规避操作风险的技术。

Method: 利用包含成功与失败例子的异构离线远程操作数据，通过保守价值学习（Conservative Value Learning, CVL）训练出任务可行性的保守成功分数，并在在线操作时用该分数调节辅助强度，同时通过学习得到的行为体提供修正运动方向。这两个信号通过主控端的关节空间阻抗接口整合，为操作员提供顺从且不会强行覆盖意图的连续引导。

Result: 在富含接触的精细操作任务实验中，相比传统远程控制与共享自治基线方法，所提框架能显著提升任务成功率，并降低操作员负荷。

Conclusion: 保守价值学习为远程双向操作引入失败感知提供了有效机制，可以在保障人类主控权的情况下提升机器人操作的安全性和成功率。

Abstract: Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at https://www.youtube.com/watch?v=XDTsvzEkDRE

</details>


### [488] [StreamVLA: Breaking the Reason-Act Cycle via Completion-State Gating](https://arxiv.org/abs/2602.01100)
*Hang Wu,Tongqing Chen,Jiasen Wang,Xiaotao Li,Lu Fang*

Main category: cs.RO

TL;DR: StreamVLA是一种高效的机器人视觉-语言-动作模型架构，通过区分高层规划和低层控制，大幅降低延迟，同时提升操作稳健性，取得了前沿性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型在每一步都做冗余推理，导致决策延迟高、目标不稳定。如何兼顾高层任务分解和低层连续控制，实现高效且稳健的长时机器人操作，是亟需解决的问题。

Method: 提出StreamVLA双系统架构，将任务文本分解、视觉目标想象和动作生成统一于一个高效骨干网络。创新设计“Lock-and-Gated”机制，仅在检测到子任务切换时才进行详细推理，生成文本指令和具体视觉完成状态（而非泛化未来帧），其余时间锁定高层意图，用Flow Matching头高效生成动作，节省72%的解码计算。

Result: StreamVLA在LIBERO基准上取得98.5%的成功率，在真实环境干扰下具备强恢复力，推理延迟比全程推理基线低48%。

Conclusion: StreamVLA通过结构创新，成功将任务分解与连续控制高效衔接，实现了低延迟、高鲁棒性的机器人操作，显著领先同类方案。

Abstract: Long-horizon robotic manipulation requires bridging the gap between high-level planning (System 2) and low-level control (System 1). Current Vision-Language-Action (VLA) models often entangle these processes, performing redundant multimodal reasoning at every timestep, which leads to high latency and goal instability. To address this, we present StreamVLA, a dual-system architecture that unifies textual task decomposition, visual goal imagination, and continuous action generation within a single parameter-efficient backbone. We introduce a "Lock-and-Gated" mechanism to intelligently modulate computation: only when a sub-task transition is detected, the model triggers slow thinking to generate a textual instruction and imagines the specific visual completion state, rather than generic future frames. Crucially, this completion state serves as a time-invariant goal anchor, making the policy robust to execution speed variations. During steady execution, these high-level intents are locked to condition a Flow Matching action head, allowing the model to bypass expensive autoregressive decoding for 72% of timesteps. This hierarchical abstraction ensures sub-goal focus while significantly reducing inference latency. Extensive evaluations demonstrate that StreamVLA achieves state-of-the-art performance, with a 98.5% success rate on the LIBERO benchmark and robust recovery in real-world interference scenarios, achieving a 48% reduction in latency compared to full-reasoning baselines.

</details>


### [489] [KAN We Flow? Advancing Robotic Manipulation with 3D Flow Matching via KAN & RWKV](https://arxiv.org/abs/2602.01115)
*Zhihao Chen,Yiyuan Ge,Ziyang Wang*

Main category: cs.RO

TL;DR: 本文提出了一种轻量且高效的3D视觉-动作策略模型，称为KAN-We-Flow，通过新颖架构大幅减少参数，并在多个机器人控制基准上取得领先表现。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在动作分布建模上效果突出，但推理过程低效，需要大量去噪步骤和庞大网络结构，难以实用部署。现有Flow Matching方法虽减少采样步骤，却依赖大型UNet架构。作者旨在解决策略模型推理速度慢、参数量大难题。

Method: 提出KAN-We-Flow策略，结合RWKV和KAN架构。通过RWKV-KAN块先进行高效时序与通道信息混合，再用GroupKAN进行基于样条的特征非线性调整。同时加入ACR辅助损失函数，使预测动作轨迹与专家示范对齐，提升训练稳定性和方法精度。

Result: 该方法无需大型UNet，将参数量减少86.8%，运行速度快，并在Adroit、Meta-World和DexArt等多个基准上取得最优成功率。

Conclusion: KAN-We-Flow证明了轻量高效骨干网对于复杂机器人任务政策学习的可行性和优越性，显著提升实际部署潜力。

Abstract: Diffusion-based visuomotor policies excel at modeling action distributions but are inference-inefficient, since recursively denoising from noise to policy requires many steps and heavy UNet backbones, which hinders deployment on resource-constrained robots. Flow matching alleviates the sampling burden by learning a one-step vector field, yet prior implementations still inherit large UNet-style architectures. In this work, we present KAN-We-Flow, a flow-matching policy that draws on recent advances in Receptance Weighted Key Value (RWKV) and Kolmogorov-Arnold Networks (KAN) from vision to build a lightweight and highly expressive backbone for 3D manipulation. Concretely, we introduce an RWKV-KAN block: an RWKV first performs efficient time/channel mixing to propagate task context, and a subsequent GroupKAN layer applies learnable spline-based, groupwise functional mappings to perform feature-wise nonlinear calibration of the action mapping on RWKV outputs. Moreover, we introduce an Action Consistency Regularization (ACR), a lightweight auxiliary loss that enforces alignment between predicted action trajectories and expert demonstrations via Euler extrapolation, providing additional supervision to stabilize training and improve policy precision. Without resorting to large UNets, our design reduces parameters by 86.8\%, maintains fast runtime, and achieves state-of-the-art success rates on Adroit, Meta-World, and DexArt benchmarks. Our project page can be viewed in \href{https://zhihaochen-2003.github.io/KAN-We-Flow.github.io/}{\textcolor{red}{link}}

</details>


### [490] [UniForce: A Unified Latent Force Model for Robot Manipulation with Diverse Tactile Sensors](https://arxiv.org/abs/2602.01153)
*Zhuo Chen,Fei Ni,Kaiyao Luo,Zhiyuan Wu,Xuyang Zhang,Emmanouil Spyrakos-Papastavridis,Lorenzo Jamone,Nathan F. Lepora,Jiankang Deng,Shan Luo*

Main category: cs.RO

TL;DR: 本文提出UniForce，一个统一的触觉表示学习框架，实现多种不同类型触觉传感器之间的共享力空间表征，可直接应用于不同传感器的机器人操作任务，实现零样本迁移，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作对力的感知依赖特定类型的触觉传感器（如光学、磁性等），不同传感器之间在原理、材料、形态等方面差异大，导致模型、校准、数据采集都需定制，制约了泛化能力和大规模应用。

Method: UniForce通过同时建模逆动力学（图像到力）和正动力学（力到图像），结合力平衡和图像重建损失，学习传感器无关的力空间表示。数据采集采用静力平衡方法，利用传感器-物体-传感器直接接触，无需昂贵的外部力/扭矩传感器，便于跨传感器配准。

Result: 在GelSight、TacTip、uSkin等多种异构触觉传感器上的大量实验结果显示，UniForce在力估计上比以往方法有显著提升，同时支持Vision-Tactile-Language-Action模型在机器人擦拭任务中的高效跨传感器协作。

Conclusion: UniForce实现了异构触觉传感器间的统一力表征，大幅简化了数据和模型迁移流程，有助于力感知在实际机器人操作中的普及推广。

Abstract: Force sensing is essential for dexterous robot manipulation, but scaling force-aware policy learning is hindered by the heterogeneity of tactile sensors. Differences in sensing principles (e.g., optical vs. magnetic), form factors, and materials typically require sensor-specific data collection, calibration, and model training, thereby limiting generalisability. We propose UniForce, a novel unified tactile representation learning framework that learns a shared latent force space across diverse tactile sensors. UniForce reduces cross-sensor domain shift by jointly modeling inverse dynamics (image-to-force) and forward dynamics (force-to-image), constrained by force equilibrium and image reconstruction losses to produce force-grounded representations. To avoid reliance on expensive external force/torque (F/T) sensors, we exploit static equilibrium and collect force-paired data via direct sensor--object--sensor interactions, enabling cross-sensor alignment with contact force. The resulting universal tactile encoder can be plugged into downstream force-aware robot manipulation tasks with zero-shot transfer, without retraining or finetuning. Extensive experiments on heterogeneous tactile sensors including GelSight, TacTip, and uSkin, demonstrate consistent improvements in force estimation over prior methods, and enable effective cross-sensor coordination in Vision-Tactile-Language-Action (VTLA) models for a robotic wiping task. Code and datasets will be released.

</details>


### [491] [Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models](https://arxiv.org/abs/2602.01166)
*Shuanghao Bai,Jing Lyu,Wanqi Zhou,Zhe Li,Dakai Wang,Lei Xing,Xiaoguang Zhao,Pengwei Wang,Zhongyuan Wang,Cheng Chi,Badong Chen,Shanghang Zhang*

Main category: cs.RO

TL;DR: LaRA-VLA提出了一种将多模态链式推理内化到连续潜在空间的VLA新架构，实现了高效的推理与控制，并极大降低了推理延迟。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-行动（VLA）模型在推理时需要生成显式的链式推理步骤，不仅增加了计算延迟，而且离散的推理方式与机器的连续感知和控制不匹配。针对这一问题，作者希望开发一种高效且更适于实际部署的VLA推理机制。

Method: LaRA-VLA通过在训练阶段采用课程学习方式，逐步从显式链式推理监督过渡到隐式的潜在空间推理，并最终将这种推理能力用于驱动行动的生成。模型在推理和预测阶段，全部在潜在空间内完成，无需推理时生成显式的文本或视觉推理链。

Result: 在两个新构建的链式推理数据集，以及仿真和真实机器人操控任务上，LaRA-VLA均显著优于当前的VLA方法。在推理延迟上，较旨在显式链式推理的方法，推理速度提升最高可达90%。

Conclusion: 将链式推理潜在化是视觉-语言-行动任务高效、实时控制的有效新范式，为复杂机器行动任务提供了更优的推理和决策能力。

Abstract: Vision-Language-Action (VLA) models benefit from chain-of-thought (CoT) reasoning, but existing approaches incur high inference overhead and rely on discrete reasoning representations that mismatch continuous perception and control. We propose Latent Reasoning VLA (\textbf{LaRA-VLA}), a unified VLA framework that internalizes multi-modal CoT reasoning into continuous latent representations for embodied action. LaRA-VLA performs unified reasoning and prediction in latent space, eliminating explicit CoT generation at inference time and enabling efficient, action-oriented control. To realize latent embodied reasoning, we introduce a curriculum-based training paradigm that progressively transitions from explicit textual and visual CoT supervision to latent reasoning, and finally adapts latent reasoning dynamics to condition action generation. We construct two structured CoT datasets and evaluate LaRA-VLA on both simulation benchmarks and long-horizon real-robot manipulation tasks. Experimental results show that LaRA-VLA consistently outperforms state-of-the-art VLA methods while reducing inference latency by up to 90\% compared to explicit CoT-based approaches, demonstrating latent reasoning as an effective and efficient paradigm for real-time embodied control. Project Page: \href{https://loveju1y.github.io/Latent-Reasoning-VLA/}{LaRA-VLA Website}.

</details>


### [492] [SPOT: Spatio-Temporal Obstacle-free Trajectory Planning for UAVs in an Unknown Dynamic Environment](https://arxiv.org/abs/2602.01189)
*Astik Srivastava,Thomas J Chackenkulam. Bitla Bhanu Teja,Antony Thomas,Madhava Krishna*

Main category: cs.RO

TL;DR: 本论文提出了一种无需建图、依赖视觉感知的四维时空规划方法，使四旋翼无人机（UAV）能在动态未知环境中实现自主避障和路径规划。


<details>
  <summary>Details</summary>
Motivation: 现有的四旋翼避障方法通常依赖与地图构建融合，导致计算复杂、效率低下，且在面对动态未知环境和障碍物时不够灵活。因此，需要一种无需构建全局地图、反应速度快且能应对动态障碍的解决方案。

Method: 作者提出了一种结合4维（空间+时间）路径规划、基于视觉的安全飞行通道（SFC）生成与轨迹优化的方法。该方法包含一个新颖的视觉分割与目标跟踪管道，区分动态与静态元素，并集成备份规划模块，在遇到死锁或障碍时能快速做出应急反应，无需依赖全局地图。

Result: 方法在仿真和真实无人机硬件平台上进行了大量实验评估，并与多种主流方法对比，显示出在动态未知环境下实现反应式导航的明显优势。

Conclusion: 所提出的视觉感知与四维规划一体的反应式导航框架，为无人机在动态、未知环境下的自主飞行带来了更高的安全性和灵活性，有效提升了避障能力和系统鲁棒性。

Abstract: We address the problem of reactive motion planning for quadrotors operating in unknown environments with dynamic obstacles. Our approach leverages a 4-dimensional spatio-temporal planner, integrated with vision-based Safe Flight Corridor (SFC) generation and trajectory optimization. Unlike prior methods that rely on map fusion, our framework is mapless, enabling collision avoidance directly from perception while reducing computational overhead. Dynamic obstacles are detected and tracked using a vision-based object segmentation and tracking pipeline, allowing robust classification of static versus dynamic elements in the scene. To further enhance robustness, we introduce a backup planning module that reactively avoids dynamic obstacles when no direct path to the goal is available, mitigating the risk of collisions during deadlock situations. We validate our method extensively in both simulation and real-world hardware experiments, and benchmark it against state-of-the-art approaches, showing significant advantages for reactive UAV navigation in dynamic, unknown environments.

</details>


### [493] [SkySim: A ROS2-based Simulation Environment for Natural Language Control of Drone Swarms using Large Language Models](https://arxiv.org/abs/2602.01226)
*Aditya Shibu,Marah Saleh,Mohamed Al-Musleh,Nidhal Abdulaziz*

Main category: cs.RO

TL;DR: 本文提出了SkySim框架，通过将大语言模型（LLM）高层次规划与低层次安全管控解耦，实现了非专家对无人机编队的自然语言控制，兼顾智能性与飞行安全。


<details>
  <summary>Details</summary>
Motivation: 当前无人机集群在物流、农业、安防等领域应用广泛，但操作复杂、对专业知识要求高，且传统方法缺乏灵活性。大量语言模型具备高层指令理解能力，却因为缺乏物理约束导致轨迹不安全。急需一种既能提供自然语言易用性，又能保障安全和可行的新框架。

Method: 提出SkySim模拟平台：使用Gemini 3.5 Pro模型将用户自然语言指令转化为三维空间航点，并基于ROS2和Gazebo实现仿真；结合人工势场（APF）安全过滤器，对航点进行碰撞规避、运动学限制和地理围栏的安全校正；以20Hz的速率实时运行。

Result: 在3、10、30台Crazyflie无人机的集群仿真实验中，SkySim能100%准确完成几何图案空间规划，实现了实时碰撞预防与系统可扩展性。

Conclusion: SkySim框架有效地将大模型的自然语言规划能力与无人机安全管控融合，显著降低了无人机集群操作门槛，并兼顾智能、可用与安全，未来可扩展至真实硬件。

Abstract: Unmanned Aerial Vehicle (UAV) swarms offer versatile applications in logistics, agriculture, and surveillance, yet controlling them requires expert knowledge for safety and feasibility. Traditional static methods limit adaptability, while Large Language Models (LLMs) enable natural language control but generate unsafe trajectories due to lacking physical grounding. This paper introduces SkySim, a ROS2-based simulation framework in Gazebo that decouples LLM high-level planning from low-level safety enforcement. Using Gemini 3.5 Pro, SkySim translates user commands (e.g., "Form a circle") into spatial waypoints, informed by real-time drone states. An Artificial Potential Field (APF) safety filter applies minimal adjustments for collision avoidance, kinematic limits, and geo-fencing, ensuring feasible execution at 20 Hz. Experiments with swarms of 3, 10, and 30 Crazyflie drones validate spatial reasoning accuracy (100% across tested geometric primitives), real-time collision prevention, and scalability. SkySim empowers non-experts to iteratively refine behaviors, bridging AI cognition with robotic safety for dynamic environments. Future work targets hardware integration.

</details>


### [494] [Reinforcement Learning for Active Perception in Autonomous Navigation](https://arxiv.org/abs/2602.01266)
*Grzegorz Malczyk,Mihir Kulkarni,Kostas Alexis*

Main category: cs.RO

TL;DR: 该论文提出了一种用于自主导航中的主动感知的端到端强化学习方法，使机器人在避障的同时能够主动调整摄像机以提升环境感知能力。


<details>
  <summary>Details</summary>
Motivation: 在复杂、未知环境下自主导航时，单纯依赖被动视觉难以全面感知环境，可能导致导航不安全或效率低。基于此，作者希望通过主动感知策略，提高机器人在挑战性场景中的感知能力和导航鲁棒性。

Method: 论文提出一种强化学习框架，输入包括机器人自身状态、当前深度图像和最近一段深度观测构建的局部几何表示。将结合同步的避障路径规划和信息驱动的主动摄像头控制，并在奖励中引入体素化信息量指标以激励探索性感知行为。

Result: 实验结果表明，与固定摄像头系统相比，该方法可实现更安全的飞行，同时能自主产生探索性行为以提升对环境的理解。

Conclusion: 通过将主动感知融入强化学习导航系统，机器人在复杂环境下能更好地兼顾目标导向移动和环境信息获取，提高导航安全性和探索能力。

Abstract: This paper addresses the challenge of active perception within autonomous navigation in complex, unknown environments. Revisiting the foundational principles of active perception, we introduce an end-to-end reinforcement learning framework in which a robot must not only reach a goal while avoiding obstacles, but also actively control its onboard camera to enhance situational awareness. The policy receives observations comprising the robot state, the current depth frame, and a particularly local geometry representation built from a short history of depth readings. To couple collision-free motion planning with information-driven active camera control, we augment the navigation reward with a voxel-based information metric. This enables an aerial robot to learn a robust policy that balances goal-directed motion with exploratory sensing. Extensive evaluation demonstrates that our strategy achieves safer flight compared to using fixed, non-actuated camera baselines while also inducing intrinsic exploratory behaviors.

</details>


### [495] [TriphiBot: A Triphibious Robot Combining FOC-based Propulsion with Eccentric Design](https://arxiv.org/abs/2602.01385)
*Xiangyu Li,Mingwei Lai,Mengke Zhang,Junxiao Lin,Tiancheng Lai,Junping Zhi,Chao Xu,Fei Gao,Yanjun Cao*

Main category: cs.RO

TL;DR: 本文提出了一种新型三栖机器人，能在空中、陆地和水中运动，且结构简约，无需额外执行器，同时克服了常见三模态机器人效率低或结构复杂的问题。


<details>
  <summary>Details</summary>
Motivation: 目前多数机器人只能在两种环境下运动，且现有三栖设计要么结构复杂、要么推进效率低，限制了实际应用。本文旨在探索更高效、结构更简约的三栖机器人设计。

Method: 该机器人基于四旋翼加两个被动轮的极简结构，通过偏心重心设计满足高效地面/水下推进，无需复杂机械变换。采用基于磁场定向控制（FOC）的统一推进系统，解决空气和水中转矩匹配问题，并实现高精度、双向推进。控制方面，提出了混合非线性模型预测控制- PID系统，保证全域稳定运动和无缝转换。

Result: 实验结果表明，该机器人能有效实现空-陆-水多环境运动及顺畅转变，推进系统具有较高效率和自适应能力。

Conclusion: 本文机器人方案在极简结构下实现了真正意义上的多域（三栖）运动和模式切换，推进及控制策略提升了效率和稳定性，对多环境自主机器人发展具有推动意义。

Abstract: Triphibious robots capable of multi-domain motion and cross-domain transitions are promising to handle complex tasks across diverse environments. However, existing designs primarily focus on dual-mode platforms, and some designs suffer from high mechanical complexity or low propulsion efficiency, which limits their application. In this paper, we propose a novel triphibious robot capable of aerial, terrestrial, and aquatic motion, by a minimalist design combining a quadcopter structure with two passive wheels, without extra actuators. To address inefficiency of ground-support motion (moving on land/seabed) for quadcopter based designs, we introduce an eccentric Center of Gravity (CoG) design that inherently aligns thrust with motion, enhancing efficiency without specialized mechanical transformation designs. Furthermore, to address the drastic differences in motion control caused by different fluids (air and water), we develop a unified propulsion system based on Field-Oriented Control (FOC). This method resolves torque matching issues and enables precise, rapid bidirectional thrust across different mediums. Grounded in the perspective of living condition and ground support, we analyse the robot's dynamics and propose a Hybrid Nonlinear Model Predictive Control (HNMPC)-PID control system to ensure stable multi-domain motion and seamless transitions. Experimental results validate the robot's multi-domain motion and cross-mode transition capability, along with the efficiency and adaptability of the proposed propulsion system.

</details>


### [496] [Instance-Guided Unsupervised Domain Adaptation for Robotic Semantic Segmentation](https://arxiv.org/abs/2602.01389)
*Michele Antonazzi,Lorenzo Signorelli,Matteo Luperto,Nicola Basilico*

Main category: cs.RO

TL;DR: 本文提出了一种结合3D地图和基础模型的无监督领域自适应（UDA）方法，有效提升机器人在实际部署环境中的语义分割性能，无需目标域标注。


<details>
  <summary>Details</summary>
Motivation: 语义分割模型通常在新环境（目标域）中性能下降，特别是训练数据（源域）与实际应用环境视觉分布不一致时。这种域偏移限制了机器人感知系统的实用性，需要无监督的方法利用机器人在环境中收集的大量数据进行自适应。

Method: 该方法基于机器人收集的三维体素地图生成多视角一致的伪标签，然后使用基础模型（如大模型）的零样本实例分割能力，进一步细化伪标签以保证实例级一致性。最终利用这些高质量伪标签对感知模型进行自监督微调，实现部署时的无监督自适应。

Result: 在真实环境数据上的实验表明，本方法在基于多视角一致性的最先进UDA基线方法上取得了更好的性能提升，且无需任何目标域的真实标签。

Conclusion: 结合3D地图建图与基础模型实例分割的新方法，不仅解决了多视角UDA中实例不一致的问题，还提高了目标域语义分割效果，有助于机器人感知系统实际应用。

Abstract: Semantic segmentation networks, which are essential for robotic perception, often suffer from performance degradation when the visual distribution of the deployment environment differs from that of the source dataset on which they were trained. Unsupervised Domain Adaptation (UDA) addresses this challenge by adapting the network to the robot's target environment without external supervision, leveraging the large amounts of data a robot might naturally collect during long-term operation. In such settings, UDA methods can exploit multi-view consistency across the environment's map to fine-tune the model in an unsupervised fashion and mitigate domain shift. However, these approaches remain sensitive to cross-view instance-level inconsistencies. In this work, we propose a method that starts from a volumetric 3D map to generate multi-view consistent pseudo-labels. We then refine these labels using the zero-shot instance segmentation capabilities of a foundation model, enforcing instance-level coherence. The refined annotations serve as supervision for self-supervised fine-tuning, enabling the robot to adapt its perception system at deployment time. Experiments on real-world data demonstrate that our approach consistently improves performance over state-of-the-art UDA baselines based on multi-view consistency, without requiring any ground-truth labels in the target domain.

</details>


### [497] [Sem-NaVAE: Semantically-Guided Outdoor Mapless Navigation via Generative Trajectory Priors](https://arxiv.org/abs/2602.01429)
*Gonzalo Olguin,Javier Ruiz-del-Solar*

Main category: cs.RO

TL;DR: 本论文提出了一种无需地图的全局导航方法，结合CVAEs生成轨迹和轻量级视觉语言模型（VLM）的语义分割能力，实现了在户外环境下的高效导航，并且在真实实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的导航方法依赖详细地图，然而在许多实际场景下获取和更新地图十分困难，限制了导航系统的应用范围。因此，开发无需地图也能可靠工作的导航方案具有重要意义。

Method: 方法结合了条件变分自编码器（CVAE）用于大量生成多样性轨迹，通过轻量级视觉语言模型（VLM）进行语义分割，在自然语言指导下按开放词汇评估、选择最优轨迹，最后利用现有的局部规划器执行速度控制指令，实现实时导航。

Result: 所提方法在真实户外环境进行了验证，实验结果显示导航成功率和效率优于主流先进方法，能够实时生成和选择多样轨迹。

Conclusion: 该方法无需依赖地图，能够高效地在户外环境中自主导航，具有良好的泛化和实际应用前景。

Abstract: This work presents a mapless global navigation approach for outdoor applications. It combines the exploratory capacity of conditional variational autoencoders (CVAEs) to generate trajectories and the semantic segmentation capabilities of a lightweight visual language model (VLM) to select the trajectory to execute. Open-vocabulary segmentation is used to score and select the generated trajectories based on natural language, and a state-of-the-art local planner executes velocity commands. One of the key features of the proposed approach is its ability to generate a large variability of trajectories and to select them and navigate in real-time. The approach was validated through real-world outdoor navigation experiments, achieving superior performance compared to state-of-the-art methods. A video showing an experimental run of the system can be found in https://www.youtube.com/watch?v=i3R5ey5O2yk.

</details>


### [498] [Towards a Novel Wearable Robotic Vest for Hemorrhage Suppression](https://arxiv.org/abs/2602.01448)
*Harshith Jella,Pejman Kheradmand,Joseph Klein,Behnam Moradkhani,Yash Chitalia*

Main category: cs.RO

TL;DR: 本文介绍了一种新型机器人系统，能够在包括太空站等特殊环境的紧急情况下处理大出血问题。该系统采用可变形环结构，通过气囊和充气环提供均匀压力，有效止血。实验验证了系统的力学性能和止血能力。


<details>
  <summary>Details</summary>
Motivation: 传统止血措施如止血带在某些特殊环境（如太空站）或解剖部位（如腹部、背部、颈部等）难以有效应用，因此亟需一种更具适应性的止血技术。

Method: 本研究设计了一种能从圆形变为椭圆形的可变形环机制，并配合不同柔韧性的环臂和充气气囊，提升其对身体不同部位的适应性。通过实验测试了不同环臂的弯曲刚度，以及气囊系统的压力输出，最后在模拟伤员装置上评估了止血效果。

Result: 测试表明，不同配置的系统均能提供有效且均匀的压力，具备一定止血能力。但设备在覆盖面积和对复杂解剖部位的适应性方面存在一定局限。

Conclusion: 该机器人系统能在模拟环境下成功控制大出血，显示出在极端或特殊环境中应用的潜力，但仍需进一步优化以提升其对复杂部位的适应性和覆盖能力。

Abstract: This paper introduces a novel robotic system designed to manage severe bleeding in emergency scenarios, including unique environments like space stations. The robot features a shape-adjustable "ring mechanism", transitioning from a circular to an elliptical configuration to adjust wound coverage across various anatomical regions. We developed various arms for this ring mechanism with varying flexibilities to improve adaptability when applied to non-extremities of the body (abdomen, back, neck, etc.). To apply equal and constant pressure across the wound, we developed an inflatable ring and airbag balloon that are compatible with this shape-changing ring mechanism. A series of experiments focused on evaluating various ring arm configurations to characterize their bending stiffness. Subsequent experiments measured the force exerted by the airbag balloon system using a digital scale. Despite its promising performance, certain limitations related to coverage area are identified. The shape-changing effect of the device is limited to scenarios involving partially inflated or deflated airbag balloons, and cannot fully conform to complex anatomical regions. Finally, the device was tested on casualty simulation kits, where it successfully demonstrated its ability to control simulated bleeding.

</details>


### [499] [TreeLoc: 6-DoF LiDAR Global Localization in Forests via Inter-Tree Geometric Matching](https://arxiv.org/abs/2602.01501)
*Minwoo Jung,Nived Chebrolu,Lucas Carvalho de Lima,Haedam Oh,Maurice Fallon,Ayoung Kim*

Main category: cs.RO

TL;DR: 论文提出了TreeLoc，一种基于激光雷达的森林场景定位方法，能够在GPS信号差、结构复杂的森林环境下实现高精度的全局定位。


<details>
  <summary>Details</summary>
Motivation: 现有的定位方法主要针对城市环境，假设有独特的结构特征用于识别，但这些假设并不适用于特征重复、遮挡和结构复杂的森林。需要专门面向森林环境的定位方法来提升鲁棒性和精度。

Method: TreeLoc 以树干及其胸径(DBH)为特征，通过其轴线对齐实现场景标准化。先用树分布直方图进行粗匹配，然后利用2D三角形描述子进行精细匹配，最后结合两步几何验证实现6自由度姿态估计。算法经过消融实验逐步验证各模块贡献。

Result: 在多个森林数据集测试中，TreeLoc 明显优于现有基线方法，实验证明其定位精度高。此外，消融实验展示了各部分算法的有效性。

Conclusion: TreeLoc 为森林环境下的机器人定位带来了鲁棒、实用的新方案。结合紧凑的全局树数据库，该方法在长期森林管理等应用中具有广阔前景，并已开源以支持学术与工程社区。

Abstract: Reliable localization is crucial for navigation in forests, where GPS is often degraded and LiDAR measurements are repetitive, occluded, and structurally complex. These conditions weaken the assumptions of traditional urban-centric localization methods, which assume that consistent features arise from unique structural patterns, necessitating forest-centric solutions to achieve robustness in these environments. To address these challenges, we propose TreeLoc, a LiDAR-based global localization framework for forests that handles place recognition and 6-DoF pose estimation. We represent scenes using tree stems and their Diameter at Breast Height (DBH), which are aligned to a common reference frame via their axes and summarized using the tree distribution histogram (TDH) for coarse matching, followed by fine matching with a 2D triangle descriptor. Finally, pose estimation is achieved through a two-step geometric verification. On diverse forest benchmarks, TreeLoc outperforms baselines, achieving precise localization. Ablation studies validate the contribution of each component. We also propose applications for long-term forest management using descriptors from a compact global tree database. TreeLoc is open-sourced for the robotics community at https://github.com/minwoo0611/TreeLoc.

</details>


### [500] [RAPT: Model-Predictive Out-of-Distribution Detection and Failure Diagnosis for Sim-to-Real Humanoid Robots](https://arxiv.org/abs/2602.01515)
*Humphrey Munn,Brendan Tidd,Peter Bohm,Marcus Gallagher,David Howard*

Main category: cs.RO

TL;DR: 本文提出了RAPT，一个自监督、轻量级的部署时监控器，能高效监测和解释类人机器人在仿真到现实（Sim-to-Real）迁移过程中的异常行为，并支持高频（50Hz）控制。RAPT不仅提升了异常检测的准确率，还能持续量化部署与训练的偏差，并自动分析故障根源。


<details>
  <summary>Details</summary>
Motivation: 现有类人机器人控制策略在从仿真到现实迁移时，易导致‘沉默式’失效，造成硬件风险。传统异常检测方法不适用于高频控制，对极低误报要求下性能较差，且大多为‘黑箱’方法，缺乏可解释性。因此亟需高效、可解释的检测与故障分析工具。

Method: 提出RAPT，通过自监督学习，建模仿真中的空间-时间概率流形，并在部署时对每维信号进行校准的预测偏差检测，实现在线OOD（分布外）检测与漂移度量。还结合基于梯度的时序显著性分析和LLM推理，实现自动故障根因诊断。

Result: 在大规模仿真中，RAPT在固定0.5%集数误报率下将TPR提升了37%，实物部署中TPR提升12.5%。RAPT还实现了对16次真实故障的75%根因识别准确率，且仅用本体传感数据。

Conclusion: RAPT能高效、可靠、可解释地监控类人机器人控制策略部署，显著提升异常检测率，并为实际机器人应用提供根因诊断和持续性能追踪，对安全部署具有重要意义。

Abstract: Deploying learned control policies on humanoid robots is challenging: policies that appear robust in simulation can execute confidently in out-of-distribution (OOD) states after Sim-to-Real transfer, leading to silent failures that risk hardware damage. Although anomaly detection can mitigate these failures, prior methods are often incompatible with high-rate control, poorly calibrated at the extremely low false-positive rates required for practical deployment, or operate as black boxes that provide a binary stop signal without explaining why the robot drifted from nominal behavior. We present RAPT, a lightweight, self-supervised deployment-time monitor for 50Hz humanoid control. RAPT learns a probabilistic spatio-temporal manifold of nominal execution from simulation and evaluates execution-time predictive deviation as a calibrated, per-dimension signal. This yields (i) reliable online OOD detection under strict false-positive constraints and (ii) a continuous, interpretable measure of Sim-to-Real mismatch that can be tracked over time to quantify how far deployment has drifted from training. Beyond detection, we introduce an automated post-hoc root-cause analysis pipeline that combines gradient-based temporal saliency derived from RAPT's reconstruction objective with LLM-based reasoning conditioned on saliency and joint kinematics to produce semantic failure diagnoses in a zero-shot setting. We evaluate RAPT on a Unitree G1 humanoid across four complex tasks in simulation and on physical hardware. In large-scale simulation, RAPT improves True Positive Rate (TPR) by 37% over the strongest baseline at a fixed episode-level false positive rate of 0.5%. On real-world deployments, RAPT achieves a 12.5% TPR improvement and provides actionable interpretability, reaching 75% root-cause classification accuracy across 16 real-world failures using only proprioceptive data.

</details>


### [501] [Co-Design of Rover Wheels and Control using Bayesian Optimization and Rover-Terrain Simulations](https://arxiv.org/abs/2602.01535)
*Huzaifa Mustafa Unjhawala,Khizar Shaikh,Luning Bakke,Radu Serban,Dan Negrut*

Main category: cs.RO

TL;DR: 本文提出了一种基于高保真、全车闭环仿真的贝叶斯优化框架，用于在可变形地形上协同优化越野移动机器人的轮子设计和控制器参数。采用连续介质模型显著提升了仿真效率，支持在复杂轨迹上进行多目标优化，实现了理想性能与计算成本的平衡，并首次实现仿真方案的开源。


<details>
  <summary>Details</summary>
Motivation: 传统离散元仿真（DEM）计算成本高昂，难以应用于全车仿真的情形，限制了机械与控制协同优化的研究与实际应用，因此需要高效可扩展的新方法。

Method: 基于连续介质建模（CRM），构建高保真的全车级闭环仿真平台，联合贝叶斯优化算法协同调整轮子参数（如半径、宽度、凸起形状）和转向PID控制参数，通过多目标函数（速度、跟踪误差、能耗）引导设计。比较了轮子与控制器同步与分步优化的两种策略，并在大量仿真中评估其性能和计算消耗。

Result: 在3,000组全车级仿真中，优化流程仅需5-9天完成，比原本DEM方案节省数月时间。实物初步实验验证了仿真优化趋势的合理性。

Conclusion: 新方案显著提升了越野车辆在可变形地形仿真中的设计优化效率，使得轮子与控制器协同优化变得可行，并降低了成本。开源的软件基础设施为后续研究提供了支持。

Abstract: While simulation is vital for optimizing robotic systems, the cost of modeling deformable terrain has long limited its use in full-vehicle studies of off-road autonomous mobility. For example, Discrete Element Method (DEM) simulations are often confined to single-wheel tests, which obscures coupled wheel-vehicle-controller interactions and prevents joint optimization of mechanical design and control. This paper presents a Bayesian optimization framework that co-designs rover wheel geometry and steering controller parameters using high-fidelity, full-vehicle closed-loop simulations on deformable terrain. Using the efficiency and scalability of a continuum-representation model (CRM) for terramechanics, we evaluate candidate designs on trajectories of varying complexity while towing a fixed load. The optimizer tunes wheel parameters (radius, width, and grouser features) and steering PID gains under a multi-objective formulation that balances traversal speed, tracking error, and energy consumption. We compare two strategies: simultaneous co-optimization of wheel and controller parameters versus a sequential approach that decouples mechanical and control design. We analyze trade-offs in performance and computational cost. Across 3,000 full-vehicle simulations, campaigns finish in five to nine days, versus months with the group's earlier DEM-based workflow. Finally, a preliminary hardware study suggests the simulation-optimized wheel designs preserve relative performance trends on the physical rover. Together, these results show that scalable, high-fidelity simulation can enable practical co-optimization of wheel design and control for off-road vehicles on deformable terrain without relying on prohibitively expensive DEM studies. The simulation infrastructure (scripts and models) is released as open source in a public repository to support reproducibility and further research.

</details>


### [502] [UniDWM: Towards a Unified Driving World Model via Multifaceted Representation Learning](https://arxiv.org/abs/2602.01536)
*Shuai Liu,Siheng Ren,Xiaoyao Zhu,Quanmin Liang,Zefeng Li,Qiang Li,Xin Hu,Kai Huang*

Main category: cs.RO

TL;DR: 本文提出了一种统一的驾驶世界模型（UniDWM），通过多维度表示学习提升自动驾驶系统的可靠性和高效性，并在轨迹规划、4D重建及生成任务上取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 在复杂的驾驶环境中，规划系统需要综合理解场景的几何结构、外观和动态变化。目前缺乏能够在感知、预测、决策等多个任务中一致地对场景进行建模与推理的统一模型。

Method: 作者提出了UniDWM模型，该模型通过联合重建路径学习场景结构（包括几何与视觉纹理），同时使用条件扩散Transformer进行动态世界的预测。此外，模型引入了结构和动态感知的潜在空间作为物理基础的状态空间，统一感知、预测和规划任务。理论上，UniDWM被解释为VAE的变体，支持其多维表示学习。

Result: UniDWM在轨迹规划、4D场景重建和生成等任务上进行了大量实验，结果显示该模型在多项任务上均表现出色，优于现有方法。

Conclusion: 多维度世界表示为实现统一自动驾驶智能模型提供了坚实基础，UniDWM在多个关键任务上验证了该理念的有效性和前景。

Abstract: Achieving reliable and efficient planning in complex driving environments requires a model that can reason over the scene's geometry, appearance, and dynamics. We present UniDWM, a unified driving world model that advances autonomous driving through multifaceted representation learning. UniDWM constructs a structure- and dynamic-aware latent world representation that serves as a physically grounded state space, enabling consistent reasoning across perception, prediction, and planning. Specifically, a joint reconstruction pathway learns to recover the scene's structure, including geometry and visual texture, while a collaborative generation framework leverages a conditional diffusion transformer to forecast future world evolution within the latent space. Furthermore, we show that our UniDWM can be deemed as a variation of VAE, which provides theoretical guidance for the multifaceted representation learning. Extensive experiments demonstrate the effectiveness of UniDWM in trajectory planning, 4D reconstruction and generation, highlighting the potential of multifaceted world representations as a foundation for unified driving intelligence. The code will be publicly available at https://github.com/Say2L/UniDWM.

</details>


### [503] [A Closed-Form Geometric Retargeting Solver for Upper Body Humanoid Robot Teleoperation](https://arxiv.org/abs/2602.01632)
*Chuizheng Kong,Yunho Cho,Wonsuhk Jung,Idris Wibowo,Parth Shinde,Sundhar Vinodh-Sangeetha,Long Kiu Chung,Zhenyang Chen,Andrew Mattei,Advaith Nidumukkala,Alexander Elias,Danfei Xu,Taylor Higgins,Shreyas Kousik*

Main category: cs.RO

TL;DR: 该论文提出了一种新的人体动作重定向方法SEW-Mimic，将其表述为方向对齐问题，并提出了一个闭式几何解算法。相比于传统方法，该方法更快更准确，适用于大多数7自由度机械臂和人形机器人，提升了远程操作任务的成功率，并对策略学习有帮助。


<details>
  <summary>Details</summary>
Motivation: 现有的人体动作重定向方法存在效率低、动作延迟或不自然等问题，而且只能将人类手部动作直接映射到机器人末端执行器，限制了机器人工作空间。为了解决这些问题，作者提出新方法。

Method: 作者将动作重定向问题重新定义为上、下臂方向对齐问题，通过肩、肘、腕（SEW）关键点对机器人各关节方向进行几何对齐，开发了一个高速（3kHz）闭式解法，无需依赖关键点来源，适用于多种机器人。

Result: 实验表明，SEW-Mimic方法在精度和计算速度上均优于现有方法。先导用户测试显示该方法提升了远程操作任务的完成率。用该方法收集的数据更平滑，有利于机器人策略学习。此外，方法可以作为提高全身重定向速度的即插即用模块。硬件实验也证明其实用性。

Conclusion: SEW-Mimic是实现双臂机器人操作和人形机器人远程操控的基础技术模块，兼具高效、准确和广泛适应性，对相关领域具有重要实用价值。

Abstract: Retargeting human motion to robot poses is a practical approach for teleoperating bimanual humanoid robot arms, but existing methods can be suboptimal and slow, often causing undesirable motion or latency. This is due to optimizing to match robot end-effector to human hand position and orientation, which can also limit the robot's workspace to that of the human. Instead, this paper reframes retargeting as an orientation alignment problem, enabling a closed-form, geometric solution algorithm with an optimality guarantee. The key idea is to align a robot arm to a human's upper and lower arm orientations, as identified from shoulder, elbow, and wrist (SEW) keypoints; hence, the method is called SEW-Mimic. The method has fast inference (3 kHz) on standard commercial CPUs, leaving computational overhead for downstream applications; an example in this paper is a safety filter to avoid bimanual self-collision. The method suits most 7-degree-of-freedom robot arms and humanoids, and is agnostic to input keypoint source. Experiments show that SEW-Mimic outperforms other retargeting methods in computation time and accuracy. A pilot user study suggests that the method improves teleoperation task success. Preliminary analysis indicates that data collected with SEW-Mimic improves policy learning due to being smoother. SEW-Mimic is also shown to be a drop-in way to accelerate full-body humanoid retargeting. Finally, hardware demonstrations illustrate SEW-Mimic's practicality. The results emphasize the utility of SEW-Mimic as a fundamental building block for bimanual robot manipulation and humanoid robot teleoperation.

</details>


### [504] [AgenticLab: A Real-World Robot Agent Platform that Can See, Think, and Act](https://arxiv.org/abs/2602.01662)
*Pengyuan Guo,Zhonghao Mai,Zhengtong Xu,Kaidi Zhang,Heng Zhang,Zichen Miao,Arash Ajoudani,Zachary Kingston,Qiang Qiu,Yu She*

Main category: cs.RO

TL;DR: 论文提出了AgenticLab，这是一个用于开放世界机器人操作的模型无关型平台和基准，用于实物机器人在非结构化环境中评估最新视觉-语言模型（VLM）代理的通用操作能力。


<details>
  <summary>Details</summary>
Motivation: 虽然大型视觉-语言模型在感知和推理上取得了进展，但其在真实机器人长时程、闭环、开放世界操作中能力尚未明确，且现有评测方法不可比、依赖模拟和特殊环境，缺乏通用、可复现的评测平台。

Method: 构建了AgenticLab平台，包括感知、任务分解、在线验证和重规划的闭环流程，并在真实非结构化环境下，使用该平台系统性评测了多种先进VLM代理的操作能力。

Result: 基准测试揭示了离线视觉-语言测试无法反映的多种失败模式，如多步一致性崩溃、物体遮挡/场景变化下的定位失败，以及空间推理不足导致操作不可靠。

Conclusion: AgenticLab能够支持标准化、可复现的机器人通用智能评测，将开源全套软硬件，有助于加速通用机器人代理领域的研究和发展。

Abstract: Recent advances in large vision-language models (VLMs) have demonstrated generalizable open-vocabulary perception and reasoning, yet their real-robot manipulation capability remains unclear for long-horizon, closed-loop execution in unstructured, in-the-wild environments. Prior VLM-based manipulation pipelines are difficult to compare across different research groups' setups, and many evaluations rely on simulation, privileged state, or specially designed setups. We present AgenticLab, a model-agnostic robot agent platform and benchmark for open-world manipulation. AgenticLab provides a closed-loop agent pipeline for perception, task decomposition, online verification, and replanning. Using AgenticLab, we benchmark state-of-the-art VLM-based agents on real-robot tasks in unstructured environments. Our benchmark reveals several failure modes that offline vision-language tests (e.g., VQA and static image understanding) fail to capture, including breakdowns in multi-step grounding consistency, object grounding under occlusion and scene changes, and insufficient spatial reasoning for reliable manipulation. We will release the full hardware and software stack to support reproducible evaluation and accelerate research on general-purpose robot agents.

</details>


### [505] [Towards Autonomous Instrument Tray Assembly for Sterile Processing Applications](https://arxiv.org/abs/2602.01679)
*Raghavasimhan Sankaranarayanan,Paul Stuart,Nicholas Ahn,Arno Sungarian,Yash Chitalia*

Main category: cs.RO

TL;DR: 本文提出了一套全自动机器人系统，实现外科手术器械的自动分拣与规范装盘，重点解决SPD部门人工组盘的低效和高差错问题。该系统显著提升了组盘精准度并减少器械碰撞。


<details>
  <summary>Details</summary>
Motivation: 当前手术器械的人工组盘耗时且易出错，存在交叉污染和器械损坏风险，迫切需要自动化、准确、高效的解决方案以提升手术准备的安全性和可靠性。

Method: 作者部署了一套融合YOLO12检测器和级联ResNet分类器的混合感知算法，辅以标注自定义数据集实现器械识别。全系统包括校准视觉模块、6自由度工业机械臂、定制双电磁吸持器和3D打印隔板装置，并通过规则驱动的物理隔离装盘算法减少运输中的器械碰撞。

Result: 实验结果表明机器人系统在器械识别和归类上具有高准确率，组盘过程中器械间的碰撞次数明显低于人工组盘，且差异具有统计学意义。

Conclusion: 该系统为SPD自动化流程提供了可扩展的技术路径，初步实现了事务自动化，有助于提升手术准备的一致性、安全性并缩短处理时间。

Abstract: The Sterile Processing and Distribution (SPD) department is responsible for cleaning, disinfecting, inspecting, and assembling surgical instruments between surgeries. Manual inspection and preparation of instrument trays is a time-consuming, error-prone task, often prone to contamination and instrument breakage. In this work, we present a fully automated robotic system that sorts and structurally packs surgical instruments into sterile trays, focusing on automation of the SPD assembly stage. A custom dataset comprising 31 surgical instruments and 6,975 annotated images was collected to train a hybrid perception pipeline using YOLO12 for detection and a cascaded ResNet-based model for fine-grained classification. The system integrates a calibrated vision module, a 6-DOF Staubli TX2-60L robotic arm with a custom dual electromagnetic gripper, and a rule-based packing algorithm that reduces instrument collisions during transport. The packing framework uses 3D printed dividers and holders to physically isolate instruments, reducing collision and friction during transport. Experimental evaluations show high perception accuracy and statistically significant reduction in tool-to-tool collisions compared to human-assembled trays. This work serves as the scalable first step toward automating SPD workflows, improving safety, and consistency of surgical preparation while reducing SPD processing times.

</details>


### [506] [GSR: Learning Structured Reasoning for Embodied Manipulation](https://arxiv.org/abs/2602.01693)
*Kewei Hu,Michael Zhang,Wei Ying,Tianhao Liu,Guoqiang Hao,Zimeng Li,Wanchan Yu,Jiajian Jing,Fangwen Chen,Hanwen Kang*

Main category: cs.RO

TL;DR: 本文提出了一种名为GSR（Grounded Scene-graph Reasoning）的新方法，通过显式建模场景图来提升机器人在复杂、长时序任务中的推理和操作能力，并通过大规模数据集和多项实验验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作方法在处理需要维持空间一致性、因果依赖及目标约束的长时序任务时表现不足，主要受限于任务推理隐含于高维表征，难以区分任务结构与感知变化。作者旨在解决任务结构与感知信息混淆的问题。

Method: 提出GSR推理范式，将环境状态建模为语义场景图，并在其上逐步推理对象状态与空间关系，显式分析动作的前置条件、后果及目标达成情况。此外，构建了Manip-Cognition-1.6M大规模数据集，联合监督对世界理解、动作规划和目标解释的学习。

Result: 在RLBench、LIBERO、GSR-benchmark以及真实机器人任务上的广泛评测显示，GSR方法在零样本泛化能力和长时序任务完成率上明显优于现有提示式方法。

Conclusion: 明确的世界状态表示（如场景图）为可扩展化机器人推理提供了关键归纳偏置，GSR方法可显著推动机器人长时序任务的推理与泛化能力。

Abstract: Despite rapid progress, embodied agents still struggle with long-horizon manipulation that requires maintaining spatial consistency, causal dependencies, and goal constraints. A key limitation of existing approaches is that task reasoning is implicitly embedded in high-dimensional latent representations, making it challenging to separate task structure from perceptual variability. We introduce Grounded Scene-graph Reasoning (GSR), a structured reasoning paradigm that explicitly models world-state evolution as transitions over semantically grounded scene graphs. By reasoning step-wise over object states and spatial relations, rather than directly mapping perception to actions, GSR enables explicit reasoning about action preconditions, consequences, and goal satisfaction in a physically grounded space. To support learning such reasoning, we construct Manip-Cognition-1.6M, a large-scale dataset that jointly supervises world understanding, action planning, and goal interpretation. Extensive evaluations across RLBench, LIBERO, GSR-benchmark, and real-world robotic tasks show that GSR significantly improves zero-shot generalization and long-horizon task completion over prompting-based baselines. These results highlight explicit world-state representations as a key inductive bias for scalable embodied reasoning.

</details>


### [507] [Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels](https://arxiv.org/abs/2602.01700)
*Ruoyu Wang,Xuchen Liu,Zongzhou Wu,Zixuan Guo,Wendi Ding,Ben M. Chen*

Main category: cs.RO

TL;DR: 本文提出了Tilt-Ropter，这是一种结合倾转旋翼和被动轮子的全新混合空地两用无人机，实现了高能效的多模式移动。采用全驱动设计和先进控制方法，显著提高了机动性与环境适应性。


<details>
  <summary>Details</summary>
Motivation: 当前的混合空地无人机大多为欠驱动结构，导致移动和环境适应能力有限，且在地面模式下能耗较高。需要一种能够提高机动性、能效和环境适应性的无人机方案，适用于长时任务和能源受限环境。

Method: 设计了一种全驱动Tilt-Ropter无人机，结合了倾转旋翼和被动轮子；开发了非线性模型预测控制器（NMPC）及控制分配模块，实现运动模式间的轨迹跟踪和能效优化，并提出了实时外力估计算法以提升地面接触的鲁棒性。

Result: 通过仿真和实物实验验证了系统，包括空地转换和轨迹跟踪任务。实验结果表明在飞行和地面模式下都有低跟踪误差，地面移动模式下动力消耗减少92.8%。

Conclusion: Tilt-Ropter在多模式高能效移动中表现优异，极大提升了续航能力和环境适应性，适用于大规模、能量受限的长期任务。

Abstract: In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system's potential for long-duration missions across large-scale and energy-constrained environments.

</details>


### [508] [Uncertainty-Aware Non-Prehensile Manipulation with Mobile Manipulators under Object-Induced Occlusion](https://arxiv.org/abs/2602.01731)
*Jiwoo Hwang,Taegeun Yang,Jeil Jeong,Minsung Yoon,Sung-Eui Yoon*

Main category: cs.RO

TL;DR: 该论文提出了CURA-PPO强化学习框架，通过建模部分可观测条件下的碰撞不确定性，实现了在传感器被遮挡场景下更安全有效的非抓取操作。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人非抓取操作受限于传感器的视野遮挡，导致 occlusion 区域容易发生碰撞，缺乏有效的方法处理遮挡下的不确定性，难以安全导航与操作。

Method: 作者提出CURA-PPO框架，在增强学习中显式建模对碰撞风险的分布预测，结合风险与不确定度引导机器人动作，同时利用置信度地图表征观测的可靠性，鼓励机器人主动感知、边操作边收集信息解决遮挡问题。

Result: 在不同物体尺寸与障碍布置的实验中，CURA-PPO比基线方法最高提升3倍成功率，并展现出更好处理遮挡的智能行为。

Conclusion: CURA-PPO为仅依赖机载感知的自主机器人在复杂遮挡环境下的安全操作提供了实效性方法，推动了部分可观测条件下智能操作的发展。

Abstract: Non-prehensile manipulation using onboard sensing presents a fundamental challenge: the manipulated object occludes the sensor's field of view, creating occluded regions that can lead to collisions. We propose CURA-PPO, a reinforcement learning framework that addresses this challenge by explicitly modeling uncertainty under partial observability. By predicting collision possibility as a distribution, we extract both risk and uncertainty to guide the robot's actions. The uncertainty term encourages active perception, enabling simultaneous manipulation and information gathering to resolve occlusions. When combined with confidence maps that capture observation reliability, our approach enables safe navigation despite severe sensor occlusion. Extensive experiments across varying object sizes and obstacle configurations demonstrate that CURA-PPO achieves up to 3X higher success rates than the baselines, with learned behaviors that handle occlusions. Our method provides a practical solution for autonomous manipulation in cluttered environments using only onboard sensing.

</details>


### [509] [RFS: Reinforcement learning with Residual flow steering for dexterous manipulation](https://arxiv.org/abs/2602.01789)
*Entong Su,Tyler Westenbroek,Anusha Nagabandi,Abhishek Gupta*

Main category: cs.RO

TL;DR: 该论文提出了一种名为Residual Flow Steering (RFS)的新方法，高效地对预训练生成式策略进行微调，从而提升模仿学习在灵巧操作任务中的泛化与适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型或流匹配的行为克隆方法虽能表达多模态动作分布，但预训练策略泛化有限且需在部署时额外微调，面临局部修正效率低和需保持全局探索能力的挑战。

Method: RFS通过联合优化残差动作和潜在噪声分布，在预训练流匹配策略上进行残差指导，实现局部残差修正与潜空间调制的全局探索协同，既保持原策略的结构优势，又提升微调速度和数据效率。

Result: RFS在模拟和真实灵巧操作任务上均能高效微调预训练基政策，展现了良好的适应性和提升效果。

Conclusion: RFS框架有效提升了生成式模仿学习策略的微调效率和泛化性能，在复杂机器人操作领域具有广泛应用前景。

Abstract: Imitation learning has emerged as an effective approach for bootstrapping sequential decision-making in robotics, achieving strong performance even in high-dimensional dexterous manipulation tasks. Recent behavior cloning methods further leverage expressive generative models, such as diffusion models and flow matching, to represent multimodal action distributions. However, policies pretrained in this manner often exhibit limited generalization and require additional fine-tuning to achieve robust performance at deployment time. Such adaptation must preserve the global exploration benefits of pretraining while enabling rapid correction of local execution errors.We propose \emph{Residual Flow Steering} (RFS), a data-efficient reinforcement learning framework for adapting pretrained generative policies. RFS steers a pretrained flow-matching policy by jointly optimizing a residual action and a latent noise distribution, enabling complementary forms of exploration: local refinement through residual corrections and global exploration through latent-space modulation. This design allows efficient adaptation while retaining the expressive structure of the pretrained policy.We demonstrate the effectiveness of RFS on dexterous manipulation tasks, showing efficient fine-tuning both in simulation and in real-world settings when adapting pretrained base policies.Project website:https://weirdlabuw.github.io/rfs.

</details>


### [510] [From Knowing to Doing Precisely: A General Self-Correction and Termination Framework for VLA models](https://arxiv.org/abs/2602.01811)
*Wentao Zhang,Aolan Sun,Wentao Mo,Xiaoyang Qu,Yuxin Zheng,Jianzong Wang*

Main category: cs.RO

TL;DR: 本文提出了一种轻量级、无需训练的自纠正控制框架VLA-SCT，用于提升视觉-语言-动作（VLA）智能体在操作任务中的稳定性和完成率。


<details>
  <summary>Details</summary>
Motivation: 目前VLA模型在操作任务中存在：1）动作输出与目标空间位置精度不足导致抓取失败；2）缺乏完成判断，易出现冗余动作或超时。本文旨在解决这些核心瓶颈。

Method: 提出VLA-SCT框架，将数据驱动的动作优化和有条件的任务终止逻辑结合，形成自纠正闭环，无需额外训练即可提升模型表现。

Result: 在LIBERO基准测试集中，VLA-SCT框架在所有数据集上均显著提高了细致操作任务的成功率与任务完成的准确性，优于基线方法。

Conclusion: VLA-SCT方法增强了VLA智能体在复杂、非结构化环境中的鲁棒性和可靠性，有助于推动其实际部署。

Abstract: While vision-language-action (VLA) models for embodied agents integrate perception, reasoning, and control, they remain constrained by two critical weaknesses: first, during grasping tasks, the action tokens generated by the language model often exhibit subtle spatial deviations from the target object, resulting in grasp failures; second, they lack the ability to reliably recognize task completion, which leads to redundant actions and frequent timeout errors. To address these challenges and enhance robustness, we propose a lightweight, training-free framework, VLA-SCT. This framework operates as a self-correcting control loop, combining data-driven action refinement with conditional logic for termination. Consequently, compared to baseline approaches, our method achieves consistent improvements across all datasets in the LIBERO benchmark, significantly increasing the success rate of fine manipulation tasks and ensuring accurate task completion, thereby promoting the deployment of more reliable VLA agents in complex, unstructured environments.

</details>


### [511] [Concept-Based Dictionary Learning for Inference-Time Safety in Vision Language Action Models](https://arxiv.org/abs/2602.01834)
*Siqi Wen,Shu Yang,Shaopeng Fu,Jingfeng Zhang,Lijie Hu,Di Wang*

Main category: cs.RO

TL;DR: 本文提出了一种基于概念字典学习的推理时安全控制方法，用于提升视觉-语言-行为（VLA）模型在现实系统中的安全性。该方法能够在不损失性能的情况下，有效拦截潜在的危险行为。


<details>
  <summary>Details</summary>
Motivation: VLA模型能基于多模态信息执行实际行为，但这种能力也带来了安全风险：攻击者可能诱导模型执行危险动作。现有防护手段介入时机或模态不当，不能根本解决问题，因此需要新的、更有效的安全控制方法。

Method: 通过构建稀疏、可解释的字典，分析模型隐藏激活，识别出与有害概念相关的方向，并在推理时依据阈值进行干预，阻断危险激活信号，从而实现安全防控。该方法无需对原模型重新训练，适用于多种VLA模型。

Result: 在Libero-Harm、BadRobot、RoboPair和IS-Bench等基准上，该方法能将攻击成功率降低70%以上，同时任务成功率基本不受影响，表现优于现有防御措施。

Conclusion: 提出的方法首次实现在 embodied systems 上利用概念解释进行推理时安全防控，提高了VLA模型的可解释性和部署安全性，并且方法简单易集成，对模型无侵入，是实现可靠VLA系统的重要进展。

Abstract: Vision Language Action (VLA) models close the perception action loop by translating multimodal instructions into executable behaviors, but this very capability magnifies safety risks: jailbreaks that merely yield toxic text in LLMs can trigger unsafe physical actions in embodied systems. Existing defenses alignment, filtering, or prompt hardening intervene too late or at the wrong modality, leaving fused representations exploitable. We introduce a concept-based dictionary learning framework for inference-time safety control. By constructing sparse, interpretable dictionaries from hidden activations, our method identifies harmful concept directions and applies threshold-based interventions to suppress or block unsafe activations. Experiments on Libero-Harm, BadRobot, RoboPair, and IS-Bench show that our approach achieves state-of-the-art defense performance, cutting attack success rates by over 70\% while maintaining task success. Crucially, the framework is plug-in and model-agnostic, requiring no retraining and integrating seamlessly with diverse VLAs. To our knowledge, this is the first inference-time concept-based safety method for embodied systems, advancing both interpretability and safe deployment of VLA models.

</details>


### [512] [Vision-only UAV State Estimation for Fast Flights Without External Localization Systems: A2RL Drone Racing Finalist Approach](https://arxiv.org/abs/2602.01860)
*Filip Novák,Matěj Petrlík,Matej Novosad,Parakh M. Gupta,Robert Pěnička,Martin Saska*

Main category: cs.RO

TL;DR: 本文提出了一种基于单目相机和IMU的高速无人机自主状态估计方法，通过引入新颖的漂移补偿模型，实现了在无GNSS、环境复杂时的高精度、强鲁棒状态估计，显著优于依赖更复杂硬件的主流方法，并在模拟与真实测试及国际竞赛中取得出色成绩。


<details>
  <summary>Details</summary>
Motivation: 目前在复杂、无GNSS环境下高速飞行的无人机需要快速、准确、鲁棒的状态估计，现有主流方法要么依赖昂贵复杂的硬件（如双目、激光），要么VIO漂移未能校正，状态估计容易失准，限制了实际应用。

Method: 采用单目RGB相机、IMU及基于视觉-惯性里程计和标志物相机测量系统的数据融合，通过新颖的数学漂移模型在线估计并矫正VIO所有状态（位置、姿态、线速度、角速度）的漂移，实现高精度状态估计。方法经过1600次仿真和大量实测验证。

Result: 提出的方法有效补偿了VIO状态漂移，在高速、剧烈机动下也能保持精确的状态估计。实际测试和A2RL世界级无人机竞赛中表现优异，团队从210支队伍中晋级四强获奖。

Conclusion: 本文的新方法性能优异、硬件需求低，能提升高速无人机在GNSS-denied、复杂环境下的应用前景，在仿真、实测和国际赛事中均得到验证。

Abstract: Fast flights with aggressive maneuvers in cluttered GNSS-denied environments require fast, reliable, and accurate UAV state estimation. In this paper, we present an approach for onboard state estimation of a high-speed UAV using a monocular RGB camera and an IMU. Our approach fuses data from Visual-Inertial Odometry (VIO), an onboard landmark-based camera measurement system, and an IMU to produce an accurate state estimate. Using onboard measurement data, we estimate and compensate for VIO drift through a novel mathematical drift model. State-of-the-art approaches often rely on more complex hardware (e.g., stereo cameras or rangefinders) and use uncorrected drifting VIO velocities, orientation, and angular rates, leading to errors during fast maneuvers. In contrast, our method corrects all VIO states (position, orientation, linear and angular velocity), resulting in accurate state estimation even during rapid and dynamic motion. Our approach was thoroughly validated through 1600 simulations and numerous real-world experiments. Furthermore, we applied the proposed method in the A2RL Drone Racing Challenge 2025, where our team advanced to the final four out of 210 teams and earned a medal.

</details>


### [513] [BTGenBot-2: Efficient Behavior Tree Generation with Small Language Models](https://arxiv.org/abs/2602.01870)
*Riccardo Andrea Izzo,Gianluca Bardaro,Matteo Matteucci*

Main category: cs.RO

TL;DR: 本文提出了BTGenBot-2，一款轻量级开源小型语言模型，能将自然语言任务描述直接转化为可执行的机器人行为树。它支持零样本生成和自动错误恢复，且在各种指标上优于现有大型模型。


<details>
  <summary>Details</summary>
Motivation: 现有机器人任务规划方法多依赖封闭源、计算资源丰富的LLM，难以实际部署于物理机器人。且缺乏统一、易用的机器人任务生成表示形式，限制了LLM在机器人的广泛应用。

Method: 作者设计了一款具有10亿参数的开源小型语言模型BTGenBot-2，可直接将自然语言描述和动作原语列表映射为XML格式的行为树，实现零样本行为树生成和推理时的错误恢复。同时，提出了首个LLM生成行为树的标准基准测试集，涵盖52类机器人任务。

Result: BTGenBot-2在NVIDIA Isaac Sim仿真平台的导航和操控任务中，各项功能和非功能指标均超越了GPT-5、Claude Opus 4.1等更大模型，零样本模式下成功率达90.38%，一次演示后成功率达98.07%，推理速度快16倍。

Conclusion: BTGenBot-2为资源受限机器人的实时任务规划提供了高效、可用的解决方案，并推动了开源行为树生成工具标准化，促进了LLM技术在机器人领域的实际应用。

Abstract: Recent advances in robot learning increasingly rely on LLM-based task planning, leveraging their ability to bridge natural language with executable actions. While prior works showcased great performances, the widespread adoption of these models in robotics has been challenging as 1) existing methods are often closed-source or computationally intensive, neglecting the actual deployment on real-world physical systems, and 2) there is no universally accepted, plug-and-play representation for robotic task generation. Addressing these challenges, we propose BTGenBot-2, a 1B-parameter open-source small language model that directly converts natural language task descriptions and a list of robot action primitives into executable behavior trees in XML. Unlike prior approaches, BTGenBot-2 enables zero-shot BT generation, error recovery at inference and runtime, while remaining lightweight enough for resource-constrained robots. We further introduce the first standardized benchmark for LLM-based BT generation, covering 52 navigation and manipulation tasks in NVIDIA Isaac Sim. Extensive evaluations demonstrate that BTGenBot-2 consistently outperforms GPT-5, Claude Opus 4.1, and larger open-source models across both functional and non-functional metrics, achieving average success rates of 90.38% in zero-shot and 98.07% in one-shot, while delivering up to 16x faster inference compared to the previous BTGenBot.

</details>


### [514] [Multimodal Large Language Models for Real-Time Situated Reasoning](https://arxiv.org/abs/2602.01880)
*Giulio Antonio Abbo,Senne Lenaerts,Tony Belpaeme*

Main category: cs.RO

TL;DR: 本研究结合多模态大语言模型GPT-4o与TurtleBot 4机器人，实现了具备视觉输入分析和实时决策能力的智能家居清扫系统，验证了模型在理解家庭情境和用户价值取向方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前机器人在家庭应用中缺乏对上下文、社交规范和用户偏好的深入理解，因此亟需提升其根据视觉环境做出符合用户价值观与实际需求的决策能力。

Method: 本研究将GPT-4o模型与TurtleBot 4智能扫地机器人结合，通过视觉感知输入评估家庭环境，并判断是否适合开启清扫任务，实现了多模态、实时的价值敏感型决策流程。

Result: 系统能在真实家庭环境中，仅凭有限的视觉信息，推断出情景和涉及的价值观，包括清洁度、舒适性和安全性，展示了推理和决策的能力。

Conclusion: 多模态大语言模型有望提升机器人自主性与情境感知水平，但在实际部署中依然面临一致性、偏见和实时性等挑战。

Abstract: In this work, we explore how multimodal large language models can support real-time context- and value-aware decision-making. To do so, we combine the GPT-4o language model with a TurtleBot 4 platform simulating a smart vacuum cleaning robot in a home. The model evaluates the environment through vision input and determines whether it is appropriate to initiate cleaning. The system highlights the ability of these models to reason about domestic activities, social norms, and user preferences and take nuanced decisions aligned with the values of the people involved, such as cleanliness, comfort, and safety. We demonstrate the system in a realistic home environment, showing its ability to infer context and values from limited visual input. Our results highlight the promise of multimodal large language models in enhancing robotic autonomy and situational awareness, while also underscoring challenges related to consistency, bias, and real-time performance.

</details>


### [515] [Path Tracking with Dynamic Control Point Blending for Autonomous Vehicles: An Experimental Study](https://arxiv.org/abs/2602.01892)
*Alexandre Lombard,Florent Perronnet,Nicolas Gaud,Abdeljalil Abbas-Turki*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的自动驾驶车辆路径跟踪框架，通过在轮基线上动态插值控制点实现更灵活的横向控制，并结合前轴和后轴控制器连续切换，提高了轨迹跟踪的稳定性和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统固定于前轴或后轴的路径跟踪方法在不同驾驶场景（如低速、倒车等）下适应性有限，导致轨迹偏差或控制不连续，迫切需要一种更平滑、适应性更强的控制策略。

Method: 方法采用了前轴Stanley控制器与基于曲率的后轴控制器进行重心插值（barycentric blending），动态计算轮基线上任意控制点的横向命令，助于实现前后轴控制的无缝切换。纵向控制则依赖基于虚拟轨迹边界和射线追踪的策略，根据前方几何约束调整速度。整体方案在仿真和真实车辆（配备GPS-RTK、雷达、里程计与IMU）上统一实现和验证。

Result: 在闭环跟踪和倒车测试中，所提出的框架实现了更高的轨迹精度、更平滑的转向以及相比固定控制点基线方法更强的适应性。

Conclusion: 动态插值控制点和创新的纵向控制策略为自动驾驶车辆提供了更稳定、准确和灵活的路径跟踪能力，有望提升实际应用中的驾驶表现。

Abstract: This paper presents an experimental study of a path-tracking framework for autonomous vehicles in which the lateral control command is applied to a dynamic control point along the wheelbase. Instead of enforcing a fixed reference at either the front or rear axle, the proposed method continuously interpolates between both, enabling smooth adaptation across driving contexts, including low-speed maneuvers and reverse motion. The lateral steering command is obtained by barycentric blending of two complementary controllers: a front-axle Stanley formulation and a rear-axle curvature-based geometric controller, yielding continuous transitions in steering behavior and improved tracking stability. In addition, we introduce a curvature-aware longitudinal control strategy based on virtual track borders and ray-tracing, which converts upcoming geometric constraints into a virtual obstacle distance and regulates speed accordingly. The complete approach is implemented in a unified control stack and validated in simulation and on a real autonomous vehicle equipped with GPS-RTK, radar, odometry, and IMU. The results in closed-loop tracking and backward maneuvers show improved trajectory accuracy, smoother steering profiles, and increased adaptability compared to fixed control-point baselines.

</details>


### [516] [Multi-Task Learning for Robot Perception with Imbalanced Data](https://arxiv.org/abs/2602.01899)
*Ozgur Erkent*

Main category: cs.RO

TL;DR: 本文提出了一种可在部分任务缺乏标签数据时仍能进行多任务学习的方法，并分析了任务间相互作用对性能提升的影响。


<details>
  <summary>Details</summary>
Motivation: 移动机器人存在资源有限、部分任务标签难以获取的问题，传统多任务学习在标签数量不平衡时表现受限，亟需新方法应对缺标签与数据不平衡。

Method: 提出一种方法可在部分任务没有真实标签情境下实现多任务学习，通过将部分任务的输出结果作为输入训练教师网络，提升其他任务表现。并对任务间性能提升的机制进行了分析。

Result: 实验基于NYUDv2和Cityscapes数据集，在语义分割与深度估计任务上检验了方法，在训练数据量较小时依然获得优化效果。验证了通过使用某些任务输出可以提升其他任务的表现。

Conclusion: 部分任务无标签时，仍可以通过合理设计学习机制提升多任务系统表现，且任务间具有协同增益。因此提出的方法对资源有限或难以标注数据的机器人应用具有实际意义。

Abstract: Multi-task problem solving has been shown to improve the accuracy of the individual tasks, which is an important feature for robots, as they have a limited resource. However, when the number of labels for each task is not equal, namely imbalanced data exist, a problem may arise due to insufficient number of samples, and labeling is not very easy for mobile robots in every environment. We propose a method that can learn tasks even in the absence of the ground truth labels for some of the tasks. We also provide a detailed analysis of the proposed method. An interesting finding is related to the interaction of the tasks. We show a methodology to find out which tasks can improve the performance of other tasks. We investigate this by training the teacher network with the task outputs such as depth as inputs. We further provide empirical evidence when trained with a small amount of data. We use semantic segmentation and depth estimation tasks on different datasets, NYUDv2 and Cityscapes.

</details>


### [517] [ForSim: Stepwise Forward Simulation for Traffic Policy Fine-Tuning](https://arxiv.org/abs/2602.01916)
*Keyu Chen,Wenchao Sun,Hao Cheng,Zheng Fu,Sifa Zheng*

Main category: cs.RO

TL;DR: 本文提出了一种新的交通仿真方法ForSim，显著提升了仿真的真实性和安全性，特别是在自动驾驶的闭环仿真训练与评估中。


<details>
  <summary>Details</summary>
Motivation: 当前交通仿真在自动驾驶训练中面临的主要挑战有：1）仿真中由于采用开放式模仿学习导致协变量移位问题；2）仿真场景难以反映真实交通中的多模态行为，仿真互动性差，降低了仿真的真实性和用处。

Method: 提出ForSim，一种逐步闭环前向仿真范式。每个仿真步长，智能体通过物理约束传播最接近参考轨迹的候选轨迹，保证多模态行为的多样性和一致性。其他交通参与者则依赖一步预测方法更新，实现了多智能体之间交互性更强的动态演化。ForSim与现有RIFT仿真框架结合，对交通策略进行优化。

Result: 实验表明，ForSim集成到RIFT框架后，在安全性提升的同时，保持了效率、真实度和舒适度，克服了以往非闭环、多模态一致性差的问题。

Conclusion: 闭环多模态交互建模对提升自动驾驶仿真的真实性和可靠性至关重要。ForSim方法有助于推动更高保真度、更可靠的交通仿真，为自动驾驶训练与评估提供更坚实基础。

Abstract: As the foundation of closed-loop training and evaluation in autonomous driving, traffic simulation still faces two fundamental challenges: covariate shift introduced by open-loop imitation learning and limited capacity to reflect the multimodal behaviors observed in real-world traffic. Although recent frameworks such as RIFT have partially addressed these issues through group-relative optimization, their forward simulation procedures remain largely non-reactive, leading to unrealistic agent interactions within the virtual domain and ultimately limiting simulation fidelity. To address these issues, we propose ForSim, a stepwise closed-loop forward simulation paradigm. At each virtual timestep, the traffic agent propagates the virtual candidate trajectory that best spatiotemporally matches the reference trajectory through physically grounded motion dynamics, thereby preserving multimodal behavioral diversity while ensuring intra-modality consistency. Other agents are updated with stepwise predictions, yielding coherent and interaction-aware evolution. When incorporated into the RIFT traffic simulation framework, ForSim operates in conjunction with group-relative optimization to fine-tune traffic policy. Extensive experiments confirm that this integration consistently improves safety while maintaining efficiency, realism, and comfort. These results underscore the importance of modeling closed-loop multimodal interactions within forward simulation and enhance the fidelity and reliability of traffic simulation for autonomous driving. Project Page: https://currychen77.github.io/ForSim/

</details>


### [518] [LIEREx: Language-Image Embeddings for Robotic Exploration](https://arxiv.org/abs/2602.01930)
*Felix Igelbrink,Lennart Niecksch,Marian Renz,Martin Günther,Martin Atzmueller*

Main category: cs.RO

TL;DR: 该论文提出利用视觉-语言基础模型（如CLIP）结合3D语义场景图，提高机器人在部分未知环境下目标导向探索的能力。


<details>
  <summary>Details</summary>
Motivation: 传统的语义地图依赖固定的符号词汇，难以处理预先未定义的新知识，导致在实际复杂环境下应用受限。

Method: 采用视觉-语言基础模型（如CLIP），将物体表示为高维嵌入向量（open-set mapping），并与现有的3D语义场景图结合，实现机器人对目标物体的自主探索。

Result: 集成方案（LIEREx）能让自主体在部分未知环境中执行基于目标的探索任务，无需依赖固定标签。

Conclusion: 结合VLFMs与3D语义场景图，能够提升机器人语义理解与开放集探索能力，拓展了机器人在实际复杂环境中应用的可能性。

Abstract: Semantic maps allow a robot to reason about its surroundings to fulfill tasks such as navigating known environments, finding specific objects, and exploring unmapped areas. Traditional mapping approaches provide accurate geometric representations but are often constrained by pre-designed symbolic vocabularies. The reliance on fixed object classes makes it impractical to handle out-of-distribution knowledge not defined at design time. Recent advances in Vision-Language Foundation Models, such as CLIP, enable open-set mapping, where objects are encoded as high-dimensional embeddings rather than fixed labels. In LIEREx, we integrate these VLFMs with established 3D Semantic Scene Graphs to enable target-directed exploration by an autonomous agent in partially unknown environments.

</details>


### [519] [Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy](https://arxiv.org/abs/2602.01939)
*Yuxin He,Ruihao Zhang,Tianao Shen,Cheng Liu,Qiang Nie*

Main category: cs.RO

TL;DR: 本文提出了一个名为EFM-10的新基准，专注于探索性与专注性操作任务，并引入了双臂主动感知策略（BAP），结合主动视觉与力感知，有效提高机器人在受遮挡环境下的操作表现。


<details>
  <summary>Details</summary>
Motivation: 随着机器人主摄像头通常安装在“头部”，在执行操作任务时更频繁地面临视觉遮挡，当前方法应对遮挡信息不足有局限，激发了作者探索更主动的信息收集方式以完成复杂任务。

Method: 作者提出“探索性与专注性操作”问题（EFM），制定了包含10个任务、4大类别的EFM-10基准，并提出“双臂主动感知策略”（BAP）；BAP利用一只机械臂主动调整视觉视角，另一只机械臂进行力感知操作。基于此策略，收集了BAPData数据集，并使用模仿学习方法验证BAP策略的有效性。

Result: 实验结果表明，BAP策略在EFM-10基准任务下能有效提升机器人在视觉遮挡等复杂环境下的信息收集和操作成功率，验证了该策略的实用价值。

Conclusion: EFM-10基准与BAP策略为机器人在主动视角与力感知下，提高对遮挡等挑战环境中的操作表现提供了新方向和基础，期望推动该领域的未来研究。

Abstract: Recently, active vision has reemerged as an important concept for manipulation, since visual occlusion occurs more frequently when main cameras are mounted on the robot heads. We reflect on the visual occlusion issue and identify its essence as the absence of information useful for task completion. Inspired by this, we come up with the more fundamental problem of Exploratory and Focused Manipulation (EFM). The proposed problem is about actively collecting information to complete challenging manipulation tasks that require exploration or focus. As an initial attempt to address this problem, we establish the EFM-10 benchmark that consists of 4 categories of tasks that align with our definition (10 tasks in total). We further come up with a Bimanual Active Perception (BAP) strategy, which leverages one arm to provide active vision and another arm to provide force sensing while manipulating. Based on this idea, we collect a dataset named BAPData for the tasks in EFM-10. With the dataset, we successfully verify the effectiveness of the BAP strategy in an imitation learning manner. We hope that the EFM-10 benchmark along with the BAP strategy can become a cornerstone that facilitates future research towards this direction. Project website: EFManipulation.github.io.

</details>


### [520] [A Unified Control Architecture for Macro-Micro Manipulation using a Active Remote Center of Compliance for Manufacturing Applications](https://arxiv.org/abs/2602.01948)
*Patrick Frank,Christian Friedrich*

Main category: cs.RO

TL;DR: 本文提出了一种新型宏-微操控系统的控制架构，将宏操控器主动纳入交互控制，提高了系统的控制带宽，并通过多项实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统宏-微操控系统中，宏操控器只负责位置控制，微操控器负责与环境的交互，导致交互控制带宽有限，难以满足高动态任务需求。作者旨在突破这一带宽瓶颈，提高整体系统性能。

Method: 提出新的控制架构，使宏操控器参与主动交互控制，并引入代理模型（surrogate models）简化控制器设计，便于对硬件变化进行适应。通过与主从式（leader-follower）和传统力控制方法对比实验，验证方案效果。

Result: 新架构下，控制带宽提升至主从法2.1倍、传统机器人力控制12.5倍，在碰撞、力跟踪、装配任务等实验中表现优异。

Conclusion: 纳入宏操控器参与交互控制，可显著提升系统控制带宽与动态性能，新方法适应性强，验证有效，适用于高性能工业任务。

Abstract: Macro-micro manipulators combine a macro manipulator with a large workspace, such as an industrial robot, with a lightweight, high-bandwidth micro manipulator. This enables highly dynamic interaction control while preserving the wide workspace of the robot. Traditionally, position control is assigned to the macro manipulator, while the micro manipulator handles the interaction with the environment, limiting the achievable interaction control bandwidth. To solve this, we propose a novel control architecture that incorporates the macro manipulator into the active interaction control. This leads to a increase in control bandwidth by a factor of 2.1 compared to the state of the art architecture, based on the leader-follower approach and factor 12.5 compared to traditional robot-based force control. Further we propose surrogate models for a more efficient controller design and easy adaptation to hardware changes. We validate our approach by comparing it against the other control schemes in different experiments, like collision with an object, following a force trajectory and industrial assembly tasks.

</details>


### [521] [Reformulating AI-based Multi-Object Relative State Estimation for Aleatoric Uncertainty-based Outlier Rejection of Partial Measurements](https://arxiv.org/abs/2602.02006)
*Thomas Jantos,Giulio Delama,Stephan Weiss,Jan Steinbrener*

Main category: cs.RO

TL;DR: 本文提出通过重新设计基于AI的物体相对状态测量方程，提高移动机器人利用深度神经网络进行精确定位的准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 基于AI的物体识别与6自由度位姿估计越来越常用于移动机器人，但深度神经网络推理的不确定性和异常值处理影响最终定位效果。如何有效融合DNN测量结果至滤波器框架，并提升不确定性管理，是实际部署中的主要挑战。

Method: 作者将AI推断得到的物体相对6-DoF位姿，直接作为扩展卡尔曼滤波器（EKF）的观测量，并对测量方程进行了重构，使位置与旋转信息可解耦，从而减弱旋转测量误差的影响并支持部分观测拒绝。此外，将原本固定的测量协方差矩阵替换为DNN预测的内禀不确定性（aleatoric uncertainty），以动态反映测量信心。

Result: 实验显示，该重构方法不仅提升了滤波器在多目标场景下的精度和一致性，还提高了对异常值和误差的鲁棒性。解耦操作和动态不确定性估计均带来显著性能增益。

Conclusion: 融合AI测量的观测方程重构和不确定性自适应，在移动机器人定位中确实有效，建议相关任务采用该方法提升定位鲁棒性与实时性。

Abstract: Precise localization with respect to a set of objects of interest enables mobile robots to perform various tasks. With the rise of edge devices capable of deploying deep neural networks (DNNs) for real-time inference, it stands to reason to use artificial intelligence (AI) for the extraction of object-specific, semantic information from raw image data, such as the object class and the relative six degrees of freedom (6-DoF) pose. However, fusing such AI-based measurements in an Extended Kalman Filter (EKF) requires quantifying the DNNs' uncertainty and outlier rejection capabilities.
  This paper presents the benefits of reformulating the measurement equation in AI-based, object-relative state estimation. By deriving an EKF using the direct object-relative pose measurement, we can decouple the position and rotation measurements, thus limiting the influence of erroneous rotation measurements and allowing partial measurement rejection. Furthermore, we investigate the performance and consistency improvements for state estimators provided by replacing the fixed measurement covariance matrix of the 6-DoF object-relative pose measurements with the predicted aleatoric uncertainty of the DNN.

</details>


### [522] [Synchronized Online Friction Estimation and Adaptive Grasp Control for Robust Gentle Grasp](https://arxiv.org/abs/2602.02026)
*Zhenwei Niu,Xiaoyi Chen,Jiayu Hu,Zhaoyang Liu,Xiaozu Ju*

Main category: cs.RO

TL;DR: 本文提出了一个结合实时摩擦力估计和自适应抓取控制的温和型机器人抓取统一框架，并用大量实验验证了其有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在机器人抓取任务中，如何实现既稳固又不损坏物体的温和抓取是一大难题，主要受摩擦力变化和控制精度影响。本文旨在通过改进感知和控制方法提升抓取柔顺性和稳定性。

Method: 利用基于视觉的触觉传感器，采用粒子滤波方法实时估计物体的摩擦系数，并将估算结果无缝集成进一个可动态调节抓取力的反应式控制器，实现闭环耦合。传感器感知与控制器调节协同工作，不断依据最新反馈调整抓力。

Result: 通过大量机器人抓取实验，验证了该框架的响应性、鲁棒性以及效率，表明其可稳定执行温和抓取。

Conclusion: 本文提出的方法可有效提升机器人抓取的稳健性与柔顺性，为温和型抓取任务提供了一种可行而高效的解决方案。

Abstract: We introduce a unified framework for gentle robotic grasping that synergistically couples real-time friction estimation with adaptive grasp control. We propose a new particle filter-based method for real-time estimation of the friction coefficient using vision-based tactile sensors. This estimate is seamlessly integrated into a reactive controller that dynamically modulates grasp force to maintain a stable grip. The two processes operate synchronously in a closed-loop: the controller uses the current best estimate to adjust the force, while new tactile feedback from this action continuously refines the estimation. This creates a highly responsive and robust sensorimotor cycle. The reliability and efficiency of the complete framework are validated through extensive robotic experiments.

</details>


### [523] [Bandwidth-Efficient Multi-Agent Communication through Information Bottleneck and Vector Quantization](https://arxiv.org/abs/2602.02035)
*Ahmad Farooq,Kamran Iqbal*

Main category: cs.RO

TL;DR: 本文提出了一种结合信息瓶颈理论与向量量化的新型多智能体通信框架，实现了任务关键信息的有效压缩和动态通信决策，显著提升了受限带宽环境下的系统协作效率。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习系统在现实机器人应用中常常受限于带宽，导致通信不畅，影响协作效率。现有方法在信息压缩和动态通信决策方面普遍存在不足，因此需要更高效、理论基础更强的通信机制。

Method: 该方法基于信息瓶颈理论，通过向量量化实现通信消息的压缩与离散化。在此基础上引入门控通信机制，根据环境语境和智能体状态动态判定是否通信，并通过信息论优化方法确保保留任务关键信息。

Result: 在复杂协调任务中，该方法在降低41.4%带宽使用的同时，相较无通信基线提升性能181.8%。在覆盖成功率-带宽曲线的帕累托前沿面积指标上，以0.198显著优于下一个最佳方法0.142，显示了在所有带宽-性能权衡下的优势。

Conclusion: 所提出方法理论基础扎实，显著超越现有通信策略，尤其适用于带宽受限的多智能体系统（如机器人集群、自动驾驶车队和分布式传感网络）实际部署。

Abstract: Multi-agent reinforcement learning systems deployed in real-world robotics applications face severe communication constraints that significantly impact coordination effectiveness. We present a framework that combines information bottleneck theory with vector quantization to enable selective, bandwidth-efficient communication in multi-agent environments. Our approach learns to compress and discretize communication messages while preserving task-critical information through principled information-theoretic optimization. We introduce a gated communication mechanism that dynamically determines when communication is necessary based on environmental context and agent states. Experimental evaluation on challenging coordination tasks demonstrates that our method achieves 181.8% performance improvement over no-communication baselines while reducing bandwidth usage by 41.4%. Comprehensive Pareto frontier analysis shows dominance across the entire success-bandwidth spectrum with area-under-curve of 0.198 vs 0.142 for next-best methods. Our approach significantly outperforms existing communication strategies and establishes a theoretically grounded framework for deploying multi-agent systems in bandwidth-constrained environments such as robotic swarms, autonomous vehicle fleets, and distributed sensor networks.

</details>


### [524] [Frictional Contact Solving for Material Point Method](https://arxiv.org/abs/2602.02038)
*Etienne Ménager,Justin Carpentier*

Main category: cs.RO

TL;DR: 本文提出了一种适用于隐式物质点法（MPM）的摩擦接触解决方案，实现了更精确、鲁棒的摩擦接触处理，提升了物理仿真的可靠性。


<details>
  <summary>Details</summary>
Motivation: 物质点法在处理含摩擦接触时存在瓶颈，如接触点检测和摩擦定律（非穿透、库仑摩擦、最大耗散原理）执行不准确，影响仿真效果。为解决这个关键难题，作者提出新方法。

Method: 作者设计了一个针对隐式MPM的摩擦接触流程：碰撞检测阶段利用粒子中心几何元件定位接触点；接触分解阶段将摩擦接触建模为接触冲量的非线性互补问题（NCP）；采用交替方向乘子法（ADMM）求解，并重用MPM内本身的线性化模块以实现高效与数值稳定。方案可无缝集成于MPM，且不依赖具体的材料模型、插值函数或转移方案。

Result: 方法在七个代表性场景下进行了评估，这些场景涵盖弹性与弹塑性反应、各类复杂变形几何体以及多种接触工况。测试表明，该方法能有效实现准确的接触定位和可靠的摩擦处理，具有广泛适用性。

Conclusion: 提出的方法兼具精度、可靠性及通用性，是面向机器人及相关领域基于MPM仿真的实用摩擦接触解决方案。

Abstract: Accurately handling contact with friction remains a core bottleneck for Material Point Method (MPM), from reliable contact point detection to enforcing frictional contact laws (non-penetration, Coulomb friction, and maximum dissipation principle). In this paper, we introduce a frictional-contact pipeline for implicit MPM that is both precise and robust. During the collision detection phase, contact points are localized with particle-centric geometric primitives; during the contact resolution phase, we cast frictional contact as a Nonlinear Complementarity Problem (NCP) over contact impulses and solve it with an Alternating Direction Method of Multipliers (ADMM) scheme. Crucially, the formulation reuses the same implicit MPM linearization, yielding efficiency and numerical stability. The method integrates seamlessly into the implicit MPM loop and is agnostic to modeling choices, including material laws, interpolation functions, and transfer schemes. We evaluate it across seven representative scenes that span elastic and elasto-plastic responses, simple and complex deformable geometries, and a wide range of contact conditions. Overall, the proposed method enables accurate contact localization, reliable frictional handling, and broad generality, making it a practical solution for MPM-based simulations in robotics and related domains.

</details>


### [525] [FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation](https://arxiv.org/abs/2602.02142)
*Ruiteng Zhao,Wenshuo Wang,Yicheng Ma,Xiaocong Li,Francis E. H. Tay,Marcelo H. Ang,Haiyue Zhu*

Main category: cs.RO

TL;DR: 该论文提出了一种无需物理力传感器即可实现力感知的VLA（视觉-语言-动作）新框架，通过“力蒸馏模块”从视觉与状态中提取力信号，增强机器人复杂操作能力，且在实验中表现优于直接使用传感器的方案。


<details>
  <summary>Details</summary>
Motivation: 传统VLA系统中，力感知依赖昂贵且易损的力/力矩传感器，限制了机器人在成本和适用性上的推广。为此，作者探索不依赖物理传感器的力感知方法，从而降低硬件要求，拓展机器人应用场景。

Method: 提出力蒸馏模块（FDM）：通过学习机制，将视觉观测和机器人状态与力信号空间对齐，生成力token；在推理阶段将该token注入预训练的VLM，实现具备力感知推理而不干扰其视觉-语言理解能力；还实现了多模态的先验融合，提升感知-动作鲁棒性。

Result: 在多个物理实验中，所提方案的“蒸馏力token”不仅优于直接的传感器力测量，还优于其他对比基线，验证了该无传感器力感知方法的有效性和优越性。

Conclusion: FD-VLA能够以更低成本、更高泛化性为机器人复杂任务带来准确的力感知与推理，促进了无力传感器机器人在实际操作场景中的应用。

Abstract: Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach.

</details>


### [526] [Extending the Law of Intersegmental Coordination: Implications for Powered Prosthetic Controls](https://arxiv.org/abs/2602.02181)
*Elad Siman Tov,Nili E. Krausz*

Main category: cs.RO

TL;DR: 该论文提出了一种新的分析和应用肌肉协调规律方法，以提升假肢步态的能量效率，重点关注下肢截肢者步行时的协调问题，并开发了开源工具箱辅助研究。


<details>
  <summary>Details</summary>
Motivation: 动力假肢虽能提供能量，但减少截肢者步行代谢成本依然困难。已有步态协调规律（ISC）在健康人步态中与能量支出相关，但在假肢应用中分析不足，有必要深入挖掘其机制及潜在应用。

Method: 论文提出并实现了用于分析下肢三维运动学数据的ISC分析方法，并据此扩展到基于力矩的协调规律。研究比较了健康人和不同假肢用户的步态ISC表现，并通过设计约束预测可补偿被动足部影响的胫骨角度和力矩。

Result: 实验发现，虽然无论采用动力或被动假肢，步态主成分（高程角）均保持共面，但基于力矩的协调性在假肢使用者中下降。通过ISC约束可预测并部分补偿被动假肢导致的步态变化。

Conclusion: 提出的分析与工具有助于理解和优化截肢者步态协调，为动力假肢的控制改进提供新思路，并支持后续对步行神经控制机制的研究。

Abstract: Powered prostheses are capable of providing net positive work to amputees and have advanced in the past two decades. However, reducing amputee metabolic cost of walking remains an open problem. The Law of Intersegmental Coordination (ISC) has been observed across gaits and has been previously implicated in energy expenditure of walking, yet it has rarely been analyzed or applied within the context of lower-limb amputee gait. This law states that the elevation angles of the thigh, shank and foot over the gait cycle are not independent. In this work, we developed a method to analyze intersegmental coordination for lower-limb 3D kinematic data, to simplify ISC analysis. Moreover, inspired by motor control, biomechanics and robotics literature, we used our method to broaden ISC toward a new law of coordination of moments. We find these Elevation Space Moments (ESM), and present results showing a moment-based coordination for able bodied gait. We also analyzed ISC for amputee gait walking with powered and passive prosthesis, and found that while elevation angles remained planar, the ESM showed less coordination. We use ISC as a constraint to predict the shank angles/moments that would compensate for alterations due to a passive foot so as to mimic a healthy thigh angle/moment profile. This may have implications for improving powered prosthetic control. We developed the ISC3d toolbox that is freely available online, which may be used to compute kinematic and kinetic ISC in 3D. This provides a means to further study the role of coordination in gait and may help address fundamental questions of the neural control of human movement.

</details>


### [527] [Online Fine-Tuning of Pretrained Controllers for Autonomous Driving via Real-Time Recurrent RL](https://arxiv.org/abs/2602.02236)
*Julian Lemmel,Felix Resch,Mónika Farsang,Ramin Hasani,Daniela Rus,Radu Grosu*

Main category: cs.RO

TL;DR: 本文提出使用实时递归强化学习（RTRRL）对已预训练策略进行在线微调，提升在动态环境下的自主控制系统性能，并通过生物启发的循环神经网络模型辅助，验证了其在仿真和真实驾驶任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 预训练的控制策略在实际应用中容易因环境变化或系统漂移而性能迅速下降，限制了学习型控制系统的实用性。亟需一种可以使策略适应新环境变化的在线自适应方法。

Method: 采用生物学上可行的实时递归强化学习（RTRRL）算法，对预训练策略进行在线微调，同时引入液态-阻抗电容循环神经网络模型（LRLC RNN），增强系统的实时自适应能力。方法在CarRacing仿真环境以及配备事件摄像头的RoboRacer真实车道跟踪任务中进行了验证。

Result: 结果显示，通过RTRRL与LRLC RNN的结合，成功实现了对自主体的预训练策略的高效在线微调。在仿真和真实任务中均提升了控制表现，证明该闭环自适应方法的有效性。

Conclusion: 本文方法能够显著提升预训练控制策略在动态或现实环境下的适应性和鲁棒性，为实际部署学习型自主系统提供了切实可行的解决方案。

Abstract: Deploying pretrained policies in real-world applications presents substantial challenges that fundamentally limit the practical applicability of learning-based control systems. When autonomous systems encounter environmental changes in system dynamics, sensor drift, or task objectives, fixed policies rapidly degrade in performance. We show that employing Real-Time Recurrent Reinforcement Learning (RTRRL), a biologically plausible algorithm for online adaptation, can effectively fine-tune a pretrained policy to improve autonomous agents' performance on driving tasks. We further show that RTRRL synergizes with a recent biologically inspired recurrent network model, the Liquid-Resistance Liquid-Capacitance RNN. We demonstrate the effectiveness of this closed-loop approach in a simulated CarRacing environment and in a real-world line-following task with a RoboRacer car equipped with an event camera.

</details>


### [528] [Bridging the Sim-to-Real Gap with multipanda ros2: A Real-Time ROS2 Framework for Multimanual Systems](https://arxiv.org/abs/2602.02269)
*Jon Škerlj,Seongjin Bien,Abdeldjallil Naceri,Sami Haddadin*

Main category: cs.RO

TL;DR: 本文介绍了一个基于ROS2的多机器人控制开源框架multipanda_ros2，侧重高实时性和精确控制，实现了平滑控制器切换、高精度仿真与现实环境的一致性优化。


<details>
  <summary>Details</summary>
Motivation: 现有机器人控制系统难以实现高频实时（1kHz）多臂协作，且仿真与实际存在较大差距，限制了复杂交互任务下的性能和安全性。

Method: 设计和实现了multipanda_ros2架构，利用ros2 control接口统一多机器人控制；提出controllet-feature设计模式，将控制器切换延迟压缩到2ms以内；集成高保真MuJoCo仿真，实现运动学和动力学相关的定量评估；采用惯性参数识别优化物理一致性，提升sim2real的一致性。

Result: 系统可在单进程下稳定控制多台Franka机器人，1kHz实时循环下延迟极低，仿真与现实的控制精度显著提升。通过惯性参数辨识，实现了力和力矩的高准确性。

Conclusion: multipanda_ros2为多机器人协作及高难交互任务提供了高可靠、可复现的平台，有效缩小了仿真与实际的性能差，并适用于高级机器人研究。

Abstract: We present $multipanda\_ros2$, a novel open-source ROS2 architecture for multi-robot control of Franka Robotics robots. Leveraging ros2 control, this framework provides native ROS2 interfaces for controlling any number of robots from a single process. Our core contributions address key challenges in real-time torque control, including interaction control and robot-environment modeling. A central focus of this work is sustaining a 1kHz control frequency, a necessity for real-time control and a minimum frequency required by safety standards. Moreover, we introduce a controllet-feature design pattern that enables controller-switching delays of $\le 2$ ms, facilitating reproducible benchmarking and complex multi-robot interaction scenarios. To bridge the simulation-to-reality (sim2real) gap, we integrate a high-fidelity MuJoCo simulation with quantitative metrics for both kinematic accuracy and dynamic consistency (torques, forces, and control errors). Furthermore, we demonstrate that real-world inertial parameter identification can significantly improve force and torque accuracy, providing a methodology for iterative physics refinement. Our work extends approaches from soft robotics to rigid dual-arm, contact-rich tasks, showcasing a promising method to reduce the sim2real gap and providing a robust, reproducible platform for advanced robotics research.

</details>


### [529] [TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour](https://arxiv.org/abs/2602.02331)
*Shaoting Zhu,Baijun Ye,Jiaxuan Wang,Jiakang Chen,Ziwen Zhuang,Linzhan Mou,Runhan Huang,Hang Zhao*

Main category: cs.RO

TL;DR: 提出了一种新的人形机器人强化学习方法，通过在测试时快速训练，使机器人能够高效适应并通过极为复杂、从未见过的地形，实现动态跑酷。核心流程包括先在虚拟多样地形预训练，再通过RGB-D重建的真实地形快速微调，最终提升了复杂环境下的通用性能。


<details>
  <summary>Details</summary>
Motivation: 目前通用的机器人行走策略虽然能应对多样地形，但面对复杂甚至全新环境时性能大幅下降。为扩展机器人在极端地形的适应能力，提高实际落地效能，需要新的方法快速适应新环境。

Method: 采用Real-to-Sim-to-Real训练框架，分两阶段：1）在程序生成的多样地形上离线预训练策略；2）针对真实新地形，通过RGB-D数据重建高保真地形网格，在线高速微调策略（测试时训练，TTT）。整个采集-重建-微调流程10分钟内完成。

Result: 实验结果显示，经过TTT微调后的人形机器人能在复杂障碍（楔形物、柱桩、箱体、梯形、窄梁）上实现鲁棒通行，策略具有很好的零样本从仿真到现实转移能力。

Conclusion: 所提出的TTT-Parkour方法显著提升了人形机器人在真实复杂新地形中完成高难度动态移动的能力，为拓展机器人适应现实极端环境提供了有效方案。

Abstract: Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot's capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability.

</details>


### [530] [Mapping-Guided Task Discovery and Allocation for Robotic Inspection of Underwater Structures](https://arxiv.org/abs/2602.02389)
*Marina Ruediger,Ashis G. Banerjee*

Main category: cs.RO

TL;DR: 本文提出一种结合SLAM数据的水下多机器人任务生成与优化方法，实现无先验几何知识下的高效检测。


<details>
  <summary>Details</summary>
Motivation: 水下环境检测通常缺乏先验的精确几何信息，传统任务分配和路径规划方式难以实现高效、灵活的覆盖检测，且难以适应复杂或未知环境。为提升多机器人系统的自主检测能力，急需新的任务生成与优化方法。

Method: 利用多机器人SLAM获取的环境网格信息，结合硬件参数与环境条件，自动生成检测任务。采用预期关键点得分和基于距离的剪枝方法进行任务优化，并通过水下实地测试评估算法效果，再与Voronoi分区和牛耕（boustrophedon）方式进行检测覆盖对比。

Result: 实验证明提出的算法在未提前获得环境几何信息情况下，能自动适应环境变化并科学分配检测任务。与传统分区法相比，覆盖范围更优且更易关注可能出现缺陷的位置。

Conclusion: 本文方法实现了无需环境几何先验知识的水下多机器人检测任务分配，提升了自适应能力和缺陷检测效率，适用于复杂和未知环境。

Abstract: Task generation for underwater multi-robot inspections without prior knowledge of existing geometry can be achieved and optimized through examination of simultaneous localization and mapping (SLAM) data. By considering hardware parameters and environmental conditions, a set of tasks is generated from SLAM meshes and optimized through expected keypoint scores and distance-based pruning. In-water tests are used to demonstrate the effectiveness of the algorithm and determine the appropriate parameters. These results are compared to simulated Voronoi partitions and boustrophedon patterns for inspection coverage on a model of the test environment. The key benefits of the presented task discovery method include adaptability to unexpected geometry and distributions that maintain coverage while focusing on areas more likely to present defects or damage.

</details>


### [531] [PRISM: Performer RS-IMLE for Single-pass Multisensory Imitation Learning](https://arxiv.org/abs/2602.02396)
*Amisha Bhaskar,Pratap Tokekar,Stefano Di Cairano,Alexander Schperberg*

Main category: cs.RO

TL;DR: PRISM是一种高效多感知仿人控制策略，结合了批量全局拒绝采样的IMLE、线性注意力生成器和多模态感知编码，实现了高频率、低延迟且表现优异的目标操作控制。


<details>
  <summary>Details</summary>
Motivation: 现有机器人模仿学习方法（如扩散模型、流匹配、IMLE）难以同时满足捕捉多模态动作分布、实时高频控制和融合多传感输入的全部需求。

Method: 提出PRISM策略，基于批量全局拒绝采样变体的IMLE，结合时间多感知编码器（融合RGB、深度、触觉、音频和本体感知）与Performer架构的线性注意力生成器，实现端到端的单步推理。

Result: 在Unitree Go2、UR5等硬件上，PRISM在复杂物理任务中的成功率超过最先进扩散策略10%-25%，且以30-50Hz高频率闭环控制。在CALVIN等大型仿真基准上，PRISM比扩散、流匹配策略分别提高约25%、20%的成功率，并将轨迹抖动减少20-50倍。

Conclusion: PRISM能以更快速度、更准表现以及更全面的多模态动作覆盖，在机器人仿人学习任务中优于传统扩散式模型，避免了迭代采样带来的高延迟。

Abstract: Robotic imitation learning typically requires models that capture multimodal action distributions while operating at real-time control rates and accommodating multiple sensing modalities. Although recent generative approaches such as diffusion models, flow matching, and Implicit Maximum Likelihood Estimation (IMLE) have achieved promising results, they often satisfy only a subset of these requirements. To address this, we introduce PRISM, a single-pass policy based on a batch-global rejection-sampling variant of IMLE. PRISM couples a temporal multisensory encoder (integrating RGB, depth, tactile, audio, and proprioception) with a linear-attention generator using a Performer architecture. We demonstrate the efficacy of PRISM on a diverse real-world hardware suite, including loco-manipulation using a Unitree Go2 with a 7-DoF arm D1 and tabletop manipulation with a UR5 manipulator. Across challenging physical tasks such as pre-manipulation parking, high-precision insertion, and multi-object pick-and-place, PRISM outperforms state-of-the-art diffusion policies by 10-25% in success rate while maintaining high-frequency (30-50 Hz) closed-loop control. We further validate our approach on large-scale simulation benchmarks, including CALVIN, MetaWorld, and Robomimic. In CALVIN (10% data split), PRISM improves success rates by approximately 25% over diffusion and approximately 20% over flow matching, while simultaneously reducing trajectory jerk by 20x-50x. These results position PRISM as a fast, accurate, and multisensory imitation policy that retains multimodal action coverage without the latency of iterative sampling.

</details>


### [532] [SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation](https://arxiv.org/abs/2602.02402)
*Mu Huang,Hui Wang,Kerui Ren,Linning Xu,Yunsong Zhou,Mulin Yu,Bo Dai,Jiangmiao Pang*

Main category: cs.RO

TL;DR: SoMA是一个基于3D高斯斑点的方法，实现端到端软体物体操纵仿真，提高了仿真精度、泛化能力，能稳定处理诸如长时间布料折叠等复杂任务。


<details>
  <summary>Details</summary>
Motivation: 现有软体物体仿真器受限于预设物理或数据驱动模型，缺乏机器人条件控制，导致仿真准确性、稳定性和泛化性不足。

Method: 提出SoMA模拟器，将可变形动态、环境力与机器人关节动作耦合于统一的潜在神经空间，通过3D高斯斑点建模学习交互，无需预设物理模型即可实现端到端的仿真。

Result: SoMA在实际机器人操纵任务的转录仿真准确率和泛化能力上提升20%，能够稳定高效地仿真诸如长期布料折叠等复杂任务。

Conclusion: SoMA为软体操纵带来更高的真实感、稳定性和泛化性能，为实际机器人复杂触物操作提供了更强有力的仿真支持。

Abstract: Simulating deformable objects under rich interactions remains a fundamental challenge for real-to-sim robot manipulation, with dynamics jointly driven by environmental effects and robot actions. Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control, limiting accuracy, stability, and generalization. This paper presents SoMA, a 3D Gaussian Splat simulator for soft-body manipulation. SoMA couples deformable dynamics, environmental forces, and robot joint actions in a unified latent neural space for end-to-end real-to-sim simulation. Modeling interactions over learned Gaussian splats enables controllable, stable long-horizon manipulation and generalization beyond observed trajectories without predefined physical models. SoMA improves resimulation accuracy and generalization on real-world robot manipulation by 20%, enabling stable simulation of complex tasks such as long-horizon cloth folding.

</details>


### [533] [Multi-Agent Monte Carlo Tree Search for Makespan-Efficient Object Rearrangement in Cluttered Spaces](https://arxiv.org/abs/2602.02411)
*Hanwen Ren,Junyong Kim,Aathman Tharmasanthiran,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: 该论文提出了一种面向多机器人在复杂环境下高效重排物体的新方法，在仿真和实际系统中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的物体重排问题经常面临非单调性（物体间相互阻挡），需要复杂的合作和中间移动，但以往研究多聚焦于单调情形，缺乏对具有广泛实际意义的多机器人协作、非单调环境问题的有效解决方案。

Method: 提出了中央控制的异步多智能体蒙特卡洛树搜索（CAM-MCTS）框架，将中央任务分配（各机器人互知计划，相互配合优化全局效率）与异步执行（通过一步前瞻成本估计，机器人无需等待他人而能自主承担新任务）结合，有效减少机器人空闲与等待时间。

Result: 在各种复杂、混乱的单调与非单调任务中，CAM-MCTS在makespan指标上优于强对比基线算法。同时，在真实多机器人系统多配置下测试，表现出有效性与鲁棒性。

Conclusion: CAM-MCTS能够高效、鲁棒地解决多机器人在复杂环境下的物体重排问题，明显降低任务总时间，具备实际应用潜力。

Abstract: Object rearrangement planning in complex, cluttered environments is a common challenge in warehouses, households, and rescue sites. Prior studies largely address monotone instances, whereas real-world tasks are often non-monotone-objects block one another and must be temporarily relocated to intermediate positions before reaching their final goals. In such settings, effective multi-agent collaboration can substantially reduce the time required to complete tasks. This paper introduces Centralized, Asynchronous, Multi-agent Monte Carlo Tree Search (CAM-MCTS), a novel framework for general-purpose makespan-efficient object rearrangement planning in challenging environments. CAM-MCTS combines centralized task assignment-where agents remain aware of each other's intended actions to facilitate globally optimized planning-with an asynchronous task execution strategy that enables agents to take on new tasks at appropriate time steps, rather than waiting for others, guided by a one-step look-ahead cost estimate. This design minimizes idle time, prevents unnecessary synchronization delays, and enhances overall system efficiency. We evaluate CAM-MCTS across a diverse set of monotone and non-monotone tasks in cluttered environments, demonstrating consistent reductions in makespan compared to strong baselines. Finally, we validate our approach on a real-world multi-agent system under different configurations, further confirming its effectiveness and robustness.

</details>


### [534] [3D Foundation Model-Based Loop Closing for Decentralized Collaborative SLAM](https://arxiv.org/abs/2602.02430)
*Pierre-Yves Lajoie,Benjamin Ramtoula,Daniele De Martini,Giovanni Beltrame*

Main category: cs.RO

TL;DR: 本文提出了一种利用3D基础模型提升多机器人去中心化SLAM回环检测的方案，显著提高了在大视角变化情况下的定位与建图的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 由于多机器人SLAM面临由于不同视角导致的地图重叠难以识别的问题，激发了作者利用新兴的3D基础模型来增进不同机器人的协作与数据融合能力。

Method: 作者将强大的3D基础模型集成到现有C-SLAM管道中，通过单目图像对可靠估算机器人间的相对位姿，并提出强健的离群点剔除方法和高效的位姿图优化算法来消除尺度歧义。

Result: 与当前最先进的方法对比，作者的方法在定位与建图精度、计算和存储效率等方面均取得了明显提升，尤其适合大规模多机器人场景。

Conclusion: 该方案展示了3D基础模型在去中心化多机器人SLAM里巨大潜力，有望推动大规模、协作式机器人地图构建技术的发展。

Abstract: Decentralized Collaborative Simultaneous Localization And Mapping (C-SLAM) techniques often struggle to identify map overlaps due to significant viewpoint variations among robots. Motivated by recent advancements in 3D foundation models, which can register images despite large viewpoint differences, we propose a robust loop closing approach that leverages these models to establish inter-robot measurements. In contrast to resource-intensive methods requiring full 3D reconstruction within a centralized map, our approach integrates foundation models into existing SLAM pipelines, yielding scalable and robust multi-robot mapping. Our contributions include: (1) integrating 3D foundation models to reliably estimate relative poses from monocular image pairs within decentralized C-SLAM; (2) introducing robust outlier mitigation techniques critical to the use of these relative poses; and (3) developing specialized pose graph optimization formulations that efficiently resolve scale ambiguities. We evaluate our method against state-of-the-art approaches, demonstrating improvements in localization and mapping accuracy, alongside significant gains in computational and memory efficiency. These results highlight the potential of our approach for deployment in large-scale multi-robot scenarios.

</details>


### [535] [World-Gymnast: Training Robots with Reinforcement Learning in a World Model](https://arxiv.org/abs/2602.02454)
*Ansh Kumar Sharma,Yixiang Sun,Ninghao Lu,Yunzhe Zhang,Jiarao Liu,Sherry Yang*

Main category: cs.RO

TL;DR: 本论文提出了一种基于现实世界视频的世界模型驱动的机器人学习方法World-Gymnast，通过在视频世界模型中进行策略微调，有效提升了真实机器人操作的表现，相较于传统监督微调与软件仿真显著优越。


<details>
  <summary>Details</summary>
Motivation: 机器人依赖物理互动进行学习成本高昂，而目前主流的专家示范监督微调和仿真强化学习方法都存在数据量不足和仿真到现实落差的问题。近期世界模型的出现让基于真实视频数据的虚拟训练成为可能，因此作者希望探索基于世界模型训练策略，是否能更有效提升真实机器人性能。

Method: 方法提出了World-Gymnast：先用现实世界的视频—动作数据训练一个可控视频世界模型，然后在该模型上通过强化学习微调视觉—语言—动作策略，利用视觉—语言模型对生成的视频进行奖励评估；过程中还能基于多样化语言指令、不同场景进行训练和测试，并支持策略与世界模型的在线协同迭代优化。

Result: 在Bridge机器人平台上，World-Gymnast在任务成功率上相比监督微调方法提升高达18倍，相比软件仿真强化学习提升2倍，并展现了对于新场景、新指令和在线迭代优化的强大适应能力。

Conclusion: 学习现实世界数据驱动的世界模型，并在其上云端训练机器人策略，有望打破仿真到现实的瓶颈，为实现人人可用的家庭机器人铺平道路。

Abstract: Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation. With the recent emergence of world models learned from real-world video-action data, we ask the question of whether training a policy in a world model can be more effective than supervised learning or software simulation in achieving better real-robot performance. We propose World-Gymnast, which performs RL finetuning of a vision-language-action (VLA) policy by rolling out the policy in an action-conditioned video world model and rewarding the rollouts with a vision-language model (VLM). On the Bridge robot setup, World-Gymnast outperforms SFT by as much as 18x and outperforms software simulator by as much as 2x. More importantly, World-Gymnast demonstrates intriguing capabilities of RL with a world model, including training on diverse language instructions and novel scenes from the world model, test-time training in a novel scene, and online iterative world model and policy improvement. Our results suggest learning a world model and training robot policies in the cloud could be the key to bridging the gap between robots that work in demonstrations and robots that can work in anyone's household.

</details>


### [536] [Relationship-Aware Hierarchical 3D Scene Graph for Task Reasoning](https://arxiv.org/abs/2602.02456)
*Albert Gassol Puigjaner,Angelos Zacharia,Kostas Alexis*

Main category: cs.RO

TL;DR: 本文提出了一种增强型分层3D场景图，结合开放词汇特征和对象关系推理，利用视觉语言模型（VLM）与大语言模型（LLM）赋能机器人对复杂环境进行理解和任务推理。


<details>
  <summary>Details</summary>
Motivation: 传统SLAM及其扩展仅能实现度量重建和基本语义映射，难以捕捉更高层次的场景结构和对象间关系，限制了自主代理对环境的推理和行动能力。

Method: 作者提出了分层的3D场景图，整合多层级的开放词汇特征并支持对象关系推理。方法利用VLM推断语义关系，结合LLM与VLM的推理模块，从而更好地解释场景信息并赋能任务推理。

Result: 所提方法部署于四足机器人，在多个环境和任务中验证，展现了出色的场景理解和任务推理能力。

Conclusion: 结合3D场景图、VLM和LLM能够大幅提升机器人对复杂场景结构和任务的理解、推理与交互能力，为智能体自主导航和环境理解提供了更高层次的表达和能力。

Abstract: Representing and understanding 3D environments in a structured manner is crucial for autonomous agents to navigate and reason about their surroundings. While traditional Simultaneous Localization and Mapping (SLAM) methods generate metric reconstructions and can be extended to metric-semantic mapping, they lack a higher level of abstraction and relational reasoning. To address this gap, 3D scene graphs have emerged as a powerful representation for capturing hierarchical structures and object relationships. In this work, we propose an enhanced hierarchical 3D scene graph that integrates open-vocabulary features across multiple abstraction levels and supports object-relational reasoning. Our approach leverages a Vision Language Model (VLM) to infer semantic relationships. Notably, we introduce a task reasoning module that combines Large Language Models (LLM) and a VLM to interpret the scene graph's semantic and relational information, enabling agents to reason about tasks and interact with their environment more intelligently. We validate our method by deploying it on a quadruped robot in multiple environments and tasks, highlighting its ability to reason about them.

</details>


### [537] [TIC-VLA: A Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments](https://arxiv.org/abs/2602.02459)
*Zhiyu Huang,Yun Zhang,Johnson Liu,Rui Song,Chen Tang,Jiaqi Ma*

Main category: cs.RO

TL;DR: 本文提出了一种面向实时控制、具备延迟感知能力的视觉-语言-动作（VLA）模型TIC-VLA，能有效补偿语义推理过程的延迟，并通过仿真和实机实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型普遍假设语义推理与动作控制的时间完全同步，然而实际中，语义推理往往比动作执行存在本质延迟，导致在动态、以人为中心的环境中机器人无法准确、及时地响应复杂指令。作者希望解决因语义延迟带来的动作执行不一致问题。

Method: 作者提出TIC-VLA框架，通过引入一个延迟感知的语义-控制接口，在动作生成时综合考虑延迟同源的语义状态、显式延迟元数据和当前观测，从而补偿推理时延。此外，作者提出了一套延迟一致的训练流程，在模仿学习与强化学习过程中主动引入推理延迟，使训练与实际部署时的异步场景相匹配。实验平台方面，开发了DynaNav仿真套件，用于评估机器人在动态环境下的导航能力。

Result: 在仿真和真实机器人实验中，TIC-VLA模型在多秒级推理延迟条件下依然保持了鲁棒而高效的实时控制，其性能始终优于传统VLA基线模型。

Conclusion: TIC-VLA通过延迟感知的语义-控制耦合和一致性训练，显著提高了机器人系统在动态环境下、时延条件下的整体控制鲁棒性和指令响应能力。

Abstract: Robots in dynamic, human-centric environments must follow language instructions while maintaining real-time reactive control. Vision-language-action (VLA) models offer a promising framework, but they assume temporally aligned reasoning and control, despite semantic inference being inherently delayed relative to real-time action. We introduce Think-in-Control (TIC)-VLA, a latency-aware framework that explicitly models delayed semantic reasoning during action generation. TIC-VLA defines a delayed semantic-control interface that conditions action generation on delayed vision-language semantic states and explicit latency metadata, in addition to current observations, enabling policies to compensate for asynchronous reasoning. We further propose a latency-consistent training pipeline that injects reasoning inference delays during imitation learning and online reinforcement learning, aligning training with asynchronous deployment. To support realistic evaluation, we present DynaNav, a physics-accurate, photo-realistic simulation suite for language-guided navigation in dynamic environments. Extensive experiments in simulation and on a real robot show that TIC-VLA consistently outperforms prior VLA models while maintaining robust real-time control under multi-second reasoning latency. Project website: https://ucla-mobility.github.io/TIC-VLA/

</details>


### [538] [HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos](https://arxiv.org/abs/2602.02473)
*Yinhuai Wang,Qihan Zhao,Yuen Fui Lau,Runyi Yu,Hok Wai Tsui,Qifeng Chen,Jingbo Wang,Jiangmiao Pang,Ping Tan*

Main category: cs.RO

TL;DR: 本文提出了HumanX框架，可将人类视频转化为可泛化的类人机器人交互技能，无需任务特定奖励，并显著提升了技能迁移与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 制造类人机器人能够灵活适应真实多变环境和任务一直很困难，现有方法受到真实交互数据稀缺或需繁琐奖励设计的限制，难以大规模推广。作者希望通过新的架构打破这一瓶颈，让机器人能更高效地学习并泛化各种交互技能。

Method: 提出HumanX全栈框架，包括：1）XGen模块，能将人类视频转换为多样、物理合理的机器人交互数据并进行扩增；2）XMimic模块，一个统一的模仿学习架构，用于学习和泛化交互技能；整个过程无需为每个任务设计奖励。

Result: 在五大领域（篮球、足球、羽毛球、搬运、对抗）评估，HumanX学会了10项复杂技能，包括复杂篮球动作和与人持续传球等，这些技能可零样本迁移到实体类人机器人，并且泛化成功率比现有方法高8倍以上。

Conclusion: HumanX为机器人学习多样、泛化的真实世界交互技能提供了一种可扩展且无需任务特定奖励的途径，有望突破机器人交互智能发展的瓶颈。

Abstract: Enabling humanoid robots to perform agile and adaptive interactive tasks has long been a core challenge in robotics. Current approaches are bottlenecked by either the scarcity of realistic interaction data or the need for meticulous, task-specific reward engineering, which limits their scalability. To narrow this gap, we present HumanX, a full-stack framework that compiles human video into generalizable, real-world interaction skills for humanoids, without task-specific rewards. HumanX integrates two co-designed components: XGen, a data generation pipeline that synthesizes diverse and physically plausible robot interaction data from video while supporting scalable data augmentation; and XMimic, a unified imitation learning framework that learns generalizable interaction skills. Evaluated across five distinct domains--basketball, football, badminton, cargo pickup, and reactive fighting--HumanX successfully acquires 10 different skills and transfers them zero-shot to a physical Unitree G1 humanoid. The learned capabilities include complex maneuvers such as pump-fake turnaround fadeaway jumpshots without any external perception, as well as interactive tasks like sustained human-robot passing sequences over 10 consecutive cycles--learned from a single video demonstration. Our experiments show that HumanX achieves over 8 times higher generalization success than prior methods, demonstrating a scalable and task-agnostic pathway for learning versatile, real-world robot interactive skills.

</details>


### [539] [Flow Policy Gradients for Robot Control](https://arxiv.org/abs/2602.02481)
*Brent Yi,Hongsuk Choi,Himanshu Gaurav Singh,Xiaoyu Huang,Takara E. Truong,Carmelo Sferrazza,Yi Ma,Rocky Duan,Pieter Abbeel,Guanya Shi,Karen Liu,Angjoo Kanazawa*

Main category: cs.RO

TL;DR: 本文提出了一种基于flow matching的策略梯度方法，突破了传统基于似然的策略梯度在动作分布选择上的局限，在多个机器人控制任务和虚实转移中取得了更好效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于似然的策略梯度方法受限于可微分动作似然，需要输出简单分布（如高斯），限制了策略表达能力。迫切需要能够支持更复杂策略分布的新方法，以提升在复杂机器人控制任务中的表现。

Method: 作者将flow matching策略梯度框架应用到机器人控制，提出了改进的目标函数，使其能够在多种复杂机器人任务（如足式行走、人形机器人运动跟踪、操控任务等）中有效训练和微调更具表现力的策略。该方法绕过了动作似然计算步骤。并进行了消融实验和训练动态分析。

Result: 实验表明，所提方法在从零开始训练时能够充分利用flow表示进行探索，在微调方面较基线方法具有更好的鲁棒性。在多个机器人任务以及两种人形机器人虚实转移中取得了优异结果。

Conclusion: flow matching策略梯度突破了传统似然约束，显著提升了复杂机器人任务中的策略训练与微调效果，为表达能力更强的控制策略提供了新路径。

Abstract: Likelihood-based policy gradient methods are the dominant approach for training robot control policies from rewards. These methods rely on differentiable action likelihoods, which constrain policy outputs to simple distributions like Gaussians. In this work, we show how flow matching policy gradients -- a recent framework that bypasses likelihood computation -- can be made effective for training and fine-tuning more expressive policies in challenging robot control settings. We introduce an improved objective that enables success in legged locomotion, humanoid motion tracking, and manipulation tasks, as well as robust sim-to-real transfer on two humanoid robots. We then present ablations and analysis on training dynamics. Results show how policies can exploit the flow representation for exploration when training from scratch, as well as improved fine-tuning robustness over baselines.

</details>
