<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 217]
- [cs.CL](#cs.CL) [Total: 99]
- [cs.RO](#cs.RO) [Total: 55]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Randomized-MLP Regularization Improves Domain Adaptation and Interpretability in DINOv2](https://arxiv.org/abs/2511.05509)
*Joel Valdivia Ortega,Lorenz Lamm,Franziska Eckardt,Benedikt Schworm,Marion Jasnin,Tingying Peng*

Main category: cs.CV

TL;DR: 本文提出了一种新的正则化方法Randomized-MLP（RMLP），在微调DINOv2类视觉Transformer用于医学及自然影像时，既提升或保持性能，又提升注意力图的可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉Transformer（如DINOv2）在不同领域表现突出，但其自注意力机制和特征图的可解释性较差，尤其在医学影像领域，域偏移问题使性能和透明度下降。该问题需要新的方法提升模型的可解释性并适应不同域。

Method: 提出一种基于对比学习的RMLP（Randomized-MLP）正则化方法，将其应用于DINOv2模型的微调过程中，增强特征表征语义一致性，并通过数学分析探究RMLP的作用。

Result: 在医学和自然领域的数据集上，RMLP能够提升或维持模型下游任务性能，同时生成更加可解释的注意力图。

Conclusion: RMLP方法有助于提升ViT类模型的可解释性，并加深了对对比学习机制的理解，对医学影像等特殊场景具有实际意义。

Abstract: Vision Transformers (ViTs), such as DINOv2, achieve strong performance across
domains but often repurpose low-informative patch tokens in ways that reduce
the interpretability of attention and feature maps. This challenge is
especially evident in medical imaging, where domain shifts can degrade both
performance and transparency. In this paper, we introduce Randomized-MLP (RMLP)
regularization, a contrastive learning-based method that encourages more
semantically aligned representations. We use RMLPs when fine-tuning DINOv2 to
both medical and natural image modalities, showing that it improves or
maintains downstream performance while producing more interpretable attention
maps. We also provide a mathematical analysis of RMLPs, offering insights into
its role in enhancing ViT-based models and advancing our understanding of
contrastive learning.

</details>


### [2] [Token Is All You Need: Cognitive Planning through Sparse Intent Alignment](https://arxiv.org/abs/2511.05540)
*Shiyao Sang*

Main category: cs.CV

TL;DR: 本文提出在端到端自动驾驶中，无需对场景做详尽建模，仅用少量语义丰富的token即可实现高效规划。实验证明，该方法不仅达到了甚至超越了复杂建模方法的表现，有望推动自动驾驶系统向更高效智能的方向发展。


<details>
  <summary>Details</summary>
Motivation: 传统端到端自动驾驶方法普遍认为需要详尽的场景建模（如预测未来场景/多模态数据推断）来实现高性能，但这导致极高的计算和数据开销。作者试图挑战这一共识，探索是否可以用更简洁的语义表征完成高效决策与规划。

Method: 作者采用少量、语义丰富的token来表征场景信息，摒弃传统详尽还原场景的方法，在nuPlan基准上用基于感知的BEV（鸟瞰图）表示进行实验，并测试不同token与解码方式对轨迹预测性能的影响。

Result: 1）在无未来预测的情况下，稀疏token表征方法获得了0.548m的ADE（平均距离误差），接近或优于先前复杂方法的0.75m；2）引入未来token预测进一步将误差降至0.479m，较当前状态基线提升12.6%；3）显式重建损失在感知准确时无益反而可能有害。实验还观察到模型自适应关注任务相关语义而非死板时间点。

Conclusion: 实现高效自动驾驶无需详尽重建世界，仅需语义丰富的信息表征。该“token is all you need”原则为未来 cognitively inspired（认知驱动）自动驾驶系统奠定基础，将重心从机械反应转向理解和想象式规划，有望突破现有技术瓶颈。

Abstract: We challenge the long-standing assumption that exhaustive scene modeling is
required for high-performance end-to-end autonomous driving (E2EAD). Unlike
world-model approaches that rely on computationally intensive future scene
generation or vision-language-action (VLA) systems constrained by Markov
assumptions, we show that a minimal set of semantically rich tokens is
sufficient for effective planning. Experiments on the nuPlan benchmark (720
scenarios, over 11,000 samples) using perception-informed BEV representations
yield three key findings: (1) even without future prediction, our sparse
representation achieves 0.548 m ADE, comparable to or surpassing prior methods
reporting around 0.75 m on nuScenes; (2) conditioning trajectory decoding on
predicted future tokens reduces ADE to 0.479 m, a 12.6% improvement over
current-state baselines; and (3) explicit reconstruction loss offers no benefit
and may degrade performance under reliable perception inputs. Notably, we
observe the emergence of temporal fuzziness, where the model adaptively attends
to task-relevant semantics rather than aligning rigidly to fixed timestamps,
providing a cognitive advantage for planning under uncertainty. Our "token is
all you need" principle marks a paradigm shift from reconstructing the world to
understanding it, laying a foundation for cognitively inspired systems that
plan through imagination rather than reaction.

</details>


### [3] [Automated Invoice Data Extraction: Using LLM and OCR](https://arxiv.org/abs/2511.05547)
*Advait Thakur,Khushi Khanchandani,Akshita Shetty,Chaitravi Reddy,Ritisa Behera*

Main category: cs.CV

TL;DR: 本文介绍了一种结合OCR、深度学习、LLM和图分析等多种AI技术的新型平台，实现了对票据信息的高质量自动提取。


<details>
  <summary>Details</summary>
Motivation: 传统OCR系统在处理票据等结构多变、包含手写内容及低质量扫描件时表现有限，主要受限于模板依赖，难以适应多样文档格式和复杂布局，因此亟需更智能、灵活的解决方案。

Method: 本文提出的AI平台融合了OCR、卷积神经网络（CNN）、Transformers、大型语言模型（LLM）和图分析，以提升实体识别、上下文理解和信息抽取能力，特别适应多种票据文档。

Result: 所述方法在版面分析、实体识别和信息抽取准确率方面较传统方法显著提升，实现了更高的自动化和文档兼容性。

Conclusion: 融合多种AI技术的智能平台能极大提升票据信息抽取的准确性和一致性，显著减少人工干预，推动行业最佳实践向自动化和智能化发展。

Abstract: Conventional Optical Character Recognition (OCR) systems are challenged by
variant invoice layouts, handwritten text, and low- quality scans, which are
often caused by strong template dependencies that restrict their flexibility
across different document structures and layouts. Newer solutions utilize
advanced deep learning models such as Convolutional Neural Networks (CNN) as
well as Transformers, and domain-specific models for better layout analysis and
accuracy across various sections over varied document types. Large Language
Models (LLMs) have revolutionized extraction pipelines at their core with
sophisticated entity recognition and semantic comprehension to support complex
contextual relationship mapping without direct programming specification.
Visual Named Entity Recognition (NER) capabilities permit extraction from
invoice images with greater contextual sensitivity and much higher accuracy
rates than older approaches. Existing industry best practices utilize hybrid
architectures that blend OCR technology and LLM for maximum scalability and
minimal human intervention. This work introduces a holistic Artificial
Intelligence (AI) platform combining OCR, deep learning, LLMs, and graph
analytics to achieve unprecedented extraction quality and consistency.

</details>


### [4] [In-Context-Learning-Assisted Quality Assessment Vision-Language Models for Metal Additive Manufacturing](https://arxiv.org/abs/2511.05551)
*Qiaojie Zheng,Jiucai Zhang,Xiaoli Zhang*

Main category: cs.CV

TL;DR: 本论文提出了利用视觉-语言模型（VLMs）与上下文学习（ICL）方法，通过极少样本高效评估增材制造成品质量，且兼具决策透明性。


<details>
  <summary>Details</summary>
Motivation: 传统增材制造的视觉质量评估依赖大量专用数据和模型，数据采集与训练成本高且耗时。如何用更少的数据投入提升质量评估效率与透明性成为急需解决的问题。

Method: 作者利用VLMs的推理能力，引入ICL方法，将少量案例知识输入模型，避免了对大规模专用数据集的需求。测试了不同ICL样本采样策略，并在Gemini-2.5-flash与Gemma3:27b两种VLM上评估其在激光能量沉积过程中的质量分类表现。此外，提出两项新指标用于衡量模型给出的解释（知识相关性和理由有效性）。

Result: ICL辅助的VLM模型仅需极少样本即可达到传统机器学习模型的分类准确水平，并能生成可供人类理解的决策理由。提出的新评估指标有效量化了这些理由的质量。

Conclusion: 所提方法能在样本极少情况下，完成特定制造任务高准确度的自动质量评估，而且通过解释性增强了决策可信度，为制造领域提供了更高效、透明的质量控制新方案。

Abstract: Vision-based quality assessment in additive manufacturing often requires
dedicated machine learning models and application-specific datasets. However,
data collection and model training can be expensive and time-consuming. In this
paper, we leverage vision-language models' (VLMs') reasoning capabilities to
assess the quality of printed parts and introduce in-context learning (ICL) to
provide VLMs with necessary application-specific knowledge and demonstration
samples. This method eliminates the requirement for large application-specific
datasets for training models. We explored different sampling strategies for ICL
to search for the optimal configuration that makes use of limited samples. We
evaluated these strategies on two VLMs, Gemini-2.5-flash and Gemma3:27b, with
quality assessment tasks in wire-laser direct energy deposition processes. The
results show that ICL-assisted VLMs can reach quality classification accuracies
similar to those of traditional machine learning models while requiring only a
minimal number of samples. In addition, unlike traditional classification
models that lack transparency, VLMs can generate human-interpretable rationales
to enhance trust. Since there are no metrics to evaluate their interpretability
in manufacturing applications, we propose two metrics, knowledge relevance and
rationale validity, to evaluate the quality of VLMs' supporting rationales. Our
results show that ICL-assisted VLMs can address application-specific tasks with
limited data, achieving relatively high accuracy while also providing valid
supporting rationales for improved decision transparency.

</details>


### [5] [EVLP:Learning Unified Embodied Vision-Language Planner with Reinforced Supervised Fine-Tuning](https://arxiv.org/abs/2511.05553)
*Xinyan Cai,Shiguang Wu,Dafeng Chi,Yuzheng Zhuang,Xingyue Quan,Jianye Hao,Qiang Guan*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的多模态统一生成框架EVLP，实现复杂长时操控任务中的语言推理和视觉想象一体化多模态规划。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多模态规划中采用分离的生成方式，导致计划不一致，无法高效精准地处理需要文本推理和视觉空间想象的复杂任务。

Method: 提出EVLP框架，采用三大创新：1）统一多模态生成框架，融合语义与空间信息并通过可学习的跨模态注意力联合建模语言和视觉；2）动态感知预训练，通过双向动态对齐策略强化多模态特征空间相关性；3）强化监督微调，通过空间逻辑一致性奖励提升文本与生成图像间的空间规划能力。

Result: 通过创新的训练流程和模型架构，EVLP能实现高效、一致、空间感知的多模态长时任务规划。

Conclusion: EVLP框架有效提升了复杂多模态任务的规划一致性和空间感知能力，有望为实际 embodied manipulation 任务带来更强的表现。

Abstract: In complex embodied long-horizon manipulation tasks, effective task
decomposition and execution require synergistic integration of textual logical
reasoning and visual-spatial imagination to ensure efficient and accurate
operation. Current methods fail to adopt a unified generation framework for
multimodal planning, lead to inconsistent in multimodal planning. To address
this challenge, we present \textbf{EVLP (Embodied Vision-Language Planner)}, an
innovative multimodal unified generation framework that jointly models
linguistic reasoning and visual generation. Our approach achieves multimodal
planning for long-horizon tasks through a novel training pipeline incorporating
dynamic pretraining and reinforced alignment. Our core innovations consist of
three key components: \textbf{1) Unified Multimodal Generation Framework}: For
understanding, We integrate semantic information with spatial features to
provide comprehensive visual perception. For generation, we directly learn the
joint distribution of discrete images for one-step visual synthesis, enabling
coordinated language-visual modeling through learnable cross-modal attention
mechanisms. \textbf{2) Dynamic Perception Pretraining}: We propose a
bidirectional dynamic alignment strategy employing inverse dynamics tasks and
forward dynamics tasks, effectively strengthening multimodal correlations
within a unified feature space. \textbf{3) Reinforced Supervised Fine-Tuning}:
While conducting instruction-based fine-tuning in the unified generation space,
we construct a reinforce loss to align the spatial logic between textual
actions and generated images, enabling the model to acquire spatio-awared
multimodal planning capabilities.

</details>


### [6] [MCFCN: Multi-View Clustering via a Fusion-Consensus Graph Convolutional Network](https://arxiv.org/abs/2511.05554)
*Chenping Pei,Fadi Dornaika,Jingjun Bi*

Main category: cs.CV

TL;DR: 本文提出了一种基于融合一致性图卷积网络(MCFCN)的多视图聚类方法，通过端到端学习实现有效的多视图一致性表示，大幅提升了聚类性能，在八个基准数据集上达到了最优效果。


<details>
  <summary>Details</summary>
Motivation: 现有多视图聚类方法虽然基于子空间学习实现了一定的一致性表示，但往往忽视了数据固有的拓扑结构，且当前基于图神经网络的多视图聚类方法在图结构噪声干扰和视图一致性处理等方面存在不足，导致聚类效果受限。

Method: 本文提出MCFCN，通过端到端方式学习多视图数据的一致性图；采用特征融合模块和统一图结构适配器（UGA）增强各视图协同；引入相似度矩阵对齐损失（SMAL）和特征表示对齐损失（FRAL）指导模型优化，利用GCN保留跨视图拓扑一致性并提升类内结构，促进高效表示学习。

Result: MCFCN在八个多视图聚类基准数据集上表现优异，达到了当前最先进的聚类效果，能够更好地区分难以区分的样本，并通过大量定性及定量实验验证了算法有效性。

Conclusion: MCFCN方法可提升多视图聚类的表示一致性及拓扑结构保留能力，克服了传统方法处理噪声和视图协同不佳等问题，为多视图聚类领域提供了更强大有效的工具。

Abstract: Existing Multi-view Clustering (MVC) methods based on subspace learning focus
on consensus representation learning while neglecting the inherent topological
structure of data. Despite the integration of Graph Neural Networks (GNNs) into
MVC, their input graph structures remain susceptible to noise interference.
Methods based on Multi-view Graph Refinement (MGRC) also have limitations such
as insufficient consideration of cross-view consistency, difficulty in handling
hard-to-distinguish samples in the feature space, and disjointed optimization
processes caused by graph construction algorithms. To address these issues, a
Multi-View Clustering method via a Fusion-Consensus Graph Convolutional Network
(MCFCN) is proposed. The network learns the consensus graph of multi-view data
in an end-to-end manner and learns effective consensus representations through
a view feature fusion model and a Unified Graph Structure Adapter (UGA). It
designs Similarity Matrix Alignment Loss (SMAL) and Feature Representation
Alignment Loss (FRAL). With the guidance of consensus, it optimizes
view-specific graphs, preserves cross-view topological consistency, promotes
the construction of intra-class edges, and realizes effective consensus
representation learning with the help of GCN to improve clustering performance.
MCFCN demonstrates state-of-the-art performance on eight multi-view benchmark
datasets, and its effectiveness is verified by extensive qualitative and
quantitative implementations. The code will be provided at
https://github.com/texttao/MCFCN.

</details>


### [7] [Compressing Multi-Task Model for Autonomous Driving via Pruning and Knowledge Distillation](https://arxiv.org/abs/2511.05557)
*Jiayuan Wang,Q. M. Jonathan Wu,Ning Zhang,Katsuya Suto,Lei Zhong*

Main category: cs.CV

TL;DR: 本文提出一种结合任务感知通道剪枝和特征层知识蒸馏的多任务模型压缩框架，实现了在多任务全景感知任务中大幅减少模型参数并保证精度，可用于自动驾驶系统实时部署。


<details>
  <summary>Details</summary>
Motivation: 多任务学习将物体检测、可行驶区域分割和车道线分割集成于同一模型，虽可提升效果但导致参数量和模型复杂度过高，不适于车载设备部署。因此需要有效的模型压缩技术。

Method: 方法包括两部分：一是结合基于泰勒展开的通道重要性评估和梯度冲突惩罚的安全剪枝，去除冗余和冲突通道，保留关键通道；二是任务头无关、以中间骨干和编码器特征为指导的知识蒸馏，将教师模型中间特征迁移到学生模型以缓解剪枝带来的性能损失。

Result: 在BDD100K数据集上，压缩后的模型参数减少32.7%，分割精度基本不损失，检测性能仅小幅下降（召回率降低1.2%、mAP50降低1.8%），仍能实时运行（32.7 FPS）。

Conclusion: 结合剪枝与知识蒸馏能够高效压缩多任务全景感知模型，在保证精度的同时显著减少模型规模，有利于其在自动驾驶实时系统中的部署。

Abstract: Autonomous driving systems rely on panoptic perception to jointly handle
object detection, drivable area segmentation, and lane line segmentation.
Although multi-task learning is an effective way to integrate these tasks, its
increasing model parameters and complexity make deployment on on-board devices
difficult. To address this challenge, we propose a multi-task model compression
framework that combines task-aware safe pruning with feature-level knowledge
distillation. Our safe pruning strategy integrates Taylor-based channel
importance with gradient conflict penalty to keep important channels while
removing redundant and conflicting channels. To mitigate performance
degradation after pruning, we further design a task head-agnostic distillation
method that transfers intermediate backbone and encoder features from a teacher
to a student model as guidance. Experiments on the BDD100K dataset demonstrate
that our compressed model achieves a 32.7% reduction in parameters while
segmentation performance shows negligible accuracy loss and only a minor
decrease in detection (-1.2% for Recall and -1.8% for mAP50) compared to the
teacher. The compressed model still runs at 32.7 FPS in real-time. These
results show that combining pruning and knowledge distillation provides an
effective compression solution for multi-task panoptic perception.

</details>


### [8] [FilletRec: A Lightweight Graph Neural Network with Intrinsic Features for Automated Fillet Recognition](https://arxiv.org/abs/2511.05561)
*Jiali Gao,Taoran Liu,Hongfei Ye,Jianjun Chen*

Main category: cs.CV

TL;DR: 本文提出了一种专门针对CAD模型中圆角特征识别与简化的端到端数据驱动框架，并发布了大规模基准数据集及高效的图神经网络FilletRec，实现了高精度、高泛化能力和高效的自动化完整流程。


<details>
  <summary>Details</summary>
Motivation: CAD模型中的圆角特征识别与简化对于CAE分析至关重要，但现有方法在复杂场景下泛化性和准确率不足，因此亟需一种高效、专用的解决方案。

Method: 作者首先构建并公开了一个大规模、多样化的圆角识别基准数据集，然后提出了FilletRec——一种轻量级的图神经网络模型。核心创新在于采用姿态无关的内在几何特征（如曲率）进行学习，从而更好地捕捉和识别复杂拓扑下的几何特征。最后，集成了有效的几何简化算法，实现从识别到简化的自动化流程。

Result: 实验结果显示，FilletRec在准确率和泛化能力方面优于现有方法，同时模型参数量仅为基线模型的0.2%-5.4%，展现了极高的模型效率。

Conclusion: 提出的方法在CAD模型圆角特征识别和简化任务中显著提升了性能，并通过新数据集和网络设计推动了该领域的发展，极具实际应用价值。

Abstract: Automated recognition and simplification of fillet features in CAD models is
critical for CAE analysis, yet it remains an open challenge. Traditional
rule-based methods lack robustness, while existing deep learning models suffer
from poor generalization and low accuracy on complex fillets due to their
generic design and inadequate training data. To address these issues, this
paper proposes an end-to-end, data-driven framework specifically for fillet
features. We first construct and release a large-scale, diverse benchmark
dataset for fillet recognition to address the inadequacy of existing data.
Based on it, we propose FilletRec, a lightweight graph neural network. The core
innovation of this network is its use of pose-invariant intrinsic geometric
features, such as curvature, enabling it to learn more fundamental geometric
patterns and thereby achieve high-precision recognition of complex geometric
topologies. Experiments show that FilletRec surpasses state-of-the-art methods
in both accuracy and generalization, while using only 0.2\%-5.4\% of the
parameters of baseline models, demonstrating high model efficiency. Finally,
the framework completes the automated workflow from recognition to
simplification by integrating an effective geometric simplification algorithm.

</details>


### [9] [M2S2L: Mamba-based Multi-Scale Spatial-temporal Learning for Video Anomaly Detection](https://arxiv.org/abs/2511.05564)
*Yang Liu,Boan Chen,Xiaoguang Zhu,Jing Liu,Peng Sun,Wei Zhou*

Main category: cs.CV

TL;DR: 该论文提出了一个基于Mamba结构的多尺度时空学习（M2S2L）框架，用于视频异常检测，既提升了检测准确率，也保证了计算效率。


<details>
  <summary>Details</summary>
Motivation: 随着视频内容的复杂性增加，传统视频异常检测方法在复杂场景下鲁棒性不佳，且难以兼顾实时处理所需的高效性。因此，需要设计一种能在有限算力条件下实现高准确率的检测方法。

Method: M2S2L框架利用分层空间编码器对多级粒度空间信息建模，并用多时序编码器抓取不同时间尺度的运动动态，还引入特征分解机制对外观和运动重建分别进行任务优化，使得异常行为建模更加细致。

Result: 在UCSD Ped2、CUHK Avenue和ShanghaiTech三个基准数据集上分别取得了98.5%、92.1%和77.9%的帧级AUC，同时保持了20.1G FLOPs和45 FPS的推理速度，展现出较高的效率。

Conclusion: M2S2L方法能有效提升现代视频监控系统中的异常检测准确率，并兼具实时处理能力，具备实际应用价值。

Abstract: Video anomaly detection (VAD) is an essential task in the image processing
community with prospects in video surveillance, which faces fundamental
challenges in balancing detection accuracy with computational efficiency. As
video content becomes increasingly complex with diverse behavioral patterns and
contextual scenarios, traditional VAD approaches struggle to provide robust
assessment for modern surveillance systems. Existing methods either lack
comprehensive spatial-temporal modeling or require excessive computational
resources for real-time applications. In this regard, we present a Mamba-based
multi-scale spatial-temporal learning (M2S2L) framework in this paper. The
proposed method employs hierarchical spatial encoders operating at multiple
granularities and multi-temporal encoders capturing motion dynamics across
different time scales. We also introduce a feature decomposition mechanism to
enable task-specific optimization for appearance and motion reconstruction,
facilitating more nuanced behavioral modeling and quality-aware anomaly
assessment. Experiments on three benchmark datasets demonstrate that M2S2L
framework achieves 98.5%, 92.1%, and 77.9% frame-level AUCs on UCSD Ped2, CUHK
Avenue, and ShanghaiTech respectively, while maintaining efficiency with 20.1G
FLOPs and 45 FPS inference speed, making it suitable for practical surveillance
deployment.

</details>


### [10] [In-Context Adaptation of VLMs for Few-Shot Cell Detection in Optical Microscopy](https://arxiv.org/abs/2511.05565)
*Shreyan Ganguly,Angona Biswas,Jaydeep Rade,Md Hasibul Hasan Hasib,Nabila Masud,Nitish Singla,Abhipsa Dash,Ushashi Bhattacharjee,Aditya Balu,Anwesha Sarkar,Adarsh Krishnamurthy,Soumik Sarkar*

Main category: cs.CV

TL;DR: 该论文探讨了主流视觉-语言大模型（VLMs）在生物医学显微图像上的应用，通过提出全新数据集Micro-OD，系统评估了VLM在显微图像少样本目标检测上的表现，并展示了in-context learning的实际价值。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在自然图像任务中表现优异，但其在医学显微镜图像上的能力尚未深入研究。而在实际医学场景中，缺乏大规模标注数据集，如何利用少量样本实现有效目标检测成为亟需解决的问题。

Method: 作者提出了Micro-OD基准，涵盖252张显微镜图像，包含11种细胞类型，并系统性地评估了8种VLM在少样本目标检测下的表现。此外，提出了结合检测头与VLM分类器的混合FSOD流程，并对含/不含推理token的不同模型变体进行了横向对比。

Result: 实验发现，在零样本（zero-shot）条件下，VLM性能有限（由于领域差异）；但仅有少量支持样本（few-shot）就可显著提升检测效果，且6个样本后提升幅度变小。有推理token的模型在端到端定位上更优，而结构更简单的模型适合已预定位图像的分类。

Conclusion: 通过in-context适应可以有效提升VLM在医学显微检测中的实用性，相关基准为生物医学成像领域开拓了开放词汇检测的研究路径，并为后续研究提供了标准化测试平台。

Abstract: Foundation vision-language models (VLMs) excel on natural images, but their
utility for biomedical microscopy remains underexplored. In this paper, we
investigate how in-context learning enables state-of-the-art VLMs to perform
few-shot object detection when large annotated datasets are unavailable, as is
often the case with microscopic images. We introduce the Micro-OD benchmark, a
curated collection of 252 images specifically curated for in-context learning,
with bounding-box annotations spanning 11 cell types across four sources,
including two in-lab expert-annotated sets. We systematically evaluate eight
VLMs under few-shot conditions and compare variants with and without implicit
test-time reasoning tokens. We further implement a hybrid Few-Shot Object
Detection (FSOD) pipeline that combines a detection head with a VLM-based
few-shot classifier, which enhances the few-shot performance of recent VLMs on
our benchmark. Across datasets, we observe that zero-shot performance is weak
due to the domain gap; however, few-shot support consistently improves
detection, with marginal gains achieved after six shots. We observe that models
with reasoning tokens are more effective for end-to-end localization, whereas
simpler variants are more suitable for classifying pre-localized crops. Our
results highlight in-context adaptation as a practical path for microscopy, and
our benchmark provides a reproducible testbed for advancing open-vocabulary
detection in biomedical imaging.

</details>


### [11] [Efficient Online Continual Learning in Sensor-Based Human Activity Recognition](https://arxiv.org/abs/2511.05566)
*Yao Zhang,Souza Leite Clayton,Yu Xiao*

Main category: cs.CV

TL;DR: 本论文提出了PTRN-HAR方法，是首个将基于预训练模型（PTM）的在线持续学习（OCL）方法成功应用于传感器人类活动识别（HAR）领域。该方法通过冻结预训练的特征提取器和引入关系模块网络，降低了计算资源需求，并用更少的标注数据取得了优异的表现，在三大公开数据集上超越当前最优模型。


<details>
  <summary>Details</summary>
Motivation: 传感器人类活动识别模型部署后需要适应新活动和用户变化。然而，现有OCL方法计算量大且标注数据需求多，不适合实际应用。计算机视觉领域PTM-OCL大幅提高了泛化和效率，但直接迁移到传感器HAR领域面临异构性高和标注稀缺等难题，因此需创新方法解决现实问题。

Method: 作者提出PTRN-HAR方法：1）用对比损失在有限数据下预训练特征抽取器并在流式学习阶段冻结；2）用关系模块网络替代传统全连接分类器。这样既减少了计算和内存消耗，又提升了小样本情况下的持续学习效果。

Result: 实验在三个公开传感器HAR数据集上进行，PTRN-HAR在保持高精度的同时，大幅减少了对标注数据和计算资源的依赖，在所有数据集上超越了当前最优状态。

Conclusion: PTRN-HAR首次将PTM-OCL成功引入传感器HAR任务，显著提升数据与计算效率。该方法为后续低资源持续学习方案在实际场景部署提供了可行路径。

Abstract: Machine learning models for sensor-based human activity recognition (HAR) are
expected to adapt post-deployment to recognize new activities and different
ways of performing existing ones. To address this need, Online Continual
Learning (OCL) mechanisms have been proposed, allowing models to update their
knowledge incrementally as new data become available while preserving
previously acquired information. However, existing OCL approaches for
sensor-based HAR are computationally intensive and require extensive labeled
samples to represent new changes. Recently, pre-trained model-based (PTM-based)
OCL approaches have shown significant improvements in performance and
efficiency for computer vision applications. These methods achieve strong
generalization capabilities by pre-training complex models on large datasets,
followed by fine-tuning on downstream tasks for continual learning. However,
applying PTM-based OCL approaches to sensor-based HAR poses significant
challenges due to the inherent heterogeneity of HAR datasets and the scarcity
of labeled data in post-deployment scenarios. This paper introduces PTRN-HAR,
the first successful application of PTM-based OCL to sensor-based HAR. Unlike
prior PTM-based OCL approaches, PTRN-HAR pre-trains the feature extractor using
contrastive loss with a limited amount of data. This extractor is then frozen
during the streaming stage. Furthermore, it replaces the conventional dense
classification layer with a relation module network. Our design not only
significantly reduces the resource consumption required for model training
while maintaining high performance, but also improves data efficiency by
reducing the amount of labeled data needed for effective continual learning, as
demonstrated through experiments on three public datasets, outperforming the
state-of-the-art. The code can be found here:
https://anonymous.4open.science/r/PTRN-HAR-AF60/

</details>


### [12] [Automatic Extraction of Road Networks by using Teacher-Student Adaptive Structural Deep Belief Network and Its Application to Landslide Disaster](https://arxiv.org/abs/2511.05567)
*Shin Kamada,Takumi Ichimura*

Main category: cs.CV

TL;DR: 本文提出了一种自适应结构学习的深度信念网络，用于自动识别和生成道路网络地图。实验显示该方法在复杂场景下精度大幅提升，并实现了轻量化部署。


<details>
  <summary>Details</summary>
Motivation: 现有道路识别系统难以应对地图中复杂特征，模型表达能力有限，且在灾害等特殊场景下难以快速部署与推理。

Method: 提出了一种基于教师—学生集成学习、自适应深度信念网络（DBN）的RoadTracer方法，通过神经元生成-消失、层生成算法自动调整网络结构，以提升对复杂道路特征的识别能力。同时开发了轻量级模型用于嵌入式边缘设备，实现快速推理。

Result: 在七个主要城市的测试中，该方法的检测准确率从40.0%提高到89.0%。在日本自然灾害发生后的卫星图像上，成功检测可用道路，验证了模型的适应性和实用性。

Conclusion: 所提出方法不仅极大提升了道路检测的准确性，还能在灾害场景下作为快速辅助决策工具，且支持轻量化部署，具备实际应用前景。

Abstract: An adaptive structural learning method of Restricted Boltzmann Machine (RBM)
and Deep Belief Network (DBN) has been developed as one of prominent deep
learning models. The neuron generation-annihilation algorithm in RBM and layer
generation algorithm in DBN make an optimal network structure for given input
during the learning. In this paper, our model is applied to an automatic
recognition method of road network system, called RoadTracer. RoadTracer can
generate a road map on the ground surface from aerial photograph data. A novel
method of RoadTracer using the Teacher-Student based ensemble learning model of
Adaptive DBN is proposed, since the road maps contain many complicated features
so that a model with high representation power to detect should be required.
The experimental results showed the detection accuracy of the proposed model
was improved from 40.0\% to 89.0\% on average in the seven major cities among
the test dataset. In addition, we challenged to apply our method to the
detection of available roads when landslide by natural disaster is occurred, in
order to rapidly obtain a way of transportation. For fast inference, a small
size of the trained model was implemented on a small embedded edge device as
lightweight deep learning. We reported the detection results for the satellite
image before and after the rainfall disaster in Japan.

</details>


### [13] [Do Street View Imagery and Public Participation GIS align: Comparative Analysis of Urban Attractiveness](https://arxiv.org/abs/2511.05570)
*Milad Malekzadeh,Elias Willberg,Jussi Torkko,Silviya Korpilo,Kamyar Hasanzadeh,Olle Järv,Tuuli Toivonen*

Main category: cs.CV

TL;DR: 本文比较了街景影像(SVI)与公共参与式地理信息系统(PPGIS)在捕捉城市感知体验中的一致性，发现二者只能部分一致，强调应结合两者更全面理解城市感知。


<details>
  <summary>Details</summary>
Motivation: 随着数字工具在空间规划中的普及，理解不同数据源如何反映城市环境中的人类体验变得重要。尤其是SVI与PPGIS作为城市感知收集手段的可比性尚未得到充分探讨。

Method: 通过赫尔辛基市的PPGIS调查和参与者评分的SVI数据，利用语义分割与机器学习，预测街景影像感受分，并与PPGIS标记的吸引/不吸引位置比较，用严格和宽松两种标准计算一致性。同时分析了包括噪音、交通、人口和用地在内的非视觉变量对结果的影响。

Result: SVI与PPGIS数据仅部分一致。在宽松标准下，对“吸引”与“不吸引”地的一致率分别为67%和77%；在严格标准下分别降至27%和29%。非视觉信息（如活动水平和环境压力）导致部分不一致。

Conclusion: SVI作为城市感知的可扩展视觉代理手段有一定价值，但无法取代PPGIS等直接反映体验丰富性的工具。建议两者结合，才能更全面地把握和服务于城市环境感知。

Abstract: As digital tools increasingly shape spatial planning practices, understanding
how different data sources reflect human experiences of urban environments is
essential. Street View Imagery (SVI) and Public Participation GIS (PPGIS)
represent two prominent approaches for capturing place-based perceptions that
can support urban planning decisions, yet their comparability remains
underexplored. This study investigates the alignment between SVI-based
perceived attractiveness and residents' reported experiences gathered via a
city-wide PPGIS survey in Helsinki, Finland. Using participant-rated SVI data
and semantic image segmentation, we trained a machine learning model to predict
perceived attractiveness based on visual features. We compared these
predictions to PPGIS-identified locations marked as attractive or unattractive,
calculating agreement using two sets of strict and moderate criteria. Our
findings reveal only partial alignment between the two datasets. While
agreement (with a moderate threshold) reached 67% for attractive and 77% for
unattractive places, agreement (with a strict threshold) dropped to 27% and
29%, respectively. By analysing a range of contextual variables, including
noise, traffic, population presence, and land use, we found that non-visual
cues significantly contributed to mismatches. The model failed to account for
experiential dimensions such as activity levels and environmental stressors
that shape perceptions but are not visible in images. These results suggest
that while SVI offers a scalable and visual proxy for urban perception, it
cannot fully substitute the experiential richness captured through PPGIS. We
argue that both methods are valuable but serve different purposes; therefore, a
more integrated approach is needed to holistically capture how people perceive
urban environments.

</details>


### [14] [C3-Diff: Super-resolving Spatial Transcriptomics via Cross-modal Cross-content Contrastive Diffusion Modelling](https://arxiv.org/abs/2511.05571)
*Xiaofei Wang,Stephen Price,Chao Li*

Main category: cs.CV

TL;DR: 本研究提出了C3-Diff方法，通过跨模态对比扩散学习与组织切片图像引导，显著提升空间转录组（ST）分辨率和基因表达预测能力。


<details>
  <summary>Details</summary>
Motivation: 当前空间转录组（ST）技术受制于低分辨率，阻碍了对基因空间表达的深入理解。提升分辨率的超分技术一般需结合组织学图像与基因表达数据，但二者交互建模存在挑战。

Method: 提出C3-Diff框架，通过跨模态跨内容对比学习提取ST和组织图像的模态无关与内容无关特征；采用基于扰动的信息增强方法提升低灵敏度ST特征丰富度；动态交叉模态填补训练以缓解ST数据稀缺问题。

Result: 在四个公开空间转录组数据集上，C3-Diff方法超越了现有竞争方法；下游任务如细胞类型定位、基因表达关联及单细胞级基因表达预测表现均显著提升。

Conclusion: C3-Diff有效提升了ST数据的分辨率和基因表达解析能力，为空间组学、AI增强生物技术和临床应用开辟了新途径。

Abstract: The rapid advancement of spatial transcriptomics (ST), i.e., spatial gene
expressions, has made it possible to measure gene expression within original
tissue, enabling us to discover molecular mechanisms. However, current ST
platforms frequently suffer from low resolution, limiting the in-depth
understanding of spatial gene expression. Super-resolution approaches promise
to enhance ST maps by integrating histology images with gene expressions of
profiled tissue spots. However, it remains a challenge to model the
interactions between histology images and gene expressions for effective ST
enhancement. This study presents a cross-modal cross-content contrastive
diffusion framework, called C3-Diff, for ST enhancement with histology images
as guidance. In C3-Diff, we firstly analyze the deficiency of traditional
contrastive learning paradigm, which is then refined to extract both
modal-invariant and content-invariant features of ST maps and histology images.
Further, to overcome the problem of low sequencing sensitivity in ST maps, we
perform nosing-based information augmentation on the surface of feature unit
hypersphere. Finally, we propose a dynamic cross-modal imputation-based
training strategy to mitigate ST data scarcity. We tested C3-Diff by
benchmarking its performance on four public datasets, where it achieves
significant improvements over competing methods. Moreover, we evaluate C3-Diff
on downstream tasks of cell type localization, gene expression correlation and
single-cell-level gene expression prediction, promoting AI-enhanced
biotechnology for biomedical research and clinical applications. Codes are
available at https://github.com/XiaofeiWang2018/C3-Diff.

</details>


### [15] [Video Text Preservation with Synthetic Text-Rich Videos](https://arxiv.org/abs/2511.05573)
*Ziyang Liu,Kevin Valencia,Justin Cui*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级方法，通过合成监督提升文本到视频（T2V）扩散模型中生成文本的清晰度与一致性。实验结果显示，采用该方法可有效改善视频中短文本的可读性并增强长文本的结构一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管T2V模型发展迅速，但在视频中生成清晰且连贯文本仍然具有挑战，现有方法难以处理短语或单词，且计算开销大，不适于视频生成。因此需要一种高效的新方法提升视频文本的表现。

Method: 使用T2I扩散模型生成富含文本的图像，再通过不敏感文本的I2V模型将其动画化为短视频，进而得到合成视频-提示对，无需改动T2V模型结构，即可通过微调进行改进。

Result: 方法提升了短文本的可读性及时序一致性，对于长文本生成也展现了一定的结构先验，大幅度改进T2V模型在文本生成上的效果。

Conclusion: 精心设计的合成数据和弱监督是提升T2V文本真实性的实际可行途径，无需对模型结构做出复杂更改即可获得性能提升。

Abstract: While Text-To-Video (T2V) models have advanced rapidly, they continue to
struggle with generating legible and coherent text within videos. In
particular, existing models often fail to render correctly even short phrases
or words and previous attempts to address this problem are computationally
expensive and not suitable for video generation. In this work, we investigate a
lightweight approach to improve T2V diffusion models using synthetic
supervision. We first generate text-rich images using a text-to-image (T2I)
diffusion model, then animate them into short videos using a text-agnostic
image-to-video (I2v) model. These synthetic video-prompt pairs are used to
fine-tune Wan2.1, a pre-trained T2V model, without any architectural changes.
Our results show improvement in short-text legibility and temporal consistency
with emerging structural priors for longer text. These findings suggest that
curated synthetic data and weak supervision offer a practical path toward
improving textual fidelity in T2V generation.

</details>


### [16] [Elements of Active Continuous Learning and Uncertainty Self-Awareness: a Narrow Implementation for Face and Facial Expression Recognition](https://arxiv.org/abs/2511.05574)
*Stanislav Selitskiy*

Main category: cs.CV

TL;DR: 本文提出了一种仿照自我意识机制的方法，通过一个监督神经网络来监控另一个主神经网络的激活模式，从而判断其预测的可信度，并在高不确定性时主动寻求人类帮助。


<details>
  <summary>Details</summary>
Motivation: 人类智能的重要特点是能反思并修正自身思维过程。实现人工通用智能，也需在机器学习算法中建模这种反思和自我修正的能力。

Method: 作者设计了一个“自我意识”机制，具体是一个监督型ANN监控底层ANN（卷积神经网络集成系统用于人脸识别和表情识别）的激活模式。监督ANN有记忆区域存储历史表现信息，并在训练过程中通过调整参数优化预测可信度。

Result: 监督ANN能识别出主ANN高不确定性的预测，在这些情况下系统会启动主动学习模式，通过主动请求人类干预以改进学习过程。

Conclusion: 该机制为窄域机器学习算法赋予了简单的自我意识能力，使其实现自我反思与主动修正，为迈向人工通用智能提供了新的技术路径。

Abstract: Reflection on one's thought process and making corrections to it if there
exists dissatisfaction in its performance is, perhaps, one of the essential
traits of intelligence. However, such high-level abstract concepts mandatory
for Artificial General Intelligence can be modelled even at the low level of
narrow Machine Learning algorithms. Here, we present the self-awareness
mechanism emulation in the form of a supervising artificial neural network
(ANN) observing patterns in activations of another underlying ANN in a search
for indications of the high uncertainty of the underlying ANN and, therefore,
the trustworthiness of its predictions. The underlying ANN is a convolutional
neural network (CNN) ensemble employed for face recognition and facial
expression tasks. The self-awareness ANN has a memory region where its past
performance information is stored, and its learnable parameters are adjusted
during the training to optimize the performance. The trustworthiness verdict
triggers the active learning mode, giving elements of agency to the machine
learning algorithm that asks for human help in high uncertainty and confusion
conditions.

</details>


### [17] [DiffSwap++: 3D Latent-Controlled Diffusion for Identity-Preserving Face Swapping](https://arxiv.org/abs/2511.05575)
*Weston Bondurant,Arkaprava Sinha,Hieu Le,Srijan Das,Stephanie Schuckers*

Main category: cs.CV

TL;DR: DiffSwap++是一种新型基于扩散模型的人脸互换方法，通过引入3D人脸特征，使换脸结果在身份保持、几何一致性以及对人脸姿势与表情的适应性方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸互换方法（尤其是基于扩散模型的方法）在面对复杂姿态与表情时，常出现细节伪影和身份丢失的问题。其根源在于未有效利用3D人脸结构特征，导致身份与角度/表情难以解耦。

Method: 作者提出DiffSwap++，在扩散模型训练中融入了3D人脸潜在特征，并在去噪过程中结合身份嵌入和人脸关键点，增强换脸时的几何一致性与身份保持能力。

Result: 在CelebA、FFHQ、CelebV-Text等数据集上的实验显示，DiffSwap++在身份保真、目标姿态和表情保持方面超过了已有方法。还引入了生物识别评测方式与用户调查，进一步验证其真实性和有效性。

Conclusion: DiffSwap++通过结合3D人脸知识和多条件扩散架构，实现了更加自然、细致且身份一致的人脸互换，突显3D结构信息对换脸技术提升的关键作用。

Abstract: Diffusion-based approaches have recently achieved strong results in face
swapping, offering improved visual quality over traditional GAN-based methods.
However, even state-of-the-art models often suffer from fine-grained artifacts
and poor identity preservation, particularly under challenging poses and
expressions. A key limitation of existing approaches is their failure to
meaningfully leverage 3D facial structure, which is crucial for disentangling
identity from pose and expression. In this work, we propose DiffSwap++, a novel
diffusion-based face-swapping pipeline that incorporates 3D facial latent
features during training. By guiding the generation process with 3D-aware
representations, our method enhances geometric consistency and improves the
disentanglement of facial identity from appearance attributes. We further
design a diffusion architecture that conditions the denoising process on both
identity embeddings and facial landmarks, enabling high-fidelity and
identity-preserving face swaps. Extensive experiments on CelebA, FFHQ, and
CelebV-Text demonstrate that DiffSwap++ outperforms prior methods in preserving
source identity while maintaining target pose and expression. Additionally, we
introduce a biometric-style evaluation and conduct a user study to further
validate the realism and effectiveness of our approach. Code will be made
publicly available at https://github.com/WestonBond/DiffSwapPP

</details>


### [18] [Beyond Softmax: Dual-Branch Sigmoid Architecture for Accurate Class Activation Maps](https://arxiv.org/abs/2511.05590)
*Yoojin Oh,Junhyug Noh*

Main category: cs.CV

TL;DR: 本文提出了一种基于双分支Sigmoid输出的新型Class Activation Mapping（CAM）方法，该方法克服了传统CAM依赖softmax分类器带来的失真问题，在定位与分类性能之间取得更优的平衡。


<details>
  <summary>Details</summary>
Motivation: 传统CAM及其变体依赖于最终的softmax分类头，导致热力图存在重要性分数失真（对数移位、符号混淆等问题），限制了可解释性和定位精度。作者意在消除这一结构性弊端，提高深度神经网络预测的可解释性。

Method: 作者提出将原有分类头克隆为两个分支：一个维持原softmax结构用于分类，另一个为按类sigmoid输出用于CAM热力图生成，并仅用二分类监督微调sigmoid分支，softmax分支参数冻结。推断时，分类依靠softmax分支，定位和解释由sigmoid分支CAM输出提供。该结构简单、易于集成到现有CAM扩展方法中，计算开销极小。

Result: 在CUB-200-2011、Stanford Cars（细粒度任务）和ImageNet-1K、OpenImages30K（WSOL基准）上实验证明，作者方法显著提升了解释热力图的保真度和Top-1定位表现，同时未对分类精度造成损失。

Conclusion: 通过将定位与分类解耦，本文提出的双分支sigmoid结构有效解决了softmax-CAM的固有缺陷，并在多个数据集和任务中取得更好的可解释性和定位性能，对视觉解释技术具有实际推动价值。

Abstract: Class Activation Mapping (CAM) and its extensions have become indispensable
tools for visualizing the evidence behind deep network predictions. However, by
relying on a final softmax classifier, these methods suffer from two
fundamental distortions: additive logit shifts that arbitrarily bias importance
scores, and sign collapse that conflates excitatory and inhibitory features. We
propose a simple, architecture-agnostic dual-branch sigmoid head that decouples
localization from classification. Given any pretrained model, we clone its
classification head into a parallel branch ending in per-class sigmoid outputs,
freeze the original softmax head, and fine-tune only the sigmoid branch with
class-balanced binary supervision. At inference, softmax retains recognition
accuracy, while class evidence maps are generated from the sigmoid branch --
preserving both magnitude and sign of feature contributions. Our method
integrates seamlessly with most CAM variants and incurs negligible overhead.
Extensive evaluations on fine-grained tasks (CUB-200-2011, Stanford Cars) and
WSOL benchmarks (ImageNet-1K, OpenImages30K) show improved explanation fidelity
and consistent Top-1 Localization gains -- without any drop in classification
accuracy. Code is available at https://github.com/finallyupper/beyond-softmax.

</details>


### [19] [Google-MedGemma Based Abnormality Detection in Musculoskeletal radiographs](https://arxiv.org/abs/2511.05600)
*Soumyajit Maity,Pranjal Kamboj,Sneha Maity,Rajat Singh,Sankhadeep Chatterjee*

Main category: cs.CV

TL;DR: 本文提出了一种基于MedGemma模型的框架，用于自动检测骨骼放射影像中的异常。与传统自编码器和神经网络方法不同，该方法利用了预训练于多种医学影像的MedGemma视觉编码器，通过轻量级多层感知机进行二分类，取得了优于传统方法的性能。


<details>
  <summary>Details</summary>
Motivation: 骨骼放射影像异常检测对临床诊断和分诊至关重要，但传统方法如卷积神经网络和自编码器在泛化和特征提取方面存在局限。因此，迫切需要依托更强基础模型，提升检测精度与泛化能力。

Method: 本方法使用MedGemma作为视觉主干，结合SigLIP-derived编码器对X光图像进行高维特征编码，并通过轻量级多层感知机完成二分类。同时利用了MedGemma的迁移学习能力及编码器部分解冻策略，提升训练效率与域适应性。

Result: 该方法在二分类检测骨骼影像异常任务中表现优异，性能超过传统卷积神经网络和自编码器方案，同时展示了出色的泛化能力和特征提取能力。

Conclusion: 基于MedGemma的分类系统能够为临床放射影像分诊提供高效且准确的异常检测，有望在自动化医学影像分析领域获得更广泛应用。

Abstract: This paper proposes a MedGemma-based framework for automatic abnormality
detection in musculoskeletal radiographs. Departing from conventional
autoencoder and neural network pipelines, the proposed method leverages the
MedGemma foundation model, incorporating a SigLIP-derived vision encoder
pretrained on diverse medical imaging modalities. Preprocessed X-ray images are
encoded into high-dimensional embeddings using the MedGemma vision backbone,
which are subsequently passed through a lightweight multilayer perceptron for
binary classification. Experimental assessment reveals that the MedGemma-driven
classifier exhibits strong performance, exceeding conventional convolutional
and autoencoder-based metrics. Additionally, the model leverages MedGemma's
transfer learning capabilities, enhancing generalization and optimizing feature
engineering. The integration of a modern medical foundation model not only
enhances representation learning but also facilitates modular training
strategies such as selective encoder block unfreezing for efficient domain
adaptation. The findings suggest that MedGemma-powered classification systems
can advance clinical radiograph triage by providing scalable and accurate
abnormality detection, with potential for broader applications in automated
medical image analysis.
  Keywords: Google MedGemma, MURA, Medical Image, Classification.

</details>


### [20] [In-process 3D Deviation Mapping and Defect Monitoring (3D-DM2) in High Production-rate Robotic Additive Manufacturing](https://arxiv.org/abs/2511.05604)
*Subash Gautam,Alejandro Vargas-Uscategui,Peter King,Hans Lohr,Alireza Bab-Hadiashar,Ivan Cole,Ehsan Asadi*

Main category: cs.CV

TL;DR: 本论文提出了一种实时检测高沉积速率机器人增材制造（HDRRAM）过程中形状偏差的方法，以提升零件质量。


<details>
  <summary>Details</summary>
Motivation: HDRRAM如冷喷增材制造（CSAM）虽然大幅提高了制造速度，但过程中的形状精度受制于现有开环系统的波动，导致成品质量不稳定且后处理工作繁重。因此，急需一种能在制造过程中实时监测和检测形状偏差的方法。

Method: 研究提出了一个实时监测系统，能够采集制造过程中不断生长的工件形状，并将其实时与近净成形参考模型进行对比，从而检测出形状偏差。此外，该系统还可以对偏差区进行分割和跟踪。

Result: 该方法能够在制造早期精准识别形状不一致，并实现对偏差区域的分割跟踪，为后续快速干预和补偿提供依据，从而提升产品一致性。

Conclusion: 该研究的监测系统能够实时发现制造过程中的形状偏差，有望减少误差传播与后处理要求，提升HDRRAM制品的整体质量和制造效率。

Abstract: Additive manufacturing (AM) is an emerging digital manufacturing technology
to produce complex and freeform objects through a layer-wise deposition. High
deposition rate robotic AM (HDRRAM) processes, such as cold spray additive
manufacturing (CSAM), offer significantly increased build speeds by delivering
large volumes of material per unit time. However, maintaining shape accuracy
remains a critical challenge, particularly due to process instabilities in
current open-loop systems. Detecting these deviations as they occur is
essential to prevent error propagation, ensure part quality, and minimize
post-processing requirements. This study presents a real-time monitoring system
to acquire and reconstruct the growing part and directly compares it with a
near-net reference model to detect the shape deviation during the manufacturing
process. The early identification of shape inconsistencies, followed by
segmenting and tracking each deviation region, paves the way for timely
intervention and compensation to achieve consistent part quality.

</details>


### [21] [Walking the Schrödinger Bridge: A Direct Trajectory for Text-to-3D Generation](https://arxiv.org/abs/2511.05609)
*Ziying Li,Xuequan Lu,Xinkui Zhao,Guanjie Cheng,Shuiguang Deng,Jianwei Yin*

Main category: cs.CV

TL;DR: 本文提出了一种新的Text-to-3D生成方法TraCe，通过重新定义生成过程，实现更高质量、更真实的3D资产生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于优化的Text-to-3D生成方法通常从预训练的Text-to-Image扩散模型中提取知识，但常会引入伪影，如过饱和、过平滑等问题。作者希望从根本上解决这些生成缺陷，提高3D生成的质量。

Method: 作者首先从理论上证明目前主流的Score Distillation Sampling（SDS）可视为Schrödinger Bridge理论下的特例。基于此，提出Trajectory-Centric Distillation（TraCe）框架：利用Schrödinger Bridge自当前渲染的分布到目标（文本引导的）分布间，寻找最优传输轨迹，基于LoRA微调模型，沿这个轨迹进行鲁棒的3D优化。

Result: 基于多个实验，TraCe在质量和细节还原上均优于当前最先进的技术，有效避免了以往的伪影现象。

Conclusion: TraCe为Text-to-3D生成方式提供了理论和实践上的新思路，不仅提升了生成质量，还具有一定的理论普适性，可为后续相关研究提供参考。

Abstract: Recent advancements in optimization-based text-to-3D generation heavily rely
on distilling knowledge from pre-trained text-to-image diffusion models using
techniques like Score Distillation Sampling (SDS), which often introduce
artifacts such as over-saturation and over-smoothing into the generated 3D
assets. In this paper, we address this essential problem by formulating the
generation process as learning an optimal, direct transport trajectory between
the distribution of the current rendering and the desired target distribution,
thereby enabling high-quality generation with smaller Classifier-free Guidance
(CFG) values. At first, we theoretically establish SDS as a simplified instance
of the Schr\"odinger Bridge framework. We prove that SDS employs the reverse
process of an Schr\"odinger Bridge, which, under specific conditions (e.g., a
Gaussian noise as one end), collapses to SDS's score function of the
pre-trained diffusion model. Based upon this, we introduce Trajectory-Centric
Distillation (TraCe), a novel text-to-3D generation framework, which
reformulates the mathematically trackable framework of Schr\"odinger Bridge to
explicitly construct a diffusion bridge from the current rendering to its
text-conditioned, denoised target, and trains a LoRA-adapted model on this
trajectory's score dynamics for robust 3D optimization. Comprehensive
experiments demonstrate that TraCe consistently achieves superior quality and
fidelity to state-of-the-art techniques.

</details>


### [22] [Pose-Aware Multi-Level Motion Parsing for Action Quality Assessment](https://arxiv.org/abs/2511.05611)
*Shuaikang Zhu,Yang Yang,Chen Sun*

Main category: cs.CV

TL;DR: 本文提出了一种基于增强空间-时间姿态特征的多层次动作解析框架，有效提升了动作分割和动作评分任务的表现，并在跳水项目数据集上取得了最新性能。


<details>
  <summary>Details</summary>
Motivation: 动作质量评估（AQA）领域中，高水平比赛的微小空间-时间姿态变化往往决定了评分的高低，现有方法难以充分利用这些细微差别，因此需要更精细、更全面的动作解析方法。

Method: 该文提出了一个多层次的动作解析框架：首先通过Action-Unit解析器结合姿态提取，实现了动作单元的精准分割和局部-全局姿态表征；接着利用Motion解析器进行空间-时间特征学习，捕捉姿态变化与外观细节；另外通过Condition解析器处理如水花等非身体因素，并引入可加权评分模块自适应各种动作需求和多尺度动作单元。

Result: 在大规模跳水运动数据集上，提出的方法在动作分割和评分任务中都取得了业界最优的表现。

Conclusion: 多层次动作解析框架能够更好地建模复杂动作中的姿态及相关因素，对高水平AQA具有实际意义，并在实际应用中展现了优越性。

Abstract: Human pose serves as a cornerstone of action quality assessment (AQA), where
subtle spatial-temporal variations in pose often distinguish excellence from
mediocrity. In high-level competitions, these nuanced differences become
decisive factors in scoring. In this paper, we propose a novel multi-level
motion parsing framework for AQA based on enhanced spatial-temporal pose
features. On the first level, the Action-Unit Parser is designed with the help
of pose extraction to achieve precise action segmentation and comprehensive
local-global pose representations. On the second level, Motion Parser is used
by spatial-temporal feature learning to capture pose changes and appearance
details for each action-unit. Meanwhile, some special conditions other than
body-related will impact action scoring, like water splash in diving. In this
work, we design an additional Condition Parser to offer users more flexibility
in their choices. Finally, Weight-Adjust Scoring Module is introduced to better
accommodate the diverse requirements of various action types and the
multi-scale nature of action-units. Extensive evaluations on large-scale diving
sports datasets demonstrate that our multi-level motion parsing framework
achieves state-of-the-art performance in both action segmentation and action
scoring tasks.

</details>


### [23] [Personalized Image Editing in Text-to-Image Diffusion Models via Collaborative Direct Preference Optimization](https://arxiv.org/abs/2511.05616)
*Connor Dunlop,Matthew Zheng,Kavana Venkatesh,Pinar Yanardag*

Main category: cs.CV

TL;DR: 本文提出了首个用于扩散模型个性化图像编辑的C-DPO（协作式直接偏好优化）方法，实现了基于用户偏好的高质量图像修改。


<details>
  <summary>Details</summary>
Motivation: 虽然文本到图像（T2I）扩散模型在图像生成和编辑方面取得了进步，但它们缺乏针对个体用户审美偏好的适应性，因而难以满足个人化需求。

Method: 作者提出为每个用户建立节点，构成动态偏好图，通过轻量级图神经网络学习用户嵌入，实现同好间信息共享。进一步将个性化嵌入整合到创新的DPO目标函数中，同时优化个体偏好匹配和邻域一致性，从而提升编辑的个性化与协同性。

Result: 实验证明，包括用户研究和量化测试，所提方法在生成与用户偏好高度一致的图像编辑方面，表现优于现有基线方法。

Conclusion: C-DPO方法有效提升了T2I扩散模型在图像个性化编辑领域的能力，为满足不同用户的视觉偏好带来新可能，具有较强实用价值。

Abstract: Text-to-image (T2I) diffusion models have made remarkable strides in
generating and editing high-fidelity images from text. Yet, these models remain
fundamentally generic, failing to adapt to the nuanced aesthetic preferences of
individual users. In this work, we present the first framework for personalized
image editing in diffusion models, introducing Collaborative Direct Preference
Optimization (C-DPO), a novel method that aligns image edits with user-specific
preferences while leveraging collaborative signals from like-minded
individuals. Our approach encodes each user as a node in a dynamic preference
graph and learns embeddings via a lightweight graph neural network, enabling
information sharing across users with overlapping visual tastes. We enhance a
diffusion model's editing capabilities by integrating these personalized
embeddings into a novel DPO objective, which jointly optimizes for individual
alignment and neighborhood coherence. Comprehensive experiments, including user
studies and quantitative benchmarks, demonstrate that our method consistently
outperforms baselines in generating edits that are aligned with user
preferences.

</details>


### [24] [Convolutional Fully-Connected Capsule Network (CFC-CapsNet): A Novel and Fast Capsule Network](https://arxiv.org/abs/2511.05617)
*Pouya Shiri,Amirali Baniasadi*

Main category: cs.CV

TL;DR: 本文提出了一种新的胶囊网络结构CFC-CapsNet，通过引入全连接卷积层（CFC层）有效提升网络效率和准确性。与传统CapsNet相比，CFC-CapsNet在CIFAR-10、SVHN和Fashion-MNIST数据集上表现出更高的准确率、更快的训练和推理速度，并减少了参数数量。


<details>
  <summary>Details</summary>
Motivation: 尽管胶囊网络（CapsNet）在小规模数据集如MNIST上表现良好，但在更复杂数据集和实际应用中性能有限，并且比CNN慢且参数量多。作者希望改进CapsNet的表现以便其更适用于实际复杂任务。

Method: 作者提出了一种新型的胶囊创建方法——全连接卷积层（CFC layer），并基于此结构设计了CFC-CapsNet。通过该方法可生成更少但功能更强的胶囊单元，并将其在多个数据集上与传统CapsNet进行比较。

Result: 实验结果显示，CFC-CapsNet在CIFAR-10、SVHN和Fashion-MNIST三个数据集上，与常规CapsNet相比，在准确率更高的同时减少了所需参数数量，实现了更快的训练和推理速度。

Conclusion: 通过引入CFC层创建胶囊，CFC-CapsNet有效克服了传统CapsNet在泛化能力、效率及参数量上的不足，提高了在复杂数据集上的实用性和表现。

Abstract: A Capsule Network (CapsNet) is a relatively new classifier and one of the
possible successors of Convolutional Neural Networks (CNNs). CapsNet maintains
the spatial hierarchies between the features and outperforms CNNs at
classifying images including overlapping categories. Even though CapsNet works
well on small-scale datasets such as MNIST, it fails to achieve a similar level
of performance on more complicated datasets and real applications. In addition,
CapsNet is slow compared to CNNs when performing the same task and relies on a
higher number of parameters. In this work, we introduce Convolutional
Fully-Connected Capsule Network (CFC-CapsNet) to address the shortcomings of
CapsNet by creating capsules using a different method. We introduce a new layer
(CFC layer) as an alternative solution to creating capsules. CFC-CapsNet
produces fewer, yet more powerful capsules resulting in higher network
accuracy. Our experiments show that CFC-CapsNet achieves competitive accuracy,
faster training and inference and uses less number of parameters on the
CIFAR-10, SVHN and Fashion-MNIST datasets compared to conventional CapsNet.

</details>


### [25] [Grounding Foundational Vision Models with 3D Human Poses for Robust Action Recognition](https://arxiv.org/abs/2511.05622)
*Nicholas Babey,Tiffany Gu,Yiheng Li,Cristian Meo,Kevin Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种通过融合世界动态和人体姿态的方式，将行为识别任务有效地定位在物理空间中，从而提升了对复杂、遮挡场景下人类行为的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于视频的行为识别模型多依赖于RGB信息，只能捕捉表面模式，难以深入理解物理交互动态和复杂场景下人体姿势，导致识别性能受限，特别是在有遮挡的情况下表现不佳。本文动机在于通过更真实地感知和理解物理世界中的人体动作，提高具身智能体对环境的交互能力。

Method: 提出了一种新型模型架构，融合了V-JEPA 2的上下文预测世界动态表示和CoMotion的具有遮挡鲁棒性的人体姿势信息，通过此融合实现空间上的行为理解。模型在InHARD和UCF-19-Y-OCC两个基准数据集上进行验证，分别针对常规和高遮挡场景。

Result: 所提模型在两大数据集上均超过了三个对比基线，尤其在复杂及遮挡场景下优势显著。

Conclusion: 与其仅依赖统计模式识别，行为识别任务应更多依托于对空间与物理世界的理解，这对于增强具身智能体对环境的适应与交互至关重要。

Abstract: For embodied agents to effectively understand and interact within the world
around them, they require a nuanced comprehension of human actions grounded in
physical space. Current action recognition models, often relying on RGB video,
learn superficial correlations between patterns and action labels, so they
struggle to capture underlying physical interaction dynamics and human poses in
complex scenes. We propose a model architecture that grounds action recognition
in physical space by fusing two powerful, complementary representations: V-JEPA
2's contextual, predictive world dynamics and CoMotion's explicit,
occlusion-tolerant human pose data. Our model is validated on both the InHARD
and UCF-19-Y-OCC benchmarks for general action recognition and high-occlusion
action recognition, respectively. Our model outperforms three other baselines,
especially within complex, occlusive scenes. Our findings emphasize a need for
action recognition to be supported by spatial understanding instead of
statistical pattern recognition.

</details>


### [26] [Registration-Free Monitoring of Unstructured Point Cloud Data via Intrinsic Geometrical Properties](https://arxiv.org/abs/2511.05623)
*Mariafrancesca Patalano,Giovanna Capizzi,Kamran Paynabar*

Main category: cs.CV

TL;DR: 本文提出了一种全新的点云数据监测方法，无需繁琐且易出错的配准与网格重建步骤，可直接高效地识别复杂形状中的不同类型缺陷。


<details>
  <summary>Details</summary>
Motivation: 传统的点云数据在3D对象的精度监测中需要先进行配准和网格重建，但这些预处理步骤不仅耗时且容易引入误差和伪影，影响后续监测结果，因此迫切需要一种无需这些步骤的高效监测方法。

Method: 作者提出了一种注册（配准）与重建无关的点云监测新方法，包括两种基于形状内在几何属性（Laplacian和测地距离）的特征学习策略，结合基于阈值的监测方案来筛选最能反映异常状态的内在特征。

Result: 通过数值实验和案例研究，所提方法能够有效识别不同类型的缺陷，验证了方法的实用性和优越性。

Conclusion: 所提无配准点云监控方法简化了流程、提升了监测效率，且在缺陷检测上表现优秀，有望应用于复杂制造场景的质量监控。

Abstract: Modern sensing technologies have enabled the collection of unstructured point
cloud data (PCD) of varying sizes, which are used to monitor the geometric
accuracy of 3D objects. PCD are widely applied in advanced manufacturing
processes, including additive, subtractive, and hybrid manufacturing. To ensure
the consistency of analysis and avoid false alarms, preprocessing steps such as
registration and mesh reconstruction are commonly applied prior to monitoring.
However, these steps are error-prone, time-consuming and may introduce
artifacts, potentially affecting monitoring outcomes. In this paper, we present
a novel registration-free approach for monitoring PCD of complex shapes,
eliminating the need for both registration and mesh reconstruction. Our
proposal consists of two alternative feature learning methods and a common
monitoring scheme. Feature learning methods leverage intrinsic geometric
properties of the shape, captured via the Laplacian and geodesic distances. In
the monitoring scheme, thresholding techniques are used to further select
intrinsic features most indicative of potential out-of-control conditions.
Numerical experiments and case studies highlight the effectiveness of the
proposed approach in identifying different types of defects.

</details>


### [27] [Culture in Action: Evaluating Text-to-Image Models through Social Activities](https://arxiv.org/abs/2511.05681)
*Sina Malakouti,Boqing Gong,Adriana Kovashka*

Main category: cs.CV

TL;DR: 该论文提出了CULTIVate基准，用于评估文本到图像（T2I）扩散模型在跨文化活动中的表现，涵盖16国576条提示和近2万张图片，并定义了四项衡量文化契合、幻象、夸张和多样性的指标。


<details>
  <summary>Details</summary>
Motivation: 当前T2I模型虽然在现实感上表现优异，但存在文化偏见，尤其是对欠代表地区和跨文化活动的描绘不足。现有评测多关注物体类别，缺乏反映社会活动及文化习俗的指标和系统性基准，所以有必要建立更符合实际文化表达的评估体系。

Method: 作者构建了CULTIVate数据集，包含来自16个国家的文化活动相关图片及描述，覆盖问候、用餐、游戏、传统舞蹈和庆典等主题。引入描述符为核心的评估框架，并提出四项新指标：文化契合度、虚构成分、夸张元素和多样性，同时引入人工评估来验证指标的相关性。

Result: 评测发现，T2I模型在“全球北方”国家表现优于“全球南方”，不同模型在跨文化活动上存在系统性失误。四项新指标与人工评价高度相关，优于现有的文本-图像评价方法。

Conclusion: CULTIVate基准及新指标能更加科学、全面地评估T2I模型的跨文化表现，发现并分析了现有系统存在的文化偏见问题，有助于后续模型优化和公平性提升。

Abstract: Text-to-image (T2I) diffusion models achieve impressive photorealism by
training on large-scale web data, but models inherit cultural biases and fail
to depict underrepresented regions faithfully. Existing cultural benchmarks
focus mainly on object-centric categories (e.g., food, attire, and
architecture), overlooking the social and daily activities that more clearly
reflect cultural norms. Few metrics exist for measuring cultural faithfulness.
We introduce CULTIVate, a benchmark for evaluating T2I models on cross-cultural
activities (e.g., greetings, dining, games, traditional dances, and cultural
celebrations). CULTIVate spans 16 countries with 576 prompts and more than
19,000 images, and provides an explainable descriptor-based evaluation
framework across multiple cultural dimensions, including background, attire,
objects, and interactions. We propose four metrics to measure cultural
alignment, hallucination, exaggerated elements, and diversity. Our findings
reveal systematic disparities: models perform better for global north countries
than for the global south, with distinct failure modes across T2I systems.
Human studies confirm that our metrics correlate more strongly with human
judgments than existing text-image metrics.

</details>


### [28] [VMDT: Decoding the Trustworthiness of Video Foundation Models](https://arxiv.org/abs/2511.05682)
*Yujin Potter,Zhun Wang,Nicholas Crispino,Kyle Montgomery,Alexander Xiong,Ethan Y. Chang,Francesco Pinto,Yuqi Chen,Rahul Gupta,Morteza Ziyadi,Christos Christodoulopoulos,Bo Li,Chenguang Wang,Dawn Song*

Main category: cs.CV

TL;DR: 本论文提出了VMDT，这是首个全面评估视频大模型（包括文本到视频和视频到文本方向）可信度的统一平台，覆盖安全性、幻觉、公平性、隐私和对抗鲁棒性五大维度。作者用该平台评测了26个主流模型，发现视频模型在安全性、公平性等方面存在严重短板，且模型规模增长未必带来安全性提升。


<details>
  <summary>Details</summary>
Motivation: 随着基础大模型的能力增强，视频作为多模态领域核心却缺乏系统的可信度评测方法。目前主流的文本和图像已经有不少相关工作，但视频模型的安全性、公平性、隐私等问题尚无统一评测标准，因此迫切需要一个覆盖多维度的评测平台。

Method: 作者提出了VMDT评价平台，涵盖T2V（文本到视频）、V2T（视频到文本）两大类模型，围绕安全、幻觉、公平、隐私、对抗鲁棒性五个维度设计评测任务。通过实际评测7个T2V和19个V2T模型，定量分析各维度性能表现。

Result: 实验发现：1）所有开源T2V模型都未能识别有害查询且经常生成有害视频，公平性也低于图像模型；2）V2T模型随规模扩展，不公平性和隐私风险加剧，幻觉和对抗鲁棒性有所改善，整体仍较低；3）安全性与模型规模无关。

Conclusion: 当前视频基础模型在多个可信度指标上表现不佳，尤其是安全性和公平性问题突出。VMDT为行业提供了一个系统的测量和追踪机制，有助于推动更健壮、可信赖的视频基础模型发展。

Abstract: As foundation models become more sophisticated, ensuring their
trustworthiness becomes increasingly critical; yet, unlike text and image, the
video modality still lacks comprehensive trustworthiness benchmarks. We
introduce VMDT (Video-Modal DecodingTrust), the first unified platform for
evaluating text-to-video (T2V) and video-to-text (V2T) models across five key
trustworthiness dimensions: safety, hallucination, fairness, privacy, and
adversarial robustness. Through our extensive evaluation of 7 T2V models and 19
V2T models using VMDT, we uncover several significant insights. For instance,
all open-source T2V models evaluated fail to recognize harmful queries and
often generate harmful videos, while exhibiting higher levels of unfairness
compared to image modality models. In V2T models, unfairness and privacy risks
rise with scale, whereas hallucination and adversarial robustness improve --
though overall performance remains low. Uniquely, safety shows no correlation
with model size, implying that factors other than scale govern current safety
levels. Our findings highlight the urgent need for developing more robust and
trustworthy video foundation models, and VMDT provides a systematic framework
for measuring and tracking progress toward this goal. The code is available at
https://sunblaze-ucb.github.io/VMDT-page/.

</details>


### [29] [Pedicle Screw Pairing and Registration for Screw Pose Estimation from Dual C-arm Images Using CAD Models](https://arxiv.org/abs/2511.05702)
*Yehyun Suh,Lin Li,Aric Plumley,Chaochao Zhou,Daniel Moyer,Kongbin Kang*

Main category: cs.CV

TL;DR: 该论文提出了一种通过双C臂影像实现椎弓根螺钉精确对应和姿态估计的方法，显著提高了螺钉配对和注册的准确性，有助于提升脊柱手术的效果。


<details>
  <summary>Details</summary>
Motivation: 在脊柱减压和固定手术中，椎弓根螺钉在正位和侧位影像中的准确配对对于手术成功至关重要，但尤其在侧位视图下配对难度较大，目前临床上仍缺乏鲁棒的自动配对方法。

Method: 该方法通过比较螺钉组合方式，在配对和注册任务中实现高准确性。同时，将2D影像与3D螺钉CAD模型对齐，以实现螺钉在两视图中的准确配对和姿态估计。

Result: 实验结果表明，正确的螺钉组合在所有测试案例中，无论注册前后，其配对和对准准确性均显著优于错误组合，注册后显著降低了投影误差。

Conclusion: 该方法能够为术中椎弓根螺钉的位置提供可靠反馈，有望提升脊柱手术的安全性与疗效。

Abstract: Accurate matching of pedicle screws in both anteroposterior (AP) and lateral
(LAT) images is critical for successful spinal decompression and stabilization
during surgery. However, establishing screw correspondence, especially in LAT
views, remains a significant clinical challenge. This paper introduces a method
to address pedicle screw correspondence and pose estimation from dual C-arm
images. By comparing screw combinations, the approach demonstrates consistent
accuracy in both pairing and registration tasks. The method also employs 2D-3D
alignment with screw CAD 3D models to accurately pair and estimate screw pose
from dual views. Our results show that the correct screw combination
consistently outperforms incorrect pairings across all test cases, even prior
to registration. After registration, the correct combination further enhances
alignment between projections and images, significantly reducing projection
error. This approach shows promise for improving surgical outcomes in spinal
procedures by providing reliable feedback on screw positioning.

</details>


### [30] [Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale](https://arxiv.org/abs/2511.05705)
*David Acuna,Chao-Han Huck Yang,Yuntian Deng,Jaehun Jung,Ximing Lu,Prithviraj Ammanabrolu,Hyunwoo Kim,Yuan-Hong Liao,Yejin Choi*

Main category: cs.CV

TL;DR: 本文提出了一种全新的多模态推理数据生成框架，生成超过100万高质量、以视觉为中心的复杂推理问题，并展示了所生成数据对视觉、多模态、音频乃至文本推理任务的卓越迁移提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推理的进展多依赖于未公开数据和私有生成方法，缺乏系统、大规模、面向复杂视觉推理的数据集，尤其在超越视觉数学的多样化推理任务中更为突出，亟需突破数据瓶颈。

Method: 提出两阶段的推理数据合成方法：（1）扩展规模；（2）提升复杂度。具体通过联合VLM（视觉语言模型）和推理型LLM（大语言模型），生成涵盖多样复杂认知行为的推理轨迹(CoT)。数据集额外包含用于偏好建模与强化学习（RL）的指令数据。

Result: 通过在生成的数据上微调Qwen2.5-VL-7B模型，实现了对主流开源视觉推理基线的全面超越，并在多个视觉基准测评（如V* Bench、CV-Bench、MMStar-V）优于封闭数据的强模型。并且该视觉推理数据对跨模态（音频MMAU、文本MMLU-Pro）和具身问答（NiEH）也有显著迁移提升。

Conclusion: 高质量、非线性推理轨迹的SFT对强化学习训练至关重要，分阶段离线RL能以更低算力达到与在线RL相当的效果，精细化SFT可极大提升异域、跨模态推理迁移能力。

Abstract: Recent progress in multimodal reasoning has been driven largely by
undisclosed datasets and proprietary data synthesis recipes, leaving open
questions about how to systematically build large-scale, vision-centric
reasoning datasets, particularly for tasks that go beyond visual math. In this
work, we introduce a new reasoning data generation framework spanning diverse
skills and levels of complexity with over 1M high-quality synthetic
vision-centric questions. The dataset also includes preference data and
instruction prompts supporting both offline and online RL. Our synthesis
framework proceeds in two stages: (1) scale; and (2) complexity. Reasoning
traces are then synthesized through a two-stage process that leverages VLMs and
reasoning LLMs, producing CoT traces for VLMs that capture the richness and
diverse cognitive behaviors found in frontier reasoning models. Remarkably, we
show that finetuning Qwen2.5-VL-7B on our data outperforms all open-data
baselines across all evaluated vision-centric benchmarks, and even surpasses
strong closed-data models such as MiMo-VL-7B-RL on V* Bench, CV-Bench and
MMStar-V. Perhaps most surprising, despite being entirely vision-centric, our
data transfers positively to text-only reasoning (MMLU-Pro) and audio reasoning
(MMAU), demonstrating its effectiveness. Similarly, despite not containing
videos or embodied visual data, we observe notable gains when evaluating on a
single-evidence embodied QA benchmark (NiEH). Finally, we use our data to
analyze the entire VLM post-training pipeline. Our empirical analysis
highlights that (i) SFT on high-quality data with non-linear reasoning traces
is essential for effective online RL, (ii) staged offline RL matches online
RL's performance while reducing compute demands, and (iii) careful SFT on high
quality data can substantially improve out-of-domain, cross-modality transfer.

</details>


### [31] [Towards Better Ultrasound Video Segmentation Foundation Model: An Empirical study on SAM2 Finetuning from Data Perspective](https://arxiv.org/abs/2511.05731)
*Xing Yao,Ahana Gangopadhyay,Hsi-Ming Chang,Ravi Soni*

Main category: cs.CV

TL;DR: 本文系统研究了如何高效地将Segment Anything Model 2（SAM2）适配于超声视频分割，重点考察了数据规模、视频时长与数据增强措施等数据相关因素的影响，并对多种训练范式进行了实验评估。结果显示数据和时序信息的重要性远超模型结构改动。


<details>
  <summary>Details</summary>
Motivation: 当前超声视频分割面临数据异质性、标注有限和运动伪影等挑战。虽然SAM2等基础模型在通用领域表现优异，但直接迁移到医学成像时性能大幅下降。现有研究更关注模型结构改造，缺乏对数据和训练策略的系统评估。

Method: 作者采用数据集为中心的方法，分析训练数据规模、视频时长、特定与通用增强策略在三种主流训练范式（任务特定微调、中间适应、多任务联合训练）下对五个SAM2模型变体的适应效果，涉及多种提示方式，并加入了六种超声特有的数据增强。

Result: 多组实验表明，相较于模型结构或预训练初始化，数据规模和时序信息对适应效果影响更加显著；联合训练在提升模态对齐和任务特化之间取得了较好平衡；超声特定的增强策略优于通用增强。

Conclusion: 本文为SAM2在超声视频分割中的高效适应提供了基于实证的数据和策略建议，强调了数据特性与训练范式的重要性，推动了医学视频自动分割方法的实用发展。

Abstract: Ultrasound (US) video segmentation remains a challenging problem due to
strong inter- and intra-dataset variability, motion artifacts, and limited
annotated data. Although foundation models such as Segment Anything Model 2
(SAM2) demonstrate strong zero-shot and prompt-guided segmentation
capabilities, their performance deteriorates substantially when transferred to
medical imaging domains. Current adaptation studies mainly emphasize
architectural modifications, while the influence of data characteristics and
training regimes has not been systematically examined. In this study, we
present a comprehensive, data-centric investigation of SAM2 adaptation for
ultrasound video segmentation. We analyze how training-set size, video
duration, and augmentation schemes affect adaptation performance under three
paradigms: task-specific fine-tuning, intermediate adaptation, and multi-task
joint training, across five SAM2 variants and multiple prompting modes. We
further design six ultrasound-specific augmentations, assessing their effect
relative to generic strategies. Experiments on three representative ultrasound
datasets reveal that data scale and temporal context play a more decisive role
than model architecture or initialization. Moreover, joint training offers an
efficient compromise between modality alignment and task specialization. This
work aims to provide empirical insights for developing efficient, data-aware
adaptation pipelines for SAM2 in ultrasound video analysis.

</details>


### [32] [A Second-Order Attention Mechanism For Prostate Cancer Segmentation and Detection in Bi-Parametric MRI](https://arxiv.org/abs/2511.05760)
*Mateo Ortiz,Juan Olmos,Fabio Martínez*

Main category: cs.CV

TL;DR: 本文提出了一种基于二阶几何注意力（SOGA）机制的新型分割网络，有效提升了双参数磁共振成像（bp-MRI）中前列腺癌病灶的检测准确性。经验证，该方法优于常规及其它注意力机制。


<details>
  <summary>Details</summary>
Motivation: 当前bp-MRI在检测临床显著性前列腺癌方面具有非侵入性优势，但其分析高度依赖专家主观解读，同时深度学习分割方法受限于对高度注释数据的需求且面临前列腺不同区域病灶变化大的难题。

Method: 论文提出SOGA注意力机制，可通过skip-connection集成于标准U-Net和nnU-Net，基于黎曼流形与对称正定（SPD）表示建模，提升网络对不同区带病灶的检测。

Result: 在公开PI-CAI数据集上，集成SOGA的网络平均精度（AP）达到0.37，AUC为0.83，优于基础网络和其他注意力方法；在独立Prostate158测试集上，AP为0.37，AUC为0.75，展现良好泛化。

Conclusion: SOGA可有效提高bp-MRI前列腺癌分割准确性，并具备一定泛化能力，为辅助临床诊断提供了新的可能。

Abstract: The detection of clinically significant prostate cancer lesions (csPCa) from
biparametric magnetic resonance imaging (bp-MRI) has emerged as a noninvasive
imaging technique for improving accurate diagnosis. Nevertheless, the analysis
of such images remains highly dependent on the subjective expert
interpretation. Deep learning approaches have been proposed for csPCa lesions
detection and segmentation, but they remain limited due to their reliance on
extensively annotated datasets. Moreover, the high lesion variability across
prostate zones poses additional challenges, even for expert radiologists. This
work introduces a second-order geometric attention (SOGA) mechanism that guides
a dedicated segmentation network, through skip connections, to detect csPCa
lesions. The proposed attention is modeled on the Riemannian manifold, learning
from symmetric positive definitive (SPD) representations. The proposed
mechanism was integrated into standard U-Net and nnU-Net backbones, and was
validated on the publicly available PI-CAI dataset, achieving an Average
Precision (AP) of 0.37 and an Area Under the ROC Curve (AUC-ROC) of 0.83,
outperforming baseline networks and attention-based methods. Furthermore, the
approach was evaluated on the Prostate158 dataset as an independent test
cohort, achieving an AP of 0.37 and an AUC-ROC of 0.75, confirming robust
generalization and suggesting discriminative learned representations.

</details>


### [33] [Sign language recognition from skeletal data using graph and recurrent neural networks](https://arxiv.org/abs/2511.05772)
*B. Mederos,J. Mejía,A. Medina-Reyes,Y. Espinosa-Almeyda,J. D. Díaz-Roman,I. Rodríguez-Mederos,M. Mejía-Carreon,F. Gonzalez-Lopez*

Main category: cs.CV

TL;DR: 本文提出了一种基于骨架姿态数据的手语手势识别新方法，采用Graph-GRU时序网络建模空间和时间依赖，有效提升了识别准确率。


<details>
  <summary>Details</summary>
Motivation: 手语识别任务复杂，需要捕捉手部和身体动作的空间与时间变化，传统方法难以充分利用这些依赖关系，因此作者希望提出更有效的建模方法提升识别效果。

Method: 作者提出基于骨架姿态数据的Graph-GRU时序网络模型，将视频序列中的骨架节点作为图结构输入，利用图结构与门控循环单元（GRU）结合，同时建模空间关系（不同身体部位之间）与时序关系（帧与帧之间），进行手语动作分类。

Result: 在AUTSL（安卡拉大学土耳其手语）数据集上进行训练与评估，实验结果显示该方法准确率较高，优于传统方法。

Conclusion: 集成图结构空间表征和时序建模的方法在手语识别任务中表现出色，表明基于骨架姿态的数据驱动方法在手语理解领域具有广阔应用前景和可扩展性。

Abstract: This work presents an approach for recognizing isolated sign language
gestures using skeleton-based pose data extracted from video sequences. A
Graph-GRU temporal network is proposed to model both spatial and temporal
dependencies between frames, enabling accurate classification. The model is
trained and evaluated on the AUTSL (Ankara university Turkish sign language)
dataset, achieving high accuracy. Experimental results demonstrate the
effectiveness of integrating graph-based spatial representations with temporal
modeling, providing a scalable framework for sign language recognition. The
results of this approach highlight the potential of pose-driven methods for
sign language understanding.

</details>


### [34] [TCSA-UDA: Text-Driven Cross-Semantic Alignment for Unsupervised Domain Adaptation in Medical Image Segmentation](https://arxiv.org/abs/2511.05782)
*Lalit Maurya,Honghai Liu,Reyer Zwiggelaar*

Main category: cs.CV

TL;DR: 该论文提出了一种结合文本驱动的跨语义对齐框架（TCSA-UDA），在医学图像分割中有效缓解了CT与MRI等模态间的域偏移问题，并在多个基准上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 在医学图像分割中，不同成像模态（如CT、MRI）间存在显著的域偏移，这使得无监督域自适应（UDA）分割任务极具挑战性。虽然视觉-语言表征学习近来取得进展，但其在UDA分割中的潜力尚未被充分挖掘。

Method: 提出TCSA-UDA框架，通过域不变的文本类别描述引导视觉特征学习。引入视觉-语言协方差余弦损失，使图像编码器特征与类别间文本语义关系直接对齐。辅以原型对齐模块，将跨域像素级特征分布与高阶语义原型对齐，减少类别级别差异并提升模态一致性。

Result: 在心脏、腹部、脑肿瘤等跨模态分割基准上，TCSA-UDA显著减小了域偏移，分割性能持续超越了当前主流的UDA方法。

Conclusion: TCSA-UDA为医学图像分割的跨模态无监督域自适应引入了语言语义驱动的新范式，显著提升了分割精度和泛化能力，为相关领域的研究提供了新的方向。

Abstract: Unsupervised domain adaptation for medical image segmentation remains a
significant challenge due to substantial domain shifts across imaging
modalities, such as CT and MRI. While recent vision-language representation
learning methods have shown promise, their potential in UDA segmentation tasks
remains underexplored. To address this gap, we propose TCSA-UDA, a Text-driven
Cross-Semantic Alignment framework that leverages domain-invariant textual
class descriptions to guide visual representation learning. Our approach
introduces a vision-language covariance cosine loss to directly align image
encoder features with inter-class textual semantic relations, encouraging
semantically meaningful and modality-invariant feature representations.
Additionally, we incorporate a prototype alignment module that aligns
class-wise pixel-level feature distributions across domains using high-level
semantic prototypes. This mitigates residual category-level discrepancies and
enhances cross-modal consistency. Extensive experiments on challenging
cross-modality cardiac, abdominal, and brain tumor segmentation benchmarks
demonstrate that our TCSA-UDA framework significantly reduces domain shift and
consistently outperforms state-of-the-art UDA methods, establishing a new
paradigm for integrating language-driven semantics into domain-adaptive medical
image analysis.

</details>


### [35] [Position-Prior-Guided Network for System Matrix Super-Resolution in Magnetic Particle Imaging](https://arxiv.org/abs/2511.05795)
*Xuqing Geng,Lei Su,Zhongwei Bian,Zewen Sun,Jiaxuan Wen,Jie Tian,Yang Du*

Main category: cs.CV

TL;DR: 本文提出在磁性粒子成像（MPI）系统矩阵（SM）校准方法中，引入物理先验知识（如位置对称性），提高深度学习超分辨率策略的效率和精度。实验显示，结合这些先验的大幅提升了2D和3D系统矩阵校准的效果。


<details>
  <summary>Details</summary>
Motivation: 传统的磁性粒子成像系统矩阵校准方法耗时且操作繁琐，深度学习虽能加速，但未充分利用系统矩阵的物理特性（如位置先验），因此有进一步提升空间。

Method: 作者将位置对称性等物理先验知识融入到已有的深度学习超分辨率系统矩阵校准框架中，并通过理论分析和2D/3D实验验证这种结合方式的有效性。

Result: 实验表明，融合物理先验知识的深度学习超分辨率方法，在系统矩阵的重建精度和效率上均优于未使用先验的常规方法。

Conclusion: 将物理先验与深度学习超分辨率方法结合，可显著提升磁性粒子成像系统矩阵校准的质量和速度，具有较高的研究与应用价值。

Abstract: Magnetic Particle Imaging (MPI) is a novel medical imaging modality. One of
the established methods for MPI reconstruction is based on the System Matrix
(SM). However, the calibration of the SM is often time-consuming and requires
repeated measurements whenever the system parameters change. Current
methodologies utilize deep learning-based super-resolution (SR) techniques to
expedite SM calibration; nevertheless, these strategies do not fully exploit
physical prior knowledge associated with the SM, such as symmetric positional
priors. Consequently, we integrated positional priors into existing frameworks
for SM calibration. Underpinned by theoretical justification, we empirically
validated the efficacy of incorporating positional priors through experiments
involving both 2D and 3D SM SR methods.

</details>


### [36] [MACMD: Multi-dilated Contextual Attention and Channel Mixer Decoding for Medical Image Segmentation](https://arxiv.org/abs/2511.05803)
*Lalit Maurya,Honghai Liu,Reyer Zwiggelaar*

Main category: cs.CV

TL;DR: 本文提出了一种基于MACMD的新型解码器结构，提升了医学图像分割的精度和计算效率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割由于解剖结构变化显著，面临细粒度特征丢失和全局上下文与局部细节整合不足两大挑战。现有CNN和Transformer方法各有优劣，仍难以高效兼顾全局与局部信息。

Method: 作者提出了MACMD-based解码器，结合分层扩张卷积、注意力机制和跨通道混合模块，强化encoder与decoder之间的skip连接，实现全局与局部信息高效融合。

Result: 在多种transformer主干网络、二元与多器官分割任务上验证，本文方法在Dice分数与计算效率上均超过主流方法。

Conclusion: MACMD结构能有效捕捉长距离依赖并保留局部上下文，显著提升医学图像分割精度与鲁棒性，具有推广和实用价值。

Abstract: Medical image segmentation faces challenges due to variations in anatomical
structures. While convolutional neural networks (CNNs) effectively capture
local features, they struggle with modeling long-range dependencies.
Transformers mitigate this issue with self-attention mechanisms but lack the
ability to preserve local contextual information. State-of-the-art models
primarily follow an encoder-decoder architecture, achieving notable success.
However, two key limitations remain: (1) Shallow layers, which are closer to
the input, capture fine-grained details but suffer from information loss as
data propagates through deeper layers. (2) Inefficient integration of local
details and global context between the encoder and decoder stages. To address
these challenges, we propose the MACMD-based decoder, which enhances attention
mechanisms and facilitates channel mixing between encoder and decoder stages
via skip connections. This design leverages hierarchical dilated convolutions,
attention-driven modulation, and a cross channel-mixing module to capture
long-range dependencies while preserving local contextual details, essential
for precise medical image segmentation. We evaluated our approach using
multiple transformer encoders on both binary and multi-organ segmentation
tasks. The results demonstrate that our method outperforms state-of-the-art
approaches in terms of Dice score and computational efficiency, highlighting
its effectiveness in achieving accurate and robust segmentation performance.
The code available at https://github.com/lalitmaurya47/MACMD

</details>


### [37] [LRANet++: Low-Rank Approximation Network for Accurate and Efficient Text Spotting](https://arxiv.org/abs/2511.05818)
*Yuchen Su,Zhineng Chen,Yongkun Du,Zuxuan Wu,Hongtao Xie,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本文提出了一个高效且精确的任意形状文本端到端检测与识别方法LRANet++，通过低秩近似和三重分支检测头，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前端到端文本检测与识别在检测任意形状文本时仍存在鲁棒性和高效性不足的瓶颈，主要由于缺乏可靠的检测方法。

Method: 1. 提出基于低秩近似的参数化文本形状方法，通过从标注文本边界直接学习低秩子空间，强化数据相关性，同时利用l1范数恢复方法增强对噪声标注的鲁棒性。
2. 设计三重分支检测头，包括稠密分支（提供丰富监督）、深度稀疏分支（用于稳定训练）、轻量稀疏分支（加速推理），实现精确和高效检测。
3. 融合增强探测模块和轻量级识别分支，构建端到端的文本检测识别框架LRANet++。

Result: LRANet++在多个具有挑战性的基准上取得了比现有最先进方法更优的性能，兼具检测精度和推理效率。

Conclusion: 通过低秩近似与创新的检测头设计，LRANet++能高效、准确地检测识别任意形状文本，为端到端文本定位与识别提供了新思路。

Abstract: End-to-end text spotting aims to jointly optimize text detection and
recognition within a unified framework. Despite significant progress, designing
an accurate and efficient end-to-end text spotter for arbitrary-shaped text
remains largely unsolved. We identify the primary bottleneck as the lack of a
reliable and efficient text detection method. To address this, we propose a
novel parameterized text shape method based on low-rank approximation for
precise detection and a triple assignment detection head to enable fast
inference. Specifically, unlike other shape representation methods that employ
data-irrelevant parameterization, our data-driven approach derives a low-rank
subspace directly from labeled text boundaries. To ensure this process is
robust against the inherent annotation noise in this data, we utilize a
specialized recovery method based on an $\ell_1$-norm formulation, which
accurately reconstructs the text shape with only a few key orthogonal vectors.
By exploiting the inherent shape correlation among different text contours, our
method achieves consistency and compactness in shape representation. Next, the
triple assignment scheme introduces a novel architecture where a deep sparse
branch (for stabilized training) is used to guide the learning of an
ultra-lightweight sparse branch (for accelerated inference), while a dense
branch provides rich parallel supervision. Building upon these advancements, we
integrate the enhanced detection module with a lightweight recognition branch
to form an end-to-end text spotting framework, termed LRANet++, capable of
accurately and efficiently spotting arbitrary-shaped text. Extensive
experiments on several challenging benchmarks demonstrate the superiority of
LRANet++ compared to state-of-the-art methods. Code will be available at:
https://github.com/ychensu/LRANet-PP.git

</details>


### [38] [Hilbert-Guided Block-Sparse Local Attention](https://arxiv.org/abs/2511.05832)
*Yunge Li,Lanyu Xu*

Main category: cs.CV

TL;DR: 本文提出一种利用希尔伯特曲线（Hilbert curve）重新排序图像tokens的方法，通过在1D序列上构建窗口和邻域，大幅提升了局部注意力（local attention）的计算效率。该方法与现有稀疏块核结合可极大加速2D图像的局部自注意力，且几乎不损失精度。


<details>
  <summary>Details</summary>
Motivation: 全局自注意力的二次复杂度使其难以应用于高分辨率图像。虽然局部注意力和稀疏块核能改善效率，但现有做法因窗口不连续，实际加速有限。需要新的方法提升效率并保持精度。

Method: 提出用希尔伯特曲线对图像tokens重新排序，然后在1D序列上构建连续的窗口和邻域，从而增加稀疏块度，并可与稀疏块核结合。具体实现为 Hilbert Window Attention 和 Hilbert Slide Attention，并据此构建 Hilbert Window Transformer 和 Hilbert Neighborhood Transformer。

Result: 新方法将窗口注意力和滑动注意力分别加速约4倍和18倍。相应的Transformer模型实现了端到端整体提速且几乎没有精度损失。

Conclusion: 希尔伯特曲线引导下的局部注意力与稀疏块核结合，可高效、实用地提升2D图像局部自注意力效率，有良好的泛用价值。

Abstract: The quadratic compute and memory costs of global self-attention severely
limit its use in high-resolution images. Local attention reduces complexity by
restricting attention to neighborhoods. Block-sparse kernels can further
improve the efficiency of local attention, but conventional local attention
patterns often fail to deliver significant speedups because tokens within a
window are not contiguous in the 1D sequence. This work proposes a novel method
for constructing windows and neighborhoods based on the Hilbert curve. Image
tokens are first reordered along a Hilbert curve, and windows and neighborhoods
are then formed on the reordered 1D sequence. From a block-sparse perspective,
this strategy significantly increases block sparsity and can be combined with
existing block-sparse kernels to improve the efficiency of 2D local attention.
Experiments show that the proposed Hilbert Window Attention and Hilbert Slide
Attention can accelerate window attention and slide attention by about
$4\times$ and $18\times$, respectively. To assess practicality, the strategy is
instantiated as the Hilbert Window Transformer and the Hilbert Neighborhood
Transformer, both of which achieve end-to-end speedups with minimal accuracy
loss. Overall, combining Hilbert-guided local attention with block-sparse
kernels offers a general and practical approach to enhancing the efficiency of
2D local attention for images. The code is available at
https://github.com/Yunge6666/Hilbert-Local-Attention.

</details>


### [39] [TYrPPG: Uncomplicated and Enhanced Learning Capability rPPG for Remote Heart Rate Estimation](https://arxiv.org/abs/2511.05833)
*Taixi Chen,Yiu-ming Cheung*

Main category: cs.CV

TL;DR: 本文提出了一种新型的远程心率检测算法TYrPPG，创新性地融合了GVB模块和改进的损失函数，实验结果表明其在多个数据集上均达到了领先性能。


<details>
  <summary>Details</summary>
Motivation: 当前主流的rPPG心率检测方法大多基于transformer模块，虽然有效但计算效率较低。Mamba模型因其高效特性受到关注，部分研究表明其核心模块SSM对于视觉任务并非必须，因此作者尝试开发基于Mambaout结构的新模型以提升计算效率并简化模型结构。

Method: 提出了TYrPPG算法，创新点包括引入了一种基于Mambaout结构的门控视频理解模块（GVB），融合了2D-CNN和3D-CNN以提升视频理解；此外设计了全面监督损失函数（CSL）及其弱监督变体以增强模型学习能力。

Result: 实验证明TYrPPG方法在常用rPPG数据集上取得了当前最优的性能表现，优于已有的主流方法。

Conclusion: TYrPPG不仅提升了远程心率检测的准确性及运算效率，还展示了Mambaout结构在视觉任务中的应用潜力，预示着未来在无创生理信号提取领域的广阔前景。

Abstract: Remote photoplethysmography (rPPG) can remotely extract physiological signals
from RGB video, which has many advantages in detecting heart rate, such as low
cost and no invasion to patients. The existing rPPG model is usually based on
the transformer module, which has low computation efficiency. Recently, the
Mamba model has garnered increasing attention due to its efficient performance
in natural language processing tasks, demonstrating potential as a substitute
for transformer-based algorithms. However, the Mambaout model and its variants
prove that the SSM module, which is the core component of the Mamba model, is
unnecessary for the vision task. Therefore, we hope to prove the feasibility of
using the Mambaout-based module to remotely learn the heart rate. Specifically,
we propose a novel rPPG algorithm called uncomplicated and enhanced learning
capability rPPG (TYrPPG). This paper introduces an innovative gated video
understanding block (GVB) designed for efficient analysis of RGB videos. Based
on the Mambaout structure, this block integrates 2D-CNN and 3D-CNN to enhance
video understanding for analysis. In addition, we propose a comprehensive
supervised loss function (CSL) to improve the model's learning capability,
along with its weakly supervised variants. The experiments show that our TYrPPG
can achieve state-of-the-art performance in commonly used datasets, indicating
its prospects and superiority in remote heart rate estimation. The source code
is available at https://github.com/Taixi-CHEN/TYrPPG.

</details>


### [40] [Understanding Cross Task Generalization in Handwriting-Based Alzheimer's Screening via Vision Language Adaptation](https://arxiv.org/abs/2511.05841)
*Changqing Gong,Huafeng Qin,Mounim A. El-Yacoubi*

Main category: cs.CV

TL;DR: 该论文提出了一种基于手写分析的阿尔茨海默病（AD）早期筛查方法，并首次将大规模视觉语言模型（如CLIP）应用于此任务，系统性研究了不同手写任务对诊断性能和泛化能力的影响。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病早期检测至关重要，手写能力的下降常见于AD早期，但现有基于手写的AD检测多依赖人工特征和特定任务，缺乏对手写任务类型影响和模型跨任务泛化能力的系统研究。与此同时，视觉语言模型在医学图像异常检测表现出强适应性，但尚未在手写疾病检测领域得到应用。

Method: 作者提出了一种轻量级的Cross-Layer Fusion Adapter（CLFA）框架，将CLIP模型中视觉编码器进行多层适配，实现手写医疗特征的逐步对齐。该方法无需提示词，支持高效的零样本推理。在此基础上，通过“跨任务泛化”实验（即在特定手写任务上训练，评估时用未见过的任务）系统分析了不同手写任务在区分AD方面的有效性。

Result: 实验表明，所提框架在不同手写任务之间具有良好的泛化能力，并揭示了特定书写模式及任务类型对AD早期筛查的诊断价值。系统性的分析还发现了一些典型的笔画特征和任务因素有助于早期发现AD。

Conclusion: 本文不仅为基于手写的认知评估和AD早期筛查提供了新的技术和数据基准，也加深了对手写行为与神经退行性疾病关联机制的理解，为今后非侵入式认知障碍筛查工具的研发奠定了基础。

Abstract: Alzheimer's disease is a prevalent neurodegenerative disorder for which early
detection is critical. Handwriting-often disrupted in prodromal AD-provides a
non-invasive and cost-effective window into subtle motor and cognitive decline.
Existing handwriting-based AD studies, mostly relying on online trajectories
and hand-crafted features, have not systematically examined how task type
influences diagnostic performance and cross-task generalization. Meanwhile,
large-scale vision language models have demonstrated remarkable zero or
few-shot anomaly detection in natural images and strong adaptability across
medical modalities such as chest X-ray and brain MRI. However,
handwriting-based disease detection remains largely unexplored within this
paradigm. To close this gap, we introduce a lightweight Cross-Layer Fusion
Adapter framework that repurposes CLIP for handwriting-based AD screening. CLFA
implants multi-level fusion adapters within the visual encoder to progressively
align representations toward handwriting-specific medical cues, enabling
prompt-free and efficient zero-shot inference. Using this framework, we
systematically investigate cross-task generalization-training on a specific
handwriting task and evaluating on unseen ones-to reveal which task types and
writing patterns most effectively discriminate AD. Extensive analyses further
highlight characteristic stroke patterns and task-level factors that contribute
to early AD identification, offering both diagnostic insights and a benchmark
for handwriting-based cognitive assessment.

</details>


### [41] [Enhancing Diffusion Model Guidance through Calibration and Regularization](https://arxiv.org/abs/2511.05844)
*Seyed Alireza Javid,Amirhossein Bagheri,Nuria González-Prelcic*

Main category: cs.CV

TL;DR: 本论文提出了提升分类器指导扩散模型生成质量的新方法，通过校准与新颖采样策略，有效克服早期去噪阶段梯度消失的问题，在ImageNet任务上取得领先的生成表现。


<details>
  <summary>Details</summary>
Motivation: 分类器指导的扩散模型虽然在条件图像生成领域表现优秀，但在去噪早期步骤中，分类器容易产生过度自信的预测，导致指导梯度消失，影响生成质量。因此，如何改进分类器的校准性并增强采样指导成为亟需解决的问题。

Method: 作者提出了两项主要改进：（1）提出基于平滑期望校准误差（Smooth ECE）的可微校准目标，通过轻量微调改善分类器的校准性；（2）开发了适用于现有分类器的增强采样指导方法，包括批级重加权的倾斜采样、自适应熵正则采样、以及利用f-散度的采样策略，在无需重新训练扩散模型的前提下增强类别一致性和多样性。

Result: 在ImageNet 128x128实验中，应用本文提出的方法，ResNet-101分类器指导的扩散模型在不需重新训练的情况下，FID分数提升至2.13，超越现有的同类方法。

Conclusion: 经过合理校准与散度感知采样，分类器指导扩散模型的生成性能显著提升，为大规模、条件生成任务提供了实用高效的新途径。

Abstract: Classifier-guided diffusion models have emerged as a powerful approach for
conditional image generation, but they suffer from overconfident predictions
during early denoising steps, causing the guidance gradient to vanish. This
paper introduces two complementary contributions to address this issue. First,
we propose a differentiable calibration objective based on the Smooth Expected
Calibration Error (Smooth ECE), which improves classifier calibration with
minimal fine-tuning and yields measurable improvements in Frechet Inception
Distance (FID). Second, we develop enhanced sampling guidance methods that
operate on off-the-shelf classifiers without requiring retraining. These
include tilted sampling with batch-level reweighting, adaptive
entropy-regularized sampling to preserve diversity, and a novel
f-divergence-based sampling strategy that strengthens class-consistent guidance
while maintaining mode coverage. Experiments on ImageNet 128x128 demonstrate
that our divergence-regularized guidance achieves an FID of 2.13 using a
ResNet-101 classifier, improving upon existing classifier-guided diffusion
methods while requiring no diffusion model retraining. The results show that
principled calibration and divergence-aware sampling provide practical and
effective improvements for classifier-guided diffusion.

</details>


### [42] [Point Cloud Segmentation of Integrated Circuits Package Substrates Surface Defects Using Causal Inference: Dataset Construction and Methodology](https://arxiv.org/abs/2511.05853)
*Bingyang Guo,Qiang Zuo,Ruiyun Yu*

Main category: cs.CV

TL;DR: 本论文针对集成电路陶瓷封装基板（CPS）表面缺陷的3D点云分割问题，建立了高质量的点云数据集CPS3D-Seg，并提出了基于因果推断的新型分割方法CINet，实验证明新方法优于现有主流算法。


<details>
  <summary>Details</summary>
Motivation: CPS作为集成电路封装的重要电子材料，对其表面缺陷检测需求极高。但其结构复杂、缺陷微小且缺乏公开数据集，限制了表面缺陷智能检测技术的发展。

Method: 作者构建了高分辨率、精确标注的CPS3D-Seg点云数据集（1300份20类产品），并基于现有主流算法进行了基线评估。同时，提出了结合结构细化和质量评估模块的因果推断3D分割网络CINet，用以量化点云中的潜在混杂因素。

Result: 基于CPS3D-Seg的数据集实验证明，CINet在mIoU和准确率方面均显著优于现有点云分割算法。

Conclusion: CPS3D-Seg填补了相关公开数据集的空白，CINet推动了3D点云缺陷分割技术发展，有望促进陶瓷基板表面缺陷自动检测工业应用的进步。

Abstract: The effective segmentation of 3D data is crucial for a wide range of
industrial applications, especially for detecting subtle defects in the field
of integrated circuits (IC). Ceramic package substrates (CPS), as an important
electronic material, are essential in IC packaging owing to their superior
physical and chemical properties. However, the complex structure and minor
defects of CPS, along with the absence of a publically available dataset,
significantly hinder the development of CPS surface defect detection. In this
study, we construct a high-quality point cloud dataset for 3D segmentation of
surface defects in CPS, i.e., CPS3D-Seg, which has the best point resolution
and precision compared to existing 3D industrial datasets. CPS3D-Seg consists
of 1300 point cloud samples under 20 product categories, and each sample
provides accurate point-level annotations. Meanwhile, we conduct a
comprehensive benchmark based on SOTA point cloud segmentation algorithms to
validate the effectiveness of CPS3D-Seg. Additionally, we propose a novel 3D
segmentation method based on causal inference (CINet), which quantifies
potential confounders in point clouds through Structural Refine (SR) and
Quality Assessment (QA) Modules. Extensive experiments demonstrate that CINet
significantly outperforms existing algorithms in both mIoU and accuracy.

</details>


### [43] [CGCE: Classifier-Guided Concept Erasure in Generative Models](https://arxiv.org/abs/2511.05865)
*Viet Nguyen,Vishal M. Patel*

Main category: cs.CV

TL;DR: 本文提出了一种高效且实用的概念消除框架CGCE，能在不牺牲生成质量的前提下，有效应对生成式模型中的安全风险和攻击。


<details>
  <summary>Details</summary>
Motivation: 随着大规模生成模型的发展，图像和视频生成质量显著提升，但易生成不安全内容带来了重大安全隐患。现有消除“有害概念”方法易被攻破，且常影响无关内容的生成质量，难以平衡安全和性能。

Method: 提出Classifier-Guided Concept Erasure（CGCE）框架。该方法利用一个轻量级分类器，对文本嵌入进行检测和改写，只在推理阶段对不安全的嵌入做处理。不更改原模型权重，实现快速、多概念的消除，还可并行多个分类器以增强扩展性。

Result: 大量实验表明，CGCE能在各种模型和攻击下实现最先进的概念消除鲁棒性。同时在安全和性能之间达成了领先的平衡，在保持有效防护的同时，最大限度保留模型生成良性内容的能力。

Conclusion: CGCE为生成式AI模型的安全防护提供了实用、扩展性强的解决方案，可适配多种T2I和T2V模型，在安全性和生成质量之间取得优异表现。

Abstract: Recent advancements in large-scale generative models have enabled the
creation of high-quality images and videos, but have also raised significant
safety concerns regarding the generation of unsafe content. To mitigate this,
concept erasure methods have been developed to remove undesirable concepts from
pre-trained models. However, existing methods remain vulnerable to adversarial
attacks that can regenerate the erased content. Moreover, achieving robust
erasure often degrades the model's generative quality for safe, unrelated
concepts, creating a difficult trade-off between safety and performance. To
address this challenge, we introduce Classifier-Guided Concept Erasure (CGCE),
an efficient plug-and-play framework that provides robust concept erasure for
diverse generative models without altering their original weights. CGCE uses a
lightweight classifier operating on text embeddings to first detect and then
refine prompts containing undesired concepts. This approach is highly scalable,
allowing for multi-concept erasure by aggregating guidance from several
classifiers. By modifying only unsafe embeddings at inference time, our method
prevents harmful content generation while preserving the model's original
quality on benign prompts. Extensive experiments show that CGCE achieves
state-of-the-art robustness against a wide range of red-teaming attacks. Our
approach also maintains high generative utility, demonstrating a superior
balance between safety and performance. We showcase the versatility of CGCE
through its successful application to various modern T2I and T2V models,
establishing it as a practical and effective solution for safe generative AI.

</details>


### [44] [Light-Field Dataset for Disparity Based Depth Estimation](https://arxiv.org/abs/2511.05866)
*Suresh Nehra,Aupendu Kar,Jayanta Mukhopadhyay,Prabir Kumar Biswas*

Main category: cs.CV

TL;DR: 本文介绍了一个新的光场图像数据集，包括285张Lytro相机采集的真实图像和13张合成图像，以及模拟双目系统获得的真实与合成光场数据，旨在支持基于视差的光场深度估计算法的开发和测试。


<details>
  <summary>Details</summary>
Motivation: 当前的光场数据集在视差特性、焦点多样性等方面存在不足，限制了新型光场深度估计算法的研究和性能测试。因此，作者有必要创建更全面、特性丰富的数据集以推动该领域进步。

Method: 作者利用Lytro Illum光场相机采集真实光场图像，并通过Blender制作合成数据，还用机械滑轨系统辅助构建双目光场数据，从而获得兼具真实与合成、单目与双目的多样化光场数据集。

Result: 作者成功构建了包含真实光场、合成光场以及双目光场共计近300张多样性极高的光场图像数据集，并分析了焦点位置对三维点视差的影响，指出了现有数据集的不足。

Conclusion: 该公开光场图像数据集为基于视差的光场深度估计算法的研究、开发与测试提供了丰富资源，有助于推动三维重建研究和光场技术应用的发展。

Abstract: A Light Field (LF) camera consists of an additional two-dimensional array of
micro-lenses placed between the main lens and sensor, compared to a
conventional camera. The sensor pixels under each micro-lens receive light from
a sub-aperture of the main lens. This enables the image sensor to capture both
spatial information and the angular resolution of a scene point. This
additional angular information is used to estimate the depth of a 3-D scene.
The continuum of virtual viewpoints in light field data enables efficient depth
estimation using Epipolar Line Images (EPIs) with robust occlusion handling.
However, the trade-off between angular information and spatial information is
very critical and depends on the focal position of the camera. To design,
develop, implement, and test novel disparity-based light field depth estimation
algorithms, the availability of suitable light field image datasets is
essential. In this paper, a publicly available light field image dataset is
introduced and thoroughly described. We have also demonstrated the effect of
focal position on the disparity of a 3-D point as well as the shortcomings of
the currently available light field dataset. The proposed dataset contains 285
light field images captured using a Lytro Illum LF camera and 13 synthetic LF
images. The proposed dataset also comprises a synthetic dataset with similar
disparity characteristics to those of a real light field camera. A real and
synthetic stereo light field dataset is also created by using a mechanical
gantry system and Blender. The dataset is available at
https://github.com/aupendu/light-field-dataset.

</details>


### [45] [MoEGCL: Mixture of Ego-Graphs Contrastive Representation Learning for Multi-View Clustering](https://arxiv.org/abs/2511.05876)
*Jian Zhu,Xin Zou,Jun Sun,Cheng Luo,Lei Liu,Lingfang Zeng,Ning Zhang,Bian Wu,Chang Tang,Lirong Dai*

Main category: cs.CV

TL;DR: 本文提出了一种用于多视图聚类的图神经网络新方法MoEGCL，通过创新的自中心图细粒度融合和对比学习，显著提升了聚类效果。


<details>
  <summary>Details</summary>
Motivation: 现有多视图聚类方法在融合不同视图图结构时，多只做粗粒度的视图级别加权，容易丢失细节信息，限制了聚类性能提升。作者希望通过对图结构进行更细致的融合，实现更优的聚类表现。

Method: 作者提出MoEGCL框架，包含两个关键模块：（1）Mixture of Ego-Graphs Fusion（MoEGF）：以样本为中心构建自中心图，用专家混合网络在样本层面而非仅视图层面细粒度融合各视图信息；（2）Ego Graph Contrastive Learning（EGCL）：通过对比学习促进融合表示与视图特定表示的一致性，并增强同簇样本间的表示相似性。

Result: 大量实验显示，MoEGCL在多项深度多视图聚类任务上均超越了现有方法，达到了最新的性能水平。

Conclusion: MoEGCL通过引入细粒度自中心图融合方案和对比学习，有效解决了传统粗粒度融合的局限，提升了多视图聚类性能，实验效果突出且方法具备实际应用价值。

Abstract: In recent years, the advancement of Graph Neural Networks (GNNs) has
significantly propelled progress in Multi-View Clustering (MVC). However,
existing methods face the problem of coarse-grained graph fusion. Specifically,
current approaches typically generate a separate graph structure for each view
and then perform weighted fusion of graph structures at the view level, which
is a relatively rough strategy. To address this limitation, we present a novel
Mixture of Ego-Graphs Contrastive Representation Learning (MoEGCL). It mainly
consists of two modules. In particular, we propose an innovative Mixture of
Ego-Graphs Fusion (MoEGF), which constructs ego graphs and utilizes a
Mixture-of-Experts network to implement fine-grained fusion of ego graphs at
the sample level, rather than the conventional view-level fusion. Additionally,
we present the Ego Graph Contrastive Learning (EGCL) module to align the fused
representation with the view-specific representation. The EGCL module enhances
the representation similarity of samples from the same cluster, not merely from
the same sample, further boosting fine-grained graph representation. Extensive
experiments demonstrate that MoEGCL achieves state-of-the-art results in deep
multi-view clustering tasks. The source code is publicly available at
https://github.com/HackerHyper/MoEGCL.

</details>


### [46] [Towards Frequency-Adaptive Learning for SAR Despeckling](https://arxiv.org/abs/2511.05890)
*Ziqing Ma,Chang Yang,Zhichang Guo,Yao Li*

Main category: cs.CV

TL;DR: 本文提出一种适用于合成孔径雷达(SAR)图像的新型去斑网络SAR-FAH，能更好地去除噪声并保持结构和纹理。


<details>
  <summary>Details</summary>
Motivation: SAR图像存在严重斑点噪声，影响高精度应用。现有深度学习去斑方法大多用单一网络处理整图，忽视了不同区域有不同噪声特性，导致伪影、边缘模糊和纹理失真。

Method: 提出基于分而治之的频率自适应异构模型：首先利用小波分解将图像划分为不同频带；针对不同频率特性设计专用子网络，低频分支用神经常微分方程维持结构，抑制伪影，高频分支采用变形卷积U-Net增强边缘和纹理。

Result: 在模拟与真实SAR图像上的实验表明，该模型在去噪和结构保持方面优于现有方法。

Conclusion: 频率自适应、异构分治的网络能更有效地处理不同特性区域的噪声，实现更优的SAR图像去斑，既提升去噪效果，又保留细节。

Abstract: Synthetic Aperture Radar (SAR) images are inherently corrupted by speckle
noise, limiting their utility in high-precision applications. While deep
learning methods have shown promise in SAR despeckling, most methods employ a
single unified network to process the entire image, failing to account for the
distinct speckle statistics associated with different spatial physical
characteristics. It often leads to artifacts, blurred edges, and texture
distortion. To address these issues, we propose SAR-FAH, a frequency-adaptive
heterogeneous despeckling model based on a divide-and-conquer architecture.
First, wavelet decomposition is used to separate the image into frequency
sub-bands carrying different intrinsic characteristics. Inspired by their
differing noise characteristics, we design specialized sub-networks for
different frequency components. The tailored approach leverages statistical
variations across frequencies, improving edge and texture preservation while
suppressing noise. Specifically, for the low-frequency part, denoising is
formulated as a continuous dynamic system via neural ordinary differential
equations, ensuring structural fidelity and sufficient smoothness that prevents
artifacts. For high-frequency sub-bands rich in edges and textures, we
introduce an enhanced U-Net with deformable convolutions for noise suppression
and enhanced features. Extensive experiments on synthetic and real SAR images
validate the superior performance of the proposed model in noise removal and
structural preservation.

</details>


### [47] [Hybrid second-order gradient histogram based global low-rank sparse regression for robust face recognition](https://arxiv.org/abs/2511.05893)
*Hongxia Li,Ying Ji,Yongxin Dong,Yuehua Feng*

Main category: cs.CV

TL;DR: 本文提出了一种新的人脸识别回归模型H2H-GLRSR，结合了新型特征描述符和全局低秩稀疏回归，显著提升了在遮挡和光照变化条件下的识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有低秩稀疏回归模型在人脸识别中对遮挡和光照变化的鲁棒性仍有限，因此需要设计更有效的特征提取和鲁棒的回归方法，提升实际场景下的识别准确率。

Method: 1）提出了融合二阶梯度信息的“混合二阶梯度直方图”（H2H）特征，用于更好地刻画人脸局部结构特征；2）将H2H特征与稀疏正则化矩阵回归（SR_NMR）相结合，并对残差矩阵引入全局低秩约束，以捕捉结构化噪声的全局相关性。

Result: 实验显示，在含有遮挡、强光照变化和非受控环境下，该方法在分类准确率上显著优于现有基于回归的人脸识别方法。

Conclusion: H2H-GLRSR模型通过结合新型特征描述符和全局低秩约束，在复杂环境下大幅提升了人脸识别的鲁棒性和准确率，具有良好的应用前景。

Abstract: Low-rank sparse regression models have been widely applied in the field of
face recognition. To further address the challenges caused by complex
occlusions and illumination variations, this paper proposes a Hybrid
Second-Order Gradient Histogram based Global Low-Rank Sparse Regression
(H2H-GLRSR) model. Specifically, a novel feature descriptor called the Hybrid
Second-Order Gradient Histogram (H2H) is first designed to more effectively
characterize the local structural features of facial images. Then, this
descriptor is integrated with the Sparse Regularized Nuclear Norm based Matrix
Regression (SR$\_$NMR). Moreover, a global low-rank constraint is imposed on
the residual matrix, enabling the model to better capture the global
correlations inherent in structured noise. Experimental results demonstrate
that the proposed method significantly outperforms existing regression-based
classification approaches under challenging scenarios involving occlusions,
illumination changes, and unconstrained environments.

</details>


### [48] [Open-World 3D Scene Graph Generation for Retrieval-Augmented Reasoning](https://arxiv.org/abs/2511.05894)
*Fei Yu,Quan Deng,Shengeng Tang,Yuehua Li,Lechao Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种结合视觉-语言模型和检索增强推理的统一框架，实现开放世界3D场景图生成，提升了3D场景理解的通用性和交互性。


<details>
  <summary>Details</summary>
Motivation: 此前3D场景理解受限于封闭词汇监督和静态标注，难以适应开放世界中多变和广泛的场景。为突破此瓶颈，需要让系统能在未知类别和多场景下进行泛化和交互。

Method: 方法包括两个核心模块：（1）动态场景图生成模块，无需固定标签集即可检测目标和推理语义关系；（2）检索增强推理流程，将场景图编码进向量数据库，支持基于文本/图像的查询与多模态探索。核心在于融合视觉-语言模型与高效检索机制。

Result: 在3DSSG和Replica等基准数据集上，方法在场景问答、视觉指代、实例检索和任务规划四项任务中表现出强大的泛化能力和优异性能。

Conclusion: 结合开放词汇感知和检索增强推理为3D场景理解提供了可扩展、高效的解决方案，更适用于多样化和动态的实际应用场景。

Abstract: Understanding 3D scenes in open-world settings poses fundamental challenges
for vision and robotics, particularly due to the limitations of
closed-vocabulary supervision and static annotations. To address this, we
propose a unified framework for Open-World 3D Scene Graph Generation with
Retrieval-Augmented Reasoning, which enables generalizable and interactive 3D
scene understanding. Our method integrates Vision-Language Models (VLMs) with
retrieval-based reasoning to support multimodal exploration and language-guided
interaction. The framework comprises two key components: (1) a dynamic scene
graph generation module that detects objects and infers semantic relationships
without fixed label sets, and (2) a retrieval-augmented reasoning pipeline that
encodes scene graphs into a vector database to support text/image-conditioned
queries. We evaluate our method on 3DSSG and Replica benchmarks across four
tasks-scene question answering, visual grounding, instance retrieval, and task
planning-demonstrating robust generalization and superior performance in
diverse environments. Our results highlight the effectiveness of combining
open-vocabulary perception with retrieval-based reasoning for scalable 3D scene
understanding.

</details>


### [49] [GABFusion: Rethinking Feature Fusion for Low-Bit Quantization of Multi-Task Networks](https://arxiv.org/abs/2511.05898)
*Zhaoyang Wang,Dong Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种新的多任务网络量化方法，通过梯度感知的特征融合和注意力分布对齐方案，在不同架构和量化算法下显著提升量化模型的性能，易于集成，无需改动原网络结构。


<details>
  <summary>Details</summary>
Motivation: 现有的感知量化训练（QAT）在多任务神经网络中表现不佳，主要由于不同任务间特征和梯度的冲突，导致模型性能显著下降。需要有方法来平衡多任务学习下的特征和梯度，并且适配低比特量化。

Method: 提出了梯度感知平衡特征融合（GABFusion），动态平衡任务间的梯度、融合特定特征，适应量化需求。同时提出注意力分布对齐（ADA），通过特征级蒸馏方案指导量化模型。整个框架可与任何QAT方法结合，无需改变网络结构。

Result: 在VOC与COCO等主流数据集上，方法对均值mAP提供了3.3%和1.6%的提升。在YOLOv5的4位量化上，准确率损失仅1.7%。广泛实验验证了该方法对不同架构、不同量化宽度的通用性和有效性。

Conclusion: 论文展示了一个模块化、易集成、广泛兼容的QAT增强框架，有效缓解多任务量化中的特征差异和梯度冲突问题，实验证明其能在低比特率下显著提升模型性能。

Abstract: Despite the effectiveness of quantization-aware training (QAT) in compressing
deep neural networks, its performance on multi-task architectures often
degrades significantly due to task-specific feature discrepancies and gradient
conflicts. To address these challenges, we propose Gradient-Aware Balanced
Feature Fusion (GABFusion), which dynamically balances gradient magnitudes and
fuses task-specific features in a quantization-friendly manner. We further
introduce Attention Distribution Alignment (ADA), a feature-level distillation
strategy tailored for quantized models. Our method demonstrates strong
generalization across network architectures and QAT algorithms, with
theoretical guarantees on gradient bias reduction. Extensive experiments
demonstrate that our strategy consistently enhances a variety of QAT methods
across different network architectures and bit-widths. On PASCAL VOC and COCO
datasets, the proposed approach achieves average mAP improvements of
approximately 3.3% and 1.6%, respectively. When applied to YOLOv5 under 4-bit
quantization, our method narrows the accuracy gap with the full-precision model
to only 1.7% on VOC, showcasing its effectiveness in preserving performance
under low-bit constraints. Notably, the proposed framework is modular, easy to
integrate, and compatible with any existing QAT technique-enhancing the
performance of quantized models without requiring modifications to the original
network architecture.

</details>


### [50] [Causal Tracing of Object Representations in Large Vision Language Models: Mechanistic Interpretability and Hallucination Mitigation](https://arxiv.org/abs/2511.05923)
*Qiming Li,Zekai Ye,Xiaocheng Feng,Weihong Zhong,Weitao Ma,Xiachong Feng*

Main category: cs.CV

TL;DR: 本文提出了一种名为FCCT的细粒度跨模态因果追踪框架，对大型视觉-语言模型的可解释性进行了系统性分析，并提出了无需训练的表现提升方法IRI。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉-语言模型取得了显著进步，但对其机理可解释性的研究仍不充分，现有方法覆盖面有限，无法有效提升模型输出的可靠性和下游任务表现，尤其是在减少幻觉方面。

Method: 提出Fine-grained Cross-modal Causal Tracing（FCCT）框架，系统性量化视觉对象感知因果效应，分析视觉和文本所有token，以及模型的三个核心组件（多头自注意力、前馈网络、隐藏状态）在所有解码器层的作用，并基于分析结果提出IRI（Intermediate Representation Injection）推理时注入中间表征的方法强化视觉信息流。

Result: 发现中间层最后token的多头自注意力对跨模态信息聚合至关重要，前馈网络则表现出视觉对象表征分层存储与传递过程。提出的IRI方法在五个主流基准和多个LVLM上实现一致且领先的性能提升。

Conclusion: FCCT框架实现了对LVLM多层次、多模块的可解释性剖析，结合IRI技术可在不影响推理速度的前提下提升模型感知表现并减少幻觉，推动可用性和下游应用发展。

Abstract: Despite the remarkable advancements of Large Vision-Language Models (LVLMs),
the mechanistic interpretability remains underexplored. Existing analyses are
insufficiently comprehensive and lack examination covering visual and textual
tokens, model components, and the full range of layers. This limitation
restricts actionable insights to improve the faithfulness of model output and
the development of downstream tasks, such as hallucination mitigation. To
address this limitation, we introduce Fine-grained Cross-modal Causal Tracing
(FCCT) framework, which systematically quantifies the causal effects on visual
object perception. FCCT conducts fine-grained analysis covering the full range
of visual and textual tokens, three core model components including multi-head
self-attention (MHSA), feed-forward networks (FFNs), and hidden states, across
all decoder layers. Our analysis is the first to demonstrate that MHSAs of the
last token in middle layers play a critical role in aggregating cross-modal
information, while FFNs exhibit a three-stage hierarchical progression for the
storage and transfer of visual object representations. Building on these
insights, we propose Intermediate Representation Injection (IRI), a
training-free inference-time technique that reinforces visual object
information flow by precisely intervening on cross-modal representations at
specific components and layers, thereby enhancing perception and mitigating
hallucination. Consistent improvements across five widely used benchmarks and
LVLMs demonstrate IRI achieves state-of-the-art performance, while preserving
inference speed and other foundational performance.

</details>


### [51] [CoMA: Complementary Masking and Hierarchical Dynamic Multi-Window Self-Attention in a Unified Pre-training Framework](https://arxiv.org/abs/2511.05929)
*Jiaxuan Li,Qing Xu,Xiangjian He,Ziyu Liu,Chang Xing,Zhen Chen,Daokun Zhang,Rong Qu,Chang Wen Chen*

Main category: cs.CV

TL;DR: 本文提出了一种高效的自监督视觉预训练方法CoMA和一种新型动态多窗口视觉Transformer（DyViT），在保持或提升表现的同时，极大提升了预训练效率与模型参数利用率。


<details>
  <summary>Details</summary>
Motivation: 尽管MAE及其类似方法通过自监督学习提升了图像预训练的效率，并在下游任务表现优异，但随机遮挡策略导致需要更多预训练轮数以保证模型适应性；同时MAE所用的ViT模型因空间分辨率固定导致参数利用效率有限。

Method: 提出Complementary Masked Autoencoders (CoMA)，采用互补遮挡策略统一采样所有像素，促进更全面特征学习；并设计了DyViT，这是一种采用动态多窗口自注意力的分层Transformer，进一步减少参数和计算量，并提升细粒度特征提取能力。

Result: 在ImageNet-1K上预训练时，DyViT（结合CoMA）只需12%的MAE预训练轮数即可达到相同下游表现，每轮预训练时间减少10%，展现出更优的预训练效果与效率。

Conclusion: CoMA和DyViT在提升模型预训练效率、适应性与参数利用率方面表现优异，为自监督视觉预训练提供了新思路。

Abstract: Masked Autoencoders (MAE) achieve self-supervised learning of image
representations by randomly removing a portion of visual tokens and
reconstructing the original image as a pretext task, thereby significantly
enhancing pretraining efficiency and yielding excellent adaptability across
downstream tasks. However, MAE and other MAE-style paradigms that adopt random
masking generally require more pre-training epochs to maintain adaptability.
Meanwhile, ViT in MAE suffers from inefficient parameter use due to fixed
spatial resolution across layers. To overcome these limitations, we propose the
Complementary Masked Autoencoders (CoMA), which employ a complementary masking
strategy to ensure uniform sampling across all pixels, thereby improving
effective learning of all features and enhancing the model's adaptability.
Furthermore, we introduce DyViT, a hierarchical vision transformer that employs
a Dynamic Multi-Window Self-Attention (DM-MSA), significantly reducing the
parameters and FLOPs while improving fine-grained feature learning. Pre-trained
on ImageNet-1K with CoMA, DyViT matches the downstream performance of MAE using
only 12% of the pre-training epochs, demonstrating more effective learning. It
also attains a 10% reduction in pre-training time per epoch, further
underscoring its superior pre-training efficiency.

</details>


### [52] [AD-DAE: Unsupervised Modeling of Longitudinal Alzheimer's Disease Progression with Diffusion Auto-Encoder](https://arxiv.org/abs/2511.05934)
*Ayantika Das,Arunima Sarkar,Keerthi Ram,Mohanasankar Sivaprakasam*

Main category: cs.CV

TL;DR: 本文提出了一种可控的扩散自编码生成模型，用于在无监督条件下从基线医学影像生成疾病进展序列。该方法在阿尔茨海默症数据集上的实验结果表明，其生成的影像质量高，能够捕捉和模拟疾病进展特征。


<details>
  <summary>Details</summary>
Motivation: 当前生成建模方法虽能够高维建模，但对疾病进展建模的可控性有限，尤其是在缺乏个体纵向随访图像的无监督场景下。

Method: 提出了一种可调控的扩散自编码器框架，将医学影像编码到紧致潜在空间，并对潜在表示做受控变换，生成随时间进展的影像，无需个体特定的纵向监督。可控性通过将变化限制在与疾病进展相关的子空间中实现。

Result: 该方法在来自两个不同来源和不同疾病分型的阿尔茨海默症数据集上进行了验证。通过图像质量评估、体积进展分析和下游分类任务，证实了生成影像的有效性和疾病进展建模能力。

Conclusion: 这种方法可在无个体纵向标签的情况下，实现疾病进展的可控影像生成，适用于阿尔茨海默症等慢性疾病进展建模。

Abstract: Generative modeling frameworks have emerged as an effective approach to
capture high-dimensional image distributions from large datasets without
requiring domain-specific knowledge, a capability essential for longitudinal
disease progression modeling. Recent generative modeling approaches have
attempted to capture progression by mapping images into a latent
representational space and then controlling and guiding the representations to
generate follow-up images from a baseline image. However, existing approaches
impose constraints on distribution learning, leading to latent spaces with
limited controllability to generate follow-up images without explicit
supervision from subject-specific longitudinal images. In order to enable
controlled movements in the latent representational space and generate
progression images from a baseline image in an unsupervised manner, we
introduce a conditionable Diffusion Auto-encoder framework. The explicit
encoding mechanism of image-diffusion auto-encoders forms a compact latent
space capturing high-level semantics, providing means to disentangle
information relevant for progression. Our approach leverages this latent space
to condition and apply controlled shifts to baseline representations for
generating follow-up. Controllability is induced by restricting these shifts to
a subspace, thereby isolating progression-related factors from subject
identity-preserving components. The shifts are implicitly guided by correlating
with progression attributes, without requiring subject-specific longitudinal
supervision. We validate the generations through image quality metrics,
volumetric progression analysis, and downstream classification in Alzheimer's
disease datasets from two different sources and disease categories. This
demonstrates the effectiveness of our approach for Alzheimer's progression
modeling and longitudinal image generation.

</details>


### [53] [Interaction-Centric Knowledge Infusion and Transfer for Open-Vocabulary Scene Graph Generation](https://arxiv.org/abs/2511.05935)
*Lin Li,Chuhan Zhang,Dong Zhang,Chong Sun,Chen Li,Long Chen*

Main category: cs.CV

TL;DR: 本论文提出了一种新的端到端开放词汇场景图生成（OVSGG）框架ACC，核心为以交互为中心，显著减少了现有方法对象交互判别不清的问题，实验取得了新SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有OVSGG方法不能很好地区分同类对象的交互与非交互关系，导致伪监督噪声大与fine-tune时匹配模糊，严重影响识别真实交互场景的能力。

Method: 提出交互中心的ACC框架，分为两阶段：1) 用双向交互prompt在预训练阶段生成鲁棒伪监督信号，提升交互知识注入效果；2) 微调时通过交互引导的query选择和交互一致知识蒸馏，分别提升交互对象匹配准确率和关系判别鲁棒性。

Result: 在三个主流数据集上进行了大量实验，ACC在开放词汇场景图生成任务上取得了最新的SOTA性能。

Conclusion: 以交互为驱动的场景图生成范式可显著提升OVSGG能力，ACC作为代表显示了广泛应用潜力。

Abstract: Open-vocabulary scene graph generation (OVSGG) extends traditional SGG by
recognizing novel objects and relationships beyond predefined categories,
leveraging the knowledge from pre-trained large-scale models. Existing OVSGG
methods always adopt a two-stage pipeline: 1) \textit{Infusing knowledge} into
large-scale models via pre-training on large datasets; 2) \textit{Transferring
knowledge} from pre-trained models with fully annotated scene graphs during
supervised fine-tuning. However, due to a lack of explicit interaction
modeling, these methods struggle to distinguish between interacting and
non-interacting instances of the same object category. This limitation induces
critical issues in both stages of OVSGG: it generates noisy pseudo-supervision
from mismatched objects during knowledge infusion, and causes ambiguous query
matching during knowledge transfer. To this end, in this paper, we propose an
inter\textbf{AC}tion-\textbf{C}entric end-to-end OVSGG framework (\textbf{ACC})
in an interaction-driven paradigm to minimize these mismatches. For
\textit{interaction-centric knowledge infusion}, ACC employs a bidirectional
interaction prompt for robust pseudo-supervision generation to enhance the
model's interaction knowledge. For \textit{interaction-centric knowledge
transfer}, ACC first adopts interaction-guided query selection that prioritizes
pairing interacting objects to reduce interference from non-interacting ones.
Then, it integrates interaction-consistent knowledge distillation to bolster
robustness by pushing relational foreground away from the background while
retaining general knowledge. Extensive experimental results on three benchmarks
show that ACC achieves state-of-the-art performance, demonstrating the
potential of interaction-centric paradigms for real-world applications.

</details>


### [54] [Global Multiple Extraction Network for Low-Resolution Facial Expression Recognition](https://arxiv.org/abs/2511.05938)
*Jingyi Shi*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的全局多重特征提取网络（GME-Net），有效提升了低分辨率人脸表情识别的性能。


<details>
  <summary>Details</summary>
Motivation: 当前人脸表情识别算法在高分辨率图像上效果突出，但对低分辨率图像的识别性能明显降低。主要原因在于低分辨率图片细节缺失以及现有方法对全局建模能力较弱，难以提取区分性强的特征。

Method: 作者提出了一种全局多重特征提取网络（GME-Net），包含：1）基于混合注意力机制的局部特征提取模块，结合注意力相似性知识蒸馏，从高分辨率网络中学习图像细节；2）具备准对称结构的多尺度全局特征提取模块，用于减弱局部噪声影响并提升全局特征捕捉能力。

Result: 在多个主流数据集上的大量实验证明，GME-Net在低分辨率人脸表情识别上优于现有方法，取得了更好的表现。

Conclusion: GME-Net能有效提取与表情相关的区分性特征，显著提升低分辨率人脸表情识别精度，优于当前同类模型。

Abstract: Facial expression recognition, as a vital computer vision task, is garnering
significant attention and undergoing extensive research. Although facial
expression recognition algorithms demonstrate impressive performance on
high-resolution images, their effectiveness tends to degrade when confronted
with low-resolution images. We find it is because: 1) low-resolution images
lack detail information; 2) current methods complete weak global modeling,
which make it difficult to extract discriminative features. To alleviate the
above issues, we proposed a novel global multiple extraction network (GME-Net)
for low-resolution facial expression recognition, which incorporates 1) a
hybrid attention-based local feature extraction module with attention
similarity knowledge distillation to learn image details from high-resolution
network; 2) a multi-scale global feature extraction module with quasi-symmetric
structure to mitigate the influence of local image noise and facilitate
capturing global image features. As a result, our GME-Net is capable of
extracting expression-related discriminative features. Extensive experiments
conducted on several widely-used datasets demonstrate that the proposed GME-Net
can better recognize low-resolution facial expression and obtain superior
performance than existing solutions.

</details>


### [55] [Polymap: generating high definition map based on rasterized polygons](https://arxiv.org/abs/2511.05944)
*Shiyu Gao,Hao Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种基于实例分割的高精地图构建方法，提升了在自动标注系统中的泛化能力，并在Nuscene数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有检测式方法虽能实时构建高精地图，但泛化能力不足，难以适用于自动标注系统。为提升泛化能力，作者重新定义道路元素表达方式。

Method: 将道路元素视为栅格化多边形，采用基于实例分割的transformer生成实例掩码，并通过Potrace模块后处理实现向量化输出，实现端到端高精地图构建。

Result: 在Nuscene数据集上验证了该方法的有效性和良好的泛化能力，优于检测式方法。

Conclusion: 基于实例分割和向量化后处理的高精地图构建方法有助于提升自动标注系统的泛化性能，并获得了实证支持。

Abstract: The perception of high-definition maps is an integral component of
environmental perception in autonomous driving systems. Existing research have
often focused on online construction of high-definition maps. For instance, the
Maptr[9] series employ a detection-based method to output vectorized map
instances parallelly in an end-to-end manner. However, despite their capability
for real-time construction, detection-based methods are observed to lack robust
generalizability[19], which hampers their applicability in auto-labeling
systems. Therefore, aiming to improve the generalizability, we reinterpret road
elements as rasterized polygons and design a concise framework based on
instance segmentation. Initially, a segmentation-based transformer is employed
to deliver instance masks in an end-to-end manner; succeeding this step, a
Potrace-based[17] post-processing module is used to ultimately yield vectorized
map elements. Quantitative results attained on the Nuscene[1] dataset
substantiate the effectiveness and generaliz-ability of our method.

</details>


### [56] [Runtime Safety Monitoring of Deep Neural Networks for Perception: A Survey](https://arxiv.org/abs/2511.05982)
*Albert Schotschneider,Svetlana Pavlitska,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 本文综述了在DNN推理阶段并行运行的安全监控方法，用以检测泛化误差、OOD输入及对抗攻击等安全风险，梳理了输入、内部表征及输出三类监控技术，并分析了各自优缺点与挑战。


<details>
  <summary>Details</summary>
Motivation: DNN虽在自动驾驶和机器人感知系统广泛应用，但易受各种安全问题影响，可能导致危害性故障。因此需要无需修改模型即可实时保障DNN推理安全的监控技术。

Method: 对现有运行时安全监控方法进行系统性分类，总结为输入、内部表征和输出三大类，分别梳理主流技术、优缺点及实际应对的安全风险，并探讨方法局限与未解决问题。

Result: 明确了不同类别监控方法对应的实际用途和研究现状，指出各自目前能够覆盖的风险类型及难点，同时梳理了方法之间的互补性和空白点。

Conclusion: 运行时安全监控为DNN在安全关键领域的部署提供有效保障，但仍存在待解决问题。未来需突破当前技术瓶颈，提升监控全面性与可靠性。

Abstract: Deep neural networks (DNNs) are widely used in perception systems for
safety-critical applications, such as autonomous driving and robotics. However,
DNNs remain vulnerable to various safety concerns, including generalization
errors, out-of-distribution (OOD) inputs, and adversarial attacks, which can
lead to hazardous failures. This survey provides a comprehensive overview of
runtime safety monitoring approaches, which operate in parallel to DNNs during
inference to detect these safety concerns without modifying the DNN itself. We
categorize existing methods into three main groups: Monitoring inputs, internal
representations, and outputs. We analyze the state-of-the-art for each
category, identify strengths and limitations, and map methods to the safety
concerns they address. In addition, we highlight open challenges and future
research directions.

</details>


### [57] [Reperio-rPPG: Relational Temporal Graph Neural Networks for Periodicity Learning in Remote Physiological Measurement](https://arxiv.org/abs/2511.05946)
*Ba-Thinh Nguyen,Thach-Ha Ngoc Pham,Hoang-Long Duc Nguyen,Thi-Duyen Ngo,Thanh-Ha Le*

Main category: cs.CV

TL;DR: 本文提出了一种新的rPPG框架Reperio-rPPG，通过结合关系卷积网络和图变换器，更好地捕捉生理信号的周期性特征，并使用定制的CutMix增强手段提升模型泛化能力。在三个主流数据集上展现出领先性能和强健适应性。


<details>
  <summary>Details</summary>
Motivation: 现有的rPPG方法在非接触式生理信号测量中取得了进展，但对信号固有的周期性建模不足，限制了模型在复杂真实环境下的时序动态捕捉能力。此外，公开数据集多样性不足也影响了泛化能力。

Method: 作者提出Reperio-rPPG框架，将关系卷积网络与图变换器结合，有针对性地建模生理信号的周期结构；同时提出定制的CutMix数据增强方法，提升模型应对多样场景的能力。

Result: 在PURE、UBFC-rPPG、MMPD三个主流数据集上的大量实验结果显示，该方法不仅取得了领先的准确率，还在多种运动和光照条件下展现出优越的鲁棒性和泛化性。

Conclusion: Reperio-rPPG有效提升了rPPG技术对生理信号周期性的捕捉能力，并能更好地适应实际复杂环境，对远程健康监测等应用具有重要意义。

Abstract: Remote photoplethysmography (rPPG) is an emerging contactless physiological
sensing technique that leverages subtle color variations in facial videos to
estimate vital signs such as heart rate and respiratory rate. This non-invasive
method has gained traction across diverse domains, including telemedicine,
affective computing, driver fatigue detection, and health monitoring, owing to
its scalability and convenience. Despite significant progress in remote
physiological signal measurement, a crucial characteristic - the intrinsic
periodicity - has often been underexplored or insufficiently modeled in
previous approaches, limiting their ability to capture fine-grained temporal
dynamics under real-world conditions. To bridge this gap, we propose
Reperio-rPPG, a novel framework that strategically integrates Relational
Convolutional Networks with a Graph Transformer to effectively capture the
periodic structure inherent in physiological signals. Additionally, recognizing
the limited diversity of existing rPPG datasets, we further introduce a
tailored CutMix augmentation to enhance the model's generalizability. Extensive
experiments conducted on three widely used benchmark datasets - PURE,
UBFC-rPPG, and MMPD - demonstrate that Reperio-rPPG not only achieves
state-of-the-art performance but also exhibits remarkable robustness under
various motion (e.g., stationary, rotation, talking, walking) and illumination
conditions (e.g., nature, low LED, high LED). The code is publicly available at
https://github.com/deconasser/Reperio-rPPG.

</details>


### [58] [On Accurate and Robust Estimation of 3D and 2D Circular Center: Method and Application to Camera-Lidar Calibration](https://arxiv.org/abs/2511.06611)
*Jiajun Jiang,Xiao Hu,Wancheng Liu,Wei Jiang*

Main category: cs.CV

TL;DR: 提出了一种用于激光雷达-相机外参标定的新方法，显著提高了用圆形靶标实现高精度3D-2D圆心对应的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 圆形标靶因几何一致性和易于检测被广泛用于激光雷达-相机外参标定。但如何准确确定3D-2D圆心对应关系依然存在较大难度，当前方法易受3D/2D圆心估计误差影响，导致标定精度下降。

Method: 作者提出两项技术：1）结合保形几何代数与RANSAC的鲁棒3D圆心估计算法；2）利用弦长方差最小化方法精确恢复2D投影圆心，并通过单应性验证或RANSAC机制解决中心歧义问题。

Result: 在合成数据与真实数据集上，提出的方法在外参估计误差和鲁棒性方面均明显优于当前主流方法，对各类传感器与圆形靶标（包括自然物体）均表现良好。

Conclusion: 本文方法在提升圆形靶标标定准确性、鲁棒性及适用范围方面效果显著，有助于推动激光雷达-相机标定实用性和标准化，代码将开源以促进复现与应用。

Abstract: Circular targets are widely used in LiDAR-camera extrinsic calibration due to
their geometric consistency and ease of detection. However, achieving accurate
3D-2D circular center correspondence remains challenging. Existing methods
often fail due to decoupled 3D fitting and erroneous 2D ellipse-center
estimation. To address this, we propose a geometrically principled framework
featuring two innovations: (i) a robust 3D circle center estimator based on
conformal geometric algebra and RANSAC; and (ii) a chord-length variance
minimization method to recover the true 2D projected center, resolving its
dual-minima ambi- guity via homography validation or a quasi-RANSAC fallback.
Evaluated on synthetic and real-world datasets, our framework significantly
outperforms state-of-the-art approaches. It reduces extrinsic estimation error
and enables robust calibration across diverse sensors and target types,
including natural circular objects. Our code will be publicly released for
reproducibility.

</details>


### [59] [U(PM)$^2$:Unsupervised polygon matching with pre-trained models for challenging stereo images](https://arxiv.org/abs/2511.05949)
*Chang Li,Xingtao Peng*

Main category: cs.CV

TL;DR: 本论文提出了一种新颖的无监督多边形匹配方法U(PM)^2，通过结合自动学习和手工特征，有效解决了多边形匹配中视差不连续、尺度变化、训练依赖和泛化性等挑战。


<details>
  <summary>Details</summary>
Motivation: 传统的立体图像匹配已经广泛研究，但多边形匹配领域几乎未被探索，且面临诸多如视差、尺度变化、对模型训练的需求及泛化能力等实际应用问题。

Method: 提出的U(PM)^2方法包括：1）利用预训练的分割模型（SAM）获得掩码，向量化转化为多边形和图结构；2）全局匹配器借助双向金字塔策略和LoFTR应对全局视点变化和尺度变化；3）局部匹配器结合局部几何与多特征匹配，并通过匈牙利算法处理局部视差和拓扑不一致。

Result: 在ScanNet和SceneFlow数据集上，基于提出的新评测指标进行测试，U(PM)^2在无需训练的前提下，实现了最快、最优的准确率，同时具有较强的泛化能力和低成本优势。

Conclusion: U(PM)^2方法在多边形匹配领域取得了前所未有的效果，并且避免了昂贵的模型训练，显示出良好的应用前景和推广可能性。

Abstract: Stereo image matching is a fundamental task in computer vision,
photogrammetry and remote sensing, but there is an almost unexplored field,
i.e., polygon matching, which faces the following challenges: disparity
discontinuity, scale variation, training requirement, and generalization. To
address the above-mentioned issues, this paper proposes a novel U(PM)$^2$:
low-cost unsupervised polygon matching with pre-trained models by uniting
automatically learned and handcrafted features, of which pipeline is as
follows: firstly, the detector leverages the pre-trained segment anything model
to obtain masks; then, the vectorizer converts the masks to polygons and
graphic structure; secondly, the global matcher addresses challenges from
global viewpoint changes and scale variation based on bidirectional-pyramid
strategy with pre-trained LoFTR; finally, the local matcher further overcomes
local disparity discontinuity and topology inconsistency of polygon matching by
local-joint geometry and multi-feature matching strategy with Hungarian
algorithm. We benchmark our U(PM)$^2$ on the ScanNet and SceneFlow datasets
using our proposed new metric, which achieved state-of-the-art accuracy at a
competitive speed and satisfactory generalization performance at low cost
without any training requirement.

</details>


### [60] [PanoNav: Mapless Zero-Shot Object Navigation with Panoramic Scene Parsing and Dynamic Memory](https://arxiv.org/abs/2511.06840)
*Qunchao Jin,Yilin Wu,Changhao Chen*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为PanoNav的新方法，实现了仅用RGB输入、无需地图的零样本物体导航，在导航基准测试上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统的零样本物体导航(ZSON)方法常依赖深度传感器或预建地图，限制了多模态大语言模型(MLLMs)的空间推理能力；现有无地图导航容易导致局部死锁，缺乏历史信息利用。

Method: 提出PanoNav框架，完全基于RGB输入，无需地图，包含两个关键模块：1) 全景场景解析模块，利用全景RGB输入提升MLLMs的空间解析能力；2) 记忆引导决策模块，通过动态有界记忆队列整合探索历史，避免导航死锁。

Result: 在公开的导航基准测试集上，PanoNav在SR（成功率）和SPL（路径效率）指标上显著超越基线方法。

Conclusion: PanoNav有效提升了基于RGB输入且无地图的零样本物体导航性能，表明全景解析与记忆强化机制能够更好地利用MLLMs的能力并克服现有不足。

Abstract: Zero-shot object navigation (ZSON) in unseen environments remains a
challenging problem for household robots, requiring strong perceptual
understanding and decision-making capabilities. While recent methods leverage
metric maps and Large Language Models (LLMs), they often depend on depth
sensors or prebuilt maps, limiting the spatial reasoning ability of Multimodal
Large Language Models (MLLMs). Mapless ZSON approaches have emerged to address
this, but they typically make short-sighted decisions, leading to local
deadlocks due to a lack of historical context. We propose PanoNav, a fully
RGB-only, mapless ZSON framework that integrates a Panoramic Scene Parsing
module to unlock the spatial parsing potential of MLLMs from panoramic RGB
inputs, and a Memory-guided Decision-Making mechanism enhanced by a Dynamic
Bounded Memory Queue to incorporate exploration history and avoid local
deadlocks. Experiments on the public navigation benchmark show that PanoNav
significantly outperforms representative baselines in both SR and SPL metrics.

</details>


### [61] [CSGaze: Context-aware Social Gaze Prediction](https://arxiv.org/abs/2511.05955)
*Surbhi Madan,Shreya Ghosh,Ramanathan Subramanian,Abhinav Dhall,Tom Gedeon*

Main category: cs.CV

TL;DR: 本论文提出了一种结合上下文、视觉场景与面部信息的多模态方法CSGaze，用于在多人人像图像中预测和解释对话中的社交视线模式。


<details>
  <summary>Details</summary>
Motivation: 人的视线反应揭示了注意力、社交参与度和自信心，如何有效利用多种环境线索对社交视线进行精准预测一直是计算机视觉领域的难题。

Method: 提出CSGaze模型，结合面部特征、场景信息等多模态输入，并设计以主讲人细粒度注意力机制，捕捉社交视线动态。

Result: 在多个数据集（GP-Static、UCO-LAEO和AVA-LAEO）上，CSGaze在社交视线预测任务中与最新方法相媲美，且在公开集数据测试中显示出良好的泛化性和鲁棒性。模型还通过注意力分数提供了初步可解释性。

Conclusion: 融合上下文信息和细粒度注意力机制可以显著提升社交视线预测的准确性，CSGaze模型具备较强的通用性与解释性，为理解和建模多方互动中的视线模式提供了新思路。

Abstract: A person's gaze offers valuable insights into their focus of attention, level
of social engagement, and confidence. In this work, we investigate how
contextual cues combined with visual scene and facial information can be
effectively utilized to predict and interpret social gaze patterns during
conversational interactions. We introduce CSGaze, a context aware multimodal
approach that leverages facial, scene information as complementary inputs to
enhance social gaze pattern prediction from multi-person images. The model also
incorporates a fine-grained attention mechanism centered on the principal
speaker, which helps in better modeling social gaze dynamics. Experimental
results show that CSGaze performs competitively with state-of-the-art methods
on GP-Static, UCO-LAEO and AVA-LAEO. Our findings highlight the role of
contextual cues in improving social gaze prediction. Additionally, we provide
initial explainability through generated attention scores, offering insights
into the model's decision-making process. We also demonstrate our model's
generalizability by testing our model on open set datasets that demonstrating
its robustness across diverse scenarios.

</details>


### [62] [Aerial Image Stitching Using IMU Data from a UAV](https://arxiv.org/abs/2511.06841)
*Selim Ahmet Iz,Mustafa Unel*

Main category: cs.CV

TL;DR: 本文提出了一种结合IMU数据和计算机视觉技术的无人机图像拼接新方法，提升了对大范围、高分辨率图像的拼接精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统无人机图像拼接常用特征点方法，但在特征检测与匹配时易出现误差和歧义，尤其在大位移、旋转或相机姿态变化大时效果不佳。因此，需探索更可靠的拼接手段。

Method: 该方法结合IMU数据推算无人机间的位移和旋转，校正透视畸变，计算单应性矩阵，再采用标准图像拼接算法对多幅图像进行融合，有效利用了IMU带来的额外信息。

Result: 实验显示该方法在大位移、旋转和复杂相机姿态下，精度和可靠性优于部分现有特征点图像拼接算法，表现更为稳健。

Conclusion: 结合IMU和视觉信息的拼接方法能改善传统拼接技术的局限，适用于无人机高分辨率图像获取，且易于融入现有工作流。

Abstract: Unmanned Aerial Vehicles (UAVs) are widely used for aerial photography and
remote sensing applications. One of the main challenges is to stitch together
multiple images into a single high-resolution image that covers a large area.
Featurebased image stitching algorithms are commonly used but can suffer from
errors and ambiguities in feature detection and matching. To address this,
several approaches have been proposed, including using bundle adjustment
techniques or direct image alignment. In this paper, we present a novel method
that uses a combination of IMU data and computer vision techniques for
stitching images captured by a UAV. Our method involves several steps such as
estimating the displacement and rotation of the UAV between consecutive images,
correcting for perspective distortion, and computing a homography matrix. We
then use a standard image stitching algorithm to align and blend the images
together. Our proposed method leverages the additional information provided by
the IMU data, corrects for various sources of distortion, and can be easily
integrated into existing UAV workflows. Our experiments demonstrate the
effectiveness and robustness of our method, outperforming some of the existing
feature-based image stitching algorithms in terms of accuracy and reliability,
particularly in challenging scenarios such as large displacements, rotations,
and variations in camera pose.

</details>


### [63] [Adaptive Agent Selection and Interaction Network for Image-to-point cloud Registration](https://arxiv.org/abs/2511.05965)
*Zhixin Cheng,Xiaotian Yin,Jiacheng Deng,Bohao Liao,Yujia Chen,Xu Zhou,Baoqun Yin,Tianzhu Zhang*

Main category: cs.CV

TL;DR: 本论文提出了一种新颖的跨模态图像到点云配准框架，通过引入迭代代理选择和可靠代理交互模块来提升鲁棒性和精度，并在主流数据集上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有无检测的图像与点云配准方法依赖transformer聚合跨模态特征，但在存在噪声且无专门特征选择设计的情况下，易导致错误匹配和低鲁棒性。

Method: 作者提出了包含两个关键模块的框架：1）迭代代理选择（IAS）模块通过相位图强化结构特征感知，并用强化学习高效选择可靠代理。2）可靠代理交互（RAI）模块利用这些被选代理引导跨模态特征交互，减少错配。

Result: 在RGB-D Scenes v2和7-Scenes基准数据集上，大量实验证明该方法稳定取得了最新最优的配准性能。

Conclusion: 该方法有效增强了配准过程中的鲁棒性和准确率，尤其在噪声环境下表现突出，并刷新了相关任务的性能纪录。

Abstract: Typical detection-free methods for image-to-point cloud registration leverage
transformer-based architectures to aggregate cross-modal features and establish
correspondences. However, they often struggle under challenging conditions,
where noise disrupts similarity computation and leads to incorrect
correspondences. Moreover, without dedicated designs, it remains difficult to
effectively select informative and correlated representations across
modalities, thereby limiting the robustness and accuracy of registration. To
address these challenges, we propose a novel cross-modal registration framework
composed of two key modules: the Iterative Agents Selection (IAS) module and
the Reliable Agents Interaction (RAI) module. IAS enhances structural feature
awareness with phase maps and employs reinforcement learning principles to
efficiently select reliable agents. RAI then leverages these selected agents to
guide cross-modal interactions, effectively reducing mismatches and improving
overall robustness. Extensive experiments on the RGB-D Scenes v2 and 7-Scenes
benchmarks demonstrate that our method consistently achieves state-of-the-art
performance.

</details>


### [64] [Real-Time LiDAR Super-Resolution via Frequency-Aware Multi-Scale Fusion](https://arxiv.org/abs/2511.07377)
*June Moh Goo,Zichao Zeng,Jan Boehm*

Main category: cs.CV

TL;DR: FLASH是一种用于LiDAR超分辨率的新框架，通过空间和频率双域处理，显著提升了点云重建质量，并在KITTI数据集上实现了新的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 目前基于Transformer的方法（如TULIP）虽然表现出色，但只在空间域操作，感受野有限，难以兼顾局部和全局特征，且对不确定性建模依赖高计算成本的多次推理。为此，亟需设计一个能高效兼顾空间和频率信息，且能以单次推理高效处理不确定性的LiDAR点云超分辨率方法。

Method: 提出FLASH框架，核心包括两大创新：1）频率感知窗口注意力机制，结合局部空间自注意力和全局频域分析（通过FFT），提升细节与周期模式捕获能力；2）自适应多尺度融合，用学习型特征聚合机制取代传统跳跃连接，并用CBAM注意力动态优化特征选择，从而提升融合效果。

Result: 在KITTI数据集上进行大量实验，FLASH在所有评测指标上均做到SOTA，超越依赖多次推理的不确定性增强方法。单次推理即可实现高精度和高效率，尤其优于TULIP结合Monte Carlo Dropout的方案。

Conclusion: FLASH通过架构创新将空间与频率域特征深度融合，显著提升了LiDAR点云超分辨率能力，并以高效单次推理设计使其实用性大幅增强，为自动驾驶等实时场景带来实际应用价值。

Abstract: LiDAR super-resolution addresses the challenge of achieving high-quality 3D
perception from cost-effective, low-resolution sensors. While recent
transformer-based approaches like TULIP show promise, they remain limited to
spatial-domain processing with restricted receptive fields. We introduce FLASH
(Frequency-aware LiDAR Adaptive Super-resolution with Hierarchical fusion), a
novel framework that overcomes these limitations through dual-domain
processing. FLASH integrates two key innovations: (i) Frequency-Aware Window
Attention that combines local spatial attention with global frequency-domain
analysis via FFT, capturing both fine-grained geometry and periodic scanning
patterns at log-linear complexity. (ii) Adaptive Multi-Scale Fusion that
replaces conventional skip connections with learned position-specific feature
aggregation, enhanced by CBAM attention for dynamic feature selection.
Extensive experiments on KITTI demonstrate that FLASH achieves state-of-the-art
performance across all evaluation metrics, surpassing even uncertainty-enhanced
baselines that require multiple forward passes. Notably, FLASH outperforms
TULIP with Monte Carlo Dropout while maintaining single-pass efficiency, which
enables real-time deployment. The consistent superiority across all distance
ranges validates that our dual-domain approach effectively handles uncertainty
through architectural design rather than computationally expensive stochastic
inference, making it practical for autonomous systems.

</details>


### [65] [Commonality in Few: Few-Shot Multimodal Anomaly Detection via Hypergraph-Enhanced Memory](https://arxiv.org/abs/2511.05966)
*Yuxuan Lin,Hanjing Yan,Xuan Tong,Yang Chang,Huanzhen Wang,Ziheng Zhou,Shuyong Gao,Yan Wang,Wenqiang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的少样本多模态工业异常检测方法CIF，利用超图提取训练样本的结构共性，通过结构信息提升检测性能，并在主流数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 少样本环境下，工业异常检测样本数量有限，训练样本难以覆盖测试样本的多样性，因此亟需有效利用有限数据的结构信息来提升检测效果。

Method: 方法包括三大创新：(1)设计了语义感知超图构建模块，专为单语义工业图像建模并提取结构共性，填充结构记忆库；(2)提出无训练超图消息传递模块，更新测试样本视觉特征，缩小测试与记忆库特征分布距离；(3)新颖的超边引导记忆检索模块，利用结构信息辅助检索，降低误报率。

Result: 在MVTec 3D-AD和Eyecandies数据集上进行实验，CIF方法在少样本场景下表现优于现有最佳方法（SOTA）。

Conclusion: 结构共性挖掘和超图建模显著提升了多模态少样本工业异常检测能力，为今后相关研究提供了新思路。

Abstract: Few-shot multimodal industrial anomaly detection is a critical yet
underexplored task, offering the ability to quickly adapt to complex industrial
scenarios. In few-shot settings, insufficient training samples often fail to
cover the diverse patterns present in test samples. This challenge can be
mitigated by extracting structural commonality from a small number of training
samples. In this paper, we propose a novel few-shot unsupervised multimodal
industrial anomaly detection method based on structural commonality, CIF
(Commonality In Few). To extract intra-class structural information, we employ
hypergraphs, which are capable of modeling higher-order correlations, to
capture the structural commonality within training samples, and use a memory
bank to store this intra-class structural prior. Firstly, we design a
semantic-aware hypergraph construction module tailored for single-semantic
industrial images, from which we extract common structures to guide the
construction of the memory bank. Secondly, we use a training-free hypergraph
message passing module to update the visual features of test samples, reducing
the distribution gap between test features and features in the memory bank. We
further propose a hyperedge-guided memory search module, which utilizes
structural information to assist the memory search process and reduce the false
positive rate. Experimental results on the MVTec 3D-AD dataset and the
Eyecandies dataset show that our method outperforms the state-of-the-art (SOTA)
methods in few-shot settings. Code is available at
https://github.com/Sunny5250/CIF.

</details>


### [66] [TwinOR: Photorealistic Digital Twins of Dynamic Operating Rooms for Embodied AI Research](https://arxiv.org/abs/2511.07412)
*Han Zhang,Yiqing Shen,Roger D. Soberanis-Mukul,Ankita Ghosh,Hao Ding,Lalithkumar Seenivasan,Jose L. Porras,Zhekai Mao,Chenjia Li,Wenjie Xiao,Lonny Yarmus,Angela Christine Argento,Masaru Ishii,Mathias Unberath*

Main category: cs.CV

TL;DR: 该论文提出了TwinOR框架，实现了手术室(OR)的高保真、动态数字孪生，为具身AI的开发和评估提供了安全可控的仿真环境。实验表明，TwinOR合成数据下的感知和定位任务表现与真实数据相近，支持安全、可扩展的AI开发。


<details>
  <summary>Details</summary>
Motivation: 受限于手术室的安全法规和操作要求，具身AI无法在真实环境中自由学习和评估，缺乏既真实又能动态模拟手术室环境的数字孪生系统。

Method: TwinOR框架通过视频预扫描重建静态几何信息，并利用多视角感知动态捕捉手术室内的人员与设备运动，再将二者融合，构建沉浸式三维环境，支持模拟与交互。

Result: TwinOR能以厘米级精度重建手术室几何结构，保留动态交互特性。实验中，FoundationStereo和ORB-SLAM3等模型在TwinOR数据上感知和定位性能接近真实数据，证明了其传感器级真实性。

Conclusion: TwinOR实现了手术室环境的动态、写实数字孪生，为具身AI的研发和验证提供了安全、高效、可扩展的平台，有助于AI从仿真到现实的快速部署。

Abstract: Developing embodied AI for intelligent surgical systems requires safe,
controllable environments for continual learning and evaluation. However,
safety regulations and operational constraints in operating rooms (ORs) limit
embodied agents from freely perceiving and interacting in realistic settings.
Digital twins provide high-fidelity, risk-free environments for exploration and
training. How we may create photorealistic and dynamic digital representations
of ORs that capture relevant spatial, visual, and behavioral complexity remains
unclear. We introduce TwinOR, a framework for constructing photorealistic,
dynamic digital twins of ORs for embodied AI research. The system reconstructs
static geometry from pre-scan videos and continuously models human and
equipment motion through multi-view perception of OR activities. The static and
dynamic components are fused into an immersive 3D environment that supports
controllable simulation and embodied exploration. The proposed framework
reconstructs complete OR geometry with centimeter level accuracy while
preserving dynamic interaction across surgical workflows, enabling realistic
renderings and a virtual playground for embodied AI systems. In our
experiments, TwinOR simulates stereo and monocular sensor streams for geometry
understanding and visual localization tasks. Models such as FoundationStereo
and ORB-SLAM3 on TwinOR-synthesized data achieve performance within their
reported accuracy on real indoor datasets, demonstrating that TwinOR provides
sensor-level realism sufficient for perception and localization challenges. By
establishing a real-to-sim pipeline for constructing dynamic, photorealistic
digital twins of OR environments, TwinOR enables the safe, scalable, and
data-efficient development and benchmarking of embodied AI, ultimately
accelerating the deployment of embodied AI from sim-to-real.

</details>


### [67] [Adapted Foundation Models for Breast MRI Triaging in Contrast-Enhanced and Non-Contrast Enhanced Protocols](https://arxiv.org/abs/2511.05967)
*Tri-Thien Nguyen,Lorenz A. Kapsner,Tobias Hepp,Shirin Heidarikahkesh,Hannes Schreiter,Luise Brock,Dominika Skwierawska,Dominique Hadler,Julian Hossbach,Evelyn Wenkel,Sabine Ohlmeyer,Frederik B. Laun,Andrzej Liebert,Andreas Maier,Michael Uder,Sebastian Bickelhaupt*

Main category: cs.CV

TL;DR: 本研究评估了一种基于DINOv2的医学切片变换器（MST）在乳腺MRI筛查中的应用，旨在高灵敏度下有效排除重大阳性病例，提高诊断效率。


<details>
  <summary>Details</summary>
Motivation: 乳腺MRI虽对癌症检测灵敏，但读片耗时，AI有望辅助预筛查以减轻医生负担，提高筛查效率。

Method: 回顾性分析1847例自建乳腺MRI和924例外部验证（Duke）。测试四种不同协议下的模型表现，以AUC评价诊断准确度，通过交叉验证多种灵敏度（90%、95%、97.5%）下的能力，比较AUC以及分析漏诊病例和注意力图质量。

Result: T1sub+T2w协议的AUC为0.77，在97.5%灵敏度下，特异性19%；DWI1500+T2w为0.74，特异性17%。漏诊病灶多数为<10mm的非肿块强化。外部数据集AUC同为0.77，88%的注意力图评价良好或中等。

Conclusion: MST模型能在高灵敏度下显著筛除非重要阴性病例，对提高MRI筛查效率有潜力，但临床应用前仍需更多研究验证。

Abstract: Background: Magnetic resonance imaging (MRI) has high sensitivity for breast
cancer detection, but interpretation is time-consuming. Artificial intelligence
may aid in pre-screening. Purpose: To evaluate the DINOv2-based Medical Slice
Transformer (MST) for ruling out significant findings (Breast Imaging Reporting
and Data System [BI-RADS] >=4) in contrast-enhanced and non-contrast-enhanced
abbreviated breast MRI. Materials and Methods: This institutional review board
approved retrospective study included 1,847 single-breast MRI examinations (377
BI-RADS >=4) from an in-house dataset and 924 from an external validation
dataset (Duke). Four abbreviated protocols were tested: T1-weighted early
subtraction (T1sub), diffusion-weighted imaging with b=1500 s/mm2 (DWI1500),
DWI1500+T2-weighted (T2w), and T1sub+T2w. Performance was assessed at 90%, 95%,
and 97.5% sensitivity using five-fold cross-validation and area under the
receiver operating characteristic curve (AUC) analysis. AUC differences were
compared with the DeLong test. False negatives were characterized, and
attention maps of true positives were rated in the external dataset. Results: A
total of 1,448 female patients (mean age, 49 +/- 12 years) were included.
T1sub+T2w achieved an AUC of 0.77 +/- 0.04; DWI1500+T2w, 0.74 +/- 0.04
(p=0.15). At 97.5% sensitivity, T1sub+T2w had the highest specificity (19% +/-
7%), followed by DWI1500+T2w (17% +/- 11%). Missed lesions had a mean diameter
<10 mm at 95% and 97.5% thresholds for both T1sub and DWI1500, predominantly
non-mass enhancements. External validation yielded an AUC of 0.77, with 88% of
attention maps rated good or moderate. Conclusion: At 97.5% sensitivity, the
MST framework correctly triaged cases without BI-RADS >=4, achieving 19%
specificity for contrast-enhanced and 17% for non-contrast-enhanced MRI.
Further research is warranted before clinical implementation.

</details>


### [68] [DiA-gnostic VLVAE: Disentangled Alignment-Constrained Vision Language Variational AutoEncoder for Robust Radiology Reporting with Missing Modalities](https://arxiv.org/abs/2511.05968)
*Nagur Shareef Shaik,Teja Krishna Cherukuri,Adnan Masood,Dong Hye Ye*

Main category: cs.CV

TL;DR: 本文提出了一种名为DiA-gnostic VLVAE的新方法，以更高效且稳健的方式生成医学影像报告。该方法在缺失模态和特征混杂情况下表现出色，并在重要公开数据集上取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 现有自动生成医学影像报告的方法依赖大型语言模型或静态知识图谱，导致高资源消耗，且在实际临床环境中面临缺失模态和特征混杂的问题，难以生成既准确又有临床实际意义的报告。

Method: 作者提出了DiA-gnostic VLVAE框架，通过基于Mixture-of-Experts的视觉-语言变分自编码器（VLVAE）将各模态的共享特征和专有特征进行解耦，并通过受约束优化目标实现特征的正交性和对齐。然后用轻量级的LLaMA-X解码器基于这些解耦特征高效生成医学报告。

Result: 在IU X-Ray和MIMIC-CXR两个数据集上，DiA-gnostic VLVAE获得了BLEU@4分别为0.266和0.134的好成绩，显著优于现有最先进方法。

Conclusion: 该方法能有效处理因模态缺失和特征混杂带来的实际挑战，用更低的资源消耗生成高质量、可信赖的放射学报告，对临床实践具有潜在应用价值。

Abstract: The integration of medical images with clinical context is essential for
generating accurate and clinically interpretable radiology reports. However,
current automated methods often rely on resource-heavy Large Language Models
(LLMs) or static knowledge graphs and struggle with two fundamental challenges
in real-world clinical data: (1) missing modalities, such as incomplete
clinical context , and (2) feature entanglement, where mixed modality-specific
and shared information leads to suboptimal fusion and clinically unfaithful
hallucinated findings. To address these challenges, we propose the DiA-gnostic
VLVAE, which achieves robust radiology reporting through Disentangled
Alignment. Our framework is designed to be resilient to missing modalities by
disentangling shared and modality-specific features using a Mixture-of-Experts
(MoE) based Vision-Language Variational Autoencoder (VLVAE). A constrained
optimization objective enforces orthogonality and alignment between these
latent representations to prevent suboptimal fusion. A compact LLaMA-X decoder
then uses these disentangled representations to generate reports efficiently.
On the IU X-Ray and MIMIC-CXR datasets, DiA has achieved competetive BLEU@4
scores of 0.266 and 0.134, respectively. Experimental results show that the
proposed method significantly outperforms state-of-the-art models.

</details>


### [69] [A Dual-Mode ViT-Conditioned Diffusion Framework with an Adaptive Conditioning Bridge for Breast Cancer Segmentation](https://arxiv.org/abs/2511.05989)
*Prateek Singh,Moumita Dholey,P. K. Vinod*

Main category: cs.CV

TL;DR: 本文提出了一种结合改进UNet生成解码器和ViT编码器的有条件扩散模型，有效提升了乳腺超声图像病灶分割准确性，并引入了多尺度语义融合、自适应损失和双分支架构，取得了新SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 乳腺超声图像由于低对比度、散斑噪声及边界模糊，使得病灶分割极具挑战，传统卷积网络难以获得全局信息，分割结果常有解剖结构不一致的问题，需要新的方法提升分割精度和解剖一致性。

Method: 提出一种有条件扩散去噪模型，利用ViT编码器提取全局特征、改进UNet解码器生成分割，创新性地引入自适应条件桥（ACB）实现多尺度特征融合，提出拓扑结构一致性损失（TDC）抑制分割结构不一致，并设计双头结构加速推断。

Result: 在BUSI、BrEaST和BUS-UCLM三大公开乳腺超声数据集上分别获得Dice分数0.96、0.90、0.97，均达到新的SOTA；消融实验证明每个组件都对模型性能提升至关重要。

Conclusion: 所提模型不仅提升了乳腺超声分割的准确率，还保证了解剖结构的合理性，对小数据集有良好泛化能力。创新方法和架构为医学分割任务提供了新思路。

Abstract: In breast ultrasound images, precise lesion segmentation is essential for
early diagnosis; however, low contrast, speckle noise, and unclear boundaries
make this difficult. Even though deep learning models have demonstrated
potential, standard convolutional architectures frequently fall short in
capturing enough global context, resulting in segmentations that are
anatomically inconsistent. To overcome these drawbacks, we suggest a flexible,
conditional Denoising Diffusion Model that combines an enhanced UNet-based
generative decoder with a Vision Transformer (ViT) encoder for global feature
extraction. We introduce three primary innovations: 1) an Adaptive Conditioning
Bridge (ACB) for efficient, multi-scale fusion of semantic features; 2) a novel
Topological Denoising Consistency (TDC) loss component that regularizes
training by penalizing structural inconsistencies during denoising; and 3) a
dual-head architecture that leverages the denoising objective as a powerful
regularizer, enabling a lightweight auxiliary head to perform rapid and
accurate inference on smaller datasets and a noise prediction head. Our
framework establishes a new state-of-the-art on public breast ultrasound
datasets, achieving Dice scores of 0.96 on BUSI, 0.90 on BrEaST and 0.97 on
BUS-UCLM. Comprehensive ablation studies empirically validate that the model
components are critical for achieving these results and for producing
segmentations that are not only accurate but also anatomically plausible.

</details>


### [70] [Exploring Category-level Articulated Object Pose Tracking on SE(3) Manifolds](https://arxiv.org/abs/2511.05996)
*Xianhui Meng,Yukang Huo,Li Zhang,Liu Liu,Haonan Jiang,Yan Zhong,Pingrui Zhang,Cewu Lu,Jun Liu*

Main category: cs.CV

TL;DR: PPF-Tracker是一种面向关节物体的新型姿态跟踪框架，通过SE(3)群空间的点云准规范化和点对特征实现鲁棒、多环境下的姿态跟踪，实验显示其有效性与强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 关节物体广泛存在于日常生活与机器人操作任务中，但由于其固有的运动学约束，相比刚体物体，关节物体的姿态跟踪问题研究较少，且更具挑战性。现有方法难以行之有效地处理这类约束与特性，亟需新架构提升跟踪效果。

Method: 提出了一种基于点对特征（PPF）的关节物体姿态跟踪框架（PPF-Tracker）。首先，对点云在SE(3)李群空间进行类规范化，随后采用PPF建模关节物体，通过利用SE(3)的不变性质预测姿态参数。最后，融入关节轴的语义信息，为所有物体部分施加统一运动学约束。

Result: PPF-Tracker在合成数据集与真实场景中进行了系统评估，在多种复杂环境下表现出良好的泛化能力和鲁棒性，支持多帧关节物体的精准跟踪。实验证明该方法在效果和稳定性上优于或媲美现有技术。

Conclusion: PPF-Tracker为关节物体的多帧姿态跟踪提供了新思路，展现出强泛化和高稳健性，有望推动机器人、具身智能及增强现实等领域的进一步发展。

Abstract: Articulated objects are prevalent in daily life and robotic manipulation
tasks. However, compared to rigid objects, pose tracking for articulated
objects remains an underexplored problem due to their inherent kinematic
constraints. To address these challenges, this work proposes a novel
point-pair-based pose tracking framework, termed \textbf{PPF-Tracker}. The
proposed framework first performs quasi-canonicalization of point clouds in the
SE(3) Lie group space, and then models articulated objects using Point Pair
Features (PPF) to predict pose voting parameters by leveraging the invariance
properties of SE(3). Finally, semantic information of joint axes is
incorporated to impose unified kinematic constraints across all parts of the
articulated object. PPF-Tracker is systematically evaluated on both synthetic
datasets and real-world scenarios, demonstrating strong generalization across
diverse and challenging environments. Experimental results highlight the
effectiveness and robustness of PPF-Tracker in multi-frame pose tracking of
articulated objects. We believe this work can foster advances in robotics,
embodied intelligence, and augmented reality. Codes are available at
https://github.com/mengxh20/PPFTracker.

</details>


### [71] [MALeR: Improving Compositional Fidelity in Layout-Guided Generation](https://arxiv.org/abs/2511.06002)
*Shivank Saxena,Dhruv Srivastava,Makarand Tapaswi*

Main category: cs.CV

TL;DR: MALeR是一种针对文本生成图像模型在复杂组合场景中出现的问题提出的新方法，通过改进布局和属性绑定机制，有效解决了先前方法存在的主题出界、属性泄露等问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像的模型在处理含有多个主题和属性的组合性场景时，容易出现主题错位、内容分布不自然以及属性混淆等问题，影响生成图像质量和可控性。本文旨在解决这些难题，提升用户对复杂场景生成的控制能力。

Method: 提出了MALeR方法。该方法在输入文本提示和布局信息的基础上，通过防止主题出现在布局以外的区域，同时使用掩码和属性感知的绑定机制，防止属性在不同主题间的泄漏。这样使得每个主题可以同时准确地表现多个属性，适应复杂组合场景生成的需求。

Result: 实验结果显示，MALeR在组合性准确性、生成一致性以及属性绑定方面，均优于现有方法，无论是定性还是定量评价都有显著提升。特别是在多主题、多属性场景的图像生成任务中表现出色。

Conclusion: MALeR有效提升了文本到图像合成模型在复杂组合性场景下的表现，显著增强了细粒度、可控性强的高质量图像生成能力，是解决该领域现有方法不足的一项重要进展。

Abstract: Recent advances in text-to-image models have enabled a new era of creative
and controllable image generation. However, generating compositional scenes
with multiple subjects and attributes remains a significant challenge. To
enhance user control over subject placement, several layout-guided methods have
been proposed. However, these methods face numerous challenges, particularly in
compositional scenes. Unintended subjects often appear outside the layouts,
generated images can be out-of-distribution and contain unnatural artifacts, or
attributes bleed across subjects, leading to incorrect visual outputs. In this
work, we propose MALeR, a method that addresses each of these challenges. Given
a text prompt and corresponding layouts, our method prevents subjects from
appearing outside the given layouts while being in-distribution. Additionally,
we propose a masked, attribute-aware binding mechanism that prevents attribute
leakage, enabling accurate rendering of subjects with multiple attributes, even
in complex compositional scenes. Qualitative and quantitative evaluation
demonstrates that our method achieves superior performance in compositional
accuracy, generation consistency, and attribute binding compared to previous
work. MALeR is particularly adept at generating images of scenes with multiple
subjects and multiple attributes per subject.

</details>


### [72] [How Reasoning Influences Intersectional Biases in Vision Language Models](https://arxiv.org/abs/2511.06005)
*Adit Desai,Sudipta Roy,Mohna Chakraborty*

Main category: cs.CV

TL;DR: 本文系统性地分析了五个开源视觉语言模型（VLMs）在职业预测任务中的社会偏见。实验表明，这些模型存在系统性偏见，其推理方式与人类不同，且可能加剧下游应用的不公平。


<details>
  <summary>Details</summary>
Motivation: 随着VLMs被广泛应用于实际任务，其训练数据中隐含的社会偏见也会体现在输出结果中。VLMs不像人类利用社会和情境线索理解图片，因此可能带来有害影响，故有必要深入分析其推理和偏见。

Method: 作者选用FairFace数据集，在32种职业和三类提示风格下，对五个VLMs的输出和推理进行系统分析，以探究模型在职业预测过程中展现出的社会偏见规律。

Result: 实验揭示了VLMs存在系统性偏见，其推理模式导致了交叉群体的差异，加剧了社会不公平。不同的模型、职业类别和提示风格均反映出这种偏见。

Conclusion: 研究强调在VLM下游部署前，有必要对其推理方式进行人类价值对齐，以减少或防止社会偏见扩散，使模型输出更加公平和合乎伦理。

Abstract: Vision Language Models (VLMs) are increasingly deployed across downstream
tasks, yet their training data often encode social biases that surface in
outputs. Unlike humans, who interpret images through contextual and social
cues, VLMs process them through statistical associations, often leading to
reasoning that diverges from human reasoning. By analyzing how a VLM reasons,
we can understand how inherent biases are perpetuated and can adversely affect
downstream performance. To examine this gap, we systematically analyze social
biases in five open-source VLMs for an occupation prediction task, on the
FairFace dataset. Across 32 occupations and three different prompting styles,
we elicit both predictions and reasoning. Our findings reveal that the biased
reasoning patterns systematically underlie intersectional disparities,
highlighting the need to align VLM reasoning with human values prior to its
downstream deployment.

</details>


### [73] [Distributed Deep Learning for Medical Image Denoising with Data Obfuscation](https://arxiv.org/abs/2511.06006)
*Sulaimon Oyeniyi Adebayo,Ayaz H. Khan*

Main category: cs.CV

TL;DR: 本研究对胸部X光影像去噪进行了探讨，比较了不同深度学习模型和分布式训练策略，实现了快速高效的去噪，并公开了实现代码。


<details>
  <summary>Details</summary>
Motivation: 医学影像去噪有助于提升图像质量并保护敏感信息，尤其是在处理大规模临床数据时，需在加快处理速度与保持准确性间取得平衡。

Method: 使用NIH Chest X-ray14数据集，通过叠加高斯噪声实现轻量级图像模糊保护，比较U-Net与U-Net++两种神经网络模型，并在单GPU、标准多GPU（DataParallel）、优化分布式多GPU（DDP+AMP）配置下利用PyTorch训练和评估。

Result: U-Net++在PSNR和SSIM上表现最好，结构保真度高，但在LPIPS指标上略逊于U-Net。优化后的分布式训练流程将训练时间缩短60%，比标准DataParallel快40%，准确率降低有限。

Conclusion: U-Net++结构加分布式训练的组合实现了高效的医学影像去噪。软件级分布式优化具备在实际临床和科研中提速、增强处理管线能力的潜力。

Abstract: Medical image denoising is essential for improving image quality while
minimizing the exposure of sensitive information, particularly when working
with large-scale clinical datasets. This study explores distributed deep
learning for denoising chest X-ray images from the NIH Chest X-ray14 dataset,
using additive Gaussian noise as a lightweight obfuscation technique. We
implement and evaluate U-Net and U-Net++ architectures under single-GPU,
standard multi-GPU (DataParallel), and optimized multi-GPU training
configurations using PyTorch's DistributedDataParallel (DDP) and Automatic
Mixed Precision (AMP). Our results show that U-Net++ consistently delivers
superior denoising performance, achieving competitive Peak Signal to Noise
Ratio (PSNR) and Structured Similarity Index Method (SSIM) scores, though with
less performance in Learned Perceptual Image Patch Similarity (LPIPS) compared
to U-Net under low and moderate noise levels. This indicates U-Net++'s enhanced
structural fidelity and low perceptual similarity. Meanwhile, our optimized
training pipeline reduces training time by over 60% for both models compared to
single-GPU training, and outperforms standard DataParallel by over 40%, with
only a minor accuracy drop for both models (trading some accuracy for speed).
These findings highlight the effectiveness of software-level optimization in
distributed learning for medical imaging. This work demonstrates the practical
viability of combining architectural design, lightweight obfuscation, and
advanced distributed training strategies to accelerate and enhance medical
image processing pipelines in real-world clinical and research environments.
The full implementation is publicly available at:
https://github.com/Suadey/medical-image-denoising-ddp.

</details>


### [74] [One-Shot Knowledge Transfer for Scalable Person Re-Identification](https://arxiv.org/abs/2511.06016)
*Longhua Li,Lei Qi,Xin Geng*

Main category: cs.CV

TL;DR: 论文提出了一种名为OSKT（One-Shot Knowledge Transfer，一次性知识迁移）的新方法，用于边缘计算场景下人体再识别任务的模型压缩与迁移，以解决传统方法需针对每个目标模型重复计算的问题。


<details>
  <summary>Details</summary>
Motivation: 在边缘计算场景下，需根据不同资源约束生成不同大小的ReID模型，传统压缩方法需为每个学生模型单独计算，效率低下且过程繁琐。因此，亟需一种高效、灵活的知识迁移和模型压缩方法。

Method: 通过将教师模型的知识整合到一个“权重链”中，OSKT方法允许在下游需要具体模型规模时，无需额外计算即可通过扩展权重链获得目标模型，实现一次性知识迁移。

Result: OSKT在人体再识别任务中，显著优于当前最先进的模型压缩方法，同时实现了一次性知识迁移，省去了为每个目标模型单独迁移的计算开销。

Conclusion: OSKT为边缘计算ReID模型压缩与部署提供了高效方案，能根据实际需求快速生成不同规模的模型，显著减少重复计算，推动实际应用。

Abstract: Edge computing in person re-identification (ReID) is crucial for reducing the
load on central cloud servers and ensuring user privacy. Conventional
compression methods for obtaining compact models require computations for each
individual student model. When multiple models of varying sizes are needed to
accommodate different resource conditions, this leads to repetitive and
cumbersome computations. To address this challenge, we propose a novel
knowledge inheritance approach named OSKT (One-Shot Knowledge Transfer), which
consolidates the knowledge of the teacher model into an intermediate carrier
called a weight chain. When a downstream scenario demands a model that meets
specific resource constraints, this weight chain can be expanded to the target
model size without additional computation. OSKT significantly outperforms
state-of-the-art compression methods, with the added advantage of one-time
knowledge transfer that eliminates the need for frequent computations for each
target model.

</details>


### [75] [MiVID: Multi-Strategic Self-Supervision for Video Frame Interpolation using Diffusion Model](https://arxiv.org/abs/2511.06019)
*Priyansh Srivastava,Romit Chatterjee,Abir Sen,Aradhana Behura,Ratnakar Dash*

Main category: cs.CV

TL;DR: MiVID提出了一种轻量级、无需运动估计和高帧率监督的自监督扩散式视频插帧框架，在资源有限条件下依然实现了媲美主流有监督方法的效果。


<details>
  <summary>Details</summary>
Motivation: 传统的VFI方法依赖光流或密集标签，难以应对遮挡、领域偏移和不确定运动等实际问题，因此需要无需密集标签、鲁棒性更强的方法。

Method: 提出了MiVID框架，基于3D U-Net和时域Transformer注意力，无需显式运动估计。使用混合掩码训练策略（模仿遮挡和运动不确定性）、余弦进度混合掩码和自适应损失调度，实现自监督训练，无需高帧率标签。

Result: 在UCF101-7和DAVIS-7数据集上，MiVID仅用CPU和9帧短序列训练，50个epoch内达到与多种有监督基线相竞争的插帧效果。

Conclusion: MiVID展现了自监督扩散先验在时序一致帧合成上的潜力，为可推广、低资源消耗的视频插帧系统提供了可扩展路径。

Abstract: Video Frame Interpolation (VFI) remains a cornerstone in video enhancement,
enabling temporal upscaling for tasks like slow-motion rendering, frame rate
conversion, and video restoration. While classical methods rely on optical flow
and learning-based models assume access to dense ground-truth, both struggle
with occlusions, domain shifts, and ambiguous motion. This article introduces
MiVID, a lightweight, self-supervised, diffusion-based framework for video
interpolation. Our model eliminates the need for explicit motion estimation by
combining a 3D U-Net backbone with transformer-style temporal attention,
trained under a hybrid masking regime that simulates occlusions and motion
uncertainty. The use of cosine-based progressive masking and adaptive loss
scheduling allows our network to learn robust spatiotemporal representations
without any high-frame-rate supervision. Our framework is evaluated on UCF101-7
and DAVIS-7 datasets. MiVID is trained entirely on CPU using the datasets and
9-frame video segments, making it a low-resource yet highly effective pipeline.
Despite these constraints, our model achieves optimal results at just 50
epochs, competitive with several supervised baselines.This work demonstrates
the power of self-supervised diffusion priors for temporally coherent frame
synthesis and provides a scalable path toward accessible and generalizable VFI
systems.

</details>


### [76] [Towards Implicit Aggregation: Robust Image Representation for Place Recognition in the Transformer Era](https://arxiv.org/abs/2511.06024)
*Feng Lu,Tong Jin,Canming Ye,Yunpeng Liu,Xiangyuan Lan,Chun Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种无需传统聚合器的新型视觉位置识别(VPR)方法，仅依赖于变换器骨干网络和可学习的聚合token来生成全局特征描述子，简化结构并提升效果，在多个数据集和挑战赛中性能优异。


<details>
  <summary>Details</summary>
Motivation: VPR传统方法依赖骨干网络与专用聚合器相结合来生成全局特征描述。这种范式在CNN及Transformer时代均占主流，然而聚合器部分是否必要仍有待探讨。

Method: 作者提出直接在Transformer骨干网络中引入可学习的聚合token，并置于特定Transformer模块前。这些token与图像patch token共同输入，通过自注意力机制使聚合token自动融合关键信息，最终拼接输出聚合token作为图像全局描述。同时，论文通过实证探索最佳token插入位置和初始化策略。

Result: 实验证明，新方法在多个VPR公开数据集上优于现有主流方法，并在MSLS挑战榜单上夺得第一，表现出更高效和更高精度。

Conclusion: 变换器时代无需专门聚合器，背骨加可学习聚合token即可实现高效、强健的VPR全局特征描述。论文提出的token插入和初始化策略有效，有潜力简化VPR建模流程。

Abstract: Visual place recognition (VPR) is typically regarded as a specific image
retrieval task, whose core lies in representing images as global descriptors.
Over the past decade, dominant VPR methods (e.g., NetVLAD) have followed a
paradigm that first extracts the patch features/tokens of the input image using
a backbone, and then aggregates these patch features into a global descriptor
via an aggregator. This backbone-plus-aggregator paradigm has achieved
overwhelming dominance in the CNN era and remains widely used in
transformer-based models. In this paper, however, we argue that a dedicated
aggregator is not necessary in the transformer era, that is, we can obtain
robust global descriptors only with the backbone. Specifically, we introduce
some learnable aggregation tokens, which are prepended to the patch tokens
before a particular transformer block. All these tokens will be jointly
processed and interact globally via the intrinsic self-attention mechanism,
implicitly aggregating useful information within the patch tokens to the
aggregation tokens. Finally, we only take these aggregation tokens from the
last output tokens and concatenate them as the global representation. Although
implicit aggregation can provide robust global descriptors in an extremely
simple manner, where and how to insert additional tokens, as well as the
initialization of tokens, remains an open issue worthy of further exploration.
To this end, we also propose the optimal token insertion strategy and token
initialization method derived from empirical studies. Experimental results show
that our method outperforms state-of-the-art methods on several VPR datasets
with higher efficiency and ranks 1st on the MSLS challenge leaderboard. The
code is available at https://github.com/lu-feng/image.

</details>


### [77] [S2ML: Spatio-Spectral Mutual Learning for Depth Completion](https://arxiv.org/abs/2511.06033)
*Zihui Zhao,Yifei Zhang,Zheng Wang,Yang Li,Kui Jiang,Zihan Geng,Chia-Wen Lin*

Main category: cs.CV

TL;DR: 该论文提出了一种新的空间-频谱互学习（S2ML）框架，用于RGB-D相机深度图像的补全，能够结合空间和频率信息，有效提升深度补全的准确性，并在主流数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于TOF或结构光的RGB-D相机采集的原始深度图像常出现无效像素，影响后续视觉任务。现有方法仅在图像域补全，未充分利用深度图的物理特性，特别是频率分布的变化。

Method: 提出S2ML框架，结合空间域与频域特征，单独处理幅度谱和相位谱，并设计专门的光谱融合模块。同时，将空间和频域特征在统一嵌入空间中建模，进行相互表示与细化，增强网络对物理信息的挖掘。

Result: 在NYU-Depth V2和SUN RGB-D两个主流深度数据集上，S2ML方法分别比最先进方法CFormer提升了0.828 dB和0.834 dB，展示了更有效的深度补全能力。

Conclusion: S2ML方法通过融合空间和频率特征，深挖物理先验，有效提升深度补全的精度，为RGB-D图像应用带来了新思路。

Abstract: The raw depth images captured by RGB-D cameras using Time-of-Flight (TOF) or
structured light often suffer from incomplete depth values due to weak
reflections, boundary shadows, and artifacts, which limit their applications in
downstream vision tasks. Existing methods address this problem through depth
completion in the image domain, but they overlook the physical characteristics
of raw depth images. It has been observed that the presence of invalid depth
areas alters the frequency distribution pattern. In this work, we propose a
Spatio-Spectral Mutual Learning framework (S2ML) to harmonize the advantages of
both spatial and frequency domains for depth completion. Specifically, we
consider the distinct properties of amplitude and phase spectra and devise a
dedicated spectral fusion module. Meanwhile, the local and global correlations
between spatial-domain and frequency-domain features are calculated in a
unified embedding space. The gradual mutual representation and refinement
encourage the network to fully explore complementary physical characteristics
and priors for more accurate depth completion. Extensive experiments
demonstrate the effectiveness of our proposed S2ML method, outperforming the
state-of-the-art method CFormer by 0.828 dB and 0.834 dB on the NYU-Depth V2
and SUN RGB-D datasets, respectively.

</details>


### [78] [StreamSTGS: Streaming Spatial and Temporal Gaussian Grids for Real-Time Free-Viewpoint Video](https://arxiv.org/abs/2511.06046)
*Zhihui Ke,Yuyang Liu,Xiaobo Zhou,Tie Qiu*

Main category: cs.CV

TL;DR: 本论文提出一种名为StreamSTGS的新型FVV表示方法，实现了高效实时的自由视点视频流传输，在保证画质的同时大幅降低了存储需求。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D Gaussian Splatting（3DGS）的自由视点视频（FVV）虽然在训练和渲染速度上有进步，但存储开销巨大，一帧就可达10MB，导致实时流传输不可行。因此需要兼顾压缩和质量的新方法。

Method: StreamSTGS用规范三维高斯、时序特征和形变场描述动态场景。为了高效压缩，将规范高斯属性编码为2D图片，将时序特征编码为视频。同时引入滑动窗口聚合相邻时序特征学习局部运动，再借助Transformer辅助模块学习全局运动，还支持自适应码率。

Result: 在多个自由视点视频基准测试中，StreamSTGS各项指标表现优异，相比主流方法提升PSNR约1dB，平均单帧大小压缩到仅170KB。

Conclusion: StreamSTGS在保证画质的情况下显著提升了FVV流传输的时效性和压缩率，并支持码率自适应，有较大的应用前景。

Abstract: Streaming free-viewpoint video~(FVV) in real-time still faces significant
challenges, particularly in training, rendering, and transmission efficiency.
Harnessing superior performance of 3D Gaussian Splatting~(3DGS), recent
3DGS-based FVV methods have achieved notable breakthroughs in both training and
rendering. However, the storage requirements of these methods can reach up to
$10$MB per frame, making stream FVV in real-time impossible. To address this
problem, we propose a novel FVV representation, dubbed StreamSTGS, designed for
real-time streaming. StreamSTGS represents a dynamic scene using canonical 3D
Gaussians, temporal features, and a deformation field. For high compression
efficiency, we encode canonical Gaussian attributes as 2D images and temporal
features as a video. This design not only enables real-time streaming, but also
inherently supports adaptive bitrate control based on network condition without
any extra training. Moreover, we propose a sliding window scheme to aggregate
adjacent temporal features to learn local motions, and then introduce a
transformer-guided auxiliary training module to learn global motions. On
diverse FVV benchmarks, StreamSTGS demonstrates competitive performance on all
metrics compared to state-of-the-art methods. Notably, StreamSTGS increases the
PSNR by an average of $1$dB while reducing the average frame size to just
$170$KB. The code is publicly available on https://github.com/kkkzh/StreamSTGS.

</details>


### [79] [Neodragon: Mobile Video Generation using Diffusion Transformer](https://arxiv.org/abs/2511.06055)
*Animesh Karnewar,Denis Korzhenkov,Ioannis Lelekas,Adil Karjauv,Noor Fathima,Hanwen Xiong,Vancheeswaran Vaidyanathan,Will Zeng,Rafael Esteves,Tushar Singhal,Fatih Porikli,Mohsen Ghafoorian,Amirhossein Habibian*

Main category: cs.CV

TL;DR: 本文提出了Neodragon，一个可在高通NPU上高效运行，直接生成2秒（49帧，24fps）、分辨率为640x1024视频的文本转视频系统。该系统在性能与效率上对现有模型进行了多项专门优化，实现移动端高保真高效生成。


<details>
  <summary>Details</summary>
Motivation: 当前流行的文本转视频模型在移动端部署时面临运算量大、内存需求高和推理速度慢等瓶颈，阻碍了AI视频创作的大众化与隐私保护需求。本文旨在突破移动设备的性能限制，实现低成本、高效、可靠的端侧文本生成视频。

Method: 提出四项关键技术：(1) 用新颖的蒸馏方法将大型T5xxl文本编码器缩减为0.2B大小的DistilT5，实现大幅度参数精简同时保持质量。(2) 提出非对称解码器蒸馏，从而可更换解码器而不破坏生成潜空间。(3) 通过评估MMDiT模块重要性进行剪枝，结合两阶段蒸馏恢复性能。(4) 针对去噪器采用分步蒸馏以减少推理计算量，并配合优化的首帧生成及超分网络，构建端到端高效管道。

Result: Neodragon完整模型仅4.945B参数，运行时峰值内存3.5GB，端到端生成2秒视频仅需6.7秒，VBench得分81.61，远超同领域移动端模型。

Conclusion: Neodragon极大提升了移动端文本生成视频的效率与质量，赋能私有、低成本视频内容创作，促进AI视频创作工具在大众中的普及。源码与模型将开源。

Abstract: We introduce Neodragon, a text-to-video system capable of generating 2s (49
frames @24 fps) videos at the 640x1024 resolution directly on a Qualcomm
Hexagon NPU in a record 6.7s (7 FPS). Differing from existing transformer-based
offline text-to-video generation models, Neodragon is the first to have been
specifically optimised for mobile hardware to achieve efficient and
high-fidelity video synthesis. We achieve this through four key technical
contributions: (1) Replacing the original large 4.762B T5xxl Text-Encoder with
a much smaller 0.2B DT5 (DistilT5) with minimal quality loss, enabled through a
novel Text-Encoder Distillation procedure. (2) Proposing an Asymmetric Decoder
Distillation approach allowing us to replace the native codec-latent-VAE
decoder with a more efficient one, without disturbing the generative
latent-space of the generation pipeline. (3) Pruning of MMDiT blocks within the
denoiser backbone based on their relative importance, with recovery of original
performance through a two-stage distillation process. (4) Reducing the NFE
(Neural Functional Evaluation) requirement of the denoiser by performing step
distillation using DMD adapted for pyramidal flow-matching, thereby
substantially accelerating video generation. When paired with an optimised
SSD1B first-frame image generator and QuickSRNet for 2x super-resolution, our
end-to-end Neodragon system becomes a highly parameter (4.945B full model),
memory (3.5GB peak RAM usage), and runtime (6.7s E2E latency) efficient
mobile-friendly model, while achieving a VBench total score of 81.61. By
enabling low-cost, private, and on-device text-to-video synthesis, Neodragon
democratizes AI-based video content creation, empowering creators to generate
high-quality videos without reliance on cloud services. Code and model will be
made publicly available at our website:
https://qualcomm-ai-research.github.io/neodragon

</details>


### [80] [LoopExpose: An Unsupervised Framework for Arbitrary-Length Exposure Correction](https://arxiv.org/abs/2511.06066)
*Ao Li,Chen Chen,Zhenyu Wang,Tao Huang,Fangfang Wu,Weisheng Dong*

Main category: cs.CV

TL;DR: 本文提出了一种基于伪标签的无监督曝光校正方法，被称为LoopExpose，通过嵌套循环优化策略联合优化曝光校正模型和伪监督信息，实现对任意长度曝光序列的校正。该方法在不同数据集上优于现有无监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有曝光校正方法大多依赖大规模有标注数据集，但实际中很难获得。为了解决标注数据稀缺的问题，作者希望设计一种不依赖人工标注的新方法。

Method: 作者提出了LoopExpose方法，采用嵌套循环优化策略：上层用由下层多曝光融合生成的伪标签训练校正模型，下层则对纠正过的图片进行反馈，优化伪标签，形成自增强学习闭环。同时引入亮度排序损失，利用输入序列中的相对亮度作为自监督约束。

Result: 在多个基准数据集上进行实验，LoopExpose在曝光校正和融合效果上都超过了现有最优的无监督方法。

Conclusion: LoopExpose能在无标注数据情况下，高效提升图像曝光质量，具有良好的泛化能力和实际应用价值。

Abstract: Exposure correction is essential for enhancing image quality under
challenging lighting conditions. While supervised learning has achieved
significant progress in this area, it relies heavily on large-scale labeled
datasets, which are difficult to obtain in practical scenarios. To address this
limitation, we propose a pseudo label-based unsupervised method called
LoopExpose for arbitrary-length exposure correction. A nested loop optimization
strategy is proposed to address the exposure correction problem, where the
correction model and pseudo-supervised information are jointly optimized in a
two-level framework. Specifically, the upper-level trains a correction model
using pseudo-labels generated through multi-exposure fusion at the lower level.
A feedback mechanism is introduced where corrected images are fed back into the
fusion process to refine the pseudo-labels, creating a self-reinforcing
learning loop. Considering the dominant role of luminance calibration in
exposure correction, a Luminance Ranking Loss is introduced to leverage the
relative luminance ordering across the input sequence as a self-supervised
constraint. Extensive experiments on different benchmark datasets demonstrate
that LoopExpose achieves superior exposure correction and fusion performance,
outperforming existing state-of-the-art unsupervised methods. Code is available
at https://github.com/FALALAS/LoopExpose.

</details>


### [81] [An Artificial Intelligence-based Assistant for the Visually Impaired](https://arxiv.org/abs/2511.06080)
*Luis Marquez-Carpintero,Francisco Gomez-Donoso,Zuria Bauer,Bessie Dominguez-Dager,Alvaro Belmonte-Baeza,Mónica Pina-Navarro,Francisco Morillas-Espejo,Felix Escalona,Miguel Cazorla*

Main category: cs.CV

TL;DR: 本文介绍了为视障人士开发的人工智能助手AIDEN，能够识别和描述物体、读取文本，并回答相关环境问题，提高其生活质量。


<details>
  <summary>Details</summary>
Motivation: 视障人士在识别物体、阅读文本和环境导航等方面面临诸多困难，现有如盲文、有声书和屏幕阅读器等辅助工具在某些场景下效果有限。本文旨在利用新一代AI技术，全面提升视障人士的自主性和日常信息获取能力。

Method: AIDEN采用了YOLO（You Only Look Once）架构用于视觉识别，并结合大型语言与视觉助手，实现对环境的综合感知和交互。同时，设计了便于用户获取文本和图像信息的多种交互方式。

Result: 通过用户反馈，AIDEN应用在提升视障用户的自主性和日常应用体验方面表现良好，用户对其实用性和信息获取能力普遍认可。

Conclusion: AIDEN成功拓展了视障人士对日常环境的认知和操作能力，为提升其生活质量做出了积极贡献。

Abstract: This paper describes an artificial intelligence-based assistant application,
AIDEN, developed during 2023 and 2024, aimed at improving the quality of life
for visually impaired individuals. Visually impaired individuals face
challenges in identifying objects, reading text, and navigating unfamiliar
environments, which can limit their independence and reduce their quality of
life. Although solutions such as Braille, audio books, and screen readers
exist, they may not be effective in all situations. This application leverages
state-of-the-art machine learning algorithms to identify and describe objects,
read text, and answer questions about the environment. Specifically, it uses
You Only Look Once architectures and a Large Language and Vision Assistant. The
system incorporates several methods to facilitate the user's interaction with
the system and access to textual and visual information in an appropriate
manner. AIDEN aims to enhance user autonomy and access to information,
contributing to an improved perception of daily usability, as supported by user
feedback.

</details>


### [82] [Hybrid CNN-ViT Framework for Motion-Blurred Scene Text Restoration](https://arxiv.org/abs/2511.06087)
*Umar Rashid,Muhammad Arslan Arshad,Ghulam Ahmad,Muhammad Zeeshan Anjum,Rizwan Khan,Muhammad Akmal*

Main category: cs.CV

TL;DR: 本文提出了一种结合卷积神经网络（CNN）与视觉Transformer（ViT）的混合深度学习框架，用于恢复运动模糊场景文字图像，既提升了去模糊效果，又保证了模型的轻量和高效。


<details>
  <summary>Details</summary>
Motivation: 运动模糊显著影响场景文字图像的可读性和下游计算机视觉任务的可靠性，尤其是在自动驾驶、文档数字化和视觉信息检索等应用中。传统的去模糊方法难以处理空间变化的模糊，并且无法有效建模恢复文本清晰度所需的长距离依赖关系。

Method: 提出了一种CNN与ViT结合的混合架构，CNN编码-解码器用于局部结构细节保留，Transformer模块通过自注意机制增强全局感知能力。训练数据来自TextOCR数据集，采用真实运动模糊核生成多大小和方向的模糊样本。损失函数综合了MAE、MSE、感知相似度以及结构相似性（SSIM）。

Result: 模型在PSNR（峰值信噪比）上取得32.20 dB、SSIM为0.934，并且模型仅有283万参数，平均推理时间为61毫秒，展现了优异的效果和高计算效率。

Conclusion: 所提出的CNN-ViT混合架构不仅在运动模糊场景文本恢复任务中表现突出，还兼具高效的推理速度和轻量化模型体积，具备实际应用价值。

Abstract: Motion blur in scene text images severely impairs readability and hinders the
reliability of computer vision tasks, including autonomous driving, document
digitization, and visual information retrieval. Conventional deblurring
approaches are often inadequate in handling spatially varying blur and
typically fall short in modeling the long-range dependencies necessary for
restoring textual clarity. To overcome these limitations, we introduce a hybrid
deep learning framework that combines convolutional neural networks (CNNs) with
vision transformers (ViTs), thereby leveraging both local feature extraction
and global contextual reasoning. The architecture employs a CNN-based
encoder-decoder to preserve structural details, while a transformer module
enhances global awareness through self-attention. Training is conducted on a
curated dataset derived from TextOCR, where sharp scene-text samples are paired
with synthetically blurred versions generated using realistic motion-blur
kernels of multiple sizes and orientations. Model optimization is guided by a
composite loss that incorporates mean absolute error (MAE), squared error
(MSE), perceptual similarity, and structural similarity (SSIM). Quantitative
eval- uations show that the proposed method attains 32.20 dB in PSNR and 0.934
in SSIM, while remaining lightweight with 2.83 million parameters and an
average inference time of 61 ms. These results highlight the effectiveness and
computational efficiency of the CNN-ViT hybrid design, establishing its
practicality for real-world motion-blurred scene-text restoration.

</details>


### [83] [DiLO: Disentangled Latent Optimization for Learning Shape and Deformation in Grouped Deforming 3D Objects](https://arxiv.org/abs/2511.06115)
*Mostofa Rafid Uddin,Jana Armouti,Umong Sain,Md Asib Rahman,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: 本文提出了一种无监督的3D对象形状与变形的解耦参数化方法，性能优异、实用性强。


<details>
  <summary>Details</summary>
Motivation: 3D对象在动画、医疗等领域广泛应用，准确地将物体的形状与变形进行解耦参数化是提升下游任务的关键。目前这类方法通常复杂且需要监督，而简洁、无监督的方法需求突出。

Method: 该方法基于潜变量优化，结合生成网络以及特定正则化方法，在无监督下共同优化形状与变形因子。第二阶段利用两个基于PointNet的顺序无关编码器，实现有效的自动推断。

Result: 在3D人体、动物、面部表情数据集上的实验证明，该简单方法在解耦传递、分类与可解释性等下游任务上，效果优于或不逊色于更复杂的现有方法。

Conclusion: 本文方法结构简单、无监督、有效，适用于多种3D对象与变形场景，对现有复杂方法具有很强的替代意义。

Abstract: In this work, we propose a disentangled latent optimization-based method for
parameterizing grouped deforming 3D objects into shape and deformation factors
in an unsupervised manner. Our approach involves the joint optimization of a
generator network along with the shape and deformation factors, supported by
specific regularization techniques. For efficient amortized inference of
disentangled shape and deformation codes, we train two order-invariant
PoinNet-based encoder networks in the second stage of our method. We
demonstrate several significant downstream applications of our method,
including unsupervised deformation transfer, deformation classification, and
explainability analysis. Extensive experiments conducted on 3D human, animal,
and facial expression datasets demonstrate that our simple approach is highly
effective in these downstream tasks, comparable or superior to existing methods
with much higher complexity.

</details>


### [84] [Latent Refinement via Flow Matching for Training-free Linear Inverse Problem Solving](https://arxiv.org/abs/2511.06138)
*Hossein Askari,Yadan Luo,Hongfu Sun,Fred Roosta*

Main category: cs.CV

TL;DR: 本文提出了一种新的反问题求解框架LFlow，通过在潜在空间中利用预训练流模型，高效提升重建质量并突破了现有流方法在高分辨率和后验引导方面的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 已有流模型在反问题求解中效率高于扩散模型，但主要应用在像素空间，导致计算资源消耗大且难以扩展到高分辨率。同时，过去的指导策略未考虑先验后验的协方差，影响了模型对生成轨迹的对齐和后验覆盖能力。为了克服这些局限，作者提出在潜在空间进行求解，并改进后验协方差的计算。

Method: 本文提出了LFlow框架，利用预训练的潜在流模型，在潜在空间中通过ODE采样与流匹配高效求解，从而绕过像素空间的高资源消耗。作者同时提出了基于最优向量场理论推导的后验协方差，有效增强流的引导能力。重要的是该方法无需训练，直接基于现有模型实现。

Result: 实验结果显示，LFlow在大多数线性反问题任务中，重建质量优于主流的潜在扩散求解器。该方法尤其在高分辨率和高效性方面表现突出。

Conclusion: LFlow弥补了现有流反问题求解方法在计算和后验引导上的不足，实现了更高效、更优质的重建，为流模型在高分辨率和复杂反问题中的应用打下了基础。

Abstract: Recent advances in inverse problem solving have increasingly adopted flow
priors over diffusion models due to their ability to construct straight
probability paths from noise to data, thereby enhancing efficiency in both
training and inference. However, current flow-based inverse solvers face two
primary limitations: (i) they operate directly in pixel space, which demands
heavy computational resources for training and restricts scalability to
high-resolution images, and (ii) they employ guidance strategies with
prior-agnostic posterior covariances, which can weaken alignment with the
generative trajectory and degrade posterior coverage. In this paper, we propose
LFlow (Latent Refinement via Flows), a training-free framework for solving
linear inverse problems via pretrained latent flow priors. LFlow leverages the
efficiency of flow matching to perform ODE sampling in latent space along an
optimal path. This latent formulation further allows us to introduce a
theoretically grounded posterior covariance, derived from the optimal vector
field, enabling effective flow guidance. Experimental results demonstrate that
our proposed method outperforms state-of-the-art latent diffusion solvers in
reconstruction quality across most tasks. The code will be publicly available
at https://github.com/hosseinaskari-cs/LFlow .

</details>


### [85] [Real-Time Bundle Adjustment for Ultra-High-Resolution UAV Imagery Using Adaptive Patch-Based Feature Tracking](https://arxiv.org/abs/2511.06152)
*Selim Ahmet Iz,Francesco Nex,Norman Kerle,Henry Meissner,Ralf Berger*

Main category: cs.CV

TL;DR: 本文提出了一种针对无人机高分辨率图像的实时光束法束平差（BA）新框架，实现了无需降采样即可在实地/实时高精度地图生成。该方法通过动态分块、利用导航与地表模型辅助特征匹配，显著提升精度与处理速度，适用于灾害应急等紧急地理信息需求场景。


<details>
  <summary>Details</summary>
Motivation: 在灾害响应等需要快速地理信息的应用中，实时高精度处理无人机大幅面影像十分关键。但现有BA方案要么降采样损失细节，要么处理过慢，难以满足实用。

Method: 本方法将每张全分辨率原始影像划分为可设定子块（如NxN网格），利用无人机GNSS/IMU与粗略地表模型（DSM）保持分块间空间一致性，增强特征提取与匹配鲁棒性。借助导航信息实时判断影像重叠关系，仅对子块重叠的局部影像滑动集群做BA优化，实现了实时高效且精度兼顾。

Result: 在50MP全幅影像MACS数据集测试，无需GPU，仅用CPU即可在2秒内完成全图像BA，仍保持高精度相机定向及多条航带高保真地图生成能力。

Conclusion: 该方法同时兼顾了高分辨率图像细节与实时处理要求，适应野外应急测绘场景，已能无缝集成进MACS系统，适合灾害响应、基础设施监测和海岸保护等大面积即时测绘应用。

Abstract: Real-time processing of UAV imagery is crucial for applications requiring
urgent geospatial information, such as disaster response, where rapid
decision-making and accurate spatial data are essential. However, processing
high-resolution imagery in real time presents significant challenges due to the
computational demands of feature extraction, matching, and bundle adjustment
(BA). Conventional BA methods either downsample images, sacrificing important
details, or require extensive processing time, making them unsuitable for
time-critical missions. To overcome these limitations, we propose a novel
real-time BA framework that operates directly on fullresolution UAV imagery
without downsampling. Our lightweight, onboard-compatible approach divides each
image into user-defined patches (e.g., NxN grids, default 150x150 pixels) and
dynamically tracks them across frames using UAV GNSS/IMU data and a coarse,
globally available digital surface model (DSM). This ensures spatial
consistency for robust feature extraction and matching between patches.
Overlapping relationships between images are determined in real time using UAV
navigation system, enabling the rapid selection of relevant neighbouring images
for localized BA. By limiting optimization to a sliding cluster of overlapping
images, including those from adjacent flight strips, the method achieves
real-time performance while preserving the accuracy of global BA. The proposed
algorithm is designed for seamless integration into the DLR Modular Aerial
Camera System (MACS), supporting largearea mapping in real time for disaster
response, infrastructure monitoring, and coastal protection. Validation on MACS
datasets with 50MP images demonstrates that the method maintains precise camera
orientations and high-fidelity mapping across multiple strips, running full
bundle adjustment in under 2 seconds without GPU acceleration.

</details>


### [86] [MambaOVSR: Multiscale Fusion with Global Motion Modeling for Chinese Opera Video Super-Resolution](https://arxiv.org/abs/2511.06172)
*Hua Chang,Xin Xu,Wei Liu,Wei Wang,Xin Yuan,Kui Jiang*

Main category: cs.CV

TL;DR: 该论文针对中国戏曲老视频的画质提升难题，提出了新的大规模数据集与多尺度融合超分网络，有效优化了此类内容的空间-时间超分辨率效果。


<details>
  <summary>Details</summary>
Motivation: 由于老旧的拍摄设备限制，中国戏曲珍贵影像普遍存在帧率低、分辨率低等问题，限制了经典艺术的数字化保存。同时，现有的视频超分模型因缺乏相关数据集和大幅度运动建模能力，难以还原戏曲表演中的复杂细节。

Method: 作者构建了大规模COVC中国戏曲视频片段数据集，并提出以Mamba为核心的多尺度融合网络（MambaOVSR），包括：全局融合模块（GFM）处理大幅运动，多尺度协同Mamba模块（MSMM）进行不同序列长度对齐，以及MambaVR模块解决特征伪影和位置信息丢失。

Result: 实验结果表明，MambaOVSR在COVC数据集上较主流STVSR方法平均提升了1.86 dB（PSNR），显著改善了重建画质。

Conclusion: 提出的COVC数据集和MambaOVSR模型为中国戏曲老视频的数字化修复提供了有效工具，显著提升了超分辨率效果，并推动相关方法与数据集的公开，促进后续研究。

Abstract: Chinese opera is celebrated for preserving classical art. However, early
filming equipment limitations have degraded videos of last-century performances
by renowned artists (e.g., low frame rates and resolution), hindering archival
efforts. Although space-time video super-resolution (STVSR) has advanced
significantly, applying it directly to opera videos remains challenging. The
scarcity of datasets impedes the recovery of high frequency details, and
existing STVSR methods lack global modeling capabilities, compromising visual
quality when handling opera's characteristic large motions. To address these
challenges, we pioneer a large scale Chinese Opera Video Clip (COVC) dataset
and propose the Mamba-based multiscale fusion network for space-time Opera
Video Super-Resolution (MambaOVSR). Specifically, MambaOVSR involves three
novel components: the Global Fusion Module (GFM) for motion modeling through a
multiscale alternating scanning mechanism, and the Multiscale Synergistic Mamba
Module (MSMM) for alignment across different sequence lengths. Additionally,
our MambaVR block resolves feature artifacts and positional information loss
during alignment. Experimental results on the COVC dataset show that MambaOVSR
significantly outperforms the SOTA STVSR method by an average of 1.86 dB in
terms of PSNR. Dataset and Code will be publicly released.

</details>


### [87] [NURBGen: High-Fidelity Text-to-CAD Generation through LLM-Driven NURBS Modeling](https://arxiv.org/abs/2511.06194)
*Muhammad Usama,Mohammad Sadil Khan,Didier Stricker,Muhammad Zeshan Afzal*

Main category: cs.CV

TL;DR: NURBGen提出了一个可以将自然语言直接转换为高质量三维CAD模型的系统，依托NURBS曲面参数生成，更精确且高度可编辑。


<details>
  <summary>Details</summary>
Motivation: 当前文本到CAD的系统普遍只能生成网格模型，或依赖稀缺的设计历史数据，难以生成真正可编辑、工业级高质量的CAD模型，因此亟需新的解决方案。

Method: 本研究通过微调大型语言模型（LLM），将自然语言文本转换为包含NURBS参数（控制点、节点矢量、阶数、权重）的JSON表示，进而用Python直接转为BRep格式。此外，引入了NURBS与解析体混合表示法，提高了对裁剪和退化区域的处理能力，还自动构建了部件级的CAD数据集partABC。

Result: NURBGen能在多种文本提示下生成高保真和高精度的三维CAD模型，几何形状和尺寸准确性优于以往方法，并得到了专家评估的认可。

Conclusion: NURBGen实现了从自然语言到工程可用的3D CAD模型的直接生成，改善了几何保真度和后续可编辑性，为自动化CAD设计带来了重大进展。代码和数据集将公开发布。

Abstract: Generating editable 3D CAD models from natural language remains challenging,
as existing text-to-CAD systems either produce meshes or rely on scarce
design-history data. We present NURBGen, the first framework to generate
high-fidelity 3D CAD models directly from text using Non-Uniform Rational
B-Splines (NURBS). To achieve this, we fine-tune a large language model (LLM)
to translate free-form texts into JSON representations containing NURBS surface
parameters (\textit{i.e}, control points, knot vectors, degrees, and rational
weights) which can be directly converted into BRep format using Python. We
further propose a hybrid representation that combines untrimmed NURBS with
analytic primitives to handle trimmed surfaces and degenerate regions more
robustly, while reducing token complexity. Additionally, we introduce partABC,
a curated subset of the ABC dataset consisting of individual CAD components,
annotated with detailed captions using an automated annotation pipeline.
NURBGen demonstrates strong performance on diverse prompts, surpassing prior
methods in geometric fidelity and dimensional accuracy, as confirmed by expert
evaluations. Code and dataset will be released publicly.

</details>


### [88] [Scene-Aware Urban Design: A Human-AI Recommendation Framework Using Co-Occurrence Embeddings and Vision-Language Models](https://arxiv.org/abs/2511.06201)
*Rodrigo Gallardo,Oz Fishman,Alexander Htet Kyaw*

Main category: cs.CV

TL;DR: 该论文提出了一种结合生成式AI和人类参与的计算机视觉框架，用于在公共空间进行微尺度设计干预，增强社区持续参与。


<details>
  <summary>Details</summary>
Motivation: 城市空间设计往往采取自上而下的规划方式，难以充分体现本地居民的需求和日常经验。作者希望借助AI技术推动更加细致和本地化的设计参与。

Method: 该方法利用Grounding DINO模型和ADE20K数据集对城市环境中的物体进行检测，计算共现嵌入，识别常见空间配置。系统为用户指定的锚定物体生成五个高概率互补物体建议。之后，利用视觉语言模型结合场景图像和选定物体对，进一步推荐第三个能完善更复杂城市策略的对象。整个流程强调人类参与，用户始终控制选择和微调。

Result: 系统实现了城市空间中物体的智能识别和基于空间语义关系的设计建议生成，通过人机协作方式为城市空间设计提供辅助工具。

Conclusion: 这种人机协作的设计框架能够更好地将城市空间设计与本地实际需求相结合，突破传统规划的局限，提升居民参与度，有望促进城市空间微观改造的创新性和实用性。

Abstract: This paper introduces a human-in-the-loop computer vision framework that uses
generative AI to propose micro-scale design interventions in public space and
support more continuous, local participation. Using Grounding DINO and a
curated subset of the ADE20K dataset as a proxy for the urban built
environment, the system detects urban objects and builds co-occurrence
embeddings that reveal common spatial configurations. From this analysis, the
user receives five statistically likely complements to a chosen anchor object.
A vision language model then reasons over the scene image and the selected pair
to suggest a third object that completes a more complex urban tactic. The
workflow keeps people in control of selection and refinement and aims to move
beyond top-down master planning by grounding choices in everyday patterns and
lived experience.

</details>


### [89] [MoRA: Missing Modality Low-Rank Adaptation for Visual Recognition](https://arxiv.org/abs/2511.06225)
*Shu Zhao,Nilesh Ahuja,Tan Yu,Tianyi Shen,Vijaykrishnan Narayanan*

Main category: cs.CV

TL;DR: 提出了MoRA方法，在多模态输入缺失情况下提升视觉识别表现，同时计算与参数开销大幅降低。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言预训练模型在训练和推理均假定多模态数据完整，但实际应用常因隐私、采集等原因存在模态缺失，现有解决方法难以捕获跨模态关系且计算开销大。

Method: 提出MoRA（参数高效微调方法），在文本与视觉编码器间引入模态通用参数，实现双向知识迁移，同时结合模态特定参数，兼顾模态间互动和模态内自适应能力。

Result: 在标准基准测试中，MoRA在缺失模态场景下平均性能提升5.24%，推理速度是现有SOTA方法的3.9倍，仅用0.11%的可训练参数即可达到效果。

Conclusion: MoRA能有效提升多模态视觉识别在模态缺失场景的表现，并大幅降低推理时间和参数开销，优于目前主流方法。

Abstract: Pre-trained vision language models have shown remarkable performance on
visual recognition tasks, but they typically assume the availability of
complete multimodal inputs during both training and inference. In real-world
scenarios, however, modalities may be missing due to privacy constraints,
collection difficulties, or resource limitations. While previous approaches
have addressed this challenge using prompt learning techniques, they fail to
capture the cross-modal relationships necessary for effective multimodal visual
recognition and suffer from inevitable computational overhead. In this paper,
we introduce MoRA, a parameter-efficient fine-tuning method that explicitly
models cross-modal interactions while maintaining modality-specific
adaptations. MoRA introduces modality-common parameters between text and vision
encoders, enabling bidirectional knowledge transfer. Additionally, combined
with the modality-specific parameters, MoRA allows the backbone model to
maintain inter-modality interaction and enable intra-modality flexibility.
Extensive experiments on standard benchmarks demonstrate that MoRA achieves an
average performance improvement in missing-modality scenarios by 5.24% and uses
only 25.90% of the inference time compared to the SOTA method while requiring
only 0.11% of trainable parameters compared to full fine-tuning.

</details>


### [90] [Temporal-Guided Visual Foundation Models for Event-Based Vision](https://arxiv.org/abs/2511.06238)
*Ruihao Xia,Junhong Cai,Luziwei Leng,Liuyi Wang,Chengju Liu,Ran Cheng,Yang Tang,Pan Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种将视觉基础模型（VFMs）与事件相机结合的新框架TGVFM，大幅提升了事件视觉任务的表现。


<details>
  <summary>Details</summary>
Motivation: 事件相机在复杂环境下具备独特优势，但事件数据的异步性导致处理难度高。现有方法依赖特殊结构或消耗大量资源训练，而如何高效利用基于图像预训练的视觉基础模型（VFM）仍待探索。

Method: 作者提出了TGVFM框架，将图像预训练VFMs与自研的时序上下文融合模块整合。此模块包含：（1）长范围时序注意力，捕捉全局时序关联；（2）双重时空注意力，实现多尺度帧相关性建模；（3）深度特征引导机制，用于语义—时序特征融合。通过在真实事件数据上重训练事件到视频模型并结合transformer结构，提升了时空特征的表达能力。

Result: 在语义分割、深度估计、目标检测三项任务中，TGVFM相较现有方法分别提升16%、21%、16%，实现了最新最好性能。

Conclusion: 该方法充分释放了基于图像预训练VFM在事件视觉任务中的潜力，通过引入时序建模和特征融合机制，极大提升了事件视觉的多任务表现。代码已开源，有望推动跨模态视觉研究。

Abstract: Event cameras offer unique advantages for vision tasks in challenging
environments, yet processing asynchronous event streams remains an open
challenge. While existing methods rely on specialized architectures or
resource-intensive training, the potential of leveraging modern Visual
Foundation Models (VFMs) pretrained on image data remains under-explored for
event-based vision. To address this, we propose Temporal-Guided VFM (TGVFM), a
novel framework that integrates VFMs with our temporal context fusion block
seamlessly to bridge this gap. Our temporal block introduces three key
components: (1) Long-Range Temporal Attention to model global temporal
dependencies, (2) Dual Spatiotemporal Attention for multi-scale frame
correlation, and (3) Deep Feature Guidance Mechanism to fuse semantic-temporal
features. By retraining event-to-video models on real-world data and leveraging
transformer-based VFMs, TGVFM preserves spatiotemporal dynamics while
harnessing pretrained representations. Experiments demonstrate SoTA performance
across semantic segmentation, depth estimation, and object detection, with
improvements of 16%, 21%, and 16% over existing methods, respectively. Overall,
this work unlocks the cross-modality potential of image-based VFMs for
event-based vision with temporal reasoning. Code is available at
https://github.com/XiaRho/TGVFM.

</details>


### [91] [Physics-Informed Image Restoration via Progressive PDE Integration](https://arxiv.org/abs/2511.06244)
*Shamika Likhite,Santiago López-Tapia,Aggelos K. Katsaggelos*

Main category: cs.CV

TL;DR: 本文提出了一种融合物理先验的运动去模糊新框架，在当前顶尖深度学习去模糊网络中引入PDE动力学，仅略微增加计算量即可显著提升图像恢复质量。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习去运动模糊方法难以捕捉运动模糊中蕴含的长距离空间关系，卷积网络受限于感受野且需极深的网络才能表达全局特征，亟需新方法将物理知识嵌入特征恢复过程。

Method: 作者提出一种渐进式训练框架，将物理驱动的偏微分方程（PDEs）动力学引入到顶尖恢复网络，通过引入平流-扩散方程精准建模运动模糊的方向流特性，实现全局空间建模，提升网络的特征演化能力。

Result: 所提方法在常用运动去模糊基准数据集上，在FFTformer、NAFNet、Restormer和Stripformer等四种架构中都显著提高了PSNR与SSIM，仅增加约1%的计算量，且感知质量全面优于原有模型。

Conclusion: 将数学物理中的PDE模型与深度学习结构融合，可系统提升复杂图像恢复任务的表现。物理驱动的全局层为计算机视觉中的神经网络架构设计提供了重要的新方向。

Abstract: Motion blur, caused by relative movement between camera and scene during
exposure, significantly degrades image quality and impairs downstream computer
vision tasks such as object detection, tracking, and recognition in dynamic
environments. While deep learning-based motion deblurring methods have achieved
remarkable progress, existing approaches face fundamental challenges in
capturing the long-range spatial dependencies inherent in motion blur patterns.
Traditional convolutional methods rely on limited receptive fields and require
extremely deep networks to model global spatial relationships. These
limitations motivate the need for alternative approaches that incorporate
physical priors to guide feature evolution during restoration. In this paper,
we propose a progressive training framework that integrates physics-informed
PDE dynamics into state-of-the-art restoration architectures. By leveraging
advection-diffusion equations to model feature evolution, our approach
naturally captures the directional flow characteristics of motion blur while
enabling principled global spatial modeling. Our PDE-enhanced deblurring models
achieve superior restoration quality with minimal overhead, adding only
approximately 1\% to inference GMACs while providing consistent improvements in
perceptual quality across multiple state-of-the-art architectures.
Comprehensive experiments on standard motion deblurring benchmarks demonstrate
that our physics-informed approach improves PSNR and SSIM significantly across
four diverse architectures, including FFTformer, NAFNet, Restormer, and
Stripformer. These results validate that incorporating mathematical physics
principles through PDE-based global layers can enhance deep learning-based
image restoration, establishing a promising direction for physics-informed
neural network design in computer vision applications.

</details>


### [92] [Gait Recognition via Collaborating Discriminative and Generative Diffusion Models](https://arxiv.org/abs/2511.06245)
*Haijun Xiong,Bin Feng,Bang Wang,Xinggang Wang,Wenyu Liu*

Main category: cs.CV

TL;DR: 本文提出了一种结合扩散模型与判别模型的步态识别新方法CoD$^2$，通过多级条件控制策略实现对高低层语义特征的联合建模，显著提升了步态识别性能。


<details>
  <summary>Details</summary>
Motivation: 尽管步态识别领域的判别模型取得了显著成果，生成模型潜力尚未被充分挖掘。本文旨在探索生成模型，尤其是扩散模型在步态识别中对鲁棒特征提取和判别性能提升的价值。

Method: 提出了CoD$^2$框架，将扩散模型的数据分布建模能力与判别模型的语义表示能力融合。具体方法是通过多层条件控制策略，其中高层由判别器提取身份相关的高层语义特征指导生成，低层保留外观和动作等详细视觉信息，生成的一致性步态序列还反过来促进判别模型学习更丰富的高层特征。

Result: 在SUSTech1K、CCPG、GREW和Gait3D四个数据集上进行了大量实验，CoD$^2$取得了新的SOTA效果，与现有判别方法结合时可带来持续性能提升。

Conclusion: CoD$^2$通过创新性地融合扩散与判别模型，不仅实现了对步态高层和低层语义的全面建模，还能无缝集成到现有方法中，有效推动步态识别技术发展。

Abstract: Gait recognition offers a non-intrusive biometric solution by identifying
individuals through their walking patterns. Although discriminative models have
achieved notable success in this domain, the full potential of generative
models remains largely underexplored. In this paper, we introduce
\textbf{CoD$^2$}, a novel framework that combines the data distribution
modeling capabilities of diffusion models with the semantic representation
learning strengths of discriminative models to extract robust gait features. We
propose a Multi-level Conditional Control strategy that incorporates both
high-level identity-aware semantic conditions and low-level visual details.
Specifically, the high-level condition, extracted by the discriminative
extractor, guides the generation of identity-consistent gait sequences, whereas
low-level visual details, such as appearance and motion, are preserved to
enhance consistency. Furthermore, the generated sequences facilitate the
discriminative extractor's learning, enabling it to capture more comprehensive
high-level semantic features. Extensive experiments on four datasets
(SUSTech1K, CCPG, GREW, and Gait3D) demonstrate that CoD$^2$ achieves
state-of-the-art performance and can be seamlessly integrated with existing
discriminative methods, yielding consistent improvements.

</details>


### [93] [AdaDrive: Self-Adaptive Slow-Fast System for Language-Grounded Autonomous Driving](https://arxiv.org/abs/2511.06253)
*Ruifei Zhang,Junlin Xie,Wei Zhang,Weikai Chen,Xiao Tan,Xiang Wan,Guanbin Li*

Main category: cs.CV

TL;DR: AdaDrive是一种自适应协作的slow-fast框架，能灵活地调用大语言模型（LLM）辅助自动驾驶决策，在不影响实时性的前提下提升准确率。


<details>
  <summary>Details</summary>
Motivation: 传统将LLM集成到自动驾驶系统的方法要么调用过于频繁导致计算开销过大，要么采用固定调度机制难以适应复杂多变的驾驶场景。

Method: 提出AdaDrive框架，包括：1）自适应激活机制，通过比较学习动态判断是否调用LLM，仅在复杂和关键情况下激活；2）自适应融合策略，根据场景复杂度和预测置信度调整LLM影响的程度，实现与传统模块的无缝协同。

Result: 大量实验证明AdaDrive在多语言自动驾驶基准测试上，在行驶准确性和计算效率方面均达到了最新的SOTA（State-of-the-Art）水平。

Conclusion: AdaDrive实现了高效、灵活、上下文感知的LLM与自动驾驶的协作，有效提升决策表现且兼顾了实时性。

Abstract: Effectively integrating Large Language Models (LLMs) into autonomous driving
requires a balance between leveraging high-level reasoning and maintaining
real-time efficiency. Existing approaches either activate LLMs too frequently,
causing excessive computational overhead, or use fixed schedules, failing to
adapt to dynamic driving conditions. To address these challenges, we propose
AdaDrive, an adaptively collaborative slow-fast framework that optimally
determines when and how LLMs contribute to decision-making. (1) When to
activate the LLM: AdaDrive employs a novel adaptive activation loss that
dynamically determines LLM invocation based on a comparative learning
mechanism, ensuring activation only in complex or critical scenarios. (2) How
to integrate LLM assistance: Instead of rigid binary activation, AdaDrive
introduces an adaptive fusion strategy that modulates a continuous, scaled LLM
influence based on scene complexity and prediction confidence, ensuring
seamless collaboration with conventional planners. Through these strategies,
AdaDrive provides a flexible, context-aware framework that maximizes decision
accuracy without compromising real-time performance. Extensive experiments on
language-grounded autonomous driving benchmarks demonstrate that AdaDrive
state-of-the-art performance in terms of both driving accuracy and
computational efficiency. Code is available at
https://github.com/ReaFly/AdaDrive.

</details>


### [94] [VLDrive: Vision-Augmented Lightweight MLLMs for Efficient Language-grounded Autonomous Driving](https://arxiv.org/abs/2511.06256)
*Ruifei Zhang,Wei Zhang,Xiao Tan,Sibei Yang,Xiang Wan,Xiaonan Luo,Guanbin Li*

Main category: cs.CV

TL;DR: VLDrive是一种轻量级多模态大语言模型（MLLM）架构，结合创新视觉优化与跨模态机制，在显著减少参数量的同时提升自动驾驶表现，取得了最新最优结果。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）赋能下的自动驾驶系统在认知与推理表现卓越，但受到视觉表达能力不足和超大参数量的限制，导致驾驶中频繁出现碰撞、阻碍并难以实际部署。该文为了解决视觉表征不足与模型庞大这两大核心问题，提出新方法。

Method: VLDrive采用轻量级MLLM架构，整合：1）循环一致性动态视觉裁剪（cycle-consistent dynamic visual pruning）和记忆增强特征聚合（memory-enhanced feature aggregation）等创新策略，精简视觉Token，2）提出距离解耦的指令注意力机制（distance-decoupled instruction attention），提升长距离视觉Token的视觉-语言特征联合学习。

Result: 在CARLA自动驾驶仿真环境下，VLDrive在小距离、中距离和长距离闭环评测中，分别提升驾驶得分15.4%、16.8%、7.6%；参数量从7B大幅减少至1.3B，降幅达81%，超越此前同类方法。

Conclusion: VLDrive在极大压缩参数规模的同时，实现了更优的自动驾驶性能，验证了其在视觉优化和视觉-语言融合方面的有效性，推动了语言指导的自动驾驶方法向高效实用方向发展。

Abstract: Recent advancements in language-grounded autonomous driving have been
significantly promoted by the sophisticated cognition and reasoning
capabilities of large language models (LLMs). However, current LLM-based
approaches encounter critical challenges: (1) Failure analysis reveals that
frequent collisions and obstructions, stemming from limitations in visual
representations, remain primary obstacles to robust driving performance. (2)
The substantial parameters of LLMs pose considerable deployment hurdles. To
address these limitations, we introduce VLDrive, a novel approach featuring a
lightweight MLLM architecture with enhanced vision components. VLDrive achieves
compact visual tokens through innovative strategies, including cycle-consistent
dynamic visual pruning and memory-enhanced feature aggregation. Furthermore, we
propose a distance-decoupled instruction attention mechanism to improve joint
visual-linguistic feature learning, particularly for long-range visual tokens.
Extensive experiments conducted in the CARLA simulator demonstrate VLDrive`s
effectiveness. Notably, VLDrive achieves state-of-the-art driving performance
while reducing parameters by 81% (from 7B to 1.3B), yielding substantial
driving score improvements of 15.4%, 16.8%, and 7.6% at tiny, short, and long
distances, respectively, in closed-loop evaluations. Code is available at
https://github.com/ReaFly/VLDrive.

</details>


### [95] [Robust Nearest Neighbour Retrieval Using Targeted Manifold Manipulation](https://arxiv.org/abs/2511.06261)
*B. Ghosh,H. Harikumar,S. Rana*

Main category: cs.CV

TL;DR: 本文提出了一种新的最近邻检索方法TMM-NN（Targeted Manifold Manipulation-Nearest Neighbour），通过在特征流形上评估样本被‘推动’进入目标区域的容易程度来定义邻域。通过向查询图像添加特定触发补丁，并用弱‘后门’方式调整网络，使得与查询图像相似的样本更容易被分类到指定类别，从而提升了语义相关性检索效果。实验和鲁棒性分析显示，该方法在噪声和多任务场景下优于传统度量。


<details>
  <summary>Details</summary>
Motivation: 现有的最近邻检索依赖手动调节特征层和距离度量，难以充分捕捉样本之间的语义相关性，且检索效果受人工选择影响较大。作者希望通过新的邻域定义方法，提升检索的语义相关性和鲁棒性。

Method: 提出TMM-NN方法：为每个查询，生成专用的触发补丁，将其加到查询图像上。网络经过‘弱后门’训练后，带有该补丁的输入被引向一个虚拟类别。与查询相似的候选样本仅需较小扰动即可被归为该类别，非相似样本影响较小。根据分类为虚拟类别的概率对邻居排序，实现检索。

Result: TMM-NN方法在多项基准检索任务和鲁棒性（如抗噪声）分析中表现优异，检索排名优于传统基于距离的最近邻方法。

Conclusion: TMM-NN通过将邻域定义为‘响应于目标扰动’的样本集合，提升了最近邻检索的语义相关性和鲁棒性，对可解释AI和分类检索有新启示。

Abstract: Nearest-neighbour retrieval is central to classification and explainable-AI
pipelines, but current practice relies on hand-tuning feature layers and
distance metrics. We propose Targeted Manifold Manipulation-Nearest Neighbour
(TMM-NN), which reconceptualises retrieval by assessing how readily each sample
can be nudged into a designated region of the feature manifold; neighbourhoods
are defined by a sample's responsiveness to a targeted perturbation rather than
absolute geometric distance. TMM-NN implements this through a lightweight,
query-specific trigger patch. The patch is added to the query image, and the
network is weakly ``backdoored'' so that any input with the patch is steered
toward a dummy class. Images similar to the query need only a slight shift and
are classified as the dummy class with high probability, while dissimilar ones
are less affected. By ranking candidates by this confidence, TMM-NN retrieves
the most semantically related neighbours. Robustness analysis and benchmark
experiments confirm this trigger-based ranking outperforms traditional metrics
under noise and across diverse tasks.

</details>


### [96] [A Mixture-of-Experts Framework with Log-Logistic Components for Survival Analysis on Histopathology Images](https://arxiv.org/abs/2511.06266)
*Ardhendu Sekhar,Vasu Soni,Keshav Aske,Shivam Madnoorkar,Pranav Jeevan,Amit Sethi*

Main category: cs.CV

TL;DR: 本文提出了一种模块化框架，用于基于全切片病理图像（WSI）预测癌症特异性生存期，整体性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于WSI的癌症生存预测方法无法充分挖掘组织区域信息、表型异质性及复杂生存分布建模，限制了预测性能提升。

Method: 框架包含四个主要模块：（1）通过分位数阈值的Quantile Gated Patch Selection筛选有预后意义的组织区域；（2）基于K近邻图的Graph Guided Clustering捕获空间及形态表型异质性；（3）Hierarchical Context Attention学习簇内及簇间交互关系；（4）专家引导的Log logistics混合模型用于估算复杂生存分布。

Result: 在TCGA LUAD、KIRC、BRCA三个数据集上，模型的协调指数分别达到0.644、0.751和0.752，优于现有同类方法。

Conclusion: 该方法综合利用WSI图像多级信息和专家先验，提升了癌症生存预测的准确率与泛化能力。

Abstract: We propose a modular framework for predicting cancer specific survival from
whole slide pathology images (WSIs). The method integrates four components: (i)
Quantile Gated Patch Selection via quantile based thresholding to isolate
prognostically informative tissue regions; (ii) Graph Guided Clustering using a
k nearest neighbor graph to capture phenotype level heterogeneity through
spatial and morphological coherence; (iii) Hierarchical Context Attention to
learn intra and inter cluster interactions; and (iv) an Expert Driven Mixture
of Log logistics framework to estimate complex survival distributions using Log
logistics distributions. The model attains a concordance index of 0.644 on TCGA
LUAD, 0.751 on TCGA KIRC, and 0.752 on TCGA BRCA respectively, outperforming
existing state of the art approaches.

</details>


### [97] [LLM-Driven Completeness and Consistency Evaluation for Cultural Heritage Data Augmentation in Cross-Modal Retrieval](https://arxiv.org/abs/2511.06268)
*Jian Zhang,Junyi Guo,Junyi Yuan,Huanda Lu,Yanlin Zhou,Fangyu Wu,Qiufeng Wang,Dongming Lu*

Main category: cs.CV

TL;DR: 本文提出了一种名为C^3的数据增强框架，通过提升LLM生成描述的完整性和一致性，提升了跨模态检索在文化遗产等多数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的文化遗产跨模态检索受限于文本描述的残缺或不一致，这主要源于历史数据丢失和专家标注成本高昂。大语言模型能够生成文本，但常包含幻觉或丢失视觉细节，因此亟需提升文本描述的质量。

Method: 提出C^3框架，其包含：1）完整性评价模块，结合视觉线索和语言模型输出评估语义覆盖范围；2）为缓解事实不一致，设计了基于马尔可夫决策过程的链式思考推理，将一致性评估转化为自适应查询控制的决策过程。

Result: 在文化遗产数据集CulTi和TimeTravel，以及通用数据集MSCOCO和Flickr30K上的实验表明，C^3在微调和零样本设置下都取得了当前最优性能。

Conclusion: C^3通过提升大语言模型生成文本的完整性与一致性，显著增强了跨模态检索的效果，在多种领域和场景下均表现优越。

Abstract: Cross-modal retrieval is essential for interpreting cultural heritage data,
but its effectiveness is often limited by incomplete or inconsistent textual
descriptions, caused by historical data loss and the high cost of expert
annotation. While large language models (LLMs) offer a promising solution by
enriching textual descriptions, their outputs frequently suffer from
hallucinations or miss visually grounded details. To address these challenges,
we propose $C^3$, a data augmentation framework that enhances cross-modal
retrieval performance by improving the completeness and consistency of
LLM-generated descriptions. $C^3$ introduces a completeness evaluation module
to assess semantic coverage using both visual cues and language-model outputs.
Furthermore, to mitigate factual inconsistencies, we formulate a Markov
Decision Process to supervise Chain-of-Thought reasoning, guiding consistency
evaluation through adaptive query control. Experiments on the cultural heritage
datasets CulTi and TimeTravel, as well as on general benchmarks MSCOCO and
Flickr30K, demonstrate that $C^3$ achieves state-of-the-art performance in both
fine-tuned and zero-shot settings.

</details>


### [98] [RelightMaster: Precise Video Relighting with Multi-plane Light Images](https://arxiv.org/abs/2511.06271)
*Weikang Bian,Xiaoyu Shi,Zhaoyang Huang,Jianhong Bai,Qinghe Wang,Xintao Wang,Pengfei Wan,Kun Gai,Hongsheng Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频重光照（relighting）方法RelightMaster，可以实现高质量、精确且可控的视频重光照，克服了现有T2V模型无法细致控制光照的问题。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型虽然在视频生成和编辑上表现优异，但仍无法精确控制光照效果，主要由于文本对光照细节描述能力有限，且现有训练数据中光照控制的数据极为稀缺。然而，视频的光照调整对于塑造场景氛围和观众关注点至关重要，因此有必要探索可控视频重光照方法。

Method: 1）基于虚幻引擎制作了RelightVideo，这是首个拥有同一动态场景、不同精细光照条件的数据集；2）借鉴多平面图像（MPI）思想，提出多平面光照图像（MPLI）作为视觉提示，能建模多源3D光照的位置信息、强度和颜色等；3）设计light image adapter模块，可将MPLI信息通过编码压缩后注入到预训练的视频扩散Transformer（Video DiT）中，实现不忘记原模型的前提下引入光照可控性。

Result: 实验表明，RelightMaster能够合成物理合理的光照和阴影，并能很好地保持原有场景内容，显著优于现有T2V方法。

Conclusion: RelightMaster为视频生成和编辑领域带来了精细的、可控的视频重光照能力，开创了光照精准操控新方向，同时提供了关键数据集和模型工具，以促进相关研究发展。

Abstract: Recent advances in diffusion models enable high-quality video generation and
editing, but precise relighting with consistent video contents, which is
critical for shaping scene atmosphere and viewer attention, remains unexplored.
Mainstream text-to-video (T2V) models lack fine-grained lighting control due to
text's inherent limitation in describing lighting details and insufficient
pre-training on lighting-related prompts. Additionally, constructing
high-quality relighting training data is challenging, as real-world
controllable lighting data is scarce. To address these issues, we propose
RelightMaster, a novel framework for accurate and controllable video
relighting. First, we build RelightVideo, the first dataset with identical
dynamic content under varying precise lighting conditions based on the Unreal
Engine. Then, we introduce Multi-plane Light Image (MPLI), a novel visual
prompt inspired by Multi-Plane Image (MPI). MPLI models lighting via K
depth-aligned planes, representing 3D light source positions, intensities, and
colors while supporting multi-source scenarios and generalizing to unseen light
setups. Third, we design a Light Image Adapter that seamlessly injects MPLI
into pre-trained Video Diffusion Transformers (DiT): it compresses MPLI via a
pre-trained Video VAE and injects latent light features into DiT blocks,
leveraging the base model's generative prior without catastrophic forgetting.
Experiments show that RelightMaster generates physically plausible lighting and
shadows and preserves original scene content. Demos are available at
https://wkbian.github.io/Projects/RelightMaster/.

</details>


### [99] [LaneDiffusion: Improving Centerline Graph Learning via Prior Injected BEV Feature Generation](https://arxiv.org/abs/2511.06272)
*Zijie Wang,Weiming Zhang,Wei Zhang,Xiao Tan,Hongxing Liu,Yaowei Wang,Guanbin Li*

Main category: cs.CV

TL;DR: 本论文提出了一种新的生成式方法（LaneDiffusion），利用扩散模型在BEV特征层面生成车道中心线先验，实现了对中心线图的高精度预测，并在相关数据集上超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统确定性方法虽然在自动驾驶的路径规划中用于学习中心线图，但往往缺乏空间推理能力，对被遮挡或不可见的中心线处理不足。生成式方法有望改善这些问题，但在该领域尚未被充分探索。

Method: LaneDiffusion引入了扩散模型，在BEV特征级别生成车道中心线先验，而不是直接向量化中心线。方法包括两个关键模块：中心线先验注入模块（LPIM）和中心线先验扩散模块（LPDM），分别用于构建扩散目标和管理扩散过程。最终从注入先验的BEV特征中解码得到向量化中心线和拓扑。

Result: 在nuScenes与Argoverse2数据集上，LaneDiffusion在细粒度点级指标（GEO F1、TOPO F1、JTOPO F1、APLS、SDA）和段级指标（IoU、mAP_cf、DET_l、TOP_ll）上均大幅度优于现有方法，提升幅度达1.8%至6.8%。

Conclusion: LaneDiffusion实现了车道中心线图学习的最新性能，验证了生成式模型在该任务中的有效性，并为相关领域探索提供了新思路。

Abstract: Centerline graphs, crucial for path planning in autonomous driving, are
traditionally learned using deterministic methods. However, these methods often
lack spatial reasoning and struggle with occluded or invisible centerlines.
Generative approaches, despite their potential, remain underexplored in this
domain. We introduce LaneDiffusion, a novel generative paradigm for centerline
graph learning. LaneDiffusion innovatively employs diffusion models to generate
lane centerline priors at the Bird's Eye View (BEV) feature level, instead of
directly predicting vectorized centerlines. Our method integrates a Lane Prior
Injection Module (LPIM) and a Lane Prior Diffusion Module (LPDM) to effectively
construct diffusion targets and manage the diffusion process. Furthermore,
vectorized centerlines and topologies are then decoded from these
prior-injected BEV features. Extensive evaluations on the nuScenes and
Argoverse2 datasets demonstrate that LaneDiffusion significantly outperforms
existing methods, achieving improvements of 4.2%, 4.6%, 4.7%, 6.4% and 1.8% on
fine-grained point-level metrics (GEO F1, TOPO F1, JTOPO F1, APLS and SDA) and
2.3%, 6.4%, 6.8% and 2.1% on segment-level metrics (IoU, mAP_cf, DET_l and
TOP_ll). These results establish state-of-the-art performance in centerline
graph learning, offering new insights into generative models for this task.

</details>


### [100] [VideoSSR: Video Self-Supervised Reinforcement Learning](https://arxiv.org/abs/2511.06281)
*Zefeng He,Xiaoye Qu,Yafu Li,Siyuan Huang,Daizong Liu,Yu Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频自监督强化学习框架VideoSSR，用于提升多模态大语言模型（MLLMs）的视频理解能力，通过设计新颖的自监督任务，自行生成高质量可验证训练数据，在多个基准上取得了明显提升。


<details>
  <summary>Details</summary>
Motivation: 现有视频数据集复杂度已不再满足MLLMs快速提升的需求，而高质量新数据的人工标注成本极高。因此作者关心能否充分利用视频本身的内在信息，自发生成优质的训练数据。

Method: 提出了三种自监督预训练任务：异常定位（Anomaly Grounding）、目标计数（Object Counting）和时间拼图（Temporal Jigsaw）。据此构建新的综合测试集VIUBench，用来验证难度，并基于这些预训练任务开发了VideoSSR-30K数据集和VideoSSR自监督强化学习框架。

Result: 实验覆盖了17个基准数据，涉及四大视频领域。在所有指标上，VideoSSR均提升了MLLMs的表现，平均提升超过5%。特别是在复杂的理解和推理类任务上表现突出。

Conclusion: VideoSSR为发展更先进的视频理解型多模态大模型奠定了坚实基础，展现出强有力的应用前景。其预训练任务及公开数据集为社区提供了新的研究工具。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has substantially
advanced the video understanding capabilities of Multimodal Large Language
Models (MLLMs). However, the rapid progress of MLLMs is outpacing the
complexity of existing video datasets, while the manual annotation of new,
high-quality data remains prohibitively expensive. This work investigates a
pivotal question: Can the rich, intrinsic information within videos be
harnessed to self-generate high-quality, verifiable training data? To
investigate this, we introduce three self-supervised pretext tasks: Anomaly
Grounding, Object Counting, and Temporal Jigsaw. We construct the Video
Intrinsic Understanding Benchmark (VIUBench) to validate their difficulty,
revealing that current state-of-the-art MLLMs struggle significantly on these
tasks. Building upon these pretext tasks, we develop the VideoSSR-30K dataset
and propose VideoSSR, a novel video self-supervised reinforcement learning
framework for RLVR. Extensive experiments across 17 benchmarks, spanning four
major video domains (General Video QA, Long Video QA, Temporal Grounding, and
Complex Reasoning), demonstrate that VideoSSR consistently enhances model
performance, yielding an average improvement of over 5\%. These results
establish VideoSSR as a potent foundational framework for developing more
advanced video understanding in MLLMs. The code is available at
https://github.com/lcqysl/VideoSSR.

</details>


### [101] [From ACR O-RADS 2022 to Explainable Deep Learning: Comparative Performance of Expert Radiologists, Convolutional Neural Networks, Vision Transformers, and Fusion Models in Ovarian Masses](https://arxiv.org/abs/2511.06282)
*Ali Abbasian Ardakani,Afshin Mohammadi,Alisa Mohebbi,Anushya Vijayananthan,Sook Sam Leong,Lim Yi Ting,Mohd Kamil Bin Mohamad Fabell,U Rajendra Acharya,Sepideh Hatamikia*

Main category: cs.CV

TL;DR: 本研究比较了放射科医师采用O-RADS v2022系统评估附件肿块与多种深度学习（CNN和ViT）模型的诊断表现，并评估了人机结合诊断框架的优势。


<details>
  <summary>Details</summary>
Motivation: O-RADS v2022系统虽提升了风险分层，但人工解读易受主观影响和保守阈值限制，深度学习模型在图像诊断中已显示优越潜力，亟需对比两者并探索其融合效益。

Method: 回顾性纳入227名患者512个附件肿块图像，训练并验证16种主流CNN和ViT深度学习模型，评估各模型及其与O-RADS积分融合的人机协同框架诊断性能。

Result: 放射科医师O-RADS诊断AUC为0.683，准确率68%。CNN模型AUC为0.620-0.908，准确率59.2%-86.4%；ViT模型最佳AUC达0.941，准确率87.4%。人机融合框架可显著提升CNN表现，对ViT模型无显著提升。

Conclusion: 深度学习模型显著优于单独人工O-RADS评估，将专家评分与AI结果结合可获得最高诊断准确性与判别力，人机结合模式有望标准化盆腔超声解读、降低假阳性并提升高危病灶检出。

Abstract: Background: The 2022 update of the Ovarian-Adnexal Reporting and Data System
(O-RADS) ultrasound classification refines risk stratification for adnexal
lesions, yet human interpretation remains subject to variability and
conservative thresholds. Concurrently, deep learning (DL) models have
demonstrated promise in image-based ovarian lesion characterization. This study
evaluates radiologist performance applying O-RADS v2022, compares it to leading
convolutional neural network (CNN) and Vision Transformer (ViT) models, and
investigates the diagnostic gains achieved by hybrid human-AI frameworks.
Methods: In this single-center, retrospective cohort study, a total of 512
adnexal mass images from 227 patients (110 with at least one malignant cyst)
were included. Sixteen DL models, including DenseNets, EfficientNets, ResNets,
VGGs, Xception, and ViTs, were trained and validated. A hybrid model
integrating radiologist O-RADS scores with DL-predicted probabilities was also
built for each scheme. Results: Radiologist-only O-RADS assessment achieved an
AUC of 0.683 and an overall accuracy of 68.0%. CNN models yielded AUCs of 0.620
to 0.908 and accuracies of 59.2% to 86.4%, while ViT16-384 reached the best
performance, with an AUC of 0.941 and an accuracy of 87.4%. Hybrid human-AI
frameworks further significantly enhanced the performance of CNN models;
however, the improvement for ViT models was not statistically significant
(P-value >0.05). Conclusions: DL models markedly outperform radiologist-only
O-RADS v2022 assessment, and the integration of expert scores with AI yields
the highest diagnostic accuracy and discrimination. Hybrid human-AI paradigms
hold substantial potential to standardize pelvic ultrasound interpretation,
reduce false positives, and improve detection of high-risk lesions.

</details>


### [102] [TinyChemVL: Advancing Chemical Vision-Language Models via Efficient Visual Token Reduction and Complex Reaction Tasks](https://arxiv.org/abs/2511.06283)
*Xuanle Zhao,Shuxin Zeng,Yinyuan Cai,Xiang Cheng,Duzhen Zhang,Xiuyi Chen,Bo Xu*

Main category: cs.CV

TL;DR: 本文提出了一种高效强大的化学视觉语言模型TinyChemVL，通过视觉token压缩和反应层级任务提升模型效率和推理能力，并推出了化学反应水平基准ChemRxn-V，实验结果显示在保持小体量的同时模型性能超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在化学领域应用受限，主要聚焦文本，忽视了分子结构等关键信息，并在处理化学任务时存在计算低效及任务狭窄两大问题，难以支持更深入的化学推理。因此亟需为化学领域设计高效且适用的VLM。

Method: 提出TinyChemVL模型，采用视觉token压缩策略减少计算资源消耗，并引入反应层级任务提升推理能力。同时，建立了ChemRxn-V基准测试集，专注于视觉感知化学反应识别与预测任务。

Result: TinyChemVL参数量仅为4B，在分子及化学反应任务上均优于现有模型且推理、训练速度明显加快，仅用1/16视觉token就超越了ChemVLM。

Conclusion: 通过模型结构优化与任务设计协同，TinyChemVL兼具高效性与高性能，有望推动化学领域视觉语言模型的实用化和智能化进展。

Abstract: While Vision Language Models (VLMs) have demonstrated remarkable capabilities
in general visual understanding, their application in the chemical domain has
been limited, with previous works predominantly focusing on text and thus
overlooking critical visual information, such as molecular structures. Current
approaches that directly adopt standard VLMs for chemical tasks suffer from two
primary issues: (i) computational inefficiency of processing entire chemical
images with non-informative backgrounds. (ii) a narrow scope on molecular-level
tasks that restricts progress in chemical reasoning. In this work, we propose
\textbf{TinyChemVL}, an efficient and powerful chemical VLM that leverages
visual token reduction and reaction-level tasks to improve model efficiency and
reasoning capacity. Also, we propose \textbf{ChemRxn-V}, a reaction-level
benchmark for assessing vision-based reaction recognition and prediction tasks.
Directly predicting reaction products from molecular images poses a non-trivial
challenge, as it requires models to integrate both recognition and reasoning
capacities. Our results demonstrate that with only 4B parameters, TinyChemVL
achieves superior performance on both molecular and reaction tasks while
demonstrating faster inference and training speeds compared to existing models.
Notably, TinyChemVL outperforms ChemVLM while utilizing only 1/16th of the
visual tokens. This work builds efficient yet powerful VLMs for chemical
domains by co-designing model architecture and task complexity.

</details>


### [103] [Enhancing Multimodal Misinformation Detection by Replaying the Whole Story from Image Modality Perspective](https://arxiv.org/abs/2511.06284)
*Bing Wang,Ximing Li,Yanjun Wang,Changchun Li,Lin Yuanbo Wu,Buyu Wang,Shengsheng Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种新的多模态虚假信息检测方法（RETSIMD），利用文本分段生成增强图像，并通过辅助目标和图神经网络提升检测效果。实验结果显示效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有多模态虚假信息检测中，图片信息贡献通常较小，文本信息往往更完整地描述事件。本研究希望通过优化多模态融合方式，提升检测准确性。

Method: 提出将文本拆分为多个片段，每个片段对应生成一张增强图像，然后通过辅助目标函数（文本-图像、图像-标签的互信息）进行辅助训练，并在额外数据集上对生成模型做后训练。进一步，构建三种启发式图像关系，利用图神经网络融合特征。

Result: 大量实验证明，RETSIMD方法在多模态虚假信息检测任务中，能够有效提升检测性能。

Conclusion: 文本主导的多模态增强，以及多辅助目标和图结构融合能显著提升虚假信息检测的表现。

Abstract: Multimodal Misinformation Detection (MMD) refers to the task of detecting
social media posts involving misinformation, where the post often contains text
and image modalities. However, by observing the MMD posts, we hold that the
text modality may be much more informative than the image modality because the
text generally describes the whole event/story of the current post but the
image often presents partial scenes only. Our preliminary empirical results
indicate that the image modality exactly contributes less to MMD. Upon this
idea, we propose a new MMD method named RETSIMD. Specifically, we suppose that
each text can be divided into several segments, and each text segment describes
a partial scene that can be presented by an image. Accordingly, we split the
text into a sequence of segments, and feed these segments into a pre-trained
text-to-image generator to augment a sequence of images. We further incorporate
two auxiliary objectives concerning text-image and image-label mutual
information, and further post-train the generator over an auxiliary
text-to-image generation benchmark dataset. Additionally, we propose a graph
structure by defining three heuristic relationships between images, and use a
graph neural network to generate the fused features. Extensive empirical
results validate the effectiveness of RETSIMD.

</details>


### [104] [Learning-Based Vision Systems for Semi-Autonomous Forklift Operation in Industrial Warehouse Environments](https://arxiv.org/abs/2511.06295)
*Vamshika Sutar,Mahek Maheshwari,Archak Mittal*

Main category: cs.CV

TL;DR: 本论文提出了一种基于单目摄像头的托盘及托盘孔检测与映射方法，利用了经过优化的YOLOv8和YOLOv11模型，实现了高精度和高稳定性的检测，助力仓库自动化。


<details>
  <summary>Details</summary>
Motivation: 仓库搬运自动化日益依赖低成本、稳健的感知系统。然而，现有方法在精度、成本和可部署性方面存在不足，尤其是在托盘识别与叉车对接环节。本研究旨在通过视觉感知提升仓库自动化水平，降低运维门槛。

Method: 采用单目摄像头，结合YOLOv8和YOLOv11神经网络，通过Optuna进行超参数优化，并加入空间后处理提升检测稳定性与精度。提出创新托盘孔映射模块，将检测结果转化为空间位置信息，便于叉车作业。

Result: 在含真实仓库图像的自建数据集上实验，YOLOv8实现了高检测准确率，优化后的YOLOv11在精度和收敛性上更优。整体系统展现出良好的效果和可移植性。

Conclusion: 本研究验证了低成本视觉感知模块在仓库自动化中的可行性，为智能物流和叉车自动化作业提供了可扩展的新方案，具备经济性、安全性和部署灵活性。

Abstract: The automation of material handling in warehouses increasingly relies on
robust, low cost perception systems for forklifts and Automated Guided Vehicles
(AGVs). This work presents a vision based framework for pallet and pallet hole
detection and mapping using a single standard camera. We utilized YOLOv8 and
YOLOv11 architectures, enhanced through Optuna driven hyperparameter
optimization and spatial post processing. An innovative pallet hole mapping
module converts the detections into actionable spatial representations,
enabling accurate pallet and pallet hole association for forklift operation.
Experiments on a custom dataset augmented with real warehouse imagery show that
YOLOv8 achieves high pallet and pallet hole detection accuracy, while YOLOv11,
particularly under optimized configurations, offers superior precision and
stable convergence. The results demonstrate the feasibility of a cost
effective, retrofittable visual perception module for forklifts. This study
proposes a scalable approach to advancing warehouse automation, promoting
safer, economical, and intelligent logistics operations.

</details>


### [105] [SFFR: Spatial-Frequency Feature Reconstruction for Multispectral Aerial Object Detection](https://arxiv.org/abs/2511.06298)
*Xin Zuo,Yuchen Qu,Haibo Zhan,Jifeng Shen,Wankou Yang*

Main category: cs.CV

TL;DR: 本文提出了一种创新的多光谱目标检测方法（SFFR），融合空间与频率域特征，显著提升了无人机多光谱感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖于卷积神经网络（CNN）或Transformer进行空间域特征融合，很少探索频率域特征的潜力，导致对多光谱数据的互补信息利用不足。因此，作者希望设计方法有效结合空间与频率域信息，提升多光谱目标检测效果。

Method: 作者提出空间与频率特征重建（SFFR）方法，核心包括两个模块：1）频率成分交换KAN（FCEKAN），通过RGB和红外（IR）图像的频率特征，选择性交换频率成分，增强跨模态特征互补性和一致性；2）多尺度高斯KAN（MSGKAN），基于多尺度高斯基函数，有效感知无人机飞行高度变化带来的尺度特征变化，提升模型稳健性和自适应能力。两模块结合，充分挖掘空间和频率语义特征，对特征融合有重要作用。

Result: 在SeaDroneSee、DroneVehicle、DVTOD等无人机多光谱目标检测数据集上，所提方法取得了优于现有方法的性能，表现出显著优势。实验验证了FCEKAN和MSGKAN模块的互补性及其对特征融合的实际提升。

Conclusion: 提出的SFFR方法在空间与频率域实现创新性特征重建和融合，有效提升了无人机多光谱目标感知效果和模型对尺度变化的适应能力。该方法为多光谱目标检测提供了新的思路，具有实际应用价值。

Abstract: Recent multispectral object detection methods have primarily focused on
spatial-domain feature fusion based on CNNs or Transformers, while the
potential of frequency-domain feature remains underexplored. In this work, we
propose a novel Spatial and Frequency Feature Reconstruction method (SFFR)
method, which leverages the spatial-frequency feature representation mechanisms
of the Kolmogorov-Arnold Network (KAN) to reconstruct complementary
representations in both spatial and frequency domains prior to feature fusion.
The core components of SFFR are the proposed Frequency Component Exchange KAN
(FCEKAN) module and Multi-Scale Gaussian KAN (MSGKAN) module. The FCEKAN
introduces an innovative selective frequency component exchange strategy that
effectively enhances the complementarity and consistency of cross-modal
features based on the frequency feature of RGB and IR images. The MSGKAN module
demonstrates excellent nonlinear feature modeling capability in the spatial
domain. By leveraging multi-scale Gaussian basis functions, it effectively
captures the feature variations caused by scale changes at different UAV flight
altitudes, significantly enhancing the model's adaptability and robustness to
scale variations. It is experimentally validated that our proposed FCEKAN and
MSGKAN modules are complementary and can effectively capture the frequency and
spatial semantic features respectively for better feature fusion. Extensive
experiments on the SeaDroneSee, DroneVehicle and DVTOD datasets demonstrate the
superior performance and significant advantages of the proposed method in UAV
multispectral object perception task. Code will be available at
https://github.com/qchenyu1027/SFFR.

</details>


### [106] [Physics-Informed Deformable Gaussian Splatting: Towards Unified Constitutive Laws for Time-Evolving Material Field](https://arxiv.org/abs/2511.06299)
*Haoqin Hong,Ding Fan,Fubin Dou,Zhi-Li Zhou,Haoran Sun,Congcong Zhu,Jingrun Chen*

Main category: cs.CV

TL;DR: 本文提出了Physics-Informed Deformable Gaussian Splatting (PIDG)方法，在三维高斯粒子表示的基础上，结合物理约束提升动态场景的新视角合成质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D Gaussian Splatting（3DGS）方法虽能处理动态新视角合成，但由于完全数据驱动，难以捕捉动态场景中的物理规律驱动的复杂运动模式。为解决这一短板，作者引入物理知识辅助。

Method: 将每个高斯粒子视为具有时变本构参数的Lagrange物质点，并通过2D光流投影进行运动约束；采用静态-动态解耦的4D哈希编码重建形状和运动；引入Cauchy动量残差作为物理约束，并预测粒子的速度与应力；通过粒子流与相机补偿光流的对齐进一步监督，提高收敛速度和泛化能力。

Result: 在定制的物理数据集、标准合成以及真实数据集上，所提方法在物理一致性和单目动态重建质量上均显著优于现有方法。

Conclusion: 融合物理知识的高斯粒子方法能有效提升动态场景的物理合规性与重建精度，对单目动态新视角合成有很大推动作用。

Abstract: Recently, 3D Gaussian Splatting (3DGS), an explicit scene representation
technique, has shown significant promise for dynamic novel-view synthesis from
monocular video input. However, purely data-driven 3DGS often struggles to
capture the diverse physics-driven motion patterns in dynamic scenes. To fill
this gap, we propose Physics-Informed Deformable Gaussian Splatting (PIDG),
which treats each Gaussian particle as a Lagrangian material point with
time-varying constitutive parameters and is supervised by 2D optical flow via
motion projection. Specifically, we adopt static-dynamic decoupled 4D
decomposed hash encoding to reconstruct geometry and motion efficiently.
Subsequently, we impose the Cauchy momentum residual as a physics constraint,
enabling independent prediction of each particle's velocity and constitutive
stress via a time-evolving material field. Finally, we further supervise data
fitting by matching Lagrangian particle flow to camera-compensated optical
flow, which accelerates convergence and improves generalization. Experiments on
a custom physics-driven dataset as well as on standard synthetic and real-world
datasets demonstrate significant gains in physical consistency and monocular
dynamic reconstruction quality.

</details>


### [107] [HiMo-CLIP: Modeling Semantic Hierarchy and Monotonicity in Vision-Language Alignment](https://arxiv.org/abs/2511.06653)
*Ruijia Wu,Ping Chen,Fei Shen,Shaoan Zhao,Qiang Hui,Huanlin Gao,Ting Lu,Zhaoxiang Liu,Fang Zhao,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: 本文提出了HiMo-CLIP，通过提升CLIP等模型对文本语义结构的理解能力，增强了图文检索任务中的表现。HiMo-CLIP引入了分层语义分解和单调性损失，结构性提升了跨模态语义对齐效果，尤其适用于复杂或长文本描述。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言对比学习模型（如CLIP）在图文检索中表现优异，但通常将文本视为扁平序列，难以处理复杂、组合性强及长文本描述，无法捕捉语言的层次性和单调性等关键特征。

Method: 提出HiMo-CLIP表示级增强框架，无需修改编码器。包含两个核心模块：1）层次分解（HiDe）模块，利用批量PCA分析长文本，分解为不同语义层级；2）单调性对比损失（MoLo），联合对齐全局和局部文本表示，使模型学习语义单调性和层级结构。

Result: 在多个图文检索基准上，HiMo-CLIP在面对长或组合性描述任务时，效果优于主流强基线。

Conclusion: HiMo-CLIP通过结构化建模文本语义层次和单调性，提升了跨模态对齐能力，对复杂文本场景下图文检索任务特别有效。

Abstract: Contrastive vision-language models like CLIP have achieved impressive results
in image-text retrieval by aligning image and text representations in a shared
embedding space. However, these models often treat text as flat sequences,
limiting their ability to handle complex, compositional, and long-form
descriptions. In particular, they fail to capture two essential properties of
language: semantic hierarchy, which reflects the multi-level compositional
structure of text, and semantic monotonicity, where richer descriptions should
result in stronger alignment with visual content.To address these limitations,
we propose HiMo-CLIP, a representation-level framework that enhances CLIP-style
models without modifying the encoder architecture. HiMo-CLIP introduces two key
components: a hierarchical decomposition (HiDe) module that extracts latent
semantic components from long-form text via in-batch PCA, enabling flexible,
batch-aware alignment across different semantic granularities, and a
monotonicity-aware contrastive loss (MoLo) that jointly aligns global and
component-level representations, encouraging the model to internalize semantic
ordering and alignment strength as a function of textual completeness.These
components work in concert to produce structured, cognitively-aligned
cross-modal representations. Experiments on multiple image-text retrieval
benchmarks show that HiMo-CLIP consistently outperforms strong baselines,
particularly under long or compositional descriptions. The code is available at
https://github.com/UnicomAI/HiMo-CLIP.

</details>


### [108] [Adaptive 3D Reconstruction via Diffusion Priors and Forward Curvature-Matching Likelihood Updates](https://arxiv.org/abs/2511.06310)
*Seunghyeok Shin,Dabin Kim,Hongki Lim*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于前向曲率匹配（FCM）更新方式的扩散采样方法，实现了更高质量和更高效率的从图像重建点云。该方法无需重新训练，支持多种输入视角和数据类型，在ShapeNet与CO3D数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型用于图像到点云的重建时，存在对训练条件依赖强、输入视角数固定、不同观测需重新训练等不灵活性问题。近期方法试图通过联合先验与似然优化迭代改进，但采用启发式固定步长更新，导致收敛慢且重建质量不佳。因此需要一种高效且无需重训练、适应多输入类型的方法。

Method: 本文提出了前向曲率匹配（FCM）更新算法，将其融入扩散采样流程。该方法利用前向自动微分和有限差分估算曲率，动态调整最优步长，实现对似然更新的精确优化，能够灵活支持单视图、多视图及多模态输入，无需模型重训，仅需算子替换即可处理不同输入。

Result: 在ShapeNet和CO3D数据集上的实验显示，该方法在相同或更低扩散步数下实现了更高的F-score、更低的Chamfer Distance（CD）与Earth Mover’s Distance（EMD），证明了方法的重建质量优越、收敛速度更快。

Conclusion: FCM更新的扩散采样方法有效提升了点云重建的精度和效率，具有高度适应性和无需重训的实用优势，在多种实际输入和场景下均表现出色。

Abstract: Reconstructing high-quality point clouds from images remains challenging in
computer vision. Existing generative-model-based approaches, particularly
diffusion-model approaches that directly learn the posterior, may suffer from
inflexibility -- they require conditioning signals during training, support
only a fixed number of input views, and need complete retraining for different
measurements. Recent diffusion-based methods have attempted to address this by
combining prior models with likelihood updates, but they rely on heuristic
fixed step sizes for the likelihood update that lead to slow convergence and
suboptimal reconstruction quality. We advance this line of approach by
integrating our novel Forward Curvature-Matching (FCM) update method with
diffusion sampling. Our method dynamically determines optimal step sizes using
only forward automatic differentiation and finite-difference curvature
estimates, enabling precise optimization of the likelihood update. This
formulation enables high-fidelity reconstruction from both single-view and
multi-view inputs, and supports various input modalities through simple
operator substitution -- all without retraining. Experiments on ShapeNet and
CO3D datasets demonstrate that our method achieves superior reconstruction
quality at matched or lower NFEs, yielding higher F-score and lower CD and EMD,
validating its efficiency and adaptability for practical applications. Code is
available at https://github.com/Seunghyeok0715/FCM

</details>


### [109] [Revisiting the Data Sampling in Multimodal Post-training from a Difficulty-Distinguish View](https://arxiv.org/abs/2511.06722)
*Jianyu Qi,Ding Zou,Wenrui Yan,Rui Ma,Jiaxu Li,Zhijie Zheng,Zhiguo Yang,Rongchang Zhao*

Main category: cs.CV

TL;DR: 本文提出了两种新的难度感知数据采样策略，以提升多模态大语言模型在感知与推理上的整体性能。实验证明，基于难度分层采样的训练优于传统方法，且可减少对监督微调的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型的后训练范式主要关注数学数据集，但普遍缺乏能够量化样本难度的指标，且未能同时优化模型在感知与推理方面的能力。

Method: 提出了两种难度感知采样策略：1) 渐进式图像语义遮挡（PISM），通过系统性图像降质量化样本难度；2) 跨模态注意力平衡（CMAB），通过分析注意力分布评估跨模态交互复杂度。基于这些度量，设计了分层训练框架，结合GRPO训练和SFT+GRPO混合训练，并在六个基准数据集上进行实验对比。

Result: 实验证明，基于难度分层采样的数据，经GRPO训练后，在各项指标上均优于传统SFT+GRPO训练流程。策略性采样能显著提升模型表现，并减少对监督微调的依赖。

Conclusion: 难度感知采样能够有效提升多模态大模型在推理与感知任务的整体表现，是后续多模态建模训练的重要方向。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have spurred
significant progress in Chain-of-Thought (CoT) reasoning. Building on the
success of Deepseek-R1, researchers extended multimodal reasoning to
post-training paradigms based on reinforcement learning (RL), focusing
predominantly on mathematical datasets. However, existing post-training
paradigms tend to neglect two critical aspects: (1) The lack of quantifiable
difficulty metrics capable of strategically screening samples for post-training
optimization. (2) Suboptimal post-training paradigms that fail to jointly
optimize perception and reasoning capabilities. To address this gap, we propose
two novel difficulty-aware sampling strategies: Progressive Image Semantic
Masking (PISM) quantifies sample hardness through systematic image degradation,
while Cross-Modality Attention Balance (CMAB) assesses cross-modal interaction
complexity via attention distribution analysis. Leveraging these metrics, we
design a hierarchical training framework that incorporates both GRPO-only and
SFT+GRPO hybrid training paradigms, and evaluate them across six benchmark
datasets. Experiments demonstrate consistent superiority of GRPO applied to
difficulty-stratified samples compared to conventional SFT+GRPO pipelines,
indicating that strategic data sampling can obviate the need for supervised
fine-tuning while improving model accuracy. Our code will be released at
https://github.com/qijianyu277/DifficultySampling.

</details>


### [110] [Seq2Seq Models Reconstruct Visual Jigsaw Puzzles without Seeing Them](https://arxiv.org/abs/2511.06315)
*Gur Elkn,Ofir Itzhak Shahar,Ohad Ben-Shahar*

Main category: cs.CV

TL;DR: 本论文提出利用语言模型（而非视觉模型）解决方块拼图问题，仅通过对拼图片的离散token序列进行推理，达到甚至超过传统视觉方法的效果。


<details>
  <summary>Details</summary>
Motivation: 传统拼图重建主要依赖视觉特征，但作者希望探索语言模型在无视觉输入情况下，是否也能有效解决此问题，以挖掘语言模型在非原生领域的潜力。

Method: 作者设计了一种自定义tokenizer，将每个拼图片编码为token序列，并利用encoder-decoder结构的transformer，将拼图还原任务转化为序列到序列预测问题，让模型仅通过token推理原始布局。

Result: 所提出方法在多个基准任务上都取得了最先进（state-of-the-art）的成绩，在许多情况下甚至优于基于视觉的方法。

Conclusion: 语言模型即使无法访问原始视觉信息，也能精确拼合拼图。该结果表明语言模型的推理能力超出预期，为拼图及相关问题研究提供了新的思路。

Abstract: Jigsaw puzzles are primarily visual objects, whose algorithmic solutions have
traditionally been framed from a visual perspective. In this work, however, we
explore a fundamentally different approach: solving square jigsaw puzzles using
language models, without access to raw visual input. By introducing a
specialized tokenizer that converts each puzzle piece into a discrete sequence
of tokens, we reframe puzzle reassembly as a sequence-to-sequence prediction
task. Treated as "blind" solvers, encoder-decoder transformers accurately
reconstruct the original layout by reasoning over token sequences alone.
Despite being deliberately restricted from accessing visual input, our models
achieve state-of-the-art results across multiple benchmarks, often
outperforming vision-based methods. These findings highlight the surprising
capability of language models to solve problems beyond their native domain, and
suggest that unconventional approaches can inspire promising directions for
puzzle-solving research.

</details>


### [111] [SpatialThinker: Reinforcing 3D Reasoning in Multimodal LLMs via Spatial Rewards](https://arxiv.org/abs/2511.07403)
*Hunar Batra,Haoqin Tu,Hardy Chen,Yuanze Lin,Cihang Xie,Ronald Clark*

Main category: cs.CV

TL;DR: 本文提出了SpatialThinker，一种具备3D空间理解能力的多模态大模型，显著提升了模型在空间推理和视觉问答任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型（MLLMs）在视觉-语言任务上虽然取得进展，但在空间理解上仍有明显不足。以往方法往往依赖复杂的3D输入或特殊架构，并受限于大规模数据集和稀疏的监督，限制了其空间推理能力。

Method: 作者提出SpatialThinker，它通过强化学习（RL）框架结合稠密的空间奖励约束，实现了结构化空间定位和多步空间推理。该方法包括两个核心创新：1）构建一个自动化数据合成流程，生成高质量空间视觉问答数据集STVQA-7K；2）采用多目标稠密空间奖励，强化空间理解。模型通过构建场景图模拟人类空间感知，并持续优化推理过程。

Result: SpatialThinker-7B在空间理解和真实世界视觉问答基准上优于有监督微调和稀疏RL基线，空间推理性能几乎翻倍，并超过了GPT-4o。

Conclusion: 结合空间监督与奖励对齐推理的方法大幅提升了有限数据条件下3D空间理解的能力，推动多模态大模型向人类水平的视觉推理迈进。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress in
vision-language tasks, but they continue to struggle with spatial
understanding. Existing spatial MLLMs often rely on explicit 3D inputs or
architecture-specific modifications, and remain constrained by large-scale
datasets or sparse supervision. To address these limitations, we introduce
SpatialThinker, a 3D-aware MLLM trained with RL to integrate structured spatial
grounding with multi-step reasoning. The model simulates human-like spatial
perception by constructing a scene graph of task-relevant objects and spatial
relations, and reasoning towards an answer via dense spatial rewards.
SpatialThinker consists of two key contributions: (1) a data synthesis pipeline
that generates STVQA-7K, a high-quality spatial VQA dataset, and (2) online RL
with a multi-objective dense spatial reward enforcing spatial grounding.
SpatialThinker-7B outperforms supervised fine-tuning and the sparse RL baseline
on spatial understanding and real-world VQA benchmarks, nearly doubling the
base-model gain compared to sparse RL, and surpassing GPT-4o. These results
showcase the effectiveness of combining spatial supervision with reward-aligned
reasoning in enabling robust 3D spatial understanding with limited data and
advancing MLLMs towards human-level visual reasoning.

</details>


### [112] [CINEMAE: Leveraging Frozen Masked Autoencoders for Cross-Generator AI Image Detection](https://arxiv.org/abs/2511.06325)
*Minsuk Jang,Hyeonseo Jeong,Minseok Son,Changick Kim*

Main category: cs.CV

TL;DR: CINEMAE是一种新的AIGC（AI生成内容）图像检测方法，通过模仿文本检测中的上下文一致性原理，实现了对各类生成器的强泛化能力。只需在一个生成器上训练，便能在多个生成器上的公开基准测试中获得超95%的准确率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于上下文的方法在检测AI文本生成内容时表现优异，能较好地泛化处理不同生成模型。然而，现有的AI生成图像检测器容易在特定的生成器伪影上过拟合，很难泛化到其他生成模型。该论文旨在解决AIGC图像检测中缺乏泛化能力的问题。

Method: 提出CINEMAE的方法。核心思想是将文本生成检测中的上下文一致性机制推广到视觉领域。作者用Masked AutoEncoder（MAE）训练模型，重建被遮挡的图像块，通过探测局部语义异常来检测AI生成图像。具体做法是以条件负对数似然（NLL）度量重建不确定性，并将片段级统计数据与全局特征融合，实现最终判别。此外，整个方法只依赖一种生成器（Stable Diffusion v1.4）进行训练。

Result: 在GenImage公开基准测试中，CINEMAE模型仅用一个生成器训练，在另外8个此前未见生成器生成的样本上检测准确率超过95%，大幅优于现有最先进的检测方法。

Conclusion: 利用条件重建不确定性作为信号，CINEMAE实现了高水平、可迁移的AIGC图像检测能力，有望成为该领域具有广泛适应性的强检测工具。

Abstract: While context-based detectors have achieved strong generalization for
AI-generated text by measuring distributional inconsistencies, image-based
detectors still struggle with overfitting to generator-specific artifacts. We
introduce CINEMAE, a novel paradigm for AIGC image detection that adapts the
core principles of text detection methods to the visual domain. Our key insight
is that Masked AutoEncoder (MAE), trained to reconstruct masked patches
conditioned on visible context, naturally encodes semantic consistency
expectations. We formalize this reconstruction process probabilistically,
computing conditional Negative Log-Likelihood (NLL, p(masked | visible)) to
quantify local semantic anomalies. By aggregating these patch-level statistics
with global MAE features through learned fusion, CINEMAE achieves strong
cross-generator generalization. Trained exclusively on Stable Diffusion v1.4,
our method achieves over 95% accuracy on all eight unseen generators in the
GenImage benchmark, substantially outperforming state-of-the-art detectors.
This demonstrates that context-conditional reconstruction uncertainty provides
a robust, transferable signal for AIGC detection.

</details>


### [113] [Improving Multimodal Sentiment Analysis via Modality Optimization and Dynamic Primary Modality Selection](https://arxiv.org/abs/2511.06328)
*Dingkang Yang,Mingcheng Li,Xuecheng Wu,Zhaoyu Chen,Kaixun Jiang,Keliang Liu,Peng Zhai,Lihua Zhang*

Main category: cs.CV

TL;DR: 本文提出一种针对多模态情感分析中模态失衡和冗余噪声问题的新方法，能够动态选择主导模态并优化融合效果，有效提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析常用语言、音频和视觉数据预测情感，但不同模态表现不均衡，且现有方法通常固定主模态，难以适应每个样本中模态重要性的动态变化。音频和视觉模态还常有冗余和噪声，进一步影响融合表现。

Method: 提出了一种模态优化与动态主模态选择框架（MODS）。具体包括：1）利用图神经网络和胶囊网络构建序列压缩器（GDC）以减少音频/视觉冗余；2）引入样本自适应的主模态选择器（MSelector），为每个样本动态确定主导模态；3）设计主模态中心交叉注意力模块（PCCA），加强主模态并促进跨模态信息交互。

Result: 在四个基准数据集上进行了大量实验，结果表明MODS方法全面优于现有主流方法，在平衡各模态贡献、消除冗余噪声方面表现突出。

Conclusion: MODS框架通过动态优化主模态选择与去除冗余噪声，有效提升多模态情感分析性能，对相关研究具有参考价值。

Abstract: Multimodal Sentiment Analysis (MSA) aims to predict sentiment from language,
acoustic, and visual data in videos. However, imbalanced unimodal performance
often leads to suboptimal fused representations. Existing approaches typically
adopt fixed primary modality strategies to maximize dominant modality
advantages, yet fail to adapt to dynamic variations in modality importance
across different samples. Moreover, non-language modalities suffer from
sequential redundancy and noise, degrading model performance when they serve as
primary inputs. To address these issues, this paper proposes a modality
optimization and dynamic primary modality selection framework (MODS). First, a
Graph-based Dynamic Sequence Compressor (GDC) is constructed, which employs
capsule networks and graph convolution to reduce sequential redundancy in
acoustic/visual modalities. Then, we develop a sample-adaptive Primary Modality
Selector (MSelector) for dynamic dominance determination. Finally, a
Primary-modality-Centric Cross-Attention (PCCA) module is designed to enhance
dominant modalities while facilitating cross-modal interaction. Extensive
experiments on four benchmark datasets demonstrate that MODS outperforms
state-of-the-art methods, achieving superior performance by effectively
balancing modality contributions and eliminating redundant noise.

</details>


### [114] [Label-Efficient 3D Forest Mapping: Self-Supervised and Transfer Learning for Individual, Structural, and Species Analysis](https://arxiv.org/abs/2511.06331)
*Aldino Rizaldy,Fabian Ewald Fassnacht,Ahmed Jamal Afifi,Hua Jiang,Richard Gloaguen,Pedram Ghamisi*

Main category: cs.CV

TL;DR: 本文提出了一种结合自监督学习与迁移学习的统一框架，用于提升森林激光点云中单木结构和种类信息提取的效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 当今林业精细管理、生物多样性保护及碳储量估算越来越依赖单木尺度的精准结构和种类信息。激光点云为大规模快速获取此类数据提供了可能，但高质量三维点云的人工标注极为耗时且难以扩展。需探索更高效的数据利用与模型训练策略。

Method: 作者采用自监督学习、域适应和分层迁移学习技术，针对实例分割、语义分割和树种分类任务优化深度学习模型训练，减少对大规模人工标注数据的依赖，并将相关任务集成至统一框架中，实现端到端自动处理。

Result: 实验证明：自监督+域适应方法能显著提升实例分割性能（AP50提升16.98%）；语义分割仅用自监督即有提升（mIoU提升1.79%）；分层迁移学习提升对新树种的分类准确性（Jaccard提升6.07%）。预训练模型还可减少21%的能耗和碳排放。

Conclusion: 该开源方法框架大幅度简化并提速了森林激光点云单木信息的自动提取，对林业、生态监测与碳盘查等应用具有重要支持作用。

Abstract: Detailed structural and species information on individual tree level is
increasingly important to support precision forestry, biodiversity
conservation, and provide reference data for biomass and carbon mapping. Point
clouds from airborne and ground-based laser scanning are currently the most
suitable data source to rapidly derive such information at scale. Recent
advancements in deep learning improved segmenting and classifying individual
trees and identifying semantic tree components. However, deep learning models
typically require large amounts of annotated training data which limits further
improvement. Producing dense, high-quality annotations for 3D point clouds,
especially in complex forests, is labor-intensive and challenging to scale. We
explore strategies to reduce dependence on large annotated datasets using
self-supervised and transfer learning architectures. Our objective is to
improve performance across three tasks: instance segmentation, semantic
segmentation, and tree classification using realistic and operational training
sets. Our findings indicate that combining self-supervised learning with domain
adaptation significantly enhances instance segmentation compared to training
from scratch (AP50 +16.98%), self-supervised learning suffices for semantic
segmentation (mIoU +1.79%), and hierarchical transfer learning enables accurate
classification of unseen species (Jaccard +6.07%). To simplify use and
encourage uptake, we integrated the tasks into a unified framework,
streamlining the process from raw point clouds to tree delineation, structural
analysis, and species classification. Pretrained models reduce energy
consumption and carbon emissions by ~21%. This open-source contribution aims to
accelerate operational extraction of individual tree information from laser
scanning point clouds to support forestry, biodiversity, and carbon mapping.

</details>


### [115] [BuildingWorld: A Structured 3D Building Dataset for Urban Foundation Models](https://arxiv.org/abs/2511.06337)
*Shangfeng Huang,Ruisheng Wang,Xin Wang*

Main category: cs.CV

TL;DR: 本文提出了一个名为BuildingWorld的大规模结构化3D建筑数据集，涵盖全球多样化建筑风格，为更通用的城市级AI模型开发提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的3D城市建模方法，所用数据集中建筑风格多样性有限，导致模型难以泛化到不同城市和建筑类型。需要一个覆盖全球、丰富多样的建筑3D数据集以提升研究与应用效果。

Method: 作者构建了BuildingWorld数据集，收集了来自全球五大洲大约五百万个LOD2级别的3D建筑模型，以及真实和模拟的机载LiDAR点云；并引入虚拟城市Cyber City来生成无限量、可定制、结构多样的训练数据。还提供了用于建筑重建的标准化评测指标。

Result: BuildingWorld数据集能够支持3D建筑重建、检测和分割等多项任务测试，并为大规模基础模型的开发和评估提供了丰富的数据源和标准化的测试环境。

Conclusion: BuildingWorld数据集弥补了现有3D建筑数据集在建筑风格和地理多样性上的不足，将促进城市级AI基础模型的发展，有助于智能城市等相关领域的研究与实践。

Abstract: As digital twins become central to the transformation of modern cities,
accurate and structured 3D building models emerge as a key enabler of
high-fidelity, updatable urban representations. These models underpin diverse
applications including energy modeling, urban planning, autonomous navigation,
and real-time reasoning. Despite recent advances in 3D urban modeling, most
learning-based models are trained on building datasets with limited
architectural diversity, which significantly undermines their generalizability
across heterogeneous urban environments. To address this limitation, we present
BuildingWorld, a comprehensive and structured 3D building dataset designed to
bridge the gap in stylistic diversity. It encompasses buildings from
geographically and architecturally diverse regions -- including North America,
Europe, Asia, Africa, and Oceania -- offering a globally representative dataset
for urban-scale foundation modeling and analysis. Specifically, BuildingWorld
provides about five million LOD2 building models collected from diverse
sources, accompanied by real and simulated airborne LiDAR point clouds. This
enables comprehensive research on 3D building reconstruction, detection and
segmentation. Cyber City, a virtual city model, is introduced to enable the
generation of unlimited training data with customized and structurally diverse
point cloud distributions. Furthermore, we provide standardized evaluation
metrics tailored for building reconstruction, aiming to facilitate the
training, evaluation, and comparison of large-scale vision models and
foundation models in structured 3D urban environments.

</details>


### [116] [GazeVLM: A Vision-Language Model for Multi-Task Gaze Understanding](https://arxiv.org/abs/2511.06348)
*Athul M. Mathew,Haithem Hermassi,Thariq Khalid,Arshad Ali Khan,Riad Souissi*

Main category: cs.CV

TL;DR: 论文提出了GazeVLM模型，将人与目标检测、凝视目标检测和凝视对象识别统一到同一视觉-语言框架，并在公开数据集上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有凝视理解方法无法同时结合视觉和语言信息，高效统一完成凝视相关多任务。作者希望通过融合多模态改进凝视理解系统的实用性和准确性。

Method: 提出了GazeVLM视觉-语言模型，通过融合RGB图像、HHA深度编码和文本输入，对人物检测、凝视目标检测及凝视对象识别进行统一建模。引入了新的凝视对象检测指标，并进行了消融实验分析不同输入组合的影响。

Result: GazeVLM在GazeFollow和VideoAttentionTarget数据集上实现了新的最优评测成绩，其RGB与HHA深度融合方案效果最佳。新提出的对象级凝视检测指标亦展示出模型识别能力提升。

Conclusion: GazeVLM首次将视觉-语言模型应用于多任务凝视理解，有效提升了相关任务的性能，为视觉注意力和意图估计研究提供了新的方法和度量。

Abstract: Gaze understanding unifies the detection of people, their gaze targets, and
objects of interest into a single framework, offering critical insight into
visual attention and intent estimation. Although prior research has modelled
gaze cues in visual scenes, a unified system is still needed for gaze
understanding using both visual and language prompts. This paper introduces
GazeVLM, a novel Vision-Language Model (VLM) for multi-task gaze understanding
in images, addressing person detection, gaze target detection, and gaze object
identification. While other transformer-based methods exist for gaze analysis,
GazeVLM represents, to our knowledge, the first application of a VLM to these
combined tasks, allowing for selective execution of each task. Through the
integration of visual (RGB and depth) and textual modalities, our ablation
study on visual input combinations revealed that a fusion of RGB images with
HHA-encoded depth maps, guided by text prompts, yields superior performance. We
also introduce an object-level gaze detection metric for gaze object
identification ($AP_{ob}$). Through experiments, GazeVLM demonstrates
significant improvements, notably achieving state-of-the-art evaluation scores
on GazeFollow and VideoAttentionTarget datasets.

</details>


### [117] [AesTest: Measuring Aesthetic Intelligence from Perception to Production](https://arxiv.org/abs/2511.06360)
*Guolong Wang,Heng Huang,Zhiqiang Zhang,Wentian Li,Feilong Ma,Xin Jin*

Main category: cs.CV

TL;DR: 该论文提出了AesTest，这是一个面向多模态大模型（MLLM）审美感知与生产的全面性基准测试，从多个维度评估模型的审美认知与产出能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在图像审美评估方面的能力尚待深入探索，且现有基准测试局限性较大，难以系统性和多样性地评价模型的审美产出能力，因此需要更完备且更具代表性的基准。

Method: 作者构建了AesTest基准，包括涵盖审美感知、欣赏、创作和摄影的十项任务，均基于生成性学习的心理学理论设计；整合了专业编辑、构图教程和众包偏好的多元数据，确保内容既有专家水准也反映真实多样性；支持多种审美查询类型，如属性分析、情感共鸣、构图选择和风格推理。

Result: 通过AesTest对调优后的审美模型和通用多模态大模型进行评测，发现现有技术在构建审美智能方面仍面临显著挑战。

Conclusion: AesTest填补了多模态大模型审美评估的基准空白，未来将在社区公开以促进相关领域的研究进展。

Abstract: Perceiving and producing aesthetic judgments is a fundamental yet
underexplored capability for multimodal large language models (MLLMs). However,
existing benchmarks for image aesthetic assessment (IAA) are narrow in
perception scope or lack the diversity needed to evaluate systematic aesthetic
production. To address this gap, we introduce AesTest, a comprehensive
benchmark for multimodal aesthetic perception and production, distinguished by
the following features: 1) It consists of curated multiple-choice questions
spanning ten tasks, covering perception, appreciation, creation, and
photography. These tasks are grounded in psychological theories of generative
learning. 2) It integrates data from diverse sources, including professional
editing workflows, photographic composition tutorials, and crowdsourced
preferences. It ensures coverage of both expert-level principles and real-world
variation. 3) It supports various aesthetic query types, such as
attribute-based analysis, emotional resonance, compositional choice, and
stylistic reasoning. We evaluate both instruction-tuned IAA MLLMs and general
MLLMs on AesTest, revealing significant challenges in building aesthetic
intelligence. We will publicly release AesTest to support future research in
this area.

</details>


### [118] [V-Shuffle: Zero-Shot Style Transfer via Value Shuffle](https://arxiv.org/abs/2511.06365)
*Haojun Tang,Qiwei Lin,Tongda Xu,Lida Huang,Yan Wang*

Main category: cs.CV

TL;DR: 本文提出了V-Shuffle，一种利用多张同风格图片、基于注意力机制的风格迁移方法，有效缓解了内容泄露问题，在多图和单图场景下均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前的风格迁移方法常因注意力机制造成内容泄漏，即风格图像不需要的语义内容被带入结果。

Method: V-Shuffle通过在扩散模型的自注意力层中打乱style图片的value特征，抑制其语义内容但保留低层风格特征；同时提出混合风格正则化，增强高层风格纹理，提高风格保真度。

Result: 实验证明，V-Shuffle在使用多张风格图像时表现优秀，对单图也优于现有最优方法。

Conclusion: V-Shuffle有效平衡了内容保存与风格表达，提升了风格迁移质量，尤其是在抑制内容泄漏方面效果突出。

Abstract: Attention injection-based style transfer has achieved remarkable progress in
recent years. However, existing methods often suffer from content leakage,
where the undesired semantic content of the style image mistakenly appears in
the stylized output. In this paper, we propose V-Shuffle, a zero-shot style
transfer method that leverages multiple style images from the same style domain
to effectively navigate the trade-off between content preservation and style
fidelity. V-Shuffle implicitly disrupts the semantic content of the style
images by shuffling the value features within the self-attention layers of the
diffusion model, thereby preserving low-level style representations. We further
introduce a Hybrid Style Regularization that complements these low-level
representations with high-level style textures to enhance style fidelity.
Empirical results demonstrate that V-Shuffle achieves excellent performance
when utilizing multiple style images. Moreover, when applied to a single style
image, V-Shuffle outperforms previous state-of-the-art methods.

</details>


### [119] [InfoAffect: A Dataset for Affective Analysis of Infographics](https://arxiv.org/abs/2511.06404)
*Zihang Fu,Yunchao Wang,Chenyu Huang,Guodao Sun,Ronghua Liang*

Main category: cs.CV

TL;DR: 本文提出了一个包含3.5k标注样本的情感信息图数据集InfoAffect，并利用多模态大模型和多项方法提升其情感识别质量，实现高准确度评估。


<details>
  <summary>Details</summary>
Motivation: 尽管信息图表常用于传达复杂信息，但其情感维度研究不足，主要因缺乏相应的数据资源。该研究旨在填补这一空白，为信息图的情感分析提供数据和方法支持。

Method: 从六个领域收集信息图，通过预处理、文本优先法和三项质量保障策略对数据进行对齐与筛选。构建情感表进行受限标注后，利用五种主流多模态大语言模型对文本和图像信息进行多模态分析，并采用RRF算法融合各模型输出，实现情感标注与置信度评估。最后通过用户实验和CACI指标对数据集的实用性和一致性进行评测。

Result: 通过用户研究，使用CACI综合评估InfoAffect数据集获得0.986的高分，表明该数据集具有很高的准确度和一致性。

Conclusion: InfoAffect数据集为信息图的情感分析研究提供了珍贵的数据资源，并证明所提出的方法在情感一致性和准确度上表现优异，有助于相关领域的深入研究和实际应用。

Abstract: Infographics are widely used to convey complex information, yet their
affective dimensions remain underexplored due to the scarcity of data
resources. We introduce a 3.5k-sample affect-annotated InfoAffect dataset,
which combines textual content with real-world infographics. We first collect
the raw data from six domains and aligned them via preprocessing, the
accompanied-text-priority method, and three strategies to guarantee the quality
and compliance. After that we construct an affect table and use it to constrain
annotation. Five state-of-the-art multimodal large language models (MLLMs) then
analyze both modalities, and their outputs are fused with Reciprocal Rank
Fusion (RRF) algorithm to yield robust affects and confidences. We conducted a
user study with two experiments to validate usability and assess InfoAffect
dataset using the Composite Affect Consistency Index (CACI), achieving an
overall score of 0.986, which indicates high accuracy.

</details>


### [120] [On Modality Incomplete Infrared-Visible Object Detection: An Architecture Compatibility Perspective](https://arxiv.org/abs/2511.06406)
*Shuo Yang,Yinghui Xing,Shizhou Zhang,Zhilong Niu*

Main category: cs.CV

TL;DR: 本文提出了一种针对红外与可见光目标检测（IVOD）的新方法，旨在解决当输入数据缺失一种模态时现有检测模型性能大幅下降的问题。作者设计了Scarf Neck模块，使DETR类检测器能灵活适应单一或双重模态输入，并通过训练中的伪模态丢弃策略提升模型鲁棒性。实验表明该方法在模态缺失和模态完整场景下性能均优。


<details>
  <summary>Details</summary>
Motivation: 尽管IVOD在全天候场景中十分重要，但主流检测模型在缺失主导模态数据时表现不佳，因此亟需新的方法来提升模型在模态不完整情况下的鲁棒性和适应性。

Method: 作者提出了Scarf Neck模块，这是一种可插拔的架构，采用模态无关的可变形注意力机制，可兼容单模态与双模态数据。同时，训练阶段引入伪模态丢弃策略以增强模型对模态缺失情况的适应能力。此外，作者搭建了针对模态缺失IVOD的综合评测基准。

Result: 提出的Scarf-DETR模型在缺失主要或次要模态的各种情况下均表现优异，并且在传统双模态完整基准上也达到领先水平。

Conclusion: Scarf-DETR显著提升了IVOD任务中检测模型对模态不完整输入的适应与鲁棒性，且在模态完整和缺失场景下表现均优，推动了IVOD实用化。

Abstract: Infrared and visible object detection (IVOD) is essential for numerous
around-the-clock applications. Despite notable advancements, current IVOD
models exhibit notable performance declines when confronted with incomplete
modality data, particularly if the dominant modality is missing. In this paper,
we take a thorough investigation on modality incomplete IVOD problem from an
architecture compatibility perspective. Specifically, we propose a
plug-and-play Scarf Neck module for DETR variants, which introduces a
modality-agnostic deformable attention mechanism to enable the IVOD detector to
flexibly adapt to any single or double modalities during training and
inference. When training Scarf-DETR, we design a pseudo modality dropout
strategy to fully utilize the multi-modality information, making the detector
compatible and robust to both working modes of single and double modalities.
Moreover, we introduce a comprehensive benchmark for the modality-incomplete
IVOD task aimed at thoroughly assessing situations where the absent modality is
either dominant or secondary. Our proposed Scarf-DETR not only performs
excellently in missing modality scenarios but also achieves superior
performances on the standard IVOD modality complete benchmarks. Our code will
be available at https://github.com/YinghuiXing/Scarf-DETR.

</details>


### [121] [VDNeRF: Vision-only Dynamic Neural Radiance Field for Urban Scenes](https://arxiv.org/abs/2511.06408)
*Zhengyu Zou,Jingfeng Li,Hao Li,Xiaolei Hou,Jinwen Hu,Jingkun Chen,Lechao Cheng,Dingwen Zhang*

Main category: cs.CV

TL;DR: 本文提出了VDNeRF方法，无需额外相机位姿信息或高价传感器，仅通过视觉数据实现动态城市环境中相机轨迹恢复和动态场景重建，优于现有无位姿NeRF方法。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF方法很难在自动驾驶和机器人感知等应用中获得准确的相机位姿，且对大规模动态环境的处理能力有限。因此需要能仅用视觉信息，准确建模和重建动态场景的新方法。

Method: VDNeRF利用两个独立的NeRF模型进行场景联合重建：静态NeRF用于优化相机位姿和静态背景，动态NeRF通过3D场景流对动态物体进行重建。为了消除相机运动与独立物体运动的歧义，设计了自监督分离静态与动态元素的训练框架，提升了相机位姿估计的鲁棒性。

Result: 在主流城市驾驶数据集上，VDNeRF无论在相机位姿估计还是动态新视角合成任务上，都优于当前最先进的无位姿NeRF基方法。

Conclusion: VDNeRF有效实现了大规模动态城市场景中无需外部位姿信息的三维重建和新视角合成，对自动驾驶等领域具有重要应用价值。

Abstract: Neural Radiance Fields (NeRFs) implicitly model continuous three-dimensional
scenes using a set of images with known camera poses, enabling the rendering of
photorealistic novel views. However, existing NeRF-based methods encounter
challenges in applications such as autonomous driving and robotic perception,
primarily due to the difficulty of capturing accurate camera poses and
limitations in handling large-scale dynamic environments. To address these
issues, we propose Vision-only Dynamic NeRF (VDNeRF), a method that accurately
recovers camera trajectories and learns spatiotemporal representations for
dynamic urban scenes without requiring additional camera pose information or
expensive sensor data. VDNeRF employs two separate NeRF models to jointly
reconstruct the scene. The static NeRF model optimizes camera poses and static
background, while the dynamic NeRF model incorporates the 3D scene flow to
ensure accurate and consistent reconstruction of dynamic objects. To address
the ambiguity between camera motion and independent object motion, we design an
effective and powerful training framework to achieve robust camera pose
estimation and self-supervised decomposition of static and dynamic elements in
a scene. Extensive evaluations on mainstream urban driving datasets demonstrate
that VDNeRF surpasses state-of-the-art NeRF-based pose-free methods in both
camera pose estimation and dynamic novel view synthesis.

</details>


### [122] [DiffusionUavLoc: Visually Prompted Diffusion for Cross-View UAV Localization](https://arxiv.org/abs/2511.06422)
*Tao Liu,Kan Ren,Qian Chen*

Main category: cs.CV

TL;DR: 本论文提出了一种名为DiffusionUavLoc的无人机定位方法，针对GNSS不可用环境下的跨视角图像检索问题，通过训练自由的几何渲染与扩散模型，实现了无需文本和复杂标注的鲁棒定位。


<details>
  <summary>Details</summary>
Motivation: 低空经济迅速发展使无人机成为智能巡检系统的核心平台，但传统基于卫星信号的定位方法在GNSS受阻场景下容易失效。现有的跨视角图像检索方法存在视角和外观域差异大，以及网络复杂或依赖大量标注、文本提示等问题，急需更高效、泛化能力强、适用于实际应用的新方法。

Method: 提出DiffusionUavLoc方法。首先，使用训练自由的几何渲染，将无人机斜视图像转化为伪卫星正射图作为结构提示。其次，设计了一个无文本的条件扩散模型，将多模态结构信息融合，学习对视角变化鲁棒的特征。在推理时，于固定时刻提取描述符并用余弦相似度进行比对。整个流程无需文本、依赖少量或无需人工标注。

Result: 在University-1652和SUES-200数据集上，所提出方法在无人机与卫星图像间的跨视角定位任务上表现出色，特别是University-1652数据集上的satellite-to-drone类型任务表现突出。

Conclusion: DiffusionUavLoc框架有效提升了GNSS-denied环境下无人机的跨视角定位能力，简化了模型依赖和工程实现，同时减少了对复杂网络架构和人工标注工作的需求，展现出较强的实际应用价值和泛化潜力。

Abstract: With the rapid growth of the low-altitude economy, unmanned aerial vehicles
(UAVs) have become key platforms for measurement and tracking in intelligent
patrol systems. However, in GNSS-denied environments, localization schemes that
rely solely on satellite signals are prone to failure. Cross-view image
retrieval-based localization is a promising alternative, yet substantial
geometric and appearance domain gaps exist between oblique UAV views and nadir
satellite orthophotos. Moreover, conventional approaches often depend on
complex network architectures, text prompts, or large amounts of annotation,
which hinders generalization. To address these issues, we propose
DiffusionUavLoc, a cross-view localization framework that is image-prompted,
text-free, diffusion-centric, and employs a VAE for unified representation. We
first use training-free geometric rendering to synthesize pseudo-satellite
images from UAV imagery as structural prompts. We then design a text-free
conditional diffusion model that fuses multimodal structural cues to learn
features robust to viewpoint changes. At inference, descriptors are computed at
a fixed time step t and compared using cosine similarity. On University-1652
and SUES-200, the method performs competitively for cross-view localization,
especially for satellite-to-drone in University-1652.Our data and code will be
published at the following URL:
https://github.com/liutao23/DiffusionUavLoc.git.

</details>


### [123] [Diagnose Like A REAL Pathologist: An Uncertainty-Focused Approach for Trustworthy Multi-Resolution Multiple Instance Learning](https://arxiv.org/abs/2511.06433)
*Sungrae Hong,Sol Lee,Jisu Shin,Mun Yong Yi*

Main category: cs.CV

TL;DR: 本文针对组织病理学切片的多分辨率图像诊断，提出了一种新的多示例学习（MIL）方法UFC-MIL，实现了更高效且校准良好的AI病理辅助诊断。


<details>
  <summary>Details</summary>
Motivation: 为满足日益增长的组织病理切片诊断需求，目前多示例学习方法（MIL）虽提升了AI诊断性能，但过于关注准确率，忽视了对于临床专家可依赖的模型预测校准，影响实际应用的可信度。

Method: 提出了UFC-MIL方法：(1) 采用多分辨率图像输入以更贴近病理医生检查流程；(2) 设计了新的patch-wise损失函数与注意力机制，以及邻域patch聚合模块，深入学习实例潜在特征及表达分类不确定性；(3) 利用patch级不确定性对模型预测进行校准，无需多轮推理，提升实用性。

Result: 在公开难度较高的数据集上，UFC-MIL在模型校准表现上优于同类方法，分类准确率与最新方法持平。

Conclusion: UFC-MIL不仅保持了优秀的分类准确性，还极大提升了模型预测的可信度和实用性，为AI辅助病理诊断提供了更可信赖的方案。

Abstract: With the increasing demand for histopathological specimen examination and
diagnostic reporting, Multiple Instance Learning (MIL) has received heightened
research focus as a viable solution for AI-centric diagnostic aid. Recently, to
improve its performance and make it work more like a pathologist, several MIL
approaches based on the use of multiple-resolution images have been proposed,
delivering often higher performance than those that use single-resolution
images. Despite impressive recent developments of multiple-resolution MIL,
previous approaches only focus on improving performance, thereby lacking
research on well-calibrated MIL that clinical experts can rely on for
trustworthy diagnostic results. In this study, we propose Uncertainty-Focused
Calibrated MIL (UFC-MIL), which more closely mimics the pathologists'
examination behaviors while providing calibrated diagnostic predictions, using
multiple images with different resolutions. UFC-MIL includes a novel patch-wise
loss that learns the latent patterns of instances and expresses their
uncertainty for classification. Also, the attention-based architecture with a
neighbor patch aggregation module collects features for the classifier. In
addition, aggregated predictions are calibrated through patch-level uncertainty
without requiring multiple iterative inferences, which is a key practical
advantage. Against challenging public datasets, UFC-MIL shows superior
performance in model calibration while achieving classification accuracy
comparable to that of state-of-the-art methods.

</details>


### [124] [Countering Multi-modal Representation Collapse through Rank-targeted Fusion](https://arxiv.org/abs/2511.06450)
*Seulgi Kim,Kiran Kokilepersaud,Mohit Prabhushankar,Ghassan AlRegib*

Main category: cs.CV

TL;DR: 该论文提出了一种新的多模态融合方法Rank-enhancing Token Fuser，能够同时有效避免特征坍塌和模态坍塌问题，并在动作预判任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态融合会遭遇特征坍塌（特征维度表达能力减弱）和模态坍塌（部分模态主导融合结果）。目前方法只能分别缓解这两种问题，缺乏统一的高效框架。

Method: 提出以有效秩为基础的融合框架Rank-enhancing Token Fuser，理论上通过选择性融合不同模态的互补信息，提升整体表达的有效秩，减少特征和模态的坍塌。同时通过评估不同模态组合，选择可以增加有效秩的融合对，验证RGB与Depth的互补性。

Result: 在NTURGBD、UTKinect、DARai等动作预判数据集上进行实证，所提方法在表现优于现有主流方法，提升最高可达3.74%。

Conclusion: Rank-enhancing Token Fuser提供了同时解决特征和模态坍塌的有效途径，在多模态动作预测等任务上显著提升表现，优于现有技术方案。

Abstract: Multi-modal fusion methods often suffer from two types of representation
collapse: feature collapse where individual dimensions lose their
discriminative power (as measured by eigenspectra), and modality collapse where
one dominant modality overwhelms the other. Applications like human action
anticipation that require fusing multifarious sensor data are hindered by both
feature and modality collapse. However, existing methods attempt to counter
feature collapse and modality collapse separately. This is because there is no
unifying framework that efficiently addresses feature and modality collapse in
conjunction. In this paper, we posit the utility of effective rank as an
informative measure that can be utilized to quantify and counter both the
representation collapses. We propose \textit{Rank-enhancing Token Fuser}, a
theoretically grounded fusion framework that selectively blends less
informative features from one modality with complementary features from another
modality. We show that our method increases the effective rank of the fused
representation. To address modality collapse, we evaluate modality combinations
that mutually increase each others' effective rank. We show that depth
maintains representational balance when fused with RGB, avoiding modality
collapse. We validate our method on action anticipation, where we present
\texttt{R3D}, a depth-informed fusion framework. Extensive experiments on
NTURGBD, UTKinect, and DARai demonstrate that our approach significantly
outperforms prior state-of-the-art methods by up to 3.74\%. Our code is
available at:
\href{https://github.com/olivesgatech/R3D}{https://github.com/olivesgatech/R3D}.

</details>


### [125] [EIDSeg: A Pixel-Level Semantic Segmentation Dataset for Post-Earthquake Damage Assessment from Social Media Images](https://arxiv.org/abs/2511.06456)
*Huili Huang,Chengeng Liu,Danrong Zhang,Shail Patel,Anastasiya Masalava,Sagar Sadak,Parisa Babolhavaeji,WeiHong Low,Max Mahdi Roozbahani,J. David Frost*

Main category: cs.CV

TL;DR: 本文提出EIDSeg，首个专为地震后社交网络图片设立的大规模语义分割数据集，并证明可以通过非专业标注者高效、一致地生成五大基础损毁类别的像素级标注。测试表明现有分割模型在该任务上可达高精度。


<details>
  <summary>Details</summary>
Motivation: 快速、精准的地震损毁评估对于救援和资源调度十分关键，然而现有遥感方法成本高、依赖专家标注且仅生成粗糙的二分类图，社交网络地面照片有着丰富且细粒度的信息，利用其数据需有针对性的数据集和标注体系。

Method: 构建EIDSeg数据集，收集2008-2023年九次大地震共3266张社交网络图片，设计三阶段、跨学科的非专业标注流程，根据基础五类别（未损坏建筑、损坏建筑、毁坏建筑、未损坏道路、损坏道路）进行像素级标注；对多种主流分割模型进行基准测试。

Result: 非专业标注者之间一致性超过70%；Encoder-only Mask Transformer（EoMT）分割模型在该数据集上获得80.8%的mIoU，表现最佳，其他主流方法亦有较好表现。

Conclusion: 利用社交网络地面图片并结合高效的标注方法可实现快速、精细的地震后损毁评估，有助于灾后响应与决策，EIDSeg数据集构建为相关研究提供了有力基础。

Abstract: Rapid post-earthquake damage assessment is crucial for rescue and resource
planning. Still, existing remote sensing methods depend on costly aerial
images, expert labeling, and produce only binary damage maps for early-stage
evaluation. Although ground-level images from social networks provide a
valuable source to fill this gap, a large pixel-level annotated dataset for
this task is still unavailable. We introduce EIDSeg, the first large-scale
semantic segmentation dataset specifically for post-earthquake social media
imagery. The dataset comprises 3,266 images from nine major earthquakes
(2008-2023), annotated across five classes of infrastructure damage: Undamaged
Building, Damaged Building, Destroyed Building, Undamaged Road, and Damaged
Road. We propose a practical three-phase cross-disciplinary annotation protocol
with labeling guidelines that enables consistent segmentation by non-expert
annotators, achieving over 70% inter-annotator agreement. We benchmark several
state-of-the-art segmentation models, identifying Encoder-only Mask Transformer
(EoMT) as the top-performing method with a Mean Intersection over Union (mIoU)
of 80.8%. By unlocking social networks' rich ground-level perspective, our work
paves the way for a faster, finer-grained damage assessment in the
post-earthquake scenario.

</details>


### [126] [Inpaint360GS: Efficient Object-Aware 3D Inpainting via Gaussian Splatting for 360° Scenes](https://arxiv.org/abs/2511.06457)
*Shaoxiang Wang,Shihong Zhang,Christen Millerdurai,Rüdiger Westermann,Didier Stricker,Alain Pagani*

Main category: cs.CV

TL;DR: 本文提出了一种用于360度全景场景3D修复的新方法Inpaint360GS，能够支持多目标移除和高质量填补，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 虽然近年来NeRF和3D Gaussian Splatting(3DGS)在单一目标前视方向修复任务中已取得重要进展，但针对更复杂的360度全景多目标场景的3D修复问题仍面临巨大挑战，如目标识别、严重遮挡和多视角一致性不足。

Method: 提出Inpaint360GS框架，基于3DGS，结合2D分割向3D场景知识蒸馏，利用虚拟相机辅助判断目标和提供上下文信息，实现多目标精确编辑与场景补全。同时，构建针对该任务的数据集。

Result: 实验表明，Inpaint360GS在多目标全景3D修复任务上，在准确性、一致性和质量方面均超越了现有主流方法，达到了新的state-of-the-art。

Conclusion: Inpaint360GS有效解决了360度场景下的多目标3D修复难题，并推动了360度场景编辑任务的前沿发展。

Abstract: Despite recent advances in single-object front-facing inpainting using NeRF
and 3D Gaussian Splatting (3DGS), inpainting in complex 360{\deg} scenes
remains largely underexplored. This is primarily due to three key challenges:
(i) identifying target objects in the 3D field of 360{\deg} environments, (ii)
dealing with severe occlusions in multi-object scenes, which makes it hard to
define regions to inpaint, and (iii) maintaining consistent and high-quality
appearance across views effectively. To tackle these challenges, we propose
Inpaint360GS, a flexible 360{\deg} editing framework based on 3DGS that
supports multi-object removal and high-fidelity inpainting in 3D space. By
distilling 2D segmentation into 3D and leveraging virtual camera views for
contextual guidance, our method enables accurate object-level editing and
consistent scene completion. We further introduce a new dataset tailored for
360{\deg} inpainting, addressing the lack of ground truth object-free scenes.
Experiments demonstrate that Inpaint360GS outperforms existing baselines and
achieves state-of-the-art performance. Project page:
https://dfki-av.github.io/inpaint360gs/

</details>


### [127] [NOAH: Benchmarking Narrative Prior driven Hallucination and Omission in Video Large Language Models](https://arxiv.org/abs/2511.06475)
*Kyuho Lee,Euntae Kim,Jinwoo Choi,Buru Chang*

Main category: cs.CV

TL;DR: 本文发现视频大语言模型（Video LLMs）过于注重叙事连贯性会引发“幻觉”和“遗漏”两类错误，并提出了大规模评测基准NOAH系统化研究该问题。


<details>
  <summary>Details</summary>
Motivation: 目前视频大语言模型在生成字幕、摘要和问答等任务上表现强劲，多通过增强事件间的叙事连贯来优化输出。然而，这一做法引入了“叙事优先”偏置，影响模型严格依据视觉证据作答，容易产生内容幻觉或事实遗漏。亟需一种系统化方法评测和理解该偏置带来的具体影响。

Method: 作者提出了NOAH（Narrative Omissions And Hallucinations）大规模基准数据集，通过在目标视频中插入不同语义相似度的外部片段，系统化地扰动叙事结构，并控制插入位置以分析模型应对处理能力。基准包括一个字幕生成任务和三个问答任务（存在性、时序性、叙事性），并设计了配套评价指标，总共产生超6万条样本，用于量化不同模型的叙事偏置错误表现。

Result: 实验表明：1）主流视频大语言模型普遍存在由叙事优先导致的幻觉与遗漏现象；2）不同模型架构对于此类错误的易感性不同，且受事件插入的语义相似度与位置影响；3）当帧采样密度降低或事件连续性变弱时，模型对叙事连贯的依赖增强，相关错误显著放大。

Conclusion: NOAH首次为视频大语言模型的叙事优先导致的幻觉与遗漏问题建立了规范化评测框架，有助于后续研究设计更可靠、可信的多模态模型。NOAH数据集和代码已开源，推动领域发展。

Abstract: Video large language models (Video LLMs) have recently achieved strong
performance on tasks such as captioning, summarization, and question answering.
Many models and training methods explicitly encourage continuity across events
to enhance narrative coherence. While this improves fluency, it also introduces
an inductive bias that prioritizes storyline consistency over strict grounding
in visual evidence. We identify this bias, which we call narrative prior, as a
key driver of two errors: hallucinations, where non-existent events are
introduced or existing ones are misinterpreted, and omissions, where factual
events are suppressed because they are misaligned with surrounding context. To
systematically evaluate narrative prior-induced errors, we introduce NOAH, a
large-scale benchmark that constructs composite videos by inserting clips from
other sources into target videos. By varying semantic similarity and insertion
position, our benchmark enables controlled and scalable analysis of narrative
priors. We design one captioning task with tailored metrics and three QA tasks
- Existence, Temporal, and Narrative - yielding more than 60K evaluation
samples. Extensive experiments yield three key findings: (i) most Video LLMs
exhibit hallucinations and omissions driven by narrative priors, (ii) the
patterns of these errors vary across architectures and depend on event
similarity and insertion position, and (iii) reliance on narrative priors
intensifies under sampling with fewer frames, amplifying errors when event
continuity is weak. We establish NOAH as the first standardized evaluation of
narrative prior-induced hallucination and omission in Video LLMs, providing a
foundation for developing more reliable and trustworthy models. Our benchmark
and code are available at https://anonymous550520.github.io/.

</details>


### [128] [Zooming into Comics: Region-Aware RL Improves Fine-Grained Comic Understanding in Vision-Language Models](https://arxiv.org/abs/2511.06490)
*Yule Chen,Yufan Ren,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 本文提出并发布了一个面向漫画视觉-语言模型的细粒度基准AI4VA-FG，并通过不同的后训练策略提升模型的漫画理解能力，验证了新方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的视觉-语言模型（VLMs）在处理自然图像任务上表现优异，但它们在漫画等复杂视觉叙事（如线稿、拟声词、多面板排版等）上的表现很差。因此，缺乏适合VLM漫画理解评价的公开基准和提升方法。

Method: 1）设计并构建了AI4VA-FG基准数据集，涵盖识别、检测、角色推理和叙事生成等任务，并附有丰富的标注；2）全面评测业界主流专有（GPT-4o、Gemini-2.5）与开源（Qwen2.5-VL）模型在该基准的表现；3）尝试多种后训练策略：解题式SFT（SFT-S）、推理轨迹SFT（SFT-R）、强化学习（RL），以及创新性提出了区域感知强化学习（RARL），让模型通过“放大”机制聚焦关键区域。

Result: 现有视觉语言模型在漫画理解各核心任务上的表现显著不足。RL和RARL方法应用到Qwen2.5-VL模型后，在实体识别和故事排序等低层及高层任务上都取得了明显提升。

Conclusion: 本工作首次提供详细基准推进VLM在漫画领域的能力检验，通过各种后训练方法（尤其是RARL）显著改进了模型性能，为漫画理解及相关应用打下基础和方向。

Abstract: Complex visual narratives, such as comics, present a significant challenge to
Vision-Language Models (VLMs). Despite excelling on natural images, VLMs often
struggle with stylized line art, onomatopoeia, and densely packed multi-panel
layouts. To address this gap, we introduce AI4VA-FG, the first fine-grained and
comprehensive benchmark for VLM-based comic understanding. It spans tasks from
foundational recognition and detection to high-level character reasoning and
narrative construction, supported by dense annotations for characters, poses,
and depth. Beyond that, we evaluate state-of-the-art proprietary models,
including GPT-4o and Gemini-2.5, and open-source models such as Qwen2.5-VL,
revealing substantial performance deficits across core tasks of our benchmarks
and underscoring that comic understanding remains an unsolved challenge. To
enhance VLMs' capabilities in this domain, we systematically investigate
post-training strategies, including supervised fine-tuning on solutions
(SFT-S), supervised fine-tuning on reasoning trajectories (SFT-R), and
reinforcement learning (RL). Beyond that, inspired by the emerging "Thinking
with Images" paradigm, we propose Region-Aware Reinforcement Learning (RARL)
for VLMs, which trains models to dynamically attend to relevant regions through
zoom-in operations. We observe that when applied to the Qwen2.5-VL model, RL
and RARL yield significant gains in low-level entity recognition and high-level
storyline ordering, paving the way for more accurate and efficient VLM
applications in the comics domain.

</details>


### [129] [SportR: A Benchmark for Multimodal Large Language Model Reasoning in Sports](https://arxiv.org/abs/2511.06499)
*Haotian Xia,Haonan Ge,Junbo Zou,Hyun Woo Choi,Xuebin Zhang,Danny Suradja,Botao Rui,Ethan Tran,Wendy Jin,Zhen Ye,Xiyang Lin,Christopher Lai,Shengjie Zhang,Junwen Miao,Shichao Chen,Rhys Tracy,Vicente Ordonez,Weining Shen,Hanjie Chen*

Main category: cs.CV

TL;DR: 本文提出并发布了SportR，这是第一个面向多种体育项目的大规模基准，用于训练和评估多模态大模型在体育智能推理领域的能力。基准包括超过5000张图片和2000段视频，并针对推理深度设计了多层次的问题-答案链，且附有人类标注的思维链解释。实验显示现有主流模型在数据集的高阶任务上表现很差，即便强化学习和有监督微调后也难以缩小差距。


<details>
  <summary>Details</summary>
Motivation: 当前体育领域的多模态挑战在于模型需要具备精细视觉感知、抽象规则理解与视觉证据结合的推理能力。以往基准仅覆盖单一运动，且缺乏多步推理链与视觉定位的细致测评，难以全方位评价模型核心能力。本文通过构建多体育项目且层次丰富的评测体系，旨在填补现有基准的空白，推动多模态体育智能技术发展。

Method: 作者提出SportR多体育基准，包含5017张图片与2101段视频，辅以手工标注的图片框，实现精细视觉定位。问题-答案设计上自浅入深，涵盖违规识别、复杂判罚预测、战术解释等多步推理能力测试，特别提供了7118条高质量人类撰写的推理链注释（CoT）。同时构建训练与验证流程，采用有监督微调和强化学习提升模型表现。

Result: 实验结果表明，当前最先进多模态大模型无论直接测试还是在SportR集上定向训练，面对高阶复杂推理任务（如判罚解释、战术推演）均表现不佳，得分显著低于初级任务，凸显现有方法的显著不足。

Conclusion: SportR作为全新大规模多体育多模态推理基准，为学界提供了更具挑战性与细致度的测试平台。试验证明现有多模态模型距离体育智能推理高度仍有较大提升空间。该基准将有助于推动未来相关领域的方法创新与性能进步。

Abstract: Deeply understanding sports requires an intricate blend of fine-grained
visual perception and rule-based reasoning - a challenge that pushes the limits
of current multimodal models. To succeed, models must master three critical
capabilities: perceiving nuanced visual details, applying abstract sport rule
knowledge, and grounding that knowledge in specific visual evidence. Current
sports benchmarks either cover single sports or lack the detailed reasoning
chains and precise visual grounding needed to robustly evaluate these core
capabilities in a multi-sport context. To address this gap, we introduce
SportR, the first multi-sports large-scale benchmark designed to train and
evaluate MLLMs on the fundamental reasoning required for sports intelligence.
Our benchmark provides a dataset of 5,017 images and 2,101 videos. To enable
granular evaluation, we structure our benchmark around a progressive hierarchy
of question-answer (QA) pairs designed to probe reasoning at increasing depths
- from simple infraction identification to complex penalty prediction. For the
most advanced tasks requiring multi-step reasoning, such as determining
penalties or explaining tactics, we provide 7,118 high-quality, human-authored
Chain of Thought (CoT) annotations. In addition, our benchmark incorporates
both image and video modalities and provides manual bounding box annotations to
test visual grounding in the image part directly. Extensive experiments
demonstrate the profound difficulty of our benchmark. State-of-the-art baseline
models perform poorly on our most challenging tasks. While training on our data
via Supervised Fine-Tuning and Reinforcement Learning improves these scores,
they remain relatively low, highlighting a significant gap in current model
capabilities. SportR presents a new challenge for the community, providing a
critical resource to drive future research in multimodal sports reasoning.

</details>


### [130] [Video Dataset for Surgical Phase, Keypoint, and Instrument Recognition in Laparoscopic Surgery (PhaKIR)](https://arxiv.org/abs/2511.06549)
*Tobias Rueckert,Raphaela Maerkl,David Rauber,Leonard Klausmann,Max Gutbrod,Daniel Rueckert,Hubertus Feussner,Dirk Wilhelm,Christoph Palm*

Main category: cs.CV

TL;DR: 本论文介绍了PhaKIR数据集，这是首个联合提供手术阶段标签、手术器械位姿信息和像素级分割标注的多中心腹腔镜手术数据集，并包含完整的手术序列，可用于多种计算机视觉任务。


<details>
  <summary>Details</summary>
Motivation: 以往公开手术影像数据集多仅适用于单一任务，缺乏时间连续性或多机构多样性，限制了基于计算机视觉的手术器械识别与手术流程理解研究。作者希望通过丰富、全面的数据集推动多任务高质量研究，并提升成果的临床适用性。

Method: 作者收集了来自三家医疗中心的8例腹腔镜胆囊切除术全程视频，逐帧标注了手术阶段（共485,875帧）、器械关键点及实例分割（各19,435帧），数据用于支撑多任务挑战（如EndoVis Challenge 2024），并公开发布。

Result: PhaKIR数据集成为首个覆盖多个机构、完整手术流程、多任务且具像素级分割与关键点标注的数据资源，已作为MICCAI 2024相关挑战赛标准数据库，提高了社区对方法的评测和对比的公平性。

Conclusion: PhaKIR数据集极大地推动了外科视觉理解领域，兼顾多任务、多机构和手术全过程，突破了现有数据集的局限，为今后研究提供了高质量、多样化的基准资源。

Abstract: Robotic- and computer-assisted minimally invasive surgery (RAMIS) is
increasingly relying on computer vision methods for reliable instrument
recognition and surgical workflow understanding. Developing such systems often
requires large, well-annotated datasets, but existing resources often address
isolated tasks, neglect temporal dependencies, or lack multi-center
variability. We present the Surgical Procedure Phase, Keypoint, and Instrument
Recognition (PhaKIR) dataset, comprising eight complete laparoscopic
cholecystectomy videos recorded at three medical centers. The dataset provides
frame-level annotations for three interconnected tasks: surgical phase
recognition (485,875 frames), instrument keypoint estimation (19,435 frames),
and instrument instance segmentation (19,435 frames). PhaKIR is, to our
knowledge, the first multi-institutional dataset to jointly provide phase
labels, instrument pose information, and pixel-accurate instrument
segmentations, while also enabling the exploitation of temporal context since
full surgical procedure sequences are available. It served as the basis for the
PhaKIR Challenge as part of the Endoscopic Vision (EndoVis) Challenge at MICCAI
2024 to benchmark methods in surgical scene understanding, thereby further
validating the dataset's quality and relevance. The dataset is publicly
available upon request via the Zenodo platform.

</details>


### [131] [Spatial-Frequency Enhanced Mamba for Multi-Modal Image Fusion](https://arxiv.org/abs/2511.06593)
*Hui Sun,Long Lv,Pingping Zhang,Tongdan Tang,Feng Tian,Weibing Sun,Huchuan Lu*

Main category: cs.CV

TL;DR: 本文提出了一种新型多模态图像融合（MMIF）方法，克服传统CNN和Transformer的局限，通过增强后的Mamba模型在空间和频率域上实现高效特征融合，大幅提升了融合效果。


<details>
  <summary>Details</summary>
Motivation: 传统MMIF方法主要依赖CNN或Transformer提取特征，但存在感受野有限或计算成本高的问题。新兴的Mamba虽高效但空间和频率感知不足，且如何高效利用图像重建作为辅助任务仍是难点。

Method: 提出SFMFusion框架：（1）设计三分支结构联动MMIF与图像重建，保留原图内容。（2）设计空间-频率增强Mamba模块（SFMB），加强空间和频率域特征提取能力。（3）提出动态融合Mamba模块（DFMB），实现多分支间的动态特征融合。

Result: 在六个MMIF公开数据集上，SFMFusion在多个指标上优于大多数现有方法，证明了其有效性和优越性。

Conclusion: SFMFusion通过空间和频率增强，结合动态融合机制，实现了更优的多模态图像融合效果，为MMIF任务提供了新思路和高效的解决方案。

Abstract: Multi-Modal Image Fusion (MMIF) aims to integrate complementary image
information from different modalities to produce informative images. Previous
deep learning-based MMIF methods generally adopt Convolutional Neural Networks
(CNNs) or Transformers for feature extraction. However, these methods deliver
unsatisfactory performances due to the limited receptive field of CNNs and the
high computational cost of Transformers. Recently, Mamba has demonstrated a
powerful potential for modeling long-range dependencies with linear complexity,
providing a promising solution to MMIF. Unfortunately, Mamba lacks full spatial
and frequency perceptions, which are very important for MMIF. Moreover,
employing Image Reconstruction (IR) as an auxiliary task has been proven
beneficial for MMIF. However, a primary challenge is how to leverage IR
efficiently and effectively. To address the above issues, we propose a novel
framework named Spatial-Frequency Enhanced Mamba Fusion (SFMFusion) for MMIF.
More specifically, we first propose a three-branch structure to couple MMIF and
IR, which can retain complete contents from source images. Then, we propose the
Spatial-Frequency Enhanced Mamba Block (SFMB), which can enhance Mamba in both
spatial and frequency domains for comprehensive feature extraction. Finally, we
propose the Dynamic Fusion Mamba Block (DFMB), which can be deployed across
different branches for dynamic feature fusion. Extensive experiments show that
our method achieves better results than most state-of-the-art methods on six
MMIF datasets. The source code is available at
https://github.com/SunHui1216/SFMFusion.

</details>


### [132] [Explainable Cross-Disease Reasoning for Cardiovascular Risk Assessment from LDCT](https://arxiv.org/abs/2511.06625)
*Yifei Zhang,Jiashuo Zhang,Xiaofeng Yang,Liang Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种可解释的交叉疾病推理框架，在低剂量胸部CT（LDCT）中联合评估肺部和心血管健康，实现了基于单次影像数据的心肺风险精准预测，同时还具备医学合理性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前LDCT本可同时获得肺部与心脏结构信息，但绝大多数现有方法将二者割裂，忽视了两者之间的生理相关性及共享影像生物标志物，因此需要一种结合并能解释两者关系的新方法。

Method: 作者提出了一个交叉疾病解释推理框架，包括三大模块：肺部感知模块（分析肺部异常）、知识引导推理模块（推断肺异常对心血管的影响）、心脏表征模块（编码结构性生物标志）。三者输出融合后进行整体性心血管风险预测，实现判读与解释一体化。

Result: 该框架在NLST数据集上的心血管疾病筛查和死亡率预测表现优于单一疾病或仅基于图像的基线方法，在准确率及生理解释性上都取得了SOTA（最优）成绩。

Conclusion: 本文提出了统一且可解释的LDCT心血管分析新范式，成功实现了图像预测与机制解释的融合，推动了心肺联合分析的发展。

Abstract: Low-dose chest computed tomography (LDCT) inherently captures both pulmonary
and cardiac structures, offering a unique opportunity for joint assessment of
lung and cardiovascular health. However, most existing approaches treat these
domains as independent tasks, overlooking their physiological interplay and
shared imaging biomarkers. We propose an Explainable Cross-Disease Reasoning
Framework that enables interpretable cardiopulmonary risk assessment from a
single LDCT scan. The framework introduces an agentic reasoning process that
emulates clinical diagnostic thinking-first perceiving pulmonary findings, then
reasoning through established medical knowledge, and finally deriving a
cardiovascular judgment with explanatory rationale. It integrates three
synergistic components: a pulmonary perception module that summarizes lung
abnormalities, a knowledge-guided reasoning module that infers their
cardiovascular implications, and a cardiac representation module that encodes
structural biomarkers. Their outputs are fused to produce a holistic
cardiovascular risk prediction that is both accurate and physiologically
grounded. Experiments on the NLST cohort demonstrate that the proposed
framework achieves state-of-the-art performance for CVD screening and mortality
prediction, outperforming single-disease and purely image-based baselines.
Beyond quantitative gains, the framework provides human-verifiable reasoning
that aligns with cardiological understanding, revealing coherent links between
pulmonary abnormalities and cardiac stress mechanisms. Overall, this work
establishes a unified and explainable paradigm for cardiovascular analysis from
LDCT, bridging the gap between image-based prediction and mechanism-based
medical interpretation.

</details>


### [133] [DIAL-GS: Dynamic Instance Aware Reconstruction for Label-free Street Scenes with 4D Gaussian Splatting](https://arxiv.org/abs/2511.06632)
*Chenpeng Su,Wenhua Wu,Chensheng Peng,Tianchen Deng,Zhe Liu,Hesheng Wang*

Main category: cs.CV

TL;DR: 本文提出DIAL-GS，一种无需标签的动态实例感知重建方法，能更准确地区分和处理街景中的静态与动态元素，实现高质量的城市场景重建及实例级编辑。


<details>
  <summary>Details</summary>
Motivation: 现有监督式城市场景重建方法依赖昂贵且难以扩展的人工标注，现有自监督方法难以区分静态与动态元素，无法有效定位和编辑单独的动态物体，阻碍了精细的街景建模与仿真。

Method: 提出DIAL-GS方法，利用外观与位置在渲染和实际观测之间的不一致性来精准识别动态实例。基于实例级动态感知，采用实例感知的4D高斯表示，统一建模动态与静态元素。此外，引入‘身份-动态’互补机制，使得动态属性与实例一致性共同提升建模效果。

Result: 在城市自动驾驶场景实验中，DIAL-GS显著优于现有自监督方法，在重建质量和实例级编辑能力上都取得了更好结果。

Conclusion: DIAL-GS为城市街景的高质量、实例级动态建模提供了高效，无需人工标注的解决方案，推进了自动驾驶数据增强和闭环测试的发展。

Abstract: Urban scene reconstruction is critical for autonomous driving, enabling
structured 3D representations for data synthesis and closed-loop testing.
Supervised approaches rely on costly human annotations and lack scalability,
while current self-supervised methods often confuse static and dynamic elements
and fail to distinguish individual dynamic objects, limiting fine-grained
editing. We propose DIAL-GS, a novel dynamic instance-aware reconstruction
method for label-free street scenes with 4D Gaussian Splatting. We first
accurately identify dynamic instances by exploiting appearance-position
inconsistency between warped rendering and actual observation. Guided by
instance-level dynamic perception, we employ instance-aware 4D Gaussians as the
unified volumetric representation, realizing dynamic-adaptive and
instance-aware reconstruction. Furthermore, we introduce a reciprocal mechanism
through which identity and dynamics reinforce each other, enhancing both
integrity and consistency. Experiments on urban driving scenarios show that
DIAL-GS surpasses existing self-supervised baselines in reconstruction quality
and instance-level editing, offering a concise yet powerful solution for urban
scene modeling.

</details>


### [134] [UniADC: A Unified Framework for Anomaly Detection and Classification](https://arxiv.org/abs/2511.06644)
*Ximiao Zhang,Min Xu,Zheng Zhang,Junlin Hu,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: 本论文提出UniADC，一种能够统一检测图像异常区域并分类其类别的新方法，通过训练无关的可控修复网络生成异常样本，并采用多任务判别器提升检测和分类性能，实验结果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将异常检测和分类视为独立任务，忽略了两者间的内在联系，导致信息共享受限，性能不佳。本文旨在突破这一限制，实现异常检测与分类任务的协同优化。

Method: 提出UniADC方法，包含训练无关的可控修复网络和多任务判别器。修复网络基于异常先验控制生成特定类别的异常样本，并对少量异常样本进行补充。多任务判别器基于合成样本训练，实现异常检测与精细分类。

Result: 在MVTec-FS、MTD和WFDD三个数据集上，UniADC在异常检测、定位和分类任务上均取得了比现有方法更好的表现。

Conclusion: UniADC能够有效统一异常检测与分类任务，在几乎没有异常样本的情况下也能取得优异效果，具有良好的应用前景。

Abstract: In this paper, we introduce the task of unified anomaly detection and
classification, which aims to simultaneously detect anomalous regions in images
and identify their specific categories. Existing methods typically treat
anomaly detection and classification as separate tasks, thereby neglecting
their inherent correlation, limiting information sharing, and resulting in
suboptimal performance. To address this, we propose UniADC, a unified anomaly
detection and classification model that can effectively perform both tasks with
only a few or even no anomaly images. Specifically, UniADC consists of two key
components: a training-free controllable inpainting network and a multi-task
discriminator. The inpainting network can synthesize anomaly images of specific
categories by repainting normal regions guided by anomaly priors, and can also
repaint few-shot anomaly samples to augment the available anomaly data. The
multi-task discriminator is then trained on these synthesized samples, enabling
precise anomaly detection and classification by aligning fine-grained image
features with anomaly-category embeddings. We conduct extensive experiments on
three anomaly detection and classification datasets, including MVTec-FS, MTD,
and WFDD, and the results demonstrate that UniADC consistently outperforms
existing methods in anomaly detection, localization, and classification. The
code is available at https://github.com/cnulab/UniADC.

</details>


### [135] [FreqGRL: Suppressing Low-Frequency Bias and Mining High-Frequency Knowledge for Cross-Domain Few-Shot Learning](https://arxiv.org/abs/2511.06648)
*Siqi Hui,Sanping Zhou,Ye deng,Wenli Huang,Jinjun Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于频域分析的新方法（FreqGRL），通过处理源域和目标域数据的频率成分，提升了跨域小样本学习的表现。


<details>
  <summary>Details</summary>
Motivation: 在跨域小样本学习（CD-FSL）任务中，现有方法虽然尝试利用有限的目标域标签数据提升性能，但源域和目标域数据量严重不平衡，导致表示学习面临挑战，尤其难以获得跨域泛化能力强的特征。

Method: 提出了FreqGRL框架，包括低频替换（LFR）模块和高频增强（HFE）模块。LFR将源域数据的低频部分替换为目标域低频成分，缓解源域偏置；HFE过滤低频，直接在高频域做特征学习，增强高频、可泛化的特征。再结合全局频率滤波（GFF）模块，抑制无关频率，突出有效信号，降低过拟合。

Result: 在五个标准的CD-FSL基准上进行了大量实验，所提出的方法在所有指标上都达到了当前最优性能。

Conclusion: 频域视角及相关模块的引入有效缓解了数据不平衡带来的问题，促进了跨域特征的泛化，FreqGRL方法在跨域小样本学习领域展现出显著的效果。

Abstract: Cross-domain few-shot learning (CD-FSL) aims to recognize novel classes with
only a few labeled examples under significant domain shifts. While recent
approaches leverage a limited amount of labeled target-domain data to improve
performance, the severe imbalance between abundant source data and scarce
target data remains a critical challenge for effective representation learning.
We present the first frequency-space perspective to analyze this issue and
identify two key challenges: (1) models are easily biased toward
source-specific knowledge encoded in the low-frequency components of source
data, and (2) the sparsity of target data hinders the learning of
high-frequency, domain-generalizable features. To address these challenges, we
propose \textbf{FreqGRL}, a novel CD-FSL framework that mitigates the impact of
data imbalance in the frequency space. Specifically, we introduce a
Low-Frequency Replacement (LFR) module that substitutes the low-frequency
components of source tasks with those from the target domain to create new
source tasks that better align with target characteristics, thus reducing
source-specific biases and promoting generalizable representation learning. We
further design a High-Frequency Enhancement (HFE) module that filters out
low-frequency components and performs learning directly on high-frequency
features in the frequency space to improve cross-domain generalization.
Additionally, a Global Frequency Filter (GFF) is incorporated to suppress noisy
or irrelevant frequencies and emphasize informative ones, mitigating
overfitting risks under limited target supervision. Extensive experiments on
five standard CD-FSL benchmarks demonstrate that our frequency-guided framework
achieves state-of-the-art performance.

</details>


### [136] [NOVO: Bridging LLaVA and SAM with Visual-only Prompts for Reasoning Segmentation](https://arxiv.org/abs/2511.06651)
*Kyung-Yoon Yoon,Yeong-Jun Cho*

Main category: cs.CV

TL;DR: 文章提出了一种新的视觉分割框架NOVO，完全基于视觉提示并兼容Segment Anything Model，无需文本输入，在多个指标和模型尺度上达到了当前最佳的分割效果。


<details>
  <summary>Details</summary>
Motivation: 目前结合视觉-语言模型与分割模型的方法大多依赖文本输入生成的提示，但这可能限制分割模型的能力与泛化性，因此需要新的方法仅利用视觉信息高效实现分割任务。

Method: NOVO框架绕过文本提示，直接从视觉-语言模型输出中生成粗分割掩码和点状提示（visual prompts），并与SAM模型兼容。此外，提出了免训练的分割细化模块以提升分割边界和实例分割质量，同时还构建了新测试基准RISeg用于充分评估。

Result: 在RISeg等数据集上，NOVO在多项分割指标和不同模型尺寸下均取得了比现有方法更优的效果。

Conclusion: NOVO证明了仅用视觉提示就能高质量地桥接视觉-语言模型和分割模型，提升分割性能并具有良好可扩展性，对视觉理解与分割领域有重要意义。

Abstract: In this study, we propose NOVO (NO text, Visual-Only prompts), a novel
framework that bridges vision-language models (VLMs) and segmentation models
through visual-only prompts. Unlike prior approaches that feed text-derived SEG
token embeddings into segmentation models, NOVO instead generates a coarse mask
and point prompts from the VLM output. These visual prompts are compatible with
the Segment Anything Model (SAM), preserving alignment with its pretrained
capabilities. To further enhance boundary quality and enable instance-level
segmentation, we introduce a training-free refinement module that reduces
visual artifacts and improves the quality of segmentation masks. We also
present RISeg, a new benchmark comprising 918 images, 2,533 instance-level
masks, and diverse reasoning queries to evaluate this task. Experiments
demonstrate that NOVO achieves state-of-the-art performance across multiple
metrics and model sizes, demonstrating its effectiveness and scalability in
reasoning segmentation.

</details>


### [137] [Active Learning for Animal Re-Identification with Ambiguity-Aware Sampling](https://arxiv.org/abs/2511.06658)
*Depanshu Sani,Mehar Khurana,Saket Anand*

Main category: cs.CV

TL;DR: 本文针对动物再识别(Animal Re-ID)任务提出了一种新的主动学习框架，只需极少人工标注，就显著提升了现有基线方法在已知及未知物种上的零样本识别表现。


<details>
  <summary>Details</summary>
Motivation: 动物再识别对于提高生物多样性监测具有重要意义，但因环境变化、物种新颖性、细微区别难以捕捉以及开放集特性导致任务极具挑战。现有的大模型虽可零样本识别，但实验证明其性能有限，而大规模标注代价高昂，现有无监督和主动学习方法在动物Re-ID上表现也不理想。

Method: 提出一种新颖的主动学习(Active Learning)框架，利用互补聚类方法找到嵌入空间内结构模糊区域，有针对性地挖掘代表性、高信息量的样本对，通过采集专家的“必须链接/禁止链接”反馈约束，并通过提出的约束聚类细化算法与现有无监督学习方法整合，简化标注流程同时提升识别效果。

Result: 在13个野生动物数据集上，所提方法用仅0.033%的标注数，平均mAP分别比基础大模型、无监督方法、主动学习方法提升10.49%、11.19%、3.99%，并在所有数据集上达到了最新的性能。在开放世界下（识别未知个体），分别提升11.09%、8.2%、2.06%。

Conclusion: 该框架有效降低了数据标注成本，同时大幅提升了动物Re-ID的零样本识别性能，优于当前主流基础、无监督及主动学习方法，在实际生物多样性监测等任务中具有较强应用价值。

Abstract: Animal Re-ID has recently gained substantial attention in the AI research
community due to its high impact on biodiversity monitoring and unique research
challenges arising from environmental factors. The subtle distinguishing
patterns, handling new species and the inherent open-set nature make the
problem even harder. To address these complexities, foundation models trained
on labeled, large-scale and multi-species animal Re-ID datasets have recently
been introduced to enable zero-shot Re-ID. However, our benchmarking reveals
significant gaps in their zero-shot Re-ID performance for both known and
unknown species. While this highlights the need for collecting labeled data in
new domains, exhaustive annotation for Re-ID is laborious and requires domain
expertise. Our analyses show that existing unsupervised (USL) and AL Re-ID
methods underperform for animal Re-ID. To address these limitations, we
introduce a novel AL Re-ID framework that leverages complementary clustering
methods to uncover and target structurally ambiguous regions in the embedding
space for mining pairs of samples that are both informative and broadly
representative. Oracle feedback on these pairs, in the form of must-link and
cannot-link constraints, facilitates a simple annotation interface, which
naturally integrates with existing USL methods through our proposed constrained
clustering refinement algorithm. Through extensive experiments, we demonstrate
that, by utilizing only 0.033% of all annotations, our approach consistently
outperforms existing foundational, USL and AL baselines. Specifically, we
report an average improvement of 10.49%, 11.19% and 3.99% (mAP) on 13 wildlife
datasets over foundational, USL and AL methods, respectively, while attaining
state-of-the-art performance on each dataset. Furthermore, we also show an
improvement of 11.09%, 8.2% and 2.06% for unknown individuals in an open-world
setting.

</details>


### [138] [Sim4Seg: Boosting Multimodal Multi-disease Medical Diagnosis Segmentation with Region-Aware Vision-Language Similarity Masks](https://arxiv.org/abs/2511.06665)
*Lingran Song,Yucheng Zhou,Jianbing Shen*

Main category: cs.CV

TL;DR: 本文提出了一种新的医学视觉-语言任务 MDS（医疗诊断分割），并引入了 M3DS 数据集，同时提出了 Sim4Seg 框架，有效提升分割与诊断性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割模型很少同时考虑分割与诊断任务，缺乏向患者提供可解释诊断的能力，因此需要新的任务和方法促进模型在分割和诊断上的联合发展。

Method: 作者提出了 MDS（Medical Diagnosis Segmentation）任务，要求模型对于医学图像在理解临床问题后，输出分割掩码和诊断结果。为支持该任务，作者构建了 M3DS 数据集（包含多模态、多疾病样本、分割掩码和诊断推理链），并提出了 Sim4Seg 框架，其中RVLS2M模块将区域感知的视觉-语言相似性映射到掩码。同时，作者在测试阶段加入了可扩展性策略提升整体表现。

Result: 实验结果显示，提出的方法在分割和诊断两方面均优于现有基线方法。

Conclusion: 联合视觉-语言信息可以有效提升医学图像分割及诊断性能，为医疗 AI 系统提供更可解释和精准的结果。

Abstract: Despite significant progress in pixel-level medical image analysis, existing
medical image segmentation models rarely explore medical segmentation and
diagnosis tasks jointly. However, it is crucial for patients that models can
provide explainable diagnoses along with medical segmentation results. In this
paper, we introduce a medical vision-language task named Medical Diagnosis
Segmentation (MDS), which aims to understand clinical queries for medical
images and generate the corresponding segmentation masks as well as diagnostic
results. To facilitate this task, we first present the Multimodal Multi-disease
Medical Diagnosis Segmentation (M3DS) dataset, containing diverse multimodal
multi-disease medical images paired with their corresponding segmentation masks
and diagnosis chain-of-thought, created via an automated diagnosis
chain-of-thought generation pipeline. Moreover, we propose Sim4Seg, a novel
framework that improves the performance of diagnosis segmentation by taking
advantage of the Region-Aware Vision-Language Similarity to Mask (RVLS2M)
module. To improve overall performance, we investigate a test-time scaling
strategy for MDS tasks. Experimental results demonstrate that our method
outperforms the baselines in both segmentation and diagnosis.

</details>


### [139] [REOcc: Camera-Radar Fusion with Radar Feature Enrichment for 3D Occupancy Prediction](https://arxiv.org/abs/2511.06666)
*Chaehee Song,Sanmin Kim,Hyeonjun Jeong,Juyeb Shin,Joonhee Lim,Dongsuk Kum*

Main category: cs.CV

TL;DR: 本文提出了一种名为REOcc的相机-雷达融合网络，通过提升雷达特征的空间密度和质量，显著提升了3D Occupancy预测性能。


<details>
  <summary>Details</summary>
Motivation: 仅依赖摄像头的3D空间占用预测在复杂场景下表现不佳，因此需要融合其他传感器（如雷达），但雷达数据稀疏和噪声大，融合效果有限。

Method: 设计了REOcc网络，并引入了Radar Densifier和Radar Amplifier两个模块，分别用于整合空间与上下文信息，增强雷达特征的密度和质量，从而改善摄像头与雷达的融合表现。

Result: 在Occ3D-nuScenes基准上，REOcc在动态目标类别上取得了明显高于仅用摄像头的基线模型的性能提升，有效减少了雷达数据的稀疏性和噪声影响。

Conclusion: REOcc能更好地实现雷达与相机的互补融合，为稳健可靠的3D Occupancy预测提供了有力手段。

Abstract: Vision-based 3D occupancy prediction has made significant advancements, but
its reliance on cameras alone struggles in challenging environments. This
limitation has driven the adoption of sensor fusion, among which camera-radar
fusion stands out as a promising solution due to their complementary strengths.
However, the sparsity and noise of the radar data limits its effectiveness,
leading to suboptimal fusion performance. In this paper, we propose REOcc, a
novel camera-radar fusion network designed to enrich radar feature
representations for 3D occupancy prediction. Our approach introduces two main
components, a Radar Densifier and a Radar Amplifier, which refine radar
features by integrating spatial and contextual information, effectively
enhancing spatial density and quality. Extensive experiments on the
Occ3D-nuScenes benchmark demonstrate that REOcc achieves significant
performance gains over the camera-only baseline model, particularly in dynamic
object classes. These results underscore REOcc's capability to mitigate the
sparsity and noise of the radar data. Consequently, radar complements camera
data more effectively, unlocking the full potential of camera-radar fusion for
robust and reliable 3D occupancy prediction.

</details>


### [140] [Flexible Concept Bottleneck Model](https://arxiv.org/abs/2511.06678)
*Xingbo Du,Qiantong Dou,Lei Fan,Rui Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种灵活概念瓶颈模型（Flexible Concept Bottleneck Model, FCBM），能够无需重新训练即可灵活适配和替换中间层的人类可解释概念，实现强可迁移性和解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-语言模型（VLM）的概念瓶颈模型（CBM）在添加或替换新概念时通常需要模型整体重训，难以适应实际需求和新模型的快速发展，因此需要有更灵活适配能力的CBM。

Method: 作者提出FCBM，将概念嵌入输入到超网络，超网络输出分类权重，从而实现对于新概念的无缝集成；同时设计了带可学习温度参数的稀疏化模块，实现对最相关概念的动态选择和关注。

Result: 在五个公开数据集上进行大量实验，FCBM以相似的有效概念数量取得与最新方法相当的准确率。此外，FCBM在遇到未见过的新概念时只需微调一个epoch即可达到良好效果。

Conclusion: FCBM具备强的可适应性、灵活性和良好解释性，为实际落地的概念可解释AI提供了更优解。

Abstract: Concept bottleneck models (CBMs) improve neural network interpretability by
introducing an intermediate layer that maps human-understandable concepts to
predictions. Recent work has explored the use of vision-language models (VLMs)
to automate concept selection and annotation. However, existing VLM-based CBMs
typically require full model retraining when new concepts are involved, which
limits their adaptability and flexibility in real-world scenarios, especially
considering the rapid evolution of vision-language foundation models. To
address these issues, we propose Flexible Concept Bottleneck Model (FCBM),
which supports dynamic concept adaptation, including complete replacement of
the original concept set. Specifically, we design a hypernetwork that generates
prediction weights based on concept embeddings, allowing seamless integration
of new concepts without retraining the entire model. In addition, we introduce
a modified sparsemax module with a learnable temperature parameter that
dynamically selects the most relevant concepts, enabling the model to focus on
the most informative features. Extensive experiments on five public benchmarks
demonstrate that our method achieves accuracy comparable to state-of-the-art
baselines with a similar number of effective concepts. Moreover, the model
generalizes well to unseen concepts with just a single epoch of fine-tuning,
demonstrating its strong adaptability and flexibility.

</details>


### [141] [AnoStyler: Text-Driven Localized Anomaly Generation via Lightweight Style Transfer](https://arxiv.org/abs/2511.06687)
*Yulim So,Seokho Kang*

Main category: cs.CV

TL;DR: 提出了一种轻量级且高效的异常生成方法AnoStyler，通过文本引导的风格迁移实现零样本异常生成，提升了异常检测任务中的异常样本合成质量。


<details>
  <summary>Details</summary>
Motivation: 现有异常图像生成方法存在视觉真实感不足、依赖大量真实图像、模型参数量大等问题，限制了实际应用。亟需一种小巧高效且能够生成高质量异常图像的方法。

Method: 提出AnoStyler，将零样本异常生成建模为文本引导的风格迁移任务。以单张正常图像及其标签和缺陷类型为输入，结合类别无关的异常掩码和常/异常状态文本提示，利用轻量级U-Net并结合CLIP损失进行风格化处理，生成视觉真实、语义对齐的异常图像。

Result: 在MVTec-AD和VisA数据集上实验表明，AnoStyler在合成的异常图像的质量和多样性方面优于现有方法，且生成的异常数据可进一步提升异常检测性能。

Conclusion: AnoStyler不仅在生成高质量异常图像方面效果突出，而且通过提高异常检测性能，展示了其实用价值。该方法能以较低成本实现在多场景下的有效异常样本扩增。

Abstract: Anomaly generation has been widely explored to address the scarcity of
anomaly images in real-world data. However, existing methods typically suffer
from at least one of the following limitations, hindering their practical
deployment: (1) lack of visual realism in generated anomalies; (2) dependence
on large amounts of real images; and (3) use of memory-intensive, heavyweight
model architectures. To overcome these limitations, we propose AnoStyler, a
lightweight yet effective method that frames zero-shot anomaly generation as
text-guided style transfer. Given a single normal image along with its category
label and expected defect type, an anomaly mask indicating the localized
anomaly regions and two-class text prompts representing the normal and anomaly
states are generated using generalizable category-agnostic procedures. A
lightweight U-Net model trained with CLIP-based loss functions is used to
stylize the normal image into a visually realistic anomaly image, where
anomalies are localized by the anomaly mask and semantically aligned with the
text prompts. Extensive experiments on the MVTec-AD and VisA datasets show that
AnoStyler outperforms existing anomaly generation methods in generating
high-quality and diverse anomaly images. Furthermore, using these generated
anomalies helps enhance anomaly detection performance.

</details>


### [142] [SPAN: Spatial-Projection Alignment for Monocular 3D Object Detection](https://arxiv.org/abs/2511.06702)
*Yifan Wang,Yian Zhao,Fanqi Pu,Xiaochen Yang,Yang Tang,Xi Chen,Wenming Yang*

Main category: cs.CV

TL;DR: 提出了一种新的单目3D目标检测方法SPAN，通过空间点对齐和3D-2D投影对齐提升几何一致性，实现性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 目前单目3D检测方法通常将3D框的各个属性（如中心、深度、尺寸、朝向）拆分为多个分支分别预测，这虽简化了学习过程，但忽略了这些属性之间的几何协同约束，导致预测结果在几何上不一致，因此实际性能受限。

Method: 论文提出了SPAN方法，包括两个关键组件：（1）空间点对齐，对预测和真实3D框在空间中显式做整体约束，减少单一属性回归导致的空间漂移；（2）3D-2D投影对齐，确保3D框投影紧贴于其对应2D框，修正以往方法忽略的投影失配。此外，提出层次化任务学习策略，在预测逐步精细化时逐步引入空间-投影对齐以保证训练稳定。

Result: 实验表明，SPAN可以无缝集成到现有单目3D检测器中，并带来显著性能提升。

Conclusion: 引入空间点对齐和3D-2D投影对齐，有效提升了单目3D目标检测的几何一致性和整体检测性能，方法具备良好的兼容性和推广性。

Abstract: Existing monocular 3D detectors typically tame the pronounced nonlinear
regression of 3D bounding box through decoupled prediction paradigm, which
employs multiple branches to estimate geometric center, depth, dimensions, and
rotation angle separately. Although this decoupling strategy simplifies the
learning process, it inherently ignores the geometric collaborative constraints
between different attributes, resulting in the lack of geometric consistency
prior, thereby leading to suboptimal performance. To address this issue, we
propose novel Spatial-Projection Alignment (SPAN) with two pivotal components:
(i). Spatial Point Alignment enforces an explicit global spatial constraint
between the predicted and ground-truth 3D bounding boxes, thereby rectifying
spatial drift caused by decoupled attribute regression. (ii). 3D-2D Projection
Alignment ensures that the projected 3D box is aligned tightly within its
corresponding 2D detection bounding box on the image plane, mitigating
projection misalignment overlooked in previous works. To ensure training
stability, we further introduce a Hierarchical Task Learning strategy that
progressively incorporates spatial-projection alignment as 3D attribute
predictions refine, preventing early stage error propagation across attributes.
Extensive experiments demonstrate that the proposed method can be easily
integrated into any established monocular 3D detector and delivers significant
performance improvements.

</details>


### [143] [K-Stain: Keypoint-Driven Correspondence for H&E-to-IHC Virtual Staining](https://arxiv.org/abs/2511.06709)
*Sicheng Yang,Zhaohu Xing,Haipeng Zhou,Lei Zhu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为K-Stain的新框架，通过关键点增强H&E向IHC虚拟染色转换的图像空间一致性和细节，提升合成IHC图像的准确性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有H&E向IHC虚拟染色的方法，常因为组织切片间的错位，难以有效利用空间信息，导致图像合成精度受限。该研究旨在解决空间对齐问题，提高合成IHC图像的结构还原能力和视觉一致性。

Method: 提出K-Stain框架，包括：1) 层次化空间关键点检测器（HSKD）用于在染色图像中检测关键点；2) 关键点感知增强生成器（KEG），在图像生成时融入关键点信息；3) 关键点引导判别器（KGD），提升判别器对空间细节的敏感度。此外，方法利用临近切片的上下文信息，进一步增加图像生成准确性。

Result: 在多个定量指标和视觉质量评估中，该方法均优于现有主流虚拟染色算法，能合成更加精确和逼真的IHC图像。

Conclusion: 通过引入关键点驱动的空间与语义对齐机制，K-Stain能够显著提升虚拟IHC图像的结构一致性和可用性，对虚拟染色技术发展具有推动作用。

Abstract: Virtual staining offers a promising method for converting Hematoxylin and
Eosin (H&E) images into Immunohistochemical (IHC) images, eliminating the need
for costly chemical processes. However, existing methods often struggle to
utilize spatial information effectively due to misalignment in tissue slices.
To overcome this challenge, we leverage keypoints as robust indicators of
spatial correspondence, enabling more precise alignment and integration of
structural details in synthesized IHC images. We introduce K-Stain, a novel
framework that employs keypoint-based spatial and semantic relationships to
enhance synthesized IHC image fidelity. K-Stain comprises three main
components: (1) a Hierarchical Spatial Keypoint Detector (HSKD) for identifying
keypoints in stain images, (2) a Keypoint-aware Enhancement Generator (KEG)
that integrates these keypoints during image generation, and (3) a Keypoint
Guided Discriminator (KGD) that improves the discriminator's sensitivity to
spatial details. Our approach leverages contextual information from adjacent
slices, resulting in more accurate and visually consistent IHC images.
Extensive experiments show that K-Stain outperforms state-of-the-art methods in
quantitative metrics and visual quality.

</details>


### [144] [MirrorMamba: Towards Scalable and Robust Mirror Detection in Videos](https://arxiv.org/abs/2511.06716)
*Rui Song,Jiaying Lin,Rynson W. H. Lau*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频镜像检测方法MirrorMamba，通过结合多种特征和创新网络结构，在多个数据集上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频镜像检测方法受限于特征单一、网络感受野有限或计算复杂度高，导致性能和鲁棒性不足，无法适应复杂多变的镜像场景。

Method: 作者提出MirrorMamba方法，融合了深度信息、对应关系和光流等多种线索。创新性地采用了基于Mamba的多方向对应提取器，有效利用Mamba空间状态模型的全局感受野和线性复杂度，此外还设计了基于Mamba的分层边界解码器以解决深度图模糊导致的边界不清问题。

Result: 在主流视频镜像检测基准数据集上，MirrorMamba显著优于现有方法；在最具挑战性的图像镜像检测数据集上同样取得了SOTA表现，验证了其强大的鲁棒性和泛化性。

Conclusion: MirrorMamba首次将Mamba结构成功应用于镜像检测领域，通过多模态、多尺度融合和高效解码提升了检测效果和泛化能力，为镜像检测提供了新的思路和方法。

Abstract: Video mirror detection has received significant research attention, yet
existing methods suffer from limited performance and robustness. These
approaches often over-rely on single, unreliable dynamic features, and are
typically built on CNNs with limited receptive fields or Transformers with
quadratic computational complexity. To address these limitations, we propose a
new effective and scalable video mirror detection method, called MirrorMamba.
Our approach leverages multiple cues to adapt to diverse conditions,
incorporating perceived depth, correspondence and optical. We also introduce an
innovative Mamba-based Multidirection Correspondence Extractor, which benefits
from the global receptive field and linear complexity of the emerging Mamba
spatial state model to effectively capture correspondence properties.
Additionally, we design a Mamba-based layer-wise boundary enforcement decoder
to resolve the unclear boundary caused by the blurred depth map. Notably, this
work marks the first successful application of the Mamba-based architecture in
the field of mirror detection. Extensive experiments demonstrate that our
method outperforms existing state-of-the-art approaches for video mirror
detection on the benchmark datasets. Furthermore, on the most challenging and
representative image-based mirror detection dataset, our approach achieves
state-of-the-art performance, proving its robustness and generalizability.

</details>


### [145] [MRT: Learning Compact Representations with Mixed RWKV-Transformer for Extreme Image Compression](https://arxiv.org/abs/2511.06717)
*Han Liu,Hengyu Man,Xingtao Wang,Wenrui Li,Debin Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种结合RWKV和Transformer的图像极限压缩新架构MRT，比现有方法更有效地将图像编码为1-D潜变量，显著提高了低比特率重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统基于CNN或Transformer的图像压缩多以2-D潜变量形式编码，存在显著空间冗余，限制了压缩效率。本文旨在突破此瓶颈，通过1-D潜变量提升压缩性能。

Method: 设计混合RWKV-Transformer（MRT）架构，将图像切分为窗口，通过RWKV捕捉全局依赖、Transformer建模窗口内局部冗余，实现1-D层次注意力高效表征。并创新性地引入专用于中间1-D特征的RWKV压缩模型（RCM），进一步提升压缩效率。

Result: 在Kodak和CLIC2020标准图像压缩基准上，大量实验验证了方法有效性。MRT在低于0.02 bpp下具备更优重建质量，DISTS度量下，分别比最强2-D结构GLC节省43.75%、30.59%的码率。

Conclusion: MRT能以更紧凑的1-D潜变量表征, 大幅提升极限压缩下的图像质量和码率效率，是当前极限图像压缩领域的领先方案。

Abstract: Recent advances in extreme image compression have revealed that mapping pixel
data into highly compact latent representations can significantly improve
coding efficiency. However, most existing methods compress images into 2-D
latent spaces via convolutional neural networks (CNNs) or Swin Transformers,
which tend to retain substantial spatial redundancy, thereby limiting overall
compression performance. In this paper, we propose a novel Mixed
RWKV-Transformer (MRT) architecture that encodes images into more compact 1-D
latent representations by synergistically integrating the complementary
strengths of linear-attention-based RWKV and self-attention-based Transformer
models. Specifically, MRT partitions each image into fixed-size windows,
utilizing RWKV modules to capture global dependencies across windows and
Transformer blocks to model local redundancies within each window. The
hierarchical attention mechanism enables more efficient and compact
representation learning in the 1-D domain. To further enhance compression
efficiency, we introduce a dedicated RWKV Compression Model (RCM) tailored to
the structure characteristics of the intermediate 1-D latent features in MRT.
Extensive experiments on standard image compression benchmarks validate the
effectiveness of our approach. The proposed MRT framework consistently achieves
superior reconstruction quality at bitrates below 0.02 bits per pixel (bpp).
Quantitative results based on the DISTS metric show that MRT significantly
outperforms the state-of-the-art 2-D architecture GLC, achieving bitrate
savings of 43.75%, 30.59% on the Kodak and CLIC2020 test datasets,
respectively.

</details>


### [146] [Relative Energy Learning for LiDAR Out-of-Distribution Detection](https://arxiv.org/abs/2511.06720)
*Zizhao Li,Zhengkang Xiang,Jiayang Ao,Joseph West,Kourosh Khoshelham*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的激光雷达点云分布外检测（OOD）方法REL，通过相对能量学习和数据合成有效提升了自动驾驶场景的异常物体识别能力，在实际基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶安全极度依赖对环境异常物体的检测，但现有的OOD检测方法大多集中于2D图像，对于3D激光雷达点云效果不佳，常出现高误报和过度自信的问题。因此，亟需新的3D点云OOD检测方法。

Method: 提出Relative Energy Learning（REL）框架，通过正负类别logits的能量间隙作为相对评分函数，缓解能量原值校准难题。同时创新性地设计Point Raise数据合成方法，通过扰动已知点云产生辅助异常样本，无需改动原有语义，实现无OOD数据训练。

Result: 在SemanticKITTI和Spotting the Unexpected（STU）基准上，REL显著优于现有点云分布外检测方法，尤其在区分罕见异常和普通类别方面，大幅降低了误报率。

Conclusion: 相对能量建模结合简单的异常数据合成策略，为3D点云OOD检测提供了可扩展且鲁棒的解决方案，有助于提升自动驾驶系统的安全性和可靠性。

Abstract: Out-of-distribution (OOD) detection is a critical requirement for reliable
autonomous driving, where safety depends on recognizing road obstacles and
unexpected objects beyond the training distribution. Despite extensive research
on OOD detection in 2D images, direct transfer to 3D LiDAR point clouds has
been proven ineffective. Current LiDAR OOD methods struggle to distinguish rare
anomalies from common classes, leading to high false-positive rates and
overconfident errors in safety-critical settings. We propose Relative Energy
Learning (REL), a simple yet effective framework for OOD detection in LiDAR
point clouds. REL leverages the energy gap between positive (in-distribution)
and negative logits as a relative scoring function, mitigating calibration
issues in raw energy values and improving robustness across various scenes. To
address the absence of OOD samples during training, we propose a lightweight
data synthesis strategy called Point Raise, which perturbs existing point
clouds to generate auxiliary anomalies without altering the inlier semantics.
Evaluated on SemanticKITTI and the Spotting the Unexpected (STU) benchmark, REL
consistently outperforms existing methods by a large margin. Our results
highlight that modeling relative energy, combined with simple synthetic
outliers, provides a principled and scalable solution for reliable OOD
detection in open-world autonomous driving.

</details>


### [147] [AvatarTex: High-Fidelity Facial Texture Reconstruction from Single-Image Stylized Avatars](https://arxiv.org/abs/2511.06721)
*Yuda Qiu,Zitong Xiao,Yiwei Zuo,Zisheng Ye,Weikai Chen,Xiaoguang Han*

Main category: cs.CV

TL;DR: AvatarTex是一种能够从单张图片生成高质量写实和艺术风格头像纹理的重建框架，通过三阶段diffusion-to-GAN流程解决了现有方法在多风格和几何一致性上的难题，并引入了全新的高分辨率多风格纹理数据集TexHub。


<details>
  <summary>Details</summary>
Motivation: 现有面部纹理重建方法在处理多风格（尤其是艺术化）纹理时效果较差，原因在于数据集风格不足且难以保持几何一致性。为此，该文希望提升单张图片驱动的多风格及几何一致的纹理重建质量。

Method: 提出三阶段diffusion-to-GAN流程：首先用扩散模型（diffusion）补全缺失区域，然后用GAN优化确保风格和结构一致，最后再次利用扩散模型细化细节。此外，收集并发布了2万张多风格高分辨率UV纹理的TexHub数据集。

Result: 实验表明，AvatarTex在艺术与几何一致性、细节层次、风格多样性等方面均达到当前最优水平，显著优于已有方法，并依赖于新的TexHub多风格数据集。

Conclusion: AvatarTex通过diffusion与GAN结合、配合TexHub数据集，实现了高保真、多风格的一致面部纹理重建，推动了多风格头像生成领域的发展。TexHub将公开，有助于促进相关研究。

Abstract: We present AvatarTex, a high-fidelity facial texture reconstruction framework
capable of generating both stylized and photorealistic textures from a single
image. Existing methods struggle with stylized avatars due to the lack of
diverse multi-style datasets and challenges in maintaining geometric
consistency in non-standard textures. To address these limitations, AvatarTex
introduces a novel three-stage diffusion-to-GAN pipeline. Our key insight is
that while diffusion models excel at generating diversified textures, they lack
explicit UV constraints, whereas GANs provide a well-structured latent space
that ensures style and topology consistency. By integrating these strengths,
AvatarTex achieves high-quality topology-aligned texture synthesis with both
artistic and geometric coherence. Specifically, our three-stage pipeline first
completes missing texture regions via diffusion-based inpainting, refines style
and structure consistency using GAN-based latent optimization, and enhances
fine details through diffusion-based repainting. To address the need for a
stylized texture dataset, we introduce TexHub, a high-resolution collection of
20,000 multi-style UV textures with precise UV-aligned layouts. By leveraging
TexHub and our structured diffusion-to-GAN pipeline, AvatarTex establishes a
new state-of-the-art in multi-style facial texture reconstruction. TexHub will
be released upon publication to facilitate future research in this field.

</details>


### [148] [Argus: Quality-Aware High-Throughput Text-to-Image Inference Serving System](https://arxiv.org/abs/2511.06724)
*Shubham Agarwal,Subrata Mitra,Saud Iqbal*

Main category: cs.CV

TL;DR: 本文提出了高吞吐量的文本生成图像（T2I）推理系统Argus，通过智能选择不同近似策略，提升了系统的吞吐量与生成质量，并显著减少了延迟SLO违规。


<details>
  <summary>Details</summary>
Motivation: T2I扩散模型在推理阶段计算密集、慢，难以满足高并发和低延迟要求，现有系统难以兼顾效率和生成质量，因此需要新的系统架构来解决该难题。

Method: 作者发现大部分请求可以通过更快速、近似的模型进行推理，但需根据具体请求动态调整近似程度以避免质量损失。Argus系统根据请求内容选择合适的模型与近似参数，通过智能切换多种近似策略，实现质量和效率的平衡。

Result: 在两个真实负载测试中，Argus系统相比主流基线方法获得了10倍的延迟SLO违规减少，平均生成质量提升10%，并实现40%的吞吐量提升。

Conclusion: Argus有效提升了T2I模型在固定资源下的服务能力，实现了高质量、高吞吐量和低延迟的平衡，可作为高并发场景下T2I推理的有效系统方案。

Abstract: Text-to-image (T2I) models have gained significant popularity. Most of these
are diffusion models with unique computational characteristics, distinct from
both traditional small-scale ML models and large language models. They are
highly compute-bound and use an iterative denoising process to generate images,
leading to very high inference time. This creates significant challenges in
designing a high-throughput system. We discovered that a large fraction of
prompts can be served using faster, approximated models. However, the
approximation setting must be carefully calibrated for each prompt to avoid
quality degradation. Designing a high-throughput system that assigns each
prompt to the appropriate model and compatible approximation setting remains a
challenging problem. We present Argus, a high-throughput T2I inference system
that selects the right level of approximation for each prompt to maintain
quality while meeting throughput targets on a fixed-size cluster. Argus
intelligently switches between different approximation strategies to satisfy
both throughput and quality requirements. Overall, Argus achieves 10x fewer
latency service-level objective (SLO) violations, 10% higher average quality,
and 40% higher throughput compared to baselines on two real-world workload
traces.

</details>


### [149] [Rethinking Rainy 3D Scene Reconstruction via Perspective Transforming and Brightness Tuning](https://arxiv.org/abs/2511.06734)
*Qianfeng Yang,Xiang Chen,Pengpeng Li,Qiyuan Guan,Guiyue Jin,Jiyu Jin*

Main category: cs.CV

TL;DR: 本文提出了OmniRain3D数据集和REVR-GSNet框架，以提升雨天条件下三维重建的精度和真实性。


<details>
  <summary>Details</summary>
Motivation: 多视角图像在三维场景重建中非常重要，但雨天导致图像质量下降，进而影响重建效果。现有数据集未能充分反映真实雨天场景中雨丝视角依赖性和降雨导致的亮度变化。

Method: （1）构建OmniRain3D数据集，模拟雨丝在不同视角下的表现及亮度动态变化。（2）提出REVR-GSNet重建框架，融合递归亮度增强、高斯基元优化和基于高斯分布的雨丝消除机制，通过协同交替优化提升雨天条件下的三维重建质量。

Result: 实验结果表明，新数据集和方法可以有效提升雨天条件下的三维场景重建的精度和真实性。

Conclusion: OmniRain3D数据集和REVR-GSNet为多视角图像去雨和雨天三维重建提供了基础，推动了相关研究的发展。

Abstract: Rain degrades the visual quality of multi-view images, which are essential
for 3D scene reconstruction, resulting in inaccurate and incomplete
reconstruction results. Existing datasets often overlook two critical
characteristics of real rainy 3D scenes: the viewpoint-dependent variation in
the appearance of rain streaks caused by their projection onto 2D images, and
the reduction in ambient brightness resulting from cloud coverage during
rainfall. To improve data realism, we construct a new dataset named OmniRain3D
that incorporates perspective heterogeneity and brightness dynamicity, enabling
more faithful simulation of rain degradation in 3D scenes. Based on this
dataset, we propose an end-to-end reconstruction framework named REVR-GSNet
(Rain Elimination and Visibility Recovery for 3D Gaussian Splatting).
Specifically, REVR-GSNet integrates recursive brightness enhancement, Gaussian
primitive optimization, and GS-guided rain elimination into a unified
architecture through joint alternating optimization, achieving high-fidelity
reconstruction of clean 3D scenes from rain-degraded inputs. Extensive
experiments show the effectiveness of our dataset and method. Our dataset and
method provide a foundation for future research on multi-view image deraining
and rainy 3D scene reconstruction.

</details>


### [150] [SinSEMI: A One-Shot Image Generation Model and Data-Efficient Evaluation Framework for Semiconductor Inspection Equipment](https://arxiv.org/abs/2511.06740)
*ChunLiang Wu,Xiaochun Li*

Main category: cs.CV

TL;DR: 本论文提出SinSEMI方法，通过单张光学影像生成多样且高保真的图像，以应对半导体设备开发早期数据稀缺问题，验证其优越性并适合AI训练数据。


<details>
  <summary>Details</summary>
Motivation: 半导体制造初期难以获取大量原始光学图像，导致AI应用受阻，需要能够以少量样本生成大量高质量训练数据的方法。

Method: 提出SinSEMI方法，利用多尺度流模型并结合LPIPS能量引导，实现从单张光学图像生成多样、逼真的新图像。并设计了仅需两张参考图像的评估框架，对生成效果进行全面评估。

Result: 通过与多种现有单样本生成技术对比，SinSEMI在视觉质量、定量指标、下游任务表现上均表现更优。实验表明该方法生成图像兼具高保真和有意义的多样性。

Conclusion: SinSEMI适用于半导体AI应用的数据扩充，能有效缓解数据稀缺问题，并提升AI系统性能。

Abstract: In the early stages of semiconductor equipment development, obtaining large
quantities of raw optical images poses a significant challenge. This data
scarcity hinder the advancement of AI-powered solutions in semiconductor
manufacturing. To address this challenge, we introduce SinSEMI, a novel
one-shot learning approach that generates diverse and highly realistic images
from single optical image. SinSEMI employs a multi-scale flow-based model
enhanced with LPIPS (Learned Perceptual Image Patch Similarity) energy guidance
during sampling, ensuring both perceptual realism and output variety. We also
introduce a comprehensive evaluation framework tailored for this application,
which enables a thorough assessment using just two reference images. Through
the evaluation against multiple one-shot generation techniques, we demonstrate
SinSEMI's superior performance in visual quality, quantitative measures, and
downstream tasks. Our experimental results demonstrate that SinSEMI-generated
images achieve both high fidelity and meaningful diversity, making them
suitable as training data for semiconductor AI applications.

</details>


### [151] [Otter: Mitigating Background Distractions of Wide-Angle Few-Shot Action Recognition with Enhanced RWKV](https://arxiv.org/abs/2511.06741)
*Wenbo Huang,Jinghui Zhang,Zhenghao Chen,Guang Li,Lei Zhang,Yang Cao,Fang Dong,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CV

TL;DR: 本论文提出了一种名为Otter的新方法，针对宽视角下小样本动作识别中的主体突出和时序关系建模难题，取得了最新的性能。


<details>
  <summary>Details</summary>
Motivation: 在宽视角小样本视频动作识别任务中，背景干扰严重、主体难以突出以及时序关系由于类似背景帧而被削弱，导致识别困难。以往的RWKV虽能全局建模，但受背景信息影响，其对主体的突出效果有限，因此需要对该问题进行专门设计。

Method: 作者设计了Otter方法，包括：1）复合分割模块（CSM）对每帧进行关键区域分割，强调主体特征，抑制背景；2）时序重建模块（TRM）通过双向扫描，重构时序关系；3）常规表型与时序增强表型结合，进一步增强主体强调和时序建模能力。

Result: 在SSv2、Kinetics、UCF101和HMDB51等公开数据集上进行了大量实验，Otter方法取得了业内最优的识别准确率；在VideoBadminton宽视角数据集上的额外实验也验证了Otter的优越性。

Conclusion: Otter通过主体分割和时序关系重建，有效解决了宽视角小样本动作识别中面临的背景干扰和时序信息缺失问题，并在多个数据集上展现了卓越性能，具备推广价值。

Abstract: Wide-angle videos in few-shot action recognition (FSAR) effectively express
actions within specific scenarios. However, without a global understanding of
both subjects and background, recognizing actions in such samples remains
challenging because of the background distractions. Receptance Weighted Key
Value (RWKV), which learns interaction between various dimensions, shows
promise for global modeling. While directly applying RWKV to wide-angle FSAR
may fail to highlight subjects due to excessive background information.
Additionally, temporal relation degraded by frames with similar backgrounds is
difficult to reconstruct, further impacting performance. Therefore, we design
the CompOund SegmenTation and Temporal REconstructing RWKV (Otter).
Specifically, the Compound Segmentation Module~(CSM) is devised to segment and
emphasize key patches in each frame, effectively highlighting subjects against
background information. The Temporal Reconstruction Module (TRM) is
incorporated into the temporal-enhanced prototype construction to enable
bidirectional scanning, allowing better reconstruct temporal relation.
Furthermore, a regular prototype is combined with the temporal-enhanced
prototype to simultaneously enhance subject emphasis and temporal modeling,
improving wide-angle FSAR performance. Extensive experiments on benchmarks such
as SSv2, Kinetics, UCF101, and HMDB51 demonstrate that Otter achieves
state-of-the-art performance. Extra evaluation on the VideoBadminton dataset
further validates the superiority of Otter in wide-angle FSAR.

</details>


### [152] [PointCubeNet: 3D Part-level Reasoning with 3x3x3 Point Cloud Blocks](https://arxiv.org/abs/2511.06744)
*Da-Yeong Kim,Yeong-Jun Cho*

Main category: cs.CV

TL;DR: 本文提出了PointCubeNet，一种无需部件标注即可实现部件级推理的多模态3D理解框架，首次在无监督条件下实现了3D部件级推理，并取得了可靠和有意义的效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法进行3D物体分析时常常需要部件级标注，获取标注代价高昂且效率低。本文旨在解决没有部件标注情况下的3D部件级理解问题，提高3D感知能力并降低人工成本。

Method: PointCubeNet包含全局和局部分支。局部分支将点云划分为3x3x3的局部块，对应局部文本标签，实现点云子区域的部件级分析。通过伪标签生成方法和特定的局部损失函数，无需真实部件标注实现了有效训练。

Result: 实验结果表明，PointCubeNet能够有效提升3D物体整体的理解能力，并首次在无监督条件下实现了可靠的3D部件级推理，获得有意义的分割和理解。

Conclusion: 无需部件标注的PointCubeNet能够实现多模态3D部件级推理，为今后无监督3D理解方法提供了新思路，并验证了部件级理解对整体3D认知的积极作用。

Abstract: In this paper, we propose PointCubeNet, a novel multi-modal 3D understanding
framework that achieves part-level reasoning without requiring any part
annotations. PointCubeNet comprises global and local branches. The proposed
local branch, structured into 3x3x3 local blocks, enables part-level analysis
of point cloud sub-regions with the corresponding local text labels. Leveraging
the proposed pseudo-labeling method and local loss function, PointCubeNet is
effectively trained in an unsupervised manner. The experimental results
demonstrate that understanding 3D object parts enhances the understanding of
the overall 3D object. In addition, this is the first attempt to perform
unsupervised 3D part-level reasoning and achieves reliable and meaningful
results.

</details>


### [153] [Image Restoration via Primal Dual Hybrid Gradient and Flow Generative Model](https://arxiv.org/abs/2511.06748)
*Ji Li,Chao Wang*

Main category: cs.CV

TL;DR: 本文提出了一种结合流匹配生成模型的高效Plug-and-Play算法，适用于更广泛的成像反问题，可选择不同的损失函数以适应不同噪声类型，并在多种成像任务中验证了优越性。


<details>
  <summary>Details</summary>
Motivation: 传统的成像反问题优化常依赖规则化项来约束未知图像的先验属性。尽管近年来生成模型在此类问题中表现出强大建模能力，现有PnP方法多仅适用于带高斯噪声的$\ell_2$光滑数据保真项，对于更一般的、尤其是非高斯噪声对应的数据保真项适用性有限。本文旨在扩展PnP框架，使其适用于更广泛的数据保真项和噪声类型。

Method: 作者提出基于流匹配生成模型的PnP方法，并将生成先验引入基于近端分裂的PnP框架，将规则化项的近端算子替换为生成模型推导出的时间相关去噪器。方法上借鉴了Primal-Dual Hybrid Gradient (PDHG)方法，兼顾计算效率和内存友好，支持$\ell_1$、$\ell_2$等多种损失范数，能适应泊松和脉冲噪声等非高斯噪声。

Result: 实验验证了所提方法在去噪、超分辨、去模糊和修复等多种图像修复任务上的有效性，尤其在存在非高斯噪声时，$\ell_1$、$\ell_2$保真项相比传统的平方$\ell_2$损失表现更优。

Conclusion: 本文提出的泛化PnP算法不仅可高效处理多种成像反问题，还能通过选择不同保真项增强对各种噪声类型的鲁棒性，为生成模型与成像反问题的结合提供了更灵活有效的解决思路。

Abstract: Regularized optimization has been a classical approach to solving imaging
inverse problems, where the regularization term enforces desirable properties
of the unknown image. Recently, the integration of flow matching generative
models into image restoration has garnered significant attention, owing to
their powerful prior modeling capabilities. In this work, we incorporate such
generative priors into a Plug-and-Play (PnP) framework based on proximal
splitting, where the proximal operator associated with the regularizer is
replaced by a time-dependent denoiser derived from the generative model. While
existing PnP methods have achieved notable success in inverse problems with
smooth squared $\ell_2$ data fidelity--typically associated with Gaussian
noise--their applicability to more general data fidelity terms remains
underexplored. To address this, we propose a general and efficient PnP
algorithm inspired by the primal-dual hybrid gradient (PDHG) method. Our
approach is computationally efficient, memory-friendly, and accommodates a wide
range of fidelity terms. In particular, it supports both $\ell_1$ and $\ell_2$
norm-based losses, enabling robustness to non-Gaussian noise types such as
Poisson and impulse noise. We validate our method on several image restoration
tasks, including denoising, super-resolution, deblurring, and inpainting, and
demonstrate that $\ell_1$ and $\ell_2$ fidelity terms outperform the
conventional squared $\ell_2$ loss in the presence of non-Gaussian noise.

</details>


### [154] [Med-SORA: Symptom to Organ Reasoning in Abdomen CT Images](https://arxiv.org/abs/2511.06752)
*You-Kyoung Na,Yeong-Jun Cho*

Main category: cs.CV

TL;DR: 该论文提出Med-SORA，一种针对腹部CT图像的症状-器官推理多模态模型，能够处理一对多症状器官映射并融合2D-3D特征，在临床推理任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有医学多模态模型大多采用一对一硬标签，忽视了现实中症状与多个器官相关的复杂关系，并且主要用2D图像特征，缺乏3D全局解剖信息，导致推理能力有限。

Method: 作者提出Med-SORA：1）用基于RAG的方法构建数据集；2）引入带可学习器官锚点的软标签，建模一对多症状-器官关系；3）设计2D-3D跨注意力结构，融合局部与全局图像特征。

Result: 实验表明，Med-SORA在症状-器官推理和准确实现3D临床推理方面，性能优于主流医学多模态模型。

Conclusion: Med-SORA首次系统性解决了医学多模态学习中症状-器官推理问题，为更真实、精确的临床决策提供了有力工具。

Abstract: Understanding symptom-image associations is crucial for clinical reasoning.
However, existing medical multimodal models often rely on simple one-to-one
hard labeling, oversimplifying clinical reality where symptoms relate to
multiple organs. In addition, they mainly use single-slice 2D features without
incorporating 3D information, limiting their ability to capture full anatomical
context. In this study, we propose Med-SORA, a framework for symptom-to-organ
reasoning in abdominal CT images. Med-SORA introduces RAG-based dataset
construction, soft labeling with learnable organ anchors to capture one-to-many
symptom-organ relationships, and a 2D-3D cross-attention architecture to fuse
local and global image features. To our knowledge, this is the first work to
address symptom-to-organ reasoning in medical multimodal learning. Experimental
results show that Med-SORA outperforms existing medical multimodal models and
enables accurate 3D clinical reasoning.

</details>


### [155] [CAST-LUT: Tokenizer-Guided HSV Look-Up Tables for Purple Flare Removal](https://arxiv.org/abs/2511.06764)
*Pu Wang,Shuning Sun,Jialang Lu,Chen Wu,Zhihua Zhang,Youshan Zhang,Chenggang Shan,Dianjie Lu,Guijuan Zhang,Zhuoran Zheng*

Main category: cs.CV

TL;DR: 本论文针对图像高光区域常见的紫晕（Purple flare）色差伪影问题，提出了一种基于HSV解耦查找表（LUT）的深度学习方法，并建立了首个大规模紫晕数据集，实验效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 紫晕伪影严重影响图像的色调和颜色表现，现有的传统方法依赖手工特征和固定先验，灵活性差；深度学习方法则因缺乏配对数据难以训练，因此亟需灵活且高效的紫晕去除方法和数据集支持。

Method: 作者构建了一种基于解耦HSV查找表的两阶段网络结构。第一阶段用色度感知谱标记器（CAST）将RGB图像转换到HSV空间，独立提取H和V通道的语义标记以刻画紫晕状态。第二阶段，HSV-LUT模块以这些标记为输入，为H、S、V三通道独立生成动态校正曲线（1D-LUTs），实现色彩分量的独立矫正。此外，论文还建立了大规模紫晕数据集，并设计了特定的评价指标和损失函数。

Result: 在大规模实验中，该方法无论是在视觉效果还是各项定量指标上均显著优于现有方法，取得了最新最优（state-of-the-art）表现。

Conclusion: 本文提出的基于解耦HSV-LUT的网络能有效解决传统方法的色彩耦合问题，显著提升了紫晕去除的效果，并为相关研究提供了有力数据和工具支持。

Abstract: Purple flare, a diffuse chromatic aberration artifact commonly found around
highlight areas, severely degrades the tone transition and color of the image.
Existing traditional methods are based on hand-crafted features, which lack
flexibility and rely entirely on fixed priors, while the scarcity of paired
training data critically hampers deep learning. To address this issue, we
propose a novel network built upon decoupled HSV Look-Up Tables (LUTs). The
method aims to simplify color correction by adjusting the Hue (H), Saturation
(S), and Value (V) components independently. This approach resolves the
inherent color coupling problems in traditional methods. Our model adopts a
two-stage architecture: First, a Chroma-Aware Spectral Tokenizer (CAST)
converts the input image from RGB space to HSV space and independently encodes
the Hue (H) and Value (V) channels into a set of semantic tokens describing the
Purple flare status; second, the HSV-LUT module takes these tokens as input and
dynamically generates independent correction curves (1D-LUTs) for the three
channels H, S, and V. To effectively train and validate our model, we built the
first large-scale purple flare dataset with diverse scenes. We also proposed
new metrics and a loss function specifically designed for this task. Extensive
experiments demonstrate that our model not only significantly outperforms
existing methods in visual effects but also achieves state-of-the-art
performance on all quantitative metrics.

</details>


### [156] [Robust and High-Fidelity 3D Gaussian Splatting: Fusing Pose Priors and Geometry Constraints for Texture-Deficient Outdoor Scenes](https://arxiv.org/abs/2511.06765)
*Meijun Guo,Yongliang Shi,Caiyun Liu,Yixiao Feng,Ming Ma,Tinghai Yan,Weining Lu,Bin Liang*

Main category: cs.CV

TL;DR: 该论文针对大规模室外场景中弱纹理或重复纹理导致的三维高斯投影（3DGS）渲染不稳定和重建失真问题，提出结合LiDAR-IMU先验与三维高斯约束的方法，显著提升了相机定位和场景表达的鲁棒性与精度。


<details>
  <summary>Details</summary>
Motivation: 在大规模、弱纹理或重复纹理的户外场景下，现有3DGS方法面临相机位姿估计不稳定和场景表达失真等难题，影响数字资产的高质量创建。作者希望克服这些瓶颈，提高3DGS的效果与实用性。

Method: 1）相机位姿估计方面，引入LiDAR-IMU里程计获取先验位姿，并结合COLMAP的三角化和束调整优化，提高位姿精度和鲁棒性。2）场景表示方面，提出法向量约束和有效秩正则化，强制高斯基元的方向和形状保持一致，与光度损失联合优化，从而增强表达质量。

Result: 实验在公开及自采数据集上进行。位姿估计算法只用原方法三分之一时间，且保持精度和鲁棒性；场景表达在弱/重复纹理数据上大幅超越传统3DGS，呈现更优可视化效果和整体表现。

Conclusion: 本文方法能高效且稳健地处理大规模弱纹理场景，实现更高质量的3DGS渲染和表达，对数字资产创建具有重要实际价值。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a key rendering pipeline for
digital asset creation due to its balance between efficiency and visual
quality. To address the issues of unstable pose estimation and scene
representation distortion caused by geometric texture inconsistency in large
outdoor scenes with weak or repetitive textures, we approach the problem from
two aspects: pose estimation and scene representation. For pose estimation, we
leverage LiDAR-IMU Odometry to provide prior poses for cameras in large-scale
environments. These prior pose constraints are incorporated into COLMAP's
triangulation process, with pose optimization performed via bundle adjustment.
Ensuring consistency between pixel data association and prior poses helps
maintain both robustness and accuracy. For scene representation, we introduce
normal vector constraints and effective rank regularization to enforce
consistency in the direction and shape of Gaussian primitives. These
constraints are jointly optimized with the existing photometric loss to enhance
the map quality. We evaluate our approach using both public and self-collected
datasets. In terms of pose optimization, our method requires only one-third of
the time while maintaining accuracy and robustness across both datasets. In
terms of scene representation, the results show that our method significantly
outperforms conventional 3DGS pipelines. Notably, on self-collected datasets
characterized by weak or repetitive textures, our approach demonstrates
enhanced visualization capabilities and achieves superior overall performance.
Codes and data will be publicly available at
https://github.com/justinyeah/normal_shape.git.

</details>


### [157] [ConeGS: Error-Guided Densification Using Pixel Cones for Improved Reconstruction with Fewer Primitives](https://arxiv.org/abs/2511.06810)
*Bartłomiej Baranowski,Stefano Esposito,Patricia Gschoßmann,Anpei Chen,Andreas Geiger*

Main category: cs.CV

TL;DR: 本文提出了ConeGS方法，通过图像空间辅助的加密策略，提升了3D Gaussian Splatting技术在新视角合成中的效果，尤其是在稀疏原始体预算（primitive budget）下的重建质量和渲染效率。


<details>
  <summary>Details</summary>
Motivation: 现有的3DGS技术在加密时主要依赖于现有几何体，通过克隆传播高斯原始体，导致场景覆盖效率低下，需要大量原始体才能达到较好效果。本文旨在解决这一空间分布次优及低效问题。

Method: 提出ConeGS方法，先利用iNGP网络快速重建场景获得像素级深度作为几何代理，然后在3DGS优化阶段检测高误差像素，并在其视锥对应深度插入新高斯原始体，初始化其大小。同时引入不透明度惩罚和原始体预算控制，动态或固定管理原始体数量，高效剔除冗余原始体。

Result: 实验表明，在不同的原始体预算下，ConeGS方法都能提升重建质量和渲染速度，特别是在原始体数量受限时优势更加明显。

Conclusion: ConeGS通过引入图像空间信息和高效的密度分配策略，有效优化了高斯原始体分布，提高了3DGS的重建性能和可扩展性。

Abstract: 3D Gaussian Splatting (3DGS) achieves state-of-the-art image quality and
real-time performance in novel view synthesis but often suffers from a
suboptimal spatial distribution of primitives. This issue stems from
cloning-based densification, which propagates Gaussians along existing
geometry, limiting exploration and requiring many primitives to adequately
cover the scene. We present ConeGS, an image-space-informed densification
framework that is independent of existing scene geometry state. ConeGS first
creates a fast Instant Neural Graphics Primitives (iNGP) reconstruction as a
geometric proxy to estimate per-pixel depth. During the subsequent 3DGS
optimization, it identifies high-error pixels and inserts new Gaussians along
the corresponding viewing cones at the predicted depth values, initializing
their size according to the cone diameter. A pre-activation opacity penalty
rapidly removes redundant Gaussians, while a primitive budgeting strategy
controls the total number of primitives, either by a fixed budget or by
adapting to scene complexity, ensuring high reconstruction quality. Experiments
show that ConeGS consistently enhances reconstruction quality and rendering
performance across Gaussian budgets, with especially strong gains under tight
primitive constraints where efficient placement is crucial.

</details>


### [158] [TiS-TSL: Image-Label Supervised Surgical Video Stereo Matching via Time-Switchable Teacher-Student Learning](https://arxiv.org/abs/2511.06817)
*Rui Wang,Ying Zhou,Hao Wang,Wenwei Zhang,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的时序可切换师生学习(TiS-TSL)框架，以解决微创手术中立体匹配的稀疏标注与时空一致性问题。实验表明，TiS-TSL在两个公开数据集上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在微创手术中，立体匹配对导航和增强现实至关重要。然而，因解剖结构限制，难以获得稠密标注，现有师生学习方法仅提供空间置信度，缺乏时序一致性，导致预测不稳定和明显闪烁问题。

Method: 文中提出了TiS-TSL，一种集成三种模式（图像预测、正向视频预测、反向视频预测）的统一模型，并采用两阶段学习策略：图像到视频(I2V)将稀疏图像知识用于初始化时序建模，视频到视频(V2V)通过正反向预测比较，实现双向时空一致性，筛选噪声标签并增强时序连贯性。

Result: 在公开手术数据集上，TiS-TSL方法显著提升了TEPE和EPE指标，分别至少提高2.11%和4.54%，超越了现有的基于图像的方法。

Conclusion: TiS-TSL充分利用图像和视频级稀疏标注，实现稠密、稳定、时序一致的立体匹配预测，有效提升了微创手术场景下的导航和增强现实能力。

Abstract: Stereo matching in minimally invasive surgery (MIS) is essential for
next-generation navigation and augmented reality. Yet, dense disparity
supervision is nearly impossible due to anatomical constraints, typically
limiting annotations to only a few image-level labels acquired before the
endoscope enters deep body cavities. Teacher-Student Learning (TSL) offers a
promising solution by leveraging a teacher trained on sparse labels to generate
pseudo labels and associated confidence maps from abundant unlabeled surgical
videos. However, existing TSL methods are confined to image-level supervision,
providing only spatial confidence and lacking temporal consistency estimation.
This absence of spatio-temporal reliability results in unstable disparity
predictions and severe flickering artifacts across video frames. To overcome
these challenges, we propose TiS-TSL, a novel time-switchable teacher-student
learning framework for video stereo matching under minimal supervision. At its
core is a unified model that operates in three distinct modes: Image-Prediction
(IP), Forward Video-Prediction (FVP), and Backward Video-Prediction (BVP),
enabling flexible temporal modeling within a single architecture. Enabled by
this unified model, TiS-TSL adopts a two-stage learning strategy. The
Image-to-Video (I2V) stage transfers sparse image-level knowledge to initialize
temporal modeling. The subsequent Video-to-Video (V2V) stage refines temporal
disparity predictions by comparing forward and backward predictions to
calculate bidirectional spatio-temporal consistency. This consistency
identifies unreliable regions across frames, filters noisy video-level pseudo
labels, and enforces temporal coherence. Experimental results on two public
datasets demonstrate that TiS-TSL exceeds other image-based state-of-the-arts
by improving TEPE and EPE by at least 2.11% and 4.54%, respectively..

</details>


### [159] [Integrating Reweighted Least Squares with Plug-and-Play Diffusion Priors for Noisy Image Restoration](https://arxiv.org/abs/2511.06823)
*Ji Li,Chao Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于生成扩散先验的可插拔图像恢复框架，能够有效处理包括脉冲噪声在内的各种噪声类型，并在基准数据集上取得了优异的恢复效果。


<details>
  <summary>Details</summary>
Motivation: 现有的可插拔图像恢复方法主要针对高斯噪声，对于非高斯噪声（如脉冲噪声）的研究较少。为提升对更广泛噪声类型的鲁棒处理能力，需要发展能适应多种噪声的恢复框架。

Method: 本文在最大后验（MAP）估计框架下，针对不同噪声模型自适应修改数据保真项。不同于传统的最小二乘损失（仅适用于高斯噪声），作者提出了基于广义高斯尺度混合分布的损失，实现了$ll_q$范数（$0<q\leq2$）保真项。所提出的优化问题通过迭代加权最小二乘（IRLS）方法求解，生成先验的近端步骤则用基于扩散模型的去噪器高效实现。

Result: 实验结果显示，该方法能有效去除非高斯脉冲噪声，在多个基准数据集上恢复性能优于现有方法。

Conclusion: 将生成扩散先验引入可插拔恢复框架并结合适用于多种噪声分布的泛化保真项，能够实现在非高斯噪声下的有效图像恢复，拓展了生成先验在实际复杂噪声条件下的应用。

Abstract: Existing plug-and-play image restoration methods typically employ
off-the-shelf Gaussian denoisers as proximal operators within classical
optimization frameworks based on variable splitting. Recently, denoisers
induced by generative priors have been successfully integrated into regularized
optimization methods for image restoration under Gaussian noise. However, their
application to non-Gaussian noise--such as impulse noise--remains largely
unexplored. In this paper, we propose a plug-and-play image restoration
framework based on generative diffusion priors for robust removal of general
noise types, including impulse noise. Within the maximum a posteriori (MAP)
estimation framework, the data fidelity term is adapted to the specific noise
model. Departing from the conventional least-squares loss used for Gaussian
noise, we introduce a generalized Gaussian scale mixture-based loss, which
approximates a wide range of noise distributions and leads to an $\ell_q$-norm
($0<q\leq2$) fidelity term. This optimization problem is addressed using an
iteratively reweighted least squares (IRLS) approach, wherein the proximal step
involving the generative prior is efficiently performed via a diffusion-based
denoiser. Experimental results on benchmark datasets demonstrate that the
proposed method effectively removes non-Gaussian impulse noise and achieves
superior restoration performance.

</details>


### [160] [MUGSQA: Novel Multi-Uncertainty-Based Gaussian Splatting Quality Assessment Method, Dataset, and Benchmarks](https://arxiv.org/abs/2511.06830)
*Tianang Chen,Jian Jin,Shilv Cai,Zhuangzi Li,Weisi Lin*

Main category: cs.CV

TL;DR: 本文提出了一个新的人类主观质量评估方法，建立了适用于高斯散点（GS）重建3D对象的质量评测数据集和基准。


<details>
  <summary>Details</summary>
Motivation: 随着高斯散点（GS）在3D物体重建领域的快速发展，如何公平、有效地评测不同GS变体的感知质量成为研究难题。现有方法难以准确反映实际应用中的用户体验。

Method: 提出一种统一的多距离主观质量评测方法，模拟真实应用中人眼观看行为，并据此构建了包含多种输入不确定性的新评测数据集MUGSQA。这些不确定性包括输入视图数量、分辨率、观察距离及点云准确性。此外，建立了两个基准：一个评测方法鲁棒性，另一个评测质量指标性能。

Result: 构建了新数据集MUGSQA和两个基准，可用于全面评测GS方法在多重不确定性下的重建效果与质量评估算法的性能。相关数据和代码即将公开。

Conclusion: 本文工作为GS-based 3D重建的主观质量评估提供了新方法和丰富的数据资源，有助于推动该领域相关方法的标准化和发展。

Abstract: Gaussian Splatting (GS) has recently emerged as a promising technique for 3D
object reconstruction, delivering high-quality rendering results with
significantly improved reconstruction speed. As variants continue to appear,
assessing the perceptual quality of 3D objects reconstructed with different
GS-based methods remains an open challenge. To address this issue, we first
propose a unified multi-distance subjective quality assessment method that
closely mimics human viewing behavior for objects reconstructed with GS-based
methods in actual applications, thereby better collecting perceptual
experiences. Based on it, we also construct a novel GS quality assessment
dataset named MUGSQA, which is constructed considering multiple uncertainties
of the input data. These uncertainties include the quantity and resolution of
input views, the view distance, and the accuracy of the initial point cloud.
Moreover, we construct two benchmarks: one to evaluate the robustness of
various GS-based reconstruction methods under multiple uncertainties, and the
other to evaluate the performance of existing quality assessment metrics. Our
dataset and benchmark code will be released soon.

</details>


### [161] [ConsistTalk: Intensity Controllable Temporally Consistent Talking Head Generation with Diffusion Noise Search](https://arxiv.org/abs/2511.06833)
*Zhenjie Liu,Jianzhang Lu,Renjie Lu,Cong Liang,Shangfei Wang*

Main category: cs.CV

TL;DR: 提出了ConsistTalk框架，显著提升了音驱人像动画的视频一致性、身份保持和音视同步，解决以往方法中的闪烁和身份漂移等难题。


<details>
  <summary>Details</summary>
Motivation: 现有音驱说话人头视频生成方法存在闪烁、身份漂移、音视不同步等问题，主要因为外观与运动表征纠缠，推理过程不稳定。本研究旨在解决这些关键挑战。

Method: 1）提出基于光流的时序模块（OFT），利用面部光流将运动与静态外观特征解耦，加强时序一致性，减少闪烁。2）基于多模态教师-学生蒸馏的音频到强度（A2I）模型，将语音和面部速度特征转化为逐帧强度序列，实现细粒度的动态控制及音视同步。3）引入扩散噪声初始化（IC-Init）策略，在推理时通过搜索背景与运动连续性的最优噪声，实现更好的身份保持和运动优化。

Result: ConsistTalk在减少闪烁、身份保持和视频时序一致性等方面，通过大量实验证明优于以往方法，生成的说话人头视频高保真且稳定。

Conclusion: ConsistTalk解决了音驱人像动画中关键的一致性与同步问题，实现了高质量、可控、稳定的说话人头生成，对实际应用有积极促进作用。

Abstract: Recent advancements in video diffusion models have significantly enhanced
audio-driven portrait animation. However, current methods still suffer from
flickering, identity drift, and poor audio-visual synchronization. These issues
primarily stem from entangled appearance-motion representations and unstable
inference strategies. In this paper, we introduce \textbf{ConsistTalk}, a novel
intensity-controllable and temporally consistent talking head generation
framework with diffusion noise search inference. First, we propose \textbf{an
optical flow-guided temporal module (OFT)} that decouples motion features from
static appearance by leveraging facial optical flow, thereby reducing visual
flicker and improving temporal consistency. Second, we present an
\textbf{Audio-to-Intensity (A2I) model} obtained through multimodal
teacher-student knowledge distillation. By transforming audio and facial
velocity features into a frame-wise intensity sequence, the A2I model enables
joint modeling of audio and visual motion, resulting in more natural dynamics.
This further enables fine-grained, frame-wise control of motion dynamics while
maintaining tight audio-visual synchronization. Third, we introduce a
\textbf{diffusion noise initialization strategy (IC-Init)}. By enforcing
explicit constraints on background coherence and motion continuity during
inference-time noise search, we achieve better identity preservation and refine
motion dynamics compared to the current autoregressive strategy. Extensive
experiments demonstrate that ConsistTalk significantly outperforms prior
methods in reducing flicker, preserving identity, and delivering temporally
stable, high-fidelity talking head videos.

</details>


### [162] [NeuroBridge: Bio-Inspired Self-Supervised EEG-to-Image Decoding via Cognitive Priors and Bidirectional Semantic Alignment](https://arxiv.org/abs/2511.06836)
*Wenjiang Zhang,Sifeng Wang,Yuwei Su,Xinyu Li,Chen Zhang,Suyu Zhong*

Main category: cs.CV

TL;DR: 本论文提出了名为NeuroBridge的全新自监督神经视觉解码框架，有效提升了从脑电信号重建或识别视觉刺激的准确率，超越了现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 视觉神经解码旨在通过脑活动还原或推断感知到的视觉刺激，这对认知研究和脑机接口有着重要意义。然而，受限于高质量脑-视觉数据的稀缺性和大脑表征与视觉内容之间的语义不匹配，现有方法效果有限。

Method: 作者提出NeuroBridge框架，结合了两项核心创新：（1）认知先验增强（CPA）：对脑电信号和图像分别施加不对称、特定模态的变换，模拟知觉多样性并增强语义丰富性；（2）共享语义投射器（SSP）：通过协同自适应，将脑电和视觉特征映射到一个共享语义空间，实现有效的跨模态对齐和特征互补。

Result: 实验证明NeuroBridge在intra-subject和inter-subject两种场景下均超越以往方法。在200类零样本检索任务中，intra-subject的top-1和top-5准确率分别提升了12.3%和10.2%，达到63.2%和89.9%。此外，框架在多项评测中表现出优异的鲁棒性和可扩展性。

Conclusion: NeuroBridge通过自监督和创新的跨模态对齐方法，有效解决了神经视觉解码领域的关键瓶颈，为脑机接口及相关研究提供了强有力的技术支撑。

Abstract: Visual neural decoding seeks to reconstruct or infer perceived visual stimuli
from brain activity patterns, providing critical insights into human cognition
and enabling transformative applications in brain-computer interfaces and
artificial intelligence. Current approaches, however, remain constrained by the
scarcity of high-quality stimulus-brain response pairs and the inherent
semantic mismatch between neural representations and visual content. Inspired
by perceptual variability and co-adaptive strategy of the biological systems,
we propose a novel self-supervised architecture, named NeuroBridge, which
integrates Cognitive Prior Augmentation (CPA) with Shared Semantic Projector
(SSP) to promote effective cross-modality alignment. Specifically, CPA
simulates perceptual variability by applying asymmetric, modality-specific
transformations to both EEG signals and images, enhancing semantic diversity.
Unlike previous approaches, SSP establishes a bidirectional alignment process
through a co-adaptive strategy, which mutually aligns features from two
modalities into a shared semantic space for effective cross-modal learning.
NeuroBridge surpasses previous state-of-the-art methods under both
intra-subject and inter-subject settings. In the intra-subject scenario, it
achieves the improvements of 12.3% in top-1 accuracy and 10.2% in top-5
accuracy, reaching 63.2% and 89.9% respectively on a 200-way zero-shot
retrieval task. Extensive experiments demonstrate the effectiveness,
robustness, and scalability of the proposed framework for neural visual
decoding.

</details>


### [163] [Gaussian-Augmented Physics Simulation and System Identification with Complex Colliders](https://arxiv.org/abs/2511.06846)
*Federico Vasile,Ri-Zhao Qiu,Lorenzo Natale,Xiaolong Wang*

Main category: cs.CV

TL;DR: 本论文提出了一种名为AS-DiffMPM的可微分物质点法（MPM）框架，实现了在任意复杂形状碰撞体下的物理属性估计，从而提升了系统识别的复杂度和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有基于MPM和可微渲染的方法在处理非平面碰撞体时能力有限，无法满足需要在真实复杂环境下进行对象与环境交互的系统识别任务。

Method: 作者提出AS-DiffMPM，通过引入可微分的碰撞处理机制，实现了对复杂刚体与目标对象的碰撞建模，并保持端到端的可优化性。该方法还可与新颖视角合成方法结合，从视觉观测中实现系统属性识别。

Result: AS-DiffMPM能够对复杂形状的碰撞体环境下的目标对象物理属性进行准确估计，突破了以往只能处理平面碰撞面的限制。

Conclusion: AS-DiffMPM显著推动了系统识别技术在多样化与复杂环境中的应用，实现了准确、高效的几何、外观及物理属性估计，并具备良好的方法扩展性。

Abstract: System identification involving the geometry, appearance, and physical
properties from video observations is a challenging task with applications in
robotics and graphics. Recent approaches have relied on fully differentiable
Material Point Method (MPM) and rendering for simultaneous optimization of
these properties. However, they are limited to simplified object-environment
interactions with planar colliders and fail in more challenging scenarios where
objects collide with non-planar surfaces. We propose AS-DiffMPM, a
differentiable MPM framework that enables physical property estimation with
arbitrarily shaped colliders. Our approach extends existing methods by
incorporating a differentiable collision handling mechanism, allowing the
target object to interact with complex rigid bodies while maintaining
end-to-end optimization. We show AS-DiffMPM can be easily interfaced with
various novel view synthesis methods as a framework for system identification
from visual observations.

</details>


### [164] [Distillation Dynamics: Towards Understanding Feature-Based Distillation in Vision Transformers](https://arxiv.org/abs/2511.06848)
*Huiyuan Tian,Bonan Xu Shijian Li*

Main category: cs.CV

TL;DR: 论文分析了为什么基于特征的知识蒸馏在视觉Transformer（ViT）模型中表现不佳，甚至不如传统的logit蒸馏，并通过频谱分析和信息理论进行研究，提出了解决途径。


<details>
  <summary>Details</summary>
Motivation: 尽管基于特征的知识蒸馏在CNN压缩中表现出色，但在ViT模型上效果很差。论文旨在揭示其原理、原因及改进方向，帮助设计更有效的ViT蒸馏和压缩方案。

Method: 提出了一种新的分析框架“蒸馏动力学”，结合频谱分析、信息熵度量和激活量监测，对知识蒸馏在ViT中的过程进行全面跟踪，剖析模型表征能力差异。

Result: 发现ViT存在独特的U型信息处理模式（先压缩后扩展），高维和分布式编码方式导致教师和学生模型存在表征范式不匹配问题，传统特征对齐反而抑制了学生模型性能。

Conclusion: ViT模型知识蒸馏不能机械模仿特征，应考虑表征的约束和差异，设计新的蒸馏策略。论文为ViT的高效压缩提供了理论依据。

Abstract: While feature-based knowledge distillation has proven highly effective for
compressing CNNs, these techniques unexpectedly fail when applied to Vision
Transformers (ViTs), often performing worse than simple logit-based
distillation. We provide the first comprehensive analysis of this phenomenon
through a novel analytical framework termed as ``distillation dynamics",
combining frequency spectrum analysis, information entropy metrics, and
activation magnitude tracking. Our investigation reveals that ViTs exhibit a
distinctive U-shaped information processing pattern: initial compression
followed by expansion. We identify the root cause of negative transfer in
feature distillation: a fundamental representational paradigm mismatch between
teacher and student models. Through frequency-domain analysis, we show that
teacher models employ distributed, high-dimensional encoding strategies in
later layers that smaller student models cannot replicate due to limited
channel capacity. This mismatch causes late-layer feature alignment to actively
harm student performance. Our findings reveal that successful knowledge
transfer in ViTs requires moving beyond naive feature mimicry to methods that
respect these fundamental representational constraints, providing essential
theoretical guidance for designing effective ViTs compression strategies. All
source code and experimental logs are provided in the supplementary material.

</details>


### [165] [Ambiguity-aware Truncated Flow Matching for Ambiguous Medical Image Segmentation](https://arxiv.org/abs/2511.06857)
*Fanding Li,Xiangyu Li,Xianghe Su,Xingyu Qiu,Suyu Dong,Wei Wang,Kuanquan Wang,Gongning Luo,Shuo Li*

Main category: cs.CV

TL;DR: 本文提出了ATFM方法，有效提升了医学图像分割中结果的准确性与多样性，并优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 在医学图像分割任务中，如何同时提升预测的准确性和多样性一直存在挑战。当前TDPMs方法难以兼顾这两种需求，且生成图像的真实感和合理性存在不足。

Method: 作者提出了ATFM（Ambiguity-aware Truncated Flow Matching）方法，包含三大创新：(1)数据分层推断(Data-Hierarchical Inference)，分别在数据分布和样本层面提升准确性和多样性，实现有效解耦；(2)高斯截断表示(Gaussian Truncation Representation)，用高斯分布建模截断点$T_{trunc}$提升结果真实感与分布可靠性；(3)分割流匹配(Segmentation Flow Matching)，通过扩展流匹配的语义建模提升预测多样性的合理性。

Result: ATFM在LIDC和ISIC3医学图像分割数据集上效果显著，GED与HM-IoU指标比先进方法分别提升12%和7.3%，推断也更高效。

Conclusion: ATFM显著提升了AMIS中预测的准确性、多样性及推断效率，有望成为新一代医学图像分割的标准方法。

Abstract: A simultaneous enhancement of accuracy and diversity of predictions remains a
challenge in ambiguous medical image segmentation (AMIS) due to the inherent
trade-offs. While truncated diffusion probabilistic models (TDPMs) hold strong
potential with a paradigm optimization, existing TDPMs suffer from entangled
accuracy and diversity of predictions with insufficient fidelity and
plausibility. To address the aforementioned challenges, we propose
Ambiguity-aware Truncated Flow Matching (ATFM), which introduces a novel
inference paradigm and dedicated model components. Firstly, we propose
Data-Hierarchical Inference, a redefinition of AMIS-specific inference
paradigm, which enhances accuracy and diversity at data-distribution and
data-sample level, respectively, for an effective disentanglement. Secondly,
Gaussian Truncation Representation (GTR) is introduced to enhance both fidelity
of predictions and reliability of truncation distribution, by explicitly
modeling it as a Gaussian distribution at $T_{\text{trunc}}$ instead of using
sampling-based approximations.Thirdly, Segmentation Flow Matching (SFM) is
proposed to enhance the plausibility of diverse predictions by extending
semantic-aware flow transformation in Flow Matching (FM). Comprehensive
evaluations on LIDC and ISIC3 datasets demonstrate that ATFM outperforms SOTA
methods and simultaneously achieves a more efficient inference. ATFM improves
GED and HM-IoU by up to $12\%$ and $7.3\%$ compared to advanced methods.

</details>


### [166] [VAEVQ: Enhancing Discrete Visual Tokenization through Variational Modeling](https://arxiv.org/abs/2511.06863)
*Sicheng Yang,Xing Hu,Qiang Wu,Dawei Yang*

Main category: cs.CV

TL;DR: 该论文提出了一种改进的向量量化方法VAEVQ，有效提升了生成模型中的重构与生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有向量量化（VQ）框架存在潜在空间不平滑、特征对齐弱、连续与离散域一致性差等问题，导致码字学习不稳定、码本利用不足，影响重构和生成性能。

Method: 作者提出VAEVQ，包括三大关键组件：(1) 用变分自编码器（VAE）替换自编码器实现变分潜在量化（VLQ），带来结构化、平滑的潜在空间；(2) 提出表示一致性策略（RCS），自适应调节量化前后特征对齐强度，增强一致性、防止对噪声过拟合；(3) 引入分布一致性正则（DCR），对齐整个码本分布与连续潜在分布，提高码本利用率。

Result: 在两个基准数据集上的实验表明，与最新方法相比，VAEVQ在重构与生成任务中表现更优。

Conclusion: VAEVQ通过结构创新和正则机制显著缓解了VQ相关问题，提升了模型性能，为生成建模提供了更优的离散表征方式。

Abstract: Vector quantization (VQ) transforms continuous image features into discrete
representations, providing compressed, tokenized inputs for generative models.
However, VQ-based frameworks suffer from several issues, such as non-smooth
latent spaces, weak alignment between representations before and after
quantization, and poor coherence between the continuous and discrete domains.
These issues lead to unstable codeword learning and underutilized codebooks,
ultimately degrading the performance of both reconstruction and downstream
generation tasks. To this end, we propose VAEVQ, which comprises three key
components: (1) Variational Latent Quantization (VLQ), replacing the AE with a
VAE for quantization to leverage its structured and smooth latent space,
thereby facilitating more effective codeword activation; (2) Representation
Coherence Strategy (RCS), adaptively modulating the alignment strength between
pre- and post-quantization features to enhance consistency and prevent
overfitting to noise; and (3) Distribution Consistency Regularization (DCR),
aligning the entire codebook distribution with the continuous latent
distribution to improve utilization. Extensive experiments on two benchmark
datasets demonstrate that VAEVQ outperforms state-of-the-art methods.

</details>


### [167] [Generating an Image From 1,000 Words: Enhancing Text-to-Image With Structured Captions](https://arxiv.org/abs/2511.06876)
*Eyal Gutflaish,Eliran Kachlon,Hezi Zisman,Tal Hacham,Nimrod Sarid,Alexander Visheratin,Saar Huberman,Gal Davidi,Guy Bukchin,Kfir Goldberg,Ron Mokady*

Main category: cs.CV

TL;DR: 现有文生图模型通常仅根据简短提示生成丰富图像，导致输入输出信息不对称、可控性不足。本文提出了可基于结构化长文本描述进行生成的开源文生图模型FIBO，配合DimFusion融合机制和TaBR评测协议，提升了模型对细节的表达和可控性。


<details>
  <summary>Details</summary>
Motivation: 目前主流的文生图模型多基于短文本提示，容易丢失用户细致化表达，模型只能凭经验“猜测”细节，降低了专业场景下的适用性和精确度。作者希望解决文本提示与图像输出之间的信息缺口，提高生成质量与可控性。

Method: 1）采用长结构化文本描述，每个样本注释统一的细粒度属性，以增强可控性和表达范围。2）提出DimFusion机制，将轻量级LLM的中间token与主模型输出高效融合，不增加token总数。3）提出TaBR评测协议，通过“描述—生成—重描述”循环定量评测模型对长描述的还原与可控性。4）训练大规模FIBO模型，并开源权重。

Result: FIBO模型对长文本描述的细节表达和可控性显著提升，在开源文生图模型中达到了业界领先的提示对齐性能。DimFusion与TaBR新机制效果良好。相关权重已在Hugging Face平台开源。

Conclusion: 本文首次用结构化长文本构建高可控的开源文生图模型，显著扩展了模型的表现力和可控性，并提出评估协议TaBR，支持专业和高精度生成需求，为行业广泛应用铺平道路。

Abstract: Text-to-image models have rapidly evolved from casual creative tools to
professional-grade systems, achieving unprecedented levels of image quality and
realism. Yet, most models are trained to map short prompts into detailed
images, creating a gap between sparse textual input and rich visual outputs.
This mismatch reduces controllability, as models often fill in missing details
arbitrarily, biasing toward average user preferences and limiting precision for
professional use. We address this limitation by training the first open-source
text-to-image model on long structured captions, where every training sample is
annotated with the same set of fine-grained attributes. This design maximizes
expressive coverage and enables disentangled control over visual factors. To
process long captions efficiently, we propose DimFusion, a fusion mechanism
that integrates intermediate tokens from a lightweight LLM without increasing
token length. We also introduce the Text-as-a-Bottleneck Reconstruction (TaBR)
evaluation protocol. By assessing how well real images can be reconstructed
through a captioning-generation loop, TaBR directly measures controllability
and expressiveness, even for very long captions where existing evaluation
methods fail. Finally, we demonstrate our contributions by training the
large-scale model FIBO, achieving state-of-the-art prompt alignment among
open-source models. Model weights are publicly available at
https://huggingface.co/briaai/FIBO

</details>


### [168] [A Two-Stage System for Layout-Controlled Image Generation using Large Language Models and Diffusion Models](https://arxiv.org/abs/2511.06888)
*Jan-Hendrik Koch,Jonas Krumme,Konrad Gadzicki*

Main category: cs.CV

TL;DR: 本文提出了一种两阶段的文本到图像生成系统，实现了对象数量和空间排布的精确控制。第一阶段利用大语言模型（LLM）生成结构化布局，第二阶段使用布局条件扩散模型生成真实感图像。该方法显著提升了复杂场景中的对象召回率，并比较了两种主流的布局条件生成方法（ControlNet和GLIGEN）。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型在生成质量上表现优异，但难以精确控制生成图像中对象的数量和空间位置限制，限制了在需要结构化合成任务中的应用能力。作者针对这一问题提出了新的解决办法。

Method: 分两阶段处理：第一阶段使用LLM基于对象列表生成结构化布局，采用分步任务分解方式（先核心对象，再用规则补全），提升空间规划准确性。第二阶段利用布局驱动的扩散模型合成图像，并对比了ControlNet与GLIGEN两种布局条件生成技术。

Result: 通过对复杂场景（如摆盘）实验，任务分解和规则插入显著提升了对象召回率（从57.2%提升到99.9%）。在图像合成中，ControlNet更好保持文本风格，但易产生对象幻觉；GLIGEN布局更准确，但灵活性降低。端到端系统整体效果理想，实现了对象数量与空间排布的高度可控性。

Conclusion: 两阶段分离式构图方案可大幅提升文本到图像合成中的结构可控性，为具有复杂组合需求的生成任务提供了可扩展的解决思路。

Abstract: Text-to-image diffusion models exhibit remarkable generative capabilities,
but lack precise control over object counts and spatial arrangements. This work
introduces a two-stage system to address these compositional limitations. The
first stage employs a Large Language Model (LLM) to generate a structured
layout from a list of objects. The second stage uses a layout-conditioned
diffusion model to synthesize a photorealistic image adhering to this layout.
We find that task decomposition is critical for LLM-based spatial planning; by
simplifying the initial generation to core objects and completing the layout
with rule-based insertion, we improve object recall from 57.2% to 99.9% for
complex scenes. For image synthesis, we compare two leading conditioning
methods: ControlNet and GLIGEN. After domain-specific finetuning on
table-setting datasets, we identify a key trade-off: ControlNet preserves
text-based stylistic control but suffers from object hallucination, while
GLIGEN provides superior layout fidelity at the cost of reduced prompt-based
controllability. Our end-to-end system successfully generates images with
specified object counts and plausible spatial arrangements, demonstrating the
viability of a decoupled approach for compositionally controlled synthesis.

</details>


### [169] [Adaptive Morph-Patch Transformer for Arotic Vessel Segmentation](https://arxiv.org/abs/2511.06897)
*Zhenxi Zhang,Fuchen Zheng,Adnan Iltaf,Yifei Han,Zhenyu Cheng,Yue Du,Bin Li,Tianyong Liu,Shoujun Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新的Transformer架构——自适应形态补丁Transformer（MPT），用于主动脉血管结构的精确分割，通过自适应分块和语义聚类机制，有效提升了复杂血管分割的准确性，并在多个公开数据集上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 传统的Transformer模型虽然擅长捕捉血管特征间的长距离依赖，但其依赖固定大小的矩形补丁，难以适配和保持复杂血管结构的连续性，导致分割准确率下降。为此，急需设计更适合血管形态结构的方法。

Method: 提出了两大创新点：一是自适应补丁划分策略，能够根据血管复杂形态动态生成更符合结构分布的补丁，从而保留血管语义完整性；二是引入语义聚类注意力机制，可动态聚合来自语义相似补丁的特征信息，增强对不同尺寸血管的建模与分割能力。

Result: 在三个公开数据集AVT、AortaSeg24和TBAD上进行了大量实验，MPT在分割复杂血管结构任务上均取得了最新最好的性能，明显优于传统的基线方法。

Conclusion: MPT通过自适应分块与语义聚类显著提升了复杂主动脉血管结构的分割效果，为临床心血管疾病诊断与治疗中的血管分割提供了更为准确可靠的方法。

Abstract: Accurate segmentation of aortic vascular structures is critical for
diagnosing and treating cardiovascular diseases.Traditional Transformer-based
models have shown promise in this domain by capturing long-range dependencies
between vascular features. However, their reliance on fixed-size rectangular
patches often influences the integrity of complex vascular structures, leading
to suboptimal segmentation accuracy. To address this challenge, we propose the
adaptive Morph Patch Transformer (MPT), a novel architecture specifically
designed for aortic vascular segmentation. Specifically, MPT introduces an
adaptive patch partitioning strategy that dynamically generates
morphology-aware patches aligned with complex vascular structures. This
strategy can preserve semantic integrity of complex vascular structures within
individual patches. Moreover, a Semantic Clustering Attention (SCA) method is
proposed to dynamically aggregate features from various patches with similar
semantic characteristics. This method enhances the model's capability to
segment vessels of varying sizes, preserving the integrity of vascular
structures. Extensive experiments on three open-source dataset(AVT, AortaSeg24
and TBAD) demonstrate that MPT achieves state-of-the-art performance, with
improvements in segmenting intricate vascular structures.

</details>


### [170] [Classification of Microplastic Particles in Water using Polarized Light Scattering and Machine Learning Methods](https://arxiv.org/abs/2511.06901)
*Leonard Saur,Marc von Pawlowski,Ulrich Gengenbach,Ingo Sieber,Hossein Shirali,Lorenz Wührl,Rainer Kiko,Christian Pylatiuk*

Main category: cs.CV

TL;DR: 本论文提出并验证了一种基于偏振光散射的新颖反射法，可用于水环境中原位、实时监测和识别微塑料，分类准确率最高达到80%。


<details>
  <summary>Details</summary>
Motivation: 微塑料污染严重，需要持续且大规模的监测，但现有“金标准”方法在水体中受限，难以高效、准确地检测和鉴别微塑料。

Method: 采用线偏振激光照射无色微塑料（50-300微米），利用偏振敏感摄像头捕获反射信号，结合深度卷积神经网络（CNN）对图像进行分类。分析了不同偏振信号（AOLP和DOLP）对分类性能的影响。

Result: CNN成功对三种常见聚合物（高密度聚乙烯、低密度聚乙烯和聚丙烯）进行分类，测试集准确率最高达80%。AOLP信号对背景噪声更鲁棒，尤其能有效区分两种聚乙烯，而DOLP对聚丙烯识别效果更佳。

Conclusion: 基于偏振反射法结合深度学习可有效实现水环境中微塑料的原位检测和分类，AOLP和DOLP信号具有互补优势，对提升微塑料在线监测有重要意义。

Abstract: Facing the critical need for continuous, large-scale microplastic monitoring,
which is hindered by the limitations of gold-standard methods in aquatic
environments, this paper introduces and validates a novel, reflection-based
approach for the in-situ classification and identification of microplastics
directly in water bodies, which is based on polarized light scattering. In this
experiment, we classify colorless microplastic particles (50-300 $\mu$m) by
illuminating them with linearly polarized laser light and capturing their
reflected signals using a polarization-sensitive camera. This reflection-based
technique successfully circumvents the transmission-based interference issues
that plague many conventional methods when applied in water. Using a deep
convolutional neural network (CNN) for image-based classification, we
successfully identified three common polymer types, high-density polyethylene,
low-density polyethylene, and polypropylene, achieving a peak mean
classification accuracy of 80% on the test dataset. A subsequent feature
hierarchy analysis demonstrated that the CNN's decision-making process relies
mainly on the microstructural integrity and internal texture (polarization
patterns) of the particle rather than its macroshape. Critically, we found that
the Angle of Linear Polarization (AOLP) signal is significantly more robust
against contextual noise than the Degree of Linear Polarization (DOLP) signal.
While the AOLP-based classification achieved superior overall performance, its
strength lies in distinguishing between the two polyethylene plastics, showing
a lower confusion rate between high-density and low-density polyethylene.
Conversely, the DOLP signal demonstrated slightly worse overall classification
results but excels at accurately identifying the polypropylene class, which it
isolated with greater success than AOLP.

</details>


### [171] [Mono3DVG-EnSD: Enhanced Spatial-aware and Dimension-decoupled Text Encoding for Monocular 3D Visual Grounding](https://arxiv.org/abs/2511.06908)
*Yuzhen Li,Min Liu,Zhaoyang Li,Yuan Bian,Xueping Wang,Erbo Zhai,Yaonan Wang*

Main category: cs.CV

TL;DR: 本文提出了Mono3DVG-EnSD新框架，用于提升基于文本描述的单目3D视觉定位精度，有效缓解对显性关键字的依赖和跨维度干扰问题，并在Mono3DRefer数据集上取得了最优结果，特别是在远距离目标定位任务中提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有的单目3D视觉定位方法过于依赖显性关键字，忽略了重要的空间描述信息。此外，文本特征混合了2D和3D描述，导致在特征对齐和引导过程中产生跨维度干扰，影响模型的感知能力。本文旨在解决上述两个核心瓶颈。

Method: 提出了Mono3DVG-EnSD框架，包括两个核心模块：第一，CLIP引导的词汇置信度适配器（CLIP-LCA），通过动态屏蔽高置信度关键字，保留低置信度空间描述，强制模型理解更深层次的空间关系；第二，维度解耦模块（D2M），将2D/3D文本特征从通用文本特征中解耦，保证同维度跨模态交互，从而缓解交叉维度的信息干扰问题。

Result: 在Mono3DRefer数据集上进行的对比和消融实验表明，该方法在所有指标上均取得了SOTA表现，尤其在‘Far(Acc@0.5)’挑战场景下提升了13.54%。

Conclusion: Mono3DVG-EnSD有效缓解了过度依赖显性关键字和跨维度干扰问题，显著增强了单目3D视觉定位的性能。提出的解耦和自适应机制为多模态理解提供了新的技术路径。

Abstract: Monocular 3D Visual Grounding (Mono3DVG) is an emerging task that locates 3D
objects in RGB images using text descriptions with geometric cues. However,
existing methods face two key limitations. Firstly, they often over-rely on
high-certainty keywords that explicitly identify the target object while
neglecting critical spatial descriptions. Secondly, generalized textual
features contain both 2D and 3D descriptive information, thereby capturing an
additional dimension of details compared to singular 2D or 3D visual features.
This characteristic leads to cross-dimensional interference when refining
visual features under text guidance. To overcome these challenges, we propose
Mono3DVG-EnSD, a novel framework that integrates two key components: the
CLIP-Guided Lexical Certainty Adapter (CLIP-LCA) and the Dimension-Decoupled
Module (D2M). The CLIP-LCA dynamically masks high-certainty keywords while
retaining low-certainty implicit spatial descriptions, thereby forcing the
model to develop a deeper understanding of spatial relationships in captions
for object localization. Meanwhile, the D2M decouples dimension-specific
(2D/3D) textual features from generalized textual features to guide
corresponding visual features at same dimension, which mitigates
cross-dimensional interference by ensuring dimensionally-consistent cross-modal
interactions. Through comprehensive comparisons and ablation studies on the
Mono3DRefer dataset, our method achieves state-of-the-art (SOTA) performance
across all metrics. Notably, it improves the challenging Far(Acc@0.5) scenario
by a significant +13.54%.

</details>


### [172] [DTTNet: Improving Video Shadow Detection via Dark-Aware Guidance and Tokenized Temporal Modeling](https://arxiv.org/abs/2511.06925)
*Zhicheng Li,Kunyang Sun,Rui Yao,Hancheng Zhu,Fuyuan Hu,Jiaqi Zhao,Zhiwen Shao,Yong Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频阴影检测方法，通过结合视觉-语言信息和时序建模，实现了更精准且高效的阴影检测。


<details>
  <summary>Details</summary>
Motivation: 视频阴影检测面临阴影与复杂背景区分困难，以及动态不规则阴影形变下的时变建模，尤其是在光照变化剧烈的场景下，这两个问题极大影响检测准确性与实用性。

Method: 1. 设计视觉-语言匹配模块（VMM）引入语言先验，利用描述性文本引导特征显著区分阴影与暗物体；2. 新增暗感知语义模块（DSB）进一步提取文字引导特征提升判别性；3. 训练阶段采用自适应掩码重加权，降低半影区对模型的干扰，并在解码尾端用边缘掩码加强监督；4. 提出令牌化时序块（TTB），以解耦时空学习，通过跨帧学习得到可训练时序token，实现低计算开销下的时序信息高效建模。

Result: 在多个基准数据集上的实验充分验证了方法的先进性，取得了最新的检测精度，同时推理速度达到实时水平。

Conclusion: 本文提出的结合视觉-语言先验与令牌化时序建模的视频阴影检测框架不仅实现了精度和效率的双提升，也为视频中的阴影检测提供了创新的解决途径。

Abstract: Video shadow detection confronts two entwined difficulties: distinguishing
shadows from complex backgrounds and modeling dynamic shadow deformations under
varying illumination. To address shadow-background ambiguity, we leverage
linguistic priors through the proposed Vision-language Match Module (VMM) and a
Dark-aware Semantic Block (DSB), extracting text-guided features to explicitly
differentiate shadows from dark objects. Furthermore, we introduce adaptive
mask reweighting to downweight penumbra regions during training and apply edge
masks at the final decoder stage for better supervision. For temporal modeling
of variable shadow shapes, we propose a Tokenized Temporal Block (TTB) that
decouples spatiotemporal learning. TTB summarizes cross-frame shadow semantics
into learnable temporal tokens, enabling efficient sequence encoding with
minimal computation overhead. Comprehensive Experiments on multiple benchmark
datasets demonstrate state-of-the-art accuracy and real-time inference
efficiency. Codes are available at https://github.com/city-cheng/DTTNet.

</details>


### [173] [PlantTraitNet: An Uncertainty-Aware Multimodal Framework for Global-Scale Plant Trait Inference from Citizen Science Data](https://arxiv.org/abs/2511.06943)
*Ayushi Sharma,Johanna Trost,Daniel Lusk,Johannes Dollinger,Julian Schrader,Christian Rossi,Javier Lopatin,Etienne Laliberté,Simon Haberstroh,Jana Eichel,Daniel Mederer,Jose Miguel Cerda-Paredes,Shyam S. Phartyal,Lisa-Maricia Schwarz,Anja Linstädter,Maria Conceição Caldeira,Teja Kattenborn*

Main category: cs.CV

TL;DR: 论文提出了一种新的通过公民科学照片推断全球植物性状的方法，显著提升了全球性状地图的精度和覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 现有的全球植物性状地图由于依赖实地测量，存在成本高、地理覆盖有限的问题。作者希望利用公民科学提供的大量植物照片，挖掘其在植物性状推断方面的潜力，以提升全球性状数据的可用性与准确性。

Method: 作者设计了PlantTraitNet框架，它是一个多模态、多任务、可感知不确定性的深度学习模型。该模型利用弱监督，结合公民科学收集的地理标记植株照片，预测四类关键植物性状（植株高度、叶面积、比叶面积、氮含量），并通过空间聚合生成全球性状分布图。模型结果与独立地面调查数据（如sPlotOpen）和先进的现有全球性状产品进行对比验证。

Result: PlantTraitNet对所有评估的性状均优于现有主流全球性状地图，说明结合公民科学图像和人工智能方法能够生成更具可扩展性和更高精度的全球性状数据。

Conclusion: 研究证明了公民科学图像结合地理空间AI在生态学研究和地球系统建模中的巨大潜力，为解决全球尺度植物性状获取难题开辟了新途径。

Abstract: Global plant maps of plant traits, such as leaf nitrogen or plant height, are
essential for understanding ecosystem processes, including the carbon and
energy cycles of the Earth system. However, existing trait maps remain limited
by the high cost and sparse geographic coverage of field-based measurements.
Citizen science initiatives offer a largely untapped resource to overcome these
limitations, with over 50 million geotagged plant photographs worldwide
capturing valuable visual information on plant morphology and physiology. In
this study, we introduce PlantTraitNet, a multi-modal, multi-task
uncertainty-aware deep learning framework that predictsfour key plant traits
(plant height, leaf area, specific leaf area, and nitrogen content) from
citizen science photos using weak supervision. By aggregating individual trait
predictions across space, we generate global maps of trait distributions. We
validate these maps against independent vegetation survey data (sPlotOpen) and
benchmark them against leading global trait products. Our results show that
PlantTraitNet consistently outperforms existing trait maps across all evaluated
traits, demonstrating that citizen science imagery, when integrated with
computer vision and geospatial AI, enables not only scalable but also more
accurate global trait mapping. This approach offers a powerful new pathway for
ecological research and Earth system modeling.

</details>


### [174] [From Attribution to Action: Jointly ALIGNing Predictions and Explanations](https://arxiv.org/abs/2511.06944)
*Dongsheng Hong,Chao Chen,Yanhui Chen,Shanshan Lin,Zhihao Chen,Xiangwen Liao*

Main category: cs.CV

TL;DR: 本文提出了一种名为ALIGN的解释-引导学习（EGL）新框架，通过联合训练分类器与遮罩器，在提升模型可解释性的同时提升预测性能，在多个领域泛化基准上的表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有EGL方法多依赖外部标注或启发式分割来监督模型解释，这些监督信号噪声大、精度低且难以扩展，甚至可能削弱模型性能。因此，亟需开发一种不用依赖低质量外部监督、更高效可靠的解释-引导学习方法。

Method: 提出ALIGN框架，采用分类器与遮罩器的联合迭代训练。遮罩器生成任务相关的软掩码，突出关键信息区域；分类器则优化预测精度，并通过其显著性图与生成掩码对齐。即，模型准确性与解释性共同优化。

Result: 在两个领域泛化基准（VLCS和Terra Incognita）上的实验证明，ALIGN在分布内和分布外的场景中都持续超越六个强基线方法。此外，在解释的充分性和完整性指标上也表现出色。

Conclusion: ALIGN框架不仅提升了模型的可解释性和泛化能力，还避免了低质量监督对性能的负面影响，是构建准确且可解释模型的有效方法。

Abstract: Explanation-guided learning (EGL) has shown promise in aligning model
predictions with interpretable reasoning, particularly in computer vision
tasks. However, most approaches rely on external annotations or heuristic-based
segmentation to supervise model explanations, which can be noisy, imprecise and
difficult to scale. In this work, we provide both empirical and theoretical
evidence that low-quality supervision signals can degrade model performance
rather than improve it. In response, we propose ALIGN, a novel framework that
jointly trains a classifier and a masker in an iterative manner. The masker
learns to produce soft, task-relevant masks that highlight informative regions,
while the classifier is optimized for both prediction accuracy and alignment
between its saliency maps and the learned masks. By leveraging high-quality
masks as guidance, ALIGN improves both interpretability and generalizability,
showing its superiority across various settings. Experiments on the two domain
generalization benchmarks, VLCS and Terra Incognita, show that ALIGN
consistently outperforms six strong baselines in both in-distribution and
out-of-distribution settings. Besides, ALIGN also yields superior explanation
quality concerning sufficiency and comprehensiveness, highlighting its
effectiveness in producing accurate and interpretable models.

</details>


### [175] [FoCLIP: A Feature-Space Misalignment Framework for CLIP-Based Image Manipulation and Detection](https://arxiv.org/abs/2511.06947)
*Yulin Chen,Zeyuan Wang,Tianyuan Yu,Yingmei Wei,Liang Bai*

Main category: cs.CV

TL;DR: 本文提出了一种可以欺骗基于CLIP的图像质量评价指标（如CLIPscore）的方法，并探索了相应的检测与防御策略。


<details>
  <summary>Details</summary>
Motivation: CLIP及其衍生的质量评估分数(CLIPscore)因其良好的跨模态对齐能力，被广泛用于图像质量评价，但其对输入的脆弱性和潜在安全风险尚未充分研究。本文旨在揭示并利用这种脆弱性。

Method: 提出了FoCLIP框架，包括特征对齐主模块、分数分布平衡模块及像素保护正则项，借助随机梯度下降生成可以迷惑CLIPscore的示例图像。同时发现将图片转换为灰度会削弱CLIP特征，从而提出基于颜色通道敏感性的篡改检测方法。

Result: 在艺术大师作品和ImageNet子集上实验表明，FoCLIP生成的图像能在CLIPscore上大幅提升分数，同时保持高视觉保真度；而灰度化大幅降低CLIPscore。所提检测机制在标准基准上达到91%的准确率。

Conclusion: 本研究提出了一种可行的特征错位攻击方案以欺骗CLIP-based多模态系统，并给出了实用的防御检测手段，为多模态系统安全性研究提供了新的思路。

Abstract: The well-aligned attribute of CLIP-based models enables its effective
application like CLIPscore as a widely adopted image quality assessment metric.
However, such a CLIP-based metric is vulnerable for its delicate multimodal
alignment. In this work, we propose \textbf{FoCLIP}, a feature-space
misalignment framework for fooling CLIP-based image quality metric. Based on
the stochastic gradient descent technique, FoCLIP integrates three key
components to construct fooling examples: feature alignment as the core module
to reduce image-text modality gaps, the score distribution balance module and
pixel-guard regularization, which collectively optimize multimodal output
equilibrium between CLIPscore performance and image quality. Such a design can
be engineered to maximize the CLIPscore predictions across diverse input
prompts, despite exhibiting either visual unrecognizability or semantic
incongruence with the corresponding adversarial prompts from human perceptual
perspectives. Experiments on ten artistic masterpiece prompts and ImageNet
subsets demonstrate that optimized images can achieve significant improvement
in CLIPscore while preserving high visual fidelity. In addition, we found that
grayscale conversion induces significant feature degradation in fooling images,
exhibiting noticeable CLIPscore reduction while preserving statistical
consistency with original images. Inspired by this phenomenon, we propose a
color channel sensitivity-driven tampering detection mechanism that achieves
91% accuracy on standard benchmarks. In conclusion, this work establishes a
practical pathway for feature misalignment in CLIP-based multimodal systems and
the corresponding defense method.

</details>


### [176] [PADM: A Physics-aware Diffusion Model for Attenuation Correction](https://arxiv.org/abs/2511.06948)
*Trung Kien Pham,Hoang Minh Vu,Anh Duc Chu,Dac Thai Nguyen,Trung Thanh Nguyen,Thao Nguyen Truong,Mai Hong Son,Thanh Trung Nguyen,Phi Le Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种全新的无CT心脏SPECT衰减校正方法，通过物理感知扩散模型（PADM），有效解决衰减伪影对心脏灌注显像（MPI）诊断精度的影响。该方法仅用非衰减校正（NAC）图像输入，结合物理先验和知识蒸馏机制，实现高效衰减校正，优于现有生成模型。


<details>
  <summary>Details</summary>
Motivation: 传统SPECT衰减校正需依赖昂贵且存在额外辐射的CT设备。现有无CT方法无法有效结合物理先验以消除伪影，限制了其广泛应用和校正效果。

Method: 提出了一种融合物理先验的扩散生成模型（PADM），通过教师-学生蒸馏机制在训练阶段利用含物理信息的监督信号，推理阶段仅需NAC图像，实现无CT衰减校正。此外，作者还建设了CardiAC数据集用于系统评估。

Result: PADM在424例实际患者数据上测试，重建质量在量化指标和视觉表现上均优于最先进的生成类模型，有效修正SPECT影像的伪影。

Conclusion: PADM为心脏SPECT衰减校正提供了一条无需昂贵CT设备、兼具物理先验和高重建性能的新路径，有望提升MPI诊断的临床推广性和可用性。

Abstract: Attenuation artifacts remain a significant challenge in cardiac Myocardial
Perfusion Imaging (MPI) using Single-Photon Emission Computed Tomography
(SPECT), often compromising diagnostic accuracy and reducing clinical
interpretability. While hybrid SPECT/CT systems mitigate these artifacts
through CT-derived attenuation maps, their high cost, limited accessibility,
and added radiation exposure hinder widespread clinical adoption. In this
study, we propose a novel CT-free solution to attenuation correction in cardiac
SPECT. Specifically, we introduce Physics-aware Attenuation Correction
Diffusion Model (PADM), a diffusion-based generative method that incorporates
explicit physics priors via a teacher--student distillation mechanism. This
approach enables attenuation artifact correction using only
Non-Attenuation-Corrected (NAC) input, while still benefiting from
physics-informed supervision during training. To support this work, we also
introduce CardiAC, a comprehensive dataset comprising 424 patient studies with
paired NAC and Attenuation-Corrected (AC) reconstructions, alongside
high-resolution CT-based attenuation maps. Extensive experiments demonstrate
that PADM outperforms state-of-the-art generative models, delivering superior
reconstruction fidelity across both quantitative metrics and visual assessment.

</details>


### [177] [GFix: Perceptually Enhanced Gaussian Splatting Video Compression](https://arxiv.org/abs/2511.06953)
*Siyue Teng,Ge Gao,Duolikun Danier,Yuxuan Jiang,Fan Zhang,Thomas Davis,Zoe Liu,David Bull*

Main category: cs.CV

TL;DR: 本文提出了GFix框架，通过单步扩散模型对3D高斯投影（3DGS）编码的视频进行感知质量增强，还改进了模型的适应性训练方式，实现了压缩率和视觉质量的双重提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于3DGS的视频编解码虽有快速渲染和高精度3D重建的优势，但压缩后视频易出现明显的视觉伪影，感知质量和压缩比有待提高。

Method: 提出一种名为GFix的内容自适应框架，核心为精简的单步扩散模型，用作神经增强器，对3DGS编码视频的伪影进行修复。此外，提出调制型LoRA方案，冻结低秩分解参数，仅调制中间隐藏状态，提高适应效率并减少模型更新存储量。

Result: 实验表明，GFix在感知质量上明显优于现有3DGS视频编码方案GSVC，在LPIPS和FID指标上分别实现最高72.1%和21.4%的BD-rate节省。

Conclusion: GFix框架能有效提升3DGS视频压缩的视觉质量，并且具备高效的模型适应能力，在视频压缩领域具有实际应用潜力。

Abstract: 3D Gaussian Splatting (3DGS) enhances 3D scene reconstruction through
explicit representation and fast rendering, demonstrating potential benefits
for various low-level vision tasks, including video compression. However,
existing 3DGS-based video codecs generally exhibit more noticeable visual
artifacts and relatively low compression ratios. In this paper, we specifically
target the perceptual enhancement of 3DGS-based video compression, based on the
assumption that artifacts from 3DGS rendering and quantization resemble noisy
latents sampled during diffusion training. Building on this premise, we propose
a content-adaptive framework, GFix, comprising a streamlined, single-step
diffusion model that serves as an off-the-shelf neural enhancer. Moreover, to
increase compression efficiency, We propose a modulated LoRA scheme that
freezes the low-rank decompositions and modulates the intermediate hidden
states, thereby achieving efficient adaptation of the diffusion backbone with
highly compressible updates. Experimental results show that GFix delivers
strong perceptual quality enhancement, outperforming GSVC with up to 72.1%
BD-rate savings in LPIPS and 21.4% in FID.

</details>


### [178] [Learning from the Right Patches: A Two-Stage Wavelet-Driven Masked Autoencoder for Histopathology Representation Learning](https://arxiv.org/abs/2511.06958)
*Raneen Younis,Louay Hamdi,Lukas Chavez,Zahra Ahmadi*

Main category: cs.CV

TL;DR: 本文提出了一种结合小波变换的结构化和生物相关性补丁选择策略WISE-MAE，用于组织病理切片自监督表示学习。该方法通过两步筛选有意义区域，提高模型效率和表现。


<details>
  <summary>Details</summary>
Motivation: 传统MAE在处理极高清组织切片时，随机分割补丁常包含无信息或噪声区域，导致特征学习效果受限。作者希望通过加入结构和生物相关性引导，提高自监督学习的有效性。

Method: WISE-MAE采用“由粗到细”策略：首先在低倍率下使用小波变换筛选结构丰富区域，然后在高分辨率下提取补丁，供MAE预训练使用，模拟病理医生诊断流程。

Result: 在肺癌、肾癌、结直肠癌等多组公开数据集上测试，WISE-MAE表现出与主流方法相当甚至更优的特征表示和分类性能，并在弱监督条件下保持高效率。

Conclusion: 通过结构化补丁选择，WISE-MAE能够更好地学习组织学有意义特征，在数字病理自监督学习中具有很高的应用潜力。

Abstract: Whole-slide images are central to digital pathology, yet their extreme size
and scarce annotations make self-supervised learning essential. Masked
Autoencoders (MAEs) with Vision Transformer backbones have recently shown
strong potential for histopathology representation learning. However,
conventional random patch sampling during MAE pretraining often includes
irrelevant or noisy regions, limiting the model's ability to capture meaningful
tissue patterns. In this paper, we present a lightweight and domain-adapted
framework that brings structure and biological relevance into MAE-based
learning through a wavelet-informed patch selection strategy. WISE-MAE applies
a two-step coarse-to-fine process: wavelet-based screening at low magnification
to locate structurally rich regions, followed by high-resolution extraction for
detailed modeling. This approach mirrors the diagnostic workflow of
pathologists and improves the quality of learned representations. Evaluations
across multiple cancer datasets, including lung, renal, and colorectal tissues,
show that WISE-MAE achieves competitive representation quality and downstream
classification performance while maintaining efficiency under weak supervision.

</details>


### [179] [Exploring the "Great Unseen" in Medieval Manuscripts: Instance-Level Labeling of Legacy Image Collections with Zero-Shot Models](https://arxiv.org/abs/2511.07004)
*Christofer Meinecke,Estelle Guéville,David Joseph Wrisley*

Main category: cs.CV

TL;DR: 本论文提出了一个更全面的理论框架，利用最新技术对中世纪手稿的页面进行分割与描述，以提升计算机视觉模型对这种特定内容的训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究在处理中世纪手稿时，往往只关注部分内容，忽视了整体页面的复杂性，缺乏系统性的训练数据，限制了计算机视觉模型的表现，因此需要更全面的标注和理论框架。

Method: 采用先进的实例分割技术，对整页中世纪手稿进行了细致分割与多模态描述，生成更为丰富和精准的训练数据，为后续计算机视觉模型和多模态模型提供支持。

Result: 实验结果表明，丰富的全页面标注数据，有效提升了实例分割与多模态视觉模型在处理中世纪手稿内容时的表现。

Conclusion: 系统性全页分割与描述方法能够促进相关AI模型对中世纪手稿的理解与分析，为数字人文研究提供技术支持。

Abstract: We aim to theorize the medieval manuscript page and its contents more
holistically, using state-of-the-art techniques to segment and describe the
entire manuscript folio, for the purpose of creating richer training data for
computer vision techniques, namely instance segmentation, and multimodal models
for medieval-specific visual content.

</details>


### [180] [TrueCity: Real and Simulated Urban Data for Cross-Domain 3D Scene Understanding](https://arxiv.org/abs/2511.07007)
*Duc Nguyen,Yan-Ling Lai,Qilin Zhang,Prabin Gyawali,Benedikt Schwab,Olaf Wysocki,Thomas H. Kolbe*

Main category: cs.CV

TL;DR: 本文提出了TrueCity，这是首个同时包含厘米级标注真实点云、三维城市模型与一致模拟点云的城市级三维语义分割基准，用于研究和量化合成到真实领域的迁移问题。


<details>
  <summary>Details</summary>
Motivation: 由于三维场景理解需依赖大量高质量标注数据，现实数据难以取得且存在标签匮乏，通常采用合成数据补充，但合成数据与真实环境存在域差距，且缺乏可对比研究此差距的公开基准。

Method: 作者构建了TrueCity数据集，其中包含高度精确的真实城市点云及其与国际标准对齐的分割标签，还提供了仿真的点云（来自同一城市）及三维语义模型。通过基线实验评估并量化了合成与真实数据之间的域差异，并探讨了利用合成数据提升真实场景理解的策略。

Result: 实验结果表明，TrueCity能够有效衡量和分析合成到真实的迁移效果。基线方法下，合成数据可在一定程度提升真实三维场景理解，但域差异依然明显。提出的对齐分类体系也便于标准化评估。

Conclusion: TrueCity作为首个此类基准，有助于量化和缓解三维语义分割中的合成-真实域差异问题，支持更具普适性的3D场景理解方法的发展，为领域相关研究提供标准化数据和评价平台。

Abstract: 3D semantic scene understanding remains a long-standing challenge in the 3D
computer vision community. One of the key issues pertains to limited real-world
annotated data to facilitate generalizable models. The common practice to
tackle this issue is to simulate new data. Although synthetic datasets offer
scalability and perfect labels, their designer-crafted scenes fail to capture
real-world complexity and sensor noise, resulting in a synthetic-to-real domain
gap. Moreover, no benchmark provides synchronized real and simulated point
clouds for segmentation-oriented domain shift analysis. We introduce TrueCity,
the first urban semantic segmentation benchmark with cm-accurate annotated
real-world point clouds, semantic 3D city models, and annotated simulated point
clouds representing the same city. TrueCity proposes segmentation classes
aligned with international 3D city modeling standards, enabling consistent
evaluation of synthetic-to-real gap. Our extensive experiments on common
baselines quantify domain shift and highlight strategies for exploiting
synthetic data to enhance real-world 3D scene understanding. We are convinced
that the TrueCity dataset will foster further development of sim-to-real gap
quantification and enable generalizable data-driven models. The data, code, and
3D models are available online: https://tum-gis.github.io/TrueCity/

</details>


### [181] [Performance Decay in Deepfake Detection: The Limitations of Training on Outdated Data](https://arxiv.org/abs/2511.07009)
*Jack Richings,Margaux Leblanc,Ian Groves,Victoria Nockles*

Main category: cs.CV

TL;DR: 本文提出了一种两阶段的深度伪造检测方法，在现有深度伪造数据上取得了超过99.8%的AUROC表现，但很快随着新技术的出现，模型性能下降明显，表明检测面临严重挑战。


<details>
  <summary>Details</summary>
Motivation: 随着深度伪造技术发展，虚假信息、诈骗和骚扰的威胁加剧，检测技术亟需提升。

Method: 提出了一种简单有效的两阶段检测方法，主要利用静态帧层面的特征来判别伪造内容。

Result: 该方法在最新的深度伪造上取得了极高的检测准确率（AUROC超99.8%），但在六个月后出现的新型伪造内容上召回率下降30%以上。

Conclusion: 稳健的深度伪造检测需要不断收集和扩充多样化的数据集，同时未来应重点发展高效的帧级特征检测器。

Abstract: The continually advancing quality of deepfake technology exacerbates the
threats of disinformation, fraud, and harassment by making
maliciously-generated synthetic content increasingly difficult to distinguish
from reality. We introduce a simple yet effective two-stage detection method
that achieves an AUROC of over 99.8% on contemporary deepfakes. However, this
high performance is short-lived. We show that models trained on this data
suffer a recall drop of over 30% when evaluated on deepfakes created with
generation techniques from just six months later, demonstrating significant
decay as threats evolve. Our analysis reveals two key insights for robust
detection. Firstly, continued performance requires the ongoing curation of
large, diverse datasets. Second, predictive power comes primarily from static,
frame-level artifacts, not temporal inconsistencies. The future of effective
deepfake detection therefore depends on rapid data collection and the
development of advanced frame-level feature detectors.

</details>


### [182] [Certified L2-Norm Robustness of 3D Point Cloud Recognition in the Frequency Domain](https://arxiv.org/abs/2511.07029)
*Liang Zhou,Qiming Wang,Tianze Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种针对3D点云分类的新型频域认证防御方法FreqCert，相较于以往在空间域的防御方式，频域方法可增强对结构化扰动的鲁棒性，并在实验中取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 3D点云分类广泛应用于自动驾驶、机器人等安全关键场景，但现有模型易受结构化对抗扰动和几何畸变影响，现有认证防御主要限制于点级扰动，难以应对整体或结构级扰动，影响实际部署安全。

Method: FreqCert方法采用图傅立叶变换（GFT）将点云投影到频域，并基于频谱相似性进行亚点云的结构化采样，而非传统的空间邻近方式。各亚点云经独立分类，最终结果通过投票得出。理论上，作者推导了在最小假设下L2鲁棒性认证半径的闭式下界，并证明其紧致性。

Result: 在ModelNet40和ScanObjectNN数据集上，FreqCert在面对强扰动时，认证精度和实际精度均超过现有方法。

Conclusion: 频域表征为3D点云分类提供有效的可认证鲁棒性路径，FreqCert能在结构化扰动下提升鲁棒性，理论和实验效果均优于传统方法。

Abstract: 3D point cloud classification is a fundamental task in safety-critical
applications such as autonomous driving, robotics, and augmented reality.
However, recent studies reveal that point cloud classifiers are vulnerable to
structured adversarial perturbations and geometric corruptions, posing risks to
their deployment in safety-critical scenarios. Existing certified defenses
limit point-wise perturbations but overlook subtle geometric distortions that
preserve individual points yet alter the overall structure, potentially leading
to misclassification. In this work, we propose FreqCert, a novel certification
framework that departs from conventional spatial domain defenses by shifting
robustness analysis to the frequency domain, enabling structured certification
against global L2-bounded perturbations. FreqCert first transforms the input
point cloud via the graph Fourier transform (GFT), then applies structured
frequency-aware subsampling to generate multiple sub-point clouds. Each
sub-cloud is independently classified by a standard model, and the final
prediction is obtained through majority voting, where sub-clouds are
constructed based on spectral similarity rather than spatial proximity, making
the partitioning more stable under L2 perturbations and better aligned with the
object's intrinsic structure. We derive a closed-form lower bound on the
certified L2 robustness radius and prove its tightness under minimal and
interpretable assumptions, establishing a theoretical foundation for frequency
domain certification. Extensive experiments on the ModelNet40 and ScanObjectNN
datasets demonstrate that FreqCert consistently achieves higher certified
accuracy and empirical accuracy under strong perturbations. Our results suggest
that spectral representations provide an effective pathway toward certifiable
robustness in 3D point cloud recognition.

</details>


### [183] [3D-ANC: Adaptive Neural Collapse for Robust 3D Point Cloud Recognition](https://arxiv.org/abs/2511.07040)
*Yuanmin Huang,Wenxuan Li,Mi Zhang,Xiaohan Zhang,Xiaoyu You,Min Yang*

Main category: cs.CV

TL;DR: 本文提出一种针对3D点云识别神经网络的鲁棒性增强方法3D-ANC，有效提升其抗对抗攻击能力，并在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 3D点云识别的深度神经网络虽然表现优异，但对对抗扰动极为脆弱，现有防御方法难以应对多样化的攻击模式，亟需新的泛化且有效的防御机制。

Method: 作者分析了现有防御机制的瓶颈后，提出利用Neural Collapse（神经坍塌）机制，结合ETF（等角紧框）的特性，指导判别性特征学习。针对点云存在类别不平衡与类间几何相似难题，设计了ETF对齐分类模块及包含表征均衡学习（RBL）与动态特征方向损失（FDL）的自适应训练框架。该方案可无缝集成至现有3D点云模型中。

Result: 3D-ANC方法在多个结构的模型与两个常用点云数据集上进行了评估，显著提升模型鲁棒性。例如，在ModelNet40上，DGCNN的抗攻击分类准确率由27.2%提升至80.9%，相较主流基线高出34%。

Conclusion: 3D-ANC有效提升了3D点云模型抗对抗攻击的能力，为实际应用中的安全性提供了新思路，且具备较好的通用性及集成能力。

Abstract: Deep neural networks have recently achieved notable progress in 3D point
cloud recognition, yet their vulnerability to adversarial perturbations poses
critical security challenges in practical deployments. Conventional defense
mechanisms struggle to address the evolving landscape of multifaceted attack
patterns. Through systematic analysis of existing defenses, we identify that
their unsatisfactory performance primarily originates from an entangled feature
space, where adversarial attacks can be performed easily. To this end, we
present 3D-ANC, a novel approach that capitalizes on the Neural Collapse (NC)
mechanism to orchestrate discriminative feature learning. In particular, NC
depicts where last-layer features and classifier weights jointly evolve into a
simplex equiangular tight frame (ETF) arrangement, establishing maximally
separable class prototypes. However, leveraging this advantage in 3D
recognition confronts two substantial challenges: (1) prevalent class imbalance
in point cloud datasets, and (2) complex geometric similarities between object
categories. To tackle these obstacles, our solution combines an ETF-aligned
classification module with an adaptive training framework consisting of
representation-balanced learning (RBL) and dynamic feature direction loss
(FDL). 3D-ANC seamlessly empowers existing models to develop disentangled
feature spaces despite the complexity in 3D data distribution. Comprehensive
evaluations state that 3D-ANC significantly improves the robustness of models
with various structures on two datasets. For instance, DGCNN's classification
accuracy is elevated from 27.2% to 80.9% on ModelNet40 -- a 53.7% absolute gain
that surpasses leading baselines by 34.0%.

</details>


### [184] [From Pretrain to Pain: Adversarial Vulnerability of Video Foundation Models Without Task Knowledge](https://arxiv.org/abs/2511.07049)
*Hui Lu,Yi Yu,Song Xia,Yiming Yang,Deepu Rajan,Boon Poh Ng,Alex Kot,Xudong Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种针对大规模开源视频基础模型（VFMs）的新型可迁移性对抗攻击方案，并通过实验验证了其对下游任务和多模态大模型的威胁。


<details>
  <summary>Details</summary>
Motivation: 随着开源视频基础模型在各类视频任务中的广泛应用，其完整开源特性带来了严重的安全风险，攻击者可以借机发动威胁。然而，目前针对这类模型的现实攻击场景研究较少，尤其是在无下游任务、训练数据、模型查询和架构信息的情况下如何发起有效攻击尚不明确。

Method: 作者提出了Transferable Video Attack（TVA），这是一种考虑时序特性的对抗攻击方法。TVA利用双向对比学习机制增强干净特征与对抗特征的差异，并引入时序一致性损失，结合运动信息增强扰动的时序影响力。该方法无需训练昂贵的代理模型或访问领域特定数据，具有很强的实用性和高效性。

Result: 通过在24个视频相关任务上的大量实验，TVA在针对下游模型和多模态大模型领域展现出显著的攻击效能，能够大幅度削弱这些模型在广泛任务中的性能。

Conclusion: TVA方法揭示了开源视频模型部署中被忽视的安全漏洞，提醒相关领域重视VFMs在实际应用中的对抗鲁棒性问题，亟需设计更安全的视频基础模型。

Abstract: Large-scale Video Foundation Models (VFMs) has significantly advanced various
video-related tasks, either through task-specific models or Multi-modal Large
Language Models (MLLMs). However, the open accessibility of VFMs also
introduces critical security risks, as adversaries can exploit full knowledge
of the VFMs to launch potent attacks. This paper investigates a novel and
practical adversarial threat scenario: attacking downstream models or MLLMs
fine-tuned from open-source VFMs, without requiring access to the victim task,
training data, model query, and architecture. In contrast to conventional
transfer-based attacks that rely on task-aligned surrogate models, we
demonstrate that adversarial vulnerabilities can be exploited directly from the
VFMs. To this end, we propose the Transferable Video Attack (TVA), a
temporal-aware adversarial attack method that leverages the temporal
representation dynamics of VFMs to craft effective perturbations. TVA
integrates a bidirectional contrastive learning mechanism to maximize the
discrepancy between the clean and adversarial features, and introduces a
temporal consistency loss that exploits motion cues to enhance the sequential
impact of perturbations. TVA avoids the need to train expensive surrogate
models or access to domain-specific data, thereby offering a more practical and
efficient attack strategy. Extensive experiments across 24 video-related tasks
demonstrate the efficacy of TVA against downstream models and MLLMs, revealing
a previously underexplored security vulnerability in the deployment of video
models.

</details>


### [185] [Improving Deepfake Detection with Reinforcement Learning-Based Adaptive Data Augmentation](https://arxiv.org/abs/2511.07051)
*Yuxuan Zhou,Tao Yu,Wen Huang,Yuheng Zhang,Tao Dai,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 本文提出了一种新方法CRDA，通过动态、递进的数据增强策略，有效提升了deepfake检测器在跨域环境下的泛化能力，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有deepfake检测器的数据增强方法多采用固定策略，难以应对不断变化和日益复杂的伪造特征，导致模型泛化能力不足。论文旨在探索一种能够动态适应不同伪造风格、提升检测器实际应用泛化能力的方法。

Method: 提出CRDA框架：结合强化学习与因果推断，动态选择伪造操作进行数据增强。强化学习代理根据检测器当前表现调整增强策略，递进生成从简单到复杂的多域伪造特征样本；同时引入因果推断，抑制与检测无关的偏置，聚焦于因果不变的伪造特征。

Result: 通过跨域数据集的大量实验证明，CRDA方法相比当前SOTA方法显著提升了deepfake检测模型的泛化能力，在多种数据集上取得更优性能。

Conclusion: CRDA框架通过动态、灵活、因果驱动的数据增强，有效应对真实伪造环境的复杂多变性，提升模型泛化能力，对实际deepfake检测具有重要意义。

Abstract: The generalization capability of deepfake detectors is critical for
real-world use. Data augmentation via synthetic fake face generation
effectively enhances generalization, yet current SoTA methods rely on fixed
strategies-raising a key question: Is a single static augmentation sufficient,
or does the diversity of forgery features demand dynamic approaches? We argue
existing methods overlook the evolving complexity of real-world forgeries
(e.g., facial warping, expression manipulation), which fixed policies cannot
fully simulate. To address this, we propose CRDA (Curriculum
Reinforcement-Learning Data Augmentation), a novel framework guiding detectors
to progressively master multi-domain forgery features from simple to complex.
CRDA synthesizes augmented samples via a configurable pool of forgery
operations and dynamically generates adversarial samples tailored to the
detector's current learning state. Central to our approach is integrating
reinforcement learning (RL) and causal inference. An RL agent dynamically
selects augmentation actions based on detector performance to efficiently
explore the vast augmentation space, adapting to increasingly challenging
forgeries. Simultaneously, the agent introduces action space variations to
generate heterogeneous forgery patterns, guided by causal inference to mitigate
spurious correlations-suppressing task-irrelevant biases and focusing on
causally invariant features. This integration ensures robust generalization by
decoupling synthetic augmentation patterns from the model's learned
representations. Extensive experiments show our method significantly improves
detector generalizability, outperforming SOTA methods across multiple
cross-domain datasets.

</details>


### [186] [RaLD: Generating High-Resolution 3D Radar Point Clouds with Latent Diffusion](https://arxiv.org/abs/2511.07067)
*Ruijie Zhang,Bixin Zeng,Shengpeng Wang,Fuhui Zhou,Wei Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的生成框架RaLD，显著提升毫米波雷达点云的稠密性与准确性。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达虽然成本低、抗干扰，但点云稀疏且分辨率低，难以满足自动驾驶等高精度3D感知需求。当前主流生成方法主要基于稠密体素，存在效率低和结构细节丢失问题，需寻求更优方案。

Method: 作者提出RaLD框架，利用场景级的基于视锥的LiDAR自动编码、顺序无关的潜在表征以及直接基于雷达原始频谱的条件生成，将潜在扩散模型（LDM）首次高效引入雷达3D生成任务。

Result: 实验结果显示，RaLD能够从原始雷达频谱生成稠密且准确的3D点云，在保留结构细节的同时提升生成效率。

Conclusion: RaLD为复杂环境下的毫米波雷达3D感知提供了一种高效、鲁棒的新颖解决方案，有望推动相关自动驾驶和机器人感知技术的发展。

Abstract: Millimeter-wave radar offers a promising sensing modality for autonomous
systems thanks to its robustness in adverse conditions and low cost. However,
its utility is significantly limited by the sparsity and low resolution of
radar point clouds, which poses challenges for tasks requiring dense and
accurate 3D perception. Despite that recent efforts have shown great potential
by exploring generative approaches to address this issue, they often rely on
dense voxel representations that are inefficient and struggle to preserve
structural detail. To fill this gap, we make the key observation that latent
diffusion models (LDMs), though successful in other modalities, have not been
effectively leveraged for radar-based 3D generation due to a lack of compatible
representations and conditioning strategies. We introduce RaLD, a framework
that bridges this gap by integrating scene-level frustum-based LiDAR
autoencoding, order-invariant latent representations, and direct radar spectrum
conditioning. These insights lead to a more compact and expressive generation
process. Experiments show that RaLD produces dense and accurate 3D point clouds
from raw radar spectrums, offering a promising solution for robust perception
in challenging environments.

</details>


### [187] [ClusterMine: Robust Label-Free Visual Out-Of-Distribution Detection via Concept Mining from Text Corpora](https://arxiv.org/abs/2511.07068)
*Nikolas Adaloglou,Diana Petrusheva,Mohamed Asker,Felix Michels,Markus Kollmann*

Main category: cs.CV

TL;DR: 本文提出了一种无需预先定义正类标签名的新方法ClusterMine，用于实现完全无监督的大规模视觉OOD检测，并在多个CLIP模型下取得了最优效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-语言模型（如CLIP）的OOD检测方法依赖于固定的ID标签名，这些标签在实际大规模应用中常常难以获得、可能不可靠，或在部署后由于分布变化变得不相关，严重限制了方法的实用性和鲁棒性。

Method: 作者提出无监督的正类标签挖掘框架，通过在大规模文本语料库中自动挖掘正类标签并结合视觉样本聚类和零样本图文一致性，提出ClusterMine方法，无需真值正类标签即可进行正类概念提取与OOD检测。

Result: ClusterMine在多个CLIP模型和不同类型分布变化下实现了最优性能，展现出良好的可扩展性和对分布漂移的鲁棒性。

Conclusion: ClusterMine显著降低了视觉OOD检测对监督信息的依赖，在实际大规模和分布不稳定的场景中具有重要应用前景和推广价值。

Abstract: Large-scale visual out-of-distribution (OOD) detection has witnessed
remarkable progress by leveraging vision-language models such as CLIP. However,
a significant limitation of current methods is their reliance on a pre-defined
set of in-distribution (ID) ground-truth label names (positives). These fixed
label names can be unavailable, unreliable at scale, or become less relevant
due to in-distribution shifts after deployment. Towards truly unsupervised OOD
detection, we utilize widely available text corpora for positive label mining,
bypassing the need for positives. In this paper, we utilize widely available
text corpora for positive label mining under a general concept mining paradigm.
Within this framework, we propose ClusterMine, a novel positive label mining
method. ClusterMine is the first method to achieve state-of-the-art OOD
detection performance without access to positive labels. It extracts positive
concepts from a large text corpus by combining visual-only sample consistency
(via clustering) and zero-shot image-text consistency. Our experimental study
reveals that ClusterMine is scalable across a plethora of CLIP models and
achieves state-of-the-art robustness to covariate in-distribution shifts. The
code is available at https://github.com/HHU-MMBS/clustermine_wacv_official.

</details>


### [188] [LeCoT: revisiting network architecture for two-view correspondence pruning](https://arxiv.org/abs/2511.07078)
*Luanyuan Dai,Xiaoyu Du,Jinhui Tang*

Main category: cs.CV

TL;DR: 本文针对两视图对应点去除错误匹配问题，提出了新的Transformer网络LeCoT，通过创新性设计有效提升全局上下文信息建模能力，在多项计算机视觉任务中均取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有两视图对应点剔除方法多基于MLP主干，但MLP对全局上下文信息建模能力有限，通常需额外设计辅助模块。为简化结构并提升性能，作者关注于直接提升网络自然获取全局上下文的能力。

Method: 提出LeCoT网络，其核心为新颖的空间-通道融合Transformer块（Spatial-Channel Fusion Transformer block），可高效融合稀疏对应点的空间与通道的全局上下文信息。同时引入多阶段概率集预测模块，在中间阶段产生概率指导后续阶段学习，实现概率信息的逐步细致优化，减缓传统方法的信息损失问题。

Result: LeCoT在多项任务（对应点剔除、相对姿态估计、单应性估计、视觉定位、三维重建等）上的表现超越了当前先进方法。

Conclusion: LeCoT利用创新的Transformer设计和分阶段概率指导，有效提升了两视图对应点剔除的准确性与鲁棒性，方法结构简洁且适应多种下游视觉任务，实验验证了其优越性能。

Abstract: Two-view correspondence pruning aims to accurately remove incorrect
correspondences (outliers) from initial ones and is widely applied to various
computer vision tasks. Current popular strategies adopt multilayer perceptron
(MLP) as the backbone, supplemented by additional modules to enhance the
network ability to handle context information, which is a known limitation of
MLPs. In contrast, we introduce a novel perspective for capturing
correspondence context information without extra design modules. To this end,
we design a two-view correspondence pruning network called LeCoT, which can
naturally leverage global context information at different stages.
Specifically, the core design of LeCoT is the Spatial-Channel Fusion
Transformer block, a newly proposed component that efficiently utilizes both
spatial and channel global context information among sparse correspondences. In
addition, we integrate the proposed prediction block that utilizes
correspondence features from intermediate stages to generate a probability set,
which acts as guiding information for subsequent learning phases, allowing the
network to more effectively capture robust global context information. Notably,
this prediction block progressively refines the probability set, thereby
mitigating the issue of information loss that is common in the traditional one.
Extensive experiments prove that the proposed LeCoT outperforms
state-of-the-art methods in correspondence pruning, relative pose estimation,
homography estimation, visual localization, and $3$D~reconstruction tasks. The
code is provided in
https://github.com/Dailuanyuan2024/LeCoT-Revisiting-Network-Architecture-for-Two-View-Correspondence-Pruning.

</details>


### [189] [Pandar128 dataset for lane line detection](https://arxiv.org/abs/2511.07084)
*Filip Beránek,Václav Diviš,Ivan Gruber*

Main category: cs.CV

TL;DR: 本文发布了Pandar128，这是迄今为止最大的公开128线激光雷达车道线检测数据集，同时提出了轻量级基线算法和新的评测指标。相关数据和代码均已公开。


<details>
  <summary>Details</summary>
Motivation: 现有激光雷达车道线检测的数据资源受限，且评测方法缺乏标准化，阻碍了领域进步。因此需要高质量大规模数据集和标准评测体系。

Method: 1) 采集并发布包含52000+相机帧和34000+激光雷达扫描的数据集，附带完整传感器标定和同步里程计信息；2) 提出SimpleLidarLane基线，结合BEV分割、聚类和多项式拟合完成车道线重建；3) 提出新的基于折线插值的IAM-F1指标，用于更科学评估检测性能。

Result: SimpleLidarLane基线方法在多种复杂路况下（如雨天、点云稀疏）表现优良，验证了高质量、模块化流程和标准评测的有效性。

Conclusion: Pandar128数据集和配套方法及评测工具，为激光雷达车道线检测提供了坚实基础，将推动该领域算法发展和公平对比。

Abstract: We present Pandar128, the largest public dataset for lane line detection
using a 128-beam LiDAR. It contains over 52,000 camera frames and 34,000 LiDAR
scans, captured in diverse real-world conditions in Germany. The dataset
includes full sensor calibration (intrinsics, extrinsics) and synchronized
odometry, supporting tasks such as projection, fusion, and temporal modeling.
  To complement the dataset, we also introduce SimpleLidarLane, a light-weight
baseline method for lane line reconstruction that combines BEV segmentation,
clustering, and polyline fitting. Despite its simplicity, our method achieves
strong performance under challenging various conditions (e.g., rain, sparse
returns), showing that modular pipelines paired with high-quality data and
principled evaluation can compete with more complex approaches.
  Furthermore, to address the lack of standardized evaluation, we propose a
novel polyline-based metric - Interpolation-Aware Matching F1 (IAM-F1) - that
employs interpolation-aware lateral matching in BEV space.
  All data and code are publicly released to support reproducibility in
LiDAR-based lane detection.

</details>


### [190] [How Bias Binds: Measuring Hidden Associations for Bias Control in Text-to-Image Compositions](https://arxiv.org/abs/2511.07091)
*Jeng-Lin Li,Ming-Ching Chang,Wei-Chao Chen*

Main category: cs.CV

TL;DR: 现有的文本到图像生成模型在与敏感属性相关时容易产生偏见。目前多数研究仅关注单对象且上下文简单的提示词，但实际每个提示词中的对象或属性都可能引入偏见，且语义绑定（对象与属性的结合）会放大偏见。本文提出了度量对象-属性绑定偏见的新方法，并探索了基于token解耦的去偏框架，取得了显著改善。结果表明，目前主流去偏技术在处理复杂语义绑定时存在明显局限。


<details>
  <summary>Details</summary>
Motivation: 尽管去偏技术逐步发展，但在复杂、有上下文交互的语义绑定情境下，生成模型的偏见问题未被充分关注与解决。本文动机在于揭示提示词中对象-属性的联合作用如何放大偏见，并探索更有效的去偏策略。

Method: 提出了一种偏见依附评分（bias adherence score）来量化特定对象-属性绑定偏见激活程度，并设计了无训练的上下文去偏控制框架，通过token解耦的方法，实现了在不训练模型情况下的去偏实验。

Result: 所提出方法在组合生成任务上去偏表现提升超过10%。对比了不同绑定形态下偏见分布和token去相关的效果，发现去偏与保持语义关系之间存在权衡。

Conclusion: 本文揭示了在存在语义绑定时的生成模型偏见问题及主流去偏技术的局限性，强调了重新审视去偏策略的必要性。

Abstract: Text-to-image generative models often exhibit bias related to sensitive
attributes. However, current research tends to focus narrowly on single-object
prompts with limited contextual diversity. In reality, each object or attribute
within a prompt can contribute to bias. For example, the prompt "an assistant
wearing a pink hat" may reflect female-inclined biases associated with a pink
hat. The neglected joint effects of the semantic binding in the prompts cause
significant failures in current debiasing approaches. This work initiates a
preliminary investigation on how bias manifests under semantic binding, where
contextual associations between objects and attributes influence generative
outcomes. We demonstrate that the underlying bias distribution can be amplified
based on these associations. Therefore, we introduce a bias adherence score
that quantifies how specific object-attribute bindings activate bias. To delve
deeper, we develop a training-free context-bias control framework to explore
how token decoupling can facilitate the debiasing of semantic bindings. This
framework achieves over 10% debiasing improvement in compositional generation
tasks. Our analysis of bias scores across various attribute-object bindings and
token decorrelation highlights a fundamental challenge: reducing bias without
disrupting essential semantic relationships. These findings expose critical
limitations in current debiasing approaches when applied to semantically bound
contexts, underscoring the need to reassess prevailing bias mitigation
strategies.

</details>


### [191] [GEWDiff: Geometric Enhanced Wavelet-based Diffusion Model for Hyperspectral Image Super-resolution](https://arxiv.org/abs/2511.07103)
*Sirui Wang,Jiang He,Natàlia Blasco Andreo,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种新型几何增强小波扩散模型（GEWDiff），用于实现高质量、高分辨率的高光谱图像重建，取得了多项指标的领先效果。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像（HSIs）超分辨率重建在遥感等领域极为重要，但HSIs高光谱维度导致的内存消耗过高，传统生成模型对地物拓扑和几何结构理解不足，同时现有扩散模型损失函数的噪声级优化方式收敛不直观、生成质量有限。需要一种兼顾高效性、结构保留和收敛性的新方法。

Method: 提出GEWDiff框架，引入小波编码-解码器，将HSIs压缩进潜空间以保存光谱-空间信息，有效提升内存效率。在生成过程中使用几何增强的扩散机制，保持几何特征不失真；同时设计多层次损失函数，引导扩散过程达到稳定收敛和高保真重建。该方法可实现4倍超分辨率重建。

Result: GEWDiff在多项关键指标（包括保真度、光谱准确性、视觉真实感和清晰度）上均达到了目前最优或领先水平。

Conclusion: GEWDiff模型有效解决了传统扩散模型在高光谱图像超分辨率生成中的内存开销大、结构丢失和收敛不稳定的问题，为高质量HSI重建提供了新的范式。

Abstract: Improving the quality of hyperspectral images (HSIs), such as through
super-resolution, is a crucial research area. However, generative modeling for
HSIs presents several challenges. Due to their high spectral dimensionality,
HSIs are too memory-intensive for direct input into conventional diffusion
models. Furthermore, general generative models lack an understanding of the
topological and geometric structures of ground objects in remote sensing
imagery. In addition, most diffusion models optimize loss functions at the
noise level, leading to a non-intuitive convergence behavior and suboptimal
generation quality for complex data. To address these challenges, we propose a
Geometric Enhanced Wavelet-based Diffusion Model (GEWDiff), a novel framework
for reconstructing hyperspectral images at 4-times super-resolution. A
wavelet-based encoder-decoder is introduced that efficiently compresses HSIs
into a latent space while preserving spectral-spatial information. To avoid
distortion during generation, we incorporate a geometry-enhanced diffusion
process that preserves the geometric features. Furthermore, a multi-level loss
function was designed to guide the diffusion process, promoting stable
convergence and improved reconstruction fidelity. Our model demonstrated
state-of-the-art results across multiple dimensions, including fidelity,
spectral accuracy, visual realism, and clarity.

</details>


### [192] [HENet++: Hybrid Encoding and Multi-task Learning for 3D Perception and End-to-end Autonomous Driving](https://arxiv.org/abs/2511.07106)
*Zhongyu Xia,Zhiwei Lin,Yongtao Wang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 本文提出了HENet与HENet++多任务三维感知框架，突破性地实现了高效、兼容多任务、资源友好性的3D特征提取与端到端自动驾驶系统，并在nuScenes基准测试上取得了最佳多任务性能和最低碰撞率。


<details>
  <summary>Details</summary>
Motivation: 现有3D特征提取方法在提升感知准确性的同时，常受限于计算资源，难以兼容大图像编码器、高分辨率和长时序输入。此外，多任务并行时模型难以同时保持多个任务的高精度，限制了一体化端到端自动驾驶系统的实际应用。为解决多任务兼容性和资源分配矛盾，亟需新型架构创新。

Method: 提出混合图像编码网络，对短期帧采用大规模图像编码器，长期帧采用小型编码器。设计可以同时提取稠密与稀疏特征的网络，针对不同任务采用最优特征表达。架构支持多模态输入，并与现有3D特征提取方法兼容。

Result: HENet++在nuScenes多任务3D感知基准测试上取得了最优性能，同时在端到端自动驾驶测试中实现了最低碰撞率。

Conclusion: HENet及HENet++能够有效整合多尺度和多时间维度信息，为多任务3D感知与端到端自动驾驶带来性能和兼容性大幅提升，推动自动驾驶整体系统发展。

Abstract: Three-dimensional feature extraction is a critical component of autonomous
driving systems, where perception tasks such as 3D object detection,
bird's-eye-view (BEV) semantic segmentation, and occupancy prediction serve as
important constraints on 3D features. While large image encoders,
high-resolution images, and long-term temporal inputs can significantly enhance
feature quality and deliver remarkable performance gains, these techniques are
often incompatible in both training and inference due to computational resource
constraints. Moreover, different tasks favor distinct feature representations,
making it difficult for a single model to perform end-to-end inference across
multiple tasks while maintaining accuracy comparable to that of single-task
models. To alleviate these issues, we present the HENet and HENet++ framework
for multi-task 3D perception and end-to-end autonomous driving. Specifically,
we propose a hybrid image encoding network that uses a large image encoder for
short-term frames and a small one for long-term frames. Furthermore, our
framework simultaneously extracts both dense and sparse features, providing
more suitable representations for different tasks, reducing cumulative errors,
and delivering more comprehensive information to the planning module. The
proposed architecture maintains compatibility with various existing 3D feature
extraction methods and supports multimodal inputs. HENet++ achieves
state-of-the-art end-to-end multi-task 3D perception results on the nuScenes
benchmark, while also attaining the lowest collision rate on the nuScenes
end-to-end autonomous driving benchmark.

</details>


### [193] [Sparse4DGS: 4D Gaussian Splatting for Sparse-Frame Dynamic Scene Reconstruction](https://arxiv.org/abs/2511.07122)
*Changyue Shi,Chuxiao Yang,Xinyuan Hu,Minghao Chen,Wenwen Pan,Yan Yang,Jiajun Ding,Zhou Yu,Jun Yu*

Main category: cs.CV

TL;DR: 本文提出了Sparse4DGS方法，实现了稀疏帧动态场景重建，并在多个数据集上取得了优异的效果。


<details>
  <summary>Details</summary>
Motivation: 当前的4D动态高斯体重建方法依赖于密集帧视频，但实际应用中常常只能获得稀疏帧。稀疏帧下现有方法性能显著下降，尤其在高纹理区域效果更差。为解决稀疏帧动态场景重建难题，作者提出了新方法。

Method: Sparse4DGS针对纹理丰富区域，通过两个关键创新提升重建性能：（1）在形变网络中提出了基于纹理的深度对齐损失（Texture-Aware Deformation Regularization），以规范高斯体的形变；（2）在Canonical Gaussian场中引入基于纹理的噪声梯度优化（Texture-Aware Canonical Optimization），提高优化效果。

Result: 实验表明，在NeRF-Synthetic、HyperNeRF、NeRF-DS和iPhone-4D等数据集上，Sparse4DGS在稀疏帧输入条件下，性能优于现有的动态重建和小样本重建方法。

Conclusion: Sparse4DGS显著提升了稀疏帧动态场景重建的质量，为实际应用中的动态场景重建提供了有效解决方案，拓展了4D重建方法的适用性。

Abstract: Dynamic Gaussian Splatting approaches have achieved remarkable performance
for 4D scene reconstruction. However, these approaches rely on dense-frame
video sequences for photorealistic reconstruction. In real-world scenarios, due
to equipment constraints, sometimes only sparse frames are accessible. In this
paper, we propose Sparse4DGS, the first method for sparse-frame dynamic scene
reconstruction. We observe that dynamic reconstruction methods fail in both
canonical and deformed spaces under sparse-frame settings, especially in areas
with high texture richness. Sparse4DGS tackles this challenge by focusing on
texture-rich areas. For the deformation network, we propose Texture-Aware
Deformation Regularization, which introduces a texture-based depth alignment
loss to regulate Gaussian deformation. For the canonical Gaussian field, we
introduce Texture-Aware Canonical Optimization, which incorporates
texture-based noise into the gradient descent process of canonical Gaussians.
Extensive experiments show that when taking sparse frames as inputs, our method
outperforms existing dynamic or few-shot techniques on NeRF-Synthetic,
HyperNeRF, NeRF-DS, and our iPhone-4D datasets.

</details>


### [194] [MPJudge: Towards Perceptual Assessment of Music-Induced Paintings](https://arxiv.org/abs/2511.07137)
*Shiqi Jiang,Tianyi Liang,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: 本文提出了一种创新方法，通过直接建模音乐与绘画间的感知一致性，更准确地评估音乐诱导绘画的相关性，并引入大规模的专家标注数据集与首个专用模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于情感识别的音乐-绘画相关性评估方法噪声大，局限于情感线索，不能很好反映音乐与绘画感知一致性的本质需求。

Method: 作者构建了MPD大规模音乐-绘画配对数据集，包含专家标注的感知一致性标签与成对偏好信息。基于该数据集，提出了MPJudge模型，通过调制型融合机制将音乐特征融入视觉编码器，并采用直接偏好优化方法训练模型，有效处理含糊样本。

Result: 实验表明，所提方法比现有方法更优，能更精确地识别画作中与音乐相关的区域，并在定量和定性测试中表现突出。

Conclusion: 通过直接建模音乐与绘画的感知一致性，结合大规模标注数据和创新模型，极大提升了音乐诱导绘画相关性评估的准确性与实用性。

Abstract: Music induced painting is a unique artistic practice, where visual artworks
are created under the influence of music. Evaluating whether a painting
faithfully reflects the music that inspired it poses a challenging perceptual
assessment task. Existing methods primarily rely on emotion recognition models
to assess the similarity between music and painting, but such models introduce
considerable noise and overlook broader perceptual cues beyond emotion. To
address these limitations, we propose a novel framework for music induced
painting assessment that directly models perceptual coherence between music and
visual art. We introduce MPD, the first large scale dataset of music painting
pairs annotated by domain experts based on perceptual coherence. To better
handle ambiguous cases, we further collect pairwise preference annotations.
Building on this dataset, we present MPJudge, a model that integrates music
features into a visual encoder via a modulation based fusion mechanism. To
effectively learn from ambiguous cases, we adopt Direct Preference Optimization
for training. Extensive experiments demonstrate that our method outperforms
existing approaches. Qualitative results further show that our model more
accurately identifies music relevant regions in paintings.

</details>


### [195] [ProcGen3D: Learning Neural Procedural Graph Representations for Image-to-3D Reconstruction](https://arxiv.org/abs/2511.07142)
*Xinyi Zhang,Daoyi Gao,Naiqi Li,Angela Dai*

Main category: cs.CV

TL;DR: 提出了一种基于程序图的新型3D内容生成方法ProcGen3D，该方法通过图式抽象生成复杂3D对象，实现了更高效且图像对齐的3D重建。


<details>
  <summary>Details</summary>
Motivation: 现有3D对象生成方法在多样性、细节与对输入图像的对齐方面仍有不足，而传统程序生成在实际3D建模应用中表现优秀。因此亟需将程序化生成与学习方法结合，提升复杂3D资产的自动生成能力和对输入的还原效果。

Method: 1）以序列化的程序图抽象表示3D资产，用边为单位进行token化编码；2）训练变换器模型，预测下一个token，条件为输入的RGB图像；3）利用蒙特卡洛树搜索（MCTS）引导采样，增强生成程序图与输入图像的匹配度。

Result: 在多类物体（仙人掌、树、桥）上的实验显示，该方法的神经程序图生成在精度和多样性上超越其他SOTA生成式3D方法及领域建模技术，并在仅用合成数据训练时对真实图片仍具较好泛化能力。

Conclusion: ProcGen3D能高效生成与输入图片高度一致的复杂3D内容，为3D资产生成提供了一种新范式，显示出优异的通用性和实用性，尤其适用于仅有合成数据的场景。

Abstract: We introduce ProcGen3D, a new approach for 3D content creation by generating
procedural graph abstractions of 3D objects, which can then be decoded into
rich, complex 3D assets. Inspired by the prevalent use of procedural generators
in production 3D applications, we propose a sequentialized, graph-based
procedural graph representation for 3D assets. We use this to learn to
approximate the landscape of a procedural generator for image-based 3D
reconstruction. We employ edge-based tokenization to encode the procedural
graphs, and train a transformer prior to predict the next token conditioned on
an input RGB image. Crucially, to enable better alignment of our generated
outputs to an input image, we incorporate Monte Carlo Tree Search (MCTS) guided
sampling into our generation process, steering output procedural graphs towards
more image-faithful reconstructions. Our approach is applicable across a
variety of objects that can be synthesized with procedural generators.
Extensive experiments on cacti, trees, and bridges show that our neural
procedural graph generation outperforms both state-of-the-art generative 3D
methods and domain-specific modeling techniques. Furthermore, this enables
improved generalization on real-world input images, despite training only on
synthetic data.

</details>


### [196] [Federated Learning for Video Violence Detection: Complementary Roles of Lightweight CNNs and Vision-Language Models for Energy-Efficient Use](https://arxiv.org/abs/2511.07171)
*Sébastien Thuau,Siba Haidar,Rachid Chelouah*

Main category: cs.CV

TL;DR: 本文比较了三种在分布式视频暴力检测任务中的方法，并评估了它们的准确率、能耗和碳排放。所有方法在暴力检测上准确率均超90%。3D CNN方法能耗最低；大模型多模态推理能力更强。提出基于高效CNN和选择性VLM结合的混合部署建议。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习在视频监控中的广泛应用，用户越来越关注隐私保护和环境可持续性。传统的中心化训练有泄露隐私与能耗高的问题。虽然联邦学习能保护数据隐私，但引入大型视觉-语言模型又会带来更高的能耗与碳排放。因此，需要系统性地比较不同模型在隐私保护、性能与能耗上的权衡。

Method: 作者在两个真实分布（non-IID）的视频暴力检测数据集（RWF-2000与RLVS）上，比较了三种方法：1）使用预训练VLMs进行零样本推理；2）基于LoRA微调的LLaVA-NeXT-Video-7B联邦学习；3）个性化的65.8M参数3D CNN联邦学习。同时，对能耗和二氧化碳排放进行了定量比较，并通过层次化类别分组提升VLM多类检测准确率。

Result: 所有方法在二分类暴力检测任务中准确率均超过90%。3D CNN在能耗和校准性指标（ROC AUC 92.59%）上表现更优，仅为LoRA方法能耗的一半。VLM方法具备更强的多模态推理能力，经类别分组在UCF-Crime多类别检测准确率由65.31%提升至81%。

Conclusion: 对于日常检测，应优先采用高效能的3D CNN模型以节省能耗；对于复杂场景则可按需调用VLM实现更丰富的语境理解。本文首次比较了LoRA调优VLM与个性化CNN在联邦暴力检测中的表现，并量化了其能耗和碳排放，结果为现实场景下的混合部署策略提供了理论参考。

Abstract: Deep learning-based video surveillance increasingly demands
privacy-preserving architectures with low computational and environmental
overhead. Federated learning preserves privacy but deploying large
vision-language models (VLMs) introduces major energy and sustainability
challenges. We compare three strategies for federated violence detection under
realistic non-IID splits on the RWF-2000 and RLVS datasets: zero-shot inference
with pretrained VLMs, LoRA-based fine-tuning of LLaVA-NeXT-Video-7B, and
personalized federated learning of a 65.8M-parameter 3D CNN. All methods exceed
90% accuracy in binary violence detection. The 3D CNN achieves superior
calibration (ROC AUC 92.59%) at roughly half the energy cost (240 Wh vs. 570
Wh) of federated LoRA, while VLMs provide richer multimodal reasoning.
Hierarchical category grouping (based on semantic similarity and class
exclusion) boosts VLM multiclass accuracy from 65.31% to 81% on the UCF-Crime
dataset. To our knowledge, this is the first comparative simulation study of
LoRA-tuned VLMs and personalized CNNs for federated violence detection, with
explicit energy and CO2e quantification. Our results inform hybrid deployment
strategies that default to efficient CNNs for routine inference and selectively
engage VLMs for complex contextual reasoning.

</details>


### [197] [LiteUpdate: A Lightweight Framework for Updating AI-Generated Image Detectors](https://arxiv.org/abs/2511.07192)
*Jiajie Lu,Zhenkan Fu,Na Zhao,Long Xing,Kejiang Chen,Weiming Zhang,Nenghai Yu*

Main category: cs.CV

TL;DR: 本文提出LiteUpdate框架，有效提升AI生成图像检测器的更新效率和检测准确率，尤其针对新型生成模型。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的快速发展，新的生成模型不断涌现，导致原有的检测方法性能严重下降，因此急需一种高效并能适应新生成器的检测器更新机制。

Method: 提出LiteUpdate框架，包括代表性样本选择模块（依据置信度与梯度判别特征选择边界样本）和模型融合模块（融合多条微调路径下的权重）。该方法既提升对新分布的适应性，又缓解灾难性遗忘。

Result: 实验证明LiteUpdate能大幅提升多种检测器的性能，在AIDE平台上，Midjourney的平均检测准确率由87.63%提升至93.03%，相对提升6.16%。

Conclusion: LiteUpdate框架显著提升了AI生成图像检测器的更新效率与适应性，能更好跟上新生成模型的发展并缓解遗忘问题。

Abstract: The rapid progress of generative AI has led to the emergence of new
generative models, while existing detection methods struggle to keep pace,
resulting in significant degradation in the detection performance. This
highlights the urgent need for continuously updating AI-generated image
detectors to adapt to new generators. To overcome low efficiency and
catastrophic forgetting in detector updates, we propose LiteUpdate, a
lightweight framework for updating AI-generated image detectors. LiteUpdate
employs a representative sample selection module that leverages image
confidence and gradient-based discriminative features to precisely select
boundary samples. This approach improves learning and detection accuracy on new
distributions with limited generated images, significantly enhancing detector
update efficiency. Additionally, LiteUpdate incorporates a model merging module
that fuses weights from multiple fine-tuning trajectories, including
pre-trained, representative, and random updates. This balances the adaptability
to new generators and mitigates the catastrophic forgetting of prior knowledge.
Experiments demonstrate that LiteUpdate substantially boosts detection
performance in various detectors. Specifically, on AIDE, the average detection
accuracy on Midjourney improved from 87.63% to 93.03%, a 6.16% relative
increase.

</details>


### [198] [Automated Estimation of Anatomical Risk Metrics for Endoscopic Sinus Surgery Using Deep Learning](https://arxiv.org/abs/2511.07199)
*Konrad Reuter,Lennart Thaysen,Bilkay Doruk,Sarah Latus,Brigitte Holst,Benjamin Becker,Dennis Eggert,Christian Betz,Anna-Sophie Hoffmann,Alexander Schlaefer*

Main category: cs.CV

TL;DR: 本论文提出一种基于深度学习的自动化管道，用于在鼻窦手术前自动评估解剖风险分数，显著减少人工测量时间且准确率较高。


<details>
  <summary>Details</summary>
Motivation: 鼻窦内窥镜手术依赖于精准的颅底解剖评估，而现有的Keros、Gera和TMS等风险评估体系虽然标准化，却需要繁琐且耗时的手动CT/CBCT测量，效率低下。

Method: 提出了一个自动化深度学习管道，通过热图回归自动定位关键解剖点以估算风险分数。并比较了直接预测与全球到局部的分步学习策略。

Result: 系统在Keros、Gera、TMS测量上的平均绝对误差分别为0.506mm、4.516度和0.802mm/0.777mm，性能优良。

Conclusion: 该深度学习自动化方法可以快速准确地实现术前解剖风险评分，有助于提升手术安全性和效率。

Abstract: Endoscopic sinus surgery requires careful preoperative assessment of the
skull base anatomy to minimize risks such as cerebrospinal fluid leakage.
Anatomical risk scores like the Keros, Gera and Thailand-Malaysia-Singapore
score offer a standardized approach but require time-consuming manual
measurements on coronal CT or CBCT scans. We propose an automated deep learning
pipeline that estimates these risk scores by localizing key anatomical
landmarks via heatmap regression. We compare a direct approach to a specialized
global-to-local learning strategy and find mean absolute errors on the relevant
anatomical measurements of 0.506mm for the Keros, 4.516{\deg} for the Gera and
0.802mm / 0.777mm for the TMS classification.

</details>


### [199] [Geometric implicit neural representations for signed distance functions](https://arxiv.org/abs/2511.07206)
*Luiz Schirmer,Tiago Novello,Vinícius da Silva,Guilherme Schardong,Daniel Perazzo,Hélio Lopes,Nuno Gonçalves,Luiz Velho*

Main category: cs.CV

TL;DR: 本文综述了通过隐式神经表示（INR）近似带符号距离函数（SDF）以实现3D表面重建的研究进展，特别关注将微分几何工具（如法向量、曲率）引入损失函数的“几何INR”方法。


<details>
  <summary>Details</summary>
Motivation: 传统3D重建方法在处理稠密表面、高精度重建及复杂拓扑时存在局限。近年来，INR作为一种新的信号低维表示方法被广泛关注，将其用于SDF表达理论上可实现更好的几何一致性和全局约束。这激发了使用几何约束优化INR以提升重建精度和鲁棒性的研究热潮。

Method: 本文梳理了以神经网络参数化SDF的核心思想，重点介绍了如何从微分几何角度，将法向量、曲率等逻辑嵌入到损失函数中（即正则项），并总结了针对点云或有姿态的图像采样和训练方案。

Result: 引入几何约束的INR，不仅有效提升了SDF重建表面的准确性，还加强了模型对全局几何属性（如单位梯度）的满足能力。据综述所述，基于此方法的3D重建在点云和姿态图像数据下均取得了显著进展。

Conclusion: 几何INR方法通过将微分几何特征融入损失函数，显著提升了3D表面重建的质量与鲁棒性，为未来神经表示及其在三维感知领域的应用提供了坚实理论和实践基础。

Abstract: \textit{Implicit neural representations} (INRs) have emerged as a promising
framework for representing signals in low-dimensional spaces. This survey
reviews the existing literature on the specialized INR problem of approximating
\textit{signed distance functions} (SDFs) for surface scenes, using either
oriented point clouds or a set of posed images. We refer to neural SDFs that
incorporate differential geometry tools, such as normals and curvatures, in
their loss functions as \textit{geometric} INRs. The key idea behind this 3D
reconstruction approach is to include additional \textit{regularization} terms
in the loss function, ensuring that the INR satisfies certain global properties
that the function should hold -- such as having unit gradient in the case of
SDFs. We explore key methodological components, including the definition of
INR, the construction of geometric loss functions, and sampling schemes from a
differential geometry perspective. Our review highlights the significant
advancements enabled by geometric INRs in surface reconstruction from oriented
point clouds and posed images.

</details>


### [200] [Breaking the Stealth-Potency Trade-off in Clean-Image Backdoors with Generative Trigger Optimization](https://arxiv.org/abs/2511.07210)
*Binyan Xu,Fan Yang,Di Tang,Xilin Dai,Kehuan Zhang*

Main category: cs.CV

TL;DR: 本论文提出了生成式干净图像后门（GCB）方法，可利用条件InfoGAN自动寻找自然存在的图像特征作为隐蔽后门触发器，大幅减少传统后门攻击对模型干净准确率（CA）的影响。


<details>
  <summary>Details</summary>
Motivation: 现有干净图像后门攻击须较高污染率才能成功，导致清洁准确率明显下降，易被察觉，限制了其实用性及隐蔽性。作者希望研发能大幅降低CA下降的新型干净图像后门方法。

Method: 作者设计了GCB框架，利用条件InfoGAN模型生成并筛选分布自然、易与任务特征分离的隐蔽后门触发器，只需少量带标签的攻击样本即可诱导模型植入后门，实现有效攻击而避免CA明显下降。

Result: 实验证明GCB可在六个数据集、五类网络结构、四种任务（包括首次对回归和分割任务）中有效实施后门攻击，且干净准确率降幅不足1%，相比以往方法更隐蔽，且对现有防御手段具有较强鲁棒性。

Conclusion: GCB开创了新范式，证明了通过优化触发器特性能够突破现有干净图像后门攻击的隐蔽性瓶颈，显著提升后门攻击的实用性与难检测性。

Abstract: Clean-image backdoor attacks, which use only label manipulation in training
datasets to compromise deep neural networks, pose a significant threat to
security-critical applications. A critical flaw in existing methods is that the
poison rate required for a successful attack induces a proportional, and thus
noticeable, drop in Clean Accuracy (CA), undermining their stealthiness. This
paper presents a new paradigm for clean-image attacks that minimizes this
accuracy degradation by optimizing the trigger itself. We introduce Generative
Clean-Image Backdoors (GCB), a framework that uses a conditional InfoGAN to
identify naturally occurring image features that can serve as potent and
stealthy triggers. By ensuring these triggers are easily separable from benign
task-related features, GCB enables a victim model to learn the backdoor from an
extremely small set of poisoned examples, resulting in a CA drop of less than
1%. Our experiments demonstrate GCB's remarkable versatility, successfully
adapting to six datasets, five architectures, and four tasks, including the
first demonstration of clean-image backdoors in regression and segmentation.
GCB also exhibits resilience against most of the existing backdoor defenses.

</details>


### [201] [Omni-View: Unlocking How Generation Facilitates Understanding in Unified 3D Model based on Multiview images](https://arxiv.org/abs/2511.07222)
*JiaKui Hu,Shanshan Zhao,Qing-Guo Chen,Xuerui Qiu,Jialun Liu,Zhao Xu,Weihua Luo,Kaifu Zhang,Yanye Lu*

Main category: cs.CV

TL;DR: Omni-View提出了一种多模态3D场景理解与生成框架，通过联合建模3D理解、视图合成和几何估计，实现了场景理解和生成任务的协同提升。


<details>
  <summary>Details</summary>
Motivation: 当前多模态理解与生成多集中于2D场景，对于3D场景的联合理解和生成仍存在挑战，尤其是在如何充分利用生成促进理解方面。本工作旨在填补这方面的空白，提升3D场景多模态能力。

Method: Omni-View由理解模型、纹理模块和几何模块组成，联合完成3D场景理解、新视图合成和几何估计。其纹理模块专注于外观合成，把时空建模能力引入3D，几何模块则提供显式几何约束。采用两阶段训练策略，实现有效协同。

Result: 在VSI-Bench基准上，Omni-View取得了55.4的业界领先得分，超过现有主流3D理解模型，同时在新视图合成和3D生成任务上也表现优异。

Conclusion: Omni-View展示了生成促进理解的有效性，通过模块协同，显著提升了3D场景多模态理解与生成的整体水平。

Abstract: This paper presents Omni-View, which extends the unified multimodal
understanding and generation to 3D scenes based on multiview images, exploring
the principle that "generation facilitates understanding". Consisting of
understanding model, texture module, and geometry module, Omni-View jointly
models scene understanding, novel view synthesis, and geometry estimation,
enabling synergistic interaction between 3D scene understanding and generation
tasks. By design, it leverages the spatiotemporal modeling capabilities of its
texture module responsible for appearance synthesis, alongside the explicit
geometric constraints provided by its dedicated geometry module, thereby
enriching the model's holistic understanding of 3D scenes. Trained with a
two-stage strategy, Omni-View achieves a state-of-the-art score of 55.4 on the
VSI-Bench benchmark, outperforming existing specialized 3D understanding
models, while simultaneously delivering strong performance in both novel view
synthesis and 3D scene generation.

</details>


### [202] [Mapping Reduced Accessibility to WASH Facilities in Rohingya Refugee Camps with Sub-Meter Imagery](https://arxiv.org/abs/2511.07231)
*Kyeongjin Ahn,YongHun Suh,Sungwon Han,Jeasurk Yang,Hannes Taubenböck,Meeyoung Cha*

Main category: cs.CV

TL;DR: 本文提出一种基于遥感的半监督分割方法，用于评估孟加拉国科克斯巴扎尔罗兴亚难民营中WASH（饮用水、卫生设施和清洁）服务可及性，并揭示了设施稀缺和性别平等问题。


<details>
  <summary>Details</summary>
Motivation: 难民营中饮用水、卫生和清洁服务供应一直是重大公共卫生挑战，尤其是像罗兴亚难民营这样人口极度密集的环境。缺乏有效的监测和资源分配机制限制了服务提升，尤其难以获取不平等问题的空间分布数据。

Method: 利用亚米级卫星影像，开发半监督分割方法，自动检测难民庇护所位置，实现对设施分布与人群密度的时空分析，结合多年度数据和性别分布进行细致分析。该框架在庇护所检测中获得76.4%的F1分数。

Result: 多年度数据揭示随着难民人口增加，WASH设施人均可及性下降（每个设施服务人数由2022年的25人上升至2025年的29.4人）。性别拆分分析发现，女性和女孩受限于设施隔离和安全隐患，可及性更差。

Conclusion: 建议在有限预算下，采取需求响应型资源分配策略，聚焦低服务地区和女性群体。高分辨率遥感与机器学习手段可用于公平规划，及时发现和应对人道主义场景中的不平等资源分布难题。

Abstract: Access to Water, Sanitation, and Hygiene (WASH) services remains a major
public health concern in refugee camps. This study introduces a remote
sensing-driven framework to quantify WASH accessibility-specifically to water
pumps, latrines, and bathing cubicles-in the Rohingya camps of Cox's Bazar, one
of the world's most densely populated displacement settings. Detecting refugee
shelters in such emergent camps presents substantial challenges, primarily due
to their dense spatial configuration and irregular geometric patterns. Using
sub-meter satellite images, we develop a semi-supervised segmentation framework
that achieves an F1-score of 76.4% in detecting individual refugee shelters.
Applying the framework across multi-year data reveals declining WASH
accessibility, driven by rapid refugee population growth and reduced facility
availability, rising from 25 people per facility in 2022 to 29.4 in 2025.
Gender-disaggregated analysis further shows that women and girls experience
reduced accessibility, in scenarios with inadequate safety-related segregation
in WASH facilities. These findings suggest the importance of demand-responsive
allocation strategies that can identify areas with under-served
populations-such as women and girls-and ensure that limited infrastructure
serves the greatest number of people in settings with fixed or shrinking
budgets. We also discuss the value of high-resolution remote sensing and
machine learning to detect inequality and inform equitable resource planning in
complex humanitarian environments.

</details>


### [203] [Noise & pattern: identity-anchored Tikhonov regularization for robust structural anomaly detection](https://arxiv.org/abs/2511.07233)
*Alexander Bauer,Klaus-Robert Müller*

Main category: cs.CV

TL;DR: 本文提出了一种用于工业自动化检测中的结构异常检测的新方法，基于自监督自动编码器，能够高效检测与分割缺陷，并在MVTec AD基准上取得了最新最好成绩。


<details>
  <summary>Details</summary>
Motivation: 在工业视觉检测中，异常往往罕见且难以穷举所有异常样本，迫切需要无需缺陷样本即可实现高效检测的方法。

Method: 通过自监督的自动编码器，让模型学习修复被人为扰动（模拟结构性缺陷）的输入图像。与传统去噪自编码器不同，采用结构化空间相关扰动，而非独立同分布的噪声，并在遮挡区上额外加入高斯噪声，作为上差正则项，增强重建函数的稳健性。

Result: 在MVTec AD基准上，缺陷检测和分割的AUROC分别达到99.9和99.4，达到新SOTA。

Conclusion: 该方法不仅理论上有效、能稳定重构，而且在实际工业自动化检测场景中大幅提升了异常检测性能，极具实用价值。

Abstract: Anomaly detection plays a pivotal role in automated industrial inspection,
aiming to identify subtle or rare defects in otherwise uniform visual patterns.
As collecting representative examples of all possible anomalies is infeasible,
we tackle structural anomaly detection using a self-supervised autoencoder that
learns to repair corrupted inputs. To this end, we introduce a corruption model
that injects artificial disruptions into training images to mimic structural
defects. While reminiscent of denoising autoencoders, our approach differs in
two key aspects. First, instead of unstructured i.i.d.\ noise, we apply
structured, spatially coherent perturbations that make the task a hybrid of
segmentation and inpainting. Second, and counterintuitively, we add and
preserve Gaussian noise on top of the occlusions, which acts as a Tikhonov
regularizer anchoring the Jacobian of the reconstruction function toward
identity. This identity-anchored regularization stabilizes reconstruction and
further improves both detection and segmentation accuracy. On the MVTec AD
benchmark, our method achieves state-of-the-art results (I/P-AUROC: 99.9/99.4),
supporting our theoretical framework and demonstrating its practical relevance
for automatic inspection.

</details>


### [204] [Leveraging Text-Driven Semantic Variation for Robust OOD Segmentation](https://arxiv.org/abs/2511.07238)
*Seungheon Song,Jaekoo Lee*

Main category: cs.CV

TL;DR: 该论文提出了一种基于文本驱动的OOD（分布外）分割模型，通过结合视觉-语言编码器和transformer解码器，实现了在自动驾驶等场景下对异常物体的鲁棒检测，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶与机器人领域，安全和决策高度依赖对道路场景中异常物体（OOD）的检测，而现有方法对利用视觉-语言空间中文本知识挖掘尚不充分。作者认为结合语言线索可以提升系统对复杂场景中未知物体的识别能力。

Method: 提出一种文本驱动的OOD分割方法：结合视觉-语言模型编码器与transformer解码器，设计了基于语义距离的OOD提示和语义增强机制，将视觉信息与文本信息对齐，提升了对未知对象的判别能力。

Result: 在Fishyscapes、Segment-Me-If-You-Can、Road Anomaly等公开OOD分割数据集上，所提方法在像素级与目标级评测中均取得了SOTA（最优）性能。

Conclusion: 基于视觉-语言结合的OOD分割方法不仅提升了自动驾驶系统面对复杂环境时的安全性与可靠性，还验证了此类方法在异常检测领域的前景。

Abstract: In autonomous driving and robotics, ensuring road safety and reliable
decision-making critically depends on out-of-distribution (OOD) segmentation.
While numerous methods have been proposed to detect anomalous objects on the
road, leveraging the vision-language space-which provides rich linguistic
knowledge-remains an underexplored field. We hypothesize that incorporating
these linguistic cues can be especially beneficial in the complex contexts
found in real-world autonomous driving scenarios.
  To this end, we present a novel approach that trains a Text-Driven OOD
Segmentation model to learn a semantically diverse set of objects in the
vision-language space. Concretely, our approach combines a vision-language
model's encoder with a transformer decoder, employs Distance-Based OOD prompts
located at varying semantic distances from in-distribution (ID) classes, and
utilizes OOD Semantic Augmentation for OOD representations. By aligning visual
and textual information, our approach effectively generalizes to unseen objects
and provides robust OOD segmentation in diverse driving environments.
  We conduct extensive experiments on publicly available OOD segmentation
datasets such as Fishyscapes, Segment-Me-If-You-Can, and Road Anomaly datasets,
demonstrating that our approach achieves state-of-the-art performance across
both pixel-level and object-level evaluations. This result underscores the
potential of vision-language-based OOD segmentation to bolster the safety and
reliability of future autonomous driving systems.

</details>


### [205] [4DSTR: Advancing Generative 4D Gaussians with Spatial-Temporal Rectification for High-Quality and Consistent 4D Generation](https://arxiv.org/abs/2511.07241)
*Mengmeng Liu,Jiuming Liu,Yunpeng Zhang,Jiangtao Li,Michael Ying Yang,Francesco Nex,Hao Cheng*

Main category: cs.CV

TL;DR: 本论文提出了一种新的4D内容生成网络4DSTR，有效提升了4D生成的空间-时间一致性和对快速变化的适应能力，实验结果显示其在多项指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 受2D图像和3D形状生成技术突破的启发，当前领域亟需动态的4D内容生成方法。然而，现有方法在空间-时间一致性和应对快速时序变化方面表现不足，因此有必要提出新的建模和生成策略。

Method: 作者提出了4DSTR网络，结合了生成式4D高斯溅射与空间-时间校正。其核心创新包括1）利用时序相关性校正变形的尺度和旋转，提升时序一致性；2）采用自适应空间密集化与剪枝机制，根据前一帧运动动态调整高斯点数目，以应对剧烈变化。

Result: 通过大量实验，4DSTR在视频到4D生成任务中，在重建质量、空间-时间一致性以及快速运动适应性方面均取得了当前最优的性能。

Conclusion: 4DSTR有效解决了现有4D生成方法的空间-时间一致性差和对快速变化不敏感的问题，是4D内容生成领域的重要进步。

Abstract: Remarkable advances in recent 2D image and 3D shape generation have induced a
significant focus on dynamic 4D content generation. However, previous 4D
generation methods commonly struggle to maintain spatial-temporal consistency
and adapt poorly to rapid temporal variations, due to the lack of effective
spatial-temporal modeling. To address these problems, we propose a novel 4D
generation network called 4DSTR, which modulates generative 4D Gaussian
Splatting with spatial-temporal rectification. Specifically, temporal
correlation across generated 4D sequences is designed to rectify deformable
scales and rotations and guarantee temporal consistency. Furthermore, an
adaptive spatial densification and pruning strategy is proposed to address
significant temporal variations by dynamically adding or deleting Gaussian
points with the awareness of their pre-frame movements. Extensive experiments
demonstrate that our 4DSTR achieves state-of-the-art performance in video-to-4D
generation, excelling in reconstruction quality, spatial-temporal consistency,
and adaptation to rapid temporal movements.

</details>


### [206] [MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal LLMs](https://arxiv.org/abs/2511.07250)
*Tianhao Peng,Haochen Wang,Yuanxing Zhang,Zekun Wang,Zili Wang,Ge Zhang,Jian Yang,Shihao Li,Yanghai Wang,Xintao Wang,Houyi Li,Wei Ji,Pengfei Wan,Wenhao Huang,Zhaoxiang Zhang,Jiaheng Liu*

Main category: cs.CV

TL;DR: 本文提出了MVU-Eval，第一个针对多视频理解的多模态大模型（MLLMs）评测基准，包含多个领域4959个视频和1824个问答对，评测当前模型的多视频理解能力，揭示了现有模型的不足。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs评测主要局限于单视频理解，缺乏覆盖实际多视频场景（如运动分析、自动驾驶）的系统性测试，导致多视频理解能力未被充分考察和发展。

Method: 作者设计并整理了MVU-Eval基准，涵盖8项核心能力，涉及1824个问题、4959个多领域视频，内容覆盖基础感知与高级推理任务，并与实际需求（如多传感器融合、体育分析）紧密结合。随后对SOTA开源和闭源模型进行了系统评测。

Result: 结果显示，当前MLLMs在多视频理解方面存在明显性能差距和不足，难以胜任复杂的多视频场景需求。

Conclusion: MVU-Eval填补了多视频理解评测的空白，将推动MLLMs在更复杂和实际多模态应用场景的研究与发展，相关资源将公开以促进社区进步。

Abstract: The advent of Multimodal Large Language Models (MLLMs) has expanded AI
capabilities to visual modalities, yet existing evaluation benchmarks remain
limited to single-video understanding, overlooking the critical need for
multi-video understanding in real-world scenarios (e.g., sports analytics and
autonomous driving). To address this significant gap, we introduce MVU-Eval,
the first comprehensive benchmark for evaluating Multi-Video Understanding for
MLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies
through 1,824 meticulously curated question-answer pairs spanning 4,959 videos
from diverse domains, addressing both fundamental perception tasks and
high-order reasoning tasks. These capabilities are rigorously aligned with
real-world applications such as multi-sensor synthesis in autonomous systems
and cross-angle sports analytics. Through extensive evaluation of
state-of-the-art open-source and closed-source models, we reveal significant
performance discrepancies and limitations in current MLLMs' ability to perform
understanding across multiple videos. The benchmark will be made publicly
available to foster future research.

</details>


### [207] [StreamKV: Streaming Video Question-Answering with Segment-based KV Cache Retrieval and Compression](https://arxiv.org/abs/2511.07278)
*Yilong Chen,Xiang Bai,Zhibin Wang,Chengyu Bai,Yuhan Dai,Ming Lu,Shanghang Zhang*

Main category: cs.CV

TL;DR: Video-LLMs在处理长视频上有短板，StreamKV提出无训练新框架，提供更高效的KV缓存分割与压缩，实现更好的视频问答表现，相关代码已开源。


<details>
  <summary>Details</summary>
Motivation: 当前Video-LLMs难以高效处理长时长真实世界视频，尤其在KV cache（Key-Value缓存）的检索和压缩方面还未被充分挖掘优化，影响了问答等应用的表现。

Method: 提出StreamKV框架：无需训练即可为Video-LLMs加装高级KV缓存检索与压缩功能；采用动态语义分割而非统一分割，利用摘要向量保留语义信息，通过指导性提示优化KV缓存压缩，并在单一模块、不同网络层自适应执行检索和压缩。

Result: 在StreamingVQA等公共基准上，StreamKV显著优于现有的在线Video-LLMs，在准确率、内存效率和计算延迟等方面均有较大提升。

Conclusion: StreamKV极大改善了长视频流式问答的实用性与效率，为Video-LLMs未来应用提供了更高效的KV缓存管理方式。

Abstract: Video Large Language Models (Video-LLMs) have demonstrated significant
potential in the areas of video captioning, search, and summarization. However,
current Video-LLMs still face challenges with long real-world videos. Recent
methods have introduced a retrieval mechanism that retrieves query-relevant KV
caches for question answering, enhancing the efficiency and accuracy of long
real-world videos. However, the compression and retrieval of KV caches are
still not fully explored. In this paper, we propose \textbf{StreamKV}, a
training-free framework that seamlessly equips Video-LLMs with advanced KV
cache retrieval and compression. Compared to previous methods that used uniform
partitioning, StreamKV dynamically partitions video streams into semantic
segments, which better preserves semantic information. For KV cache retrieval,
StreamKV calculates a summary vector for each segment to retain segment-level
information essential for retrieval. For KV cache compression, StreamKV
introduces a guidance prompt designed to capture the key semantic elements
within each segment, ensuring only the most informative KV caches are retained
for answering questions. Moreover, StreamKV unifies KV cache retrieval and
compression within a single module, performing both in a layer-adaptive manner,
thereby further improving the effectiveness of streaming video question
answering. Extensive experiments on public StreamingVQA benchmarks demonstrate
that StreamKV significantly outperforms existing Online Video-LLMs, achieving
superior accuracy while substantially improving both memory efficiency and
computational latency. The code has been released at
https://github.com/sou1p0wer/StreamKV.

</details>


### [208] [Segmentation of Ischemic Stroke Lesions using Transfer Learning on Multi-sequence MRI](https://arxiv.org/abs/2511.07281)
*R. P. Chowdhury,T. Rahman*

Main category: cs.CV

TL;DR: 本文提出了一种基于Res-Unet架构的自动化方法，用于多种MRI序列中缺血性脑卒中病灶的快速自动分割，最终在ISLES 2015数据集上取得了有竞争力的分割准确度。


<details>
  <summary>Details</summary>
Motivation: 手工分割缺血性脑卒中MRI病灶不仅耗时、繁琐，还容易产生人为主观性误差，亟需高效且一致性强的自动分割方法。以往方法依赖手工特征，难以准确刻画病灶复杂形状。有必要通过更智能的深度学习手段改善分割效果。

Method: 采用Res-Unet深度网络架构对T1、T2、DWI和FLAIR序列MRI图像进行缺血性脑卒中病灶分割。模型训练分别在有预训练权重与无预训练权重两种情况下进行，比较迁移学习的效果。最终引入多数投票分类器，将不同轴向的分割结果融合，以获得更加稳健的分割输出。

Result: 方法在ISLES 2015脑卒中数据集上经过评估，获得了80.5%的Dice分数和74.03%的准确率，显示出优良的分割性能。

Conclusion: 所提深度学习分割框架能够快速且自动地在多种MRI序列中准确分割缺血性脑卒中病灶，优于传统手工特征方法，适合临床应用，有望提升诊疗效率和一致性。

Abstract: The accurate understanding of ischemic stroke lesions is critical for
efficient therapy and prognosis of stroke patients. Magnetic resonance imaging
(MRI) is sensitive to acute ischemic stroke and is a common diagnostic method
for stroke. However, manual lesion segmentation performed by experts is
tedious, time-consuming, and prone to observer inconsistency. Automatic medical
image analysis methods have been proposed to overcome this challenge. However,
previous approaches have relied on hand-crafted features that may not capture
the irregular and physiologically complex shapes of ischemic stroke lesions. In
this study, we present a novel framework for quickly and automatically
segmenting ischemic stroke lesions on various MRI sequences, including
T1-weighted, T2-weighted, DWI, and FLAIR. The proposed methodology is validated
on the ISLES 2015 Brain Stroke sequence dataset, where we trained our model
using the Res-Unet architecture twice: first, with pre-existing weights, and
then without, to explore the benefits of transfer learning. Evaluation metrics,
including the Dice score and sensitivity, were computed across 3D volumes.
Finally, a Majority Voting Classifier was integrated to amalgamate the outcomes
from each axis, resulting in a comprehensive segmentation method. Our efforts
culminated in achieving a Dice score of 80.5\% and an accuracy of 74.03\%,
showcasing the efficacy of our segmentation approach.

</details>


### [209] [Glioma C6: A Novel Dataset for Training and Benchmarking Cell Segmentation](https://arxiv.org/abs/2511.07286)
*Roman Malashin,Svetlana Pashkevich,Daniil Ilyukhin,Arseniy Volkov,Valeria Yachnaya,Andrey Denisov,Maria Mikhalkova*

Main category: cs.CV

TL;DR: 本文介绍了Glioma C6，一个专为胶质瘤C6细胞实例分割设计的全新开放数据集，包含超过12,000个注释细胞，支持深度学习模型训练和评估。


<details>
  <summary>Details</summary>
Motivation: 现有的用于胶质瘤细胞实例分割的数据集有限，且在高分辨显微图像和复杂形态学标注方面不足，难以推动更加鲁棒和泛化的深度学习模型研发。

Method: 构建并公开了包含75张高分辨率显微图像的Glioma C6数据集，所有细胞经人工注释（包含胞体及形态类别信息），分为受控参数子集（用于基准评测）和变化条件子集（用于泛化测试）；并评估了多种主流分割模型在该数据集上的表现。

Result: 实验发现，主流分割模型在Glioma C6数据集上的表现存在不足，同时证实如果用该数据集训练模型，可以显著提升实例分割性能。

Conclusion: Glioma C6数据集为细胞实例分割提供了真实且具有挑战性的测试与训练平台，有助于提升模型的鲁棒性与泛化能力，并已开放给研究人员使用。

Abstract: We present Glioma C6, a new open dataset for instance segmentation of glioma
C6 cells, designed as both a benchmark and a training resource for deep
learning models. The dataset comprises 75 high-resolution phase-contrast
microscopy images with over 12,000 annotated cells, providing a realistic
testbed for biomedical image analysis. It includes soma annotations and
morphological cell categorization provided by biologists. Additional
categorization of cells, based on morphology, aims to enhance the utilization
of image data for cancer cell research. Glioma C6 consists of two parts: the
first is curated with controlled parameters for benchmarking, while the second
supports generalization testing under varying conditions. We evaluate the
performance of several generalist segmentation models, highlighting their
limitations on our dataset. Our experiments demonstrate that training on Glioma
C6 significantly enhances segmentation performance, reinforcing its value for
developing robust and generalizable models. The dataset is publicly available
for researchers.

</details>


### [210] [LMM-IQA: Image Quality Assessment for Low-Dose CT Imaging](https://arxiv.org/abs/2511.07298)
*Kagan Celik,Mehmet Ozan Unal,Metin Ertas,Isa Yildirim*

Main category: cs.CV

TL;DR: 本研究提出了一套基于大语言模型（LLM）的低剂量CT图像质量评价系统，实现了对噪声、模糊和对比度损失等降质的自动评分和文字描述，在保证评估一致性和鲁棒性的同时，提升了评估结果的相关性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 低剂量CT通过减少辐射提升了患者安全性，但图像质量的下降（如噪声、模糊、对比度损失）影响了诊断准确性。因此，临床中亟需一种可靠一致的图像质量评估方法。

Method: 提出了基于大语言模型（LLM）的自动化图像质量评价系统。系统能够输出噪声、模糊、对比度损失等质量问题的数值分数和文字描述。同时，系统性地比较了零样本推理、元数据集成、误差反馈等不同推理策略对性能的影响。

Result: 该系统得出的质量分数不仅与人工评价高度相关，还能给出可解释性的文字评价，为临床工作流程提供有价值的参考。不同推理策略的逐步引入力度提升了系统总体表现。

Conclusion: 基于LLM的图像质量评价系统为低剂量CT的临床应用提供了自动化、客观且具有临床可解释性的质量评估工具，有助于提升临床诊断的可靠性和效率。

Abstract: Low-dose computed tomography (CT) represents a significant improvement in
patient safety through lower radiation doses, but increased noise, blur, and
contrast loss can diminish diagnostic quality. Therefore, consistency and
robustness in image quality assessment become essential for clinical
applications. In this study, we propose an LLM-based quality assessment system
that generates both numerical scores and textual descriptions of degradations
such as noise, blur, and contrast loss. Furthermore, various inference
strategies - from the zero-shot approach to metadata integration and error
feedback - are systematically examined, demonstrating the progressive
contribution of each method to overall performance. The resultant assessments
yield not only highly correlated scores but also interpretable output, thereby
adding value to clinical workflows. The source codes of our study are available
at https://github.com/itu-biai/lmms_ldct_iqa.

</details>


### [211] [VADER: Towards Causal Video Anomaly Understanding with Relation-Aware Large Language Models](https://arxiv.org/abs/2511.07299)
*Ying Cheng,Yu-Ho Lin,Min-Hung Chen,Fu-En Yang,Shang-Hong Lai*

Main category: cs.CV

TL;DR: 本文提出了一个名为VADER的框架，用于视频异常理解，不仅检测异常，还能提供丰富的因果和语义解释。


<details>
  <summary>Details</summary>
Motivation: 当前视频异常检测方法只关注异常事件的检测与定位，缺乏对事件因果关系和对象交互的深度理解。本研究旨在弥补这一不足，实现对异常的全面解释。

Method: VADER框架结合关键帧中的对象关系特征和视觉线索。首先用异常评分器对每帧异常程度评分，通过上下文感知采样策略捕捉异常事件的因果上下文。之后，关系特征提取器与对比关系编码器共同建模对象间的动态交互，生成紧凑的关系表示，再与大模型集成，输出详细、因果相关的异常解释和问答。

Result: VADER在多个真实世界数据集上表现优异，在异常描述、解释和因果推理任务上均取得较强效果，显著提升了视频异常分析的解释性。

Conclusion: VADER框架有效提升了视频异常事件的理解和解释能力，其因果推理和问答支持推进了可解释性视频异常分析技术的发展。

Abstract: Video anomaly understanding (VAU) aims to provide detailed interpretation and
semantic comprehension of anomalous events within videos, addressing
limitations of traditional methods that focus solely on detecting and
localizing anomalies. However, existing approaches often neglect the deeper
causal relationships and interactions between objects, which are critical for
understanding anomalous behaviors. In this paper, we propose VADER, an
LLM-driven framework for Video Anomaly unDErstanding, which integrates keyframe
object Relation features with visual cues to enhance anomaly comprehension from
video. Specifically, VADER first applies an Anomaly Scorer to assign per-frame
anomaly scores, followed by a Context-AwarE Sampling (CAES) strategy to capture
the causal context of each anomalous event. A Relation Feature Extractor and a
COntrastive Relation Encoder (CORE) jointly model dynamic object interactions,
producing compact relational representations for downstream reasoning. These
visual and relational cues are integrated with LLMs to generate detailed,
causally grounded descriptions and support robust anomaly-related question
answering. Experiments on multiple real-world VAU benchmarks demonstrate that
VADER achieves strong results across anomaly description, explanation, and
causal reasoning tasks, advancing the frontier of explainable video anomaly
analysis.

</details>


### [212] [Beyond Boundaries: Leveraging Vision Foundation Models for Source-Free Object Detection](https://arxiv.org/abs/2511.07301)
*Huizai Yao,Sicheng Zhao,Pengteng Li,Yi Cui,Shuo Lu,Weiyu Guo,Yunfan Lu,Yijie Xu,Hui Xiong*

Main category: cs.CV

TL;DR: 提出在源数据不可用的目标检测自适应场景下，将视觉基础模型（VFMs）作为外部知识源，有效提升跨域泛化能力和伪标签质量，综合提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有SFOD方法严重依赖原始模型内的知识，容易产生带有偏见的伪标签，导致模型转移能力和判别能力有限。而VFMs预训练自大规模多样化数据，泛化能力强，因此希望引入VFMs外部知识克服上述问题。

Method: 提出利用VFMs的SFOD新框架，设计三个关键模块：1）基于patch权重的全局特征对齐（PGFA）；2）基于原型的实例特征对齐（PIFA）；3）双源增强的伪标签融合（DEPF），分别提升全局与实例的特征对齐和伪标签质量。

Result: 在六个公开基准数据集上实验，所得结果优于已有方法，验证了方法在提升迁移性和判别性上的有效性。

Conclusion: 将VFMs引入SFOD，为无源数据目标检测领域开辟了新思路，可双重提升特征转移性能和伪标签准确度，达到当前最佳水平。

Abstract: Source-Free Object Detection (SFOD) aims to adapt a source-pretrained object
detector to a target domain without access to source data. However, existing
SFOD methods predominantly rely on internal knowledge from the source model,
which limits their capacity to generalize across domains and often results in
biased pseudo-labels, thereby hindering both transferability and
discriminability. In contrast, Vision Foundation Models (VFMs), pretrained on
massive and diverse data, exhibit strong perception capabilities and broad
generalization, yet their potential remains largely untapped in the SFOD
setting. In this paper, we propose a novel SFOD framework that leverages VFMs
as external knowledge sources to jointly enhance feature alignment and label
quality. Specifically, we design three VFM-based modules: (1) Patch-weighted
Global Feature Alignment (PGFA) distills global features from VFMs using
patch-similarity-based weighting to enhance global feature transferability; (2)
Prototype-based Instance Feature Alignment (PIFA) performs instance-level
contrastive learning guided by momentum-updated VFM prototypes; and (3)
Dual-source Enhanced Pseudo-label Fusion (DEPF) fuses predictions from
detection VFMs and teacher models via an entropy-aware strategy to yield more
reliable supervision. Extensive experiments on six benchmarks demonstrate that
our method achieves state-of-the-art SFOD performance, validating the
effectiveness of integrating VFMs to simultaneously improve transferability and
discriminability.

</details>


### [213] [YoNoSplat: You Only Need One Model for Feedforward 3D Gaussian Splatting](https://arxiv.org/abs/2511.07321)
*Botao Ye,Boqi Chen,Haofei Xu,Daniel Barath,Marc Pollefeys*

Main category: cs.CV

TL;DR: 本文提出了一种名为YoNoSplat的新方法，可以高效地从任意数量的无结构图片中重建高质量的3D高斯斑点表示。该方法适用于有/无相机姿态、标定/未标定的输入，并实现了业界领先的重建速度和精度。


<details>
  <summary>Details</summary>
Motivation: 当前的3D场景重建面临效率和灵活性不足的问题，尤其是在处理不规则、多样化输入（如未知相机姿态或参数）时更具挑战性。作者希望提出一个适用性强、训练高效且结果优良的方法，能从多样图片集中快速重建3D场景。

Method: YoNoSplat为前馈模型，可同时预测每个视图的本地高斯和相机姿态，再通过预测或外部提供的姿态将这些本地高斯聚合成全局3D场景。为消除3D高斯和相机参数联合学习时的耦合，提出了mixing training策略：先用真实姿态，渐渐混入预测姿态，从而防止训练不稳定和曝光偏差。提出新颖的两两相机距离归一化和内参嵌入方法解决尺度不确定和未标定输入问题，并能直接预测相机内参。

Result: YoNoSplat在NVIDIA GH200 GPU上，仅用2.69秒即可重建100视角（280x518分辨率）的场景。在有无相机姿态的标准基准测试上均取得了最先进成绩，兼具高速度与高精度。

Conclusion: YoNoSplat实现了3D高斯重建领域在速度、灵活性和精度方面的新突破。该方法能同时处理有无姿态与有无内参的输入，训练稳定，表现优异，对实际应用与后续研究都具有重要价值。

Abstract: Fast and flexible 3D scene reconstruction from unstructured image collections
remains a significant challenge. We present YoNoSplat, a feedforward model that
reconstructs high-quality 3D Gaussian Splatting representations from an
arbitrary number of images. Our model is highly versatile, operating
effectively with both posed and unposed, calibrated and uncalibrated inputs.
YoNoSplat predicts local Gaussians and camera poses for each view, which are
aggregated into a global representation using either predicted or provided
poses. To overcome the inherent difficulty of jointly learning 3D Gaussians and
camera parameters, we introduce a novel mixing training strategy. This approach
mitigates the entanglement between the two tasks by initially using
ground-truth poses to aggregate local Gaussians and gradually transitioning to
a mix of predicted and ground-truth poses, which prevents both training
instability and exposure bias. We further resolve the scale ambiguity problem
by a novel pairwise camera-distance normalization scheme and by embedding
camera intrinsics into the network. Moreover, YoNoSplat also predicts intrinsic
parameters, making it feasible for uncalibrated inputs. YoNoSplat demonstrates
exceptional efficiency, reconstructing a scene from 100 views (at 280x518
resolution) in just 2.69 seconds on an NVIDIA GH200 GPU. It achieves
state-of-the-art performance on standard benchmarks in both pose-free and
pose-dependent settings. Our project page is at
https://botaoye.github.io/yonosplat/.

</details>


### [214] [Garbage Vulnerable Point Monitoring using IoT and Computer Vision](https://arxiv.org/abs/2511.07325)
*R. Kumar,A. Lall,S. Chaudhari,M. Kale,A. Vattem*

Main category: cs.CV

TL;DR: 本文提出利用物联网与计算机视觉监控城市垃圾乱丢行为，结合街道摄像头和目标检测算法，能够精准实时检测和分析垃圾倾倒情况。在多个深度学习模型对比实验中，YOLO11m表现最佳，识别准确率达92.39%。


<details>
  <summary>Details</summary>
Motivation: 随着城市化进程加快，垃圾乱倒问题日益严重，传统人工监控效率低下，需要自动化、高效的智能手段提升垃圾管理水平。

Method: 系统采用街道摄像头获取实时影像数据，并基于YOLOv8、YOLOv10、YOLO11m和RT-DETR等目标检测算法进行垃圾识别。通过收集印度Sangareddy区的实际数据，实验分析各模型性能。

Result: YOLO11m模型在垃圾识别准确率上达到92.39%，mAP@50为0.91，明显优于其他模型。系统不仅准确检测垃圾，还可以统计分析不同时段的垃圾倾倒规律。

Conclusion: 本文提出的方法实现了垃圾违规倾倒的高效、全天候自动监测，评估结果表明其在实际城市场景中具备良好的应用前景。

Abstract: This paper proposes a smart way to manage municipal solid waste by using the
Internet of Things (IoT) and computer vision (CV) to monitor illegal waste
dumping at garbage vulnerable points (GVPs) in urban areas. The system can
quickly detect and monitor dumped waste using a street-level camera and object
detection algorithm. Data was collected from the Sangareddy district in
Telangana, India. A series of comprehensive experiments was carried out using
the proposed dataset to assess the accuracy and overall performance of various
object detection models. Specifically, we performed an in-depth evaluation of
YOLOv8, YOLOv10, YOLO11m, and RT-DETR on our dataset. Among these models,
YOLO11m achieved the highest accuracy of 92.39\% in waste detection,
demonstrating its effectiveness in detecting waste. Additionally, it attains an
mAP@50 of 0.91, highlighting its high precision. These findings confirm that
the object detection model is well-suited for monitoring and tracking waste
dumping events at GVP locations. Furthermore, the system effectively captures
waste disposal patterns, including hourly, daily, and weekly dumping trends,
ensuring comprehensive daily and nightly monitoring.

</details>


### [215] [Inference-Time Scaling of Diffusion Models for Infrared Data Generation](https://arxiv.org/abs/2511.07362)
*Kai A. Horstmann,Maxim Clouser,Kia Khezeli*

Main category: cs.CV

TL;DR: 本文提出了一种提升红外图像生成质量的方法，通过在推理阶段使用领域自适应的CLIP验证器来引导扩散模型，缓解了红外领域高质量标注数据稀缺的问题。实验在KAIST数据集上显著提升了生成效果。


<details>
  <summary>Details</summary>
Motivation: 红外图像因其对低可见性环境的适应性，在场景理解任务中极具应用价值。然而，开发高性能红外视觉模型受限于高质量标注数据的稀缺，且红外标注需专业知识。合成红外图像可为模型训练提供数据，但基础级扩散模型受限于数据规模。因此，寻找提升红外图像生成质量的新方法具有重要意义。

Method: 作者采用了推理时指导方案，引入领域自适应的CLIP验证器，以提升红外图像生成质量。具体做法是将FLUX.1-dev（先进的文生图扩散模型）通过参数高效的微调技术在小规模红外图像上适配红外领域，然后训练一个红外领域的CLIP验证器，在推理过程中用来引导扩散采样，优化文本与红外图像的对齐与视觉效果。

Result: 实验结果显示，所提出的方法在KAIST多光谱行人检测基准数据集上，生成红外图像的FID分数相比无验证器基线样本下降了10%，生成质量有了持续且明显的提升。

Conclusion: 推理时利用领域自适应验证器指导扩散模型，是解决小数据红外图像生成领域差异的有效方法，为未来低数据红外场景下的模型开发提供了新方向。

Abstract: Infrared imagery enables temperature-based scene understanding using passive
sensors, particularly under conditions of low visibility where traditional RGB
imaging fails. Yet, developing downstream vision models for infrared
applications is hindered by the scarcity of high-quality annotated data, due to
the specialized expertise required for infrared annotation. While synthetic
infrared image generation has the potential to accelerate model development by
providing large-scale, diverse training data, training foundation-level
generative diffusion models in the infrared domain has remained elusive due to
limited datasets. In light of such data constraints, we explore an
inference-time scaling approach using a domain-adapted CLIP-based verifier for
enhanced infrared image generation quality. We adapt FLUX.1-dev, a
state-of-the-art text-to-image diffusion model, to the infrared domain by
finetuning it on a small sample of infrared images using parameter-efficient
techniques. The trained verifier is then employed during inference to guide the
diffusion sampling process toward higher quality infrared generations that
better align with input text prompts. Empirically, we find that our approach
leads to consistent improvements in generation quality, reducing FID scores on
the KAIST Multispectral Pedestrian Detection Benchmark dataset by 10% compared
to unguided baseline samples. Our results suggest that inference-time guidance
offers a promising direction for bridging the domain gap in low-data infrared
settings.

</details>


### [216] [StreamDiffusionV2: A Streaming System for Dynamic and Interactive Video Generation](https://arxiv.org/abs/2511.07399)
*Tianrui Feng,Zhi Li,Shuo Yang,Haocheng Xi,Muyang Li,Xiuyu Li,Lvmin Zhang,Keting Yang,Kelly Peng,Song Han,Maneesh Agrawala,Kurt Keutzer,Akio Kodaira,Chenfeng Xu*

Main category: cs.CV

TL;DR: 本文提出StreamDiffusionV2系统，实现了基于视频扩散模型的高效实时生成式直播流。该系统在保证低延迟的同时提升了帧率和可扩展性，适用于各种规模的在线直播需求。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像的扩散模型虽为直播内容创造带来创新，但在时间一致性和实时性上存在瓶颈。视频扩散模型虽然在离线生成有进展，但在严格的实时直播SLO（服务级目标）、多GPU伸缩以及低延迟方面尚未破解。研究者希望实现真正高效、可扩展且时延可控的生成式视频直播系统。

Method: 提出StreamDiffusionV2训练无关管线，包含SLO感知批处理调度器、区块调度器、sink-token引导KV cache、运动感知噪声控制及多项系统层优化，支持分布式多GPU环境。还设计了管线编排方法，实现去噪步骤和网络层并行，保障线性帧率提升。

Result: 在4块H100 GPU上，无需TensorRT或量化，StreamDiffusionV2可用14B参数模型于0.5秒内出首帧，达到58.28 FPS；1.3B模型下达64.52 FPS，大大超越现有同类方案。系统支持灵活的去噪步数、低延迟及高画质多种模式，表现优异且稳定。

Conclusion: StreamDiffusionV2创新性地解决了生成式直播视频的实时性和可扩展性难题，让大规模高质量直播生成应用变得实际可用，适用于个人到企业级场景，推动行业发展。

Abstract: Generative models are reshaping the live-streaming industry by redefining how
content is created, styled, and delivered. Previous image-based streaming
diffusion models have powered efficient and creative live streaming products
but have hit limits on temporal consistency due to the foundation of
image-based designs. Recent advances in video diffusion have markedly improved
temporal consistency and sampling efficiency for offline generation. However,
offline generation systems primarily optimize throughput by batching large
workloads. In contrast, live online streaming operates under strict
service-level objectives (SLOs): time-to-first-frame must be minimal, and every
frame must meet a per-frame deadline with low jitter. Besides, scalable
multi-GPU serving for real-time streams remains largely unresolved so far. To
address this, we present StreamDiffusionV2, a training-free pipeline for
interactive live streaming with video diffusion models. StreamDiffusionV2
integrates an SLO-aware batching scheduler and a block scheduler, together with
a sink-token--guided rolling KV cache, a motion-aware noise controller, and
other system-level optimizations. Moreover, we introduce a scalable pipeline
orchestration that parallelizes the diffusion process across denoising steps
and network layers, achieving near-linear FPS scaling without violating latency
guarantees. The system scales seamlessly across heterogeneous GPU environments
and supports flexible denoising steps (e.g., 1--4), enabling both
ultra-low-latency and higher-quality modes. Without TensorRT or quantization,
StreamDiffusionV2 renders the first frame within 0.5s and attains 58.28 FPS
with a 14B-parameter model and 64.52 FPS with a 1.3B-parameter model on four
H100 GPUs, making state-of-the-art generative live streaming practical and
accessible--from individual creators to enterprise-scale platforms.

</details>


### [217] [DIMO: Diverse 3D Motion Generation for Arbitrary Objects](https://arxiv.org/abs/2511.07409)
*Linzhan Mou,Jiahui Lei,Chen Wang,Lingjie Liu,Kostas Daniilidis*

Main category: cs.CV

TL;DR: 本文提出了一种称为DIMO的新方法，可以基于单张图片为任意物体生成多样化的3D动作。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以从单张图片自动生成丰富多样的3D动作，且对动作多样性和新颖性支持有限。因此，亟需能够生成多样化3D动作的高效方法。

Method: 作者利用训练良好的视频模型挖掘共性的运动模式，并将其嵌入到低维潜在空间中。具体步骤包括：1）生成同一物体的多段不同动作视频；2）将每个动作映射成潜在向量；3）训练共享动作解码器，得到结构化、紧凑的动作表示（即神经关键点轨迹）；4）用3D高斯驱动关键点，并融合生成物体的3D几何和外观模型。

Result: 该方法可实现单次前向推理下的多样3D动作采样，并支持3D动作插值、文本引导动作生成等多种有趣的应用。

Conclusion: DIMO展示了从单图生成多样化3D动作的巨大潜力，并拓展了3D动作建模和相关应用的可能性。

Abstract: We present DIMO, a generative approach capable of generating diverse 3D
motions for arbitrary objects from a single image. The core idea of our work is
to leverage the rich priors in well-trained video models to extract the common
motion patterns and then embed them into a shared low-dimensional latent space.
Specifically, we first generate multiple videos of the same object with diverse
motions. We then embed each motion into a latent vector and train a shared
motion decoder to learn the distribution of motions represented by a structured
and compact motion representation, i.e., neural key point trajectories. The
canonical 3D Gaussians are then driven by these key points and fused to model
the geometry and appearance. During inference time with learned latent space,
we can instantly sample diverse 3D motions in a single-forward pass and support
several interesting applications including 3D motion interpolation and
language-guided motion generation. Our project page is available at
https://linzhanm.github.io/dimo.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [218] [Ming-UniAudio: Speech LLM for Joint Understanding, Generation and Editing with Unified Representation](https://arxiv.org/abs/2511.05516)
*Canxiang Yan,Chunxiang Jin,Dawei Huang,Haibing Yu,Han Peng,Hui Zhan,Jie Gao,Jing Peng,Jingdong Chen,Jun Zhou,Kaimeng Ren,Ming Yang,Mingxue Yang,Qiang Xu,Qin Zhao,Ruijie Xiong,Shaoxiong Lin,Xuezhi Wang,Yi Yuan,Yifei Wu,Yongjie Lyu,Zhengyu He,Zhihao Qiu,Zhiqiang Fang,Ziyuan Huang*

Main category: cs.CL

TL;DR: 本文提出了一个统一语音理解、生成与编辑的新框架，核心是能同时兼顾语义和声学特征的连续语音分词器（MingTok-Audio），并基于此开发了新型语音模型，在多个评测上取得领先；还首次实现了基于自然语言指令的自由语音编辑，并开源了相关模型和评测基准。


<details>
  <summary>Details</summary>
Motivation: 现有语音模型在理解和生成任务中对token表示的需求不同，导致难以实现基于指令的自由编辑。这种表征差异限制了语音大模型在统一理解和生成，以及编辑任务上的能力。

Method: 核心创新点在于提出了连续语音分词器MingTok-Audio，能有效整合语义与声学特征。基于此分词器，构建了统一的语音大模型Ming-UniAudio，实现理解与生成能力的平衡。进一步，还发展了专门的语音编辑模型Ming-UniAudio-Edit，通过自然语言指令实现对语音的语义和声学自由编辑。并构建了指令驱动的自由语音编辑基准集Ming-Freeform-Audio-Edit，进行多维评测。

Result: Ming-UniAudio在ContextASR基准的12项指标中有8项达到SOTA，在中文语音克隆任务中Seed-TTS-WER仅为0.95。Ming-UniAudio-Edit实现了首次无需时间戳条件、仅靠自然语句控制的语音编辑。并建立了新基准，全面评估编辑任务的表现。

Conclusion: 本文实现了语音理解、生成、编辑的统一，并首次实现了依托自然语言指令的普适型自由语音编辑。相关模型、分词器和基准均已开源，为后续统一语音处理研究和应用奠定基础。

Abstract: Existing speech models suffer from competing requirements on token
representations by understanding and generation tasks. This discrepancy in
representation prevents speech language models from performing
instruction-based free-form editing. To solve this challenge, we introduce a
novel framework that unifies speech understanding, generation, and editing. The
core of our unified model is a unified continuous speech tokenizer
MingTok-Audio, the first continuous tokenizer to effectively integrate semantic
and acoustic features, which makes it suitable for both understanding and
generation tasks. Based on this unified continuous audio tokenizer, we
developed the speech language model Ming-UniAudio, which achieved a balance
between generation and understanding capabilities. Ming-UniAudio sets new
state-of-the-art (SOTA) records on 8 out of 12 metrics on the ContextASR
benchmark. Notably, for Chinese voice cloning, it achieves a highly competitive
Seed-TTS-WER of 0.95. Leveraging this foundational model, we further trained a
dedicated speech editing model Ming-UniAudio-Edit, the first speech language
model that enables universal, free-form speech editing guided solely by natural
language instructions, handling both semantic and acoustic modifications
without timestamp condition. To rigorously assess the editing capability and
establish a foundation for future research, we introduce
Ming-Freeform-Audio-Edit, the first comprehensive benchmark tailored for
instruction-based free-form speech editing, featuring diverse scenarios and
evaluation dimensions spanning semantic correctness, acoustic quality, and
instruction alignment. We open-sourced the continuous audio tokenizer, the
unified foundational model, and the free-form instruction-based editing model
to facilitate the development of unified audio understanding, generation, and
manipulation.

</details>


### [219] [Retracing the Past: LLMs Emit Training Data When They Get Lost](https://arxiv.org/abs/2511.05518)
*Myeongseob Ko,Nikhil Reddy Billa,Adam Nguyen,Charles Fleming,Ming Jin,Ruoxi Jia*

Main category: cs.CL

TL;DR: 本文提出了一种新的攻击方法——困惑诱导攻击（CIA），能够更高效地从大型语言模型（LLMs）中提取被其记忆的训练数据，该方法性能优于现有提取数据的技术。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型在训练过程中可能记忆训练数据，这带来了隐私和版权等诸多风险。当前主流的数据提取方法（尤其是启发式发散攻击）在有效性和理论解释上都存在不足。因此，亟需一种原理性更强、效果更好的训练数据提取方法来评估和暴露模型记忆泄露的风险。

Method: 作者提出了困惑诱导攻击（CIA）框架，通过系统地最大化模型的不确定性，有效提取记忆数据。实验证明，在模型生成记忆文本前，其token级预测熵会持续上升。CIA方法通过优化输入片段，诱导模型进入连续高熵状态。此外，作者提出错配监督微调（Mismatched Supervised Fine-tuning, SFT）策略，削弱已对齐LLMs的对齐性，并定向加剧其困惑，提升提取成功率。

Result: 在多种未对齐和已对齐LLM上的实验表明，CIA及其改进方法在无需训练数据先验知识的情况下，能比现有方法更有效地提取原文及近似原文的训练数据。

Conclusion: 本文揭示了现有LLMs广泛存在的记忆风险，并提出了一种更系统化的方法评估模型的泄露脆弱性，对模型安全性研究和隐私保护有重要启示。

Abstract: The memorization of training data in large language models (LLMs) poses
significant privacy and copyright concerns. Existing data extraction methods,
particularly heuristic-based divergence attacks, often exhibit limited success
and offer limited insight into the fundamental drivers of memorization leakage.
This paper introduces Confusion-Inducing Attacks (CIA), a principled framework
for extracting memorized data by systematically maximizing model uncertainty.
We empirically demonstrate that the emission of memorized text during
divergence is preceded by a sustained spike in token-level prediction entropy.
CIA leverages this insight by optimizing input snippets to deliberately induce
this consecutive high-entropy state. For aligned LLMs, we further propose
Mismatched Supervised Fine-tuning (SFT) to simultaneously weaken their
alignment and induce targeted confusion, thereby increasing susceptibility to
our attacks. Experiments on various unaligned and aligned LLMs demonstrate that
our proposed attacks outperform existing baselines in extracting verbatim and
near-verbatim training data without requiring prior knowledge of the training
data. Our findings highlight persistent memorization risks across various LLMs
and offer a more systematic method for assessing these vulnerabilities.

</details>


### [220] [Beyond One-Size-Fits-All: Personalized Harmful Content Detection with In-Context Learning](https://arxiv.org/abs/2511.05532)
*Rufan Zhang,Lin Zhang,Xianghang Mi*

Main category: cs.CL

TL;DR: 本文提出了一种新的内容审核框架，利用大模型的in-context learning（ICL）能力，实现对有害内容（如有毒言论、垃圾信息、负面情绪）的一体化检测，并支持轻量级的用户个性化，而无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有的内容审核系统往往是集中化、针对特定任务设计的，缺乏透明度和灵活性，难以满足用户多样化的需求及对隐私和去中心化环境的要求。

Method: 本框架利用基础大模型与ICL，通过简单的prompt引导，统一完成多种有害内容的检测（涵盖二分类、多分类、多标签）。用户可通过提供示例或定义，便捷地添加/解除屏蔽新类别或变体，实现个性化，无需模型重训练。

Result: 在多个公开基准（TextDetox, UCI SMS, SST2）及一个新标注的Mastodon数据集上，框架具备良好的跨任务泛化性，个性化仅需极少示例，结合标签定义/理由提升对噪声数据的鲁棒性，性能媲美甚至优于专用微调模型。

Conclusion: ICL方法为内容审核系统提供了一种实用、隐私友好、高度可适应的新路径，推动审核从一刀切走向以用户为中心的多样化智能化。相关代码和数据集已开源，有助于促进后续研究。

Abstract: The proliferation of harmful online content--e.g., toxicity, spam, and
negative sentiment--demands robust and adaptable moderation systems. However,
prevailing moderation systems are centralized and task-specific, offering
limited transparency and neglecting diverse user preferences--an approach
ill-suited for privacy-sensitive or decentralized environments. We propose a
novel framework that leverages in-context learning (ICL) with foundation models
to unify the detection of toxicity, spam, and negative sentiment across binary,
multi-class, and multi-label settings. Crucially, our approach enables
lightweight personalization, allowing users to easily block new categories,
unblock existing ones, or extend detection to semantic variations through
simple prompt-based interventions--all without model retraining. Extensive
experiments on public benchmarks (TextDetox, UCI SMS, SST2) and a new,
annotated Mastodon dataset reveal that: (i) foundation models achieve strong
cross-task generalization, often matching or surpassing task-specific
fine-tuned models; (ii) effective personalization is achievable with as few as
one user-provided example or definition; and (iii) augmenting prompts with
label definitions or rationales significantly enhances robustness to noisy,
real-world data. Our work demonstrates a definitive shift beyond
one-size-fits-all moderation, establishing ICL as a practical,
privacy-preserving, and highly adaptable pathway for the next generation of
user-centric content safety systems. To foster reproducibility and facilitate
future research, we publicly release our code on GitHub and the annotated
Mastodon dataset on Hugging Face.

</details>


### [221] [MCP4IFC: IFC-Based Building Design Using Large Language Models](https://arxiv.org/abs/2511.05533)
*Bharathi Kannan Nithyanantham,Tobias Sesterhenn,Ashwin Nedungadi,Sergio Peral Garijo,Janis Zenkner,Christian Bartelt,Stefan Lüdtke*

Main category: cs.CL

TL;DR: 本文提出并开源了MCP4IFC框架，使大语言模型能直接“读写”建筑工业基础数据格式（IFC），推动生成式AI在建筑工程领域落地。


<details>
  <summary>Details</summary>
Motivation: 推动生成式AI在建筑、工程和施工（AEC）领域的实际应用，需要系统能理解自然语言并将其转为对标准化建筑数据模型的操作，实现AI辅助建筑信息建模（BIM）。

Method: 提出MCP4IFC开源框架：包含一套BIM工具（如场景检索、建筑元素创建与修改函数），配备动态代码生成系统，结合上下文学习与检索增强生成（RAG），使LLM能处理超出内置工具范围的复杂任务。

Result: 实验表明，采用该框架的大语言模型能够完成从新建简单房屋到查询、编辑现有IFC数据的多种复杂任务。

Conclusion: MCP4IFC框架能够极大促进LLM在BIM建模中的应用，相关工具和代码已开源，为后续AI驱动的BIM设计研究和工作流奠定基础。

Abstract: Bringing generative AI into the architecture, engineering and construction
(AEC) field requires systems that can translate natural language instructions
into actions on standardized data models. We present MCP4IFC, a comprehensive
open-source framework that enables Large Language Models (LLMs) to directly
manipulate Industry Foundation Classes (IFC) data through the Model Context
Protocol (MCP). The framework provides a set of BIM tools, including scene
querying tools for information retrieval, predefined functions for creating and
modifying common building elements, and a dynamic code-generation system that
combines in-context learning with retrieval-augmented generation (RAG) to
handle tasks beyond the predefined toolset. Experiments demonstrate that an LLM
using our framework can successfully perform complex tasks, from building a
simple house to querying and editing existing IFC data. Our framework is
released as open-source to encourage research in LLM-driven BIM design and
provide a foundation for AI-assisted modeling workflows. Our code is available
at https://show2instruct.github.io/mcp4ifc/.

</details>


### [222] [FlowMM: Cross-Modal Information Flow Guided KV Cache Merging for Efficient Multimodal Context Inference](https://arxiv.org/abs/2511.05534)
*Kunxi Li,Yufan Xiong,Zhonghua Jiang,Yiyun Zhou,Zhaode Wang,Chengfei Lv,Shengyu Zhang*

Main category: cs.CL

TL;DR: 提出了一种针对多模态大模型KV缓存的高效合并方法FlowMM，能大幅降低内存和延迟，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存淘汰策略常因丢弃重要上下文而影响生成质量，尤其在多模态场景因模态及跨模态分布偏差，导致策略效果受限。

Method: 提出FlowMM框架，依据跨模态信息流动态调整各层KV合并策略，同时设计了敏感度自适应的Token匹配机制，结合Token相似度与任务关键性动态合并低风险Token，并保留高敏感度Token。

Result: 在主流多模态大模型上，大规模实验显示FlowMM可减少80%-95%的KV缓存内存，并将解码延迟降低1.3-1.8倍，性能几乎不下降。

Conclusion: FlowMM为多模态KV缓存提供了高效、可靠的管理方法，显著提升内存效率和推理速度，同时维持任务表现，具备很高实用价值。

Abstract: Traditional KV cache eviction strategies, which discard less critical
KV-pairs based on attention scores, often degrade generation quality, causing
context loss or hallucinations. Recent efforts shift toward KV merging, merging
eviction tokens with retention tokens based on similarity. However, in
multimodal scenarios, distributional biases across modality tokens and
attentional biases in cross-modal interactions limit its effectiveness. This
work introduces FlowMM, an adaptive framework for cross-modal information
flow-guided multimodal KV cache merging. FlowMM leverages cross-modal
information flow to dynamically apply layer-specific merging strategies,
capturing modality-specific patterns while preserving contextual integrity.
Furthermore, we introduce a sensitivity-adaptive token matching mechanism that
jointly evaluates token similarity and task-critical sensitivity, merging
low-risk tokens while safeguarding high-sensitivity ones. Extensive experiments
across diverse leading MLLMs show that FlowMM reduces KV cache memory by 80% to
95% and decoding latency by 1.3-1.8x, while maintaining competitive task
performance.

</details>


### [223] [Future of AI Models: A Computational perspective on Model collapse](https://arxiv.org/abs/2511.05535)
*Trivikram Satharasi,S Sitharama Iyengar*

Main category: cs.CL

TL;DR: 本文研究了由人工智能生成内容在网络上的快速扩展及其对语义多样性的影响，利用语义相似度分析定量评估潜在的数据与模型崩塌时间，警示AI污染或将威胁未来数据和模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型和扩散模型等AI技术的普及，互联网上合成内容剧增，人工撰写数据比例下降，这将可能导致后续AI训练数据同质化和语义能力衰减，即模型崩塌现象。本文旨在通过实际数据分析，量化并预测这一风险的发生时间。

Method: 作者选取2013至2025年间英文维基百科（基于Common Crawl）为对象，采用Transformer嵌入和余弦相似度指标，按年度分析内容语义相似性趋势，评价AI内容递归污染的程度和演变。

Result: 结果显示，在大型语言模型广泛应用前，语义相似度逐步升高（主要归因于早期RNN/LSTM应用）；但LLM大规模普及后，相似度呈指数增长，显示大量内容趋同。相关波动主要由年数据量、采样误差及残留语言多样性决定。

Conclusion: 研究提供了递归AI内容对数据多样性威胁的定量估算，提醒我们必须关注合成数据主导下的模型泛化能力减弱、数据贫乏和模型崩塌等长期风险。

Abstract: Artificial Intelligence, especially Large Language Models (LLMs), has
transformed domains such as software engineering, journalism, creative writing,
academia, and media (Naveed et al. 2025; arXiv:2307.06435). Diffusion models
like Stable Diffusion generate high-quality images and videos from text.
Evidence shows rapid expansion: 74.2% of newly published webpages now contain
AI-generated material (Ryan Law 2025), 30-40% of the active web corpus is
synthetic (Spennemann 2025; arXiv:2504.08755), 52% of U.S. adults use LLMs for
writing, coding, or research (Staff 2025), and audits find AI involvement in
18% of financial complaints and 24% of press releases (Liang et al. 2025). The
underlying neural architectures, including Transformers (Vaswani et al. 2023;
arXiv:1706.03762), RNNs, LSTMs, GANs, and diffusion networks, depend on large,
diverse, human-authored datasets (Shi & Iyengar 2019). As synthetic content
dominates, recursive training risks eroding linguistic and semantic diversity,
producing Model Collapse (Shumailov et al. 2024; arXiv:2307.15043; Dohmatob et
al. 2024; arXiv:2402.07712). This study quantifies and forecasts collapse onset
by examining year-wise semantic similarity in English-language Wikipedia
(filtered Common Crawl) from 2013 to 2025 using Transformer embeddings and
cosine similarity metrics. Results reveal a steady rise in similarity before
public LLM adoption, likely driven by early RNN/LSTM translation and
text-normalization pipelines, though modest due to a smaller scale. Observed
fluctuations reflect irreducible linguistic diversity, variable corpus size
across years, finite sampling error, and an exponential rise in similarity
after the public adoption of LLM models. These findings provide a data-driven
estimate of when recursive AI contamination may significantly threaten data
richness and model generalization.

</details>


### [224] [Temporal Sparse Autoencoders: Leveraging the Sequential Nature of Language for Interpretability](https://arxiv.org/abs/2511.05541)
*Usha Bhalla,Alex Oesterling,Claudio Mayrink Verdun,Himabindu Lakkaraju,Flavio P. Calmon*

Main category: cs.CL

TL;DR: 该论文提出了一种用于语言模型解释的新方法Temporal Sparse Autoencoders（T-SAEs），通过增强特征在邻近token间的一致性，提高了模型对语义特征的识别能力，而不仅仅关注表面或噪声特征。


<details>
  <summary>Details</summary>
Motivation: 近年来，词典学习方法如稀疏自编码器（SAEs）可用于揭示语言模型内部结构中的可解释特征，但实际上这些方法倾向于捕捉浅层、与具体token相关或噪声较大的特征，难以发现驱动人类理解的重要语义信息。作者认为，这是因为现有的无监督训练方法未能充分利用语言的结构知识。

Method: 作者提出了一种改进的稀疏自编码器T-SAE。核心方法是在训练过程中引入一种新的对比损失，使得高层特征在相邻token间维持更一致的激活状态。该调整根据语言的基本特性：语义信息具有较强的长程依赖性和连续性，而句法信息则更局部。这样可以在无监督条件下更好地分离语义和句法特征。

Result: 实验表明，在多个数据集和模型上，新方法能够恢复出更平滑、更连贯的语义特征，并且不会影响原有的重构质量。T-SAEs能够在没有显式语义信号的情况下，提取出清晰的语义结构。

Conclusion: T-SAEs提供了一种无需监督、能更好地分离语义与句法特征的解释性工具，为理解和解释大语言模型内部机制提供了新途径。

Abstract: Translating the internal representations and computations of models into
concepts that humans can understand is a key goal of interpretability. While
recent dictionary learning methods such as Sparse Autoencoders (SAEs) provide a
promising route to discover human-interpretable features, they suffer from a
variety of problems, including a systematic failure to capture the rich
conceptual information that drives linguistic understanding. Instead, they
exhibit a bias towards shallow, token-specific, or noisy features, such as "the
phrase 'The' at the start of sentences". In this work, we propose that this is
due to a fundamental issue with how dictionary learning methods for LLMs are
trained. Language itself has a rich, well-studied structure spanning syntax,
semantics, and pragmatics; however, current unsupervised methods largely ignore
this linguistic knowledge, leading to poor feature discovery that favors
superficial patterns over meaningful concepts. We focus on a simple but
important aspect of language: semantic content has long-range dependencies and
tends to be smooth over a sequence, whereas syntactic information is much more
local. Building on this insight, we introduce Temporal Sparse Autoencoders
(T-SAEs), which incorporate a novel contrastive loss encouraging consistent
activations of high-level features over adjacent tokens. This simple yet
powerful modification enables SAEs to disentangle semantic from syntactic
features in a self-supervised manner. Across multiple datasets and models,
T-SAEs recover smoother, more coherent semantic concepts without sacrificing
reconstruction quality. Strikingly, they exhibit clear semantic structure
despite being trained without explicit semantic signal, offering a new pathway
for unsupervised interpretability in language models.

</details>


### [225] [Sample-Efficient Language Modeling with Linear Attention and Lightweight Enhancements](https://arxiv.org/abs/2511.05560)
*Patrick Haller,Jonas Golde,Alan Akbik*

Main category: cs.CL

TL;DR: 本论文提出了一种在小数据和计算受限环境下高效进行语言建模的方法，并通过改良架构和优化器显著提升了模型表现。


<details>
  <summary>Details</summary>
Motivation: 目前主流的大语言模型通常依赖于大规模数据和算力，而BabyLM 2025任务要求在受限资源下实现高效的语言建模。因此，研究者希望探索能兼顾效率和效果的新方案。

Method: 作者提出BLaLM模型，用线性时间的mLSTM取代自注意力机制，加入轻量级强化方法（如短卷积、滑动窗口注意力及Hedgehog特征映射），并针对低资源场景构建高质量、重视可读性与教学结构的数据集。同时尝试了Muon优化器以提升训练表现。

Result: 在STRICT和STRICT-SMALL两个分组下，结合滑动窗口注意力的线性注意力方法稳定提升了零样本任务表现，且Muon优化器在收敛性和困惑度上均优于传统优化器AdamW。

Conclusion: 结果表明，合理的架构改进及优化器选择可以在不依赖大规模数据和算力的前提下，有效提升小模型语言建模的效果，为高效语言建模提供了可行策略。

Abstract: We study architectural and optimization tech- niques for sample-efficient
language modeling under the constraints of the BabyLM 2025 shared task. Our
model, BLaLM, replaces self-attention with a linear-time mLSTM to- ken mixer
and explores lightweight enhance- ments, including short convolutions, sliding
window attention with dynamic modulation, and Hedgehog feature maps. To support
train- ing in low-resource settings, we curate a high- quality corpus
emphasizing readability and ped- agogical structure. Experiments across both
STRICT and STRICT-SMALL tracks show that (1) linear attention combined with
sliding win- dow attention consistently improves zero-shot performance, and (2)
the Muon optimizer stabi- lizes convergence and reduces perplexity over AdamW.
These results highlight effective strate- gies for efficient language modeling
without relying on scale.

</details>


### [226] [UTF-8 Plumbing: Byte-level Tokenizers Unavoidably Enable LLMs to Generate Ill-formed UTF-8](https://arxiv.org/abs/2511.05578)
*Preston Firestone,Shubham Ugare,Gagandeep Singh,Sasa Misailovic*

Main category: cs.CL

TL;DR: 本文探讨了子词分词中使用字节为基础的词表会导致生成非有效UTF-8序列的问题，并分析了其在真实系统中的影响和应对措施。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，采用字节为粒度的分词器能够避免词表过大和词表外问题，但也可能产生非有效的UTF-8编码，影响模型与下游系统的兼容性。

Method: 作者用幺半群（monoid）理论形式化分析分词过程，证明若分词器词表含非标准UTF-8子串，生成的序列可能整体上不是有效的UTF-8。同时，展示了分步和整体还原Token序列时UTF-8解析结果的差异，并通过案例及实际模型评估潜在问题和缓解方法。

Result: 理论证明了带有非标准UTF-8子词的分词器总能生成不合法UTF-8序列。实际分析了主流大模型、推理引擎、生成系统中出现的相关Bug和破坏现象，并评估了多种缓解方案的有效性。

Conclusion: 分词器设计时，若使用基于字节的词表，需严格考虑UTF-8有效性问题，否则可能导致模型行为异常或输出系统失效。论文倡导在模型应用时要采取相应的兼容措施以避免此类隐患。

Abstract: Subword tokenization segments input text according to a pre-defined
vocabulary to feed it into a language model; the language model, in turn,
generates a sequence made from this same vocabulary. The members of the
vocabulary can be built of code points or bytes. Using code points means that
all members of the vocabulary are valid UTF-8 characters. However, it also
requires thousands of initial members to achieve acceptable coverage of inputs.
Beginning with bytes, on the contrary, avoids out-of-vocabulary errors with
only 256 initial members of the vocabulary, but the members of the vocabulary
and sequences of them are not guaranteed to be valid UTF-8. Sequences that are
not valid UTF-8 break code that assumes its input to be valid UTF-8.
Applications of language models must account for the breakage thereby
introduced. In this paper, we formalize tokenization using monoid theory and
prove that tokenizers whose vocabularies contain tokens that are ill-formed
UTF-8 can always produce sequences that are ill-formed UTF-8. We demonstrate
formally that attempting to incrementally convert tokens back to a string and
interpret the results as UTF-8 gives different results than converting the
whole sequence of tokens at once. This formal result predicts real-world bugs:
we evaluate mitigations for the problem identified and provide case studies of
major foundation models, serving engines, and constrained generation systems.

</details>


### [227] [Optimizing Diversity and Quality through Base-Aligned Model Collaboration](https://arxiv.org/abs/2511.05650)
*Yichen Wang,Chenghao Yang,Tenghao Huang,Muhao Chen,Jonathan May,Mina Lee*

Main category: cs.CL

TL;DR: 提出了一种在推理阶段结合基础LLM和对齐LLM的方法（BACo），动态选取两者输出以在不牺牲质量的前提下提升生成多样性。实验证明BACo在多项任务和多指标下都优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐技术虽然提升了输出质量，但减少了生成多样性。传统提升多样性的方法如重训练、多采样等存在质量下降或高计算成本问题，因此需要一种既保持高质量又提升多样性的方案。

Method: 作者提出BACo框架，在推理时每个token动态选择由基础模型或对齐模型生成，依据下一个token预测的不确定性和语义角色，根据不同路由策略实现多样性和质量的权衡。实验在三个任务和13个多样性与质量指标上进行，比较了多种路由实现方式。

Result: BACo在所有任务和评价指标上都显著超越了现有推理阶段提升多样性的基线方法。采用最佳路由策略时，BACo在多样性与质量的综合指标上提升21.3%。人工评价结果与自动指标一致。

Conclusion: 基础模型与对齐模型的协作可以更好地控制并优化生成的多样性与质量。BACo无需复杂重训练，推理时即可提升LLM性能，具有较强实际意义。

Abstract: Alignment has greatly improved large language models (LLMs)' output quality
at the cost of diversity, yielding highly similar outputs across generations.
We propose Base-Aligned Model Collaboration (BACo), an inference-time
token-level model collaboration framework that dynamically combines a base LLM
with its aligned counterpart to optimize diversity and quality. Inspired by
prior work (Fei et al., 2025), BACo employs routing strategies that determine,
at each token, from which model to decode based on next-token prediction
uncertainty and predicted contents' semantic role. Prior diversity-promoting
methods, such as retraining, prompt engineering, and multi-sampling methods,
improve diversity but often degrade quality or require costly decoding or
post-training. In contrast, BACo achieves both high diversity and quality post
hoc within a single pass, while offering strong controllability. We explore a
family of routing strategies, across three open-ended generation tasks and 13
metrics covering diversity and quality, BACo consistently surpasses
state-of-the-art inference-time baselines. With our best router, BACo achieves
a 21.3% joint improvement in diversity and quality. Human evaluations also
mirror these improvements. The results suggest that collaboration between base
and aligned models can optimize and control diversity and quality.

</details>


### [228] [OckBench: Measuring the Efficiency of LLM Reasoning](https://arxiv.org/abs/2511.05722)
*Zheng Du,Hao Kang,Song Han,Tushar Krishna,Ligeng Zhu*

Main category: cs.CL

TL;DR: 本文提出了OckBench基准，用于综合评估大语言模型（如GPT-4、Claude 3、Gemini系列）在推理和代码生成中的准确率及token效率，弥补了以往只关注输出质量却忽略token消耗的不足。


<details>
  <summary>Details</summary>
Motivation: 当前主流基准主要评估大语言模型的准确性和输出质量，忽视了模型在生成文本时的token数量，这对实际应用中的延迟、成本和能耗有很大影响。作者希望填补这一评测空白。

Method: 作者提出了OckBench基准，具有模型无关性和硬件无关性，可同时评估推理和编程任务中模型的准确率与token消耗。通过对多种开源和闭源模型的实验比较，分析它们在token效率上的差异。

Result: 实验发现，许多准确率相近的模型在token消耗上有巨大差异，token效率成为区分模型优劣、尚未被重视的重要维度。文中还展示了准确率-效率关系的帕累托前沿。

Conclusion: 呼吁行业改变评测范式，不应再将token消耗视为“免费”指标；OckBench为研究与实践中的token高效推理提供了统一的测试平台。

Abstract: Large language models such as GPT-4, Claude 3, and the Gemini series have
improved automated reasoning and code generation. However, existing benchmarks
mainly focus on accuracy and output quality, and they ignore an important
factor: decoding token efficiency. In real systems, generating 10,000 tokens
versus 100,000 tokens leads to large differences in latency, cost, and energy.
In this work, we introduce OckBench, a model-agnostic and hardware-agnostic
benchmark that evaluates both accuracy and token count for reasoning and coding
tasks. Through experiments comparing multiple open- and closed-source models,
we uncover that many models with comparable accuracy differ wildly in token
consumption, revealing that efficiency variance is a neglected but significant
axis of differentiation. We further demonstrate Pareto frontiers over the
accuracy-efficiency plane and argue for an evaluation paradigm shift: we should
no longer treat tokens as "free" to multiply. OckBench provides a unified
platform for measuring, comparing, and guiding research in token-efficient
reasoning. Our benchmarks are available at https://ockbench.github.io/ .

</details>


### [229] [In-Context Learning Without Copying](https://arxiv.org/abs/2511.05743)
*Kerem Sahin,Sheridan Feucht,Adam Belfki,Jannik Brinkmann,Aaron Mueller,David Bau,Chris Wendler*

Main category: cs.CL

TL;DR: 研究探索了归纳拷贝(inductive copying)是否是transformer模型获得复杂上下文学习能力的必要基础，通过有针对性地抑制这一机制，发现模型仍可在抽象性ICL任务中表现良好，甚至优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 过往研究认为induction head（归纳head）是更复杂上下文学习能力的前提，因此常将其作为神经模型训练中的关键步骤。但作者质疑：当归纳拷贝被抑制时，Transformer还能否习得抽象性ICL能力？这关系到transformer学习机制的本质理解。

Method: 提出Hapax训练设置：对于任何可以被归纳head正确预测的token，其损失不计入整体loss，从而减弱inductive copying的作用，对比标准训练，检验模型在抽象ICL任务上的表现及机制变化。

Result: 尽管归纳拷贝大幅下降，去除31.7%的token loss后模型在13/21个抽象ICL任务上仍优于常规模型。在归纳拷贝不能预测的位置也获得更低损失。同时，Hapax模型实际产生的归纳head更少更弱，但ICL能力依然存在。

Conclusion: 归纳拷贝并非获得抽象上下文学习机制的关键，transformer模型可以在弱归纳拷贝下习得ICL能力，对理解大模型学习机制有重要意义。

Abstract: Induction heads are attention heads that perform inductive copying by
matching patterns from earlier context and copying their continuations
verbatim. As models develop induction heads, they often experience a sharp drop
in training loss, a phenomenon cited as evidence that induction heads may serve
as a prerequisite for more complex in-context learning (ICL) capabilities. In
this work, we ask whether transformers can still acquire ICL capabilities when
inductive copying is suppressed. We propose Hapax, a setting where we omit the
loss contribution of any token that can be correctly predicted by induction
heads. Despite a significant reduction in inductive copying, performance on
abstractive ICL tasks (i.e., tasks where the answer is not contained in the
input context) remains comparable and surpasses the vanilla model on 13 of 21
tasks, even though 31.7\% of tokens are omitted from the loss. Furthermore, our
model achieves lower loss values on token positions that cannot be predicted
correctly by induction heads. Mechanistic analysis further shows that models
trained with Hapax develop fewer and weaker induction heads but still preserve
ICL capabilities. Taken together, our findings indicate that inductive copying
is not essential for learning abstractive ICL mechanisms.

</details>


### [230] [Multi-Scale Feature Fusion and Graph Neural Network Integration for Text Classification with Large Language Models](https://arxiv.org/abs/2511.05752)
*Xiangchen Song,Yulin Huang,Jinxu Guo,Yuchen Liu,Yaxuan Luan*

Main category: cs.CL

TL;DR: 本研究提出了一种混合式文本分类方法，将大语言模型的深度特征、特征金字塔的多尺度融合以及图神经网络的结构化建模相结合，大幅提升了复杂语义环境下的文本分类性能，综合效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 文本分类任务中，复杂语义和多级特征信息难以充分融合与利用，现有模型在捕捉全球-局部、语义-结构等多层次信息时表现有限。为提升模型在复杂场景下的表现，作者希望结合先进的特征提取和结构建模方法。

Method: 首先利用大语言模型深度提取文本上下文依赖和语义特征；随后，采用特征金字塔机制对多尺度特征进行融合，兼顾全局与局部语义；接着将融合后的特征转为图结构，应用图神经网络挖掘潜在的语义关系和逻辑依赖，最后通过分类模块输出结果。

Result: 实验证明，该方法在鲁棒性、对齐实验，以及ACC、F1-Score、AUC、Precision等多项评价指标上均超过现有主流模型，验证了方法的有效性与稳定性。

Conclusion: 本研究构建了平衡全局-局部与语义-结构信息的集成框架，并为多尺度特征融合和结构化语义建模在文本分类中的应用提供了新的思路。

Abstract: This study investigates a hybrid method for text classification that
integrates deep feature extraction from large language models, multi-scale
fusion through feature pyramids, and structured modeling with graph neural
networks to enhance performance in complex semantic contexts. First, the large
language model captures contextual dependencies and deep semantic
representations of the input text, providing a rich feature foundation for
subsequent modeling. Then, based on multi-level feature representations, the
feature pyramid mechanism effectively integrates semantic features of different
scales, balancing global information and local details to construct
hierarchical semantic expressions. Furthermore, the fused features are
transformed into graph representations, and graph neural networks are employed
to capture latent semantic relations and logical dependencies in the text,
enabling comprehensive modeling of complex interactions among semantic units.
On this basis, the readout and classification modules generate the final
category predictions. The proposed method demonstrates significant advantages
in robustness alignment experiments, outperforming existing models on ACC,
F1-Score, AUC, and Precision, which verifies the effectiveness and stability of
the framework. This study not only constructs an integrated framework that
balances global and local information as well as semantics and structure, but
also provides a new perspective for multi-scale feature fusion and structured
semantic modeling in text classification tasks.

</details>


### [231] [Language Generation: Complexity Barriers and Implications for Learning](https://arxiv.org/abs/2511.05759)
*Marcelo Arenas,Pablo Barceló,Luis Cofré,Alexander Kozachinskiy*

Main category: cs.CL

TL;DR: 理论上，机器可以学会生成任何目标语言的句子，但在实际操作中，所需样本数量可能极其庞大，甚至不可计算。


<details>
  <summary>Details</summary>
Motivation: Kleinberg和Mullainathan证明了理论上语言生成的可行性，但并未考虑实际操作中的可行性，即学习一个目标语言到底需要多少正例。作者希望揭示理论与实际学习效率之间的鸿沟。

Method: 对正则语言和上下文无关语言等简单语言族，分析了达到高质量生成所需的样本复杂度，给出了理论下限，包括某些情况下样本量不受任何可计算函数约束的证明。

Result: 发现即便是简单语言族，学会生成能力所需的正例数量也可能极大，某些情况下连可计算下界都不存在。

Conclusion: 理论可行性和实际高效学习之间存在巨大差距。理解现代语言生成模型为何能高效学习，需要考虑自然语言的结构性等现实属性。

Abstract: Kleinberg and Mullainathan showed that, in principle, language generation is
always possible: with sufficiently many positive examples, a learner can
eventually produce sentences indistinguishable from those of a target language.
However, the existence of such a guarantee does not speak to its practical
feasibility. In this work, we show that even for simple and well-studied
language families -- such as regular and context-free languages -- the number
of examples required for successful generation can be extraordinarily large,
and in some cases not bounded by any computable function. These results reveal
a substantial gap between theoretical possibility and efficient learnability.
They suggest that explaining the empirical success of modern language models
requires a refined perspective -- one that takes into account structural
properties of natural language that make effective generation possible in
practice.

</details>


### [232] [DRAGON: Guard LLM Unlearning in Context via Negative Detection and Reasoning](https://arxiv.org/abs/2511.05784)
*Yaxuan Wang,Chris Yuhao Liu,Quan Liu,Jinglong Pang,Wei Wei,Yujia Bao,Yang Liu*

Main category: cs.CL

TL;DR: 该论文提出了一种无需保留数据、可在实际场景下高效实施的LLM遗忘（unlearning）方法——DRAGON，通过推理链（CoT）与检测机制增强安全防护，并验证了其实用性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有大模型遗忘方法大多依赖于微调和保留数据，但实际场景下往往无法获得用于保留的完整数据，导致在数据有限情况下缺乏有效的遗忘方案。亟需一种更实际、无需保留数据的新方法来保障隐私与安全。

Method: 提出DRAGON系统框架，不修改底层模型，采用轻量级检测模块识别需要遗忘的输入（prompt），并将其通过专门的链式推理守卫模型进行安全干预。这种方法利用LLM的指令跟随能力，结合场景中的链式思考提示，无需用到保留数据。设计了新的评价指标和持续遗忘场景进行验证。

Result: 在涵盖三种代表性遗忘任务的大量实验中，DRAGON在遗忘能力、扩展性和实际适用性方面表现优异，显著优于仅能在有“保留数据”环境下工作的传统方法。

Conclusion: DRAGON为资源受限、实际环境中的大模型遗忘任务提供了一种高效、可扩展且成熟的方法，突破了现有技术对于保留数据的依赖，有望推动安全AI的实际部署。

Abstract: Unlearning in Large Language Models (LLMs) is crucial for protecting private
data and removing harmful knowledge. Most existing approaches rely on
fine-tuning to balance unlearning efficiency with general language
capabilities. However, these methods typically require training or access to
retain data, which is often unavailable in real world scenarios. Although these
methods can perform well when both forget and retain data are available, few
works have demonstrated equivalent capability in more practical, data-limited
scenarios. To overcome these limitations, we propose Detect-Reasoning Augmented
GeneratiON (DRAGON), a systematic, reasoning-based framework that utilizes
in-context chain-of-thought (CoT) instructions to guard deployed LLMs before
inference. Instead of modifying the base model, DRAGON leverages the inherent
instruction-following ability of LLMs and introduces a lightweight detection
module to identify forget-worthy prompts without any retain data. These are
then routed through a dedicated CoT guard model to enforce safe and accurate
in-context intervention. To robustly evaluate unlearning performance, we
introduce novel metrics for unlearning performance and the continual unlearning
setting. Extensive experiments across three representative unlearning tasks
validate the effectiveness of DRAGON, demonstrating its strong unlearning
capability, scalability, and applicability in practical scenarios.

</details>


### [233] [Quantifying Edits Decay in Fine-tuned LLMs](https://arxiv.org/abs/2511.05852)
*Yinjie Cheng,Paul Youssef,Christin Seifert,Jörg Schlötterer,Zhixue Zhao*

Main category: cs.CL

TL;DR: 本论文系统地研究了知识编辑和微调在大语言模型中的相互作用，发现微调后编辑内容会衰减，并提出了有效的策略以结合两者。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的知识编辑（KE）和微调（fine-tuning）技术通常分别独立研究，未回答“微调后，编辑过的知识还会保留吗”这一关键实际问题。这关系到编辑恶意内容的移除和有益内容的保留，两者在安全性与实用性上均有重要意义。

Method: 论文评估了两种前沿编辑方法（MEMIT，AlphaEdit）与三种微调方法（全参数、LoRA、DoRA），涵盖五个LLM和三个数据集，共232种实验配置。并提出了只微调编辑层这种选择性微调方案。

Result: 实验发现，不同配置下知识编辑确实会因微调而衰减，例如AlphaEdit损失比MEMIT多。只微调编辑过的层能有效移除编辑内容，而影响下游性能较小。出乎意料的是，仅微调未编辑层损害编辑内容更多。

Conclusion: 论文建立了知识编辑与微调相结合的实证基线和可操作建议，强调知识编辑评价应考虑整条实际应用流程。

Abstract: Knowledge editing has emerged as a lightweight alternative to retraining for
correcting or injecting specific facts in large language models (LLMs).
Meanwhile, fine-tuning remains the default operation for adapting LLMs to new
domains and tasks. Despite their widespread adoption, these two post-training
interventions have been studied in isolation, leaving open a crucial question:
if we fine-tune an edited model, do the edits survive? This question is
motivated by two practical scenarios: removing covert or malicious edits, and
preserving beneficial edits. If fine-tuning impairs edits as shown in Figure 1,
current KE methods become less useful, as every fine-tuned model would require
re-editing, which significantly increases the cost; if edits persist,
fine-tuned models risk propagating hidden malicious edits, raising serious
safety concerns. To this end, we systematically quantify edits decay after
fine-tuning, investigating how fine-tuning affects knowledge editing. We
evaluate two state-of-the-art editing methods (MEMIT, AlphaEdit) and three
fine-tuning approaches (full-parameter, LoRA, DoRA) across five LLMs and three
datasets, yielding 232 experimental configurations. Our results show that edits
decay after fine-tuning, with survival varying across configurations, e.g.,
AlphaEdit edits decay more than MEMIT edits. Further, we propose
selective-layer fine-tuning and find that fine-tuning edited layers only can
effectively remove edits, though at a slight cost to downstream performance.
Surprisingly, fine-tuning non-edited layers impairs more edits than full
fine-tuning. Overall, our study establishes empirical baselines and actionable
strategies for integrating knowledge editing with fine-tuning, and underscores
that evaluating model editing requires considering the full LLM application
pipeline.

</details>


### [234] [Retrieval-Augmented Generation in Medicine: A Scoping Review of Technical Implementations, Clinical Applications, and Ethical Considerations](https://arxiv.org/abs/2511.05901)
*Rui Yang,Matthew Yu Heng Wong,Huitao Li,Xin Li,Wentao Zhu,Jingchi Liao,Kunyu Yu,Jonathan Chong Kai Liew,Weihao Xuan,Yingjian Chen,Yuhe Ke,Jasmine Chiat Ling Ong,Douglas Teodoro,Chuan Hong,Daniel Shi Wei Ting,Nan Liu*

Main category: cs.CL

TL;DR: 本研究综述了RAG（检索增强生成）在医学领域的应用，发现大多数研究仍处于早期阶段，面临数据、语言和安全等多方面挑战。


<details>
  <summary>Details</summary>
Motivation: 医学知识快速增长和临床复杂性提升，使传统人工处理难以满足需求，促使研究者寻求更智能的AI解决方案，如LLMs，但它们仍有局限。RAG被认为有潜力提升LLMs在医学领域的实用性。

Method: 通过文献回顾系统分析当前医学领域RAG应用，包括数据源、检索方式、生成模型与评估手段等。

Result: 当前医学RAG主要依赖公开数据和通用英文模型，医学专用和多语言模型应用有限。评估多以自动指标为主，人工评估关注度局限，缺乏对偏见和安全性考量。RAG主要应用在问答、报告生成、摘要和信息抽取。

Conclusion: 医学RAG尚处早期发展阶段，需在临床验证、多语种适配和低资源场景支持方面取得进展，以实现全球范围内可信、安全的应用。

Abstract: The rapid growth of medical knowledge and increasing complexity of clinical
practice pose challenges. In this context, large language models (LLMs) have
demonstrated value; however, inherent limitations remain. Retrieval-augmented
generation (RAG) technologies show potential to enhance their clinical
applicability. This study reviewed RAG applications in medicine. We found that
research primarily relied on publicly available data, with limited application
in private data. For retrieval, approaches commonly relied on English-centric
embedding models, while LLMs were mostly generic, with limited use of
medical-specific LLMs. For evaluation, automated metrics evaluated generation
quality and task performance, whereas human evaluation focused on accuracy,
completeness, relevance, and fluency, with insufficient attention to bias and
safety. RAG applications were concentrated on question answering, report
generation, text summarization, and information extraction. Overall, medical
RAG remains at an early stage, requiring advances in clinical validation,
cross-linguistic adaptation, and support for low-resource settings to enable
trustworthy and responsible global use.

</details>


### [235] [NILC: Discovering New Intents with LLM-assisted Clustering](https://arxiv.org/abs/2511.05913)
*Hongtao Wang,Renchi Yang,Wenqing Lin*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的意图发现聚类框架NILC，借助大语言模型（LLMs）辅助，显著提升对未知和已知意图的识别能力，在六个基准数据集上均取得了优异结果。


<details>
  <summary>Details</summary>
Motivation: 传统新意图发现方法多采用两阶段流程，先进行文本编码，再通过聚类识别意图。但此流程无法实现编码和聚类之间的反馈，也难以捕捉细粒度语义，导致性能受限。本文旨在解决反馈循环缺失及单一嵌入局限性的问题。

Method: 提出NILC迭代聚类框架。核心做法有：1）利用LLMs为每个簇生成补充语义中心，丰富嵌入向量的语境信息；2）借助LLMs改写困难样本，提升模糊样本聚类正确率；3）在半监督设定下，结合seeding和soft must links增强监督信号。聚类过程不断通过LLMs优化文本嵌入和簇中心。

Result: 在六个不同领域的基准数据集上，NILC在无监督和半监督两类场景下均显著优于多种最新主流方法，表现出更高的意图识别准确率和泛化能力。

Conclusion: NILC充分利用大语言模型在意图发现中的语义理解和样本增强能力，有效打通文本编码与聚类的反馈通道，在新意图发现任务中显著提升了识别表现。

Abstract: New intent discovery (NID) seeks to recognize both new and known intents from
unlabeled user utterances, which finds prevalent use in practical dialogue
systems. Existing works towards NID mainly adopt a cascaded architecture,
wherein the first stage focuses on encoding the utterances into informative
text embeddings beforehand, while the latter is to group similar embeddings
into clusters (i.e., intents), typically by K-Means. However, such a cascaded
pipeline fails to leverage the feedback from both steps for mutual refinement,
and, meanwhile, the embedding-only clustering overlooks nuanced textual
semantics, leading to suboptimal performance. To bridge this gap, this paper
proposes NILC, a novel clustering framework specially catered for effective
NID. Particularly, NILC follows an iterative workflow, in which clustering
assignments are judiciously updated by carefully refining cluster centroids and
text embeddings of uncertain utterances with the aid of large language models
(LLMs). Specifically, NILC first taps into LLMs to create additional semantic
centroids for clusters, thereby enriching the contextual semantics of the
Euclidean centroids of embeddings. Moreover, LLMs are then harnessed to augment
hard samples (ambiguous or terse utterances) identified from clusters via
rewriting for subsequent cluster correction. Further, we inject supervision
signals through non-trivial techniques seeding and soft must links for more
accurate NID in the semi-supervised setting. Extensive experiments comparing
NILC against multiple recent baselines under both unsupervised and
semi-supervised settings showcase that NILC can achieve significant performance
improvements over six benchmark datasets of diverse domains consistently.

</details>


### [236] [IDALC: A Semi-Supervised Framework for Intent Detection and Active Learning based Correction](https://arxiv.org/abs/2511.05921)
*Ankan Mullick,Sukannya Purkayastha,Saransh Sharma,Pawan Goyal,Niloy Ganguly*

Main category: cs.CL

TL;DR: 本文提出了一种名为IDALC的半监督意图检测和主动学习校正框架，有效减少了语音对话系统中的人工标注需求，同时提升了意图识别精度。


<details>
  <summary>Details</summary>
Motivation: 语音助手虽能处理多种用户请求，但当模型信心度不足时，系统会拒绝部分输入，需人工标注，且新增意图需再次标注，导致高昂的人力成本，因此亟需降低标注成本并提升识别效率。

Method: 作者提出了IDALC框架，结合意图检测与主动学习，针对系统拒绝的语句筛选出高价值样本进行自动修正与标注，最大程度减小对人工标注的依赖。

Result: 在多项基准数据集上，IDALC系统的准确率比基线方法高5-10%，macro-F1提升4-8%。同时，人工标注的总体成本控制在未标注数据的6-10%。

Conclusion: IDALC有效提升了用户意图检测的性能，大幅降低了人工标注工作量，是语音助手及对话系统持续进化的高效方案。

Abstract: Voice-controlled dialog systems have become immensely popular due to their
ability to perform a wide range of actions in response to diverse user queries.
These agents possess a predefined set of skills or intents to fulfill specific
user tasks. But every system has its own limitations. There are instances
where, even for known intents, if any model exhibits low confidence, it results
in rejection of utterances that necessitate manual annotation. Additionally, as
time progresses, there may be a need to retrain these agents with new intents
from the system-rejected queries to carry out additional tasks. Labeling all
these emerging intents and rejected utterances over time is impractical, thus
calling for an efficient mechanism to reduce annotation costs. In this paper,
we introduce IDALC (Intent Detection and Active Learning based Correction), a
semi-supervised framework designed to detect user intents and rectify
system-rejected utterances while minimizing the need for human annotation.
Empirical findings on various benchmark datasets demonstrate that our system
surpasses baseline methods, achieving a 5-10% higher accuracy and a 4-8%
improvement in macro-F1. Remarkably, we maintain the overall annotation cost at
just 6-10% of the unlabelled data available to the system. The overall
framework of IDALC is shown in Fig. 1

</details>


### [237] [Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs](https://arxiv.org/abs/2511.05933)
*Renfei Zhang,Manasa Kaniselvan,Niloofar Mireshghallah*

Main category: cs.CL

TL;DR: 本文发现通过强化学习（RL）微调的大模型在纯知识回忆类任务上的表现不仅未下降，反而优于基础模型和单纯监督微调（SFT）模型，尤其在层级化、结构化知识查找（如医学编码）任务上优势明显。


<details>
  <summary>Details</summary>
Motivation: 现有共识认为强化学习微调虽然提升了推理与泛化能力，但会削弱模型的记忆性。作者质疑并实证了RL微调模型的知识回忆能力反而更强，动机在于深入理解RL为何能提升知识检索表现。

Method: 作者对比RL微调、SFT、基础模型在医学编码等层级知识回忆任务上的表现，并通过结构化提示词（structured prompting）实验探究模型路径导航能力，加之层内激活分析区分'事实表征'和'查询表征'的内在变化。

Result: RL模型在知识回忆任务上表现最佳。SFT模型结合结构化提示后大幅缩小与RL模型之间的差距，但RL模型在深层检索中的路径回忆依然更强。激活分析显示，RL主要影响模型的查询过程（即如何检索知识），而不是知识表征内容本身。

Conclusion: 强化学习并未削弱大模型知识记忆，相反通过提升其结构化、程序化的检索能力，令其更善于高效提取和利用已有知识。

Abstract: Reinforcement learning (RL) is often credited with improving language model
reasoning and generalization at the expense of degrading memorized knowledge.
We challenge this narrative by observing that RL-enhanced models consistently
outperform their base and supervised fine-tuned (SFT) counterparts on pure
knowledge recall tasks, particularly those requiring traversal of hierarchical,
structured knowledge (e.g., medical codes). We hypothesize these gains stem not
from newly acquired data, but from improved procedural skills in navigating and
searching existing knowledge hierarchies within the model parameters. To
support this hypothesis, we show that structured prompting, which explicitly
guides SFTed models through hierarchical traversal, recovers most of the
performance gap (reducing 24pp to 7pp on MedConceptsQA for DeepSeek-V3/R1). We
further find that while prompting improves final-answer accuracy, RL-enhanced
models retain superior ability to recall correct procedural paths on
deep-retrieval tasks. Finally our layer-wise internal activation analysis
reveals that while factual representations (e.g., activations for the statement
"code 57.95 refers to urinary infection") maintain high cosine similarity
between SFT and RL models, query representations (e.g., "what is code 57.95")
diverge noticeably, indicating that RL primarily transforms how models traverse
knowledge rather than the knowledge representation itself.

</details>


### [238] [Interpretable Recognition of Cognitive Distortions in Natural Language Texts](https://arxiv.org/abs/2511.05969)
*Anton Kolonin,Anna Arinicheva*

Main category: cs.CL

TL;DR: 本文提出了一种基于加权结构化模式（如N-gram）的多因子文本分类新方法，能够提升心理健康领域中认知扭曲自动检测的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的文本分类和认知扭曲检测方法在可解释性、鲁棒性和准确率方面存在不足，影响实际心理护理应用。

Method: 方法基于N-gram等结构化特征，综合考虑其多层级（异构）关系，通过加权模式进行特征提取与组合，开发了可解释的AI模型和配套学习、识别算法。

Result: 在两个公开数据集上，本文方法在认知扭曲检测任务中的F1得分显著高于已有研究结果，并通过调参获得了最优性能。

Conclusion: 提出的方法在心理健康相关文本分析中具备更强的性能、可解释性和适用性，并开源了代码和模型以促进领域发展。

Abstract: We propose a new approach to multi-factor classification of natural language
texts based on weighted structured patterns such as N-grams, taking into
account the heterarchical relationships between them, applied to solve such a
socially impactful problem as the automation of detection of specific cognitive
distortions in psychological care, relying on an interpretable, robust and
transparent artificial intelligence model. The proposed recognition and
learning algorithms improve the current state of the art in this field. The
improvement is tested on two publicly available datasets, with significant
improvements over literature-known F1 scores for the task, with optimal
hyper-parameters determined, having code and models available for future use by
the community.

</details>


### [239] [Revisiting Entropy in Reinforcement Learning for Large Reasoning Models](https://arxiv.org/abs/2511.05993)
*Renren Jin,Pengzhi Gao,Yuqi Ren,Zhuowen Han,Tongxuan Zhang,Wuwei Huang,Wei Liu,Jian Luan,Deyi Xiong*

Main category: cs.CL

TL;DR: 本文系统性分析了在可验证奖励的强化学习（RLVR）框架下，大语言模型的熵动力学，阐明了在RLVR中导致模型熵下降的关键因素，并提出了有效调节模型熵的方法。


<details>
  <summary>Details</summary>
Motivation: 虽然RLVR可以提升大语言模型的推理表现，但在训练过程中，模型输出的熵往往会发生坍缩，使多样性和性能提升受限，对此机制和对策的系统性研究仍缺乏。

Method: 作者通过大量实证实验，系统考察了RLVR训练下模型熵的变化，并分析了模型熵与输出多样性、校准度以及实际性能的关系。作者还理论与实证结合，揭示了具有正优势的token是熵下降的主要原因，并通过调整正负优势token在损失中的权重来调节熵。

Result: 实验结果表明，离策略更新的次数、训练数据多样性和目标函数中的剪裁阈值是影响RLVR训练模型熵的关键因素。进一步，理论与实证都验证了正优势token在熵坍缩中起核心作用，并通过损失权重调节能有效管理模型熵。

Conclusion: 这项工作揭示了RLVR框架下模型熵的关键影响因素及其机理，并提出了调节熵、提升模型性能和多样性的有效思路，为LLM的RLVR训练实践提供了理论和方法依据。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a
predominant approach for enhancing the reasoning capabilities of large language
models (LLMs). However, the entropy of LLMs usually collapses during RLVR
training, causing premature convergence to suboptimal local minima and hinder
further performance improvement. Although various approaches have been proposed
to mitigate entropy collapse, a comprehensive study of entropy in RLVR remains
lacking. To address this gap, we conduct extensive experiments to investigate
the entropy dynamics of LLMs trained with RLVR and analyze how model entropy
correlates with response diversity, calibration, and performance across various
benchmarks. Our findings reveal that the number of off-policy updates, the
diversity of training data, and the clipping thresholds in the optimization
objective are critical factors influencing the entropy of LLMs trained with
RLVR. Moreover, we theoretically and empirically demonstrate that tokens with
positive advantages are the primary contributors to entropy collapse, and that
model entropy can be effectively regulated by adjusting the relative loss
weights of tokens with positive and negative advantages during training.

</details>


### [240] [LLMs Do Not See Age: Assessing Demographic Bias in Automated Systematic Review Synthesis](https://arxiv.org/abs/2511.06000)
*Favour Yahdii Aghaebe,Tanefa Apekey,Elizabeth Williams,Nafise Sadat Moosavi*

Main category: cs.CL

TL;DR: 本研究评估了先进的语言模型在医学综述摘要生成中保留年龄相关信息的能力，发现模型对不同年龄群体表现不一，存在信息丢失和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 不同年龄群体（儿童、成年人、老年人）在医学干预中的反应存在显著差异，然而当前AI辅助的生物医学文献总结流程往往忽视了这些重要的人口学特征，可能造成年龄相关信息的失真或缺失。因此，研究团队旨在分析主流大模型在汇总医学研究时，能否准确反映与年龄相关的信息。

Method: 作者构建了一个按年龄分层（儿童、成年人、老年人）的系统综述主文献数据集DemogSummary，评估了三个主流具摘要能力的大型语言模型（Qwen、Longformer、GPT-4.1 Nano）。除了常规评测指标，还提出了用于衡量年龄实体保留与错报的Demographic Salience Score(DSS)。

Result: 不同模型和不同年龄组存在系统性差异。其中，面向成年人的摘要在人口特征保真度上最低，而在代表性不足的人群中幻觉现象更为常见。

Conclusion: 当前的大型语言模型在医学综述摘要中难以做到对年龄信息的忠实保留，并存在偏见问题。未来生物医学NLP需引入关注公平性的评测体系与摘要流程。

Abstract: Clinical interventions often hinge on age: medications and procedures safe
for adults may be harmful to children or ineffective for older adults. However,
as language models are increasingly integrated into biomedical evidence
synthesis workflows, it remains uncertain whether these systems preserve such
crucial demographic distinctions. To address this gap, we evaluate how well
state-of-the-art language models retain age-related information when generating
abstractive summaries of biomedical studies. We construct DemogSummary, a novel
age-stratified dataset of systematic review primary studies, covering child,
adult, and older adult populations. We evaluate three prominent
summarisation-capable LLMs, Qwen (open-source), Longformer (open-source) and
GPT-4.1 Nano (proprietary), using both standard metrics and a newly proposed
Demographic Salience Score (DSS), which quantifies age-related entity retention
and hallucination. Our results reveal systematic disparities across models and
age groups: demographic fidelity is lowest for adult-focused summaries, and
under-represented populations are more prone to hallucinations. These findings
highlight the limitations of current LLMs in faithful and bias-free
summarisation and point to the need for fairness-aware evaluation frameworks
and summarisation pipelines in biomedical NLP.

</details>


### [241] [Multi-Reward GRPO Fine-Tuning for De-biasing Large Language Models: A Study Based on Chinese-Context Discrimination Data](https://arxiv.org/abs/2511.06023)
*Deng Yixuan,Ji Xiaoqiang*

Main category: cs.CL

TL;DR: 本文提出了一种多奖励组相对策略优化（GRPO）框架，用于微调大语言模型（LLM），以减少其在特定文化背景下的歧视性偏见，同时保证输出的流畅性和信息量。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型常存在反映社会刻板印象的偏见问题，尤其难以处理具有文化特性的多维度歧视；且主流对齐技术（RLHF、DPO）在这方面仍有局限。作者希望提出新方法更好地从多维度减少模型输出中的偏见。

Method: 作者基于中国语境下的歧视类别（如地域、族群、职业），人工构建了带中性和歧视性回复的英语合成数据集。训练DeBERTa-v3奖励模型，对公平性、中立性和语言质量等维度评分，并用该奖励模型指导GRPO微调LLM，使其输出更符合伦理和无偏标准。

Result: 实验表明，经GRPO多奖励优化后的LLM，在减少偏见强度和提升非歧视性行为对齐方面效果显著，且没有牺牲输出的流畅性和信息性。

Conclusion: GRPO框架能够有效实现大语言模型的去偏差优化，特别适用于应对特定文化环境下的多维度歧视，有望为伦理对齐任务提供可复用方法。

Abstract: Large Language Models (LLMs) often exhibit implicit biases and discriminatory
tendencies that reflect underlying social stereotypes. While recent alignment
techniques such as RLHF and DPO have mitigated some of these issues, they
remain limited in addressing culturally specific and multi-dimensional forms of
discrimination. This paper proposes a Multi-Reward Group Relative Policy
Optimization (GRPO) framework to fine-tune LLMs toward ethical and bias-free
behavior. Our approach constructs a synthetic English-language dataset derived
from Chinese-context discrimination categories, including regional, ethnic, and
occupational biases. Each instance is paired with both neutral and biased
responses to train a reward model based on DeBERTa-v3, which provides
multi-dimensional reward signals capturing fairness, neutrality, and linguistic
quality. The trained reward model then guides GRPO fine-tuning to optimize
model outputs along these ethical dimensions. Experimental results demonstrate
significant reductions in bias intensity and improved alignment with
non-discriminatory standards without compromising fluency or informativeness.
This study highlights the effectiveness of GRPO-based multi-reward optimization
for de-biasing LLMs and offers a replicable framework for cultural-contextual
ethical alignment.

</details>


### [242] [Visual Exploration of Feature Relationships in Sparse Autoencoders with Curated Concepts](https://arxiv.org/abs/2511.06048)
*Xinyuan Yan,Shusen Liu,Kowshik Thopalli,Bei Wang*

Main category: cs.CL

TL;DR: 该论文提出了一种针对稀疏自编码器（SAE）特征探索的新型交互式可视化系统，以提升在大型语言模型中可解释性特征的分析效率和深度。该系统结合了基于拓扑的编码和降维方法，专注于重点特征和概念。


<details>
  <summary>Details</summary>
Motivation: 随着LLM中通过SAE发现的可解释特征数量激增，全面探索这些方向变得不可行。而目前常用的降维可视化方法（如UMAP）存在压缩失真、重叠遮挡和邻域失真等问题。因此需要更高效且准确的分类与分析方法。

Method: 作者提出了一种聚焦式探索框架，优先分析精选的概念及其对应的SAE特征，而非一次性展现所有特征。具体方法是在可视化系统中融合拓扑可视化编码和常规降维技术，从而在保证局部及全局关系准确表达的前提下，对特征子集进行交互式分析。

Result: 新系统可以帮助用户在更小且有意义的特征子集中，深入探索SAE行为空间中的语义表示，相比于传统系统，能更有效地识别和分析关键概念及其相互关系。

Conclusion: 通过集中探索代表性子集与优化可视化方式，该方法提升了用户针对LLM隐空间可解释性特征的发现效率和分析深度，为深入理解模型内在机制提供了新的工具和视角。

Abstract: Sparse autoencoders (SAEs) have emerged as a powerful tool for uncovering
interpretable features in large language models (LLMs) through the sparse
directions they learn. However, the sheer number of extracted directions makes
comprehensive exploration intractable. While conventional embedding techniques
such as UMAP can reveal global structure, they suffer from limitations
including high-dimensional compression artifacts, overplotting, and misleading
neighborhood distortions. In this work, we propose a focused exploration
framework that prioritizes curated concepts and their corresponding SAE
features over attempts to visualize all available features simultaneously. We
present an interactive visualization system that combines topology-based visual
encoding with dimensionality reduction to faithfully represent both local and
global relationships among selected features. This hybrid approach enables
users to investigate SAE behavior through targeted, interpretable subsets,
facilitating deeper and more nuanced analysis of concept representation in
latent space.

</details>


### [243] [Efficient Hate Speech Detection: A Three-Layer LoRA-Tuned BERTweet Framework](https://arxiv.org/abs/2511.06051)
*Mahmoud El-Bahnasawi*

Main category: cs.CL

TL;DR: 本文提出了一种高效的仇恨言论检测系统，兼顾性能与实用性，适合实时部署。


<details>
  <summary>Details</summary>
Motivation: 现有高性能仇恨言论检测多依赖庞大模型，难以在资源受限的真实环境中实时部署，因此需要更经济高效的检测解决方案。

Method: 设计了三层架构，结合规则过滤、经过LoRA微调的BERTweet模型和持续学习机制，采用数据集统一与优化微调策略。

Result: 在保持小模型规模（仅134M参数，远小于主流大模型）的前提下，检测性能达0.85 macro F1分，相当于更大模型的94%。训练所需参数和时间大幅降低，仅需1.87M参数和2小时T4 GPU训练。

Conclusion: 该方法在极低的资源消耗下，实现了与主流大模型接近的仇恨言论检测性能，非常适合资源受限环境的实时应用。

Abstract: This paper addresses the critical challenge of developing computationally
efficient hate speech detection systems that maintain competitive performance
while being practical for real-time deployment. We propose a novel three-layer
framework that combines rule-based pre-filtering with a parameter-efficient
LoRA-tuned BERTweet model and continuous learning capabilities. Our approach
achieves 0.85 macro F1 score - representing 94% of the performance of
state-of-the-art large language models like SafePhi (Phi-4 based) while using a
base model that is 100x smaller (134M vs 14B parameters). Compared to
traditional BERT-based approaches with similar computational requirements, our
method demonstrates superior performance through strategic dataset unification
and optimized fine-tuning. The system requires only 1.87M trainable parameters
(1.37% of full fine-tuning) and trains in approximately 2 hours on a single T4
GPU, making robust hate speech detection accessible in resource-constrained
environments while maintaining competitive accuracy for real-world deployment.

</details>


### [244] [ReMoD: Rethinking Modality Contribution in Multimodal Stance Detection via Dual Reasoning](https://arxiv.org/abs/2511.06057)
*Bingbing Wang,Zhengda Jin,Bin Liang,Jing Li,Ruifeng Xu*

Main category: cs.CL

TL;DR: 本文提出了一种用于多模态立场检测（MSD）的新方法——ReMoD，实现了对不同模态在立场表达中贡献的动态权重分配，有效提升了立场检测的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态立场检测方法简单融合各模态信息，忽略了不同模态贡献的不均衡，导致立场误判甚至引入噪声。因此，需要更细致地考量各模态在立场表达中的实际作用。

Method: 受人类认知双过程理论启发，作者提出ReMoD，该方法通过直觉与反思两个阶段：直觉阶段利用历史经验池初步判断立场，优先考虑影响较大的模态；反思阶段通过两条推理链分别优化模态融合与语义理解，动态调整各模态贡献，持续优化经验池。

Result: 在公开数据集MMSD上的大量实验表明，ReMoD在准确性和泛化性上均优于大多数现有方法。

Conclusion: ReMoD通过结合直觉和反思推理，实现对模态贡献的动态建模，大大提升了多模态立场检测的效果，有效解决了单纯粗暴融合带来的问题。

Abstract: Multimodal Stance Detection (MSD) is a crucial task for understanding public
opinion on social media. Existing work simply fuses information from various
modalities to learn stance representations, overlooking the varying
contributions of stance expression from different modalities. Therefore, stance
misunderstanding noises may be drawn into the stance learning process due to
the risk of learning errors by rough modality combination. To address this, we
get inspiration from the dual-process theory of human cognition and propose
**ReMoD**, a framework that **Re**thinks **Mo**dality contribution of stance
expression through a **D**ual-reasoning paradigm. ReMoD integrates
*experience-driven intuitive reasoning* to capture initial stance cues with
*deliberate reflective reasoning* to adjust for modality biases, refine stance
judgments, and thereby dynamically weight modality contributions based on their
actual expressive power for the target stance. Specifically, the intuitive
stage queries the Modality Experience Pool (MEP) and Semantic Experience Pool
(SEP) to form an initial stance hypothesis, prioritizing historically impactful
modalities. This hypothesis is then refined in the reflective stage via two
reasoning chains: Modality-CoT updates MEP with adaptive fusion strategies to
amplify relevant modalities, while Semantic-CoT refines SEP with deeper
contextual insights of stance semantics. These dual experience structures are
continuously refined during training and recalled at inference to guide robust
and context-aware stance decisions. Extensive experiments on the public MMSD
benchmark demonstrate that our ReMoD significantly outperforms most baseline
models and exhibits strong generalization capabilities.

</details>


### [245] [Automating Hardware Design and Verification from Architectural Papers via a Neural-Symbolic Graph Framework](https://arxiv.org/abs/2511.06067)
*Haoyue Yang,Xuanle Zhao,Yujie Liu,Zhuojun Zou,Kailin Lyu,Changchun Zhou,Yao Zhu,Jie Hao*

Main category: cs.CL

TL;DR: 本文提出了ArchCraft框架，可将学术论文中的硬件架构描述自动转换为可综合的Verilog项目，并实现寄存器传输级（RTL）验证。提出了相关基准测试集ArchSynthBench，用于系统化评估该方法。结果显示，ArchCraft在理解论文和代码生成方面均领先于现有方案，生成的代码在物理实现中性能可靠。


<details>
  <summary>Details</summary>
Motivation: 学术论文中硬件架构往往缺乏源码公开，且硬件描述语言复杂，导致难以复现和验证这些架构。因此，迫切需要一种自动化工具，将学术论文中的抽象架构描述转换成可实现、可验证的硬件代码。

Method: 提出了ArchCraft框架，通过形式化图和符号结构化挖掘学术论文中的蓝图和功能规格，实现架构信息到Verilog RTL代码及测试平台的自动生成。同时，提出了ArchSynthBench基准，提供50个项目级电路和约600个电路模块供评测。方法还支持独立的验证和调试流程，输出包括功耗、面积和性能等指标。

Result: 在ArchSynthBench基准上系统评估表明，ArchCraft在论文理解和代码自动生成方面优于直接生成方法和VerilogCoder框架。生成的RTL代码经物理实现后无时序违例，性能指标与原论文一致。

Conclusion: ArchCraft能高效、准确地将学术论文中的硬件架构转换为实际可综合的Verilog项目，并且具备系统化验证能力。该框架有望促进学术界硬件设计的复现和自动化，实现学术与工程间的桥梁。

Abstract: The reproduction of hardware architectures from academic papers remains a
significant challenge due to the lack of publicly available source code and the
complexity of hardware description languages (HDLs). To this end, we propose
\textbf{ArchCraft}, a Framework that converts abstract architectural
descriptions from academic papers into synthesizable Verilog projects with
register-transfer level (RTL) verification. ArchCraft introduces a structured
workflow, which uses formal graphs to capture the Architectural Blueprint and
symbols to define the Functional Specification, translating unstructured
academic papers into verifiable, hardware-aware designs. The framework then
generates RTL and testbench (TB) code decoupled via these symbols to facilitate
verification and debugging, ultimately reporting the circuit's Power, Area, and
Performance (PPA). Moreover, we propose the first benchmark,
\textbf{ArchSynthBench}, for synthesizing hardware from architectural
descriptions, with a complete set of evaluation indicators, 50 project-level
circuits, and around 600 circuit blocks. We systematically assess ArchCraft on
ArchSynthBench, where the experiment results demonstrate the superiority of our
proposed method, surpassing direct generation methods and the VerilogCoder
framework in both paper understanding and code completion. Furthermore,
evaluation and physical implementation of the generated executable RTL code
show that these implementations meet all timing constraints without violations,
and their performance metrics are consistent with those reported in the
original papers.

</details>


### [246] [Stemming Hallucination in Language Models Using a Licensing Oracle](https://arxiv.org/abs/2511.06073)
*Simeon Emanuilov,Richard Ackermann*

Main category: cs.CL

TL;DR: 该论文提出一种名为Licensing Oracle的新型架构，用于有效抑制大模型的幻觉现象，通过知识图谱进行事实验证，只允许输出真实信息，实验表现优于主流方法，具有里程碑意义。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型生成能力强，但常因幻觉问题生成虚假内容，现有细调或检索增强方法无法彻底消除这一现象，因此亟需一种能从架构层面根本约束虚假生成的方案。

Method: 设计并引入Licensing Oracle架构，在生成过程中内嵌确定性的知识图谱验证步骤，输出前对内容事实准确性强制约束，与基础模型、细调和RAG等方法进行对比实验，综合评估效果。

Result: Licensing Oracle在实验中取得完美的弃答精度（AP=1.0）和零虚假答案（FAR-NE=0.0），在事实响应上准确率达89.1%，显著优于RAG和各类细调模型，彻底杜绝事实类幻觉出现。

Conclusion: Licensing Oracle展示了通过架构创新，尤其是在结构化知识领域，可充分解决模型幻觉问题，并提供了统计方法无法达到的理论保证，为后续可信赖AI模型的设计指明了新方向。

Abstract: Language models exhibit remarkable natural language generation capabilities
but remain prone to hallucinations, generating factually incorrect information
despite producing syntactically coherent responses. This study introduces the
Licensing Oracle, an architectural solution designed to stem hallucinations in
LMs by enforcing truth constraints through formal validation against structured
knowledge graphs. Unlike statistical approaches that rely on data scaling or
fine-tuning, the Licensing Oracle embeds a deterministic validation step into
the model's generative process, ensuring that only factually accurate claims
are made. We evaluated the effectiveness of the Licensing Oracle through
experiments comparing it with several state-of-the-art methods, including
baseline language model generation, fine-tuning for factual recall, fine-tuning
for abstention behavior, and retrieval-augmented generation (RAG). Our results
demonstrate that although RAG and fine-tuning improve performance, they fail to
eliminate hallucinations. In contrast, the Licensing Oracle achieved perfect
abstention precision (AP = 1.0) and zero false answers (FAR-NE = 0.0), ensuring
that only valid claims were generated with 89.1% accuracy in factual responses.
This work shows that architectural innovations, such as the Licensing Oracle,
offer a necessary and sufficient solution for hallucinations in domains with
structured knowledge representations, offering guarantees that statistical
methods cannot match. Although the Licensing Oracle is specifically designed to
address hallucinations in fact-based domains, its framework lays the groundwork
for truth-constrained generation in future AI systems, providing a new path
toward reliable, epistemically grounded models.

</details>


### [247] [MuonAll: Muon Variant for Efficient Finetuning of Large Language Models](https://arxiv.org/abs/2511.06086)
*Saurabh Page,Advait Joshi,S. S. Sonawane*

Main category: cs.CL

TL;DR: 本文提出了MuonAll优化器，并在多项语言模型微调任务中表现良好，与主流AdamW优化器性能相当。作者还开源了相关代码。


<details>
  <summary>Details</summary>
Motivation: Muon优化器在语言模型预训练中有出色表现，但在已有模型微调任务上的效果尚未被系统研究。此外，现有Muon仍需与AdamW结合，参数集成不够完整，有待改进。

Method: 作者提出MuonAll优化器，将所有参数通过2D矩阵方式整合进Muon架构内，能够独立完成优化而不依赖AdamW。通过在多种公开语言模型及不同参数规模下开展全面微调实验，与AdamW和Muon进行系统对比。

Result: 实验表明，Muon和MuonAll在多个主流基准数据集上微调时，与AdamW具有相当的效果。

Conclusion: Muon及其改进版MuonAll可作为AdamW的有效替代优化器，适用于大规模语言模型的微调。实现已开源，有利于社区进一步应用和研究。

Abstract: Muon optimizer has demonstrated robust results in pretraining of language
models but its performance in finetuning of existing public pretrained models
is not yet explored. Currently, Muon is used along with AdamW introducing a
scope of improvement for adopting all parameters inside Muon. We introduce
MuonAll, which incorporates all the parameters inside Muon by transforming into
2D matrices. We conduct extensive finetuning experiments across publicly
available language models with model sizes upto half billion parameters. Muon
and MuonAll perform at par with AdamW across major benchmarks, highlighting
their effectiveness as alternative optimizers. We open-source the distributed
implementations of Muon and MuonAll, available at
https://github.com/Saurabh750/optimizer

</details>


### [248] [Evaluation of retrieval-based QA on QUEST-LOFT](https://arxiv.org/abs/2511.06125)
*Nathan Scales,Nathanael Schärli,Olivier Bousquet*

Main category: cs.CL

TL;DR: 论文分析了当前检索增强生成（RAG）方法在处理信息分布于多文档或需复杂推理的问题时的局限性，并提出通过结构化输出和答案复核优化RAG的方法，可大幅提升性能。


<details>
  <summary>Details</summary>
Motivation: RAG广泛用于基础问答，但在需要多文档信息整合和复杂推理的问题上表现不佳，现有长上下文模型也有类似问题，QUEST基准展现了明显改进空间。论文希望找出性能瓶颈并优化方法。

Method: 对QUEST-LOFT基准上RAG和长上下文模型表现进行深入分析，基于人工评估发布更准确的实验结果，并提出结合结构化输出格式和答案复核的RAG优化方案。

Result: 通过新的评测和优化方法，RAG在结构化输出和证据推理支持下，显著优于传统长上下文模型。

Conclusion: 优化后的RAG方法通过引入结构化输出和答案复核，在多文档分布和复杂推理问答任务中效果更好，优于现有基于长上下文的解决方案。

Abstract: Despite the popularity of retrieval-augmented generation (RAG) as a solution
for grounded QA in both academia and industry, current RAG methods struggle
with questions where the necessary information is distributed across many
documents or where retrieval needs to be combined with complex reasoning.
Recently, the LOFT study has shown that this limitation also applies to
approaches based on long-context language models, with the QUEST benchmark
exhibiting particularly large headroom. In this paper, we provide an in-depth
analysis of the factors contributing to the poor performance on QUEST-LOFT,
publish updated numbers based on a thorough human evaluation, and demonstrate
that RAG can be optimized to significantly outperform long-context approaches
when combined with a structured output format containing reasoning and
evidence, optionally followed by answer re-verification.

</details>


### [249] [Referring Expressions as a Lens into Spatial Language Grounding in Vision-Language Models](https://arxiv.org/abs/2511.06146)
*Akshar Tumu,Varad Shinde,Parisa Kordjamshidi*

Main category: cs.CL

TL;DR: 本文提出利用指代表达理解任务（Referring Expression Comprehension）来更深入评估视觉语言模型（VLMs）在空间推理方面的表现，揭示现有模型在处理复杂空间语义时的瓶颈和研究方向。


<details>
  <summary>Details</summary>
Motivation: 人类认知中空间推理能力十分重要，但现有视觉语言模型在此方面依然表现不佳。过去的工作多数采用图像描述和视觉问答方法分析模型的空间推理；作者认为这些方法不足以覆盖空间理解的复杂性，因此提出新的分析方法。

Method: 作者以指代表达理解任务为评估平台，设计涵盖目标检测歧义、复杂长句空间关系、否定表达等多种情况的任务，通过比较任务型架构和大规模通用VLMs，系统评估它们在不同空间语义（如拓扑、方向、接近性等）下的表现差异。

Result: 结果显示，所有类型的模型在上述三类空间推理情景下均表现出挑战性，但不同模型和不同空间语义类别下的具体表现差异较大。论文实验总结了模型的优劣和潜在不足。

Conclusion: 论文指出了视觉语言模型在空间推理理解方面的突出挑战和典型问题，为后续该领域的优化和研究方向提供了新的视角和启示。

Abstract: Spatial Reasoning is an important component of human cognition and is an area
in which the latest Vision-language models (VLMs) show signs of difficulty. The
current analysis works use image captioning tasks and visual question
answering. In this work, we propose using the Referring Expression
Comprehension task instead as a platform for the evaluation of spatial
reasoning by VLMs. This platform provides the opportunity for a deeper analysis
of spatial comprehension and grounding abilities when there is 1) ambiguity in
object detection, 2) complex spatial expressions with a longer sentence
structure and multiple spatial relations, and 3) expressions with negation
('not'). In our analysis, we use task-specific architectures as well as large
VLMs and highlight their strengths and weaknesses in dealing with these
specific situations. While all these models face challenges with the task at
hand, the relative behaviors depend on the underlying models and the specific
categories of spatial semantics (topological, directional, proximal, etc.). Our
results highlight these challenges and behaviors and provide insight into
research gaps and future directions.

</details>


### [250] [BookAsSumQA: An Evaluation Framework for Aspect-Based Book Summarization via Question Answering](https://arxiv.org/abs/2511.06183)
*Ryuhei Miyazato,Ting-Ruen Wei,Xuyang Wu,Hsin-Tai Wu,Kei Harada*

Main category: cs.CL

TL;DR: 该论文提出了一种用于评估面向特定方面的书籍自动摘要的方法BookAsSumQA，通过自动生成基于知识图谱的QA对以评估摘要质量。结果显示，随文本长度增加，基于检索增强生成（RAG）的方法比大型语言模型（LLM）更有效。


<details>
  <summary>Details</summary>
Motivation: 面向特定方面的摘要可提升个性化与针对性，但实际应用于长文本（如书籍）时，缺乏参考摘要极大限制了研究与应用。为解决长文档摘要评估困难，亟需新型自动评测方法。

Method: 作者提出BookAsSumQA框架，利用叙事知识图谱自动生成与方面相关的QA对，通过摘要在相应问答任务中的表现来评价摘要质量，无需人工建立参考摘要。

Result: 实验发现，对于较短文本，大型语言模型（LLM）摘要性能较好；但文档长度增长后，基于检索增强生成（RAG）的方法展现出更高效且实用的表现。

Conclusion: BookAsSumQA为长文本面向方面的摘要评测提供了一条可行途径，RAG方法在长书籍场景下表现优异，有望推进自动化个性化书籍摘要的发展。

Abstract: Aspect-based summarization aims to generate summaries that highlight specific
aspects of a text, enabling more personalized and targeted summaries. However,
its application to books remains unexplored due to the difficulty of
constructing reference summaries for long text. To address this challenge, we
propose BookAsSumQA, a QA-based evaluation framework for aspect-based book
summarization. BookAsSumQA automatically generates aspect-specific QA pairs
from a narrative knowledge graph to evaluate summary quality based on its
question-answering performance. Our experiments using BookAsSumQA revealed that
while LLM-based approaches showed higher accuracy on shorter texts, RAG-based
methods become more effective as document length increases, making them more
efficient and practical for aspect-based book summarization.

</details>


### [251] [Confidence-Guided Stepwise Model Routing for Cost-Efficient Reasoning](https://arxiv.org/abs/2511.06190)
*Sangmook Lee,Dohyung Kim,Hyukhun Koh,Nakyeong Yang,Kyomin Jung*

Main category: cs.CL

TL;DR: 提出了一种新的高效模型推理框架STEER, 利用小模型自身的置信度信息分步动态切换至大模型, 达到显著降低算力开销的同时保持甚至提升推理准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理能力增强但推理成本高，现有降低成本的方法如用路由器模型分配任务易受领域变化影响且训练代价高，急需更稳健、高效的新方法。

Method: STEER框架通过小模型每一步推理前的logits置信分数决定是否切换至大模型，无需外部路由模型，也不需额外训练和数据合成，分步粒度地动态选择合适模型。

Result: 在数学推理、多跳问答、规划等多领域基准测试上，STEER方法可实现高达+20%准确率提升并减少48%算力消耗，表现优于依赖外部训练模块的基线方法。

Conclusion: STEER证明了模型内部置信度是健壮、通用的模型路由信号，为大模型推理高效部署提供了可扩展路径。

Abstract: Recent advances in Large Language Models (LLMs) - particularly model scaling
and test-time techniques - have greatly enhanced the reasoning capabilities of
language models at the expense of higher inference costs. To lower inference
costs, prior works train router models or deferral mechanisms that allocate
easy queries to a small, efficient model, while forwarding harder queries to
larger, more expensive models. However, these trained router models often lack
robustness under domain shifts and require expensive data synthesis techniques
such as Monte Carlo rollouts to obtain sufficient ground-truth routing labels
for training. In this work, we propose Confidence-Guided Stepwise Model Routing
for Cost-Efficient Reasoning (STEER), a domain-agnostic framework that performs
fine-grained, step-level routing between smaller and larger LLMs without
utilizing external models. STEER leverages confidence scores from the smaller
model's logits prior to generating a reasoning step, so that the large model is
invoked only when necessary. Extensive evaluations using different LLMs on a
diverse set of challenging benchmarks across multiple domains such as
Mathematical Reasoning, Multi-Hop QA, and Planning tasks indicate that STEER
achieves competitive or enhanced accuracy while reducing inference costs (up to
+20% accuracy with 48% less FLOPs compared to solely using the larger model on
AIME), outperforming baselines that rely on trained external modules. Our
results establish model-internal confidence as a robust, domain-agnostic signal
for model routing, offering a scalable pathway for efficient LLM deployment.

</details>


### [252] [Explicit Knowledge-Guided In-Context Learning for Early Detection of Alzheimer's Disease](https://arxiv.org/abs/2511.06215)
*Puzhen Su,Yongzhu Miao,Chunxi Guo,Jintao Tang,Shasha Li,Ting Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新方法EK-ICL，通过引入结构化显式知识，提升大语言模型在阿尔茨海默病（AD）文本检测中的稳定性和效果，特别适用于数据稀缺和分布外（OOD）场景。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在叙述文本中检测AD任务时，在数据不足或与训练数据分布不同的情况下表现不佳。常用的ICL方法存在任务识别失败、演示样本选择不佳及标签与任务目标未对齐等问题，这些在临床领域尤为突出。

Method: 提出EK-ICL框架，结合显式知识增强ICL表现。具体包括：（1）使用小型语言模型的置信分数强化与任务相关的预测；（2）用句法特征分数辅助演示样本挑选、捕捉结构性差异；（3）通过标签词替换解决LLM推理中语义错配。除此之外，还引入以解析为基础的检索方法和集成预测策略来缓解AD文本高度同质化带来的影响。

Result: EK-ICL在三个AD公开数据集上大幅优于现有最优的微调和ICL基线方法。分析表明，ICL在AD检测中的表现极度依赖于标签语义和任务上下文的对齐。

Conclusion: 引入显式、结构化知识能显著提高大语言模型在临床领域低资源、任务困难条件下的推理可靠性和性能。EK-ICL为类似任务提供了有效的新思路。

Abstract: Detecting Alzheimer's Disease (AD) from narrative transcripts remains a
challenging task for large language models (LLMs), particularly under
out-of-distribution (OOD) and data-scarce conditions. While in-context learning
(ICL) provides a parameter-efficient alternative to fine-tuning, existing ICL
approaches often suffer from task recognition failure, suboptimal demonstration
selection, and misalignment between label words and task objectives, issues
that are amplified in clinical domains like AD detection. We propose Explicit
Knowledge In-Context Learners (EK-ICL), a novel framework that integrates
structured explicit knowledge to enhance reasoning stability and task alignment
in ICL. EK-ICL incorporates three knowledge components: confidence scores
derived from small language models (SLMs) to ground predictions in
task-relevant patterns, parsing feature scores to capture structural
differences and improve demo selection, and label word replacement to resolve
semantic misalignment with LLM priors. In addition, EK-ICL employs a
parsing-based retrieval strategy and ensemble prediction to mitigate the
effects of semantic homogeneity in AD transcripts. Extensive experiments across
three AD datasets demonstrate that EK-ICL significantly outperforms
state-of-the-art fine-tuning and ICL baselines. Further analysis reveals that
ICL performance in AD detection is highly sensitive to the alignment of label
semantics and task-specific context, underscoring the importance of explicit
knowledge in clinical reasoning under low-resource conditions.

</details>


### [253] [SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization](https://arxiv.org/abs/2511.06222)
*Yue Huang,Xiangqi Wang,Xiangliang Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种新的大语言模型（LLM）对齐范式，优先保证可信（如安全、诚实）后再提升有用性，从而平衡高风险场景下的安全与实用性。


<details>
  <summary>Details</summary>
Motivation: 在自残、法律、医疗等高风险场景下，模型既要可靠也要有用，但这两个目标常常冲突，因此需要新的方法优先确保模型输出的安全与可信。

Method: 提出“优先级对齐”，要求输出先达成可信标准再优化有用性。具体采用完全无监督的SPA框架，模型自生成多样响应、自评并精炼答案，双标准降噪去除不一致和异常，生成字典序排序偏好对。最后利用带不确定性加权的对齐损失进行微调，强化高置信、高差距的决策。

Result: 在多个基准测试上，SPA在不损害安全性的前提下提升模型有用性，表现优于现有强基线，并保持了模型通用能力。

Conclusion: SPA为关键场景下大模型安全对齐提供了一种可扩展、具有可解释性的解决方案，兼顾了安全与帮助性。

Abstract: In high-stakes scenarios-such as self-harm, legal, or medical queries-LLMs
must be both trustworthy and helpful. However, these goals often conflict. We
propose priority alignment, a new alignment paradigm that enforces a strict
"trustworthy-before-helpful" ordering: optimization of helpfulness is
conditioned on first meeting trustworthy thresholds (e.g., harmlessness or
honesty). To realize this, we introduce Self-Priority Alignment (SPA)-a fully
unsupervised framework that generates diverse responses, self-evaluates them
and refines them by the model itself, and applies dual-criterion denoising to
remove inconsistency and control variance. From this, SPA constructs
lexicographically ordered preference pairs and fine-tunes the model using an
uncertainty-weighted alignment loss that emphasizes high-confidence, high-gap
decisions. Experiments across multiple benchmarks show that SPA improves
helpfulness without compromising safety, outperforming strong baselines while
preserving general capabilities. Our results demonstrate that SPA provides a
scalable and interpretable alignment strategy for critical LLM applications.

</details>


### [254] [Overview of CHIP 2025 Shared Task 2: Discharge Medication Recommendation for Metabolic Diseases Based on Chinese Electronic Health Records](https://arxiv.org/abs/2511.06230)
*Juntao Li,Haobin Yuan,Ling Luo,Tengxiao Lv,Yan Jiang,Fan Wang,Ping Zhang,Huiyi Lv,Jian Wang,Yuanyuan Sun,Hongfei Lin*

Main category: cs.CL

TL;DR: 本文总结了CHIP 2025第二项比赛，旨在推进基于中国真实电子健康记录（EHR）的自动出院药物推荐。


<details>
  <summary>Details</summary>
Motivation: 慢性代谢性疾病患者出院后，如何延续治疗、避免再入院，并提升长期管理效果，亟需精准的出院药物推荐系统。

Method: 构建了包括5894份去标识化住院记录的CDrugRed高质量数据集，汇聚全中国3190名患者的真实数据，比赛中多队伍采用先进的大语言模型（LLM）与集成方法进行多标签药物推荐任务。

Result: 共有526支队伍报名，167支和95支团队分别完成A、B阶段测试。表现最好的团队在最终测试集上取得Jaccard分数0.5102，F1分数0.6267，显示LLM集成方法有很大潜力。

Conclusion: 大语言模型在中文EHR药物推荐中的前景可观，但实际应用尚存不少挑战，未来需继续优化。

Abstract: Discharge medication recommendation plays a critical role in ensuring
treatment continuity, preventing readmission, and improving long-term
management for patients with chronic metabolic diseases. This paper present an
overview of the CHIP 2025 Shared Task 2 competition, which aimed to develop
state-of-the-art approaches for automatically recommending appro-priate
discharge medications using real-world Chinese EHR data. For this task, we
constructed CDrugRed, a high-quality dataset consisting of 5,894 de-identified
hospitalization records from 3,190 patients in China. This task is challenging
due to multi-label nature of medication recommendation, het-erogeneous clinical
text, and patient-specific variability in treatment plans. A total of 526 teams
registered, with 167 and 95 teams submitting valid results to the Phase A and
Phase B leaderboards, respectively. The top-performing team achieved the
highest overall performance on the final test set, with a Jaccard score of
0.5102, F1 score of 0.6267, demonstrating the potential of advanced large
language model (LLM)-based ensemble systems. These re-sults highlight both the
promise and remaining challenges of applying LLMs to medication recommendation
in Chinese EHRs. The post-evaluation phase remains open at
https://tianchi.aliyun.com/competition/entrance/532411/.

</details>


### [255] [Analyzing and Mitigating Negation Artifacts using Data Augmentation for Improving ELECTRA-Small Model Accuracy](https://arxiv.org/abs/2511.06234)
*Mojtaba Noghabaei*

Main category: cs.CL

TL;DR: 本文针对自然语言推理（NLI）中的预训练模型经常依赖数据集伪相关而非真正理解否定表达的问题，发现ELECTRA-small模型在否定判别上表现不足。通过数据增强（加入否定相关对比集和对抗样本），提升了模型对否定类样本的识别能力，且整体表现无明显下降。


<details>
  <summary>Details</summary>
Motivation: 当前主流的NLI预训练模型虽然在基准数据集上表现优异，但很多时候是基于数据集中的伪相关特征而非真正语言推理能力，特别是在否定句处理方面存在短板。作者希望通过分析与改进，解决模型对否定理解不足的问题，提升其实用性和鲁棒性。

Method: 1. 选用ELECTRA-small模型在SNLI数据集上微调并分析其处理否定的表现；2. 通过构建基于否定的对比集与对抗性样本，对训练集进行定向增强；3. 评估加强后模型在一般样本和包含否定样本上的分类表现。

Result: 模型经过加入针对否定的数据增强后，在包含否定的样本上的分类准确率有明显提升，而整体任务表现未受到负面影响，说明针对性增强措施有效缓解了由数据集伪相关带来的问题。

Conclusion: 通过有针对性的对比集与对抗数据增强，可以有效提升NLI模型对否定表述的理解，有助于解决预训练模型过度依赖数据伪相关、对复杂语言现象敏感性不足的问题。

Abstract: Pre-trained models for natural language inference (NLI) often achieve high
performance on benchmark datasets by using spurious correlations, or dataset
artifacts, rather than understanding language touches such as negation. In this
project, we investigate the performance of an ELECTRA-small model fine-tuned on
the Stanford Natural Language Inference (SNLI) dataset, focusing on its
handling of negation. Through analysis, we identify that the model struggles
with correctly classifying examples containing negation. To address this, we
augment the training data with contrast sets and adversarial examples
emphasizing negation. Our results demonstrate that this targeted data
augmentation improves the model's accuracy on negation-containing examples
without adversely affecting overall performance, therefore mitigating the
identified dataset artifact.

</details>


### [256] [TimeSense:Making Large Language Models Proficient in Time-Series Analysis](https://arxiv.org/abs/2511.06344)
*Zhirui Zhang,Changhua Pei,Tianyi Gao,Zhe Xie,Yibo Hao,Zhaoyang Yu,Longlong Xu,Tong Xiao,Jing Han,Dan Pei*

Main category: cs.CL

TL;DR: 该论文提出了TimeSense框架，通过平衡文本推理与时间序列感知，提升大语言模型处理时序数据的能力，并构建了EvalTS评测基准集验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前结合文本和时序数据的方法虽然利用了大语言模型的推理能力，但由于训练时过度依赖文本标签，会使模型忽视完整的时间特征，导致输出与时序实际情况相矛盾。为解决这一问题，需要开发兼具文本推理与时序感知能力的新方法，并提供更具挑战性和现实性的评测基准。

Method: 作者构建了EvalTS评测基准，涵盖10项任务和不同难度，全面测试模型在基础模式识别到复杂推理场景下的能力。提出了TimeSense多模态框架，包含一个Temporal Sense模块，可在模型上下文中复现输入的时间序列，并加入基于坐标的位置嵌入，提升对时序空间依赖的捕捉能力。

Result: 实验结果显示，TimeSense在多个任务上均获得了当前最优表现，尤其在复杂多维的时序推理任务上，明显优于现有方法。

Conclusion: TimeSense能有效提升大语言模型在时序分析领域的综合能力，对于复杂时序推理任务尤其具有优势；EvalTS基准也为未来的相关研究提供了更系统的评测平台。

Abstract: In the time-series domain, an increasing number of works combine text with
temporal data to leverage the reasoning capabilities of large language models
(LLMs) for various downstream time-series understanding tasks. This enables a
single model to flexibly perform tasks that previously required specialized
models for each domain. However, these methods typically rely on text labels
for supervision during training, biasing the model toward textual cues while
potentially neglecting the full temporal features. Such a bias can lead to
outputs that contradict the underlying time-series context. To address this
issue, we construct the EvalTS benchmark, comprising 10 tasks across three
difficulty levels, from fundamental temporal pattern recognition to complex
real-world reasoning, to evaluate models under more challenging and realistic
scenarios. We also propose TimeSense, a multimodal framework that makes LLMs
proficient in time-series analysis by balancing textual reasoning with a
preserved temporal sense. TimeSense incorporates a Temporal Sense module that
reconstructs the input time-series within the model's context, ensuring that
textual reasoning is grounded in the time-series dynamics. Moreover, to enhance
spatial understanding of time-series data, we explicitly incorporate
coordinate-based positional embeddings, which provide each time point with
spatial context and enable the model to capture structural dependencies more
effectively. Experimental results demonstrate that TimeSense achieves
state-of-the-art performance across multiple tasks, and it particularly
outperforms existing methods on complex multi-dimensional time-series reasoning
tasks.

</details>


### [257] [HatePrototypes: Interpretable and Transferable Representations for Implicit and Explicit Hate Speech Detection](https://arxiv.org/abs/2511.06391)
*Irina Proskurina,Marc-Antoine Carpentier,Julien Velcin*

Main category: cs.CL

TL;DR: 本文提出利用HatePrototypes（一种类别级向量表示）实现不同类型仇恨言论检测的高效与可迁移方法，无需反复微调。


<details>
  <summary>Details</summary>
Motivation: 当前仇恨言论检测多通过现有基准集反复预训练或微调模型，这些基准集主要针对对受保护群体的明显仇恨，往往忽视如贬低比较、排斥号召、隐晦歧视等间接或隐性仇恨，这亟需改进。

Method: 通过语言模型为仇恨言论检测和安全审核任务优化后，提取每一类别的HatePrototype向量，并用少量示例（每类仅需约50个）对其进行构建，从而实现不同类型任务（显性与隐性仇恨）间的迁移检测，且支持无参数的早期退出推理机制。

Result: HatePrototypes能够高效且准确地跨任务应用于显性和隐性仇恨检测，且不同基准数据集之间的原型可互换。此外，这一方法无需参数调整就能进行早期退出，提升检测效率。

Conclusion: HatePrototypes方法在仇恨言论检测领域具有高效、可迁移和节省资源的优势，有助于更全面地覆盖多类仇恨言论。相关代码和资源已公开，以推动该方向研究进展。

Abstract: Optimization of offensive content moderation models for different types of
hateful messages is typically achieved through continued pre-training or
fine-tuning on new hate speech benchmarks. However, existing benchmarks mainly
address explicit hate toward protected groups and often overlook implicit or
indirect hate, such as demeaning comparisons, calls for exclusion or violence,
and subtle discriminatory language that still causes harm. While explicit hate
can often be captured through surface features, implicit hate requires deeper,
full-model semantic processing. In this work, we question the need for repeated
fine-tuning and analyze the role of HatePrototypes, class-level vector
representations derived from language models optimized for hate speech
detection and safety moderation. We find that these prototypes, built from as
few as 50 examples per class, enable cross-task transfer between explicit and
implicit hate, with interchangeable prototypes across benchmarks. Moreover, we
show that parameter-free early exiting with prototypes is effective for both
hate types. We release the code, prototype resources, and evaluation scripts to
support future research on efficient and transferable hate speech detection.

</details>


### [258] [SugarTextNet: A Transformer-Based Framework for Detecting Sugar Dating-Related Content on Social Media with Context-Aware Focal Loss](https://arxiv.org/abs/2511.06402)
*Lionel Z. Wang,Shihan Ben,Yulu Huang,Simeng Qing*

Main category: cs.CL

TL;DR: 本文提出了一种基于transformer的新模型SugarTextNet，有效识别社交媒体上与“援交”相关的内容，并在中文微博数据集上显著优于常规方法。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上“援交”内容增多，带来社会及监管问题；但因表达隐晦、语言模糊、数据极度类别不平衡，检测非常困难，急需更有效的自动检测方法。

Method: 提出SugarTextNet框架，结合预训练transformer编码器、基于注意力的线索提取器和上下文短语编码器，挖掘文本显性和隐性特征；为应对类别不平衡，引入结合上下文加权的Context-Aware Focal Loss损失函数。

Result: 在人工标注的3067条中文微博数据集上，SugarTextNet在各项指标上均超过传统机器学习、深度学习基线和大语言模型；消融实验验证各模块的重要性。

Conclusion: 领域专属、关注上下文的模型对于敏感内容检测至关重要，SugarTextNet为复杂现实场景中的内容监管提供了有效解决方案。

Abstract: Sugar dating-related content has rapidly proliferated on mainstream social
media platforms, giving rise to serious societal and regulatory concerns,
including commercialization of intimate relationships and the normalization of
transactional relationships.~Detecting such content is highly challenging due
to the prevalence of subtle euphemisms, ambiguous linguistic cues, and extreme
class imbalance in real-world data.~In this work, we present SugarTextNet, a
novel transformer-based framework specifically designed to identify sugar
dating-related posts on social media.~SugarTextNet integrates a pretrained
transformer encoder, an attention-based cue extractor, and a contextual phrase
encoder to capture both salient and nuanced features in user-generated text.~To
address class imbalance and enhance minority-class detection, we introduce
Context-Aware Focal Loss, a tailored loss function that combines focal loss
scaling with contextual weighting.~We evaluate SugarTextNet on a newly curated,
manually annotated dataset of 3,067 Chinese social media posts from Sina Weibo,
demonstrating that our approach substantially outperforms traditional machine
learning models, deep learning baselines, and large language models across
multiple metrics.~Comprehensive ablation studies confirm the indispensable role
of each component.~Our findings highlight the importance of domain-specific,
context-aware modeling for sensitive content detection, and provide a robust
solution for content moderation in complex, real-world scenarios.

</details>


### [259] [How Well Do LLMs Understand Drug Mechanisms? A Knowledge + Reasoning Evaluation Dataset](https://arxiv.org/abs/2511.06418)
*Sunil Mohan,Theofanis Karaletsos*

Main category: cs.CL

TL;DR: 本文关注LLMs在药物开发、个性化医疗领域的推理和知识能力，构建了评估数据集，并对多种模型进行实验比较，验证了部分小型模型的优越性及推理难度特点。


<details>
  <summary>Details</summary>
Motivation: 目前药物开发和个性化医疗领域越来越依赖LLMs，但需要检验其既能记忆事实，又能推断药物作用机制的能力，特别是在新颖和反事实的情境下。

Method: 作者构建了一个专门评估药物作用机制知识和推理能力的数据集，其中包括标准事实机制和反事实情境。利用该数据集，评测了不同规模和厂商的LLMs在开放世界和封闭世界任务，以及不同类型反事实推理上的表现。

Result: 实验发现，o4-mini模型优于OpenAI的4o、o3和o3-mini，Qwen3-4B-thinking模型的表现接近甚至部分超越o4-mini。开放世界任务比封闭世界更具挑战性，影响推理链中内部环节的反事实任务难度显著更高。

Conclusion: LLMs在药物作用机制推理中的表现受限于知识的召回与推断能力，数据集能有效反映其实际能力；部分小型模型表现优异，新颖情境、复杂推理链是下一步提升的关键。

Abstract: Two scientific fields showing increasing interest in pre-trained large
language models (LLMs) are drug development / repurposing, and personalized
medicine. For both, LLMs have to demonstrate factual knowledge as well as a
deep understanding of drug mechanisms, so they can recall and reason about
relevant knowledge in novel situations. Drug mechanisms of action are described
as a series of interactions between biomedical entities, which interlink into
one or more chains directed from the drug to the targeted disease. Composing
the effects of the interactions in a candidate chain leads to an inference
about whether the drug might be useful or not for that disease. We introduce a
dataset that evaluates LLMs on both factual knowledge of known mechanisms, and
their ability to reason about them under novel situations, presented as
counterfactuals that the models are unlikely to have seen during training.
Using this dataset, we show that o4-mini outperforms the 4o, o3, and o3-mini
models from OpenAI, and the recent small Qwen3-4B-thinking model closely
matches o4-mini's performance, even outperforming it in some cases. We
demonstrate that the open world setting for reasoning tasks, which requires the
model to recall relevant knowledge, is more challenging than the closed world
setting where the needed factual knowledge is provided. We also show that
counterfactuals affecting internal links in the reasoning chain present a much
harder task than those affecting a link from the drug mentioned in the prompt.

</details>


### [260] [Dutch Metaphor Extraction from Cancer Patients' Interviews and Forum Data using LLMs and Human in the Loop](https://arxiv.org/abs/2511.06427)
*Lifeng Han,David Lindevelt,Sander Puts,Erik van Mulligen,Suzan Verberne*

Main category: cs.CL

TL;DR: 本文研究荷兰癌症患者交流中隐喻语言的提取，利用大语言模型分析患者叙述与论坛数据，提出有效提示策略，最终建立了健康隐喻语料库HealthQuote.NL。


<details>
  <summary>Details</summary>
Motivation: 隐喻在医患沟通中对促进理解与病患健康素养提升至关重要，但自动提取患者实际使用的隐喻存在挑战。该研究旨在开发和验证如何高效提取并利用这些隐喻资源。

Method: 作者收集了两类荷兰癌症患者语言数据（访谈叙述和线上论坛），采用大语言模型（LLM），对比链式思维推理、少样本学习和自提示等不同提示策略，并通过人工审核建立可信隐喻语料库。

Result: 实验发现不同的提示策略对隐喻抽取表现不同，通过人工参与确保了数据质量，并最终汇总整理出HealthQuote.NL隐喻语料库。相关资源和提示方法也对外共享。

Conclusion: 研究表明，大语言模型可辅助高质量自动化患者隐喻抽取，有助于医患沟通优化、知情决策及健康素养提升，还可指导个性化医疗路径设计。

Abstract: Metaphors and metaphorical language (MLs) play an important role in
healthcare communication between clinicians, patients, and patients' family
members. In this work, we focus on Dutch language data from cancer patients. We
extract metaphors used by patients using two data sources: (1) cancer patient
storytelling interview data and (2) online forum data, including patients'
posts, comments, and questions to professionals. We investigate how current
state-of-the-art large language models (LLMs) perform on this task by exploring
different prompting strategies such as chain of thought reasoning, few-shot
learning, and self-prompting. With a human-in-the-loop setup, we verify the
extracted metaphors and compile the outputs into a corpus named HealthQuote.NL.
We believe the extracted metaphors can support better patient care, for example
shared decision making, improved communication between patients and clinicians,
and enhanced patient health literacy. They can also inform the design of
personalized care pathways. We share prompts and related resources at
https://github.com/aaronlifenghan/HealthQuote.NL

</details>


### [261] [Towards Resource-Efficient Multimodal Intelligence: Learned Routing among Specialized Expert Models](https://arxiv.org/abs/2511.06441)
*Mayank Saini,Arit Kumar Bishwas*

Main category: cs.CL

TL;DR: 本文提出一种统一的、模块化的智能任务路由框架，根据查询的类型（文本、多模态或复杂性）自动选择最适合的专家模型，从而实现高质量、低成本的推理服务。与始终调用大型语言模型相比，该方法在保持同等或更高性能的情况下，显著降低了对高成本模型的依赖（减少67%以上）。


<details>
  <summary>Details</summary>
Motivation: 随着AI应用扩展到视觉、音频和文档理解领域，大型语言模型虽然表现优异，但推理成本高昂，难以实时规模化部署。小模型虽成本低，但在复杂或多模态任务上能力有限。因此，亟需在成本与质量间寻求更优权衡。

Method: 提出了一个模块化设计的统一框架，利用学习型路由网络，根据输入查询的特征自动分配任务给合适的专家模型（文本、多模态等），具体包括：视觉任务采用高效的两阶段开源流水线，在子任务上复用高效经典视觉组件；整体通过多代理系统实现可扩展高效的模型编排。

Result: 在MMLU和VQA等基准上，实现与始终使用大型语言模型相当或超越的性能，同时高成本模型调用下降67%以上，展现出极高的资源利用效率。

Conclusion: 该多代理、多模型智能路由方案能够在保持高性能的同时，极大降低推理成本，为可扩展AI服务提供了新路径。

Abstract: As AI moves beyond text, large language models (LLMs) increasingly power
vision, audio, and document understanding; however, their high inference costs
hinder real-time, scalable deployment. Conversely, smaller open-source models
offer cost advantages but struggle with complex or multimodal queries. We
introduce a unified, modular framework that intelligently routes each query -
textual, multimodal, or complex - to the most fitting expert model, using a
learned routing network that balances cost and quality. For vision tasks, we
employ a two-stage open-source pipeline optimized for efficiency and reviving
efficient classical vision components where they remain SOTA for sub-tasks. On
benchmarks such as Massive Multitask Language Understanding (MMLU) and Visual
Question Answering (VQA), we match or exceed the performance of always-premium
LLM (monolithic systems with one model serving all query types) performance,
yet reduce the reliance on costly models by over 67%. With its extensible,
multi-agent orchestration, we deliver high-quality, resource-efficient AI at
scale.

</details>


### [262] [SR-KI: Scalable and Real-Time Knowledge Integration into LLMs via Supervised Attention](https://arxiv.org/abs/2511.06446)
*Bohan Yu,Wei Huang,Kang Liu*

Main category: cs.CL

TL;DR: 本文提出SR-KI方法，实现大规模结构化知识库实时集成到大语言模型（LLM）中，通过编码、注入及新型训练范式，提升知识检索和生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将知识库集成到LLM时，通常依赖外部检索器和复杂的多阶段流程，导致效率低且难以动态更新知识。此外，高效压缩与快速知识检索也是难题。作者希望解决如何实现端到端、高效、可动态更新的大规模知识库注入。

Method: 作者提出SR-KI框架，首先将知识库编码成键值对形式，通过预训练编码器注入到LLM的KV cache。训练时采用两阶段范式：第一阶段定位模型内专门的检索层，第二阶段在该层应用基于注意力的损失函数，显式监督模型关注相关知识库条目，实现端到端的检索和生成，无需依赖外部检索器。

Result: 实验证明SR-KI可在单块A100 40GB显卡上集成多达4万条知识库，7B参数模型在最佳任务上Recall@10超过98%，所有任务平均超过88%。在问答和知识库ID生成任务上，SR-KI在高性能的同时，实现高达99.75%的知识库压缩。

Conclusion: SR-KI方法实现了大规模知识库注入LLM的高效、灵活和高性能检索，提升了端到端推理效率，为知识与模型深度融合提供了有效途径。

Abstract: This paper proposes SR-KI, a novel approach for integrating real-time and
large-scale structured knowledge bases (KBs) into large language models (LLMs).
SR-KI begins by encoding KBs into key-value pairs using a pretrained encoder,
and injects them into LLMs' KV cache. Building on this representation, we
employ a two-stage training paradigm: first locating a dedicated retrieval
layer within the LLM, and then applying an attention-based loss at this layer
to explicitly supervise attention toward relevant KB entries. Unlike
traditional retrieval-augmented generation methods that rely heavily on the
performance of external retrievers and multi-stage pipelines, SR-KI supports
end-to-end inference by performing retrieval entirely within the models latent
space. This design enables efficient compression of injected knowledge and
facilitates dynamic knowledge updates. Comprehensive experiments demonstrate
that SR-KI enables the integration of up to 40K KBs into a 7B LLM on a single
A100 40GB GPU, and achieves strong retrieval performance, maintaining over 98%
Recall@10 on the best-performing task and exceeding 88% on average across all
tasks. Task performance on question answering and KB ID generation also
demonstrates that SR-KI maintains strong performance while achieving up to
99.75% compression of the injected KBs.

</details>


### [263] [Rethinking what Matters: Effective and Robust Multilingual Realignment for Low-Resource Languages](https://arxiv.org/abs/2511.06497)
*Quang Phuoc Nguyen,David Anugraha,Felix Gaschi,Jun Bin Cheng,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: 本文实证分析了多语种语言模型中重对齐（realignment）策略对跨语言迁移的影响，尤其关注低资源语言（LRLs）。发现用经过挑选的多样化语种子集，可实现甚至超越全语种重对齐的效果，且对低资源语言特别有效。


<details>
  <summary>Details</summary>
Motivation: 虽然重对齐被证明能提升跨语言迁移能力，但针对类型学差异大或低资源语言的实验效果却参差不齐，同时，现有方法依赖高质量平行语料，对低资源语言不现实。因此作者旨在探索：是否需要所有语言数据才能让重对齐有效，或者仅用少量精选语种亦能达成类似效果，进而减轻数据收集压力。

Method: 作者设计了受控实验，对比多语种重对齐与仅用精挑子集（具语言多样性）进行重对齐的表现，重点考察对低资源语言的迁移能力提升。

Result: 实验显示，精心选择的语言子集可以媲美甚至优于全部语言覆盖，尤其在未见过的低资源语言上的迁移效果突出，表明数据覆盖不必面面俱到。

Conclusion: 重对齐对低资源语言尤其有效，而且合理的子集挑选策略可显著减少数据工作量，实现更高效鲁棒的跨语言迁移，无需全面收集所有语言数据。

Abstract: Realignment is a promising strategy to improve cross-lingual transfer in
multilingual language models. However, empirical results are mixed and often
unreliable, particularly for typologically distant or low-resource languages
(LRLs) compared to English. Moreover, word realignment tools often rely on
high-quality parallel data, which can be scarce or noisy for many LRLs. In this
work, we conduct an extensive empirical study to investigate whether
realignment truly benefits from using all available languages, or if
strategically selected subsets can offer comparable or even improved
cross-lingual transfer, and study the impact on LRLs. Our controlled
experiments show that realignment can be particularly effective for LRLs and
that using carefully selected, linguistically diverse subsets can match full
multilingual alignment, and even outperform it for unseen LRLs. This indicates
that effective realignment does not require exhaustive language coverage and
can reduce data collection overhead, while remaining both efficient and robust
when guided by informed language selection.

</details>


### [264] [You Had One Job: Per-Task Quantization Using LLMs' Hidden Representations](https://arxiv.org/abs/2511.06516)
*Amit LeVi,Raz Lapid,Rom Himelstein,Yaniv Nemcovsky,Ravid Shwartz Ziv,Avi Mendelson*

Main category: cs.CL

TL;DR: 本文提出了两种面向任务的后训练量化(PTQ)方法，通过利用任务相关的隐藏表示更合理地分配量化精度，从而在保证大语言模型(LLMs)精度的前提下降低计算和存储消耗。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽然在众多任务上表现优异，但许多应用实际只用到其部分能力，完整大模型往往造成内存和延迟浪费。传统PTQ方法忽略了不同任务在模型各层的表现导致的差异，难以高效兼顾效率和性能。

Method: 作者提出了两种新的面向任务的PTQ算法：TAQ(Task-Aware Quantization)利用任务条件下的激活统计信息来分配位宽；TAQO则根据直接的层敏感度测试分配精度。两者均通过少量校准集区分对任务更重要的模型层，对这些层保留较高精度，其余层进行更激进的量化。

Result: TAQ和TAQO方法在多个模型上性能超越了现有的基线方法。具体如在Phi-4模型上，TAQ达到42.33 EM / 50.81 F1，远高于AWQ的2.25/7.07，并且在较低平均精度下模型准确率损失小于1%。TAQ在Phi-4上表现最佳，而TAQO在Llama-3.1、Qwen3和Qwen2.5等模型上表现更优。

Conclusion: 本研究证实了引入任务相关信号进行量化能显著提升大模型的效率与性能均衡，不仅实现了更精细化的量化控制，也为任务专用模型的快速高效部署提供了方法论支撑。

Abstract: Large Language Models (LLMs) excel across diverse tasks, yet many
applications require only limited capabilities, making large variants
inefficient in memory and latency. Existing approaches often combine
distillation and quantization, but most post-training quantization (PTQ)
methods are task-agnostic, ignoring how task-specific signals are distributed
across layers. In this work, we propose to use hidden representations that
encode task-salient signals as a guideline for quantization. In order to fully
utilize our innovative idea, this paper compares two new task-aware PTQ
methods: Task-Aware Quantization (TAQ), which allocates bitwidths using
task-conditioned statistics from hidden activations, and TAQO, which allocates
precision based on direct layer sensitivity tests. From a small calibration
set, these approaches identify task-relevant layers, preserving their precision
while aggressively quantizing the rest. This yields stable task sensitivity
profiles and efficient task-specialized models. Across models, TAQ and TAQO
outperform the baselines; TAQ leads on Phi-4, while TAQO leads on Llama-3.1,
Qwen3, and Qwen2.5. For instances, on Phi-4 it achieves 42.33 EM / 50.81 F1,
far surpassing Activation-aware Weight Quantization (AWQ) (2.25 / 7.07), while
remaining within < 1.0% of the original accuracy at lower average precision.

</details>


### [265] [Better Datasets Start From RefineLab: Automatic Optimization for High-Quality Dataset Refinement](https://arxiv.org/abs/2511.06530)
*Xiaonan Luo,Yue Huang,Ping He,Xiangliang Zhang*

Main category: cs.CL

TL;DR: RefineLab是一个基于大模型自动优化问答（QA）数据集的新方法，通过受控token预算实现高质量数据集构建，能定向提升覆盖率、难度分布等方面，显著缩小与专家数据集的差距。


<details>
  <summary>Details</summary>
Motivation: 现有高质量QA数据集即使由专家编写，也存在领域覆盖不全、难度分布不均和事实错误等问题。近期生成式模型辅助的数据集更加剧了质量挑战，因此需要一种能够高效、自动化提升数据集质量的方法。

Method: 提出RefineLab框架：输入原始QA数据集、目标质量指标和Token预算，允许如改写、干扰项替换等精挑细选的优化操作。由分配模块自动决定每个样本采用哪些操作，以最优方式最大化整体质量，受预算约束。

Result: 实验显示，RefineLab能在覆盖率、难度分布、事实准确性、干扰项质量等多维度显著缩小与专家数据集的差距。

Conclusion: RefineLab实现了可控、可扩展与可重现的高质量QA数据集设计，对未来大语言模型的评测具有广泛意义。

Abstract: High-quality Question-Answer (QA) datasets are foundational for reliable
Large Language Model (LLM) evaluation, yet even expert-crafted datasets exhibit
persistent gaps in domain coverage, misaligned difficulty distributions, and
factual inconsistencies. The recent surge in generative model-powered datasets
has compounded these quality challenges. In this work, we introduce RefineLab,
the first LLM-driven framework that automatically refines raw QA textual data
into high-quality datasets under a controllable token-budget constraint.
RefineLab takes a set of target quality attributes (such as coverage and
difficulty balance) as refinement objectives, and performs selective edits
within a predefined token budget to ensure practicality and efficiency. In
essence, RefineLab addresses a constrained optimization problem: improving the
quality of QA samples as much as possible while respecting resource
limitations. With a set of available refinement operations (e.g., rephrasing,
distractor replacement), RefineLab takes as input the original dataset, a
specified set of target quality dimensions, and a token budget, and determines
which refinement operations should be applied to each QA sample. This process
is guided by an assignment module that selects optimal refinement strategies to
maximize overall dataset quality while adhering to the budget constraint.
Experiments demonstrate that RefineLab consistently narrows divergence from
expert datasets across coverage, difficulty alignment, factual fidelity, and
distractor quality. RefineLab pioneers a scalable, customizable path to
reproducible dataset design, with broad implications for LLM evaluation.

</details>


### [266] [Ibom NLP: A Step Toward Inclusive Natural Language Processing for Nigeria's Minority Languages](https://arxiv.org/abs/2511.06531)
*Oluwadara Kalejaiye,Luel Hagos Beyene,David Ifeoluwa Adelani,Mmekut-Mfon Gabriel Edet,Aniefon Daniel Akpan,Eno-Abasi Urua,Anietie Andy*

Main category: cs.CL

TL;DR: 本文提出了一个名为ibom的新数据集，旨在支持尼日利亚阿夸伊博姆州四种海岸语的机器翻译和主题分类，为语言多样性提供支持，并评估了目前大模型在这些语言上的能力。


<details>
  <summary>Details</summary>
Motivation: 当前大部分NLP研究仅聚焦于尼日利亚常用的四种语言，而忽视了大量的本地小语种，部分原因是缺乏相关文本数据。这限制了数字技术和AI在本地区语言的公平发展与覆盖。

Method: 作者创建了ibom数据集，涵盖Anaang、Efik、Ibibio和Oro四种极少见的语言，并将该数据集扩展到Flores-200基准，且与SIB-200主题分类标签进行对齐。同时，基于该数据集，作者对当前主流大语言模型的机器翻译和主题分类能力进行了测试和分析。

Result: 实验发现，主流大语言模型在这些本地语言的机器翻译任务中，无论是zero-shot还是few-shot表现都很差，但在主题分类任务中，随着few-shot样本数的增加，模型表现有明显提升。

Conclusion: ibom数据集为尼日利亚少数民族语言的NLP任务提供了宝贵资源。实验显示当前大模型对低资源语言支持不足，呼吁更多资源和关注用于提升这些语言在AI领域的表现。

Abstract: Nigeria is the most populous country in Africa with a population of more than
200 million people. More than 500 languages are spoken in Nigeria and it is one
of the most linguistically diverse countries in the world. Despite this,
natural language processing (NLP) research has mostly focused on the following
four languages: Hausa, Igbo, Nigerian-Pidgin, and Yoruba (i.e <1% of the
languages spoken in Nigeria). This is in part due to the unavailability of
textual data in these languages to train and apply NLP algorithms. In this
work, we introduce ibom -- a dataset for machine translation and topic
classification in four Coastal Nigerian languages from the Akwa Ibom State
region: Anaang, Efik, Ibibio, and Oro. These languages are not represented in
Google Translate or in major benchmarks such as Flores-200 or SIB-200. We focus
on extending Flores-200 benchmark to these languages, and further align the
translated texts with topic labels based on SIB-200 classification dataset. Our
evaluation shows that current LLMs perform poorly on machine translation for
these languages in both zero-and-few shot settings. However, we find the
few-shot samples to steadily improve topic classification with more shots.

</details>


### [267] [Rep2Text: Decoding Full Text from a Single LLM Token Representation](https://arxiv.org/abs/2511.06571)
*Haiyan Zhao,Zirui He,Fan Yang,Ali Payani,Mengnan Du*

Main category: cs.CL

TL;DR: 本文提出了Rep2Text框架，能够通过LLM的最后一个token的内部表示还原出原始输入文本，验证了大语言模型内部表示存储了大量语义信息。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型（LLMs）在多种任务上取得了重大进展，但其内部机制仍不透明。本研究关注一个基础问题：能否、以及在多大程度上，仅凭最后一个token的内部表示恢复原始输入文本。

Method: 提出Rep2Text框架，它利用可训练的adapter将目标模型的最后token表示投射到解码语言模型的嵌入空间，随后通过自回归方式重建输入文本。通过不同模型组合（如Llama和Gemma系列）进行实验评估。

Result: 实验表明，长度为16的token序列，超过一半的信息可以通过最后token的内部表征还原，且文本语义连贯性较强。分析发现存在信息瓶颈效应：序列变长时，token级还原率下降，但语义完整性依然较高。此外，该方法在医疗领域等分布外数据上表现出较强的泛化能力。

Conclusion: 最后token的内部表征包含大量关于原始文本的语义信息，虽然存在信息压缩瓶颈，但Rep2Text展示了跨分布、跨领域的强大重建能力，对理解LLM的表示机制和安全性研究具有启发意义。

Abstract: Large language models (LLMs) have achieved remarkable progress across diverse
tasks, yet their internal mechanisms remain largely opaque. In this work, we
address a fundamental question: to what extent can the original input text be
recovered from a single last-token representation within an LLM? We propose
Rep2Text, a novel framework for decoding full text from last-token
representations. Rep2Text employs a trainable adapter that projects a target
model's internal representations into the embedding space of a decoding
language model, which then autoregressively reconstructs the input text.
Experiments on various model combinations (Llama-3.1-8B, Gemma-7B,
Mistral-7B-v0.1, Llama-3.2-3B) demonstrate that, on average, over half of the
information in 16-token sequences can be recovered from this compressed
representation while maintaining strong semantic integrity and coherence.
Furthermore, our analysis reveals an information bottleneck effect: longer
sequences exhibit decreased token-level recovery while preserving strong
semantic integrity. Besides, our framework also demonstrates robust
generalization to out-of-distribution medical data.

</details>


### [268] [TabRAG: Tabular Document Retrieval via Structured Language Representations](https://arxiv.org/abs/2511.06582)
*Jacob Si,Mike Qu,Michelle Lee,Yingzhen Li*

Main category: cs.CL

TL;DR: 本文提出了TabRAG，一种针对表格为主文档的RAG解析管道，通过结构化语言表示提升检索和生成性能，并超过了现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有RAG数据摄取方式主要有两种：直接在目标语料微调embedding模型（虽准确但硬件要求高），或通过解析将文档转换为embedding模型编码（但针对表格数据提取效果欠佳）。本研究旨在解决基于解析方法在表格数据处理方面表现不佳的问题。

Method: 提出TabRAG，这是一种基于解析、专门面向表格丰富文档的RAG流程，采用结构化语言表示方法，优化表格信息向量化与检索增强生成任务的处理能力。

Result: TabRAG在生成和检索任务上均优于现有的主流基于解析的方法，表现提升明显。

Conclusion: TabRAG有效提升了RAG系统对表格丰富文档的处理能力，为表格数据提供了更优的解析与生成解决方案。

Abstract: Ingesting data for Retrieval-Augmented Generation (RAG) involves either
fine-tuning the embedding model directly on the target corpus or parsing
documents for embedding model encoding. The former, while accurate, incurs high
computational hardware requirements, while the latter suffers from suboptimal
performance when extracting tabular data. In this work, we address the latter
by presenting TabRAG, a parsing-based RAG pipeline designed to tackle
table-heavy documents via structured language representations. TabRAG
outperforms existing popular parsing-based methods for generation and
retrieval. Code is available at https://github.com/jacobyhsi/TabRAG.

</details>


### [269] [MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making](https://arxiv.org/abs/2511.06592)
*Zhi Rui Tam,Yun-Nung Chen*

Main category: cs.CL

TL;DR: 该论文指出，大语言模型在临床场景下由文本转为音频交互时，会因语音属性带来严重的新偏见问题。实验发现，音频输入下的手术推荐与文本输入相比差异高达35%，且存在年龄、性别等偏见，需在临床部署前解决模型偏见。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型应用拓展到临床音频交互，作者担忧音频中额外的副语言线索可能给医疗决策带来新风险和偏见，因此评估并揭示相应的漏洞。

Method: 研究者将170个临床案例由36种不同声音特征（覆盖年龄、性别、情绪）合成语音，输入至大语言模型，比较其在音频与文本输入下的决策差异，并针对不同声音属性分析偏见表现。

Result: 音频输入下推荐手术的频率与文本输入高达35%的差异，有模型减少推荐达80%。年龄属性导致最大12%的临床建议差异，且大多模型在链式思维提示下依旧表现出年龄偏见。性别偏见可被显性推理消除，情绪因识别差未测出明显影响。

Conclusion: 音频大模型在临床应用时，易受声音特征影响做出不基于医学证据的决策，导致医疗不公，必须研发具备偏见防控能力的模型架构，方可安全应用于医疗场景。

Abstract: As large language models transition from text-based interfaces to audio
interactions in clinical settings, they might introduce new vulnerabilities
through paralinguistic cues in audio. We evaluated these models on 170 clinical
cases, each synthesized into speech from 36 distinct voice profiles spanning
variations in age, gender, and emotion. Our findings reveal a severe modality
bias: surgical recommendations for audio inputs varied by as much as 35%
compared to identical text-based inputs, with one model providing 80% fewer
recommendations. Further analysis uncovered age disparities of up to 12%
between young and elderly voices, which persisted in most models despite
chain-of-thought prompting. While explicit reasoning successfully eliminated
gender bias, the impact of emotion was not detected due to poor recognition
performance. These results demonstrate that audio LLMs are susceptible to
making clinical decisions based on a patient's voice characteristics rather
than medical evidence, a flaw that risks perpetuating healthcare disparities.
We conclude that bias-aware architectures are essential and urgently needed
before the clinical deployment of these models.

</details>


### [270] [Duality-based Mode Operations and Pyramid Multilayer Mapping for Rhetorical Modes](https://arxiv.org/abs/2511.06601)
*Zi-Niu Wu*

Main category: cs.CL

TL;DR: 本文提出了通过基于“二元对立”的操作扩展修辞模式，并量化其多样性与复杂性，旨在推动学术、教学与计算领域修辞研究的结合与发展。


<details>
  <summary>Details</summary>
Motivation: 修辞模式在学术、非学术写作以及语言学和计算建模领域均有重要作用，但目前各领域之间缺乏概念桥梁，难以互通有无与相互促进。因此，寻求一种方法能够实现多领域修辞知识的桥接与价值提升。

Method: 作者引入了基于二元对立概念的修辞操作：分合、正逆、扩缩、正交等，以及衍生的组合与泛化模式，以丰富修辞模态。此外，提出了金字塔式多层映射框架，将修辞模型层、认知层、认识论层进行分层映射，并用二项式计数与香农熵对多样性和复杂性加以量化，进一步提出了边际修辞位（MRB）和修辞可缩放参数用于刻画表达增长速度。

Result: 通过数理方法分析显示，多层次、分层选择方式相比于平铺选择显著减少了表达的不确定性，并将原本静态不可量化的分类体系转化为可动态调整且可度量的系统。

Conclusion: 该方法不仅增强了修辞表达和知识建模的多样性与可度量性，也为未来AI系统建立基于层级结构的深层修辞推理能力提供了思路，有助于语言学、教学、学术和计算研究的跨界融合。

Abstract: Rhetorical modes are useful in both academic and non-academic writing, and
can be subjects to be studied within linguistic research and computational
modeling. Establishing a conceptual bridge among these domains could enable
each to benefit from the others. This paper proposes duality-based mode
operations (split-unite, forward-backward, expansion-reduction and orthogonal
dualities) to expand the set of rhetorical modes, introducing generated modes
like combination and generalization, thereby enhancing epistemic diversity
across multiple applications. It further presents a pyramid multilayer mapping
framework (e.g., three layers from the rhetorical model layer, to cognitive
layer, and to epistemic layers) that reduces the resulting cognitive
complexity. The degrees of expressive diversity and complexity reduction are
quantified through binomial combinatorics and Shannon entropy analysis. A
Marginal Rhetorical Bit (MRB) is identified, permitting the definition of a
rhetorical-scalable parameter that measures expressive growth speed in bits per
stage. A direct entropy measure shows that hierarchical selection over smaller
subsets markedly reduces choice uncertainty compared with flat selection across
all modes. These considerations appear to transform static and non-measurable
rhetorical taxonomies into more dynamic and more measurable systems for
discourse design. From this work, it would be possible to identify a pathway
for future AI systems to operate not only on language tokens but on layered
rhetorical reasoning structures, bridging linguistic, pedagogical, academic,
and computational research

</details>


### [271] [How AI Fails: An Interactive Pedagogical Tool for Demonstrating Dialectal Bias in Automated Toxicity Models](https://arxiv.org/abs/2511.06676)
*Subhojit Ghimire*

Main category: cs.CL

TL;DR: 本论文揭示了当前主流的AI敏感内容检测模型（如toxic-bert）在处理非标准英语（AAE）时存在显著偏见，并通过统计和交互工具展示了这一现象及其危害。


<details>
  <summary>Details</summary>
Motivation: 近年来，AI驱动的内容审核应用逐渐普及，但人们日益担心这些系统存在偏见，尤其是在语言多样性下对不同群体的不公正待遇。作者希望定量检验和公开展示这些偏见，促进大众对AI偏见及其机制的认知。

Method: 作者首先使用主流毒性检测模型对比了非洲裔美国英语（AAE）和标准美式英语（SAE）的处理结果，统计二者在‘毒性’与‘身份仇恨’标签下的评分差异。同时，作者开发了一个交互式教学工具，允许用户通过调整‘敏感度阈值’直观感受模型偏见和人工策略的联合影响。

Result: 定量分析显示，AAE文本被模型平均判定为比SAE高1.8倍毒性、‘身份仇恨’标签高8.8倍。交互工具还证明，仅有模型分数偏见还不是最严重的，实际危害在于看似中立的人为策略如何加剧歧视影响。

Conclusion: 本文为AI审核模型的群体偏见提供了直接的统计证据，并通过互动工具帮助大众直观理解和质疑AI决策，为提升AI素养及公平性制定了有力基础。

Abstract: Now that AI-driven moderation has become pervasive in everyday life, we often
hear claims that "the AI is biased". While this is often said jokingly, the
light-hearted remark reflects a deeper concern. How can we be certain that an
online post flagged as "inappropriate" was not simply the victim of a biased
algorithm? This paper investigates this problem using a dual approach. First, I
conduct a quantitative benchmark of a widely used toxicity model
(unitary/toxic-bert) to measure performance disparity between text in
African-American English (AAE) and Standard American English (SAE). The
benchmark reveals a clear, systematic bias: on average, the model scores AAE
text as 1.8 times more toxic and 8.8 times higher for "identity hate". Second,
I introduce an interactive pedagogical tool that makes these abstract biases
tangible. The tool's core mechanic, a user-controlled "sensitivity threshold,"
demonstrates that the biased score itself is not the only harm; instead, the
more-concerning harm is the human-set, seemingly neutral policy that ultimately
operationalises discrimination. This work provides both statistical evidence of
disparate impact and a public-facing tool designed to foster critical AI
literacy.

</details>


### [272] [Steering LLMs toward Korean Local Speech: Iterative Refinement Framework for Faithful Dialect Translation](https://arxiv.org/abs/2511.06680)
*Keunhyeung Park,Seunguk Yu,Youngbin Kim*

Main category: cs.CL

TL;DR: 本文提出了DIA-REFINE框架，通过翻译-验证-反馈的迭代过程提升大模型标准语到方言的翻译质量，并提出了替代传统n-gram指标的新评价方法。实验显示该框架显著提升了方言转换的真实性。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在标准语到方言翻译中存在差距，且传统以n-gram为主的自动评测方法偏向机械照搬源文本，无法衡量翻译出来的方言真实有效。需要一种能引导模型更好地产生目标方言，并准确评价其质量的方法。

Method: 提出DIA-REFINE框架，包含翻译、利用外部方言分类器验证和反馈三个步骤，循环迭代优化译文。同时，设计了DFS（表达方言转换程度）和TDR（衡量目标方言特征比率）两个新指标来评价输出。

Result: 在韩语多种方言的翻译实验中，无论是zero-shot还是in-context提示，该框架都能提升方言的表达真实度。新指标能有效区分n-gram高分但缺乏方言转换的假成功情况和n-gram低分但有尝试方言转换的真实努力。

Conclusion: DIA-REFINE为标准语到方言机器翻译提供了高效的目标导向框架和评测工具，促进了大模型包容性和方言表达质量的双提升，对后续的方言翻译和评价研究有指导意义。

Abstract: Standard-to-dialect machine translation remains challenging due to a
persistent dialect gap in large language models and evaluation distortions
inherent in n-gram metrics, which favor source copying over authentic dialect
translation. In this paper, we propose the dialect refinement (DIA-REFINE)
framework, which guides LLMs toward faithful target dialect outputs through an
iterative loop of translation, verification, and feedback using external
dialect classifiers. To address the limitations of n-gram-based metrics, we
introduce the dialect fidelity score (DFS) to quantify linguistic shift and the
target dialect ratio (TDR) to measure the success of dialect translation.
Experiments on Korean dialects across zero-shot and in-context learning
baselines demonstrate that DIA-REFINE consistently enhances dialect fidelity.
The proposed metrics distinguish between False Success cases, where high n-gram
scores obscure failures in dialectal translation, and True Attempt cases, where
genuine attempts at dialectal translation yield low n-gram scores. We also
observed that models exhibit varying degrees of responsiveness to the
framework, and that integrating in-context examples further improves the
translation of dialectal expressions. Our work establishes a robust framework
for goal-directed, inclusive dialect translation, providing both rigorous
evaluation and critical insights into model performance.

</details>


### [273] [Textual Self-attention Network: Test-Time Preference Optimization through Textual Gradient-based Attention](https://arxiv.org/abs/2511.06682)
*Shibing Mo,Haoyang Ruan,Kai Wu,Jing Liu*

Main category: cs.CL

TL;DR: 该论文提出了一种新方法TSAN，实现无需参数更新即可利用多答案优势提升大语言模型输出质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在泛化能力上表现优秀，但很难通过人工反馈高效地对齐输出与人类偏好，现有测试时方法大多只能修正单个答案，无法系统性地结合多个答案的优势。

Method: 作者提出Textual Self-Attention Network（TSAN），将多个候选答案以自然语言方式格式化为Key/Value，通过LLM-based注意力机制加权分析并融合各自优势生成新的回复，整个过程无需参数调整，在可解释的“文本梯度空间”中迭代优化。

Result: 实验证明，TSAN只需三次测试时迭代，就能超越如Llama-3.1-70B-Instruct等有监督微调模型，并优于当前最佳的测试时对齐方法。

Conclusion: TSAN提供了一种高效、无须参数微调的多答案融合法，为大语言模型输出对齐人类偏好、提升输出质量提供了新思路。

Abstract: Large Language Models (LLMs) have demonstrated remarkable generalization
capabilities, but aligning their outputs with human preferences typically
requires expensive supervised fine-tuning. Recent test-time methods leverage
textual feedback to overcome this, but they often critique and revise a single
candidate response, lacking a principled mechanism to systematically analyze,
weigh, and synthesize the strengths of multiple promising candidates. Such a
mechanism is crucial because different responses may excel in distinct aspects
(e.g., clarity, factual accuracy, or tone), and combining their best elements
may produce a far superior outcome. This paper proposes the Textual
Self-Attention Network (TSAN), a new paradigm for test-time preference
optimization that requires no parameter updates. TSAN emulates self-attention
entirely in natural language to overcome this gap: it analyzes multiple
candidates by formatting them into textual keys and values, weighs their
relevance using an LLM-based attention module, and synthesizes their strengths
into a new, preference-aligned response under the guidance of the learned
textual attention. This entire process operates in a textual gradient space,
enabling iterative and interpretable optimization. Empirical evaluations
demonstrate that with just three test-time iterations on a base SFT model, TSAN
outperforms supervised models like Llama-3.1-70B-Instruct and surpasses the
current state-of-the-art test-time alignment method by effectively leveraging
multiple candidate solutions.

</details>


### [274] [Sentiment Analysis On YouTube Comments Using Machine Learning Techniques Based On Video Games Content](https://arxiv.org/abs/2511.06708)
*Adi Danish Bin Muhammad Amin,Mohaiminul Islam Bhuiyan,Nur Shazwani Kamarudin,Zulfahmi Toh,Nur Syafiqah Nafis*

Main category: cs.CL

TL;DR: 本研究利用YouTube评论，通过机器学习方法对视频游戏相关的用户情感进行分析，以揭示游戏社区的情感趋势和用户偏好。


<details>
  <summary>Details</summary>
Motivation: 随着游戏行业和社区的快速发展，理解用户在社交媒体（如YouTube）上表达的情感对游戏开发与优化非常重要。

Method: 作者利用YouTube API收集了多个视频游戏的视频评论，使用TextBlob进行初步情感分析，然后采用朴素贝叶斯、逻辑回归和支持向量机（SVM）等机器学习算法对评论进行分类。

Result: 在多种算法中，SVM在各测试数据集中都表现出最高的分类准确率。分析结果揭示了用户喜好和对游戏的批评等情感趋势。

Conclusion: 通过高级情感分析工具可以有效捕捉用户评论中细腻的情感，这对提升游戏设计与用户体验具有重要参考价值。未来研究将探索更复杂的自然语言处理技术和更多数据源来进一步提升分析效果。

Abstract: The rapid evolution of the gaming industry, driven by technological
advancements and a burgeoning community, necessitates a deeper understanding of
user sentiments, especially as expressed on popular social media platforms like
YouTube. This study presents a sentiment analysis on video games based on
YouTube comments, aiming to understand user sentiments within the gaming
community. Utilizing YouTube API, comments related to various video games were
collected and analyzed using the TextBlob sentiment analysis tool. The
pre-processed data underwent classification using machine learning algorithms,
including Na\"ive Bayes, Logistic Regression, and Support Vector Machine (SVM).
Among these, SVM demonstrated superior performance, achieving the highest
classification accuracy across different datasets. The analysis spanned
multiple popular gaming videos, revealing trends and insights into user
preferences and critiques. The findings underscore the importance of advanced
sentiment analysis in capturing the nuanced emotions expressed in user
comments, providing valuable feedback for game developers to enhance game
design and user experience. Future research will focus on integrating more
sophisticated natural language processing techniques and exploring additional
data sources to further refine sentiment analysis in the gaming domain.

</details>


### [275] [Rethinking Retrieval-Augmented Generation for Medicine: A Large-Scale, Systematic Expert Evaluation and Practical Insights](https://arxiv.org/abs/2511.06738)
*Hyunjae Kim,Jiwoong Sohn,Aidan Gilson,Nicholas Cochran-Caggiano,Serina Applebaum,Heeju Jin,Seihee Park,Yujin Park,Jiyeong Park,Seoyoung Choi,Brittany Alexandra Herrera Contreras,Thomas Huang,Jaehoon Yun,Ethan F. Wei,Roy Jiang,Leah Colucci,Eric Lai,Amisha Dave,Tuo Guo,Maxwell B. Singer,Yonghoe Koo,Ron A. Adelman,James Zou,Andrew Taylor,Arman Cohan,Hua Xu,Qingyu Chen*

Main category: cs.CL

TL;DR: 本文对医学领域的RAG（检索增强生成）方法进行了深入专家评估，发现标准RAG在多个方面表现不佳，提出改进措施可显著提升效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在医学应用中面临知识快速更新与推理可信度不足的问题，RAG被广泛视作补救手段，但其实效性尚未被系统验证。

Method: 组织十八位医学专家，对GPT-4o和Llama-3.1-8B模型在200个真实医疗及USMLE风格问题下生成的800个回答进行细致标注，拆解RAG为检索、证据选取和响应生成三个环节，逐项评估表现。

Result: 标准RAG性能不及预期：检索相关性仅22%，证据选取精度41-43%、召回率27-49%，而事实性和完整性较无RAG方案下降。改用证据过滤和查询重述后，在MedMCQA和MedXpertQA提升最高达12%和8.2%。

Conclusion: 目前RAG的检索和证据选取环节是主要瓶颈，需重新审视其在医学中的作用，并强调分阶段评估和系统性设计以保障医疗大模型的可靠性。

Abstract: Large language models (LLMs) are transforming the landscape of medicine, yet
two fundamental challenges persist: keeping up with rapidly evolving medical
knowledge and providing verifiable, evidence-grounded reasoning.
Retrieval-augmented generation (RAG) has been widely adopted to address these
limitations by supplementing model outputs with retrieved evidence. However,
whether RAG reliably achieves these goals remains unclear. Here, we present the
most comprehensive expert evaluation of RAG in medicine to date. Eighteen
medical experts contributed a total of 80,502 annotations, assessing 800 model
outputs generated by GPT-4o and Llama-3.1-8B across 200 real-world patient and
USMLE-style queries. We systematically decomposed the RAG pipeline into three
components: (i) evidence retrieval (relevance of retrieved passages), (ii)
evidence selection (accuracy of evidence usage), and (iii) response generation
(factuality and completeness of outputs). Contrary to expectation, standard RAG
often degraded performance: only 22% of top-16 passages were relevant, evidence
selection remained weak (precision 41-43%, recall 27-49%), and factuality and
completeness dropped by up to 6% and 5%, respectively, compared with non-RAG
variants. Retrieval and evidence selection remain key failure points for the
model, contributing to the overall performance drop. We further show that
simple yet effective strategies, including evidence filtering and query
reformulation, substantially mitigate these issues, improving performance on
MedMCQA and MedXpertQA by up to 12% and 8.2%, respectively. These findings call
for re-examining RAG's role in medicine and highlight the importance of
stage-aware evaluation and deliberate system design for reliable medical LLM
applications.

</details>


### [276] [Sensitivity of Small Language Models to Fine-tuning Data Contamination](https://arxiv.org/abs/2511.06763)
*Nicy Scaria,Silvester John Joseph Kennedy,Deepak Subramani*

Main category: cs.CL

TL;DR: 本论文系统地分析了小型语言模型（SLMs）在指令微调过程中对数据污染的脆弱性，特别是受到句法和语义污染影响时的表现差异及其机制。


<details>
  <summary>Details</summary>
Motivation: 随着SLM在资源受限场景中的应用增加，模型对指令微调过程中的数据污染的行为鲁棒性尚不明确，因此需要系统化地评估SLM对不同类型污染的敏感性和脆弱性。

Method: 作者研究了23个不同大小（270M-4B参数）、不同家族的SLM，对它们在指令微调阶段接受不同水平（25%、50%、75%、100%）的句法（字符、单词反转）和语义（无关、反事实响应）污染时的表现。并且对比了基础模型和指令微调变体的鲁棒性。

Result: （1）句法污染（尤其是字符反转）会带来灾难性性能退化，几乎所有模型均失效，且与模型大小无关；（2）语义污染导致的损伤表现出阈值特性，且对核心语言能力的影响较小，较大型号反而更容易学习有害指令；（3）对齐训练带来的鲁棒性提升不一致，有时甚至适得其反。

Conclusion: 小型语言模型对句法型污染极度脆弱，而对语义污染展现一定韧性。现有鲁棒性假设不适合SLM模型，需制定污染感知的训练协议以指导安全部署。

Abstract: Small Language Models (SLMs) are increasingly being deployed in
resource-constrained environments, yet their behavioral robustness to data
contamination during instruction tuning remains poorly understood. We
systematically investigate the contamination sensitivity of 23 SLMs (270M to 4B
parameters) across multiple model families by measuring susceptibility to
syntactic and semantic transformation types during instruction tuning:
syntactic transformations (character and word reversal) and semantic
transformations (irrelevant and counterfactual responses), each applied at
contamination levels of 25\%, 50\%, 75\%, and 100\%. Our results reveal
fundamental asymmetries in vulnerability patterns: syntactic transformations
cause catastrophic performance degradation, with character reversal producing
near-complete failure across all models regardless of size or family, while
semantic transformations demonstrate distinct threshold behaviors and greater
resilience in core linguistic capabilities. Critically, we discover a
``\textit{capability curse}" where larger, more capable models become more
susceptible to learning semantic corruptions, effectively following harmful
instructions more readily, while our analysis of base versus instruction-tuned
variants reveals that alignment provides inconsistent robustness benefits,
sometimes even reducing resilience. Our work establishes three core
contributions: (1) empirical evidence of SLMs' disproportionate vulnerability
to syntactic pattern contamination, (2) identification of asymmetric
sensitivity patterns between syntactic and semantic transformations, and (3)
systematic evaluation protocols for contamination robustness assessment. These
findings have immediate deployment implications, suggesting that current
robustness assumptions may not hold for smaller models and highlighting the
need for contamination-aware training protocols.

</details>


### [277] [SAFENLIDB: A Privacy-Preserving Safety Alignment Framework for LLM-based Natural Language Database Interfaces](https://arxiv.org/abs/2511.06778)
*Ruiheng Liu,XiaoBing Chen,Jinyu Zhang,Qiongwen Zhang,Yu Zhang,Bailong Yang*

Main category: cs.CL

TL;DR: 论文提出了SafeNlidb框架，通过自动化生成混合链式推理数据和优化预训练，使LLM在自然语言转SQL过程中兼顾隐私和安全，显著提升了数据泄漏防护能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自然语言数据库接口（NLIDB）在实际交互中可能无意间泄露敏感信息，且传统防护手段面对复杂推理攻击表现不佳，误报率高且影响查询结果可靠性。因此急需一种安全性与可靠性兼顾的新方法。

Method: SafeNlidb框架通过自动化管道，从零生成融合安全推理与SQL生成的混合链式推理训练数据。同时，引入推理预热和交替偏好优化机制，解决直接偏好优化（DPO）过程中的多偏好震荡问题，无需人工打标偏好数据，提升了LLM安全性认知能力。

Result: 实验表明，SafeNlidb在安全性和实用性上均优于大规模LLM和理想基线，在提升防攻击能力的同时基本保持SQL查询实用性。

Conclusion: SafeNlidb为LLM驱动的NLIDB提供了更高层次的隐私与安全保障，同时兼顾了查询的可用性。该方法推动了安全可控的NLIDB系统落地和发展。

Abstract: The rapid advancement of Large Language Models (LLMs) has driven significant
progress in Natural Language Interface to Database (NLIDB). However, the
widespread adoption of LLMs has raised critical privacy and security concerns.
During interactions, LLMs may unintentionally expose confidential database
contents or be manipulated by attackers to exfiltrate data through seemingly
benign queries. While current efforts typically rely on rule-based heuristics
or LLM agents to mitigate this leakage risk, these methods still struggle with
complex inference-based attacks, suffer from high false positive rates, and
often compromise the reliability of SQL queries. To address these challenges,
we propose \textsc{SafeNlidb}, a novel privacy-security alignment framework for
LLM-based NLIDB. The framework features an automated pipeline that generates
hybrid chain-of-thought interaction data from scratch, seamlessly combining
implicit security reasoning with SQL generation. Additionally, we introduce
reasoning warm-up and alternating preference optimization to overcome the
multi-preference oscillations of Direct Preference Optimization (DPO), enabling
LLMs to produce security-aware SQL through fine-grained reasoning without the
need for human-annotated preference data. Extensive experiments demonstrate
that our method outperforms both larger-scale LLMs and ideal-setting baselines,
achieving significant security improvements while preserving high
utility.WARNING: This work may contain content that is offensive and harmful!

</details>


### [278] [Learning to Focus: Focal Attention for Selective and Scalable Transformers](https://arxiv.org/abs/2511.06818)
*Dhananjay Ram,Wei Xia,Stefano Soatto*

Main category: cs.CL

TL;DR: 本文提出了一种改进transformer注意力机制的方法，称为Focal Attention，通过调节softmax温度参数实现更精准的注意力分布，显著提升模型效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统的softmax注意力机制容易产生分布噪声，影响层间特征选择特别是在处理长文本时的问题。因此需要一种更能聚焦于有效信息的注意力机制。

Method: 引入Focal Attention机制，通过将softmax的温度设为固定超参数或可学习参数，使得注意力分布更加集中于重要token，抑制无关信息。

Result: Focal Attention在不同基准测试中，与标准transformer相比，用更少参数（最多减少42%）或更少训练数据（最多减少33%）达到相同精度；在长文本任务中，性能提升17%-82%。

Conclusion: Focal Attention简单有效，特别适合长上下文任务，具有显著的实际应用前景。

Abstract: Attention is a core component of transformer architecture, whether
encoder-only, decoder-only, or encoder-decoder model. However, the standard
softmax attention often produces noisy probability distribution, which can
impair effective feature selection at every layer of these models, particularly
for long contexts. We propose Focal Attention, a simple yet effective
modification that sharpens the attention distribution by controlling the
softmax temperature, either as a fixed hyperparameter or as a learnable
parameter during training. This sharpening enables the model to concentrate on
the most relevant tokens while suppressing irrelevant ones. Empirically, Focal
Attention scales more favorably than standard transformer with respect to model
size, training data, and context length. Across diverse benchmarks, it achieves
the same accuracy with up to 42% fewer parameters or 33% less training data. On
long-context tasks, it delivers substantial relative improvements ranging from
17% to 82%, demonstrating its effectiveness in real world applications.

</details>


### [279] [Beyond Plain Demos: A Demo-centric Anchoring Paradigm for In-Context Learning in Alzheimer's Disease Detection](https://arxiv.org/abs/2511.06826)
*Puzhen Su,Haoran Yin,Yongzhu Miao,Jintao Tang,Shasha Li,Ting Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为DA4ICL的新方法，以增强大语言模型对阿尔茨海默病（AD）识别任务中的感知能力。DA4ICL通过增加示例多样性和信号深度，显著提升了模型在涉及叙述转录本的AD检测任务中的表现，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前，大语言模型（LLMs）在阿尔茨海默病检测任务中表现不佳，主要原因是模型缺乏针对该任务的预训练资料，且示例具有高度同质性，导致模型难以捕捉细粒度、区分类信号。因此，提升模型的“上下文感知”能力成为关键。

Method: 作者分析了当前基于in-context learning（ICL）的方法和Task Vector（TV）方法的局限，提出DA4ICL框架：利用Diverse and Contrastive Retrieval（DCR）扩展示例多样性（宽度），用Projected Vector Anchoring（PVA）在Transformer每一层深化示例信号（深度），实现上下文感知的提升。

Result: 在3个AD基准测试上，DA4ICL比ICL和TV基线方法取得了显著且稳定的提升效果，表明该方法在细粒度、低资源和分布外（OOD）任务适应性上具有优势。

Conclusion: DA4ICL为面向细粒度、低资源领域任务的大模型适应提供了新范式，特别适合阿尔茨海默病检测这类需求高细致信号识别的情境，优于现有主流方法。

Abstract: Detecting Alzheimer's disease (AD) from narrative transcripts challenges
large language models (LLMs): pre-training rarely covers this
out-of-distribution task, and all transcript demos describe the same scene,
producing highly homogeneous contexts. These factors cripple both the model's
built-in task knowledge (\textbf{task cognition}) and its ability to surface
subtle, class-discriminative cues (\textbf{contextual perception}). Because
cognition is fixed after pre-training, improving in-context learning (ICL) for
AD detection hinges on enriching perception through better demonstration (demo)
sets. We demonstrate that standard ICL quickly saturates, its demos lack
diversity (context width) and fail to convey fine-grained signals (context
depth), and that recent task vector (TV) approaches improve broad task
adaptation by injecting TV into the LLMs' hidden states (HSs), they are
ill-suited for AD detection due to the mismatch of injection granularity,
strength and position. To address these bottlenecks, we introduce
\textbf{DA4ICL}, a demo-centric anchoring framework that jointly expands
context width via \emph{\textbf{Diverse and Contrastive Retrieval}} (DCR) and
deepens each demo's signal via \emph{\textbf{Projected Vector Anchoring}} (PVA)
at every Transformer layer. Across three AD benchmarks, DA4ICL achieves large,
stable gains over both ICL and TV baselines, charting a new paradigm for
fine-grained, OOD and low-resource LLM adaptation.

</details>


### [280] [CLiFT-ASR: A Cross-Lingual Fine-Tuning Framework for Low-Resource Taiwanese Hokkien Speech Recognition](https://arxiv.org/abs/2511.06860)
*Hung-Yang Sung,Chien-Chun Wang,Kuan-Tang Huang,Tien-Hong Lo,Yu-Sheng Tsao,Yung-Chang Hsu,Berlin Chen*

Main category: cs.CL

TL;DR: 本文提出了一种针对低资源语种（如台语）的自动语音识别（ASR）框架CLiFT-ASR，实现了相较于现有方法24.88%的字符错误率相对下降。


<details>
  <summary>Details</summary>
Motivation: 低资源语言（如台语）由于标注数据稀缺，ASR模型难以获得优良表现，且现有方法难以兼顾声韵和词汇/语法信息，也缺乏结合不同标注类型的策略。

Method: 提出了一种跨语言微调框架CLiFT-ASR，首先在普通话HuBERT模型基础上，采用两阶段微调：第一阶段利用台语罗马拼音（Tai-lo）标注学习音韵、声调特征，第二阶段利用汉字标注捕捉词汇和句法信息，从而实现语音与书写结构的高效对齐。

Result: 在TAT-MOE语料库测试中，CLiFT-ASR模型相较强基线模型，字符错误率（CER）降低了24.88%。

Conclusion: CLiFT-ASR为台语等低资源语音识别提供了一种有效且参数高效的方案，并显示出在其他低资源语种场景中的应用潜力。

Abstract: Automatic speech recognition (ASR) for low-resource languages such as
Taiwanese Hokkien is difficult due to the scarcity of annotated data. However,
direct fine-tuning on Han-character transcriptions often fails to capture
detailed phonetic and tonal cues, while training only on romanization lacks
lexical and syntactic coverage. In addition, prior studies have rarely explored
staged strategies that integrate both annotation types. To address this gap, we
present CLiFT-ASR, a cross-lingual fine-tuning framework that builds on
Mandarin HuBERT models and progressively adapts them to Taiwanese Hokkien. The
framework employs a two-stage process in which it first learns acoustic and
tonal representations from phonetic Tai-lo annotations and then captures
vocabulary and syntax from Han-character transcriptions. This progressive
adaptation enables effective alignment between speech sounds and orthographic
structures. Experiments on the TAT-MOE corpus demonstrate that CLiFT-ASR
achieves a 24.88\% relative reduction in character error rate (CER) compared
with strong baselines. The results indicate that CLiFT-ASR provides an
effective and parameter-efficient solution for Taiwanese Hokkien ASR and that
it has potential to benefit other low-resource language scenarios.

</details>


### [281] [Inclusion of Role into Named Entity Recognition and Ranking](https://arxiv.org/abs/2511.06886)
*Neelesh Kumar Shukla,Sanasam Ranbir Singh*

Main category: cs.CL

TL;DR: 本文探讨了在自然语言处理中对实体角色进行自动检测的方法，将该任务建模为命名实体识别（NER）和实体检索/排序问题，并提出了利用自动学习代表性词语和短语来构建角色及实体表征的方法。


<details>
  <summary>Details</summary>
Motivation: 许多NLP任务（如信息抽取、问答、摘要等）需识别实体及其在不同上下文中的角色，但角色常依赖于领域且难以明确定义和标注，如何从有限数据和无领域限制下实现高效、准确的角色检测成为一个挑战。

Method: 作者将实体角色检测任务分别转化为NER和实体检索问题：在NER中，视角色为互斥类别，采用序列标注方法；在实体检索中，将角色视为查询，将角色相关实体视为集合，针对间接定义的实体和角色，自动学习能代表角色/实体的词与短语，并在句子与文档层面构建上下文表征。

Result: 提出了一种无需大规模领域特定数据，能在较小、无领域限制语料中自动检测实体角色的方法。探索了多种上下文粒度对方法效果的影响，并展示自动构建角色与实体表征的可行性。

Conclusion: 实体角色检测任务可通过NER与实体检索思路建模，借助自动提取词语/短语构建表征的方法具有领域无关性和适应小数据集的潜力，为无需大规模知识库的角色检测任务提供了有效解决思路。

Abstract: Most of the Natural Language Processing sys- tems are involved in
entity-based processing for several tasks like Information Extraction,
Question-Answering, Text-Summarization and so on. A new challenge comes when
entities play roles according to their act or attributes in certain context.
Entity Role Detection is the task of assigning such roles to the entities. Usu-
ally real-world entities are of types: person, lo- cation and organization etc.
Roles could be con- sidered as domain-dependent subtypes of these types. In the
cases, where retrieving a subset of entities based on their roles is needed,
poses the problem of defining the role and entities having those roles. This
paper presents the study of study of solving Entity Role Detection prob- lem by
modeling it as Named Entity Recogni- tion (NER) and Entity Retrieval/Ranking
task. In NER, these roles could be considered as mutually exclusive classes and
standard NER methods like sequence tagging could be used. For Entity Retrieval,
Roles could be formulated as Query and entities as Collection on which the
query needs to be executed. The aspect of Entity Retrieval task, which is
different than document retrieval task is that the entities and roles against
which they need to be retrieved are indirectly described. We have formulated
au- tomated ways of learning representative words and phrases and building
representations of roles and entities using them. We have also explored
different contexts like sentence and document. Since the roles depend upon con-
text, so it is not always possible to have large domain-specific dataset or
knowledge bases for learning purposes, so we have tried to exploit the
information from small dataset in domain- agnostic way.

</details>


### [282] [EduGuardBench: A Holistic Benchmark for Evaluating the Pedagogical Fidelity and Adversarial Safety of LLMs as Simulated Teachers](https://arxiv.org/abs/2511.06890)
*Yilin Jiang,Mingzi Zhang,Xuanyu Yin,Sheng Jin,Suyu Lu,Zuocan Ying,Zengyi Yu,Xiangjie Kong*

Main category: cs.CL

TL;DR: 本文提出了一套针对大型语言模型（LLM）扮演教师角色时的专业能力和伦理安全进行评估的新基准EduGuardBench，解决了现有基准无法有效评估角色扮演真实性和教育特有危害的问题。


<details>
  <summary>Details</summary>
Motivation: LLM在教育领域的应用潜力巨大，尤其是在个性化教学中担当教师角色，但现有方法无法衡量模型是否真正胜任、是否具备责任感及伦理约束。需要系统性基准以确保AI教师既专业又安全。

Method: 设计了双组件基准：一是通过角色扮演真实性得分（RFS）评价模型专业能力，并诊断教学特有的危害；二是利用基于角色的人格化对抗攻击（adversarial prompts）检测安全漏洞，重点考察学术不端相关风险，并结合攻击成功率（ASR）和三级拒绝质量评价模型的安全应对能力。

Result: 对14个主流模型的实验证明：推理能力强的模型专业表现更佳，但整体上"不胜任"是主流失败类型。安全对抗测试发现中等规模模型反而最易受攻击，颠覆了“越大越安全”的假设。最安全的模型能够将有害请求转化为教学时刻（即理想教育性拒绝），与ASR高度负相关。

Conclusion: EduGuardBench为教育AI专业性、伦理和教学适应性提供了系统、可复现的评估框架，有助于识别和优化AI教师的复杂动态行为，对可信教育AI的部署具重要意义。

Abstract: Large Language Models for Simulating Professions (SP-LLMs), particularly as
teachers, are pivotal for personalized education. However, ensuring their
professional competence and ethical safety is a critical challenge, as existing
benchmarks fail to measure role-playing fidelity or address the unique teaching
harms inherent in educational scenarios. To address this, we propose
EduGuardBench, a dual-component benchmark. It assesses professional fidelity
using a Role-playing Fidelity Score (RFS) while diagnosing harms specific to
the teaching profession. It also probes safety vulnerabilities using
persona-based adversarial prompts targeting both general harms and,
particularly, academic misconduct, evaluated with metrics including Attack
Success Rate (ASR) and a three-tier Refusal Quality assessment. Our extensive
experiments on 14 leading models reveal a stark polarization in performance.
While reasoning-oriented models generally show superior fidelity, incompetence
remains the dominant failure mode across most models. The adversarial tests
uncovered a counterintuitive scaling paradox, where mid-sized models can be the
most vulnerable, challenging monotonic safety assumptions. Critically, we
identified a powerful Educational Transformation Effect: the safest models
excel at converting harmful requests into teachable moments by providing ideal
Educational Refusals. This capacity is strongly negatively correlated with ASR,
revealing a new dimension of advanced AI safety. EduGuardBench thus provides a
reproducible framework that moves beyond siloed knowledge tests toward a
holistic assessment of professional, ethical, and pedagogical alignment,
uncovering complex dynamics essential for deploying trustworthy AI in
education. See https://github.com/YL1N/EduGuardBench for Materials.

</details>


### [283] [RPTS: Tree-Structured Reasoning Process Scoring for Faithful Multimodal Evaluation](https://arxiv.org/abs/2511.06899)
*Haofeng Wang,Yu Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种新指标RPTS，用于更细致地评估大规模视觉语言模型（LVLMs）的多模态推理能力，并发布了一个配套基准RPTS-Eval，用于验证LVLMs推理过程中的优劣。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推理基准往往只以选择题或简答题形式评估输出，忽略了模型推理过程本身，且对推理过程的考察较为粗糙，无法有效捕捉到推理过程中的具体失误与跨模态关系的影响。

Method: 提出Reasoning Process Tree Score（RPTS），将推理步骤结构化为推理树，并分层赋予权重，根据推理过程的层次性给予不同的信任分数，动态调整以细粒度反映推理的正确性和失误点。同时，构建RPTS-Eval新基准，包含374张图片和390个推理实例，并定义三类跨模态关系辅助分析推理过程。

Result: 在RPTS-Eval上对代表性LVLMs（如GPT4o、Llava-Next）进行评测，揭示了其在多模态推理上的不足，并比较了开源与闭源模型在推理表现上的差异。

Conclusion: RPTS和RPTS-Eval为多模态推理模型的细致评估提供了新工具，有助于推动多模态推理领域的研究发展。

Abstract: Large Vision-Language Models (LVLMs) excel in multimodal reasoning and have
shown impressive performance on various multimodal benchmarks. However, most of
these benchmarks evaluate models primarily through multiple-choice or
short-answer formats, which do not take the reasoning process into account.
Although some benchmarks assess the reasoning process, their methods are often
overly simplistic and only examine reasoning when answers are incorrect. This
approach overlooks scenarios where flawed reasoning leads to correct answers.
In addition, these benchmarks do not consider the impact of intermodal
relationships on reasoning. To address this issue, we propose the Reasoning
Process Tree Score (RPTS), a tree structure-based metric to assess reasoning
processes. Specifically, we organize the reasoning steps into a reasoning tree
and leverage its hierarchical information to assign weighted faithfulness
scores to each reasoning step. By dynamically adjusting these weights, RPTS not
only evaluates the overall correctness of the reasoning, but also pinpoints
where the model fails in the reasoning. To validate RPTS in real-world
multimodal scenarios, we construct a new benchmark, RPTS-Eval, comprising 374
images and 390 reasoning instances. Each instance includes reliable
visual-textual clues that serve as leaf nodes of the reasoning tree.
Furthermore, we define three types of intermodal relationships to investigate
how intermodal interactions influence the reasoning process. We evaluated
representative LVLMs (e.g., GPT4o, Llava-Next), uncovering their limitations in
multimodal reasoning and highlighting the differences between open-source and
closed-source commercial LVLMs. We believe that this benchmark will contribute
to the advancement of research in the field of multimodal reasoning.

</details>


### [284] [HLPD: Aligning LLMs to Human Language Preference for Machine-Revised Text Detection](https://arxiv.org/abs/2511.06942)
*Fangqi Dai,Xingjian Jiang,Zizhuang Deng*

Main category: cs.CL

TL;DR: 本论文提出了一种全新的检测方法HLPD，能够更有效地识别由大型语言模型（LLM）生成或修改的文本，有效提升了在黑盒和对抗场景中的检测能力。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成内容的真实性增强，假信息的制造和传播带来了社会风险。现有检测方法虽然能够识别完全由LLM生成的文本，但在遇到模型未知或经过机器多任务修改的文本时效果变差。为应对此挑战，需要开发更加可靠且适应性强的检测手段。

Method: 作者提出了基于人类写作风格的特征，设计了一种“人类语言偏好检测（HLPD）”方法。其核心机制是通过Human Language Preference Optimization（HLPO）对检测模型进行奖励式训练，使模型更敏感于人类写作风格，进而提升检测机器改写文本的能力，并在多任务对抗性评测框架下对方法有效性进行评估。

Result: HLPD方法在检测GPT家族模型改写文本时，AUROC比主流ImBD方法提升15.11%，比Fast-DetectGPT提升45.56%；在更高级别LLM生成文本检测中，HLPD的平均AUROC高于ImBD 5.53%、高于Fast-DetectGPT 34.14%，表现突出。

Conclusion: HLPD显著提升了识别由LLM生成或改写文本的能力，尤其适用于对抗性和黑盒场景。该方法为解决AI生成内容带来的信任危机提供了有力技术支撑，具有重要理论与应用价值，相关代码已开源。

Abstract: To prevent misinformation and social issues arising from trustworthy-looking
content generated by LLMs, it is crucial to develop efficient and reliable
methods for identifying the source of texts. Previous approaches have
demonstrated exceptional performance in detecting texts fully generated by
LLMs. However, these methods struggle when confronting more advanced LLM output
or text with adversarial multi-task machine revision, especially in the
black-box setting, where the generating model is unknown. To address this
challenge, grounded in the hypothesis that human writing possesses distinctive
stylistic patterns, we propose Human Language Preference Detection (HLPD). HLPD
employs a reward-based alignment process, Human Language Preference
Optimization (HLPO), to shift the scoring model's token distribution toward
human-like writing, making the model more sensitive to human writing, therefore
enhancing the identification of machine-revised text. We test HLPD in an
adversarial multi-task evaluation framework that leverages a five-dimensional
prompt generator and multiple advanced LLMs to create diverse revision
scenarios. When detecting texts revised by GPT-series models, HLPD achieves a
15.11% relative improvement in AUROC over ImBD, surpassing Fast-DetectGPT by
45.56%. When evaluated on texts generated by advanced LLMs, HLPD achieves the
highest average AUROC, exceeding ImBD by 5.53% and Fast-DetectGPT by 34.14%.
Code will be made available at https://github.com/dfq2021/HLPD.

</details>


### [285] [SCOPE: Intrinsic Semantic Space Control for Mitigating Copyright Infringement in LLMs](https://arxiv.org/abs/2511.07001)
*Zhenliang Zhang,Xinyu Hu,Xiaojun Wan*

Main category: cs.CL

TL;DR: 该论文提出了一种消除大语言模型在推理时版权侵权风险的新方法SCOPE，通过在语义空间内限制特定子空间的激活，实现了不影响模型性能前提下减少版权内容泄露。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型容易在生成内容时意外泄露受版权保护的文本，现有方法往往依赖词级检测、黑名单或外部过滤器，不仅增加部署复杂性，还难以处理语义层面的变体。因此，需要一种更本质且高效的防御方式。

Method: 作者提出将版权内容防控视为语义空间内的控制任务，并设计了SCOPE方法：利用稀疏自动编码器（SAE）将隐藏状态投影到高维、近乎单一语义的空间，识别与版权相关的子空间后在解码时“钳制”该子空间的激活，以降低版权内容泄漏风险，无需更新参数或引入辅助过滤器。

Result: 在主流基准测试上，SCOPE方法能够有效减少大语言模型输出中的版权内容，同时保持一般任务性能不降低。进一步解释性分析也验证了所隔离的语义子空间确实捕捉到了高级语义信息。

Conclusion: SCOPE提供了一种无需更改模型参数、无需额外过滤，仅通过调整语义空间子空间激活即可缓解版权泄露风险的新方法，既有效又实用，有望推动语言模型安全部署。

Abstract: Large language models sometimes inadvertently reproduce passages that are
copyrighted, exposing downstream applications to legal risk. Most existing
studies for inference-time defences focus on surface-level token matching and
rely on external blocklists or filters, which add deployment complexity and may
overlook semantically paraphrased leakage. In this work, we reframe copyright
infringement mitigation as intrinsic semantic-space control and introduce
SCOPE, an inference-time method that requires no parameter updates or auxiliary
filters. Specifically, the sparse autoencoder (SAE) projects hidden states into
a high-dimensional, near-monosemantic space; benefiting from this
representation, we identify a copyright-sensitive subspace and clamp its
activations during decoding. Experiments on widely recognized benchmarks show
that SCOPE mitigates copyright infringement without degrading general utility.
Further interpretability analyses confirm that the isolated subspace captures
high-level semantics.

</details>


### [286] [Automated Circuit Interpretation via Probe Prompting](https://arxiv.org/abs/2511.07002)
*Giuseppe Birardi*

Main category: cs.CL

TL;DR: 本文提出了一种自动化的归因图分析方法（probe prompting），显著提升了神经网络可解释性的效率与质量。该方法将复杂的归因图压缩为更具语义且易于解释的子图，有效保持了对模型行为的解释力。


<details>
  <summary>Details</summary>
Motivation: 传统的归因图分析需要大量人工操作，效率低且耗时（有经验的分析者分析一个prompt需2小时），限制了机械可解释性的实际应用。研究动机是提升分析自动化程度与可用性，降低人力成本。

Method: 作者提出‘probe prompting’自动流程：从种子prompt和目标logit出发，选取高影响特征，生成概念相关但语境变化的探针，将特征按在不同prompt下的激活模式（语义型、关系型、Say-X型）进行分组，采用透明的决策规则自动化处理。

Result: 在5组经典实验（如“capitals”回路）上，probe-prompted子图在大幅简化复杂度的同时保留了高解释覆盖率（Completeness 0.83，Replacement 0.54）。与传统几何聚类相比，所提出方法的概念一致性显著提升（token consistency 2.3倍提升，activation similarity 5.8倍提升），并且揭示了特征在Transformer中分层迁移与专业化的现象。

Conclusion: probe prompting方法高效、自动化地提升了神经网络归因可解释性，并以实验证明其有效性和优势。作者开源代码和交互式演示，促进社区复现与应用，有望成为机制可解释性研究的重要工具。

Abstract: Mechanistic interpretability aims to understand neural networks by
identifying which learned features mediate specific behaviors. Attribution
graphs reveal these feature pathways, but interpreting them requires extensive
manual analysis -- a single prompt can take approximately 2 hours for an
experienced circuit tracer. We present probe prompting, an automated pipeline
that transforms attribution graphs into compact, interpretable subgraphs built
from concept-aligned supernodes. Starting from a seed prompt and target logit,
we select high-influence features, generate concept-targeted yet
context-varying probes, and group features by cross-prompt activation
signatures into Semantic, Relationship, and Say-X categories using transparent
decision rules.
  Across five prompts including classic "capitals" circuits, probe-prompted
subgraphs preserve high explanatory coverage while compressing complexity
(Completeness 0.83, mean across circuits; Replacement 0.54). Compared to
geometric clustering baselines, concept-aligned groups exhibit higher
behavioral coherence: 2.3x higher peak-token consistency (0.425 vs 0.183) and
5.8x higher activation-pattern similarity (0.762 vs 0.130), despite lower
geometric compactness. Entity-swap tests reveal a layerwise hierarchy:
early-layer features transfer robustly (64% transfer rate, mean layer 6.3),
while late-layer Say-X features specialize for output promotion (mean layer
16.4), supporting a backbone-and-specialization view of transformer
computation.
  We release code (https://github.com/peppinob-ol/attribution-graph-probing),
an interactive demo
(https://huggingface.co/spaces/Peppinob/attribution-graph-probing), and minimal
artifacts enabling immediate reproduction and community adoption.

</details>


### [287] [Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs](https://arxiv.org/abs/2511.07003)
*Yingfeng Luo,Ziqiang Xu,Yuxuan Ouyang,Murun Yang,Dingyang Lin,Kaiyan Chang,Tong Zheng,Bei Li,Peinan Feng,Quan Du,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 提出了LMT多语言大模型，创新性地解决了现有多语言翻译中的方向退化、语言覆盖不均和英中中心化等问题，方法简单有效，且在同等体量模型下大幅超越已有SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有的多语言大模型翻译存在语言覆盖不足、翻译质量不均和过度依赖英文的局限，还存在被忽视的“方向退化”问题（即多向微调数据导致反向译文表现退化），亟需改进以提升真正多语言、多方向的翻译表现。

Method: 1. 构建LMT，多语言大规模翻译框架，支持60种语言和234个方向；2. 提出“战略性降采样”，通过调整数据采样比例，减少多对一映射导致的方向退化；3. 设计“并行多语提示（PMP）”，利用相近语系的辅助语言提示增强迁移能力；4. 精细筛选和适配数据，提升模型泛化。

Result: LMT在同语言覆盖范围比对下，显著优于Aya-101-13B和NLLB-54B等大模型。4B参数量的LMT-60-4B在多语言多方向翻译上均取得SOTA，比体量大得多的模型还要优秀。

Conclusion: LMT为大规模多语言翻译提供了更优的方案，突破了语言覆盖、方向退化、多语迁移等多个难题，将成为未来多语言翻译研究和应用的强大基线。公开多种型号，便于社区进一步探索。

Abstract: Large language models have significantly advanced Multilingual Machine
Translation (MMT), yet the broad language coverage, consistent translation
quality, and English-centric bias remain open challenges. To address these
challenges, we introduce \textbf{LMT}, a suite of \textbf{L}arge-scale
\textbf{M}ultilingual \textbf{T}ranslation models centered on both Chinese and
English, covering 60 languages and 234 translation directions. During
development, we identify a previously overlooked phenomenon of
\textbf{directional degeneration}, where symmetric multi-way fine-tuning data
overemphasize reverse directions (X $\to$ En/Zh), leading to excessive
many-to-one mappings and degraded translation quality. We propose
\textbf{Strategic Downsampling}, a simple yet effective method to mitigate this
degeneration. In addition, we design \textbf{Parallel Multilingual Prompting
(PMP)}, which leverages typologically related auxiliary languages to enhance
cross-lingual transfer. Through rigorous data curation and refined adaptation
strategies, LMT achieves SOTA performance among models of comparable language
coverage, with our 4B model (LMT-60-4B) surpassing the much larger Aya-101-13B
and NLLB-54B models by a substantial margin. We release LMT in four sizes
(0.6B/1.7B/4B/8B) to catalyze future research and provide strong baselines for
inclusive, scalable, and high-quality MMT
\footnote{\href{https://github.com/NiuTrans/LMT}{https://github.com/NiuTrans/LMT}}.

</details>


### [288] [A Picture is Worth a Thousand (Correct) Captions: A Vision-Guided Judge-Corrector System for Multimodal Machine Translation](https://arxiv.org/abs/2511.07010)
*Siddharth Betala,Kushan Raj,Vipul Betala,Rohan Saswade*

Main category: cs.CL

TL;DR: 本文提出了一种用于英语到多种印度语言（印地语、孟加拉语、马拉雅拉姆语和奥迪亚语）翻译的数据改进和模型微调系统，在WAT 2025赛道上取得了显著成效。核心包括使用多模态模型自动检测和修正训练数据的翻译错误，然后用LoRA对译模进行高效微调，带来BLEU分数提升。


<details>
  <summary>Details</summary>
Motivation: 多语言翻译系统中文本-图像多模态翻译存在数据质量差和翻译错误等问题，影响最终模型效果。作者希望通过系统性的数据清洗和高效模型微调提高English-to-Indic多语言机器翻译质量。

Method: 提出两阶段方法：首先设计基于多模态语言模型的自动纠错流水线——judge-corrector pipeline。Judge部分将翻译进行正确、需图片判别或翻译质量差分类；需图像情景的由GPT-4o-mini重生成，纯翻译错误的由IndicTrans2纠正。该流程纠正了28,928条训练数据中平均17.1%的caption。第二阶段利用Low-Rank Adaptation（LoRA）对IndicTrans2翻译模型在原始和修正数据集上进一步微调。

Result: 使用改正后的数据训练模型，在四种语言的任务上取得了BLEU分数提升：英语-孟加拉语在评测集提升1.3分，在挑战集提升0.7分；英语-奥迪亚语在评测集提升0.6分；英语-印地语在挑战集提升0.1分。

Conclusion: 自动化、多模态的数据修复流水线与参数高效的微调方法结合，可大幅提升English-to-Indic翻译任务中的机器翻译效果，证明系统性训练数据质量提升与模型微调协同有显著益处。

Abstract: In this paper, we describe our system under the team name BLEU Monday for the
English-to-Indic Multimodal Translation Task at WAT 2025. We participate in the
text-only translation tasks for English-Hindi, English-Bengali,
English-Malayalam, and English-Odia language pairs. We present a two-stage
approach that addresses quality issues in the training data through automated
error detection and correction, followed by parameter-efficient model
fine-tuning.
  Our methodology introduces a vision-augmented judge-corrector pipeline that
leverages multimodal language models to systematically identify and correct
translation errors in the training data. The judge component classifies
translations into three categories: correct, visually ambiguous (requiring
image context), or mistranslated (poor translation quality). Identified errors
are routed to specialized correctors: GPT-4o-mini regenerates captions
requiring visual disambiguation, while IndicTrans2 retranslates cases with pure
translation quality issues. This automated pipeline processes 28,928 training
examples across four languages, correcting an average of 17.1% of captions per
language.
  We then apply Low-Rank Adaptation (LoRA) to fine-tune the IndicTrans2
en-indic 200M distilled model on both original and corrected datasets. Training
on corrected data yields consistent improvements, with BLEU score gains of
+1.30 for English-Bengali on the evaluation set (42.00 -> 43.30) and +0.70 on
the challenge set (44.90 -> 45.60), +0.60 for English-Odia on the evaluation
set (41.00 -> 41.60), and +0.10 for English-Hindi on the challenge set (53.90
-> 54.00).

</details>


### [289] [Multilingual Lexical Feature Analysis of Spoken Language for Predicting Major Depression Symptom Severity](https://arxiv.org/abs/2511.07011)
*Anastasiia Tokareva,Judith Dineley,Zoe Firth,Pauline Conde,Faith Matcham,Sara Siddi,Femke Lamers,Ewan Carr,Carolin Oetzmann,Daniel Leightley,Yuezhou Zhang,Amos A. Folarin,Josep Maria Haro,Brenda W. J. H. Penninx,Raquel Bailon,Srinivasan Vairavan,Til Wykes,Richard J. B. Dobson,Vaibhav A. Narayan,Matthew Hotopf,Nicholas Cummins,The RADAR-CNS Consortium*

Main category: cs.CL

TL;DR: 本研究通过分析临床抑郁症患者多国语言下的口语数据，探索语言特征与抑郁症状严重程度之间的关系，但目前发现预测能力有限，需要更大样本和更优方法进一步研究。


<details>
  <summary>Details</summary>
Motivation: 移动设备获取的口语数据可为抑郁症的客观监测和复发早期发现提供新手段，但现有研究多集中于书面语或难以解释的复杂模型，不便于临床实际应用，缺乏跨语言临床样本研究。

Method: 分析了来自英国、荷兰和西班牙586名参与者的5,836份口语数据及PHQ-8测评，应用线性混合效应模型发现与抑郁症状相关的可解释词汇特征，并将这些特征及高维词向量投入4种回归机器学习模型进行预测评估。

Result: 在英语数据中，发现7个词汇特征（如词汇多样性、绝对主义词汇）与抑郁症状相关；荷兰语中与句子词数和正面词频有关；西班牙语未发现显著相关特征。所有语言下，词汇特征和向量嵌入对抑郁评分的预测能力接近随机水平。

Conclusion: 目前基于口语的词汇特征对于抑郁评估的预测力有限。未来应扩大样本、优化数据采集和建模流程，特别是开发适用于多语言的工具，并考虑语言使用的个体与群体差异，以提升语言指标在抑郁症临床研究与实践中的应用价值。

Abstract: Background: Captured between clinical appointments using mobile devices,
spoken language has potential for objective, more regular assessment of symptom
severity and earlier detection of relapse in major depressive disorder.
However, research to date has largely been in non-clinical cross-sectional
samples of written language using complex machine learning (ML) approaches with
limited interpretability.
  Methods: We describe an initial exploratory analysis of longitudinal speech
data and PHQ-8 assessments from 5,836 recordings of 586 participants in the UK,
Netherlands, and Spain, collected in the RADAR-MDD study. We sought to identify
interpretable lexical features associated with MDD symptom severity with linear
mixed-effects modelling. Interpretable features and high-dimensional vector
embeddings were also used to test the prediction performance of four regressor
ML models.
  Results: In English data, MDD symptom severity was associated with 7 features
including lexical diversity measures and absolutist language. In Dutch,
associations were observed with words per sentence and positive word frequency;
no associations were observed in recordings collected in Spain. The predictive
power of lexical features and vector embeddings was near chance level across
all languages.
  Limitations: Smaller samples in non-English speech and methodological
choices, such as the elicitation prompt, may have also limited the effect sizes
observable. A lack of NLP tools in languages other than English restricted our
feature choice.
  Conclusion: To understand the value of lexical markers in clinical research
and practice, further research is needed in larger samples across several
languages using improved protocols, and ML models that account for within- and
between-individual variations in language.

</details>


### [290] [Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for Multilingual and Cross-Lingual Tasks](https://arxiv.org/abs/2511.07025)
*Yauhen Babakhin,Radek Osmulski,Ronay Ak,Gabriel Moreira,Mengyao Xu,Benedikt Schifferer,Bo Liu,Even Oldridge*

Main category: cs.CL

TL;DR: 本文介绍了llama-embed-nemotron-8b，一款开源的文本嵌入模型，在多语种大型文本嵌入基准（MMTEB）排行榜上取得了最优成绩。


<details>
  <summary>Details</summary>
Motivation: 当前嵌入模型虽表现优异，但训练数据或方法常不公开。作者希望通过开源模型权重、详细消融实验以及公开训练数据集，推动社区透明和可复现性。

Method: 模型以1600多万条公开和合成的查询-文档对混合训练，包括对比损失实现、合成数据生成策略及模型融合的系统消融实验；具备指令感知，可根据用户需求优化任务表现。

Result: 模型在检索、分类、语义文本相似度等主流任务上表现突出，尤其在多语种、低资源和跨语言环境下取得了顶尖成绩。

Conclusion: llama-embed-nemotron-8b兼具高性能、广泛适用性和灵活性，有望成为通用的文本嵌入解决方案。

Abstract: We introduce llama-embed-nemotron-8b, an open-weights text embedding model
that achieves state-of-the-art performance on the Multilingual Massive Text
Embedding Benchmark (MMTEB) leaderboard as of October 21, 2025. While recent
models show strong performance, their training data or methodologies are often
not fully disclosed. We aim to address this by developing a fully open-source
model, publicly releasing its weights and detailed ablation studies, and
planning to share the curated training datasets. Our model demonstrates
superior performance across all major embedding tasks -- including retrieval,
classification and semantic textual similarity (STS) -- and excels in
challenging multilingual scenarios, such as low-resource languages and
cross-lingual setups. This state-of-the-art performance is driven by a novel
data mix of 16.1 million query-document pairs, split between 7.7 million
samples from public datasets and 8.4 million synthetically generated examples
from various open-weight LLMs. One of our key contributions is a detailed
ablation study analyzing core design choices, including a comparison of
contrastive loss implementations, an evaluation of synthetic data generation
(SDG) strategies, and the impact of model merging. The llama-embed-nemotron-8b
is an instruction-aware model, supporting user-defined instructions to enhance
performance for specific use-cases. This combination of top-tier performance,
broad applicability, and user-driven flexibility enables it to serve as a
universal text embedding solution.

</details>


### [291] [Evaluating LLMs for Anxiety, Depression, and Stress Detection Evaluating Large Language Models for Anxiety, Depression, and Stress Detection: Insights into Prompting Strategies and Synthetic Data](https://arxiv.org/abs/2511.07044)
*Mihael Arcan,David-Paul Niland*

Main category: cs.CL

TL;DR: 本文比较了多种模型在文本精神健康检测上的表现，包括大型语言模型（如Llama和GPT）、传统机器学习和BERT、XLNet、Distil-RoBERTa等Transformer架构，并使用合成数据改进模型表现。实验发现基于Transformer的方法效果突出，合成数据增强了模型召回率和泛化性。


<details>
  <summary>Details</summary>
Motivation: 随着心理健康障碍影响全球大量成年人，基于文本自动检测精神健康状况受限于症状表达的隐晦与多样性。亟需评估和对比不同自然语言处理模型提升检测效果。

Method: 作者采用DAIC-WOZ临床对话数据集，对多种模型进行了焦虑、抑郁和压力类别的微调，并通过生成合成数据缓解类别不平衡。对比了Transformer、LLMs及传统机器学习方法表现。

Result: Distil-RoBERTa在GAD-2任务上取得最高F1（0.883），XLNet在PHQ任务表现最佳（F1最高达0.891），压力检测任务中利用合成数据的零样本方法（SD+Zero-Shot-Basic）F1达到0.884，AUC为0.886。

Conclusion: Transformer模型表现优异；合成数据增强对提高召回率和泛化性有益，但需注意避免准确率下降。结合先进语言模型与数据增强，有望提升基于文本的心理健康自动评估。

Abstract: Mental health disorders affect over one-fifth of adults globally, yet
detecting such conditions from text remains challenging due to the subtle and
varied nature of symptom expression. This study evaluates multiple approaches
for mental health detection, comparing Large Language Models (LLMs) such as
Llama and GPT with classical machine learning and transformer-based
architectures including BERT, XLNet, and Distil-RoBERTa. Using the DAIC-WOZ
dataset of clinical interviews, we fine-tuned models for anxiety, depression,
and stress classification and applied synthetic data generation to mitigate
class imbalance. Results show that Distil-RoBERTa achieved the highest F1 score
(0.883) for GAD-2, while XLNet outperformed others on PHQ tasks (F1 up to
0.891). For stress detection, a zero-shot synthetic approach
(SD+Zero-Shot-Basic) reached an F1 of 0.884 and ROC AUC of 0.886. Findings
demonstrate the effectiveness of transformer-based models and highlight the
value of synthetic data in improving recall and generalization. However,
careful calibration is required to prevent precision loss. Overall, this work
emphasizes the potential of combining advanced language models and data
augmentation to enhance automated mental health assessment from text.

</details>


### [292] [When Sufficient is not Enough: Utilizing the Rashomon Effect for Complete Evidence Extraction](https://arxiv.org/abs/2511.07055)
*Katharina Beckh,Stefan Rüping*

Main category: cs.CL

TL;DR: 本文关注于特征归因方法在模型决策解释方面的局限性，提出了‘完整证据’识别的需求，并通过医疗数据集实证分析了单模型与多模型组合同归因表现。


<details>
  <summary>Details</summary>
Motivation: 传统的特征归因方法通常只给出模型判决的最小必要解释，但在实际应用中，比如合规或信息归档时，需要获得全部相关特征（完整证据）以全面解释模型决策。

Method: 作者通过一个包含人工标注‘完整证据’的医疗数据集进行案例研究，比较了单个模型与多个模型组合（集成）对完整证据的检出能力，并考察了训练方式、动态集成和置信度阈值等因素对表现的影响。

Result: 实验证明，单一模型通常只能找到部分证据（证据召回率约为0.60），而整合多个模型的归因结果可大幅提升召回率至0.86左右，尽管如此，召回率与精确率之间有权衡关系。

Conclusion: 依赖单模型归因不足以实现完整证据的识别；多模型集成可在提升证据召回率方面表现优越，对实际应用如合规审查有重要意义，同时需考虑准确率下降的权衡。

Abstract: Feature attribution methods typically provide minimal sufficient evidence
justifying a model decision. However, in many applications this is inadequate.
For compliance and cataloging, the full set of contributing features must be
identified - complete evidence. We perform a case study on a medical dataset
which contains human-annotated complete evidence. We show that individual
models typically recover only subsets of complete evidence and that aggregating
evidence from several models improves evidence recall from $\sim$0.60 (single
best model) to $\sim$0.86 (ensemble). We analyze the recall-precision
trade-off, the role of training with evidence, dynamic ensembles with certainty
thresholds, and discuss implications.

</details>


### [293] [Aligning Attention with Human Rationales for Self-Explaining Hate Speech Detection](https://arxiv.org/abs/2511.07065)
*Brage Eilertsen,Røskva Bjørgfinsdóttir,Francielle Vargas,Ali Ramezani-Kebrya*

Main category: cs.CL

TL;DR: 本文提出了一种结合人类推理的监督性注意力机制（SRA），可提升仇恨言论检测系统的可解释性和公平性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型的不透明性限制了仇恨言论检测系统的伦理部署，因此亟需提高模型的可解释性和公平性。

Method: 提出Supervised Rational Attention（SRA）框架，将监督性注意力机制融入基于Transformer的分类器，使模型注意力与人工标注的推理依据对齐，同时联合优化分类损失和注意力-推理一致性损失。

Result: SRA在英语（HateXplain）和葡萄牙语（HateBRXplain）仇恨言论数据集上，解释性提升2.4倍，并生成更忠实且与人类更一致的词级解释。在公平性指标上，SRA在检测针对身份群体的有害言论时表现为次优，其他指标与主流方法持平。

Conclusion: 将人类推理纳入注意力机制能在不影响公平性的前提下，提升模型的可解释性和忠实性。

Abstract: The opaque nature of deep learning models presents significant challenges for
the ethical deployment of hate speech detection systems. To address this
limitation, we introduce Supervised Rational Attention (SRA), a framework that
explicitly aligns model attention with human rationales, improving both
interpretability and fairness in hate speech classification. SRA integrates a
supervised attention mechanism into transformer-based classifiers, optimizing a
joint objective that combines standard classification loss with an alignment
loss term that minimizes the discrepancy between attention weights and
human-annotated rationales. We evaluated SRA on hate speech benchmarks in
English (HateXplain) and Portuguese (HateBRXplain) with rationale annotations.
Empirically, SRA achieves 2.4x better explainability compared to current
baselines, and produces token-level explanations that are more faithful and
human-aligned. In terms of fairness, SRA achieves competitive fairness across
all measures, with second-best performance in detecting toxic posts targeting
identity groups, while maintaining comparable results on other metrics. These
findings demonstrate that incorporating human rationales into attention
mechanisms can enhance interpretability and faithfulness without compromising
fairness.

</details>


### [294] [Importance-Aware Data Selection for Efficient LLM Instruction Tuning](https://arxiv.org/abs/2511.07074)
*Tingyu Jiang,Shen Li,Yiyao Song,Lan Zhang,Hualei Zhu,Yuan Zhao,Xiaohang Xu,Kenjiro Taura,Hao Henry Wang*

Main category: cs.CL

TL;DR: 该论文提出了一种新的度量方法（MIWV），用于挑选最有助于提升大语言模型（LLM）指令微调性能的数据，通过只选取1%高MIWV数据即可超过全量数据训练的效果。


<details>
  <summary>Details</summary>
Motivation: 现有指令微调依赖高质量数据，但如何有效选择最能提升LLM性能的数据仍不明确。以往只关注数据“质量分数”，忽视了模型自身对于数据的敏感性与提升空间。

Method: 作者提出Model Instruction Weakness Value（MIWV），通过测量模型在上下文学习（ICL）情况下表现的不一致性，定量评估每条指令数据对模型能力提升的重要性，据此进行高效数据选择。

Result: 实验证明，只选取MIWV排名前1%的数据进行微调，模型性能能超越用全部数据训练的结果。

Conclusion: 不仅验证了MIWV方法强于传统“数据质量分数”过滤策略，还为未来指令微调数据选择提供了新的路径。

Abstract: Instruction tuning plays a critical role in enhancing the performance and
efficiency of Large Language Models (LLMs). Its success depends not only on the
quality of the instruction data but also on the inherent capabilities of the
LLM itself. Some studies suggest that even a small amount of high-quality data
can achieve instruction fine-tuning results that are on par with, or even
exceed, those from using a full-scale dataset. However, rather than focusing
solely on calculating data quality scores to evaluate instruction data, there
is a growing need to select high-quality data that maximally enhances the
performance of instruction tuning for a given LLM. In this paper, we propose
the Model Instruction Weakness Value (MIWV) as a novel metric to quantify the
importance of instruction data in enhancing model's capabilities. The MIWV
metric is derived from the discrepancies in the model's responses when using
In-Context Learning (ICL), helping identify the most beneficial data for
enhancing instruction tuning performance. Our experimental results demonstrate
that selecting only the top 1\% of data based on MIWV can outperform training
on the full dataset. Furthermore, this approach extends beyond existing
research that focuses on data quality scoring for data selection, offering
strong empirical evidence supporting the effectiveness of our proposed method.

</details>


### [295] [EmoBang: Detecting Emotion From Bengali Texts](https://arxiv.org/abs/2511.07077)
*Abdullah Al Maruf,Aditi Golder,Zakaria Masud Jiyad,Abdullah Al Numan,Tarannum Shaila Zaman*

Main category: cs.CL

TL;DR: 本文针对孟加拉语情感识别任务，构建了八类情感标注数据集，并提出两种新模型，均大幅提升了识别准确率，填补了该领域基准数据和高性能模型的空白。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语作为世界第四大语言，情感识别相关研究严重不足，主要受限于缺乏大规模标准化数据集和高效深度模型。作者希望通过构建数据集和引入先进模型，推进该领域发展。

Method: 1）构建包含八种情感类别的孟加拉语数据集。2）提出两种自动识别模型：混合卷积递归神经网络（EmoBangHybrid）和AdaBoost-BERT集成模型（EmoBangEnsemble）。3）评测六个基线模型和五种特征工程方法，并测试零样本、少样本大模型表现。

Result: 提出的EmoBangHybrid和EmoBangEnsemble分别取得92.86%和93.69%的准确率，均优于现有方法；为孟加拉语情感识别领域提供了首个系统性基准。

Conclusion: 本文数据集和模型有效推动了孟加拉语情感识别研究，为后续相关工作提供了强有力的基准和工具。

Abstract: Emotion detection from text seeks to identify an individual's emotional or
mental state - positive, negative, or neutral - based on linguistic cues. While
significant progress has been made for English and other high-resource
languages, Bengali remains underexplored despite being the world's fourth most
spoken language. The lack of large, standardized datasets classifies Bengali as
a low-resource language for emotion detection. Existing studies mainly employ
classical machine learning models with traditional feature engineering,
yielding limited performance. In this paper, we introduce a new Bengali emotion
dataset annotated across eight emotion categories and propose two models for
automatic emotion detection: (i) a hybrid Convolutional Recurrent Neural
Network (CRNN) model (EmoBangHybrid) and (ii) an AdaBoost-Bidirectional Encoder
Representations from Transformers (BERT) ensemble model (EmoBangEnsemble).
Additionally, we evaluate six baseline models with five feature engineering
techniques and assess zero-shot and few-shot large language models (LLMs) on
the dataset. To the best of our knowledge, this is the first comprehensive
benchmark for Bengali emotion detection. Experimental results show that
EmoBangH and EmoBangE achieve accuracies of 92.86% and 93.69%, respectively,
outperforming existing methods and establishing strong baselines for future
research.

</details>


### [296] [Wasm: A Pipeline for Constructing Structured Arabic Interleaved Multimodal Corpora](https://arxiv.org/abs/2511.07080)
*Khalil Hennara,Ahmad Bastati,Muhammad Hreden,Mohamed Motasim Hamed,Zeina Aldallal,Sara Chrouf,Safwan AlModhayan*

Main category: cs.CL

TL;DR: 本文提出了一种新的阿拉伯语多模态数据集构建方法，能够保持互联网文档的结构完整性，并公开相关数据与处理流程，旨在推动阿拉伯语大模型的发展。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大模型的进步很大程度依赖于高质量、大规模的预训练数据集，而阿拉伯语领域缺乏能够保留文档结构的多模态数据集，严重制约了相关模型的发展。

Method: 作者开发了一个名为Wasm的处理流程，从Common Crawl数据集中提取数据，生成带有Markdown格式输出的新型阿拉伯语多模态数据集。该方法不仅提取文本，还保留网页内容的结构，适用于纯文本和多模态预训练场景。并与现有主流数据集处理流程进行了详细比较，阐述其过滤策略和设计选择的合理性。

Result: 新数据集不仅在结构完整性上优于以往阿拉伯语数据集，还具备多模态处理能力。同时，所述数据集与数据处理流程已向社会公开，便于后续相关研究。

Conclusion: 本文工作为阿拉伯语大型多模态模型的进一步发展奠定了基础，通过公开相关数据和流程，推动了该领域数据资源和方法的进步。

Abstract: The performance of large language models (LLMs) and large multimodal models
(LMMs) depends heavily on the quality and scale of their pre-training datasets.
Recent research shows that large multimodal models trained on natural documents
where images and text are interleaved outperform those trained only on
image-text pairs across a wide range of benchmarks, leveraging advanced pre-
trained models to enforce semantic alignment, image-sequence consistency, and
textual coherence. For Arabic, however, the lack of high-quality multimodal
datasets that preserve document structure has limited progress. In this paper,
we present our pipeline Wasm for processing the Common Crawl dataset to create
a new Arabic multimodal dataset that uniquely provides markdown output. Unlike
existing Arabic corpora that focus solely on text extraction, our approach
preserves the structural integrity of web content while maintaining flexibility
for both text-only and multimodal pre-training scenarios. We provide a
comprehensive comparative analysis of our data processing pipeline against
those used for major existing datasets, highlighting the convergences in
filtering strategies and justifying our specific design choices. To support
future research, we publicly release a representative dataset dump along with
the multimodal processing pipeline for Arabic.

</details>


### [297] [More Agents Helps but Adversarial Robustness Gap Persists](https://arxiv.org/abs/2511.07112)
*Khashayar Alavi,Zhastay Yeltay,Lucie Flek,Akbar Karimi*

Main category: cs.CL

TL;DR: 论文研究了多个大语言模型（LLM）Agent协作在数学问题解答中的鲁棒性，发现协作能提升准确率，但对抗噪声（特别是类似真人拼写错误）影响依然较大，且多Agent方案无法完全解决鲁棒性短板。


<details>
  <summary>Details</summary>
Motivation: 单个LLM在解答数学题时的鲁棒性有限，而多Agent协作已被证明能提升解答能力。作者希望明确，多Agent系统在遇到对抗噪声时是否也比单Agent表现更鲁棒。

Method: 提出Agent Forest框架，在数学基准题库（GSM8K、MATH等）上，用六种开源模型、不同数量的Agent（1到25），对带有三等级标点噪声、以及真实与仿真人类错别字的题目进行评测。

Result: （1）不同噪声类型影响差异大：标点噪声影响与干扰强度相关，但人类错别字影响最大且是主要瓶颈，哪怕Agent数量增加也无法弥补；（2）Agent数量增加稳定提升准确率，1到5提升最大，之后收益递减；但多Agent方案对鲁棒性的提升有限，对抗噪声影响仍存。

Conclusion: 多Agent协作能提升数学问题的解答准确率，但即便Agent数量增加，针对对抗性输入的鲁棒性短板依旧存在，特别是遇到人类拼写错误时，模型表现仍然远不如无噪声场景。

Abstract: When LLM agents work together, they seem to be more powerful than a single
LLM in mathematical question answering. However, are they also more robust to
adversarial inputs? We investigate this question using adversarially perturbed
math questions. These perturbations include punctuation noise with three
intensities (10, 30, and 50 percent), plus real-world and human-like typos
(WikiTypo, R2ATA). Using a unified sampling-and-voting framework (Agent
Forest), we evaluate six open-source models (Qwen3-4B/14B, Llama3.1-8B,
Mistral-7B, Gemma3-4B/12B) across four benchmarks (GSM8K, MATH, MMLU-Math,
MultiArith), with various numbers of agents n from one to 25 (1, 2, 5, 10, 15,
20, 25). Our findings show that (1) Noise type matters: punctuation noise harm
scales with its severity, and the human typos remain the dominant bottleneck,
yielding the largest gaps to Clean accuracy and the highest ASR even with a
large number of agents. And (2) Collaboration reliably improves accuracy as the
number of agents, n, increases, with the largest gains from one to five agents
and diminishing returns beyond 10 agents. However, the adversarial robustness
gap persists regardless of the agent count.

</details>


### [298] [Think Consistently, Reason Efficiently: Energy-Based Calibration for Implicit Chain-of-Thought](https://arxiv.org/abs/2511.07124)
*Zhikang Chen,Sen Cui,Deheng Ye,Yu Zhang,Yatao Bian,Tingting Zhu*

Main category: cs.CL

TL;DR: 本论文提出了EBM-CoT方法，通过能量模型校准大语言模型隐含推理轨迹，从而增强多步推理的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有显式Chain-of-Thought（CoT）方法依赖于离散的tokens推理，容易导致错误传播和表达受限，推理路径僵化且不一致。隐式或连续推理虽缓解此问题，但缺少确保推理一致性的机制，易产生分歧和不稳定结果。因此，亟需方法提升推理一致性和稳健性。

Method: 作者提出EBM-CoT框架，将能量模型（EBM）引入CoT，用于动态调整大模型在隐空间的推理轨迹，使其趋向低能量、高一致性的区域，以提升推理表现。该方法无需修改原有语言模型。

Result: 在数学、常识和符号推理基准上，作者方法显著提升了LLMs多步推理的一致性和效率。

Conclusion: EBM-CoT能够有效提升大语言模型多步推理时的准确性和一致性，为改进现有Chain-of-Thought推理方式提供了有力工具。

Abstract: Large Language Models (LLMs) have demonstrated strong reasoning capabilities
through \emph{Chain-of-Thought} (CoT) prompting, which enables step-by-step
intermediate reasoning. However, explicit CoT methods rely on discrete
token-level reasoning processes that are prone to error propagation and limited
by vocabulary expressiveness, often resulting in rigid and inconsistent
reasoning trajectories. Recent research has explored implicit or continuous
reasoning in latent spaces, allowing models to perform internal reasoning
before generating explicit output. Although such approaches alleviate some
limitations of discrete CoT, they generally lack explicit mechanisms to enforce
consistency among reasoning steps, leading to divergent reasoning paths and
unstable outcomes. To address this issue, we propose EBM-CoT, an Energy-Based
Chain-of-Thought Calibration framework that refines latent thought
representations through an energy-based model (EBM). Our method dynamically
adjusts latent reasoning trajectories toward lower-energy, high-consistency
regions in the embedding space, improving both reasoning accuracy and
consistency without modifying the base language model. Extensive experiments
across mathematical, commonsense, and symbolic reasoning benchmarks demonstrate
that the proposed framework significantly enhances the consistency and
efficiency of multi-step reasoning in LLMs.

</details>


### [299] [LoRA on the Go: Instance-level Dynamic LoRA Selection and Merging](https://arxiv.org/abs/2511.07129)
*Seungeon Lee,Soumi Das,Manish Gupta,Krishna P. Gummadi*

Main category: cs.CL

TL;DR: 本文提出了一种名为LoGo的无训练新框架，可以在推理阶段动态选取并融合不同的LoRA适配器，无需额外标注数据或任务特定训练，提升多任务场景下大模型微调实用性。


<details>
  <summary>Details</summary>
Motivation: 传统的LoRA适配器一般为单一任务训练，难以满足真实应用中的多样化输入需求。现有提升多任务性能的方法需要标注数据或额外训练，成本高、扩展性弱。作者希望解决这一实际困境。

Method: LoGo框架在无需训练的前提下，通过对各LoRA适配器进行一次正向传播，提取信号用于动态选出最相关的适配器并分配不同权重，实现实例级的适配器融合。

Result: 在5个NLP基准、27个数据集和3个模型族上评测，LoGo在部分任务上超越需训练的基线方法（最高优3.6%），其余任务表现同样具竞争力，并保持高效推理速度。

Conclusion: LoGo框架有效提升了LoRA在多任务、多域场景下的适应性，无需额外训练或标注，具备很强的实用价值和推广潜力。

Abstract: Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient approach for
fine-tuning large language models.However, conventional LoRA adapters are
typically trained for a single task, limiting their applicability in real-world
settings where inputs may span diverse and unpredictable domains. At inference
time, existing approaches combine multiple LoRAs for improving performance on
diverse tasks, while usually requiring labeled data or additional task-specific
training, which is expensive at scale. In this work, we introduce LoRA on the
Go (LoGo), a training-free framework that dynamically selects and merges
adapters at the instance level without any additional requirements. LoGo
leverages signals extracted from a single forward pass through LoRA adapters,
to identify the most relevant adapters and determine their contributions
on-the-fly. Across 5 NLP benchmarks, 27 datasets, and 3 model families, LoGo
outperforms training-based baselines on some tasks upto a margin of 3.6% while
remaining competitive on other tasks and maintaining inference throughput,
highlighting its effectiveness and practicality.

</details>


### [300] [TCM-Eval: An Expert-Level Dynamic and Extensible Benchmark for Traditional Chinese Medicine](https://arxiv.org/abs/2511.07148)
*Zihao Cheng,Yuheng Lu,Huaiqian Ye,Zeming Liu,Minqi Wang,Jingjing Liu,Zihan Li,Wei Fan,Yuanfang Guo,Ruiji Fu,Shifeng She,Gang Wang,Yunhong Wang*

Main category: cs.CL

TL;DR: 本文提出了TCM-Eval基准和ZhiMingTang大型语言模型，提升了大模型在中医领域的能力，并通过社区榜单促进持续发展。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在现代医学中的表现优异，但因缺乏标准化基准及高质量数据，导致在中医领域应用受限。作者希望通过构建标准化评测和优质数据集，推动大模型在中医领域的发展。

Method: 1. 构建了第一个动态、可扩展的中医标准化评测基准TCM-Eval，并邀请中医专家进行验证。2. 构建了大规模中医训练语料。3. 提出自迭代链式思维增强（SI-CoTE）方法，通过拒绝采样让模型自主生成并优化带有推理链的问题-答案对，促进数据与模型的联动进化。4. 利用上述数据训练了ZhiMingTang（ZMT）中医专用大模型。

Result: ZMT模型在TCM-Eval基准测试中大幅超过了中医执业医师的合格分数，展现出优异的中医问答能力。

Conclusion: ZMT模型及相应的数据和评测体系，有效推动了大模型在中医领域的应用和发展，作者通过公开榜单促进社区持续研究和迭代。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
modern medicine, yet their application in Traditional Chinese Medicine (TCM)
remains severely limited by the absence of standardized benchmarks and the
scarcity of high-quality training data. To address these challenges, we
introduce TCM-Eval, the first dynamic and extensible benchmark for TCM,
meticulously curated from national medical licensing examinations and validated
by TCM experts. Furthermore, we construct a large-scale training corpus and
propose Self-Iterative Chain-of-Thought Enhancement (SI-CoTE) to autonomously
enrich question-answer pairs with validated reasoning chains through rejection
sampling, establishing a virtuous cycle of data and model co-evolution. Using
this enriched training data, we develop ZhiMingTang (ZMT), a state-of-the-art
LLM specifically designed for TCM, which significantly exceeds the passing
threshold for human practitioners. To encourage future research and
development, we release a public leaderboard, fostering community engagement
and continuous improvement.

</details>


### [301] [Categorical Emotions or Appraisals - Which Emotion Model Explains Argument Convincingness Better?](https://arxiv.org/abs/2511.07162)
*Lynn Greschner,Meike Bauer,Sabine Weber,Roman Klinger*

Main category: cs.CL

TL;DR: 论文探讨了说服性论证中情感（pathos）的主观性，发现基于评价理论（appraisal theories）的情感分析对说服性预测有更大提升作用。


<details>
  <summary>Details</summary>
Motivation: 传统上，情感在论证说服性中的作用被认为较为粗略（如情绪强度或情绪类别），但作者认为情感的唤起本质上是主观的，受接受者目标、标准、知识和立场等影响。目前缺乏从主观视角评估论证说服性的有效方法。

Method: 利用ContArgA数据集，基于人工标注和预测情感及评价特征，采用zero-shot prompt实验，比较情绪类别信息与评价特征对说服性标签预测的效力。

Result: 虽然情绪类别对于说服性有一定提升，但基于主观评价的情感分析提升更显著。首次系统性比较了不同情感模型在说服性预测中的表现。

Conclusion: 基于评价理论的主观情感分析在论证说服性研究中具有明显优势，拓展了计算论证领域情感建模的理论与实际应用。

Abstract: The convincingness of an argument does not only depend on its structure
(logos), the person who makes the argument (ethos), but also on the emotion
that it causes in the recipient (pathos). While the overall intensity and
categorical values of emotions in arguments have received considerable
attention in the research community, we argue that the emotion an argument
evokes in a recipient is subjective. It depends on the recipient's goals,
standards, prior knowledge, and stance. Appraisal theories lend themselves as a
link between the subjective cognitive assessment of events and emotions. They
have been used in event-centric emotion analysis, but their suitability for
assessing argument convincingness remains unexplored. In this paper, we
evaluate whether appraisal theories are suitable for emotion analysis in
arguments by considering subjective cognitive evaluations of the importance and
impact of an argument on its receiver. Based on the annotations in the recently
published ContArgA corpus, we perform zero-shot prompting experiments to
evaluate the importance of gold-annotated and predicted emotions and appraisals
for the assessment of the subjective convincingness labels. We find that, while
categorical emotion information does improve convincingness prediction, the
improvement is more pronounced with appraisals. This work presents the first
systematic comparison between emotion models for convincingness prediction,
demonstrating the advantage of appraisals, providing insights for theoretical
and practical applications in computational argumentation.

</details>


### [302] [AdaRec: Adaptive Recommendation with LLMs via Narrative Profiling and Dual-Channel Reasoning](https://arxiv.org/abs/2511.07166)
*Meiyun Wang,Charin Polpanumas*

Main category: cs.CL

TL;DR: 本文提出了一种名为AdaRec的个性化推荐框架，通过大语言模型进行少样本的上下文学习，并能高效适应不同任务。实验表明，AdaRec在少样本和零样本场景均显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的推荐系统需要大量监督或手动特征工程，难以高效适应多任务，且对新用户数据不足时表现有限。为提升个性化和任务泛化能力，作者提出新的方法。

Method: AdaRec采用“叙事画像”把用户-物品交互转化为自然语言，实现任务统一处理和增强可读性。其双通道架构结合行为一致性和因果溯源，自动挖掘用户偏好关键因素，避免了传统手动设计特征的问题，并能适应少样本新任务。

Result: 实验证明AdaRec在真实电商数据上，少样本条件下比传统机器学习和现有大模型方法最高提升8%，零样本下比专家策略提升19%。利用AdaRec生成的合成数据轻量微调后，模型效果与大规模全量微调相当。

Conclusion: AdaRec能实现高效、泛化能力强的个性化推荐，特别适用于长尾用户和数据稀疏场景，并极大简化了推荐系统的特征工程和调整成本。

Abstract: We propose AdaRec, a few-shot in-context learning framework that leverages
large language models for an adaptive personalized recommendation. AdaRec
introduces narrative profiling, transforming user-item interactions into
natural language representations to enable unified task handling and enhance
human readability. Centered on a bivariate reasoning paradigm, AdaRec employs a
dual-channel architecture that integrates horizontal behavioral alignment,
discovering peer-driven patterns, with vertical causal attribution,
highlighting decisive factors behind user preferences. Unlike existing
LLM-based approaches, AdaRec eliminates manual feature engineering through
semantic representations and supports rapid cross-task adaptation with minimal
supervision. Experiments on real ecommerce datasets demonstrate that AdaRec
outperforms both machine learning models and LLM-based baselines by up to eight
percent in few-shot settings. In zero-shot scenarios, it achieves up to a
nineteen percent improvement over expert-crafted profiling, showing
effectiveness for long-tail personalization with minimal interaction data.
Furthermore, lightweight fine-tuning on synthetic data generated by AdaRec
matches the performance of fully fine-tuned models, highlighting its efficiency
and generalization across diverse tasks.

</details>


### [303] [EMODIS: A Benchmark for Context-Dependent Emoji Disambiguation in Large Language Models](https://arxiv.org/abs/2511.07193)
*Jiacheng Huang,Ning Yu,Xiaoyin Yi*

Main category: cs.CL

TL;DR: 该论文提出了EMODIS基准，用于评估大语言模型（LLMs）在最小但对比性文本上下文中解释表情符号歧义的能力。实验结果表明，现有主流LLM在细微上下文线索面前分辨含义时表现有限，表现出对主导解释的系统性偏见和对语用对比的不敏感。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM已广泛应用于实际交流中，其解决依赖上下文歧义能力的研究仍然不足，尤其是在表情符号产生歧义时。因此，作者希望通过新基准评估LLM在复杂语境中的语义推理水平。

Method: 作者提出EMODIS基准，每个测试项包含含糊句子、表情、两个有差异的消歧语境和需结合上下文进行推理的问题。用该基准对开源和API型LLM进行系统评测。

Result: 实验发现，即便是最先进的模型，在上下文仅有微妙差异时，区分多重含义的能力仍然欠佳，且有明显的解释偏倚和对语境变化不敏感。

Conclusion: EMODIS为测试LLM上下文消歧能力提供了严格工具，并揭示了当前LLM在语义推理上与人类之间的明显差距。

Abstract: Large language models (LLMs) are increasingly deployed in real-world
communication settings, yet their ability to resolve context-dependent
ambiguity remains underexplored. In this work, we present EMODIS, a new
benchmark for evaluating LLMs' capacity to interpret ambiguous emoji
expressions under minimal but contrastive textual contexts. Each instance in
EMODIS comprises an ambiguous sentence containing an emoji, two distinct
disambiguating contexts that lead to divergent interpretations, and a specific
question that requires contextual reasoning. We evaluate both open-source and
API-based LLMs, and find that even the strongest models frequently fail to
distinguish meanings when only subtle contextual cues are present. Further
analysis reveals systematic biases toward dominant interpretations and limited
sensitivity to pragmatic contrast. EMODIS provides a rigorous testbed for
assessing contextual disambiguation, and highlights the gap in semantic
reasoning between humans and LLMs.

</details>


### [304] [Discourse Graph Guided Document Translation with Large Language Models](https://arxiv.org/abs/2511.07230)
*Viet-Thanh Pham,Minghan Wang,Hao-Han Liao,Thuy-Trang Vu*

Main category: cs.CL

TL;DR: 本文提出了TransGraph，一种基于话语图结构引导的文档级机器翻译方法，有效建模文档内部的依赖关系，提升翻译质量并减少计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在文档级机器翻译中难以捕捉长距离依赖和保持全局连贯性，现有方法（如多智能体与持续记忆）虽然能缓解上下文窗口限制，但计算成本高，对记忆检索策略敏感。

Method: 提出TransGraph框架，通过结构化的话语图显式建模文档中各分块间关系，并在翻译每个片段时有选择地利用相关邻域语境，而不依赖顺序或全局上下文。

Result: 在三套涵盖六种语言和不同领域的文档级翻译基准上，TransGraph在翻译质量和术语一致性方面均优于强基线系统，同时显著降低token开销。

Conclusion: TransGraph实现了更有效的文档级机器翻译，兼顾翻译效果和资源消耗，为长文本翻译任务提供了新思路。

Abstract: Adapting large language models to full document translation remains
challenging due to the difficulty of capturing long-range dependencies and
preserving discourse coherence throughout extended texts. While recent agentic
machine translation systems mitigate context window constraints through
multi-agent orchestration and persistent memory, they require substantial
computational resources and are sensitive to memory retrieval strategies. We
introduce TransGraph, a discourse-guided framework that explicitly models
inter-chunk relationships through structured discourse graphs and selectively
conditions each translation segment on relevant graph neighbourhoods rather
than relying on sequential or exhaustive context. Across three document-level
MT benchmarks spanning six languages and diverse domains, TransGraph
consistently surpasses strong baselines in translation quality and terminology
consistency while incurring significantly lower token overhead.

</details>


### [305] [Who Is the Story About? Protagonist Entity Recognition in News](https://arxiv.org/abs/2511.07296)
*Jorge Gabín,M. Eduardo Ares,Javier Parapar*

Main category: cs.CL

TL;DR: 该论文提出了“主角实体识别（PER）”任务，用于识别新闻报道中的核心机构实体，并用大模型实现自动化标注和评测。


<details>
  <summary>Details</summary>
Motivation: 以往实体识别任务未能区分新闻报道中真正驱动故事发展的关键机构实体，影响了事件突出性、影响力及叙事焦点相关下游任务。通过标注主角实体，可提升信息抽取的叙事理解能力。

Method: 提出PER任务，定义主角实体。通过人工专家标注建立金标准语料库，再采用大模型（LLM）预测进行对比，分析人机一致性。利用NER辅助提示方法，用LLM对大规模新闻语料自动标注主角实体，并进一步测试在信息受限场景下LLM的主角推断能力。

Result: 1）专家间及专家与LLM的主角实体判定达到高一致性；2）大模型在NER辅助下可自动高质量标注主角实体，具有良好可扩展性；3）在上下文受限和无明确候选实体提示时，LLM依然能较好推断主角。

Conclusion: PER是信息抽取落地于叙事层面的有益补充，基于指导性提示的大模型能够高效近似人工对新闻主角的重要性判断。

Abstract: News articles often reference numerous organizations, but traditional Named
Entity Recognition (NER) treats all mentions equally, obscuring which entities
genuinely drive the narrative. This limits downstream tasks that rely on
understanding event salience, influence, or narrative focus. We introduce
Protagonist Entity Recognition (PER), a task that identifies the organizations
that anchor a news story and shape its main developments. To validate PER, we
compare he predictions of Large Language Models (LLMs) against annotations from
four expert annotators over a gold corpus, establishing both inter-annotator
consistency and human-LLM agreement. Leveraging these findings, we use
state-of-the-art LLMs to automatically label large-scale news collections
through NER-guided prompting, generating scalable, high-quality supervision. We
then evaluate whether other LLMs, given reduced context and without explicit
candidate guidance, can still infer the correct protagonists. Our results
demonstrate that PER is a feasible and meaningful extension to
narrative-centered information extraction, and that guided LLMs can approximate
human judgments of narrative importance at scale.

</details>


### [306] [Retriv at BLP-2025 Task 1: A Transformer Ensemble and Multi-Task Learning Approach for Bangla Hate Speech Identification](https://arxiv.org/abs/2511.07304)
*Sourav Saha,K M Nafi Asib,Mohammed Moshiul Hoque*

Main category: cs.CL

TL;DR: 本文提出了使用多模型集成和多任务学习方法识别孟加拉语仇恨言论，并在相关竞赛中排名居中，效果较好。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语属于低资源语言，仇恨言论识别对于社会有重要意义，但面临语言和资源双重挑战，因此亟需有效方法提升识别水平。

Method: 针对三个子任务，分别使用了BanglaBERT、MuRIL、IndicBERTv2等transformer模型的软投票集成（1A和1B），以及多种多任务模型的加权集成（1C）。

Result: 各子任务的micro-f1分别为72.75%、72.69%和72.62%，在竞赛榜单中分列第9、第10和第7。

Conclusion: transformer模型集成和多任务加权方法在低资源情境下提升了孟加拉语仇恨言论识别效果，研究具有实际推广价值。

Abstract: This paper addresses the problem of Bangla hate speech identification, a
socially impactful yet linguistically challenging task. As part of the "Bangla
Multi-task Hate Speech Identification" shared task at the BLP Workshop,
IJCNLP-AACL 2025, our team "Retriv" participated in all three subtasks: (1A)
hate type classification, (1B) target group identification, and (1C) joint
detection of type, severity, and target. For subtasks 1A and 1B, we employed a
soft-voting ensemble of transformer models (BanglaBERT, MuRIL, IndicBERTv2).
For subtask 1C, we trained three multitask variants and aggregated their
predictions through a weighted voting ensemble. Our systems achieved micro-f1
scores of 72.75% (1A) and 72.69% (1B), and a weighted micro-f1 score of 72.62%
(1C). On the shared task leaderboard, these corresponded to 9th, 10th, and 7th
positions, respectively. These results highlight the promise of transformer
ensembles and weighted multitask frameworks for advancing Bangla hate speech
detection in low-resource contexts. We made experimental scripts publicly
available for the community.

</details>


### [307] [ACE-ICD: Acronym Expansion As Data Augmentation For Automated ICD Coding](https://arxiv.org/abs/2511.07311)
*Tuan-Dung Le,Shohreh Haddadan,Thanh Q. Thieu*

Main category: cs.CL

TL;DR: 本文提出了一种利用大型语言模型扩展医学缩写，并结合一致性训练，提高自动ICD编码性能的方法，在MIMIC-III数据集上取得了最新最佳表现。


<details>
  <summary>Details</summary>
Motivation: 现有自动ICD编码方法主要关注代码层次和同义词，对临床笔记中普遍存在的医学缩写利用不足，影响了ICD代码推断准确性。

Method: 提出了一种数据增强技术，借助大语言模型自动扩展临床文档中的医学缩写，并让模型在包含缩写全称的表示上训练。同时，引入一致性训练，通过约束原始文档和增强文档的预测一致性来正则化模型。

Result: 在MIMIC-III数据集上的大量实验表明，该方法（ACE-ICD）在常见代码、罕见代码以及全量代码场景下均实现了新的最优性能。

Conclusion: 医学缩写扩展及一致性训练可以显著提升自动ICD编码的准确度，方法通用有效，代码已开源，可为实际临床应用提供改进和参考。

Abstract: Automatic ICD coding, the task of assigning disease and procedure codes to
electronic medical records, is crucial for clinical documentation and billing.
While existing methods primarily enhance model understanding of code
hierarchies and synonyms, they often overlook the pervasive use of medical
acronyms in clinical notes, a key factor in ICD code inference. To address this
gap, we propose a novel effective data augmentation technique that leverages
large language models to expand medical acronyms, allowing models to be trained
on their full form representations. Moreover, we incorporate consistency
training to regularize predictions by enforcing agreement between the original
and augmented documents. Extensive experiments on the MIMIC-III dataset
demonstrate that our approach, ACE-ICD establishes new state-of-the-art
performance across multiple settings, including common codes, rare codes, and
full-code assignments. Our code is publicly available.

</details>


### [308] [RLVE: Scaling Up Reinforcement Learning for Language Models with Adaptive Verifiable Environments](https://arxiv.org/abs/2511.07317)
*Zhiyuan Zeng,Hamish Ivison,Yiping Wang,Lifan Yuan,Shuyue Stella Li,Zhuorui Ye,Siting Li,Jacqueline He,Runlong Zhou,Tong Chen,Chenyang Zhao,Yulia Tsvetkov,Simon Shaolei Du,Natasha Jaques,Hao Peng,Pang Wei Koh,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: 作者提出了RLVE方法，通过适应性可验证环境提升语言模型的推理能力，并构建了RLVE-Gym环境库进行验证，实验表明该方法显著提升了模型在多项推理基准上的表现。


<details>
  <summary>Details</summary>
Motivation: 传统静态数据分布在强化学习训练语言模型时，常因任务过难或过易导致学习信号消失，限制了模型能力提升。因此需要能动态适应模型能力、提供可验证反馈的新方法。

Method: 提出RLVE方法，通过可验证环境动态调整问题难度，并开发RLVE-Gym——含有400个手工设计的可验证环境。模型在RLVE-Gym中进行RL训练，环境难度与模型水平自适应匹配。

Result: 在RLVE-Gym中联合400个环境训练，语言模型在六个推理基准测试上相较同规模模型有3.37%的平均绝对提升。相比之下，原有泛化RL训练仅提升0.49%，且计算资源消耗更多。

Conclusion: RLVE有效提升了语言模型的推理泛化能力，环境多样性和适应性是关键。提出的方法在计算效率和效果上优于传统训练方案，并已开源代码以促进后续研究。

Abstract: We introduce Reinforcement Learning (RL) with Adaptive Verifiable
Environments (RLVE), an approach using verifiable environments that
procedurally generate problems and provide algorithmically verifiable rewards,
to scale up RL for language models (LMs). RLVE enables each verifiable
environment to dynamically adapt its problem difficulty distribution to the
policy model's capabilities as training progresses. In contrast, static data
distributions often lead to vanishing learning signals when problems are either
too easy or too hard for the policy. To implement RLVE, we create RLVE-Gym, a
large-scale suite of 400 verifiable environments carefully developed through
manual environment engineering. Using RLVE-Gym, we show that environment
scaling, i.e., expanding the collection of training environments, consistently
improves generalizable reasoning capabilities. RLVE with joint training across
all 400 environments in RLVE-Gym yields a 3.37% absolute average improvement
across six reasoning benchmarks, starting from one of the strongest 1.5B
reasoning LMs. By comparison, continuing this LM's original RL training yields
only a 0.49% average absolute gain despite using over 3x more compute. We
release our code publicly.

</details>


### [309] [When Bias Pretends to Be Truth: How Spurious Correlations Undermine Hallucination Detection in LLMs](https://arxiv.org/abs/2511.07318)
*Shaowen Wang,Yiqi Dong,Ruinian Chang,Tansheng Zhu,Yuebo Sun,Kaifeng Lyu,Jian Li*

Main category: cs.CL

TL;DR: 该论文关注大模型由于数据中虚假相关性引发的一类置信度高但错误的幻觉，这种幻觉目前检测和解决方法无效，需要新方法应对。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型有显著进步，但依然存在“幻觉”问题，而其中由训练集虚假相关性（如姓氏与国籍的关联）导致的幻觉很少被系统关注。

Method: 作者利用系统化的合成实验、真实的实验评估多个主流LLM（包括GPT-5），比较当前主流幻觉检测方法（如置信度过滤、内部状态探测）在虚假相关性驱动场景下的表现，并从理论上分析其失败原因。

Result: 实验和理论结果都表明，当前基于置信度的检测方法在面对虚假相关性引发幻觉时基本失效，而且这些幻觉对模型规模和微调具有免疫力。

Conclusion: 当前幻觉检测方法不能解决由数据虚假相关性导致的新型幻觉问题，未来需开发专门处理该方向的新技术。

Abstract: Despite substantial advances, large language models (LLMs) continue to
exhibit hallucinations, generating plausible yet incorrect responses. In this
paper, we highlight a critical yet previously underexplored class of
hallucinations driven by spurious correlations -- superficial but statistically
prominent associations between features (e.g., surnames) and attributes (e.g.,
nationality) present in the training data. We demonstrate that these spurious
correlations induce hallucinations that are confidently generated, immune to
model scaling, evade current detection methods, and persist even after refusal
fine-tuning. Through systematically controlled synthetic experiments and
empirical evaluations on state-of-the-art open-source and proprietary LLMs
(including GPT-5), we show that existing hallucination detection methods, such
as confidence-based filtering and inner-state probing, fundamentally fail in
the presence of spurious correlations. Our theoretical analysis further
elucidates why these statistical biases intrinsically undermine
confidence-based detection techniques. Our findings thus emphasize the urgent
need for new approaches explicitly designed to address hallucinations caused by
spurious correlations.

</details>


### [310] [FinRpt: Dataset, Evaluation System and LLM-based Multi-agent Framework for Equity Research Report Generation](https://arxiv.org/abs/2511.07322)
*Song Jin,Shuqi Li,Shukun Zhang,Rui Yan*

Main category: cs.CL

TL;DR: 本文首次提出了利用大模型自动化生成股票研究报告（ERR）任务，建立了一个开放的评测基准FinRpt，并提出多智能体框架FinRpt-Gen，经实验验证该框架在该领域的优越性能。


<details>
  <summary>Details</summary>
Motivation: 目前大模型在金融领域部分任务（如股票预测、问答）表现优秀，但在自动化生成股票研究报告（ERR）方面仍属空白，因此亟需构建实现路径和评测标准。

Method: 作者提出了ERR自动生成任务，设计了一条集成7类金融数据的数据集自动构建流程，形成高质量数据集用于训练和评测。同时，开发了包含11项评估指标的综合评测体系。此外，提出多智能体框架FinRpt-Gen，并结合有监督微调与强化学习对基于大模型的智能体进行训练。

Result: 实验表明，FinRpt基准提供了高质量数据与有效评测指标，FinRpt-Gen框架在ERR生成任务上表现出色。相关代码与数据集已全部开源。

Conclusion: 本文构建了完整的数据和评测环境，并提出高效的多智能体自动报告生成系统，为金融报告自动化和金融大模型研究注入了创新动力。

Abstract: While LLMs have shown great success in financial tasks like stock prediction
and question answering, their application in fully automating Equity Research
Report generation remains uncharted territory. In this paper, we formulate the
Equity Research Report (ERR) Generation task for the first time. To address the
data scarcity and the evaluation metrics absence, we present an open-source
evaluation benchmark for ERR generation - FinRpt. We frame a Dataset
Construction Pipeline that integrates 7 financial data types and produces a
high-quality ERR dataset automatically, which could be used for model training
and evaluation. We also introduce a comprehensive evaluation system including
11 metrics to assess the generated ERRs. Moreover, we propose a multi-agent
framework specifically tailored to address this task, named FinRpt-Gen, and
train several LLM-based agents on the proposed datasets using Supervised
Fine-Tuning and Reinforcement Learning. Experimental results indicate the data
quality and metrics effectiveness of the benchmark FinRpt and the strong
performance of FinRpt-Gen, showcasing their potential to drive innovation in
the ERR generation field. All code and datasets are publicly available.

</details>


### [311] [Selecting Auxiliary Data via Neural Tangent Kernels for Low-Resource Domains](https://arxiv.org/abs/2511.07380)
*Pingjie Wang,Hongcheng Liu,Yusheng Liao,Ziqing Fan,Yaxin Du,Shuo Tang,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 本文提出NTK-Selector方法，通过有效选择通用领域辅助数据，大幅提升大模型在低资源领域的表现，优于仅使用领域数据。


<details>
  <summary>Details</summary>
Motivation: 大模型在低资源领域受限于数据稀缺和过拟合风险，难以取得优异成绩。虽然特定领域数据少，但通用领域有大量数据。如何高效选择最有价值的辅助数据以增强领域表现，是亟需解决的问题，尤其是在传统做法不可行时。

Method: 提出NTK-Selector框架，通过神经切线核（NTK）理论指导辅助数据选择。针对LLM直接应用NTK的假设及算力瓶颈，作者通过实验证明LLM在LoRA微调过程中表现出稳定的NTK特性，并提出免雅可比矩阵的新近似方法以降低计算成本。

Result: 在医学、金融、法律、心理四个低资源领域，大量实验显示NTK-Selector显著提升下游性能。例如，使用1000条领域数据微调仅提升0.8~0.9分，而加入由NTK-Selector选出的9000条辅助数据后，性能提升可达5.1~8.7分，提升幅度为领域数据场景下的5.7~10.9倍。

Conclusion: NTK-Selector能稳定有效地筛选通用辅助数据，极大提升LLM在低资源领域中的表现。该方法在验证资源有限时，提供一种高效实用的解决方案，有望拓展大模型在更多特定领域的适用性。

Abstract: Large language models (LLMs) have achieved remarkable success across
widespread tasks, yet their application in low-resource domains remains a
significant challenge due to data scarcity and the high risk of overfitting.
While in-domain data is limited, there exist vast amounts of similar
general-domain data, and our initial findings reveal that they could
potentially serve as auxiliary supervision for domain enhancement. This
observation leads us to our central research question: \textbf{\textit{how to
effectively select the most valuable auxiliary data to maximize domain-specific
performance}}, particularly when traditional methods are inapplicable due to a
lack of large in-domain data pools or validation sets. To address this, we
propose \textbf{NTK-Selector}, a principled and efficient framework for
selecting general-domain auxiliary data to enhance domain-specific performance
via neural tangent kernels (NTK). Our method tackles two challenges of directly
applying NTK to LLMs, theoretical assumptions and prohibitive computational
cost, by empirically demonstrating a stable NTK-like behavior in LLMs during
LoRA fine-tuning and proposing a Jacobian-free approximation method. Extensive
experiments across four low-resource domains (medical, financial, legal, and
psychological) demonstrate that NTK-Selector consistently improves downstream
performance. Specifically, fine-tuning on 1,000 in-domain samples alone only
yielded +0.8 points for Llama3-8B-Instruct and +0.9 points for Qwen3-8B. In
contrast, enriching with 9,000 auxiliary samples selected by NTK-Selector led
to substantial \textbf{gains of +8.7 and +5.1 points}, which corresponds to a
\textbf{10.9x and 5.7x improvement} over the domain-only setting.

</details>


### [312] [Retriv at BLP-2025 Task 2: Test-Driven Feedback-Guided Framework for Bangla-to-Python Code Generation](https://arxiv.org/abs/2511.07382)
*K M Nafi Asib,Sourav Saha,Mohammed Moshiul Hoque*

Main category: cs.CL

TL;DR: 本文提出了一种结合提示工程与测试驱动、反馈指导下的迭代优化方法，利用微调后的Qwen2.5-14B模型提升了孟加拉语到Python代码的自动生成能力，并在国际竞赛中取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 当前主流大模型支持的代码生成大多聚焦于高资源语言，而孟加拉语等低资源语言缺乏数据与基准，代码生成效果较差。因此，亟需探索和改进低资源语言的代码生成方法。

Method: 利用微调的Qwen2.5-14B大模型，输入为孟加拉语描述，通过多轮生成和单元测试反馈指导模型修复不通过的代码，反复迭代三次提升结果，并结合了指令提示学习。

Result: 该方法在BLP@IJCNLP-AACL 2025的孟加拉语代码生成任务中获得Pass@1分数0.934，排名第二。

Conclusion: 实验验证了结合自动测试和迭代优化能有效提升低资源语言代码生成质量，指出针对低资源语言和特定语言理解的专门方法亟待发展，相关实验脚本已共享。

Abstract: Large Language Models (LLMs) have advanced the automated generation of code
from natural language prompts. However, low-resource languages (LRLs) like
Bangla remain underrepresented due to the limited availability of
instruction-to-code datasets and evaluation benchmarks. To address this, the
BLP Workshop at IJCNLP-AACL 2025 introduced a shared task on "Code Generation
in Bangla". In this work, we propose a method that combines instruction
prompting with a test-driven, feedback-guided iterative refinement process
using a fine-tuned Qwen2.5-14B model. The model generates code from Bangla
instructions, tests it against unit tests, and iteratively refines any failing
outputs through three evaluation passes, using test feedback to guide each
step. This approach helped our team "Retriv" to secure 2nd place in the shared
task with a Pass@1 score of 0.934. The analysis highlights challenges in Bangla
instruction understanding and Python code generation, emphasizing the need for
targeted methods in LRLs. We made experimental scripts publicly available for
the community.

</details>


### [313] [Teaching Pretrained Language Models to Think Deeper with Retrofitted Recurrence](https://arxiv.org/abs/2511.07384)
*Sean McLeish,Ang Li,John Kirchenbauer,Dayal Singh Kalra,Brian R. Bartoldson,Bhavya Kailkhura,Avi Schwarzschild,Jonas Geiping,Tom Goldstein,Micah Goldblum*

Main category: cs.CL

TL;DR: 本文探讨了如何将现有的预训练非循环语言模型转化为深度循环模型，通过递归式训练课程增加模型深度，在保证性能的同时降低总计算成本。实验证明，在相同计算预算下，转换后的深度循环模型在数学任务上优于直接微调的非循环模型。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型规模扩大，参数量和计算需求同步增长。深度循环语言模型通过在测试时重用参数，能够在不增加参数量和预训练计算的前提下降低推理阶段的计算成本。该研究旨在探索如何高效地将现有的大型非循环预训练模型转化为推理高效的循环结构。

Method: 采用递归课程学习方法，逐步增加模型在训练过程中的有效深度，并将预训练的非循环语言模型重新训练为深度循环结构。通过对比实验，分析这种方法在各类任务中的表现。

Result: 实验显示，在数学任务上，将非循环预训练模型转换为深度循环模型，能在相同的计算预算下获得更好的性能表现，相较于直接微调传统非循环模型更为高效。

Conclusion: 通过递归式训练将预训练的非循环语言模型转化为深度循环结构可在保证模型性能的同时有效降低总计算量，特别适合计算资源受限的实际场景。

Abstract: Recent advances in depth-recurrent language models show that recurrence can
decouple train-time compute and parameter count from test-time compute. In this
work, we study how to convert existing pretrained non-recurrent language models
into depth-recurrent models. We find that using a curriculum of recurrences to
increase the effective depth of the model over the course of training preserves
performance while reducing total computational cost. In our experiments, on
mathematics, we observe that converting pretrained models to recurrent ones
results in better performance at a given compute budget than simply
post-training the original non-recurrent language model.

</details>


### [314] [Surgical Agent Orchestration Platform for Voice-directed Patient Data Interaction](https://arxiv.org/abs/2511.07392)
*Hyeryun Park,Byung Mo Gu,Jun Hee Lee,Byeong Hyeon Choi,Sekeun Kim,Hyun Koo Kim,Kyungsang Kim*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型（LLM）的语音驱动外科助手平台（SAOP），用于辅助达芬奇机器人手术中多模态数据的访问与操作，实现了高效准确的语音指令解析与任务执行。


<details>
  <summary>Details</summary>
Motivation: 达芬奇机器人手术过程中，医生需同时专注于手术操作和查看多源患者数据，但手眼高度占用使数据访问受限、效率降低，因此需一种无需中断手术流程的智能化数据交互方式。

Method: 构建了一个分层多智能体模型的语音助手平台（SAOP），包括一个编排智能体与三个基于LLM的任务智能体，可根据语音命令自动规划、细化、验证并推理生成具体操作（如临床信息检索、CT操控、3D模型导航）；并设计MOEM多层次编排评测指标来多角度评判系统性能。

Result: SAOP在240条语音命令下实现了高准确率和高成功率，LLM 智能体提升了对语音识别错误及多样或模糊自由命令的鲁棒性。

Conclusion: SAOP平台展示出强大潜力，可高效稳健地辅助微创达芬奇机器人手术流程，提升临床可用性与智能化水平。

Abstract: In da Vinci robotic surgery, surgeons' hands and eyes are fully engaged in
the procedure, making it difficult to access and manipulate multimodal patient
data without interruption. We propose a voice-directed Surgical Agent
Orchestrator Platform (SAOP) built on a hierarchical multi-agent framework,
consisting of an orchestration agent and three task-specific agents driven by
Large Language Models (LLMs). These LLM-based agents autonomously plan, refine,
validate, and reason to map voice commands into specific tasks such as
retrieving clinical information, manipulating CT scans, or navigating 3D
anatomical models on the surgical video. We also introduce a Multi-level
Orchestration Evaluation Metric (MOEM) to comprehensively assess the
performance and robustness from command-level and category-level perspectives.
The SAOP achieves high accuracy and success rates across 240 voice commands,
while LLM-based agents improve robustness against speech recognition errors and
diverse or ambiguous free-form commands, demonstrating strong potential to
support minimally invasive da Vinci robotic surgery.

</details>


### [315] [ConvFill: Model Collaboration for Responsive Conversational Voice Agents](https://arxiv.org/abs/2511.07397)
*Vidya Srinivas,Zachary Englhardt,Maximus Powers,Shwetak Patel,Vikram Iyer*

Main category: cs.CL

TL;DR: 该论文提出一种“对话填充”新任务，通过结合本地轻量模型与云端大模型，实现响应迅速且智能丰富的语音对话系统。提出的ConvFill模型大幅提升小模型对话准确率，同时保证极低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型虽然知识丰富、推理能力强，但部署于云端会带来延迟，影响自然对话；本地模型响应快，却缺乏复杂性。如何兼得低延迟与高能力是核心挑战。

Method: 该文提出“对话填充”任务：本地轻量模型负责生成即时对话，云端强大模型流式提供知识补充。二者结合，缓解延迟与能力间矛盾。具体实现为ConvFill模型（参数量3.6亿），在合成的多领域对话数据上训练。

Result: 多种后端大模型配合下，ConvFill模型在不用牺牲响应时间（保持在200毫秒以内）的条件下，将小模型的对话准确率提升了36-42%。

Conclusion: 对话填充方法能让本地语音助手系统既具高响应性又具丰富知识，展现出面向实际应用的可行性和优势。

Abstract: Deploying conversational voice agents with large language models faces a
critical challenge: cloud-based foundation models provide deep reasoning and
domain knowledge but introduce latency that disrupts natural conversation,
while on-device models respond immediately but lack sophistication. We propose
conversational infill, a task where a lightweight on-device model generates
contextually appropriate dialogue while seamlessly incorporating streaming
knowledge from a powerful backend model. This approach decouples response
latency from model capability, enabling systems that feel responsive while
accessing the full power of large-scale models. We present ConvFill, a 360M
parameter model trained on synthetic multi-domain conversations. Evaluation
across multiple backend models shows that conversational infill can be
successfully learned, with ConvFill achieving accuracy improvements of 36-42%
over standalone small models of the same size while consistently retaining
sub-200ms response latencies. Our results demonstrate the promise of this
approach for building on-device conversational agents that are both immediately
responsive and knowledgeable.

</details>


### [316] [SPOT: An Annotated French Corpus and Benchmark for Detecting Critical Interventions in Online Conversations](https://arxiv.org/abs/2511.07405)
*Manon Berriche,Célia Nouri,Chloé Clavel,Jean-Philippe Cointet*

Main category: cs.CL

TL;DR: 本论文提出了SPOT（Stopping Points in Online Threads）语料库，并将社交学中的“停止点”概念引入为可复现的自然语言处理任务，专注于分析在线讨论中出现的可以暂停或重定向对话的关键干预。作者构建了基于法语Facebook评论的大型人工标注数据集，并验证了不同模型对该任务的效果表现。


<details>
  <summary>Details</summary>
Motivation: 现有的言论审核和对话干预研究往往忽略了可以有效暂停或引导对话走向的“停止点”现象，这对于理解和管理错误信息传播、维护良好社交氛围具有重要意义。亟需将这一社会学现象转化为可通过机器学习方法研究的NLP任务。

Method: 将“停止点”定义为二分类任务，手动标注了43,305条法语Facebook评论，并提供详细的注释指南和丰富的上下文元数据。使用细调的CamemBERT编码器模型和指令微调的大语言模型（LLM）在不同提示策略下进行基线评测，同时分析了上下文信息对模型表现的影响。

Result: 细调的编码器模型在F1分数上比指令微调的LLM高10个百分点以上，显示监督学习在新兴非英语社交媒体任务上的优势；增加上下文元数据后，编码器模型F1得分由0.75提升到0.78。

Conclusion: SPOT语料库首次系统性定义并标注了在线讨论中的“停止点”，为相关NLP任务研究提供了坚实基础。实验显示，监督学习配合上下文信息提升了模型效果，强调了准确数据和方法的重要性。研究公开数据集和代码，推动透明和可复现的学术进展。

Abstract: We introduce SPOT (Stopping Points in Online Threads), the first annotated
corpus translating the sociological concept of stopping point into a
reproducible NLP task. Stopping points are ordinary critical interventions that
pause or redirect online discussions through a range of forms (irony, subtle
doubt or fragmentary arguments) that frameworks like counterspeech or social
correction often overlook. We operationalize this concept as a binary
classification task and provide reliable annotation guidelines. The corpus
contains 43,305 manually annotated French Facebook comments linked to URLs
flagged as false information by social media users, enriched with contextual
metadata (article, post, parent comment, page or group, and source). We
benchmark fine-tuned encoder models (CamemBERT) and instruction-tuned LLMs
under various prompting strategies. Results show that fine-tuned encoders
outperform prompted LLMs in F1 score by more than 10 percentage points,
confirming the importance of supervised learning for emerging non-English
social media tasks. Incorporating contextual metadata further improves encoder
models F1 scores from 0.75 to 0.78. We release the anonymized dataset, along
with the annotation guidelines and code in our code repository, to foster
transparency and reproducible research.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [317] [Lite VLA: Efficient Vision-Language-Action Control on CPU-Bound Edge Robots](https://arxiv.org/abs/2511.05642)
*Justin Williams,Kishor Datta Gupta,Roy George,Mrinmoy Sarkar*

Main category: cs.RO

TL;DR: 该论文展示了在移动机器人上部署小型视觉-语言模型(VLM)以便在边缘设备上实时完成场景理解与推理的可行性。实现了在完全本地硬件下同步移动和推理，无需依赖云端。


<details>
  <summary>Details</summary>
Motivation: 随着自主机器人在无GPS环境中的应用日益增加，如何在本地高效地进行智能推理成为关键。已有方法往往将感知与运动分开，难以满足资源受限场景的实时响应。

Method: 提出并部署了一种集成紧凑型VLM和多模态感知的边缘推理框架，使机器人可以在嵌入式硬件上直接完成场景理解和上下文推理。所有运行均在本地，解决了传统方案对云端依赖的问题。

Result: 在实验验证中，系统在计算效率、任务准确度和响应性间取得了良好平衡。实际在移动机器人上的部署，展示了小型VLM同时支持边缘推理和自主移动的首次成功应用。

Conclusion: 该研究为服务机器人、灾难应对和国防等领域的可扩展、可靠的自主应用奠定了技术基础。

Abstract: The deployment of artificial intelligence models at the edge is increasingly
critical for autonomous robots operating in GPS-denied environments where
local, resource-efficient reasoning is essential. This work demonstrates the
feasibility of deploying small Vision-Language Models (VLMs) on mobile robots
to achieve real-time scene understanding and reasoning under strict
computational constraints. Unlike prior approaches that separate perception
from mobility, the proposed framework enables simultaneous movement and
reasoning in dynamic environments using only on-board hardware. The system
integrates a compact VLM with multimodal perception to perform contextual
interpretation directly on embedded hardware, eliminating reliance on cloud
connectivity. Experimental validation highlights the balance between
computational efficiency, task accuracy, and system responsiveness.
Implementation on a mobile robot confirms one of the first successful
deployments of small VLMs for concurrent reasoning and mobility at the edge.
This work establishes a foundation for scalable, assured autonomy in
applications such as service robotics, disaster response, and defense
operations.

</details>


### [318] [VLM-driven Skill Selection for Robotic Assembly Tasks](https://arxiv.org/abs/2511.05680)
*Jeong-Jung Kim,Doo-Yeol Koh,Chang-Hyun Kim*

Main category: cs.RO

TL;DR: 本论文提出了一个结合视觉语言模型（VLMs）和模仿学习的机器人装配框架，大大提升了机器人在装配任务中的灵活性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统的机器人装配在灵活性和泛化能力上存在不足。本研究旨在利用VLM和模仿学习，提高机器人对复杂指令的理解能力及适应多样装配任务的能力。

Method: 系统采用一台带有夹持器的三维移动机器人，整合视觉感知、自然语言理解与学习到的原始技能。通过分解装配任务，将复杂操作转化为可解释的技能模块，并结合模仿学习训练机器人的执行能力。

Result: 实验结果表明，该方法在多种装配场景下都能实现高成功率，且通过技能分解增强了系统的可解释性。

Conclusion: 融合视觉语言模型与模仿学习的机器人装配框架能够提升装配任务的灵活性、效果以及任务执行的可解释性，对未来复杂智能装配具有重要价值。

Abstract: This paper presents a robotic assembly framework that combines
Vision-Language Models (VLMs) with imitation learning for assembly manipulation
tasks. Our system employs a gripper-equipped robot that moves in 3D space to
perform assembly operations. The framework integrates visual perception,
natural language understanding, and learned primitive skills to enable flexible
and adaptive robotic manipulation. Experimental results demonstrate the
effectiveness of our approach in assembly scenarios, achieving high success
rates while maintaining interpretability through the structured primitive skill
decomposition.

</details>


### [319] [TumorMap: A Laser-based Surgical Platform for 3D Tumor Mapping and Fully-Automated Tumor Resection](https://arxiv.org/abs/2511.05723)
*Guangshen Ma,Ravi Prakash,Beatrice Schleupner,Jeffrey Everitt,Arpit Mishra,Junqin Chen,Brian Mann,Boyuan Chen,Leila Bridgeman,Pei Zhong,Mark Draelos,William C. Eward,Patrick J. Codd*

Main category: cs.RO

TL;DR: 本文介绍了“TumorMap”外科机器人平台，结合多种激光传感和深度学习，实现了对恶性肿瘤的术中三维建模与自主切除。实验显示该平台具有高精度、全自动、无接触切除能力。


<details>
  <summary>Details</summary>
Motivation: 现有恶性肿瘤切除依赖外科医生技能，但面临3D高精度建模难、组织模型泛化差以及手术过程中的物理局限（如手抖、疲劳）的挑战。为提高手术精度与自动化迫切需要新方法。

Method: TumorMap平台集成三种激光（光学相干断层扫描、内源性荧光激发、切割激光手术刀）与深度学习模型，能够全自动、无接触地在术中重建肿瘤边界并切除组织。

Result: 在小鼠骨肉瘤和软组织肉瘤模型中进行验证，并建立新型病理学评估流程，TumorMap实现了亚毫米级切除精度和无需人工干预的手术过程。

Conclusion: TumorMap展示了多模态传感器引导的自主肿瘤手术的可行性，有望大幅提升肿瘤切除的智能化与安全性。

Abstract: Surgical resection of malignant solid tumors is critically dependent on the
surgeon's ability to accurately identify pathological tissue and remove the
tumor while preserving surrounding healthy structures. However, building an
intraoperative 3D tumor model for subsequent removal faces major challenges due
to the lack of high-fidelity tumor reconstruction, difficulties in developing
generalized tissue models to handle the inherent complexities of tumor
diagnosis, and the natural physical limitations of bimanual operation,
physiologic tremor, and fatigue creep during surgery. To overcome these
challenges, we introduce "TumorMap", a surgical robotic platform to formulate
intraoperative 3D tumor boundaries and achieve autonomous tissue resection
using a set of multifunctional lasers. TumorMap integrates a three-laser
mechanism (optical coherence tomography, laser-induced endogenous fluorescence,
and cutting laser scalpel) combined with deep learning models to achieve
fully-automated and noncontact tumor resection. We validated TumorMap in murine
osteoscarcoma and soft-tissue sarcoma tumor models, and established a novel
histopathological workflow to estimate sensor performance. With submillimeter
laser resection accuracy, we demonstrated multimodal sensor-guided autonomous
tumor surgery without any human intervention.

</details>


### [320] [A Unified Stochastic Mechanism Underlying Collective Behavior in Ants, Physical Systems, and Robotic Swarms](https://arxiv.org/abs/2511.05785)
*Lianhao Yin,Haiping Yu,Pascal Spino,Daniela Rus*

Main category: cs.RO

TL;DR: 本文发现了生物群体、物理系统和机器人群体在随机行为上的共同统计机制——在不同能量约束下的最大化原理。


<details>
  <summary>Details</summary>
Motivation: 一直以来，生物群体（如蚂蚁）和物理系统（如气体、液体）都表现出分散、随机的微观行为，但目前缺乏一个能够统一解释两者随机性的理论框架。

Method: 作者通过实验证据研究红林蚁(Formica polyctena)的群体行为，并构建了能量最大化约束下的统计模型。此外，将该机制应用到机器人群体，观察其表现出的协作和相变类物理行为。

Result: 实验证明蚂蚁和物理粒子的随机行为均可用能量函数约束下的最大化来描述，并且机器人群体也能基于此机制，实现分散、可扩展的协作和物理相变样的行为。

Conclusion: 本文提出了一个统一的随机行为模型，连接了生物、物理和机器人群体，为设计健壮、智能的群体机器人系统提供了可扩展的理论依据。

Abstract: Biological swarms, such as ant colonies, achieve collective goals through
decentralized and stochastic individual behaviors. Similarly, physical systems
composed of gases, liquids, and solids exhibit random particle motion governed
by entropy maximization, yet do not achieve collective objectives. Despite this
analogy, no unified framework exists to explain the stochastic behavior in both
biological and physical systems. Here, we present empirical evidence from
\textit{Formica polyctena} ants that reveals a shared statistical mechanism
underlying both systems: maximization under different energy function
constraints. We further demonstrate that robotic swarms governed by this
principle can exhibit scalable, decentralized cooperation, mimicking physical
phase-like behaviors with minimal individual computation. These findings
established a unified stochastic model linking biological, physical, and
robotic swarms, offering a scalable principle for designing robust and
intelligent swarm robotics.

</details>


### [321] [VLAD-Grasp: Zero-shot Grasp Detection via Vision-Language Models](https://arxiv.org/abs/2511.05791)
*Manav Kulshrestha,S. Talha Bukhari,Damon Conover,Aniket Bera*

Main category: cs.RO

TL;DR: 本文提出了一种在无须训练和人工标注的情况下，利用视觉-语言大模型直接实现机器抓取的新方法（VLAD-Grasp），在标准数据集和真实机器人实验中取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有机器人抓取方法大多依赖大量专家标注，需要针对新物体反复训练，泛化性差。研究动机在于突破对标注和再训练的依赖，使机器人能零样本泛化地抓取新物体。

Method: 方法基于大规模视觉-语言模型。具体流程为：1. 输入单张RGB-D图片，通过视觉-语言模型生成一个目标图像（如直杆“刺穿”物体，代表对抓点）；2. 利用深度和分割技术将该图像提升为三维；3. 采用主成分分析和无对应优化，将生成的点云与观测到的物体点云对齐，从而恢复可执行的抓取位姿。整个过程无需训练，也不依赖于抓取数据集。

Result: 在Cornell和Jacquard公开抓取数据集上，VLAD-Grasp与最先进的有监督方法表现相当甚至更优。此外，该方法还能在真实的Franka机器人上实现对新物体的零样本泛化抓取。

Conclusion: VLAD-Grasp无需训练和数据集标注，利用视觉-语言基础模型作为强先验，在机器人操作领域展现出零样本泛化能力，有望显著简化机器人抓取系统的部署与扩展。

Abstract: Robotic grasping is a fundamental capability for autonomous manipulation;
however, most existing methods rely on large-scale expert annotations and
necessitate retraining to handle new objects. We present VLAD-Grasp, a
Vision-Language model Assisted zero-shot approach for Detecting grasps. From a
single RGB-D image, our method (1) prompts a large vision-language model to
generate a goal image where a straight rod "impales" the object, representing
an antipodal grasp, (2) predicts depth and segmentation to lift this generated
image into 3D, and (3) aligns generated and observed object point clouds via
principal component analysis and correspondence-free optimization to recover an
executable grasp pose. Unlike prior work, our approach is training-free and
does not rely on curated grasp datasets. Despite this, VLAD-Grasp achieves
performance that is competitive with or superior to that of state-of-the-art
supervised models on the Cornell and Jacquard datasets. We further demonstrate
zero-shot generalization to novel real-world objects on a Franka Research 3
robot, highlighting vision-language foundation models as powerful priors for
robotic manipulation.

</details>


### [322] [An Open-Source, Reproducible Tensegrity Robot that can Navigate Among Obstacles](https://arxiv.org/abs/2511.05798)
*William R. Johnson III,Patrick Meng,Nelson Chen,Luca Cimatti,Augustin Vercoutere,Mridul Aanjaneya,Rebecca Kramer-Bottiglio,Kostas E. Bekris*

Main category: cs.RO

TL;DR: 本论文提出了一个完整开源的3杆张力完整性（tensegrity）机器人导航系统，包括硬件设计和软件平台，可支持物理建模、路径规划与避障，并在多种复杂环境中实验验证了系统的鲁棒性和可复现性。


<details>
  <summary>Details</summary>
Motivation: 张力完整性机器人虽然具有抗冲击、轻质和适应复杂地形的优势，但其柔顺性和耦合动力学带来建模与控制的难题，限制了自主导航和避障能力。作者希望解决实际部署与复现性问题，为学术界和工业界提供便捷的研究与开发平台。

Method: 系统包括低成本开源硬件和一套集成的软件栈。软件涵盖物理建模、系统辨识、状态估计、路径规划和控制。所有设计均开源，供科研复现与扩展。通过搭建的系统，机器人可感知自身姿态，并基于环境障碍物实现避障导航。多实验室多组设备进行了全流程实验验证。

Result: 实验包括应对垂直落差、坡道、粒状介质等非建模挑战，并在户外实现路径规划和导航。不同实验室的设备均成功复现了该系统。机器人在已知障碍环境中能够可靠实现避障导航，展现出良好适应性和鲁棒性。

Conclusion: 本工作为张力完整性机器人导航系统提供了完整、易复现、公开可用的软硬件平台。该系统可促进相关领域后续导航技术和新型机器人平台的研究与应用。

Abstract: Tensegrity robots, composed of rigid struts and elastic tendons, provide
impact resistance, low mass, and adaptability to unstructured terrain. Their
compliance and complex, coupled dynamics, however, present modeling and control
challenges, hindering path planning and obstacle avoidance. This paper presents
a complete, open-source, and reproducible system that enables navigation for a
3-bar tensegrity robot. The system comprises: (i) an inexpensive, open-source
hardware design, and (ii) an integrated, open-source software stack for
physics-based modeling, system identification, state estimation, path planning,
and control. All hardware and software are publicly available at
https://sites.google.com/view/tensegrity-navigation/. The proposed system
tracks the robot's pose and executes collision-free paths to a specified goal
among known obstacle locations. System robustness is demonstrated through
experiments involving unmodeled environmental challenges, including a vertical
drop, an incline, and granular media, culminating in an outdoor field
demonstration. To validate reproducibility, experiments were conducted using
robot instances at two different laboratories. This work provides the robotics
community with a complete navigation system for a compliant, impact-resistant,
and shape-morphing robot. This system is intended to serve as a springboard for
advancing the navigation capabilities of other unconventional robotic
platforms.

</details>


### [323] [Adversarial Game-Theoretic Algorithm for Dexterous Grasp Synthesis](https://arxiv.org/abs/2511.05809)
*Yu Chen,Botao He,Yuemin Mao,Arthur Jakobsson,Jeffrey Ke,Yiannis Aloimonos,Guanya Shi,Howie Choset,Jiayuan Mao,Jeffrey Ichnowski*

Main category: cs.RO

TL;DR: 这篇论文提出了一种基于双人博弈的新型多指机械手抓取策略，能提升对物体的稳定抓取率，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多指机械手在实际应用中仍难以实现稳定且可靠的物体抓取，主要因为现有方法通常只抵抗单一作用力（wrench），无法有效应对物体的对抗性逃脱行为，导致实际抓取易失败。

Method: 作者将抓取问题建模为一个双人博弈：一方是控制机械手以生成可行抓取的“玩家”，另一方是控制物体尝试逃脱的“对抗玩家”。通过博弈，促使抓取策略不仅可行还能应对物体的各种对抗性运动。方法在不同机械手平台和物体上通过仿真和实物测试验证。

Result: 仿真实验中，该方法的成功抓取率为75.78%，比现有最优基线提高了多达19.61%。引入博弈机制比不带博弈的方法抓取成功率提升了27.40%。平均规划时间0.28-1.04秒，满足实际使用需求。真实机器人实验中，ShadowHand成功率为85.0%，LeapHand达87.5%。

Conclusion: 本文方法有效综合了物体的对抗性动作，极大提升了多指机械手抓取的稳定性和鲁棒性，适合在实际机器人系统中部署。

Abstract: For many complex tasks, multi-finger robot hands are poised to revolutionize
how we interact with the world, but reliably grasping objects remains a
significant challenge. We focus on the problem of synthesizing grasps for
multi-finger robot hands that, given a target object's geometry and pose,
computes a hand configuration. Existing approaches often struggle to produce
reliable grasps that sufficiently constrain object motion, leading to
instability under disturbances and failed grasps. A key reason is that during
grasp generation, they typically focus on resisting a single wrench, while
ignoring the object's potential for adversarial movements, such as escaping. We
propose a new grasp-synthesis approach that explicitly captures and leverages
the adversarial object motion in grasp generation by formulating the problem as
a two-player game. One player controls the robot to generate feasible grasp
configurations, while the other adversarially controls the object to seek
motions that attempt to escape from the grasp. Simulation experiments on
various robot platforms and target objects show that our approach achieves a
success rate of 75.78%, up to 19.61% higher than the state-of-the-art baseline.
The two-player game mechanism improves the grasping success rate by 27.40% over
the method without the game formulation. Our approach requires only 0.28-1.04
seconds on average to generate a grasp configuration, depending on the robot
platform, making it suitable for real-world deployment. In real-world
experiments, our approach achieves an average success rate of 85.0% on
ShadowHand and 87.5% on LeapHand, which confirms its feasibility and
effectiveness in real robot setups.

</details>


### [324] [3D Mapping Using a Lightweight and Low-Power Monocular Camera Embedded inside a Gripper of Limbed Climbing Robots](https://arxiv.org/abs/2511.05816)
*Taku Okawara,Ryo Nishibe,Mao Kasano,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 本文提出了一种基于单目手眼相机的三维地形映射系统，适用于装备四足或多足的攀爬机器人，旨在实现低能耗、高效精确的三维环境感知，无需传统笨重的RGB-D相机。


<details>
  <summary>Details</summary>
Motivation: 月球和火星等极端环境中的立体攀爬机器人，需要能耗低、体积小但精确的三维感知系统。现有的RGB-D相机虽然直观，但动力消耗大且体积大，不适于深空任务，迫切需要更高效、轻量化的方案。

Method: 提出一种融合单目视觉约束与肢体运动学（limb forward kinematics）信息的SLAM方法，通过因子图优化，联合估计机器人手爪的时序姿态和全局三维地图的尺度，实现单目相机的无尺度歧义三维重建。

Result: 在物理仿真和实际机器人实验中验证了方法的有效性，结果显示该方法能实时构建公平比例（三维尺度准确）的地形地图，并支持机器人的自主抓取地形凸起部分，实现了以单目相机替代RGB-D相机的高效三维感知。

Conclusion: 该方法为未来空间任务中的攀爬机器人提供了更具可扩展性、低能耗的三维环境感知技术，有助于推动深空探索任务中机器人自主能力的发展。

Abstract: Limbed climbing robots are designed to explore challenging vertical walls,
such as the skylights of the Moon and Mars. In such robots, the primary role of
a hand-eye camera is to accurately estimate 3D positions of graspable points
(i.e., convex terrain surfaces) thanks to its close-up views. While
conventional climbing robots often employ RGB-D cameras as hand-eye cameras to
facilitate straightforward 3D terrain mapping and graspable point detection,
RGB-D cameras are large and consume considerable power.
  This work presents a 3D terrain mapping system designed for space exploration
using limbed climbing robots equipped with a monocular hand-eye camera.
Compared to RGB-D cameras, monocular cameras are more lightweight, compact
structures, and have lower power consumption. Although monocular SLAM can be
used to construct 3D maps, it suffers from scale ambiguity. To address this
limitation, we propose a SLAM method that fuses monocular visual constraints
with limb forward kinematics. The proposed method jointly estimates time-series
gripper poses and the global metric scale of the 3D map based on factor graph
optimization.
  We validate the proposed framework through both physics-based simulations and
real-world experiments. The results demonstrate that our framework constructs a
metrically scaled 3D terrain map in real-time and enables autonomous grasping
of convex terrain surfaces using a monocular hand-eye camera, without relying
on RGB-D cameras. Our method contributes to scalable and energy-efficient
perception for future space missions involving limbed climbing robots. See the
video summary here: https://youtu.be/fMBrrVNKJfc

</details>


### [325] [Gentle Manipulation Policy Learning via Demonstrations from VLM Planned Atomic Skills](https://arxiv.org/abs/2511.05855)
*Jiayu Zhou,Qiwei Wu,Jian Li,Zhe Chen,Xiaogang Xiong,Renjing Xu*

Main category: cs.RO

TL;DR: 本文提出了一种结合了语义分解、强化学习、视觉语言模型及知识蒸馏的新框架，实现无需大量人类演示的数据高效机器人操纵任务学习。


<details>
  <summary>Details</summary>
Motivation: 当前机器人长程、复杂操纵任务需要大量真实世界数据和专家工程，导致高成本、难扩展。本研究旨在解决这一瓶颈。

Method: 将复杂任务分解为原子技能，每个技能通过强化学习在仿真环境中训练，并显式引入力约束以保护物体。利用视觉语言模型（VLM）进行高层次任务分解和技能规划，产生专家演示，再通过视觉触觉扩散策略将其蒸馏为端到端统一策略。通过消融研究和模仿学习算法比较优化流程。

Result: 在大规模仿真和真实机器人实验中，验证了本框架无需高成本人类演示便能学习到有效的长程操纵策略。VLM引导的技能原子化结构也展现出良好的任务泛化能力。

Conclusion: 本文方法降低了机器人复杂操纵学习的人力与数据成本，并通过模块化设计提升了任务的扩展性和泛化性，为机器人自主执行多样复杂任务提供了有效新范式。

Abstract: Autonomous execution of long-horizon, contact-rich manipulation tasks
traditionally requires extensive real-world data and expert engineering, posing
significant cost and scalability challenges. This paper proposes a novel
framework integrating hierarchical semantic decomposition, reinforcement
learning (RL), visual language models (VLMs), and knowledge distillation to
overcome these limitations. Complex tasks are decomposed into atomic skills,
with RL-trained policies for each primitive exclusively in simulation.
Crucially, our RL formulation incorporates explicit force constraints to
prevent object damage during delicate interactions. VLMs perform high-level
task decomposition and skill planning, generating diverse expert
demonstrations. These are distilled into a unified policy via Visual-Tactile
Diffusion Policy for end-to-end execution. We conduct comprehensive ablation
studies exploring different VLM-based task planners to identify optimal
demonstration generation pipelines, and systematically compare imitation
learning algorithms for skill distillation. Extensive simulation experiments
and physical deployment validate that our approach achieves policy learning for
long-horizon manipulation without costly human demonstrations, while the
VLM-guided atomic skill framework enables scalable generalization to diverse
tasks.

</details>


### [326] [ViTaMIn-B: A Reliable and Efficient Visuo-Tactile Bimanual Manipulation Interface](https://arxiv.org/abs/2511.05858)
*Chuanyu Li,Chaoyi Liu,Daotan Wang,Shuyu Zhang,Lusong Li,Zecui Zeng,Fangchen Liu,Jing Xu,Rui Chen*

Main category: cs.RO

TL;DR: 本文提出了ViTaMIn-B，一个更高效且功能更强大的手持式数据采集系统，在复杂的双手和接触丰富的操作任务中表现优越。新的传感器设计和姿态跟踪技术提升了系统的采集质量和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前手持设备在大规模高质量示范采集方面潜力巨大，但普遍缺乏稳健的触觉感知和可靠的位姿跟踪，难以胜任复杂、富有接触和双手操作的任务，亟需更先进方案。

Method: 1. 设计了新型的DuoTact合顺从性视觉-触觉传感器，能承受较大操作力并捕获高分辨率接触几何。2. 将传感器的整体变形重建为3D点云，并以此作为策略输入以增强跨传感器的泛化能力。3. 利用Meta Quest控制器开发统一、稳健的6自由度双手姿态获取方法，有效解决常规SLAM方法中的轨迹漂移难题。4. 通过用户研究和任务实验验证系统性能。

Result: 用户试验证明ViTaMIn-B对新手和专家均效率高、易用性好。在四类双手操作任务中的实验结果显示，ViTaMIn-B的任务表现优于现有系统。

Conclusion: ViTaMIn-B能高效、高质量地采集复杂的双手和接触操作数据，传感和姿态跟踪表现突出，具有显著的实用价值和普适潜力。

Abstract: Handheld devices have opened up unprecedented opportunities to collect
large-scale, high-quality demonstrations efficiently. However, existing systems
often lack robust tactile sensing or reliable pose tracking to handle complex
interaction scenarios, especially for bimanual and contact-rich tasks. In this
work, we propose ViTaMIn-B, a more capable and efficient handheld data
collection system for such tasks. We first design DuoTact, a novel compliant
visuo-tactile sensor built with a flexible frame to withstand large contact
forces during manipulation while capturing high-resolution contact geometry. To
enhance the cross-sensor generalizability, we propose reconstructing the
sensor's global deformation as a 3D point cloud and using it as the policy
input. We further develop a robust, unified 6-DoF bimanual pose acquisition
process using Meta Quest controllers, which eliminates the trajectory drift
issue in common SLAM-based methods. Comprehensive user studies confirm the
efficiency and high usability of ViTaMIn-B among novice and expert operators.
Furthermore, experiments on four bimanual manipulation tasks demonstrate its
superior task performance relative to existing systems.

</details>


### [327] [Fair and Safe: A Real-Time Hierarchical Control Framework for Intersections](https://arxiv.org/abs/2511.05886)
*Lei Shi,Yongju Kim,Xinzhi Zhong,Wissam Kontar,Qichao Liu,Soyoung Ahn*

Main category: cs.RO

TL;DR: 该论文提出了一种关注公平性的分层控制框架，用于联网自动驾驶车辆在路口的协调管理，并通过仿真验证其在提高公平性、消除碰撞和减少延迟方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 在车联网与自动驾驶车辆场景下路口协调时，公平性对社会接受度和系统可持续高效运作至关重要，但在安全关键的实时交通控制领域尚缺乏充分研究。

Method: 提出一个分层控制框架：顶层为集中式分配模块，最大化包含等待时间、紧急程度、控制历史和速度偏差等因素的效用函数，赋予单车控制权；底层被授权车辆采用LQR执行轨迹，并结合高阶控制障碍函数（HOCBF）实现实时避碰。

Result: 在不同流量和配比下的仿真结果显示，所提方法能够实现近乎完美的公平性，消除碰撞、降低平均延迟，并保持实时性。

Conclusion: 公平性可以系统且明确地融入自动驾驶车辆的交叉口协调管理，而不需牺牲安全性与性能，为未来可扩展和公平的自动化交通系统奠定基础。

Abstract: Ensuring fairness in the coordination of connected and automated vehicles at
intersections is essential for equitable access, social acceptance, and
long-term system efficiency, yet it remains underexplored in safety-critical,
real-time traffic control. This paper proposes a fairness-aware hierarchical
control framework that explicitly integrates inequity aversion into
intersection management. At the top layer, a centralized allocation module
assigns control authority (i.e., selects a single vehicle to execute its
trajectory) by maximizing a utility that accounts for waiting time, urgency,
control history, and velocity deviation. At the bottom layer, the authorized
vehicle executes a precomputed trajectory using a Linear Quadratic Regulator
(LQR) and applies a high-order Control Barrier Function (HOCBF)-based safety
filter for real-time collision avoidance. Simulation results across varying
traffic demands and demand distributions demonstrate that the proposed
framework achieves near-perfect fairness, eliminates collisions, reduces
average delay, and maintains real-time feasibility. These results highlight
that fairness can be systematically incorporated without sacrificing safety or
performance, enabling scalable and equitable coordination for future autonomous
traffic systems.

</details>


### [328] [From Words to Safety: Language-Conditioned Safety Filtering for Robot Navigation](https://arxiv.org/abs/2511.05889)
*Zeyuan Feng,Haimingyue Zhang,Somil Bansal*

Main category: cs.RO

TL;DR: 本文提出了一个模块化框架，使机器人能够根据自然语言指令安全地在复杂环境中导航。


<details>
  <summary>Details</summary>
Motivation: 随着机器人在现实开放环境中的普及，单靠传统的障碍规避等简单安全策略已无法满足人机交互的实际需求，需要让机器人能理解和执行更复杂的、以自然语言表达的安全约束。

Method: 作者设计了三大模块：1）基于大型语言模型的模块，将用户自由表达的语言指令转化为结构化的安全规范；2）感知模块，通过维护对象级别的三维环境表示，支撑规范落地；3）基于MPC的安全过滤器，实时执行语义和几何安全约束。

Result: 通过仿真与真实机器人实验，框架能够稳定、准确地理解并执行多种语言表达下的安全约束，并适应不同环境和场景。

Conclusion: 所提框架提升了机器人对自然语言安全指令的理解和执行能力，推动了机器人在人类环境中安全可信地工作。

Abstract: As robots become increasingly integrated into open-world, human-centered
environments, their ability to interpret natural language instructions and
adhere to safety constraints is critical for effective and trustworthy
interaction. Existing approaches often focus on mapping language to reward
functions instead of safety specifications or address only narrow constraint
classes (e.g., obstacle avoidance), limiting their robustness and
applicability. We propose a modular framework for language-conditioned safety
in robot navigation. Our framework is composed of three core components: (1) a
large language model (LLM)-based module that translates free-form instructions
into structured safety specifications, (2) a perception module that grounds
these specifications by maintaining object-level 3D representations of the
environment, and (3) a model predictive control (MPC)-based safety filter that
enforces both semantic and geometric constraints in real time. We evaluate the
effectiveness of the proposed framework through both simulation studies and
hardware experiments, demonstrating that it robustly interprets and enforces
diverse language-specified constraints across a wide range of environments and
scenarios.

</details>


### [329] [10 Open Challenges Steering the Future of Vision-Language-Action Models](https://arxiv.org/abs/2511.05936)
*Soujanya Poria,Navonil Majumder,Chia-Yu Hung,Amir Ali Bagherzadeh,Chuan Li,Kenneth Kwok,Ziwei Wang,Cheston Tan,Jiajun Wu,David Hsu*

Main category: cs.RO

TL;DR: 本论文回顾了VLA模型在多模态理解、推理、数据、评测、泛化、效率、协作等10个发展关键里程碑，并探讨空间理解、世界动态建模等发展趋势，以展望其研究前景和应用。


<details>
  <summary>Details</summary>
Motivation: 随着大模型（如LLM、VLM）取得成功，VLA模型因能理解自然语言指令，在具身智能领域越来越受关注，但其发展还面临诸多挑战和机遇。作者希望总结发展进展和挑战，促进社区关注与发展。

Method: 作者回顾并梳理了VLA模型当前在10个里程碑方向的关键进展和存在的问题；同时分析了空间理解、世界动态建模、后训练和数据合成等新趋势，提出未来研究方向。

Result: 本文明确总结了VLA模型发展的10个重要方面，并对每个方向的进展与未来挑战进行了深入讨论，特别关注加速VLA模型被更广泛接受的研究路径。

Conclusion: 作者认为，关注上述里程碑和新兴趋势，有望加速VLA模型走向实际广泛应用，为具身智能和多模态AI的发展注入动力。

Abstract: Due to their ability of follow natural language instructions,
vision-language-action (VLA) models are increasingly prevalent in the embodied
AI arena, following the widespread success of their precursors -- LLMs and
VLMs. In this paper, we discuss 10 principal milestones in the ongoing
development of VLA models -- multimodality, reasoning, data, evaluation,
cross-robot action generalization, efficiency, whole-body coordination, safety,
agents, and coordination with humans. Furthermore, we discuss the emerging
trends of using spatial understanding, modeling world dynamics, post training,
and data synthesis -- all aiming to reach these milestones. Through these
discussions, we hope to bring attention to the research avenues that may
accelerate the development of VLA models into wider acceptability.

</details>


### [330] [Robustness study of the bio-inspired musculoskeletal arm robot based on the data-driven iterative learning algorithm](https://arxiv.org/abs/2511.05995)
*Jianbo Yuan,Jing Dai,Yerui Fan,Yaxiong Wu,Yunpeng Liang,Weixin Yan*

Main category: cs.RO

TL;DR: 本研究提出了一种新型轻量级肌腱驱动肌骨臂（LTDM-Arm），采用七自由度骨骼关节系统与模块化人工肌肉系统，并通过数据驱动的迭代学习控制方法，使机械臂在模拟和实验中都展现出良好的抗扰性能和精准轨迹跟踪能力。


<details>
  <summary>Details</summary>
Motivation: 人类手臂在力量和精确性方面具备高度灵巧性，研究者希望通过仿生肌骨结构使机器人实现类似的人类操作性能，特别是在复杂和非结构化环境中的应用。

Method: 设计并实现了具备7自由度的肌骨臂（LTDM-Arm），含15个驱动的模块化人工肌肉，使用Hill型肌肉模型和数据驱动的迭代学习控制方法（DDILC），使机械臂在有限时间内不断优化重复任务的控制信号。通过仿真和实际实验测试其抗负载干扰能力。

Result: LTDM-Arm在20%仿真负载干扰和15%实际实验干扰下，依然能有效完成期望的轨迹跟踪任务，表现出优异的抗干扰性能。

Conclusion: 本研究创新性地实现了高自由度、模块化且具有人类肌骨特色的机械臂，验证了其在复杂环境中类人操作和抗干扰能力，为进一步开发高性能仿人机器人系统提供了基础。

Abstract: The human arm exhibits remarkable capabilities, including both explosive
power and precision, which demonstrate dexterity, compliance, and robustness in
unstructured environments. Developing robotic systems that emulate human-like
operational characteristics through musculoskeletal structures has long been a
research focus. In this study, we designed a novel lightweight tendon-driven
musculoskeletal arm (LTDM-Arm), featuring a seven degree-of-freedom (DOF)
skeletal joint system and a modularized artificial muscular system (MAMS) with
15 actuators. Additionally, we employed a Hilly-type muscle model and
data-driven iterative learning control (DDILC) to learn and refine activation
signals for repetitive tasks within a finite time frame. We validated the
anti-interference capabilities of the musculoskeletal system through both
simulations and experiments. The results show that the LTDM-Arm system can
effectively achieve desired trajectory tracking tasks, even under load
disturbances of 20 % in simulation and 15 % in experiments. This research lays
the foundation for developing advanced robotic systems with human-like
operational performance.

</details>


### [331] [Development and testing of novel soft sleeve actuators](https://arxiv.org/abs/2511.06102)
*Mohammed Abboodi*

Main category: cs.RO

TL;DR: 本文提出了一种新型的软套式驱动装置，能有效提高可穿戴辅助设备的动力传递与舒适性，并适合人体解剖结构。


<details>
  <summary>Details</summary>
Motivation: 随着人口老龄化和神经、肌肉骨骼疾病的增加，人们对高效、舒适和解剖兼容的可穿戴辅助设备需求增长。现有设备普遍采用刚性结构，影响受力传递与佩戴性，因此亟需开发更加柔性且高效的人体适配装置。

Method: 研究引入了三种依据软套结构的驱动器，可实现线性、弯曲与扭转三种运动，并创新性地设计了可同时实现上述动作的全向驱动器。该装置采用热塑性弹性体和专用熔融丝材3D打印工艺制造，以增强气密性与柔顺性，并通过专门实验平台测试其运动与受力性能，同时对参数如结构几何与材料性质对性能影响进行了系统研究。

Result: 结果显示，该软套驱动器能实现稳定、可重复的多轴运动，较好地将动力传递至人体，且简化了复杂的安装硬件。与传统方案相比，动力与运动表现均有提升。

Conclusion: 该研究建立了一套统一且可制造化的软套驱动技术框架，为开发更紧凑、以用户为中心、动力和运动性能更优的可穿戴辅助设备提供了新思路。

Abstract: Aging populations and the rising prevalence of neurological and
musculoskeletal disorders increase the demand for wearable mobility assistive
devices that are effective, comfortable, and anatomically compatible. Many
existing systems use rigid mechanisms and bulky interfaces that impede force
transmission and reduce wearability. This study introduces a soft sleeve
actuation architecture that conforms to the limb while transmitting forces and
moments efficiently. We develop three soft sleeve actuators that produce
linear, bending, and twisting motion, and an omnidirectional design that
combines these motions in one device. Actuators are fabricated from
thermoplastic elastomers using a customized fused filament fabrication process
that produces airtight and compliant structures and resolves leakage observed
with conventional methods. A dedicated experimental platform quantifies
kinematic outputs such as displacement, angle, and twist, and kinetic outputs
such as force and torque under low pneumatic pressures. A parametric study
varies geometric features and material properties to determine their influence
on performance. Results show reproducible multi axis motion with improved
transfer of force to the limb and reduced need for complex attachment hardware.
The work establishes a unified and manufacturable framework for soft sleeve
actuation that enables compact and user centered assistive technologies with
enhanced kinematic and kinetic performance.

</details>


### [332] [PlaCo: a QP-based robot planning and control framework](https://arxiv.org/abs/2511.06141)
*Marc Duclusaud,Grégoire Passault,Vincent Padois,Olivier Ly*

Main category: cs.RO

TL;DR: 本文介绍了PlaCo框架，旨在简化机器人系统中基于二次规划（QP）的规划与控制问题的求解流程。框架具有高级接口，支持模块化、直观的任务与约束描述，同时兼容Python和C++。


<details>
  <summary>Details</summary>
Motivation: 机器人系统的规划与控制通常涉及复杂的QP数学建模，这对开发者提出了较高门槛。为提升开发效率、降低入门难度、便于原型开发与实用部署，需有一个既高效又易用的工具。

Method: PlaCo框架通过提供高层抽象接口，屏蔽QP问题的底层数学细节，使用户可以以模块化、直观的方式定义任务和约束；同时支持Python快速原型和C++高性能计算。

Result: PlaCo框架实现了用户友好性与高性能的结合，提高了QP问题建模和解决的效率、便捷性和适用面。

Conclusion: PlaCo为机器人领域的科研与工程人员提供了高效、易用的QP问题规划与控制平台，有助于加速相关研究与应用落地。

Abstract: This article introduces PlaCo, a software framework designed to simplify the
formulation and solution of Quadratic Programming (QP)-based planning and
control problems for robotic systems. PlaCo provides a high-level interface
that abstracts away the low-level mathematical formulation of QP problems,
allowing users to specify tasks and constraints in a modular and intuitive
manner. The framework supports both Python bindings for rapid prototyping and a
C++ implementation for real-time performance.

</details>


### [333] [OpenVLN: Open-world aerial Vision-Language Navigation](https://arxiv.org/abs/2511.06182)
*Peican Lin,Gan Sun,Chenxi Liu,Fazeng Li,Weihong Ren,Yang Cong*

Main category: cs.RO

TL;DR: 本文提出了一种高效的数据利用的新框架OpenVLN，用于提升无人机在复杂低数据环境下基于视觉-语言的导航能力，在长距离规划任务上有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着视觉-语言模型（VLMs）在地面导航中的广泛应用，作者注意到当前无人机在户外复杂环境下导航需要解决数据采集难和长期轨迹规划难等新问题，因此有必要开发适用于航拍场景的高效VLM方法。

Method: 作者设计了OpenVLN框架，在有限训练数据下，通过优化强化学习架构和规则驱动策略高效微调VLM；同时引入基于价值奖励的长距离路径规划器生成高精度无人机行动序列。

Result: 在TravelUAV基准上的实验表明，该方法在不同数据集和奖励设置下，成功率提升4.34%，Oracle成功率提升6.19%，路径加权成功率提升4.07%，均优于现有基线。

Conclusion: OpenVLN显著提高了复杂航拍环境下无人机的远距离视觉-语言导航效率和能力，为相关实际部署提供了有效解决方案。

Abstract: Vision-language models (VLMs) have been widely-applied in ground-based
vision-language navigation (VLN). However, the vast complexity of outdoor
aerial environments compounds data acquisition challenges and imposes
long-horizon trajectory planning requirements on Unmanned Aerial Vehicles
(UAVs), introducing novel complexities for aerial VLN. To address these
challenges, we propose a data-efficient Open-world aerial Vision-Language
Navigation (i.e., OpenVLN) framework, which could execute language-guided
flight with limited data constraints and enhance long-horizon trajectory
planning capabilities in complex aerial environments. Specifically, we
reconfigure a reinforcement learning framework to optimize the VLM for UAV
navigation tasks, which can efficiently fine-tune VLM by using rule-based
policies under limited training data. Concurrently, we introduce a long-horizon
planner for trajectory synthesis that dynamically generates precise UAV actions
via value-based rewards. To the end, we conduct sufficient navigation
experiments on the TravelUAV benchmark with dataset scaling across diverse
reward settings. Our method demonstrates consistent performance gains of up to
4.34% in Success Rate, 6.19% in Oracle Success Rate, and 4.07% in Success
weighted by Path Length over baseline methods, validating its deployment
efficacy for long-horizon UAV navigation in complex aerial environments.

</details>


### [334] [ExpReS-VLA: Specializing Vision-Language-Action Models Through Experience Replay and Retrieval](https://arxiv.org/abs/2511.06202)
*Shahram Najam Syed,Yatharth Ahuja,Arthur Jakobsson,Jeff Ichnowski*

Main category: cs.RO

TL;DR: ExpReS-VLA提出了一种高效适配新环境的方法，用于提升视觉-语言-动作模型在有限任务集下的机器人操作表现，同时避免遗忘，并极大压缩了内存需求，在仿真与实物测试中均大幅提升了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的Vision-Language-Action模型尽管具备广泛的零样本泛化能力，但在实际部署时，模型更需要对有限任务持续高效表现而非广泛泛化。当前方法在新环境适应时往往效率低且易遗忘已学知识，难以满足实际需求。

Method: ExpReS-VLA通过经验回放与检索机制，专注于对预训练VLA模型的适配。其核心包括：1)仅存储视觉主干输出的紧凑特征，极大减少内存占用；2)通过余弦相似度检索相关历史经验引导适应；3)采用优先经验回放强调成功轨迹学习；4)利用阈值混合对比损失函数，兼顾成功与失败经验进行训练。

Result: 在LIBERO仿真测试中，ExpReS-VLA空间推理任务成功率从82.6%提升至93.1%，长程任务从61%提升至72.3%。物理机器人五项操作测试中，在见过和未见过环境分别达到98%的成功率，显著优于普通微调方法（分别为84.7%和32%）。在单张RTX 5090 GPU和12条演示下，适应过程仅需31秒。

Conclusion: ExpReS-VLA显著提升了VLA模型在特定任务与环境下的适应与表现，兼顾高效适应和防遗忘，资源占用极低，具备实际机器人部署的实用价值。

Abstract: Vision-Language-Action models such as OpenVLA show impressive zero-shot
generalization across robotic manipulation tasks but often fail to adapt
efficiently to new deployment environments. In many real-world applications,
consistent high performance on a limited set of tasks is more important than
broad generalization. We propose ExpReS-VLA, a method for specializing
pre-trained VLA models through experience replay and retrieval while preventing
catastrophic forgetting. ExpReS-VLA stores compact feature representations from
the frozen vision backbone instead of raw image-action pairs, reducing memory
usage by approximately 97 percent. During deployment, relevant past experiences
are retrieved using cosine similarity and used to guide adaptation, while
prioritized experience replay emphasizes successful trajectories. We also
introduce Thresholded Hybrid Contrastive Loss, which enables learning from both
successful and failed attempts. On the LIBERO simulation benchmark, ExpReS-VLA
improves success rates from 82.6 to 93.1 percent on spatial reasoning tasks and
from 61 to 72.3 percent on long-horizon tasks. On physical robot experiments
with five manipulation tasks, it reaches 98 percent success on both seen and
unseen settings, compared to 84.7 and 32 percent for naive fine-tuning.
Adaptation takes 31 seconds using 12 demonstrations on a single RTX 5090 GPU,
making the approach practical for real robot deployment.

</details>


### [335] [Affordance-Guided Coarse-to-Fine Exploration for Base Placement in Open-Vocabulary Mobile Manipulation](https://arxiv.org/abs/2511.06240)
*Tzu-Jung Lin,Jia-Fong Yeh,Hung-Ting Su,Chung-Yi Lin,Yi-Ting Chen,Winston H. Hsu*

Main category: cs.RO

TL;DR: 本文提出了一种用于开放词汇移动操作（OVMM）的零样本底座放置框架，通过结合视觉-语言模型的语义理解和几何可行性，提升了机器人操作的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的OVMM方法通常只根据距离选择机器人的位置，未考虑操作对象的可供性（affordance），导致任务频繁失败。作者希望通过引入语义与空间对齐的方法，提高操作的成功率和泛化能力。

Method: 提出了一种“可供性引导的粗到细探索”框架：首先利用视觉-语言模型（VLMs）获得粗略的语义先验，引导机器人关注任务相关区域，再结合利用几何约束进行细致优化。方法中设计了跨模态表示（Affordance RGB和Obstacle Map+），将语义与实际空间信息结合。整个过程为零样本（zero-shot）优化，无需针对任务单独训练模型。

Result: 该方法在五个多样化的OVMM任务中验证，平均操作成功率为85%，显著优于传统几何规划器和单纯基于VLM的方法。

Conclusion: 成果表明，可供性感知和多模态推理有助于实现更强泛化性和指令响应能力的移动操作机器人任务规划。该框架为OVMM中的通用可扩展规划展示了很大潜力。

Abstract: In open-vocabulary mobile manipulation (OVMM), task success often hinges on
the selection of an appropriate base placement for the robot. Existing
approaches typically navigate to proximity-based regions without considering
affordances, resulting in frequent manipulation failures. We propose
Affordance-Guided Coarse-to-Fine Exploration, a zero-shot framework for base
placement that integrates semantic understanding from vision-language models
(VLMs) with geometric feasibility through an iterative optimization process.
Our method constructs cross-modal representations, namely Affordance RGB and
Obstacle Map+, to align semantics with spatial context. This enables reasoning
that extends beyond the egocentric limitations of RGB perception. To ensure
interaction is guided by task-relevant affordances, we leverage coarse semantic
priors from VLMs to guide the search toward task-relevant regions and refine
placements with geometric constraints, thereby reducing the risk of convergence
to local optima. Evaluated on five diverse open-vocabulary mobile manipulation
tasks, our system achieves an 85% success rate, significantly outperforming
classical geometric planners and VLM-based methods. This demonstrates the
promise of affordance-aware and multimodal reasoning for generalizable,
instruction-conditioned planning in OVMM.

</details>


### [336] [Robust Differentiable Collision Detection for General Objects](https://arxiv.org/abs/2511.06267)
*Jiayi Chen,Wei Zhao,Liangwang Ruan,Baoquan Chen,He Wang*

Main category: cs.RO

TL;DR: 本文提出了一种鲁棒高效的可微分碰撞检测框架，支持复杂几何体，显著优于现有方法，并能直接用于抓取任务优化。


<details>
  <summary>Details</summary>
Motivation: 传统的碰撞检测算法如GJK+EPA虽然能准确计算最近点或穿透点，但由于不可微分，限制了其在需要梯度的优化任务（如机器人抓取与操作）中的应用。现有的可微分方法对几何体类型有限，且在复杂形状上表现不佳，因此亟需更通用、鲁棒且高效的可微分碰撞检测方法。

Method: 提出了一种基于距离的随机平滑（randomized smoothing）、自适应采样以及等价梯度传递的新方法，使得碰撞检测对梯度更敏感且适用于复杂的凸/凹体。该方法实现了高效、鲁棒的可微分最近点与穿透点计算。

Result: 在DexGraspNet和Objaverse上的复杂网格实验显示，本文方法在准确性与效率上显著优于现有可微分碰撞检测基线。同时，实验验证了该方法可直接用于提升精细抓取任务的质量。

Conclusion: 本研究提出的方法大大拓展了可微分碰撞检测的应用范围与实际效用，为机器人抓取与操控等接触密集型任务提供了强有力的优化工具。

Abstract: Collision detection is a core component of robotics applications such as
simulation, control, and planning. Traditional algorithms like GJK+EPA compute
witness points (i.e., the closest or deepest-penetration pairs between two
objects) but are inherently non-differentiable, preventing gradient flow and
limiting gradient-based optimization in contact-rich tasks such as grasping and
manipulation. Recent work introduced efficient first-order randomized smoothing
to make witness points differentiable; however, their direction-based
formulation is restricted to convex objects and lacks robustness for complex
geometries. In this work, we propose a robust and efficient differentiable
collision detection framework that supports both convex and concave objects
across diverse scales and configurations. Our method introduces distance-based
first-order randomized smoothing, adaptive sampling, and equivalent gradient
transport for robust and informative gradient computation. Experiments on
complex meshes from DexGraspNet and Objaverse show significant improvements
over existing baselines. Finally, we demonstrate a direct application of our
method for dexterous grasp synthesis to refine the grasp quality. The code is
available at https://github.com/JYChen18/DiffCollision.

</details>


### [337] [External Photoreflective Tactile Sensing Based on Surface Deformation Measurement](https://arxiv.org/abs/2511.06311)
*Seiichi Yamamoto,Hiroki Ishizuka,Takumi Kawasetsu,Koh Hosoda,Takayuki Kameoka,Kango Yanagida,Takato Horii,Sei Ikeda,Osamu Oshiro*

Main category: cs.RO

TL;DR: 该论文提出了一种可外部附加的光反射式模块，用于通过软体机器人表面的形变来感知触觉，无需在皮肤内部嵌入传感器。


<details>
  <summary>Details</summary>
Motivation: 现有的软体机器人触觉传感通常需要在皮肤内部嵌入液体或电线，这导致复杂度高、易损坏且柔软性降低。作者希望实现一种既能感知力，又能减少制造与维护复杂性的触觉解决方案。

Method: 通过将光反射式传感模块外置，读取硅胶皮肤表面的形变量来估算接触力，避免了传感器直接暴露于接触面。系统设计包括对光学元件和皮肤力学性能的表征，并通过压缩实验验证了传感输出的单调性、低迟滞和高重复性。还将传感模块集成到软体机器人夹爪上，实现了抓取事件的识别。

Result: 该传感方法实验表现出与理论相符的单调力输出、低迟滞和高重复性，对多次压缩和不同速度均有良好响应。模块化结构比传统的液体或线材传感皮肤更耐用，布线简洁，易于适配多种机器人。

Conclusion: 提出的基于表面形变的外部光学模块，为软体机器人提供了一种实用且鲁棒的触觉感知途径，极大提升了系统柔软性和可制造性，有望促进机器人安全协作和应用场景扩展。

Abstract: We present a tactile sensing method enabled by the mechanical compliance of
soft robots; an externally attachable photoreflective module reads surface
deformation of silicone skin to estimate contact force without embedding
tactile transducers. Locating the sensor off the contact interface reduces
damage risk, preserves softness, and simplifies fabrication and maintenance. We
first characterize the optical sensing element and the compliant skin,
thendetermine the design of a prototype tactile sensor. Compression experiments
validate the approach, exhibiting a monotonic force output relationship
consistent with theory, low hysteresis, high repeatability over repeated
cycles, and small response indentation speeds. We further demonstrate
integration on a soft robotic gripper, where the module reliably detects grasp
events. Compared with liquid filled or wireembedded tactile skins, the proposed
modular add on architecture enhances durability, reduces wiring complexity, and
supports straightforward deployment across diverse robot geometries. Because
the sensing principle reads skin strain patterns, it also suggests extensions
to other somatosensory cues such as joint angle or actuator state estimation
from surface deformation. Overall, leveraging surface compliance with an
external optical module provides a practical and robust route to equip soft
robots with force perception while preserving structural flexibility and
manufacturability, paving the way for robotic applications and safe human robot
collaboration.

</details>


### [338] [Towards Adaptive Humanoid Control via Multi-Behavior Distillation and Reinforced Fine-Tuning](https://arxiv.org/abs/2511.06371)
*Yingnan Zhao,Xinmiao Wang,Dewei Wang,Xinzhe Liu,Dan Lu,Qilong Han,Peng Liu,Chenjia Bai*

Main category: cs.RO

TL;DR: 本文提出了一种自适应类人机器人控制方法（AHC），通过两阶段框架实现机器人在不同技能和地形下的高适应性移动，优于以往需分别训练的单技能策略。


<details>
  <summary>Details</summary>
Motivation: 目前类人机器人在行走、奔跑等运动技能方面大有前景，但主流方法需对每项技能单独训练，泛化性差，面对复杂环境时表现不佳，因此需要一种能跨多技能与地形的自适应控制方法。

Method: 首先训练多个基础移动策略，通过多行为蒸馏过程获得基础多行为控制器，实现环境驱动的行为切换。随后通过在多样地形上收集在线反馈进行强化微调，提升控制器的地形适应能力。

Result: 在仿真和真实Unitree G1机器人实验中，所提方法展现了在多种情境和地形上的强适应性。

Conclusion: AHC实现了类人机器人跨多技能和地形的高适应性运动控制，显著提升了泛化能力和实地表现，有望推动实际应用。

Abstract: Humanoid robots are promising to learn a diverse set of human-like locomotion
behaviors, including standing up, walking, running, and jumping. However,
existing methods predominantly require training independent policies for each
skill, yielding behavior-specific controllers that exhibit limited
generalization and brittle performance when deployed on irregular terrains and
in diverse situations. To address this challenge, we propose Adaptive Humanoid
Control (AHC) that adopts a two-stage framework to learn an adaptive humanoid
locomotion controller across different skills and terrains. Specifically, we
first train several primary locomotion policies and perform a multi-behavior
distillation process to obtain a basic multi-behavior controller, facilitating
adaptive behavior switching based on the environment. Then, we perform
reinforced fine-tuning by collecting online feedback in performing adaptive
behaviors on more diverse terrains, enhancing terrain adaptability for the
controller. We conduct experiments in both simulation and real-world
experiments in Unitree G1 robots. The results show that our method exhibits
strong adaptability across various situations and terrains. Project website:
https://ahc-humanoid.github.io.

</details>


### [339] [ArtReg: Visuo-Tactile based Pose Tracking and Manipulation of Unseen Articulated Objects](https://arxiv.org/abs/2511.06378)
*Prajval Kumar Murali,Mohsen Kaboli*

Main category: cs.RO

TL;DR: 本文提出ArtReg方法，利用视觉和触觉点云融合及SE(3)李群上的无迹卡尔曼滤波，实现未知复杂关节物体的无先验姿态跟踪与操控，性能超越当前技术。


<details>
  <summary>Details</summary>
Motivation: 机器人在现实环境操作时，经常遇到结构复杂或带关节的未知物体，如何在无先验信息的情况下可靠感知、跟踪并操控这些物体，是机器人领域的核心难题。

Method: 作者提出ArtReg（Articulated Registration）方法，将视触点云数据整合到SE(3)李群空间下的无迹卡尔曼滤波框架，实现点云配准与关节检测。通过双机器人团队对物体施加推拉等动作，揭示并跟踪物体潜在关节，并设计闭环控制器，实现在目标位姿下的主动操控。

Result: 通过多种真实机器人实验，ArtReg成功在不同类型、质量分布、光照和背景条件下，对未知关节物体进行鲁棒、准确的姿态跟踪与控制。并在标准基准数据集上在姿态精度等指标上超越现有方法。

Conclusion: 基于视触信息的ArtReg系统能显著提升机器人对未知复杂结构和关节物体的感知和交互能力，为无先验知识条件下的机器人操作提供了有效解决方案。

Abstract: Robots operating in real-world environments frequently encounter unknown
objects with complex structures and articulated components, such as doors,
drawers, cabinets, and tools. The ability to perceive, track, and manipulate
these objects without prior knowledge of their geometry or kinematic properties
remains a fundamental challenge in robotics. In this work, we present a novel
method for visuo-tactile-based tracking of unseen objects (single, multiple, or
articulated) during robotic interaction without assuming any prior knowledge
regarding object shape or dynamics. Our novel pose tracking approach termed
ArtReg (stands for Articulated Registration) integrates visuo-tactile point
clouds in an unscented Kalman Filter formulation in the SE(3) Lie Group for
point cloud registration. ArtReg is used to detect possible articulated joints
in objects using purposeful manipulation maneuvers such as pushing or
hold-pulling with a two-robot team. Furthermore, we leverage ArtReg to develop
a closed-loop controller for goal-driven manipulation of articulated objects to
move the object into the desired pose configuration. We have extensively
evaluated our approach on various types of unknown objects through real robot
experiments. We also demonstrate the robustness of our method by evaluating
objects with varying center of mass, low-light conditions, and with challenging
visual backgrounds. Furthermore, we benchmarked our approach on a standard
dataset of articulated objects and demonstrated improved performance in terms
of pose accuracy compared to state-of-the-art methods. Our experiments indicate
that robust and accurate pose tracking leveraging visuo-tactile information
enables robots to perceive and interact with unseen complex articulated objects
(with revolute or prismatic joints).

</details>


### [340] [From Demonstrations to Safe Deployment: Path-Consistent Safety Filtering for Diffusion Policies](https://arxiv.org/abs/2511.06385)
*Ralf Römer,Julian Balletshofer,Jakob Thumm,Marco Pavone,Angela P. Schoellig,Matthias Althoff*

Main category: cs.RO

TL;DR: 本文提出了一种针对扩散策略（DPs）的路径一致安全过滤（PACS）方法，确保机器人在执行任务时兼顾安全与表现，显著优于以往安全机制。


<details>
  <summary>Details</summary>
Motivation: 当前扩散策略虽在复杂操控任务中实现了最先进表现，但缺乏安全保障，传统的外部安全机制在测试时又会导致行为失常和性能下降。

Method: 提出了一种路径一致安全过滤方法（PACS），在扩散策略产生的整条动作序列上应用一致性刹车，并结合集值可达性分析以实时验证安全性，从而确保行为与训练分布一致。

Result: 在仿真和三项真实世界的人机交互任务中实验，PACS能提供动态环境下的正式安全保证，保持任务完成率，并在任务成功率上最多提升68%，优于反应型的安全机制如控制障碍函数。

Conclusion: PACS不仅大幅提升了扩散策略在安全关键场景下的可靠性，还保留了其高效的任务执行能力，显著改善了现有安全机制的局限。

Abstract: Diffusion policies (DPs) achieve state-of-the-art performance on complex
manipulation tasks by learning from large-scale demonstration datasets, often
spanning multiple embodiments and environments. However, they cannot guarantee
safe behavior, so external safety mechanisms are needed. These, however, alter
actions in ways unseen during training, causing unpredictable behavior and
performance degradation. To address these problems, we propose path-consistent
safety filtering (PACS) for DPs. Our approach performs path-consistent braking
on a trajectory computed from the sequence of generated actions. In this way,
we keep execution consistent with the policy's training distribution,
maintaining the learned, task-completing behavior. To enable a real-time
deployment and handle uncertainties, we verify safety using set-based
reachability analysis. Our experimental evaluation in simulation and on three
challenging real-world human-robot interaction tasks shows that PACS (a)
provides formal safety guarantees in dynamic environments, (b) preserves task
success rates, and (c) outperforms reactive safety approaches, such as control
barrier functions, by up to 68% in terms of task success. Videos are available
at our project website: https://tum-lsy.github.io/pacs/.

</details>


### [341] [Whole-Body Control With Terrain Estimation of A 6-DoF Wheeled Bipedal Robot](https://arxiv.org/abs/2511.06397)
*Cong Wen,Yunfei Li,Kexin Liu,Yixin Qiu,Xuanhong Liao,Tianyu Wang,Dingchuan Liu,Tao Zhang,Ximin Lyu*

Main category: cs.RO

TL;DR: 该论文提出了一种新型六自由度轮式双足机器人，重点在于完善动力学建模、地形估算与全身控制框架，实现了机器人在崎岖地形上的稳定运动和良好通过性。


<details>
  <summary>Details</summary>
Motivation: 目前轮式双足机器人常常忽略腿部动力学，导致运动能力无法完全发挥，并且在不平坦地形上运动存在挑战。为了解决这一问题，作者希望建立更全面的动力学模型，并提升机器人的地形适应能力。

Method: 作者提出了包含闭环动力学和基于地面法向量的接触模型的完整动力学建模方法，利用LiDAR惯性里程计和PCA改进法做地形估算，并结合PD和LQR方法用于姿态与质心动力学平衡控制，最后采用分层优化方法求解全身控制，实现运动规划。

Result: 通过仿真和实物实验，验证了地形估计算法、控制方法在不平坦地表的实际有效性与鲁棒性，机器人成功通过复杂地形。

Conclusion: 新型6自由度轮式双足机器人结合创新地形估算与全身动力学控制，有效提升了复杂地形上的运动能力，为轮足机器人实地应用提供了方法支撑。

Abstract: Wheeled bipedal robots have garnered increasing attention in exploration and
inspection. However, most research simplifies calculations by ignoring leg
dynamics, thereby restricting the robot's full motion potential. Additionally,
robots face challenges when traversing uneven terrain. To address the
aforementioned issue, we develop a complete dynamics model and design a
whole-body control framework with terrain estimation for a novel 6 degrees of
freedom wheeled bipedal robot. This model incorporates the closed-loop dynamics
of the robot and a ground contact model based on the estimated ground normal
vector. We use a LiDAR inertial odometry framework and improved Principal
Component Analysis for terrain estimation. Task controllers, including PD
control law and LQR, are employed for pose control and centroidal
dynamics-based balance control, respectively. Furthermore, a hierarchical
optimization approach is used to solve the whole-body control problem. We
validate the performance of the terrain estimation algorithm and demonstrate
the algorithm's robustness and ability to traverse uneven terrain through both
simulation and real-world experiments.

</details>


### [342] [Real Garment Benchmark (RGBench): A Comprehensive Benchmark for Robotic Garment Manipulation featuring a High-Fidelity Scalable Simulator](https://arxiv.org/abs/2511.06434)
*Wenkang Hu,Xincheng Tang,Yanzhi E,Yitong Li,Zhengjie Shu,Wei Li,Huamin Wang,Ruigang Yang*

Main category: cs.RO

TL;DR: 本文提出了RGBench，这是一个专为机器人操作衣物设计的基准测试套件，包括6000多个衣物网格模型、高性能模拟器和详细评估协议，显著提升了布料模拟的准确性和速度。


<details>
  <summary>Details</summary>
Motivation: 对于刚体物体的机器人操作已经取得了很大进展，但在可变形物体（如衣物）的操作上，由于缺乏合适的模型和真实的模拟器，发展受限。为推动该领域发展，需要高质量的基准和工具。

Method: 作者开发了RGBench，内容包括：1）一个包含6000余种衣物网格的多样化模型集；2）一款高性能衣物模拟器；3）基于实际衣物动态严格测量的模拟质量评估协议。然后，将他们的模拟器与现有布料模拟器进行了实验性对比。

Result: 实验结果显示，RGBench的模拟器将布料模拟误差降低了20%，且速度提升了3倍，明显优于现有模拟工具。

Conclusion: RGBench能够极大推动机器人衣物操作研究的发展，作者也承诺开源该工具，方便学界进一步创新。

Abstract: While there has been significant progress to use simulated data to learn
robotic manipulation of rigid objects, applying its success to deformable
objects has been hindered by the lack of both deformable object models and
realistic non-rigid body simulators. In this paper, we present Real Garment
Benchmark (RGBench), a comprehensive benchmark for robotic manipulation of
garments. It features a diverse set of over 6000 garment mesh models, a new
high-performance simulator, and a comprehensive protocol to evaluate garment
simulation quality with carefully measured real garment dynamics. Our
experiments demonstrate that our simulator outperforms currently available
cloth simulators by a large margin, reducing simulation error by 20% while
maintaining a speed of 3 times faster. We will publicly release RGBench to
accelerate future research in robotic garment manipulation. Website:
https://rgbench.github.io/

</details>


### [343] [Sim-to-Real Transfer in Deep Reinforcement Learning for Bipedal Locomotion](https://arxiv.org/abs/2511.06465)
*Lingfan Bao,Tianhu Peng,Chengxu Zhou*

Main category: cs.RO

TL;DR: 本章讨论了深度强化学习在双足行走中“仿真到现实”迁移的关键挑战，并总结了缩小仿真与现实差距的系统性方法。


<details>
  <summary>Details</summary>
Motivation: 仿真与现实之间的差距大大影响了深度强化学习在实际机器人双足行走中的表现，解决这一痛点对于机器人控制的落地至关重要。

Method: 首先分析了仿真与现实之间差距的主要来源，包括机器人动力学、接触建模、状态估计及数值求解器等。随后，提出两大类方案：一是提升仿真器的物理精确性；二是通过鲁棒性训练和部署后自适应提升策略对模型误差的耐受性。

Result: 系统梳理了上述方法在缩小仿真到现实迁移差距中的作用和效果，展示了两大手段的互补性。

Conclusion: 通过结合模型改进和策略鲁棒性训练，构建出一套具有指导意义的仿真到现实迁移解决路线，为后续研究和实践提供了策略框架。

Abstract: This chapter addresses the critical challenge of simulation-to-reality
(sim-to-real) transfer for deep reinforcement learning (DRL) in bipedal
locomotion. After contextualizing the problem within various control
architectures, we dissect the ``curse of simulation'' by analyzing the primary
sources of sim-to-real gap: robot dynamics, contact modeling, state estimation,
and numerical solvers. Building on this diagnosis, we structure the solutions
around two complementary philosophies. The first is to shrink the gap through
model-centric strategies that systematically improve the simulator's physical
fidelity. The second is to harden the policy, a complementary approach that
uses in-simulation robustness training and post-deployment adaptation to make
the policy inherently resilient to model inaccuracies. The chapter concludes by
synthesizing these philosophies into a strategic framework, providing a clear
roadmap for developing and evaluating robust sim-to-real solutions.

</details>


### [344] [A Low-Rank Method for Vision Language Model Hallucination Mitigation in Autonomous Driving](https://arxiv.org/abs/2511.06496)
*Keke Long,Jiacheng Guo,Tianyun Zhang,Hongkai Yu,Xiaopeng Li*

Main category: cs.RO

TL;DR: 本论文提出了一种新颖的自包含低秩方法，无需外部参考或访问模型内部，即可自动筛选出视觉语言模型生成的候选描述中幻觉最少的结果。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在自动驾驶中应用广泛，但由于有时会产生幻觉（即与视觉内容无关的虚假细节），在没有真实参照和不可访问模型内部信息时，如何检测和减少这些幻觉成为一项重要且有挑战性的任务。

Method: 作者通过构建候选描述的句子嵌入矩阵，并将其分解为低秩一致性部分和稀疏残差部分，利用稀疏残差大小对描述进行排序，选择残差最小的视为幻觉最少的描述。该方法无需外部参照，也不需要访问VLM内部信息，仅依赖生成的描述文本本身。

Result: 在NuScenes数据集上的实验表明，提出的方法在挑选最少幻觉描述的准确率达87%，较未经筛选的基线提升19%，较多代理辩论法提升6%-10%。

Conclusion: 该方法不仅有效地检测并排序幻觉水平，还能极大提升推理效率（比辩论法快51%-67%），与人工评判结果高度一致，适合自动驾驶等对时效性有要求的实际场景中应用。

Abstract: Vision Language Models (VLMs) are increasingly used in autonomous driving to
help understand traffic scenes, but they sometimes produce hallucinations,
which are false details not grounded in the visual input. Detecting and
mitigating hallucinations is challenging when ground-truth references are
unavailable and model internals are inaccessible. This paper proposes a novel
self-contained low-rank approach to automatically rank multiple candidate
captions generated by multiple VLMs based on their hallucination levels, using
only the captions themselves without requiring external references or model
access. By constructing a sentence-embedding matrix and decomposing it into a
low-rank consensus component and a sparse residual, we use the residual
magnitude to rank captions: selecting the one with the smallest residual as the
most hallucination-free. Experiments on the NuScenes dataset demonstrate that
our approach achieves 87% selection accuracy in identifying hallucination-free
captions, representing a 19% improvement over the unfiltered baseline and a
6-10% improvement over multi-agent debate method. The sorting produced by
sparse error magnitudes shows strong correlation with human judgments of
hallucinations, validating our scoring mechanism. Additionally, our method,
which can be easily parallelized, reduces inference time by 51-67% compared to
debate approaches, making it practical for real-time autonomous driving
applications.

</details>


### [345] [Adaptive PID Control for Robotic Systems via Hierarchical Meta-Learning and Reinforcement Learning with Physics-Based Data Augmentation](https://arxiv.org/abs/2511.06500)
*JiaHao Wu,ShengWen Yu*

Main category: cs.RO

TL;DR: 本文提出了一种结合元学习和强化学习的分层控制框架，用于自动初始化和自适应调优机器人中的PID控制器。通过基于物理的增广，模型可以用较少的实际数据实现高效的泛化和自适应能力。该方法在两种不同机器人平台上均取得显著性能提升，尤其在高负载关节表现突出。还发现了强化学习改善受限于元学习表现的“优化天花板效应”。


<details>
  <summary>Details</summary>
Motivation: 现有工业机器人普遍采用PID控制器，但参数调节依赖人工，费时且需要丰富经验。随着机器人多样化和场景复杂化，传统参数整定难以高效满足多平台需求，因此需要一种自动化且普适的PID参数优化方法。

Method: 采用基于元学习机制自动初始化PID参数，并引入强化学习进行在线自适应。为提升样本效率，通过物理参数扰动方式生成虚拟机器人数据用于增广，从而减小了对实际数据的依赖。方法在9自由度操作臂和12自由度四足机器人上进行实证评估。

Result: 在Franka Panda机器人上，整体性能提升平均16.6%，高负载关节（如J2）提升高达80.4%，误差大幅下降。对Laikago机器人，若元学习基线表现已好，后续强化学习无法继续带来提升（0%收益）。系统在参数不确定和干扰条件下表现依然稳健，且训练时间仅需10分钟。多次随机实验（100次）下性能也很稳定。

Conclusion: 本文的方法能显著改善机器人PID控制器自动化参数优化，并且强化学习对系统性能的提升需以元学习质量为前提。工作为分层控制系统设计和优化提供了理论及实践指导，同时指出了强化学习存在“优化天花板效应”的重要限制。

Abstract: Proportional-Integral-Derivative (PID) controllers remain the predominant
choice in industrial robotics due to their simplicity and reliability. However,
manual tuning of PID parameters for diverse robotic platforms is time-consuming
and requires extensive domain expertise. This paper presents a novel
hierarchical control framework that combines meta-learning for PID
initialization and reinforcement learning (RL) for online adaptation. To
address the sample efficiency challenge, a \textit{physics-based data
augmentation} strategy is introduced that generates virtual robot
configurations by systematically perturbing physical parameters, enabling
effective meta-learning with limited real robot data. The proposed approach is
evaluated on two heterogeneous platforms: a 9-DOF Franka Panda manipulator and
a 12-DOF Laikago quadruped robot. Experimental results demonstrate that the
proposed method achieves 16.6\% average improvement on Franka Panda (6.26{\deg}
MAE), with exceptional gains in high-load joints (J2: 80.4\% improvement from
12.36{\deg} to 2.42{\deg}). Critically, this work discovers the
\textit{optimization ceiling effect}: RL achieves dramatic improvements when
meta-learning exhibits localized high-error joints, but provides no benefit
(0.0\%) when baseline performance is uniformly strong, as observed in Laikago.
The method demonstrates robust performance under disturbances (parameter
uncertainty: +19.2\%, no disturbance: +16.6\%, average: +10.0\%) with only 10
minutes of training time. Multi-seed analysis across 100 random initializations
confirms stable performance (4.81+/-1.64\% average). These results establish
that RL effectiveness is highly dependent on meta-learning baseline quality and
error distribution, providing important design guidance for hierarchical
control systems.

</details>


### [346] [Koopman global linearization of contact dynamics for robot locomotion and manipulation enables elaborate control](https://arxiv.org/abs/2511.06515)
*Cormac O'Neill,Jasmine Terrones,H. Harry Asada*

Main category: cs.RO

TL;DR: 本文提出了使用Koopman算子，将因接触变化导致的分段动力学统一为全局线性模型，有效简化了涉及接触的机器人控制优化问题。方法实现了足式机器人和操纵器的高效实时控制，对其他领域也有潜力。


<details>
  <summary>Details</summary>
Motivation: 机器人在与环境动态接触（如足式行走、物体抓取等）时，由于接触边界的动力学切换，控制变得非常困难，现有预测控制器因此遇到非凸优化难题，亟需有效方法简化建模和控制。

Method: 作者利用Koopman算子将机器人与环境接触带来的分段非线性动力学映射至嵌入空间中的统一线性模型，通过黏弹性接触建模确保对控制输入无近似假设。基于该模型，结合凸优化实现模型预测控制（MPC）和实时控制。

Result: 方法在足式机器人上实现了凸MPC控制，在动态推动任务中实现了操纵器的实时控制。实验证明该方法能让机器人在多次接触切换下实时获得复杂的控制策略。

Conclusion: 通过Koopman算子实现动力学统一建模，有效处理了带接触机器人的控制难题，提升了实时性和控制质量，该方法对机器人及更广泛领域控制系统具有推广意义。

Abstract: Controlling robots that dynamically engage in contact with their environment
is a pressing challenge. Whether a legged robot making-and-breaking contact
with a floor, or a manipulator grasping objects, contact is everywhere.
Unfortunately, the switching of dynamics at contact boundaries makes control
difficult. Predictive controllers face non-convex optimization problems when
contact is involved. Here, we overcome this difficulty by applying Koopman
operators to subsume the segmented dynamics due to contact changes into a
unified, globally-linear model in an embedding space. We show that viscoelastic
contact at robot-environment interactions underpins the use of Koopman
operators without approximation to control inputs. This methodology enables the
convex Model Predictive Control of a legged robot, and the real-time control of
a manipulator engaged in dynamic pushing. In this work, we show that our method
allows robots to discover elaborate control strategies in real-time over time
horizons with multiple contact changes, and the method is applicable to broad
fields beyond robotics.

</details>


### [347] [CoFineLLM: Conformal Finetuning of LLMs for Language-Instructed Robot Planning](https://arxiv.org/abs/2511.06575)
*Jun Wang,Yevgeniy Vorobeychik,Yiannis Kantaros*

Main category: cs.RO

TL;DR: LLM 在任务规划中可靠性受限，Conformal Prediction（CP）虽可提升可信度但带来过多人工干预。本文提出 CoFineLLM，通过引入 CP 感知微调框架，显著减少了 LLM 输出的预测集大小，降低人工参与频率，并在多种机器人规划任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）被应用于机器人/代理任务规划，但在高准确率的要求下，现有结合 Conformal Prediction 的方法产生过大预测集，导致过于频繁的人类干预，降低系统自主性。急需一种方法在保障可信度的同时，减少人工参与。

Method: 提出了一种 CP 感知的微调方法 CoFineLLM，使 LLM 在微调阶段意识到预测集概念，通过优化损失函数直接最小化预测集大小，同时保持高置信度。方法在多语言指令机器人规划任务上进行评估，并与主流微调/不确定性感知方法对比。

Result: CoFineLLM 显著缩小了预测集尺寸，降低了人工帮助率，在不同机器人规划任务中均优于对比方法。其在硬件实验中也展示了对分布外任务的强鲁棒性。

Conclusion: 将 CP 概念融入 LLM 微调，可提升 LLM 在规划任务上的自主性和可靠性，减少用户参与，且具备较强泛化和工程实用性。

Abstract: Large Language Models (LLMs) have recently emerged as planners for
language-instructed agents, generating sequences of actions to accomplish
natural language tasks. However, their reliability remains a challenge,
especially in long-horizon tasks, since they often produce overconfident yet
wrong outputs. Conformal Prediction (CP) has been leveraged to address this
issue by wrapping LLM outputs into prediction sets that contain the correct
action with a user-defined confidence. When the prediction set is a singleton,
the planner executes that action; otherwise, it requests help from a user. This
has led to LLM-based planners that can ensure plan correctness with a
user-defined probability. However, as LLMs are trained in an
uncertainty-agnostic manner, without awareness of prediction sets, they tend to
produce unnecessarily large sets, particularly at higher confidence levels,
resulting in frequent human interventions limiting autonomous deployment. To
address this, we introduce CoFineLLM (Conformal Finetuning for LLMs), the first
CP-aware finetuning framework for LLM-based planners that explicitly reduces
prediction-set size and, in turn, the need for user interventions. We evaluate
our approach on multiple language-instructed robot planning problems and show
consistent improvements over uncertainty-aware and uncertainty-agnostic
finetuning baselines in terms of prediction-set size, and help rates. Finally,
we demonstrate robustness of our method to out-of-distribution scenarios in
hardware experiments.

</details>


### [348] [Underactuated Biomimetic Autonomous Underwater Vehicle for Ecosystem Monitoring](https://arxiv.org/abs/2511.06578)
*Kaustubh Singh,Shivam Kumar,Shashikant Pawar,Sandeep Manjanna*

Main category: cs.RO

TL;DR: 本文提出了一种适用于海洋和淡水生态系统监测的仿生鱼形水下机器人，并结合强化学习技术优化最小驱动行为。


<details>
  <summary>Details</summary>
Motivation: 当前生态系统监测对灵活、低成本且不影响生态的水下机器人有较大需求，且仿鱼机器人因结构简单、扰动小而成为理想选择。

Method: 改进了鱼形机器人的机械结构，设计了新的尾部摆动机构，并结合强化学习方法来学习最低限度驱动下的游动行为。相关游动机制和行为在FishGym仿真环境中进行测试。

Result: 展示了初步的机械结构和鱼机器人在仿真器中的游动表现，验证了强化学习用于低驱动行为设计的可行性。

Conclusion: 提出的仿生水下机器人成本低、结构简单，有潜力应用于生态监测。强化学习方法可显著优化机器人的驱动与行为。

Abstract: In this paper, we present an underactuated biomimetic underwater robot that
is suitable for ecosystem monitoring in both marine and freshwater
environments. We present an updated mechanical design for a fish-like robot and
propose minimal actuation behaviors learned using reinforcement learning
techniques. We present our preliminary mechanical design of the tail
oscillation mechanism and illustrate the swimming behaviors on FishGym
simulator, where the reinforcement learning techniques will be tested on

</details>


### [349] [How Do VLAs Effectively Inherit from VLMs?](https://arxiv.org/abs/2511.06619)
*Chuheng Zhang,Rushuai Yang,Xiaoyu Chen,Kaixin Wang,Li Zhao,Yi Chen,Jiang Bian*

Main category: cs.RO

TL;DR: 本文提出了GrinningFace诊断基准，通过让机器人完成将物体放置到表情符号上的任务，研究视觉-语言-动作模型（VLA）如何有效继承大规模视觉-语言模型（VLM）的先验知识。通过对不同知识迁移技术的对比，揭示了保持VLM先验对VLA泛化能力的重要性，并为通用 embodied AI 研究提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型依赖VLM的丰富语义先验以实现泛化控制，但VLA如何有效继承和迁移这些先验知识尚不明确，因此需要新的诊断性任务来验证知识传递的效果。

Method: 设计了一项基准任务（GrinningFace）：机器人需根据语言指令将物体放置到带有相应表情符号的区域。该任务覆盖了VLM预训练数据常见的 emoji 知识，但在机器人数据集中很少出现，因此可有效测试知识迁移。对比了多种知识迁移技术，如参数高效微调、VLM冻结、联合训练、离散动作预测与潜在动作预测。在仿真和真实机器人环境中进行了系统评估。

Result: 不同的知识迁移方法表现有明显差异，结果显示只有有效保留和利用VLM先验知识，VLA系统才能在面对新奇任务时保持泛化能力。

Conclusion: 本研究证实了VLM先验在VLA泛化能力中的核心作用，提出的GrinningFace基准为评测和推动通用化 embodied AI 提供了重要工具和设计准则。

Abstract: Vision-language-action (VLA) models hold the promise to attain generalizable
embodied control. To achieve this, a pervasive paradigm is to leverage the rich
vision-semantic priors of large vision-language models (VLMs). However, the
fundamental question persists: How do VLAs effectively inherit the prior
knowledge from VLMs? To address this critical question, we introduce a
diagnostic benchmark, GrinningFace, an emoji tabletop manipulation task where
the robot arm is asked to place objects onto printed emojis corresponding to
language instructions. This task design is particularly revealing -- knowledge
associated with emojis is ubiquitous in Internet-scale datasets used for VLM
pre-training, yet emojis themselves are largely absent from standard robotics
datasets. Consequently, they provide a clean proxy: successful task completion
indicates effective transfer of VLM priors to embodied control. We implement
this diagnostic task in both simulated environment and a real robot, and
compare various promising techniques for knowledge transfer. Specifically, we
investigate the effects of parameter-efficient fine-tuning, VLM freezing,
co-training, predicting discretized actions, and predicting latent actions.
Through systematic evaluation, our work not only demonstrates the critical
importance of preserving VLM priors for the generalization of VLA but also
establishes guidelines for future research in developing truly generalizable
embodied AI systems.

</details>


### [350] [Rapidly Learning Soft Robot Control via Implicit Time-Stepping](https://arxiv.org/abs/2511.06667)
*Andrew Choi,Dezhong Tong*

Main category: cs.RO

TL;DR: 本文提出了一种利用隐式时间步进和新颖的控制方法，实现软体机器人策略快速学习的通用模拟器DisMech，并通过实验显示其显著优于主流工具。


<details>
  <summary>Details</summary>
Motivation: 当前刚体仿真推动了刚体机器人策略学习的普及，但软体机器人由于通用框架缺乏和物理仿真计算昂贵，相关学习发展受限。作者希望突破软体机器人策略学习的瓶颈。

Method: 作者提出使用全面支持软体动力学与摩擦接触的隐式模拟器DisMech，并设计了类比刚体关节控制的delta自然曲率控制方法，使控制变得直观有效。实验对比了四类任务，并与主流软体模拟Elastica在多环境并行仿真下速度和效果进行比较。

Result: 在无接触场景下，DisMech在500环境并行下速度提升6倍；接触丰富场景下提升高达40倍。策略迁移评估中，使用隐式时间步进提升显著，且速度优势不以准确性为代价。

Conclusion: 隐式时间步进和delta曲率控制大幅提升软体机器人策略学习效率，DisMech为该领域提供了高效且通用的仿真和训练平台，有助于推动软体机器人研究。

Abstract: With the explosive growth of rigid-body simulators, policy learning in
simulation has become the de facto standard for most rigid morphologies. In
contrast, soft robotic simulation frameworks remain scarce and are seldom
adopted by the soft robotics community. This gap stems partly from the lack of
easy-to-use, general-purpose frameworks and partly from the high computational
cost of accurately simulating continuum mechanics, which often renders policy
learning infeasible. In this work, we demonstrate that rapid soft robot policy
learning is indeed achievable via implicit time-stepping. Our simulator of
choice, DisMech, is a general-purpose, fully implicit soft-body simulator
capable of handling both soft dynamics and frictional contact. We further
introduce delta natural curvature control, a method analogous to delta joint
position control in rigid manipulators, providing an intuitive and effective
means of enacting control for soft robot learning. To highlight the benefits of
implicit time-stepping and delta curvature control, we conduct extensive
comparisons across four diverse soft manipulator tasks against one of the most
widely used soft-body frameworks, Elastica. With implicit time-stepping,
parallel stepping of 500 environments achieves up to 6x faster speeds for
non-contact cases and up to 40x faster for contact-rich scenarios. Finally, a
comprehensive sim-to-sim gap evaluation--training policies in one simulator and
evaluating them in another--demonstrates that implicit time-stepping provides a
rare free lunch: dramatic speedups achieved without sacrificing accuracy.

</details>


### [351] [Programmable Telescopic Soft Pneumatic Actuators for Deployable and Shape Morphing Soft Robots](https://arxiv.org/abs/2511.06673)
*Joel Kemp,Andre Farinha,David Howard,Krishna Manaswi Digumarti,Josh Pinskier*

Main category: cs.RO

TL;DR: 该论文提出了一类可参数化软驱动器（PTSPA），能够根据高层设计需求生成定制化结构，用于折叠部署和狭小空间的操作，并验证了关键参数与性能的关系，最后在软体四足机器人上展示了其实用性。


<details>
  <summary>Details</summary>
Motivation: 软体机器人具有高度自由度，但由于设计维度过高，当前难以直接有效地利用这些自由度。本研究旨在通过参数化设计，提高软体机器人设计的可控性和应用可能性。

Method: 作者设计了一种可编程伸缩软气动执行器（PTSPA），基于几何参数生成工具，根据高层输入快速定制驱动器模型，并通过半自动化实验系统性研究了关键设计参数（如伸长、变形和刚度）与性能之间的关系。

Result: 实验显示，不同设计参数对驱动器的伸长、弯曲、膨胀和刚度有明确影响，揭示了参数与性能的直接联系。

Conclusion: PTSPA为可部署、形态可变结构和需要大尺度长度变化的场合提供了新的设计范式，并在软体机器人上实现了自动部署和空间自适应，证明其实用前景。

Abstract: Soft Robotics presents a rich canvas for free-form and continuum devices
capable of exerting forces in any direction and transforming between arbitrary
configurations. However, there is no current way to tractably and directly
exploit the design freedom due to the curse of dimensionality. Parameterisable
sets of designs offer a pathway towards tractable, modular soft robotics that
appropriately harness the behavioural freeform of soft structures to create
rich embodied behaviours. In this work, we present a parametrised class of soft
actuators, Programmable Telescopic Soft Pneumatic Actuators (PTSPAs). PTSPAs
expand axially on inflation for deployable structures and manipulation in
challenging confined spaces. We introduce a parametric geometry generator to
customise actuator models from high-level inputs, and explore the new design
space through semi-automated experimentation and systematic exploration of key
parameters. Using it we characterise the actuators' extension/bending,
expansion, and stiffness and reveal clear relationships between key design
parameters and performance. Finally we demonstrate the application of the
actuators in a deployable soft quadruped whose legs deploy to walk, enabling
automatic adaptation to confined spaces. PTSPAs present new design paradigm for
deployable and shape morphing structures and wherever large length changes are
required.

</details>


### [352] [Physically-Grounded Goal Imagination: Physics-Informed Variational Autoencoder for Self-Supervised Reinforcement Learning](https://arxiv.org/abs/2511.06745)
*Lan Thi Ha Nguyen,Kien Ton Manh,Anh Do Duc,Nam Pham Hai*

Main category: cs.RO

TL;DR: 本文提出了Physics-Informed RIG (PI-RIG)，通过物理约束改进自动生成目标的方法，提升机器人自监督条件强化学习中的目标可行性和学习效率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉自监督强化学习方法（如RIG）在目标设置时可能生成物理上不可行的目标，这会妨碍机器人的有效学习；因此，需要一种能够生成物理一致且可实现目标的新方法。

Method: 提出了Enhanced Physics-Informed Variational Autoencoder（Enhanced p3-VAE）的新架构，把潜在空间明确划分为描述物体物理状态和环境视觉因素的部分，并在VAE训练过程中引入微分方程约束及守恒定律保留物理一致性，由此生成兼具可视性和物理可行性的目标。

Result: 在多个视觉机器人操作实验（包括到达、推动、抓取放置等任务）中，PI-RIG生成的目标展现出更高的物理可实现性，带来更有效的探索和更高效的技能学习。

Conclusion: 引入物理信息作为先验到目标生成，可以显著提升自监督条件强化学习中目标设定的合理性与物理可行性，进而促进机器人技能的自主进化和学习效率。

Abstract: Self-supervised goal-conditioned reinforcement learning enables robots to
autonomously acquire diverse skills without human supervision. However, a
central challenge is the goal setting problem: robots must propose feasible and
diverse goals that are achievable in their current environment. Existing
methods like RIG (Visual Reinforcement Learning with Imagined Goals) use
variational autoencoder (VAE) to generate goals in a learned latent space but
have the limitation of producing physically implausible goals that hinder
learning efficiency. We propose Physics-Informed RIG (PI-RIG), which integrates
physical constraints directly into the VAE training process through a novel
Enhanced Physics-Informed Variational Autoencoder (Enhanced p3-VAE), enabling
the generation of physically consistent and achievable goals. Our key
innovation is the explicit separation of the latent space into physics
variables governing object dynamics and environmental factors capturing visual
appearance, while enforcing physical consistency through differential equation
constraints and conservation laws. This enables the generation of physically
consistent and achievable goals that respect fundamental physical principles
such as object permanence, collision constraints, and dynamic feasibility.
Through extensive experiments, we demonstrate that this physics-informed goal
generation significantly improves the quality of proposed goals, leading to
more effective exploration and better skill acquisition in visual robotic
manipulation tasks including reaching, pushing, and pick-and-place scenarios.

</details>


### [353] [Semi-distributed Cross-modal Air-Ground Relative Localization](https://arxiv.org/abs/2511.06749)
*Weining Lu,Deer Bin,Lian Ma,Ming Ma,Zhihao Ma,Xiangyang Chen,Longfei Wang,Yixiao Feng,Zhouxian Jiang,Yongliang Shi,Bin Liang*

Main category: cs.RO

TL;DR: 本文提出了一种高效、精确且灵活的空地协作机器人相对定位方法，实现了空地异构协作的低带宽、高精度相对定位。


<details>
  <summary>Details</summary>
Motivation: 目前多机器人相对定位方法多为同构、分布式SLAM，对传感器配置和全局状态估计高度依赖，降低了系统的灵活性与准确性。因此，亟需一种能充分利用地面机器人多传感器能力、实现空地异构协作的相对定位新方法。

Method: 提出了半分布式的空地异构相对定位框架：UGV与UAV各自独立进行SLAM，并基于深度学习提取关键点与全局描述子，实现相对定位和全局状态解耦。UGV通过融合激光、相机和IMU进行局部捆绑调整（BA），分为先通过激光-惯导轨迹优化相机位姿，再估算UGV与UAV间的相对位姿。同时，采用深度学习描述子的增量回环检测实现关键帧有效管理。通信端仅传输关键点像素和描述子，极大降低带宽消耗。

Result: 实验结果显示该方法在定位精度和运算效率方面成绩优异，通信带宽仅需0.3 Mbps，显著优于需传输图像或点云的传统多机器人SLAM方法。

Conclusion: 所提框架兼具高效性、精准性及低带宽传输优势，为空地异构多机器人协作场景中的相对定位提供了新的解决方案，有望推动相关领域进一步发展。

Abstract: Efficient, accurate, and flexible relative localization is crucial in
air-ground collaborative tasks. However, current approaches for robot relative
localization are primarily realized in the form of distributed multi-robot SLAM
systems with the same sensor configuration, which are tightly coupled with the
state estimation of all robots, limiting both flexibility and accuracy. To this
end, we fully leverage the high capacity of Unmanned Ground Vehicle (UGV) to
integrate multiple sensors, enabling a semi-distributed cross-modal air-ground
relative localization framework. In this work, both the UGV and the Unmanned
Aerial Vehicle (UAV) independently perform SLAM while extracting deep
learning-based keypoints and global descriptors, which decouples the relative
localization from the state estimation of all agents. The UGV employs a local
Bundle Adjustment (BA) with LiDAR, camera, and an IMU to rapidly obtain
accurate relative pose estimates. The BA process adopts sparse keypoint
optimization and is divided into two stages: First, optimizing camera poses
interpolated from LiDAR-Inertial Odometry (LIO), followed by estimating the
relative camera poses between the UGV and UAV. Additionally, we implement an
incremental loop closure detection algorithm using deep learning-based
descriptors to maintain and retrieve keyframes efficiently. Experimental
results demonstrate that our method achieves outstanding performance in both
accuracy and efficiency. Unlike traditional multi-robot SLAM approaches that
transmit images or point clouds, our method only transmits keypoint pixels and
their descriptors, effectively constraining the communication bandwidth under
0.3 Mbps. Codes and data will be publicly available on
https://github.com/Ascbpiac/cross-model-relative-localization.git.

</details>


### [354] [SlotVLA: Towards Modeling of Object-Relation Representations in Robotic Manipulation](https://arxiv.org/abs/2511.06754)
*Taisei Hanyu,Nhat Chung,Huy Le,Toan Nguyen,Yuki Ikebe,Anthony Gunderman,Duy Nguyen Ho Minh,Khoa Vo,Tung Kieu,Kashu Yamazaki,Chase Rainwater,Anh Nguyen,Ngan Le*

Main category: cs.RO

TL;DR: 本文提出了一种以对象及对象关系为核心的表示方法，用于实现高效且可解释的多任务机器人操作，并发布了支持该研究的全新数据集LIBERO+和方法SlotVLA。


<details>
  <summary>Details</summary>
Motivation: 现有的多任务机器人模型大多基于稠密嵌入方式，将对象和背景信息混合，导致效率与可解释性不足。受人类基于离散对象及其关系进行推理的启发，作者希望通过对象及对象关系为核心的紧凑表示，提升机器人操作的结构化、效率和可解释性。

Method: 1）提出LIBERO+数据集，包含丰富的对象级注释（如包围框、掩码标签、实例级时间追踪），为对象关系推理提供基准；2）提出SlotVLA方法，采用slot-attention机制，通过slot-based视觉分词器和关系解码器构建对象及关系表示，并结合大模型模块将这些表征转换为可执行操作。

Result: 在LIBERO+数据集上的实验表明，基于对象slot和对象关系slot的表征方案可大幅减少视觉token数量，并具备有竞争力的泛化能力。

Conclusion: 以对象及对象关系为核心的表示方式，相比传统混杂表示更紧凑、易于解释且能有效支持多任务机器人操作。 LIBERO+数据集和SlotVLA方法为此研究方向提供了稳固基础。

Abstract: Inspired by how humans reason over discrete objects and their relationships,
we explore whether compact object-centric and object-relation representations
can form a foundation for multitask robotic manipulation. Most existing robotic
multitask models rely on dense embeddings that entangle both object and
background cues, raising concerns about both efficiency and interpretability.
In contrast, we study object-relation-centric representations as a pathway to
more structured, efficient, and explainable visuomotor control. Our
contributions are two-fold. First, we introduce LIBERO+, a fine-grained
benchmark dataset designed to enable and evaluate object-relation reasoning in
robotic manipulation. Unlike prior datasets, LIBERO+ provides object-centric
annotations that enrich demonstrations with box- and mask-level labels as well
as instance-level temporal tracking, supporting compact and interpretable
visuomotor representations. Second, we propose SlotVLA, a slot-attention-based
framework that captures both objects and their relations for action decoding.
It uses a slot-based visual tokenizer to maintain consistent temporal object
representations, a relation-centric decoder to produce task-relevant
embeddings, and an LLM-driven module that translates these embeddings into
executable actions. Experiments on LIBERO+ demonstrate that object-centric slot
and object-relation slot representations drastically reduce the number of
required visual tokens, while providing competitive generalization. Together,
LIBERO+ and SlotVLA provide a compact, interpretable, and effective foundation
for advancing object-relation-centric robotic manipulation.

</details>


### [355] [Human-Level Actuation for Humanoids](https://arxiv.org/abs/2511.06796)
*MD-Nazmus Sunbeam*

Main category: cs.RO

TL;DR: 本文提出了一个量化和评价仿人机器人是否真正达到“人类级”驱动水平的完整框架，涵盖标准化的度量方式、测试流程和综合评分体系。


<details>
  <summary>Details</summary>
Motivation: 目前“人类级”驱动的宣传泛滥，但量化标准和评价方式匮乏，现有限定如峰值力矩或速度并不能反映仿人机器人在真实任务中持续输出力矩、功率和工作范围的能力。因此，研究者亟需一个科学且可实际操作的评价体系来指导仿人机器人设计和跨系统公平比较。

Method: 该框架包含三部分：1）建立标准的自由度（DoF）字典，确保对比过程中人类与机器人关节的一致性；2）通过‘Human-Equivalence Envelopes’（HEE）在具体任务与关节动态下，精准定义和量化人机各关节等效输出要求；3）提出‘Human-Level Actuation Score’（HLAS），综合考量工作空间覆盖、HEE涵盖率、力矩-模式带宽、能效和热可持续等六大硬性指标，并制定了细致可重复的测试流程。

Result: 通过具体多关节仿人机器人案例，演示了HLAS的实际计算流程，并展示了该评分如何揭示相关系统中传统峰值规格（如齿比）难以反映的性能权衡（如带宽与效率之间的平衡关系等）。

Conclusion: 该框架既是仿人机器人驱动系统设计的重要指标，也是跨平台性能评价的通用标准，其所有评判方法均以公开的人体生物力学数据为基础，更具有科学性和实际应用价值。

Abstract: Claims that humanoid robots achieve ``human-level'' actuation are common but
rarely quantified. Peak torque or speed specifications tell us little about
whether a joint can deliver the right combination of torque, power, and
endurance at task-relevant postures and rates. We introduce a comprehensive
framework that makes ``human-level'' measurable and comparable across systems.
Our approach has three components. First, a kinematic \emph{DoF atlas}
standardizes joint coordinate systems and ranges of motion using ISB-based
conventions, ensuring that human and robot joints are compared in the same
reference frames. Second, \emph{Human-Equivalence Envelopes (HEE)} define
per-joint requirements by measuring whether a robot meets human torque
\emph{and} power simultaneously at the same joint angle and rate $(q,\omega)$,
weighted by positive mechanical work in task-specific bands (walking, stairs,
lifting, reaching, and hand actions). Third, the \emph{Human-Level Actuation
Score (HLAS)} aggregates six physically grounded factors: workspace coverage
(ROM and DoF), HEE coverage, torque-mode bandwidth, efficiency, and thermal
sustainability. We provide detailed measurement protocols using dynamometry,
electrical power monitoring, and thermal testing that yield every HLAS input
from reproducible experiments. A worked example demonstrates HLAS computation
for a multi-joint humanoid, showing how the score exposes actuator trade-offs
(gearing ratio versus bandwidth and efficiency) that peak-torque specifications
obscure. The framework serves as both a design specification for humanoid
development and a benchmarking standard for comparing actuation systems, with
all components grounded in published human biomechanics data.

</details>


### [356] [Vision-Aided Online A* Path Planning for Efficient and Safe Navigation of Service Robots](https://arxiv.org/abs/2511.06801)
*Praveen Kumar,Tushar Sandhan*

Main category: cs.RO

TL;DR: 本论文提出了一种将语义感知与路径规划紧密集成的新框架，使低成本服务机器人能够在识别和遵循关键视觉线索的同时实现高效导航，成功突破了传统纯几何派导航系统的局限。


<details>
  <summary>Details</summary>
Motivation: 传统的机器人导航主依靠LiDAR等昂贵且只能感知几何特征的传感器，缺乏理解环境语义信息的能力，导致无法区分对任务至关重要的物体与普通障碍物，进而限制了机器人在人类环境下的应用。现有的语义分割技术虽然先进，但未能与实时路径规划器有效整合，尤其难以应用在低成本嵌入式硬件上。

Method: 作者设计了一个轻量级的语义感知模块，与在线A*路径规划器紧密集成。通过语义分割网络识别用户定义的视觉约束（如关键纸张或安全线），再将其融合到全局地图中作为非几何障碍物，使机器人在全局地图中以实时、上下文相关的方式规划路径。

Result: 在高保真仿真和现实机器人平台上进行了大量实验，结果表明该框架能够以实时、高效的方式在复杂环境中安全导航，正确避开传统几何方法不能识别的关键视觉障碍物。

Conclusion: 本研究证明，低成本机器人通过整合语义感知与路径规划技术，能够实现基于上下文的安全导航，可广泛应用于需要语义理解的人类环境中，突破了传统机器人规划的视觉局限。

Abstract: The deployment of autonomous service robots in human-centric environments is
hindered by a critical gap in perception and planning. Traditional navigation
systems rely on expensive LiDARs that, while geometrically precise, are seman-
tically unaware, they cannot distinguish a important document on an office
floor from a harmless piece of litter, treating both as physically traversable.
While advanced semantic segmentation exists, no prior work has successfully
integrated this visual intelligence into a real-time path planner that is
efficient enough for low-cost, embedded hardware. This paper presents a frame-
work to bridge this gap, delivering context-aware navigation on an affordable
robotic platform. Our approach centers on a novel, tight integration of a
lightweight perception module with an online A* planner. The perception system
employs a semantic segmentation model to identify user-defined visual
constraints, enabling the robot to navigate based on contextual importance
rather than physical size alone. This adaptability allows an operator to define
what is critical for a given task, be it sensitive papers in an office or
safety lines in a factory, thus resolving the ambiguity of what to avoid. This
semantic perception is seamlessly fused with geometric data. The identified
visual constraints are projected as non-geometric obstacles onto a global map
that is continuously updated from sensor data, enabling robust navigation
through both partially known and unknown environments. We validate our
framework through extensive experiments in high-fidelity simulations and on a
real-world robotic platform. The results demonstrate robust, real-time
performance, proving that a cost- effective robot can safely navigate complex
environments while respecting critical visual cues invisible to traditional
planners.

</details>


### [357] [Vision-Based System Identification of a Quadrotor](https://arxiv.org/abs/2511.06839)
*Selim Ahmet Iz,Mustafa Unel*

Main category: cs.RO

TL;DR: 本论文研究了基于视觉的系统辨识技术在四旋翼建模与控制中的应用，展示了其能够提升模型一致性和性能。


<details>
  <summary>Details</summary>
Motivation: 四旋翼飞行器的精确建模存在推力和阻力系数等参数不确定问题，传统方法存在一定局限性，因而需要新的方法提升辨识与控制效果。

Method: 采用灰箱建模方法减少建模不确定性，并应用机载视觉系统进行数据采集，通过所获得的数据进行系统辨识，并基于该模型设计LQR控制器。

Result: 基于视觉的系统辨识方法生成的模型具有良好一致性，控制实验结果也验证了模型的有效性和该方法的可行性。

Conclusion: 基于视觉的系统辨识技术能够提升四旋翼的建模精度和控制性能，对未来在性能提升、故障检测及决策等领域的研究具有重要意义。

Abstract: This paper explores the application of vision-based system identification
techniques in quadrotor modeling and control. Through experiments and analysis,
we address the complexities and limitations of quadrotor modeling, particularly
in relation to thrust and drag coefficients. Grey-box modeling is employed to
mitigate uncertainties, and the effectiveness of an onboard vision system is
evaluated. An LQR controller is designed based on a system identification model
using data from the onboard vision system. The results demonstrate consistent
performance between the models, validating the efficacy of vision based system
identification. This study highlights the potential of vision-based techniques
in enhancing quadrotor modeling and control, contributing to improved
performance and operational capabilities. Our findings provide insights into
the usability and consistency of these techniques, paving the way for future
research in quadrotor performance enhancement, fault detection, and
decision-making processes.

</details>


### [358] [Multi-Agent AI Framework for Road Situation Detection and C-ITS Message Generation](https://arxiv.org/abs/2511.06892)
*Kailin Tong,Selim Solmaz,Kenan Mujkic,Gottfried Allmer,Bo Leng*

Main category: cs.RO

TL;DR: 本文提出将多模态大语言模型（MLLMs）与基于视觉的感知结合，研发多智能体AI框架用于道路状况检测，实验表明在检测场景方面有极高召回率，但语义理解和部分指标仍有限。


<details>
  <summary>Details</summary>
Motivation: 现有道路检测方法在新场景下泛化能力差，且缺乏语义解释，影响交通推荐系统的可靠性，因此需要更智能、可解释的检测框架。

Method: 研究构建了一个结合MLLMs与视觉感知的多智能体AI系统，分别负责情况检测、距离估计、决策和C-ITS信息生成，并在自制数据集和TAD视频上评测了Gemini-2.0-Flash和Gemini-2.5-Flash两款模型。

Result: 情况检测的召回率达到100%，消息格式完全正确，但模型在车道数量、驾驶状态识别等任务上表现不足，且出现假阳性，Gemini-2.5-Flash在准确率和语义理解上甚至弱于2.0版且延迟更高。

Conclusion: 尽管所提框架在检测准确率上表现突出，但在细粒度识别和语义解释上仍有提升空间，未来应针对交通智能应用进一步微调定制MLLMs或专用模型以优化性能。

Abstract: Conventional road-situation detection methods achieve strong performance in
predefined scenarios but fail in unseen cases and lack semantic interpretation,
which is crucial for reliable traffic recommendations. This work introduces a
multi-agent AI framework that combines multimodal large language models (MLLMs)
with vision-based perception for road-situation monitoring. The framework
processes camera feeds and coordinates dedicated agents for situation
detection, distance estimation, decision-making, and Cooperative Intelligent
Transport System (C-ITS) message generation. Evaluation is conducted on a
custom dataset of 103 images extracted from 20 videos of the TAD dataset. Both
Gemini-2.0-Flash and Gemini-2.5-Flash were evaluated. The results show 100\%
recall in situation detection and perfect message schema correctness; however,
both models suffer from false-positive detections and have reduced performance
in terms of number of lanes, driving lane status and cause code. Surprisingly,
Gemini-2.5-Flash, though more capable in general tasks, underperforms
Gemini-2.0-Flash in detection accuracy and semantic understanding and incurs
higher latency (Table II). These findings motivate further work on fine-tuning
specialized LLMs or MLLMs tailored for intelligent transportation applications.

</details>


### [359] [Integration of Visual SLAM into Consumer-Grade Automotive Localization](https://arxiv.org/abs/2511.06919)
*Luis Diener,Jens Kalkkuhl,Markus Enzweiler*

Main category: cs.RO

TL;DR: 本文提出将视觉SLAM与车辆横向动力学模型融合，实现汽车行驶中陀螺仪的在线标定，有效提升了消费者级车辆自定位精度，并在公私数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有消费者级车辆主要依靠轮速计和IMU等本体传感器进行定位，但这些传感器受系统误差和标定精度限制，影响运动估计准确性。尽管机器人领域视觉惯性SLAM已普及，汽车领域集成此类技术仍罕见，急需提升车辆定位性能。

Method: 作者提出融合视觉SLAM与车辆横向动力学模型的方案，实现陀螺仪的在线自动标定。通过该框架，将视觉信息辅助惯性传感器，提高运动估计的准确性。

Result: 实验证明，融合视觉SLAM后，陀螺仪标定的准确性显著提升，定位精度较现有最先进方法表现更佳。论文在专有和公开数据集上均取得优秀结果，尤其在公开基准测试中超越现有方法。

Conclusion: 将视觉SLAM与车辆动力学融合，为消费者级车辆实现更高精度定位提供了可行途径，为未来汽车高精度定位研究及应用带来新思路。

Abstract: Accurate ego-motion estimation in consumer-grade vehicles currently relies on
proprioceptive sensors, i.e. wheel odometry and IMUs, whose performance is
limited by systematic errors and calibration. While visual-inertial SLAM has
become a standard in robotics, its integration into automotive ego-motion
estimation remains largely unexplored. This paper investigates how visual SLAM
can be integrated into consumer-grade vehicle localization systems to improve
performance. We propose a framework that fuses visual SLAM with a lateral
vehicle dynamics model to achieve online gyroscope calibration under realistic
driving conditions. Experimental results demonstrate that vision-based
integration significantly improves gyroscope calibration accuracy and thus
enhances overall localization performance, highlighting a promising path toward
higher automotive localization accuracy. We provide results on both proprietary
and public datasets, showing improved performance and superior localization
accuracy on a public benchmark compared to state-of-the-art methods.

</details>


### [360] [Raspi$^2$USBL: An open-source Raspberry Pi-Based Passive Inverted Ultra-Short Baseline Positioning System for Underwater Robotics](https://arxiv.org/abs/2511.06998)
*Jin Huang,Yingqiang Wang,Ying Chen*

Main category: cs.RO

TL;DR: 本文提出了一种基于Raspberry Pi的开源低成本被动倒置超短基线（piUSBL）水下定位系统，验证其性能优于0.1%距离误差和0.1度方位误差，且最大可达1.3公里，适用于水下机器人定位。


<details>
  <summary>Details</summary>
Motivation: 水下机器人由于GNSS信号无法穿透水面，精确定位一直是重要难题。现有商业定位系统昂贵，限制了普及和创新，因此有必要设计一种低成本、可复现的高精度水下定位方案。

Method: 开发了Raspberry Pi为核心的超短基线定位系统，包括模块化硬件（多通道水听器、前置放大器、温控振荡器、数据采集板、主动信标等）及开源C++软件（高精度时钟同步与触发、实时信号处理如匹配滤波、波束形成和自适应增益等），实现单向飞行时间及到达方向估算。

Result: 在消声水池、淡水湖和大海环境下测试，成果显示斜距误差小于0.1%，方位误差在0.1度以内，最大定位距离达1.3公里，证明系统高度精确且性能稳定。

Conclusion: 低成本开源硬件同样可实现研究级水下定位精度。Raspi$^2$USBL作为统一开源平台，有助于降低实验室门槛、促进复现和水下机器人群体导航领域的创新协作。

Abstract: Precise underwater positioning remains a fundamental challenge for underwater
robotics since global navigation satellite system (GNSS) signals cannot
penetrate the sea surface. This paper presents Raspi$^2$USBL, an open-source,
Raspberry Pi-based passive inverted ultra-short baseline (piUSBL) positioning
system designed to provide a low-cost and accessible solution for underwater
robotic research. The system comprises a passive acoustic receiver and an
active beacon. The receiver adopts a modular hardware architecture that
integrates a hydrophone array, a multichannel preamplifier, an oven-controlled
crystal oscillator (OCXO), a Raspberry Pi 5, and an MCC-series data acquisition
(DAQ) board. Apart from the Pi 5, OCXO, and MCC board, the beacon comprises an
impedance-matching network, a power amplifier, and a transmitting transducer.
An open-source C++ software framework provides high-precision clock
synchronization and triggering for one-way travel-time (OWTT) messaging, while
performing real-time signal processing, including matched filtering, array
beamforming, and adaptive gain control, to estimate the time of flight (TOF)
and direction of arrival (DOA) of received signals. The Raspi$^2$USBL system
was experimentally validated in an anechoic tank, freshwater lake, and open-sea
trials. Results demonstrate a slant-range accuracy better than 0.1%, a bearing
accuracy within 0.1$^\circ$, and stable performance over operational distances
up to 1.3 km. These findings confirm that low-cost, reproducible hardware can
deliver research-grade underwater positioning accuracy. By releasing both the
hardware and software as open-source, Raspi$^2$USBL provides a unified
reference platform that lowers the entry barrier for underwater robotics
laboratories, fosters reproducibility, and promotes collaborative innovation in
underwater acoustic navigation and swarm robotics.

</details>


### [361] [HDCNet: A Hybrid Depth Completion Network for Grasping Transparent and Reflective Objects](https://arxiv.org/abs/2511.07081)
*Guanghu Xie,Mingxu Li,Songwei Wu,Yang Liu,Zongwu Xie,Baoshi Cao,Hong Liu*

Main category: cs.RO

TL;DR: 针对透明和反光物体的深度感知难题，提出了一种融合Transformer、CNN和Mamba的新型深度补全网络HDCNet，在多项数据集和机器人抓取实验中效果显著。


<details>
  <summary>Details</summary>
Motivation: 传统深度传感器难以准确感知透明与反光表面，导致机器人在感知和抓取任务中性能受限。为提升对这类物体的深度感知能力，需要开发更鲁棒的深度补全方法。

Method: 设计了HDCNet网络，采用Transformer-CNN双分支编码器以提取模态特定特征，在浅层引入轻量级多模态融合模块以整合低层特征，在瓶颈层采用Transformer-Mamba混合融合模块，实现高层语义与全球上下文信息的深度集成，从而提升深度补全的准确性和鲁棒性。

Result: 在多个公开数据集上，HDCNet在深度补全任务中取得了SOTA效果；实际机器人抓取实验表明，HDCNet在处理透明和反光物体时，抓取成功率提升高达60%。

Conclusion: HDCNet显著提升了机器人在复杂表面物体感知与操作中的深度补全精度与抓取表现，是处理透明和反光物体深度感知问题的有效方案。

Abstract: Depth perception of transparent and reflective objects has long been a
critical challenge in robotic manipulation.Conventional depth sensors often
fail to provide reliable measurements on such surfaces, limiting the
performance of robots in perception and grasping tasks. To address this issue,
we propose a novel depth completion network,HDCNet,which integrates the
complementary strengths of Transformer,CNN and Mamba
architectures.Specifically,the encoder is designed as a dual-branch
Transformer-CNN framework to extract modality-specific features. At the shallow
layers of the encoder, we introduce a lightweight multimodal fusion module to
effectively integrate low-level features. At the network bottleneck,a
Transformer-Mamba hybrid fusion module is developed to achieve deep integration
of high-level semantic and global contextual information, significantly
enhancing depth completion accuracy and robustness. Extensive evaluations on
multiple public datasets demonstrate that HDCNet achieves
state-of-the-art(SOTA) performance in depth completion
tasks.Furthermore,robotic grasping experiments show that HDCNet substantially
improves grasp success rates for transparent and reflective objects,achieving
up to a 60% increase.

</details>


### [362] [Dynamics-Decoupled Trajectory Alignment for Sim-to-Real Transfer in Reinforcement Learning for Autonomous Driving](https://arxiv.org/abs/2511.07155)
*Thomas Steinecker,Alexander Bienemann,Denis Trescher,Thorsten Luettel,Mirko Maehlisch*

Main category: cs.RO

TL;DR: 本文提出了一种将强化学习（RL）从仿真环境无缝转移到真实车辆的框架，通过空间和时间对齐策略实现运动规划与车辆控制的解耦，在真实车辆上取得了鲁棒的零样本迁移效果。


<details>
  <summary>Details</summary>
Motivation: 强化学习在机器人领域表现出色，但真实车辆的动力学复杂性及“仿真与现实差距”问题导致RL模型难以直接迁移到实际应用，这成为阻碍其落地的重要障碍。

Method: 作者提出将运动规划与车辆控制解耦。首先在仿真中训练一个基于运动学自行车模型的RL智能体，输出连续控制动作。再将其行为“蒸馏”为能预测轨迹的智能体，生成有限时域内的自主车辆轨迹，实现虚拟与真实车辆的同步。在实车部署时，横向控制采用Stanley控制器，纵向采用自适应更新机制，对虚实偏差进行补偿。

Result: 在真实车辆上的实验证明，该对齐策略可实现RL运动规划方法的稳健“零样本迁移”，并实现高层轨迹生成与低层车辆控制的有效解耦。

Conclusion: 本文框架有效克服了RL从仿真到现实的动力学和环境差异问题，为RL在真实车辆中的实际部署提供了一种通用且可扩展的解决思路。

Abstract: Reinforcement learning (RL) has shown promise in robotics, but deploying RL
on real vehicles remains challenging due to the complexity of vehicle dynamics
and the mismatch between simulation and reality. Factors such as tire
characteristics, road surface conditions, aerodynamic disturbances, and vehicle
load make it infeasible to model real-world dynamics accurately, which hinders
direct transfer of RL agents trained in simulation. In this paper, we present a
framework that decouples motion planning from vehicle control through a spatial
and temporal alignment strategy between a virtual vehicle and the real system.
An RL agent is first trained in simulation using a kinematic bicycle model to
output continuous control actions. Its behavior is then distilled into a
trajectory-predicting agent that generates finite-horizon ego-vehicle
trajectories, enabling synchronization between virtual and real vehicles. At
deployment, a Stanley controller governs lateral dynamics, while longitudinal
alignment is maintained through adaptive update mechanisms that compensate for
deviations between virtual and real trajectories. We validate our approach on a
real vehicle and demonstrate that the proposed alignment strategy enables
robust zero-shot transfer of RL-based motion planning from simulation to
reality, successfully decoupling high-level trajectory generation from
low-level vehicle control.

</details>


### [363] [Automated Generation of Continuous-Space Roadmaps for Routing Mobile Robot Fleets](https://arxiv.org/abs/2511.07175)
*Marvin Rüdt,Constantin Enke,Kai Furmans*

Main category: cs.RO

TL;DR: 该论文提出了一种自动化路线图生成方法，用于提升移动机器人车队在内物流场景下的高效导航与协调，实验证明其优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有移动机器人导航方案在精度与实际约束间存在权衡——网格法缺乏几何精度，连续空间法忽略实际约束，影响系统吞吐和效率。因此需要一种能够兼顾几何精度与现实需求的路线图生成方法。

Method: 提出一种连续空间下的自动化路线图生成方法，将站点间运输需求、节点和边的最小距离约束纳入建图流程，结合空间离散化、运输需求驱动的K最短路径优化和路径平滑技术，以生成适用于内物流的高效路线图。

Result: 在多个内物流实际案例中，该方法在结构复杂度更低、冗余度更高、路径接近最优等核心指标上，均超过了4连通网格、8连通网格和随机采样等基线方法。

Conclusion: 该方法能自动生成更高效、更鲁棒、适应实际物流需求的路线图，显著提升移动机器人车队的调度性能。

Abstract: Efficient routing of mobile robot fleets is crucial in intralogistics, where
delays and deadlocks can substantially reduce system throughput. Roadmap
design, specifying feasible transport routes, directly affects fleet
coordination and computational performance. Existing approaches are either
grid-based, compromising geometric precision, or continuous-space approaches
that disregard practical constraints. This paper presents an automated roadmap
generation approach that bridges this gap by operating in continuous-space,
integrating station-to-station transport demand and enforcing minimum distance
constraints for nodes and edges. By combining free space discretization,
transport demand-driven $K$-shortest-path optimization, and path smoothing, the
approach produces roadmaps tailored to intralogistics applications. Evaluation
across multiple intralogistics use cases demonstrates that the proposed
approach consistently outperforms established baselines (4-connected grid,
8-connected grid, and random sampling), achieving lower structural complexity,
higher redundancy, and near-optimal path lengths, enabling efficient and robust
routing of mobile robot fleets.

</details>


### [364] [Robotic versus Human Teleoperation for Remote Ultrasound](https://arxiv.org/abs/2511.07275)
*David Black,Septimiu Salcudean*

Main category: cs.RO

TL;DR: 本论文比较了远程超声操作中的人类与机器人远程操作方式，发现在人机性能相近的情况下，人类远程操作方式更实用、灵活且力量施加更一致。


<details>
  <summary>Details</summary>
Motivation: 超声诊断虽然广泛、安全且低成本，但需要专业操作员，乡村地区往往缺乏这类人力资源，因此需要开发远程操作技术以提升偏远地区医疗服务能力。

Method: 对机器人远程操作与人类远程操作（通过混合现实由专家远程指导新手完成超声检查）进行了对比研究，分析其在设备设置时间、灵活性、完成时长、位置追踪和施加力量一致性等方面的表现差别。

Result: 研究发现在人类与机器人远程操作实现超声检查时，完成时间和位置准确性无显著统计差异，均值差异分别为1.8%和0.5%；人类远程操作在力量施加一致性方面更优。同时，人类远程操作在实用性和可接近性方面有明显优势。

Conclusion: 人类远程操作方式在维持与机器人类似性能指标的同时，更加实用、成本更低并更适合小型或偏远社区，因此值得优先推广应用。

Abstract: Diagnostic medical ultrasound is widely used, safe, and relatively low cost
but requires a high degree of expertise to acquire and interpret the images.
Personnel with this expertise are often not available outside of larger cities,
leading to difficult, costly travel and long wait times for rural populations.
To address this issue, tele-ultrasound techniques are being developed,
including robotic teleoperation and recently human teleoperation, in which a
novice user is remotely guided in a hand-over-hand manner through mixed reality
to perform an ultrasound exam. These methods have not been compared, and their
relative strengths are unknown. Human teleoperation may be more practical than
robotics for small communities due to its lower cost and complexity, but this
is only relevant if the performance is comparable. This paper therefore
evaluates the differences between human and robotic teleoperation, examining
practical aspects such as setup time and flexibility and experimentally
comparing performance metrics such as completion time, position tracking, and
force consistency. It is found that human teleoperation does not lead to
statistically significant differences in completion time or position accuracy,
with mean differences of 1.8% and 0.5%, respectively, and provides more
consistent force application despite being substantially more practical and
accessible.

</details>


### [365] [PlanT 2.0: Exposing Biases and Structural Flaws in Closed-Loop Driving](https://arxiv.org/abs/2511.07292)
*Simon Gerstenecker,Andreas Geiger,Katrin Renz*

Main category: cs.RO

TL;DR: 本文介绍了PlanT 2.0，一种为CARLA自动驾驶研究设计的轻量级、面向对象的规划Transformer，通过系统化扰动输入来定位模型失败的根因，并对结果进行分析。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶领域更关注指标提升和新方法，缺乏对模型失效、偏差及取巧学习的深入研究，导致改进有限且理解不深。因此，作者希望通过系统性分析深入理解模型失败的根本原因。

Method: 提出PlanT 2.0，一个基于对象表示的规划Transformer。输入可控地进行扰动，比如更改单个物体的位置或添加/删除物体，对比以传感器为基础的模型更利于分析。针对CARLA Leaderboard 2.0的新挑战，PlanT 2.0在算法结构上多处改进，并在多项任务上实现SOTA表现。

Result: PlanT 2.0在Longest6 v2、Bench2Drive和CARLA验证路线等任务取得了最优成绩。分析揭示了模型的典型失效问题，如障碍物多样性低导致的场景理解不足、过于僵化的专家行为出现可供取巧的捷径、模型过拟合于固定专家轨迹等。

Conclusion: 作者建议从“以数据为中心”的角度推动自动驾驶系统发展，构建更丰富、健壮、低偏差的数据集，并已开源代码和模型以促进研究社区的发展。

Abstract: Most recent work in autonomous driving has prioritized benchmark performance
and methodological innovation over in-depth analysis of model failures, biases,
and shortcut learning. This has led to incremental improvements without a deep
understanding of the current failures. While it is straightforward to look at
situations where the model fails, it is hard to understand the underlying
reason. This motivates us to conduct a systematic study, where inputs to the
model are perturbed and the predictions observed. We introduce PlanT 2.0, a
lightweight, object-centric planning transformer designed for autonomous
driving research in CARLA. The object-level representation enables controlled
analysis, as the input can be easily perturbed (e.g., by changing the location
or adding or removing certain objects), in contrast to sensor-based models. To
tackle the scenarios newly introduced by the challenging CARLA Leaderboard 2.0,
we introduce multiple upgrades to PlanT, achieving state-of-the-art performance
on Longest6 v2, Bench2Drive, and the CARLA validation routes. Our analysis
exposes insightful failures, such as a lack of scene understanding caused by
low obstacle diversity, rigid expert behaviors leading to exploitable
shortcuts, and overfitting to a fixed set of expert trajectories. Based on
these findings, we argue for a shift toward data-centric development, with a
focus on richer, more robust, and less biased datasets. We open-source our code
and model at https://github.com/autonomousvision/plant2.

</details>


### [366] [Exact Smooth Reformulations for Trajectory Optimization Under Signal Temporal Logic Specifications](https://arxiv.org/abs/2511.07375)
*Shaohang Han,Joris Verhagen,Jana Tumova*

Main category: cs.RO

TL;DR: 本文提出了一种将信号时序逻辑（STL）下的运动规划问题转化为可微分轨迹优化问题的方法，并通过引入精确的max和min重构保证了无近似误差，方法具备精确性和可行性，并通过仿真实验证明其实用性。


<details>
  <summary>Details</summary>
Motivation: 信号时序逻辑（STL）能够形式化地表达空间-时间上的复杂约束，但相关的运动规划仍面临优化难、不可微分、误差累积等问题。作者希望提出无近似误差、可优化求解的新方法。

Method: 将STL约束下的运动规划任务转化为基于STL鲁棒性的轨迹优化问题，并提出了一种max和min操作的精确可微分重构方法，整体求解过程精确、可微且严格。

Result: 所提方法在数值仿真环境下进行了验证，结果显示该方法不仅精确且平滑，还能在实际问题中有效地处理STL约束下的运动规划任务。

Conclusion: 通过精确重构与优化建模，该方法在保证理论上的严格性的同时，能够高效地求解实际应用中的时序逻辑运动规划问题，具有较强实用性与理论价值。

Abstract: We study motion planning under Signal Temporal Logic (STL), a useful
formalism for specifying spatial-temporal requirements. We pose STL synthesis
as a trajectory optimization problem leveraging the STL robustness semantics.
To obtain a differentiable problem without approximation error, we introduce an
exact reformulation of the max and min operators. The resulting method is
exact, smooth, and sound. We validate it in numerical simulations,
demonstrating its practical performance.

</details>


### [367] [Residual Rotation Correction using Tactile Equivariance](https://arxiv.org/abs/2511.07381)
*Yizhe Zhu,Zhang Ye,Boce Hu,Haibo Zhao,Yu Qi,Dian Wang,Robert Platt*

Main category: cs.RO

TL;DR: 文章提出了EquiTac框架，利用物体旋转的SO(2)对称性提升了视觉-触觉策略学习的样本效率和泛化能力，能更好地支持复杂操控任务。


<details>
  <summary>Details</summary>
Motivation: 由于采集触觉数据代价高昂，提升视觉-触觉策略学习的样本效率成为关键难题。现有方法很难将环境中的对称性（如物体旋转的对称）纳入策略建模，导致泛化能力差、数据利用率低。

Method: EquiTac首先从基于视觉的触觉传感器原始RGB输入重建表面法线，通过SO(2)等变网络，使法向量场的旋转对应于物体在手中的旋转。在测试阶段，网络预测残差旋转动作，并在不增加演示数据的前提下对基础视觉-动作策略进行实时旋转修正。

Result: 在真实机器人上，EquiTac即使只用很少的训练样本，也能准确地实现零样本泛化，适应未见过的手中物体姿态。而其他方法即使用更多数据也无法达到类似效果。

Conclusion: EquiTac首次将触觉等变性显式地编码进策略学习中，模块轻量、对称性感知强，有效提升了复杂接触任务中策略的可靠性和泛化能力，对触觉强化学习领域具有重要推动作用。

Abstract: Visuotactile policy learning augments vision-only policies with tactile
input, facilitating contact-rich manipulation. However, the high cost of
tactile data collection makes sample efficiency the key requirement for
developing visuotactile policies. We present EquiTac, a framework that exploits
the inherent SO(2) symmetry of in-hand object rotation to improve sample
efficiency and generalization for visuotactile policy learning. EquiTac first
reconstructs surface normals from raw RGB inputs of vision-based tactile
sensors, so rotations of the normal vector field correspond to in-hand object
rotations. An SO(2)-equivariant network then predicts a residual rotation
action that augments a base visuomotor policy at test time, enabling real-time
rotation correction without additional reorientation demonstrations. On a real
robot, EquiTac accurately achieves robust zero-shot generalization to unseen
in-hand orientations with very few training samples, where baselines fail even
with more training data. To our knowledge, this is the first tactile learning
method to explicitly encode tactile equivariance for policy learning, yielding
a lightweight, symmetry-aware module that improves reliability in contact-rich
tasks.

</details>


### [368] [Unified Humanoid Fall-Safety Policy from a Few Demonstrations](https://arxiv.org/abs/2511.07407)
*Zhengjie Xu,Ye Li,Kwan-yee Lin,Stella X. Yu*

Main category: cs.RO

TL;DR: 本文提出了一种融合人类示范、强化学习和自适应扩散记忆的新颖策略，实现了人形机器人在跌倒前预防、跌倒过程中减缓冲击以及跌倒后快速恢复的全流程自动化，从而提升了机器人面对真实复杂环境中的安全性与韧性。


<details>
  <summary>Details</summary>
Motivation: 现有控制和学习方法难以完全避免人形机器人跌倒，并且对摔倒的应对机制割裂，缺乏一体化的冲击缓解与恢复策略，导致机器人在不可预料的情况下安全性不足。

Method: 通过融合稀疏的人类示范数据、强化学习算法以及基于扩散的自适应记忆机制，系统地训练机器人掌握防跌、防冲击和自恢复的全身综合动作，并通过实物（Unitree G1）和仿真环境进行测试验证。

Result: 实验结果显示，新策略具备优良的仿真到现实迁移能力，在各种干扰下能有效降低冲击力，缩短恢复时间，实现了稳定性和安全性的大幅提升。

Conclusion: 该方法实现了跌倒相关全过程的智能、安全自适应控制，为未来机器人更好地适应现实复杂环境提供了可行途径，推动机器人安全性与韧性的新进展。

Abstract: Falling is an inherent risk of humanoid mobility. Maintaining stability is
thus a primary safety focus in robot control and learning, yet no existing
approach fully averts loss of balance. When instability does occur, prior work
addresses only isolated aspects of falling: avoiding falls, choreographing a
controlled descent, or standing up afterward. Consequently, humanoid robots
lack integrated strategies for impact mitigation and prompt recovery when real
falls defy these scripts. We aim to go beyond keeping balance to make the
entire fall-and-recovery process safe and autonomous: prevent falls when
possible, reduce impact when unavoidable, and stand up when fallen. By fusing
sparse human demonstrations with reinforcement learning and an adaptive
diffusion-based memory of safe reactions, we learn adaptive whole-body
behaviors that unify fall prevention, impact mitigation, and rapid recovery in
one policy. Experiments in simulation and on a Unitree G1 demonstrate robust
sim-to-real transfer, lower impact forces, and consistently fast recovery
across diverse disturbances, pointing towards safer, more resilient humanoids
in real environments. Videos are available at https://firm2025.github.io/.

</details>


### [369] [Using Vision Language Models as Closed-Loop Symbolic Planners for Robotic Applications: A Control-Theoretic Perspective](https://arxiv.org/abs/2511.07410)
*Hao Wang,Sathwik Karnik,Bea Lim,Somil Bansal*

Main category: cs.RO

TL;DR: 本文探索了在机器人控制中如何更有效地作为闭环符号规划器来使用视觉语言模型（VLMs）。作者从控制理论视角出发，分析了控制时域和热启动策略对VLM规划效果的影响，并给出了优化建议。


<details>
  <summary>Details</summary>
Motivation: 虽然大模型（LLM 和 VLM）被广泛用于具身智能的符号规划，但其不可预测性和黑盒特性导致在闭环（循环反馈调整）规划中容易出错，进而影响机器人高层次决策，因此需要探究更有效和稳健的使用方式。

Method: 作者采用控制理论视角，主要关注两点：1）控制时域（规划决策的步长/周期）；2）热启动（利用先前信息初始化规划过程）。通过设计和实施一系列受控实验，评估这些因素对VLM闭环符号规划性能的影响。

Result: 实验展示了控制时域的设定和热启动方式对VLM作为闭环规划器时的决策效率和稳健性有显著影响。合理的时域设定与热启动可明显改善VLM所产生符号规划的可靠性和效果。

Conclusion: VLM 作为闭环符号规划器时，通过调控控制时域和采用热启动方法，可以提升应用于机器人系统中的表现。作者提出了一些具备普遍性的实用建议，为后续类似任务提供了理论和实践的参考。

Abstract: Large Language Models (LLMs) and Vision Language Models (VLMs) have been
widely used for embodied symbolic planning. Yet, how to effectively use these
models for closed-loop symbolic planning remains largely unexplored. Because
they operate as black boxes, LLMs and VLMs can produce unpredictable or costly
errors, making their use in high-level robotic planning especially challenging.
In this work, we investigate how to use VLMs as closed-loop symbolic planners
for robotic applications from a control-theoretic perspective. Concretely, we
study how the control horizon and warm-starting impact the performance of VLM
symbolic planners. We design and conduct controlled experiments to gain
insights that are broadly applicable to utilizing VLMs as closed-loop symbolic
planners, and we discuss recommendations that can help improve the performance
of VLM symbolic planners.

</details>


### [370] [Robot Learning from a Physical World Model](https://arxiv.org/abs/2511.07416)
*Jiageng Mao,Sicheng He,Hao-Ning Wu,Yang You,Shuyang Sun,Zhicheng Wang,Yanan Bao,Huizhong Chen,Leonidas Guibas,Vitor Guizilini,Howard Zhou,Yue Wang*

Main category: cs.RO

TL;DR: 本文提出PhysWorld框架，将视频生成与物理世界建模结合，实现通过视频生成提升机器人学习能力，极大提升了无需真实数据时的操控准确性。


<details>
  <summary>Details</summary>
Motivation: 目前视频生成模型可以根据语言和图像合成高质量演示视频，为机器人学习提供了丰富的训练信号，但直接依据生成视频像素运动迁移到机器人行为会忽略物理世界约束，导致操控动作精度不足。

Method: PhysWorld方法从单张图片和任务指令入手，先生成与任务相关的视频，再从视频中重建物理世界结构，通过基于对象的残差强化学习，将视频中的动作转化为符合物理规律的机器人操控轨迹。无需真实机器人数据，实现零样本泛化。

Result: 在多种真实世界的操作任务中，PhysWorld显著提升了机器人执行任务的准确率，相比于以往的方法有明显优势。

Conclusion: PhysWorld展示了将隐式视觉指导转化为具备可执行性的物理操控轨迹的潜力，消除了对现实数据采集的依赖，对机器人学习领域具有积极意义和推广价值。

Abstract: We introduce PhysWorld, a framework that enables robot learning from video
generation through physical world modeling. Recent video generation models can
synthesize photorealistic visual demonstrations from language commands and
images, offering a powerful yet underexplored source of training signals for
robotics. However, directly retargeting pixel motions from generated videos to
robots neglects physics, often resulting in inaccurate manipulations. PhysWorld
addresses this limitation by coupling video generation with physical world
reconstruction. Given a single image and a task command, our method generates
task-conditioned videos and reconstructs the underlying physical world from the
videos, and the generated video motions are grounded into physically accurate
actions through object-centric residual reinforcement learning with the
physical world model. This synergy transforms implicit visual guidance into
physically executable robotic trajectories, eliminating the need for real robot
data collection and enabling zero-shot generalizable robotic manipulation.
Experiments on diverse real-world tasks demonstrate that PhysWorld
substantially improves manipulation accuracy compared to previous approaches.
Visit \href{https://pointscoder.github.io/PhysWorld_Web/}{the project webpage}
for details.

</details>


### [371] [Lightning Grasp: High Performance Procedural Grasp Synthesis with Contact Fields](https://arxiv.org/abs/2511.07418)
*Zhao-Heng Yin,Pieter Abbeel*

Main category: cs.RO

TL;DR: 提出Lightning Grasp算法，实现了灵巧手实时多样抓取的大幅提速，无需监督即可对复杂物体生成抓取方案，并开源系统推动领域发展。


<details>
  <summary>Details</summary>
Motivation: 机器人与计算机图形领域一直难以实现灵巧机械手快速、稳定、多样的实时抓取合成，现有方法对能量函数和初始化要求高，且计算复杂。本研究旨在解决这些瓶颈。

Method: 提出了一种高效的程序化抓取合成算法Lightning Grasp，核心创新是引入Contact Field数据结构，将复杂的几何计算与搜索过程分离，大幅简化问题难度和搜索速度。

Result: 方法在无需监督的情形下，大幅超越现有方法，针对不规则、工具状物体生成多样抓取，速度提升几个数量级。

Conclusion: Lightning Grasp突破了现有抓取合成的多项瓶颈，推动了机器人操作领域发展，并通过开源促进社区创新。

Abstract: Despite years of research, real-time diverse grasp synthesis for dexterous
hands remains an unsolved core challenge in robotics and computer graphics. We
present Lightning Grasp, a novel high-performance procedural grasp synthesis
algorithm that achieves orders-of-magnitude speedups over state-of-the-art
approaches, while enabling unsupervised grasp generation for irregular,
tool-like objects. The method avoids many limitations of prior approaches, such
as the need for carefully tuned energy functions and sensitive initialization.
This breakthrough is driven by a key insight: decoupling complex geometric
computation from the search process via a simple, efficient data structure -
the Contact Field. This abstraction collapses the problem complexity, enabling
a procedural search at unprecedented speeds. We open-source our system to
propel further innovation in robotic manipulation.

</details>
