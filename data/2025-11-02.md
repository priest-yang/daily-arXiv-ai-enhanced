<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 71]
- [cs.CL](#cs.CL) [Total: 58]
- [cs.RO](#cs.RO) [Total: 29]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Enhancing Underwater Object Detection through Spatio-Temporal Analysis and Spatial Attention Networks](https://arxiv.org/abs/2510.25797)
*Sai Likhith Karri,Ansh Saxena*

Main category: cs.CV

TL;DR: 本研究比较了时空建模和空间注意力机制在深度学习水下目标检测中的有效性，提出并改进了基于YOLOv5的目标检测模型。


<details>
  <summary>Details</summary>
Motivation: 水下环境中的目标检测由于动态变化、遮挡和运动等复杂性，传统检测方法存在准确率和鲁棒性不足的问题，需通过增强模型的时空建模和注意力机制来提升检测性能。

Method: 研究分两阶段：首先引入时序增强型YOLOv5（T-YOLOv5）与标准YOLOv5进行对比；然后在T-YOLOv5基础上集成卷积块注意力模块（CBAM），形成T-YOLOv5 with CBAM，并对三者在复杂海洋环境中检测性能进行测评。

Result: 实验测试结果为：YOLOv5的mAP@50-95为0.563，T-YOLOv5达到0.813，T-YOLOv5 with CBAM为0.811。改进方法在运动、遮挡、渐变等复杂场景下显著提高了检测准确率和泛化能力。

Conclusion: T-YOLOv5相比标准模型在水下目标检测中增强了检测可靠性，集成CBAM后在复杂场景表现更佳，但简单场景准确率略有下降。

Abstract: This study examines the effectiveness of spatio-temporal modeling and the
integration of spatial attention mechanisms in deep learning models for
underwater object detection. Specifically, in the first phase, the performance
of temporal-enhanced YOLOv5 variant T-YOLOv5 is evaluated, in comparison with
the standard YOLOv5. For the second phase, an augmented version of T-YOLOv5 is
developed, through the addition of a Convolutional Block Attention Module
(CBAM). By examining the effectiveness of the already pre-existing YOLOv5 and
T-YOLOv5 models and of the newly developed T-YOLOv5 with CBAM. With CBAM, the
research highlights how temporal modeling improves detection accuracy in
dynamic marine environments, particularly under conditions of sudden movements,
partial occlusions, and gradual motion. The testing results showed that YOLOv5
achieved a mAP@50-95 of 0.563, while T-YOLOv5 and T-YOLOv5 with CBAM
outperformed with mAP@50-95 scores of 0.813 and 0.811, respectively,
highlighting their superior accuracy and generalization in detecting complex
objects. The findings demonstrate that T-YOLOv5 significantly enhances
detection reliability compared to the standard model, while T-YOLOv5 with CBAM
further improves performance in challenging scenarios, although there is a loss
of accuracy when it comes to simpler scenarios.

</details>


### [2] [MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency](https://arxiv.org/abs/2510.25897)
*Nicolas Dufour,Lucas Degeorge,Arijit Ghosh,Vicky Kalogeiton,David Picard*

Main category: cs.CV

TL;DR: 本文提出了一种在训练期间结合多个奖励模型的新方法，从而直接对用户偏好进行建模，显著提升了生成图像的视觉质量和训练效率，并在多个用户偏好基准上达到了最新水平。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成模型虽然具备多样性，但往往不能很好地对齐用户偏好。现有方法多采用奖励模型在生成后进行筛选，但会丢弃有价值的数据，且只优化单一指标，导致多样性与语义一致性下降。

Method: 本文提出在训练阶段结合多个奖励模型对生成模型进行条件约束，使模型能够直接学习用户偏好，而非依赖生成后的二次筛选。该方法命名为MIRO。

Result: 实验表明，MIRO方法在GenEval组合理解基准以及PickAScore、ImageReward、HPSv2等用户偏好评分上均取得了最先进的表现，并且训练过程显著加速。

Conclusion: 通过在训练阶段直接集成多个奖励模型，可以有效提升生成图像的用户偏好对齐、视觉质量和训练效率，优于传统的事后筛选方法。

Abstract: Current text-to-image generative models are trained on large uncurated
datasets to enable diverse generation capabilities. However, this does not
align well with user preferences. Recently, reward models have been
specifically designed to perform post-hoc selection of generated images and
align them to a reward, typically user preference. This discarding of
informative data together with the optimizing for a single reward tend to harm
diversity, semantic fidelity and efficiency. Instead of this post-processing,
we propose to condition the model on multiple reward models during training to
let the model learn user preferences directly. We show that this not only
dramatically improves the visual quality of the generated images but it also
significantly speeds up the training. Our proposed method, called MIRO,
achieves state-of-the-art performances on the GenEval compositional benchmark
and user-preference scores (PickAScore, ImageReward, HPSv2).

</details>


### [3] [BikeScenes: Online LiDAR Semantic Segmentation for Bicycles](https://arxiv.org/abs/2510.25901)
*Denniz Goren,Holger Caesar*

Main category: cs.CV

TL;DR: 本文提出了一种针对自行车应用的3D LiDAR感知方法，并建立了专用数据集，用以提升自行车环境下的目标分割性能。实验显示，基于该数据集微调后，分割精度显著优于仅用汽车场景预训练的模型。


<details>
  <summary>Details</summary>
Motivation: 随着电动自行车等快速自行车的流行，骑行者的安全风险增大，但现有感知技术多为汽车设计，难以直接应用于自行车。亟需开发专属的感知与安全技术以保护骑行者。

Method: 作者开发了SenseBike多传感器平台，设计了适用于自行车的3D LiDAR分割算法，并构建了BikeScenes-lidarseg数据集（包含3021帧、29类动态与静态目标）。通过与汽车领域数据的微调对比，验证了方法的有效性。

Result: 在新建BikeScenes数据集上微调后，分割模型的mIoU达到63.6%，远优于采用汽车领域SemanticKITTI预训练的13.8%。

Conclusion: 自行车专属的数据和训练流程对提升感知性能不可或缺。BikeScenes数据集和提出的方法为后续骑行者安全相关感知研究打下重要基础，并指出了硬件受限、自行车专有挑战需持续关注。

Abstract: The vulnerability of cyclists, exacerbated by the rising popularity of faster
e-bikes, motivates adapting automotive perception technologies for bicycle
safety. We use our multi-sensor 'SenseBike' research platform to develop and
evaluate a 3D LiDAR segmentation approach tailored to bicycles. To bridge the
automotive-to-bicycle domain gap, we introduce the novel BikeScenes-lidarseg
Dataset, comprising 3021 consecutive LiDAR scans around the university campus
of the TU Delft, semantically annotated for 29 dynamic and static classes. By
evaluating model performance, we demonstrate that fine-tuning on our BikeScenes
dataset achieves a mean Intersection-over-Union (mIoU) of 63.6%, significantly
outperforming the 13.8% obtained with SemanticKITTI pre-training alone. This
result underscores the necessity and effectiveness of domain-specific training.
We highlight key challenges specific to bicycle-mounted, hardware-constrained
perception systems and contribute the BikeScenes dataset as a resource for
advancing research in cyclist-centric LiDAR segmentation.

</details>


### [4] [Generative Image Restoration and Super-Resolution using Physics-Informed Synthetic Data for Scanning Tunneling Microscopy](https://arxiv.org/abs/2510.25921)
*Nikola L. Kolev,Tommaso Rodani,Neil J. Curson,Taylor J. Z. Stock,Alberto Cazzaniga*

Main category: cs.CV

TL;DR: 本文通过机器学习方法对扫描隧道显微镜（STM）图像进行修复和超分辨处理，提升成像效率和质量。


<details>
  <summary>Details</summary>
Motivation: STM技术尽管可以获得原子级分辨率，但常因探针退化和成像速度慢受限，同时探针在制备和使用中的形貌变化也影响实验效率。急需一种提升图像采集效率并减少维护的方法。

Method: 作者利用36张高质量的Si(001):H实验图像，建立了基于物理知识的合成数据生成流程，训练了流匹配和扩散类的先进生成模型。模型不仅修复损坏图像，还实现了基于稀疏数据的高质量重构。

Result: 用CLIP最大均值差异（CMMD）、结构相似性等指标量化评估，模型能够有效还原STM图像，可实现2-4倍采集速度提升。

Conclusion: 该机器学习框架大幅提升STM实验效率，有望减少探针维护频率，提升高帧率STM系统成像速度，对未来STM应用具有重要推动作用。

Abstract: Scanning tunnelling microscopy (STM) enables atomic-resolution imaging and
atom manipulation, but its utility is often limited by tip degradation and slow
serial data acquisition. Fabrication adds another layer of complexity since the
tip is often subjected to large voltages, which may alter the shape of its
apex, requiring it to be conditioned. Here, we propose a machine learning (ML)
approach for image repair and super-resolution to alleviate both challenges.
Using a dataset of only 36 pristine experimental images of Si(001):H, we
demonstrate that a physics-informed synthetic data generation pipeline can be
used to train several state-of-the-art flow-matching and diffusion models.
Quantitative evaluation with metrics such as the CLIP Maximum Mean Discrepancy
(CMMD) score and structural similarity demonstrates that our models are able to
effectively restore images and offer a two- to fourfold reduction in image
acquisition time by accurately reconstructing images from sparsely sampled
data. Our framework has the potential to significantly increase STM
experimental throughput by offering a route to reducing the frequency of
tip-conditioning procedures and to enhancing frame rates in existing high-speed
STM systems.

</details>


### [5] [SplitFlow: Flow Decomposition for Inversion-Free Text-to-Image Editing](https://arxiv.org/abs/2510.25970)
*Sung-Hoon Yoon,Minghan Li,Gaspard Beaudouin,Congcong Wen,Muhammad Rafay Azhar,Mengyu Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的无反演流分解与聚合框架，用于提升图像编辑任务中的语义对齐和多样性，显著优于现有零样本编辑方法。


<details>
  <summary>Details</summary>
Motivation: 虽然rectified flow在图像生成任务中表现出色，但在图像编辑时面临两个关键问题：真实图像难以准确反演至潜在空间，以及在编辑时梯度纠缠导致输出与目标描述不一致。现有的基于ODE的方法虽然规避了反演，但编辑效果仍不理想。因此需要新的方法提升编辑的准确性和语义解耦。

Method: 作者提出一种基于无反演的流分解与聚合框架：首先将目标提示（prompt）语义分解为多个子提示，分别为每个子目标计算独立flow，再通过带有投影和软聚合机制的权重聚合这些子流。该机制通过借鉴多任务学习中的梯度冲突解决方法，自适应调整各子目标的流动方向以减少语义冗余，并突出不同属性的区分。

Result: 实验证明该方法在图像编辑中的语义保真度和属性解耦性指标上，优于目前主流的零样本图像编辑方法。

Conclusion: 通过将流模型分解到多个子任务并智能聚合，本文有效地解决了现有方法在图像编辑中语义对齐不精确和多样性不足的问题，推动了无反演图像编辑技术的发展，并已开源代码方便后续研究。

Abstract: Rectified flow models have become a de facto standard in image generation due
to their stable sampling trajectories and high-fidelity outputs. Despite their
strong generative capabilities, they face critical limitations in image editing
tasks: inaccurate inversion processes for mapping real images back into the
latent space, and gradient entanglement issues during editing often result in
outputs that do not faithfully reflect the target prompt. Recent efforts have
attempted to directly map source and target distributions via ODE-based
approaches without inversion; however,these methods still yield suboptimal
editing quality. In this work, we propose a flow decomposition-and-aggregation
framework built upon an inversion-free formulation to address these
limitations. Specifically, we semantically decompose the target prompt into
multiple sub-prompts, compute an independent flow for each, and aggregate them
to form a unified editing trajectory. While we empirically observe that
decomposing the original flow enhances diversity in the target space,
generating semantically aligned outputs still requires consistent guidance
toward the full target prompt. To this end, we design a projection and
soft-aggregation mechanism for flow, inspired by gradient conflict resolution
in multi-task learning. This approach adaptively weights the sub-target
velocity fields, suppressing semantic redundancy while emphasizing distinct
directions, thereby preserving both diversity and consistency in the final
edited output. Experimental results demonstrate that our method outperforms
existing zero-shot editing approaches in terms of semantic fidelity and
attribute disentanglement. The code is available at
https://github.com/Harvard-AI-and-Robotics-Lab/SplitFlow.

</details>


### [6] [Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer](https://arxiv.org/abs/2510.25976)
*Roman Beliy,Amit Zalcher,Jonathan Kogman,Navve Wasserman,Michal Irani*

Main category: cs.CV

TL;DR: 本文提出了一种名为Brain-IT的新方法，通过引入Brain Interaction Transformer（BIT），提升了利用fMRI脑信号重建人类所见图像的准确性和保真度。


<details>
  <summary>Details</summary>
Motivation: 虽然利用扩散模型用fMRI信号重建图像取得进展，但现有方法对真实图像的还原度较低，因此亟需更能忠实反映大脑视觉内容的图像重建方法。

Method: Brain-IT通过Brain Interaction Transformer (BIT)实现脑内功能相似体素的聚类交互，这些聚类跨个体共享，并用于引导图像重建。BIT同时预测高层语义特征（指导扩散模型捕获图像语义内容）和低层结构特征（帮助初始化图像粗略布局），实现脑信号到图像局部特征的信息直接映射。此外，所有模型组件在不同聚类和受试者之间共享，提高数据利用效率。

Result: Brain-IT能生成对本来图像更具还原度的重建图像，无论是主观视觉效果还是客观指标均优于现有主流方法。即使只有1小时的新受试者fMRI数据，效果也可媲美用40小时数据训练的现有方法。

Conclusion: Brain-IT显著提升了fMRI到图像的重建质量，使其对受试者视觉输入的还原更准确，同时大幅减少了新受试者数据需求，推进了该领域的实际应用潜力。

Abstract: Reconstructing images seen by people from their fMRI brain recordings
provides a non-invasive window into the human brain. Despite recent progress
enabled by diffusion models, current methods often lack faithfulness to the
actual seen images. We present "Brain-IT", a brain-inspired approach that
addresses this challenge through a Brain Interaction Transformer (BIT),
allowing effective interactions between clusters of functionally-similar
brain-voxels. These functional-clusters are shared by all subjects, serving as
building blocks for integrating information both within and across brains. All
model components are shared by all clusters & subjects, allowing efficient
training with a limited amount of data. To guide the image reconstruction, BIT
predicts two complementary localized patch-level image features: (i)high-level
semantic features which steer the diffusion model toward the correct semantic
content of the image; and (ii)low-level structural features which help to
initialize the diffusion process with the correct coarse layout of the image.
BIT's design enables direct flow of information from brain-voxel clusters to
localized image features. Through these principles, our method achieves image
reconstructions from fMRI that faithfully reconstruct the seen images, and
surpass current SotA approaches both visually and by standard objective
metrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve
results comparable to current methods trained on full 40-hour recordings.

</details>


### [7] [Fine-tuning Segment Anything for Real-Time Tumor Tracking in Cine-MRI](https://arxiv.org/abs/2510.25990)
*Valentin Boussot,Cédric Hémon,Jean-Claude Nunes,Jean-Louis Dillenseger*

Main category: cs.CV

TL;DR: 本论文提出了一种基于SAM 2.1变体的prompt交互分割方法，用于在数据稀缺和实时要求下进行肿瘤追踪，实现了TrackRAD2025挑战赛第六名的成绩。


<details>
  <summary>Details</summary>
Motivation: 在胸腹部cine-MRI序列中进行实时肿瘤追踪对于肿瘤放疗具有重要意义，但面临着强数据稀缺和极短推理时间（1秒）等挑战。现有方法在数据少或速度快等场景下难以达标，因此需要针对这些约束提出创新方法。

Method: 作者探索了两种互补策略：(1) 利用IMPACT相似性度量的无监督配准方法；(2) 基于SAM 2.1及衍生变体，结合提示（prompt）交互的分割方法。最终因1秒推理时间要求，选择了SAM-based方法，采用第一张标注切片的掩膜作为提示，仅在TrackRAD2025的小型标注子集上微调。训练细致控制过拟合，用1024x1024 patch（batch size=1）、标准增强、Dice+IoU损失、低学习率、全参数微调，训练300轮。

Result: 最终模型在验证集Dice指标最高时被选为最终模型。在TrackRAD2025隐藏测试集上取得Dice分数0.8794，综合排名第六。测试时采用和训练一致的推理策略，不使用TTA。

Conclusion: 本研究表明，基础模型（SAM及其变体）结合少量数据微调和巧妙prompt策略，有潜力在MRI实时肿瘤追踪中达到高精度和高效率，对放疗具有重要应用价值。

Abstract: In this work, we address the TrackRAD2025 challenge of real-time tumor
tracking in cine-MRI sequences of the thoracic and abdominal regions under
strong data scarcity constraints. Two complementary strategies were explored:
(i) unsupervised registration with the IMPACT similarity metric and (ii)
foundation model-based segmentation leveraging SAM 2.1 and its recent variants
through prompt-based interaction. Due to the one-second runtime constraint, the
SAM-based method was ultimately selected. The final configuration used SAM2.1
b+ with mask-based prompts from the first annotated slice, fine-tuned solely on
the small labeled subset from TrackRAD2025. Training was configured to minimize
overfitting, using 1024x1024 patches (batch size 1), standard augmentations,
and a balanced Dice + IoU loss. A low uniform learning rate (0.0001) was
applied to all modules (prompt encoder, decoder, Hiera backbone) to preserve
generalization while adapting to annotator-specific styles. Training lasted 300
epochs (~12h on RTX A6000, 48GB). The same inference strategy was consistently
applied across all anatomical sites and MRI field strengths. Test-time
augmentation was considered but ultimately discarded due to negligible
performance gains. The final model was selected based on the highest Dice
Similarity Coefficient achieved on the validation set after fine-tuning. On the
hidden test set, the model reached a Dice score of 0.8794, ranking 6th overall
in the TrackRAD2025 challenge. These results highlight the strong potential of
foundation models for accurate and real-time tumor tracking in MRI-guided
radiotherapy.

</details>


### [8] [Larger Hausdorff Dimension in Scanning Pattern Facilitates Mamba-Based Methods in Low-Light Image Enhancement](https://arxiv.org/abs/2510.26001)
*Xinhua Wang,Caibo Feng,Xiangjun Fu,Chunxiao Liu*

Main category: cs.CV

TL;DR: 本文提出对Mamba框架进行创新性增强，通过引入Hilbert Selective Scan机制，提高扫描模式的Hausdorff维数，从而提升在低光照图像增强任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有Mamba框架在特征空间覆盖和细节捕捉方面存在不足，难以兼顾信息一致性、空间局部性与长程依赖的处理，因此需要一种更有效的扫描模式。

Method: 作者设计了一种新的Hilbert Selective Scan机制，从理论上提升了扫描路径的Hausdorff维数，使特征空间探索更充分，能够更好地捕获细致的局部信息和较大范围的依赖关系。

Result: 在多个公开基准上的大量实验表明，该方法不仅提升了现有Mamba框架的定量指标和可视化真实感，还减少了计算资源消耗与推理时延。

Conclusion: 该策略显著推动了低光照图像增强领域Mamba技术的前沿进展，同时具备在其他相关领域推广应用的潜力。

Abstract: We propose an innovative enhancement to the Mamba framework by increasing the
Hausdorff dimension of its scanning pattern through a novel Hilbert Selective
Scan mechanism. This mechanism explores the feature space more effectively,
capturing intricate fine-scale details and improving overall coverage. As a
result, it mitigates information inconsistencies while refining spatial
locality to better capture subtle local interactions without sacrificing the
model's ability to handle long-range dependencies. Extensive experiments on
publicly available benchmarks demonstrate that our approach significantly
improves both the quantitative metrics and qualitative visual fidelity of
existing Mamba-based low-light image enhancement methods, all while reducing
computational resource consumption and shortening inference time. We believe
that this refined strategy not only advances the state-of-the-art in low-light
image enhancement but also holds promise for broader applications in fields
that leverage Mamba-based techniques.

</details>


### [9] [CAVE: Detecting and Explaining Commonsense Anomalies in Visual Environments](https://arxiv.org/abs/2510.26006)
*Rishika Bhagwatkar,Syrielle Montariol,Angelika Romanou,Beatriz Borges,Irina Rish,Antoine Bosselut*

Main category: cs.CV

TL;DR: 本文提出了CAVE，这是第一个包含真实世界视觉异常的基准数据集，并支持异常的描述、解释和合理化等任务，通过细致的标注为研究视觉-语言模型（VLMs）的异常检测和理解能力提供了评测框架。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉领域对异常检测的研究多局限于工业缺陷或人为合成异常，难以涵盖真实世界中异常的多样性和不可预测性。本研究动机在于推动对真实视觉异常的全面理解和研究。

Method: 作者构建并发布了CAVE基准，涵盖真实世界视觉异常的样本和多维度细致标注，包括异常类型、复杂性、严重程度和普遍性等，并借鉴认知科学中人类识别和处理异常的机制。基于该基准，评测了多种主流视觉-语言模型在异常检测、理解及推理方面的表现。

Result: 实验结果显示，当前最先进的视觉-语言模型在异常感知和常识推理任务上表现不佳，即便采用了先进的提示策略，也未能有效完成任务。

Conclusion: CAVE数据集为视觉-语言模型在异常检测和常识推理能力的研究提供了真实且具认知学基础的新基准，有助于未来相关领域的突破性进展。

Abstract: Humans can naturally identify, reason about, and explain anomalies in their
environment. In computer vision, this long-standing challenge remains limited
to industrial defects or unrealistic, synthetically generated anomalies,
failing to capture the richness and unpredictability of real-world anomalies.
In this work, we introduce CAVE, the first benchmark of real-world visual
anomalies. CAVE supports three open-ended tasks: anomaly description,
explanation, and justification; with fine-grained annotations for visual
grounding and categorizing anomalies based on their visual manifestations,
their complexity, severity, and commonness. These annotations draw inspiration
from cognitive science research on how humans identify and resolve anomalies,
providing a comprehensive framework for evaluating Vision-Language Models
(VLMs) in detecting and understanding anomalies. We show that state-of-the-art
VLMs struggle with visual anomaly perception and commonsense reasoning, even
with advanced prompting strategies. By offering a realistic and cognitively
grounded benchmark, CAVE serves as a valuable resource for advancing research
in anomaly detection and commonsense reasoning in VLMs.

</details>


### [10] [Climate Adaptation-Aware Flood Prediction for Coastal Cities Using Deep Learning](https://arxiv.org/abs/2510.26017)
*Bilal Hassan,Areg Karapetyan,Aaron Chung Hin Chow,Samer Madanat*

Main category: cs.CV

TL;DR: 本文提出了一种基于CNN的深度学习模型，用于在不同海平面上升和海岸适应情景下预测沿海洪水，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 气候变化和海平面上升威胁日益加剧，沿海城市急需高效且精准的洪水风险预测工具。传统的基于物理的水动力模型虽然精确但运行成本高、难以应用于城市级别的规划，因此需要寻找高效、可扩展的方法。

Method: 采用了一种最新的视觉驱动、低资源需求的深度学习框架，开发了轻量级卷积神经网络（CNN）模型，根据不同的海平面上升预测和岸线适应情境，预测沿海洪水情况。模型在阿布扎比和旧金山两个地区的数据集上进行了泛化与测试。

Result: 该方法在洪水深度地图的平均绝对误差（MAE）方面，相较当前最先进方法平均提升约20%。

Conclusion: 提出的方法在沿海洪水风险预测方面表现优异，具有良好的泛化能力和高效性，有望成为沿海防灾管理的实用和可扩展工具，帮助决策者制定有效的缓解策略。

Abstract: Climate change and sea-level rise (SLR) pose escalating threats to coastal
cities, intensifying the need for efficient and accurate methods to predict
potential flood hazards. Traditional physics-based hydrodynamic simulators,
although precise, are computationally expensive and impractical for city-scale
coastal planning applications. Deep Learning (DL) techniques offer promising
alternatives, however, they are often constrained by challenges such as data
scarcity and high-dimensional output requirements. Leveraging a recently
proposed vision-based, low-resource DL framework, we develop a novel,
lightweight Convolutional Neural Network (CNN)-based model designed to predict
coastal flooding under variable SLR projections and shoreline adaptation
scenarios. Furthermore, we demonstrate the ability of the model to generalize
across diverse geographical contexts by utilizing datasets from two distinct
regions: Abu Dhabi and San Francisco. Our findings demonstrate that the
proposed model significantly outperforms state-of-the-art methods, reducing the
mean absolute error (MAE) in predicted flood depth maps on average by nearly
20%. These results highlight the potential of our approach to serve as a
scalable and practical tool for coastal flood management, empowering
decision-makers to develop effective mitigation strategies in response to the
growing impacts of climate change. Project Page: https://caspiannet.github.io/

</details>


### [11] [Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders](https://arxiv.org/abs/2510.26027)
*Ali Rasekh,Erfan Bagheri Soula,Omid Daliran,Simon Gottschalk,Mohsen Fayyaz*

Main category: cs.CV

TL;DR: 本文提出了一种在视觉编码器中直接引入堆叠时序注意力模块的Video-LLM新架构，显著提升了视频时序推理能力，在多个基准测试集上取得最优成绩。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视频理解方面，尤其是复杂时序动态的把握上仍存在明显不足，现有架构难以处理涉及动作序列与时序进展的理解任务。

Method: 在视觉编码器内加入堆叠时序注意力模块，通过在视觉特征提取早期阶段强化时序信息的建模，从而为下游的大语言模型提供更精准的时序特征表征。

Result: 新方法在VITATECS、MVBench和Video-MME等多项视频问答与动作识别任务上相较现有模型有显著提升，最高性能提升达5.5%。

Conclusion: 增强视觉编码器的时序结构，有效填补了Video-LLM在视频复杂时序理解方面的短板，为相关任务带来新的性能突破。

Abstract: Despite significant advances in Multimodal Large Language Models (MLLMs),
understanding complex temporal dynamics in videos remains a major challenge.
Our experiments show that current Video Large Language Model (Video-LLM)
architectures have critical limitations in temporal understanding, struggling
with tasks that require detailed comprehension of action sequences and temporal
progression. In this work, we propose a Video-LLM architecture that introduces
stacked temporal attention modules directly within the vision encoder. This
design incorporates a temporal attention in vision encoder, enabling the model
to better capture the progression of actions and the relationships between
frames before passing visual tokens to the LLM. Our results show that this
approach significantly improves temporal reasoning and outperforms existing
models in video question answering tasks, specifically in action recognition.
We improve on benchmarks including VITATECS, MVBench, and Video-MME by up to
+5.5%. By enhancing the vision encoder with temporal structure, we address a
critical gap in video understanding for Video-LLMs. Project page and code are
available at: https://alirasekh.github.io/STAVEQ2/.

</details>


### [12] [FlexICL: A Flexible Visual In-context Learning Framework for Elbow and Wrist Ultrasound Segmentation](https://arxiv.org/abs/2510.26049)
*Yuyue Zhou,Jessica Knight,Shrimanti Ghosh,Banafshe Felfeliyan,Jacob L. Jaremko,Abhilash R. Hareendranathan*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的动态图像分割框架FlexICL，显著提升了医疗超声图像中骨结构分割的效率和准确性，仅需极少的标注数据即可超越现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 儿童肘部和腕部骨折常见，超声图像辅助诊断对自动化骨结构分割有迫切需求，但精确像素级标注通常费时费力，限制了深度学习方法的推广。因此，急需一种能够减少标注需求并提升分割效果的新方法。

Method: 作者提出了FlexICL框架，将in-context learning原理应用于超声视频图像分割，只需少量标注帧即能自动分割其他帧。方法上，系统评估与创新了多种图像拼接和训练策略，并结合多重数据增强方案，从而优化视觉ICL表现。

Result: 在四个腕部和肘部超声数据集上，FlexICL只需5%的训练样本便实现了强健分割，其Dice系数比Painter、MAE-VQGAN等当前主流视觉ICL模型以及传统U-Net、TransUNet高出1-27%。

Conclusion: FlexICL为医疗超声图像分割提供了高效、可扩展的方案，特别适用于标注样本稀缺的医学场景，具有广阔应用前景。

Abstract: Elbow and wrist fractures are the most common fractures in pediatric
populations. Automatic segmentation of musculoskeletal structures in ultrasound
(US) can improve diagnostic accuracy and treatment planning. Fractures appear
as cortical defects but require expert interpretation. Deep learning (DL) can
provide real-time feedback and highlight key structures, helping lightly
trained users perform exams more confidently. However, pixel-wise expert
annotations for training remain time-consuming and costly. To address this
challenge, we propose FlexICL, a novel and flexible in-context learning (ICL)
framework for segmenting bony regions in US images. We apply it to an
intra-video segmentation setting, where experts annotate only a small subset of
frames, and the model segments unseen frames. We systematically investigate
various image concatenation techniques and training strategies for visual ICL
and introduce novel concatenation methods that significantly enhance model
performance with limited labeled data. By integrating multiple augmentation
strategies, FlexICL achieves robust segmentation performance across four wrist
and elbow US datasets while requiring only 5% of the training images. It
outperforms state-of-the-art visual ICL models like Painter, MAE-VQGAN, and
conventional segmentation models like U-Net and TransUNet by 1-27% Dice
coefficient on 1,252 US sweeps. These initial results highlight the potential
of FlexICL as an efficient and scalable solution for US image segmentation well
suited for medical imaging use cases where labeled data is scarce.

</details>


### [13] [Dynamic VLM-Guided Negative Prompting for Diffusion Models](https://arxiv.org/abs/2510.26052)
*Hoyeon Chang,Seungjin Kim,Yoonseok Choi*

Main category: cs.CV

TL;DR: 本文提出一种基于视觉-语言模型（VLM）的扩散模型动态负向提示新方法，并在多个数据集上验证了性能。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型负向提示方法使用固定的负向提示，缺乏灵活性和上下文感知能力，影响了负向指导的有效性。作者希望提升负向提示的自适应性和文本-图像对齐效果。

Method: 在去噪过程中，动态生成中间图像预测，然后利用VLM根据这些图像生成上下文相关的负向提示，实现每一步的自适应负向引导。

Result: 该方法在多个基准数据集上进行评估，实验表明负向引导强度与文本-图像对齐精度之间存在权衡，可以根据具体需求优化。

Conclusion: 与传统固定负向提示方法相比，本文方法在提升适应性和生成质量方面表现优异，为扩散模型负向提示提供了新的思路。

Abstract: We propose a novel approach for dynamic negative prompting in diffusion
models that leverages Vision-Language Models (VLMs) to adaptively generate
negative prompts during the denoising process. Unlike traditional Negative
Prompting methods that use fixed negative prompts, our method generates
intermediate image predictions at specific denoising steps and queries a VLM to
produce contextually appropriate negative prompts. We evaluate our approach on
various benchmark datasets and demonstrate the trade-offs between negative
guidance strength and text-image alignment.

</details>


### [14] [Security Risk of Misalignment between Text and Image in Multi-modal Model](https://arxiv.org/abs/2510.26105)
*Xiaosen Wang,Zhijin Ge,Shaokang Wang*

Main category: cs.CV

TL;DR: 本文揭示并演示了多模态扩散模型（如文本到图像模型）在文本与图像对齐上的漏洞，并提出了一种仅通过修改输入图像即可影响生成内容的新型攻击方法PReMA，显著威胁了这类模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 多模态扩散模型在文本和图像的交互生成方面应用广泛，但其对抗性输入防御不足，特别是在模型生成不当或NSFW内容方面存在安全隐患。研究者希望探索并加强这类模型在安全性和可靠性上的能力。

Method: 作者提出了Prompt-Restricted Multi-modal Attack（PReMA）攻击方法，即在不修改文本提示的情况下，通过对输入图像的修改，实现对生成内容的操控。这一方法与以往主要依赖对抗性文本提示的攻击方式不同，具备独特的威胁属性。作者在图像修复和风格迁移等多个任务及模型上对PReMA进行了系统测试。

Result: 实验结果显示，PReMA能够有效操控生成内容，在多种任务和模型上均表现出较强的攻击能力，显著突破了现有对抗性提示方法的局限。

Conclusion: 多模态扩散模型在文本与图像对齐上存在安全隐患，PReMA为这类模型带来了新的安全威胁，特别是在固定提示词的图像编辑场景下。研究呼吁需进一步关注和防御这类对抗性攻击。

Abstract: Despite the notable advancements and versatility of multi-modal diffusion
models, such as text-to-image models, their susceptibility to adversarial
inputs remains underexplored. Contrary to expectations, our investigations
reveal that the alignment between textual and Image modalities in existing
diffusion models is inadequate. This misalignment presents significant risks,
especially in the generation of inappropriate or Not-Safe-For-Work (NSFW)
content. To this end, we propose a novel attack called Prompt-Restricted
Multi-modal Attack (PReMA) to manipulate the generated content by modifying the
input image in conjunction with any specified prompt, without altering the
prompt itself. PReMA is the first attack that manipulates model outputs by
solely creating adversarial images, distinguishing itself from prior methods
that primarily generate adversarial prompts to produce NSFW content.
Consequently, PReMA poses a novel threat to the integrity of multi-modal
diffusion models, particularly in image-editing applications that operate with
fixed prompts. Comprehensive evaluations conducted on image inpainting and
style transfer tasks across various models confirm the potent efficacy of
PReMA.

</details>


### [15] [EgoExo-Con: Exploring View-Invariant Video Temporal Understanding](https://arxiv.org/abs/2510.26113)
*Minjoon Jung,Junbin Xiao,Junghyun Kim,Byoung-Tak Zhang,Angela Yao*

Main category: cs.CV

TL;DR: 本文提出了EgoExo-Con基准，专注于评估视频大语言模型（Video-LLMs）不同视角下的时序一致性能力，并提出强化学习方法View-GRPO，有效提升视角间时序理解一致性。


<details>
  <summary>Details</summary>
Motivation: 随着多视角视频数据的普及，如何保证Video-LLMs在不同视点采集的同一事件下表现出一致的时序理解变得越来越重要。但目前缺乏专门的基准用以系统评估其跨视角一致性能力。

Method: 作者提出EgoExo-Con基准，包含同步采集的第一人称（egocentric）和第三人称（exocentric）视频对，并设计了两项时序理解任务：时序验证（Temporal Verification）和时序定位（Temporal Grounding），结合人工精炼的自然语言查询，对模型的正确性与一致性进行评估。此外，作者还提出了View-GRPO，一种强化学习方法，旨在提升模型跨视角时序推理能力，一致性更高。

Result: 分析发现，现有Video-LLMs在多视角一致性表现远逊于单视角表现。通过简单联合微调虽能提升一致性，但单模型整体表现不佳。提出的View-GRPO方法在提升跨视角一致性表现上明显优于传统微调与GRPO方法。

Conclusion: 目前Video-LLMs在不同视角的时序理解一致性方面存在显著不足。新提出的EgoExo-Con提供了系统评测手段，View-GRPO方法有效提升了跨视角时序理解一致性，为未来基于多视角的Video-LLMs研究指明方向。

Abstract: Can Video-LLMs achieve consistent temporal understanding when videos capture
the same event from different viewpoints? To study this, we introduce
EgoExo-Con (Consistency), a benchmark of comprehensively synchronized
egocentric and exocentric video pairs with human-refined queries in natural
language. EgoExo-Con emphasizes two temporal understanding tasks: Temporal
Verification and Temporal Grounding. It evaluates not only correctness but
consistency across viewpoints. Our analysis reveals two critical limitations of
existing Video-LLMs: (1) models often fail to maintain consistency, with
results far worse than their single-view performances. (2) When naively
finetuned with synchronized videos of both viewpoints, the models show improved
consistency but often underperform those trained on a single view. For
improvements, we propose View-GRPO, a novel reinforcement learning framework
that effectively strengthens view-specific temporal reasoning while encouraging
consistent comprehension across viewpoints. Our method demonstrates its
superiority over naive SFT and GRPO, especially for improving cross-view
consistency. All resources will be made publicly available.

</details>


### [16] [OracleAgent: A Multimodal Reasoning Agent for Oracle Bone Script Research](https://arxiv.org/abs/2510.26114)
*Caoshuo Li,Zengmao Ding,Xiaobin Hu,Bang Li,Donghao Luo,Xu Peng,Taisong Jin,Yongge Liu,Shengwei Han,Jing Yang,Xiaoping He,Feng Gao,AndyPian Wu,SevenShu,Chaoyang Wang,Chengjie Wang*

Main category: cs.CV

TL;DR: 本文提出了OracleAgent，这是首个面向甲骨文（OBS）相关信息的结构化管理与检索的智能体系统，集成多模态工具与大模型，极大提升了OBS研究效率。


<details>
  <summary>Details</summary>
Motivation: 甲骨文是最早的文字系统之一，保存了丰富的历史文化价值。但甲骨文研究流程复杂，且信息整理和检索效率低下，成为研究瓶颈。

Method: 提出OracleAgent，集成多种甲骨文分析工具和大语言模型（LLMs），能够灵活调度组件；同时建立了超过140万张甲骨文字照片和8万条释文的大规模专业多模态知识库；OracleAgent通过多模态工具辅助专家实现高效的字符、文献、释文和照片检索。

Result: 实验证明OracleAgent在多模态推理与生成任务上表现优于主流多模态大模型（如GPT-4o）；案例研究显示其显著减少了专家的研究时间。

Conclusion: OracleAgent为OBS辅助研究和自动释读系统的实际应用迈出了重要一步，可极大提高甲骨文研究的自动化和效率。

Abstract: As one of the earliest writing systems, Oracle Bone Script (OBS) preserves
the cultural and intellectual heritage of ancient civilizations. However,
current OBS research faces two major challenges: (1) the interpretation of OBS
involves a complex workflow comprising multiple serial and parallel sub-tasks,
and (2) the efficiency of OBS information organization and retrieval remains a
critical bottleneck, as scholars often spend substantial effort searching for,
compiling, and managing relevant resources. To address these challenges, we
present OracleAgent, the first agent system designed for the structured
management and retrieval of OBS-related information. OracleAgent seamlessly
integrates multiple OBS analysis tools, empowered by large language models
(LLMs), and can flexibly orchestrate these components. Additionally, we
construct a comprehensive domain-specific multimodal knowledge base for OBS,
which is built through a rigorous multi-year process of data collection,
cleaning, and expert annotation. The knowledge base comprises over 1.4M
single-character rubbing images and 80K interpretation texts. OracleAgent
leverages this resource through its multimodal tools to assist experts in
retrieval tasks of character, document, interpretation text, and rubbing image.
Extensive experiments demonstrate that OracleAgent achieves superior
performance across a range of multimodal reasoning and generation tasks,
surpassing leading mainstream multimodal large language models (MLLMs) (e.g.,
GPT-4o). Furthermore, our case study illustrates that OracleAgent can
effectively assist domain experts, significantly reducing the time cost of OBS
research. These results highlight OracleAgent as a significant step toward the
practical deployment of OBS-assisted research and automated interpretation
systems.

</details>


### [17] [JOGS: Joint Optimization of Pose Estimation and 3D Gaussian Splatting](https://arxiv.org/abs/2510.26117)
*Yuxuan Li,Tao Wang,Xianben Yang*

Main category: cs.CV

TL;DR: 提出了一种不依赖预先估计相机位姿的新型新视角合成（NVS）方法，通过共同优化3D高斯点与相机位姿提升重建质量，超过现有算法。


<details>
  <summary>Details</summary>
Motivation: 现有的新视角合成方法通常依赖外部相机位姿估计工具（如COLMAP），这些工具计算量大且容易引入误差，影响最终效果。因此，研究者希望移除对预估相机位姿的依赖，提升效率与重建精度。

Method: 提出端到端联合优化3D高斯点与相机位姿的新框架。利用交替式两阶段优化：先在固定位姿下通过可微渲染优化3D高斯参数，再用结合几何与光度约束的自定义3D光流算法精细优化相机位姿。重复迭代，强化两者精度。

Result: 新方法在多个数据集上效果显著优于现有不依赖COLMAP的方法，并且整体上也优于标准的COLMAP基线；尤其在视角变化大、特征稀疏的困难场景表现突出。

Conclusion: 该方法有效克服了传统新视角合成流程对预估位姿的依赖，提高了重建效果与相机姿态估计精度，为相关应用带来新思路。

Abstract: Traditional novel view synthesis methods heavily rely on external camera pose
estimation tools such as COLMAP, which often introduce computational
bottlenecks and propagate errors. To address these challenges, we propose a
unified framework that jointly optimizes 3D Gaussian points and camera poses
without requiring pre-calibrated inputs. Our approach iteratively refines 3D
Gaussian parameters and updates camera poses through a novel co-optimization
strategy, ensuring simultaneous improvements in scene reconstruction fidelity
and pose accuracy. The key innovation lies in decoupling the joint optimization
into two interleaved phases: first, updating 3D Gaussian parameters via
differentiable rendering with fixed poses, and second, refining camera poses
using a customized 3D optical flow algorithm that incorporates geometric and
photometric constraints. This formulation progressively reduces projection
errors, particularly in challenging scenarios with large viewpoint variations
and sparse feature distributions, where traditional methods struggle. Extensive
evaluations on multiple datasets demonstrate that our approach significantly
outperforms existing COLMAP-free techniques in reconstruction quality, and also
surpasses the standard COLMAP-based baseline in general.

</details>


### [18] [WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios](https://arxiv.org/abs/2510.26125)
*Runsheng Xu,Hubert Lin,Wonseok Jeon,Hao Feng,Yuliang Zou,Liting Sun,John Gorman,Kate Tolstaya,Sarah Tang,Brandyn White,Ben Sapp,Mingxing Tan,Jyh-Jing Hwang,Drago Anguelov*

Main category: cs.CV

TL;DR: 本文提出了面向复杂驾驶场景的端到端自动驾驶数据集WOD-E2E，并引入了新评价指标RFS，提高了对长尾场景下自动驾驶系统的评估能力。


<details>
  <summary>Details</summary>
Motivation: 当前的端到端自动驾驶评测集中在常规场景，无法充分测试系统在罕见、复杂长尾情景下的表现，且传统开放环评价指标难以反映驾驶的多模态性和复杂性。

Method: 作者构建了包含4021个驾驶片段、总时长约12小时的WOD-E2E数据集，专门针对低于0.03%概率出现的挑战性长尾场景。每段数据包括高层路由信息、自车状态、8路360度相机图像，并为验证集全部片段标注了人工偏好轨迹。评价方面，提出了RFS指标，基于人工优选轨迹而非单纯与log数据的误差作为参考。

Result: 数据集和评价指标已公布，全部验证集已提供人工偏好标注，测试集用于2025挑战赛。新方法更好地促进了对复杂驾驶场景的广泛和深入研究。

Conclusion: WOD-E2E数据集和RFS评价机制为稳定、可泛化、安全的端到端自动驾驶研究提供了新的基准和工具，有望推动自动驾驶系统在复杂现实环境下的进步。

Abstract: Vision-based end-to-end (E2E) driving has garnered significant interest in
the research community due to its scalability and synergy with multimodal large
language models (MLLMs). However, current E2E driving benchmarks primarily
feature nominal scenarios, failing to adequately test the true potential of
these systems. Furthermore, existing open-loop evaluation metrics often fall
short in capturing the multi-modal nature of driving or effectively evaluating
performance in long-tail scenarios. To address these gaps, we introduce the
Waymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021
driving segments (approximately 12 hours), specifically curated for challenging
long-tail scenarios that that are rare in daily life with an occurring
frequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the
high-level routing information, ego states, and 360-degree camera views from 8
surrounding cameras. To evaluate the E2E driving performance on these long-tail
situations, we propose a novel open-loop evaluation metric: Rater Feedback
Score (RFS). Unlike conventional metrics that measure the distance between
predicted way points and the logs, RFS measures how closely the predicted
trajectory matches rater-annotated trajectory preference labels. We have
released rater preference labels for all WOD-E2E validation set segments, while
the held out test set labels have been used for the 2025 WOD-E2E Challenge.
Through our work, we aim to foster state of the art research into
generalizable, robust, and safe end-to-end autonomous driving agents capable of
handling complex real-world situations.

</details>


### [19] [Exploring Object-Aware Attention Guided Frame Association for RGB-D SLAM](https://arxiv.org/abs/2510.26131)
*Ali Caglayan,Nevrez Imamoglu,Oguzhan Guclu,Ali Osman Serhatoglu,Ahmet Burak Can,Ryosuke Nakamura*

Main category: cs.CV

TL;DR: 本文将基于网络梯度的注意力信息与CNN特征融合，应用于RGB-D室内SLAM任务，实现帧关联性能提升，尤其在大环境下效果显著。


<details>
  <summary>Details</summary>
Motivation: 虽然可视化技术可揭示CNN在图像识别中的关注区域，但将注意力显式集成进CNN以增强语义物体理解的研究尚少，且这对像SLAM这样依赖语义与空间信息的任务具有潜力。

Method: 提出将由网络梯度获得的逐层注意力显式融合到CNN特征表示中，从而提升RGB-D室内SLAM过程中帧与帧之间的关联性能。

Result: 实验表明，本方法在与基线方法比较时，特别是在大规模环境下能显著提升性能。

Conclusion: 基于网络注意力的信息能有效提升SLAM任务中的图像关联和理解能力，具有广泛的应用前景。

Abstract: Attention models have recently emerged as a powerful approach, demonstrating
significant progress in various fields. Visualization techniques, such as class
activation mapping, provide visual insights into the reasoning of convolutional
neural networks (CNNs). Using network gradients, it is possible to identify
regions where the network pays attention during image recognition tasks.
Furthermore, these gradients can be combined with CNN features to localize more
generalizable, task-specific attentive (salient) regions within scenes.
However, explicit use of this gradient-based attention information integrated
directly into CNN representations for semantic object understanding remains
limited. Such integration is particularly beneficial for visual tasks like
simultaneous localization and mapping (SLAM), where CNN representations
enriched with spatially attentive object locations can enhance performance. In
this work, we propose utilizing task-specific network attention for RGB-D
indoor SLAM. Specifically, we integrate layer-wise attention information
derived from network gradients with CNN feature representations to improve
frame association performance. Experimental results indicate improved
performance compared to baseline methods, particularly for large environments.

</details>


### [20] [FullPart: Generating each 3D Part at Full Resolution](https://arxiv.org/abs/2510.26140)
*Lihe Ding,Shaocong Dong,Yaokun Li,Chenjian Gao,Xiao Chen,Rui Han,Yihao Kuang,Hong Zhang,Bo Huang,Zhanpeng Huang,Zibin Wang,Dan Xu,Tianfan Xue*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D部件生成框架FullPart，结合了隐式和显式方法，提升了部件细节表达，并引入了大规模3D部件数据集PartVerse-XL，实验结果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于隐式向量集的生成器细节表现不足，基于体素的生成器因全局网格分配导致小部件质量低下，限制了3D部件生成在实际应用中的效果，有必要寻找一种既兼顾细节又保证全局一致性的新方法。

Method: FullPart首先利用隐式盒向量集扩散过程确定部件的边界框布局，之后每个部件在自己的全分辨率体素网格中独立生成，提高了细节表现。同时引入中心点编码策略，优化不同实际尺寸部件间的信息对齐与全局一致性。此外，论文还构建了大规模高质量的3D部件数据集PartVerse-XL。

Result: 在多个实验评测下，FullPart在3D部件生成任务上取得了当前最优性能，能够生成高细节且全局协调的3D部件结构，效果明显优于传统方法。

Conclusion: FullPart克服了现有3D部件生成方法的多项局限，能高效生成高质量细节丰富的3D部件，并将相关代码、数据与模型开源，对推动3D部件生成领域研究具有重要意义。

Abstract: Part-based 3D generation holds great potential for various applications.
Previous part generators that represent parts using implicit vector-set tokens
often suffer from insufficient geometric details. Another line of work adopts
an explicit voxel representation but shares a global voxel grid among all
parts; this often causes small parts to occupy too few voxels, leading to
degraded quality. In this paper, we propose FullPart, a novel framework that
combines both implicit and explicit paradigms. It first derives the bounding
box layout through an implicit box vector-set diffusion process, a task that
implicit diffusion handles effectively since box tokens contain little
geometric detail. Then, it generates detailed parts, each within its own fixed
full-resolution voxel grid. Instead of sharing a global low-resolution space,
each part in our method - even small ones - is generated at full resolution,
enabling the synthesis of intricate details. We further introduce a
center-point encoding strategy to address the misalignment issue when
exchanging information between parts of different actual sizes, thereby
maintaining global coherence. Moreover, to tackle the scarcity of reliable part
data, we present PartVerse-XL, the largest human-annotated 3D part dataset to
date with 40K objects and 320K parts. Extensive experiments demonstrate that
FullPart achieves state-of-the-art results in 3D part generation. We will
release all code, data, and model to benefit future research in 3D part
generation.

</details>


### [21] [BasicAVSR: Arbitrary-Scale Video Super-Resolution via Image Priors and Enhanced Motion Compensation](https://arxiv.org/abs/2510.26149)
*Wei Shang,Wanying Zhang,Shuhang Gu,Pengfei Zhu,Qinghua Hu,Dongwei Ren*

Main category: cs.CV

TL;DR: 该论文提出了一个名为BasicAVSR的任意尺度视频超分辨率（AVSR）强基线方法，提高了不同放大倍率下的视频清晰度，兼顾了高性能与广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 任意尺度视频超分辨率需要应对空间细节还原、时间一致性和计算复杂度等多方面挑战。现有方法在提升灵活性和适应多场景应用方面仍然有限，因此亟需一个强大且可扩展的基线模型。

Method: 本方法结合了四个关键部分：多尺度频率先验（基于Laplacian金字塔）、光流引导传播单元（二阶运动补偿）、超采样单元（生成适应缩放的卷积核），并据应用场景实现三种传播变体（严格在线、有限延迟在线、离线）。

Result: 实验表明，BasicAVSR在不同超分辨场景下均优于已有方法，在超分辨质量、泛化能力与推理速度方面表现突出。

Conclusion: BasicAVSR推进了AVSR的技术发展，并且其核心组件可灵活集成于多种框架，适用不同应用场景。

Abstract: Arbitrary-scale video super-resolution (AVSR) aims to enhance the resolution
of video frames, potentially at various scaling factors, which presents several
challenges regarding spatial detail reproduction, temporal consistency, and
computational complexity. In this paper, we propose a strong baseline BasicAVSR
for AVSR by integrating four key components: 1) adaptive multi-scale frequency
priors generated from image Laplacian pyramids, 2) a flow-guided propagation
unit to aggregate spatiotemporal information from adjacent frames, 3) a
second-order motion compensation unit for more accurate spatial alignment of
adjacent frames, and 4) a hyper-upsampling unit to generate scale-aware and
content-independent upsampling kernels. To meet diverse application demands, we
instantiate three propagation variants: (i) a unidirectional RNN unit for
strictly online inference, (ii) a unidirectional RNN unit empowered with a
limited lookahead that tolerates a small output delay, and (iii) a
bidirectional RNN unit designed for offline tasks where computational resources
are less constrained. Experimental results demonstrate the effectiveness and
adaptability of our model across these different scenarios. Through extensive
experiments, we show that BasicAVSR significantly outperforms existing methods
in terms of super-resolution quality, generalization ability, and inference
speed. Our work not only advances the state-of-the-art in AVSR but also extends
its core components to multiple frameworks for diverse scenarios. The code is
available at https://github.com/shangwei5/BasicAVSR.

</details>


### [22] [MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction](https://arxiv.org/abs/2510.26151)
*Shunjie-Fabian Zheng,Hyeonjun Lee,Thijs Kooi,Ali Diba*

Main category: cs.CV

TL;DR: 本文提出了一种新型多视角乳腺X线影像与语言模型，用于乳腺癌分类和风险预测，通过结合合成影像-文本对进行自监督训练，无需昂贵的精细标注，即可在多项任务上取得领先效果。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌辅助诊断模型需要大量详细标注的数据集，而高质量标注数据获取成本高且耗时。利用视觉-语言模型以提升模型鲁棒性和数据效率成为潜在解决方案。

Method: 作者设计了MV-MLM多视角乳腺摄影及语言模型，结合多视图X线影像和合成放射学报告，基于跨模态自监督机制学习丰富表征。采用联合视觉-文本学习策略，以提升对不同乳腺组织、病变特征和风险的识别能力。模型训练无需实际放射学报告，仅需合成文本。

Result: 在私有与公开数据集上，MV-MLM在恶性/亚型/风险分类三大任务中取得了SOTA（最佳）表现。在小数据或无手工标注文本的情况下，该方法的数据利用效率优于现有全监督及VLM基线。

Conclusion: MV-MLM模型成功实现无高成本人工精细标注下的乳腺癌诊断任务，在实际数据受限场景下具有很强的应用前景，为医疗影像辅助智能诊断提供新途径。

Abstract: Large annotated datasets are essential for training robust Computer-Aided
Diagnosis (CAD) models for breast cancer detection or risk prediction. However,
acquiring such datasets with fine-detailed annotation is both costly and
time-consuming. Vision-Language Models (VLMs), such as CLIP, which are
pre-trained on large image-text pairs, offer a promising solution by enhancing
robustness and data efficiency in medical imaging tasks. This paper introduces
a novel Multi-View Mammography and Language Model for breast cancer
classification and risk prediction, trained on a dataset of paired mammogram
images and synthetic radiology reports. Our MV-MLM leverages multi-view
supervision to learn rich representations from extensive radiology data by
employing cross-modal self-supervision across image-text pairs. This includes
multiple views and the corresponding pseudo-radiology reports. We propose a
novel joint visual-textual learning strategy to enhance generalization and
accuracy performance over different data types and tasks to distinguish breast
tissues or cancer characteristics(calcification, mass) and utilize these
patterns to understand mammography images and predict cancer risk. We evaluated
our method on both private and publicly available datasets, demonstrating that
the proposed model achieves state-of-the-art performance in three
classification tasks: (1) malignancy classification, (2) subtype
classification, and (3) image-based cancer risk prediction. Furthermore, the
model exhibits strong data efficiency, outperforming existing fully supervised
or VLM baselines while trained on synthetic text reports and without the need
for actual radiology reports.

</details>


### [23] [Detecting Unauthorized Vehicles using Deep Learning for Smart Cities: A Case Study on Bangladesh](https://arxiv.org/abs/2510.26154)
*Sudipto Das Sukanto,Diponker Roy,Fahim Shakil,Nirjhar Singha,Abdullah Asik,Aniket Joarder,Mridha Md Nafis Fuad,Muhammad Ibrahim*

Main category: cs.CV

TL;DR: 该论文提出了一种基于机器学习的方法，利用YOLOv8模型实现了孟加拉国城市交通中自动人力车的自动检测，并取得了较高的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 在南亚国家，自动人力车（auto-rickshaw）是常见的交通工具，但因其与其他车辆外形相似，现有监控系统难以自动区分，手工分析效率低。因此，交通管理需要一种高效的自动检测方法。

Method: 作者构建了包含1730张在不同交通环境下拍摄的标注图片的数据集，并利用YOLOv8实时目标检测模型进行训练和测试，自动识别交通图像中的自动人力车。

Result: 实验结果显示该模型在auto-rickshaw检测任务上的mAP50达到83.447%，精准率和召回率均高于78%，可有效适应密集及稀疏交通场景。

Conclusion: 本文提出的方法能够准确高效地在交通监控图像中检测自动人力车，并公开了相关数据集，为后续相关研究提供了基础。

Abstract: Modes of transportation vary across countries depending on geographical
location and cultural context. In South Asian countries rickshaws are among the
most common means of local transport. Based on their mode of operation,
rickshaws in cities across Bangladesh can be broadly classified into non-auto
(pedal-powered) and auto-rickshaws (motorized). Monitoring the movement of
auto-rickshaws is necessary as traffic rules often restrict auto-rickshaws from
accessing certain routes. However, existing surveillance systems make it quite
difficult to monitor them due to their similarity to other vehicles, especially
non-auto rickshaws whereas manual video analysis is too time-consuming. This
paper presents a machine learning-based approach to automatically detect
auto-rickshaws in traffic images. In this system, we used real-time object
detection using the YOLOv8 model. For training purposes, we prepared a set of
1,730 annotated images that were captured under various traffic conditions. The
results show that our proposed model performs well in real-time auto-rickshaw
detection and offers an mAP50 of 83.447% and binary precision and recall values
above 78%, demonstrating its effectiveness in handling both dense and sparse
traffic scenarios. The dataset has been publicly released for further research.

</details>


### [24] [CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark](https://arxiv.org/abs/2510.26160)
*Jiaqi Wang,Xiao Yang,Kai Sun,Parth Suresh,Sanat Sharma,Adam Czyzewski,Derek Andersen,Surya Appini,Arkav Banerjee,Sajal Choudhary,Shervin Ghasemlou,Ziqiang Guan,Akil Iyer,Haidar Khan,Lingkun Kong,Roy Luo,Tiffany Ma,Zhen Qiao,David Tran,Wenfang Xu,Skyler Yeatman,Chen Zhou,Gunveer Gujral,Yinglong Xia,Shane Moon,Nicolas Scheffer,Nirav Shah,Eun Chang,Yue Liu,Florian Metze,Tammy Stark,Zhaleh Feizollahi,Andrea Jessee,Mangesh Pujari,Ahmed Aly,Babak Damavandi,Rakesh Wanga,Anuj Kumar,Rohit Patel,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CV

TL;DR: 本文提出了CRAG-MM，一个专注于可穿戴设备场景下多模态多轮对话的综合性基准数据集。该数据集覆盖广泛真实场景，包含6.5K数据三元组与2K多轮对话，旨在推动多模态检索增强生成（MM-RAG）技术发展。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对可穿戴设备场景、多模态多轮对话的综合性评测基准，现有MM-RAG系统在这些应用中的效果和挑战尚未得到系统性探索。本文力图填补该空白，促进相关技术进步。

Method: 作者构建了CRAG-MM数据集，涵盖6.5K（图片，问题，答案）三元组、2K多轮对话，并纳入6.2K仿真可穿戴设备视角的图片。问题设计注重多样性，被细致设置为涵盖多种图片质量问题、问题类型、实体流行度、信息动态性和对话轮次。还设计了三种增强及多轮对话任务，并且提供图像知识库和网页检索API。

Result: 对主流RAG方法及业界先进方案在CRAG-MM上的评估显示，其单轮、多轮QA事实性分别仅有32%和43%，即使业界最新方法也与此相近，展现出很大改进空间。比赛推动下最佳方案对基线提升达28%。

Conclusion: CRAG-MM填补了可穿戴场景下多模态RAG大众评测基准的空白，实验和比赛评测揭示了现有方法有较大提升空间，数据集及评测任务对推动相关领域发展具有重要价值。

Abstract: Wearable devices such as smart glasses are transforming the way people
interact with their surroundings, enabling users to seek information regarding
entities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG)
plays a key role in supporting such questions, yet there is still no
comprehensive benchmark for this task, especially regarding wearables
scenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG
benchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse
set of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn
conversations across 13 domains, including 6.2K egocentric images designed to
mimic captures from wearable devices. We carefully constructed the questions to
reflect real-world scenarios and challenges, including five types of
image-quality issues, six question types, varying entity popularity, differing
information dynamism, and different conversation turns. We design three tasks:
single-source augmentation, multi-source augmentation, and multi-turn
conversations -- each paired with an associated retrieval corpus and APIs for
both image-KG retrieval and webpage retrieval. Our evaluation shows that
straightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM
single- and multi-turn QA, respectively, whereas state-of-the-art industry
solutions have similar quality (32%/45%), underscoring ample room for
improvement. The benchmark has hosted KDD Cup 2025, attracting about 1K
participants and 5K submissions, with winning solutions improving baseline
performance by 28%, highlighting its early impact on advancing the field.

</details>


### [25] [MoTDiff: High-resolution Motion Trajectory estimation from a single blurred image using Diffusion models](https://arxiv.org/abs/2510.26173)
*Wontae Choi,Jaelin Lee,Hyung Sup Yun,Byeungwoo Jeon,Il Yong Chun*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的高分辨率运动轨迹估计方法MoTDiff，能够从单张运动模糊图像中高质量地恢复运动信息，其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有从单幅模糊图像中提取运动信息的方法通常精度较低、结果粗糙，难以得到高分辨率且精确的运动轨迹。因此，研究者希望能够设计出高质量的运动信息提取方法，以提升在计算成像和视觉应用中的表现。

Method: MoTDiff方法提出了两个核心创新：（1）引入条件扩散模型，利用从单幅模糊图像中提取的多尺度特征作为条件信息；（2）设计了一种新的训练方法，促进对细粒度、高精度运动轨迹的识别，并保证位置一致性和像素连通性。

Result: 实验显示，MoTDiff在盲图像去模糊和编码曝光摄影两个任务中均优于已有的最新方法。

Conclusion: MoTDiff能够更高效地从单帧模糊图像中还原高分辨率、高质量的运动轨迹，提升了下游视觉任务的表现，具有较大的应用和研究价值。

Abstract: Accurate estimation of motion information is crucial in diverse computational
imaging and computer vision applications. Researchers have investigated various
methods to extract motion information from a single blurred image, including
blur kernels and optical flow. However, existing motion representations are
often of low quality, i.e., coarse-grained and inaccurate. In this paper, we
propose the first high-resolution (HR) Motion Trajectory estimation framework
using Diffusion models (MoTDiff). Different from existing motion
representations, we aim to estimate an HR motion trajectory with high-quality
from a single motion-blurred image. The proposed MoTDiff consists of two key
components: 1) a new conditional diffusion framework that uses multi-scale
feature maps extracted from a single blurred image as a condition, and 2) a new
training method that can promote precise identification of a fine-grained
motion trajectory, consistent estimation of overall shape and position of a
motion path, and pixel connectivity along a motion trajectory. Our experiments
demonstrate that the proposed MoTDiff can outperform state-of-the-art methods
in both blind image deblurring and coded exposure photography applications.

</details>


### [26] [ConceptScope: Characterizing Dataset Bias via Disentangled Visual Concepts](https://arxiv.org/abs/2510.26186)
*Jinho Choi,Hyesu Lim,Steffen Schneider,Jaegul Choo*

Main category: cs.CV

TL;DR: 本文提出了ConceptScope框架，一种利用稀疏自编码器和视觉基础模型实现可扩展、自动化地识别视觉数据集偏见的工具。该方法无需昂贵的细粒度标注，能够发现并量化数据集中的可解释性概念，实现偏见检测和数据集分析。


<details>
  <summary>Details</summary>
Motivation: 在机器学习应用中，数据集偏见普遍存在，这些偏见会影响模型泛化能力和公平性。然而，当前系统性识别偏见通常依赖于高昂、耗时的属性标签，缺乏自动化且可解释的方法。作者希望开发一种无需昂贵标注、可扩展的工具来检测和分析各种视觉数据集中的偏见。

Method: 作者提出ConceptScope框架，利用预训练视觉基础模型的特征表示，并通过稀疏自编码器挖掘数据中的可解释概念。ConceptScope将这些概念按照与任务标签的统计相关性和语义相关性分为目标、上下文和偏见三类。随后，基于这些概念对数据集进行分组，实现偏见检测和鲁棒性评估。方法在多个标注数据集上进行了验证，通过与人工注释对比，评估了概念的覆盖范围和语义一致性。

Result: 实验表明，ConceptScope能自动识别对象、纹理、背景、面部属性、情感、动作等多种概念。其生成的空间归因热力图能够与语义上有意义的图像区域对齐。在数据集偏见检测方面，该方法不仅能发现如Waterbirds数据集中的背景偏见，还能在ImageNet等数据集中自动揭示未曾注释的偏见（如共现物体）。

Conclusion: ConceptScope为视觉数据集偏见识别和分析提供了实用且自动化的工具，无需高昂的人工标注。它能有效帮助数据集审核和模型诊断，对于提升数据质量和模型公正性有重要价值。

Abstract: Dataset bias, where data points are skewed to certain concepts, is ubiquitous
in machine learning datasets. Yet, systematically identifying these biases is
challenging without costly, fine-grained attribute annotations. We present
ConceptScope, a scalable and automated framework for analyzing visual datasets
by discovering and quantifying human-interpretable concepts using Sparse
Autoencoders trained on representations from vision foundation models.
ConceptScope categorizes concepts into target, context, and bias types based on
their semantic relevance and statistical correlation to class labels, enabling
class-level dataset characterization, bias identification, and robustness
evaluation through concept-based subgrouping. We validate that ConceptScope
captures a wide range of visual concepts, including objects, textures,
backgrounds, facial attributes, emotions, and actions, through comparisons with
annotated datasets. Furthermore, we show that concept activations produce
spatial attributions that align with semantically meaningful image regions.
ConceptScope reliably detects known biases (e.g., background bias in
Waterbirds) and uncovers previously unannotated ones (e.g, co-occurring objects
in ImageNet), offering a practical tool for dataset auditing and model
diagnostics.

</details>


### [27] [On Robustness of Vision-Language-Action Model against Multi-Modal Perturbations](https://arxiv.org/abs/2510.00037)
*Jianing Guo,Zhenhong Wu,Chang Tu,Yiyao Ma,Xiangqi Kong,Zhiqian Liu,Jiaming Ji,Shuning Zhang,Yuanpei Chen,Kai Chen,Qi Dou,Yaodong Yang,Xianglong Liu,Huijie Zhao,Weifeng Lv,Simin Li*

Main category: cs.CV

TL;DR: 本文提出RobustVLA，提升视觉-语言-动作（VLA）模型在多模态干扰下的鲁棒性，在四大模态17种干扰下取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，VLA模型需应对多种输入输出干扰，但现有方法多只关注视觉扰动，缺乏对跨模态干扰全面性的鲁棒性提升方案。

Method: （1）系统性评估主流VLA在四模态17类干扰下的鲁棒性；（2）分析pi0模型基于扩散的action head表现；（3）提出RobustVLA，从输入端增强一致性输出，从输出端进行离线鲁棒优化，对抗最坏情形的动作噪声，同时将干扰视为多臂老虎机问题，自动识别最致命噪声并调整优化。

Result: RobustVLA在LIBERO数据集17种干扰下，分别在pi0与OpenVLA骨干模型之上实现12.6%和10.4%的鲁棒性能提升，推理速度提升50.6倍，混合干扰下模型增强10.4%。在真实FR5机器人上，仅基于有限示范，RobustVLA在四模态干扰下提升65.6%。

Conclusion: RobustVLA能有效提升VLA模型对多模态真实环境干扰的鲁棒性，具备广泛落地潜力。

Abstract: In Vision-Language-Action (VLA) models, robustness to real-world
perturbations is critical for deployment. Existing methods target simple visual
disturbances, overlooking the broader multi-modal perturbations that arise in
actions, instructions, environments, and observations. Here, we first evaluate
the robustness of mainstream VLAs under 17 perturbations across four
modalities. We find (1) actions as the most fragile modality, (2) Existing
visual-robust VLA do not gain robustness in other modality, and (3) pi0
demonstrates superior robustness with a diffusion-based action head. To build
multi-modal robust VLAs, we propose RobustVLA against perturbations in VLA
inputs and outputs. For output robustness, we perform offline robust
optimization against worst-case action noise that maximizes mismatch in flow
matching objective. This can be seen as adversarial training, label smoothing,
and outlier penalization. For input robustness, we enforce consistent actions
across input variations that preserve task semantics. To account for multiple
perturbations, we formulate robustness as a multi-armed bandit problem and
apply an upper confidence bound algorithm to automatically identify the most
harmful noise. Experiments on LIBERO demonstrate our RobustVLA delivers
absolute gains over baselines of 12.6% on the pi0 backbone and 10.4% on the
OpenVLA backbone across all 17 perturbations, achieving 50.6x faster inference
than existing visual-robust VLAs, and a 10.4% gain under mixed perturbations.
Our RobustVLA is particularly effective on real-world FR5 robot with limited
demonstrations, showing absolute gains by 65.6% under perturbations of four
modalities.

</details>


### [28] [Sketch2PoseNet: Efficient and Generalized Sketch to 3D Human Pose Prediction](https://arxiv.org/abs/2510.26196)
*Li Wang,Yiyu Zhuang,Yanwen Wang,Xun Cao,Chuan Guo,Xinxin Zuo,Hao Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种利用扩散模型合成数据的新方法，解决了从草图到3D人体姿态估计中的数据稀缺与风格多变问题，显著提升了准确率和速度。


<details>
  <summary>Details</summary>
Motivation: 以往草图到3D姿态估计方法受限于缺乏大规模标注数据，且依赖人工规则优化，效率低下，泛化能力弱。因此需要一种新的自动化、可扩展的方法来提升该任务的性能。

Method: 首先利用扩散模型，将从3D人体投影到2D的姿态合成不同风格的草图，构建了一个包含12万对草图-3D姿态对的大规模合成数据集SKEP-120K。随后，提出一个端到端的数据驱动框架，结合2D姿态检测、扩散先验与前馈神经网络，实现高效的2D和3D人体姿态与形状估计。此外，引入多种启发式损失保证3D/2D姿态的几何一致性及自接触准确。

Result: 在多种客观和主观的定量与定性评测中，所提方法在估计准确率与速度等指标上均大幅优于以往基线方法。

Conclusion: 通过合成大规模数据集与端到端的数据驱动框架，本文方法显著提升了草图到3D人体姿态估计的准确性和实时性，为动画、影视等领域提供了更实用的技术方案。

Abstract: 3D human pose estimation from sketches has broad applications in computer
animation and film production. Unlike traditional human pose estimation, this
task presents unique challenges due to the abstract and disproportionate nature
of sketches. Previous sketch-to-pose methods, constrained by the lack of
large-scale sketch-3D pose annotations, primarily relied on optimization with
heuristic rules-an approach that is both time-consuming and limited in
generalizability. To address these challenges, we propose a novel approach
leveraging a "learn from synthesis" strategy. First, a diffusion model is
trained to synthesize sketch images from 2D poses projected from 3D human
poses, mimicking disproportionate human structures in sketches. This process
enables the creation of a synthetic dataset, SKEP-120K, consisting of 120k
accurate sketch-3D pose annotation pairs across various sketch styles. Building
on this synthetic dataset, we introduce an end-to-end data-driven framework for
estimating human poses and shapes from diverse sketch styles. Our framework
combines existing 2D pose detectors and generative diffusion priors for sketch
feature extraction with a feed-forward neural network for efficient 2D pose
estimation. Multiple heuristic loss functions are incorporated to guarantee
geometric coherence between the derived 3D poses and the detected 2D poses
while preserving accurate self-contacts. Qualitative, quantitative, and
subjective evaluations collectively show that our model substantially surpasses
previous ones in both estimation accuracy and speed for sketch-to-pose tasks.

</details>


### [29] [Developing a Multi-task Ensemble Geometric Deep Network for Supply Chain Sustainability and Risk Management](https://arxiv.org/abs/2510.26203)
*Mehdi Khaleghi,Nastaran Khaleghi,Sobhan Sheykhivand,Sebelan Danishvar*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的Chebyshev集合几何深度网络（Ch-EGN），用于提高供应链的可持续性和风险管理，取得了高于其他方法的准确率。


<details>
  <summary>Details</summary>
Motivation: 供应链风险管理和可持续性日益重要，而现有方法在产品分类和关系挖掘上表现有限。深度学习在其它领域取得进展，促使作者借助其探索供应链问题。

Method: 提出了一种融合卷积和几何深度学习的集合深度网络Ch-EGN，并在SupplyGraph和DataCo数据集上分别进行产品分类、边分类和交付状态预测实验。

Result: Ch-EGN在风险管理（交付状态）上平均准确率98.95%；产品五类分类准确率100%，关系四类分类98.07%，公司关系25类分类92.37%；均优于现有方法。

Conclusion: Ch-EGN能有效提升供应链可持续性相关任务的准确率和效率，是供应链数据挖掘和风险管理领域的新进展。

Abstract: The sustainability of supply chain plays a key role in achieving optimal
performance in controlling the supply chain. The management of risks that occur
in a supply chain is a fundamental problem for the purpose of developing the
sustainability of the network and elevating the performance efficiency of the
supply chain. The correct classification of products is another essential
element in a sustainable supply chain. Acknowledging recent breakthroughs in
the context of deep networks, several architectural options have been deployed
to analyze supply chain datasets. A novel geometric deep network is used to
propose an ensemble deep network. The proposed Chebyshev ensemble geometric
network (Ch-EGN) is a hybrid convolutional and geometric deep learning. This
network is proposed to leverage the information dependencies in supply chain to
derive invisible states of samples in the database. The functionality of the
proposed deep network is assessed on the two different databases. The
SupplyGraph Dataset and DataCo are considered in this research. The prediction
of delivery status of DataCo supply chain is done for risk administration. The
product classification and edge classification are performed using the
SupplyGraph database to enhance the sustainability of the supply network. An
average accuracy of 98.95% is obtained for the ensemble network for risk
management. The average accuracy of 100% and 98.07% are obtained for
sustainable supply chain in terms of 5 product group classification and 4
product relation classification, respectively. The average accuracy of 92.37%
is attained for 25 company relation classification. The results confirm an
average improvement and efficiency of the proposed method compared to the
state-of-the-art approaches.

</details>


### [30] [OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation](https://arxiv.org/abs/2510.26213)
*Hengrui Kang,Zhuangcheng Gu,Zhiyuan Zhao,Zichen Wen,Bin Wang,Weijia Li,Conghui He*

Main category: cs.CV

TL;DR: 本文提出了一个百万级、多样化文档布局数据集OmniLayout-1M，并据此设计了新布局生成模型OmniLayout-LLM，在多个领域显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前文档AI更多关注在文档布局分析（DLA），而布局生成领域因缺乏多样化、高质量数据集，研究滞后且局限于学术论文等曼哈顿结构。现实中如报纸、杂志等布局类型很少被涉及。该研究想要解决开放世界文档布局数据稀缺和生成模型泛化能力弱的问题。

Method: 1) 构建OmniLayout-1M数据集，涵盖6种常见文档类型，来源多样，包含现代布局；2) 提出OmniLayout-LLM模型（5亿参数），采用两阶段Coarse-to-Fine学习范式，先用粗类别标签学习通用布局原则，再用细标签微调到特定领域。

Result: 在M6Doc等多个数据集上，该方法在多种情境下优于当前最强的专业布局生成模型和多个最新通用大模型，性能显著提升。

Conclusion: OmniLayout-1M和OmniLayout-LLM极大丰富了文档布局生成领域的数据和方法，推动了开放域文档自动布局生成的研究和应用前景；相关资源将开源，有助学术和工业进一步研究。

Abstract: Document AI has advanced rapidly and is attracting increasing attention. Yet,
while most efforts have focused on document layout analysis (DLA), its
generative counterpart, document layout generation, remains underexplored. A
major obstacle lies in the scarcity of diverse layouts: academic papers with
Manhattan-style structures dominate existing studies, while open-world genres
such as newspapers and magazines remain severely underrepresented. To address
this gap, we curate OmniLayout-1M, the first million-scale dataset of diverse
document layouts, covering six common document types and comprising
contemporary layouts collected from multiple sources. Moreover, since existing
methods struggle in complex domains and often fail to arrange long sequences
coherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage
Coarse-to-Fine learning paradigm: 1) learning universal layout principles from
OmniLayout-1M with coarse category definitions, and 2) transferring the
knowledge to a specific domain with fine-grained annotations. Extensive
experiments demonstrate that our approach achieves strong performance on
multiple domains in M$^{6}$Doc dataset, substantially surpassing both existing
layout generation experts and several latest general-purpose LLMs. Our code,
models, and dataset will be publicly released.

</details>


### [31] [Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models](https://arxiv.org/abs/2510.26241)
*Shiho Matta,Lis Kanashiro Pereira,Peitao Han,Fei Cheng,Shigeru Kitazawa*

Main category: cs.CV

TL;DR: 该论文提出了一个新基准AoT-PsyPhyBENCH，用于评估视觉-语言模型(VLMs)在判断视频时间箭头（前后播放）上的能力，结果显示主流模型表现接近随机，远落后于人类。


<details>
  <summary>Details</summary>
Motivation: 尽管现代视觉-语言模型在多模态任务上表现优异，但它们对视频中时间信息的理解能力较弱且未被充分评估。论文认为，理解时间箭头——即分辨视频是正向还是反向播放，是考察模型时序推理能力的关键问题，因此设计了相关基准。

Method: 作者构建了AoT-PsyPhyBENCH基准，通过让VLMs判断自然视频片段的时间方向（正向或反向），并采用与人类相同的测试材料和行为基线进行评测。实验涵盖开源与专有模型、带推理能力和非推理能力的VLMs，并将其在物理不可逆过程和因果操作识别任务上与人类进行对比。

Result: 评测结果显示，大多数视觉-语言模型在判断时间箭头任务上的表现接近随机水平，即便是最好的模型，在处理物理不可逆过程（如自由落体、扩散/爆炸）和因果操作（如分割/合并）时仍远逊于人类。

Conclusion: 当前多模态系统虽然已能捕捉丰富的视觉-语义相关性，但缺乏对时序连续性和因果关系的先验归纳偏置。作者公开了AoT-PsyPhyBENCH数据和代码，旨在推动VLMs在物理和时间推理能力上的进步。

Abstract: Modern vision-language models (VLMs) excel at many multimodal tasks, yet
their grasp of temporal information in video remains weak and, crucially,
under-evaluated. We probe this gap with a deceptively simple but revealing
challenge: judging the arrow of time (AoT)-whether a short clip is played
forward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated
benchmark that tests whether VLMs can infer temporal direction in natural
videos using the same stimuli and behavioral baselines established for humans.
Our comprehensive evaluation of open-weight and proprietary, reasoning and
non-reasoning VLMs reveals that most models perform near chance, and even the
best lag far behind human accuracy on physically irreversible processes (e.g.,
free fall, diffusion/explosion) and causal manual actions (division/addition)
that humans recognize almost instantly. These results highlight a fundamental
gap in current multimodal systems: while they capture rich visual-semantic
correlations, they lack the inductive biases required for temporal continuity
and causal understanding. We release the code and data for AoT-PsyPhyBENCH to
encourage further progress in the physical and temporal reasoning capabilities
of VLMs.

</details>


### [32] [Spiking Patches: Asynchronous, Sparse, and Efficient Tokens for Event Cameras](https://arxiv.org/abs/2510.26614)
*Christoffer Koo Øhrstrøm,Ronja Güldenring,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: 本文提出了一种专为事件相机设计的事件流分词器——Spiking Patches，实现了更快且高效的事件表示。与传统帧或体素方法相比，Spiking Patches在保持稀疏性与异步性的同时，速度更快且精度相当甚至更优。


<details>
  <summary>Details</summary>
Motivation: 传统事件相机数据的表示主要通过帧或体素化。这些方法尽管准确率较高，但会丢失事件数据特有的异步性和空间稀疏性，因此需要一种能更好保持事件相机特性的表示方式。

Method: 提出了一种新的事件数据分词方法Spiking Patches，将异步、稀疏的事件流转化为token，并与主流的GNN、PCN和Transformer模型结合用于手势识别和目标检测任务。实验比较了分帧和体素表示下的性能。

Result: 采用Spiking Patches方法后，在推理速度上比体素token快3.4倍，比帧快10.4倍；在保持甚至提升了手势识别（绝对提升最高3.8）和目标检测（最高提升1.4）的准确率的情况下，显著提升效率。

Conclusion: 事件流的分词是事件视觉领域的新方向。Spiking Patches不仅保留了事件相机的独特数据特性，还在效率和准确率上表现优异，为事件视觉处理方法开辟了新途径。

Abstract: We propose tokenization of events and present a tokenizer, Spiking Patches,
specifically designed for event cameras. Given a stream of asynchronous and
spatially sparse events, our goal is to discover an event representation that
preserves these properties. Prior works have represented events as frames or as
voxels. However, while these representations yield high accuracy, both frames
and voxels are synchronous and decrease the spatial sparsity. Spiking Patches
gives the means to preserve the unique properties of event cameras and we show
in our experiments that this comes without sacrificing accuracy. We evaluate
our tokenizer using a GNN, PCN, and a Transformer on gesture recognition and
object detection. Tokens from Spiking Patches yield inference times that are up
to 3.4x faster than voxel-based tokens and up to 10.4x faster than frames. We
achieve this while matching their accuracy and even surpassing in some cases
with absolute improvements up to 3.8 for gesture recognition and up to 1.4 for
object detection. Thus, tokenization constitutes a novel direction in
event-based vision and marks a step towards methods that preserve the
properties of event cameras.

</details>


### [33] [Revisiting Generative Infrared and Visible Image Fusion Based on Human Cognitive Laws](https://arxiv.org/abs/2510.26268)
*Lin Guo,Xiaoqing Luo,Wei Xie,Zhancheng Zhang,Hui Li,Rui Wang,Zhenhua Feng,Xiaoning Song*

Main category: cs.CV

TL;DR: 提出了一种基于人类认知启发、融合物理规律和概率生成能力的新型红外与可见光图像融合方法HCLFuse，该方法在多个数据集上达到了最优性能，提升了融合结果的结构一致性与细节质量。


<details>
  <summary>Details</summary>
Motivation: 现有红外与可见光图像融合方法在平衡模态信息时存在困难，生成式方法生成能力有限且对复杂场景下的模态信息选择缺乏解释性，影响可靠性和一致性。为解决这一问题，受到人类认知规律的启发，作者希望提升融合方法的结构细节表现与可解释性。

Method: 提出HCLFuse方法，创新点包括：1）提出多尺度mask调控的变分瓶颈编码器，通过后验概率建模与信息分解，精准提取低层次模态信息，提升结构细节重建能力；2）将扩散模型的概率生成能力与物理规律结合，形成时变物理引导机制，自适应调控生成过程以增强对数据本质结构的感知并降低对数据质量的依赖。

Result: 在多个数据集上的定性和定量评估中，HCLFuse方法实现了最优融合性能，并在语义分割指标上取得了显著提升。

Conclusion: HCLFuse充分证明了融合人类认知规律的生成式融合方法在提升结构一致性和细节质量方面的优势，对图像融合领域具有重要参考意义。

Abstract: Existing infrared and visible image fusion methods often face the dilemma of
balancing modal information. Generative fusion methods reconstruct fused images
by learning from data distributions, but their generative capabilities remain
limited. Moreover, the lack of interpretability in modal information selection
further affects the reliability and consistency of fusion results in complex
scenarios. This manuscript revisits the essence of generative image fusion
under the inspiration of human cognitive laws and proposes a novel infrared and
visible image fusion method, termed HCLFuse. First, HCLFuse investigates the
quantification theory of information mapping in unsupervised fusion networks,
which leads to the design of a multi-scale mask-regulated variational
bottleneck encoder. This encoder applies posterior probability modeling and
information decomposition to extract accurate and concise low-level modal
information, thereby supporting the generation of high-fidelity structural
details. Furthermore, the probabilistic generative capability of the diffusion
model is integrated with physical laws, forming a time-varying physical
guidance mechanism that adaptively regulates the generation process at
different stages, thereby enhancing the ability of the model to perceive the
intrinsic structure of data and reducing dependence on data quality.
Experimental results show that the proposed method achieves state-of-the-art
fusion performance in qualitative and quantitative evaluations across multiple
datasets and significantly improves semantic segmentation metrics. This fully
demonstrates the advantages of this generative image fusion method, drawing
inspiration from human cognition, in enhancing structural consistency and
detail quality.

</details>


### [34] [Exploring Complementarity and Explainability in CNNs for Periocular Verification Across Acquisition Distances](https://arxiv.org/abs/2510.26282)
*Fernando Alonso-Fernandez,Kevin Hernandez Diaz,Jose M. Buades,Kiran Raja,Josef Bigun*

Main category: cs.CV

TL;DR: 本文研究了不同类型卷积神经网络（CNN）用于不同距离虹膜区域身份验证时的互补性。通过融合SqueezeNet、MobileNetv2和ResNet50三种模型，取得了优于单一模型和现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有虹膜区域身份验证方法在距离变化和网络架构不同的情况下表现有限，探索多模型互补性有望提升识别准确率。

Method: 分别用SqueezeNet、MobileNetv2和ResNet50在大规模VGGFace2虹膜图像数据上训练，利用余弦距离和卡方距离做匹配，尝试不同网络初始化，并通过逻辑回归融合分数。同时，应用LIME热图和Jensen-Shannon散度分析不同模型的关注区域分布以解释互补性。

Result: ResNet50单模型效果最好，但多模型分数级别融合显著提升了整体性能，超过了以往的最佳结果。热图显示，不同网络关注图像的不同区域，互补性明显。

Conclusion: 多CNN模型的分数级融合能有效利用互补性，大幅提升虹膜区域身份验证的准确率，并创造新的最好成绩。

Abstract: We study the complementarity of different CNNs for periocular verification at
different distances on the UBIPr database. We train three architectures of
increasing complexity (SqueezeNet, MobileNetv2, and ResNet50) on a large set of
eye crops from VGGFace2. We analyse performance with cosine and chi2 metrics,
compare different network initialisations, and apply score-level fusion via
logistic regression. In addition, we use LIME heatmaps and Jensen-Shannon
divergence to compare attention patterns of the CNNs. While ResNet50
consistently performs best individually, the fusion provides substantial gains,
especially when combining all three networks. Heatmaps show that networks
usually focus on distinct regions of a given image, which explains their
complementarity. Our method significantly outperforms previous works on UBIPr,
achieving a new state-of-the-art.

</details>


### [35] [Beyond Imitation: Constraint-Aware Trajectory Generation with Flow Matching For End-to-End Autonomous Driving](https://arxiv.org/abs/2510.26292)
*Lin Liu,Guanyi Yu,Ziying Song,Junqiao Li,Caiyan Jia,Feiyang Jia,Peiliang Wu,Yandan Luo*

Main category: cs.CV

TL;DR: 本文提出了一种新的自动驾驶规划方法CATG，通过约束流匹配（Constrained Flow Matching）生成安全且多样化的行驶轨迹，有效克服了现有模仿学习和生成方法的局限。


<details>
  <summary>Details</summary>
Motivation: 现有的模仿学习方法在自动驾驶轨迹规划时容易出现模式崩溃（mode collapse），无法生成多样化轨迹。而现有生成式方法又难以直接在生成过程中融入安全和物理约束，通常需要额外的优化步骤，这限制了其实用性和效率。

Method: 提出了一种名为CATG的新型规划框架，通过“约束流匹配（Constrained Flow Matching）”方法在生成过程中显式建模轨迹流动，将诸如安全、动力学等约束直接整合进生成流程，同时将驾驶激进程度参数化为控制信号，使生成的轨迹具备可调控的风格和行为。

Result: CATG在NavSim v2自动驾驶规划挑战赛中获得第2名，EPDMS分数为51.31，并获得创新奖，验证了方法在安全性、多样性与实用性上的突出表现。

Conclusion: CATG克服了传统模仿学习与生成式轨迹规划的局限，可为自动驾驶场景提供更安全且可控的轨迹生成方案，有望推动自动驾驶技术进一步发展。

Abstract: Planning is a critical component of end-to-end autonomous driving. However,
prevailing imitation learning methods often suffer from mode collapse, failing
to produce diverse trajectory hypotheses. Meanwhile, existing generative
approaches struggle to incorporate crucial safety and physical constraints
directly into the generative process, necessitating an additional optimization
stage to refine their outputs. To address these limitations, we propose CATG, a
novel planning framework that leverages Constrained Flow Matching. Concretely,
CATG explicitly models the flow matching process, which inherently mitigates
mode collapse and allows for flexible guidance from various conditioning
signals. Our primary contribution is the novel imposition of explicit
constraints directly within the flow matching process, ensuring that the
generated trajectories adhere to vital safety and kinematic rules. Secondly,
CATG parameterizes driving aggressiveness as a control signal during
generation, enabling precise manipulation of trajectory style. Notably, on the
NavSim v2 challenge, CATG achieved 2nd place with an EPDMS score of 51.31 and
was honored with the Innovation Award.

</details>


### [36] [Leveraging Large-Scale Face Datasets for Deep Periocular Recognition via Ocular Cropping](https://arxiv.org/abs/2510.26294)
*Fernando Alonso-Fernandez,Kevin Hernandez-Diaz,Jose Maria Buades Rubio,Josef Bigun*

Main category: cs.CV

TL;DR: 本文针对眼周生物特征识别，利用深度卷积神经网络，并在大规模数据集进行训练与评估。


<details>
  <summary>Details</summary>
Motivation: 眼周区域具有较高辨识度且易于获取，然而现有研究多依赖小规模数据集，难以充分评估深度学习模型的识别能力。

Method: 作者评估了三种不同深度与复杂度的卷积神经网络架构，使用VGGFace2数据库中提取的190多万张眼部图像进行训练，并在野外情境（VGGFace2-Pose）和手机自拍图像（UFPR-Periocular）测试其表现。

Result: 在不受控环境下（VGGFace2），眼部识别错误率（EER）为9-15%，高于完整人脸的3-6%；而在高质量、统一采集条件下（UFPR-Periocular），EER降至1-2%，刷新了该数据库的最低纪录。

Conclusion: 在高质量、采集一致的条件下，眼周识别技术结合深度学习方法可实现非常低的识别错误率；大规模训练数据有助于提升深度模型性能，对实际应用具有重要意义。

Abstract: We focus on ocular biometrics, specifically the periocular region (the area
around the eye), which offers high discrimination and minimal acquisition
constraints. We evaluate three Convolutional Neural Network architectures of
varying depth and complexity to assess their effectiveness for periocular
recognition. The networks are trained on 1,907,572 ocular crops extracted from
the large-scale VGGFace2 database. This significantly contrasts with existing
works, which typically rely on small-scale periocular datasets for training
having only a few thousand images. Experiments are conducted with ocular images
from VGGFace2-Pose, a subset of VGGFace2 containing in-the-wild face images,
and the UFPR-Periocular database, which consists of selfies captured via mobile
devices with user guidance on the screen. Due to the uncontrolled conditions of
VGGFace2, the Equal Error Rates (EERs) obtained with ocular crops range from
9-15%, noticeably higher than the 3-6% EERs achieved using full-face images. In
contrast, UFPR-Periocular yields significantly better performance (EERs of
1-2%), thanks to higher image quality and more consistent acquisition
protocols. To the best of our knowledge, these are the lowest reported EERs on
the UFPR dataset to date.

</details>


### [37] [Towards Realistic Earth-Observation Constellation Scheduling: Benchmark and Methodology](https://arxiv.org/abs/2510.26297)
*Luting Wang,Yinghao Xiang,Hongliang Huang,Dongjun Li,Chen Gao,Si Liu*

Main category: cs.CV

TL;DR: 本文提出了一个统一框架，包括首个大规模高保真敏捷地球观测卫星（AEOS）调度基准和一种基于Transformer的调度模型，有效提升了实际星座调度的性能。


<details>
  <summary>Details</summary>
Motivation: AEOS星座调度在大规模、动态和高约束情景下面临挑战，现实应用中现有方法受限于对复杂性的简化，缺乏高保真、标准化的调度基准和高效模型。

Method: 1. 构建AEOS-Bench：包含3907个调优卫星、16410个现实情景，并提供真实调度标注，全面模拟实际卫星行为。2. 提出AEOS-Former：基于Transformer，内嵌约束感知注意力和物理、运维约束显式建模模块，利用仿真迭代学习适应多样化调度场景。

Result: 实验显示，AEOS-Former在任务完成率和能耗效率上明显优于主流基线模型。消融实验验证了各组成部分提升模型性能的贡献。

Conclusion: 本文首次提出了高保真、大规模的AEOS调度基准和性能优异的Transformer调度模型，为实际星座调度研究和应用提供了有力工具。此外，代码与数据集已公开，便于后续研究与实践。

Abstract: Agile Earth Observation Satellites (AEOSs) constellations offer unprecedented
flexibility for monitoring the Earth's surface, but their scheduling remains
challenging under large-scale scenarios, dynamic environments, and stringent
constraints. Existing methods often simplify these complexities, limiting their
real-world performance. We address this gap with a unified framework
integrating a standardized benchmark suite and a novel scheduling model. Our
benchmark suite, AEOS-Bench, contains $3,907$ finely tuned satellite assets and
$16,410$ scenarios. Each scenario features $1$ to $50$ satellites and $50$ to
$300$ imaging tasks. These scenarios are generated via a high-fidelity
simulation platform, ensuring realistic satellite behavior such as orbital
dynamics and resource constraints. Ground truth scheduling annotations are
provided for each scenario. To our knowledge, AEOS-Bench is the first
large-scale benchmark suite tailored for realistic constellation scheduling.
Building upon this benchmark, we introduce AEOS-Former, a Transformer-based
scheduling model that incorporates a constraint-aware attention mechanism. A
dedicated internal constraint module explicitly models the physical and
operational limits of each satellite. Through simulation-based iterative
learning, AEOS-Former adapts to diverse scenarios, offering a robust solution
for AEOS constellation scheduling. Experimental results demonstrate that
AEOS-Former outperforms baseline models in task completion and energy
efficiency, with ablation studies highlighting the contribution of each
component. Code and data are provided in
https://github.com/buaa-colalab/AEOSBench.

</details>


### [38] [Exploring the correlation between the type of music and the emotions evoked: A study using subjective questionnaires and EEG](https://arxiv.org/abs/2510.26304)
*Jelizaveta Jankowska,Bożena Kostek,Fernando Alonso-Fernandez,Prayag Tiwari*

Main category: cs.CV

TL;DR: 本研究探讨了不同类型音乐对人类情感的影响，通过主观问卷和脑电波（EEG）数据进行分析，发现了情感反应与脑电活动的相关性。


<details>
  <summary>Details</summary>
Motivation: 音乐能够显著影响人类情绪，但其不同类型对大脑活动及情感影响的具体机制尚不明确，作者希望通过脑电技术进行深入研究。

Method: 招募不同性别和音乐偏好的被试者，播放不同类型的音乐，同时采集被试的主观问卷反馈和脑电活动数据，再对问卷与脑电信号关系进行分析。

Result: 分析结果显示，不同音乐类型引发了多样的情感反应，并且这些情感变化与特定的脑电波活动存在关联。

Conclusion: 不同类型的音乐确实会对人的情绪和大脑活动产生显著影响，这一发现为进一步研究情感与脑电间的关系提供了实验证据。

Abstract: The subject of this work is to check how different types of music affect
human emotions. While listening to music, a subjective survey and brain
activity measurements were carried out using an EEG helmet. The aim is to
demonstrate the impact of different music genres on emotions. The research
involved a diverse group of participants of different gender and musical
preferences. This had the effect of capturing a wide range of emotional
responses to music. After the experiment, a relationship analysis of the
respondents' questionnaires with EEG signals was performed. The analysis
revealed connections between emotions and observed brain activity.

</details>


### [39] [A Hybrid Framework Bridging CNN and ViT based on Theory of Evidence for Diabetic Retinopathy Grading](https://arxiv.org/abs/2510.26315)
*Junlai Qiu,Yunzhu Chen,Hao Zheng,Yawen Huang,Yuexiang Li*

Main category: cs.CV

TL;DR: 本文提出了一种将CNN和ViT骨干网络特征有效融合的新范式，以提升糖尿病视网膜病变（DR）自动诊断的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是导致中老年人视力丧失的主要原因，目前单一的CNN或ViT方法在DR自动诊断上已陷入性能瓶颈。现有方法难以兼顾CNN的局部特征提取和ViT的全局信息提取能力，因此亟需融合多种骨干网络以进一步提升自动诊断性能。

Method: 作者提出了一种基于证据理论的特征融合范式。具体做法是利用深度证据网络将不同骨干网络（CNN和ViT）提取的特征转化为支持证据，并据此形成聚合的诊断意见，从而自适应地调节融合模式，充分发挥各自优势。

Result: 在两个公开的DR分级数据集上，所提出的混合模型在分级准确率上明显优于现有主流方法，并且在特征融合和决策方面展现了优越的可解释性。

Conclusion: 通过基于证据理论的特征融合方法，有效融合CNN和ViT的优势，可显著提升DR自动诊断的准确率和可解释性，为相关疾病诊断系统的开发提供了新的思路。

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss among middle-aged
and elderly people, which significantly impacts their daily lives and mental
health. To improve the efficiency of clinical screening and enable the early
detection of DR, a variety of automated DR diagnosis systems have been recently
established based on convolutional neural network (CNN) or vision Transformer
(ViT). However, due to the own shortages of CNN / ViT, the performance of
existing methods using single-type backbone has reached a bottleneck. One
potential way for the further improvements is integrating different kinds of
backbones, which can fully leverage the respective strengths of them
(\emph{i.e.,} the local feature extraction capability of CNN and the global
feature capturing ability of ViT). To this end, we propose a novel paradigm to
effectively fuse the features extracted by different backbones based on the
theory of evidence. Specifically, the proposed evidential fusion paradigm
transforms the features from different backbones into supporting evidences via
a set of deep evidential networks. With the supporting evidences, the
aggregated opinion can be accordingly formed, which can be used to adaptively
tune the fusion pattern between different backbones and accordingly boost the
performance of our hybrid model. We evaluated our method on two publicly
available DR grading datasets. The experimental results demonstrate that our
hybrid model not only improves the accuracy of DR grading, compared to the
state-of-the-art frameworks, but also provides the excellent interpretability
for feature fusion and decision-making.

</details>


### [40] [GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?](https://arxiv.org/abs/2510.26339)
*Mingyu Sung,Seungjae Ham,Kangwoo Kim,Yeokyoung Yoon,Sangseok Yun,Il-Min Kim,Jae-Mo Kang*

Main category: cs.CV

TL;DR: 本文提出了一种名为GLYPH-SR的图像超分辨率方法，结合视觉-语言引导的扩散模型，实现了场景文本的高可读性和高视觉质量，在多个数据集上对比基线显著提升了OCR准确率。


<details>
  <summary>Details</summary>
Motivation: 以往超分方法主要优化通用的像素或感知指标，往往忽略了场景文本的清晰度，而文本信息在现实应用中至关重要，如自动驾驶、安防等。此前专注于文本超分方法多基于简化的字符场景，无法解决复杂自然场景下文本的还原问题。为实用化，需显式地同时优化文本可读性和整体视觉质量。

Method: 提出了GLYPH-SR框架，利用视觉-语言引导的扩散模型，并引入专门的Text-SR控制分支与“乒乓”调度器，使超分过程中交替关注文本区域和整体画面。文本还原相关模块在合成语料上单独训练，主SR分支参数冻结，充分融合OCR数据，实现针对性文本修复。

Result: 在SVT、SCUT-CTW1500和CUTE80数据集（4倍和8倍放大）上，GLYPH-SR在OCR评测（F1分数）上相比扩散/GAN基线最高提升15.18个百分点，同时保持MANIQA、CLIP-IQA等感知指标竞争力。

Conclusion: GLYPH-SR能够兼顾场景文本的可读性和整体图像的真实感，为实用场景下提供了“看得真、认得准”的超分辨率解决方案，具有较强的实际应用价值。

Abstract: Image super-resolution(SR) is fundamental to many vision system-from
surveillance and autonomy to document analysis and retail analytics-because
recovering high-frequency details, especially scene-text, enables reliable
downstream perception. Scene-text, i.e., text embedded in natural images such
as signs, product labels, and storefronts, often carries the most actionable
information; when characters are blurred or hallucinated, optical character
recognition(OCR) and subsequent decisions fail even if the rest of the image
appears sharp. Yet previous SR research has often been tuned to distortion
(PSNR/SSIM) or learned perceptual metrics (LIPIS, MANIQA, CLIP-IQA, MUSIQ) that
are largely insensitive to character-level errors. Furthermore, studies that do
address text SR often focus on simplified benchmarks with isolated characters,
overlooking the challenges of text within complex natural scenes. As a result,
scene-text is effectively treated as generic texture. For SR to be effective in
practical deployments, it is therefore essential to explicitly optimize for
both text legibility and perceptual quality. We present GLYPH-SR, a
vision-language-guided diffusion framework that aims to achieve both objectives
jointly. GLYPH-SR utilizes a Text-SR Fusion ControlNet(TS-ControlNet) guided by
OCR data, and a ping-pong scheduler that alternates between text- and
scene-centric guidance. To enable targeted text restoration, we train these
components on a synthetic corpus while keeping the main SR branch frozen.
Across SVT, SCUT-CTW1500, and CUTE80 at x4, and x8, GLYPH-SR improves OCR F1 by
up to +15.18 percentage points over diffusion/GAN baseline (SVT x8, OpenOCR)
while maintaining competitive MANIQA, CLIP-IQA, and MUSIQ. GLYPH-SR is designed
to satisfy both objectives simultaneously-high readability and high visual
realism-delivering SR that looks right and reds right.

</details>


### [41] [EEG-Driven Image Reconstruction with Saliency-Guided Diffusion Models](https://arxiv.org/abs/2510.26391)
*Igor Abramov,Ilya Makarov*

Main category: cs.CV

TL;DR: 该论文提出了一种结合EEG特征和空间显著性图的双条件框架，用以提升脑电驱动图像重建的质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的脑电驱动图像生成方法往往忽视了空间注意力机制，导致重建图像在细节和语义上一致性不足。为此，作者希望通过引入空间显著性信息提升生成效果。

Method: 作者使用了ATM（自适应思维映射器）提取EEG特征，并通过LoRA对Stable Diffusion 2.1进行微调，使脑电信号与视觉语义对齐。同时，增加了ControlNet分支，将空间显著性图作为条件，指导生成过程对空间分布进行控制。

Result: 在THINGS-EEG数据集上的实验表明，所提方法在图像的低级和高级特征质量方面均优于现有方法，并且生成结果与人体视觉注意高度一致。

Conclusion: 整合空间注意先验能够消除EEG信号的不确定性，实现高保真重建。该方法在医疗诊断及神经自适应界面等领域具有应用潜力，并推动了基于预训练扩散模型的脑信号解码研究。

Abstract: Existing EEG-driven image reconstruction methods often overlook spatial
attention mechanisms, limiting fidelity and semantic coherence. To address
this, we propose a dual-conditioning framework that combines EEG embeddings
with spatial saliency maps to enhance image generation. Our approach leverages
the Adaptive Thinking Mapper (ATM) for EEG feature extraction and fine-tunes
Stable Diffusion 2.1 via Low-Rank Adaptation (LoRA) to align neural signals
with visual semantics, while a ControlNet branch conditions generation on
saliency maps for spatial control. Evaluated on THINGS-EEG, our method achieves
a significant improvement in the quality of low- and high-level image features
over existing approaches. Simultaneously, strongly aligning with human visual
attention. The results demonstrate that attentional priors resolve EEG
ambiguities, enabling high-fidelity reconstructions with applications in
medical diagnostics and neuroadaptive interfaces, advancing neural decoding
through efficient adaptation of pre-trained diffusion models.

</details>


### [42] [LoCoT2V-Bench: A Benchmark for Long-Form and Complex Text-to-Video Generation](https://arxiv.org/abs/2510.26412)
*Xiangqing Zheng,Chengyue Wu,Kehai Chen,Min Zhang*

Main category: cs.CV

TL;DR: 本文提出了LoCoT2V-Bench，一个专为复杂输入条件下的长视频生成任务设计的基准，系统性解决了现有评测机制的不足，促进了更细粒度和更高层次的生成效果改进。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频生成的评测多依赖简化的提示与低层级指标，缺乏对复杂长视频生成质量的严格和全面评估，尤其在叙事连贯性和主题表达等抽象维度存在评测盲区。为推动长视频生成技术发展，亟需更具挑战性和现实性的评测基准。

Method: 构建了LoCoT2V-Bench基准，采用真实世界的长视频并设计了包含场景切换、事件动态等元素的复杂提示词。同时，提出了多维度评价体系，包括事件级对齐、细粒度时序一致性、内容清晰度及HERD等新指标，从视觉、时序及叙事层面全面评测生成效果。

Result: 利用该基准全面评测了九种代表性长视频生成模型，发现当前方法虽然在基础视觉和时序维度表现良好，但在事件一致性、细粒度对齐及高层次主题表达等方面均存在明显不足。

Conclusion: LoCoT2V-Bench为长视频生成提供了更系统、全面及可靠的评测平台，明确指出了现有模型亟需改进的关键方向，为后续研究提供重要参考和推动力。

Abstract: Recently text-to-video generation has made impressive progress in producing
short, high-quality clips, but evaluating long-form outputs remains a major
challenge especially when processing complex prompts. Existing benchmarks
mostly rely on simplified prompts and focus on low-level metrics, overlooking
fine-grained alignment with prompts and abstract dimensions such as narrative
coherence and thematic expression. To address these gaps, we propose
LoCoT2V-Bench, a benchmark specifically designed for long video generation
(LVG) under complex input conditions. Based on various real-world videos,
LoCoT2V-Bench introduces a suite of realistic and complex prompts incorporating
elements like scene transitions and event dynamics. Moreover, it constructs a
multi-dimensional evaluation framework that includes our newly proposed metrics
such as event-level alignment, fine-grained temporal consistency, content
clarity, and the Human Expectation Realization Degree (HERD) that focuses on
more abstract attributes like narrative flow, emotional response, and character
development. Using this framework, we conduct a comprehensive evaluation of
nine representative LVG models, finding that while current methods perform well
on basic visual and temporal aspects, they struggle with inter-event
consistency, fine-grained alignment, and high-level thematic adherence, etc.
Overall, LoCoT2V-Bench provides a comprehensive and reliable platform for
evaluating long-form complex text-to-video generation and highlights critical
directions for future method improvement.

</details>


### [43] [A-TPT: Angular Diversity Calibration Properties for Test-Time Prompt Tuning of Vision-Language Models](https://arxiv.org/abs/2510.26441)
*Shihab Aaqil Ahamed,Udaya S. K. P. Miriya Thanthrige,Ranga Rodrigo,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 本文提出了一种新的测试时提示调整（A-TPT）方法，通过最大化特征之间的最小角距离，在单位超球面上促进文本特征的角度多样性，从而改善大模型在无标签新任务上的校准表现。实验显示该方法显著减少了校准误差，且适应性优良。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉-语言模型（VLMs）在处理未知任务时，缺乏标签依赖变得重要，但测试时提示调整存在文本特征分布分散度不足的问题，影响模型的可靠性与安全性。以往方法主要关注平均分散度或正交约束，但对类别间特征角度多样性刻画不足。

Method: A-TPT框架通过最大化类别间文本特征的最小角距离，鼓励其在单位超球面上均匀分布，从而实现更好的特征分散度和角度多样性。方法包含数学优化设计，并在多个主干网络和数据集上进行了实验和理论分析。

Result: A-TPT在不同数据集和网络架构中普遍优于现有方法，显著降低了平均校准误差，同时保持甚至提升准确率，尤其在自然分布转移和医学数据集上的零样本校准表现突出。

Conclusion: 促进文本特征的角度多样性对于提升VLMs的测试时适应校准效果极为有效。A-TPT不仅提升校准能力，且兼具广泛适应性，具有实际应用前景。代码将开源。

Abstract: Test-time prompt tuning (TPT) has emerged as a promising technique for
adapting large vision-language models (VLMs) to unseen tasks without relying on
labeled data. However, the lack of dispersion between textual features can hurt
calibration performance, which raises concerns about VLMs' reliability,
trustworthiness, and safety. Current TPT approaches primarily focus on
improving prompt calibration by either maximizing average textual feature
dispersion or enforcing orthogonality constraints to encourage angular
separation. However, these methods may not always have optimal angular
separation between class-wise textual features, which implies overlooking the
critical role of angular diversity. To address this, we propose A-TPT, a novel
TPT framework that introduces angular diversity to encourage uniformity in the
distribution of normalized textual features induced by corresponding learnable
prompts. This uniformity is achieved by maximizing the minimum pairwise angular
distance between features on the unit hypersphere. We show that our approach
consistently surpasses state-of-the-art TPT methods in reducing the aggregate
average calibration error while maintaining comparable accuracy through
extensive experiments with various backbones on different datasets. Notably,
our approach exhibits superior zero-shot calibration performance on natural
distribution shifts and generalizes well to medical datasets. We provide
extensive analyses, including theoretical aspects, to establish the grounding
of A-TPT. These results highlight the potency of promoting angular diversity to
achieve well-dispersed textual features, significantly improving VLM
calibration during test-time adaptation. Our code will be made publicly
available.

</details>


### [44] [PointSt3R: Point Tracking through 3D Grounded Correspondence](https://arxiv.org/abs/2510.26443)
*Rhodri Guerrier,Adam W. Harley,Dima Damen*

Main category: cs.CV

TL;DR: 本文提出将3D重建模型DUSt3R和MASt3R用于3D点追踪任务，通过结合重建损失、动态对应训练与可见性预测，仅用较少合成数据即可提升动态与静态点追踪性能，并在多数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前3D重建模型在静态场景匹配表现优异，但点追踪任务中如何充分发挥这些模型能力，尤其是动态点的追踪效果提升，是亟需解决的问题。

Method: 首先验证DUSt3R和MASt3R在静态点追踪的竞争力。然后创新性地将重建损失和动态点对应训练结合，引入可见性Head，并以少量合成数据微调MASt3R。训练和评估时仅用含查询点的帧对，不利用完整时序上下文，动态与静态点训练混合。

Result: 在EgoPoints数据集上，相比CoTracker2提升33.5%；在四个主流数据集上点追踪精度达到或超过SOTA方法，如在TAP-Vid-DAVIS上具竞争力，在EgoPoints和RGB-S均显著优于CoTracker3。

Conclusion: 3D重建模型经过改造和针对性训练，可高效地融合动态-静态点对应，带来优异的点追踪表现，为相关任务提供更强基础。

Abstract: Recent advances in foundational 3D reconstruction models, such as DUSt3R and
MASt3R, have shown great potential in 2D and 3D correspondence in static
scenes. In this paper, we propose to adapt them for the task of point tracking
through 3D grounded correspondence. We first demonstrate that these models are
competitive point trackers when focusing on static points, present in current
point tracking benchmarks ($+33.5\%$ on EgoPoints vs. CoTracker2). We propose
to combine the reconstruction loss with training for dynamic correspondence
along with a visibility head, and fine-tuning MASt3R for point tracking using a
relatively small amount of synthetic data. Importantly, we only train and
evaluate on pairs of frames where one contains the query point, effectively
removing any temporal context. Using a mix of dynamic and static point
correspondences, we achieve competitive or superior point tracking results on
four datasets (e.g. competitive on TAP-Vid-DAVIS 73.8 $\delta_{avg}$ / 85.8\%
occlusion acc. for PointSt3R compared to 75.7 / 88.3\% for CoTracker2; and
significantly outperform CoTracker3 on EgoPoints 61.3 vs 54.2 and RGB-S 87.0 vs
82.8). We also present results on 3D point tracking along with several
ablations on training datasets and percentage of dynamic correspondences.

</details>


### [45] [Towards Fine-Grained Vision-Language Alignment for Few-Shot Anomaly Detection](https://arxiv.org/abs/2510.26464)
*Yuanting Fan,Jun Liu,Xiaochen Chen,Bin-Bin Gao,Jian Li,Yong Liu,Jinlong Peng,Chengjie Wang*

Main category: cs.CV

TL;DR: 本论文针对少样本异常检测（FSAD）任务，提出利用多层次细粒度文本描述来提升异常区域定位精度，并设计了FineGrainedAD方法，实验结果显示在主流数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有FSAD方法依赖于预训练视觉-语言模型（VLM），通过文本和图像的特征相似性检测异常区域。但受限于文本描述不够细致，仅能为图像整体设定描述，导致描述与视觉异常存在语义错配，影响异常区域定位精度。

Method: 提出多层次细粒度语义标注（MFSC）方法，自动为异常检测数据集生成多层次和细粒度的文本描述。基于此，设计了FineGrainedAD框架，包括多层次可学习提示（MLLP）和多层次语义对齐（MLSA）：前者通过自动替换与拼接，为提示引入细粒度语义；后者利用区域聚合和多层对齐训练，使提示更好地对齐视觉区域。

Result: FineGrainedAD方法在MVTec-AD和VisA主流少样本异常检测数据集上实现了更优的整体检测和定位表现。

Conclusion: 引入多层次细粒度文本语义和对应的可学习提示与对齐机制，可以有效提升FSAD任务中异常区域的定位能力，优于现有方法。

Abstract: Few-shot anomaly detection (FSAD) methods identify anomalous regions with few
known normal samples. Most existing methods rely on the generalization ability
of pre-trained vision-language models (VLMs) to recognize potentially anomalous
regions through feature similarity between text descriptions and images.
However, due to the lack of detailed textual descriptions, these methods can
only pre-define image-level descriptions to match each visual patch token to
identify potential anomalous regions, which leads to the semantic misalignment
between image descriptions and patch-level visual anomalies, achieving
sub-optimal localization performance. To address the above issues, we propose
the Multi-Level Fine-Grained Semantic Caption (MFSC) to provide multi-level and
fine-grained textual descriptions for existing anomaly detection datasets with
automatic construction pipeline. Based on the MFSC, we propose a novel
framework named FineGrainedAD to improve anomaly localization performance,
which consists of two components: Multi-Level Learnable Prompt (MLLP) and
Multi-Level Semantic Alignment (MLSA). MLLP introduces fine-grained semantics
into multi-level learnable prompts through automatic replacement and
concatenation mechanism, while MLSA designs region aggregation strategy and
multi-level alignment training to facilitate learnable prompts better align
with corresponding visual regions. Experiments demonstrate that the proposed
FineGrainedAD achieves superior overall performance in few-shot settings on
MVTec-AD and VisA datasets.

</details>


### [46] [Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition](https://arxiv.org/abs/2510.26466)
*Pei Peng,MingKun Xie,Hang Hao,Tong Jin,ShengJun Huang*

Main category: cs.CV

TL;DR: 该论文提出了一种新的因果推断方法，缓解视觉-语言模型中的物体-背景捷径问题，通过合成反事实嵌入，有效提升模型零样本泛化能力，无需重新训练或复杂提示设计。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（如CLIP）常依赖于训练中常见的物体-环境共现关系，这一捷径使得模型在新场景下的零样本可靠性大打折扣。因此，亟需解决该模型对背景的依赖，提升其因果推理和泛化能力。

Method: 作者将该问题重新表述为因果推断问题，通过在CLIP的表征空间内分别估算物体与背景的统计期望，然后将同一物体特征与来自外部数据集、同批邻居、或文本描述中采样的多样化背景进行反事实合成。进一步，作者利用因果推断中的Total Direct Effect思想，通过减去仅含背景的激活来净化物体表示。

Result: 所提方法在无需模型重训练或特殊提示设计的情况下，显著提升了零样本场景下，各类依赖背景信息的基准测试上的最差组和平均准确率，并创造了新的零样本SOTA表现。

Conclusion: 本文提出了一种轻量级的反事实嵌入因果推断方法，有效缓解了视觉-语言模型物体-背景依赖问题，为多模态模型的去偏和可靠推理提供了实用新途径。

Abstract: Object-context shortcuts remain a persistent challenge in vision-language
models, undermining zero-shot reliability when test-time scenes differ from
familiar training co-occurrences. We recast this issue as a causal inference
problem and ask: Would the prediction remain if the object appeared in a
different environment? To answer this at inference time, we estimate object and
background expectations within CLIP's representation space, and synthesize
counterfactual embeddings by recombining object features with diverse
alternative contexts sampled from external datasets, batch neighbors, or
text-derived descriptions. By estimating the Total Direct Effect and simulating
intervention, we further subtract background-only activation, preserving
beneficial object-context interactions while mitigating hallucinated scores.
Without retraining or prompt design, our method substantially improves both
worst-group and average accuracy on context-sensitive benchmarks, establishing
a new zero-shot state of the art. Beyond performance, our framework provides a
lightweight representation-level counterfactual approach, offering a practical
causal avenue for debiased and reliable multimodal reasoning.

</details>


### [47] [Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing](https://arxiv.org/abs/2510.26474)
*Xin Guo,Zhiheng Xi,Yiwen Ding,Yitao Zhai,Xiaowei Shi,Xunliang Cai,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CV

TL;DR: 本论文发现，大型视觉-语言模型（LVLM）在自我提升过程中过于关注简单问题，逐渐导致对复杂问题的忽视，从而出现“马太效应”，研究提出分布重塑和轨迹重采样等策略，有效缓解这种失衡，实验验证方法能显著提升视觉推理能力。


<details>
  <summary>Details</summary>
Motivation: 自我提升成为LVLM提升推理能力的主流方式，但模型在迭代学习中对简单任务不断强化，复杂推理能力没有同步提升，导致性能瓶颈。解决这一不平衡有助于LVLM更好地处理复杂推理场景。

Method: 设计了四种高效策略，基于分布重塑（distribution-reshaping）和轨迹重采样（trajectory-resampling）两大方向，使自我提升学习过程能对简单与复杂任务达到平衡。

Result: 在Qwen2-VL-7B-Instruct和InternVL2.5-4B两个主流LVLM上进行大量视觉推理实验，新方法平均优于基础自我提升方法3.86个点，视觉推理能力整体提升。

Conclusion: 本文提出的平衡策略能有效缓解LVLM自我提升中的“马太效应”，显著提高模型在复杂视觉推理任务中的表现，为自我提升方法的发展指明新方向。

Abstract: Self-improvement has emerged as a mainstream paradigm for advancing the
reasoning capabilities of large vision-language models (LVLMs), where models
explore and learn from successful trajectories iteratively. However, we
identify a critical issue during this process: the model excels at generating
high-quality trajectories for simple queries (i.e., head data) but struggles
with more complex ones (i.e., tail data). This leads to an imbalanced
optimization that drives the model to prioritize simple reasoning skills, while
hindering its ability to tackle more complex reasoning tasks. Over iterations,
this imbalance becomes increasingly pronounced--a dynamic we term the "Matthew
effect"--which ultimately hinders further model improvement and leads to
performance bottlenecks. To counteract this challenge, we introduce four
efficient strategies from two perspectives: distribution-reshaping and
trajectory-resampling, to achieve head-tail re-balancing during the
exploration-and-learning self-improvement process. Extensive experiments on
Qwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks
demonstrate that our methods consistently improve visual reasoning
capabilities, outperforming vanilla self-improvement by 3.86 points on average.

</details>


### [48] [Analysis of the Robustness of an Edge Detector Based on Cellular Automata Optimized by Particle Swarm](https://arxiv.org/abs/2510.26509)
*Vinícius Ferraria,Eurico Ruivo*

Main category: cs.CV

TL;DR: 本文提出并分析了一种结合二维元胞自动机、元启发式优化与迁移学习技术的自适应边缘检测方法，并验证其对不同图像类别的适应性。


<details>
  <summary>Details</summary>
Motivation: 传统边缘检测器在检测松散边缘和利用上下文信息方面存在不足，难以适应不同图像特性。本文旨在开发一种能更好适应不同图像属性的自适应边缘检测方法。

Method: 提出一种基于二维元胞自动机的边缘检测方法，并通过元启发式方法进行优化，结合迁移学习提升适应能力。研究侧重分析优化阶段扩展搜索空间和评估模型对自然图像及其子集的适应性与鲁棒性。

Result: 实验结果显示，在本研究所选图像集下，扩展优化搜索空间并无效果。同时，迁移学习技术对模型性能提升不显著。无论采用何种验证方法，所提模型均能较好适应输入数据。

Conclusion: 扩展搜索空间升级优化流程对当前数据集无效，迁移学习对模型无显著帮助，但所提自适应边缘检测器本身具有较强的数据适应能力。

Abstract: The edge detection task is essential in image processing aiming to extract
relevant information from an image. One recurring problem in this task is the
weaknesses found in some detectors, such as the difficulty in detecting loose
edges and the lack of context to extract relevant information from specific
problems. To address these weaknesses and adapt the detector to the properties
of an image, an adaptable detector described by two-dimensional cellular
automaton and optimized by meta-heuristic combined with transfer learning
techniques was developed. This study aims to analyze the impact of expanding
the search space of the optimization phase and the robustness of the
adaptability of the detector in identifying edges of a set of natural images
and specialized subsets extracted from the same image set. The results obtained
prove that expanding the search space of the optimization phase was not
effective for the chosen image set. The study also analyzed the adaptability of
the model through a series of experiments and validation techniques and found
that, regardless of the validation, the model was able to adapt to the input
and the transfer learning techniques applied to the model showed no significant
improvements.

</details>


### [49] [SA$^{2}$Net: Scale-Adaptive Structure-Affinity Transformation for Spine Segmentation from Ultrasound Volume Projection Imaging](https://arxiv.org/abs/2510.26568)
*Hao Xie,Zixun Huang,Yushen Zuo,Yakun Ju,Frank H. F. Leung,N. F. Law,Kin-Man Lam,Yong-Ping Zheng,Sai Ho Ling*

Main category: cs.CV

TL;DR: 该论文提出了一种新型尺度自适应结构感知网络（SA$^{2}$Net），显著提升了基于超声影像的脊柱分割性能，有效推动智能脊柱侧弯诊断应用。


<details>
  <summary>Details</summary>
Motivation: 当前基于超声体积投影成像的脊柱分割存在两个主要挑战：一是未充分利用骨骼间丰富的空间相关性，导致全局上下文信息学习不足；二是脊柱结构的形状与位置信息未被系统编码进分割流程。这些问题限制了智能化脊柱侧弯诊断的分割精度。

Method: 作者提出了SA$^{2}$Net，主要包括三点创新：(1) 尺度自适应互补策略，用于捕捉脊柱影像的跨维度长距离相关特征；(2) 结构亲和转换机制，将多头自注意力模块与结构语义关联结合，用以提升结构认知；(3) 引入特征混合损失聚合，有助于模型训练，提高分割鲁棒性与准确度。

Result: 大量实验表明，SA$^{2}$Net分割性能优于现有先进方法，并且对不同主干网络有较强适应性。

Conclusion: SA$^{2}$Net在提升智能脊柱影像分析与侧弯诊断方面展现出较大潜力，为临床应用提供了有力支持。

Abstract: Spine segmentation, based on ultrasound volume projection imaging (VPI),
plays a vital role for intelligent scoliosis diagnosis in clinical
applications. However, this task faces several significant challenges. Firstly,
the global contextual knowledge of spines may not be well-learned if we neglect
the high spatial correlation of different bone features. Secondly, the spine
bones contain rich structural knowledge regarding their shapes and positions,
which deserves to be encoded into the segmentation process. To address these
challenges, we propose a novel scale-adaptive structure-aware network
(SA$^{2}$Net) for effective spine segmentation. First, we propose a
scale-adaptive complementary strategy to learn the cross-dimensional
long-distance correlation features for spinal images. Second, motivated by the
consistency between multi-head self-attention in Transformers and semantic
level affinity, we propose structure-affinity transformation to transform
semantic features with class-specific affinity and combine it with a
Transformer decoder for structure-aware reasoning. In addition, we adopt a
feature mixing loss aggregation method to enhance model training. This method
improves the robustness and accuracy of the segmentation process. The
experimental results demonstrate that our SA$^{2}$Net achieves superior
segmentation performance compared to other state-of-the-art methods. Moreover,
the adaptability of SA$^{2}$Net to various backbones enhances its potential as
a promising tool for advanced scoliosis diagnosis using intelligent spinal
image analysis. The code and experimental demo are available at
https://github.com/taetiseo09/SA2Net.

</details>


### [50] [AdSum: Two-stream Audio-visual Summarization for Automated Video Advertisement Clipping](https://arxiv.org/abs/2510.26569)
*Wen Xie,Yanjun Zhu,Gijs Overgoor,Yakov Bart,Agata Lapedriza Garcia,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: 本论文提出一种用于自动化广告视频剪辑的新框架，有效节省人工剪辑时间，并在多个评测标准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 广告主通常需要同一广告的不同时长版本，传统人工剪辑耗时费力，因此亟需自动化工具提升效率。

Method: 作者首次将广告剪辑建模为镜头选择问题，提出结合视觉与音频信息的双流融合模型，以更精准预测广告视频帧的重要性；并创建了专属于广告领域的新数据集AdSum204辅助研究。

Result: 提出的方法在多个主流指标（Average Precision、AUC、Spearman和Kendall）上均优于目前最先进的视频摘要方法。

Conclusion: 针对广告场景，结合音频和视觉的自动视频剪辑方法可显著提升短广告生成的效率和质量，对行业具有应用价值。

Abstract: Advertisers commonly need multiple versions of the same advertisement (ad) at
varying durations for a single campaign. The traditional approach involves
manually selecting and re-editing shots from longer video ads to create shorter
versions, which is labor-intensive and time-consuming. In this paper, we
introduce a framework for automated video ad clipping using video summarization
techniques. We are the first to frame video clipping as a shot selection
problem, tailored specifically for advertising. Unlike existing general video
summarization methods that primarily focus on visual content, our approach
emphasizes the critical role of audio in advertising. To achieve this, we
develop a two-stream audio-visual fusion model that predicts the importance of
video frames, where importance is defined as the likelihood of a frame being
selected in the firm-produced short ad. To address the lack of ad-specific
datasets, we present AdSum204, a novel dataset comprising 102 pairs of
30-second and 15-second ads from real advertising campaigns. Extensive
experiments demonstrate that our model outperforms state-of-the-art methods
across various metrics, including Average Precision, Area Under Curve,
Spearman, and Kendall.

</details>


### [51] [Dynamic Context-Aware Scene Reasoning Using Vision-Language Alignment in Zero-Shot Real-World Scenarios](https://arxiv.org/abs/2510.26580)
*Manjunath Prasad Holenarasipura Rajiv,B. M. Vidyavathi*

Main category: cs.CV

TL;DR: 本文提出了一种动态上下文感知场景推理框架，通过融合视觉与语言信息，提升AI系统在零样本场景下的理解能力。


<details>
  <summary>Details</summary>
Motivation: 在现实环境中，AI系统常常会遇到无标注的新场景，传统的场景理解模型难以泛化到这些未知环境，限制了其在动态、非结构化环境中的部署。

Method: 作者提出了动态上下文感知场景推理框架，将预训练视觉Transformer与大型语言模型相结合，对齐视觉语义与自然语言描述。框架包括一个动态推理模块，结合全局场景线索、对象级别交互与语言先验进行预测优化。

Result: 在COCO、Visual Genome和Open Images等零样本场景基准上，该方法比基线模型在复杂与未知环境下的场景理解准确率提升高达18％，并在模糊或拥挤场景中表现出更强的鲁棒性。

Conclusion: 该框架为上下文感知推理提供了一种可扩展且可解释的解决方案，提升了AI系统在动态和真实世界环境的零样本泛化能力。

Abstract: In real-world environments, AI systems often face unfamiliar scenarios
without labeled data, creating a major challenge for conventional scene
understanding models. The inability to generalize across unseen contexts limits
the deployment of vision-based applications in dynamic, unstructured settings.
This work introduces a Dynamic Context-Aware Scene Reasoning framework that
leverages Vision-Language Alignment to address zero-shot real-world scenarios.
The goal is to enable intelligent systems to infer and adapt to new
environments without prior task-specific training. The proposed approach
integrates pre-trained vision transformers and large language models to align
visual semantics with natural language descriptions, enhancing contextual
comprehension. A dynamic reasoning module refines predictions by combining
global scene cues and object-level interactions guided by linguistic priors.
Extensive experiments on zero-shot benchmarks such as COCO, Visual Genome, and
Open Images demonstrate up to 18% improvement in scene understanding accuracy
over baseline models in complex and unseen environments. Results also show
robust performance in ambiguous or cluttered scenes due to the synergistic
fusion of vision and language. This framework offers a scalable and
interpretable approach for context-aware reasoning, advancing zero-shot
generalization in dynamic real-world settings.

</details>


### [52] [CATCH: A Modular Cross-domain Adaptive Template with Hook](https://arxiv.org/abs/2510.26582)
*Xinjin Li,Yulie Lu,Jinghan Cao,Yu Ma,Zhenglin Li,Yeyang Zhou*

Main category: cs.CV

TL;DR: 提出了一个可插拔的跨领域适应性框架CATCH，用于提升VQA模型在不同领域的泛化能力，无需对主体模型重新训练。


<details>
  <summary>Details</summary>
Motivation: 当前VQA（视觉问答）模型虽然在自然图像领域表现优异，但在遥感、医疗影像、数学图表等非自然图像领域存在泛化能力不足的问题，主要原因包括分布差异大和缺乏有效的领域适配机制。现有方法多依赖于每个领域独立微调或定制化方案，成本高、不灵活、难以扩展。

Method: 提出CATCH框架。核心思想是视觉与语言的适配解耦，通过引入两个轻量模块：域分类器（识别图像类型）和双适配器机制（语言提示适配器与视觉适配器）。这两个模块通过统一hook接口动态注入，无需对原始主干模型进行重新训练。

Result: 在四个不同领域的VQA数据集上进行实验，CATCH框架在无需主干模型重训练的条件下，实现了稳定的性能提升。例如在MathVQA提升2.3 BLEU分数，在MedVQA-RAD提升2.6 VQA分数，在ChartQA提升3.1 ROUGE分数。

Conclusion: CATCH框架为多领域的VQA提供了一种可扩展、易部署的方法，能够在保持主干模型不变的情况下，有效提升跨域泛化能力，适用于实际多领域应用场景。

Abstract: Recent advances in Visual Question Answering (VQA) have demonstrated
impressive performance in natural image domains, with models like LLaVA
leveraging large language models (LLMs) for open-ended reasoning. However,
their generalization degrades significantly when transferred to out-of-domain
scenarios such as remote sensing, medical imaging, or math diagrams, due to
large distributional shifts and the lack of effective domain adaptation
mechanisms. Existing approaches typically rely on per-domain fine-tuning or
bespoke pipelines, which are costly, inflexible, and not scalable across
diverse tasks. In this paper, we propose CATCH, a plug-and-play framework for
cross-domain adaptation that improves the generalization of VQA models while
requiring minimal changes to their core architecture. Our key idea is to
decouple visual and linguistic adaptation by introducing two lightweight
modules: a domain classifier to identify the input image type, and a dual
adapter mechanism comprising a Prompt Adapter for language modulation and a
Visual Adapter for vision feature adjustment. Both modules are dynamically
injected via a unified hook interface, requiring no retraining of the backbone
model. Experimental results across four domain-specific VQA benchmarks
demonstrate that our framework achieves consistent performance gains without
retraining the backbone model, including +2.3 BLEU on MathVQA, +2.6 VQA on
MedVQA-RAD, and +3.1 ROUGE on ChartQA. These results highlight that CATCH
provides a scalable and extensible approach to multi-domain VQA, enabling
practical deployment across diverse application domains.

</details>


### [53] [Emu3.5: Native Multimodal Models are World Learners](https://arxiv.org/abs/2510.26583)
*Yufeng Cui,Honghao Chen,Haoge Deng,Xu Huang,Xinghang Li,Jirong Liu,Yang Liu,Zhuoyan Luo,Jinsheng Wang,Wenxuan Wang,Yueze Wang,Chengyuan Wang,Fan Zhang,Yingli Zhao,Ting Pan,Xianduo Li,Zecheng Hao,Wenxuan Ma,Zhuo Chen,Yulong Ao,Tiejun Huang,Zhongyuan Wang,Xinlong Wang*

Main category: cs.CV

TL;DR: Emu3.5 是一种新型大规模多模态世界模型，通过在超大规模视觉-语言数据上进行端到端训练，可以原生地处理和生成交错的视觉与语言信息，并在推理效率、多样化任务完成能力等方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 目前多模态模型在同时处理和生成视觉、语言内容上仍存挑战，通用性与推理效率亟需提升。作者希望通过大模型端到端联合视觉与语言建模，并高效推理，从而拓展模型在复杂世界任务上的适应能力。

Method: Emu3.5 采用统一的下一个token预测目标，在超过10万亿token的大规模视觉-语言数据上端到端预训练，并利用大规模强化学习进行后训练，以增强其多模态推理和生成能力。同时提出“离散扩散自适应”（DiDA），将逐token解码转为双向并行预测，提高推理效率。

Result: Emu3.5 原生具备长时序视觉-语言生成、任意模态到图像生成（X2I）、复杂文本丰富图像生成等能力。在图像生成和编辑任务上表现与Gemini 2.5 Flash Image（Nano Banana）相当，在多项交错生成任务上优于同类模型。支持时空一致性世界推理和开放式场景操控。

Conclusion: Emu3.5 在多模态理解与生成领域设立了新的基准，并通过创新的推理加速技术显著提升了多模态世界模型的效率和通用性。相关模型已开源，方便学界进一步研究。

Abstract: We introduce Emu3.5, a large-scale multimodal world model that natively
predicts the next state across vision and language. Emu3.5 is pre-trained
end-to-end with a unified next-token prediction objective on a corpus of
vision-language interleaved data containing over 10 trillion tokens, primarily
derived from sequential frames and transcripts of internet videos. The model
naturally accepts interleaved vision-language inputs and generates interleaved
vision-language outputs. Emu3.5 is further post-trained with large-scale
reinforcement learning to enhance multimodal reasoning and generation. To
improve inference efficiency, we propose Discrete Diffusion Adaptation (DiDA),
which converts token-by-token decoding into bidirectional parallel prediction,
accelerating per-image inference by about 20x without sacrificing performance.
Emu3.5 exhibits strong native multimodal capabilities, including long-horizon
vision-language generation, any-to-image (X2I) generation, and complex
text-rich image generation. It also exhibits generalizable world-modeling
abilities, enabling spatiotemporally consistent world exploration and
open-world embodied manipulation across diverse scenarios and tasks. For
comparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image
(Nano Banana) on image generation and editing tasks and demonstrates superior
results on a suite of interleaved generation tasks. We open-source Emu3.5 at
https://github.com/baaivision/Emu3.5 to support community research.

</details>


### [54] [ResMatching: Noise-Resilient Computational Super-Resolution via Guided Conditional Flow Matching](https://arxiv.org/abs/2510.26601)
*Anirban Ray,Vera Galinova,Florian Jug*

Main category: cs.CV

TL;DR: 该论文提出了一种新的基于引导条件流匹配（ResMatching）的计算型荧光显微超分辨率方法，并在多种生物结构数据集上，与多种基线方法比较，取得了更好的数据保真和感知质量平衡表现。


<details>
  <summary>Details</summary>
Motivation: 在荧光显微超分辨率领域，传统方法面对频率外插和先验假设弱的问题。随着数据驱动的机器学习方法的发展，更强的数据先验能被学习，并可能推动成像极限的突破。

Method: 论文提出了ResMatching方法，通过引导条件流匹配（guided conditional flow matching）机制自动学习并利用数据先验，从低分辨率图像推断高分辨细节。该方法还能对采样的后验分布进行建模，进而实现像素级不确定性估计。

Result: 在BioSR数据集中涉及的4种不同生物结构上，ResMatching与7种基线方法进行对比，始终实现了数据一致性和感知真实性间的最佳折中，尤其适合低分辨率输入噪声较大、先验学习困难的场景。

Conclusion: ResMatching为计算型荧光显微超分辨率提供了一种有效的数据驱动新方法，不仅提升了重建质量，还能评估预测不确定性，为用户提供了更可靠的结果和指导。

Abstract: Computational Super-Resolution (CSR) in fluorescence microscopy has, despite
being an ill-posed problem, a long history. At its very core, CSR is about
finding a prior that can be used to extrapolate frequencies in a micrograph
that have never been imaged by the image-generating microscope. It stands to
reason that, with the advent of better data-driven machine learning techniques,
stronger prior can be learned and hence CSR can lead to better results. Here,
we present ResMatching, a novel CSR method that uses guided conditional flow
matching to learn such improved data-priors. We evaluate ResMatching on 4
diverse biological structures from the BioSR dataset and compare its results
against 7 baselines. ResMatching consistently achieves competitive results,
demonstrating in all cases the best trade-off between data fidelity and
perceptual realism. We observe that CSR using ResMatching is particularly
effective in cases where a strong prior is hard to learn, e.g. when the given
low-resolution images contain a lot of noise. Additionally, we show that
ResMatching can be used to sample from an implicitly learned posterior
distribution and that this distribution is calibrated for all tested use-cases,
enabling our method to deliver a pixel-wise data-uncertainty term that can
guide future users to reject uncertain predictions.

</details>


### [55] [CYPRESS: Crop Yield Prediction via Regression on Prithvi's Encoder for Satellite Sensing](https://arxiv.org/abs/2510.26609)
*Shayan Nejadshamsi,Yuanyuan Zhang,Shadi Zaki,Brock Porth,Lysa Porth,Vahab Khoshdel*

Main category: cs.CV

TL;DR: 本文提出了CYPRESS模型，基于大规模遥感预训练模型，实现了对油菜田块内高分辨率产量预测，并优于现有深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统产量预测方法难以满足精细农业对高分辨率和大规模应用的需求，因此需要可扩展且精细的方法辅助农业管理和全球粮食安全。

Method: 提出CYPRESS，一种利用Prithvi-EO-2.0-600M大模型的深度学习方法，将多时相卫星遥感影像转化为像素级连续产量图，对基础模型进行微调，适配为回归任务。

Result: CYPRESS在加拿大草原油菜数据集上优于当前深度学习预测模型，能输出连续高分辨率产量图，更适用于精准农业。

Conclusion: 微调大规模地理基础模型可提升农作物产量预测的精度和分辨率，为精准农业监测和生产决策提供了可扩展的新方法。

Abstract: Accurate and timely crop yield prediction is crucial for global food security
and modern agricultural management. Traditional methods often lack the
scalability and granularity required for precision farming. This paper
introduces CYPRESS (Crop Yield Prediction via Regression on Prithvi's Encoder
for Satellite Sensing), a deep learning model designed for high-resolution,
intra-field canola yield prediction. CYPRESS leverages a pre-trained,
large-scale geospatial foundation model (Prithvi-EO-2.0-600M) and adapts it for
a continuous regression task, transforming multi-temporal satellite imagery
into dense, pixel-level yield maps. Evaluated on a comprehensive dataset from
the Canadian Prairies, CYPRESS demonstrates superior performance over existing
deep learning-based yield prediction models, highlighting the effectiveness of
fine-tuning foundation models for specialized agricultural applications. By
providing a continuous, high-resolution output, CYPRESS offers a more
actionable tool for precision agriculture than conventional classification or
county-level aggregation methods. This work validates a novel approach that
bridges the gap between large-scale Earth observation and on-farm
decision-making, offering a scalable solution for detailed agricultural
monitoring.

</details>


### [56] [PT-DETR: Small Target Detection Based on Partially-Aware Detail Focus](https://arxiv.org/abs/2510.26630)
*Bingcong Huo,Zhiming Wang*

Main category: cs.CV

TL;DR: 本论文提出了一种名为PT-DETR的新型检测算法，有效提升了无人机图像中小目标的检测能力。


<details>
  <summary>Details</summary>
Motivation: 无人机目标检测在复杂背景、遮挡严重、小目标密集、光照多变等条件下，传统检测算法性能不足，急需提升小目标检测的精度和鲁棒性。

Method: 以RT-DETR为基础，提出PT-DETR。具体包括：骨干网络中加入部分感知细节增强（PADF）模块，提升对小目标的特征提取能力；设计中值频率特征融合（MFFF）模块，加强对小目标细节和上下文信息的捕获；引入Focaler-SIoU优化框选匹配，使模型对小目标更为敏感。

Result: 与RT-DETR相比，PT-DETR在VisDrone2019数据集上的mAP提升了1.6%和1.7%，同时计算复杂度和参数量更低。

Conclusion: PT-DETR在实现更优小目标检测精度和鲁棒性的同时，具备较高的计算效率，验证了其在小目标检测任务中的可行性和实用性。

Abstract: To address the challenges in UAV object detection, such as complex
backgrounds, severe occlusion, dense small objects, and varying lighting
conditions,this paper proposes PT-DETR based on RT-DETR, a novel detection
algorithm specifically designed for small objects in UAV imagery. In the
backbone network, we introduce the Partially-Aware Detail Focus (PADF) Module
to enhance feature extraction for small objects. Additionally,we design the
Median-Frequency Feature Fusion (MFFF) module,which effectively improves the
model's ability to capture small-object details and contextual information.
Furthermore,we incorporate Focaler-SIoU to strengthen the model's bounding box
matching capability and increase its sensitivity to small-object features,
thereby further enhancing detection accuracy and robustness. Compared with
RT-DETR, our PT-DETR achieves mAP improvements of 1.6% and 1.7% on the
VisDrone2019 dataset with lower computational complexity and fewer parameters,
demonstrating its robustness and feasibility for small-object detection tasks.

</details>


### [57] [All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles](https://arxiv.org/abs/2510.26641)
*Sayed Pedram Haeri Boroujeni,Niloufar Mehrabi,Hazim Alzorgan,Ahmad Sarlak,Mahlagha Fazeli,Abolfazl Razi*

Main category: cs.CV

TL;DR: 本文综述了自动驾驶汽车（AV）目标检测领域的最新进展，重点关注多模态感知、上下文推理与协同智能，以及新兴的视觉-语言模型、生成式AI等前沿技术的发展与应用。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶汽车对可靠目标检测能力有高度依赖，但当前在多模态感知、上下文推理、协同智能等领域的知识与技术较为碎片化，亟需整合最新技术趋势，为领域发展指明方向。

Method: 文章系统梳理了自动驾驶相关传感器（如摄像头、超声波、激光雷达、毫米波雷达）及其融合策略，并结合最新视觉-语言模型（VLM）、大语言模型（LLM）等AI技术的应用前景和现有限制，分类回顾了不同类型AV数据集，深入分析了2D/3D检测管线、混合传感器融合、基于Transformer的新型检测方法等。

Result: 本综述对当前AV目标检测技术进行了全面梳理，总结了各类检测方法、传感器融合方式和数据集结构，对比了不同方法的优劣及适用场景，尤其指出了Vision-Language等新范式的应用潜力。

Conclusion: 文章为AV目标检测领域提供了清晰的技术现状、尚待攻克的挑战与未来发展机遇的全景图，为后续研究和技术落地指明了方向。

Abstract: Autonomous Vehicles (AVs) are transforming the future of transportation
through advances in intelligent perception, decision-making, and control
systems. However, their success is tied to one core capability, reliable object
detection in complex and multimodal environments. While recent breakthroughs in
Computer Vision (CV) and Artificial Intelligence (AI) have driven remarkable
progress, the field still faces a critical challenge as knowledge remains
fragmented across multimodal perception, contextual reasoning, and cooperative
intelligence. This survey bridges that gap by delivering a forward-looking
analysis of object detection in AVs, emphasizing emerging paradigms such as
Vision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI
rather than re-examining outdated techniques. We begin by systematically
reviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR,
and Radar) and their fusion strategies, highlighting not only their
capabilities and limitations in dynamic driving environments but also their
potential to integrate with recent advances in LLM/VLM-driven perception
frameworks. Next, we introduce a structured categorization of AV datasets that
moves beyond simple collections, positioning ego-vehicle, infrastructure-based,
and cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a
cross-analysis of data structures and characteristics. Ultimately, we analyze
cutting-edge detection methodologies, ranging from 2D and 3D pipelines to
hybrid sensor fusion, with particular attention to emerging transformer-driven
approaches powered by Vision Transformers (ViTs), Large and Small Language
Models (SLMs), and VLMs. By synthesizing these perspectives, our survey
delivers a clear roadmap of current capabilities, open challenges, and future
opportunities.

</details>


### [58] [Towards Reliable Sea Ice Drift Estimation in the Arctic Deep Learning Optical Flow on RADARSAT-2](https://arxiv.org/abs/2510.26653)
*Daniela Martin,Joseph Gallego*

Main category: cs.CV

TL;DR: 本文评估了48种深度学习光流模型在北极海冰SAR影像中的漂移估计能力，模型实现了亚公里级精度，显示其优于经典方法，有望用于极区导航和气候建模。


<details>
  <summary>Details</summary>
Motivation: 准确估算海冰漂移对北极导航、气候研究和业务预报至关重要，但现有经典光流方法在复杂情景下表现有限，深度学习方法在计算机视觉领域已被证实效果优异，本文希望验证其能否提升地球物理遥感场景中的表现。

Method: 本文基于RADARSAT-2 ScanSAR海冰影像，首次大规模评测48种深度学习光流模型的漂移估计性能，并用GNSS漂流浮标观测值作为真值，通过EPE和Fl等指标进行评价。

Result: 多种深度学习模型在海冰漂移估计中实现了6到8像素（约300到400米）的端点误差，相较海冰运动尺度和北极航行需求，这一误差很小，模型还能够捕捉一致的区域漂移模式。

Conclusion: 深度学习光流方法显著提升了海冰漂移估算精度，并且可以补完整海域的漂移场，对于极地遥感、导航和气候建模具有重要应用前景。

Abstract: Accurate estimation of sea ice drift is critical for Arctic navigation,
climate research, and operational forecasting. While optical flow, a computer
vision technique for estimating pixel wise motion between consecutive images,
has advanced rapidly in computer vision, its applicability to geophysical
problems and to satellite SAR imagery remains underexplored. Classical optical
flow methods rely on mathematical models and strong assumptions about motion,
which limit their accuracy in complex scenarios. Recent deep learning based
approaches have substantially improved performance and are now the standard in
computer vision, motivating their application to sea ice drift estimation. We
present the first large scale benchmark of 48 deep learning optical flow models
on RADARSAT 2 ScanSAR sea ice imagery, evaluated with endpoint error (EPE) and
Fl all metrics against GNSS tracked buoys. Several models achieve sub kilometer
accuracy (EPE 6 to 8 pixels, 300 to 400 m), a small error relative to the
spatial scales of sea ice motion and typical navigation requirements in the
Arctic. Our results demonstrate that the models are capable of capturing
consistent regional drift patterns and that recent deep learning based optical
flow methods, which have substantially improved motion estimation accuracy
compared to classical methods, can be effectively transferred to polar remote
sensing. Optical flow produces spatially continuous drift fields, providing
motion estimates for every image pixel rather than at sparse buoy locations,
offering new opportunities for navigation and climate modeling.

</details>


### [59] [Improving Classification of Occluded Objects through Scene Context](https://arxiv.org/abs/2510.26681)
*Courtney M. King,Daniel D. Leeds,Damian Lyons,George Kalaitzis*

Main category: cs.CV

TL;DR: 本文提出通过融合场景信息来增强物体检测网络在有遮挡情况时的鲁棒性，有效提升检测的准确率和召回率。


<details>
  <summary>Details</summary>
Motivation: 常见的强大物体识别算法在遮挡情况下表现不佳。生物视觉中的场景上下文有助于物体识别，因而探索将场景信息用于提升算法对有遮挡物体的识别能力。

Method: 作者提出两种融合场景信息的方法：一种在预测前，依据场景背景选择合适的检测网络；另一种在目标检测后，将场景知识与RPN输出的初始物体分数进行融合。这两种方法都结合了RPN-DCNN架构。

Result: 在具有局部遮挡的挑战性数据集上，两种方法均显著提升了检测的召回率和精度，优于基线方法。同时，作者发现同时用有遮挡和无遮挡图片训练模型能获得更好的性能。

Conclusion: 融合场景信息的方法能提升遮挡环境下的目标检测表现，方法具备可解释性且便于扩展到其他数据集，具有良好的应用前景和进一步研究价值。

Abstract: The presence of occlusions has provided substantial challenges to
typically-powerful object recognition algorithms. Additional sources of
information can be extremely valuable to reduce errors caused by occlusions.
Scene context is known to aid in object recognition in biological vision. In
this work, we attempt to add robustness into existing Region Proposal
Network-Deep Convolutional Neural Network (RPN-DCNN) object detection networks
through two distinct scene-based information fusion techniques. We present one
algorithm under each methodology: the first operates prior to prediction,
selecting a custom object network to use based on the identified background
scene, and the second operates after detection, fusing scene knowledge into
initial object scores output by the RPN. We demonstrate our algorithms on
challenging datasets featuring partial occlusions, which show overall
improvement in both recall and precision against baseline methods. In addition,
our experiments contrast multiple training methodologies for occlusion
handling, finding that training on a combination of both occluded and
unoccluded images demonstrates an improvement over the others. Our method is
interpretable and can easily be adapted to other datasets, offering many future
directions for research and practical applications.

</details>


### [60] [Process Integrated Computer Vision for Real-Time Failure Prediction in Steel Rolling Mill](https://arxiv.org/abs/2510.26684)
*Vaibhav Kurrey,Sivakalyan Pujari,Gagan Raj Gupta*

Main category: cs.CV

TL;DR: 本论文提出并长期部署了一套基于机器视觉的异常检测系统，用于钢轧机设备故障预测，通过实时视频流分析与传感器数据融合，提高了维护效率并降低故障成本。


<details>
  <summary>Details</summary>
Motivation: 钢轧机等工业制造环境中，设备故障会带来高昂的非计划停机成本。现有故障检测多依赖传统传感器和人工巡检，难以及时、精准地发现异常，影响生产效率与设备寿命。因此，亟需高效、实时的智能系统提升故障预测和定位能力。

Method: 系统集成工业相机监控设备运行状态和生产线热钢条动态，采集实时视频流，汇入中心化视频服务器。通过深度学习模型对视频流进行分析，结合来自数据采集系统的传感器数据，推断故障发生位置及可能根因。模型推理在服务器端完成，极大降低工业现场PLC等自动化系统负载，实现跨生产线的可扩展部署。

Result: 在实际钢轧生产环境中长期部署，系统能够及时预警设备故障与流程中断，协助定位故障部位及原因。该系统有效减少了非计划停机次数和维修成本，提供了可执行的维护建议，提升了生产稳定性和运营效益。

Conclusion: 集成机器视觉与深度学习的在线异常检测系统，在钢轧制造环境中表现优异，能够准确预测设备失效，提升维护前瞻性和生产线可靠性。该方案具备可扩展性，为工业自动化智能升级提供了参考与实践基础。

Abstract: We present a long-term deployment study of a machine vision-based anomaly
detection system for failure prediction in a steel rolling mill. The system
integrates industrial cameras to monitor equipment operation, alignment, and
hot bar motion in real time along the process line. Live video streams are
processed on a centralized video server using deep learning models, enabling
early prediction of equipment failures and process interruptions, thereby
reducing unplanned breakdown costs. Server-based inference minimizes the
computational load on industrial process control systems (PLCs), supporting
scalable deployment across production lines with minimal additional resources.
By jointly analyzing sensor data from data acquisition systems and visual
inputs, the system identifies the location and probable root causes of
failures, providing actionable insights for proactive maintenance. This
integrated approach enhances operational reliability, productivity, and
profitability in industrial manufacturing environments.

</details>


### [61] [Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark](https://arxiv.org/abs/2510.26802)
*Ziyu Guo,Xinyan Chen,Renrui Zhang,Ruichuan An,Yu Qi,Dongzhi Jiang,Xiangtai Li,Manyuan Zhang,Hongsheng Li,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 本文系统性评估了主流视频生成模型Veo-3在零样本视觉推理场景下的能力，揭示其在短程推理有一定表现但在复杂长程推理等方面仍有明显不足。


<details>
  <summary>Details</summary>
Motivation: 尽管视频生成模型在合成真实、连贯视频方面取得显著进步，并展现出视觉感知及操控能力，但尚不清楚它们能否直接充当视觉推理任务的零样本推理者。文章旨在全面检验当前视频模型在复杂视觉推理中的实际表现，为后续相关研究提供依据。

Method: 作者围绕Veo-3模型设计了包括空间、几何、物理、时间、体感逻辑等12类推理任务，并构建了MME-CoF评测基准。通过实证研究分析其推理行为，包括优势与局限。

Result: 研究发现，Veo-3等现有视频模型在短时空间一致性、细粒度对齐以及局部动态方面表现良好，具备一定推理能力。然而，在长时因果推理、严格几何约束及抽象逻辑方面表现有限，存在明显失败模式。

Conclusion: 当前视频生成模型尚不适合作为独立的零样本视觉推理者，但在辅助专用推理模型时展现出较大潜力，可作为视觉引擎补充推理任务。

Abstract: Recent video generation models can produce high-fidelity, temporally coherent
videos, indicating that they may encode substantial world knowledge. Beyond
realistic synthesis, they also exhibit emerging behaviors indicative of visual
perception, modeling, and manipulation. Yet, an important question still
remains: Are video models ready to serve as zero-shot reasoners in challenging
visual reasoning scenarios? In this work, we conduct an empirical study to
comprehensively investigate this question, focusing on the leading and popular
Veo-3. We evaluate its reasoning behavior across 12 dimensions, including
spatial, geometric, physical, temporal, and embodied logic, systematically
characterizing both its strengths and failure modes. To standardize this study,
we curate the evaluation data into MME-CoF, a compact benchmark that enables
in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our
findings reveal that while current video models demonstrate promising reasoning
patterns on short-horizon spatial coherence, fine-grained grounding, and
locally consistent dynamics, they remain limited in long-horizon causal
reasoning, strict geometric constraints, and abstract logic. Overall, they are
not yet reliable as standalone zero-shot reasoners, but exhibit encouraging
signs as complementary visual engines alongside dedicated reasoning models.
Project page: https://video-cof.github.io

</details>


### [62] [The Impact and Outlook of 3D Gaussian Splatting](https://arxiv.org/abs/2510.26694)
*Bernhard Kerbl*

Main category: cs.CV

TL;DR: 3D Gaussian Splatting（3DGS）技术自提出以来，对3D场景表示产生巨大影响，推动了众多相关研究。本文综述了3DGS在高效性、可扩展性、实际应用等方面的重要进展。


<details>
  <summary>Details</summary>
Motivation: 3DGS技术虽具突破性，但在资源消耗、动态场景表示、移动端应用等方面仍存在挑战。因此，推动3DGS向更高效、更广泛应用方向发展成为研究热点。

Method: 本文对3DGS后续发展的主要方向进行了总结，包括资源高效的训练与渲染方法、动态图像（4DGS）表示的进化、基础数学建模的深入研究、移动和VR平台上的应用扩展、大规模环境适配、以及快速重建的技术演进。

Result: 3DGS的发展推动了相关技术在移动和VR平台上的应用，实现了更高效、更大规模、甚至近实时的三维场景重建。同时，理论基础的完善也提升了其应用潜力。

Conclusion: 3DGS已从最初的创新性场景表示手段，成长为三维视觉与图形领域中多功能、基础性的工具，极大丰富了三维内容的表现与处理方式。

Abstract: Since its introduction, 3D Gaussian Splatting (3DGS) has rapidly transformed
the landscape of 3D scene representations, inspiring an extensive body of
associated research. Follow-up work includes analyses and contributions that
enhance the efficiency, scalability, and real-world applicability of 3DGS. In
this summary, we present an overview of several key directions that have
emerged in the wake of 3DGS. We highlight advances enabling resource-efficient
training and rendering, the evolution toward dynamic (or four-dimensional,
4DGS) representations, and deeper exploration of the mathematical foundations
underlying its appearance modeling and rendering process. Furthermore, we
examine efforts to bring 3DGS to mobile and virtual reality platforms, its
extension to massive-scale environments, and recent progress toward
near-instant radiance field reconstruction via feed-forward or distributed
computation. Collectively, these developments illustrate how 3DGS has evolved
from a breakthrough representation into a versatile and foundational tool for
3D vision and graphics.

</details>


### [63] [SteerVLM: Robust Model Control through Lightweight Activation Steering for Vision Language Models](https://arxiv.org/abs/2510.26769)
*Anushka Sivakumar,Andrew Zhang,Zaber Hakim,Chris Thomas*

Main category: cs.CV

TL;DR: 本文提出SteerVLM，一种轻量级引导模块，可在不改变模型权重的前提下，动态调整视觉-语言模型的输出，提升模型按照指令生成结果的能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型（VLMs）在按照复杂指令生成多模态输出时缺乏灵活、细粒度的可控性和鲁棒性。现有方法多需修改模型权重或手动干预，限制了实用性，因此需要更高效且通用的模型控制方案。

Method: 作者提出SteerVLM模块，通过学习目标和对立行为的成对prompt的潜在嵌入，动态调节语言与图像上下文之间的激活，实现推理时对输出的微调。该模块训练参数仅占原模型0.14%，并可跨层自适应引导，无需预提取静态向量或手调干预点。同时，作者新构建了VNIA数据集，专为VLM引导技术开发和评测设计。

Result: SteerVLM在VLM的引导与幻觉缓解任务上均优于现有干预技术，并在多模态模型控制方面表现出更好的鲁棒性。

Conclusion: SteerVLM为多模态视觉-语言模型调控提供了高效、灵活和可推广的解决方案，在不影响模型原有性能的情况下降低了对复杂手工干预的需求。

Abstract: This work introduces SteerVLM, a lightweight steering module designed to
guide Vision-Language Models (VLMs) towards outputs that better adhere to
desired instructions. Our approach learns from the latent embeddings of paired
prompts encoding target and converse behaviors to dynamically adjust
activations connecting the language modality with image context. This allows
for fine-grained, inference-time control over complex output semantics without
modifying model weights while preserving performance on off-target tasks. Our
steering module requires learning parameters equal to 0.14% of the original
VLM's size. Our steering module gains model control through dimension-wise
activation modulation and adaptive steering across layers without requiring
pre-extracted static vectors or manual tuning of intervention points.
Furthermore, we introduce VNIA (Visual Narrative Intent Alignment), a
multimodal dataset specifically created to facilitate the development and
evaluation of VLM steering techniques. Our method outperforms existing
intervention techniques on steering and hallucination mitigation benchmarks for
VLMs and proposes a robust solution for multimodal model control through
activation engineering.

</details>


### [64] [Surpassing state of the art on AMD area estimation from RGB fundus images through careful selection of U-Net architectures and loss functions for class imbalance](https://arxiv.org/abs/2510.26778)
*Valentyna Starodub,Mantas Lukoševičius*

Main category: cs.CV

TL;DR: 本研究提出改进的RGB眼底图像AMD病灶分割方法，在ADAM挑战赛数据集上取得了优于以往所有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 老年性黄斑变性（AMD）是60岁以上人群不可逆视力损伤的主要原因之一，亟需高效、无创的检测手段。现有自动检测与分割方法效果尚有限。

Method: 以U-Net为基础，对模型结构与训练流程多方面改进，包括预处理、不同复杂度的主干网络选择、以及针对类别不平衡问题设计的损失函数，针对RGB眼底图像的AMD病灶进行多类别分割任务。

Result: 所提出的最终配置在ADAM公开数据集和挑战赛多类别AMD病灶分割任务中，分割性能超过所有之前的参赛方案。

Conclusion: 提出的框架在AMD病灶自动分割上达到了当前最优水平，有助于无创、低成本的AMD大规模筛查应用，相关代码已开源。

Abstract: Age-related macular degeneration (AMD) is one of the leading causes of
irreversible vision impairment in people over the age of 60. This research
focuses on semantic segmentation for AMD lesion detection in RGB fundus images,
a non-invasive and cost-effective imaging technique. The results of the ADAM
challenge - the most comprehensive AMD detection from RGB fundus images
research competition and open dataset to date - serve as a benchmark for our
evaluation. Taking the U-Net connectivity as a base of our framework, we
evaluate and compare several approaches to improve the segmentation model's
architecture and training pipeline, including pre-processing techniques,
encoder (backbone) deep network types of varying complexity, and specialized
loss functions to mitigate class imbalances on image and pixel levels. The main
outcome of this research is the final configuration of the AMD detection
framework, which outperforms all the prior ADAM challenge submissions on the
multi-class segmentation of different AMD lesion types in non-invasive RGB
fundus images. The source code used to conduct the experiments presented in
this paper is made freely available.

</details>


### [65] [ChartAB: A Benchmark for Chart Grounding & Dense Alignment](https://arxiv.org/abs/2510.26781)
*Aniruddh Bansal,Davit Soselia,Dang Nguyen,Tianyi Zhou*

Main category: cs.CV

TL;DR: 本文提出了ChartAlign Benchmark (ChartAB)，用于系统评估视觉语言模型（VLMs）在图表细粒度感知与推理能力，揭示了现有模型在图表理解中的诸多不足。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs在图表的细节感知和结构抽取能力上表现有限，难以精确进行图表比对和推理，这阻碍了其在数据分析、知识交流等实际应用中的效果。

Method: 作者提出ChartAB数据集，涵盖多种类型和复杂度图表的标注任务，包括数据提取、图表元素定位及属性识别，并设计了专用JSON模板与评测指标。同时引入两阶段推理流程，用于评估VLMs在跨图表元素比对和对齐能力。

Result: 通过ChartAB对多种现有VLMs进行实验，评估分析其在图表感知上的偏差、薄弱环节、鲁棒性和幻觉现象，揭示了模型在细粒度图表理解上的显著差异。

Conclusion: 实验结果表明，当前VLMs在图表理解上存在具体待提升能力，ChartAB能够有效刻画这些短板，为后续模型改进提供方向。

Abstract: Charts play an important role in visualization, reasoning, data analysis, and
the exchange of ideas among humans. However, existing vision-language models
(VLMs) still lack accurate perception of details and struggle to extract
fine-grained structures from charts. Such limitations in chart grounding also
hinder their ability to compare multiple charts and reason over them. In this
paper, we introduce a novel "ChartAlign Benchmark (ChartAB)" to provide a
comprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting
tabular data, localizing visualization elements, and recognizing various
attributes from charts of diverse types and complexities. We design a JSON
template to facilitate the calculation of evaluation metrics specifically
tailored for each grounding task. By incorporating a novel two-stage inference
workflow, the benchmark can further evaluate VLMs' capability to align and
compare elements/attributes across two charts. Our analysis of evaluations on
several recent VLMs reveals new insights into their perception biases,
weaknesses, robustness, and hallucinations in chart understanding. These
findings highlight the fine-grained discrepancies among VLMs in chart
understanding tasks and point to specific skills that need to be strengthened
in current models.

</details>


### [66] [HEIR: Learning Graph-Based Motion Hierarchies](https://arxiv.org/abs/2510.26786)
*Cheng Zheng,William Koch,Baiang Li,Felix Heide*

Main category: cs.CV

TL;DR: 本文提出了一种通用的分层运动建模方法，能从数据中自动学习结构化且可解释的运动关系，并在多个运动场景下验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 以人为定义的分层运动模型（如固定的运动元或启发式分层结构）在面对不同任务时泛化能力有限。因此，需设计一种能自适应学习运动层次结构的通用方法。

Method: 该方法利用基于图的层级结构表示运动，将整体运动分解为继承自父节点的模式和局部残差，并通过可微分的图学习方法（Graph Neural Networks）自动推断运动结构中的父子关系。

Result: 在1D平移、2D旋转和3D场景形变（高斯斑点表示）实验中，所提方法能成功还原本质的运动层次关系，并在3D形变任务上相比基线方法产生更真实且可解释的重建结果。

Conclusion: 提出的方法为各类以运动为核心的任务提供了适应性强、基于数据驱动的分层建模新思路，具有良好的通用性和可解释性。

Abstract: Hierarchical structures of motion exist across research fields, including
computer vision, graphics, and robotics, where complex dynamics typically arise
from coordinated interactions among simpler motion components. Existing methods
to model such dynamics typically rely on manually-defined or heuristic
hierarchies with fixed motion primitives, limiting their generalizability
across different tasks. In this work, we propose a general hierarchical motion
modeling method that learns structured, interpretable motion relationships
directly from data. Our method represents observed motions using graph-based
hierarchies, explicitly decomposing global absolute motions into
parent-inherited patterns and local motion residuals. We formulate hierarchy
inference as a differentiable graph learning problem, where vertices represent
elemental motions and directed edges capture learned parent-child dependencies
through graph neural networks. We evaluate our hierarchical reconstruction
approach on three examples: 1D translational motion, 2D rotational motion, and
dynamic 3D scene deformation via Gaussian splatting. Experimental results show
that our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases,
and produces more realistic and interpretable deformations compared to the
baseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable,
data-driven hierarchical modeling paradigm, our method offers a formulation
applicable to a broad range of motion-centric tasks. Project Page:
https://light.princeton.edu/HEIR/

</details>


### [67] [The Quest for Generalizable Motion Generation: Data, Model, and Evaluation](https://arxiv.org/abs/2510.26794)
*Jing Lin,Ruisi Wang,Junzhe Lu,Ziqi Huang,Guorui Song,Ailing Zeng,Xian Liu,Chen Wei,Wanqi Yin,Qingping Sun,Zhongang Cai,Lei Yang,Ziwei Liu*

Main category: cs.CV

TL;DR: 本文提出了一种从视频生成（ViGen）领域转移知识到3D人体动作生成（MoGen）的方法，显著提升了动作生成模型的泛化能力，并发布了大规模数据集、模型和评测基准。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体动作生成模型在标准数据集上进展明显，但泛化能力有限；而视频生成相关领域已表现强大发挥，动作生成亟需借鉴其思路。

Method: （1）推出包含22.8万高质量动作样本的数据集ViMoGen-228K，融合了高精度光学动捕数据、网络视频语义标注动作及主流ViGen模型合成的数据；（2）提出ViMoGen模型，在流匹配扩散Transformer结构中融合MoCap和ViGen知识，采用门控多模态条件机制；（3）精简版ViMoGen-light不依赖视频生成模型但保持良好泛化；（4）构建细致分层的MBench基准，覆盖动作质量、文本一致性和泛化能力评测。

Result: ViMoGen及其简化版在自动和人工评测中均大幅优于已有方法，幕本、代码和数据集可公开获取。

Conclusion: 借助于ViGen领域的知识迁移及多方面创新，本文提出的框架有效突破了动作生成模型的泛化瓶颈，为后续相关研究提供了基础和资源。

Abstract: Despite recent advances in 3D human motion generation (MoGen) on standard
benchmarks, existing models still face a fundamental bottleneck in their
generalization capability. In contrast, adjacent generative fields, most
notably video generation (ViGen), have demonstrated remarkable generalization
in modeling human behaviors, highlighting transferable insights that MoGen can
leverage. Motivated by this observation, we present a comprehensive framework
that systematically transfers knowledge from ViGen to MoGen across three key
pillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a
large-scale dataset comprising 228,000 high-quality motion samples that
integrates high-fidelity optical MoCap data with semantically annotated motions
from web videos and synthesized samples generated by state-of-the-art ViGen
models. The dataset includes both text-motion pairs and text-video-motion
triplets, substantially expanding semantic diversity. Second, we propose
ViMoGen, a flow-matching-based diffusion transformer that unifies priors from
MoCap data and ViGen models through gated multimodal conditioning. To enhance
efficiency, we further develop ViMoGen-light, a distilled variant that
eliminates video generation dependencies while preserving strong
generalization. Finally, we present MBench, a hierarchical benchmark designed
for fine-grained evaluation across motion quality, prompt fidelity, and
generalization ability. Extensive experiments show that our framework
significantly outperforms existing approaches in both automatic and human
evaluations. The code, data, and benchmark will be made publicly available.

</details>


### [68] [Scaling Image Geo-Localization to Continent Level](https://arxiv.org/abs/2510.26795)
*Philipp Lindenberger,Paul-Edouard Sarlin,Jan Hosang,Matteo Balice,Marc Pollefeys,Simon Lynen,Eduard Trulls*

Main category: cs.CV

TL;DR: 本文提出了一种结合代理分类任务和地理嵌入的新方法，可大规模实现精细化图像地理定位，在欧洲大范围数据集上，68%以上的查询能精确到200米以内。


<details>
  <summary>Details</summary>
Motivation: 现有全球范围地理定位方法存在效率低（因数据量庞大）与精度粗的问题，特别是跨视角检索面临域差异，准确率不高。需要一种能够在大范围内实现高精度定位的新方法。

Method: 该方法在训练阶段引入代理分类任务，学习能够隐式编码精准地理位置的信息特征表示。还将这种特征原型与航拍图像的嵌入结合，以增强对地面图像数据稀疏情况下的鲁棒性，从而实现跨多个国家的直接、细粒度图像检索。

Result: 在覆盖欧洲大部分地区的数据集上，所提方法能够将超过68%的查询精确定位在200米以内，显示出在大范围区域内实现高精度图像定位的能力。

Conclusion: 通过混合代理分类与图像嵌入的方法，可以有效缓解地面数据稀疏和域差异造成的问题，实现大规模高精度图像地理定位。代码已开源，便于学术和工业界进一步研究。

Abstract: Determining the precise geographic location of an image at a global scale
remains an unsolved challenge. Standard image retrieval techniques are
inefficient due to the sheer volume of images (>100M) and fail when coverage is
insufficient. Scalable solutions, however, involve a trade-off: global
classification typically yields coarse results (10+ kilometers), while
cross-view retrieval between ground and aerial imagery suffers from a domain
gap and has been primarily studied on smaller regions. This paper introduces a
hybrid approach that achieves fine-grained geo-localization across a large
geographic expanse the size of a continent. We leverage a proxy classification
task during training to learn rich feature representations that implicitly
encode precise location information. We combine these learned prototypes with
embeddings of aerial imagery to increase robustness to the sparsity of
ground-level data. This enables direct, fine-grained retrieval over areas
spanning multiple countries. Our extensive evaluation demonstrates that our
approach can localize within 200m more than 68\% of queries of a dataset
covering a large part of Europe. The code is publicly available at
https://scaling-geoloc.github.io.

</details>


### [69] [SEE4D: Pose-Free 4D Generation via Auto-Regressive Video Inpainting](https://arxiv.org/abs/2510.26796)
*Dongyue Lu,Ao Liang,Tianxin Huang,Xiao Fu,Yuyang Zhao,Baorui Ma,Liang Pan,Wei Yin,Lingdong Kong,Wei Tsang Ooi,Ziwei Liu*

Main category: cs.CV

TL;DR: 本文提出了一种无需3D监督和相机位姿信息的4D时空内容合成方法SEE4D，通过固定虚拟相机和视频修复模型，实现了从日常视频中直接生成多视角4D场景，扩展了现有视频到4D的实用性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有4D内容生成技术常需昂贵的3D监督和人工相机姿态标注，尤其对于自然场景中的随拍视频不切实际，因此亟需一种无需位姿标签、可拓展性强的方法。

Method: SEE4D提出以固定虚拟相机替换传统的轨迹预测，将不同视角的合成与场景建模解耦：通过视角条件视频修复模型，先对经过仿真变换的图像进行去噪和修复，再采用时空自回归推理机制，实现高效的视频续接和4D合成，无需3D标注。

Result: SEE4D在跨视角视频生成和稀疏重建等任务上，通过定量和定性实验，优于依赖相机位姿或轨迹标签的方法，在通用性和性能方面均取得领先。

Conclusion: SEE4D显著简化了4D世界建模的流程，消除了对复杂监督的依赖，为直接从日常视频生成高质量4D内容提供了可行且高效的解决方案。

Abstract: Immersive applications call for synthesizing spatiotemporal 4D content from
casual videos without costly 3D supervision. Existing video-to-4D methods
typically rely on manually annotated camera poses, which are labor-intensive
and brittle for in-the-wild footage. Recent warp-then-inpaint approaches
mitigate the need for pose labels by warping input frames along a novel camera
trajectory and using an inpainting model to fill missing regions, thereby
depicting the 4D scene from diverse viewpoints. However, this
trajectory-to-trajectory formulation often entangles camera motion with scene
dynamics and complicates both modeling and inference. We introduce SEE4D, a
pose-free, trajectory-to-camera framework that replaces explicit trajectory
prediction with rendering to a bank of fixed virtual cameras, thereby
separating camera control from scene modeling. A view-conditional video
inpainting model is trained to learn a robust geometry prior by denoising
realistically synthesized warped images and to inpaint occluded or missing
regions across virtual viewpoints, eliminating the need for explicit 3D
annotations. Building on this inpainting core, we design a spatiotemporal
autoregressive inference pipeline that traverses virtual-camera splines and
extends videos with overlapping windows, enabling coherent generation at
bounded per-step complexity. We validate See4D on cross-view video generation
and sparse reconstruction benchmarks. Across quantitative metrics and
qualitative assessments, our method achieves superior generalization and
improved performance relative to pose- or trajectory-conditioned baselines,
advancing practical 4D world modeling from casual videos.

</details>


### [70] [Masked Diffusion Captioning for Visual Feature Learning](https://arxiv.org/abs/2510.26799)
*Chao Feng,Zihao Wei,Andrew Owens*

Main category: cs.CV

TL;DR: 本文提出了一种名为Masked Diffusion Captioning (MDC)的新方法，通过以视觉特征为条件，掩码文本重建，实现视觉特征学习。


<details>
  <summary>Details</summary>
Motivation: 以往通过图像描述（captioning）学习视觉特征的方法主要依赖自回归模型，这些模型学习信号依赖于文本序列中位置，存在局限性。本研究旨在探索更高效、均匀的视觉特征学习方式。

Method: 在训练阶段，将图像和其对应文本描述配对，将描述中的文本token随机掩码，利用以视觉特征为条件的扩散解码器重建原文本。模型训练完成后，用获得的视觉特征进行下游视觉任务。

Result: 通过一系列线性探测实验（linear probing），验证在不同规模的模型和数据集上，MDC学习到的视觉特征与现有自回归、对比学习方法获得的特征具有竞争力。

Conclusion: MDC方法能够消除自回归captioning对位置依赖的问题，减少对辅助目标的需求，在视觉特征学习方面表现优异，具有应用前景。

Abstract: We learn visual features by captioning images with an image-conditioned
masked diffusion language model, a formulation we call masked diffusion
captioning (MDC). During training, text tokens in each image-caption pair are
masked at a randomly chosen ratio, and a decoder conditioned on visual features
is trained to reconstruct the original text. After training, the learned visual
features can be applied to downstream vision tasks. Unlike autoregressive
captioning, the strength of the visual learning signal in MDC does not depend
on each token's position in the sequence, reducing the need for auxiliary
objectives. Linear probing experiments across a variety of academic-scale
models and datasets show that the learned visual features are competitive with
those produced by autoregressive and contrastive approaches.

</details>


### [71] [OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes](https://arxiv.org/abs/2510.26800)
*Yukun Huang,Jiwen Yu,Yanning Zhou,Jianan Wang,Xintao Wang,Pengfei Wan,Xihui Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新方法OmniX，将二维生成模型扩展用于全景感知，实现可用于物理渲染、重光照和仿真的高质量三维场景生成。


<details>
  <summary>Details</summary>
Motivation: 现有全景2D提升方法多聚焦于外观生成，忽视了三维场景的内在属性感知，难以实现可物理渲染的多样化三维场景。作者希望充分利用强大的2D生成先验，提升三维场景的真实性和应用性。

Method: 提出OmniX框架，通过高效的跨模态适配器结构，复用2D生成模型在全景语义、材质、结构等多方面感知能力，并联合构建大规模高质量全景多模态合成数据集。方法支持多种全景视觉任务，包括感知、生成和补全。

Result: 在大量室内外场景实验中，OmniX在全景视觉感知及面向图形渲染的三维场景生成方面表现优异，生成的三维场景可直接用于物理渲染和虚拟仿真。

Conclusion: OmniX打通了二维生成模型到高真实感三维场景生成的管道，推动了沉浸式物理真实虚拟世界的生成，为三维感知与生成领域带来新可能。

Abstract: There are two prevalent ways to constructing 3D scenes: procedural generation
and 2D lifting. Among them, panorama-based 2D lifting has emerged as a
promising technique, leveraging powerful 2D generative priors to produce
immersive, realistic, and diverse 3D environments. In this work, we advance
this technique to generate graphics-ready 3D scenes suitable for physically
based rendering (PBR), relighting, and simulation. Our key insight is to
repurpose 2D generative models for panoramic perception of geometry, textures,
and PBR materials. Unlike existing 2D lifting approaches that emphasize
appearance generation and ignore the perception of intrinsic properties, we
present OmniX, a versatile and unified framework. Based on a lightweight and
efficient cross-modal adapter structure, OmniX reuses 2D generative priors for
a broad range of panoramic vision tasks, including panoramic perception,
generation, and completion. Furthermore, we construct a large-scale synthetic
panorama dataset containing high-quality multimodal panoramas from diverse
indoor and outdoor scenes. Extensive experiments demonstrate the effectiveness
of our model in panoramic visual perception and graphics-ready 3D scene
generation, opening new possibilities for immersive and physically realistic
virtual world generation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [72] [StreetMath: Study of LLMs' Approximation Behaviors](https://arxiv.org/abs/2510.25776)
*Chiung-Yi Tseng,Somshubhra Roy,Maisha Thasin,Danyang Zhang,Blessing Effiong*

Main category: cs.CL

TL;DR: 本论文提出了StreetMath基准，用于评估大语言模型在现实情境下进行近似运算的能力，并分析了不同架构模型的表现及其内部计算状态。


<details>
  <summary>Details</summary>
Motivation: 虽然已有很多文献关注大语言模型在精确算术上的推理能力，但其在日常、快速近似计算方面的能力，以及此类能力在非自回归模型中的表现，尚未被系统研究。因此需要专门的基准和工具对这方面能力进行评测。

Method: 作者创建了StreetMath基准，专为考察模型的近似运算能力设计，同时对多种大语言模型（如Qwen系列、Dream-v0、Falcon-Mamba、Mamba-GPT等）进行广泛测试，并结合机械可解释性方法探测其内部计算状态。

Result: 实验发现，无论任务需求是否为近似，模型通常都试图计算精确值或调用外部工具；即使模型在推理的早期阶段已得出正确答案，解决近似问题仍然消耗较多token。实验还表明，精确与近似算术在神经元层面上依赖的模块基本分离。

Conclusion: 当前大语言模型在近似运算任务中缺乏类似人类的'认知吝啬'现象，更倾向于进行复杂的精确计算。StreetMath可作为研究该领域的重要工具，相关资源已开源。

Abstract: There is a substantial body of literature examining the mathematical
reasoning capabilities of large language models (LLMs), particularly their
performance on precise arithmetic operations in autoregressive architectures.
However, their ability to perform approximate reasoning in informal, fast-paced
mathematical operations has received far less attention, especially among
non-autoregressive decoder models. Our work addresses this gap by introducing
StreetMath, a benchmark designed to evaluate models' approximation abilities
under real-world approximation scenarios. We conduct extensive evaluations
across different LLM architectures: Qwen3-4B-Instruct-2507,
Qwen3-4B-Thinking-2507, Dream-v0-Instruct-7B, Falcon-Mamba-7B-Instruct, and
Mamba-GPT-3B. Furthermore, we apply mechanistic interpretability techniques to
probe their internal computational states. Our analysis reveals that LLMs
generally attempt to compute exact values or invoke external tools even in
tasks that call for approximation. Moreover, while models sometimes reach the
correct answer in early layers or steps, they still consume more tokens when
solving approximation tasks. Additional experiments indicate that exact and
approximate arithmetic operations rely on largely separate neural components.
Drawing upon research on cognitive psychology, we argue that LLMs do not
exhibit cognitive miserliness in the same way humans do in street math
settings. We open source our work https://github.com/ctseng777/StreetMath

</details>


### [73] [Review Based Entity Ranking using Fuzzy Logic Algorithmic Approach: Analysis](https://arxiv.org/abs/2510.25778)
*Pratik N. Kalamkar,Anupama G. Phakatkar*

Main category: cs.CL

TL;DR: 本文提出了一种基于模糊逻辑的意见挖掘方法，不仅判断评论的倾向（正面或负面），还结合意见强度进行细致分类，从而实现对实体的多粒度排序。


<details>
  <summary>Details</summary>
Motivation: 传统的词典方法在情感极性分析中未能充分考虑评论强度的差异，无法区分如“很强烈的负面”与“弱负面”等细微分级。因此，需要一种考虑意见强度的方法以提升情感分析的细致度和实体排名的准确性。

Method: 方法上，结合了模糊逻辑算法对情感词（包括副词、形容词、名词和动词）进行多维度强度分类（如很弱、弱、中等、强、很强），同时利用句法依存分析来捕捉情感词与产品某一方面之间的关系，根据每个方面相关的情感词决定实体各方面得分。

Result: 结果上，通过对实体评论和用户查询的分析，能更细致地区分和排名实体在不同方面上的情感得分，实现基于评论强度的详细评价和排序。

Conclusion: 研究表明，该方法可以提升实体评价的细致度和客观性，更好地适应实际应用中的多层次情感分析需求。

Abstract: Opinion mining, also called sentiment analysis, is the field of study that
analyzes people opinions, sentiments, evaluations, appraisals, attitudes, and
emotions towards entities such as products, services, organizations,
individuals, issues, events, topics, and their attributes. Holistic
lexicon-based approach does not consider the strength of each opinion, i.e.,
whether the opinion is very strongly negative (or positive), strongly negative
(or positive), moderate negative (or positive), very weakly negative (or
positive) and weakly negative (or positive). In this paper, we propose approach
to rank entities based on orientation and strength of the entity reviews and
user's queries by classifying them in granularity levels (i.e. very weak, weak,
moderate, very strong and strong) by combining opinion words (i.e. adverb,
adjective, noun and verb) that are related to aspect of interest of certain
product. We shall use fuzzy logic algorithmic approach in order to classify
opinion words into different category and syntactic dependency resolution to
find relations for desired aspect words. Opinion words related to certain
aspects of interest are considered to find the entity score for that aspect in
the review.

</details>


### [74] [LASTIST: LArge-Scale Target-Independent STance dataset](https://arxiv.org/abs/2510.25783)
*DongJae Kim,Yaejin Lee,Minsu Park,Eunil Park*

Main category: cs.CL

TL;DR: 该论文提出并介绍了一个大规模的朝鲜语立场检测数据集LASTIST，用于推动低资源语言上的立场检测研究，特别是目标无关型立场检测。


<details>
  <summary>Details</summary>
Motivation: 目前大部分立场检测研究和数据集集中在英文和目标相关任务，低资源语言（如朝鲜语）以及目标无关型任务受到的关注较少，导致相关模型难以发展。作者希望通过构建朝鲜语的大型数据集来填补这一空白。

Method: 作者从韩国政党新闻稿中收集、标注了563,299条朝鲜语句子，制作了LASTIST数据集。文中详细介绍了数据的收集和构建过程，并基于现有最新深度学习与立场检测模型进行训练和实验。

Result: LASTIST数据集成功完成收集与标注，公开数据集并展示其可用于多种立场检测任务（包括目标无关和历时性立场检测），基准模型在该数据集上取得了合理的效果。

Conclusion: LASTIST是一个涵盖多任务、适用于低资源语言的大规模朝鲜语立场检测数据集，有助于促进该领域的发展，并支持目标无关、长期演变等多样立场检测任务。

Abstract: Stance detection has emerged as an area of research in the field of
artificial intelligence. However, most research is currently centered on the
target-dependent stance detection task, which is based on a person's stance in
favor of or against a specific target. Furthermore, most benchmark datasets are
based on English, making it difficult to develop models in low-resource
languages such as Korean, especially for an emerging field such as stance
detection. This study proposes the LArge-Scale Target-Independent STance
(LASTIST) dataset to fill this research gap. Collected from the press releases
of both parties on Korean political parties, the LASTIST dataset uses 563,299
labeled Korean sentences. We provide a detailed description of how we collected
and constructed the dataset and trained state-of-the-art deep learning and
stance detection models. Our LASTIST dataset is designed for various tasks in
stance detection, including target-independent stance detection and diachronic
evolution stance detection. We deploy our dataset on
https://anonymous.4open.science/r/LASTIST-3721/.

</details>


### [75] [zFLoRA: Zero-Latency Fused Low-Rank Adapters](https://arxiv.org/abs/2510.25784)
*Dhananjaya Gowda,Seoha Song,Harshith Goka,Junhyun Lee*

Main category: cs.CL

TL;DR: 本文提出了一种新的低延迟融合低秩适配器（zFLoRA），可为大型语言模型（LLM）实现零或可忽略的推理延迟开销，同时保持甚至优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 虽然适配器参数在模型总参数中所占比例极小，但在实际推理中却会带来显著的额外计算负担，显著影响推理速度。因此亟需一种既能实现有效任务适应，同时又不影响推理时延的适配方法。

Method: 作者提出zero-latency fused low-rank adapter（zFLoRA），该方法通过特殊设计，使适配器参数与基础模型高效融合，实现近乎零的额外推理延迟。方法在1B、3B和7B的LLM上进行实证，对比LoRA、全量微调等主流方法。

Result: zFLoRA在常识推理、数学推理和摘要-对话三大类共18个任务上，与当前低秩适配器（LoRA）和全量微调（FFT）等方法相当或更优。延迟测量显示，在NPU和GPU平台上，zFLoRA适配器几乎不引入额外推理延迟。

Conclusion: zFLoRA方法克服了当前适配器因特殊结构而导致推理延迟过高的问题，实现了在维持高性能的同时，近乎于无感地集成多任务适配能力，是LLM实际部署的重要进展。

Abstract: Large language models (LLMs) are increasingly deployed with task-specific
adapters catering to multiple downstream applications. In such a scenario, the
additional compute associated with these apparently insignificant number of
adapter parameters (typically less than 1% of the base model) turns out to be
disproportionately significant during inference time (upto 2.5x times that of
the base model). In this paper, we propose a new zero-latency fused low-rank
adapter (zFLoRA) that introduces zero or negligible latency overhead on top of
the base model. Experimental results on LLMs of size 1B, 3B and 7B show that
zFLoRA compares favorably against the popular supervised fine-tuning benchmarks
including low-rank adapters (LoRA) as well as full fine-tuning (FFT).
Experiments are conducted on 18 different tasks across three different
categories namely commonsense reasoning, math reasoning and summary-dialogue.
Latency measurements made on NPU (Samsung Galaxy S25+) as well as GPU (NVIDIA
H100) platforms show that the proposed zFLoRA adapters introduce zero to
negligible latency overhead.

</details>


### [76] [BlackboxNLP-2025 MIB Shared Task: Improving Circuit Faithfulness via Better Edge Selection](https://arxiv.org/abs/2510.25786)
*Yaniv Nikankin,Dana Arad,Itay Itzhak,Anja Reusch,Adi Simhi,Gal Kesten-Pomeranz,Yonatan Belinkov*

Main category: cs.CL

TL;DR: 本文提出了三种用于电路发现的改进方法，使可解释性基准测试中发现模型内部机制更加高效、准确。


<details>
  <summary>Details</summary>
Motivation: 电路发现是机制可解释性研究中的核心难题，需要更好地识别模型中负责特定任务的部分。本文旨在提升电路发现的准确性和可靠性，以更好地解释复杂模型。

Method: 1）使用自助法（bootstrapping）识别具有一致归因分数的边；2）提出基于比例的选择策略，优先选择强正向归因分数边，平衡性能与忠实性；3）采用整数线性规划替代传统贪心算法用于选择重要边。

Result: 上述方法在多项MIB任务和多种模型上均优于现有电路发现方法，得到更加忠实的电路结构。

Conclusion: 本文的方法有效提升了电路发现的表现，为机制可解释性研究提供了更优的工具。代码已开源，方便社区复现与应用。

Abstract: One of the main challenges in mechanistic interpretability is circuit
discovery, determining which parts of a model perform a given task. We build on
the Mechanistic Interpretability Benchmark (MIB) and propose three key
improvements to circuit discovery. First, we use bootstrapping to identify
edges with consistent attribution scores. Second, we introduce a simple
ratio-based selection strategy to prioritize strong positive-scoring edges,
balancing performance and faithfulness. Third, we replace the standard greedy
selection with an integer linear programming formulation. Our methods yield
more faithful circuits and outperform prior approaches across multiple MIB
tasks and models. Our code is available at:
https://github.com/technion-cs-nlp/MIB-Shared-Task.

</details>


### [77] [LISTEN to Your Preferences: An LLM Framework for Multi-Objective Selection](https://arxiv.org/abs/2510.25799)
*Adam S. Jovine,Tinghan Ye,Francis Bahk,Jingjing Wang,David B. Shmoys,Peter I. Frazier*

Main category: cs.CL

TL;DR: 本文提出了LISTEN框架，利用大语言模型（LLM）作为零样本偏好判别器，通过自然语言指导专家多目标决策，显著简化了复杂偏好获取过程。文中设计了两种迭代算法LISTEN-U和LISTEN-T，并在多种实际任务中验证了效果。


<details>
  <summary>Details</summary>
Motivation: 传统多目标决策过程中，专家由于难以形式化复杂的隐性偏好，常在大量可选项中难以快速选出最佳方案。如何用自然语言直接指导决策、简化偏好获取流程成为亟待解决的问题。

Method: 设计了一套LISTEN框架，利用LLM作为零样本偏好判别器，仅需专家用自然语言表达高层次目标即可。提出了两种迭代算法：LISTEN-U用LLM优化参数化效用函数；LISTEN-T以非参数方式通过小规模分组淘汰法选择最优解。

Result: 在航班预订、购物、考试安排等多样任务上测试，LISTEN-U在偏好可参数化表达时表现最佳，LISTEN-T表现更稳健。这些结论通过设计的新一致性度量指标加以验证。

Conclusion: LISTEN证明了用LLM直接对接专家自然语言偏好，有效减轻了传统多目标决策中偏好提取的认知负担，为复杂决策场景开辟了新的可能性。

Abstract: Human experts often struggle to select the best option from a large set of
items with multiple competing objectives, a process bottlenecked by the
difficulty of formalizing complex, implicit preferences. To address this, we
introduce LISTEN, a framework that leverages a Large Language Model (LLM) as a
zero-shot preference oracle, guided only by an expert's high-level priorities
in natural language. To operate within LLM constraints like context windows and
inference costs, we propose two iterative algorithms: LISTEN-U, which uses the
LLM to refine a parametric utility function, and LISTEN-T, a non-parametric
method that performs tournament-style selections over small batches of
solutions. Evaluated on diverse tasks including flight booking, shopping, and
exam scheduling, our results show LISTEN-U excels when preferences are
parametrically aligned (a property we measure with a novel concordance metric),
while LISTEN-T offers more robust performance. This work explores a promising
direction for steering complex multi-objective decisions directly with natural
language, reducing the cognitive burden of traditional preference elicitation.

</details>


### [78] [Beyond Length: Quantifying Long-Range Information for Long-Context LLM Pretraining Data](https://arxiv.org/abs/2510.25804)
*Haoran Deng,Yingyu Lin,Zhenghao Lin,Xiao Liu,Yizhou Sun,Yi-An Ma,Yeyun Gong*

Main category: cs.CL

TL;DR: 本文提出了LongFilter框架，通过筛选包含长距离依赖数据提升长文本语言模型的训练效率，显著增强了语言模型的长上下文处理能力。


<details>
  <summary>Details</summary>
Motivation: 长上下文语言模型依赖于长距离文本信息，但现有可用数据中很多长文本并不具备有效长距离依赖，仅靠局部上下文即可预测，导致训练效率低下。因此，需要有效筛选对提升长上下文能力有实际作用的数据。

Method: 提出LongFilter框架，通过分别对模型在长上下文与短上下文下的预测进行对比，计算长上下文带来的信息增益，选出那些依赖长距离信息的高质量样本，用于训练扩展长上下文能力的模型。

Result: 将LongFilter应用于将LLaMA-3-8B的上下文长度从8K扩展到64K的训练中，在HELMET、LongBench和RULER等基准测试上，筛选数据后训练的模型取得了显著改进。

Conclusion: LongFilter能够高效筛选适合长上下文模型训练的数据，提升了模型在长距离推理与理解任务上的表现，证明了数据质量对模型长上下文能力训练的重要性。

Abstract: Long-context language models unlock advanced capabilities in reasoning, code
generation, and document summarization by leveraging dependencies across
extended spans of text. However, a significant portion of readily available
long-text data lacks meaningful long-distance dependencies; most spans can be
predicted using only local context. Training on such data is inefficient,
making careful data selection crucial. Therefore, we introduce LongFilter, a
framework for curating training data tailored to long-context pretraining.
LongFilter measures the information gain provided by extended context by
contrasting model predictions under long-context versus short-context settings,
thereby identifying samples where long-range dependencies are essential.
Experiments with LLaMA-3-8B, extending its context length from 8K to 64K, show
that LongFilter efficiently selects high-quality data and yields substantial
improvements on benchmarks such as HELMET, LongBench, and RULER.

</details>


### [79] [Ideology-Based LLMs for Content Moderation](https://arxiv.org/abs/2510.25805)
*Stefano Civelli,Pietro Bernardelle,Nardiena A. Pratama,Gianluca Demartini*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型(LLMs)在扮演不同人设（persona）时，对有害内容判定的公平性与一致性。结果显示，虽然整体准确率变化不大，但不同政治倾向的人设会显著影响模型的判断表现，可能引入意识形态偏见。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在内容审核领域的广泛应用，确保AI判定的公平性和中立性变得尤为重要。本研究试图揭示人物设定（persona）对有害内容判定是否会带入意识形态偏差。

Method: 在不同的LLM架构、模型规模和内容形态（文本/多模态）上，对人格设定如何影响有害内容识别进行实验。同时通过一致性分析，对模型在不同意识形态的人设下的表现进行对比，还加入专门的政治倾向任务做交叉验证。

Result: 结果表明，虽然总准确率变化不明显，但不同政治倾向人设下，模型对于“有害”的判断倾向不同，模型与同政治倾向人设表现更为一致，意识形态分歧加大。政治任务研究也证实同一意识形态表现更一致，且有偏向维护自身立场、弱化对立观点危害的趋势。

Conclusion: 人设设定会引入潜在的意识形态偏见，这种偏见可在内容审核等实际应用中伪装成“中立”，对AI公平性和公正性构成挑战。

Abstract: Large language models (LLMs) are increasingly used in content moderation
systems, where ensuring fairness and neutrality is essential. In this study, we
examine how persona adoption influences the consistency and fairness of harmful
content classification across different LLM architectures, model sizes, and
content modalities (language vs. vision). At first glance, headline performance
metrics suggest that personas have little impact on overall classification
accuracy. However, a closer analysis reveals important behavioral shifts.
Personas with different ideological leanings display distinct propensities to
label content as harmful, showing that the lens through which a model "views"
input can subtly shape its judgments. Further agreement analyses highlight that
models, particularly larger ones, tend to align more closely with personas from
the same political ideology, strengthening within-ideology consistency while
widening divergence across ideological groups. To show this effect more
directly, we conducted an additional study on a politically targeted task,
which confirmed that personas not only behave more coherently within their own
ideology but also exhibit a tendency to defend their perspective while
downplaying harmfulness in opposing views. Together, these findings highlight
how persona conditioning can introduce subtle ideological biases into LLM
outputs, raising concerns about the use of AI systems that may reinforce
partisan perspectives under the guise of neutrality.

</details>


### [80] [Beyond Long Context: When Semantics Matter More than Tokens](https://arxiv.org/abs/2510.25816)
*Tarun Kumar Chawdhury,Jon D. Duke*

Main category: cs.CL

TL;DR: 本研究提出并验证了基于实体感知的检索方法（CLEAR），在临床问答中明显优于传统嵌入式方法，特别是在处理超长病历文本时兼顾高准确性与高效性。


<details>
  <summary>Details</summary>
Motivation: 传统EHR（电子健康记录）中的文本因编码和结构复杂，给基于语义的问答系统带来很大挑战，尤其是现有的向量检索方法难以捕捉细微的临床实体关系。作者希望提升临床文本问答在准确率和处理效率上的表现。

Method: 提出了Clinical Entity Augmented Retrieval（CLEAR）方法，通过引入实体感知检索增强临床语义问答效果。并开发了一个评测平台，比较CLEAR与零样本推理及传统分块生成在12份涵盖1万至6.5万token的实际病历文本上的表现。

Result: CLEAR的F1分数为0.90，显著优于基于嵌入的0.86，同时token消耗减少超过70%。综合测试表现CLEAR达到58.3%胜率、平均语义相似度0.878，并在超长病历（>65,000 tokens）上胜率达75%。

Conclusion: 基于实体感知的检索不仅提升了临床自然语言处理问答系统的准确率，也大幅降低了计算消耗。提出的评估平台可复用，为相关系统评测提供了透明且可靠的新基准。

Abstract: Electronic Health Records (EHR) store clinical documentation as base64
encoded attachments in FHIR DocumentReference resources, which makes semantic
question answering difficult. Traditional vector database methods often miss
nuanced clinical relationships. The Clinical Entity Augmented Retrieval (CLEAR)
method, introduced by Lopez et al. 2025, uses entity aware retrieval and
achieved improved performance with an F1 score of 0.90 versus 0.86 for
embedding based retrieval, while using over 70 percent fewer tokens. We
developed a Clinical Notes QA Evaluation Platform to validate CLEAR against
zero shot large context inference and traditional chunk based retrieval
augmented generation. The platform was tested on 12 clinical notes ranging from
10,000 to 65,000 tokens representing realistic EHR content. CLEAR achieved a
58.3 percent win rate, an average semantic similarity of 0.878, and used 78
percent fewer tokens than wide context processing. The largest performance
gains occurred on long notes, with a 75 percent win rate for documents
exceeding 65,000 tokens. These findings confirm that entity aware retrieval
improves both efficiency and accuracy in clinical natural language processing.
The evaluation framework provides a reusable and transparent benchmark for
assessing clinical question answering systems where semantic precision and
computational efficiency are critical.

</details>


### [81] [A Survey on Efficient Large Language Model Training: From Data-centric Perspectives](https://arxiv.org/abs/2510.25817)
*Junyu Luo,Bohan Wu,Xiao Luo,Zhiping Xiao,Yiqiao Jin,Rong-Cheng Tu,Nan Yin,Yifan Wang,Jingyang Yuan,Wei Ju,Ming Zhang*

Main category: cs.CL

TL;DR: 本论文对大语言模型（LLMs）高效利用数据进行后训练进行了系统综述，提出了相关分类法，并指明未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 后训练提升LLMs通用与领域能力需大量数据，但人工标注昂贵且数据规模回报递减，因此提升数据利用效率成为研究重点。

Method: 作者以数据为中心，系统梳理了数据高效LLM后训练方案，从数据选择、质量提升、合成数据、数据压缩蒸馏及自演进数据生态五方面提出分类法，并总结各类代表性方法。

Result: 总结了每一类别方法的研究现状，归纳了各自代表性工作，并整理了一份相关优秀论文列表供参考。

Conclusion: 提出了数据高效后训练面临的挑战与开放问题，给出未来探索方向，为提升大模型数据利用潜力提供指导和灵感。

Abstract: Post-training of Large Language Models (LLMs) is crucial for unlocking their
task generalization potential and domain-specific capabilities. However, the
current LLM post-training paradigm faces significant data challenges, including
the high costs of manual annotation and diminishing marginal returns on data
scales. Therefore, achieving data-efficient post-training has become a key
research question. In this paper, we present the first systematic survey of
data-efficient LLM post-training from a data-centric perspective. We propose a
taxonomy of data-efficient LLM post-training methods, covering data selection,
data quality enhancement, synthetic data generation, data distillation and
compression, and self-evolving data ecosystems. We summarize representative
approaches in each category and outline future research directions. By
examining the challenges in data-efficient LLM post-training, we highlight open
problems and propose potential research avenues. We hope our work inspires
further exploration into maximizing the potential of data utilization in
large-scale model training. Paper List:
https://github.com/luo-junyu/Awesome-Data-Efficient-LLM

</details>


### [82] [Evaluating the Impact of LLM-Assisted Annotation in a Perspectivized Setting: the Case of FrameNet Annotation](https://arxiv.org/abs/2510.25904)
*Frederico Belcavello,Ely Matos,Arthur Lorenzi,Lisandra Bonoto,Lívia Ruiz,Luiz Fernando Pereira,Victor Herbst,Yulla Navarro,Helen de Andrade Abreu,Lívia Dutra,Tiago Timponi Torrent*

Main category: cs.CL

TL;DR: 本文评估了基于大型语言模型（LLM）的自动和半自动语义角色标注工具在FrameNet风格语义注释任务中的表现，与人工注释进行了对比。


<details>
  <summary>Details</summary>
Motivation: 现有LLM工具能够加速甚至替代人工进行语言资源和数据集的构建，但其在特定视角下的注释质量、效率等综合表现仍缺乏系统性评估。

Method: 作者使用LLM驱动的语义角色标注器，对比了三种注释方式：纯手工、全自动、半自动（人工+LLM协作），并评估了它们的注释用时、覆盖率和多样性。

Result: 半自动注释能提升语义框架多样性，注释覆盖率与人工注释相当，但效率更高；而纯自动注释虽然最快，但在覆盖率和多样性均落后于其它方式。

Conclusion: 结合人类和LLM的半自动注释方法，在保证注释质量的同时可提升多样性和效率，是当前最佳的注释模式。

Abstract: The use of LLM-based applications as a means to accelerate and/or substitute
human labor in the creation of language resources and dataset is a reality.
Nonetheless, despite the potential of such tools for linguistic research,
comprehensive evaluation of their performance and impact on the creation of
annotated datasets, especially under a perspectivized approach to NLP, is still
missing. This paper contributes to reduction of this gap by reporting on an
extensive evaluation of the (semi-)automatization of FrameNet-like semantic
annotation by the use of an LLM-based semantic role labeler. The methodology
employed compares annotation time, coverage and diversity in three experimental
settings: manual, automatic and semi-automatic annotation. Results show that
the hybrid, semi-automatic annotation setting leads to increased frame
diversity and similar annotation coverage, when compared to the human-only
setting, while the automatic setting performs considerably worse in all
metrics, except for annotation time.

</details>


### [83] [RECAP: Reproducing Copyrighted Data from LLMs Training with an Agentic Pipeline](https://arxiv.org/abs/2510.25941)
*André V. Duarte,Xuying li,Bin Zeng,Arlindo L. Oliveira,Lei Li,Zhuo Li*

Main category: cs.CL

TL;DR: 本文提出了RECAP，一种针对大语言模型（LLM）输出中训练数据记忆提取和验证的自动化流程。通过循环反馈和校正提示，有效提升模型提取目标内容的能力，并通过越狱模块应对因对齐策略导致的拒绝。实验在新基准EchoTrace上验证了方法的优越性。


<details>
  <summary>Details</summary>
Motivation: 当无法直接访问LLM训练数据时，评估模型是否记忆或输出了受版权保护等敏感内容变得非常困难。现有方法在有效性和精细化上有所不足，有必要提出更高效、更精确的自动化检测与修正方法。

Method: RECAP流程包括：1）初步让LLM输出目标内容；2）使用第二个LLM与参考文档对比检测差异；3）生成最小化修正提示，反馈给目标模型指导下一轮输出；4）为避免因对齐策略导致的拒绝，引入了越狱（jailbreak）模块。整个过程多轮迭代，提升输出准确度。

Result: 在包含30部完整书籍的新数据集EchoTrace上评估，RECAP多轮迭代方法相比单轮方法有显著提升。以GPT-4.1为例，受版权保护文本的ROUGE-L分数从0.38提升至0.47，提升近24%。

Conclusion: RECAP为自动发现、提取和验证LLM训练数据中的记忆内容提供了高效工具，尤其在绕过模型对齐拒绝和提高提取精度方面表现突出，对模型安全和合规性分析具有重要意义。

Abstract: If we cannot inspect the training data of a large language model (LLM), how
can we ever know what it has seen? We believe the most compelling evidence
arises when the model itself freely reproduces the target content. As such, we
propose RECAP, an agentic pipeline designed to elicit and verify memorized
training data from LLM outputs. At the heart of RECAP is a feedback-driven
loop, where an initial extraction attempt is evaluated by a secondary language
model, which compares the output against a reference passage and identifies
discrepancies. These are then translated into minimal correction hints, which
are fed back into the target model to guide subsequent generations. In
addition, to address alignment-induced refusals, RECAP includes a jailbreaking
module that detects and overcomes such barriers. We evaluate RECAP on
EchoTrace, a new benchmark spanning over 30 full books, and the results show
that RECAP leads to substantial gains over single-iteration approaches. For
instance, with GPT-4.1, the average ROUGE-L score for the copyrighted text
extraction improved from 0.38 to 0.47 - a nearly 24% increase.

</details>


### [84] [Revisiting Multilingual Data Mixtures in Language Model Pretraining](https://arxiv.org/abs/2510.25947)
*Negar Foroutan,Paul Teiletche,Ayush Kumar Tarun,Antoine Bosselut*

Main category: cs.CL

TL;DR: 本文通过在1.1B和3B参数规模的大型语言模型上，使用多达400种语言的多语料进行预训练，研究多语种数据混合对模型表现的影响，质疑了多语种带来性能损失的传统看法。


<details>
  <summary>Details</summary>
Motivation: 多语种大型语言模型的训练中，关于多语覆盖与性能之间存在权衡的“多语咒语”问题一直备受争论。本文旨在通过实证研究，验证这些关于多语训练的常见假设是否成立。

Method: 训练1.1B和3B参数大小的大型语言模型，分别在包含25至400种语言的多语种多样语料库上进行对比实验，并重点分析不同语种数量、英语作为枢纽语言等设计对模型表现的影响。

Result: 1）只要每种语言的语料数足够，将英语与多语数据结合不会削弱各自在本语言的表现。2）英语作为枢纽语言利于多语泛化，比仅用同语系枢纽语言更有效。3）1.1B/3B模型下，扩展多语种未显著引发“多语咒语”；只要数据平衡，多语数据可增强模型。

Conclusion: 结论表明：只要训练语料平衡，加入多语种不会显著损害模型的总体表现，甚至对低资源语言有积极作用，这对多语种大模型的预训练实践具有指导意义。

Abstract: The impact of different multilingual data mixtures in pretraining large
language models (LLMs) has been a topic of ongoing debate, often raising
concerns about potential trade-offs between language coverage and model
performance (i.e., the curse of multilinguality). In this work, we investigate
these assumptions by training 1.1B and 3B parameter LLMs on diverse
multilingual corpora, varying the number of languages from 25 to 400. Our study
challenges common beliefs surrounding multilingual training. First, we find
that combining English and multilingual data does not necessarily degrade the
in-language performance of either group, provided that languages have a
sufficient number of tokens included in the pretraining corpus. Second, we
observe that using English as a pivot language (i.e., a high-resource language
that serves as a catalyst for multilingual generalization) yields benefits
across language families, and contrary to expectations, selecting a pivot
language from within a specific family does not consistently improve
performance for languages within that family. Lastly, we do not observe a
significant "curse of multilinguality" as the number of training languages
increases in models at this scale. Our findings suggest that multilingual data,
when balanced appropriately, can enhance language model capabilities without
compromising performance, even in low-resource settings

</details>


### [85] [Semantic Label Drift in Cross-Cultural Translation](https://arxiv.org/abs/2510.25967)
*Mohsinul Kabir,Tasnim Ahmed,Md Mezbaur Rahman,Polydoros Giannouris,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 本文研究了机器翻译在不同文化间由于文化不一致导致标签（如情感）漂移的问题，即在翻译过程中语义标签可能发生变化。


<details>
  <summary>Details</summary>
Motivation: 尽管机器翻译广泛用于低资源语言的数据增强，现有研究多聚焦于情感等标签的保留，但往往忽视了源语言和目标语言文化差异可能导致的标签漂移。作者认为文化对语义标签变化有重大影响。

Method: 作者设计实验，针对文化敏感和文化中性领域，将源语言与目标语言进行不同翻译工具（包括大型语言模型和传统统计机器翻译）的对比，分析语义标签在翻译中的变化，评估文化相似度对标签保真的影响。

Result: 主要结果有三：1）不论是传统还是现代平行翻译系统，尤其是在文化敏感领域，翻译会带来语义标签漂移；2）大型语言模型因编码了文化知识，标签漂移问题甚至比传统模型更突出；3）源、目标语言间的文化相似性极大影响语义标签的保留效果。

Conclusion: 机器翻译如果忽略了文化差异，容易出现标签不保真等问题，甚至可能带来误解和文化冲突。研究凸显了在自然语言处理应用中，将文化因素纳入翻译评估和系统设计的重要性。

Abstract: Machine Translation (MT) is widely employed to address resource scarcity in
low-resource languages by generating synthetic data from high-resource
counterparts. While sentiment preservation in translation has long been
studied, a critical but underexplored factor is the role of cultural alignment
between source and target languages. In this paper, we hypothesize that
semantic labels are drifted or altered during MT due to cultural divergence.
Through a series of experiments across culturally sensitive and neutral
domains, we establish three key findings: (1) MT systems, including modern
Large Language Models (LLMs), induce label drift during translation,
particularly in culturally sensitive domains; (2) unlike earlier statistical MT
tools, LLMs encode cultural knowledge, and leveraging this knowledge can
amplify label drift; and (3) cultural similarity or dissimilarity between
source and target languages is a crucial determinant of label preservation. Our
findings highlight that neglecting cultural factors in MT not only undermines
label fidelity but also risks misinterpretation and cultural conflict in
downstream applications.

</details>


### [86] [SymCode: A Neurosymbolic Approach to Mathematical Reasoning via Verifiable Code Generation](https://arxiv.org/abs/2510.25975)
*Sina Bagheri Nezhad,Yao Li,Ameeta Agrawal*

Main category: cs.CL

TL;DR: 本文提出SymCode框架，将大语言模型（LLM）的数学推理任务转化为基于SymPy库的可验证代码生成，提升了解题准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在复杂数学推理任务中表现不佳，主要由于生成的文字性解答难以验证且存在算术错误。传统方法如Chain of Thought缺乏可验证机制，导致结果不可靠。

Method: SymCode通过神经符号方法，将数学问题求解转化为可以用SymPy库验证的代码生成任务，结合符号计算的确定性引擎提升解题可验证性。作者在多个权威数学推理基准（如MATH-500和OlympiadBench）上对该框架进行了评测。

Result: SymCode相较于现有基线方法，在准确率上最高提升13.6个百分点，并表现出更高的token效率，错误也从难以定位的逻辑谬误转变为更透明的程序性错误。

Conclusion: SymCode将LLM推理与确定性符号引擎结合，极大增强了AI在数学等形式化领域的准确性与可信度，是向高可靠AI发展的重要一步。

Abstract: Large Language Models (LLMs) often struggle with complex mathematical
reasoning, where prose-based generation leads to unverified and arithmetically
unsound solutions. Current prompting strategies like Chain of Thought still
operate within this unreliable medium, lacking a mechanism for deterministic
verification. To address these limitations, we introduce SymCode, a
neurosymbolic framework that reframes mathematical problem-solving as a task of
verifiable code generation using the SymPy library. We evaluate SymCode on
challenging benchmarks, including MATH-500 and OlympiadBench, demonstrating
significant accuracy improvements of up to 13.6 percentage points over
baselines. Our analysis shows that SymCode is not only more token-efficient but
also fundamentally shifts model failures from opaque logical fallacies towards
transparent, programmatic errors. By grounding LLM reasoning in a deterministic
symbolic engine, SymCode represents a key step towards more accurate and
trustworthy AI in formal domains.

</details>


### [87] [NeuronMM: High-Performance Matrix Multiplication for LLM Inference on AWS Trainium](https://arxiv.org/abs/2510.25977)
*Dinghong Song,Jierui Xu,Weichu Yang,Pengfei Su,Dong Li*

Main category: cs.CL

TL;DR: 本文提出了一种针对AWS Trainium加速器优化的大型语言模型(LLM)推理中的高性能矩阵乘法(matmul)实现，有效提升了推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有AI加速器（如AWS Trainium）虽然为LLM训练和推理提供了高性能和低成本的选择，但其特殊的体系结构（如脉动阵列与特殊数据布局要求）使得高效利用其硬件变得困难，尤其是在核心算子矩阵乘法的实现上存在瓶颈。

Method: 作者为Trainium定制了高性能的matmul内核，采用了内核融合和新颖的缓存策略，减少了数据在手动管理的内存层级中的移动，最大化了SRAM带宽，并避免了昂贵的矩阵转置操作。

Result: 通过在9个数据集和4个主流LLM上的评测，优化后的matmul内核在Trainium上的性能比AWS官方实现平均提升1.35倍（最高2.22倍）；整体LLM推理速度平均提升1.66倍（最高2.49倍）。

Conclusion: 针对Trainium异构架构定制的优化方法能显著提升LLM推理中的矩阵乘法和整体性能，为在该平台上部署AI应用提供了有效技术路径。

Abstract: AI accelerators, customized to AI workloads, provide cost-effective and
high-performance solutions for training and inference. Trainium, an AI
accelerator recently developed by Amazon Web Services (AWS), provides an
attractive option for LLM training and inference through its heterogeneous
architecture. However, leveraging Trainium architecture for high performance
can be challenging because of its systolic array architecture and special
requirement on data layout. In this paper, we design high-performance matrix
multiplication (matmul), a critical compute kernel, for LLM inference on
Trainium. We introduce a series of techniques customized to Trainium based on
kernel fusion and novel caching strategies to reduce data movement across the
software-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive
matrix transpose. Evaluating with nine datasets and four recent LLMs, we show
that our system largely outperforms the state-of-the-art matmul implemented by
AWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x
speedup (up to 2.22x), which translates to an average 1.66x speedup (up to
2.49x) for end-to-end LLM inference.

</details>


### [88] [AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache](https://arxiv.org/abs/2510.25979)
*Dinghong Song,Yuan Feng,Yiwei Wang,Shangye Chen,Cyril Guyot,Filip Blagojevic,Hyeran Jeon,Pengfei Su,Dong Li*

Main category: cs.CL

TL;DR: 本文提出了一种名为AttnCache的方法，通过检索和复用相似的注意力图，从而加速大型语言模型（LLMs）在prefill阶段的推理，显著提升了推理速度且几乎不损失准确率。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型在推理prefill阶段，主要瓶颈是self-attention计算，其复杂度随序列长度呈二次增长，导致实际部署时极大拖慢了包括分类、问答、推荐和文本嵌入等真实应用场景的推理速度。作者发现不同语义的句子生成的注意力图其实常常非常相似，启发他们思考能否通过重复利用这些注意力图来减少计算量。

Method: 作者提出了AttnCache框架，建立基于注意力图的记忆库，通过缓存和高效的相似性检索技术，在推理过程中识别并重用已有的（缓存的）注意力图，从而减少每次推理时冗余的self-attention计算开销。

Result: 在CPU上，AttnCache实现了平均1.2倍整体推理加速和2倍self-attention加速；在GPU上实现了1.6倍整体推理加速和3倍self-attention加速，而且准确率基本未下降。

Conclusion: AttnCache为实际大型语言模型推理提供了一种行之有效的加速途径，能够在保证模型性能的前提下，显著改善prefill场景下的推理效率。

Abstract: Large Language Models (LLMs) are widely used in generative applications such
as chatting, code generation, and reasoning. However, many realworld workloads
such as classification, question answering, recommendation, and text embedding
rely solely on the prefill stage of inference, where the model encodes input
sequences without performing autoregressive decoding. In these prefill only
scenarios, the self-attention computation becomes the primary performance
bottleneck due to its quadratic complexity with respect to sequence length. In
this paper, we observe that semantically different sentences often produce
similar attention maps across layers and heads. Building on this insight, we
propose AttnCache, a framework that accelerates the prefill stage of LLM
inference by retrieving and reusing similar attention maps. Based on an
attention map memorization database, AttnCache employs efficient caching and
similarity search techniques to identify and reuse pre-cached attention maps
during inference, thereby reducing the computational overhead of
self-attention. Experimental results show that AttnCache achieves an average of
1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x
attention speedup on GPU, with negligible accuracy degradation.

</details>


### [89] [Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning](https://arxiv.org/abs/2510.25992)
*Yihe Deng,I-Hung Hsu,Jun Yan,Zifeng Wang,Rujun Han,Gufeng Zhang,Yanfei Chen,Wei Wang,Tomas Pfister,Chen-Yu Lee*

Main category: cs.CL

TL;DR: 本文提出了一种名为Supervised Reinforcement Learning（SRL）的新训练框架，用于提升小型大语言模型（LLM）在多步推理任务中的表现。SRL结合了监督和强化学习的优势，通过引导模型逐步进行逻辑行动生成和内部推理，以获得更高效、更灵活的学习信号。


<details>
  <summary>Details</summary>
Motivation: 现有小型开源LLM在解决多步推理问题时面临挑战。传统的RLVR方法在正确解极少采样到时失效，而SFT方法则易因死板模仿长演示而过拟合。因而亟需一种能提供更丰富监督信号并提升模型推理能力的新方法。

Method: SRL将问题求解重新表述为逻辑行动的序列生成，并训练模型在每步行动前先生成内部推理独白。奖励机制基于模型动作与专家动作的相似性做平滑反馈，哪怕所有输出都错也能传递有效学习信号，从而鼓励模型向专家演示灵活学习。

Result: SRL使小型模型得以学会以往SFT或RLVR都无法学会的挑战性多步推理任务。此外，用SRL做预训练再用RLVR精炼，表现最为强劲。SRL还成功泛化至软件工程等agent任务，显示出强大适用性。

Conclusion: SRL是一种更健壮、通用的推理导向型大语言模型训练框架，可有效弥补SFT与RLVR在复杂推理任务中的不足，推动小模型性能突破。

Abstract: Large Language Models (LLMs) often struggle with problems that require
multi-step reasoning. For small-scale open-source models, Reinforcement
Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely
sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to
overfit long demonstrations through rigid token-by-token imitation. To address
this gap, we propose Supervised Reinforcement Learning (SRL), a framework that
reformulates problem solving as generating a sequence of logical "actions". SRL
trains the model to generate an internal reasoning monologue before committing
to each action. It provides smoother rewards based on the similarity between
the model's actions and expert actions extracted from the SFT dataset in a
step-wise manner. This supervision offers richer learning signals even when all
rollouts are incorrect, while encouraging flexible reasoning guided by expert
demonstrations. As a result, SRL enables small models to learn challenging
problems previously unlearnable by SFT or RLVR. Moreover, initializing training
with SRL before refining with RLVR yields the strongest overall performance.
Beyond reasoning benchmarks, SRL generalizes effectively to agentic software
engineering tasks, establishing it as a robust and versatile training framework
for reasoning-oriented LLMs.

</details>


### [90] [PORTool: Tool-Use LLM Training with Rewarded Tree](https://arxiv.org/abs/2510.26020)
*Feijie Wu,Weiwu Zhu,Yuxiang Zhang,Soumya Chatterjee,Jiarong Zhu,Fan Mo,Rodin Luo,Jing Gao*

Main category: cs.CL

TL;DR: 提出了一种名为PORTool的强化学习方法，提升了大语言模型在多工具调用和动态环境中解决问题的能力。实验表明PORTool有效提升了准确率和操作效率。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在多步骤调用工具时通常只是模仿固定模式，缺乏探索不同解决方案的能力，面对动态环境时表现有限。

Method: 提出PORTool方法，基于强化学习，通过生成多条问题解决轨迹，利用树状结构为每一步分配奖励，并以这些奖励训练模型。奖励设计既考虑分叉下的相对优势，也结合整体轨迹优势。

Result: 实验覆盖17个工具、多个主题，PORTool相较于其他训练方法，显著提升了最终准确率与工具调用的效率。消融实验证明分步奖励机制的必要性及其设计的稳健性。

Conclusion: PORTool显著提升了工具型大语言模型在动态环境下探索多种高效解法的能力，有助于实现更智能、更灵活的工具集成推理。

Abstract: Current tool-use large language models (LLMs) are trained on static datasets,
enabling them to interact with external tools and perform multi-step,
tool-integrated reasoning, which produces tool-call trajectories. However,
these models imitate how a query is resolved in a generic tool-call routine,
thereby failing to explore possible solutions and demonstrating limited
performance in an evolved, dynamic tool-call environment. In this work, we
propose PORTool, a reinforcement learning (RL) method that encourages a
tool-use LLM to explore various trajectories yielding the correct answer.
Specifically, this method starts with generating multiple rollouts for a given
query, and some of them share the first few tool-call steps, thereby forming a
tree-like structure. Next, we assign rewards to each step, based on its ability
to produce a correct answer and make successful tool calls. A shared step
across different trajectories receives the same reward, while different steps
under the same fork receive different rewards. Finally, these step-wise rewards
are used to calculate fork-relative advantages, blended with
trajectory-relative advantages, to train the LLM for tool use. The experiments
utilize 17 tools to address user queries, covering both time-sensitive and
time-invariant topics. We conduct ablation studies to systematically justify
the necessity and the design robustness of step-wise rewards. Furthermore, we
compare the proposed PORTool with other training approaches and demonstrate
significant improvements in final accuracy and the number of tool-call steps.

</details>


### [91] [Rethinking Cross-lingual Alignment: Balancing Transfer and Cultural Erasure in Multilingual LLMs](https://arxiv.org/abs/2510.26024)
*HyoJung Han,Sweta Agrawal,Eleftheria Briakou*

Main category: cs.CL

TL;DR: 本文提出跨语种对齐（CLA）虽提升了LLMs跨语言知识迁移能力，却引发“文化抹除”问题。作者构建了评估框架，揭示现有CLA方法在提升事实性知识迁移的同时，牺牲了文化本地化。为此，作者提出层级激活调控新方法以兼得知识迁移与文化本地化。


<details>
  <summary>Details</summary>
Motivation: 跨语种对齐有助于多语言知识传递，但作者怀疑对齐过程可能导致生成结果缺乏文化多样性，即“文化抹除”，有损模型为不同语言用户定制内容的能力，亟需系统评估和改进。

Method: 作者设计了“迁移-本地化平面”评估框架，对CLA方法在六种语言上的表现进行量化，分析模型内部表征，进而提出“外科式引导”方法，通过在不同层级定向激活，实现知识迁移与本地化的有效分离和均衡。

Result: 实证发现，现有CLA方法无一例外地提升了知识迁移能力但削弱文化本地化。作者的方法则能依据不同层级调控，实现两者折中，优于现有对齐技术。

Conclusion: 当前对齐技术在促进知识迁移的同时不可避免引发文化抹除。作者新方法实现了两者平衡，有望提升多语种LLMs在全球化与本地化需求下的表现。

Abstract: Cross-lingual alignment (CLA) aims to align multilingual representations,
enabling Large Language Models (LLMs) to seamlessly transfer knowledge across
languages. While intuitive, we hypothesize, this pursuit of representational
convergence can inadvertently cause "cultural erasure", the functional loss of
providing culturally-situated responses that should diverge based on the query
language. In this work, we systematically analyze this trade-off by introducing
a holistic evaluation framework, the transfer-localization plane, which
quantifies both desirable knowledge transfer and undesirable cultural erasure.
Using this framework, we re-evaluate recent CLA approaches and find that they
consistently improve factual transfer at the direct cost of cultural
localization across all six languages studied. Our investigation into the
internal representations of these models reveals a key insight: universal
factual transfer and culturally-specific knowledge are optimally steerable at
different model layers. Based on this finding, we propose Surgical Steering, a
novel inference-time method that disentangles these two objectives. By applying
targeted activation steering to distinct layers, our approach achieves a better
balance between the two competing dimensions, effectively overcoming the
limitations of current alignment techniques.

</details>


### [92] [Artificial Intelligence-Enabled Analysis of Radiology Reports: Epidemiology and Consequences of Incidental Thyroid Findings](https://arxiv.org/abs/2510.26032)
*Felipe Larios,Mariana Borras-Osorio,Yuqi Wu,Ana Gabriela Claros,David Toro-Tobon,Esteban Cabezas,Ricardo Loor-Torres,Maria Mateo Chavez,Kerly Guevara Maldonado,Luis Vilatuna Andrango,Maria Lizarazo Jimenez,Ivan Mateo Alzamora,Misk Al Zahidy,Marcelo Montero,Ana Cristina Proano,Cristian Soto Jacome,Jungwei W. Fan,Oscar J. Ponce-Ponte,Megan E. Branda,Naykky Singh Ospina,Juan P. Brito*

Main category: cs.CL

TL;DR: 该研究通过自然语言处理（NLP）系统大规模分析放射影像报告，发现偶发性甲状腺发现(ITFs)常见，易导致过度诊断和后续医疗干预。


<details>
  <summary>Details</summary>
Motivation: 影像检查越来越常发现非预期的甲状腺异常，但其发生率、特征及对临床影响尚未明确，因此需建立科学的识别和评估方法。

Method: 构建并验证了基于transformer的自然语言处理管道，用于自动识别Mayo Clinic 2017-2023年多模态影像报告中的ITFs，分析其流行病学、影像特征和后续医疗结果，并用回归分析影响因素。

Result: 在115,683例无甲状腺病史成人中，7.8%发现了ITF，92.9%为结节，ITF患者更常见于女性、年长者、高BMI及肿瘤/内科就诊者。ITF患者随访活检、手术和甲状腺癌诊断概率显著升高，癌多为低风险乳头状癌。报告中的结节详细特征记录较差。

Conclusion: 偶发甲状腺发现普遍存在，显著提高过度诊断及医疗干预可能性。需标准化报告和更有选择性的随访策略以减少过度诊疗。

Abstract: Importance Incidental thyroid findings (ITFs) are increasingly detected on
imaging performed for non-thyroid indications. Their prevalence, features, and
clinical consequences remain undefined. Objective To develop, validate, and
deploy a natural language processing (NLP) pipeline to identify ITFs in
radiology reports and assess their prevalence, features, and clinical outcomes.
Design, Setting, and Participants Retrospective cohort of adults without prior
thyroid disease undergoing thyroid-capturing imaging at Mayo Clinic sites from
July 1, 2017, to September 30, 2023. A transformer-based NLP pipeline
identified ITFs and extracted nodule characteristics from image reports from
multiple modalities and body regions. Main Outcomes and Measures Prevalence of
ITFs, downstream thyroid ultrasound, biopsy, thyroidectomy, and thyroid cancer
diagnosis. Logistic regression identified demographic and imaging-related
factors. Results Among 115,683 patients (mean age, 56.8 [SD 17.2] years; 52.9%
women), 9,077 (7.8%) had an ITF, of which 92.9% were nodules. ITFs were more
likely in women, older adults, those with higher BMI, and when imaging was
ordered by oncology or internal medicine. Compared with chest CT, ITFs were
more likely via neck CT, PET, and nuclear medicine scans. Nodule
characteristics were poorly documented, with size reported in 44% and other
features in fewer than 15% (e.g. calcifications). Compared with patients
without ITFs, those with ITFs had higher odds of thyroid nodule diagnosis,
biopsy, thyroidectomy and thyroid cancer diagnosis. Most cancers were
papillary, and larger when detected after ITFs vs no ITF. Conclusions ITFs were
common and strongly associated with cascades leading to the detection of small,
low-risk cancers. These findings underscore the role of ITFs in thyroid cancer
overdiagnosis and the need for standardized reporting and more selective
follow-up.

</details>


### [93] [QCoder Benchmark: Bridging Language Generation and Quantum Hardware through Simulator-Based Feedback](https://arxiv.org/abs/2510.26101)
*Taku Mikuriya,Tatsuya Ishigaki,Masayuki Kawarada,Shunya Minami,Tadashi Kadowaki,Yohichi Suzuki,Soshun Naito,Shunya Takata,Takumi Kato,Tamotsu Basseda,Reo Yamada,Hiroya Takamura*

Main category: cs.CL

TL;DR: 本文提出了QCoder Benchmark，以评估大语言模型（LLM）在量子编程代码生成任务中的能力。该基准可通过量子模拟环境反馈领域特定指标，并收集了大量人类书写代码以支持更全面的评测。实验显示当前LLM在该领域表现尚有限。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在自动代码生成领域取得了很大进展，但其在需与硬件交互的量子编程等领域的能力尚未被充分探索，因此需要针对这些领域提供专门评测平台。

Method: 提出QCoder Benchmark，结合量子模拟器提供如电路深度、执行时间、错误分类等定制指标反馈，并引入真实编程竞赛中人类代码，实现对LLM生成代码的定量与定性评估。

Result: 实验显示GPT-4o准确率仅为18.97%，而具备推理能力的模型o3达到78%，明显高于人类平均成功率（39.98%）。

Conclusion: 当前LLM在量子编程中的自动代码生成仍有巨大挑战，QCoder Benchmark可以推动领域进一步发展，数据与API已公开，支持相关研究深化。

Abstract: Large language models (LLMs) have increasingly been applied to automatic
programming code generation. This task can be viewed as a language generation
task that bridges natural language, human knowledge, and programming logic.
However, it remains underexplored in domains that require interaction with
hardware devices, such as quantum programming, where human coders write Python
code that is executed on a quantum computer. To address this gap, we introduce
QCoder Benchmark, an evaluation framework that assesses LLMs on quantum
programming with feedback from simulated hardware devices. Our benchmark offers
two key features. First, it supports evaluation using a quantum simulator
environment beyond conventional Python execution, allowing feedback of
domain-specific metrics such as circuit depth, execution time, and error
classification, which can be used to guide better generation. Second, it
incorporates human-written code submissions collected from real programming
contests, enabling both quantitative comparisons and qualitative analyses of
LLM outputs against human-written codes. Our experiments reveal that even
advanced models like GPT-4o achieve only around 18.97% accuracy, highlighting
the difficulty of the benchmark. In contrast, reasoning-based models such as o3
reach up to 78% accuracy, outperforming averaged success rates of human-written
codes (39.98%). We release the QCoder Benchmark dataset and public evaluation
API to support further research.

</details>


### [94] [Reasoning Path Divergence: A New Metric and Curation Strategy to Unlock LLM Diverse Thinking](https://arxiv.org/abs/2510.26122)
*Feng Ju,Zeyu Qin,Rui Min,Zhitao He,Lingpeng Kong,Yi R. Fung*

Main category: cs.CL

TL;DR: 本文提出用“一个问题，多种解决方案”（1PNS）替代传统“一个问题，一个解决方案”（1P1S）的训练方式，通过提升推理路径多样性，增强大模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型（LLM）通过一种标准答案进行训练（1P1S），导致模型推理过程缺乏多样性，从而影响模型在复杂推理任务上的表现。

Method: 作者提出1PNS训练范式，让模型暴露于同一问题的多种真实有效的推理路径。同时，发明了Reasoning Path Divergence (RPD)度量，用来评估和筛选推理路径的差异性。借助RPD，作者整理了一套多样化推理数据，并微调了Qwen3-4B-Base模型。

Result: 实验证明，采用RPD筛选后的1PNS训练数据微调模型，输出推理更丰富，pass@16指标相较传统方法提升2.8%，在AIME24数据集上提升4.99%。

Conclusion: 1PNS训练有效提升了模型的推理多样性和实际推理能力，且能与Test-Time Scaling产生协同增益。

Abstract: While Test-Time Scaling (TTS) has proven effective in improving the reasoning
ability of large language models (LLMs), low diversity in model outputs often
becomes a bottleneck; this is partly caused by the common "one problem, one
solution" (1P1S) training practice, which provides a single canonical answer
and can push models toward a narrow set of reasoning paths. To address this, we
propose a "one problem, multiple solutions" (1PNS) training paradigm that
exposes the model to a variety of valid reasoning trajectories and thus
increases inference diversity. A core challenge for 1PNS is reliably measuring
semantic differences between multi-step chains of thought, so we introduce
Reasoning Path Divergence (RPD), a step-level metric that aligns and scores
Long Chain-of-Thought solutions to capture differences in intermediate
reasoning. Using RPD, we curate maximally diverse solution sets per problem and
fine-tune Qwen3-4B-Base. Experiments show that RPD-selected training yields
more varied outputs and higher pass@k, with an average +2.80% gain in pass@16
over a strong 1P1S baseline and a +4.99% gain on AIME24, demonstrating that
1PNS further amplifies the effectiveness of TTS. Our code is available at
https://github.com/fengjujf/Reasoning-Path-Divergence .

</details>


### [95] [On the Influence of Discourse Relations in Persuasive Texts](https://arxiv.org/abs/2510.26124)
*Nawar Turk,Sevag Kaspar,Leila Kosseim*

Main category: cs.CL

TL;DR: 本文探索了说服技巧与话语关系之间的联系，利用大语言模型和提示工程对原有数据集进行了双重标注，揭示了特定话语关系在说服文本中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 目前缺少同时标注说服技巧（PT）和话语关系（DR）的数据集，而理解二者关系对检测宣传、虚假信息及改进传播理论具有重要意义。

Method: 作者以SemEval 2023 Task 3数据集（含19种PT）为基础，利用4种LLM和10种提示共训练出40个话语关系分类器，对数据集标注为22种PDTB 3.0二级话语关系，并通过集成模型及不同策略生成了5个新的“银标”数据集。

Result: 分析显示6类话语关系（因果、目的、对比、因果+信念、让步、条件）在说服文本中尤其关键，并与Loaded Language、夸大/缩小、重复、质疑等说服技巧高度相关。

Conclusion: 这些发现有助于更好地理解有效传播方式，也能提升对网络宣传与虚假信息检测的能力。

Abstract: This paper investigates the relationship between Persuasion Techniques (PTs)
and Discourse Relations (DRs) by leveraging Large Language Models (LLMs) and
prompt engineering. Since no dataset annotated with both PTs and DRs exists, we
took the SemEval 2023 Task 3 dataset labelled with 19 PTs as a starting point
and developed LLM-based classifiers to label each instance of the dataset with
one of the 22 PDTB 3.0 level-2 DRs. In total, four LLMs were evaluated using 10
different prompts, resulting in 40 unique DR classifiers. Ensemble models using
different majority-pooling strategies were used to create 5 silver datasets of
instances labelled with both persuasion techniques and level-2 PDTB senses. The
silver dataset sizes vary from 1,281 instances to 204 instances, depending on
the majority pooling technique used. Statistical analysis of these silver
datasets shows that six discourse relations (namely Cause, Purpose, Contrast,
Cause+Belief, Concession, and Condition) play a crucial role in persuasive
texts, especially in the use of Loaded Language, Exaggeration/Minimisation,
Repetition and to cast Doubt. This insight can contribute to detecting online
propaganda and misinformation, as well as to our general understanding of
effective communication.

</details>


### [96] [MossNet: Mixture of State-Space Experts is a Multi-Head Attention](https://arxiv.org/abs/2510.26182)
*Shikhar Tuli,James Seale Smith,Haris Jeelani,Chi-Heng Lin,Abhishek Patel,Vasili Ramanishka,Yen-Chang Hsu,Hongxia Jin*

Main category: cs.CL

TL;DR: MossNet提出了一种结合多专家混合和状态空间的新型语言模型架构，在效率和性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于状态空间和门控递归模型（SSM/GRM）的方法多只模拟单一注意力头，导致模型表达能力有限。因此，亟需更高效且表达能力更强的新型架构。

Method: MossNet采用混合专家（MoE）方法，不仅在MLP通道混合块中，也在时间混合的SSM核中实现多个“注意力头”，从而模拟线性多头注意力机制，并进行大规模实验验证。

Result: MossNet在语言建模和多个下游任务上优于同等规模的数据和模型规模的transformer及SSM架构。大规模MossNet模型在数万亿token训练下表现出良好的扩展性和强劲性能。同时，在实际设备上（Galaxy S24 Ultra和Nvidia A100）测试显示其运行速度和资源利用优于同类基线。

Conclusion: MossNet是一种高效、高性能的循环式大语言模型架构，为新一代高效LLM提供了新的设计方向。

Abstract: Large language models (LLMs) have significantly advanced generative
applications in natural language processing (NLP). Recent trends in model
architectures revolve around efficient variants of transformers or
state-space/gated-recurrent models (SSMs, GRMs). However, prevailing
SSM/GRM-based methods often emulate only a single attention head, potentially
limiting their expressiveness. In this work, we propose MossNet, a novel
mixture-of-state-space-experts architecture that emulates a linear multi-head
attention (MHA). MossNet leverages a mixture-of-experts (MoE) implementation
not only in channel-mixing multi-layered perceptron (MLP) blocks but also in
the time-mixing SSM kernels to realize multiple "attention heads." Extensive
experiments on language modeling and downstream evaluations show that MossNet
outperforms both transformer- and SSM-based architectures of similar model size
and data budgets. Larger variants of MossNet, trained on trillions of tokens,
further confirm its scalability and superior performance. In addition,
real-device profiling on a Samsung Galaxy S24 Ultra and an Nvidia A100 GPU
demonstrate favorable runtime speed and resource usage compared to similarly
sized baselines. Our results suggest that MossNet is a compelling new direction
for efficient, high-performing recurrent LLM architectures.

</details>


### [97] [Similarity-Distance-Magnitude Language Models](https://arxiv.org/abs/2510.26183)
*Allen Schmaltz*

Main category: cs.CL

TL;DR: 文章提出了一种名为Similarity-Distance-Magnitude（SDM）的语言模型，通过在模型最后一层加入SDM激活层，以提高模型在高置信度区域内的预测性能，并提升模型的二元分类能力。该方法能够使预训练的解码式Transformer模型通过有监督微调快速转变为SDM模型，并获得与强基线相比更高的统计效率和更低的拒答率。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型在生成时存在置信度校准不足、拒答率较高等问题，影响了其在需要高置信度输出场景中的表现。进一步提高模型在高置信度区域的生成能力，并降低无效输出的概率，有助于提升模型在下游任务中的实际应用价值。

Method: 作者提出将预训练的解码器型Transformer模型，通过有监督微调，引入SDM激活层以实现更细致的置信度区分；模型在训练过程中采用对比性输入编码，并在线生成更多的困难负样本来提升判别能力。最终，使用SDM层对instruction-following（二元分类任务）进行高效优化。

Result: 经实验证明，模型转化为SDM LMs后，相较于现有有监督基线，拒答率显著降低，统计效率提升，说明其更能在高置信度区间做出稳定、可靠的输出。

Conclusion: 该方法简单高效，能够直接赋能预训练Transformer模型，提升其在需要置信度判别任务下的实用性，并兼具改善输出可靠性和提高统计效率的作用。

Abstract: We introduce Similarity-Distance-Magnitude (SDM) language models (LMs), which
are sequence prediction models fine-tuned to maximize the proportion of
generations in the well-calibrated, high-probability region partitioned by a
final-layer SDM activation layer used for binary classification of
instruction-following. We demonstrate that existing pre-trained decoder-only
Transformer LMs can be readily converted into SDM LMs via supervised
fine-tuning, using the final-layer SDM activation layer during training to
estimate a change-of-base for a supervised next-token loss over a contrastive
input encoding scheme, with additional hard negative examples generated online
during training. This results in reduced abstentions (i.e., improved
statistical efficiency) compared to strong supervised baselines.

</details>


### [98] [RCScore: Quantifying Response Consistency in Large Language Models](https://arxiv.org/abs/2510.26193)
*Dongjun Jang,Youngchae Ahn,Hyopil Shin*

Main category: cs.CL

TL;DR: 本文提出了一种名为RCScore的新评估框架，针对不同指令风格对大模型表现的影响进行量化分析，揭示现有评测忽视的模型脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）评测大多只使用单一指令模板，未能反映模型对于不同指令风格敏感性，这会影响模型在真实世界部署的可靠性和稳健性。

Method: 作者设计了RCScore评测方法，将评测基准中的问题转化为多种不同的指令表达，再对多款LLM模型在这些不同指令上的表现进行系统测试，引入了Cross-Response Similarity（CRS）指标来衡量模型在不同指令下输出的一致性，并与任务准确率进行相关性分析。

Result: 在十个主流LLM和四个推理基准上进行实验，发现同一任务因指令表述变化准确率可波动高达16.7%。CRS指标与任务准确率高度相关，说明指令下输出风格一致性是衡量模型可靠性的有效代理。此外，确定性解码生成风格更稳定的输出，且更大规模模型跨风格一致性更好。

Conclusion: RCScore方法能够更全面评估LLM模型的指令健壮性，有助于揭示和提升模型在真实环境下的稳定性和可靠性。

Abstract: Current LLM evaluations often rely on a single instruction template,
overlooking models' sensitivity to instruction style-a critical aspect for
real-world deployments. We present RCScore, a multi-dimensional framework
quantifying how instruction formulation affects model responses. By
systematically transforming benchmark problems into multiple instruction
styles, RCScore reveals performance variations undetected by conventional
metrics. Our experiments across ten LLMs on four reasoning benchmarks
demonstrate that instruction style can shift accuracy by up to 16.7% points. We
introduce Cross-Response Similarity (CRS), a method applying RCScore metrics to
measure stylistic self-consistency, and establish its strong correlation with
task accuracy, suggesting consistency as a valuable proxy for model
reliability. Additional findings show that deterministic decoding produces more
stylistically stable outputs, and model scale correlates positively with
cross-style consistency. RCScore offers a principled approach to assess
instruction robustness.

</details>


### [99] [Don't Let It Fade: Preserving Edits in Diffusion Language Models via Token Timestep Allocation](https://arxiv.org/abs/2510.26200)
*Woojin Kim,Jaeyoung Do*

Main category: cs.CL

TL;DR: 本文提出并分析了扩散语言模型（DLMs）在实际可控性中的核心问题——更新遗忘，并提出了一种名为Token Timestep Allocation（TTA）的解决方法，有效提升了生成文本的可控性和流畅性。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型可实现细粒度编辑，但在实际应用中难以保持对生成文本的连续控制，表现为先前的语义编辑易被后续步骤遗忘，导致文本不连贯或语义漂移，因此需要寻找机制提升其可控性和语义一致性。

Method: 作者提出Token Timestep Allocation（TTA）方法：在每个时间步为每个token分配不同的更新计划，重要token在早期即被冻结，表达不确定的token则持续更新。该方法可采用固定策略或由任务信号自适应调整，可应用于多种DLM推理阶段，无需模型结构或训练修改。

Result: TTA在情感控制任务中将准确率提升20%以上，困惑度减半且推理步骤减少五分之四，在文本净化任务中最大毒性值和困惑度均显著降低，表现出更优的可控性和文本流畅性。

Conclusion: 通过引入timestep分配实现软排序，有效缓解了DLM中的更新遗忘问题，是实现稳定、可控扩散文本生成的关键手段。

Abstract: While diffusion language models (DLMs) enable fine-grained refinement, their
practical controllability remains fragile. We identify and formally
characterize a central failure mode called update forgetting, in which uniform
and context agnostic updates induce token level fluctuations across timesteps,
erasing earlier semantic edits and disrupting the cumulative refinement
process, thereby degrading fluency and coherence. As this failure originates in
uniform and context agnostic updates, effective control demands explicit token
ordering. We propose Token Timestep Allocation (TTA), which realizes soft and
semantic token ordering via per token timestep schedules: critical tokens are
frozen early, while uncertain tokens receive continued refinement. This
timestep based ordering can be instantiated as either a fixed policy or an
adaptive policy driven by task signals, thereby supporting a broad spectrum of
refinement strategies. Because it operates purely at inference time, it applies
uniformly across various DLMs and naturally extends to diverse supervision
sources. Empirically, TTA improves controllability and fluency: on sentiment
control, it yields more than 20 percent higher accuracy and nearly halves
perplexity using less than one fifth the steps; in detoxification, it lowers
maximum toxicity (12.2 versus 14.5) and perplexity (26.0 versus 32.0).
Together, these results demonstrate that softened ordering via timestep
allocation is the critical lever for mitigating update forgetting and achieving
stable and controllable diffusion text generation.

</details>


### [100] [What's In My Human Feedback? Learning Interpretable Descriptions of Preference Data](https://arxiv.org/abs/2510.26202)
*Rajiv Movva,Smitha Milli,Sewon Min,Emma Pierson*

Main category: cs.CL

TL;DR: 本文提出了一种名为WIMHF（What's In My Human Feedback?）的方法，通过稀疏自编码器对人类反馈数据特征进行自动解释，有效发现反馈中的关键可解释特征，用于提升模型安全性和个性化。


<details>
  <summary>Details</summary>
Motivation: 目前在利用人类反馈改进语言模型时，反馈数据中具体编码了什么人类偏好尚不明确，且现有研究多依赖于预先假设，难以自动发现反馈中的核心特征。这限制了对反馈有效性和安全性的把控。

Method: 作者设计了WIMHF方法，基于稀疏自编码器自动从反馈数据中提取能够解释数据偏好的特征，并区分数据可以衡量的偏好与标注者实际表达的偏好。不依赖于先验假设，直接从数据中发现主要的、可解释的人类偏好特征。

Result: 在7个数据集上，WIMHF都能提取出少量可解释特征，这些特征贡献了大部分偏好预测信号，并揭示了不同数据集（如Reddit与HH-RLHF、PRISM等）用户的偏好差异。还可以发现潜在的有害偏好，如对有害内容的支持。利用其特征进行数据重标记，可显著提升安全性，同时可实现个性化偏好预测。

Conclusion: WIMHF为分析和利用人类反馈提供了一种具有人本视角的新工具，帮助从业者理解和改进偏好数据，提高模型安全及个性化水平。

Abstract: Human feedback can alter language models in unpredictable and undesirable
ways, as practitioners lack a clear understanding of what feedback data
encodes. While prior work studies preferences over certain attributes (e.g.,
length or sycophancy), automatically extracting relevant features without
pre-specifying hypotheses remains challenging. We introduce What's In My Human
Feedback? (WIMHF), a method to explain feedback data using sparse autoencoders.
WIMHF characterizes both (1) the preferences a dataset is capable of measuring
and (2) the preferences that the annotators actually express. Across 7
datasets, WIMHF identifies a small number of human-interpretable features that
account for the majority of the preference prediction signal achieved by
black-box models. These features reveal a wide diversity in what humans prefer,
and the role of dataset-level context: for example, users on Reddit prefer
informality and jokes, while annotators in HH-RLHF and PRISM disprefer them.
WIMHF also surfaces potentially unsafe preferences, such as that LMArena users
tend to vote against refusals, often in favor of toxic content. The learned
features enable effective data curation: re-labeling the harmful examples in
Arena yields large safety gains (+37%) with no cost to general performance.
They also allow fine-grained personalization: on the Community Alignment
dataset, we learn annotator-specific weights over subjective features that
improve preference prediction. WIMHF provides a human-centered analysis method
for practitioners to better understand and use preference data.

</details>


### [101] [Towards Global Retrieval Augmented Generation: A Benchmark for Corpus-Level Reasoning](https://arxiv.org/abs/2510.26205)
*Qi Luo,Xiaonan Li,Tingshuo Fan,Xinchi Chen,Xipeng Qiu*

Main category: cs.CL

TL;DR: 该论文提出了GlobalQA评测基准，用于衡量大语言模型在全局RAG（跨文档集合级推理）任务上的能力，并提出GlobalRAG方法，显著提升了全局任务表现。


<details>
  <summary>Details</summary>
Motivation: 目前RAG方法大多关注于检索文档中局部相关内容，难以应对实际应用中需要全局信息整合与分析的需求，例如统计型、排序型等全局问题。

Method: 作者提出了GlobalQA数据集，涵盖计数、极值、排序、top-k提取等任务，并系统评测现有RAG模型。同时，设计了GlobalRAG框架，通过分块检索、LLM驱动筛选、聚合模块等多步协作，提升全局推理的准确性。

Result: 实验证明，现有RAG方法在全局任务上的表现很差，最强基线F1仅1.51。GlobalRAG方法在Qwen2.5-14B上的F1达到6.63，显著超过基线。

Conclusion: 传统RAG方案难以应对全局跨文档任务，GlobalRAG显著改进了大模型在此类任务上的能力，并为研究提供了新的评测标准和技术路线。

Abstract: Retrieval-augmented generation (RAG) has emerged as a leading approach to
reducing hallucinations in large language models (LLMs). Current RAG evaluation
benchmarks primarily focus on what we call local RAG: retrieving relevant
chunks from a small subset of documents to answer queries that require only
localized understanding within specific text chunks. However, many real-world
applications require a fundamentally different capability -- global RAG --
which involves aggregating and analyzing information across entire document
collections to derive corpus-level insights (for example, "What are the top 10
most cited papers in 2023?"). In this paper, we introduce GlobalQA -- the first
benchmark specifically designed to evaluate global RAG capabilities, covering
four core task types: counting, extremum queries, sorting, and top-k
extraction. Through systematic evaluation across different models and
baselines, we find that existing RAG methods perform poorly on global tasks,
with the strongest baseline achieving only 1.51 F1 score. To address these
challenges, we propose GlobalRAG, a multi-tool collaborative framework that
preserves structural coherence through chunk-level retrieval, incorporates
LLM-driven intelligent filters to eliminate noisy documents, and integrates
aggregation modules for precise symbolic computation. On the Qwen2.5-14B model,
GlobalRAG achieves 6.63 F1 compared to the strongest baseline's 1.51 F1,
validating the effectiveness of our method.

</details>


### [102] [Pragmatic Theories Enhance Understanding of Implied Meanings in LLMs](https://arxiv.org/abs/2510.26253)
*Takuma Sato,Seiya Kawano,Koichiro Yoshino*

Main category: cs.CL

TL;DR: 本研究提出利用语用学理论作为提示词，提升语言模型理解隐含意义的能力，实验表明该方法比常规推理提升显著。


<details>
  <summary>Details</summary>
Motivation: 语言模型在理解隐含意义（语用推断）方面还不够理想，而这对于人与人之间的高效沟通至关重要，因此需要探索如何提升模型在此类任务中的表现。

Method: 采用以Grice语用学、关联理论等语用学理论概述作为提示词，引导语言模型逐步推理理解隐含意义。将该方法与传统的无理论解释推理进行实验对比。

Result: 在语用推理任务中，引入语用学理论提示词的方法比0-shot CoT基线最高提升9.6%；即使仅提及理论名称，也可带来1-3%的提升（大型模型）。

Conclusion: 在模型推理时结合语用学理论能有效提升其隐含意义理解能力，建议未来相关任务设计考虑将相关理论知识融入提示词。

Abstract: The ability to accurately interpret implied meanings plays a crucial role in
human communication and language use, and language models are also expected to
possess this capability. This study demonstrates that providing language models
with pragmatic theories as prompts is an effective in-context learning approach
for tasks to understand implied meanings. Specifically, we propose an approach
in which an overview of pragmatic theories, such as Gricean pragmatics and
Relevance Theory, is presented as a prompt to the language model, guiding it
through a step-by-step reasoning process to derive a final interpretation.
Experimental results showed that, compared to the baseline, which prompts
intermediate reasoning without presenting pragmatic theories (0-shot
Chain-of-Thought), our methods enabled language models to achieve up to 9.6\%
higher scores on pragmatic reasoning tasks. Furthermore, we show that even
without explaining the details of pragmatic theories, merely mentioning their
names in the prompt leads to a certain performance improvement (around 1-3%) in
larger models compared to the baseline.

</details>


### [103] [Language Models Are Borrowing-Blind: A Multilingual Evaluation of Loanword Identification across 10 Languages](https://arxiv.org/abs/2510.26254)
*Mérilin Sousa Silva,Sina Ahmadi*

Main category: cs.CL

TL;DR: 该论文研究了主流语言模型在识别外来词方面的能力，发现其在区分外来词和本土词汇上表现较差，这对少数民族语言的NLP工具开发和保护有重要影响。


<details>
  <summary>Details</summary>
Motivation: 由于历史上词汇在语言间的借用很常见，且外来词和本土词的区分对语言保护尤其重要，作者希望了解当前主流预训练语言模型，特别是大语言模型，是否具备识别外来词的能力，这是推动少数民族语言NLP应用和保护的关键。

Method: 作者选择了多种预训练语言模型和大语言模型，针对10种不同语言，要求模型区分外来词和母语词汇，并在有明确指令和语境下系统性评估其识别能力。

Result: 实验结果显示，无论给出怎样的指令和语境提示，当前主流语言模型普遍难以有效区分外来词和本土词。此外，模型普遍对外来词表现出较高的接受性，存在偏向外来词的现象。

Conclusion: 现有NLP系统在处理外来词识别方面存在显著短板，特别是在支持少数民族语言和受主导语种影响大的语言环境中。这一发现提示后续在开发相关NLP工具时应特别关注对本族语的支持，以促进语言保护和多样性。

Abstract: Throughout language history, words are borrowed from one language to another
and gradually become integrated into the recipient's lexicon. Speakers can
often differentiate these loanwords from native vocabulary, particularly in
bilingual communities where a dominant language continuously imposes lexical
items on a minority language. This paper investigates whether pretrained
language models, including large language models, possess similar capabilities
for loanword identification. We evaluate multiple models across 10 languages.
Despite explicit instructions and contextual information, our results show that
models perform poorly in distinguishing loanwords from native ones. These
findings corroborate previous evidence that modern NLP systems exhibit a bias
toward loanwords rather than native equivalents. Our work has implications for
developing NLP tools for minority languages and supporting language
preservation in communities under lexical pressure from dominant languages.

</details>


### [104] [Distilling Multilingual Vision-Language Models: When Smaller Models Stay Multilingual](https://arxiv.org/abs/2510.26271)
*Sukrit Sriratanawilai,Jhayahgrit Thongwat,Romrawin Chumpu,Patomporn Payoungkhamdee,Sarana Nutanong,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: 本文研究了在视觉-语言模型(VLMs)压缩过程中，不同知识蒸馏(KD)方法对多语言一致性和下游任务稳定性的影响，揭示了模型设计中的权衡。


<details>
  <summary>Details</summary>
Motivation: 目前多语言视觉-语言模型在不同语言间表现不均匀，且压缩模型体积后问题变严重。知识蒸馏虽能传递大模型知识到小模型，但其在多语言场景下应用较少。因此，了解KD方法在模型压缩下对多语言表现的影响很重要。

Method: 作者以CLIP和SigLIP2为基础，选取五种典型的知识蒸馏方式，通过严格对比实验，评估在模型尺寸减半情况下，不同KD方法对跨语言表达一致性和下游任务（领域内检索与视觉问答）性能稳定性的影响。

Result: 部分知识蒸馏配置能在模型显著压缩后保持甚至提升多语言检索鲁棒性，但有些配置跨任务稳定性差，在某些任务上表现退化，表明蒸馏方法设计对整体表现有重要影响，且这种影响无法单纯通过总准确率反映。

Conclusion: 论文强调多语言模型压缩时，知识蒸馏方式的选择至关重要，应结合跨语言一致性和多任务稳定性综合权衡，不能仅看总体准确率。

Abstract: Vision-language models (VLMs) exhibit uneven performance across languages, a
problem that is often exacerbated when the model size is reduced. While
Knowledge distillation (KD) demonstrates promising results in transferring
knowledge from larger to smaller VLMs, applying KD in multilingualism is an
underexplored area. This paper presents a controlled empirical study of KD
behavior across five distillation approaches, isolating their effects on
cross-lingual representation consistency and downstream performance stability
under model compression. We study five distillation formulations across CLIP
and SigLIP2, and evaluate them on in-domain retrieval and out-of-domain visual
QA. We find that some configurations preserve or even improve multilingual
retrieval robustness despite halving model size, but others fail to maintain
cross-task stability, exposing design-sensitive trade-offs that aggregate
accuracy alone does not reveal.

</details>


### [105] [Do LLMs Signal When They're Right? Evidence from Neuron Agreement](https://arxiv.org/abs/2510.26277)
*Kang Chen,Yaoning Wang,Kai Xiong,Zhuoka Feng,Wenhe Sun,Haotian Chen,Yixin Cao*

Main category: cs.CL

TL;DR: 本文提出了一种基于神经元激活信号的语言模型解码方法（NAD），可提升推理表现并极大降低算力消耗。


<details>
  <summary>Details</summary>
Motivation: 以往提升类语言模型推理性能的集成策略主要依赖输出层信号（如概率、熵、自评），但这些信号在模型微调后往往不可靠。作者希望探索模型内部更本质的神经元激活信号以改进解码。

Method: 作者通过分析模型内部神经元激活行为，发现正确答案激活更少神经元、且多样输出中一致性更高。基于这些现象，提出Neuron Agreement Decoding（NAD）：利用激活稀疏性和跨样本一致性筛选候选答案，只依赖内部信号并可大幅提前终止无效解码。

Result: 在数学、科学等有标准答案的任务上，NAD与多数投票表现相当；在开放式代码任务（多数投票不可用）上，NAD优于业界基线Avg@64。NAD可提前大幅剪枝，无明显影响下减少99%算力消耗。

Conclusion: 神经元激活信号能够有效指导大模型的无标签集成推理解码，既高效又可靠，具有推广与实际部署价值。

Abstract: Large language models (LLMs) commonly boost reasoning via
sample-evaluate-ensemble decoders, achieving label free gains without ground
truth. However, prevailing strategies score candidates using only external
outputs such as token probabilities, entropies, or self evaluations, and these
signals can be poorly calibrated after post training. We instead analyze
internal behavior based on neuron activations and uncover three findings: (1)
external signals are low dimensional projections of richer internal dynamics;
(2) correct responses activate substantially fewer unique neurons than
incorrect ones throughout generation; and (3) activations from correct
responses exhibit stronger cross sample agreement, whereas incorrect ones
diverge. Motivated by these observations, we propose Neuron Agreement Decoding
(NAD), an unsupervised best-of-N method that selects candidates using
activation sparsity and cross sample neuron agreement, operating solely on
internal signals and without requiring comparable textual outputs. NAD enables
early correctness prediction within the first 32 generated tokens and supports
aggressive early stopping. Across math and science benchmarks with verifiable
answers, NAD matches majority voting; on open ended coding benchmarks where
majority voting is inapplicable, NAD consistently outperforms Avg@64. By
pruning unpromising trajectories early, NAD reduces token usage by 99% with
minimal loss in generation quality, showing that internal signals provide
reliable, scalable, and efficient guidance for label free ensemble decoding.

</details>


### [106] [Unravelling the Mechanisms of Manipulating Numbers in Language Models](https://arxiv.org/abs/2510.26285)
*Michal Štefánik,Timothee Mickus,Marek Kadlčík,Bertram Højer,Michal Spiegel,Raúl Vázquez,Aman Sinha,Josef Kuchař,Philipp Mondorf*

Main category: cs.CL

TL;DR: 本论文研究了大型语言模型（LLM）对数字的表示和操作机制，发现不同模型间对数字有高度一致且准确的输入嵌入表达，但在输出时仍常有数值错误。作者进一步探究了产生这些错误的深层原因。


<details>
  <summary>Details</summary>
Motivation: 以往研究显示，不同LLM能生成准确且相似的数字输入Embedding，但它们处理数字时在输出阶段却经常出错。作者希望解释这一矛盾，并深入理解LLM内数字处理机制。

Method: 本文分析了多个LLM在不同上下文和隐藏层状态下的数字表示方式，并构建了可通用的探针，用于追踪信息流和定位产生错误的模型层级。

Result: 研究发现，不同LLM对数字的内部表达高度一致且可互换，这种表达方式具有系统性、高准确性和普遍性。作者开发的通用探针能够追踪信息流动及错误来源层。

Conclusion: 论文为预训练LLM数字操作机制提供了基础性理解，并提出改进模型结构和探测数字处理准确性的潜力方向。

Abstract: Recent work has shown that different large language models (LLMs) converge to
similar and accurate input embedding representations for numbers. These
findings conflict with the documented propensity of LLMs to produce erroneous
outputs when dealing with numeric information. In this work, we aim to explain
this conflict by exploring how language models manipulate numbers and quantify
the lower bounds of accuracy of these mechanisms. We find that despite
surfacing errors, different language models learn interchangeable
representations of numbers that are systematic, highly accurate and universal
across their hidden states and the types of input contexts. This allows us to
create universal probes for each LLM and to trace information -- including the
causes of output errors -- to specific layers. Our results lay a fundamental
understanding of how pre-trained LLMs manipulate numbers and outline the
potential of more accurate probing techniques in addressed refinements of LLMs'
architectures.

</details>


### [107] [Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games](https://arxiv.org/abs/2510.26298)
*Jingran Zhang,Ning Li,Justin Cui*

Main category: cs.CL

TL;DR: 本论文评估了ChatGPT Atlas在浏览器游戏中的交互性能，发现其逻辑推理表现优秀，但在实时动态游戏上表现不足。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注ChatGPT Atlas的信息检索能力，其在动态互动环境中的表现尚不清楚。通过这一评估，旨在明确其在实际Web交互场景下的能力和局限。

Method: 作者选取了Google T-Rex Runner、数独、Flappy Bird和Stein.world等浏览器游戏作为评测环境，通过统计游戏分数等量化指标，比较了Atlas在不同任务类型下的表现。

Result: Atlas在数独等逻辑推理任务上表现优异，完成速度明显快于人类，但在如T-Rex Runner和Flappy Bird等需要精确时机和动态操作的游戏中表现很差，常常无法通过初始障碍。

Conclusion: ChatGPT Atlas具备较强分析和推理能力，但在动态、实时Web交互上存在明显局限，难以胜任需要快速和准确实时反馈的任务。

Abstract: OpenAI's ChatGPT Atlas introduces new capabilities for web interaction,
enabling the model to analyze webpages, process user intents, and execute
cursor and keyboard inputs directly within the browser. While its capacity for
information retrieval tasks has been demonstrated, its performance in dynamic,
interactive environments remains less explored. In this study, we conduct an
early evaluation of Atlas's web interaction capabilities using browser-based
games as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird,
and Stein.world. We employ in-game performance scores as quantitative metrics
to assess performance across different task types. Our results show that Atlas
performs strongly in logical reasoning tasks like Sudoku, completing puzzles
significantly faster than human baselines, but struggles substantially in
real-time games requiring precise timing and motor control, often failing to
progress beyond initial obstacles. These findings suggest that while Atlas
demonstrates capable analytical processing, there remain notable limitations in
dynamic web environments requiring real-time interaction. The website of our
project can be found at https://atlas-game-eval.github.io.

</details>


### [108] [SCRIBE: Structured Chain Reasoning for Interactive Behaviour Explanations using Tool Calling](https://arxiv.org/abs/2510.26322)
*Fares Fawzi,Vinitra Swamy,Dominik Glandorf,Tanya Nazaretsky,Tanja Käser*

Main category: cs.CL

TL;DR: 本文提出了SCRIBE框架，通过多步推理和工具增强，为教育领域中的学生评语反馈生成高质量、个性化回复。实验显示，小参数模型也能达到大型模型的效果，适用于对隐私和算力有较高要求的实际应用。


<details>
  <summary>Details</summary>
Motivation: 现实教育场景对AI反馈需兼顾隐私、低算力和响应质量，现有大模型难以满足全部需求，亟需小型、开源、可本地部署的解决方案。

Method: SCRIBE框架结合多跳推理与领域特定工具，具备自反推理流程，支持迭代推理、工具调用和纠错。采用二阶段LoRA精调，初步基于GPT-4o生成数据，训练出3B和8B小模型。

Result: 评测显示，8B-SCRIBE模型在相关性、可操作性等核心指标上与更大规模模型（如GPT-4o、Llama3 70B）性能相当甚至更优。用户调研也显示学生给予与顶尖模型类似的认可度。

Conclusion: SCRIBE能够以低资源、强隐私的优势，在实际教育场景下提供高质量、可信赖的学生反馈，验证了小模型在该领域的应用潜力。

Abstract: Language models can be used to provide interactive, personalized student
feedback in educational settings. However, real-world deployment faces three
key challenges: privacy concerns, limited computational resources, and the need
for pedagogically valid responses. These constraints require small, open-source
models that can run locally and reliably ground their outputs in correct
information. We introduce SCRIBE, a framework for multi-hop, tool-augmented
reasoning designed to generate valid responses to student questions about
feedback reports. SCRIBE combines domain-specific tools with a self-reflective
inference pipeline that supports iterative reasoning, tool use, and error
recovery. We distil these capabilities into 3B and 8B models via two-stage LoRA
fine-tuning on synthetic GPT-4o-generated data. Evaluation with a human-aligned
GPT-Judge and a user study with 108 students shows that 8B-SCRIBE models
achieve comparable or superior quality to much larger models in key dimensions
such as relevance and actionability, while being perceived on par with GPT-4o
and Llama-3.3 70B by students. These findings demonstrate the viability of
SCRIBE for low-resource, privacy-sensitive educational applications.

</details>


### [109] [From Amateur to Master: Infusing Knowledge into LLMs via Automated Curriculum Learning](https://arxiv.org/abs/2510.26336)
*Nishit Neema,Srinjoy Mukherjee,Sapan Shah,Gokul Ramakrishnan,Ganesh Venkatesh*

Main category: cs.CL

TL;DR: 本文提出一种名为ACER的教学法，用于将通用大语言模型转变为各领域专家，提高其在经济学、心理学等专业领域的能力，同时保持广泛任务能力。不仅提升了目标领域的表现，还带来知识正迁移。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽然在一般任务上表现优异，但在像经济学、心理学等需要深层、系统知识的专业领域常常表现不佳，需要一种既能增强专业能力又不损失通用性的训练方式。

Method: 提出ACER方法：首先自动生成以教科书为蓝本的课程大纲，然后基于该大纲和Bloom认知目标分级，系统生成从易到难的问题-答案对，形成合成语料，并采用交错式课程预训练策略对模型持续训练，保证知识深度和系统性。

Result: 在Llama 3.2（1B和3B）模型上，ACER在MMLU的专业子集成绩有显著提升，如微观经济学领域准确率提升5个百分点，所有领域平均提升3个百分点，非目标领域依然正向提升0.7分。知识密集型基准测试（如ARC和GPQA）成绩提升2分以上;通用推理能力保持稳定。

Conclusion: ACER方法能大幅弥合大语言模型在专业领域与通用领域的表现差距，提升专业任务表现，同时保持甚至促进跨领域知识迁移，是拓展LLM专业性的可扩展有效方案。

Abstract: Large Language Models (LLMs) excel at general tasks but underperform in
specialized domains like economics and psychology, which require deep,
principled understanding. To address this, we introduce ACER (Automated
Curriculum-Enhanced Regimen) that transforms generalist models into domain
experts without sacrificing their broad capabilities. ACER first synthesizes a
comprehensive, textbook-style curriculum by generating a table of contents for
a subject and then creating question-answer (QA) pairs guided by Bloom's
taxonomy. This ensures systematic topic coverage and progressively increasing
difficulty. The resulting synthetic corpus is used for continual pretraining
with an interleaved curriculum schedule, aligning learning across both content
and cognitive dimensions.
  Experiments with Llama 3.2 (1B and 3B) show significant gains in specialized
MMLU subsets. In challenging domains like microeconomics, where baselines
struggle, ACER boosts accuracy by 5 percentage points. Across all target
domains, we observe a consistent macro-average improvement of 3 percentage
points. Notably, ACER not only prevents catastrophic forgetting but also
facilitates positive cross-domain knowledge transfer, improving performance on
non-target domains by 0.7 points. Beyond MMLU, ACER enhances performance on
knowledge-intensive benchmarks like ARC and GPQA by over 2 absolute points,
while maintaining stable performance on general reasoning tasks. Our results
demonstrate that ACER offers a scalable and effective recipe for closing
critical domain gaps in LLMs.

</details>


### [110] [MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data](https://arxiv.org/abs/2510.26345)
*Mykhailo Poliakov,Nadiya Shvai*

Main category: cs.CL

TL;DR: 本文提出MisSynth流程，利用合成数据微调大模型，有效提升对科学虚假信息识别能力。


<details>
  <summary>Details</summary>
Motivation: 健康相关的虚假信息泛滥，传统识别方法难以应对尤其那些歪曲科学事实的谬误。数据标注成本高，训练资源有限，提升大模型识别虚假论证能力成为迫切需求。

Method: 提出MisSynth流程，使用RAG技术生成合成谬误样本，利用这些样本微调现有大模型（如LLaMA 3.1 8B），并在MISSCI数据集框架下测试。

Result: 微调后模型准确率显著提升，例如LLaMA 3.1 8B的F1分数比原始基线提升35%以上。合成数据增强提升了模型在科学虚假信息分类任务上的表现，尤其是在资源有限情况下。

Conclusion: 通过引入合成谬误数据微调LLMs，可以大幅提升模型零样本科学虚假信息识别能力，为解决标注数据稀缺和计算资源有限的瓶颈提供了有效途径。

Abstract: Health-related misinformation is very prevalent and potentially harmful. It
is difficult to identify, especially when claims distort or misinterpret
scientific findings. We investigate the impact of synthetic data generation and
lightweight fine-tuning techniques on the ability of large language models
(LLMs) to recognize fallacious arguments using the MISSCI dataset and
framework. In this work, we propose MisSynth, a pipeline that applies
retrieval-augmented generation (RAG) to produce synthetic fallacy samples,
which are then used to fine-tune an LLM model. Our results show substantial
accuracy gains with fine-tuned models compared to vanilla baselines. For
instance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score
absolute improvement on the MISSCI test split over its vanilla baseline. We
demonstrate that introducing synthetic fallacy data to augment limited
annotated resources can significantly enhance zero-shot LLM classification
performance on real-world scientific misinformation tasks, even with limited
computational resources. The code and synthetic dataset are available on
https://github.com/mxpoliakov/MisSynth.

</details>


### [111] [The Geometry of Dialogue: Graphing Language Models to Reveal Synergistic Teams for Multi-Agent Collaboration](https://arxiv.org/abs/2510.26352)
*Kotaro Furuya,Yuichi Kitagawa*

Main category: cs.CL

TL;DR: 本文提出了一种基于交互的框架，实现无需先验知识下的大型语言模型（LLM）团队自动组合，通过语言模型对话生成语义关联图，发现协作最优团队，实验结果超越随机团队，与人工精心组队效果相当。


<details>
  <summary>Details</summary>
Motivation: 多智能体（Multi-Agent）基于大语言模型有可能超过单一模型的能力，但组建最佳团队受限于对模型内部特性的认知不足，因此亟需无须先验知识的团队组建方法。

Method: 提出基于交互的框架：通过模型间配对对话的语义一致性构建关系图，再采用社区检测找出潜在的最佳团队组合，整个过程无需模型内部结构、训练数据或任务表现等先验知识。

Result: 对多种LLM实验显示，该方法能够发现功能一致且具有协同效应的模型组，针对特定话题组成的团队，在后续任务中性能优于随机基线，与基于已知专业化手动组队的结果相当。

Conclusion: 本研究为协作型多智能体LLM团队自动设计提供了新思路，能够自动发现最优组队，拓展了多模型协同工作的应用前景。

Abstract: While a multi-agent approach based on large language models (LLMs) represents
a promising strategy to surpass the capabilities of single models, its success
is critically dependent on synergistic team composition. However, forming
optimal teams is a significant challenge, as the inherent opacity of most
models obscures the internal characteristics necessary for effective
collaboration. In this paper, we propose an interaction-centric framework for
automatic team composition that does not require any prior knowledge including
their internal architectures, training data, or task performances. Our method
constructs a "language model graph" that maps relationships between models from
the semantic coherence of pairwise conversations, and then applies community
detection to identify synergistic model clusters. Our experiments with diverse
LLMs demonstrate that the proposed method discovers functionally coherent
groups that reflect their latent specializations. Priming conversations with
specific topics identified synergistic teams which outperform random baselines
on downstream benchmarks and achieve comparable accuracy to that of
manually-curated teams based on known model specializations. Our findings
provide a new basis for the automated design of collaborative multi-agent LLM
teams.

</details>


### [112] [On the Role of Context for Discourse Relation Classification in Scientific Writing](https://arxiv.org/abs/2510.26354)
*Stephen Wan,Wei Liu,Michael Strube*

Main category: cs.CL

TL;DR: 本文探索了在科学写作中利用话语结构信息提升AI生成科学论断的证据支持，重点研究了预训练语言模型和大语言模型在科学论文的话语关系分类任务中的表现。实验发现，基于话语结构的上下文信息有助于改进分类效果，并分析了哪些话语关系类型受益最大。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在科学研究流程中的广泛应用，科学声明的可信性验证变得愈发重要。作者希望通过理解科学文本中的话语结构，从而为AI生成的科学声明寻找专家级的支持证据。这一研究的首要任务就是在科学写作中推断和分类话语关系。

Method: 本研究选取科学论文作为话语关系分类任务的目标文本，采用了预训练语言模型（PLM）和大语言模型（LLM）的方法评估其在该领域的表现。重点分析了加入基于话语结构定义的上下文对分类任务的提升作用。

Result: 实验证明，包括基于话语结构的上下文会普遍提升话语关系分类的表现。作者还进一步分析了具体哪些科学话语关系类型受益于上下文增强。

Conclusion: 上下文信息，尤其是基于话语结构的信息，对提升科学论文中话语关系分类有重要作用。这为后续在科学AI生成声明的证据支持方向打下了基础。

Abstract: With the increasing use of generative Artificial Intelligence (AI) methods to
support science workflows, we are interested in the use of discourse-level
information to find supporting evidence for AI generated scientific claims. A
first step towards this objective is to examine the task of inferring discourse
structure in scientific writing.
  In this work, we present a preliminary investigation of pretrained language
model (PLM) and Large Language Model (LLM) approaches for Discourse Relation
Classification (DRC), focusing on scientific publications, an under-studied
genre for this task. We examine how context can help with the DRC task, with
our experiments showing that context, as defined by discourse structure, is
generally helpful. We also present an analysis of which scientific discourse
relation types might benefit most from context.

</details>


### [113] [OmniEduBench: A Comprehensive Chinese Benchmark for Evaluating Large Language Models in Education](https://arxiv.org/abs/2510.26422)
*Min Zhang,Hao Chen,Hao Chen,Wenqi Zhang,Didi Zhu,Xin Lin,Bo Jiang,Aimin Zhou,Fei Wu,Kun Kuang*

Main category: cs.CL

TL;DR: 本文提出了一个面向中文教育领域的新基准OmniEduBench，全面评估大语言模型（LLMs）在知识和能力素养两大维度的表现，并揭示了当前模型在能力素养方面与人类智能的显著差距。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型及其评价体系主要关注知识维度，忽视了现实教育中同样重要的能力素养评估，同时基准数据集覆盖面窄，缺乏多样性，尤其在中文语境下问题更为突出。为填补这一空白，作者设计了更全面、细致的中文教育基准。

Method: 构建了OmniEduBench数据集，包含24,602条高质量问答对，覆盖知识和能力素养两大核心维度，各自细分6类，共涉61门学科，并包含11种常见考试题型。作者采用该数据对11个主流的开源和闭源大模型开展了系统实验和分析。

Result: 在知识维度上，仅Gemini-2.5 Pro模型准确率超过60%；在能力素养维度，表现最佳的QWQ模型与人类智能相比仍有近30%的差距，显示现有模型在复杂素养能力方面表现较弱。

Conclusion: OmniEduBench填补了中文教育基准的空白，实验结果揭示现有LLMs在知识和素养能力方面的显著提升空间，尤其在能力素养维度，表明将LLMs应用于真实教育场景仍面临诸多挑战。

Abstract: With the rapid development of large language models (LLMs), various LLM-based
works have been widely applied in educational fields. However, most existing
LLMs and their benchmarks focus primarily on the knowledge dimension, largely
neglecting the evaluation of cultivation capabilities that are essential for
real-world educational scenarios. Additionally, current benchmarks are often
limited to a single subject or question type, lacking sufficient diversity.
This issue is particularly prominent within the Chinese context. To address
this gap, we introduce OmniEduBench, a comprehensive Chinese educational
benchmark. OmniEduBench consists of 24.602K high-quality question-answer pairs.
The data is meticulously divided into two core dimensions: the knowledge
dimension and the cultivation dimension, which contain 18.121K and 6.481K
entries, respectively. Each dimension is further subdivided into 6 fine-grained
categories, covering a total of 61 different subjects (41 in the knowledge and
20 in the cultivation). Furthermore, the dataset features a rich variety of
question formats, including 11 common exam question types, providing a solid
foundation for comprehensively evaluating LLMs' capabilities in education.
Extensive experiments on 11 mainstream open-source and closed-source LLMs
reveal a clear performance gap. In the knowledge dimension, only Gemini-2.5 Pro
surpassed 60\% accuracy, while in the cultivation dimension, the
best-performing model, QWQ, still trailed human intelligence by nearly 30\%.
These results highlight the substantial room for improvement and underscore the
challenges of applying LLMs in education.

</details>


### [114] [1+1>2: A Synergistic Sparse and Low-Rank Compression Method for Large Language Models](https://arxiv.org/abs/2510.26446)
*Zeliang Zong,Kai Zhang,Zheyang Li,Wenming Tan,Ye Ren,Yiyan Zhai,Jilin Hu*

Main category: cs.CL

TL;DR: 本文提出了一种用于大型语言模型（LLM）的联合稀疏与低秩压缩方法（SSLC），能显著降低模型体积和计算需求，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 虽然大模型表现出强大的语言理解和生成能力，但其高带宽和计算需求限制了实际应用。以往的剪枝（稀疏化）和低秩逼近技术各自取得一定成效，但两者结合用于LLM的潜力尚未被充分挖掘。本文旨在探索其协同作用，寻找更高效的模型压缩方式。

Method: 作者将稀疏化与低秩逼近理论上统一建模为一个优化问题，通过迭代优化算法联合求解。低秩逼近保持模型结构并减少信息损失，稀疏优化则进一步去除非关键参数，仅保留对泛化性至关重要的部分。

Result: 在LLaMA和Qwen2.5（7B-70B参数）模型上实验，SSLC无须任何额外训练步骤，相比单独方法取得更优压缩与加速结果，达到当前最新水平。实验显示，SSLC可将Qwen2.5压缩50%且无性能损失，并带来至少1.63倍推理加速。

Conclusion: SSLC方法实现了大模型的高效压缩和加速，为LLM的广泛落地提供了实用方案，优于单一剪枝或低秩逼近方法。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in
language comprehension and generation; however, their widespread adoption is
constrained by substantial bandwidth and computational demands. While pruning
and low-rank approximation have each demonstrated promising performance
individually, their synergy for LLMs remains underexplored. We introduce
\underline{S}ynergistic \underline{S}parse and \underline{L}ow-Rank
\underline{C}ompression (SSLC) methods for LLMs, which leverages the strengths
of both techniques: low-rank approximation compresses the model by retaining
its essential structure with minimal information loss, whereas sparse
optimization eliminates non-essential weights, preserving those crucial for
generalization. Based on theoretical analysis, we first formulate the low-rank
approximation and sparse optimization as a unified problem and solve it by
iterative optimization algorithm. Experiments on LLaMA and Qwen2.5 models
(7B-70B) show that SSLC, without any additional training steps, consistently
surpasses standalone methods, achieving state-of-the-arts results. Notably,
SSLC compresses Qwen2.5 by 50\% with no performance drop and achieves at least
1.63$\times$ speedup, offering a practical solution for efficient LLM
deployment.

</details>


### [115] [Bayesian Network Fusion of Large Language Models for Sentiment Analysis](https://arxiv.org/abs/2510.26484)
*Rasoul Amirzadeh,Dhananjay Thiruvady,Fatemeh Shiri*

Main category: cs.CL

TL;DR: 提出了一种贝叶斯网络语言模型融合（BNLF）框架，用于将多个LLM预测以概率方式融合实现更好的情感分析，并在金融领域数据上验证效果。


<details>
  <summary>Details</summary>
Motivation: 当前多种领域特定的大型语言模型存在透明度低、可解释性差、微调成本高、提示工程复杂、跨领域结果不一致、资源消耗大等问题。针对这些挑战，作者希望实现更高效、可解释以及表现鲁棒的情感分析方法。

Method: 作者设计了BNLF框架，将三个LLM（FinBERT、RoBERTa、BERTweet）的情感预测结果作为贝叶斯网络的概率节点，通过概率机制实现“late fusion”效果，用于情感分类。

Result: 在三个由人工标注、具有不同语言和上下文特征的金融语料库上测试，BNLF的准确率比单一LLM基线提高大约6%，表现出良好的一致性和数据集适应性。

Conclusion: BNLF框架不仅提升了情感分析准确率，还通过概率融合机制增强了方法的可解释性和跨数据集的鲁棒性，说明该方法在专用领域具备实际应用前景。

Abstract: Large language models (LLMs) continue to advance, with an increasing number
of domain-specific variants tailored for specialised tasks. However, these
models often lack transparency and explainability, can be costly to fine-tune,
require substantial prompt engineering, yield inconsistent results across
domains, and impose significant adverse environmental impact due to their high
computational demands. To address these challenges, we propose the Bayesian
network LLM fusion (BNLF) framework, which integrates predictions from three
LLMs, including FinBERT, RoBERTa, and BERTweet, through a probabilistic
mechanism for sentiment analysis. BNLF performs late fusion by modelling the
sentiment predictions from multiple LLMs as probabilistic nodes within a
Bayesian network. Evaluated across three human-annotated financial corpora with
distinct linguistic and contextual characteristics, BNLF demonstrates
consistent gains of about six percent in accuracy over the baseline LLMs,
underscoring its robustness to dataset variability and the effectiveness of
probabilistic fusion for interpretable sentiment classification.

</details>


### [116] [A Multi-agent Large Language Model Framework to Automatically Assess Performance of a Clinical AI Triage Tool](https://arxiv.org/abs/2510.26498)
*Adam E. Flanders,Yifan Peng,Luciano Prevedello,Robyn Ball,Errol Colak,Prahlad Menon,George Shih,Hui-Ming Lin,Paras Lakhani*

Main category: cs.CL

TL;DR: 本研究探索了多个大型语言模型（LLM）组成的集成体，能否比单一LLM更可靠地评估像素级AI分诊工具的效果。结果显示，集成LLM方法在一致性和可靠性上优于单一模型。


<details>
  <summary>Details</summary>
Motivation: 在实际临床应用中，如何准确、客观地对AI医疗影像工具进行效果评估是个难题。传统依赖单一人工或单一LLM存在主观性和波动性，作者试图探索通过多个LLM组成的集成模型，提升评估的一致性和准确性。

Method: 研究使用29766份无增强头颅CT，与商业ICH（颅内出血）AI检测工具结果对比，利用8种开源LLM和一个内部GPT-4o版本，对放射学报告多轮评估，评定有无ICH，部分样本由人工复核。比较单一模型与多模型集成、不同集成策略的评估效能。

Result: Llama3.3:70b和GPT-4o表现最佳（AUC最高）。Llama3.3:70b表现各项指标优异（召回、F1、特异度等）。理想LLM集成（包括9模型、Top-3、共识法）其一致性优于单一GPT-4o，差异无统计学意义（p>0.05）。

Conclusion: 中至大型开源LLM集成体可以更一致、可靠地为临床AI分诊工具建立评估基准，相较单个LLM优势明显。

Abstract: Purpose: The purpose of this study was to determine if an ensemble of
multiple LLM agents could be used collectively to provide a more reliable
assessment of a pixel-based AI triage tool than a single LLM.
  Methods: 29,766 non-contrast CT head exams from fourteen hospitals were
processed by a commercial intracranial hemorrhage (ICH) AI detection tool.
Radiology reports were analyzed by an ensemble of eight open-source LLM models
and a HIPAA compliant internal version of GPT-4o using a single multi-shot
prompt that assessed for presence of ICH. 1,726 examples were manually
reviewed. Performance characteristics of the eight open-source models and
consensus were compared to GPT-4o. Three ideal consensus LLM ensembles were
tested for rating the performance of the triage tool.
  Results: The cohort consisted of 29,766 head CTs exam-report pairs. The
highest AUC performance was achieved with llama3.3:70b and GPT-4o (AUC= 0.78).
The average precision was highest for Llama3.3:70b and GPT-4o (AP=0.75 & 0.76).
Llama3.3:70b had the highest F1 score (0.81) and recall (0.85), greater
precision (0.78), specificity (0.72), and MCC (0.57). Using MCC (95% CI) the
ideal combination of LLMs were: Full-9 Ensemble 0.571 (0.552-0.591), Top-3
Ensemble 0.558 (0.537-0.579), Consensus 0.556 (0.539-0.574), and GPT4o 0.522
(0.500-0.543). No statistically significant differences were observed between
Top-3, Full-9, and Consensus (p > 0.05).
  Conclusion: An ensemble of medium to large sized open-source LLMs provides a
more consistent and reliable method to derive a ground truth retrospective
evaluation of a clinical AI triage tool over a single LLM alone.

</details>


### [117] [Inside CORE-KG: Evaluating Structured Prompting and Coreference Resolution for Knowledge Graphs](https://arxiv.org/abs/2510.26512)
*Dipak Meher,Carlotta Domeniconi*

Main category: cs.CL

TL;DR: 本文系统分析了CORE-KG框架在法律文本知识图谱自动化构建中的两个关键组件，揭示了类型感知共指消解模块和结构化结构化提示对提升KG质量的显著作用。


<details>
  <summary>Details</summary>
Motivation: 面对愈发复杂且难以解析的人口走私犯罪网络，现有法律文档极为重要但结构不规范，造成知识自动提取困难。大模型虽有改进，但依旧面临节点重复和噪声信息问题。

Method: 提出了一种结构化LLM提示与类型感知共指消解模块相结合的新型知识图谱构建框架（CORE-KG），并通过消融实验定量评估两大组件对KG质量提升的贡献。

Result: 移除共指消解模块会导致节点重复增加28.32%，噪声节点提升4.32%；去除结构化提示则节点重复上升4.34%，噪声节点大幅增加73.33%。

Conclusion: 型感知共指消解和结构化提示对LLM驱动法律知识图谱建构均有显著正面影响，为复杂法律文本结构化抽取方案设计提供了实验依据。

Abstract: Human smuggling networks are increasingly adaptive and difficult to analyze.
Legal case documents offer critical insights but are often unstructured,
lexically dense, and filled with ambiguous or shifting references, which pose
significant challenges for automated knowledge graph (KG) construction. While
recent LLM-based approaches improve over static templates, they still generate
noisy, fragmented graphs with duplicate nodes due to the absence of guided
extraction and coreference resolution. The recently proposed CORE-KG framework
addresses these limitations by integrating a type-aware coreference module and
domain-guided structured prompts, significantly reducing node duplication and
legal noise. In this work, we present a systematic ablation study of CORE-KG to
quantify the individual contributions of its two key components. Our results
show that removing coreference resolution results in a 28.32% increase in node
duplication and a 4.32% increase in noisy nodes, while removing structured
prompts leads to a 4.34% increase in node duplication and a 73.33% increase in
noisy nodes. These findings offer empirical insights for designing robust
LLM-based pipelines for extracting structured representations from complex
legal texts.

</details>


### [118] [Hebrew Diacritics Restoration using Visual Representation](https://arxiv.org/abs/2510.26521)
*Yair Elboher,Yuval Pinter*

Main category: cs.CL

TL;DR: 本文提出了DIVRIT体系，它利用视觉语言模型将希伯来文去重音文本当作图片处理，从而提升了希伯来语重音符号还原（diacritics restoration）的精度。


<details>
  <summary>Details</summary>
Motivation: 希伯来语若无重音符号则极易产生多义和发音不准确，恢复重音符号对语言的正确读写具有重要意义。然而，现有的方法往往依赖繁琐的语言学特征或未能充分利用上下文信息。

Method: DIVRIT系统将重音还原视为零样本分类任务：对每个去重音单词，根据上下文，从动态候选集中选取最合适的重音模式。创新点在于将无重音文本作为图片输入，以视觉模型嵌入重音信息到向量表示中，无需复杂的语言学分析。

Result: 评估显示DIVRIT系统在无需复杂显式语言学分析的情况下，能够高效地执行重音还原。若正确选项包含于候选中，模型准确率很高。进一步通过结构和训练优化，提升了模型的泛化能力。

Conclusion: 实验表明，利用视觉表示的方法可显著提升希伯来语自动重音还原的准确性，展现了视觉模型在文本音标复原任务中的应用潜力。

Abstract: Diacritics restoration in Hebrew is a fundamental task for ensuring accurate
word pronunciation and disambiguating textual meaning. Despite the language's
high degree of ambiguity when unvocalized, recent machine learning approaches
have significantly advanced performance on this task.
  In this work, we present DIVRIT, a novel system for Hebrew diacritization
that frames the task as a zero-shot classification problem. Our approach
operates at the word level, selecting the most appropriate diacritization
pattern for each undiacritized word from a dynamically generated candidate set,
conditioned on the surrounding textual context. A key innovation of DIVRIT is
its use of a Hebrew Visual Language Model, which processes undiacritized text
as an image, allowing diacritic information to be embedded directly within the
input's vector representation.
  Through a comprehensive evaluation across various configurations, we
demonstrate that the system effectively performs diacritization without relying
on complex, explicit linguistic analysis. Notably, in an ``oracle'' setting
where the correct diacritized form is guaranteed to be among the provided
candidates, DIVRIT achieves a high level of accuracy. Furthermore, strategic
architectural enhancements and optimized training methodologies yield
significant improvements in the system's overall generalization capabilities.
These findings highlight the promising potential of visual representations for
accurate and automated Hebrew diacritization.

</details>


### [119] [The Structure of Relation Decoding Linear Operators in Large Language Models](https://arxiv.org/abs/2510.26543)
*Miranda Anna Christ,Adrián Csiszárik,Gergely Becsó,Dániel Varga*

Main category: cs.CL

TL;DR: 本文分析了Transformer语言模型中解码特定关系事实的线性算子的结构，并展示这些解码器可以高度压缩，且本质上基于属性而非具体关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究（Hernandez等人，2023）发现可以用线性算子从Transformer中解码出特定的关系事实，但只聚焦于单一关系，缺乏多个关系之间组织和冗余性理解。该文希望系统分析多种关系解码器的结构和内在联系，探索其可压缩性和泛化性。

Method: 将单一关系扩展到多个关系，系统梳理解码器的组织结构，并用简单的三阶张量网络对其进行压缩。提出交叉评估协议，将每个关系解码算子作用于其他关系的主体上，分析算子的内在语义属性。

Result: 实验证明，多关系解码器可以被简单张量网络高度压缩，且解码准确率几乎不下降。交叉评估显示，这些线性算子主要捕捉了反复出现的粗粒度语义属性（如都属于“country-of-X”），而不是独立的关系。

Conclusion: Transformer中关系解码的线性算子本质上是对常见属性的提取器，而不是每个关系独立的编码器，这解释了其高度可压缩性及泛化能力的局限性。

Abstract: This paper investigates the structure of linear operators introduced in
Hernandez et al. [2023] that decode specific relational facts in transformer
language models. We extend their single-relation findings to a collection of
relations and systematically chart their organization. We show that such
collections of relation decoders can be highly compressed by simple order-3
tensor networks without significant loss in decoding accuracy. To explain this
surprising redundancy, we develop a cross-evaluation protocol, in which we
apply each linear decoder operator to the subjects of every other relation. Our
results reveal that these linear maps do not encode distinct relations, but
extract recurring, coarse-grained semantic properties (e.g., country of capital
city and country of food are both in the country-of-X property). This
property-centric structure clarifies both the operators' compressibility and
highlights why they generalize only to new relations that are semantically
close. Our findings thus interpret linear relational decoding in transformer
language models as primarily property-based, rather than relation-specific.

</details>


### [120] [InfoFlow: Reinforcing Search Agent Via Reward Density Optimization](https://arxiv.org/abs/2510.26575)
*Kun Luo,Hongjin Qian,Zheng Liu,Ziyi Xia,Shitao Xiao,Siqi Bao,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 本论文提出了InfoFlow框架，通过分解子问题、失败引导和双智能体协作，优化深度强化学习中的奖励密度，有效提升探索效率，显著超越现有强化学习方法。


<details>
  <summary>Details</summary>
Motivation: 在深度强化学习（RLVR）中，智能体在深度搜索任务中由于奖励稀疏而探索成本极高，导致学习效率低下。解决奖励密度低的问题成为提高RLVR实际应用性的重要挑战。

Method: 作者将奖励密度优化问题形式化，并提出InfoFlow框架，从三方面提升奖励密度：（1）将长距离任务分解为子问题以获得更密集的过程奖励；（2）为失败的轨迹提供引导提示，提高成功率；（3）采用双智能体架构，由精炼智能体对搜索历史总结，降低总体探索成本。

Result: 在多个基准深度搜索任务上，InfoFlow明显优于强基线方法。即使是轻量级LLMs，结合InfoFlow后表现可比肩更先进的专有LLMs。

Conclusion: InfoFlow通过系统强化奖励传播和探索效率，解决了低奖励密度问题，为RLVR推向复杂深度搜索场景奠定了基础，对提升小型LLM能力具有实际意义。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a promising approach
for enhancing agentic deep search. However, its application is often hindered
by low \textbf{Reward Density} in deep search scenarios, where agents expend
significant exploratory costs for infrequent and often null final rewards. In
this paper, we formalize this challenge as the \textbf{Reward Density
Optimization} problem, which aims to improve the reward obtained per unit of
exploration cost. This paper introduce \textbf{InfoFlow}, a systematic
framework that tackles this problem from three aspects. 1) \textbf{Subproblem
decomposition}: breaking down long-range tasks to assign process rewards,
thereby providing denser learning signals. 2) \textbf{Failure-guided hints}:
injecting corrective guidance into stalled trajectories to increase the
probability of successful outcomes. 3) \textbf{Dual-agent refinement}:
employing a dual-agent architecture to offload the cognitive burden of deep
exploration. A refiner agent synthesizes the search history, which effectively
compresses the researcher's perceived trajectory, thereby reducing exploration
cost and increasing the overall reward density. We evaluate InfoFlow on
multiple agentic search benchmarks, where it significantly outperforms strong
baselines, enabling lightweight LLMs to achieve performance comparable to
advanced proprietary LLMs.

</details>


### [121] [Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference in Large Language Models](https://arxiv.org/abs/2510.26577)
*Yinrong Hong,Zhiquan Tan,Kai Hu*

Main category: cs.CL

TL;DR: 该论文提出了一种新的动态树型推理加速算法CAST，有效结合了GPU配置和batch size等系统变量，大幅提升了大语言模型的推理速度。


<details>
  <summary>Details</summary>
Motivation: 传统LLM由于自回归结构和模型体积庞大，导致推理延迟显著。现有投机解码方法虽然提升了解码效率，但往往忽视了GPU和batch size等硬件配置对于解码速度的影响。

Method: 作者提出CAST动态树解码算法，根据包含GPU设备和batch size等系统变量的推理成本动态调整树结构。同时进行了六个任务和六个不同LLM的大量实验，全面评估了该方法。

Result: CAST在六类任务和六种LLM上，相比常规解码加速最高可达5.2倍，相较最新SOTA方法整体提升5%到20%。

Conclusion: CAST方法在综合考虑硬件配置等变量下，显著提升LLM解码速度并优于现有投机解码技术，具有实际部署价值。

Abstract: Large Language Models (LLMs) face significant inference latency challenges
stemming from their autoregressive design and large size. To address this,
speculative decoding emerges as a solution, enabling the simultaneous
generation and validation of multiple tokens. While recent approaches like
EAGLE-2 and EAGLE-3 improve speculative decoding using dynamic tree structures,
they often neglect the impact of crucial system variables such as GPU devices
and batch sizes.
  Therefore, we introduce a new dynamic tree decoding approach called CAST that
takes into account inference costs, including factors such as GPU
configurations and batch sizes, to dynamically refine the tree structure.
Through comprehensive experimentation across six diverse tasks and utilizing
six distinct LLMs, our methodology demonstrates remarkable results, achieving
speeds up to 5.2 times faster than conventional decoding methods. Moreover, it
generally outperforms existing state-of-the-art techniques from 5% to 20%.

</details>


### [122] [SlideAgent: Hierarchical Agentic Framework for Multi-Page Visual Document Understanding](https://arxiv.org/abs/2510.26615)
*Yiqiao Jin,Rachneet Kaur,Zhen Zeng,Sumitra Ganesh,Srijan Kumar*

Main category: cs.CL

TL;DR: 本文提出了一种名为SlideAgent的新型多模态智能体架构，有效提升了对多页多布局文档（如幻灯片）的理解与推理表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽然能对文档内容进行一定的解读，但难以胜任对复杂、多页、多模态（包含布局、颜色、图标等）文档的细粒度推理，存在信息整合和深层理解的瓶颈。

Method: 提出SlideAgent智能体框架，将对文档的推理解构为全局（整体）、页面、元素三个层级。框架内设有专门子代理，分别处理各个层级的信息拆解与整合，构建一种不依赖于具体问题的结构化表征，覆盖文档主题和细致视觉/文本要素。推理阶段，SlideAgent根据实际问题激活合适代理，多层互动以输出有上下文关联、富含细节的答案。

Result: 实验表明，SlideAgent在多项评价基准上显著优于现有闭源模型（整体提升7.9分）和开源模型（整体提升9.8分）。

Conclusion: SlideAgent能更好地理解和处理复杂多模态多页文档的信息，为文档内容的深层理解与自动化应用提供切实进步。

Abstract: Multi-page visual documents such as manuals, brochures, presentations, and
posters convey key information through layout, colors, icons, and cross-slide
references. While large language models (LLMs) offer opportunities in document
understanding, current systems struggle with complex, multi-page visual
documents, particularly in fine-grained reasoning over elements and pages. We
introduce SlideAgent, a versatile agentic framework for understanding
multi-modal, multi-page, and multi-layout documents, especially slide decks.
SlideAgent employs specialized agents and decomposes reasoning into three
specialized levels-global, page, and element-to construct a structured,
query-agnostic representation that captures both overarching themes and
detailed visual or textual cues. During inference, SlideAgent selectively
activates specialized agents for multi-level reasoning and integrates their
outputs into coherent, context-aware answers. Extensive experiments show that
SlideAgent achieves significant improvement over both proprietary (+7.9
overall) and open-source models (+9.8 overall).

</details>


### [123] [Encoder-Decoder or Decoder-Only? Revisiting Encoder-Decoder Large Language Model](https://arxiv.org/abs/2510.26622)
*Biao Zhang,Yong Cheng,Siamak Shakeri,Xinyi Wang,Min Ma,Orhan Firat*

Main category: cs.CL

TL;DR: 本文对当前主流的解码器-only（decoder-only）与编码器-解码器（encoder-decoder）大语言模型（LLM）进行了系统对比，发现经过最新方法改进后的编码器-解码器模型在效率与性能上表现优异，值得重新关注。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型研究从encoder-decoder结构转向了decoder-only结构，但缺少了系统的、可扩展性视角下的对比分析，可能忽视了encoder-decoder结构的潜力。作者希望弥补这一空白。

Method: 作者对经典的encoder-decoder LLM（RedLLM）采用了来自decoder-only LLM（DecLLM）的最新训练方法，并逐尺度（大约150M到8B参数）对两者进行了对比。预训练用RedPajama V1数据集，指令微调采用FLAN。评估包括可扩展性、推理效率、各种下游任务等。

Result: RedLLM在可扩展性和上下文长度外推能力上表现与DecLLM相当。虽然DecLLM在预训练阶段计算效率更高，但RedLLM在多项下游任务上指令微调后表现可比甚至更优，并且推理效率明显更高。

Conclusion: 经过改进的encoder-decoder模型在多个方面拥有出色表现，并具备较大潜力。研究结果鼓励研究者重新审视并进一步开发该结构，以打造更强大、高效的LLM。

Abstract: Recent large language model (LLM) research has undergone an architectural
shift from encoder-decoder modeling to nowadays the dominant decoder-only
modeling. This rapid transition, however, comes without a rigorous comparative
analysis especially \textit{from the scaling perspective}, raising concerns
that the potential of encoder-decoder models may have been overlooked. To fill
this gap, we revisit encoder-decoder LLM (RedLLM), enhancing it with recent
recipes from decoder-only LLM (DecLLM). We conduct a comprehensive comparison
between RedLLM, pretrained with prefix language modeling (LM), and DecLLM,
pretrained with causal LM, at different model scales, ranging from $\sim$150M
to $\sim$8B. Using RedPajama V1 (1.6T tokens) for pretraining and FLAN for
instruction tuning, our experiments show that RedLLM produces compelling
scaling properties and surprisingly strong performance. While DecLLM is overall
more compute-optimal during pretraining, RedLLM demonstrates comparable scaling
and context length extrapolation capabilities. After instruction tuning, RedLLM
achieves comparable and even better results on various downstream tasks while
enjoying substantially better inference efficiency. We hope our findings could
inspire more efforts on re-examining RedLLM, unlocking its potential for
developing powerful and efficient LLMs.

</details>


### [124] [Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models](https://arxiv.org/abs/2510.26683)
*Mingchen Tu,Zhiqiang Liu,Juan Li,Liangyurui Liu,Junjie Wang,Lei Liang,Wen Zhang*

Main category: cs.CL

TL;DR: 本论文提出Evontree框架，通过利用少量高质量本体规则，从大型语言模型中自动提取、验证并增强医疗等专业领域知识，实现无需大量外部数据集即可提升模型在特定领域的表现。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在医疗等数据敏感领域，缺乏高质量、领域专属的训练语料，难以适应专业应用场景。而专家已有的本体规则能够结构化表达领域知识，为知识管理带来保障，但LLMs如何高效融合这些规则还未有有效方案。

Method: Evontree框架首先从原始模型中抽取领域本体，用两条核心本体规则检测概念间不一致，并通过自蒸馏微调强化优化后的知识，无需依赖大规模外部数据。

Result: 在医疗问答任务上，Evontree以Llama3-8B-Instruct和Med42-v2为基础进行了大量实验，与未修改模型和主流有监督方案相比，准确率最高提升3.7%。

Conclusion: Evontree方法在低资源领域模型适配上效果突出，兼顾有效性、高效性和稳健性，可广泛支持专业领域的LLMs能力增强。

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities
across multiple domains by leveraging massive pre-training and curated
fine-tuning data. However, in data-sensitive fields such as healthcare, the
lack of high-quality, domain-specific training corpus hinders LLMs' adaptation
for specialized applications. Meanwhile, domain experts have distilled domain
wisdom into ontology rules, which formalize relationships among concepts and
ensure the integrity of knowledge management repositories. Viewing LLMs as
implicit repositories of human knowledge, we propose Evontree, a novel
framework that leverages a small set of high-quality ontology rules to
systematically extract, validate, and enhance domain knowledge within LLMs,
without requiring extensive external datasets. Specifically, Evontree extracts
domain ontology from raw models, detects inconsistencies using two core
ontology rules, and reinforces the refined knowledge via self-distilled
fine-tuning. Extensive experiments on medical QA benchmarks with
Llama3-8B-Instruct and Med42-v2 demonstrate consistent outperformance over both
unmodified models and leading supervised baselines, achieving up to a 3.7%
improvement in accuracy. These results confirm the effectiveness, efficiency,
and robustness of our approach for low-resource domain adaptation of LLMs.

</details>


### [125] [Kimi Linear: An Expressive, Efficient Attention Architecture](https://arxiv.org/abs/2510.26692)
*Kimi Team,Yu Zhang,Zongyu Lin,Xingcheng Yao,Jiaxi Hu,Fanqing Meng,Chengyin Liu,Xin Men,Songlin Yang,Zhiyuan Li,Wentao Li,Enzhe Lu,Weizhou Liu,Yanru Chen,Weixin Xu,Longhui Yu,Yejie Wang,Yu Fan,Longguang Zhong,Enming Yuan,Dehao Zhang,Yizhi Zhang,T. Y. Liu,Haiming Wang,Shengjun Fang,Weiran He,Shaowei Liu,Yiwei Li,Jianlin Su,Jiezhong Qiu,Bo Pang,Junjie Yan,Zhejun Jiang,Weixiao Huang,Bohong Yin,Jiacheng You,Chu Wei,Zhengtao Wang,Chao Hong,Yutian Chen,Guanduo Chen,Yucheng Wang,Huabin Zheng,Feng Wang,Yibo Liu,Mengnan Dong,Zheng Zhang,Siyuan Pan,Wenhao Wu,Yuhao Wu,Longyu Guan,Jiawen Tao,Guohong Fu,Xinran Xu,Yuzhi Wang,Guokun Lai,Yuxin Wu,Xinyu Zhou,Zhilin Yang,Yulun Du*

Main category: cs.CL

TL;DR: Kimi Linear是一种混合线性注意力架构，首次在公平条件下超越了全注意力机制，适用于短上下文、长上下文和强化学习等多种场景。它不仅提升了性能，还极大地提高了推理效率，且大幅降低了内存和计算开销。


<details>
  <summary>Details</summary>
Motivation: 动机在于解决传统全注意力机制在长序列任务中的计算和存储瓶颈，尤其是KV缓存消耗和推理速度问题，同时保证短序列和RL等多领域场景下的表现不下降。

Method: 作者提出Kimi Delta Attention（KDA）模块，扩展Gated DeltaNet，通过更细粒度的门控机制提升RNN内存的利用。提出基于块的专用算法，采用特殊的对角线加低秩（DPLR）转移矩阵，降低计算成本，并与经典delta规则保持一致。预训练了含3B激活参数、48B总参数的Kimi Linear模型，并采用KDA与多头潜变量注意力（MLA）层级混合。

Result: Kimi Linear在所有评测任务下均大幅优于全MLA模型：KV缓存消耗减少高达75%，1M上下文下推理速度提升达6倍。

Conclusion: Kimi Linear可直接替换全注意力框架，带来更优的性能和效率表现，尤其对长输入、长输出任务尤为适合。作者开源了相关核心代码和预训练模型，促进后续研究。

Abstract: We introduce Kimi Linear, a hybrid linear attention architecture that, for
the first time, outperforms full attention under fair comparisons across
various scenarios -- including short-context, long-context, and reinforcement
learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an
expressive linear attention module that extends Gated DeltaNet with a
finer-grained gating mechanism, enabling more effective use of limited
finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware
efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)
transition matrices, which substantially reduces computation compared to the
general DPLR formulation while remaining more consistent with the classical
delta rule.
  We pretrain a Kimi Linear model with 3B activated parameters and 48B total
parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention
(MLA). Our experiments show that with an identical training recipe, Kimi Linear
outperforms full MLA with a sizeable margin across all evaluated tasks, while
reducing KV cache usage by up to 75% and achieving up to 6 times decoding
throughput for a 1M context. These results demonstrate that Kimi Linear can be
a drop-in replacement for full attention architectures with superior
performance and efficiency, including tasks with longer input and output
lengths.
  To support further research, we open-source the KDA kernel and vLLM
implementations, and release the pre-trained and instruction-tuned model
checkpoints.

</details>


### [126] [The End of Manual Decoding: Towards Truly End-to-End Language Models](https://arxiv.org/abs/2510.26697)
*Zhichao Wang,Dongyang Ma,Xinting Huang,Deng Cai,Tian Lan,Jiahao Xu,Haitao Mi,Xiaoying Tang,Yan Wang*

Main category: cs.CL

TL;DR: 本论文提出了AutoDeco，一种让大型语言模型（LLM）真正实现端到端生成的新方法，通过让模型自身动态学习控制解码超参数，而无需手工调节。


<details>
  <summary>Details</summary>
Motivation: 现有LLM所谓的“端到端”生成实际不成立，绝大多数模型依赖非可微分的解码过程，比如温度（temperature）和top-p，这些超参数需人工繁琐调优，效率低且难以适配各种任务或上下文。

Method: 作者提出在标准transformer模型中加入轻量的预测头，使模型在每一步预测下一个token的同时，还能动态预测最优的温度和top-p参数，实现自调控采样。所有调控在单次前向传播中完成，实现端到端可微分的调节。

Result: 在八个基准测试上，AutoDeco显著优于默认的解码策略，并能媲美“测试集作弊式”上界（即针对测试集精确调参的oracle baseline）。模型亦表现出根据自然语言指令（如“低随机性生成”）动态调整采样策略的能力。

Conclusion: AutoDeco不仅提升了解码性能，还首次实现了通过自然语言直接控制采样行为，为LLM的可控性和交互性开辟了新道路。

Abstract: The "end-to-end" label for LLMs is a misnomer. In practice, they depend on a
non-differentiable decoding process that requires laborious, hand-tuning of
hyperparameters like temperature and top-p. This paper introduces AutoDeco, a
novel architecture that enables truly "end-to-end" generation by learning to
control its own decoding strategy. We augment the standard transformer with
lightweight heads that, at each step, dynamically predict context-specific
temperature and top-p values alongside the next-token logits. This approach
transforms decoding into a parametric, token-level process, allowing the model
to self-regulate its sampling strategy within a single forward pass.
  Through extensive experiments on eight benchmarks, we demonstrate that
AutoDeco not only significantly outperforms default decoding strategies but
also achieves performance comparable to an oracle-tuned baseline derived from
"hacking the test set"-a practical upper bound for any static method.
Crucially, we uncover an emergent capability for instruction-based decoding
control: the model learns to interpret natural language commands (e.g.,
"generate with low randomness") and adjusts its predicted temperature and top-p
on a token-by-token basis, opening a new paradigm for steerable and interactive
LLM decoding.

</details>


### [127] [Value Drifts: Tracing Value Alignment During LLM Post-Training](https://arxiv.org/abs/2510.26707)
*Mehar Bhatia,Shravan Nayak,Gaurav Kamath,Marius Mosbach,Karolina Stańczak,Vered Shwartz,Siva Reddy*

Main category: cs.CL

TL;DR: 本文研究了大模型（LLMs）在后训练过程中的价值观对齐动态，发现监督微调（SFT）阶段决定了模型的价值观，而偏好优化阶段对价值观影响较小，不同算法下即使数据一致亦会产生不同对齐结果。


<details>
  <summary>Details</summary>
Motivation: 随着大模型在社会中的作用增强，其不仅需要展现通用知识，还需对齐特定的人类价值体系。但以往关注点多集中在训练完成后的对齐表现，忽略了训练过程中价值观对齐的演变和形成机制。

Method: 作者以Llama-3和Qwen-3等不同规模模型作为实验对象，结合流行的监督微调和偏好优化算法及数据集，系统监测训练期间价值观变化；同时构造可控的合成偏好数据集，分析不同偏好优化算法对价值观对齐的影响。

Result: 实验显示，SFT阶段几乎决定了模型的价值观，之后的偏好优化阶段很难实质性改变这种对齐。而用同样的偏好数据，不同优化算法却导致了不同的价值观对齐结果。

Conclusion: 研究揭示了模型在后训练环节价值观学习的关键时点和机制，对数据策划、模型与优化算法的选择提供参考，有助于提升大模型与人类价值观的对齐水平。

Abstract: As LLMs occupy an increasingly important role in society, they are more and
more confronted with questions that require them not only to draw on their
general knowledge but also to align with certain human value systems.
Therefore, studying the alignment of LLMs with human values has become a
crucial field of inquiry. Prior work, however, mostly focuses on evaluating the
alignment of fully trained models, overlooking the training dynamics by which
models learn to express human values. In this work, we investigate how and at
which stage value alignment arises during the course of a model's
post-training. Our analysis disentangles the effects of post-training
algorithms and datasets, measuring both the magnitude and time of value drifts
during training. Experimenting with Llama-3 and Qwen-3 models of different
sizes and popular supervised fine-tuning (SFT) and preference optimization
datasets and algorithms, we find that the SFT phase generally establishes a
model's values, and subsequent preference optimization rarely re-aligns these
values. Furthermore, using a synthetic preference dataset that enables
controlled manipulation of values, we find that different preference
optimization algorithms lead to different value alignment outcomes, even when
preference data is held constant. Our findings provide actionable insights into
how values are learned during post-training and help to inform data curation,
as well as the selection of models and algorithms for preference optimization
to improve model alignment to human values.

</details>


### [128] [AMO-Bench: Large Language Models Still Struggle in High School Math Competitions](https://arxiv.org/abs/2510.26768)
*Shengnan An,Xunliang Cai,Xuezhi Cao,Xiaoyu Li,Yehao Lin,Junlin Liu,Xinxuan Lv,Dan Ma,Xuanlin Wang,Ziwen Wang,Shuang Zhou*

Main category: cs.CL

TL;DR: 本文提出了AMO-Bench，这是一个由人类专家精心设计、难度达国际数学奥林匹克（IMO）甚至更高水平的高级数学推理基准测试，包含50道全新原创题。现有基准因题库熟悉度和难度饱和，已难以评估高端大模型的推理能力。AMO-Bench有助于揭示LLM在数学推理方面的瓶颈和提升空间。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在中学数学等传统基准测试上表现趋于饱和，现有题库（如AIME24/25）已无法有效区分和推动顶尖模型的数学推理水平。为了更好地评估和促进LLMs在更高层次的数学推理能力，作者提出更高难度、更严格验证的全新问题集合。

Method: 作者组织专家团队创作并交叉验证了50道原创数学题，难度均不低于国际奥数标准，确保题目纯原创、无训练数据泄露风险。所有题目仅需作答最终答案，便于自动化精准评分。随后，作者用AMO-Bench测试了26种大语言模型的表现，并分析了模型精度随计算资源变化的趋势。

Result: 实验结果显示，最优模型在AMO-Bench上的准确率仅为52.4%，多数模型低于40%。此外，随测试时推理算力提升，模型表现呈现出鼓舞人的增长趋势，但总体成绩仍显示当前模型在高级数学推理方面存在明显不足。

Conclusion: AMO-Bench为当前LLMs提供了更有挑战性的数学推理评测平台，揭示了顶尖模型在此领域依然有巨大改进空间。该基准测试的发布有助于推动语言模型在复杂推理任务上的进一步发展。

Abstract: We present AMO-Bench, an Advanced Mathematical reasoning benchmark with
Olympiad level or even higher difficulty, comprising 50 human-crafted problems.
Existing benchmarks have widely leveraged high school math competitions for
evaluating mathematical reasoning capabilities of large language models (LLMs).
However, many existing math competitions are becoming less effective for
assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To
address this, AMO-Bench introduces more rigorous challenges by ensuring all 50
problems are (1) cross-validated by experts to meet at least the International
Mathematical Olympiad (IMO) difficulty standards, and (2) entirely original
problems to prevent potential performance leakages from data memorization.
Moreover, each problem in AMO-Bench requires only a final answer rather than a
proof, enabling automatic and robust grading for evaluation. Experimental
results across 26 LLMs on AMO-Bench show that even the best-performing model
achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%.
Beyond these poor performances, our further analysis reveals a promising
scaling trend with increasing test-time compute on AMO-Bench. These results
highlight the significant room for improving the mathematical reasoning in
current LLMs. We release AMO-Bench to facilitate further research into
advancing the reasoning abilities of language models.
https://amo-bench.github.io/

</details>


### [129] [Gistify! Codebase-Level Understanding via Runtime Execution](https://arxiv.org/abs/2510.26790)
*Hyunji Lee,Minseon Kim,Chinmay Singh,Matheus Pereira,Atharv Sonwane,Isadora White,Elias Stengel-Eskin,Mohit Bansal,Zhengyan Shi,Alessandro Sordoni,Marc-Alexandre Côté,Xingdi Yuan,Lucas Caccia*

Main category: cs.CL

TL;DR: 本文提出Gistify任务，要求编码大模型（LLM）生成一个最小的、可复现代码库特定功能的独立文件，并用于评测模型对大型代码库的理解和生成能力。目前最先进的模型在该任务上表现仍不稳定，尤其在处理需要长执行路径的任务时。


<details>
  <summary>Details</summary>
Motivation: 随着编码智能体被广泛应用于大型代码库，如何对其进行高难度、真实场景的评测成为亟需解决的问题。现有的小型、片段级别的测试不足以反映模型对代码库结构和行为的真实理解。作者希望通过新的标准任务推动编码模型在复杂代码库上的进步。

Method: 作者提出“Gistify”评测任务。具体方法为：1）为编码大模型提供完整代码库及一个具体入口命令；2）要求模型生成一个最小化、仅包含必要组件的独立文件；3）该文件执行后需与在完整代码库下用同命令的输出一致。通过这种方式，评估模型对代码库结构、执行流程的理解及大规模代码生成能力。

Result: 实验发现，当前最先进的编码大模型在Gistify任务上的成功率有限。尤其在面对需要较长执行流程追踪的复杂任务时，模型难以生成正确且精简的复现文件。

Conclusion: Gistify任务对现有模型提出了新的高标准挑战，表明代码库级别的结构理解与执行推理仍是编码模型的弱点。该任务为提升未来编码智能体在真实代码基环境下的能力指明了方向，并促使更多针对复杂代码结构的模型改进。

Abstract: As coding agents are increasingly deployed in large codebases, the need to
automatically design challenging, codebase-level evaluation is central. We
propose Gistify, a task where a coding LLM must create a single, minimal,
self-contained file that can reproduce a specific functionality of a codebase.
The coding LLM is given full access to a codebase along with a specific
entrypoint (e.g., a python command), and the generated file must replicate the
output of the same command ran under the full codebase, while containing only
the essential components necessary to execute the provided command. Success on
Gistify requires both structural understanding of the codebase, accurate
modeling of its execution flow as well as the ability to produce potentially
large code patches. Our findings show that current state-of-the-art models
struggle to reliably solve Gistify tasks, especially ones with long executions
traces.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [130] [Debate2Create: Robot Co-design via Large Language Model Debates](https://arxiv.org/abs/2510.25850)
*Kevin Qiu,Marek Cygan*

Main category: cs.RO

TL;DR: 该论文提出了Debate2Create（D2C）框架，通过大语言模型代理在结构化辩论中协同优化机器人形态和控制奖励函数，实现了机器人共设计的自动化。


<details>
  <summary>Details</summary>
Motivation: 机器人形态与控制的协同设计因设计空间庞大和形体与行为高度耦合而长期存在重大挑战，亟需更高效的自动化方法拓展机器人的设计能力。

Method: D2C框架中，大语言模型分别担任设计代理和控制代理，前者提出结构调整方案，后者制定针对性奖励函数。同时，评审小组在仿真中评估并反馈设计-控制对，每轮迭代优化，辩论中逐步细化方案。

Result: D2C在四足机器人移动基准测试上获得了显著提升，其优化出的设计比默认方案远行73%，而且生成了多样和专门化的形态，尽管未明确设置多样性目标。

Conclusion: 结构化的LLM多代理辩论结合物理仿真反馈，为机器人共设计自动化提供了强大机制，是未来自动化机器人设计的一个有前景的新范式。

Abstract: Automating the co-design of a robot's morphology and control is a
long-standing challenge due to the vast design space and the tight coupling
between body and behavior. We introduce Debate2Create (D2C), a framework in
which large language model (LLM) agents engage in a structured dialectical
debate to jointly optimize a robot's design and its reward function. In each
round, a design agent proposes targeted morphological modifications, and a
control agent devises a reward function tailored to exploit the new design. A
panel of pluralistic judges then evaluates the design-control pair in
simulation and provides feedback that guides the next round of debate. Through
iterative debates, the agents progressively refine their proposals, producing
increasingly effective robot designs. Notably, D2C yields diverse and
specialized morphologies despite no explicit diversity objective. On a
quadruped locomotion benchmark, D2C discovers designs that travel 73% farther
than the default, demonstrating that structured LLM-based debate can serve as a
powerful mechanism for emergent robot co-design. Our results suggest that
multi-agent debate, when coupled with physics-grounded feedback, is a promising
new paradigm for automated robot design.

</details>


### [131] [Risk-Aware Safety Filters with Poisson Safety Functions and Laplace Guidance Fields](https://arxiv.org/abs/2510.25913)
*Gilbert Bahati,Ryan M. Bena,Meg Wilkinson,Pol Mestres,Ryan K. Cosner,Aaron D. Ames*

Main category: cs.RO

TL;DR: 本论文提出了一种结合泊松方程和拉普拉斯方程的方法，开发能够感知风险的安全过滤器，用于机器人系统在真实环境中的导航与避障。


<details>
  <summary>Details</summary>
Motivation: 在实际环境中，机器人系统需要理解环境的语义以确定位于环境中的安全行为，传统方法对环境风险辨识有限。因此，研究急需数学模型来表达环境风险并集成到安全约束中。

Method: 本文采用两步法：首先通过求解泊松方程的Dirichlet问题，生成一个安全函数，用于界定系统安全区域；然后再求解拉普拉斯方程以获取一个根据障碍物风险级别调节的引导场，该场通过可调通量边界条件实现。最终，将安全函数和引导场结合得到一个安全约束，用以生成风险感知的安全过滤器。

Result: 方法在仿真中验证，显示出能够针对不同风险级别障碍物优先避让，并有效保证导航的安全性。

Conclusion: 该方法能将先验的障碍物风险认知直接纳入安全过滤器，使机器人实现更具风险感知的安全行为，提升了实际应用中的安全保障。

Abstract: Robotic systems navigating in real-world settings require a semantic
understanding of their environment to properly determine safe actions. This
work aims to develop the mathematical underpinnings of such a representation --
specifically, the goal is to develop safety filters that are risk-aware. To
this end, we take a two step approach: encoding an understanding of the
environment via Poisson's equation, and associated risk via Laplace guidance
fields. That is, we first solve a Dirichlet problem for Poisson's equation to
generate a safety function that encodes system safety as its 0-superlevel set.
We then separately solve a Dirichlet problem for Laplace's equation to
synthesize a safe \textit{guidance field} that encodes variable levels of
caution around obstacles -- by enforcing a tunable flux boundary condition. The
safety function and guidance fields are then combined to define a safety
constraint and used to synthesize a risk-aware safety filter which, given a
semantic understanding of an environment with associated risk levels of
environmental features, guarantees safety while prioritizing avoidance of
higher risk obstacles. We demonstrate this method in simulation and discuss how
\textit{a priori} understandings of obstacle risk can be directly incorporated
into the safety filter to generate safe behaviors that are risk-aware.

</details>


### [132] [Curvature-Aware Calibration of Tactile Sensors for Accurate Force Estimation on Non-Planar Surfaces](https://arxiv.org/abs/2510.25965)
*Luoyan Zhong,Heather Jin Hee Kim,Dylan P. Losey,Cara M. Nunez*

Main category: cs.RO

TL;DR: 本文提出了一种通过神经网络曲率预测实现的柔性触觉传感器弯曲表面标定方法，能够在不同曲率下保持高精度力估算。


<details>
  <summary>Details</summary>
Motivation: 现有柔性触觉传感器多只在平坦表面标定，贴合曲面后精度和一致性下降，限制其在实际中的可靠性，因此亟需一种针对曲面应用的高效标定方法。

Method: 开发了一种针对常用电阻型柔性触觉传感器的模型，通过多层感知机神经网络，以无负载下的传感器输出为输入，预测本地曲率，并实现弯曲表面上的力精确估算。验证方法为在五个不同曲率日常物体上测试2N-8N的测力精度。

Result: 神经网络曲率预测R^2达到0.91。新方法下，传感器在不同曲率物体（2N-8N区间）上均保持一致的力计算准确度；而传统仅平面标定方法在曲率增大时低估力。

Conclusion: 基于曲率感知标定显著提升了柔性触觉传感器在曲面上的精度、一致性与可靠性，使其更好地服务于真实应用场景。

Abstract: Flexible tactile sensors are increasingly used in real-world applications
such as robotic grippers, prosthetic hands, wearable gloves, and assistive
devices, where they need to conform to curved and irregular surfaces. However,
most existing tactile sensors are calibrated only on flat substrates, and their
accuracy and consistency degrade once mounted on curved geometries. This
limitation restricts their reliability in practical use. To address this
challenge, we develop a calibration model for a widely used resistive tactile
sensor design that enables accurate force estimation on one-dimensional curved
surfaces. We then train a neural network (a multilayer perceptron) to predict
local curvature from baseline sensor outputs recorded under no applied load,
achieving an R2 score of 0.91. The proposed approach is validated on five daily
objects with varying curvatures under forces from 2 N to 8 N. Results show that
the curvature-aware calibration maintains consistent force accuracy across all
surfaces, while flat-surface calibration underestimates force as curvature
increases. Our results demonstrate that curvature-aware modeling improves the
accuracy, consistency, and reliability of flexible tactile sensors, enabling
dependable performance across real-world applications.

</details>


### [133] [A New Type of Axis-Angle Attitude Control Law for Rotational Systems: Synthesis, Analysis, and Experiments](https://arxiv.org/abs/2510.25985)
*Francisco M. F. R. Gonçalves,Ryan M. Bena,Néstor O. Pérez-Arancibia*

Main category: cs.RO

TL;DR: 本文提出了一种基于轴-角信息的新型姿态控制律，相较传统四元数方法，能更好保证唯一和稳定的平衡点，并提升旋转体（如四旋翼）恢复稳定的效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于四元数的连续姿态控制方法尽管有效，但存在无法保证闭环唯一平衡点，以及对较大姿态误差时控制作用减弱等问题。因此，亟需设计一种改进的控制律以克服这些缺点。

Method: 作者提出了一种基于姿态误差轴-角的控制律，利用严格Lyapunov函数论证了新方法能够保证唯一的、全局渐近稳定的平衡点。并通过仿真和真实四旋翼飞行实验，检验了方法有效性。

Result: 仿真实验和实际飞行测试结果表明，所提方法相比高性能的传统四元数控制器，能在更短时间内完成姿态稳定（恢复），表现出更优越的飞行性能。

Conclusion: 基于轴-角的姿态控制方法不仅有效克服了传统方法平衡点唯一性和大误差恢复困难的问题，也能进一步提升无人机等旋转体的控制性能，具有较好实际应用前景。

Abstract: Over the past few decades, continuous quaternion-based attitude control has
been proven highly effective for driving rotational systems that can be modeled
as rigid bodies, such as satellites and drones. However, methods rooted in this
approach do not enforce the existence of a unique closed-loop (CL) equilibrium
attitude-error quaternion (AEQ); and, for rotational errors about the
attitude-error Euler axis larger than {\pi}rad, their proportional-control
effect diminishes as the system state moves away from the stable equilibrium of
the CL rotational dynamics. In this paper, we introduce a new type of attitude
control law that more effectively leverages the attitude-error Euler axis-angle
information to guarantee a unique CL equilibrium AEQ and to provide greater
flexibility in the use of proportional-control efforts. Furthermore, using two
different control laws as examples-through the construction of a strict
Lyapunov function for the CL dynamics-we demonstrate that the resulting unique
equilibrium of the CL rotational system can be enforced to be uniformly
asymptotically stable. To assess and demonstrate the functionality and
performance of the proposed approach, we performed numerical simulations and
executed dozens of real-time tumble-recovery maneuvers using a small quadrotor.
These simulations and flight tests compellingly demonstrate that the proposed
axis-angle-based method achieves superior flight performance-compared with that
obtained using a high-performance quaternion-based controller-in terms of
stabilization time.

</details>


### [134] [DARTS: A Drone-Based AI-Powered Real-Time Traffic Incident Detection System](https://arxiv.org/abs/2510.26004)
*Bai Li,Achilleas Kourtellis,Rong Cao,Joseph Post,Brian Porter,Yu Zhang*

Main category: cs.RO

TL;DR: 该论文提出了一种基于无人机、人工智能和热成像的新型实时交通事件检测系统DARTS，能够有效早期发现交通事故并辅助响应与管理。实测中系统准确率达99%，极大提高了事件检测和验证的及时性与灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有交通事故检测依赖CCTV、行车记录仪等，需要大量基础设施，灵活性和扩展性不足，且检测与验证分离，导致响应延迟。因此需要一种更高效、可扩展、适应性强的新方法。

Method: DARTS系统结合了无人机的高机动性和广角空中视角、热成像以提升能见度和隐私保护，并采用轻量级深度学习框架对车辆轨迹进行实时提取和事故检测。系统还支持在线远程可视化验证和事故严重度评估。

Result: DARTS在自采集数据集上检测准确率为99%。在佛罗里达I-75高速公路实地测试中，比当地管理中心早12分钟发现并验证追尾事故，还可在线监控拥堵扩散，证明了其实用性和高效响应能力。

Conclusion: DARTS具备高灵活性、低成本、适合不同环境的部署优势，可显著提升交通事故响应效率，对于交通管理的数字化和智能化具有重要意义。

Abstract: Rapid and reliable incident detection is critical for reducing crash-related
fatalities, injuries, and congestion. However, conventional methods, such as
closed-circuit television, dashcam footage, and sensor-based detection,
separate detection from verification, suffer from limited flexibility, and
require dense infrastructure or high penetration rates, restricting
adaptability and scalability to shifting incident hotspots. To overcome these
challenges, we developed DARTS, a drone-based, AI-powered real-time traffic
incident detection system. DARTS integrates drones' high mobility and aerial
perspective for adaptive surveillance, thermal imaging for better
low-visibility performance and privacy protection, and a lightweight deep
learning framework for real-time vehicle trajectory extraction and incident
detection. The system achieved 99% detection accuracy on a self-collected
dataset and supports simultaneous online visual verification, severity
assessment, and incident-induced congestion propagation monitoring via a
web-based interface. In a field test on Interstate 75 in Florida, DARTS
detected and verified a rear-end collision 12 minutes earlier than the local
transportation management center and monitored incident-induced congestion
propagation, suggesting potential to support faster emergency response and
enable proactive traffic control to reduce congestion and secondary crash risk.
Crucially, DARTS's flexible deployment architecture reduces dependence on
frequent physical patrols, indicating potential scalability and
cost-effectiveness for use in remote areas and resource-constrained settings.
This study presents a promising step toward a more flexible and integrated
real-time traffic incident detection system, with significant implications for
the operational efficiency and responsiveness of modern transportation
management.

</details>


### [135] [RADRON: Cooperative Localization of Ionizing Radiation Sources by MAVs with Compton Cameras](https://arxiv.org/abs/2510.26018)
*Petr Stibinger,Tomas Baca,Daniela Doubravova,Jan Rusnak,Jaroslav Solc,Jan Jakubek,Petr Stepan,Martin Saska*

Main category: cs.RO

TL;DR: 本文提出利用多架微型无人机（MAVs）协作，用先进的轻型康普顿相机实时定位放射性源。


<details>
  <summary>Details</summary>
Motivation: 目前用于放射性源定位的设备笨重，难以高效移动和部署，限制了快速检测和响应能力。微型无人机和轻型探测器为快速、灵活的放射性检测提供了新机遇。

Method: 采用只有40克重的高灵敏单探测器康普顿相机安装在MAVs上，利用分布式协作的无人机群，通过融合相机稀疏测量结果，实时估算放射源位置。数据收集和处理全部在机载完成，测量反馈用于动态调整无人机运动，实现紧密编队搜索和源位置跟踪。

Result: 所提方法能极大提高放射源定位速度和精度，即使测量数据很稀疏也能实现准确估算；无人机编队可实现对移动放射源的动态跟踪。

Conclusion: 协作MAVs携带超轻康普顿相机可实现高效、实时、灵活的放射源定位与跟踪，有望应用于核监测和应急领域。

Abstract: We present a novel approach to localizing radioactive material by cooperating
Micro Aerial Vehicles (MAVs). Our approach utilizes a state-of-the-art
single-detector Compton camera as a highly sensitive, yet miniature detector of
ionizing radiation. The detector's exceptionally low weight (40 g) opens up new
possibilities of radiation detection by a team of cooperating agile MAVs. We
propose a new fundamental concept of fusing the Compton camera measurements to
estimate the position of the radiation source in real time even from extremely
sparse measurements. The data readout and processing are performed directly
onboard and the results are used in a dynamic feedback to drive the motion of
the vehicles. The MAVs are stabilized in a tightly cooperating swarm to
maximize the information gained by the Compton cameras, rapidly locate the
radiation source, and even track a moving radiation source.

</details>


### [136] [Accelerating Real-World Overtaking in F1TENTH Racing Employing Reinforcement Learning Methods](https://arxiv.org/abs/2510.26040)
*Emily Steiner,Daniel van der Spuy,Futian Zhou,Afereti Pama,Minas Liarokapis,Henry Williams*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的自动赛车超车智能体，在仿真和现实两种环境下表现良好，实现了更高效、更可靠的超车能力。相比仅训练赛道驾驶的智能体，本方法的超车成功率显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶赛车主要在单车计时赛中取得进步，但轮对轮超车能力不足，难以在现实场景中安全、稳定地完成超车。由于安全可靠的超车对于无人驾驶赛车至关重要，因此亟需开发更高效的超车算法。F1Tenth竞赛提供了物理标准平台，有利于开展相关研究。

Method: 研究提出了一种新型赛车和超车智能体，能够在仿真和现实环境中学习赛道导航和主动超车。该智能体被部署在F1Tenth车辆上，并与不同竞争算法的智能体进行实地对抗。通过与对手训练，让智能体学习更有针对性的超车行为。

Result: 实验证明，该智能体在现实中与不同算法对手竞争时，超车成功率达到87%，相比只训练赛道驾驶的智能体（56%）有显著提升。

Conclusion: 通过与对手训练，该方法有效提升了自动驾驶赛车的主动超车能力，具有广泛应用于安全可靠自动驾驶竞技的潜力。

Abstract: While autonomous racing performance in Time-Trial scenarios has seen
significant progress and development, autonomous wheel-to-wheel racing and
overtaking are still severely limited. These limitations are particularly
apparent in real-life driving scenarios where state-of-the-art algorithms
struggle to safely or reliably complete overtaking manoeuvres. This is
important, as reliable navigation around other vehicles is vital for safe
autonomous wheel-to-wheel racing. The F1Tenth Competition provides a useful
opportunity for developing wheel-to-wheel racing algorithms on a standardised
physical platform. The competition format makes it possible to evaluate
overtaking and wheel-to-wheel racing algorithms against the state-of-the-art.
This research presents a novel racing and overtaking agent capable of learning
to reliably navigate a track and overtake opponents in both simulation and
reality. The agent was deployed on an F1Tenth vehicle and competed against
opponents running varying competitive algorithms in the real world. The results
demonstrate that the agent's training against opponents enables deliberate
overtaking behaviours with an overtaking rate of 87% compared 56% for an agent
trained just to race.

</details>


### [137] [Morphology-Aware Graph Reinforcement Learning for Tensegrity Robot Locomotion](https://arxiv.org/abs/2510.26067)
*Chi Zhang,Mingrui Li,Wenzhe Tong,Xiaonan Huang*

Main category: cs.RO

TL;DR: 本文提出了一种结合图神经网络（GNN）与软演员评论家（SAC）算法的形态感知强化学习方法，有效提升了张拉整体机器人在运动控制上的学习效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 张拉整体机器人结构复杂、动力学高度耦合且欠驱动，导致运动控制难度大，现有方法在应对这种高度耦合和形态相关的系统时表现有限，因此需开发能更好利用结构信息的学习框架。

Method: 将机器人的物理拓扑表示为图结构，采用GNN建模各组件间的耦合关系，并将GNN集成入SAC强化学习算法，通过引入结构先验信息，促进学习收敛和性能提升。

Result: 在物理3杆张拉整体机器人上进行多种运动实验（如直线行进和双向转向），结果显示新方法在采样效率、对噪声与刚度变化的鲁棒性、轨迹精度等方面均优于传统MLP策略；其策略可直接从仿真迁移至硬件，无需微调即可实现稳定运动。

Conclusion: 结构先验的引入显著提升了强化学习在张拉整体机器人控制中的表现，为相关机器人的自主运动控制提供了有效新思路。

Abstract: Tensegrity robots combine rigid rods and elastic cables, offering high
resilience and deployability but posing major challenges for locomotion control
due to their underactuated and highly coupled dynamics. This paper introduces a
morphology-aware reinforcement learning framework that integrates a graph
neural network (GNN) into the Soft Actor-Critic (SAC) algorithm. By
representing the robot's physical topology as a graph, the proposed GNN-based
policy captures coupling among components, enabling faster and more stable
learning than conventional multilayer perceptron (MLP) policies. The method is
validated on a physical 3-bar tensegrity robot across three locomotion
primitives, including straight-line tracking and bidirectional turning. It
shows superior sample efficiency, robustness to noise and stiffness variations,
and improved trajectory accuracy. Notably, the learned policies transfer
directly from simulation to hardware without fine-tuning, achieving stable
real-world locomotion. These results demonstrate the advantages of
incorporating structural priors into reinforcement learning for tensegrity
robot control.

</details>


### [138] [I don't Want You to Die: A Shared Responsibility Framework for Safeguarding Child-Robot Companionship](https://arxiv.org/abs/2510.26080)
*Fan Yang,Renkai Ma,Yaxin Hu,Michael Rodgers,Lingyao Li*

Main category: cs.RO

TL;DR: 本研究以社会机器人Moxie的服务中断为案例，调查了美国家长和非家长对于由此造成儿童情感伤害的责任划分看法，并提出了儿童与机器人关系中断后的共担责任框架。


<details>
  <summary>Details</summary>
Motivation: 社会机器人越来越多地被用来陪伴儿童，形成了强烈的情感纽带。当机器人服务突然终止时，儿童可能受到情感上的伤害，这引出了关于责任归属和如何减少伤害的重要问题。

Method: 本研究通过对72名美国参与者进行定性问卷调查，分析他们对机器人公司、父母、开发者和政府等责任方分担责任的看法，并考察这些看法如何受到政治立场和父母身份影响。

Result: 研究发现，社会普遍认为责任应由多个主体共同承担，但具体责任归属随政治立场和是否为家长而变化。关于服务是否应继续，意见分歧较大：支持者呼吁技术、财政及政府干预来保障服务继续，反对者关注企业现实和避免儿童不健康依赖。

Conclusion: 本研究建立了儿童机器人陪伴关系终止后的共担责任理论框架，为设计和政策提供了实证基础，旨在通过明确责任分配和争议来源，减少机器人服务中止对儿童造成的情感伤害。

Abstract: Social robots like Moxie are designed to form strong emotional bonds with
children, but their abrupt discontinuation can cause significant struggles and
distress to children. When these services end, the resulting harm raises
complex questions of who bears responsibility when children's emotional bonds
are broken. Using the Moxie shutdown as a case study through a qualitative
survey of 72 U.S. participants, our findings show that the responsibility is
viewed as a shared duty across the robot company, parents, developers, and
government. However, these attributions varied by political ideology and
parental status of whether they have children. Participants' perceptions of
whether the robot service should continue are highly polarized; supporters
propose technical, financial, and governmental pathways for continuity, while
opponents cite business realities and risks of unhealthy emotional dependency.
Ultimately, this research contributes an empirically grounded shared
responsibility framework for safeguarding child-robot companionship by
detailing how accountability is distributed and contested, informing concrete
design and policy implications to mitigate the emotional harm of robot
discontinuation.

</details>


### [139] [Beyond the Uncanny Valley: A Mixed-Method Investigation of Anthropomorphism in Protective Responses to Robot Abuse](https://arxiv.org/abs/2510.26082)
*Fan Yang,Lingyao Li,Yaxin Hu,Michael Rodgers,Renkai Ma*

Main category: cs.RO

TL;DR: 研究了不同程度的人形化对人们保护机器人反应的影响，发现“恐怖谷”阶段并未降低道德关注，反而激发了更强的保护冲动。


<details>
  <summary>Details</summary>
Motivation: 随着人形机器人逐渐普及并进入生活，人们对机器人施虐行为的道德反应变得重要。现有的CASA理论和恐怖谷理论主要关注社会认知和情感反应，很少涉及其在道德领域（如对机器人虐待）上的作用，因此本研究希望填补这一空白。

Method: 邀请201名参与者观看对不同人形化程度（低：蜘蛛型，中：两足型，高：类人型）机器人施虐的视频，通过三种方式进行数据收集：自报问卷（情绪、怪异感），自动面部表情分析（生理数据），以及定性访谈。

Result: 保护性反应不是随人形程度线性上升。中等人形化的两足机器人最容易引发不适（恐怖谷效应）和最强烈的生理愤怒表达。两足和类人型机器人都比蜘蛛型更容易激起愤怒和负罪感。定性分析显示，人形化程度越高，道德评判从技术/财产损害转为谴责行为者品德，立法建议也从财产保护扩展到准动物权利及社会责任。

Conclusion: 恐怖谷不仅没有削弱人们对机器人的道德关注，反而强化了保护冲动。研究对机器人设计、政策制定和未来法律框架具有重要启示。

Abstract: Robots with anthropomorphic features are increasingly shaping how humans
perceive and morally engage with them. Our research investigates how different
levels of anthropomorphism influence protective responses to robot abuse,
extending the Computers as Social Actors (CASA) and uncanny valley theories
into a moral domain. In an experiment, we invite 201 participants to view
videos depicting abuse toward a robot with low (Spider), moderate (Two-Foot),
or high (Humanoid) anthropomorphism. To provide a comprehensive analysis, we
triangulate three modalities: self-report surveys measuring emotions and
uncanniness, physiological data from automated facial expression analysis, and
qualitative reflections. Findings indicate that protective responses are not
linear. The moderately anthropomorphic Two-Foot robot, rated highest in
eeriness and "spine-tingling" sensations consistent with the uncanny valley,
elicited the strongest physiological anger expressions. Self-reported anger and
guilt are significantly higher for both the Two-Foot and Humanoid robots
compared to the Spider. Qualitative findings further reveal that as
anthropomorphism increases, moral reasoning shifts from technical assessments
of property damage to condemnation of the abuser's character, while governance
proposals expand from property law to calls for quasi-animal rights and broader
societal responsibility. These results suggest that the uncanny valley does not
dampen moral concern but paradoxically heightens protective impulses, offering
critical implications for robot design, policy, and future legal frameworks.

</details>


### [140] [Embodied Intelligence for Advanced Bioinspired Microrobotics: Examples and Insights](https://arxiv.org/abs/2510.26132)
*Nestor O. Perez-Arancibia*

Main category: cs.RO

TL;DR: 本论文提出将具身智能（EI）作为微型机器人设计的重要原则，通过物理结构与控制策略的共同设计，提升机器人智能行为的生成能力。作者展示了多款采用该设计理念的机器人，并强调了结构与行为协同设计（co-design）的重要性。


<details>
  <summary>Details</summary>
Motivation: 微型机器人的感知、决策与运动通常采用分离式架构，难以在尺寸、能耗等受限条件下高效实现复杂行为。因此，需要新的设计理念，将结构、材料、环境交互与控制一体化，提升微型机器人的智能化水平。

Method: 作者采用具身智能理论，强调物理结构与行为功能的协同设计（co-design），并通过一系列自主研发的微型机器人（如Bee++、RoBeetle等）作为实例，展示了反馈、感知与驱动机制如何深度嵌入机器人本体中。论文对比了此方法与传统架构的区别，并系统分析不同平台的设计要素。

Result: 通过所举的多个微型机器人系统实例，验证了结构行为协同设计能够催生出结构动力学和物理交互驱动的复杂智能行为。各案例展示了基于物理属性嵌入反馈、决策与感知的具体实现及优势。

Conclusion: 结构与行为的协同设计不仅仅是满足约束下的工程实践，更是实现具身智能、提升机器人自适应性与健壮性的关键途径，尤其适合于毫米到厘米级机器人系统，并为突破传统控制方法提供了新思路。

Abstract: The term embodied intelligence (EI) conveys the notion that body morphology,
material properties, interaction with the environment, and control strategies
can be purposefully integrated into the process of robotic design to generate
intelligent behavior; in particular, locomotion and navigation. In this paper,
we discuss EI as a design principle for advanced microrobotics, with a
particular focus on co-design -- the simultaneous and interdependent
development of physical structure and behavioral function. To illustrate the
contrast between EI-inspired systems and traditional architectures that
decouple sensing, computation, and actuation, we present and discuss a
collection of robots developed by the author and his team at the Autonomous
Microrobotic Systems Laboratory (AMSL). These robots exhibit intelligent
behavior that emerges from their structural dynamics and the physical
interaction between their components and with the environment. Platforms such
as the Bee++, RoBeetle, SMALLBug, SMARTI, WaterStrider, VLEIBot+, and FRISSHBot
exemplify how feedback loops, decision logics, sensing mechanisms, and smart
actuation strategies can be embedded into the physical properties of the
robotic system itself. Along these lines, we contend that co-design is not only
a method for empirical optimization under constraints, but also an enabler of
EI, offering a scalable and robust alternative to classical control for
robotics at the mm-to-cm-scale.

</details>


### [141] [Kinodynamic Task and Motion Planning using VLM-guided and Interleaved Sampling](https://arxiv.org/abs/2510.26139)
*Minseo Kwon,Young J. Kim*

Main category: cs.RO

TL;DR: 该论文提出了一种结合视觉语言模型（VLM）和动力学约束的任务与运动规划（TAMP）新框架，并在仿真和现实任务中显著提升了成功率和规划效率。


<details>
  <summary>Details</summary>
Motivation: 传统TAMP方法在长时规划任务中由于运动采样过度导致计算代价高。虽然大语言模型（LLM）能提供常识，但缺乏三维空间推理能力，无法保证几何/动力学可行性。

Method: 提出基于混合状态树的动力学TAMP框架，统一表示符号与数值状态，并通过集成运动规划器与物理模拟器检验动力学约束，利用VLM基于状态视觉渲染指导搜索与回溯。

Result: 在仿真和现实任务测试中，相较传统与基于LLM的TAMP方法，平均成功率提升32.14%-1166.67%，并减少了在复杂问题上的规划时间。消融实验进一步验证了VLM引导的效果。

Conclusion: 该方法有效提升TAMP在复杂、现实场景下的效率和成功率，尤其凸显了VLM在空间推理和搜索引导中的作用，是TAMP领域的重要进展。

Abstract: Task and Motion Planning (TAMP) integrates high-level task planning with
low-level motion feasibility, but existing methods are costly in long-horizon
problems due to excessive motion sampling. While LLMs provide commonsense
priors, they lack 3D spatial reasoning and cannot ensure geometric or dynamic
feasibility. We propose a kinodynamic TAMP framework based on a hybrid state
tree that uniformly represents symbolic and numeric states during planning,
enabling task and motion decisions to be jointly decided. Kinodynamic
constraints embedded in the TAMP problem are verified by an off-the-shelf
motion planner and physics simulator, and a VLM guides exploring a TAMP
solution and backtracks the search based on visual rendering of the states.
Experiments on the simulated domains and in the real world show 32.14% -
1166.67% increased average success rates compared to traditional and LLM-based
TAMP planners and reduced planning time on complex problems, with ablations
further highlighting the benefits of VLM guidance.

</details>


### [142] [Adaptive Trajectory Refinement for Optimization-based Local Planning in Narrow Passages](https://arxiv.org/abs/2510.26142)
*Hahjin Lee,Young J. Kim*

Main category: cs.RO

TL;DR: 该论文提出了一种用于移动机器人在复杂环境中轨迹规划的自适应轨迹细化算法，显著提升了通过狭窄通道的安全性与规划效率。


<details>
  <summary>Details</summary>
Motivation: 在复杂、障碍物密集的环境中，传统的轨迹规划方法在狭窄通道容易失效或生成次优路径。作者希望解决移动机器人在此类环境中轨迹规划的安全性与效率问题。

Method: 算法分为两个阶段：第一阶段，通过分段的保守型碰撞检测，对存在风险的轨迹段递归细分，直至消除碰撞风险；第二阶段，对轨迹中的每个姿态点采用基于侵入方向和线搜索的姿态修正方法，确保每个姿态点均无碰撞且尽量远离障碍物。

Result: 算法在仿真中，相较于现有方法，实现了最高1.69倍的成功率提升及高达3.79倍的规划速度提升；实地实验亦验证了机器人能以安全且高效的方式通过狭窄通道。

Conclusion: 提出的方法能有效提高复杂环境中机器人轨迹规划的安全性与效率，尤其在狭窄通道场景下表现出卓越性能。

Abstract: Trajectory planning for mobile robots in cluttered environments remains a
major challenge due to narrow passages, where conventional methods often fail
or generate suboptimal paths. To address this issue, we propose the adaptive
trajectory refinement algorithm, which consists of two main stages. First, to
ensure safety at the path-segment level, a segment-wise conservative collision
test is applied, where risk-prone trajectory path segments are recursively
subdivided until collision risks are eliminated. Second, to guarantee
pose-level safety, pose correction based on penetration direction and line
search is applied, ensuring that each pose in the trajectory is collision-free
and maximally clear from obstacles. Simulation results demonstrate that the
proposed method achieves up to 1.69x higher success rates and up to 3.79x
faster planning times than state-of-the-art approaches. Furthermore, real-world
experiments confirm that the robot can safely pass through narrow passages
while maintaining rapid planning performance.

</details>


### [143] [Self-localization on a 3D map by fusing global and local features from a monocular camera](https://arxiv.org/abs/2510.26170)
*Satoshi Kikuch,Masaya Kato,Tsuyoshi Tasaki*

Main category: cs.RO

TL;DR: 提出了一种结合CNN和Vision Transformer的新方法，用于提升基于廉价单目摄像头的3D地图自定位在动态障碍场景下的准确性，实验证明精度优于当前SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 传统使用CNN进行图像自定位的方法在遇到动态障碍物（如行人）时表现不佳，因此需要一种更鲁棒的方法，特别是在自动驾驶和实际应用中。

Method: 将CNN提取的局部特征与Vision Transformer提取的全局特征相结合，增强特征表达能力，从而提升在动态环境下的定位精度。

Result: 新方法在包含动态障碍的CG数据集上，相比SOTA方法，精度提升率高1.5倍，且在公开数据集上的自定位误差比SOTA低20.1%。实际机器人实验中实现了平均7.51cm的定位误差，精度亦高于SOTA。

Conclusion: 结合CNN和Vision Transformer的方法能有效抵抗动态障碍，提高自定位的准确率，具有较强实际应用价值，优于当前主流方法。

Abstract: Self-localization on a 3D map by using an inexpensive monocular camera is
required to realize autonomous driving. Self-localization based on a camera
often uses a convolutional neural network (CNN) that can extract local features
that are calculated by nearby pixels. However, when dynamic obstacles, such as
people, are present, CNN does not work well. This study proposes a new method
combining CNN with Vision Transformer, which excels at extracting global
features that show the relationship of patches on whole image. Experimental
results showed that, compared to the state-of-the-art method (SOTA), the
accuracy improvement rate in a CG dataset with dynamic obstacles is 1.5 times
higher than that without dynamic obstacles. Moreover, the self-localization
error of our method is 20.1% smaller than that of SOTA on public datasets.
Additionally, our robot using our method can localize itself with 7.51cm error
on average, which is more accurate than SOTA.

</details>


### [144] [PHUMA: Physically-Grounded Humanoid Locomotion Dataset](https://arxiv.org/abs/2510.26236)
*Kyungmin Lee,Sibeen Kim,Minho Park,Hyunseung Kim,Dongyoon Hwang,Hojoon Lee,Jaegul Choo*

Main category: cs.RO

TL;DR: 本文提出了一个新的人形机器人动作模仿数据集PHUMA，能大规模采集人类视频动作，并通过物理约束技术提升动作的物理真实性和多样性，实验中显著优于现有数据集。


<details>
  <summary>Details</summary>
Motivation: 当前动作模仿多依赖昂贵且稀缺的高质量动作捕捉数据集（如AMASS），难以扩展和提升多样性。虽然已有方法尝试利用互联网视频获取大规模数据（如Humanoid-X），但常常会引入动作物理不合理的问题（如漂浮、穿模、脚打滑），影响模仿效果的真实感和稳定性。

Method: PHUMA数据集通过两个关键技术：1）大规模筛选高质量人类视频，并进行人工策划以去除常见物理瑕疵；2）采用物理约束的动作重定向方法，保证动作符号关节极限、地面接触和脚部无打滑，使得最终获得的数据既大规模又物理有效。

Result: 在两种场景下评估PHUMA：1）模仿来自自录视频的未知动作，2）仅用骨盆引导的路径跟随。实验证明，PHUMA训练出来的策略在动作多样性和模仿准确度上都显著超过Humanoid-X和AMASS。

Conclusion: PHUMA数据集不仅解决了大规模采集与物理可靠性之间的矛盾，还显著提升了动作模仿的效果，对人形机器人运动学习具有重要推动作用。

Abstract: Motion imitation is a promising approach for humanoid locomotion, enabling
agents to acquire humanlike behaviors. Existing methods typically rely on
high-quality motion capture datasets such as AMASS, but these are scarce and
expensive, limiting scalability and diversity. Recent studies attempt to scale
data collection by converting large-scale internet videos, exemplified by
Humanoid-X. However, they often introduce physical artifacts such as floating,
penetration, and foot skating, which hinder stable imitation. In response, we
introduce PHUMA, a Physically-grounded HUMAnoid locomotion dataset that
leverages human video at scale, while addressing physical artifacts through
careful data curation and physics-constrained retargeting. PHUMA enforces joint
limits, ensures ground contact, and eliminates foot skating, producing motions
that are both large-scale and physically reliable. We evaluated PHUMA in two
sets of conditions: (i) imitation of unseen motion from self-recorded test
videos and (ii) path following with pelvis-only guidance. In both cases,
PHUMA-trained policies outperform Humanoid-X and AMASS, achieving significant
gains in imitating diverse motions. The code is available at
https://davian-robotics.github.io/PHUMA.

</details>


### [145] [Thor: Towards Human-Level Whole-Body Reactions for Intense Contact-Rich Environments](https://arxiv.org/abs/2510.26280)
*Gangyang Li,Qing Shi,Youhao Hu,Jincheng Hu,Zhongyuan Wang,Xinlong Wang,Shaqi Luo*

Main category: cs.RO

TL;DR: 该论文提出了Thor框架，使仿人机器人在与环境强接触力互动下表现出类似人类的全身反应，有效提升了力量互动能力，并在实际机器人上实现了显著突破。


<details>
  <summary>Details</summary>
Motivation: 仿人机器人在服务、工业和救援场景中常需与环境进行高强度接触，但要使机器人具备人类般适应性反应仍然极具挑战。

Method: 提出基于机器人的力分析，设计了力自适应躯干倾斜（FAT2）奖励函数，鼓励机器人在力互动任务中展现人类般的反应。同时，Thor采用强化学习结构，将上身、腰部和下身控制任务解耦，各部分共享全身观测并协同更新参数。

Result: Thor架构部署在Unitree G1机器人上，显著优于其他基线方法。机器人在拉动任务中，后退拉力最大167.7N，前进145.5N，分别比最佳基线提升68.9%和74.7%。还能拉动载重货架（130N）和单手开防火门（60N）。

Conclusion: Thor框架显著提升了仿人机器人在强力互动任务中的稳定性和人类相似性，为机器人实用化迈出重要一步。

Abstract: Humanoids hold great potential for service, industrial, and rescue
applications, in which robots must sustain whole-body stability while
performing intense, contact-rich interactions with the environment. However,
enabling humanoids to generate human-like, adaptive responses under such
conditions remains a major challenge. To address this, we propose Thor, a
humanoid framework for human-level whole-body reactions in contact-rich
environments. Based on the robot's force analysis, we design a force-adaptive
torso-tilt (FAT2) reward function to encourage humanoids to exhibit human-like
responses during force-interaction tasks. To mitigate the high-dimensional
challenges of humanoid control, Thor introduces a reinforcement learning
architecture that decouples the upper body, waist, and lower body. Each
component shares global observations of the whole body and jointly updates its
parameters. Finally, we deploy Thor on the Unitree G1, and it substantially
outperforms baselines in force-interaction tasks. Specifically, the robot
achieves a peak pulling force of 167.7 N (approximately 48% of the G1's body
weight) when moving backward and 145.5 N when moving forward, representing
improvements of 68.9% and 74.7%, respectively, compared with the
best-performing baseline. Moreover, Thor is capable of pulling a loaded rack
(130 N) and opening a fire door with one hand (60 N). These results highlight
Thor's effectiveness in enhancing humanoid force-interaction capabilities.

</details>


### [146] [AgriGS-SLAM: Orchard Mapping Across Seasons via Multi-View Gaussian Splatting SLAM](https://arxiv.org/abs/2510.26358)
*Mirko Usuelli,David Rapado-Rincon,Gert Kootstra,Matteo Matteucci*

Main category: cs.RO

TL;DR: 本文提出了一种名为AgriGS-SLAM的新型视觉-激光雷达SLAM系统，能够在果园等户外环境中实现鲁棒、实时的3D场景感知，并在多季节、多环境下表现优异。


<details>
  <summary>Details</summary>
Motivation: 果园自动机器人面临具有重复性行几何结构、季节性外观变化和风吹树叶等挑战，现有SLAM方法在真实果园环境下易受遮挡、外观变化等影响，3D建图与定位精度受到限制。因此需要一种兼具多模态感知、鲁棒性和实时性的新型SLAM方法。

Method: AgriGS-SLAM融合视觉与激光雷达，结合直接激光雷达里程计、回环检测与多相机的3D Gaussian Splatting（3DGS）渲染。通过多视角批量光栅化提升遮挡条件下的结构恢复，并在关键帧之间以统一的梯度驱动地图管理方式保细节兼控内存。位姿优化引入概率化的激光雷达深度一致性约束，通过相机投影反向传播以耦合几何与外观。系统部署于果园机器人平台, 通过标准化轨迹评测协议在不同季节和果园测试，包括训练视角与新视角建图。

Result: AgriGS-SLAM在不同季节、不同果园场景下表现优于最新3DGS-SLAM基线，能输出更清晰、稳定的3D重建与更平滑的轨迹，同时保持实时运行能力。新提出的评测方案也有效降低了3DGS评测时的过拟合。

Conclusion: AgriGS-SLAM系统在果园场景下兼具鲁棒性、准确性与实时性，适用于需要稳定多模态感知的户外领域，为农业自动化与户外机器人感知提供了新方案。

Abstract: Autonomous robots in orchards require real-time 3D scene understanding
despite repetitive row geometry, seasonal appearance changes, and wind-driven
foliage motion. We present AgriGS-SLAM, a Visual--LiDAR SLAM framework that
couples direct LiDAR odometry and loop closures with multi-camera 3D Gaussian
Splatting (3DGS) rendering. Batch rasterization across complementary viewpoints
recovers orchard structure under occlusions, while a unified gradient-driven
map lifecycle executed between keyframes preserves fine details and bounds
memory. Pose refinement is guided by a probabilistic LiDAR-based depth
consistency term, back-propagated through the camera projection to tighten
geometry-appearance coupling. We deploy the system on a field platform in apple
and pear orchards across dormancy, flowering, and harvesting, using a
standardized trajectory protocol that evaluates both training-view and
novel-view synthesis to reduce 3DGS overfitting in evaluation. Across seasons
and sites, AgriGS-SLAM delivers sharper, more stable reconstructions and
steadier trajectories than recent state-of-the-art 3DGS-SLAM baselines while
maintaining real-time performance on-tractor. While demonstrated in orchard
monitoring, the approach can be applied to other outdoor domains requiring
robust multimodal perception.

</details>


### [147] [Cooperative Task Spaces for Multi-Arm Manipulation Control based on Similarity Transformations](https://arxiv.org/abs/2510.26362)
*Tobias Löw,Cem Bilaloglu,Sylvain Calinon*

Main category: cs.RO

TL;DR: 论文提出了一种基于共形几何代数的多臂机器人协作任务空间理论基础，实现复杂机器人系统的统一抽象与高效控制。方法已在多臂、仿人及多指系统的最优控制和遥操作实验中展示。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统（如多臂、多手指协作）在处理大型物体或实现精细操作时，运动协调极为复杂，当前模型难以描述高自由度系统的协作任务空间，亟需理论突破以简化建模和控制。

Method: 基于共形几何代数定义的几何基元，推导多臂协作机器人系统的任务空间理论，利用相似变换实现对多臂系统的抽象建模，使其与单臂系统的描述对齐，并推导相关的Jacobian矩阵，便于集成到经典控制框架中。

Result: 方法在二臂机器人、仿人机器人及多指机械手的最优控制与遥操作控制实验中验证，可简方便捷地实现协作几何目标的达成，并具备融入二级控制目标的拓展性。

Conclusion: 提出的基于几何基元的多臂协作任务空间理论有效统一了复杂协作系统的描述和控制基础，为未来机器人多任务、灵巧操作、协作控制提供了坚实理论依据。

Abstract: Many tasks in human environments require collaborative behavior between
multiple kinematic chains, either to provide additional support for carrying
big and bulky objects or to enable the dexterity that is required for in-hand
manipulation. Since these complex systems often have a very high number of
degrees of freedom coordinating their movements is notoriously difficult to
model. In this article, we present the derivation of the theoretical
foundations for cooperative task spaces of multi-arm robotic systems based on
geometric primitives defined using conformal geometric algebra. Based on the
similarity transformations of these cooperative geometric primitives, we derive
an abstraction of complex robotic systems that enables representing these
systems in a way that directly corresponds to single-arm systems. By deriving
the associated analytic and geometric Jacobian matrices, we then show the
straightforward integration of our approach into classical control techniques
rooted in operational space control. We demonstrate this using bimanual
manipulators, humanoids and multi-fingered hands in optimal control experiments
for reaching desired geometric primitives and in teleoperation experiments
using differential kinematics control. We then discuss how the geometric
primitives naturally embed nullspace structures into the controllers that can
be exploited for introducing secondary control objectives. This work,
represents the theoretical foundations of this cooperative manipulation control
framework, and thus the experiments are presented in an abstract way, while
giving pointers towards potential future applications.

</details>


### [148] [Towards Reinforcement Learning Based Log Loading Automation](https://arxiv.org/abs/2510.26363)
*Ilya Kurinov,Miroslav Ivanov,Grzegorz Orzechowski,Aki Mikkola*

Main category: cs.RO

TL;DR: 本文提出利用强化学习方法实现林业运输机的自动化原木装载操作，并在仿真环境下取得了94%的抓取搬运成功率。


<details>
  <summary>Details</summary>
Motivation: 林业运输机操作复杂且对操作员身心负担大，自动化技术可缓解操作压力。此前多专注于抓取环节，完整的全流程自动装载尚未实现。

Method: 设计并开发了基于NVIDIA Isaac Gym平台的林业运输机仿真模型与典型原木装载虚拟环境。采用强化学习和课程学习训练智能体，实现了从定位、抓取到搬运和投放原木的全流程操作。

Result: 最优强化学习智能体能够以94%的成功率自动完成从任意位置抓取原木并搬运至车厢的任务。

Conclusion: 本研究验证了利用强化学习实现林业运输机自动化的可行性和有效性，为进一步工程化和现场应用自动原木装载奠定了基础。

Abstract: Forestry forwarders play a central role in mechanized timber harvesting by
picking up and moving logs from the felling site to a processing area or a
secondary transport vehicle. Forwarder operation is challenging and physically
and mentally exhausting for the operator who must control the machine in remote
areas for prolonged periods of time. Therefore, even partial automation of the
process may reduce stress on the operator. This study focuses on continuing
previous research efforts in application of reinforcement learning agents in
automating log handling process, extending the task from grasping which was
studied in previous research to full log loading operation. The resulting agent
will be capable to automate a full loading procedure from locating and
grappling to transporting and delivering the log to a forestry forwarder bed.
To train the agent, a trailer type forestry forwarder simulation model in
NVIDIA's Isaac Gym and a virtual environment for a typical log loading scenario
were developed. With reinforcement learning agents and a curriculum learning
approach, the trained agent may be a stepping stone towards application of
reinforcement learning agents in automation of the forestry forwarder. The
agent learnt grasping a log in a random position from grapple's random position
and transport it to the bed with 94% success rate of the best performing agent.

</details>


### [149] [Human-in-the-loop Online Rejection Sampling for Robotic Manipulation](https://arxiv.org/abs/2510.26406)
*Guanxing Lu,Rui Zhao,Haitao Lin,He Zhang,Yansong Tang*

Main category: cs.RO

TL;DR: 提出Hi-ORS方法，用于机器人操控的高效且稳定的视觉-语言-动作模型微调，通过拒绝采样和奖励加权监督实现，实验优于传统RL和IL方法。


<details>
  <summary>Details</summary>
Motivation: RL虽然用于机器人操控表现强大，但在微调视觉-语言-动作模型时不稳定；而IL易于训练但效果有限。因此需要一种结合两者优点、能稳定高效微调的方案。

Method: 提出Hi-ORS后处理训练方法：利用在线训练中的拒绝采样，过滤负奖励样本提升值估计稳定性，并采用奖励加权的监督学习目标提供密集的中间过程监督。还开发了异步推理-训练体系，允许人类在线纠正，引导模型错误恢复。

Result: 在三个现实任务和两种机器人设备上，Hi-ORS能在仅1.5小时训练内，将初始策略高效微调以掌握复杂操作任务，且在效果和效率上均显著优于RL和IL基线。

Conclusion: Hi-ORS方法不仅提升了模型训练的稳定性和效率，还具备强大的测试时可扩展性与错误恢复能力，是机器人操控中极具潜力的在线微调解决方案。

Abstract: Reinforcement learning (RL) is widely used to produce robust robotic
manipulation policies, but fine-tuning vision-language-action (VLA) models with
RL can be unstable due to inaccurate value estimates and sparse supervision at
intermediate steps. In contrast, imitation learning (IL) is easy to train but
often underperforms due to its offline nature. In this paper, we propose
Hi-ORS, a simple yet effective post-training method that utilizes rejection
sampling to achieve both training stability and high robustness. Hi-ORS
stabilizes value estimation by filtering out negatively rewarded samples during
online fine-tuning, and adopts a reward-weighted supervised training objective
to provide dense intermediate-step supervision. For systematic study, we
develop an asynchronous inference-training framework that supports flexible
online human-in-the-loop corrections, which serve as explicit guidance for
learning error-recovery behaviors. Across three real-world tasks and two
embodiments, Hi-ORS fine-tunes a pi-base policy to master contact-rich
manipulation in just 1.5 hours of real-world training, outperforming RL and IL
baselines by a substantial margin in both effectiveness and efficiency.
Notably, the fine-tuned policy exhibits strong test-time scalability by
reliably executing complex error-recovery behaviors to achieve better
performance.

</details>


### [150] [RoboOS-NeXT: A Unified Memory-based Framework for Lifelong, Scalable, and Robust Multi-Robot Collaboration](https://arxiv.org/abs/2510.26536)
*Huajie Tan,Cheng Chi,Xiansheng Chen,Yuheng Ji,Zhongxia Zhao,Xiaoshuai Hao,Yaoxu Lyu,Mingyu Cao,Junkai Zhao,Huaihai Lyu,Enshen Zhou,Ning Chen,Yankai Fu,Cheng Peng,Wei Guo,Dong Liang,Zhuo Chen,Mengsi Lyu,Chenrui He,Yulong Ao,Yonghua Lin,Pengwei Wang,Zhongyuan Wang,Shanghang Zhang*

Main category: cs.RO

TL;DR: RoboOS-NeXT是一种为多机器人系统设计的统一记忆框架，实现了终身学习、可扩展协调和鲁棒调度，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着协作机器人在各种任务和形态中广泛应用，如何实现多智能体系统的长期适应性、规模化协作和强鲁棒性成为核心难题。现有方法受限于分散或有限的记忆，难以支持长期学习、多样化团队扩展和故障恢复。因此急需一种统一的记忆表示。

Method: 提出了RoboOS-NeXT框架，核心是空间-时间-体现(STEM)记忆，融合空间几何、时间事件和体现信息，形成多机器人共享的统一记忆表示。该框架采用类脑-小脑架构，高层负责全局规划和记忆操作，低层控制局部动作，实现认知、记忆与执行的闭环互动。

Result: 在餐厅、超市、家庭等多场景下，进行了复杂协调任务实验。结果显示RoboOS-NeXT在多样化机器人系统中性能优越，尤其体现在终身学习、可扩展性和容错协作上。

Conclusion: RoboOS-NeXT通过统一记忆与认知架构，有效解决了多机器人长期适应、多样性协作及鲁棒性难题，推动了多机器人协作系统的发展。

Abstract: The proliferation of collaborative robots across diverse tasks and
embodiments presents a central challenge: achieving lifelong adaptability,
scalable coordination, and robust scheduling in multi-agent systems. Existing
approaches, from vision-language-action (VLA) models to hierarchical
frameworks, fall short due to their reliance on limited or dividual-agent
memory. This fundamentally constrains their ability to learn over long
horizons, scale to heterogeneous teams, or recover from failures, highlighting
the need for a unified memory representation. To address these limitations, we
introduce RoboOS-NeXT, a unified memory-based framework for lifelong, scalable,
and robust multi-robot collaboration. At the core of RoboOS-NeXT is the novel
Spatio-Temporal-Embodiment Memory (STEM), which integrates spatial scene
geometry, temporal event history, and embodiment profiles into a shared
representation. This memory-centric design is integrated into a
brain-cerebellum framework, where a high-level brain model performs global
planning by retrieving and updating STEM, while low-level controllers execute
actions locally. This closed loop between cognition, memory, and execution
enables dynamic task allocation, fault-tolerant collaboration, and consistent
state synchronization. We conduct extensive experiments spanning complex
coordination tasks in restaurants, supermarkets, and households. Our results
demonstrate that RoboOS-NeXT achieves superior performance across heterogeneous
embodiments, validating its effectiveness in enabling lifelong, scalable, and
robust multi-robot collaboration. Project website:
https://flagopen.github.io/RoboOS/

</details>


### [151] [Adaptive Inverse Kinematics Framework for Learning Variable-Length Tool Manipulation in Robotics](https://arxiv.org/abs/2510.26551)
*Prathamesh Kothavale,Sravani Boddepalli*

Main category: cs.RO

TL;DR: 本文提出了一种创新框架，显著提升机器人在工具使用方面的自主能力，尤其是在不同长度工具选取、动作序列学习以及技能迁移到真实场景中。


<details>
  <summary>Details</summary>
Motivation: 传统机器人对自身运动学理解有限，只能完成预编程任务，难以高效自由地使用工具。提升机器人工具操作能力，是实现自主复杂任务的关键。

Method: 扩展机器人的逆运动学求解器，并结合在模拟环境中学习的动作轨迹，实现机器人可通过不同长度工具完成一系列操作。并将该策略从模拟迁移到真实世界，通过实验进行验证。

Result: 扩展后的逆运动学求解器在现实中误差小于1厘米，训练得到的策略在模拟中平均误差为8厘米。两种不同长度工具的实验效果几乎无差异。

Conclusion: 通过对工具选择、姿态判断、动作执行等工具使用要素的全面探索，提升了机器人复杂工具操作能力，为自动化及灵活性任务奠定基础。

Abstract: Conventional robots possess a limited understanding of their kinematics and
are confined to preprogrammed tasks, hindering their ability to leverage tools
efficiently. Driven by the essential components of tool usage - grasping the
desired outcome, selecting the most suitable tool, determining optimal tool
orientation, and executing precise manipulations - we introduce a pioneering
framework. Our novel approach expands the capabilities of the robot's inverse
kinematics solver, empowering it to acquire a sequential repertoire of actions
using tools of varying lengths. By integrating a simulation-learned action
trajectory with the tool, we showcase the practicality of transferring acquired
skills from simulation to real-world scenarios through comprehensive
experimentation. Remarkably, our extended inverse kinematics solver
demonstrates an impressive error rate of less than 1 cm. Furthermore, our
trained policy achieves a mean error of 8 cm in simulation. Noteworthy, our
model achieves virtually indistinguishable performance when employing two
distinct tools of different lengths. This research provides an indication of
potential advances in the exploration of all four fundamental aspects of tool
usage, enabling robots to master the intricate art of tool manipulation across
diverse tasks.

</details>


### [152] [FLYINGTRUST: A Benchmark for Quadrotor Navigation Across Scenarios and Vehicles](https://arxiv.org/abs/2510.26588)
*Gang Li,Chunlei Zhai,Teng Wang,Shaun Li,Shangsong Jiang,Xiangwei Zhu*

Main category: cs.RO

TL;DR: 本文提出了FLYINGTRUST基准测试框架，用于系统化评估四旋翼无人机视觉导航算法在不同平台和场景中的稳健性。框架通过两个物理可解释指标建模飞行器能力，并结合多样场景和异构平台开展规范化评测，揭示了算法在平台能力和场景结构变化下的表现规律，为实际算法设计和评估提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有四旋翼视觉导航算法在不同平台和场景间迁移时表现差异大，增加了实地部署的风险和成本，缺乏早期系统性评估工具。为填补这一空白，作者希望提供一个标准化的高保真基准测试框架，帮助研究者更全面地理解算法稳健性。

Method: 提出FLYINGTRUST框架，利用最大推重比和最大角加速度两个指标精炼建模平台能力。框架包含多样化场景库、异构真实与虚拟平台，实行标准化评测流程和复合评分体系以平衡不同因素影响。对比评测主流优化和学习型导航方法，报告带不确定性的多次实验结果。

Result: 实验揭示，导航成功高度依赖平台能力及场景结构，不同算法在不同条件下表现出明显的偏好和失效模式。FLYINGTRUST框架可系统性辨别这些模式和影响因素。

Conclusion: 平台能力和场景结构需在算法设计、评估、选择中被全面考虑。论文的方法为开发能适应多平台多场景的稳健导航算法奠定了实验基础，促进后续相关研究。

Abstract: Visual navigation algorithms for quadrotors often exhibit a large variation
in performance when transferred across different vehicle platforms and scene
geometries, which increases the cost and risk of field deployment. To support
systematic early-stage evaluation, we introduce FLYINGTRUST, a high-fidelity,
configurable benchmarking framework that measures how platform kinodynamics and
scenario structure jointly affect navigation robustness. FLYINGTRUST models
vehicle capability with two compact, physically interpretable indicators:
maximum thrust-to-weight ratio and axis-wise maximum angular acceleration. The
benchmark pairs a diverse scenario library with a heterogeneous set of real and
virtual platforms and prescribes a standardized evaluation protocol together
with a composite scoring method that balances scenario importance, platform
importance and performance stability. We use FLYINGTRUST to compare
representative optimization-based and learning-based navigation approaches
under identical conditions, performing repeated trials per platform-scenario
combination and reporting uncertainty-aware metrics. The results reveal
systematic patterns: navigation success depends predictably on platform
capability and scene geometry, and different algorithms exhibit distinct
preferences and failure modes across the evaluated conditions. These
observations highlight the practical necessity of incorporating both platform
capability and scenario structure into algorithm design, evaluation, and
selection, and they motivate future work on methods that remain robust across
diverse platforms and scenarios.

</details>


### [153] [A Sliding-Window Filter for Online Continuous-Time Continuum Robot State Estimation](https://arxiv.org/abs/2510.26623)
*Spencer Teetaert,Sven Lilge,Jessica Burgner-Kahrs,Timothy D. Barfoot*

Main category: cs.RO

TL;DR: 本文提出了一种用于连续体机器人（CRs）连续时间状态估计的滑窗滤波器（SWF），兼顾高精度和高速计算优势，实现了在线和快于实时的估计表现。


<details>
  <summary>Details</summary>
Motivation: 现有随机状态估计算法难以在连续体机器人应用中兼顾精度与效率。滑窗法尽管高效但模型简化且缺乏随机性表达；而传统随机滤波器受限于实时速率，无法充分发挥作用。近期的连续时间估计虽然原理正确，但仅能离线计算。急需突破这一矛盾，实现高效、精确且具备随机表达的在线连续时间估计。

Method: 提出一种新型滑窗滤波器，用于连续体机器人连续时间状态的在线随机估计；该方法结合滑窗机制和随机性表达，同时优化了计算速度，使其快于实时运行。

Result: 所提出方法在精度上优于传统滤波器，同时能够实现在线、快于实时的连续时间估计能力，是首个专为连续体机器人设计的随机滑窗滤波器。

Conclusion: 该工作为连续体机器人随机状态估计提供了新的思路，有望推动该领域未来研究和应用。

Abstract: Stochastic state estimation methods for continuum robots (CRs) often struggle
to balance accuracy and computational efficiency. While several recent works
have explored sliding-window formulations for CRs, these methods are limited to
simplified, discrete-time approximations and do not provide stochastic
representations. In contrast, current stochastic filter methods must run at the
speed of measurements, limiting their full potential. Recent works in
continuous-time estimation techniques for CRs show a principled approach to
addressing this runtime constraint, but are currently restricted to offline
operation. In this work, we present a sliding-window filter (SWF) for
continuous-time state estimation of CRs that improves upon the accuracy of a
filter approach while enabling continuous-time methods to operate online, all
while running at faster-than-real-time speeds. This represents the first
stochastic SWF specifically designed for CRs, providing a promising direction
for future research in this area.

</details>


### [154] [REALMS2 -- Resilient Exploration And Lunar Mapping System 2 -- A Comprehensive Approach](https://arxiv.org/abs/2510.26638)
*Dave van der Meer,Loïck P. Chovet,Gabriel M. Garcia,Abhishek Bera,Miguel A. Olivares-Mendez*

Main category: cs.RO

TL;DR: 本文提出了一种新的多机器人系统（MRS）框架REALMS2，用于太空探测与地图绘制，在实地测试中表现出良好性能。


<details>
  <summary>Details</summary>
Motivation: ESA与ESRIC发起Space Resources Challenge，希望激励研究人员和企业为多机器人系统在太空资源探索任务中提出创新方案。外太空环境下，面对通讯延迟、断联、任务复杂等问题，需要一种能应对这些挑战的多机器人系统。

Method: 作者开发了基于ROS 2的REALMS2系统，集成了视觉同步定位与建图（vSLAM）技术用于地图生成，并采用mesh自组网络实现强健的机器间通讯。所有机器人通过单一GUI界面统一管控，适用于多异构机器人的联合探索任务。

Result: REALMS2系统在ESA-ESRIC挑战第二次实地测试中，使用三台同类型探测车，在管理通讯延迟和断联状况下，成功绘制了约60%测试区域的地图。

Conclusion: REALMS2展现了在外太空复杂任务环境下，多机器人系统的可行性和鲁棒性，为今后行星表面资源勘探任务的自动化与高效化提供了有力支撑。

Abstract: The European Space Agency (ESA) and the European Space Resources Innovation
Centre (ESRIC) created the Space Resources Challenge to invite researchers and
companies to propose innovative solutions for Multi-Robot Systems (MRS) space
prospection. This paper proposes the Resilient Exploration And Lunar Mapping
System 2 (REALMS2), a MRS framework for planetary prospection and mapping.
Based on Robot Operating System version 2 (ROS 2) and enhanced with Visual
Simultaneous Localisation And Mapping (vSLAM) for map generation, REALMS2 uses
a mesh network for a robust ad hoc network. A single graphical user interface
(GUI) controls all the rovers, providing a simple overview of the robotic
mission. This system is designed for heterogeneous multi-robot exploratory
missions, tackling the challenges presented by extraterrestrial environments.
REALMS2 was used during the second field test of the ESA-ESRIC Challenge and
allowed to map around 60% of the area, using three homogeneous rovers while
handling communication delays and blackouts.

</details>


### [155] [Hybrid DQN-TD3 Reinforcement Learning for Autonomous Navigation in Dynamic Environments](https://arxiv.org/abs/2510.26646)
*Xiaoyi He,Danggui Chen,Zhenshuo Zhang,Zimeng Bai*

Main category: cs.RO

TL;DR: 提出了一种结合高层DQN与低层TD3的分层路径规划与控制框架，通过多指标奖励机制和激光雷达安全门提高路径规划的鲁棒性与安全性。在仿真环境中展现出较单一算法及规则方法更优的表现。


<details>
  <summary>Details</summary>
Motivation: 现有路径规划方法在动态、部分可观测环境下难以同时兼顾高效规划与连续控制。单一强化学习算法或规则方法难以适应复杂环境变化，因此需要集成多层次、不同类型算法来提升系统表现。

Method: 高层采用DQN算法进行离散子目标/行为选择，低层采用TD3算法实现连续的速度控制。设计了包含多因素（方向、距离、避障、平顺性等）的奖励函数，并引入基于激光雷达的数据安全门以防止不安全动作。系统在ROS+Gazebo（TurtleBot3）平台实现，并采用PathBench标准指标进行评估。

Result: 在动态和部分可观测环境中，相较于单一算法（仅用DQN或TD3）及基于规则的路径规划器，该方法在成功率、样本效率、泛化能力（对新障碍情况）和控制的平滑性方面均表现更优。

Conclusion: 分层RL架构能有效融合高层规划和低层控制优势，通过奖励整形和感知安全措施提升路径规划的安全性和泛化能力，为移动机器人路径规划提供了更优的解决方案。

Abstract: This paper presents a hierarchical path-planning and control framework that
combines a high-level Deep Q-Network (DQN) for discrete sub-goal selection with
a low-level Twin Delayed Deep Deterministic Policy Gradient (TD3) controller
for continuous actuation. The high-level module selects behaviors and
sub-goals; the low-level module executes smooth velocity commands. We design a
practical reward shaping scheme (direction, distance, obstacle avoidance,
action smoothness, collision penalty, time penalty, and progress), together
with a LiDAR-based safety gate that prevents unsafe motions. The system is
implemented in ROS + Gazebo (TurtleBot3) and evaluated with PathBench metrics,
including success rate, collision rate, path efficiency, and re-planning
efficiency, in dynamic and partially observable environments. Experiments show
improved success rate and sample efficiency over single-algorithm baselines
(DQN or TD3 alone) and rule-based planners, with better generalization to
unseen obstacle configurations and reduced abrupt control changes. Code and
evaluation scripts are available at the project repository.

</details>


### [156] [Heuristic Adaptation of Potentially Misspecified Domain Support for Likelihood-Free Inference in Stochastic Dynamical Systems](https://arxiv.org/abs/2510.26656)
*Georgios Kamaras,Craig Innes,Subramanian Ramamoorthy*

Main category: cs.RO

TL;DR: 本文提出了三种新的无似然推断（LFI）变体，以改善在机器人部署中由支持集设定不当引发的后验失真问题。通过对支持集的动态调整，这些方法在参数推断与策略学习任务中取得了更优、更加稳健的结果。


<details>
  <summary>Details</summary>
Motivation: 传统LFI方法在多次迭代过程中采用固定的采样支持集。但若初始支持集选择不佳，容易导致生成的后验虽然自信但不准确，影响后续的参数推断和策略转移。为了解决支持集设定不合理带来的性能损失，作者希望通过动态调整支持集提升LFI效率和结果质量。

Method: 作者提出三种基于启发式的LFI变体（EDGE、MODE、CENTRE），它们通过跟踪后验随推断步骤的模式变化，以不同方式动态调整采样支持集。方法在随机动力学基准下进行对比分析，并进一步用于可变结构物体（如动力可变线性物体）操控任务，评估对参数推断和策略学习的提升。

Result: 实验表明，动态支持集调整（三种变体）可以显著提升参数分辨率（物体长度、刚度分类更精细），且在基于仿真学习得到的策略被用于实际部署时，表现出更强的稳健性和泛化性。

Conclusion: 针对LFI采样支持集误设问题，文章提出的动态支持集调整方法切实提高了后验推断与基于后验策略学习的整体性能，在参数敏感、高变异环境下特别有效。

Abstract: In robotics, likelihood-free inference (LFI) can provide the domain
distribution that adapts a learnt agent in a parametric set of deployment
conditions. LFI assumes an arbitrary support for sampling, which remains
constant as the initial generic prior is iteratively refined to more
descriptive posteriors. However, a potentially misspecified support can lead to
suboptimal, yet falsely certain, posteriors. To address this issue, we propose
three heuristic LFI variants: EDGE, MODE, and CENTRE. Each interprets the
posterior mode shift over inference steps in its own way and, when integrated
into an LFI step, adapts the support alongside posterior inference. We first
expose the support misspecification issue and evaluate our heuristics using
stochastic dynamical benchmarks. We then evaluate the impact of heuristic
support adaptation on parameter inference and policy learning for a dynamic
deformable linear object (DLO) manipulation task. Inference results in a finer
length and stiffness classification for a parametric set of DLOs. When the
resulting posteriors are used as domain distributions for sim-based policy
learning, they lead to more robust object-centric agent performance.

</details>


### [157] [Hybrid Consistency Policy: Decoupling Multi-Modal Diversity and Real-Time Efficiency in Robotic Manipulation](https://arxiv.org/abs/2510.26670)
*Qianyou Zhao,Yuliang Shen,Xuanran Zhai,Ce Hao,Duidi Wu,Jin Qi,Jie Hu,Qiaojun Yu*

Main category: cs.RO

TL;DR: 提出了一种新的混合一致性策略（HCP），能够在降低采样延迟的同时保持高多模态性，在模拟和真实机器人实验中效果接近传统方法但速度更快。


<details>
  <summary>Details</summary>
Motivation: 在视觉运动机政策学习中，现有扩散式模仿学习虽能捕捉多样行为，但在推理速度和多模态性表现间存在权衡：传统扩散模型采样慢，而加速手段又损失多模态性。该研究旨在解决这一瓶颈。

Method: 提出Hybrid Consistency Policy（HCP）：先运行一段短的随机扩散过程，在自适应切换时刻后，通过一步一致性跳跃输出最终动作。训练时采用时变一致性蒸馏，结合轨迹一致性与去噪匹配目标，保证预测局部和全局的合理性。

Result: 实验表明，HCP 只需25步SDE加一步jump就能在准确率和多模态性接近传统80步的DDPM教师模型，同时大幅减少延迟。结果覆盖仿真和真实机器人平台。

Conclusion: HCP验证了实现快速采样与强多模态性可以兼得，通过自适应切换时间解耦了多模态保留和推理速度，为机器人策略提供了一种兼顾精度与效率的实用方案。

Abstract: In visuomotor policy learning, diffusion-based imitation learning has become
widely adopted for its ability to capture diverse behaviors. However,
approaches built on ordinary and stochastic denoising processes struggle to
jointly achieve fast sampling and strong multi-modality. To address these
challenges, we propose the Hybrid Consistency Policy (HCP). HCP runs a short
stochastic prefix up to an adaptive switch time, and then applies a one-step
consistency jump to produce the final action. To align this one-jump
generation, HCP performs time-varying consistency distillation that combines a
trajectory-consistency objective to keep neighboring predictions coherent and a
denoising-matching objective to improve local fidelity. In both simulation and
on a real robot, HCP with 25 SDE steps plus one jump approaches the 80-step
DDPM teacher in accuracy and mode coverage while significantly reducing
latency. These results show that multi-modality does not require slow
inference, and a switch time decouples mode retention from speed. It yields a
practical accuracy efficiency trade-off for robot policies.

</details>


### [158] [Running VLAs at Real-time Speed](https://arxiv.org/abs/2510.26742)
*Yunchao Ma,Yizhuang Zhou,Yunhuan Yang,Tiancai Wang,Haoqiang Fan*

Main category: cs.RO

TL;DR: 本论文提出了一种利用单个消费级GPU以30Hz帧率和最高480Hz轨迹频率运行pi0级多视图VLA的方法，实现了以往大规模VLA模型难以达到的实时和动态任务。


<details>
  <summary>Details</summary>
Motivation: 以往大规模VLA（视觉-语言-动作）模型由于推理开销过大，难以在实时机器人控制等动态任务场景中应用。作者旨在突破该瓶颈，实现高频率、低延迟的VLA推理。

Method: 作者提出了一系列推理加速策略，消除了模型推理过程中的性能瓶颈，能够在消费级GPU上大幅提升pi0级多视图VLA的运行速度，并实现了流式推理框架。

Result: 实验证明，使用本文策略后，pi0 policy在抓取下落钢笔的机器人任务中达到了100%的成功率，并实现了高频率和实时推理。

Conclusion: 该方法实现在单个消费级GPU上高频率、实时VLA推理，为机器人等需要动态响应的任务提供了可行方案。并且代码已开源，便于进一步研究和应用。

Abstract: In this paper, we show how to run pi0-level multi-view VLA at 30Hz frame rate
and at most 480Hz trajectory frequency using a single consumer GPU. This
enables dynamic and real-time tasks that were previously believed to be
unattainable by large VLA models. To achieve it, we introduce a bag of
strategies to eliminate the overheads in model inference. The real-world
experiment shows that the pi0 policy with our strategy achieves a 100% success
rate in grasping a falling pen task. Based on the results, we further propose a
full streaming inference framework for real-time robot control of VLA. Code is
available at https://github.com/Dexmal/realtime-vla.

</details>
