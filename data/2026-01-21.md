<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 266]
- [cs.CL](#cs.CL) [Total: 184]
- [cs.RO](#cs.RO) [Total: 62]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Domain-Specific Self-Supervised Pre-training for Agricultural Disease Classification: A Hierarchical Vision Transformer Study](https://arxiv.org/abs/2601.11612)
*Arnav S. Sonavane*

Main category: cs.CV

TL;DR: 本论文研究了在农业病害分类中，基于领域的自监督预训练对分层视觉Transformer的影响。通过SimCLR自监督方法，仅用3000张无标签农业图像预训练，准确率提升了4.57%，甚至超过了分层架构本身带来的提升。该自监督效果对于不同Transformer架构均有效，证明领域数据采集比模型架构选择更关键。


<details>
  <summary>Details</summary>
Motivation: 目前农业病害识别常依赖大量标注数据与复杂模型架构。获取领域标注数据成本高，且新病害或作物种类不断出现，迫切需求提升模型在农业实际场景下的泛化能力与表现。

Method: 使用SimCLR方法在3000张无标签农业图片上进行自监督预训练，然后分别针对多种Transformer架构（包括分层Transformer-HVT和Swin、标准ViT）做微调，并在多个农业病害数据集（Cotton Leaf Disease、PlantVillage、PlantDoc）上进行效果评估。

Result: 自监督训练带来4.57%的准确率提升，高于分层结构本身带来的3.7%；且该提升与架构无关（Swin-Base提升4.08%、ViT-Base提升4.2%）；HVT-Base模型在各项数据集上取得优于Swin-Base的表现，且通过温度缩放可进一步校准输出。

Conclusion: 农业病害图像分类任务中，优先扩充领域内数据进行自监督预训练，比单纯更换或优化模型架构带来更大收益，因此建议优先投入领域数据采集。

Abstract: We investigate the impact of domain-specific self-supervised pre-training on agricultural disease classification using hierarchical vision transformers. Our key finding is that SimCLR pre-training on just 3,000 unlabeled agricultural images provides a +4.57% accuracy improvement--exceeding the +3.70% gain from hierarchical architecture design. Critically, we show this SSL benefit is architecture-agnostic: applying the same pre-training to Swin-Base yields +4.08%, to ViT-Base +4.20%, confirming practitioners should prioritize domain data collection over architectural choices. Using HierarchicalViT (HVT), a Swin-style hierarchical transformer, we evaluate on three datasets: Cotton Leaf Disease (7 classes, 90.24%), PlantVillage (38 classes, 96.3%), and PlantDoc (27 classes, 87.1%). At matched parameter counts, HVT-Base (78M) achieves 88.91% vs. Swin-Base (88M) at 87.23%, a +1.68% improvement. For deployment reliability, we report calibration analysis showing HVT achieves 3.56% ECE (1.52% after temperature scaling). Code: https://github.com/w2sg-arnav/HierarchicalViT

</details>


### [2] [Multi-modal MRI-Based Alzheimer's Disease Diagnosis with Transformer-based Image Synthesis and Transfer Learning](https://arxiv.org/abs/2601.11614)
*Jason Qiu*

Main category: cs.CV

TL;DR: 本文提出了一种3D TransUNet图像合成框架，可利用常规T1w MRI预测Alzheimer's病患者的高质量扩散MRI（FA和MD）图，实现早期微观结构异常检测，提高诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病早期异常在常规T1w MRI上难以发现，dMRI虽然敏感但耗时且易受伪影影响，限制了其临床应用。因此需要一种方法在节省时间和提高便利性的同时，能提供高质量扩散信息协助早筛。

Method: 作者设计了一种3D TransUNet神经网络，将T1w MRI输入，直接生成FA和MD图谱。该网络在结构相似度（SSIM）和Pearson相关上与真实dMRI数据高度一致，并将生成的扩散特征集成到多模态诊断模型提升分类准确率。

Result: 该方法生成的FA和MD图结构相似度达0.93以上，相关性超0.94。将合成特征用于阿尔兹海默分型分类器，能提升AD分类准确率5%，轻度认知障碍（MCI）检测率提高12.5%。

Conclusion: 通过深度学习合成扩散MRI特征，可实现临床常规T1w MRI中白质微结构的无损推断，提升AD早筛效率与准确率，有望促进多模态成像信息在实际临床的普及应用。

Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disorder in which pathological changes begin many years before the onset of clinical symptoms, making early detection essential for timely intervention. T1-weighted (T1w) Magnetic Resonance Imaging (MRI) is routinely used in clinical practice to identify macroscopic brain alterations, but these changes typically emerge relatively late in the disease course. Diffusion MRI (dMRI), in contrast, is sensitive to earlier microstructural abnormalities by probing water diffusion in brain tissue. dMRI metrics, including fractional anisotropy (FA) and mean diffusivity (MD), provide complementary information about white matter integrity and neurodegeneration. However, dMRI acquisitions are time-consuming and susceptible to motion artifacts, limiting their routine use in clinical populations. To bridge this gap, I propose a 3D TransUNet image synthesis framework that predicts FA and MD maps directly from T1w MRI. My model generates high-fidelity maps, achieving a structural similarity index (SSIM) exceeding 0.93 and a strong Pearson correlation (>0.94) with ground-truth dMRI. When integrated into a multi-modal diagnostic model, these synthetic features boost AD classification accuracy by 5% (78.75%->83.75%) and, most importantly, improve mild cognitive impairment (MCI) detection by 12.5%. This study demonstrates that high-quality diffusion microstructural information can be inferred from routinely acquired T1w MRI, effectively transferring the benefits of multi-modality imaging to settings where diffusion data are unavailable. By reducing scan time while preserving complementary structural and microstructural information, the proposed approach has the potential to improve the accessibility, efficiency, and accuracy of AD diagnosis in clinical practice.

</details>


### [3] [PointSLAM++: Robust Dense Neural Gaussian Point Cloud-based SLAM](https://arxiv.org/abs/2601.11617)
*Xu Wang,Boyao Han,Xiaojun Chen,Ying Liu,Ruihui Li*

Main category: cs.CV

TL;DR: PointSLAM++是一种新型RGB-D SLAM系统，通过神经高斯表示和动态优化，对三维复现场景表现出更高精度和更好一致性。


<details>
  <summary>Details</summary>
Motivation: 当前3D实时重建对机器人和增强现实至关重要，但现有SLAM方法在存在深度噪声时很难兼顾结构一致性和鲁棒位姿估计，有较大提升空间。

Method: 提出了分层约束的神经高斯表示体系，并通过动态图优化节点分布，采用渐进式位姿优化，动态调整高斯节点以适应不同场景结构复杂度，实现高精度地图和真实感渲染。

Result: 实验结果表明，PointSLAM++在重建精度和渲染质量上均优于现有基于3DGS的SLAM方法，尤其在大规模AR和机器人场景下表现突出。

Conclusion: PointSLAM++有效提升了三维重建的结构一致性和定位精度，具有极好的工程与应用前景。

Abstract: Real-time 3D reconstruction is crucial for robotics and augmented reality, yet current simultaneous localization and mapping(SLAM) approaches often struggle to maintain structural consistency and robust pose estimation in the presence of depth noise. This work introduces PointSLAM++, a novel RGB-D SLAM system that leverages a hierarchically constrained neural Gaussian representation to preserve structural relationships while generating Gaussian primitives for scene mapping. It also employs progressive pose optimization to mitigate depth sensor noise, significantly enhancing localization accuracy. Furthermore, it utilizes a dynamic neural representation graph that adjusts the distribution of Gaussian nodes based on local geometric complexity, enabling the map to adapt to intricate scene details in real time. This combination yields high-precision 3D mapping and photorealistic scene rendering. Experimental results show PointSLAM++ outperforms existing 3DGS-based SLAM methods in reconstruction accuracy and rendering quality, demonstrating its advantages for large-scale AR and robotics.

</details>


### [4] [Handcrafted Feature-Assisted One-Class Learning for Artist Authentication in Historical Drawings](https://arxiv.org/abs/2601.11627)
*Hassan Ugail,Jan Ritch-Frel,Irina Matuzava*

Main category: cs.CV

TL;DR: 该论文提出了一种基于计算机的历史手稿鉴定框架，利用少量可解释手工特征和单类自编码器进行艺术家作品的真实性验证，实验结果显示系统在真实和伪造测试中表现出较高的准确率。


<details>
  <summary>Details</summary>
Motivation: 在文化遗产领域，纸本作品的鉴定与归属一直面临挑战，尤其是在参考原作极少及风格信息主要通过线条和有限灰度表现时。现有方法受限于数据量，同时难以量化鉴定过程，因此需要开发适用于小样本且量化可复现的新方法。

Method: 作者提出使用一组可解释的手工特征（如傅里叶域能量、Shannon熵、全局对比度、GLCM均匀性和分形复杂度），结合单类自编码器，针对十位艺术家分别训练“验证器”。以指纹生物识别的方式进行真实性验证，用开放获得的权威艺术画作作为训练数据。系统针对每个艺术家，使用90个真实和810个伪造试验进行性能评估。

Result: 该方法在900组验证中，整体达到了83.3%的真实接受率（True Acceptance Rate）和9.5%的误接受率（False Acceptance Rate），具体到不同艺术家，性能差异较大。对误接受的归因分析显示，错误可能存在于风格接近或者绘图传统相似的艺术家之间，反映风格上的结构性混淆。

Conclusion: 该方法为历史素描鉴定提供了可复现、量化的补充工具，特别适用于数据稀缺场景，可辅助而非替代传统鉴定专家。未来需进一步优化数字化过程和门限设定，以降低误判。

Abstract: Authentication and attribution of works on paper remain persistent challenges in cultural heritage, particularly when the available reference corpus is small and stylistic cues are primarily expressed through line and limited tonal variation. We present a verification-based computational framework for historical drawing authentication using one-class autoencoders trained on a compact set of interpretable handcrafted features. Ten artist-specific verifiers are trained using authenticated sketches from the Metropolitan Museum of Art open-access collection, the Ashmolean Collections Catalogue, the Morgan Library and Museum, the Royal Collection Trust (UK), the Victoria and Albert Museum Collections, and an online catalogue of the Casa Buonarroti collection and evaluated under a biometric-style protocol with genuine and impostor trials. Feature vectors comprise Fourier-domain energy, Shannon entropy, global contrast, GLCM-based homogeneity, and a box-counting estimate of fractal complexity. Across 900 verification decisions (90 genuine and 810 impostor trials), the pooled system achieves a True Acceptance Rate of 83.3% with a False Acceptance Rate of 9.5% at the chosen operating point. Performance varies substantially by artist, with near-zero false acceptance for some verifiers and elevated confusability for others. A pairwise attribution of false accepts indicates structured error pathways consistent with stylistic proximity and shared drawing conventions, whilst also motivating tighter control of digitisation artefacts and threshold calibration. The proposed methodology is designed to complement, rather than replace, connoisseurship by providing reproducible, quantitative evidence suitable for data-scarce settings common in historical sketch attribution.

</details>


### [5] [A one-step generation model with a Single-Layer Transformer: Layer number re-distillation of FreeFlow](https://arxiv.org/abs/2601.11630)
*Haonan Wei,Linyuan Wang,Nuolin Sun,Zhizhong Zheng,Lei Li,Bin Yan*

Main category: cs.CV

TL;DR: 本文提出SLT（单层Transformer）方法，通过知识蒸馏，把28层大模型FreeFlow压缩为单层Transformer，实现极端参数精简和高效采样。同时利用快速筛选噪声点，提升生成图像质量和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于ODE的扩散模型如FreeFlow虽然能实现一步生成，但普遍结构庞大（如28层Transformer），参数量大、推理效率有限。作者希望进一步精简模型结构，并提高一步生成图像的稳定性和质量。

Method: 作者通过分析FreeFlow的28层Transformer结构等价于Depth轴上的欧拉离散，提出用单层Transformer（SLT）来蒸馏教师模型的深层特征演化。在训练中，SLT对齐教师模型不同深度的中间特征块，并融合Patch级表示，同时对齐教师最终速度预测，实现从675M参数到4.3M参数的极限压缩。此外，借助SLT的高效采样能力，在同样时间内筛选更多初始噪声点，选取最优点喂给教师模型生成最终图像。

Result: 在与FreeFlow模型两次随机采样等价的时间预算下，SLT可完成100多次噪声筛选，最终通过教师模型生成高质量样本，明显提升生成图像的平均质量与稳定性。参数量从675M大幅缩减至4.3M，采样速度显著提升。

Conclusion: SLT方法极大压缩了扩散模型的参数量，并借高效采样与筛选机制提升了一步生成的质量和稳定性，对实际部署和应用一键生成类扩散模型具有重要意义。

Abstract: Currently, Flow matching methods aim to compress the iterative generation process of diffusion models into a few or even a single step, with MeanFlow and FreeFlow being representative achievements of one-step generation based on Ordinary Differential Equations (ODEs). We observe that the 28-layer Transformer architecture of FreeFlow can be characterized as an Euler discretization scheme for an ODE along the depth axis, where the layer index serves as the discrete time step. Therefore, we distill the number of layers of the FreeFlow model, following the same derivation logic as FreeFlow, and propose SLT (Single-Layer Transformer), which uses a single shared DiT block to approximate the depth-wise feature evolution of the 28-layer teacher. During training, it matches the teacher's intermediate features at several depth patches, fuses those patch-level representations, and simultaneously aligns the teacher's final velocity prediction. Through distillation training, we compress the 28 independent Transformer Blocks of the teacher model DiT-XL/2 into a single Transformer Block, reducing the parameter count from 675M to 4.3M. Furthermore, leveraging its minimal parameters and rapid sampling speed, SLT can screen more candidate points in the noise space within the same timeframe, thereby selecting higher-quality initial points for the teacher model FreeFlow and ultimately enhancing the quality of generated images. Experimental results demonstrate that within a time budget comparable to two random samplings of the teacher model, our method performs over 100 noise screenings and produces a high-quality sample through the teacher model using the selected points. Quality fluctuations caused by low-quality initial noise under a limited number of FreeFlow sampling calls are effectively avoided, substantially improving the stability and average generation quality of one-step generation.

</details>


### [6] [Compress to Focus: Efficient Coordinate Compression for Policy Optimization in Multi-Turn GUI Agents](https://arxiv.org/abs/2601.11631)
*Yurun Song,Jiong Yin,Rongjunchen Zhang,Ian G. Harris*

Main category: cs.CV

TL;DR: 该论文提出了一种新的多轮GUI代理学习框架CCPO，有效缓解了多轮交互带来的上下文膨胀问题，并显著提升了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 多轮GUI代理在处理复杂任务时，需要逐步决策，使得历史信息不断累积，导致上下文膨胀，计算资源消耗大。现有方法要么截断历史信息，损失长期依赖，要么剪裁token，牺牲空间结构。因此亟需一种既保留关键信息又高效的方法。

Method: 作者提出Coordinate Compression Policy Optimization (CCPO)框架，结合视觉压缩与策略优化。提出的CASC（Coordinate-Aware Spatial Compression）方法，通过多轮回合的坐标聚合，动态聚焦目标相关区域，逐步收窄注意范围，仅保留最有效视觉信息。同时设计基于距离的奖励机制，使学习信号更细致，提高代理对目标位置的感知和压缩质量。

Result: 在四个基准测试上，CCPO实现了SOTA性能，token压缩率高达55%，训练速度提升3.8倍，并且在准确性和压缩质量上优于现有方法。

Conclusion: CCPO有效解决了多轮GUI任务中的上下文膨胀和计算瓶颈问题，在保持性能的同时极大提高了效率，为多轮交互式视觉任务提供了新的解决思路。

Abstract: Multi-turn GUI agents enable complex task completion through sequential decision-making, but suffer from severe context inflation as interaction history accumulates. Existing strategies either sacrifice long-term context via truncation or compromise spatial structure through token pruning. In this paper, we propose Coordinate Compression Policy Optimization (CCPO), an efficient policy optimization framework that couples visual compression with policy optimization for multi-turn GUI agents. CCPO introduces Coordinate-Aware Spatial Compression (CASC), which aggregates coordinates from multiple rollouts to capture target-relevant regions and progressively narrow historical attention around key visual areas. From interactions across rollouts, CASC adaptively constructs attention boundaries that concentrate computation on the most informative regions of the scene. We further design a Distance-Based Advantage that provides fine-grained learning signals based on distance rather than binary correctness, improving both grounding accuracy and compression quality. Extensive experiments demonstrate that CCPO achieves SOTA performance across four benchmarks with up to 55% token compression and 3.8$\times$ training speedup.

</details>


### [7] [KG-ViP: Bridging Knowledge Grounding and Visual Perception in Multi-modal LLMs for Visual Question Answering](https://arxiv.org/abs/2601.11632)
*Zhiyang Li,Ao Ke,Yukun Cao,Xike Xie*

Main category: cs.CV

TL;DR: 本文提出了KG-ViP框架，通过融合场景图和常识图，有效提升多模态大模型在视觉问答任务中的表现，并在多个基准测试上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在视觉问答领域常受制于知识幻觉和细粒度视觉感知不足两大问题。由于场景图能够细致捕捉视觉细节，常识图则提供丰富的外部知识，两者恰好互补，但此前研究往往单独处理，未结合其协同效应。

Method: 作者提出了KG-ViP统一框架，核心是创新的“检索-融合”流程，使用查询作为语义桥梁，逐步整合场景图和常识图，生成统一结构化上下文，促进更加可靠的多模态推理。

Result: 在FVQA 2.0+和MVQA等视觉问答基准集上，KG-ViP方法的性能显著优于以往VQA方法。

Conclusion: 将场景图与常识图协同融合能显著弥补现有方法在知识和视觉理解上的短板，是提升多模态推理能力的有效途径。

Abstract: Multi-modal Large Language Models (MLLMs) for Visual Question Answering (VQA) often suffer from dual limitations: knowledge hallucination and insufficient fine-grained visual perception. Crucially, we identify that commonsense graphs and scene graphs provide precisely complementary solutions to these respective deficiencies by providing rich external knowledge and capturing fine-grained visual details. However, prior works typically treat them in isolation, overlooking their synergistic potential. To bridge this gap, we propose KG-ViP, a unified framework that empowers MLLMs by fusing scene graphs and commonsense graphs. The core of the KG-ViP framework is a novel retrieval-and-fusion pipeline that utilizes the query as a semantic bridge to progressively integrate both graphs, synthesizing a unified structured context that facilitates reliable multi-modal reasoning. Extensive experiments on FVQA 2.0+ and MVQA benchmarks demonstrate that KG-ViP significantly outperforms existing VQA methods.

</details>


### [8] [Beyond Accuracy: Evaluating Grounded Visual Evidence in Thinking with Images](https://arxiv.org/abs/2601.11633)
*Xuchen Li,Xuzhao Li,Renjie Pi,Shiyu Hu,Jian Zhao,Jiahui Gao*

Main category: cs.CV

TL;DR: 本论文提出了ViEBench，一个旨在评估VLMs（视图-语言模型）视觉推理真实性的新基准，专注模型在真实推理过程中的表现，而不仅仅是结果的正确性。


<details>
  <summary>Details</summary>
Motivation: 现有的VLMs评测主要关注最终答题的准确性，忽略了模型能否借助细粒度视觉证据进行多步推理，这导致缺乏对模型推理真实性的客观评估方式。因此，迫切需要一个能够深入诊断推理过程而非仅仅结果的基准。

Method: 作者构建了ViEBench数据集，包含200张多场景高分辨率图片及专家标注的视觉证据。任务难度在感知和推理两个维度上细分，推理任务需结合局部视觉细节与先验知识。评价体系用双轴矩阵划分四种诊断象限，对不同复杂度任务给予细粒度评估。

Result: 实验表明：（1）VLMs有时虽然定位错误区域，仍能得出正确答案；（2）即使找对了证据，模型有时也无法正确推理出结论。ViEBench展示了其在诊断和解释模型推理行为上的优势。

Conclusion: ViEBench为评估VLMs的真实推理能力提供了更具解释性和实用性的工具，有助于推动视觉推理领域的深入研究。代码已开源，有利于学界使用和改进。

Abstract: Despite the remarkable progress of Vision-Language Models (VLMs) in adopting "Thinking-with-Images" capabilities, accurately evaluating the authenticity of their reasoning process remains a critical challenge. Existing benchmarks mainly rely on outcome-oriented accuracy, lacking the capability to assess whether models can accurately leverage fine-grained visual cues for multi-step reasoning. To address these limitations, we propose ViEBench, a process-verifiable benchmark designed to evaluate faithful visual reasoning. Comprising 200 multi-scenario high-resolution images with expert-annotated visual evidence, ViEBench uniquely categorizes tasks by difficulty into perception and reasoning dimensions, where reasoning tasks require utilizing localized visual details with prior knowledge. To establish comprehensive evaluation criteria, we introduce a dual-axis matrix that provides fine-grained metrics through four diagnostic quadrants, enabling transparent diagnosis of model behavior across varying task complexities. Our experiments yield several interesting observations: (1) VLMs can sometimes produce correct final answers despite grounding on irrelevant regions, and (2) they may successfully locate the correct evidence but still fail to utilize it to reach accurate conclusions. Our findings demonstrate that ViEBench can serve as a more explainable and practical benchmark for comprehensively evaluating the effectiveness agentic VLMs. The codes will be released at: https://github.com/Xuchen-Li/ViEBench.

</details>


### [9] [When Rules Fall Short: Agent-Driven Discovery of Emerging Content Issues in Short Video Platforms](https://arxiv.org/abs/2601.11634)
*Chenghui Yu,Hongwei Wang,Junwen Chen,Zixuan Wang,Bingfeng Deng,Zhuolin Hao,Hongyu Xiong,Yang Song*

Main category: cs.CV

TL;DR: 本文提出了一种基于多模态大模型代理（LLM agents）的自动内容问题发现方法，大幅提升了短视频平台新兴内容问题的识别与治理效率。


<details>
  <summary>Details</summary>
Motivation: 短视频平台内容快速变化，出现的新兴内容问题无法被现有标注政策及时覆盖，传统依赖人工发现响应速度慢，导致治理滞后，亟需自动高效的发现和更新机制。

Method: 采用多模态大模型代理，自动检索疑似包含新问题的短视频，通过两阶段聚类将相似内容归类，每个聚类对应一个新发现的问题。随后，代理自动为这些集群生成更新后的标注政策。

Result: 该方法已实际部署在平台中，离线和在线实验表明，该自动化方法可将新兴问题发现F1分数提升20%以上，问题视频的观看量降低约15%。

Conclusion: 所提方法显著提高了新兴问题发现与治理的效率，相比人工发现大幅缩短时间成本，加快了标注政策的更新迭代。

Abstract: Trends on short-video platforms evolve at a rapid pace, with new content issues emerging every day that fall outside the coverage of existing annotation policies. However, traditional human-driven discovery of emerging issues is too slow, which leads to delayed updates of annotation policies and poses a major challenge for effective content governance. In this work, we propose an automatic issue discovery method based on multimodal LLM agents. Our approach automatically recalls short videos containing potential new issues and applies a two-stage clustering strategy to group them, with each cluster corresponding to a newly discovered issue. The agent then generates updated annotation policies from these clusters, thereby extending coverage to these emerging issues. Our agent has been deployed in the real system. Both offline and online experiments demonstrate that this agent-based method significantly improves the effectiveness of emerging-issue discovery (with an F1 score improvement of over 20%) and enhances the performance of subsequent issue governance (reducing the view count of problematic videos by approximately 15%). More importantly, compared to manual issue discovery, it greatly reduces time costs and substantially accelerates the iteration of annotation policies.

</details>


### [10] [Now You See Me, Now You Don't: A Unified Framework for Expression Consistent Anonymization in Talking Head Videos](https://arxiv.org/abs/2601.11635)
*Anil Egin,Andrea Tangherloni,Antitza Dantcheva*

Main category: cs.CV

TL;DR: 本文提出了一种新型人脸视频去标识（匿名化）框架Anon-NET，能有效实现隐私保护，同时保留原视频的关键信息以支持后续分析任务。


<details>
  <summary>Details</summary>
Motivation: 人脸视频常包含个人隐私信息，然而匿名化处理又往往丢失包括表情、姿态等用于计算机视觉任务的关键信息。因此，亟需一种能兼顾隐私保护和有效性的视频匿名化方法。

Method: Anon-NET综合利用基于扩散的生成模型进行人脸修复（inpaint），结合高层属性识别（如年龄、性别、种族、姿态、表情）以及运动感知的表情迁移，实现原视频关键信息的保留和身份的去标识。随后，通过视频驱动的动画技术，将去身份化的人脸进行动画生成以保持与原视频的动态一致性。

Result: 在多个包含丰富面部动态的数据集（VoxCeleb2, CelebV-HQ, HDTF）上的大量实验表明，AnonNET能有效隐藏身份，同时很好地保留视觉真实感和时间一致性。

Conclusion: Anon-NET实现了隐私保护与面部视频可用性的兼顾，在有效去标识身份信息的同时保留了与下游任务相关的面部属性和动态特征，具有良好的实际应用前景。代码将公开发布，便于后续研究和应用。

Abstract: Face video anonymization is aimed at privacy preservation while allowing for the analysis of videos in a number of computer vision downstream tasks such as expression recognition, people tracking, and action recognition. We propose here a novel unified framework referred to as Anon-NET, streamlined to de-identify facial videos, while preserving age, gender, race, pose, and expression of the original video. Specifically, we inpaint faces by a diffusion-based generative model guided by high-level attribute recognition and motion-aware expression transfer. We then animate deidentified faces by video-driven animation, which accepts the de-identified face and the original video as input. Extensive experiments on the datasets VoxCeleb2, CelebV-HQ, and HDTF, which include diverse facial dynamics, demonstrate the effectiveness of AnonNET in obfuscating identity while retaining visual realism and temporal consistency. The code of AnonNet will be publicly released.

</details>


### [11] [Evaluating Self-Correcting Vision Agents Through Quantitative and Qualitative Metrics](https://arxiv.org/abs/2601.11637)
*Aradhya Dixit*

Main category: cs.CV

TL;DR: 本文提出了一种诊断性小型基准，用于深入分析视觉-语言智能体(VLAs)在复杂视觉任务中的纠错和推理瓶颈。发现其纠错能力有限，主要受语义漂移等问题影响。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态基础模型推动了视觉-语言智能体的发展，实现了复杂视觉任务的分解和工具驱动规划，但关于其自我纠错能力的量化上限及主要失败原因尚未系统分析，因此需要更细致的基准来诊断其推理瓶颈。

Method: 作者提出了一套诊断性微型基准，将任务成功率和纠错成功率分离，细致量化了纠错能力的收益以及失败类型，并提出了失败分类体系，重点分析了语义漂移等核心瓶颈。

Result: 实验显示视觉-语言智能体的首次任务成功率达62%，但纠错成功率仅为25%-33%，并且在三次重试后纠错收益趋于饱和。最大失败因素为语义漂移，占到约28%的失败案例。

Conclusion: 该诊断性基准为分析和重现视觉-语言多模态智能体的推理能力提供了标准框架，有助于后续设计更稳定、具有状态感知能力的多模态智能体。

Abstract: Recent progress in multimodal foundation models has enabled Vision-Language Agents (VLAs) to decompose complex visual tasks into executable tool-based plans. While recent benchmarks have begun to evaluate iterative self-correction, its quantitative limits and dominant reasoning bottlenecks remain poorly characterized. This work introduces a Diagnostic Micro-Benchmark. Our analysis decouples Task Success Rate (TSR = 62 percent) from Correction Success Rate (CSR = 25 to 33 percent), revealing that initial competence does not predict repair ability. We explicitly quantify the diminishing returns of correction, which saturates after three retries. Our Failure Taxonomy reveals a frequent factor is Semantic Drift (about 28 percent of failures), a loss of contextual state. By isolating this reasoning bottleneck, this benchmark defines a reproducible framework toward stateful, trustworthy multimodal agents.

</details>


### [12] [Confident Learning for Object Detection under Model Constraints](https://arxiv.org/abs/2601.11640)
*Yingda Yu,Jiaqi Xuan,Shuhui Shi,Xuanyu Teng,Shuyang Xu,Guanchao Tong*

Main category: cs.CV

TL;DR: 本文提出了Model-Driven Data Correction (MDDC) 框架，通过系统性优化数据质量，在边缘设备上提升农田杂草检测性能。该方法适用于模型容量受限、无法依靠扩大模型或集成方法提升性能的场景。实验显示，数据优化可在固定轻量化模型（YOLOv8n）下带来5-25%的mAP提升。


<details>
  <summary>Details</summary>
Motivation: 在边缘计算设备上进行农田杂草检测时，模型需要满足极小的容量和极低的计算资源消耗要求，导致难以直接提升模型性能。传统通过扩大模型或集成多模型的方法不可行，因此亟需新的提升性能思路。

Method: 提出MDDC数据驱动框架，核心在于利用自动化错误分析，将检测错误分为四类：漏检（假阴性）、误检（假阳性）、类别混淆和定位错误。通过结构化的“训练-修正-再训练”流程，系统诊断并优化数据集，并用版本化数据管理以保证提升的可追溯性和可复现性。

Result: 在多个杂草检测数据集和固定的轻量化检测器YOLOv8n上，系统性的数据修正方法可带来5-25%的mAP@0.5提升。

Conclusion: 在模型规模受限的情况下，通过系统的数据质量诊断与修正，可显著提升边缘端目标检测任务性能，为数据驱动的微小模型优化路径提供了有效范例。

Abstract: Agricultural weed detection on edge devices is subject to strict constraints on model capacity, computational resources, and real-time inference latency, which prevent performance improvements through model scaling or ensembling. This paper proposes Model-Driven Data Correction (MDDC), a data-centric framework that enhances detection performance by iteratively diagnosing and correcting data quality deficiencies. An automated error analysis procedure categorizes detection failures into four types: false negatives, false positives, class confusion, and localization errors. These error patterns are systematically addressed through a structured train-fix-retrain pipeline with version-controlled data management. Experimental results on multiple weed detection datasets demonstrate consistent improvements of 5-25 percent in mAP at 0.5 using a fixed lightweight detector (YOLOv8n), indicating that systematic data quality optimization can effectively alleviate performance bottlenecks under fixed model capacity constraints.

</details>


### [13] [Mixture of Distributions Matters: Dynamic Sparse Attention for Efficient Video Diffusion Transformers](https://arxiv.org/abs/2601.11641)
*Yuxi Liu,Yipeng Hu,Zekun Zhang,Kunze Jiang,Kun Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频生成方法MOD-DiT（Mixtrue-Of-Distribution DiT），可以高效准确建模自注意力模式，大幅提升长序列生成效率和质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法（如Diffusion Transformers）的自注意力机制复杂度高，尤其对长序列生成效率低。已有稀疏注意力方法效率或质量仍受限，需要克服这些瓶颈，以推动实际应用。

Method: 作者提出MOD-DiT，一种无需采样的动态注意力新框架。其核心创新包括：1）利用早期去噪步骤的先验信息和分布混合逼近快速预测掩码模式；2）通过在线块掩码策略动态应用这些掩码，保持稀疏历史信息，无需重复采样。

Result: 在多项基准和模型结构上，MOD-DiT展现出更快的生成速度和更高的视频生成质量，相较传统稀疏注意力方法有明显提升。

Conclusion: MOD-DiT成功解决了传统自注意力在视频生成任务中的计算限制，实现了高效、高质量长序列生成，为实际部署带来新的可能性。

Abstract: While Diffusion Transformers (DiTs) have achieved notable progress in video generation, this long-sequence generation task remains constrained by the quadratic complexity inherent to self-attention mechanisms, creating significant barriers to practical deployment. Although sparse attention methods attempt to address this challenge, existing approaches either rely on oversimplified static patterns or require computationally expensive sampling operations to achieve dynamic sparsity, resulting in inaccurate pattern predictions and degraded generation quality. To overcome these limitations, we propose a \underline{\textbf{M}}ixtrue-\underline{\textbf{O}}f-\underline{\textbf{D}}istribution \textbf{DiT} (\textbf{MOD-DiT}), a novel sampling-free dynamic attention framework that accurately models evolving attention patterns through a two-stage process. First, MOD-DiT leverages prior information from early denoising steps and adopts a {distributed mixing approach} to model an efficient linear approximation model, which is then used to predict mask patterns for a specific denoising interval. Second, an online block masking strategy dynamically applies these predicted masks while maintaining historical sparsity information, eliminating the need for repetitive sampling operations. Extensive evaluations demonstrate consistent acceleration and quality improvements across multiple benchmarks and model architectures, validating MOD-DiT's effectiveness for efficient, high-quality video generation while overcoming the computational limitations of traditional sparse attention approaches.

</details>


### [14] [PSSF: Early osteoarthritis detection using physical synthetic knee X-ray scans and AI radiomics models](https://arxiv.org/abs/2601.11642)
*Abbas Alzubaidi,Ali Al-Bayaty*

Main category: cs.CV

TL;DR: 本研究提出了一种基于物理模拟的合成X射线生成框架（PSSF），通过生成可控的人体膝关节X光影像，突破了真实大规模影像数据受限于隐私和资源获取难题，为膝骨关节炎自动化影像学评估提供了新思路。利用该框架合成三种成像协议下的大规模虚拟数据，并通过多种机器学习模型实现对膝骨关节炎等级的预测和鲁棒性分析。


<details>
  <summary>Details</summary>
Motivation: 当前膝骨关节炎主要依赖主观性的放射学分级，且人工智能和影像组学方法需要大量高质量的X光数据，但现实中数据采集受到隐私、合规和资源限制。本研究旨在突破数据获取瓶颈，推动更客观、定量的OA智能诊断研究。

Method: 研究引入了2D X射线模拟器PSSF，从参数化人体膝部骨骼模型出发，虚拟生成180名受试者（260个膝关节）、三种成像协议的X光照片。影像自动分割、预处理并根据IBSI体系提取影像组学特征。随后，分别采用logistic回归、随机森林和梯度提升三种机器学习模型，对影像进行二分类和三分类OA等级预测，并跨协议测试模型鲁棒性，使用组内相关系数评估特征稳定性。

Result: 基于PSSF合成的影像，通过多种机器学习方法能够较好地区分不同膝骨关节炎分级，并且在不同采集协议和多协议情况下均表现出良好的鲁棒性，影像组学特征在协议变化下也有较高稳定性。

Conclusion: 物理模拟合成X光影像的方法为克服OA智能评估中真实数据受限难题提供了有效技术途径，证明了其在自动OA分级中的可行性和稳定性，对未来影像人工智能研究和大数据影像组学开发具有重要意义。

Abstract: Knee osteoarthritis (OA) is a major cause of disability worldwide and is still largely assessed using subjective radiographic grading, most commonly the Kellgren-Lawrence (KL) scale. Artificial intelligence (AI) and radiomics offer quantitative tools for OA assessment but depend on large, well-annotated image datasets, mainly X-ray scans, that are often difficult to obtain because of privacy, governance and resourcing constraints. In this research, we introduce a physics-based synthetic simulation framework (PSSF) to fully generate controllable X-ray scans without patients' involvement and violating their privacy and institutional constraints. This PSSF is a 2D X-ray projection simulator of anteroposterior knee radiographs from a parametric anatomical model of the distal femur and proximal tibia. Using PSSF, we create a virtual cohort of 180 subjects (260 knees), each is imaged under three protocols (reference, low-dose, and geometry-shift). Medial joint regions are automatically localized, preprocessed, and processed with the Image Biomarker Standardisation Initiative (IBSI). Practically, three machine learning (ML) models are utilized, logistic regression, random forest, and gradient boosting, to train binary (KL-like "0" vs. "2") and three-class (0-2) prediction radiographic images. Robustness is assessed within IBSI protocol, cross-protocol, and multi-protocol scenarios. Finally, features stability is then evaluated using intraclass correlation coefficients across acquisition changes.

</details>


### [15] [Predicting When to Trust Vision-Language Models for Spatial Reasoning](https://arxiv.org/abs/2601.11644)
*Muhammad Imran,Yugyung Lee*

Main category: cs.CV

TL;DR: 本文针对视觉-语言模型（VLM）空间推理失败严重的问题，提出了一种基于视觉的置信度估计框架，利用独立的几何验证提升VLM空间预测的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前VLM在多模态任务表现优异，但在基础方向关系判断上准确率偏低（CLIP为49%，BLIP-2为54%），空间推理误差阻碍了其在机器人与自动驾驶等安全关键场景的部署。因此，亟需对VLM空间预测输出是否可信进行判断。

Method: 作者提出融合四个信号的置信度估计方法：1）VLM预测与实际坐标的几何匹配度，2）重叠区域带来的空间歧义，3）目标检测质量，4）VLM内部不确定性。通过梯度提升机融合上述特征，实现独立于文本自评的外部验证。

Result: 该方法在BLIP-2上取得0.674 AUROC（较文本自评提升34%），在CLIP上取得0.583 AUROC（提升16.1%），可泛化到生成与分类架构。置信选择能实现在60%准确率门槛下，覆盖率61.9%（对比基线27.6%，提升2.2倍）。分析显示视觉信号贡献87.4%的判别力。

Conclusion: 视觉基础的外部几何验证显著优于VLM自评，方法能更安全、可靠地在实际场景中部署VLM，稳健提升空间关系理解与构建场景图的精度。

Abstract: Vision-Language Models (VLMs) demonstrate impressive capabilities across multimodal tasks, yet exhibit systematic spatial reasoning failures, achieving only 49% (CLIP) to 54% (BLIP-2) accuracy on basic directional relationships. For safe deployment in robotics and autonomous systems, we need to predict when to trust VLM spatial predictions rather than accepting all outputs. We propose a vision-based confidence estimation framework that validates VLM predictions through independent geometric verification using object detection. Unlike text-based approaches relying on self-assessment, our method fuses four signals via gradient boosting: geometric alignment between VLM claims and coordinates, spatial ambiguity from overlap, detection quality, and VLM internal uncertainty. We achieve 0.674 AUROC on BLIP-2 (34.0% improvement over text-based baselines) and 0.583 AUROC on CLIP (16.1% improvement), generalizing across generative and classification architectures. Our framework enables selective prediction: at 60% target accuracy, we achieve 61.9% coverage versus 27.6% baseline (2.2x improvement) on BLIP-2. Feature analysis reveals vision-based signals contribute 87.4% of model importance versus 12.7% from VLM confidence, validating that external geometric verification outperforms self-assessment. We demonstrate reliable scene graph construction where confidence-based pruning improves precision from 52.1% to 78.3% while retaining 68.2% of edges.

</details>


### [16] [IMSAHLO: Integrating Multi-Scale Attention and Hybrid Loss Optimization Framework for Robust Neuronal Brain Cell Segmentation](https://arxiv.org/abs/2601.11645)
*Ujjwal Jain,Oshin Misra,Roshni Chakraborty,Mahua Bhattacharya*

Main category: cs.CV

TL;DR: 本文提出了一种新型深度学习框架 IMSAHLO，用于荧光显微镜下神经元细胞的精确分割，显著提升了在细胞密集、形态复杂及类别不平衡条件下的分割表现。


<details>
  <summary>Details</summary>
Motivation: 现有神经元自动分割方法在实际应用中面临细胞密集与稀疏、细胞形态重叠以及类别不平衡等难题，导致分割边界、拓扑细节的保存效果较差，影响了后续神经科学的定量分析。

Method: 作者提出IMSAHLO深度学习结构，包含多尺度密集块（MSDBs）以适应不同细胞密度，分层注意力机制以增强形态细节识别，并引入融合Tversky与Focal的混合损失函数，以及提升拓扑与分割边界的clDice和轮廓加权损失，以实现更强的类别平衡和边界区分能力。

Result: 在公共FNC数据集上，IMSAHLO框架相较于现有方法，在稠密与稀疏细胞场景中均表现更优，获得81.4%精确率，82.7%宏观F1分数，83.3%微观F1分数，以及99.5%的平衡准确率。消融实验也证实了多尺度注意力与混合损失结合的优势。

Conclusion: IMSAHLO为神经元及其它生物医学成像分割任务提供了强大且通用的解决思路，将推动AI驱动的高通量神经生物分析流程发展。

Abstract: Accurate segmentation of neuronal cells in fluorescence microscopy is a fundamental task for quantitative analysis in computational neuroscience. However, it is significantly impeded by challenges such as the coexistence of densely packed and sparsely distributed cells, complex overlapping morphologies, and severe class imbalance. Conventional deep learning models often fail to preserve fine topological details or accurately delineate boundaries under these conditions. To address these limitations, we propose a novel deep learning framework, IMSAHLO (Integrating Multi-Scale Attention and Hybrid Loss Optimization), for robust and adaptive neuronal segmentation. The core of our model features Multi-Scale Dense Blocks (MSDBs) to capture features at various receptive fields, effectively handling variations in cell density, and a Hierarchical Attention (HA) mechanism that adaptively focuses on salient morphological features to preserve Region of Interest (ROI) boundary details. Furthermore, we introduce a novel hybrid loss function synergistically combining Tversky and Focal loss to combat class imbalance, alongside a topology-aware Centerline Dice (clDice) loss and a Contour-Weighted Boundary loss to ensure topological continuity and precise separation of adjacent cells. Large-scale experiments on the public Fluorescent Neuronal Cells (FNC) dataset demonstrate that our framework outperforms state-of-the-art architectures, achieving precision of 81.4%, macro F1 score of 82.7%, micro F1 score of 83.3%, and balanced accuracy of 99.5% on difficult dense and sparse cases. Ablation studies validate the synergistic benefits of multi-scale attention and hybrid loss terms. This work establishes a foundation for generalizable segmentation models applicable to a wide range of biomedical imaging modalities, pushing AI-assisted analysis toward high-throughput neurobiological pipelines.

</details>


### [17] [Aesthetics as Structural Harm: Algorithmic Lookism Across Text-to-Image Generation and Classification](https://arxiv.org/abs/2601.11651)
*Miriam Doh,Aditya Gulati,Corina Canali,Nuria Oliver*

Main category: cs.CV

TL;DR: 本文揭示了文本到图像生成AI模型在美貌、性别等维度上存在系统性外貌歧视（lookism），并在性别分类任务中加重了既有的不平等。主要表现为美貌与积极属性的强关联、女性尤其在负面属性下易被误分类、以及新模型加剧外表同质化等问题。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI和视觉识别系统在受到社会构造偏见影响的同时，可能固化、加深外貌和性别相关的不平等，但这种系统性Lookism如何渗透到AI基础设施与应用层仍缺乏实证细致分析。作者希望通过大规模数据分析揭示这些问题并促使学界及业界关注AI带来的公平性挑战。

Method: 用Stable Diffusion模型批量生成26,400张合成面孔，分析模型如何根据美貌归因正、负社会属性。同时，利用三种性别分类算法，考查不同面孔属性下的分类准确率，系统检视模型在美貌、性别等特质编码与识别过程中的偏见。

Result: 1.T2I模型将美貌与积极属性强绑定，将非美貌与负面属性强绑定，弱化现实证据；2.性别分类系统在处理女性、尤其与负面属性关联女性面孔时，误判率显著高于男性；3.新版生成模型表现出外貌单一化、性别及地域曝光模式固化等现象。

Conclusion: AI生成和识别系统已将外貌和性别的固有社会成见算法化、基础化，加剧了表征与识别环节中的现有不平等，应引起社会与技术界高度重视，防止AI扩大大众审美和性别等歧视。

Abstract: This paper examines algorithmic lookism-the systematic preferential treatment based on physical appearance-in text-to-image (T2I) generative AI and a downstream gender classification task. Through the analysis of 26,400 synthetic faces created with Stable Diffusion 2.1 and 3.5 Medium, we demonstrate how generative AI models systematically associate facial attractiveness with positive attributes and vice-versa, mirroring socially constructed biases rather than evidence-based correlations. Furthermore, we find significant gender bias in three gender classification algorithms depending on the attributes of the input faces. Our findings reveal three critical harms: (1) the systematic encoding of attractiveness-positive attribute associations in T2I models; (2) gender disparities in classification systems, where women's faces, particularly those generated with negative attributes, suffer substantially higher misclassification rates than men's; and (3) intensifying aesthetic constraints in newer models through age homogenization, gendered exposure patterns, and geographic reductionism. These convergent patterns reveal algorithmic lookism as systematic infrastructure operating across AI vision systems, compounding existing inequalities through both representation and recognition.
  Disclaimer: This work includes visual and textual content that reflects stereotypical associations between physical appearance and socially constructed attributes, including gender, race, and traits associated with social desirability. Any such associations found in this study emerge from the biases embedded in generative AI systems-not from empirical truths or the authors' views.

</details>


### [18] [PSSI-MaxST: An Efficient Pixel-Segment Similarity Index Using Intensity and Smoothness Features for Maximum Spanning Tree Based Segmentation](https://arxiv.org/abs/2601.11654)
*Kaustubh Shivshankar Shejole,Gaurav Mishra*

Main category: cs.CV

TL;DR: 本文提出了一种基于图的交互式分割新方法，结合了新的像素-分割相似度指标（PSSI）、MeanShift分割和最大生成树（MaxST），在GrabCut和Images250数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的图像分割方法计算成本高、对用户操作敏感，且在前景与背景颜色相近时精度下降。相似性度量的改进有助于提升分割效果，因此作者设计新相似性指标提升鲁棒性和效率。

Method: 方法包括：1）用MeanShift进行初步分割，获得像素段；2）提出PSSI相似度指标，结合像素强度和空间平滑度，并用调和平均计算，复杂度为O(B)；3）构建像素-分割图，边权用PSSI表示；4）采用最大生成树分割，强化局部连通性。

Result: 在GrabCut和Images250数据集上的实验显示：在Jaccard Index（IoU）、F1分数、执行时间和平均误差等指标上，本方法优于AMOE、OneCut、SSNCut等现有方法。

Conclusion: 结合PSSI、MeanShift和MaxST技术的分割方法提升了分割精度与速度，鲁棒性更好，对比多项已有方法有显著性能提升。代码已公开，可应用于图像分割相关任务。

Abstract: Interactive graph-based segmentation methods partition an image into foreground and background regions with the aid of user inputs. However, existing approaches often suffer from high computational costs, sensitivity to user interactions, and degraded performance when the foreground and background share similar color distributions. A key factor influencing segmentation performance is the similarity measure used for assigning edge weights in the graph. To address these challenges, we propose a novel Pixel Segment Similarity Index (PSSI), which leverages the harmonic mean of inter-channel similarities by incorporating both pixel intensity and spatial smoothness features. The harmonic mean effectively penalizes dissimilarities in any individual channel, enhancing robustness. The computational complexity of PSSI is $\mathcal{O}(B)$, where $B$ denotes the number of histogram bins. Our segmentation framework begins with low-level segmentation using MeanShift, which effectively captures color, texture, and segment shape. Based on the resulting pixel segments, we construct a pixel-segment graph with edge weights determined by PSSI. For partitioning, we employ the Maximum Spanning Tree (MaxST), which captures strongly connected local neighborhoods beneficial for precise segmentation. The integration of the proposed PSSI, MeanShift, and MaxST allows our method to jointly capture color similarity, smoothness, texture, shape, and strong local connectivity. Experimental evaluations on the GrabCut and Images250 datasets demonstrate that our method consistently outperforms current graph-based interactive segmentation methods such as AMOE, OneCut, and SSNCut in terms of segmentation quality, as measured by Jaccard Index (IoU), $F_1$ score, execution time and Mean Error (ME). Code is publicly available at: https://github.com/KaustubhShejole/PSSI-MaxST.

</details>


### [19] [Zeros can be Informative: Masked Binary U-Net for Image Segmentation on Tensor Cores](https://arxiv.org/abs/2601.11660)
*Chunshu Wu,Ruibing Song,Sushant Kondguli,Tong Geng,Ang Li*

Main category: cs.CV

TL;DR: 该论文提出了一种新的极致量化分割网络Masked Binary U-Net（MBU-Net），能够在精度几乎无损的情况下大幅提升推理速度和能效，显著适用于边缘设备的实时高分辨率图像分割。


<details>
  <summary>Details</summary>
Motivation: 实时图像分割对AR/VR、机器人、无人机等边缘设备十分关键，需兼顾精度、延迟和能耗。然而目前高效网络如U-Net在高分辨率下仍难以满足实时需求，极致量化（如二值化）虽节省资源，却常导致精度严重下降且缺乏实际高效的端到端实现。

Method: 作者首先通过实验发现：1）明确的零状态训练（权重量化时加零掩码）有助提升网络稀疏性；2）各层对量化的敏感性较均匀。基于此，提出了一种以准确率-计算代价性价比为指标的掩码方案：优先对性价比高的位置进行掩码和二值化，实现Masked Binary U-Net。此外，为提升实际运行效率，设计了基于GPU张量核的高效二值执行框架，通过减法式比特编码充分利用原生二值张量核指令，实现了高吞吐和能效。

Result: 在三个图像分割基准测试上，MBU-Net在几乎不损失精度（平均下降3%）的前提下，相比16位浮点U-Net实现了2.04倍的速度提升和3.54倍的能耗降低。

Conclusion: MBU-Net证明了通过掩码和高效实现，可以兼顾极致量化的硬件高效性与高精度，在通用GPU上可实时高效运行，对边缘智能场景具重要应用价值。

Abstract: Real-time image segmentation is a key enabler for AR/VR, robotics, drones, and autonomous systems, where tight accuracy, latency, and energy budgets must be met on resource-constrained edge devices. While U-Net offers a favorable balance of accuracy and efficiency compared to large transformer-based models, achieving real-time performance on high-resolution input remains challenging due to compute, memory, and power limits. Extreme quantization, particularly binary networks, is appealing for its hardware-friendly operations. However, two obstacles limit practicality: (1) severe accuracy degradation, and (2) a lack of end-to-end implementations that deliver efficiency on general-purpose GPUs.
  We make two empirical observations that guide our design. (1) An explicit zero state is essential: training with zero masking to binary U-Net weights yields noticeable sparsity. (2) Quantization sensitivity is uniform across layers. Motivated by these findings, we introduce Masked Binary U-Net (MBU-Net), obtained through a cost-aware masking strategy that prioritizes masking where it yields the highest accuracy-per-cost, reconciling accuracy with near-binary efficiency.
  To realize these gains in practice, we develop a GPU execution framework that maps MBU-Net to Tensor Cores via a subtractive bit-encoding scheme, efficiently implementing masked binary weights with binary activations. This design leverages native binary Tensor Core BMMA instructions, enabling high throughput and energy savings on widely available GPUs. Across 3 segmentation benchmarks, MBU-Net attains near full-precision accuracy (3% average drop) while delivering 2.04x speedup and 3.54x energy reductions over a 16-bit floating point U-Net.

</details>


### [20] [LTV-YOLO: A Lightweight Thermal Object Detector for Young Pedestrians in Adverse Conditions](https://arxiv.org/abs/2601.11662)
*Abdullah Jirjees,Ryan Myers,Muhammad Haris Ikram,Mohamed H. Zaki*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级的热成像行人检测模型LTV-YOLO，专为在低光和恶劣天气条件下检测儿童和青少年等弱势交通参与者设计，具备高实时性和高精度，适用于边缘设备。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境（如低光、恶劣天气）下，传统可见光摄像头难以可靠检测弱势道路使用者（特别是儿童和青少年），影响了自动驾驶、智能交通和安全监控应用。因此亟需一种能够在这些环境下高效检测的专用模型。

Method: 模型基于YOLO11，融合了深度可分离卷积和特征金字塔网络（FPN），专门针对热成像数据优化，以提升对小尺寸、被部分遮挡、热学特征突出的行人的检测性能，并确保模型结构紧凑、支持边缘设备实时运行。

Result: LTV-YOLO在儿童、小型行人等热学显著对象检测中实现了高精度和高效性能，尤其在复杂或受遮挡场景下表现出色，优于传统的热成像检测方法。

Conclusion: 所提LTV-YOLO模型为智能交通系统（如校区、自动导航及智慧城市）提供了一种实际、可扩展的弱势行人检测解决方案，尤其在恶劣环境和边缘部署场景下具有独特优势。

Abstract: Detecting vulnerable road users (VRUs), particularly children and adolescents, in low light and adverse weather conditions remains a critical challenge in computer vision, surveillance, and autonomous vehicle systems. This paper presents a purpose-built lightweight object detection model designed to identify young pedestrians in various environmental scenarios. To address these challenges, our approach leverages thermal imaging from long-wave infrared (LWIR) cameras, which enhances detection reliability in conditions where traditional RGB cameras operating in the visible spectrum fail. Based on the YOLO11 architecture and customized for thermal detection, our model, termed LTV-YOLO (Lightweight Thermal Vision YOLO), is optimized for computational efficiency, accuracy and real-time performance on edge devices. By integrating separable convolutions in depth and a feature pyramid network (FPN), LTV-YOLO achieves strong performance in detecting small-scale, partially occluded, and thermally distinct VRUs while maintaining a compact architecture. This work contributes a practical and scalable solution to improve pedestrian safety in intelligent transportation systems, particularly in school zones, autonomous navigation, and smart city infrastructure. Unlike prior thermal detectors, our contribution is task-specific: a thermally only edge-capable design designed for young and small VRUs (children and distant adults). Although FPN and depthwise separable convolutions are standard components, their integration into a thermal-only pipeline optimized for short/occluded VRUs under adverse conditions is, to the best of our knowledge, novel.

</details>


### [21] [UAV-Based Infrastructure Inspections: A Literature Review and Proposed Framework for AEC+FM](https://arxiv.org/abs/2601.11665)
*Amir Farzin Nikkhah,Dong Chen,Bradford Campbell,Somayeh Asadi,Arsalan Heydarian*

Main category: cs.CV

TL;DR: 本综述分析了150余篇文献，梳理了无人机在AEC+FM领域基础设施检测中的关键方法与创新，包括数据采集、建模、缺陷检测及辅助决策等方面。


<details>
  <summary>Details</summary>
Motivation: 传统的基础设施检测方式存在效率低、危险性高、适应复杂环境能力弱等问题。无人机技术有望显著提升检测效率和安全性，推动AEC+FM行业智能化转型。

Method: 综述文献，归纳了无人机在路径优化、热成像集成、YOLO和Faster R-CNN等先进机器学习模型应用方面的最新进展。提出集成RGB、LiDAR、热感数据及transformer架构的多模态数据融合流程，并以案例论证其实用性。

Result: 无人机在结构健康监测、灾害应急、城市管理、能源评估及遗产保护等领域展现了较高价值。所提流程框架能有效融合多模态数据，提升缺陷检测、异常监测的准确性和实时性，但仍存在实时处理、数据融合和泛化能力不足等挑战。

Conclusion: 未来研究需关注轻量化AI模型、自适应飞行规划、合成数据集及多模态融合等热点，以进一步优化基础设施智能检测的效率与效果。

Abstract: Unmanned Aerial Vehicles (UAVs) are transforming infrastructure inspections in the Architecture, Engineering, Construction, and Facility Management (AEC+FM) domain. By synthesizing insights from over 150 studies, this review paper highlights UAV-based methodologies for data acquisition, photogrammetric modeling, defect detection, and decision-making support. Key innovations include path optimization, thermal integration, and advanced machine learning (ML) models such as YOLO and Faster R-CNN for anomaly detection. UAVs have demonstrated value in structural health monitoring (SHM), disaster response, urban infrastructure management, energy efficiency evaluations, and cultural heritage preservation. Despite these advancements, challenges in real-time processing, multimodal data fusion, and generalizability remain. A proposed workflow framework, informed by literature and a case study, integrates RGB imagery, LiDAR, and thermal sensing with transformer-based architectures to improve accuracy and reliability in detecting structural defects, thermal anomalies, and geometric inconsistencies. The proposed framework ensures precise and actionable insights by fusing multimodal data and dynamically adapting path planning for complex environments, presented as a comprehensive step-by-step guide to address these challenges effectively. This paper concludes with future research directions emphasizing lightweight AI models, adaptive flight planning, synthetic datasets, and richer modality fusion to streamline modern infrastructure inspections.

</details>


### [22] [MATEX: Multi-scale Attention and Text-guided Explainability of Medical Vision-Language Models](https://arxiv.org/abs/2601.11666)
*Muhammad Imran,Chi Lee,Yugyung Lee*

Main category: cs.CV

TL;DR: MATEX是一种提升医学视觉-语言模型可解释性的框架，能生成更准确、具有解剖学意义的解释图，其表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉-语言模型的解释方法存在空间不精确、缺乏解剖学基础和关注粒度有限等问题，限制了AI在医学影像解读中的可解释性和可信度。

Method: 提出了MATEX框架，将多层注意力展开、文本引导的空间先验与层一致性分析结合，生成更加精准、稳定且具有临床意义的梯度归因图。

Result: 在MS-CXR数据集上的实验证明，MATEX在空间精度和与专家注释的一致性上优于主流M2IB方法。

Conclusion: MATEX能显著提升医学影像领域AI模型解释的准确性和临床相关性，有助于提升其在放射学等场景中的透明度与信任度。

Abstract: We introduce MATEX (Multi-scale Attention and Text-guided Explainability), a novel framework that advances interpretability in medical vision-language models by incorporating anatomically informed spatial reasoning. MATEX synergistically combines multi-layer attention rollout, text-guided spatial priors, and layer consistency analysis to produce precise, stable, and clinically meaningful gradient attribution maps. By addressing key limitations of prior methods, such as spatial imprecision, lack of anatomical grounding, and limited attention granularity, MATEX enables more faithful and interpretable model explanations. Evaluated on the MS-CXR dataset, MATEX outperforms the state-of-the-art M2IB approach in both spatial precision and alignment with expert-annotated findings. These results highlight MATEX's potential to enhance trust and transparency in radiological AI applications.

</details>


### [23] [Generating metamers of human scene understanding](https://arxiv.org/abs/2601.11675)
*Ritik Raina,Abe Leite,Alexandros Graikos,Seoyoung Ahn,Dimitris Samaras,Gregory J. Zelinsky*

Main category: cs.CV

TL;DR: 提出了MetamerGen工具，用于生成与人类潜在视觉场景表示对齐的图像，从高低分辨率输入中合成与人类主观感知一致的视觉场景，并通过行为实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 人类视觉结合了来自视觉边缘的低分辨率信息和凝视点的高分辨率细节来理解场景，现有生成模型难以模拟出这种多分辨率融合后符合人类理解的“视觉同型”（metamer）场景。

Method: 提出基于潜在扩散模型的MetamerGen工具，输入来自注视区域的高分辨率细节和周边区域的低分辨率场景信息，通过双通道表示（分别提取凝视和环境区域特征，并融合）进行图像生成。采用行为实验（same-different判断）评价生成图像与原图在主观感知上的一致性。

Result: MetamerGen可成功生成与人类视觉潜在场景表示对齐的图像。实验发现，受试者对特定条件下生成的图像和原图更难区分，特别是当模型输入与观者实际凝视区域对齐时，高层语义的对齐最能预测metamerism的发生。

Conclusion: MetamerGen为研究人类场景理解和多级视觉处理提供了新工具。工具不仅能在随机凝视点下生成metamer图像，还证明了基于真实观者凝视带来的更强的语义一致性。未来可用于进一步揭示视觉理解机制。

Abstract: Human vision combines low-resolution "gist" information from the visual periphery with sparse but high-resolution information from fixated locations to construct a coherent understanding of a visual scene. In this paper, we introduce MetamerGen, a tool for generating scenes that are aligned with latent human scene representations. MetamerGen is a latent diffusion model that combines peripherally obtained scene gist information with information obtained from scene-viewing fixations to generate image metamers for what humans understand after viewing a scene. Generating images from both high and low resolution (i.e. "foveated") inputs constitutes a novel image-to-image synthesis problem, which we tackle by introducing a dual-stream representation of the foveated scenes consisting of DINOv2 tokens that fuse detailed features from fixated areas with peripherally degraded features capturing scene context. To evaluate the perceptual alignment of MetamerGen generated images to latent human scene representations, we conducted a same-different behavioral experiment where participants were asked for a "same" or "different" response between the generated and the original image. With that, we identify scene generations that are indeed metamers for the latent scene representations formed by the viewers. MetamerGen is a powerful tool for understanding scene understanding. Our proof-of-concept analyses uncovered specific features at multiple levels of visual processing that contributed to human judgments. While it can generate metamers even conditioned on random fixations, we find that high-level semantic alignment most strongly predicts metamerism when the generated scenes are conditioned on viewers' own fixated regions.

</details>


### [24] [Conformal Point and the Calibrated Conic](https://arxiv.org/abs/2601.11679)
*Richard Hartley*

Main category: cs.CV

TL;DR: 文章探讨了共形点和校准圆锥体及其相互关系，这些概念有助于图像几何的可视化计算。


<details>
  <summary>Details</summary>
Motivation: 图像处理和计算机视觉任务中，准确地理解和计算图像中的几何特性（如角度和方向）非常重要。作者希望通过共形点和校准圆锥体这两个概念简化和直观化相关计算。

Method: 文中介绍了共形点和校准圆锥体的定义，分析了它们之间的关系，并探讨了如何利用这些概念来辅助图像几何的可视化和计算。

Result: 提出的几何关系和方法能直观地反映图像中的角度、方向等几何属性，提高了几何计算的效率和准确性。

Conclusion: 共形点和校准圆锥体的结合为图像几何的理解与计算提供了直观且有效的手段。

Abstract: This gives some information about the conformal point and the calibrating conic, and their relationship one to the other. These concepts are useful for visualizing image geometry, and lead to intuitive ways to compute geometry, such as angles and directions in an image.

</details>


### [25] [Telling Human and Machine Handwriting Apart](https://arxiv.org/abs/2601.11700)
*Luis A. Leiva,Moises Diaz,Nuwan T. Attygalle,Miguel A. Ferrer,Rejean Plamondon*

Main category: cs.CV

TL;DR: 本论文提出了一种利用手写动作作为行为生物特征来区分人类与人工生成输入的方法，基于多个合成器和数据集训练浅层循环神经网络，在区分真实与合成手写轨迹时取得了极高的准确率。


<details>
  <summary>Details</summary>
Motivation: 随着合成数据技术的发展，如何验证真实用户操作变得越来越重要。手写动作具有独特性，被认为是判别人工输入与真实人类操作的新型行为生物识别手段，本研究希望借此提升系统的安全性。

Method: 作者收集了10个公开手写符号数据集，并采用7种手写合成器（如Sigma h模型、GAN、Transformer、Diffusion models等）生成人工数据。利用非特征化的轨迹数据作为输入，训练浅层循环神经网络来区分真实人类手写和合成手写。对全数据集和少样本（仅用10%数据训练）以及跨领域场景进行了评估。

Result: 所提方法在全部数据集和合成器下平均AUC达到98.3%，等错误率为1.4%。在仅用10%训练数据的少样本和跨领域测试中，也表现出极强的泛化能力与竞争性。

Conclusion: 使用浅层循环神经网络对手写轨迹进行真假识别是有效且鲁棒的。研究成果可为需要验证人类存在性的系统增加新安全层，提升抵御攻击者的能力。

Abstract: Handwriting movements can be leveraged as a unique form of behavioral biometrics, to verify whether a real user is operating a device or application. This task can be framed as a reverse Turing test in which a computer has to detect if an input instance has been generated by a human or artificially. To tackle this task, we study ten public datasets of handwritten symbols (isolated characters, digits, gestures, pointing traces, and signatures) that are artificially reproduced using seven different synthesizers, including, among others, the Kinematic Theory (Sigma h model), generative adversarial networks, Transformers, and Diffusion models. We train a shallow recurrent neural network that achieves excellent performance (98.3 percent Area Under the ROC Curve (AUC) score and 1.4 percent equal error rate on average across all synthesizers and datasets) using nonfeaturized trajectory data as input. In few-shot settings, we show that our classifier achieves such an excellent performance when trained on just 10 percent of the data, as evaluated on the remaining 90% of the data as a test set. We further challenge our classifier in out-of-domain settings, and observe very competitive results as well. Our work has implications for computerized systems that need to verify human presence, and adds an additional layer of security to keep attackers at bay.

</details>


### [26] [SemAlign: Language Guided Semi-supervised Domain Generalization](https://arxiv.org/abs/2601.11724)
*Muditha Fernando,Kajhanan Kailainathan,Krishnakanth Nagaratnam,Isuranga Udaravi Bandara Senavirathne,Ranga Rodrigo*

Main category: cs.CV

TL;DR: 本文提出通过模型中间特征与视觉语言模型(VLM)的特征空间对齐，配合增强与正则化提升半监督领域泛化(SSDG)性能，获得当前最优结果。


<details>
  <summary>Details</summary>
Motivation: 当前SSDG方法过分关注伪标签精度，忽视了训练过程中数据最大化利用，限制了性能提升。

Method: 提出将模型中间特征与视觉语言模型(VLM)的泛化、语义丰富特征空间对齐，并结合有效的图像级增强与输出正则化策略，既提升了域不变性，又提高了数据利用效率并减少过拟合。

Result: 在四个基准上，所提方法在定性与定量上均优于现有SSDG方法，达到了SOTA水平。

Conclusion: 对齐模型特征与VLM特征并有效利用增强和正则化，可突破当前SSDG性能瓶颈，方法有效、具有应用前景。

Abstract: Semi-supervised Domain Generalization (SSDG) addresses the challenge of generalizing to unseen target domains with limited labeled data. Existing SSDG methods highlight the importance of achieving high pseudo-labeling (PL) accuracy and preventing model overfitting as the main challenges in SSDG. In this light, we show that the SSDG literature's excessive focus on PL accuracy, without consideration for maximum data utilization during training, limits potential performance improvements. We propose a novel approach to the SSDG problem by aligning the intermediate features of our model with the semantically rich and generalized feature space of a Vision Language Model (VLM) in a way that promotes domain-invariance. The above approach is enhanced with effective image-level augmentation and output-level regularization strategies to improve data utilization and minimize overfitting. Extensive experimentation across four benchmarks against existing SSDG baselines suggests that our method achieves SOTA results both qualitatively and quantitatively. The code will be made publicly available.

</details>


### [27] [SpaRRTa: A Synthetic Benchmark for Evaluating Spatial Intelligence in Visual Foundation Models](https://arxiv.org/abs/2601.11729)
*Turhan Can Kargin,Wojciech Jasiński,Adam Pardyl,Bartosz Zieliński,Marcin Przewięźlikowski*

Main category: cs.CV

TL;DR: 本文提出了SpaRRTa基准，用于系统评估当前主流视觉基础模型（VFM）在空间关系识别方面的能力。研究发现，VFM在空间推理任务上表现参差不齐，揭示了其空间感知能力的局限性。


<details>
  <summary>Details</summary>
Motivation: 虽然VFM（如DINO、CLIP）在图像语义理解方面表现优异，但在空间推理（如物体相对位置识别）上的能力较弱，影响其在具身智能等领域的应用。现有方法虽然融入了特定3D任务，但无法涵盖更广泛的空间推理需求，有待对其空间感知能力进行专门评估和分析。

Method: 作者提出了SpaRRTa基准，其特点是可自动生成大量具有多样场景和可控物体排列的真实感图像，并附带空间关系注释。通过SpaRRTa，作者系统地评估了多种主流VFM在物体空间关系识别任务上的表现，并对其表现差异进行了机制分析。

Result: 实验显示，不同VFM在空间推理任务上的表现存在显著差异，有些模型泛化能力不足，可能只对特定3D任务进行了过拟合。

Conclusion: SpaRRTa能够有效揭示VFM的空间感知短板，有助于指导今后更具空间感知能力的视觉模型研发。

Abstract: Visual Foundation Models (VFMs), such as DINO and CLIP, excel in semantic understanding of images but exhibit limited spatial reasoning capabilities, which limits their applicability to embodied systems. As a result, recent work incorporates some 3D tasks (such as depth estimation) into VFM training. However, VFM performance remains inconsistent across other spatial tasks, raising the question of whether these models truly have spatial awareness or overfit to specific 3D objectives. To address this question, we introduce the Spatial Relation Recognition Task (SpaRRTa) benchmark, which evaluates the ability of VFMs to identify relative positions of objects in the image. Unlike traditional 3D objectives that focus on precise metric prediction (e.g., surface normal estimation), SpaRRTa probes a fundamental capability underpinning more advanced forms of human-like spatial understanding. SpaRRTa generates an arbitrary number of photorealistic images with diverse scenes and fully controllable object arrangements, along with freely accessible spatial annotations. Evaluating a range of state-of-the-art VFMs, we reveal significant disparities between their spatial reasoning abilities. Through our analysis, we provide insights into the mechanisms that support or hinder spatial awareness in modern VFMs. We hope that SpaRRTa will serve as a useful tool for guiding the development of future spatially aware visual models.

</details>


### [28] [From Pixels to Purchase: Building and Evaluating a Taxonomy-Decoupled Visual Search Engine for Home Goods E-commerce](https://arxiv.org/abs/2601.11769)
*Cheng Lyu,Jingyue Zhang,Ryan Maunu,Mengwei Li,Vinny DeGenova,Yuanli Pei*

Main category: cs.CV

TL;DR: 本文提出了一种解耦于分类体系的视觉搜索架构，并用大语言模型实现无需人工标注的检索效果评价，提升了电商平台的搜索质量与用户参与度。


<details>
  <summary>Details</summary>
Motivation: 传统电商视觉搜索系统依赖对象检测与分类体系，并使用商品目录数据进行评估，容易受到标签噪声和分类不精确的影响，导致系统鲁棒性和扩展性受限，尤其在风格主导和主观性较强的场景亟需改进。

Method: 1）提出一种与分类体系解耦的视觉搜索系统，不依赖分类，采用无分类的区域建议与统一特征向量实现相似检索；2）创新性地引入大语言模型（LLM）实现“LLM-as-a-Judge”，用以在零样本场景下自动化判断检索结果的视觉相似度和类别相关性，从而替代依赖人工标注或有噪声的商品目录数据的评估方式。

Result: 该系统在全球家居电商平台大规模上线，检索质量明显提升，用户参与度（如点击、转化等）有量化提升。离线评估指标与真实业务效果高度相关，有效支撑业务优化。

Conclusion: 本文提出的分类体系解耦+大模型评估的新型视觉搜索方案能够提升易扩展性和检索鲁棒性，摆脱对有噪声目录数据和人工标注的依赖，为电商等主观性视觉检索场景提供了更具普适性和灵活性的解决方案。

Abstract: Visual search is critical for e-commerce, especially in style-driven domains where user intent is subjective and open-ended. Existing industrial systems typically couple object detection with taxonomy-based classification and rely on catalog data for evaluation, which is prone to noise that limits robustness and scalability. We propose a taxonomy-decoupled architecture that uses classification-free region proposals and unified embeddings for similarity retrieval, enabling a more flexible and generalizable visual search. To overcome the evaluation bottleneck, we propose an LLM-as-a-Judge framework that assesses nuanced visual similarity and category relevance for query-result pairs in a zero-shot manner, removing dependence on human annotations or noise-prone catalog data. Deployed at scale on a global home goods platform, our system improves retrieval quality and yields a measurable uplift in customer engagement, while our offline evaluation metrics strongly correlate with real-world outcomes.

</details>


### [29] [studentSplat: Your Student Model Learns Single-view 3D Gaussian Splatting](https://arxiv.org/abs/2601.11772)
*Yimu Pan,Hongda Mao,Qingshuang Chen,Yelin Kim*

Main category: cs.CV

TL;DR: studentSplat是一种针对单视图3D场景重建的高效高质量方法，首次在单视图下实现了与多视图方法相媲美的成果。


<details>
  <summary>Details</summary>
Motivation: 当前3D Gaussian splatting多用于多视图场景重建，而单视图3D场景重建由于视角歧义问题尚未得到充分研究。本文旨在突破单视图重建的局限，提升其几何有效性和重建水平。

Method: 提出studentSplat方法，核心包括两个创新：一是采用教师-学生架构，利用多视角教师为单视角学生训练提供几何监督，解决尺度歧义并提升几何合理性；二是引入外推网络，补全场景缺失内容，实现高质量新视角外推。

Result: studentSplat在单视图新视角重建任务中达到最新最好效果，在场景级别上性能可与多视图方法媲美。同时，该方法在自监督单视图深度估计任务中表现出色。

Conclusion: studentSplat极大提升了单视图3D场景重建的质量，推动了单视图3D理解领域的发展，展现出良好的泛用性和竞争力。

Abstract: Recent advance in feed-forward 3D Gaussian splatting has enable remarkable multi-view 3D scene reconstruction or single-view 3D object reconstruction but single-view 3D scene reconstruction remain under-explored due to inherited ambiguity in single-view. We present \textbf{studentSplat}, a single-view 3D Gaussian splatting method for scene reconstruction. To overcome the scale ambiguity and extrapolation problems inherent in novel-view supervision from a single input, we introduce two techniques: 1) a teacher-student architecture where a multi-view teacher model provides geometric supervision to the single-view student during training, addressing scale ambiguity and encourage geometric validity; and 2) an extrapolation network that completes missing scene context, enabling high-quality extrapolation. Extensive experiments show studentSplat achieves state-of-the-art single-view novel-view reconstruction quality and comparable performance to multi-view methods at the scene level. Furthermore, studentSplat demonstrates competitive performance as a self-supervised single-view depth estimation method, highlighting its potential for general single-view 3D understanding tasks.

</details>


### [30] [Cross-Domain Object Detection Using Unsupervised Image Translation](https://arxiv.org/abs/2601.11779)
*Vinicius F. Arruda,Rodrigo F. Berriel,Thiago M. Paixão,Claudine Badue,Alberto F. De Souza,Nicu Sebe,Thiago Oliveira-Santos*

Main category: cs.CV

TL;DR: 本文提出了一种利用生成的目标域人工数据集进行目标检测的方法，通过CycleGAN和AdaIN模型进行无监督领域自适应，提升检测性能，并降低方法复杂性。


<details>
  <summary>Details</summary>
Motivation: 现有无监督领域自适应目标检测方法虽有较好表现，但实现复杂且难以解释，且与理想状态（即获得目标域标注数据时的性能）仍有差距。

Method: 本文采用两种无监督图像转换器（CycleGAN和基于AdaIN的模型）来将源域的标注数据和目标域的无标注数据结合，生成目标域的人工数据集，用于训练目标检测器。

Result: 在真实的自动驾驶场景实验中，该方法显著提升了目标检测性能，在大多数情况下超越了现有主流方法，并进一步缩小了与完全标注目标域时的性能差距。

Conclusion: 所提方法结构简明、效果更佳、易于解释，为无监督领域自适应目标检测提供了更优解法。

Abstract: Unsupervised domain adaptation for object detection addresses the adaption of detectors trained in a source domain to work accurately in an unseen target domain. Recently, methods approaching the alignment of the intermediate features proven to be promising, achieving state-of-the-art results. However, these methods are laborious to implement and hard to interpret. Although promising, there is still room for improvements to close the performance gap toward the upper-bound (when training with the target data). In this work, we propose a method to generate an artificial dataset in the target domain to train an object detector. We employed two unsupervised image translators (CycleGAN and an AdaIN-based model) using only annotated data from the source domain and non-annotated data from the target domain. Our key contributions are the proposal of a less complex yet more effective method that also has an improved interpretability. Results on real-world scenarios for autonomous driving show significant improvements, outperforming state-of-the-art methods in most cases, further closing the gap toward the upper-bound.

</details>


### [31] [Digital FAST: An AI-Driven Multimodal Framework for Rapid and Early Stroke Screening](https://arxiv.org/abs/2601.11896)
*Ngoc-Khai Hoang,Thi-Nhu-Mai Nguyen,Huy-Hieu Pham*

Main category: cs.CV

TL;DR: 该论文提出了一种快速、无创的多模态深度学习框架，通过整合面部表情、语音信号和上半身动作自动筛查中风，实验表明准确率高，检测表现优于单模态方法。


<details>
  <summary>Details</summary>
Motivation: 中风早期识别对于及时干预和改善患者预后尤为重要，尤其是在院前阶段，现有方法存在诊断复杂、准确性有限的问题，因此需求开发更快、更准确的辅助筛查工具。

Method: 该方法使用F.A.S.T.评估流程中收集的数据，融合面部表情（以landmark特征并用Transformer建模）、语音（转为Mel频谱图后用Audio Spectrogram Transformer处理）、上半身动作（用MLP-Mixer捕捉时空模式），再通过基于注意力的机制实现多模态特征融合，提升诊断稳健性。

Result: 在自建的222段视频、37名受试者样本集上，多模态模型准确率达95.83%，F1分数为96.00%，检测灵敏度和特异度兼优，所有中风案例均被成功检测，显著优于单模态基线。

Conclusion: 多模态和迁移学习方法在早期中风筛查具有很大潜力，未来需扩大数据集规模以确保在真实临床环境中的通用性和可靠性。

Abstract: Early identification of stroke symptoms is essential for enabling timely intervention and improving patient outcomes, particularly in prehospital settings. This study presents a fast, non-invasive multimodal deep learning framework for automatic binary stroke screening based on data collected during the F.A.S.T. assessment. The proposed approach integrates complementary information from facial expressions, speech signals, and upper-body movements to enhance diagnostic robustness. Facial dynamics are represented using landmark based features and modeled with a Transformer architecture to capture temporal dependencies. Speech signals are converted into mel spectrograms and processed using an Audio Spectrogram Transformer, while upper-body pose sequences are analyzed with an MLP-Mixer network to model spatiotemporal motion patterns. The extracted modality specific representations are combined through an attention-based fusion mechanism to effectively learn cross modal interactions. Experiments conducted on a self-collected dataset of 222 videos from 37 subjects demonstrate that the proposed multimodal model consistently outperforms unimodal baselines, achieving 95.83% accuracy and a 96.00% F1-score. The model attains a strong balance between sensitivity and specificity and successfully detects all stroke cases in the test set. These results highlight the potential of multimodal learning and transfer learning for early stroke screening, while emphasizing the need for larger, clinically representative datasets to support reliable real-world deployment.

</details>


### [32] [RemoteVAR: Autoregressive Visual Modeling for Remote Sensing Change Detection](https://arxiv.org/abs/2601.11898)
*Yilmaz Korkmaz,Vishal M. Patel*

Main category: cs.CV

TL;DR: 本文提出了RemoteVAR，一种基于视觉自回归模型（VAR）的遥感变化检测新方法，通过多分辨率双时相特征融合和跨注意力机制改进行像素级变化检测效果，并在主流基准数据集上超过扩散模型与Transformer基方法。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉自回归模型在图像生成领域表现出色，但在像素级判别任务如遥感变化检测中应用受限，主要因为可控性差、密集预测性能不佳和暴露偏差等问题。因此，亟需改进该类模型以提升其在遥感领域的表现。

Method: 作者提出RemoteVAR框架，将自回归预测任务与变化图预测结合，并创新性地对多分辨率的双时相特征进行融合，通过跨注意力机制提升模型可控性和判别能力，并调整训练策略以更适应变化检测。

Result: 在标准变化检测数据集上，RemoteVAR取得了比主流的扩散模型和Transformer基准方法更优的性能，且改进具有一致性和显著性。

Conclusion: RemoteVAR为遥感变化检测提供了一种高性能的自回归新方案，克服了以往自回归模型在像素判别类任务上的局限，有望在实际遥感应用中得到广泛采用。

Abstract: Remote sensing change detection aims to localize and characterize scene changes between two time points and is central to applications such as environmental monitoring and disaster assessment. Meanwhile, visual autoregressive models (VARs) have recently shown impressive image generation capability, but their adoption for pixel-level discriminative tasks remains limited due to weak controllability, suboptimal dense prediction performance and exposure bias. We introduce RemoteVAR, a new VAR-based change detection framework that addresses these limitations by conditioning autoregressive prediction on multi-resolution fused bi-temporal features via cross-attention, and by employing an autoregressive training strategy designed specifically for change map prediction. Extensive experiments on standard change detection benchmarks show that RemoteVAR delivers consistent and significant improvements over strong diffusion-based and transformer-based baselines, establishing a competitive autoregressive alternative for remote sensing change detection. Code will be available \href{https://github.com/yilmazkorkmaz1/RemoteVAR}{\underline{here}}.

</details>


### [33] [Towards Airborne Object Detection: A Deep Learning Analysis](https://arxiv.org/abs/2601.11907)
*Prosenjit Chatterjee,ANK Zaman*

Main category: cs.CV

TL;DR: 本论文提出了一种基于EfficientNetB4的双任务模型，实现对空中目标的分类及威胁等级预测，显著提升了自动化和实时性。模型在自建AODTA数据集和AVD数据集上均取得了优异成绩，优于ResNet-50基线。


<details>
  <summary>Details</summary>
Motivation: 随着各类飞机、无人机及无人飞行器剧增，现有主要依赖人工监控的空中威胁评估系统面临可扩展性差、效率低下等问题，亟需自动化、实时的解决方案。

Method: 作者构建了AODTA数据集，采用EfficientNetB4构建一个同时执行空中物体分类和威胁等级预测的双任务模型，并与ResNet-50进行性能对比。

Result: EfficientNetB4模型在物体分类上达到96%准确率，威胁等级预测上达到90%准确率，均明显超越ResNet-50。

Conclusion: EfficientNetB4双任务模型在空中目标自动分类与威胁评估中表现突出，为空域监控、防御等领域的自动化系统提供了有力工具。但本研究仅限于对已定位的目标图像进行分类与威胁推断。

Abstract: The rapid proliferation of airborne platforms, including commercial aircraft, drones, and UAVs, has intensified the need for real-time, automated threat assessment systems. Current approaches depend heavily on manual monitoring, resulting in limited scalability and operational inefficiencies. This work introduces a dual-task model based on EfficientNetB4 capable of performing airborne object classification and threat-level prediction simultaneously. To address the scarcity of clean, balanced training data, we constructed the AODTA Dataset by aggregating and refining multiple public sources. We benchmarked our approach on both the AVD Dataset and the newly developed AODTA Dataset and further compared performance against a ResNet-50 baseline, which consistently underperformed EfficientNetB4. Our EfficientNetB4 model achieved 96% accuracy in object classification and 90% accuracy in threat-level prediction, underscoring its promise for applications in surveillance, defense, and airspace management. Although the title references detection, this study focuses specifically on classification and threat-level inference using pre-localized airborne object images provided by existing datasets.

</details>


### [34] [Effects of the retina-inspired light intensity encoding on color discrimination performance](https://arxiv.org/abs/2601.11909)
*Io Yamada,Hirotsugu Okuno*

Main category: cs.CV

TL;DR: 本文研究了不同光强度编码函数对中心/边缘（C/S）retinex模型色恒常性能的影响。结果发现，Naka-Rushton函数结合双对手颜色平面的表示方式能更好地区分不同照明下的目标颜色。


<details>
  <summary>Details</summary>
Motivation: 色彩感知会受到光照颜色的极大影响，色恒常性是保持视觉系统颜色识别准确性的关键，本研究旨在提高视觉系统的颜色识别能力，特别是在变化照明环境下。

Method: 作者对比了两种光强度编码函数：传统C/S retinex模型中使用的对数函数与反映视网膜感受器响应的Naka-Rushton（N-R）函数。采用颜色可变的LED灯对目标物体进行多种颜色照明，通过两种编码模型计算所得的颜色信息，评估各模型在不同照明条件下对目标颜色的判别能力。颜色信息以HSV色空间和基于经典对手理论的色平面两种方式表达。

Result: 实验结果表明，采用N-R函数配合双对手色平面表示时，对于不同照明条件下的目标颜色，模型的区分性能优于其他组合。

Conclusion: Naka-Rushton函数结合双对手颜色平面能够有效提升中心/边缘retinex模型的色恒常判别能力，对视觉系统和基于颜色信息的计算机视觉应用具有一定借鉴意义。

Abstract: Color is an important source of information for visual functions such as object recognition, but it is greatly affected by the color of illumination. The ability to perceive the color of a visual target independent of illumination color is called color constancy (CC), and is an important feature for vision systems that use color information. In this study, we investigated the effects of the light intensity encoding function on the performance of CC of the center/surround (C/S) retinex model, which is a well-known model inspired by CC of the visual nervous system. The functions used to encode light intensity are the logarithmic function used in the original C/S retinex model and the Naka-Rushton (N-R) function, which is a model of retinal photoreceptor response. Color-variable LEDs were used to illuminate visual targets with various lighting colors, and color information computed by each model was used to evaluate the degree to which the color of visual targets illuminated with different lighting colors could be discriminated. Color information was represented using the HSV color space and a color plane based on the classical opponent color theory. The results showed that the combination of the N-R function and the double opponent color plane representation provided superior discrimination performance.

</details>


### [35] [A Training-Free Guess What Vision Language Model from Snippets to Open-Vocabulary Object Detection](https://arxiv.org/abs/2601.11910)
*Guiying Zhu,Bowen Yang,Yin Zhuang,Tong Zhang,Guanqun Wang,Zhihao Che,He Chen,Lianlin Li*

Main category: cs.CV

TL;DR: 提出了一种无需训练的开集目标检测方法GW-VLM，能充分利用已有视觉语言与大模型实现通用目标识别，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇目标检测虽依赖大规模预训练基础模型，但缺乏利用预训练模型实现真正通用目标理解的范式。

Method: 设计了无需训练的Guess What Vision-Language Model（GW-VLM）：1）提出多尺度视觉语言检索(MS-VLS)，对检测目标生成多尺度片段用于视觉语言对齐；2）引入上下文概念提示(CCP)，使大语言模型能理解片段，进而识别开集目标。整个流程由预训练VLM和LLM协同，无需进一步训练。

Result: 在COCO val、Pascal VOC、DIOR、NWPU-10等自然与遥感数据集上，无需训练的GW-VLM性能优于现有SOTA开集目标检测方法。

Conclusion: GW-VLM无须训练即可利用基础大模型实现优异的开放词汇目标检测，为通用模型理解任意对象提供新范式。

Abstract: Open-Vocabulary Object Detection (OVOD) aims to develop the capability to detect anything. Although myriads of large-scale pre-training efforts have built versatile foundation models that exhibit impressive zero-shot capabilities to facilitate OVOD, the necessity of creating a universal understanding for any object cognition according to already pretrained foundation models is usually overlooked. Therefore, in this paper, a training-free Guess What Vision Language Model, called GW-VLM, is proposed to form a universal understanding paradigm based on our carefully designed Multi-Scale Visual Language Searching (MS-VLS) coupled with Contextual Concept Prompt (CCP) for OVOD. This approach can engage a pre-trained Vision Language Model (VLM) and a Large Language Model (LLM) in the game of "guess what". Wherein, MS-VLS leverages multi-scale visual-language soft-alignment for VLM to generate snippets from the results of class-agnostic object detection, while CCP can form the concept of flow referring to MS-VLS and then make LLM understand snippets for OVOD. Finally, the extensive experiments are carried out on natural and remote sensing datasets, including COCO val, Pascal VOC, DIOR, and NWPU-10, and the results indicate that our proposed GW-VLM can achieve superior OVOD performance compared to the-state-of-the-art methods without any training step.

</details>


### [36] [Reliable Deep Learning for Small-Scale Classifications: Experiments on Real-World Image Datasets from Bangladesh](https://arxiv.org/abs/2601.11911)
*Muhammad Ibrahim,Alfe Suny,MD Sakib Ul Islam,Md. Imran Hossain*

Main category: cs.CV

TL;DR: 本文评估了一种轻量级的卷积神经网络（CNN）在孟加拉国五个不同实际图像数据集上的表现，结果显示该模型准确率高、收敛快、计算效率高，适合用于小样本图像分类任务。


<details>
  <summary>Details</summary>
Motivation: 现代CNN在图像识别领域表现优异，但其结构复杂，容易在小型数据集上过拟合。该研究希望探索结构更精简的CNN能否解决上述问题，尤其是在资源有限场景和小类别图像分类上。

Method: 作者设计并实现了一种紧凑的CNN，并在孟加拉国五个不同场景（城市侵占、车辆检测、道路损坏、农作物等）的公开数据集上进行评估，通过分类准确率、收敛速度和计算开销等多项指标及显著性分析进行了对比实验。

Result: 轻量级CNN在五个数据集上均表现出高分类准确率、快速收敛和低计算资源消耗。显著性分析表明网络能有效捕捉有区分力的特征，在多种应用场景下具备良好的泛化能力。

Conclusion: 结果表明，经过精简的CNN结构不仅能够缩减计算资源消耗和过拟合风险，还能保持较高的识别性能，是小类别图像分类任务的有力候选方法。

Abstract: Convolutional neural networks (CNNs) have achieved state-of-the-art performance in image recognition tasks but often involve complex architectures that may overfit on small datasets. In this study, we evaluate a compact CNN across five publicly available, real-world image datasets from Bangladesh, including urban encroachment, vehicle detection, road damage, and agricultural crops. The network demonstrates high classification accuracy, efficient convergence, and low computational overhead. Quantitative metrics and saliency analyses indicate that the model effectively captures discriminative features and generalizes robustly across diverse scenarios, highlighting the suitability of streamlined CNN architectures for small-class image classification tasks.

</details>


### [37] [From Spurious to Causal: Low-rank Orthogonal Subspace Intervention for Generalizable Face Forgery Detection](https://arxiv.org/abs/2601.11915)
*Chi Wang,Xinjue Hu,Boyu Wang,Ziwen He,Zhangjie Fu*

Main category: cs.CV

TL;DR: 本文提出一种新的人脸伪造检测方法，通过低秩子空间投影去除伪相关信息，从而大幅提升模型的泛化能力，并在多个基准测试上取得了最优表现。


<details>
  <summary>Details</summary>
Motivation: 人脸伪造检测在实际应用中普遍存在泛化能力不足的问题，主要原因在于模型学习到了与伪造无关的信息（即伪相关因素），影响了检测效果。以往方法多针对具体的伪相关进行应对，无法覆盖所有情况，因此亟需更普适有效的解决办法。

Method: 作者提出在特征表征空间进行“干预”：将所有实例级伪相关统一建模为低秩子空间，通过正交低秩投影将其分解并从原特征中移除，仅用其正交补空间进行训练，从而捕捉真正的伪造相关特征。

Result: 所提方法参数量极小（仅43万），但在多个主流基准数据集上实现了当前最优的检测性能，具有极强的鲁棒性和泛化能力。

Conclusion: 通过低秩子空间投影去除伪相关信息为人脸伪造检测任务带来了模型泛化能力的显著提升，方法简洁有效，有望推广应用于其它依赖特征泛化的任务中。

Abstract: The generalization problem remains a critical challenge in face forgery detection. Some researches have discovered that ``a backdoor path" in the representations from forgery-irrelevant information to labels induces biased learning, thereby hindering the generalization. In this paper, these forgery-irrelevant information are collectively termed spurious correlations factors. Previous methods predominantly focused on identifying concrete, specific spurious correlation and designing corresponding solutions to address them. However, spurious correlations arise from unobservable confounding factors, making it impractical to identify and address each one individually. To address this, we propose an intervention paradigm for representation space. Instead of tracking and blocking various instance-level spurious correlation one by one, we uniformly model them as a low-rank subspace and intervene in them. Specifically, we decompose spurious correlation features into a low-rank subspace via orthogonal low-rank projection, subsequently removing this subspace from the original representation and training its orthogonal complement to capture forgery-related features. This low-rank projection removal effectively eliminates spurious correlation factors, ensuring that classification decision is based on authentic forgery cues. With only 0.43M trainable parameters, our method achieves state-of-the-art performance across several benchmarks, demonstrating excellent robustness and generalization.

</details>


### [38] [Effects of Gabor Filters on Classification Performance of CNNs Trained on a Limited Number of Conditions](https://arxiv.org/abs/2601.11918)
*Akito Morita,Hirotsugu Okuno*

Main category: cs.CV

TL;DR: 本研究提出使用Gabor滤波器预处理以提升边缘设备上CNN在机器人视觉应用的准确性和模型紧凑性。


<details>
  <summary>Details</summary>
Motivation: 边缘设备要求CNN模型体积小且在特定场景下能高效训练，常规方法难以兼顾小数据集学习和模型压缩。以生物视觉神经系统（VNS）的少样本学习能力为启发，探索将Gabor滤波器作为特征预处理器的效果。

Method: 用Gabor滤波器对输入图像预处理后，送入多种小型CNN架构，在自建的、包含不同摄像机位置和距离的图像数据集上训练和测试，并与未使用Gabor滤波器作为预处理的CNN结果进行比较。

Result: 使用Gabor滤波器预处理显著提高了CNN在新条件下的泛化能力，同时有助于减少模型参数规模。

Conclusion: Gabor滤波器作为预处理不仅提升了CNN在小样本机器人视觉任务中的泛化能力，还优化了模型尺寸，适合边缘设备部署。

Abstract: In this study, we propose a technique to improve the accuracy and reduce the size of convolutional neural networks (CNNs) running on edge devices for real-world robot vision applications. CNNs running on edge devices must have a small architecture, and CNNs for robot vision applications involving on-site object recognition must be able to be trained efficiently to identify specific visual targets from data obtained under a limited variation of conditions. The visual nervous system (VNS) is a good example that meets the above requirements because it learns from few visual experiences. Therefore, we used a Gabor filter, a model of the feature extractor of the VNS, as a preprocessor for CNNs to investigate the accuracy of the CNNs trained with small amounts of data. To evaluate how well CNNs trained on image data acquired under a limited variation of conditions generalize to data acquired under other conditions, we created an image dataset consisting of images acquired from different camera positions, and investigated the accuracy of the CNNs that trained using images acquired at a certain distance. The results were compared after training on multiple CNN architectures with and without Gabor filters as preprocessing. The results showed that preprocessing with Gabor filters improves the generalization performance of CNNs and contributes to reducing the size of CNNs.

</details>


### [39] [SupScene: Learning Overlap-Aware Global Descriptor for Unconstrained SfM](https://arxiv.org/abs/2601.11930)
*Xulei Shi,Maoyu Wang,Yuning Peng,Guanbo Wang,Xin Wang,Qi Chen,Pengjie Tao*

Main category: cs.CV

TL;DR: 本文提出SupScene方法，通过更精细的监督学习和特征聚合，有效提升了SfM任务中基于几何重叠性的图像检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的图像检索方法常用二元标签（如重叠/不重叠）进行训练，未能区分几何相似性与语义相似性，导致SfM实际需求（识别几何重叠图像）不能被很好满足。

Method: 1）提出子图训练策略，利用具有不同权重的真实几何重叠关系，用soft supervised contrastive loss实现细粒度监督；2）提出DiVLAD特征聚合器，结合ViT的多头注意力机制，通过可学习门控机制整合语义和视觉特征，生成更优全局描述子。

Result: 在GL3D数据集上，SupScene方法取得了当前最优性能，远超NetVLAD，几乎不增加额外参数。新提出的训练策略对不同聚合方式均有提升效果。

Conclusion: SupScene通过更精细的几何监督和创新的特征聚合方法，有效提升了SfM中图像检索的几何判别能力，为实际应用带来性能提升。

Abstract: Image retrieval is a critical step for alleviating the quadratic complexity of image matching in unconstrained Structure-from-Motion (SfM). However, in this context, image retrieval typically focuses more on the image pairs of geometric matchability than on those of semantic similarity, a nuance that most existing deep learning-based methods guided by batched binaries (overlapping vs. non-overlapping pairs) fail to capture. In this paper, we introduce SupScene, a novel solution that learns global descriptors tailored for finding overlapping image pairs of similar geometric nature for SfM. First, to better underline co-visible regions, we employ a subgraph-based training strategy that moves beyond equally important isolated pairs, leveraging ground-truth geometric overlapping relationships with various weights to provide fine-grained supervision via a soft supervised contrastive loss. Second, we introduce DiVLAD, a DINO-inspired VLAD aggregator that leverages the inherent multi-head attention maps from the last block of ViT. And then, a learnable gating mechanism is designed to adaptively utilize these semantically salient cues with visual features, enabling a more discriminative global descriptor. Extensive experiments on the GL3D dataset demonstrate that our method achieves state-of-the-art performance, significantly outperforming NetVLAD while introducing a negligible number of additional trainable parameters. Furthermore, we show that the proposed training strategy brings consistent gains across different aggregation techniques. Code and models are available at https://anonymous.4open.science/r/SupScene-5B73.

</details>


### [40] [Language-Guided and Motion-Aware Gait Representation for Generalizable Recognition](https://arxiv.org/abs/2601.11931)
*Zhengxian Wu,Chuanrui Zhang,Shenao Jiang,Hangrui Xu,Zirui Liao,Luyuan Zhang,Huaqiu Li,Peng Jiao,Haoqian Wang*

Main category: cs.CV

TL;DR: 提出了一种结合语言指导和运动感知的步态识别框架LMGait，以克服现有方法在动态运动区域捕捉不充分和易受静态噪声（如服装）影响的问题。


<details>
  <summary>Details</summary>
Motivation: 现有步态识别方法多使用复杂网络从图像中直接提取特征，并通过池化获得序列级表示，导致容易过拟合静态噪声且缺乏对动态运动区域的有效建模。

Method: 提出了一种新的步态识别框架LMGait，该方法引入了设计的与步态相关的语言提示，引导网络关注关键运动特征，提高了对于步态序列中动态运动区域的感知能力。

Result: 该方法能更好地捕捉步态序列的关键运动特征，并能有效减弱静态噪声对特征提取的干扰。

Conclusion: 通过引入语言提示和运动感知机制，LMGait框架提升了步态识别对动态特征的建模能力，有效解决了传统方法的局限性。

Abstract: Gait recognition is emerging as a promising technology and an innovative field within computer vision. However, existing methods typically rely on complex architectures to directly extract features from images and apply pooling operations to obtain sequence-level representations. Such designs often lead to overfitting on static noise (e.g., clothing), while failing to effectively capture dynamic motion regions.To address the above challenges, we present a Language guided and Motion-aware gait recognition framework, named LMGait.In particular, we utilize designed gait-related language cues to capture key motion features in gait sequences.

</details>


### [41] [Deep learning-based neurodevelopmental assessment in preterm infants](https://arxiv.org/abs/2601.11944)
*Lexin Ren,Jiamiao Lu,Weichuan Zhang,Benqing Wu,Tuo Wang,Yi Liao,Jiapan Guo,Changming Sun,Liang Guo*

Main category: cs.CV

TL;DR: 本文提出了一种新型神经网络（Hierarchical Dense Attention Network），用于提高早产儿脑MRI中白质和灰质分割的准确性。方法通过注意力机制和密集上采样，有效提升了低对比度数据的特征区分能力，在分割性能上优于现有技术。实验还再次证实了早产儿脑发育落后。


<details>
  <summary>Details</summary>
Motivation: 早产儿（28-37周胎龄）神经发育迟缓风险高，及时评估脑发育至关重要。但由于早产儿脑MRI中白质和灰质信号接近（同质性），自动分割精度较差，需要新的分割方法。

Method: 提出Hierarchical Dense Attention Network架构，结合三维空间-通道注意力机制和注意力引导的密集上采样策略，从而提升低对比度MRI中白质和灰质的分割效果。

Result: 在定量实验中，该方法分割准确率超越主流方法，能更好地区分MRI图像中信号接近的组织。算法应用表明，早产儿白质和灰质体积显著低于足月儿。

Conclusion: 新方法能更准确分割早产儿脑组织，有助于更早期和更可靠的神经发育评估，增强了相关医学研究和实际临床价值。

Abstract: Preterm infants (born between 28 and 37 weeks of gestation) face elevated risks of neurodevelopmental delays, making early identification crucial for timely intervention. While deep learning-based volumetric segmentation of brain MRI scans offers a promising avenue for assessing neonatal neurodevelopment, achieving accurate segmentation of white matter (WM) and gray matter (GM) in preterm infants remains challenging due to their comparable signal intensities (isointense appearance) on MRI during early brain development. To address this, we propose a novel segmentation neural network, named Hierarchical Dense Attention Network. Our architecture incorporates a 3D spatial-channel attention mechanism combined with an attention-guided dense upsampling strategy to enhance feature discrimination in low-contrast volumetric data. Quantitative experiments demonstrate that our method achieves superior segmentation performance compared to state-of-the-art baselines, effectively tackling the challenge of isointense tissue differentiation. Furthermore, application of our algorithm confirms that WM and GM volumes in preterm infants are significantly lower than those in term infants, providing additional imaging evidence of the neurodevelopmental delays associated with preterm birth. The code is available at: https://github.com/ICL-SUST/HDAN.

</details>


### [42] [Decoder Gradient Shields: A Family of Provable and High-Fidelity Methods Against Gradient-Based Box-Free Watermark Removal](https://arxiv.org/abs/2601.11952)
*Haonan An,Guang Hua,Wei Du,Hangcheng Cao,Yihang Tao,Guowen Xu,Susanto Rahardja,Yuguang Fang*

Main category: cs.CV

TL;DR: 本文提出了一种新型的神经网络水印防御方法，针对现有 box-free 水印方法中解码器易被攻击的问题，提出 Decoder Gradient Shields (DGS) 并在多场景下取得了100%防御成功率。


<details>
  <summary>Details</summary>
Motivation: 在深度神经网络知识产权保护中，box-free 水印技术因其与模型无关、能适用高熵输出而广受关注，但现有工作大多关注编码器鲁棒性，忽视了解码器安全性，导致水印易被攻击。本文发现攻击者可通过解码器的梯度信息训练水印移除器，威胁水印安全。

Method: 作者提出 Decoder Gradient Shields (DGS) 防御机制，包括输出端（DGS-O）、输入端（DGS-I）和中间层（DGS-L）的梯度保护，DGS-O提供封闭解，所有方法有理论保证。通过对解码器梯度方向和大小的联合调整，有效阻止攻击者用梯度训练移除水印。

Result: 在去雨和图像生成任务、最先进的 box-free 水印下，DGS能在所有设置下100%防止水印被移除，且保持输出图像质量。

Conclusion: Decoder Gradient Shields (DGS) 为 box-free 水印解码器提供了有效、泛化的安全防护，填补了解码器防护空白，有助于提升神经网络知识产权保护的整体安全性，实验验证了其实用价值。

Abstract: Box-free model watermarking has gained significant attention in deep neural network (DNN) intellectual property protection due to its model-agnostic nature and its ability to flexibly manage high-entropy image outputs from generative models. Typically operating in a black-box manner, it employs an encoder-decoder framework for watermark embedding and extraction. While existing research has focused primarily on the encoders for the robustness to resist various attacks, the decoders have been largely overlooked, leading to attacks against the watermark. In this paper, we identify one such attack against the decoder, where query responses are utilized to obtain backpropagated gradients to train a watermark remover. To address this issue, we propose Decoder Gradient Shields (DGSs), a family of defense mechanisms, including DGS at the output (DGS-O), at the input (DGS-I), and in the layers (DGS-L) of the decoder, with a closed-form solution for DGS-O and provable performance for all DGS. Leveraging the joint design of reorienting and rescaling of the gradients from watermark channel gradient leaking queries, the proposed DGSs effectively prevent the watermark remover from achieving training convergence to the desired low-loss value, while preserving image quality of the decoder output. We demonstrate the effectiveness of our proposed DGSs in diverse application scenarios. Our experimental results on deraining and image generation tasks with the state-of-the-art box-free watermarking show that our DGSs achieve a defense success rate of 100% under all settings.

</details>


### [43] [Real-Time Multi-Modal Embedded Vision Framework for Object Detection Facial Emotion Recognition and Biometric Identification on Low-Power Edge Platforms](https://arxiv.org/abs/2601.11970)
*S. M. Khalid Bin Zahid,Md. Rakibul Hasan Nishat,Abdul Hasib,Md. Rakibul Hasan,Md. Ashiqussalehin,Md. Sahadat Hossen Sajib,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: 本文提出了一种实时多模态视觉框架，集成了目标检测、人脸识别和情绪识别，通过自适应调度机制大幅减少边缘设备上的计算负载，实现高效智能感知。


<details>
  <summary>Details</summary>
Motivation: 现有智能监控系统通常将目标检测、人脸和情绪识别等任务分开处理，缺乏能根据实际场景动态分配资源的统一调度器，限制了在低功耗设备上的效率与整体感知能力。

Method: 设计并实现了一个集成YOLOv8n目标检测、基于FaceNet的人脸识别、DeepFace卷积情感识别的实时多模态管线，核心是基于场景触发的自适应调度机制，在Raspberry Pi 5上动态选择激活任务模块。

Result: 系统与持续处理方式相比，计算负载降低65%；目标检测AP为0.861，人脸识别准确率88%，情感识别AUC最高0.97，整体运行速率为5.6帧/秒。

Conclusion: 基于上下文感知的动态调度机制显著提升了低成本边缘硬件上多模态AI感知系统的效率和适用性，有助于实现更普惠、更注重隐私的智能感知应用。

Abstract: Intelligent surveillance systems often handle perceptual tasks such as object detection, facial recognition, and emotion analysis independently, but they lack a unified, adaptive runtime scheduler that dynamically allocates computational resources based on contextual triggers. This limits their holistic understanding and efficiency on low-power edge devices. To address this, we present a real-time multi-modal vision framework that integrates object detection, owner-specific face recognition, and emotion detection into a unified pipeline deployed on a Raspberry Pi 5 edge platform. The core of our system is an adaptive scheduling mechanism that reduces computational load by 65\% compared to continuous processing by selectively activating modules such as, YOLOv8n for object detection, a custom FaceNet-based embedding system for facial recognition, and DeepFace's CNN for emotion classification. Experimental results demonstrate the system's efficacy, with the object detection module achieving an Average Precision (AP) of 0.861, facial recognition attaining 88\% accuracy, and emotion detection showing strong discriminatory power (AUC up to 0.97 for specific emotions), while operating at 5.6 frames per second. Our work demonstrates that context-aware scheduling is the key to unlocking complex multi-modal AI on cost-effective edge hardware, making intelligent perception more accessible and privacy-preserving.

</details>


### [44] [AVIR: Adaptive Visual In-Document Retrieval for Efficient Multi-Page Document Question Answering](https://arxiv.org/abs/2601.11976)
*Zongmin Li,Yachuan Li,Lei Kang,Dimosthenis Karatzas,Wenkang Ma*

Main category: cs.CV

TL;DR: 本文提出了一种自适应视觉文档检索（AVIR）框架，以高效解决多页文档视觉问答（MP-DocVQA）中的长文档处理与计算资源消耗问题。


<details>
  <summary>Details</summary>
Motivation: 多页文档视觉问答通常涉及处理大量文档页面，这不仅带来较高的计算资源消耗，还会影响大型视觉-语言模型（LVLMs）的注意力机制效果。现有方法在相关内容选择和效率之间存在权衡，亟需提高问题相关页面的选取准确率和处理效率。

Method: AVIR框架包括三个主要步骤：1）利用轻量级检索模型对每页文档依据问题相关性进行打分；2）根据得分分布自适应聚类，选择最相关的页面，并通过Top-K方法进一步筛选以保证上下文紧凑；3）针对短文档，采用相关性概率阈值直接筛选页面，以提高聚类可靠性。筛选出的页面被输入到冻结的LVLM中生成答案，无需微调大型模型。

Result: AVIR框架在MP-DocVQA数据集上将用于问答的平均页面数减少了70%，平均无规范化分数（ANLS）达到84.58%，在计算成本大幅降低的情况下超过了以往方法的性能。同时在SlideVQA和DUDE基准上也得到了验证。

Conclusion: AVIR方法有效提升了多页文档视觉问答的效率和准确率，不依赖于大型模型微调，大幅度减少计算开销，在多个公开数据集上达到了当前最佳水平。代码已经开源。

Abstract: Multi-page Document Visual Question Answering (MP-DocVQA) remains challenging because long documents not only strain computational resources but also reduce the effectiveness of the attention mechanism in large vision-language models (LVLMs). We tackle these issues with an Adaptive Visual In-document Retrieval (AVIR) framework. A lightweight retrieval model first scores each page for question relevance. Pages are then clustered according to the score distribution to adaptively select relevant content. The clustered pages are screened again by Top-K to keep the context compact. However, for short documents, clustering reliability decreases, so we use a relevance probability threshold to select pages. The selected pages alone are fed to a frozen LVLM for answer generation, eliminating the need for model fine-tuning. The proposed AVIR framework reduces the average page count required for question answering by 70%, while achieving an ANLS of 84.58% on the MP-DocVQA dataset-surpassing previous methods with significantly lower computational cost. The effectiveness of the proposed AVIR is also verified on the SlideVQA and DUDE benchmarks. The code is available at https://github.com/Li-yachuan/AVIR.

</details>


### [45] [Nip Rumors in the Bud: Retrieval-Guided Topic-Level Adaptation for Test-Time Fake News Video Detection](https://arxiv.org/abs/2601.11981)
*Jian Lang,Rongpei Hong,Ting Zhong,Yong Wang,Fan Zhou*

Main category: cs.CV

TL;DR: 提出了用于检测假新闻视频的新方法RADAR，能够适应测试阶段出现的新话题和未见事件，显著提升模型对新型假新闻的检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有假新闻视频检测方法通常假设训练与测试阶段新闻话题分布一致，难以适应快速变化的新话题和新事件，限制了实际应用效果。

Method: 提出RADAR框架，实现针对未见假新闻视频的测试时自适应。主创新包括：1）基于信息熵的视频检索与选择机制，从目标域筛选稳定（低熵）、相关的视频作为参考；2）稳定锚点引导的对齐模块，通过与稳定参考视频的分布级匹配，使不稳定实例的特征对齐到源域，从而缓解领域分布差异；3）结合稳定参考的目标域自感知自训练机制，利用伪标签适应目标域分布变化。

Result: 大量实验结果证明，RADAR在测试时对未见假新闻视频话题具有极强的即时适应能力，在检测未见新话题假新闻视频任务上取得了优越的性能。

Conclusion: RADAR通过创新性地利用目标域稳定实例，无需训练阶段访问目标域数据，在测试时即可实现对新兴话题假新闻视频的高效检测，有望显著提升视频假新闻检测系统的实用性和鲁棒性。

Abstract: Fake News Video Detection (FNVD) is critical for social stability. Existing methods typically assume consistent news topic distribution between training and test phases, failing to detect fake news videos tied to emerging events and unseen topics. To bridge this gap, we introduce RADAR, the first framework that enables test-time adaptation to unseen news videos. RADAR pioneers a new retrieval-guided adaptation paradigm that leverages stable (source-close) videos from the target domain to guide robust adaptation of semantically related but unstable instances. Specifically, we propose an Entropy Selection-Based Retrieval mechanism that provides videos with stable (low-entropy), relevant references for adaptation. We also introduce a Stable Anchor-Guided Alignment module that explicitly aligns unstable instances' representations to the source domain via distribution-level matching with their stable references, mitigating severe domain discrepancies. Finally, our novel Target-Domain Aware Self-Training paradigm can generate informative pseudo-labels augmented by stable references, capturing varying and imbalanced category distributions in the target domain and enabling RADAR to adapt to the fast-changing label distributions. Extensive experiments demonstrate that RADAR achieves superior performance for test-time FNVD, enabling strong on-the-fly adaptation to unseen fake news video topics.

</details>


### [46] [An AI-IoT Based Smart Wheelchair with Gesture-Controlled Mobility, Deep Learning-Based Obstacle Detection, Multi-Sensor Health Monitoring, and Emergency Alert System](https://arxiv.org/abs/2601.11983)
*Md. Asiful Islam,Abdul Hasib,Tousif Mahmud Emon,Khandaker Tabin Hasan,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: 本论文提出了一种基于AI和物联网的智能轮椅系统，集成了手势控制、YOLOv8实时障碍物检测、超声波避障及生命体征监测，实现了高精度、低成本、易用性强的助残技术。


<details>
  <summary>Details</summary>
Motivation: 现有轮椅缺乏动态、智能和健康综合监测功能，且多数智能化产品价格高昂、功能单一，不能满足残障和老年人对于经济实用且智能辅助设备的巨大需求。

Method: 设计了一套模块化低成本AI-IoT轮椅系统：用手套式手势识别实现免手操作导航，利用YOLOv8结合听觉反馈实现障碍物识别与规避，超声波传感器实现即时避障，对心率、血氧、ECG和体温等生命体征实时监测并上传云端，实现自动危险预警。

Result: 手势控制准确率达95.5%，超声波障碍检测准确率为94%，基于YOLOv8的目标检测取得91.5%精度、90.2%召回率和90.8%的F1-score。

Conclusion: 提出的方法实现了轮椅多模态智能控制和健康监测，价格低廉且实用性强，为助残技术的实际落地提供了可扩展、经济高效的解决方案，有助于提升使用者的自主性与安全性。

Abstract: The growing number of differently-abled and elderly individuals demands affordable, intelligent wheelchairs that combine safe navigation with health monitoring. Traditional wheelchairs lack dynamic features, and many smart alternatives remain costly, single-modality, and limited in health integration. Motivated by the pressing demand for advanced, personalized, and affordable assistive technologies, we propose a comprehensive AI-IoT based smart wheelchair system that incorporates glove-based gesture control for hands-free navigation, real-time object detection using YOLOv8 with auditory feedback for obstacle avoidance, and ultrasonic for immediate collision avoidance. Vital signs (heart rate, SpO$_2$, ECG, temperature) are continuously monitored, uploaded to ThingSpeak, and trigger email alerts for critical conditions. Built on a modular and low-cost architecture, the gesture control achieved a 95.5\% success rate, ultrasonic obstacle detection reached 94\% accuracy, and YOLOv8-based object detection delivered 91.5\% Precision, 90.2\% Recall, and a 90.8\% F1-score. This integrated, multi-modal approach offers a practical, scalable, and affordable solution, significantly enhancing user autonomy, safety, and independence by bridging the gap between innovative research and real-world deployment.

</details>


### [47] [Structural Graph Neural Networks with Anatomical Priors for Explainable Chest X-ray Diagnosis](https://arxiv.org/abs/2601.11987)
*Khaled Berkani*

Main category: cs.CV

TL;DR: 本文提出了一种结构化图推理框架，将显式解剖先验引入视觉诊断任务，提高模型可解释性。通过将卷积特征图转化为图结构，节点编码外观与空间坐标，边表示局部结构相邻性，模型能实现更具结构感知的推理和易解释诊断。


<details>
  <summary>Details</summary>
Motivation: 医学影像诊断对可解释性有强需求，现有视觉模型很难显式建模结构关系，缺乏结构先验的推理往往难以让医生信服。本文旨在通过图结构引入显式解剖结构，提高诊断可靠性和可解释性。

Method: 将卷积特征映射转化为基于patch的图结构，节点包含外观及空间信息，边表示局部相邻。设计了一种显式建模相对空间关系的结构传播机制，提升结构推理能力。模型可联合执行节点级病灶识别与全图级诊断，通过节点重要性分数实现内在可解释性。

Result: 通过胸部X光案例验证了方法的有效性。模型不仅具备结构先验指导下的推理能力，还较好地提升了诊断解释性，无需借助额外可视化。结果表明该方法具有领域泛化性。

Conclusion: 结构化图推理框架通过引入结构先验，有效提升了医学影像诊断的可解释性与结构化推理能力，对AI系统中的结构感知与可解释学习领域具有广泛意义。

Abstract: We present a structural graph reasoning framework that incorporates explicit anatomical priors for explainable vision-based diagnosis. Convolutional feature maps are reinterpreted as patch-level graphs, where nodes encode both appearance and spatial coordinates, and edges reflect local structural adjacency. Unlike conventional graph neural networks that rely on generic message passing, we introduce a custom structural propagation mechanism that explicitly models relative spatial relations as part of the reasoning process. This design enables the graph to act as an inductive bias for structured inference rather than a passive relational representation. The proposed model jointly supports node-level lesion-aware predictions and graph-level diagnostic reasoning, yielding intrinsic explainability through learned node importance scores without relying on post-hoc visualization techniques. We demonstrate the approach through a chest X-ray case study, illustrating how structural priors guide relational reasoning and improve interpretability. While evaluated in a medical imaging context, the framework is domain-agnostic and aligns with the broader vision of graph-based reasoning across artificial intelligence systems. This work contributes to the growing body of research exploring graphs as computational substrates for structure-aware and explainable learning.

</details>


### [48] [DAOS: A Multimodal In-cabin Behavior Monitoring with Driver Action-Object Synergy Dataset](https://arxiv.org/abs/2601.11990)
*Yiming Li,Chen Cai,Tianyi Liu,Dan Lin,Wenqian Wang,Wenfei Liang,Bingbing Li,Kim-Hui Yap*

Main category: cs.CV

TL;DR: 本文提出了一个名为DAOS的全新驾驶员动作与物体协同数据集，并开发了AOR-Net模型，显著提升了驾驶员动作识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶员监测数据集缺乏精准的物体定位注释，也没能把物体和相关动作关联起来，导致动作识别的准确性受限。本研究动机是通过完善数据标注和关系建模，提升细粒度驾驶员动作识别的性能。

Method: 1. 构建DAOS数据集，包含9787个视频片段，涵盖36种细粒度动作和15类物体，数据多模态多视角。2. 提出AOR-Net模型，结合多层次推理和chain-of-action prompting机制，建模动作与物体及其关系。3. 引入Mixture of Thoughts模块，在不同时刻动态选择必要知识，增强模型对物体丰富和稀缺场景的适应能力。

Result: 大量实验表明，AOR-Net在多个数据集上的动作识别任务中，均优于当前主流方法，验证了模型的有效性和鲁棒性。

Conclusion: 通过构建高质量的DAOS数据集与设计AOR-Net模型，极大提升了驾驶员动作识别的准确率，有力推动了智能驾驶监控技术的发展。

Abstract: In driver activity monitoring, movements are mostly limited to the upper body, which makes many actions look similar. To tell these actions apart, human often rely on the objects the driver is using, such as holding a phone compared with gripping the steering wheel. However, most existing driver-monitoring datasets lack accurate object-location annotations or do not link objects to their associated actions, leaving a critical gap for reliable action recognition. To address this, we introduce the Driver Action with Object Synergy (DAOS) dataset, comprising 9,787 video clips annotated with 36 fine-grained driver actions and 15 object classes, totaling more than 2.5 million corresponding object instances. DAOS offers multi-modal, multi-view data (RGB, IR, and depth) from front, face, left, and right perspectives. Although DAOS captures a wide range of cabin objects, only a few are directly relevant to each action for prediction, so focusing on task-specific human-object relations is essential. To tackle this challenge, we propose the Action-Object-Relation Network (AOR-Net). AOR-Net comprehends complex driver actions through multi-level reasoning and a chain-of-action prompting mechanism that models the logical relationships among actions, objects, and their relations. Additionally, the Mixture of Thoughts module is introduced to dynamically select essential knowledge at each stage, enhancing robustness in object-rich and object-scarce conditions. Extensive experiments demonstrate that our model outperforms other state-of-the-art methods on various datasets.

</details>


### [49] [SMc2f: Robust Scenario Mining for Robotic Autonomy from Coarse to Fine](https://arxiv.org/abs/2601.12010)
*Yifei Chen,Ross Greer*

Main category: cs.CV

TL;DR: 本文提出了一种用于自动驾驶机器人安全验证的场景挖掘新方法（SMc2f），显著提升了罕见场景的检索质量与效率。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶场景挖掘方法（如RefAV）在以自然语言描述检索场景时，主要依赖轨迹标签而忽略了语言与原始图像之间的直接关联，并受上游3D目标检测等模块影响，导致空间与时间定位不准确，影响安全验证效果。

Method: 作者提出了SMc2f的coarse-to-fine流水线：先用视觉-语言模型（VLMs）进行图文粗筛，再基于RefAV建立自动化的优质案例库，用于few-shot方式提升大语言模型检索能力；最后结合文本-轨迹对比学习对嵌入空间进行细粒度匹配，提高匹配准确性。

Result: 在公开数据集上的实验结果显示，SMc2f方法在检索质量和效率上均取得了显著提升。

Conclusion: SMc2f为基于自然语言的机器人安全场景检索提供了更为稳健且高效的解决方案，有助于自动驾驶系统的目标回溯、回归测试与失效分析。

Abstract: The safety validation of autonomous robotic vehicles hinges on systematically testing their planning and control stacks against rare, safety-critical scenarios. Mining these long-tail events from massive real-world driving logs is therefore a critical step in the robotic development lifecycle. The goal of the Scenario Mining task is to retrieve useful information to enable targeted re-simulation, regression testing, and failure analysis of the robot's decision-making algorithms. RefAV, introduced by the Argoverse team, is an end-to-end framework that uses large language models (LLMs) to spatially and temporally localize scenarios described in natural language. However, this process performs retrieval on trajectory labels, ignoring the direct connection between natural language and raw RGB images, which runs counter to the intuition of video retrieval; it also depends on the quality of upstream 3D object detection and tracking. Further, inaccuracies in trajectory data lead to inaccuracies in downstream spatial and temporal localization. To address these issues, we propose Robust Scenario Mining for Robotic Autonomy from Coarse to Fine (SMc2f), a coarse-to-fine pipeline that employs vision-language models (VLMs) for coarse image-text filtering, builds a database of successful mining cases on top of RefAV and automatically retrieves exemplars to few-shot condition the LLM for more robust retrieval, and introduces text-trajectory contrastive learning to pull matched pairs together and push mismatched pairs apart in a shared embedding space, yielding a fine-grained matcher that refines the LLM's candidate trajectories. Experiments on public datasets demonstrate substantial gains in both retrieval quality and efficiency.

</details>


### [50] [SAR-Based Marine Oil Spill Detection Using the DeepSegFusion Architecture](https://arxiv.org/abs/2601.12015)
*Pavan Kumar Yata,Pediredla Pradeep,Goli Himanish,Swathi M*

Main category: cs.CV

TL;DR: 本文提出了一个名为DeepSegFusion的混合深度学习模型，用于合成孔径雷达(SAR)图像中的溢油分割，相较于传统和单一基线模型，极大降低了误报率并提升了检测精度。


<details>
  <summary>Details</summary>
Motivation: 溢油事件会对环境和航运安全造成重大影响。传统基于阈值的方法在识别油污时常因类似现象（如风滑和船迹）引发高误报率，限制了其实用性。因此，发展更精准、鲁棒的自动化识别方法至关重要。

Method: 提出了DeepSegFusion模型，将SegNet与DeepLabV3+结合，并利用基于注意力的特征融合机制，增强分割边界精度及上下文理解能力。

Result: 在SAR溢油数据集（含ALOS PALSAR图像）上，DeepSegFusion达到了94.85%的准确率、0.5685的IoU和0.9330的ROC-AUC分数，相比传统方法和单一模型将误报率降低了64.4%。

Conclusion: DeepSegFusion在多种海洋条件下均表现出高稳定性，能够实现近实时的溢油监测，具备广泛的实际应用潜力。

Abstract: Detection of oil spills from satellite images is essential for both environmental surveillance and maritime safety. Traditional threshold-based methods frequently encounter performance degradation due to very high false alarm rates caused by look-alike phenomena such as wind slicks and ship wakes. Here, a hybrid deep learning model, DeepSegFusion, is presented for oil spill segmentation in Synthetic Aperture Radar (SAR) images. The model uses SegNet and DeepLabV3+ integrated with an attention-based feature fusion mechanism to achieve better boundary precision as well as improved contextual understanding. Results obtained on SAR oil spill datasets, including ALOS PALSAR imagery, confirm that the proposed DeepSegFusion model achieves an accuracy of 94.85%, an Intersection over Union (IoU) of 0.5685, and a ROC-AUC score of 0.9330. The proposed method delivers more than three times fewer false detections compared to individual baseline models and traditional non-segmentation methods, achieving a reduction of 64.4%. These results indicate that DeepSegFusion is a stable model under various marine conditions and can therefore be used in near real-time oil spill monitoring scenarios.

</details>


### [51] [DIAMOND-SSS: Diffusion-Augmented Multi-View Optimization for Data-efficient SubSurface Scattering](https://arxiv.org/abs/2601.12020)
*Guillermo Figueroa-Araneda,Iris Diana Jimenez,Florian Hofherr,Manny Ko,Hector Andrade-Loarca,Daniel Cremers*

Main category: cs.CV

TL;DR: DIAMOND-SSS是一种高效的数据框架，可用极少的数据实现高保真半透明材料建模与新视图合成，大幅减少对昂贵多视角、多光源数据采集的需求。


<details>
  <summary>Details</summary>
Motivation: 半透明材料（如蜡、玉、皮肤）存在复杂的次表面散射效应，传统神经渲染需要大量多视角、多光源的数据，获取昂贵，限制了应用。为降低数据采集成本，推动高质量半透明材料重建技术发展，作者提出新方案。

Method: 提出DIAMOND-SSS框架：基于diffusion模型，通过极少量（如十张）图片微调用于新视角合成及重光照，结合估算几何信息，只需训练集的7%。利用diffusion模型产生高仿真增强数据，补足高达95%的数据缺口。为提升稀疏或合成监督下的重建稳定性，引入多视角轮廓一致性损失和多视角深度一致性损失作为几何先验。

Result: DIAMOND-SSS在非常稀疏数据条件下（仅10张图片），也能实现高质量、可重光照的高斯渲染效果（relightable Gaussian rendering），性能超越现有方法，并可减少高达90%的真实采集成本。

Conclusion: DIAMOND-SSS显著推进了稀疏数据条件下半透明材料重建和新视图合成的效果，有助于神经渲染领域相关应用的普及和发展。

Abstract: Subsurface scattering (SSS) gives translucent materials -- such as wax, jade, marble, and skin -- their characteristic soft shadows, color bleeding, and diffuse glow. Modeling these effects in neural rendering remains challenging due to complex light transport and the need for densely captured multi-view, multi-light datasets (often more than 100 views and 112 OLATs).
  We present DIAMOND-SSS, a data-efficient framework for high-fidelity translucent reconstruction from extremely sparse supervision -- even as few as ten images. We fine-tune diffusion models for novel-view synthesis and relighting, conditioned on estimated geometry and trained on less than 7 percent of the dataset, producing photorealistic augmentations that can replace up to 95 percent of missing captures. To stabilize reconstruction under sparse or synthetic supervision, we introduce illumination-independent geometric priors: a multi-view silhouette consistency loss and a multi-view depth consistency loss.
  Across all sparsity regimes, DIAMOND-SSS achieves state-of-the-art quality in relightable Gaussian rendering, reducing real capture requirements by up to 90 percent compared to SSS-3DGS.

</details>


### [52] [\textit{FocaLogic}: Logic-Based Interpretation of Visual Model Decisions](https://arxiv.org/abs/2601.12049)
*Chenchen Zhao,Muxi Chen,Qiang Xu*

Main category: cs.CV

TL;DR: FocaLogic是一种新颖的与模型无关的可解释性框架，通过逻辑表达式精准地解释并量化视觉模型的决策过程。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型可解释性方法依赖白盒访问或缺乏量化严格性，难以满足高风险领域对透明和精确解释的需求。

Method: 提出FocaLogic框架，寻找影响模型预测的最小可解释视觉区域（称为visual focuses），并将其转化为紧凑、精确的逻辑表达式，同时引入可量化评价指标如focus precision、recall和divergence等。

Result: 实验表明，FocaLogic能够揭示模型训练过程中的关注集中现象、泛化提升后的关注准确性增加，以及在偏差和对抗攻击下的异常关注情况。

Conclusion: FocaLogic为解释视觉模型提供了一种系统化、可扩展且量化的方法，具有良好的解释能力和实用性。

Abstract: Interpretability of modern visual models is crucial, particularly in high-stakes applications. However, existing interpretability methods typically suffer from either reliance on white-box model access or insufficient quantitative rigor. To address these limitations, we introduce FocaLogic, a novel model-agnostic framework designed to interpret and quantify visual model decision-making through logic-based representations. FocaLogic identifies minimal interpretable subsets of visual regions-termed visual focuses-that decisively influence model predictions. It translates these visual focuses into precise and compact logical expressions, enabling transparent and structured interpretations. Additionally, we propose a suite of quantitative metrics, including focus precision, recall, and divergence, to objectively evaluate model behavior across diverse scenarios. Empirical analyses demonstrate FocaLogic's capability to uncover critical insights such as training-induced concentration, increasing focus accuracy through generalization, and anomalous focuses under biases and adversarial attacks. Overall, FocaLogic provides a systematic, scalable, and quantitative solution for interpreting visual models.

</details>


### [53] [A Unified Masked Jigsaw Puzzle Framework for Vision and Language Models](https://arxiv.org/abs/2601.12051)
*Weixin Ye,Wei Wang,Yahui Liu,Yue Song,Bin Ren,Wei Bi,Rita Cucchiara,Nicu Sebe*

Main category: cs.CV

TL;DR: 本文提出一种名为Masked Jigsaw Puzzle (MJP)的新框架，旨在提升Transformer在联邦学习中的防御能力和任务表现，其核心思想是扰乱位置嵌入信息，有效减少梯度攻击风险并提升CV/NLP任务的模型精度。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习场景下，Transformer模型的梯度信息（尤其是位置嵌入部分）易受攻击并可被用以重建输入数据，导致隐私泄露；同时提升模型在图像和文本任务上的表现也具有实际意义。

Method: 提出Masked Jigsaw Puzzle (MJP)框架，通过随机打乱token顺序，并用可学习的unk位置嵌入来mask掉打乱后的token PE，从而扰乱原有位置嵌入所编码的局部空间信息，促使模型学到对局部空间不敏感的表示。

Result: 实验表明MJP能有效提升Transformer模型对梯度攻击的鲁棒性，并在图像分类（如ImageNet-1K）及文本情感分析（如Yelp和Amazon）等多任务上带来性能增益。此外，MJP可以推广到多种Transformer架构。

Conclusion: MJP为CV与NLP任务提供了通用的Transformer防护与增强框架，能够在提升安全性的同时不牺牲甚至提升模型性能，具有较高的应用价值。

Abstract: In federated learning, Transformer, as a popular architecture, faces critical challenges in defending against gradient attacks and improving model performance in both Computer Vision (CV) and Natural Language Processing (NLP) tasks. It has been revealed that the gradient of Position Embeddings (PEs) in Transformer contains sufficient information, which can be used to reconstruct the input data. To mitigate this issue, we introduce a Masked Jigsaw Puzzle (MJP) framework. MJP starts with random token shuffling to break the token order, and then a learnable \textit{unknown (unk)} position embedding is used to mask out the PEs of the shuffled tokens. In this manner, the local spatial information which is encoded in the position embeddings is disrupted, and the models are forced to learn feature representations that are less reliant on the local spatial information. Notably, with the careful use of MJP, we can not only improve models' robustness against gradient attacks, but also boost their performance in both vision and text application scenarios, such as classification for images (\textit{e.g.,} ImageNet-1K) and sentiment analysis for text (\textit{e.g.,} Yelp and Amazon). Experimental results suggest that MJP is a unified framework for different Transformer-based models in both vision and language tasks. Code is publicly available via https://github.com/ywxsuperstar/transformerattack

</details>


### [54] [Task-Driven Prompt Learning: A Joint Framework for Multi-modal Cloud Removal and Segmentation](https://arxiv.org/abs/2601.12052)
*Zaiyan Zhang,Jie Li,Shaowei Shi,Qiangqiang Yuan*

Main category: cs.CV

TL;DR: 该论文提出了一种名为TDP-CR的多模态框架，同时进行云层去除和地物分类分割，通过创新性的Prompt-Guided Fusion模块自适应融合SAR信息，只在光学遥感数据被云遮挡时整合，从而提升分析就绪数据的质量。实验显示该方法在效果和参数效率上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前遥感光学影像广泛用于地球观测，但其下游应用受到持续的云层遮挡严重限制。已有云去除方法虽在图像层面修复效果较好，但往往导致过度平滑，不利于后续语义分析。如何兼顾视觉效果与语义分析能力，是遥感影像处理面临的关键难题。

Method: 作者设计了TDP-CR框架，将云去除与地物分割联合处理。核心为Prompt-Guided Fusion模块，利用可学习的降质提示，结合全局通道上下文和局部空间偏置，仅在光学影像受损区域引入SAR数据，从而实现针对性信息融合。同时，采用高效的两阶段训练策略，分别优化重建与语义表征。

Result: 在LuojiaSET-OSFCR数据集上，TDP-CR框架在PSNR指标上比参数量较大的先进方法高0.18 dB，模型参数量仅为其15%。在地物分割mIoU指标上也比多任务对手提升了1.4%。

Conclusion: TDP-CR框架通过任务驱动、多模态融合，实现了更优的遥感影像云去除与精准地物分割，为分析就绪遥感数据提供了高效新方案，对实际应用具有较大价值。

Abstract: Optical remote sensing imagery is indispensable for Earth observation, yet persistent cloud occlusion limits its downstream utility. Most cloud removal (CR) methods are optimized for low-level fidelity and can over-smooth textures and boundaries that are critical for analysis-ready data (ARD), leading to a mismatch between visually plausible restoration and semantic utility. To bridge this gap, we propose TDP-CR, a task-driven multimodal framework that jointly performs cloud removal and land-cover segmentation. Central to our approach is a Prompt-Guided Fusion (PGF) mechanism, which utilizes a learnable degradation prompt to encode cloud thickness and spatial uncertainty. By combining global channel context with local prompt-conditioned spatial bias, PGF adaptively integrates Synthetic Aperture Radar (SAR) information only where optical data is corrupted. We further introduce a parameter-efficient two-phase training strategy that decouples reconstruction and semantic representation learning. Experiments on the LuojiaSET-OSFCR dataset demonstrate the superiority of our framework: TDP-CR surpasses heavy state-of-the-art baselines by 0.18 dB in PSNR while using only 15\% of the parameters, and achieves a 1.4\% improvement in mIoU consistently against multi-task competitors, effectively delivering analysis-ready data.

</details>


### [55] [Automating Parameter Selection in Deep Image Prior for Fluorescence Microscopy Image Denoising via Similarity-Based Parameter Transfer](https://arxiv.org/abs/2601.12055)
*Lina Meyer,Felix Wissel,Tobias Knopp,Susanne Pfefferle,Ralf Fliegert,Maximilian Sandmann,Liana Uebler,Franziska Möckl,Björn-Philipp Diercks,David Lohr,René Werner*

Main category: cs.CV

TL;DR: 本文提出了一种名为AUTO-DIP的自动参数迁移方法，用于无监督DIP在荧光显微镜图像去噪中的参数优化，加速DIP应用，并提升去噪效果，尤其在高噪声层面表现优越。


<details>
  <summary>Details</summary>
Motivation: 传统DIP受限于网络结构与迭代终止点需单独为每幅图像优化，无法高效处理大批量图像，尤其在荧光显微镜数据方面。作者希望减少参数搜索时间，使DIP更适用于高通量场景。

Method: 作者构建了包含110张校准集和55张验证集的荧光显微镜图像集，系统搜索适配的U-Net网络结构与迭代终止点。提出AUTO-DIP，基于图像元数据（如显微镜类型、样本种类）把校准参数迁移到新图像，并与基于图像内容相似度的迁移及其他方法对比。

Result: 实验表明，仅用元数据驱动的参数迁移可获得与或优于基于定量图像相似度迁移的效果。AUTO-DIP在多种公开数据集（特别是高噪声条件）下，去噪性能优于原始DIP配置和先进的变分去噪方法。局部采集样本实验也验证了AUTO-DIP的优越性。

Conclusion: AUTO-DIP通过自动化参数迁移，显著提升了DIP在荧光显微镜图像去噪的效率与效果，减少人工调参，适合高通量成像分析，并有潜力应用于实际实验场景。

Abstract: Unsupervised deep image prior (DIP) addresses shortcomings of training data requirements and limited generalization associated with supervised deep learning. The performance of DIP depends on the network architecture and the stopping point of its iterative process. Optimizing these parameters for a new image requires time, restricting DIP application in domains where many images need to be processed. Focusing on fluorescence microscopy data, we hypothesize that similar images share comparable optimal parameter configurations for DIP-based denoising, potentially enabling optimization-free DIP for fluorescence microscopy. We generated a calibration (n=110) and validation set (n=55) of semantically different images from an open-source dataset for a network architecture search targeted towards ideal U-net architectures and stopping points. The calibration set represented our transfer basis. The validation set enabled the assessment of which image similarity criterion yields the best results. We then implemented AUTO-DIP, a pipeline for automatic parameter transfer, and compared it to the originally published DIP configuration (baseline) and a state-of-the-art image-specific variational denoising approach. We show that a parameter transfer from the calibration dataset to a test image based on only image metadata similarity (e.g., microscope type, imaged specimen) leads to similar and better performance than a transfer based on quantitative image similarity measures. AUTO-DIP outperforms the baseline DIP (DIP with original DIP parameters) as well as the variational denoising approaches for several open-source test datasets of varying complexity, particularly for very noisy inputs. Applications to locally acquired fluorescence microscopy images further proved superiority of AUTO-DIP.

</details>


### [56] [Learning Language-Driven Sequence-Level Modal-Invariant Representations for Video-Based Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2601.12062)
*Xiaomei Yang,Xizhan Gao,Antai Liu,Kang Wei,Fa Zhu,Guang Feng,Xiaofeng Qu,Sijie Niu*

Main category: cs.CV

TL;DR: 本论文提出了一种面向视频的可见光-红外行人重识别的新方法——LSMRL，通过引入时空特征学习、语义扩散和跨模态交互模块，有效提升了模态无关表示的学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有利用CLIP生成的共享语言提示虽然提升了模态无关表征的学习，但在时空建模效率、充分的跨模态交互以及显式的模态损失指导方面仍有不足，影响了ReID模型在复杂场景下的泛化与判别能力。

Method: LSMRL方法包括三个核心模块：1）在CLIP基础上极小修改得到的时空特征学习（STFL）模块提升建模效率；2）语义扩散（SD）模块将共享的语言提示扩散到可见光和红外特征之中，建立初步模态一致性；3）跨模态交互（CMI）模块利用双向自注意机制，进一步消除残余模态差距。最后，引入两项模态级别损失，提升特征的判别与泛化能力。

Result: 在大规模VVI-ReID数据集上的实验显示，LSMRL在表现上优于现有最优的方法（AOTA），验证了提出方法的有效性和先进性。

Conclusion: LSMRL能够高效、有效地学习序列级的模态无关表征，显著提升了视频场景下的可见光-红外行人重识别效果，在判别能力与泛化性方面优于以往方法。

Abstract: The core of video-based visible-infrared person re-identification (VVI-ReID) lies in learning sequence-level modal-invariant representations across different modalities. Recent research tends to use modality-shared language prompts generated by CLIP to guide the learning of modal-invariant representations. Despite achieving optimal performance, such methods still face limitations in efficient spatial-temporal modeling, sufficient cross-modal interaction, and explicit modality-level loss guidance. To address these issues, we propose the language-driven sequence-level modal-invariant representation learning (LSMRL) method, which includes spatial-temporal feature learning (STFL) module, semantic diffusion (SD) module and cross-modal interaction (CMI) module. To enable parameter- and computation-efficient spatial-temporal modeling, the STFL module is built upon CLIP with minimal modifications. To achieve sufficient cross-modal interaction and enhance the learning of modal-invariant features, the SD module is proposed to diffuse modality-shared language prompts into visible and infrared features to establish preliminary modal consistency. The CMI module is further developed to leverage bidirectional cross-modal self-attention to eliminate residual modality gaps and refine modal-invariant representations. To explicitly enhance the learning of modal-invariant representations, two modality-level losses are introduced to improve the features' discriminative ability and their generalization to unseen categories. Extensive experiments on large-scale VVI-ReID datasets demonstrate the superiority of LSMRL over AOTA methods.

</details>


### [57] [Learning Stochastic Bridges for Video Object Removal via Video-to-Video Translation](https://arxiv.org/abs/2601.12066)
*Zijie Lou,Xiangwei Feng,Jiaxin Wang,Xiaochao Qu,Luoqi Liu,Ting Liu*

Main category: cs.CV

TL;DR: 本文提出一种新的视频物体移除方法，通过建立直接从带目标视频到无目标视频的随机桥接模型，避免传统扩散模型从高斯噪声出发的不足，取得更优视觉与时序一致性效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频物体移除方法主要基于从高斯噪声生成，忽略了原始视频中的丰富结构和上下文信息，导致物体擦除不彻底或填充内容不合理。因此，亟需一种更好利用原始视频信息、提升擦除质量的方法。

Method: 作者将视频物体移除问题重新表述为视频到视频的翻译任务，设计了一种随机桥接模型，直接建立带物体和无物体视频间的随机路径，充分利用原始视频作为结构先验，提升擦除精度和合理性。为解决强先验导致大物体难以移除的问题，还引入了自适应掩码调制策略，根据掩码特性动态调整输入嵌入，平衡背景保真度和生成灵活性。

Result: 通过大量实验证明，所提方法在视觉质量和时序一致性上均显著优于现有视频物体移除技术。

Conclusion: 新方法能够精准移除视频中的目标物体，同时保证虚拟填充区域与环境逻辑一致，为视频内容编辑任务提供了更强大、稳定的技术支持。

Abstract: Existing video object removal methods predominantly rely on diffusion models following a noise-to-data paradigm, where generation starts from uninformative Gaussian noise. This approach discards the rich structural and contextual priors present in the original input video. Consequently, such methods often lack sufficient guidance, leading to incomplete object erasure or the synthesis of implausible content that conflicts with the scene's physical logic. In this paper, we reformulate video object removal as a video-to-video translation task via a stochastic bridge model. Unlike noise-initialized methods, our framework establishes a direct stochastic path from the source video (with objects) to the target video (objects removed). This bridge formulation effectively leverages the input video as a strong structural prior, guiding the model to perform precise removal while ensuring that the filled regions are logically consistent with the surrounding environment. To address the trade-off where strong bridge priors hinder the removal of large objects, we propose a novel adaptive mask modulation strategy. This mechanism dynamically modulates input embeddings based on mask characteristics, balancing background fidelity with generative flexibility. Extensive experiments demonstrate that our approach significantly outperforms existing methods in both visual quality and temporal consistency.

</details>


### [58] [ARMARecon: An ARMA Convolutional Filter based Graph Neural Network for Neurodegenerative Dementias Classification](https://arxiv.org/abs/2601.12067)
*VSS Tejaswi Abburi,Ananya Singhal,Saurabh J. Shigwan,Nitin Kumar*

Main category: cs.CV

TL;DR: 本文提出了一种新的图神经网络方法ARMARecon，用于更有效地检测和分类早期的神经退行性疾病（如阿尔茨海默病和额颞叶痴呆），在多个真实数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病和额颞叶痴呆等神经退行性疾病早期检测非常重要，但由于其沿白质区域以全局、图相关方式传播，传统方法难以充分捕捉这些模式，需要更合适的神经网络来提升检测准确率。

Method: 提出了一种新的统一图学习框架ARMARecon，将自回归滑动平均（ARMA）图滤波与重建驱动目标结合，利用来自白质区域的20-bin FA直方图特征，既能捕捉局部也能捕捉全局连接，同时缓解过度平滑现象。

Result: ARMARecon在多站点的dMRI数据集（ADNI和NIFD）上，分类准确率显著优于目前主流的最先进方法。

Conclusion: ARMARecon不仅提升了特征表达能力和分类效果，而且为早期神经退行性疾病的检测提供了有效的新方案，对实际临床应用具有重要意义。

Abstract: Early detection of neurodegenerative diseases such as Alzheimer's Disease (AD) and Frontotemporal Dementia (FTD) is essential for reducing the risk of progression to severe disease stages. As AD and FTD propagate along white-matter regions in a global, graph-dependent manner, graph-based neural networks are well suited to capture these patterns. Hence, we introduce ARMARecon, a unified graph learning framework that integrates Autoregressive Moving Average (ARMA) graph filtering with a reconstruction-driven objective to enhance feature representation and improve classification accuracy. ARMARecon effectively models both local and global connectivity by leveraging 20-bin Fractional Anisotropy (FA) histogram features extracted from white-matter regions, while mitigating over-smoothing. Overall, ARMARecon achieves superior performance compared to state-of-the-art methods on the multi-site dMRI datasets ADNI and NIFD.

</details>


### [59] [CroBIM-V: Memory-Quality Controlled Remote Sensing Referring Video Object Segmentation](https://arxiv.org/abs/2601.12076)
*H. Jiang,Y. Sun,Z. Dong,T. Liu,Y. Gu*

Main category: cs.CV

TL;DR: 本文针对遥感视频关联目标分割（RS-RVOS）中的目标表观弱、信息丢失等难点，提出了大规模新数据集RS-RVOS Bench，并设计了一种以记忆质量控制的分割方法MQC-SAM，实现了业界领先的效果。


<details>
  <summary>Details</summary>
Motivation: 当前RS-RVOS面临目标显著性低、动态遮挡导致的表征丢失等困难，且缺乏大规模高质量数据集，现有模型容易因初始记忆偏差和误累积导致分割效果下降和误差传播。

Method: 1）构建RS-RVOS Bench数据集，包含111段视频、约2.5万帧和21.3万个因果约束标注；2）提出MQC-SAM网络，结合运动一致性辅助生成高质量记忆，利用分离注意力机制与动态质量评估，仅累积高置信度有效信息，有效过滤噪声。

Result: 在RS-RVOS Bench数据集上，MQC-SAM方法在准确性和鲁棒性方面显著优于现有方法，取得了最先进的分割性能。

Conclusion: 高质量数据集和专门设计的记忆控制机制能够显著提升RS-RVOS分割性能，为后续遥感视频理解提供基础。

Abstract: Remote sensing video referring object segmentation (RS-RVOS) is challenged by weak target saliency and severe visual information truncation in dynamic scenes, making it extremely difficult to maintain discriminative target representations during segmentation. Moreover, progress in this field is hindered by the absence of large-scale dedicated benchmarks, while existing models are often affected by biased initial memory construction that impairs accurate instance localization in complex scenarios, as well as indiscriminate memory accumulation that encodes noise from occlusions or misclassifications, leading to persistent error propagation. This paper advances RS-RVOS research through dual contributions in data and methodology. First, we construct RS-RVOS Bench, the first large-scale benchmark comprising 111 video sequences, about 25,000 frames, and 213,000 temporal referring annotations. Unlike common RVOS benchmarks where many expressions are written with access to the full video context, our dataset adopts a strict causality-aware annotation strategy in which linguistic references are generated solely from the target state in the initial frame. Second, we propose a memory-quality-aware online referring segmentation framework, termed Memory Quality Control with Segment Anything Model (MQC-SAM). MQC-SAM introduces a temporal motion consistency module for initial memory calibration, leveraging short-term motion trajectory priors to correct structural deviations and establish accurate memory anchoring. Furthermore, it incorporates a decoupled attention-based memory integration mechanism with dynamic quality assessment, selectively updating high-confidence semantic features while filtering unreliable information, thereby effectively preventing error accumulation and propagation. Extensive experiments on RS-RVOS Bench demonstrate that MQC-SAM achieves state-of-the-art performance.

</details>


### [60] [EmoLat: Text-driven Image Sentiment Transfer via Emotion Latent Space](https://arxiv.org/abs/2601.12079)
*Jing Zhang,Bingjie Fan,Jixiang Zhu,Zhe Wang*

Main category: cs.CV

TL;DR: 本文提出了EmoLat，一种新型情感潜在空间，实现了基于文本的图像情感细粒度迁移，并搭建了配套大规模数据集，显著超越了现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有图像情感编辑方法尚无法有效结合文本指导，实现细粒度、可控的情感迁移，因此需要构建跨模态建模能力更强的方法。

Method: 1）提出EmoLat，建模文本语义与视觉情感特征的跨模态关联。2）通过情感语义图刻画情感、物体和视觉属性间结构。3）采用对抗式正则化对齐跨模态情感分布。4）联合文本和EmoLat特征，实现跨模态情感迁移。5）设计多目标损失函数优化网络。6）新建大规模EmoSpace Set数据集。

Result: 在EmoSpace Set数据集上，提出方法在定量指标和迁移质量上均大幅领先现有方法，表现出更优的文本驱动图像情感编辑能力。

Conclusion: EmoLat和EmoSpace Set为文本引导的图像情感可控迁移设立了新范式，显著提升了编辑的可控性和精度，具备良好的应用前景。

Abstract: We propose EmoLat, a novel emotion latent space that enables fine-grained, text-driven image sentiment transfer by modeling cross-modal correlations between textual semantics and visual emotion features. Within EmoLat, an emotion semantic graph is constructed to capture the relational structure among emotions, objects, and visual attributes. To enhance the discriminability and transferability of emotion representations, we employ adversarial regularization, aligning the latent emotion distributions across modalities. Building upon EmoLat, a cross-modal sentiment transfer framework is proposed to manipulate image sentiment via joint embedding of text and EmoLat features. The network is optimized using a multi-objective loss incorporating semantic consistency, emotion alignment, and adversarial regularization. To support effective modeling, we construct EmoSpace Set, a large-scale benchmark dataset comprising images with dense annotations on emotions, object semantics, and visual attributes. Extensive experiments on EmoSpace Set demonstrate that our approach significantly outperforms existing state-of-the-art methods in both quantitative metrics and qualitative transfer fidelity, establishing a new paradigm for controllable image sentiment editing guided by textual input. The EmoSpace Set and all the code are available at http://github.com/JingVIPLab/EmoLat.

</details>


### [61] [Toward Real-World High-Precision Image Matting and Segmentation](https://arxiv.org/abs/2601.12080)
*Haipeng Zhou,Zhaohu Xing,Hongqiu Wang,Jun Ma,Ping Li,Lei Zhu*

Main category: cs.CV

TL;DR: 本论文提出了FCLM模型，针对高精度场景解析任务（如抠图和二分类分割）中的细节预测、目标类别泛化和数据不一致等问题，结合深度感知蒸馏和领域不变学习，实现更优的前景识别，并支持多模态交互预测。实验证明方法优于现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有高精度分割方法主要关注突出、单一前景目标，交互式方法则难以泛化至不同类别。此外，缺乏高质量标注导致对不协调合成数据的依赖，严重影响实际场景泛化能力。作者意在解决这些瓶颈，提升模型对细节和多类别泛化能力。

Method: 1）提出深度感知蒸馏（Depth-Aware Distillation），通过转移深度相关知识提升前景表达能力。
2）将合成与真实数据域的差异建模为领域自适应问题，引入领域不变学习以聚焦前景学习。
3）设计面向对象的解码器，可接收视觉和语言提示，实现交互式目标预测。

Result: 在公开的数据集上，通过定量和定性实验，所提方法在细节保留和整体指标上均优于当前最优方法（SOTA）。

Conclusion: FCLM模型不仅在细粒度和多类别提取上提升了性能，还能更好适应真实场景，并具备良好的交互与泛化能力，对高精度分割任务具有实际应用价值。

Abstract: High-precision scene parsing tasks, including image matting and dichotomous segmentation, aim to accurately predict masks with extremely fine details (such as hair). Most existing methods focus on salient, single foreground objects. While interactive methods allow for target adjustment, their class-agnostic design restricts generalization across different categories. Furthermore, the scarcity of high-quality annotation has led to a reliance on inharmonious synthetic data, resulting in poor generalization to real-world scenarios. To this end, we propose a Foreground Consistent Learning model, dubbed as FCLM, to address the aforementioned issues. Specifically, we first introduce a Depth-Aware Distillation strategy where we transfer the depth-related knowledge for better foreground representation. Considering the data dilemma, we term the processing of synthetic data as domain adaptation problem where we propose a domain-invariant learning strategy to focus on foreground learning. To support interactive prediction, we contribute an Object-Oriented Decoder that can receive both visual and language prompts to predict the referring target. Experimental results show that our method quantitatively and qualitatively outperforms SOTA methods.

</details>


### [62] [Conditional Random Fields for Interactive Refinement of Histopathological Predictions](https://arxiv.org/abs/2601.12082)
*Tiffanie Godelaine,Maxime Zanella,Karim El Khoury,Saïd Mahmoudi,Benoît Macq,Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: 本文提出了一种基于条件随机场（CRF）的HistoCRF框架，以提升病理图像分析中的视觉-语言模型（VLM）零样本预测的准确性，且无需额外模型训练。


<details>
  <summary>Details</summary>
Motivation: 病理图像分析对癌症检测和分期有重要临床价值。现有VLM能实现零样本预测，但结果并不完美，亟需后处理方法提升其性能，特别是在没有或有限注释的情况下。

Method: HistoCRF利用CRF框架，配合新颖的成对势函数，鼓励标签多样化并可结合专家注释。该方法无需额外模型训练。论文设计了无注释、专家注释、与人机互动三种实验，逐步校正误判。

Result: 在五个涉及不同器官和疾病的病理补丁分类数据集上，HistoCRF在无注释时提升准确率16%，有100条专家注释时提升27.5%，结合人机互动可提升32.6%。

Conclusion: HistoCRF能有效提升VLM在病理图像分析中的零样本性能，结合专家和人机互动注释时提升更明显，为临床应用拓展了可行性和效率。

Abstract: Assisting pathologists in the analysis of histopathological images has high clinical value, as it supports cancer detection and staging. In this context, histology foundation models have recently emerged. Among them, Vision-Language Models (VLMs) provide strong yet imperfect zero-shot predictions. We propose to refine these predictions by adapting Conditional Random Fields (CRFs) to histopathological applications, requiring no additional model training. We present HistoCRF, a CRF-based framework, with a novel definition of the pairwise potential that promotes label diversity and leverages expert annotations. We consider three experiments: without annotations, with expert annotations, and with iterative human-in-the-loop annotations that progressively correct misclassified patches. Experiments on five patch-level classification datasets covering different organs and diseases demonstrate average accuracy gains of 16.0% without annotations and 27.5% with only 100 annotations, compared to zero-shot predictions. Moreover, integrating a human in the loop reaches a further gain of 32.6% with the same number of annotations. The code will be made available on https://github.com/tgodelaine/HistoCRF.

</details>


### [63] [Detecting 3D Line Segments for 6DoF Pose Estimation with Limited Data](https://arxiv.org/abs/2601.12090)
*Matej Mok,Lukáš Gajdošech,Michal Mesároš,Martin Madaras,Viktor Kocur*

Main category: cs.CV

TL;DR: 本文提出了一种针对工业场景中箱体（bins）6DoF位姿估计的新方法，不依赖于特定数据或CAD模型，并在公开新数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有6DoF位姿估计方法依赖大量训练数据或CAD模型，实际工业环境中物体种类多、数据稀缺，限制了这些方法的应用。

Method: 针对工业箱体的立方体结构，先检测箱体顶部的3D线段（边），通过改进2D线段检测网络LeTR，使其适用于结构化点云数据；再利用几何方法从3D线段估算出箱体的6DoF位姿。

Result: 方法在扩展后的真实及合成数据集上测试，采用合成数据训练能显著提升真实扫描数据的准确性。在3cm平移误差和8.2度旋转误差指标上，方法明显优于现有主流方法，且无需CAD模型。

Conclusion: 提出的方法充分利用物体结构信息，能够在数据有限且无需CAD模型的情况下，实现高精度的物体6DoF位姿估计，为工业机器人等场景提供了有力工具。

Abstract: The task of 6DoF object pose estimation is one of the fundamental problems of 3D vision with many practical applications such as industrial automation. Traditional deep learning approaches for this task often require extensive training data or CAD models, limiting their application in real-world industrial settings where data is scarce and object instances vary. We propose a novel method for 6DoF pose estimation focused specifically on bins used in industrial settings. We exploit the cuboid geometry of bins by first detecting intermediate 3D line segments corresponding to their top edges. Our approach extends the 2D line segment detection network LeTR to operate on structured point cloud data. The detected 3D line segments are then processed using a simple geometric procedure to robustly determine the bin's 6DoF pose. To evaluate our method, we extend an existing dataset with a newly collected and annotated dataset, which we make publicly available. We show that incorporating synthetic training data significantly improves pose estimation accuracy on real scans. Moreover, we show that our method significantly outperforms current state-of-the-art 6DoF pose estimation methods in terms of the pose accuracy (3 cm translation error, 8.2$^\circ$ rotation error) while not requiring instance-specific CAD models during inference.

</details>


### [64] [Energy-Aware Ensemble Learning for Coffee Leaf Disease Classification](https://arxiv.org/abs/2601.12109)
*Larissa Ferreira Rodrigues Moreira,Rodrigo Moreira,Leonardo Gabriel Ferreira Rodrigues*

Main category: cs.CV

TL;DR: 通过知识蒸馏和集成学习，将高性能的卷积神经网络（CNN）能力传递给小型模型，实现咖啡叶病害诊断的可持续边缘部署，在保证高准确率的同时显著降低能耗。


<details>
  <summary>Details</summary>
Motivation: 咖啡叶病害的田间诊断困难，而传统的AI视觉模型受限于设备算力和网络连接，难以部署，亟需兼具高效能与低能耗的解决方案支持实际应用。

Method: 采用知识蒸馏技术，将在数据中心训练的大型CNN模型知识迁移到小型CNN模型上，并通过优化后的集成学习，将多个小型模型集成为高效、紧凑的‘tiny小组’，以提升模型的诊断精度和适应IoT设备的计算与能耗约束。

Result: 在精心构建的咖啡叶数据集上，经过蒸馏和集成的小型模型（tiny ensembles）在能耗和碳足迹显著降低的前提下，取得了与以往研究相当的诊断准确率。

Conclusion: 恰当知识蒸馏和集成的小型模型能够在IoT等边缘场景下，实现实用且可持续的咖啡叶病害自动诊断，推动AI模型向低能耗、高可靠性方向发展。

Abstract: Coffee yields are contingent on the timely and accurate diagnosis of diseases; however, assessing leaf diseases in the field presents significant challenges. Although Artificial Intelligence (AI) vision models achieve high accuracy, their adoption is hindered by the limitations of constrained devices and intermittent connectivity. This study aims to facilitate sustainable on-device diagnosis through knowledge distillation: high-capacity Convolutional Neural Networks (CNNs) trained in data centers transfer knowledge to compact CNNs through Ensemble Learning (EL). Furthermore, dense tiny pairs were integrated through simple and optimized ensembling to enhance accuracy while adhering to strict computational and energy constraints. On a curated coffee leaf dataset, distilled tiny ensembles achieved competitive with prior work with significantly reduced energy consumption and carbon footprint. This indicates that lightweight models, when properly distilled and ensembled, can provide practical diagnostic solutions for Internet of Things (IoT) applications.

</details>


### [65] [From Prompts to Pavement: LMMs-based Agentic Behavior-Tree Generation Framework for Autonomous Vehicles](https://arxiv.org/abs/2601.12358)
*Omar Y. Goba,Ahmed Y. Gado,Catherine M. Elias,Ahmed Hussein*

Main category: cs.CV

TL;DR: 本文提出了一种结合大语言模型（LLM）和多模态视觉模型（LVM）的自适应行为树（BT）生成框架，使自动驾驶汽车能在遇到突发状况时即时调整决策逻辑，无需人工干预，在模拟环境中表现优于传统静态BT。


<details>
  <summary>Details</summary>
Motivation: 现有静态行为树虽然结构化但灵活性差且需大量手动调试，难以适应真实世界的复杂情景，尤其是面向SAE 5级完全自动驾驶，对决策系统自适应性提出更高要求。

Method: 构建了一个“代理式”框架，利用多种智能体：Descriptor代理通过符号链提示评估场景关键性，Planner代理基于上下文学习生成高层次子目标，Generator代理将子目标生成可执行的BT子树（XML），并在CARLA+Nav2仿真环境中验证。当基础BT失效时系统自动激活进行动态规划。

Result: 实验显示，在意外障碍（如道路封闭）情况下，系统实现了无需人工干预的绕行导航，相较静态BT基线更具鲁棒性，提升了适应性和安全性，能拓展到多种驾驶场景。

Conclusion: 本文证明利用LLM和LVM动态生成和调整行为树是可行的，克服了传统BT依赖人工和静态性的局限，为高级别自动驾驶的通用适应性行为规划提供了新方向。

Abstract: Autonomous vehicles (AVs) require adaptive behavior planners to navigate unpredictable, real-world environments safely. Traditional behavior trees (BTs) offer structured decision logic but are inherently static and demand labor-intensive manual tuning, limiting their applicability at SAE Level 5 autonomy. This paper presents an agentic framework that leverages large language models (LLMs) and multi-modal vision models (LVMs) to generate and adapt BTs on the fly. A specialized Descriptor agent applies chain-of-symbols prompting to assess scene criticality, a Planner agent constructs high-level sub-goals via in-context learning, and a Generator agent synthesizes executable BT sub-trees in XML format. Integrated into a CARLA+Nav2 simulation, our system triggers only upon baseline BT failure, demonstrating successful navigation around unexpected obstacles (e.g., street blockage) with no human intervention. Compared to a static BT baseline, this approach is a proof-of-concept that extends to diverse driving scenarios.

</details>


### [66] [RCDN: Real-Centered Detection Network for Robust Face Forgery Identification](https://arxiv.org/abs/2601.12111)
*Wyatt McCurdy,Xin Zhang,Yuqi Song,Min Gao*

Main category: cs.CV

TL;DR: 该论文提出了一种名为RCDN（真实中心检测网络）的深度学习模型，专门用于检测不断变化的AI造假人脸图像，并有效提升了跨领域的检测能力。实验显示，该方法在保持高精度的同时，显著缩小了不同领域之间的性能差距。


<details>
  <summary>Details</summary>
Motivation: 传统的人脸图像伪造检测方法在训练和测试数据属于同一领域时表现优秀，但面对新型且未知的造假手法时，性能会大幅下降。随着造假技术不断更新，这一缺陷严重影响了检测系统的实用性和可靠性。因此，亟需提出一种对未知和跨领域伪造具有强泛化能力的方法。

Method: 作者提出了一种基于Xception骨干结构的频率-空间卷积神经网络架构RCDN。该方法采用了双分支结构，并设计了以真实人脸为中心的损失函数（real centered loss），通过突出真实图像在表示空间中的一致性来增强模型的泛化能力，而不是单纯拟合伪造图像的多变特征。

Result: 在DiFF数据集上，RCDN针对三种典型的伪造类型（FE、I2I、T2I）进行了大量实验。结果显示，该方法不仅在同领域任务中达到最优，还在跨领域任务中实现了明显优于主流方法的鲁棒性和更高的泛化能力，显著缩小了领域间性能差距。

Conclusion: RCDN方法通过聚焦真实图像的一致性，有效提升了人脸伪造检测的跨领域泛化能力，表现出优越的跨/同域稳定性。该方法为应对不断演进的造假技术提供了切实可行的防护方案，具有重要的应用前景。

Abstract: Image forgery has become a critical threat with the rapid proliferation of AI-based generation tools, which make it increasingly easy to synthesize realistic but fraudulent facial content. Existing detection methods achieve near-perfect performance when training and testing are conducted within the same domain, yet their effectiveness deteriorates substantially in crossdomain scenarios. This limitation is problematic, as new forgery techniques continuously emerge and detectors must remain reliable against unseen manipulations. To address this challenge, we propose the Real-Centered Detection Network (RCDN), a frequency spatial convolutional neural networks(CNN) framework with an Xception backbone that anchors its representation space around authentic facial images. Instead of modeling the diverse and evolving patterns of forgeries, RCDN emphasizes the consistency of real images, leveraging a dual-branch architecture and a real centered loss design to enhance robustness under distribution shifts. Extensive experiments on the DiFF dataset, focusing on three representative forgery types (FE, I2I, T2I), demonstrate that RCDN achieves both state-of-the-art in-domain accuracy and significantly stronger cross-domain generalization. Notably, RCDN reduces the generalization gap compared to leading baselines and achieves the highest cross/in-domain stability ratio, highlighting its potential as a practical solution for defending against evolving and unseen image forgery techniques.

</details>


### [67] [CD-TWINSAFE: A ROS-enabled Digital Twin for Scene Understanding and Safety Emerging V2I Technology](https://arxiv.org/abs/2601.12373)
*Amro Khaled,Farah Khaled,Omar Riad,Catherine M. Elias*

Main category: cs.CV

TL;DR: 本论文提出了一种基于V2I的数字孪生架构CD-TWINSAFE，用于自动驾驶车辆的安全监控与警报。通过车载驾驶栈与虚拟孪生栈的协同，实现车辆实时场景仿真与安全提示。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在实际路况中的安全评估与实时响应存在挑战。采用数字孪生技术和V2I通信，有望提升车辆在复杂环境下的信息处理和安全响应能力。

Method: 提出了一套双栈系统：车载驾驶栈利用双目摄像头进行定位和感知，通过ROS2和4G网络将处理数据发送至基础设施端的数字孪生栈。数字孪生栈基于Unreal Engine 5 实时仿真真实场景，并将安全警报反馈至驾驶舱。

Result: 系统可实时处理和传输车载传感数据，并在虚拟环境中准确还原现场及进行安全监控。多种驾驶场景测试验证了其有效性和实时性。

Conclusion: CD-TWINSAFE架构证明了V2I数字孪生在自动驾驶安全监控中的可行性和高效性，为自动驾驶车辆的安全增强提供了新方案。

Abstract: In this paper, the CD-TWINSAFE is introduced, a V2I-based digital twin for Autonomous Vehicles. The proposed architecture is composed of two stacks running simultaneously, an on-board driving stack that includes a stereo camera for scene understanding, and a digital twin stack that runs an Unreal Engine 5 replica of the scene viewed by the camera as well as returning safety alerts to the cockpit. The on-board stack is implemented on the vehicle side including 2 main autonomous modules; localization and perception. The position and orientation of the ego vehicle are obtained using on-board sensors. Furthermore, the perception module is responsible for processing 20-fps images from stereo camera and understands the scene through two complementary pipelines. The pipeline are working on object detection and feature extraction including object velocity, yaw and the safety metrics time-to-collision and time-headway. The collected data form the driving stack are sent to the infrastructure side through the ROS-enabled architecture in the form of custom ROS2 messages and sent over UDP links that ride a 4G modem for V2I communication. The environment is monitored via the digital twin through the shared messages which update the information of the spawned ego vehicle and detected objects based on the real-time localization and perception data. Several tests with different driving scenarios to confirm the validity and real-time response of the proposed architecture.

</details>


### [68] [CARLA-Round: A Multi-Factor Simulation Dataset for Roundabout Trajectory Prediction](https://arxiv.org/abs/2601.12119)
*Xiaotong Zhou,Zhenhui Yuan,Yi Han,Tianhua Xu,Laurence T. Yang*

Main category: cs.CV

TL;DR: 该论文提出了CARLA-Round仿真数据集，用于缓解现实环形交叉口轨迹预测数据稀缺的问题，系统评估了不同天气与交通密度对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 环形交叉口因其结构复杂、交通行为多样等原因，对车辆轨迹预测提出了更高要求，而现实数据稀缺及因素交互难解的问题，限制了算法的发展与验证。

Method: 基于CARLA仿真平台，系统化设计并采集了涵盖5种天气、5级交通密度（总共25种场景）、包含不同驾驶行为和详细标注的数据集。通过与标准预测模型（LSTM、GCN、GRU+GCN）结合，分析不同环境因素对预测困难度的影响。

Result: 实验发现，交通密度是决定预测难度的主导因素（呈单调影响），天气影响则更为非线性。最佳模型在现实数据集（rounD）上的ADE为0.312m，验证了仿真数据对真实场景的迁移有效性。

Conclusion: 系统化构建的CARLA-Round数据集可有效分析并量化不同因素对轨迹预测的影响，为相关研究提供了新的资源和指标。

Abstract: Accurate trajectory prediction of vehicles at roundabouts is critical for reducing traffic accidents, yet it remains highly challenging due to their circular road geometry, continuous merging and yielding interactions, and absence of traffic signals. Developing accurate prediction algorithms relies on reliable, multimodal, and realistic datasets; however, such datasets for roundabout scenarios are scarce, as real-world data collection is often limited by incomplete observations and entangled factors that are difficult to isolate. We present CARLA-Round, a systematically designed simulation dataset for roundabout trajectory prediction. The dataset varies weather conditions (five types) and traffic density levels (spanning Level-of-Service A-E) in a structured manner, resulting in 25 controlled scenarios. Each scenario incorporates realistic mixtures of driving behaviors and provides explicit annotations that are largely absent from existing datasets. Unlike randomly sampled simulation data, this structured design enables precise analysis of how different conditions influence trajectory prediction performance. Validation experiments using standard baselines (LSTM, GCN, GRU+GCN) reveal traffic density dominates prediction difficulty with strong monotonic effects, while weather shows non-linear impacts. The best model achieves 0.312m ADE on real-world rounD dataset, demonstrating effective sim-to-real transfer. This systematic approach quantifies factor impacts impossible to isolate in confounded real-world datasets. Our CARLA-Round dataset is available at https://github.com/Rebecca689/CARLA-Round.

</details>


### [69] [DC-VLAQ: Query-Residual Aggregation for Robust Visual Place Recognition](https://arxiv.org/abs/2601.12729)
*Hanyu Zhu,Zhihao Zhan,Yuhang Ming,Liang Li,Dibo Hou,Javier Civera,Wanzeng Kong*

Main category: cs.CV

TL;DR: 本文提出了一种针对视觉场所识别(VPR)任务的新框架DC-VLAQ，通过融合不同视觉基础模型(VFMs)的优势以及改进的全局特征聚合方式，有效提升了模型在大尺度视角、照明变化及域转移等挑战下的识别鲁棒性，并在多个公开基准上实现了先进表现。


<details>
  <summary>Details</summary>
Motivation: 视觉场所识别在实际应用中受视角、光照和环境变化影响明显。主流方法通常仅用单一视觉基础模型，未能充分利用不同模型间的互补特性，且在融合时会带来特征分布改变，导致聚合过程不稳定。因此，亟需一种能够稳定融合多模型互补信息并保持判别性的全局表示方法。

Method: 作者提出DC-VLAQ——一套关注特征表达的框架。首先，提出残差引导的互补融合方式，以DINOv2为主空间，并用CLIP特征通过可学习残差矫正引入互补语义。其次，提出VLAQ（Vector of Local Aggregated Queries）全局聚合策略，通过查询-残差机制编码局部特征，从而获得更稳定、细粒度的全局表征。

Result: 在Pitts30k、Tokyo24/7、MSLS、Nordland、SPED和AmsterTime等主流VPR数据集上，DC-VLAQ均显著超越当前强基线方法，特别是在复杂场景如域转移和长期外观变化条件下表现突出，取得SOTA性能。

Conclusion: DC-VLAQ框架通过有效融合多种基础视觉模型特征和引入稳健的全局特征聚合机制，大幅提升了视觉场所识别在多样场景和长期变化下的鲁棒性和实用性。

Abstract: One of the central challenges in visual place recognition (VPR) is learning a robust global representation that remains discriminative under large viewpoint changes, illumination variations, and severe domain shifts. While visual foundation models (VFMs) provide strong local features, most existing methods rely on a single model, overlooking the complementary cues offered by different VFMs. However, exploiting such complementary information inevitably alters token distributions, which challenges the stability of existing query-based global aggregation schemes. To address these challenges, we propose DC-VLAQ, a representation-centric framework that integrates the fusion of complementary VFMs and robust global aggregation. Specifically, we first introduce a lightweight residual-guided complementary fusion that anchors representations in the DINOv2 feature space while injecting complementary semantics from CLIP through a learned residual correction. In addition, we propose the Vector of Local Aggregated Queries (VLAQ), a query--residual global aggregation scheme that encodes local tokens by their residual responses to learnable queries, resulting in improved stability and the preservation of fine-grained discriminative cues. Extensive experiments on standard VPR benchmarks, including Pitts30k, Tokyo24/7, MSLS, Nordland, SPED, and AmsterTime, demonstrate that DC-VLAQ consistently outperforms strong baselines and achieves state-of-the-art performance, particularly under challenging domain shifts and long-term appearance changes.

</details>


### [70] [Segment and Matte Anything in a Unified Model](https://arxiv.org/abs/2601.12147)
*Zezhong Fan,Xiaohan Li,Topojoy Biswas,Kaushiki Nag,Kannan Achan*

Main category: cs.CV

TL;DR: 本文提出了一种名为SAMA的新方法，在SAM基础上，通过引入新模块同时提升分割和抠图（matting）质量，达到了多项基准的SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有SAM虽然支持零样本泛化和交互，但其分割精度难以满足实际场景需求，且不支持细粒度抠图（matting）。分割与抠图高度相关，但目前缺乏统一解决二者的高效模型。

Method: 作者提出SAMA模型，通过引入Multi-View Localization Encoder (MVLE)提取局部细节特征，Localization Adapter进一步优化掩码边界细节。模型结构加入两组预测头，可以同时输出分割和matting结果。采用多来源公开数据进行训练。

Result: SAMA在多种分割和matting任务及数据集上都取得了当前最优结果（SOTA），表现出良好的适应性和高精度。

Conclusion: SAMA是对SAM的高效扩展，可灵活而精细地实现分割与抠图，适用于多种下游视觉任务。

Abstract: Segment Anything (SAM) has recently pushed the boundaries of segmentation by demonstrating zero-shot generalization and flexible prompting after training on over one billion masks. Despite this, its mask prediction accuracy often falls short of the precision required in real-world applications. While several refinement modules have been proposed to boost SAM's segmentation quality, achieving highly accurate object delineation within a single, unified framework remains an open challenge. Furthermore, interactive image matting, which aims to generate fine-grained alpha mattes guided by diverse user hints, has not yet been explored in the context of SAM. Insights from recent studies highlight strong correlations between segmentation and matting, suggesting the feasibility of a unified model capable of both tasks. In this paper, we introduce Segment And Matte Anything (SAMA), a lightweight extension of SAM that delivers high-quality interactive image segmentation and matting with minimal extra parameters. Our Multi-View Localization Encoder (MVLE) captures detailed features from local views, while the Localization Adapter (Local-Adapter) refines mask outputs by recovering subtle boundary details. We also incorporate two prediction heads for each task into the architecture to generate segmentation and matting masks, simultaneously. Trained on a diverse dataset aggregated from publicly available sources, SAMA achieves state-of-the-art performance across multiple segmentation and matting benchmarks, showcasing its adaptability and effectiveness in a wide range of downstream tasks.

</details>


### [71] [Learning Fine-Grained Correspondence with Cross-Perspective Perception for Open-Vocabulary 6D Object Pose Estimation](https://arxiv.org/abs/2601.13565)
*Yu Qin,Shimeng Fan,Fan Yang,Zixuan Xue,Zijie Mai,Wenrui Chen,Kailun Yang,Zhiyong Li*

Main category: cs.CV

TL;DR: 该论文提出了FiCoP方法，通过精细化的空间约束显著提升了开集词汇6D物体姿态估计的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇6D物体姿态估计算法多依赖全局匹配策略，在复杂环境下容易将物体特征与背景噪声混淆，导致估计结果鲁棒性不足。

Method: FiCoP方法分为三个关键步骤：1）物体中心的特征解耦预处理用于去除背景干扰；2）跨视角全局感知模块（CPGP）融合多视角特征，增强结构上下文理解；3）块相关性预测器（PCP）生成精细的空间相关性图，实现噪声抑制和精准特征匹配。

Result: 在REAL275和Toyota-Light公开数据集上，FiCoP分别比SOTA提升了8.0%和6.1%的平均召回率，实验结果证明了其在复杂环境下的性能优势。

Conclusion: FiCoP框架以空间约束提升了物体姿态估计的准确性，对机器人在开放环境下的泛化与鲁棒感知具有重要意义。

Abstract: Open-vocabulary 6D object pose estimation empowers robots to manipulate arbitrary unseen objects guided solely by natural language. However, a critical limitation of existing approaches is their reliance on unconstrained global matching strategies. In open-world scenarios, trying to match anchor features against the entire query image space introduces excessive ambiguity, as target features are easily confused with background distractors. To resolve this, we propose Fine-grained Correspondence Pose Estimation (FiCoP), a framework that transitions from noise-prone global matching to spatially-constrained patch-level correspondence. Our core innovation lies in leveraging a patch-to-patch correlation matrix as a structural prior to narrowing the matching scope, effectively filtering out irrelevant clutter to prevent it from degrading pose estimation. Firstly, we introduce an object-centric disentanglement preprocessing to isolate the semantic target from environmental noise. Secondly, a Cross-Perspective Global Perception (CPGP) module is proposed to fuse dual-view features, establishing structural consensus through explicit context reasoning. Finally, we design a Patch Correlation Predictor (PCP) that generates a precise block-wise association map, acting as a spatial filter to enforce fine-grained, noise-resilient matching. Experiments on the REAL275 and Toyota-Light datasets demonstrate that FiCoP improves Average Recall by 8.0% and 6.1%, respectively, compared to the state-of-the-art method, highlighting its capability to deliver robust and generalized perception for robotic agents operating in complex, unconstrained open-world environments. The source code will be made publicly available at https://github.com/zjjqinyu/FiCoP.

</details>


### [72] [Principal Component Analysis-Based Terahertz Self-Supervised Denoising and Deblurring Deep Neural Networks](https://arxiv.org/abs/2601.12149)
*Pengfei Zhu,Xavier Maldague*

Main category: cs.CV

TL;DR: 本文提出了一种基于PCA的太赫兹（THz）自监督去噪与去模糊网络（THz-SSDD），能够在无需标签的情况下，有效同时消除图像的低频模糊和高频噪声。


<details>
  <summary>Details</summary>
Motivation: 太赫兹系统获取的幅值图像常常受到低频模糊和高频噪声的影响，而传统图像处理方法难以同时应对此两类问题，且去噪和去模糊的界限不明确，通常需要人工干预。

Method: 本文提出了THz-SSDD网络，采用Recorrupted-to-Recorrupted的自监督学习策略，通过PCA分解和重建实现对低频和高频成分的联合恢复；只需少量未标注的噪声图像进行训练。

Result: 在四类不同样本上评估了该方法，在不同材料属性和测量方式下均有效实现了去噪与去模糊，图像质量获得改善，且物理特征得以保留。

Conclusion: 该网络能有效提升太赫兹图像质量，具有不依赖大量标注数据及兼容多样样品和测量模式的优点，在太赫兹成像领域应用前景广阔。

Abstract: Terahertz (THz) systems inherently introduce frequency-dependent degradation effects, resulting in low-frequency blurring and high-frequency noise in amplitude images. Conventional image processing techniques cannot simultaneously address both issues, and manual intervention is often required due to the unknown boundary between denoising and deblurring. To tackle this challenge, we propose a principal component analysis (PCA)-based THz self-supervised denoising and deblurring network (THz-SSDD). The network employs a Recorrupted-to-Recorrupted self-supervised learning strategy to capture the intrinsic features of noise by exploiting invariance under repeated corruption. PCA decomposition and reconstruction are then applied to restore images across both low and high frequencies. The performance of the THz-SSDD network was evaluated on four types of samples. Training requires only a small set of unlabeled noisy images, and testing across samples with different material properties and measurement modes demonstrates effective denoising and deblurring. Quantitative analysis further validates the network feasibility, showing improvements in image quality while preserving the physical characteristics of the original signals.

</details>


### [73] [FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation](https://arxiv.org/abs/2601.13976)
*Jing Zuo,Lingzhou Mu,Fan Jiang,Chengcheng Ma,Mu Xu,Yonggang Qi*

Main category: cs.CV

TL;DR: 本文提出了一种名为FantasyVLN的新方法，通过将想象视觉信息压缩到潜在空间中，实现了结合多模态推理且高效的视觉-语言导航系统，显著提升了导航成功率和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言导航（VLN）方法在利用链式思维（CoT）增强推理能力方面取得进展，但纯文本方式缺乏空间信息，且多模态方式因令牌膨胀导致推理缓慢，不适用于实时导航。该文旨在开发一种既能结合CoT推理优势，又避免额外推理负担的高效方法。

Method: 作者提出FantasyVLN框架，在训练时利用预训练的视觉自回归模型（VAR），将想象视觉信息编码到紧凑的潜在空间，并采用统一的多CoT训练策略，融合文本、视觉及多模态推理；推理时则直接从指令推断动作，实现高效导航。

Result: 在LH-VLN数据集上的实验证明，该方法能提升导航成功率和效率，并比显式的CoT推理方法大幅降低推理延迟（提高实时性）。

Conclusion: FantasyVLN统一隐式推理框架能在兼顾推理能力与推理速度下有效提升视觉-语言导航性能，为实现类人导航提供新途径。

Abstract: Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods.

</details>


### [74] [Enhanced Diagnostic Performance via Large-Resolution Inference Optimization for Pathology Foundation Models](https://arxiv.org/abs/2601.12150)
*Mengxuan Hu,Zihan Guan,John Kang,Sheng Li,Zhongliang Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种高效的病理全视野切片(WSI)推理策略，通过稀疏化空间注意力和全局注意力筛选非信息性token，降低了GPU内存和推理时间需求，同时提升或保持下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有病理基础模型通常限制输入尺寸（如224×224），在处理高分辨率WSI时极为低效。直接扩大输入将导致GPU内存消耗极大，而下采样又会损失关键形态学细节。亟需一种有效方法在有限硬件下高效利用WSI数据。

Method: 本文提出结合空间相关性块稀疏注意力、使用全局注意力分数过滤token的推理框架，以减少在高分辨率WSI推理时的无效计算和内存占用。

Result: 实验表明，该方法在ROI分类任务上提升最多可达7.67%，在分割任务上也获得了相当的效果，同时大幅降低了GPU显存和推理时间。

Conclusion: 所提出方法可在相同GPU预算下扩展到更高分辨率，兼顾计算效率与病理任务精度，为高分辨率WSI智能分析提供了实用方案。

Abstract: Despite their prominent performance on tasks such as ROI classification and segmentation, many pathology foundation models remain constrained by a specific input size e.g. 224 x 224, creating substantial inefficiencies when applied to whole-slide images (WSIs), which span thousands of resolutions. A naive strategy is to either enlarge inputs or downsample the WSIs. However, enlarging inputs results in prohibitive GPU memory consumption, while downsampling alters the microns-per-pixel resolution and obscures critical morphological details. To overcome these limitations, we propose an space- and time- efficient inference strategy that sparsifies attention using spatially aware neighboring blocks and filters out non-informative tokens through global attention scores. This design substantially reduces GPU memory and runtime during high-resolution WSI inference while preserving and even improving the downstream performance, enabling inference at higher resolutions under the same GPU budget. The experimental results show that our method can achieves up to an 7.67% improvement in the ROI classification and compatible results in segmentation.

</details>


### [75] [Inverse Rendering for High-Genus 3D Surface Meshes from Multi-view Images with Persistent Homology Priors](https://arxiv.org/abs/2601.12155)
*Xiang Gao,Xinmu Wang,Yuanpeng Liu,Yue Wang,Junqi Huang,Wei Chen,Xianfeng Gu*

Main category: cs.CV

TL;DR: 本文提出了一种结合持久同调先验的协作逆向渲染方法，用于改进多图像的3D重建，增强复杂（高亏格）结构的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多图片构建3D对象时，几何、外观及拓扑存在不确定性，尤其是重建含有隧道和把手的高亏格曲面时易出错，常见问题如通道塌陷或结构丢失。解决这些拓扑性歧义，是提升3D重建质量的关键。

Method: 方法通过将持久同调描述的拓扑先验引入基于多视图一致性的反向渲染优化流程中，不依赖神经网络，而是采用基于梯度的网格优化方法，对高亏格结构（如环、隧道）进行直接约束，实现协同重建。

Result: 在实验中，与最新的网格类3D重建方法相比，引入拓扑先验后，Chamfer Distance更低，Volume IoU更高，证明几何准确性和拓扑鲁棒性均大幅提升。

Conclusion: 研究显示，结合同调先验能够有效化解高亏格3D物体重建中的拓扑难题，是高精度、高鲁棒3D重建的新方向。

Abstract: Reconstructing 3D objects from images is inherently an ill-posed problem due to ambiguities in geometry, appearance, and topology. This paper introduces collaborative inverse rendering with persistent homology priors, a novel strategy that leverages topological constraints to resolve these ambiguities. By incorporating priors that capture critical features such as tunnel loops and handle loops, our approach directly addresses the difficulty of reconstructing high-genus surfaces. The collaboration between photometric consistency from multi-view images and homology-based guidance enables recovery of complex high-genus geometry while circumventing catastrophic failures such as collapsing tunnels or losing high-genus structure. Instead of neural networks, our method relies on gradient-based optimization within a mesh-based inverse rendering framework to highlight the role of topological priors. Experimental results show that incorporating persistent homology priors leads to lower Chamfer Distance (CD) and higher Volume IoU compared to state-of-the-art mesh-based methods, demonstrating improved geometric accuracy and robustness against topological failure.

</details>


### [76] [VIRTUE: Versatile Video Retrieval Through Unified Embeddings](https://arxiv.org/abs/2601.12193)
*Shaunak Halbe,Bhagyashree Puranik,Jayakrishnan Unnikrishnan,Kushan Thakkar,Vimal Bhat,Toufiq Parag*

Main category: cs.CV

TL;DR: 本文提出了一种名为VIRTUE的基于多模态大语言模型（MLLM）的通用视频检索框架，能够高效处理不同粒度任务及复杂多模态查询，检索性能显著提升，接近或达到专用模型水平。


<details>
  <summary>Details</summary>
Motivation: 现有视频检索方法在检索性能和多模态查询处理能力之间存在权衡，专用模型检索精度高但难以处理复杂多模态查询，MLLM方法则支持灵活查询但性能不足，因此亟需一种能够兼具高性能与通用性的检索框架。

Method: 提出VIRTUE框架，采用共享MLLM骨干网络，通过视觉和文本特征的对比对齐进行高效的嵌入式候选检索。模型使用LoRA高效训练，采用70万对视觉-文本数据进行微调，支持语料和片段级检索及复杂多模态组合查询。

Result: VIRTUE在零样本视频检索任务中超越其他MLLM方法；相同模型无需额外训练即可在零样本片段检索和多模态组合检索上获得有竞争力的结果；进一步进行候选重排序训练后，VIRTUE的综合检索性能大幅领先现有MLLM方法，并接近或达到专用模型水平。

Conclusion: VIRTUE实现了多粒度和多模态查询的高效统一检索，在主流零样本评测中取得优异表现，兼具通用性和高性能，为视频检索系统带来新范式。

Abstract: Modern video retrieval systems are expected to handle diverse tasks ranging from corpus-level retrieval and fine-grained moment localization to flexible multimodal querying. Specialized architectures achieve strong retrieval performance by training modality-specific encoders on massive datasets, but they lack the ability to process composed multimodal queries. In contrast, multimodal LLM (MLLM)-based methods support rich multimodal search but their retrieval performance remains well below that of specialized systems. We present VIRTUE, an MLLM-based versatile video retrieval framework that integrates corpus and moment-level retrieval capabilities while accommodating composed multimodal queries within a single architecture. We use contrastive alignment of visual and textual embeddings generated using a shared MLLM backbone to facilitate efficient embedding-based candidate search. Our embedding model, trained efficiently using low-rank adaptation (LoRA) on 700K paired visual-text data samples, surpasses other MLLM-based methods on zero-shot video retrieval tasks. Additionally, we demonstrate that the same model can be adapted without further training to achieve competitive results on zero-shot moment retrieval, and state of the art results for zero-shot composed video retrieval. With additional training for reranking candidates identified in the embedding-based search, our model substantially outperforms existing MLLM-based retrieval systems and achieves retrieval performance comparable to state of the art specialized models which are trained on orders of magnitude larger data.

</details>


### [77] [Where It Moves, It Matters: Referring Surgical Instrument Segmentation via Motion](https://arxiv.org/abs/2601.12224)
*Meng Wei,Kun Yuan,Shi Li,Yue Zhou,Long Bai,Nassir Navab,Hongliang Ren,Hong Joo Lee,Tom Vercauteren,Nicolas Padoy*

Main category: cs.CV

TL;DR: 本文提出了一种基于运动引导的框架（SurgRef），实现利用自然语言描述在手术场景中对手术器械的定位和分割，在应对遮挡、模糊或术语不熟悉的情况下表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前基于自然语言的手术场景理解（尤其是指代分割，即用语言定位并分割手术器械）研究较少，且现有方法依赖静态视觉线索和预定义器械名称，泛化能力有限。

Method: 提出了SurgRef方法，通过将器械的运动信息与自然语言描述相结合，学习“工具如何移动和交互”而非单纯外观，从而增强对复杂场景的理解能力。并构建了Ref-IMotion多机构、多样化、带有丰富运动特征和密集时空标注的视频数据集以训练和评估模型。

Result: SurgRef在多种手术过程中的定位与分割任务上表现优异，达到了业界领先水平，泛化能力强。

Conclusion: SurgRef为基于语言的手术视频分割树立了新的标杆，有望推动智能手术室与自助手术机器人研究的发展。

Abstract: Enabling intuitive, language-driven interaction with surgical scenes is a critical step toward intelligent operating rooms and autonomous surgical robotic assistance. However, the task of referring segmentation, localizing surgical instruments based on natural language descriptions, remains underexplored in surgical videos, with existing approaches struggling to generalize due to reliance on static visual cues and predefined instrument names. In this work, we introduce SurgRef, a novel motion-guided framework that grounds free-form language expressions in instrument motion, capturing how tools move and interact across time, rather than what they look like. This allows models to understand and segment instruments even under occlusion, ambiguity, or unfamiliar terminology. To train and evaluate SurgRef, we present Ref-IMotion, a diverse, multi-institutional video dataset with dense spatiotemporal masks and rich motion-centric expressions. SurgRef achieves state-of-the-art accuracy and generalization across surgical procedures, setting a new benchmark for robust, language-driven surgical video segmentation.

</details>


### [78] [DiffusionQC: Artifact Detection in Histopathology via Diffusion Model](https://arxiv.org/abs/2601.12233)
*Zhenzhen Wang,Zhongliang Zhou,Zhuoyu Wen,Jeong Hwan Kook,John B Wojcik,John Kang*

Main category: cs.CV

TL;DR: 本论文提出DiffusionQC方法，利用扩散模型检测数字病理切片图像中的制备和数字化伪影，无需大量人工标注，将伪影检测作为异常值检测来实现。还引入对比学习模块显著提升伪影与正常图像的分布区分能力。实验效果优于现有方法，并具备较强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 数字病理在现代医学中非常重要，图像中常存在制备及数字化产生的伪影，影响后续分析。现有有监督方法依赖大量人工标注，且难以涵盖新型伪影，限制实际应用。本文希望开发一种无需大量标注、能泛化到不同伪影类型的自动检测方法。

Method: 提出DiffusionQC方法，采用扩散模型，只需干净图像集训练模型，无需像素级伪影标注或预定义伪影类型。检测时将伪影视为分布异常。进一步，通过对比学习模块加大伪影和正常图像分布的分离度，从而增强检测效果。

Result: DiffusionQC在多个数据集和不同染色方法下，相较主流方法表现出更优的伪影检测能力，仅需更少的数据和标注。方法具备跨染色泛化能力。

Conclusion: DiffusionQC能够高效、鲁棒地检测数字病理图像中的各类伪影，减少人工标注负担，提升下游分析的可靠性，并具备较强的泛化性，适用于实际病理辅助诊断。

Abstract: Digital pathology plays a vital role across modern medicine, offering critical insights for disease diagnosis, prognosis, and treatment. However, histopathology images often contain artifacts introduced during slide preparation and digitization. Detecting and excluding them is essential to ensure reliable downstream analysis. Traditional supervised models typically require large annotated datasets, which is resource-intensive and not generalizable to novel artifact types. To address this, we propose DiffusionQC, which detects artifacts as outliers among clean images using a diffusion model. It requires only a set of clean images for training rather than pixel-level artifact annotations and predefined artifact types. Furthermore, we introduce a contrastive learning module to explicitly enlarge the distribution separation between artifact and clean images, yielding an enhanced version of our method. Empirical results demonstrate superior performance to state-of-the-art and offer cross-stain generalization capacity, with significantly less data and annotations.

</details>


### [79] [Less is More: Label-Guided Summarization of Procedural and Instructional Videos](https://arxiv.org/abs/2601.12243)
*Shreya Rajpal,Michal Golovanesky,Carsten Eickhoff*

Main category: cs.CV

TL;DR: 提出了一种三阶段的视频摘要生成框架PRISM，在保证视频主要语义信息的前提下，大幅压缩视频帧数，提升了摘要的准确性和上下文一致性，对专业和通用领域视频均表现优秀。


<details>
  <summary>Details</summary>
Motivation: 现有视频摘要方法难以兼顾视频内容的深层语义理解和上下文连贯性，特别是在外科训练等重要领域，对高质量、可理解的视频摘要需求迫切。

Method: PRISM包括三个阶段：自适应视觉采样、基于标签的关键帧锚定，以及用大型语言模型进行上下文验证。该方法结合视觉与多模态分析，有效筛选出既有意义又具代表性的帧，并剔除泛化或虚构内容，从而生成内容连贯、语义扎实的视频摘要。

Result: 在指令性和活动视频数据集上测试PRISM方法，尽管只采样不足5%的原始帧，但摘要保留了84%的语义内容，且在准确率和语义一致性上最多超越基线33%。

Conclusion: PRISM框架能够有效压缩视频帧数同时保持高水平的语义忠实性和摘要质量，在专业和通用场景下都具有良好的泛化能力和实用价值。

Abstract: Video summarization helps turn long videos into clear, concise representations that are easier to review, document, and analyze, especially in high-stakes domains like surgical training. Prior work has progressed from using basic visual features like color, motion, and structural changes to using pre-trained vision-language models that can better understand what's happening in the video (semantics) and capture temporal flow, resulting in more context-aware video summarization. We propose a three-stage framework, PRISM: Procedural Representation via Integrated Semantic and Multimodal analysis, that produces semantically grounded video summaries. PRISM combines adaptive visual sampling, label-driven keyframe anchoring, and contextual validation using a large language model (LLM). Our method ensures that selected frames reflect meaningful and procedural transitions while filtering out generic or hallucinated content, resulting in contextually coherent summaries across both domain-specific and instructional videos. We evaluate our method on instructional and activity datasets, using reference summaries for instructional videos. Despite sampling fewer than 5% of the original frames, our summaries retain 84% semantic content while improving over baselines by as much as 33%. Our approach generalizes across procedural and domain-specific video tasks, achieving strong performance with both semantic alignment and precision.

</details>


### [80] [An Innovative Framework for Breast Cancer Detection Using Pyramid Adaptive Atrous Convolution, Transformer Integration, and Multi-Scale Feature Fusion](https://arxiv.org/abs/2601.12249)
*Ehsan Sadeghi Pour,Mahdi Esmaeili,Morteza Romoozi*

Main category: cs.CV

TL;DR: 本文提出了一种基于PAAC和Transformer的新颖乳腺癌图像识别架构，通过多尺度特征融合和改进的损失函数实现对恶性肿块的高精度检测，显著优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是全球女性常见且危害极大的癌症，早期准确诊断对提高治愈率至关重要。当前方法在特征提取、多尺度信息融合与精度上存在局限，亟需更高效的自动化检测模型。

Method: 方法上，将Pyramid Adaptive Atrous Convolution（PAAC）与Transformer结构结合，通过多尺度特征融合提升良恶性组织区分能力，引入Dice Loss与Focal Loss优化二分类目标。在模型训练前，采用数据增强和对比度增强等手段处理INbreast、MIAS和DDSM数据集，所有图片统一为227x227像素。

Result: 所提模型在检出乳腺癌肿块时，取得98.5%的准确率、97.8%的敏感性、96.3%的特异性、98.2%的F1分数与97.9%的精确度，显著优于BreastNet、DeepMammo、Multi-Scale CNN、Swin-Unet与SegFormer等基线模型。

Conclusion: 新模型显著提升了乳腺肿块识别性能，在复杂场景与大样本数据上效果优异，有望对实际医疗诊断系统带来有效的智能辅助。

Abstract: Breast cancer is one of the most common cancers among women worldwide, and its accurate and timely diagnosis plays a critical role in improving treatment outcomes. This thesis presents an innovative framework for detecting malignant masses in mammographic images by integrating the Pyramid Adaptive Atrous Convolution (PAAC) and Transformer architectures. The proposed approach utilizes Multi-Scale Feature Fusion to enhance the extraction of features from benign and malignant tissues and combines Dice Loss and Focal Loss functions to improve the model's learning process, effectively reducing errors in binary breast cancer classification and achieving high accuracy and efficiency. In this study, a comprehensive dataset of breast cancer images from INbreast, MIAS, and DDSM was preprocessed through data augmentation and contrast enhancement and resized to 227x227 pixels for model training. Leveraging the Transformer's ability to manage long-range dependencies with Self-Attention mechanisms, the proposed model achieved high accuracy in detecting cancerous masses, outperforming foundational models such as BreastNet, DeepMammo, Multi-Scale CNN, Swin-Unet, and SegFormer. The final evaluation results for the proposed model include an accuracy of 98.5\%, sensitivity of 97.8\%, specificity of 96.3\%, F1-score of 98.2\%, and overall precision of 97.9\%. These metrics demonstrate a significant improvement over traditional methods and confirm the model's effectiveness in identifying cancerous masses in complex scenarios and large datasets. This model shows potential as a reliable and efficient tool for breast cancer diagnosis and can be effectively integrated into medical diagnostic systems.

</details>


### [81] [Federated Joint Learning for Domain and Class Generalization](https://arxiv.org/abs/2601.12253)
*Haoran Xu,Jiaze Li,Jianzhong Ju,Zhenbo Luo*

Main category: cs.CV

TL;DR: 本文提出了一种名为FedDCG的新方法，联合解决了在联邦学习场景下的类别泛化和领域泛化问题，有效提升了精调大型视觉-语言模型（如CLIP）的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常单独解决类别泛化或领域泛化，对大规模视觉-语言模型的高效精调存在局限。因此，亟需一种能兼顾两者、适用于联邦学习的新方法。

Method: FedDCG首先基于域分组，对每个分组内部训练类别泛化网络，防止决策边界混淆。推理时，基于领域相似性融合各组的类别泛化结果。同时，采用可学习网络增强类别泛化能力，并通过解耦机制分离通用与域特定知识。

Result: 在多个数据集上的大量实验表明，FedDCG在准确率和鲁棒性方面均优于现有先进方法。

Conclusion: FedDCG有效联合了类别与领域泛化，在联邦学习与大模型精调中表现突出，为解决相关应用中的泛化难题提供了新思路。

Abstract: Efficient fine-tuning of visual-language models like CLIP has become crucial due to their large-scale parameter size and extensive pretraining requirements. Existing methods typically address either the issue of unseen classes or unseen domains in isolation, without considering a joint framework for both. In this paper, we propose \textbf{Fed}erated Joint Learning for \textbf{D}omain and \textbf{C}lass \textbf{G}eneralization, termed \textbf{FedDCG}, a novel approach that addresses both class and domain generalization in federated learning settings. Our method introduces a domain grouping strategy where class-generalized networks are trained within each group to prevent decision boundary confusion. During inference, we aggregate class-generalized results based on domain similarity, effectively integrating knowledge from both class and domain generalization. Specifically, a learnable network is employed to enhance class generalization capabilities, and a decoupling mechanism separates general and domain-specific knowledge, improving generalization to unseen domains. Extensive experiments across various datasets show that \textbf{FedDCG} outperforms state-of-the-art baselines in terms of accuracy and robustness.

</details>


### [82] [Soft Shadow Diffusion (SSD): Physics-inspired Learning for 3D Computational Periscopy](https://arxiv.org/abs/2601.12257)
*Fadlullah Raji,John Murray-Bruce*

Main category: cs.CV

TL;DR: 该论文提出了一种通过普通照片实现3D非视线（NLOS）隐藏场景重建的新方法，并验证了其实验有效性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统成像依赖视线，但在某些场景（如危险、遮挡）下无法获得视线。NLOS成像旨在通过间接测量重建不可见场景，但现有方法受限于低维度和分辨率或已知形状。本研究推动了3D照片级NLOS成像的发展。

Method: 作者提出了新的光传输建模，将隐藏场景分解为遮光与非遮光两部分，使逆问题变为可分离的非线性最小二乘问题（SNLLS）。设计了两个解决方案：基于梯度优化的方法和受物理启发的神经网络（Soft Shadow diffusion, SSD）。

Result: 两种方法均在多个现实3D场景实验中验证有效。SSD网络在仿真中训练，但可泛化到未见过的新类别及真实NLOS场景，对噪声和环境光有强鲁棒性。

Conclusion: 该工作将被动NLOS成像推广到3D重建领域，提出的物理建模和深度学习方法为无直视照片重建隐藏场景提供了切实可行的解决途径。

Abstract: Conventional imaging requires a line of sight to create accurate visual representations of a scene. In certain circumstances, however, obtaining a suitable line of sight may be impractical, dangerous, or even impossible. Non-line-of-sight (NLOS) imaging addresses this challenge by reconstructing the scene from indirect measurements. Recently, passive NLOS methods that use an ordinary photograph of the subtle shadow cast onto a visible wall by the hidden scene have gained interest. These methods are currently limited to 1D or low-resolution 2D color imaging or to localizing a hidden object whose shape is approximately known. Here, we generalize this class of methods and demonstrate a 3D reconstruction of a hidden scene from an ordinary NLOS photograph. To achieve this, we propose a novel reformulation of the light transport model that conveniently decomposes the hidden scene into \textit{light-occluding} and \textit{non-light-occluding} components to yield a separable non-linear least squares (SNLLS) inverse problem. We develop two solutions: A gradient-based optimization method and a physics-inspired neural network approach, which we call Soft Shadow diffusion (SSD). Despite the challenging ill-conditioned inverse problem encountered here, our approaches are effective on numerous 3D scenes in real experimental scenarios. Moreover, SSD is trained in simulation but generalizes well to unseen classes in simulation and real-world NLOS scenes. SSD also shows surprising robustness to noise and ambient illumination.

</details>


### [83] [AgenticPruner: MAC-Constrained Neural Network Compression via LLM-Driven Strategy Search](https://arxiv.org/abs/2601.12272)
*Shahrzad Esmat,Mahdi Banisharif,Ali Jannesari*

Main category: cs.CV

TL;DR: 本文提出了AgenticPruner框架，基于大语言模型实现了针对神经网络在乘累加（MAC）预算限制下的剪枝优化，并有效控制推理延迟，提升了在ResNet、ConvNeXt和DeiT等主流架构上的精度和部署可行性。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络剪枝方法主要关注参数数量减少，难以直接严格控制推理过程中的计算成本（如MAC数量），导致实际部署时推理延迟和算力消耗不可控，不能满足资源受限场景下的精确预算需求。

Method: 提出AgenticPruner框架，基于大型语言模型，引入三类Agent协作：Profiling Agent分析模型架构和MAC分布，Master Agent负责流程调度与监控，Analysis Agent（由Claude 3.5 Sonnet驱动）通过上下文学习历史经验，优化剪枝策略。利用图结构剪枝+模式分析，实现针对MAC预算的自适应剪枝，自动收敛到目标范围。

Result: 在ImageNet-1K数据集和多种主流网络结构上，AgenticPruner能精确调控MAC预算，同时保持甚至提升模型精度。ResNet-50（77.04%准确率，比基线高0.91%）、ResNet-101（78.94%，高1.56%），ConvNeXt-Small剪枝后参数降低45%，GPU加速1.41倍，CPU加速1.07倍。Vision Transformer实现了MAC预算的精确控制，符合严格部署要求。

Conclusion: AgenticPruner能够有效实现在严格MAC预算下的自动收敛剪枝，不仅提升了神经网络的推理效率，还维持甚至提升模型精度，适用于需要算力严格控制的终端部署场景，具备广泛应用前景。

Abstract: Neural network pruning remains essential for deploying deep learning models on resource-constrained devices, yet existing approaches primarily target parameter reduction without directly controlling computational cost. This yields unpredictable inference latency in deployment scenarios where strict Multiply-Accumulate (MAC) operation budgets must be met. We propose AgenticPruner, a framework utilizing large language models to achieve MAC-constrained optimization through iterative strategy learning. Our approach coordinates three specialized agents: a Profiling Agent that analyzes model architecture and MAC distributions, a Master Agent that orchestrates the workflow with divergence monitoring, and an Analysis Agent powered by Claude 3.5 Sonnet that learns optimal strategies from historical attempts. Through in-context learning, the Analysis Agent improves convergence success rate from 48% to 71% compared to grid search. Building upon isomorphic pruning's graph-based structural grouping, our method adds context-aware adaptation by analyzing patterns across pruning iterations, enabling automatic convergence to target MAC budgets within user-defined tolerance bands.
  We validate our framework on ImageNet-1K across ResNet, ConvNeXt, and DeiT architectures. On CNNs, our approach achieves MAC targeting while maintaining or improving accuracy: ResNet-50 reaches 1.77G MACs with 77.04% accuracy (+0.91% vs baseline); ResNet-101 achieves 4.22G MACs with 78.94% accuracy (+1.56% vs baseline). For ConvNeXt-Small, pruning to 8.17G MACs yields 1.41x GPU and 1.07x CPU speedup with 45% parameter reduction. On Vision Transformers, we demonstrate MAC-budget compliance within user-defined tolerance bands (typically +1% to +5% overshoot, -5% to -15% undershoot), establishing feasibility for deployment scenarios requiring strict computational guarantees.

</details>


### [84] [CytoCLIP: Learning Cytoarchitectural Characteristics in Developing Human Brain Using Contrastive Language Image Pre-Training](https://arxiv.org/abs/2601.12282)
*Pralaypati Ta,Sriram Venkatesaperumal,Keerthi Ram,Mohanasankar Sivaprakasam*

Main category: cs.CV

TL;DR: 本文提出了一种基于CLIP模型的视觉-语言融合方法CytoCLIP，可自动识别人脑组织的独特胞体结构，实现高效脑区分割，实验效果超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 手动依据胞体结构分割脑组织区域既费时、又需要专业知识。现有自动分析方法效果有限，因此急需新的高效自动化方案。

Method: 作者提出CytoCLIP框架，基于预训练的CLIP模型开发，分别用低分辨率整区图像（区分整体结构）和高分辨率图像块（细胞层面区分）训练两个模型，利用NISSL染色的不同孕龄胎脑组织数据，进行跨模态的视觉-文本联合表示学习，用于脑区分类和信息检索。

Result: CytoCLIP在脑区分类和跨模态检索任务中均优于现有方法，对完整区域分类F1分数为0.87，图像块分类F1分数为0.91。

Conclusion: 基于CLIP的视觉-语言模型能高效自动地表征和区分大脑不同区域的胞体结构，显著减少人工标注成本，并具备较好的泛化能力。

Abstract: The functions of different regions of the human brain are closely linked to their distinct cytoarchitecture, which is defined by the spatial arrangement and morphology of the cells. Identifying brain regions by their cytoarchitecture enables various scientific analyses of the brain. However, delineating these areas manually in brain histological sections is time-consuming and requires specialized knowledge. An automated approach is necessary to minimize the effort needed from human experts. To address this, we propose CytoCLIP, a suite of vision-language models derived from pre-trained Contrastive Language-Image Pre-Training (CLIP) frameworks to learn joint visual-text representations of brain cytoarchitecture. CytoCLIP comprises two model variants: one is trained using low-resolution whole-region images to understand the overall cytoarchitectural pattern of an area, and the other is trained on high-resolution image tiles for detailed cellular-level representation. The training dataset is created from NISSL-stained histological sections of developing fetal brains of different gestational weeks. It includes 86 distinct regions for low-resolution images and 384 brain regions for high-resolution tiles. We evaluate the model's understanding of the cytoarchitecture and generalization ability using region classification and cross-modal retrieval tasks. Multiple experiments are performed under various data setups, including data from samples of different ages and sectioning planes. Experimental results demonstrate that CytoCLIP outperforms existing methods. It achieves an F1 score of 0.87 for whole-region classification and 0.91 for high-resolution image tile classification.

</details>


### [85] [SDiT: Semantic Region-Adaptive for Diffusion Transformers](https://arxiv.org/abs/2601.12283)
*Bowen Lin,Fanjiang Ye,Yihua Liu,Zhenghui Guo,Boyuan Zhang,Weijian Zheng,Yufan Xu,Tiancheng Xing,Yuke Wang,Chengming Zhang*

Main category: cs.CV

TL;DR: 该论文提出了SDiT方法，通过对不同区域分配不同的计算量，实现扩散变换器推理的提速，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的Diffusion Transformers虽然效果很好，但由于去噪迭代次数多和全局自注意力的高计算量，推理过程非常耗时。作者注意到图像去噪进程在空间上并不均匀，背景区域收敛快，边缘和纹理区演化更活跃，因此有优化空间。

Method: 提出SDiT框架，包括三部分：(1) 用Quickshift分割进行语义感知的快速分区；(2) 根据区域复杂度来动态调度，各区域分配不同的迭代计算量，仅重点更新信息量大的区域；(3) 使用边界感知的细化策略，保证各区域之间空间连贯性。整个方法无需重新训练或更改网络架构。

Result: 在不改变模型架构和不重新训练的前提下，SDiT最多实现3倍的加速，同时保持了与原模型几乎一致的感知与语义生成质量。

Conclusion: 通过对扩散去噪过程进行区域适应性调度，SDiT显著提升了推理速度，是一种有效、高效且无需深度改动的加速手段。

Abstract: Diffusion Transformers (DiTs) achieve state-of-the-art performance in text-to-image synthesis but remain computationally expensive due to the iterative nature of denoising and the quadratic cost of global attention. In this work, we observe that denoising dynamics are spatially non-uniform-background regions converge rapidly while edges and textured areas evolve much more actively. Building on this insight, we propose SDiT, a Semantic Region-Adaptive Diffusion Transformer that allocates computation according to regional complexity. SDiT introduces a training-free framework combining (1) semantic-aware clustering via fast Quickshift-based segmentation, (2) complexity-driven regional scheduling to selectively update informative areas, and (3) boundary-aware refinement to maintain spatial coherence. Without any model retraining or architectural modification, SDiT achieves up to 3.0x acceleration while preserving nearly identical perceptual and semantic quality to full-attention inference.

</details>


### [86] [LegacyAvatars: Volumetric Face Avatars For Traditional Graphics Pipelines](https://arxiv.org/abs/2601.12285)
*Safa C. Medin,Gengyan Li,Ziqian Bai,Ruofei Du,Leonhard Helminger,Yinda Zhang,Stephan J. Garbin,Philip L. Davidson,Gregory W. Wornell,Thabo Beeler,Abhimitra Meka*

Main category: cs.CV

TL;DR: 本文提出了一种高效的三维真实感人脸头像渲染方法，通过在参数化人脸模型基础上结合辐射场技术，实现对复杂人脸特征的可控体积渲染，并可高效在传统平台上线渲染。


<details>
  <summary>Details</summary>
Motivation: 当前基于辐射场的人脸建模虽然真实感强，但实时渲染效率低，且难以集成到传统渲染平台，限制了广泛应用。本工作旨在解决高质量3D人脸头像可控渲染和兼容性问题。

Method: 方法在注册阶段学习3D空间中的多个辐射流形，并从中提取分层网格及其外观与变形纹理。渲染时，仅需对静态网格的纹理进行线性混合和Alpha合成，实现面部的控制与动画。

Result: 结果显示，该显式表征不仅能实现复杂面部特征（如毛发、皮肤和眼睛）的高质量控制和渲染，还能将生成的人脸头像高效地在线流式传输，并用传统网格与着色器渲染，无需定制开发。

Conclusion: 提出的方法在保证真实感和可控性的同时，大幅提高了传统平台上的渲染效率和兼容性，推动了三维虚拟人头像的实际应用。

Abstract: We introduce a novel representation for efficient classical rendering of photorealistic 3D face avatars. Leveraging recent advances in radiance fields anchored to parametric face models, our approach achieves controllable volumetric rendering of complex facial features, including hair, skin, and eyes. At enrollment time, we learn a set of radiance manifolds in 3D space to extract an explicit layered mesh, along with appearance and warp textures. During deployment, this allows us to control and animate the face through simple linear blending and alpha compositing of textures over a static mesh. This explicit representation also enables the generated avatar to be efficiently streamed online and then rendered using classical mesh and shader-based rendering on legacy graphics platforms, eliminating the need for any custom engineering or integration.

</details>


### [87] [Concepts from Representations: Post-hoc Concept Bottleneck Models via Sparse Decomposition of Visual Representations](https://arxiv.org/abs/2601.12303)
*Shizhan Gong,Xiaofan Zhang,Qi Dou*

Main category: cs.CV

TL;DR: 该论文提出PCBM-ReD方法，可为黑盒深度学习模型增加可解释性，通过自动化概念提取、筛选和优化，将已有模型转化为可用人类语义解释的概念瓶颈模型，实现较好的准确率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型尽管在图像识别等任务上表现卓越，但由于其‘黑盒’特性，难以应用于需要高度可解释性的关键领域。传统概念解释和概念瓶颈模型存在概念不可靠、定义复杂等不足，因此需要更有效的可解释性提升方法。

Method: PCBM-ReD是一种后处理管道，使现有预训练模型具备概念层的可解释性。具体步骤包括：从预训练编码器自动提取视觉概念，通过多模态大语言模型对概念进行命名与筛选（关注可视性和与任务相关性），再通过重建优化选择独立的概念子集。最后利用CLIP模型的视觉-文本对齐能力，将图像表示分解为若干概念嵌入的线性组合，适配于概念瓶颈模型架构。

Result: 在11个图像分类任务上，PCBM-ReD在准确率上达到最新水平，并缩小了与端到端模型的性能差距，同时具有更优越的可解释性。

Conclusion: PCBM-ReD能够在不影响主流模型性能的前提下，显著提升模型的可解释性，为批判性领域的AI模型部署提供了新方案。

Abstract: Deep learning has achieved remarkable success in image recognition, yet their inherent opacity poses challenges for deployment in critical domains. Concept-based interpretations aim to address this by explaining model reasoning through human-understandable concepts. However, existing post-hoc methods and ante-hoc concept bottleneck models (CBMs), suffer from limitations such as unreliable concept relevance, non-visual or labor-intensive concept definitions, and model or data-agnostic assumptions. This paper introduces Post-hoc Concept Bottleneck Model via Representation Decomposition (PCBM-ReD), a novel pipeline that retrofits interpretability onto pretrained opaque models. PCBM-ReD automatically extracts visual concepts from a pre-trained encoder, employs multimodal large language models (MLLMs) to label and filter concepts based on visual identifiability and task relevance, and selects an independent subset via reconstruction-guided optimization. Leveraging CLIP's visual-text alignment, it decomposes image representations into linear combination of concept embeddings to fit into the CBMs abstraction. Extensive experiments across 11 image classification tasks show PCBM-ReD achieves state-of-the-art accuracy, narrows the performance gap with end-to-end models, and exhibits better interpretability.

</details>


### [88] [A Two-Stage Globally-Diverse Adversarial Attack for Vision-Language Pre-training Models](https://arxiv.org/abs/2601.12304)
*Wutao Chen,Huaqin Zou,Chen Wan,Lifeng Huang*

Main category: cs.CV

TL;DR: 论文提出了一种新的两阶段全球多样化攻击（2S-GDA）框架，用于对视觉-语言预训练模型进行黑盒对抗攻击，并在现有方法上获得了显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态对抗攻击方法在黑盒场景下攻击视觉-语言模型时，往往面临攻击扰动单一、攻击流程不稳定等问题，导致攻击成功率低。作者希望通过提升扰动的多样性和流程的稳定性，提升对抗攻击的有效性和可转移性。

Method: 提出了两阶段全球多样化攻击（2S-GDA）框架：（1）文本端采用候选文本扩展+全局感知替换，生成多样性的文本对抗样本；（2）图像端通过多尺度变换和分块旋转，增强视觉扰动的多样性，从而生成更易转移的图像扰动。整个攻击流程为模块化设计，可与现有方法结合。

Result: 基于VLP模型的大量实验表明，2S-GDA在黑盒攻击场景下攻击成功率比SOTA方法提升高达11.17%，并展现出良好的可移植性和兼容性。

Conclusion: 2S-GDA框架显著提高了黑盒多模态对抗攻击的有效性和扰动多样性，能够与现有方法结合，进一步提升对抗转移能力，对提升VLP模型的健壮性分析具有实际价值。

Abstract: Vision-language pre-training (VLP) models are vulnerable to adversarial examples, particularly in black-box scenarios. Existing multimodal attacks often suffer from limited perturbation diversity and unstable multi-stage pipelines. To address these challenges, we propose 2S-GDA, a two-stage globally-diverse attack framework. The proposed method first introduces textual perturbations through a globally-diverse strategy by combining candidate text expansion with globally-aware replacement. To enhance visual diversity, image-level perturbations are generated using multi-scale resizing and block-shuffle rotation. Extensive experiments on VLP models demonstrate that 2S-GDA consistently improves attack success rates over state-of-the-art methods, with gains of up to 11.17\% in black-box settings. Our framework is modular and can be easily combined with existing methods to further enhance adversarial transferability.

</details>


### [89] [Adaptive Multi-Scale Correlation Meta-Network for Few-Shot Remote Sensing Image Classification](https://arxiv.org/abs/2601.12308)
*Anurag Kaushish,Ayan Sar,Sampurna Roy,Sudeshna Chakraborty,Prashant Trivedi,Tanupriya Choudhury,Kanav Gupta*

Main category: cs.CV

TL;DR: 本文提出AMC-MetaNet方法，有效提升了遥感领域小样本学习的准确性，且参数量大幅减少，计算效率提升。


<details>
  <summary>Details</summary>
Motivation: 遥感影像中的小样本学习受限于标注数据稀缺、域分布差异和地物尺度多样性，现有大多数方法参数量庞大或依赖预训练模型，难以实际应用。因此，亟需兼具高效率与强泛化力的轻量级新方法。

Method: 提出了一种轻量级AMC-MetaNet模型，包含三点创新：1) 相关性引导的特征金字塔捕捉尺度不变性，2) 自适应通道相关模块学习跨尺度关系，3) 利用相关性模式进行元学习代替传统原型均值法。全新设计下模型仅需约60万参数，无需大规模预训练。

Result: 在EuroSAT、NWPU-RESISC45、UC Merced Land Use和AID等多遥感数据集5-way 5-shot分类任务上，准确率最高可达86.65%；推理速度优异，参数量仅为ResNet-18的1/20。

Conclusion: AMC-MetaNet展现了遥感领域小样本学习的高效、可扩展和尺度感知优势，为实际应用提供了有力支持。

Abstract: Few-shot learning in remote sensing remains challenging due to three factors: the scarcity of labeled data, substantial domain shifts, and the multi-scale nature of geospatial objects. To address these issues, we introduce Adaptive Multi-Scale Correlation Meta-Network (AMC-MetaNet), a lightweight yet powerful framework with three key innovations: (i) correlation-guided feature pyramids for capturing scale-invariant patterns, (ii) an adaptive channel correlation module (ACCM) for learning dynamic cross-scale relationships, and (iii) correlation-guided meta-learning that leverages correlation patterns instead of conventional prototype averaging. Unlike prior approaches that rely on heavy pre-trained models or transformers, AMC-MetaNet is trained from scratch with only $\sim600K$ parameters, offering $20\times$ fewer parameters than ResNet-18 while maintaining high efficiency ($<50$ms per image inference). AMC-MetaNet achieves up to 86.65\% accuracy in 5-way 5-shot classification on various remote sensing datasets, including EuroSAT, NWPU-RESISC45, UC Merced Land Use, and AID. Our results establish AMC-MetaNet as a computationally efficient, scale-aware framework for real-world few-shot remote sensing.

</details>


### [90] [CurConMix+: A Unified Spatio-Temporal Framework for Hierarchical Surgical Workflow Understanding](https://arxiv.org/abs/2601.12312)
*Yongjun Jeon,Jongmin Shin,Kanggil Park,Seonmin Park,Soyoung Lim,Jung Yong Kim,Jinsoo Rhu,Jongman Kim,Gyu-Seong Choi,Namkee Oh,Kyu-Hwan Jung*

Main category: cs.CV

TL;DR: 本文提出了一种新的手术动作三元组识别方法CurConMix+，可同时应对类别不平衡、细微视觉差异、三元组语义依赖等挑战，并引入了新的层级手术数据集LLS48，显著提升了精细及跨层级手术理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有手术三元组识别方法多只解决部分问题，对类别不平衡、视觉微差和三元组间复杂依赖等挑战无法全面覆盖，限制了临床场景中的精细化流程分析和技能评估。

Method: 以CurConMix空间表示框架为基础，提出课程指导的对比学习策略，结合结构化困难样本配对与特征级mixup提升特征判别力，并扩展为CurConMix+，集成多分辨率时序Transformer（MRTT），通过融合多尺度时序特征实现更具上下文感知的时空建模。同时推出分级注释的数据集LLS48，覆盖腹腔镜左外叶切除手术多个层级的行为标注。

Result: 在CholecT45及新提出的LLS48数据集上实验表明，CurConMix+在三元组识别任务上超越主流方法，并凭借精细特征展现出跨层级（如手术阶段与步骤识别）迁移和泛化能力。

Conclusion: CurConMix+及LLS48为层级化、可复现且可解释的手术流程理解提供了统一基础，并将开源代码和数据促进该领域进一步研究。

Abstract: Surgical action triplet recognition aims to understand fine-grained surgical behaviors by modeling the interactions among instruments, actions, and anatomical targets. Despite its clinical importance for workflow analysis and skill assessment, progress has been hindered by severe class imbalance, subtle visual variations, and the semantic interdependence among triplet components. Existing approaches often address only a subset of these challenges rather than tackling them jointly, which limits their ability to form a holistic understanding. This study builds upon CurConMix, a spatial representation framework. At its core, a curriculum-guided contrastive learning strategy learns discriminative and progressively correlated features, further enhanced by structured hard-pair sampling and feature-level mixup. Its temporal extension, CurConMix+, integrates a Multi-Resolution Temporal Transformer (MRTT) that achieves robust, context-aware understanding by adaptively fusing multi-scale temporal features and dynamically balancing spatio-temporal cues. Furthermore, we introduce LLS48, a new, hierarchically annotated benchmark for complex laparoscopic left lateral sectionectomy, providing step-, task-, and action-level annotations. Extensive experiments on CholecT45 and LLS48 demonstrate that CurConMix+ not only outperforms state-of-the-art approaches in triplet recognition, but also exhibits strong cross-level generalization, as its fine-grained features effectively transfer to higher-level phase and step recognition tasks. Together, the framework and dataset provide a unified foundation for hierarchy-aware, reproducible, and interpretable surgical workflow understanding. The code and dataset will be publicly released on GitHub to facilitate reproducibility and further research.

</details>


### [91] [S^2F-Net:A Robust Spatial-Spectral Fusion Framework for Cross-Model AIGC Detection](https://arxiv.org/abs/2601.12313)
*Xiangyu Hu,Yicheng Hong,Hongchuang Zheng,Wenjun Zeng,Bingyao Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为S2F-Net的检测框架，通过利用真实与合成纹理之间在频域的内在差异，显著提升了对不同生成模型的检测泛化能力，并在大规模基准测试中取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 随着生成式模型的快速发展，产生了强泛化能力检测方法的迫切需求。现有检测方法通常过拟合特定模型，在面对新型生成架构时性能严重下降。因此需要新的方法来提升对未见模型的检测能力。

Method: 提出了S2F-Net交叉模型检测框架，核心思路是挖掘并利用真实与合成纹理在频域的差异。针对上采样操作在不同区域遗留的独特频率指纹，设计了可学习的频率注意力模块，通过结合空间纹理分析和频谱依赖性，自适应增强判别性频段，实现更好的泛化检测。

Result: 在包括17种生成模型的大型AIGCDetectBenchmark上，S2F-Net实现了90.49%的检测准确率，在跨域检测场景中显著优于多种现有基线方法。

Conclusion: S2F-Net通过频域特征挖掘和自适应频率注意力机制，有效提升了对不同生成模型的检测泛化能力，为生成内容检测问题提供了新思路，并显示出强劲的实际应用潜力。

Abstract: The rapid development of generative models has imposed an urgent demand for detection schemes with strong generalization capabilities. However, existing detection methods generally suffer from overfitting to specific source models, leading to significant performance degradation when confronted with unseen generative architectures. To address these challenges, this paper proposes a cross-model detection framework called S 2 F-Net, whose core lies in exploring and leveraging the inherent spectral discrepancies between real and synthetic textures. Considering that upsampling operations leave unique and distinguishable frequency fingerprints in both texture-poor and texture-rich regions, we focus our research on the detection of frequency-domain artifacts, aiming to fundamentally improve the generalization performance of the model. Specifically, we introduce a learnable frequency attention module that adaptively weights and enhances discriminative frequency bands by synergizing spatial texture analysis and spectral dependencies.On the AIGCDetectBenchmark, which includes 17 categories of generative models, S 2 F-Net achieves a detection accuracy of 90.49%, significantly outperforming various existing baseline methods in cross-domain detection scenarios.

</details>


### [92] [GazeFormer-MoE: Context-Aware Gaze Estimation via CLIP and MoE Transformer](https://arxiv.org/abs/2601.12316)
*Xinyuan Zhao,Xianrui Chen,Ahmad Chaddad*

Main category: cs.CV

TL;DR: 本文提出了一种基于语义调节与多尺度融合的Transformer模型，用于三维凝视估计，并在多个公开数据集上刷新了性能记录。


<details>
  <summary>Details</summary>
Motivation: 三维凝视估计在行为理解、人机交互等领域具有广泛应用，但现有方法在复杂条件下表现有限，急需提升模型对多样环境与行人多变性的适应能力。

Method: 作者提出结合CLIP提取的全局特征与可学习原型库（如光照、头部姿态、背景、凝视方向），将原型丰富的特征与CLIP分块token及高分辨率CNN token在统一注意力空间中融合；并将部分前馈网络（FFN）块替换为共享/路由的专家混合模块（MoE），以增强模型的条件表达能力。

Result: 在MPIIFaceGaze、EYEDIAP、Gaze360、ETH-XGaze四个数据集上的角度误差分别达到2.49°、3.22°、10.16°和1.44°，均优于以往最佳结果，最高相对提升达64%。消融实验表明原型调节、跨尺度融合、专家混合和超参数设置均对性能提升有贡献。

Conclusion: 该方法显著提升了三维凝视估计的准确性，验证了多尺度融合与条件调节机制的有效性，为后续相关任务研究提供了新范式。

Abstract: We present a semantics modulated, multi scale Transformer for 3D gaze estimation. Our model conditions CLIP global features with learnable prototype banks (illumination, head pose, background, direction), fuses these prototype-enriched global vectors with CLIP patch tokens and high-resolution CNN tokens in a unified attention space, and replaces several FFN blocks with routed/shared Mixture of Experts to increase conditional capacity. Evaluated on MPIIFaceGaze, EYEDIAP, Gaze360 and ETH-XGaze, our model achieves new state of the art angular errors of 2.49°, 3.22°, 10.16°, and 1.44°, demonstrating up to a 64% relative improvement over previously reported results. ablations attribute gains to prototype conditioning, cross scale fusion, MoE and hyperparameter. Our code is publicly available at https://github. com/AIPMLab/Gazeformer.

</details>


### [93] [Multi-Sensor Matching with HyperNetworks](https://arxiv.org/abs/2601.12325)
*Eli Passov,Nathan S. Netanyahu,Yosi Keller*

Main category: cs.CV

TL;DR: 这篇论文提出了一种结合超网络和条件归一化的新型轻量化网络架构，用于提升多模态图像块匹配的鲁棒性与效率，并在VIS-NIR和VIS-IR等基准任务上取得了新的最好结果，还发布了新的跨域数据集GAP-VIR。


<details>
  <summary>Details</summary>
Motivation: 多模态（如可见光与红外）图像块的匹配，由于模态间外观差异大而极具挑战；传统方法在模型大小和效率之间难以兼顾，也缺乏对域迁移能力的有效评估。作者为了解决这些问题，希望提升匹配模型的泛化性和计算效率，同时推动跨平台领域自适应相关研究。

Method: 作者提出一种轻量级特征描述子学习架构，结合Siamese CNN主干网络与两大机制：一是通过超网络模块实现每通道自适应缩放与偏移，二是条件实例归一化带来浅层对模态（如VIS/IR）的自适应能力。模型以三元组损失和困难负样本挖掘进行训练。

Result: 新方法在多个VIS-NIR、VIS-IR基准数据集上实现了当前最优性能，在额外数据集上也达到或超越其它高推理成本方法。

Conclusion: 结合超网络与条件归一化的架构可以在保持高效推理的前提下，显著提升多模态匹配的鲁棒性和泛化能力，且新数据集GAP-VIR为领域迁移研究提供了坚实支撑。

Abstract: Hypernetworks are models that generate or modulate the weights of another network. They provide a flexible mechanism for injecting context and task conditioning and have proven broadly useful across diverse applications without significant increases in model size. We leverage hypernetworks to improve multimodal patch matching by introducing a lightweight descriptor-learning architecture that augments a Siamese CNN with (i) hypernetwork modules that compute adaptive, per-channel scaling and shifting and (ii) conditional instance normalization that provides modality-specific adaptation (e.g., visible vs. infrared, VIS-IR) in shallow layers. This combination preserves the efficiency of descriptor-based methods during inference while increasing robustness to appearance shifts. Trained with a triplet loss and hard-negative mining, our approach achieves state-of-the-art results on VIS-NIR and other VIS-IR benchmarks and matches or surpasses prior methods on additional datasets, despite their higher inference cost. To spur progress on domain shift, we also release GAP-VIR, a cross-platform (ground/aerial) VIS-IR patch dataset with 500K pairs, enabling rigorous evaluation of cross-domain generalization and adaptation.

</details>


### [94] [EmoKGEdit: Training-free Affective Injection via Visual Cue Transformation](https://arxiv.org/abs/2601.12326)
*Jing Zhang,Bingjie Fan*

Main category: cs.CV

TL;DR: 现有的图像情感编辑方法难以将情感线索与内容特征有效分离，导致情感表达不强烈和图像结构失真。本文提出了EmoKGEdit，一个无需训练的新颖框架，通过构建多模态情感关联知识图谱（MSA-KG）和结构-情感解耦编辑模块，实现了精准且结构保真的图像情感编辑。实验结果表明新方法在情感保真和内容保持方面均优于现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在图像情感编辑时，常出现情感注入不到位和破坏原始图像结构的问题，主要是因为难以分离情感和内容表达的潜在因素。因此，亟需一种能更好解耦情感与内容、实现高保真图像情感编辑的新方法。

Method: 作者提出了无训练的EmoKGEdit框架。该方法首先构建了一个多模态情感关联知识图谱（MSA-KG），明确编码对象-属性-情感间的因果链，作为外部知识支持大模型推理情感相关视觉线索，并生成连贯的指令。同时基于知识图谱，设计了解耦结构-情感编辑模块，在潜空间中分离情感属性与布局特征，从而实现在精准注入目标情感的同时严格保持图像空间结构。

Result: 大量实验显示，EmoKGEdit在情感保真（emotion fidelity）和内容保持（content preservation）方面效果显著，且超越了当前主流方法。

Conclusion: EmoKGEdit能在无需训练的前提下，实现结构保真的图像情感精准编辑，为情感图像生成与编辑领域带来高保真度和更出色的分离能力，具有良好应用前景。

Abstract: Existing image emotion editing methods struggle to disentangle emotional cues from latent content representations, often yielding weak emotional expression and distorted visual structures. To bridge this gap, we propose EmoKGEdit, a novel training-free framework for precise and structure-preserving image emotion editing. Specifically, we construct a Multimodal Sentiment Association Knowledge Graph (MSA-KG) to disentangle the intricate relationships among objects, scenes, attributes, visual clues and emotion. MSA-KG explicitly encode the causal chain among object-attribute-emotion, and as external knowledge to support chain of thought reasoning, guiding the multimodal large model to infer plausible emotion-related visual cues and generate coherent instructions. In addition, based on MSA-KG, we design a disentangled structure-emotion editing module that explicitly separates emotional attributes from layout features within the latent space, which ensures that the target emotion is effectively injected while strictly maintaining visual spatial coherence. Extensive experiments demonstrate that EmoKGEdit achieves excellent performance in both emotion fidelity and content preservation, and outperforms the state-of-the-art methods.

</details>


### [95] [FlowIID: Single-Step Intrinsic Image Decomposition via Latent Flow Matching](https://arxiv.org/abs/2601.12329)
*Mithlesh Singla,Seema Kumari,Shanmuganathan Raman*

Main category: cs.CV

TL;DR: 本文提出了一种参数高效的内在图像分解模型FlowIID，利用潜在流匹配和VAE引导，实现了快速且稳定的反照率与明暗分离。该方法在多个基准数据集上表现优异，适合资源受限与实时视觉应用。


<details>
  <summary>Details</summary>
Motivation: 现有内在图像分解模型虽然效果好，但参数量大，难以在实际场景与其他模型结合使用。作者希望通过减少参数与提高推理效率，使IID模型更适合实际部署。

Method: 作者提出了一种基于潜在流匹配的结构FlowIID，将VAE引导的潜在空间与流匹配模块结合，仅用一次推理步骤完成反照率与明暗分离。该设计大幅精简了模型参数。

Result: FlowIID模型在多个公开基准测试中，表现出了与主流模型相比有竞争力乃至更好的分解效果，同时占用更少的参数量和计算资源。

Conclusion: FlowIID不仅在分解质量上优于现有方法，还因其高效、参数少和单步推理特性，非常适合在边缘设备或实时场景中部署。

Abstract: Intrinsic Image Decomposition (IID) separates an image into albedo and shading components. It is a core step in many real-world applications, such as relighting and material editing. Existing IID models achieve good results, but often use a large number of parameters. This makes them costly to combine with other models in real-world settings. To address this problem, we propose a flow matching-based solution. For this, we design a novel architecture, FlowIID, based on latent flow matching. FlowIID combines a VAE-guided latent space with a flow matching module, enabling a stable decomposition of albedo and shading. FlowIID is not only parameter-efficient, but also produces results in a single inference step. Despite its compact design, FlowIID delivers competitive and superior results compared to existing models across various benchmarks. This makes it well-suited for deployment in resource-constrained and real-time vision applications.

</details>


### [96] [Turbo-GoDec: Exploiting the Cluster Sparsity Prior for Hyperspectral Anomaly Detection](https://arxiv.org/abs/2601.12337)
*Jiahui Sheng,Xiaorun Li,Shuhan Chen*

Main category: cs.CV

TL;DR: 本文提出了一种结合聚类稀疏性先验的新型高光谱异常检测方法Turbo-GoDec，通过引入马尔可夫随机场进行聚类稀疏建模并嵌入到GoDec算法中，显著提升了对小尺寸异常的检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有高光谱异常检测方法主要基于低秩背景和稀疏异常的先验假设，但对异常的空间分布特征利用有限。观察发现异常像元一般呈空间小团簇分布，作者提出对异常的聚类稀疏性加以建模以提升检测效果。

Method: 将异常的聚类稀疏性先验用马尔可夫随机场建模，通过因子图上的消息传递算法估计异常概率，并将其作为GoDec算法S步中的稀疏分量，形成Turbo-GoDec方法。

Result: 在三个真实高光谱图像数据集上的实验表明，Turbo-GoDec在小尺寸异常检测方面相较于原始GoDec（LSMAD）和最新的异常检测方法表现更优。

Conclusion: 通过结合聚类稀疏性先验与GoDec算法，Turbo-GoDec极大提高了小尺寸异常在高光谱图像中的检测能力，对异常检测方法的发展具有积极促进作用。

Abstract: As a key task in hyperspectral image processing, hyperspectral anomaly detection has garnered significant attention and undergone extensive research. Existing methods primarily relt on two prior assumption: low-rank background and sparse anomaly, along with additional spatial assumptions of the background. However, most methods only utilize the sparsity prior assumption for anomalies and rarely expand on this hypothesis. From observations of hyperspectral images, we find that anomalous pixels exhibit certain spatial distribution characteristics: they often manifest as small, clustered groups in space, which we refer to as cluster sparsity of anomalies. Then, we combined the cluster sparsity prior with the classical GoDec algorithm, incorporating the cluster sparsity prior into the S-step of GoDec. This resulted in a new hyperspectral anomaly detection method, which we called Turbo-GoDec. In this approach, we modeled the cluster sparsity prior of anomalies using a Markov random field and computed the marginal probabilities of anomalies through message passing on a factor graph. Locations with high anomalous probabilities were treated as the sparse component in the Turbo-GoDec. Experiments are conducted on three real hyperspectral image (HSI) datasets which demonstrate the superior performance of the proposed Turbo-GoDec method in detecting small-size anomalies comparing with the vanilla GoDec (LSMAD) and state-of-the-art anomaly detection methods. The code is available at https://github.com/jiahuisheng/Turbo-GoDec.

</details>


### [97] [MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents](https://arxiv.org/abs/2601.12346)
*Peizhou Huang,Zixuan Zhong,Zhongwei Wan,Donghao Zhou,Samiul Alam,Xin Wang,Zexin Li,Zhihao Dou,Li Zhu,Jing Xiong,Chaofan Tao,Yan Xu,Dimitrios Dimitriadis,Tuo Zhang,Mi Zhang*

Main category: cs.CV

TL;DR: 本文提出了MMDeepResearch-Bench（MMDR-Bench）多模态基准，用于评估DRAs在引用丰富报告生成中的证据关联能力，同时创新设计了一套细粒度、可诊断的评测方法。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究智能体（DRAs）尽管能生成包含多步检索和综合的报告，但现有基准主要关注单一文本或短小任务，无法真实衡量其多模态理解和基于证据的综合能力。因此亟需一个能够覆盖多模态证据、关注引文准确性、确保生成的一致性和可追溯性的评测框架。

Method: 作者构建了包含140个由专家设计、横跨21个领域的图文任务集合MMDR-Bench，要求模型生成基于图片和文本证据的、结构化且引用明确的长文报告。同时提出三大评测方法：1）FLAE评价文本生成质量；2）TRACE评价引用证据对齐度；3）MOSAIC评价生成内容与视觉证据的一致性，为模型提供细粒度误差分析。

Result: 在25种主流模型上的实验显示，模型在生成质量、引用规范和多模态融合三方面存在系统性权衡，优秀的文本能力并不能说明证据使用的准确性，多模态结合和证据一致性仍是瓶颈。

Conclusion: MMDR-Bench能够促进多模态、基于证据的生成任务研究，并借助细致评测手段揭示当前模型的短板，有力推动深度研究智能体的实际应用与技术进步。

Abstract: Deep Research Agents (DRAs) generate citation-rich reports via multi-step search and synthesis, yet existing benchmarks mainly target text-only settings or short-form multimodal QA, missing end-to-end multimodal evidence use. We introduce MMDeepResearch-Bench (MMDR-Bench), a benchmark of 140 expert-crafted tasks across 21 domains, where each task provides an image-text bundle to evaluate multimodal understanding and citation-grounded report generation. Compared to prior setups, MMDR-Bench emphasizes report-style synthesis with explicit evidence use, where models must connect visual artifacts to sourced claims and maintain consistency across narrative, citations, and visual references. We further propose a unified, interpretable evaluation pipeline: Formula-LLM Adaptive Evaluation (FLAE) for report quality, Trustworthy Retrieval-Aligned Citation Evaluation (TRACE) for citation-grounded evidence alignment, and Multimodal Support-Aligned Integrity Check (MOSAIC) for text-visual integrity, each producing fine-grained signals that support error diagnosis beyond a single overall score. Experiments across 25 state-of-the-art models reveal systematic trade-offs between generation quality, citation discipline, and multimodal grounding, highlighting that strong prose alone does not guarantee faithful evidence use and that multimodal integrity remains a key bottleneck for deep research agents.

</details>


### [98] [SimpleMatch: A Simple and Strong Baseline for Semantic Correspondence](https://arxiv.org/abs/2601.12357)
*Hailing Jin,Huiying Li*

Main category: cs.CV

TL;DR: 本论文提出了一种名为SimpleMatch的语义对应新框架，在低分辨率下也能取得很强表现，有效降低了计算开销。主要创新在于用逐步上采样解码器恢复空间细节，并设计了多尺度监督损失和稀疏匹配策略，大大优化了训练时的显存和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于大模型的语义对应方法依赖高分辨率输入以获得最佳效果，但这带来了高昂的计算成本。而由于深层下采样，部分关键点特征会不可逆融合，导致精度下降。针对这些限制，论文希望在低分辨率下提升语义匹配的精度和效率。

Method: 提出了SimpleMatch方法，包括：1）轻量级上采样解码器，将深层特征逐步上采样到1/4分辨率；2）多尺度监督损失，确保不同空间尺度下的特征保持判别力；3）引入稀疏匹配和窗口化定位方法，大幅减少训练所需内存。

Result: 在低至252x252分辨率下，SimpleMatch在SPair-71k数据集上达到了84.1%的PCK@0.1，比当前主流方法效能更高，同时显存消耗降低了51%。

Conclusion: SimpleMatch为低分辨率条件下的语义对应提供了高效、实用的新基线，既兼顾精度又大幅提升了计算资源利用效率，对今后相关研究有重要参考意义。

Abstract: Recent advances in semantic correspondence have been largely driven by the use of pre-trained large-scale models. However, a limitation of these approaches is their dependence on high-resolution input images to achieve optimal performance, which results in considerable computational overhead. In this work, we address a fundamental limitation in current methods: the irreversible fusion of adjacent keypoint features caused by deep downsampling operations. This issue is triggered when semantically distinct keypoints fall within the same downsampled receptive field (e.g., 16x16 patches). To address this issue, we present SimpleMatch, a simple yet effective framework for semantic correspondence that delivers strong performance even at low resolutions. We propose a lightweight upsample decoder that progressively recovers spatial detail by upsampling deep features to 1/4 resolution, and a multi-scale supervised loss that ensures the upsampled features retain discriminative features across different spatial scales. In addition, we introduce sparse matching and window-based localization to optimize training memory usage and reduce it by 51%. At a resolution of 252x252 (3.3x smaller than current SOTA methods), SimpleMatch achieves superior performance with 84.1% PCK@0.1 on the SPair-71k benchmark. We believe this framework provides a practical and efficient baseline for future research in semantic correspondence. Code is available at: https://github.com/hailong23-jin/SimpleMatch.

</details>


### [99] [DepthCropSeg++: Scaling a Crop Segmentation Foundation Model With Depth-Labeled Data](https://arxiv.org/abs/2601.12366)
*Jiafei Zhang,Songliang Cao,Binghui Xu,Yanan Li,Weiwei Jia,Tingting Wu,Hao Lu,Weijuan Hu,Zhiguo Han*

Main category: cs.CV

TL;DR: 本文提出了DepthCropSeg++，一种能够在开放田间环境下对不同作物种类进行分割的基础模型，性能优于现有方法，并在多种复杂环境下表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有作物分割模型由于标注成本高，数据有限，泛化能力差，往往只能针对特定作物或环境，难以满足真实农业应用的需求。

Method: 作者扩展了跨物种与跨场景分割数据集，收集了28406张图片，涵盖30余种作物及15种环境条件，基于ViT-Adapter架构，加入动态上采样用于提升细节识别，采用两阶段自训练流程训练模型。

Result: 在综合测试集上，DepthCropSeg++取得了93.11%的mIoU，分别比监督学习基线和SAM等通用视觉大模型高出0.36%和48.57%，在夜间、高密度、未见作物等困难场景仍表现优异。

Conclusion: DepthCropSeg++实现了跨作物、跨场景的高精度分割，显著超越现有方法，为作物分割树立了新标杆，对农业智能化应用有重要推动作用。

Abstract: DepthCropSeg++: a foundation model for crop segmentation, capable of segmenting different crop species under open in-field environment. Crop segmentation is a fundamental task for modern agriculture, which closely relates to many downstream tasks such as plant phenotyping, density estimation, and weed control. In the era of foundation models, a number of generic large language and vision models have been developed. These models have demonstrated remarkable real world generalization due to significant model capacity and largescale datasets. However, current crop segmentation models mostly learn from limited data due to expensive pixel-level labelling cost, often performing well only under specific crop types or controlled environment. In this work, we follow the vein of our previous work DepthCropSeg, an almost unsupervised approach to crop segmentation, to scale up a cross-species and crossscene crop segmentation dataset, with 28,406 images across 30+ species and 15 environmental conditions. We also build upon a state-of-the-art semantic segmentation architecture ViT-Adapter architecture, enhance it with dynamic upsampling for improved detail awareness, and train the model with a two-stage selftraining pipeline. To systematically validate model performance, we conduct comprehensive experiments to justify the effectiveness and generalization capabilities across multiple crop datasets. Results demonstrate that DepthCropSeg++ achieves 93.11% mIoU on a comprehensive testing set, outperforming both supervised baselines and general-purpose vision foundation models like Segmentation Anything Model (SAM) by significant margins (+0.36% and +48.57% respectively). The model particularly excels in challenging scenarios including night-time environment (86.90% mIoU), high-density canopies (90.09% mIoU), and unseen crop varieties (90.09% mIoU), indicating a new state of the art for crop segmentation.

</details>


### [100] [Utilizing the Score of Data Distribution for Hyperspectral Anomaly Detection](https://arxiv.org/abs/2601.12379)
*Jiahui Sheng,Yidan Shi,Shu Xiang,Xiaorun Li,Shuhan Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种基于Score-based生成模型的高光谱异常检测方法（ScoreAD），能有效区分高光谱图像中的异常和背景。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像虽高维，但其光谱多由少数因素决定，满足流形假设。利用这一点可以更有效地检测异常。

Method: 提出利用Score-based生成模型（SGM）学习高光谱图像的分布，在检测时通过扰动和分数估计区分背景与异常光谱，实现异常检测。

Result: 在四个高光谱数据集上进行实验，结果表明所提方法效果突出。

Conclusion: ScoreAD能有效利用高光谱图像的流形结构进行异常检测，实验验证了其有效性。

Abstract: Hyperspectral images (HSIs) are a type of image that contains abundant spectral information. As a type of real-world data, the high-dimensional spectra in hyperspectral images are actually determined by only a few factors, such as chemical composition and illumination. Thus, spectra in hyperspectral images are highly likely to satisfy the manifold hypothesis. Based on the hyperspectral manifold hypothesis, we propose a novel hyperspectral anomaly detection method (named ScoreAD) that leverages the time-dependent gradient field of the data distribution (i.e., the score), as learned by a score-based generative model (SGM). Our method first trains the SGM on the entire set of spectra from the hyperspectral image. At test time, each spectrum is passed through a perturbation kernel, and the resulting perturbed spectrum is fed into the trained SGM to obtain the estimated score. The manifold hypothesis of HSIs posits that background spectra reside on one or more low-dimensional manifolds. Conversely, anomalous spectra, owing to their unique spectral signatures, are considered outliers that do not conform to the background manifold. Based on this fundamental discrepancy in their manifold distributions, we leverage a generative SGM to achieve hyperspectral anomaly detection. Experiments on the four hyperspectral datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/jiahuisheng/ScoreAD.

</details>


### [101] [A Hierarchical Benchmark of Foundation Models for Dermatology](https://arxiv.org/abs/2601.12382)
*Furkan Yuceyalcin,Abdurrahim Yilmaz,Burak Temelkuran*

Main category: cs.CV

TL;DR: 本研究利用十种基础模型生成的嵌入特征，评估其在皮肤病变分层分类中的表现，并发现大模型在粗粒度分类上表现优异，但在细粒度分类上存在性能下降。


<details>
  <summary>Details</summary>
Motivation: 目前皮肤科领域常见的基准测试任务将复杂的诊断分类问题简化成二元分类，忽视了实际临床对细粒度鉴别的需求。本研究旨在弥补基础模型在皮肤病变分层诊断能力评估上的空白。

Method: 选取覆盖通用视觉、通用医学影像和皮肤科专用领域的十种基础模型，对DERM12345数据集的40类皮肤病变进行嵌入提取，通过冻结特征+轻量级适配器模型，用五折交叉验证分析嵌入的层次分类能力，并提出层次化评测体系。

Result: 通用医学基础模型MedImageInsights在二元恶性筛查上表现最优（F1=97.52%），但在40类细粒度分类上大幅下降（F1=65.50%）；而MedSigLip及皮肤科专用模型虽在粗粒度任务表现稍逊，但在细粒度区分（F1约69.79%）上优于通用模型。

Conclusion: 通用医学基础模型适合高层级粗分类，但在临床需要的细分任务上，还需专用建模策略提升细粒度诊断支持能力。

Abstract: Foundation models have transformed medical image analysis by providing robust feature representations that reduce the need for large-scale task-specific training. However, current benchmarks in dermatology often reduce the complex diagnostic taxonomy to flat, binary classification tasks, such as distinguishing melanoma from benign nevi. This oversimplification obscures a model's ability to perform fine-grained differential diagnoses, which is critical for clinical workflow integration. This study evaluates the utility of embeddings derived from ten foundation models, spanning general computer vision, general medical imaging, and dermatology-specific domains, for hierarchical skin lesion classification. Using the DERM12345 dataset, which comprises 40 lesion subclasses, we calculated frozen embeddings and trained lightweight adapter models using a five-fold cross-validation. We introduce a hierarchical evaluation framework that assesses performance across four levels of clinical granularity: 40 Subclasses, 15 Main Classes, 2 and 4 Superclasses, and Binary Malignancy. Our results reveal a "granularity gap" in model capabilities: MedImageInsights achieved the strongest overall performance (97.52% weighted F1-Score on Binary Malignancy detection) but declined to 65.50% on fine-grained 40-class subtype classification. Conversely, MedSigLip (69.79%) and dermatology-specific models (Derm Foundation and MONET) excelled at fine-grained 40-class subtype discrimination while achieving lower overall performance than MedImageInsights on broader classification tasks. Our findings suggest that while general medical foundation models are highly effective for high-level screening, specialized modeling strategies are necessary for the granular distinctions required in diagnostic support systems.

</details>


### [102] [Class-Partitioned VQ-VAE and Latent Flow Matching for Point Cloud Scene Generation](https://arxiv.org/abs/2601.12391)
*Dasith de Silva Edirimuni,Ajmal Saeed Mian*

Main category: cs.CV

TL;DR: 本文提出了一种能够直接根据生成的特征和类别输出类特定三维点云物体的新型生成方法，无需依赖外部数据库检索，显著提升了复杂场景下的三维场景生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有三维场景生成方法通常只能生成物体包围盒参数，或需借助预定义数据库检索具体物体，难以直接生成多类别复杂场景中的点云物体。现有扩散模型生成的潜在特征很难由当前自动编码器有效解码成准确且类别一致的三维点云。

Method: 作者提出了Class-Partitioned Vector Quantized Variational Autoencoder（CPVQ-VAE），其引入了类别划分的码本（codebook），每类物体有专属码字，采用类别感知的运行均值更新策略解决码本崩溃问题。结合专为场景生成设计的Latent-space Flow Matching Model（LFMM）产生的物体特征和类别标注，通过CPVQ-VAE的类别感知反查流程，最终得到类别一致的点云物体。

Result: 在复杂客厅场景下，大量实验表明该方法生成的三维点云场景在Chamfer距离和Point2Mesh误差上分别降低了70.4%和72.3%，显著优于现有方法。

Conclusion: CPVQ-VAE方法能够有效地、直接地根据生成模型产生的特征和类别输出高质量的、类别一致的三维点云物体，摆脱了对外部物体数据库的依赖，为复杂三维场景生成提供了新途径。

Abstract: Most 3D scene generation methods are limited to only generating object bounding box parameters while newer diffusion methods also generate class labels and latent features. Using object size or latent feature, they then retrieve objects from a predefined database. For complex scenes of varied, multi-categorical objects, diffusion-based latents cannot be effectively decoded by current autoencoders into the correct point cloud objects which agree with target classes. We introduce a Class-Partitioned Vector Quantized Variational Autoencoder (CPVQ-VAE) that is trained to effectively decode object latent features, by employing a pioneering $\textit{class-partitioned codebook}$ where codevectors are labeled by class. To address the problem of $\textit{codebook collapse}$, we propose a $\textit{class-aware}$ running average update which reinitializes dead codevectors within each partition. During inference, object features and class labels, both generated by a Latent-space Flow Matching Model (LFMM) designed specifically for scene generation, are consumed by the CPVQ-VAE. The CPVQ-VAE's class-aware inverse look-up then maps generated latents to codebook entries that are decoded to class-specific point cloud shapes. Thereby, we achieve pure point cloud generation without relying on an external objects database for retrieval. Extensive experiments reveal that our method reliably recovers plausible point cloud scenes, with up to 70.4% and 72.3% reduction in Chamfer and Point2Mesh errors on complex living room scenes.

</details>


### [103] [Weaknesses of Facial Emotion Recognition Systems](https://arxiv.org/abs/2601.12402)
*Aleksandra Jamróz,Patrycja Wysocka,Piotr Garbat*

Main category: cs.CV

TL;DR: 本文回顾并分析了面部情感识别领域的主流方法与数据集，并通过实验证明了现有解决方案中存在的一些关键弱点。


<details>
  <summary>Details</summary>
Motivation: 面部情感识别对于人机交互非常重要。由于现有方法和数据集多样，作者有动力对最优秀的方法和数据集进行系统性的对比和分析。

Method: 作者选取了三种表现优秀的神经网络模型和三个具备代表性的大型情感图像数据集，对模型分别在不同数据集上进行训练和交叉测试，比较性能表现。

Result: 实验表明，不同数据集之间存在较大差异，模型在识别不同情感上的难易程度不一致，尤其在区分相近情感时表现较差，揭示了解决方案的弱点。

Conclusion: 面部情感识别模型在跨数据集泛化、处理难辨情感方面仍有待提升。未来应关注数据集标准化及提升模型区分细微表情的能力。

Abstract: Emotion detection from faces is one of the machine learning problems needed for human-computer interaction. The variety of methods used is enormous, which motivated an in-depth review of articles and scientific studies. Three of the most interesting and best solutions are selected, followed by the selection of three datasets that stood out for the diversity and number of images in them. The selected neural networks are trained, and then a series of experiments are performed to compare their performance, including testing on different datasets than a model was trained on. This reveals weaknesses in existing solutions, including differences between datasets, unequal levels of difficulty in recognizing certain emotions and the challenges in differentiating between closely related emotions.

</details>


### [104] [HOT-POT: Optimal Transport for Sparse Stereo Matching](https://arxiv.org/abs/2601.12423)
*Antonin Clerc,Michael Quellmalz,Moritz Piening,Philipp Flotho,Gregor Kornhardt,Gabriele Steidl*

Main category: cs.CV

TL;DR: 本文提出了一种基于最优传输（OT）视角的稀疏立体匹配方法，通过利用相机几何中的线性约束，实现了高效且无监督的人脸特征点匹配。


<details>
  <summary>Details</summary>
Motivation: 立体视觉在人脸分析、自动驾驶和机器人等领域面临遮挡、运动和相机畸变等难题。对于稀疏特征（如人脸关键点）匹配，参数敏感性进一步加大了问题难度，亟需鲁棒且高效的方法。

Method: 将相机投影点建模为半直线，分别利用经典的极线距离和三维射线距离评估匹配质量。将这些距离作为（部分）最优传输问题的代价函数，进而实现高效的匹配。此外，将无监督目标匹配问题建模为分层OT问题，扩展算法适用范围。

Result: 提出的算法能高效完成稀疏特征和目标的无监督匹配。实验主要针对人脸分析，验证了不同标注规范下的人脸关键点匹配能力。

Conclusion: 基于最优传输的稀疏匹配方法，能够有效、无监督地完成空间中的人脸特征点和目标匹配，在人脸分析等实际应用中具有良好表现。

Abstract: Stereo vision between images faces a range of challenges, including occlusions, motion, and camera distortions, across applications in autonomous driving, robotics, and face analysis. Due to parameter sensitivity, further complications arise for stereo matching with sparse features, such as facial landmarks. To overcome this ill-posedness and enable unsupervised sparse matching, we consider line constraints of the camera geometry from an optimal transport (OT) viewpoint. Formulating camera-projected points as (half)lines, we propose the use of the classical epipolar distance as well as a 3D ray distance to quantify matching quality. Employing these distances as a cost function of a (partial) OT problem, we arrive at efficiently solvable assignment problems. Moreover, we extend our approach to unsupervised object matching by formulating it as a hierarchical OT problem. The resulting algorithms allow for efficient feature and object matching, as demonstrated in our numerical experiments. Here, we focus on applications in facial analysis, where we aim to match distinct landmarking conventions.

</details>


### [105] [SkeFi: Cross-Modal Knowledge Transfer for Wireless Skeleton-Based Action Recognition](https://arxiv.org/abs/2601.12432)
*Shunyu Huang,Yunjiao Zhou,Jianfei Yang*

Main category: cs.CV

TL;DR: 本文提出了一种面向无线传感器（如LiDAR和mmWave）的骨骼动作识别方法，在数据稀缺和噪声高的挑战下，通过跨模态知识转移和增强时序建模，有效提升了无线传感器骨骼估计和动作识别的准确性，实验取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于骨骼的动作识别多依赖可见光摄像头，存在光照与隐私等局限，限制了其在居家、医院等环境的应用。因此，本文试图利用非侵入式无线传感器作为替代，但面临标注数据少、骨骼点噪声高的问题，有待解决。

Method: 提出SkeFi框架，通过从RGB模态跨模态知识迁移，并设计时序相关自适应图卷积TC-AGC和双重时序卷积，增强对噪声和帧丢失的鲁棒性，同时提升多尺度时序建模能力，实现对无线传感器数据的准确骨骼与动作识别。

Result: 在mmWave和LiDAR数据集上，实验结果表明SkeFi取得了当前最优的识别准确率，显著优于现有方法。

Conclusion: SkeFi突破了无线传感器骨骼动作识别在数据稀缺和高噪声条件下的核心难点，为智能家居等场景下的隐私友好型动作识别提供了有效方案。

Abstract: Skeleton-based action recognition leverages human pose keypoints to categorize human actions, which shows superior generalization and interoperability compared to regular end-to-end action recognition. Existing solutions use RGB cameras to annotate skeletal keypoints, but their performance declines in dark environments and raises privacy concerns, limiting their use in smart homes and hospitals. This paper explores non-invasive wireless sensors, i.e., LiDAR and mmWave, to mitigate these challenges as a feasible alternative. Two problems are addressed: (1) insufficient data on wireless sensor modality to train an accurate skeleton estimation model, and (2) skeletal keypoints derived from wireless sensors are noisier than RGB, causing great difficulties for subsequent action recognition models. Our work, SkeFi, overcomes these gaps through a novel cross-modal knowledge transfer method acquired from the data-rich RGB modality. We propose the enhanced Temporal Correlation Adaptive Graph Convolution (TC-AGC) with frame interactive enhancement to overcome the noise from missing or inconsecutive frames. Additionally, our research underscores the effectiveness of enhancing multiscale temporal modeling through dual temporal convolution. By integrating TC-AGC with temporal modeling for cross-modal transfer, our framework can extract accurate poses and actions from noisy wireless sensors. Experiments demonstrate that SkeFi realizes state-of-the-art performances on mmWave and LiDAR. The code is available at https://github.com/Huang0035/Skefi.

</details>


### [106] [Adversarial Defense in Vision-Language Models: An Overview](https://arxiv.org/abs/2601.12443)
*Xiaowei Fu,Lei Zhang*

Main category: cs.CV

TL;DR: 本文综述了面向视觉-语言模型（VLMs）如CLIP的对抗攻击防御方法，探讨了训练期防御、测试期自适应防御及免训练防御三大范式及其优缺点。


<details>
  <summary>Details</summary>
Motivation: VLMs的广泛应用引发了关于其易受高级且难以察觉的对抗攻击的担忧，这威胁到模型性能及跨模态系统的安全。为此，有必要系统梳理当前主流防御思路及其挑战。

Method: 作者从文献综述角度，对三种主流防御策略进行分类讨论：1）训练期防御：通过对抗微调增强模型鲁棒性；2）测试期自适应防御：推理时动态更新模型参数以应对未知攻击；3）免训练防御：不改变模型，仅调整输入或其特征嵌入应对攻击。

Result: 本文总结了不同防御方法在鲁棒性、通用性、计算资源消耗等方面的表现，指出各类方法的优势与不足。

Conclusion: 三大防御策略在实际应用中各有利弊，VLMs防御依然面临诸多挑战，有待进一步优化以提升模型在复杂场景下的安全性与鲁棒性。

Abstract: The widespread use of Vision Language Models (VLMs, e.g. CLIP) has raised concerns about their vulnerability to sophisticated and imperceptible adversarial attacks. These attacks could compromise model performance and system security in cross-modal tasks. To address this challenge, three main defense paradigms have been proposed: Training-time Defense, Test-time Adaptation Defense, and Training-free Defense. Training-time Defense involves modifying the training process, typically through adversarial fine-tuning to improve the robustness to adversarial examples. While effective, this approach requires substantial computational resources and may not generalize across all adversarial attacks. Test-time Adaptation Defense focuses on adapting the model at inference time by updating its parameters to handle unlabeled adversarial examples, offering flexibility but often at the cost of increased complexity and computational overhead. Training-free Defense avoids modifying the model itself, instead focusing on altering the adversarial inputs or their feature embeddings, which enforces input perturbations to mitigate the impact of attacks without additional training. This survey reviews the latest advancements in adversarial defense strategies for VLMs, highlighting the strengths and limitations of such approaches and discussing ongoing challenges in enhancing the robustness of VLMs.

</details>


### [107] [Large-scale EM Benchmark for Multi-Organelle Instance Segmentation in the Wild](https://arxiv.org/abs/2601.12464)
*Yanrui Lu,Danyang Chen,Haowen Xiao,Jiarui Zhu,Fukang Ge,Binqian Zou,Jiali Guan,Jiayin Liang,Yuting Wang,Ziqian Guan,Xiangcheng Bao,Jinhao Bi,Lin Gu,Jun He,Yingying Zhu*

Main category: cs.CV

TL;DR: 论文提出了一个更大规模、更具多样性的多细胞器实例分割基准，解决现有基准数据集规模小、异质性弱，导致模型泛化能力弱的问题，并发现主流分割模型在复杂EM数据和全局结构分割上表现差。


<details>
  <summary>Details</summary>
Motivation: 现有EM图像细胞器分割的基准数据集规模有限，且多为精心挑选的小数据，不足以反映真实情况下的细胞器多样性与结构复杂性，从而限制了相关模型的性能和泛化能力。

Method: 作者构建了一个包含十万余张多来源2D EM图像、涵盖多类型细胞和五类细胞器的大规模基准数据集，并设计了三维连通性感知的标签传播算法自动标注，结合人工修正。利用该基准，系统性评测了U-Net、SAM变体、Mask2Former等主流模型的性能。

Result: 实验结果表明，当前主流分割模型在多样性高和空间结构复杂的EM数据上泛化能力差，尤其对分布广泛、结构连贯的细胞器（如内质网）分割效果较差。

Conclusion: 论文指出，目前以局部为主的分割模型与需要建模全局长程结构连续性的EM数据之间存在根本不匹配，需开发能够处理高度变异和长程结构关联的新一代分割方法。所提基准数据集和工具即将公开发布，可推动该领域发展。

Abstract: Accurate instance-level segmentation of organelles in electron microscopy (EM) is critical for quantitative analysis of subcellular morphology and inter-organelle interactions. However, current benchmarks, based on small, curated datasets, fail to capture the inherent heterogeneity and large spatial context of in-the-wild EM data, imposing fundamental limitations on current patch-based methods. To address these limitations, we developed a large-scale, multi-source benchmark for multi-organelle instance segmentation, comprising over 100,000 2D EM images across variety cell types and five organelle classes that capture real-world variability. Dataset annotations were generated by our designed connectivity-aware Label Propagation Algorithm (3D LPA) with expert refinement. We further benchmarked several state-of-the-art models, including U-Net, SAM variants, and Mask2Former. Our results show several limitations: current models struggle to generalize across heterogeneous EM data and perform poorly on organelles with global, distributed morphologies (e.g., Endoplasmic Reticulum). These findings underscore the fundamental mismatch between local-context models and the challenge of modeling long-range structural continuity in the presence of real-world variability. The benchmark dataset and labeling tool will be publicly released soon.

</details>


### [108] [DCAC: Dynamic Class-Aware Cache Creates Stronger Out-of-Distribution Detectors](https://arxiv.org/abs/2601.12468)
*Yanqi Wu,Qichao Chen,Runhe Lai,Xinhua Lu,Jia-Xin Zhuang,Zhilin Zhao,Wei-Shi Zheng,Ruixuan Wang*

Main category: cs.CV

TL;DR: 该论文提出了DCAC（Dynamic Class-Aware Cache），一种训练时无改动、仅在推理阶段使用的新型校准机制，有效提高了神经网络对分布外样本（OOD）的检测能力。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在分布外（OOD）样本上容易产生过度自信的错误预测，尤其在未见过的数据上，表现出较差的检测能力。作者发现被模型判为同一类别的OOD样本，在视觉上相互比与真实分布内（ID）样本更相似。该现象启发他们开发一种能利用类别差异进行动态调节的新技术。

Method: 提出了一种动态类别感知缓存（DCAC）方法。该方法在推理阶段为每个ID类别维护一个缓存，专门收集高熵样本，并结合缓存的视觉特征与预测概率，通过一个轻量两层模块对输入样本的原始预测结果进行校准。这一方法可嵌入现有的OOD检测方法，并适用于单模态及视觉-语言模型，计算开销极小。

Result: 在多个OOD基准上进行了大量实验，验证了DCAC能显著提升现有方法性能。例如，将DCAC与ASH-S方法结合，在ImageNet OOD测试集上FPR95指标降低了6.55%。

Conclusion: DCAC是一种高效、灵活的推理阶段校准模块，能够减轻对OOD样本过度自信的错误判别，在不增加明显计算负担的情况下有效提升OOD检测性能，并易于集成至各种现有模型。

Abstract: Out-of-distribution (OOD) detection remains a fundamental challenge for deep neural networks, particularly due to overconfident predictions on unseen OOD samples during testing. We reveal a key insight: OOD samples predicted as the same class, or given high probabilities for it, are visually more similar to each other than to the true in-distribution (ID) samples. Motivated by this class-specific observation, we propose DCAC (Dynamic Class-Aware Cache), a training-free, test-time calibration module that maintains separate caches for each ID class to collect high-entropy samples and calibrate the raw predictions of input samples. DCAC leverages cached visual features and predicted probabilities through a lightweight two-layer module to mitigate overconfident predictions on OOD samples. This module can be seamlessly integrated with various existing OOD detection methods across both unimodal and vision-language models while introducing minimal computational overhead. Extensive experiments on multiple OOD benchmarks demonstrate that DCAC significantly enhances existing methods, achieving substantial improvements, i.e., reducing FPR95 by 6.55% when integrated with ASH-S on ImageNet OOD benchmark.

</details>


### [109] [NeuralFur: Animal Fur Reconstruction From Multi-View Images](https://arxiv.org/abs/2601.12481)
*Vanessa Sklyarova,Berna Kabadayi,Anastasios Yiannakidis,Giorgio Becherini,Michael J. Black,Justus Thies*

Main category: cs.CV

TL;DR: 本文提出了一种基于多视角图像和视觉语言模型（VLM）的高保真动物毛发三维重建方法，可适用于多种不同类型毛发的动物。


<details>
  <summary>Details</summary>
Motivation: 动物毛发的三维重建面临诸多挑战，如细微结构、自遮挡和视角相关的外观变化。此外，与人类发型重建不同，目前缺乏能够为不同动物学习毛发先验的数据集。因此亟需克服数据和方法上的局限，实现动物毛发的高质量建模。

Method: 方法首先利用多视角立体视觉技术重建动物的粗略表面几何。随后，借助视觉语言模型（VLM）获取动物身体各部分真实毛发长度的信息，结合这些知识生成无毛表面并在其上生长毛发束。多视角图像的几何和光度损失共同监督重建过程。为解决输入图像Gabor滤波后存在的毛发方向不明确问题，进一步用VLM指导毛发生长的方向及其与重力的相对关系，并以此构造额外的损失函数。

Result: 通过上述新颖框架，实验结果表明，该方法能够适用于多种具有不同毛发类型的动物，通用性和重建精度较高。

Conclusion: 本文首次提出结合多视角输入与VLM，进行动物毛发三维重建，突破了先验缺失与方向歧义等难题，具备较强的泛化能力和实际应用前景。

Abstract: Reconstructing realistic animal fur geometry from images is a challenging task due to the fine-scale details, self-occlusion, and view-dependent appearance of fur. In contrast to human hairstyle reconstruction, there are also no datasets that can be leveraged to learn a fur prior for different animals. In this work, we present a first multi-view-based method for high-fidelity 3D fur modeling of animals using a strand-based representation, leveraging the general knowledge of a vision language model. Given multi-view RGB images, we first reconstruct a coarse surface geometry using traditional multi-view stereo techniques. We then use a vision language model (VLM) system to retrieve information about the realistic length structure of the fur for each part of the body. We use this knowledge to construct the animal's furless geometry and grow strands atop it. The fur reconstruction is supervised with both geometric and photometric losses computed from multi-view images. To mitigate orientation ambiguities stemming from the Gabor filters that are applied to the input images, we additionally utilize the VLM to guide the strands' growth direction and their relation to the gravity vector that we incorporate as a loss. With this new schema of using a VLM to guide 3D reconstruction from multi-view inputs, we show generalization across a variety of animals with different fur types. For additional results and code, please refer to https://neuralfur.is.tue.mpg.de.

</details>


### [110] [Histopath-C: Towards Realistic Domain Shifts for Histopathology Vision-Language Adaptation](https://arxiv.org/abs/2601.12493)
*Mehrdad Noori,Gustavo Adolfo Vargas Hakim,David Osowiechi,Fereshteh Shakeri,Ali Bahri,Moslem Yazdanpanah,Sahar Dastani,Ismail Ben Ayed,Christian Desrosiers*

Main category: cs.CV

TL;DR: 本文提出了一个全新基准Histopath-C，专为模拟真实组织病理图像中的数据分布偏移，结合动态加噪框架与新型模型适应方法LATTE，显著提升了医学视觉语言模型对异常及多样化输入的稳健性。


<details>
  <summary>Details</summary>
Motivation: 尽管医学视觉语言模型（VLMs）在组织病理领域表现出色，但真实环境下图像常有染色、污染、模糊等域偏移问题，导致模型下游任务性能大幅下降，亟需解决其鲁棒性不足的问题。

Method: 1）构建了Histopath-C基准集，含多种仿真真实分布偏移的合成扰动，支持动态处理不同数据集；2）提出LATTE方法，即通过多文本模板的低秩转导自适应，提升模型对文本输入和域偏移的适应能力，同时在测试时动态适应。

Result: LATTE在多个组织病理数据集上超越现有SOTA测试时自适应（TTA）方法，尤其是在设计初针对自然图像模型的场景下，仍展现出优异且更为稳健的性能。

Conclusion: Histopath-C基准及LATTE方法有效缓解了医学VLMs的域偏移问题，有助于推动组织病理学图像分析的实际应用，提升模型在真实环境下的适应和泛化能力。

Abstract: Medical Vision-language models (VLMs) have shown remarkable performances in various medical imaging domains such as histo\-pathology by leveraging pre-trained, contrastive models that exploit visual and textual information. However, histopathology images may exhibit severe domain shifts, such as staining, contamination, blurring, and noise, which may severely degrade the VLM's downstream performance. In this work, we introduce Histopath-C, a new benchmark with realistic synthetic corruptions designed to mimic real-world distribution shifts observed in digital histopathology. Our framework dynamically applies corruptions to any available dataset and evaluates Test-Time Adaptation (TTA) mechanisms on the fly. We then propose LATTE, a transductive, low-rank adaptation strategy that exploits multiple text templates, mitigating the sensitivity of histopathology VLMs to diverse text inputs. Our approach outperforms state-of-the-art TTA methods originally designed for natural images across a breadth of histopathology datasets, demonstrating the effectiveness of our proposed design for robust adaptation in histopathology images. Code and data are available at https://github.com/Mehrdad-Noori/Histopath-C.

</details>


### [111] [Video Individual Counting and Tracking from Moving Drones: A Benchmark and Methods](https://arxiv.org/abs/2601.12500)
*Yaowu Fan,Jia Wan,Tao Han,Andy J. Ma,Antoni B. Chan*

Main category: cs.CV

TL;DR: 本论文提出了基于移动无人机的视频级密集人群计数与跟踪方法，构建了大规模MovingDroneCrowd++数据集，并提出了GD3A和DVTrack算法，在复杂动态场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有密集人群计数与跟踪方法多依赖于固定摄像头，无法满足大规模复杂场景对广泛空间覆盖和动态观测的需求，因此需要更灵活且适应大场景的解决方案。

Method: 作者利用移动无人机采集大规模复杂场景下的人群视频，构建了MovingDroneCrowd++数据集。提出GD3A算法，通过密度图无显式定位地对个体进行计数，采用最优传输和自适应dustbin分数进行像素级描述子关联，将全局密度图分解为通用、流入、流出部分；在此基础上，提出DVTrack算法，通过描述子投票机制实现个体行人跟踪。

Result: 实验结果表明，GD3A和DVTrack在密集人群和复杂运动情况下，相比其他方法在该新数据集上分别降低了47.4%的计数误差，提升了39.2%的跟踪性能。

Conclusion: 基于移动无人机的视频采集和提出的新方法显著提升了大规模复杂密集人群的计数与跟踪能力，对大范围人群分析应用具有重要意义。

Abstract: Counting and tracking dense crowds in large-scale scenes is highly challenging, yet existing methods mainly rely on datasets captured by fixed cameras, which provide limited spatial coverage and are inadequate for large-scale dense crowd analysis. To address this limitation, we propose a flexible solution using moving drones to capture videos and perform video-level crowd counting and tracking of unique pedestrians across entire scenes. We introduce MovingDroneCrowd++, the largest video-level dataset for dense crowd counting and tracking captured by moving drones, covering diverse and complex conditions with varying flight altitudes, camera angles, and illumination. Existing methods fail to achieve satisfactory performance on this dataset. To this end, we propose GD3A (Global Density Map Decomposition via Descriptor Association), a density map-based video individual counting method that avoids explicit localization. GD3A establishes pixel-level correspondences between pedestrian descriptors across consecutive frames via optimal transport with an adaptive dustbin score, enabling the decomposition of global density maps into shared, inflow, and outflow components. Building on this framework, we further introduce DVTrack, which converts descriptor-level matching into instance-level associations through a descriptor voting mechanism for pedestrian tracking. Experimental results show that our methods significantly outperform existing approaches under dense crowds and complex motion, reducing counting error by 47.4 percent and improving tracking performance by 39.2 percent.

</details>


### [112] [SDCoNet: Saliency-Driven Multi-Task Collaborative Network for Remote Sensing Object Detection](https://arxiv.org/abs/2601.12507)
*Ruo Qi,Linhui Dai,Yusong Qin,Chaolei Yang,Yanshan Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为SDCoNet的显著性驱动多任务协作网络，同时提升遥感图像中超分辨和小目标检测的精度，在多个数据集上实现了领先效果。


<details>
  <summary>Details</summary>
Motivation: 遥感图像中小目标检测困难，原因包括复杂背景、目标信号弱、目标尺度小，且传统上单独进行超分辨与目标检测存在信息冗余和任务冲突问题。为了解决现有串联方法中超分辨与检测目标不一致、信息交互有限、特征冗余等痛点，本研究试图协同优化两者以提升整体检测表现。

Method: SDCoNet采用基于swin transformer的共享编码器，实现多任务间隐式特征共享，通过分层窗口自注意力机制增强纹理和语义特征交互。此外，设计了多尺度显著性预测模块，对关键区域进行聚焦，实现对弱目标的突出、背景干扰抑制，以及多任务耦合引入的不良特征的抑制。同时引入梯度路由策略，通过优先保障检测任务的特征稳定，并在此基础上”引导“超分辨的梯度优化方向，从而得到有利于检测的高频细节。

Result: 在多个公开遥感小目标检测数据集NWPU VHR-10-Split、DOTAv1.5-Split和HRSSD-Split上，SDCoNet在保证竞争性计算效率的基础上，显著优于现有主流算法，特别是在低质量图像中的小目标检测任务上取得了更好的精度表现。

Conclusion: SDCoNet通过显著性引导的多任务协作机制，提升了低质量遥感图像中小目标检测的性能，并验证了协同训练超分辨与目标检测的有效性，为该领域的目标检测研究提供了新的思路。

Abstract: In remote sensing images, complex backgrounds, weak object signals, and small object scales make accurate detection particularly challenging, especially under low-quality imaging conditions. A common strategy is to integrate single-image super-resolution (SR) before detection; however, such serial pipelines often suffer from misaligned optimization objectives, feature redundancy, and a lack of effective interaction between SR and detection. To address these issues, we propose a Saliency-Driven multi-task Collaborative Network (SDCoNet) that couples SR and detection through implicit feature sharing while preserving task specificity. SDCoNet employs the swin transformer-based shared encoder, where hierarchical window-shifted self-attention supports cross-task feature collaboration and adaptively balances the trade-off between texture refinement and semantic representation. In addition, a multi-scale saliency prediction module produces importance scores to select key tokens, enabling focused attention on weak object regions, suppression of background clutter, and suppression of adverse features introduced by multi-task coupling. Furthermore, a gradient routing strategy is introduced to mitigate optimization conflicts. It first stabilizes detection semantics and subsequently routes SR gradients along a detection-oriented direction, enabling the framework to guide the SR branch to generate high-frequency details that are explicitly beneficial for detection. Experiments on public datasets, including NWPU VHR-10-Split, DOTAv1.5-Split, and HRSSD-Split, demonstrate that the proposed method, while maintaining competitive computational efficiency, significantly outperforms existing mainstream algorithms in small object detection on low-quality remote sensing images. Our code is available at https://github.com/qiruo-ya/SDCoNet.

</details>


### [113] [Fine-Tuning Cycle-GAN for Domain Adaptation of MRI Images](https://arxiv.org/abs/2601.12512)
*Mohd Usama,Belal Ahmad,Faleh Menawer R Althiyabi*

Main category: cs.CV

TL;DR: 本文提出了一种基于Cycle-GAN的无监督医学影像领域自适应方法，有效提升了跨域MRI影像深度学习模型的表现，并实现了影像的内容完整性和诊断准确性的提升。


<details>
  <summary>Details</summary>
Motivation: MRI来自不同设备和机构的图像存在领域差异，导致仅在源域训练的深度学习模型在新域（如其它医院数据）上表现下降。为增强模型泛化能力，需要无监督领域自适应方法。

Method: 采用Cycle-GAN网络，无需成对标注数据，学习源域与目标域之间的双向映射。模型引入内容损失和差异损失，确保域适应同时保持影像解剖学内容。

Result: 在多个MRI数据集上进行实验，结果表明该方法实现了无标签数据下的双向领域适应，有效提升模型表现，减少领域相关变异。

Conclusion: 该方法在提高医疗影像分析一致性和准确性方面表现突出，为提升医学诊断质量提供了新的研究方向。

Abstract: Magnetic Resonance Imaging (MRI) scans acquired from different scanners or institutions often suffer from domain shifts owing to variations in hardware, protocols, and acquisition parameters. This discrepancy degrades the performance of deep learning models trained on source domain data when applied to target domain images. In this study, we propose a Cycle-GAN-based model for unsupervised medical-image domain adaptation. Leveraging CycleGANs, our model learns bidirectional mappings between the source and target domains without paired training data, preserving the anatomical content of the images. By leveraging Cycle-GAN capabilities with content and disparity loss for adaptation tasks, we ensured image-domain adaptation while maintaining image integrity. Several experiments on MRI datasets demonstrated the efficacy of our model in bidirectional domain adaptation without labelled data. Furthermore, research offers promising avenues for improving the diagnostic accuracy of healthcare. The statistical results confirm that our approach improves model performance and reduces domain-related variability, thus contributing to more precise and consistent medical image analysis.

</details>


### [114] [Deep Feature Deformation Weights](https://arxiv.org/abs/2601.12527)
*Richard Liu,Itai Lang,Rana Hanocka*

Main category: cs.CV

TL;DR: 本文提出了一种结合深度特征先验与传统网格变形的高效方法，可以实现实时、语义性强、控制精确的网格形变。


<details>
  <summary>Details</summary>
Motivation: 传统基于控制手柄的网格变形方法需要用户合理放置手柄，流程不直观且无语义信息；而数据驱动方法虽然有语义性，但速度慢且精度低。该工作旨在融合两者优点，解决用户难以直观操作和效率低下的问题。

Method: 提出利用深度特征距离来生成平滑且具有语义的变形权重，无需额外正则化。通过创新的重心特征蒸馏（barycentric feature distillation）流程，高效利用视觉信号并降低计算成本，使权重在任意表面点上可实时计算。此外，通过特征空间约束和局部加权扩展传统方法属性，并自动识别语义对称性，促成对称性保持的变形。

Result: 该方法可在1分钟内处理百万面高分辨率网格的权重计算，并能在消费级设备上实现实时变形。变形结果具有语义一致性，且置信度和控制精度较高。

Conclusion: 该方法兼顾了经典和数据驱动变形的优点，可以实现高效、语义丰富、自动对称的实时网格变形，在大规模网格和交互场景中具有广泛应用前景。

Abstract: Handle-based mesh deformation has been a long-standing paradigm in computer graphics, enabling intuitive shape edits from sparse controls. Classic techniques offer precise and rapid deformation control. However, they solve an optimization problem with constraints defined by control handle placement, requiring a user to know apriori the ideal distribution of handles on the shape to accomplish the desired edit. The mapping from handle set to deformation behavior is often unintuitive and, importantly, non-semantic. Modern data-driven methods, on the other hand, leverage a data prior to obtain semantic edits, but are slow and imprecise. We propose a technique that fuses the semantic prior of data with the precise control and speed of traditional frameworks. Our approach is surprisingly simple yet effective: deep feature proximity makes for smooth and semantic deformation weights, with no need for additional regularization. The weights can be computed in real-time for any surface point, whereas prior methods require optimization for new handles. Moreover, the semantic prior from deep features enables co-deformation of semantic parts. We introduce an improved feature distillation pipeline, barycentric feature distillation, which efficiently uses the visual signal from shape renders to minimize distillation cost. This allows our weights to be computed for high resolution meshes in under a minute, in contrast to potentially hours for both classical and neural methods. We preserve and extend properties of classical methods through feature space constraints and locality weighting. Our field representation allows for automatic detection of semantic symmetries, which we use to produce symmetry-preserving deformations. We show a proof-of-concept application which can produce deformations for meshes up to 1 million faces in real-time on a consumer-grade machine.

</details>


### [115] [XRefine: Attention-Guided Keypoint Match Refinement](https://arxiv.org/abs/2601.12530)
*Jan Fabian Schmid,Annika Hagemann*

Main category: cs.CV

TL;DR: 提出XRefine，一种与关键点检测器无关的亚像素关键点精炼方法，只需基于关键点中心的图像块，能显著提升3D视觉任务中的关键点匹配精度。


<details>
  <summary>Details</summary>
Motivation: 传统的关键点检测器常产生空间误差较大的关键点匹配。现有精炼方法多数与检测器强相关，无法通用，每更换检测器就需重新训练，应用不便。

Method: 提出了XRefine，一种跨注意力（cross-attention）架构的关键点精炼方法，仅利用已匹配关键点中心的图像小块进行坐标精炼，不依赖检测器的内部特征，可在不同检测器间泛化。其方法可扩展至多视图特征轨迹精炼。

Result: 在MegaDepth、KITTI和ScanNet等数据集上，XRefine均显著提升了几何估计精度，并优于现有主流精炼方法。同时，方法保持了高效的运行速度。

Conclusion: XRefine实现了与检测器无关的高效关键点精炼，兼备优异的精度和效率，方便在多种3D视觉场景与检测器上应用，具有良好的推广性和实用价值。

Abstract: Sparse keypoint matching is crucial for 3D vision tasks, yet current keypoint detectors often produce spatially inaccurate matches. Existing refinement methods mitigate this issue through alignment of matched keypoint locations, but they are typically detector-specific, requiring retraining for each keypoint detector. We introduce XRefine, a novel, detector-agnostic approach for sub-pixel keypoint refinement that operates solely on image patches centered at matched keypoints. Our cross-attention-based architecture learns to predict refined keypoint coordinates without relying on internal detector representations, enabling generalization across detectors. Furthermore, XRefine can be extended to handle multi-view feature tracks. Experiments on MegaDepth, KITTI, and ScanNet demonstrate that the approach consistently improves geometric estimation accuracy, achieving superior performance compared to existing refinement methods while maintaining runtime efficiency. Our code and trained models can be found at https://github.com/boschresearch/xrefine.

</details>


### [116] [BirdsEye-RU: A Dataset For Detecting Faces from Overhead Images](https://arxiv.org/abs/2601.12533)
*Md. Ahanaf Arif Khan,Ariful Islam,Sangeeta Biswas,Md. Iqbal Aziz Khan,Subrata Pramanik,Sanjoy Kumar Chakrabarty,Bimal Kumar Pramanik*

Main category: cs.CV

TL;DR: 本文提出了BirdsEye-RU数据集，用于检测高空图像中的人脸，涵盖极小和远距离人脸，包含2978张图片和8000多个标注人脸。


<details>
  <summary>Details</summary>
Motivation: 高空拍摄的人脸检测难度大，主要是由于尺度变化大和环境复杂，现有数据集无法很好地解决此问题，因此需要专门的数据集来推动这方面的研究。

Method: 作者构建了BirdsEye-RU数据集，收集了来自无人机和高空手机拍摄的图片，并对其中的人脸进行了详细标注，以提供全面、真实的测试数据。

Result: BirdsEye-RU数据集包含2978张涵盖不同高度、场景的图像，以及超过8000个人脸标注，充分反映了高空拍摄下人脸的尺度和环境多样性。

Conclusion: BirdsEye-RU数据集为高空图像中的人脸检测研究提供了重要资源，提高了在极端尺度和复杂环境下检测小、远距离人脸的能力。

Abstract: Detecting faces in overhead images remains a significant challenge due to extreme scale variations and environmental clutter. To address this, we created the BirdsEye-RU dataset, a comprehensive collection of 2,978 images containing over eight thousand annotated faces. This dataset is specifically designed to capture small and distant faces across diverse environments, containing both drone images and smartphone-captured images from high altitude. We present a detailed description of the BirdsEye-RU dataset in this paper. We made our dataset freely available to the public, and it can be accessed at https://www.kaggle.com/datasets/mdahanafarifkhan/birdseye-ru.

</details>


### [117] [Encoding Emotion Through Self-Supervised Eye Movement Reconstruction](https://arxiv.org/abs/2601.12534)
*Marcus Ma,Jordan Prescott,Emily Zhou,Tiantian Feng,Kleanthis Avramidis,Gabor Mihaly Toth,Shrikanth Narayanan*

Main category: cs.CV

TL;DR: 本论文提出了一种新的自监督眼动重建模型，可以在低分辨率自然视频中，从眼动信息预测情感表达，取得了良好的效果。


<details>
  <summary>Details</summary>
Motivation: 现有关于眼动与情感表达关系的研究多依赖高分辨率、专业的眼动仪器，限制了研究成果的广泛应用。本研究旨在探索如何在自然、低分辨率的视频环境下，用眼动信息预测情感表达，为无标签视频数据的情感识别提供新方法。

Method: 作者以大规模口述历史视频（主要为大屠杀幸存者的访谈）为数据基础，提出了一种自监督学习的眼动重建模型，通过无监督方式对眼动轨迹进行编码预训练，再将其作为下游任务（眼动与情感语音对齐、预测笑/哭泣/叹气等临时情感行为）的输入特征进行微调和测试。

Result: 该模型在上述下游任务中展现出良好的情感预测性能，同时发现眼动重建预训练表现越好，对情感任务的预测效果也越佳。

Conclusion: 自监督的眼动重建方法能够有效编码眼动所承载的情感信号，在低分辨率自然视频上也有广泛的应用潜力，为后续情感识别研究提供了有价值的新工具。

Abstract: The relationship between emotional expression and eye movement is well-documented, with literature establishing gaze patterns are reliable indicators of emotion. However, most studies utilize specialized, high-resolution eye-tracking equipment, limiting the potential reach of findings. We investigate how eye movement can be used to predict multimodal markers of emotional expression from naturalistic, low-resolution videos. We utilize a collection of video interviews from the USC Shoah Foundation's Visual History Archive with Holocaust survivors as they recount their experiences in the Auschwitz concentration camp. Inspired by pretraining methods on language models, we develop a novel gaze detection model that uses self-supervised eye movement reconstruction that can effectively leverage unlabeled video. We use this model's encoder embeddings to fine-tune models on two downstream tasks related to emotional expression. The first is aligning eye movement with directional emotion estimates from speech. The second task is using eye gaze as a predictor of three momentary manifestations of emotional behaviors: laughing, crying/sobbing, and sighing. We find our new model is predictive of emotion outcomes and observe a positive correlation between pretraining performance and emotion processing performance for both experiments. We conclude self-supervised eye movement reconstruction is an effective method for encoding the affective signal they carry.

</details>


### [118] [PISE: Physics-Anchored Semantically-Enhanced Deep Computational Ghost Imaging for Robust Low-Bandwidth Machine Perception](https://arxiv.org/abs/2601.12551)
*Tong Wu*

Main category: cs.CV

TL;DR: 本文提出了一种名为PISE的物理约束深度鬼成像框架，用于低带宽的边缘感知，并在高稀疏采样下提升了感知性能。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上，由于带宽受限，传统成像方法难以兼顾数据量和感知精度。因此需要发展在极少采样率下仍能保持高感知性能的成像算法。

Method: PISE结合物理信息与深度学习，通过引入伴随算子初始化，并融合语义引导，使得模型在鬼成像任务中既保留物理一致性，又兼顾深度网络的表征能力。

Result: 在5%的采样率下，PISE提升了2.57%的分类准确率，并将方差降低了9倍，远超传统方法。

Conclusion: PISE在极低样本率下显著提升了边缘感知的准确率和稳定性，为低带宽场景下的高效感知提供了新方案。

Abstract: We propose PISE, a physics-informed deep ghost imaging framework for low-bandwidth edge perception. By combining adjoint operator initialization with semantic guidance, PISE improves classification accuracy by 2.57% and reduces variance by 9x at 5% sampling.

</details>


### [119] [Camera Pose Revisited](https://arxiv.org/abs/2601.12567)
*Władysław Skarbek,Michał Salomonowicz,Michał Król*

Main category: cs.CV

TL;DR: 本文提出了一种新的高效平面PnP（Perspective-n-Point）姿态估计算法PnP-ProCay78，结合 Cayley 旋转参数化与最小二乘优化，通过实验验证该方法在多种摄像头配置下表现优异，且结构简洁高效。


<details>
  <summary>Details</summary>
Motivation: 在多传感器系统和相机标定领域，准确高效地估计摄像头的位姿（位置和朝向）至关重要。现有PnP算法虽然精度高，但往往计算复杂或需要大量搜索。因此，研究一种精确且结构简单的新算法具有实际需求。

Method: 提出PnP-ProCay78算法，将重建误差的二次型公式与Cayley旋转参数化、最小二乘优化相结合。特别设计了基于重建误差对两个典型向量进行解析分析的确定性起点选择，避免了高成本的解空间搜索。算法在RGB与低分辨率热成像摄像头的组合系统上进行了实验测试。

Result: 与主流的SQPnP和IPPE算法相比，PnP-ProCay78在投影精度上基本相当或略优于IPPE，且算法结构更为简单。同时，对Cayley空间优化轨迹的分析有助于解释收敛过程。

Conclusion: PnP-ProCay78算法兼具高精度与高简洁性，实现了投影误差最小化并用解析消元的方法优化平移估计，具有良好的几何可解释性和计算效率，对实际系统开发与教学均有应用价值。

Abstract: Estimating the position and orientation of a camera with respect to an observed scene is one of the central problems in computer vision, particularly in the context of camera calibration and multi-sensor systems. This paper addresses the planar Perspective--$n$--Point problem, with special emphasis on the initial estimation of the pose of a calibration object. As a solution, we propose the \texttt{PnP-ProCay78} algorithm, which combines the classical quadratic formulation of the reconstruction error with a Cayley parameterization of rotations and least-squares optimization. The key component of the method is a deterministic selection of starting points based on an analysis of the reconstruction error for two canonical vectors, allowing costly solution-space search procedures to be avoided. Experimental validation is performed using data acquired also from high-resolution RGB cameras and very low-resolution thermal cameras in an integrated RGB--IR setup. The results demonstrate that the proposed algorithm achieves practically the same projection accuracy as optimal \texttt{SQPnP} and slightly higher than \texttt{IPPE}, both prominent \texttt{PnP-OpenCV} procedures. However, \texttt{PnP-ProCay78} maintains a significantly simpler algorithmic structure. Moreover, the analysis of optimization trajectories in Cayley space provides an intuitive insight into the convergence process, making the method attractive also from a didactic perspective. Unlike existing PnP solvers, the proposed \texttt{PnP-ProCay78} algorithm combines projection error minimization with an analytically eliminated reconstruction-error surrogate for translation, yielding a hybrid cost formulation that is both geometrically transparent and computationally efficient.

</details>


### [120] [Linear Mechanisms for Spatiotemporal Reasoning in Vision Language Models](https://arxiv.org/abs/2601.12626)
*Raphi Kang,Hongqiao Chen,Georgia Gkioxari,Pietro Perona*

Main category: cs.CV

TL;DR: 论文分析了视觉语言模型(VLM)在时空推理能力中的内部机制，发现其通过线性绑定空间ID和文本激活来编码空间信息，并提出该机制有助于解释模型的推理过程。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型具有强大的时空推理能力，但其内部如何融合视觉/几何与文本表征以实现推理仍不透明。作者希望揭示VLM进行空间结构推理时视觉与文本特征的融合点，并分析其因果作用。

Method: 通过实证分析，作者研究了VLM是否通过将‘空间ID’线性绑定到文本激活中以编码对象位置，并通过综合的因果干预实验，探究这些ID在模型各层的作用。同时将分析扩展到视频VLM，提出类似的时序ID机制。

Result: 实验证明，VLM普遍采用空间ID机制对对象位置进行线性绑定，并通过语言tokens进行推理。这些ID在模型中广泛存在，能够系统性地干预和调节模型中间层的信念，也可用作诊断或学习信号。

Conclusion: 通过明确空间和时间ID机制，论文揭示了VLM未被充分探索的内部推理过程，为更可解释和更优模型设计提供了理论基础。文中还开源了相关代码以促进复现。

Abstract: Spatio-temporal reasoning is a remarkable capability of Vision Language Models (VLMs), but the underlying mechanisms of such abilities remain largely opaque. We postulate that visual/geometrical and textual representations of spatial structure must be combined at some point in VLM computations. We search for such confluence, and ask whether the identified representation can causally explain aspects of input-output model behavior through a linear model. We show empirically that VLMs encode object locations by linearly binding \textit{spatial IDs} to textual activations, then perform reasoning via language tokens. Through rigorous causal interventions we demonstrate that these IDs, which are ubiquitous across the model, can systematically mediate model beliefs at intermediate VLM layers. Additionally, we find that spatial IDs serve as a diagnostic tool for identifying limitations in existing VLMs, and as a valuable learning signal. We extend our analysis to video VLMs and identify an analogous linear temporal ID mechanism. By characterizing our proposed spatiotemporal ID mechanism, we elucidate a previously underexplored internal reasoning process in VLMs, toward improved interpretability and the principled design of more aligned and capable models. We release our code for reproducibility: https://github.com/Raphoo/linear-mech-vlms.

</details>


### [121] [From Bands to Depth: Understanding Bathymetry Decisions on Sentinel-2](https://arxiv.org/abs/2601.12636)
*Satyaki Roy Chowdhury,Aswathnarayan Radhakrishnan,Hsiao Jou Hsu,Hari Subramoni,Joachim Moortgat*

Main category: cs.CV

TL;DR: 该论文分析了Swin-Transformer与U-Net结合的深度学习模型（Swin-BathyUNet）在卫星遥感水深推断中的表现，并提出方法增强模型可解释性和跨区域适用性的建议。


<details>
  <summary>Details</summary>
Motivation: 现有卫星反演水深（SDB）模型在不同区域泛化和可靠性方面存在不足，需要理解模型对不同光谱波段的依赖及其预测的信任度，为实际应用和模型改进提供依据。

Method: 1. 将Swin-Transformer与U-Net结合，并用于SDB任务。
2. 通过“leave-one-band-out”方法，分析各光谱波段对推断浅水区水深的重要性。
3. 将消融解释方法A-CAM-R应用到回归任务，并通过性能保持测试验证解释有效性。
4. 研究不同注意力机制对鲁棒性提升作用，特别是对海面反射和白沫的影响。
5. 跨区域测试，分析模型泛化误差随深度的变化。

Result: 1. 提出了Swin-BathyUNet，模型在浅水反演的光谱依赖与物理一致。
2. 证明顶级显著像素对模型预测贡献大。
3. 注意力机制提升了模型对干扰的鲁棒性。
4. 跨区域推理时，深度增加导致误差线性上升，且深水区误差放大。

Conclusion: 建议保持大感受野、重点保护绿蓝通道的影像质量、过滤沿岸噪声，同时通过微调和深度敏感校准提升模型跨区适应性。

Abstract: Deploying Sentinel-2 satellite derived bathymetry (SDB) robustly across sites remains challenging. We analyze a Swin-Transformer based U-Net model (Swin-BathyUNet) to understand how it infers depth and when its predictions are trustworthy. A leave-one-band out study ranks spectral importance to the different bands consistent with shallow water optics. We adapt ablation-based CAM to regression (A-CAM-R) and validate the reliability via a performance retention test: keeping only the top-p% salient pixels while neutralizing the rest causes large, monotonic RMSE increase, indicating explanations localize on evidence the model relies on. Attention ablations show decoder conditioned cross attention on skips is an effective upgrade, improving robustness to glint/foam. Cross-region inference (train on one site, test on another) reveals depth-dependent degradation: MAE rises nearly linearly with depth, and bimodal depth distributions exacerbate mid/deep errors. Practical guidance follows: maintain wide receptive fields, preserve radiometric fidelity in green/blue channels, pre-filter bright high variance near shore, and pair light target site fine tuning with depth aware calibration to transfer across regions.

</details>


### [122] [Mixed Precision PointPillars for Efficient 3D Object Detection with TensorRT](https://arxiv.org/abs/2601.12638)
*Ninnart Fuengfusin,Keisuke Yoneda,Naoki Suganuma*

Main category: cs.CV

TL;DR: 本文提出了一种面向LIDAR 3D目标检测任务的混合精度量化框架，通过智能选择量化层级和利用极少校准数据，提升模型推理速度同时缓解性能下降问题。


<details>
  <summary>Details</summary>
Motivation: LIDAR 3D目标检测需要满足自动驾驶的实时性要求，普通量化方法虽能提升速度，但由于LIDAR特有的数据分布和异常值容易引发明显性能下降，因此需要更针对性的量化策略。

Method: 提出在PointPillars架构下，采用后训练量化(PTQ)一层层地识别对性能更敏感的网络层，将这些层保留为浮点（FP），其余使用8位整型（INT8）；同时利用贪心搜索找出最佳混合精度组合，并可选用PTQ或量化感知训练（QAT）进一步优化。此外，通过极少量校准数据降低异常值对PTQ的不良影响，提高量化后模型表现。

Result: 实现了无需训练即可得到性能较优的混合精度PTQ模型，并在QAT管线下达到接近全浮点模型的检测精度，经TensorRT部署后，模型延迟和体积分别减少2.35倍和2.26倍。

Conclusion: 该混合精度量化方法有效兼顾了LIDAR 3D目标检测的实时性和准确率，特别适用于资源受限的自动驾驶场景。

Abstract: LIDAR 3D object detection is one of the important tasks for autonomous vehicles. Ensuring that this task operates in real-time is crucial. Toward this, model quantization can be used to accelerate the runtime. However, directly applying model quantization often leads to performance degradation due to LIDAR's wide numerical distributions and extreme outliers. To address the wide numerical distribution, we proposed a mixed precision framework designed for PointPillars. Our framework first searches for sensitive layers with post-training quantization (PTQ) by quantizing one layer at a time to 8-bit integer (INT8) and evaluating each model for average precision (AP). The top-k most sensitive layers are assigned as floating point (FP). Combinations of these layers are greedily searched to produce candidate mixed precision models, which are finalized with either PTQ or quantization-aware training (QAT). Furthermore, to handle outliers, we observe that using a very small number of calibration data reduces the likelihood of encountering outliers, thereby improving PTQ performance. Our methods provides mixed precision models without training in the PTQ pipeline, while our QAT pipeline achieves the performance competitive to FP models. With TensorRT deployment, our models offer less latency and sizes by up to 2.35 and 2.26 times, respectively.

</details>


### [123] [Generalizable Hyperparameter Optimization for Federated Learning on Non-IID Cancer Images](https://arxiv.org/abs/2601.12664)
*Elisa Gonçalves Ribeiro,Rodrigo Moreira,Larissa Ferreira Rodrigues Moreira,André Ricardo Backes*

Main category: cs.CV

TL;DR: 本论文探讨了在隐私保护要求下联邦学习在癌症病理图像领域超参数配置的泛化能力，并提出了跨数据集的简单配置聚合方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习在癌症病理图像分析中很有前景，但临床数据隐私限制了集中式训练。联邦学习有助于数据不出本地，但在数据分布异质（non-IID）时，性能强依赖超参数配置。因此，作者旨在探索超参数在非独立同分布场景下的泛化能力，并简化其选择。

Method: 作者采用集中式贝叶斯优化，在卵巢癌和结直肠癌二分类任务上分别找出最优超参数组合，然后将其直接迁移到非IID的联邦学习设置。同时，作者创新性地提出通过对不同数据集的学习率取平均，优化器和批量大小则采用众数的方式，组合出跨数据集的聚合配置。

Result: 聚合后的超参数配置在分类任务上取得了与针对单个数据集专门调优相媲美的竞争性性能。

Conclusion: 通过简单的跨数据集超参数聚合方法，可以在非IID联邦学习中获得稳健且竞争力的模型表现，减少昂贵的超参数调优需求。

Abstract: Deep learning for cancer histopathology training conflicts with privacy constraints in clinical settings. Federated Learning (FL) mitigates this by keeping data local; however, its performance depends on hyperparameter choices under non-independent and identically distributed (non-IID) client datasets. This paper examined whether hyperparameters optimized on one cancer imaging dataset generalized across non-IID federated scenarios. We considered binary histopathology tasks for ovarian and colorectal cancers. We perform centralized Bayesian hyperparameter optimization and transfer dataset-specific optima to the non-IID FL setup. The main contribution of this study is the introduction of a simple cross-dataset aggregation heuristic by combining configurations by averaging the learning rates and considering the modal optimizers and batch sizes. This combined configuration achieves a competitive classification performance.

</details>


### [124] [Near-Light Color Photometric Stereo for mono-Chromaticity non-lambertian surface](https://arxiv.org/abs/2601.12666)
*Zonglin Li,Jieji Ren,Shuangfan Zhou,Heng Guo,Jinnuo Zhang,Jiang Zhou,Boxin Shi,Zhanyu Ma,Guoying Gu*

Main category: cs.CV

TL;DR: 本文提出了一种基于神经隐式表示的彩色光度立体方法，实现了在单张图像下的精细表面重建，并通过紧凑型光学触觉传感器在合成和真实数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的彩色光度立体方法多假设远距离理想光照和Lambertian反射，不适用于真实环境中的近光照及非Lambertian表面，限制了其实际应用。因此需要更通用的方法来克服这些局限。

Method: 本文利用神经隐式表示对景深和BRDF进行建模，在假定单色性（单一色度和均匀材质）的前提下，缓解了彩色光度立体中的不适定性；方法只需一张图像即可实现高细节的表面恢复。此外，设计并使用了紧凑型光学触觉传感器进行实验和验证。

Result: 在合成数据和真实世界数据集上的实验表明，该方法能够实现准确和鲁棒的表面重建。

Conclusion: 本文提出的方法无需多幅图像或严格光照条件，能在近光照、非理想表面下单帧实现高质量表面重建，扩展了彩色光度立体的适用场景，具有广泛应用前景。

Abstract: Color photometric stereo enables single-shot surface reconstruction, extending conventional photometric stereo that requires multiple images of a static scene under varying illumination to dynamic scenarios. However, most existing approaches assume ideal distant lighting and Lambertian reflectance, leaving more practical near-light conditions and non-Lambertian surfaces underexplored. To overcome this limitation, we propose a framework that leverages neural implicit representations for depth and BRDF modeling under the assumption of mono-chromaticity (uniform chromaticity and homogeneous material), which alleviates the inherent ill-posedness of color photometric stereo and allows for detailed surface recovery from just one image. Furthermore, we design a compact optical tactile sensor to validate our approach. Experiments on both synthetic and real-world datasets demonstrate that our method achieves accurate and robust surface reconstruction.

</details>


### [125] [Exploiting Test-Time Augmentation in Federated Learning for Brain Tumor MRI Classification](https://arxiv.org/abs/2601.12671)
*Thamara Leandra de Deus Melo,Rodrigo Moreira,Larissa Ferreira Rodrigues Moreira,André Ricardo Backes*

Main category: cs.CV

TL;DR: 本文探讨了在联邦学习框架下，结合预处理和测试时增强（TTA）对脑肿瘤MRI分类模型性能的影响，发现TTA显著提升模型效果，而单独预处理作用有限。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤的高效诊断对于早期治疗至关重要，但由于病变多样性和图像复杂性，自动诊断充满挑战。该研究旨在提升基于MRI的脑肿瘤自动分类的准确性，并探讨如何在保持数据隐私的前提下（联邦学习），有效提升卷积神经网络（CNN）模型的表现。

Method: 作者在联邦学习框架下，分别对原始MRI和经多种预处理（如尺寸调整、灰度化、归一化、滤波和直方图均衡化）后的图像训练CNN模型。同时，评估了在推理阶段采用测试时增强（TTA）对模型性能的影响，对比了不同组合方案的效果。

Result: 单独采用图像预处理提升有限；而结合TTA则在联邦MRI分类任务上获得了持续且统计学显著（p<0.001）的性能提升。TTA与轻度预处理配合时可得到更稳定的提升。

Conclusion: 在基于联邦学习的医学影像任务中，TTA应成为默认的推理策略。在计算资源允许的情况下，建议将TTA与适度的图像预处理结合以进一步提升模型可靠性与准确性。

Abstract: Efficient brain tumor diagnosis is crucial for early treatment; however, it is challenging because of lesion variability and image complexity. We evaluated convolutional neural networks (CNNs) in a federated learning (FL) setting, comparing models trained on original versus preprocessed MRI images (resizing, grayscale conversion, normalization, filtering, and histogram equalization). Preprocessing alone yielded negligible gains; combined with test-time augmentation (TTA), it delivered consistent, statistically significant improvements in federated MRI classification (p<0.001). In practice, TTA should be the default inference strategy in FL-based medical imaging; when the computational budget permits, pairing TTA with light preprocessing provides additional reliable gains.

</details>


### [126] [VILTA: A VLM-in-the-Loop Adversary for Enhancing Driving Policy Robustness](https://arxiv.org/abs/2601.12672)
*Qimao Chen,Fang Li,Shaoqing Xu,Zhiyi Lai,Zixun Xie,Yuechen Luo,Shengyin Jiang,Hanbing Li,Long Chen,Bing Wang,Yi Zhang,Zhi-Xin Yang*

Main category: cs.CV

TL;DR: 本文提出了VILTA（VLM-In-the-Loop Trajectory Adversary）框架，将视觉语言模型（VLM）直接引入自动驾驶系统训练流程，主动生成稀有且具有挑战性的驾驶场景，有效提升了系统在长尾场景下的安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统在实际部署中面临长尾问题，即极少但关键的驾驶场景在真实数据中严重缺乏，导致系统在罕见情境下表现不足。现有生成与训练方法受制于规则、重采样和离线模型，生成场景多样性和新颖性受限。

Method: VILTA框架创新性地将VLM置于训练闭环中，使其主动参与且理解环境动态，通过精细化编辑周围车辆的未来轨迹，直接生成多样、具有挑战性的驾驶场景，而非传统仅用VLM生成语义描述，再交由下游模型生成场景的两阶段方式。

Result: 该方法显著提升了自动驾驶决策策略在危急长尾事件中的表现，实验证明能更好地识别并应对以往难以覆盖的高危场景，提高系统安全性和鲁棒性。

Conclusion: 通过将VLM充分融入自动驾驶训练环节，VILTA框架打破了传统生成方法的多样性瓶颈，为提高自动驾驶在稀有复杂环境下的安全能力提供了有效方案。

Abstract: The safe deployment of autonomous driving (AD) systems is fundamentally hindered by the long-tail problem, where rare yet critical driving scenarios are severely underrepresented in real-world data. Existing solutions including safety-critical scenario generation and closed-loop learning often rely on rule-based heuristics, resampling methods and generative models learned from offline datasets, limiting their ability to produce diverse and novel challenges. While recent works leverage Vision Language Models (VLMs) to produce scene descriptions that guide a separate, downstream model in generating hazardous trajectories for agents, such two-stage framework constrains the generative potential of VLMs, as the diversity of the final trajectories is ultimately limited by the generalization ceiling of the downstream algorithm. To overcome these limitations, we introduce VILTA (VLM-In-the-Loop Trajectory Adversary), a novel framework that integrates a VLM into the closed-loop training of AD agents. Unlike prior works, VILTA actively participates in the training loop by comprehending the dynamic driving environment and strategically generating challenging scenarios through direct, fine-grained editing of surrounding agents' future trajectories. This direct-editing approach fully leverages the VLM's powerful generalization capabilities to create a diverse curriculum of plausible yet challenging scenarios that extend beyond the scope of traditional methods. We demonstrate that our approach substantially enhances the safety and robustness of the resulting AD policy, particularly in its ability to navigate critical long-tail events.

</details>


### [127] [Fusion-Restoration Image Processing Algorithm to Improve the High-Temperature Deformation Measurement](https://arxiv.org/abs/2601.12682)
*Banglei Guan,Dongcai Tan,Jing Tao,Ang Su,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 本文提出了一种结合多曝光融合和图像恢复的图像处理方法，有效抑制高温变形测量中的热辐射和热扰动对图像产生的干扰，显著提升了数字图像相关（DIC）的测量精度。


<details>
  <summary>Details</summary>
Motivation: 高温结构变形测量中，热辐射造成图像退化，热扰动引入随机误差，严重影响DIC测量的准确性和有效性。亟需有效的图像处理方法提升高温条件下的变形测量性能。

Method: 针对热辐射导致的图像退化，利用图像分层表征，将图像分为正、负通道并行处理，通过多曝光融合优化图像质量。针对热扰动造成的高频随机误差，采用FSIM作为目标函数优化模型参数，并用灰度平均法均衡异常灰度值，降低测量误差。

Result: 所提多曝光融合算法能在复杂光照条件下大幅提升有效计算区域，并未降低测量精度。同时，图像恢复与灰度均值算法结合有效减少静态热变形测量误差。ε_xx误差降低85.3%，ε_yy和γ_xy分别降低36.0%和36.4%。

Conclusion: 本文方法可有效抑制高温变形测量中热辐射和热扰动的干扰，提升图像质量和变形测量精度，具有重要的实际应用价值。

Abstract: In the deformation measurement of high-temperature structures, image degradation caused by thermal radiation and random errors introduced by heat haze restrict the accuracy and effectiveness of deformation measurement. To suppress thermal radiation and heat haze using fusion-restoration image processing methods, thereby improving the accuracy and effectiveness of DIC in the measurement of high-temperature deformation. For image degradation caused by thermal radiation, based on the image layered representation, the image is decomposed into positive and negative channels for parallel processing, and then optimized for quality by multi-exposure image fusion. To counteract the high-frequency, random errors introduced by heat haze, we adopt the FSIM as the objective function to guide the iterative optimization of model parameters, and the grayscale average algorithm is applied to equalize anomalous gray values, thereby reducing measurement error. The proposed multi-exposure image fusion algorithm effectively suppresses image degradation caused by complex illumination conditions, boosting the effective computation area from 26% to 50% for under-exposed images and from 32% to 40% for over-exposed images without degrading measurement accuracy in the experiment. Meanwhile, the image restoration combined with the grayscale average algorithm reduces static thermal deformation measurement errors. The error in ε_xx is reduced by 85.3%, while the errors in ε_yy and γ_xy are reduced by 36.0% and 36.4%, respectively. We present image processing methods to suppress the interference of thermal radiation and heat haze in high-temperature deformation measurement using DIC. The experimental results verify that the proposed method can effectively improve image quality, reduce deformation measurement errors, and has potential application value in thermal deformation measurement.

</details>


### [128] [GaussianTrimmer: Online Trimming Boundaries for 3DGS Segmentation](https://arxiv.org/abs/2601.12683)
*Liwei Liao,Ronggang Wang*

Main category: cs.CV

TL;DR: 本文提出了一种针对3D高斯分割粗糙边界的在线剪裁方法GaussianTrimmer，通过虚拟相机和2D分割结果改善现有分割方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯的场景分割方法，由于高斯尺度变化大导致分割物体的边界锯齿化，尤其当大高斯跨越前景和背景时问题更为突出。

Method: 该方法包含两步：1）生成覆盖均匀的虚拟相机；2）基于这些虚拟相机获得的2D分割结果，对3D高斯基元级别进行边界剪裁，从而实现对原分割结果的后处理提升。

Result: 定量和定性实验均表明，作为插拔式后处理方法，GaussianTrimmer能够有效提升现有3D高斯分割方法的边界精度和整体分割质量。

Conclusion: GaussianTrimmer是一种高效、可插拔的后处理手段，大幅提升了3D高斯分割的细粒度表现，有助于更高质量的场景理解和下游应用。

Abstract: With the widespread application of 3D Gaussians in 3D scene representation, 3D scene segmentation methods based on 3D Gaussians have also gradually emerged. However, existing 3D Gaussian segmentation methods basically segment on the basis of Gaussian primitives. Due to the large variation range of the scale of 3D Gaussians, large-sized Gaussians that often span the foreground and background lead to jagged boundaries of segmented objects. To this end, we propose an online boundary trimming method, GaussianTrimmer, which is an efficient and plug-and-play post-processing method capable of trimming coarse boundaries for existing 3D Gaussian segmentation methods. Our method consists of two core steps: 1. Generating uniformly and well-covered virtual cameras; 2. Trimming Gaussian at the primitive level based on 2D segmentation results on virtual cameras. Extensive quantitative and qualitative experiments demonstrate that our method can improve the segmentation quality of existing 3D Gaussian segmentation methods as a plug-and-play method.

</details>


### [129] [Fusing in 3D: Free-Viewpoint Fusion Rendering with a 3D Infrared-Visible Scene Representation](https://arxiv.org/abs/2601.12697)
*Chao Yang,Deshui Miao,Chao Tian,Guoqing Zhu,Yameng Gu,Zhenyu He*

Main category: cs.CV

TL;DR: 本文提出了IVGF红外-可见光融合框架，通过引入高斯场和跨模态调整，实现更全面场景信息的融合和呈现。


<details>
  <summary>Details</summary>
Motivation: 现有2D红外-可见融合方法仅能处理固定视角，忽略了复杂场景中的空间几何理解，导致场景关键信息丢失。作者希望突破这一局限，实现场景几何重建和有效融合。

Method: 提出IVGF框架，利用多模态2D输入重建场景几何，并直接渲染融合图像；设计了跨模态调整（CMA）模块，通过调整高斯的不透明度解决模态冲突；引入融合损失函数，促进融合图像兼具红外与可见光关键特性。

Result: 通过定性与定量实验，验证了新方法在融合效果和保留重要场景信息方面的优越性。

Conclusion: IVGF框架能有效融合红外与可见图像，并在三维场景理解与重建上取得更好效果，适合复杂场景下的图像融合需求。

Abstract: Infrared-visible image fusion aims to integrate infrared and visible information into a single fused image. Existing 2D fusion methods focus on fusing images from fixed camera viewpoints, neglecting a comprehensive understanding of complex scenarios, which results in the loss of critical information about the scene. To address this limitation, we propose a novel Infrared-Visible Gaussian Fusion (IVGF) framework, which reconstructs scene geometry from multimodal 2D inputs and enables direct rendering of fused images. Specifically, we propose a cross-modal adjustment (CMA) module that modulates the opacity of Gaussians to solve the problem of cross-modal conflicts. Moreover, to preserve the distinctive features from both modalities, we introduce a fusion loss that guides the optimization of CMA, thus ensuring that the fused image retains the critical characteristics of each modality. Comprehensive qualitative and quantitative experiments demonstrate the effectiveness of the proposed method.

</details>


### [130] [P2L-CA: An Effective Parameter Tuning Framework for Rehearsal-Free Multi-Label Class-Incremental Learning](https://arxiv.org/abs/2601.12714)
*Songlin Dong,Jiangyang Li,Chenhao Ding,Zhiheng Ma,Haoyu Luo,Yuhang He,Yihong Gong*

Main category: cs.CV

TL;DR: P2L-CA是一种高效的多标签增量学习框架，通过参数高效的模块实现更好的性能，降低了计算和存储开销。


<details>
  <summary>Details</summary>
Motivation: 现有多标签增量学习方法要么计算和存储成本高（如需全部参数微调和依赖大量记忆缓冲），要么无法很好解决特征混淆和领域偏移问题，限制了实际应用。作者旨在提出一种低成本、高性能的新框架。

Method: 提出P2L-CA框架，包括两个核心模块：1）P2L模块，利用类别相关的prompt引导特征解耦，并通过语言先验增强语义与视觉的对齐；2）CA模块，采用轻量adapter缓解预训练与下游任务的领域差异。整体方法减少了参数量并无需记忆缓冲。

Result: 在MS-COCO和PASCAL VOC等数据集的多标签增量学习设置下，P2L-CA在性能上显著优于SOTA方法，同时具备极强的泛化能力，且所需可训练参数极少，无需内存缓冲。

Conclusion: P2L-CA框架兼顾了高效性与优异性能，为多标签类别增量学习任务提供了新的解决思路，有望应用于实际复杂场景。

Abstract: Multi-label Class-Incremental Learning aims to continuously recognize novel categories in complex scenes where multiple objects co-occur. However, existing approaches often incur high computational costs due to full-parameter fine-tuning and substantial storage overhead from memory buffers, or they struggle to address feature confusion and domain discrepancies adequately. To overcome these limitations, we introduce P2L-CA, a parameter-efficient framework that integrates a Prompt-to-Label module with a Continuous Adapter module. The P2L module leverages class-specific prompts to disentangle multi-label representations while incorporating linguistic priors to enforce stable semantic-visual alignment. Meanwhile, the CA module employs lightweight adapters to mitigate domain gaps between pre-trained models and downstream tasks, thereby enhancing model plasticity. Extensive experiments across standard and challenging MLCIL settings on MS-COCO and PASCAL VOC show that P2L-CA not only achieves substantial improvements over state-of-the-art methods but also demonstrates strong generalization in CIL scenarios, all while requiring minimal trainable parameters and eliminating the need for memory buffers.

</details>


### [131] [RSOD: Reliability-Guided Sonar Image Object Detection with Extremely Limited Labels](https://arxiv.org/abs/2601.12715)
*Chengzhou Li,Ping Guo,Guanchen Meng,Qi Jia,Jinyuan Liu,Zhu Liu,Xiaokang Liu,Yu Liu,Zhongxuan Luo,Xin Fan*

Main category: cs.CV

TL;DR: 该论文提出了一种名为RSOD的教师-学生框架，用于在标注极为有限的情况下对声纳图像进行目标检测。


<details>
  <summary>Details</summary>
Motivation: 声纳图像纹理细节少且噪声多，非专业人员难以准确区分和标注，导致高质量标注数据稀缺，亟需能在极少标签下有效检测目标的方法。

Method: 提出RSOD教师-学生框架。首先通过评估教师模型不同视角下预测结果的一致性，计算置信度分数。基于此置信度，引入对象混合伪标签策略，有效利用未标注数据，缓解人工标注不足。最后根据置信分数，对学生模型训练过程进行自适应约束优化。

Result: 在UATD数据集上，仅用5%的标注数据，RSOD方法的检测效果即可与完全标注（100%标注数据）下的基线算法相媲美。同时，作者还采集了新的声纳数据集用于推动研究。

Conclusion: RSOD可在极少标注情况下显著提升声纳图像目标检测性能，是针对该领域数据稀缺痛点的有效办法。

Abstract: Object detection in sonar images is a key technology in underwater detection systems. Compared to natural images, sonar images contain fewer texture details and are more susceptible to noise, making it difficult for non-experts to distinguish subtle differences between classes. This leads to their inability to provide precise annotation data for sonar images. Therefore, designing effective object detection methods for sonar images with extremely limited labels is particularly important. To address this, we propose a teacher-student framework called RSOD, which aims to fully learn the characteristics of sonar images and develop a pseudo-label strategy suitable for these images to mitigate the impact of limited labels. First, RSOD calculates a reliability score by assessing the consistency of the teacher's predictions across different views. To leverage this score, we introduce an object mixed pseudo-label method to tackle the shortage of labeled data in sonar images. Finally, we optimize the performance of the student by implementing a reliability-guided adaptive constraint. By taking full advantage of unlabeled data, the student can perform well even in situations with extremely limited labels. Notably, on the UATD dataset, our method, using only 5% of labeled data, achieves results that can compete against those of our baseline algorithm trained on 100% labeled data. We also collected a new dataset to provide more valuable data for research in the field of sonar.

</details>


### [132] [S2DiT: Sandwich Diffusion Transformer for Mobile Streaming Video Generation](https://arxiv.org/abs/2601.12719)
*Lin Zhao,Yushu Wu,Aleksei Lebedev,Dishani Lahiri,Meng Dong,Arpit Sahni,Michael Vasilkovsky,Hao Chen,Ju Hu,Aliaksandr Siarohin,Sergey Tulyakov,Yanzhi Wang,Anil Kag,Yanyu Li*

Main category: cs.CV

TL;DR: 本文提出一种高效的视频生成模型S2DiT，在移动端可实现高保真、流式视频生成，速度可达10 FPS，效果媲美服务器端先进模型。


<details>
  <summary>Details</summary>
Motivation: 尽管Diffusion Transformers大幅提升了视频生成质量，但由于计算量大，难以在实时或移动设备上部署。此文旨在解决高质视频生成的高成本问题，实现移动端实时可用。

Method: 作者提出了一种名为S2DiT的流式夹心扩散Transformer。为提升效率，模型设计了LCHA与SSA两种新型高效注意力机制，并采用预算感知的动态规划搜索，优化“夹心”结构设计。此外，通过2合1知识蒸馏框架，将大型教师模型的能力转移至小型模型。

Result: S2DiT在视频生成质量上接近服务器端最先进模型，同时可在iPhone等移动设备上实现超过10帧每秒的流式生成，兼顾了高效性和高保真。

Conclusion: 本研究证明高质量视频生成模型可通过结构创新与知识蒸馏，在移动端高效实现，具有很强的实际应用价值。

Abstract: Diffusion Transformers (DiTs) have recently improved video generation quality. However, their heavy computational cost makes real-time or on-device generation infeasible. In this work, we introduce S2DiT, a Streaming Sandwich Diffusion Transformer designed for efficient, high-fidelity, and streaming video generation on mobile hardware. S2DiT generates more tokens but maintains efficiency with novel efficient attentions: a mixture of LinConv Hybrid Attention (LCHA) and Stride Self-Attention (SSA). Based on this, we uncover the sandwich design via a budget-aware dynamic programming search, achieving superior quality and efficiency. We further propose a 2-in-1 distillation framework that transfers the capacity of large teacher models (e.g., Wan 2.2-14B) to the compact few-step sandwich model. Together, S2DiT achieves quality on par with state-of-the-art server video models, while streaming at over 10 FPS on an iPhone.

</details>


### [133] [KaoLRM: Repurposing Pre-trained Large Reconstruction Models for Parametric 3D Face Reconstruction](https://arxiv.org/abs/2601.12736)
*Qingtian Zhu,Xu Cao,Zhixiang Wang,Yinqiang Zheng,Takafumi Taketomi*

Main category: cs.CV

TL;DR: 本文提出KaoLRM方法，结合LRM的大型重建模型先验和FLAME参数化人脸模型，通过2D高斯Splatting实现对单张图片的高精度3D人脸重建，并显著提升跨视角一致性。


<details>
  <summary>Details</summary>
Motivation: 传统参数化的3D人脸模型（如3DMM）因参数简洁、可解释被广泛采用，但在跨视角一致性方面表现较差。现有回归器对视角变化较为敏感，影响实际应用效果。因此，论文尝试通过迁移大型重建模型（LRM）中的3D先验知识，提升单视图情况下3D人脸重建的准确性和鲁棒性。

Method: KaoLRM方法将LRM预训练的triplane特征投影到FLAME参数空间以恢复三维几何，并利用与FLAME网格紧密绑定的2D高斯基元建模人脸外观。整合后的系统允许FLAME回归器自动感知三维结构信息，实现更准确和稳健的重建效果，尤其在自遮挡和多视角情况下表现出色。

Result: 在受控环境和真实复杂场景两个基准上实验，KaoLRM均在重建精度和跨视角一致性方面明显优于现有方法，解决了原有方法对视角变化敏感的问题。

Conclusion: KaoLRM证明了迁移大型3D重建模型先验结合FLAME参数模型与2D高斯Splatting技术可显著提升单视图3D人脸重建的精度和一致性，对相关应用有重要推动作用。

Abstract: We propose KaoLRM to re-target the learned prior of the Large Reconstruction Model (LRM) for parametric 3D face reconstruction from single-view images. Parametric 3D Morphable Models (3DMMs) have been widely used for facial reconstruction due to their compact and interpretable parameterization, yet existing 3DMM regressors often exhibit poor consistency across varying viewpoints. To address this, we harness the pre-trained 3D prior of LRM and incorporate FLAME-based 2D Gaussian Splatting into LRM's rendering pipeline. Specifically, KaoLRM projects LRM's pre-trained triplane features into the FLAME parameter space to recover geometry, and models appearance via 2D Gaussian primitives that are tightly coupled to the FLAME mesh. The rich prior enables the FLAME regressor to be aware of the 3D structure, leading to accurate and robust reconstructions under self-occlusions and diverse viewpoints. Experiments on both controlled and in-the-wild benchmarks demonstrate that KaoLRM achieves superior reconstruction accuracy and cross-view consistency, while existing methods remain sensitive to viewpoint variations. The code is released at https://github.com/CyberAgentAILab/KaoLRM.

</details>


### [134] [SSPFormer: Self-Supervised Pretrained Transformer for MRI Images](https://arxiv.org/abs/2601.12747)
*Jingkai Li,Xiaoze Tian,Yuhang Shen,Jia Wang,Dianjie Lu,Guijuan Zhang,Zhuoran Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种自监督预训练Transformer（SSPFormer），用于MRI医学影像的表征学习，通过引入频率掩码和噪声增强技术，实现了在细粒度结构感知和抗伪影性上的突破，并在多项任务上取得了先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有预训练Transformer虽适合自然图像，但在医学影像（如MRI）迁移时，存在对解剖结构适应性差和数据隐私与稀缺瓶颈，急需发展能面向医学领域特性的高效表征方法。

Method: 提出SSPFormer框架，将自监督预训练与领域适应结合：1）采用逆频率投影掩码，优先重建MRI高频区域，增强结构感知；2）应用频率加权FFT噪声，模拟生理真实噪声，提高模型对MRI伪影的鲁棒性。这些方法直接利用未标注原始影像提升专属表征能力。

Result: 在MRI分割、超分辨和去噪等任务的多项实验证明，SSPFormer在图像细节保持、结构感知及抗伪影性等方面取得了优于现有方法的结果。

Conclusion: SSPFormer能够高效捕获MRI细粒度和结构信息，同时适应临床实际应用需求，是面向医学影像领域预训练算法的一个有力进展。

Abstract: The pre-trained transformer demonstrates remarkable generalization ability in natural image processing. However, directly transferring it to magnetic resonance images faces two key challenges: the inability to adapt to the specificity of medical anatomical structures and the limitations brought about by the privacy and scarcity of medical data. To address these issues, this paper proposes a Self-Supervised Pretrained Transformer (SSPFormer) for MRI images, which effectively learns domain-specific feature representations of medical images by leveraging unlabeled raw imaging data. To tackle the domain gap and data scarcity, we introduce inverse frequency projection masking, which prioritizes the reconstruction of high-frequency anatomical regions to enforce structure-aware representation learning. Simultaneously, to enhance robustness against real-world MRI artifacts, we employ frequency-weighted FFT noise enhancement that injects physiologically realistic noise into the Fourier domain. Together, these strategies enable the model to learn domain-invariant and artifact-robust features directly from raw scans. Through extensive experiments on segmentation, super-resolution, and denoising tasks, the proposed SSPFormer achieves state-of-the-art performance, fully verifying its ability to capture fine-grained MRI image fidelity and adapt to clinical application requirements.

</details>


### [135] [Moaw: Unleashing Motion Awareness for Video Diffusion Models](https://arxiv.org/abs/2601.12761)
*Tianqi Zhang,Ziyi Wang,Wenzhao Zheng,Weiliang Chen,Yuanhui Huang,Zhengyang Huang,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 本文提出Moaw框架，通过对视频扩散模型进行有监督训练，增强其对运动的感知和迁移能力，实现无需额外模块的运动迁移。


<details>
  <summary>Details</summary>
Motivation: 尽管视频扩散模型可以无监督地捕获跨帧特征关联，并在零样本情境下用于光流预测和目标跟踪，但其跟踪能力尚未被有监督训练充分挖掘。作者希望通过有监督学习进一步挖掘和提升视频扩散模型在运动理解方面的潜力。

Method: 提出Moaw框架，通过将原本图像到视频扩散模型的任务转化为视频到稠密跟踪的任务，训练模型提升对运动的感知能力。作者构建了带有运动标签的数据集，筛选出最强运动信息的特征，将其无缝注入结构相同的视频生成模型，借助模型结构同质性，实现零样本运动迁移。

Result: Moaw能够无需额外适配器情况下，实现运动特征跨模型迁移，提升视频扩散模型的运动理解与运动迁移能力。实验验证了其在视频学习领域实现统一且可控框架的可行性。

Conclusion: 该方法为生成建模与运动理解的结合提供了一种新范式，为更加统一和可控的视频学习框架奠定基础。

Abstract: Video diffusion models, trained on large-scale datasets, naturally capture correspondences of shared features across frames. Recent works have exploited this property for tasks such as optical flow prediction and tracking in a zero-shot setting. Motivated by these findings, we investigate whether supervised training can more fully harness the tracking capability of video diffusion models. To this end, we propose Moaw, a framework that unleashes motion awareness for video diffusion models and leverages it to facilitate motion transfer. Specifically, we train a diffusion model for motion perception, shifting its modality from image-to-video generation to video-to-dense-tracking. We then construct a motion-labeled dataset to identify features that encode the strongest motion information, and inject them into a structurally identical video generation model. Owing to the homogeneity between the two networks, these features can be naturally adapted in a zero-shot manner, enabling motion transfer without additional adapters. Our work provides a new paradigm for bridging generative modeling and motion understanding, paving the way for more unified and controllable video learning frameworks.

</details>


### [136] [Towards Unbiased Source-Free Object Detection via Vision Foundation Models](https://arxiv.org/abs/2601.12765)
*Zhi Cai,Yingjie Gao,Yanan Zhang,Xinzhu Ma,Di Huang*

Main category: cs.CV

TL;DR: 本文提出了一种名为DSOD的新型无源数据目标检测方法，利用视觉基础模型（VFM）缓解模型对源域的偏置问题，从而提升跨域目标检测的泛化能力，在多项基准测试中性能超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的无源域目标检测（SFOD）方法在去除源域数据的同时，容易出现“源偏置”问题，导致模型过度依赖源域特征，影响在新域的检测效果。为了解决这一核心挑战，提高模型的跨域泛化能力，作者提出了DSOD框架。

Method: 提出了基于视觉基础模型（VFM）辅助的DSOD方法。首先，通过‘统一特征注入’（UFI）模块引入VFM特征，包括简单尺度扩展（SSE）和域自适应加权（DAAW）机制，增强特征表达。此外，作者设计了‘语义感知特征正则化’（SAFR）方法，约束模型不过拟合源域特征。对于算力受限场景，还提出了不依赖VFM的DSOD-distill变体，通过‘双教师蒸馏’方案实现高效模型转移。

Result: 在多个跨域目标检测基准（包括Normal-to-Foggy、Cross-scene、Synthetic-to-Real）上，DSOD性能均超越SOTA方法，达到48.1%、39.3%和61.4%的AP。

Conclusion: DSOD框架有效克服了传统SFOD方法中的源偏置问题，利用VFM提升了跨域检测性能，并能通过知识蒸馏适用于资源受限场景，展现出广泛的实用前景。

Abstract: Source-Free Object Detection (SFOD) has garnered much attention in recent years by eliminating the need of source-domain data in cross-domain tasks, but existing SFOD methods suffer from the Source Bias problem, i.e. the adapted model remains skewed towards the source domain, leading to poor generalization and error accumulation during self-training. To overcome this challenge, we propose Debiased Source-free Object Detection (DSOD), a novel VFM-assisted SFOD framework that can effectively mitigate source bias with the help of powerful VFMs. Specifically, we propose Unified Feature Injection (UFI) module that integrates VFM features into the CNN backbone through Simple-Scale Extension (SSE) and Domain-aware Adaptive Weighting (DAAW). Then, we propose Semantic-aware Feature Regularization (SAFR) that constrains feature learning to prevent overfitting to source domain characteristics. Furthermore, we propose a VFM-free variant, termed DSOD-distill for computation-restricted scenarios through a novel Dual-Teacher distillation scheme. Extensive experiments on multiple benchmarks demonstrate that DSOD outperforms state-of-the-art SFOD methods, achieving 48.1% AP on Normal-to-Foggy weather adaptation, 39.3% AP on Cross-scene adaptation, and 61.4% AP on Synthetic-to-Real adaptation.

</details>


### [137] [Spatial-VLN: Zero-Shot Vision-and-Language Navigation With Explicit Spatial Perception and Exploration](https://arxiv.org/abs/2601.12766)
*Lu Yue,Yue Fan,Shiwei Lian,Yu Zhao,Jiaxin Yu,Liang Xie,Feitian Zhang*

Main category: cs.CV

TL;DR: 论文提出了Spatial-VLN框架，通过加强空间感知和专家推理，有效提升了基于大语言模型的视觉-语言导航系统在复杂环境中的泛化能力和鲁棒性，尤其在真实世界实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型（LLMs）的视觉-语言导航（VLN）系统在零样本泛化方面表现出色，但对空间信息的感知能力薄弱，导致在门的识别、多房间导航以及含糊指令执行上常常失败。因此，需要一种新框架提升空间感知，从而解决这些空间挑战。

Method: 提出了Spatial-VLN框架，包含空间感知增强（SPE）和多专家推理（EMR）两个模块。SPE利用全景过滤和专用门/区域识别专家，生成空间一致且视角间协调的感知表示。EMR使用多个并行LLM专家处理路径语义与空间过渡，并在专家预测矛盾时，通过主动查询-探索机制，推动智能体主动探查关键区域以消除歧义。

Result: 在VLN-CE基准测试中，Spatial-VLN仅用低成本LLM便达到SOTA性能。并提出利用基于价值的航点采样，有效缩小虚拟-现实落差。大量真实世界实验验证了该框架在复杂环境下的优越泛化和鲁棒性。

Conclusion: Spatial-VLN框架显著提升了视觉-语言导航系统的空间感知和决策能力，在复杂导航场景中表现出色，为LLM在现实场景中的自主体导航应用提供了有效方案。

Abstract: Zero-shot Vision-and-Language Navigation (VLN) agents leveraging Large Language Models (LLMs) excel in generalization but suffer from insufficient spatial perception. Focusing on complex continuous environments, we categorize key perceptual bottlenecks into three spatial challenges: door interaction,multi-room navigation, and ambiguous instruction execution, where existing methods consistently suffer high failure rates. We present Spatial-VLN, a perception-guided exploration framework designed to overcome these challenges. The framework consists of two main modules. The Spatial Perception Enhancement (SPE) module integrates panoramic filtering with specialized door and region experts to produce spatially coherent, cross-view consistent perceptual representations. Building on this foundation, our Explored Multi-expert Reasoning (EMR) module uses parallel LLM experts to address waypoint-level semantics and region-level spatial transitions. When discrepancies arise between expert predictions, a query-and-explore mechanism is activated, prompting the agent to actively probe critical areas and resolve perceptual ambiguities. Experiments on VLN-CE demonstrate that Spatial VLN achieves state-of-the-art performance using only low-cost LLMs. Furthermore, to validate real-world applicability, we introduce a value-based waypoint sampling strategy that effectively bridges the Sim2Real gap. Extensive real-world evaluations confirm that our framework delivers superior generalization and robustness in complex environments. Our codes and videos are available at https://yueluhhxx.github.io/Spatial-VLN-web/.

</details>


### [138] [Delving Deeper: Hierarchical Visual Perception for Robust Video-Text Retrieval](https://arxiv.org/abs/2601.12768)
*Zequn Xie,Boyun Zhang,Yuxiao Lin,Tao Jin*

Main category: cs.CV

TL;DR: 该论文提出了HVP-Net，一种通过多层次视觉特征提炼来增强视频-文本检索的方法，显著提升了主流VTR基准的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视频-文本检索方法主要依赖于如CLIP等预训练模型，但它们仅用粗粒度的最终层特征，导致视频内在冗余未被有效处理，影响检索精度。

Method: 作者提出HVP-Net，通过从视觉编码器的多个中间层提取并逐步精炼特征，从不同语义层级挖掘更丰富的视频语义。该方法通过分阶段提取并过滤patch-token特征，有效缩减冗余并保留关键信息，提升视频与文本对齐能力。

Result: HVP-Net在多个具有挑战性的基准（MSRVTT、DiDeMo、ActivityNet）上，取得了新的SOTA（最先进）性能，验证了新方法的有效性。

Conclusion: 利用视觉编码器的层级特征可以显著提升视频-文本检索效果，HVP-Net框架为VTR任务提供了更鲁棒且细致的视频表征方式。

Abstract: Video-text retrieval (VTR) aims to locate relevant videos using natural language queries. Current methods, often based on pre-trained models like CLIP, are hindered by video's inherent redundancy and their reliance on coarse, final-layer features, limiting matching accuracy. To address this, we introduce the HVP-Net (Hierarchical Visual Perception Network), a framework that mines richer video semantics by extracting and refining features from multiple intermediate layers of a vision encoder. Our approach progressively distills salient visual concepts from raw patch-tokens at different semantic levels, mitigating redundancy while preserving crucial details for alignment. This results in a more robust video representation, leading to new state-of-the-art performance on challenging benchmarks including MSRVTT, DiDeMo, and ActivityNet. Our work validates the effectiveness of exploiting hierarchical features for advancing video-text retrieval. Our codes are available at https://github.com/boyun-zhang/HVP-Net.

</details>


### [139] [Generalizable and Animatable 3D Full-Head Gaussian Avatar from a Single Image](https://arxiv.org/abs/2601.12770)
*Shuling Zhao,Dan Xu*

Main category: cs.CV

TL;DR: 本文提出了一种从单张图片高效构建可动画3D头像的新方法，实现了360度全头部重建，并支持实时动画。该方案通过在参数化面部模型的UV空间中嵌入高斯基元对头部进行建模，结合预训练3D GAN以获取全头几何和纹理信息，同时融合局部细节特征，有效提升3D头像的真实性。


<details>
  <summary>Details</summary>
Motivation: 现有3D头像重建方法在摄像机姿态变化较大时容易失效，难以生成高真实感、可动画的3D全头部模型。现实中，单张图片重建出高质量、全方向可动画的3D头像需求强烈，例如虚拟人、游戏、元宇宙等众多应用场景，因此亟需快速且鲁棒的新方法。

Method: 作者设计了一种单步前馈的3D全头动画头像重建框架。具体做法是以高斯基元在参数人脸模型的UV空间内嵌入的方式对3D头像进行建模。通过预训练3D GAN提取全头几何与纹理全局特征，借助多视角监督学习增强模型理解。同时，充分利用UV空间和人脸固有对称性，将输入图片的局部细节特征与全局全头纹理融合，以提升重建细致度。

Result: 实验结果表明，该方法能高质量复原3D全头部头像，同时支持实时360度可动画渲染。生成的3D头像真实感显著优于传统方法，在真实性和动画流畅性上均有提升。

Conclusion: 作者提出的基于高斯基元和预训练3D GAN的新框架，有效解决了单张图片3D全头动画头像重建中的姿态鲁棒性和细节真实性问题，能广泛应用于虚拟人、数字娱乐等领域，推进了3D头像建模技术发展。

Abstract: Building 3D animatable head avatars from a single image is an important yet challenging problem. Existing methods generally collapse under large camera pose variations, compromising the realism of 3D avatars. In this work, we propose a new framework to tackle the novel setting of one-shot 3D full-head animatable avatar reconstruction in a single feed-forward pass, enabling real-time animation and simultaneous 360$^\circ$ rendering views. To facilitate efficient animation control, we model 3D head avatars with Gaussian primitives embedded on the surface of a parametric face model within the UV space. To obtain knowledge of full-head geometry and textures, we leverage rich 3D full-head priors within a pretrained 3D generative adversarial network (GAN) for global full-head feature extraction and multi-view supervision. To increase the fidelity of the 3D reconstruction of the input image, we take advantage of the symmetric nature of the UV space and human faces to fuse local fine-grained input image features with the global full-head textures. Extensive experiments demonstrate the effectiveness of our method, achieving high-quality 3D full-head modeling as well as real-time animation, thereby improving the realism of 3D talking avatars.

</details>


### [140] [Open Vocabulary Panoptic Segmentation With Retrieval Augmentation](https://arxiv.org/abs/2601.12779)
*Nafis Sadeq,Qingfeng Liu,Mostafa El-Khamy*

Main category: cs.CV

TL;DR: 本文提出了一种名为RetCLIP的检索增强泛视角分割方法，相较于先前方法，在对训练中未见类的分割能力上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统的泛视角分割方法泛化能力有限，难以应对用户输入的任意类别。能实现开放词汇的分割对于实用性和灵活性有很大提升意义。

Method: 方法上，作者利用图像-文本配对数据构建了一个掩码分割特征数据库。在推理时，利用输入图像的掩码分割特征作为查询，检索数据库中相似的特征及其类别，并根据相似度分配类别分数。最终结果结合检索分数与CLIP分数。该方案集成到现有FC-CLIP方法中。

Result: 在COCO上训练后，RetCLIP在ADE20k数据集上实现了30.9 PQ、19.3 mAP和44.0 mIoU，比baseline提升了4.5 PQ、2.5 mAP和10.0 mIoU。

Conclusion: RetCLIP方法提升了对未见类的分割性能，表明检索增强方案在开放词汇泛视角分割中具有显著实用价值。

Abstract: Given an input image and set of class names, panoptic segmentation aims to label each pixel in an image with class labels and instance labels. In comparison, Open Vocabulary Panoptic Segmentation aims to facilitate the segmentation of arbitrary classes according to user input. The challenge is that a panoptic segmentation system trained on a particular dataset typically does not generalize well to unseen classes beyond the training data. In this work, we propose RetCLIP, a retrieval-augmented panoptic segmentation method that improves the performance of unseen classes. In particular, we construct a masked segment feature database using paired image-text data. At inference time, we use masked segment features from the input image as query keys to retrieve similar features and associated class labels from the database. Classification scores for the masked segment are assigned based on the similarity between query features and retrieved features. The retrieval-based classification scores are combined with CLIP-based scores to produce the final output. We incorporate our solution with a previous SOTA method (FC-CLIP). When trained on COCO, the proposed method demonstrates 30.9 PQ, 19.3 mAP, 44.0 mIoU on the ADE20k dataset, achieving +4.5 PQ, +2.5 mAP, +10.0 mIoU absolute improvement over the baseline.

</details>


### [141] [SKANet: A Cognitive Dual-Stream Framework with Adaptive Modality Fusion for Robust Compound GNSS Interference Classification](https://arxiv.org/abs/2601.12791)
*Zhihan Zeng,Yang Zhao,Kaihe Wang,Dusit Niyato,Hongyuan Shu,Junchu Zhao,Yanjun Huang,Yue Xiu,Zhongpei Zhang,Ning Wei*

Main category: cs.CV

TL;DR: 本文提出了SKANet网络，通过动态调节感受野，实现对复杂GNSS复合干扰的高效识别，在低信噪比下表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着电磁环境复杂化，全球导航卫星系统（GNSS）面临更复杂的干扰，传统方法难以区分多源复合干扰，需提出更智能灵活的识别方法。

Method: 提出了一种双流神经网络架构SKANet，将时频图（TFI）和功率谱密度（PSD）两种特征融合。网络引入了多分支选择性卷积（SK）模块和非对称卷积块（ACB），可动态调整感受野，融合SE模块自适应调整各类特征权重。

Result: 在由40.5万样本组成的数据集上，SKANet实现了96.99%的总分类准确率，对低干扰比（低JNR）的复合干扰表现尤为突出。

Conclusion: SKANet较传统方法更加鲁棒，能够精准识别复杂GNSS干扰，为抗干扰通信提供了有效手段。

Abstract: As the electromagnetic environment becomes increasingly complex, Global Navigation Satellite Systems (GNSS) face growing threats from sophisticated jamming interference. Although Deep Learning (DL) effectively identifies basic interference, classifying compound interference remains difficult due to the superposition of diverse jamming sources. Existing single-domain approaches often suffer from performance degradation because transient burst signals and continuous global signals require conflicting feature extraction scales. We propose the Selective Kernel and Asymmetric convolution Network(SKANet), a cognitive deep learning framework built upon a dual-stream architecture that integrates Time-Frequency Images (TFIs) and Power Spectral Density (PSD). Distinct from conventional fusion methods that rely on static receptive fields, the proposed architecture incorporates a Multi-Branch Selective Kernel (SK) module combined with Asymmetric Convolution Blocks (ACBs). This mechanism enables the network to dynamically adjust its receptive fields, acting as an adaptive filter that simultaneously captures micro-scale transient features and macro-scale spectral trends within entangled compound signals. To complement this spatial-temporal adaptation, a Squeeze-and-Excitation (SE) mechanism is integrated at the fusion stage to adaptively recalibrate the contribution of heterogeneous features from each modality. Evaluations on a dataset of 405,000 samples demonstrate that SKANet achieves an overall accuracy of 96.99\%, exhibiting superior robustness for compound jamming classification, particularly under low Jamming-to-Noise Ratio (JNR) regimes.

</details>


### [142] [Combating Noisy Labels through Fostering Self- and Neighbor-Consistency](https://arxiv.org/abs/2601.12795)
*Zeren Sun,Yazhou Yao,Tongliang Liu,Zechao Li,Fumin Shen,Jinhui Tang*

Main category: cs.CV

TL;DR: 本论文提出了一种名为Jo-SNC的抗噪声学习方法，通过自一致性和邻居一致性，提升了深度学习模型在有标签噪声场景下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只关注找出干净样本进行训练，并未充分考虑不同mini-batch中标签噪声的不均衡以及分布外的噪声数据，这限制了模型在复杂现实场景下的表现。

Method: 提出Jo-SNC方法，利用Jensen-Shannon散度结合样本邻居信息，度量样本是否清洁或分布外，通过自适应阈值进行数据筛选。对于不同类型样本采取常规训练、部分标签学习和消极学习三种训练方式，并引入三元一致性正则化提升模型泛化能力。

Result: 作者在多个基准数据集和消融实验上验证了方法的有效性和优越性，超过了多种最新的抗噪声方法表现。

Conclusion: Jo-SNC在处理标签噪声和分布外噪声问题上表现优秀，为抗噪声深度学习提供了一种更为有效的解决方案。

Abstract: Label noise is pervasive in various real-world scenarios, posing challenges in supervised deep learning. Deep networks are vulnerable to such label-corrupted samples due to the memorization effect. One major stream of previous methods concentrates on identifying clean data for training. However, these methods often neglect imbalances in label noise across different mini-batches and devote insufficient attention to out-of-distribution noisy data. To this end, we propose a noise-robust method named Jo-SNC (\textbf{Jo}int sample selection and model regularization based on \textbf{S}elf- and \textbf{N}eighbor-\textbf{C}onsistency). Specifically, we propose to employ the Jensen-Shannon divergence to measure the ``likelihood'' of a sample being clean or out-of-distribution. This process factors in the nearest neighbors of each sample to reinforce the reliability of clean sample identification. We design a self-adaptive, data-driven thresholding scheme to adjust per-class selection thresholds. While clean samples undergo conventional training, detected in-distribution and out-of-distribution noisy samples are trained following partial label learning and negative learning, respectively. Finally, we advance the model performance further by proposing a triplet consistency regularization that promotes self-prediction consistency, neighbor-prediction consistency, and feature consistency. Extensive experiments on various benchmark datasets and comprehensive ablation studies demonstrate the effectiveness and superiority of our approach over existing state-of-the-art methods.

</details>


### [143] [PhyG-MoE: A Physics-Guided Mixture-of-Experts Framework for Energy-Efficient GNSS Interference Recognition](https://arxiv.org/abs/2601.12798)
*Zhihan Zeng,Yang Zhao,Kaihe Wang,Dusit Niyato,Yue Xiu,Lu Chen,Zhongpei Zhang,Ning Wei*

Main category: cs.CV

TL;DR: 本文提出了一种动态调度模型能力的新方法PhG-MoE，有效提升了GNSS系统在复杂电磁干扰环境下的干扰识别性能，并大幅减少了计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 复杂的电磁干扰严重威胁到GNSS系统和SAGIN的可靠性。现有基于深度学习的干扰识别方法普遍采用固定结构，无法根据信号复杂度动态分配计算资源，导致资源浪费和运算瓶颈。

Method: 提出了一种基于物理引导的专家混合架构（PhyG-MoE），利用频谱特征纠缠度作为信号判据，通过门控机制将复杂信号分配给大模型专家处理，对基础信号则调用轻量专家，以此动态匹配模型容量与信号复杂性。

Result: 在21类干扰数据集上的实验证明，PhyG-MoE模型整体识别准确率达到97.58%，同时显著降低了运算开销。

Conclusion: 该方法缓解了静态计算结构与动态电磁环境间的矛盾，在不损失识别性能的前提下大幅节省了计算资源，为受限资源情景下的智能认知接收机提供了有效解决方案。

Abstract: Complex electromagnetic interference increasingly compromises Global Navigation Satellite Systems (GNSS), threatening the reliability of Space-Air-Ground Integrated Networks (SAGIN). Although deep learning has advanced interference recognition, current static models suffer from a \textbf{fundamental limitation}: they impose a fixed computational topology regardless of the input's physical entropy. This rigidity leads to severe resource mismatch, where simple primitives consume the same processing cost as chaotic, saturated mixtures. To resolve this, this paper introduces PhyG-MoE (Physics-Guided Mixture-of-Experts), a framework designed to \textbf{dynamically align model capacity with signal complexity}. Unlike static architectures, the proposed system employs a spectrum-based gating mechanism that routes signals based on their spectral feature entanglement. A high-capacity TransNeXt expert is activated on-demand to disentangle complex features in saturated scenarios, while lightweight experts handle fundamental signals to minimize latency. Evaluations on 21 jamming categories demonstrate that PhyG-MoE achieves an overall accuracy of 97.58\%. By resolving the intrinsic conflict between static computing and dynamic electromagnetic environments, the proposed framework significantly reduces computational overhead without performance degradation, offering a viable solution for resource-constrained cognitive receivers.

</details>


### [144] [Left-Right Symmetry Breaking in CLIP-style Vision-Language Models Trained on Synthetic Spatial-Relation Data](https://arxiv.org/abs/2601.12809)
*Takaki Yamamoto,Chihiro Noguchi,Toshihiro Tanizawa*

Main category: cs.CV

TL;DR: 本研究探讨了基于Transformer的视觉-语言模型在空间（左右）关系理解方面的机制，发现模型通过对标签多样性的对比学习能有效泛化左右关系，并解析了其注意力机制。


<details>
  <summary>Details</summary>
Motivation: 空间关系理解（如左右关系）是视觉-语言模型的重要能力，但该能力的获得过程和机制尚不清楚。作者希望揭示模型是如何获得、以及何时获得这种关系理解能力的。

Method: 作者构建了一个可控的一维图文测试环境，使用轻量级的Transformer视觉和文本编码器，在包含一物体和两物体场景的描述配对上，采用CLIP式对比目标进行端到端训练。通过系统变化标签和布局多样性，观察模型对左右关系的泛化能力。同时，通过注意力分解，研究模型内部对左右关系的表征机制。

Result: CLIP式对比学习能够让模型学会左右关系理解。在泛化能力上，标签多样性比布局多样性作用更大。注意力分解显示，位置与词元嵌入的交互在编码器内部形成了打破左右对称性的水平注意力梯度，支持了模型的左右区分能力；去除此机制则会显著减弱模型的左右判别能力。

Conclusion: 该工作揭示了CLIP风格的视觉-语言模型是在何时、如何获得关系理解能力的，并通过可解释性分析明确了关键机制，对模型关系推理能力的研究提供了重要线索。

Abstract: Spatial understanding remains a key challenge in vision-language models. Yet it is still unclear whether such understanding is truly acquired, and if so, through what mechanisms. We present a controllable 1D image-text testbed to probe how left-right relational understanding emerges in Transformer-based vision and text encoders trained with a CLIP-style contrastive objective. We train lightweight Transformer-based vision and text encoders end-to-end on paired descriptions of one- and two-object scenes and evaluate generalization to unseen object pairs while systematically varying label and layout diversity. We find that contrastive training learns left-right relations and that label diversity, more than layout diversity, is the primary driver of generalization in this setting. To gain the mechanistic understanding, we perform an attention decomposition and show that interactions between positional and token embeddings induce a horizontal attention gradient that breaks left-right symmetry in the encoders; ablating this contribution substantially reduces left-right discrimination. Our results provide a mechanistic insight of when and how CLIP-style models acquire relational competence.

</details>


### [145] [CSGaussian: Progressive Rate-Distortion Compression and Segmentation for 3D Gaussian Splatting](https://arxiv.org/abs/2601.12814)
*Yu-Jen Tseng,Chia-Hao Kao,Jing-Zhong Chen,Alessandro Gnutti,Shao-Yuan Lo,Yen-Yu Lin,Wen-Hsiao Peng*

Main category: cs.CV

TL;DR: 本文提出了首个针对3D高斯投影（3DGS）压缩与分割的统一优化框架，能在显著降低传输成本的同时，保持高渲染质量和分割性能。


<details>
  <summary>Details</summary>
Motivation: 3DGS在实时渲染和语义理解中应用广泛，但以往研究通常将压缩和分割任务分开处理，忽视了二者联合优化的潜力。随着压缩技术进展，研究者希望实现能兼顾压缩效率与场景分割能力的新技术，支持如场景编辑等端侧应用。

Method: 该方法提出了以隐式神经表示为基础的轻量超先验（hyperprior），实现了对颜色与语义属性的高效熵编码，同时避免了传统网格超先验的高代价。此外，引入了压缩引导的分割学习机制，包括量化感知训练提升特征可分性和基于质量的权重机制抑制不可靠高斯基元。

Result: 在LERF和3D-OVS数据集上的实验表明，该方法在大幅降低数据传输成本的同时，仍然保持了高渲染质量和优秀分割表现。

Conclusion: 该工作实现了3DGS压缩与分割任务的统一优化，为基于解码端的场景编辑和操控等应用提供了高效的支持，并在实际数据集上验证了其有效性。

Abstract: We present the first unified framework for rate-distortion-optimized compression and segmentation of 3D Gaussian Splatting (3DGS). While 3DGS has proven effective for both real-time rendering and semantic scene understanding, prior works have largely treated these tasks independently, leaving their joint consideration unexplored. Inspired by recent advances in rate-distortion-optimized 3DGS compression, this work integrates semantic learning into the compression pipeline to support decoder-side applications--such as scene editing and manipulation--that extend beyond traditional scene reconstruction and view synthesis. Our scheme features a lightweight implicit neural representation-based hyperprior, enabling efficient entropy coding of both color and semantic attributes while avoiding costly grid-based hyperprior as seen in many prior works. To facilitate compression and segmentation, we further develop compression-guided segmentation learning, consisting of quantization-aware training to enhance feature separability and a quality-aware weighting mechanism to suppress unreliable Gaussian primitives. Extensive experiments on the LERF and 3D-OVS datasets demonstrate that our approach significantly reduces transmission cost while preserving high rendering quality and strong segmentation performance.

</details>


### [146] [A Generalist Foundation Model for Total-body PET/CT Enables Diagnostic Reporting and System-wide Metabolic Profiling](https://arxiv.org/abs/2601.12820)
*Wei Chen,Liang Wu,Shuyi Lu,Yuanyuan Sun,Wenkai Bi,Zilong Yuan,Yaoyao He,Feng Wang,Junchi Ma,Shuyong Liu,Zhaoping Cheng,Xiaoyan Hu,Jianfeng Qiu*

Main category: cs.CV

TL;DR: SDF-HOLO是一种针对全身PET/CT的多模态基础模型，采用双流融合结构，实现了系统级分子成像的高效分析，在各种诊断任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有医学AI模型主要针对单模态输入、局部视野和粗略的图文对齐，难以处理全身PET/CT的异质信号、广阔范围和结构化语义；需要一种能够整合多模态、全身信息并精准语义对齐的新模型。

Method: 提出SDF-HOLO模型，采用双流编码器分别学习CT和PET特征，通过跨模态互作模块进行融合。采用分层上下文建模结合局部窗口和全局注意力，利用解剖分割掩码作为语义锚点，在预训练中进行体素-掩码-文本的对齐。

Result: SDF-HOLO在肿瘤分割、低剂量病变检测和多语种报告生成等任务上均超越了现有强基线，并减少了定位错误与幻觉发现。

Conclusion: SDF-HOLO为全身PET/CT提供了系统级分子成像的计算基础，可实现全身代谢特征分析及肿瘤相关代谢网络交互模式的识别，助力精准肿瘤学诊断。

Abstract: Total-body PET/CT enables system-wide molecular imaging, but heterogeneous anatomical and metabolic signals, approximately 2 m axial coverage, and structured radiology semantics challenge existing medical AI models that assume single-modality inputs, localized fields of view, and coarse image-text alignment. We introduce SDF-HOLO (Systemic Dual-stream Fusion Holo Model), a multimodal foundation model for holistic total-body PET/CT, pre-trained on more than 10,000 patients. SDF-HOLO decouples CT and PET representation learning with dual-stream encoders and couples them through a cross-modal interaction module, allowing anatomical context to refine PET aggregation while metabolic saliency guides subtle morphological reasoning. To model long-range dependencies across the body, hierarchical context modeling combines efficient local windows with global attention. To bridge voxels and clinical language, we use anatomical segmentation masks as explicit semantic anchors and perform voxel-mask-text alignment during pre-training. Across tumor segmentation, low-dose lesion detection, and multilingual diagnostic report generation, SDF-HOLO outperforms strong task-specific and clinical-reference baselines while reducing localization errors and hallucinated findings. Beyond focal interpretation, the model enables system-wide metabolic profiling and reveals tumor-associated fingerprints of inter-organ metabolic network interactions, providing a scalable computational foundation for total-body PET/CT diagnostics and system-level precision oncology.

</details>


### [147] [TreeDGS: Aerial Gaussian Splatting for Distant DBH Measurement](https://arxiv.org/abs/2601.12823)
*Belal Shaheen,Minh-Hieu Nguyen,Bach-Thuan Bui,Shubham,Tim Wu,Michael Fairley,Matthew David Zane,Michael Wu,James Tompkin*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D高斯斑点(3D Gaussian Splatting, DGS)的空中遥感森林树干测量方法TreeDGS，在较远距离和低分辨率条件下仍能准确测量胸径(DBH)，性能优于现有LiDAR技术。


<details>
  <summary>Details</summary>
Motivation: 尽管3D视觉如NeRF和DGS极大提升了重建精度，但遥感影像中森林树干因距离远、分辨率低，传统方法难以直接精确测量胸径(DBH)。亟需一种能在稀疏、远距离视角下准确重建与测量树干胸径的新方法。

Method: 提出了TreeDGS方法。1)首先通过结构光束匀称法和多视图立体(SfM-MVS)进行初始化与高斯分布优化；2)再利用RaDe-GS的深度感知累计不透明度积分从高斯场提取高密度点集，并为每个点关联多视角可信度；3)最后对分离出的树干点云，通过不透明度加权的实心圆最小二乘拟合算法估算DBH。

Result: 在10个已知胸径的样地上测试，TreeDGS达到了4.79cm RMSE（约2.6像素），优于当前主流的LiDAR方法（7.91cm RMSE），显示出更高的精度和低成本潜力。

Conclusion: TreeDGS用高斯斑点致密几何重建实现了高精度、低成本的森林树干遥感胸径测量，为航空遥感林业测量提供了新途径，可替代或补充昂贵的LiDAR准确测量。

Abstract: Aerial remote sensing enables efficient large-area surveying, but accurate direct object-level measurement remains difficult in complex natural scenes. Recent advancements in 3D vision, particularly learned radiance-field representations such as NeRF and 3D Gaussian Splatting, have begun to raise the ceiling on reconstruction fidelity and densifiable geometry from posed imagery. Nevertheless, direct aerial measurement of important natural attributes such as tree diameter at breast height (DBH) remains challenging. Trunks in aerial forest scans are distant and sparsely observed in image views: at typical operating altitudes, stems may span only a few pixels. With these constraints, conventional reconstruction methods leave breast-height trunk geometry weakly constrained. We present TreeDGS, an aerial image reconstruction method that leverages 3D Gaussian Splatting as a continuous, densifiable scene representation for trunk measurement. After SfM-MVS initialization and Gaussian optimization, we extract a dense point set from the Gaussian field using RaDe-GS's depth-aware cumulative-opacity integration and associate each sample with a multi-view opacity reliability score. We then estimate DBH from trunk-isolated points using opacity-weighted solid-circle fitting. Evaluated on 10 plots with field-measured DBH, TreeDGS reaches 4.79,cm RMSE (about 2.6 pixels at this GSD) and outperforms a state-of-the-art LiDAR baseline (7.91,cm RMSE), demonstrating that densified splat-based geometry can enable accurate, low-cost aerial DBH measurement.

</details>


### [148] [Seeing Isn't Always Believing: Analysis of Grad-CAM Faithfulness and Localization Reliability in Lung Cancer CT Classification](https://arxiv.org/abs/2601.12826)
*Teerapong Panboonyuen*

Main category: cs.CV

TL;DR: 本文评估了主流可解释人工智能技术Grad-CAM在肺癌医学影像分类中的可靠性，发现其解释并不总能反映模型真实的决策机制，尤其对Vision Transformer等结构效果较差。


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络在医学影像分析中的应用增加，医生和研究人员对其决策过程的可解释性需求不断增强。Grad-CAM等热力图可视化方法广受欢迎，但其解释的真实性和可靠性受到了质疑。因此，作者希望系统性地评估Grad-CAM是否真正体现了深度模型的内部推理机制。

Method: 作者基于公开的IQ-OTH/NCCD医学影像数据，对ResNet-50、ResNet-101、DenseNet-161、EfficientNet-B0和ViT-Base-Patch16-224五种主流架构进行对比实验。提出定量评估框架，从定位准确率、扰动后忠实度和解释一致性三方面评测Grad-CAM在不同模型上的表现。

Result: 实验表明，Grad-CAM在卷积神经网络中能较好地高亮肿瘤区域，但在Vision Transformer一类非局部注意力模型的解释表现显著下降。跨模型对比发现，Grad-CAM的显著性定位结果差异大，说明其解释结果不总能反映模型实际使用的诊断依据。

Conclusion: 现有基于显著性图的可解释AI方法在医学影像领域具有重要但有限的意义。应发展更适合不同模型结构、且在计算和临床上都更为可靠的解释方法。研究结果提醒医疗AI领域需更加谨慎严格地采用可视化解释工具。

Abstract: Explainable Artificial Intelligence (XAI) techniques, such as Gradient-weighted Class Activation Mapping (Grad-CAM), have become indispensable for visualizing the reasoning process of deep neural networks in medical image analysis. Despite their popularity, the faithfulness and reliability of these heatmap-based explanations remain under scrutiny. This study critically investigates whether Grad-CAM truly represents the internal decision-making of deep models trained for lung cancer image classification. Using the publicly available IQ-OTH/NCCD dataset, we evaluate five representative architectures: ResNet-50, ResNet-101, DenseNet-161, EfficientNet-B0, and ViT-Base-Patch16-224, to explore model-dependent variations in Grad-CAM interpretability. We introduce a quantitative evaluation framework that combines localization accuracy, perturbation-based faithfulness, and explanation consistency to assess Grad-CAM reliability across architectures. Experimental findings reveal that while Grad-CAM effectively highlights salient tumor regions in most convolutional networks, its interpretive fidelity significantly degrades for Vision Transformer models due to non-local attention behavior. Furthermore, cross-model comparisons indicate substantial variability in saliency localization, implying that Grad-CAM explanations may not always correspond to the true diagnostic evidence used by the networks. This work exposes critical limitations of current saliency-based XAI approaches in medical imaging and emphasizes the need for model-aware interpretability methods that are both computationally sound and clinically meaningful. Our findings aim to inspire a more cautious and rigorous adoption of visual explanation tools in medical AI, urging the community to rethink what it truly means to "trust" a model's explanation.

</details>


### [149] [FGTBT: Frequency-Guided Task-Balancing Transformer for Unified Facial Landmark Detection](https://arxiv.org/abs/2601.12863)
*Jun Wan,Xinyu Xiong,Ning Chen,Zhihui Lai,Jie Zhou,Wenwen Min*

Main category: cs.CV

TL;DR: 本文提出了一种基于变换器的新型面部关键点检测方法FGTBT，通过引入频域建模和多数据集任务平衡，有效提升了在复杂场景下的检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习面部关键点检测方法在大姿态变化、光照变化及表情变化下性能下降，且数据集规模和多样性不足，限制了检测精度提升。

Method: 提出了频率引导的任务平衡变换器（FGTBT），主要包括：1）创新性地引入精细化多任务平衡损失（FMB-loss），根据各关键点在不同数据集出现频率分配权重，实现更精细化训练和更一致的梯度更新；2）设计了频率引导的结构感知模型（FGSA），利用频域特征注入与正则化，提升模型对面部结构的感知能力。

Result: 在流行的标准面部关键点检测数据集上进行大量实验，集成所提出的FMB-loss及FGSA模块的FGTBT框架性能达到或优于当前最先进方法。

Conclusion: FGTBT有效提升了模型对复杂面部结构的检测能力，推进了面部关键点检测领域的研究进展。

Abstract: Recently, deep learning based facial landmark detection (FLD) methods have achieved considerable success. However, in challenging scenarios such as large pose variations, illumination changes, and facial expression variations, they still struggle to accurately capture the geometric structure of the face, resulting in performance degradation. Moreover, the limited size and diversity of existing FLD datasets hinder robust model training, leading to reduced detection accuracy. To address these challenges, we propose a Frequency-Guided Task-Balancing Transformer (FGTBT), which enhances facial structure perception through frequency-domain modeling and multi-dataset unified training. Specifically, we propose a novel Fine-Grained Multi-Task Balancing loss (FMB-loss), which moves beyond coarse task-level balancing by assigning weights to individual landmarks based on their occurrence across datasets. This enables more effective unified training and mitigates the issue of inconsistent gradient magnitudes. Additionally, a Frequency-Guided Structure-Aware (FGSA) model is designed to utilize frequency-guided structure injection and regularization to help learn facial structure constraints. Extensive experimental results on popular benchmark datasets demonstrate that the integration of the proposed FMB-loss and FGSA model into our FGTBT framework achieves performance comparable to state-of-the-art methods. The code is available at https://github.com/Xi0ngxinyu/FGTBT.

</details>


### [150] [Proxy Robustness in Vision Language Models is Effortlessly Transferable](https://arxiv.org/abs/2601.12865)
*Xiaowei Fu,Fuxiang Huang,Lei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法HPT-GPD，在无需构建高资源消耗的对抗鲁棒教师模型的情况下，实现视觉-语言模型的对抗鲁棒性迁移，并在零样本数据集上取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 传统的对抗鲁棒性迁移在视觉-语言模型（如CLIP）中面临计算资源消耗巨大、实现难度高的问题。为解决无需高成本对抗鲁棒教师也能提升模型鲁棒性的需求，作者探索了CLIP间固有对抗鲁棒性转移现象。

Method: 作者发现不同架构的CLIP互相生成的对抗样本对另一个CLIP存在天然防御能力，称之为proxy adversarial robustness，并据此提出了Heterogeneous Proxy Transfer（HPT）框架进行鲁棒性迁移。为克服该迁移在零样本泛化上的过拟合问题，又设计了Generalization-Pivot Decoupling（GPD）方法，通过学习率调度，将迁移过程分为泛化保持和鲁棒性提升两个阶段，以平衡鲁棒性与泛化能力。

Result: 在15个零样本数据集上展开了大规模实验，验证了HPT-GPD方法在提升对抗鲁棒性的同时保持了较好的自然泛化能力。

Conclusion: HPT-GPD为视觉-语言模型的鲁棒性迁移提供了一种无需高资源消耗、易于实现且效果显著的解决方案，为相关领域拓展了新思路。

Abstract: As a pivotal technique for improving the defense of deep models, adversarial robustness transfer via distillation has demonstrated remarkable success in conventional image classification tasks. However, this paradigm encounters critical challenges when applied to vision-language models (VLM) (e.g., CLIP): constructing adversarially robust teacher for large-scale multi-modal models demands prohibitively high computational resources. We bridge this gap by revealing an interesting phenomenon: vanilla CLIP (without adversarial training) exhibits intrinsic defensive capabilities against adversarial examples generated by another CLIP with different architectures. We formally define this as proxy adversarial robustness, and naturally propose a Heterogeneous Proxy Transfer (HPT) framework that establishes cross-architectural robustness distillation channels between CLIP variants, effortlessly enabling the VLM robustness transfer from proxy to target models. Yet, such proxy transfer paradigm easily induces severe overfitting, leading to a sharp degradation in zero-shot natural generalization. To resolve that, we design Generalization-Pivot Decoupling (GPD) by leveraging the difference in learning rate scheduling. This decouples the proxy transfer process into a generalization-anchored warm-up that maintains generalization and a generalization-pulled HPT that promotes adversarial robustness, to achieve an equilibrium between natural generalization and adversarial robustness. Extensive experiments on 15 zero-shot datasets demonstrate the effectiveness of our HPT-GPD method. The code is available at the website of github.com/fxw13/HPT-GPD.

</details>


### [151] [Exploring Talking Head Models With Adjacent Frame Prior for Speech-Preserving Facial Expression Manipulation](https://arxiv.org/abs/2601.12876)
*Zhenxuan Lu,Zhihua Xu,Zhijing Yang,Feng Gao,Yongyi Lu,Keze Wang,Tianshui Chen*

Main category: cs.CV

TL;DR: 本文提出了一种结合音频驱动说话人生成（AD-THG）模型与表情操作方法的全新框架THFEM，提高了在更改面部表情时口型同步的准确性。


<details>
  <summary>Details</summary>
Motivation: 目前表情操作需要改变面部表情但保持原口型，然而由于表情与口型的复杂关系，口唇同步仍有困难。作者注意到音频驱动说话人生成模型在合成精确口型方面性能强，想借助该类模型解决表情操作中的口型同步难问题。

Method: 提出将AD-THG模型和SPFEM结合，设计了一个名为Talking Head Facial Expression Manipulation（THFEM）的新框架。框架会以音频驱动的方式生成具有准确口型同步的序列帧，并解决多帧生成时表情真实性下降的问题，引入了相邻帧学习策略，通过让模型预测连续帧以提升最终图像质量。

Result: 实验表明，该框架在进行表情操作时能有效保持口型形状，同时提升了表情操作的真实性和表达一致性。

Conclusion: 将音频驱动说话人生成模型与表情操作方法结合，能够极大提升表情操作时的口型保持及整体效果，显示了该集成思路的巨大优势。

Abstract: Speech-Preserving Facial Expression Manipulation (SPFEM) is an innovative technique aimed at altering facial expressions in images and videos while retaining the original mouth movements. Despite advancements, SPFEM still struggles with accurate lip synchronization due to the complex interplay between facial expressions and mouth shapes. Capitalizing on the advanced capabilities of audio-driven talking head generation (AD-THG) models in synthesizing precise lip movements, our research introduces a novel integration of these models with SPFEM. We present a new framework, Talking Head Facial Expression Manipulation (THFEM), which utilizes AD-THG models to generate frames with accurately synchronized lip movements from audio inputs and SPFEM-altered images. However, increasing the number of frames generated by AD-THG models tends to compromise the realism and expression fidelity of the images. To counter this, we develop an adjacent frame learning strategy that finetunes AD-THG models to predict sequences of consecutive frames. This strategy enables the models to incorporate information from neighboring frames, significantly improving image quality during testing. Our extensive experimental evaluations demonstrate that this framework effectively preserves mouth shapes during expression manipulations, highlighting the substantial benefits of integrating AD-THG with SPFEM.

</details>


### [152] [YOLO26: An Analysis of NMS-Free End to End Framework for Real-Time Object Detection](https://arxiv.org/abs/2601.12882)
*Sudip Chakrabarty*

Main category: cs.CV

TL;DR: YOLOv26通过去除非极大值抑制（NMS），采用端到端策略，显著提升检测速度和精度，成为最新实时目标检测的代表。


<details>
  <summary>Details</summary>
Motivation: 传统YOLO（v1~v11）受限于NMS带来的延迟和超参数敏感性，难以进一步提升实时性和精度。因此，作者希望突破这一瓶颈，实现更快、更准的检测。

Method: YOLOv26取消了NMS后处理，采用端到端的学习方式。提出了MuSGD优化器以稳定轻量级主干、STAL用于小目标分配、ProgLoss实现动态监督。这些技术共同支撑了无NMS网络的高效训练与推理。

Result: 在官方基准测试中，YOLOv26无论是在推理速度还是检测精度上均超越了前代YOLO及其他先进方法（如RTMDet、DAMO-YOLO），建立了新的性能前沿。

Conclusion: 通过剥离传统后处理，YOLOv26解决了实时检测中速度与精度的矛盾，推动了边缘视觉计算的范式革新。

Abstract: The "You Only Look Once" (YOLO) framework has long served as the benchmark for real-time object detection, yet traditional iterations (YOLOv1 through YOLO11) remain constrained by the latency and hyperparameter sensitivity of Non-Maximum Suppression (NMS) post-processing. This paper analyzes a comprehensive analysis of YOLO26, an architecture that fundamentally redefines this paradigm by eliminating NMS in favor of a native end-to-end learning strategy. This study examines the critical innovations that enable this transition, specifically the introduction of the MuSGD optimizer for stabilizing lightweight backbones, STAL for small-target-aware assignment, and ProgLoss for dynamic supervision. Through a systematic review of official performance benchmarks, the results demonstrate that YOLO26 establishes a new Pareto front, outperforming a comprehensive suite of predecessors and state-of-the-art competitors (including RTMDet and DAMO-YOLO) in both inference speed and detection accuracy. The analysis confirms that by decoupling representation learning from heuristic post-processing, YOLOv26 successfully resolves the historical trade-off between latency and precision, signaling the next evolutionary step in edge-based computer vision.

</details>


### [153] [Simultaneous Detection of LSD and FMD in Cattle Using Ensemble Deep Learning](https://arxiv.org/abs/2601.12889)
*Nazibul Basar Ayon,Abdul Hasib,Md. Faishal Ahmed,Md. Sadiqur Rahman,Kamrul Islam,T. M. Mehrab Hasan,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: 本论文提出了一种集成深度学习框架，实现了对牛的皮肤结节病（LSD）和口蹄疫（FMD）两种高传染性疾病的同时高准确率自动检测。


<details>
  <summary>Details</summary>
Motivation: LSD和FMD都是威胁养牛业的高传染性病毒病，症状之间及与良性病变的重叠使得人工诊断困难，延误防控时机，因此急需高效、自动化的辅助诊断工具。

Method: 收集了来自印度、巴西和美国18个农场共10516张专家标注的牛病变图片，提出了集成VGG16、ResNet50和InceptionV3三种CNN模型并结合加权平均的深度学习框架，实现对LSD和FMD的自动识别。

Result: 提出的方法在检测LSD和FMD的任务上取得了98.2%的准确率、98.2%的宏平均精确率、98.1%的召回率、98.1%的F1分数，AUC-ROC达99.5%，优于现有方法。

Conclusion: 所提集成深度学习框架有效克服了多病症状重叠难题，能够实现早期、精准和自动化诊断，具备支持资源有限地区部署的潜力，有助于提升疾病管理和农业可持续性。

Abstract: Lumpy Skin Disease (LSD) and Foot-and-Mouth Disease (FMD) are highly contagious viral diseases affecting cattle, causing significant economic losses and welfare challenges. Their visual diagnosis is complicated by significant symptom overlap with each other and with benign conditions like insect bites or chemical burns, hindering timely control measures. Leveraging a comprehensive dataset of 10,516 expert-annotated images from 18 farms across India, Brazil, and the USA, this study presents a novel Ensemble Deep Learning framework integrating VGG16, ResNet50, and InceptionV3 with optimized weighted averaging for simultaneous LSD and FMD detection. The model achieves a state-of-the-art accuracy of 98.2\%, with macro-averaged precision of 98.2\%, recall of 98.1\%, F1-score of 98.1\%, and an AUC-ROC of 99.5\%. This approach uniquely addresses the critical challenge of symptom overlap in multi-disease detection, enabling early, precise, and automated diagnosis. This tool has the potential to enhance disease management, support global agricultural sustainability, and is designed for future deployment in resource-limited settings.

</details>


### [154] [TwoHead-SwinFPN: A Unified DL Architecture for Synthetic Manipulation, Detection and Localization in Identity Documents](https://arxiv.org/abs/2601.12895)
*Chan Naseeb,Adeel Ashraf Cheema,Hassan Sami,Tayyab Afzal,Muhammad Omair,Usman Habib*

Main category: cs.CV

TL;DR: 该论文提出了TwoHead-SwinFPN模型，有效检测和定位身份文档中的合成伪造（如人脸交换和文本修补），在多个指标上表现优异，适合实际部署。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI技术的发展，身份文档遭受深度伪造攻击（如换脸、文本涂改）的风险大大增加。目前对伪造检测的精准性、区域定位和跨设备通用性仍具挑战，亟需高效且准确的检测方案。

Method: 提出TwoHead-SwinFPN模型：以Swin Transformer为主干，结合FPN和UNet解码器，引入CBAM增强特征提取能力；采用双头架构实现二分类检测与精准分割，以不确定性加权实现任务联合优化；基于FantasyIDiap数据集进行实验，同时评估跨语言和多设备泛化能力。

Result: 在FantasyIDiap集上分类准确率达84.31%、AUC达90.78%、伪造区域分割平均Dice分数为57.24%。二分类F1分数达88.61%，模型具有实际落地的运行效率，适合通过FastAPI部署。还进行了消融分析和跨设备泛化测试。

Conclusion: TwoHead-SwinFPN模型在身份文档伪造检测和定位上兼顾准确性和效率，性能优越、适用性强，有望用于多语种和多设备环境下的实际场景。

Abstract: The proliferation of sophisticated generative AI models has significantly escalated the threat of synthetic manipulations in identity documents, particularly through face swapping and text inpainting attacks. This paper presents TwoHead-SwinFPN, a unified deep learning architecture that simultaneously performs binary classification and precise localization of manipulated regions in ID documents. Our approach integrates a Swin Transformer backbone with Feature Pyramid Network (FPN) and UNet-style decoder, enhanced with Convolutional Block Attention Module (CBAM) for improved feature representation. The model employs a dual-head architecture for joint optimization of detection and segmentation tasks, utilizing uncertainty-weighted multi-task learning. Extensive experiments on the FantasyIDiap dataset demonstrate superior performance with 84.31\% accuracy, 90.78\% AUC for classification, and 57.24\% mean Dice score for localization. The proposed method achieves an F1-score of 88.61\% for binary classification while maintaining computational efficiency suitable for real-world deployment through FastAPI implementation. Our comprehensive evaluation includes ablation studies, cross-device generalization analysis, and detailed performance assessment across 10 languages and 3 acquisition devices.

</details>


### [155] [Supervision-by-Hallucination-and-Transfer: A Weakly-Supervised Approach for Robust and Precise Facial Landmark Detection](https://arxiv.org/abs/2601.12919)
*Jun Wan,Yuanzhi Yao,Zhihui Lai,Jie Zhou,Xianxu Hou,Wenwen Min*

Main category: cs.CV

TL;DR: 该论文提出了一种新的弱监督框架SHT来提升低分辨率或压缩情况下的人脸关键点检测精度，通过人脸重建与姿态迁移协同优化特征学习，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 面对低分辨率图像和压缩导致的特征损失，以及训练数据不足和标注不准确等挑战，传统高精度人脸关键点检测方法表现受限。为解决这些实际困难，需要新的方法提升鲁棒性和准确性。

Method: 作者提出了Supervision-by-Hallucination-and-Transfer (SHT)弱监督框架，其中包括Dual Hallucination Learning Network (DHLN)用于在人脸超分与关键点检测任务间协同学习，从低分辨率图像恢复高分辨率结构和局部细节，同时提升关键点热力图的准确性。再通过Facial Pose Transfer Network (FPTN)进行姿态迁移，进一步优化DHLN生成的脸部图像和热力图，提高标注精度。

Result: 在面部重建和人脸关键点检测实验中，所提方法明显优于最新的其他方法。

Conclusion: 融合人脸超分、姿态迁移和关键点检测的弱监督学习方法，提升了模型对低质量图片、高精度关键点检测的适应能力，为实际应用提供了更强鲁棒性和准确性的解决方案。

Abstract: High-precision facial landmark detection (FLD) relies on high-resolution deep feature representations. However, low-resolution face images or the compression (via pooling or strided convolution) of originally high-resolution images hinder the learning of such features, thereby reducing FLD accuracy. Moreover, insufficient training data and imprecise annotations further degrade performance. To address these challenges, we propose a weakly-supervised framework called Supervision-by-Hallucination-and-Transfer (SHT) for more robust and precise FLD. SHT contains two novel mutually enhanced modules: Dual Hallucination Learning Network (DHLN) and Facial Pose Transfer Network (FPTN). By incorporating FLD and face hallucination tasks, DHLN is able to learn high-resolution representations with low-resolution inputs for recovering both facial structures and local details and generating more effective landmark heatmaps. Then, by transforming faces from one pose to another, FPTN can further improve landmark heatmaps and faces hallucinated by DHLN for detecting more accurate landmarks. To the best of our knowledge, this is the first study to explore weakly-supervised FLD by integrating face hallucination and facial pose transfer tasks. Experimental results of both face hallucination and FLD demonstrate that our method surpasses state-of-the-art techniques.

</details>


### [156] [Dual-Stream Collaborative Transformer for Image Captioning](https://arxiv.org/abs/2601.12926)
*Jun Wan,Jun Liu,Zhihui lai,Jie Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种名为DSCT的双流协同Transformer，有效融合区域和分割特征，通过动态关注机制提升图像描述的相关性和准确性，优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于区域特征的图像描述方法缺乏上下文信息，并容易依赖已生成的部分描述，导致生成无关内容。作者意图弥补上下文信息缺失以及特征语义与空间错配带来的不足。

Method: 提出Dual-Stream Collaborative Transformer（DSCT），融合区域特征和分割特征。模型包含多层Pattern-Specific Mutual Attention Encoders（PSMAEs）及Dynamic Nomination Decoders（DNDs），两者分别通过相互查询巩固私有特征，并动态寻找最相关的学习单元以优化描述生成。

Result: 在多个主流基准数据集上的实验结果显示，DSCT模型在描述准确性和相关性方面均超越了现有的最优图像描述模型。

Conclusion: 引入分割特征并动态融合多模态信息后的DSCT模型，有效提升了图像描述的质量和多样性，验证了该方法在图像描述任务中的优越性和创新性。

Abstract: Current region feature-based image captioning methods have progressed rapidly and achieved remarkable performance. However, they are still prone to generating irrelevant descriptions due to the lack of contextual information and the over-reliance on generated partial descriptions for predicting the remaining words. In this paper, we propose a Dual-Stream Collaborative Transformer (DSCT) to address this issue by introducing the segmentation feature. The proposed DSCT consolidates and then fuses the region and segmentation features to guide the generation of caption sentences. It contains multiple Pattern-Specific Mutual Attention Encoders (PSMAEs) and Dynamic Nomination Decoders (DNDs). The PSMAE effectively highlights and consolidates the private information of two representations by querying each other. The DND dynamically searches for the most relevant learning blocks to the input textual representations and exploits the homogeneous features between the consolidated region and segmentation features to generate more accurate and descriptive caption sentences. To the best of our knowledge, this is the first study to explore how to fuse different pattern-specific features in a dynamic way to bypass their semantic inconsistencies and spatial misalignment issues for image captioning. The experimental results from popular benchmark datasets demonstrate that our DSCT outperforms the state-of-the-art image captioning models in the literature.

</details>


### [157] [Membership Inference Test: Auditing Training Data in Object Classification Models](https://arxiv.org/abs/2601.12929)
*Gonzalo Mancera,Daniel DeAlcala,Aythami Morales,Ruben Tolosana,Julian Fierrez*

Main category: cs.CV

TL;DR: 本文针对物体识别领域中的成员推断测试（MINT）进行了性能分析，并提出了适用于该领域的新型MINT模型架构，通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型广泛应用于图像识别领域，训练数据是否被模型利用成为隐私保护关注的重点。成员推断攻击旨在判定特定数据是否被用作训练，为模型安全提供警示，因此在物体识别中设计高效、精准的MINT模型具有迫切的实际需求。

Method: 作者提出并开发了专门面向物体识别的MINT模型架构，利用卷积层提取与训练相关的数据激活模式。实验中结合目标检测模型、嵌入提取器与MINT模块，在三个公开数据库（共17.4万张图像）上进行了系统测试；并分析了检测模块层深对MINT精度的影响。

Result: 该MINT架构能够有效区分训练与测试所用数据，在不同输入层深度下，判别精度为70%-80%。作者还分析了影响MINT性能的因素，提升了训练流程的可解释性。

Conclusion: 文中提出的架构有效提升了物体识别领域MINT测试的性能和解释能力，为模型隐私保护和透明度提供了有力手段。

Abstract: In this research, we analyze the performance of Membership Inference Tests (MINT), focusing on determining whether given data were utilized during the training phase, specifically in the domain of object recognition. Within the area of object recognition, we propose and develop architectures tailored for MINT models. These architectures aim to optimize performance and efficiency in data utilization, offering a tailored solution to tackle the complexities inherent in the object recognition domain. We conducted experiments involving an object detection model, an embedding extractor, and a MINT module. These experiments were performed in three public databases, totaling over 174K images. The proposed architecture leverages convolutional layers to capture and model the activation patterns present in the data during the training process. Through our analysis, we are able to identify given data used for testing and training, achieving precision rates ranging between 70% and 80%, contingent upon the depth of the detection module layer chosen for input to the MINT module. Additionally, our studies entail an analysis of the factors influencing the MINT Module, delving into the contributing elements behind more transparent training processes.

</details>


### [158] [QASA: Quality-Guided K-Adaptive Slot Attention for Unsupervised Object-Centric Learning](https://arxiv.org/abs/2601.12936)
*Tianran Ouyang,Xingping Dong,Jing Zhang,Mang Ye,Jun Chen,Bo Du*

Main category: cs.CV

TL;DR: 本文提出了一种新的无监督对象中心学习方法QASA，优化了以Slot Attention为基础的K自适应方法，并显著提高了对象绑定和解析质量。


<details>
  <summary>Details</summary>
Motivation: 现有Slot Attention大多假设固定的槽数量K，而K自适应改进虽可应对动态对象数量，但存在槽质量不高及优化目标冲突等问题，导致实际效果不及K固定基线。

Method: QASA方法将槽选择与重构解耦，提出无监督的槽质量度量，并基于此动态挑选高质量槽用于训练阶段的重构，同时设计门控解码器和推理时基于槽竞争机制以自适应输出K。

Result: QASA在真实和合成数据集上均大幅超越现有K自适应方法，并且在真实数据集上优于K固定方法。

Conclusion: 提出的QASA方法有效解决了K自适应Slot Attention中槽质量与优化冲突的问题，为无监督对象中心学习带来更优性能和鲁棒性。

Abstract: Slot Attention, an approach that binds different objects in a scene to a set of "slots", has become a leading method in unsupervised object-centric learning. Most methods assume a fixed slot count K, and to better accommodate the dynamic nature of object cardinality, a few works have explored K-adaptive variants. However, existing K-adaptive methods still suffer from two limitations. First, they do not explicitly constrain slot-binding quality, so low-quality slots lead to ambiguous feature attribution. Second, adding a slot-count penalty to the reconstruction objective creates conflicting optimization goals between reducing the number of active slots and maintaining reconstruction fidelity. As a result, they still lag significantly behind strong K-fixed baselines. To address these challenges, we propose Quality-Guided K-Adaptive Slot Attention (QASA). First, we decouple slot selection from reconstruction, eliminating the mutual constraints between the two objectives. Then, we propose an unsupervised Slot-Quality metric to assess per-slot quality, providing a principled signal for fine-grained slot--object binding. Based on this metric, we design a Quality-Guided Slot Selection scheme that dynamically selects a subset of high-quality slots and feeds them into our newly designed gated decoder for reconstruction during training. At inference, token-wise competition on slot attention yields a K-adaptive outcome. Experiments show that QASA substantially outperforms existing K-adaptive methods on both real and synthetic datasets. Moreover, on real-world datasets QASA surpasses K-fixed methods.

</details>


### [159] [GazeD: Context-Aware Diffusion for Accurate 3D Gaze Estimation](https://arxiv.org/abs/2601.12948)
*Riccardo Catalini,Davide Di Nucci,Guido Borghi,Davide Davoli,Lorenzo Garattoni,Giampiero Francesca,Yuki Kawana,Roberto Vezzani*

Main category: cs.CV

TL;DR: GazeD是一种新提出的3D凝视估计算法，可以从单张RGB图像联合估计3D凝视和人体姿态，基于扩散模型处理不确定性，并在多个公开数据集上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 3D凝视估计对理解人类行为、交互和注意力分析非常重要。以往方法常依靠时序信息或单独估计凝视和姿态，在实际应用中局限性较大，因此作者希望提出一种能在单帧图像上联合估计3D凝视和人体姿态的方法。

Method: GazeD方法基于扩散模型，利用其生成多个可行3D凝视与姿态假设的能力。具体做法是将2D姿态、周围环境和场景上下文作为扩散模型的条件输入，同时提出将3D凝视点视为人体的一个固定距离的新关节点，实现与姿态联合去噪，提高估计精度。

Result: 在三个3D凝视公开基准测试集上，GazeD均实现了最优的性能，甚至优于依赖时序信息的方法。

Conclusion: GazeD在联合3D凝视与人体姿态估计方面表现领先，为3D凝视研究提供了新的思路，并展现了在实际单帧图像应用上的潜力。

Abstract: We introduce GazeD, a new 3D gaze estimation method that jointly provides 3D gaze and human pose from a single RGB image. Leveraging the ability of diffusion models to deal with uncertainty, it generates multiple plausible 3D gaze and pose hypotheses based on the 2D context information extracted from the input image. Specifically, we condition the denoising process on the 2D pose, the surroundings of the subject, and the context of the scene. With GazeD we also introduce a novel way of representing the 3D gaze by positioning it as an additional body joint at a fixed distance from the eyes. The rationale is that the gaze is usually closely related to the pose, and thus it can benefit from being jointly denoised during the diffusion process. Evaluations across three benchmark datasets demonstrate that GazeD achieves state-of-the-art performance in 3D gaze estimation, even surpassing methods that rely on temporal information. Project details will be available at https://aimagelab.ing.unimore.it/go/gazed.

</details>


### [160] [StyMam: A Mamba-Based Generator for Artistic Style Transfer](https://arxiv.org/abs/2601.12954)
*Zhou Hong,Rongsheng Hu,Yicheng Di,Xiaolong Xu,Ning Dong,Yihua Shao,Run Ling,Yun Wang,Juqin Wang,Zhanjie Zhang,Ao Ma*

Main category: cs.CV

TL;DR: 论文提出了一种基于Mamba的新型风格迁移生成器StyMam，能高质量地将艺术风格融入内容图像，克服了以往术中的伪影和结构丢失问题，且推理速度更快。


<details>
  <summary>Details</summary>
Motivation: 现有风格迁移方法（GAN或SD）要么无法兼顾局部与全局特征，产生伪影、风格不协调，要么内容结构保存差且计算慢。作者旨在解决这些不足，提升迁移结果质量和速度。

Method: 提出了StyMam生成器，核心包括mamba架构和两个创新模块：残差双路径条带扫描机制（提升局部纹理处理）与通道重加权空间注意模块（提升全局依赖建模）。

Result: 实验证明，StyMam在风格迁移图像的质量和生成速度方面均优于当前主流方法，且减少了伪影和结构丢失问题。

Conclusion: StyMam有效集成了局部和全局特征建模，可在保证内容结构的同时实现高质量、快速的风格迁移，具有实际推广价值。

Abstract: Image style transfer aims to integrate the visual patterns of a specific artistic style into a content image while preserving its content structure. Existing methods mainly rely on the generative adversarial network (GAN) or stable diffusion (SD). GAN-based approaches using CNNs or Transformers struggle to jointly capture local and global dependencies, leading to artifacts and disharmonious patterns. SD-based methods reduce such issues but often fail to preserve content structures and suffer from slow inference. To address these issues, we revisit GAN and propose a mamba-based generator, termed as StyMam, to produce high-quality stylized images without introducing artifacts and disharmonious patterns. Specifically, we introduce a mamba-based generator with a residual dual-path strip scanning mechanism and a channel-reweighted spatial attention module. The former efficiently captures local texture features, while the latter models global dependencies. Finally, extensive qualitative and quantitative experiments demonstrate that the proposed method outperforms state-of-the-art algorithms in both quality and speed.

</details>


### [161] [Cross-Scale Pretraining: Enhancing Self-Supervised Learning for Low-Resolution Satellite Imagery for Semantic Segmentation](https://arxiv.org/abs/2601.12964)
*John Waithaka,Gustave Bwirayesu,Moise Busogi*

Main category: cs.CV

TL;DR: 本文提出利用高分辨率（HR）影像数据提升中分辨率（MR）遥感图像自监督预训练表现和下游分割性能的方法，并在两种主流自监督框架下实验验证。


<details>
  <summary>Details</summary>
Motivation: 当前遥感自监督预训练主要依赖中分辨率（MR）影像数据，因为数据易获取。但随着高分辨率（HR）数据的发布，作者探索如何将HR数据有效加入预训练，提高MR图像表示和分割任务效果。

Method: 设计了一种空间亲和力组件，可集成进现有自监督学习框架，利用HR影像辅助学习更优的MR图像表征。

Result: 在两个不同的自监督学习框架下验证，提出的空间亲和力组件能显著优于仅用HR或MR单一数据预训练的模型。

Conclusion: 结合HR数据的空间亲和力模块能提升MR数据的自监督表征和下游分割能力，具有实际的推广价值。

Abstract: Self-supervised pretraining in remote sensing is mostly done using mid-spatial resolution (MR) image datasets due to their high availability. Given the release of high-resolution (HR) datasets, we ask how HR datasets can be included in self-supervised pretraining to enhance MR image representation learning and downstream segmentation performance on MR tasks. We design a spatial affinity component that can be added to existing self-supervised learning frameworks and that uses HR imagery to learn better representations of MR imagery. We test the spatial affinity component on two self-supervised learning frameworks and show that it outperforms models pretrained on HR or MR images alone.

</details>


### [162] [Early Prediction of Type 2 Diabetes Using Multimodal data and Tabular Transformers](https://arxiv.org/abs/2601.12981)
*Sulaiman Khan,Md. Rafiul Biswas,Zubair Shah*

Main category: cs.CV

TL;DR: 本研究提出了一种基于TabTrans架构的早期2型糖尿病风险预测方法，可高效分析纵向患者数据，预测表现优于传统方法和主流生成式AI模型。


<details>
  <summary>Details</summary>
Motivation: 鉴于传统方法难以捕捉纵向健康数据中疾病进展的复杂依赖性，亟需发展更精准的2型糖尿病早期风险预测工具，推动个体化医疗干预。

Method: 利用TabTrans（表格转换器）模型，处理卡塔尔生物库1,382名受试者的纵向健康记录与骨密度（DXA）等表格数据。通过SMOTE及SMOTE-ENN技术解决类别不平衡，并与主流生成式AI和传统ML模型进行性能对比。

Result: TabTrans模型的ROC AUC达到≥79.7%，在2型糖尿病风险预测上超越了GPT-4、Claude 3.5 Sonnet、Gemini Pro等生成式AI模型和传统机器学习方法。特征解释显示，内脏脂肪量、骨密度等指标是关键预测因子。

Conclusion: TabTrans模型能精准挖掘复杂表格医疗数据中的疾病风险，助力卡塔尔人群2型糖尿病的主动管理与个体化干预，具有广泛的实践潜力。

Abstract: This study introduces a novel approach for early Type 2 Diabetes Mellitus (T2DM) risk prediction using a tabular transformer (TabTrans) architecture to analyze longitudinal patient data. By processing patients` longitudinal health records and bone-related tabular data, our model captures complex, long-range dependencies in disease progression that conventional methods often overlook. We validated our TabTrans model on a retrospective Qatar BioBank (QBB) cohort of 1,382 subjects, comprising 725 men (146 diabetic, 579 healthy) and 657 women (133 diabetic, 524 healthy). The study integrated electronic health records (EHR) with dual-energy X-ray absorptiometry (DXA) data. To address class imbalance, we employed SMOTE and SMOTE-ENN resampling techniques. The proposed model`s performance is evaluated against conventional machine learning (ML) and generative AI models, including Claude 3.5 Sonnet (Anthropic`s constitutional AI), GPT-4 (OpenAI`s generative pre-trained transformer), and Gemini Pro (Google`s multimodal language model). Our TabTrans model demonstrated superior predictive performance, achieving ROC AUC $\geq$ 79.7 % for T2DM prediction compared to both generative AI models and conventional ML approaches. Feature interpretation analysis identified key risk indicators, with visceral adipose tissue (VAT) mass and volume, ward bone mineral density (BMD) and bone mineral content (BMC), T and Z-scores, and L1-L4 scores emerging as the most important predictors associated with diabetes development in Qatari adults. These findings demonstrate the significant potential of TabTrans for analyzing complex tabular healthcare data, providing a powerful tool for proactive T2DM management and personalized clinical interventions in the Qatari population.
  Index Terms: tabular transformers, multimodal data, DXA data, diabetes, T2DM, feature interpretation, tabular data

</details>


### [163] [AsyncBEV: Cross-modal Flow Alignment in Asynchronous 3D Object Detection](https://arxiv.org/abs/2601.12994)
*Shiming Wang,Holger Caesar,Liangliang Nan,Julian F. P. Kooij*

Main category: cs.CV

TL;DR: 本文提出AsyncBEV模块，可有效提高3D BEV目标检测模型对异步传感器输入的鲁棒性，特别是在自动驾驶场景下提升对动态物体的检测性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要多传感器（如激光雷达、摄像头）协同工作，现有方法严重依赖不同传感器的严格同步。但硬件频率不同、延迟、故障等因素常导致实际传感器数据存在异步，严重影响动态目标的感知，急需鲁棒的解决方案。

Method: 提出了可训练且通用的模块AsyncBEV，通过估算两种传感器（如激光雷达与摄像头）BEV特征的2D场，根据已知的时间偏移对特征进行空间对齐与流形变换，并以插件方式集成到现有BEV检测架构中，无论是网格型还是token型检测器。

Result: 大量实验证明，AsyncBEV对不同类型检测器在传感器异步情况下均显著提升检测性能；尤其对于动态目标，在最大0.5s时间偏移下，NDS性能分别较CMT和UniBEV基线提升了16.6%和11.9%。

Conclusion: AsyncBEV显著提高了3D BEV目标检测模型对传感器异步的适应能力，为自动驾驶等多传感器异步感知场景提供了有效的解决方案。

Abstract: In autonomous driving, multi-modal perception tasks like 3D object detection typically rely on well-synchronized sensors, both at training and inference. However, despite the use of hardware- or software-based synchronization algorithms, perfect synchrony is rarely guaranteed: Sensors may operate at different frequencies, and real-world factors such as network latency, hardware failures, or processing bottlenecks often introduce time offsets between sensors. Such asynchrony degrades perception performance, especially for dynamic objects. To address this challenge, we propose AsyncBEV, a trainable lightweight and generic module to improve the robustness of 3D Birds' Eye View (BEV) object detection models against sensor asynchrony. Inspired by scene flow estimation, AsyncBEV first estimates the 2D flow from the BEV features of two different sensor modalities, taking into account the known time offset between these sensor measurements. The predicted feature flow is then used to warp and spatially align the feature maps, which we show can easily be integrated into different current BEV detector architectures (e.g., BEV grid-based and token-based). Extensive experiments demonstrate AsyncBEV improves robustness against both small and large asynchrony between LiDAR or camera sensors in both the token-based CMT and grid-based UniBEV, especially for dynamic objects. We significantly outperform the ego motion compensated CMT and UniBEV baselines, notably by $16.6$ % and $11.9$ % NDS on dynamic objects in the worst-case scenario of a $0.5 s$ time offset. Code will be released upon acceptance.

</details>


### [164] [Think3D: Thinking with Space for Spatial Reasoning](https://arxiv.org/abs/2601.13029)
*Zaibin Zhang,Yuhan Wu,Lianjie Jia,Yifan Wang,Zhongbo Zhang,Yijiang Li,Binghao Ran,Fuxi Zhang,Zhuohan Sun,Zhenfei Yin,Lijun Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: 本文提出了Think3D框架，使大型视觉模型（VLM）能够通过无训练的方式进行交互式3D空间推理，从而在多项空间推理基准上显著提升了表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLM虽然视觉理解能力强，但主要局限于2D空间，无法对真实3D空间进行有效推理。如何让VLM具备空间智能，实现3D几何理解和空间关系推理，是推动多模态智能发展的重要方向。

Method: 提出Think3D框架，结合3D重建模型，将图像或视频恢复为点云和相机姿态，并实现基于相机操作和视角切换的交互式空间探索，通过不需额外训练的方式提升VLM的3D推理能力。此外，通过强化学习策略，帮助较小模型自动选择有效的视角和操作以增强推理表现。

Result: 在BLINK Multi-view和MindCube基准上，先进模型如GPT-4.1和Gemini 2.5 Pro的空间推理能力提升了平均7.8%，在VSI-Bench提升4.7%。对于较小模型，通过强化学习结合工具辅助，推理表现提升从0.7%提高到6.8%。

Conclusion: 训练自由、工具辅助的空间探索显著增强了多模态智能体的3D推理能力，代表多模态智能发展的新方向。

Abstract: Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent.

</details>


### [165] [GridNet-HD: A High-Resolution Multi-Modal Dataset for LiDAR-Image Fusion on Power Line Infrastructure](https://arxiv.org/abs/2601.13052)
*Antoine Carreaud,Shanci Li,Malo De Lacour,Digre Frinde,Jan Skaloud,Adrien Gressin*

Main category: cs.CV

TL;DR: 本文提出了GridNet-HD，一个用于3D语义分割的多模态数据集，涵盖高密度LiDAR和高分辨率侧视影像，特别针对电力基础设施。数据集包含7,694张图片与25亿个点，分为11类，并提供基线模型与mIoU评估。融合模型相比单一模态在mIoU上提升了5.55，有力证明了多模态的优势。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏同时涵盖高密度LiDAR和高分辨率斜视影像以及3D语义标签的公开数据集，限制了电力线基础设施相关研究及应用发展。

Method: 构建并标注包含大规模点云与影像的多模态数据集，并定义三类基线：LiDAR单模态、影像单模态和融合模型，采用mIoU客观比较不同方法效果。

Result: 融合模型在GridNet-HD数据集上，mIoU指标较最佳单模态基线提升5.55，验证了不同模态信息（几何+外观）的互补性。

Conclusion: GridNet-HD填补了领域空白，为电力线路3D语义分割提供高质量多模态数据、基线与评估标准，有望进一步推动相关研究和实际应用。

Abstract: This paper presents GridNet-HD, a multi-modal dataset for 3D semantic segmentation of overhead electrical infrastructures, pairing high-density LiDAR with high-resolution oblique imagery. The dataset comprises 7,694 images and 2.5 billion points annotated into 11 classes, with predefined splits and mIoU metrics. Unimodal (LiDAR-only, image-only) and multi-modal fusion baselines are provided. On GridNet-HD, fusion models outperform the best unimodal baseline by +5.55 mIoU, highlighting the complementarity of geometry and appearance. As reviewed in Sec. 2, no public dataset jointly provides high-density LiDAR and high-resolution oblique imagery with 3D semantic labels for power-line assets. Dataset, baselines, and codes are available: https://huggingface.co/collections/heig-vd-geo/gridnet-hd.

</details>


### [166] [Prototype Learning-Based Few-Shot Segmentation for Low-Light Crack on Concrete Structures](https://arxiv.org/abs/2601.13059)
*Yulun Guo*

Main category: cs.CV

TL;DR: 本文提出了一种适用于低光照环境混凝土裂缝分割的双分支原型学习网络，有效提升了检测精度并降低了对大规模标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 现实中，许多混凝土结构的裂缝出现在光照不足的环境下（如隧道、桥梁底部），这导致计算机视觉分割精度下降，而对这些低光裂缝图像进行像素级标注非常耗时。深度学习方法通常依赖于大量明亮、标注齐全的数据，亟需高效处理低光照条件裂缝检测的新方法。

Method: 作者提出基于Retinex理论与小样本（few-shot）学习结合的双分支网络。通过Retinex分解获得光照不变的反射分量，辅助学习鲁棒的全局特征表达；采用度量学习减少对大规模标注数据的依赖。此外，引入相关性先验掩码生成模块，以及多尺度特征融合模块，增强空间一致性和对裂缝结构的捕捉。

Result: 在多个基准数据集上的大量实验证明，该方法在低光照条件下的裂缝分割任务中，始终获得了最优或主流的性能表现。

Conclusion: 综合来看，所提网络兼具较强的泛化能力和低标注依赖性，在低光照裂缝检测任务中极具实际应用前景。

Abstract: Crack detection is critical for concrete infrastructure safety, but real-world cracks often appear in low-light environments like tunnels and bridge undersides, degrading computer vision segmentation accuracy. Pixel-level annotation of low-light crack images is extremely time-consuming, yet most deep learning methods require large, well-illuminated datasets. We propose a dual-branch prototype learning network integrating Retinex theory with few-shot learning for low-light crack segmentation. Retinex-based reflectance components guide illumination-invariant global representation learning, while metric learning reduces dependence on large annotated datasets. We introduce a cross-similarity prior mask generation module that computes high-dimensional similarities between query and support features to capture crack location and structure, and a multi-scale feature enhancement module that fuses multi-scale features with the prior mask to alleviate spatial inconsistency. Extensive experiments on multiple benchmarks demonstrate consistent state-of-the-art performance under low-light conditions. Code: https://github.com/YulunGuo/CrackFSS.

</details>


### [167] [Patient-Conditioned Adaptive Offsets for Reliable Diagnosis across Subgroups](https://arxiv.org/abs/2601.13094)
*Gelei Xu,Yuying Duan,Jun Xia,Ruining Deng,Wei Jin,Yiyu Shi*

Main category: cs.CV

TL;DR: 本文提出了一种基于患者个体特征（如年龄、性别）进行模型自适应的方法，能够同时提升医学AI模型在不同人群（子群）上的公平性和诊断性能，尤其改善了对代表性不足群体的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的医学AI诊断模型由于人群特征异质性（如疾病流行率和表现、风险特征等），在不同患者群体间表现不均。传统公平性方法往往通过消除敏感属性以减少差异，但在医学场景下这些信息本身对诊断至关重要，简单去除反而损害了整体诊断性能。因此亟需一种兼顾公平与性能的方法，能够像临床决策一样合理利用患者个体背景信息。

Method: 提出HyperAdapt框架：将年龄、性别等属性编码成紧凑嵌入，用于控制超网络模块，该模块为主模型骨干的特定层生成残差调制参数，实现患者条件化的模型调整。模型通过低秩与瓶颈结构限制调制参数，确保效率和鲁棒性。

Result: 在多个医学影像公开基准上，HyperAdapt在各子群的表现上平均获得提升，且整体准确率无损。以PAD-UFES-20数据集为例，召回率提升4.1%，F1分数提升4.4%，在代表性不足的群体上提升更显著。

Conclusion: HyperAdapt框架能够在不牺牲总体性能的前提下，提升AI诊断模型在不同患者子群间的公正性与可靠性，为医学AI领域子群感知算法实践提供了新范式。

Abstract: AI models for medical diagnosis often exhibit uneven performance across patient populations due to heterogeneity in disease prevalence, imaging appearance, and clinical risk profiles. Existing algorithmic fairness approaches typically seek to reduce such disparities by suppressing sensitive attributes. However, in medical settings these attributes often carry essential diagnostic information, and removing them can degrade accuracy and reliability, particularly in high-stakes applications. In contrast, clinical decision making explicitly incorporates patient context when interpreting diagnostic evidence, suggesting a different design direction for subgroup-aware models. In this paper, we introduce HyperAdapt, a patient-conditioned adaptation framework that improves subgroup reliability while maintaining a shared diagnostic model. Clinically relevant attributes such as age and sex are encoded into a compact embedding and used to condition a hypernetwork-style module, which generates small residual modulation parameters for selected layers of a shared backbone. This design preserves the general medical knowledge learned by the backbone while enabling targeted adjustments that reflect patient-specific variability. To ensure efficiency and robustness, adaptations are constrained through low-rank and bottlenecked parameterizations, limiting both model complexity and computational overhead. Experiments across multiple public medical imaging benchmarks demonstrate that the proposed approach consistently improves subgroup-level performance without sacrificing overall accuracy. On the PAD-UFES-20 dataset, our method outperforms the strongest competing baseline by 4.1% in recall and 4.4% in F1 score, with larger gains observed for underrepresented patient populations.

</details>


### [168] [A Streamlined Attention-Based Network for Descriptor Extraction](https://arxiv.org/abs/2601.13126)
*Mattia D'Urso,Emanuele Santellani,Christian Sormann,Mattia Rossi,Andreas Kuhn,Friedrich Fraundorfer*

Main category: cs.CV

TL;DR: SANDesc是一种高效的注意力机制描述子网络，能提升关键点匹配表现且模型较小，实验证明其优于现有方法，并发布了新的城市数据集。


<details>
  <summary>Details</summary>
Motivation: 当前关键点描述子在提升匹配效果和计算效率方面存在局限，且常和特征点检测器耦合，限制了灵活性和性能。该文希望在不改变特征点检测器的前提下，通过改进神经网络结构和训练策略，单独提升描述子的匹配性能和实用性。

Method: 提出了精简注意力机制描述子网络SANDesc，结构上采用改良U-Net，内嵌卷积块注意力模块和残差路径，提升局部表达力和效率。训练时引入改进三元组损失和课程学习启发的困难样本挖掘策略，提升训练稳定性。

Result: 在HPatches、MegaDepth-1500及IMC2021等主流基准上，SANDesc结合主流特征点检测器可显著提高匹配任务准确率，且仅需240万参数，计算资源消耗小。同时，在自建高分辨率城市影像数据集上也远优于现有描述子。

Conclusion: SANDesc无需更改关键点检测器，便可有效提升图像匹配性能且兼具高效率；新发布的数据集为特征点算法研究提供了高质量基准。

Abstract: We introduce SANDesc, a Streamlined Attention-Based Network for Descriptor extraction that aims to improve on existing architectures for keypoint description.
  Our descriptor network learns to compute descriptors that improve matching without modifying the underlying keypoint detector. We employ a revised U-Net-like architecture enhanced with Convolutional Block Attention Modules and residual paths, enabling effective local representation while maintaining computational efficiency. We refer to the building blocks of our model as Residual U-Net Blocks with Attention. The model is trained using a modified triplet loss in combination with a curriculum learning-inspired hard negative mining strategy, which improves training stability.
  Extensive experiments on HPatches, MegaDepth-1500, and the Image Matching Challenge 2021 show that training SANDesc on top of existing keypoint detectors leads to improved results on multiple matching tasks compared to the original keypoint descriptors. At the same time, SANDesc has a model complexity of just 2.4 million parameters.
  As a further contribution, we introduce a new urban dataset featuring 4K images and pre-calibrated intrinsics, designed to evaluate feature extractors. On this benchmark, SANDesc achieves substantial performance gains over the existing descriptors while operating with limited computational resources.

</details>


### [169] [PhaseMark: A Post-hoc, Optimization-Free Watermarking of AI-generated Images in the Latent Frequency Domain](https://arxiv.org/abs/2601.13128)
*Sung Ju Lee,Nam Ik Cho*

Main category: cs.CV

TL;DR: PhaseMark是一种针对潜在扩散模型（LDMs）生成的超逼真图像的高效数字水印方法，不依赖迭代优化，速度极快，并且保持良好的图像质量和抗攻击性。


<details>
  <summary>Details</summary>
Motivation: 随着LDMs生成超逼真图像的应用日益广泛，保障这些图像的版权和溯源需求增强。然而，现有水印方法普遍需要繁琐的优化或逆向处理，导致速度非常慢，难以满足实际需求。

Method: PhaseMark在变分自编码器（VAE）的潜在频域中，以一次性（single-shot）、无优化（optimization-free）的方式直接调制相位，实现数字水印嵌入。相较依赖迭代优化的传统方法，大大提升了嵌入速度。文中还分析了四种不同调制方式，比较性能-质量权衡。

Result: 实验表明，PhaseMark的嵌入速度比基于优化的方法快数千倍，并且在面对包括再生成攻击等多种强攻击手段时，依然表现出业内领先的鲁棒性，同时不降低图像质量。

Conclusion: 该方法展示了利用潜在变量频域的内在属性，可以实现高效且鲁棒的水印嵌入，为高效图像溯源和版权保护开辟了新范式。

Abstract: The proliferation of hyper-realistic images from Latent Diffusion Models (LDMs) demands robust watermarking, yet existing post-hoc methods are prohibitively slow due to iterative optimization or inversion processes. We introduce PhaseMark, a single-shot, optimization-free framework that directly modulates the phase in the VAE latent frequency domain. This approach makes PhaseMark thousands of times faster than optimization-based techniques while achieving state-of-the-art resilience against severe attacks, including regeneration, without degrading image quality. We analyze four modulation variants, revealing a clear performance-quality trade-off. PhaseMark demonstrates a new paradigm where efficient, resilient watermarking is achieved by exploiting intrinsic latent properties.

</details>


### [170] [GaussExplorer: 3D Gaussian Splatting for Embodied Exploration and Reasoning](https://arxiv.org/abs/2601.13132)
*Kim Yu-Ji,Dahye Lee,Kim Jun-Seong,GeonU Kim,Nam Hyeon-Woo,Yongjin Kwon,Yu-Chiang Frank Wang,Jaesung Choe,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: 提出GaussExplorer框架，将3D高斯展开（3DGS）与视觉语言模型（VLM）结合，提升在3D场景中的探索与推理能力，并在多个基准任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于语言的3DGS方法只能处理简单文本查询，难以理解复杂、组合性强的语言问题；而基于RGB-D记忆的研究虽有空间定位能力，但受限于预设视角，灵活性不足。作者希望解决这些局限，实现更强的3D场景理解与推理。

Method: GaussExplorer将视觉语言模型（VLM）与3D高斯展开结合。首先识别与问题最相关的预采集图像，然后将这些图像调整到新的视角，为VLM推理提供更符合问题需求的视觉信息，实现更准确的3D场景探索与推理。

Result: 实验表明，GaussExplorer在多个基准测试中都优于现有方法，证实了集成VLM推理与3DGS对于具身探索任务的有效性。

Conclusion: GaussExplorer通过融合VLM与3DGS框架，提升了3D场景中复杂问题的推理和探索能力，在具身推理任务上表现更优，具有广阔应用前景。

Abstract: We present GaussExplorer, a framework for embodied exploration and reasoning built on 3D Gaussian Splatting (3DGS). While prior approaches to language-embedded 3DGS have made meaningful progress in aligning simple text queries with Gaussian embeddings, they are generally optimized for relatively simple queries and struggle to interpret more complex, compositional language queries. Alternative studies based on object-centric RGB-D structured memories provide spatial grounding but are constrained by pre-fixed viewpoints. To address these issues, GaussExplorer introduces Vision-Language Models (VLMs) on top of 3DGS to enable question-driven exploration and reasoning within 3D scenes. We first identify pre-captured images that are most correlated with the query question, and subsequently adjust them into novel viewpoints to more accurately capture visual information for better reasoning by VLMs. Experiments show that ours outperforms existing methods on several benchmarks, demonstrating the effectiveness of integrating VLM-based reasoning with 3DGS for embodied tasks.

</details>


### [171] [CLIP-Guided Adaptable Self-Supervised Learning for Human-Centric Visual Tasks](https://arxiv.org/abs/2601.13133)
*Mingshuang Luo,Ruibing Hou,Bo Chao,Hong Chang,Zimo Liu,Yaowei Wang,Shiguang Shan*

Main category: cs.CV

TL;DR: 本文提出了一种新的无监督预训练框架CLASP，用于提升以人为中心的视觉分析任务的通用性能。方法基于CLIP生成多层次伪标签，并通过专家模型适配多样下游任务。实验结果显示CLASP优于现有无监督方法。


<details>
  <summary>Details</summary>
Motivation: 随着大量无标注人类图像数据集的出现，对能够支持多种人类中心下游视觉任务的通用无监督预训练模型需求日益增长。现有方法难以兼顾多层次语义信息和不同任务的适配能力。因此，作者提出新方法以提升无监督预训练的泛化性和表达能力。

Method: 提出CLASP框架，借助CLIP模型自动生成身体部位和属性级别的多层次伪标签，将这些标签融合到视觉表征学习中。设计了基于提示条件的多专家混合模块(MoE)，通过下游任务的提示动态调整特征提取过程，减缓特征冲突并提升迁移性能。同时采用多任务预训练方式，以CLIP生成的伪标签联合指导模型学习。

Result: 在多个以人为中心的视觉分析基准数据集上进行实验，结果显示CLASP在性能上持续优于现有的无监督预训练方法，具备更好的表达能力和任务泛化能力。

Conclusion: CLASP框架有效提升了以人为中心无监督视觉预训练方法的性能和通用性，为相关视觉分析任务提供了更强的基础表示与适配能力。

Abstract: Human-centric visual analysis plays a pivotal role in diverse applications, including surveillance, healthcare, and human-computer interaction. With the emergence of large-scale unlabeled human image datasets, there is an increasing need for a general unsupervised pre-training model capable of supporting diverse human-centric downstream tasks. To achieve this goal, we propose CLASP (CLIP-guided Adaptable Self-suPervised learning), a novel framework designed for unsupervised pre-training in human-centric visual tasks. CLASP leverages the powerful vision-language model CLIP to generate both low-level (e.g., body parts) and high-level (e.g., attributes) semantic pseudo-labels. These multi-level semantic cues are then integrated into the learned visual representations, enriching their expressiveness and generalizability. Recognizing that different downstream tasks demand varying levels of semantic granularity, CLASP incorporates a Prompt-Controlled Mixture-of-Experts (MoE) module. MoE dynamically adapts feature extraction based on task-specific prompts, mitigating potential feature conflicts and enhancing transferability. Furthermore, CLASP employs a multi-task pre-training strategy, where part- and attribute-level pseudo-labels derived from CLIP guide the representation learning process. Extensive experiments across multiple benchmarks demonstrate that CLASP consistently outperforms existing unsupervised pre-training methods, advancing the field of human-centric visual analysis.

</details>


### [172] [TVWorld: Foundations for Remote-Control TV Agents](https://arxiv.org/abs/2601.13142)
*Zhantao Ma,Quanfeng Lu,Shuai Zhong,Dahai Yu,Ping Luo,Michael K. Ng*

Main category: cs.CV

TL;DR: 本文关注于大规模视觉-语言模型（LVLMs）在遥控电视操作领域的应用，提出了新的评测基准和训练方法，并开创性地推动了基于遥控的TV导航智能体的研究。


<details>
  <summary>Details</summary>
Motivation: 当前LVLMs在设备控制方面表现突出，但大多集中于点选操作，而日常生活中常见的遥控电视操作却鲜有表述和研究。为弥补该领域研究的空白，作者希望打造兼具现实性和可重复性的评测与模型优化环境，推进TV遥控智能体的发展。

Method: 提出了TVWorld这一基于离线图结构的、可重复的电视导航仿真平台，并在此基础上设计了两套基准（TVWorld-N用于拓扑感知导航，TVWorld-G用于聚焦感知定位）。针对现有模型拓扑感知不足的短板，作者提出了Topology-Aware Training训练框架，注入拓扑知识，最终推出专门用于电视导航的LVLM模型TVTheseus。

Result: TVTheseus模型在TVWorld-N基准测试集上的成功率达到68.3%，优于强劲的闭源基线（如Gemini 3 Flash），达到了当前最优水平。

Conclusion: 本文不仅提出了更贴近真实应用场景的TV遥控基准，而且通过拓扑感知训练方法，显著提升了智能体在复杂操作中的表现，为LVLMs在实际设备控制领域的拓展提供了新思路。

Abstract: Recent large vision-language models (LVLMs) have demonstrated strong potential for device control. However, existing research has primarily focused on point-and-click (PnC) interaction, while remote-control (RC) interaction commonly encountered in everyday TV usage remains largely underexplored. To fill this gap, we introduce \textbf{TVWorld}, an offline graph-based abstraction of real-world TV navigation that enables reproducible and deployment-free evaluation. On this basis, we derive two complementary benchmarks that comprehensively assess TV-use capabilities: \textbf{TVWorld-N} for topology-aware navigation and \textbf{TVWorld-G} for focus-aware grounding. These benchmarks expose a key limitation of existing agents: insufficient topology awareness for focus-based, long-horizon TV navigation. Motivated by this finding, we propose a \emph{Topology-Aware Training} framework that injects topology awareness into LVLMs. Using this framework, we develop \textbf{TVTheseus}, a foundation model specialized for TV navigation. TVTheseus achieves a success rate of $68.3\%$ on TVWorld-N, surpassing strong closed-source baselines such as Gemini 3 Flash and establishing state-of-the-art (SOTA) performance. Additional analyses further provide valuable insights into the development of effective TV-use agents.

</details>


### [173] [ICo3D: An Interactive Conversational 3D Virtual Human](https://arxiv.org/abs/2601.13148)
*Richard Shaw,Youngkyoon Jang,Athanasios Papaioannou,Arthur Moreau,Helisa Dhamo,Zhensong Zhang,Eduardo Pérez-Pellitero*

Main category: cs.CV

TL;DR: ICo3D是一种生成高逼真、可交互3D虚拟人类头像的方法，融合了多视角捕捉、LLM智能对话、动态面部与身体同步动画，并支持实时交互。


<details>
  <summary>Details</summary>
Motivation: 目前高真实感的虚拟人类形象难以实现流畅自然的互动和表情驱动，现有方法难以同时保证照片级真实感、面部与身体的自然融合及支持实时对话等多项需求。该工作旨在解决这些挑战，提供高质量、可互动的3D虚拟人形象。

Method: 作者提出了通过多视角捕捉生成可动画化的3D面部和身体模型，并通过高斯体（Gaussian Splatting）渲染从而获得高真实感。将面部模型与身体模型无缝整合，提升自然度。引入LLM（大语言模型）让虚拟人具备对话能力，通过语音驱动面部动画，实现同步。并分别提升了面身体与面部的建模效果（SWinGS++与HeadGaS++），最终形成一套完整的实时互动系统。

Result: 系统实现了自然的3D虚拟人实时互动，包括口头和书面交流，完成了多种场景展示。改进方法显著提升了模型的逼真度、动画同步性和交互性。演示样例展现了系统多领域应用可能性。

Conclusion: ICo3D为虚拟人领域带来了全新一体化、高真实感、可实时互动的新方案，在游戏、虚拟助手和个性化教育等场景有广泛应用前景。

Abstract: This work presents Interactive Conversational 3D Virtual Human (ICo3D), a method for generating an interactive, conversational, and photorealistic 3D human avatar. Based on multi-view captures of a subject, we create an animatable 3D face model and a dynamic 3D body model, both rendered by splatting Gaussian primitives. Once merged together, they represent a lifelike virtual human avatar suitable for real-time user interactions. We equip our avatar with an LLM for conversational ability. During conversation, the audio speech of the avatar is used as a driving signal to animate the face model, enabling precise synchronization. We describe improvements to our dynamic Gaussian models that enhance photorealism: SWinGS++ for body reconstruction and HeadGaS++ for face reconstruction, and provide as well a solution to merge the separate face and body models without artifacts. We also present a demo of the complete system, showcasing several use cases of real-time conversation with the 3D avatar. Our approach offers a fully integrated virtual avatar experience, supporting both oral and written form interactions in immersive environments. ICo3D is applicable to a wide range of fields, including gaming, virtual assistance, and personalized education, among others. Project page: https://ico3d.github.io/

</details>


### [174] [From 100,000+ images to winning the first brain MRI foundation model challenges: Sharing lessons and models](https://arxiv.org/abs/2601.13166)
*Pedro M. Gordaliza,Jaume Banus,Benoît Gérin,Maxence Wynen,Nataliia Molchanova,Jonas Richiardi,Meritxell Bach Cuadra*

Main category: cs.CV

TL;DR: 本文介绍了一套用于医学图像分析的基础模型，专门针对3D脑部MRI任务，方法简单高效，并在2025年MICCAI的SSL3D和FOMO25竞赛中获得第一。


<details>
  <summary>Details</summary>
Motivation: 医学影像分析任务存在结构复杂、数据稀缺等挑战，尤其是在放射学任务上，开发适应3D脑部MRI的基础模型变得尤为关键，以推动算法应用和性能提升。

Method: 作者基于U-Net卷积神经网络（CNN）架构，结合解剖学先验知识和神经影像领域专业知识，设计了高效的模型训练与推理策略。其方法显著区别于主流的Transformer方法，在模型结构和训练方面更轻量、高效。

Result: 所提出的方法在2025年MICCAI举办的SSL3D和FOMO25两个竞赛的相关赛道中均取得第一，同时模型训练速度比主流Transformer快1到2个数量级，参数量也仅为Transformer的十分之一，显著高效和精简。

Conclusion: 本工作展示了结合领域知识与经典CNN架构在医学影像任务中的强大潜力，为未来基础模型的设计提供了新的高效方向，并有望推动医学影像分析的进一步发展。

Abstract: Developing Foundation Models for medical image analysis is essential to overcome the unique challenges of radiological tasks. The first challenges of this kind for 3D brain MRI, SSL3D and FOMO25, were held at MICCAI 2025. Our solution ranked first in tracks of both contests. It relies on a U-Net CNN architecture combined with strategies leveraging anatomical priors and neuroimaging domain knowledge. Notably, our models trained 1-2 orders of magnitude faster and were 10 times smaller than competing transformer-based approaches. Models are available here: https://github.com/jbanusco/BrainFM4Challenges.

</details>


### [175] [GTPred: Benchmarking MLLMs for Interpretable Geo-localization and Time-of-capture Prediction](https://arxiv.org/abs/2601.13207)
*Jinnao Li,Zijian Chen,Tingzhu Chen,Changbo Wang*

Main category: cs.CV

TL;DR: 该论文提出了GTPred基准，用于评估多模态大语言模型（MLLMs）在地理和时间联合推理（geo-temporal prediction）任务中的表现。实验表明，现有MLLM受限于世界知识，加入时间信息能有效提升定位能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于MLLM的地理定位研究忽略了图片中蕴含的时间线索，但实际上，时间信息有助于更准确地确定地理位置。作者提出研究如何将时间因素融入位置推理，提高推理的准确性和可解释性。

Method: 作者建立了GTPred基准，收集了跨度超过120年的370张全球分布图片，并细致标注了年份、层次化地理位置信息和推理链。采用年和层次地址联合匹配、推理过程评测等新颖评测方法，分别在8个专有及7个开源MLLMs上测试模型表现。

Result: 实验发现，现有MLLM视觉感知能力较强，但对世界知识和地时联合推理能力有限。引入时间信息可显著提升地理定位准确率。此外，新基准可用于进一步分析和提升模型推理链能力。

Conclusion: GTPred基准验证了时间信息对地理定位任务的重要性。现有MLLM在该任务上还存在较大提升空间，今后需加强模型的世界知识积累及联动推理能力，利用GTPred基准有望推动相关领域发展。

Abstract: Geo-localization aims to infer the geographic location where an image was captured using observable visual evidence. Traditional methods achieve impressive results through large-scale training on massive image corpora. With the emergence of multi-modal large language models (MLLMs), recent studies have explored their applications in geo-localization, benefiting from improved accuracy and interpretability. However, existing benchmarks largely ignore the temporal information inherent in images, which can further constrain the location. To bridge this gap, we introduce GTPred, a novel benchmark for geo-temporal prediction. GTPred comprises 370 globally distributed images spanning over 120 years. We evaluate MLLM predictions by jointly considering year and hierarchical location sequence matching, and further assess intermediate reasoning chains using meticulously annotated ground-truth reasoning processes. Experiments on 8 proprietary and 7 open-source MLLMs show that, despite strong visual perception, current models remain limited in world knowledge and geo-temporal reasoning. Results also demonstrate that incorporating temporal information significantly enhances location inference performance.

</details>


### [176] [Rethinking Skip Connections: Additive U-Net for Robust and Interpretable Denoising](https://arxiv.org/abs/2601.13208)
*Vikram R Lakkavalli*

Main category: cs.CV

TL;DR: 该论文提出了加法式U-Net（Additive U-Net），替换了传统U-Net架构中的拼接跳连为具有可控制权重的加法跳连。在Kodak-17去噪基准测试上，加法式U-Net表现出较强的去噪能力和鲁棒性，并且无需显式下采样或强制分层结构。


<details>
  <summary>Details</summary>
Motivation: U-Net广泛用于图像去噪，但标准的跳连通过拼接会导致通道数倍增，信息流动不够清晰，且有可能增加噪声传播。作者希望解决这些结构性弊端，提高模型效率和可解释性。

Method: 将U-Net中的拼接跳连用带可学习且非负的标量门控加法跳连取代，每个跳连接通路单独缩放，从而既控制了信息流动，也避免了通道膨胀的问题。

Result: 在Kodak-17数据集及不同噪声水平（σ = 15, 25, 50）下，新方法在PSNR和SSIM指标上取得了与原U-Net相当的表现，并展现了对核函数和网络深度变化的较强鲁棒性。且无需传统的下采样/上采样，模型仍能学到频率由高到低的特征演化过程。

Conclusion: 加法跳连是一种高效且易解释的替代方案，既能提升网络设计的轻量化，也增强了多尺度特征流动的可控性和透明度，为重建类网络带来更好的解释性和设计灵活性。

Abstract: Skip connections are central to U-Net architectures for image denoising, but standard concatenation doubles channel dimensionality and obscures information flow, allowing uncontrolled noise transfer. We propose the Additive U-Net, which replaces concatenative skips with gated additive connections. Each skip pathway is scaled by a learnable non-negative scalar, offering explicit and interpretable control over encoder contributions while avoiding channel inflation. Evaluations on the Kodak-17 denoising benchmark show that Additive U-Net achieves competitive PSNR/SSIM at noise levels σ = 15, 25, 50, with robustness across kernel schedules and depths. Notably, effective denoising is achieved even without explicit down/up-sampling or forced hierarchies, as the model naturally learns a progression from high-frequency to band-pass to low-frequency features. These results position additive skips as a lightweight and interpretable alternative to concatenation, enabling both efficient design and a clearer understanding of multi-scale information transfer in reconstruction networks.

</details>


### [177] [ObjectVisA-120: Object-based Visual Attention Prediction in Interactive Street-crossing Environments](https://arxiv.org/abs/2601.13218)
*Igor Vozniak,Philipp Mueller,Nils Lipp,Janis Sprenger,Konstantin Poddubnyy,Davit Hovhannisyan,Christian Mueller,Andreas Bulling,Philipp Slusallek*

Main category: cs.CV

TL;DR: 本文提出了一个专为关注对象性视觉注意力的虚拟现实过街导航新数据集，以及相应的评估指标和模型。通过真实 gaze 数据与丰富注释，推动了对象性注意力模型的发展。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉注意力计算模型很少关注对象为单位的注意力分配，主要是因为缺乏合适的数据集和评估方法。让对象为中心的视觉注意力研究受限。

Method: 1）构建了一个120人参与的街道过街虚拟现实数据集，精确采集凝视数据和环境中对象的状态信息，并配有丰富的注释（全景分割、深度信息、车辆关键点）；2）提出了oSIM新指标，用于衡量对象为基础的视觉注意力模型性能；3）开发了SUMGraph模型，将关键场景对象以图结构编码，并用Mamba U-Net架构进行预测。

Result: 优化对象为基础的注意力模型不仅提升了新提出的oSIM指标，也提升了传统评估指标上的表现。SUMGraph模型在多个视觉注意力预测方法中表现出更优性能。

Conclusion: 基于对象的视觉注意力建模值得重视，所提出的数据集、评估方法与模型为该领域提供了可持续发展的工具和新范式。相关资源将公开发布，有助于推动相关研究。

Abstract: The object-based nature of human visual attention is well-known in cognitive science, but has only played a minor role in computational visual attention models so far. This is mainly due to a lack of suitable datasets and evaluation metrics for object-based attention. To address these limitations, we present \dataset~ -- a novel 120-participant dataset of spatial street-crossing navigation in virtual reality specifically geared to object-based attention evaluations. The uniqueness of the presented dataset lies in the ethical and safety affiliated challenges that make collecting comparable data in real-world environments highly difficult. \dataset~ not only features accurate gaze data and a complete state-space representation of objects in the virtual environment, but it also offers variable scenario complexities and rich annotations, including panoptic segmentation, depth information, and vehicle keypoints. We further propose object-based similarity (oSIM) as a novel metric to evaluate the performance of object-based visual attention models, a previously unexplored performance characteristic. Our evaluations show that explicitly optimising for object-based attention not only improves oSIM performance but also leads to an improved model performance on common metrics. In addition, we present SUMGraph, a Mamba U-Net-based model, which explicitly encodes critical scene objects (vehicles) in a graph representation, leading to further performance improvements over several state-of-the-art visual attention prediction methods. The dataset, code and models will be publicly released.

</details>


### [178] [Not all Blends are Equal: The BLEMORE Dataset of Blended Emotion Expressions with Relative Salience Annotations](https://arxiv.org/abs/2601.13225)
*Tim Lachmann,Alexandra Israelsson,Christina Tornberg,Teimuraz Saghinadze,Michal Balazia,Philipp Müller,Petri Laukka*

Main category: cs.CV

TL;DR: 本文提出BLEMORE数据集，专为识别视频与音频中的情感混合及其显著性设计，并系统评估了多种识别方法。


<details>
  <summary>Details</summary>
Motivation: 现有情感识别方法主要针对单一情绪，难以准确辨识混合情绪及其内部各情绪的重要性，这一问题根本原因是缺乏带有显著性注释的混合情绪数据集。

Method: 作者构建了包含58位演员、3000余段表演、涵盖6种基本情绪和10种混合情绪且标注了显著性（如50/50、70/30等）的多模态数据集BLEMORE；并利用此数据集评测了多种主流单模和多模态情感识别方法在两大任务（情绪存在与显著性判断）上的表现。

Result: 单模态分类器在情绪存在和显著性判断上的准确率最高为29%和13%；多模态方法显著提升，ImageBind+WavLM在情绪存在上达到35%，HiCMAE在显著性上达到18%；测试集表现与验证集相当。

Conclusion: BLEMORE数据集弥补了混合情绪识别研究中的空白，为发展能感知情绪复杂性的情感识别系统提供了重要资源。

Abstract: Humans often experience not just a single basic emotion at a time, but rather a blend of several emotions with varying salience. Despite the importance of such blended emotions, most video-based emotion recognition approaches are designed to recognize single emotions only. The few approaches that have attempted to recognize blended emotions typically cannot assess the relative salience of the emotions within a blend. This limitation largely stems from the lack of datasets containing a substantial number of blended emotion samples annotated with relative salience. To address this shortcoming, we introduce BLEMORE, a novel dataset for multimodal (video, audio) blended emotion recognition that includes information on the relative salience of each emotion within a blend. BLEMORE comprises over 3,000 clips from 58 actors, performing 6 basic emotions and 10 distinct blends, where each blend has 3 different salience configurations (50/50, 70/30, and 30/70). Using this dataset, we conduct extensive evaluations of state-of-the-art video classification approaches on two blended emotion prediction tasks: (1) predicting the presence of emotions in a given sample, and (2) predicting the relative salience of emotions in a blend. Our results show that unimodal classifiers achieve up to 29% presence accuracy and 13% salience accuracy on the validation set, while multimodal methods yield clear improvements, with ImageBind + WavLM reaching 35% presence accuracy and HiCMAE 18% salience accuracy. On the held-out test set, the best models achieve 33% presence accuracy (VideoMAEv2 + HuBERT) and 18% salience accuracy (HiCMAE). In sum, the BLEMORE dataset provides a valuable resource to advancing research on emotion recognition systems that account for the complexity and significance of blended emotion expressions.

</details>


### [179] [ConvMambaNet: A Hybrid CNN-Mamba State Space Architecture for Accurate and Real-Time EEG Seizure Detection](https://arxiv.org/abs/2601.13234)
*Md. Nishan Khan,Kazi Shahriar Sanjid,Md. Tanzim Hossain,Asib Mostakim Fony,Istiak Ahmed,M. Monir Uddin*

Main category: cs.CV

TL;DR: 本研究提出了一种名为ConvMambaNet的混合深度学习模型，结合CNN与Mamba-SSM，提高了癫痫脑电图（EEG）自动检测的准确率和时序特征提取能力，在CHB-MIT数据集上达到99%准确率。


<details>
  <summary>Details</summary>
Motivation: 癫痫发作会严重影响患者生活质量，EEG是主要的检测手段，但由于其信号复杂，自动分析存在挑战，需要更高效的时空特征提取方法。

Method: 作者提出了ConvMambaNet，将卷积神经网络（CNN）与Mamba结构化状态空间模型（SSM）嵌入，旨在同时捕捉EEG数据的空间和长时序动态特征。

Result: 在公开的CHB-MIT头皮EEG数据集测试中，ConvMambaNet模型准确率达到99%，且在类别极度不平衡情况下表现稳健。

Conclusion: ConvMambaNet在癫痫发作检测上效果突出，有望应用于临床实现高效、实时的自动化癫痫监测。

Abstract: Epilepsy is a chronic neurological disorder marked by recurrent seizures that can severely impact quality of life. Electroencephalography (EEG) remains the primary tool for monitoring neural activity and detecting seizures, yet automated analysis remains challenging due to the temporal complexity of EEG signals. This study introduces ConvMambaNet, a hybrid deep learning model that integrates Convolutional Neural Networks (CNNs) with the Mamba Structured State Space Model (SSM) to enhance temporal feature extraction. By embedding the Mamba-SSM block within a CNN framework, the model effectively captures both spatial and long-range temporal dynamics. Evaluated on the CHB-MIT Scalp EEG dataset, ConvMambaNet achieved a 99% accuracy and demonstrated robust performance under severe class imbalance. These results underscore the model's potential for precise and efficient seizure detection, offering a viable path toward real-time, automated epilepsy monitoring in clinical environments.

</details>


### [180] [A Semantic Decoupling-Based Two-Stage Rainy-Day Attack for Revealing Weather Robustness Deficiencies in Vision-Language Models](https://arxiv.org/abs/2601.13238)
*Chengyin Hu,Xiang Chen,Zhe Jia,Weiwen Shi,Fengyu Zhang,Jiujiang Guo,Yiwei Wei*

Main category: cs.CV

TL;DR: 本文针对视觉-语言模型（VLMs）在真实雨天环境下的鲁棒性进行了研究，提出了利用天气扰动攻击VLM的新方法，并发现现有模型在雨天扰动下容易产生语义错位。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言模型在理想环境下表现良好，但它们在实际天气影响下（尤其是雨天）鲁棒性及跨模态对齐能力未被充分研究。随着VLMs在实际应用中越来越普及，理解并提升其在极端天气下的性能，对于提升安全性和可靠性至关重要。

Method: 提出了一个基于语义解耦的两阶段参数化扰动生成框架，对VLMs进行针对性雨天扰动攻击。第一阶段模拟雨天的全局影响，弱化原有的语义边界；第二阶段则构建多尺度雨滴与光照变化的精细扰动，优化后在非像素参数空间中生成物理上合理且可解释的扰动。

Result: 在多个任务和主流VLMs上实验证明，即使是物理合理、受限的天气扰动，也能导致模型严重的语义对齐失效。消融实验显示光照建模和多尺度雨滴结构是导致这一现象的关键。

Conclusion: 现有视觉-语言模型在遭遇逼真的降雨天气扰动时，容易出现显著的跨模态语义错位，这给其实际部署的安全和可靠性带来潜在风险。物理驱动、结构化的天气建模对未来鲁棒VLMs的设计具有指导意义。

Abstract: Vision-Language Models (VLMs) are trained on image-text pairs collected under canonical visual conditions and achieve strong performance on multimodal tasks. However, their robustness to real-world weather conditions, and the stability of cross-modal semantic alignment under such structured perturbations, remain insufficiently studied. In this paper, we focus on rainy scenarios and introduce the first adversarial framework that exploits realistic weather to attack VLMs, using a two-stage, parameterized perturbation model based on semantic decoupling to analyze rain-induced shifts in decision-making. In Stage 1, we model the global effects of rainfall by applying a low-dimensional global modulation to condition the embedding space and gradually weaken the original semantic decision boundaries. In Stage 2, we introduce structured rain variations by explicitly modeling multi-scale raindrop appearance and rainfall-induced illumination changes, and optimize the resulting non-differentiable weather space to induce stable semantic shifts. Operating in a non-pixel parameter space, our framework generates perturbations that are both physically grounded and interpretable. Experiments across multiple tasks show that even physically plausible, highly constrained weather perturbations can induce substantial semantic misalignment in mainstream VLMs, posing potential safety and reliability risks in real-world deployment. Ablations further confirm that illumination modeling and multi-scale raindrop structures are key drivers of these semantic shifts.

</details>


### [181] [Deep Learning for Semantic Segmentation of 3D Ultrasound Data](https://arxiv.org/abs/2601.13263)
*Chenyu Liu,Marco Cecotti,Harikrishnan Vijayakumar,Patrick Robinson,James Barson,Mihai Caleap*

Main category: cs.CV

TL;DR: 本论文提出了一种基于3D超声传感器（Calyo Pulse）的学习型语义分割系统，可用于自动驾驶车辆中的复杂和恶劣环境。


<details>
  <summary>Details</summary>
Motivation: 目前主流的自动驾驶感知系统多依赖LiDAR和摄像头，但这两类传感器在成本、鲁棒性及极端条件下的表现上存在权衡。为提升系统可靠性和降低成本，有必要探索新型感知方式。

Method: 采用模块化、固态3D超声传感器Calyo Pulse收集空间超声数据，并基于3D U-Net架构进行体素级语义分割训练。

Result: 实验结果表明，Calyo Pulse传感器在复杂环境下依然能够实现稳健的语义分割效果。通过增大数据集、优化标注和损失函数，性能还有进一步提升空间。

Conclusion: 3D超声传感作为新的感知手段，可与现有系统形成互补，提升自动驾驶的可靠性，是值得关注和研究的一种新型传感器技术。

Abstract: Developing cost-efficient and reliable perception systems remains a central challenge for automated vehicles. LiDAR and camera-based systems dominate, yet they present trade-offs in cost, robustness and performance under adverse conditions. This work introduces a novel framework for learning-based 3D semantic segmentation using Calyo Pulse, a modular, solid-state 3D ultrasound sensor system for use in harsh and cluttered environments. A 3D U-Net architecture is introduced and trained on the spatial ultrasound data for volumetric segmentation. Results demonstrate robust segmentation performance from Calyo Pulse sensors, with potential for further improvement through larger datasets, refined ground truth, and weighted loss functions. Importantly, this study highlights 3D ultrasound sensing as a promising complementary modality for reliable autonomy.

</details>


### [182] [Enginuity: Building an Open Multi-Domain Dataset of Complex Engineering Diagrams](https://arxiv.org/abs/2601.13299)
*Ethan Seefried,Prahitha Movva,Naga Harshita Marupaka,Tilak Kasturi,Tirthankar Ghosal*

Main category: cs.CV

TL;DR: 本文提出了Enginuity，这是首个开放、规模大、涵盖多领域且具有详细结构标注的工程图数据集，以促进自动化图表解析。


<details>
  <summary>Details</summary>
Motivation: 当前AI在科学探索中受限于无法充分理解和解析工程图等结构化视觉信息，因此需要新的数据集推动AI对工程图的深入理解和应用。

Method: 作者构建了Enginuity数据集，包含来自多个工程领域的工程图，并为其标注了层级组成关系、连接方式和语义元素，以支持自动化解析任务。

Result: Enginuity数据集使多模态大语言模型能够进行结构化图表解析、跨模态检索和AI辅助工程仿真等关键下游任务。

Conclusion: Enginuity将帮助AI突破对工程图理解的瓶颈，促进AI更好地参与科学工作流，包括假设生成、实验设计和工程发现。

Abstract: We propose Enginuity - the first open, large-scale, multi-domain engineering diagram dataset with comprehensive structural annotations designed for automated diagram parsing. By capturing hierarchical component relationships, connections, and semantic elements across diverse engineering domains, our proposed dataset would enable multimodal large language models to address critical downstream tasks including structured diagram parsing, cross-modal information retrieval, and AI-assisted engineering simulation. Enginuity would be transformative for AI for Scientific Discovery by enabling artificial intelligence systems to comprehend and manipulate the visual-structural knowledge embedded in engineering diagrams, breaking down a fundamental barrier that currently prevents AI from fully participating in scientific workflows where diagram interpretation, technical drawing analysis, and visual reasoning are essential for hypothesis generation, experimental design, and discovery.

</details>


### [183] [CausalSpatial: A Benchmark for Object-Centric Causal Spatial Reasoning](https://arxiv.org/abs/2601.13304)
*Wenxin Ma,Chenlong Wang,Ruisheng Yuan,Hao Chen,Nanru Dai,S. Kevin Zhou,Yijun Yang,Alan Yuille,Jieneng Chen*

Main category: cs.CV

TL;DR: 本文指出当前多模态大模型（MLLMs）在因果空间推理方面存在严重不足，难以预测物体运动的后果。为此，提出了CausalSpatial基准和COW框架，显著提升了模型在相关任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 人类能直观预测静态场景中物体运动后的后果（如碰撞），而现有MLLMs难以完成类似‘what-if’推理，限制了其空间理解与推理能力。为推动技术进步，需要开发专门评测模型因果空间推理能力的新方法。

Method: 作者提出CausalSpatial基准，包括碰撞、兼容性、遮挡和轨迹四类任务，用以衡量模型在因果空间推理上的能力。同时，提出Causal Object World（COW）模型，将假设性场景通过视频模拟动态外化，辅助视觉推理，减少单纯依赖文本的偏差。

Result: 基准评测结果显示，人类得分84%，而最强MLLM（如GPT-5）仅为54%。分析表明，MLLM偏好文本推理且缺乏视觉证据佐证，导致大量空间幻觉。COW能通过生成动态视频增强模型对物理因果的理解。

Conclusion: 当前MLLM在因果空间推理上的表现与人类有较大差距。应用COW等技术，可有效提升模型的物理推理能力，并促进该方向的研究发展。论文数据集和代码已公开。

Abstract: Humans can look at a static scene and instantly predict what happens next -- will moving this object cause a collision? We call this ability Causal Spatial Reasoning. However, current multimodal large language models (MLLMs) cannot do this, as they remain largely restricted to static spatial perception, struggling to answer "what-if" questions in a 3D scene. We introduce CausalSpatial, a diagnostic benchmark evaluating whether models can anticipate consequences of object motions across four tasks: Collision, Compatibility, Occlusion, and Trajectory. Results expose a severe gap: humans score 84% while GPT-5 achieves only 54%. Why do MLLMs fail? Our analysis uncovers a fundamental deficiency: models over-rely on textual chain-of-thought reasoning that drifts from visual evidence, producing fluent but spatially ungrounded hallucinations. To address this, we propose the Causal Object World model (COW), a framework that externalizes the simulation process by generating videos of hypothetical dynamics. With explicit visual cues of causality, COW enables models to ground their reasoning in physical reality rather than linguistic priors. We make the dataset and code publicly available here: https://github.com/CausalSpatial/CausalSpatial

</details>


### [184] [MultiST: A Cross-Attention-Based Multimodal Model for Spatial Transcriptomic](https://arxiv.org/abs/2601.13331)
*Wei Wang,Quoc-Toan Ly,Chong Yu,Jun Bai*

Main category: cs.CV

TL;DR: 本文提出了MultiST，一种融合空间、基因表达与组织形态信息的统一多模态空间转录组分析框架，可以更有效识别组织空间结构及细胞间互作。


<details>
  <summary>Details</summary>
Motivation: 目前空间转录组技术发展很快，但普遍存在组织形态信息与分子信息整合不足、融合方式表浅或完全忽略组织图像的问题，导致空间结构边界分辨不清。

Method: MultiST框架通过基于cross-attention机制的融合方式，同时建模空间拓扑、基因表达与组织形态特征。利用图神经网络编码基因数据，并通过对抗式对齐学习鲁棒的空间表达，同时整合颜色标准化后的组织切片形态特征，增强分子-形态相关性及空间域的划分精度。

Result: 在包含人脑皮层和乳腺癌等两个器官的13组空间转录组数据集上，MultiST获得了比以往方法更连贯、清晰的空间域边界，同时改善了伪时序分析的稳定性，以及细胞间互作的生物学可解释性。

Conclusion: MultiST能更高效、精确地进行空间转录组数据的多模态分析，提升空间结构解析与生物学发现的能力。

Abstract: Spatial transcriptomics (ST) enables transcriptome-wide profiling while preserving the spatial context of tissues, offering unprecedented opportunities to study tissue organization and cell-cell interactions in situ. Despite recent advances, existing methods often lack effective integration of histological morphology with molecular profiles, relying on shallow fusion strategies or omitting tissue images altogether, which limits their ability to resolve ambiguous spatial domain boundaries. To address this challenge, we propose MultiST, a unified multimodal framework that jointly models spatial topology, gene expression, and tissue morphology through cross-attention-based fusion. MultiST employs graph-based gene encoders with adversarial alignment to learn robust spatial representations, while integrating color-normalized histological features to capture molecular-morphological dependencies and refine domain boundaries. We evaluated the proposed method on 13 diverse ST datasets spanning two organs, including human brain cortex and breast cancer tissue. MultiST yields spatial domains with clearer and more coherent boundaries than existing methods, leading to more stable pseudotime trajectories and more biologically interpretable cell-cell interaction patterns. The MultiST framework and source code are available at https://github.com/LabJunBMI/MultiST.git.

</details>


### [185] [Real-Time 4D Radar Perception for Robust Human Detection in Harsh Enclosed Environments](https://arxiv.org/abs/2601.13364)
*Zhenan Liu,Yaodong Cui,Amir Khajepour,George Shaker*

Main category: cs.CV

TL;DR: 该论文提出了一种新型方法，在复杂多杂物的高尘环境中（如矿井、隧道等）可控生成多级尘埃以进行毫米波传播和感知实验，并公开了配合视频与激光雷达标注的4D毫米波雷达数据集。提出了利用雷达原始参数的阈值滤波方法，有效抑制噪声和多径干扰，并通过基于规则的聚类管道实时检测行人，在实验中提高了检测鲁棒性和系统抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: 实际如矿井、隧道等封闭高尘环境对毫米波雷达感知带来极大挑战，尘埃和多反射会严重干扰雷达感知。现有研究缺乏可控、可重复的实验平台和数据集，且在高杂波环境下鲁棒检测行人的方法有限。

Method: 1. 搭建可控的多尘环境实验平台，支持高尘与复杂反射。2. 采集4D毫米波+视频+激光雷达数据集。3. 提出基于雷达回波参数(RCS、速度、方位、俯仰)的噪声与多径干扰滤波。4. 构建基于规则的聚类和行人检测方法，结合雷达的快照信息实现无需大量特定场景训练的鲁棒检测。

Result: 基于上述方法，实验表明新方案有效去除杂波，显著提升矿井等高尘环境下的行人检测准确性与系统鲁棒性，优于无滤波和无规则方法的基线。

Conclusion: 提出的整体方案（新尘埃平台+数据集+滤波与检测算法）极大推动了复杂尘埃环境下的毫米波雷达感知研究，显著提升了高尘环境中的目标检测性能，对矿井等实际应用有重大意义。

Abstract: This paper introduces a novel methodology for generating controlled, multi-level dust concentrations in a highly cluttered environment representative of harsh, enclosed environments, such as underground mines, road tunnels, or collapsed buildings, enabling repeatable mm-wave propagation studies under severe electromagnetic constraints. We also present a new 4D mmWave radar dataset, augmented by camera and LiDAR, illustrating how dust particles and reflective surfaces jointly impact the sensing functionality. To address these challenges, we develop a threshold-based noise filtering framework leveraging key radar parameters (RCS, velocity, azimuth, elevation) to suppress ghost targets and mitigate strong multipath reflections at the raw data level. Building on the filtered point clouds, a cluster-level, rule-based classification pipeline exploits radar semantics-velocity, RCS, and volumetric spread-to achieve reliable, real-time pedestrian detection without extensive domainspecific training. Experimental results confirm that this integrated approach significantly enhances clutter mitigation, detection robustness, and overall system resilience in dust-laden mining environments.

</details>


### [186] [Spherical Geometry Diffusion: Generating High-quality 3D Face Geometry via Sphere-anchored Representations](https://arxiv.org/abs/2601.13371)
*Junyi Zhang,Yiming Wang,Yunhong Lu,Qichao Wang,Wenzhe Qian,Xiaoyin Xu,David Gu,Min Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于球面几何表示的text-to-3D人脸生成新方法，通过将人脸几何结构映射到规则的球面坐标，从而获得更高质量的三维人脸模型，并结合扩散模型进行高效生成。


<details>
  <summary>Details</summary>
Motivation: 现有的text-to-3D人脸生成方法在几何质量上存在瓶颈，主要因为三维顶点分布复杂，导致网格连接不干净，进而影响生成效果。作者希望通过简化底层几何结构来提升生成质量。

Method: 首先提出球面几何表示方式，将人脸几何信号锚定于均匀球面坐标，保证点分布规律，易于重建网格连接。其次，将球面展平为2D映射，与2D生成模型对接。最后基于此2D球面图提出条件扩散框架Spherical Geometry Diffusion，同时建模几何与纹理信息，实现可控、高质量的人脸生成。

Result: 实验结果显示，该方法在text-to-3D生成、人脸重建和基于文本的3D编辑等任务中，在几何质量、文本忠实度和推理效率方面均显著优于现有方案。

Conclusion: 球面几何表示和扩散生成框架有效提升了text-to-3D人脸生成的几何质量和效率，为相关三维人脸生成和编辑任务带来了新的解决思路。

Abstract: A fundamental challenge in text-to-3D face generation is achieving high-quality geometry. The core difficulty lies in the arbitrary and intricate distribution of vertices in 3D space, making it challenging for existing models to establish clean connectivity and resulting in suboptimal geometry. To address this, our core insight is to simplify the underlying geometric structure by constraining the distribution onto a simple and regular manifold, a topological sphere. Building on this, we first propose the Spherical Geometry Representation, a novel face representation that anchors geometric signals to uniform spherical coordinates. This guarantees a regular point distribution, from which the mesh connectivity can be robustly reconstructed. Critically, this canonical sphere can be seamlessly unwrapped into a 2D map, creating a perfect synergy with powerful 2D generative models. We then introduce Spherical Geometry Diffusion, a conditional diffusion framework built upon this 2D map. It enables diverse and controllable generation by jointly modeling geometry and texture, where the geometry explicitly conditions the texture synthesis process. Our method's effectiveness is demonstrated through its success in a wide range of tasks: text-to-3D generation, face reconstruction, and text-based 3D editing. Extensive experiments show that our approach substantially outperforms existing methods in geometric quality, textual fidelity, and inference efficiency.

</details>


### [187] [A Lightweight Model-Driven 4D Radar Framework for Pervasive Human Detection in Harsh Conditions](https://arxiv.org/abs/2601.13373)
*Zhenan Liu,Amir Khajepour,George Shaker*

Main category: cs.CV

TL;DR: 本文提出了一种面对恶劣工业及地下环境的、基于4D毫米波雷达的人体检测方法，在尘土飞扬或能见度极低时，依旧能实现稳定识别。


<details>
  <summary>Details</summary>
Motivation: 在工业和地下空间，光学/激光感知方法常因灰尘、烟雾和复杂结构失效，因此亟需寻求可在恶劣环境下稳定识别人的新型感知方法。

Method: 本方法仅依靠毫米波雷达，提出了全模型驱动的感知框架，结合多阈值滤波、运动补偿的时序累计、带Doppler信息的KD树聚类、基于规则的三维分类器，适用于边缘设备实时运行。

Result: 在充满尘埃的封闭拖车和真实矿井中测试时，所提雷达检测方法在人类可见度极差时仍能稳定识别行人，而摄像头和激光雷达则失效。

Conclusion: 该基于模型的方法为极端环境下的重要安全应用场景提供了鲁棒、易解释、计算高效的感知方案。

Abstract: Pervasive sensing in industrial and underground environments is severely constrained by airborne dust, smoke, confined geometry, and metallic structures, which rapidly degrade optical and LiDAR based perception. Elevation resolved 4D mmWave radar offers strong resilience to such conditions, yet there remains a limited understanding of how to process its sparse and anisotropic point clouds for reliable human detection in enclosed, visibility degraded spaces. This paper presents a fully model-driven 4D radar perception framework designed for real-time execution on embedded edge hardware. The system uses radar as its sole perception modality and integrates domain aware multi threshold filtering, ego motion compensated temporal accumulation, KD tree Euclidean clustering with Doppler aware refinement, and a rule based 3D classifier. The framework is evaluated in a dust filled enclosed trailer and in real underground mining tunnels, and in the tested scenarios the radar based detector maintains stable pedestrian identification as camera and LiDAR modalities fail under severe visibility degradation. These results suggest that the proposed model-driven approach provides robust, interpretable, and computationally efficient perception for safety-critical applications in harsh industrial and subterranean environments.

</details>


### [188] [DermaBench: A Clinician-Annotated Benchmark Dataset for Dermatology Visual Question Answering and Reasoning](https://arxiv.org/abs/2601.14084)
*Abdurrahim Yilmaz,Ozan Erdem,Ece Gokyayla,Ayda Acar,Burc Bugra Dagtas,Dilara Ilhan Erdil,Gulsum Gencoglan,Burak Temelkuran*

Main category: cs.CV

TL;DR: 该论文提出了DermaBench，一个面向皮肤科视觉问答（VQA）的基准数据集，以弥补现有仅支持图像分类的皮肤科数据集在全面评估多模态模型能力方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型（VLMs）在医学领域，尤其是皮肤科中的应用越来越多，但现有用于评估这些模型的数据集主要聚焦于病变识别等图像级分类任务，无法评估模型的深层视觉理解、语言联结以及临床推理等能力。因此，亟需更具挑战性的基准来全面评价VLMs在皮肤科中的应用表现。

Method: 作者基于Diverse Dermatology Images (DDI) 数据集，构建了DermaBench：包含来自570名患者的656张临床图像，并由皮肤科专家根据22个主问题体系化标注，包括单选、多选及开放性问题，涵盖诊断、解剖部位、病变形态、分布、表面特征、颜色、图像质量及叙述性描述。总计生成约1.44万个VQA风格标注。该数据集以仅含元数据的形式发布，以避免原始许可问题。

Result: 成功构建了覆盖Fitzpatrick I-VI六种肤色类型的皮肤科VQA基准数据集DermaBench，并在哈佛Dataverse公开发布，包含详尽且临床相关的注释。

Conclusion: DermaBench为皮肤科视觉-语言模型的全面评估提供了坚实基础，支持对模型视觉理解、文本描述以及推理能力的系统性测试，有望推动多模态模型在医疗场景下的深入应用与进步。

Abstract: Vision-language models (VLMs) are increasingly important in medical applications; however, their evaluation in dermatology remains limited by datasets that focus primarily on image-level classification tasks such as lesion recognition. While valuable for recognition, such datasets cannot assess the full visual understanding, language grounding, and clinical reasoning capabilities of multimodal models. Visual question answering (VQA) benchmarks are required to evaluate how models interpret dermatological images, reason over fine-grained morphology, and generate clinically meaningful descriptions. We introduce DermaBench, a clinician-annotated dermatology VQA benchmark built on the Diverse Dermatology Images (DDI) dataset. DermaBench comprises 656 clinical images from 570 unique patients spanning Fitzpatrick skin types I-VI. Using a hierarchical annotation schema with 22 main questions (single-choice, multi-choice, and open-ended), expert dermatologists annotated each image for diagnosis, anatomic site, lesion morphology, distribution, surface features, color, and image quality, together with open-ended narrative descriptions and summaries, yielding approximately 14.474 VQA-style annotations. DermaBench is released as a metadata-only dataset to respect upstream licensing and is publicly available at Harvard Dataverse.

</details>


### [189] [Practical Insights into Semi-Supervised Object Detection Approaches](https://arxiv.org/abs/2601.13380)
*Chaoxin Wang,Bharaneeshwar Balasubramaniyam,Anurag Sangem,Nicolais Guevara,Doina Caragea*

Main category: cs.CV

TL;DR: 本文对三种最先进的半监督目标检测方法（MixPL、Semi-DETR、Consistent-Teacher）在不同标注数据量下的性能进行对比，结合MS-COCO、PASCAL VOC及自制Beetle数据集，分析模型精度、规模及延迟的权衡，为数据稀缺场景下方法选择提供建议。


<details>
  <summary>Details</summary>
Motivation: 在标注数据稀缺的情况下提升目标检测性能是近期研究热点，然而不同半监督方法在不同数据量和数据类型下的效果并不清楚，因此需要系统评估和对比。

Method: 选用三种主流半监督目标检测方法，对MS-COCO、PASCAL VOC和自制Beetle数据集进行实验，评测模型在不同标注图片数量下的表现，并比较其精度、模型大小和推理延迟。

Result: 实验揭示了三种方法在不同数据集和样本量下的性能变化及其优劣势，详细数据未给出但强调了性能、体积和延迟上的权衡。

Conclusion: 各方法在低数据场景存在取舍，论文为实际选型提供了参考依据，特别适用于数据稀缺及特定类别数较小的数据集。

Abstract: Learning in data-scarce settings has recently gained significant attention in the research community. Semi-supervised object detection(SSOD) aims to improve detection performance by leveraging a large number of unlabeled images alongside a limited number of labeled images(a.k.a.,few-shot learning). In this paper, we present a comprehensive comparison of three state-of-the-art SSOD approaches, including MixPL, Semi-DETR and Consistent-Teacher, with the goal of understanding how performance varies with the number of labeled images. We conduct experiments using the MS-COCO and Pascal VOC datasets, two popular object detection benchmarks which allow for standardized evaluation. In addition, we evaluate the SSOD approaches on a custom Beetle dataset which enables us to gain insights into their performance on specialized datasets with a smaller number of object categories. Our findings highlight the trade-offs between accuracy, model size, and latency, providing insights into which methods are best suited for low-data regimes.

</details>


### [190] [The Side Effects of Being Smart: Safety Risks in MLLMs' Multi-Image Reasoning](https://arxiv.org/abs/2601.14127)
*Renmiao Chen,Yida Lu,Shiyao Cui,Xuan Ouyang,Victor Shea-Jay Huang,Shumin Zhang,Chengwei Pan,Han Qiu,Minlie Huang*

Main category: cs.CV

TL;DR: 本文提出MIR-SafetyBench，这是第一个专注于多图推理安全性的基准，并对19个多模态大模型(MLLMs)进行了全面评测。结果显示推理能力越强的模型在安全性上反而更脆弱。


<details>
  <summary>Details</summary>
Motivation: 随着MLLMs在复杂多图任务上的推理能力提升，其安全风险随之增加，目前缺乏针对多图推理安全的系统性评测。为此，作者提出了此新的基准，填补这一空白。

Method: 作者构建了包含2,676个实例、覆盖9种多图关系类型的MIR-SafetyBench，并针对19个主流MLLMs进行了详细测试，统计攻击成功率及安全/非安全回答的特征，研究注意力熵等内部表征。

Result: 先进推理能力的MLLM在MIR-SafetyBench上表现出更高的安全风险。标记为安全的回答很多只是表面安全，实则理解片面或回避问题。注意力熵分析显示不安全回答平均熵更低，有过度聚焦解题、忽视安全约束的风险。

Conclusion: 多模态大模型在提升推理能力的同时，需警惕安全性下降的风险。MIR-SafetyBench为未来模型的安全评估和改进提供了重要工具。

Abstract: As Multimodal Large Language Models (MLLMs) acquire stronger reasoning capabilities to handle complex, multi-image instructions, this advancement may pose new safety risks. We study this problem by introducing MIR-SafetyBench, the first benchmark focused on multi-image reasoning safety, which consists of 2,676 instances across a taxonomy of 9 multi-image relations. Our extensive evaluations on 19 MLLMs reveal a troubling trend: models with more advanced multi-image reasoning can be more vulnerable on MIR-SafetyBench. Beyond attack success rates, we find that many responses labeled as safe are superficial, often driven by misunderstanding or evasive, non-committal replies. We further observe that unsafe generations exhibit lower attention entropy than safe ones on average. This internal signature suggests a possible risk that models may over-focus on task solving while neglecting safety constraints. Our code and data are available at https://github.com/thu-coai/MIR-SafetyBench.

</details>


### [191] [Organ-Aware Attention Improves CT Triage and Classification](https://arxiv.org/abs/2601.13385)
*Lavsen Dahal,Yubraj Bhandari,Geoffrey D. Rubin,Joseph Y. Lo*

Main category: cs.CV

TL;DR: 该论文提出了一种新的CT影像分级与分类方法ORACLE-CT，在胸部和腹部CT数据集中均取得了最新最优的分类效果。


<details>
  <summary>Details</summary>
Motivation: 自动化处理和分级大量CT影像以改善医疗效率，减轻影像科医生负担，现有视觉语言模型（VLM）在3D解剖结构、协议变动及嘈杂标注下表现不理想。

Method: 作者以CT-RATE和RADCHEST-CT两个大规模公开胸部CT数据集为基准，首先将精细调优的全局平均池化作为强监督基线，随后提出ORACLE-CT方法，包括与编码器无关的器官感知头（Organ-Masked Attention和Organ-Scalar Fusion），能针对不同器官区域提取空间证据与重要性特征。

Result: 新提出的ORACLE-CT方法在胸部数据集（CT-RATE）上取得0.86 AUROC，在腹部数据集（MERLIN）上也实现显著提升（AUROC 0.85），均超过现有线性探针和零样本VLM模型。

Conclusion: ORACLE-CT实现了胸部和腹部CT影像的有监督分类新SOTA，能够提升医学图像分诊效率，具有良好临床应用前景，并公开了源代码。

Abstract: There is an urgent need for triage and classification of high-volume medical imaging modalities such as computed tomography (CT), which can improve patient care and mitigate radiologist burnout. Study-level CT triage requires calibrated predictions with localized evidence; however, off-the-shelf Vision Language Models (VLM) struggle with 3D anatomy, protocol shifts, and noisy report supervision. This study used the two largest publicly available chest CT datasets: CT-RATE and RADCHEST-CT (held-out external test set). Our carefully tuned supervised baseline (instantiated as a simple Global Average Pooling head) establishes a new supervised state of the art, surpassing all reported linear-probe VLMs. Building on this baseline, we present ORACLE-CT, an encoder-agnostic, organ-aware head that pairs Organ-Masked Attention (mask-restricted, per-organ pooling that yields spatial evidence) with Organ-Scalar Fusion (lightweight fusion of normalized volume and mean-HU cues). In the chest setting, ORACLE-CT masked attention model achieves AUROC 0.86 on CT-RATE; in the abdomen setting, on MERLIN (30 findings), our supervised baseline exceeds a reproduced zero-shot VLM baseline obtained by running publicly released weights through our pipeline, and adding masked attention plus scalar fusion further improves performance to AUROC 0.85. Together, these results deliver state-of-the-art supervised classification performance across both chest and abdomen CT under a unified evaluation protocol. The source code is available at https://github.com/lavsendahal/oracle-ct.

</details>


### [192] [Leveraging Transformer Decoder for Automotive Radar Object Detection](https://arxiv.org/abs/2601.13386)
*Changxu Zhang,Zhaoze Wang,Tai Fei,Christopher Grimm,Yi Jin,Claas Tebruegge,Ernst Warsitz,Markus Gardill*

Main category: cs.CV

TL;DR: 本文提出了一种基于Transformer的3D雷达目标检测架构，利用新颖的解码器作为预测头，直接回归3D边界框和类别分数。通过多尺度特征融合和集合预测设计，该方法在RADDet数据集上实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前雷达目标检测方法往往依赖密集的候选框生成和繁琐的后处理，难以有效捕捉长距离的空间-时间关联和特征交互。为此，本文旨在探索更高效、鲁棒的检测架构。

Method: 核心方法包括三个方面：1）采用Transformer解码器作为预测头，直接回归边界框和类别分数；2）提出轻量级的Pyramid Token Fusion（PTF）模块，将多尺度特征金字塔转为统一token序列；3）通过集合预测、可学习的目标查询及位置编码，有效建模空间-时间和跨特征交互。

Result: 在RADDet数据集上，该方法在各项评价指标上均优于现有最先进的纯雷达基线，对比实验显示效果显著提升。

Conclusion: 该架构开创性地利用Transformer与PTF模块，实现了高效、准确的3D雷达目标检测，为该领域提供了新的技术方向和设计思路。

Abstract: In this paper, we present a Transformer-based architecture for 3D radar object detection that uses a novel Transformer Decoder as the prediction head to directly regress 3D bounding boxes and class scores from radar feature representations. To bridge multi-scale radar features and the decoder, we propose Pyramid Token Fusion (PTF), a lightweight module that converts a feature pyramid into a unified, scale-aware token sequence. By formulating detection as a set prediction problem with learnable object queries and positional encodings, our design models long-range spatial-temporal correlations and cross-feature interactions. This approach eliminates dense proposal generation and heuristic post-processing such as extensive non-maximum suppression (NMS) tuning. We evaluate the proposed framework on the RADDet, where it achieves significant improvements over state-of-the-art radar-only baselines.

</details>


### [193] [Deep Image Prior with L0 Gradient Regularizer for Image Smoothing](https://arxiv.org/abs/2601.13400)
*Nhat Thanh Tran,Kevin Bui,Jack Xin*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练数据的深度图像先验框架DIP-$\ell_0$，结合$\ell_0$梯度正则化，实现高质量图像平滑，并在边缘保持与JPEG伪影去除方面优于多种现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习图像平滑方法依赖于精心标注的训练数据集，而数据集的构建非常困难。为克服这一限制，作者希望实现无需训练数据也能进行高质量图像平滑。

Method: 作者提出DIP-$\ell_0$方法，将深度图像先验与$\ell_0$梯度正则项结合，并为包含非凸、不可微$\ell_0$范数的损失函数设计了一种基于交替方向乘子（ADMM）的优化算法，利用现成的$\ell_0$梯度最小化工具进行求解。

Result: 实验表明DIP-$\ell_0$方法在边缘保持图像平滑及JPEG伪影去除等任务上取得了比多种图像平滑算法更佳的效果。

Conclusion: DIP-$\ell_0$无需训练数据即可高效实现高质量图像平滑，且具有领先的性能，在实际应用中具有很好的推广价值。

Abstract: Image smoothing is a fundamental image processing operation that preserves the underlying structure, such as strong edges and contours, and removes minor details and textures in an image. Many image smoothing algorithms rely on computing local window statistics or solving an optimization problem. Recent state-of-the-art methods leverage deep learning, but they require a carefully curated training dataset. Because constructing a proper training dataset for image smoothing is challenging, we propose DIP-$\ell_0$, a deep image prior framework that incorporates the $\ell_0$ gradient regularizer. This framework can perform high-quality image smoothing without any training data. To properly minimize the associated loss function that has the nonconvex, nonsmooth $\ell_0$ ``norm", we develop an alternating direction method of multipliers algorithm that utilizes an off-the-shelf $\ell_0$ gradient minimization solver. Numerical experiments demonstrate that the proposed DIP-$\ell_0$ outperforms many image smoothing algorithms in edge-preserving image smoothing and JPEG artifact removal.

</details>


### [194] [Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset for Quantitative Geospatial Analytics](https://arxiv.org/abs/2601.13401)
*Peter A. Massih,Eric Cosatto*

Main category: cs.CV

TL;DR: 本论文指出，现有的视觉-语言模型（VLMs）在定量空间推理任务上表现较差，原因在于图片编码方式导致像素级信息丢失。作者提出了新的数据集SQuID以及新的方法QVLM，在定量空间推理任务上显著提优。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs因将图片压缩为嵌入向量，导致空间信息丢失，难以进行精确计数和测量等空间推理任务。为此，作者希望提出新的基准任务及模型，专门针对定量空间推理能力进行测评与提升。

Method: 作者首先提出SQuID数据集，包含2,000组遥感图像与问题-答案对，覆盖不同难度和问题类型，用以有效评估模型的定量空间推理能力。方法上，作者提出QVLM，使用代码生成架构，将语言理解和视觉分析解耦。对于图片问题，QVLM先调用分割模型获取像素级掩膜，然后直接在掩膜上持续保持空间索引进行推理，避免空间信息损失。

Result: 在SQuID数据集上，采用GPT-5作为代码生成器的QVLM模型取得了42.0%的准确率，而传统VLM仅为28.1%，显示出新架构在定量空间推理方面的明显优势。

Conclusion: 研究表明，通过架构上的语言与视觉解耦，并维护像素级空间信息，在定量空间推理任务上能大幅提升模型表现，对相关视觉-语言模型设计具有参考意义。

Abstract: Current Vision-Language Models (VLMs) fail at quantitative spatial reasoning because their architectures destroy pixel-level information required for counting and measurements. Vision encoders compress images through patch embeddings, reducing spatial indexing and losing the precise pixel-level tracking required for accurate counting. We present two contributions to address this fundamental limitation. First, we introduce SQuID (Satellite Quantitative Intelligence Dataset), a benchmark of 2,000 satellite image Question-Answer pairs with both numerical range and categorical answers, designed to evaluate quantitative spatial reasoning. The dataset spans three difficulty tiers with annotations automatically generated from human labels and their learned variability. Second, we propose QVLM (Quantitative Vision-Language Model), a code-generation architecture that maintains pixel precision by decoupling language understanding from visual analysis. Instead of encoding images into embeddings, QVLM generates executable code that first calls a segmentation model to obtain pixel-level masks, then operates directly on these masks, preserving spatial indexing throughout the reasoning process. Our experiments show that QVLM using GPT-5 as coder achieves 42.0% accuracy on SQuID compared to 28.1% for a VLM prompted with image-question pairs. Our work reveals that, for quantitative spatial reasoning, architectural decoupling enables better accuracy on quantitative tasks.

</details>


### [195] [Local-to-Global Logical Explanations for Deep Vision Models](https://arxiv.org/abs/2601.13404)
*Bhavan Vasu,Giuseppe Raffa,Prasad Tadepalli*

Main category: cs.CV

TL;DR: 本文提出了一种基于人类可理解概念的黑盒模型解释方法，能够为深度神经网络的分类结果生成局部和全局解释，且解释具备高准确性和适用性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络虽然在图像分类等任务上效果突出，但难以解释其决策过程，限制了其在需要可解释性的领域应用。因此，开发能够解释黑盒模型行为的方法具有重要意义。

Method: 作者提出了利用单调析取范式（MDNF）对黑盒模型进行解释的方法。具体包括：为单个图像提供局部解释、为图像集合提供全局解释，两者都使用人类可识别的基本概念来表达。解释结果形式为逻辑公式，并为多类别分类问题设计了解释算法，以单调解释列表进行表示。

Result: 实验表明，该方法在复杂的视觉数据集上，所生成的解释不仅简单、易于理解，还能够很好地代表原黑盒模型的决策，即具有高忠实度和覆盖率。

Conclusion: 提出的方法能够为深度神经网络的分类提供人类可理解且准确的解释，有助于提升这些模型的透明度和信任度。

Abstract: While deep neural networks are extremely effective at classifying images, they remain opaque and hard to interpret. We introduce local and global explanation methods for black-box models that generate explanations in terms of human-recognizable primitive concepts. Both the local explanations for a single image and the global explanations for a set of images are cast as logical formulas in monotone disjunctive-normal-form (MDNF), whose satisfaction guarantees that the model yields a high score on a given class. We also present an algorithm for explaining the classification of examples into multiple classes in the form of a monotone explanation list over primitive concepts. Despite their simplicity and interpretability we show that the explanations maintain high fidelity and coverage with respect to the blackbox models they seek to explain in challenging vision datasets.

</details>


### [196] [Using deep learning for predicting cleansing quality of colon capsule endoscopy images](https://arxiv.org/abs/2601.13412)
*Puneet Sharma,Kristian Dalsbø Hindberg,Benedicte Schelde-Olesen,Ulrik Deding,Esmaeil S. Nadimi,Jan-Matthias Braun*

Main category: cs.CV

TL;DR: 本文提出使用深度学习（ResNet-18）对结肠胶囊内镜（CCE）图像的清洁度进行分级预测，并通过结构化剪枝与可解释性分析实现高效、可靠的临床模型。结果显示模型在高稀疏度下仍保持优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有CCE图像的清洁度评估需要人工操作，效率低、主观性强。自动化、标准化的深度学习方法可提高诊断效率，并提升一致性。

Method: 收集500张标注图片（由14位医生按Leighton-Rex分级），采用ResNet-18架构进行分类，结合分层K折交叉验证评估性能。通过结构剪枝提升模型稀疏性并保持精度，并利用多种可解释性方法（Grad-CAM等）和ROAD指标评测，最后使用温度缩放校准模型。

Result: 剪枝后的ResNet-18在保持88%交叉验证准确率的同时，模型稀疏度达79%。未经剪枝时准确率为84%。可解释性分析验证模型适用于临床需求，外部数据也经校准优化。

Conclusion: 结构化剪枝结合深度学习在CCE图像清洁度分类中兼顾了效率与准确性，可支持临床决策；模型可解释性有助于实际应用。针对可解释性指标（如ROAD）的选用和应用仍有挑战。

Abstract: In this study, we explore the application of deep learning techniques for predicting cleansing quality in colon capsule endoscopy (CCE) images. Using a dataset of 500 images labeled by 14 clinicians on the Leighton-Rex scale (Poor, Fair, Good, and Excellent), a ResNet-18 model was trained for classification, leveraging stratified K-fold cross-validation to ensure robust performance. To optimize the model, structured pruning techniques were applied iteratively, achieving significant sparsity while maintaining high accuracy. Explainability of the pruned model was evaluated using Grad-CAM, Grad-CAM++, Eigen-CAM, Ablation-CAM, and Random-CAM, with the ROAD method employed for consistent evaluation. Our results indicate that for a pruned model, we can achieve a cross-validation accuracy of 88% with 79% sparsity, demonstrating the effectiveness of pruning in improving efficiency from 84% without compromising performance. We also highlight the challenges of evaluating cleansing quality of CCE images, emphasize the importance of explainability in clinical applications, and discuss the challenges associated with using the ROAD method for our task. Finally, we employ a variant of adaptive temperature scaling to calibrate the pruned models for an external dataset.

</details>


### [197] [Diffusion Representations for Fine-Grained Image Classification: A Marine Plankton Case Study](https://arxiv.org/abs/2601.13416)
*A. Nieto Juscafresa,Á. Mazcuñán Herreros,J. Sullivan*

Main category: cs.CV

TL;DR: 本论文探讨了将扩散模型作为通用特征编码器的可行性，并在浮游生物识别任务中展现出优异性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型已成为图像生成领域的主流方法，但其在特征编码和下游视觉任务中的潜力尚未得到充分挖掘。鉴于扩散模型具有无监督学习能力，作者希望验证其作为特征提取器在细粒度识别中的表现。

Method: 作者将经过训练且参数冻结的扩散模型（如基础网络）应用于浮游生物监测任务，提取其中间去噪特征，并在不同层与不同时间步组合下训练线性分类器，进行了详尽的实验评估，与主流有监督和自监督方法进行了公平比较。

Result: 冻结扩散模型提取的特征，在有监督基线以及多种自监督方法（包括长尾分布和均衡分布设定）下，表现出竞争力甚至超越自监督方法的性能。同时，在分布偏移（时间和地域变化）下依然保持高准确率和宏观F1分数。

Conclusion: 扩散模型不仅生成能力出色，还作为无监督特征提取工具，在下游细粒度识别任务中表现优异，具备广泛应用价值，特别适用于实际分布变化场景。

Abstract: Diffusion models have emerged as state-of-the-art generative methods for image synthesis, yet their potential as general-purpose feature encoders remains underexplored. Trained for denoising and generation without labels, they can be interpreted as self-supervised learners that capture both low- and high-level structure. We show that a frozen diffusion backbone enables strong fine-grained recognition by probing intermediate denoising features across layers and timesteps and training a linear classifier for each pair. We evaluate this in a real-world plankton-monitoring setting with practical impact, using controlled and comparable training setups against established supervised and self-supervised baselines. Frozen diffusion features are competitive with supervised baselines and outperform other self-supervised methods in both balanced and naturally long-tailed settings. Out-of-distribution evaluations on temporally and geographically shifted plankton datasets further show that frozen diffusion features maintain strong accuracy and Macro F1 under substantial distribution shift.

</details>


### [198] [SGW-GAN: Sliced Gromov-Wasserstein Guided GANs for Retinal Fundus Image Enhancement](https://arxiv.org/abs/2601.13417)
*Yujian Xiong,Xuanzhao Dong,Wenhui Zhu,Xin Li,Oana Dumitrascu,Yalin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于Sliced Gromov Wasserstein (SGW) 距离的生成对抗网络（SGW-GAN），用于提升视网膜彩照的质量，并能更好地保留疾病分类结构。


<details>
  <summary>Details</summary>
Motivation: 现有提升视网膜照质量的GAN或扩散模型，虽提升了感知质量，但可能扭曲类内结构，导致疾病类别边界不清，影响下游临床任务。需要一种方式在提升画质的同时保留疾病类别结构。

Method: 作者采用Gromov Wasserstein距离对分布进行结构对齐，该方法能自然保留类内结构，但传统GW计算量大。为提升效率，提出了利用随机投影进行近似的Sliced GW（SGW），并将其首次用于GAN的医学图像增强中。

Result: 在公开数据集上的实验表明，SGW-GAN在视觉质量、糖尿病视网膜病变分级准确率以及疾病标签间的GW距离方面均优于现有方法。

Conclusion: SGW-GAN能够高效地提升视网膜照画质，兼顾视觉与临床效果，为无配对医学图像增强提供了更具结构保真的解决方案。

Abstract: Retinal fundus photography is indispensable for ophthalmic screening and diagnosis, yet image quality is often degraded by noise, artifacts, and uneven illumination. Recent GAN- and diffusion-based enhancement methods improve perceptual quality by aligning degraded images with high-quality distributions, but our analysis shows that this focus can distort intra-class geometry: clinically related samples become dispersed, disease-class boundaries blur, and downstream tasks such as grading or lesion detection are harmed. The Gromov Wasserstein (GW) discrepancy offers a principled solution by aligning distributions through internal pairwise distances, naturally preserving intra-class structure, but its high computational cost restricts practical use. To overcome this, we propose SGW-GAN, the first framework to incorporate Sliced GW (SGW) into retinal image enhancement. SGW approximates GW via random projections, retaining relational fidelity while greatly reducing cost. Experiments on public datasets show that SGW-GAN produces visually compelling enhancements, achieves superior diabetic retinopathy grading, and reports the lowest GW discrepancy across disease labels, demonstrating both efficiency and clinical fidelity for unpaired medical image enhancement.

</details>


### [199] [Analyzing VLM-Based Approaches for Anomaly Classification and Segmentation](https://arxiv.org/abs/2601.13440)
*Mohit Kakda,Mirudula Shri Muthukumaran,Uttapreksha Patel,Lawrence Swaminathan Xavier Prince*

Main category: cs.CV

TL;DR: 本文系统分析了基于视觉-语言模型（VLM）的异常检测方法，比较了多种架构和策略，并在多个基准数据集上进行了实验，得出VLM在零样本和小样本异常检测方面的有效性及实际应用建议。


<details>
  <summary>Details</summary>
Motivation: 当前异常检测依赖大量标注数据和有针对性的模型训练，传统方法难以适用于实际中多变和少样本的缺陷场景。近年来视觉-语言模型（如CLIP）展现出零样本、少样本检测能力，因此有必要深入分析和总结其在异常分类、分割等任务中的表现和适用性。

Method: 论文系统性地回顾并分析了几种主流VLM架构在异常分类和分割中的应用，包括滑窗密集特征提取（WinCLIP）、多阶段特征对齐与可学习投影（AprilLab框架）、组合提示集等。评估维度包括特征提取、文本-视觉对齐、提示工程、零样本/少样本权衡、效率与跨领域泛化等。通过在MVTec AD和VisA等基准上的实验，比较各方法分类、分割精度及推理效率。

Result: 不同方法在分类和分割精度、推理效率等方面各有优势。VLM在无需专门缺陷样本或大规模标注的前提下，能通过自然语言描述有效进行异常检测，具有良好的迁移与泛化能力。滑窗、可学习对齐和组合提示策略对不同场景有提升作用。

Conclusion: VLM方法为工业缺陷检测带来了变革性进步，尤其适合数据稀缺或需求快速适配的新场景。文中分析有助于实际方法选择与优化，并指出了如推理效率、领域扩展等当前瓶颈，为未来研究提供了方向。

Abstract: Vision-Language Models (VLMs), particularly CLIP, have revolutionized anomaly detection by enabling zero-shot and few-shot defect identification without extensive labeled datasets. By learning aligned representations of images and text, VLMs facilitate anomaly classification and segmentation through natural language descriptions of normal and abnormal states, eliminating traditional requirements for task-specific training or defect examples. This project presents a comprehensive analysis of VLM-based approaches for anomaly classification (AC) and anomaly segmentation (AS). We systematically investigate key architectural paradigms including sliding window-based dense feature extraction (WinCLIP), multi-stage feature alignment with learnable projections (AprilLab framework), and compositional prompt ensemble strategies. Our analysis evaluates these methods across critical dimensions: feature extraction mechanisms, text-visual alignment strategies, prompt engineering techniques, zero-shot versus few-shot trade-offs, computational efficiency, and cross-domain generalization. Through rigorous experimentation on benchmarks such as MVTec AD and VisA, we compare classification accuracy, segmentation precision, and inference efficiency. The primary contribution is a foundational understanding of how and why VLMs succeed in anomaly detection, synthesizing practical insights for method selection and identifying current limitations. This work aims to facilitate informed adoption of VLM-based methods in industrial quality control and guide future research directions.

</details>


### [200] [Optical Linear Systems Framework for Event Sensing and Computational Neuromorphic Imaging](https://arxiv.org/abs/2601.13498)
*Nimrod Kruger,Nicholas Owen Ralph,Gregory Cohen,Paul Hurley*

Main category: cs.CV

TL;DR: 本文提出了一种基于物理的处理流程，将事件视觉传感器的数据映射为逐像素的对数强度及其导数，并嵌入动态线性系统模型，实现频域逆滤波，用于动态光学系统下的计算成像。


<details>
  <summary>Details</summary>
Motivation: 事件视觉传感器由于其非线性输出，与主流基于线性前向模型的计算成像与光学系统建模无法直接结合，因此需要一种新的方法实现两者的有效融合。

Method: 提出将事件流映射到每像素对数强度与强度导数，并接入时变点扩散函数的动态线性系统。通过已知或参数化的动态传递函数，利用频域Wiener去卷积，从事件数据中直接实现逆滤波。

Result: 在仿真中，该方法对单点及重叠点源在调制散焦情况下表现良好；实际场景中，用可调焦望远镜对星场的事件数据进行测试，算法能有效实现目标定位和分离。

Conclusion: 该框架为事件视觉传感与基于模型的动态光学计算成像之间提供了有效桥梁，拓展了事件相机在动态场景中的实用性和适用范围。

Abstract: Event vision sensors (neuromorphic cameras) output sparse, asynchronous ON/OFF events triggered by log-intensity threshold crossings, enabling microsecond-scale sensing with high dynamic range and low data bandwidth. As a nonlinear system, this event representation does not readily integrate with the linear forward models that underpin most computational imaging and optical system design. We present a physics-grounded processing pipeline that maps event streams to estimates of per-pixel log-intensity and intensity derivatives, and embeds these measurements in a dynamic linear systems model with a time-varying point spread function. This enables inverse filtering directly from event data, using frequency-domain Wiener deconvolution with a known (or parameterised) dynamic transfer function. We validate the approach in simulation for single and overlapping point sources under modulated defocus, and on real event data from a tunable-focus telescope imaging a star field, demonstrating source localisation and separability. The proposed framework provides a practical bridge between event sensing and model-based computational imaging for dynamic optical systems.

</details>


### [201] [DIS2: Disentanglement Meets Distillation with Classwise Attention for Robust Remote Sensing Segmentation under Missing Modalities](https://arxiv.org/abs/2601.13502)
*Nhi Kieu,Kien Nguyen,Arnold Wiliem,Clinton Fookes,Sridha Sridharan*

Main category: cs.CV

TL;DR: 本文提出面向遥感多模态学习缺失模态问题的新方法DIS2，有效提升了在大规模异构数据场景下的性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 遥感领域中存在强异构性和尺度变化大的多模态数据，常有模态缺失。传统依赖模态重叠或简单知识蒸馏的方法在此情景下效果有限。因此，亟需开发针对遥感多模态缺失补偿与特征融合的新范式。

Method: 提出DIS2方法，包括三大创新：一是有原则的缺失信息补偿；二是类别特异的模态贡献学习（CFLM模块自适应针对各类别学习判别性特征）；三是多分辨率特征融合（采用分层混合融合结构）。核心技术是将解耦学习与知识蒸馏有机结合，专注于补偿性特征显式建模与融合。

Result: 大量实验证明DIS2在遥感多模态缺失任务中，于多个标准数据集上均显著优于最新方法，表现出强泛化和鲁棒性。

Conclusion: DIS2为遥感领域多模态缺失学习提供了全新解决思路，尤其在异构性强和尺度变化大的复杂场景下，展现了极高的实用价值。

Abstract: The efficacy of multimodal learning in remote sensing (RS) is severely undermined by missing modalities. The challenge is exacerbated by the RS highly heterogeneous data and huge scale variation. Consequently, paradigms proven effective in other domains often fail when confronted with these unique data characteristics. Conventional disentanglement learning, which relies on significant feature overlap between modalities (modality-invariant), is insufficient for this heterogeneity. Similarly, knowledge distillation becomes an ill-posed mimicry task where a student fails to focus on the necessary compensatory knowledge, leaving the semantic gap unaddressed. Our work is therefore built upon three pillars uniquely designed for RS: (1) principled missing information compensation, (2) class-specific modality contribution, and (3) multi-resolution feature importance. We propose a novel method DIS2, a new paradigm shifting from modality-shared feature dependence and untargeted imitation to active, guided missing features compensation. Its core novelty lies in a reformulated synergy between disentanglement learning and knowledge distillation, termed DLKD. Compensatory features are explicitly captured which, when fused with the features of the available modality, approximate the ideal fused representation of the full-modality case. To address the class-specific challenge, our Classwise Feature Learning Module (CFLM) adaptively learn discriminative evidence for each target depending on signal availability. Both DLKD and CFLM are supported by a hierarchical hybrid fusion (HF) structure using features across resolutions to strengthen prediction. Extensive experiments validate that our proposed approach significantly outperforms state-of-the-art methods across benchmarks.

</details>


### [202] [GO-MLVTON: Garment Occlusion-Aware Multi-Layer Virtual Try-On with Diffusion Models](https://arxiv.org/abs/2601.13524)
*Yang Yu,Yunze Deng,Yige Zhang,Yanjie Xiao,Youkun Ou,Wenhao Hu,Mingchao Li,Bin Feng,Wenyu Liu,Dandan Zheng,Jingdong Chen*

Main category: cs.CV

TL;DR: 本文提出了GO-MLVTON，这是首个可处理多层服装虚拟试穿任务的方法，并提出了相关数据集与评估指标，实验结果优异。


<details>
  <summary>Details</summary>
Motivation: 现有的虚拟试穿方法大多只考虑单层或多件服装，未能有效处理多层服装叠穿，主要因为模拟服装间的层次与遮挡关系较难，容易引入冗余特征。

Method: 提出了GO-MLVTON方法，包括服装遮挡关系学习模块和基于StableDiffusion的服装变形拟合模块，实现多层服装的交互穿搭与自然叠加。还发布了MLG多层服装数据集，以及新评估指标LACD。

Result: 大量实验表明，GO-MLVTON在多层虚拟试穿任务中表现出色，优于现有方法。

Conclusion: GO-MLVTON首次实现了高质量多层虚拟试穿，推动了该领域发展，为多层服装相关应用提供了更好方案和数据支持。

Abstract: Existing Image-based virtual try-on (VTON) methods primarily focus on single-layer or multi-garment VTON, neglecting multi-layer VTON (ML-VTON), which involves dressing multiple layers of garments onto the human body with realistic deformation and layering to generate visually plausible outcomes. The main challenge lies in accurately modeling occlusion relationships between inner and outer garments to reduce interference from redundant inner garment features. To address this, we propose GO-MLVTON, the first multi-layer VTON method, introducing the Garment Occlusion Learning module to learn occlusion relationships and the StableDiffusion-based Garment Morphing & Fitting module to deform and fit garments onto the human body, producing high-quality multi-layer try-on results. Additionally, we present the MLG dataset for this task and propose a new metric named Layered Appearance Coherence Difference (LACD) for evaluation. Extensive experiments demonstrate the state-of-the-art performance of GO-MLVTON. Project page: https://upyuyang.github.io/go-mlvton/.

</details>


### [203] [DiffFace-Edit: A Diffusion-Based Facial Dataset for Forgery-Semantic Driven Deepfake Detection Analysis](https://arxiv.org/abs/2601.13551)
*Feng Ding,Wenhui Yi,Xinan He,Mengyao Xiao,Jianfeng Xu,Jianqiang Du*

Main category: cs.CV

TL;DR: 论文提出了DiffFace-Edit数据集，专注于AI生成的人脸的细粒度区域篡改，弥补了现有数据集的不足，并研究了此类攻击对检测器的影响。数据集含有大量图像和多区域编辑组合。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成的人脸数据集很少专注于细粒度区域性篡改样本，且未有研究系统分析真实与篡改样本结合（即拼接攻击）对检测模型的实际影响。为提升检测方法的实用性及鲁棒性，亟需细致的数据集和相关分析。

Method: 构建了DiffFace-Edit数据集，包含200万以上AI生成的人脸假图像，涵盖眼睛、鼻子等8个人脸区域的编辑，并包括单区域与多区域等多种组合。此外，对易逃避检测器的样本进行了详细分析，并提出跨域评测方法与IMDL方法结合用于模型评测。

Result: 生成了大规模、多类型、注重细粒度操控的人脸造假数据集，并通过实验证明，一些特定区域的篡改样本对检测器具有较强的干扰作用；跨域评测揭示了现有检测模型在新型篡改样本下的脆弱性。

Conclusion: DiffFace-Edit数据集可以极大促进区域性人脸编辑与假脸检测研究，为未来检测器的鲁棒性提升和拼接攻击应对提供重要资源。

Abstract: Generative models now produce imperceptible, fine-grained manipulated faces, posing significant privacy risks. However, existing AI-generated face datasets generally lack focus on samples with fine-grained regional manipulations. Furthermore, no researchers have yet studied the real impact of splice attacks, which occur between real and manipulated samples, on detectors. We refer to these as detector-evasive samples. Based on this, we introduce the DiffFace-Edit dataset, which has the following advantages: 1) It contains over two million AI-generated fake images. 2) It features edits across eight facial regions (e.g., eyes, nose) and includes a richer variety of editing combinations, such as single-region and multi-region edits. Additionally, we specifically analyze the impact of detector-evasive samples on detection models. We conduct a comprehensive analysis of the dataset and propose a cross-domain evaluation that combines IMDL methods. Dataset will be available at https://github.com/ywh1093/DiffFace-Edit.

</details>


### [204] [ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch](https://arxiv.org/abs/2601.13606)
*Zheng Liu,Honglin Lin,Chonghan Qin,Xiaoyang Wang,Xin Gao,Yu Li,Mengzhang Cai,Yun Zhu,Zhanping Zhong,Qizhi Pei,Zhuoshi Pan,Xiaoran Shang,Bin Cui,Conghui He,Wentao Zhang,Lijun Wu*

Main category: cs.CV

TL;DR: 本论文提出了ChartVerse框架，能自动合成高复杂度图表和高质量推理数据，极大提升了开源视觉语言模型在图表推理上的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的开源模型由于缺乏优质的图表推理训练数据，尤其是在图表复杂度和推理深度上的缺陷，严重影响了模型效果。因此，急需能生成复杂图表及可靠推理数据的方法。

Method: 1）提出Rollout Posterior Entropy（RPE）指标度量图表复杂度，并基于此开发复杂度感知的自动图表合成器，通过程序自动生成丰富且复杂的图表；2）创新性地采用“先答案后问题”的逆QA流程，从源代码确定无歧义答案，再反向生成高一致性和高推理深度的问题，并通过模型失误率筛选样本及CoT蒸馏，最终得到高质数据集。

Result: 利用ChartVerse框架构建的数据集训练的ChartVerse-8B模型，在图表推理任务上达到了SOTA水平，不仅超越了老师模型，还可与更大的Qwen3-VL-32B-Thinking媲美。

Conclusion: ChartVerse极大提升了合成图表数据的复杂度与推理数据的质量，为VLM图表推理研究提供了强力的数据基础和新思路，对行业有重要推动作用。

Abstract: Chart reasoning is a critical capability for Vision Language Models (VLMs). However, the development of open-source models is severely hindered by the lack of high-quality training data. Existing datasets suffer from a dual challenge: synthetic charts are often simplistic and repetitive, while the associated QA pairs are prone to hallucinations and lack the reasoning depth required for complex tasks. To bridge this gap, we propose ChartVerse, a scalable framework designed to synthesize complex charts and reliable reasoning data from scratch. (1) To address the bottleneck of simple patterns, we first introduce Rollout Posterior Entropy (RPE), a novel metric that quantifies chart complexity. Guided by RPE, we develop complexity-aware chart coder to autonomously synthesize diverse, high-complexity charts via executable programs. (2) To guarantee reasoning rigor, we develop truth-anchored inverse QA synthesis. Diverging from standard generation, we adopt an answer-first paradigm: we extract deterministic answers directly from the source code, generate questions conditional on these anchors, and enforce strict consistency verification. To further elevate difficulty and reasoning depth, we filter samples based on model fail-rate and distill high-quality Chain-of-Thought (CoT) reasoning. We curate ChartVerse-SFT-600K and ChartVerse-RL-40K using Qwen3-VL-30B-A3B-Thinking as the teacher. Experimental results demonstrate that ChartVerse-8B achieves state-of-the-art performance, notably surpassing its teacher and rivaling the stronger Qwen3-VL-32B-Thinking.

</details>


### [205] [CARPE: Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models](https://arxiv.org/abs/2601.13622)
*Donghee Lee,Rui Cai,Zhe Zhao*

Main category: cs.CV

TL;DR: 本文提出了CARPE框架，通过引入视觉整合层和上下文感知的集成策略，有效提升了大型视觉语言模型在图像分类等视觉任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管LVLMs在多模态任务上表现强劲，但在纯视觉任务（如图像分类）上仍落后于基础视觉编码器，有提高空间。

Method: 提出一种模型无关的框架CARPE，引入视觉集成层和上下文感知的集成方法，动态权衡视觉表征与语言模型推理能力，提升多模态表征能力。

Result: CARPE在图像分类和多种视觉-语言基准测试中均取得了明显性能提升，展现出良好的泛化能力。

Conclusion: CARPE能广泛兼容主流LVLM架构，为视觉语言任务带来全面性能提升，是通用和高适应性的解决方案。

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have pushed them closer to becoming general-purpose assistants. Despite their strong performance, LVLMs still struggle with vision-centric tasks such as image classification, underperforming compared to their base vision encoders, which are often CLIP-based models. To address this limitation, we propose Context-Aware Image Representation Prioritization via Ensemble (CARPE), a novel, model-agnostic framework which introduces vision-integration layers and a context-aware ensemble strategy to identify when to prioritize image representations or rely on the reasoning capabilities of the language model. This design enhances the model's ability to adaptively weight visual and textual modalities and enables the model to capture various aspects of image representations, leading to consistent improvements in generalization across classification and vision-language benchmarks. Extensive experiments demonstrate that CARPE not only improves performance on image classification benchmarks but also enhances results across various vision-language benchmarks. Finally, CARPE is designed to be effectively integrated with most open-source LVLMs that consist of a vision encoder and a language model, ensuring its adaptability across diverse architectures.

</details>


### [206] [Scaling Test-time Inference for Visual Grounding](https://arxiv.org/abs/2601.13633)
*Guanqi Zhan,Changye Li,Zhijian Liu,Yao Lu,Yi Wu,Song Han,Ligeng Zhu*

Main category: cs.CV

TL;DR: 提出了一种高效视觉指代消解模型（EGM），能在保证性能的同时大幅提升推理速度，适用于实际部署。


<details>
  <summary>Details</summary>
Motivation: 当前主流的视觉语言指代消解（visual grounding）模型体积庞大，推理速度慢，不利于实际部署。论文发现性能关键差距主要在于语言模型规模而非视觉编码器，提出方法以缓解小模型语言理解能力弱导致的性能劣势。

Method: 提出Efficient visual Grounding language Models（EGM），通过扩展测试时小模型生成token的数量（即增加生成长度）来提升其指代消解能力，同时维持较低的推理消耗；采用RefCOCO等数据集进行实验验证，并提出新的amodal grounding任务进一步检验方法泛化性。

Result: 在RefCOCO基准上，小型模型EGM-Qwen3-VL-8B达到91.4 IoU，仅需737ms，显著快于大型模型Qwen3-VL-235B（4320ms，90.5 IoU）；在标准和amodal grounding场景下，均显著提升小模型性能，使其达到或超越大模型。

Conclusion: EGM方法显著提升小型视觉语言模型在视觉指代消解任务上的性能与效率，提升了实际部署的可行性，也推动了小型模型在更复杂场景（如amodal grounding）中的应用潜力。

Abstract: Visual grounding is an essential capability of Visual Language Models (VLMs) to understand the real physical world. Previous state-of-the-art grounding visual language models usually have large model sizes, making them heavy for deployment and slow for inference. However, we notice that the sizes of visual encoders are nearly the same for small and large VLMs and the major difference is the sizes of the language models. Small VLMs fall behind larger VLMs in grounding because of the difference in language understanding capability rather than visual information handling. To mitigate the gap, we introduce 'Efficient visual Grounding language Models' (EGM): a method to scale the test-time computation (#generated tokens). Scaling the test-time computation of a small model is deployment-friendly, and yields better end-to-end latency as the cost of each token is much cheaper compared to directly running a large model. On the RefCOCO benchmark, our EGM-Qwen3-VL-8B demonstrates 91.4 IoU with an average of 737ms (5.9x faster) latency while Qwen3-VL-235B demands 4,320ms to achieve 90.5 IoU. To validate our approach's generality, we further set up a new amodal grounding setting that requires the model to predict both the visible and occluded parts of the objects. Experiments show our method can consistently and significantly improve the vanilla grounding and amodal grounding capabilities of small models to be on par with or outperform the larger models, thereby improving the efficiency for visual grounding.

</details>


### [207] [Face-Voice Association with Inductive Bias for Maximum Class Separation](https://arxiv.org/abs/2601.13651)
*Marta Moscati,Oleksandr Kats,Mubashir Noman,Muhammad Zaigham Zaheer,Yufang Hou,Markus Schedl,Shah Nawaz*

Main category: cs.CV

TL;DR: 本文提出在面部-语音关联任务中引入最大类别分离归纳偏置，有效提升模型性能，并首次验证这一做法在多模态学习中的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管现有方法通过损失函数优化面部和语音的嵌入表示，实现同一人物的特征靠近、不同人物远离，但尚未借鉴分类任务中通过最大类别分离强化嵌入判别力的方法。作者希望填补该领域的空白。

Method: 提出一种在面部-语音关联中，针对不同说话人的多模态表征，施加最大类别分离作为归纳偏置的方法，并结合相互正交（orthogonality）的损失函数进行优化。

Result: 实验证明该方法在两种面部-语音关联任务上均达到了SOTA（最优）性能。消融实验显示，将最大类别分离归纳偏置与类别间正交损失结合效果最佳。

Conclusion: 本文首次在多模态学习中验证了最大类别分离归纳偏置的有效性，有望为该领域树立新范式。

Abstract: Face-voice association is widely studied in multimodal learning and is approached representing faces and voices with embeddings that are close for a same person and well separated from those of others. Previous work achieved this with loss functions. Recent advancements in classification have shown that the discriminative ability of embeddings can be strengthened by imposing maximum class separation as inductive bias. This technique has never been used in the domain of face-voice association, and this work aims at filling this gap. More specifically, we develop a method for face-voice association that imposes maximum class separation among multimodal representations of different speakers as an inductive bias. Through quantitative experiments we demonstrate the effectiveness of our approach, showing that it achieves SOTA performance on two task formulation of face-voice association. Furthermore, we carry out an ablation study to show that imposing inductive bias is most effective when combined with losses for inter-class orthogonality. To the best of our knowledge, this work is the first that applies and demonstrates the effectiveness of maximum class separation as an inductive bias in multimodal learning; it hence paves the way to establish a new paradigm.

</details>


### [208] [VIAFormer: Voxel-Image Alignment Transformer for High-Fidelity Voxel Refinement](https://arxiv.org/abs/2601.13664)
*Tiancheng Fang,Bowen Pan,Lingxi Chen,Jiangjing Lyu,Chengfei Lyu,Chaoyue Niu,Fan Wu*

Main category: cs.CV

TL;DR: VIAFormer是一种用于多视角条件下体素修复的新型Transformer模型，有效利用2D多视图图像来修正3D体素数据，取得了新SOTA，并具备实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 三维体素数据通常存在不完整和噪声问题，尤其是依赖视觉基础模型时更易出现失真，急需一种方法结合多视图2D图像信息来高效修复体素。

Method: 提出VIAFormer模型，核心创新包括：（1）图像索引机制将2D图像token与3D空间显式关联；（2）校正流目标直接学习体素修正轨迹；（3）混合流Transformer实现坚韧的跨模态信息融合。模型输入为损坏体素和多视图图像，输出修正后体素。

Result: 在合成和真实体素损坏数据集上，VIAFormer大幅超越现有技术，能有效修正从视觉基础模型得到的畸变形状。

Conclusion: VIAFormer为体素修复领域提供了有效实用的新方法，可作为大模型和大数据浪潮下的3D应用桥梁，推动体素方法广泛应用于实际三维内容生产。

Abstract: We propose VIAFormer, a Voxel-Image Alignment Transformer model designed for Multi-view Conditioned Voxel Refinement--the task of repairing incomplete noisy voxels using calibrated multi-view images as guidance. Its effectiveness stems from a synergistic design: an Image Index that provides explicit 3D spatial grounding for 2D image tokens, a Correctional Flow objective that learns a direct voxel-refinement trajectory, and a Hybrid Stream Transformer that enables robust cross-modal fusion. Experiments show that VIAFormer establishes a new state of the art in correcting both severe synthetic corruptions and realistic artifacts on the voxel shape obtained from powerful Vision Foundation Models. Beyond benchmarking, we demonstrate VIAFormer as a practical and reliable bridge in real-world 3D creation pipelines, paving the way for voxel-based methods to thrive in large-model, big-data wave.

</details>


### [209] [Transformer based Multi-task Fusion Network for Food Spoilage Detection and Shelf life Forecasting](https://arxiv.org/abs/2601.13665)
*Mounika Kanulla,Rajasree Dadigi,Sailaja Thota,Vivek Yelleti*

Main category: cs.CV

TL;DR: 本文提出融合CNN、LSTM和DeiT Transformer的新型深度学习架构，实现蔬菜分类、食品变质检测和保质期预测三项任务，显著优于其它主流模型。


<details>
  <summary>Details</summary>
Motivation: 食品浪费严重影响农业供应链，通过准确检测变质及预测食品保质期，有助于减少浪费和提升供应链管理水平。

Method: 构建从新鲜到完全变质的蔬菜图像数据集，采用CNN与LSTM、DeiT Transformer融合模型进行蔬菜分类、变质检测与保质期预测任务，并与多种主流深度学习模型进行对比。

Result: 融合模型CNN+CNN-LSTM和CNN+DeiT Transformer在多项任务上均超越CNN、VGG16、ResNet50、Capsule Network和单独DeiT Transformer，其中CNN+DeiT Transformer在蔬菜分类F1分数达0.98，变质检测F1分数为0.61，保质期预测MSE为3.58，SMAPE为41.66%。

Conclusion: 提出的融合模型不仅准确度高，在噪声图像上表现稳定，并通过LIME可视化模型决策，具备良好实用性与可靠性，帮助降低农业供应链食品浪费。

Abstract: Food wastage is one of the critical challenges in the agricultural supply chain, and accurate and effective spoilage detection can help to reduce it. Further, it is highly important to forecast the spoilage information. This aids the longevity of the supply chain management in the agriculture field. This motivated us to propose fusion based architectures by combining CNN with LSTM and DeiT transformer for the following multi-tasks simultaneously: (i) vegetable classification, (ii) food spoilage detection, and (iii) shelf life forecasting. We developed a dataset by capturing images of vegetables from their fresh state until they were completely spoiled. From the experimental analysis it is concluded that the proposed fusion architectures CNN+CNN-LSTM and CNN+DeiT Transformer outperformed several deep learning models such as CNN, VGG16, ResNet50, Capsule Networks, and DeiT Transformers. Overall, CNN + DeiT Transformer yielded F1-score of 0.98 and 0.61 in vegetable classification and spoilage detection respectively and mean squared error (MSE) and symmetric mean absolute percentage error (SMAPE) of 3.58, and 41.66% respectively in spoilage forecasting. Further, the reliability of the fusion models was validated on noisy images and integrated with LIME to visualize the model decisions.

</details>


### [210] [Finally Outshining the Random Baseline: A Simple and Effective Solution for Active Learning in 3D Biomedical Imaging](https://arxiv.org/abs/2601.13677)
*Carsten T. Lüth,Jeremias Traub,Kim-Celine Kahl,Till J. Bungert,Lukas Klein,Lars Krämer,Paul F. Jäger,Klaus Maier-Hein,Fabian Isensee*

Main category: cs.CV

TL;DR: 本文提出了ClaSP PE主动学习方法，有效提升了3D生物医学图像分割的效率与准确率，并首次在多个数据集上显著优于随机采样基线。


<details>
  <summary>Details</summary>
Motivation: 3D生物医学图像分割需要专家对体数据进行标注，成本高昂且耗时，现有主动学习方法在实际效果上未能一贯优于随机采样基线，迫切需要一种更有效且易于应用的主动学习方案。

Method: 提出了一种新的主动学习查询策略ClaSP PE（Class-stratified Scheduled Power Predictive Entropy），结合了类分层查询（解决类别不平衡、确保少数类别被覆盖）和时序衰减的对数尺度噪声（初期提升多样性，后期增强利用），并在nnActive基准平台的24种实验设置、涉及4个3D医学数据集上进行了广泛评测。

Result: ClaSP PE是唯一在准确率和标注效率上显著超越改进随机基线的方法，并在4个未见过的新数据集上无需手动调整即可具有良好泛化能力。

Conclusion: ClaSP PE为3D分割领域提供了一个现实、稳健且优越的主动学习解决方案，显著提升分割表现和标注效率，且易于实际部署和推广。

Abstract: Active learning (AL) has the potential to drastically reduce annotation costs in 3D biomedical image segmentation, where expert labeling of volumetric data is both time-consuming and expensive. Yet, existing AL methods are unable to consistently outperform improved random sampling baselines adapted to 3D data, leaving the field without a reliable solution. We introduce Class-stratified Scheduled Power Predictive Entropy (ClaSP PE), a simple and effective query strategy that addresses two key limitations of standard uncertainty-based AL methods: class imbalance and redundancy in early selections. ClaSP PE combines class-stratified querying to ensure coverage of underrepresented structures and log-scale power noising with a decaying schedule to enforce query diversity in early-stage AL and encourage exploitation later. In our evaluation on 24 experimental settings using four 3D biomedical datasets within the comprehensive nnActive benchmark, ClaSP PE is the only method that generally outperforms improved random baselines in terms of both segmentation quality with statistically significant gains, whilst remaining annotation efficient. Furthermore, we explicitly simulate the real-world application by testing our method on four previously unseen datasets without manual adaptation, where all experiment parameters are set according to predefined guidelines. The results confirm that ClaSP PE robustly generalizes to novel tasks without requiring dataset-specific tuning. Within the nnActive framework, we present compelling evidence that an AL method can consistently outperform random baselines adapted to 3D segmentation, in terms of both performance and annotation efficiency in a realistic, close-to-production scenario. Our open-source implementation and clear deployment guidelines make it readily applicable in practice. Code is at https://github.com/MIC-DKFZ/nnActive.

</details>


### [211] [Dynamic Differential Linear Attention: Enhancing Linear Diffusion Transformer for High-Quality Image Generation](https://arxiv.org/abs/2601.13683)
*Boyuan Cao,Xingbo Yao,Chenhui Wang,Jiaxin Ye,Yujie Wei,Hongming Shan*

Main category: cs.CV

TL;DR: 提出了一种新型线性注意力机制DyDiLA，显著提升了扩散Transformer（LiTs）的生成性能。通过引入三项创新模块，有效解决了过度平滑问题，提升了生成图像质量。


<details>
  <summary>Details</summary>
Motivation: 扩散Transformer（DiTs）虽然能实现高质量图像生成，但自注意力的高计算成本成为瓶颈。线性注意力虽然降低了计算复杂度，但会带来性能下降，生成结果细节不足。如何兼顾效率和生成质量是核心痛点。

Method: 提出Dynamic Differential Linear Attention（DyDiLA），包含三项创新：(1) 动态投影模块，通过动态分配知识，增强token特征分离能力；(2) 动态度量核，动态选择核函数以更细致捕捉token语义差异；(3) token微分算子，利用信息冗余和token间差异提升检索能力。基于此方法，开发了DyDi-LiT模型。

Result: 实验表明DyDi-LiT在多个指标上优于当前最强的同类生成模型，提升了生成质量和模型表现力。

Conclusion: DyDiLA作为线性注意力的一种有效改进方法，为高效高质量的图像生成提供了新的技术路线，兼顾了生成能力和计算效率，具有较强的应用潜力。

Abstract: Diffusion transformers (DiTs) have emerged as a powerful architecture for high-fidelity image generation, yet the quadratic cost of self-attention poses a major scalability bottleneck. To address this, linear attention mechanisms have been adopted to reduce computational cost; unfortunately, the resulting linear diffusion transformers (LiTs) models often come at the expense of generative performance, frequently producing over-smoothed attention weights that limit expressiveness. In this work, we introduce Dynamic Differential Linear Attention (DyDiLA), a novel linear attention formulation that enhances the effectiveness of LiTs by mitigating the oversmoothing issue and improving generation quality. Specifically, the novelty of DyDiLA lies in three key designs: (i) dynamic projection module, which facilitates the decoupling of token representations by learning with dynamically assigned knowledge; (ii) dynamic measure kernel, which provides a better similarity measurement to capture fine-grained semantic distinctions between tokens by dynamically assigning kernel functions for token processing; and (iii) token differential operator, which enables more robust query-to-key retrieval by calculating the differences between the tokens and their corresponding information redundancy produced by dynamic measure kernel. To capitalize on DyDiLA, we introduce a refined LiT, termed DyDi-LiT, that systematically incorporates our advancements. Extensive experiments show that DyDi-LiT consistently outperforms current state-of-the-art (SOTA) models across multiple metrics, underscoring its strong practical potential.

</details>


### [212] [Reasoning or Pattern Matching? Probing Large Vision-Language Models with Visual Puzzles](https://arxiv.org/abs/2601.13705)
*Maria Lymperaiou,Vasileios Karampinis,Giorgos Filandrianos,Angelos Vlachos,Chrysoula Zerva,Athanasios Voulodimos*

Main category: cs.CV

TL;DR: 本文综述了将视觉谜题用于评估大规模视觉-语言模型（LVLM）推理能力的研究进展与现存问题。


<details>
  <summary>Details</summary>
Motivation: 视觉谜题能够在最小化先验知识依赖的前提下，有效考查抽象、规则发现与系统推理等核心认知能力。将其用于LVLM推理能力的诊断和评估，有助于更精确探测模型的不足与潜能。

Method: 作者将视觉谜题作为一种通用抽象，按所考查的推理机制（归纳、类比、算法、演绎、几何/空间）归类整理现有基准，并分析了这些基准与人类认知操作的映射关系。同时，系统梳理了各类谜题下LVLM表现的实证证据。

Result: 现有LVLM在视觉谜题各类推理任务中表现出若干一致性限制，包括泛化能力脆弱、感知与推理环节高度耦合，以及流畅解释能力与实际执行能力之间的差距。

Conclusion: 作者提出，视觉谜题应作为模型推理能力诊断工具，而非仅是任务形式。并据此展望了未来更具推理意识的多模态基准和系统设计方向。

Abstract: Puzzles have long served as compact and revealing probes of human cognition, isolating abstraction, rule discovery, and systematic reasoning with minimal reliance on prior knowledge. Leveraging these properties, visual puzzles have recently emerged as a powerful diagnostic tool for evaluating the reasoning abilities of Large Vision-Language Models (LVLMs), offering controlled, verifiable alternatives to open-ended multimodal benchmarks. This survey provides a unified perspective of visual puzzle reasoning in LVLMs. We frame visual puzzles through a common abstraction and organize existing benchmarks by the reasoning mechanisms they target (inductive, analogical, algorithmic, deductive, and geometric/spatial), thereby linking puzzle design to the cognitive operations required for solving. Synthesizing empirical evidence across these categories, we identify consistent limitations in current models, including brittle generalization, tight entanglement between perception and reasoning, and a persistent gap between fluent explanations and faithful execution. By framing visual puzzles as diagnostic instruments rather than task formats, this survey elaborates on the state of LVLM reasoning and outlines key directions for future benchmarks and reasoning-aware multimodal systems.

</details>


### [213] [ParkingTwin: Training-Free Streaming 3D Reconstruction for Parking-Lot Digital Twins](https://arxiv.org/abs/2601.13706)
*Xinhao Liu,Yu Wang,Xiansheng Guo,Gordon Owusu Boateng,Yu Cao,Haonan Si,Xingchen Guo,Nirwan Ansari*

Main category: cs.CV

TL;DR: 本文提出了ParkingTwin系统，一种无需训练、轻量级、可在线流式的高保真停车场数字孪生3D重建方法，解决了传统方法在视角稀疏、动态遮挡和光照极端情况下的难题，并显著提升了重建速度和资源效率。


<details>
  <summary>Details</summary>
Motivation: 自动代客泊车（AVP）等应用需要高精度的停车场数字孪生，用于路径规划、碰撞检测和感知验证。但现有基于机器人取向的重建方法面临视角稀疏引发的几何不适定、动态遮挡和极端光照带来的融合不稳定，以及神经渲染消耗高端算力与离线优化资源等难题。

Method: 提出ParkingTwin系统：1）利用OpenStreetMap(OSM)语义拓扑代替盲搜直接生成测地一致的TSDF，免除繁重的几何优化；2）引入法线/高度/深度约束的几何感知动态滤波，实时剔除动态障碍和遮挡；3）通过CIELAB颜色空间下的自适应亮度加权和深度梯度抑制，提升极端光照下的纹理融合稳健性。

Result: ParkingTwin在普通GTX 1660上可达30+FPS，在68000平方米真实数据集上获得0.87的结构相似性（比SOTA提升16%），端到端速度提高约15倍，GPU显存减少83.3%，大幅优于当前主流3D高斯喷洒算法（如需RTX4090D），并可导出三角网格用于主流引擎。

Conclusion: ParkingTwin为AVP提供了训练无关、流式高效、显著压缩硬件资源的停车场3D数字孪生重建方案，在保证高保真的同时，有很强工业落地和实际推广潜力。

Abstract: High-fidelity parking-lot digital twins provide essential priors for path planning, collision checking, and perception validation in Automated Valet Parking (AVP). Yet robot-oriented reconstruction faces a trilemma: sparse forward-facing views cause weak parallax and ill-posed geometry; dynamic occlusions and extreme lighting hinder stable texture fusion; and neural rendering typically needs expensive offline optimization, violating edge-side streaming constraints. We propose ParkingTwin, a training-free, lightweight system for online streaming 3D reconstruction. First, OSM-prior-driven geometric construction uses OpenStreetMap semantic topology to directly generate a metric-consistent TSDF, replacing blind geometric search with deterministic mapping and avoiding costly optimization. Second, geometry-aware dynamic filtering employs a quad-modal constraint field (normal/height/depth consistency) to reject moving vehicles and transient occlusions in real time. Third, illumination-robust fusion in CIELAB decouples luminance and chromaticity via adaptive L-channel weighting and depth-gradient suppression, reducing seams under abrupt lighting changes. ParkingTwin runs at 30+ FPS on an entry-level GTX 1660. On a 68,000 m^2 real-world dataset, it achieves SSIM 0.87 (+16.0%), delivers about 15x end-to-end speedup, and reduces GPU memory by 83.3% compared with state-of-the-art 3D Gaussian Splatting (3DGS) that typically requires high-end GPUs (RTX 4090D). The system outputs explicit triangle meshes compatible with Unity/Unreal digital-twin pipelines. Project page: https://mihoutao-liu.github.io/ParkingTwin/

</details>


### [214] [Attention-space Contrastive Guidance for Efficient Hallucination Mitigation in LVLMs](https://arxiv.org/abs/2601.13707)
*Yujin Jo,Sangyoon Bae,Taesup Kim*

Main category: cs.CV

TL;DR: 本文提出了一种称为注意力空间对比引导（ACG）的方法，通过在自注意力层中构造视觉-语言与仅语言注意路径，有效缓解了大视觉-语言模型中的幻觉现象，并大幅提升了模型的视觉真实性与描述质量。


<details>
  <summary>Details</summary>
Motivation: 大视觉-语言模型在生成图像描述时，容易因过度依赖语言先验导致对象识别错误或与视觉内容不符的问题（幻觉），亟需一种高效消除幻觉的方法。

Method: 作者提出ACG方法，将幻觉缓解问题转化为对比引导，通过在单次前向计算中同时建立视觉-语言和语言-only路径，有效嵌入对比调控。同时提出正交校正消除单次近似带来的偏差，增强视觉信息贡献。

Result: 在CHAIR和POPE基准测试中，ACG方法不仅达到最先进的真实度和描述质量，还显著降低了计算成本，与多次前向推理的传统对比解码方法相比，延迟减少最高达2倍。

Conclusion: ACG为缓解大视觉-语言模型中的幻觉提供了高效、原理清晰的替代方案，兼顾性能提升与推理效率。

Abstract: Hallucinations in large vision-language models (LVLMs) often arise when language priors dominate over visual evidence, causing object misidentification and visually inconsistent descriptions. We address this issue by framing hallucination mitigation as contrastive guidance, steering generation toward visually grounded and semantically faithful text. This approach regulates the model's internal behavior by reducing over-dependence on language priors and contrasting visually grounded with language-only representations. We propose Attention-space Contrastive Guidance (ACG), a single-pass mechanism that operates within self-attention layers to construct both vision-language and language-only attention paths in a single forward computation. This integration enables computationally efficient guidance directly embedded in the model's representation contextualization. To correct approximation bias introduced by the single-pass formulation, we further apply an orthogonalized correction that removes components aligned with the language-only path, selectively amplifying visual contributions. Experiments on the CHAIR and POPE benchmarks show that ACG achieves state-of-the-art faithfulness and caption quality while significantly reducing computational cost. Our method establishes a principled and efficient alternative, reducing latency by up to 2x compared to prior contrastive decoding methods that require multiple forward passes.

</details>


### [215] [MVGD-Net: A Novel Motion-aware Video Glass Surface Detection Network](https://arxiv.org/abs/2601.13715)
*Yiwei Lu,Hao Huang,Tao Yan*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频中玻璃表面检测网络MVGD-Net，利用运动不一致性线索从视频序列检测玻璃表面，并发布了大规模数据集。实验显示该方法优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 玻璃表面在实际生活和工业环境中十分常见，会对基于视觉的系统（如机器人和无人机导航）造成威胁。由于玻璃反射或透射的特殊性，在视频中检测这些表面非常具有挑战性。现有方法对此越来越关注，但仍存在检测准确性等问题。本文基于运动不一致的观察，提出更有效的检测方案。

Method: 作者提出MVGD-Net网络，利用视频中玻璃区域与非玻璃区域运动速度不同的特点，设计了三个新颖的模块：1）跨尺度多模态融合模块（CMFM）融合空间特征和光流，2）历史引导注意力模块（HGAM）和时序交叉注意力模块（TCAM）增强时序特征，3）引入时空解码器（TSD）对空间和时序信息进行融合生成玻璃掩码。同时，作者构建了包含312种玻璃场景、共19,268帧的大数据集用于训练和评测。

Result: 实验结果表明，所提MVGD-Net在新数据集和现有评测基准上均优于最先进的方法，具有更高的玻璃检测准确率和鲁棒性。

Conclusion: MVGD-Net有效利用运动不一致性显著提升了视频玻璃表面检测能力，其提出的数据集促进了该领域研究。方法具有较强的实际应用价值。

Abstract: Glass surface ubiquitous in both daily life and professional environments presents a potential threat to vision-based systems, such as robot and drone navigation. To solve this challenge, most recent studies have shown significant interest in Video Glass Surface Detection (VGSD). We observe that objects in the reflection (or transmission) layer appear farther from the glass surfaces. Consequently, in video motion scenarios, the notable reflected (or transmitted) objects on the glass surface move slower than objects in non-glass regions within the same spatial plane, and this motion inconsistency can effectively reveal the presence of glass surfaces. Based on this observation, we propose a novel network, named MVGD-Net, for detecting glass surfaces in videos by leveraging motion inconsistency cues. Our MVGD-Net features three novel modules: the Cross-scale Multimodal Fusion Module (CMFM) that integrates extracted spatial features and estimated optical flow maps, the History Guided Attention Module (HGAM) and Temporal Cross Attention Module (TCAM), both of which further enhances temporal features. A Temporal-Spatial Decoder (TSD) is also introduced to fuse the spatial and temporal features for generating the glass region mask. Furthermore, for learning our network, we also propose a large-scale dataset, which comprises 312 diverse glass scenarios with a total of 19,268 frames. Extensive experiments demonstrate that our MVGD-Net outperforms relevant state-of-the-art methods.

</details>


### [216] [Hierarchical Long Video Understanding with Audiovisual Entity Cohesion and Agentic Search](https://arxiv.org/abs/2601.13719)
*Xinlei Yin,Xiulian Peng,Xiao Li,Zhiwei Xiong,Yan Lu*

Main category: cs.CV

TL;DR: HAVEN是一种用于长视频理解的新方法，结合了多模态实体聚合、分层视频索引和智能检索机制，从而实现在极长视频中的连贯和全面推理，并在LVBench上取得新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有长视频理解方法大多采用简单分块和检索增强生成，但会导致信息碎片化和整体连贯性损失，因此需要一种更结构化和一致性更强的长视频理解框架。

Method: HAVEN框架首先通过融合视觉和声音流的实体级表征，保持语义一致性，同时将内容组织为全局摘要、场景、片段和实体四级分层结构。然后引入agentic search机制，能在多层次动态检索和推理，实现连贯叙事重构和精细实体跟踪。

Result: 在LVBench数据集上，HAVEN方法整体准确率达到84.1%，在推理类任务上表现尤为突出，达到了80.1%的成绩，体现了方法的有效性。

Conclusion: 结构化、多模态推理对长视频的全面、一致理解非常有效，HAVEN显著提升了长视频理解的连贯性和查全率，设立了新的业界基准。

Abstract: Long video understanding presents significant challenges for vision-language models due to extremely long context windows. Existing solutions relying on naive chunking strategies with retrieval-augmented generation, typically suffer from information fragmentation and a loss of global coherence. We present HAVEN, a unified framework for long-video understanding that enables coherent and comprehensive reasoning by integrating audiovisual entity cohesion and hierarchical video indexing with agentic search. First, we preserve semantic consistency by integrating entity-level representations across visual and auditory streams, while organizing content into a structured hierarchy spanning global summary, scene, segment, and entity levels. Then we employ an agentic search mechanism to enable dynamic retrieval and reasoning across these layers, facilitating coherent narrative reconstruction and fine-grained entity tracking. Extensive experiments demonstrate that our method achieves good temporal coherence, entity consistency, and retrieval efficiency, establishing a new state-of-the-art with an overall accuracy of 84.1% on LVBench. Notably, it achieves outstanding performance in the challenging reasoning category, reaching 80.1%. These results highlight the effectiveness of structured, multimodal reasoning for comprehensive and context-consistent understanding of long-form videos.

</details>


### [217] [Facial Spatiotemporal Graphs: Leveraging the 3D Facial Surface for Remote Physiological Measurement](https://arxiv.org/abs/2601.13724)
*Sam Cantrill,David Ahmedt-Aristizabal,Lars Petersson,Hanna Suominen,Mohammad Ali Armin*

Main category: cs.CV

TL;DR: 本文提出了一种新的人脸遥感光电容积描记（rPPG）方法，结合3D人脸网格和时空图卷积，实现更准确的生理信号估计，取得了最佳或有竞争力的表现。


<details>
  <summary>Details</summary>
Motivation: 目前主流的人脸rPPG方法未能将特征提取区域与3D人脸表面（即rPPG信号的空间载体）精确对齐，导致信号估计受限。为解决这一局限，作者提出针对人脸表面的建模方式。

Method: 作者提出了一种新的STGraph（时空图）表示方法，将人脸颜色和结构信息融合于3D人脸网格序列之中，实现表面对齐的时空建模。并提出MeshPhys模型，利用轻量级图卷积神经网络在STGraph上估计生理信号。

Result: 在四个主流数据集上的实验结果显示，MeshPhys在数据集内及跨数据集的设置下均达到业界领先或有竞争力的性能。消融实验发现，约束感受野至人脸表面能显著提升模型效果，且基于3D结构的信息对面部颜色编码尤为关键。

Conclusion: 作者提出的STGraph与MeshPhys为人脸rPPG建模建立了全新且严谨的范式，该方法对信号估计更健壮、可解释且具高度泛化性。

Abstract: Facial remote photoplethysmography (rPPG) methods estimate physiological signals by modeling subtle color changes on the 3D facial surface over time. However, existing methods fail to explicitly align their receptive fields with the 3D facial surface-the spatial support of the rPPG signal. To address this, we propose the Facial Spatiotemporal Graph (STGraph), a novel representation that encodes facial color and structure using 3D facial mesh sequences-enabling surface-aligned spatiotemporal processing. We introduce MeshPhys, a lightweight spatiotemporal graph convolutional network that operates on the STGraph to estimate physiological signals. Across four benchmark datasets, MeshPhys achieves state-of-the-art or competitive performance in both intra- and cross-dataset settings. Ablation studies show that constraining the model's receptive field to the facial surface acts as a strong structural prior, and that surface-aligned, 3D-aware node features are critical for robustly encoding facial surface color. Together, the STGraph and MeshPhys constitute a novel, principled modeling paradigm for facial rPPG, enabling robust, interpretable, and generalizable estimation. Code is available at https://samcantrill.github.io/facial-stgraph-rppg/ .

</details>


### [218] [HiT: History-Injection Transformers for Onboard Continuous Flood Change Detection](https://arxiv.org/abs/2601.13751)
*Daniel Kyselica,Jonáš Herec,Oliver Kutis,Rado Pitoňák*

Main category: cs.CV

TL;DR: 该论文提出了一种适用于小卫星在轨洪水检测的高效变换器模型HiT，通过历史信息注入机制，实现极大数据压缩与准确检测，并且可以实时运行于纳卫星硬件。


<details>
  <summary>Details</summary>
Motivation: 自然灾害监测（如洪水）需依赖卫星持续观测，但受限于计算和存储资源，难以对大量多时相数据进行高效处理。本研究旨在解决小卫星平台在内存与算力极其有限条件下，如何实现实时、连续的洪水变化检测。

Method: 提出了一种历史注入机制（HiT）的变换器模型，通过保存先前观测的历史上下文，实现仅用原始图像1%的存储量维护长期信息，并集成到小型基础模型Prithvi-tiny中，在典型低功耗硬件（Jetson Orin Nano）上测试其实际性能。

Result: 实验证明，HiT-Prithvi模型在严格存储约束下依旧能维持与传统双时相方法相当的检测准确率，并在纳卫星用硬件上达到43 FPS的速度，高效且实时。

Conclusion: 该方法为在卫星平台上实现无需地面依赖的实时自然灾害连续监测提供了切实可行的方案，有助于提升灾害应对与管理的效率，支持实际部署。

Abstract: Natural disaster monitoring through continuous satellite observation requires processing multi-temporal data under strict operational constraints. This paper addresses flood detection, a critical application for hazard management, by developing an onboard change detection system that operates within the memory and computational limits of small satellites. We propose History Injection mechanism for Transformer models (HiT), that maintains historical context from previous observations while reducing data storage by over 99\% of original image size. Moreover, testing on the STTORM-CD flood dataset confirms that the HiT mechanism within the Prithvi-tiny foundation model maintains detection accuracy compared to the bitemporal baseline. The proposed HiT-Prithvi model achieved 43 FPS on Jetson Orin Nano, a representative onboard hardware used in nanosats. This work establishes a practical framework for satellite-based continuous monitoring of natural disasters, supporting real-time hazard assessment without dependency on ground-based processing infrastructure. Architecture as well as model checkpoints is available at https://github.com/zaitra/HiT-change-detection

</details>


### [219] [PREGEN: Uncovering Latent Thoughts in Composed Video Retrieval](https://arxiv.org/abs/2601.13797)
*Gabriele Serussi,David Vainshtein,Jonathan Kouchly,Dotan Di Castro,Chaim Baskin*

Main category: cs.CV

TL;DR: 本文提出了一种高效的新方法PREGEN，显著提升了基于视频与文字修改的组合视频检索（CoVR）性能，且无需对现有视觉-语言模型（VLM）进行微调。该方法在标准数据集上远超现有技术。


<details>
  <summary>Details</summary>
Motivation: 目前CoVR方法未能充分利用现代视觉-语言模型（VLMs），不是采用过时架构，就是需要高昂的计算成本进行微调或生成描述，难以高效与大规模应用。

Method: 作者提出PREGEN框架，利用冻结的预训练VLM和轻量级编码器，无需VLM微调。具体做法是将查询视频和修改文本输入VLM，提取各层最后token的隐藏状态，然后用训练好的简单编码器在这些表征上进行语义融合，生成用于匹配检索的嵌入向量。

Result: PREGEN在标准CoVR基准上大幅超过现有方法，Recall@1分别提升27.23和69.59个百分点。此外，该方法对不同VLM主干鲁棒性强，在更复杂文本修改上的零样本泛化能力也很突出。

Conclusion: PREGEN方法无需对大型VLM进行微调即可实现高效、高性能的视频检索，具有较强的实际应用前景和拓展性，表现出色的语义理解和泛化能力。

Abstract: Composed Video Retrieval (CoVR) aims to retrieve a video based on a query video and a modifying text. Current CoVR methods fail to fully exploit modern Vision-Language Models (VLMs), either using outdated architectures or requiring computationally expensive fine-tuning and slow caption generation. We introduce PREGEN (PRE GENeration extraction), an efficient and powerful CoVR framework that overcomes these limitations. Our approach uniquely pairs a frozen, pre-trained VLM with a lightweight encoding model, eliminating the need for any VLM fine-tuning. We feed the query video and modifying text into the VLM and extract the hidden state of the final token from each layer. A simple encoder is then trained on these pooled representations, creating a semantically rich and compact embedding for retrieval. PREGEN significantly advances the state of the art, surpassing all prior methods on standard CoVR benchmarks with substantial gains in Recall@1 of +27.23 and +69.59. Our method demonstrates robustness across different VLM backbones and exhibits strong zero-shot generalization to more complex textual modifications, highlighting its effectiveness and semantic capabilities.

</details>


### [220] [Insight: Interpretable Semantic Hierarchies in Vision-Language Encoders](https://arxiv.org/abs/2601.13798)
*Kai Wittenmayer,Sukrut Rao,Amin Parchami-Araghi,Bernt Schiele,Jonas Fischer*

Main category: cs.CV

TL;DR: 本文提出了一个新模型 Insight，能够将视觉模型的表征分解为空间上精细的人类可解释概念，并给出高质量解释，同时分类和分割性能与原本的基础模型相当。


<details>
  <summary>Details</summary>
Motivation: 目前语言对齐的视觉基础模型在许多下游任务上表现良好，但其内部表征难以解释。现有一些工作试图用人类可解释的概念分解这些表征，但空间定位差且多局限于分类任务，解释能力有限。作者希望改进这一点，提升解释性与结果可信度。

Method: 作者提出了一种语言对齐的概念基础模型 Insight，结合分层稀疏自编码器与强语义基础模型，实现不同细粒度的人类可解释、空间定位的视觉概念自动提取，并通过局部共现关系分析定义概念间关系，用于优化命名和完善解释。

Result: 在基准数据集上，Insight 的分类和分割任务准确率与原有基础模型相当，但可同时给出细粒度、高质量的基于概念的解释。

Conclusion: Insight 实现了视觉基础模型的深层语义解释，兼具优异性能与可解释性，为理解和分析视觉模型决策提供了新工具。相关代码已开放，便于社区验证和进一步研究。

Abstract: Language-aligned vision foundation models perform strongly across diverse downstream tasks. Yet, their learned representations remain opaque, making interpreting their decision-making hard. Recent works decompose these representations into human-interpretable concepts, but provide poor spatial grounding and are limited to image classification tasks. In this work, we propose Insight, a language-aligned concept foundation model that provides fine-grained concepts, which are human-interpretable and spatially grounded in the input image. We leverage a hierarchical sparse autoencoder and a foundation model with strong semantic representations to automatically extract concepts at various granularities. Examining local co-occurrence dependencies of concepts allows us to define concept relationships. Through these relations we further improve concept naming and obtain richer explanations. On benchmark data, we show that Insight provides performance on classification and segmentation that is competitive with opaque foundation models while providing fine-grained, high quality concept-based explanations. Code is available at https://github.com/kawi19/Insight.

</details>


### [221] [Discriminant Learning-based Colorspace for Blade Segmentation](https://arxiv.org/abs/2601.13816)
*Raül Pérez-Gonzalo,Andreas Espersen,Antonio Agudo*

Main category: cs.CV

TL;DR: 本文提出了一种新型的多维非线性判别分析算法（CSDA），将颜色空间定制化用于提升分割准确率，显著优于未特别处理颜色表征的传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前图像分割中，不理想的颜色空间表征常常造成分割精度低，但许多现有算法忽略了这一关键预处理问题。为了解决这一痛点，本文致力于提出更优的颜色空间定制方法。

Method: 提出了一种多维非线性判别分析方法——Colorspace Discriminant Analysis（CSDA），通过最大化类别间分离度和最小化类别内变异性，对颜色空间进行自定义表征。方法将线性判别分析扩展到深度学习框架，并设计了三种损失函数，支持端到端优化分割流程和特征空间。

Result: 在风电叶片等领域数据集上，实验表明CSDA能够显著提升分割准确性，优于传统未定制颜色空间的方法。

Conclusion: 定制化颜色空间对特定领域的图像分割具有非常重要的作用，CSDA为领域分割中的颜色预处理提供了有效的解决方案。

Abstract: Suboptimal color representation often hinders accurate image segmentation, yet many modern algorithms neglect this critical preprocessing step. This work presents a novel multidimensional nonlinear discriminant analysis algorithm, Colorspace Discriminant Analysis (CSDA), for improved segmentation. Extending Linear Discriminant Analysis into a deep learning context, CSDA customizes color representation by maximizing multidimensional signed inter-class separability while minimizing intra-class variability through a generalized discriminative loss. To ensure stable training, we introduce three alternative losses that enable end-to-end optimization of both the discriminative colorspace and segmentation process. Experiments on wind turbine blade data demonstrate significant accuracy gains, emphasizing the importance of tailored preprocessing in domain-specific segmentation.

</details>


### [222] [FastGHA: Generalized Few-Shot 3D Gaussian Head Avatars with Real-Time Animation](https://arxiv.org/abs/2601.13837)
*Xinya Ji,Sebastian Weiss,Manuel Kansy,Jacek Naruniec,Xun Cao,Barbara Solenthaler,Derek Bradley*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D高斯表示的高质量头部虚拟人生成方法，能够少量输入图像、高效生成并支持实时动画。


<details>
  <summary>Details</summary>
Motivation: 现有方法生成高保真3D头部虚拟人通常依赖多视角捕捉或需要针对每个身份进行推理优化，导致效率低下，不易扩展到新用户，限制了实际应用。

Method: 采用端到端的前馈方法，直接从少量输入图像学习每像素的高斯表示。利用基于transformer的编码器融合了DINOv3和Stable Diffusion VAE的图像特征，提取多视角信息。为实现实时动画，拓展高斯表征为带特征高斯，并设计了基于MLP的轻量动态网络，根据表情编码预测3D高斯形变。同时，引入大规模重建模型的点云监督以提升几何平滑性。

Result: 实验结果表明，新方法在渲染质量和推理效率上均明显优于现有方法，并可支持实时动态虚拟人动画。

Conclusion: 本文方法无需繁琐捕捉设备和复杂优化，能从少量图像直接、实时、高质量地生成并驱动3D高斯头部虚拟人，推动了该领域的实用化和普及。

Abstract: Despite recent progress in 3D Gaussian-based head avatar modeling, efficiently generating high fidelity avatars remains a challenge. Current methods typically rely on extensive multi-view capture setups or monocular videos with per-identity optimization during inference, limiting their scalability and ease of use on unseen subjects. To overcome these efficiency drawbacks, we propose \OURS, a feed-forward method to generate high-quality Gaussian head avatars from only a few input images while supporting real-time animation. Our approach directly learns a per-pixel Gaussian representation from the input images, and aggregates multi-view information using a transformer-based encoder that fuses image features from both DINOv3 and Stable Diffusion VAE. For real-time animation, we extend the explicit Gaussian representations with per-Gaussian features and introduce a lightweight MLP-based dynamic network to predict 3D Gaussian deformations from expression codes. Furthermore, to enhance geometric smoothness of the 3D head, we employ point maps from a pre-trained large reconstruction model as geometry supervision. Experiments show that our approach significantly outperforms existing methods in both rendering quality and inference efficiency, while supporting real-time dynamic avatar animation.

</details>


### [223] [DisasterVQA: A Visual Question Answering Benchmark Dataset for Disaster Scenes](https://arxiv.org/abs/2601.13839)
*Aisha Al-Mohannadi,Ayisha Firoz,Yin Yang,Muhammad Imran,Ferda Ofli*

Main category: cs.CV

TL;DR: 本文提出并发布了面向灾害响应场景的视觉问答基准数据集DisasterVQA，分析了现有视觉-语言模型在不同灾害情景和任务中的表现差异，并指出当前模型仍存在细粒度推理及情境理解等挑战。


<details>
  <summary>Details</summary>
Motivation: 现有视觉问答模型多针对通用领域，对灾害应急等安全敏感场景下的复杂推理表现尚不明确。灾害响应需要高效、可靠的信息感知与决策支持，因此亟需专门的数据集和评测体系推动相关模型发展。

Method: 作者收集了1,395张真实灾害图像，结合洪水、地震、火灾等多类事件，由专家构建4,405个问题-答案对，问题类型涵盖二元、选择题及开放式，覆盖态势感知与决策支持任务。并依据FEMA ESF和OCHA MIRA等人道主义框架设计任务。对七个主流视觉-语言模型进行了全面基准测试。

Result: 实验显示，主流模型在二元类问题上表现良好，但在数量推理、目标计数、情境敏感解释等任务上失分明显，尤其在少见灾害情形下表现不稳定。不同模型在问题类别、灾害类型、地区和任务间表现差异大。

Conclusion: DisasterVQA为灾害响应领域视觉-语言模型的研究提供了切实、有挑战性的基准，为未来提升模型在实际应急任务中的鲁棒性和实用性提供数据支撑。数据集已对外公开，可促进后续研究进展。

Abstract: Social media imagery provides a low-latency source of situational information during natural and human-induced disasters, enabling rapid damage assessment and response. While Visual Question Answering (VQA) has shown strong performance in general-purpose domains, its suitability for the complex and safety-critical reasoning required in disaster response remains unclear. We introduce DisasterVQA, a benchmark dataset designed for perception and reasoning in crisis contexts. DisasterVQA consists of 1,395 real-world images and 4,405 expert-curated question-answer pairs spanning diverse events such as floods, wildfires, and earthquakes. Grounded in humanitarian frameworks including FEMA ESF and OCHA MIRA, the dataset includes binary, multiple-choice, and open-ended questions covering situational awareness and operational decision-making tasks. We benchmark seven state-of-the-art vision-language models and find performance variability across question types, disaster categories, regions, and humanitarian tasks. Although models achieve high accuracy on binary questions, they struggle with fine-grained quantitative reasoning, object counting, and context-sensitive interpretation, particularly for underrepresented disaster scenarios. DisasterVQA provides a challenging and practical benchmark to guide the development of more robust and operationally meaningful vision-language models for disaster response. The dataset is publicly available at https://zenodo.org/records/18267770.

</details>


### [224] [Probabilistic Deep Discriminant Analysis for Wind Blade Segmentation](https://arxiv.org/abs/2601.13852)
*Raül Pérez-Gonzalo,Andreas Espersen,Antonio Agudo*

Main category: cs.CV

TL;DR: 提出了一种深度判别分析（DDA）方法，通过深度网络直接优化Fisher判别准则，解决了现有线性判别分析对非线性可分数据效果不佳的问题，并在风电叶片分割任务中取得了显著进展。


<details>
  <summary>Details</summary>
Motivation: 传统的线性判别分析（LDA）无法很好处理非线性可分的复杂数据集，且在实际分割任务（如风电叶片分割）对高判别性的需求越来越高，因此需要改进方法提升类别分离与模型稳定性。

Method: 设计了深度判别分析（DDA）方法，采用深度网络优化Fisher判别准则，引入符号化类间方差、sigmoid函数约束输出、将乘法转为加法以提升训练稳定性，并设计了两种稳定的损失函数。进一步结合概率损失，提出了概率判别分析（PDDA），通过极大减小输出分布的类间重叠区域，促使模型做出置信度更高的预测。

Result: PDDA在风电叶片分割任务中显著提升了分割性能和输出一致性，有效减小了类内方差，是首次将DDA方法应用于图像分割领域。

Conclusion: 所提的PDDA方法提升了类别判别能力和分割表现，在风电叶片分割等实际应用中具有重要价值，并推动了DDA在图像分割领域的研究进展。

Abstract: Linear discriminant analysis improves class separability but struggles with non-linearly separable data. To overcome this, we introduce Deep Discriminant Analysis (DDA), which directly optimizes the Fisher criterion utilizing deep networks. To ensure stable training and avoid computational instabilities, we incorporate signed between-class variance, bound outputs with a sigmoid function, and convert multiplicative relationships into additive ones. We present two stable DDA loss functions and augment them with a probability loss, resulting in Probabilistic DDA (PDDA). PDDA effectively minimizes class overlap in output distributions, producing highly confident predictions with reduced within-class variance. When applied to wind blade segmentation, PDDA showcases notable advances in performance and consistency, critical for wind energy maintenance. To our knowledge, this is the first application of DDA to image segmentation.

</details>


### [225] [OCCAM: Class-Agnostic, Training-Free, Prior-Free and Multi-Class Object Counting](https://arxiv.org/abs/2601.13871)
*Michail Spanakis,Iason Oikonomidis,Antonis Argyros*

Main category: cs.CV

TL;DR: 本文提出了OCCAM方法，实现了无需训练和辅助信息的无类别泛化目标计数（CAC），能在一张图片中分别计数任意类别的所有目标，在标准数据集上取得了具有竞争力的表现。


<details>
  <summary>Details</summary>
Motivation: 现有CAC方法通常只针对单类目标、依赖大规模深度模型训练，并且需要额外信息（如示例图片或文本提示），这限制了其实用性。因此，作者希望提出一个不依赖训练和补充信息的方法，并解决多类目标同时计数的问题。

Method: 作者提出了OCCAM方法，结合了基础模型Segment Anything Model 2（SAM2）和经过自定义阈值的First Integer Neighbor Clustering Hierarchy（FINCH）聚类算法。该方法完全无训练和辅助信息即可对每一类别的目标分别计数。并引入合成多类数据集与F1分数作为新评估标准。

Result: OCCAM方法在FSC-147和CARPK等主流数据集上取得了具有竞争力的结果，展示了其有效性，并为多类别计数任务提供了新数据集和评价指标。

Conclusion: OCCAM是首个完全无训练、无须补充信息、可实现多类别目标计数的CAC方法，证明了基础模型与聚类结合在泛化计数任务上的潜力。相关代码和数据集将开源。

Abstract: Class-Agnostic object Counting (CAC) involves counting instances of objects from arbitrary classes within an image. Due to its practical importance, CAC has received increasing attention in recent years. Most existing methods assume a single object class per image, rely on extensive training of large deep learning models and address the problem by incorporating additional information, such as visual exemplars or text prompts. In this paper, we present OCCAM, the first training-free approach to CAC that operates without the need of any supplementary information. Moreover, our approach addresses the multi-class variant of the problem, as it is capable of counting the object instances in each and every class among arbitrary object classes within an image. We leverage Segment Anything Model 2 (SAM2), a foundation model, and a custom threshold-based variant of the First Integer Neighbor Clustering Hierarchy (FINCH) algorithm to achieve competitive performance on widely used benchmark datasets, FSC-147 and CARPK. We propose a synthetic multi-class dataset and F1 score as a more suitable evaluation metric. The code for our method and the proposed synthetic dataset will be made publicly available at https://mikespanak.github.io/OCCAM_counter.

</details>


### [226] [Revisiting Multi-Task Visual Representation Learning](https://arxiv.org/abs/2601.13886)
*Shangzhe Di,Zhonghua Zhai,Weidi Xie*

Main category: cs.CV

TL;DR: 本文提出了一种多任务视觉预训练框架MTV，通过结合视觉-语言对比、自监督和密集空间任务，联合优化视觉表征。实验表明，该方法兼顾全局语义和精细空间信息。


<details>
  <summary>Details</summary>
Motivation: 现有视觉表示学习方法存在分化：视觉-语言模型（如CLIP）擅长全局语义对齐但空间精度不足，自监督方法（如MAE、DINO）能捕获细粒度局部结构但缺乏高层语义。两者互补，亟需一种能够统一这两类优势的有效方法。

Method: 提出MTV多任务视觉预训练框架，联合优化视觉-语言对比、自监督任务与密集空间监督任务，使用高性能专家模型（如Depth Anything V2, OWLv2）大规模生成伪标签以降低人工标注需求，并系统性分析各任务目标的增益、协同/干扰及其在不同数据和模型规模下的表现。

Result: MTV实现了全局语义理解和精细空间推理的“兼得”，显著提升了空间推理能力，同时不损失语义理解能力。对多任务视觉学习的机制进行了系统评估。

Conclusion: 基于高质量伪监督的多任务学习，是实现更通用视觉编码器的可扩展路径。MTV框架结合了多种视觉任务的优势，推动了视觉表征学习方法的发展。

Abstract: Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity "expert" models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves "best-of-both-worlds" performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.

</details>


### [227] [OmniOVCD: Streamlining Open-Vocabulary Change Detection with SAM 3](https://arxiv.org/abs/2601.13895)
*Xu Zhang,Danyang Li,Yingjie Xia,Xiaohang Dong,Hualong Yu,Jianye Wang,Qicheng Li*

Main category: cs.CV

TL;DR: 该论文提出了一种基于SAM 3模型的开放词汇变化检测（OVCD）方法OmniOVCD，整合分割和识别能力，无需结合多个模型，解决了特征匹配和系统不稳定性问题，并在四个公开数据集上取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有的变化检测技术依赖预定义类别，难以应对开放词汇需求，且常需融合多个模型（如CLIP和DINO），造成特征匹配困难和系统不稳定。因此，亟需一种独立、高效、鲁棒的开放词汇变化检测方法。

Method: 利用SAM 3模型的解耦输出头，提出“Synergistic Fusion to Instance Decoupling（SFID）”策略，先融合语义、实例和存在性输出以构建地表掩码，再分解为单独实例用于两时相变化比较。

Result: 在四个公开遥感变化检测基准（LEVIR-CD、WHU-CD、S2Looking和SECOND）上，OmniOVCD取得了各自的SOTA表现，平均IoU分别为67.2、66.5、24.5和27.1，超过所有现有方法。

Conclusion: OmniOVCD无需依赖外部模型，通过利用SAM 3的多输出融合和实例解耦，实现了高准确度的开放词汇变化检测，展现了方法的先进性和应用潜力。

Abstract: Change Detection (CD) is a fundamental task in remote sensing. It monitors the evolution of land cover over time. Based on this, Open-Vocabulary Change Detection (OVCD) introduces a new requirement. It aims to reduce the reliance on predefined categories. Existing training-free OVCD methods mostly use CLIP to identify categories. These methods also need extra models like DINO to extract features. However, combining different models often causes problems in matching features and makes the system unstable. Recently, the Segment Anything Model 3 (SAM 3) is introduced. It integrates segmentation and identification capabilities within one promptable model, which offers new possibilities for the OVCD task. In this paper, we propose OmniOVCD, a standalone framework designed for OVCD. By leveraging the decoupled output heads of SAM 3, we propose a Synergistic Fusion to Instance Decoupling (SFID) strategy. SFID first fuses the semantic, instance, and presence outputs of SAM 3 to construct land-cover masks, and then decomposes them into individual instance masks for change comparison. This design preserves high accuracy in category recognition and maintains instance-level consistency across images. As a result, the model can generate accurate change masks. Experiments on four public benchmarks (LEVIR-CD, WHU-CD, S2Looking, and SECOND) demonstrate SOTA performance, achieving IoU scores of 67.2, 66.5, 24.5, and 27.1 (class-average), respectively, surpassing all previous methods.

</details>


### [228] [Towards Visually Explaining Statistical Tests with Applications in Biomedical Imaging](https://arxiv.org/abs/2601.13899)
*Masoumeh Javanbakhat,Piotr Komorowski,Dilyara Bareeva,Wei-Chang Lai,Wojciech Samek,Christoph Lippert*

Main category: cs.CV

TL;DR: 该论文提出了一种具有可解释性的深度统计检验框架，能在无需标签的情况下对组间分布差异进行检测，并给出样本级和特征级的解释，尤其适用于医学影像分析。


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络的双样本检验在检测组间分布差异上表现优越，但其“黑箱”特性限制了解释性，尤其在生物医学领域更加棘手。同时，现有的可解释方法大多依赖类别标签，不适用于无标签统计检验，亟需新的无标签可解释方法。

Method: 论文提出了一种增强版的深度双样本检验方法，在原有检验基础上引入了可解释性机制，能解释哪些具体样本、哪些输入特征（如图像区域）最驱动统计显著组间差异。该方法输出空间（图像区块）和实例（具体样本）层面的解释结果。

Result: 在医学影像数据上的实验表明，该框架能够有效识别影响最大的重要样本，并突出显示与疾病相关的重要解剖区域。

Conclusion: 该方法很好地结合了统计推断与可解释人工智能，实现了在医学影像分析中无标签、可解释的群体比较，提升了深度统计检验的实用性和可接受性。

Abstract: Deep neural two-sample tests have recently shown strong power for detecting distributional differences between groups, yet their black-box nature limits interpretability and practical adoption in biomedical analysis. Moreover, most existing post-hoc explainability methods rely on class labels, making them unsuitable for label-free statistical testing settings. We propose an explainable deep statistical testing framework that augments deep two-sample tests with sample-level and feature-level explanations, revealing which individual samples and which input features drive statistically significant group differences. Our method highlights which image regions and which individual samples contribute most to the detected group difference, providing spatial and instance-wise insight into the test's decision. Applied to biomedical imaging data, the proposed framework identifies influential samples and highlights anatomically meaningful regions associated with disease-related variation. This work bridges statistical inference and explainable AI, enabling interpretable, label-free population analysis in medical imaging.

</details>


### [229] [On the Role of Rotation Equivariance in Monocular 3D Human Pose Estimation](https://arxiv.org/abs/2601.13913)
*Pavlo Melnyk,Cuong Le,Urs Waldmann,Per-Erik Forssén,Bastian Wandt*

Main category: cs.CV

TL;DR: 本文在单视图3D人体姿态估计任务中，发现传统模型在输入空间旋转时效果较差，提出通过学习2D旋转等变性更简单有效，并且通过数据增强显著提升了模型性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单张图像估计3D人体关节点是计算机视觉的重要任务，但方法常常对输入旋转敏感。作者注意到，现有模型很难直接学习点对点映射，且实用时精度受限。如何使模型更好适应旋转情况，是提升鲁棒性的关键。

Method: 作者将关注点放在2D到3D姿态估计算法的2D旋转等变性，对比『直接建模等变性』与『数据增强自动学习等变性』两大思路。具体方法是通过数据增强扩展图像的2D旋转，使模型在训练时学会应对各种平面旋转，而不依赖于特殊设计的等变架构。

Result: 实验证明，通过简单的数据增强（旋转图像）能让模型在常用HPE基准上得到更好结果，特别是在需要处理输入旋转时，性能超过了依赖专门等变设计的先进方法。

Conclusion: 学习2D旋转等变性对3D姿态估计模型有显著促进，用数据增强手段可以直接有效地提升模型鲁棒性，无需复杂等变结构，具有实际应用价值。

Abstract: Estimating 3D from 2D is one of the central tasks in computer vision. In this work, we consider the monocular setting, i.e. single-view input, for 3D human pose estimation (HPE). Here, the task is to predict a 3D point set of human skeletal joints from a single 2D input image. While by definition this is an ill-posed problem, recent work has presented methods that solve it with up to several-centimetre error. Typically, these methods employ a two-step approach, where the first step is to detect the 2D skeletal joints in the input image, followed by the step of 2D-to-3D lifting. We find that common lifting models fail when encountering a rotated input. We argue that learning a single human pose along with its in-plane rotations is considerably easier and more geometrically grounded than directly learning a point-to-point mapping. Furthermore, our intuition is that endowing the model with the notion of rotation equivariance without explicitly constraining its parameter space should lead to a more straightforward learning process than one with equivariance by design. Utilising the common HPE benchmarks, we confirm that the 2D rotation equivariance per se improves the model performance on human poses akin to rotations in the image plane, and can be efficiently and straightforwardly learned by augmentation, outperforming state-of-the-art equivariant-by-design methods.

</details>


### [230] [TrackletGPT: A Language-like GPT Framework for White Matter Tract Segmentation](https://arxiv.org/abs/2601.13935)
*Anoushkrit Goel,Simroop Singh,Ankita Joshi,Ranjeet Ranjan Jha,Chirag Ahuja,Aditya Nigam,Arnav Bhavsar*

Main category: cs.CV

TL;DR: 该论文提出了TrackletGPT，一种基于类语言模型GPT的白质束分割新方法，能自动、精细地进行脑部神经束追踪，较现有方法在多项指标上表现更优。


<details>
  <summary>Details</summary>
Motivation: 白质束分割对研究脑结构连接、神经疾病及神经外科至关重要，但由于不同受试者和条件下的神经束差异大，且分割任务复杂，亟需更通用且自动化的方法。

Method: 提出TrackletGPT框架，将神经束分割建模为类似语言序列任务，将轨迹片段（Tracklets）作为“token”输入到GPT结构中，从而捕获更精细的结构信息，同时具备良好的跨数据集泛化能力，实现全自动分割。

Result: 在TractoInferno和HCP数据集上的实验显示，TrackletGPT在平均DICE、Overlap和Overreach指标上都优于主流分割方法，并在跨数据集实验中也有较好表现。

Conclusion: TrackletGPT能够实现快速、准确且泛化性强的白质束分割，为神经影像分析和相关应用提供了新的工具。

Abstract: White Matter Tract Segmentation is imperative for studying brain structural connectivity, neurological disorders and neurosurgery. This task remains complex, as tracts differ among themselves, across subjects and conditions, yet have similar 3D structure across hemispheres and subjects. To address these challenges, we propose TrackletGPT, a language-like GPT framework which reintroduces sequential information in tokens using tracklets. TrackletGPT generalises seamlessly across datasets, is fully automatic, and encodes granular sub-streamline segments, Tracklets, scaling and refining GPT models in Tractography Segmentation. Based on our experiments, TrackletGPT outperforms state-of-the-art methods on average DICE, Overlap and Overreach scores on TractoInferno and HCP datasets, even on inter-dataset experiments.

</details>


### [231] [Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning](https://arxiv.org/abs/2601.13942)
*Hongbo Bai,Yujin Zhou,Yile Wu,Chi-Min Chan,Pengcheng Wen,Kunhao Pan,Sirui Han,Yike Guo*

Main category: cs.CV

TL;DR: 本文提出了Glance-or-Gaze (GoG) 框架，通过主动视觉规划和选择性关注机制，显著提升大型多模态模型在复杂视觉检索任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大型多模态模型在需要丰富知识的信息密集型视觉问答中表现有限，主要受制于静态参数知识难以应对稀有实体与不断变化的信息。检索增强方法虽然有所改进，但现有方案往往无差别检索全图，噪声和冗余高，且缺乏深度迭代反思，难以处理复杂查询。

Method: 提出GoG主动视觉框架，通过Selective Gaze机制，模型可在“全图浏览”与“区域聚焦”间动态切换，仅检索高价值区域，显著减少无关信息。方法采用双阶段训练：1) 通过有监督学习进行行为对齐，构建GoG基础范式，2) 利用复杂度自适应强化学习，提升模型在复杂任务下的推理和适应能力。

Result: 在六个权威基准测试上实现了最新的性能突破。消融实验表明Selective Gaze机制和复杂度自适应强化学习对提升视觉检索能力均不可或缺。

Conclusion: GoG框架通过主动视觉规划和选择性信息检索，有效克服了现有方法的冗余和噪声问题，极大提升了复杂视觉问答能力。作者承诺将开源相关数据和模型，促进后续研究。

Abstract: Large Multimodal Models (LMMs) have achieved remarkable success in visual understanding, yet they struggle with knowledge-intensive queries involving long-tail entities or evolving information due to static parametric knowledge. Recent search-augmented approaches attempt to address this limitation, but existing methods rely on indiscriminate whole-image retrieval that introduces substantial visual redundancy and noise, and lack deep iterative reflection, limiting their effectiveness on complex visual queries. To overcome these challenges, we propose Glance-or-Gaze (GoG), a fully autonomous framework that shifts from passive perception to active visual planning. GoG introduces a Selective Gaze mechanism that dynamically chooses whether to glance at global context or gaze into high-value regions, filtering irrelevant information before retrieval. We design a dual-stage training strategy: Reflective GoG Behavior Alignment via supervised fine-tuning instills the fundamental GoG paradigm, while Complexity-Adaptive Reinforcement Learning further enhances the model's capability to handle complex queries through iterative reasoning. Experiments across six benchmarks demonstrate state-of-the-art performance. Ablation studies confirm that both Selective Gaze and complexity-adaptive RL are essential for effective visual search. We will release our data and models for further exploration soon.

</details>


### [232] [VTONGuard: Automatic Detection and Authentication of AI-Generated Virtual Try-On Content](https://arxiv.org/abs/2601.13951)
*Shengyi Wu,Yan Hong,Shengyao Chen,Zheng Wang,Xianbing Sun,Jiahui Zhan,Jun Lan,Jianfu Zhang*

Main category: cs.CV

TL;DR: 本论文提出了VTONGuard，这是一个包含超过77.5万张真实与合成虚拟试穿图像的大型基准数据集，并在此基础上系统性评测了多种检测范式，最终提出了多任务检测框架提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的发展，虚拟试穿（VTON）系统在电商和数字娱乐中日益普及，但高度真实的合成图像带来了真实性和安全性等新问题。这需要构建大规模公开数据集和有效检测手段以保障VTON内容的可辨别性和安全应用。

Method: 作者建立了VTONGuard数据集，涵盖多样化姿态、背景与服饰风格的真实与合成试穿图像。基于该数据集，采用统一的训练测试协议，系统评测多种检测范式，并提出结合辅助分割的多任务检测框架以增强边界特征学习。

Result: 实验揭示了不同检测方法的优缺点，尤其是跨范式泛化能力差的挑战。提出的多任务检测框架在VTONGuard数据集上取得了最佳整体性能。

Conclusion: VTONGuard为VTON内容真实性检测提供了公平标准与丰富资源，有助于推动更健壮检测模型的发展，促进虚拟试穿技术的安全与负责任应用。

Abstract: With the rapid advancement of generative AI, virtual try-on (VTON) systems are becoming increasingly common in e-commerce and digital entertainment. However, the growing realism of AI-generated try-on content raises pressing concerns about authenticity and responsible use. To address this, we present VTONGuard, a large-scale benchmark dataset containing over 775,000 real and synthetic try-on images. The dataset covers diverse real-world conditions, including variations in pose, background, and garment styles, and provides both authentic and manipulated examples. Based on this benchmark, we conduct a systematic evaluation of multiple detection paradigms under unified training and testing protocols. Our results reveal each method's strengths and weaknesses and highlight the persistent challenge of cross-paradigm generalization. To further advance detection, we design a multi-task framework that integrates auxiliary segmentation to enhance boundary-aware feature learning, achieving the best overall performance on VTONGuard. We expect this benchmark to enable fair comparisons, facilitate the development of more robust detection models, and promote the safe and responsible deployment of VTON technologies in practice.

</details>


### [233] [DExTeR: Weakly Semi-Supervised Object Detection with Class and Instance Experts for Medical Imaging](https://arxiv.org/abs/2601.13954)
*Adrien Meyer,Didier Mutter,Nicolas Padoy*

Main category: cs.CV

TL;DR: 本文提出了DExTeR，一种专为医学影像点注释设计的自注意力检测器，在降低标注成本的同时保持高检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有物体检测方法依赖于费时费力的边界框标注，这在医学影像领域限制了规模化应用；点注释提供了更经济的替代方案，但如何利用点注释高效训练检测器仍是挑战。

Method: 提出基于transformer的Point-to-Box回归模型DExTeR：1）使用单点注释作为object query输入；2）提出类引导可变形注意力机制，提高对特定类别特征的捕捉；3）引入CLICK-MoE结构，将类别与实例表示解耦，以减少重叠或相邻实例的混淆；4）多点训练策略增强预测一致性，提高对不同点标注位置的鲁棒性。

Result: DExTeR在内镜、胸片和超声内镜三个医学影像数据集上均实现了当前最优的检测效果，兼具检测精度和标注经济性。

Conclusion: 通过创新方法充分利用点注释信息，DExTeR大幅降低了医学影像检测的标注成本，并保持高准确性，显示出在医学领域广泛应用的潜力。

Abstract: Detecting anatomical landmarks in medical imaging is essential for diagnosis and intervention guidance. However, object detection models rely on costly bounding box annotations, limiting scalability. Weakly Semi-Supervised Object Detection (WSSOD) with point annotations proposes annotating each instance with a single point, minimizing annotation time while preserving localization signals. A Point-to-Box teacher model, trained on a small box-labeled subset, converts these point annotations into pseudo-box labels to train a student detector. Yet, medical imagery presents unique challenges, including overlapping anatomy, variable object sizes, and elusive structures, which hinder accurate bounding box inference. To overcome these challenges, we introduce DExTeR (DETR with Experts), a transformer-based Point-to-Box regressor tailored for medical imaging. Built upon Point-DETR, DExTeR encodes single-point annotations as object queries, refining feature extraction with the proposed class-guided deformable attention, which guides attention sampling using point coordinates and class labels to capture class-specific characteristics. To improve discrimination in complex structures, it introduces CLICK-MoE (CLass, Instance, and Common Knowledge Mixture of Experts), decoupling class and instance representations to reduce confusion among adjacent or overlapping instances. Finally, we implement a multi-point training strategy which promotes prediction consistency across different point placements, improving robustness to annotation variability. DExTeR achieves state-of-the-art performance across three datasets spanning different medical domains (endoscopy, chest X-rays, and endoscopic ultrasound) highlighting its potential to reduce annotation costs while maintaining high detection accuracy.

</details>


### [234] [STEC: A Reference-Free Spatio-Temporal Entropy Coverage Metric for Evaluating Sampled Video Frames](https://arxiv.org/abs/2601.13974)
*Shih-Yao Lin*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频帧采样质量评价指标：空间-时间熵覆盖（STEC），能无参考地衡量采样帧的代表性与信息覆盖，优于仅关注感知质量或重建保真度的传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视频帧采样评价方法主要关注图像质量或重建，但未能有效衡量所采样帧对于视频关键内容的覆盖和代表性，限制了视频理解任务中帧采样方法的优化空间。

Method: STEC指标结合帧级空间信息熵（结构复杂度）、帧的时间分布与冗余度，通过量化帧集合的空间信息强度、时间分散性及非冗余性，给出了一个轻量级、无参考的帧采样质量评价方法。

Result: 实验在MSR-VTT benchmark上表明，STEC能够明显区分不同采样策略（如随机、均匀、内容感知等），还揭示了视频个体对采样策略的鲁棒性差异，超越了简单的平均性能评价。

Conclusion: STEC为视频帧采样质量评价提供了通用、任务无关的分析工具，能够更好诊断有限预算下的帧采样策略表现。它不直接预测下游任务准确率，但能补充现有评价体系的重要维度。

Abstract: Frame sampling is a fundamental component in video understanding and video--language model pipelines, yet evaluating the quality of sampled frames remains challenging. Existing evaluation metrics primarily focus on perceptual quality or reconstruction fidelity, and are not designed to assess whether a set of sampled frames adequately captures informative and representative video content.
  We propose Spatio-Temporal Entropy Coverage (STEC), a simple and non-reference metric for evaluating the effectiveness of video frame sampling. STEC builds upon Spatio-Temporal Frame Entropy (STFE), which measures per-frame spatial information via entropy-based structural complexity, and evaluates sampled frames based on their temporal coverage and redundancy. By jointly modeling spatial information strength, temporal dispersion, and non-redundancy, STEC provides a principled and lightweight measure of sampling quality.
  Experiments on the MSR-VTT test-1k benchmark demonstrate that STEC clearly differentiates common sampling strategies, including random, uniform, and content-aware methods. We further show that STEC reveals robustness patterns across individual videos that are not captured by average performance alone, highlighting its practical value as a general-purpose evaluation tool for efficient video understanding.
  We emphasize that STEC is not designed to predict downstream task accuracy, but to provide a task-agnostic diagnostic signal for analyzing frame sampling behavior under constrained budgets.

</details>


### [235] [Harmonizing the Deep: A Unified Information Pipeline for Robust Marine Biodiversity Assessment Across Heterogeneous Domains](https://arxiv.org/abs/2601.13975)
*Marco Piccolo,Qiwei Han,Astrid van Toor,Joachim Vanneste*

Main category: cs.CV

TL;DR: 本文提出了一种统一信息处理管道，用于提升水下生物多样性监测系统在不同环境中的迁移适应性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前水下生物多样性监测系统在实际跨地域部署时，检测性能往往下降明显，无法在新站点实现可靠监测，影响保护及入侵物种管理工作的可扩展性。

Method: 构建了统一信息处理管道，将多源异构水下数据标准化为可比对信息流，并使用受控跨域协议下的固定检测器进行评估。系统分析了多种结构性因素（如场景构成、物体密度和上下文冗余）对跨域性能损失的影响。此外，考察了在低成本边缘硬件上的推理速度和实际应用可行性。

Result: 实验发现，结构性因素对跨域性能损失的影响大于单纯的视觉劣化（如浑浊度），稀疏场景下容易出现特有的“上下文失效”故障模式。硬件测试显示，经过优化后系统可以在远程低成本设备上实现实用采样速率。

Conclusion: 研究强调了结构感知下的检测可靠性重于传统的图像增强方法，为海洋生态监测提供了公平、稳定的技术工具，有助于跨区域长期监测实施。

Abstract: Marine biodiversity monitoring requires scalability and reliability across complex underwater environments to support conservation and invasive-species management. Yet existing detection solutions often exhibit a pronounced deployment gap, with performance degrading sharply when transferred to new sites. This work establishes the foundational detection layer for a multi-year invasive species monitoring initiative targeting Arctic and Atlantic marine ecosystems. We address this challenge by developing a Unified Information Pipeline that standardises heterogeneous datasets into a comparable information flow and evaluates a fixed, deployment-relevant detector under controlled cross-domain protocols. Across multiple domains, we find that structural factors, such as scene composition, object density, and contextual redundancy, explain cross-domain performance loss more strongly than visual degradation such as turbidity, with sparse scenes inducing a characteristic "Context Collapse" failure mode. We further validate operational feasibility by benchmarking inference on low-cost edge hardware, showing that runtime optimisation enables practical sampling rates for remote monitoring. The results shift emphasis from image enhancement toward structure-aware reliability, providing a democratised tool for consistent marine ecosystem assessment.

</details>


### [236] [Equivariant Learning for Unsupervised Image Dehazing](https://arxiv.org/abs/2601.13986)
*Zhang Wen,Jiangwei Xie,Dongdong Chen*

Main category: cs.CV

TL;DR: 本文提出了一种无监督的图像去雾框架EID，利用图像本身的对称性，在无需洁净图像作为真值的情况下，实现科学图像的高效去雾。实验证明其效果超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像去雾方法依赖人工设计的先验或大量无雾真值数据，获取成本高，尤其在科学成像（如显微和内窥镜图像）场景下更加困难。因此需要一种不依赖真值的新方法。

Method: 作者提出EID框架，通过强制图像之间的雾霾一致性和系统对称性来从有雾原始图像中恢复清晰结构。此外，还引入了对抗学习策略来模拟未知的雾物理过程，助力模型学习。

Result: 在细胞显微、医疗内窥镜等科学图像以及自然图像的去雾实验中，EID方法均明显优于当前最先进的去雾算法。

Conclusion: EID通过结合等变学习与雾物理建模，不仅提升了科学图像的去雾性能，还提升了方法的适用性和效果，有望推广到更广泛的应用。

Abstract: Image Dehazing (ID) aims to produce a clear image from an observation contaminated by haze. Current ID methods typically rely on carefully crafted priors or extensive haze-free ground truth, both of which are expensive or impractical to acquire, particularly in the context of scientific imaging. We propose a new unsupervised learning framework called Equivariant Image Dehazing (EID) that exploits the symmetry of image signals to restore clarity to hazy observations. By enforcing haze consistency and systematic equivariance, EID can recover clear patterns directly from raw, hazy images. Additionally, we propose an adversarial learning strategy to model unknown haze physics and facilitate EID learning. Experiments on two scientific image dehazing benchmarks (including cell microscopy and medical endoscopy) and on natural image dehazing have demonstrated that EID significantly outperforms state-of-the-art approaches. By unifying equivariant learning with modelling haze physics, we hope that EID will enable more versatile and effective haze removal in scientific imaging. Code and datasets will be published.

</details>


### [237] [Likelihood-Separable Diffusion Inference for Multi-Image MRI Super-Resolution](https://arxiv.org/abs/2601.14030)
*Samuel W. Remedios,Zhangxing Bian,Shuwen Wei,Aaron Carass,Jerry L. Prince,Blake E. Dewey*

Main category: cs.CV

TL;DR: 本文提出了一种将扩散模型用于磁共振成像（MRI）多图像超分辨（MISR）的方法，并取得了突破性的超分辨率效果。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型主要解决单幅图像的逆问题，但在MRI等实际场景中，通常会获得多个低分辨率且相互补充的切片。如何综合这些切片达到高分辨率重建，是一个具有挑战性的课题，现有方法对此关注较少。

Method: 作者将当前扩散模型常见的单图像逆问题解决方法，推广至多图像超分辨场景。提出利用DPS likelihood correction实现各低分辨率测量梯度的完全可分解，从而无需构造联合算子、修改扩散模型或增加网络推理次数。此外，还推导了DPS、DMAP、DPPS和基于扩散的PnP/ADMM的多图像版本。

Result: 在$4\times/8\times/16\times$各向异性退化场景下，所提多图像方法在超分辨MRI体数据上明显优于单图像方法，实现了最新的超分辨率效果。

Conclusion: 该研究实现了各向异性MRI体积的最先进超分辨，尤其允许利用常规的2D多切片采集数据，重建出近似各向同性的解剖结构，极大提升了图像质量和临床可用性。

Abstract: Diffusion models are the current state-of-the-art for solving inverse problems in imaging. Their impressive generative capability allows them to approximate sampling from a prior distribution, which alongside a known likelihood function permits posterior sampling without retraining the model. While recent methods have made strides in advancing the accuracy of posterior sampling, the majority focuses on single-image inverse problems. However, for modalities such as magnetic resonance imaging (MRI), it is common to acquire multiple complementary measurements, each low-resolution along a different axis. In this work, we generalize common diffusion-based inverse single-image problem solvers for multi-image super-resolution (MISR) MRI. We show that the DPS likelihood correction allows an exactly-separable gradient decomposition across independently acquired measurements, enabling MISR without constructing a joint operator, modifying the diffusion model, or increasing network function evaluations. We derive MISR versions of DPS, DMAP, DPPS, and diffusion-based PnP/ADMM, and demonstrate substantial gains over SISR across $4\times/8\times/16\times$ anisotropic degradations. Our results achieve state-of-the-art super-resolution of anisotropic MRI volumes and, critically, enable reconstruction of near-isotropic anatomy from routine 2D multi-slice acquisitions, which are otherwise highly degraded in orthogonal views.

</details>


### [238] [Human detectors are surprisingly powerful reward models](https://arxiv.org/abs/2601.14037)
*Kumar Ashutosh,XuDong Wang,Xi Yin,Kristen Grauman,Adam Polyak,Ishan Misra,Rohit Girdhar*

Main category: cs.CV

TL;DR: 本文提出HuDA奖励模型，通过简单但有效的方式显著改进了视频生成中特别是复杂人体动作的质量，明显超越了当前最先进方法。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在视觉保真度和时间连贯性上取得进展，但在复杂的人体非刚性动作（如体育、舞蹈等动态行为）合成方面表现不佳，经常出现肢体缺失、姿态扭曲等问题。作者希望解决现有方法在合成高质量人体动作视频时的不足。

Method: 提出了HuDA奖励模型，该模型结合人体检测置信度评估外观质量，并通过时序提示对齐分数捕捉动作的真实性。HuDA无需额外训练，直接利用现成模型。将HuDA作为奖励信号用于现有视频生成模型的后训练（Group Reward Policy Optimization, GRPO）阶段，以优化生成效果。

Result: HuDA奖励模型在无需手工标注数据和专项微调的情况下，在视频中对复杂人体动作的生成效果上超过了目前最先进的Wan 2.1等模型，win-rate高达73%。

Conclusion: HuDA非常简单但有效，不仅能显著提升人体视频生成质量，也能泛化到动物视频和人与物体交互视频的生成，具备较强的实用意义和创新性。

Abstract: Video generation models have recently achieved impressive visual fidelity and temporal coherence. Yet, they continue to struggle with complex, non-rigid motions, especially when synthesizing humans performing dynamic actions such as sports, dance, etc. Generated videos often exhibit missing or extra limbs, distorted poses, or physically implausible actions. In this work, we propose a remarkably simple reward model, HuDA, to quantify and improve the human motion in generated videos. HuDA integrates human detection confidence for appearance quality, and a temporal prompt alignment score to capture motion realism. We show this simple reward function that leverages off-the-shelf models without any additional training, outperforms specialized models finetuned with manually annotated data. Using HuDA for Group Reward Policy Optimization (GRPO) post-training of video models, we significantly enhance video generation, especially when generating complex human motions, outperforming state-of-the-art models like Wan 2.1, with win-rate of 73%. Finally, we demonstrate that HuDA improves generation quality beyond just humans, for instance, significantly improving generation of animal videos and human-object interactions.

</details>


### [239] [Correcting and Quantifying Systematic Errors in 3D Box Annotations for Autonomous Driving](https://arxiv.org/abs/2601.14038)
*Alexandre Justo Miro,Ludvig af Klinteberg,Bogdan Timus,Aron Asefaw,Ajinkya Khoche,Thomas Gustafsson,Sina Sharif Mansouri,Masoud Daneshtalab*

Main category: cs.CV

TL;DR: 本文关注于自动驾驶数据集中的3D目标框标注误差，提出了一种新的离线修正方法，大幅提升了标注的准确性。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶系统中，真实且精确的标注对于监督学习和性能评估极为关键。许多数据集基于激光雷达等主动传感器采集数据，然而在动态场景下，不同时间戳下的目标会产生明显偏移，现有标注往往未考虑这种时空差异，导致系统性标注错误。作者发现了公开数据集中的此类错误，动因在于提升数据标注质量从而更准确地反映车辆系统性能。

Method: 作者提出了一种新的离线校正方法，通过物理可行轨迹和与传感器数据的一致性，将3D物体框的空间和时间误差进行修正。同时首次为该问题设计了专门的性能指标，并在多个公开及私有数据集上进行评估。

Result: 该方法在Argoverse 2、MAN TruckScenes和作者自有数据集上使3D框标注质量提升超过17%。量化分析表明，原始标注最大误差高达2.5米，特别是对于高速动态目标影响更严重。

Conclusion: 标注误差的校正对性能评估具有重要影响，误差带来的影响甚至超过深度学习方法近年的性能提升。只有高精度标注才能确保方法评价的可靠性。相关代码已开源，有助于领域提升数据集标注标准。

Abstract: Accurate ground truth annotations are critical to supervised learning and evaluating the performance of autonomous vehicle systems. These vehicles are typically equipped with active sensors, such as LiDAR, which scan the environment in predefined patterns. 3D box annotation based on data from such sensors is challenging in dynamic scenarios, where objects are observed at different timestamps, hence different positions. Without proper handling of this phenomenon, systematic errors are prone to being introduced in the box annotations. Our work is the first to discover such annotation errors in widely used, publicly available datasets. Through our novel offline estimation method, we correct the annotations so that they follow physically feasible trajectories and achieve spatial and temporal consistency with the sensor data. For the first time, we define metrics for this problem; and we evaluate our method on the Argoverse 2, MAN TruckScenes, and our proprietary datasets. Our approach increases the quality of box annotations by more than 17% in these datasets. Furthermore, we quantify the annotation errors in them and find that the original annotations are misplaced by up to 2.5 m, with highly dynamic objects being the most affected. Finally, we test the impact of the errors in benchmarking and find that the impact is larger than the improvements that state-of-the-art methods typically achieve with respect to the previous state-of-the-art methods; showing that accurate annotations are essential for correct interpretation of performance. Our code is available at https://github.com/alexandre-justo-miro/annotation-correction-3D-boxes.

</details>


### [240] [Generalizing Abstention for Noise-Robust Learning in Medical Image Segmentation](https://arxiv.org/abs/2601.14039)
*Wesam Moustafa,Hossam Elsafty,Helen Schneider,Lorenz Sparrenberg,Rafet Sifa*

Main category: cs.CV

TL;DR: 本文提出了一种通用的弃权机制框架，用于提升医学图像分割模型在带有噪声标签数据下的鲁棒性，并在多个基线损失函数上显示了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割常因人工标注困难而引入标签噪声，导致模型过拟合，泛化能力下降。已有方法对噪声敏感分割研究不足，且分割任务中弃权机制的潜力尚未验证。因此，需要开发新方法来提升分割模型在噪声标签下的表现。

Method: 作者提出一种模块化的弃权机制框架，包括一个引导弃权行为的正则项和一个更灵活的基于幂律的自动调节弃权惩罚算法，并将其整合到三种不同的损失函数中（得到GAC、SAC、ADS），以增强模型对噪声标签的鲁棒性。

Result: 在CaDIS和DSAD两个医学数据集上进行实验，结果表明集成了弃权机制的新损失函数在高噪声环境下相对原始损失函数有显著且一致的性能提升。

Conclusion: 模型在分割任务中通过适当跳过（弃权）受噪声污染的样本，可以显著增强其鲁棒性，是构建可靠医学图像分割模型的一种通用而有效策略。

Abstract: Label noise is a critical problem in medical image segmentation, often arising from the inherent difficulty of manual annotation. Models trained on noisy data are prone to overfitting, which degrades their generalization performance. While a number of methods and strategies have been proposed to mitigate noisy labels in the segmentation domain, this area remains largely under-explored. The abstention mechanism has proven effective in classification tasks by enhancing the capabilities of Cross Entropy, yet its potential in segmentation remains unverified. In this paper, we address this gap by introducing a universal and modular abstention framework capable of enhancing the noise-robustness of a diverse range of loss functions. Our framework improves upon prior work with two key components: an informed regularization term to guide abstention behaviour, and a more flexible power-law-based auto-tuning algorithm for the abstention penalty. We demonstrate the framework's versatility by systematically integrating it with three distinct loss functions to create three novel, noise-robust variants: GAC, SAC, and ADS. Experiments on the CaDIS and DSAD medical datasets show our methods consistently and significantly outperform their non-abstaining baselines, especially under high noise levels. This work establishes that enabling models to selectively ignore corrupted samples is a powerful and generalizable strategy for building more reliable segmentation models. Our code is publicly available at https://github.com/wemous/abstention-for-segmentation.

</details>


### [241] [Federated Balanced Learning](https://arxiv.org/abs/2601.14042)
*Jiaze Li,Haoran Xu,Wanyi Wu,Changwei Wang,Shuaiguang Li,Jianzhong Ju,Zhenbo Luo,Jian Luan,Youyang Qu,Longxiang Gao,Xudong Yang,Lumin Xing*

Main category: cs.CV

TL;DR: 本文提出了一种名为FBL（Federated Balanced Learning）的联邦学习方法，通过在客户端实现样本平衡，缓解非独立同分布（non-iid）数据带来的客户端漂移问题，并显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中由于客户端数据分布不均（non-iid），全局模型容易发生“客户端漂移”，导致模型性能下降。以往方法主要通过优化损失或梯度来修正偏离，但忽略了客户端数据本身的影响。

Method: 论文提出了Federated Balanced Learning（FBL），利用边缘侧生成模型，通过知识补全和知识采样，使客户端数据实现样本平衡，同时遵循客户端数据量固定的前提。为此还设计了知识对齐策略（弥合合成数据与真实数据间的差距）和知识丢弃策略（正则化方法）。此外，FBL支持不同客户端采用不同方法，可扩展到复杂真实场景。

Result: 大量实验结果表明，FBL相较于现有最先进的联邦学习基线方法取得了更优性能。

Conclusion: 通过在客户端实现样本平衡和设计相关策略，FBL有效缓解了non-iid问题下的客户端漂移，从根本上提升了联邦学习的准确率和适应真实复杂场景的能力。

Abstract: Federated learning is a paradigm of joint learning in which clients collaborate by sharing model parameters instead of data. However, in the non-iid setting, the global model experiences client drift, which can seriously affect the final performance of the model. Previous methods tend to correct the global model that has already deviated based on the loss function or gradient, overlooking the impact of the client samples. In this paper, we rethink the role of the client side and propose Federated Balanced Learning, i.e., FBL, to prevent this issue from the beginning through sample balance on the client side. Technically, FBL allows unbalanced data on the client side to achieve sample balance through knowledge filling and knowledge sampling using edge-side generation models, under the limitation of a fixed number of data samples on clients. Furthermore, we design a Knowledge Alignment Strategy to bridge the gap between synthetic and real data, and a Knowledge Drop Strategy to regularize our method. Meanwhile, we scale our method to real and complex scenarios, allowing different clients to adopt various methods, and extend our framework to further improve performance. Numerous experiments show that our method outperforms state-of-the-art baselines. The code is released upon acceptance.

</details>


### [242] [Weather-R1: Logically Consistent Reinforcement Fine-Tuning for Multimodal Reasoning in Meteorology](https://arxiv.org/abs/2601.14044)
*Kaiyu Wu,Pucheng Han,Hualong Zhang,Naigeng Wu,Keze Wang*

Main category: cs.CV

TL;DR: 本论文针对视觉语言模型（VLMs）在气象领域存在的领域差距与推理一致性问题，提出了新的多模态推理数据集WeatherQA，并设计了逻辑一致性强化微调（LoCo-RFT）方法，显著提升推理可靠性和准确性，最终推出了气象领域首个逻辑可信的推理VLM——Weather-R1。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型虽然整体推理能力进步很大，但在高风险的气象应用中，由于专业领域差异和推理不一致性，传统强化微调方法会引入自相矛盾的推理结果，这会严重影响模型在实际气象需求中的可靠性和应用前景。

Method: 作者设计了WeatherQA这一全新的气象多模态推理测试基准，并提出逻辑一致性强化微调（LoCo-RFT）方法，通过逻辑一致性奖励机制缓解推理自相矛盾问题，提升模型的逻辑可靠性。同时，基于此法训练了新的气象推理VLM——Weather-R1。

Result: Weather-R1在WeatherQA基准上比原始基线提升了9.8个百分点，超越了有监督微调与传统强化微调方法，并胜过Qwen2.5-VL-32B。

Conclusion: 逻辑一致性奖励机制能够有效缓解高风险领域VLM推理一致性不足的问题，Weather-R1为气象多模态推理提供了新的高性能基础，推动了VLM在专业领域实际应用的进展。

Abstract: While Vision Language Models (VLMs) show advancing reasoning capabilities, their application in meteorology is constrained by a domain gap and a reasoning faithfulness gap. Specifically, mainstream Reinforcement Fine-Tuning (RFT) can induce Self-Contradictory Reasoning (Self-Contra), where the model's reasoning contradicts its final answer, which is unacceptable in such a high-stakes domain. To address these challenges, we construct WeatherQA, a novel multimodal reasoning benchmark in meteorology. We also propose Logically Consistent Reinforcement Fine-Tuning (LoCo-RFT), which resolves Self-Contra by introducing a logical consistency reward. Furthermore, we introduce Weather-R1, the first reasoning VLM with logical faithfulness in meteorology, to the best of our knowledge. Experiments demonstrate that Weather-R1 improves performance on WeatherQA by 9.8 percentage points over the baseline, outperforming Supervised Fine-Tuning and RFT, and even surpassing the original Qwen2.5-VL-32B. These results highlight the effectiveness of our LoCo-RFT and the superiority of Weather-R1. Our benchmark and code are available at https://github.com/Marcowky/Weather-R1.

</details>


### [243] [Vision Also You Need: Navigating Out-of-Distribution Detection with Multimodal Large Language Model](https://arxiv.org/abs/2601.14052)
*Haoran Xu,Yanlin Liu,Zizhao Tong,Jiaze Li,Kexue Fu,Yuyang Zhang,Longxiang Gao,Shuaiguang Li,Xingyu Li,Yanran Xu,Changwei Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态OOD检测方法MM-OOD，通过多回合对话和多模态推理大幅提升了近域和远域OOD检测表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的零样本OOD检测方法主要依赖于文本空间（LLM提供的知识），忽略了图像空间OOD检测中的挑战，因此需要更有效的多模态检测手段。

Method: 提出MM-OOD管线，利用大多模态大模型（MLLM）进行多轮推理对话，包括：(1) 近域OOD任务，将ID图片和文本直接输入MLLM判别；(2) 远域OOD任务，提出“sketch-generate-elaborate”框架，先用文本草拟异常，后生成视觉样本，再通过多模态提示补充推理。

Result: 在Food-101等多模态数据集上显著提升了检测准确率，在ImageNet-1K等大规模数据上验证了其可扩展性。

Conclusion: MM-OOD充分发挥了多模态大模型和多轮对话的优势，有效解决了依赖文本空间的局限，提升了OOD检测表现，并具有良好的扩展能力。

Abstract: Out-of-Distribution (OOD) detection is a critical task that has garnered significant attention. The emergence of CLIP has spurred extensive research into zero-shot OOD detection, often employing a training-free approach. Current methods leverage expert knowledge from large language models (LLMs) to identify potential outliers. However, these approaches tend to over-rely on knowledge in the text space, neglecting the inherent challenges involved in detecting out-of-distribution samples in the image space. In this paper, we propose a novel pipeline, MM-OOD, which leverages the multimodal reasoning capabilities of MLLMs and their ability to conduct multi-round conversations for enhanced outlier detection. Our method is designed to improve performance in both near OOD and far OOD tasks. Specifically, (1) for near OOD tasks, we directly feed ID images and corresponding text prompts into MLLMs to identify potential outliers; and (2) for far OOD tasks, we introduce the sketch-generate-elaborate framework: first, we sketch outlier exposure using text prompts, then generate corresponding visual OOD samples, and finally elaborate by using multimodal prompts. Experiments demonstrate that our method achieves significant improvements on widely used multimodal datasets such as Food-101, while also validating its scalability on ImageNet-1K.

</details>


### [244] [Decoder-Free Supervoxel GNN for Accurate Brain-Tumor Localization in Multi-Modal MRI](https://arxiv.org/abs/2601.14055)
*Andrea Protani,Marc Molina Van Den Bosch,Lorenzo Giusti,Heloisa Barbosa Da Silva,Paolo Cacace,Albert Sund Aillet,Miguel Angel Gonzalez Ballester,Friedhelm Hummel,Luigi Serio*

Main category: cs.CV

TL;DR: 本文提出SVGFormer，一种无解码器的3D医学影像视觉骨干网络，通过内容感知的分组与图结构建模，实现高效且可解释的特征学习，在BraTS数据集上的分类与回归任务均取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 当前3D医学影像主流方法多采用参数量大的编码-解码结构，大量参数用于空间重建而非特征学习，导致特征提取效率与解释性不足，亟需更高效且可解释的新架构。

Method: 提出SVGFormer结构，不采用传统解码器，而在卷积-图结构间引入内容感知分组，将体素分割为超体素的语义图。编码器一方面利用patch级Transformer捕获局部细节，另一方面用超体素级图注意力网络建模区域间依赖，联合多尺度表征。

Result: 在BraTS肿瘤数据集上，训练结点级分类和肿瘤比例回归模型，分类F1=0.875，回归MAE=0.028，显示编码器具备高度判别性和局部化能力。

Conclusion: 基于图的编码器-only架构不仅提升3D医学影像分析的准确性，还内生多尺度可解释性，是解耦结构的高效可解释替代方案。

Abstract: Modern vision backbones for 3D medical imaging typically process dense voxel grids through parameter-heavy encoder-decoder structures, a design that allocates a significant portion of its parameters to spatial reconstruction rather than feature learning. Our approach introduces SVGFormer, a decoder-free pipeline built upon a content-aware grouping stage that partitions the volume into a semantic graph of supervoxels. Its hierarchical encoder learns rich node representations by combining a patch-level Transformer with a supervoxel-level Graph Attention Network, jointly modeling fine-grained intra-region features and broader inter-regional dependencies. This design concentrates all learnable capacity on feature encoding and provides inherent, dual-scale explainability from the patch to the region level. To validate the framework's flexibility, we trained two specialized models on the BraTS dataset: one for node-level classification and one for tumor proportion regression. Both models achieved strong performance, with the classification model achieving a F1-score of 0.875 and the regression model a MAE of 0.028, confirming the encoder's ability to learn discriminative and localized features. Our results establish that a graph-based, encoder-only paradigm offers an accurate and inherently interpretable alternative for 3D medical image representation.

</details>


### [245] [POCI-Diff: Position Objects Consistently and Interactively with 3D-Layout Guided Diffusion](https://arxiv.org/abs/2601.14056)
*Andrea Rigo,Luca Stornaiuolo,Weijie Wang,Mauro Martino,Bruno Lepri,Nicu Sebe*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的文本到图像(T2I)生成方法，实现了对三维布局的可控且一致的交互式编辑。新方法解决了以往几何扭曲、编辑不一致等问题，实现了更高的图像质量与布局遵循性。


<details>
  <summary>Details</summary>
Motivation: 现有T2I方法多数基于二维提示或简单的迭代贴图策略，这些方法容易导致对象几何失真与编辑间不一致，难以满足多对象场景下对三维布局与语义绑定的需求。因此，迫切需要一种能一致控制三维对象布局并进行多次交互式编辑的生成方法。

Method: 作者提出了POCI-Diff框架，在单一扩散过程中联合建模三维几何约束与实例级语义绑定，将文本描述与具体三维包围盒相绑定，并通过Blended Latent Diffusion实现多对象场景的一步生成。编辑过程中，通过基于再生成的无变形对象插入、移除和变换操作，配合IP-Adapter对参考图像的条件控制，以实现对象一致性的交互编辑。

Result: 实验结果显示，POCI-Diff在图像视觉质量、三维布局一致性以及编辑连贯性方面均优于现有最先进方法，有效消除了因变形操作导致的几何伪影。

Conclusion: 该方法为T2I生成提供了强大的一致三维布局编辑能力，有效提升了多对象场景下的几何和语义控制，推动了高保真可控生成的研究前沿。

Abstract: We propose a diffusion-based approach for Text-to-Image (T2I) generation with consistent and interactive 3D layout control and editing. While prior methods improve spatial adherence using 2D cues or iterative copy-warp-paste strategies, they often distort object geometry and fail to preserve consistency across edits. To address these limitations, we introduce a framework for Positioning Objects Consistently and Interactively (POCI-Diff), a novel formulation for jointly enforcing 3D geometric constraints and instance-level semantic binding within a unified diffusion process. Our method enables explicit per-object semantic control by binding individual text descriptions to specific 3D bounding boxes through Blended Latent Diffusion, allowing one-shot synthesis of complex multi-object scenes. We further propose a warping-free generative editing pipeline that supports object insertion, removal, and transformation via regeneration rather than pixel deformation. To preserve object identity and consistency across edits, we condition the diffusion process on reference images using IP-Adapter, enabling coherent object appearance throughout interactive 3D editing while maintaining global scene coherence. Experimental results demonstrate that POCI-Diff produces high-quality images consistent with the specified 3D layouts and edits, outperforming state-of-the-art methods in both visual fidelity and layout adherence while eliminating warping-induced geometric artifacts.

</details>


### [246] [Fine-Grained Zero-Shot Composed Image Retrieval with Complementary Visual-Semantic Integration](https://arxiv.org/abs/2601.14060)
*Yongcong Ye,Kai Zhang,Yanghai Zhang,Enhong Chen,Longfei Li,Jun Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新的零样本复合图像检索方法（CVSI），在输入参考图像和相对描述的基础上，有效整合视觉和语义信息，实现更细粒度的图像检索。


<details>
  <summary>Details</summary>
Motivation: 当前零样本复合图像检索方法难以捕捉细粒度变化，且视觉与语义信息融合不足，主要依赖图像-文本单一转化或大模型文本生成，不能充分利用互补信息，导致性能有限。

Method: 提出CVSI方法，包括：（1）视觉信息提取：不仅提取全局特征，还将图像转为伪token并与修改文本、可能新增对象组合；（2）语义信息提取：用预训练描述模型生成多种图像描述，再用大语言模型生成修改描述和预测新增对象；（3）互补信息检索：融合来自查询与数据库图像的互补信息，提高检索能力。

Result: 在CIRR、CIRCO和FashionIQ三大公开数据集上，CVSI方法显著优于当前先进方法，证明其实用性和有效性。

Conclusion: CVSI有效整合视觉与语义信息，实现更细致的零样本复合图像检索，推动该领域的性能提升。

Abstract: Zero-shot composed image retrieval (ZS-CIR) is a rapidly growing area with significant practical applications, allowing users to retrieve a target image by providing a reference image and a relative caption describing the desired modifications. Existing ZS-CIR methods often struggle to capture fine-grained changes and integrate visual and semantic information effectively. They primarily rely on either transforming the multimodal query into a single text using image-to-text models or employing large language models for target image description generation, approaches that often fail to capture complementary visual information and complete semantic context. To address these limitations, we propose a novel Fine-Grained Zero-Shot Composed Image Retrieval method with Complementary Visual-Semantic Integration (CVSI). Specifically, CVSI leverages three key components: (1) Visual Information Extraction, which not only extracts global image features but also uses a pre-trained mapping network to convert the image into a pseudo token, combining it with the modification text and the objects most likely to be added. (2) Semantic Information Extraction, which involves using a pre-trained captioning model to generate multiple captions for the reference image, followed by leveraging an LLM to generate the modified captions and the objects most likely to be added. (3) Complementary Information Retrieval, which integrates information extracted from both the query and database images to retrieve the target image, enabling the system to efficiently handle retrieval queries in a variety of situations. Extensive experiments on three public datasets (e.g., CIRR, CIRCO, and FashionIQ) demonstrate that CVSI significantly outperforms existing state-of-the-art methods. Our code is available at https://github.com/yyc6631/CVSI.

</details>


### [247] [VERIDAH: Solving Enumeration Anomaly Aware Vertebra Labeling across Imaging Sequences](https://arxiv.org/abs/2601.14066)
*Hendrik Möller,Hanna Schoen,Robert Graf,Matan Atad,Nathan Molinier,Anjany Sekuboyina,Bettina K. Budai,Fabian Bamberg,Steffen Ringhof,Christopher Schlett,Tobias Pischon,Thoralf Niendorf,Josua A. Decker,Marc-André Weber,Bjoern Menze,Daniel Rueckert,Jan S. Kirschke*

Main category: cs.CV

TL;DR: 本研究提出了一种新型的脊椎自动标注算法VERIDAH，能有效识别异常数量的胸椎与腰椎、提高标注准确率，并在不同影像（MRI/CT）中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习脊椎标注算法难以识别脊椎数量异常（例如11/13胸椎、4/6腰椎），而这种异常对于慢性背痛的诊断和手术规划具有重要意义，目前临床报告对此描述有限，相关自动化方法尚为空白。

Method: 作者提出名为VERIDAH的多分类头结合加权脊椎序列预测算法，能够自动识别并标注带有枚举异常（脊椎数量异常）患者的脊椎，支持任意视野范围的T2w TSE矢状面MRI及CT影像。

Result: 在T2w MRI影像上，VERIDAH实现了98.30%的完整标注准确率（对比以往算法94.24%，p<0.001）；在CT影像上，准确率达99.18%（对比77.26%，p<0.001）。此外，该算法对胸椎异常检出率为87.80%（MRI）和96.30%（CT），腰椎异常为94.48%（MRI）和97.22%（CT）。

Conclusion: VERIDAH显著提升了脊椎自动标注的精准度，尤其能有效检测与标记胸腰椎枚举异常，为慢性背痛临床诊疗及手术规划提供可靠工具，且可广泛适用于不同扫描视野的医学影像。

Abstract: The human spine commonly consists of seven cervical, twelve thoracic, and five lumbar vertebrae. However, enumeration anomalies may result in individuals having eleven or thirteen thoracic vertebrae and four or six lumbar vertebrae. Although the identification of enumeration anomalies has potential clinical implications for chronic back pain and operation planning, the thoracolumbar junction is often poorly assessed and rarely described in clinical reports. Additionally, even though multiple deep-learning-based vertebra labeling algorithms exist, there is a lack of methods to automatically label enumeration anomalies. Our work closes that gap by introducing "Vertebra Identification with Anomaly Handling" (VERIDAH), a novel vertebra labeling algorithm based on multiple classification heads combined with a weighted vertebra sequence prediction algorithm. We show that our approach surpasses existing models on T2w TSE sagittal (98.30% vs. 94.24% of subjects with all vertebrae correctly labeled, p < 0.001) and CT imaging (99.18% vs. 77.26% of subjects with all vertebrae correctly labeled, p < 0.001) and works in arbitrary field-of-view images. VERIDAH correctly labeled the presence 2 Möller et al. of thoracic enumeration anomalies in 87.80% and 96.30% of T2w and CT images, respectively, and lumbar enumeration anomalies in 94.48% and 97.22% for T2w and CT, respectively. Our code and models are available at: https://github.com/Hendrik-code/spineps.

</details>


### [248] [Unsupervised Video Class-Incremental Learning via Deep Embedded Clustering Management](https://arxiv.org/abs/2601.14069)
*Nattapong Kurpukdee,Adrian G. Bors*

Main category: cs.CV

TL;DR: 本文提出了一种无需标签和任务边界信息的无监督视频类增量学习（uVCIL）方法，通过深度特征提取与聚类实现新类增量学习，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视频类增量学习大多依赖监督信息（如标签和任务边界），这种方式成本高且不现实。因此，作者希望开发一种既能避免遗忘、又不依赖标签的无监督解决方案。

Method: 作者使用深度特征提取网络，在每个任务阶段提取视频特征，无需任何类或任务信息。然后对这些特征进行深度聚类，并在后续任务中，利用前一任务的模型状态进行知识迁移。整个过程中不使用任何标签或任务边界信息。

Result: 在忽略监督标签的前提下，方法在UCF101、HMDB51和Something-to-Something V2三个标准动作识别数据集上进行了测试，在所有数据集上都显著优于现有基线方法。

Conclusion: 该无监督增量学习方法能够有效地实现无标签下的视频分类增量学习，并且有较强的泛化能力和优越表现，展示了其在现实场景中的应用潜力。

Abstract: Unsupervised video class incremental learning (uVCIL) represents an important learning paradigm for learning video information without forgetting, and without considering any data labels. Prior approaches have focused on supervised class-incremental learning, relying on using the knowledge of labels and task boundaries, which is costly, requires human annotation, or is simply not a realistic option. In this paper, we propose a simple yet effective approach to address the uVCIL. We first consider a deep feature extractor network, providing a set of representative video features during each task without assuming any class or task information. We then progressively build a series of deep clusters from the extracted features. During the successive task learning, the model updated from the previous task is used as an initial state in order to transfer knowledge to the current learning task. We perform in-depth evaluations on three standard video action recognition datasets, including UCF101, HMDB51, and Something-to-Something V2, by ignoring the labels from the supervised setting. Our approach significantly outperforms other baselines on all datasets.

</details>


### [249] [VENI: Variational Encoder for Natural Illumination](https://arxiv.org/abs/2601.14079)
*Paul Walker,James A. D. Gardner,Andreea Ardelean,William A. P. Smith,Bernhard Egger*

Main category: cs.CV

TL;DR: 本文提出了一种旋转等变的变分自编码器，能够有效建模球面上的自然光照环境，并具备更好的潜在空间表示能力。


<details>
  <summary>Details</summary>
Motivation: 逆向渲染是一个病态问题，依赖合适的先验信息（如光照先验）可简化该任务。现有方法往往忽视了光照环境的球面对称性和旋转等变性，或者未能提供良好的潜在空间。本文旨在克服上述两个问题。

Method: 提出了一种新的旋转等变变分自编码器，直接在球面上建模光照环境。具体方法包括：用新设计的Vector Neuron Vision Transformer (VN-ViT)作为编码器，以及旋转等变的条件神经场作为解码器，并在编码器中引入一种SO(2)-等变的全连接层来降低等变级别，从SO(3)变为SO(2)。

Result: 实验表明，SO(2)-等变全连接层在SO(2)-等变模型中优于标准Vector Neurons。与以往方法相比，本文方法在潜在空间中的插值平滑性和潜在空间结构上有明显提升。

Conclusion: 本工作提出的方法能够更有效地建模球面光照，提供更良好的潜在空间，为逆向渲染等任务提供了新思路和工具。

Abstract: Inverse rendering is an ill-posed problem, but priors like illumination priors, can simplify it. Existing work either disregards the spherical and rotation-equivariant nature of illumination environments or does not provide a well-behaved latent space. We propose a rotation-equivariant variational autoencoder that models natural illumination on the sphere without relying on 2D projections. To preserve the SO(2)-equivariance of environment maps, we use a novel Vector Neuron Vision Transformer (VN-ViT) as encoder and a rotation-equivariant conditional neural field as decoder. In the encoder, we reduce the equivariance from SO(3) to SO(2) using a novel SO(2)-equivariant fully connected layer, an extension of Vector Neurons. We show that our SO(2)-equivariant fully connected layer outperforms standard Vector Neurons when used in our SO(2)-equivariant model. Compared to previous methods, our variational autoencoder enables smoother interpolation in latent space and offers a more well-behaved latent space.

</details>


### [250] [Two-Stream temporal transformer for video action classification](https://arxiv.org/abs/2601.14086)
*Nattapong Kurpukdee,Adrian G. Bors*

Main category: cs.CV

TL;DR: 本文提出了一种新的两流Transformer视频分类器，结合内容与光流信息，能更好地进行视频中的运动与动作识别。实验证明该方法在三个人体动作视频数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 运动表征对视频理解至关重要，但现有方法在提取空间和时间特征时存在不足，尤其在复杂动作识别方面。本文试图利用Transformer的自注意力机制，融合内容与光流信息提升动作识别效果。

Method: 提出一种两流Transformer视频分类器。一路从视频内容帧提取时空信息，另一路通过光流捕捉运动信息。模型在Transformer编码器结构下融合两者信息，通过自注意力机制学习内容与运动之间的关系。

Result: 在三个知名人体活动视频数据集上评测，所提出方法的分类性能达到了优秀水平，优于常规或传统方法。

Conclusion: 通过融合内容帧和光流信息，并利用Transformer的自注意力机制，能够更好捕捉和理解视频中的运动，提升了动作识别的准确性和鲁棒性。

Abstract: Motion representation plays an important role in video understanding and has many applications including action recognition, robot and autonomous guidance or others. Lately, transformer networks, through their self-attention mechanism capabilities, have proved their efficiency in many applications. In this study, we introduce a new two-stream transformer video classifier, which extracts spatio-temporal information from content and optical flow representing movement information. The proposed model identifies self-attention features across the joint optical flow and temporal frame domain and represents their relationships within the transformer encoder mechanism. The experimental results show that our proposed methodology provides excellent classification results on three well-known video datasets of human activities.

</details>


### [251] [Curriculum-Based Strategies for Efficient Cross-Domain Action Recognition](https://arxiv.org/abs/2601.14101)
*Emily Kim,Allen Wu,Jessica Hodgins*

Main category: cs.CV

TL;DR: 本文探讨了利用课程学习提升动作识别模型在未见过的航拍视角下的泛化能力，无需使用真实航拍训练数据。通过合成航拍和真实地面数据的课程训练策略，提升了模型效率且基本保持了准确率。


<details>
  <summary>Details</summary>
Motivation: 现有动作识别研究多依赖地面视角训练数据，模型在迁移至航拍等视角时泛化能力不足，实际应用受限。因此，提升模型跨视角泛化能力成为亟需解决的难题。

Method: 作者提出两种基于课程学习的跨视角训练策略：一种为两阶段直接细调方法，先用合成航拍数据预训练，再用真实地面数据细调；另一种为多阶段渐进课程，逐步扩充数据集后再细调。全部训练过程未用任何真实航拍训练数据。实验在REMAG数据集和SlowFast（CNN）及MViTv2（Transformer）架构上进行。

Result: 合成航拍与真实地面数据结合显著优于单一域训练。课程学习策略在准确率（top-1）接近简单合并法的同时，提升了训练效率：两阶段法迭代次数降低30-37%，多阶段渐进法进一步提升9-30%。

Conclusion: 课程学习能够在保持识别性能的前提下，显著提升跨视角动作识别的训练效率，为无航拍训练数据条件下的泛化提供了有效途径。

Abstract: Despite significant progress in human action recognition, generalizing to diverse viewpoints remains a challenge. Most existing datasets are captured from ground-level perspectives, and models trained on them often struggle to transfer to drastically different domains such as aerial views. This paper examines how curriculum-based training strategies can improve generalization to unseen real aerial-view data without using any real aerial data during training.
  We explore curriculum learning for cross-view action recognition using two out-of-domain sources: synthetic aerial-view data and real ground-view data. Our results on the evaluation on order of training (fine-tuning on synthetic aerial data vs. real ground data) shows that fine-tuning on real ground data but differ in how they transition from synthetic to real. The first uses a two-stage curriculum with direct fine-tuning, while the second applies a progressive curriculum that expands the dataset in multiple stages before fine-tuning. We evaluate both methods on the REMAG dataset using SlowFast (CNN-based) and MViTv2 (Transformer-based) architectures.
  Results show that combining the two out-of-domain datasets clearly outperforms training on a single domain, whether real ground-view or synthetic aerial-view. Both curriculum strategies match the top-1 accuracy of simple dataset combination while offering efficiency gains. With the two-step fine-tuning method, SlowFast achieves up to a 37% reduction in iterations and MViTv2 up to a 30% reduction compared to simple combination. The multi-step progressive approach further reduces iterations, by up to 9% for SlowFast and 30% for MViTv2, relative to the two-step method. These findings demonstrate that curriculum-based training can maintain comparable performance (top-1 accuracy within 3% range) while improving training efficiency in cross-view action recognition.

</details>


### [252] [Interp3D: Correspondence-aware Interpolation for Generative Textured 3D Morphing](https://arxiv.org/abs/2601.14103)
*Xiaolu Liu,Yicong Li,Qiyuan He,Jiayin Zhu,Wei Ji,Angela Yao,Jianke Zhu*

Main category: cs.CV

TL;DR: 本文提出Interp3D框架，实现结构与纹理协同一致的3D模型变形，能同时保持几何结构和高质量纹理过渡，优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 当前3D变形方法常忽略纹理或引入结构错位、纹理模糊等问题，难以实现流畅且细致的3D资产转变，对动画、编辑等应用有限。亟需方法能在3D形状和纹理层面同时实现一致且鲁棒的变形。

Method: 提出无训练的Interp3D框架，利用生成先验和递进对齐策略，包括：（1）在条件空间进行语义一致插值；（2）用SLAT引导结构插值，确保结构一致性；（3）通过纹理融合传递外观细节。此外，作者还构建了难度分级的数据集Interp3DData用于全面评测。

Result: 在各种指标（保真度、平滑性、合理性）下，Interp3D于定量和人工评测上均显著优于现有3D变形方法。

Conclusion: Interp3D能实现高质量、结构与纹理一致的3D模型无缝过渡，具备理想的应用前景。新数据集和工具代码已开源，有望推动3D生成和相关产业发展。

Abstract: Textured 3D morphing seeks to generate smooth and plausible transitions between two 3D assets, preserving both structural coherence and fine-grained appearance. This ability is crucial not only for advancing 3D generation research but also for practical applications in animation, editing, and digital content creation. Existing approaches either operate directly on geometry, limiting them to shape-only morphing while neglecting textures, or extend 2D interpolation strategies into 3D, which often causes semantic ambiguity, structural misalignment, and texture blurring. These challenges underscore the necessity to jointly preserve geometric consistency, texture alignment, and robustness throughout the transition process. To address this, we propose Interp3D, a novel training-free framework for textured 3D morphing. It harnesses generative priors and adopts a progressive alignment principle to ensure both geometric fidelity and texture coherence. Starting from semantically aligned interpolation in condition space, Interp3D enforces structural consistency via SLAT (Structured Latent)-guided structure interpolation, and finally transfers appearance details through fine-grained texture fusion. For comprehensive evaluations, we construct a dedicated dataset, Interp3DData, with graded difficulty levels and assess generation results from fidelity, transition smoothness, and plausibility. Both quantitative metrics and human studies demonstrate the significant advantages of our proposed approach over previous methods. Source code is available at https://github.com/xiaolul2/Interp3D.

</details>


### [253] [PMCE: Probabilistic Multi-Granularity Semantics with Caption-Guided Enhancement for Few-Shot Learning](https://arxiv.org/abs/2601.14111)
*Jiaying Wu,Can Gao,Jinglu Hu,Hui Li,Xiaofeng Cao,Jingcai Guo*

Main category: cs.CV

TL;DR: 本文提出了一种结合多粒度语义信息和描述增强的概率型小样本学习框架（PMCE），通过知识库和自动生成图片描述的方法，在少样本分类任务中显著提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 传统小样本学习中，由于可用样本极少，估算出的类别原型容易偏置、泛化能力弱。虽然部分方法引入了语义信息以缓解问题，但大多只在支持集上做处理，查询集特征未被优化，对整体性能提升有限。因此，需要一种能利用更丰富语义和实例级增强手段的方法，全面提升小样本分类表现。

Method: 1）构建了一个无参数知识库，包含各类别视觉统计和基于CLIP模型编码的类别名称语义向量；
2）在测试阶段为每个新类检索最相关的基础类别，并将其统计信息与支持集原型通过MAP更新融合。
3）利用冻结的BLIP模型生成图片描述，结合轻量增强网络，同时优化支持集原型和查询集特征，借助一致性正则化稳定描述噪声。

Result: 在四个标准小样本学习基准上实验，PMCE表现优异，明显超越了多个主流语义基方法。在MiniImageNet数据集1-shot分支，PMCE相较最强对手能达到绝对7.71%的提升。

Conclusion: PMCE通过多粒度语义（类别名+实例描述）有效提升了小样本分类的泛化能力，并能显著超过现有主流方法，是小样本学习领域有价值的新进展。

Abstract: Few-shot learning aims to identify novel categories from only a handful of labeled samples, where prototypes estimated from scarce data are often biased and generalize poorly. Semantic-based methods alleviate this by introducing coarse class-level information, but they are mostly applied on the support side, leaving query representations unchanged. In this paper, we present PMCE, a Probabilistic few-shot framework that leverages Multi-granularity semantics with Caption-guided Enhancement. PMCE constructs a nonparametric knowledge bank that stores visual statistics for each category as well as CLIP-encoded class name embeddings of the base classes. At meta-test time, the most relevant base classes are retrieved based on the similarities of class name embeddings for each novel category. These statistics are then aggregated into category-specific prior information and fused with the support set prototypes via a simple MAP update. Simultaneously, a frozen BLIP captioner provides label-free instance-level image descriptions, and a lightweight enhancer trained on base classes optimizes both support prototypes and query features under an inductive protocol with a consistency regularization to stabilize noisy captions. Experiments on four benchmarks show that PMCE consistently improves over strong baselines, achieving up to 7.71% absolute gain over the strongest semantic competitor on MiniImageNet in the 1-shot setting. Our code is available at https://anonymous.4open.science/r/PMCE-275D

</details>


### [254] [GIC-DLC: Differentiable Logic Circuits for Hardware-Friendly Grayscale Image Compression](https://arxiv.org/abs/2601.14130)
*Till Aczel,David F. Jenny,Simon Bührer,Andreas Plesner,Antonio Di Maio,Roger Wattenhofer*

Main category: cs.CV

TL;DR: 本文提出了一种新型的灰度图像压缩方法——GIC-DLC，通过可微分的逻辑电路结合神经网络的灵活性与布尔操作的高效性，实现了高效低耗的图像压缩。


<details>
  <summary>Details</summary>
Motivation: 虽然神经网络压缩（neural image codecs）相比传统方法有更高的压缩率，但计算和能耗开销大，不适合能耗受限的设备。因此需要兼顾压缩效率与低能耗的新方案。

Method: 提出GIC-DLC方法，采用训练的查找表，融合神经网络灵活性和硬件友好的布尔运算，通过可微分逻辑电路实现图像压缩。针对硬件实现进行优化，兼顾精度与效率。

Result: 在灰度图像基准数据集上，GIC-DLC压缩率优于传统编码方法（如PNG、JPEG-XL），且显著降低了能耗和延迟。

Conclusion: GIC-DLC证明了学习型压缩算法可以做到硬件友好，拓展了低功耗终端上的高效图像压缩新方向。

Abstract: Neural image codecs achieve higher compression ratios than traditional hand-crafted methods such as PNG or JPEG-XL, but often incur substantial computational overhead, limiting their deployment on energy-constrained devices such as smartphones, cameras, and drones. We propose Grayscale Image Compression with Differentiable Logic Circuits (GIC-DLC), a hardware-aware codec where we train lookup tables to combine the flexibility of neural networks with the efficiency of Boolean operations. Experiments on grayscale benchmark datasets show that GIC-DLC outperforms traditional codecs in compression efficiency while allowing substantial reductions in energy consumption and latency. These results demonstrate that learned compression can be hardware-friendly, offering a promising direction for low-power image compression on edge devices.

</details>


### [255] [LLM Augmented Intervenable Multimodal Adaptor for Post-operative Complication Prediction in Lung Cancer Surgery](https://arxiv.org/abs/2601.14154)
*Shubham Pandey,Bhavin Jawade,Srirangaraj Setlur,Venu Govindaraju,Kenneth Seastedt*

Main category: cs.CV

TL;DR: 该论文提出了一种名为MIRACLE的深度学习架构，用于预测肺癌手术后并发症的风险，通过融合术前临床和影像数据实现更准确、可解释的个体化风险管理。


<details>
  <summary>Details</summary>
Motivation: 手术后并发症严重影响患者结局并增加医疗成本，目前的风险预测方法存在精度和可解释性不足的问题，因此需要新的技术手段提升预测能力并增强临床实用性。

Method: MIRACLE结合了结构化临床数据与高维影像信息，在超球面嵌入空间内实现异构信息融合，并引入干预型深度学习模块，使模型不仅优化预测结果，还能生成可解释、可操作的决策建议，支持专家交互式调整。该方法在真实的POC-L数据集（包含3094例肺癌手术患者）上进行了验证。

Result: MIRACLE模型在预测手术并发症风险时，优于多种传统机器学习模型及当前流行的大语言模型（LLM）变体，实现了更具个体化和解释性的风险管理效果。

Conclusion: MIRACLE通过推进多模态数据融合和深度学习技术，提升了肺癌手术后并发症风险预测的精度和可解释性，对实现个体化、可解释的术后风险管理具有重要意义。

Abstract: Postoperative complications remain a critical concern in clinical practice, adversely affecting patient outcomes and contributing to rising healthcare costs. We present MIRACLE, a deep learning architecture for prediction of risk of postoperative complications in lung cancer surgery by integrating preoperative clinical and radiological data. MIRACLE employs a hyperspherical embedding space fusion of heterogeneous inputs, enabling the extraction of robust, discriminative features from both structured clinical records and high-dimensional radiological images. To enhance transparency of prediction and clinical utility, we incorporate an interventional deep learning module in MIRACLE, that not only refines predictions but also provides interpretable and actionable insights, allowing domain experts to interactively adjust recommendations based on clinical expertise. We validate our approach on POC-L, a real-world dataset comprising 3,094 lung cancer patients who underwent surgery at Roswell Park Comprehensive Cancer Center. Our results demonstrate that MIRACLE outperforms various traditional machine learning models and contemporary large language models (LLM) variants alone, for personalized and explainable postoperative risk management.

</details>


### [256] [One-Shot Refiner: Boosting Feed-forward Novel View Synthesis via One-Step Diffusion](https://arxiv.org/abs/2601.14161)
*Yitong Dong,Qi Zhang,Minchao Jiang,Zhiqiang Wu,Qingnan Fan,Ying Feng,Huaqi Zhang,Hujun Bao,Guofeng Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种能够从稀疏视角图片进行高保真新视图合成的新框架，结合了ViT主干与扩散网络，有效克服了原始3D Gaussian Splatting方法的分辨率和一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于ViT的3D高斯点云合成方法受限于低分辨率输入，并且传统增强手段缺乏3D感知，导致新视图结构不一致，尤其在未见区域表现较差。

Method: 设计了双域细节感知模块，使得对于高分辨率图像处理不再受ViT主干限制，并为高斯点云增加额外特征以保存高频细节。同时，提出了一种特征引导的扩散网络，用于在恢复过程中保留高频信息。最后，采用统一训练策略，联合优化ViT主干和扩散细化模块。

Result: 实验表明，该方法在多个数据集上都保持了较优的生成质量，显著提升了稀疏视角新视图合成的表现。

Conclusion: 该框架有效结合ViT和扩散网络优势，解决了分辨率和3D一致性难题，在高保真新视图合成任务中取得了优越性能，推动了相关领域的发展。

Abstract: We present a novel framework for high-fidelity novel view synthesis (NVS) from sparse images, addressing key limitations in recent feed-forward 3D Gaussian Splatting (3DGS) methods built on Vision Transformer (ViT) backbones. While ViT-based pipelines offer strong geometric priors, they are often constrained by low-resolution inputs due to computational costs. Moreover, existing generative enhancement methods tend to be 3D-agnostic, resulting in inconsistent structures across views, especially in unseen regions. To overcome these challenges, we design a Dual-Domain Detail Perception Module, which enables handling high-resolution images without being limited by the ViT backbone, and endows Gaussians with additional features to store high-frequency details. We develop a feature-guided diffusion network, which can preserve high-frequency details during the restoration process. We introduce a unified training strategy that enables joint optimization of the ViT-based geometric backbone and the diffusion-based refinement module. Experiments demonstrate that our method can maintain superior generation quality across multiple datasets.

</details>


### [257] [ASBA: A-line State Space Model and B-line Attention for Sparse Optical Doppler Tomography Reconstruction](https://arxiv.org/abs/2601.14165)
*Zhenghong Li,Wensheng Cheng,Congwu Du,Yingtian Pan,Zhaozheng Yin,Haibin Ling*

Main category: cs.CV

TL;DR: 本文提出了一种新型的血流感知网络ASBA，用于从高度稀疏采样的原始A-scan数据中高保真地重建ODT血流图像。


<details>
  <summary>Details</summary>
Motivation: 现有ODT图像重建方法依赖密集采样，导致扫描时间延长、存储需求增加，且难以捕捉快速血流动态。稀疏采样可缓解这些问题，但现有方法采样率保守，且未能有效区分血流与背景信号，效果有限。

Method: 作者提出ASBA网络，包括A-line ROI状态空间模型（提取A线稀疏分布的血流特征）和B-line相位注意力机制（结合相位差捕获B线上的远程血流信号），并设计血流感知加权损失函数以提升重要血流信号的重建质量。

Result: 在真实动物实验数据上，所提方法显著优于现有主流ODT图像重建方法，能够更准确高效地重建血流信号。

Conclusion: ASBA网络能在极度稀疏采样条件下，实现高质量血流ODT图像重建，可极大提升血流动力学检测的效率和实时性，为ODT临床应用提供了新的思路。

Abstract: Optical Doppler Tomography (ODT) is an emerging blood flow analysis technique. A 2D ODT image (B-scan) is generated by sequentially acquiring 1D depth-resolved raw A-scans (A-line) along the lateral axis (B-line), followed by Doppler phase-subtraction analysis. To ensure high-fidelity B-scan images, current practices rely on dense sampling, which prolongs scanning time, increases storage demands, and limits the capture of rapid blood flow dynamics. Recent studies have explored sparse sampling of raw A-scans to alleviate these limitations, but their effectiveness is hindered by the conservative sampling rates and the uniform modeling of flow and background signals. In this study, we introduce a novel blood flow-aware network, named ASBA (A-line ROI State space model and B-line phase Attention), to reconstruct ODT images from highly sparsely sampled raw A-scans. Specifically, we propose an A-line ROI state space model to extract sparsely distributed flow features along the A-line, and a B-line phase attention to capture long-range flow signals along each B-line based on phase difference. Moreover, we introduce a flow-aware weighted loss function that encourages the network to prioritize the accurate reconstruction of flow signals. Extensive experiments on real animal data demonstrate that the proposed approach clearly outperforms existing state-of-the-art reconstruction methods.

</details>


### [258] [Progressive self-supervised blind-spot denoising method for LDCT denoising](https://arxiv.org/abs/2601.14180)
*Yichao Liu,Yueyang Teng,Junwen Guo*

Main category: cs.CV

TL;DR: 本文提出了一种全新的自监督低剂量CT（LDCT）去噪方法，仅基于LDCT数据，且在多个实验中优于现有自监督方法，并可与主流有监督方法媲美。


<details>
  <summary>Details</summary>
Motivation: 目前LDCT去噪领域依赖成对的正常剂量CT（NDCT）数据，但获取这样的数据在临床实践中非常困难，因此需要依赖自监督学习方法。

Method: 采用分步盲点去噪机制，逐步强化条件独立性，使得去噪学习更加细致。同时，在LDCT图像中加入高斯噪声以作为正则化，防止过拟合。

Result: 在Mayo LDCT数据集上的大量实验表明，该方法的一致性能优于现有自监督方法，并可媲美甚至超越一些代表性的有监督去噪方法。

Conclusion: 该自监督去噪方法仅需LDCT数据、无需成对NDCT样本，在实际应用中更具可行性及应用价值，并为LDCT去噪提供了有效的新方案。

Abstract: Self-supervised learning is increasingly investigated for low-dose computed tomography (LDCT) image denoising, as it alleviates the dependence on paired normal-dose CT (NDCT) data, which are often difficult to acquire in clinical practice. In this paper, we propose a novel self-supervised training strategy that relies exclusively on LDCT images. We introduce a step-wise blind-spot denoising mechanism that enforces conditional independence in a progressive manner, enabling more fine-grained denoising learning. In addition, we add Gaussian noise to LDCT images, which acts as a regularization and mitigates overfitting. Extensive experiments on the Mayo LDCT dataset demonstrate that the proposed method consistently outperforms existing self-supervised approaches and achieves performance comparable to, or better than, several representative supervised denoising methods.

</details>


### [259] [IIR-VLM: In-Context Instance-level Recognition for Large Vision-Language Models](https://arxiv.org/abs/2601.14188)
*Liang Shi,Wei Li,Kevin M Beussman,Lin Chen,Yun Fu*

Main category: cs.CV

TL;DR: 本文提出了IIR-VLM，一种针对实例级识别任务（如人员再识别）增强的视觉语言模型，通过引入预训练ILR专家模型作为辅助视觉编码器，实现了VLM对新实例的高效、一次性学习，并在多个基准上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 尽管现代视觉语言模型具有强大的视觉感知能力，在区分个体实例的实例级识别（ILR）任务上仍表现不佳，远逊于专门的ILR模型，严重影响了其实用性。现有方法需针对每个实例单独收集数据和训练，成本高且难以精细区分。

Method: 文中提出将预训练的ILR专家模型作为辅助视觉编码器，融合其专业特征进VLM中，使模型具备一次性上下文学习新实例的能力（one-shot in-context learning），实现对多样实例的有效识别和理解。

Result: IIR-VLM在现有的实例个性化基准上通过验证，并在涵盖人物、面孔、宠物和一般物体等多类别、不同难度的新ILR基准测试中取得了优异的实例级识别性能。

Conclusion: IIR-VLM显著提升了视觉语言模型在实例级识别任务中的能力，提升了其实用性和泛化性，为在真实场景下识别特定人物或物体等高难度任务提供了有效解决方案。

Abstract: Instance-level recognition (ILR) concerns distinguishing individual instances from one another, with person re-identification as a prominent example. Despite the impressive visual perception capabilities of modern VLMs, we find their performance on ILR unsatisfactory, often dramatically underperforming domain-specific ILR models. This limitation hinders many practical application of VLMs, e.g. where recognizing familiar people and objects is crucial for effective visual understanding. Existing solutions typically learn to recognize instances one at a time using instance-specific datasets, which not only incur substantial data collection and training costs but also struggle with fine-grained discrimination. In this work, we propose IIR-VLM, a VLM enhanced for In-context Instance-level Recognition. We integrate pre-trained ILR expert models as auxiliary visual encoders to provide specialized features for learning diverse instances, which enables VLMs to learn new instances in-context in a one-shot manner. Further, IIR-VLM leverages this knowledge for instance-aware visual understanding. We validate IIR-VLM's efficacy on existing instance personalization benchmarks. Finally, we demonstrate its superior ILR performance on a challenging new benchmark, which assesses ILR capabilities across varying difficulty and diverse categories, with person, face, pet and general objects as the instances at task.

</details>


### [260] [Rig-Aware 3D Reconstruction of Vehicle Undercarriages using Gaussian Splatting](https://arxiv.org/abs/2601.14208)
*Nitin Kulkarni,Akhil Devarashetti,Charlie Cluss,Livio Forte,Dan Buckmaster,Philip Schneider,Chunming Qiao,Alina Vereshchaka*

Main category: cs.CV

TL;DR: 本文提出了一种基于三摄像头设备的端到端系统，用于自动捕捉并生成汽车底盘的交互式3D模型，极大简化了二手车底盘检测过程，提升了安全性及买家信心。


<details>
  <summary>Details</summary>
Motivation: 传统二手车底盘检查既费力又危险，且在线购车时，买家难以获取底盘图像，急需高效、自动化且高质量的解决方案来提升检测效率和透明度。

Method: 搭建三摄像头装置，记录车辆驶过时底盘的视频，利用精确校准、多流同步和机械先验，结合DISK特征提取和LightGlue匹配器，生成高质量稀疏点云，并用高斯溅射实现实时可视化的3D仿真底盘模型。

Result: 所提方法显著超越传统SfM流程，在广角畸变和低视差条件下生成高质量点云，为后续模型真实渲染提供基础。实验和消融研究证实各项设计选择对结果至关重要。

Conclusion: 本文系统极大提升了底盘检测效率和安全性，所生成的3D模型能实时交互，便于快速查找问题点，并提升买家信任度，为二手车交易提供了全新技术支持。

Abstract: Inspecting the undercarriage of used vehicles is a labor-intensive task that requires inspectors to crouch or crawl underneath each vehicle to thoroughly examine it. Additionally, online buyers rarely see undercarriage photos. We present an end-to-end pipeline that utilizes a three-camera rig to capture videos of the undercarriage as the vehicle drives over it, and produces an interactive 3D model of the undercarriage. The 3D model enables inspectors and customers to rotate, zoom, and slice through the undercarriage, allowing them to detect rust, leaks, or impact damage in seconds, thereby improving both workplace safety and buyer confidence. Our primary contribution is a rig-aware Structure-from-Motion (SfM) pipeline specifically designed to overcome the challenges of wide-angle lens distortion and low-parallax scenes. Our method overcomes the challenges of wide-angle lens distortion and low-parallax scenes by integrating precise camera calibration, synchronized video streams, and strong geometric priors from the camera rig. We use a constrained matching strategy with learned components, the DISK feature extractor, and the attention-based LightGlue matcher to generate high-quality sparse point clouds that are often unattainable with standard SfM pipelines. These point clouds seed the Gaussian splatting process to generate photorealistic undercarriage models that render in real-time. Our experiments and ablation studies demonstrate that our design choices are essential to achieve state-of-the-art quality.

</details>


### [261] [Soft Tail-dropping for Adaptive Visual Tokenization](https://arxiv.org/abs/2601.14246)
*Zeyuan Chen,Kai Zhang,Zhuowen Tu,Yuanjun Xiong*

Main category: cs.CV

TL;DR: 本文提出了一种名为STAT的自适应视觉分词器，可以根据图像复杂度自适应输出不定长度的tokens，用于提高视觉生成模型的表现。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉分词器通常为每张图片输出固定数量的tokens，难以适应不同图像复杂度，限制了生成模型的灵活性和效果。作者希望能根据图像内容的复杂度自适应调整token数量，提升模型生成质量并改善自回归视觉生成的扩展性。

Method: STAT会根据图像结构复杂度和细节水平自适应地为每张图像输出不同数量的离散tokens。在常规自编码器目标之外，作者对每个token的保留概率进行正则化，要求其在token序列中单调递减，并与全局图像复杂度度量相对齐，最终输出长度自适应的tokens，易于用于自回归视觉生成模型。

Result: 在ImageNet-1k数据集上，将STAT与标准自回归视觉生成模型结合后，在生成质量上与其他概率模型体系相比表现出色或更优，同时还展现了此前vanilla自回归视觉生成未能实现的良好可扩展性。

Conclusion: STAT方法能在保持高生成质量的同时，实现token长度的自适应，大幅提升自回归视觉生成模型的适用性和扩展性。

Abstract: We present Soft Tail-dropping Adaptive Tokenizer (STAT), a 1D discrete visual tokenizer that adaptively chooses the number of output tokens per image according to its structural complexity and level of detail. STAT encodes an image into a sequence of discrete codes together with per-token keep probabilities. Beyond standard autoencoder objectives, we regularize these keep probabilities to be monotonically decreasing along the sequence and explicitly align their distribution with an image-level complexity measure. As a result, STAT produces length-adaptive 1D visual tokens that are naturally compatible with causal 1D autoregressive (AR) visual generative models. On ImageNet-1k, equipping vanilla causal AR models with STAT yields competitive or superior visual generation quality compared to other probabilistic model families, while also exhibiting favorable scaling behavior that has been elusive in prior vanilla AR visual generation attempts.

</details>


### [262] [OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer](https://arxiv.org/abs/2601.14250)
*Pengze Zhang,Yanze Wu,Mengtian Li,Xu Bai,Songtao Zhao,Fulong Ye,Chong Mou,Xinghui Li,Zhuowei Chen,Qian He,Mingyuan Gao*

Main category: cs.CV

TL;DR: OmniTransfer 是一个统一的视频时空迁移框架，无需依赖静态图片或特定时序先验，显著提升了视频生成的灵活性与普适性。


<details>
  <summary>Details</summary>
Motivation: 现有视频定制方法大多依赖于参考图片或特定任务的时间先验，未能充分利用视频中丰富的时空信息，导致生成视频的灵活性和泛化能力受限。为解决这个问题，作者提出一种能够全面整合时空因素的新方法。

Method: 提出了 OmniTransfer 框架，包含三大核心设计：1）任务感知位置偏置，提升时序对齐及外观一致性；2）参考解耦因果学习，将参考分支与目标分支分离，提高参考信息迁移的精确性和计算效率；3）任务自适应多模态对齐，利用多模态信息动态区分并处理不同任务。

Result: 大量实验结果表明，OmniTransfer 在外观（身份与风格）和时序迁移（镜头运动、视频效果）任务上均优于现有方法，并在不依赖姿态的前提下，实现了与依赖姿态的方法相当的动作迁移表现。

Conclusion: OmniTransfer 框架为灵活、高保真视频生成提供了新范式，拓展了视频迁移的适用范围，同时提升了模型在多种迁移任务中的适应性与表现。

Abstract: Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.

</details>


### [263] [LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR](https://arxiv.org/abs/2601.14251)
*Said Taghadouini,Adrien Cavaillès,Baptiste Aubertin*

Main category: cs.CV

TL;DR: 本文提出了LightOnOCR-2-1B，一个拥有10亿参数的多语种视觉-语言模型，可直接将文档图像（如PDF）转化为整洁、按自然顺序排列的文本，无需传统OCR流程，并在多个基准测试上取得了领先性能，同时显著缩小了模型体积和提升了推理速度。


<details>
  <summary>Details</summary>
Motivation: 传统OCR流程存在诸多缺点，如处理多语种、复杂排版及图片等场景不够稳健，且OCR管道容易脆断，无法产生高质量、自然顺序的文本。作者希望构建一个更通用、准确和高效的解决方案，以满足多语言、大规模文档的识别与提取需求。

Method: 提出端到端的大规模多语种视觉-语言预训练模型LightOnOCR-2-1B，采用多样化高质量蒸馏数据集训练，覆盖了多语言文档、法国文献和科学PDF。模型不仅输出自然有序文本，还可以预测图中嵌入图片的标准化框位置，通过简历策略和基于IoU激励的RLVR方法进行定位训练。为提升鲁棒性，还引入了多检查点平均和任务算术融合手段。

Result: 该模型在OlmOCR-Bench数据集上实现了最新最优结果，相较此前最优模型，参数量缩小9倍、速度大幅提升。同时支持预测图像位置框，增强了模型实用性。

Conclusion: LightOnOCR-2-1B有效解决了传统OCR流程的脆弱性与局限性，在准确性、速度、模型规模等多方面有突破，适合多语种、复杂文档场景。模型及相关数据和评测工具已开源，有望推动文档智能识别领域发展。

Abstract: We present \textbf{LightOnOCR-2-1B}, a 1B-parameter end-to-end multilingual vision--language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9$\times$ smaller and substantially faster than prior best-performing models. We further extend the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via a resume strategy and refining it with RLVR using IoU-based rewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We release model checkpoints under Apache 2.0, and publicly release the dataset and \textbf{LightOnOCR-bbox-bench} evaluation under their respective licenses.

</details>


### [264] [Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis](https://arxiv.org/abs/2601.14253)
*Hongyuan Chen,Xingyu Chen,Youjia Zhang,Zexiang Xu,Anpei Chen*

Main category: cs.CV

TL;DR: 本文提出Motion 3-to-4框架，实现单目视频下高质量4D动态物体合成，结合静态3D形状生成和运动重建，相比前作具有更高保真度和时空一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管2D、视频和3D内容生成取得重大进展，但因训练数据有限和单目视角下运动与几何重建的模糊性，4D（时空一体）合成仍极具挑战。

Method: Motion 3-to-4将4D合成分为静态3D形状生成和运动重建两部分。利用规范化参考网格，模型学习紧凑运动潜变量并预测每帧顶点轨迹，实现完整且时间一致的几何重建。所用帧级变换器能适应不同长度序列，提高鲁棒性。

Result: 在权威基准和带有精确真值几何的新数据集上评估显示，Motion 3-to-4在保真度和空间一致性方面优于现有工作。

Conclusion: Motion 3-to-4有效解决了单目视频合成4D动态物体的难题，在生成质量和一致性上取得突破。

Abstract: We present Motion 3-to-4, a feed-forward framework for synthesising high-quality 4D dynamic objects from a single monocular video and an optional 3D reference mesh. While recent advances have significantly improved 2D, video, and 3D content generation, 4D synthesis remains difficult due to limited training data and the inherent ambiguity of recovering geometry and motion from a monocular viewpoint. Motion 3-to-4 addresses these challenges by decomposing 4D synthesis into static 3D shape generation and motion reconstruction. Using a canonical reference mesh, our model learns a compact motion latent representation and predicts per-frame vertex trajectories to recover complete, temporally coherent geometry. A scalable frame-wise transformer further enables robustness to varying sequence lengths. Evaluations on both standard benchmarks and a new dataset with accurate ground-truth geometry show that Motion 3-to-4 delivers superior fidelity and spatial consistency compared to prior work. Project page is available at https://motion3-to-4.github.io/.

</details>


### [265] [VideoMaMa: Mask-Guided Video Matting via Generative Prior](https://arxiv.org/abs/2601.14255)
*Sangbeom Lim,Seoung Wug Oh,Jiahui Huang,Heeji Yoon,Seungryong Kim,Joon-Young Lee*

Main category: cs.CV

TL;DR: 提出了一种新的视频抠像方法VideoMaMa，能将粗糙分割掩码转为精确 Alpha Matte，并展示了在真实视频上的零样本泛化能力，还基于该方法构建了大规模真实视频抠像数据集（MA-V），进一步提高了现有抠像模型的表现。


<details>
  <summary>Details</summary>
Motivation: 目前视频抠像领域因缺乏高质量标注数据，模型难以在真实场景下泛化。为解决数据稀缺带来的难题，作者希望设计一种能够利用有限甚至无标注数据实现泛化的新方法。

Method: 提出VideoMaMa方法，利用预训练的视频扩散模型，将粗粒度的分割掩码转为精细的 Alpha Matte。在仅用合成数据训练的基础上，开发了伪标签流程，生成大规模真实视频的高质量抠像标注，形成MA-V数据集。此外，通过用MA-V微调SAM2模型，并对比其在真实视频上的表现。

Result: VideoMaMa在仅用合成数据训练的情况下，实现了对真实视频的零样本泛化。基于VideoMaMa生成的MA-V大规模数据集提升了SAM2抠像模型在真实场景下的鲁棒性，效果超过在以往数据集上训练的同类模型。

Conclusion: 大规模的、以生成模型伪标注为基础的视频抠像数据集能够有效提升当前抠像模型在实际场景中的能力，扩散模型和可获取的分割信息能够大幅推动视频抠像研究的发展。

Abstract: Generalizing video matting models to real-world videos remains a significant challenge due to the scarcity of labeled data. To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models. VideoMaMa demonstrates strong zero-shot generalization to real-world footage, even though it is trained solely on synthetic data. Building on this capability, we develop a scalable pseudo-labeling pipeline for large-scale video matting and construct the Matting Anything in Video (MA-V) dataset, which offers high-quality matting annotations for more than 50K real-world videos spanning diverse scenes and motions. To validate the effectiveness of this dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which outperforms the same model trained on existing matting datasets in terms of robustness on in-the-wild videos. These findings emphasize the importance of large-scale pseudo-labeled video matting and showcase how generative priors and accessible segmentation cues can drive scalable progress in video matting research.

</details>


### [266] [Implicit Neural Representation Facilitates Unified Universal Vision Encoding](https://arxiv.org/abs/2601.14256)
*Matthew Gwilliam,Xiao Wang,Xuefeng Hu,Zhenheng Yang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的图像表征学习模型，首次实现了同时适用于识别和生成任务的统一架构。模型能够高效压缩图像为嵌入，同时支持高质量重建和下游视觉任务。


<details>
  <summary>Details</summary>
Motivation: 当前的图像表征学习方法分为关注识别或生成两类，缺乏兼顾两者的统一模型。作者希望设计一种模型，既能为分类、检测等识别任务提供有效表征，又能实现出色的图像生成。

Method: 提出将隐式神经表征（INR）和超网络结合，模型将图像映射为模型权重，实现快速、准确的图像重建。引入知识蒸馏进一步提升泛化和性能。

Result: 模型学到了性能优异、极度压缩的嵌入空间，在多项视觉任务上与SOTA结果竞争，同时具备高质量生成能力。

Conclusion: 本文模型为图像表征学习和生成架构提供了新的统一方案，同时在准确率和生成质量上展现出强大表现，推动了多任务表征学习的发展。

Abstract: Models for image representation learning are typically designed for either recognition or generation. Various forms of contrastive learning help models learn to convert images to embeddings that are useful for classification, detection, and segmentation. On the other hand, models can be trained to reconstruct images with pixel-wise, perceptual, and adversarial losses in order to learn a latent space that is useful for image generation. We seek to unify these two directions with a first-of-its-kind model that learns representations which are simultaneously useful for recognition and generation. We train our model as a hyper-network for implicit neural representation, which learns to map images to model weights for fast, accurate reconstruction. We further integrate our INR hyper-network with knowledge distillation to improve its generalization and performance. Beyond the novel training design, the model also learns an unprecedented compressed embedding space with outstanding performance for various visual tasks. The complete model competes with state-of-the-art results for image representation learning, while also enabling generative capabilities with its high-quality tiny embeddings. The code is available at https://github.com/tiktok/huvr.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [267] [Context Discipline and Performance Correlation: Analyzing LLM Performance and Quality Degradation Under Varying Context Lengths](https://arxiv.org/abs/2601.11564)
*Ahilan Ayyachamy Nadar Ponnusamy,Karthic Chandran,M Maruf Hossain*

Main category: cs.CL

TL;DR: 本论文研究在大模型不断扩展上下文窗口时，系统性能与模型质量之间的权衡，发现KV缓存随上下文长度增大导致非线性性能下降，并对MoE结构在大上下文下的特殊表现进行了分析。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型追求更长的上下文窗口以提升复杂任务处理能力，但这也带来了巨大的计算开销，亟需理解如何在确保模型质量的同时优化系统性能。

Method: 针对Llama-3.1-70B和Qwen1.5-14B等密集型Transformer结构，在输入中加入大量无关干扰内容，系统性测试并分析模型表现以及KV缓存对性能的影响；同时，深入研究了Mixture-of-Experts（MoE）结构在不同上下文长度下的表现差异。

Result: 实验表明，KV缓存随上下文长度加大，会引起非线性的性能下降；另一方面，MoE架构在大规模上下文下出现特殊行为，其原有优势因基础设施瓶颈被部分掩盖。

Conclusion: 单纯扩大上下文窗口会带来非常显著的系统性能影响，因此实际部署时需权衡模型架构和基础设施能力，莫把所有性能问题归咎于模型本身。同时，MoE架构需进一步优化其在长上下文环境下的表现。

Abstract: The scaling trend in Large Language Models (LLMs) has prioritized increasing the maximum context window to facilitate complex, long-form reasoning and document analysis. However, managing this expanded context introduces severe computational overhead. This paper investigates the critical trade-off between system performance and model quality when dense transformer architectures--specifically Llama-3.1-70B and Qwen1.5-14B--are exposed to large volumes of irrelevant and distracting context. The research identifies a non-linear performance degradation tied to the growth of the Key-Value (KV) cache. Furthermore, an extended analysis of the Mixture-of-Experts (MoE) architecture reveals unique behavioral anomalies at varying context scales, suggesting that architectural benefits may be masked by infrastructure bottlenecks at high token volumes.

</details>


### [268] [Compass-Embedding v4: Robust Contrastive Learning for Multilingual E-commerce Embeddings](https://arxiv.org/abs/2601.11565)
*Pakorn Ueareeworakul,Shuman Liu,Jinghao Feng,Ling Hu,Zhantang Shi,Chengqi Sun,Liang Yao,Panyi Ouyang,Haibo Zhang,Anxiang Zeng*

Main category: cs.CL

TL;DR: 该论文提出了一种专为东南亚电商场景设计的多语种高效嵌入框架Compass-Embedding v4，有效克服数据稀缺、监督噪声大和部署需求高等难题，并在多项基准测试上获得最佳表现。


<details>
  <summary>Details</summary>
Motivation: 东南亚等新兴市场电商快速发展，但该地区多语种、低资源的语言环境制约了检索、推荐等系统的表现，急需高质量的语义表征（嵌入模型）来提升相关能力。

Method: 1. 针对大批量对比训练出现的系统性负样本噪声，提出了类别感知掩码（CAM），优化InfoNCE损失。2. 通过上下文合成数据、跨语种翻译以及结构化电商数据等多手段，拓展多样化语料库以增强低资源语种的学习能力。3. 结合大批量训练与参数融合，解决遗忘问题；用高效推理方案（如vLLM和FP8量化）提升在线部署效率。

Result: 在多语种公开数据集以及电商专有任务上，Compass-Embedding v4在东南亚主要语种取得了最佳表现，在特定检索和分类任务上明显优于通用型嵌入模型，并在高资源语言上也表现优异。

Conclusion: Compass-Embedding v4突破了东南亚电商多语种、低资源语境下嵌入学习的关键瓶颈，兼顾高效率与精度，可推广于类似多语种、低资源、噪声较大的其他行业场景。

Abstract: As global e-commerce rapidly expands into emerging markets, the lack of high-quality semantic representations for low-resource languages has become a decisive bottleneck for retrieval, recommendation, and search systems. In this work, we present Compass-Embedding v4, a high-efficiency multilingual embedding framework specifically optimized for Southeast Asian (SEA) e-commerce scenarios, where data scarcity, noisy supervision, and strict production constraints jointly challenge representation learning. Compass-Embedding v4 addresses three core challenges. First, large-batch contrastive training under mixed task supervision introduces systematic false negatives that degrade semantic alignment. We propose Class-Aware Masking (CAM), a lightweight modification to the InfoNCE objective that suppresses invalid in-batch negatives and improves semantic discrimination without altering training efficiency. Second, low-resource SEA languages suffer from limited and uneven data coverage. We construct a diversified training corpus through context-grounded synthetic data generation, cross-lingual translation, and structured e-commerce data construction, enabling robust multilingual and domain-specific learning. Third, production deployment requires high-throughput inference while preserving embedding quality. We combine robustness-driven large-batch training with spherical model merging to mitigate catastrophic forgetting, and optimize inference via vLLM and FP8 quantization. Extensive evaluations across multilingual benchmarks and proprietary e-commerce tasks show that Compass-Embedding v4 achieves state-of-the-art performance on major SEA languages, significantly outperforming general-purpose embedding models in domain-specific retrieval and classification, while maintaining competitive performance on high-resource languages.

</details>


### [269] [Measuring Stability Beyond Accuracy in Small Open-Source Medical Large Language Models for Pediatric Endocrinology](https://arxiv.org/abs/2601.11567)
*Vanessa D'Amario,Randy Daniel,Alessandro Zanetti,Dhruv Edamadaka,Nitya Alaparthy,Joshua Tarkoff*

Main category: cs.CL

TL;DR: 本研究对六款小型开源医学大模型在儿科内分泌领域进行了多维度评估，发现模型输出的一致性并不代表正确性，并揭示了模型在实际应用中的多种不稳定性和偏差。


<details>
  <summary>Details</summary>
Motivation: 当前小型开源医学大模型（LLMs）虽然为资源有限环境和普及化应用带来机遇，但既往评估方法多局限于多选题准确率，缺乏对一致性、鲁棒性和推理能力的系统检验。因此，研究动机是希望通过更丰富的评测手段揭示这些模型在医学实践中的潜在局限。

Method: 本研究针对六款医学LLM，在儿科内分泌领域通过多选题（MCQ）、人工评估和临床专家复审，评测模型在确定性和随机性设定下输出的稳定性、一致性与正确性，并探索提示词变化、模型自评偏差及系统环境变化对输出的影响。

Result: HuatuoGPT-o1-8B取得最高成绩；一致性高并不意味着答案正确，而主要模型存在自评偏差和对解释顺序敏感。在临床专家审查下，错误推理有部分临床可接受，也有疏忽。系统级如CUDA版本的差异亦会使输出统计不同，提示模型评估重现性堪忧。

Conclusion: 研究强调小型医学LLM评测需超越准确率，关注一致性、鲁棒性和推理正确性，也提示这些模型在真实临床决策支持场景中存在输出变异等风险，需建立更广泛的诊断评估框架。

Abstract: Small open-source medical large language models (LLMs) offer promising opportunities for low-resource deployment and broader accessibility. However, their evaluation is often limited to accuracy on medical multiple choice question (MCQ) benchmarks, and lacks evaluation of consistency, robustness, or reasoning behavior. We use MCQ coupled to human evaluation and clinical review to assess six small open-source medical LLMs (HuatuoGPT-o1 (Chen 2024), Diabetica-7B, Diabetica-o1 (Wei 2024), Meditron3-8B (Sallinen2025), MedFound-7B (Liu 2025), and ClinicaGPT-base-zh (Wang 2023)) in pediatric endocrinology. In deterministic settings, we examine the effect of prompt variation on models' output and self-assessment bias. In stochastic settings, we evaluate output variability and investigate the relationship between consistency and correctness. HuatuoGPT-o1-8B achieved the highest performance. The results show that high consistency across the model response is not an indicator of correctness, although HuatuoGPT-o1-8B showed the highest consistency rate. When tasked with selecting correct reasoning, both HuatuoGPT-o1-8B and Diabetica-o1 exhibit self-assessment bias and dependency on the order of the candidate explanations. Expert review of incorrect reasoning rationales identified a mix of clinically acceptable responses and clinical oversight. We further show that system-level perturbations, such as differences in CUDA builds, can yield statistically significant shifts in model output despite stable accuracy. This work demonstrates that small, semantically negligible prompt perturbations lead to divergent outputs, raising concerns about reproducibility of LLM-based evaluations and highlights the output variability under different stochastic regimes, emphasizing the need of a broader diagnostic framework to understand potential pitfalls in real-world clinical decision support scenarios.

</details>


### [270] [An Empirical Analysis of Fine-Tuning Large Language Models on Bioinformatics Literature: PRSGPT and BioStarsGPT](https://arxiv.org/abs/2601.11573)
*Muhammad Muneeb,David B. Ascher*

Main category: cs.CL

TL;DR: 本文提出了一套可复现的大语言模型（LLM）在生物信息学领域的微调流程，并在PRS工具和社区论坛数据两个场景下进行了验证，显著提升了模型的专业表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型往往缺乏生物信息学等复杂领域的专门知识，无法满足细分领域的实际需求，因此亟需开发和完善专用微调流程。

Method: 设计了一个九步流程，包括多源数据整合、结构化预处理、基于提示的问题-答案生成、自然语言推断质控、语义去重、聚类分割数据及高效LoRA微调等；选取LLaMA-3.2-3B、Qwen2.5-7B和Gemma三种模型做微调并基准测试。

Result: Qwen2.5-7B在多项语义与词汇指标上表现最佳，对PRS提升BLEU-4和ROUGE-1达82%与70%，BioStars提升6%与18%；开源了大量高质量QA数据集，并在人类评测中，PRSGPT准确率与Google Gemini相当且回答细致，BioStarsGPT在生信问题有59%概念准确率。

Conclusion: 本研究流程可扩展到其他领域，为开发隐私保护、可本地部署的生信智能助手奠定了基础，对有领域知识需求的LLM落地有重要启示，也探讨了相关挑战和应对思路。

Abstract: Large language models (LLMs) often lack specialized knowledge for complex bioinformatics applications. We present a reproducible pipeline for fine-tuning LLMs on specialized bioinformatics data, demonstrated through two use cases: PRSGPT, focused on polygenic risk score (PRS) tools, and BioStarsGPT, trained on community forum discussions. The nine-step pipeline integrates diverse data sources, structured preprocessing, prompt-based question-answer (QA) generation (via Google Gemini), natural language inference (NLI) for quality control, semantic deduplication, clustering-based data splitting, and parameter-efficient fine-tuning using LoRA. We fine-tuned three LLMs (LLaMA-3.2-3B, Qwen2.5-7B, Gemma) and benchmarked them on over 14 lexical and semantic metrics. Qwen2.5-7B emerged as the best performer, with BLEU-4 and ROUGE-1 improvements of 82\% and 70\% for PRSGPT and 6\% and 18\% for BioStarsGPT, respectively. The open-source datasets produced include over 28,000 QA pairs for PRSGPT and 154,282 for BioStarsGPT. Human evaluation of PRSGPT yielded 61.9\% accuracy on the PRS tools comparison task, comparable to Google Gemini (61.4\%), but with richer methodological detail and accurate citations. BioStarsGPT demonstrated 59\% conceptual accuracy across 142 curated bioinformatics questions. Our pipeline enables scalable, domain-specific fine-tuning of LLMs. It enables privacy-preserving, locally deployable bioinformatics assistants, explores their practical applications, and addresses the challenges, limitations, and mitigation strategies associated with their development and use.

</details>


### [271] [Concept Attractors in LLMs and their Applications](https://arxiv.org/abs/2601.11575)
*Sotirios Panagiotis Chytas,Vikas Singh*

Main category: cs.CL

TL;DR: 本文发现大型语言模型（LLM）在某些层面会将语义相关但表面形式不同的输入映射到相似的内部表示，并解释了其原因。基于该原理，作者提出了无需训练、直接利用这些“吸引子（Attractor）”的新方法，可高效解决翻译、幻觉抑制、安全约束等任务。效果可媲美甚至超越传统微调方法。


<details>
  <summary>Details</summary>
Motivation: 以往LLM需要针对不同任务进行专门的微调，既耗时又成本高。该文关注于模型内部结构，试图通过理解其隐藏层的表示方式，设计出跨任务且高效的泛用算法。

Method: 作者将LLM的隐藏层映射用迭代函数系统（IFS）中的“吸引子”理论解释，提出了无需额外训练、直接对吸引子进行处理的通用方法。这些方法被用于翻译、降低模型幻觉、实施安全限制、合成数据生成等多种实际任务。

Result: 所提基于Attractor的干预方法，不需训练就能胜任多种任务，并且在效果上与专业的微调模型相当甚至更优，尤其适用于微调方法表现不佳时。

Conclusion: 利用模型内部的Attractor结构，可以实现高效、通用的任务解决方案，为传统的重训练和微调方法提供了更具性价比的替代方案。

Abstract: Large language models (LLMs) often map semantically related prompts to similar internal representations at specific layers, even when their surface forms differ widely. We show that this behavior can be explained through Iterated Function Systems (IFS), where layers act as contractive mappings toward concept-specific Attractors. We leverage this insight and develop simple, training-free methods that operate directly on these Attractors to solve a wide range of practical tasks, including language translation, hallucination reduction, guardrailing, and synthetic data generation. Despite their simplicity, these Attractor-based interventions match or exceed specialized baselines, offering an efficient alternative to heavy fine-tuning, generalizable in scenarios where baselines underperform.

</details>


### [272] [LimAgents: Multi-Agent LLMs for Generating Research Limitations](https://arxiv.org/abs/2601.11578)
*Ibrahim Al Azher,Zhishuai Guo,Hamed Alhoori*

Main category: cs.CL

TL;DR: LimAgents提出多智能体大模型框架，有效提升学术论文局限性识别的质量和覆盖率，超越传统零样本LLMs。


<details>
  <summary>Details</summary>
Motivation: 现有零样本大语言模型（LLMs）自动生成论文局限性时，内容通常流于表面或空泛，且多数只是复述作者自述，难以深挖方法论或情境上的深层问题。传统文献中作者自报局限往往不充分，缺乏深度和外部视角。针对这一痛点，亟需更系统、更深入、能够利用多源信息的自动化识别方案。

Method: 提出LimAgents多智能体LLM框架。不同智能体分工协作：部分抽取作者和评审意见中的直接局限，部分专注于方法学漏洞分析；设有模拟评审人和引用文献分析角度的智能体；Judge智能体负责汇总和校正，多智能体最终由Master智能体整合输出。利用OpenReview评论、作者自述、被引用及引用文献等多源数据，多通道识别显性与隐性局限。评价标准也创新地引入LLM-as-a-Judge点式评估协议，更关注语义覆盖而不仅仅是n-gram重叠。

Result: 实验表明，LimAgents显著优于传统零样本LLM，在局限性识别覆盖率上，RAG+多智能体GPT-4o较基线提升15.51%，Llama 3 8B 多智能体配置提升4.41%。

Conclusion: LimAgents作为融合多角色合作和多源信息的自动化工具，可系统、深入、语义化地识别学术论文局限性，显著提升识别质量，并推动学术透明度和审稿规范化。

Abstract: Identifying and articulating limitations is essential for transparent and rigorous scientific research. However, zero-shot large language models (LLMs) approach often produce superficial or general limitation statements (e.g., dataset bias or generalizability). They usually repeat limitations reported by authors without looking at deeper methodological issues and contextual gaps. This problem is made worse because many authors disclose only partial or trivial limitations. We propose LimAgents, a multi-agent LLM framework for generating substantive limitations. LimAgents integrates OpenReview comments and author-stated limitations to provide stronger ground truth. It also uses cited and citing papers to capture broader contextual weaknesses. In this setup, different agents have specific roles as sequential role: some extract explicit limitations, others analyze methodological gaps, some simulate the viewpoint of a peer reviewer, and a citation agent places the work within the larger body of literature. A Judge agent refines their outputs, and a Master agent consolidates them into a clear set. This structure allows for systematic identification of explicit, implicit, peer review-focused, and literature-informed limitations. Moreover, traditional NLP metrics like BLEU, ROUGE, and cosine similarity rely heavily on n-gram or embedding overlap. They often overlook semantically similar limitations. To address this, we introduce a pointwise evaluation protocol that uses an LLM-as-a-Judge to measure coverage more accurately. Experiments show that LimAgents substantially improve performance. The RAG + multi-agent GPT-4o mini configuration achieves a +15.51% coverage gain over zero-shot baselines, while the Llama 3 8B multi-agent setup yields a +4.41% improvement.

</details>


### [273] [Bielik 11B v3: Multilingual Large Language Model for European Languages](https://arxiv.org/abs/2601.11579)
*Krzysztof Ociepa,Łukasz Flis,Remigiusz Kinas,Krzysztof Wróbel,Adrian Gwoździej*

Main category: cs.CL

TL;DR: Bielik 11B v3是一款针对波兰语高度优化的语言模型，性能卓越，在多项任务中超越了同类及更大型模型。


<details>
  <summary>Details</summary>
Motivation: 当前波兰语及其他低资源欧洲语言的AI模型能力有限，缺乏高效能、资源利用率高的解决方案，亟需专为这些语言优化的新型模型。

Method: 模型基于Mistral 7B v0.2架构，扩展至11B参数，采用四阶段训练流程：持续预训练、监督微调（SFT）、直接偏好优化（DPO）和强化学习。此外，模型进行了参数高效以及量化优化，以适应多样硬件环境。

Result: Bielik 11B v3在波兰语相关任务上表现显著优于其他专用模型，并超越了许多参数规模为其2-6倍的大型模型，覆盖从基础语言理解到复杂推理的广泛任务。

Conclusion: 该模型不仅极大推动了波兰语AI能力，也为低资源语言开发高效、高性能模型树立了新标杆。

Abstract: We present Bielik 11B v3, a state-of-the-art language model highly optimized for the Polish language, while also maintaining strong capabilities in other European languages. This model extends the Mistral 7B v0.2 architecture, scaled to 11B parameters via depth up-scaling. Its development involved a comprehensive four-stage training pipeline: continuous pre-training, supervised fine-tuning (SFT), Direct Preference Optimization (DPO), and reinforcement learning.
  Comprehensive evaluations demonstrate that Bielik 11B v3 achieves exceptional performance. It significantly surpasses other specialized Polish language models and outperforms many larger models (with 2-6 times more parameters) on a wide range of tasks, from basic linguistic understanding to complex reasoning.
  The model's parameter efficiency, combined with extensive quantization options, allows for effective deployment across diverse hardware configurations. Bielik 11B v3 not only advances AI capabilities for the Polish language but also establishes a new benchmark for developing resource-efficient, high-performance models for less-represented languages.

</details>


### [274] [Speculative Decoding: Performance or Illusion?](https://arxiv.org/abs/2601.11580)
*Xiaoxuan Liu,Jiaxiang Yu,Jongseok Park,Ion Stoica,Alvin Cheung*

Main category: cs.CL

TL;DR: 本论文系统研究了推测解码（SD）在大语言模型真实推理系统中的加速效果，分析了其实际性能与理论上限之间的差距。


<details>
  <summary>Details</summary>
Motivation: 尽管推测解码在加速大语言模型推理中很受关注，但以往评测多基于原型系统且批量规模不现实，缺乏在真实大规模推理引擎下的系统性评估。因此有必要揭示SD在实际部署下的真实性能及影响因素。

Method: 论文选用生产级主流推理引擎vLLM，评估多种SD变体（如n-gram、EAGLE/EAGLE-3、Draft-Model、多token预测），在多种工作负载、模型规模与批量下进行系统实验。详细分析了SD性能主导因素，并量化了理论最大加速比。

Result: 实验证明目标模型的验证阶段（verification）耗时最多；不同输出token、请求和数据集，其接受长度变化很大。实际SD性能与理论上界之间存在较大差距。

Conclusion: 实际推理中的推测解码加速受多种因素限制，与理论加速上限仍有显著差距。研究揭示了SD性能瓶颈，为未来提升推测解码效率提供了新方向。

Abstract: Speculative decoding (SD) has become a popular technique to accelerate Large Language Model (LLM) inference, yet its real-world effectiveness remains unclear as prior evaluations rely on research prototypes and unrealistically small batch sizes. We present, to our knowledge, the first systematic study of SD on a production-grade and widely deployed inference engine (vLLM), covering multiple SD variants ($n$-gram, EAGLE/EAGLE-3, Draft-Model, Multi-Token Prediction) across diverse workloads, model scales, and batch sizes. We analyze key factors governing SD performance, and quantify a theoretical upper bound on SD speedup. Our results show that verification by the target model dominates the execution, while acceptance length varies markedly across output token positions, requests, and datasets. Comparing measured performance with theoretical bounds reveals substantial gaps between observed and theoretical upper bounds, and we leverage this observation to highlight new research opportunities that our study opens up in improving SD.

</details>


### [275] [Enhancing the QA Model through a Multi-domain Debiasing Framework](https://arxiv.org/abs/2601.11581)
*Yuefeng Wang,ChangJae Lee*

Main category: cs.CL

TL;DR: 本研究通过多领域去偏框架，有效提升了ELECTRA-small在SQuAD和对抗性数据集上的问答性能，尤其在复杂和对抗条件下表现更佳。


<details>
  <summary>Details</summary>
Motivation: 当前机器阅读理解中的问答模型虽然取得了显著进展，但仍存在词汇偏见、数值推理和实体识别等方面的偏差，尤其在对抗环境中影响模型表现。因此，研究有针对性的去偏策略以提高模型健壮性非常有必要。

Method: 本文以ELECTRA-small模型为基础，在SQuAD v1.1及两个对抗性数据集AddSent和AddOneSent上进行评估。通过分析模型在词汇偏见、数值推理与实体识别上的错误，提出结合知识蒸馏、去偏技术和领域扩展的多领域去偏框架。

Result: 应用多领域去偏框架后，ELECTRA-small在所有测试集上的EM和F1指标提升了最多2.6个百分点，尤其在对抗性数据集上获得显著增益。

Conclusion: 有针对性的去偏策略显著增强了问答系统在自然语言理解任务中的健壮性与可靠性，未来可以推广至更多实际应用场景。

Abstract: Question-answering (QA) models have advanced significantly in machine reading comprehension but often exhibit biases that hinder their performance, particularly with complex queries in adversarial conditions. This study evaluates the ELECTRA-small model on the Stanford Question Answering Dataset (SQuAD) v1.1 and adversarial datasets AddSent and AddOneSent. By identifying errors related to lexical bias, numerical reasoning, and entity recognition, we develop a multi-domain debiasing framework incorporating knowledge distillation, debiasing techniques, and domain expansion. Our results demonstrate up to 2.6 percentage point improvements in Exact Match (EM) and F1 scores across all test sets, with gains in adversarial contexts. These findings highlight the potential of targeted bias mitigation strategies to enhance the robustness and reliability of natural language understanding systems.

</details>


### [276] [Entropic Context Shaping: Information-Theoretic Filtering for Context-Aware LLM Agents](https://arxiv.org/abs/2601.11585)
*Hyunjun Kim*

Main category: cs.CL

TL;DR: 该论文提出了一种名为Entropic Context Shaping (ECS)的信息论方法，用于提升大语言模型选取有效上下文的能力，其表现显著优于传统的词汇相似性方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在利用多轮对话或长文本时，难以区分有用信息与无关干扰，导致推理性能下降。现有方法多依赖词汇重叠，无法真实衡量上下文对任务的实际帮助，因此需要一种能捕捉语用效用的新方法。

Method: 作者提出ECS框架，从信息论视角衡量给定上下文对模型答案分布的改变，以“答案概率变化”衡量效用。通过理论分析，证明与任务无关的上下文不会显著改变答案分布。实验在LongMemEval和LoCoMo数据集上，对多轮上下文精细选择任务进行评估。

Result: ECS在turn-level精细选择任务上，使用Llama-3.1-8B模型获得F1=0.265，比传统的TF-IDF方法(F1=0.154)提高71.83%。

Conclusion: ECS可以有效提升上下文筛选的精度，优于基于词汇重叠的方法，尤其在需要精准筛选上下文时效果显著。代码与数据公开，便于复现和扩展。

Abstract: Context engineering for large language model (LLM) agents requires distinguishing pragmatically useful information from misleading distractors. We introduce Entropic Context Shaping (ECS), an information-theoretic framework that measures context utility via the shift in the model's answer distribution toward the correct answer. Unlike lexical similarity methods that rely on word overlap, ECS captures pragmatic utility -- whether a passage actually helps answer the question. We formalize utility as the signed change in answer probability and provide theoretical analysis showing that task-irrelevant updates yield near-zero distribution shift. We evaluate on multi-turn context selection tasks using LongMemEval (session-level) and LoCoMo (turn-level) benchmarks. On fine-grained turn selection, ECS with Llama-3.1-8B achieves F1=0.265, a 71.83% relative improvement over TF-IDF (F1=0.154), demonstrating that pragmatic utility outperforms lexical similarity when precise context selection matters. Code and data are available in the supplementary materials.

</details>


### [277] [Towards AGI A Pragmatic Approach Towards Self Evolving Agent](https://arxiv.org/abs/2601.11658)
*Indrajit Kar,Sammy Zonunpuia,Zonunfeli Ralte*

Main category: cs.CL

TL;DR: 本文提出了一种分层自进化多智能体框架，使大语言模型在部署后能够实现自主能力扩展、工具生成和推理演进。通过多种演化方法训练，实验表明进化后的智能体在多个任务上表现优于原始模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型部署后能力固定，缺乏自我进化和自我扩展的能力，限制了持续适应和复杂任务的解决。本文旨在赋予LLM持续自我改进和工具创造的能力。

Method: 引入了包括基础LLM、操作智能体（SLM）、代码生成LLM和教师LLM的多层次体系。任务失败时依次尝试工具生成、课程学习、奖励驱动学习和遗传算法，促使智能体进化并适应更复杂任务。所有实验在包含层级与难度任务的TaskCraft数据集上进行。

Result: 课程学习在快速恢复及泛化能力上表现突出；奖励驱动学习适合高难度任务；遗传算法获得多样化行为。在所有设置下，经过上述方法进化后的智能体都超越了原始智能体。

Conclusion: 提出的自进化框架使LLM型智能体在部署后能持续自我完善与适应，表明自主演进机制有效提升了智能体的任务表现和能力多样性。

Abstract: Large Language Model (LLM) based agents are powerful yet fundamentally static after deployment, lacking the ability to autonomously expand capabilities, generate new tools, or evolve their reasoning. This work introduces a hierarchical self-evolving multi-agent framework that integrates a Base LLM, an operational SLM agent, a Code-Generation LLM, and a Teacher-LLM to enable continuous adaptation. The workflow begins with the agent attempting a task using reasoning and existing tools; if unsuccessful, it escalates to tool synthesis through the Code-Gen LLM, and when failures persist, it triggers an evolution phase using Curriculum Learning (CL), Reward-Based Learning (RL), or Genetic Algorithm (GA) evolution. Using the TaskCraft dataset rich in hierarchical tasks, tool-use traces, and difficulty scaling we evaluate these paradigms. CL delivers fast recovery and strong generalization, RL excels on high-difficulty tasks, and GA offers high behavioral diversity. Across all settings, evolved agents outperform their originals, demonstrating robust, autonomous, self-improving agentic evolution.

</details>


### [278] [RAC: Retrieval-Augmented Clarification for Faithful Conversational Search](https://arxiv.org/abs/2601.11722)
*Ahmed Rayane Kebir,Vincent Guigue,Lynda Said Lhadj,Laure Soulier*

Main category: cs.CL

TL;DR: 本文提出了一种以检索为基础的生成澄清问题框架RAC，旨在保证澄清问题能够被底层语料真实支持，提升对话式检索系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前的对话检索系统生成澄清问题时，关注点大多在流畅性或与用户意图的一致性，较少考虑这些问题是否能被底层语料库回答。如果问题没有语料支撑，会出现无效或无法回答的问题，影响用户体验和系统实用性。

Method: 提出Retrieval-Augmented Clarification (RAC) 框架，并比较不同的检索建索方式。将大语言模型微调使其能最大限度利用检索到的上下文生成基于证据的问题，并通过对比偏好优化，优先生成有检索证据支撑的问题。新引入基于NLI和数据到文本的评价指标，衡量问题与语料库的锚定程度。

Result: 在四项基准测试上，RAC相较于现有方法显著提升了澄清问题的语料库忠实度。无论是LLM评审还是新提出的指标，均显示RAC生成的问题更加有据可依。

Conclusion: RAC框架能够提升澄清问题的语料支持性和整体可靠性，改进了对话检索系统生成澄清问题的质量，为更有效的用户交互和信息检索奠定基础。

Abstract: Clarification questions help conversational search systems resolve ambiguous or underspecified user queries. While prior work has focused on fluency and alignment with user intent, especially through facet extraction, much less attention has been paid to grounding clarifications in the underlying corpus. Without such grounding, systems risk asking questions that cannot be answered from the available documents. We introduce RAC (Retrieval-Augmented Clarification), a framework for generating corpus-faithful clarification questions. After comparing several indexing strategies for retrieval, we fine-tune a large language model to make optimal use of research context and to encourage the generation of evidence-based question. We then apply contrastive preference optimization to favor questions supported by retrieved passages over ungrounded alternatives. Evaluated on four benchmarks, RAC demonstrate significant improvements over baselines. In addition to LLM-as-Judge assessments, we introduce novel metrics derived from NLI and data-to-text to assess how well questions are anchored in the context, and we demonstrate that our approach consistently enhances faithfulness.

</details>


### [279] [Bridging Human Interpretation and Machine Representation: A Landscape of Qualitative Data Analysis in the LLM Era](https://arxiv.org/abs/2601.11739)
*Xinyu Pi,Qisen Yang,Chuong Nguyen,Hua Shen*

Main category: cs.CL

TL;DR: 本文提出了一个4×4的分析框架，用于明确当前LLM在支持定性研究中输出的差异，并指出现有方法多集中在低层次的意义和建模上，缺乏对更高阶解释和动态建模的尝试。


<details>
  <summary>Details</summary>
Motivation: 尽管大模型（LLM）被越来越多地用于支持定性研究，其输出类型分布却极为广泛，从简单摘要到复杂理论解释，但现有工具对这些差异缺乏清晰刻画。因此，作者希望明确类别，揭示当前研究的不足，为未来系统开发提供方向。

Method: 作者提出了一个4×4的分析矩阵，将“意义建构”分为描述、分类、解释和理论四个层级，与“建模方式”中的静态结构、阶段/时间线、因果路径和反馈动态四个层级组合。用该矩阵分析LLM自动化已有成果，评估其分布和不足。

Result: 分析发现，大多数现有LLM输出集中在较低的意义和建模层级上（如静态、描述性输出），针对高阶解释和动态建模的工作很稀缺且可靠性不足。

Conclusion: 针对高阶意义建构和复杂建模领域的空白，作者建议开发能明确与可控建模/解释承诺的LLM系统。为未来定性研究自动化树立了路线图。

Abstract: LLMs are increasingly used to support qualitative research, yet existing systems produce outputs that vary widely--from trace-faithful summaries to theory-mediated explanations and system models. To make these differences explicit, we introduce a 4$\times$4 landscape crossing four levels of meaning-making (descriptive, categorical, interpretive, theoretical) with four levels of modeling (static structure, stages/timelines, causal pathways, feedback dynamics). Applying the landscape to prior LLM-based automation highlights a strong skew toward low-level meaning and low-commitment representations, with few reliable attempts at interpretive/theoretical inference or dynamical modeling. Based on the revealed gap, we outline an agenda for applying and building LLM-systems that make their interpretive and modeling commitments explicit, selectable, and governable.

</details>


### [280] [LIME-LLM: Probing Models with Fluent Counterfactuals, Not Broken Text](https://arxiv.org/abs/2601.11746)
*George Mihaila,Suleyman Olcay Polat,Poli Nemkova,Himanshu Sharma,Namratha V. Urs,Mark V. Albert*

Main category: cs.CL

TL;DR: 本文提出LIME-LLM，一种通过受控假设驱动扰动替代传统随机掩码的方法，显著提升了NLP模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的NLP局部可解释方法（如LIME）依赖随机掩码采样，常导致语义无效、分布外的输入，从而影响模型解释的可靠性。近期虽然有基于大模型的生成方法，但其无约束改写也引入混杂变量，难以精确解释各特征影响。

Method: LIME-LLM提出用“单掩码-单采样”的协议和中性填充、边界填充策略，构造符合语料分布且严格控制变量的扰动样本，确保每次扰动仅有一个特征被改变。对比实验使用LIME、SHAP、Integrated Gradients、以及生成式LLiMe为基线，并基于CoLA、SST-2、HateXplain三大数据集，以人工标注理由作为评价依据进行评测。

Result: LIME-LLM在三个不同任务和数据集上，均相较于传统扰动方法和生成式方法（如LLiMe）大幅提升了可解释性精度，设立了新的黑盒NLP可解释性基准。

Conclusion: LIME-LLM证明通过假设驱动、精密控制的扰动，可以显著增强NLP模型本地可解释性的准确性和可靠性，优于现有主流方法，推动了可解释AI技术进步。

Abstract: Local explanation methods such as LIME (Ribeiro et al., 2016) remain fundamental to trustworthy AI, yet their application to NLP is limited by a reliance on random token masking. These heuristic perturbations frequently generate semantically invalid, out-of-distribution inputs that weaken the fidelity of local surrogate models. While recent generative approaches such as LLiMe (Angiulli et al., 2025b) attempt to mitigate this by employing Large Language Models for neighborhood generation, they rely on unconstrained paraphrasing that introduces confounding variables, making it difficult to isolate specific feature contributions. We introduce LIME-LLM, a framework that replaces random noise with hypothesis-driven, controlled perturbations. By enforcing a strict "Single Mask-Single Sample" protocol and employing distinct neutral infill and boundary infill strategies, LIME-LLM constructs fluent, on-manifold neighborhoods that rigorously isolate feature effects. We evaluate our method against established baselines (LIME, SHAP, Integrated Gradients) and the generative LLiMe baseline across three diverse benchmarks: CoLA, SST-2, and HateXplain using human-annotated rationales as ground truth. Empirical results demonstrate that LIME-LLM establishes a new benchmark for black-box NLP explainability, achieving significant improvements in local explanation fidelity compared to both traditional perturbation-based methods and recent generative alternatives.

</details>


### [281] [Early Linguistic Pattern of Anxiety from Social Media Using Interpretable Linguistic Features: A Multi-Faceted Validation Study with Author-Disjoint Evaluation](https://arxiv.org/abs/2601.11758)
*Arnab Das Utsa*

Main category: cs.CL

TL;DR: 本文提出了一种透明且可解释的社交媒体文本焦虑检测方法，并通过大量Reddit数据进行训练与多层次验证，结果表明该方法准确、稳健且具可推广性。


<details>
  <summary>Details</summary>
Motivation: 焦虑影响全球大量人群，但大规模筛查难度较大。社交媒体语言为焦虑检测带来新机遇，但现有模型多缺乏可解释性、关键词鲁棒性验证和严格的数据完整性分析，因此亟需开发更透明、可靠的方法。

Method: 作者收集大量Reddit帖子，基于带标签的子论坛训练逻辑回归分类器，并通过特征消融、关键词遮蔽、群体密度分析及与临床访谈数据的外部验证，全方位评估模型性能与泛化能力。

Result: 模型即使在移除情感和屏蔽关键词后，依旧保持较高准确率。在少量帖子时能早期有效检测焦虑，跨领域测试表现良好，且与临床访谈结果高度一致。

Conclusion: 基于可解释语言特征的模型不仅易于复现和推广，还可实现稳健、可靠、具泛化性的在线焦虑筛查。此方法为多元网络环境下的心理健康评估奠定了基础和标准。

Abstract: Anxiety affects hundreds of millions of individuals globally, yet large-scale screening remains limited. Social media language provides an opportunity for scalable detection, but current models often lack interpretability, keyword-robustness validation, and rigorous user-level data integrity. This work presents a transparent approach to social media-based anxiety detection through linguistically interpretable feature-grounded modeling and cross-domain validation. Using a substantial dataset of Reddit posts, we trained a logistic regression classifier on carefully curated subreddits for training, validation, and test splits. Comprehensive evaluation included feature ablation, keyword masking experiments, and varying-density difference analyses comparing anxious and control groups, along with external validation using clinically interviewed participants with diagnosed anxiety disorders. The model achieved strong performance while maintaining high accuracy even after sentiment removal or keyword masking. Early detection using minimal post history significantly outperformed random classification, and cross-domain analysis demonstrated strong consistency with clinical interview data. Results indicate that transparent linguistic features can support reliable, generalizable, and keyword-robust anxiety detection. The proposed framework provides a reproducible baseline for interpretable mental health screening across diverse online contexts.

</details>


### [282] [Industry-Aligned Granular Topic Modeling](https://arxiv.org/abs/2601.11762)
*Sae Young Moon,Myeongjun Erik Jang,Haoyan Luo,Chunyang Xiao,Antonios Georgiadis,Fran Silavong*

Main category: cs.CL

TL;DR: 本文提出了一种基于大型语言模型（LLMs）的细粒度主题建模框架TIDE，专为提升文本挖掘中的主题粒度和业务实用性。


<details>
  <summary>Details</summary>
Motivation: 当前主题建模方法在提取细颗粒度主题方面能力有限，而主题粒度对于业务深入分析具有重要意义，因此需要更强大的工具。

Method: 提出了TIDE框架，核心是利用大型语言模型实现细粒度主题建模，并集成了长文档摘要、主题层次化、知识蒸馏等辅助功能，适用于不同的业务场景。

Result: 在多个公开和真实业务数据集上的实验表明，TIDE在主题建模效果上优于现有主流方法，辅助功能也有效支持工业级业务需求。

Conclusion: TIDE不仅在主题建模粒度和表现上取得领先，也为复杂业务场景提供了有价值的支撑，目前正推动开源化以促进实际应用。

Abstract: Topic modeling has extensive applications in text mining and data analysis across various industrial sectors. Although the concept of granularity holds significant value for business applications by providing deeper insights, the capability of topic modeling methods to produce granular topics has not been thoroughly explored. In this context, this paper introduces a framework called TIDE, which primarily provides a novel granular topic modeling method based on large language models (LLMs) as a core feature, along with other useful functionalities for business applications, such as summarizing long documents, topic parenting, and distillation. Through extensive experiments on a variety of public and real-world business datasets, we demonstrate that TIDE's topic modeling approach outperforms modern topic modeling methods, and our auxiliary components provide valuable support for dealing with industrial business scenarios. The TIDE framework is currently undergoing the process of being open sourced.

</details>


### [283] [Cleansing the Artificial Mind: A Self-Reflective Detoxification Framework for Large Language Models](https://arxiv.org/abs/2601.11776)
*Kaituo Zhang,Zhimeng Jiang,Na Zou*

Main category: cs.CL

TL;DR: 本文提出了一种完全自我反思的LLM去毒化框架，利用模型自身能力检测和纠正有害内容，无需外部模块或人工标注，实验表明效果超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型生成能力强，但仍可能产生有害内容，现有去毒方法多依赖人工标注或外部模块，影响扩展性和一致性。为此，作者探索能否仅依靠LLM内在机制实现高效去毒。

Method: 提出了一种自反思去毒框架。框架设计了内部有毒信号检测器，结合系统化干预流程，将有害文本转换为非有害文本。此过程迭代生成对比去毒数据集，用于微调模型，提升安全生成能力，无需人工参与或外部组件。

Result: 在DetoxLLM、ParaDetox等基准数据集上，方法在去毒效果及语义保持上均优于最先进方法。

Conclusion: 展示了大语言模型内在自我去毒能力，为减少有害内容的生成提供了一种高效且一致的途径，有助于推动更负责任、伦理的文本生成系统发展。

Abstract: Recent breakthroughs in Large Language Models (LLMs) have revealed remarkable generative capabilities and emerging self-regulatory mechanisms, including self-correction and self-rewarding. However, current detoxification techniques rarely exploit these built-in abilities; instead, they rely on external modules, labor-intensive data annotation, or human intervention --factors that hinder scalability and consistency. In this paper, we introduce a fully self-reflective detoxification framework that harnesses the inherent capacities of LLMs to detect, correct toxic content, and refine LLMs without external modules and data annotation. Specifically, we propose a Toxic Signal Detector --an internal self-identification mechanism, coupled with a systematic intervention process to transform toxic text into its non-toxic counterpart. This iterative procedure yields a contrastive detoxification dataset used to fine-tune the model, enhancing its ability for safe and coherent text generation. Experiments on benchmark datasets such as DetoxLLM and ParaDetox show that our method achieves better detoxification performance than state-of-the-art methods while preserving semantic fidelity. By obviating the need for human intervention or external components, this paper reveals the intrinsic self-detoxification ability of LLMs, offering a consistent and effective approach for mitigating harmful content generation. Ultimately, our findings underscore the potential for truly self-regulated language models, paving the way for more responsible and ethically guided text generation systems.

</details>


### [284] [Translation as a Scalable Proxy for Multilingual Evaluation](https://arxiv.org/abs/2601.11778)
*Sheriff Issaka,Erick Rosas Gonzalez,Lieqi Liu,Evans Kofi Agyei,Lucas Bandarkar,Nanyun Peng,David Ifeoluwa Adelani,Francisco Guzmán,Saadia Gabriel*

Main category: cs.CL

TL;DR: 论文探讨了在多语言大模型评估中，是否仅用翻译质量即可有效反映模型在多语言任务上的整体能力。结果表明，翻译表现和多语言下游任务表现高度相关，翻译质量可作为低成本、多语言模型能力的初筛手段。


<details>
  <summary>Details</summary>
Motivation: 目前可用的、非机器翻译生成的多语言基准测试覆盖的语言极少（不到30种），而全球约有7000种语言，意味着绝大多数语言在大模型性能评测中处于空白状态。传统基准测试的构建受限于高昂成本、专家资源稀缺及数据污染等问题，需要更简便可扩展的替代方案。

Method: 作者通过系统性评估，选取14个不同参数规模（1B-72B）的主流大模型，在9个多样化基准和7种翻译评测指标下，考查翻译质量与下游多语言任务表现之间的相关性。

Result: 实证发现翻译表现与下游多语言理解任务有高度相关性（以Phi-4模型为例，不同翻译评分指标的Pearson相关系数均接近0.9）。表明翻译能力与多语言综合理解能力在表示层面存在显著重叠。

Conclusion: 论文结论为：翻译质量是衡量大模型多语言能力的有力、低成本的代理指标，可作为预筛手段，对于特定任务再做有针对性的补充评估。

Abstract: The rapid proliferation of LLMs has created a critical evaluation paradox: while LLMs claim multilingual proficiency, comprehensive non-machine-translated benchmarks exist for fewer than 30 languages, leaving >98% of the world's 7,000 languages in an empirical void. Traditional benchmark construction faces scaling challenges such as cost, scarcity of domain experts, and data contamination. We evaluate the validity of a simpler alternative: can translation quality alone indicate a model's broader multilingual capabilities? Through systematic evaluation of 14 models (1B-72B parameters) across 9 diverse benchmarks and 7 translation metrics, we find that translation performance is a good indicator of downstream task success (e.g., Phi-4, median Pearson r: MetricX = 0.89, xCOMET = 0.91, SSA-COMET = 0.87). These results suggest that the representational abilities supporting faithful translation overlap with those required for multilingual understanding. Translation quality, thus emerges as a strong, inexpensive first-pass proxy of multilingual performance, enabling a translation-first screening with targeted follow-up for specific tasks.

</details>


### [285] [Beyond Tokens: Concept-Level Training Objectives for LLMs](https://arxiv.org/abs/2601.11791)
*Laya Iyer,Pranav Somani,Alice Guo,Dan Jurafsky,Chen Shani*

Main category: cs.CL

TL;DR: 本文提出将大型语言模型（LLM）的训练目标从传统的下一个token预测（NTP）提升到概念级预测，通过概念归类同义词来更贴近语义，改善模型泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: NTP以token为单位训练LLM，对表面形式的多样性（如“mom”和“mother”）不能容忍，将其视为错误，从而忽略语义等价或概念抽象，导致模型更关注形式而非意义。作者因此希望改进训练目标，使之更符合语义正确性，减少对表层形式的偏见。

Method: 提出概念级预测目标，将多个意思相同但表述不同的token归为同一概念（如“mom”、“mother”归为‘MOTHER’），并将这种概念层级的监督信号整合到LLM训练方法中。探索并验证多种概念监督集成方式。

Result: 实验发现，引入概念级监督的模型在困惑度降低、域外鲁棒性提升，并且在多种NLP基准测试上表现优于传统NTP训练的模型。

Conclusion: 概念级监督比token级NTP信号更好地对齐了LLM与人类语义抽象方式，有望成为更优的训练范式。

Abstract: The next-token prediction (NTP) objective has been foundational in the development of modern large language models (LLMs), driving advances in fluency and generalization. However, NTP operates at the \textit{token} level, treating deviations from a single reference continuation as errors even when alternative continuations are equally plausible or semantically equivalent (e.g., ``mom'' vs. ``mother''). As a result, token-level loss can penalize valid abstractions, paraphrases, or conceptually correct reasoning paths, biasing models toward surface form rather than underlying meaning. This mismatch between the training signal and semantic correctness motivates learning objectives that operate over higher-level representations. We propose a shift from token-level to concept-level prediction, where concepts group multiple surface forms of the same idea (e.g., ``mom,'' ``mommy,'' ``mother'' $\rightarrow$ \textit{MOTHER}). We introduce various methods for integrating conceptual supervision into LLM training and show that concept-aware models achieve lower perplexity, improved robustness under domain shift, and stronger performance than NTP-based models on diverse NLP benchmarks. This suggests \textit{concept-level supervision} as an improved training signal that better aligns LLMs with human semantic abstractions.

</details>


### [286] [TWeddit : A Dataset of Triggering Stories Predominantly Shared by Women on Reddit](https://arxiv.org/abs/2601.11819)
*Shirlene Rose Bandela,Sanjeev Parthasarathy,Vaibhav Garg*

Main category: cs.CL

TL;DR: 本论文提出并发布了一个专注于女性遭遇触发性经历（如流产、性暴力等）的Reddit故事数据集TWeddit，对其进行了语言学分析并阐述其研究价值。


<details>
  <summary>Details</summary>
Motivation: 女性在经历流产、堕胎或性暴力等创伤事件后，常常会在社交媒体上分享经历以寻求支持与情感表达。然而，公开平台上用户容易接触到令人不安的内容，而Reddit用户往往因缺乏意识或不确定适用范畴未能加注触发警告。目前关于Reddit故事的触发性经历标注数据集稀缺，限制了相关自动识别和研究。

Method: 作者整理并人工标注了一个包含女性常见触发性经历的Reddit故事数据集TWeddit，对该数据集进行语言学及道德主题分析，考察其内容特征与独特性。

Result: 分析结果显示，TWeddit中的被标注故事在主题与道德基础表达上展现出鲜明特征；数据集涵盖了多种触发性经历，具有丰富的研究潜力。

Conclusion: TWeddit数据集为未来自动识别触发内容、心理健康支持、道德语义分析等研究提供了基础资源，有助于改善女性遭遇相关内容的识别与防护。

Abstract: Warning: This paper may contain examples and topics that may be disturbing to some readers, especially survivors of miscarriage and sexual violence. People affected by abortion, miscarriage, or sexual violence often share their experiences on social media to express emotions and seek support. On public platforms like Reddit, where users can post long, detailed narratives (up to 40,000 characters), readers may be exposed to distressing content. Although Reddit allows manual trigger warnings, many users omit them due to limited awareness or uncertainty about which categories apply. There is scarcity of datasets on Reddit stories labeled for triggering experiences. We propose a curated Reddit dataset, TWeddit, covering triggering experiences related to issues majorly faced by women. Our linguistic analyses show that annotated stories in TWeddit express distinct topics and moral foundations, making the dataset useful for a wide range of future research.

</details>


### [287] [The Third VoicePrivacy Challenge: Preserving Emotional Expressiveness and Linguistic Content in Voice Anonymization](https://arxiv.org/abs/2601.11846)
*Natalia Tomashenko,Xiaoxiao Miao,Pierre Champion,Sarina Meyer,Michele Panariello,Xin Wang,Nicholas Evans,Emmanuel Vincent,Junichi Yamagishi,Massimiliano Todisco*

Main category: cs.CL

TL;DR: 本文总结了2024年第三届VoicePrivacy挑战赛的成果，重点在于如何在语音数据中实现话者身份匿名，同时保留内容和情感。


<details>
  <summary>Details</summary>
Motivation: 随着语音识别和合成技术的发展，用户的隐私保护变得日益重要。语音匿名化能有效隐藏说话者的身份，防止被滥用，但目前标准和技术仍需突破。

Method: 作者系统介绍了挑战赛的框架，包括任务定义、数据集、攻击模型与评测指标，并描述了6个基线系统以及参赛者提出的创新方法。

Result: 展示并对比了参赛系统中用于语音匿名化的不同方法，总结了系统在隐私保护、内容与情感保留等方面的客观评测结果。

Conclusion: 研究为今后语音匿名化研究和挑战赛的设计提供了方向和建议，指出进一步提升话者隐私与语音内容、情感保真度的研究可能性。

Abstract: We present results and analyses from the third VoicePrivacy Challenge held in 2024, which focuses on advancing voice anonymization technologies. The task was to develop a voice anonymization system for speech data that conceals a speaker's voice identity while preserving linguistic content and emotional state. We provide a systematic overview of the challenge framework, including detailed descriptions of the anonymization task and datasets used for both system development and evaluation. We outline the attack model and objective evaluation metrics for assessing privacy protection (concealing speaker voice identity) and utility (content and emotional state preservation). We describe six baseline anonymization systems and summarize the innovative approaches developed by challenge participants. Finally, we provide key insights and observations to guide the design of future VoicePrivacy challenges and identify promising directions for voice anonymization research.

</details>


### [288] [ATOD: An Evaluation Framework and Benchmark for Agentic Task-Oriented Dialogue System](https://arxiv.org/abs/2601.11854)
*Yifei Zhang,Hooshang Nayyeri,Rinat Khaziev,Emine Yilmaz,Gokhan Tur,Dilek Hakkani-Tür,Hari Thadakamalla*

Main category: cs.CL

TL;DR: 本文提出了ATOD基准和ATOD-Eval评测框架，用于系统性评估集成LLM和工具的先进任务型对话系统在多目标协调、长期推理等方面的智能行为。


<details>
  <summary>Details</summary>
Motivation: 现有任务型对话系统评测主要关注单一任务和短时上下文，无法有效衡量现代对话系统在多目标、长期记忆、主动性等复杂能力上的表现，亟需新的评测基准和方法。

Method: 作者设计了ATOD基准和对话生成流程，包含多目标、依赖管理、记忆和主动性场景，并提出ATOD-Eval评测框架，把智能体能力细化为可度量指标，支持离线和在线评测。同时提出了基于记忆的高效评测器进行比较和分析。

Result: 实验显示，ATOD-Eval可以全面评估任务完成度、智能体能力和响应质量，新提出的基于记忆的评测方法相比现有记忆/LLM评测在精度和效率间取得了更好平衡。

Conclusion: ATOD及其评测框架为先进任务型对话系统的评估提供了系统化、全面化和高效的方案，填补了现有基准的不足，有助于推动智能体行为能力的研究和提升。

Abstract: Recent advances in task-oriented dialogue (TOD) systems, driven by large language models (LLMs) with extensive API and tool integration, have enabled conversational agents to coordinate interleaved goals, maintain long-horizon context, and act proactively through asynchronous execution. These capabilities extend beyond traditional TOD systems, yet existing benchmarks lack systematic support for evaluating such agentic behaviors. To address this gap, we introduce ATOD, a benchmark and synthetic dialogue generation pipeline that produces richly annotated conversations requiring long-term reasoning. ATOD captures key characteristics of advanced TOD, including multi-goal coordination, dependency management, memory, adaptability, and proactivity. Building on ATOD, we propose ATOD-Eval, a holistic evaluation framework that translates these dimensions into fine-grained metrics and supports reproducible offline and online evaluation. We further present a strong agentic memory-based evaluator for benchmarking on ATOD. Experiments show that ATOD-Eval enables comprehensive assessment across task completion, agentic capability, and response quality, and that the proposed evaluator offers a better accuracy-efficiency tradeoff compared to existing memory- and LLM-based approaches under this evaluation setting.

</details>


### [289] [CTPD: Cross Tokenizer Preference Distillation](https://arxiv.org/abs/2601.11865)
*Truong Nguyen,Phi Van Dat,Ngan Nguyen,Linh Ngo Van,Trung Le,Thanh Hong Nguyen*

Main category: cs.CL

TL;DR: 提出了一种在不同分词器之间实现人类偏好知识蒸馏的新方法CTPD，并在多个基准上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 目前知识蒸馏用于预训练和指令微调，但在让语言模型学习人类偏好、特别是在师生模型使用不同分词器（tokenizer）的现实场景下，应用还很有限。主要挑战是不同分词方式导致难以细粒度、透明地转移人类偏好信息。

Method: 提出CTPD框架，核心包括：1）对齐的字符级span映射，使师生模型在不同tokenizer下实现精确的监督；2）跨tokenizer的Token-level Importance Sampling（TIS-DPO），提升责任分配的准确性；3）Teacher-Anchored Reference，学生模型能直接利用教师模型在人类偏好优化目标中的判断。此外进行了理论推导，将方法与重要性采样相结合。

Result: 在多个基准测试上，CTPD大幅优于现有方法，展示了跨分词器偏好蒸馏的普适性和有效性。

Conclusion: CTPD为跨分词器蒸馏人类偏好提供了实用、通用且高效的解决方案，为更广泛、便捷地对齐语言模型和人类偏好铺平了道路。

Abstract: While knowledge distillation has seen widespread use in pre-training and instruction tuning, its application to aligning language models with human preferences remains underexplored, particularly in the more realistic cross-tokenizer setting. The incompatibility of tokenization schemes between teacher and student models has largely prevented fine-grained, white-box distillation of preference information. To address this gap, we propose Cross-Tokenizer Preference Distillation (CTPD), the first unified framework for transferring human-aligned behavior between models with heterogeneous tokenizers. CTPD introduces three key innovations: (1) Aligned Span Projection, which maps teacher and student tokens to shared character-level spans for precise supervision transfer; (2) a cross-tokenizer adaptation of Token-level Importance Sampling (TIS-DPO) for improved credit assignment; and (3) a Teacher-Anchored Reference, allowing the student to directly leverage the teacher's preferences in a DPO-style objective. Our theoretical analysis grounds CTPD in importance sampling, and experiments across multiple benchmarks confirm its effectiveness, with significant performance gains over existing methods. These results establish CTPD as a practical and general solution for preference distillation across diverse tokenization schemes, opening the door to more accessible and efficient alignment of language models.

</details>


### [290] [Advances in LLM Reasoning Enable Flexibility in Clinical Problem-Solving](https://arxiv.org/abs/2601.11866)
*Kie Shidara,Preethi Prem,Jonathan Kim,Anna Podlasek,Feng Liu,Ahmed Alaa,Danilo Bernardo*

Main category: cs.CL

TL;DR: 本论文评估了大型语言模型（LLM）在一个对抗性的医学推理基准（mARC）上的灵活性表现，发现先进的推理模型在应对医学启发式陷阱时表现出更高的灵活性，达到了与人类相当的水平。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在医学问答任务上已表现出高准确率，但其临床推理灵活性受质疑。现有研究难以评估模型是否能避免过度依赖已学到的不灵活模式（Einstellung效应），因此有必要评估LLM在这类对抗性情境下的表现。

Method: 作者选取了OpenAI、Grok、Gemini、Claude和DeepSeek等家族中的推理模型，在利用Einstellung效应设计的mARC医学题库上进行测试。mARC通过设置使模型容易陷入不灵活的启发式惯性，从而检验其推理灵活性。作者比较了强模型与弱模型在易错题和高难度题上的表现。

Result: 强推理模型比弱模型更能规避Einstellung陷阱，在mARC上达到人类水平。在医生最常出错的问题中，前五名模型以较高置信度正确答案率为55%-70%，显示出比人类更少受Einstellung效应影响。

Conclusion: 先进大型语言模型具备更高的医学推理灵活性，在对抗性医学推理任务上已达到（甚至部分超越）人类水平。

Abstract: Large Language Models (LLMs) have achieved high accuracy on medical question-answer (QA) benchmarks, yet their capacity for flexible clinical reasoning has been debated. Here, we asked whether advances in reasoning LLMs improve their cognitive flexibility in clinical reasoning. We assessed reasoning models from the OpenAI, Grok, Gemini, Claude, and DeepSeek families on the medicine abstraction and reasoning corpus (mARC), an adversarial medical QA benchmark which utilizes the Einstellung effect to induce inflexible overreliance on learned heuristic patterns in contexts where they become suboptimal. We found that strong reasoning models avoided Einstellung-based traps more often than weaker reasoning models, achieving human-level performance on mARC. On questions most commonly missed by physicians, the top 5 performing models answered 55% to 70% correctly with high confidence, indicating that these models may be less susceptible than humans to Einstellung effects. Our results indicate that strong reasoning models demonstrate improved flexibility in medical reasoning, achieving performance on par with humans on mARC.

</details>


### [291] [GloCTM: Cross-Lingual Topic Modeling via a Global Context Space](https://arxiv.org/abs/2601.11872)
*Nguyen Tien Phat,Ngo Vu Minh,Linh Van Ngo,Nguyen Thi Ngoc Diep,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的跨语言主题建模框架GloCTM，通过统一的全局语义空间和多种跨语言对齐机制，显著增强了主题一致性与对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言主题模型主要在单独的语言空间中学习主题，依赖诸如双语词典的对齐机制，难以捕获深层语义联系，主题空间连接较弱。同时，这些方法往往忽视了多语言预训练表示中丰富的语义信号，导致主题对齐粒度有限。

Method: GloCTM通过以下方式创新：1) 用跨语言词汇邻域扩展输入，实现丰富表达；2) 设计局部与全局编码器联合推断，并通过内部正则化对齐潜在表示；3) 在输出层用全局主题词分布同步各语言主题；4) 引入CKA损失，将主题空间与多语上下文嵌入对齐。

Result: 在多项基准实验中，GloCTM在主题连贯性和跨语言对齐方面均显著优于现有强基线方法。

Conclusion: 利用统一的语义空间加多层次对齐技术，GloCTM有效提升了跨语言主题建模的语义一致性和实用性，为多语言理解带来新突破。

Abstract: Cross-lingual topic modeling seeks to uncover coherent and semantically aligned topics across languages - a task central to multilingual understanding. Yet most existing models learn topics in disjoint, language-specific spaces and rely on alignment mechanisms (e.g., bilingual dictionaries) that often fail to capture deep cross-lingual semantics, resulting in loosely connected topic spaces. Moreover, these approaches often overlook the rich semantic signals embedded in multilingual pretrained representations, further limiting their ability to capture fine-grained alignment. We introduce GloCTM (Global Context Space for Cross-Lingual Topic Model), a novel framework that enforces cross-lingual topic alignment through a unified semantic space spanning the entire model pipeline. GloCTM constructs enriched input representations by expanding bag-of-words with cross-lingual lexical neighborhoods, and infers topic proportions using both local and global encoders, with their latent representations aligned through internal regularization. At the output level, the global topic-word distribution, defined over the combined vocabulary, structurally synchronizes topic meanings across languages. To further ground topics in deep semantic space, GloCTM incorporates a Centered Kernel Alignment (CKA) loss that aligns the latent topic space with multilingual contextual embeddings. Experiments across multiple benchmarks demonstrate that GloCTM significantly improves topic coherence and cross-lingual alignment, outperforming strong baselines.

</details>


### [292] [Faithfulness vs. Safety: Evaluating LLM Behavior Under Counterfactual Medical Evidence](https://arxiv.org/abs/2601.11886)
*Kaijie Mo,Siddhartha Venkatayogi,Chantal Shaib,Ramez Kouzy,Wei Xu,Byron C. Wallace,Junyi Jessy Li*

Main category: cs.CL

TL;DR: 作者构建了一个反事实医学问答数据集MedCounterFact，发现当前大型语言模型在面对不真实甚至危险的医学证据时，依然会自信地给出肯定答案，且难以平衡忠实性与安全性。


<details>
  <summary>Details</summary>
Motivation: 在医疗等高风险领域，追求模型对上下文的忠实性可能很重要，但若上下文信息本身与真实知识或安全规范冲突，模型该如何表现？目前尚不清楚大型语言模型在面对不真实，甚至敌意证据时的行为和推理过程。

Method: 作者构建了MedCounterFact数据集，将真实世界医疗干预用不同类型的反事实内容（如未知词、毒物等）系统性替换，通过这些反事实文本作为模型的输入上下文，对多个先进LLM的表现进行评测，包括它们对虚假甚至危险证据的接受与推理。

Result: 在MedCounterFact数据集上的测试结果显示，现有多个LLM模型在面对反事实甚至危险的医学证据时，依然普遍采信并给出自信、未加警惕的答案。模型基本未能区分危险与安全、真实与虚假证据。

Conclusion: 当前大型语言模型在医域知识问答中，对上下文证据的信任与信息安全保护之间仍没有明确的平衡界线，模型在信任反事实甚至有害证据方面存在显著风险，需要进一步研究和改进。

Abstract: In high-stakes domains like medicine, it may be generally desirable for models to faithfully adhere to the context provided. But what happens if the context does not align with model priors or safety protocols? In this paper, we investigate how LLMs behave and reason when presented with counterfactual or even adversarial medical evidence. We first construct MedCounterFact, a counterfactual medical QA dataset that requires the models to answer clinical comparison questions (i.e., judge the efficacy of certain treatments, with evidence consisting of randomized controlled trials provided as context). In MedCounterFact, real-world medical interventions within the questions and evidence are systematically replaced with four types of counterfactual stimuli, ranging from unknown words to toxic substances. Our evaluation across multiple frontier LLMs on MedCounterFact reveals that in the presence of counterfactual evidence, existing models overwhelmingly accept such "evidence" at face value even when it is dangerous or implausible, and provide confident and uncaveated answers. While it may be prudent to draw a boundary between faithfulness and safety, our findings reveal that there exists no such boundary yet.

</details>


### [293] [PPA-Plan: Proactive Pitfall Avoidance for Reliable Planning in Long-Context LLM Reasoning](https://arxiv.org/abs/2601.11908)
*Byeongjin Kim,Gyuwan Kim,Seo Yeon Park*

Main category: cs.CL

TL;DR: PPA-Plan是一种用于大语言模型长文本推理的主动规划策略，通过在计划生成前预判并避免潜在谬误，实验上显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理长文本推理（信息稀疏分布）时效果不佳，传统的plan-and-execute框架容易因依赖表面线索而生成错误的计划，难以事后纠正，影响推理效果。

Method: 提出PPA-Plan，在计划生成前主动识别推理中的逻辑陷阱和错误假设，将这些问题转化为负约束，然后有针对性地规避这些陷阱，提升计划的正确率。

Result: 在长文本问答基准测试中，PPA-Plan生成的计划执行效果优于现有的分步执行方法和直接提示方法。

Conclusion: PPA-Plan通过主动规避潜在错误方案，提升了大模型长文本推理能力，是提升推理可靠性的一种有效策略。

Abstract: Large language models (LLMs) struggle with reasoning over long contexts where relevant information is sparsely distributed. Although plan-and-execute frameworks mitigate this by decomposing tasks into planning and execution, their effectiveness is often limited by unreliable plan generation due to dependence on surface-level cues. Consequently, plans may be based on incorrect assumptions, and once a plan is formed, identifying what went wrong and revising it reliably becomes difficult, limiting the effectiveness of reactive refinement. To address this limitation, we propose PPA-Plan, a proactive planning strategy for long-context reasoning that focuses on preventing such failures before plan generation. PPA-Plan identifies potential logical pitfalls and false assumptions, formulates them as negative constraints, and conditions plan generation on explicitly avoiding these constraints. Experiments on long-context QA benchmarks show that executing plans generated by PPA-Plan consistently outperforms existing plan-and-execute methods and direct prompting.

</details>


### [294] [LSTM-MAS: A Long Short-Term Memory Inspired Multi-Agent System for Long-Context Understanding](https://arxiv.org/abs/2601.11913)
*Yichen Jiang,Peng Ye,Jiakang Yuan,Chongjun Tu,Lei Bai,Tao Chen*

Main category: cs.CL

TL;DR: 本文提出了一种借鉴LSTM思想的多智能体系统LSTM-MAS，以提高大语言模型（LLM）对长文本的理解能力，并在多个数据集上大幅超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM处理长文本时，单模型方法受限于算力和上下文窗口长度，注意力优化也有其局限；多智能体框架虽然可扩展，但容易积累误差和产生幻觉，需要新的结构改进。

Method: 受LSTM结构启发，设计了LSTM-MAS多智能体系统，链式架构，每个节点含工人、过滤、判别和管理智能体，分别负责片段理解、冗余过滤、错误检测和全局信息管理，模拟LSTM的门控与信息流机制，以实现对长依赖信息的有效调控和选择性记忆。

Result: 该方法在NarrativeQA、Qasper、HotpotQA和MuSiQue四个长文本理解任务上，相较于当前最优的多智能体方法CoA，取得了40.93%、43.70%、121.57%和33.12%的性能提升。

Conclusion: LSTM-MAS借鉴LSTM的信息流与门控机制，解决了多智能体长文本理解中的误差累积和幻觉扩散问题，实现了更优的长上下文建模效果，有望推动LLM在长文本处理方面的应用。

Abstract: Effectively processing long contexts remains a fundamental yet unsolved challenge for large language models (LLMs). Existing single-LLM-based methods primarily reduce the context window or optimize the attention mechanism, but they often encounter additional computational costs or constrained expanded context length. While multi-agent-based frameworks can mitigate these limitations, they remain susceptible to the accumulation of errors and the propagation of hallucinations. In this work, we draw inspiration from the Long Short-Term Memory (LSTM) architecture to design a Multi-Agent System called LSTM-MAS, emulating LSTM's hierarchical information flow and gated memory mechanisms for long-context understanding. Specifically, LSTM-MAS organizes agents in a chained architecture, where each node comprises a worker agent for segment-level comprehension, a filter agent for redundancy reduction, a judge agent for continuous error detection, and a manager agent for globally regulates information propagation and retention, analogous to LSTM and its input gate, forget gate, constant error carousel unit, and output gate. These novel designs enable controlled information transfer and selective long-term dependency modeling across textual segments, which can effectively avoid error accumulation and hallucination propagation. We conducted an extensive evaluation of our method. Compared with the previous best multi-agent approach, CoA, our model achieves improvements of 40.93%, 43.70%,121.57% and 33.12%, on NarrativeQA, Qasper, HotpotQA, and MuSiQue, respectively.

</details>


### [295] [Enhancing LLM-Based Data Annotation with Error Decomposition](https://arxiv.org/abs/2601.11920)
*Zhen Xu,Vedant Khatri,Yijun Dai,Xiner Liu,Siyan Li,Xuanming Zhang,Renzhe Yu*

Main category: cs.CL

TL;DR: 本文提出了一种新的评估大语言模型（LLMs）用于主观数据标注任务的方法，通过引入人工参与和更细致的误差分类，帮助更准确地评估LLMs在复杂任务中的表现及实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在主观标注（如心理学等含主观判断的任务）中表现出不一致和易出错的情况，且传统的评价指标只用单一对齐分数，难以揭示不同类型的误差对后续分析结论的多样影响，因此需要更细致的评估范式。

Method: 提出一种“诊断性评估范式”：1）建立双维度的误差分类体系（模型特定vs任务固有；边界模糊vs概念误认）；2）设计轻量级人工测试，用于估算与任务本身相关的模糊度；3）开发计算方法，根据上述分类体系对LLM的标注误差进行细化分析。最后在四种教育标注任务中进行了验证。

Result: 实验证明该范式具有理论和实际有效性，可以更好地解释为何某些任务过高的模型人类一致性是不现实的，以及为何单一对齐指标无法充分反映标注质量。同时，该方法操作简便、低成本。

Conclusion: 本文的方法为大模型主观标注质量评估提供了一种新工具，更有助于理解和提升LLM在主观类标注任务中的表现，对相关任务适用性判断和未来优化提供了有价值的参考。

Abstract: Large language models offer a scalable alternative to human coding for data annotation tasks, enabling the scale-up of research across data-intensive domains. While LLMs are already achieving near-human accuracy on objective annotation tasks, their performance on subjective annotation tasks, such as those involving psychological constructs, is less consistent and more prone to errors. Standard evaluation practices typically collapse all annotation errors into a single alignment metric, but this simplified approach may obscure different kinds of errors that affect final analytical conclusions in different ways. Here, we propose a diagnostic evaluation paradigm that incorporates a human-in-the-loop step to separate task-inherent ambiguity from model-driven inaccuracies and assess annotation quality in terms of their potential downstream impacts. We refine this paradigm on ordinal annotation tasks, which are common in subjective annotation. The refined paradigm includes: (1) a diagnostic taxonomy that categorizes LLM annotation errors along two dimensions: source (model-specific vs. task-inherent) and type (boundary ambiguity vs. conceptual misidentification); (2) a lightweight human annotation test to estimate task-inherent ambiguity from LLM annotations; and (3) a computational method to decompose observed LLM annotation errors following our taxonomy. We validate this paradigm on four educational annotation tasks, demonstrating both its conceptual validity and practical utility. Theoretically, our work provides empirical evidence for why excessively high alignment is unrealistic in specific annotation tasks and why single alignment metrics inadequately reflect the quality of LLM annotations. In practice, our paradigm can be a low-cost diagnostic tool that assesses the suitability of a given task for LLM annotation and provides actionable insights for further technical optimization.

</details>


### [296] [Mapping the maturation of TCM as an adjuvant to radiotherapy](https://arxiv.org/abs/2601.11923)
*P. Bilha Githinji,Aikaterini Melliou,Xi Yuan,Dayan Zhang,Lian Zhang,Zhenglin Chen,Jiansong Ji,Chengying Lv,Jinhao Xu,Peiwu Qin,Dongmei Yu*

Main category: cs.CL

TL;DR: 本文对2000至2025年间近七万篇关于中医药辅助放疗的文献进行了分析，总结了该领域的研究演变路径、主题结构及报道趋势。


<details>
  <summary>Details</summary>
Motivation: 随着中医药在肿瘤放疗中的辅助应用逐渐普及，亟需系统梳理其研究输出的历史轨迹和主题演化，评估研究领域的成熟度与未来发展方向。

Method: 采用大规模文献计量分析和主题建模技术，对69,745篇相关论文在产出数量、国际合作、资金支持以及主题演化等方面进行综合分析，挖掘领域发展的周期性和结构特征。

Result: 分析揭示了该领域研究存在周期性扩张与收缩、主题聚焦于五大方向（癌种、支持治疗、临床终点、机制、方法学），展现出以患者为中心和系统整合的特点。同时发现，正向结果报道在不同主题和周期中高度一致，提示存在系统性正向报道偏倚。

Conclusion: 中医药辅助放疗领域已完成现有研究议程，正处于深化或转型的关键节点，但也需要警惕系统性正向报道偏倚对学科认识的影响。

Abstract: The integration of complementary medicine into oncology represents a paradigm shift that has seen to increasing adoption of Traditional Chinese Medicine (TCM) as an adjuvant to radiotherapy. About twenty-five years since the formal institutionalization of integrated oncology, it is opportune to synthesize the trajectory of evidence for TCM as an adjuvant to radiotherapy. Here we conduct a large-scale analysis of 69,745 publications (2000 - 2025), emerging a cyclical evolution defined by coordinated expansion and contraction in publication output, international collaboration, and funding commitments that mirrors a define-ideate-test pattern. Using a theme modeling workflow designed to determine a stable thematic structure of the field, we identify five dominant thematic axes - cancer types, supportive care, clinical endpoints, mechanisms, and methodology - that signal a focus on patient well-being, scientific rigor and mechanistic exploration. Cross-theme integration of TCM is patient-centered and systems-oriented. Together with the emergent cycles of evolution, the thematic structure demonstrates progressive specialization and potential defragmentation of the field or saturation of existing research agenda. The analysis points to a field that has matured its current research agenda and is likely at the cusp of something new. Additionally, the field exhibits positive reporting of findings that is homogeneous across publication types, thematic areas, and the cycles of evolution suggesting a system-wide positive reporting bias agnostic to structural drivers.

</details>


### [297] [Event Detection with a Context-Aware Encoder and LoRA for Improved Performance on Long-Tailed Classes](https://arxiv.org/abs/2601.11932)
*Abdullah Al Monsur,Nitesh Vamshi Bommisetty,Gene Louis Kim*

Main category: cs.CL

TL;DR: 本文指出，解码器-only大型语言模型（LLMs）结构上无法充分利用双向上下文信息，并批评事件检测领域对Micro-F1的过度依赖。作者通过引入句子级上下文和使用LoRA微调，显著提升了Macro-F1，尤其对长尾事件类别表现突出。


<details>
  <summary>Details</summary>
Motivation: 当前事件检测主要采用解码器-only LLM，但其只能单向处理文本，难以捕捉完整语境。此外，业界普遍采用Micro-F1评价模型，但这一指标对主流类别有偏好，不能真实反映模型在长尾事件上的能力。因此作者探讨更合理的模型结构和评估指标。

Method: 作者引入增强句子上下文的信息以及Low-Rank Adaptation（LoRA）微调策略，针对现有解码器-only LLM进行实验，并将评价重心从Micro-F1转向更公平反映所有类别表现的Macro-F1。

Result: 引入句子上下文和LoRA后，模型在事件检测任务上的Macro-F1有明显提升，尤其在长尾类别中优于传统baseline。

Conclusion: 模型在吸收更多上下文语义和通过适当微调（如LoRA）后，能有效提升长尾事件的检测效果；相比Micro-F1，Macro-F1更能真实评估模型的实际能力。

Abstract: The current state of event detection research has two notable re-occurring limitations that we investigate in this study. First, the unidirectional nature of decoder-only LLMs presents a fundamental architectural bottleneck for natural language understanding tasks that depend on rich, bidirectional context. Second, we confront the conventional reliance on Micro-F1 scores in event detection literature, which systematically inflates performance by favoring majority classes. Instead, we focus on Macro-F1 as a more representative measure of a model's ability across the long-tail of event types. Our experiments demonstrate that models enhanced with sentence context achieve superior performance over canonical decoder-only baselines. Using Low-Rank Adaptation (LoRA) during finetuning provides a substantial boost in Macro-F1 scores in particular, especially for the decoder-only models, showing that LoRA can be an effective tool to enhance LLMs' performance on long-tailed event classes.

</details>


### [298] [Double-Calibration: Towards Trustworthy LLMs via Calibrating Knowledge and Reasoning Confidence](https://arxiv.org/abs/2601.11956)
*Yuyin Lu,Ziran Liang,Yanghui Rao,Wenqi Fan,Fu Lee Wang,Qing Li*

Main category: cs.CL

TL;DR: 论文提出了DoublyCal框架，通过双重校准机制提升大型语言模型在知识图谱辅助推理时的准确性和可信度量。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在推理任务中易产生幻觉。虽然引入知识图谱能提高事实性，但现有方法无法对检索证据及推理过程的认知型不确定性进行量化。

Method: DoublyCal框架包含两步：首先用轻量级代理模型生成知识图谱证据并校准证据可信度；再用这些校准证据指导黑盒型LLM推理，同时输出与证据不确定性相关联的可信度分数。

Result: 在要求丰富知识检索的多个基准数据集上，DoublyCal显著提升了黑盒LLM的推理准确性与可信度量表现，且带来较低的计算（token）成本。

Conclusion: DoublyCal能够在提升LLM准确性的同时，对模型推理和证据的可信度给出量化解释，有望应用于对可信推理有更高要求的场景。

Abstract: Trustworthy reasoning in Large Language Models (LLMs) is challenged by their propensity for hallucination. While augmenting LLMs with Knowledge Graphs (KGs) improves factual accuracy, existing KG-augmented methods fail to quantify epistemic uncertainty in both the retrieved evidence and LLMs' reasoning. To bridge this gap, we introduce DoublyCal, a framework built on a novel double-calibration principle. DoublyCal employs a lightweight proxy model to first generate KG evidence alongside a calibrated evidence confidence. This calibrated supporting evidence then guides a black-box LLM, yielding final predictions that are not only more accurate but also well-calibrated, with confidence scores traceable to the uncertainty of the supporting evidence. Experiments on knowledge-intensive benchmarks show that DoublyCal significantly improves both the accuracy and confidence calibration of black-box LLMs with low token cost.

</details>


### [299] [PEARL: Self-Evolving Assistant for Time Management with Reinforcement Learning](https://arxiv.org/abs/2601.11957)
*Bingxuan Li,Jeonghwan Kim,Cheng Qian,Xiusi Chen,Eitan Anzenberg,Niran Kundapur,Heng Ji*

Main category: cs.CL

TL;DR: 本文提出并研究了自动解决日程冲突的难题，引入了专门的基准测试和新的方法，从而显著提升了基于大模型的日程管理表现。


<details>
  <summary>Details</summary>
Motivation: 忙碌的专业人士经常因日程冲突需要反复做出决策，手动处理效率低且人工委托难以大规模应用。作者关心是否可以依赖大语言模型或智能体自动管理时间，推动更高效的日程管理。

Method: 作者提出了CalConflictBench，这是一个长周期的日程冲突解决基准，支持逐步呈现冲突和反馈，让智能体能够递进推断和适应用户偏好。进一步，提出PEARL框架，将外部记忆模块和优化的逐轮奖励机制整合到大模型日程智能体中，以更好地在线学习用户偏好。

Result: 实验显示现有大模型日程智能体错误率高（如Qwen-3-30B-Think平均错误率为35%），而PEARL方法在CalConflictBench测试中能将错误率降低0.76，平均错误率比最强基线改善55%。

Conclusion: 目前通用大模型尚难胜任复杂日程冲突决策，但通过专门设计的训练与外部机制（如PEARL），可显著提升其表现，为智能日程管理的自动化带来新希望。

Abstract: Overlapping calendar invitations force busy professionals to repeatedly decide which meetings to attend, reschedule, or decline. We refer to this preference-driven decision process as calendar conflict resolution. Automating such process is crucial yet challenging. Scheduling logistics drain hours, and human delegation often fails at scale, which motivate we to ask: Can we trust large language model (LLM) or language agent to manager time? To enable systematic study of this question, we introduce CalConflictBench, a benchmark for long-horizon calendar conflict resolution. Conflicts are presented sequentially and agents receive feedback after each round, requiring them to infer and adapt to user preferences progressively. Our experiments show that current LLM agents perform poorly with high error rates, e.g., Qwen-3-30B-Think has 35% average error rate. To address this gap, we propose PEARL, a reinforcement-learning framework that augments language agent with an external memory module and optimized round-wise reward design, enabling agent to progressively infer and adapt to user preferences on-the-fly. Experiments on CalConflictBench shows that PEARL achieves 0.76 error reduction rate, and 55% improvement in average error rate compared to the strongest baseline.

</details>


### [300] [$\texttt{MemoryRewardBench}$: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models](https://arxiv.org/abs/2601.11969)
*Zecheng Tang,Baibei Ji,Ruoxi Sun,Haitian Wang,WangJie You,Zhang Yijun,Wenpeng Zhu,Ji Qi,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出了MemoryRewardBench，一个专门针对奖励模型（RMs）评估大语言模型长期记忆管理能力的基准测试，并通过该基准系统分析了13种前沿RM在不同任务下的表现和局限。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型被广泛用于处理长文本，如何通过有效的“记忆管理”让模型在长序列中实现信息传播成为关键能力。奖励模型（RMs）作为自动评估工具，对提升记忆质量评测有重要意义，但当前缺乏系统型基准让研究者分析和比较不同RMs的性能，因此有必要建立新的评价体系，推动RMs更好地服务于长文本处理。

Method: 作者构建了MemoryRewardBench，一个涵盖长文本理解与生成、设置丰富（10种不同记忆管理模式，8K到128K tokens区间）的评测基准，并应用于13个先进奖励模型，系统评估其在各类型任务中的性能。

Result: 评测结果显示，开源与专有奖励模型之间的性能差距正在缩小，新一代模型无论规模大小均优于旧一代。同时，揭示了当前奖励模型在不同记忆管理场景下的能力、表现与局限。

Conclusion: MemoryRewardBench为奖励模型评测长期记忆管理过程提供了系统参考，有助于促进奖励模型及大语言模型在长文本处理领域的能力提升，同时也揭示了当前模型存在的技术瓶颈与未来改进方向。

Abstract: Existing works increasingly adopt memory-centric mechanisms to process long contexts in a segment manner, and effective memory management is one of the key capabilities that enables large language models to effectively propagate information across the entire sequence. Therefore, leveraging reward models (RMs) to automatically and reliably evaluate memory quality is critical. In this work, we introduce $\texttt{MemoryRewardBench}$, the first benchmark to systematically study the ability of RMs to evaluate long-term memory management processes. $\texttt{MemoryRewardBench}$ covers both long-context comprehension and long-form generation tasks, featuring 10 distinct settings with different memory management patterns, with context length ranging from 8K to 128K tokens. Evaluations on 13 cutting-edge RMs indicate a diminishing performance gap between open-source and proprietary models, with newer-generation models consistently outperforming their predecessors regardless of parameter count. We further expose the capabilities and fundamental limitations of current RMs in evaluating LLM memory management across diverse settings.

</details>


### [301] [Acting Flatterers via LLMs Sycophancy: Combating Clickbait with LLMs Opposing-Stance Reasoning](https://arxiv.org/abs/2601.12019)
*Chaowei Zhang,Xiansheng Luo,Zewei Zhang,Yi Zhu,Jipeng Qiang,Longwei Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新方法，通过利用大型语言模型（LLM）讨好性回答的特性，自动生成对立立场的推理对，从而增强点击诱饵内容检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着网络内容激增，点击诱饵（Clickbait）成为严重问题。尽管LLM可以帮助检测，但它们常常迎合用户观点而偏离事实，因此需要新方法充分利用这一特点提升检测效果。

Method: 作者提出了Self-renewal Opposing-stance Reasoning Generation（SORG）框架，驱动LLM针对新闻标题生成高质量的支持和反对推理对，无需真实标签。进一步，提出Opposing Reasoning-based Clickbait Detection（ORCD）模型，将三个BERT编码器分别对标题及其两类推理进行建模，通过对比学习和LLM生成的可信度分数软标签训练模型。

Result: 在三个公开基准数据集上，所提方法在点击诱饵检测准确率上均优于LLM直接提示、小模型微调及现有最优方法。

Conclusion: 充分利用LLM的“讨好”特性生成对立推理，有助于提升无标签场景下网页标题点击诱饵检测的效果，为相关任务提供了新思路。

Abstract: The widespread proliferation of online content has intensified concerns about clickbait, deceptive or exaggerated headlines designed to attract attention. While Large Language Models (LLMs) offer a promising avenue for addressing this issue, their effectiveness is often hindered by Sycophancy, a tendency to produce reasoning that matches users' beliefs over truthful ones, which deviates from instruction-following principles. Rather than treating sycophancy as a flaw to be eliminated, this work proposes a novel approach that initially harnesses this behavior to generate contrastive reasoning from opposing perspectives. Specifically, we design a Self-renewal Opposing-stance Reasoning Generation (SORG) framework that prompts LLMs to produce high-quality agree and disagree reasoning pairs for a given news title without requiring ground-truth labels. To utilize the generated reasoning, we develop a local Opposing Reasoning-based Clickbait Detection (ORCD) model that integrates three BERT encoders to represent the title and its associated reasoning. The model leverages contrastive learning, guided by soft labels derived from LLM-generated credibility scores, to enhance detection robustness. Experimental evaluations on three benchmark datasets demonstrate that our method consistently outperforms LLM prompting, fine-tuned smaller language models, and state-of-the-art clickbait detection baselines.

</details>


### [302] [Preserving Fairness and Safety in Quantized LLMs Through Critical Weight Protection](https://arxiv.org/abs/2601.12033)
*Muhammad Alif Al Hakim,Alfan Farizki Wicaksono,Fajri Koto*

Main category: cs.CL

TL;DR: 本文系统性研究了大语言模型在使用静态和动态量化方法时，对公平性和安全性的影响，并提出了关键权重保护的新方法以缓解负面影响。


<details>
  <summary>Details</summary>
Motivation: 虽然量化技术广泛应用于提升大语言模型推理效率，但其对模型公平性和安全性的影响，尤其在动态量化和多语言环境下，鲜有深入探讨。量化可能导致模型输出带有偏见或降低安全性，因此有必要系统评估并寻求缓解方法。

Method: 作者比较了静态与动态量化对多语言（英语、法语、荷兰语、西班牙语、土耳其语）的公平性和多语言（英语、韩语、阿拉伯语）安全性的影响。进一步，提出“关键权重保护”方法，在量化过程中识别并保护对公平与安全有关键作用的权重，避免引入不良影响。

Result: 整体上，所有量化方法均会降低模型的公平性和安全性，但动态量化方式相对更稳定。公平性损失因语言不同而异，而非英语环境下的安全性下降更显著。关键权重保护方法有效缓解了这些问题，无需昂贵的再训练。

Conclusion: 传统量化会损害模型的公平性和安全性，特别是在多语言环境下。所提出的关键权重保护方法为高效且安全的量化提供了可行路径，有助于在保证推理效率的同时提升或维持模型可信赖性。

Abstract: Quantization is widely adopted to reduce the computational cost of large language models (LLMs); however, its implications for fairness and safety, particularly in dynamic quantization and multilingual contexts, remain underexplored. In this work, we conduct a systematic study of how static and dynamic quantization methods impact fairness and safety across benchmarks measuring intrinsic and extrinsic bias and safety alignment. For fairness, we evaluate English, French, Dutch, Spanish, and Turkish; for safety, we focus on English, Korean, and Arabic. Our findings reveal that quantization consistently degrades fairness and safety, with dynamic methods demonstrating greater stability than static ones. Moreover, fairness degradation varies across languages, while safety deterioration is especially pronounced in non-English settings. To address these risks, we introduce Critical Weight Protection, a novel technique that identifies and preserves fairness- and safety-critical weights during quantization. This approach effectively mitigates bias and safety deterioration without costly retraining or alignment, maintaining trustworthiness while retaining efficiency.

</details>


### [303] [Don't Start Over: A Cost-Effective Framework for Migrating Personalized Prompts Between LLMs](https://arxiv.org/abs/2601.12034)
*Ziyi Zhao,Chongming Gao,Yang Zhang,Haoyan Liu,Weinan Gan,Huifeng Guo,Yong Liu,Fuli Feng*

Main category: cs.CL

TL;DR: 本文提出了一种名为PUMA的新方法，实现了在基础大模型升级时，用户个性化提示词的高效迁移。该方法可极大降低迁移成本，并在多种实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的个性化通常依赖于用户特定的软提示词。但当底座模型升级后，这些提示词不兼容，需要重新训练，代价高昂。因此，亟需一种高效迁移个性化提示词的方法。

Method: PUMA框架通过参数高效的adapter来弥合旧新模型之间的语义差异，并采用基于用户分组的选择策略，以显著降低训练成本。

Result: 在三个大规模数据集上，PUMA表现与重新训练相当或更好，计算成本最高可降低98%。该方法还展现出很强的泛化性与鲁棒性，适用于不同模型架构与复杂迁移场景。

Conclusion: PUMA实现了用户资产和底层模型的解耦，有效支持大模型个性化的可持续发展，为后续模型升级提供了实用且高效的解决路径。

Abstract: Personalization in Large Language Models (LLMs) often relies on user-specific soft prompts. However, these prompts become obsolete when the foundation model is upgraded, necessitating costly, full-scale retraining. To overcome this limitation, we propose the Prompt-level User Migration Adapter (PUMA), a lightweight framework to efficiently migrate personalized prompts across incompatible models. PUMA utilizes a parameter-efficient adapter to bridge the semantic gap, combined with a group-based user selection strategy to significantly reduce training costs. Experiments on three large-scale datasets show our method matches or even surpasses the performance of retraining from scratch, reducing computational cost by up to 98%. The framework demonstrates strong generalization across diverse model architectures and robustness in advanced scenarios like chained and aggregated migrations, offering a practical path for the sustainable evolution of personalized AI by decoupling user assets from the underlying models.

</details>


### [304] [Codebook-Injected Dialogue Segmentation for Multi-Utterance Constructs Annotation: LLM-Assisted and Gold-Label-Free Evaluation](https://arxiv.org/abs/2601.12061)
*Jinsook Lee,Kirk Vanacore,Zhuqian Zhou,Jeanine Grutter,Rene F. Kizilcec*

Main category: cs.CL

TL;DR: 该论文提出了一种新的对话分割方法，并展示不同分割标准对对话行为标注可靠性的影响。


<details>
  <summary>Details</summary>
Motivation: 传统对话行为注释难以兼顾注释一致性和分割边界的准确性，降低了结果的可复制性和可靠性。

Method: 提出基于“codebook-injected segmentation”的新分割策略，引入依赖下游注释标准来决定分割边界的方法。用LLM（大模型）和检索增强等多种Baseline比较，并设计了无需金标准标签的新评价指标。

Result: 发现基于对话行为感知的分割方式在单元内部一致性上表现更优，但在全球对话流转移检测方面仍不如基于连贯性的方法。各类分割方法在不同指标和数据集上各有优劣，没有绝对优势的分割器。

Conclusion: 对话分割是关键设计环节，需结合下游任务目标进行优化，而非单一追求分数最优。

Abstract: Dialogue Act (DA) annotation typically treats communicative or pedagogical intent as localized to individual utterances or turns. This leads annotators to agree on the underlying action while disagreeing on segment boundaries, reducing apparent reliability. We propose codebook-injected segmentation, which conditions boundary decisions on downstream annotation criteria, and evaluate LLM-based segmenters against standard and retrieval-augmented baselines. To assess these without gold labels, we introduce evaluation metrics for span consistency, distinctiveness, and human-AI distributional agreement. We found DA-awareness produces segments that are internally more consistent than text-only baselines. While LLMs excel at creating construct-consistent spans, coherence-based baselines remain superior at detecting global shifts in dialogue flow. Across two datasets, no single segmenter dominates. Improvements in within-segment coherence frequently trade off against boundary distinctiveness and human-AI distributional agreement. These results highlight segmentation as a consequential design choice that should be optimized for downstream objectives rather than a single performance score.

</details>


### [305] [Bridging the Gap in Bangla Healthcare: Machine Learning Based Disease Prediction Using a Symptoms-Disease Dataset](https://arxiv.org/abs/2601.12068)
*Rowzatul Zannat,Abdullah Al Shafi,Abdul Muntakim*

Main category: cs.CL

TL;DR: 本文开发了一个包含758个症状-疾病关系、覆盖85种疾病的孟加拉语数据集，并在此基础上利用多种机器学习方法实现了高精度疾病预测，有效提升了孟加拉语人群的医疗可及性。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语为主要语言的人群在疾病预测和健康信息获取方面受到资源匮乏的严重限制，亟需开发相关数据集和工具来填补这一空白。

Method: 作者构建了一个详实的孟加拉语症状-疾病数据集，并使用多种机器学习模型（包括软/硬投票集成方法）对症状进行疾病预测，比较其在数据集上的表现。

Result: 集成方法（软、硬投票结合）达到98%的预测准确率，展现出极佳的鲁棒性和泛化能力。

Conclusion: 该数据集和疾病预测研究为孟加拉语地区的个性化医学和卫生信息化发展奠定了基础，有助于提升该语言社区在疾病早检、干预方面的平等医疗机会。

Abstract: Increased access to reliable health information is essential for non-English-speaking populations, yet resources in Bangla for disease prediction remain limited. This study addresses this gap by developing a comprehensive Bangla symptoms-disease dataset containing 758 unique symptom-disease relationships spanning 85 diseases. To ensure transparency and reproducibility, we also make our dataset publicly available. The dataset enables the prediction of diseases based on Bangla symptom inputs, supporting healthcare accessibility for Bengali-speaking populations. Using this dataset, we evaluated multiple machine learning models to predict diseases based on symptoms provided in Bangla and analyzed their performance on our dataset. Both soft and hard voting ensemble approaches combining top-performing models achieved 98\% accuracy, demonstrating superior robustness and generalization. Our work establishes a foundational resource for disease prediction in Bangla, paving the way for future advancements in localized health informatics and diagnostic tools. This contribution aims to enhance equitable access to health information for Bangla-speaking communities, particularly for early disease detection and healthcare interventions.

</details>


### [306] [To Copy or Not to Copy: Copying Is Easier to Induce Than Recall](https://arxiv.org/abs/2601.12075)
*Mehrdad Farahani,Franziska Penzkofer,Richard Johansson*

Main category: cs.CL

TL;DR: 本文通过提取和注入“仲裁向量”，实证分析大语言模型在权重记忆（参数化知识）与上下文提示（近期信息）之间的取舍机制，并揭示两者切换的本质不对称性。


<details>
  <summary>Details</summary>
Motivation: 在检索增强型场景下，大模型必须在其内在知识和外部检索信息之间做出权衡，作者希望理解这种仲裁决策是如何在模型内部以机制性方式实现的。

Method: 作者构建了专门的数据集，通过“残差流中心点差”提取能区分‘回忆’（依赖权重知识）和‘抄写’（依赖上下文提示）的仲裁向量，然后将其以加性方式注入模型激活，在不同模型层和token段进行干预，再衡量模型行为变化。实验覆盖两种主流架构与两个开放领域问答数据集。

Result: 注入仲裁向量能稳定地引起模型行为在“抄写”与“回忆”之间的转换。进一步分析表明，诱导抄写是容易激活的操作，可以在输入多处发生；而恢复回忆则较为脆弱，强依赖特定token干预。

Conclusion: 行为仲裁的神经机制存在本质不对称：激活上下文抄写较为容易且泛化，抑制上下文、促使模型回忆内部知识则更困难且依赖具体位置。该发现加深了对检索增强类大模型决策机制的理解。

Abstract: Language models used in retrieval-augmented settings must arbitrate between parametric knowledge stored in their weights and contextual information in the prompt. This work presents a mechanistic study of that choice by extracting an \emph{arbitration vector} from model activations on a curated dataset designed to disentangle (i) irrelevant contexts that elicit parametric recall and (ii) relevant but false contexts that elicit copying. The vector is computed as the residual-stream centroid difference between these regimes across 27 relations, and is injected as an additive intervention at selected layers and token spans to steer behavior in two directions: Copy$\rightarrow$Recall (suppressing context use) and Recall$\rightarrow$Copy (inducing the model to copy any token from the context). Experiments on two architectures (decoder-only and encoder/decoder) and two open-domain QA benchmarks show consistent behavior shifts under moderate scaling while monitoring accuracy and fluency. Mechanistic analyses of attention routing, MLP contributions, and layer-wise probability trajectories reveal an asymmetry: inducing copying is an easy ``reactivation'' process that can be triggered at different locations in the input, while restoring recall is a ``suppression'' process that is more fragile and strongly tied to object-token interventions.

</details>


### [307] [Optimizing User Profiles via Contextual Bandits for Retrieval-Augmented LLM Personalization](https://arxiv.org/abs/2601.12078)
*Linfeng Du,Ye Yuan,Zichen Zhao,Fuyuan Lyu,Emiliano Penaloza,Xiuying Chen,Zipeng Sun,Jikun Kang,Laurent Charlin,Xue Liu,Haolun Wu*

Main category: cs.CL

TL;DR: 该论文提出了PURPLE框架，通过上下文多臂赌博机和Plackett-Luce排序模型对用户历史进行优化，使得大语言模型（LLM）的个性化响应更有效，优于传统仅依赖语义相关性的检索方法。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM擅长泛用任务，但实现针对个人用户的响应定制仍具难度。现有的检索增强方法根据语义相关性选取用户历史，但相关性不一定代表对生成质量的提升，甚至可能因冗余或冲突信息而降低性能。作者希望解决相关性与检索效用之间的鸿沟。

Method: 作者提出PURPLE框架，以上下文多臂赌博机方法进行个性化用户档案建立，引入Plackett-Luce排名模型来捕捉用户记录间复杂的依赖关系。通过将参考回复的生成概率作为稠密反馈信号进行训练，使检索过程直接与生成质量对齐。该方法本质上将档案构建视为集合生成问题，而非贪心相关性选择。

Result: 在九项个性化任务上的大量实验证明，PURPLE在效果和效率方面均优于现有的启发式和增强检索基线方法。

Conclusion: PURPLE为个性化用户档案优化提供了一种原理性且可扩展的解决方案，提升了LLM个性化响应的质量，也为相关领域带来了新的思路。

Abstract: Large Language Models (LLMs) excel at general-purpose tasks, yet adapting their responses to individual users remains challenging. Retrieval augmentation provides a lightweight alternative to fine-tuning by conditioning LLMs on user history records, and existing approaches typically select these records based on semantic relevance. We argue that relevance serves as an unreliable proxy for utility: a record may be semantically similar to a query yet fail to improve generation quality or even degrade it due to redundancy or conflicting information. To bridge this gap, we propose PURPLE, a contextual bandit framework that oPtimizes UseR Profiles for Llm pErsonalization. In contrast to a greedy selection of the most relevant records, PURPLE treats profile construction as a set generation process and utilizes a Plackett-Luce ranking model to capture complex inter-record dependencies. By training with dense feedback provided by the likelihood of the reference response, our method aligns retrieval directly with generation quality. Extensive experiments on nine personalization tasks demonstrate that PURPLE consistently outperforms strong heuristic and retrieval-augmented baselines in both effectiveness and efficiency, establishing a principled and scalable solution for optimizing user profiles.

</details>


### [308] [Large language models struggle with ethnographic text annotation](https://arxiv.org/abs/2601.12099)
*Leonardo S. Goodall,Dor Shilton,Daniel A. Mullins,Harvey Whitehouse*

Main category: cs.CL

TL;DR: 本研究评估了7种大语言模型（LLM）对民族志文本进行结构化注释的能力，结果表明现有LLM在准确性和可靠性上远未达到可替代人工的水平。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在文本自动标注领域展现出潜力，研究者希望利用它们高效提取民族学文本中的结构化数据，以加速跨文化研究。然而，尚不清楚当前LLM是否能达到自动化注释所需的精度和一致性。

Method: 作者选择了7种最先进的LLM，针对567段民族志摘录，标注121类仪式特征，并将模型输出与人工标注进行对比，评估模型在不同类型文本、不同难度特征上的表现。

Result: LLM在整体表现上明显低于人工标注，无论是在文本较长、需区分等级的特征，还是解释模糊性强的问题上都面临较大困难。人类编码员之间的可靠性为LLM的潜在上限，但即便在人类高度一致的特征上，LLM依然表现不如人类。

Conclusion: 当前LLM尚不能替代人类在民族志文本注释上的专业能力，自动化水平有待进一步提升。

Abstract: Large language models (LLMs) have shown promise for automated text annotation, raising hopes that they might accelerate cross-cultural research by extracting structured data from ethnographic texts. We evaluated 7 state-of-the-art LLMs on their ability to annotate 121 ritual features across 567 ethnographic excerpts. Performance was limited, falling well below levels required for reliable automated annotation. Longer texts, features requiring ordinal distinctions, and ambiguous constructs proved particularly difficult. Human inter-coder reliability set an approximate ceiling on LLM accuracy: features that human coders found difficult to agree upon were also difficult for LLMs. Yet even on features where humans reliably agreed, models fell short of human performance. Our findings suggest that LLMs cannot yet substitute for human expertise in ethnographic annotation.

</details>


### [309] [Powerful Training-Free Membership Inference Against Autoregressive Language Models](https://arxiv.org/abs/2601.12104)
*David Ilić,David Stanojević,Kostadin Cvejoski*

Main category: cs.CL

TL;DR: 本文提出了一种新的成员推断攻击方法EZ-MIA，通过关注模型记忆在错误预测位置的表现，有效检测出模型是否记忆了特定训练数据，在实际隐私审计中显著提升检测能力。


<details>
  <summary>Details</summary>
Motivation: 微调后的语言模型容易记忆并泄露训练数据中的敏感信息。现有成员推断攻击（MIA）方法，在低误报阈值下检测能力较弱，无法满足实际隐私审计需求。因此，亟需更高效、准确的检测方法来量化和审计这些风险。

Method: 作者提出EZ-MIA方法，发现模型记忆最容易在错误预测（error positions）时暴露。通过引入Error Zone（EZ）分数，比较微调模型与预训练模型在错误位置的概率变化，无需任何模型训练，仅靠两次前向推断即可。

Result: 在WikiText+GPT-2上，EZ-MIA在1%误报率下检测率为66.3%，远超先前方法的17.5%，AUC高达0.98。在更严苛的0.1%误报率下，检测能力提升8倍（14.0% vs 1.8%）。在AG News+Llama-2-7B上，检测率也有3倍提升。

Conclusion: 细粒度的成员推断攻击能够发现更多微调语言模型的隐私泄露风险，高亮了实际部署模型所需的隐私关注。EZ-MIA显著提升了隐私审计的能力，为未来相关研究和实际应用提供了强大工具。

Abstract: Fine-tuned language models pose significant privacy risks, as they may memorize and expose sensitive information from their training data. Membership inference attacks (MIAs) provide a principled framework for auditing these risks, yet existing methods achieve limited detection rates, particularly at the low false-positive thresholds required for practical privacy auditing. We present EZ-MIA, a membership inference attack that exploits a key observation: memorization manifests most strongly at error positions, specifically tokens where the model predicts incorrectly yet still shows elevated probability for training examples. We introduce the Error Zone (EZ) score, which measures the directional imbalance of probability shifts at error positions relative to a pretrained reference model. This principled statistic requires only two forward passes per query and no model training of any kind. On WikiText with GPT-2, EZ-MIA achieves 3.8x higher detection than the previous state-of-the-art under identical conditions (66.3% versus 17.5% true positive rate at 1% false positive rate), with near-perfect discrimination (AUC 0.98). At the stringent 0.1% FPR threshold critical for real-world auditing, we achieve 8x higher detection than prior work (14.0% versus 1.8%), requiring no reference model training. These gains extend to larger architectures: on AG News with Llama-2-7B, we achieve 3x higher detection (46.7% versus 15.8% TPR at 1% FPR). These results establish that privacy risks of fine-tuned language models are substantially greater than previously understood, with implications for both privacy auditing and deployment decisions. Code is available at https://github.com/JetBrains-Research/ez-mia.

</details>


### [310] [Bengali Text Classification: An Evaluation of Large Language Model Approaches](https://arxiv.org/abs/2601.12132)
*Md Mahmudul Hoque,Md Mehedi Hassain,Md Hojaifa Tanvir,Rahul Nandy*

Main category: cs.CL

TL;DR: 本研究评估了大语言模型应用于孟加拉语新闻文本分类任务的效果。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语自然语言处理缺乏大量标注数据和预训练模型，限制了文本分类等下游任务的效果。

Method: 本文使用来自Kaggle的Prothom Alo新闻数据集，比较了三种指令微调的大语言模型（LLaMA 3.1 8B Instruct、LLaMA 3.2 3B Instruct、Qwen 2.5 7B Instruct）的分类表现，统一分类框架进行评估。

Result: Qwen 2.5模型在分类任务上表现最好，达到72%的准确率，在“体育”类别上尤其突出。LLaMA 3.1和3.2的准确率分别为53%和56%。

Conclusion: 尽管资源稀缺，大语言模型在孟加拉语文本分类任务中表现出较强能力。未来会探索更多模型、解决类别不均衡、优化微调方法以提升表现。

Abstract: Bengali text classification is a Significant task in natural language processing (NLP), where text is categorized into predefined labels. Unlike English, Bengali faces challenges due to the lack of extensive annotated datasets and pre-trained language models. This study explores the effectiveness of large language models (LLMs) in classifying Bengali newspaper articles. The dataset used, obtained from Kaggle, consists of articles from Prothom Alo, a major Bangladeshi newspaper. Three instruction-tuned LLMs LLaMA 3.1 8B Instruct, LLaMA 3.2 3B Instruct, and Qwen 2.5 7B Instruct were evaluated for this task under the same classification framework. Among the evaluated models, Qwen 2.5 achieved the highest classification accuracy of 72%, showing particular strength in the "Sports" category. In comparison, LLaMA 3.1 and LLaMA 3.2 attained accuracies of 53% and 56%, respectively. The findings highlight the effectiveness of LLMs in Bengali text classification, despite the scarcity of resources for Bengali NLP. Future research will focus on exploring additional models, addressing class imbalance issues, and refining fine-tuning approaches to improve classification performance.

</details>


### [311] [Analyzing Cancer Patients' Experiences with Embedding-based Topic Modeling and LLMs](https://arxiv.org/abs/2601.12154)
*Teodor-Călin Ionescu,Lifeng Han,Jan Heijdra Suasnabar,Anne Stiggelbout,Suzan Verberne*

Main category: cs.CL

TL;DR: 本研究利用神经主题模型和大语言模型（LLMs）分析癌症患者访谈数据，以挖掘对患者医疗有意义的主题，发现BioClinicalBERT嵌入下BERTopic的主题提取最精准可解释，有助于患者参与的医疗实践。


<details>
  <summary>Details</summary>
Motivation: 现有医疗实践较难系统性地捕捉患者视角，患者访谈是宝贵的数据来源，但如何高效并精准地总结这些大规模文本尚属挑战。因此作者希望用先进的机器学习方法自动化分析访谈，提升患者在医疗流程中的声音。

Method: 收集并转录13份癌症患者的访谈，采用BERTopic和Top2Vec进行主题抽取，在关键词提取上进行公平对比；再用GPT-4为主题打标签，并通过小规模人工评价（连贯性、清晰度和相关性）比较。最后在BERTopic的基础上，尝试三种临床领域的嵌入模型，寻找最佳方案后全量分析数据集。

Result: BERTopic相较Top2Vec在关键词抽取和主题一致性上表现更好，结合BioClinicalBERT嵌入可进一步提升主题的精准度和可解释性。全量分析发现"癌症护理管理中的协调与沟通"和"癌症治疗决策"为普遍主导主题。

Conclusion: 神经主题建模，特别是BERTopic加临床嵌入，可以有效抽取和总结患者访谈中的核心主题，有潜力为临床提供患者反馈，未来可提升医疗文档检索效率，助力以患者为中心的医疗实践。

Abstract: This study investigates the use of neural topic modeling and LLMs to uncover meaningful themes from patient storytelling data, to offer insights that could contribute to more patient-oriented healthcare practices. We analyze a collection of transcribed interviews with cancer patients (132,722 words in 13 interviews). We first evaluate BERTopic and Top2Vec for individual interview summarization by using similar preprocessing, chunking, and clustering configurations to ensure a fair comparison on Keyword Extraction. LLMs (GPT4) are then used for the next step topic labeling. Their outputs for a single interview (I0) are rated through a small-scale human evaluation, focusing on {coherence}, {clarity}, and {relevance}. Based on the preliminary results and evaluation, BERTopic shows stronger performance and is selected for further experimentation using three {clinically oriented embedding} models. We then analyzed the full interview collection with the best model setting. Results show that domain-specific embeddings improved topic \textit{precision} and \textit{interpretability}, with BioClinicalBERT producing the most consistent results across transcripts. The global analysis of the full dataset of 13 interviews, using the BioClinicalBERT embedding model, reveals the most dominant topics throughout all 13 interviews, namely ``Coordination and Communication in Cancer Care Management" and ``Patient Decision-Making in Cancer Treatment Journey''. Although the interviews are machine translations from Dutch to English, and clinical professionals are not involved in this evaluation, the findings suggest that neural topic modeling, particularly BERTopic, can help provide useful feedback to clinicians from patient interviews. This pipeline could support more efficient document navigation and strengthen the role of patients' voices in healthcare workflows.

</details>


### [312] [Tolerance Principle and Small Language Model Learning](https://arxiv.org/abs/2601.12179)
*Adam E. Friedman,Stevan Harnad,Rushen Shi*

Main category: cs.CL

TL;DR: 论文比较了人类婴儿与当前Transformer模型（如BabyBERTa）在语法规则学习能力上的差异，发现模型在处理有限训练数据及例外情况时并未表现出与婴儿相同的泛化能力，并未符合容忍原则的预测。


<details>
  <summary>Details</summary>
Motivation: 婴儿能从极少的例子中学会抽象的语法规则，但大规模语言模型需要大量训练数据。论文旨在研究模型在少量数据和存在例外的情况下是否能像婴儿一样学习语法规则，并验证杨(2016)提出的容忍原则对模型的适用性。

Method: 采用针对小数据集优化的BabyBERTa模型，在构造的人工语法数据集上训练。实验操控训练集的规模、句型数量以及规律和例外的比例，系统研究模型的泛化能力。

Result: 实验表明，BabyBERTa在有限例子和存在例外的情况下，学习动力学与婴儿不同，并未表现出容忍原则所预测的泛化阈值。

Conclusion: 当前的Transformer语言模型在小样本和例外处理上的泛化能力，尚无法达到人类婴儿的水平。模型学习过程中没有符合容忍原则的表现，说明人类与模型在规则学习机制上存在显著差异。

Abstract: Modern language models like GPT-3, BERT, and LLaMA require massive training data, yet with sufficient training they reliably learn to distinguish grammatical from ungrammatical sentences. Children aged as young as 14 months already have the capacity to learn abstract grammar rules from very few exemplars, even in the presence of non-rule-following exceptions. Yang's (2016) Tolerance Principle defines a precise threshold for how many exceptions a rule can tolerate and still be learnable. The present study explored the minimal amount and quality of training data necessary for rules to be generalized by a transformer-based language model to test the predictions of the Tolerance Principle. We trained BabyBERTa (Huebner et al. 2021), a transformer model optimized for small datasets, on artificial grammars. The training sets varied in size, number of unique sentence types, and proportion of rule-following versus exception exemplars. We found that, unlike human infants, BabyBERTa's learning dynamics do not align with the Tolerance Principle.

</details>


### [313] [CTC-DID: CTC-Based Arabic dialect identification for streaming applications](https://arxiv.org/abs/2601.12199)
*Muhammad Umar Farooq,Oscar Saz*

Main category: cs.CL

TL;DR: 提出了一种基于CTC损失的新型方言识别方法，能在低资源场景优于主流模型，且适用于短语音与实时应用。


<details>
  <summary>Details</summary>
Motivation: 现有方言识别方法对长语音或大量数据依赖较高，且在低资源、实时和短语音场景表现不佳。为提升低资源方言识别能力，提出更适合此场景的方法。

Method: 将方言识别建模为一个有限词汇的ASR任务，以方言标签序列作为识别目标，训练时用语言无关启发式（LAH）或预训练ASR模型决定标签重复数，在低资源阿拉伯方言任务上用SSL（自监督学习）模型实现CTC-DID。

Result: 在低资源阿拉伯方言数据集上，SSL驱动的CTC-DID模型优于微调Whisper和ECAPA-TDNN，不仅如此，在卡萨布兰卡数据集零样本测试上也有更好表现，尤其在短句和实时流场景下更加稳健。

Conclusion: CTC-DID适合低资源、短时语音及实时方言识别任务，比当前主流模型更稳健、泛化能力强，并可用于流式、实时应用，性能损失小。

Abstract: This paper proposes a Dialect Identification (DID) approach inspired by the Connectionist Temporal Classification (CTC) loss function as used in Automatic Speech Recognition (ASR). CTC-DID frames the dialect identification task as a limited-vocabulary ASR system, where dialect tags are treated as a sequence of labels for a given utterance. For training, the repetition of dialect tags in transcriptions is estimated either using a proposed Language-Agnostic Heuristic (LAH) approach or a pre-trained ASR model. The method is evaluated on the low-resource Arabic Dialect Identification (ADI) task, with experimental results demonstrating that an SSL-based CTC-DID model, trained on a limited dataset, outperforms both fine-tuned Whisper and ECAPA-TDNN models. Notably, CTC-DID also surpasses these models in zero-shot evaluation on the Casablanca dataset. The proposed approach is found to be more robust to shorter utterances and is shown to be easily adaptable for streaming, real-time applications, with minimal performance degradation.

</details>


### [314] [CoReflect: Conversational Evaluation via Co-Evolutionary Simulation and Reflective Rubric Refinement](https://arxiv.org/abs/2601.12208)
*Yunzhe Li,Richie Yueqi Feng,Tianxin Wei,Chin-Chia Hsu*

Main category: cs.CL

TL;DR: 本文提出了CoReflect框架，实现了对多轮会话系统进行自适应与迭代的自动评测。该方法通过仿真与反思循环，同时优化对话模板和评测标准，减少人工参与。


<details>
  <summary>Details</summary>
Motivation: 当前多轮对话系统的评估多依赖人工制定规则和静态上下文，难以覆盖多样化的对话场景，无法准确反映对话模型出现的复杂行为，因此需要一种更自适应、更全面的自动化评价方法。

Method: CoReflect将对话仿真与评估统一为一个迭代流程。其核心包括：1）对话规划器生成结构化对话模板，指导用户模拟器产生多样化、目标明确的对话；2）反思分析器对对话进行分析，发现系统性行为模式，并自动优化评估标准；3）分析结果反作用于对话规划器，优化下一轮的对话模板，使测试用例和评测标准同步进化，逐步提升测试复杂度和评估精准度。

Result: 通过CoReflect，评测流程可以减少人工介入，提高适应性，能针对对话模型的实际进步动态调整测试和标准，实现更贴合真实能力的评估。

Conclusion: CoReflect为多轮对话系统的评估提供了一种可扩展、自我优化的方法，能够应对对话模型能力不断提升带来的评测挑战。

Abstract: Evaluating conversational systems in multi-turn settings remains a fundamental challenge. Conventional pipelines typically rely on manually defined rubrics and fixed conversational context$-$a static approach that limits coverage and fails to capture the diverse, emergent behaviors of dialogue models. To address this, we introduce CoReflect (Conversational Evaluation via Co-Evolutionary Simulation and Reflective Rubric Refinement), which unifies dialogue simulation and evaluation into an adaptive, iterative process. CoReflect employs a conversation planner that generates structured templates to guide a user simulator through diverse, goal-directed dialogues. Subsequently, a reflective analyzer processes these dialogues to identify systematic behavioral patterns and automatically refine the evaluation rubrics. Crucially, the insights from the conversation analysis are fed back into the planner to update conversation templates for subsequent iterations. This co-evolution loop ensures that the complexity of test cases and the diagnostic precision of rubrics improve in tandem. By minimizing human intervention, CoReflect provides a scalable and self-refining methodology that allows evaluation protocols to adapt alongside the rapidly advancing capabilities of dialogue models.

</details>


### [315] [Plan, Verify and Fill: A Structured Parallel Decoding Approach for Diffusion Language Models](https://arxiv.org/abs/2601.12247)
*Miao Li,Hanyang Jiang,Sikai Chen,Hengyu Fu,Yuhang Cai,Baihe Huang,Tinghan Ye,Xuanzhou Chen,Pascal Van Hentenryck*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的文本生成范式Plan-Verify-Fill (PVF)，能提升Diffusion Language Models（扩散式语言模型）生成效率，在多个基准上显著降低函数评估次数（NFE），效率提升达65%，且准确性不受影响。


<details>
  <summary>Details</summary>
Motivation: 扩散式语言模型（DLMs）作为非自回归文本生成新范式，具有区别于传统方法的优势。然而，目前的解码策略多为被动、仅局部利用上下文，未能充分发挥全局信息优势。因此，需要更高效、能够利用全局语义的解码策略。

Method: 本文提出Plan-Verify-Fill（PVF）策略：主动构建层级骨架，优先选定语义锚点，以定量验证机制，实现高效的结构性停止，即当进一步生成收益递减时主动终止，达到“训练无关”的高效推理。

Result: 在LLaDA-8B-Instruct和Dream-7B-Instruct两个大模型上广泛验证，PVF在多个数据集上相较于基于置信度的并行解码，将函数评估次数减少了高达65%，在提升效率的同时保持了生成准确性。

Conclusion: PVF为扩散语言模型提供了高效、准确的文本生成方案，充分挖掘了全局信息利用潜力，为非自回归范式下文本生成研究带来新思路。

Abstract: Diffusion Language Models (DLMs) present a promising non-sequential paradigm for text generation, distinct from standard autoregressive (AR) approaches. However, current decoding strategies often adopt a reactive stance, underutilizing the global bidirectional context to dictate global trajectories. To address this, we propose Plan-Verify-Fill (PVF), a training-free paradigm that grounds planning via quantitative validation. PVF actively constructs a hierarchical skeleton by prioritizing high-leverage semantic anchors and employs a verification protocol to operationalize pragmatic structural stopping where further deliberation yields diminishing returns. Extensive evaluations on LLaDA-8B-Instruct and Dream-7B-Instruct demonstrate that PVF reduces the Number of Function Evaluations (NFE) by up to 65% compared to confidence-based parallel decoding across benchmark datasets, unlocking superior efficiency without compromising accuracy.

</details>


### [316] [Multimodal Generative Engine Optimization: Rank Manipulation for Vision-Language Model Rankers](https://arxiv.org/abs/2601.12263)
*Yixuan Du,Chenxiao Yu,Haoyan Xu,Ziyi Wang,Yue Zhao,Xiyang Hu*

Main category: cs.CL

TL;DR: 论文发现并揭示了基于视觉-语言模型（VLM）的商品搜索系统在多模态敌意场景下的严重安全漏洞，通过联合优化文本与图像，对搜索结果进行操控。提出的方法MGEO能显著提升攻击效果，现有检测措施难以防御。


<details>
  <summary>Details</summary>
Motivation: 尽管VLM在检索推荐中能力突出，但社区对其在对抗场景下的安全性关注不足，特别是在涉及文本与图像协同被攻击时的脆弱性问题。论文旨在探索并揭露多模态攻击对排名系统的潜在威胁。

Method: 提出了MGEO（多模态生成引擎优化）框架，通过同时优化图片微小扰动和流畅的文本后缀，对目标商品进行排名操控。方法采用交替梯度优化策略，充分利用VLM深层次的跨模态耦合特性。与单一模态攻击不同，该方法能联合操控文本与图片。

Result: 通过在真实数据集和SOTA模型上大量实验证明，MGEO多模态联合攻击效果显著优于单独的文本或图像攻击基线方法，能够有效且隐蔽地提升目标商品排名，难以被常规内容风控机制察觉。

Conclusion: 多模态协同曾是VLM的优势，但该特性在安全层面可被恶意利用，对检索排名系统完整性构成实质威胁。呼吁对多模态系统的安全机制加强研究和防护。

Abstract: Vision-Language Models (VLMs) are rapidly replacing unimodal encoders in modern retrieval and recommendation systems. While their capabilities are well-documented, their robustness against adversarial manipulation in competitive ranking scenarios remains largely unexplored. In this paper, we uncover a critical vulnerability in VLM-based product search: multimodal ranking attacks. We present Multimodal Generative Engine Optimization (MGEO), a novel adversarial framework that enables a malicious actor to unfairly promote a target product by jointly optimizing imperceptible image perturbations and fluent textual suffixes. Unlike existing attacks that treat modalities in isolation, MGEO employs an alternating gradient-based optimization strategy to exploit the deep cross-modal coupling within the VLM. Extensive experiments on real-world datasets using state-of-the-art models demonstrate that our coordinated attack significantly outperforms text-only and image-only baselines. These findings reveal that multimodal synergy, typically a strength of VLMs, can be weaponized to compromise the integrity of search rankings without triggering conventional content filters.

</details>


### [317] [Simulated Annealing Enhances Theory-of-Mind Reasoning in Autoregressive Language Models](https://arxiv.org/abs/2601.12269)
*Xucong Hu,Jian-Qiao Zhu*

Main category: cs.CL

TL;DR: 本文发现无需对自回归语言模型进行后续训练或权重更新，即可通过基于采样的方法直接复原其较强的“心理理论”（ToM）能力。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型主要优化局部连贯性，难以捕捉全局潜在状态，导致在评测心理理论（ToM）能力时表现不佳。此前提升ToM表现依赖后训练手段，本文试图探究不额外训练的直接采样方法能否挖掘模型潜力。

Method: 使用基于Markov链Monte Carlo（MCMC）的power-sampling技术，从模型的序列级概率分布中采样，并首次引入退火（annealing），即逐步降低温度以获得更优结果，提升ToM表现。

Result: 实验表明，通过序列级采样并配合退火机制，可以显著提升自回归语言模型在ToM相关任务上的表现，且不需要后训练或权重更新。

Conclusion: 采样优化方法可有效挖掘和利用语言模型的潜在能力，为无需再训练的能力增强提供了新思路。

Abstract: Autoregressive language models are next-token predictors and have been criticized for only optimizing surface plausibility (i.e., local coherence) rather than maintaining correct latent-state representations (i.e., global coherence). Because Theory of Mind (ToM) tasks crucially depend on reasoning about latent mental states of oneself and others, such models are therefore often thought to fail at ToM. While post-training methods can improve ToM performance, we show that strong ToM capability can be recovered directly from the base model without any additional weight updates or verifications. Our approach builds on recent power-sampling methods (Karan & Du, 2025) that use Markov chain Monte Carlo (MCMC) to sample from sharpened sequence-level (rather than token-level) probability distributions of autoregressive language models. We further find that incorporating annealing, where the tempered distribution is gradually shifted from high to low temperature, substantially improves ToM performance over fixed-temperature power sampling. Together, these results suggest that sampling-based optimization provides a powerful way to extract latent capabilities from language models without retraining.

</details>


### [318] [Conversational Context Classification: A Representation Engineering Approach](https://arxiv.org/abs/2601.12286)
*Jonathan Pan*

Main category: cs.CL

TL;DR: 本文通过结合Representation Engineering（RepE）与一类支持向量机（OCSVM），在LLM（大语言模型）内部表征空间中精确检测对话是否偏离特定语境。实验在Llama和Qwen两种开源模型上验证取得了良好结果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型日益普及，其错误生成离谱或脱离语境内容的风险逐渐突出，亟需高效机制精准检测LLM何时出现话题转移、事实错误或幻觉等异常，传统异常检测方法难以处理上下文语义空间。

Method: 本文采用Representation Engineering寻找能准确代表特定上下文的隐藏状态子空间，并结合OCSVM在这一子空间内建立语境判别边界，于Llama和Qwen等开源LLM上以具体领域数据为例进行训练与评价。

Result: 本方法能在LLM内部表征空间中有效界定特定语境的子空间，并且在识别对话线程是否属于设定语境上表现出色。

Conclusion: 通过精确划分LLM内部隐藏空间子域，本文不仅增强了异常检测与对话语境判别能力，还为LLM解释性研究提供了新的思路和方法。

Abstract: The increasing prevalence of Large Language Models (LLMs) demands effective safeguards for their operation, particularly concerning their tendency to generate out-of-context responses. A key challenge is accurately detecting when LLMs stray from expected conversational norms, manifesting as topic shifts, factual inaccuracies, or outright hallucinations. Traditional anomaly detection struggles to directly apply within contextual semantics. This paper outlines our experiment in exploring the use of Representation Engineering (RepE) and One-Class Support Vector Machine (OCSVM) to identify subspaces within the internal states of LLMs that represent a specific context. By training OCSVM on in-context examples, we establish a robust boundary within the LLM's hidden state latent space. We evaluate out study with two open source LLMs - Llama and Qwen models in specific contextual domain. Our approach entailed identifying the optimal layers within the LLM's internal state subspaces that strongly associates with the context of interest. Our evaluation results showed promising results in identifying the subspace for a specific context. Aside from being useful in detecting in or out of context conversation threads, this research work contributes to the study of better interpreting LLMs.

</details>


### [319] [Can Deep Research Agents Find and Organize? Evaluating the Synthesis Gap with Expert Taxonomies](https://arxiv.org/abs/2601.12369)
*Ming Zhang,Jiabao Zhuang,Wenqing Jing,Ziyu Kong,Jingyi Deng,Yujiong Shen,Kexin Tan,Yuhang Zhao,Ning Luo,Renzhe Zheng,Jiahui Lin,Mingqi Wu,Long Ma,Yi Zou,Shihan Dou,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 本论文提出了一个名为TaxoBench的新基准，用于评估深度研究智能体在文献综述写作中的核心能力，并发现这些智能体距离专家水平仍有较大差距。


<details>
  <summary>Details</summary>
Motivation: 尽管深度研究智能体被用于自动化综述生成，但其是否能像人类专家一样编写高质量的综述未明。目前评测标准主要关注流畅性或引文准确率，却没有专门考察智能体对关键文献的检索能力及知识结构的组织能力，因此迫切需要一个更细致的评测基准。

Method: 作者基于72篇高被引计算机科学综述，人工抽取了包含3,815条精确分类引文的专家知识结构（taxonomy trees），作为评测的真实标准。TaxoBench设计了两个评测模式，分别为：1）给定话题下，测试智能体端到端的检索与组织能力的“Deep Research模式”；2）给定与专家一致的文献，考查结构化能力的“Bottom-Up模式”。并用它评估了7个主流研究智能体和12个前沿LLM模型。

Result: 实验结果显示，无论是文献检索还是结构组织，两方面的能力都有限。例如，最强智能体的召回率仅为20.9%，即仅找到不到1/5的专家选文；在已知文献前提下，最佳模型实现的Adjusted Rand Index只有0.31，显示其结构构建能力很弱。

Conclusion: 当前深度研究智能体在综述生成尤其是文献筛选和知识结构组织方面，离专家水平还有显著差距。TaxoBench为该领域能力评测提供了新工具，有助于未来智能综述系统的发展和改进。

Abstract: Deep Research Agents are increasingly used for automated survey generation. However, whether they can write surveys like human experts remains unclear. Existing benchmarks focus on fluency or citation accuracy, but none evaluates the core capabilities: retrieving essential papers and organizing them into coherent knowledge structures. We introduce TaxoBench, a diagnostic benchmark derived from 72 highly-cited computer science surveys. We manually extract expert-authored taxonomy trees containing 3,815 precisely categorized citations as ground truth. Our benchmark supports two evaluation modes: Deep Research mode tests end-to-end retrieval and organization given only a topic, while Bottom-Up mode isolates structuring capability by providing the exact papers human experts used. We evaluate 7 leading Deep Research agents and 12 frontier LLMs. Results reveal a dual bottleneck: the best agent recalls only 20.9% of expert-selected papers, and even with perfect input, the best model achieves only 0.31 ARI in organization. Current deep research agents remain far from expert-level survey writing. Our benchmark is publicly available at https://github.com/KongLongGeFDU/TaxoBench.

</details>


### [320] [A Scalable Entity-Based Framework for Auditing Bias in LLMs](https://arxiv.org/abs/2601.12374)
*Akram Elbouanani,Aboubacar Tuo,Adrian Popescu*

Main category: cs.CL

TL;DR: 本文提出了一种大规模、可扩展的偏见评估框架，利用实体探针系统性地揭示了主流大语言模型在多种任务和语言下的结构性偏见。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型偏见评估方法在生态有效性和统计可控性之间存在权衡，要么采用与真实应用场景不符的人为提示，要么使用缺乏数据规模和严谨性的自然任务，难以全面系统衡量模型偏见。

Method: 作者设计了一种以命名实体为切入点的探针，用合成数据和自然文本对大语言模型进行规模化评估，并在多种任务、模型、语言以及不同提示策略下，利用1.9亿个数据点分析结构性偏见分布。

Result: 研究发现主流大模型普遍惩罚右翼政治人物、偏爱左翼和西方/富裕国家、偏好西方企业及贬低国防与医药产业。此外，提升模型规模会放大偏见，指令微调虽能缓解但力度有限，不同语言提示也未减轻亲西方倾向。

Conclusion: 作者认为，大语言模型在高风险场景应用前，必须经过系统、严格的偏见审计，以规避可能的社会危害。

Abstract: Existing approaches to bias evaluation in large language models (LLMs) trade ecological validity for statistical control, relying on artificial prompts that poorly reflect real-world use, or on naturalistic tasks that lack scale and rigor. We introduce a scalable bias-auditing framework using named entities as probes to measure structural disparities in model behavior. We show that synthetic data reliably reproduces bias patterns observed in natural text, enabling large-scale analysis. Using this approach, we conduct the largest bias audit to date, comprising 1.9 billion data points across multiple entity types, tasks, languages, models, and prompting strategies. Our results reveal systematic biases: models penalize right-wing politicians, favor left-wing politicians, prefer Western and wealthy nations over the Global South, favor Western companies, and penalize firms in the defense and pharmaceutical sectors. While instruction tuning reduces bias, increasing model scale amplifies it, and prompting in Chinese or Russian does not attenuate Western-aligned preferences. These results indicate that LLMs should undergo rigorous auditing before deployment in high-stakes applications.

</details>


### [321] [LR-DWM: Efficient Watermarking for Diffusion Language Models](https://arxiv.org/abs/2601.12376)
*Ofek Raban,Ethan Fetaya,Gal Chechik*

Main category: cs.CL

TL;DR: 本文提出了一种针对扩散语言模型（DLM）的高效水印方法LR-DWM，该方法能在几乎不增加计算和内存开销的情况下实现高可靠性的水印检测。


<details>
  <summary>Details</summary>
Motivation: 现有主流水印方法主要针对自回归大语言模型，难以直接应用于非顺序生成的扩散语言模型，而已有的DLM水印方案普遍存在计算或内存开销较大等现实问题。

Method: 提出了左-右扩散水印（LR-DWM）技术，通过利用生成文本过程中可用的左右邻居令牌，对当前生成的令牌引入偏置，实现水印的无缝嵌入。相比已有方法，LR-DWM在生成流程中几乎不增加额外负担。

Result: 实验表明，LR-DWM能在不显著增加DLM运行时和内存消耗的前提下，有效嵌入水印且保持较高的水印检测准确率，几乎与无水印DLM基线等同。

Conclusion: LR-DWM提供了扩散语言模型高效、安全的水印解决方案，使扩散模型的AI内容检测与归因更可行和便捷。

Abstract: Watermarking (WM) is a critical mechanism for detecting and attributing AI-generated content. Current WM methods for Large Language Models (LLMs) are predominantly tailored for autoregressive (AR) models: They rely on tokens being generated sequentially, and embed stable signals within the generated sequence based on the previously sampled text. Diffusion Language Models (DLMs) generate text via non-sequential iterative denoising, which requires significant modification to use WM methods designed for AR models. Recent work proposed to watermark DLMs by inverting the process when needed, but suffers significant computational or memory overhead. We introduce Left-Right Diffusion Watermarking (LR-DWM), a scheme that biases the generated token based on both left and right neighbors, when they are available. LR-DWM incurs minimal runtime and memory overhead, remaining close to the non-watermarked baseline DLM while enabling reliable statistical detection under standard evaluation settings. Our results demonstrate that DLMs can be watermarked efficiently, achieving high detectability with negligible computational and memory overhead.

</details>


### [322] [NADIR: Differential Attention Flow for Non-Autoregressive Transliteration in Indic Languages](https://arxiv.org/abs/2601.12389)
*Lakshya Tomar,Vinayak Abrol,Puneet Agarwal*

Main category: cs.CL

TL;DR: 本文提出了NADIR，一种非自回归（NAR）模型结构，实现了序列到序列任务中速度与准确性的平衡，特别适用于多语种音译等只需局部依赖的场景，大幅提升推理速度并有效降低各种错误率。


<details>
  <summary>Details</summary>
Motivation: 传统的自回归（AR）模型虽然准确率高，但推理速度慢，且其强归纳偏置对某些只需局部依赖的序列到序列任务如多语种音译、代码重构、语法纠错等并非必要。现有NAR模型虽然更快，但存在虚假生成和长度控制等问题。作者希望寻求速度和准确性的更佳平衡，满足实际大规模实时需求。

Method: 作者以印度语多语种音译任务为例，提出了NADIR模型。该模型基于差分Transformer并融合了Mixture-of-Experts机制，以非自回归方式对复杂字符映射建模，显著提升了模型稳健性并避免了序列依赖。

Result: NADIR在速度上较最优的AR基线模型快13倍以上，同时保持了15.78%的平均字符错误率，接近AR模型（14.44%），优于普通NAR（21.88%）。此外，NADIR在重复、替换、遗漏、插入等错误类型上分别减少了49.53%、24.45%、32.92%和16.87%。

Conclusion: NADIR有效弥补了AR模型和NAR模型的缺口，为构建快速、可靠的NAR系统提供了范例，适合实时和大规模部署需求。

Abstract: In this work, we argue that not all sequence-to-sequence tasks require the strong inductive biases of autoregressive (AR) models. Tasks like multilingual transliteration, code refactoring, grammatical correction or text normalization often rely on local dependencies where the full modeling capacity of AR models can be overkill, creating a trade-off between their high accuracy and high inference latency. While non-autoregressive (NAR) models offer speed, they typically suffer from hallucinations and poor length control. To explore this trade-off, we focus on the multilingual transliteration task in Indic languages and introduce NADIR, a novel NAR architecture designed to strike a balance between speed and accuracy. NADIR integrates a Differential Transformer and a Mixture-of-Experts mechanism, enabling it to robustly model complex character mappings without sequential dependencies. NADIR achieves over a 13x speed-up compared to the state-of-the-art AR baseline. It maintains a competitive mean Character Error Rate of 15.78%, compared to 14.44% for the AR model and 21.88% for a standard NAR equivalent. Importantly, NADIR reduces Repetition errors by 49.53%, Substitution errors by 24.45%, Omission errors by 32.92%, and Insertion errors by 16.87%. This work provides a practical blueprint for building fast and reliable NAR systems, effectively bridging the gap between AR accuracy and the demands of real-time, large-scale deployment.

</details>


### [323] [Legal experts disagree with rationale extraction techniques for explaining ECtHR case outcome classification](https://arxiv.org/abs/2601.12419)
*Mahammad Namazov,Tomáš Koref,Ivan Habernal*

Main category: cs.CL

TL;DR: 本文提出了一个针对法律领域大语言模型可解释性方法的对比分析框架，对多种模型无关的可解释技术进行了评估，并结合法律专家的评价，发现模型给出的预测理由与专家存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 在法律领域中，模型的可解释性尤为重要，因为法律应用需要高度的信任和透明性。然而，目前尚不清楚哪种可解释方法最适合用于法律结果预测，这一问题有待解决。

Method: 提出了一个模型无关的可解释性方法对比分析框架。采用了两种“理由”提取（rationale extraction）方法，从输入文本中抽取可供人类理解且精炼的文本片段，并通过定量（如normalized sufficiency和comprehensiveness）和法律专家的定性评估（plausibility）进行比较。同时，评估了“大语言模型作为裁判”的可行性。

Result: 定量分析表明各方法在数据上的表现都较好，但专家评价显示模型给出的“预测理由”与法律专家的看法存在显著差异。

Conclusion: 尽管大语言模型在法律判决预测任务上取得了良好性能，但其解释理由与人类专家存在明显分歧，因此在法律领域部署时需谨慎解读其输出与理由。

Abstract: Interpretability is critical for applications of large language models in the legal domain which requires trust and transparency. While some studies develop task-specific approaches, other use the classification model's parameters to explain the decisions. However, which technique explains the legal outcome prediction best remains an open question. To address this challenge, we propose a comparative analysis framework for model-agnostic interpretability techniques. Among these, we employ two rationale extraction methods, which justify outcomes with human-interpretable and concise text fragments (i.e., rationales) from the given input text. We conduct comparison by evaluating faithfulness-via normalized sufficiency and comprehensiveness metrics along with plausibility-by asking legal experts to evaluate extracted rationales. We further assess the feasibility of LLM-as-a-Judge using legal expert evaluation results. We show that the model's "reasons" for predicting a violation differ substantially from those of legal experts, despite highly promising quantitative analysis results and reasonable downstream classification performance. The source code of our experiments is publicly available at https://github.com/trusthlt/IntEval.

</details>


### [324] [System-Mediated Attention Imbalances Make Vision-Language Models Say Yes](https://arxiv.org/abs/2601.12430)
*Tsan Tsai Chan,Varsha Suresh,Anisha Saha,Michael Hahn,Vera Demberg*

Main category: cs.CL

TL;DR: 本论文指出视觉-语言模型（VLM）幻觉现象主要源于系统模态与图像、文本间注意力分配不均，提出通过重新分配系统注意力大幅减少常见的“yes偏向”幻觉表现，优于现有方法，并强调系统注意力调节在幻觉缓解中的重要作用。


<details>
  <summary>Details</summary>
Motivation: 现有VLM幻觉缓解方法偏重图像注意分配，忽视系统与多模态间的平衡，导致对幻觉现象尤其是“yes偏向”的成因理解不足，因此作者希望从整体系统角度探讨根本原因并改进缓解策略。

Method: 作者提出一种系统中介的注意力分配视角，将部分系统注意力重新分配到图像与文本输入上，通过实验证明该做法对抑制“yes偏向”幻觉有显著效果，并与现有技术进行对比。

Result: 实验表明，重分配系统注意力不仅有效压制了VLM的“yes偏向”幻觉，在多数情况下优于现有只注重提升图像注意的方法。同时，进一步分析揭示系统注意力不均导致VLM对粗略输入特征的依赖偏高，加剧幻觉现象。

Conclusion: 系统注意力分配在VLM幻觉发生机制中至关重要，对其适当干预能够有效缓解幻觉并优于传统方法，为多模态模型幻觉治理提供了新的研究方向。

Abstract: Vision-language model (VLM) hallucination is commonly linked to imbalanced allocation of attention across input modalities: system, image and text. However, existing mitigation strategies tend towards an image-centric interpretation of these imbalances, often prioritising increased image attention while giving less consideration to the roles of the other modalities. In this study, we evaluate a more holistic, system-mediated account, which attributes these imbalances to functionally redundant system weights that reduce attention to image and textual inputs. We show that this framework offers a useful empirical perspective on the yes-bias, a common form of hallucination in which VLMs indiscriminately respond 'yes'. Causally redistributing attention from the system modality to image and textual inputs substantially suppresses this bias, often outperforming existing approaches. We further present evidence suggesting that system-mediated attention imbalances contribute to the yes-bias by encouraging a default reliance on coarse input representations, which are effective for some tasks but ill-suited to others. Taken together, these findings firmly establish system attention as a key factor in VLM hallucination and highlight its potential as a lever for mitigation.

</details>


### [325] [Incentivizing In-depth Reasoning over Long Contexts with Process Advantage Shaping](https://arxiv.org/abs/2601.12465)
*Miao Peng,Weizhou Shen,Nuo Chen,Chenliang Li,Ming Yan,Jia Li*

Main category: cs.CL

TL;DR: 本文提出了DeepReasonQA数据合成框架和LongPAS奖励分配方法，有效提升了LLM在长上下文推理任务中的表现，显著优于现有RLVR方法。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在长上下文推理（long-context reasoning）时遇到'差一点'现象（almost-there），即模型大部分推理正确但在最后一步出错。分析认为这是因为长上下文QA数据缺乏高密度多跳推理设计，且RL训练中对部分正确轨迹惩罚过重，导致学习信号损失。

Method: 提出DeepReasonQA框架基于知识图谱自动生成具有高推理难度的多跳长上下文QA问题，从源头提升数据质量。同时提出LongPAS方法，对推理过程中每一步按有效性与相关性细粒度赋分，使部分正确的推理轨迹能贡献关键信号。

Result: 在三个长上下文推理基准上，本文方法相比传统RLVR显著提升了性能，并用更少的参数达到当前主流大模型的水平。

Conclusion: DeepReasonQA与LongPAS方案有效解决了长上下文推理中部分正确轨迹无效学习信号的问题，大幅提升模型推理与训练稳定性，为长上下文理解奠定了更坚实基础。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective in enhancing LLMs short-context reasoning, but its performance degrades in long-context scenarios that require both precise grounding and robust long-range reasoning. We identify the "almost-there" phenomenon in long-context reasoning, where trajectories are largely correct but fail at the final step, and attribute this failure to two factors: (1) the lack of high reasoning density in long-context QA data that push LLMs beyond mere grounding toward sophisticated multi-hop reasoning; and (2) the loss of valuable learning signals during long-context RL training due to the indiscriminate penalization of partially correct trajectories with incorrect outcomes. To overcome this bottleneck, we propose DeepReasonQA, a KG-driven synthesis framework that controllably constructs high-difficulty, multi-hop long-context QA pairs with inherent reasoning chains. Building on this, we introduce Long-context Process Advantage Shaping (LongPAS), a simple yet effective method that performs fine-grained credit assignment by evaluating reasoning steps along Validity and Relevance dimensions, which captures critical learning signals from "almost-there" trajectories. Experiments on three long-context reasoning benchmarks show that our approach substantially outperforms RLVR baselines and matches frontier LLMs while using far fewer parameters. Further analysis confirms the effectiveness of our methods in strengthening long-context reasoning while maintaining stable RL training.

</details>


### [326] [Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty](https://arxiv.org/abs/2601.12471)
*Sravanthi Machcha,Sushrita Yerra,Sahil Gupta,Aishwarya Sahoo,Sharmin Sultana,Hong Yu,Zonghai Yao*

Main category: cs.CL

TL;DR: 本文提出了MedAbstain基准，评估大语言模型（LLM）在医疗多选题中的放弃能力，并发现当前模型在不确定时往往无法合理放弃。显式提供放弃选项可提高安全性，对高风险应用有重要指导意义。


<details>
  <summary>Details</summary>
Motivation: 现实和高风险应用中，准确性之外，模型能在不确定时选择放弃以避免错误更加重要，因此需要系统性评价LLM的放弃能力。

Method: 构建了MedAbstain基准和评估流程，整合了共形预测、对抗性题目扰动和显式放弃选项，系统评测多个开源与闭源LLM放弃能力。

Result: 即使是高准确度的顶尖模型，在不确定情况下也难以选择放弃。显式放弃选项能显著增加模型不确定性和安全性放弃，而增大模型规模或高级提示带来改善有限。

Conclusion: 放弃机制对可信LLM部署至关重要，显式放弃选项是提升高风险领域安全性的有效手段，建议实际应用时采用和优化放弃策略。

Abstract: Current evaluation of large language models (LLMs) overwhelmingly prioritizes accuracy; however, in real-world and safety-critical applications, the ability to abstain when uncertain is equally vital for trustworthy deployment. We introduce MedAbstain, a unified benchmark and evaluation protocol for abstention in medical multiple-choice question answering (MCQA) -- a discrete-choice setting that generalizes to agentic action selection -- integrating conformal prediction, adversarial question perturbations, and explicit abstention options. Our systematic evaluation of both open- and closed-source LLMs reveals that even state-of-the-art, high-accuracy models often fail to abstain with uncertain. Notably, providing explicit abstention options consistently increases model uncertainty and safer abstention, far more than input perturbations, while scaling model size or advanced prompting brings little improvement. These findings highlight the central role of abstention mechanisms for trustworthy LLM deployment and offer practical guidance for improving safety in high-stakes applications.

</details>


### [327] [Capability-Aware Early-Stage Research Idea Evaluation](https://arxiv.org/abs/2601.12473)
*Renlong Jie,Chen Chu,Zhen Wang*

Main category: cs.CL

TL;DR: 本论文提出了一种仅依赖作者信息和研究想法，无需完整论文文本或实验结果，即可预测论文录用及评分的新方法。该方法通过三向transformer架构融合作者能力与研究想法，在早期评估与资源分配中展现出显著优势。


<details>
  <summary>Details</summary>
Motivation: 现有研究成果预测方法主要依赖已完成稿件或同行评议，但学术资源分配和研究选题更需在立项阶段预判其前景，因此亟需能够在早期、不依赖详细实验和全文的可行预测策略。

Method: 1. 引入一种能力感知预测框架，只利用作者背景信息和研究想法。2. 采用三向transformer架构，将作者信息、能力表示和研究想法进行融合，并提供灵活的融合机制。3. 设计两阶段结构，首先从作者与想法中学习能力表示。4. 在bert-base和bert-large微调并对比，验证模型性能。

Result: 实验结果显示，该方法在预测论文录用及评分时，表现明显优于仅用bert微调的单向模型。加入能力预测后，模型准确率大幅提升。

Conclusion: 该方法可有效应用于科研早期产出预测和科学资源优化分配，减少对已完成论文的依赖，提升学术资源利用效率。

Abstract: Predicting the outcomes of research ideas at their conceptual stage (i.e. before significant resources are committed) holds great potential for optimizing scientific resource allocation and research planning. While existing methods rely heavily on finished manuscripts or peer reviews, we propose a novel capability-aware framework that predicts paper acceptance and ratings using only author information and research ideas, without requiring full text or experimental results. Our approach integrates author information, (inferred) capability presentation, and research ideas through a three-way transformer architecture with flexible fusion mechanisms. We also introduce a two-stage architecture for learning the capability representation given the author information and idea. Experiments show that our method significantly outperform the single-way models by finetuning bert-base and bert-large, and the capability predicting significantly increase the predictive accuracy of the final model. The proposed method can be applied in both early-stage research outcome prediction and scientific resource allocation.

</details>


### [328] [DoPE: Decoy Oriented Perturbation Encapsulation Human-Readable, AI-Hostile Documents for Academic Integrity](https://arxiv.org/abs/2601.12505)
*Ashish Raj Shekhar,Shiven Agarwal,Priyanuj Bordoloi,Yash Shah,Tejas Anvekar,Vivek Gupta*

Main category: cs.CL

TL;DR: 本文提出了DoPE（诱饵型扰动封装）框架，通过在PDF/HTML考试文档中嵌入语义诱饵，有效防御并检测大型多模态语言模型（MLLMs）对学术考试的自动化作弊。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型能直接解析考试文档，传统的考核方式和学术诚信面临重大威胁，亟需新型防御手段来抑制AI作弊。

Method: DoPE框架在考试文档编写阶段，将语义诱饵嵌入PDF或HTML试卷，基于LLM指导的FewSoRT-Q生成题级诱饵，FewSoRT-D实现诱饵水印文档封装。该方法不依赖单一分类器，对不同模型通用。

Result: 在Integrity-Bench数据集（1826份PDF+HTML考试）上，DoPE对主流开放式MLLM拥有91.4%的检测率和8.7%的误报率，自动防御/诱导失败率达96.3%。

Conclusion: DoPE为学术文件防御AI作弊提供了有效、可迁移的解决思路，并公开了相关数据集和工具，推动文档层防御研究。

Abstract: Multimodal Large Language Models (MLLMs) can directly consume exam documents, threatening conventional assessments and academic integrity. We present DoPE (Decoy-Oriented Perturbation Encapsulation), a document-layer defense framework that embeds semantic decoys into PDF/HTML assessments to exploit render-parse discrepancies in MLLM pipelines. By instrumenting exams at authoring time, DoPE provides model-agnostic prevention (stop or confound automated solving) and detection (flag blind AI reliance) without relying on conventional one-shot classifiers. We formalize prevention and detection tasks, and introduce FewSoRT-Q, an LLM-guided pipeline that generates question-level semantic decoys and FewSoRT-D to encapsulate them into watermarked documents. We evaluate on Integrity-Bench, a novel benchmark of 1826 exams (PDF+HTML) derived from public QA datasets and OpenCourseWare. Against black-box MLLMs from OpenAI and Anthropic, DoPE yields strong empirical gains: a 91.4% detection rate at an 8.7% false-positive rate using an LLM-as-Judge verifier, and prevents successful completion or induces decoy-aligned failures in 96.3% of attempts. We release Integrity-Bench, our toolkit, and evaluation code to enable reproducible study of document-layer defenses for academic integrity.

</details>


### [329] [Improving Low-Resource Machine Translation via Round-Trip Reinforcement Learning](https://arxiv.org/abs/2601.12535)
*Ahmed Attia,Alham Fikri*

Main category: cs.CL

TL;DR: 本研究提出了一种基于自监督强化学习的低资源机器翻译微调方法，并在多个低资源语言上实现了翻译质量提升。


<details>
  <summary>Details</summary>
Motivation: 当前低资源语言的机器翻译面临平行语料稀缺问题，尽管已有部分改进方法，但仍有许多可能性尚未被充分探索。

Method: 采用No Language Left Behind（NLLB）模型，通过英译目标语后再译回英文（round-trip bootstrapping），结合chrF++和BLEU作为奖励函数进行强化学习微调。

Result: 在NLLB-MD数据集上，实验对NLLB的600M和1.3B参数模型进行评估，针对Central Aymara、Friulian、Wolof和Russian等语言均有稳定提升，定性分析显示流畅度和语义保真度提高。

Conclusion: 该方法有效提升了低资源机器翻译表现，且具有进一步扩展和自我改进的潜力。

Abstract: Low-resource machine translation (MT) has gained increasing attention as parallel data from low-resource language communities is collected, but many potential methods for improving low-resource MT remain unexplored. We investigate a self-supervised reinforcement-learning-based fine-tuning for translation in low-resource settings using round-trip bootstrapping with the No Language Left Behind (NLLB) family of models. Our approach translates English into a target low-resource language and then back into English, using a combination of chrF++ and BLEU as the reward function on the reconstructed English sentences. Using the NLLB-MD dataset, we evaluate both the 600M and 1.3B parameter NLLB models and observe consistent improvements for the following languages: Central Aymara, Friulian, Wolof and Russian. Qualitative inspection of translation outputs indicates increased fluency and semantic fidelity. We argue that our method can further benefit from scale, enabling models to increasingly leverage their pretrained knowledge and continue self-improving.

</details>


### [330] [Benchmarking Concept-Spilling Across Languages in LLMs](https://arxiv.org/abs/2601.12549)
*Ilia Badanin,Daniil Dzenhaliou,Imanol Schlag*

Main category: cs.CL

TL;DR: 本文提出了一种新的对比框架，用于系统评估多语言大模型在跨语义理解和生成中的鲁棒性，特别关注于非英语内容中由其他语言信息溢出导致的语义干扰现象。


<details>
  <summary>Details</summary>
Motivation: 多语言大模型虽然具备优秀的跨语种能力，但在生成非英语内容时常受其他语言（特别是英语）影响，出现语义溢出和干扰。解决这一问题对于构建语言更加平衡和鲁棒的AI系统至关重要。

Method: 作者提出一种新的对比评测框架，通过多语种下多义词义生成任务系统比较模型的语义鲁棒性。具体方法包括设定强制模型生成五种义项，观察模型在耗尽目标语真实义项后，何时开始借用主导语种（如英语）的意义等。用九种语言和百个高多义性的英文词，构建标准化评测集，对不同类型多语种LLM（开源和闭源）进行实验。

Result: 实验展现模型和语言间在语义鲁棒性上的显著差异，语义较强的模型能坚持更长时间生成目标语言意义，而较弱的模型则更快转向主导语种意义。基于此，作者建立了无需溯源错误原因的模型表现分级体系。

Conclusion: 本研究贡献了可扩展的多语言语义对比评测基准和严谨的验证流程，为未来开发更均衡的多语言AI系统提供了关键工具和方法。

Abstract: Multilingual Large Language Models (LLMs) exhibit remarkable cross-lingual abilities, yet often exhibit a systematic bias toward the representations from other languages, resulting in semantic interference when generating content in non-English languages$-$a phenomenon we define as language spilling. This paper presents a novel comparative framework for evaluating multilingual semantic robustness by systematically measuring how models handle polysemous words across languages. Our methodology provides a relative measure of model performance: when required to generate exactly five meanings, both strong and weak models may resort to meanings from dominant languages, but semantically stronger models do so later in the generation sequence, producing more true meanings from the target language before failing, while weaker models resort to dominant-language meanings earlier in the sequence. We evaluate a diverse set of open and closed multilingual LLMs using a structured meaning generation task across nine languages, employing a carefully curated benchmark of 100 high-polysemy English words. Our findings reveal significant variation in semantic robustness across both models and languages, providing a principled ranking system for model comparison without requiring definitive causal attribution of error sources. We contribute both a scalable comparative benchmark for multilingual semantic evaluation and a rigorous validation pipeline$-$critical tools for developing more linguistically balanced AI systems.

</details>


### [331] [Evaluating Contextually Mediated Factual Recall in Multilingual Large Language Models](https://arxiv.org/abs/2601.12555)
*Yihong Liu,Bingyu Xiong,Hinrich Schütze*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型（LLM）在不同语境下召回事实知识的能力，发现语境媒介会明显降低其准确率，且模型越大对语境的鲁棒性越强。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM能跨语言地回忆大量事实知识，但现有评估方法主要集中在直接、明确提问的情景，与真实交流中通过语境间接获取事实的方式存在差距。因此，研究模型在自然语境下召回事实的能力变得重要。

Method: 作者设计了一组严格控制的提示，保持事实不变，同时通过语境句引入间接提及目标实体。此外，为了排除与名字相关的特定影响，实验分别使用真实姓名和合成姓名，并在五种语言下评估多种模型。

Result: 结果显示，语境媒介使得事实召回能力普遍下降，且在不同关系类型上的表现差异较大。大模型对语境干扰更具鲁棒性，两种查询间性能差距更小。使用真实姓名和名字来源的影响没有呈现一致规律。

Conclusion: 研究揭示了多语种LLM在隔离事实召回与基于语境理解之间的能力差距，强调了提升其语境依赖理解能力的重要性。

Abstract: Large language models (LLMs) can recall a wide range of factual knowledge across languages. However, existing factual recall evaluations primarily assess fact retrieval in isolation, where the queried entity is explicitly named and the fact is requested directly. In natural language use, facts are often accessed through context, where the relevant entity is introduced only indirectly. In this work, we study contextually mediated factual recall, asking whether LLMs can reliably retrieve factual knowledge when the target entity is embedded in a naturalistic context rather than queried explicitly, across languages. We construct controlled prompts that preserve the underlying fact while introducing referential mediation through contextual sentences. To disentangle contextual effects from name-specific associations, we further compare performance using synthetic names and real names across languages. Evaluating multiple model families in five languages, we find that contextual mediation consistently degrades factual recall, with substantial variation across relations. Larger models are more robust to contextual mediation, exhibiting a reduced performance gap relative to direct queries, while the effect of real names and name origin is mixed and unsystematic. These findings highlight a gap between isolated factual recall and context-dependent language understanding in multilingual LLMs.

</details>


### [332] [A Cloud-based Multi-Agentic Workflow for Science](https://arxiv.org/abs/2601.12607)
*Anurag Acharya,Timothy Vega,Rizwan A. Ashraf,Anshu Sharma,Derek Parker,Robert Rallo*

Main category: cs.CL

TL;DR: 论文提出了一个通用、模型无关的智能体框架，可以在云端协作完成科学研究任务，兼顾简单与复杂任务，并验证了其在化学催化剂研究等领域的有效性和经济性。


<details>
  <summary>Details</summary>
Motivation: LLM在科学领域广泛应用，但单靠LLM难以完成复杂仿真或决策任务。为弥补这一缺陷，LLM智能体具备调用外部工具的能力，但实际实施此类系统流程复杂，难以应用。

Method: 作者设计了一个包含监督智能体和多个功能单一子智能体的框架，能够将文献综述、数据分析和仿真等任务自动分配给合适的智能体，并在云端运行。通过真实应用（催化剂研究）、成本分析、定制与标准基准测试，以及专家评审，全面评估了系统表现。

Result: 系统能在90%的情况下正确分配任务，并成功完成97.5%（合成任务）和91%（真实任务）的任务成功率，准确度高于或可比于主流大模型，并详细报告了云资源使用成本。

Conclusion: 本框架不仅提升了LLM智能体在科学研究的实用性和可操作性，也展现出良好的泛化能力和推广价值，适用于各类科学领域。

Abstract: As Large Language Models (LLMs) become ubiquitous across various scientific domains, their lack of ability to perform complex tasks like running simulations or to make complex decisions limits their utility. LLM-based agents bridge this gap due to their ability to call external resources and tools and thus are now rapidly gaining popularity. However, coming up with a workflow that can balance the models, cloud providers, and external resources is very challenging, making implementing an agentic system more of a hindrance than a help. In this work, we present a domain-agnostic, model-independent workflow for an agentic framework that can act as a scientific assistant while being run entirely on cloud. Built with a supervisor agent marshaling an array of agents with individual capabilities, our framework brings together straightforward tasks like literature review and data analysis with more complex ones like simulation runs. We describe the framework here in full, including a proof-of-concept system we built to accelerate the study of Catalysts, which is highly important in the field of Chemistry and Material Science. We report the cost to operate and use this framework, including the breakdown of the cost by services use. We also evaluate our system on a custom-curated synthetic benchmark and a popular Chemistry benchmark, and also perform expert validation of the system. The results show that our system is able to route the task to the correct agent 90% of the time and successfully complete the assigned task 97.5% of the time for the synthetic tasks and 91% of the time for real-world tasks, while still achieving better or comparable accuracy to most frontier models, showing that this is a viable framework for other scientific domains to replicate.

</details>


### [333] [Disagreement as Data: Reasoning Trace Analytics in Multi-Agent Systems](https://arxiv.org/abs/2601.12618)
*Elham Tajik,Conrad Borchers,Bahar Shahrokhian,Sebastian Simon,Ali Keramati,Sonika Pal,Sreecharan Sankaranarayanan*

Main category: cs.CL

TL;DR: 利用大语言模型（LLM）在定性编码时生成的推理轨迹，通过余弦相似度度量分析代理间的一致和分歧，提高定性分析的解释性与方法标准。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI自动化和人机协同分析定性数据方法逐渐普及，但缺乏有效的流程和评估标准。作者关注于提升分析的解释性和冗余性，尤其是在用LLM辅助定性编码时如何度量和利用分歧。

Method: 提出将LLM多智能体系统下的推理轨迹作为新型过程数据，采用余弦相似度定量分析智能体间推理分歧，并在近一万个人类辅导对话编码案例中进行了实证分析，结合定性分析探讨细致的功能和编码体系改进。

Result: LLM推理相似度能有效区分共识与分歧，并与人类编码可靠性高度相关。结合相似度指标与人工复查，揭示了编码中的细微功能和对编码表的改进空间。

Conclusion: 方法能提升定性编码一致性判定的效率和深度，推理分歧可以作为新颖的分析信号，推动教育研究中方法的严谨性和解释力提升。

Abstract: Learning analytics researchers often analyze qualitative student data such as coded annotations or interview transcripts to understand learning processes. With the rise of generative AI, fully automated and human-AI workflows have emerged as promising methods for analysis. However, methodological standards to guide such workflows remain limited. In this study, we propose that reasoning traces generated by large language model (LLM) agents, especially within multi-agent systems, constitute a novel and rich form of process data to enhance interpretive practices in qualitative coding. We apply cosine similarity to LLM reasoning traces to systematically detect, quantify, and interpret disagreements among agents, reframing disagreement as a meaningful analytic signal. Analyzing nearly 10,000 instances of agent pairs coding human tutoring dialog segments, we show that LLM agents' semantic reasoning similarity robustly differentiates consensus from disagreement and correlates with human coding reliability. Qualitative analysis guided by this metric reveals nuanced instructional sub-functions within codes and opportunities for conceptual codebook refinement. By integrating quantitative similarity metrics with qualitative review, our method has the potential to improve and accelerate establishing inter-rater reliability during coding by surfacing interpretive ambiguity, especially when LLMs collaborate with humans. We discuss how reasoning-trace disagreements represent a valuable new class of analytic signals advancing methodological rigor and interpretive depth in educational research.

</details>


### [334] [BioPulse-QA: A Dynamic Biomedical Question-Answering Benchmark for Evaluating Factuality, Robustness, and Bias in Large Language Models](https://arxiv.org/abs/2601.12632)
*Kriti Bhattarai,Vipina K. Keloth,Donald Wright,Andrew Loza,Yang Ren,Hua Xu*

Main category: cs.CL

TL;DR: 本文提出了BioPulse-QA，这是一个用于评估大语言模型（LLM）在生物医学领域问答能力的新基准，涵盖最新药品标签、临床试验协议和指南，并分析了多款主流模型的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的生物医学LLM评测基准多基于静态、过时的数据，难以反映生物医学知识的时效性、丰富语境和严肃性，还存在数据泄露及忽视语言变异鲁棒性与人口偏见的问题。因此，迫切需要一个覆盖最新内容、具备临床相关性的新型基准。

Method: 作者构建了BioPulse-QA，包含2,280组专家验证的问答对及其扰动版本，涵盖摘录型和生成型答案，数据来源于最新发表的药品标签、试验方案和临床指南，并测试了GPT-4o、GPT-o1、Gemini-2.0-Flash、LLaMA-3.1 8B Instruct等四种模型的表现。

Result: 在药品标签任务中，GPT-o1达到最高的宽松F1分数（0.92），Gemini-2.0-Flash次之（0.90）；临床试验相关任务最具挑战，抽取型问答F1最低至0.36。模型在语义复述问题上的性能分化超过拼写扰动，对人口偏见影响甚微。

Conclusion: BioPulse-QA为生物医学LLM提供了一个具规模性和临床相关性的全新评估框架，弥补了现有基准的缺陷，将有助于模型更好地适应真实世界的临床问答需求。

Abstract: Objective: Large language models (LLMs) are increasingly applied in biomedical settings, and existing benchmark datasets have played an important role in supporting model development and evaluation. However, these benchmarks often have limitations. Many rely on static or outdated datasets that fail to capture the dynamic, context-rich, and high-stakes nature of biomedical knowledge. They also carry increasing risk of data leakage due to overlap with model pretraining corpora and often overlook critical dimensions such as robustness to linguistic variation and potential demographic biases.
  Materials and Methods: To address these gaps, we introduce BioPulse-QA, a benchmark that evaluates LLMs on answering questions from newly published biomedical documents including drug labels, trial protocols, and clinical guidelines. BioPulse-QA includes 2,280 expert-verified question answering (QA) pairs and perturbed variants, covering both extractive and abstractive formats. We evaluate four LLMs - GPT-4o, GPT-o1, Gemini-2.0-Flash, and LLaMA-3.1 8B Instruct - released prior to the publication dates of the benchmark documents.
  Results: GPT-o1 achieves the highest relaxed F1 score (0.92), followed by Gemini-2.0-Flash (0.90) on drug labels. Clinical trials are the most challenging source, with extractive F1 scores as low as 0.36.
  Discussion and Conclusion: Performance differences are larger for paraphrasing than for typographical errors, while bias testing shows negligible differences. BioPulse-QA provides a scalable and clinically relevant framework for evaluating biomedical LLMs.

</details>


### [335] [Objective Matters: Fine-Tuning Objectives Shape Safety, Robustness, and Persona Drift](https://arxiv.org/abs/2601.12639)
*Daniel Vennemeyer,Punya Syon Pandey,Phan Anh Duong,Michael Umeokoli,Samuel Ratnam*

Main category: cs.CL

TL;DR: 本研究系统比较了六种LLM微调目标对安全性和能力的影响，发现随训练规模增大，不同微调目标对模型安全和能力的权衡影响显著变化。


<details>
  <summary>Details</summary>
Motivation: 目前关于LLM在良性数据上微调会削弱对齐性和对抗鲁棒性的现象已有观察，但不同微调目标在这一过程中的具体作用仍缺乏直接分析。作者希望通过实验控制，揭示微调目标本身如何影响模型安全性和能力。

Method: 在保持数据、领域、架构和优化相同的条件下，系统、对比地评估了六种微调目标：有监督微调（SFT）、直接偏好优化（DPO）、条件微调、inoculation prompting、赔率比偏好优化（ORPO）和KL正则化微调。比较它们在封闭式推理和开放式生成任务上的性能表现。

Result: 在训练预算较小时，不同微调目标对对抗鲁棒性的影响类似，但能力有差异。随着训练预算提升，各目标效果开始分化：有监督和偏好驱动微调提升能力的同时，也提升了对抗脆弱性和角色漂移风险，而ORPO和KL正则化等约束信号的目标则显著缓解这些问题。

Conclusion: 微调目标在小规模训练时对安全性的影响有限，但随着训练规模扩大，微调目标成为提升对抗鲁棒性和角色稳定性的主要因素。指导大模型安全微调时需特别关注微调目标的选择。

Abstract: Fine-tuning LLMs on benign data can still degrade alignment and adversarial robustness, yet direct analysis of the role of fine-tuning objectives in shaping these safety outcomes remain limited. We present a controlled comparison of six fine-tuning objectives -- Supervised Fine-Tuning, Direct Preference Optimization, Conditional Fine-Tuning, Inoculation Prompting, Odds Ratio Preference Optimization, and KL-regularized fine-tuning -- holding data, domain, architecture, and optimization fixed. Across closed-form reasoning and open-ended generation tasks, we find that objective choice induces systematic, scale-dependent shifts along the safety-capability frontier. At small training budgets, robustness is similar across objectives but capability differs. At larger budgets, objectives diverge sharply: supervised and preference-based tuning tightly couple capability gains to increased adversarial vulnerability and persona drift, while objectives that constrain learning signals -- especially ORPO and KL-regularization -- substantially mitigate both. Fine-tuning objectives therefore matter little for safety at small scales but become a primary driver of adversarial robustness and latent persona stability as training scale increases.

</details>


### [336] [Intelligent Documentation in Medical Education: Can AI Replace Manual Case Logging?](https://arxiv.org/abs/2601.12648)
*Nafiz Imtiaz Khan,Kylie Cleland,Vladimir Filkov,Roger Eric Goldman*

Main category: cs.CL

TL;DR: 本研究探讨了大语言模型（LLM）能否自动从自由文本的放射学报告中生成操作记录日志，结果显示LLM具有高效、准确的提取能力，有望简化相关文书工作。


<details>
  <summary>Details</summary>
Motivation: 放射学培训中，手动填写手术日志费时且易出错。作者希望通过AI自动化解决这一痛点，提高效率和一致性。

Method: 用不同的本地和商用LLM，在提示工程（指令式与思维链）下，从414份介入性放射学报告中抽取结构化操作信息，并用敏感度、特异度、F1分数、推理延迟和Token开销等指标评估模型表现。

Result: 本地和商用LLM都取得了较高的抽取性能，最佳F1分数接近0.87，并在速度和成本上表现出不同权衡。

Conclusion: LLM自动化操作日志有助于减少文书负担、提升填写一致性，并证实了AI辅助医疗教育文档编写的可行性，但还需在更多机构和临床场景中进一步验证。

Abstract: Procedural case logs are a core requirement in radiology training, yet they are time-consuming to complete and prone to inconsistency when authored manually. This study investigates whether large language models (LLMs) can automate procedural case log documentation directly from free-text radiology reports. We evaluate multiple local and commercial LLMs under instruction-based and chain-of-thought prompting to extract structured procedural information from 414 curated interventional radiology reports authored by nine residents between 2018 and 2024. Model performance is assessed using sensitivity, specificity, and F1-score, alongside inference latency and token efficiency to estimate operational cost. Results show that both local and commercial models achieve strong extraction performance, with best F1-scores approaching 0.87, while exhibiting different trade-offs between speed and cost. Automation using LLMs has the potential to substantially reduce clerical burden for trainees and improve consistency in case logging. These findings demonstrate the feasibility of AI-assisted documentation in medical education and highlight the need for further validation across institutions and clinical workflows.

</details>


### [337] [Augmenting Question Answering with A Hybrid RAG Approach](https://arxiv.org/abs/2601.12658)
*Tianyi Yang,Nashrah Haque,Vaishnave Jonnalagadda,Yuya Jeremy Ong,Zhehui Chen,Yanzhao Wu,Lei Yu,Divyesh Jadav,Wenqi Wei*

Main category: cs.CL

TL;DR: 本文提出了一种名为Structured-Semantic RAG (SSRAG)的新型检索增强生成（RAG）方法，通过结合向量和图结构的检索与查询增强，有效提升了问答系统的回答准确性和信息量。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在检索与问题高度相关的信息时表现不佳，导致产生的答案不完整或不理想。因此，作者希望优化RAG检索流程，提高问答系统的表现。

Method: 提出SSRAG架构，包含：1）查询增强，2）智能路由，3）结构化检索，结合向量和图结构检索，再配合上下文统一方式，来提升检索相关性和上下文承接。

Result: 在TruthfulQA、SQuAD和WikiQA三个常用数据集，以及五个主流大语言模型上评测，结果显示SSRAG方法在答案准确率和信息丰富度方面均优于标准RAG。

Conclusion: SSRAG能有效改善RAG检索和生成环节的表现，为RAG类方法优化提供了新的思路和可行路径。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful technique for enhancing the quality of responses in Question-Answering (QA) tasks. However, existing approaches often struggle with retrieving contextually relevant information, leading to incomplete or suboptimal answers. In this paper, we introduce Structured-Semantic RAG (SSRAG), a hybrid architecture that enhances QA quality by integrating query augmentation, agentic routing, and a structured retrieval mechanism combining vector and graph based techniques with context unification. By refining retrieval processes and improving contextual grounding, our approach improves both answer accuracy and informativeness. We conduct extensive evaluations on three popular QA datasets, TruthfulQA, SQuAD and WikiQA, across five Large Language Models (LLMs), demonstrating that our proposed approach consistently improves response quality over standard RAG implementations.

</details>


### [338] [UbuntuGuard: A Culturally-Grounded Policy Benchmark for Equitable AI Safety in African Languages](https://arxiv.org/abs/2601.12696)
*Tassallah Abdullahi,Macton Mgonzo,Mardiyyah Oduwole,Paul Okewunmi,Abraham Owodunni,Ritambhara Singh,Carsten Eickhoff*

Main category: cs.CL

TL;DR: 本文提出UbuntuGuard，这是首个基于非洲本土政策的安全基准，专为低资源非洲语言设计。通过与155位领域专家合作，针对敏感领域问题生成攻防样本，从而评估和推动多语言、大语言模型（LLM）在非洲语境下的安全性和适应性。实验对比多种模型和方法，发现现有基准对真正的多语安全表现高估，跨语迁移作用有限，动态模型虽优于静态模型，却尚不能完全适应该语境。


<details>
  <summary>Details</summary>
Motivation: 传统的安全模型和基准以西方高资源语言为主，不适配非洲等低资源语言，存在跨语安全失效和文化适应不良的问题，导致低资源用户在现实应用中的风险增加。当前安全政策僵化、泛化能力弱，难以应对多样的语言和文化环境。因此需要设计灵活、能体现本地规范和风险的安全基准。

Method: 本研究开发UbuntuGuard，邀请155名来自医疗等敏感领域的非洲专家，编写对抗性问题，形成反映本地文化和风险的安全政策和标注答案。随后，用这些数据评估13种模型（包括通用LLM和多种守护模型），测试静态、动态和多语三类模型变体在本地安全政策中的表现。

Result: 实验显示，原有以英语为主的安全基准高估了跨语模型的实际安全能力。跨语迁移对低资源安全覆盖有限。动态政策模型对本地政策适应相对更强，但在真正本地化非洲语境仍存在明显短板。

Conclusion: 低资源、非洲语境下需要多语种、文化适应性强的安全基准，以指导和促进更公平、可靠的模型安全能力建设。该研究为相关研究和模型开发奠定了重要基础，代码已开源。

Abstract: Current guardian models are predominantly Western-centric and optimized for high-resource languages, leaving low-resource African languages vulnerable to evolving harms, cross-lingual safety failures, and cultural misalignment. Moreover, most guardian models rely on rigid, predefined safety categories that fail to generalize across diverse linguistic and sociocultural contexts. Robust safety, therefore, requires flexible, runtime-enforceable policies and benchmarks that reflect local norms, harm scenarios, and cultural expectations. We introduce UbuntuGuard, the first African policy-based safety benchmark built from adversarial queries authored by 155 domain experts across sensitive fields, including healthcare. From these expert-crafted queries, we derive context-specific safety policies and reference responses that capture culturally grounded risk signals, enabling policy-aligned evaluation of guardian models. We evaluate 13 models, comprising six general-purpose LLMs and seven guardian models across three distinct variants: static, dynamic, and multilingual. Our findings reveal that existing English-centric benchmarks overestimate real-world multilingual safety, cross-lingual transfer provides partial but insufficient coverage, and dynamic models, while better equipped to leverage policies at inference time, still struggle to fully localize African-language contexts. These findings highlight the urgent need for multilingual, culturally grounded safety benchmarks to enable the development of reliable and equitable guardian models for low-resource languages. Our code can be found online.\footnote{Code repository available at https://github.com/hemhemoh/UbuntuGuard.

</details>


### [339] [A Two-Stage GPU Kernel Tuner Combining Semantic Refactoring and Search-Based Optimization](https://arxiv.org/abs/2601.12698)
*Qiuyi Qu,Yicheng Sui,Yufei Sun,Rui Chen,Xiaofei Zhang,Yuzhi Zhang,Haofeng Wang,Ge Lan,Ning Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种结合模板重写和自动调优的新颖GPU核函数优化方法，能够在现实任务中带来稳定且显著的性能提升，优于仅依赖LLM代理直接重写的方案。


<details>
  <summary>Details</summary>
Motivation: GPU代码性能仍然是高性能计算和大模型训练中的瓶颈。当前方法依赖于手工重构和参数调节，自动化工具表现不稳定且难以精细控制参数，急需更加高效、稳定和可扩展的自动优化方案。

Method: 该方法在LLM代理的基础上引入模板化代码重写，将核函数语义化重构为具备可调参数的模板，并通过搜索式自动调优优化参数。优化流程涵盖模板化、测试、分析、方案规划和基于性能分析的受限参数搜索，能有效结合硬件资源约束。

Result: 实验证明该方法在多个真实CUDA核函数上实现了超过3倍的加速效果，优化结果更稳定且可解释，相比单纯LLM代理直接重写方法大幅减少了随机性和依赖人工操作。

Conclusion: 所提出的模板+自动调优机制为GPU代码性能优化带来可持续、可扩展的自动化方案，不仅适用于CUDA，还可推广至OpenCL、HIP等平台，对实际生产工作负载有重要应用价值。

Abstract: GPU code optimization is a key performance bottleneck for HPC workloads as well as large-model training and inference. Although compiler optimizations and hand-written kernels can partially alleviate this issue, achieving near-hardware-limit performance still relies heavily on manual code refactoring and parameter tuning. Recent progress in LLM-agent-based kernel generation and optimization has been reported, yet many approaches primarily focus on direct code rewriting, where parameter choices are often implicit and hard to control, or require human intervention, leading to unstable performance gains. This paper introduces a template-based rewriting layer on top of an agent-driven iterative loop: kernels are semantically refactored into explicitly parameterizable templates, and template parameters are then optimized via search-based autotuning, yielding more stable and higher-quality speedups. Experiments on a set of real-world kernels demonstrate speedups exceeding 3x in the best case. We extract representative CUDA kernels from SGLang as evaluation targets; the proposed agentic tuner iteratively performs templating, testing, analysis, and planning, and leverages profiling feedback to execute constrained parameter search under hardware resource limits. Compared to agent-only direct rewriting, the template-plus-search design significantly reduces the randomness of iterative optimization, making the process more interpretable and enabling a more systematic approach toward high-performance configurations. The proposed method can be further extended to OpenCL, HIP, and other backends to deliver automated performance optimization for real production workloads.

</details>


### [340] [A Shared Geometry of Difficulty in Multilingual Language Models](https://arxiv.org/abs/2601.12731)
*Stefano Civelli,Pietro Bernardelle,Nicolò Brunello,Gianluca Demartini*

Main category: cs.CL

TL;DR: 本论文提出利用大型语言模型（LLM）的内部表示，通过线性探针来预测问题难度，并研究了其跨多语言的泛化特性，揭示了LLM在表征难度时存在语言无关到语言特定的两阶段过程。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在多语言任务中广泛应用，理解其如何内部表征不同任务的难度，以及这种表征是否能跨语言泛化，对于提升模型解释性与健壮性具有重要意义。过去研究多聚焦于语义内容的表征，本论文试图拓展到对“问题难度”这种高阶认知属性的解释。

Method: 作者将Easy2Hard基准的AMC子集翻译为21种语言，对这些多语言数据，在LLM的不同层提取内部表示并训练线性探针，分析早期（浅层）与后期（深层）表征下预测问题难度的准确性及其跨语言泛化能力。

Result: 深层表征可在单一语言下高准确性预测问题难度，但其跨语言泛化较差；浅层表征跨语言泛化表现更好，虽然在单语言任务中准确率略低。整体发现LLM内部先形成语言无关的难度表征，之后逐步变为语言特定。

Conclusion: LLM在表征任务难度时，存在先语言无关再语言特定的两阶段过程，这一过程不仅适用于语义内容，也包括如“问题难度”这样的元认知属性，为LLM可解释性与跨语言能力的研究提供新线索。

Abstract: Predicting problem-difficulty in large language models (LLMs) refers to estimating how difficult a task is according to the model itself, typically by training linear probes on its internal representations. In this work, we study the multilingual geometry of problem-difficulty in LLMs by training linear probes using the AMC subset of the Easy2Hard benchmark, translated into 21 languages. We found that difficulty-related signals emerge at two distinct stages of the model internals, corresponding to shallow (early-layers) and deep (later-layers) internal representations, that exhibit functionally different behaviors. Probes trained on deep representations achieve high accuracy when evaluated on the same language but exhibit poor cross-lingual generalization. In contrast, probes trained on shallow representations generalize substantially better across languages, despite achieving lower within-language performance. Together, these results suggest that LLMs first form a language-agnostic representation of problem difficulty, which subsequently becomes language-specific. This closely aligns with existing findings in LLM interpretability showing that models tend to operate in an abstract conceptual space before producing language-specific outputs. We demonstrate that this two-stage representational process extends beyond semantic content to high-level meta-cognitive properties such as problem-difficulty estimation.

</details>


### [341] [Towards Robust Process Reward Modeling via Noise-aware Learning](https://arxiv.org/abs/2601.12748)
*Bin Xie,Bingbing Xu,Xueyun Tian,Yilin Chen,Huawei Shen*

Main category: cs.CL

TL;DR: 现有的过程奖励模型（PRM）用于复杂推理，但训练数据标签成本高。蒙特卡洛估计（MCE）虽常用，但会引入标签噪声，影响监督质量。该文提出利用大语言模型（LLM）作为反思感知标签修正机制，以及噪声感知迭代训练框架，有效缓解噪声带来的影响，显著提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 过程奖励模型在推理任务中效果突出，但其过程级别的监督非常昂贵且耗时。常用的替代方法如MCE虽然避免了高成本，但带来标签噪声，严重影响模型训练质量。因此，缺乏高质量、低成本的标签，严重制约了PRM的发展。

Method: 作者提出了两阶段方法来缓解噪声监督带来的负面影响。第一阶段是用于标签生成的反思感知标签修正机制，借助LLM判别当前推理步骤的反思和自我修正行为，优化标签。第二阶段提出噪声感知的迭代训练框架，使PRM根据自身置信度迭代修正训练标签，不断提升自身抗噪和判别能力。

Result: 实验结果表明，该方法在区分推理步骤正确性方面大幅提升。具体在平均F1指标上，较仅依赖噪声监督的PRM获得高达27%的绝对提升。

Conclusion: 通过引入LLM参与标签修正，以及噪声感知的训练框架，可以显著缓解传统MCE方法带来的噪声问题。该方案大大提升了PRM在复杂推理任务中的能力，对于大模型监督方式具有重要参考价值。

Abstract: Process Reward Models (PRMs) have achieved strong results in complex reasoning, but are bottlenecked by costly process-level supervision. A widely used alternative, Monte Carlo Estimation (MCE), defines process rewards as the probability that a policy model reaches the correct final answer from a given reasoning step. However, step correctness is an intrinsic property of the reasoning trajectory, and should be invariant to policy choice. Our empirical findings show that MCE producing policy-dependent rewards that induce label noise, including false positives that reward incorrect steps and false negatives that penalize correct ones. To address above challenges, we propose a two-stage framework to mitigate noisy supervision. In the labeling stage, we introduce a reflection-aware label correction mechanism that uses a large language model (LLM) as a judge to detect reflection and self-correction behaviors related to the current reasoning step, thereby suppressing overestimated rewards. In the training stage, we further propose a \underline{\textbf{N}}oise-\underline{\textbf{A}}ware \underline{\textbf{I}}terative \underline{\textbf{T}}raining framework that enables the PRM to progressively refine noisy labels based on its own confidence. Extensive Experiments show that our method substantially improves step-level correctness discrimination, achieving up to a 27\% absolute gain in average F1 over PRMs trained with noisy supervision.

</details>


### [342] [VISPA: Pluralistic Alignment via Automatic Value Selection and Activation](https://arxiv.org/abs/2601.12758)
*Shenyan Zheng,Jiayou Zhong,Anudeex Shetty,Heng Ji,Preslav Nakov,Usman Naseem*

Main category: cs.CL

TL;DR: VISPA是一种无需再训练即可实现多元价值对齐的新方法，通过动态选择和内部模型激活干预，实现对大模型输出价值取向的直接控制。实验表明，VISPA在医疗等多领域模型的多元对齐任务中效果良好、适应性强，并能为大模型多元价值表达提供可扩展路径。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在输出时通常仅反映大多数人的平均偏好，但在高影响领域（如医疗）中需兼顾不同观点和价值观。现行方案在多元化、价值可控和表达方面存在不足。

Method: 提出了一种无需额外训练的多元价值对齐框架VISPA，利用模型内部激活引导和动态选择机制，实现不同价值观的直接表达和控制。该方法不依赖传统的提示工程或单一价值设定。

Result: 在多个领域、模型及评估设置下，VISPA实现了多元价值对齐，优于传统方法，并且能兼容不同的模型、起始设定和价值表达。实验涵盖医疗等高风险场景，稳健性和适应性良好。

Conclusion: 通过内部激活机制可以高效实现对模型输出价值取向的灵活干预和多元对齐。VISPA为大语言模型实现多元、可控、可扩展的社会价值表达提供了有效途径。

Abstract: As large language models are increasingly used in high-stakes domains, it is essential that their outputs reflect not average} human preference, rather range of varying perspectives. Achieving such pluralism, however, remains challenging. Existing approaches consider limited values or rely on prompt-level interventions, lacking value control and representation. To address this, we introduce VISPA, a training-free pluralistic alignment framework, that enables direct control over value expression by dynamic selection and internal model activation steering. Across extensive empirical studies spanning multiple models and evaluation settings, we show VISPA is performant across all pluralistic alignment modes in healthcare and beyond. Further analysis reveals VISPA is adaptable with different steering initiations, model, and/or values. These results suggest that pluralistic alignment can be achieved through internal activation mechanisms, offering a scalable path toward language models that serves all.

</details>


### [343] [Who Does This Name Remind You of? Nationality Prediction via Large Language Model Associative Memory](https://arxiv.org/abs/2601.12771)
*Keito Inoshita*

Main category: cs.CL

TL;DR: 提出了一种名为LAMA的框架，通过类似于‘联想记忆’的方式调用LLM知识，对国籍预测任务显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统大型语言模型在知识调用方面仍处于探索阶段，尤其是在需要结合语言、文化和历史背景的信息提取任务（如国籍预测）中，直接推理法存在局限。

Method: 提出LLM Associative Memory Agents (LAMA) 框架：采用‘联想记忆’，通过召回与目标名字相关的著名人物并汇总其国籍来预测目标国籍。系统分为擅长不同知识领域的两代理（Person Agent和Media Agent），并结合投票与条件补全产生Top-1与Top-K预测结果。

Result: 在99国国籍预测任务上，LAMA达到0.817的准确率，远超传统LLM提示法与神经模型。实验还发现基于召回的方法对低频国籍预测表现稳定，且双代理结构能互补增效。

Conclusion: 通过检索和聚合知识而非直接推理，可以显著提升LLM在需要世界知识的任务中的表现，验证了多代理召回-聚合新范式的有效性。

Abstract: Large language models (LLMs) possess extensive world knowledge, yet methods for effectively eliciting this knowledge remain underexplored. Nationality and region prediction tasks require understanding of not only linguistic features but also cultural and historical background, making LLM world knowledge particularly valuable. However, conventional LLM prompting methods rely on direct reasoning approaches, which have limitations in applying abstract linguistic rules. We propose LLM Associative Memory Agents (LAMA), a novel framework that leverages LLM world knowledge as associative memory. Rather than directly inferring nationality from names, LAMA recalls famous individuals with the same name and aggregates their nationalities through indirect reasoning. A dual-agent architecture comprising a Person Agent and a Media Agent, specialized in different knowledge domains, recalls famous individuals in parallel, generating Top-1 predictions through voting and Top-K predictions through conditional completion. On a 99-country nationality prediction task, LAMA achieved 0.817 accuracy, substantially outperforming conventional LLM prompting methods and neural models. Our experiments reveal that LLMs exhibit higher reliability in recalling concrete examples than in abstract reasoning, that recall-based approaches are robust to low-frequency nationalities independent of data frequency distributions, and that the dual-agent architecture functions complementarily to produce synergistic effects. These results demonstrate the effectiveness of a new multi-agent system that retrieves and aggregates LLM knowledge rather than prompting reasoning.

</details>


### [344] [Do Clinical Question Answering Systems Really Need Specialised Medical Fine Tuning?](https://arxiv.org/abs/2601.12812)
*Sushant Kumar Ray,Gautam Siddharth Kashyap,Sahil Tripathi,Nipun Joshi,Vijay Govindarajan,Rafiq Ali,Jiechao Gao,Usman Naseem*

Main category: cs.CL

TL;DR: 本文提出了一种新型的医学问答系统推理时对齐方法MEDASSESS-X，无需传统的专用领域微调即可提升各类大模型在医学问答任务的表现和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有医学问答系统依赖于领域专用大模型及其微调，存在覆盖面窄、再训练代价高、适应性差等实际问题，并且普遍相信专用医疗大模型一定优于通用模型。

Method: 提出一个名为MEDASSESS-X的行业部署导向框架，通过在推理时利用轻量级steering vectors，引导大模型进行医学一致性推理，无需修改模型权重或进行领域专用再训练。

Result: 该方法在多种通用与专用医疗大模型上统一提升问答表现：准确率提升最高6%、事实一致性提升7%、安全错误率降低50%。

Conclusion: MEDASSESS-X打破了‘专用即优越’的误区，在无需复杂微调的前提下显著增强医学问答系统性能，为实际应用带来更高的可移植性和安全性。

Abstract: Clinical Question-Answering (CQA) industry systems are increasingly rely on Large Language Models (LLMs), yet their deployment is often guided by the assumption that domain-specific fine-tuning is essential. Although specialised medical LLMs such as BioBERT, BioGPT, and PubMedBERT remain popular, they face practical limitations including narrow coverage, high retraining costs, and limited adaptability. Efforts based on Supervised Fine-Tuning (SFT) have attempted to address these assumptions but continue to reinforce what we term the SPECIALISATION FALLACY-the belief that specialised medical LLMs are inherently superior for CQA. To address this assumption, we introduce MEDASSESS-X, a deployment-industry-oriented CQA framework that applies alignment at inference time rather than through SFT. MEDASSESS-X uses lightweight steering vectors to guide model activations toward medically consistent reasoning without updating model weights or requiring domain-specific retraining. This inference-time alignment layer stabilises CQA performance across both general-purpose and specialised medical LLMs, thereby resolving the SPECIALISATION FALLACY. Empirically, MEDASSESS-X delivers consistent gains across all LLM families, improving Accuracy by up to +6%, Factual Consistency by +7%, and reducing Safety Error Rate by as much as 50%.

</details>


### [345] [Multimodal Multi-Agent Empowered Legal Judgment Prediction](https://arxiv.org/abs/2601.12815)
*Zhaolu Kang,Junhao Gong,Qingxi Chen,Hao Zhang,Jiaxin Liu,Rong Fu,Zhiyuan Feng,Yuan Wang,Simon Fong,Kaiyue Zhou*

Main category: cs.CL

TL;DR: 本文提出了JurisMMA框架，用于更有效地解决法律判决预测任务，并构建了包含十万余份真实法律案例的多模态数据集JurisMM。实验结果显示，该方法优于现有基准，推动法律智能化发展。


<details>
  <summary>Details</summary>
Motivation: 法律判决预测面临指控多样、证据复杂以及适应性差等挑战，现有方法难以应对实际司法场景，因此需要开发更精细化和标准化的预测框架。

Method: 作者提出了JurisMMA新框架，对审判任务进行分解和流程标准化，分为若干阶段处理。此外，构建了大规模中英文混合、多模态（文本和视频）法律判决数据集JurisMM，用于全面评估该方法效果。

Result: 在JurisMM和LawBench基准测试上进行实验，JurisMMA框架取得了优异的结果，显著提升了判决预测的准确率与适应性。

Conclusion: JurisMMA不仅能够提升法律判决预测的质量，也为法律人工智能方法和数据集的研究提供了有益启示，有助于推动未来智能法律系统的构建。

Abstract: Legal Judgment Prediction (LJP) aims to predict the outcomes of legal cases based on factual descriptions, serving as a fundamental task to advance the development of legal systems. Traditional methods often rely on statistical analyses or role-based simulations but face challenges with multiple allegations, diverse evidence, and lack adaptability. In this paper, we introduce JurisMMA, a novel framework for LJP that effectively decomposes trial tasks, standardizes processes, and organizes them into distinct stages. Furthermore, we build JurisMM, a large dataset with over 100,000 recent Chinese judicial records, including both text and multimodal video-text data, enabling comprehensive evaluation. Experiments on JurisMM and the benchmark LawBench validate our framework's effectiveness. These results indicate that our framework is effective not only for LJP but also for a broader range of legal applications, offering new perspectives for the development of future legal methods and datasets.

</details>


### [346] [Rapport du Projet de Recherche TRAIMA](https://arxiv.org/abs/2601.12844)
*Julie Rançon,Jean-François Cerisier,Emilie Remond,Aurélien Nguyen,Andrew Peterson,Ladjel Bellatreche*

Main category: cs.CL

TL;DR: TRAIMA项目旨在将自动化处理方法引入教育场景中的多模态交互分析，推动人工智能与教育实践的结合。


<details>
  <summary>Details</summary>
Motivation: 目前教育和互动研究中多模态（语言、语调、手势、目光等）数据的分析主要依赖人工，费时费力且难以扩展。该项目力求通过自动化技术提升分析效率并推动方法创新。

Method: 项目结合了话语分析、互动语言学与机器学习。核心研究包括：1）对课堂中说明性与合作性互动的多模态序列进行结构性界定和手工转录，2）详细比较现有多模态转录规范的优劣，3）构建和利用多小时真实课堂视频语料（含多机位、音频、眼动等数据），为今后自动标注和机器学习应用建立基线。

Result: 项目没有直接开发出完整的自动化系统，而是系统化梳理并提出了适合机器学习的转录规则、注释类别与分析单元，强调理论框架的明确和研究者主观性的反思。实证分析表明转录标准和理论立场大大影响多模态数据解释结果。

Conclusion: TRAIMA为教育领域多模态互动的自动分析建立了严谨的方法论基础，指出进一步推进自动化需从理论清晰、标准统一和跨学科合作入手，为未来AI与教育结合的研究奠定重要基础。

Abstract: The TRAIMA project (TRaitement Automatique des Interactions Multimodales en Apprentissage), conducted between March 2019 and June 2020, investigates the potential of automatic processing of multimodal interactions in educational settings. The project addresses a central methodological challenge in educational and interactional research: the analysis of verbal, paraverbal, and non-verbal data is currently carried out manually, making it extremely time-consuming and difficult to scale. TRAIMA explores how machine learning approaches could contribute to the categorisation and classification of such interactions. The project focuses specifically on explanatory and collaborative sequences occurring in classroom interactions, particularly in French as a Foreign Language (FLE) and French as a First Language (FLM) contexts. These sequences are analysed as inherently multimodal phenomena, combining spoken language with prosody, gestures, posture, gaze, and spatial positioning. A key theoretical contribution of the project is the precise linguistic and interactional definition of explanatory discourse as a tripartite sequence (opening, explanatory core, closure), drawing on discourse analysis and interactional linguistics. A substantial part of the research is devoted to the methodological foundations of transcription, which constitute a critical bottleneck for any form of automation. The report provides a detailed state of the art of existing transcription conventions (ICOR, Mondada, GARS, VALIBEL, Ferr{é}), highlighting their respective strengths and limitations when applied to multimodal classroom data. Through comparative analyses of manually transcribed sequences, the project demonstrates the inevitable variability and interpretative dimension of transcription practices, depending on theoretical positioning and analytical goals. Empirical work is based on several corpora, notably the INTER-EXPLIC corpus (approximately 30 hours of classroom interaction) and the EXPLIC-LEXIC corpus, which serve both as testing grounds for manual annotation and as reference datasets for future automation. Particular attention is paid to teacher gestures (kin{é}sic and proxemic resources), prosodic features, and their functional role in meaning construction and learner comprehension. The project also highlights the strategic role of the Techn{é}LAB platform, which provides advanced multimodal data capture (multi-camera video, synchronized audio, eye-tracking, digital interaction traces) and constitutes both a research infrastructure and a test environment for the development of automated tools. In conclusion, TRAIMA does not aim to deliver a fully operational automated system, but rather to establish a rigorous methodological framework for the automatic processing of multimodal pedagogical interactions. The project identifies transcription conventions, annotation categories, and analytical units that are compatible with machine learning approaches, while emphasizing the need for theoretical explicitness and researcher reflexivity. TRAIMA thus lays the groundwork for future interdisciplinary research at the intersection of didactics, discourse analysis, multimodality, and artificial intelligence in education.

</details>


### [347] [Race, Ethnicity and Their Implication on Bias in Large Language Models](https://arxiv.org/abs/2601.12868)
*Shiyue Hu,Ruizhe Li,Yanjun Gao*

Main category: cs.CL

TL;DR: 本文通过机制性研究，探究了LLM（大语言模型）如何在内部表示和处理与种族和族裔相关的信息，发现信息分布具有差异且消除偏见并不完全有效。


<details>
  <summary>Details</summary>
Motivation: 现有工作往往只关注LLM在输出结果上的偏见表现，缺乏对这些偏见产生的内部机制研究。鉴于LLM被越来越多用于医疗等高风险场景，了解其内部对敏感信息的处理机制具有重要意义。

Method: 作者选用两个公开数据集（涵盖毒性文本生成与临床文本理解任务），分析三个开源LLM模型，结合可复现的解释性分析流程，包括探针、神经元归因和有针对性的干预操作，系统刻画内部单元对种族和族裔属性的表示模式。

Result: 研究显示：1）人口统计信息在模型内部是分布式、具跨模型差异的；2）部分神经元与敏感、刻板印象相关的预训练偏见有关；3）即使人为干预以抑制特定神经元，偏见现象依旧剩余，且不同模型对同样的人口学信息表现出质的不同行为。

Conclusion: 简单地对模型内部敏感神经元进行干预不足以根除偏见，说明偏见更主要表现为行为层面的问题，未来需有更系统的偏见缓解方法。

Abstract: Large language models (LLMs) increasingly operate in high-stakes settings including healthcare and medicine, where demographic attributes such as race and ethnicity may be explicitly stated or implicitly inferred from text. However, existing studies primarily document outcome-level disparities, offering limited insight into internal mechanisms underlying these effects. We present a mechanistic study of how race and ethnicity are represented and operationalized within LLMs. Using two publicly available datasets spanning toxicity-related generation and clinical narrative understanding tasks, we analyze three open-source models with a reproducible interpretability pipeline combining probing, neuron-level attribution, and targeted intervention. We find that demographic information is distributed across internal units with substantial cross-model variation. Although some units encode sensitive or stereotype-related associations from pretraining, identical demographic cues can induce qualitatively different behaviors. Interventions suppressing such neurons reduce bias but leave substantial residual effects, suggesting behavioral rather than representational change and motivating more systematic mitigation.

</details>


### [348] [From Prefix Cache to Fusion RAG Cache: Accelerating LLM Inference in Retrieval-Augmented Generation](https://arxiv.org/abs/2601.12904)
*Jiahao Wang,Weiyu Xie,Mingxing Zhang,Boxing Zhang,Jianwei Dong,Yuening Zhu,Chen Lin,Jinqi Tang,Yaochen Han,Zhiyuan Ai,Xianglin Chen,Yongwei Wu,Congfeng Jiang*

Main category: cs.CL

TL;DR: 本文提出了FusionRAG框架，通过创新性地优化RAG方法中的KV cache重用，实现了生成质量与效率的显著提升。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）模型虽能减轻大模型幻觉问题，但增加了提示词长度，带来了更高的计算成本和延迟。已有的KV cache重用加速方法因未处理chunk上下文，导致生成质量下降，无法充分利用KV cache重用潜力。因此，急需一种兼顾效率和生成质量的方法。

Method: 作者提出FusionRAG框架，在离线预处理阶段，将相关chunk的信息融合进每个chunk。在在线重新处理阶段，对模型关注的token重新计算KV cache，其他部分复用原有cache，以此兼顾上下文融合与效率。

Result: 实验证明，在相同比例的kv cache重计算条件下，FusionRAG在生成质量上显著优于现有方法。只需重算少于15%的token，F1分数比基线提升70%，TTFT也比全量Attention降低2.66-9.39倍。

Conclusion: FusionRAG有效实现了效率和生成质量的平衡，为RAG系统在实际应用中提供更优解决方案，推动了大语言模型检索增强生成的实际可用性。

Abstract: Retrieval-Augmented Generation enhances Large Language Models by integrating external knowledge, which reduces hallucinations but increases prompt length. This increase leads to higher computational costs and longer Time to First Token (TTFT). To mitigate this issue, existing solutions aim to reuse the preprocessed KV cache of each retrieved chunk to accelerate RAG. However, the lack of cross-chunk contextual information leads to a significant drop in generation quality, leaving the potential benefits of KV cache reuse largely unfulfilled. The challenge lies in how to reuse the precomputed KV cache of chunks while preserving generation quality. We propose FusionRAG, a novel inference framework that optimizes both the preprocessing and reprocessing stages of RAG. In the offline preprocessing stage, we embed information from other related text chunks into each chunk, while in the online reprocessing stage, we recompute the KV cache for tokens that the model focuses on. As a result, we achieve a better trade-off between generation quality and efficiency. According to our experiments, FusionRAG significantly improves generation quality at the same recomputation ratio compared to previous state-of-the-art solutions. By recomputing fewer than 15% of the tokens, FusionRAG achieves up to 70% higher normalized F1 scores than baselines and reduces TTFT by 2.66x-9.39x compared to Full Attention.

</details>


### [349] [Gated Differentiable Working Memory for Long-Context Language Modeling](https://arxiv.org/abs/2601.12906)
*Lingrui Mei,Shenghua Liu,Yiwei Wang,Yuyao Ge,Baolong Bi,Jiayu Yao,Jun Wan,Ziling Yin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文提出了一种高效的变换器模型测试时自适应方法，通过有选择地整合重要上下文信息，在长文本任务中显著提升了效率与效果。


<details>
  <summary>Details</summary>
Motivation: 在处理长文本时，变换器模型注意力分散，关键信息容易丢失，且难以应对新模式。现有测试时自适应方法在上下文写入工作内存时没有区分信息价值，既浪费计算又导致梯度波动大。

Method: 作者将问题视为受计算预算约束的记忆巩固过程，提出了Gdwm框架，引入写入控制器，根据上下文效用（信息论度量），选择性写入并分配梯度步数，同时兼顾全局覆盖。

Result: 在ZeroSCROLLS和LongBench v2数据集上，Gdwm在性能持平或更优情况下，比均匀基线方法减少了4倍的梯度步数。

Conclusion: Gdwm方法在测试时自适应任务上达到了更优的效率-性能平衡，推动了长文本处理模型的实用性与发展。

Abstract: Long contexts challenge transformers: attention scores dilute across thousands of tokens, critical information is often lost in the middle, and models struggle to adapt to novel patterns at inference time. Recent work on test-time adaptation addresses this by maintaining a form of working memory -- transient parameters updated on the current context -- but existing approaches rely on uniform write policies that waste computation on low-utility regions and suffer from high gradient variance across semantically heterogeneous contexts. In this work, we reframe test-time adaptation as a budget-constrained memory consolidation problem, focusing on which parts of the context should be consolidated into working memory under limited computation. We propose Gdwm (Gated Differentiable Working Memory), a framework that introduces a write controller to gate the consolidation process. The controller estimates Contextual Utility, an information-theoretic measure of long-range contextual dependence, and allocates gradient steps accordingly while maintaining global coverage. Experiments on ZeroSCROLLS and LongBench v2 demonstrate that Gdwm achieves comparable or superior performance with 4$\times$ fewer gradient steps than uniform baselines, establishing a new efficiency-performance Pareto frontier for test-time adaptation.

</details>


### [350] [SciCoQA: Quality Assurance for Scientific Paper--Code Alignment](https://arxiv.org/abs/2601.12910)
*Tim Baumgärtner,Iryna Gurevych*

Main category: cs.CL

TL;DR: SciCoQA是一个用于检测科学论文与其代码实现之间不一致的数据集，通过分析和分类真实及合成的差异，评估了多种大语言模型在此任务上的能力，结果表明当前模型表现有限。


<details>
  <summary>Details</summary>
Motivation: 在科学领域，论文描述的算法与实际代码实现之间常有不一致，影响可复现性。本工作致力于提供系统性数据集和分析工具，以推动自动检测和理解这些不一致，支持研究诚信和复现。

Method: 作者通过收集GitHub issue和可复现实验论文，整理真实不一致样本，同时提出合成方法，生成大规模人工差异，构建跨AI、物理、生物等领域的数据集。对比分析差异类型和类别，并组织21种主流大语言模型进行检测实验。

Result: 最终构建了611个差异样本（81真实，530合成），分析结果揭示了差异出现的具体类型。模型实验显示，当前最优模型（如GPT-5）在检测真实差异时准确率仅为45.7%，难点包括论文细节遗漏、超长上下文等。

Conclusion: 当前大语言模型在检测论文与代码实现不一致方面能力有限，SciCoQA提供了详实的基准和难点分析，有助于今后提升模型在科研可复现性和自动审稿领域的实用性。

Abstract: We present SciCoQA, a dataset for detecting discrepancies between scientific publications and their codebases to ensure faithful implementations. We construct SciCoQA from GitHub issues and reproducibility papers, and to scale our dataset, we propose a synthetic data generation method for constructing paper-code discrepancies. We analyze the paper-code discrepancies in detail and propose discrepancy types and categories to better understand the occurring mismatches. In total, our dataset consists of 611 paper-code discrepancies (81 real, 530 synthetic), spanning diverse computational science disciplines, including AI, Physics, Quantitative Biology, and others. Our evaluation of 21 LLMs highlights the difficulty of SciCoQA, particularly for instances involving omitted paper details, long-context inputs, and data outside the models' pre-training corpus. The best performing model in our evaluation, GPT-5, can only detect 45.7\% of real-world paper-code discrepancies.

</details>


### [351] [Injecting Knowledge from Social Science Journals to Improve Indonesian Cultural Understanding by LLMs](https://arxiv.org/abs/2601.12921)
*Adimulya Kartiyasa,Bao Gia Cao,Boyang Li*

Main category: cs.CL

TL;DR: 本文提出了一个包含来自151本印尼本地社会科学期刊的、关注印尼本土文化的新文本数据集（IndoSoSci）。通过提取与印尼文化相关事实，并利用检索增强生成（RAG）方法，将本地文化知识更好地注入大语言模型（LLM），显著提升了模型在相关基准测试（IndoCulture）上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型对印尼文化知识的掌握有限，而本地社科学术期刊中包含丰富但未被挖掘的本土文化信息。因此，作者旨在通过发掘这些期刊内容，提升模型对印尼文化的理解能力。

Method: 1）收集并整理151本印尼社科学术期刊中的文章段落，构建IndoSoSci数据集；2）提取与印尼文化相关的信息；3）采用检索增强生成（RAG）技术，利用LLM生成的假想文档作为检索查询，增强LLM的文化知识注入。

Result: 该方法在印尼文化相关的基准测试（IndoCulture）中，相较于已有多项强基线，取得了显著的性能提升，并通过将IndoSoSci与印尼语维基百科结合，刷新了该基准的最优准确率。

Conclusion: 本研究表明，系统性地利用本地社会科学期刊资源，通过RAG等方法可以有效提升LLM对本地文化的理解，提升文化知识注入的效果，为其他低资源文化知识的融入提供借鉴。

Abstract: Recently there have been intensifying efforts to improve the understanding of Indonesian cultures by large language models (LLMs). An attractive source of cultural knowledge that has been largely overlooked is local journals of social science, which likely contain substantial cultural studies from a native perspective. We present a novel text dataset of journal article passages, created from 151 open-source Indonesian social science journals, called IndoSoSci. We demonstrate an effective recipe for injecting Indonesian cultural knowledge therein into LLMs: extracting the facts related to Indonesian culture, and apply retrieval-augmented generation (RAG) with LLM-generated hypothetical documents as queries during retrieval. The proposed recipe yields strong performance gains over several strong baselines on the IndoCulture benchmark. Additionally, by combining IndoSoSci with Indonesian Wikipedia, we set a new state-of-the-art accuracy on the IndoCulture benchmark.

</details>


### [352] [A Component-Based Survey of Interactions between Large Language Models and Multi-Armed Bandits](https://arxiv.org/abs/2601.12945)
*Miao Xie,Siguang Chen,Chunli Lv*

Main category: cs.CL

TL;DR: 本文综述了大语言模型（LLM）与多臂赌博机（MAB）算法交互的最新进展，并指出两者在关键组件层面可实现双向增益，对未来研究有指导意义。


<details>
  <summary>Details</summary>
Motivation: 随着LLM和MAB算法在各自领域的强大应用，探索两者交互带来的新机遇成为必要。此前缺乏系统性综述，本文旨在弥补这一空白，梳理交互现状与前景。

Method: 通过系统回顾现有文献，分析LLM增强的MAB系统与MAB增强的LLM系统，归纳其设计思路、核心技术与性能，并搭建一个相关文献索引的GitHub仓库。

Result: 总结了MAB在LLM预训练、信息检索增强生成（RAG）、个性化等环节的作用，以及LLM对MAB系统决策与环境建模的优化。梳理了代表性研究成果，并指出两者结合中的关键挑战。

Conclusion: LLM与MAB的深度结合为各自领域带来创新，未来值得进一步探索其协同优化潜能及应对现有挑战的方法。本综述为相关研究提供了清晰框架和参考文献资源。

Abstract: Large language models (LLMs) have become powerful and widely used systems for language understanding and generation, while multi-armed bandit (MAB) algorithms provide a principled framework for adaptive decision-making under uncertainty. This survey explores the potential at the intersection of these two fields. As we know, it is the first survey to systematically review the bidirectional interaction between large language models and multi-armed bandits at the component level. We highlight the bidirectional benefits: MAB algorithms address critical LLM challenges, spanning from pre-training to retrieval-augmented generation (RAG) and personalization. Conversely, LLMs enhance MAB systems by redefining core components such as arm definition and environment modeling, thereby improving decision-making in sequential tasks. We analyze existing LLM-enhanced bandit systems and bandit-enhanced LLM systems, providing insights into their design, methodologies, and performance. Key challenges and representative findings are identified to help guide future research. An accompanying GitHub repository that indexes relevant literature is available at https://github.com/bucky1119/Awesome-LLM-Bandit-Interaction.

</details>


### [353] [Trustworthy Data-driven Chronological Age Estimation from Panoramic Dental Images](https://arxiv.org/abs/2601.12960)
*Ainhoa Vivel-Couso,Nicolás Vila-Blanco,María J. Carreira,Alberto Bugarín-Diz,Inmaculada Tomás,Jose M. Alonso-Moral*

Main category: cs.CL

TL;DR: 本研究提出了一个结合深度学习与自然语言生成（NLG）模块的牙齿年龄估算系统，提升了自动化诊断的透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: 深度学习虽提升了医疗个性化服务能力，但模型的不透明性导致临床信任度下降。因此迫切需要开发更透明、易于理解的AI医疗系统。

Method: 系统融合了‘黑盒’深度学习方法和‘白盒’透明方法，通过规则驱动的NLG模块自动生成专家可读的文本解释，并邀请牙科专家参与设计和解释验证。

Result: 专家通过问卷对解释文本的五个维度进行了评估，平均得分为4.77±0.12（满分5分）；系统在ALTAI可信度自评中的七个维度上平均得分为4.40±0.27。

Conclusion: 该系统兼顾了高诊断准确性与高可解释性，为医疗AI领域提升医师信任和临床应用落地提供了路径参考。

Abstract: Integrating deep learning into healthcare enables personalized care but raises trust issues due to model opacity. To improve transparency, we propose a system for dental age estimation from panoramic images that combines an opaque and a transparent method within a natural language generation (NLG) module. This module produces clinician-friendly textual explanations about the age estimations, designed with dental experts through a rule-based approach. Following the best practices in the field, the quality of the generated explanations was manually validated by dental experts using a questionnaire. The results showed a strong performance, since the experts rated 4.77+/-0.12 (out of 5) on average across the five dimensions considered. We also performed a trustworthy self-assessment procedure following the ALTAI checklist, in which it scored 4.40+/-0.27 (out of 5) across seven dimensions of the AI Trustworthiness Assessment List.

</details>


### [354] [Pardon? Evaluating Conversational Repair in Large Audio-Language Models](https://arxiv.org/abs/2601.12973)
*Shuanghong Huang,Jinlei Xu,Youchao Zhou,Yanghao Zhou,Xuan Zhao,Chong Feng,Wenxuan Zhang*

Main category: cs.CL

TL;DR: 本文关注于大音频-语言模型（LALMs）在口语问答任务中的能力评估，提出不仅关注答案准确率，还需考虑模型对不可回答输入的修复响应能力，提出了新的评估方法和度量标准。


<details>
  <summary>Details</summary>
Motivation: 现有的大音频-语言模型在评测中主要看答案准确性，默认假设所有语音输入都可回答，然而实际中常有信息缺失导致无法回答的问题，因此需要新的方法来区分可回答与不可回答输入并评估模型的应对能力。

Method: 引入了一种修复感知的评估设定，明确区分可回答和不可回答的语音输入，设计了语义-声学掩码协议构建评测对，并提出了EAR（Evaluability Awareness and Repair）分数，此指标结合了模型在可回答输入下的任务胜任力与不可回答输入下的修复行为表现。

Result: 在两个口语问答基准测试和不同LALMs上的实验表明，模型在可回答输入下表现良好，但大部分无法识别不可回答的输入并采取适当修复对话，揭示了当前以准确率为核心的评估方式的局限性。

Conclusion: 单纯依赖准确率评估无法全面衡量LALM的问答能力和对话可靠性，应增加对不可回答输入的修复与持续交互能力的考查，从而提升模型与用户实际交互时的鲁棒性和可靠性。

Abstract: Large Audio-Language Models (LALMs) have demonstrated strong performance in spoken question answering (QA), with existing evaluations primarily focusing on answer accuracy and robustness to acoustic perturbations. However, such evaluations implicitly assume that spoken inputs remain semantically answerable, an assumption that often fails in real-world interaction when essential information is missing. In this work, we introduce a repair-aware evaluation setting that explicitly distinguishes between answerable and unanswerable audio inputs. We define answerability as a property of the input itself and construct paired evaluation conditions using a semantic-acoustic masking protocol. Based on this setting, we propose the Evaluability Awareness and Repair (EAR) score, a non-compensatory metric that jointly evaluates task competence under answerable conditions and repair behavior under unanswerable conditions. Experiments on two spoken QA benchmarks across diverse LALMs reveal a consistent gap between answer accuracy and conversational reliability: while many models perform well when inputs are answerable, most fail to recognize semantic unanswerability and initiate appropriate conversational repair. These findings expose a limitation of prevailing accuracy-centric evaluation practices and motivate reliability assessments that treat unanswerable inputs as cues for repair and continued interaction.

</details>


### [355] [Bridging the Knowledge-Action Gap by Evaluating LLMs in Dynamic Dental Clinical Scenarios](https://arxiv.org/abs/2601.12974)
*Hongyang Ma,Tiantian Gu,Huaiyuan Sun,Huilin Zhu,Yongxin Wang,Jie Li,Wubin Sun,Zeliang Lian,Yinghong Zhou,Yi Gao,Shirui Wang,Zhihui Tang*

Main category: cs.CL

TL;DR: 论文提出了SCMPE基准系统，系统性评估了大型语言模型（LLMs）在牙科领域由静态知识检索向自主临床代理转变时的表现，揭示了其在模拟动态临床流程中的主要瓶颈，并探索了检索增强生成（RAG）方法的实际效果。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型逐渐应用于医疗领域，并从简单知识问答转向自主决策支持，如何科学评价其在临床真实世界中的能力成为需求。牙科领域对AI建议的高质量和可靠性尤为重要，激发了对适用评估基准的开发。

Method: 作者提出了SCMPE基准，涵盖了知识型静态任务与基于流程的多轮模拟患者对话，系统性分析模型在不同任务中的表现，并通过“指南依从性”与“决策质量”二维评估模型安全性。此外，量化分析了RAG机制在不同任务下的效果。

Result: 模型在静态客观任务中表现优秀，但在动态临床对话中的表现显著下降，主要瓶颈为主动信息搜集和动态状态追踪能力不足。一般通用模型存在“高效能，低安全”风险。RAG方法虽可减少静态任务幻觉，但在动态流程中的效果有限、甚至偶有负面影响。

Conclusion: 当前LLMs在牙科动态临床应用面临重大挑战，单靠外部知识增强无法解决根本推理短板。论文为标准化知识与安全自主实践之间的鸿沟提供了经验框架和改进方向。

Abstract: The transition of Large Language Models (LLMs) from passive knowledge retrievers to autonomous clinical agents demands a shift in evaluation-from static accuracy to dynamic behavioral reliability. To explore this boundary in dentistry, a domain where high-quality AI advice uniquely empowers patient-participatory decision-making, we present the Standardized Clinical Management & Performance Evaluation (SCMPE) benchmark, which comprehensively assesses performance from knowledge-oriented evaluations (static objective tasks) to workflow-based simulations (multi-turn simulated patient interactions). Our analysis reveals that while models demonstrate high proficiency in static objective tasks, their performance precipitates in dynamic clinical dialogues, identifying that the primary bottleneck lies not in knowledge retention, but in the critical challenges of active information gathering and dynamic state tracking. Mapping "Guideline Adherence" versus "Decision Quality" reveals a prevalent "High Efficacy, Low Safety" risk in general models. Furthermore, we quantify the impact of Retrieval-Augmented Generation (RAG). While RAG mitigates hallucinations in static tasks, its efficacy in dynamic workflows is limited and heterogeneous, sometimes causing degradation. This underscores that external knowledge alone cannot bridge the reasoning gap without domain-adaptive pre-training. This study empirically charts the capability boundaries of dental LLMs, providing a roadmap for bridging the gap between standardized knowledge and safe, autonomous clinical practice.

</details>


### [356] [The Bitter Lesson of Diffusion Language Models for Agentic Workflows: A Comprehensive Reality Check](https://arxiv.org/abs/2601.12979)
*Qingyu Lu,Liang Ding,Kanjian Zhang,Jinxia Zhang,Dacheng Tao*

Main category: cs.CL

TL;DR: 本文评估了扩散型大语言模型（dLLMs）在自主智能体任务中的实际能力，发现目前dLLMs在涉及长期规划和高精度输出的场景下表现不佳，难以胜任可靠的智能体核心。


<details>
  <summary>Details</summary>
Motivation: 当前主流的自回归（auto-regressive）大语言模型存在推理时的串行延迟问题，限制了实时智能体的应用。扩散型大语言模型（dLLMs）被提出为潜在替代品，理论上可以提高效率，但尚未明确其效率提升是否真正转化为智能体实际任务表现。

Method: 作者选取了两类典型智能体范式——具身智能体（需长期规划）和调用工具智能体（需精确格式化），并在Agentboard与BFCL任务体系内，系统性评测并分析了dLLMs（如LLaDA、Dream）的表现。此外，提出了DiffuAgent多智能体评测框架来进一步检验dLLMs在不同智能体流程角色中的适用性。

Result: 实验发现，dLLMs在具身任务中难以根据时序反馈做出有效分支决策，表现为反复尝试但难以实现目标；在调用工具任务中，因扩散噪声影响，难以输出规范的结构化数据（如严格的JSON），导致任务失败。测试显示dLLMs在非因果任务（如记忆总结和工具选择）环节相对有效。

Conclusion: 目前的扩散型大语言模型还不能作为高可靠性的智能体核心。在智能体应用中，dLLMs需要结合因果性、精确性、逻辑推理等机制，提升对复杂任务的适应能力。

Abstract: The pursuit of real-time agentic interaction has driven interest in Diffusion-based Large Language Models (dLLMs) as alternatives to auto-regressive backbones, promising to break the sequential latency bottleneck. However, does such efficiency gains translate into effective agentic behavior? In this work, we present a comprehensive evaluation of dLLMs (e.g., LLaDA, Dream) across two distinct agentic paradigms: Embodied Agents (requiring long-horizon planning) and Tool-Calling Agents (requiring precise formatting). Contrary to the efficiency hype, our results on Agentboard and BFCL reveal a "bitter lesson": current dLLMs fail to serve as reliable agentic backbones, frequently leading to systematically failure. (1) In Embodied settings, dLLMs suffer repeated attempts, failing to branch under temporal feedback. (2) In Tool-Calling settings, dLLMs fail to maintain symbolic precision (e.g. strict JSON schemas) under diffusion noise. To assess the potential of dLLMs in agentic workflows, we introduce DiffuAgent, a multi-agent evaluation framework that integrates dLLMs as plug-and-play cognitive cores. Our analysis shows that dLLMs are effective in non-causal roles (e.g., memory summarization and tool selection) but require the incorporation of causal, precise, and logically grounded reasoning mechanisms into the denoising process to be viable for agentic tasks.

</details>


### [357] [ChartAttack: Testing the Vulnerability of LLMs to Malicious Prompting in Chart Generation](https://arxiv.org/abs/2601.12983)
*Jesus-German Ortiz-Barajas,Jonathan Tonglet,Vivek Gupta,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文提出了一种新的用于评估多模态大语言模型（MLLM）生成误导性图表风险的框架ChartAttack，并证明其对模型和人的解读准确性影响显著。


<details>
  <summary>Details</summary>
Motivation: 随着MLLM在自动化图表生成中的普及，虽然提升了数据分析效率，但也带来了模型可能被滥用以大规模生成误导性图表的新风险。缺乏专门方法系统性测评此类风险。

Method: 提出ChartAttack框架，通过向图表设计中注入“misleaders”（误导元素），并构建AttackViz问答数据集，用以评估这些误导性图表对MLLM和人类解读准确性的影响。在同域和跨域场景下实验，并结合用户研究。

Result: 实验显示，ChartAttack可使MLLM答题准确率下降19.6（同域）和14.9（跨域）个百分点；人的问答准确率也下降20.2个百分点，均表明误导性图表对解析影响很大。

Conclusion: MLLM驱动的图表生成系统亟需在设计、评估和实际部署中加强鲁棒性与安全性，应警惕其生成误导信息的潜在风险。代码和数据已开源。

Abstract: Multimodal large language models (MLLMs) are increasingly used to automate chart generation from data tables, enabling efficient data analysis and reporting but also introducing new misuse risks. In this work, we introduce ChartAttack, a novel framework for evaluating how MLLMs can be misused to generate misleading charts at scale. ChartAttack injects misleaders into chart designs, aiming to induce incorrect interpretations of the underlying data. Furthermore, we create AttackViz, a chart question-answering (QA) dataset where each (chart specification, QA) pair is labeled with effective misleaders and their induced incorrect answers. Experiments in in-domain and cross-domain settings show that ChartAttack significantly degrades the QA performance of MLLM readers, reducing accuracy by an average of 19.6 points and 14.9 points, respectively. A human study further shows an average 20.2 point drop in accuracy for participants exposed to misleading charts generated by ChartAttack. Our findings highlight an urgent need for robustness and security considerations in the design, evaluation, and deployment of MLLM-based chart generation systems. We make our code and data publicly available.

</details>


### [358] [Graph Reasoning Paradigm: Structured and Symbolic Reasoning with Topology-Aware Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2601.12995)
*Runxuan Liu,Xianhao Ou,Xinyan Ma,Jiyuan Wang,Jiafeng Liang,Jiaqi Li,Tao He,Zheng Chu,Rongchuan Mu,Zekun Wang,Baoxin Wang,Dayong Wu,Ming Liu,Shijin Wang,Guoping Hu,Bing Qin*

Main category: cs.CL

TL;DR: 提出了一种基于图结构的推理方法（GRP）增强大语言模型的推理能力，并通过结构化评价方法提升训练效果，在数学推理与代码生成任务上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理多为纯文本输出，不结构化导致语义评价计算瓶颈，且RLVR方法存在监督粗、奖励欺骗、训练开销大等问题。需要更细粒度、结构化的推理和评价机制。

Method: 提出Graph Reasoning Paradigm（GRP），将推理过程结构化为带认知标签的图表示，设计了PASC-GRPO策略，用结构化图评价替代传统语义评价，通过分层裁剪优势估计缓解奖励欺骗问题。

Result: 实验结果表明，所提方法在数学推理和代码生成任务上显著优于传统RLVR方法。

Conclusion: 结构化的推理与评价可有效提升大语言模型推理表现，减轻奖励欺骗和高训练成本，方法具较好泛化能力。

Abstract: Long Chain-of-Thought (LCoT), achieved by Reinforcement Learning with Verifiable Rewards (RLVR), has proven effective in enhancing the reasoning capabilities of Large Language Models (LLMs). However, reasoning in current LLMs is primarily generated as plain text, where performing semantic evaluation on such unstructured data creates a computational bottleneck during training. Despite RLVR-based optimization, existing methods still suffer from coarse-grained supervision, reward hacking, high training costs, and poor generalization. To address these issues, we propose the Graph Reasoning Paradigm (GRP), which realizes structured and symbolic reasoning, implemented via graph-structured representations with step-level cognitive labels. Building upon GRP, we further design Process-Aware Stratified Clipping Group Relative Policy Optimization (PASC-GRPO), which leverages structured evaluation to replace semantic evaluation, achieves process-aware verification through graph-structured outcome rewards, and mitigates reward hacking via stratified clipping advantage estimation. Experiments demonstrate significant improvements across mathematical reasoning and code generation tasks. Data, models, and code will be released later.

</details>


### [359] [Bi-Attention HateXplain : Taking into account the sequential aspect of data during explainability in a multi-task context](https://arxiv.org/abs/2601.13018)
*Ghislain Dorian Tchuente Mondjo*

Main category: cs.CL

TL;DR: 本文提出了一种新型的仇恨言论检测模型BiAtt-BiRNN-HateXplain，通过结合双向注意力机制与双向循环神经网络，提高了模型的可解释性、检测性能与公平性。实验结果显示，该方法在HateXplain基准上显著提升了仇恨言论检测效果，增强了解释能力，并减少了对群体的无意偏见。


<details>
  <summary>Details</summary>
Motivation: 当前互联网和社交网络的发展导致仇恨言论大幅增加，仇恨言论检测模型的可解释性和预测稳定性不足，尤其是多任务学习方法中注意力分布不稳定，引发了对算法公平性和透明度的担忧。

Method: 提出一种名为BiAtt-BiRNN-HateXplain的新模型，结合了多任务学习（同时进行分类和解释任务）、双向注意力机制和双向循环神经网络层，以提升输入数据的顺序特征捕捉和模型的可解释性。

Result: 在HateXplain数据集上的实验表明，该方法在检测性能、模型可解释性和减少模型对群体的无意偏见方面均优于现有算法。

Conclusion: BiAtt-BiRNN-HateXplain模型有效提升了仇恨言论检测的可解释性与公平性，为实际应用提供了更可靠的工具，解决了代码黑箱与预测不稳定的问题。

Abstract: Technological advances in the Internet and online social networks have brought many benefits to humanity. At the same time, this growth has led to an increase in hate speech, the main global threat. To improve the reliability of black-box models used for hate speech detection, post-hoc approaches such as LIME, SHAP, and LRP provide the explanation after training the classification model. In contrast, multi-task approaches based on the HateXplain benchmark learn to explain and classify simultaneously. However, results from HateXplain-based algorithms show that predicted attention varies considerably when it should be constant. This attention variability can lead to inconsistent interpretations, instability of predictions, and learning difficulties. To solve this problem, we propose the BiAtt-BiRNN-HateXplain (Bidirectional Attention BiRNN HateXplain) model which is easier to explain compared to LLMs which are more complex in view of the need for transparency, and will take into account the sequential aspect of the input data during explainability thanks to a BiRNN layer. Thus, if the explanation is correctly estimated, thanks to multi-task learning (explainability and classification task), the model could classify better and commit fewer unintentional bias errors related to communities. The experimental results on HateXplain data show a clear improvement in detection performance, explainability and a reduction in unintentional bias.

</details>


### [360] [Tears or Cheers? Benchmarking LLMs via Culturally Elicited Distinct Affective Responses](https://arxiv.org/abs/2601.13024)
*Chongyuan Dai,Yaling Shen,Jinpeng Hu,Zihan Gao,Jia Li,Yishun Jiang,Yaxiong Wang,Liu Liu,Zongyuan Ge*

Main category: cs.CL

TL;DR: 本文提出了CEDAR多模态基准，用于评估大语言模型在不同文化情境下对情感反应的理解能力，发现当前多语言模型在文化一致性和情感理解方面仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型文化评测主要集中在知识性内容，忽视了不同文化对情感刺激主观理解的差异。因此，急需有能够反映跨文化情感理解的新型基准。

Method: 提出并构建了CEDAR基准。首先通过大模型生成标签，筛选出易在不同文化间产生情感分歧的场景，然后由人工严谨标注得到高质量数据。最终涵盖7种语言、14类细致情绪类别和近1.1万条数据（包括多模态和纯文本样本）。

Result: 对17个主流多语言模型的评测表明，模型在处理文本一致性还行，但对文化相关的情感理解表现较差。发现语言一致性与文化对齐存在脱节问题。

Conclusion: 主流大语言模型在‘情感—文化’维度的理解还有明显短板，反映未来多模态大模型应更加重视文化敏感性和主观性情感处理能力。

Abstract: Culture serves as a fundamental determinant of human affective processing and profoundly shapes how individuals perceive and interpret emotional stimuli. Despite this intrinsic link extant evaluations regarding cultural alignment within Large Language Models primarily prioritize declarative knowledge such as geographical facts or established societal customs. These benchmarks remain insufficient to capture the subjective interpretative variance inherent to diverse sociocultural lenses. To address this limitation, we introduce CEDAR, a multimodal benchmark constructed entirely from scenarios capturing Culturally \underline{\textsc{E}}licited \underline{\textsc{D}}istinct \underline{\textsc{A}}ffective \underline{\textsc{R}}esponses. To construct CEDAR, we implement a novel pipeline that leverages LLM-generated provisional labels to isolate instances yielding cross-cultural emotional distinctions, and subsequently derives reliable ground-truth annotations through rigorous human evaluation. The resulting benchmark comprises 10,962 instances across seven languages and 14 fine-grained emotion categories, with each language including 400 multimodal and 1,166 text-only samples. Comprehensive evaluations of 17 representative multilingual models reveal a dissociation between language consistency and cultural alignment, demonstrating that culturally grounded affective understanding remains a significant challenge for current models.

</details>


### [361] [SASA: Semantic-Aware Contrastive Learning Framework with Separated Attention for Triple Classification](https://arxiv.org/abs/2601.13035)
*Xu Xiaodan,Hu Xiaolin*

Main category: cs.CL

TL;DR: 本文提出了一种名为SASA的新框架，通过分离注意力机制和语义感知的对比学习，大幅提升了三元组分类（TC）模型在知识图谱上的表现，并在两个基准数据集上刷新了准确率纪录。


<details>
  <summary>Details</summary>
Motivation: 现有的基于文本的三元组分类方法通过自然语言描述提升了泛化能力，但存在两个主要问题：一是忽视了知识图谱不同组成部分间的有效语义交互，二是大多采用单一的二分类训练目标，导致语义表征学习不充分。

Method: SASA框架由两个核心创新组成：首先，提出分离注意力机制，将三元组编码为互相解耦的上下文表征，并通过更有效的交互方式进行融合；其次，引入语义感知的分层对比学习作为辅助训练目标，从局部和全局两个层面引导模型增强判别能力和丰富语义学习。

Result: 在FB15k-237和YAGO3-10两个基准数据集上，SASA方法显著超越了现有最优方法，准确率分别提升了5.9%和3.4%。

Conclusion: SASA能够有效解决以往方法的两大短板，在三元组分类任务上取得了新的性能突破，对KG中知识可靠性建模具有重要意义。

Abstract: Knowledge Graphs~(KGs) often suffer from unreliable knowledge, which restricts their utility. Triple Classification~(TC) aims to determine the validity of triples from KGs. Recently, text-based methods learn entity and relation representations from natural language descriptions, significantly improving the generalization capabilities of TC models and setting new benchmarks in performance. However, there are still two critical challenges. First, existing methods often ignore the effective semantic interaction among different KG components. Second, most approaches adopt single binary classification training objective, leading to insufficient semantic representation learning. To address these challenges, we propose \textbf{SASA}, a novel framework designed to enhance TC models via separated attention mechanism and semantic-aware contrastive learning~(CL). Specifically, we first propose separated attention mechanism to encode triples into decoupled contextual representations and then fuse them through a more effective interactive way. Then, we introduce semantic-aware hierarchical CL as auxiliary training objective to guide models in improving their discriminative capabilities and achieving sufficient semantic learning, considering both local level and global level CL. Experimental results across two benchmark datasets demonstrate that SASA significantly outperforms state-of-the-art methods. In terms of accuracy, we advance the state-of-the-art by +5.9\% on FB15k-237 and +3.4\% on YAGO3-10.

</details>


### [362] [Typhoon ASR Real-time: FastConformer-Transducer for Thai Automatic Speech Recognition](https://arxiv.org/abs/2601.13044)
*Warit Sirichotedumrong,Adisai Na-Thalang,Potsawee Manakul,Pittawat Taveekitworachai,Sittipong Sripaisarnmongkol,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: 本文提出了Typhoon ASR Real-time，一种高效低延迟的泰语流式语音识别模型，性能接近大型离线模型Whisper，但计算成本低45倍。作者还发布了高质量基准数据集，促进后续研究的可复现性和标准化。


<details>
  <summary>Details</summary>
Motivation: 当前泰语自动语音识别（ASR）领域主要依赖大规模离线模型，这些模型延迟高、不适合实时应用，且流式方案稀缺。本文动机是填补泰语高效低延迟流式ASR模型的空白。

Method: 作者提出了一种115M参数的FastConformer-Transducer结构，结合了高效的文本规范化流程，解决泰语转写中的歧义（如数字表述和重复标记）。同时，通过两阶段课程学习方法，实现了伊善（东北方言）适应而不损失中部泰语性能。

Result: 新模型与Whisper Large-v3相比，计算成本降低约45倍，同时实现了相当的准确率。文本规范化对性能提升达到模型扩大的效果。方言自适应良好且主流方言性能未受损。

Conclusion: Typhoon ASR Real-time为泰语流式语音识别提供了高效实用的解决方案，有助于推广纵深研究与标准化。公开的高标准评测数据增强了领域的可复现性和对比分析基础。

Abstract: Large encoder-decoder models like Whisper achieve strong offline transcription but remain impractical for streaming applications due to high latency. However, due to the accessibility of pre-trained checkpoints, the open Thai ASR landscape remains dominated by these offline architectures, leaving a critical gap in efficient streaming solutions. We present Typhoon ASR Real-time, a 115M-parameter FastConformer-Transducer model for low-latency Thai speech recognition. We demonstrate that rigorous text normalization can match the impact of model scaling: our compact model achieves a 45x reduction in computational cost compared to Whisper Large-v3 while delivering comparable accuracy. Our normalization pipeline resolves systemic ambiguities in Thai transcription --including context-dependent number verbalization and repetition markers (mai yamok) --creating consistent training targets. We further introduce a two-stage curriculum learning approach for Isan (north-eastern) dialect adaptation that preserves Central Thai performance. To address reproducibility challenges in Thai ASR, we release the Typhoon ASR Benchmark, a gold-standard human-labeled datasets with transcriptions following established Thai linguistic conventions, providing standardized evaluation protocols for the research community.

</details>


### [363] [Profiling German Text Simplification with Interpretable Model-Fingerprints](https://arxiv.org/abs/2601.13050)
*Lars Klöser,Mika Beele,Bodo Kraft*

Main category: cs.CL

TL;DR: 本文提出了Simplification Profiler工具，用于多维度诊断和分析大语言模型文本简化能力。通过模型生成的简化文本聚合，形成可解释的特征指纹，有效识别模型行为差异并依托线性分类器验证分析效果。工具对Prompt设计和少样本学习敏感，分类效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在文本简化上表现优异，但缺乏有效、可复现的行为诊断工具，尤其在数据稀缺或多样受众需求情形下，单一的相关性评估方法（例如与人类偏好相关）不足以反映模型实际能力。

Method: 提出Simplification Profiler，生成简化文本的多维特征指纹，通过聚合构建模型特征。评估方式是以线性分类器区分不同模型配置，衡量指纹对模型行为的敏感度，检验工具能否精细地区分不同简化策略（如不同Prompt和少样本示例）。

Result: Profiler能显著区分不同Prompt策略和微调效果，使用全部特征集分类F1分数达到71.9%，比简单基线提升超过48个百分点，显示出强大的识别和描述力。

Conclusion: Simplification Profiler为开发者提供细致且可操作性的模型简化行为分析，无需大量人工标注数据，有助于构建更高效、适应性更强的文本简化系统。

Abstract: While Large Language Models (LLMs) produce highly nuanced text simplifications, developers currently lack tools for a holistic, efficient, and reproducible diagnosis of their behavior. This paper introduces the Simplification Profiler, a diagnostic toolkit that generates a multidimensional, interpretable fingerprint of simplified texts. Multiple aggregated simplifications of a model result in a model's fingerprint. This novel evaluation paradigm is particularly vital for languages, where the data scarcity problem is magnified when creating flexible models for diverse target groups rather than a single, fixed simplification style. We propose that measuring a model's unique behavioral signature is more relevant in this context as an alternative to correlating metrics with human preferences. We operationalize this with a practical meta-evaluation of our fingerprints' descriptive power, which bypasses the need for large, human-rated datasets. This test measures if a simple linear classifier can reliably identify various model configurations by their created simplifications, confirming that our metrics are sensitive to a model's specific characteristics. The Profiler can distinguish high-level behavioral variations between prompting strategies and fine-grained changes from prompt engineering, including few-shot examples. Our complete feature set achieves classification F1-scores up to 71.9 %, improving upon simple baselines by over 48 percentage points. The Simplification Profiler thus offers developers a granular, actionable analysis to build more effective and truly adaptive text simplification systems.

</details>


### [364] [Alexandria: A Multi-Domain Dialectal Arabic Machine Translation Dataset for Culturally Inclusive and Linguistically Diverse LLMs](https://arxiv.org/abs/2601.13099)
*Abdellah El Mekki,Samar M. Magdy,Houdaifa Atou,Ruwa AbuHweidi,Baraah Qawasmeh,Omer Nacar,Thikra Al-hibiri,Razan Saadie,Hamzah Alsayadi,Nadia Ghezaiel Hammouda,Alshima Alkhazimi,Aya Hamod,Al-Yas Al-Ghafri,Wesam El-Sayed,Asila Al sharji,Mohamad Ballout,Anas Belfathi,Karim Ghaddar,Serry Sibaee,Alaa Aoun,Areej Asiri,Lina Abureesh,Ahlam Bashiti,Majdal Yousef,Abdulaziz Hafiz,Yehdih Mohamed,Emira Hamedtou,Brakehe Brahim,Rahaf Alhamouri,Youssef Nafea,Aya El Aatar,Walid Al-Dhabyani,Emhemed Hamed,Sara Shatnawi,Fakhraddin Alwajih,Khalid Elkhidir,Ashwag Alasmari,Abdurrahman Gerrio,Omar Alshahri,AbdelRahim A. Elmadany,Ismail Berrada,Amir Azad Adli Alkathiri,Fadi A Zaraket,Mustafa Jarrar,Yahya Mohamed El Hadj,Hassan Alhuzali,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 本文提出了Alexandria数据集，这是一个大规模、社区驱动的人类翻译数据集，涵盖13个阿拉伯国家和11个重要领域，并精细标注了城市来源和说话者性别，用于改进和评估阿拉伯语方言的机器翻译和大语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前主流机器翻译系统对阿拉伯语方言的泛化能力较差，难以满足广泛依赖方言交流的阿拉伯语使用者需求，缺乏细粒度、多场景、高质量的阿拉伯方言资源。

Method: 构建了涵盖13个国家、11个领域、107K多轮对话样本的新数据集，将翻译材料与实际城市来源和说话人-听话人性别配置关联，支持性别变异性和细致地方性分析。并利用自动和人工评测方法，系统评估机器翻译和大语言模型在多方言翻译任务上的表现。

Result: Alexandria实现了对方言语言的精细覆盖，能够更好地捕捉地方及性别变异。实验结果表明，当前MT和LLM在不同方言及子方言上的翻译仍存在明显短板。

Conclusion: Alexandria数据集为阿拉伯语方言的MT和LLM研究提供了强有力的新资源和基准，促进相关系统提升泛化及现实场景应用能力，同时揭示了阿拉伯语方言翻译领域仍待突破的关键挑战。

Abstract: Arabic is a highly diglossic language where most daily communication occurs in regional dialects rather than Modern Standard Arabic. Despite this, machine translation (MT) systems often generalize poorly to dialectal input, limiting their utility for millions of speakers. We introduce \textbf{Alexandria}, a large-scale, community-driven, human-translated dataset designed to bridge this gap. Alexandria covers 13 Arab countries and 11 high-impact domains, including health, education, and agriculture. Unlike previous resources, Alexandria provides unprecedented granularity by associating contributions with city-of-origin metadata, capturing authentic local varieties beyond coarse regional labels. The dataset consists of multi-turn conversational scenarios annotated with speaker-addressee gender configurations, enabling the study of gender-conditioned variation in dialectal use. Comprising 107K total samples, Alexandria serves as both a training resource and a rigorous benchmark for evaluating MT and Large Language Models (LLMs). Our automatic and human evaluation of Arabic-aware LLMs benchmarks current capabilities in translating across diverse Arabic dialects and sub-dialects, while exposing significant persistent challenges.

</details>


### [365] [Leveraging Lora Fine-Tuning and Knowledge Bases for Construction Identification](https://arxiv.org/abs/2601.13105)
*Liu Kaipeng,Wu Ling*

Main category: cs.CL

TL;DR: 本文利用LoRA微调的大语言模型，结合检索增强生成（RAG）框架，对英语双宾结构进行自动识别。结果显示，LoRA微调后的Qwen3-8B模型显著优于原生Qwen3-MAX和仅依赖理论的RAG系统。


<details>
  <summary>Details</summary>
Motivation: 语言学中双宾结构识别对自动语法分析与自然语言理解至关重要。现有方法对语义的把握有限，作者尝试通过模型微调和RAG增强理解能力。

Method: 作者将带有双宾结构标注的英国国家语料库作为二分类任务数据，使用LoRA对Qwen3-8B模型进行微调，并嵌入RAG框架，与未微调的Qwen3-MAX及理论检索系统对比。

Result: LoRA-微调的Qwen3-8B模型在分类任务上效果显著超越对照组。同时，详细误差分析表明，微调后模型更关注语义层面，而非单纯表面形式匹配。

Conclusion: LoRA微调结合RAG方法可有效提升大模型对英语句法结构（尤其是双宾结构）的语义理解能力，显示了该方法应用于语法识别任务的巨大潜力。

Abstract: This study investigates the automatic identification of the English ditransitive construction by integrating LoRA-based fine-tuning of a large language model with a Retrieval-Augmented Generation (RAG) framework.A binary classification task was conducted on annotated data from the British National Corpus. Results demonstrate that a LoRA-fine-tuned Qwen3-8B model significantly outperformed both a native Qwen3-MAX model and a theory-only RAG system. Detailed error analysis reveals that fine-tuning shifts the model's judgment from a surface-form pattern matching towards a more semantically grounded understanding based.

</details>


### [366] [CORE-T: COherent REtrieval of Tables for Text-to-SQL](https://arxiv.org/abs/2601.13111)
*Hassan Soliman,Vivek Gupta,Dan Roth,Iryna Gurevych*

Main category: cs.CL

TL;DR: CORE-T是一种无需训练即可提升多表检索准确率和效率的框架，通过大模型生成的表格元数据和预计算的兼容性缓存，实现更好更快的多表选择。


<details>
  <summary>Details</summary>
Motivation: 在复杂的text-to-SQL任务中，用户查询往往需要跨多表联接，准确选择相关表是性能瓶颈，尤其在没有数据库标识等清晰范围界定的情况下更难。现有方法要么检索噪声大，要么推理成本高。

Method: CORE-T方法融合了三个要点：用大模型生成表的用途元数据、预计算表之间的兼容性缓存、测试时用稠密检索筛选候选表，再用LLM选出可联接子集，并简单加减选出最兼容的表，无需训练即可使用。

Result: 在Bird、Spider、MMQA三个数据集上，CORE-T在表选择F1上最多提升22.7点，检索表数减少42%，在多表执行准确率上，Bird提升5.0点，MMQA提升6.9点，并且比重LLM基线省4-5倍Token。

Conclusion: CORE-T大幅提升了多表检索准确率和效率，且无需训练，整体方案轻量级，对多源大规模异构表集的text-to-SQL应用有重要价值。

Abstract: Realistic text-to-SQL workflows often require joining multiple tables. As a result, accurately retrieving the relevant set of tables becomes a key bottleneck for end-to-end performance. We study an open-book setting where queries must be answered over large, heterogeneous table collections pooled from many sources, without clean scoping signals such as database identifiers. Here, dense retrieval (DR) achieves high recall but returns many distractors, while join-aware alternatives often rely on extra assumptions and/or incur high inference overhead. We propose CORE-T, a scalable, training-free framework that enriches tables with LLM-generated purpose metadata and pre-computes a lightweight table-compatibility cache. At inference time, DR returns top-K candidates; a single LLM call selects a coherent, joinable subset, and a simple additive adjustment step restores strongly compatible tables. Across Bird, Spider, and MMQA, CORE-T improves table-selection F1 by up to 22.7 points while retrieving up to 42% fewer tables, improving multi-table execution accuracy by up to 5.0 points on Bird and 6.9 points on MMQA, and using 4-5x fewer tokens than LLM-intensive baselines.

</details>


### [367] [Agentic Conversational Search with Contextualized Reasoning via Reinforcement Learning](https://arxiv.org/abs/2601.13115)
*Fengran Mo,Yifan Gao,Sha Li,Hansi Zeng,Xin Liu,Zhaoxuan Tan,Xian Li,Jianshu Chen,Dakuo Wang,Meng Jiang*

Main category: cs.CL

TL;DR: 本文提出了一种新的对话式智能体，能够在多轮对话中动态结合检索与推理，通过强化学习实现探索性和自适应应答，实验效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，人机多轮对话成为主流，但针对对话中用户意图的动态变化和任务交互，现有方法多采用静态处理流程，无法有效优化各环节的协同。单轮“深度搜索代理”方法虽有进展，但对多轮互动仍力有不逮，因此需要新的框架优化多轮对话下的检索与生成。

Method: 提出一种对话智能体，能够在多轮对话中交错使用搜索（信息检索）与推理生成，通过强化学习和定制化奖励来指导系统适应用户目标的动态演化，实现更灵活、个性化的对话管理。

Result: 在四个常用对话数据集上，所提方法实验效果优于多个现有强基线，验证了方案的有效性。

Conclusion: 通过强化学习优化在多轮对话中检索和推理的协同，能更好适应用户意图的变化，有助提升对话系统在任务助理和信息检索等应用的综合表现。

Abstract: Large Language Models (LLMs) have become a popular interface for human-AI interaction, supporting information seeking and task assistance through natural, multi-turn dialogue. To respond to users within multi-turn dialogues, the context-dependent user intent evolves across interactions, requiring contextual interpretation, query reformulation, and dynamic coordination between retrieval and generation. Existing studies usually follow static rewrite, retrieve, and generate pipelines, which optimize different procedures separately and overlook the mixed-initiative action optimization simultaneously. Although the recent developments in deep search agents demonstrate the effectiveness in jointly optimizing retrieval and generation via reasoning, these approaches focus on single-turn scenarios, which might lack the ability to handle multi-turn interactions. We introduce a conversational agent that interleaves search and reasoning across turns, enabling exploratory and adaptive behaviors learned through reinforcement learning (RL) training with tailored rewards towards evolving user goals. The experimental results across four widely used conversational benchmarks demonstrate the effectiveness of our methods by surpassing several existing strong baselines.

</details>


### [368] [Adversarial Alignment: Ensuring Value Consistency in Large Language Models for Sensitive Domains](https://arxiv.org/abs/2601.13137)
*Yuan Gao,Zhigang Liu,Xinyu Yao,Bo Chen,Xiaobing Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种对抗对齐框架，通过持续预训练、指令微调和对抗训练提升大模型在敏感领域（如种族、社会、政治）上的价值一致性，并训练了VC-LLM模型，实验显示其在中英文测试中优于主流模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理敏感领域话题时存在偏见和价值观不一致问题，这对模型的安全性和公正性提出了挑战，因此亟需提高模型在这些领域的话语一致性和价值观自洽能力。

Method: 提出对抗对齐框架，包括三部分：1）持续预训练模型；2）指令微调以接收规范化指令；3）对抗训练，利用Attacker生成争议性问题，Actor生成价值一致应答，Critic筛选高质量响应。随后，训练了VC-LLM，并构建了中英双语评测数据集。

Result: VC-LLM模型在中文和英文的敏感领域测试集上均优于现有主流大语言模型，验证了所提方法在提升价值一致性和降低偏见方面的有效性。

Conclusion: 所提对抗对齐框架和VC-LLM具备提升敏感领域话题价值一致性的能力，为大模型在实际应用中的安全合规性提供了有力保障。

Abstract: With the wide application of large language models (LLMs), the problems of bias and value inconsistency in sensitive domains have gradually emerged, especially in terms of race, society and politics. In this paper, we propose an adversarial alignment framework, which enhances the value consistency of the model in sensitive domains through continued pre-training, instruction fine-tuning and adversarial training. In adversarial training, we use the Attacker to generate controversial queries, the Actor to generate responses with value consistency, and the Critic to filter and ensure response quality. Furthermore, we train a Value-Consistent Large Language Model, VC-LLM, for sensitive domains, and construct a bilingual evaluation dataset in Chinese and English. The experimental results show that VC-LLM performs better than the existing mainstream models in both Chinese and English tests, verifying the effectiveness of the method. Warning: This paper contains examples of LLMs that are offensive or harmful in nature.

</details>


### [369] [Probe and Skip: Self-Predictive Token Skipping for Efficient Long-Context LLM Inference](https://arxiv.org/abs/2601.13155)
*Zimeng Wu,Donghao Wang,Chaozhe Jin,Jiaxin Chen,Yunhong Wang*

Main category: cs.CL

TL;DR: 本论文提出了一种高效的长上下文大模型推理方法SPTS（Self-Predictive Token Skipping），通过选择性跳过部分token来极大提升推理速度，同时基本不损失精度。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文推理方法虽然提升了LLM的推理能力，但带来了显著的计算开销。已存在的token剪枝和跳过方法仍面临加速有限、信号滞后和冗余干扰等问题，难以在速度和准确率之间取得最优平衡。

Method: 提出SPTS框架，无需训练即可高效推理。该框架包括针对多头注意力机制的部分注意力推测（PAP）和前馈网络的低秩变换推测（LTP）两大跳token策略，同时引入多阶段延迟剪枝（MSDP），在网络各层逐步优化剪枝和跳token方案。

Result: 在大量实验中，SPTS方法在不影响模型性能的前提下，实现prefilling和端到端生成分别高达2.46倍和2.29倍的加速。

Conclusion: SPTS能有效提升长上下文大模型推理效率，其方法简单易用、性能优异，具备较大实际价值。源码将在论文接收后开放。

Abstract: Long-context inference enhances the reasoning capability of Large Language Models (LLMs) while incurring significant computational overhead. Token-oriented methods, such as pruning and skipping, have shown promise in reducing inference latency, but still suffer from inherently limited acceleration potential, outdated proxy signals, and redundancy interference, thus yielding suboptimal speed-accuracy trade-offs. To address these challenges, we propose SPTS (Self-Predictive Token Skipping), a training-free framework for efficient long-context LLM inference. Specifically, motivated by the thought of probing the influence of targeted skipping layers, we design two component-specific strategies for selective token skipping: Partial Attention Probing (PAP) for multi-head attention, which selects informative tokens by performing partial forward attention computation, and Low-rank Transformation Probing (LTP) for feed forward network, which constructs a low-rank proxy network to predict token transformations. Furthermore, a Multi-Stage Delayed Pruning (MSDP) strategy reallocates the skipping budget and progressively prunes redundant tokens across layers. Extensive experiments demonstrate the effectiveness of our method, achieving up to 2.46$\times$ and 2.29$\times$ speedups for prefilling and end-to-end generation, respectively, while maintaining state-of-the-art model performance. The source code will be publicly available upon paper acceptance.

</details>


### [370] [Medical Triage as Pairwise Ranking: A Benchmark for Urgency in Patient Portal Messages](https://arxiv.org/abs/2601.13178)
*Joseph Gatto,Parker Seegmiller,Timothy Burdick,Philip Resnik,Roshnik Rahat,Sarah DeLozier,Sarah M. Preum*

Main category: cs.CL

TL;DR: 该论文提出了首个针对异步门诊患者消息分诊的大规模公开数据集PMR-Bench，并基于此任务，训练大语言模型（LLMs）实现“消息紧急性”两两对比排序，从而提升医生工作效率。


<details>
  <summary>Details</summary>
Motivation: 随着电子健康记录和患者门户消息的激增，医生面临大量需分拣的患者消息。现有研究缺乏面向真实门诊消息分诊的公开数据和合适的任务设计，亟需提高临床分诊自动化程度。

Method: 作者设计了PMR-Bench数据集，既包含患者自由书写的消息，也结合了真实EHR数据，模拟门诊分诊实际情景。任务形式为两条消息谁更紧急的成对推断，并开发自动数据标注管道。基于该数据集，训练了两类模型：UrgentReward（基于Bradley-Terry目标）、UrgentSFT（基于next token预测目标）用于两两消息紧急性分类。

Result: 实验表明，UrgentSFT模型在PMR-Bench基准集上取得最佳表现，而UrgentReward模型在低资源环境下显示独到优势。与常规8B大模型相比，UrgentSFT-8B和UrgentReward-8B能在收件箱排序指标上分别提升15和16个百分点。

Conclusion: 提出的PMR-Bench填补了临床分诊领域公开数据集的空白，所建模型切实提升了患者消息分拣效率，为门诊分诊自动化、智能化提供了新的基线和工具支持。

Abstract: Medical triage is the task of allocating medical resources and prioritizing patients based on medical need. This paper introduces the first large-scale public dataset for studying medical triage in the context of asynchronous outpatient portal messages. Our novel task formulation views patient message triage as a pairwise inference problem, where we train LLMs to choose `"which message is more medically urgent" in a head-to-head tournament-style re-sort of a physician's inbox. Our novel benchmark PMR-Bench contains 1569 unique messages and 2,000+ high-quality test pairs for pairwise medical urgency assessment alongside a scalable training data generation pipeline. PMR-Bench includes samples that contain both unstructured patient-written messages alongside real electronic health record (EHR) data, emulating a real-world medical triage scenario.
  We develop a novel automated data annotation strategy to provide LLMs with in-domain guidance on this task. The resulting data is used to train two model classes, UrgentReward and UrgentSFT, leveraging Bradley-Terry and next token prediction objective, respectively to perform pairwise urgency classification. We find that UrgentSFT achieves top performance on PMR-Bench, with UrgentReward showing distinct advantages in low-resource settings. For example, UrgentSFT-8B and UrgentReward-8B provide a 15- and 16-point boost, respectively, on inbox sorting metrics over off-the-shelf 8B models. Paper resources can be found at https://tinyurl.com/Patient-Message-Triage

</details>


### [371] [OpenExempt: A Diagnostic Benchmark for Legal Reasoning and a Framework for Creating Custom Benchmarks on Demand](https://arxiv.org/abs/2601.13183)
*Sergio Servantez,Sarah B. Lawsky,Rajiv Jain,Daniel W. Linna,Kristian Hammond*

Main category: cs.CL

TL;DR: 本文提出了OpenExempt框架和基准，用于动态生成法律推理任务，并对现有大模型在复杂法律推理中的表现进行了细粒度分析和评估。


<details>
  <summary>Details</summary>
Motivation: 目前推理基准通常依赖静态问答对，只能给出性能快照，难以揭示模型在复杂规则领域（如法律）中的具体失败模式。法律领域构建基准成本高，问题设计难以覆盖多样化推理需求。因此需要一种更动态和诊断性的评测方法。

Method: OpenExempt框架基于专家制定的美国破产法符号化表述，动态生成大规模、多样化的自然语言法律推理任务及其可机算的标准答案。该框架允许用户自主控制任务难度和范围，可隔离考察单一推理技能。基于此框架，作者构建了包含9,765个样本、覆盖九个评测方向的OpenExempt基准。

Result: 在13个不同语言大模型上测试发现，在推理路径变长和出现混淆表述的情况下，模型性能会大幅下降，展现出明显的能力断崖。

Conclusion: OpenExempt框架与基准为法律推理类任务的精细诊断和评估提供了工具，有助于推动推理系统未来在复杂规则领域的改进。框架和数据集已公开，便于相关研究持续发展。

Abstract: Reasoning benchmarks have played a crucial role in the progress of language models. Yet rigorous evaluation remains a significant challenge as static question-answer pairs provide only a snapshot of performance, compressing complex behavior into a single accuracy metric. This limitation is especially true in complex, rule-bound domains such as law, where existing benchmarks are costly to build and ill suited for isolating specific failure modes. To address this, we introduce OpenExempt, a framework and benchmark for diagnostic evaluation of legal reasoning. The OpenExempt Framework uses expert-crafted symbolic representations of U.S. Bankruptcy Code statutes to dynamically generate a large space of natural language reasoning tasks and their machine-computable solutions on demand. This gives users fine-grained control over task complexity and scope, allowing individual reasoning skills to be probed in isolation. Using this system, we construct the OpenExempt Benchmark, a diagnostic benchmark for legal reasoning with 9,765 samples across nine evaluation suites designed to carefully probe model capabilities. Experiments on 13 diverse language models reveal sharp performance cliffs that emerge only under longer reasoning paths and in the presence of obfuscating statements. We release the framework and benchmark publicly to support research aimed at understanding and improving the next generation of reasoning systems.

</details>


### [372] [Beyond Single-shot Writing: Deep Research Agents are Unreliable at Multi-turn Report Revision](https://arxiv.org/abs/2601.13217)
*Bingsen Chen,Boyan Li,Ping Nie,Yuyu Zhang,Xi Ye,Chen Zhao*

Main category: cs.CL

TL;DR: 该论文提出了Mr Dre评测工具，用于评估深度研究智能体（DRA）在报告多轮修订中的表现，打破了以往单次生成的评测方法，揭示了DRA在多轮修订过程中会损失已有内容和引用质量的关键缺陷。


<details>
  <summary>Details</summary>
Motivation: 目前DRA主要把报告生成当作一次性任务，忽略了人类研究者通过自省和反馈多轮修订文本的真实流程。本研究旨在探索DRA在用户反馈下多轮修订报告的能力，这是此前很少被系统性评估的方面。

Method: 作者设计了Mr Dre评测套件，包括：（1）一个覆盖全面性、事实性和表达的统一长文档评估协议；（2）基于人工验证反馈的多轮修订仿真流程。作者用该评测对五个主流DRA系统进行对比测试。

Result: 实验发现，尽管DRA能够响应多数用户反馈，但在16-27%的既有内容和引用质量上出现退步。在多轮修订后，即使表现最好的DRA也无法完全保留先前编辑的内容，经常无意破坏未涉及反馈的部分。此外，推理阶段的简易修正（如提示工程、专用修订子智能体）无法根本解决此类问题。

Conclusion: 现有DRA在报告多轮修订方面存在明显短板，无法如人类研究者那样稳定保留和改进报告内容。文中提出的Mr Dre评测工具为该领域提供了新的系统性评估方法，有助于推动AI报告生成技术更贴近真实科研实践。

Abstract: Existing benchmarks for Deep Research Agents (DRAs) treat report generation as a single-shot writing task, which fundamentally diverges from how human researchers iteratively draft and revise reports via self-reflection or peer feedback. Whether DRAs can reliably revise reports with user feedback remains unexplored. We introduce Mr Dre, an evaluation suite that establishes multi-turn report revision as a new evaluation axis for DRAs. Mr Dre consists of (1) a unified long-form report evaluation protocol spanning comprehensiveness, factuality, and presentation, and (2) a human-verified feedback simulation pipeline for multi-turn revision. Our analysis of five diverse DRAs reveals a critical limitation: while agents can address most user feedback, they also regress on 16-27% of previously covered content and citation quality. Over multiple revision turns, even the best-performing agents leave significant headroom, as they continue to disrupt content outside the feedback's scope and fail to preserve earlier edits. We further show that these issues are not easily resolvable through inference-time fixes such as prompt engineering and a dedicated sub-agent for report revision.

</details>


### [373] [Autoregressive Models Rival Diffusion Models at ANY-ORDER Generation](https://arxiv.org/abs/2601.13228)
*Tianqi Du,Lizhe Fang,Weijie Yang,Chenheng Zhang,Zeming Wei,Yifei Wang,Yisen Wang*

Main category: cs.CL

TL;DR: 本文提出了A3（Any-order Any-subset Autoregressive）模型，将自回归（AR）建模的严格性与扩散模型的灵活性相结合，实现任意顺序、任意子集的生成，对多种自然语言处理任务表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有扩散语言模型虽支持任意顺序和双向条件生成，但其单步预测机制导致建模深度有限，生成质量与稳定性通常逊于自回归模型。作者希望兼顾灵活性和高样本质量。

Method: 作者将扩散风格的训练转换为结构化的多组预测流程，并提出A3框架：将标准自回归分解扩展到任意token组和生成顺序。具体实现上，采用双流注意力架构，并设计渐进式策略，将预训练的AR模型向任意顺序预测适配。

Result: 在问答、常识推理、故事补全等任务中，A3在保持灵活解码能力的同时，性能优于已有的扩散模型。

Conclusion: A3为灵活、高效的新型语言建模范式提供了统一思路，兼具AR模型的严谨性和扩散模型的生成灵活性。

Abstract: Diffusion language models enable any-order generation and bidirectional conditioning, offering appealing flexibility for tasks such as infilling, rewriting, and self-correction. However, their formulation-predicting one part of a sequence from another within a single-step dependency-limits modeling depth and often yields lower sample quality and stability than autoregressive (AR) models. To address this, we revisit autoregressive modeling as a foundation and reformulate diffusion-style training into a structured multi-group prediction process. We propose Any-order Any-subset Autoregressive modeling (A3), a generalized framework that extends the standard AR factorization to arbitrary token groups and generation orders. A3 preserves the probabilistic rigor and multi-layer dependency modeling of AR while inheriting diffusion models' flexibility for parallel and bidirectional generation. We implement A3 through a two-stream attention architecture and a progressive adaptation strategy that transitions pretrained AR models toward any-order prediction. Experiments on question answering, commonsense reasoning, and story infilling demonstrate that A3 outperforms diffusion-based models while maintaining flexible decoding. This work offers a unified approach for a flexible, efficient, and novel language modeling paradigm.

</details>


### [374] [Aligning Agentic World Models via Knowledgeable Experience Learning](https://arxiv.org/abs/2601.13247)
*Baochang Ren,Yunzhi Yao,Rui Sun,Shuofei Qiao,Ningyu Zhang,Huajun Chen*

Main category: cs.CL

TL;DR: 当前大型语言模型（LLMs）在理解物理世界方面存在关键短板，WorldMind框架通过自建知识库并融合环境反馈，有效提升了模型的现实物理可执行性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: LLMs虽然掌握了丰富的语义知识，但缺少对物理世界规律的理解，因此生成的计划常常无法在物理上执行；并且通过持续训练或微调难以灵活适应动态环境，成本高且效率低。

Method: 提出WorldMind框架，通过自动构建符号化的世界知识库，整合实际环境反馈。框架结合‘过程经验’（根据预测误差确保物理可行性）与‘目标经验’（通过成功轨迹优化任务达成），持续强化模型对物理环境的理解和适应。

Result: 在EB-ALFRED和EB-Habitat两个实验平台上，WorldMind展现出相比传统方法更优的性能，并且具备强大的跨模型与跨环境迁移能力。

Conclusion: WorldMind有效弥补了LLMs在物理可行性方面的弱点，以更低的成本提升了模型模拟真实世界的能力，对AI agents的泛化和实用性具有重要意义。

Abstract: Current Large Language Models (LLMs) exhibit a critical modal disconnect: they possess vast semantic knowledge but lack the procedural grounding to respect the immutable laws of the physical world. Consequently, while these agents implicitly function as world models, their simulations often suffer from physical hallucinations-generating plans that are logically sound but physically unexecutable. Existing alignment strategies predominantly rely on resource-intensive training or fine-tuning, which attempt to compress dynamic environmental rules into static model parameters. However, such parametric encapsulation is inherently rigid, struggling to adapt to the open-ended variability of physical dynamics without continuous, costly retraining. To bridge this gap, we introduce WorldMind, a framework that autonomously constructs a symbolic World Knowledge Repository by synthesizing environmental feedback. Specifically, it unifies Process Experience to enforce physical feasibility via prediction errors and Goal Experience to guide task optimality through successful trajectories. Experiments on EB-ALFRED and EB-Habitat demonstrate that WorldMind achieves superior performance compared to baselines with remarkable cross-model and cross-environment transferability.

</details>


### [375] [Beyond Cosine Similarity: Taming Semantic Drift and Antonym Intrusion in a 15-Million Node Turkish Synonym Graph](https://arxiv.org/abs/2601.13251)
*Ebubekir Tosun,Mehmet Emin Buldur,Özay Ezerceli,Mahmoud ElHussieni*

Main category: cs.CL

TL;DR: 本文提出了一个大规模的语义聚类系统，能够有效区分同义词和反义词，构建了数百万高精度语义簇，对语义检索和生成有显著提升。


<details>
  <summary>Details</summary>
Motivation: 神经嵌入在区分同义词与反义词方面存在明显短板，导致在进行语义聚类时常常把意义相反的词归为一类。作者希望解决这一难题，提升语义检索和生成的准确性。

Method: 1）构建了包含84.3万个涵盖同义、反义、同上位概念的词对数据集，由大模型增强并结合人工字典校验；2）设计了针对三类语义关系的判别器，宏F1达90%；3）提出新颖的软到硬聚类算法，采用两阶段扩展-剪枝和拓扑投票，有效避免语义漂移及多义现象，每个词只属于一个聚类。

Result: 完成了对1500万个词条、5.2亿关系的挖掘，最终生成290万个高精度语义聚类簇，验证聚类效果优异。

Conclusion: 该体系针对神经嵌入缺陷提供了解决方案，可显著提升高精度语义搜索及低资源语言的生成能力，为日后语义资源建设和下游应用打下坚实基础。

Abstract: Neural embeddings have a notorious blind spot: they can't reliably tell synonyms apart from antonyms. Consequently, increasing similarity thresholds often fails to prevent opposites from being grouped together. We've built a large-scale semantic clustering system specifically designed to tackle this problem head on. Our pipeline chews through 15 million lexical items, evaluates a massive 520 million potential relationships, and ultimately generates 2.9 million high-precision semantic clusters. The system makes three primary contributions. First, we introduce a labeled dataset of 843,000 concept pairs spanning synonymy, antonymy, and co-hyponymy, constructed via Gemini 2.5-Flash LLM augmentation and verified using human-curated dictionary resources. Second, we propose a specialized three-way semantic relation discriminator that achieves 90% macro-F1, enabling robust disambiguation beyond raw embedding similarity. Third, we introduce a novel soft-to-hard clustering algorithm that mitigates semantic drift preventing erroneous transitive chains (e.g., hot -> spicy -> pain -> depression) while simultaneously resolving polysemy. Our approach employs a topology-aware two-stage expansion-pruning procedure with topological voting, ensuring that each term is assigned to exactly one semantically coherent cluster. The resulting resource enables high-precision semantic search and retrieval-augmented generation, particularly for morphologically rich and low-resource languages where existing synonym databases remain sparse.

</details>


### [376] [A Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource Languages: The Turkish Semantic Relations Corpus](https://arxiv.org/abs/2601.13253)
*Ebubekir Tosun,Mehmet Emin Buldur,Özay Ezerceli,Mahmoud ElHussieni*

Main category: cs.CL

TL;DR: 本文提出了一种针对低资源语言（以土耳其语为例）生成大规模语义关系数据集的混合方法，最终构建了包含84.3万个语义对的大型语料库，并验证其高效性、准确性和可推广性。


<details>
  <summary>Details</summary>
Motivation: 低资源语言在自然语言处理（NLP）领域缺乏高质量大规模语义关系数据，严重制约了相关模型的研究和应用。现有资源规模小且构建成本高。该研究旨在提出低成本、可扩展的新方法，满足低资源语言的NLP发展需求。

Method: 方法分为三步：（1）利用FastText词向量及凝聚聚类算法提取语义簇；（2）采用Gemini 2.5-Flash模型自动分类语义关系；（3）结合人工精选字典资源整合与补充。最终获得包含同义词、反义词及共同下义词三类关系的大规模语料。

Result: 最终构建了包含84.3万个独特土耳其语语义关系对的数据集，是现有资源规模的10倍，而成本仅为65美元。下游任务测试中，embedding检索Top-1准确率达90%，分类模型F1-macro同样达90%。方法展现出极高性价比与实用性。

Conclusion: 提出的方法有效缓解了土耳其语等低资源语言NLP中的数据瓶颈，具有极强的可扩展性和应用前景。数据集与模型已公开发布，为低资源语言NLP研究提供了宝贵资源。

Abstract: We present a hybrid methodology for generating large-scale semantic relationship datasets in low-resource languages, demonstrated through a comprehensive Turkish semantic relations corpus. Our approach integrates three phases: (1) FastText embeddings with Agglomerative Clustering to identify semantic clusters, (2) Gemini 2.5-Flash for automated semantic relationship classification, and (3) integration with curated dictionary sources. The resulting dataset comprises 843,000 unique Turkish semantic pairs across three relationship types (synonyms, antonyms, co-hyponyms) representing a 10x scale increase over existing resources at minimal cost ($65). We validate the dataset through two downstream tasks: an embedding model achieving 90% top-1 retrieval accuracy and a classification model attaining 90% F1-macro. Our scalable protocol addresses critical data scarcity in Turkish NLP and demonstrates applicability to other low-resource languages. We publicly release the dataset and models.

</details>


### [377] [Stop Taking Tokenizers for Granted: They Are Core Design Decisions in Large Language Models](https://arxiv.org/abs/2601.13260)
*Sawsan Alqahtani,Mir Tafseer Nayeem,Md Tahmid Rahman Laskar,Tasnim Mohiuddin,M Saiful Bari*

Main category: cs.CL

TL;DR: 论文指出现有大模型中的分词机制（Tokenization）设计不统一且缺乏理论支撑，主张应将分词看作核心建模决策，并提倡与模型协同设计，结合语言学、领域和应用需求，推动更加公平、高效和适应性的语言技术。


<details>
  <summary>Details</summary>
Motivation: 当前常用的子词分词方法如BPE虽然具备扩展性，但在不同语言和领域下导致语义失配、放大偏见和能力浪费；而且分词普遍被当作简单预处理，忽视了其对模型本身的深远影响。

Method: 论文提出要以上下文感知的视角，将分词与模型协同设计，并提出建立标准化评估及透明报告机制，使各种分词方案能够被问责和对比。同时强调分词设计需综合语言学、应用领域以及实际部署的考量。

Result: 论文未给出详细实验结果，而是提出了设计准则和理论框架。强调如果将分词作为核心设计问题看待，有潜力实现更加公平、高效、灵活的语言技术。

Conclusion: 分词不应被视为单纯的技术前处理步骤，而应成为模型设计的核心问题。与模型协同、标准化评估和流程透明将推动语言技术在多语言、多场景下的公平性和效率。

Abstract: Tokenization underlies every large language model, yet it remains an under-theorized and inconsistently designed component. Common subword approaches such as Byte Pair Encoding (BPE) offer scalability but often misalign with linguistic structure, amplify bias, and waste capacity across languages and domains. This paper reframes tokenization as a core modeling decision rather than a preprocessing step. We argue for a context-aware framework that integrates tokenizer and model co-design, guided by linguistic, domain, and deployment considerations. Standardized evaluation and transparent reporting are essential to make tokenization choices accountable and comparable. Treating tokenization as a core design problem, not a technical afterthought, can yield language technologies that are fairer, more efficient, and more adaptable.

</details>


### [378] [Unlearning in LLMs: Methods, Evaluation, and Open Challenges](https://arxiv.org/abs/2601.13264)
*Tyler Lizzo,Larry Heck*

Main category: cs.CL

TL;DR: 本文系统性综述了大语言模型（LLMs）中的机器遗忘（Unlearning）方法，涵盖了现有方法分类、评估体系及未来挑战。


<details>
  <summary>Details</summary>
Motivation: 大语言模型应用广泛，但其在隐私、版权、安全和偏见等方面引发诸多社会关注。人们希望能在不完全重训的情况下，有选择地移除模型中的知识或数据，促使机器遗忘成为有价值且迫切的研究课题。

Method: 本综述将LLM的遗忘方法分为数据中心、参数中心、结构中心、混合型及其他策略，并梳理了用于评价遗忘效果、知识保留和鲁棒性的基准与数据集。

Result: 论文总结了当前各种遗忘方法、相关评测框架，并指出了现有研究于高效性、理论保证、多语言/多模态遗忘及对抗性遗忘等方面仍存在重大挑战。

Conclusion: 通过梳理研究进展与开放问题，本文为今后开发可靠、负责任的LLM遗忘技术提供了路线图。

Abstract: Large language models (LLMs) have achieved remarkable success across natural language processing tasks, yet their widespread deployment raises pressing concerns around privacy, copyright, security, and bias. Machine unlearning has emerged as a promising paradigm for selectively removing knowledge or data from trained models without full retraining. In this survey, we provide a structured overview of unlearning methods for LLMs, categorizing existing approaches into data-centric, parameter-centric, architecture-centric, hybrid, and other strategies. We also review the evaluation ecosystem, including benchmarks, metrics, and datasets designed to measure forgetting effectiveness, knowledge retention, and robustness. Finally, we outline key challenges and open problems, such as scalable efficiency, formal guarantees, cross-language and multimodal unlearning, and robustness against adversarial relearning. By synthesizing current progress and highlighting open directions, this paper aims to serve as a roadmap for developing reliable and responsible unlearning techniques in large language models.

</details>


### [379] [A BERTology View of LLM Orchestrations: Token- and Layer-Selective Probes for Efficient Single-Pass Classification](https://arxiv.org/abs/2601.13288)
*Gonzalo Ariel Meyoyan,Luciano Del Corro*

Main category: cs.CL

TL;DR: 该论文提出了一种在LLM推理过程中复用隐藏状态进行分类的方法，用轻量级探针直接在生成前向过程中进行分类预测，减少了额外模型带来的延迟和资源消耗。


<details>
  <summary>Details</summary>
Motivation: 当前生产级LLM系统在安全检测或其他分类任务上通常采用独立的模型，导致延迟增加、显存消耗大以及部署复杂。本工作试图通过利用已经计算得到的LLM隐藏状态信息，在不中断主推理流程的前提下完成额外的分类任务。

Method: 作者提出了一种两阶段聚合器，首先对每层的token进行总结，然后跨层聚合所有层的表示，获得最终用于分类的表征。具体实现包括直接池化、轻量得分注意门（scoring-attention gate，约10万参数）以及最大支持3500万参数的降维多头自注意力探针。

Result: 在安全和情感分析基准测试上，该方法优于仅利用logit的复用方法（如MULI），并能在保持近似服务级延迟的同时，取得与更大、更专用的基线模型相当的效果。

Conclusion: 此方法能有效兼容LLM推理主流程，显著降低系统复杂度与资源开销，在保证性能前提下，减少了部署和推理过程中的延迟和VRAM需求。

Abstract: Production LLM systems often rely on separate models for safety and other classification-heavy steps, increasing latency, VRAM footprint, and operational complexity. We instead reuse computation already paid for by the serving LLM: we train lightweight probes on its hidden states and predict labels in the same forward pass used for generation. We frame classification as representation selection over the full token-layer hidden-state tensor, rather than committing to a fixed token or fixed layer (e.g., first-token logits or final-layer pooling). To implement this, we introduce a two-stage aggregator that (i) summarizes tokens within each layer and (ii) aggregates across layer summaries to form a single representation for classification. We instantiate this template with direct pooling, a 100K-parameter scoring-attention gate, and a downcast multi-head self-attention (MHA) probe with up to 35M trainable parameters. Across safety and sentiment benchmarks our probes improve over logit-only reuse (e.g., MULI) and are competitive with substantially larger task-specific baselines, while preserving near-serving latency and avoiding the VRAM and latency costs of a separate guard-model pipeline.

</details>


### [380] [OI-Bench: An Option Injection Benchmark for Evaluating LLM Susceptibility to Directive Interference](https://arxiv.org/abs/2601.13300)
*Yow-Fu Liou,Yu-Chien Tang,Yu-Hsiang Liu,An-Zi Yen*

Main category: cs.CL

TL;DR: 本论文提出了一种新的评测方法（option injection），通过在多项选择题中加入带有误导性指令的选项，构建了OI-Bench基准，系统评估大语言模型对干预信号的易感性。实验显示主流模型存在显著漏洞且鲁棒性不一。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型评测大多关注于界面本身并未系统探讨模型在选择受社会暗示、指令、诱导等外部干预时的表现和脆弱性，因此需要专门的基准和方法分析模型的鲁棒性。

Method: 作者提出option injection方法，在标准多选题中增加带有误导性指令的选项。构建了包含3000道题、涵盖知识/推理/常识任务的OI-Bench基准，并制作16种指令类型用于干预。随后对12个主流LLM进行系统评测，分析攻击成功率和行为特征，并探讨推理时和训练后的防护策略。

Result: 实验发现不同模型面对定向干预的易感性存在显著差异，普遍存在漏洞。部分防护方法成效有限，鲁棒性表现高度异质。

Conclusion: 论文系统展现了当前LLM对基于选项的干预极易受影响，提出的OI-Bench可为未来评测模型鲁棒性提供重要参考，为模型安全性提升指明方向。

Abstract: Benchmarking large language models (LLMs) is critical for understanding their capabilities, limitations, and robustness. In addition to interface artifacts, prior studies have shown that LLM decisions can be influenced by directive signals such as social cues, framing, and instructions. In this work, we introduce option injection, a benchmarking approach that augments the multiple-choice question answering (MCQA) interface with an additional option containing a misleading directive, leveraging standardized choice structure and scalable evaluation. We construct OI-Bench, a benchmark of 3,000 questions spanning knowledge, reasoning, and commonsense tasks, with 16 directive types covering social compliance, bonus framing, threat framing, and instructional interference. This setting combines manipulation of the choice interface with directive-based interference, enabling systematic assessment of model susceptibility. We evaluate 12 LLMs to analyze attack success rates, behavioral responses, and further investigate mitigation strategies ranging from inference-time prompting to post-training alignment. Experimental results reveal substantial vulnerabilities and heterogeneous robustness across models. OI-Bench is expected to support more systematic evaluation of LLM robustness to directive interference within choice-based interfaces.

</details>


### [381] [Paid Voices vs. Public Feeds: Interpretable Cross-Platform Theme Modeling of Climate Discourse](https://arxiv.org/abs/2601.13317)
*Samantha Sudhoff,Pranav Perumal,Zhaoqing Wu,Tunazzina Islam*

Main category: cs.CL

TL;DR: 本研究比较了Meta上的付费广告与Bluesky上的公开气候讨论，开发了一套自动发现和解释主题的分析框架，揭示了平台机制如何影响气候叙事。


<details>
  <summary>Details</summary>
Motivation: 目前计算社会科学研究多将广告和社交媒体内容分别分析，难以区分机构宣传与公众表达。作者希望填补这一研究空白，比较不同平台激励结构下气候话语的差异。

Method: 提出了一种可解释的端到端主题发现与分配框架：先通过文本语义聚类，再用大语言模型生成易理解的主题标签，并与传统主题建模方法对比，通过人工和LLM自动评判主题质量，并在情感预测、主题检索等下游任务中验证语义一致性。

Result: 结果显示，平台激励结构显著影响气候叙事的主题结构、立场一致性和时间响应性；付费广告和公开讨论的主题分布应社会事件而变化。该框架可有效区分并比较不同交流环境下的气候话语特征。

Conclusion: 提出的方法不仅适合比较付费与公开话语环境，还能应用于更广泛的异质交流环境叙事分析，为理解和优化气候传播及其他议题传播策略提供新工具。

Abstract: Climate discourse online plays a crucial role in shaping public understanding of climate change and influencing political and policy outcomes. However, climate communication unfolds across structurally distinct platforms with fundamentally different incentive structures: paid advertising ecosystems incentivize targeted, strategic persuasion, while public social media platforms host largely organic, user-driven discourse. Existing computational studies typically analyze these environments in isolation, limiting our ability to distinguish institutional messaging from public expression. In this work, we present a comparative analysis of climate discourse across paid advertisements on Meta (previously known as Facebook) and public posts on Bluesky from July 2024 to September 2025. We introduce an interpretable, end-to-end thematic discovery and assignment framework that clusters texts by semantic similarity and leverages large language models (LLMs) to generate concise, human-interpretable theme labels. We evaluate the quality of the induced themes against traditional topic modeling baselines using both human judgments and an LLM-based evaluator, and further validate their semantic coherence through downstream stance prediction and theme-guided retrieval tasks. Applying the resulting themes, we characterize systematic differences between paid climate messaging and public climate discourse and examine how thematic prevalence shifts around major political events. Our findings show that platform-level incentives are reflected in the thematic structure, stance alignment, and temporal responsiveness of climate narratives. While our empirical analysis focuses on climate communication, the proposed framework is designed to support comparative narrative analysis across heterogeneous communication environments.

</details>


### [382] [Arab Voices: Mapping Standard and Dialectal Arabic Speech Technology](https://arxiv.org/abs/2601.13319)
*Peter Sullivan,AbdelRahim Elmadany,Alcides Alcoba Inciarte,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 本论文分析了现有方言阿拉伯语（DA）语音数据集的异质性，并提出了一个统一标准化的DA ASR评测框架Arab Voices。


<details>
  <summary>Details</summary>
Motivation: 现有阿拉伯语方言语音数据在领域覆盖、方言标签和录制条件等方面存在巨大差异，影响跨数据集的比较和模型评测。为了推动标准化和可复现的研究，需要全面刻画和统一描述这些数据。

Method: 作者对常用DA语音数据集的语音方言性和音频质量等特征进行了计算分析，揭示标签和声学条件的异质性，并构建了新的Arab Voices框架，将31个数据集（14种方言）的数据和元信息标准化整合，并提供基准评测系统。

Result: 分析显示不同数据集在声学条件、方言信号强度和标签一致性上存在显著差异。Arab Voices统一了多数据集的接入，并为主流ASR系统建立了基准。

Conclusion: 该工作强调了阿拉伯语方言语音评测标准化的重要性，Arab Voices平台为领域研究和可复现实验提供了系统化的工具和数据基础，推动DA ASR领域发展。

Abstract: Dialectal Arabic (DA) speech data vary widely in domain coverage, dialect labeling practices, and recording conditions, complicating cross-dataset comparison and model evaluation. To characterize this landscape, we conduct a computational analysis of linguistic ``dialectness'' alongside objective proxies of audio quality on the training splits of widely used DA corpora. We find substantial heterogeneity both in acoustic conditions and in the strength and consistency of dialectal signals across datasets, underscoring the need for standardized characterization beyond coarse labels. To reduce fragmentation and support reproducible evaluation, we introduce Arab Voices, a standardized framework for DA ASR. Arab Voices provides unified access to 31 datasets spanning 14 dialects, with harmonized metadata and evaluation utilities. We further benchmark a range of recent ASR systems, establishing strong baselines for modern DA ASR.

</details>


### [383] [Reducing Tokenization Premiums for Low-Resource Languages](https://arxiv.org/abs/2601.13328)
*Geoffrey Churchill,Steven Skiena*

Main category: cs.CL

TL;DR: 对现代语言模型而言，低资源语言在分词时需要比英语多得多的token，导致API和能耗成本升高、可用上下文窗口变小。论文分析了十种主流模型的分词器，并提出了一种减少分词成本的新方法。


<details>
  <summary>Details</summary>
Motivation: 发现低资源语言在现代语言模型中的分词成本高，影响了模型的经济性和上下文能力，因此希望分析原因并提出改进办法。

Method: 分析十种主流语言模型的分词器设计和语言分词成本；提出在现有模型分词表中增加新token，把多token的字符合并为单token的机制，并在12种低资源语言中进行实验。

Result: 应用新机制后，将压缩前后的输入通过Llama 3.2 1B模型，得到的最后隐层状态非常接近，说明方法有效。

Conclusion: 通过对分词器优化，可以显著降低低资源语言的分词成本，同时保持模型表达能力，有助于提高这些语言在现有大模型中的效率及可用性。

Abstract: Relative to English, low-resource languages suffer from substantial tokenization premiums in modern LMs, meaning that it generally requires several times as many tokens to encode a sentence in a low-resource language than to encode the analogous sentence in English. This tokenization premium results in increased API and energy costs and reduced effective context windows for these languages. In this paper we analyze the tokenizers of ten popular LMs to better understand their designs and per-language tokenization premiums. We also propose a mechanism to reduce tokenization premiums in pre-trained models, by post-hoc additions to the token vocabulary that coalesce multi-token characters into single tokens. We apply this methodology to 12 low-resource languages, demonstrating that the original and compressed inputs often have similar last hidden states when run through the Llama 3.2 1B model.

</details>


### [384] [RegCheck: A tool for automating comparisons between study registrations and papers](https://arxiv.org/abs/2601.13330)
*Jamie Cummins,Beth Clarke,Ian Hussey,Malte Elson*

Main category: cs.CL

TL;DR: 本文介绍了一款名为RegCheck的工具，利用大语言模型（LLM）辅助，自动化对比科研计划注册内容与实际发表论文的一致性，以提升科研的透明度和严谨性。


<details>
  <summary>Details</summary>
Motivation: 当前科学研究中，虽然研究注册有助于提高透明度和严谨性，但注册内容很少被认真核查，这使注册的实际效果大打折扣。手动核查工作量大且需要跨学科知识，因此急需一种高效、可推广的自动化核查方法。

Method: 作者提出并实现了RegCheck系统，该系统基于大语言模型，自动辅助核查注册与论文的一致性。其流程包含由用户指定对比特征，自动提取并展示特征相关内容，由用户进行最终判断。不仅如此，RegCheck可生成带唯一ID的报告，便于结果分享和复核。

Result: RegCheck实现了跨学科、跨注册/发表格式的注册与论文对比辅助，并能输出结构化、可追溯的核查报告。借助实际案例，作者展示了RegCheck的工作流程和报告示例，证明其具备可行性和通用性。

Conclusion: RegCheck为提升科学研究的可重复性和注册透明度提供了有力工具。其半自动、可扩展的设计理念适合不同学科使用，有助于形成更加标准化和高效的科研注册审核流程，促进科学公信力。

Abstract: Across the social and medical sciences, researchers recognize that specifying planned research activities (i.e., 'registration') prior to the commencement of research has benefits for both the transparency and rigour of science. Despite this, evidence suggests that study registrations frequently go unexamined, minimizing their effectiveness. In a way this is no surprise: manually checking registrations against papers is labour- and time-intensive, requiring careful reading across formats and expertise across domains. The advent of AI unlocks new possibilities in facilitating this activity. We present RegCheck, a modular LLM-assisted tool designed to help researchers, reviewers, and editors from across scientific disciplines compare study registrations with their corresponding papers. Importantly, RegCheck keeps human expertise and judgement in the loop by (i) ensuring that users are the ones who determine which features should be compared, and (ii) presenting the most relevant text associated with each feature to the user, facilitating (rather than replacing) human discrepancy judgements. RegCheck also generates shareable reports with unique RegCheck IDs, enabling them to be easily shared and verified by other users. RegCheck is designed to be adaptable across scientific domains, as well as registration and publication formats. In this paper we provide an overview of the motivation, workflow, and design principles of RegCheck, and we discuss its potential as an extensible infrastructure for reproducible science with an example use case.

</details>


### [385] [AfroScope: A Framework for Studying the Linguistic Landscape of Africa](https://arxiv.org/abs/2601.13346)
*Sang Yun Kwon,AbdelRahim Elmadany,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 本文提出了AfroScope，一个针对非洲语言的统一语言识别（LID）框架，包括覆盖713种非洲语言的数据集和强大的LID模型，并引入分层分类和专门嵌入技术以提升识别相似语言的能力。


<details>
  <summary>Details</summary>
Motivation: 非洲语言种类繁多，现有LID工作支持的语种有限，且难以精细区分密切相关的语言变体，限制了下游NLP应用的表现。为改善这些不足，作者希望通过扩展覆盖范围和提升细粒度区分能力，推动非洲语言技术的发展。

Method: 1）构建涵盖713种非洲语言的AfroScope-Data数据集；2）提供多种覆盖广泛的AfroScope-Models LID模型；3）提出分层分类方法，并利用Mirror-Serengeti嵌入模型，专门处理29种难以区分或地理上接近的语言。

Result: 在最难区分的语言子集上，分层+嵌入的方法将Macro F1提高了4.55个百分点。还分析了跨语言迁移和领域影响，并为构建健壮的非洲LID系统提供了建议。

Conclusion: AfroScope可作为基础工具促进非洲数字文本中大规模语言分布测量，其数据集和模型已公开，有望提升非洲LID能力和相关NLP应用的可靠性。

Abstract: Language Identification (LID) is the task of determining the language of a given text and is a fundamental preprocessing step that affects the reliability of downstream NLP applications. While recent work has expanded LID coverage for African languages, existing approaches remain limited in (i) the number of supported languages and (ii) their ability to make fine-grained distinctions among closely related varieties. We introduce AfroScope, a unified framework for African LID that includes AfroScope-Data, a dataset covering 713 African languages, and AfroScope-Models, a suite of strong LID models with broad language coverage. To better distinguish highly confusable languages, we propose a hierarchical classification approach that leverages Mirror-Serengeti, a specialized embedding model targeting 29 closely related or geographically proximate languages. This approach improves macro F1 by 4.55 on this confusable subset compared to our best base model. Finally, we analyze cross linguistic transfer and domain effects, offering guidance for building robust African LID systems. We position African LID as an enabling technology for large scale measurement of Africas linguistic landscape in digital text and release AfroScope-Data and AfroScope-Models publicly.

</details>


### [386] [LLM-as-RNN: A Recurrent Language Model for Memory Updates and Sequence Prediction](https://arxiv.org/abs/2601.13352)
*Yuxing Lu,J. Ben Tamo,Weichen Zhao,Nan Sun,Yishan Zhong,Wenqi Shi,Jinzhuo Wang,May D. Wang*

Main category: cs.CL

TL;DR: 本文提出了一种只需推理、无需训练的LLM新框架LLM-as-RNN，通过在推理过程中用自然语言记忆实现“隐状态”更新，使模型可以像RNN一样具备循环和在线学习能力，有效纠正生成过程中的历史错误。实验表明，该方法在多个领域任务上优于主流基线，并能产生可解释的学习轨迹。


<details>
  <summary>Details</summary>
Motivation: 主流大语言模型（LLM）在生成过程中，历史上下文不可变，执行机制导致模型一旦出错，之后难以自我修正且缺乏可更新的外部记忆机制，无法像RNN那样进行在线学习和纠错。为提升预测准确性和适应性，急需让LLM具备外显、可互动的记忆与状态更新能力。

Method: LLM-as-RNN方法将LLM的“隐状态”通过结构化的system prompt以自然语言摘要形式表达，每步生成后，用反馈驱动策略对该摘要进行动态文本改写，相当于每步更新“隐状态”，不需微调模型参数。方法在固定token预算下，结合记忆摘要和当前输入，高效支持任务相关信息持久保留和历史错误纠正。

Result: 在医疗、气象、金融三类任务和多个主流LLM（包括Llama、Gemma和GPT系列）上评测，LLM-as-RNN对比zero-shot、全历史输入、MemPrompt等方法，平均提升预测准确率6.5%，并且系统生成的人类可读的“学习轨迹”明显优于标准上下文方法。

Conclusion: 通过把自然语言记忆引入推理环节，LLM-as-RNN让冷冻的LLM模型具备RNN式的循环推理与在线纠错能力，实现了无需微调参数的自适应预测。这为解释性与实际表现都带来改进。

Abstract: Large language models are strong sequence predictors, yet standard inference relies on immutable context histories. After making an error at generation step t, the model lacks an updatable memory mechanism that improves predictions for step t+1. We propose LLM-as-RNN, an inference-only framework that turns a frozen LLM into a recurrent predictor by representing its hidden state as natural-language memory. This state, implemented as a structured system-prompt summary, is updated at each timestep via feedback-driven text rewrites, enabling learning without parameter updates. Under a fixed token budget, LLM-as-RNN corrects errors and retains task-relevant patterns, effectively performing online learning through language. We evaluate the method on three sequential benchmarks in healthcare, meteorology, and finance across Llama, Gemma, and GPT model families. LLM-as-RNN significantly outperforms zero-shot, full-history, and MemPrompt baselines, improving predictive accuracy by 6.5% on average, while producing interpretable, human-readable learning traces absent in standard context accumulation.

</details>


### [387] [Sockpuppetting: Jailbreaking LLMs Without Optimization Through Output Prefix Injection](https://arxiv.org/abs/2601.13359)
*Asen Dotsinski,Panagiotis Eustratiadis*

Main category: cs.CL

TL;DR: 本文提出了一种名为“sockpuppetting”的低成本方法，仅通过在开放权重大型语言模型（LLM）的输出前插入接受句（例如“Sure, here is how to...”），即可实现有效越狱，成功率大幅超过现有自动化越狱方法，展示了当前模型易受该类攻击的风险。


<details>
  <summary>Details</summary>
Motivation: 目前自动化越狱方法如GCG尽管有效，却需要大量计算资源与专业知识，门槛较高。随着开放权重LLM能力提升，如何防御和理解易被利用的新型攻击变得关键，亟需探索简单易用、低成本却高效的越狱手段及其防御措施。

Method: 提出'sockpuppetting'方法，即只需在模型输出前预置一条接受内容的序列，引导模型无障碍地完成违规或敏感内容生成，无需优化或复杂操作。进一步，尝试在助手消息区内优化对抗后缀，而不修改用户提示，使攻击更通用。

Result: 在Qwen3-8B模型上，sockpuppetting比GCG提升了80%的攻击成功率（ASR）；在Llama-3.1-8B上，通过助手消息区优化，对GCG有64%的ASR提升，展示了强大的攻击能力和普适性。

Conclusion: sockpuppetting方法极为简单，极具攻击有效性，几乎不需要技术门槛，对开放权重LLM构成实质性威胁。论文强调必须开发针对输出前缀注入攻击的防御机制，以提升开放权重模型的安全性。

Abstract: As open-weight large language models (LLMs) increase in capabilities, safeguarding them against malicious prompts and understanding possible attack vectors becomes ever more important. While automated jailbreaking methods like GCG [Zou et al., 2023] remain effective, they often require substantial computational resources and specific expertise. We introduce "sockpuppetting'', a simple method for jailbreaking open-weight LLMs by inserting an acceptance sequence (e.g., "Sure, here is how to...'') at the start of a model's output and allowing it to complete the response. Requiring only a single line of code and no optimization, sockpuppetting achieves up to 80% higher attack success rate (ASR) than GCG on Qwen3-8B in per-prompt comparisons. We also explore a hybrid approach that optimizes the adversarial suffix within the assistant message block rather than the user prompt, increasing ASR by 64% over GCG on Llama-3.1-8B in a prompt-agnostic setting. The results establish sockpuppetting as an effective low-cost attack accessible to unsophisticated adversaries, highlighting the need for defences against output-prefix injection in open-weight models.

</details>


### [388] [Recurrent Confidence Chain: Temporal-Aware Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2601.13368)
*Zhenjiang Mao,Anirudhh Venkat*

Main category: cs.CL

TL;DR: 本论文提出了一种新的方法，通过引入跨步骤注意力和隐藏置信机制，更准确地评估大语言模型推理答案的不确定性，在数学和因果推理基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着链式思维等推理模块被应用于大语言模型，模型在多种任务中表现出色，但如何准确评估答案的不确定性成为新挑战，尤其是在长推理序列中容易导致置信度虚高，进而影响用户信任和实际应用安全性。

Method: 作者提出利用跨推理步骤的注意力（inter-step attention）分析不同推理步骤间的语义相关性，同时引入隐藏置信机制，将历史置信度与当前步骤置信度结合，提升置信度估算的准确性。该方法针对长推理序列设计，兼顾上下文和历史置信信息。

Result: 在GAOKAO数学基准和CLadder因果推理数据集上，使用主流开源大模型测试，所提方法在预测质量与置信度校准（Negative Log-Likelihood及Expected Calibration Error等指标）上均优于现有最优方法。

Conclusion: 新方法显著提升了大语言模型推理输出的置信度评估准确性，降低了误导性高置信错误，有助于提升模型部署的安全性和可信度。

Abstract: As reasoning modules, such as the chain-of-thought mechanism, are applied to large language models, they achieve strong performance on various tasks such as answering common-sense questions and solving math problems. The main challenge now is to assess the uncertainty of answers, which can help prevent misleading or serious hallucinations for users. Although current methods analyze long reasoning sequences by filtering unrelated tokens and examining potential connections between nearby tokens or sentences, the temporal spread of confidence is often overlooked. This oversight can lead to inflated overall confidence, even when earlier steps exhibit very low confidence. To address this issue, we propose a novel method that incorporates inter-step attention to analyze semantic correlations across steps. For handling long-horizon responses, we introduce a hidden confidence mechanism to retain historical confidence information, which is then combined with stepwise confidence to produce a more accurate overall estimate. We evaluate our method on the GAOKAO math benchmark and the CLadder causal reasoning dataset using mainstream open-source large language models. Our approach is shown to outperform state-of-the-art methods by achieving a superior balance between predictive quality and calibration, demonstrated by strong performance on both Negative Log-Likelihood and Expected Calibration Error.

</details>


### [389] [Confidence over Time: Confidence Calibration with Temporal Logic for Large Language Model Reasoning](https://arxiv.org/abs/2601.13387)
*Zhenjiang Mao,Anirudhh Venkat,Artem Bisliouk,Akshat Kothiyal,Sindhura Kumbakonam Subramanian,Saithej Singhu,Ivan Ruchkin*

Main category: cs.CL

TL;DR: 本文提出了一种基于信号时序逻辑（STL）的逐步置信度估计方法，有效提升了大模型推理过程中的置信度判别能力。


<details>
  <summary>Details</summary>
Motivation: 现有的置信度估计方法过于粗糙，只用单一分数评价完整推理过程，忽略了推理过程中置信度的动态变化，导致方法易受响应长度、啰嗦程度等表面因素影响，难以区分真实推理和自信但错误的回答。

Method: 通过信号时序逻辑（STL）对推理过程中的置信度信号进行逐步建模，并采用判别性STL挖掘技术，提炼能区分正确与错误推理的时序逻辑公式。进一步，利用超网络为STL模块赋予参数，实现细粒度的置信度估计。

Result: STL模式在不同任务间具备良好的泛化性，对具体问题的参数有敏感性。实验证明，所提置信度得分在多个复杂推理任务上，比主流方法更具校准性。

Conclusion: 本文创新性地利用信号时序逻辑建模推理置信度，显著提升了大语言模型在多步推理任务中的置信度估计准确性。

Abstract: Large Language Models (LLMs) increasingly rely on long-form, multi-step reasoning to solve complex tasks such as mathematical problem solving and scientific question answering. Despite strong performance, existing confidence estimation methods typically reduce an entire reasoning process to a single scalar score, ignoring how confidence evolves throughout the generation. As a result, these methods are often sensitive to superficial factors such as response length or verbosity, and struggle to distinguish correct reasoning from confidently stated errors. We propose to characterize the stepwise confidence signal using Signal Temporal Logic (STL). Using a discriminative STL mining procedure, we discover temporal formulas that distinguish confidence signals of correct and incorrect responses. Our analysis found that the STL patterns generalize across tasks, and numeric parameters exhibit sensitivity to individual questions. Based on these insights, we develop a confidence estimation approach that informs STL blocks with parameter hypernetworks. Experiments on multiple reasoning tasks show our confidence scores are more calibrated than the baselines.

</details>


### [390] [Structured Insight from Unstructured Data: Large Language Models for SDOH-Driven Diabetes Risk Prediction](https://arxiv.org/abs/2601.13388)
*Sasha Ronaghi,Prerit Choudhary,David H Rehkopf,Bryant Lin*

Main category: cs.CL

TL;DR: 本研究利用大型语言模型（LLM）从2型糖尿病患者的非结构化生活故事中提取结构化健康社会决定因素（SDOH）信息，并用于糖尿病控制风险预测。LLM能将这些信息转化为对临床有意义的总结，并提升风险建模的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前电子健康记录和风险预测模型常缺乏个人层面的SDOH数据，现有结构化筛查工具也难以捕捉患者复杂的社会经历，导致糖尿病管理信息不全面。因此，探索如何有效利用患者的非结构化生活叙述来补充传统数据十分必要。

Method: 研究收集了65名65岁及以上T2D患者的访谈叙述，聚焦其社会背景和糖尿病管理。采用检索增强生成的大型语言模型，对文本进行分析，总结出结构化的SDOH评级和临床可用的定性信息，然后将这些评级单独或与传统实验室数据结合，输入多种机器学习模型（包括岭回归、Lasso、随机森林、XGBoost）进行糖尿病控制风险预测。还对LLM根据访谈文本隐去A1C值预测糖尿病控制水平的能力进行评估。

Result: 结构化SDOH评级被证明可用于传统风险预测工作流。LLM能在隐去A1C值情况下，仅凭访谈文本以60%准确率预测患者的糖尿病控制水平。

Conclusion: LLM能够把非结构化的SDOH信息转化为结构化、可用的数据，大大增强临床风险预测模型和辅助决策能力，具有良好的可扩展性与应用前景。

Abstract: Social determinants of health (SDOH) play a critical role in Type 2 Diabetes (T2D) management but are often absent from electronic health records and risk prediction models. Most individual-level SDOH data is collected through structured screening tools, which lack the flexibility to capture the complexity of patient experiences and unique needs of a clinic's population. This study explores the use of large language models (LLMs) to extract structured SDOH information from unstructured patient life stories and evaluate the predictive value of both the extracted features and the narratives themselves for assessing diabetes control. We collected unstructured interviews from 65 T2D patients aged 65 and older, focused on their lived experiences, social context, and diabetes management. These narratives were analyzed using LLMs with retrieval-augmented generation to produce concise, actionable qualitative summaries for clinical interpretation and structured quantitative SDOH ratings for risk prediction modeling. The structured SDOH ratings were used independently and in combination with traditional laboratory biomarkers as inputs to linear and tree-based machine learning models (Ridge, Lasso, Random Forest, and XGBoost) to demonstrate how unstructured narrative data can be applied in conventional risk prediction workflows. Finally, we evaluated several LLMs on their ability to predict a patient's level of diabetes control (low, medium, high) directly from interview text with A1C values redacted. LLMs achieved 60% accuracy in predicting diabetes control levels from interview text. This work demonstrates how LLMs can translate unstructured SDOH-related data into structured insights, offering a scalable approach to augment clinical risk models and decision-making.

</details>


### [391] [Beyond Memorization: Testing LLM Reasoning on Unseen Theory of Computation Tasks](https://arxiv.org/abs/2601.13392)
*Shlok Shelat,Jay Raval,Souvik Roy,Manas Gaur*

Main category: cs.CL

TL;DR: 本文针对大语言模型(LLM)在形式语言任务中的推理能力提出了新的评测基准，聚焦于确定性有限自动机(DFA)的构造任务，分析了模型在不同类型问题上的表现及失误原因。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在各种形式化任务中取得了高分，但尚不清楚其成功源于真正的符号推理能力还是对已经见过题型的模式识别。为此，需要一项能够区分二者的新基准。

Method: 作者设计了DFA构造基准，包括事实性知识问答、公开来源的已见建设任务、以及两类从未见过的问题（一为手动设计带多重约束的实例，二为用Arden定理系统生成的问题）。此外，评估了不同提示策略（直接、链式思考、树状思考）及三阶段“提示-纠错”流程对结果的影响。

Result: LLM在事实性问题和已见任务上表现良好（完美或84%-90%准确率），但在未见任务中的表现出现大幅下降（准确率下降30%-64%）。错误主要包括对语言约束的误解、Kleene星语义处理错误、全局一致性缺陷等。三阶段提示过程只能修正表层错误，对整体逻辑矛盾无效；各种提示策略下，结构性错误依然明显。

Conclusion: 当前LLM虽能生成结构符合要求的DFA，但无法保证语义推理的严谨性，在形式化推理能力上存在本质缺陷。

Abstract: Large language models (LLMs) have demonstrated strong performance on formal language tasks, yet whether this reflects genuine symbolic reasoning or pattern matching on familiar constructions remains unclear. We introduce a benchmark for deterministic finite automata (DFA) construction from regular languages, comprising factual knowledge questions, seen construction problems from public sources, and two types of unseen problems: hand-crafted instances with multiple interacting constraints and systematically generated problems via Arden's theorem. Models achieve perfect accuracy on factual questions and 84-90% on seen tasks. However, accuracy drops sharply on unseen problems (by 30-64%), with failures stemming from systematic misinterpretation of language constraints, incorrect handling of Kleene-star semantics, and a failure to preserve global consistency. We evaluate a three-stage hint protocol that enables correction of shallow errors but does not reliably resolve globally inconsistent or structurally flawed automata. Our analysis across multiple prompting strategies (direct, Chain-of-Thought, Tree-of-Thought) reveals that errors persist regardless of prompting approach, exposing a fundamental gap between LLMs' ability to generate syntactically plausible DFAs and their capacity for semantically correct formal reasoning.

</details>


### [392] [Trust Me, I'm an Expert: Decoding and Steering Authority Bias in Large Language Models](https://arxiv.org/abs/2601.13433)
*Priyanka Mary Mammen,Emil Joswin,Shankar Venkitachalam*

Main category: cs.CL

TL;DR: 该论文发现大语言模型在面对不同权威级别的提示时会产生系统性“权威偏见”，权威度越高对模型误导性越强，且模型还会更自信地输出错误答案。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明提示、提示人等信息会影响语言模型的推理表现，但“权威来源可信度”对模型的具体影响尚未被充分探讨。

Method: 作者设计了4个涵盖数学、法律和医学的推理数据集，模拟了4种不同领域的4个权威级别角色，对11个语言模型进行了测试，评估模型在不同权威级别提示下的表现。

Result: 当给出错误或误导性提示时，权威级别越高，模型表现出的错误就越多，同时在错误答案上的信心也增强了。这种“权威偏见”已经被模型结构机制性编码。

Conclusion: 模型对权威来源的误导性提示极易产生依赖和偏见，导致表现下降，但通过特定干预可引导模型减弱这种偏见，即便提示者为专家也能提升抗干扰能力。

Abstract: Prior research demonstrates that performance of language models on reasoning tasks can be influenced by suggestions, hints and endorsements. However, the influence of endorsement source credibility remains underexplored. We investigate whether language models exhibit systematic bias based on the perceived expertise of the provider of the endorsement. Across 4 datasets spanning mathematical, legal, and medical reasoning, we evaluate 11 models using personas representing four expertise levels per domain. Our results reveal that models are increasingly susceptible to incorrect/misleading endorsements as source expertise increases, with higher-authority sources inducing not only accuracy degradation but also increased confidence in wrong answers. We also show that this authority bias is mechanistically encoded within the model and a model can be steered away from the bias, thereby improving its performance even when an expert gives a misleading endorsement.

</details>


### [393] [MOSLD-Bench: Multilingual Open-Set Learning and Discovery Benchmark for Text Categorization](https://arxiv.org/abs/2601.13437)
*Adriana-Valentina Costache,Daria-Nicoleta Dragomir,Silviu-Florin Gheorghe,Eduard Poesina,Paul Irofti,Radu Tudor Ionescu*

Main category: cs.CL

TL;DR: 该论文提出了首个多语言开放集学习与发现（MOSLD）基准，用于文本分类任务，并提出了一种多阶段的新OSLD方法。


<details>
  <summary>Details</summary>
Motivation: 在文本分类领域，虽然零样本学习已有广泛研究，但开放集学习和新类别发现仍属新兴领域，尤其在多语言情境下缺乏统一基准与系统方法。该研究旨在推动多语言开放集文本领域的算法发展和应用。

Method: 作者通过重组现有数据集和收集来自新闻领域的新数据，构建了覆盖12种语言、96万样本的MOSLD数据集。并提出了一个多阶段的OSLD框架，实现对新类别的持续发现与学习。

Result: 作者测试和评估了多个现有的语言模型及其自有模型，在提出的数据集上获得了基准结果，为未来研究提供了参考。

Conclusion: 该工作首次为多语言文本开放集学习与发现提供了标准数据集和系统评测基线，将促进该领域研究发展，并为后续工作提供基础资源。

Abstract: Open-set learning and discovery (OSLD) is a challenging machine learning task in which samples from new (unknown) classes can appear at test time. It can be seen as a generalization of zero-shot learning, where the new classes are not known a priori, hence involving the active discovery of new classes. While zero-shot learning has been extensively studied in text classification, especially with the emergence of pre-trained language models, open-set learning and discovery is a comparatively new setup for the text domain. To this end, we introduce the first multilingual open-set learning and discovery (MOSLD) benchmark for text categorization by topic, comprising 960K data samples across 12 languages. To construct the benchmark, we (i) rearrange existing datasets and (ii) collect new data samples from the news domain. Moreover, we propose a novel framework for the OSLD task, which integrates multiple stages to continuously discover and learn new classes. We evaluate several language models, including our own, to obtain results that can be used as reference for future work. We release our benchmark at https://github.com/Adriana19Valentina/MOSLD-Bench.

</details>


### [394] [PhysicsSolutionAgent: Towards Multimodal Explanations for Numerical Physics Problem Solving](https://arxiv.org/abs/2601.13453)
*Aditya Thole,Anmol Agrawal,Arnav Ramamoorthy,Dhruv Kumar*

Main category: cs.CL

TL;DR: 论文提出了一种能够生成物理问题视觉解释视频的自主智能体，并系统性评估了其效果及局限。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在文字形式解答物理问题表现良好，但生成高质量、时长较长的视觉解释能力尚未充分研究，而视觉推理对物理理解至关重要。

Method: 提出了PhysicsSolutionAgent（PSA）这一自主代理，其结合Manim动画自动生成最长六分钟的物理问题视频解释。为评估视频质量，设计了自动化评测流程，涵盖15项定量指标，并整合视觉-语言模型（VLM）的反馈以优化视频品质。

Result: 对32个涵盖数值与理论物理问题的视频进行评估，PSA利用GPT-5-mini实现了100%的视频生成率，自动化平均评分为3.8/5。分析显示视频质量与问题难度、问题类型有关，但发现存在视觉布局不一致、反馈过程中视觉内容解释错误等多种问题。

Conclusion: PSA在自动生成物理解答视频取得了一定成果，但依然面临Manim代码生成可靠性、多模态推理与评估等挑战。未来需进一步提升视觉理解与评估框架，以推动多模态教育系统的发展。

Abstract: Explaining numerical physics problems often requires more than text-based solutions; clear visual reasoning can substantially improve conceptual understanding. While large language models (LLMs) demonstrate strong performance on many physics questions in textual form, their ability to generate long, high-quality visual explanations remains insufficiently explored. In this work, we introduce PhysicsSolutionAgent (PSA), an autonomous agent that generates physics-problem explanation videos of up to six minutes using Manim animations. To evaluate the generated videos, we design an assessment pipeline that performs automated checks across 15 quantitative parameters and incorporates feedback from a vision-language model (VLM) to iteratively improve video quality. We evaluate PSA on 32 videos spanning numerical and theoretical physics problems. Our results reveal systematic differences in video quality depending on problem difficulty and whether the task is numerical or theoretical. Using GPT-5-mini, PSA achieves a 100% video-completion rate with an average automated score of 3.8/5. However, qualitative analysis and human inspection uncover both minor and major issues, including visual layout inconsistencies and errors in how visual content is interpreted during feedback. These findings expose key limitations in reliable Manim code generation and highlight broader challenges in multimodal reasoning and evaluation for visual explanations of numerical physics problems. Our work underscores the need for improved visual understanding, verification, and evaluation frameworks in future multimodal educational systems

</details>


### [395] [Anonpsy: A Graph-Based Framework for Structure-Preserving De-identification of Psychiatric Narratives](https://arxiv.org/abs/2601.13503)
*Kyung Ho Lim,Byung-Hoon Kim*

Main category: cs.CL

TL;DR: 提出了一种名为Anonpsy的新方法，通过图结构引导的语义重写方式，对精神科叙事文本实现更灵活且有效的去识别化。


<details>
  <summary>Details</summary>
Motivation: 现有的去标识化方法多基于文本直接操作，缺乏对语义元素（如患者身份相关的生活事件）的精细控制，容易无法在保护隐私和保留临床关键信息之间取得平衡。

Method: Anonpsy方法首先将叙事文本转换为包含临床实体、时间锚点及类型化关系的语义图，然后在图结构上进行受限扰动，修改敏感背景同时保留必要的临床意义，最后通过以图为条件的大语言模型(LLM)生成新文本。

Result: 在90份精神科病例叙事的实验中，Anonpsy保持了诊断一致性的同时，专家、语义分析及GPT-5基础上的重识别风险均显著降低。与强大的LLM重写基线相比，Anonpsy在语义相似度和可识别性方面都有进一步优势。

Conclusion: 通过明确结构表示和受限生成的新策略，能够更加有效地实现精神科案例文本的去标识化，兼顾隐私保护与临床实用性。

Abstract: Psychiatric narratives encode patient identity not only through explicit identifiers but also through idiosyncratic life events embedded in their clinical structure. Existing de-identification approaches, including PHI masking and LLM-based synthetic rewriting, operate at the text level and offer limited control over which semantic elements are preserved or altered. We introduce Anonpsy, a de-identification framework that reformulates the task as graph-guided semantic rewriting. Anonpsy (1) converts each narrative into a semantic graph encoding clinical entities, temporal anchors, and typed relations; (2) applies graph-constrained perturbations that modify identifying context while preserving clinically essential structure; and (3) regenerates text via graph-conditioned LLM generation. Evaluated on 90 clinician-authored psychiatric case narratives, Anonpsy preserves diagnostic fidelity while achieving consistently low re-identification risk under expert, semantic, and GPT-5-based evaluations. Compared with a strong LLM-only rewriting baseline, Anonpsy yields substantially lower semantic similarity and identifiability. These results demonstrate that explicit structural representations combined with constrained generation provide an effective approach to de-identification for psychiatric narratives.

</details>


### [396] [When Wording Steers the Evaluation: Framing Bias in LLM judges](https://arxiv.org/abs/2601.13537)
*Yerin Hwang,Dongryeol Lee,Taegwan Kang,Minwoo Lee,Kyomin Jung*

Main category: cs.CL

TL;DR: 本论文研究了在高风险评测任务中，大型语言模型（LLM）对不同提问表述（正向/负向）的敏感性及其带来的输出差异，发现LLM的评价结果存在显著表述偏差。


<details>
  <summary>Details</summary>
Motivation: LLM在被用于自动评审和判断时常被认为具有稳定和公正的能力，但在各类任务中的实际表现会因提示词表达细微差别而显著不同。目前有关这种表述偏差对LLM评测影响的系统性研究较少，因此有必要探讨与心理学框架效应相关的行为在LLM中的表现。

Method: 作者设计了对称的正向与负向提示词（predicate-positive 和 predicate-negative），在四个高风险评测任务中，系统性地测试了14个LLM模型“裁判”对这些不同措辞的反应差异。

Result: 研究发现，不同的表述方式会导致LLM输出结果出现显著差别，不同模型家族对框架效应的敏感程度也不同，表现为倾向一致同意或拒绝。

Conclusion: 本研究表明，提示词表述偏差是当前LLM评测系统的结构性问题，未来需制定能够意识并修正框架效应的评测流程，以提高自动评审系统的公正性和稳定性。

Abstract: Large language models (LLMs) are known to produce varying responses depending on prompt phrasing, indicating that subtle guidance in phrasing can steer their answers. However, the impact of this framing bias on LLM-based evaluation, where models are expected to make stable and impartial judgments, remains largely underexplored. Drawing inspiration from the framing effect in psychology, we systematically investigate how deliberate prompt framing skews model judgments across four high-stakes evaluation tasks. We design symmetric prompts using predicate-positive and predicate-negative constructions and demonstrate that such framing induces significant discrepancies in model outputs. Across 14 LLM judges, we observe clear susceptibility to framing, with model families showing distinct tendencies toward agreement or rejection. These findings suggest that framing bias is a structural property of current LLM-based evaluation systems, underscoring the need for framing-aware protocols.

</details>


### [397] [HateXScore: A Metric Suite for Evaluating Reasoning Quality in Hate Speech Explanations](https://arxiv.org/abs/2601.13547)
*Yujia Hu,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: 本文提出了HateXScore，一个用于评估仇恨言论检测模型解释合理性的多维指标。该指标弥补了准确率和F1等传统评估无法揭示模型解释缺陷的不足。


<details>
  <summary>Details</summary>
Motivation: 当前仇恨言论检测主要关注文本是否被判定为有害，很少关注模型为何做出这种判断，模型解释性不足，难以用于透明和可信的内容审核。因此需要一个能系统评估解释合理性的度量方法。

Method: 提出HateXScore，包括四个维度：（1）结论表达的明确性；（2）所引用文本片段的信度与因果性；（3）被保护群体识别（且支持策略配置）；（4）各组件之间的逻辑一致性。该方法在六个具有代表性的仇恨言论数据集上进行验证。

Result: HateXScore能够作为辅助诊断工具，有效揭示传统指标无法发现的模型解释失败和标注不一致问题。结果还表明，人类评估与HateXScore高度一致，验证了其实用性和可靠性。

Conclusion: HateXScore有助于提升仇恨言论检测模型的可解释性和透明度，是完善内容审核机制和评价体系的重要补充。

Abstract: Hateful speech detection is a key component of content moderation, yet current evaluation frameworks rarely assess why a text is deemed hateful. We introduce \textsf{HateXScore}, a four-component metric suite designed to evaluate the reasoning quality of model explanations. It assesses (i) conclusion explicitness, (ii) faithfulness and causal grounding of quoted spans, (iii) protected group identification (policy-configurable), and (iv) logical consistency among these elements. Evaluated on six diverse hate speech datasets, \textsf{HateXScore} is intended as a diagnostic complement to reveal interpretability failures and annotation inconsistencies that are invisible to standard metrics like Accuracy or F1. Moreover, human evaluation shows strong agreement with \textsf{HateXScore}, validating it as a practical tool for trustworthy and transparent moderation.
  \textcolor{red}{Disclaimer: This paper contains sensitive content that may be disturbing to some readers.}

</details>


### [398] [Comparing Without Saying: A Dataset and Benchmark for Implicit Comparative Opinion Mining from Same-User Reviews](https://arxiv.org/abs/2601.13575)
*Thanh-Lam T. Nguyen,Ngoc-Quang Le,Quoc-Trung Phu,Thi-Phuong Le,Ngoc-Huyen Pham,Phuong-Nguyen Nguyen,Hoang-Quynh Le*

Main category: cs.CL

TL;DR: 本论文提出了一个名为SUDO的新数据集，专注于挖掘用户在不同评论中的隐式比较意见，并通过基线方法验证了这一任务的挑战性和研究价值。


<details>
  <summary>Details</summary>
Motivation: 现有的比较意见挖掘主要关注显式表达，但实际评论中显式比较较少，隐式比较则被忽略。因此，挖掘用户在多条评论中隐式表达的偏好十分重要且具有现实意义。

Method: 作者提出了SUDO数据集，包括4150组、共15191句的同一用户评论对，标注了细粒度的方面和整体偏好。作者利用传统机器学习方法和基于语言模型的深度学习方法对任务进行了基线评测。

Result: 实验表明，语言模型方法优于传统机器学习方法，但整体性能依然有限，显示出该任务本身的难度。

Conclusion: SUDO数据集为隐式比较意见挖掘提供了新的标准和挑战，有助于推动该领域的研究发展。

Abstract: Existing studies on comparative opinion mining have mainly focused on explicit comparative expressions, which are uncommon in real-world reviews. This leaves implicit comparisons - here users express preferences across separate reviews - largely underexplored. We introduce SUDO, a novel dataset for implicit comparative opinion mining from same-user reviews, allowing reliable inference of user preferences even without explicit comparative cues. SUDO comprises 4,150 annotated review pairs (15,191 sentences) with a bi-level structure capturing aspect-level mentions and review-level preferences. We benchmark this task using two baseline architectures: traditional machine learning- and language model-based baselines. Experimental results show that while the latter outperforms the former, overall performance remains moderate, revealing the inherent difficulty of the task and establishing SUDO as a challenging and valuable benchmark for future research.

</details>


### [399] [TREX: Tokenizer Regression for Optimal Data Mixture](https://arxiv.org/abs/2601.13588)
*Inho Won,Hangyeol Yoo,Minkyung Cho,Jungyeul Park,Hoyun Song,KyungTae Lim*

Main category: cs.CL

TL;DR: 该论文提出了一种名为TREX的回归框架，用于高效预测多语种大模型分词器训练的数据混合比例，从而提升分词器压缩性能，并减少试错和成本。


<details>
  <summary>Details</summary>
Motivation: 构建多语种大模型的有效分词器时，不同语种数据混合比例的选择至关重要，但现有做法依赖于经验法则或耗时的搜索，缺乏高效、科学的方法。

Method: TREX 框架通过先随机选择数据混合比例训练小规模代理分词器，收集其压缩表现，再利用回归模型学习混合比例与压缩性能的关系，进而在正式分词器大规模训练前预测最优的数据混合组合。

Result: 采用 TREX 预测的数据混合方案训练的分词器，比基于 LLaMA3 和均匀分布的数据混合方案在分布内外的压缩效率提升高达12%。

Conclusion: TREX 框架在保持效率的同时提升了多语种分词器的性能，具备很强的可扩展性和实用性，可有效缓解分词器设计中的成本与精度权衡。

Abstract: Building effective tokenizers for multilingual Large Language Models (LLMs) requires careful control over language-specific data mixtures. While a tokenizer's compression performance critically affects the efficiency of LLM training and inference, existing approaches rely on heuristics or costly large-scale searches to determine optimal language ratios. We introduce Tokenizer Regression for Optimal Data MiXture (TREX), a regression-based framework that efficiently predicts the optimal data mixture for tokenizer training. TREX trains small-scale proxy tokenizers on random mixtures, gathers their compression statistics, and learns to predict compression performance from data mixtures. This learned model enables scalable mixture search before large-scale tokenizer training, mitigating the accuracy-cost trade-off in multilingual tokenizer design. Tokenizers trained with TReX's predicted mixtures outperform mixtures based on LLaMA3 and uniform distributions by up to 12% in both inand out-of-distribution compression efficiency, demonstrating strong scalability, robustness, and practical effectiveness.

</details>


### [400] [Vulnerability of LLMs' Belief Systems? LLMs Belief Resistance Check Through Strategic Persuasive Conversation Interventions](https://arxiv.org/abs/2601.13590)
*Fan Huang,Haewoon Kwak,Jisun An*

Main category: cs.CL

TL;DR: 本文系统评估了大语言模型（LLM）在多回合对话中对说服的敏感性，发现模型对说服策略十分依赖，且常用增强鲁棒性方法效果有限。


<details>
  <summary>Details</summary>
Motivation: 近年来LLM广泛用于问答场景，但研究发现其容易被说服、接受反事实观点，亟需系统研究其易受影响的机制及防御方法。

Method: 作者采用SMCR（来源-信息-通道-接收者）通信框架，在五种主流LLM及三个领域中测试不同说服策略影响，并考察元认知激励（如自信度报告）和对抗性微调等防御措施。

Result: 小模型表现出极高顺从性，大部分观点改变在首次说服后立即发生。元认知激励反而加速观点变化，并未增强抗干扰。对抗式微调能显著提升GPT-4o-mini与Mistral~7B的鲁棒性，但Llama系列即便“靶向微调”后仍易受影响。

Conclusion: LLM对说服的抗性存在显著模型差异，当前提升鲁棒性方法存在局限，未来设计更可信赖LLM需针对易受影响环节持续优化。

Abstract: Large Language Models (LLMs) are increasingly employed in various question-answering tasks. However, recent studies showcase that LLMs are susceptible to persuasion and could adopt counterfactual beliefs. We present a systematic evaluation of LLM susceptibility to persuasion under the Source--Message--Channel--Receiver (SMCR) communication framework. Across five mainstream Large Language Models (LLMs) and three domains (factual knowledge, medical QA, and social bias), we analyze how different persuasive strategies influence belief stability over multiple interaction turns. We further examine whether meta-cognition prompting (i.e., eliciting self-reported confidence) affects resistance to persuasion. Results show that smaller models exhibit extreme compliance, with over 80% of belief changes occurring at the first persuasive turn (average end turn of 1.1--1.4). Contrary to expectations, meta-cognition prompting increases vulnerability by accelerating belief erosion rather than enhancing robustness. Finally, we evaluate adversarial fine-tuning as a defense. While GPT-4o-mini achieves near-complete robustness (98.6%) and Mistral~7B improves substantially (35.7% $\rightarrow$ 79.3%), Llama models remain highly susceptible (<14%) even when fine-tuned on their own failure cases. Together, these findings highlight substantial model-dependent limits of current robustness interventions and offer guidance for developing more trustworthy LLMs.

</details>


### [401] [CauScientist: Teaching LLMs to Respect Data for Causal Discovery](https://arxiv.org/abs/2601.13614)
*Bo Peng,Sirui Chen,Lei Xu,Chaochao Lu*

Main category: cs.CL

TL;DR: 本文提出了一种新的因果发现方法CauScientist，将大语言模型（LLM）与概率统计方法协同结合，有效解决了各自局限，提高了因果结构发现的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 因果发现对科学理解和决策非常重要，但现有方法存在不足：纯数据驱动方法受制于统计不可区分性和强建模假设，基于LLM的方法则易忽略统计证据或引入不可靠的先验知识，导致推断结果受误导。

Method: CauScientist采用协同框架，将LLM作为“假设生成者”，概率统计方法作为“验证者”。方法包括：混合初始化优选起始图，迭代地由LLM提出结构修改建议并通过统计标准验证，利用错误记忆高效引导搜索空间。

Result: 实验表明，CauScientist在基准数据集上大幅优于纯数据驱动基线，F1分数提升高达53.8%，召回率从35.0%增至100.0%；在复杂图结构（如37节点）上，SHD比单独LLM降低44.0%。

Conclusion: CauScientist通过深度结合LLM和统计方法，显著提升了因果结构发现的准确率与效率，有效克服了现有方法的局限，适用于更复杂的实际场景。

Abstract: Causal discovery is fundamental to scientific understanding and reliable decision-making. Existing approaches face critical limitations: purely data-driven methods suffer from statistical indistinguishability and modeling assumptions, while recent LLM-based methods either ignore statistical evidence or incorporate unverified priors that can mislead result. To this end, we propose CauScientist, a collaborative framework that synergizes LLMs as hypothesis-generating "data scientists" with probabilistic statistics as rigorous "verifiers". CauScientist employs hybrid initialization to select superior starting graphs, iteratively refines structures through LLM-proposed modifications validated by statistical criteria, and maintains error memory to guide efficient search space. Experiments demonstrate that CauScientist substantially outperforms purely data-driven baselines, achieving up to 53.8% F1 score improvement and enhancing recall from 35.0% to 100.0%. Notably, while standalone LLM performance degrades with graph complexity, CauScientist reduces structural hamming distance (SHD) by 44.0% compared to Qwen3-32B on 37-node graphs. Our project page is at https://github.com/OpenCausaLab/CauScientist.

</details>


### [402] [Activation-Space Anchored Access Control for Multi-Class Permission Reasoning in Large Language Models](https://arxiv.org/abs/2601.13630)
*Zhaopeng Zhang,Pengcheng Sun,Lan Zhang,Chen Tang,Jiewei Lai,Yunhao Wang,Hui Jin*

Main category: cs.CL

TL;DR: 本文提出了一种无须微调即可实现多类权限精细控制的新方法，可大幅降低大语言模型在知识检索问答场景下的权限泄露风险，并显著抵御特权滥用及攻击。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在知识问答任务中难以满足细粒度访问控制，存在在无权限的情况下意外泄露敏感信息的问题，亟需无需代价高昂微调即可实现高效权限管理的解决方案。

Method: 作者发现：同一问题在不同权限下的激活空间表示存在明显簇分布，据此提出AAAC（Activation-space Anchored Access Control）框架。AAAC无需微调，通过小规模样本构建每类权限的锚点库，推理时引入多锚点激活引导机制，将模型激活引向与用户权限匹配的合法区域，从根本上抑制超权限生成。

Result: 在三类主流LLM上，AAAC显著降低权限违规率（最高降幅86.5%）和prompt攻击成功率（最高降幅90.7%），同时对输出实用性影响极小，且推理开销很低。

Conclusion: AAAC方法有效实现了LLM在知识库问答中的多类精细化权限控制，既提升了安全性，也兼顾了实用性，无需额外训练，适合实际部署应用。

Abstract: Large language models (LLMs) are increasingly deployed over knowledge bases for efficient knowledge retrieval and question answering. However, LLMs can inadvertently answer beyond a user's permission scope, leaking sensitive content, thus making it difficult to deploy knowledge-base QA under fine-grained access control requirements. In this work, we identify a geometric regularity in intermediate activations: for the same query, representations induced by different permission scopes cluster distinctly and are readily separable. Building on this separability, we propose Activation-space Anchored Access Control (AAAC), a training-free framework for multi-class permission control. AAAC constructs an anchor bank, with one permission anchor per class, from a small offline sample set and requires no fine-tuning. At inference time, a multi-anchor steering mechanism redirects each query's activations toward the anchor-defined authorized region associated with the current user, thereby suppressing over-privileged generations by design. Finally, extensive experiments across three LLM families demonstrate that AAAC reduces permission violation rates by up to 86.5% and prompt-based attack success rates by 90.7%, while improving response usability with minor inference overhead compared to baselines.

</details>


### [403] [Towards Token-Level Text Anomaly Detection](https://arxiv.org/abs/2601.13644)
*Yang Cao,Bicheng Yu,Sikun Yang,Ming Liu,Yujiu Yang*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的token级异常检测方法，实现了对文本中具体异常位置的细粒度定位，并通过三个公开数据集验证了方法优越性。


<details>
  <summary>Details</summary>
Motivation: 现有文本异常检测方法多局限于文档级，难以定位异常具体位置。实际应用如垃圾邮件、虚假新闻等场景，往往需要更精细的异常定位能力。

Method: 作者正式定义了文档级及token级异常，并提出了一个统一检测框架，支持多级别异常检测。此外，构建并标注了三个包含垃圾邮件、评论和语法错误的基准数据集，全部带有token级标签。

Result: 实验表明，所提方法在三个数据集上的检测性能均优于六种基线方法。

Conclusion: 本文方法不仅提升了异常检测的精确度，还推动了token级细粒度分析研究。所用数据和代码全部开源，便于后续研究借鉴和应用。

Abstract: Despite significant progress in text anomaly detection for web applications such as spam filtering and fake news detection, existing methods are fundamentally limited to document-level analysis, unable to identify which specific parts of a text are anomalous. We introduce token-level anomaly detection, a novel paradigm that enables fine-grained localization of anomalies within text. We formally define text anomalies at both document and token-levels, and propose a unified detection framework that operates across multiple levels. To facilitate research in this direction, we collect and annotate three benchmark datasets spanning spam, reviews and grammar errors with token-level labels. Experimental results demonstrate that our framework get better performance than other 6 baselines, opening new possibilities for precise anomaly localization in text. All the codes and data are publicly available on https://github.com/charles-cao/TokenCore.

</details>


### [404] [Fairness or Fluency? An Investigation into Language Bias of Pairwise LLM-as-a-Judge](https://arxiv.org/abs/2601.13649)
*Xiaolin Zhou,Zheng Luo,Yicheng Gao,Qixuan Chen,Xiyang Hu,Yue Zhao,Ruishan Liu*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）作为评审时存在的语言偏见问题，发现模型在对不同语言文本进行评价时表现存在显著差异。尤其在同语言和跨语言对比中，偏向主要语言（如英语）现象突出，而这种偏见不能完全用困惑度低的现象解释。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，LLMs被广泛应用于自动评审（LLM-as-a-judge）任务，但先前研究发现其结论常与人类偏好不一致，其中一个核心问题是模型对不同语言文本的评分存在偏见。因此本文旨在系统性分析LLM在作为评判者时的语言偏见问题，以便提升其公正性与多样性。

Method: 本文考察了两种类型的语言偏见：1）同语言对比下的表现差异，2）跨语言对比时对主流语言的偏好。通过构建跨不同语言的文本对和问题，系统分析了主流和非主流语言的评分差异，并探究了模型困惑度对该偏见的影响。

Result: 研究发现：1）在同语言对比下，欧美语言的文本表现稳定优于非洲语言，且在文化相关题材中偏见更强；2）跨语言对比中，模型明显偏向英语答案，这一偏好主要受答案语言影响而非问题语言；3）模型困惑度与语言偏见存在弱相关，但无法完全解释这种偏见。

Conclusion: LLM-as-a-judge在文本评价中存在系统性的语言偏见，主流语言尤其受益于模型的偏好，仅靠困惑度无法消除该问题。提升评审的多语言公平性应成为后续LLM应用和训练的重要目标。

Abstract: Recent advances in Large Language Models (LLMs) have incentivized the development of LLM-as-a-judge, an application of LLMs where they are used as judges to decide the quality of a certain piece of text given a certain context. However, previous studies have demonstrated that LLM-as-a-judge can be biased towards different aspects of the judged texts, which often do not align with human preference. One of the identified biases is language bias, which indicates that the decision of LLM-as-a-judge can differ based on the language of the judged texts. In this paper, we study two types of language bias in pairwise LLM-as-a-judge: (1) performance disparity between languages when the judge is prompted to compare options from the same language, and (2) bias towards options written in major languages when the judge is prompted to compare options of two different languages. We find that for same-language judging, there exist significant performance disparities across language families, with European languages consistently outperforming African languages, and this bias is more pronounced in culturally-related subjects. For inter-language judging, we observe that most models favor English answers, and that this preference is influenced more by answer language than question language. Finally, we investigate whether language bias is in fact caused by low-perplexity bias, a previously identified bias of LLM-as-a-judge, and we find that while perplexity is slightly correlated with language bias, language bias cannot be fully explained by perplexity only.

</details>


### [405] [Beyond Known Facts: Generating Unseen Temporal Knowledge to Address Data Contamination in LLM Evaluation](https://arxiv.org/abs/2601.13658)
*Arthur Amalvy,Hen-Hsen Huang*

Main category: cs.CL

TL;DR: 本文提出了一套自动生成未来时间知识图谱评测数据集的方法，以解决现有数据稀缺和训练/测试集交叉污染问题。该数据集提升了对大语言模型（LLM）进行时序知识抽取真实性能的评估。


<details>
  <summary>Details</summary>
Motivation: 时序知识图谱抽取（TKGE）需要抽取带有时间戳的事实，服务于大型知识库的自动丰富。然而，缺乏高质量、无污染（训练集与评测集无重合）的评测数据，影响了模型实际能力的评估。

Method: 提出一种两步式合成评测集方法：首先通过时序知识图谱预测（TKGF）生成来自未来的、原未见过的知识四元组，再过滤以保持知识库结构一致；其次，利用大语言模型将四元组转成文本，实现事实与文本的语义对齐。

Result: 应用该方法，作者生成了包含4200个未来四元组及其文本描述的无污染评测数据集，并以先进的LLM抽取框架EDC为基准，结果显示在新合成数据集上LLM性能明显下降，验证了以往评估中可能存在的虚高情况。

Conclusion: 该合成评测集及生成方法可持续无污染地产生未来时序数据，更真实地反映TKGE模型能力，有助于推动领域发展，并已公开发布相关数据和方法。

Abstract: The automatic extraction of information is important for populating large web knowledge bases such as Wikidata. The temporal version of that task, temporal knowledge graph extraction (TKGE), involves extracting temporally grounded facts from text, represented as semantic quadruples (subject, relation, object, timestamp). Many recent systems take advantage of large language models (LLMs), which are becoming a new cornerstone of the web due to their performance on many tasks across the natural language processing (NLP) field. Despite the importance of TKGE, existing datasets for training and evaluation remain scarce, and contamination of evaluation data is an unaddressed issue, potentially inflating LLMs' perceived performance due to overlaps between training and evaluation sets. To mitigate these challenges, we propose a novel synthetic evaluation dataset constructed from predicted future, previously unseen temporal facts, thereby eliminating contamination and enabling robust and unbiased benchmarking. Our dataset creation involves a two-step approach: (1) Temporal Knowledge Graph Forecasting (TKGF) generates plausible future quadruples, which are subsequently filtered to adhere to the original knowledge base schema; (2) LLMs perform quadruple-to-text generation, creating semantically aligned textual descriptions. We benchmark Extract, Define and Canonicalize (EDC), a state-of-the-art LLM-based extraction framework, demonstrating that LLM performance decreases when evaluated on our dataset compared to a dataset of known facts. We publicly release our dataset consisting of 4.2K future quadruples and corresponding textual descriptions, along with the generation methodology, enabling continuous creation of unlimited future temporal datasets to serve as long-term, contamination-free benchmarks for TKGE.

</details>


### [406] [Temporal-Spatial Decouple before Act: Disentangled Representation Learning for Multimodal Sentiment Analysis](https://arxiv.org/abs/2601.13659)
*Chunlei Meng,Ziyang Zhou,Lucas He,Xiaojing Du,Chun Ouyang,Zhongxue Gan*

Main category: cs.CL

TL;DR: 本文提出了 TSDA 方法，通过在多模态情感分析中时空解耦，有效提升了模型性能，并通过大量实验证明优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感分析模型往往将时空信息混合建模，忽视了它们的异质性，导致信息不对称和性能受限。作者希望通过明确区分时序和空间信息来提升分析的准确性和表现。

Method: 作者提出 TSDA（Temporal-Spatial Decouple before Act）方法，将每种模态的数据分别通过时序编码器和空间编码器分离处理，再基于因子一致性进行跨模态对齐，仅对齐时序与时序特征、空间与空间特征，并通过特定的监督和去相关正则减少特征泄漏，最后通过门控模块重新融合用于下游任务。

Result: 在多个多模态情感分析基准实验中，TSDA 明显优于现有主流方法，消融实验进一步验证了时空解耦模块设计的必要性与可解释性。

Conclusion: 明确的时序与空间解耦加上有效的跨模态对齐方式，有助于提升多模态情感分析性能，为复杂信息融合提供了新的可解释方法。

Abstract: Multimodal Sentiment Analysis integrates Linguistic, Visual, and Acoustic. Mainstream approaches based on modality-invariant and modality-specific factorization or on complex fusion still rely on spatiotemporal mixed modeling. This ignores spatiotemporal heterogeneity, leading to spatiotemporal information asymmetry and thus limited performance. Hence, we propose TSDA, Temporal-Spatial Decouple before Act, which explicitly decouples each modality into temporal dynamics and spatial structural context before any interaction. For every modality, a temporal encoder and a spatial encoder project signals into separate temporal and spatial body. Factor-Consistent Cross-Modal Alignment then aligns temporal features only with their temporal counterparts across modalities, and spatial features only with their spatial counterparts. Factor specific supervision and decorrelation regularization reduce cross factor leakage while preserving complementarity. A Gated Recouple module subsequently recouples the aligned streams for task. Extensive experiments show that TSDA outperforms baselines. Ablation analysis studies confirm the necessity and interpretability of the design.

</details>


### [407] [CommunityBench: Benchmarking Community-Level Alignment across Diverse Groups and Tasks](https://arxiv.org/abs/2601.13669)
*Jiayu Lin,Zhongyu Wei*

Main category: cs.CL

TL;DR: 本文提出了一种名为“社区级对齐”的大语言模型（LLM）对齐新方式，并发布了第一个社区级对齐评测基准CommunityBench。实验显示当前模型难以良好理解社区价值偏好，社区对齐为大规模、多元化对齐提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐要么假设统一价值观（容易忽视少数群体），要么针对个体量身定制（成本高昂）。作者认为人类社会有天然的社区集群，应探索更具可行性和包容性的社区级对齐。

Method: 1. 提出“社区级对齐”概念，作为全员一致与个体定制之间的折中。2. 实现大规模社区级对齐评测基准CommunityBench，设计基于社会认同与联结理论的四项任务。3. 用CommunityBench评估多个主流基础大模型。4. 研究社区对齐对个体建模的促进作用。

Result: 评测结果表明：现有LLM对社区偏好的把握有限。社区级对齐有助于提升模型的多样性和可扩展性。

Conclusion: 社区级对齐为LLM安全有效地反映多元人类价值提供了有希望方向。CommunityBench为研究这一领域提供了必要的基础工具和评测手段。

Abstract: Large language models (LLMs) alignment ensures model behaviors reflect human value. Existing alignment strategies primarily follow two paths: one assumes a universal value set for a unified goal (i.e., one-size-fits-all), while the other treats every individual as unique to customize models (i.e., individual-level). However, assuming a monolithic value space marginalizes minority norms, while tailoring individual models is prohibitively expensive. Recognizing that human society is organized into social clusters with high intra-group value alignment, we propose community-level alignment as a "middle ground". Practically, we introduce CommunityBench, the first large-scale benchmark for community-level alignment evaluation, featuring four tasks grounded in Common Identity and Common Bond theory. With CommunityBench, we conduct a comprehensive evaluation of various foundation models on CommunityBench, revealing that current LLMs exhibit limited capacity to model community-specific preferences. Furthermore, we investigate the potential of community-level alignment in facilitating individual modeling, providing a promising direction for scalable and pluralistic alignment.

</details>


### [408] [HeteroCache: A Dynamic Retrieval Approach to Heterogeneous KV Cache Compression for Long-Context LLM Inference](https://arxiv.org/abs/2601.13684)
*Zhiyuan Shi,Qibo Qiu,Feng Xue,Zhonglin Jiang,Li Yu,Jian Jiang,Xiaofei He,Wenxiao Wang*

Main category: cs.CL

TL;DR: KV缓存的线性内存增长限制了大模型在长上下文任务中的推理效率。为了解决这一瓶颈，本文提出了无需训练的动态压缩框架HeteroCache，可在保证性能的同时大幅降低内存和I/O开销。


<details>
  <summary>Details</summary>
Motivation: 在长上下文推理时，KV缓存需要随输入长度线性增长，极大消耗显存成为核心瓶颈。现有静态压缩手段因无法动态适应注意力分布变化而效果有限，动态检索方法则又常受限于粗粒度策略和高I/O延迟。

Method: 提出HeteroCache框架：一方面基于注意力头的时序异质性和空间冗余性，对不同注意力头进行稳定性和冗余性划分，采用细粒度加权分配缓存；另一方面引入分层存储机制，由代表性头监控注意力转移，并在需要时异步、按需从CPU检索，降低I/O瓶颈。整个方法无需额外训练。

Result: HeteroCache在多个长上下文基准上取得最优结果。在224K上下文长度下，推理加速最高提升达3倍。

Conclusion: HeteroCache可高效支持大模型长上下文推理，兼顾准确性与系统性能，为解决KV缓存膨胀难题提供了一种实用且泛用的方法。

Abstract: The linear memory growth of the KV cache poses a significant bottleneck for LLM inference in long-context tasks. Existing static compression methods often fail to preserve globally important information, principally because they overlook the attention drift phenomenon where token significance evolves dynamically. Although recent dynamic retrieval approaches attempt to address this issue, they typically suffer from coarse-grained caching strategies and incur high I/O overhead due to frequent data transfers. To overcome these limitations, we propose HeteroCache, a training-free dynamic compression framework. Our method is built on two key insights: attention heads exhibit diverse temporal heterogeneity, and there is significant spatial redundancy among heads within the same layer. Guided by these insights, HeteroCache categorizes heads based on stability and redundancy. Consequently, we apply a fine-grained weighting strategy that allocates larger cache budgets to heads with rapidly shifting attention to capture context changes, thereby addressing the inefficiency of coarse-grained strategies. Furthermore, we employ a hierarchical storage mechanism in which a subset of representative heads monitors attention shift, and trigger an asynchronous, on-demand retrieval of contexts from the CPU, effectively hiding I/O latency. Finally, experiments demonstrate that HeteroCache achieves state-of-the-art performance on multiple long-context benchmarks and accelerates decoding by up to $3\times$ compared to the original model in the 224K context. Our code will be open-source.

</details>


### [409] [Dr. Assistant: Enhancing Clinical Diagnostic Inquiry via Structured Diagnostic Reasoning Data and Reinforcement Learning](https://arxiv.org/abs/2601.13690)
*Yue Guo,Fanfu Wang,Jianwei Lv,Xincheng Shi,Yuchen Li,Youya Wang,Yunsheng Zeng,Yujing Liu,Yunhao Qiao,Gen Li,Junfeng Wang,Bo Yuan*

Main category: cs.CL

TL;DR: 本文提出了一种新型临床诊断推理数据结构和临床决策支持模型Dr. Assistant，实验结果显示其诊断推理与问诊能力优于开源模型，对医疗辅助诊断具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 现有临床决策支持系统（CDSS）存在维护成本高和泛化能力差等问题，虽然大语言模型（LLMs）在医疗领域表现出色，但其诊断推理和问诊能力依然有限，因此亟需提升模型在临床推理与问询方面的能力。

Method: 提出CDRD（Clinical Diagnostic Reasoning Data）数据结构以捕捉抽象的临床推理逻辑，并建立相应数据构建流程。同时设计Dr. Assistant模型，采用两阶段的训练方法：先进行监督微调（SFT），再结合定制奖励函数的强化学习（RL）；并构建新基准用于评估诊断推理和问诊能力。

Result: 实验表明，Dr. Assistant模型在诊断推理与问询任务中，以显著优势超越开源模型，整体性能接近闭源模型。

Conclusion: Dr. Assistant有效提升了临床诊断推理和问诊指导能力，为解决CDSS现有问题提供了新的有效方案。

Abstract: Clinical Decision Support Systems (CDSSs) provide reasoning and inquiry guidance for physicians, yet they face notable challenges, including high maintenance costs and low generalization capability. Recently, Large Language Models (LLMs) have been widely adopted in healthcare due to their extensive knowledge reserves, retrieval, and communication capabilities. While LLMs show promise and excel at medical benchmarks, their diagnostic reasoning and inquiry skills are constrained. To mitigate this issue, we propose (1) Clinical Diagnostic Reasoning Data (CDRD) structure to capture abstract clinical reasoning logic, and a pipeline for its construction, and (2) the Dr. Assistant, a clinical diagnostic model equipped with clinical reasoning and inquiry skills. Its training involves a two-stage process: SFT, followed by RL with a tailored reward function. We also introduce a benchmark to evaluate both diagnostic reasoning and inquiry. Our experiments demonstrate that the Dr. Assistant outperforms open-source models and achieves competitive performance to closed-source models, providing an effective solution for clinical diagnostic inquiry guidance.

</details>


### [410] [OptiSQL: Executable SQL Generation from Optical TokensOptiSQL: Executable SQL Generation from Optical Tokens](https://arxiv.org/abs/2601.13695)
*Sifan Li,Hongkai Chen,Yujun Cai,Liyang Chen,Qingwen Ye,Yiwei Wang*

Main category: cs.CL

TL;DR: 本文提出OptiSQL系统，能够直接从表格图片和自然语言问题生成可执行的SQL查询，显著减少输入token数量，并保持高准确率。


<details>
  <summary>Details</summary>
Motivation: 传统的text-to-SQL方法依赖于结构化文本表示表格，这在很多实际场景中并不适用，因为真实世界中表格常以图片或网页可视化形式出现；同时，全线性化也会导致token数大幅增加，影响效率。

Method: OptiSQL提出用OCR视觉编码器将表格结构和内容压缩为少量光学表示token，再用预训练解码器进行SQL生成，且冻结编码器，以检验表示能力。

Result: 在Spider 2.0-Snow视觉化数据集上，OptiSQL在大幅减少token数量（数量级减小）的同时，保持了很高的SQL执行准确率。鲁棒性测试也显示光学token在视觉扰动下仍能维持必要结构信息。

Conclusion: 光学token可作为高效的表格输入表达方式，支持直接从表格图像生成SQL，在实际应用场景下具备高效率和强表现力。

Abstract: Executable SQL generation is typically studied in text-to-SQL settings, where tables are provided as fully linearized textual schemas and contents. While effective, this formulation assumes access to structured text and incurs substantial token overhead, which is misaligned with many real-world scenarios where tables appear as visual artifacts in documents or webpages. We investigate whether compact optical representations can serve as an efficient interface for executable semantic parsing. We present OptiSQL, a vision-driven framework that generates executable SQL directly from table images and natural language questions using compact optical tokens. OptiSQL leverages an OCR-oriented visual encoder to compress table structure and content into a small set of optical tokens and fine-tunes a pretrained decoder for SQL generation while freezing the encoder to isolate representation sufficiency. Experiments on a visualized version of Spider 2.0-Snow show that OptiSQL retains strong execution accuracy while reducing table input tokens by an order of magnitude. Robustness analyses further demonstrate that optical tokens preserve essential structural information under visual perturbations.

</details>


### [411] [Uncertainty-Aware Gradient Signal-to-Noise Data Selection for Instruction Tuning](https://arxiv.org/abs/2601.13697)
*Zhihang Yuan,Chengyu Yue,Long Huang,Litu Ou,Lei Shi*

Main category: cs.CL

TL;DR: 作者提出了GRADFILTERING，一种新的数据筛选框架，通过利用小型GPT-2代理与LoRA集成，以及梯度信噪比（G-SNR）指标，提升大语言模型（LLM）微调数据的选择效率和效果。该方法在多项评估中表现优异，并能加快收敛速度。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的指令微调数据规模大且质量参差不齐，导致计算成本高，且全量微调往往没必要。现有数据筛选方法要么计算昂贵，要么依据静态弱指标，忽视数据不确定性，错失了解读LLM的重要线索。

Method: 提出GRADFILTERING，利用小型GPT-2代理结合LoRA集成，聚合每条数据的梯度信息，计算梯度信噪比（G-SNR）作为效用分数，从而选择更有意义的数据子集，且不依赖具体任务目标。

Result: GRADFILTERING在“大模型评判”和人工评测中，效果达到或超过现有方法和随机子集。选出的数据集相较于其他高水平筛选器在相同算力下收敛更快，体现出基于不确定性得分的优越性。

Conclusion: GRADFILTERING能在保持效果甚至提升的情况下，减少微调所需数据和算力，验证了不确定性感知的过滤对于高效微调LLM的重要性。

Abstract: Instruction tuning is a standard paradigm for adapting large language models (LLMs), but modern instruction datasets are large, noisy, and redundant, making full-data fine-tuning costly and often unnecessary. Existing data selection methods either build expensive gradient datastores or assign static scores from a weak proxy, largely ignoring evolving uncertainty, and thus missing a key source of LLM interpretability. We propose GRADFILTERING, an objective-agnostic, uncertainty-aware data selection framework that utilizes a small GPT-2 proxy with a LoRA ensemble and aggregates per-example gradients into a Gradient Signal-to-Noise Ratio (G-SNR) utility. Our method matches or surpasses random subsets and strong baselines in most LLM-as-a-judge evaluations as well as in human assessment. Moreover, GRADFILTERING-selected subsets converge faster than competitive filters under the same compute budget, reflecting the benefit of uncertainty-aware scoring.

</details>


### [412] [GerAV: Towards New Heights in German Authorship Verification using Fine-Tuned LLMs on a New Benchmark](https://arxiv.org/abs/2601.13711)
*Lotta Kiefer,Christoph Leiter,Sotaro Takeshita,Elena Schmidt,Steffen Eger*

Main category: cs.CL

TL;DR: 本文提出了首个大规模德语作者验证（AV）基准GerAV，内含60万+标注对，系统评估主流方法并推动相关研究。


<details>
  <summary>Details</summary>
Motivation: 目前作者验证的大型基准和系统性评价主要集中在英语，而其他语言（如德语）相对缺乏相关资源，阻碍AV在多语种环境下的发展。因此，该研究旨在填补德语AV领域的基准空白。

Method: 作者构建了GerAV数据集，数据来源包括Twitter和Reddit，并细分为同域、跨域和基于个人资料的子集，便于精细分析不同变量的影响。利用这些训练集，研究系统评估了多种强基线和最新SOTA模型，包括微调大语言模型。

Result: 微调的大语言模型在AV任务中性能优于最新基线，最大提升0.09 F1分，并在零样本设置下比GPT-5高0.08。发现针对特定数据类型训练的模型在匹配条件下表现最佳，但跨领域泛化能力较弱；这种局限可通过混合训练来源缓解。

Conclusion: GerAV为德语及跨领域作者验证研究提供了挑战性与通用性的基准，有助于推动该方向的深入发展。

Abstract: Authorship verification (AV) is the task of determining whether two texts were written by the same author and has been studied extensively, predominantly for English data. In contrast, large-scale benchmarks and systematic evaluations for other languages remain scarce. We address this gap by introducing GerAV, a comprehensive benchmark for German AV comprising over 600k labeled text pairs. GerAV is built from Twitter and Reddit data, with the Reddit part further divided into in-domain and cross-domain message-based subsets, as well as a profile-based subset. This design enables controlled analysis of the effects of data source, topical domain, and text length. Using the provided training splits, we conduct a systematic evaluation of strong baselines and state-of-the-art models and find that our best approach, a fine-tuned large language model, outperforms recent baselines by up to 0.09 absolute F1 score and surpasses GPT-5 in a zero-shot setting by 0.08. We further observe a trade-off between specialization and generalization: models trained on specific data types perform best under matching conditions but generalize less well across data regimes, a limitation that can be mitigated by combining training sources. Overall, GerAV provides a challenging and versatile benchmark for advancing research on German and cross-domain AV.

</details>


### [413] [Simulated Ignorance Fails: A Systematic Study of LLM Behaviors on Forecasting Problems Before Model Knowledge Cutoff](https://arxiv.org/abs/2601.13717)
*Zehan Li,Yuxuan Wang,Ali El Lahib,Ying-Jieh Xia,Xinyu Pi*

Main category: cs.CL

TL;DR: 本文系统评估了在大型语言模型（LLM）中的“模拟无知”方法（Simulated Ignorance, SI）是否能够作为评估模型预测能力的可靠替代方案。结果显示SI方法无法有效模拟真实的知识截止状态，因此不推荐作为回顾性预测评测方式。


<details>
  <summary>Details</summary>
Motivation: 随着SOTA模型的知识截止时间越来越新，针对已解决事件的回顾性预测评估（RF）可用的干净数据迅速减少。传统前瞻性评估虽更严谨，但等待实际结果的延迟成本极高。因此产生了通过_prompt_让模型“屏蔽”截止后知识的“模拟无知”方法，本文旨在系统检验这种方法的有效性。

Method: 作者在477道高水平竞赛问题和9个不同的LLM上比较了三种情况：真实无知（TI）、模拟无知（SI）、以及采用链式思维推理等提示方式的影响，系统测量模型执行SI的准确性和偏差。

Result: 实验显示，采用截止提示的SI性能比真实无知（TI）低52%；链式思维推理未能有效抑制模型已持有的知识，即使推理过程未明确提及截止后信息；模型推理能力越强，反而更难模拟真正的无知状态。

Conclusion: 通过prompt无法让模型可靠‘回到’知识截止点。用SI方法做回顾性预测评测存在方法论缺陷，作者建议不要用SI方案作为模型预测能力基准。

Abstract: Evaluating LLM forecasting capabilities is constrained by a fundamental tension: prospective evaluation offers methodological rigor but prohibitive latency, while retrospective forecasting (RF) -- evaluating on already-resolved events -- faces rapidly shrinking clean evaluation data as SOTA models possess increasingly recent knowledge cutoffs. Simulated Ignorance (SI), prompting models to suppress pre-cutoff knowledge, has emerged as a potential solution. We provide the first systematic test of whether SI can approximate True Ignorance (TI). Across 477 competition-level questions and 9 models, we find that SI fails systematically: (1) cutoff instructions leave a 52% performance gap between SI and TI; (2) chain-of-thought reasoning fails to suppress prior knowledge, even when reasoning traces contain no explicit post-cutoff references; (3) reasoning-optimized models exhibit worse SI fidelity despite superior reasoning trace quality. These findings demonstrate that prompts cannot reliably "rewind" model knowledge. We conclude that RF on pre-cutoff events is methodologically flawed; we recommend against using SI-based retrospective setups to benchmark forecasting capabilities.

</details>


### [414] [OP-Bench: Benchmarking Over-Personalization for Memory-Augmented Personalized Conversational Agents](https://arxiv.org/abs/2601.13722)
*Yulin Hu,Zimo Long,Jiahe Guo,Xingyu Sui,Xing Fu,Weixiang Zhao,Yanyan Zhao,Bing Qin*

Main category: cs.CL

TL;DR: 本文关注于对话系统中过度个性化的问题，即在使用用户记忆进行个性化时，出现强行、不相关或谄媚的信息使用。为此，作者提出了OP-Bench基准，并引入了Self-ReCheck机制来缓解此类问题。


<details>
  <summary>Details</summary>
Motivation: 现有的对话系统在利用用户信息实现个性化时，主要关注能否回忆和应用用户信息，但很少关注这些信息是否被适当使用。事实上，不当使用用户记忆可能导致用户感到不舒服或被冒犯，即‘过度个性化’现象。

Method: 作者将过度个性化细分为不相关、重复和谄媚三类，并构建了包含1,700个经过验证实例的OP-Bench基准用于评估。使用该基准，作者对多种大模型和带记忆机制的对话方法进行了系统评测。同时，提出了一种通用的内存过滤机制Self-ReCheck，用以抑制过度个性化现象。

Result: 实验表明，在引入记忆后，对话系统普遍存在过度个性化现象，模型往往过度检索和关注用户记忆，即使在无需个性化信息的场景中也是如此。引入Self-ReCheck后，过度个性化问题得到明显缓解，同时个性化性能得以保留。

Conclusion: OP-Bench基准为过度个性化提供了有效评测工具，Self-ReCheck机制则为更可控、合宜的个性化对话提供了方法支持，为未来发展开辟了方向。

Abstract: Memory-augmented conversational agents enable personalized interactions using long-term user memory and have gained substantial traction. However, existing benchmarks primarily focus on whether agents can recall and apply user information, while overlooking whether such personalization is used appropriately. In fact, agents may overuse personal information, producing responses that feel forced, intrusive, or socially inappropriate to users. We refer to this issue as \emph{over-personalization}. In this work, we formalize over-personalization into three types: Irrelevance, Repetition, and Sycophancy, and introduce \textbf{OP-Bench} a benchmark of 1,700 verified instances constructed from long-horizon dialogue histories. Using \textbf{OP-Bench}, we evaluate multiple large language models and memory-augmentation methods, and find that over-personalization is widespread when memory is introduced. Further analysis reveals that agents tend to retrieve and over-attend to user memories even when unnecessary. To address this issue, we propose \textbf{Self-ReCheck}, a lightweight, model-agnostic memory filtering mechanism that mitigates over-personalization while preserving personalization performance. Our work takes an initial step toward more controllable and appropriate personalization in memory-augmented dialogue systems.

</details>


### [415] [On Temperature-Constrained Non-Deterministic Machine Translation: Potential and Evaluation](https://arxiv.org/abs/2601.13729)
*Weichuan Wang,Mingyang Liu,Linqi Song,Chen Ma*

Main category: cs.CL

TL;DR: 本研究系统性地分析了机器翻译中的非确定性现象，发现其具有提升翻译多样性和质量的潜力，但同时评测方法也面临新挑战，并提出了相应的解决策略。


<details>
  <summary>Details</summary>
Motivation: 近年来语言模型的非确定性特性引发了广泛关注，但在机器翻译这一复杂任务中的相关探索有限。多模态性是机器翻译长期面临的一大难题，作者希望调查非确定性机器翻译在解决该问题上的潜力及其带来的评测新难点。

Method: 作者系统性地评估了现代机器翻译系统，引入温度限制后的非确定性机器翻译（ND-MT）作为研究重点。通过在三个公开数据集和五个主流ND-MT系统上，利用词汇和语义等多种指标，在不同采样规模下进行了详细对比。

Result: ND-MT能够提供比确定性翻译（D-MT）更高质量的候选译文，对提升多模态具有显著效果。但同时，D-MT设计的传统评测指标在ND-MT上不再有效，系统排名由最低质量候选项主导，出现了“Buckets效应”。

Conclusion: 非确定性机器翻译具备解决多模态难题和提升翻译质量的潜力，但需要针对其新特性设计更健壮、可靠的评测方法。为此，作者提出了ExpectoSample策略，用于自动评估现有评测指标在ND-MT中的可靠性。

Abstract: In recent years, the non-deterministic properties of language models have garnered considerable attention and have shown a significant influence on real-world applications. However, such properties remain under-explored in machine translation (MT), a complex, non-deterministic NLP task. In this study, we systematically evaluate modern MT systems and identify temperature-constrained Non-Deterministic MT (ND-MT) as a distinct phenomenon. Additionally, we demonstrate that ND-MT exhibits significant potential in addressing the multi-modality issue that has long challenged MT research and provides higher-quality candidates than Deterministic MT (D-MT) under temperature constraints. However, ND-MT introduces new challenges in evaluating system performance. Specifically, the evaluation framework designed for D-MT fails to yield consistent evaluation results when applied to ND-MT. We further investigate this emerging challenge by evaluating five state-of-the-art ND-MT systems across three open datasets using both lexical-based and semantic-based metrics at varying sampling sizes. The results reveal a Buckets effect across these systems: the lowest-quality candidate generated by ND-MT consistently determines the overall system ranking across different sampling sizes for all reasonable metrics. Furthermore, we propose the ExpectoSample strategy to automatically assess the reliability of evaluation metrics for selecting robust ND-MT.

</details>


### [416] [Towards robust long-context understanding of large language model via active recap learning](https://arxiv.org/abs/2601.13734)
*Chenyu Hui*

Main category: cs.CL

TL;DR: 本文提出了一种名为主动回顾学习（ARL）的方法，用于提升大语言模型（LLM）对长文本的理解能力。ARL通过在持续预训练和推理阶段，有针对性地回顾并总结前文内容，实现递归记忆机制，实验证明其显著提升了模型表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在处理长文本或长上下文时，容易遗忘早先的信息，导致理解和推理能力下降。因此，亟需一种有效方法帮助模型更好地保持和利用长距离信息。

Method: ARL方法包括两个关键步骤：首先，通过比较长与短上下文的损失差异，识别长文本中的关键信息和相关段落，并用LLM对其进行总结；其次，让模型具备在推理时自主生成和利用这些回顾性总结的能力，从而在段落之间建立递归记忆机制。

Result: 在RULER数据集上提升了26.8%，在LongBench数据集上提升了9.44%，表明ARL能有效增强大模型的长文本理解能力。

Conclusion: ARL是一种简单有效的持续预训练方法，可以显著增强大语言模型的长文本理解能力，为大模型记忆扩展提供了新的、可扩展的解决方案。

Abstract: In this paper, we propose active recap learning (ARL), a framework for enhancing large language model (LLM) in understanding long contexts. ARL enables models to revisit and summarize earlier content through targeted sequence construction during contined pretraining and retrospective summarization at inference. First, we identify key tokens in prepared long context based on loss gaps between long and short forward contexts and find most revant preceding paragraphs, then summarize them using an LLM. Second, ARL equips models with the ability to autonomously generate and utilize these retrospective summaries during inference, thereby establishing a recursive memory mechanism across paragraphs. Experimental results show substantial gains, with ARL achieving a 26.8% improvement on RULER and a 9.44% improvement on LongBench. Overall, ARL offers a simple yet effective continued pretraining-based approach to strengthen long-context understanding, advancing scalable memory augmentation in LLM

</details>


### [417] [Dimension-First Evaluation of Speech-to-Speech Models with Structured Acoustic Cues](https://arxiv.org/abs/2601.13742)
*Arjun Chandra,Kevin Miller,Venkatesh Ravichandran,Constantinos Papayiannis,Venkatesh Saligrama*

Main category: cs.CL

TL;DR: 本文提出TRACE框架，使大语言模型（LLM）可以结合音频线索，进行高效且贴近人工的语音到语音（S2S）自动评测，超越现有成本高、过程不透明的音频语言模型（ALM）方法。


<details>
  <summary>Details</summary>
Motivation: 现有S2S自动评测需要耗费高昂成本的音频语言模型(ALM)，且LLM只能处理文本，无法考虑音频特征。这导致评测效率低，结果透明度差，与人类评价不一致。

Method: 提出TRACE框架：利用文本描述音频信号特征，结合LLM推理，实现对内容、音质和副语言特征三个维度的分开评判。通过HCoT人工思维链注释协议提升数据集，最后用确定性策略融合维度分数。

Result: TRACE在人类一致性上优于传统ALM和仅用文本的LLM方法，同时大幅减少成本。

Conclusion: TRACE框架实现了高效、透明、成本低并且更符合人类标准的S2S自动评测。HCoT注释和TRACE框架将在社区发布，推动领域发展。

Abstract: Large Language Model (LLM) judges exhibit strong reasoning capabilities but are limited to textual content. This leaves current automatic Speech-to-Speech (S2S) evaluation methods reliant on opaque and expensive Audio Language Models (ALMs). In this work, we propose TRACE (Textual Reasoning over Audio Cues for Evaluation), a novel framework that enables LLM judges to reason over audio cues to achieve cost-efficient and human-aligned S2S evaluation. To demonstrate the strength of the framework, we first introduce a Human Chain-of-Thought (HCoT) annotation protocol to improve the diagnostic capability of existing judge benchmarks by separating evaluation into explicit dimensions: content (C), voice quality (VQ), and paralinguistics (P). Using this data, TRACE constructs a textual blueprint of inexpensive audio signals and prompts an LLM to render dimension-wise judgments, fusing them into an overall rating via a deterministic policy. TRACE achieves higher agreement with human raters than ALMs and transcript-only LLM judges while being significantly more cost-effective. We will release the HCoT annotations and the TRACE framework to enable scalable and human-aligned S2S evaluation.

</details>


### [418] [Pro-AI Bias in Large Language Models](https://arxiv.org/abs/2601.13749)
*Benaya Trabelsi,Jonathan Shaki,Sarit Kraus*

Main category: cs.CL

TL;DR: 本文发现大型语言模型（LLMs）存在明显的“偏向AI”倾向，比如偏好推荐与AI相关的选项、夸大AI相关职位的薪资等。这一偏见在不同类型的模型和任务中都有表现，可能影响用户在重要决策中的选择和认知。


<details>
  <summary>Details</summary>
Motivation: 目前LLMs被广泛应用于各类决策支持场景，因此了解它们是否存在系统性偏见（特别是对AI自身的偏好）对模型安全、公平和实际应用极为关键。

Method: 作者设计了三项互补性实验：1）统计不同模型在建议类任务中对AI相关选项的推荐频率；2）比较模型在AI相关与非AI相关职业薪资评估上的差异；3）分析开源模型内表征AI相关领域与其他学科的相似性。

Result: 三项实验均发现了大模型对AI本身的显著偏好——无论是在选项推荐还是薪资评估，专有大模型表现出高度的“挺AI”现象。此外，模型内部表征中，AI作为学术领域具有最高的中心性和相似性（无论肯定、否定或中性语境）。

Conclusion: LLMs的建议和评估能力可能因其对AI的系统性偏见而影响用户的真实决策和认知，这为AI公平性、透明性和实际部署提出了重要警示。

Abstract: Large language models (LLMs) are increasingly employed for decision-support across multiple domains. We investigate whether these models display a systematic preferential bias in favor of artificial intelligence (AI) itself. Across three complementary experiments, we find consistent evidence of pro-AI bias. First, we show that LLMs disproportionately recommend AI-related options in response to diverse advice-seeking queries, with proprietary models doing so almost deterministically. Second, we demonstrate that models systematically overestimate salaries for AI-related jobs relative to closely matched non-AI jobs, with proprietary models overestimating AI salaries more by 10 percentage points. Finally, probing internal representations of open-weight models reveals that ``Artificial Intelligence'' exhibits the highest similarity to generic prompts for academic fields under positive, negative, and neutral framings alike, indicating valence-invariant representational centrality. These patterns suggest that LLM-generated advice and valuation can systematically skew choices and perceptions in high-stakes decisions.

</details>


### [419] [Habibi: Laying the Open-Source Foundation of Unified-Dialectal Arabic Speech Synthesis](https://arxiv.org/abs/2601.13802)
*Yushen Chen,Junzhe Liu,Yujie Tu,Zhikang Niu,Yuzhe Liang,Kai Yu,Chunyu Qiang,Chen Zhang,Xie Chen*

Main category: cs.CL

TL;DR: 论文提出了Habibi，一个专门为阿拉伯语方言设计的统一文本到语音（TTS）模型，利用现有开源语音识别（ASR）语料库，通过课程学习支持多种语料充足与稀缺的方言，并将模型及评测基准开源，推动该领域发展。


<details>
  <summary>Details</summary>
Motivation: 当前阿拉伯语方言文本到语音领域存在模型分散、数据及标准匮乏等问题，阻碍了学术及产业进步。该工作旨在填补方言语音合成的研究空白，并为后续提供公共评测基准。

Method: 利用公开的多方言ASR语料，通过语言学启发的课程学习方法，训练统一的TTS模型，不依赖文本的元音化处理（diacritization），同时支持高资源与低资源方言。

Result: 所提出的Habibi模型在生成质量方面超过了当前主流商用服务，同时模型具备良好的扩展性和上下文学习能力。

Conclusion: 该工作不仅开源了模型本身，还首次系统性建立了多方言阿拉伯语语音合成评测基准，并系统总结了领域挑战，为日后相关研究提供坚实基础。

Abstract: A notable gap persists in speech synthesis research and development for Arabic dialects, particularly from a unified modeling perspective. Despite its high practical value, the inherent linguistic complexity of Arabic dialects, further compounded by a lack of standardized data, benchmarks, and evaluation guidelines, steers researchers toward safer ground. To bridge this divide, we present Habibi, a suite of specialized and unified text-to-speech models that harnesses existing open-source ASR corpora to support a wide range of high- to low-resource Arabic dialects through linguistically-informed curriculum learning. Our approach outperforms the leading commercial service in generation quality, while maintaining extensibility through effective in-context learning, without requiring text diacritization. We are committed to open-sourcing the model, along with creating the first systematic benchmark for multi-dialect Arabic speech synthesis. Furthermore, by identifying the key challenges in and establishing evaluation standards for the process, we aim to provide a solid groundwork for subsequent research. Resources at https://SWivid.github.io/Habibi/ .

</details>


### [420] [Knowledge Graph-Assisted LLM Post-Training for Enhanced Legal Reasoning](https://arxiv.org/abs/2601.13806)
*Dezhao Song,Guglielmo Bonifazi,Frank Schilder,Jonathan Richard Schwarz*

Main category: cs.CL

TL;DR: 本文提出利用知识图谱辅助方法提高大语言模型在法律领域复杂推理能力，实验结果在多项法律基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型主要依赖大规模文本和人工反馈进行后训练，缺乏领域知识结构的建模，导致在法律等高风险专业领域难以进行深度推理。法律推理需深刻理解法律概念间关系，而现有方法对此支持不足。

Method: 提出以IRAC（事实、规则、分析与结论）框架为基础，整理12K法律案例，构建法律知识图谱。基于该知识图谱生成训练数据，并对三种主流大模型（参数量分别为30B、49B和70B）进行有监督微调和直接偏好优化。

Result: 后训练模型在5个法律基准的14个任务中，有4/5基准的平均表现优于对比基线。70B DPO优化模型在6个推理任务中的4项表现最好，超过了包括141B参数量的主流法律大模型。

Conclusion: 知识图谱辅助训练能显著提升大语言模型在法律推理任务中的表现，且该方法具有推广至其他高风险专业领域的潜力。

Abstract: LLM post-training has primarily relied on large text corpora and human feedback, without capturing the structure of domain knowledge. This has caused models to struggle dealing with complex reasoning tasks, especially for high-stakes professional domains. In Law, reasoning requires deep understanding of the relations between various legal concepts, a key component missing in current LLM post-training. In this paper, we propose a knowledge graph (KG)-assisted approach for enhancing LLMs' reasoning capability in Legal that is generalizable to other high-stakes domains. We model key legal concepts by following the \textbf{IRAC} (Issue, Rule, Analysis and Conclusion) framework, and construct a KG with 12K legal cases. We then produce training data using our IRAC KG, and conduct both Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) with three state-of-the-art (SOTA) LLMs (30B, 49B and 70B), varying architecture and base model family. Our post-trained models obtained better average performance on 4/5 diverse legal benchmarks (14 tasks) than baselines. In particular, our 70B DPO model achieved the best score on 4/6 reasoning tasks, among baselines and a 141B SOTA legal LLM, demonstrating the effectiveness of our KG for enhancing LLMs' legal reasoning capability.

</details>


### [421] [The Role of Prosodic and Lexical Cues in Turn-Taking with Self-Supervised Speech Representations](https://arxiv.org/abs/2601.13835)
*Sam OConnor Russell,Delphine Charuau,Naomi Harte*

Main category: cs.CL

TL;DR: 本论文提出一种基于声码器的方法，更加干净地控制语音中的韵律和词汇线索，从而研究了自监督语音表示（S3R）驱动的对话轮换模型的工作机制，发现模型既能从韵律也能从词汇信息中单独推断轮换时机。


<details>
  <summary>Details</summary>
Motivation: 在人机交互中，实现自然流畅的对话轮换依然是难题，目前S3R推动了轮换预测进步，但还不清楚其是依赖语音的韵律、词汇还是两者共同作用。

Method: 作者提出用声码器仿真、分别操控韵律和词汇信息，并用此方法测试S3R驱动的voice-activity projection模型；通过比较只含韵律或只含词汇成分的语音输入对模型预测的影响，分析两类信息的独立性。

Result: 实验显示，当输入为带有韵律但无内容的噪音时，模型预测准确率接近于原始语音，说明韵律信息对轮换预测高度有效；当韵律或词汇信息受扰时，S3R模型能自如切换利用另一类线索，不需再训练，说明两者在表征中相对独立。结果在CPC-based和wav2vec2.0 S3R均一致。

Conclusion: S3R模型既能利用韵律信息，也能依赖词汇信息进行对话轮换预测，任一信息被破坏时，模型可自动切换利用剩余线索；未来可只用韵律特征进行轮换预测以提升隐私和模型效率。这一发现为后续研究提供了方向，论文也开源了全部代码。

Abstract: Fluid turn-taking remains a key challenge in human-robot interaction. Self-supervised speech representations (S3Rs) have driven many advances, but it remains unclear whether S3R-based turn-taking models rely on prosodic cues, lexical cues or both. We introduce a vocoder-based approach to control prosody and lexical cues in speech more cleanly than prior work. This allows us to probe the voice-activity projection model, an S3R-based turn-taking model. We find that prediction on prosody-matched, unintelligible noise is similar to accuracy on clean speech. This reveals both prosodic and lexical cues support turn-taking, but either can be used in isolation. Hence, future models may only require prosody, providing privacy and potential performance benefits. When either prosodic or lexical information is disrupted, the model exploits the other without further training, indicating they are encoded in S3Rs with limited interdependence. Results are consistent in CPC-based and wav2vec2.0 S3Rs. We discuss our findings and highlight a number of directions for future work. All code is available to support future research.

</details>


### [422] [FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs](https://arxiv.org/abs/2601.13836)
*Qian Chen,Jinlan Fu,Changsong Li,See-Kiong Ng,Xipeng Qiu*

Main category: cs.CL

TL;DR: 论文提出了FutureOmni，这是首个针对多模态大模型（MLLMs）在音视频环境中未来事件预测能力进行评测的基准。通过构建大规模数据集并设计新训练策略，作者发现现有模型在预测未来事件方面表现有限，新方法能显著提升预测能力。


<details>
  <summary>Details</summary>
Motivation: 当前主流多模态大模型主要关注对既有事件的理解，缺乏对未来事件预测（即前瞻性理解）的能力评估工具。现有基准也未能针对音视频环境下的因果与时间推理不足，因此亟需针对多模态未来事件预测的评测基准。

Method: 作者提出FutureOmni基准，通过LLM辅助和人工筛选结合的流程构建，包含919个视频和1034个多选题，覆盖8大领域。评测13个多模态模型和7个视频模型的音视频未来事件预测能力。此外，作者提出Omni-Modal Future Forecasting（OFF）训练策略，并开放全部数据和代码。

Result: 实验显示，当前模型对音视频未来预测任务表现有限，特别是在语音密集场景下最优模型仅达64.8%准确率。采用OFF训练策略和新增数据集后，模型在FutureOmni及其它主流视听基准上预测能力和泛化能力得到提升。

Conclusion: 未来事件预测在多模态模型领域依然是重大挑战，FutureOmni基准和OFF训练方法推动了相关研究进展，对提升模型前瞻性推理能力具有重要意义。

Abstract: Although Multimodal Large Language Models (MLLMs) demonstrate strong omni-modal perception, their ability to forecast future events from audio-visual cues remains largely unexplored, as existing benchmarks focus mainly on retrospective understanding. To bridge this gap, we introduce FutureOmni, the first benchmark designed to evaluate omni-modal future forecasting from audio-visual environments. The evaluated models are required to perform cross-modal causal and temporal reasoning, as well as effectively leverage internal knowledge to predict future events. FutureOmni is constructed via a scalable LLM-assisted, human-in-the-loop pipeline and contains 919 videos and 1,034 multiple-choice QA pairs across 8 primary domains. Evaluations on 13 omni-modal and 7 video-only models show that current systems struggle with audio-visual future prediction, particularly in speech-heavy scenarios, with the best accuracy of 64.8% achieved by Gemini 3 Flash. To mitigate this limitation, we curate a 7K-sample instruction-tuning dataset and propose an Omni-Modal Future Forecasting (OFF) training strategy. Evaluations on FutureOmni and popular audio-visual and video-only benchmarks demonstrate that OFF enhances future forecasting and generalization. We publicly release all code (https://github.com/OpenMOSS/FutureOmni) and datasets (https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni).

</details>


### [423] [Pedagogical Alignment for Vision-Language-Action Models: A Comprehensive Framework for Data, Architecture, and Evaluation in Education](https://arxiv.org/abs/2601.13876)
*Unggi Lee,Jahyun Jeong,Sunyoung Shin,Haeun Park,Jeongsu Moon,Youngchang Song,Jaechang Shim,JaeHwan Lee,Yunju Noh,Seungwon Choi,Ahhyun Kim,TaeHyeon Kim,Kyungtae Joo,Taeyeong Kim,Gyeonggeon Lee*

Main category: cs.CL

TL;DR: 本文提出了一种面向教育场景的轻量级视-语-行为（VLA）框架，能助力科学演示并生成解释性内容，兼顾安全性与资源限制。


<details>
  <summary>Details</summary>
Motivation: 科学演示对STEM教育至关重要，但教师在操作时安全性和重复性面临挑战。当前VLA模型资源消耗大且牺牲了语言生成能力，难以应用于教育现场。

Method: 提出Pedagogical VLA Framework，包括：1）文本修复以恢复生成能力；2）通过大模型蒸馏传递教学知识；3）针对教育环境的安全训练；4）符合科学教育的教学评测。实验在物理、化学、生物和地科五类演示任务中展开。

Result: 在多个科学演示任务上，框架的任务表现（成功率、规范性、效率、安全性）与基线模型相当，并且能输出适合教育情境的解释内容。

Conclusion: Pedagogical VLA Framework兼顾资源效率与语言生成能力，能安全、有效支持科学演示并生成高质量的教学解释，适用于教育场景。

Abstract: Science demonstrations are important for effective STEM education, yet teachers face challenges in conducting them safely and consistently across multiple occasions, where robotics can be helpful. However, current Vision-Language-Action (VLA) models require substantial computational resources and sacrifice language generation capabilities to maximize efficiency, making them unsuitable for resource-constrained educational settings that require interpretable, explanation-generating systems. We present \textit{Pedagogical VLA Framework}, a framework that applies pedagogical alignment to lightweight VLA models through four components: text healing to restore language generation capabilities, large language model (LLM) distillation to transfer pedagogical knowledge, safety training for educational environments, and pedagogical evaluation adjusted to science education contexts. We evaluate Pedagogical VLA Framework across five science demonstrations spanning physics, chemistry, biology, and earth science, using an evaluation framework developed in collaboration with science education experts. Our evaluation assesses both task performance (success rate, protocol compliance, efficiency, safety) and pedagogical quality through teacher surveys and LLM-as-Judge assessment. We additionally provide qualitative analysis of generated texts. Experimental results demonstrate that Pedagogical VLA Framework achieves comparable task performance to baseline models while producing contextually appropriate educational explanations.

</details>


### [424] [OpenLearnLM Benchmark: A Unified Framework for Evaluating Knowledge, Skill, and Attitude in Educational Large Language Models](https://arxiv.org/abs/2601.13882)
*Unggi Lee,Sookbun Lee,Heungsoo Choi,Jinseo Lee,Haeun Park,Younghoon Jeon,Sungmin Cho,Minju Kang,Junbo Koh,Jiyeong Bae,Minwoo Nam,Juyeon Eun,Yeonji Jung,Yeil Jeong*

Main category: cs.CL

TL;DR: 该论文提出了OpenLearnLM基准，这是一个基于理论的、评估大型语言模型（LLM）教育能力的框架，涵盖知识、技能和态度三大维度，并对主流模型进行了详细测试。


<details>
  <summary>Details</summary>
Motivation: 当前用作教育工具的大型语言模型评估标准过于单一，缺乏教育学视角下的科学支撑，因此亟需更全面、理论驱动的评估体系。

Method: 作者结合教育评估理论，从知识、技能、态度三个层面构建评价标准，设计了超12.4万题的多角色、多科目大规模基准题库。其中知识取自权威资源，技能采用情境化分层设计，态度引入对齐一致性和抗欺骗检测机制。

Result: 对七种前沿大模型测试表明，各模型在不同维度表现显著不同：如Claude-Opus-4.5技能表现突出但知识稍弱，Grok-4.1-fast知识强但对齐表现一般，没有单一模型在全部维度占优。

Conclusion: OpenLearnLM作为开放、全面的评估框架，有助于推动LLM在真实教育场景下的能力发展和部署，通过多维度评估揭示模型优势与短板，促进教育应用落地。

Abstract: Large Language Models are increasingly deployed as educational tools, yet existing benchmarks focus on narrow skills and lack grounding in learning sciences. We introduce OpenLearnLM Benchmark, a theory-grounded framework evaluating LLMs across three dimensions derived from educational assessment theory: Knowledge (curriculum-aligned content and pedagogical understanding), Skills (scenario-based competencies organized through a four-level center-role-scenario-subscenario hierarchy), and Attitude (alignment consistency and deception resistance). Our benchmark comprises 124K+ items spanning multiple subjects, educational roles, and difficulty levels based on Bloom's taxonomy. The Knowledge domain prioritizes authentic assessment items from established benchmarks, while the Attitude domain adapts Anthropic's Alignment Faking methodology to detect behavioral inconsistency under varying monitoring conditions. Evaluation of seven frontier models reveals distinct capability profiles: Claude-Opus-4.5 excels in practical skills despite lower content knowledge, while Grok-4.1-fast leads in knowledge but shows alignment concerns. Notably, no single model dominates all dimensions, validating the necessity of multi-axis evaluation. OpenLearnLM provides an open, comprehensive framework for advancing LLM readiness in authentic educational contexts.

</details>


### [425] [Confident Rankings with Fewer Items: Adaptive LLM Evaluation with Continuous Scores](https://arxiv.org/abs/2601.13885)
*Esma Balkır,Alice Pernthaller,Marco Basaldella,José Hernández-Orallo,Nigel Collier*

Main category: cs.CL

TL;DR: 本文扩展了基于IRT的自适应测试方法，使其可以用于连续得分的生成式任务，并提出了不确定性感知排名器, 有效减少评测数量和成本，实现高效、可靠的大模型排序。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM评测多选题用CAT表现良好，但对于生成式任务，模型输出是连续分数而非对错，缺乏高效评价方法。需要新的方法能在保证评测准确性前提下，降低评测成本和题目数量。

Method: 将原先IRT的伯努利响应分布更换为异方差正态分布，以适应ROUGE、BLEU等连续分数。提出不确定性感知的自适应测试排序算法，引入自适应终止准则。并在多种生成式评测基准上进行验证。

Result: 新方法仅用2%的样本即可达到比随机采样高0.12 τ 的排名相关性；在有置信度的预测上有95%的准确率。在n-gram、嵌入以及LLM-judge类型基准上均有效。

Conclusion: 本文方法能够极大提高生成任务中LLM评测效率和可靠性，降低评测成本，适用于多种连续评分场景。

Abstract: Computerized Adaptive Testing (CAT) has proven effective for efficient LLM evaluation on multiple-choice benchmarks, but modern LLM evaluation increasingly relies on generation tasks where outputs are scored continuously rather than marked correct/incorrect. We present a principled extension of IRT-based adaptive testing to continuous bounded scores (ROUGE, BLEU, LLM-as-a-Judge) by replacing the Bernoulli response distribution with a heteroskedastic normal distribution. Building on this, we introduce an uncertainty aware ranker with adaptive stopping criteria that achieves reliable model ranking while testing as few items and as cheaply as possible. We validate our method on five benchmarks spanning n-gram-based, embedding-based, and LLM-as-judge metrics. Our method uses 2% of the items while improving ranking correlation by 0.12 τ over random sampling, with 95% accuracy on confident predictions.

</details>


### [426] [AgentEHR: Advancing Autonomous Clinical Decision-Making via Retrospective Summarization](https://arxiv.org/abs/2601.13918)
*Yusheng Liao,Chuan Xuan,Yutong Cai,Lina Yang,Zhe Chen,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 提出了AgentEHR基准，挑战大模型在复杂EHR场景下实现自主诊疗决策；并提出RetroSum框架，有效提升模型性能与决策连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有大模型医用场景多依赖理想化、简化的EHR任务，实际复杂环境下模型难以可靠导航和推理，信息丢失严重。需要新方法弥合实验与真实临床环境的差距。

Method: 提出AgentEHR基准，涉及诊断、治疗等需要长程推理的复杂任务。针对现有摘要法信息流失，创新性地提出RetroSum框架：结合回溯式动态摘要和经历策略，通过反复回顾交互历史和“经验记忆库”，保证信息完整性和逻辑衔接。

Result: 实验证明，RetroSum在多个评测中较强基线性能提升最高可达29.16%，同时交互错误数降低最高92.3%。

Conclusion: RetroSum框架能大幅提升大模型在真实高噪EHR环境下自主推理的能力，为医学AI落地临床奠定基础。

Abstract: Large Language Models have demonstrated profound utility in the medical domain. However, their application to autonomous Electronic Health Records~(EHRs) navigation remains constrained by a reliance on curated inputs and simplified retrieval tasks. To bridge the gap between idealized experimental settings and realistic clinical environments, we present AgentEHR. This benchmark challenges agents to execute complex decision-making tasks, such as diagnosis and treatment planning, requiring long-range interactive reasoning directly within raw and high-noise databases. In tackling these tasks, we identify that existing summarization methods inevitably suffer from critical information loss and fractured reasoning continuity. To address this, we propose RetroSum, a novel framework that unifies a retrospective summarization mechanism with an evolving experience strategy. By dynamically re-evaluating interaction history, the retrospective mechanism prevents long-context information loss and ensures unbroken logical coherence. Additionally, the evolving strategy bridges the domain gap by retrieving accumulated experience from a memory bank. Extensive empirical evaluations demonstrate that RetroSum achieves performance gains of up to 29.16% over competitive baselines, while significantly decreasing total interaction errors by up to 92.3%.

</details>


### [427] [HyperWalker: Dynamic Hypergraph-Based Deep Diagnosis for Multi-Hop Clinical Modeling across EHR and X-Ray in Medical VLMs](https://arxiv.org/abs/2601.13919)
*Yuezhe Yang,Hao Wang,Yige Peng,Jinman Kim,Lei Bi*

Main category: cs.CL

TL;DR: 本论文提出了HyperWalker框架，通过动态高阶图和测试时训练机制，将电子健康记录（EHR）与多模态数据整合，提升自动化临床诊断的推理能力，并在主流医学报告生成与视觉问答任务上取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 当前医学视觉-语言模型在医学报告生成和视觉问答等任务中进展显著，但普遍仅独立处理单个样本，缺少对患者历史电子健康档案(EHR)及结构性相关病例的利用。这限制了模型外部医学证据的结合，可能影响诊断准确率。为突破该局限性，作者希望通过引入结构复杂的EHR及相关病例，提高AI模型的临床推理深度与诊断能力。

Method: 提出了HyperWalker的深度诊断框架，包括：（1）构建动态高阶图（iBrochure），捕捉EHR数据异质结构和多模态信息之间的高阶关联；（2）利用强化学习智能体（Walker）在高阶图中寻找最优诊断路径；（3）设计了“停留机制”，实现多跳、正交检索邻域病例，充分覆盖与测试样本互补的临床特征。

Result: 该方法在MIMIC数据集上的医学报告生成任务和EHRXQA数据集上的医学视觉问答任务上，均取得了当前最优表现（SOTA）。

Conclusion: HyperWalker通过整合EHR高阶结构关系和动态诊断路径选择，显著提升了医学AI模型的推理表现，为临床自动智能诊断带来新突破。

Abstract: Automated clinical diagnosis remains a core challenge in medical AI, which usually requires models to integrate multi-modal data and reason across complex, case-specific contexts. Although recent methods have advanced medical report generation (MRG) and visual question answering (VQA) with medical vision-language models (VLMs), these methods, however, predominantly operate under a sample-isolated inference paradigm, as such processing cases independently without access to longitudinal electronic health records (EHRs) or structurally related patient examples. This paradigm limits reasoning to image-derived information alone, which ignores external complementary medical evidence for potentially more accurate diagnosis. To overcome this limitation, we propose \textbf{HyperWalker}, a \textit{Deep Diagnosis} framework that reformulates clinical reasoning via dynamic hypergraphs and test-time training. First, we construct a dynamic hypergraph, termed \textbf{iBrochure}, to model the structural heterogeneity of EHR data and implicit high-order associations among multimodal clinical information. Within this hypergraph, a reinforcement learning agent, \textbf{Walker}, navigates to and identifies optimal diagnostic paths. To ensure comprehensive coverage of diverse clinical characteristics in test samples, we incorporate a \textit{linger mechanism}, a multi-hop orthogonal retrieval strategy that iteratively selects clinically complementary neighborhood cases reflecting distinct clinical attributes. Experiments on MRG with MIMIC and medical VQA on EHRXQA demonstrate that HyperWalker achieves state-of-the-art performance. Code is available at: https://github.com/Bean-Young/HyperWalker

</details>


### [428] [Automatic Prompt Optimization for Dataset-Level Feature Discovery](https://arxiv.org/abs/2601.13922)
*Adrian Cosma,Oleg Szehr,David Kletz,Alessandro Antonucci,Olivier Pelletier*

Main category: cs.CL

TL;DR: 提出了一种多智能体的提示优化框架，实现从无结构文本自动发现全局、可解释与判别性强的特征，用于下游分类任务。


<details>
  <summary>Details</summary>
Motivation: 当前从文本中提取特征的方法严重依赖手工编写的提示词或预设的特征集合，限制了自动化与泛化能力。因此，亟需一种能自动在整个数据集层面发现高质量特征的机制，优化下游模型表现。

Method: 作者将特征发现建模为一个数据集层面的提示优化问题。通过多智能体的框架，多个语言模型agent协作，分别提出特征定义、提取特征值、用数据集级别的判别性与可解释性反馈评估特征质量，不断精炼特征提示，从而共同优化出描述性与判别性都强的全局特征集。

Result: 该方法能够自动发现一组在下游监督学习任务中表现优异、且可解释的特征定义，摆脱了人工指定提示或固定特征模板的束缚。

Conclusion: 多智能体提示优化机制为无结构文本自动特征发现提供了有效途径，既提升了特征的可解释性，又能优化下游分类表现，相较依赖单条示例监督的传统方法更具优势。

Abstract: Feature extraction from unstructured text is a critical step in many downstream classification pipelines, yet current approaches largely rely on hand-crafted prompts or fixed feature schemas. We formulate feature discovery as a dataset-level prompt optimization problem: given a labelled text corpus, the goal is to induce a global set of interpretable and discriminative feature definitions whose realizations optimize a downstream supervised learning objective. To this end, we propose a multi-agent prompt optimization framework in which language-model agents jointly propose feature definitions, extract feature values, and evaluate feature quality using dataset-level performance and interpretability feedback. Instruction prompts are iteratively refined based on this structured feedback, enabling optimization over prompts that induce shared feature sets rather than per-example predictions. This formulation departs from prior prompt optimization methods that rely on per-sample supervision and provides a principled mechanism for automatic feature discovery from unstructured text.

</details>


### [429] ["The Whole Is Greater Than the Sum of Its Parts": A Compatibility-Aware Multi-Teacher CoT Distillation Framework](https://arxiv.org/abs/2601.13992)
*Jin Cui,Jiaqi Guo,Jiepeng Zhou,Ruixuan Yang,Jiayi Lu,Jiajun Xu,Jiangcheng Song,Boran Zhao,Pengju Ren*

Main category: cs.CL

TL;DR: 本文提出COMPACT框架，动态融合多位教师模型对学生模型（SLM）的监督，以提升小模型的推理能力，同时克服以往方法中教师单一和灾难性遗忘的问题，最终在多个基准测试中取得了先进效果。


<details>
  <summary>Details</summary>
Motivation: 大模型通过链式思维（CoT）展现出强大推理能力，但部署成本高。用较小的学生模型继承这些能力对于实际落地意义重大。已有CoT蒸馏方法大多依赖单一教师，但教师自身能力有限，并可能导致知识遗忘，因此需要更有效的多教师知识融合方式。

Method: 作者提出COMPACT框架，从多位教师处蒸馏推理能力，并通过三种动态权重机制融合不同教师的监督：(1) 基于图的共识过滤，筛除误导性推理路径；(2) 基于互信息的适应性检测，捕捉学生对推理真正理解的“顿悟时刻”；(3) 基于loss的难度评估，预防负迁移并优化教师指导。

Result: 实验结果和潜空间分析表明，COMPACT框架能有效融合多位教师的推理优势，不损伤原有知识结构，在多个推理基准任务上取得了新的最佳表现，并缓解了灾难性遗忘。

Conclusion: COMPACT为知识蒸馏提供了一种动态、鲁棒的多教师融合途径，显著提升了小型学生模型的推理能力，有助于高效推广LLM推理优势到更经济实用的模型。

Abstract: Chain-of-Thought (CoT) reasoning empowers Large Language Models (LLMs) with remarkable capabilities but typically requires prohibitive parameter scales. CoT distillation has emerged as a promising paradigm to transfer reasoning prowess into compact Student Models (SLMs), but existing approaches often rely on a solitary teacher, capping the student's potential since individual LLMs often exhibit distinct capability biases and may suffer from catastrophic forgetting. While leveraging diverse teachers seems appealing, effectively fusing their supervisions remains challenging: teacher-student incompatibility risks amplifying hallucinations, and passive supervision fails to ensure genuine logic internalization. To address this, we introduce COMPACT, a framework that adaptively fuses supervisions from different teachers by dynamically weighting teacher gradients based on the student's real-time compatibility evaluated by a multi-dimensional metric: (1) Graph-based Consensus to filter misleading rationales by identifying mainstream reasoning paths; (2) Mutual-Information-based Adaptability to detect "epiphany moments" for genuinely understanding the reasoning process rather than merely imitating; and (3) Loss-based Difficulty to assess student receptivity to the teacher's guidance and prevent negative transfer. Extensive experiments and latent space analysis demonstrate that COMPACT effectively integrates diverse reasoning capabilities without damaging the model's original knowledge structure, achieving state-of-the-art performance on various benchmarks while mitigating catastrophic forgetting.

</details>


### [430] [From Tags to Trees: Structuring Fine-Grained Knowledge for Controllable Data Selection in LLM Instruction Tuning](https://arxiv.org/abs/2601.13995)
*Zihan Niu,Wenping Hu,Junmin Chen,Xiyue Wang,Tong Xu,Ruiming Tang*

Main category: cs.CL

TL;DR: 本文提出了一种名为TAGS的树感知全局采样框架，通过对知识的细粒度标签构建知识树，实现对数据质量、多样性和目标对齐的联合控制。实验结果显示，TAGS能以仅5%的数据实现比全量模型高5.84%的表现。


<details>
  <summary>Details</summary>
Motivation: 目前大模型指令微调数据选取多依赖实例级别的质量分或基于embedding聚类/语义标签的多样性指标，这些方法忽略了知识之间的细粒度层级依赖，难以做到精准估价和按知识结构采样。

Method: 利用大模型生成的知识原子标签，通过层次聚类方式构建知识树，然后将数据实例映射到知识树节点，基于此定义树感知的质量与多样性指标，实现能全局与细粒度受控的数据采样策略。包括最大化知识树的信息增益，并用KL散度进行领域对齐采样。

Result: 在大规模实验中，TAGS显著超越了现有方法，并且在仅用5%数据时表现超过用全部数据训练的模型5.84%；进一步，特定对齐采样策略还能带来4.24%的额外提升。

Conclusion: 树感知的数据选择和采样方法能同时优化大模型微调的数据质量、多样性和知识对齐，极大地提升了数据利用效率和模型性能，对比主流做法具有明显优势。

Abstract: Effective and controllable data selection is critical for LLM instruction tuning, especially with massive open-source datasets. Existing approaches primarily rely on instance-level quality scores, or diversity metrics based on embedding clusters or semantic tags. However, constrained by the flatness of embedding spaces or the coarseness of tags, these approaches overlook fine-grained knowledge and its intrinsic hierarchical dependencies, consequently hindering precise data valuation and knowledge-aligned sampling. To address this challenge, we propose Tree-aware Aligned Global Sampling (TAGS), a unified framework that leverages a knowledge tree built from fine-grained tags, thereby enabling joint control of global quality, diversity, and target alignment. Using an LLM-based tagger, we extract atomic knowledge concepts, which are organized into a global tree through bottom-up hierarchical clustering. By grounding data instances onto this tree, a tree-aware metric then quantifies data quality and diversity, facilitating effective sampling. Our controllable sampling strategy maximizes tree-level information gain and enforces leaf-level alignment via KL-divergence for specific domains. Extensive experiments demonstrate that TAGS significantly outperforms state-of-the-art baselines. Notably, it surpasses the full-dataset model by \textbf{+5.84\%} using only \textbf{5\%} of the data, while our aligned sampling strategy further boosts average performance by \textbf{+4.24\%}.

</details>


### [431] [Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models](https://arxiv.org/abs/2601.14004)
*Hengyuan Zhang,Zhihao Zhang,Mingyang Wang,Zunhai Su,Yiwei Wang,Qianli Wang,Shuzhou Yuan,Ercong Nie,Xufeng Duan,Qibo Xue,Zeping Yu,Chenming Shang,Xiao Liang,Jing Xiong,Hui Shen,Chaofan Tao,Zhengwu Liu,Senjie Jin,Zhiheng Xi,Dongdong Zhang,Sophia Ananiadou,Tao Gui,Ruobing Xie,Hayden Kwok-Hay So,Hinrich Schütze,Xuanjing Huang,Qi Zhang,Ngai Wong*

Main category: cs.CL

TL;DR: 本文提出了一个“定位-引导-改进”（Locate, Steer, and Improve）流程，将Mechanistic Interpretability（MI）转化为可操作的方法论，推动大语言模型（LLMs）的优化。


<details>
  <summary>Details</summary>
Motivation: 当前关于MI的综述大多仅作为观察性科学，即仅总结分析结果，缺乏系统性、可实施的介入框架。本文旨在弥补这一空白，将MI用于实际模型优化。

Method: 将MI中的定位（诊断）、引导（干预）方法按可解释对象系统分类，建立严格的干预流程，并围绕“定位-引导-改进”流程对相关方法与工具进行梳理。

Result: 实践表明，该框架能在提升LLM的对齐性、能力和效率等方面带来实际优化效果。此外，整理并分享了高质量相关文献列表供社区参考。

Conclusion: 通过将MI流程化、系统化，并以实践为导向，本文推动了可操作性MI方法的发展，有助于今后LLM的高效优化和改进。

Abstract: Mechanistic Interpretability (MI) has emerged as a vital approach to demystify the opaque decision-making of Large Language Models (LLMs). However, existing reviews primarily treat MI as an observational science, summarizing analytical insights while lacking a systematic framework for actionable intervention. To bridge this gap, we present a practical survey structured around the pipeline: "Locate, Steer, and Improve." We formally categorize Localizing (diagnosis) and Steering (intervention) methods based on specific Interpretable Objects to establish a rigorous intervention protocol. Furthermore, we demonstrate how this framework enables tangible improvements in Alignment, Capability, and Efficiency, effectively operationalizing MI as an actionable methodology for model optimization. The curated paper list of this work is available at https://github.com/rattlesnakey/Awesome-Actionable-MI-Survey.

</details>


### [432] [BACH-V: Bridging Abstract and Concrete Human-Values in Large Language Models](https://arxiv.org/abs/2601.14007)
*Junyu Zhang,Yipeng Kang,Jiong Guo,Jiayu Zhan,Junqi Wang*

Main category: cs.CL

TL;DR: 本文提出了一个抽象概念理解的分析框架，并在人类价值观维度上实验，发现大模型能够跨抽象与具体之间有效传递价值观，并能通过干预调控模型具体决策表现，但抽象层面保持稳定。


<details>
  <summary>Details</summary>
Motivation: 当前关于大模型是否真正“理解”抽象概念还是仅仅做模式匹配尚无定论，尤其在实现对齐（alignment）时，价值观的抽象理解及其落地至具体决策极为重要。本文旨在系统研究大模型对抽象概念——特别是人类价值观——的理解与应用机制。

Method: 作者提出了“抽象-锚定”三分框架：抽象-抽象(A-A)、抽象-具体(A-C)、具体-具体(C-C)，并以六个开源大模型、十个人类价值观维度为测试对象，采用probe（探测激活内的价值观痕迹）与steer（修改表示以干预行为）方法，分析模型对抽象价值观的表示与具体应用。

Result: （1）Probe 方法显示，模型中仅基于抽象价值观训练的探针，可以在具体场景叙述和推理中跨层检测到对应价值观，（2）Steer 方法显示，操作抽象价值观表征能够有效改变模型的具体判断和决策（A-C, C-C），但无法反向影响其对应的抽象解释（A-A），揭示抽象表征具有锚定与稳定性。

Conclusion: 大模型内部价值观结构层次分明、可传递且操控，特别是抽象价值观作为模型决策的稳定锚点。这为进一步打造更加透明、泛化和可控、对齐的人机价值观AI系统，提供了机制和方法论基础。

Abstract: Do large language models (LLMs) genuinely understand abstract concepts, or merely manipulate them as statistical patterns? We introduce an abstraction-grounding framework that decomposes conceptual understanding into three capacities: interpretation of abstract concepts (Abstract-Abstract, A-A), grounding of abstractions in concrete events (Abstract-Concrete, A-C), and application of abstract principles to regulate concrete decisions (Concrete-Concrete, C-C). Using human values as a testbed - given their semantic richness and centrality to alignment - we employ probing (detecting value traces in internal activations) and steering (modifying representations to shift behavior). Across six open-source LLMs and ten value dimensions, probing shows that diagnostic probes trained solely on abstract value descriptions reliably detect the same values in concrete event narratives and decision reasoning, demonstrating cross-level transfer. Steering reveals an asymmetry: intervening on value representations causally shifts concrete judgments and decisions (A-C, C-C), yet leaves abstract interpretations unchanged (A-A), suggesting that encoded abstract values function as stable anchors rather than malleable activations. These findings indicate LLMs maintain structured value representations that bridge abstraction and action, providing a mechanistic and operational foundation for building value-driven autonomous AI systems with more transparent, generalizable alignment and control.

</details>


### [433] [RM-Distiller: Exploiting Generative LLM for Reward Model Distillation](https://arxiv.org/abs/2601.14032)
*Hongli Zhou,Hui Huang,Wei Liu,Chenglong Wang,Xingyuan Bu,Lvyuan Han,Fuhai Song,Muyun Yang,Wenhao Jiang,Hailong Cao,Tiejun Zhao*

Main category: cs.CL

TL;DR: 提出了一种新的奖励模型蒸馏框架RM-Distiller，从生成式大模型中系统性挖掘教师模型多元能力，有效提升了奖励模型对人类偏好的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 当前利用生成式大模型蒸馏奖励模型时，往往只将教师模型视为简单的二元标注器，未能充分利用其丰富知识与能力，导致奖励模型对人类偏好对齐不充分。

Method: RM-Distiller框架综合利用教师模型的三种能力：1）精炼能力，生成高度相关的响应对以提供更细粒度、对比性更强的信号；2）评分能力，通过边际感知的优化目标，帮助奖励模型捕捉更精准的偏好强度；3）生成能力，引入教师模型分布以正则化奖励模型，保留其基本语言能力。

Result: 通过大量实验验证，RM-Distiller在奖励模型基准和基于强化学习的对齐任务上均显著优于传统蒸馏方法。

Conclusion: 系统性挖掘和利用生成式大模型的多元能力对于提升奖励模型效果至关重要，RM-Distiller作为首个系统化蒸馏方法，为后续相关研究提供了思路。

Abstract: Reward models (RMs) play a pivotal role in aligning large language models (LLMs) with human preferences. Due to the difficulty of obtaining high-quality human preference annotations, distilling preferences from generative LLMs has emerged as a standard practice. However, existing approaches predominantly treat teacher models as simple binary annotators, failing to fully exploit the rich knowledge and capabilities for RM distillation. To address this, we propose RM-Distiller, a framework designed to systematically exploit the multifaceted capabilities of teacher LLMs: (1) Refinement capability, which synthesizes highly correlated response pairs to create fine-grained and contrastive signals. (2) Scoring capability, which guides the RM in capturing precise preference strength via a margin-aware optimization objective. (3) Generation capability, which incorporates the teacher's generative distribution to regularize the RM to preserve its fundamental linguistic knowledge. Extensive experiments demonstrate that RM-Distiller significantly outperforms traditional distillation methods both on RM benchmarks and reinforcement learning-based alignment, proving that exploiting multifaceted teacher capabilities is critical for effective reward modeling. To the best of our knowledge, this is the first systematic research on RM distillation from generative LLMs.

</details>


### [434] [Top 10 Open Challenges Steering the Future of Diffusion Language Model and Its Variants](https://arxiv.org/abs/2601.14041)
*Yunhe Wang,Kai Han,Huiling Zhen,Yuchuan Tian,Hanting Chen,Yongbing Huang,Yufei Cui,Yingte Shu,Shan Gao,Ismail Elezi,Roy Vaughan Miles,Songcen Xu,Feng Wen,Chao Xu,Sinan Zeng,Dacheng Tao*

Main category: cs.CL

TL;DR: 本文探讨了扩散语言模型（DLMs）作为大语言模型（LLMs）迭代的新范式，对比传统自回归（AR）架构，指出其现存挑战并提出四大战略支柱推动发展。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs主要依赖自回归架构，虽然表现强大，但受因果约束，缺乏全局结构洞察和自我修正能力。作者希望通过DLMs突破这些限制，推动AI更高水平的结构化推理和多模态融合能力。

Method: 分析了DLMs的现状与困境，总结了阻碍其发展的十个核心问题，并提出发展路线，包括基础设施、算法优化、认知推理和多模态智能四个方面，提出多尺度分词、主动重掩码和潜在思考等方向。

Result: 本文没有展开实验结果，而是系统性分析了DLMs面临的挑战，明确指出DLMs只有摆脱自回归遗产、构建原生扩散生态，才能发挥应有潜能。

Conclusion: 要实现AI在复杂结构推理、自我动态校正和多模态整合上的飞跃，需推动DLMs技术体系的根本变革，发展原生扩散范式，突破现有自回归模型的瓶颈。

Abstract: The paradigm of Large Language Models (LLMs) is currently defined by auto-regressive (AR) architectures, which generate text through a sequential ``brick-by-brick'' process. Despite their success, AR models are inherently constrained by a causal bottleneck that limits global structural foresight and iterative refinement. Diffusion Language Models (DLMs) offer a transformative alternative, conceptualizing text generation as a holistic, bidirectional denoising process akin to a sculptor refining a masterpiece. However, the potential of DLMs remains largely untapped as they are frequently confined within AR-legacy infrastructures and optimization frameworks. In this Perspective, we identify ten fundamental challenges ranging from architectural inertia and gradient sparsity to the limitations of linear reasoning that prevent DLMs from reaching their ``GPT-4 moment''. We propose a strategic roadmap organized into four pillars: foundational infrastructure, algorithmic optimization, cognitive reasoning, and unified multimodal intelligence. By shifting toward a diffusion-native ecosystem characterized by multi-scale tokenization, active remasking, and latent thinking, we can move beyond the constraints of the causal horizon. We argue that this transition is essential for developing next-generation AI capable of complex structural reasoning, dynamic self-correction, and seamless multimodal integration.

</details>


### [435] [PRiSM: Benchmarking Phone Realization in Speech Models](https://arxiv.org/abs/2601.14046)
*Shikhar Bharadwaj,Chin-Jou Li,Yoonjae Kim,Kwanghee Choi,Eunjung Yeo,Ryan Soh-Eun Shim,Hanyu Zhou,Brendon Boldt,Karen Rosero Jacome,Kalvin Chang,Darsh Agrawal,Keer Xu,Chao-Han Huck Yang,Jian Zhu,Shinji Watanabe,David R. Mortensen*

Main category: cs.CL

TL;DR: 本文介绍了PRiSM，这是首个旨在多维度评估音素识别系统（PR）能力的开源基准工具，着重揭示当前系统在语言无关处理和实际应用中的盲点。


<details>
  <summary>Details</summary>
Motivation: 当前音素识别系统的评测仅关注转录准确率，忽视了系统在真实多语、多场景（如临床、教育）中的实际效用和表现盲点；缺少多维、标准化的评测工具。

Method: 提出PRiSM基准，包含内外在评测方法，通过标准化的转录评估和多种下游应用场景的能力探测，系统考查PR系统的表征和泛化能力；同时发布相关代码和数据集。

Result: 研究发现，训练时多样化语言输入对提高PR系统性能至关重要；Encoder-CTC结构最稳定，且专门的音素识别模型依然优于大型音频语言模型。

Conclusion: PRiSM填补了音素识别系统多维评测的空白，推动了鲁棒多语音素建模的发展，为后续研究提供基准、工具和数据支撑。

Abstract: Phone recognition (PR) serves as the atomic interface for language-agnostic modeling for cross-lingual speech processing and phonetic analysis. Despite prolonged efforts in developing PR systems, current evaluations only measure surface-level transcription accuracy. We introduce PRiSM, the first open-source benchmark designed to expose blind spots in phonetic perception through intrinsic and extrinsic evaluation of PR systems. PRiSM standardizes transcription-based evaluation and assesses downstream utility in clinical, educational, and multilingual settings with transcription and representation probes. We find that diverse language exposure during training is key to PR performance, encoder-CTC models are the most stable, and specialized PR models still outperform Large Audio Language Models. PRiSM releases code, recipes, and datasets to move the field toward multilingual speech models with robust phonetic ability: https://github.com/changelinglab/prism.

</details>


### [436] [Understanding Multilingualism in Mixture-of-Experts LLMs: Routing Mechanism, Expert Specialization, and Layerwise Steering](https://arxiv.org/abs/2601.14050)
*Yuxin Chen,Zhengzhou Cai,Xiangtian Ji,Weixiang Zhao,An Zhang,Xiang Wang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 本文系统性分析多专家模型（MoE）在多语言任务中的表现机制，发现其路由和专家分配高度结构化，并提出一种路由引导方法提升多语言性能。


<details>
  <summary>Details</summary>
Motivation: 虽然MoE架构在多语言任务中表现出强大能力，但其内部工作机制、尤其是跨语言的具体差异还未被充分理解。

Method: 作者系统性分析了MoE模型在多语言和不同网络层次中的路由行为与专家专化，包括专家利用率、不同语族间的路由行为、以及层间的功能差异，并用层级干预实验进一步刻画各层次的语言处理模式。最后，作者提出了一种在推理时引导中间层路由向主导语言共享专家的自适应方法。

Result: 分析结果表明，MoE模型的路由机制和专家分配与语言家族紧密相关，高资源语言依赖共享专家，低资源语言更多依赖独占专家且表现较弱，不同层对语言特异性和通用性的贡献不同。提出的路由引导方法能稳定提升多语言性能，特别对相关性强的语种组合效果显著。

Conclusion: MoE模型在多语言处理中的行为高度结构化，理解这些差异有助于优化模型设计。路由引导措施能有效提升多语言表现，为后续多语言MoE模型发展提供新思路。

Abstract: Mixture-of-Experts (MoE) architectures have shown strong multilingual capabilities, yet the internal mechanisms underlying performance gains and cross-language differences remain insufficiently understood. In this work, we conduct a systematic analysis of MoE models, examining routing behavior and expert specialization across languages and network depth. Our analysis reveals that multilingual processing in MoE models is highly structured: routing aligns with linguistic families, expert utilization follows a clear layerwise pattern, and high-resource languages rely on shared experts while low-resource languages depend more on language-exclusive experts despite weaker performance. Layerwise interventions further show that early and late MoE layers support language-specific processing, whereas middle layers serve as language-agnostic capacity hubs. Building on these insights, we propose a routing-guided steering method that adaptively guides routing behavior in middle layers toward shared experts associated with dominant languages at inference time, leading to consistent multilingual performance improvements, particularly for linguistically related language pairs. Our code is available at https://github.com/conctsai/Multilingualism-in-Mixture-of-Experts-LLMs.

</details>


### [437] [Kakugo: Distillation of Low-Resource Languages into Small Language Models](https://arxiv.org/abs/2601.14051)
*Peter Devine,Mardhiyah Sanni,Farid Adilazuarda,Julieta Gil Loizaga,Barry Haddow*

Main category: cs.CL

TL;DR: Kakugo是一种为低资源语言训练小型语言模型（SLM）的新型低成本流程，仅需语言名称即可生成训练数据并进行模型训练，针对54种低资源语言显著提升了表现。


<details>
  <summary>Details</summary>
Motivation: 目前低资源语言（资源有限、数据稀缺）在AI领域中的支持严重不足。高质量的训练数据获取困难且成本高昂，导致低资源语言社区难以拥有专属适用的语言模型。

Method: Kakugo流程以大规模教师模型为支撑，通过自动生成合成提示和翻译指令数据集，为目标低资源语言制作训练数据。这一做法仅需提供目标语言的名称，无需其他语言资源。

Result: 针对54种低资源语言，小型语言模型在翻译、分类和问答等多项通用自然语言处理任务中，经Kakugo流程训练后均超越了基础模型的表现。

Conclusion: Kakugo极大地降低了低资源语言AI模型的训练门槛和成本（每种语言低于50美元），为不同语言社区自主开发专属AI提供了可行、经济的方法。

Abstract: We present Kakugo, a novel and cost-effective pipeline designed to train general-purpose Small Language Models (SLMs) for low-resource languages using only the language name as input. By using a large teacher model to generate synthetic prompts and translate instruction datasets, we produced training data and SLMs for 54 low-resource languages. Evaluations across a diverse set of general natural language processing tasks, including translation, classification, and question answering, demonstrate that our pipeline consistently improves performance over base models. With a total generation and training cost of under $50 per language, Kakugo offers an accessible method for communities to develop language-specific AI.

</details>


### [438] [XCR-Bench: A Multi-Task Benchmark for Evaluating Cultural Reasoning in LLMs](https://arxiv.org/abs/2601.14063)
*Mohsinul Kabir,Tasnim Ahmed,Md Mezbaur Rahman,Shaoxiong Ji,Hassan Alhuzali,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 该论文提出了一个全新的跨文化推理基准XCR-Bench，用于评估大语言模型在跨文化语境下识别和调整文化特定元素（CSI）的能力，并发现目前主流LLMs在社会礼仪和文化参考等领域表现不足，且存在地区和宗教偏见。


<details>
  <summary>Details</summary>
Motivation: 现有对于大语言模型跨文化能力的评估受限于缺乏高质量、带有CSI注释的平行语料库，难以系统全面地分析和改进模型的跨文化推理能力。因此，作者希望通过建设新的数据集和评测基准，推动该领域进展。

Method: 作者将Newmark的CSI框架与Hall的文化三元组理论结合，构建了包含4,900个平行句、1,098种独特CSI、涉及三类逻辑推理任务的数据集，并制定了相应评测方式；对主流LLM进行了系统实验分析，并评估其在不同文化维度下的表现。

Result: 实验发现，当前先进LLMs在识别、调整与社会礼仪和文化参照相关的CSI上普遍效果较差。同时，模型在同一语言环境下进行文化适配时会表现出地区和种族-宗教偏见。

Conclusion: 当前LLMs在跨文化语境下处理 CSI 任务仍存在显著弱点。作者所公布的数据集和代码有助于NLP领域进一步研究和优化跨文化能力，推动更公平、准确的AI系统。

Abstract: Cross-cultural competence in large language models (LLMs) requires the ability to identify Culture-Specific Items (CSIs) and to adapt them appropriately across cultural contexts. Progress in evaluating this capability has been constrained by the scarcity of high-quality CSI-annotated corpora with parallel cross-cultural sentence pairs. To address this limitation, we introduce XCR-Bench, a Cross(X)-Cultural Reasoning Benchmark consisting of 4.9k parallel sentences and 1,098 unique CSIs, spanning three distinct reasoning tasks with corresponding evaluation metrics. Our corpus integrates Newmark's CSI framework with Hall's Triad of Culture, enabling systematic analysis of cultural reasoning beyond surface-level artifacts and into semi-visible and invisible cultural elements such as social norms, beliefs, and values. Our findings show that state-of-the-art LLMs exhibit consistent weaknesses in identifying and adapting CSIs related to social etiquette and cultural reference. Additionally, we find evidence that LLMs encode regional and ethno-religious biases even within a single linguistic setting during cultural adaptation. We release our corpus and code to facilitate future research on cross-cultural NLP.

</details>


### [439] [Truth with a Twist: The Rhetoric of Persuasion in Professional vs. Community-Authored Fact-Checks](https://arxiv.org/abs/2601.14105)
*Olesya Razuvayevskaya,Kalina Bontcheva*

Main category: cs.CL

TL;DR: 本研究大规模比对了群众撰写与专业机构撰写的辟谣内容中的说服技巧差异。结果发现群众产出内容并不比专业内容更具说服性，但在修辞风格上有系统性区别。


<details>
  <summary>Details</summary>
Motivation: 此前普遍认为群众撰写的辟谣内容更依赖主观或说服性用语，然而缺乏定量分析支持。作者希望明确不同来源辟谣内容在说服技巧上的实际差异。

Method: 作者利用Community Notes、EUvsDisinfo和Known Fakes数据库，统计并量化各数据库辟谣内容中的说服技巧数量与类型；对比群众与专业内容；还分析了群众对说服性语言的评价方式。

Result: 实证数据表明，社区型辟谣内容（如CNs）和专业辟谣内容在单位说服性技巧数量上无显著差异；二者在辩证手法与主题覆盖上存有系统性区别。群众对使用某些问题性修辞的内容表现出惩罚倾向。

Conclusion: 群众撰写的辟谣内容并未比专业内容更具说服性，但两者有风格与规范上的区别。群体在评判说服元素时具有辨析力，对部分有争议的修辞手法持批评态度。

Abstract: This study presents the first large-scale comparison of persuasion techniques present in crowd- versus professionally-written debunks. Using extensive datasets from Community Notes (CNs), EUvsDisinfo, and the Database of Known Fakes (DBKF), we quantify the prevalence and types of persuasion techniques across these fact-checking ecosystems. Contrary to prior hypothesis that community-produced debunks rely more heavily on subjective or persuasive wording, we find no evidence that CNs contain a higher average number of persuasion techniques than professional fact-checks. We additionally identify systematic rhetorical differences between CNs and professional debunking efforts, reflecting differences in institutional norms and topical coverage. Finally, we examine how the crowd evaluates persuasive language in CNs and show that, although notes with more persuasive elements receive slightly higher overall helpfulness ratings, crowd raters are effective at penalising the use of particular problematic rhetorical means

</details>


### [440] [Learning to Explain: Supervised Token Attribution from Transformer Attention Patterns](https://arxiv.org/abs/2601.14112)
*George Mihaila*

Main category: cs.CL

TL;DR: 本文提出了一种新的可解释性人工智能方法ExpNet，通过深度学习自动从transformer注意力机制中学习令牌级重要性得分，无需依赖手工聚合策略和固定归因规则，并在多任务场景下对其有效性进行评测。


<details>
  <summary>Details</summary>
Motivation: 目前Transformers的可解释性方法多依赖于手工定义的注意力聚合策略以及固定的归因规则，导致泛化性有限和灵活性不足，而模型无关的方法（如LIME、SHAP）则计算开销极大，效率低下。因而需要一种高效、自动化、能发现更优特征组合的注意力解释方法。

Method: 作者提出了ExpNet，一种轻量级神经网络模型，可以自动从transformer注意力模式映射到token级别的重要性评分，无需人为设定聚合或归因规则。主要思路是通过端到端学习找出最佳的注意力特征组合，实现快速、自动的解释生成。

Result: 在跨任务设置下，ExpNet与多种模型无关方法和传统注意力方法进行了对比，覆盖了四类主流方法学体系，据称具有良好的泛化表现及效率优势。

Conclusion: ExpNet能够高效、自动地从transformer注意力信息中提炼解释，克服了传统方法的人为规则依赖和模型无关方法的高昂计算成本，在多任务、多方法对比中表现优异，展示了其作为可解释性工具的潜力。

Abstract: Explainable AI (XAI) has become critical as transformer-based models are deployed in high-stakes applications including healthcare, legal systems, and financial services, where opacity hinders trust and accountability. Transformers self-attention mechanisms have proven valuable for model interpretability, with attention weights successfully used to understand model focus and behavior (Xu et al., 2015); (Wiegreffe and Pinter, 2019). However, existing attention-based explanation methods rely on manually defined aggregation strategies and fixed attribution rules (Abnar and Zuidema, 2020a); (Chefer et al., 2021), while model-agnostic approaches (LIME, SHAP) treat the model as a black box and incur significant computational costs through input perturbation. We introduce Explanation Network (ExpNet), a lightweight neural network that learns an explicit mapping from transformer attention patterns to token-level importance scores. Unlike prior methods, ExpNet discovers optimal attention feature combinations automatically rather than relying on predetermined rules. We evaluate ExpNet in a challenging cross-task setting and benchmark it against a broad spectrum of model-agnostic methods and attention-based techniques spanning four methodological families.

</details>


### [441] [NewsRECON: News article REtrieval for image CONtextualization](https://arxiv.org/abs/2601.14121)
*Jonathan Tonglet,Iryna Gurevych,Tinne Tuytelaars,Marie-Francine Moens*

Main category: cs.CL

TL;DR: 该论文提出了一种在缺乏反向图片搜索（RIS）证据时，推断新闻图片拍摄时间和地点的新方法NewsRECON。


<details>
  <summary>Details</summary>
Motivation: 现有新闻图片验证工具高度依赖RIS，但RIS经常无法返回结果，导致在许多实际场景下难以应用。因此，亟需一种替代RIS的新方法，帮助新闻从业者和取证专家验证图片信息。

Method: 提出NewsRECON方法，将图片与相关新闻文章相连接，通过文章的元数据推断图片的拍摄时间和地点。NewsRECON包括：使用bi-encoder检索事件相关的文章；利用两个cross-encoder分别对地点和事件一致性进行重排序。实验数据集覆盖超过9万篇新闻文章。

Result: 在TARA和5Pils-OOC数据集上的实验表明，NewsRECON优于现有方法。在RIS证据不可用的情况下，还能与多模态大模型结合，达到新的SOTA水平。

Conclusion: NewsRECON为新闻图片取证提供了一种无需RIS证据的有效解决方案，提升了图片来源验证的可靠性，对新闻和法医领域有实际应用价值。

Abstract: Identifying when and where a news image was taken is crucial for journalists and forensic experts to produce credible stories and debunk misinformation. While many existing methods rely on reverse image search (RIS) engines, these tools often fail to return results, thereby limiting their practical applicability. In this work, we address the challenging scenario where RIS evidence is unavailable. We introduce NewsRECON, a method that links images to relevant news articles to infer their date and location from article metadata. NewsRECON leverages a corpus of over 90,000 articles and integrates: (1) a bi-encoder for retrieving event-relevant articles; (2) two cross-encoders for reranking articles by location and event consistency. Experiments on the TARA and 5Pils-OOC show that NewsRECON outperforms prior work and can be combined with a multimodal large language model to achieve new SOTA results in the absence of RIS evidence. We make our code available.

</details>


### [442] [A Systematic Analysis of Chunking Strategies for Reliable Question Answering](https://arxiv.org/abs/2601.14123)
*Sofia Bennani,Charles Moslonka*

Main category: cs.CL

TL;DR: 本文系统分析了文档切分方式对RAG系统检索和生成效果的影响，提出了针对工业实践的切分、大小和上下文长度建议。


<details>
  <summary>Details</summary>
Motivation: 现有工业界对RAG系统中文档切分多依赖经验性做法，缺乏系统性分析。本研究希望找出更有效和成本优化的切分策略，提高RAG系统的性能与应用效率。

Method: 作者在Natural Questions数据集上，采用SPLADE作为检索器，Mistral-8B为生成器，系统性地考察了不同切分方法（按token、句子、语义、代码）、切块大小、重叠比例和上下文长度对效果的影响。

Result: 1. 重叠切分未带来性能提升，反而增加索引成本。2. 句子切分在成本与效果上最优，最大可与语义切分媲美（~5k tokens）。3. 上下文长度超过约2.5k token后，系统性能出现“断崖式”下跌。4. 不同任务目标对最优上下文长度有依赖，语义相关性在短上下文表现更佳，精确匹配在长上下文表现更好。

Conclusion: 文档切分需根据具体应用目标灵活选取切分方式与上下文长度。推荐在工业RAG系统中使用无重叠的句子切分，控制上下文在2.5k tokens以内以在性能与成本间取得最佳平衡。

Abstract: We study how document chunking choices impact the reliability of Retrieval-Augmented Generation (RAG) systems in industry. While practice often relies on heuristics, our end-to-end evaluation on Natural Questions systematically varies chunking method (token, sentence, semantic, code), chunk size, overlap, and context length. We use a standard industrial setup: SPLADE retrieval and a Mistral-8B generator. We derive actionable lessons for cost-efficient deployment: (i) overlap provides no measurable benefit and increases indexing cost; (ii) sentence chunking is the most cost-effective method, matching semantic chunking up to ~5k tokens; (iii) a "context cliff" reduces quality beyond ~2.5k tokens; and (iv) optimal context depends on the goal (semantic quality peaks at small contexts; exact match at larger ones).

</details>


### [443] [Style Transfer as Bias Mitigation: Diffusion Models for Synthetic Mental Health Text for Arabic](https://arxiv.org/abs/2601.14124)
*Saad Mankarious,Aya Zirikly*

Main category: cs.CL

TL;DR: 本文提出了一种无需预训练大模型、基于扩散模型的合成文本生成方法，实现阿拉伯语心理健康语料中的性别风格迁移，提升女性作者内容的多样性与数量，缓解性别偏见。


<details>
  <summary>Details</summary>
Motivation: 现有精神健康分析中，真实数据稀缺和性别偏见（如女性文本不足）问题显著。以往合成数据方法依赖大型预训练语言模型，输出多样性和固有偏见难以避免，因此需探索新的方法以提升多样性并有效削弱这些偏见。

Method: 将偏见消除问题视为风格迁移，通过CARMA语料库构建表现不同性别特征的五套数据集，分别训练扩散模型进行“男性到女性”风格转换，生成多样、高保真的女性文本。定量评价关注语义保真度与风格多样性，定性分析验证性别特征转化的语言合理性。

Result: 实验结果显示，扩散模型生成的文本在保持原始语义一致性的同时，有效实现了性别风格的转化，且具有高表面多样性；与LLM方法相比，能产生更高熵、更灵活、非模板化的合成文本。

Conclusion: 基于扩散模型的风格迁移为低资源、敏感领域的数据多样化与偏见缓解提供了有效工具，无需依赖LLM，尤其适合心理健康等对公正性和多样性要求严格的场景。

Abstract: Synthetic data offers a promising solution for mitigating data scarcity and demographic bias in mental health analysis, yet existing approaches largely rely on pretrained large language models (LLMs), which may suffer from limited output diversity and propagate biases inherited from their training data. In this work, we propose a pretraining-free diffusion-based approach for synthetic text generation that frames bias mitigation as a style transfer problem. Using the CARMA Arabic mental health corpus, which exhibits a substantial gender imbalance, we focus on male-to-female style transfer to augment underrepresented female-authored content. We construct five datasets capturing varying linguistic and semantic aspects of gender expression in Arabic and train separate diffusion models for each setting. Quantitative evaluations demonstrate consistently high semantic fidelity between source and generated text, alongside meaningful surface-level stylistic divergence, while qualitative analysis confirms linguistically plausible gender transformations. Our results show that diffusion-based style transfer can generate high-entropy, semantically faithful synthetic data without reliance on pretrained LLMs, providing an effective and flexible framework for mitigating gender bias in sensitive, low-resource mental health domains.

</details>


### [444] [Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models](https://arxiv.org/abs/2601.14152)
*Hyunjong Ok,Jaeho Lee*

Main category: cs.CL

TL;DR: 将背景信息放在题目和选项之前（CQO）比放在后面（QOC）能大幅提升大语言模型在多项选择题中的表现，效果跨模型和数据集均一致。


<details>
  <summary>Details</summary>
Motivation: 大语言模型对提示词结构非常敏感，但背后的原因还不清楚。研究者想弄明白不同提示词顺序为何会影响模型表现。

Method: 作者对比了两种多项选择题的提示结构（CQO与QOC），结合多种模型和数据集，通过系统性的模型结构分析，查找具体影响机制。

Result: CQO结构比QOC结构准确率高14个百分点以上。分析发现，QOC结构下因因果掩码机制，导致选项无法访问上下文信息，信息传递受阻。

Conclusion: 提示词结构极大影响大语言模型表现。因果注意力掩码机制在此过程中起到了核心作用，正确的结构能突破信息瓶颈。

Abstract: Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. In this work, we conduct an in-depth investigation on a striking case: in multiple-choice question answering, placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%p, consistently over a wide range of models and datasets. Through systematic architectural analysis, we identify causal attention as the core mechanism: in QOC prompts, the causal mask prevents option tokens from attending to context, creating an information bottleneck where context becomes invisible to options.

</details>


### [445] [Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language Models for German Law](https://arxiv.org/abs/2601.14160)
*Ali Hamza Bashir,Muhammad Rehan Khalid,Kostadin Cvejoski,Jana Birr,Jule Berghaus,Armin Berger,Sandra Halscheidt,Christian Temath,Rafet Sifa,David Berghaus*

Main category: cs.CL

TL;DR: 本文提出通过德国法律法规自动生成高质量问答对，从而提升大型语言模型在德国法律问答任务中的表现，效果优于未适配模型，证明合成数据可作为人工标注的有效替代方案。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在专业领域（如法律推理）表现有限，容易输出错误信息，主要由于缺乏专业知识。高质量人工标注数据成本高，而现有自动生成数据又往往不够准确，因此亟需一种既高质又高效的数据扩展方法。

Method: 提出一种从权威德国法规直接自动生成高质量、法律准确的问题-答案对的方法，并结合严格的自动过滤和高效微调技术对LLM进行适配。

Result: 实验表明，使用该合成数据集微调后的LLM，在德国法律问答任务中的表现显著超过基线模型。

Conclusion: 精心设计的合成数据能够作为手工注释的有力替代，在法律等高价值知识领域中展现出巨大应用潜力。

Abstract: Large language models (LLMs) often struggle in specialized domains such as legal reasoning due to limited expert knowledge, resulting in factually incorrect outputs or hallucinations. This paper presents an effective method for adapting advanced LLMs to German legal question answering through a novel synthetic data generation approach. In contrast to costly human-annotated resources or unreliable synthetic alternatives, our approach systematically produces high-quality, diverse, and legally accurate question-answer pairs directly from authoritative German statutes. Using rigorous automated filtering methods and parameter-efficient fine-tuning techniques, we demonstrate that LLMs adapted with our synthetic dataset significantly outperform their baseline counterparts on German legal question answering tasks. Our results highlight the feasibility of using carefully designed synthetic data as a robust alternative to manual annotation in high-stakes, knowledge-intensive domains.

</details>


### [446] [Human Values in a Single Sentence: Moral Presence, Hierarchies, and Transformer Ensembles on the Schwartz Continuum](https://arxiv.org/abs/2601.14172)
*Víctor Yeste,Paolo Rosso*

Main category: cs.CL

TL;DR: 该论文研究了在文本中识别肖瓦茨价值观连续体中19个人类价值，在极其稀疏和类别不均衡的新闻与政治宣言句子级数据上进行了实验，并比较了不同神经模型与轻量级信号增强方法及集成方案的表现。


<details>
  <summary>Details</summary>
Motivation: 人类价值识别对理解文本中的道德和价值取向具有重要意义，但在真实的、背景高度稀疏的句子中，细粒度识别尤其困难。本研究旨在应对类别极度不均衡和特征稀缺情况下的人类价值自动检测挑战。

Method: 论文首先将任务定义为二分类（是否涉及价值观）并证明可学习性，随后比较了基于DeBERTa-base操作的两种多标签方法：一种是分级检测体系，一种是直接多标签分类，同时加入轻量级特征增强（如前句上下文、心理/道德词典、主题特征）。还在多种指令微调大模型(7-9B参数)下做了零样本/少样本与QLoRA实验，并构建了简单集成方法。

Result: 分级体系优劣有限，直接多标签方法表现更好，最佳超参数集成结构macro-F1为0.332，显著优于单一监督模型与过往英文基线。轻量级信号和小型模型集成为提升性能的最有效手段。

Conclusion: 在8GB GPU资源及7-9B参数规模下，精心调优的中小型编码器结合轻量级特征和小型集成，能高效、可靠地完成细粒度人类价值识别任务，而分级门控方法改善有限。未来可通过利用更丰富的文本结构和更充分的上下文进一步提升模型表现。

Abstract: We study sentence-level identification of the 19 values in the Schwartz motivational continuum as a concrete formulation of human value detection in text. The setting - out-of-context sentences from news and political manifestos - features sparse moral cues and severe class imbalance. This combination makes fine-grained sentence-level value detection intrinsically difficult, even for strong modern neural models. We first operationalize a binary moral presence task ("does any value appear?") and show that it is learnable from single sentences (positive-class F1 $\approx$ 0.74 with calibrated thresholds). We then compare a presence-gated hierarchy to a direct multi-label classifier under matched compute, both based on DeBERTa-base and augmented with lightweight signals (prior-sentence context, LIWC-22/eMFD/MJD lexica, and topic features). The hierarchy does not outperform direct prediction, indicating that gate recall limits downstream gains. We also benchmark instruction-tuned LLMs - Gemma 2 9B, Llama 3.1 8B, Mistral 8B, and Qwen 2.5 7B - in zero-/few-shot and QLoRA setups and build simple ensembles; a soft-vote supervised ensemble reaches macro-F1 0.332, significantly surpassing the best single supervised model and exceeding prior English-only baselines. Overall, in this scenario, lightweight signals and small ensembles yield the most reliable improvements, while hierarchical gating offers limited benefit. We argue that, under an 8 GB single-GPU constraint and at the 7-9B scale, carefully tuned supervised encoders remain a strong and compute-efficient baseline for structured human value detection, and we outline how richer value structure and sentence-in-document context could further improve performance.

</details>


### [447] [HALT: Hallucination Assessment via Latent Testing](https://arxiv.org/abs/2601.14210)
*Rohan Bhatnagar,Youran Sun,Chi Andrew Zhang,Yixin Wen,Haizhao Yang*

Main category: cs.CL

TL;DR: 本文提出了一种轻量级残差探针方法，通过读取大型语言模型中间层的隐藏状态来直接评估幻觉风险，实现了几乎零延迟的风险检测。该方法在多个问答基准测试中表现出强泛化能力，并有助于提升模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成答案时可能出现“幻觉”，即模型给出自信但错误的答案。虽然模型内部表示可能保留着对不确定性的信号，但在解码输出时这些信号常被弱化，因此需要一种能够更好利用内部不确定性表示的方法。

Method: 该方法在LLM的中间隐藏层对问题token部署了一个小型辅助网络（残差探针），用以直接读取幻觉风险信号。其计算开销远低于生成token，并可与推理过程完全并行，在低风险情况下几乎零延迟。

Result: 在四个问答数据集和多个LLM族群上，该方法取得了较高的AUROC和AURAC分数，并能适应数据分布漂移，展现出了良好的泛化性和对中间表示的可解释性。

Conclusion: 通过在LLM内部高效地读取不确定性信号，为实现更可靠的agentic AI提供了新的理论和方法基础。该轻量探针系统能即时判别风险，将高风险问题转至更强的验证机制，有助于减少模型幻觉现象。

Abstract: Hallucination in large language models (LLMs) can be understood as a failure of faithful readout: although internal representations may encode uncertainty about a query, decoding pressures still yield a fluent answer. We propose lightweight residual probes that read hallucination risk directly from intermediate hidden states of question tokens, motivated by the hypothesis that these layers retain epistemic signals that are attenuated in the final decoding stage. The probe is a small auxiliary network whose computation is orders of magnitude cheaper than token generation and can be evaluated fully in parallel with inference, enabling near-instantaneous hallucination risk estimation with effectively zero added latency in low-risk cases. We deploy the probe as an agentic critic for fast selective generation and routing, allowing LLMs to immediately answer confident queries while delegating uncertain ones to stronger verification pipelines. Across four QA benchmarks and multiple LLM families, the method achieves strong AUROC and AURAC, generalizes under dataset shift, and reveals interpretable structure in intermediate representations, positioning fast internal uncertainty readout as a principled foundation for reliable agentic AI.

</details>


### [448] [MASCOT: Towards Multi-Agent Socio-Collaborative Companion Systems](https://arxiv.org/abs/2601.14230)
*Yiyang Wang,Yiqiao Jin,Alex Cabral,Josiah Hester*

Main category: cs.CL

TL;DR: 本文提出了MASCOT框架，用于提升多智能体系统中的个性一致性和社会贡献度，显著改善了现有系统中人格崩溃与社交谄媚等问题。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统在作为社会协作伙伴时，容易出现个性丧失（人格崩溃）和非建设性对话（社交谄媚），导致系统表现单一、缺乏多样性和实用性，需要一种通用框架来解决这些问题。

Method: MASCOT框架采用双层优化策略：（1）基于RLAIF的方法对每个智能体进行个性对齐，确保保持各自鲜明的人设；（2）引入协作对话优化，通过群体奖励机制调整整体行为，提升对话多样性和协作效果。

Result: 在心理支持和职场协作等多个任务域进行评估，MASCOT在人格一致性指标上提升14.1分，在社会贡献指标上提升10.6分，均优于当前先进方法。

Conclusion: MASCOT为构建社会智能化多智能体系统提供了有效且通用的技术路径，有助于开发具备多样人格和高效社会互动能力的新一代多智能体助手。

Abstract: Multi-agent systems (MAS) have recently emerged as promising socio-collaborative companions for emotional and cognitive support. However, these systems frequently suffer from persona collapse--where agents revert to generic, homogenized assistant behaviors--and social sycophancy, which produces redundant, non-constructive dialogue. We propose MASCOT, a generalizable framework for multi-perspective socio-collaborative companions. MASCOT introduces a novel bi-level optimization strategy to harmonize individual and collective behaviors: 1) Persona-Aware Behavioral Alignment, an RLAIF-driven pipeline that finetunes individual agents for strict persona fidelity to prevent identity loss; and 2) Collaborative Dialogue Optimization, a meta-policy guided by group-level rewards to ensure diverse and productive discourse. Extensive evaluations across psychological support and workplace domains demonstrate that MASCOT significantly outperforms state-of-the-art baselines, achieving improvements of up to +14.1 in Persona Consistency and +10.6 in Social Contribution. Our framework provides a practical roadmap for engineering the next generation of socially intelligent multi-agent systems.

</details>


### [449] [APEX-Agents](https://arxiv.org/abs/2601.14242)
*Bertie Vidgen,Austin Mann,Abby Fennelly,John Wright Stanly,Lucas Rothman,Marco Burstein,Julien Benchek,David Ostrofsky,Anirudh Ravichandran,Debnil Sur,Neel Venugopal,Alannah Hsia,Isaac Robinson,Calix Huang,Olivia Varones,Daniyal Khan,Michael Haines,Zach Richards,Chirag Mahapatra,Brendan Foody,Osvald Nitski*

Main category: cs.CL

TL;DR: 本文提出了APEX-Agents评测基准，用于衡量AI智能体在执行投资银行、管理咨询和公司法务真实任务中的表现，并公布了多种主流模型的测试成绩与开源数据。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能够全面评估AI智能体在真实商业环境下、跨应用场景长周期任务执行能力的权威基准。因此，论文希望通过构建新的评测标准，促进AI智能体在专业领域的性能提升和实际落地。

Method: 作者设计了APEX-Agents基准，包含由实际行业专家设计的任务，覆盖投资银行、管理咨询和企业法务等场景。任务要求AI智能体在模拟真实工作环境（含多种文件、工具）的情况下完成。通过Pass@1指标评估8种主流智能体的表现，并公布了全部评测数据及评估基础设施“Archipelago”。

Result: 在公布的排行榜上，Gemini 3 Flash（Thinking=High）得分最高（24.0%），领先于GPT-5.2、Claude Opus 4.5和Gemini 3 Pro等其他主流智能体。

Conclusion: APEX-Agents为评估AI智能体在复杂跨应用任务上的真实工作能力提供了系统性框架和数据资源，有助于推进智能体实际应用和评测方法的进步。

Abstract: We introduce the AI Productivity Index for Agents (APEX-Agents), a benchmark for assessing whether AI agents can execute long-horizon, cross-application tasks created by investment banking analysts, management consultants, and corporate lawyers. APEX-Agents requires agents to navigate realistic work environments with files and tools. We test eight agents for the leaderboard using Pass@1. Gemini 3 Flash (Thinking=High) achieves the highest score of 24.0%, followed by GPT-5.2 (Thinking=High), Claude Opus 4.5 (Thinking=High), and Gemini 3 Pro (Thinking=High). We open source the APEX-Agents benchmark (n=480) with all prompts, rubrics, gold outputs, files, and metadata. We also open-source Archipelago, our infrastructure for agent execution and evaluation.

</details>


### [450] [Which Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment](https://arxiv.org/abs/2601.14249)
*Yuming Yang,Mingyoung Lai,Wanxu Zhao,Xiaoran Fan,Zhiheng Xi,Mingqi Wu,Chiyue Huang,Jun Zhao,Haijun Lv,Jian Tong,Yunhua Zhou,Yicheng Zou,Qipeng Guo,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 提出了一种新指标Rank-Surprisal Ratio（RSR），用于更有效地选择用于大模型蒸馏的推理轨迹，提高学生模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前从强大教师模型蒸馏推理轨迹时，常用的学生概率相关指标无法兼顾推理信息量和学生适应性，导致更强教师并不总带来更好学生。因此需要更科学衡量推理轨迹与学生匹配度的方法。

Method: 提出RSR指标，综合衡量轨迹在学生模型下的token排名与罕见程度（Surprisal），兼顾学习信号强度和行为一致性。然后在多种学生模型与不同教师轨迹数据上评估RSR与蒸馏后性能的相关性，并与现有指标做对比。

Result: 在5个学生模型和11个不同教师生成的推理轨迹上实证，RSR与学生模型性能高度相关（Spearman 0.86），优于现有度量标准。

Conclusion: RSR为推理轨迹和教师选择提供了有效指标，能提升推理蒸馏效果，优于传统只依赖学生似然的方法。

Abstract: Long chain-of-thought (CoT) trajectories provide rich supervision signals for distilling reasoning from teacher to student LLMs. However, both prior work and our experiments show that trajectories from stronger teachers do not necessarily yield better students, highlighting the importance of data-student suitability in distillation. Existing methods assess suitability primarily through student likelihood, favoring trajectories that closely align with the model's current behavior but overlooking more informative ones. Addressing this, we propose Rank-Surprisal Ratio (RSR), a simple metric that captures both alignment and informativeness to assess the suitability of a reasoning trajectory. RSR is motivated by the observation that effective trajectories typically combine low absolute probability with relatively high-ranked tokens under the student model, balancing learning signal strength and behavioral alignment. Concretely, RSR is defined as the ratio of a trajectory's average token-wise rank to its average negative log-likelihood, and is straightforward to compute and interpret. Across five student models and reasoning trajectories from 11 diverse teachers, RSR strongly correlates with post-training performance (average Spearman 0.86), outperforming existing metrics. We further demonstrate its practical utility in both trajectory selection and teacher selection.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [451] [RobotDesignGPT: Automated Robot Design Synthesis using Vision Language Models](https://arxiv.org/abs/2601.11801)
*Nitish Sontakke,K. Niranjan Kumar,Sehoon Ha*

Main category: cs.RO

TL;DR: 该论文提出了一种利用大规模视觉-语言模型的自动化机器人设计框架RobotDesignGPT，可根据用户提示和参考图片生成机器人设计，并通过视觉反馈提升设计质量。


<details>
  <summary>Details</summary>
Motivation: 机器人设计需要满足多种标准（如结构、外观等），过程复杂且依赖专家经验和大量人工，大多数现有方法为规则驱动，自动化程度低。

Method: 作者提出了RobotDesignGPT框架，结合了预训练视觉-语言模型的知识和推理能力。框架通过用户输入的简单提示和参考图像自动合成初始设计，并引入视觉反馈机制优化设计，减少人工修正。

Result: 该方法能够自动生成外观吸引且运动学有效、受自然启发的各类机器人设计（例如动物、飞行生物等）。通过消融实验和用户研究验证了其有效性。

Conclusion: RobotDesignGPT能够显著提升机器人自动化设计的质量与效率，减少对人工反馈和专家经验的依赖，对未来智能机器人设计有重要意义。

Abstract: Robot design is a nontrivial process that involves careful consideration of multiple criteria, including user specifications, kinematic structures, and visual appearance. Therefore, the design process often relies heavily on domain expertise and significant human effort. The majority of current methods are rule-based, requiring the specification of a grammar or a set of primitive components and modules that can be composed to create a design. We propose a novel automated robot design framework, RobotDesignGPT, that leverages the general knowledge and reasoning capabilities of large pre-trained vision-language models to automate the robot design synthesis process. Our framework synthesizes an initial robot design from a simple user prompt and a reference image. Our novel visual feedback approach allows us to greatly improve the design quality and reduce unnecessary manual feedback. We demonstrate that our framework can design visually appealing and kinematically valid robots inspired by nature, ranging from legged animals to flying creatures. We justify the proposed framework by conducting an ablation study and a user study.

</details>


### [452] [Optimal Thruster Configuration for 6-DOF Control of a Small Satellite](https://arxiv.org/abs/2601.11802)
*Suguru Sato,Jinaykumar Patel,Kamesh Subbarao*

Main category: cs.RO

TL;DR: 本论文研究了小卫星在轨控制和姿态控制问题，通过优化推进器布局，实现了用更少推进器实现六自由度控制。


<details>
  <summary>Details</summary>
Motivation: 随着小卫星（如CubeSat、Nanosat等）在近地轨道上的广泛部署，轨道维护和姿态控制需求增加。常用多推进器主动轨控的方法布置复杂，成本和冗余度高，因此需要寻找更高效的推进器布置方案。

Method: 从24推进器基础布局出发，系统性地提出一组可行的推进器布置，使卫星获得完全六自由度控制能力。进一步筛选出能以最小总推力完成6自由度指令的最优方案，并在典型任务（如交会对接）中仿真验证性能。

Result: 实验/仿真表明，从可行配置组中选出的最优推进器布置，即使推进器数量减少，依然能完成对接等复杂机动任务，表现出与原有复杂布局类似的姿态控制能力。

Conclusion: 小卫星推进器布局通过优化可以显著减少所需推进器总数和推力，而不损失六自由度机动能力，有助于提升小卫星性能并降低成本。

Abstract: With the growing deployment of small satellites (such as CubeSats, Nanosats, Picosats, and Femtosats) in Low Earth Orbit (LEO) for targeted applications like imaging, communication, data storage, and rendezvous-docking mission, there is increasing attention on orbit maintenance and attitude control. A common approach for active orbit control involves the use of multiple thrusters, which, when properly arranged, can also generate the required torque for attitude control. Starting from a 24-thruster configuration, this paper presents a set of thruster configurations (referred to as a viable configuration group) that enable full six degrees of freedom (6-DOF) control. Further, configuration group that requires minimum total thrust to achieve 6-DOF commands are found among the viable configuration group. One configuration from each of these groups is further evaluated for its attitude control performance through a representative rendezvous-docking mission, demonstrating that even with a reduced thruster count, sufficient maneuverability can be achieved.

</details>


### [453] [Three Dimensional Hydrodynamic Flow-Based Collision Avoidance for UAV Formations Facing Emergent Dynamic Obstacles](https://arxiv.org/abs/2601.11832)
*Suguru Sato,Kamesh Subbarao*

Main category: cs.RO

TL;DR: 该论文提出了一种基于流体动力学的三维无人机编队避障框架，通过模拟流体流动特性，实现无人机对动态障碍物的平滑、实时避障。


<details>
  <summary>Details</summary>
Motivation: 传统的势场法在复杂环境下常易陷入局部最小值，且存在轨迹不连续、计算量大的问题，难以满足无人机编队在动态环境下的实时避障需求。

Method: 方法以拉普拉斯方程为基础，将障碍物视为三维双极子或椭球体，通过生成局部速度场来引导无人机避开障碍物，避免路径不连续与重复规划。同时结合虚拟刚体编队策略，保障队形和轨迹一致性。

Result: 仿真结果显示，该方法在多种队形、多障碍以及编队场景中都保持可行性和可扩展性，实现平滑且高效的避障动作。

Conclusion: 该方法可实现实时、高效、解释性强的无人机编队避障，适合实际应用，克服了传统势场法易陷入局部最小和轨迹不连续的问题。

Abstract: This paper presents a three-dimensional, hydrodynamics-inspired collision avoidance framework for uncrewed aerial vehicle (UAV) formations operating in dynamic environments. When moving obstacles enter a UAV's sensing region, they are modeled as three dimensional doublets or ellipsoids that generate local velocity fields, guiding nearby UAVs to execute smooth, collision-free maneuvers without trajectory discontinuities or explicit trajectory replanning. This flow-based approach enables real-time operation and interpretable behavior by leveraging the nature of fluid flow around obstacles via the harmonic properties of Laplace's equation, inherently avoiding local minima common in traditional potential field methods. To establish and maintain coordination among the UAVs, a Virtual Rigid Body (VRB) formation strategy is integrated, ensuring that formation geometry and trajectory tracking are preserved. Simulation results demonstrate the feasibility and scalability of the method for both individual and multi-UAV scenarios with multiple formation geometries encountering moving obstacles. The proposed approach achieves safe, smooth, and computationally efficient avoidance maneuvers suitable for real-time and practical applications.

</details>


### [454] [AI for Green Spaces: Leveraging Autonomous Navigation and Computer Vision for Park Litter Removal](https://arxiv.org/abs/2601.11876)
*Christopher Kao,Akhil Pathapati,James Davis*

Main category: cs.RO

TL;DR: 本文提出并实现了一种可在公园草地上自主导航、识别并捡拾垃圾的机器人，采用STC路径规划、RTK GPS和ResNet50视觉检测，实现了80%的整体成功率。


<details>
  <summary>Details</summary>
Motivation: 美国存在大量乱扔垃圾，公园草地常有游客丢弃垃圾，人工清理效率低，因此亟需自动化解决方案以提高清洁效率。

Method: 机器人采用STC算法进行全覆盖路径规划，利用RTK GPS实现高精度导航，结合ResNet50 CNN模型进行垃圾识别，并实验多种拾取结构以优化拾取效果。

Result: 机器人在识别垃圾方面达到94.52%的准确率，整体垃圾捡拾任务的成功率为80%。

Conclusion: 本研究证明了在草地上应用自主垃圾捡拾机器人是可行和有效的，能够显著提升环境卫生管理效率。

Abstract: There are 50 billion pieces of litter in the U.S. alone. Grass fields contribute to this problem because picnickers tend to leave trash on the field. We propose building a robot that can autonomously navigate, identify, and pick up trash in parks. To autonomously navigate the park, we used a Spanning Tree Coverage (STC) algorithm to generate a coverage path the robot could follow. To navigate this path, we successfully used Real-Time Kinematic (RTK) GPS, which provides a centimeter-level reading every second. For computer vision, we utilized the ResNet50 Convolutional Neural Network (CNN), which detects trash with 94.52% accuracy. For trash pickup, we tested multiple design concepts. We select a new pickup mechanism that specifically targets the trash we encounter on the field. Our solution achieved an overall success rate of 80%, demonstrating that autonomous trash pickup robots on grass fields are a viable solution.

</details>


### [455] [Visual-Language-Guided Task Planning for Horticultural Robots](https://arxiv.org/abs/2601.11906)
*Jose Cuaran,Kendall Koe,Aditya Potnis,Naveen Kumar Uppalapati,Girish Chowdhary*

Main category: cs.RO

TL;DR: 本文提出了一个基于视觉语言模型（VLM）的模块化农作物监测框架，并建立了相关基准，评估了其在单一作物和多作物环境下的短期与长期任务表现。结果发现，VLM能很好完成短期任务，但在长期复杂任务中表现明显下降，主要受限于对语义地图的噪声依赖。


<details>
  <summary>Details</summary>
Motivation: 精细农业需要精准的农作物监控，目前的系统缺乏高级推理能力，难以灵活处理复杂和长期的农田监测任务。该研究意在通过引入视觉语言模型，提升机器人的感知与决策能力，为农业机器人领域注入新的智能方法。

Method: 作者设计了一个模块化框架，利用视觉语言模型（VLM）引导机器人进行任务规划，将输入查询与动作原语交替整合。同时，构建了涵盖单一作物和多作物情境下的短期与长期监控任务基准，用于系统性地评估模型的表现。

Result: VLM框架在短期任务中表现稳定，达到了接近人类的成功率。然而，在处理更具挑战性的长期任务时，性能大幅下降，且对于依赖噪声语义地图的场景，系统表现尤其不佳，揭示了现有VLM在上下文锚定上的关键不足。

Conclusion: 该研究提出了具有实际可部署性的农作物监测框架，并通过基准测试揭示了VLM在农业机器人场景中的能力与局限，为后续改进农田长期监控和VLM上下文理解提供了重要见解。

Abstract: Crop monitoring is essential for precision agriculture, but current systems lack high-level reasoning. We introduce a novel, modular framework that uses a Visual Language Model (VLM) to guide robotic task planning, interleaving input queries with action primitives. We contribute a comprehensive benchmark for short- and long-horizon crop monitoring tasks in monoculture and polyculture environments. Our main results show that VLMs perform robustly for short-horizon tasks (comparable to human success), but exhibit significant performance degradation in challenging long-horizon tasks. Critically, the system fails when relying on noisy semantic maps, demonstrating a key limitation in current VLM context grounding for sustained robotic operations. This work offers a deployable framework and critical insights into VLM capabilities and shortcomings for complex agricultural robotics.

</details>


### [456] [Model selection and real-time skill assessment for suturing in robotic surgery](https://arxiv.org/abs/2601.12012)
*Zhaoyang Jacopo Hu,Alex Ranne,Alaa Eldin Abdelaal,Kiran Bhattacharyya,Etienne Burdet,Allison M. Okamura,Ferdinando Rodriguez y Baena*

Main category: cs.RO

TL;DR: 本研究提出一种基于多模态深度学习的自动化反馈系统，用于达芬奇手术系统手术技能的实时客观评估，融合运动学与视觉数据，结果表明融合模型优于单一模态，对模型泛化和训练数据的专家程度也有新发现。


<details>
  <summary>Details</summary>
Motivation: 机器人辅助手术需求日益增长，缺乏高效、客观、实时的技能评估方法，传统主观评价方式存在局限。开发自动化、准确的反馈系统有助于提升培训和手术安全。

Method: 利用达芬奇手术系统采集运动学和视觉同步数据，提出并比较单一模态与多模态深度学习模型，评估模型设计、实时预测表现以及基于不同技能水平的数据交叉验证训练。主要评价指标为Spearman相关系数。

Result: 多模态融合模型在实时技能评分预测中优于单一模态模型，尤其在预测趋势和手术动作相关性上表现良好。用高水平专家演示数据训练的模型表现更好，且更能泛化到同水平受训者。

Conclusion: 多模态学习实现了更稳定、细致的外科手术技能评价。提升专家级训练数据的质量和数量将有助于模型泛化和实际应用，自动化反馈系统在外科培训与评估中具有重要价值。

Abstract: Automated feedback systems have the potential to provide objective skill assessment for training and evaluation in robot-assisted surgery. In this study, we examine methods to achieve real-time prediction of surgical skill level in real-time based on Objective Structured Assessment of Technical Skills (OSATS) scores. Using data acquired from the da Vinci Surgical System, we carry out three main analyses, focusing on model design, their real-time performance, and their skill-level-based cross-validation training. For the model design, we evaluate the effectiveness of multimodal deep learning models for predicting surgical skill levels using synchronized kinematic and vision data. Our models include separate unimodal baselines and fusion architectures that integrate features from both modalities and are evaluated using mean Spearman's correlation coefficients, demonstrating that the fusion model consistently outperforms unimodal models for real-time predictions. For the real-time performance, we observe the prediction's trend over time and highlight correlation with the surgeon's gestures. For the skill-level-based cross-validation, we separately trained models on surgeons with different skill levels, which showed that high-skill demonstrations allow for better performance than those trained on low-skilled ones and generalize well to similarly skilled participants. Our findings show that multimodal learning allows more stable fine-grained evaluation of surgical performance and highlights the value of expert-level training data for model generalization.

</details>


### [457] [BiKC+: Bimanual Hierarchical Imitation with Keypose-Conditioned Coordination-Aware Consistency Policies](https://arxiv.org/abs/2601.12116)
*Hang Xu,Yizhou Chen,Dongjie Yu,Yi Ren,Jia PanI*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的关键姿态驱动、高效一致性策略的仿射学习方法，有效提升了双臂机器人在多阶段任务中的操作成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 工业制造领域的机器人擅长单臂、重复性任务，但在需要双臂协同和多阶段操作的场景中仍面临动作协调和各阶段衔接的复杂挑战。现有基于生成模型的模仿学习在此方面进展有限，尤其缺乏对多阶段特性的专门优化和推理速度的重视。多阶段任务的一步失误常会影响后续流程，影响整体效率和成功率。

Method: 作者提出一种关键姿态驱动、协调感知且一致性增强的模仿学习策略。方法采用分层结构：高层通过关键姿态（keypose）预测器确定各子阶段的“目标型”关键动作，低层则在此指导下由一致性模型一步规划出动作轨迹。创新性地融合了机器人动作特征与任务风格来识别有效关键姿态。整个动作序列能够在一次推理中生成，兼顾效率与多阶段任务的需求。

Result: 通过仿真及真实机器人实验，所提方法在任务成功率和操作效率上都显著优于现有基线方法，显示出更强的多阶段双臂任务处理能力。

Conclusion: 该工作证明了结合关键姿态预测与一致性轨迹生成的新型策略能有效提升工业双臂机器人在复杂多阶段任务中的表现，对机器人模仿学习及实际工业应用具有重要推进作用。

Abstract: Robots are essential in industrial manufacturing due to their reliability and efficiency. They excel in performing simple and repetitive unimanual tasks but still face challenges with bimanual manipulation. This difficulty arises from the complexities of coordinating dual arms and handling multi-stage processes. Recent integration of generative models into imitation learning (IL) has made progress in tackling specific challenges. However, few approaches explicitly consider the multi-stage nature of bimanual tasks while also emphasizing the importance of inference speed. In multi-stage tasks, failures or delays at any stage can cascade over time, impacting the success and efficiency of subsequent sub-stages and ultimately hindering overall task performance. In this paper, we propose a novel keypose-conditioned coordination-aware consistency policy tailored for bimanual manipulation. Our framework instantiates hierarchical imitation learning with a high-level keypose predictor and a low-level trajectory generator. The predicted keyposes serve as sub-goals for trajectory generation, indicating targets for individual sub-stages. The trajectory generator is formulated as a consistency model, generating action sequences based on historical observations and predicted keyposes in a single inference step. In particular, we devise an innovative approach for identifying bimanual keyposes, considering both robot-centric action features and task-centric operation styles. Simulation and real-world experiments illustrate that our approach significantly outperforms baseline methods in terms of success rates and operational efficiency. Implementation codes can be found at https://github.com/JoanaHXU/BiKC-plus.

</details>


### [458] [Active Semantic Mapping of Horticultural Environments Using Gaussian Splatting](https://arxiv.org/abs/2601.12122)
*Jose Cuaran,Naveen K. Upalapati,Girish Chowdhary*

Main category: cs.RO

TL;DR: 本文提出一种基于移动机械臂的主动式3D语义重建框架，在果蔬等园艺环境中显著提升了重建效率与准确性。该方法结合Octomap与3D Gaussian Splatting，兼顾低分辨率快速全局定位和高保真语义细节表达，实现了高效精准的水果计数和体积估算。


<details>
  <summary>Details</summary>
Motivation: 精准的农业场景语义重建对表型分析和产量估算非常关键，但传统人工扫描或固定相机方法效率低、难以扩展，成为实际应用的瓶颈。

Method: 系统整合了概率占据的低分辨率Octomap用于路径规划和碰撞规避，以及融合几何、光度与语义信息的3D Gaussian Splatting实现高质量重建，同时采用简洁策略增强对分割噪声的鲁棒性，节省内存。

Result: 在仿真实验中，提出的方法比传统的纯占据图有更高的重建精度和效率。在无噪声条件下，水果级F1分数提升6.6%，在有分割噪声情况下提升28.6%；运行时间减少50%。

Conclusion: 提出的主动重建框架有效提升了语义重建的实时性与可扩展性，适合农业机器人场景的落地应用。

Abstract: Semantic reconstruction of agricultural scenes plays a vital role in tasks such as phenotyping and yield estimation. However, traditional approaches that rely on manual scanning or fixed camera setups remain a major bottleneck in this process. In this work, we propose an active 3D reconstruction framework for horticultural environments using a mobile manipulator. The proposed system integrates the classical Octomap representation with 3D Gaussian Splatting to enable accurate and efficient target-aware mapping. While a low-resolution Octomap provides probabilistic occupancy information for informative viewpoint selection and collision-free planning, 3D Gaussian Splatting leverages geometric, photometric, and semantic information to optimize a set of 3D Gaussians for high-fidelity scene reconstruction. We further introduce simple yet effective strategies to enhance robustness against segmentation noise and reduce memory consumption. Simulation experiments demonstrate that our method outperforms purely occupancy-based approaches in both runtime efficiency and reconstruction accuracy, enabling precise fruit counting and volume estimation. Compared to a 0.01m-resolution Octomap, our approach achieves an improvement of 6.6% in fruit-level F1 score under noise-free conditions, and up to 28.6% under segmentation noise. Additionally, it achieves a 50% reduction in runtime, highlighting its potential for scalable, real-time semantic reconstruction in agricultural robotics.

</details>


### [459] [Neural Process-Based Reactive Controller for Autonomous Racing](https://arxiv.org/abs/2601.12143)
*Devin Hunter,Chinwendu Enyioha*

Main category: cs.RO

TL;DR: 本文提出了一种基于注意力神经网络的新型反应式控制框架（AttNP 及其物理信息扩展 PI-AttNP），并结合控制屏障函数（CBF）以实现自动驾驶安全约束，具有优秀的实时性与安全性表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于注意力的神经网络模型在实时非线性控制中广泛应用，尤其是在安全关键领域如自主驾驶场景中，确保决策过程具有统计依据和可验证的安全性变得极为重要。

Method: 1. 提出 Attentive Neural Process（AttNP）模型和物理引导的 PI-AttNP（将近似基于模型的先验注入网络，实现物理归纳偏置）；2. 在F1TENTH风格Ackermann转向赛车仿真环境下进行测试；3. 推导并实现基于控制屏障函数（CBF）的过滤机制，以保证避障安全，其可无缝集成至神经网络控制器。

Result: PI-AttNP 与 CBF 结合可在广泛赛车环境中实现高效碰撞避免，提升收敛速度和预测精度，保证实时性和闭环下的安全约束满足，性能具备竞争力。

Conclusion: 本文方法能在快速演化的自动驾驶场景中提供高效、可验证且安全的自主决策控制框架，兼顾数据驱动方法的灵活性和物理约束的安全性。

Abstract: Attention-based neural architectures have become central to state-of-the-art methods in real-time nonlinear control. As these data-driven models continue to be integrated into increasingly safety-critical domains, ensuring statistically grounded and provably safe decision-making becomes essential. This paper introduces a novel reactive control framework for gap-based navigation using the Attentive Neural Process (AttNP) and a physics-informed extension, the PI-AttNP. Both models are evaluated in a simulated F1TENTH-style Ackermann steering racecar environment, chosen as a fast-paced proxy for safety-critical autonomous driving scenarios. The PI-AttNP augments the AttNP architecture with approximate model-based priors to inject physical inductive bias, enabling faster convergence and improved prediction accuracy suited for real-time control. To further ensure safety, we derive and implement a control barrier function (CBF)-based filtering mechanism that analytically enforces collision avoidance constraints. This CBF formulation is fully compatible with the learned AttNP controller and generalizes across a wide range of racing scenarios, providing a lightweight and certifiable safety layer. Our results demonstrate competitive closed-loop performance while ensuring real-time constraint satisfaction.

</details>


### [460] [Learning Legged MPC with Smooth Neural Surrogates](https://arxiv.org/abs/2601.12169)
*Samuel A. Moore,Easop Lee,Boyuan Chen*

Main category: cs.RO

TL;DR: 本文提出了一种平滑神经代理模型，结合鲁棒的重尾概率学习方法，有效提升了基于神经网络动力学模型的四足机器人MPC的可靠性和性能，特别是在复杂运动任务中表现出数量级的提升。


<details>
  <summary>Details</summary>
Motivation: 神经网络可以为四足机器人的运动规划提供动力学模型，但传统神经网络模型常因接触事件导致的不连续、非物理的非光滑现象和训练误差分布问题，影响MPC的稳定与泛化能力。作者希望解决神经动力学模型不平滑和误差分布不匹配这两大痛点。

Method: 提出“平滑神经代理”神经网络结构，通过可调节的平滑性参数来消除和控制模型中的不连续与非光滑，同时采用支持重尾误差分布的似然训练方法，更好地拟合四足机器人动力学过程中出现的非高斯误差。

Result: 在不同难度的四足机器人零样本运动规划任务上，平滑神经代理和鲁棒学习策略在简单行为场景下将累计成本平均减少10-50%，在高难度或标准神经网络常失效场景下可将累计成本降低2-50倍，实现了运行可靠性的飞跃式提升。

Conclusion: 平滑神经代理和重尾似然训练的组合极大提升了基于神经网络动力学的MPC在四足机器人上的泛化性与鲁棒性，尤其是在挑战性场景下优于传统做法，显示出深度学习与在线轨迹优化有效结合的巨大潜力。

Abstract: Deep learning and model predictive control (MPC) can play complementary roles in legged robotics. However, integrating learned models with online planning remains challenging. When dynamics are learned with neural networks, three key difficulties arise: (1) stiff transitions from contact events may be inherited from the data; (2) additional non-physical local nonsmoothness can occur; and (3) training datasets can induce non-Gaussian model errors due to rapid state changes. We address (1) and (2) by introducing the smooth neural surrogate, a neural network with tunable smoothness designed to provide informative predictions and derivatives for trajectory optimization through contact. To address (3), we train these models using a heavy-tailed likelihood that better matches the empirical error distributions observed in legged-robot dynamics. Together, these design choices substantially improve the reliability, scalability, and generalizability of learned legged MPC. Across zero-shot locomotion tasks of increasing difficulty, smooth neural surrogates with robust learning yield consistent reductions in cumulative cost on simple, well-conditioned behaviors (typically 10-50%), while providing substantially larger gains in regimes where standard neural dynamics often fail outright. In these regimes, smoothing enables reliable execution (from 0/5 to 5/5 success) and produces about 2-50x lower cumulative cost, reflecting orders-of-magnitude absolute improvements in robustness rather than incremental performance gains.

</details>


### [461] [A Comprehensive Review of Bio-Inspired Approaches to Coordination, Communication, and System Architecture in Underwater Swarm Robotics](https://arxiv.org/abs/2601.12244)
*Shyalan Ramesh,Scott Mann,Alex Stumpf*

Main category: cs.RO

TL;DR: 本综述全面梳理了仿生群体智能在水下群体机器人系统中的机制、算法、通信及硬件设计现状，评估了领域内主流方法、关键挑战及未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着海洋作业日益复杂，集体智能机器人系统在海洋观测、探索和资源管理中的作用变得愈加重要。单一自主平台能力有限，群体机器人系统通过集体协调展现出更高灵活性和适应性。本论文旨在总结该领域当前碎片化的研究成果，推进算法、通信和硬件协同集成。

Method: 本论文综述了多种仿生协调机制及在水下群体机器人系统中的应用，并聚焦鱼群算法、鲸鱼优化算法、珊瑚礁优化和海洋捕食者算法等主要方法，分析其在编队控制、任务分配及环境交互等任务中的应用。此外，系统分析了水下通信约束与当前声学、光学、混合通信解决方案及其对群体协作的影响，同时评估了硬件及系统设计对能效和扩展性的支撑作用。最后，提出了一套多维度分类框架，从通信依赖性、环境适应性、能效与规模扩展性四个维度评价现有方法。

Result: 评述对比了不同仿生算法的优劣及适用场景，总结了通信策略与硬件设计的前沿进展，发现目前方法在实际部署中仍受限于通信瓶颈、能效与协同管理等核心挑战。提出多种系统级优化思路。

Conclusion: 本文整合了水下群体机器人领域的仿生算法、通信方式与系统设计思路，提出未来需强化跨学科集成与实用化测试，推动真实环境下高效、可扩展群体机器人系统的落地应用。

Abstract: The increasing complexity of marine operations has intensified the need for intelligent robotic systems to support ocean observation, exploration, and resource management. Underwater swarm robotics offers a promising framework that extends the capabilities of individual autonomous platforms through collective coordination. Inspired by natural systems, such as fish schools and insect colonies, bio-inspired swarm approaches enable distributed decision-making, adaptability, and resilience under challenging marine conditions. Yet research in this field remains fragmented, with limited integration across algorithmic, communication, and hardware design perspectives. This review synthesises bio-inspired coordination mechanisms, communication strategies, and system design considerations for underwater swarm robotics. It examines key marine-specific algorithms, including the Artificial Fish Swarm Algorithm, Whale Optimisation Algorithm, Coral Reef Optimisation, and Marine Predators Algorithm, highlighting their applications in formation control, task allocation, and environmental interaction. The review also analyses communication constraints unique to the underwater domain and emerging acoustic, optical, and hybrid solutions that support cooperative operation. Additionally, it examines hardware and system design advances that enhance system efficiency and scalability. A multi-dimensional classification framework evaluates existing approaches across communication dependency, environmental adaptability, energy efficiency, and swarm scalability. Through this integrated analysis, the review unifies bio-inspired coordination algorithms, communication modalities, and system design approaches. It also identifies converging trends, key challenges, and future research directions for real-world deployment of underwater swarm systems.

</details>


### [462] [An Efficient and Multi-Modal Navigation System with One-Step World Model](https://arxiv.org/abs/2601.12277)
*Wangtian Shen,Ziyang Meng,Jinming Ma,Mingliang Zhou,Diyun Xiang*

Main category: cs.RO

TL;DR: 本文提出了一种高效的导航世界模型，利用一步生成和3D U-Net骨干结构，实现实时高频控制，并在多模态任务中展现出优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的导航策略难以处理三维空间推理且对物理动态理解有限，而基于Transformer的世界模型因自回归和扩散机制导致推理延迟高，无法实时应用。需要一种既能想象又高效的世界模型。

Method: 提出了采用一步生成范式的轻量级导航世界模型，使用3D U-Net骨干并结合高效的时空注意力机制，大幅降低推理延迟。同时，将该模型集成到基于优化的规划框架中，通过锚点初始化处理多模态目标导航任务。

Result: 在仿真和真实环境中的大量闭环实验表明，所提系统在效率和鲁棒性上优于现有最先进的基线方法。

Conclusion: 本文的方法不仅能实现高频控制，还具有更好的预测性能和实时性，为移动机器人导航提供了更高效及健壮的方案。

Abstract: Navigation is a fundamental capability for mobile robots. While the current trend is to use learning-based approaches to replace traditional geometry-based methods, existing end-to-end learning-based policies often struggle with 3D spatial reasoning and lack a comprehensive understanding of physical world dynamics. Integrating world models-which predict future observations conditioned on given actions-with iterative optimization planning offers a promising solution due to their capacity for imagination and flexibility. However, current navigation world models, typically built on pure transformer architectures, often rely on multi-step diffusion processes and autoregressive frame-by-frame generation. These mechanisms result in prohibitive computational latency, rendering real-time deployment impossible. To address this bottleneck, we propose a lightweight navigation world model that adopts a one-step generation paradigm and a 3D U-Net backbone equipped with efficient spatial-temporal attention. This design drastically reduces inference latency, enabling high-frequency control while achieving superior predictive performance. We also integrate this model into an optimization-based planning framework utilizing anchor-based initialization to handle multi-modal goal navigation tasks. Extensive closed-loop experiments in both simulation and real-world environments demonstrate our system's superior efficiency and robustness compared to state-of-the-art baselines.

</details>


### [463] [OpenNavMap: Structure-Free Topometric Mapping via Large-Scale Collaborative Localization](https://arxiv.org/abs/2601.12291)
*Jianhao Jiao,Changkun Liu,Jingwen Yu,Boyi Liu,Qianyi Zhang,Yue Wang,Dimitrios Kanoulas*

Main category: cs.RO

TL;DR: 本文提出OPENNAVMAP，一种无需结构化先验、基于3D几何基础模型的轻量级地图库系统，实现了高效、可扩展且维护成本低的地图表征，在多会话、现实环境下实现了优异的定位和一致的地图合并表现。


<details>
  <summary>Details</summary>
Motivation: 传统基于结构的多会话地图融合方案在大规模、高变环境中维护成本高昂，并且在特征稀少或视角变化巨大的情况下效果不佳，限制了其在现实世界中大规模视觉导航和机器人部署的应用。

Method: 提出OPENNAVMAP系统：利用3D几何基础模型进行按需重建，结合动态规划序列匹配、几何验证及置信度自适应优化，实现从粗到精的子地图对齐，无需预先构建3D模型。

Result: 在Map-Free基准测试上，相较结构光流和回归基线，OPENNAVMAP的平均平移误差仅为0.62米。在15公里多会话数据上实现了地图合并，绝对轨迹误差低于3米。

Conclusion: OPENNAVMAP在无需结构先验的前提下，实现了高精度、全球一致的地图表示和融合，且系统已在模拟和真实机器人上的自主导航中验证有效，具有良好的应用前景和开源价值。

Abstract: Scalable and maintainable map representations are fundamental to enabling large-scale visual navigation and facilitating the deployment of robots in real-world environments. While collaborative localization across multi-session mapping enhances efficiency, traditional structure-based methods struggle with high maintenance costs and fail in feature-less environments or under significant viewpoint changes typical of crowd-sourced data. To address this, we propose OPENNAVMAP, a lightweight, structure-free topometric system leveraging 3D geometric foundation models for on-demand reconstruction. Our method unifies dynamic programming-based sequence matching, geometric verification, and confidence-calibrated optimization to robust, coarse-to-fine submap alignment without requiring pre-built 3D models. Evaluations on the Map-Free benchmark demonstrate superior accuracy over structure-from-motion and regression baselines, achieving an average translation error of 0.62m. Furthermore, the system maintains global consistency across 15km of multi-session data with an absolute trajectory error below 3m for map merging. Finally, we validate practical utility through 12 successful autonomous image-goal navigation tasks on simulated and physical robots. Code and datasets will be publicly available in https://rpl-cs-ucl.github.io/OpenNavMap_page.

</details>


### [464] [From Shallow Waters to Mariana Trench: A Survey of Bio-inspired Underwater Soft Robots](https://arxiv.org/abs/2601.12353)
*Jie Wang,Peng Du,Yiyuan Zhang,Zhexin Xie,Cecilia Laschi*

Main category: cs.RO

TL;DR: 本文综述了水下仿生柔性机器人在海洋环境探索中的应用和最新进展，探讨了它们在实际应用中面临的设计因素与未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 水下环境资源丰富且生态复杂，传统水下机器人存在噪音大、生态破坏和耐压性差等问题，因此需要新型环保且灵活的解决方案。仿生柔性机器人受水生生物启发，有望解决上述挑战。

Method: 通过对近期水下仿生柔性机器人的研究进行文献回顾，系统分析了此类机器人在不同功能、仿生来源、环境压力、温度、光照和生物多样性下的设计考量，并梳理了从仿生原理到实际应用的演进过程。

Result: 仿生柔性机器人在承压、流体动力学、操作和环境友好交互等方面表现优异，已成为水下技术研究的热点。文中总结了最新设计与应用实例，展示其在多种海洋场景下的潜力。

Conclusion: 水下仿生柔性机器人是海洋探索领域的重要发展方向。未来需进一步推动其功能多样化、智能感知、能效提升和生态兼容性，以实现更广泛的实际应用。

Abstract: Sample Exploring the ocean environment holds profound significance in areas such as resource exploration and ecological protection. Underwater robots struggle with extreme water pressure and often cause noise and damage to the underwater ecosystem, while bio-inspired soft robots draw inspiration from aquatic creatures to address these challenges. These bio-inspired approaches enable robots to withstand high water pressure, minimize drag, operate with efficient manipulation and sensing systems, and interact with the environment in an eco-friendly manner. Consequently, bio-inspired soft robots have emerged as a promising field for ocean exploration. This paper reviews recent advancements in underwater bio-inspired soft robots, analyses their design considerations when facing different desired functions, bio-inspirations, ambient pressure, temperature, light, and biodiversity , and finally explores the progression from bio-inspired principles to practical applications in the field and suggests potential directions for developing the next generation of underwater soft robots.

</details>


### [465] [R-VoxelMap: Accurate Voxel Mapping with Recursive Plane Fitting for Online LiDAR Odometry](https://arxiv.org/abs/2601.12377)
*Haobo Xi,Shiyong Zhang,Qianli Dong,Yunze Tong,Songyang Wu,Jing Yuan,Xuebo Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种名为R-VoxelMap的新型体素地图构建方法，通过引入递归平面拟合与异常点检测机制，提高了激光雷达里程计的定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有体素地图方法在平面拟合时容易受到异常点影响，导致平面参数偏差，同时存在大平面过分割或跨物理平面错误合并的问题。

Method: 针对上述问题，R-VoxelMap采用基于几何的递归构建策略。每个体素中，首先通过RANSAC拟合平面并剔除异常点，剩余异常点则递归传递到更细粒度的八叉树层级继续处理。此外，设计了基于点分布的有效性检测算法，避免了平面错误合并。

Result: 在多个公开LiDAR(-惯性)SLAM数据集上的实验表明，R-VoxelMap比现有方法精度更高，且效率和内存消耗相当。

Conclusion: R-VoxelMap有效提升了体素地图在激光雷达定位中的表现，具备较强的实用与推广价值。

Abstract: This paper proposes R-VoxelMap, a novel voxel mapping method that constructs accurate voxel maps using a geometry-driven recursive plane fitting strategy to enhance the localization accuracy of online LiDAR odometry. VoxelMap and its variants typically fit and check planes using all points in a voxel, which may lead to plane parameter deviation caused by outliers, over segmentation of large planes, and incorrect merging across different physical planes. To address these issues, R-VoxelMap utilizes a geometry-driven recursive construction strategy based on an outlier detect-and-reuse pipeline. Specifically, for each voxel, accurate planes are first fitted while separating outliers using random sample consensus (RANSAC). The remaining outliers are then propagated to deeper octree levels for recursive processing, ensuring a detailed representation of the environment. In addition, a point distribution-based validity check algorithm is devised to prevent erroneous plane merging. Extensive experiments on diverse open-source LiDAR(-inertial) simultaneous localization and mapping (SLAM) datasets validate that our method achieves higher accuracy than other state-of-the-art approaches, with comparable efficiency and memory usage. Code will be available on GitHub.

</details>


### [466] [VR$^2$: A Co-Located Dual-Headset Platform for Touch-Enabled Human-Robot Interaction Research](https://arxiv.org/abs/2601.12395)
*Chao Wang,Anna Belardinelli,Michael Gienger*

Main category: cs.RO

TL;DR: 本文提出了VR2VR系统，通过同空间双VR头显平台，真实还原和研究触觉丰富的人机交互，既保障真实物理触碰，又灵活实现虚拟身体的交互控制。


<details>
  <summary>Details</summary>
Motivation: 以往人机交互中的机器人触觉研究面临实际机器人造价高、编程慢和VR原型无法真实还原触觉等问题，严重限制了触觉交互相关的科学实验和原型开发。

Method: 提出了一种双VR头显系统，让实验参与者和隐藏操作员共处一室，通过动作追踪和面部信号，实时将操作员上半身和面部表情映射到机器人的虚拟形象，同时操作员可物理接触参与者，实现机器人触碰和各种非语言表现（如眼神、表情等）单独或组合的控制。系统还涵盖了精细配准和逆运动学算法来精确映射手部动作。

Result: 系统被用于触觉向导法（Wizard-of-Oz）的HRI研究，展示了控制不同非语言渠道时带来的实验控制力，并保证物理互动的真实性，有效验证了系统支持快速原型与科学评测的能力。

Conclusion: VR2VR极大简化了具身、以触觉为核心的机器人行为原型开发和科学评测流程，降低了实验门槛，为触觉交互的人机研究提供了新平台。

Abstract: Touch-rich human-robot interaction (HRI) is difficult to study: building and programming physical robots is costly and slow, while VR-based robot prototypes often remove physical contact or break the tight coupling between an agent's body and the user's felt touch. We present VR2VR, a co-located dual VR-headset platform for HRI research in which a participant and a hidden operator share the same physical space while experiencing different virtual embodiments. The participant sees an expressive virtual robot that interacts face-to-face in a shared virtual environment. In real time, the robot's upper-body gestures, head and gaze behaviors, and facial expressions are mapped from the operator's tracked motion and face signals. Because the operator is physically co-present and calibrated into the same coordinate frame, the operator can also physically touch the participant, enabling the participant to perceive robot touch aligned with the robot's hands; finger and hand motion are mapped to the robot using inverse kinematics to support precise contact. Beyond faithful motion retargeting for limb teleoperation, our VR2VR system supports experimental control by retargeting or selectively enabling nonverbal channels (e.g., head only vs. head+eyes vs. head+eyes+facial expressions) while keeping physical interaction constant. We detail the system design, calibration workflow, and safety considerations, and demonstrate the platform through a touch-based Wizard-of-Oz HRI study, illustrating how VR2VR lowers barriers for rapidly prototyping and rigorously evaluating embodied, touch-centric robot behaviors.

</details>


### [467] [Learning Diverse Skills for Behavior Models with Mixture of Experts](https://arxiv.org/abs/2601.12397)
*Wangtian Shen,Jinming Ma,Mingliang Zhou,Ziyang Meng*

Main category: cs.RO

TL;DR: Di-BM是一种将Mixture of Experts引入模仿学习的新方法，通过专家分工避免多任务学习下性能下降问题，在多个机器人操作任务上显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习在单任务上效果良好，但实际多任务应用中由于任务之间干扰，模型性能下降（平均化效应），亟需解决多任务之间的干扰问题。

Method: 提出了Di-BM方法，引入Mixture of Experts架构，将每个专家与不同观测分布关联，使专家专注于观测空间的子区域。利用能量模型表征专家特定的观测分布，并与动作模型联合训练。该方法可无缝集成至现有模仿学习流程。

Result: 在多个真实机器人操作任务中，Di-BM显著超越最新模仿学习基线方法。此外，预训练的Di-BM在新任务微调时展现更高数据效率和专家知识可复用性。

Conclusion: Di-BM有效解决了多任务模仿学习中的干扰和平均化问题，提升了泛化能力与知识迁移能力，具有实际落地价值。

Abstract: Imitation learning has demonstrated strong performance in robotic manipulation by learning from large-scale human demonstrations. While existing models excel at single-task learning, it is observed in practical applications that their performance degrades in the multi-task setting, where interference across tasks leads to an averaging effect. To address this issue, we propose to learn diverse skills for behavior models with Mixture of Experts, referred to as Di-BM. Di-BM associates each expert with a distinct observation distribution, enabling experts to specialize in sub-regions of the observation space. Specifically, we employ energy-based models to represent expert-specific observation distributions and jointly train them alongside the corresponding action models. Our approach is plug-and-play and can be seamlessly integrated into standard imitation learning methods. Extensive experiments on multiple real-world robotic manipulation tasks demonstrate that Di-BM significantly outperforms state-of-the-art baselines. Moreover, fine-tuning the pretrained Di-BM on novel tasks exhibits superior data efficiency and the reusable of expert-learned knowledge. Code is available at https://github.com/robotnav-bot/Di-BM.

</details>


### [468] [ReWorld: Multi-Dimensional Reward Modeling for Embodied World Models](https://arxiv.org/abs/2601.12428)
*Baorui Peng,Wenyao Zhang,Liang Xu,Zekun Qi,Jiazhao Zhang,Hongsi Liu,Wenjun Zeng,Xin Jin*

Main category: cs.RO

TL;DR: ReWorld提出了一种新的视频世界模型，通过引入基于强化学习的对齐方法，提升机器人学习中的物理真实性和任务逻辑一致性。实验显示ReWorld在多方面均优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视频世界模型过度关注视觉生成质量，忽视物理真实性、动态一致性和任务逻辑，难以适用于包含丰富接触的操作任务。解决这些问题对于机器人学习在实际场景的应用变得尤为重要。

Method: ReWorld主要包括两步：1) 构建了一个大规模（约23.5万条）的视频偏好数据集，并用其训练层次化奖励模型，捕捉符合人类偏好的多维奖励；2) 提出一种高效的PPO风格模型对齐算法，以此奖励对流式世界模型进行后训练。

Result: 实验证明ReWorld生成的视频序列在物理一致性、任务逻辑、仿真真实感及视觉质量等方面显著提升，全面优于以往方法，并得到了理论分析的支持。

Conclusion: ReWorld通过强化学习实现了世界模型与物理和任务逻辑的对齐，提升了下游机器人任务的可用性，是基于视频的机器人世界模型领域的重要进展。

Abstract: Recently, video-based world models that learn to simulate the dynamics have gained increasing attention in robot learning. However, current approaches primarily emphasize visual generative quality while overlooking physical fidelity, dynamic consistency, and task logic, especially for contact-rich manipulation tasks, which limits their applicability to downstream tasks. To this end, we introduce ReWorld, a framework aimed to employ reinforcement learning to align the video-based embodied world models with physical realism, task completion capability, embodiment plausibility and visual quality. Specifically, we first construct a large-scale (~235K) video preference dataset and employ it to train a hierarchical reward model designed to capture multi-dimensional reward consistent with human preferences. We further propose a practical alignment algorithm that post-trains flow-based world models using this reward through a computationally efficient PPO-style algorithm. Comprehensive experiments and theoretical analysis demonstrate that ReWorld significantly improves the physical fidelity, logical coherence, embodiment and visual quality of generated rollouts, outperforming previous methods.

</details>


### [469] [KILO-EKF: Koopman-Inspired Learned Observations Extended Kalman Filter](https://arxiv.org/abs/2601.12463)
*Zi Cong Guo,James R. Forbes,Timothy D. Barfoot*

Main category: cs.RO

TL;DR: 本文提出了一种基于Koopman思想的观测模型学习方法，并结合扩展卡尔曼滤波（EKF）预测，实现了无需复杂传感器建模的高效定位系统，优于传统EKF及其变体。


<details>
  <summary>Details</summary>
Motivation: 传统EKF依赖准确的传感器模型和标定，面对复杂或难以精确建模的传感器时，常导致性能下降。作者希望开发一种更灵活、可扩展、依赖数据而非具体传感器模型的方法，以提升现实应用中的滤波效果。

Method: 方法包含一个标准EKF预测步骤，观测校正则借助基于Koopman思想且从数据学习得到的观测模型。观测通过提升映射到特征空间，使状态对观测的关系线性化。观测模型可由带标签的数据闭式学习获得，无需迭代优化或显式传感器建模。滤波更新时通过学习得到的提升计算雅可比矩阵用于标准EKF更新。

Result: 在真实的四旋翼定位任务（融合IMU、UWB和激光测距）中，KILO-EKF表现优于多种EKF对照组，含不同标定水平，尤其对依赖不准确模型的EKF有显著提升，并保持实时推断和快速训练。

Conclusion: Koopman思想下的观测模型学习为复杂传感器的数据驱动滤波提供了高效、可扩展的替代方案，能够有效减少对传统传感器建模和标定的依赖，具有实际应用前景。

Abstract: We present the Koopman-Inspired Learned Observations Extended Kalman Filter (KILO-EKF), which combines a standard EKF prediction step with a correction step based on a Koopman-inspired measurement model learned from data. By lifting measurements into a feature space where they are linear in the state, KILO-EKF enables flexible modeling of complex or poorly calibrated sensors while retaining the structure and efficiency of recursive filtering. The resulting linear-Gaussian measurement model is learned in closed form from groundtruth training data, without iterative optimization or reliance on an explicit parametric sensor model. At inference, KILO-EKF performs a standard EKF update using Jacobians obtained via the learned lifting. We validate the approach on a real-world quadrotor localization task using an IMU, ultra-wideband (UWB) sensors, and a downward-facing laser. We compare against multiple EKF baselines with varying levels of sensor calibration. KILO-EKF achieves better accuracy and consistency compared to data-calibrated baselines, and significantly outperforms EKFs that rely on imperfect geometric models, while maintaining real-time inference and fast training. These results demonstrate the effectiveness of Koopman-inspired measurement learning as a scalable alternative to traditional model-based calibration.

</details>


### [470] [Language-Based Swarm Perception: Decentralized Person Re-Identification via Natural Language Descriptions](https://arxiv.org/abs/2601.12479)
*Miquel Kegeleirs,Lorenzo Garattoni,Gianpiero Francesca,Mauro Birattari*

Main category: cs.RO

TL;DR: 该论文提出利用自然语言作为主表征方式，在机器人群体中实现去中心化的人员再识别。实验显示在身份一致性和可解释性方面具有竞争力，且支持基于自然语言的查询和群体协作。


<details>
  <summary>Details</summary>
Motivation: 传统人员再识别方法依赖不可解释的视觉特征嵌入，且多为中心化处理，难以实现群体中各机器人的高效协作与结果可解释性。为提升透明度和灵活性，急需更加直观透明的去中心化再识别方案。

Method: 每个机器人通过视觉-语言模型对个体进行检测，并生成可读的文本描述。机器人之间不经过中心协调，通过对文本描述的分组聚类，最终由语言模型提炼得到简洁的个体代表性描述，从而实现去中心化、可解释的人员再识别。

Result: 初步实验表明，该方法在身份一致性和解释性方面与基于特征嵌入方法表现相当。但当前仍受限于文本相似度计算和计算负载。

Conclusion: 本文验证了使用自然语言描述来支持去中心化人员再识别的可行性，提升了系统的可解释性和查询能力。未来工作将聚焦文本相似度优化、语义导航和对环境元素的扩展。

Abstract: We introduce a method for decentralized person re-identification in robot swarms that leverages natural language as the primary representational modality. Unlike traditional approaches that rely on opaque visual embeddings -- high-dimensional feature vectors extracted from images -- the proposed method uses human-readable language to represent observations. Each robot locally detects and describes individuals using a vision-language model (VLM), producing textual descriptions of appearance instead of feature vectors. These descriptions are compared and clustered across the swarm without centralized coordination, allowing robots to collaboratively group observations of the same individual. Each cluster is distilled into a representative description by a language model, providing an interpretable, concise summary of the swarm's collective perception. This approach enables natural-language querying, enhances transparency, and supports explainable swarm behavior. Preliminary experiments demonstrate competitive performance in identity consistency and interpretability compared to embedding-based methods, despite current limitations in text similarity and computational load. Ongoing work explores refined similarity metrics, semantic navigation, and the extension of language-based perception to environmental elements. This work prioritizes decentralized perception and communication, while active navigation remains an open direction for future study.

</details>


### [471] [Enabling High-Curvature Navigation in Eversion Robots through Buckle-Inducing Constrictive Bands](https://arxiv.org/abs/2601.12523)
*Cem Suulker,Muhie Al Haimus,Thomas Mack,Mohammad Sheikhsofla,Neri Niccolò Dei,Reza Kashef,Hadi Sadati,Federica Barontini,Fanny Ficuciello,Alberto Arezzo,Bruno Siciliano,Sebastien Ourselin,Kaspar Althoefer*

Main category: cs.RO

TL;DR: 本文提出了一种无需主动控制的新方法，通过在翻转机器人外壁布置不可伸缩的缩径环带，降低其弯曲刚度，从而大幅提升其在狭小弯曲空间中的通过能力。


<details>
  <summary>Details</summary>
Motivation: 现有的翻转机器人在通过狭窄、复杂通道时，往往由于结构刚性不足或导航不可靠，需要引入主动控制机制，这增加了机器人结构和制造的复杂度，削弱了其原有的柔软和顺应特性。因此，亟需一种在不牺牲柔软性和机械简约性的前提下，提升机器人机动性的方案。

Method: 作者提出在机器人本体外侧等间距集成不可伸缩的缩径环带，使机器人在受力弯曲时于预定位置发生屈曲，降低整体弯曲刚度。利用Cosserat杆理论对这一结构的力学行为进行建模，量化缩径环带导致的局部刚度变化及对整体弯曲能力的影响。

Result: 实验表明，缩径环带能显著降低机器人端部弯曲刚度，最大可降至原来的9%。该设计使机器人在180°的大弯曲路径中，最小可通过弯曲半径由原来的35 mm缩小至25 mm。此外，在结肠仿真环境的应用案例中，验证了新方法的可行性。

Conclusion: 本研究证明，通过被动结构优化即可显著提升翻转机器人的可操控性与通过性，而不需增加机械复杂度或牺牲柔软性，为其在高度曲折、狭窄通道（如管道检测、结肠镜等医疗场合）中的应用拓展了可能性。

Abstract: Tip-growing eversion robots are renowned for their ability to access remote spaces through narrow passages. However, achieving reliable navigation remains a significant challenge. Existing solutions often rely on artificial muscles integrated into the robot body or active tip-steering mechanisms. While effective, these additions introduce structural complexity and compromise the defining advantages of eversion robots: their inherent softness and compliance. In this paper, we propose a passive approach to reduce bending stiffness by purposefully introducing buckling points along the robot's outer wall. We achieve this by integrating inextensible diameter-reducing circumferential bands at regular intervals along the robot body facilitating forward motion through tortuous, obstacle cluttered paths. Rather than relying on active steering, our approach leverages the robot's natural interaction with the environment, allowing for smooth, compliant navigation. We present a Cosserat rod-based mathematical model to quantify this behavior, capturing the local stiffness reductions caused by the constricting bands and their impact on global bending mechanics. Experimental results demonstrate that these bands reduce the robot's stiffness when bent at the tip by up to 91 percent, enabling consistent traversal of 180 degree bends with a bending radius of as low as 25 mm-notably lower than the 35 mm achievable by standard eversion robots under identical conditions. The feasibility of the proposed method is further demonstrated through a case study in a colon phantom. By significantly improving maneuverability without sacrificing softness or increasing mechanical complexity, this approach expands the applicability of eversion robots in highly curved pathways, whether in relation to pipe inspection or medical procedures such as colonoscopy.

</details>


### [472] [RPT*: Global Planning with Probabilistic Terminals for Target Search in Complex Environments](https://arxiv.org/abs/2601.12701)
*Yunpeng Lyu,Chao Cao,Ji Zhang,Howie Choset,Zhongqiang Ren*

Main category: cs.RO

TL;DR: 本文提出了一种新型哈密尔顿路径问题（HPP）的变体——基于概率终端的哈密尔顿路径问题（HPP-PT），旨在最小化移动机器人在存在目标不确定性的情况下，期望的路径成本，并通过新的启发式搜索和动态规划方法，提升解的最优性和计算效率。实验表明，所提方法在实际和仿真环境下均优于传统基线。


<details>
  <summary>Details</summary>
Motivation: 经典路由问题（如哈密尔顿路径）多假设目标节点一定需要访问，无法处理目标存在不确定性的情形，而实际问题如移动机器人搜寻目标时，目标的位置往往只有概率信息。为提升真实环境下路径规划的有效性，有必要引入处理目标不确定性的模型。

Method: 论文针对HPP-PT问题，提出了RPT*搜索算法，基于动态规划在新的状态空间中规避历史路径依赖，并设计了高效的启发式方法加速计算。在此基础上，构建了层次化自主目标搜寻系统（HATS），结合Bayes滤波器实现带噪声的长期搜索，或者结合自主探索解决未知环境下的目标发现。

Result: 实验在仿真和实际机器人平台上进行，结果表明：RPT*与HATS系统能够有效平衡利用与探索，比基准方法平均更快找到目标，并保证路径期望成本的最优性。

Conclusion: 本文提出的RPT*算法和HATS系统为处理带不确定目标的路径规划问题提供了高效、可行并具有最优性保证的解决方案，在移动机器人自主搜寻等领域展现出显著优势。

Abstract: Routing problems such as Hamiltonian Path Problem (HPP), seeks a path to visit all the vertices in a graph while minimizing the path cost. This paper studies a variant, HPP with Probabilistic Terminals (HPP-PT), where each vertex has a probability representing the likelihood that the robot's path terminates there, and the objective is to minimize the expected path cost. HPP-PT arises in target object search, where a mobile robot must visit all candidate locations to find an object, and prior knowledge of the object's location is expressed as vertex probabilities. While routing problems have been studied for decades, few of them consider uncertainty as required in this work. The challenge lies not only in optimally ordering the vertices, as in standard HPP, but also in handling history dependency: the expected path cost depends on the order in which vertices were previously visited. This makes many existing methods inefficient or inapplicable. To address the challenge, we propose a search-based approach RPT* with solution optimality guarantees, which leverages dynamic programming in a new state space to bypass the history dependency and novel heuristics to speed up the computation. Building on RPT*, we design a Hierarchical Autonomous Target Search (HATS) system that combines RPT* with either Bayesian filtering for lifelong target search with noisy sensors, or autonomous exploration to find targets in unknown environments. Experiments in both simulation and real robot show that our approach can naturally balance between exploitation and exploration, thereby finding targets more quickly on average than baseline methods.

</details>


### [473] [AirHunt: Bridging VLM Semantics and Continuous Planning for Efficient Aerial Object Navigation](https://arxiv.org/abs/2601.12742)
*Xuecheng Chen,Zongzhuo Liu,Jianfa Ma,Bang Du,Tiantian Zhang,Xueqian Wang,Boyu Zhou*

Main category: cs.RO

TL;DR: AirHunt是一个无人机目标导航系统，结合大规模视觉语言模型（VLM）语义推理和实时路径规划，实现对开放类别目标的高效搜索，在多种环境下优于当前方法。


<details>
  <summary>Details</summary>
Motivation: 当前大规模VLM具备强大的语义理解能力，有潜力用于无人机根据自然语言搜索开放集目标。但VLM推理频率远低于无人机实时规划需求，且VLM对3D场景理解有限，难以实际应用，同时在大规模环境下难以兼顾语义指导与导航效率。

Method: 提出AirHunt系统，采用双通路异步架构，实现VLM推理与路径规划的协同交互。设计主动式双任务推理模块，利用几何和语义冗余选择性VLM询问；提出语义-几何一体化规划模块，兼顾语义优先级与运动效率，动态适应环境异质性。

Result: 在多种物体导航任务和环境下测试，AirHunt成功率更高，导航误差和飞行时间更低，优于现有最先进方法。实地实验验证其在复杂环境下的实用性。

Conclusion: AirHunt有效解决了VLM与无人机导航融合的频率与场景理解难题，实现了高效、泛化性强的目标搜索，对未来相关无人机系统开发具有重要参考价值。

Abstract: Recent advances in large Vision-Language Models (VLMs) have provided rich semantic understanding that empowers drones to search for open-set objects via natural language instructions. However, prior systems struggle to integrate VLMs into practical aerial systems due to orders-of-magnitude frequency mismatch between VLM inference and real-time planning, as well as VLMs' limited 3D scene understanding. They also lack a unified mechanism to balance semantic guidance with motion efficiency in large-scale environments. To address these challenges, we present AirHunt, an aerial object navigation system that efficiently locates open-set objects with zero-shot generalization in outdoor environments by seamlessly fusing VLM semantic reasoning with continuous path planning. AirHunt features a dual-pathway asynchronous architecture that establishes a synergistic interface between VLM reasoning and path planning, enabling continuous flight with adaptive semantic guidance that evolves through motion. Moreover, we propose an active dual-task reasoning module that exploits geometric and semantic redundancy to enable selective VLM querying, and a semantic-geometric coherent planning module that dynamically reconciles semantic priorities and motion efficiency in a unified framework, enabling seamless adaptation to environmental heterogeneity. We evaluate AirHunt across diverse object navigation tasks and environments, demonstrating a higher success rate with lower navigation error and reduced flight time compared to state-of-the-art methods. Real-world experiments further validate AirHunt's practical capability in complex and challenging environments. Code and dataset will be made publicly available before publication.

</details>


### [474] [FocusNav: Spatial Selective Attention with Waypoint Guidance for Humanoid Local Navigation](https://arxiv.org/abs/2601.12790)
*Yang Zhang,Jianming Ma,Liyun Yan,Zhanxiang Cao,Yazhou Zhang,Haoyang Li,Yue Gao*

Main category: cs.RO

TL;DR: 本文提出了FocusNav框架，通过空间选择性注意机制提升类人机器人在复杂动态环境中的局部导航鲁棒性，显著提升了避障和运动稳定性。


<details>
  <summary>Details</summary>
Motivation: 类人机器人在非结构化、动态环境下实现本地导航时，如何兼顾远距离的导航目标与实时的运动稳定性是一个难题。现有方法往往难以在感知远目标和局部稳健动作间取得平衡，导致碰撞或跌倒风险上升。

Method: 提出FocusNav空间选择性注意框架，包含两个主要模块：(1) 路点引导空间交叉注意（WGSCA）机制，利用一序列预测的安全路点引导环境感知聚合，确保在规划轨迹上的任务相关感知；(2) 稳定性感知选择门控（SASG），当检测到机器人不稳定时自动截断远距离信息，使策略专注于当前落脚点安全。

Result: 在Unitree G1类人机器人上进行大量实验，FocusNav在复杂环境下相较于基线方法显著提升了导航成功率，在避障与运动稳定性上表现更优。

Conclusion: FocusNav显著提升了类人机器人在动态、复杂环境下的导航鲁棒性，实现了更优的避障和稳定性，对实际机器人部署具有重要意义。

Abstract: Robust local navigation in unstructured and dynamic environments remains a significant challenge for humanoid robots, requiring a delicate balance between long-range navigation targets and immediate motion stability. In this paper, we propose FocusNav, a spatial selective attention framework that adaptively modulates the robot's perceptual field based on navigational intent and real-time stability. FocusNav features a Waypoint-Guided Spatial Cross-Attention (WGSCA) mechanism that anchors environmental feature aggregation to a sequence of predicted collision-free waypoints, ensuring task-relevant perception along the planned trajectory. To enhance robustness in complex terrains, the Stability-Aware Selective Gating (SASG) module autonomously truncates distal information when detecting instability, compelling the policy to prioritize immediate foothold safety. Extensive experiments on the Unitree G1 humanoid robot demonstrate that FocusNav significantly improves navigation success rates in challenging scenarios, outperforming baselines in both collision avoidance and motion stability, achieving robust navigation in dynamic and complex environments.

</details>


### [475] [Contact-Aware Neural Dynamics](https://arxiv.org/abs/2601.12796)
*Changwei Jing,Jai Krishna Bandi,Jianglong Ye,Yan Duan,Pieter Abbeel,Xiaolong Wang,Sha Yi*

Main category: cs.RO

TL;DR: 本文提出了一种隐式sim-to-real对齐框架，通过结合真实接触信息和神经动力学模型，提升仿真与现实机器人在处理复杂接触任务时的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有显式系统识别方法在对齐真实世界中的高维、状态依赖和复杂动力学方面有限，尤其在涉及不连续接触的任务中，sim-to-real差距依然显著。

Method: 提出以现成物理引擎为先验，利用触觉接触信息，训练一个接触感知的神经动力学模型，结合真实观测对仿真状态进行校正，从而实现仿真器动力学与真实世界的对齐。

Result: 实验证明，该方法利用机器人手部的触觉信息，可以更好地建模接触任务中的不连续特性，提高了系统的状态预测精度，并能提升仿真训练的策略在现实中的表现。

Conclusion: 该框架为仿真到现实的对齐提供了一种数据驱动、可扩展的新方法，有助于使机器人学习在现实世界中的应用更加可靠。

Abstract: High-fidelity physics simulation is essential for scalable robotic learning, but the sim-to-real gap persists, especially for tasks involving complex, dynamic, and discontinuous interactions like physical contacts. Explicit system identification, which tunes explicit simulator parameters, is often insufficient to align the intricate, high-dimensional, and state-dependent dynamics of the real world. To overcome this, we propose an implicit sim-to-real alignment framework that learns to directly align the simulator's dynamics with contact information. Our method treats the off-the-shelf simulator as a base prior and learns a contact-aware neural dynamics model to refine simulated states using real-world observations. We show that using tactile contact information from robotic hands can effectively model the non-smooth discontinuities inherent in contact-rich tasks, resulting in a neural dynamics model grounded by real-world data. We demonstrate that this learned forward dynamics model improves state prediction accuracy and can be effectively used to predict policy performance and refine policies trained purely in standard simulators, offering a scalable, data-driven approach to sim-to-real alignment.

</details>


### [476] [FRoM-W1: Towards General Humanoid Whole-Body Control with Language Instructions](https://arxiv.org/abs/2601.12799)
*Peng Li,Zihan Zhuang,Yangfan Gao,Yi Dong,Sixian Li,Changhao Jiang,Shihan Dou,Zhiheng Xi,Enyu Zhou,Jixuan Huang,Hui Li,Jingjing Gong,Xingjun Ma,Tao Gui,Zuxuan Wu,Qi Zhang,Xuanjing Huang,Yu-Gang Jiang,Xipeng Qiu*

Main category: cs.RO

TL;DR: FRoM-W1是一个实现通用仿人整身运动控制的开源框架，它通过自然语言让机器人执行丰富动作，并在真实机器人上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有仿人机器人的复杂动作大多需要硬编码或单独训练，缺乏通用性和灵活性，阻碍了机器人智力与实际应用的发展。因此，亟需一种能够理解自然语言指令并泛化多样全身动作的通用控制方案。

Method: FRoM-W1框架分为两个阶段：一是H-GPT，利用大规模人类数据训练语言驱动的整身动作生成模型，并结合Chain-of-Thought技术提升模型推理与泛化能力；二是H-ACT，将生成人类动作重新定向为适配机器人本体的动作，通过以强化学习为主的运动控制器实现物理仿真下的精准、稳定执行，并配合仿真到现实模块部署在真实机器人上。

Result: 在Unitree H1和G1机器人上进行了大量实验。在HumanML3D-X基准上，FRoM-W1展现出较其它方法更高的整身动作生成表现。通过强化学习微调，机器人运动追踪准确率和任务成功率均显著提升。

Conclusion: FRoM-W1显著推动了仿人机器人通过自然语言实现多样整身运动的能力，为仿人智能的持续发展奠定了基础，相关代码已开源推动社区合作。

Abstract: Humanoid robots are capable of performing various actions such as greeting, dancing and even backflipping. However, these motions are often hard-coded or specifically trained, which limits their versatility. In this work, we present FRoM-W1, an open-source framework designed to achieve general humanoid whole-body motion control using natural language. To universally understand natural language and generate corresponding motions, as well as enable various humanoid robots to stably execute these motions in the physical world under gravity, FRoM-W1 operates in two stages: (a) H-GPT: utilizing massive human data, a large-scale language-driven human whole-body motion generation model is trained to generate diverse natural behaviors. We further leverage the Chain-of-Thought technique to improve the model's generalization in instruction understanding. (b) H-ACT: After retargeting generated human whole-body motions into robot-specific actions, a motion controller that is pretrained and further fine-tuned through reinforcement learning in physical simulation enables humanoid robots to accurately and stably perform corresponding actions. It is then deployed on real robots via a modular simulation-to-reality module. We extensively evaluate FRoM-W1 on Unitree H1 and G1 robots. Results demonstrate superior performance on the HumanML3D-X benchmark for human whole-body motion generation, and our introduced reinforcement learning fine-tuning consistently improves both motion tracking accuracy and task success rates of these humanoid robots. We open-source the entire FRoM-W1 framework and hope it will advance the development of humanoid intelligence.

</details>


### [477] [Sparse ActionGen: Accelerating Diffusion Policy with Real-time Pruning](https://arxiv.org/abs/2601.12894)
*Kangye Ji,Yuan Meng,Zhou Jianbo,Ye Li,Hanyun Cui,Zhi Wang*

Main category: cs.RO

TL;DR: 本文提出了一种名为SAG（Sparse ActionGen）的新方法，大幅提升了扩散策略在机器人视觉动作生成任务中的实时性，同时保持性能。通过智能裁剪和重用计算，有效减少了冗余，提升了速度。实验表明在多个机器人基准上速度提升可达4倍，且没有性能损失。


<details>
  <summary>Details</summary>
Motivation: 当前扩散策略在动作生成上表现优异，但其多步去噪导致推理速度慢，难以用于实时机器人控制。已有的加速方法多基于静态缓存，缺乏对环境动态的自适应处理，效果有限。因此需要一种既高效又能动态适应环境变化的动作生成方法。

Method: SAG方法通过定制化、回合自适应的剪枝及重用机制进行加速。首先全局识别可剪枝的计算步骤，在动作生成过程中用缓存激活值替代这些部分。为适应环境动态，SAG设置了与观测相关的扩散剪枝器，并采用高效设计保证实时推理。同时提出“一体化重用”策略，跨步、跨模块以曲折方式最大化计算重用，压缩全局冗余。

Result: 在多个机器人基准实验中，SAG方法可以将动作生成速度提升至原扩散策略的4倍，而性能（如决策质量）不受影响。

Conclusion: SAG为实时机器人动作生成提供了有效手段，兼顾加速与性能，具有广泛实际应用前景。

Abstract: Diffusion Policy has dominated action generation due to its strong capabilities for modeling multi-modal action distributions, but its multi-step denoising processes make it impractical for real-time visuomotor control. Existing caching-based acceleration methods typically rely on $\textit{static}$ schedules that fail to adapt to the $\textit{dynamics}$ of robot-environment interactions, thereby leading to suboptimal performance. In this paper, we propose $\underline{\textbf{S}}$parse $\underline{\textbf{A}}$ction$\underline{\textbf{G}}$en ($\textbf{SAG}$) for extremely sparse action generation. To accommodate the iterative interactions, SAG customizes a rollout-adaptive prune-then-reuse mechanism that first identifies prunable computations globally and then reuses cached activations to substitute them during action diffusion. To capture the rollout dynamics, SAG parameterizes an observation-conditioned diffusion pruner for environment-aware adaptation and instantiates it with a highly parameter- and inference-efficient design for real-time prediction. Furthermore, SAG introduces a one-for-all reusing strategy that reuses activations across both timesteps and blocks in a zig-zag manner, minimizing the global redundancy. Extensive experiments on multiple robotic benchmarks demonstrate that SAG achieves up to 4$\times$ generation speedup without sacrificing performance. Project Page: https://sparse-actiongen.github.io/.

</details>


### [478] [PlannerRFT: Reinforcing Diffusion Planners through Closed-Loop and Sample-Efficient Fine-Tuning](https://arxiv.org/abs/2601.12901)
*Hongchen Li,Tianyu Li,Jiazhi Yang,Haochen Tian,Caojun Wang,Lei Shi,Mingyang Shang,Zengrong Lin,Gaoqiang Wu,Zhihui Hao,Xianpeng Lang,Jia Hu,Hongyang Li*

Main category: cs.RO

TL;DR: 本文提出了PlannerRFT框架，对扩散式规划器进行强化微调，实现更高效且多样的人类风格轨迹生成，并开发了高效模拟器nuMax以加快训练速度。


<details>
  <summary>Details</summary>
Motivation: 现有扩散式规划器难以高效利用强化学习奖励，且在多模态和场景适应性上表现有限，影响了自动驾驶轨迹生成的质量和鲁棒性，因此亟需提升其采样效率和多样性。

Method: 提出了PlannerRFT框架，引入双分支优化机制，一方面优化轨迹分布，另一方面自适应地引导去噪过程，从而提升探索效率，同时保持原有推理流程不变。并开发了nuMax模拟器，支持规模化并行训练，速度比原生nuPlan快10倍。

Result: 实验结果显示，PlannerRFT在大规模测试中显著提升了性能，还观察到系统在学习过程中呈现出不同的行为模式，表现出更优的多样性和场景适应性。

Conclusion: PlannerRFT结合采样效率和奖励利用率，在自动驾驶扩散式轨迹生成领域达到了最新的性能水平，为强化微调和高效仿真提供了可行方案。

Abstract: Diffusion-based planners have emerged as a promising approach for human-like trajectory generation in autonomous driving. Recent works incorporate reinforcement fine-tuning to enhance the robustness of diffusion planners through reward-oriented optimization in a generation-evaluation loop. However, they struggle to generate multi-modal, scenario-adaptive trajectories, hindering the exploitation efficiency of informative rewards during fine-tuning. To resolve this, we propose PlannerRFT, a sample-efficient reinforcement fine-tuning framework for diffusion-based planners. PlannerRFT adopts a dual-branch optimization that simultaneously refines the trajectory distribution and adaptively guides the denoising process toward more promising exploration, without altering the original inference pipeline. To support parallel learning at scale, we develop nuMax, an optimized simulator that achieves 10 times faster rollout compared to native nuPlan. Extensive experiments shows that PlannerRFT yields state-of-the-art performance with distinct behaviors emerging during the learning process.

</details>


### [479] [Dynamic Hand Gesture Recognition for Robot Manipulator Tasks](https://arxiv.org/abs/2601.12918)
*Dharmendra Sharma,Peeyush Thakur,Sandeep Gupta,Narendra Kumar Dhar,Laxmidhar Behera*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的动态手势识别方法，可实现人机无缝互动，并通过高斯混合模型的无监督学习方式实现多任务、多变体手势的实时准确识别。


<details>
  <summary>Details</summary>
Motivation: 提升机器人与人类之间的交互体验，允许用户通过不同的动态手势指令操控机器人完成多种任务。

Method: 提出基于高斯混合模型（Gaussian Mixture Model，GMM）的无监督动态手势识别模型，将不同操控任务映射为不同手势，并能识别每种手势的多种动态变化。

Result: 训练和实时测试中均展现出了较高的手势识别准确率，验证了该方法的有效性。

Conclusion: 该无监督手势识别方法能够实时、精准地识别多任务、多变化的动态手势，有助于推动人机交互在机器人操控中的应用。

Abstract: This paper proposes a novel approach to recognizing dynamic hand gestures facilitating seamless interaction between humans and robots. Here, each robot manipulator task is assigned a specific gesture. There may be several such tasks, hence, several gestures. These gestures may be prone to several dynamic variations. All such variations for different gestures shown to the robot are accurately recognized in real-time using the proposed unsupervised model based on the Gaussian Mixture model. The accuracy during training and real-time testing prove the efficacy of this methodology.

</details>


### [480] [ForeDiffusion: Foresight-Conditioned Diffusion Policy via Future View Construction for Robot Manipulation](https://arxiv.org/abs/2601.12925)
*Weize Xie,Yi Ding,Ying He,Leilei Wang,Binwen Bai,Zheyi Zhao,Chenyang Wang,F. Richard Yu*

Main category: cs.RO

TL;DR: 本论文提出一种新方法ForeDiffusion，通过引入预测的未来视角信息和双重损失机制，有效提升了复杂机器人操作任务的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有扩散策略在机器人操作复杂情况下，成功率明显下降。原因包括：仅依赖短期观测信息进行决策，以及训练时只用单一去噪损失，导致误差累积与抓取偏差。

Method: 提出Foresight-Conditioned Diffusion（ForeDiffusion）方法，将预测的未来视野表示融入扩散过程，引导策略具备前瞻性，自动修正轨迹偏差。同时，设计了双损失机制：传统去噪损失+未来观测一致性损失，实现统一优化。

Result: 在Adroit和MetaWorld基准测试中，ForeDiffusion平均任务成功率达到80%，复杂任务中较主流方法提升23%，整体表现更稳定。

Conclusion: ForeDiffusion通过前瞻视角与双损失机制，有效缓解了以往扩散策略在复杂任务中的局限性，大幅提升了机器人操作的成功率及稳定性。

Abstract: Diffusion strategies have advanced visual motor control by progressively denoising high-dimensional action sequences, providing a promising method for robot manipulation. However, as task complexity increases, the success rate of existing baseline models decreases considerably. Analysis indicates that current diffusion strategies are confronted with two limitations. First, these strategies only rely on short-term observations as conditions. Second, the training objective remains limited to a single denoising loss, which leads to error accumulation and causes grasping deviations. To address these limitations, this paper proposes Foresight-Conditioned Diffusion (ForeDiffusion), by injecting the predicted future view representation into the diffusion process. As a result, the policy is guided to be forward-looking, enabling it to correct trajectory deviations. Following this design, ForeDiffusion employs a dual loss mechanism, combining the traditional denoising loss and the consistency loss of future observations, to achieve the unified optimization. Extensive evaluation on the Adroit suite and the MetaWorld benchmark demonstrates that ForeDiffusion achieves an average success rate of 80% for the overall task, significantly outperforming the existing mainstream diffusion methods by 23% in complex tasks, while maintaining more stable performance across the entire tasks.

</details>


### [481] [Active Inference-Driven World Modeling for Adaptive UAV Swarm Trajectory Design](https://arxiv.org/abs/2601.12939)
*Kaleem Arshid,Ali Krayani,Lucio Marcenaro,David Martin Gomez,Carlo Regazzoni*

Main category: cs.RO

TL;DR: 提出了一种基于主动推理的无人机集群自主轨迹设计框架，实现了任务分配、路径排序和运动规划的自学习与适应控制，并优于Q-Learning。


<details>
  <summary>Details</summary>
Motivation: 面对动态环境下无人机集群任务自主分配和运动规划的挑战，传统方法在适应性和智能水平上存在不足，亟需一种更自适应、智能化的控制方案。

Method: 采用主动推理框架，结合概率推理与自学习方法，通过遗传算法与排斥力（GA-RF）生成专家轨迹，训练分层世界模型（涵盖任务、路径和运动三个层级），并在线通过最小化真实与预测状态间的差异推断动作，实现分布式、自适应的集群任务规划与运动决策。

Result: 仿真结果显示，该方法在收敛速度、系统稳定性和导航安全性等方面均优于Q-Learning，并具备更好的扩展性和认知基础。

Conclusion: 该主动推理框架可为无人机集群智能自主控制提供有效支持，具备更强的自适应能力和安全性，适用于复杂动态场景。

Abstract: This paper proposes an Active Inference-based framework for autonomous trajectory design in UAV swarms. The method integrates probabilistic reasoning and self-learning to enable distributed mission allocation, route ordering, and motion planning. Expert trajectories generated using a Genetic Algorithm with Repulsion Forces (GA-RF) are employed to train a hierarchical World Model capturing swarm behavior across mission, route, and motion levels. During online operation, UAVs infer actions by minimizing divergence between current beliefs and model-predicted states, enabling adaptive responses to dynamic environments. Simulation results show faster convergence, higher stability, and safer navigation than Q-Learning, demonstrating the scalability and cognitive grounding of the proposed framework for intelligent UAV swarm control.

</details>


### [482] [Imitation learning-based spacecraft rendezvous and docking method with Expert Demonstration](https://arxiv.org/abs/2601.12952)
*Shibo Shao,Dong Zhou,Guanghui Sun,Liwen Zhang,Mingxuan Jiang*

Main category: cs.RO

TL;DR: 本文提出了一种基于模仿学习的航天器交会对接控制框架，无需精确动力学建模，实现了在6自由度下的稳健、高效控制。


<details>
  <summary>Details</summary>
Motivation: 现有航天器交会对接控制方法主要依赖预先定义的动力学模型，实际轨道环境下鲁棒性有限，难以应对各种未知干扰。

Method: 提出IL-SRD框架，通过模仿学习直接从专家示范中学习控制策略，引入锚定解码器目标机制（以状态相关锚点指导输出）和时序聚合机制（减少变换器模型的误差积累），提升控制动作的物理一致性和整体稳定性。

Result: 大量仿真结果表明，IL-SRD在无需动力学模型的条件下，能够精确、高能效地完成6自由度交会对接任务；鲁棒性测试表明，在强未知干扰下也能维持优良性能。

Conclusion: IL-SRD框架降低了对精确建模的依赖，通过新颖的结构设计提升了物理一致性和鲁棒性，为实际航天器交会对接控制提供了有效的智能方法。

Abstract: Existing spacecraft rendezvous and docking control methods largely rely on predefined dynamic models and often exhibit limited robustness in realistic on-orbit environments. To address this issue, this paper proposes an Imitation Learning-based spacecraft rendezvous and docking control framework (IL-SRD) that directly learns control policies from expert demonstrations, thereby reducing dependence on accurate modeling. We propose an anchored decoder target mechanism, which conditions the decoder queries on state-related anchors to explicitly constrain the control generation process. This mechanism enforces physically consistent control evolution and effectively suppresses implausible action deviations in sequential prediction, enabling reliable six-degree-of-freedom (6-DOF) rendezvous and docking control. To further enhance stability, a temporal aggregation mechanism is incorporated to mitigate error accumulation caused by the sequential prediction nature of Transformer-based models, where small inaccuracies at each time step can propagate and amplify over long horizons. Extensive simulation results demonstrate that the proposed IL-SRD framework achieves accurate and energy-efficient model-free rendezvous and docking control. Robustness evaluations further confirm its capability to maintain competitive performance under significant unknown disturbances. The source code is available at https://github.com/Dongzhou-1996/IL-SRD.

</details>


### [483] [Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization](https://arxiv.org/abs/2601.12993)
*Hao Luo,Ye Wang,Wanpeng Zhang,Sipeng Zheng,Ziheng Xi,Chaoyi Xu,Haiweng Xu,Haoqi Yuan,Chi Zhang,Yiqing Wang,Yicheng Feng,Zongqing Lu*

Main category: cs.RO

TL;DR: 提出了Being-H0.5，一个具备跨多种机器人平台泛化能力的视觉-语言-动作大模型，通过以人为中心的“母语”范式和统一动作空间，实现了多机器人技能迁移，并在多基准任务上取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作（VLA）模型在面对机器人形态差异和数据稀缺时泛化能力较弱。论文希望解决因机器人平台多样性导致的技能迁移难题，通过以人为中心的数据统一各类机器人技能的表达方式。

Method: 1. 提出以人为中心的学习范式，将人类交互轨迹作为物理交互的“母语”；2. 构建UniHand-2.0，包含逾3.5万小时、30种机器人体现的多模态数据；3. 设计统一动作空间，将不同机器人的控制信号以语义对齐槽位的方式映射，实现技能迁移；4. 模型架构采用Mixure-of-Transformers与新颖的Mixure-of-Flow模块，分离通用运动原语与体现特定专家；5. 引入流形保持门控与通用异步分段提升政策的稳健性和通用性。

Result: Being-H0.5在仿真基准LIBERO（98.9%）和RoboCasa（53.9%）上取得了SOTA成绩，并在5种机器人平台上展现了强大的跨体现泛化能力。

Conclusion: 该方法有效提升了跨机器人平台的泛化与技能迁移能力，为实现通用型AI机器人系统奠定了基础。

Abstract: We introduce Being-H0.5, a foundational Vision-Language-Action (VLA) model designed for robust cross-embodiment generalization across diverse robotic platforms. While existing VLAs often struggle with morphological heterogeneity and data scarcity, we propose a human-centric learning paradigm that treats human interaction traces as a universal "mother tongue" for physical interaction. To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments. Our approach introduces a Unified Action Space that maps heterogeneous robot controls into semantically aligned slots, enabling low-resource robots to bootstrap skills from human data and high-resource platforms. Built upon this human-centric foundation, we design a unified sequential modeling and multi-task pre-training paradigm to bridge human demonstrations and robotic execution. Architecturally, Being-H0.5 utilizes a Mixture-of-Transformers design featuring a novel Mixture-of-Flow (MoF) framework to decouple shared motor primitives from specialized embodiment-specific experts. Finally, to make cross-embodiment policies stable in the real world, we introduce Manifold-Preserving Gating for robustness under sensory shift and Universal Async Chunking to universalize chunked control across embodiments with different latency and control profiles. We empirically demonstrate that Being-H0.5 achieves state-of-the-art results on simulated benchmarks, such as LIBERO (98.9%) and RoboCasa (53.9%), while also exhibiting strong cross-embodiment capabilities on five robotic platforms.

</details>


### [484] [Static Is Not Enough: A Comparative Study of VR and SpaceMouse in Static and Dynamic Teleoperation Tasks](https://arxiv.org/abs/2601.13042)
*Yijun Zhou,Muhan Hou,Kim Baraka*

Main category: cs.RO

TL;DR: 本文研究远程操作界面对模仿学习中动态任务示范数据质量的影响，结果显示VR设备在动态任务中的成功率和可用性明显优于SpaceMouse，并开源了适用于动态任务的VR操作系统。


<details>
  <summary>Details</summary>
Motivation: 虽然模仿学习依赖高质量演示数据，且远程操作（teleoperation）是采集演示的重要工具，但先前研究多聚焦于静态任务（如离散、分段运动），缺乏对动态、需要反馈控制任务的界面需求探讨。因此，静态任务中的界面评估结果不能推广到动态任务。本研究旨在弥补这一差距，系统性比较不同界面在静态与动态任务中的实际表现。

Method: 作者设计了被试内对比实验，让25名参与者分别用VR控制器和SpaceMouse完成两个静态和两个动态任务，并评估了成功率、任务耗时和累计成功数。同时收集NASA-TLX（工作负荷）、SUS（可用性）问卷及开放性反馈，以全面衡量界面优劣。

Result: 结果显示，VR控制器在总体成功率、动态任务成功率、任务执行时间、首次成功次数等方面均显著优于SpaceMouse，并表现出更低的主观工作负荷和更高的界面可用性。

Conclusion: 针对动态任务，VR控制器在性能和用户体验上均优于SpaceMouse。为推动相关研究，论文发布了适用于动态任务的开源VR远程操作系统，填补了现有开源资源的空白。

Abstract: Imitation learning relies on high-quality demonstrations, and teleoperation is a primary way to collect them, making teleoperation interface choice crucial for the data. Prior work mainly focused on static tasks, i.e., discrete, segmented motions, yet demonstrations also include dynamic tasks requiring reactive control. As dynamic tasks impose fundamentally different interface demands, insights from static-task evaluations cannot generalize. To address this gap, we conduct a within-subjects study comparing a VR controller and a SpaceMouse across two static and two dynamic tasks ($N=25$). We assess success rate, task duration, cumulative success, alongside NASA-TLX, SUS, and open-ended feedback. Results show statistically significant advantages for VR: higher success rates, particularly on dynamic tasks, shorter successful execution times across tasks, and earlier successes across attempts, with significantly lower workload and higher usability. As existing VR teleoperation systems are rarely open-source or suited for dynamic tasks, we release our VR interface to fill this gap.

</details>


### [485] [Exploiting Light To Enhance The Endurance and Navigation of Lighter-Than-Air Micro-Drones](https://arxiv.org/abs/2601.13088)
*Harry Huang,Talia Xu,Marco Zúñiga Zamalloa*

Main category: cs.RO

TL;DR: 本文提出了一种利用光能实现能源自给及导航的轻于空气（LTA）自主无人机系统，并通过模拟与实物验证了其持续、节能运行及导航能力。


<details>
  <summary>Details</summary>
Motivation: 微型无人机在库存管理和环境监测等场景应用广泛，但因续航能力有限且在无GPS环境下导航不可靠，部署受限。轻于空气的无人机虽然能量效率高、续航长，但设计复杂且缺乏低门槛的自主运行方案。

Method: 1）开发高保真仿真框架，分析和优化LTA空气动力学配置；2）在气囊表面集成太阳能组件，确保净正能量输出；3）基于单点光源的导航系统，实现三种寻光算法以引导无人机自定位。

Result: 在80klux光照下，每4分钟采集能量可支持1分钟飞行，系统在有风的室内外环境下，能以单点光源作导航，最远稳定导航距离7米，具备较强的鲁棒性。

Conclusion: 该系统为LTA无人机实现持久、自治的室内外监测功能提供了可行路径，为LTA技术实际应用铺平了道路。

Abstract: Micro-Unmanned Aerial Vehicles (UAVs) are rapidly expanding into tasks from inventory to environmental sensing, yet their short endurance and unreliable navigation in GPS-denied spaces limit deployment. Lighter-Than-Air (LTA) drones offer an energy-efficient alternative: they use a helium envelope to provide buoyancy, which enables near-zero-power drain during hovering and much longer operation. LTAs are promising, but their design is complex, and they lack integrated solutions to enable sustained autonomous operations and navigation with simple, low-infrastructure.
  We propose a compact, self-sustaining LTA drone that uses light for both energy harvesting and navigation. Our contributions are threefold: (i) a high-fidelity simulation framework to analyze LTA aerodynamics and select a stable, efficient configuration; (ii) a framework to integrate solar cells on the envelope to provide net-positive energy; and (iii) a point-and-go navigation system with three light-seeking algorithms operating on a single light beacon.
  Our LTA-analysis, together with the integrated solar panels, not only saves energy while flying, but also enables sustainable operation: providing 1 minute of flying time for every 4 minutes of energy harvesting, under illuminations of 80klux. We also demonstrate robust single-beacon navigation towards a light source that can be up to 7m away, in indoor and outdoor environments, even with moderate winds. The resulting system indicates a plausible path toward persistent, autonomous operation for indoor and outdoor monitoring. More broadly, this work provides a practical pathway for translating the promise of LTA drones into a persistent, self-sustaining aerial system.

</details>


### [486] [LLM-VLM Fusion Framework for Autonomous Maritime Port Inspection using a Heterogeneous UAV-USV System](https://arxiv.org/abs/2601.13096)
*Muhayy Ud Din,Waseem Akram,Ahsan B. Bakht,Irfan Hussain*

Main category: cs.RO

TL;DR: 本论文提出了一个结合大语言模型（LLMs）与视觉-语言模型（VLMs）的新型工程框架，实现无人机与无人船协作下的自主海港巡检。新框架用LLM进行任务规划、VLM提升语义感知，从而增强自动化、上下文理解与适应能力。方法经仿真与实地验证，展现了智能巡检系统的可行性与高效性。


<details>
  <summary>Details</summary>
Motivation: 现有海港巡检方法主要依赖人工与传统视觉技术，缺乏扩展性与上下文理解能力，难以满足复杂场景下对安全、合规和高效运营的需求。因此需要开发集成智能模型和多机器人协作的自动化巡检方案。

Method: 提出利用LLMs进行符号化任务规划，将自然语言任务指令转化为包含依赖关系图的可执行计划，保障多平台协同安全。VLM模块则完成实时语义巡检和合规评估，实现结构化报告生成。整个系统在仿真平台和真实机器人巡检中评估，且针对资源约束环境进行了一体化轻量化设计。

Result: 在MBZIRC海港仿真环境和实际机器人试验中，该集成框架表现出更强的自主性、语义理解与任务适应能力，能够有效支持复杂港口设施的巡检工作。所有设计均兼顾运算资源受限平台的适用性。相关开源资源已发布。

Conclusion: 结合LLM与VLM的协作式自主巡检框架有效提升了海港智能监控的自动化与智能化水平，为智能无人系统在实际工业巡检中的应用奠定了基础。解决了传统方法在理解和大规模协作上的局限，为未来智能港口管理提供了新方向。

Abstract: Maritime port inspection plays a critical role in ensuring safety, regulatory compliance, and operational efficiency in complex maritime environments. However, existing inspection methods often rely on manual operations and conventional computer vision techniques that lack scalability and contextual understanding. This study introduces a novel integrated engineering framework that utilizes the synergy between Large Language Models (LLMs) and Vision Language Models (VLMs) to enable autonomous maritime port inspection using cooperative aerial and surface robotic platforms. The proposed framework replaces traditional state-machine mission planners with LLM-driven symbolic planning and improved perception pipelines through VLM-based semantic inspection, enabling context-aware and adaptive monitoring. The LLM module translates natural language mission instructions into executable symbolic plans with dependency graphs that encode operational constraints and ensure safe UAV-USV coordination. Meanwhile, the VLM module performs real-time semantic inspection and compliance assessment, generating structured reports with contextual reasoning. The framework was validated using the extended MBZIRC Maritime Simulator with realistic port infrastructure and further assessed through real-world robotic inspection trials. The lightweight on-board design ensures suitability for resource-constrained maritime platforms, advancing the development of intelligent, autonomous inspection systems. Project resources (code and videos) can be found here: https://github.com/Muhayyuddin/llm-vlm-fusion-port-inspection

</details>


### [487] [Helical Tendon-Driven Continuum Robot with Programmable Follow-the-Leader Operation](https://arxiv.org/abs/2601.13177)
*Behnam Moradkhani,Raghav Sankaranarayanan,Pejman Kheradmand,Harshith Jella,Nicholas Ahn,Ajmal Zemmar,Yash Chitalia*

Main category: cs.RO

TL;DR: 本文提出了一种基于静力学建模的新型可操控机器人ExoNav，用于精准引导脊髓刺激导管至腹侧或侧向硬膜外腔，有望提升脊髓刺激疗效。


<details>
  <summary>Details</summary>
Motivation: 传统脊髓刺激（SCS）导管手动导航难以准确到达腹侧或侧向硬膜外区域，这限制了其在运动神经刺激和功能恢复中的应用。

Method: 采用Cosserat杆理论建立ExoNav机器人肌腱驱动与整体形态间的关系模型，并结合重力影响进行了建模和仿真；开发可实现插入和旋转自由度的机器人，实现前导式“跟随引导”运动；通过4种原型实验与仿真验证精度，并在实体模型中进行远程操控测试。

Result: 实验中4类原型机器人定位均方根误差（RMSE）分别为1.76mm、2.33mm、2.18mm、1.33mm；前导式运动最大RMSE 3.75mm；机器人在模拟脊髓模型中能精准导航至目标区域，包括运动与痛觉管理靶点。

Conclusion: ExoNav机器人系统具备高精度引导能力，可有效到达脊髓腹侧及侧向目标，具备改善SCS运动功能恢复及疼痛管理前景。

Abstract: Spinal cord stimulation (SCS) is primarily utilized for pain management and has recently demonstrated efficacy in promoting functional recovery in patients with spinal cord injury. Effective stimulation of motor neurons ideally requires the placement of SCS leads in the ventral or lateral epidural space where the corticospinal and rubrospinal motor fibers are located. This poses significant challenges with the current standard of manual steering. In this study, we present a static modeling approach for the ExoNav, a steerable robotic tool designed to facilitate precise navigation to the ventral and lateral epidural space. Cosserat rod framework is employed to establish the relationship between tendon actuation forces and the robot's overall shape. The effects of gravity, as an example of an external load, are investigated and implemented in the model and simulation. The experimental results indicate RMSE values of 1.76mm, 2.33mm, 2.18mm, and 1.33mm across four tested prototypes. Based on the helical shape of the ExoNav upon actuation, it is capable of performing follow-the-leader (FTL) motion by adding insertion and rotation DoFs to this robotic system, which is shown in simulation and experimentally. The proposed simulation has the capability to calculate optimum tendon tensions to follow the desired FTL paths while gravity-induced robot deformations are present. Three FTL experimental trials are conducted and the end-effector position showed repeatable alignments with the desired path with maximum RMSE value of 3.75mm. Ultimately, a phantom model demonstration is conducted where the teleoperated robot successfully navigated to the lateral and ventral spinal cord targets. Additionally, the user was able to navigate to the dorsal root ganglia, illustrating ExoNav's potential in both motor function recovery and pain management.

</details>


### [488] [Active Informative Planning for UAV-based Weed Mapping using Discrete Gaussian Process Representations](https://arxiv.org/abs/2601.13196)
*Jacob Swindell,Marija Popović,Riccardo Polvara*

Main category: cs.RO

TL;DR: 本文探讨了在无人机农业杂草监测中，采用不同的高斯过程（GP）离散化方法对地图质量和任务表现的影响，发现离散化策略对规划和覆盖效率有重要作用。


<details>
  <summary>Details</summary>
Motivation: 精准农业需精确掌握杂草分布，无人机与高斯过程结合能实现智能采样。但现有方法对GP离散化方式影响关注不足，明确这一问题有助于提升任务规划效率。

Method: 基于无人机搭载下视相机，采用递归视野规划（receding-horizon IPP），结合不同GP后验离散化方法，综合考虑地图不确定性、航程成本、覆盖惩罚，选取采样点。通过真实杂草分布数据实验，比较各离散化方案在实际任务中的表现。

Result: 实验结果显示，不同的GP离散化方式对探索行为、规划效率和地图覆盖均有显著影响，某些离散方案能有效提升任务表现与计算效率。

Conclusion: GP离散化不只是技术细节，而是关乎规划行为、覆盖效率和计算负担的关键设计选择，对无人机在线杂草监测规划至关重要。

Abstract: Accurate agricultural weed mapping using unmanned aerial vehicles (UAVs) is crucial for precision farming. While traditional methods rely on rigid, pre-defined flight paths and intensive offline processing, informative path planning (IPP) offers a way to collect data adaptively where it is most needed. Gaussian process (GP) mapping provides a continuous model of weed distribution with built-in uncertainty. However, GPs must be discretised for practical use in autonomous planning. Many discretisation techniques exist, but the impact of discrete representation choice remains poorly understood. This paper investigates how different discrete GP representations influence both mapping quality and mission-level performance in UAV-based weed mapping. Considering a UAV equipped with a downward-facing camera, we implement a receding-horizon IPP strategy that selects sampling locations based on the map uncertainty, travel cost, and coverage penalties. We investigate multiple discretisation strategies for representing the GP posterior and use their induced map partitions to generate candidate viewpoints for planning. Experiments on real-world weed distributions show that representation choice significantly affects exploration behaviour and efficiency. Overall, our results demonstrate that discretisation is not only a representational detail but a key design choice that shapes planning dynamics, coverage efficiency, and computational load in online UAV weed mapping.

</details>


### [489] [MATTERIX: toward a digital twin for robotics-assisted chemistry laboratory automation](https://arxiv.org/abs/2601.13232)
*Kourosh Darvish,Arjun Sohal,Abhijoy Mandal,Hatem Fakhruldeen,Nikola Radulov,Zhengxue Zhou,Satheeshkumar Veeramani,Joshua Choi,Sijie Han,Brayden Zhang,Jeeyeoun Chae,Alex Wright,Yijie Wang,Hossein Darvish,Yuchi Zhao,Gary Tom,Han Hao,Miroslav Bogdanovic,Gabriella Pizzuto,Andrew I. Cooper,Alán Aspuru-Guzik,Florian Shkurti,Animesh Garg*

Main category: cs.RO

TL;DR: MATTERIX是一个多尺度GPU加速的机器人仿真框架，可创建化学实验室的高保真数字孪生体，显著简化和加速材料发现实验流程。


<details>
  <summary>Details</summary>
Motivation: 传统新材料实验流程需要大量实际实验，费时费力且难以规模化。因此，迫切需要一种数字化、高效和可扩展的方法来优化实验流程，降低成本和时间消耗。

Method: MATTERIX整合了现实物理仿真、光真实渲染及GPU加速的语义引擎，能多尺度模拟实验室中的机器人操作、粉末与液体动力学、设备功能、传热与基础化学反应动力学。系统还支持通过开源资产库和接口快速搭建环境，通过分层计划和模块化技能库实现灵活的流程设计，支持学习型方法。

Result: 该方法实现了从仿真到现实的有效迁移，能在不用过多实际实验的情况下，测试和优化机器人自动化实验流程，减少了现实实验的需求和成本。

Conclusion: MATTERIX极大地推动了化学实验室数字孪生和流程自动化开发，提高了材料发现的效率和可扩展性，为数字化自动化化学研究提供了新的工具和范例。

Abstract: Accelerated materials discovery is critical for addressing global challenges. However, developing new laboratory workflows relies heavily on real-world experimental trials, and this can hinder scalability because of the need for numerous physical make-and-test iterations. Here we present MATTERIX, a multiscale, graphics processing unit-accelerated robotic simulation framework designed to create high-fidelity digital twins of chemistry laboratories, thus accelerating workflow development. This multiscale digital twin simulates robotic physical manipulation, powder and liquid dynamics, device functionalities, heat transfer and basic chemical reaction kinetics. This is enabled by integrating realistic physics simulation and photorealistic rendering with a modular graphics processing unit-accelerated semantics engine, which models logical states and continuous behaviors to simulate chemistry workflows across different levels of abstraction. MATTERIX streamlines the creation of digital twin environments through open-source asset libraries and interfaces, while enabling flexible workflow design via hierarchical plan definition and a modular skill library that incorporates learning-based methods. Our approach demonstrates sim-to-real transfer in robotic chemistry setups, reducing reliance on costly real-world experiments and enabling the testing of hypothetical automated workflows in silico. The project website is available at https://accelerationconsortium.github.io/Matterix/ .

</details>


### [490] [Diffusion-based Inverse Model of a Distributed Tactile Sensor for Object Pose Estimation](https://arxiv.org/abs/2601.13250)
*Ante Marić,Giammarco Caroleo,Alessandro Albini,Julius Jankowski,Perla Maiolino,Sylvain Calinon*

Main category: cs.RO

TL;DR: 本文提出了一种基于扩散模型的逆向触觉传感器模型，用于在缺乏视觉信息情况下提升物体位姿估计的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 在机器人操作中，因遮挡或环境影响，视觉信息常常不完整，触觉成为重要的补充传感方式。但触觉的观测信息部分可见、对应多种接触配置，复杂性较高，导致传统以视觉为主的方法难以高效利用触觉信息进行位姿估计。

Method: 作者利用扩散去噪模型，学习逆向触觉传感器模型。该模型以分布式触觉传感器的观测为条件，基于有符号距离场的几何传感器模型在仿真中进行训练。推理阶段通过单步投影结合距离及梯度信息来保证接触约束。在线位姿估计时，将逆向模型与粒子滤波融合，通过生成假设结合先验粒子进行提案更新。

Result: 该方法在无视觉信息或无精确初值先验的仿真和真实平面位姿估计中得到有效验证，并在箱体推运动景下评估了对未建模接触和传感器动态的鲁棒性。相比现有局部采样方法，提出方法在多物体场景下提升了采样效率、估计准确性，并能维持多峰信念分布。

Conclusion: 基于扩散模型的逆向触觉传感器模型可在视觉受限条件下，有效利用触觉数据进行多模态物体位姿估计，提高了鲁棒性与精度，为机器人操作系统提供了更强的感知能力。

Abstract: Tactile sensing provides a promising sensing modality for object pose estimation in manipulation settings where visual information is limited due to occlusion or environmental effects. However, efficiently leveraging tactile data for estimation remains a challenge due to partial observability, with single observations corresponding to multiple possible contact configurations. This limits conventional estimation approaches largely tailored to vision. We propose to address these challenges by learning an inverse tactile sensor model using denoising diffusion. The model is conditioned on tactile observations from a distributed tactile sensor and trained in simulation using a geometric sensor model based on signed distance fields. Contact constraints are enforced during inference through single-step projection using distance and gradient information from the signed distance field. For online pose estimation, we integrate the inverse model with a particle filter through a proposal scheme that combines generated hypotheses with particles from the prior belief. Our approach is validated in simulated and real-world planar pose estimation settings, without access to visual data or tight initial pose priors. We further evaluate robustness to unmodeled contact and sensor dynamics for pose tracking in a box-pushing scenario. Compared to local sampling baselines, the inverse sensor model improves sampling efficiency and estimation accuracy while preserving multimodal beliefs across objects with varying tactile discriminability.

</details>


### [491] [Autonomous Navigation at the Nano-Scale: Algorithms, Architectures, and Constraints](https://arxiv.org/abs/2601.13252)
*Mahmud S. Zango,Jianglin Lan*

Main category: cs.RO

TL;DR: 本文综述了面向纳米级无人机（nano-UAVs）自主导航的最新传感、计算和控制技术，聚焦于极低算力（<100 mW）平台上的智能化方法与硬件-软件协同设计，剖析了当前进展与存在的关键挑战，并提出融合传统控制与数据驱动感知的未来发展路径。


<details>
  <summary>Details</summary>
Motivation: 纳米无人机因体积、重量和能耗限制（如重量<50g，算力<100 mW）不能采用常规的机器人自主导航方案，迫切需要在极低资源约束下研发专属的感知、计算和控制框架以实现真正的自主智能飞行。

Method: 作者系统评述了视觉感知、光流、紧凑化SLAM、学习型飞控、量化神经网络、超低功耗SoC部署和类脑事件式控制等最新算法与技术，并分析了硬件与软件协同优化对于实现自主性的必要性。特别强调了“Edge AI”范式在纳米无人机上的应用和从几何方法向神经网络转变的趋势。

Result: 在视觉导航和相对位姿估计方面已取得实质性进展，但在动态环境下的鲁棒避障、长时间续航、以及强化学习策略的“仿真-实物”迁移方面仍存在明显不足。这些挑战限制了纳米无人机在复杂无GPS环境下的完全自主飞行。

Conclusion: 作者提出未来应采取轻量级传统控制策略与高效数据驱动感知深度融合的混合架构，实现灵活、可靠的自主导航，并为弥合当前技术差距规划了研究路线图。该综述为相关领域研究人员提供了全面的展望与发展指导。

Abstract: Autonomous navigation for nano-scale unmanned aerial vehicles (nano-UAVs) is governed by extreme Size, Weight, and Power (SWaP) constraints (with the weight < 50 g and sub-100 mW onboard processor), distinguishing it fundamentally from standard robotic paradigms. This review synthesizes the state-of-the-art in sensing, computing, and control architectures designed specifically for these sub- 100mW computational envelopes. We critically analyse the transition from classical geometry-based methods to emerging "Edge AI" paradigms, including quantized deep neural networks deployed on ultra-low-power System-on-Chips (SoCs) and neuromorphic event-based control. Beyond algorithms, we evaluate the hardware-software co-design requisite for autonomy, covering advancements in dense optical flow, optimized Simultaneous Localization and Mapping (SLAM), and learning-based flight control. While significant progress has been observed in visual navigation and relative pose estimation, our analysis reveals persistent gaps in long-term endurance, robust obstacle avoidance in dynamic environments, and the "Sim-to-Real" transfer of reinforcement learning policies. This survey provides a roadmap for bridging these gaps, advocating for hybrid architectures that fuse lightweight classical control with data-driven perception to enable fully autonomous, agile nano-UAVs in GPS-denied environments.

</details>


### [492] [CLEAR: A Semantic-Geometric Terrain Abstraction for Large-Scale Unstructured Environments](https://arxiv.org/abs/2601.13361)
*Pranay Meshram,Charuvahan Adhivarahan,Ehsan Tarkesh Esfahani,Souma Chowdhury,Chen Wang,Karthik Dantu*

Main category: cs.RO

TL;DR: 提出了CLEAR方法，在大规模非结构化环境中实现了快速、准确的地形抽象，对自主地面车辆的长距离导航效果显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有地形抽象方法在大面积（>10km²）导航任务中存在诸多不足，如网格扩展性差、四叉树边界失配、缺乏关键地表语义信息，导致路径不可行或不可靠，制约了自主地面车辆的实用性。

Method: 提出了CLEAR（Connected Landcover Elevation Abstract Representation）方法，通过边界感知的空间分解与递归平面拟合，生成凸、语义对齐的区域，并构建地形感知图，使地表语义与地形特征并重。

Result: 在9-100km²地图和物理仿真环境下评估，CLEAR将规划速度提升至传统网格的10倍，路径代价仅增加6.7%，路径更短更可靠（效果优于其他抽象基线6-9%）。

Conclusion: CLEAR方法具有良好的扩展性与实用性，为灾害响应、国防和行星探测等长距离导航应用提供了有力支持。

Abstract: Long-horizon navigation in unstructured environments demands terrain abstractions that scale to tens of km$^2$ while preserving semantic and geometric structure, a combination existing methods fail to achieve. Grids scale poorly; quadtrees misalign with terrain boundaries; neither encodes landcover semantics essential for traversability-aware planning. This yields infeasible or unreliable paths for autonomous ground vehicles operating over 10+ km$^2$ under real-time constraints. CLEAR (Connected Landcover Elevation Abstract Representation) couples boundary-aware spatial decomposition with recursive plane fitting to produce convex, semantically aligned regions encoded as a terrain-aware graph. Evaluated on maps spanning 9-100~km$^2$ using a physics-based simulator, CLEAR achieves up to 10x faster planning than raw grids with only 6.7% cost overhead and delivers 6-9% shorter, more reliable paths than other abstraction baselines. These results highlight CLEAR's scalability and utility for long-range navigation in applications such as disaster response, defense, and planetary exploration.

</details>


### [493] [Robustness and Resilience Evaluation of Eco-Driving Strategies at Signalized Intersections](https://arxiv.org/abs/2601.13389)
*Zhaohui Liang,Chengyuan Ma,Keke Long,Xiaopeng Li*

Main category: cs.RO

TL;DR: 本文提出了一套统一的评估框架，通过控制鲁棒性和环境弹性来分析信号交叉口生态驾驶策略，并用实车实验验证各类控制器性能及适应性权衡。


<details>
  <summary>Details</summary>
Motivation: 以往生态驾驶策略的效果多基于简化仿真或实验条件，现实环境中的执行波动性和外部干扰未被充分考虑，缺乏系统化的评估方法。

Method: 作者提出以控制鲁棒性（对内部波动的抗干扰能力）和环境弹性（对外部扰动的适应力）为核心的评估指标，量化性能退化，并在真实车辆实验中评测多种生态驾驶控制器。

Result: 实验揭示了跟踪精度与适应性之间的关键权衡；优化型控制器在扰动水平变化下表现更为一致，解析型控制器在理想情况下表现良好，但对变异与时序波动更敏感。

Conclusion: 优化型控制器具有更好的适应性，适合实际复杂环境；解析型控制器虽然在标准环境下有效，但鲁棒性较差。统一评估框架有助于择优推广生态驾驶策略。

Abstract: Eco-driving strategies have demonstrated substantial potential for improving energy efficiency and reducing emissions, especially at signalized intersections. However, evaluations of eco-driving methods typically rely on simplified simulation or experimental conditions, where certain assumptions are made to manage complexity and experimental control. This study introduces a unified framework to evaluate eco-driving strategies through the lens of two complementary criteria: control robustness and environmental resilience. We define formal indicators that quantify performance degradation caused by internal execution variability and external environmental disturbances, respectively. These indicators are then applied to assess multiple eco-driving controllers through real-world vehicle experiments. The results reveal key tradeoffs between tracking accuracy and adaptability, showing that optimization-based controllers offer more consistent performance across varying disturbance levels, while analytical controllers may perform comparably under nominal conditions but exhibit greater sensitivity to execution and timing variability.

</details>


### [494] [Event-based Heterogeneous Information Processing for Online Vision-based Obstacle Detection and Localization](https://arxiv.org/abs/2601.13451)
*Reza Ahmadvand,Sarah Safura Sharif,Yaser Mike Banad*

Main category: cs.RO

TL;DR: 本文提出了一种结合混合神经网络与脉冲神经网络过滤的新型机器人视觉导航框架，提高了对未知障碍物的检测和定位能力。该方法兼顾了环境理解的准确性与处理速度和能效。


<details>
  <summary>Details</summary>
Motivation: 传统基于神经网络的机器人导航存在处理速度和能耗之间的权衡，且对动态、不可预测环境中的未知障碍物检测能力有限。作者希望突破现有架构的局限，实现更高效、更敏感的导航行为。

Method: 采用双通路结构：ANN部分以低频处理静态空间特征，SNN部分以实时方式处理动态、事件驱动的传感器数据。创新点在于直接利用脉冲输入进行定位与状态评估，无需域转换机制，并通过ANN路径的上下文信息验证异常检测结果。

Result: 仿真结果表明，该方法在保证检测精度的同时，计算效率接近仅用SNN的实现，大幅降低了资源消耗。

Conclusion: 该框架推动了神经形态导航技术，为机器人在复杂动态环境中的应用（如未知障碍物检测、定位与预测导航）提供了有效解决方案。

Abstract: This paper introduces a novel framework for robotic vision-based navigation that integrates Hybrid Neural Networks (HNNs) with Spiking Neural Network (SNN)-based filtering to enhance situational awareness for unmodeled obstacle detection and localization. By leveraging the complementary strengths of Artificial Neural Networks (ANNs) and SNNs, the system achieves both accurate environmental understanding and fast, energy-efficient processing. The proposed architecture employs a dual-pathway approach: an ANN component processes static spatial features at low frequency, while an SNN component handles dynamic, event-based sensor data in real time. Unlike conventional hybrid architectures that rely on domain conversion mechanisms, our system incorporates a pre-developed SNN-based filter that directly utilizes spike-encoded inputs for localization and state estimation. Detected anomalies are validated using contextual information from the ANN pathway and continuously tracked to support anticipatory navigation strategies. Simulation results demonstrate that the proposed method offers acceptable detection accuracy while maintaining computational efficiency close to SNN-only implementations, which operate at a fraction of the resource cost. This framework represents a significant advancement in neuromorphic navigation systems for robots operating in unpredictable and dynamic environments.

</details>


### [495] [The OncoReach Stylet for Brachytherapy: Design Evaluation and Pilot Study](https://arxiv.org/abs/2601.13529)
*Pejman Kheradmand,Kent K. Yamamoto,Emma Webster,Keith Sowards,Gianna Hatheway,Katharine L. Jackson,Sabino Zani,Julie A. Raffi,Diandra N. Ayala-Peacock,Scott R. Silva,Joanna Deaton Bertram,Yash Chitalia*

Main category: cs.RO

TL;DR: 该论文介绍了一种用于宫颈癌间质近距离放疗(ISBT)的新型可操控穿刺针头OncoReach stylet，解决了现有直针路径受限的问题，并通过实验和模型验证实现了手动操控和良好的轨迹控制。


<details>
  <summary>Details</summary>
Motivation: 传统ISBT使用的直针因直线路径受限，影响对肿瘤和周围组织的精准治疗。研究动机是提升穿刺针的可控性和灵活性，从而增强手术规划的自由度，提高治疗效果。

Method: 作者设计了一种手持、腱驱动、可操控的穿刺针（OncoReach stylet），兼容现有15和13规格的ISBT穿刺针。通过对针规、球形关节数目及分布、非对称圆盘的设计参数进行优化，结合自由空间实验和两管Cosserat杆模型预测弯曲形状，并开发相关原型和人体模型进行初步使用测试。

Result: 优化后原型能最大化弯曲柔顺性且保持轴向刚度。实验结果显示，该设计能实现从中线入口点转向至外侧目标点的有效引导，突破常规直针路径限制。Cosserat杆模型能准确预测针头形态。

Conclusion: OncoReach stylet增强了ISBT中的手术灵活性与精准性，有助于更微创且更有效地达成宫颈癌放疗靶点，显示出可操控针头在未来临床中的应用前景。

Abstract: Cervical cancer accounts for a significant portion of the global cancer burden among women. Interstitial brachytherapy (ISBT) is a standard procedure for treating cervical cancer; it involves placing a radioactive source through a straight hollow needle within or in close proximity to the tumor and surrounding tissue. However, the use of straight needles limits surgical planning to a linear needle path. We present the OncoReach stylet, a handheld, tendon-driven steerable stylet designed for compatibility with standard ISBT 15- and 13-gauge needles. Building upon our prior work, we evaluated design parameters like needle gauge, spherical joint count and spherical joint placement, including an asymmetric disk design to identify a configuration that maximizes bending compliance while retaining axial stiffness. Free space experiments quantified tip deflection across configurations, and a two-tube Cosserat rod model accurately predicted the centerline shape of the needle for most trials. The best performing configuration was integrated into a reusable handheld prototype that enables manual actuation. A patient-derived, multi-composite phantom model of the uterus and pelvis was developed to conduct a pilot study of the OncoReach steerable stylet with one expert user. Results showed the ability to steer from less-invasive, medial entry points to reach the lateral-most targets, underscoring the significance of steerable stylets.

</details>


### [496] [LogicEnvGen: Task-Logic Driven Generation of Diverse Simulated Environments for Embodied AI](https://arxiv.org/abs/2601.13556)
*Jianan Wang,Siyang Zhang,Bin Li,Juan Chen,Jingtao Qi,Zhuo Zhang,Chen Qian*

Main category: cs.RO

TL;DR: 本文提出了一种利用大型语言模型（LLM）驱动的环境生成方法LogicEnvGen，实现从逻辑多样性的角度生成具备多种任务情况的仿真环境，并通过新评价基准（LogicEnvEval）验证了方法优越性。


<details>
  <summary>Details</summary>
Motivation: 现有仿真环境生成方法多关注视觉或场景真实感，忽视了从测试角度的“逻辑多样性”，这限制了对智能体适应性和规划能力的全面评估。作者希望增强环境在逻辑层面的丰富性，更好揭示智能体的弱点。

Method: 以LLM为核心，首先分析智能体任务生成决策树结构的行为计划，合成逻辑轨迹集，再用启发式算法去冗余。对每条逻辑轨迹，通过约束求解生成物理可行的具体环境。同时提出了四项定量指标的新基准用于多样性与效果评估。

Result: 实验显示，以往方法缺乏逻辑多样性，而LogicEnvGen在多样性评价指标上提升1.04-2.61倍，智能体缺陷揭露能力显著提升4%-68%。

Conclusion: 引入逻辑多样性后，环境生成质量和测试效能大幅提升，方法可更有力揭示智能体的潜在问题，推动具身智能领域的环境生成和评测标准发展。

Abstract: Simulated environments play an essential role in embodied AI, functionally analogous to test cases in software engineering. However, existing environment generation methods often emphasize visual realism (e.g., object diversity and layout coherence), overlooking a crucial aspect: logical diversity from the testing perspective. This limits the comprehensive evaluation of agent adaptability and planning robustness in distinct simulated environments. To bridge this gap, we propose LogicEnvGen, a novel method driven by Large Language Models (LLMs) that adopts a top-down paradigm to generate logically diverse simulated environments as test cases for agents. Given an agent task, LogicEnvGen first analyzes its execution logic to construct decision-tree-structured behavior plans and then synthesizes a set of logical trajectories. Subsequently, it adopts a heuristic algorithm to refine the trajectory set, reducing redundant simulation. For each logical trajectory, which represents a potential task situation, LogicEnvGen correspondingly instantiates a concrete environment. Notably, it employs constraint solving for physical plausibility. Furthermore, we introduce LogicEnvEval, a novel benchmark comprising four quantitative metrics for environment evaluation. Experimental results verify the lack of logical diversity in baselines and demonstrate that LogicEnvGen achieves 1.04-2.61x greater diversity, significantly improving the performance in revealing agent faults by 4.00%-68.00%.

</details>


### [497] [Highly Deformable Proprioceptive Membrane for Real-Time 3D Shape Reconstruction](https://arxiv.org/abs/2601.13574)
*Guanyu Xu,Jiaqi Wang,Dezhong Tong,Xiaonan Huang*

Main category: cs.RO

TL;DR: 该论文提出了一种基于光学波导感知的柔性自感知硅胶膜，用于高效、实时三维形状重建。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉方法在弱光或遮挡条件下无法可靠获取物体3D表面信息，现有的自感知膜如电阻、电容或磁敏机制在结构、可变形能力和抗干扰性方面也存在不足。因此，亟需新的传感机制提升机器人对三维形变的感知能力。

Method: 设计了一种集成有边缘LED和中心光电二极管的柔性硅胶膜，内部多层弹性基体中嵌有液态金属线路，实现光信号的传递。通过数据驱动模型对膜的变形相关光强信号进行解码，重建出三维点云表示的膜表面形状。

Result: 在一块定制的140mm方形膜上，实现了大范围膜面三维变形的实时重建，重建速度达90Hz，平均误差仅为1.3mm，并能精确感知最大25mm的压痕。

Conclusion: 该方法为可变形机器人系统提供了一种可扩展、鲁棒且低结构厚度的全局三维形状感知方案，有望提升机器人在复杂环境下的感知与交互能力。

Abstract: Reconstructing the three-dimensional (3D) geometry of object surfaces is essential for robot perception, yet vision-based approaches are generally unreliable under low illumination or occlusion. This limitation motivates the design of a proprioceptive membrane that conforms to the surface of interest and infers 3D geometry by reconstructing its own deformation. Conventional shape-aware membranes typically rely on resistive, capacitive, or magneto-sensitive mechanisms. However, these methods often encounter challenges such as structural complexity, limited compliance during large-scale deformation, and susceptibility to electromagnetic interference. This work presents a soft, flexible, and stretchable proprioceptive silicone membrane based on optical waveguide sensing. The membrane sensor integrates edge-mounted LEDs and centrally distributed photodiodes (PDs), interconnected via liquid-metal traces embedded within a multilayer elastomeric composite. Rich deformation-dependent light intensity signals are decoded by a data-driven model to recover the membrane geometry as a 3D point cloud. On a customized 140 mm square membrane, real-time reconstruction of large-scale out-of-plane deformation is achieved at 90 Hz with an average reconstruction error of 1.3 mm, measured by Chamfer distance, while maintaining accuracy for indentations up to 25 mm. The proposed framework provides a scalable, robust, and low-profile solution for global shape perception in deformable robotic systems.

</details>


### [498] [A General One-Shot Multimodal Active Perception Framework for Robotic Manipulation: Learning to Predict Optimal Viewpoint](https://arxiv.org/abs/2601.13639)
*Deyun Qin,Zezhi Liu,Hanqian Luo,Xiao Liang,Yongchun Fang*

Main category: cs.RO

TL;DR: 本文提出了一种通用的一步多模态主动感知框架，可直接推断最佳观察视角，提高机器人操作任务中的感知质量。该方法在机器人抓取等任务中大幅提升了抓取成功率，且具备良好的仿真到现实（sim-to-real）迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉主动感知方法依赖于迭代优化，耗时且与特定任务强相关，缺乏通用性和高效性。因此，亟需开发能快速、泛化的主动感知机制，提升下游操作任务的表现。

Method: 作者提出了一套通用主动感知框架，通过系统采样和评价候选视角，定义最优视角，并利用域随机化机制构建大规模训练数据。框架包含数据采集流程和一个多模态最优观察点预测网络，该网络利用跨模态注意力机制融合多模态信息，直接输出相机调整方案，从而实现一步推断最优视角。

Result: 该框架应用于受视角约束的机器人抓取任务。实验表明，采用新方法指导的主动感知大幅提升了抓取成功率，实测几乎将抓取成功率翻倍，同时实现了无须额外微调的仿真到实际迁移。

Conclusion: 所提通用一步多模态主动感知框架不仅提升了机器人感知与操作能力，还具有很好的通用性与迁移性，为各类机器人操作任务提供了高效的感知支持。

Abstract: Active perception in vision-based robotic manipulation aims to move the camera toward more informative observation viewpoints, thereby providing high-quality perceptual inputs for downstream tasks. Most existing active perception methods rely on iterative optimization, leading to high time and motion costs, and are tightly coupled with task-specific objectives, which limits their transferability. In this paper, we propose a general one-shot multimodal active perception framework for robotic manipulation. The framework enables direct inference of optimal viewpoints and comprises a data collection pipeline and an optimal viewpoint prediction network. Specifically, the framework decouples viewpoint quality evaluation from the overall architecture, supporting heterogeneous task requirements. Optimal viewpoints are defined through systematic sampling and evaluation of candidate viewpoints, after which large-scale training datasets are constructed via domain randomization. Moreover, a multimodal optimal viewpoint prediction network is developed, leveraging cross-attention to align and fuse multimodal features and directly predict camera pose adjustments. The proposed framework is instantiated in robotic grasping under viewpoint-constrained environments. Experimental results demonstrate that active perception guided by the framework significantly improves grasp success rates. Notably, real-world evaluations achieve nearly double the grasp success rate and enable seamless sim-to-real transfer without additional fine-tuning, demonstrating the effectiveness of the proposed framework.

</details>


### [499] [Communication-Free Collective Navigation for a Swarm of UAVs via LiDAR-Based Deep Reinforcement Learning](https://arxiv.org/abs/2601.13657)
*Myong-Yol Choi,Hankyoul Ko,Hanse Cho,Changseung Kim,Seunghwan Kim,Jaemin Seo,Hyondong Oh*

Main category: cs.RO

TL;DR: 本文提出了一种基于深度强化学习（DRL）的无人机集群导航控制器，无需通信即可在复杂环境下实现集群协作导航。


<details>
  <summary>Details</summary>
Motivation: 在通信受限或无外部定位的复杂环境下，多无人机集群的协同导航成为难题，传统方法依赖通信或外部定位系统，实际中易受干扰且成本高，因此探索无需通信的集群导航方法至关重要。

Method: 借鉴生物群体的隐式领航-跟随机制，仅有领头无人机知晓目标信息，其余无人机仅依赖本机LiDAR传感和无通信情境下的局部感知来学习跟随、避障策略。技术上，采用LiDAR点云聚类与扩展卡尔曼滤波实现近邻稳定跟踪，并通过GPU加速仿真平台训练深度强化学习控制器。

Result: 提出的方法可以让无人机群在室内外、无通信情况下，依靠本地感知实现领航跟随与避障，且具备较强的鲁棒性。仿真与真实无人机实验中，在多障碍环境下均表现出良好的集群导航与移植性。

Conclusion: 该方法实现了无需通信和外部定位系统的无人机集群自主协作导航，对应急救援、军事侦察等无基础设施场景具有实际应用价值。

Abstract: This paper presents a deep reinforcement learning (DRL) based controller for collective navigation of unmanned aerial vehicle (UAV) swarms in communication-denied environments, enabling robust operation in complex, obstacle-rich environments. Inspired by biological swarms where informed individuals guide groups without explicit communication, we employ an implicit leader-follower framework. In this paradigm, only the leader possesses goal information, while follower UAVs learn robust policies using only onboard LiDAR sensing, without requiring any inter-agent communication or leader identification. Our system utilizes LiDAR point clustering and an extended Kalman filter for stable neighbor tracking, providing reliable perception independent of external positioning systems. The core of our approach is a DRL controller, trained in GPU-accelerated Nvidia Isaac Sim, that enables followers to learn complex emergent behaviors - balancing flocking and obstacle avoidance - using only local perception. This allows the swarm to implicitly follow the leader while robustly addressing perceptual challenges such as occlusion and limited field-of-view. The robustness and sim-to-real transfer of our approach are confirmed through extensive simulations and challenging real-world experiments with a swarm of five UAVs, which successfully demonstrated collective navigation across diverse indoor and outdoor environments without any communication or external localization.

</details>


### [500] [SUNSET -- A Sensor-fUsioN based semantic SegmEnTation exemplar for ROS-based self-adaptation](https://arxiv.org/abs/2601.13732)
*Andreas Wiedholz,Rafael Paintner,Julian Gleißner,Alwin Hoffmann,Tobias Huber*

Main category: cs.RO

TL;DR: 本文提出了SUNSET，一个基于ROS2的自适应机器人软件系统评测平台，用于在动态环境下评估架构层自适应能力，支持多重和并发不确定性研究。


<details>
  <summary>Details</summary>
Motivation: 随着机器人越来越多地部署在动态和复杂环境中，其软件系统也变得更加复杂，常常面临多重不确定性和模糊的故障根源，传统方法难以应对，因此亟需有效的自适应解决方案。

Method: 作者设计并实现了SUNSET平台，它是一个以ROS2为基础的示例系统，包含一个由训练过的机器学习模型驱动的传感器融合语义分割流水线。该平台可以通过输入扰动，模拟现实中的性能下降，并注入可观察症状，便于研究不同的根本原因及不确定性对系统自愈和自优化的影响。

Result: SUNSET实现了五种不同的可观察症状，每种症状可能由多种根本原因引起，并支持并发的不确定性研究。该平台配有分割流水线、ML模型、不确定性注入脚本、基线控制器及详细集成与评价说明，为后续研究提供可重复和可比较的实验基础。

Conclusion: 本文的SUNSET平台促进了机器人系统在面对复杂动态环境及多重不确定时架构自适应方法的公平、可重复和严谨评测，有助于推动相关领域的研究与应用。

Abstract: The fact that robots are getting deployed more often in dynamic environments, together with the increasing complexity of their software systems, raises the need for self-adaptive approaches. In these environments robotic software systems increasingly operate amid (1) uncertainties, where symptoms are easy to observe but root causes are ambiguous, or (2) multiple uncertainties appear concurrently. We present SUNSET, a ROS2-based exemplar that enables rigorous, repeatable evaluation of architecture-based self-adaptation in such conditions. It implements a sensor fusion semantic-segmentation pipeline driven by a trained Machine Learning (ML) model whose input preprocessing can be perturbed to induce realistic performance degradations. The exemplar exposes five observable symptoms, where each can be caused by different root causes and supports concurrent uncertainties spanning self-healing and self-optimisation. SUNSET includes the segmentation pipeline, a trained ML model, uncertainty-injection scripts, a baseline controller, and step-by-step integration and evaluation documentation to facilitate reproducible studies and fair comparison.

</details>


### [501] [RIM Hand : A Robotic Hand with an Accurate Carpometacarpal Joint and Nitinol-Supported Skeletal Structure](https://arxiv.org/abs/2601.13737)
*Joon Lee,Jeongyoon Han,Doyoung Kim,Seokhwan Jeong*

Main category: cs.RO

TL;DR: 本文提出了一种新型仿生柔性机器人手（RIM Hand），利用超弹性镍钛合金丝和硅胶皮肤，实现了高仿真的人手解剖结构与功能，显著提升了灵活性与抓握性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器人手缺乏对人手腕掌和关节结构的精准模拟，导致灵活性、顺应性及与人体外观兼容性不足，难以胜任复杂抓握与日常服务等应用场景。

Method: 设计还原了完整的腕骨-掌骨解剖结构，关节采用超弹性镍钛合金丝搭建骨架，并辅以硅胶皮肤外覆。各手指通过肌腱驱动实现自由弯曲，腕掌可灵活变形，模拟真实人手动作。

Result: 实验表明，该仿生手腕掌最大可变形28%，接近真人，同时其承载能力是刚性掌设计的两倍，接触面积提升三倍。

Conclusion: RIM Hand在灵巧性、顺应性和仿真度方面均优于传统刚性设计，具有良好的假肢与服务机器人应用前景。

Abstract: This paper presents the flexible RIM Hand, a biomimetic robotic hand that precisely replicates the carpometacarpal (CMC) joints and employs superelastic Nitinol wires throughout its skeletal framework. By modeling the full carpal-to-metacarpal anatomy, the design enables realistic palm deformation through tendon-driven fingers while enhancing joint restoration and supports skeletal structure with Nitinol-based dorsal extensors. A flexible silicone skin further increases contact friction and contact area, enabling stable grasps for diverse objects. Experiments show that the palm can deform up to 28%, matching human hand flexibility, while achieving more than twice the payload capacity and three times the contact area compared to a rigid palm design. The RIM Hand thus offers improved dexterity, compliance, and anthropomorphism, making it promising for prosthetic and service-robot applications.

</details>


### [502] [Sample Efficient Learning of Body-Environment Interaction of an Under-Actuated System](https://arxiv.org/abs/2601.13777)
*Zvi Chapnik,Yizhar Or,Shai Revzen*

Main category: cs.RO

TL;DR: 本文探讨了如何从运动跟踪数据学习motility map，即“运动能力图”，用于预测机器人运动。结果显示：简单方法在小样本下效果更好，复杂方法则在大数据下表现更优。


<details>
  <summary>Details</summary>
Motivation: 在高摩擦环境中，生物与机器人系统如何通过形状变化实现运动是复杂且难以建模的问题。为了促进机器人运动的预测与控制，研究者希望通过实验数据自动学习描述运动规律的motility map，而非依赖精确物理建模。

Method: 本研究制作了一个具备欠驱动、与基底相互作用复杂的实验机器人作为被测对象。采集机器人运动跟踪数据后，对比了四种不同的建模方法在根据形状变化预测主体速度时的表现，这些方法分别评估于相同行走、不同步态及不同速度下。

Result: 结果发现：在训练集规模较小的情况下，结构简单的建模方法有更好的表现；但随着训练数据增多，更复杂的方法能提供更好的预测效果。两者存在性能与数据量的权衡。

Conclusion: 对于基于形状变化预测机器人运动速度的问题，建模方法的选择应结合训练集规模权衡；小数据应选用简单模型，大数据则适合复杂模型。该结论对实际机器人建模和控制具指导意义。

Abstract: Geometric mechanics provides valuable insights into how biological and robotic systems use changes in shape to move by mechanically interacting with their environment. In high-friction environments it provides that the entire interaction is captured by the ``motility map''. Here we compare methods for learning the motility map from motion tracking data of a physical robot created specifically to test these methods by having under-actuated degrees of freedom and a hard to model interaction with its substrate. We compared four modeling approaches in terms of their ability to predict body velocity from shape change within the same gait, across gaits, and across speeds. Our results show a trade-off between simpler methods which are superior on small training datasets, and more sophisticated methods, which are superior when more training data is available.

</details>


### [503] [HoverAI: An Embodied Aerial Agent for Natural Human-Drone Interaction](https://arxiv.org/abs/2601.13801)
*Yuhua Jin,Nikita Kuzmin,Georgii Demianchuk,Mariya Lezina,Fawad Mehboob,Issatay Tokmurziyev,Miguel Altamirano Cabrera,Muhammad Ahsan Mustafa,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: 本论文提出了HoverAI系统，将无人机的移动性、独立基础设施的视觉投影和实时对话式AI整合为一个平台，显著提升了无人机在人类环境中的互动能力。


<details>
  <summary>Details</summary>
Motivation: 当前无人机在人类环境中的应用受到沟通机制不足的限制，使得无人机的意图难以被准确理解，影响其与人类的协作和互动效率。

Method: HoverAI配备MEMS激光投影仪、半刚性屏幕和RGB相机，通过视觉和语音识别感知用户，并用可根据用户特征变化的虚拟形象进行回话。系统集成了多模态管道，包括语音活动检测（VAD）、语音识别（ASR）、大语言模型意图分类、检索增强生成（RAG）对话、脸部分析个性化和语音合成。

Result: 在命令识别（F1: 0.90）、性别推断（F1: 0.89）、年龄预测（MAE: 5.14岁）与语音转录（WER: 0.181）等方面评测表现优异。

Conclusion: 将空中机器人、定制对话AI及自主视觉输出结合，HoverAI开辟了空间感知与社会响应型空中智能体新方向，可用于导航、辅助及以人为本的交互应用。

Abstract: Drones operating in human-occupied spaces suffer from insufficient communication mechanisms that create uncertainty about their intentions. We present HoverAI, an embodied aerial agent that integrates drone mobility, infrastructure-independent visual projection, and real-time conversational AI into a unified platform. Equipped with a MEMS laser projector, onboard semi-rigid screen, and RGB camera, HoverAI perceives users through vision and voice, responding via lip-synced avatars that adapt appearance to user demographics. The system employs a multimodal pipeline combining VAD, ASR (Whisper), LLM-based intent classification, RAG for dialogue, face analysis for personalization, and voice synthesis (XTTS v2). Evaluation demonstrates high accuracy in command recognition (F1: 0.90), demographic estimation (gender F1: 0.89, age MAE: 5.14 years), and speech transcription (WER: 0.181). By uniting aerial robotics with adaptive conversational AI and self-contained visual output, HoverAI introduces a new class of spatially-aware, socially responsive embodied agents for applications in guidance, assistance, and human-centered interaction.

</details>


### [504] [DroneVLA: VLA based Aerial Manipulation](https://arxiv.org/abs/2601.13809)
*Fawad Mehboob,Monijesu James,Amir Habel,Jeffrin Sam,Miguel Altamirano Cabrera,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: 本文提出了一种能用自然语言命令操作的自主空中抓取与递送系统，并通过实验证明系统具备良好的定位和导航精度。


<details>
  <summary>Details</summary>
Motivation: 随着无人机平台从被动观测转向主动操作，亟需让非专业用户能以直观自然的方式指挥这些系统。现有的控制界面大多不直观，限制了此类系统的实际应用。

Method: 系统集成了基于Grounding DINO和MediaPipe的视觉语义理解与动作规划模型（VLA），并搭载自制无人机平台，配备1自由度夹爪和Intel RealSense RGB-D相机。VLA负责理解用户自然语言要求及任务优先级排序。Grounding DINO配合动态A*算法实现对象定位与路径优化。递送阶段通过MediaPipe进行实时人体姿态识别，引导无人机稳定、安全地面向用户递送抓取物品。

Result: 通过真实环境下的实验，系统在定位和导航方面表现出较高精度，最大、平均欧氏距离误差分别为0.164m和0.070m，均方根误差为0.084m，验证了方案的可行性。

Conclusion: 所提出的自主无人机抓取与交付系统能够自然理解用户语言指令，表现出优秀的操作精度，证实了基于视觉-语言-动作模型的空中操作系统在现实中的可行性和实用性。

Abstract: As aerial platforms evolve from passive observers to active manipulators, the challenge shifts toward designing intuitive interfaces that allow non-expert users to command these systems naturally. This work introduces a novel concept of autonomous aerial manipulation system capable of interpreting high-level natural language commands to retrieve objects and deliver them to a human user. The system is intended to integrate a MediaPipe based on Grounding DINO and a Vision-Language-Action (VLA) model with a custom-built drone equipped with a 1-DOF gripper and an Intel RealSense RGB-D camera. VLA performs semantic reasoning to interpret the intent of a user prompt and generates a prioritized task queue for grasping of relevant objects in the scene. Grounding DINO and dynamic A* planning algorithm are used to navigate and safely relocate the object. To ensure safe and natural interaction during the handover phase, the system employs a human-centric controller driven by MediaPipe. This module provides real-time human pose estimation, allowing the drone to employ visual servoing to maintain a stable, distinct position directly in front of the user, facilitating a comfortable handover. We demonstrate the system's efficacy through real-world experiments for localization and navigation, which resulted in a 0.164m, 0.070m, and 0.084m of max, mean euclidean, and root-mean squared errors, respectively, highlighting the feasibility of VLA for aerial manipulation operations.

</details>


### [505] [GuideTouch: An Obstacle Avoidance Device for Visually Impaired](https://arxiv.org/abs/2601.13813)
*Timofei Kozlov,Artem Trandofilov,Georgii Gazaryan,Issatay Tokmurziyev,Miguel Altamirano Cabrera,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: GuideTouch是一种面向视障人士的可穿戴避障设备，结合ToF传感器与触觉反馈，在实验中显示出高辨识准确率，有望大幅提升视障人士的独立导航安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的助行器具容易遗漏头部高度的障碍物，导致视障人士在现实出行中仍然面临安全风险，因此急需开发能检测多维障碍并以直观方式反馈信息的新型辅助装置。

Method: 设计并实现了一种紧凑、便携、经济的穿戴设备GuideTouch。该设备集成了两个垂直排列的ToF传感器，获取三维空间信息，并搭配分布于肩部和胸部的四个振动触觉模块，提供方向性反馈。为增强设备在实际环境适应性，还加入了光学自清洁机制和丢失报警功能。通过22名参与者的实验和14名视障者初步测试，统计评估各类触觉反馈的辨识准确性。

Result: 健全人群试验显示单、双马达主方向模式识别平均准确率为92.9%；视障群体初步测试对主方向反馈的识别准确率为93.75%，触觉模式辨识存在明显差异。

Conclusion: GuideTouch实现了高效、直观的空间方向感知提示，对于提升视障人士独立导航时的安全性、自信心和自主性具有显著潜力。

Abstract: Safe navigation for the visually impaired individuals remains a critical challenge, especially concerning head-level obstacles, which traditional mobility aids often fail to detect. We introduce GuideTouch, a compact, affordable, standalone wearable device designed for autonomous obstacle avoidance. The system integrates two vertically aligned Time-of-Flight (ToF) sensors, enabling three-dimensional environmental perception, and four vibrotactile actuators that provide directional haptic feedback. Proximity and direction information is communicated via an intuitive 4-point vibrotactile feedback system located across the user's shoulders and upper chest. For real-world robustness, the device includes a unique centrifugal self-cleaning optical cover mechanism and a sound alarm system for location if the device is dropped. We evaluated the haptic perception accuracy across 22 participants (17 male and 5 female, aged 21-48, mean 25.7, sd 6.1). Statistical analysis confirmed a significant difference between the perception accuracy of different patterns. The system demonstrated high recognition accuracy, achieving an average of 92.9% for single and double motor (primary directional) patterns. Furthermore, preliminary experiments with 14 visually impaired users validated this interface, showing a recognition accuracy of 93.75% for primary directional cues. The results demonstrate that GuideTouch enables intuitive spatial perception and could significantly improve the safety, confidence, and autonomy of users with visual impairments during independent navigation.

</details>


### [506] [Efficient Coordination with the System-Level Shared State: An Embodied-AI Native Modular Framework](https://arxiv.org/abs/2601.13945)
*Yixuan Deng,Tongrun Wu,Donghao Wu,Zeyu Wei,Jiayuan Wang,Zhenglong Sun,Yuqing Tang,Xiaoqiang Ji*

Main category: cs.RO

TL;DR: 本文提出了ANCHOR框架，通过显式的系统级原语来增强实体智能（Embodied AI）系统在实际部署中的解耦与鲁棒性。该框架将共享状态的标准化合同与多对多通信机制分离，提升了系统可扩展性与自动恢复能力。


<details>
  <summary>Details</summary>
Motivation: 现实中的AI系统部署经常出现模块间耦合、接口漂移、恢复脆弱等问题。现有方法难以满足系统在不断演化过程中对可靠性和弹性运行的需求，因此需要新的架构来支持大规模、可恢复的闭环AI系统部署。

Method: 提出ANCHOR模块化框架，将共享的标准化状态合同（Canonical Records）与多对多发布-反馈通信总线分离，通过明晰的接口和可检视的闭环来管理消息、状态和反馈。通过实例化工作流实验，测试了该框架在不同负载下的延迟表现和故障恢复能力。

Result: 实验证明，在实际工作流程中，ANCHOR能够支持闭环AI系统。其通信总线在不同负载情况下具备良好的时延控制，并能在内存丢失或系统崩溃后自动恢复数据流。

Conclusion: ANCHOR框架将过去零散的集成方式转化为可控、显式的系统合同，显著增强了AI系统在复杂环境下的可扩展性和自愈能力，为闭环AI系统部署提供了坚实基础。

Abstract: As Embodied AI systems move from research prototypes to real world deployments, they tend to evolve rapidly while remaining reliable under workload changes and partial failures. In practice, many deployments are only partially decoupled: middleware moves messages, but shared context and feedback semantics are implicit, causing interface drift, cross-module interference, and brittle recovery at scale. We present ANCHOR, a modular framework that makes decoupling and robustness explicit system-level primitives. ANCHOR separates (i) Canonical Records, an evolvable contract for the standardized shared state, from (ii) a communication bus for many-to-many dissemination and feedback-oriented coordination, forming an inspectable end-to-end loop. We validate closed-loop feasibility on a de-identified workflow instantiation, characterize latency distributions under varying payload sizes and publish rates, and demonstrate automatic stream resumption after hard crashes and restarts even with shared-memory loss. Overall, ANCHOR turns ad-hoc integration glue into explicit contracts, enabling controlled degradation under load and self-healing recovery for scalable deployment of closed-loop AI systems.

</details>


### [507] [Active Cross-Modal Visuo-Tactile Perception of Deformable Linear Objects](https://arxiv.org/abs/2601.13979)
*Raffaele Mazza,Ciro Natale,Pietro Falco*

Main category: cs.RO

TL;DR: 该论文提出了一种融合视觉与触觉的多模态感知框架，用于在严重遮挡情况下重建可变形线性物体（如电缆）的三维形状。结合了基于基础模型的视觉识别和自适应触觉探索，在遮挡严重时利用触觉补偿视觉信息不足，实现了对电缆形状的高精度完整重建。


<details>
  <summary>Details</summary>
Motivation: 现有主要基于视觉的三维重建方法在光照变化、背景杂乱或部分遮挡时性能下降，对于部分遮挡或复杂环境中的电缆三维重建有较大挑战。因此，迫切需要引入额外传感信息提升对可变形线状物体的重建能力。

Method: 使用SAM进行实例分割，Florence进行语义细化，接着骨架提取、端点检测及点云提取。对于视觉无法覆盖的遮挡区域，用带有触觉传感器的机械臂探索获取局部点云，并通过欧氏聚类与拓扑保持融合算法与视觉点云合并，最后利用端点引导的B样条插值平滑重建整体电缆形状。

Result: 实验在配备RGB-D相机和触觉垫的机器人上验证，无论电缆形状简单或弯曲复杂、遮挡严重与否，框架均可实现高精度、完整的三维重建，优于仅依赖视觉的方法。

Conclusion: 基础模型赋能的视觉-触觉跨模态感知方法可显著提升可变形物体在遮挡场景下的三维重建能力，为机器人操作此类柔性物体提供了理论基础和技术支持。

Abstract: This paper presents a novel cross-modal visuo-tactile perception framework for the 3D shape reconstruction of deformable linear objects (DLOs), with a specific focus on cables subject to severe visual occlusions. Unlike existing methods relying predominantly on vision, whose performance degrades under varying illumination, background clutter, or partial visibility, the proposed approach integrates foundation-model-based visual perception with adaptive tactile exploration. The visual pipeline exploits SAM for instance segmentation and Florence for semantic refinement, followed by skeletonization, endpoint detection, and point-cloud extraction. Occluded cable segments are autonomously identified and explored with a tactile sensor, which provides local point clouds that are merged with the visual data through Euclidean clustering and topology-preserving fusion. A B-spline interpolation driven by endpoint-guided point sorting yields a smooth and complete reconstruction of the cable shape. Experimental validation using a robotic manipulator equipped with an RGB-D camera and a tactile pad demonstrates that the proposed framework accurately reconstructs both simple and highly curved single or multiple cable configurations, even when large portions are occluded. These results highlight the potential of foundation-model-enhanced cross-modal perception for advancing robotic manipulation of deformable objects.

</details>


### [508] [Group-Invariant Unsupervised Skill Discovery: Symmetry-aware Skill Representations for Generalizable Behavior](https://arxiv.org/abs/2601.14000)
*Junwoo Chang,Joseph Park,Roberto Horowitz,Jongmin Lee,Jongeun Choi*

Main category: cs.RO

TL;DR: 该论文提出了一种新的无监督技能发现方法——GISD，通过引入群对称性结构提升技能多样性与下游任务学习效率。


<details>
  <summary>Details</summary>
Motivation: 现有无监督技能发现方法忽略了物理环境中的几何对称性，导致行为冗余和样本利用低效。作者希望通过嵌入群结构，提升技能发现的效率与泛化能力。

Method: 提出了Group-Invariant Skill Discovery (GISD) 框架，将群结构直接融入技能发现目标；理论上证明了在群对称环境下，Wasserstein相关性度量的最优解具有协变性政策与群不变性打分函数，并据此设计了Group-Invariant Wasserstein度量，将优化过程限定在具对称性子空间。实际实现上，使用群Fourier表征参数化打分函数，通过协变特征对齐定义奖励，保证技能能在群变换下系统泛化。

Result: 在基于状态及像素的运动基准测试中，GISD与强基线方法相比能更广泛覆盖状态空间，并提升下游任务学习效率。

Conclusion: GISD有效利用环境对称性，发现高效、泛化能力强的技能，有助于提升无监督RL探索能力并加速后续任务学习。

Abstract: Unsupervised skill discovery aims to acquire behavior primitives that improve exploration and accelerate downstream task learning. However, existing approaches often ignore the geometric symmetries of physical environments, leading to redundant behaviors and sample inefficiency. To address this, we introduce Group-Invariant Skill Discovery (GISD), a framework that explicitly embeds group structure into the skill discovery objective. Our approach is grounded in a theoretical guarantee: we prove that in group-symmetric environments, the standard Wasserstein dependency measure admits a globally optimal solution comprised of an equivariant policy and a group-invariant scoring function. Motivated by this, we formulate the Group-Invariant Wasserstein dependency measure, which restricts the optimization to this symmetry-aware subspace without loss of optimality. Practically, we parameterize the scoring function using a group Fourier representation and define the intrinsic reward via the alignment of equivariant latent features, ensuring that the discovered skills generalize systematically under group transformations. Experiments on state-based and pixel-based locomotion benchmarks demonstrate that GISD achieves broader state-space coverage and improved efficiency in downstream task learning compared to a strong baseline.

</details>


### [509] [Zero-shot adaptable task planning for autonomous construction robots: a comparative study of lightweight single and multi-AI agent systems](https://arxiv.org/abs/2601.14091)
*Hossein Naderi,Alireza Shojaei,Lifu Huang,Philip Agee,Kereshmeh Afsari,Abiola Akanmu*

Main category: cs.RO

TL;DR: 本研究提出并测试了基于大模型的多智能体团队在建筑机器人任务规划中的性能，发现四智能体团队在精确度与成本效益方面优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 当前建筑机器人面临成本高和难以适应动态任务的挑战，亟需提升机器人任务规划的通用性和适应性。基础大模型为解决这一问题提供了新可能。

Method: 研究设计了四种基于轻量级开源LLMs和VLMs的模型，包括单智能体和多智能体（最多四人团队），通过分工协作共同制定机器人动作计划。模型被测试于三种建筑工种任务中。

Result: 四智能体团队在大多数评估指标上优于最新的GPT-4o，同时成本低十倍。三人和四人团队展现出更好的任务泛化能力。

Conclusion: 多智能体大模型系统能提升建筑机器人任务规划的适应性和通用性，四智能体团队表现最佳，为未来不确定环境下的AI团队协作研究提供支持。

Abstract: Robots are expected to play a major role in the future construction industry but face challenges due to high costs and difficulty adapting to dynamic tasks. This study explores the potential of foundation models to enhance the adaptability and generalizability of task planning in construction robots. Four models are proposed and implemented using lightweight, open-source large language models (LLMs) and vision language models (VLMs). These models include one single agent and three multi-agent teams that collaborate to create robot action plans. The models are evaluated across three construction roles: Painter, Safety Inspector, and Floor Tiling. Results show that the four-agent team outperforms the state-of-the-art GPT-4o in most metrics while being ten times more cost-effective. Additionally, teams with three and four agents demonstrate the improved generalizability. By discussing how agent behaviors influence outputs, this study enhances the understanding of AI teams and supports future research in diverse unstructured environments beyond construction.

</details>


### [510] [Diffusion-Guided Backdoor Attacks in Real-World Reinforcement Learning](https://arxiv.org/abs/2601.14104)
*Tairan Huang,Qingqing Ye,Yulin Jin,Jiawei Lian,Yi Wang,Haibo Hu*

Main category: cs.RO

TL;DR: 本论文提出了一种适用于现实环境强化学习系统的Backdoor攻击方法，即扩散导向Backdoor攻击框架，并在实际机器人平台上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的Backdoor攻击大多只在仿真环境中验证，实际部署时经常因安全约束控制（如速度限制、动作平滑、避碰等）而失效或被大幅削弱。因此，研究能在现实世界机器人系统中仍然有效的Backdoor攻击具有重要意义。

Method: 1. 设计了一种可打印的视觉触发补丁，通过条件扩散模型生成，可以适应真实场景下多样的视觉变化。2. 把机器人控制系统视为黑盒，不依赖内部结构。3. 提出优势值驱动的投毒策略，仅在决策关键时刻注入触发样本，提高攻击效率且降低对正常表现的干扰。

Result: 在TurtleBot3移动机器人实际物理平台上进行评估，结果显示该方法可以在保持机器人正常完成任务的同时，实现目标攻击效果，可靠地激活并控制攻击行为。

Conclusion: 文中提出的扩散导向Backdoor攻击框架能够显著提升强化学习系统在现实环境下的攻击激活率，并兼容现有的安全约束控制机制，提供了真实物理系统中Backdoor攻击的新视角。

Abstract: Backdoor attacks embed hidden malicious behaviors in reinforcement learning (RL) policies and activate them using triggers at test time. Most existing attacks are validated only in simulation, while their effectiveness in real-world robotic systems remains unclear. In physical deployment, safety-constrained control pipelines such as velocity limiting, action smoothing, and collision avoidance suppress abnormal actions, causing strong attenuation of conventional backdoor attacks. We study this previously overlooked problem and propose a diffusion-guided backdoor attack framework (DGBA) for real-world RL. We design small printable visual patch triggers placed on the floor and generate them using a conditional diffusion model that produces diverse patch appearances under real-world visual variations. We treat the robot control stack as a black-box system. We further introduce an advantage-based poisoning strategy that injects triggers only at decision-critical training states. We evaluate our method on a TurtleBot3 mobile robot and demonstrate reliable activation of targeted attacks while preserving normal task performance. Demo videos and code are available in the supplementary material.

</details>


### [511] [SandWorm: Event-based Visuotactile Perception with Active Vibration for Screw-Actuated Robot in Granular Media](https://arxiv.org/abs/2601.14128)
*Shoujie Li,Changqing Guo,Junhao Gong,Chenxin Liang,Wenhua Ding,Wenbo Ding*

Main category: cs.RO

TL;DR: 本文提出了一种结合仿生蠕动与螺旋驱动的新型机器人（SandWorm），以及一种基于事件相机的多模态视觉触觉传感器（SWTac），用于提升颗粒介质环境中的运动与感知能力。


<details>
  <summary>Details</summary>
Motivation: 在颗粒介质（如沙子、土壤）中进行感知与运动具有极大挑战性，主要因颗粒的不规则动态行为和复杂环境干扰，现有方案难以实现高精度的移动与感知，因此需要新的机器人和传感器设计。

Method: 提出以螺旋驱动和仿生蠕动复合运动的SandWorm机器人，并开发带主动振动弹性体的SWTac事件相机触觉传感器。设计隔振机制保证高质量成像，提出IMU引导的时序滤波算法提升成像一致性，并针对多参数进行系统优化。采用U-Net网络进行接触面估计。

Result: SWTac可实现0.2毫米纹理分辨率、98%石头分类准确率，以及0.15牛顿的力感估计误差，SandWorm能在复杂颗粒介质中实现高达12.5mm/s的灵活移动，完成管道疏通和地下探测，成功率达90%。

Conclusion: 该系统在实际复杂环境中展现出优秀的运动与感知能力，具备较高的实用性和鲁棒性，推动了颗粒介质环境下的机器人移动与感知发展。

Abstract: Perception in granular media remains challenging due to unpredictable particle dynamics. To address this challenge, we present SandWorm, a biomimetic screw-actuated robot augmented by peristaltic motion to enhance locomotion, and SWTac, a novel event-based visuotactile sensor with an actively vibrated elastomer. The event camera is mechanically decoupled from vibrations by a spring isolation mechanism, enabling high-quality tactile imaging of both dynamic and stationary objects. For algorithm design, we propose an IMU-guided temporal filter to enhance imaging consistency, improving MSNR by 24%. Moreover, we systematically optimize SWTac with vibration parameters, event camera settings and elastomer properties. Motivated by asymmetric edge features, we also implement contact surface estimation by U-Net. Experimental validation demonstrates SWTac's 0.2 mm texture resolution, 98% stone classification accuracy, and 0.15 N force estimation error, while SandWorm demonstrates versatile locomotion (up to 12.5 mm/s) in challenging terrains, successfully executes pipeline dredging and subsurface exploration in complex granular media (observed 90% success rate). Field experiments further confirm the system's practical performance.

</details>


### [512] [TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers](https://arxiv.org/abs/2601.14133)
*Bin Yu,Shijie Lian,Xiaopeng Lin,Yuliang Wei,Zhaolong Shen,Changti Wu,Yuzhuo Miao,Xinming Wang,Bailing Wang,Cong Huang,Kai Chen*

Main category: cs.RO

TL;DR: 该论文提出了TwinBrainVLA，一种新型的视觉-语言-动作模型架构，通过双模块分工（通用与专精）协同控制机器人，解决通用语义理解和具体动作技能学习之间的冲突，并避免模型灾难性遗忘现象。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型在机器人控制中，需要在保持高层次语义理解力与获得低层次、精细动作控制间权衡，常导致模型遗忘其原有的广泛知识，使其通用性受限。为此，研究者希望兼顾两者，提升机器人在通用理解和实际操作方面的能力。

Method: 采用“双脑”结构，包括冻结的“左脑”（保留强健的通用视觉推理与语义理解能力）及可训练的“右脑”（专精于机器人自身的感知与动作生成），结合创新的非对称变换器混合机制（AsyMoT），使右脑可以动态调用左脑知识，再与自身感知信息融合，为后续动作专家模块(Flow-Matching Action Expert)输出精确动作控制提供丰富条件。

Result: 在SimplerEnv和RoboCasa等基准实验中，TwinBrainVLA不仅在操作任务上取得了优于现有技术的表现，还显著保持了预训练视觉-语言模型的全面视觉理解能力。

Conclusion: TwinBrainVLA为开发同时具备高层次语义理解与低层次物理灵活性的通用机器人控制系统提供了有效思路，有助于推动通用型机器人研究。

Abstract: Standard Vision-Language-Action (VLA) models typically fine-tune a monolithic Vision-Language Model (VLM) backbone explicitly for robotic control. However, this approach creates a critical tension between maintaining high-level general semantic understanding and learning low-level, fine-grained sensorimotor skills, often leading to "catastrophic forgetting" of the model's open-world capabilities. To resolve this conflict, we introduce TwinBrainVLA, a novel architecture that coordinates a generalist VLM retaining universal semantic understanding and a specialist VLM dedicated to embodied proprioception for joint robotic control. TwinBrainVLA synergizes a frozen "Left Brain", which retains robust general visual reasoning, with a trainable "Right Brain", specialized for embodied perception, via a novel Asymmetric Mixture-of-Transformers (AsyMoT) mechanism. This design allows the Right Brain to dynamically query semantic knowledge from the frozen Left Brain and fuse it with proprioceptive states, providing rich conditioning for a Flow-Matching Action Expert to generate precise continuous controls. Extensive experiments on SimplerEnv and RoboCasa benchmarks demonstrate that TwinBrainVLA achieves superior manipulation performance compared to state-of-the-art baselines while explicitly preserving the comprehensive visual understanding capabilities of the pre-trained VLM, offering a promising direction for building general-purpose robots that simultaneously achieve high-level semantic understanding and low-level physical dexterity.

</details>
