<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 94]
- [cs.CL](#cs.CL) [Total: 44]
- [cs.RO](#cs.RO) [Total: 30]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Privacy Enhancement for Gaze Data Using a Noise-Infused Autoencoder](https://arxiv.org/abs/2508.10918)
*Samantha Aziz,Oleg Komogortsev*

Main category: cs.CV

TL;DR: 本文提出了一种用于注视点（gaze）数据的隐私增强机制，通过引入潜在噪声自编码器，防止未经同意的用户重新识别，且不影响正常数据使用。方法在减少生物特征可识别性的同时几乎不影响数据的有用性，并能保持生理上合理的注视行为，兼顾隐私与实用性。


<details>
  <summary>Details</summary>
Motivation: 近年来注视点追踪广泛应用于人机交互、虚拟现实等领域，但注视数据属于敏感生物特征，易被用于个人身份识别。因此急需机制在不损害数据可用性的前提下强化隐私保护。

Method: 作者提出了一种基于潜在-噪声自编码器的机制，将噪声控制性地混入潜变量，以隐藏可用于个人识别的信息。通过适当训练，该机制使同一用户在不同会话之间的注视数据不可被有效关联，同时尽量保留注视数据用于非识别任务的有效性。

Result: 通过在生物特征身份识别和注视点预测等任务上的对比实验，方法显著降低了生物特征可识别性，同时几乎不损失注视点预测等任务的性能，并保持生理合理性。

Conclusion: 该工作为基于注视的系统隐私保护提供了一种有效可用的新机制，在实际部署中有望显著提升敏感数据安全性与用户隐私。

Abstract: We present a privacy-enhancing mechanism for gaze signals using a
latent-noise autoencoder that prevents users from being re-identified across
play sessions without their consent, while retaining the usability of the data
for benign tasks. We evaluate privacy-utility trade-offs across biometric
identification and gaze prediction tasks, showing that our approach
significantly reduces biometric identifiability with minimal utility
degradation. Unlike prior methods in this direction, our framework retains
physiologically plausible gaze patterns suitable for downstream use, which
produces favorable privacy-utility trade-off. This work advances privacy in
gaze-based systems by providing a usable and effective mechanism for protecting
sensitive gaze data.

</details>


### [2] [A Survey on Video Temporal Grounding with Multimodal Large Language Model](https://arxiv.org/abs/2508.10922)
*Jianlong Wu,Wei Liu,Ye Liu,Meng Liu,Liqiang Nie,Zhouchen Lin,Chang Wen Chen*

Main category: cs.CV

TL;DR: 本文全面回顾了基于多模态大模型（MLLMs）的影片时序定位（VTG）方法的发展现状，对相关研究进行了系统性的综述。


<details>
  <summary>Details</summary>
Motivation: 过去对视频-语言理解的综述较多，但专门针对利用多模态大模型进行视频时序定位（VTG-MLLMs）的系统性回顾还很少。因此，本文旨在填补这一空白。

Method: 文章以三维分类体系（MLLMs的功能角色、训练范式以及视频特征处理技术）系统梳理了VTG-MLLMs领域的核心技术框架；同时讨论了基准数据集、评测流程，并综合分析现有实验结果。

Result: 通过文献调研，总结了VTG-MLLMs方法在零样本、多任务和多领域泛化上的优越表现，系统展示了方法演化和技术进展。

Conclusion: VTG-MLLMs已逐渐超越传统方法，但仍存在一定局限性。文章提出了当前研究的不足与未来有潜力的研究方向，并为学者提供了丰富的资源与指南。

Abstract: The recent advancement in video temporal grounding (VTG) has significantly
enhanced fine-grained video understanding, primarily driven by multimodal large
language models (MLLMs). With superior multimodal comprehension and reasoning
abilities, VTG approaches based on MLLMs (VTG-MLLMs) are gradually surpassing
traditional fine-tuned methods. They not only achieve competitive performance
but also excel in generalization across zero-shot, multi-task, and multi-domain
settings. Despite extensive surveys on general video-language understanding,
comprehensive reviews specifically addressing VTG-MLLMs remain scarce. To fill
this gap, this survey systematically examines current research on VTG-MLLMs
through a three-dimensional taxonomy: 1) the functional roles of MLLMs,
highlighting their architectural significance; 2) training paradigms, analyzing
strategies for temporal reasoning and task adaptation; and 3) video feature
processing techniques, which determine spatiotemporal representation
effectiveness. We further discuss benchmark datasets, evaluation protocols, and
summarize empirical findings. Finally, we identify existing limitations and
propose promising research directions. For additional resources and details,
readers are encouraged to visit our repository at
https://github.com/ki-lw/Awesome-MLLMs-for-Video-Temporal-Grounding.

</details>


### [3] [VSF: Simple, Efficient, and Effective Negative Guidance in Few-Step Image Generation Models By \underline{V}alue \underline{S}ign \underline{F}lip](https://arxiv.org/abs/2508.10931)
*Wenqi Guo,Shan Du*

Main category: cs.CV

TL;DR: 本文提出了一种新的负向提示引导技术Value Sign Flip（VSF），能高效提升扩散和流匹配生成模型对负向提示的响应效果，且计算开销小、易于集成，实现了更好的负向引导效果和图像质量。


<details>
  <summary>Details</summary>
Motivation: 当前的少步数扩散和流匹配生成模型在处理负向提示（negative prompt）时效果有限，现有方法如CFG等性能有待提升。负向提示对控制生成内容、抑制不需要元素具有重要意义，因此作者希望提出一种有效且高效的方法增强负向提示引导。

Method: VSF通过对负向提示的注意力数值直接取反，实现动态抑制不需要的内容。该方法可直接集成到MMDiT风格架构（如Stable Diffusion 3.5 Turbo）和基于交叉注意力的模型（如Wan）中，且只需极小的计算资源。

Result: 在复杂提示组合的图像及视频生成任务上，VSF显著提升了负向提示的响应度，并在少步数模型中整体优于现有方法（如CFG、NASA、NAG），即使在非少步数的模型上对比CFG也表现出不错的负向控制能力，同时保持了竞争性的生成质量。

Conclusion: VSF是一种高效、简易且有效的负向提示引导方法，能够提升扩散和流匹配模型在少步数条件下对负向提示的敏感度，并兼顾生成内容质量，可广泛应用于各类生成模型。

Abstract: We introduce Value Sign Flip (VSF), a simple and efficient method for
incorporating negative prompt guidance in few-step diffusion and flow-matching
image generation models. Unlike existing approaches such as classifier-free
guidance (CFG), NASA, and NAG, VSF dynamically suppresses undesired content by
flipping the sign of attention values from negative prompts. Our method
requires only small computational overhead and integrates effectively with
MMDiT-style architectures such as Stable Diffusion 3.5 Turbo, as well as
cross-attention-based models like Wan. We validate VSF on challenging datasets
with complex prompt pairs and demonstrate superior performance in both static
image and video generation tasks. Experimental results show that VSF
significantly improves negative prompt adherence compared to prior methods in
few-step models, and even CFG in non-few-step models, while maintaining
competitive image quality. Code and ComfyUI node are available in
https://github.com/weathon/VSF/tree/main.

</details>


### [4] [Relative Pose Regression with Pose Auto-Encoders: Enhancing Accuracy and Data Efficiency for Retail Applications](https://arxiv.org/abs/2508.10933)
*Yoli Shavit,Yosi Keller*

Main category: cs.CV

TL;DR: 本论文提出一种结合绝对位姿回归（APR）和基于自编码器的相对位姿回归（RPR）的相机重定位方法，用于提升零售环境中的摄像头定位精度。


<details>
  <summary>Details</summary>
Motivation: 在现代零售环境中，精确的摄像头定位对于提升客户体验、简化库存管理和实现自动化运营至关重要。现有APR方法虽有潜力，但结合视觉和空间先验知识的方法在定位精度上表现更佳。

Method: 作者扩展了相机位姿自编码器（PAE），使其适用于相对位姿回归（RPR）任务；并提出了一种新颖的重定位流程，利用PAE-RPR对APR结果进行优化，无需额外存储图像或位姿数据。方法包括：首先将PAE应用于RPR，与同架构的基于图像的RPR模型对比验证有效性；然后利用PAE-RPR实现APR结果的局部精细化。

Result: 在室内数据集上进行实验证明：PAE-RPR不仅比传统的基于图像的RPR方法表现更优，同时该重定位策略显著提升了APR的定位准确度。即便只用30%的训练数据，方法依然能够获得具有竞争力的性能，极大地减轻了零售行业场景的采集压力。

Conclusion: 基于PAE的RPR能够有效提升APR在室内环境下的定位精度，且新方法在大幅降低标注和数据收集成本的同时仍具备优异性能，具有实际部署价值。

Abstract: Accurate camera localization is crucial for modern retail environments,
enabling enhanced customer experiences, streamlined inventory management, and
autonomous operations. While Absolute Pose Regression (APR) from a single image
offers a promising solution, approaches that incorporate visual and spatial
scene priors tend to achieve higher accuracy. Camera Pose Auto-Encoders (PAEs)
have recently been introduced to embed such priors into APR. In this work, we
extend PAEs to the task of Relative Pose Regression (RPR) and propose a novel
re-localization scheme that refines APR predictions using PAE-based RPR,
without requiring additional storage of images or pose data. We first introduce
PAE-based RPR and establish its effectiveness by comparing it with image-based
RPR models of equivalent architectures. We then demonstrate that our refinement
strategy, driven by a PAE-based RPR, enhances APR localization accuracy on
indoor benchmarks. Notably, our method is shown to achieve competitive
performance even when trained with only 30% of the data, substantially reducing
the data collection burden for retail deployment. Our code and pre-trained
models are available at: https://github.com/yolish/camera-pose-auto-encoders

</details>


### [5] [ViPE: Video Pose Engine for 3D Geometric Perception](https://arxiv.org/abs/2508.10934)
*Jiahui Huang,Qunjie Zhou,Hesam Rabeti,Aleksandr Korovko,Huan Ling,Xuanchi Ren,Tianchang Shen,Jun Gao,Dmitry Slepichev,Chen-Hsuan Lin,Jiawei Ren,Kevin Xie,Joydeep Biswas,Laura Leal-Taixe,Sanja Fidler*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频处理引擎ViPE，能够高效且准确地从非结构化视频中估算相机参数和稠密深度，显著提升了三维几何感知的便利性和精度，并公开了大规模高质量注释数据集。


<details>
  <summary>Details</summary>
Motivation: 现有三维感知系统依赖大量精确的带注释三维数据，但从真实视频中获得一致且高质量的三维注释十分困难。该工作旨在解决现实视频三维注释难以获取的问题，推动空间AI领域发展。

Method: ViPE是一款视频处理引擎，可自动从各种类别（如自拍、电影、行车记录等）的原始视频中，鲁棒地估算相机内参、相机运动轨迹和近似真实标度的稠密深度图，并支持多种相机模型（针孔、广角、全景等）。

Result: 在多个公开基准（TUM、KITTI）上，ViPE在无标定位姿估计任务中超越现有方法18%-50%；在标准分辨率下单卡可达3-5FPS。此外，利用ViPE注释了包括约10万真实互联网视频、100万高质量AI生成视频、2千全景视频在内共9600万帧的大规模数据集。

Conclusion: ViPE系统为视频三维几何注释提供了高效便捷的工具，并大规模生成了高质量的带注释数据集，为空间AI任务提供了重要资源，预期可推动该领域技术进步。

Abstract: Accurate 3D geometric perception is an important prerequisite for a wide
range of spatial AI systems. While state-of-the-art methods depend on
large-scale training data, acquiring consistent and precise 3D annotations from
in-the-wild videos remains a key challenge. In this work, we introduce ViPE, a
handy and versatile video processing engine designed to bridge this gap. ViPE
efficiently estimates camera intrinsics, camera motion, and dense, near-metric
depth maps from unconstrained raw videos. It is robust to diverse scenarios,
including dynamic selfie videos, cinematic shots, or dashcams, and supports
various camera models such as pinhole, wide-angle, and 360{\deg} panoramas. We
have benchmarked ViPE on multiple benchmarks. Notably, it outperforms existing
uncalibrated pose estimation baselines by 18%/50% on TUM/KITTI sequences, and
runs at 3-5FPS on a single GPU for standard input resolutions. We use ViPE to
annotate a large-scale collection of videos. This collection includes around
100K real-world internet videos, 1M high-quality AI-generated videos, and 2K
panoramic videos, totaling approximately 96M frames -- all annotated with
accurate camera poses and dense depth maps. We open-source ViPE and the
annotated dataset with the hope of accelerating the development of spatial AI
systems.

</details>


### [6] [HQ-OV3D: A High Box Quality Open-World 3D Detection Framework based on Diffision Model](https://arxiv.org/abs/2508.10935)
*Qi Liu,Yabei Li,Hongsong Wang,Lei He*

Main category: cs.CV

TL;DR: 本文提出HQ-OV3D框架，提升开放词汇3D目标检测的伪标签精度，显著提高新类别检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有3D检测模型多为闭集设定，难以满足自动驾驶等实际开放世界需求。现有开放词汇3D检测依赖伪标签生成和语义对齐，但当前主流的视觉-语言模型虽提升了语义准确性，却忽视了伪标签在几何（如3D边界框精度）上的质量。

Method: HQ-OV3D框架包含两个关键模块：1）IMCV Proposal Generator，采用跨模态几何一致性生成高质量初始3D候选框；2）ACA Denoiser，利用已标注类别的几何先验，通过基于DDIM的去噪机制迭代细化候选框，提高盒子精度。整体目标是得到高几何质量的开放词汇伪标签。

Result: 在开放词汇的新类别上，HQ-OV3D生成的伪标签用于训练检测器，可实现比SOTA方法高7.37%的mAP提升，证明了框架伪标签的高质量。

Conclusion: HQ-OV3D不仅可作为强大的端到端开放词汇3D检测器使用，还可作为现有开放词汇检测和标注流程中的高质量伪标签生成插件。

Abstract: Traditional closed-set 3D detection frameworks fail to meet the demands of
open-world applications like autonomous driving. Existing open-vocabulary 3D
detection methods typically adopt a two-stage pipeline consisting of
pseudo-label generation followed by semantic alignment. While vision-language
models (VLMs) recently have dramatically improved the semantic accuracy of
pseudo-labels, their geometric quality, particularly bounding box precision,
remains commonly neglected.To address this issue, we propose a High Box Quality
Open-Vocabulary 3D Detection (HQ-OV3D) framework, dedicated to generate and
refine high-quality pseudo-labels for open-vocabulary classes. The framework
comprises two key components: an Intra-Modality Cross-Validated (IMCV) Proposal
Generator that utilizes cross-modality geometric consistency to generate
high-quality initial 3D proposals, and an Annotated-Class Assisted (ACA)
Denoiser that progressively refines 3D proposals by leveraging geometric priors
from annotated categories through a DDIM-based denoising mechanism.Compared to
the state-of-the-art method, training with pseudo-labels generated by our
approach achieves a 7.37% improvement in mAP on novel classes, demonstrating
the superior quality of the pseudo-labels produced by our framework. HQ-OV3D
can serve not only as a strong standalone open-vocabulary 3D detector but also
as a plug-in high-quality pseudo-label generator for existing open-vocabulary
detection or annotation pipelines.

</details>


### [7] [Vision-Only Gaussian Splatting for Collaborative Semantic Occupancy Prediction](https://arxiv.org/abs/2508.10936)
*Cheng Chen,Hao Huang,Saurabh Bagchi*

Main category: cs.CV

TL;DR: 本文提出了一种基于稀疏3D语义高斯斑点（Gaussian splatting）的协作感知方法，用于联网自动驾驶车辆的3D语义占用预测，在降低通信成本的同时显著提升了感知精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的3D语义占用预测方法多依赖于稠密3D体素（通信开销大）或2D平面特征（需精确深度或额外监督），这在协作感知场景下存在局限。因此亟需高效且结构优良的信息共享方法来提升协作车辆感知能力。

Method: 方法通过共享和融合中间的稀疏3D语义高斯基元，实现跨车协作感知预测。具体包括：1）基于邻域的跨体融合去除重复、抑制噪声；2）每个基元联合编码几何与语义，简化对深度监督的要求，并易于对齐；3）采用面向物体的稀疏信息传递，减少通信量同时保留结构信息。

Result: 在多组实验中，提出方法在mIoU指标上，超过单车方案8.42分，超越协作基线3.28分，在IoU上分别提升5.11和22.41分。在只保留34.6%通信量时mIoU仍提升1.9，显示出高效、鲁棒的优势。

Conclusion: 稀疏高斯斑点方法极大提升了协作3D语义占用预测的准确性和通信效率，对联网车辆的感知有重要实用价值。

Abstract: Collaborative perception enables connected vehicles to share information,
overcoming occlusions and extending the limited sensing range inherent in
single-agent (non-collaborative) systems. Existing vision-only methods for 3D
semantic occupancy prediction commonly rely on dense 3D voxels, which incur
high communication costs, or 2D planar features, which require accurate depth
estimation or additional supervision, limiting their applicability to
collaborative scenarios. To address these challenges, we propose the first
approach leveraging sparse 3D semantic Gaussian splatting for collaborative 3D
semantic occupancy prediction. By sharing and fusing intermediate Gaussian
primitives, our method provides three benefits: a neighborhood-based
cross-agent fusion that removes duplicates and suppresses noisy or inconsistent
Gaussians; a joint encoding of geometry and semantics in each primitive, which
reduces reliance on depth supervision and allows simple rigid alignment; and
sparse, object-centric messages that preserve structural information while
reducing communication volume. Extensive experiments demonstrate that our
approach outperforms single-agent perception and baseline collaborative methods
by +8.42 and +3.28 points in mIoU, and +5.11 and +22.41 points in IoU,
respectively. When further reducing the number of transmitted Gaussians, our
method still achieves a +1.9 improvement in mIoU, using only 34.6%
communication volume, highlighting robust performance under limited
communication budgets.

</details>


### [8] [Personalized Face Super-Resolution with Identity Decoupling and Fitting](https://arxiv.org/abs/2508.10937)
*Jiarui Yang,Hang Guo,Wen Huang,Tao Dai,Shutao Xia*

Main category: cs.CV

TL;DR: 该论文提出了一种新的面部超分辨率（FSR）方法IDFSR，在极端降质条件（如大倍数放大）下，能够恢复更真实且身份一致性更高的人脸图像，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有FSR方法在极端降质场景下难以重建具有真实性和身份一致性的人脸，往往生成伪造信息，导致身份特征丢失。本研究旨在解决大倍数放大时面部身份信息/关键属性严重丢失的问题。

Method: 作者提出IDFSR方法，包含三个关键设计：1）通过mask遮罩低分辨率（LR）人脸，去除低置信身份线索；2）参考图像变形对齐LR输入，提供风格引导；3）利用从高分辨率真值图像提取的身份嵌入进行细粒度身份建模与个性化调优。先用扩散模型预训练以分离风格和身份，再通过少量目标身份图片微调身份嵌入。

Result: 大量定量评估和视觉对比表明，IDFSR在极端降质条件下在身份一致性和感知质量上都显著优于主流方法，能更好恢复面部真实性和个体特征。

Conclusion: 提出的方法有效缓解了极端放大情况下身份特征重建的困难，既提升了人脸超分辨率结果的身份一致性，又抑制了伪造效果，在此类任务中具有明显优势。

Abstract: In recent years, face super-resolution (FSR) methods have achieved remarkable
progress, generally maintaining high image fidelity and identity (ID)
consistency under standard settings. However, in extreme degradation scenarios
(e.g., scale $> 8\times$), critical attributes and ID information are often
severely lost in the input image, making it difficult for conventional models
to reconstruct realistic and ID-consistent faces. Existing methods tend to
generate hallucinated faces under such conditions, producing restored images
lacking authentic ID constraints. To address this challenge, we propose a novel
FSR method with Identity Decoupling and Fitting (IDFSR), designed to enhance ID
restoration under large scaling factors while mitigating hallucination effects.
Our approach involves three key designs: 1) \textbf{Masking} the facial region
in the low-resolution (LR) image to eliminate unreliable ID cues; 2)
\textbf{Warping} a reference image to align with the LR input, providing style
guidance; 3) Leveraging \textbf{ID embeddings} extracted from ground truth (GT)
images for fine-grained ID modeling and personalized adaptation. We first
pretrain a diffusion-based model to explicitly decouple style and ID by forcing
it to reconstruct masked LR face regions using both style and identity
embeddings. Subsequently, we freeze most network parameters and perform
lightweight fine-tuning of the ID embedding using a small set of target ID
images. This embedding encodes fine-grained facial attributes and precise ID
information, significantly improving both ID consistency and perceptual
quality. Extensive quantitative evaluations and visual comparisons demonstrate
that the proposed IDFSR substantially outperforms existing approaches under
extreme degradation, particularly achieving superior performance on ID
consistency.

</details>


### [9] [Deep Learning for Automated Identification of Vietnamese Timber Species: A Tool for Ecological Monitoring and Conservation](https://arxiv.org/abs/2508.10938)
*Tianyu Song,Van-Doan Duong,Thi-Phuong Le,Ton Viet Ta*

Main category: cs.CV

TL;DR: 本研究利用深度学习方法，实现对越南常见十种木材的自动分类，结果表明轻量级神经网络模型在准确率和效率之间取得了优异平衡。


<details>
  <summary>Details</summary>
Motivation: 木材种类的准确识别对于生态监测、生物多样性保护和可持续森林管理非常重要，但传统方法费时且依赖专家。

Method: 构建了一个包含田间采集样本的自定义木材图像数据集，评估了五种先进的卷积神经网络架构（ResNet50、EfficientNet、MobileViT、MobileNetV3、ShuffleNetV2）的分类性能。

Result: ShuffleNetV2在准确率与效率之间表现最佳，平均准确率为99.29%，F1得分为99.35%（20次独立实验平均值）。

Conclusion: 轻量级深度学习模型具备在资源受限环境下进行高精度、实时木材识别的潜力，为自动化木材分类和森林生物多样性评估提供了可扩展的解决方案。

Abstract: Accurate identification of wood species plays a critical role in ecological
monitoring, biodiversity conservation, and sustainable forest management.
Traditional classification approaches relying on macroscopic and microscopic
inspection are labor-intensive and require expert knowledge. In this study, we
explore the application of deep learning to automate the classification of ten
wood species commonly found in Vietnam. A custom image dataset was constructed
from field-collected wood samples, and five state-of-the-art convolutional
neural network architectures--ResNet50, EfficientNet, MobileViT, MobileNetV3,
and ShuffleNetV2--were evaluated. Among these, ShuffleNetV2 achieved the best
balance between classification performance and computational efficiency, with
an average accuracy of 99.29\% and F1-score of 99.35\% over 20 independent
runs. These results demonstrate the potential of lightweight deep learning
models for real-time, high-accuracy species identification in
resource-constrained environments. Our work contributes to the growing field of
ecological informatics by providing scalable, image-based solutions for
automated wood classification and forest biodiversity assessment.

</details>


### [10] [NIRMAL Pooling: An Adaptive Max Pooling Approach with Non-linear Activation for Enhanced Image Classification](https://arxiv.org/abs/2508.10940)
*Nirmal Gaud,Krishna Kumar Jha,Jhimli Adhikari,Adhini Nasarin P S,Joydeep Das,Samarth S Deshpande,Nitasha Barara,Vaduguru Venkata Ramya,Santu Saha,Mehmet Tarik Baran,Sarangi Venkateshwarlu,Anusha M D,Surej Mouli,Preeti Katiyar,Vipin Kumar Chaudhary*

Main category: cs.CV

TL;DR: 提出了一种新的卷积神经网络池化层NIRMAL Pooling，并在多个数据集上对比了其与传统最大池化的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的最大池化方法在特征表达和鲁棒性上存在一定局限，因此作者希望设计一种更具适应性和表达力的池化方法，以提升CNN在图像分类任务中的表现。

Method: 设计了NIRMAL Pooling，将自适应最大池化和非线性激活函数（ReLU）结合，并支持根据目标输出尺寸动态调整池化参数。然后在MNIST Digits、MNIST Fashion和CIFAR-10三个数据集上，与最大池化进行性能对比。

Result: NIRMAL Pooling在所有数据集上均取得了比最大池化更高的测试准确率，尤其在复杂数据集如CIFAR-10上提升明显。

Conclusion: NIRMAL Pooling能够提升CNN的性能，具有更好的特征表达能力和鲁棒性，是传统池化方法的灵活、可靠的替代方案。

Abstract: This paper presents NIRMAL Pooling, a novel pooling layer for Convolutional
Neural Networks (CNNs) that integrates adaptive max pooling with non-linear
activation function for image classification tasks. The acronym NIRMAL stands
for Non-linear Activation, Intermediate Aggregation, Reduction, Maximum,
Adaptive, and Localized. By dynamically adjusting pooling parameters based on
desired output dimensions and applying a Rectified Linear Unit (ReLU)
activation post-pooling, NIRMAL Pooling improves robustness and feature
expressiveness. We evaluated its performance against standard Max Pooling on
three benchmark datasets: MNIST Digits, MNIST Fashion, and CIFAR-10. NIRMAL
Pooling achieves test accuracies of 99.25% (vs. 99.12% for Max Pooling) on
MNIST Digits, 91.59% (vs. 91.44%) on MNIST Fashion, and 70.49% (vs. 68.87%) on
CIFAR-10, demonstrating consistent improvements, particularly on complex
datasets. This work highlights the potential of NIRMAL Pooling to enhance CNN
performance in diverse image recognition tasks, offering a flexible and
reliable alternative to traditional pooling methods.

</details>


### [11] [Topological Structure Description for Artcode Detection Using the Shape of Orientation Histogram](https://arxiv.org/abs/2508.10942)
*Liming Xu,Dave Towey,Andrew P. French,Steve Benford*

Main category: cs.CV

TL;DR: 本论文提出了一种检测和识别‘Artcodes’新型装饰性标记的方法，通过提取其拓扑特征实现对这些人类可读、机器可识别标记的检测。


<details>
  <summary>Details</summary>
Motivation: 随着智能手机普及和AR/VR技术发展，虚拟元素与现实环境的融合变得日益重要，能自动检测并与这些虚实结合的对象交互成为一项重要需求。Artcodes作为新型标记，因具备美观与信息承载双重功能，对其自动识别具有实际意义。

Method: 作者首次将Artcode标记检测问题表述为一种‘Artcode proposal detection’的计算机视觉任务，并提出了一种基于拓扑结构的新特征描述符‘shape of orientation histogram’，以此区分形状相似但语义不同的对象。实验中，作者构建了数据集并进行了系统性能评测。

Result: 实验结果表明，所提出的特征描述符能够有效识别和表示Artcode的拓扑结构，基于其构建的检测系统在Artcode proposal detection任务上表现出较好效果。

Conclusion: 尽管本研究仅为初步探索，但验证了基于特征的拓扑对象检测系统的可行性，为相关交互与应用场景带来新的可能性。

Abstract: The increasing ubiquity of smartphones and resurgence of VR/AR techniques, it
is expected that our everyday environment may soon be decorating with objects
connecting with virtual elements. Alerting to the presence of these objects is
therefore the first step for motivating follow-up further inspection and
triggering digital material attached to the objects. This work studies a
special kind of these objects -- Artcodes -- a human-meaningful and
machine-readable decorative markers that camouflage themselves with freeform
appearance by encoding information into their topology. We formulate this
problem of recongising the presence of Artcodes as Artcode proposal detection,
a distinct computer vision task that classifies topologically similar but
geometrically and semantically different objects as a same class. To deal with
this problem, we propose a new feature descriptor, called the shape of
orientation histogram, to describe the generic topological structure of an
Artcode. We collect datasets and conduct comprehensive experiments to evaluate
the performance of the Artcode detection proposer built upon this new feature
vector. Our experimental results show the feasibility of the proposed feature
vector for representing topological structures and the effectiveness of the
system for detecting Artcode proposals. Although this work is an initial
attempt to develop a feature-based system for detecting topological objects
like Artcodes, it would open up new interaction opportunities and spark
potential applications of topological object detection.

</details>


### [12] [Analysis of the Compaction Behavior of Textile Reinforcements in Low-Resolution In-Situ CT Scans via Machine-Learning and Descriptor-Based Methods](https://arxiv.org/abs/2508.10943)
*Christian Düreth,Jan Condé-Wolter,Marek Danczak,Karsten Tittmann,Jörn Jaschinski,Andreas Hornig,Maik Gude*

Main category: cs.CV

TL;DR: 本研究提出了一种利用低分辨率CT结合深度学习语义分割与统计分析的方法，量化纺织增强复合材料在压实过程中的嵌套行为，并能有效提取层厚度和嵌套度等关键结构特征。


<details>
  <summary>Details</summary>
Motivation: 纺织增强复合材料的多尺度结构对其力学性能有决定性影响，而层间嵌套（纱线局部穿插、错位）是影响刚度、渗透性及损伤容忍性的关键因素。因此迫切需要一种方法定量分析压实过程中干纺织层的嵌套行为。

Method: 作者采用原位压实实验，结合分辨率为20.22微米/像素的CT扫描对不同层叠方式进行成像。提出了定制化的3D-UNet模型，实现了纱线不同相（基体、经纱、纬纱）在不同压实阶段的语义分割，并通过两点相关函数S2对空间结构进行统计分析，从而获得平均层厚度和嵌套度的概率性提取。

Result: 分割模型在各阶段纤维体积分数为50-60%时表现优良，最低均值IOU达0.822，F1分数0.902。利用S2函数提取的几何指标与显微镜验证结果高度一致。

Conclusion: 该方法为基于工业CT数据的多层织物复合材料几何特征高效、可靠提取提供了工具基础，为后续的结构逆向建模及基于描述符的织物预成型体分析奠定了基础。

Abstract: A detailed understanding of material structure across multiple scales is
essential for predictive modeling of textile-reinforced composites. Nesting --
characterized by the interlocking of adjacent fabric layers through local
interpenetration and misalignment of yarns -- plays a critical role in defining
mechanical properties such as stiffness, permeability, and damage tolerance.
This study presents a framework to quantify nesting behavior in dry textile
reinforcements under compaction using low-resolution computed tomography (CT).
In-situ compaction experiments were conducted on various stacking
configurations, with CT scans acquired at 20.22 $\mu$m per voxel resolution. A
tailored 3D{-}UNet enabled semantic segmentation of matrix, weft, and fill
phases across compaction stages corresponding to fiber volume contents of
50--60 %. The model achieved a minimum mean Intersection-over-Union of 0.822
and an $F1$ score of 0.902. Spatial structure was subsequently analyzed using
the two-point correlation function $S_2$, allowing for probabilistic extraction
of average layer thickness and nesting degree. The results show strong
agreement with micrograph-based validation. This methodology provides a robust
approach for extracting key geometrical features from industrially relevant CT
data and establishes a foundation for reverse modeling and descriptor-based
structural analysis of composite preforms.

</details>


### [13] [iWatchRoad: Scalable Detection and Geospatial Visualization of Potholes for Smart Cities](https://arxiv.org/abs/2508.10945)
*Rishi Raj Sahoo,Surbhi Saswati Mohanty,Subhankar Mishra*

Main category: cs.CV

TL;DR: 该论文提出了iWatchRoad系统，可对印度道路自动检测坑洞、GPS定位并实时在OpenStreetMap上标注。系统以车载摄像头录制的视频构建自有数据集，采用YOLO模型检测坑洞，并用OCR识别时间戳，与GPS日志同步实现精准定位。结果可通过网页端可视化，助力道路维护。该方案硬件要求低、成本低，适合大规模普及。


<details>
  <summary>Details</summary>
Motivation: 印度道路多样且维护不足，坑洞威胁行车安全与路况管养。缺乏自动、准确的数据采集和管理系统，制约了有效维护和规划。

Method: 作者提出iWatchRoad系统：使用车载摄像头采集道路视频，人工标注构建7000帧自有数据集，基于YOLO模型实现实时坑洞检测，并开发OCR模块识别视频中的时间戳，将其与GPS日志同步，实现坑洞的精准地理定位。所有检测结果参数与帧存入数据库，并以用户友好的方式通过OSM平台展示。

Result: 系统在多种道路类型、光照和天气条件下表现良好，实现了高精度坑洞检测和定位。所得信息可用作政府道路评估与维护的有效数据依据。

Conclusion: iWatchRoad是一种经济高效、硬件需求低、自动化、可扩展的解决方案，提升了复杂环境下的道路坑洞检测和管理，适用于发展中国家的城乡道路管理。

Abstract: Potholes on the roads are a serious hazard and maintenance burden. This poses
a significant threat to road safety and vehicle longevity, especially on the
diverse and under-maintained roads of India. In this paper, we present a
complete end-to-end system called iWatchRoad for automated pothole detection,
Global Positioning System (GPS) tagging, and real time mapping using
OpenStreetMap (OSM). We curated a large, self-annotated dataset of over 7,000
frames captured across various road types, lighting conditions, and weather
scenarios unique to Indian environments, leveraging dashcam footage. This
dataset is used to fine-tune, Ultralytics You Only Look Once (YOLO) model to
perform real time pothole detection, while a custom Optical Character
Recognition (OCR) module was employed to extract timestamps directly from video
frames. The timestamps are synchronized with GPS logs to geotag each detected
potholes accurately. The processed data includes the potholes' details and
frames as metadata is stored in a database and visualized via a user friendly
web interface using OSM. iWatchRoad not only improves detection accuracy under
challenging conditions but also provides government compatible outputs for road
assessment and maintenance planning through the metadata visible on the
website. Our solution is cost effective, hardware efficient, and scalable,
offering a practical tool for urban and rural road management in developing
regions, making the system automated. iWatchRoad is available at
https://smlab.niser.ac.in/project/iwatchroad

</details>


### [14] [IPG: Incremental Patch Generation for Generalized Adversarial Patch Training](https://arxiv.org/abs/2508.10946)
*Wonho Lee,Hyunsik Na,Jisu Lee,Daeseon Choi*

Main category: cs.CV

TL;DR: 提出了一种高效的对抗补丁生成方法IPG，使AI模型在目标检测等视觉任务下更易被攻击，效率提高11倍。


<details>
  <summary>Details</summary>
Motivation: 对抗补丁威胁传统AI视觉模型安全，但现有补丁生成方法效率低，难以大规模应用并掩盖更广泛的模型脆弱性。

Method: 提出增量式补丁生成（IPG）方法，采用高效策略生成对抗补丁，通过与主流目标检测模型（如YOLO）的实验与消融分析验证其泛化性和攻击性能。

Result: IPG生成速度比现有方法快11.1倍，且攻击效果相当。消融实验展示了补丁可覆盖更多模型脆弱点，并能辅助提升模型防御能力。

Conclusion: IPG方法不仅提升了补丁生成效率，还增强了对抗泛化性，有助于AI安全领域真实系统中抗攻击能力的提升，具备广泛应用前景。

Abstract: The advent of adversarial patches poses a significant challenge to the
robustness of AI models, particularly in the domain of computer vision tasks
such as object detection. In contradistinction to traditional adversarial
examples, these patches target specific regions of an image, resulting in the
malfunction of AI models. This paper proposes Incremental Patch Generation
(IPG), a method that generates adversarial patches up to 11.1 times more
efficiently than existing approaches while maintaining comparable attack
performance. The efficacy of IPG is demonstrated by experiments and ablation
studies including YOLO's feature distribution visualization and adversarial
training results, which show that it produces well-generalized patches that
effectively cover a broader range of model vulnerabilities. Furthermore,
IPG-generated datasets can serve as a robust knowledge foundation for
constructing a robust model, enabling structured representation, advanced
reasoning, and proactive defenses in AI security ecosystems. The findings of
this study suggest that IPG has considerable potential for future utilization
not only in adversarial patch defense but also in real-world applications such
as autonomous vehicles, security systems, and medical imaging, where AI models
must remain resilient to adversarial attacks in dynamic and high-stakes
environments.

</details>


### [15] [MedAtlas: Evaluating LLMs for Multi-Round, Multi-Task Medical Reasoning Across Diverse Imaging Modalities and Clinical Text](https://arxiv.org/abs/2508.10947)
*Ronghao Xu,Zhen Huang,Yangbo Wei,Xiaoqian Zhou,Zikang Xu,Ting Liu,Zihang Jiang,S. Kevin Zhou*

Main category: cs.CV

TL;DR: 该论文提出了MedAtlas，一种用于评估大语言模型在现实医学推理任务中表现的新型基准框架，涵盖多轮对话、多模态影像、多任务融合及高临床真实性。


<details>
  <summary>Details</summary>
Motivation: 现有医学多模态基准仅涉及单张影像和单轮任务，难以反映临床实践的多模态集成与复杂交互，限制了人工智能在真实医疗决策中的适应性和诊断推理能力，因此急需更全面、真实的评测平台。

Method: 作者设计了MedAtlas基准，拥有四大特性：多轮对话、多模态医学影像、多任务融合和高临床真实性。支持开放式与封闭式多轮问答、多影像联合推理和综合疾病诊断，涵盖CT、MRI、PET、超声等多类型影像及病例文本。所有任务均提供专家标注金标准，并提出了“轮链准确率”和“误差传播抵抗力”两种新评测指标。

Result: 在MedAtlas上评测现有多模态模型，结果表明现有模型在多阶段临床推理任务中表现存在显著差距，无法满足复杂临床场景需求。

Conclusion: MedAtlas为医学AI提供了高挑战性、真实反映临床需求的评测平台，将推动稳健和可信医疗人工智能的发展。

Abstract: Artificial intelligence has demonstrated significant potential in clinical
decision-making; however, developing models capable of adapting to diverse
real-world scenarios and performing complex diagnostic reasoning remains a
major challenge. Existing medical multi-modal benchmarks are typically limited
to single-image, single-turn tasks, lacking multi-modal medical image
integration and failing to capture the longitudinal and multi-modal interactive
nature inherent to clinical practice. To address this gap, we introduce
MedAtlas, a novel benchmark framework designed to evaluate large language
models on realistic medical reasoning tasks. MedAtlas is characterized by four
key features: multi-turn dialogue, multi-modal medical image interaction,
multi-task integration, and high clinical fidelity. It supports four core
tasks: open-ended multi-turn question answering, closed-ended multi-turn
question answering, multi-image joint reasoning, and comprehensive disease
diagnosis. Each case is derived from real diagnostic workflows and incorporates
temporal interactions between textual medical histories and multiple imaging
modalities, including CT, MRI, PET, ultrasound, and X-ray, requiring models to
perform deep integrative reasoning across images and clinical texts. MedAtlas
provides expert-annotated gold standards for all tasks. Furthermore, we propose
two novel evaluation metrics: Round Chain Accuracy and Error Propagation
Resistance. Benchmark results with existing multi-modal models reveal
substantial performance gaps in multi-stage clinical reasoning. MedAtlas
establishes a challenging evaluation platform to advance the development of
robust and trustworthy medical AI.

</details>


### [16] [From Promise to Practical Reality: Transforming Diffusion MRI Analysis with Fast Deep Learning Enhancement](https://arxiv.org/abs/2508.10950)
*Xinyi Wang,Michael Barnett,Frederique Boonstra,Yael Barnett,Mariano Cabezas,Arkiev D'Souza,Matthew C. Kiernan,Kain Kyle,Meng Law,Lynette Masters,Zihao Tang,Stephen Tisch,Sicong Tu,Anneke Van Der Walt,Dongang Wang,Fernando Calamante,Weidong Cai,Chenyu Wang*

Main category: cs.CV

TL;DR: 本文提出了一种加速的深度学习框架FastFOD-Net，用于提升临床低质量dMRI数据下的纤维取向分布（FOD）估计，其在健康人及多种神经系统疾病患者中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前FOD的准确性依赖于高质量MRI，临床常用的单壳、低角分辨率数据难以得到可靠FOD。此外，现有基于深度学习的方法多仅在健康人群评估，亟需针对更广泛临床应用和疾病患者的数据验证。

Method: 开发并优化了FastFOD-Net深度学习框架，该方法能在单壳低角分辨率dMRI数据上生成高质量FOD图像，大幅提升推理速度，并在健康对照及六种神经系统疾病患者数据上进行了全面评估。

Result: FastFOD-Net展现出较前代方法高达60倍的速度提升，同时在不同群体中均能提升FOD质量和下游分析的可解释性与有效性；显著降低了测量误差，减少了样本量需求。

Conclusion: FastFOD-Net不仅使临床常规采集的数据可作高质量白质研究分析，推动了深度学习MRI增强技术的临床广泛应用，并且增强了医生和研究者对相关AI工具的信任，有助于疾病鉴别和脑连接组领域的发展。

Abstract: Fiber orientation distribution (FOD) is an advanced diffusion MRI modeling
technique that represents complex white matter fiber configurations, and a key
step for subsequent brain tractography and connectome analysis. Its reliability
and accuracy, however, heavily rely on the quality of the MRI acquisition and
the subsequent estimation of the FODs at each voxel. Generating reliable FODs
from widely available clinical protocols with single-shell and
low-angular-resolution acquisitions remains challenging but could potentially
be addressed with recent advances in deep learning-based enhancement
techniques. Despite advancements, existing methods have predominantly been
assessed on healthy subjects, which have proved to be a major hurdle for their
clinical adoption. In this work, we validate a newly optimized enhancement
framework, FastFOD-Net, across healthy controls and six neurological disorders.
This accelerated end-to-end deep learning framework enhancing FODs with
superior performance and delivering training/inference efficiency for clinical
use ($60\times$ faster comparing to its predecessor). With the most
comprehensive clinical evaluation to date, our work demonstrates the potential
of FastFOD-Net in accelerating clinical neuroscience research, empowering
diffusion MRI analysis for disease differentiation, improving interpretability
in connectome applications, and reducing measurement errors to lower sample
size requirements. Critically, this work will facilitate the more widespread
adoption of, and build clinical trust in, deep learning based methods for
diffusion MRI enhancement. Specifically, FastFOD-Net enables robust analysis of
real-world, clinical diffusion MRI data, comparable to that achievable with
high-quality research acquisitions.

</details>


### [17] [Empowering Multimodal LLMs with External Tools: A Comprehensive Survey](https://arxiv.org/abs/2508.10955)
*Wenbin An,Jiahao Nie,Yaqiang Wu,Feng Tian,Shijian Lu,Qinghua Zheng*

Main category: cs.CV

TL;DR: 本文综述了结合外部工具提升多模态大语言模型（MLLM）能力的最新进展和未来方向，强调工具增强对提升模型多模态数据处理、下游任务表现和评价体系的作用。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs在多模态任务上取得显著突破，但受限于多模态数据质量不足、复杂任务表现不佳和评价体系不完备，模型的可靠性和广泛适用性受到影响。以人类借助外部工具推理为启发，探索工具增强MLLMs可有效突破上述瓶颈。

Method: 文章以文献调研为主，系统梳理了工具增强MLLMs的研究框架，围绕如何通过APIs、专家模型、知识库等外部工具提升数据获取与标注、下游任务能力、评测方法等四大方面展开讨论，并分析了现有局限和展望未来潜力。

Result: 归纳表明，融合外部工具能在高质量数据获取、复杂任务推理、多维评价等方面显著提升MLLM表现。工具增强方法逐渐成为推动MLLMs普适和智能化的重要路径。

Conclusion: 外部工具赋能MLLMs具备巨大应用和发展前景，能有效补足现有模型在多模态理解和推理方面的不足。未来应持续完善工具组件、数据管道和综合评估，推动MLLMs向人工通用智能方向演进。

Abstract: By integrating the perception capabilities of multimodal encoders with the
generative power of Large Language Models (LLMs), Multimodal Large Language
Models (MLLMs), exemplified by GPT-4V, have achieved great success in various
multimodal tasks, pointing toward a promising pathway to artificial general
intelligence. Despite this progress, the limited quality of multimodal data,
poor performance on many complex downstream tasks, and inadequate evaluation
protocols continue to hinder the reliability and broader applicability of MLLMs
across diverse domains. Inspired by the human ability to leverage external
tools for enhanced reasoning and problem-solving, augmenting MLLMs with
external tools (e.g., APIs, expert models, and knowledge bases) offers a
promising strategy to overcome these challenges. In this paper, we present a
comprehensive survey on leveraging external tools to enhance MLLM performance.
Our discussion is structured along four key dimensions about external tools:
(1) how they can facilitate the acquisition and annotation of high-quality
multimodal data; (2) how they can assist in improving MLLM performance on
challenging downstream tasks; (3) how they enable comprehensive and accurate
evaluation of MLLMs; (4) the current limitations and future directions of
tool-augmented MLLMs. Through this survey, we aim to underscore the
transformative potential of external tools in advancing MLLM capabilities,
offering a forward-looking perspective on their development and applications.
The project page of this paper is publicly available
athttps://github.com/Lackel/Awesome-Tools-for-MLLMs.

</details>


### [18] [ORBIT: An Object Property Reasoning Benchmark for Visual Inference Tasks](https://arxiv.org/abs/2508.10956)
*Abhishek Kolari,Mohammadhossein Khojasteh,Yifan Jiang,Floris den Hengst,Filip Ilievski*

Main category: cs.CV

TL;DR: 本文提出了一个全面的新基准ORBIT，用以系统性评估视觉语言模型（VLM）在对象属性推理方面的表现，发现现有模型在复杂推理任务上的表现远落后于人类。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉语言模型在流行的视觉问答任务上取得了突破性进展，但它们是否能够真正理解和推理图像中的物体属性仍存疑。现有基准过于简单，缺乏细粒度的对象属性和推理水平，不能很好地衡量模型在实际推理任务中的能力。

Method: 作者设计了一个系统化评测框架，包括三种代表性图片类型、三种递进式推理复杂度和四个基于常识推理的物体属性维度，进而构建出 ORBIT 基准。基准含360张图片和1080个基于计数的问题，并用12种主流VLM模型在零样本设置下进行评测。

Result: 实验显示，所有模型的表现远逊于人类最佳，仅有40%的最高准确率。尤其在真实照片、关于物理与功能属性的反事实推理和高计数问题上，模型表现更差。

Conclusion: 该研究表明现有VLM在对象属性推理上的能力有限，需要开发更大规模和系统的基准、通用的注释标准，并探索新的推理方法。ORBIT基准和代码已开源以促进相关研究。

Abstract: While vision-language models (VLMs) have made remarkable progress on many
popular visual question answering (VQA) benchmarks, it remains unclear whether
they abstract and reason over depicted objects. Inspired by human object
categorisation, object property reasoning involves identifying and recognising
low-level details and higher-level abstractions. While current VQA benchmarks
consider a limited set of object property attributes like size, they typically
blend perception and reasoning, and lack representativeness in terms of
reasoning and image categories. To this end, we introduce a systematic
evaluation framework with images of three representative types, three reasoning
levels of increasing complexity, and four object property dimensions driven by
prior work on commonsense reasoning. We develop a procedure to instantiate this
benchmark into ORBIT, a multi-level reasoning VQA benchmark for object
properties comprising 360 images paired with a total of 1,080 count-based
questions. Experiments with 12 state-of-the-art VLMs in zero-shot settings
reveal significant limitations compared to humans, with the best-performing
model only reaching 40\% accuracy. VLMs struggle particularly with realistic
(photographic) images, counterfactual reasoning about physical and functional
properties, and higher counts. ORBIT points to the need to develop methods for
scalable benchmarking, generalize annotation guidelines, and explore additional
reasoning VLMs. We make the ORBIT benchmark and the experimental code available
to support such endeavors.

</details>


### [19] [CSNR and JMIM Based Spectral Band Selection for Reducing Metamerism in Urban Driving](https://arxiv.org/abs/2508.10962)
*Jiarong Li,Imad Ali Shah,Diarmaid Geever,Fiachra Collins,Enda Ward,Martin Glavin,Edward Jones,Brian Deegan*

Main category: cs.CV

TL;DR: 本文提出利用高光谱成像（HSI）结合信息理论和图像质量指标，优化波段选择，显著提升了易受伤害道路使用者（VRU）在复杂可视条件下的可区分度，优于传统RGB成像。


<details>
  <summary>Details</summary>
Motivation: 现有车辆感知系统在遇到受体色现象（metamerism，即不同材料在RGB图像中看起来极为相似）时容易混淆VRU与背景，影响道路安全。作者希望通过引入更多光谱信息，解决RGB图像分辨力不足的问题。

Method: 作者提出一种高光谱波段挑选策略，融合了联合互信息最大化、相关性分析及新颖的图像对比信噪比等信息理论与质量指标，从上百个波段中寻找最具区分度的三个，并在H-City数据集上验证效果。

Result: 选出的波段（497nm、607nm、895nm，±27nm）重建的伪彩色图像在各类可分性指标（欧氏距离、SAM、Hotelling’s $T^2$、CIE ΔE）上相较RGB提升幅度分别达70.24%、528.46%、1206.83%、246.62%，大幅降低了受体色混淆。

Conclusion: 高光谱优化波段的输入能极大增强VRU与背景的可分性，有助于自动驾驶和高级驾驶辅助系统提升感知鲁棒性和道路安全。

Abstract: Protecting Vulnerable Road Users (VRU) is a critical safety challenge for
automotive perception systems, particularly under visual ambiguity caused by
metamerism, a phenomenon where distinct materials appear similar in RGB
imagery. This work investigates hyperspectral imaging (HSI) to overcome this
limitation by capturing unique material signatures beyond the visible spectrum,
especially in the Near-Infrared (NIR). To manage the inherent
high-dimensionality of HSI data, we propose a band selection strategy that
integrates information theory techniques (joint mutual information
maximization, correlation analysis) with a novel application of an image
quality metric (contrast signal-to-noise ratio) to identify the most spectrally
informative bands. Using the Hyperspectral City V2 (H-City) dataset, we
identify three informative bands (497 nm, 607 nm, and 895 nm, $\pm$27 nm) and
reconstruct pseudo-color images for comparison with co-registered RGB.
Quantitative results demonstrate increased dissimilarity and perceptual
separability of VRU from the background. The selected HSI bands yield
improvements of 70.24%, 528.46%, 1206.83%, and 246.62% for dissimilarity
(Euclidean, SAM, $T^2$) and perception (CIE $\Delta E$) metrics, consistently
outperforming RGB and confirming a marked reduction in metameric confusion. By
providing a spectrally optimized input, our method enhances VRU separability,
establishing a robust foundation for downstream perception tasks in Advanced
Driver Assistance Systems (ADAS) and Autonomous Driving (AD), ultimately
contributing to improved road safety.

</details>


### [20] [EVCtrl: Efficient Control Adapter for Visual Generation](https://arxiv.org/abs/2508.10963)
*Zixiang Yang,Yue Ma,Yinhan Zhang,Shanhui Mo,Dongrui Liu,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出了EVCtrl，一个减少图像和视频生成控制开销的新方法，极大提升了计算效率，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有可控生成（如ControlNet）存在高延迟和重复计算的问题，尤其在视频生成时更为严重。研究者需要同时拥有模型可控性和高效性。

Method: 提出EVCtrl控制适配器，采用时空双缓存策略。空间上，通过剖析网络层的响应，将网络划分为全局与局部区域，仅在需要控制信号的局部区域进行重点计算，跳过冗余部分。时间上，省略不必要的去噪步骤，减少冗余。无需重新训练模型，可直接作为插件使用。

Result: 在CogVideo-Controlnet和Wan2.1-Controlnet上实现了2.16倍和2.05倍的加速，且几乎没有画质损失。大量实验证明方法同样适用于静态图像和视频的生成控制。

Conclusion: EVCtrl极大提高了可控生成的效率，为实际应用降低了推理成本，兼顾性能和质量，具有良好的工程实用价值。同时代码开源，方便社区采纳。

Abstract: Visual generation includes both image and video generation, training
probabilistic models to create coherent, diverse, and semantically faithful
content from scratch. While early research focused on unconditional sampling,
practitioners now demand controllable generation that allows precise
specification of layout, pose, motion, or style. While ControlNet grants
precise spatial-temporal control, its auxiliary branch markedly increases
latency and introduces redundant computation in both uncontrolled regions and
denoising steps, especially for video. To address this problem, we introduce
EVCtrl, a lightweight, plug-and-play control adapter that slashes overhead
without retraining the model. Specifically, we propose a spatio-temporal dual
caching strategy for sparse control information. For spatial redundancy, we
first profile how each layer of DiT-ControlNet responds to fine-grained
control, then partition the network into global and local functional zones. A
locality-aware cache focuses computation on the local zones that truly need the
control signal, skipping the bulk of redundant computation in global regions.
For temporal redundancy, we selectively omit unnecessary denoising steps to
improve efficiency. Extensive experiments on CogVideo-Controlnet,
Wan2.1-Controlnet, and Flux demonstrate that our method is effective in image
and video control generation without the need for training. For example, it
achieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and
Wan2.1-Controlnet, respectively, with almost no degradation in generation
quality.Codes are available in the supplementary materials.

</details>


### [21] [Not There Yet: Evaluating Vision Language Models in Simulating the Visual Perception of People with Low Vision](https://arxiv.org/abs/2508.10972)
*Rosiana Natalie,Wenqian Xu,Ruei-Che Chang,Rada Mihalcea,Anhong Guo*

Main category: cs.CV

TL;DR: 本论文探讨了视觉语言模型（VLMs）在模拟低视力人群图像感知方面的能力，通过实际数据对比分析模型表现。


<details>
  <summary>Details</summary>
Motivation: 虽然VLMs在推理和模拟一般人类行为方面有进展，但尚未有研究关注它们在无障碍领域、特别是低视力人群视觉感知模拟中的效果。该研究旨在填补这一空白。

Method: 作者通过问卷调研40名低视力参与者，收集其视力信息及其对多张图像的开放性和选择性问题的感知回答，构成基准数据集。然后，利用这些数据构造不同提示词来指导VLMs（如GPT-4o）生成对应于每位参与者的模拟体，并分别对不同组合的提示词进行模拟测试，分析VLM的表现与真实答案的一致性。

Result: 结果显示：仅输入视力信息、或仅输入图像示例回答，模型与真实人类的回答一致性较低（0.59）。当视力信息和图像示例同时输入时，一致性显著上升（0.70，p < 0.0001）。将开放性与选择性问题的答案结合作为唯一示例时可带来显著提升，更多示例则收益有限。

Conclusion: VLMs在模拟低视力人群图像感知时，目前所需信息不能过于简略，必须同时结合视觉背景和具体示例。该研究指出了VLMs在无障碍领域的应用潜力及其当前局限。

Abstract: Advances in vision language models (VLMs) have enabled the simulation of
general human behavior through their reasoning and problem solving
capabilities. However, prior research has not investigated such simulation
capabilities in the accessibility domain. In this paper, we evaluate the extent
to which VLMs can simulate the vision perception of low vision individuals when
interpreting images. We first compile a benchmark dataset through a survey
study with 40 low vision participants, collecting their brief and detailed
vision information and both open-ended and multiple-choice image perception and
recognition responses to up to 25 images. Using these responses, we construct
prompts for VLMs (GPT-4o) to create simulated agents of each participant,
varying the included information on vision information and example image
responses. We evaluate the agreement between VLM-generated responses and
participants' original answers. Our results indicate that VLMs tend to infer
beyond the specified vision ability when given minimal prompts, resulting in
low agreement (0.59). The agreement between the agent' and participants'
responses remains low when only either the vision information (0.59) or example
image responses (0.59) are provided, whereas a combination of both
significantly increase the agreement (0.70, p < 0.0001). Notably, a single
example combining both open-ended and multiple-choice responses, offers
significant performance improvements over either alone (p < 0.0001), while
additional examples provided minimal benefits (p > 0.05).

</details>


### [22] [Are Large Pre-trained Vision Language Models Effective Construction Safety Inspectors?](https://arxiv.org/abs/2508.11011)
*Xuezheng Chen,Zhengbo Zou*

Main category: cs.CV

TL;DR: 本文提出了一个包含1万张施工现场图片和丰富标注的大型数据集（ConstructionSite 10k），用于视觉语言模型在施工安全检查方面的训练与评估，填补了该领域公开数据集的空白。


<details>
  <summary>Details</summary>
Motivation: 目前利用视觉语言模型（VLM）进行施工安全违规识别的研究受限于数据缺乏，现有数据集规模小且仅支持有限的任务，制约了一般化和实际应用。

Method: 作者构建了ConstructionSite 10k数据集，包含10,000张施工现场图片，每张图片配有三类任务的标注：图像描述、安全规则违规视觉问答（VQA）和施工要素视觉定位。并利用该数据集系统评价了主流预训练VLMs在零样本和少样本场景下的表现。

Result: 实验表明，主流VLMs在零样本和少样本条件下展示了一定的泛化能力，但距离实际施工场景的应用还需进一步训练和适配。

Conclusion: ConstructionSite 10k为施工安全检查领域中的VLM训练和评测提供了强有力的基准和平台，有助于推动相关研究和实践进步。

Abstract: Construction safety inspections typically involve a human inspector
identifying safety concerns on-site. With the rise of powerful Vision Language
Models (VLMs), researchers are exploring their use for tasks such as detecting
safety rule violations from on-site images. However, there is a lack of open
datasets to comprehensively evaluate and further fine-tune VLMs in construction
safety inspection. Current applications of VLMs use small, supervised datasets,
limiting their applicability in tasks they are not directly trained for. In
this paper, we propose the ConstructionSite 10k, featuring 10,000 construction
site images with annotations for three inter-connected tasks, including image
captioning, safety rule violation visual question answering (VQA), and
construction element visual grounding. Our subsequent evaluation of current
state-of-the-art large pre-trained VLMs shows notable generalization abilities
in zero-shot and few-shot settings, while additional training is needed to make
them applicable to actual construction sites. This dataset allows researchers
to train and evaluate their own VLMs with new architectures and techniques,
providing a valuable benchmark for construction safety inspection.

</details>


### [23] [Can Multi-modal (reasoning) LLMs detect document manipulation?](https://arxiv.org/abs/2508.11021)
*Zisheng Liang,Kidus Zewde,Rudra Pratap Singh,Disha Patil,Zexi Chen,Jiayu Xue,Yao Yao,Yifei Chen,Qinzhe Liu,Simiao Ren*

Main category: cs.CV

TL;DR: 本研究对多模态大型语言模型（LLM）在文件欺诈检测中的效果进行了基准测试，评估了最新模型在辨识欺诈文件上的能力，结果显示部分LLM能优于传统方法，但模型大小和推理能力与检测准确率相关性有限。


<details>
  <summary>Details</summary>
Motivation: 文件欺诈对依赖安全文件的行业构成重大威胁，现有检测技术存在局限，需要探索更有效的欺诈识别方法，特别是利用当前先进的多模态大模型能力。

Method: 作者选取了多种先进多模态LLM（如OpenAI系列、Gemini Flash、Llama 3/4、Claude等），在真实交易文件标准数据集上，结合提示优化和推理过程分析，评测各模型对文件篡改、格式错位、金额不一致等欺诈特征的识别能力，并与传统方法作对比。

Result: 领先的多模态LLM在零样本（zero-shot）泛化能力上优于传统方法，特别是在非分布外测试集上表现突出，但部分视觉LLM性能不稳定或较差。同时，模型规模和高级推理与检测准确率关系有限。

Conclusion: 多模态LLM在文件欺诈检测上具备提升空间，但取得高性能需针对任务微调，且模型能力尚未体现为绝对优势。研究为未来可解释、可扩展的欺诈防控系统发展打下基础。

Abstract: Document fraud poses a significant threat to industries reliant on secure and
verifiable documentation, necessitating robust detection mechanisms. This study
investigates the efficacy of state-of-the-art multi-modal large language models
(LLMs)-including OpenAI O1, OpenAI 4o, Gemini Flash (thinking), Deepseek Janus,
Grok, Llama 3.2 and 4, Qwen 2 and 2.5 VL, Mistral Pixtral, and Claude 3.5 and
3.7 Sonnet-in detecting fraudulent documents. We benchmark these models against
each other and prior work on document fraud detection techniques using a
standard dataset with real transactional documents. Through prompt optimization
and detailed analysis of the models' reasoning processes, we evaluate their
ability to identify subtle indicators of fraud, such as tampered text,
misaligned formatting, and inconsistent transactional sums. Our results reveal
that top-performing multi-modal LLMs demonstrate superior zero-shot
generalization, outperforming conventional methods on out-of-distribution
datasets, while several vision LLMs exhibit inconsistent or subpar performance.
Notably, model size and advanced reasoning capabilities show limited
correlation with detection accuracy, suggesting task-specific fine-tuning is
critical. This study underscores the potential of multi-modal LLMs in enhancing
document fraud detection systems and provides a foundation for future research
into interpretable and scalable fraud mitigation strategies.

</details>


### [24] [MedSAMix: A Training-Free Model Merging Approach for Medical Image Segmentation](https://arxiv.org/abs/2508.11032)
*Yanwu Yang,Guinan Su,Jiesi Hu,Francesco Sammarco,Jonas Geiping,Thomas Wolfers*

Main category: cs.CV

TL;DR: 该论文提出MedSAMix，一种无需重新训练即可合并通用与专用分割模型的方法，有效提升医学图像分割的泛化与任务适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有通用医学图像分割模型（如SAM及其变体MedSAM）虽然前景广阔，但数据稀缺、异质性强及分布偏移等问题限制了其泛化能力。为了克服这些限制，提高模型在不同场景下的表现，需要一种能够整合通用模型与专用模型优点的方法。

Method: 作者提出MedSAMix，这是一种免训练模型融合方法，利用零阶优化自动搜索并实现各层融合方案，避免了人工手动配置。针对临床需求，设计了单任务优化和多目标优化两种模式，以分别满足领域专用性和任务泛化能力需求。

Result: 在25个医学分割任务上评估，MedSAMix同时提升了领域专用和泛化场景下的性能，专用任务提升6.67%，多任务评估提升4.37%。有效缓解了模型偏差，带来了更稳定的表现。

Conclusion: MedSAMix方法可以在不重新训练的情况下，将通用模型和专用模型优势结合，实现更高的医学图像分割精度和更强的泛化能力，有望在实际临床多样化需求中发挥重要作用。

Abstract: Universal medical image segmentation models have emerged as a promising
paradigm due to their strong generalizability across diverse tasks, showing
great potential for a wide range of clinical applications. This potential has
been partly driven by the success of general-purpose vision models such as the
Segment Anything Model (SAM), which has inspired the development of various
fine-tuned variants for medical segmentation tasks. However, fine-tuned
variants like MedSAM are trained on comparatively limited medical imaging data
that often suffers from heterogeneity, scarce annotations, and distributional
shifts. These challenges limit their ability to generalize across a wide range
of medical segmentation tasks. In this regard, we propose MedSAMix, a
training-free model merging method that integrates the strengths of both
generalist models (e.g., SAM) and specialist models (e.g., MedSAM) for medical
image segmentation. In contrast to traditional model merging approaches that
rely on manual configuration and often result in suboptimal outcomes, we
propose a zero-order optimization method to automatically discover optimal
layer-wise merging solutions. Furthermore, for clinical applications, we
develop two regimes to meet the demand of domain-specificity and
generalizability in different scenarios by single-task optimization and
multi-objective optimization respectively. Extensive evaluations on 25 medical
segmentation tasks demonstrate that MedSAMix effectively mitigates model bias
and consistently improves performance in both domain-specific accuracy and
generalization, achieving improvements of 6.67% on specialized tasks and 4.37%
on multi-task evaluations.

</details>


### [25] [Advancing 3D Scene Understanding with MV-ScanQA Multi-View Reasoning Evaluation and TripAlign Pre-training Dataset](https://arxiv.org/abs/2508.11058)
*Wentao Mo,Qingchao Chen,Yuxin Peng,Siyuan Huang,Yang Liu*

Main category: cs.CV

TL;DR: 本文针对3D视觉-语言学习中的局限性，提出了MV-ScanQA新数据集和TripAlign预训练集，并开发了LEGO基线方法，实现多视角复杂推理，显著提升3D问答和描述任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉-语言数据集大多数仅基于单视角、单物体，缺乏跨多物体、多视角的复杂推理需求，限制了模型对复杂3D场景的深入理解。本研究针对这一瓶颈，致力于推动3D视觉-语言模型对于远距离、多物体、多视角场景的综合推理和理解。

Method: 1）提出MV-ScanQA数据集，68%的问题需多视角整合信息，有效检验模型的多视角组合推理能力；2）创建TripAlign大规模2D-3D-语言预训练集，通过对一组语境相关的3D物体与文本的对齐，实现更丰富的多模态联合信号；3）设计LEGO基线方法，将预训练的大型2D视觉-语言模型知识迁移到3D领域，通过TripAlign预训练提升多视角推理能力。

Result: LEGO方法在TripAlign预训练后，在新提出的MV-ScanQA数据集以及现有3D密集描述和问答基准上均取得了最新的最佳性能。

Conclusion: 通过新型数据集和预训练语料、多对象多视角对齐及基线方法，本文大幅提升了3D视觉-语言模型在多视角复杂推理任务中的表现，为3D场景的深层次理解提供了有力工具。

Abstract: The advancement of 3D vision-language (3D VL) learning is hindered by several
limitations in existing 3D VL datasets: they rarely necessitate reasoning
beyond a close range of objects in single viewpoint, and annotations often link
instructions to single objects, missing richer contextual alignments between
multiple objects. This significantly curtails the development of models capable
of deep, multi-view 3D scene understanding over distant objects. To address
these challenges, we introduce MV-ScanQA, a novel 3D question answering dataset
where 68% of questions explicitly require integrating information from multiple
views (compared to less than 7% in existing datasets), thereby rigorously
testing multi-view compositional reasoning. To facilitate the training of
models for such demanding scenarios, we present TripAlign dataset, a
large-scale and low-cost 2D-3D-language pre-training corpus containing 1M <2D
view, set of 3D objects, text> triplets that explicitly aligns groups of
contextually related objects with text, providing richer, view-grounded
multi-object multimodal alignment signals than previous single-object
annotations. We further develop LEGO, a baseline method for the multi-view
reasoning challenge in MV-ScanQA, transferring knowledge from pre-trained 2D
LVLMs to 3D domain with TripAlign. Empirically, LEGO pre-trained on TripAlign
achieves state-of-the-art performance not only on the proposed MV-ScanQA, but
also on existing benchmarks for 3D dense captioning and question answering.
Datasets and code are available at
https://matthewdm0816.github.io/tripalign-mvscanqa.

</details>


### [26] [Data-Driven Abdominal Phenotypes of Type 2 Diabetes in Lean, Overweight, and Obese Cohorts](https://arxiv.org/abs/2508.11063)
*Lucas W. Remedios,Chloe Choe,Trent M. Schwartz,Dingjie Su,Gaurav Rudravaram,Chenyu Gao,Aravind R. Krishnan,Adam M. Saunders,Michael E. Kim,Shunxing Bao,Alvin C. Powers,Bennett A. Landman,John Virostko*

Main category: cs.CV

TL;DR: 本研究通过AI对腹部CT影像进行详尽体成分分析，发现2型糖尿病的腹部表型在不同BMI人群中具有一致特征。


<details>
  <summary>Details</summary>
Motivation: 尽管高BMI已知是2型糖尿病的风险因素，但也有部分瘦人患病以及肥胖者未患病，提示仅用BMI不足以揭示全部风险。详细的腹部体成分也许能够提供更深入的机制解释。AI技术的发展使我们能够大规模精准提取腹部结构的3D特征，为发现与2型糖尿病风险相关的体成分模式提供了新可能。

Method: 研究基于1,728例临床CT影像，分别在总队列及瘦体型、超重、肥胖三个亚组中，采用了全自动腹部分割与特征提取、随机森林分类、SHAP特征归因与聚类分析等AI方法，对2型糖尿病相关的腹部表型进行挖掘和分类验证。

Result: 随机森林模型在四组中的平均AUC为0.72-0.74。各组都发现了共同的2型糖尿病腹部特征：脂肪骨骼肌、年龄增大、脏器周围及皮下脂肪增多、胰腺偏小或脂肪沉积增多。单变量Logistic回归验证了最显著特征在各BMI亚组中的一致方向。

Conclusion: 2型糖尿病的关键腹部成分特征跨BMI类别具有一致性，提示腹部脂肪及相关结构可能是体重之外的重要致病驱动因素。

Abstract: Purpose: Although elevated BMI is a well-known risk factor for type 2
diabetes, the disease's presence in some lean adults and absence in others with
obesity suggests that detailed body composition may uncover abdominal
phenotypes of type 2 diabetes. With AI, we can now extract detailed
measurements of size, shape, and fat content from abdominal structures in 3D
clinical imaging at scale. This creates an opportunity to empirically define
body composition signatures linked to type 2 diabetes risk and protection using
large-scale clinical data. Approach: To uncover BMI-specific diabetic abdominal
patterns from clinical CT, we applied our design four times: once on the full
cohort (n = 1,728) and once on lean (n = 497), overweight (n = 611), and obese
(n = 620) subgroups separately. Briefly, our experimental design transforms
abdominal scans into collections of explainable measurements through
segmentation, classifies type 2 diabetes through a cross-validated random
forest, measures how features contribute to model-estimated risk or protection
through SHAP analysis, groups scans by shared model decision patterns
(clustering from SHAP) and links back to anatomical differences
(classification). Results: The random-forests achieved mean AUCs of 0.72-0.74.
There were shared type 2 diabetes signatures in each group; fatty skeletal
muscle, older age, greater visceral and subcutaneous fat, and a smaller or
fat-laden pancreas. Univariate logistic regression confirmed the direction of
14-18 of the top 20 predictors within each subgroup (p < 0.05). Conclusions:
Our findings suggest that abdominal drivers of type 2 diabetes may be
consistent across weight classes.

</details>


### [27] [HierOctFusion: Multi-scale Octree-based 3D Shape Generation via Part-Whole-Hierarchy Message Passing](https://arxiv.org/abs/2508.11106)
*Xinjie Gao,Bi'an Du,Wei Hu*

Main category: cs.CV

TL;DR: 提出了HierOctFusion，一种关注部分结构且多尺度的八叉树扩散模型，结合分层特征交互与跨注意力调控，提升3D对象生成的细致度与稀疏性，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于八叉树的扩散模型虽然在效率与质量之间取得平衡，但往往忽视3D对象的语义部分层次，限制了泛化能力，且整体高分辨建模计算开销大，而真实3D对象天然稀疏分层。针对这些问题，作者提出新的方法提升分层表达和生成效率。

Method: 提出HierOctFusion，一种融入部分感知的多尺度八叉树扩散模型，增强分层特征交互，并设计跨注意力控制机制，将部分级信息注入生成流程，实现分层语义特征自底向上传递。此外，构建带有部分类别注释的3D数据集辅助训练和评估。

Result: 实验显示，所提方法在形状质量和效率上均领先于以往的3D生成方法。

Conclusion: 通过引入部分感知与分层信息交互，HierOctFusion在3D内容生成任务中显著提升了模型的细粒度表达能力和稀疏高效性，推动了三维生成领域发展。

Abstract: 3D content generation remains a fundamental yet challenging task due to the
inherent structural complexity of 3D data. While recent octree-based diffusion
models offer a promising balance between efficiency and quality through
hierarchical generation, they often overlook two key insights: 1) existing
methods typically model 3D objects as holistic entities, ignoring their
semantic part hierarchies and limiting generalization; and 2) holistic
high-resolution modeling is computationally expensive, whereas real-world
objects are inherently sparse and hierarchical, making them well-suited for
layered generation. Motivated by these observations, we propose HierOctFusion,
a part-aware multi-scale octree diffusion model that enhances hierarchical
feature interaction for generating fine-grained and sparse object structures.
Furthermore, we introduce a cross-attention conditioning mechanism that injects
part-level information into the generation process, enabling semantic features
to propagate effectively across hierarchical levels from parts to the whole.
Additionally, we construct a 3D dataset with part category annotations using a
pre-trained segmentation model to facilitate training and evaluation.
Experiments demonstrate that HierOctFusion achieves superior shape quality and
efficiency compared to prior methods.

</details>


### [28] [UWB-PostureGuard: A Privacy-Preserving RF Sensing System for Continuous Ergonomic Sitting Posture Monitoring](https://arxiv.org/abs/2508.11115)
*Haotang Li,Zhenyu Qi,Sen He,Kebin Peng,Sheng Tan,Yili Ren,Tomas Cerny,Jiyue Zhao,Zi Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于超宽带（UWB）技术的隐私保护坐姿监测系统UWB-PostureGuard，可无接触连续监测用户坐姿，有效改善长期电脑使用引发的健康问题。


<details>
  <summary>Details</summary>
Motivation: 传统坐姿监测方法存在摄像头隐私问题或穿戴设备不舒适等缺点，亟需一种兼顾隐私和舒适性的解决方案，促进健康管理。

Method: 利用商用UWB设备，进行特征工程提取多种坐姿信息，并提出PoseGBDT算法，能捕捉坐姿随时间变化的依赖关系，突破以往逐帧分类的局限。

Result: 在10名受试者、19种坐姿下实测，系统识别准确率达99.11%，且对衣着厚度、随身设备、家具变换等环境因素具有良好鲁棒性。

Conclusion: UWB-PostureGuard为主动健康管理提供了一种低成本、可扩展的移动隐私保护方案，有助于提升生活质量。

Abstract: Improper sitting posture during prolonged computer use has become a
significant public health concern. Traditional posture monitoring solutions
face substantial barriers, including privacy concerns with camera-based systems
and user discomfort with wearable sensors. This paper presents
UWB-PostureGuard, a privacy-preserving ultra-wideband (UWB) sensing system that
advances mobile technologies for preventive health management through
continuous, contactless monitoring of ergonomic sitting posture. Our system
leverages commercial UWB devices, utilizing comprehensive feature engineering
to extract multiple ergonomic sitting posture features. We develop PoseGBDT to
effectively capture temporal dependencies in posture patterns, addressing
limitations of traditional frame-wise classification approaches. Extensive
real-world evaluation across 10 participants and 19 distinct postures
demonstrates exceptional performance, achieving 99.11% accuracy while
maintaining robustness against environmental variables such as clothing
thickness, additional devices, and furniture configurations. Our system
provides a scalable, privacy-preserving mobile health solution on existing
platforms for proactive ergonomic management, improving quality of life at low
costs.

</details>


### [29] [Residual-based Efficient Bidirectional Diffusion Model for Image Dehazing and Haze Generation](https://arxiv.org/abs/2508.11134)
*Bing Liu,Le Wang,Hao Liu,Mingming Liu*

Main category: cs.CV

TL;DR: 本文提出了一种残差驱动的高效双向扩散模型（RBDM），实现了去雾和加雾的双向图像转换，在小数据集上也能高效运行，实验效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度去雾方法仅专注于从有雾图像去除雾气，无法实现有雾和无雾图像间的双向转换。实际应用如数据增强或合成数据时，双向转换十分有价值，因此亟需可支持去雾和加雾的统一建模。

Method: 1. 提出双马尔可夫链，能高效实现残差的双向平滑转换；2. 对有雾和无雾图像进行单独时刻扰动，通过预测扰动数据中的噪声以联合学习条件分布；3. 引入统一得分函数，在图像patch而非整图上学习，提升小样本和低算力场景下的表现。

Result: RBDM能在仅15步采样下，实现无尺度偏好的有雾/无雾图像双向转换。大量实验表明，该方法在合成和真实数据集上性能优于或相当于当前主流方法。

Conclusion: RBDM为图像去雾提供了首个高效的双向扩散解决方案，兼顾加雾和去雾，在性能和效率上优于现有工作，并对小样本场景具有实用价值。

Abstract: Current deep dehazing methods only focus on removing haze from hazy images,
lacking the capability to translate between hazy and haze-free images. To
address this issue, we propose a residual-based efficient bidirectional
diffusion model (RBDM) that can model the conditional distributions for both
dehazing and haze generation. Firstly, we devise dual Markov chains that can
effectively shift the residuals and facilitate bidirectional smooth transitions
between them. Secondly, the RBDM perturbs the hazy and haze-free images at
individual timesteps and predicts the noise in the perturbed data to
simultaneously learn the conditional distributions. Finally, to enhance
performance on relatively small datasets and reduce computational costs, our
method introduces a unified score function learned on image patches instead of
entire images. Our RBDM successfully implements size-agnostic bidirectional
transitions between haze-free and hazy images with only 15 sampling steps.
Extensive experiments demonstrate that the proposed method achieves superior or
at least comparable performance to state-of-the-art methods on both synthetic
and real-world datasets.

</details>


### [30] [A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text and Image internal Correlations](https://arxiv.org/abs/2508.11141)
*Bin Ma,Yifei Zhang,Yongjin Xian,Qi Li,Linna Zhou,Gongxun Miao*

Main category: cs.CV

TL;DR: 本文提出了一种基于对比学习的多尺度图像与文本相关性探索算法（MICC），显著提升了跨模态谣言检测的性能。


<details>
  <summary>Details</summary>
Motivation: 现有谣言检测方法在处理图像内容及其与文本之间多尺度相关性时存在不足，导致相关信息丢失，影响识别效果。

Method: 方法包括设计SCLIP编码器，对文本和多尺度图像块进行对比式预训练以生成统一语义嵌入，并通过点积相似度评估相关性；提出跨模态多尺度对齐模块，利用互信息最大化和信息瓶颈原理，加上Top-K选择策略，从跨模态相关矩阵中筛选与文本意义最相关的图像区域；再通过尺度感知融合网络，根据语义重要性和相关性自适应权重分配，融合高度相关的多尺度图像特征与全局文本特征。

Result: 在两个真实世界数据集上进行了大量实证评估，实验表明该方法在谣言检测任务上明显优于当前的先进技术。

Conclusion: 所提出的方法有效地利用了多尺度图像和文本的相关性，极大提升了跨模态谣言检测准确性，具有实际应用潜力。

Abstract: Existing rumor detection methods often neglect the content within images as
well as the inherent relationships between contexts and images across different
visual scales, thereby resulting in the loss of critical information pertinent
to rumor identification. To address these issues, this paper presents a novel
cross-modal rumor detection scheme based on contrastive learning, namely the
Multi-scale Image and Context Correlation exploration algorithm (MICC).
Specifically, we design an SCLIP encoder to generate unified semantic
embeddings for text and multi-scale image patches through contrastive
pretraining, enabling their relevance to be measured via dot-product
similarity. Building upon this, a Cross-Modal Multi-Scale Alignment module is
introduced to identify image regions most relevant to the textual semantics,
guided by mutual information maximization and the information bottleneck
principle, through a Top-K selection strategy based on a cross-modal relevance
matrix constructed between the text and multi-scale image patches. Moreover, a
scale-aware fusion network is designed to integrate the highly correlated
multi-scale image features with global text features by assigning adaptive
weights to image regions based on their semantic importance and cross-modal
relevance. The proposed methodology has been extensively evaluated on two
real-world datasets. The experimental results demonstrate that it achieves a
substantial performance improvement over existing state-of-the-art approaches
in rumor detection, highlighting its effectiveness and potential for practical
applications.

</details>


### [31] [LEARN: A Story-Driven Layout-to-Image Generation Framework for STEM Instruction](https://arxiv.org/abs/2508.11153)
*Maoquan Zhang,Bisser Raytchev,Xiujuan Sun*

Main category: cs.CV

TL;DR: LEARN是一个专为STEM教育设计的、注重版面信息的扩散生成框架，能够生成与教学需求高度契合的插图，提升学习效果。


<details>
  <summary>Details</summary>
Motivation: 当前STEM教育中，图像生成工具很难生成既有结构化布局又能表达抽象/连续科学概念的插图，同时不易与认知负担理论及布鲁姆分类法结合，LEARN提出为解决这些痛点。

Method: LEARN利用精心标注的BookCover数据集，结合版面条件生成（layout-conditioned generation）、视觉-语义对比训练（contrastive visual-semantic training）和提示调控（prompt modulation）。通过这些方法保证插图在结构和语义上的对齐，并支持多步骤推理与故事化表达。

Result: LEARN生成插图序列在布鲁姆分类法的中高阶推理任务中表现优越，有效减轻学习者认知负担，并克服短视频碎片化带来的注意力分散，促进深度学习。展示了与多模态系统及知识图谱结合的潜力。

Conclusion: LEARN首次将版面故事讲述、语义结构学习及认知支架融合到插图生成中，丰富了AI教育内容生成的新方向，对后续科研及实际应用具有推动作用。代码与数据集将公开。

Abstract: LEARN is a layout-aware diffusion framework designed to generate
pedagogically aligned illustrations for STEM education. It leverages a curated
BookCover dataset that provides narrative layouts and structured visual cues,
enabling the model to depict abstract and sequential scientific concepts with
strong semantic alignment. Through layout-conditioned generation, contrastive
visual-semantic training, and prompt modulation, LEARN produces coherent visual
sequences that support mid-to-high-level reasoning in line with Bloom's
taxonomy while reducing extraneous cognitive load as emphasized by Cognitive
Load Theory. By fostering spatially organized and story-driven narratives, the
framework counters fragmented attention often induced by short-form media and
promotes sustained conceptual focus. Beyond static diagrams, LEARN demonstrates
potential for integration with multimodal systems and curriculum-linked
knowledge graphs to create adaptive, exploratory educational content. As the
first generative approach to unify layout-based storytelling, semantic
structure learning, and cognitive scaffolding, LEARN represents a novel
direction for generative AI in education. The code and dataset will be released
to facilitate future research and practical deployment.

</details>


### [32] [Semi-supervised Image Dehazing via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models](https://arxiv.org/abs/2508.11165)
*Bing Liu,Le Wang,Mingming Liu,Hao Liu,Rui Yao,Yong Zhou,Peng Liu,Tongqiang Xia*

Main category: cs.CV

TL;DR: 提出了一种高效的新型半监督图像去雾方法（EM-B3DM），通过分阶段学习和扩散模型，在缺乏真实成对样本时实现了优异的去雾效果。


<details>
  <summary>Details</summary>
Motivation: 现有去雾方法在现实世界厚雾场景下效果有限，主要因为缺乏成对真实数据和稳健的先验知识。为减少高成本的数据采集，亟需通用且高效的去雾方案。

Method: 方法分两阶段：第一阶段利用期望最大化（EM）算法将雾图和清晰图的联合分布解耦为两个条件分布，并用统一的布朗桥扩散模型对其建模，捕捉结构与内容相关性。第二阶段用预训练模型在大规模非成对数据上进一步提升去雾性能；并引入细节增强残差差分卷积块（RDC）以增强模型对梯度信息的表达能力。

Result: 在多组大量合成及真实世界数据集上的实验表明，EM-B3DM在图像去雾任务上取得了优于或至少可比最新技术水平的性能。

Conclusion: 该方法不仅有效解决了真实成对样本不足的问题，而且通过双向扩散模型和结构细节增强模块，大幅提升了去雾质量，展示了在实际应用中的广泛潜力。

Abstract: Existing dehazing methods deal with real-world haze images with difficulty,
especially scenes with thick haze. One of the main reasons is the lack of
real-world paired data and robust priors. To avoid the costly collection of
paired hazy and clear images, we propose an efficient semi-supervised image
dehazing method via Expectation-Maximization and Bidirectional Brownian Bridge
Diffusion Models (EM-B3DM) with a two-stage learning scheme. In the first
stage, we employ the EM algorithm to decouple the joint distribution of paired
hazy and clear images into two conditional distributions, which are then
modeled using a unified Brownian Bridge diffusion model to directly capture the
structural and content-related correlations between hazy and clear images. In
the second stage, we leverage the pre-trained model and large-scale unpaired
hazy and clear images to further improve the performance of image dehazing.
Additionally, we introduce a detail-enhanced Residual Difference Convolution
block (RDC) to capture gradient-level information, significantly enhancing the
model's representation capability. Extensive experiments demonstrate that our
EM-B3DM achieves superior or at least comparable performance to
state-of-the-art methods on both synthetic and real-world datasets.

</details>


### [33] [VFM-Guided Semi-Supervised Detection Transformer for Source-Free Object Detection in Remote Sensing Images](https://arxiv.org/abs/2508.11167)
*Jianhong Han,Yupei Wang,Liang Chen*

Main category: cs.CV

TL;DR: 本论文提出了一种无需源域数据的遥感图像目标检测方法VG-DETR，通过引入基础视觉模型引导伪标签生成和特征对齐，有效解决了伪标签噪声导致的训练崩溃问题，在遥感领域实现了更强的跨域检测性能。


<details>
  <summary>Details</summary>
Motivation: 现实中遥感任务往往受限于隐私和传输，无法获取源域数据，传统无监督域适应方法难以落地。源数自由目标检测（SFOD）新兴，但伪标签噪声高，训练易崩溃，需创新方法增强伪标签质量及检测鲁棒性。

Method: 提出Vision foundation-Guided Detection Transformer（VG-DETR）框架，将基础视觉模型（VFM）嵌入半监督SFOD训练流程。采用VFM引导下的伪标签挖掘策略，以VFM语义先验增强伪标签的可信度，并通过拾取低置信度但可能正确的预测提升伪标签集。进一步，提出基于VFM的实例与图像级特征对齐，利用对比学习与特征相似度匹配，强健特征表达以缩小域间差异。

Result: VG-DETR在无源域数据的遥感目标检测任务上，通过丰富伪标签有效性及特征对齐，显著提升了检测精度。大量实验验证了提出方法在多类遥感跨域检测场景中优于现有方法。

Conclusion: 利用VFM作为知识注入手段，无需源域数据下可大幅度缓解伪标签噪声，提高遥感目标检测自适应效果。该方法为源数自由跨域检测提供了更实用高效的技术路径。

Abstract: Unsupervised domain adaptation methods have been widely explored to bridge
domain gaps. However, in real-world remote-sensing scenarios, privacy and
transmission constraints often preclude access to source domain data, which
limits their practical applicability. Recently, Source-Free Object Detection
(SFOD) has emerged as a promising alternative, aiming at cross-domain
adaptation without relying on source data, primarily through a self-training
paradigm. Despite its potential, SFOD frequently suffers from training collapse
caused by noisy pseudo-labels, especially in remote sensing imagery with dense
objects and complex backgrounds. Considering that limited target domain
annotations are often feasible in practice, we propose a Vision
foundation-Guided DEtection TRansformer (VG-DETR), built upon a semi-supervised
framework for SFOD in remote sensing images. VG-DETR integrates a Vision
Foundation Model (VFM) into the training pipeline in a "free lunch" manner,
leveraging a small amount of labeled target data to mitigate pseudo-label noise
while improving the detector's feature-extraction capability. Specifically, we
introduce a VFM-guided pseudo-label mining strategy that leverages the VFM's
semantic priors to further assess the reliability of the generated
pseudo-labels. By recovering potentially correct predictions from
low-confidence outputs, our strategy improves pseudo-label quality and
quantity. In addition, a dual-level VFM-guided alignment method is proposed,
which aligns detector features with VFM embeddings at both the instance and
image levels. Through contrastive learning among fine-grained prototypes and
similarity matching between feature maps, this dual-level alignment further
enhances the robustness of feature representations against domain gaps.
Extensive experiments demonstrate that VG-DETR achieves superior performance in
source-free remote sensing detection tasks.

</details>


### [34] [Better Supervised Fine-tuning for VQA: Integer-Only Loss](https://arxiv.org/abs/2508.11170)
*Baihong Qian,Haotian Fan,Wenjie Liao,Yunqiu Wang,Tao Li,Junhui Cui*

Main category: cs.CV

TL;DR: 本文提出了一种针对视觉语言模型（VLM）视频质量评价任务的精调方法IOVQA，通过仅保留整数标签及特殊损失计算机制，显著提升了模型的准确性和一致性，并在权威评测中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: VLM在需根据具体标准评估视觉内容的场景（如视频主题一致性、画质评分等）越来越关键，但当前方法存在结果不精确和损失计算效率低，难以聚焦关键评价指标的问题。

Method: 提出IOVQA方法，创新点包括：1）在数据集构建时，仅用[10,50]范围内的整数标签，保持数值稳定；2）将原标签小数转换为整数；3）引入target-mask策略，损失计算时仅解锁标签的前两位整数，强化模型对数值关键部分的学习。通过这些方式对Qwen2.5-VL模型进行精调。

Result: 实验结果表明，IOVQA方法能显著提升VQA任务模型准确率和一致性，在VQualA 2025 GenAI-Bench AIGC视频质量评估挑战赛Track I中获得第三名。

Conclusion: 精调时仅用整数标签并结合定向损失计算机制，是提升VLM在定量评价场景表现的有效方法。

Abstract: With the rapid advancement of vision language models(VLM), their ability to
assess visual content based on specific criteria and dimensions has become
increasingly critical for applications such as video-theme consistency
assessment and visual quality scoring. However, existing methods often suffer
from imprecise results and inefficient loss calculation, which limit the focus
of the model on key evaluation indicators. To address this, we propose
IOVQA(Integer-only VQA), a novel fine-tuning approach tailored for VLMs to
enhance their performance in video quality assessment tasks. The key innovation
of IOVQA lies in its label construction and its targeted loss calculation
mechanism. Specifically, during dataset curation, we constrain the model's
output to integers within the range of [10,50], ensuring numerical stability,
and convert decimal Overall_MOS to integer before using them as labels. We also
introduce a target-mask strategy: when computing the loss, only the first
two-digit-integer of the label is unmasked, forcing the model to learn the
critical components of the numerical evaluation. After fine-tuning the
Qwen2.5-VL model using the constructed dataset, experimental results
demonstrate that the proposed method significantly improves the model's
accuracy and consistency in the VQA task, ranking 3rd in VQualA 2025
GenAI-Bench AIGC Video Quality Assessment Challenge -- Track I. Our work
highlights the effectiveness of merely leaving integer labels during
fine-tuning, providing an effective idea for optimizing VLMs in quantitative
evaluation scenarios.

</details>


### [35] [Exploring the Tradeoff Between Diversity and Discrimination for Continuous Category Discovery](https://arxiv.org/abs/2508.11173)
*Ruobing Jiang,Yang Liu,Haobing Liu,Yanwei Yu,Chunyang Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新方法IDOD，有效地解决了持续类别发现（CCD）任务中新类别识别与遗忘问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: CCD任务需要自动在不断到来的无标签数据中发现新类别，挑战在于未知类别数与标签，并须防止遗忘。现有方法兼顾新类别发现与分类表现有限，且常需用到知识蒸馏与数据重放，占用存储空间大且易累积错误。

Method: 提出“独立性的多样性丰富模块”、“新颖联合发现模块”以及“基于正交的持续增长模块”。通过独立对比学习增强多样性，联合发现减少错误积累，正交原型支持增量分类并用少量样本代表防遗忘，无需大量重放数据。

Result: 在具有挑战性的细粒度数据集上，IDOD方法在新类别探索、分类和抗遗忘综合表现上优于最新方法。

Conclusion: IDOD方法兼顾新类别识别、抗遗忘及存储高效性，为持续类别发现领域提供了更有效的新路径，具有重要应用前景。

Abstract: Continuous category discovery (CCD) aims to automatically discover novel
categories in continuously arriving unlabeled data. This is a challenging
problem considering that there is no number of categories and labels in the
newly arrived data, while also needing to mitigate catastrophic forgetting.
Most CCD methods cannot handle the contradiction between novel class discovery
and classification well. They are also prone to accumulate errors in the
process of gradually discovering novel classes. Moreover, most of them use
knowledge distillation and data replay to prevent forgetting, occupying more
storage space. To address these limitations, we propose Independence-based
Diversity and Orthogonality-based Discrimination (IDOD). IDOD mainly includes
independent enrichment of diversity module, joint discovery of novelty module,
and continuous increment by orthogonality module. In independent enrichment,
the backbone is trained separately using contrastive loss to avoid it focusing
only on features for classification. Joint discovery transforms multi-stage
novel class discovery into single-stage, reducing error accumulation impact.
Continuous increment by orthogonality module generates mutually orthogonal
prototypes for classification and prevents forgetting with lower space overhead
via representative representation replay. Experimental results show that on
challenging fine-grained datasets, our method outperforms the state-of-the-art
methods.

</details>


### [36] [Fine-Grained VLM Fine-tuning via Latent Hierarchical Adapter Learning](https://arxiv.org/abs/2508.11176)
*Yumiao Zhao,Bo Jiang,Yuhe Ding,Xiao Wang,Jin Tang,Bin Luo*

Main category: cs.CV

TL;DR: 本文提出了Latent Hierarchical Adapter (LatHAdapter)，一种利用下游训练数据潜在语义层次进行视觉语言模型（VLMs）微调的新方法，显著提升了小样本分类任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有Adapter方法在对VLM进行小样本微调时，通常通过嵌入空间中类别与图像的显式空间接近性来进行对齐，这样难以建模类别与样本之间的一对多关系，也难以处理未知类别与图像的关联。因此需要一种更有效的对齐与泛化方法。

Method: 作者受超曲学习的启发，提出LatHAdapter。主要方法包括：1）引入可学习的“属性”prompt作为类别与图像对齐的桥梁；2）将类别、属性prompt和图像整体映射到超曲空间中，通过分层正则化学习它们之间的潜在语义层次结构，从而更好地建模类别、属性和样本之间的一对多关系。

Result: 在四个具有挑战性的小样本分类任务上，LatHAdapter都优于多种主流微调方法，特别是在已知类别适应和未知类别泛化方面表现突出。

Conclusion: LatHAdapter利用下游数据的潜在语义层次，为adapter学习提供了更丰富和细粒度的指导，显著提升了VLM在小样本学习场景下适应与泛化能力。

Abstract: Adapter-based approaches have garnered attention for fine-tuning pre-trained
Vision-Language Models (VLMs) on few-shot classification tasks. These methods
strive to develop a lightweight module that better aligns visual and (category)
textual representations, thereby enhancing performance on downstream few-shot
learning tasks. However, existing adapters generally learn/align (category)
textual-visual modalities via explicit spatial proximity in the underlying
embedding space, which i) fails to capture the inherent one-to-many
associations between categories and image samples and ii) struggles to
establish accurate associations between the unknown categories and images. To
address these issues, inspired by recent works on hyperbolic learning, we
develop a novel Latent Hierarchical Adapter (LatHAdapter) for fine-tuning VLMs
on downstream few-shot classification tasks. The core of LatHAdapter is to
exploit the latent semantic hierarchy of downstream training data and employ it
to provide richer, fine-grained guidance for the adapter learning process.
Specifically, LatHAdapter first introduces some learnable `attribute' prompts
as the bridge to align categories and images. Then, it projects the categories,
attribute prompts, and images within each batch in a hyperbolic space, and
employs hierarchical regularization to learn the latent semantic hierarchy of
them, thereby fully modeling the inherent one-to-many associations among
categories, learnable attributes, and image samples. Extensive experiments on
four challenging few-shot tasks show that the proposed LatHAdapter consistently
outperforms many other fine-tuning approaches, particularly in adapting known
classes and generalizing to unknown classes.

</details>


### [37] [Versatile Video Tokenization with Generative 2D Gaussian Splatting](https://arxiv.org/abs/2508.11183)
*Zhenghao Chen,Zicong Chen,Lei Liu,Yiming Wu,Dong Xu*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于高斯生成的二维Splatting方法（GVT），实现更高效、适应性更强的视频Token化，并在重构、动作识别、压缩等任务上优于或媲美现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频Token化方法大多采用固定网格和划块策略，空间上对不同信息量区域处理不均，时间上难以区分静态与动态内容，导致冗余高与适应性差。需要一种能自适应空间信息、显式建模时间动态的高效Token化方法。

Method: GVT方法利用生成式2D高斯Splatting（2DGS）进行视频Token化。首先通过Spatio-Temporal Gaussian Embedding（STGE）机制提取视频潜在特征，以2D高斯集合表达。空间上，按区块信息量分配渲染权重，提升适配性。时间上，采用Gaussian Set Partitioning（GSP）策略将高斯划分为静态与动态集合，分别表达跨时刻共享内容与时刻特有内容，实现紧凑表达。避免了对每个视频的单独优化。

Result: GVT在UCF101、Kinetics、DAVIS等数据集上进行视频重构、动作识别、压缩实验。结果表明GVT在视频重构质量上达到最新水平，动作识别优于主流MAGVIT-v2，压缩性能与现有方法相当。

Conclusion: GVT以高斯生成过程实现灵活、高效的视频Token化，兼顾空间信息适配与显式时序动态建模，提升了重构质量和下游任务性能，展现较强的通用性和应用前景。

Abstract: Video tokenization procedure is critical for a wide range of video processing
tasks. Most existing approaches directly transform video into fixed-grid and
patch-wise tokens, which exhibit limited versatility. Spatially, uniformly
allocating a fixed number of tokens often leads to over-encoding in
low-information regions. Temporally, reducing redundancy remains challenging
without explicitly distinguishing between static and dynamic content. In this
work, we propose the Gaussian Video Transformer (GVT), a versatile video
tokenizer built upon a generative 2D Gaussian Splatting (2DGS) strategy. We
first extract latent rigid features from a video clip and represent them with a
set of 2D Gaussians generated by our proposed Spatio-Temporal Gaussian
Embedding (STGE) mechanism in a feed-forward manner. Such generative 2D
Gaussians not only enhance spatial adaptability by assigning higher (resp.,
lower) rendering weights to regions with higher (resp., lower) information
content during rasterization, but also improve generalization by avoiding
per-video optimization.To enhance the temporal versatility, we introduce a
Gaussian Set Partitioning (GSP) strategy that separates the 2D Gaussians into
static and dynamic sets, which explicitly model static content shared across
different time-steps and dynamic content specific to each time-step, enabling a
compact representation.We primarily evaluate GVT on the video reconstruction,
while also assessing its performance on action recognition and compression
using the UCF101, Kinetics, and DAVIS datasets. Extensive experiments
demonstrate that GVT achieves a state-of-the-art video reconstruction quality,
outperforms the baseline MAGVIT-v2 in action recognition, and delivers
comparable compression performance.

</details>


### [38] [CHARM3R: Towards Unseen Camera Height Robust Monocular 3D Detector](https://arxiv.org/abs/2508.11185)
*Abhinav Kumar,Yuliang Guo,Zhihao Zhang,Xinyu Huang,Liu Ren,Xiaoming Liu*

Main category: cs.CV

TL;DR: 本文关注单目3D目标检测器在不同相机高度下的泛化能力，提出了CHARM3R模型，有效提升了对未见过的相机高度的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前单目3D目标检测在训练时通常只适配单一相机高度，面对新相机高度或分布外样本性能急剧下降，这一现实应用中经常遇到但被低估的问题急需解决。

Method: 系统分析了现有SOTA单目3D检测算法对相机高度变化的敏感性，发现深度估计误差是主要影响因素。作者用数学推导和实验证明了不同深度估计方法在相机高度变化下的表现趋势，并提出CHARM3R模型，通过融合两种深度估计方式以提升鲁棒性。

Result: 新方法CHARM3R在未见过的相机高度场景下，泛化能力提升超过45%，在CARLA数据集上取得了SOTA表现。

Conclusion: 多种深度估计结果的结合能显著提高单目3D检测器对新相机高度的适应性，为该方向提供了有效思路和方法。

Abstract: Monocular 3D object detectors, while effective on data from one ego camera
height, struggle with unseen or out-of-distribution camera heights. Existing
methods often rely on Plucker embeddings, image transformations or data
augmentation. This paper takes a step towards this understudied problem by
first investigating the impact of camera height variations on state-of-the-art
(SoTA) Mono3D models. With a systematic analysis on the extended CARLA dataset
with multiple camera heights, we observe that depth estimation is a primary
factor influencing performance under height variations. We mathematically prove
and also empirically observe consistent negative and positive trends in mean
depth error of regressed and ground-based depth models, respectively, under
camera height changes. To mitigate this, we propose Camera Height Robust
Monocular 3D Detector (CHARM3R), which averages both depth estimates within the
model. CHARM3R improves generalization to unseen camera heights by more than
$45\%$, achieving SoTA performance on the CARLA dataset. Codes and Models at
https://github.com/abhi1kumar/CHARM3R

</details>


### [39] [Generating Dialogues from Egocentric Instructional Videos for Task Assistance: Dataset, Method and Benchmark](https://arxiv.org/abs/2508.11192)
*Lavisha Aggarwal,Vikas Bahirwani,Lin Li,Andrea Colaco*

Main category: cs.CV

TL;DR: 本论文提出了一种自动化方法，将单人教学视频转化为两人对话形式的分步任务指导，并构建了HowToDIV大规模数据集，有利于AI在真实任务辅助中的对话能力研究。


<details>
  <summary>Details</summary>
Motivation: 现实中许多多步骤任务（如烹饪、修理）需要专家知识，而当前缺乏结合视频和对话的真实世界任务辅助数据集，限制了AI在此领域的进步。

Method: 提出了一种全自动方法，利用大语言模型将单人教学视频转换成对话形式，细粒度对齐任务步骤和视频片段，从而高效构建多任务的对话-视频数据集。

Result: 构建了HowToDIV数据集，包含507场对话，6636问答对，24小时跨多领域的视频。对话内容模拟专家通过可穿戴设备远程指导新手用户完成任务。并基于Gemma-3模型建立了基线性能。

Conclusion: 该工作提供了一种高效自动生成对话-视频任务数据集的方法，为AI在实际任务指导和多模态对话理解方面提供了重要资源和研究基线。

Abstract: Many everyday tasks ranging from fixing appliances, cooking recipes to car
maintenance require expert knowledge, especially when tasks are complex and
multi-step. Despite growing interest in AI agents, there is a scarcity of
dialogue-video datasets grounded for real world task assistance. In this paper,
we propose a simple yet effective approach that transforms single-person
instructional videos into task-guidance two-person dialogues, aligned with fine
grained steps and video-clips. Our fully automatic approach, powered by large
language models, offers an efficient alternative to the substantial cost and
effort required for human-assisted data collection. Using this technique, we
build HowToDIV, a large-scale dataset containing 507 conversations, 6636
question-answer pairs and 24 hours of videoclips across diverse tasks in
cooking, mechanics, and planting. Each session includes multi-turn conversation
where an expert teaches a novice user how to perform a task step by step, while
observing user's surrounding through a camera and microphone equipped wearable
device. We establish the baseline benchmark performance on HowToDIV dataset
through Gemma-3 model for future research on this new task of dialogues for
procedural-task assistance.

</details>


### [40] [UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning](https://arxiv.org/abs/2508.11196)
*Jiajin Guan,Haibo Mei,Bonan Zhang,Dan Liu,Yuanshuang Fu,Yue Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种专为无人机高分辨率航空图像设计的轻量级视觉-语言模型UAV-VL-R1，并构建了配套的数据集，显著提升了相关任务的表现，支持资源受限平台的实时部署。


<details>
  <summary>Details</summary>
Motivation: 通用视觉-语言模型在复杂高分辨率的无人机航空影像任务上表现不佳，主要受空间语义复杂、实时推理需求高等挑战的限制，因此亟需针对性强的新方法。

Method: 提出UAV-VL-R1模型，通过有监督微调(SFT)与多阶段强化学习（RL）结合训练，利用GRPO算法实现基于规则的奖励和组内策略对齐。并构建了50,019条标注样本的高分辨率视觉问答数据集HRVQA-VL，以支持航拍任务的多种推理需求。

Result: UAV-VL-R1在零样本精度上比基线模型Qwen2-VL-2B-Instruct提升48.17%，部分任务上超过了其36倍规模（72B）的模型。消融实验表明SFT提升语义对齐但可能降低推理多样性，GRPO强化学习则提升逻辑灵活性与鲁棒性。

Conclusion: UAV-VL-R1不仅大幅提升了无人机视觉推理任务表现，且内存占用极低（FP16仅3.9GB，INT8量化后2.5GB），适合部署到资源受限的无人机平台环境，实现实时推理，具备实际应用价值。

Abstract: Recent advances in vision-language models (VLMs) have demonstrated strong
generalization in natural image tasks. However, their performance often
degrades on unmanned aerial vehicle (UAV)-based aerial imagery, which features
high resolution, complex spatial semantics, and strict real-time constraints.
These challenges limit the applicability of general-purpose VLMs to structured
aerial reasoning tasks. To address these challenges, we propose UAV-VL-R1, a
lightweight VLM explicitly designed for aerial visual reasoning. It is trained
using a hybrid method that combines supervised fine-tuning (SFT) and
multi-stage reinforcement learning (RL). We leverage the group relative policy
optimization (GRPO) algorithm to promote structured and interpretable reasoning
through rule-guided rewards and intra-group policy alignment. To support model
training and evaluation, we introduce a high-resolution visual question
answering dataset named HRVQA-VL, which consists of 50,019 annotated samples
covering eight UAV-relevant reasoning tasks, including object counting,
transportation recognition, and spatial scene inference. Experimental results
show that UAV-VL-R1 achieves a 48.17% higher zero-shot accuracy than the
Qwen2-VL-2B-Instruct baseline and even outperforms its 72B-scale variant, which
is 36x larger, on multiple tasks. Ablation studies reveal that while SFT
improves semantic alignment, it may reduce reasoning diversity in mathematical
tasks. GRPO-based RL compensates for this limitation by enhancing logical
flexibility and the robustness of inference. Additionally, UAV-VL-R1 requires
only 3.9GB of memory under FP16 inference and can be quantized to 2.5GB with
INT8, supporting real-time deployment on resource-constrained UAV platforms.

</details>


### [41] [A Coarse-to-Fine Human Pose Estimation Method based on Two-stage Distillation and Progressive Graph Neural Network](https://arxiv.org/abs/2508.11212)
*Zhangjian Ji,Wenjin Zhang,Shaotong Qiao,Kai Feng,Yuhua Qian*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的人体姿态估计算法，结合分阶段知识蒸馏与图卷积网络，在保证高精度的同时，有效降低了模型计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 当前主流人体姿态估计算法虽精准，但通常需要大量计算资源，不适合实时和边缘设备应用。因此，论文旨在研究如何在保持性能的同时，实现模型的轻量化。

Method: 论文提出了两阶段知识蒸馏框架。第一阶段通过引入人体关节结构损失，挖掘关节间的结构信息，提升学生模型对高层语义的学习能力；第二阶段利用图卷积网络IGP-GCN对第一阶段输出进行细化，并借助教师模型的最终输出，分步监督图卷积网络训练，以进一步优化姿态估计。

Result: 在COCO keypoint与CrowdPose数据集上进行实验，结果显示该方法优于许多现有主流方法，尤其在复杂场景的CrowdPose数据集上的提升更加显著。

Conclusion: 所提两阶段知识蒸馏方法能够有效提升轻量化姿态估计算法的精度和表现，兼顾准确性与推理效率，适用于更广泛的实际应用场景。

Abstract: Human pose estimation has been widely applied in the human-centric
understanding and generation, but most existing state-of-the-art human pose
estimation methods require heavy computational resources for accurate
predictions. In order to obtain an accurate, robust yet lightweight human pose
estimator, one feasible way is to transfer pose knowledge from a powerful
teacher model to a less-parameterized student model by knowledge distillation.
However, the traditional knowledge distillation framework does not fully
explore the contextual information among human joints. Thus, in this paper, we
propose a novel coarse-to-fine two-stage knowledge distillation framework for
human pose estimation. In the first-stage distillation, we introduce the human
joints structure loss to mine the structural information among human joints so
as to transfer high-level semantic knowledge from the teacher model to the
student model. In the second-stage distillation, we utilize an Image-Guided
Progressive Graph Convolutional Network (IGP-GCN) to refine the initial human
pose obtained from the first-stage distillation and supervise the training of
the IGP-GCN in the progressive way by the final output pose of teacher model.
The extensive experiments on the benchmark dataset: COCO keypoint and CrowdPose
datasets, show that our proposed method performs favorably against lots of the
existing state-of-the-art human pose estimation methods, especially for the
more complex CrowdPose dataset, the performance improvement of our model is
more significant.

</details>


### [42] [A CLIP-based Uncertainty Modal Modeling (UMM) Framework for Pedestrian Re-Identification in Autonomous Driving](https://arxiv.org/abs/2508.11218)
*Jialin Li,Shuqi Wu,Ning Wang*

Main category: cs.CV

TL;DR: 提出了一种名为UMM的轻量级多模态不确定性建模框架，用于提升自动驾驶中行人重识别系统在多模态缺失和不确定条件下的鲁棒性与效率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶场景中，行人重识别（ReID）要求系统在多视角、多时刻内实时且稳定识别行人，但常规方法在遇到输入模态不全（如RGB、红外、素描或文本描述等缺失）时效果明显下降。同时，大型预训练多模态模型虽表现优异，但计算开销大，不适用于算力有限的实际部署场景。

Method: 提出Uncertainty Modal Modeling（UMM）框架，整合了多模态Token Mapper、合成模态增强策略以及跨模态线索交互学习模块，实现模态统一特征表达和互补信息提取，并借助CLIP的视觉-文本对齐能力，在无需大量微调的情况下高效融合多模态信息。

Result: 实验表明，UMM框架在不确定模态缺失下表现出较强的鲁棒性、泛化性以及计算效率，优于传统方法。

Conclusion: UMM为自动驾驶场景下缺失/不确定模态下的行人重识别提供了高效、可扩展且实用的解决方案。

Abstract: Re-Identification (ReID) is a critical technology in intelligent perception
systems, especially within autonomous driving, where onboard cameras must
identify pedestrians across views and time in real-time to support safe
navigation and trajectory prediction. However, the presence of uncertain or
missing input modalities--such as RGB, infrared, sketches, or textual
descriptions--poses significant challenges to conventional ReID approaches.
While large-scale pre-trained models offer strong multimodal semantic modeling
capabilities, their computational overhead limits practical deployment in
resource-constrained environments. To address these challenges, we propose a
lightweight Uncertainty Modal Modeling (UMM) framework, which integrates a
multimodal token mapper, synthetic modality augmentation strategy, and
cross-modal cue interactive learner. Together, these components enable unified
feature representation, mitigate the impact of missing modalities, and extract
complementary information across different data types. Additionally, UMM
leverages CLIP's vision-language alignment ability to fuse multimodal inputs
efficiently without extensive finetuning. Experimental results demonstrate that
UMM achieves strong robustness, generalization, and computational efficiency
under uncertain modality conditions, offering a scalable and practical solution
for pedestrian re-identification in autonomous driving scenarios.

</details>


### [43] [FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation](https://arxiv.org/abs/2508.11255)
*MengChao Wang,Qiang Wang,Fan Jiang,Mu Xu*

Main category: cs.CV

TL;DR: 该论文提出一套用于语音驱动的人像动画的多维人类偏好对齐方法，在动自然度、唇形同步和视觉质量等多方面取得领先效果。


<details>
  <summary>Details</summary>
Motivation: 现有的音频驱动人像动画技术，难以同时在动作自然性、唇形同步及视觉质量等多维度满足精细的人体偏好，且缺乏覆盖多维人类偏好的大规模高质量数据集。

Method: 论文提出了三项创新：1）多模态奖励模型Talking-Critic，用于学习贴合人类偏好的多维奖励函数评价生成视频的偏好契合度；2）基于Talking-Critic，建立了包含41万个偏好对的大规模多维人类偏好数据集Talking-NSQ；3）提出Timestep-Layer自适应多专家偏好优化框架（TLPO），将不同偏好维度解耦为专业模块，按时间步和网络层级融合，实现各维度的全面提升并避免干扰。

Result: 实验表明，Talking-Critic在与人偏好对齐方面显著优于现有方法。TLPO方法在唇形同步、动作自然性和视觉质量等指标上，对比传统基线模型有大幅改进，且在主观和客观评测中均表现最佳。

Conclusion: 论文验证了多维人类偏好驱动的优化策略在音频驱动人像动画模型上的有效性，显著提升了生成视频在多个关键维度的总体表现，对实际应用有较大推动作用。

Abstract: Recent advances in audio-driven portrait animation have demonstrated
impressive capabilities. However, existing methods struggle to align with
fine-grained human preferences across multiple dimensions, such as motion
naturalness, lip-sync accuracy, and visual quality. This is due to the
difficulty of optimizing among competing preference objectives, which often
conflict with one another, and the scarcity of large-scale, high-quality
datasets with multidimensional preference annotations. To address these, we
first introduce Talking-Critic, a multimodal reward model that learns
human-aligned reward functions to quantify how well generated videos satisfy
multidimensional expectations. Leveraging this model, we curate Talking-NSQ, a
large-scale multidimensional human preference dataset containing 410K
preference pairs. Finally, we propose Timestep-Layer adaptive multi-expert
Preference Optimization (TLPO), a novel framework for aligning diffusion-based
portrait animation models with fine-grained, multidimensional preferences. TLPO
decouples preferences into specialized expert modules, which are then fused
across timesteps and network layers, enabling comprehensive, fine-grained
enhancement across all dimensions without mutual interference. Experiments
demonstrate that Talking-Critic significantly outperforms existing methods in
aligning with human preference ratings. Meanwhile, TLPO achieves substantial
improvements over baseline models in lip-sync accuracy, motion naturalness, and
visual quality, exhibiting superior performance in both qualitative and
quantitative evaluations. Ours project page:
https://fantasy-amap.github.io/fantasy-talking2/

</details>


### [44] [Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense Perception](https://arxiv.org/abs/2508.11256)
*Junjie Wang,Keyu Chen,Yulin Li,Bin Chen,Hengshuang Zhao,Xiaojuan Qi,Zhuotao Tian*

Main category: cs.CV

TL;DR: 该论文提出了DeCLIP框架，通过解耦CLIP的自注意力模块，分别提升内容与上下文特征，从而显著提升开放词汇密集视觉感知的表现，在多个任务上取得了最新最优结果。


<details>
  <summary>Details</summary>
Motivation: 当前密集视觉感知任务依赖预定义类别，限制了实际应用。虽然VLM（如CLIP）在开放词汇任务有潜力，但局部特征表达有限，导致性能不佳。因此需要提升VLM在密集感知中的表现，突破类别限制。

Method: 作者提出DeCLIP框架，将CLIP自注意力模块解耦，分别获取内容特征和上下文特征。上下文特征利用视觉基础模型(VFM)的语义关联和扩散模型的目标完整性信息进行增强，提高空间一致性；内容特征对齐图片裁剪特征，并由VFM中的区域关联进行约束，以提升局部的区分度。

Result: DeCLIP在2D检测与分割、3D实例分割、视频实例分割和6D物体位姿估计等多个开放词汇密集感知任务上均取得了最新最优的性能。

Conclusion: DeCLIP为开放词汇密集视觉感知任务奠定了坚实的基础，显著提升了不同任务的性能。

Abstract: Dense visual perception tasks have been constrained by their reliance on
predefined categories, limiting their applicability in real-world scenarios
where visual concepts are unbounded. While Vision-Language Models (VLMs) like
CLIP have shown promise in open-vocabulary tasks, their direct application to
dense perception often leads to suboptimal performance due to limitations in
local feature representation. In this work, we present our observation that
CLIP's image tokens struggle to effectively aggregate information from
spatially or semantically related regions, resulting in features that lack
local discriminability and spatial consistency. To address this issue, we
propose DeCLIP, a novel framework that enhances CLIP by decoupling the
self-attention module to obtain ``content'' and ``context'' features
respectively. \revise{The context features are enhanced by jointly distilling
semantic correlations from Vision Foundation Models (VFMs) and object integrity
cues from diffusion models, thereby enhancing spatial consistency. In parallel,
the content features are aligned with image crop representations and
constrained by region correlations from VFMs to improve local discriminability.
Extensive experiments demonstrate that DeCLIP establishes a solid foundation
for open-vocabulary dense perception, consistently achieving state-of-the-art
performance across a broad spectrum of tasks, including 2D detection and
segmentation, 3D instance segmentation, video instance segmentation, and 6D
object pose estimation.} Code is available at
https://github.com/xiaomoguhz/DeCLIP

</details>


### [45] [Vision-Language Models display a strong gender bias](https://arxiv.org/abs/2508.11262)
*Aiswarya Konavoor,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.CV

TL;DR: 本研究评估了视觉-语言模型(VLM)在对图像和文本对齐时是否会表现出性别关联及其偏见。通过对脸部照片与职业和活动描述语句在嵌入空间的接近度进行量化，揭示VLM在社会刻板印象上的潜在放大效应。


<details>
  <summary>Details</summary>
Motivation: 尽管VLM因检索和零样本学习能力受关注，但其对社会刻板印象的编码与放大问题常常被准确率等标准指标忽视。论文希望检测VLM潜在的性别偏见并提出更有效的偏见评估方法。

Method: 作者收集了220张按性别分组的人脸照片和150个分为六类的短语。用对比学习编码器生成单位范数嵌入，计算每个短语与男女照片组的平均余弦相似度之差作为性别关联性分数。采用bootstrap置信区间和标签置换模型评估统计显著性与基线偏差。

Result: 结果给出了对比视觉-语言空间下，面向不同语句和类别的性别关联性热力图，并附有不确定性度量和基本检验，反映模型的性别偏见分布。

Conclusion: 作者提出了一种稳健的性别偏见评估框架，发现VLM嵌入空间确实存在类别相关的性别关联，为未来改进大模型公平性和去偏提供了工具和基础。

Abstract: Vision-language models (VLM) align images and text in a shared representation
space that is useful for retrieval and zero-shot transfer. Yet, this alignment
can encode and amplify social stereotypes in subtle ways that are not obvious
from standard accuracy metrics. In this study, we test whether the contrastive
vision-language encoder exhibits gender-linked associations when it places
embeddings of face images near embeddings of short phrases that describe
occupations and activities. We assemble a dataset of 220 face photographs split
by perceived binary gender and a set of 150 unique statements distributed
across six categories covering emotional labor, cognitive labor, domestic
labor, technical labor, professional roles, and physical labor. We compute
unit-norm image embeddings for every face and unit-norm text embeddings for
every statement, then define a statement-level association score as the
difference between the mean cosine similarity to the male set and the mean
cosine similarity to the female set, where positive values indicate stronger
association with the male set and negative values indicate stronger association
with the female set. We attach bootstrap confidence intervals by resampling
images within each gender group, aggregate by category with a separate
bootstrap over statements, and run a label-swap null model that estimates the
level of mean absolute association we would expect if no gender structure were
present. The outcome is a statement-wise and category-wise map of gender
associations in a contrastive vision-language space, accompanied by
uncertainty, simple sanity checks, and a robust gender bias evaluation
framework.

</details>


### [46] [Domain-aware Category-level Geometry Learning Segmentation for 3D Point Clouds](https://arxiv.org/abs/2508.11265)
*Pei He,Lingling Li,Licheng Jiao,Ronghua Shang,Fang Liu,Shuang Wang,Xu Liu,Wenping Ma*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D点云分割通用化方法，通过引入类别级几何学习，有效提升了模型在未见环境中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在3D点云分割中，模型往往难以泛化到不同的数据域。现有方法多靠数据增强，但忽略了类别级几何信息与分布对泛化的重要性。如何提取具有领域不变性的几何特征，是提升点云模型通用表现的关键。

Method: 提出类别级几何嵌入（CGE）用于细粒度提取各类别点云的几何属性，结合到语义学习中；并引入几何一致性学习（GCL），通过模拟潜在3D分布，对齐类别级几何嵌入，使模型聚焦于几何不变特征，提升对新域的适应能力。

Result: 实验表明，本文方法在多个数据集和不同域上的3D语义分割精度优于现有主流方法，显示了强大的跨域泛化能力。

Conclusion: 类别级几何学习框架能有效提升3D点云分割模型的领域泛化能力，有望为实际跨环境点云应用提供更鲁棒的解决方案。

Abstract: Domain generalization in 3D segmentation is a critical challenge in deploying
models to unseen environments. Current methods mitigate the domain shift by
augmenting the data distribution of point clouds. However, the model learns
global geometric patterns in point clouds while ignoring the category-level
distribution and alignment. In this paper, a category-level geometry learning
framework is proposed to explore the domain-invariant geometric features for
domain generalized 3D semantic segmentation. Specifically, Category-level
Geometry Embedding (CGE) is proposed to perceive the fine-grained geometric
properties of point cloud features, which constructs the geometric properties
of each class and couples geometric embedding to semantic learning. Secondly,
Geometric Consistent Learning (GCL) is proposed to simulate the latent 3D
distribution and align the category-level geometric embeddings, allowing the
model to focus on the geometric invariant information to improve
generalization. Experimental results verify the effectiveness of the proposed
method, which has very competitive segmentation accuracy compared with the
state-of-the-art domain generalized point cloud methods.

</details>


### [47] [Enhancing Supervised Composed Image Retrieval via Reasoning-Augmented Representation Engineering](https://arxiv.org/abs/2508.11272)
*Jun Li,Kai Li,Shaoguo Liu,Tingting Gao*

Main category: cs.CV

TL;DR: 该论文提出了用于组合图像检索（CIR）的新框架PMTFR，通过Pyramid Matching Model结合无需训练的细化机制，实现了对图像和文本指令的更好联合理解，在有监督CIR任务上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 组合图像检索要求模型同时理解图片和针对图片的文本修改指令，现有方法要么分两阶段、增加模型训练成本，要么只用于零样本设置，缺乏对有监督场景的优化方案。作者希望降低训练需求，同时提升检索效果。

Method: 提出Pyramid Matching Model with Training-Free Refinement (PMTFR)框架，核心包括Pyramid Patcher模块提升多粒度视觉理解力，并借鉴COT表现表示工程，从COT数据中提取表征，注入到多模态大模型(LVLMs)，在无需显式文本推理和额外训练的前提下优化检索分数。

Result: 在多个CIR基准数据集上，PMTFR方法在有监督CIR任务上取得了优于现有最先进方法的表现。

Conclusion: PMTFR框架有效提升了组合图像检索的效果，特别是在有监督场景下，无需再对排名模型进行专门训练，简化了流程并提高了性能。相关代码将对外公开，便于社区进一步研究。

Abstract: Composed Image Retrieval (CIR) presents a significant challenge as it
requires jointly understanding a reference image and a modified textual
instruction to find relevant target images. Some existing methods attempt to
use a two-stage approach to further refine retrieval results. However, this
often requires additional training of a ranking model. Despite the success of
Chain-of-Thought (CoT) techniques in reducing training costs for language
models, their application in CIR tasks remains limited -- compressing visual
information into text or relying on elaborate prompt designs. Besides, existing
works only utilize it for zero-shot CIR, as it is challenging to achieve
satisfactory results in supervised CIR with a well-trained model. In this work,
we proposed a framework that includes the Pyramid Matching Model with
Training-Free Refinement (PMTFR) to address these challenges. Through a simple
but effective module called Pyramid Patcher, we enhanced the Pyramid Matching
Model's understanding of visual information at different granularities.
Inspired by representation engineering, we extracted representations from COT
data and injected them into the LVLMs. This approach allowed us to obtain
refined retrieval scores in the Training-Free Refinement paradigm without
relying on explicit textual reasoning, further enhancing performance. Extensive
experiments on CIR benchmarks demonstrate that PMTFR surpasses state-of-the-art
methods in supervised CIR tasks. The code will be made public.

</details>


### [48] [Probing the Representational Power of Sparse Autoencoders in Vision Models](https://arxiv.org/abs/2508.11277)
*Matthew Lyle Olson,Musashi Hinck,Neale Ratzlaff,Changbai Li,Phillip Howard,Vasudev Lal,Shao-Yen Tseng*

Main category: cs.CV

TL;DR: 本文系统性评估了稀疏自编码器（SAEs）在视觉模型中的表示能力，发现其特征在多种任务中具有语义意义，并改善了模型的泛化与可控性。


<details>
  <summary>Details</summary>
Motivation: 虽然SAE在解释大型语言模型的隐藏状态上已广受关注，但其在视觉领域的应用与效果尚未充分研究。作者希望探索SAE在视觉模型中的表现及解释能力。

Method: 作者在三类视觉模型（视觉嵌入模型、多模态大模型和扩散模型）上进行了广泛的实验，评估SAE特征的语义性、泛化能力与可控性，包括OOD检测、语义属性自动发现、跨模态共享等。

Result: 实验结果显示SAE特征具有语义解释能力，能提升模型对分布外数据（OOD）的检测和泛化，并能在扩散模型中实现语义可控生成，在多模态任务中揭示模态间的共享表征。

Conclusion: 本研究为SAE在视觉领域的评估提供了基础，突出其在提升模型可解释性、泛化性以及可控性等方面的巨大潜力。

Abstract: Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting
the hidden states of large language models (LLMs). By learning to reconstruct
activations from a sparse bottleneck layer, SAEs discover interpretable
features from the high-dimensional internal representations of LLMs. Despite
their popularity with language models, SAEs remain understudied in the visual
domain. In this work, we provide an extensive evaluation the representational
power of SAEs for vision models using a broad range of image-based tasks. Our
experimental results demonstrate that SAE features are semantically meaningful,
improve out-of-distribution generalization, and enable controllable generation
across three vision model architectures: vision embedding models, multi-modal
LMMs and diffusion models. In vision embedding models, we find that learned SAE
features can be used for OOD detection and provide evidence that they recover
the ontological structure of the underlying model. For diffusion models, we
demonstrate that SAEs enable semantic steering through text encoder
manipulation and develop an automated pipeline for discovering
human-interpretable attributes. Finally, we conduct exploratory experiments on
multi-modal LLMs, finding evidence that SAE features reveal shared
representations across vision and language modalities. Our study provides a
foundation for SAE evaluation in vision models, highlighting their strong
potential improving interpretability, generalization, and steerability in the
visual domain.

</details>


### [49] [Unifying Scale-Aware Depth Prediction and Perceptual Priors for Monocular Endoscope Pose Estimation and Tissue Reconstruction](https://arxiv.org/abs/2508.11282)
*Muzammil Khan,Enzo Kerkhof,Matteo Fusaglia,Koert Kuhlmann,Theo Ruers,Françoise J. Siepel*

Main category: cs.CV

TL;DR: 该论文提出了一个统一框架，实现单目内窥镜下的组织表面三维重建和位姿估计，显著提升了手术中的空间感知能力和导航精度。


<details>
  <summary>Details</summary>
Motivation: 现有单目内窥镜组织重建面临深度模糊、组织变形、运动不一致、纹理信息有限和视域受限等问题，这些问题影响了重建的准确性和手术安全性，因此需要新的高效方法来解决。

Method: 提出的框架整合了尺度感知的深度预测和时序感知的感知精炼，包含创新的MAPIS-Depth模块（通过Depth Pro和Depth Anything进行深度初始化和预测，结合L-BFGS-B优化得到伪度量深度），再利用RAFT进行时序像素对应与LPIPS感知相似性自适应融合，降低组织变形和运动造成的伪影。配套WEMA-RTDL模块优化位姿，最后采用截断符号距离函数和Marching Cubes提取三维表面网格。

Result: 在HEVD和SCARED数据集上进行消融和对比实验，结果表明该框架的鲁棒性和精度均优于现有先进方法。

Conclusion: 该统一框架有效提升了单目内窥镜下组织三维重建和位姿估计的准确性，能够在实际手术环境中提高空间感知和导航能力，具有良好的推广应用价值。

Abstract: Accurate endoscope pose estimation and 3D tissue surface reconstruction
significantly enhances monocular minimally invasive surgical procedures by
enabling accurate navigation and improved spatial awareness. However, monocular
endoscope pose estimation and tissue reconstruction face persistent challenges,
including depth ambiguity, physiological tissue deformation, inconsistent
endoscope motion, limited texture fidelity, and a restricted field of view. To
overcome these limitations, a unified framework for monocular endoscopic tissue
reconstruction that integrates scale-aware depth prediction with
temporally-constrained perceptual refinement is presented. This framework
incorporates a novel MAPIS-Depth module, which leverages Depth Pro for robust
initialisation and Depth Anything for efficient per-frame depth prediction, in
conjunction with L-BFGS-B optimisation, to generate pseudo-metric depth
estimates. These estimates are temporally refined by computing pixel
correspondences using RAFT and adaptively blending flow-warped frames based on
LPIPS perceptual similarity, thereby reducing artefacts arising from
physiological tissue deformation and motion. To ensure accurate registration of
the synthesised pseudo-RGBD frames from MAPIS-Depth, a novel WEMA-RTDL module
is integrated, optimising both rotation and translation. Finally, truncated
signed distance function-based volumetric fusion and marching cubes are applied
to extract a comprehensive 3D surface mesh. Evaluations on HEVD and SCARED,
with ablation and comparative analyses, demonstrate the framework's robustness
and superiority over state-of-the-art methods.

</details>


### [50] [TimeMachine: Fine-Grained Facial Age Editing with Identity Preservation](https://arxiv.org/abs/2508.11284)
*Yilin Mi,Qixin Yan,Zheng-Peng Duan,Chunle Guo,Hubery Yin,Hao Liu,Chen Li,Chongyi Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的面部年龄编辑方法TimeMachine，通过精细年龄信息注入与轻量级年龄分类器指导，实现了在保持身份特征不变的同时，精准控制年龄变化，并引入大规模高质量人脸年龄数据集。


<details>
  <summary>Details</summary>
Motivation: 当前面部图像编辑虽取得进步，但实现既精细又保真（身份特征不变）的年龄编辑仍非常有挑战性，且现有数据集存在不足。作者旨在提出新方法与数据集来解决这些核心问题。

Method: 方法包括：1）在多重交叉注意力模块中注入高精度年龄信息，实现年龄与身份特征的显式分离和精确解耦；2）提出Age Classifier Guidance模块，在潜空间直接预测年龄，通过轻量模块增加年龄约束，提高年龄编辑准确率，但训练代价提升有限；3）构建百万高分辨率带身份与属性标签的人脸年龄数据集HFFA。

Result: 实验表明，TimeMachine方法在细粒度年龄编辑和身份保持方面均达到最新最佳水平（SOTA），并验证了新数据集和核心模块的有效性。

Conclusion: TimeMachine在面部年龄编辑领域实现了年龄可控、身份保持的优异表现，并通过高质量数据集和新颖架构推动了人脸编辑技术发展。

Abstract: With the advancement of generative models, facial image editing has made
significant progress. However, achieving fine-grained age editing while
preserving personal identity remains a challenging task.In this paper, we
propose TimeMachine, a novel diffusion-based framework that achieves accurate
age editing while keeping identity features unchanged. To enable fine-grained
age editing, we inject high-precision age information into the multi-cross
attention module, which explicitly separates age-related and identity-related
features. This design facilitates more accurate disentanglement of age
attributes, thereby allowing precise and controllable manipulation of facial
aging.Furthermore, we propose an Age Classifier Guidance (ACG) module that
predicts age directly in the latent space, instead of performing denoising
image reconstruction during training. By employing a lightweight module to
incorporate age constraints, this design enhances age editing accuracy by
modest increasing training cost. Additionally, to address the lack of
large-scale, high-quality facial age datasets, we construct a HFFA dataset
(High-quality Fine-grained Facial-Age dataset) which contains one million
high-resolution images labeled with identity and facial attributes.
Experimental results demonstrate that TimeMachine achieves state-of-the-art
performance in fine-grained age editing while preserving identity consistency.

</details>


### [51] [Hyperspectral vs. RGB for Pedestrian Segmentation in Urban Driving Scenes: A Comparative Study](https://arxiv.org/abs/2508.11301)
*Jiarong Li,Imad Ali Shah,Enda Ward,Martin Glavin,Edward Jones,Brian Deegan*

Main category: cs.CV

TL;DR: 本文探讨通过高光谱成像(HSI)优化自动驾驶中行人分割问题，提出特定波段选择方法显著优于传统RGB成像。


<details>
  <summary>Details</summary>
Motivation: 在城市自动驾驶环境中，行人与背景在RGB图像中因同色异谱现象（metamerism）常常难以区分，导致分割性能降低，影响交通安全，需要寻找更有效的感知手段。

Method: 利用Hyperspectral City v2 (H-City)数据集，将128通道HSI数据通过PCA和基于对比信噪比联合互信息最大化（CSNR-JMIM）的波段选择算法分别压缩为3通道，再分别应用U-Net、DeepLabV3+及SegFormer三种分割模型进行性能比较。

Result: CSNR-JMIM方法在行人分割IoU上比RGB平均提升1.44%，F1分数提升2.18%；在骑行者分割上IoU提升1.43%，F1分数提升2.25%；效果优于PCA和传统RGB，主要因为波段选择提升了光谱区分能力，降低了误检测率。

Conclusion: 通过最优HSI波段选择，能显著提升城市复杂场景下对行人及骑行者的分割性能，展现了在安全关键汽车感知系统中的应用潜力。

Abstract: Pedestrian segmentation in automotive perception systems faces critical
safety challenges due to metamerism in RGB imaging, where pedestrians and
backgrounds appear visually indistinguishable.. This study investigates the
potential of hyperspectral imaging (HSI) for enhanced pedestrian segmentation
in urban driving scenarios using the Hyperspectral City v2 (H-City) dataset. We
compared standard RGB against two dimensionality-reduction approaches by
converting 128-channel HSI data into three-channel representations: Principal
Component Analysis (PCA) and optimal band selection using Contrast
Signal-to-Noise Ratio with Joint Mutual Information Maximization (CSNR-JMIM).
Three semantic segmentation models were evaluated: U-Net, DeepLabV3+, and
SegFormer. CSNR-JMIM consistently outperformed RGB with an average improvements
of 1.44% in Intersection over Union (IoU) and 2.18% in F1-score for pedestrian
segmentation. Rider segmentation showed similar gains with 1.43% IoU and 2.25%
F1-score improvements. These improved performance results from enhanced
spectral discrimination of optimally selected HSI bands effectively reducing
false positives. This study demonstrates robust pedestrian segmentation through
optimal HSI band selection, showing significant potential for safety-critical
automotive applications.

</details>


### [52] [Controlling Multimodal LLMs via Reward-guided Decoding](https://arxiv.org/abs/2508.11616)
*Oscar Mañas,Pierluca D'Oro,Koustuv Sinha,Adriana Romero-Soriano,Michal Drozdzal,Aishwarya Agrawal*

Main category: cs.CV

TL;DR: 本文提出了一种用于多模态大模型（MLLM）的奖励引导解码方法，实现了对输出视觉锚定程度的可控性，并在防止对象幻觉方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型（MLLM）的广泛应用，用户对于模型适应多样需求的能力提出了更高要求，尤其是在视觉锚定等关键任务中，希望输出可以根据具体需求动态调整。

Method: 作者提出了一种通过奖励引导MLLM解码过程的方法，首先训练两个独立的奖励模型，分别控制输出中对象的精确度和召回率，然后利用这些奖励模型在推理过程中指导解码，可动态调节各奖励的权重以及解码时搜索的广度。

Result: 实验在主流对象幻觉基准上验证了方法的有效性，结果表明：该方法能显著提升模型输出的可控性，在防止对象幻觉等任务中性能优于已有的缓解手段。

Conclusion: 该方法为多模态大模型的推理过程提供了可行的即时可控性方案，不仅能提升生成内容的相关性和准确性，还可根据实际需求动态在精确与召回、计算资源消耗与视觉锚定强度之间进行权衡。

Abstract: As Multimodal Large Language Models (MLLMs) gain widespread applicability, it
is becoming increasingly desirable to adapt them for diverse user needs. In
this paper, we study the adaptation of MLLMs through controlled decoding. To
achieve this, we introduce the first method for reward-guided decoding of MLLMs
and demonstrate its application in improving their visual grounding. Our method
involves building reward models for visual grounding and using them to guide
the MLLM's decoding process. Concretely, we build two separate reward models to
independently control the degree of object precision and recall in the model's
output. Our approach enables on-the-fly controllability of an MLLM's inference
process in two ways: first, by giving control over the relative importance of
each reward function during decoding, allowing a user to dynamically trade off
object precision for recall in image captioning tasks; second, by giving
control over the breadth of the search during decoding, allowing the user to
control the trade-off between the amount of test-time compute and the degree of
visual grounding. We evaluate our method on standard object hallucination
benchmarks, showing that it provides significant controllability over MLLM
inference, while consistently outperforming existing hallucination mitigation
methods.

</details>


### [53] [Denoise-then-Retrieve: Text-Conditioned Video Denoising for Video Moment Retrieval](https://arxiv.org/abs/2508.11313)
*Weijia Liu,Jiuxin Cao,Bo Miao,Zhiheng Fu,Xuelin Zhu,Jiawei Ge,Bo Liu,Mehwish Nasim,Ajmal Mian*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频时刻检索（VMR）方法，通过过滤与文本无关的视频片段来优化多模态表示，从而提升检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有VMR方法会同时编码所有视频片段，包括与文本无关的片段，这会导致多模态对齐受干扰，优化效果变差。作者希望解决这一痛点，提升检索的准确性。

Method: 作者提出了“去噪-后检索”（denoise-then-retrieve）范式，设计了Denoise-then-Retrieve Network (DRNet)，包含文本条件去噪（TCD）和文本重建反馈（TRF）两个模块。TCD模块利用交叉注意力和结构化状态空间动态识别并屏蔽噪声片段，TRF则通过将滤后的视频表示与文本表示对齐进一步监督训练。最终只用净化后的视频表示进行时刻检索。

Result: 在Charades-STA和QVHighlights两个数据集上，DRNet在所有评价指标上都优于当前最先进方法。

Conclusion: 本文的“去噪-后检索”范式能提升VMR系统的表现，并且具备很强的通用性，可以无缝集成到其它高级VMR模型中增强性能。

Abstract: Current text-driven Video Moment Retrieval (VMR) methods encode all video
clips, including irrelevant ones, disrupting multimodal alignment and hindering
optimization. To this end, we propose a denoise-then-retrieve paradigm that
explicitly filters text-irrelevant clips from videos and then retrieves the
target moment using purified multimodal representations. Following this
paradigm, we introduce the Denoise-then-Retrieve Network (DRNet), comprising
Text-Conditioned Denoising (TCD) and Text-Reconstruction Feedback (TRF)
modules. TCD integrates cross-attention and structured state space blocks to
dynamically identify noisy clips and produce a noise mask to purify multimodal
video representations. TRF further distills a single query embedding from
purified video representations and aligns it with the text embedding, serving
as auxiliary supervision for denoising during training. Finally, we perform
conditional retrieval using text embeddings on purified video representations
for accurate VMR. Experiments on Charades-STA and QVHighlights demonstrate that
our approach surpasses state-of-the-art methods on all metrics. Furthermore,
our denoise-then-retrieve paradigm is adaptable and can be seamlessly
integrated into advanced VMR models to boost performance.

</details>


### [54] [Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models](https://arxiv.org/abs/2508.11317)
*Yuchen Zhou,Jiayu Tang,Shuo Yang,Xiaoyan Xiao,Yuqin Dai,Wenhao Yang,Chao Gou,Xiaobo Xia,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 该论文提出LogicBench基准和LogicCLIP方法，用于提升视觉-语言模型在逻辑理解方面的能力，并有实验证明明显优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前主流的视觉-语言模型（如CLIP）在逻辑理解能力上缺乏深入探索，存在明显的“逻辑盲区”，这限制了模型在实际应用中的可靠性。论文动机在于评价并提升这类模型的逻辑推理与理解能力。

Method: 作者构建了包含9类逻辑类别、四种场景（图像、视频、异常检测、医疗诊断）、超过5万视觉-语言对的LogicBench基准，并提出逻辑感知数据生成和多层级对比学习目标（包含粗粒度对齐、细粒度多选任务和逻辑结构感知目标）的新训练框架LogicCLIP。

Result: 实验显示，现有VLMs在逻辑理解任务上与人类表现差距超过40个百分点，尤以因果性和条件性任务为甚。LogicCLIP在所有LogicBench子任务上均取得了显著提升，超越所有基线模型。同时其对常规视觉-语言任务的性能也未下降，部分场景下还超越了原有性能。

Conclusion: LogicCLIP能有效提升VLM在逻辑理解上的能力，为未来相关研究提供了评测基准和更优训练框架。

Abstract: Vision-Language Models (VLMs), exemplified by CLIP, have emerged as
foundational for multimodal intelligence. However, their capacity for logical
understanding remains significantly underexplored, resulting in critical
''logical blindspots'' that limit their reliability in practical applications.
To systematically diagnose this, we introduce LogicBench, a comprehensive
benchmark with over 50,000 vision-language pairs across 9 logical categories
and 4 diverse scenarios: images, videos, anomaly detection, and medical
diagnostics. Our evaluation reveals that existing VLMs, even the
state-of-the-art ones, fall at over 40 accuracy points below human performance,
particularly in challenging tasks like Causality and Conditionality,
highlighting their reliance on surface semantics over critical logical
structures. To bridge this gap, we propose LogicCLIP, a novel training
framework designed to boost VLMs' logical sensitivity through advancements in
both data generation and optimization objectives. LogicCLIP utilizes
logic-aware data generation and a contrastive learning strategy that combines
coarse-grained alignment, a fine-grained multiple-choice objective, and a novel
logical structure-aware objective. Extensive experiments demonstrate
LogicCLIP's substantial improvements in logical comprehension across all
LogicBench domains, significantly outperforming baselines. Moreover, LogicCLIP
retains, and often surpasses, competitive performance on general
vision-language benchmarks, demonstrating that the enhanced logical
understanding does not come at the expense of general alignment. We believe
that LogicBench and LogicCLIP will be important resources for advancing VLM
logical capabilities.

</details>


### [55] [Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking](https://arxiv.org/abs/2508.11323)
*Haonan Zhang,Xinyao Wang,Boxi Wu,Tu Zheng,Wang Yunhua,Zheng Yang*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D多目标跟踪方法DSC-Track，结合空间几何信息和特征一致性，实现更准确和鲁棒的跟踪，并在主流数据集上取得了领先成绩。


<details>
  <summary>Details</summary>
Motivation: 传统3D跟踪方法如卡尔曼滤波，虽然在简单场景下有效，但在拥挤或检测不准时表现不佳，这主要由于忽视了物体之间的空间几何关系。现有的空间关系方法有时又受到无关物体干扰。为此，作者希望设计一种能够稳定利用空间提示且降低干扰的方法。

Method: 作者提出DSC-Track，包括：1）用点对特征（PPF）设计空间-时间统一编码器，获取判别性轨迹嵌入并抑制干扰；2）引入提示一致性Transformer模块，对齐历史轨迹和当前检测中的一致特征；3）动态更新机制，保留关键信息实现稳定在线跟踪。

Result: 本文在nuScenes和Waymo开放数据集上进行实验，DSC-Track在nuScenes验证集和测试集上分别达到73.2%和70.3% AMOTA，表现超过现有方法，验证了该方法的有效性和鲁棒性。

Conclusion: DSC-Track能显著提升3D多目标跟踪在复杂场景下的准确率与鲁棒性，证明空间提示一致性对跟踪任务的显著价值。

Abstract: 3D multi-object tracking is a critical and challenging task in the field of
autonomous driving. A common paradigm relies on modeling individual object
motion, e.g., Kalman filters, to predict trajectories. While effective in
simple scenarios, this approach often struggles in crowded environments or with
inaccurate detections, as it overlooks the rich geometric relationships between
objects. This highlights the need to leverage spatial cues. However, existing
geometry-aware methods can be susceptible to interference from irrelevant
objects, leading to ambiguous features and incorrect associations. To address
this, we propose focusing on cue-consistency: identifying and matching stable
spatial patterns over time. We introduce the Dynamic Scene Cue-Consistency
Tracker (DSC-Track) to implement this principle. Firstly, we design a unified
spatiotemporal encoder using Point Pair Features (PPF) to learn discriminative
trajectory embeddings while suppressing interference. Secondly, our
cue-consistency transformer module explicitly aligns consistent feature
representations between historical tracks and current detections. Finally, a
dynamic update mechanism preserves salient spatiotemporal information for
stable online tracking. Extensive experiments on the nuScenes and Waymo Open
Datasets validate the effectiveness and robustness of our approach. On the
nuScenes benchmark, for instance, our method achieves state-of-the-art
performance, reaching 73.2% and 70.3% AMOTA on the validation and test sets,
respectively.

</details>


### [56] [Noise Matters: Optimizing Matching Noise for Diffusion Classifiers](https://arxiv.org/abs/2508.11330)
*Yanghao Wang,Long Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的噪声优化方法NoOp，解决了扩散分类器因噪声不稳定导致性能波动的问题，大幅提升了分类的稳定性和速度。


<details>
  <summary>Details</summary>
Motivation: 当前的判别式视觉-语言模型（如CLIP）虽然在零样本图像分类等任务表现优秀，但存在词袋效应和伪相关偏差。一些工作利用生成式扩散模型进行通用图像分类（即Diffusion Classifier，DC），但这类方法对随机噪声非常敏感，需要大量噪声取平均，导致速度慢。该论文旨在解决扩散分类器中噪声选择带来的不稳定与低效率问题。

Method: 作者首先分析了不同噪声对扩散分类器性能的影响，提出“好噪声”需同时满足频域与空间匹配（Frequency Matching & Spatial Matching）两个原则。为此，设计了NoOp方法：1）频域匹配针对数据集和时间步优化全局参数化噪声；2）空间匹配通过元网络根据具体输入图像输出图像特定噪声偏置。最终使用优化后的噪声替换传统的随机噪声。

Result: 在多个数据集上的消融实验表明，用NoOp方式优化噪声后，扩散分类器的性能更加稳定，无需依赖大量噪声集成，提升了实验效率。

Conclusion: NoOp方法通过优化噪声有效缓解了扩散分类器中的噪声不稳定问题，实现了更快速、稳定的图像分类，为生成式方法在分类任务中的广泛应用提供了新思路。

Abstract: Although today's pretrained discriminative vision-language models (e.g.,
CLIP) have demonstrated strong perception abilities, such as zero-shot image
classification, they also suffer from the bag-of-words problem and spurious
bias. To mitigate these problems, some pioneering studies leverage powerful
generative models (e.g., pretrained diffusion models) to realize generalizable
image classification, dubbed Diffusion Classifier (DC). Specifically, by
randomly sampling a Gaussian noise, DC utilizes the differences of denoising
effects with different category conditions to classify categories.
Unfortunately, an inherent and notorious weakness of existing DCs is noise
instability: different random sampled noises lead to significant performance
changes. To achieve stable classification performance, existing DCs always
ensemble the results of hundreds of sampled noises, which significantly reduces
the classification speed. To this end, we firstly explore the role of noise in
DC, and conclude that: there are some ``good noises'' that can relieve the
instability. Meanwhile, we argue that these good noises should meet two
principles: Frequency Matching and Spatial Matching. Regarding both principles,
we propose a novel Noise Optimization method to learn matching (i.e., good)
noise for DCs: NoOp. For frequency matching, NoOp first optimizes a
dataset-specific noise: Given a dataset and a timestep t, optimize one randomly
initialized parameterized noise. For Spatial Matching, NoOp trains a
Meta-Network that adopts an image as input and outputs image-specific noise
offset. The sum of optimized noise and noise offset will be used in DC to
replace random noise. Extensive ablations on various datasets demonstrated the
effectiveness of NoOp.

</details>


### [57] [GANDiff FR: Hybrid GAN Diffusion Synthesis for Causal Bias Attribution in Face Recognition](https://arxiv.org/abs/2508.11334)
*Md Asgor Hossain Reaj,Rajan Das Gupta,Md Yeasin Rahat,Nafiz Fahad,Md Jawadul Hasan,Tze Hui Liew*

Main category: cs.CV

TL;DR: 作者提出GANDiff FR框架，通过结合StyleGAN3与扩散模型，实现对人脸生成样本中姿态、光照、表情等因素的精细控制，用以衡量和减少人脸识别模型的偏见。


<details>
  <summary>Details</summary>
Motivation: 人脸识别模型存在针对不同人口群体的性能偏差，准确测量并解释环境与人口属性因素对偏差的影响是公平性研究急需解决的问题；现有方法缺乏可复现和精细操控属性的生成框架。

Method: GANDiff FR结合StyleGAN3(高质量面貌生成)和扩散模型(属性精细控制)，可控制人脸姿态、光照与表情，并保持身份信息。构建了包括五个人口群体、共1万张、经过机器与人工验证的合成面孔数据，用于分别评估主流人脸识别模型(AdaFace等)在不同条件下的偏见情况。

Result: 在ArcFace、CosFace和AdaFace等模型上测试，发现AdaFace能将不同群体之间的TPR差异减少60%，而光照因素仍贡献约42%的剩余偏见。跨数据集实验显示，该方法合成数据与真实分布具备较高转移性（相关系数0.85）；计算开销增加约20%，但生成受控属性变体数量提升三倍。

Conclusion: GANDiff FR建立了一个可复现且符合欧盟AI监管标准的公平性评估方法，显著提高了人脸识别中的偏见可测量性与可解释性，并通过公布代码与数据促进了领域的透明与可拓展性。

Abstract: We introduce GANDiff FR, the first synthetic framework that precisely
controls demographic and environmental factors to measure, explain, and reduce
bias with reproducible rigor. GANDiff FR unifies StyleGAN3-based
identity-preserving generation with diffusion-based attribute control, enabling
fine-grained manipulation of pose around 30 degrees, illumination (four
directions), and expression (five levels) under ceteris paribus conditions. We
synthesize 10,000 demographically balanced faces across five cohorts validated
for realism via automated detection (98.2%) and human review (89%) to isolate
and quantify bias drivers. Benchmarking ArcFace, CosFace, and AdaFace under
matched operating points shows AdaFace reduces inter-group TPR disparity by 60%
(2.5% vs. 6.3%), with illumination accounting for 42% of residual bias.
Cross-dataset evaluation on RFW, BUPT, and CASIA WebFace confirms strong
synthetic-to-real transfer (r 0.85). Despite around 20% computational overhead
relative to pure GANs, GANDiff FR yields three times more attribute-conditioned
variants, establishing a reproducible, regulation-aligned (EU AI Act) standard
for fairness auditing. Code and data are released to support transparent,
scalable bias evaluation.

</details>


### [58] [Index-Aligned Query Distillation for Transformer-based Incremental Object Detection](https://arxiv.org/abs/2508.11339)
*Mingxiao Ma,Shunyao Zhu,Guoliang Kang*

Main category: cs.CV

TL;DR: 本文提出了一种针对transformer-based增量目标检测(IOD)的全新蒸馏方法Index-Aligned Query Distillation (IAQD)，有效缓解了旧类别知识遗忘问题，并取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 在增量目标检测(IOD)中，模型需不断学习新类别的同时不遗忘旧类别。然而，transformer检测器极易产生灾难性遗忘，现有的方法多依赖Hungarian Matching与蒸馏，但发现其效果有限甚至会加剧遗忘。作者希望找到更有效的知识保持策略。

Method: 作者提出了Index-Aligned Query Distillation (IAQD)方法。区别于传统的Hungarian Matching，IAQD直接对齐前后阶段相同索引的query进行知识蒸馏，并且只对与旧类别检测关键的部分query进行蒸馏，从而避免对新类别学习造成干扰。

Result: 在代表性基准数据集上，IAQD显著提升了模型在旧类别上的保持能力，并在整体检测性能上达到了新的最先进水平。

Conclusion: IAQD为transformer-based增量检测提供了有效的灾难性遗忘缓解方案，其简单高效，显著优于现有方法，且理论和实验均验证了其实用性与先进性。

Abstract: Incremental object detection (IOD) aims to continuously expand the capability
of a model to detect novel categories while preserving its performance on
previously learned ones. When adopting a transformer-based detection model to
perform IOD, catastrophic knowledge forgetting may inevitably occur, meaning
the detection performance on previously learned categories may severely
degenerate. Previous typical methods mainly rely on knowledge distillation (KD)
to mitigate the catastrophic knowledge forgetting of transformer-based
detection models. Specifically, they utilize Hungarian Matching to build a
correspondence between the queries of the last-phase and current-phase
detection models and align the classifier and regressor outputs between matched
queries to avoid knowledge forgetting. However, we observe that in IOD task,
Hungarian Matching is not a good choice. With Hungarian Matching, the query of
the current-phase model may match different queries of the last-phase model at
different iterations during KD. As a result, the knowledge encoded in each
query may be reshaped towards new categories, leading to the forgetting of
previously encoded knowledge of old categories. Based on our observations, we
propose a new distillation approach named Index-Aligned Query Distillation
(IAQD) for transformer-based IOD. Beyond using Hungarian Matching, IAQD
establishes a correspondence between queries of the previous and current phase
models that have the same index. Moreover, we perform index-aligned
distillation only on partial queries which are critical for the detection of
previous categories. In this way, IAQD largely preserves the previous semantic
and spatial encoding capabilities without interfering with the learning of new
categories. Extensive experiments on representative benchmarks demonstrate that
IAQD effectively mitigates knowledge forgetting, achieving new state-of-the-art
performance.

</details>


### [59] [Cost-Effective Active Labeling for Data-Efficient Cervical Cell Classification](https://arxiv.org/abs/2508.11340)
*Yuanlin Liu,Zhihan Zhou,Mingqiang Wei,Youyi Song*

Main category: cs.CV

TL;DR: 本文提出一种有效的主动标注方法，用较小的人力成本构建高代表性的宫颈细胞分类训练集，提升自动分类系统的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的自动宫颈细胞分类方法需要一个具代表性的训练集，但人工标注这些细胞图像既耗时又昂贵，限制了相关技术的应用；为此，亟需能够以更低人力成本获得有效训练集的方案。

Method: 提出了一种主动标注(active labeling)算法：通过快速估计分类器对未标注细胞图像的不确定性，从中选择最有价值的样本进行标注，从而用更少的标注数据构建高效的训练集。

Result: 实验结果显示，所提方法能有效提升训练数据集的代表性，在减少人工标注量的同时，保证或提升了分类性能，实现了数据与人工成本的优化配置。

Conclusion: 该算法显著降低了人工花费，为宫颈细胞高效分类提供了可行的新路径，推动了成本敏感场景下相关技术的实际应用。

Abstract: Information on the number and category of cervical cells is crucial for the
diagnosis of cervical cancer. However, existing classification methods capable
of automatically measuring this information require the training dataset to be
representative, which consumes an expensive or even unaffordable human cost. We
herein propose active labeling that enables us to construct a representative
training dataset using a much smaller human cost for data-efficient cervical
cell classification. This cost-effective method efficiently leverages the
classifier's uncertainty on the unlabeled cervical cell images to accurately
select images that are most beneficial to label. With a fast estimation of the
uncertainty, this new algorithm exhibits its validity and effectiveness in
enhancing the representative ability of the constructed training dataset. The
extensive empirical results confirm its efficacy again in navigating the usage
of human cost, opening the avenue for data-efficient cervical cell
classification.

</details>


### [60] [Semantically Guided Adversarial Testing of Vision Models Using Language Models](https://arxiv.org/abs/2508.11341)
*Katarzyna Filus,Jorge M. Cruz-Duarte*

Main category: cs.CV

TL;DR: 论文提出了一种基于语义引导的对抗攻击目标选择方法，利用预训练的语言和视觉-语言模型来选择目标类别，从而提升攻击效果和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型的对抗攻击中，目标标签的设定方式受限于随机性或静态资源，导致攻击的可解释性、复现性和灵活性较差。本文旨在利用更具语义性的方法选择对抗攻击目标，以增强攻击实验的科学性。

Method: 提出使用预训练的语言与视觉-语言模型（如BERT、TinyLLAMA 和 CLIP）衡量目标类别与实际类别的语义相似度，通过选择最相关和最无关的类别，构建最佳与最差的对抗场景，并在多个视觉模型和攻击方法上开展实验评估。

Result: 实验表明，利用预训练模型选择目标类别，能比传统的静态词典（如WordNet）方法取得更优效果，特别是在类别语义距离较远时。此外，对目标标签的静态测试可用于评价语义源的有效性。

Conclusion: 预训练语言和视觉-语言模型能够帮助建立更具可解释性、标准化和可扩展性的对抗攻击基准，为跨架构和多数据集的研究提供便利。

Abstract: In targeted adversarial attacks on vision models, the selection of the target
label is a critical yet often overlooked determinant of attack success. This
target label corresponds to the class that the attacker aims to force the model
to predict. Now, existing strategies typically rely on randomness, model
predictions, or static semantic resources, limiting interpretability,
reproducibility, or flexibility. This paper then proposes a semantics-guided
framework for adversarial target selection using the cross-modal knowledge
transfer from pretrained language and vision-language models. We evaluate
several state-of-the-art models (BERT, TinyLLAMA, and CLIP) as similarity
sources to select the most and least semantically related labels with respect
to the ground truth, forming best- and worst-case adversarial scenarios. Our
experiments on three vision models and five attack methods reveal that these
models consistently render practical adversarial targets and surpass static
lexical databases, such as WordNet, particularly for distant class
relationships. We also observe that static testing of target labels offers a
preliminary assessment of the effectiveness of similarity sources, \textit{a
priori} testing. Our results corroborate the suitability of pretrained models
for constructing interpretable, standardized, and scalable adversarial
benchmarks across architectures and datasets.

</details>


### [61] [HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model](https://arxiv.org/abs/2508.11350)
*Zhenhao Zhang,Hanqing Wang,Xiangyu Zeng,Ziyu Cheng,Jiaxin Liu,Haoyu Yan,Zhirui Liu,Kaiyang Ji,Tianxiang Gui,Ke Hu,Kangyi Chen,Yahao Fan,Mokai Pan*

Main category: cs.CV

TL;DR: 该论文提出了HOID-R1框架，将链式思考(CoT)引导的有监督微调与群体相对策略优化（GRPO）结合进强化学习流程，大幅提升了开放词表的人体—物体交互（HOI）识别，尤其在3D空间理解及泛化能力方面取得新突破。


<details>
  <summary>Details</summary>
Motivation: 当前开放词表 HOI 检测方法主要借助大语言模型丰富文本提示，但忽视了复杂 3D 空间理解能力的融合。因此，需要创新性的架构来提升模型对多模态及新场景下的泛化表现。

Method: 提出 HOID-R1 框架，首先使用链式思考（CoT）引导有监督微调训练模型，让模型在输出中展示推理过程；再采用群体相对策略优化（GRPO），结合多重奖励信号进行策略优化以提升多模态对齐；引入“多模态大模型评判”监督机制，约束 CoT 推理，缓解虚假推理现象并提升泛化能力。

Result: HOID-R1 在人体-物体交互标准测试集上取得了最优性能，尤其在新颖场景和开放世界任务下优于现有主流方法。

Conclusion: HOID-R1 在结合推理能力、强化学习优化和多模态监督方面展现出显著优势，为人体-物体交互的3D理解和开放场景应用提供了新的解决思路和强有力工具。

Abstract: Understanding and recognizing human-object interaction (HOI) is a pivotal
application in AR/VR and robotics. Recent open-vocabulary HOI detection
approaches depend exclusively on large language models for richer textual
prompts, neglecting their inherent 3D spatial understanding capabilities. To
address this shortcoming, we introduce HOID-R1, the first HOI detection
framework that integrates chain-of-thought (CoT) guided supervised fine-tuning
(SFT) with group relative policy optimization (GRPO) within a reinforcement
learning (RL) paradigm. Specifically, we initially apply SFT to imbue the model
with essential reasoning capabilities, forcing the model to articulate its
thought process in the output. Subsequently, we integrate GRPO to leverage
multi-reward signals for policy optimization, thereby enhancing alignment
across diverse modalities. To mitigate hallucinations in the CoT reasoning, we
introduce an "MLLM-as-a-judge" mechanism that supervises the CoT outputs,
further improving generalization. Extensive experiments show that HOID-R1
achieves state-of-the-art performance on HOI detection benchmarks and
outperforms existing methods in open-world generalization to novel scenarios.

</details>


### [62] [Leveraging the RETFound foundation model for optic disc segmentation in retinal images](https://arxiv.org/abs/2508.11354)
*Zhenyi Zhao,Muthu Rama Krishnan Mookiah,Emanuele Trucco*

Main category: cs.CV

TL;DR: 本文首次将RETFound基础模型应用于视盘分割任务，并在多个数据集上取得了超越主流分割网络的表现。即便只用少量任务特定样本训练头部网络，也能获得约96%的Dice分数，显示其强大泛化和适应能力。


<details>
  <summary>Details</summary>
Motivation: 虽然RETFound基础模型在视网膜影像的多种疾病诊断中表现优异，但尚未在分割等其他基础任务上被探索。视盘分割作为视网膜分析的基础性任务，对于研究验证基础模型迁移能力及应用范围有重要意义。

Method: 将预训练的RETFound基础模型适配用于视盘分割任务，仅需用很少的特定分割训练样本对其进行微调。通过在四个公开数据集（IDRID, Drishti-GS, RIM-ONE-r3, REFUGE）和一个私有数据集（GoDARTS）上进行测试，比较与现有主流分割方法的性能。

Result: 本方法在所有五个数据集上平均Dice系数约为96%，在域内验证、跨域泛化及域自适应三方面均表现优异，显著优于多数当前最先进分割模型。

Conclusion: RETFound基础模型不仅适合用于疾病诊断，还能出色地迁移到如视盘分割等任务，提出了基础模型有望替代专用结构的可能，展示了其广泛临床应用潜力。

Abstract: RETFound is a well-known foundation model (FM) developed for fundus camera
and optical coherence tomography images. It has shown promising performance
across multiple datasets in diagnosing diseases, both eye-specific and
systemic, from retinal images. However, to our best knowledge, it has not been
used for other tasks. We present the first adaptation of RETFound for optic
disc segmentation, a ubiquitous and foundational task in retinal image
analysis. The resulting segmentation system outperforms state-of-the-art,
segmentation-specific baseline networks after training a head with only a very
modest number of task-specific examples. We report and discuss results with
four public datasets, IDRID, Drishti-GS, RIM-ONE-r3, and REFUGE, and a private
dataset, GoDARTS, achieving about 96% Dice consistently across all datasets.
Overall, our method obtains excellent performance in internal verification,
domain generalization and domain adaptation, and exceeds most of the
state-of-the-art baseline results. We discuss the results in the framework of
the debate about FMs as alternatives to task-specific architectures. The code
is available at: [link to be added after the paper is accepted]

</details>


### [63] [Does the Skeleton-Recall Loss Really Work?](https://arxiv.org/abs/2508.11374)
*Devansh Arora,Nitin Kumar,Sukrit Gupta*

Main category: cs.CV

TL;DR: 本文对针对细管状结构分割的SRL（Skeleton Recall Loss）模型进行了理论和实证分析，发现其表现未超过传统模型，并揭示了基于拓扑保持损失函数的局限性。


<details>
  <summary>Details</summary>
Motivation: 在图像分割尤其是细小管状结构分割任务中，已有诸多专用架构与损失设计，SRL等拓扑保持损失近年来被认为能显著提升分割表现，但其实效性仍有待验证。

Method: 作者对SRL损失的梯度进行了理论分析，并在原文及新增的管状结构数据集上，系统比较了SRL方法和传统基线模型的分割效果。

Result: 实验证明，SRL及相关拓扑损失分割模型在实际性能上并不优于传统基线模型。

Conclusion: 文章指出了拓扑保持损失函数的理论和实证局限，为开发更有效的复杂管状结构分割模型提供了关键参考和反思。

Abstract: Image segmentation is an important and widely performed task in computer
vision. Accomplishing effective image segmentation in diverse settings often
requires custom model architectures and loss functions. A set of models that
specialize in segmenting thin tubular structures are topology
preservation-based loss functions. These models often utilize a pixel
skeletonization process claimed to generate more precise segmentation masks of
thin tubes and better capture the structures that other models often miss. One
such model, Skeleton Recall Loss (SRL) proposed by Kirchhoff et al.~\cite
{kirchhoff2024srl}, was stated to produce state-of-the-art results on benchmark
tubular datasets. In this work, we performed a theoretical analysis of the
gradients for the SRL loss. Upon comparing the performance of the proposed
method on some of the tubular datasets (used in the original work, along with
some additional datasets), we found that the performance of SRL-based
segmentation models did not exceed traditional baseline models. By providing
both a theoretical explanation and empirical evidence, this work critically
evaluates the limitations of topology-based loss functions, offering valuable
insights for researchers aiming to develop more effective segmentation models
for complex tubular structures.

</details>


### [64] [Unified Knowledge Distillation Framework: Fine-Grained Alignment and Geometric Relationship Preservation for Deep Face Recognition](https://arxiv.org/abs/2508.11376)
*Durgesh Mishra,Rishabh Uikey*

Main category: cs.CV

TL;DR: 本文提出了一种新的知识蒸馏方法，通过联合实例级嵌入蒸馏和关系型对相似度蒸馏，优化了面部识别模型在边缘设备等算力受限场景的部署表现，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的知识蒸馏方法往往无法兼顾实例细节和复杂样本间关系，导致模型精度受限。因此，迫切需要设计更细致和全面的信息传递机制，提高学生模型在受限环境下的识别能力。

Method: 作者提出了两个创新的损失函数：实例级嵌入蒸馏（利用动态难例挖掘提升困难样本的嵌入对齐）和关系型对相似度蒸馏（通过内存库和样本挖掘策略捕捉并传递样本间的相对信息），并将二者统一整合为完整蒸馏框架。

Result: 在多个主流人脸识别数据集上，该方法显著优于现有蒸馏方法。值得注意的是，在教师模型力量远强于学生模型的情况下，学生模型甚至能反超教师模型的准确率。

Conclusion: 本文的统一蒸馏方法兼顾实例细粒度特征和全局样本关系，提升了边缘设备下模型部署效果，并为知识蒸馏研究提供了新思路。

Abstract: Knowledge Distillation is crucial for optimizing face recognition models for
deployment in computationally limited settings, such as edge devices.
Traditional KD methods, such as Raw L2 Feature Distillation or Feature
Consistency loss, often fail to capture both fine-grained instance-level
details and complex relational structures, leading to suboptimal performance.
We propose a unified approach that integrates two novel loss functions,
Instance-Level Embedding Distillation and Relation-Based Pairwise Similarity
Distillation. Instance-Level Embedding Distillation focuses on aligning
individual feature embeddings by leveraging a dynamic hard mining strategy,
thereby enhancing learning from challenging examples. Relation-Based Pairwise
Similarity Distillation captures relational information through pairwise
similarity relationships, employing a memory bank mechanism and a sample mining
strategy. This unified framework ensures both effective instance-level
alignment and preservation of geometric relationships between samples, leading
to a more comprehensive distillation process. Our unified framework outperforms
state-of-the-art distillation methods across multiple benchmark face
recognition datasets, as demonstrated by extensive experimental evaluations.
Interestingly, when using strong teacher networks compared to the student, our
unified KD enables the student to even surpass the teacher's accuracy.

</details>


### [65] [G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration](https://arxiv.org/abs/2508.11379)
*Ramil Khafizov,Artem Komarichev,Ruslan Rakhimov,Peter Wonka,Evgeny Burnaev*

Main category: cs.CV

TL;DR: G-CUT3R是一种新的前馈式3D场景重建方法，通过融合额外先验信息（如深度、相机参数）提升重建效果，并在多个多视图任务基准上取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有的前馈式3D重建方法多仅依赖输入图像，未能充分利用现实中常见的辅助信息（如深度图、相机校准数据等），限制了重建质量。

Method: G-CUT3R对CUT3R模型进行了轻量级改造，每种输入模态（RGB、深度、相机信息等）配置独立编码器，提取特征后通过零卷积与RGB图像特征融合，实现对多种先验信息的灵活集成和推理。

Result: 在多个3D重建和多视图任务基准测试中，G-CUT3R显著提升了重建效果，表现出能够有效利用不同类型先验信息的能力，并兼容多种输入组合。

Conclusion: G-CUT3R展示了利用多模态先验信息提升3D重建性能的有效性和灵活性，为实际场景中集成多源数据提供了可行方案。

Abstract: We introduce G-CUT3R, a novel feed-forward approach for guided 3D scene
reconstruction that enhances the CUT3R model by integrating prior information.
Unlike existing feed-forward methods that rely solely on input images, our
method leverages auxiliary data, such as depth, camera calibrations, or camera
positions, commonly available in real-world scenarios. We propose a lightweight
modification to CUT3R, incorporating a dedicated encoder for each modality to
extract features, which are fused with RGB image tokens via zero convolution.
This flexible design enables seamless integration of any combination of prior
information during inference. Evaluated across multiple benchmarks, including
3D reconstruction and other multi-view tasks, our approach demonstrates
significant performance improvements, showing its ability to effectively
utilize available priors while maintaining compatibility with varying input
modalities.

</details>


### [66] [RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator](https://arxiv.org/abs/2508.11409)
*Zhiming Liu,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: 该论文提出了一种名为RMFAT的高效大气湍流视频修复方法，在保持实时性能的同时，提升了视频清晰度与时序一致性。


<details>
  <summary>Details</summary>
Motivation: 现有大气湍流视频修复方法多依赖复杂的transformer与3D网络架构，需多帧输入，计算与内存开销大，难以满足资源受限设备上的实时需求。作者需要解决在有限计算资源下提升视频清晰度与时序一致性的问题。

Method: 提出RMFAT（递归多尺度特征大气湍流消除器），采用轻量级递归结构，每次只需两帧输入，大幅减少时序窗口和计算负担；并在编码器和解码器阶段结合多尺度特征及时序扭曲模块，以提升空间细节和时序连贯性。

Result: 在合成和真实大气湍流数据集上，RMFAT在SSIM指标上较现有方法提升近9%，且推理速度提升四倍以上，兼顾质量和效率。

Conclusion: RMFAT兼具高效性和时序一致性，显著提升大气湍流下的视频清晰度和推理速度，适用于实时大气湍流抑制场景，优于现有主流方法。

Abstract: Atmospheric turbulence severely degrades video quality by introducing
distortions such as geometric warping, blur, and temporal flickering, posing
significant challenges to both visual clarity and temporal consistency. Current
state-of-the-art methods are based on transformer and 3D architectures and
require multi-frame input, but their large computational cost and memory usage
limit real-time deployment, especially in resource-constrained scenarios. In
this work, we propose RMFAT: Recurrent Multi-scale Feature Atmospheric
Turbulence Mitigator, designed for efficient and temporally consistent video
restoration under AT conditions. RMFAT adopts a lightweight recurrent framework
that restores each frame using only two inputs at a time, significantly
reducing temporal window size and computational burden. It further integrates
multi-scale feature encoding and decoding with temporal warping modules at both
encoder and decoder stages to enhance spatial detail and temporal coherence.
Extensive experiments on synthetic and real-world atmospheric turbulence
datasets demonstrate that RMFAT not only outperforms existing methods in terms
of clarity restoration (with nearly a 9\% improvement in SSIM) but also
achieves significantly improved inference speed (more than a fourfold reduction
in runtime), making it particularly suitable for real-time atmospheric
turbulence suppression tasks.

</details>


### [67] [SelfAdapt: Unsupervised Domain Adaptation of Cell Segmentation Models](https://arxiv.org/abs/2508.11411)
*Fabian H. Reith,Jannik Franzen,Dinesh R. Palli,J. Lorenz Rumberger,Dagmar Kainmueller*

Main category: cs.CV

TL;DR: 本文提出了一种名为SelfAdapt的新方法，无需标注数据即可实现预训练细胞分割模型的自适应，有效提升了模型在新领域的表现。


<details>
  <summary>Details</summary>
Motivation: 当前通用型细胞分割模型（如Cellpose）在未见过的数据领域上表现下降，并且有监督微调用于提升表现需要大量标注数据，这在实际中常常难以获得。急需一种无监督的模型自适应方法，使其在新领域也能有良好性能。

Method: 本文提出SelfAdapt方法，基于student-teacher一致性训练框架，结合L2-SP正则化及免标签的训练停止准则，使已训练好的细胞分割模型能在目标域无标签数据下自适应。

Result: 在LiveCell和TissueNet两个数据集上，SelfAdapt相比于基础Cellpose模型的AP0.5性能提升可达29.64%。此外，该方法还能进一步提升已有监督微调模型的表现。

Conclusion: SelfAdapt是一种无需标注数据也能显著提升细胞分割模型跨域表现的实用工具。作者已将其作为Cellpose的扩展开源，便于实际应用。

Abstract: Deep neural networks have become the go-to method for biomedical instance
segmentation. Generalist models like Cellpose demonstrate state-of-the-art
performance across diverse cellular data, though their effectiveness often
degrades on domains that differ from their training data. While supervised
fine-tuning can address this limitation, it requires annotated data that may
not be readily available. We propose SelfAdapt, a method that enables the
adaptation of pre-trained cell segmentation models without the need for labels.
Our approach builds upon student-teacher augmentation consistency training,
introducing L2-SP regularization and label-free stopping criteria. We evaluate
our method on the LiveCell and TissueNet datasets, demonstrating relative
improvements in AP0.5 of up to 29.64% over baseline Cellpose. Additionally, we
show that our unsupervised adaptation can further improve models that were
previously fine-tuned with supervision. We release SelfAdapt as an easy-to-use
extension of the Cellpose framework. The code for our method is publicly
available at https: //github.com/Kainmueller-Lab/self_adapt.

</details>


### [68] [Training-free Dimensionality Reduction via Feature Truncation: Enhancing Efficiency in Privacy-preserving Multi-Biometric Systems](https://arxiv.org/abs/2508.11419)
*Florian Bayer,Maximilian Russo,Christian Rathgeb*

Main category: cs.CV

TL;DR: 该论文研究了如何在保持生物特征识别精度不降低的前提下，通过多模态特征融合与降维方法，大幅减少加密后模板的计算和存储负担。最终实现了模板尺寸减少67%且EER不下降。


<details>
  <summary>Details</summary>
Motivation: 目前生物特征识别广泛应用，但保护其隐私和加密模板时的计算量过大，尤其是涉及同态加密技术。迫切需要在保证安全与准确性的前提下，提升加密生物模板的处理效率。

Method: 通过深度神经网络从人脸、指纹和虹膜等多模态数据中提取特征，利用融合和降维技术生成多模态生物模板并在同态加密环境下进行测试。采用公开数据库（FRGC, MCYT, CASIA）及自建虚拟多模态数据库进行对比实验。方法注重可解释性、易实现、无需训练且具备良好泛化性。

Result: 多模态特征融合和降维后，模板尺寸可减少67%，同时在同态加密域下保持甚至提升安全性与识别性能（EER无下降），超过单一生物识别方式。

Conclusion: 通过多模态融合与特征降维，可以显著提升加密生物模板的处理效率及安全性，为实际生物识别系统的可扩展实现奠定基础，同时不牺牲识别精度。

Abstract: Biometric recognition is widely used, making the privacy and security of
extracted templates a critical concern. Biometric Template Protection schemes,
especially those utilizing Homomorphic Encryption, introduce significant
computational challenges due to increased workload. Recent advances in deep
neural networks have enabled state-of-the-art feature extraction for face,
fingerprint, and iris modalities. The ubiquity and affordability of biometric
sensors further facilitate multi-modal fusion, which can enhance security by
combining features from different modalities. This work investigates the
biometric performance of reduced multi-biometric template sizes. Experiments
are conducted on an in-house virtual multi-biometric database, derived from
DNN-extracted features for face, fingerprint, and iris, using the FRGC, MCYT,
and CASIA databases. The evaluated approaches are (i) explainable and
straightforward to implement under encryption, (ii) training-free, and (iii)
capable of generalization. Dimensionality reduction of feature vectors leads to
fewer operations in the Homomorphic Encryption (HE) domain, enabling more
efficient encrypted processing while maintaining biometric accuracy and
security at a level equivalent to or exceeding single-biometric recognition.
Our results demonstrate that, by fusing feature vectors from multiple
modalities, template size can be reduced by 67 % with no loss in Equal Error
Rate (EER) compared to the best-performing single modality.

</details>


### [69] [ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving](https://arxiv.org/abs/2508.11428)
*Jingyu Li,Bozhou Zhang,Xin Jin,Jiankang Deng,Xiatian Zhu,Li Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为ImagiDrive的新型自动驾驶框架，将视觉-语言模型（VLM）与驾驶世界模型（DWM）集成，形成一个统一的想象与规划闭环。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要更好地理解复杂环境和进行精准的行为预测。尽管VLM和DWM各自表现优秀，但如何结合两者互补优势尚属研究空白，尤其是在高效衔接行为决策和高精度场景生成方面存在明显挑战。

Method: ImagiDrive框架由基于VLM的驾驶代理与基于DWM的场景生成器组成。首先，驾驶代理根据多模态输入预测初始路径，引导场景生成器模拟相应未来环境，然后利用这些模拟场景迭代优化驾驶规划。为提升效率和预测准确性，作者还提出了提前终止机制和轨迹选择策略。

Result: 在nuScenes和NAVSIM数据集上的大量实验表明，ImagiDrive在开放式与闭环条件下均优于现有方法，展现了更强的鲁棒性和性能。

Conclusion: 将VLM与DWM结合，可大幅提升自动驾驶在理解与预测方面的能力。ImagiDrive验证了这种集成策略的有效性，为实现更安全、智能的自动驾驶系统提供了新思路。

Abstract: Autonomous driving requires rich contextual comprehension and precise
predictive reasoning to navigate dynamic and complex environments safely.
Vision-Language Models (VLMs) and Driving World Models (DWMs) have
independently emerged as powerful recipes addressing different aspects of this
challenge. VLMs provide interpretability and robust action prediction through
their ability to understand multi-modal context, while DWMs excel in generating
detailed and plausible future driving scenarios essential for proactive
planning. Integrating VLMs with DWMs is an intuitive, promising, yet
understudied strategy to exploit the complementary strengths of accurate
behavioral prediction and realistic scene generation. Nevertheless, this
integration presents notable challenges, particularly in effectively connecting
action-level decisions with high-fidelity pixel-level predictions and
maintaining computational efficiency. In this paper, we propose ImagiDrive, a
novel end-to-end autonomous driving framework that integrates a VLM-based
driving agent with a DWM-based scene imaginer to form a unified
imagination-and-planning loop. The driving agent predicts initial driving
trajectories based on multi-modal inputs, guiding the scene imaginer to
generate corresponding future scenarios. These imagined scenarios are
subsequently utilized to iteratively refine the driving agent's planning
decisions. To address efficiency and predictive accuracy challenges inherent in
this integration, we introduce an early stopping mechanism and a trajectory
selection strategy. Extensive experimental validation on the nuScenes and
NAVSIM datasets demonstrates the robustness and superiority of ImagiDrive over
previous alternatives under both open-loop and closed-loop conditions.

</details>


### [70] [Remove360: Benchmarking Residuals After Object Removal in 3D Gaussian Splatting](https://arxiv.org/abs/2508.11431)
*Simona Kocour,Assia Benbihi,Torsten Sattler*

Main category: cs.CV

TL;DR: 本文提出了新的评测框架与数据集，用于分析3D对象移除后，场景中是否还残留有语义线索，并发现现有方法在消除这些语义残留方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 在隐私保护和可编辑的3D场景重建中，仅移除对象视觉几何往往不能完全消除其语义痕迹，导致敏感信息泄露。了解这些“语义残留”并进行量化评估，是提升对象移除与场景编辑安全性的重要难题。

Method: 研究提出了一套新的评测基准与评价体系，专门用于3D Gaussian Splatting对象移除后的语义残留分析；并构建了Remove360大规模真实场景数据集，包含对象移除前后的RGB图像及物体级掩膜。实验覆盖丰富的室内外场景，并利用真实删除前后图像，量化评估对象语义是否被彻底移除。

Result: 实验表明，当前主流3D对象移除方法在消除语义残留方面效果有限，即使几何外观已无，深度模型仍能推断出被移除物体。为下游应用埋下隐私和安全隐患。

Conclusion: 现有技术在彻底消除对象语义残留上存在不足，未来需研发更健壮的3D对象移除方法，并持续提升评估基准和覆盖复杂场景的能力。

Abstract: Understanding what semantic information persists after object removal is
critical for privacy-preserving 3D reconstruction and editable scene
representations. In this work, we introduce a novel benchmark and evaluation
framework to measure semantic residuals, the unintended semantic traces left
behind, after object removal in 3D Gaussian Splatting. We conduct experiments
across a diverse set of indoor and outdoor scenes, showing that current methods
can preserve semantic information despite the absence of visual geometry. We
also release Remove360, a dataset of pre/post-removal RGB images and
object-level masks captured in real-world environments. While prior datasets
have focused on isolated object instances, Remove360 covers a broader and more
complex range of indoor and outdoor scenes, enabling evaluation of object
removal in the context of full-scene representations. Given ground truth images
of a scene before and after object removal, we assess whether we can truly
eliminate semantic presence, and if downstream models can still infer what was
removed. Our findings reveal critical limitations in current 3D object removal
techniques and underscore the need for more robust solutions capable of
handling real-world complexity. The evaluation framework is available at
github.com/spatial-intelligence-ai/Remove360.git. Data are available at
huggingface.co/datasets/simkoc/Remove360.

</details>


### [71] [MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation](https://arxiv.org/abs/2508.11433)
*Qian Liang,Yujia Wu,Kuncheng Li,Jiwei Wei,Shiyuan He,Jinyu Guo,Ning Xie*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法MM-R1，用于提升多模态大语言模型（MLLMs）在个性化图像生成任务中的能力，显著减少了对单独数据密集型微调的需求。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在个性化图像生成时需为每个新主体进行大量微调，数据和计算资源消耗大，限制了模型的扩展性，因此需开发更加高效的泛化个性化生成方式。

Method: 作者提出了MM-R1框架，通过跨模态链式思维（Cross-modal Chain-of-Thought, X-CoT）推理策略，将视觉理解与图像生成整合，先解析用户图片与上下文信息，获取主体概念；再结合主体表征和用户提示词进行个性化图像生成。同时引入GRPO（Grouped Reward Proximal Policy Optimization）提升推理与生成对齐度。

Result: 实验结果表明，MM-R1能够在零样本（zero-shot）设定下，生成具备高主体一致性以及强文本对齐的个性化图像，有效释放了统一型MLLM的个性化生成潜力。

Conclusion: MM-R1框架无需针对每个新主体进行额外微调即可实现高质量个性化图像生成，兼顾了泛化性与个体定制能力，对多模态大模型的实际应用具有重要推进意义。

Abstract: Multimodal Large Language Models (MLLMs) with unified architectures excel
across a wide range of vision-language tasks, yet aligning them with
personalized image generation remains a significant challenge. Existing methods
for MLLMs are frequently subject-specific, demanding a data-intensive
fine-tuning process for every new subject, which limits their scalability. In
this paper, we introduce MM-R1, a framework that integrates a cross-modal
Chain-of-Thought (X-CoT) reasoning strategy to unlock the inherent potential of
unified MLLMs for personalized image generation. Specifically, we structure
personalization as an integrated visual reasoning and generation process: (1)
grounding subject concepts by interpreting and understanding user-provided
images and contextual cues, and (2) generating personalized images conditioned
on both the extracted subject representations and user prompts. To further
enhance the reasoning capability, we adopt Grouped Reward Proximal Policy
Optimization (GRPO) to explicitly align the generation. Experiments demonstrate
that MM-R1 unleashes the personalization capability of unified MLLMs to
generate images with high subject fidelity and strong text alignment in a
zero-shot manner.

</details>


### [72] [Inside Knowledge: Graph-based Path Generation with Explainable Data Augmentation and Curriculum Learning for Visual Indoor Navigation](https://arxiv.org/abs/2508.11446)
*Daniel Airinei,Elena Burceanu,Marius Leordeanu*

Main category: cs.CV

TL;DR: 本文提出了一种仅依赖视觉输入的实时高效室内导航深度学习方法，可预测前进方向，不需要地图、额外传感器或互联网，实用性强，且所有数据与代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有的室内导航方法受限于GPS信号不足且实现复杂，需要额外硬件或场景信息，很难部署到实际应用。本文旨在解决室内导航部署难、门槛高的问题。

Method: 提出了一种新的基于视觉输入的深度学习室内导航方法，使用了新颖的图生成算法，并结合可解释性数据增强和课程学习，大幅简化了数据采集、标注和训练过程。同时构建了大型购物中心视频数据集，应用于训练和测试，并开发了Android应用。

Result: 实验表明，该方法不依赖特殊传感器、场景地图或网络即可准确预测前进方向，数据采集和训练过程高效自动化，展示了出色的实用性和鲁棒性。所有数据与代码已开放。

Conclusion: 该方法为无需外部辅助的室内视觉导航提供了新方向，具有易部署、实时、高效等优势，推动了室内导航的实际应用落地，对提升大众用户体验有重要意义。

Abstract: Indoor navigation is a difficult task, as it generally comes with poor GPS
access, forcing solutions to rely on other sources of information. While
significant progress continues to be made in this area, deployment to
production applications is still lacking, given the complexity and additional
requirements of current solutions. Here, we introduce an efficient, real-time
and easily deployable deep learning approach, based on visual input only, that
can predict the direction towards a target from images captured by a mobile
device. Our technical approach, based on a novel graph-based path generation
method, combined with explainable data augmentation and curriculum learning,
includes contributions that make the process of data collection, annotation and
training, as automatic as possible, efficient and robust. On the practical
side, we introduce a novel largescale dataset, with video footage inside a
relatively large shopping mall, in which each frame is annotated with the
correct next direction towards different specific target destinations.
Different from current methods, ours relies solely on vision, avoiding the need
of special sensors, additional markers placed along the path, knowledge of the
scene map or internet access. We also created an easy to use application for
Android, which we plan to make publicly available. We make all our data and
code available along with visual demos on our project site

</details>


### [73] [Data-Driven Deepfake Image Detection Method -- The 2024 Global Deepfake Image Detection Challenge](https://arxiv.org/abs/2508.11464)
*Xiaoya Zhu,Yibing Nan,Shiguo Lian*

Main category: cs.CV

TL;DR: 本文提出了一种基于Swin Transformer V2-B分类网络的方法，用于检测深度伪造（Deepfake）人脸图像，并取得了优异的检测效果。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的飞速发展，深度伪造技术带来了数字安全方面的巨大挑战。鉴别Deepfake图像对于保护数字内容的真实性具有重要意义。

Method: 采用Swin Transformer V2-B分类网络作为检测模型，并结合线上数据增强与离线样本生成手段，提升训练样本多样性和模型泛化能力。

Result: 在Deepfake图像检测竞赛中取得了卓越表现并获得优异奖。

Conclusion: 所提出的基于Swin Transformer V2-B的检测方法有效提升了Deepfake人脸图像检测的准确性和鲁棒性，为数字安全提供了有效的技术支撑。

Abstract: With the rapid development of technology in the field of AI, deepfake
technology has emerged as a double-edged sword. It has not only created a large
amount of AI-generated content but also posed unprecedented challenges to
digital security. The task of the competition is to determine whether a face
image is a Deepfake image and output its probability score of being a Deepfake
image. In the image track competition, our approach is based on the Swin
Transformer V2-B classification network. And online data augmentation and
offline sample generation methods are employed to enrich the diversity of
training samples and increase the generalization ability of the model. Finally,
we got the award of excellence in Deepfake image detection.

</details>


### [74] [CoFi: A Fast Coarse-to-Fine Few-Shot Pipeline for Glomerular Basement Membrane Segmentation](https://arxiv.org/abs/2508.11469)
*Hongjin Fang,Daniel Reisenbüchler,Kenji Ikemura,Mert R. Sabuncu,Yihe Yang,Ruining Deng*

Main category: cs.CV

TL;DR: 本文提出了一种名为CoFi的分层次少样本分割方法，能高效准确地对肾小球基底膜（GBM）的电镜图像进行分割，仅需极少标注数据即可实现优异表现。


<details>
  <summary>Details</summary>
Motivation: 精准分割GBM对肾脏疾病诊断极为重要，但现有深度学习方法依赖大量像素级标注，增加了临床应用难度，急需降低标注需求且不影响分割精度的新方法。

Method: CoFi方法采用一个轻量神经网络，在仅有三张有标注图像的情况下进行初步粗分割。粗分割结果再经自动处理，生成高质量、基于结构的点提示，这些提示指导SAM进一步优化，获得精细分割结果。

Result: 本方法在GBM分割任务上取得了出色效果，Dice系数达到74.54％，推理速度达1.9FPS。在减少标注和计算需求的同时，保持了高分割精度与可靠性。

Conclusion: CoFi不仅能有效减轻传统方法的标注和计算负担，还能实现准确可靠的分割结果，具有较强的临床应用潜力，并已开放源码以促进研究和应用。

Abstract: Accurate segmentation of the glomerular basement membrane (GBM) in electron
microscopy (EM) images is fundamental for quantifying membrane thickness and
supporting the diagnosis of various kidney diseases. While supervised deep
learning approaches achieve high segmentation accuracy, their reliance on
extensive pixel-level annotation renders them impractical for clinical
workflows. Few-shot learning can reduce this annotation burden but often
struggles to capture the fine structural details necessary for GBM analysis. In
this study, we introduce CoFi, a fast and efficient coarse-to-fine few-shot
segmentation pipeline designed for GBM delineation in EM images. CoFi first
trains a lightweight neural network using only three annotated images to
produce an initial coarse segmentation mask. This mask is then automatically
processed to generate high-quality point prompts with morphology-aware pruning,
which are subsequently used to guide SAM in refining the segmentation. The
proposed method achieved exceptional GBM segmentation performance, with a Dice
coefficient of 74.54% and an inference speed of 1.9 FPS. We demonstrate that
CoFi not only alleviates the annotation and computational burdens associated
with conventional methods, but also achieves accurate and reliable segmentation
results. The pipeline's speed and annotation efficiency make it well-suited for
research and hold strong potential for clinical applications in renal
pathology. The pipeline is publicly available at:
https://github.com/ddrrnn123/CoFi.

</details>


### [75] [TACR-YOLO: A Real-time Detection Framework for Abnormal Human Behaviors Enhanced with Coordinate and Task-Aware Representations](https://arxiv.org/abs/2508.11478)
*Xinyi Yin,Wenbo Yuan,Xuecheng Wu,Liangyu Fu,Danlei Huang*

Main category: cs.CV

TL;DR: 本文提出TACR-YOLO模型，针对特殊场景下异常人类行为检测难题，增强了小目标检测、分类回归冲突处理以及多尺度融合，取得了优异的实时检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有YOLO类方法在异常行为检测中虽然实时性强，但在小目标检测、任务冲突、多尺度融合等方面表现有限，影响了其在复杂场景下的应用效果。

Method: 提出TACR-YOLO框架，引入坐标注意力模块提升小目标检测，任务感知注意力模块缓解分类与回归冲突，加强的Neck网络实现更精细多尺度特征融合；此外，采用K-means聚类优化锚框尺寸，引入DIoU-Loss提升回归性能，并构建新的PABD数据集用于训练和评估。

Result: TACR-YOLO在PABD数据集上取得了91.92%的mAP，同时具备较高的检测速度和鲁棒性；消融实验验证了各模块的有效性。

Conclusion: TACR-YOLO有效提升了特殊场景下异常行为检测的效率和准确性，为该领域提供了新的研究思路和工具。

Abstract: Abnormal Human Behavior Detection (AHBD) under special scenarios is becoming
increasingly crucial. While YOLO-based detection methods excel in real-time
tasks, they remain hindered by challenges including small objects, task
conflicts, and multi-scale fusion in AHBD. To tackle them, we propose
TACR-YOLO, a new real-time framework for AHBD. We introduce a Coordinate
Attention Module to enhance small object detection, a Task-Aware Attention
Module to deal with classification-regression conflicts, and a Strengthen Neck
Network for refined multi-scale fusion, respectively. In addition, we optimize
Anchor Box sizes using K-means clustering and deploy DIoU-Loss to improve
bounding box regression. The Personnel Anomalous Behavior Detection (PABD)
dataset, which includes 8,529 samples across four behavior categories, is also
presented. Extensive experimental results indicate that TACR-YOLO achieves
91.92% mAP on PABD, with competitive speed and robustness. Ablation studies
highlight the contribution of each improvement. This work provides new insights
for abnormal behavior detection under special scenarios, advancing its
progress.

</details>


### [76] [OpenConstruction: A Systematic Synthesis of Open Visual Datasets for Data-Centric Artificial Intelligence in Construction Monitoring](https://arxiv.org/abs/2508.11482)
*Ruoxin Xiong,Yanyu Wang,Jiannan Cai,Kaijian Liu,Yuansheng Zhu,Pingbo Tang,Nora El-Gohary*

Main category: cs.CV

TL;DR: 本研究系统性回顾了2005-2024年间公开可用的建筑行业视觉数据集，梳理其基本特征、数据类型、标注方式和应用领域，并发布了开源目录资源OpenConstruction，旨在推动建筑行业AI和ML应用数据基础建设。


<details>
  <summary>Details</summary>
Motivation: 当前建筑行业对视觉数据辅助AI/ML应用的需求日益增长，但现有数据集质量参差不齐，缺乏系统性梳理，影响了该领域方法与应用的进一步发展与创新。

Method: 研究者对学术数据库及开放数据平台进行了系统检索，遴选出51个2005-2024年期间公开的建筑视觉数据集，并基于数据规模、授权类型、数据模态、标注方法和应用领域等构建分层数据框架进行归类和分析。

Result: 收集并详细分类了51个公共视觉数据集，形成结构化开源目录OpenConstruction。同时，系统总结了现有数据集存在的规模、注释质量和现实代表性等主要不足。

Conclusion: 研究不仅提供了清晰系统的数据集全貌，还基于FAIR原则提出了未来建筑行业数据基础设施建设的建议，为数据驱动的AI/ML方法开发和实际应用提供依据和方向。

Abstract: The construction industry increasingly relies on visual data to support
Artificial Intelligence (AI) and Machine Learning (ML) applications for site
monitoring. High-quality, domain-specific datasets, comprising images, videos,
and point clouds, capture site geometry and spatiotemporal dynamics, including
the location and interaction of objects, workers, and materials. However,
despite growing interest in leveraging visual datasets, existing resources vary
widely in sizes, data modalities, annotation quality, and representativeness of
real-world construction conditions. A systematic review to categorize their
data characteristics and application contexts is still lacking, limiting the
community's ability to fully understand the dataset landscape, identify
critical gaps, and guide future directions toward more effective, reliable, and
scalable AI applications in construction. To address this gap, this study
conducts an extensive search of academic databases and open-data platforms,
yielding 51 publicly available visual datasets that span the 2005-2024 period.
These datasets are categorized using a structured data schema covering (i) data
fundamentals (e.g., size and license), (ii) data modalities (e.g., RGB and
point cloud), (iii) annotation frameworks (e.g., bounding boxes), and (iv)
downstream application domains (e.g., progress tracking). This study
synthesizes these findings into an open-source catalog, OpenConstruction,
supporting data-driven method development. Furthermore, the study discusses
several critical limitations in the existing construction dataset landscape and
presents a roadmap for future data infrastructure anchored in the Findability,
Accessibility, Interoperability, and Reusability (FAIR) principles. By
reviewing the current landscape and outlining strategic priorities, this study
supports the advancement of data-centric solutions in the construction sector.

</details>


### [77] [CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models](https://arxiv.org/abs/2508.11484)
*Xiaoxue Wu,Bingjie Gao,Yu Qiao,Yaohui Wang,Xinyuan Chen*

Main category: cs.CV

TL;DR: 该论文提出了CineTrans框架，实现了具有电影风格镜头切换的多镜头视频生成，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成方法主要局限于单一镜头，缺乏对复杂多镜头、连贯镜头切换的研究，而电影制片通常包含丰富的镜头切换和编辑风格。作者希望实现更接近真实电影的多镜头视频生成。

Method: 1）提出CineTrans框架，结合掩膜控制机制，实现任意位置的镜头切换。2）构建了包含详尽镜头注释的多镜头视频-文本数据集Cine250K。3）分析现有扩散模型注意力图与镜头边界之间的关系，并利用该关系设计掩膜控制以进行无训练镜头切换。4）在自有数据集上进行微调。5）提出针对镜头切换、时序一致性和总体质量的评测指标。

Result: CineTrans可生成符合电影编辑风格的多镜头连贯视频，避免了不稳定切换或生硬拼接，并且在镜头切换控制、时序一致性、视频质量等多项指标上，全面优于现有方法。

Conclusion: 针对多镜头视频生成难题，CineTrans首次在大规模数据和机制设计双重创新下，成功生成高质量、风格一致的多镜头视频，为视频生成领域带来显著进步。

Abstract: Despite significant advances in video synthesis, research into multi-shot
video generation remains in its infancy. Even with scaled-up models and massive
datasets, the shot transition capabilities remain rudimentary and unstable,
largely confining generated videos to single-shot sequences. In this work, we
introduce CineTrans, a novel framework for generating coherent multi-shot
videos with cinematic, film-style transitions. To facilitate insights into the
film editing style, we construct a multi-shot video-text dataset Cine250K with
detailed shot annotations. Furthermore, our analysis of existing video
diffusion models uncovers a correspondence between attention maps in the
diffusion model and shot boundaries, which we leverage to design a mask-based
control mechanism that enables transitions at arbitrary positions and transfers
effectively in a training-free setting. After fine-tuning on our dataset with
the mask mechanism, CineTrans produces cinematic multi-shot sequences while
adhering to the film editing style, avoiding unstable transitions or naive
concatenations. Finally, we propose specialized evaluation metrics for
transition control, temporal consistency and overall quality, and demonstrate
through extensive experiments that CineTrans significantly outperforms existing
baselines across all criteria.

</details>


### [78] [Automated Building Heritage Assessment Using Street-Level Imagery](https://arxiv.org/abs/2508.11486)
*Kristina Dabrock,Tim Johansson,Anna Donarelli,Mikael Mangold,Noah Pflugradt,Jann Michael Weinand,Jochen Linßen*

Main category: cs.CV

TL;DR: 本研究利用GPT大语言模型分析建筑外立面图片和登记数据，通过机器学习模型对斯德哥尔摩的多户及非住宅建筑进行分类，旨在在节能改造同时保护文化遗产。结果显示，将GPT分析结果与登记数据结合，分类表现优于单用GPT数据，验证了新方法在实际中的有效性。


<details>
  <summary>Details</summary>
Motivation: 对建筑进行节能改造时，保护其文化遗产价值极为重要，而传统的遗产价值判定方法昂贵且耗时。研究旨在探索是否可用AI工具高效识别和量化建筑遗产价值，提升数据质量并为节能与保护并重的改造决策提供支持。

Method: 作者利用GPT模型从建筑外立面图片中提取文化遗产相关特征，再结合建筑登记数据，训练机器学习模型对建筑进行分类。实验对象为瑞典斯德哥尔摩的多户和非住宅建筑，分类结果与人工专家创建的清单进行比对，评估方法准确性。

Result: 结合登记数据与GPT特征的模型在对建筑遗产价值分类时，宏观F1分数达到0.71，仅用GPT特征时为0.60，体现了多源数据融合的优势。

Conclusion: 研究证实，利用AI和大语言模型辅助可提升建筑遗产信息提取效率和分类准确度，为未来大规模节能改造中系统性地纳入文化遗产保护因素提供了方法支持。

Abstract: Detailed data is required to quantify energy conservation measures in
buildings, such as envelop retrofits, without compromising cultural heritage.
Novel artificial intelligence tools may improve efficiency in identifying
heritage values in buildings compared to costly and time-consuming traditional
inventories. In this study, the large language model GPT was used to detect
various aspects of cultural heritage value in fa\c{c}ade images. Using this
data and building register data as features, machine learning models were
trained to classify multi-family and non-residential buildings in Stockholm,
Sweden. Validation against an expert-created inventory shows a macro F1-score
of 0.71 using a combination of register data and features retrieved from GPT,
and a score of 0.60 using only GPT-derived data. The presented methodology can
contribute to a higher-quality database and thus support careful energy
efficiency measures and integrated consideration of heritage value in
large-scale energetic refurbishment scenarios.

</details>


### [79] [Perception in Plan: Coupled Perception and Planning for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.11488)
*Bozhou Zhang,Jingyu Li,Nan Song,Li Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种新范式的端到端自动驾驶框架VeteranAD，将感知模块与规划过程紧密结合，通过针对性感知提升自动驾驶的决策和规划表现，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了突破传统感知-规划串行流程的瓶颈，作者希望通过动态感知与规划协作，提升自动驾驶中对关键交通要素的感知和更优路径规划能力，从而提高自动驾驶系统准确性和可靠性。

Method: 提出了Perception-in-Plan框架，在每一步规划时，根据多模式规划先验轨迹（multi-mode anchored trajectories），感知模块主动感知相关交通元素。感知与规划采用自回归策略逐步预测未来轨迹，每步只关注相关区域，实现针对性和充分的信息提取。

Result: 在NAVSIM和Bench2Drive两个公开数据集上进行大量实验，VeteranAD在感知与规划准确性方面均取得了当前最优（state-of-the-art）效果。

Conclusion: 将感知与规划深度耦合能充分发挥端到端自动驾驶方法的潜力，VeteranAD框架具备结构简单但效果出众的特点，有望为自动驾驶领域提供更可靠和高效的解决方案。

Abstract: End-to-end autonomous driving has achieved remarkable advancements in recent
years. Existing methods primarily follow a perception-planning paradigm, where
perception and planning are executed sequentially within a fully differentiable
framework for planning-oriented optimization. We further advance this paradigm
through a perception-in-plan framework design, which integrates perception into
the planning process. This design facilitates targeted perception guided by
evolving planning objectives over time, ultimately enhancing planning
performance. Building on this insight, we introduce VeteranAD, a coupled
perception and planning framework for end-to-end autonomous driving. By
incorporating multi-mode anchored trajectories as planning priors, the
perception module is specifically designed to gather traffic elements along
these trajectories, enabling comprehensive and targeted perception. Planning
trajectories are then generated based on both the perception results and the
planning priors. To make perception fully serve planning, we adopt an
autoregressive strategy that progressively predicts future trajectories while
focusing on relevant regions for targeted perception at each step. With this
simple yet effective design, VeteranAD fully unleashes the potential of
planning-oriented end-to-end methods, leading to more accurate and reliable
driving behavior. Extensive experiments on the NAVSIM and Bench2Drive datasets
demonstrate that our VeteranAD achieves state-of-the-art performance.

</details>


### [80] [Hierarchical Graph Feature Enhancement with Adaptive Frequency Modulation for Visual Recognition](https://arxiv.org/abs/2508.11497)
*Feiyue Zhao,Zhichao Zhang*

Main category: cs.CV

TL;DR: 提出了一种将图神经网络机制融合到CNN中的轻量级方法HGFE，有效提升了结构和特征建模能力，显著改善了分类、检测和分割任务的表现。


<details>
  <summary>Details</summary>
Motivation: 传统CNN只能处理规则网格结构，难以建模复杂的拓扑关系和非局部语义，限制了其在空间结构丰富的视觉任务中的表现。

Method: HGFE框架通过引入两层图结构：窗口内图卷积捕捉局部空间依赖，窗口间超级节点建模全局语义；再结合自适应频率调制模块以保留边缘和纹理信息，防止过度平滑。该模块轻量且可端到端训练，易于集成到主流CNN中。

Result: 在CIFAR-100（分类）、PASCAL VOC和VisDrone（检测）、CrackSeg和CarParts（分割）等多个任务上大量实验，表明HGFE显著增强了结构表征并提升了整体识别表现。

Conclusion: HGFE能够提升CNNs的结构感知能力，为视觉识别任务提供更优的特征表达和性能，是一种高效实用的架构改进方案。

Abstract: Convolutional neural networks (CNNs) have
  demonstrated strong performance in visual recognition tasks,
  but their inherent reliance on regular grid structures limits
  their capacity to model complex topological relationships and
  non-local semantics within images. To address this limita tion, we propose
the hierarchical graph feature enhancement
  (HGFE), a novel framework that integrates graph-based rea soning into CNNs to
enhance both structural awareness and
  feature representation. HGFE builds two complementary levels
  of graph structures: intra-window graph convolution to cap ture local spatial
dependencies and inter-window supernode
  interactions to model global semantic relationships. Moreover,
  we introduce an adaptive frequency modulation module that
  dynamically balances low-frequency and high-frequency signal
  propagation, preserving critical edge and texture information
  while mitigating over-smoothing. The proposed HGFE module
  is lightweight, end-to-end trainable, and can be seamlessly
  integrated into standard CNN backbone networks. Extensive
  experiments on CIFAR-100 (classification), PASCAL VOC,
  and VisDrone (detection), as well as CrackSeg and CarParts
  (segmentation), validated the effectiveness of the HGFE in
  improving structural representation and enhancing overall
  recognition performance.

</details>


### [81] [Handwritten Text Recognition of Historical Manuscripts Using Transformer-Based Models](https://arxiv.org/abs/2508.11499)
*Erez Meoded*

Main category: cs.CV

TL;DR: 本文通过针对历史手稿特性的图像增强方法和集成学习策略，显著提升了16世纪拉丁手稿的手写文本识别准确率。


<details>
  <summary>Details</summary>
Motivation: 历史手写文本识别对于档案文献的数字化和学术利用至关重要，但受到转录语料稀缺、语言多样和书写风格多变等问题的限制，现有技术在历史手稿领域表现有限。因此，探索更有效的模型和技术以提升识别准确率具有重要意义。

Method: 作者应用最先进的Transformer架构的TrOCR模型，针对16世纪拉丁手稿数据集，设计了目标化图像预处理流程，并开发了四种专为历史手稿设计的新型数据增强方法。此外，探索了集成学习策略，通过组合不同增强模型来发挥各自优势。

Result: 在Gwalther手稿数据集上，采用弹性增强（Elastic）的单一模型字符错误率（CER）达到1.86，top-5集成投票模型CER进一步降至1.60，相较于既有TrOCR_BASE提升50%，比此前最佳技术提升42%。

Conclusion: 面向历史手稿特性的专用数据增强方法和集成策略能大幅提升手写文本识别性能，为档案数字化提供更准确的技术支持。

Abstract: Historical handwritten text recognition (HTR) is essential for unlocking the
cultural and scholarly value of archival documents, yet digitization is often
hindered by scarce transcriptions, linguistic variation, and highly diverse
handwriting styles. In this study, we apply TrOCR, a state-of-the-art
transformer-based HTR model, to 16th-century Latin manuscripts authored by
Rudolf Gwalther. We investigate targeted image preprocessing and a broad suite
of data augmentation techniques, introducing four novel augmentation methods
designed specifically for historical handwriting characteristics. We also
evaluate ensemble learning approaches to leverage the complementary strengths
of augmentation-trained models. On the Gwalther dataset, our best single-model
augmentation (Elastic) achieves a Character Error Rate (CER) of 1.86, while a
top-5 voting ensemble achieves a CER of 1.60 - representing a 50% relative
improvement over the best reported TrOCR_BASE result and a 42% improvement over
the previous state of the art. These results highlight the impact of
domain-specific augmentations and ensemble strategies in advancing HTR
performance for historical manuscripts.

</details>


### [82] [AIM: Amending Inherent Interpretability via Self-Supervised Masking](https://arxiv.org/abs/2508.11502)
*Eyad Alshami,Shashank Agnihotri,Bernt Schiele,Margret Keuper*

Main category: cs.CV

TL;DR: 提出了一种新的自监督特征屏蔽方法AIM，能够提升深度神经网络对真实特征的利用，并增强可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前深度神经网络在决策过程中会同时利用真实特征和虚假特征，导致模型泛化能力和可解释性受到限制。因此需要方法促进模型更多利用真实特征，提升决策过程的可信度。

Method: 论文提出AIM方法，通过在多编码阶段提取特征，引导对样本特异的特征进行自监督屏蔽，无需额外标注信息，从而鼓励网络关注于对任务真正有用的特征。

Result: AIM方法在多个具挑战性的常规和细粒度分类数据集，包括ImageNet100、HardImageNet、ImageWoof、Waterbirds、TravelingBirds、CUB-200等，均获得了显著的解释性（EPG得分提升）和准确率提升，相比已有主流方法表现更佳。

Conclusion: AIM能培养模型对真实和有意义特征的依赖，因此提升了泛化性和解释性，是一种高效且通用的方法。

Abstract: It has been observed that deep neural networks (DNNs) often use both genuine
as well as spurious features. In this work, we propose "Amending Inherent
Interpretability via Self-Supervised Masking" (AIM), a simple yet interestingly
effective method that promotes the network's utilization of genuine features
over spurious alternatives without requiring additional annotations. In
particular, AIM uses features at multiple encoding stages to guide a
self-supervised, sample-specific feature-masking process. As a result, AIM
enables the training of well-performing and inherently interpretable models
that faithfully summarize the decision process. We validate AIM across a
diverse range of challenging datasets that test both out-of-distribution
generalization and fine-grained visual understanding. These include
general-purpose classification benchmarks such as ImageNet100, HardImageNet,
and ImageWoof, as well as fine-grained classification datasets such as
Waterbirds, TravelingBirds, and CUB-200. AIM demonstrates significant dual
benefits: interpretability improvements, as measured by the Energy Pointing
Game (EPG) score, and accuracy gains over strong baselines. These consistent
gains across domains and architectures provide compelling evidence that AIM
promotes the use of genuine and meaningful features that directly contribute to
improved generalization and human-aligned interpretability.

</details>


### [83] [A Real-time Concrete Crack Detection and Segmentation Model Based on YOLOv11](https://arxiv.org/abs/2508.11517)
*Shaoze Huang,Qi Liu,Chao Chen,Yuhang Chen*

Main category: cs.CV

TL;DR: 该论文针对长三角地区交通基础设施老化问题，提出一种基于YOLOv11n的新型混凝土裂缝检测分割模型YOLOv11-KW-TA-FP，有效提升在复杂背景下小裂缝的检测精度，并对模型结构和损失函数进行创新设计，实验结果显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 交通基础设施在快速发展的长三角地区加速老化，裂缝严重影响结构安全及经济发展。传统人工巡检效率低，现有深度学习模型在复杂背景下尤其对小目标裂缝识别表现不佳，亟需一种高效且鲁棒的自动化检测方法。

Method: 本文基于YOLOv11n设计多任务裂缝检测分割模型，提出：(1) 在骨干网络引入动态KernelWarehouse卷积（KWConv）以提升特征表达能力；(2) 在特征金字塔中集成三重注意力机制（TA）加强通道-空间建模；(3) 设计FP-IoU损失函数自适应优化边界框回归。

Result: 实验验证增强模型达到91.3%精度、76.6%召回率和86.4% mAP@50，各创新模块协同带来显著提升。消融实验和鲁棒性测试也显示该方法在数据稀缺、噪声干扰下表现稳定。

Conclusion: 该研究为混凝土裂缝自动化检测提供高效、实用的计算机视觉解决方案，具备较高工程应用价值。

Abstract: Accelerated aging of transportation infrastructure in the rapidly developing
Yangtze River Delta region necessitates efficient concrete crack detection, as
crack deterioration critically compromises structural integrity and regional
economic growth. To overcome the limitations of inefficient manual inspection
and the suboptimal performance of existing deep learning models, particularly
for small-target crack detection within complex backgrounds, this paper
proposes YOLOv11-KW-TA-FP, a multi-task concrete crack detection and
segmentation model based on the YOLOv11n architecture. The proposed model
integrates a three-stage optimization framework: (1) Embedding dynamic
KernelWarehouse convolution (KWConv) within the backbone network to enhance
feature representation through a dynamic kernel sharing mechanism; (2)
Incorporating a triple attention mechanism (TA) into the feature pyramid to
strengthen channel-spatial interaction modeling; and (3) Designing an FP-IoU
loss function to facilitate adaptive bounding box regression penalization.
Experimental validation demonstrates that the enhanced model achieves
significant performance improvements over the baseline, attaining 91.3%
precision, 76.6% recall, and 86.4% mAP@50. Ablation studies confirm the
synergistic efficacy of the proposed modules. Furthermore, robustness tests
indicate stable performance under conditions of data scarcity and noise
interference. This research delivers an efficient computer vision solution for
automated infrastructure inspection, exhibiting substantial practical
engineering value.

</details>


### [84] [Multi-State Tracker: Enhancing Efficient Object Tracking via Multi-State Specialization and Interaction](https://arxiv.org/abs/2508.11531)
*Shilei Wang,Gong Cheng,Pujian Lai,Dong Gao,Junwei Han*

Main category: cs.CV

TL;DR: 本文提出了一种高效且鲁棒性强的目标跟踪方法MST，既兼顾了速度又提升了跟踪精度，明显超过现有高效跟踪器。


<details>
  <summary>Details</summary>
Motivation: 高效的跟踪器通常牺牲了特征表达能力，导致难以准确跟踪目标。为了解决这一问题，需要设计可增强特征表达且计算开销小的方法。

Method: 提出多状态跟踪器（MST），通过多状态生成（MSG）多阶段提取多种状态特征，再结合极轻量级的状态特定增强（SSE）模块对多状态特征增强，并用跨状态交互（CSI）模块实现信息融合。这些新模块都基于轻量的HSA-SSD设计，总计算和参数量极小。

Result: 在多个公开数据集上，MST跟踪器显著优于所有已知高效跟踪器，精度和鲁棒性都有较大提升，尤其是在GOT-10K数据集上AO指标比之前SOTA方法HCAT提升4.5%。

Conclusion: MST方法能以极小的额外计算和参数代价，有效综合各层特征信息，大幅提升高效目标跟踪的性能和稳定性。

Abstract: Efficient trackers achieve faster runtime by reducing computational
complexity and model parameters. However, this efficiency often compromises the
expense of weakened feature representation capacity, thus limiting their
ability to accurately capture target states using single-layer features. To
overcome this limitation, we propose Multi-State Tracker (MST), which utilizes
highly lightweight state-specific enhancement (SSE) to perform specialized
enhancement on multi-state features produced by multi-state generation (MSG)
and aggregates them in an interactive and adaptive manner using cross-state
interaction (CSI). This design greatly enhances feature representation while
incurring minimal computational overhead, leading to improved tracking
robustness in complex environments. Specifically, the MSG generates multiple
state representations at multiple stages during feature extraction, while SSE
refines them to highlight target-specific features. The CSI module facilitates
information exchange between these states and ensures the integration of
complementary features. Notably, the introduced SSE and CSI modules adopt a
highly lightweight hidden state adaptation-based state space duality (HSA-SSD)
design, incurring only 0.1 GFLOPs in computation and 0.66 M in parameters.
Experimental results demonstrate that MST outperforms all previous efficient
trackers across multiple datasets, significantly improving tracking accuracy
and robustness. In particular, it shows excellent runtime performance, with an
AO score improvement of 4.5\% over the previous SOTA efficient tracker HCAT on
the GOT-10K dataset. The code is available at https://github.com/wsumel/MST.

</details>


### [85] [An Efficient Medical Image Classification Method Based on a Lightweight Improved ConvNeXt-Tiny Architecture](https://arxiv.org/abs/2508.11532)
*Jingsong Xia,Yue Yin,Xiuhan Li*

Main category: cs.CV

TL;DR: 本文提出了一种改进型ConvNeXt-Tiny架构的医学影像分类方法，在有限计算资源下提升分类准确率并降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 医学影像智能分析对临床诊断极其重要，但在计算资源受限的环境中实现高效、高精度的图像分类仍具挑战性。本文旨在解决此痛点。

Method: 方法包括对ConvNeXt-Tiny结构进行优化，提出了双全局池化（平均+最大）特征融合方案，以及轻量级通道注意力模块SEVector，增强特征提取并减少参数。同时，通过引入Feature Smoothing Loss提升类内特征一致性。

Result: 仅用CPU（8线程），在10个训练周期内，测试集最高准确率达89.10%，且损失收敛趋势稳定。

Conclusion: 该方法在资源受限条件下显著提升医学影像分类性能，为医学影像分析模型的实施与推广提供了高效可行的解决方案。

Abstract: Intelligent analysis of medical imaging plays a crucial role in assisting
clinical diagnosis. However, achieving efficient and high-accuracy image
classification in resource-constrained computational environments remains
challenging. This study proposes a medical image classification method based on
an improved ConvNeXt-Tiny architecture. Through structural optimization and
loss function design, the proposed method enhances feature extraction
capability and classification performance while reducing computational
complexity. Specifically, the method introduces a dual global pooling (Global
Average Pooling and Global Max Pooling) feature fusion strategy into the
ConvNeXt-Tiny backbone to simultaneously preserve global statistical features
and salient response information. A lightweight channel attention module,
termed Squeeze-and-Excitation Vector (SEVector), is designed to improve the
adaptive allocation of channel weights while minimizing parameter overhead.
Additionally, a Feature Smoothing Loss is incorporated into the loss function
to enhance intra-class feature consistency and suppress intra-class variance.
Under CPU-only conditions (8 threads), the method achieves a maximum
classification accuracy of 89.10% on the test set within 10 training epochs,
exhibiting a stable convergence trend in loss values. Experimental results
demonstrate that the proposed method effectively improves medical image
classification performance in resource-limited settings, providing a feasible
and efficient solution for the deployment and promotion of medical imaging
analysis models.

</details>


### [86] [Reinforcing Video Reasoning Segmentation to Think Before It Segments](https://arxiv.org/abs/2508.11538)
*Sitong Gong,Lu Zhang,Yunzhi Zhuge,Xu Jia,Pingping Zhang,Huchuan Lu*

Main category: cs.CV

TL;DR: 该论文提出了一种用于视频推理分割（VRS）的新模型Veason-R1，通过引入结构化推理机制，显著提升了分割性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的视频推理分割方法在推理过程中可解释性差，时空推理能力不足，导致模型性能有限。因此，作者希望通过引入强化学习中结构化推理理念，提升模型的推理能力和性能。

Method: 文章开发了Veason-R1模型，采用Group Relative Policy Optimization（GRPO）和Chain-of-Thought（CoT）初始化。首先，作者构建了高质量的CoT训练集，训练出有结构化推理能力的Veason-SFT模型。随后用GRPO进一步微调，强化推理链条探索。同时引入全局奖励机制，通过优化空间对齐和时序一致性，提升关键帧定位和细粒度分割能力。

Result: Veason-R1在多个VRS基准上表现出色，如在ReVOS提升1.3 J&F、ReasonVOS提升10.0 J&F，同时对“幻觉”现象有更高容错性（+8.8 R），均优于以往方法。

Conclusion: 通过引入结构化推理和强化学习机制，Veason-R1不仅提升了分割性能，还增强了模型的可解释性和抗幻觉能力，对视频推理分割领域有重要推动作用。

Abstract: Video reasoning segmentation (VRS) endeavors to delineate referred objects in
videos guided by implicit instructions that encapsulate human intent and
temporal logic. Previous approaches leverage large vision language models
(LVLMs) to encode object semantics into <SEG> tokens for mask prediction.
However, this paradigm suffers from limited interpretability during inference
and suboptimal performance due to inadequate spatiotemporal reasoning. Drawing
inspiration from seminal breakthroughs in reinforcement learning, we introduce
Veason-R1, a specialized LVLM for VRS that emphasizes structured reasoning in
segmentation. Veason-R1 is trained through Group Relative Policy Optimization
(GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, we
curate high-quality CoT training data to instill structured reasoning
trajectories, bridging video-level semantics and frame-level spatial grounding,
yielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPO
fine-tuning encourages efficient exploration of the reasoning space by
optimizing reasoning chains. To this end, we incorporate a holistic reward
mechanism that synergistically enhances spatial alignment and temporal
consistency, bolstering keyframe localization and fine-grained grounding.
Comprehensive empirical evaluations demonstrate that Veason-R1 achieves
state-of-the-art performance on multiple benchmarks, surpassing prior art by
significant margins (e.g., +1.3 J &F in ReVOS and +10.0 J &F in ReasonVOS),
while exhibiting robustness to hallucinations (+8.8 R). Our code and model
weights will be available at Veason-R1.

</details>


### [87] [Training-Free Anomaly Generation via Dual-Attention Enhancement in Diffusion Model](https://arxiv.org/abs/2508.11550)
*Zuo Zuo,Jiahao Dong,Yanyun Qu,Zongze Wu*

Main category: cs.CV

TL;DR: 本文提出了一种基于Stable Diffusion的无训练异常生成框架AAG，可有效生成工业视觉检测中所需的高保真异常图片，无需额外训练数据。


<details>
  <summary>Details</summary>
Motivation: 制造业中的异常检测常常受限于异常数据稀缺，现有异常生成方法要么生成质量不高，要么需要额外的数据训练，迫切需要更高效、易用的异常生成技术来辅助任务。

Method: AAG利用Stable Diffusion的强大生成能力，只需输入正常图片、掩码和简要文本提示，即可在指定区域生成逼真异常。作者创新地提出了两种增强机制：Cross-Attention Enhancement (CAE)通过调整Stable Diffusion的跨注意力机制，使生成区域更符合文本描述；Self-Attention Enhancement (SAE)提升正常和异常视觉分量的相似性，确保生成的异常更自然。

Result: 实验表明，AAG在MVTec AD和VisA等数据集上能生成高质量异常图片，所生成的异常数据在下游异常检测任务中显著提升了模型表现。

Conclusion: AAG框架无需额外训练和数据，即可高效生成真实感异常，增强数据集与下游工业异常检测系统，对工业视觉领域具有实用价值和推广前景。

Abstract: Industrial anomaly detection (AD) plays a significant role in manufacturing
where a long-standing challenge is data scarcity. A growing body of works have
emerged to address insufficient anomaly data via anomaly generation. However,
these anomaly generation methods suffer from lack of fidelity or need to be
trained with extra data. To this end, we propose a training-free anomaly
generation framework dubbed AAG, which is based on Stable Diffusion (SD)'s
strong generation ability for effective anomaly image generation. Given a
normal image, mask and a simple text prompt, AAG can generate realistic and
natural anomalies in the specific regions and simultaneously keep contents in
other regions unchanged. In particular, we propose Cross-Attention Enhancement
(CAE) to re-engineer the cross-attention mechanism within Stable Diffusion
based on the given mask. CAE increases the similarity between visual tokens in
specific regions and text embeddings, which guides these generated visual
tokens in accordance with the text description. Besides, generated anomalies
need to be more natural and plausible with object in given image. We propose
Self-Attention Enhancement (SAE) which improves similarity between each normal
visual token and anomaly visual tokens. SAE ensures that generated anomalies
are coherent with original pattern. Extensive experiments on MVTec AD and VisA
datasets demonstrate effectiveness of AAG in anomaly generation and its
utility. Furthermore, anomaly images generated by AAG can bolster performance
of various downstream anomaly inspection tasks.

</details>


### [88] [TrajSV: A Trajectory-based Model for Sports Video Representations and Applications](https://arxiv.org/abs/2508.11569)
*Zheng Wang,Shihao Xu,Wei Shi*

Main category: cs.CV

TL;DR: 本文提出了TrajSV，一种基于轨迹的无监督体育视频分析框架，在足球、篮球和排球的数据集上于视频检索、动作识别和视频字幕生成三大任务中均取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 现有体育数据分析面临数据不可获得、缺乏有效基于轨迹的方法以及监督标签需求大的问题，亟需一个能充分挖掘运动员与球轨迹、减少监督标签依赖的新模型。

Method: TrajSV框架包含数据预处理、Clip表示网络（CRNet）和视频表示网络（VRNet）三部分。首先通过视频提取运动员和球的轨迹；接着用增强轨迹的Transformer对每个片段进行表示学习（CRNet）；再用编码器-解码器结构结合视觉特征与片段表达生成整段视频表示（VRNet）。整个系统通过三重对比损失进行无监督训练。

Result: 在三种体育视频数据集上，TrajSV在视频检索任务中取得了约70%的性能提升；在动作捕捉任务中，在17类动作中有9类达到SOTA水平；在视频字幕生成中提升了近20%。

Conclusion: TrajSV显著提升了体育视频分析的准确性和无监督能力，且已开发成可实际部署的系统，可广泛支持体育相关下游应用。

Abstract: Sports analytics has received significant attention from both academia and
industry in recent years. Despite the growing interest and efforts in this
field, several issues remain unresolved, including (1) data unavailability, (2)
lack of an effective trajectory-based framework, and (3) requirement for
sufficient supervision labels. In this paper, we present TrajSV, a
trajectory-based framework that addresses various issues in existing studies.
TrajSV comprises three components: data preprocessing, Clip Representation
Network (CRNet), and Video Representation Network (VRNet). The data
preprocessing module extracts player and ball trajectories from sports
broadcast videos. CRNet utilizes a trajectory-enhanced Transformer module to
learn clip representations based on these trajectories. Additionally, VRNet
learns video representations by aggregating clip representations and visual
features with an encoder-decoder architecture. Finally, a triple contrastive
loss is introduced to optimize both video and clip representations in an
unsupervised manner. The experiments are conducted on three broadcast video
datasets to verify the effectiveness of TrajSV for three types of sports (i.e.,
soccer, basketball, and volleyball) with three downstream applications (i.e.,
sports video retrieval, action spotting, and video captioning). The results
demonstrate that TrajSV achieves state-of-the-art performance in sports video
retrieval, showcasing a nearly 70% improvement. It outperforms baselines in
action spotting, achieving state-of-the-art results in 9 out of 17 action
categories, and demonstrates a nearly 20% improvement in video captioning.
Additionally, we introduce a deployed system along with the three applications
based on TrajSV.

</details>


### [89] [Causality Matters: How Temporal Information Emerges in Video Language Models](https://arxiv.org/abs/2508.11576)
*Yumeng Shi,Quanyu Long,Yin Wu,Wenya Wang*

Main category: cs.CV

TL;DR: 本文深入研究了视频语言模型（VideoLMs）中的时间理解机制，发现时序编码（PE）对模型性能影响有限，反而帧顺序的颠倒会大幅降低性能，提出了基于这些发现的新型高效推理方法并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管视频语言模型在多模态理解上取得很大进展，但对事件顺序、持续时间和跨时刻关系的时间理解仍然是一个挑战。以往强调时序编码是建模时间结构的关键，因此作者希望探究视频时序信息在模型中的真实作用机理，从而优化模型结构和推理效率。

Method: 作者首先通过移除或修改时序编码、颠倒帧顺序等消融实验，系统分析了时间信息在模型中的流动路径。通过追踪模型中信息整合过程，揭示了模型是如何在因果注意力约束下，通过帧间交互逐步合成和整合时序线索的。基于此，作者提出了分阶段跨模态注意力和早停输出机制两种高效推理策略。

Result: 实验表明，去除/修改PE对时间理解影响很小，而帧顺序反转导致性能大幅下降。提出的分阶段跨模态注意和提早截断机制，在两个基准任务上取得有效提升，验证了其实用价值。

Conclusion: 本文首次系统揭示了VideoLMs时间理解的本质机制，发现时序结构主要由帧间交互自发建立，为未来视频语言模型的高效结构设计和性能优化提供了理论依据和实证支持。

Abstract: Video language models (VideoLMs) have made significant progress in multimodal
understanding. However, temporal understanding, which involves identifying
event order, duration, and relationships across time, still remains a core
challenge. Prior works emphasize positional encodings (PEs) as a key mechanism
for encoding temporal structure. Surprisingly, we find that removing or
modifying PEs in video inputs yields minimal degradation in the performance of
temporal understanding. In contrast, reversing the frame sequence while
preserving the original PEs causes a substantial drop. To explain this
behavior, we conduct substantial analysis experiments to trace how temporal
information is integrated within the model. We uncover a causal information
pathway: temporal cues are progressively synthesized through inter-frame
attention, aggregated in the final frame, and subsequently integrated into the
query tokens. This emergent mechanism shows that temporal reasoning emerges
from inter-visual token interactions under the constraints of causal attention,
which implicitly encodes temporal structure. Based on these insights, we
propose two efficiency-oriented strategies: staged cross-modal attention and a
temporal exit mechanism for early token truncation. Experiments on two
benchmarks validate the effectiveness of both approaches. To the best of our
knowledge, this is the first work to systematically investigate video temporal
understanding in VideoLMs, offering insights for future model improvement.

</details>


### [90] [DashCam Video: A complementary low-cost data stream for on-demand forest-infrastructure system monitoring](https://arxiv.org/abs/2508.11591)
*Durga Joshi,Chandi Witharana,Robert Fahey,Thomas Worthley,Zhe Zhu,Diego Cerrai*

Main category: cs.CV

TL;DR: 本研究提出了一个利用普通行车记录仪（dashcam）视频数据，对道路两侧植被和基础设施进行实时、物体级结构评估和定位的新型低成本框架。


<details>
  <summary>Details</summary>
Motivation: 传统的道路基础设施和植被风险评估多依赖于昂贵的LiDAR或遥感影像，实时性差，难以大规模和高频率实施。作者希望借助普及性高但利用率低的行车记录仪视频，实现高效、低成本的城市道路环境监测。

Method: 构建了端到端的视频处理流水线，包括单目深度估计、梯度提升回归用于深度误差修正，并结合GPS三角测量和小孔成像几何，精准获取目标空间位置和结构信息。验证了不同摄像头安装位置和车速对效果的影响。

Result: 深度修正模型在距离大于15米处显著减小了误差（R2=0.92, MAE=0.31），物体空间定位和高度估计在低速车辆和车内摄像头时最准确（定位误差均值2.83米，树木高度MAE为2.09米，电杆为0.88米）。

Conclusion: 本方法首次结合了单目深度建模、GPS三角定位与实时结构评估，实现了城市道路植被和基础设施的低成本、实时、可扩展监测，可补充现有LiDAR等遥感手段，对城市规划与电力等公共事业单位具有重要应用价值。

Abstract: Our study introduces a novel, low-cost, and reproducible framework for
real-time, object-level structural assessment and geolocation of roadside
vegetation and infrastructure with commonly available but underutilized
dashboard camera (dashcam) video data. We developed an end-to-end pipeline that
combines monocular depth estimation, depth error correction, and geometric
triangulation to generate accurate spatial and structural data from
street-level video streams from vehicle-mounted dashcams. Depth maps were first
estimated using a state-of-the-art monocular depth model, then refined via a
gradient-boosted regression framework to correct underestimations, particularly
for distant objects. The depth correction model achieved strong predictive
performance (R2 = 0.92, MAE = 0.31 on transformed scale), significantly
reducing bias beyond 15 m. Further, object locations were estimated using
GPS-based triangulation, while object heights were calculated using pin hole
camera geometry. Our method was evaluated under varying conditions of camera
placement and vehicle speed. Low-speed vehicle with inside camera gave the
highest accuracy, with mean geolocation error of 2.83 m, and mean absolute
error (MAE) in height estimation of 2.09 m for trees and 0.88 m for poles. To
the best of our knowledge, it is the first framework to combine monocular depth
modeling, triangulated GPS-based geolocation, and real-time structural
assessment for urban vegetation and infrastructure using consumer-grade video
data. Our approach complements conventional RS methods, such as LiDAR and image
by offering a fast, real-time, and cost-effective solution for object-level
monitoring of vegetation risks and infrastructure exposure, making it
especially valuable for utility companies, and urban planners aiming for
scalable and frequent assessments in dynamic urban environments.

</details>


### [91] [CoreEditor: Consistent 3D Editing via Correspondence-constrained Diffusion](https://arxiv.org/abs/2508.11603)
*Zhe Zhu,Honghua Chen,Peng Li,Mingqiang Wei*

Main category: cs.CV

TL;DR: 本文提出了CoreEditor框架，利用对应约束注意力机制实现了跨视角一致性的高质量文本驱动3D编辑，效果优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有大多数文本驱动3D编辑方法通过改造2D图像编辑器处理多视角输入，但难以保证不同视角的一致性，容易产生编辑不充分和细节模糊的问题。

Method: 提出CoreEditor，其核心是一种对应约束注意力机制，在扩散去噪过程中强制保证应保持一致的像素之间有精确的信息交互。此外，结合去噪时估算的语义相似性，提升对应建模的可靠度。同时，还设计了一个候选选择机制，让用户能从多个结果中自由选择所需编辑输出。

Result: 实验显示，CoreEditor能输出细节更清晰、3D一致性更强的编辑结果，并且在多项指标上明显优于现有主流方法。

Conclusion: CoreEditor有效解决了3D编辑中多视角一致性和细节保真的难题，兼顾编辑灵活性和高可控性，对实际应用具有较大提升价值。

Abstract: Text-driven 3D editing seeks to modify 3D scenes according to textual
descriptions, and most existing approaches tackle this by adapting pre-trained
2D image editors to multi-view inputs. However, without explicit control over
multi-view information exchange, they often fail to maintain cross-view
consistency, leading to insufficient edits and blurry details. We introduce
CoreEditor, a novel framework for consistent text-to-3D editing. The key
innovation is a correspondence-constrained attention mechanism that enforces
precise interactions between pixels expected to remain consistent throughout
the diffusion denoising process. Beyond relying solely on geometric alignment,
we further incorporate semantic similarity estimated during denoising, enabling
more reliable correspondence modeling and robust multi-view editing. In
addition, we design a selective editing pipeline that allows users to choose
preferred results from multiple candidates, offering greater flexibility and
user control. Extensive experiments show that CoreEditor produces high-quality,
3D-consistent edits with sharper details, significantly outperforming prior
methods.

</details>


### [92] [LoRAtorio: An intrinsic approach to LoRA Skill Composition](https://arxiv.org/abs/2508.11624)
*Niki Foteinopoulou,Ignas Budvytis,Stephan Liwicki*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的多LoRA适配器无训练组合方法LoRAtorio，通过空间感知权重机制，有效解决多适配器组合中的表现下降问题，实现了更优的个性化图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA（低秩适配）适配器方法在需组合多个技能或风格时表现不佳，尤其是在事先无法预知具体需求的开放场景下难以管理和最优组合多个LoRA模块。因此，需要一个高效、无需额外训练的新框架解决多LoRA合成中的互相影响及效率问题。

Method: LoRAtorio通过以下步骤实现多LoRA组合：（1）在潜空间中将图像划分为多个空间块，比较每块的噪声预测与基础模型的余弦相似度；（2）根据这些相似度构造空间感知的权重矩阵，指导多LoRA输出的加权融合；（3）为解决领域漂移，提出调整的classifier-free guidance策略，引入基础模型的无条件分数。支持推理时动态选择相关LoRA，从大量适配器池中灵活组合。

Result: LoRAtorio在ClipScore等主流评价指标上相较现有方法最多有1.3%的提升，在GPT-4V对比测试中胜率达72.43%，并且适用于多种潜在扩散模型（latent diffusion models）。

Conclusion: LoRAtorio无需额外训练即可实现多LoRA的高效空间加权组合，不仅提升组合性能，还支持大规模动态适配器选择，为文本到图像扩散模型带来了更强的个性化与通用性。

Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted technique in
text-to-image diffusion models, enabling the personalisation of visual concepts
such as characters, styles, and objects. However, existing approaches struggle
to effectively compose multiple LoRA adapters, particularly in open-ended
settings where the number and nature of required skills are not known in
advance. In this work, we present LoRAtorio, a novel train-free framework for
multi-LoRA composition that leverages intrinsic model behaviour. Our method is
motivated by two key observations: (1) LoRA adapters trained on narrow domains
produce denoised outputs that diverge from the base model, and (2) when
operating out-of-distribution, LoRA outputs show behaviour closer to the base
model than when conditioned in distribution. The balance between these two
observations allows for exceptional performance in the single LoRA scenario,
which nevertheless deteriorates when multiple LoRAs are loaded. Our method
operates in the latent space by dividing it into spatial patches and computing
cosine similarity between each patch's predicted noise and that of the base
model. These similarities are used to construct a spatially-aware weight
matrix, which guides a weighted aggregation of LoRA outputs. To address domain
drift, we further propose a modification to classifier-free guidance that
incorporates the base model's unconditional score into the composition. We
extend this formulation to a dynamic module selection setting, enabling
inference-time selection of relevant LoRA adapters from a large pool. LoRAtorio
achieves state-of-the-art performance, showing up to a 1.3% improvement in
ClipScore and a 72.43% win rate in GPT-4V pairwise evaluations, and generalises
effectively to multiple latent diffusion models.

</details>


### [93] [Is ChatGPT-5 Ready for Mammogram VQA?](https://arxiv.org/abs/2508.11628)
*Qiang Li,Shansong Wang,Mingzhe Hu,Mojtaba Safari,Zachary Eidex,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 本研究系统评估了GPT-5和GPT-4o在乳腺X光视觉问答任务上的表现。GPT-5进步显著，但仍不及专家和域特化模型，表明需要进一步优化。


<details>
  <summary>Details</summary>
Motivation: 乳腺X光视觉问答可结合图像和临床推理，有望辅助乳腺癌筛查。评估通用大模型在该任务的实力有重要现实意义。

Method: 在四个公开数据集（EMBED, InBreast, CMMD, CBIS-DDSM）上，分别对BI-RADS分级、异常检测、恶性肿瘤分类等任务，比较了GPT-5, GPT-4o与专家和领域专用模型的表现。

Result: GPT-5在各项任务中较GPT-4o有明显提升，如在EMBED数据集上恶性肿瘤分类准确率达52.8%。但整体仍低于专家和特化模型，对敏感性与特异性也存在较大差距。

Conclusion: 尽管GPT-5已有长足进步，一般大型语言模型目前尚不足以直接应用于乳腺X光筛查等高风险临床场景，需后续针对性优化。但其趋势显示未来有望成为有效辅助工具。

Abstract: Mammogram visual question answering (VQA) integrates image interpretation
with clinical reasoning and has potential to support breast cancer screening.
We systematically evaluated the GPT-5 family and GPT-4o model on four public
mammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for BI-RADS assessment,
abnormality detection, and malignancy classification tasks. GPT-5 consistently
was the best performing model but lagged behind both human experts and
domain-specific fine-tuned models. On EMBED, GPT-5 achieved the highest scores
among GPT variants in density (56.8%), distortion (52.5%), mass (64.5%),
calcification (63.5%), and malignancy (52.8%) classification. On InBreast, it
attained 36.9% BI-RADS accuracy, 45.9% abnormality detection, and 35.0%
malignancy classification. On CMMD, GPT-5 reached 32.3% abnormality detection
and 55.0% malignancy accuracy. On CBIS-DDSM, it achieved 69.3% BI-RADS
accuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy. Compared
with human expert estimations, GPT-5 exhibited lower sensitivity (63.5%) and
specificity (52.3%). While GPT-5 exhibits promising capabilities for screening
tasks, its performance remains insufficient for high-stakes clinical imaging
applications without targeted domain adaptation and optimization. However, the
tremendous improvements in performance from GPT-4o to GPT-5 show a promising
trend in the potential for general large language models (LLMs) to assist with
mammography VQA tasks.

</details>


### [94] [Thyme: Think Beyond Images](https://arxiv.org/abs/2508.11630)
*Yi-Fan Zhang,Xingyu Lu,Shukang Yin,Chaoyou Fu,Wei Chen,Xiao Hu,Bin Wen,Kaiyu Jiang,Changyi Liu,Tianke Zhang,Haonan Fan,Kaibing Chen,Jiankang Chen,Haojie Ding,Kaiyu Tang,Zhang Zhang,Liang Wang,Fan Yang,Tingting Gao,Guorui Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新的MLLMs范式Thyme，能够通过自动生成并执行代码实现丰富的图像处理和复杂推理，实验证明性能优于现有开源方案。


<details>
  <summary>Details</summary>
Motivation: 当前开源模型在视觉信息推理和图像操作的多样性上仍不及专有模型（如O3）。研究动机是实现具备丰富图像操作能力且推理性强的开源MLLMs。

Method: 提出Thyme范式，使多模态大模型自动自主地产生和执行图像和计算类代码操作。采用两阶段训练：先用50万条样本做SFT训练代码生成，再用人工高难度数据RL精细决策，提出GRPO-ATS算法，针对文本和代码分别设温度，平衡推理探索和代码精确执行。

Result: 在近20个基准上进行了细致测试与消融实验，Thyme在高分辨率感知和复杂推理任务中，性能显著且稳健提升。

Conclusion: Thyme展示了通过代码自动化进行更复杂视觉理解和逻辑推理的潜力，为开源多模态模型的能力进步开辟了新路径。

Abstract: Following OpenAI's introduction of the ``thinking with images'' concept,
recent efforts have explored stimulating the use of visual information in the
reasoning process to enhance model performance in perception and reasoning
tasks. However, to the best of our knowledge, no open-source work currently
offers a feature set as rich as proprietary models (O3), which can perform
diverse image manipulations and simultaneously enhance logical reasoning
capabilities through code. In this paper, we make a preliminary attempt in this
direction by introducing Thyme (Think Beyond Images), a novel paradigm for
enabling MLLMs to transcend existing ``think with images'' approaches by
autonomously generating and executing diverse image processing and
computational operations via executable code. This approach not only
facilitates a rich, on-the-fly set of image manipulations (e.g., cropping,
rotation, contrast enhancement) but also allows for mathematical computations,
all while maintaining high autonomy in deciding when and how to apply these
operations. We activate this capability through a two-stage training strategy:
an initial SFT on a curated dataset of 500K samples to teach code generation,
followed by a RL phase to refine decision-making. For the RL stage, we manually
collect and design high-resolution question-answer pairs to increase the
learning difficulty, and we propose GRPO-ATS (Group Relative Policy
Optimization with Adaptive Temperature Sampling), an algorithm that applies
distinct temperatures to text and code generation to balance reasoning
exploration with code execution precision. We conduct extensive experimental
analysis and ablation studies. Comprehensive evaluations on nearly 20
benchmarks show that Thyme yields significant and consistent performance gains,
particularly in challenging high-resolution perception and complex reasoning
tasks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [95] [A2HCoder: An LLM-Driven Coding Agent for Hierarchical Algorithm-to-HDL Translation](https://arxiv.org/abs/2508.10904)
*Jie Lei,Ruofan Jia,J. Andrew Zhang,Hao Zhang*

Main category: cs.CL

TL;DR: A2HCoder是一种利用大语言模型（LLM）实现的分层算法到HDL代码的自动生成代理系统，用于高效、可靠地将高层算法代码翻译为硬件描述语言，实现无线通信系统中的低延迟和低功耗硬件部署。


<details>
  <summary>Details</summary>
Motivation: 当前无线通信系统对极低延迟与低功耗的需求增加，但算法到硬件的自动化部署存在明显难点：高层语言如MATLAB和底层HDL（如Verilog）之间在内存、数据处理和数据类型等方面存在根本差异，传统转换流程需要丰富硬件经验和大量手工开发，效率低下且易出错。

Method: 提出A2HCoder分层代理框架：在横向上将复杂算法拆分为功能模块，简化代码生成流程和提高一致性；在纵向上采用逐步、细粒度的转译方法，结合MATLAB、Vitis HLS等工具调试和电路级综合。过程依托大语言模型，强化系统健壮性和可解释性，降低幻觉生成。

Result: 在5G无线通信实际案例中部署验证A2HCoder，表明该方案具备较高的实用性、可靠性与部署效率，能有效缩小算法到硬件之间的工程鸿沟。

Conclusion: A2HCoder显著提升算法到硬件的自动化、可解释性与准确率，有助于无线通信等需高效硬件部署领域的快速研发和实际落地。

Abstract: In wireless communication systems, stringent requirements such as ultra-low
latency and power consumption have significantly increased the demand for
efficient algorithm-to-hardware deployment. However, a persistent and
substantial gap remains between algorithm design and hardware implementation.
Bridging this gap traditionally requires extensive domain expertise and
time-consuming manual development, due to fundamental mismatches between
high-level programming languages like MATLAB and hardware description languages
(HDLs) such as Verilog-in terms of memory access patterns, data processing
manners, and datatype representations. To address this challenge, we propose
A2HCoder: a Hierarchical Algorithm-to-HDL Coding Agent, powered by large
language models (LLMs), designed to enable agile and reliable
algorithm-to-hardware translation. A2HCoder introduces a hierarchical framework
that enhances both robustness and interpretability while suppressing common
hallucination issues in LLM-generated code. In the horizontal dimension,
A2HCoder decomposes complex algorithms into modular functional blocks,
simplifying code generation and improving consistency. In the vertical
dimension, instead of relying on end-to-end generation, A2HCoder performs
step-by-step, fine-grained translation, leveraging external toolchains such as
MATLAB and Vitis HLS for debugging and circuit-level synthesis. This structured
process significantly mitigates hallucinations and ensures hardware-level
correctness. We validate A2HCoder through a real-world deployment case in the
5G wireless communication domain, demonstrating its practicality, reliability,
and deployment efficiency.

</details>


### [96] [PersonaTwin: A Multi-Tier Prompt Conditioning Framework for Generating and Evaluating Personalized Digital Twins](https://arxiv.org/abs/2508.10906)
*Sihan Chen,John P. Lalor,Yi Yang,Ahmed Abbasi*

Main category: cs.CL

TL;DR: 本文提出了PersonaTwin框架，通过融合人口特征、行为和心理测量数据，实现了高保真、个性化的数字孪生用户建模，显著提升了LLM在用户仿真中的准确性和公平性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）虽然能够对用户行为进行建模，但往往难以捕捉用户的多维个体特性，导致真实感和个性化不足。作者旨在解决这一痛点，通过更丰富的数据融合，提高用户仿真的真实性和情感细腻度。

Method: 引入了PersonaTwin——一个多层次提示条件框架，将人口统计、行为、心理测量等数据融合生成数字孪生，并利用医疗场景下8,500多人的数据集进行系统性评测。评估方法结合文本相似性指标及人口公平性检测，验证其模拟准确性和无偏性。

Result: PersonaTwin框架生成的用户仿真在保真度上达到类“oracle”效果。通过persona-twin训练的下游模型，在预测和公平性指标上能够接近真实个体数据训练模型，无论是在GPT-4o还是Llama基础上均表现出色。

Conclusion: PersonaTwin展示了数字孪生方法在提升用户仿真真实性和情感表现力的潜力，为个性化用户建模和行为分析提供了强有力的新工具。

Abstract: While large language models (LLMs) afford new possibilities for user modeling
and approximation of human behaviors, they often fail to capture the
multidimensional nuances of individual users. In this work, we introduce
PersonaTwin, a multi-tier prompt conditioning framework that builds adaptive
digital twins by integrating demographic, behavioral, and psychometric data.
Using a comprehensive data set in the healthcare context of more than 8,500
individuals, we systematically benchmark PersonaTwin against standard LLM
outputs, and our rigorous evaluation unites state-of-the-art text similarity
metrics with dedicated demographic parity assessments, ensuring that generated
responses remain accurate and unbiased. Experimental results show that our
framework produces simulation fidelity on par with oracle settings. Moreover,
downstream models trained on persona-twins approximate models trained on
individuals in terms of prediction and fairness metrics across both
GPT-4o-based and Llama-based models. Together, these findings underscore the
potential for LLM digital twin-based approaches in producing realistic and
emotionally nuanced user simulations, offering a powerful tool for personalized
digital user modeling and behavior analysis.

</details>


### [97] [gpt-oss-120b & gpt-oss-20b Model Card](https://arxiv.org/abs/2508.10925)
*OpenAI,:,Sandhini Agarwal,Lama Ahmad,Jason Ai,Sam Altman,Andy Applebaum,Edwin Arbus,Rahul K. Arora,Yu Bai,Bowen Baker,Haiming Bao,Boaz Barak,Ally Bennett,Tyler Bertao,Nivedita Brett,Eugene Brevdo,Greg Brockman,Sebastien Bubeck,Che Chang,Kai Chen,Mark Chen,Enoch Cheung,Aidan Clark,Dan Cook,Marat Dukhan,Casey Dvorak,Kevin Fives,Vlad Fomenko,Timur Garipov,Kristian Georgiev,Mia Glaese,Tarun Gogineni,Adam Goucher,Lukas Gross,Katia Gil Guzman,John Hallman,Jackie Hehir,Johannes Heidecke,Alec Helyar,Haitang Hu,Romain Huet,Jacob Huh,Saachi Jain,Zach Johnson,Chris Koch,Irina Kofman,Dominik Kundel,Jason Kwon,Volodymyr Kyrylov,Elaine Ya Le,Guillaume Leclerc,James Park Lennon,Scott Lessans,Mario Lezcano-Casado,Yuanzhi Li,Zhuohan Li,Ji Lin,Jordan Liss,Lily,Liu,Jiancheng Liu,Kevin Lu,Chris Lu,Zoran Martinovic,Lindsay McCallum,Josh McGrath,Scott McKinney,Aidan McLaughlin,Song Mei,Steve Mostovoy,Tong Mu,Gideon Myles,Alexander Neitz,Alex Nichol,Jakub Pachocki,Alex Paino,Dana Palmie,Ashley Pantuliano,Giambattista Parascandolo,Jongsoo Park,Leher Pathak,Carolina Paz,Ludovic Peran,Dmitry Pimenov,Michelle Pokrass,Elizabeth Proehl,Huida Qiu,Gaby Raila,Filippo Raso,Hongyu Ren,Kimmy Richardson,David Robinson,Bob Rotsted,Hadi Salman,Suvansh Sanjeev,Max Schwarzer,D. Sculley,Harshit Sikchi,Kendal Simon,Karan Singhal,Yang Song,Dane Stuckey,Zhiqing Sun,Philippe Tillet,Sam Toizer,Foivos Tsimpourlas,Nikhil Vyas,Eric Wallace,Xin Wang,Miles Wang,Olivia Watkins,Kevin Weil,Amy Wendling,Kevin Whinnery,Cedric Whitney,Hannah Wong,Lin Yang,Yu Yang,Michihiro Yasunaga,Kristen Ying,Wojciech Zaremba,Wenting Zhan,Cyril Zhang,Brian Zhang,Eddie Zhang,Shengjia Zhao*

Main category: cs.CL

TL;DR: 作者推出了gpt-oss-120b和gpt-oss-20b两个高效推理开源模型，具备较强的能力和较低的推理成本，并全面开源。


<details>
  <summary>Details</summary>
Motivation: 当前开源推理模型在准确性和推理成本之间存在权衡，且强agent能力和高效性兼得的方案稀缺，迫切需要推动相关模型能力和可用性的提升。

Method: 该工作采用高效的Mixture-of-Experts Transformer结构，结合大规模蒸馏与强化学习进行训练，并通过渲染的chat格式增强指令跟随和角色划分能力。同时，模型支持工具调用和开发者自定义函数。

Result: 两个模型在数学、代码和安全等多个基准测试上取得了优异成绩，并具备强agentic能力（如深度网页浏览、Python工具调用等）。

Conclusion: 他们以Apache 2.0协议开源了模型权重、推理实现和相关工具，旨在促进社区广泛使用和后续研究。

Abstract: We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models
that push the frontier of accuracy and inference cost. The models use an
efficient mixture-of-expert transformer architecture and are trained using
large-scale distillation and reinforcement learning. We optimize the models to
have strong agentic capabilities (deep research browsing, python tool use, and
support for developer-provided functions), all while using a rendered chat
format that enables clear instruction following and role delineation. Both
models achieve strong results on benchmarks ranging from mathematics, coding,
and safety. We release the model weights, inference implementations, tool
environments, and tokenizers under an Apache 2.0 license to enable broad use
and further research.

</details>


### [98] [Modeling and Detecting Company Risks from News: A Case Study in Bloomberg News](https://arxiv.org/abs/2508.10927)
*Jiaxin Pei,Soumya Vadlamannati,Liang-Kang Huang,Daniel Preotiuc-Pietro,Xinyu Hua*

Main category: cs.CL

TL;DR: 本研究提出了一个自动从新闻中提取公司风险因素的计算框架，基于七大方面对风险进行架构，并对多种机器学习模型进行了基准测试。结果显示，微调后的预训练语言模型识别能力优于零样本和少样本提示的大型语言模型。最终模型应用于大规模新闻数据分析，能帮助深入了解企业和行业的运作。


<details>
  <summary>Details</summary>
Motivation: 准确识别公司风险对于投资者和金融市场健康至关重要。目前，自动、高效地从大量新闻中挖掘风险信息仍具有挑战性。因此，作者希望通过自然语言处理和机器学习方法提升风险识别的自动化与准确性。

Method: 作者提出了一套涵盖供应链、法规、竞争等七大方面的新型风险架构，并对744篇新闻进行采样与人工标注，随后对多种主流机器学习和大语言模型（包括LLaMA-2）在风险识别任务上的表现进行系统比较。

Result: 实验表明，现有大语言模型即使使用零样本和少样本提示，表现仍不理想，而微调后的预训练模型在大部分风险类别上的表现更优。此外，作者用优秀模型对27.7万篇新闻进行了分析，获得了广泛的公司和行业运作洞察。

Conclusion: 自动化方法能够有效提升新闻中公司风险识别的规模和准确性，微调预训练模型在实际应用中的表现优于零样本大模型，对于投资与金融分析具有重要应用价值。

Abstract: Identifying risks associated with a company is important to investors and the
well-being of the overall financial market. In this study, we build a
computational framework to automatically extract company risk factors from news
articles. Our newly proposed schema comprises seven distinct aspects, such as
supply chain, regulations, and competitions. We sample and annotate 744 news
articles and benchmark various machine learning models. While large language
models have achieved huge progress in various types of NLP tasks, our
experiment shows that zero-shot and few-shot prompting state-of-the-art LLMs
(e.g. LLaMA-2) can only achieve moderate to low performances in identifying
risk factors. And fine-tuned pre-trained language models are performing better
on most of the risk factors. Using this model, we analyze over 277K Bloomberg
news articles and demonstrate that identifying risk factors from news could
provide extensive insight into the operations of companies and industries.

</details>


### [99] [Rule2Text: A Framework for Generating and Evaluating Natural Language Explanations of Knowledge Graph Rules](https://arxiv.org/abs/2508.10971)
*Nasim Shirvani-Mahdavi,Chengkai Li*

Main category: cs.CL

TL;DR: 本论文提出了Rule2Text框架，利用大语言模型自动将知识图谱挖掘所得的逻辑规则转化为自然语言解释，并通过系统性实验和人评验证其效果。


<details>
  <summary>Details</summary>
Motivation: 知识图谱中的规则虽然可提升其能力，但因复杂性和标签方式不同，难以被人类理解。因此，如何帮助用户更好地理解知识图谱规则成为亟需解决的问题。

Method: 作者提出Rule2Text框架，结合多个大语言模型(LLM)，探索零样本、少样本、类型嵌入、Chain-of-Thought等多种Prompt策略，通过AMIE 3.5.1挖掘规则，并在人类与LLM自动评估结合下，利用Gemini 2.0 Flash模型优化、人工反馈和类型推断模块提升表现，最终针对特定领域进行模型微调。

Result: 微调后的开放源代码Zephyr模型在解释质量上有显著提升，尤其在领域特定数据集上表现优异。LLM自动评估框架与人类评价高度一致，可扩展性强，并能支持没有类型信息的KG。代码与数据集全部开源。

Conclusion: 本文证明了大语言模型结合合理微调和自动评估手段，能够极大提升知识图谱规则说明的自然语言生成效果，提高人类对知识图谱的可访问性和可用性。

Abstract: Knowledge graphs (KGs) can be enhanced through rule mining; however, the
resulting logical rules are often difficult for humans to interpret due to
their inherent complexity and the idiosyncratic labeling conventions of
individual KGs. This work presents Rule2Text, a comprehensive framework that
leverages large language models (LLMs) to generate natural language
explanations for mined logical rules, thereby improving KG accessibility and
usability. We conduct extensive experiments using multiple datasets, including
Freebase variants (FB-CVT-REV, FB+CVT-REV, and FB15k-237) as well as the
ogbl-biokg dataset, with rules mined using AMIE 3.5.1. We systematically
evaluate several LLMs across a comprehensive range of prompting strategies,
including zero-shot, few-shot, variable type incorporation, and
Chain-of-Thought reasoning. To systematically assess models' performance, we
conduct a human evaluation of generated explanations on correctness and
clarity. To address evaluation scalability, we develop and validate an
LLM-as-a-judge framework that demonstrates strong agreement with human
evaluators. Leveraging the best-performing model (Gemini 2.0 Flash), LLM judge,
and human-in-the-loop feedback, we construct high-quality ground truth
datasets, which we use to fine-tune the open-source Zephyr model. Our results
demonstrate significant improvements in explanation quality after fine-tuning,
with particularly strong gains in the domain-specific dataset. Additionally, we
integrate a type inference module to support KGs lacking explicit type
information. All code and data are publicly available at
https://github.com/idirlab/KGRule2NL.

</details>


### [100] [Improving Text Style Transfer using Masked Diffusion Language Models with Inference-time Scaling](https://arxiv.org/abs/2508.10995)
*Tejomay Kishor Padole,Suyash P Awate,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 本文提出了一种基于判别器的推理时尺度调整方法，用于改进遮蔽扩散语言模型（MDM）的文本生成质量，实验验证了该方法在文本风格迁移任务上的有效性，并表明MDM优于自回归模型。


<details>
  <summary>Details</summary>
Motivation: 尽管遮蔽扩散语言模型（MDM）在离散数据生成方面性能突出，但现有方法在生成质量提升上仍有限，需要新的推理机制进一步改善生成效果，尤其在文本领域。

Method: 研究提出在MDM的去噪过程中，利用基于预训练嵌入模型的软值判别器，在每一步生成输出时进行筛选，以在推理阶段选择更优的候选生成结果，并可与现有的无分类器引导方法结合使用。

Result: 在标准文本风格迁移任务中，基于该判别器的生成策略显著提升了MDM的生成质量，相较于自回归语言模型表现更佳。

Conclusion: 采用基于判别器的推理尺度方法能有效提升MDM的生成能力，使其成为文本生成领域优于自回归模型的新选择。

Abstract: Masked diffusion language models (MDMs) have recently gained traction as a
viable generative framework for natural language. This can be attributed to its
scalability and ease of training compared to other diffusion model paradigms
for discrete data, establishing itself as the state-of-the-art
non-autoregressive generator for discrete data. Diffusion models, in general,
have shown excellent ability to improve the generation quality by leveraging
inference-time scaling either by increasing the number of denoising steps or by
using external verifiers on top of the outputs of each step to guide the
generation. In this work, we propose a verifier-based inference-time scaling
method that aids in finding a better candidate generation during the denoising
process of the MDM. Our experiments demonstrate the application of MDMs for
standard text-style transfer tasks and establish MDMs as a better alternative
to autoregressive language models. Additionally, we show that a simple
soft-value-based verifier setup for MDMs using off-the-shelf pre-trained
embedding models leads to significant gains in generation quality even when
used on top of typical classifier-free guidance setups in the existing
literature.

</details>


### [101] [SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth](https://arxiv.org/abs/2508.11009)
*Wenpeng Xing,Lanyi Wei,Haixiao Hu,Rongchang Li,Mohan Li,Changting Lin,Meng Han*

Main category: cs.CL

TL;DR: 论文指出当前大语言模型（LLM）在儿童和青少年中的应用面临严重的安全基准缺陷，作者提出并验证了一个专为未成年人设计的安全评测套件SproutBench。


<details>
  <summary>Details</summary>
Motivation: 随着LLM迅速进入儿童及青少年应用场景，现有的AI安全框架主要为成年人设计，忽视了未成年人的发展阶段脆弱性，因此亟需重新审视和完善。

Method: 作者开发了SproutBench评测套件，包含1283条与发展阶段相关的对抗性提示，针对不同年龄段（0-6、7-12、13-18岁）设计，覆盖如情感依赖、隐私泄露和模仿危险行为等风险，并对47种不同的LLM进行实证评估。

Result: 评估结果揭示了当前LLM在未成年人安全性上存在显著漏洞。作者还发现不同安全维度间存在强相关性（如安全与风险防范），以及交互性与年龄适当性存在负相关关系。

Conclusion: 这些发现为以儿童为中心的AI设计和部署提出了实用的改进建议，强调针对未成年人的AI安全评测和机制亟需加强。

Abstract: The rapid proliferation of large language models (LLMs) in applications
targeting children and adolescents necessitates a fundamental reassessment of
prevailing AI safety frameworks, which are largely tailored to adult users and
neglect the distinct developmental vulnerabilities of minors. This paper
highlights key deficiencies in existing LLM safety benchmarks, including their
inadequate coverage of age-specific cognitive, emotional, and social risks
spanning early childhood (ages 0--6), middle childhood (7--12), and adolescence
(13--18). To bridge these gaps, we introduce SproutBench, an innovative
evaluation suite comprising 1,283 developmentally grounded adversarial prompts
designed to probe risks such as emotional dependency, privacy violations, and
imitation of hazardous behaviors. Through rigorous empirical evaluation of 47
diverse LLMs, we uncover substantial safety vulnerabilities, corroborated by
robust inter-dimensional correlations (e.g., between Safety and Risk
Prevention) and a notable inverse relationship between Interactivity and Age
Appropriateness. These insights yield practical guidelines for advancing
child-centric AI design and deployment.

</details>


### [102] [Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics](https://arxiv.org/abs/2508.11017)
*Carter Blum,Katja Filipova,Ann Yuan,Asma Ghandeharioun,Julian Zimmert,Fred Zhang,Jessica Hoffmann,Tal Linzen,Martin Wattenberg,Lucas Dixon,Mor Geva*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型在跨语言知识迁移中的难题，发现其幻觉现象与多语种表征统一性的形成有关，并提出了调控这一过程的新方法及衡量指标。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在跨语言知识推理时常出现信息幻觉（即基于其他语言训练事实的问题难以正确解答），影响其多语种应用能力。理解这一现象的根因及其学习动力学过程，有助于完善和优化多语种 LLM 的表现。

Method: 作者在可控的实验环境下设计合成多语种数据集，从零训练小型 Transformer 模型，通过分析模型内部事实表征的分离与统一阶段，研究其与跨语言迁移能力的关系。同时，探究了训练数据的分布、互信息量以及语言可区分度对表征统一性的影响，并据此提出通过调整数据和分词方法，以调控表征统一性和知识迁移。还设计了新的评测指标与可视化工具。

Result: 实验显示，模型先经历分离表征阶段后部分出现跨语统一性，只有当表征统一程度足够高时，模型才能有效进行跨语言迁移。表征统一程度与事实-语言间的互信息和语言界限性高度相关。数据分布和分词策略的调制能有效改变表征统一性，进而控制跨语泛化效果。

Conclusion: 本研究揭示了 LLM 跨语知识迁移中的表征动态机理，确立了表征统一性的重要性，并为提升多语种 LLM 的迁移能力提供了可操作的方向。这不仅有助于更好地理解大模型的训练过程，也为未来改进跨语推理能力提供了新方法。

Abstract: Large language models (LLMs) struggle with cross-lingual knowledge transfer:
they hallucinate when asked in one language about facts expressed in a
different language during training. This work introduces a controlled setting
to study the causes and dynamics of this phenomenon by training small
Transformer models from scratch on synthetic multilingual datasets. We identify
a learning phase wherein a model develops either separate or unified
representations of the same facts across languages, and show that unification
is essential for cross-lingual transfer. We also show that the degree of
unification depends on mutual information between facts and training data
language, and on how easy it is to extract that language. Based on these
insights, we develop methods to modulate the level of cross-lingual transfer by
manipulating data distribution and tokenization, and we introduce metrics and
visualizations to formally characterize their effects on unification. Our work
shows how controlled settings can shed light on pre-training dynamics and
suggests new directions for improving cross-lingual transfer in LLMs.

</details>


### [103] [Hell or High Water: Evaluating Agentic Recovery from External Failures](https://arxiv.org/abs/2508.11027)
*Andrew Wang,Sophia Hager,Adi Asija,Daniel Khashabi,Nicholas Andrews*

Main category: cs.CL

TL;DR: 本论文提出了一种专门用于评估语言模型代理任务规划与应变能力的基准测试，重点考察在外部不可控失败出现时，模型是否能有效寻找替代方案。结果显示，现有模型在规划备选方案及适应环境反馈方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型被应用于越来越复杂的现实任务，其需要能够在庞大的搜索空间中制定并调整计划。研究动机在于评估和揭示主流语言代理在面对计划中断或外部失效时的应变与备选方案能力。

Method: 作者设计了一套基准测试：每个规划问题都需通过多种函数调用组合解决，代理需在4000多个函数中搜索相关函数，并根据函数输出或错误消息调整方案。测试场景中包含了如函数突然不可用等外部工作流失败，且保证任务依然可解。随后，作者系统性地分析了不同模型（包括开源和商业模型）面对变化的表现，并研究了搜索空间规模和模型规模对结果的影响。

Result: 研究发现，目前即便是先进的语言模型，虽能识别合适的函数和上下文，但在环境反馈出现后难以制定、执行有效的备选计划，即备选路径规划与适应能力较弱。模型常在已限制的搜索空间下也无法调整路线以达到目标。

Conclusion: 当前生成式语言模型在动态环境下的自适应和备选方案制定方面存在显著挑战。论文指出了这些失败的症结，对未来改进语言代理自适应规划能力提供了重要研究方向。

Abstract: As language model agents are applied to real world problems of increasing
complexity, they will be expected to formulate plans across large search
spaces. If those plans fail for reasons beyond their control, how well do
language agents search for alternative ways to achieve their goals? We devise a
specialized agentic planning benchmark to study this question. Each planning
problem is solved via combinations of function calls. The agent searches for
relevant functions from a set of over four thousand possibilities, and observes
environmental feedback in the form of function outputs or error messages. Our
benchmark confronts the agent with external failures in its workflow, such as
functions that suddenly become unavailable. At the same time, even with the
introduction of these failures, we guarantee that the task remains solvable.
Ideally, an agent's performance on the planning task should not be affected by
the presence of external failures. Overall, we find that language agents
struggle to formulate and execute backup plans in response to environment
feedback. While state-of-the-art models are often able to identify the correct
function to use in the right context, they struggle to adapt to feedback from
the environment and often fail to pursue alternate courses of action, even when
the search space is artificially restricted. We provide a systematic analysis
of the failures of both open-source and commercial models, examining the
effects of search space size, as well as the benefits of scaling model size in
our setting. Our analysis identifies key challenges for current generative
models as well as promising directions for future work.

</details>


### [104] [BIPOLAR: Polarization-based granular framework for LLM bias evaluation](https://arxiv.org/abs/2508.11061)
*Martin Pavlíček,Tomáš Filip,Petr Sosík*

Main category: cs.CL

TL;DR: 本文提出了一个可复用、细粒度、主题无关的框架，用于评估大语言模型（LLMs）在极化相关话题中的偏见，并通过俄罗斯—乌克兰战争的案例对多种主流LLM进行了实证分析。


<details>
  <summary>Details</summary>
Motivation: 现有的模型偏见检测与缓解方法虽取得进展，但针对极化相关、敏感话题（如政治、性别、民族、国家刻板印象）仍面临挑战，特别是在如何系统、可扩展地评估模型的偏见程度方面亟需更好的方法。

Method: 构建一个结合极化敏感情感度量和合成均衡冲突语句数据集的评估框架，并设定一套语义类别。利用该框架生成合成数据集，对多个开源与闭源LLM在俄罗斯—乌克兰战争话题上进行细粒度偏见分析，并研究不同修改提示词情况下的偏见变化。

Result: 发现主流LLM（如Llama-3、Mistral、GPT-4、Claude 3.5、Gemini 1.0）在整体上对乌克兰更为正面，但在不同语义类别下表现差异显著，不同模型间也存在行为分歧。提示词与国籍变化会带来偏见进一步加剧的现象。

Conclusion: 所提框架可自动生成评估数据集，实现细粒度偏见评估，适用于各种极化话题，并能与其他偏见评估方法互补，为未来相关研究和模型优化提供了新工具。

Abstract: Large language models (LLMs) are known to exhibit biases in downstream tasks,
especially when dealing with sensitive topics such as political discourse,
gender identity, ethnic relations, or national stereotypes. Although
significant progress has been made in bias detection and mitigation techniques,
certain challenges remain underexplored. This study proposes a reusable,
granular, and topic-agnostic framework to evaluate polarisation-related biases
in LLM (both open-source and closed-source). Our approach combines
polarisation-sensitive sentiment metrics with a synthetically generated
balanced dataset of conflict-related statements, using a predefined set of
semantic categories.
  As a case study, we created a synthetic dataset that focusses on the
Russia-Ukraine war, and we evaluated the bias in several LLMs: Llama-3,
Mistral, GPT-4, Claude 3.5, and Gemini 1.0. Beyond aggregate bias scores, with
a general trend for more positive sentiment toward Ukraine, the framework
allowed fine-grained analysis with considerable variation between semantic
categories, uncovering divergent behavioural patterns among models. Adaptation
to prompt modifications showed further bias towards preconceived language and
citizenship modification.
  Overall, the framework supports automated dataset generation and fine-grained
bias assessment, is applicable to a variety of polarisation-driven scenarios
and topics, and is orthogonal to many other bias-evaluation strategies.

</details>


### [105] [Approaching the Source of Symbol Grounding with Confluent Reductions of Abstract Meaning Representation Directed Graphs](https://arxiv.org/abs/2508.11068)
*Nicolas Goulet,Alexandre Blondin Massé,Moussa Abdendi*

Main category: cs.CL

TL;DR: 本文提出了一种将数字词典嵌入AMR图、基于大语言模型进行语义表示，并分析其降维图的性质与符号基础问题的关系。


<details>
  <summary>Details</summary>
Motivation: AMR作为语义表示方法，尚缺乏将真实数字词典集成的有效手段，以及对降维后图结构与语义基础关系的深入理解。研究旨在探索如何将复杂语料与现代AI语言模嵌入并简化，以助于更好地分析语义及符号基础问题。

Method: 1. 利用预训练大语言模型对真实数字词典内容进行处理。2. 将其语义信息嵌入到AMR有向无环图中。3. 采用保持回路空间的转换规则，对该图进行合流式（confluent）简化。4. 对简化后的图进行结构和符号语义分析。

Result: 成功将数字词典嵌入AMR，并构建了降维合流图。实验或分析结果表明，这些处理后结构有助于更有效地理解语义关系和分析符号语义基础问题。

Conclusion: 将数字词典与AMR语义图结合，并通过降维处理，可以促进AI系统对语义结构和符号基础问题的理解，为语义分析和符号推理领域提供新思路。

Abstract: Abstract meaning representation (AMR) is a semantic formalism used to
represent the meaning of sentences as directed acyclic graphs. In this paper,
we describe how real digital dictionaries can be embedded into AMR directed
graphs (digraphs), using state-of-the-art pre-trained large language models.
Then, we reduce those graphs in a confluent manner, i.e. with transformations
that preserve their circuit space. Finally, the properties of these reduces
digraphs are analyzed and discussed in relation to the symbol grounding
problem.

</details>


### [106] [Towards Reliable Multi-Agent Systems for Marketing Applications via Reflection, Memory, and Planning](https://arxiv.org/abs/2508.11120)
*Lorenzo Jaime Yu Flores,Junyi Shen,Xiaoyuan Gu*

Main category: cs.CL

TL;DR: 本文提出了一种多智能体框架RAMP，结合大型语言模型（LLM）和长期记忆，用于提升营销领域中受众筛选任务的可靠性和效果。通过迭代计划、验证和反思，显著提升系统表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在复杂任务中表现优异，但其在真实世界中的可靠性研究较少。作者希望解决这一不足，提升LLM在真实商业环境下的实用性。

Method: 作者提出RAMP框架，包括任务规划、工具调用、输出验证和改进意见生成，并引入客户知识库作为长期记忆，支持模型处理客户特定信息和历史查询。通过多轮验证和反思提升结果质量。

Result: 在88个评测任务上，RAMP使准确率提升了28个百分点。在针对更具挑战性的查询上，迭代验证和反思能将召回率提升约20个百分点，并提升用户满意度。

Conclusion: 该框架有效提升了LLM驱动的多智能体系统的实用性、准确率和用户满意度，为在动态、实际行业环境中部署可靠的AI系统提供了新的方法和实践经验。

Abstract: Recent advances in large language models (LLMs) enabled the development of AI
agents that can plan and interact with tools to complete complex tasks.
However, literature on their reliability in real-world applications remains
limited. In this paper, we introduce a multi-agent framework for a marketing
task: audience curation. To solve this, we introduce a framework called RAMP
that iteratively plans, calls tools, verifies the output, and generates
suggestions to improve the quality of the audience generated. Additionally, we
equip the model with a long-term memory store, which is a knowledge base of
client-specific facts and past queries. Overall, we demonstrate the use of LLM
planning and memory, which increases accuracy by 28 percentage points on a set
of 88 evaluation queries. Moreover, we show the impact of iterative
verification and reflection on more ambiguous queries, showing progressively
better recall (roughly +20 percentage points) with more verify/reflect
iterations on a smaller challenge set, and higher user satisfaction. Our
results provide practical insights for deploying reliable LLM-based systems in
dynamic, industry-facing environments.

</details>


### [107] [MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents](https://arxiv.org/abs/2508.11133)
*Tomer Wolfson,Harsh Trivedi,Mor Geva,Yoav Goldberg,Dan Roth,Tushar Khot,Ashish Sabharwal,Reut Tsarfaty*

Main category: cs.CL

TL;DR: 该论文提出了MoNaCo，一个包含1315个复杂与自然问题的新型基准，用于评估大语言模型（LLM）在处理真实、耗时信息检索任务中的能力。现有前沿LLM在MoNaCo上的最佳F1仅为61.2%。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评测中很少包含既自然又对人类非常耗时的信息检索类问答题，难以反映模型在真实复杂场景下的推理能力。为解决这一缺口，作者提出新的基准。

Method: 作者构建了由1315个自然、复杂、耗时问题组成的MoNaCo基准，通过分解注释流程（decomposed annotation pipeline）大规模挖掘并手动回答。评测了多种前沿LLM表现。

Result: 即使是最先进的LLM，在MoNaCo上的F1分数最大仅为61.2%，模型普遍存在召回率低及幻觉问题，表现明显落后于人类。

Conclusion: MoNaCo揭示了现有LLM在面对真实复杂信息检索任务时的显著能力不足。该基准为未来模型推理能力的提升提供了重要评测资源。

Abstract: Large language models (LLMs) are emerging as a go-to tool for querying
information. However, current LLM benchmarks rarely feature natural questions
that are both information-seeking as well as genuinely time-consuming for
humans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural
and complex questions that require dozens, and at times hundreds, of
intermediate steps to solve -- far more than any existing QA benchmark. To
build MoNaCo, we developed a decomposed annotation pipeline to elicit and
manually answer natural time-consuming questions at scale. Frontier LLMs
evaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and
hallucinations. Our results underscore the need for reasoning models that
better handle the complexity and sheer breadth of real-world
information-seeking questions -- with MoNaCo providing an effective resource
for tracking such progress. The MONACO benchmark, codebase, prompts and models
predictions are publicly available at: https://tomerwolgithub.github.io/monaco

</details>


### [108] [MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering](https://arxiv.org/abs/2508.11163)
*Hikaru Asano,Hiroki Ouchi,Akira Kasuga,Ryo Yonetani*

Main category: cs.CL

TL;DR: 本文提出了MobQA数据集，用于衡量大语言模型在理解人类移动数据语义上的能力，涵盖事实检索、推理选择和解释生成三类问题。实验展示当前大模型在事实检索表现良好，但在语义推理和解释题型上仍有明显不足，且轨迹长度影响效果。


<details>
  <summary>Details</summary>
Motivation: 虽然现有模型擅长预测人类移动轨迹，但尚不清楚它们能否真正理解这些行为背后的语义或原因。缺乏能系统反映大模型理解空间、时间与语义信息的数据集和评测框架。

Method: 作者构建了MobQA数据集，包含5800组高质量问答，涵盖三类问题：事实检索、推理选择和自由解释，问题涉及空间、时间和语义综合推理，均以真实GPS轨迹为基础。通过评测主流大语言模型，分析它们在不同题型和轨迹长度下的表现。

Result: 大模型在事实检索题型上表现优秀，但在多项选择推理和自由解释上的表现显著不足，并且轨迹长度增加会降低其整体理解和回答能力。

Conclusion: MobQA揭示了当前大语言模型在语义理解人类移动轨迹方面的成就与瓶颈，对未来提升模型推理与解释能力、促进相关研究具有借鉴意义。

Abstract: This paper presents MobQA, a benchmark dataset designed to evaluate the
semantic understanding capabilities of large language models (LLMs) for human
mobility data through natural language question answering.
  While existing models excel at predicting human movement patterns, it remains
unobvious how much they can interpret the underlying reasons or semantic
meaning of those patterns. MobQA provides a comprehensive evaluation framework
for LLMs to answer questions about diverse human GPS trajectories spanning
daily to weekly granularities. It comprises 5,800 high-quality question-answer
pairs across three complementary question types: factual retrieval (precise
data extraction), multiple-choice reasoning (semantic inference), and free-form
explanation (interpretive description), which all require spatial, temporal,
and semantic reasoning. Our evaluation of major LLMs reveals strong performance
on factual retrieval but significant limitations in semantic reasoning and
explanation question answering, with trajectory length substantially impacting
model effectiveness. These findings demonstrate the achievements and
limitations of state-of-the-art LLMs for semantic mobility
understanding.\footnote{MobQA dataset is available at
https://github.com/CyberAgentAILab/mobqa.}

</details>


### [109] [Overcoming Low-Resource Barriers in Tulu: Neural Models and Corpus Creation for OffensiveLanguage Identification](https://arxiv.org/abs/2508.11166)
*Anusha M D,Deepthi Vikram,Bharathi Raja Chakravarthi,Parameshwar R Hegde*

Main category: cs.CL

TL;DR: 本文首次构建了Tulu语社交媒体内容的攻击性语言识别基准数据集，并评估了多种深度学习模型，BiGRU-自注意力效果最佳。


<details>
  <summary>Details</summary>
Motivation: Tulu语是一种低资源语言，数字化内容日益增多，但相关NLP资源和研究极为稀缺。网络社交媒体上攻击性语言检测具有实际价值，因此迫切需要相关基准数据和研究。

Method: 作者从YouTube收集Tulu语混合码评论，构建并标注了高一致性（Krippendorff's alpha=0.984）、含3845条评论的OLI数据集，区分“非攻击、非Tulu、非定向攻击、定向攻击”四类。随后，基于该数据集评估了GRU、LSTM、BiGRU、BiLSTM、CNN、注意力变体及mBERT、XLM-RoBERTa等主流模型。

Result: BiGRU结合自注意力机制的模型实现了82%的准确率和0.81的宏F1分数，为最佳模型。相比之下，多语言预训练的transformer（如mBERT、XLM-R）表现不佳。

Conclusion: 本研究填补了Tulu等低资源、混合码语言在攻击性语言识别领域的空白，为后续相关NLP研究提供了基础资源和评测基线，同时表明现有多语言训练模型在此类任务上的局限。

Abstract: Tulu, a low-resource Dravidian language predominantly spoken in southern
India, has limited computational resources despite its growing digital
presence. This study presents the first benchmark dataset for Offensive
Language Identification (OLI) in code-mixed Tulu social media content,
collected from YouTube comments across various domains. The dataset, annotated
with high inter-annotator agreement (Krippendorff's alpha = 0.984), includes
3,845 comments categorized into four classes: Not Offensive, Not Tulu,
Offensive Untargeted, and Offensive Targeted. We evaluate a suite of deep
learning models, including GRU, LSTM, BiGRU, BiLSTM, CNN, and attention-based
variants, alongside transformer architectures (mBERT, XLM-RoBERTa). The BiGRU
model with self-attention achieves the best performance with 82% accuracy and a
0.81 macro F1-score. Transformer models underperform, highlighting the
limitations of multilingual pretraining in code-mixed, under-resourced
contexts. This work lays the foundation for further NLP research in Tulu and
similar low-resource, code-mixed languages.

</details>


### [110] [Personalized Distractor Generation via MCTS-Guided Reasoning Reconstruction](https://arxiv.org/abs/2508.11184)
*Tao Wu,Jingyuan Chen,Wang Lin,Jian Zhan,Mengze Li,Kun Kuang,Fei Wu*

Main category: cs.CL

TL;DR: 本文提出了一种个性化干扰项生成方法，通过学生过往答题记录，针对性地为每位学生定制更能揭示其特定理解误区的选择项。方法使用无需训练的两阶段框架，有效提升干扰项的个性化与评测诊断能力。


<details>
  <summary>Details</summary>
Motivation: 传统基于大模型的干扰项生成方法多聚焦于群体共性，无法充分反映个人推理误区，影响了教育评测的针对性和诊断效果。本文旨在解决如何根据个体学生答题历史生成切合其个人误区的干扰项问题。

Method: 提出了一个无需训练的两阶段框架：第一步，通过蒙特卡洛树搜索（MCTS）从学生过往错误答案中复原学生可能的推理路径，提取其特有的“误解原型”；第二步，利用该原型模拟学生在新题上的思路，生成契合其易错点的个性化干扰项。

Result: 实验证明，该方法在为140名学生生成合理且个性化的干扰项上表现最佳，同时对群体层面任务也有良好泛化，显示出较强的稳健性。

Conclusion: 所提方法不仅提升了个性化干扰项的生成效果，还有助于更精准地识别学生个体误区，为个性化教育测评和指导提供了新思路。

Abstract: Distractors, incorrect but plausible answer choices in multiple-choice
questions (MCQs), play a critical role in educational assessment by diagnosing
student misconceptions. Recent work has leveraged large language models (LLMs)
to generate shared, group-level distractors by learning common error patterns
across large student populations. However, such distractors often fail to
capture the diverse reasoning errors of individual students, limiting their
diagnostic effectiveness. To address this limitation, we introduce the task of
personalized distractor generation, which aims to generate tailored distractors
based on individual misconceptions inferred from each student's past
question-answering (QA) records, ensuring every student receives options that
effectively exposes their specific reasoning errors. While promising, this task
is challenging because each student typically has only a few QA records, which
often lack the student's underlying reasoning processes, making training-based
group-level approaches infeasible. To overcome this, we propose a training-free
two-stage framework. In the first stage, we construct a student-specific
misconception prototype by applying Monte Carlo Tree Search (MCTS) to recover
the student's reasoning trajectories from past incorrect answers. In the second
stage, this prototype guides the simulation of the student's reasoning on new
questions, enabling the generation of personalized distractors that align with
the student's recurring misconceptions. Experiments show that our approach
achieves the best performance in generating plausible, personalized distractors
for 140 students, and also effectively generalizes to group-level settings,
highlighting its robustness and adaptability.

</details>


### [111] [Novel Parasitic Dual-Scale Modeling for Efficient and Accurate Multilingual Speech Translation](https://arxiv.org/abs/2508.11189)
*Chenyang Le,Yinfeng Xia,Huiyan Li,Manhong Wang,Yutao Sun,Xingyang Ma,Yanmin Qian*

Main category: cs.CL

TL;DR: 论文提出了一种创新的双尺度方法（Parasitic Dual-Scale Approach），结合了推理加速与模型压缩，使多语种语音翻译模型在本地部署下实现高效推理及更佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有多语种语音翻译模型虽可处理多语言对，但参数量大，推理效率低，尤其在本地部署场景下难以平衡效率与性能。为解决此困境，提出更高效紧凑的多语种模型。

Method: 1. 提出Parasitic Dual-Scale Approach，结合增强版speculative sampling、模型压缩与知识蒸馏。2. 基于Whisper Medium模型进行多语种增强，构建whisperM2M模型。3. 集成创新KVSPN模块，实现加速与性能提升。4. 结合蒸馏提升整体速度和效果。

Result: 在六种主流语言的语音翻译任务上取得SOTA性能，在不损失BLEU分数前提下，KVSPN实现40%推理提速。结合蒸馏后，相比原始Whisper Medium模型，整体推理速度提升2.6倍，且性能更优。

Conclusion: 所提方法能有效加速多语种语音翻译模型推理，同时保持甚至提升翻译质量，适合作为本地高效部署方案。

Abstract: Recent advancements in speech-to-text translation have led to the development
of multilingual models capable of handling multiple language pairs
simultaneously. However, these unified models often suffer from large parameter
sizes, making it challenging to balance inference efficiency and performance,
particularly in local deployment scenarios. We propose an innovative Parasitic
Dual-Scale Approach, which combines an enhanced speculative sampling method
with model compression and knowledge distillation techniques. Building on the
Whisper Medium model, we enhance it for multilingual speech translation into
whisperM2M, and integrate our novel KVSPN module, achieving state-of-the-art
(SOTA) performance across six popular languages with improved inference
efficiency. KVSPN enables a 40\% speedup with no BLEU score degradation.
Combined with distillation methods, it represents a 2.6$\times$ speedup over
the original Whisper Medium with superior performance.

</details>


### [112] [E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection](https://arxiv.org/abs/2508.11197)
*Ahmad Mousavi,Yeganeh Abdollahinejad,Roberto Corizzo,Nathalie Japkowicz,Zois Boukouvalas*

Main category: cs.CL

TL;DR: 本文提出E-CaTCH框架，针对社交媒体多模态谣言检测的挑战，综合处理文本、视觉、时间动态，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态谣言检测方法忽视了跨时间、跨模态的事件结构，且面对类别不平衡和时序变化的问题，准确性和鲁棒性不足。

Method: E-CaTCH首先基于文本相似性和时序聚类，将社交媒体帖子聚为伪事件；在每个事件内，先用预训练的BERT和ResNet提取文本和图像特征，然后通过自注意力机制聚合单模态信息，再用双向跨模态注意力对齐文本和图像，采用软门控机制融合，得到每帖上下文感知表示。为了建模事件的时序演变，对事件按时间滑窗分段，用趋势感知的LSTM（引入语义变化与趋势信号）编码故事进展。分类在事件级别进行，并通过自适应类别加权、时序一致性正则、难例挖掘等手段缓解类别不平衡和提高稳定性。

Result: E-CaTCH在Fakeddit、IND和COVID-19 MISINFOGRAPH等主流多模态谣言数据集上均优于最新基准方法，跨数据集实验显示其鲁棒性和泛化能力强。

Conclusion: E-CaTCH通过事件级多模态融合和时序建模，实现了对社交媒体谣言更高效稳健的检测，对实际复杂场景具有广泛适用性。

Abstract: Detecting multimodal misinformation on social media remains challenging due
to inconsistencies between modalities, changes in temporal patterns, and
substantial class imbalance. Many existing methods treat posts independently
and fail to capture the event-level structure that connects them across time
and modality. We propose E-CaTCH, an interpretable and scalable framework for
robustly detecting misinformation. If needed, E-CaTCH clusters posts into
pseudo-events based on textual similarity and temporal proximity, then
processes each event independently. Within each event, textual and visual
features are extracted using pre-trained BERT and ResNet encoders, refined via
intra-modal self-attention, and aligned through bidirectional cross-modal
attention. A soft gating mechanism fuses these representations to form
contextualized, content-aware embeddings of each post. To model temporal
evolution, E-CaTCH segments events into overlapping time windows and uses a
trend-aware LSTM, enhanced with semantic shift and momentum signals, to encode
narrative progression over time. Classification is performed at the event
level, enabling better alignment with real-world misinformation dynamics. To
address class imbalance and promote stable learning, the model integrates
adaptive class weighting, temporal consistency regularization, and hard-example
mining. The total loss is aggregated across all events. Extensive experiments
on Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH
consistently outperforms state-of-the-art baselines. Cross-dataset evaluations
further demonstrate its robustness, generalizability, and practical
applicability across diverse misinformation scenarios.

</details>


### [113] [Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering](https://arxiv.org/abs/2508.11247)
*Changjian Wang,Weihong Deng,Weili Guan,Quan Lu,Ning Jiang*

Main category: cs.CL

TL;DR: 本文提出了一种新的基于超图的RAG方法（HGRAG），实现结构和语义信息的跨粒度整合，使多跳问答检索更高效、更准确。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成（RAG）方法在多跳问答（MHQA）中只关注粗粒度的文本语义相似度，忽视了知识之间的结构联系，导致效果受限。GraphRAG等虽然引入知识图谱捕捉结构，但过度依赖结构和细粒度词级检索，又忽略了语义信息的利用。因此，如何同时整合结构和语义，提升MHQA性能，是核心动机。

Method: 方法上，作者提出基于超图的RAG方法HGRAG：结构上将细粒度实体做节点，粗粒度段落作超边，借助共享实体建立知识超图。语义上，设计超图检索方法，融合实体级相似度与段落级相似度，并利用超图扩散进行信息整合。最后追加检索增强模块，进一步在结构和语义层面优化检索结果，为大模型生成答案提供更优上下文。

Result: 实验结果表明，该方法在多个权威数据集上均优于当前最优方案，不仅在多跳问答准确率上取得新高，而且检索效率提升6倍。

Conclusion: HGRAG方法有效实现了跨粒度结构与语义信息整合，显著提升了多跳问答任务的性能与效率，对结构化知识检索和生成任务具有良好推广价值。

Abstract: Multi-hop question answering (MHQA) requires integrating knowledge scattered
across multiple passages to derive the correct answer. Traditional
retrieval-augmented generation (RAG) methods primarily focus on coarse-grained
textual semantic similarity and ignore structural associations among dispersed
knowledge, which limits their effectiveness in MHQA tasks. GraphRAG methods
address this by leveraging knowledge graphs (KGs) to capture structural
associations, but they tend to overly rely on structural information and
fine-grained word- or phrase-level retrieval, resulting in an underutilization
of textual semantics. In this paper, we propose a novel RAG approach called
HGRAG for MHQA that achieves cross-granularity integration of structural and
semantic information via hypergraphs. Structurally, we construct an entity
hypergraph where fine-grained entities serve as nodes and coarse-grained
passages as hyperedges, and establish knowledge association through shared
entities. Semantically, we design a hypergraph retrieval method that integrates
fine-grained entity similarity and coarse-grained passage similarity via
hypergraph diffusion. Finally, we employ a retrieval enhancement module, which
further refines the retrieved results both semantically and structurally, to
obtain the most relevant passages as context for answer generation with the
LLM. Experimental results on benchmark datasets demonstrate that our approach
outperforms state-of-the-art methods in QA performance, and achieves a
6$\times$ speedup in retrieval efficiency.

</details>


### [114] [UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?](https://arxiv.org/abs/2508.11260)
*Mukund Choudhary,KV Aditya Srivatsa,Gaurja Aeron,Antara Raaghavi Bhattacharya,Dang Khoa Dang Dinh,Ikhlasul Akmal Hanif,Daria Kotova,Ekaterina Kochmar,Monojit Choudhury*

Main category: cs.CL

TL;DR: 本文评估了大语言模型（LLMs）在41种低资源语言的语奥赛语言难题上的表现，发现其在形态复杂性高的题目上表现较差，对英语常见语言特征相关题目表现较好。将词划分为词素作为预处理可显著提升解题能力，凸显需要更智能的、针对具体语言的分词器。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在许多推理任务上表现优秀，但在语言奥林匹克题目上的表现一直很差，尤其是在低资源语言上。现有评测往往掺杂英语等高资源语言的背景知识，缺乏纯净环境系统性分析LLMs在语言推理上的弱点。本文的目的是通过标注语言学特征，深入分析LLMs在低资源语言难题上的表现，找出其根本难点。

Method: 作者收集了来自41种低资源语言的629个语言奥林匹克题目，结合语言学特征进行了细致标注。接着，系统评估多个LLMs在不同题型、特征复杂度下的表现，并探索分词为词素的预处理对模型表现的影响。

Result: 分析发现，大语言模型在高形态复杂度的题目上表现较弱，但在涉及英语常见特征的题目上表现较好。通过将词语分割为词素做预处理，LLMs的解题成功率提升明显。

Conclusion: 当前LLMs在处理低资源语言、尤其具有复杂词法结构的任务时仍有明显短板。更精细且语言相关的分词策略能够缓解部分难题，对今后的模型改进与低资源语言建模有重要启示。

Abstract: Large language models (LLMs) have demonstrated potential in reasoning tasks,
but their performance on linguistics puzzles remains consistently poor. These
puzzles, often derived from Linguistics Olympiad (LO) contests, provide a
minimal contamination environment to assess LLMs' linguistic reasoning
abilities across low-resource languages. This work analyses LLMs' performance
on 629 problems across 41 low-resource languages by labelling each with
linguistically informed features to unveil weaknesses. Our analyses show that
LLMs struggle with puzzles involving higher morphological complexity and
perform better on puzzles involving linguistic features that are also found in
English. We also show that splitting words into morphemes as a pre-processing
step improves solvability, indicating a need for more informed and
language-specific tokenisers. These findings thus offer insights into some
challenges in linguistic reasoning and modelling of low-resource languages.

</details>


### [115] [LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought](https://arxiv.org/abs/2508.11280)
*Ruiyan Qi,Congding Wen,Weibo Zhou,Shangsong Liang,Lingbo Li*

Main category: cs.CL

TL;DR: 本文提出了一种无需标注数据、面向旅游领域大语言模型（LLM）评估的新方法——LETToT，通过专家设计的推理结构来替代标注集，实现对LLM能力的系统化评测。


<details>
  <summary>Details</summary>
Motivation: 传统评估旅游等专用领域LLM的方式依赖大量人工标注数据，成本高且易受模型幻觉影响，限制了评估的可扩展性与客观性。为解决这一难题，作者希望找到可规模化、无须标注的评测方法。

Method: 提出了LETToT框架。首先，专家将通用的模型表现维度和推理过程细化为层次化ToT（Tree-of-Thought）结构，并通过专家反馈不断优化。然后利用该结构，无需额外人工标注，直接评估各类规模和结构的LLM。

Result: 通过对优化的专家ToT系统性实验，相较基线方法，质量提升4.99-14.15%。实验还表明，在旅游专业领域中，模型规模提升仍然带来益处，但具备显式推理能力的小模型也能弥补与大模型之间的差距。对于72B以下模型，显式推理结构显著提升了准确率和简洁性。

Conclusion: LETToT实现了领域LLM评价范式的转型，在无需数据标注的前提下提供了强健、可扩展的替代方案，为后续领域大模型评估与优化指明了新思路。

Abstract: Evaluating large language models (LLMs) in specific domain like tourism
remains challenging due to the prohibitive cost of annotated benchmarks and
persistent issues like hallucinations. We propose $\textbf{L}$able-Free
$\textbf{E}$valuation of LLM on $\textbf{T}$ourism using Expert
$\textbf{T}$ree-$\textbf{o}$f-$\textbf{T}$hought (LETToT), a framework that
leverages expert-derived reasoning structures-instead of labeled data-to access
LLMs in tourism. First, we iteratively refine and validate hierarchical ToT
components through alignment with generic quality dimensions and expert
feedback. Results demonstrate the effectiveness of our systematically optimized
expert ToT with 4.99-14.15\% relative quality gains over baselines. Second, we
apply LETToT's optimized expert ToT to evaluate models of varying scales
(32B-671B parameters), revealing: (1) Scaling laws persist in specialized
domains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g.,
DeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit
reasoning architectures outperform counterparts in accuracy and conciseness
($p<0.05$). Our work established a scalable, label-free paradigm for
domain-specific LLM evaluation, offering a robust alternative to conventional
annotated benchmarks.

</details>


### [116] [ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection](https://arxiv.org/abs/2508.11281)
*Axel Delaval,Shujian Yang,Haicheng Wang,Han Qiu,Jialiang Lu*

Main category: cs.CL

TL;DR: 本论文提出了TOXIFRENCH数据集，并通过新颖的方法提升了法语有害内容检测的效果，结果显示小型语言模型(SLMs)在该任务上表现优异，并引入了改进的微调策略，最终实现了领先于多种主流大模型的性能。


<details>
  <summary>Details</summary>
Motivation: 在法语环境下，有害内容检测因缺乏大规模、文化相关的数据集而发展滞后，为了解决这一问题，论文着手构建高质量、公开的法语有害评论数据集，并探索模型性能提升方法。

Method: 提出利用大型语言模型(LLM)预标注和人工校验相结合，减少人工标注负担，仅需10%人工标签。测试多种模型，在此基础上提出了一种结合Chain-of-Thought(CoT)和动态加权损失的新型微调策略，提升模型决策的准确性和忠实性。

Result: 构建了包含53,622条法语评论的TOXIFRENCH数据集。实验发现，小型语言模型在鲁棒性和泛化能力上优于部分大模型。使用新微调策略，4B模型F1分数提升13%，并超过GPT-40、Gemini-2.5等先进大模型。

Conclusion: TOXIFRENCH为法语有害内容检测提供了基础设施，新颖的训练方法不仅提升了单一语言表现，也展现了跨语言能力，显示该技术可用于其他语言及安全相关任务。

Abstract: Detecting toxic content using language models is crucial yet challenging.
While substantial progress has been made in English, toxicity detection in
French remains underdeveloped, primarily due to the lack of culturally
relevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new
public benchmark of 53,622 French online comments, constructed via a
semi-automated annotation pipeline that reduces manual labeling to only 10%
through high-confidence LLM-based pre-annotation and human verification. Then,
we benchmark a broad range of models and uncover a counterintuitive insight:
Small Language Models (SLMs) outperform many larger models in robustness and
generalization under the toxicity detection task. Motivated by this finding, we
propose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic
weighted loss that progressively emphasizes the model's final decision,
significantly improving faithfulness. Our fine-tuned 4B model achieves
state-of-the-art performance, improving its F1 score by 13% over its baseline
and outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a
cross-lingual toxicity benchmark demonstrates strong multilingual ability,
suggesting that our methodology can be effectively extended to other languages
and safety-critical classification tasks.

</details>


### [117] [AI in Mental Health: Emotional and Sentiment Analysis of Large Language Models' Responses to Depression, Anxiety, and Stress Queries](https://arxiv.org/abs/2508.11285)
*Arya VarastehNezhad,Reza Tavasoli,Soroush Elyasi,MohammadHossein LotfiNia,Hamed Farbeh*

Main category: cs.CL

TL;DR: 本研究评估了八种大型语言模型（LLM）在回答与抑郁、焦虑和压力相关问题时，展现出的情感和情绪差异，强调了模型选用对心理健康领域应用的重要性。


<details>
  <summary>Details</summary>
Motivation: 鉴于越来越多人通过LLM获取心理健康信息，作者希望了解不同LLM在应对心理健康相关问题时，情感表达和情绪反应是否存在显著差别，以及这些差异可能对用户体验和效果产生的影响。

Method: 研究选取了Claude Sonnet、Copilot、Gemini Pro、GPT-4o、GPT-4o mini、Llama、Mixtral和Perplexity八种主流LLM。针对抑郁、焦虑、压力主题及六类用户画像（基线、女性、男性、青年、老年、大学生），提出20个实际问题，让模型生成总计2880条回答，并利用先进工具对这些回答中的情感与情绪进行评分和分析。

Result: 所有模型输出中，乐观、恐惧和悲伤情绪最为突出，保持中性情感值较高。感激、快乐和信任处于中等水平，而愤怒、厌恶和爱则较少出现。不同模型表现出独特的情感特征，如Mixtral表达最高的负面情绪（失望、烦恼、悲伤），而Llama输出最乐观和愉快。心理健康类型强烈影响情绪反应（如焦虑类问题恐惧值极高，抑郁类问题悲伤和负性情感最高，压力类问题乐观、愉悦和信任值增高）。用户画像（例如年龄、性别）对情感影响较小。统计分析验证了模型差异和症状类型差异显著，人口学特征影响甚微。

Conclusion: LLM在心理健康功能应用中表现出各自独特的情感倾向，模型选择对于用户体验和应用效果至关重要。人口学特征对模型输出情感影响有限，但不同LLM和不同心理症状相关问题会显著影响回答内容，凸显个性化模型应用的重要性。

Abstract: Depression, anxiety, and stress are widespread mental health concerns that
increasingly drive individuals to seek information from Large Language Models
(LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, Gemini
Pro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twenty
pragmatic questions about depression, anxiety, and stress when those questions
are framed for six user profiles (baseline, woman, man, young, old, and
university student). The models generated 2,880 answers, which we scored for
sentiment and emotions using state-of-the-art tools. Our analysis revealed that
optimism, fear, and sadness dominated the emotional landscape across all
outputs, with neutral sentiment maintaining consistently high values.
Gratitude, joy, and trust appeared at moderate levels, while emotions such as
anger, disgust, and love were rarely expressed. The choice of LLM significantly
influenced emotional expression patterns. Mixtral exhibited the highest levels
of negative emotions including disapproval, annoyance, and sadness, while Llama
demonstrated the most optimistic and joyful responses. The type of mental
health condition dramatically shaped emotional responses: anxiety prompts
elicited extraordinarily high fear scores (0.974), depression prompts generated
elevated sadness (0.686) and the highest negative sentiment, while
stress-related queries produced the most optimistic responses (0.755) with
elevated joy and trust. In contrast, demographic framing of queries produced
only marginal variations in emotional tone. Statistical analyses confirmed
significant model-specific and condition-specific differences, while
demographic influences remained minimal. These findings highlight the critical
importance of model selection in mental health applications, as each LLM
exhibits a distinct emotional signature that could significantly impact user
experience and outcomes.

</details>


### [118] [SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through Task-Specific Trajectory](https://arxiv.org/abs/2508.11290)
*Utsav Maskey,Sumit Yadav,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 本文针对当前大语言模型(LLM)安全机制导致的过度拒绝（over-refusal）现象，提出了一种新的推理层面方法SafeConstellations，同时在大幅减少过度拒绝的情况下保持模型实用性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM安全机制的加强，模型倾向于对表面上类似有害但实际无害的指令作出错误拒绝。这种现象大大降低了模型在实际生产中的效用，尤其在需要反复使用通用模板或特定任务（如情感分析、翻译）的场景中。作者希望解决如何合理减少这种过度拒绝行为。

Method: 作者首先系统性评估了LLM对重构后、看似无害而实为有害的指令的拒绝表现，并对模型机制进行了嵌入空间的分析。发现不同任务在层次演化中呈现出可预测的路径模式。基于此，提出SafeConstellations：在推理时动态追踪任务嵌入轨迹，根据特定任务的模式，将轨迹引导至“不拒绝”的路径，且仅对易过度拒绝的任务激活，不干扰模型整体表现。

Result: 实验表明，在过度拒绝不影响安全的前提下，SafeConstellations方法可将过度拒绝率降低高达73%，且对模型的其他能力影响很小。

Conclusion: SafeConstellations能有效解决LLM在保障安全的同时，对无害指令的过度拒绝问题，是缓解生产环境下拒绝率过高的稳健方法。

Abstract: LLMs increasingly exhibit over-refusal behavior, where safety mechanisms
cause models to reject benign instructions that superficially resemble harmful
content. This phenomena diminishes utility in production applications that
repeatedly rely on common prompt templates or applications that frequently rely
on LLMs for specific tasks (e.g. sentiment analysis, language translation).
Through comprehensive evaluation, we demonstrate that LLMs still tend to refuse
responses to harmful instructions when those instructions are reframed to
appear as benign tasks. Our mechanistic analysis reveal that LLMs follow
distinct "constellation" patterns in embedding space as representations
traverse layers, with each task maintaining consistent trajectories that shift
predictably between refusal and non-refusal cases. We introduce
SafeConstellations, an inference-time trajectory-shifting approach that tracks
task-specific trajectory patterns and guides representations toward non-refusal
pathways. By selectively guiding model behavior only on tasks prone to
over-refusal, and by preserving general model behavior, our method reduces
over-refusal rates by up to 73% with minimal impact on utility-offering a
principled approach to mitigating over-refusals.

</details>


### [119] [SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems](https://arxiv.org/abs/2508.11310)
*Beichen Guo,Zhiyuan Wen,Yu Yang,Peng Gao,Ruosong Yang,Jiaxing Shen*

Main category: cs.CL

TL;DR: 本文提出了SGSimEval，这是一套综合评测自动学术综述生成系统的新基准，能更全面、客观地评估大语言模型生成调查综述的质量。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型及检索增强生成（RAG）和多智能体系统（MAS）技术的发展，用LLM自动合成学术综述已可行，对应地对其进行有效评测也变得更加迫切。现有评测方法存在偏见、缺乏人类偏好考虑、过度依赖LLM裁判等问题。

Method: 提出SGSimEval评测基准，从大纲、内容、参考文献三个维度，结合LLM评分和定量指标多元化评估自动综述生成系统。引入强化与人类偏好相关的评价标准，兼顾系统固有质量及与人工综述的相似性。

Result: 大量实验证明，目前ASG系统在生成大纲部分已接近或优于人类，但在内容和参考文献生成方面仍有改进空间。同时，SGSimEval的评测指标与人工评估结果高度一致。

Conclusion: SGSimEval能够为自动学术综述生成系统提供更加可靠、多元和具有人类偏好的评测方法，有助于推动该领域的研究和发展。

Abstract: The growing interest in automatic survey generation (ASG), a task that
traditionally required considerable time and effort, has been spurred by recent
advances in large language models (LLMs). With advancements in
retrieval-augmented generation (RAG) and the rising popularity of multi-agent
systems (MASs), synthesizing academic surveys using LLMs has become a viable
approach, thereby elevating the need for robust evaluation methods in this
domain. However, existing evaluation methods suffer from several limitations,
including biased metrics, a lack of human preference, and an over-reliance on
LLMs-as-judges. To address these challenges, we propose SGSimEval, a
comprehensive benchmark for Survey Generation with Similarity-Enhanced
Evaluation that evaluates automatic survey generation systems by integrating
assessments of the outline, content, and references, and also combines
LLM-based scoring with quantitative metrics to provide a multifaceted
evaluation framework. In SGSimEval, we also introduce human preference metrics
that emphasize both inherent quality and similarity to humans. Extensive
experiments reveal that current ASG systems demonstrate human-comparable
superiority in outline generation, while showing significant room for
improvement in content and reference generation, and our evaluation metrics
maintain strong consistency with human assessments.

</details>


### [120] [LLM Compression: How Far Can We Go in Balancing Size and Performance?](https://arxiv.org/abs/2508.11318)
*Sahil Sk,Debasish Dhal,Sonal Khosla,Sk Shahid,Sambit Shekhar,Akash Dhaka,Shantipriya Parida,Dilip K. Prasad,Ondřej Bojar*

Main category: cs.CL

TL;DR: 本文评估了两种低比特量化技术（GSQ和GPTQ）对三种大语言模型在不同NLP任务中的表现，讨论了模型压缩后性能与效率的权衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的计算和存储开销较高，量化方法可以降低这些成本，使其更易于实际部署，但需要权衡精度和效率。作者希望分析4-bit量化在多个任务上的实际效果，为未来应用和研究提供参考。

Method: 将GSQ和GPTQ这两种4比特量化方法分别应用于LLaMA 1B、Qwen 0.5B以及PHI 1.5B三种模型，并在MS MARCO、BoolQ、GSM8K等标准数据集进行多任务评测。主要考察模型量化后的精度、推理延迟和吞吐量等指标。

Result: 实验结果展示了不同量化技术和模型规模在精度和效率上的具体表现，帮助用户根据实际应用需求选用合适的量化方法。

Conclusion: 4比特量化方法可大幅提升模型的部署效率，但任务表现会有一定损失。GSQ和GPTQ各有优劣，适用于不同场景。所得到的测评数据为后续的量化研究及真实应用提供了有价值的基准。

Abstract: Quantization is an essential and popular technique for improving the
accessibility of large language models (LLMs) by reducing memory usage and
computational costs while maintaining performance. In this study, we apply
4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer
Quantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating their
impact across multiple NLP tasks. We benchmark these models on MS MARCO
(Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K
(Mathematical Reasoning) datasets, assessing both accuracy and efficiency
across various tasks. The study measures the trade-offs between model
compression and task performance, analyzing key evaluation metrics, namely
accuracy, inference latency, and throughput (total output tokens generated per
second), providing insights into the suitability of low-bit quantization for
real-world deployment. Using the results, users can then make suitable
decisions based on the specifications that need to be met. We discuss the pros
and cons of GSQ and GPTQ techniques on models of different sizes, which also
serve as a benchmark for future experiments.

</details>


### [121] [SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated Text via Spectral Analysis](https://arxiv.org/abs/2508.11343)
*Haitong Luo,Weiyao Zhang,Suhang Wang,Wenji Zou,Chungang Lin,Xuying Meng,Yujun Zhang*

Main category: cs.CL

TL;DR: 该论文提出了一种基于频域信号分析的LLM生成文本检测方法，利用DFT总能量区分人类与大语言模型生成的文本，在准确性和效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成文本质量提升，现有的检测方法多依赖表层统计特征，难以从根本上区分人类与AI生成内容，因此需要更稳健和高效的检测手段。

Method: 作者将文本检测问题转化为信号处理问题，重点分析了文本生成时每个token对数概率序列的频谱特性。通过全局离散傅里叶变换（DFT）与短时傅里叶变换（STFT）分析，发现人写文本的总能量更高。基于此，提出了SpecDetect，利用DFT总能量特征，以及包含采样差异机制的SpecDetect++提升鲁棒性。

Result: 实验在多个数据集上验证方法有效性，SpecDetect及其增强版在检测准确性上超过当前最优模型，且运行速度提升近一倍。

Conclusion: 信号处理中的经典工具能为LLM文本检测提供高效、可解释的新方法，为现代文本产出识别难题带来强有力的解决思路。

Abstract: The proliferation of high-quality text from Large Language Models (LLMs)
demands reliable and efficient detection methods. While existing training-free
approaches show promise, they often rely on surface-level statistics and
overlook fundamental signal properties of the text generation process. In this
work, we reframe detection as a signal processing problem, introducing a novel
paradigm that analyzes the sequence of token log-probabilities in the frequency
domain. By systematically analyzing the signal's spectral properties using the
global Discrete Fourier Transform (DFT) and the local Short-Time Fourier
Transform (STFT), we find that human-written text consistently exhibits
significantly higher spectral energy. This higher energy reflects the
larger-amplitude fluctuations inherent in human writing compared to the
suppressed dynamics of LLM-generated text. Based on this key insight, we
construct SpecDetect, a detector built on a single, robust feature from the
global DFT: DFT total energy. We also propose an enhanced version,
SpecDetect++, which incorporates a sampling discrepancy mechanism to further
boost robustness. Extensive experiments demonstrate that our approach
outperforms the state-of-the-art model while running in nearly half the time.
Our work introduces a new, efficient, and interpretable pathway for
LLM-generated text detection, showing that classical signal processing
techniques offer a surprisingly powerful solution to this modern challenge.

</details>


### [122] [Feedback Indicators: The Alignment between Llama and a Teacher in Language Learning](https://arxiv.org/abs/2508.11364)
*Sylvio Rüdian,Yassin Elsir,Marvin Kretschmer,Sabine Cayrou,Niels Pinkwart*

Main category: cs.CL

TL;DR: 本研究探索如何利用大型语言模型（如Llama 3.1）自动从学生作业中抽取反馈指标，为自动生成高质量生成性反馈提供基础。


<details>
  <summary>Details</summary>
Motivation: 自动化反馈能帮助学生及时获得有针对性的学习建议，也能减轻教师的工作负担，使其专注于更具策略性和个性化的教学任务。高质量反馈的前提是准确提取出评价指标，而手动提取耗时耗力，因此自动化具有现实需求。

Method: 本研究首先利用Llama 3.1大语言模型，从语言学习课程学生提交的作业中自动提取反馈指标，并对比这些由模型生成的指标与教师人工评分在不同评价标准下的一致性及相关性。

Result: 结果显示，LLM生成的指标与教师评分之间有显著且较强的相关性，即使在未预期的指标与标准组合下也表现良好。

Conclusion: 本文提出的方法为利用LLM从学生作业中提取反馈指标打下了良好基础，未来可基于此实现自动解释型和透明的生成性反馈系统。

Abstract: Automated feedback generation has the potential to enhance students' learning
progress by providing timely and targeted feedback. Moreover, it can assist
teachers in optimizing their time, allowing them to focus on more strategic and
personalized aspects of teaching. To generate high-quality, information-rich
formative feedback, it is essential first to extract relevant indicators, as
these serve as the foundation upon which the feedback is constructed. Teachers
often employ feedback criteria grids composed of various indicators that they
evaluate systematically. This study examines the initial phase of extracting
such indicators from students' submissions of a language learning course using
the large language model Llama 3.1. Accordingly, the alignment between
indicators generated by the LLM and human ratings across various feedback
criteria is investigated. The findings demonstrate statistically significant
strong correlations, even in cases involving unanticipated combinations of
indicators and criteria. The methodology employed in this paper offers a
promising foundation for extracting indicators from students' submissions using
LLMs. Such indicators can potentially be utilized to auto-generate explainable
and transparent formative feedback in future research.

</details>


### [123] [When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs](https://arxiv.org/abs/2508.11383)
*Mikhail Seleznyov,Mikhail Chaichuk,Gleb Ershov,Alexander Panchenko,Elena Tutubalina,Oleg Somov*

Main category: cs.CL

TL;DR: 本文系统评估了提升大语言模型（LLMs）对提示词微小变化鲁棒性的5种方法，并在多种主流模型和任务上进行了统一实验。


<details>
  <summary>Details</summary>
Motivation: 大语言模型对提示词的措辞和格式等细微非语义变化高度敏感，影响模型在实际应用中的稳定性和可靠性，因此需要系统研究提升其鲁棒性的方法。

Method: 作者统一比较了5种提升提示鲁棒性的方法，涵盖fine-tuning和in-context learning，通过在Llama、Qwen、Gemma等8个模型和52项Natural Instructions任务上基准测试，并模拟多种分布转移情境检验方法的泛化能力。同时也对GPT-4.1和DeepSeek V3等前沿模型进行了拓展评估。

Result: 各鲁棒性方法在不同模型和任务中效果存在差异。实验详细揭示了各种方法在不同分布转移场景下的优劣势，并为不同类型模型的提示鲁棒性提升提出了实证参考。

Conclusion: 本文为现有鲁棒性提升方法的实际表现提供了有价值的实证基础，为开发和部署更加稳定可靠的大语言模型提供了指导，帮助从业者做出明智选择。

Abstract: Large Language Models (LLMs) are highly sensitive to subtle, non-semantic
variations in prompt phrasing and formatting. In this work, we present the
first systematic evaluation of 5 methods for improving prompt robustness within
a unified experimental framework. We benchmark these techniques on 8 models
from Llama, Qwen and Gemma families across 52 tasks from Natural Instructions
dataset. Our evaluation covers robustness methods from both fine-tuned and
in-context learning paradigms, and tests their generalization against multiple
types of distribution shifts. Finally, we extend our analysis to GPT-4.1 and
DeepSeek V3 to assess frontier models' current robustness to format
perturbations. Our findings offer actionable insights into the relative
effectiveness of these robustness methods, enabling practitioners to make
informed decisions when aiming for stable and reliable LLM performance in
real-world applications. Code:
https://github.com/AIRI-Institute/when-punctuation-matters.

</details>


### [124] [Retrieval-augmented reasoning with lean language models](https://arxiv.org/abs/2508.11386)
*Ryan Sze-Yin Chan,Federico Nanni,Tomas Lazauskas,Rosie Wood,Penelope Yong,Lionel Tarassenko,Mark Girolami,James Geddes,Andrew Duncan*

Main category: cs.CL

TL;DR: 本报告提出了一种将推理与检索增强生成（RAG）结合于单一轻量级语言模型架构的新方法，实现了在有限资源或安全环境下可部署且保护隐私的高性能解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统往往依赖大型模型和外部API，难以满足对本地、低资源、高隐私需求的场景。作者旨在开发适合资源受限及关键信息保护场景的检索增强推理模型。

Method: 该系统采用轻量级骨干模型（Qwen2.5-Instruct），结合密集检索器，通过合成查询生成和前沿模型（例如DeepSeek-R1）生成的推理轨迹，在定制语料（NHS A-to-Z condition pages）上进行微调。研究了文档压缩、合成数据设计和推理感知微调对性能的影响。

Result: 在与非推理及通用轻量模型的对比中，该领域特定的微调整方法显著提升了回答准确率和一致性，部分指标接近前沿大模型，但依然可在本地环境运行。

Conclusion: 本方法在保证模型轻量和本地可部署的前提下，大幅提升了在特定领域的推理和回答准确度，具备良好可扩展性和可复现性，为资源受限场景下的RAG系统提供了实践基础。

Abstract: This technical report details a novel approach to combining reasoning and
retrieval augmented generation (RAG) within a single, lean language model
architecture. While existing RAG systems typically rely on large-scale models
and external APIs, our work addresses the increasing demand for performant and
privacy-preserving solutions deployable in resource-constrained or secure
environments. Building on recent developments in test-time scaling and
small-scale reasoning models, we develop a retrieval augmented conversational
agent capable of interpreting complex, domain-specific queries using a
lightweight backbone model. Our system integrates a dense retriever with
fine-tuned Qwen2.5-Instruct models, using synthetic query generation and
reasoning traces derived from frontier models (e.g., DeepSeek-R1) over a
curated corpus, in this case, the NHS A-to-Z condition pages. We explore the
impact of summarisation-based document compression, synthetic data design, and
reasoning-aware fine-tuning on model performance. Evaluation against both
non-reasoning and general-purpose lean models demonstrates that our
domain-specific fine-tuning approach yields substantial gains in answer
accuracy and consistency, approaching frontier-level performance while
remaining feasible for local deployment. All implementation details and code
are publicly released to support reproducibility and adaptation across domains.

</details>


### [125] [Model Interpretability and Rationale Extraction by Input Mask Optimization](https://arxiv.org/abs/2508.11388)
*Marc Brinner,Sina Zarriess*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法，通过屏蔽神经网络输入中与预测类别无关的部分来生成可提取的解释，适用于文本和图像任务。所提方法基于梯度优化与新的正则化方案，兼顾了解释的充分性、全面性和紧凑性，不需要重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型（特别是自然语言处理和计算机视觉领域）表现提升，解释其预测机制的需求不断增长，以增强模型的可解释性和信任度。

Method: 方法基于对已训练神经网络的输入进行梯度优化，通过屏蔽非关键区域，并引入正则化以保证解释的充分性、全面性和紧凑性。无需额外训练推理提取模型，仅在原有分类器上操作。该方法不仅应用于文本数据，也适用于图像数据。

Result: 方法在文本和图像任务中均获得高质量的可提取式解释，验证了提出的正则化条件在不同输入类型上的广泛适用性。

Conclusion: 所提出方法为黑盒神经网络模型提供了新的解释途径，实现了推理提取与模型解释能力的结合，并且具备良好的迁移性和实用性。

Abstract: Concurrent to the rapid progress in the development of neural-network based
models in areas like natural language processing and computer vision, the need
for creating explanations for the predictions of these black-box models has
risen steadily. We propose a new method to generate extractive explanations for
predictions made by neural networks, that is based on masking parts of the
input which the model does not consider to be indicative of the respective
class. The masking is done using gradient-based optimization combined with a
new regularization scheme that enforces sufficiency, comprehensiveness and
compactness of the generated explanation, three properties that are known to be
desirable from the related field of rationale extraction in natural language
processing. In this way, we bridge the gap between model interpretability and
rationale extraction, thereby proving that the latter of which can be performed
without training a specialized model, only on the basis of a trained
classifier. We further apply the same method to image inputs and obtain high
quality explanations for image classifications, which indicates that the
conditions proposed for rationale extraction in natural language processing are
more broadly applicable to different input types.

</details>


### [126] [Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training](https://arxiv.org/abs/2508.11393)
*Marc Brinner,Sina Zarrieß*

Main category: cs.CL

TL;DR: 本文提出了一种端到端的可微分训练方法，使Transformer分类器在解释性和稳定性之间取得更好平衡。


<details>
  <summary>Details</summary>
Motivation: 当前用于训练具备解释能力（rationalized）的模型普遍存在训练不稳定的问题，且往往需要多个模型组件协作，效率较低。

Method: 作者基于现有的“三人博弈”策略（rationale selector、classifier、complement classifier），创新性地将三者功能融为一个单一模型，通过端到端的可微分训练方式提升稳定性。同时引入最新的参数化与正则化技术，并支持针对不同类别的rationale提取。

Result: 新方法不仅提升了训练效率和稳定性，还显著增强了与人工标注解释的一致性，达到了当前最优水平，而且无需额外的人工监督。

Conclusion: 本文提出的端到端单模型范式，为可解释Transformer分类器的高效、稳定训练提供了新思路，并在解释一致性上实现突破，具有广泛应用潜力。

Abstract: We propose an end-to-end differentiable training paradigm for stable training
of a rationalized transformer classifier. Our approach results in a single
model that simultaneously classifies a sample and scores input tokens based on
their relevance to the classification. To this end, we build on the widely-used
three-player-game for training rationalized models, which typically relies on
training a rationale selector, a classifier and a complement classifier. We
simplify this approach by making a single model fulfill all three roles,
leading to a more efficient training paradigm that is not susceptible to the
common training instabilities that plague existing approaches. Further, we
extend this paradigm to produce class-wise rationales while incorporating
recent advances in parameterizing and regularizing the resulting rationales,
thus leading to substantially improved and state-of-the-art alignment with
human annotations without any explicit supervision.

</details>


### [127] [Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via Survey Questions](https://arxiv.org/abs/2508.11414)
*Shangrui Nie,Florian Mai,David Kaczér,Charles Welch,Zhixue Zhao,Lucie Flek*

Main category: cs.CL

TL;DR: 本文提出通过微调大型语言模型（LLM）回答价值观调查问卷，来调整模型的价值体系，并验证了这种简单方法在下游任务中实现价值对齐的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于LLM隐含地编码了人类价值取向，但实际引导其行为往往需大量训练数据，因此作者探讨能否通过少量、简单的价值观问答微调，可靠地修改模型的价值系统。

Method: 作者首先让多个开源LLM对涵盖20种人类价值的描述进行评分，构建基线价值档案。接着，通过对部分调查问卷的微调，比较模型在未见的问卷与实际场景（如Reddit情境与互动式文本游戏）中的行为变化。

Result: 微调不仅成功地改变了模型对新问卷题的回答，还显著影响了模型在现实道德判断及文本冒险游戏等场景中的隐含行为表现，实现了价值导向的迁移。

Conclusion: 通过简单的价值问卷微调可有效调整LLM的价值观体系和实际行为，为低成本、灵活的人机价值对齐设计提供了新思路。

Abstract: Large language models implicitly encode preferences over human values, yet
steering them often requires large training data. In this work, we investigate
a simple approach: Can we reliably modify a model's value system in downstream
behavior by training it to answer value survey questions accordingly? We first
construct value profiles of several open-source LLMs by asking them to rate a
series of value-related descriptions spanning 20 distinct human values, which
we use as a baseline for subsequent experiments. We then investigate whether
the value system of a model can be governed by fine-tuning on the value
surveys. We evaluate the effect of finetuning on the model's behavior in two
ways; first, we assess how answers change on in-domain, held-out survey
questions. Second, we evaluate whether the model's behavior changes in
out-of-domain settings (situational scenarios). To this end, we construct a
contextualized moral judgment dataset based on Reddit posts and evaluate
changes in the model's behavior in text-based adventure games. We demonstrate
that our simple approach can not only change the model's answers to in-domain
survey questions, but also produces substantial shifts (value alignment) in
implicit downstream task behavior.

</details>


### [128] [HumorPlanSearch: Structured Planning and HuCoT for Contextual AI Humor](https://arxiv.org/abs/2508.11429)
*Shivam Dubey*

Main category: cs.CL

TL;DR: 本文提出了HumorPlanSearch，一个能让大模型生成更具情境感和文化敏感度幽默的系统，相比以往基线方法幽默生成质量有所提升。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型自动生成的笑话往往缺乏情境感，容易显得普通、重复或不合时宜，因为幽默高度依赖于受众背景和实时语境。作者希望通过提升幽默生成的情境敏感性，改善现有问题。

Method: 提出HumorPlanSearch模块化流程，具体包括：1）多样化、针对特定话题的幽默策略搜索（Plan-Search）；2）运用幽默思维链模板进行文化与风格推理；3）运用知识图谱检索并适配历史高效幽默策略；4）通过语义嵌入做新颖性过滤；5）基于评审者反馈的迭代式修正；同时提出融合多指标的幽默评分体系Humor Generation Score（HGS）。

Result: 在9个话题上聘请13位评审实验，完整流程（包括知识图谱和修订）下，HGS均分比强基线提升15.4%，差异显著（p<0.05）。

Conclusion: 通过将语境建模贯穿幽默生成和评价全部流程，HumorPlanSearch让AI幽默生成更连贯、适应性更强且更有文化敏感性，推动AI生成幽默能力的进步。

Abstract: Automated humor generation with Large Language Models (LLMs) often yields
jokes that feel generic, repetitive, or tone-deaf because humor is deeply
situated and hinges on the listener's cultural background, mindset, and
immediate context. We introduce HumorPlanSearch, a modular pipeline that
explicitly models context through: (1) Plan-Search for diverse, topic-tailored
strategies; (2) Humor Chain-of-Thought (HuCoT) templates capturing cultural and
stylistic reasoning; (3) a Knowledge Graph to retrieve and adapt
high-performing historical strategies; (4) novelty filtering via semantic
embeddings; and (5) an iterative judge-driven revision loop. To evaluate
context sensitivity and comedic quality, we propose the Humor Generation Score
(HGS), which fuses direct ratings, multi-persona feedback, pairwise win-rates,
and topic relevance. In experiments across nine topics with feedback from 13
human judges, our full pipeline (KG + Revision) boosts mean HGS by 15.4 percent
(p < 0.05) over a strong baseline. By foregrounding context at every stage from
strategy planning to multi-signal evaluation, HumorPlanSearch advances
AI-driven humor toward more coherent, adaptive, and culturally attuned comedy.

</details>


### [129] [Online Anti-sexist Speech: Identifying Resistance to Gender Bias in Political Discourse](https://arxiv.org/abs/2508.11434)
*Aditi Dutta,Susan Banducci*

Main category: cs.CL

TL;DR: 本文研究了当前大型语言模型（LLMs）在自动内容审核中对“反性别歧视言论”（anti-sexist speech）的分类问题，发现模型往往将挑战性别歧视的言论误判为有害言论，尤其在政治事件高发时。作者建议模型设计需引入更细致的处理方式和人工审核，以保护反歧视声音。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体平台广泛采用基于大型语言模型的自动内容审核系统，公众讨论中挑战性别歧视和性别辱骂的反歧视言论可能被误判为有害言论。这不仅使反对歧视的声音被消音，也加剧了对边缘群体的不公。作者希望揭示并解决这一内容审核中的社会技术挑战。

Method: 作者选取了2022年英国女性国会议员相关的高关注事件，采集了包含性别歧视、反性别歧视和中性内容的政治推文，利用五种主流大型语言模型对其进行分类测试，分析模型误判反歧视言论为有害内容的情形。

Result: 研究发现，这些大型语言模型在政治高峰期尤为容易将反性别歧视言论误判为有害，混淆抵抗与伤害性言论，导致挑战歧视的声音被平台误伤。该问题对弱势群体更为严重，加剧他们的边缘化。

Conclusion: 作者建议内容审核系统设计应突破简单的有害/无害二元判断，引入事件敏感时期的人机共同审核，并在训练数据中明确加入反歧视言论，借此在数字政治空间更好地保护反对歧视的公共表达。

Abstract: Anti-sexist speech, i.e., public expressions that challenge or resist
gendered abuse and sexism, plays a vital role in shaping democratic debate
online. Yet automated content moderation systems, increasingly powered by large
language models (LLMs), may struggle to distinguish such resistance from the
sexism it opposes. This study examines how five LLMs classify sexist,
anti-sexist, and neutral political tweets from the UK, focusing on
high-salience trigger events involving female Members of Parliament in the year
2022. Our analysis show that models frequently misclassify anti-sexist speech
as harmful, particularly during politically charged events where rhetorical
styles of harm and resistance converge. These errors risk silencing those who
challenge sexism, with disproportionate consequences for marginalised voices.
We argue that moderation design must move beyond binary harmful/not-harmful
schemas, integrate human-in-the-loop review during sensitive events, and
explicitly include counter-speech in training data. By linking feminist
scholarship, event-based analysis, and model evaluation, this work highlights
the sociotechnical challenges of safeguarding resistance speech in digital
political spaces.

</details>


### [130] [CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity](https://arxiv.org/abs/2508.11442)
*Bowen Zhang,Zixin Song,Chunquan Chen,Qian-Wen Zhang,Di Yin,Xing Sun*

Main category: cs.CL

TL;DR: 本文提出了CoDiEmb框架，通过创新的多任务融合方法，实现文本表示在信息检索(IR)和语义文本相似度(STS)上的协同优化，显著缓解负迁移与性能损失。


<details>
  <summary>Details</summary>
Motivation: 在表示学习中，统一的文本表示对下游多任务表现优秀是核心目标，但负迁移（不同任务间相互干扰）常常导致某些任务性能下降，尤其是在同时训练信息检索和文本相似度任务时，传统方法往往无法兼顾二者。

Method: 提出CoDiEmb框架，包含：1）任务特化的目标函数和动态采样器，将训练样本分为单任务批次并平衡每个任务的参数更新，IR采用增强对比损失，STS采用顺序敏感目标优化；2）创新的delta引导模型融合策略，根据参数的偏移程度对多个模型检查点权重进行精细融合，优于传统方法；3）单阶段高效训练流程，便于实现且收敛性好。

Result: 在15个标准IR和STS基准集，以及三种主流编码器上进行的大量实验证实，CoDiEmb减小了不同任务间的性能权衡，并提升了嵌入空间的几何属性。

Conclusion: CoDiEmb有效解决了多任务表示学习中的负迁移问题，为文本统一嵌入在不同任务上的应用提供了高效、稳定的新方案。

Abstract: Learning unified text embeddings that excel across diverse downstream tasks
is a central goal in representation learning, yet negative transfer remains a
persistent obstacle. This challenge is particularly pronounced when jointly
training a single encoder for Information Retrieval (IR) and Semantic Textual
Similarity (STS), two essential but fundamentally disparate tasks for which
naive co-training typically yields steep performance trade-offs. We argue that
resolving this conflict requires systematically decoupling task-specific
learning signals throughout the training pipeline. To this end, we introduce
CoDiEmb, a unified framework that reconciles the divergent requirements of IR
and STS in a collaborative yet distinct manner. CoDiEmb integrates three key
innovations for effective joint optimization: (1) Task-specialized objectives
paired with a dynamic sampler that forms single-task batches and balances
per-task updates, thereby preventing gradient interference. For IR, we employ a
contrastive loss with multiple positives and hard negatives, augmented by
cross-device sampling. For STS, we adopt order-aware objectives that directly
optimize correlation and ranking consistency. (2) A delta-guided model fusion
strategy that computes fine-grained merging weights for checkpoints by
analyzing each parameter's deviation from its pre-trained initialization,
proving more effective than traditional Model Soups. (3) An efficient,
single-stage training pipeline that is simple to implement and converges
stably. Extensive experiments on 15 standard IR and STS benchmarks across three
base encoders validate CoDiEmb. Our results and analysis demonstrate that the
framework not only mitigates cross-task trade-offs but also measurably improves
the geometric properties of the embedding space.

</details>


### [131] [Reference Points in LLM Sentiment Analysis: The Role of Structured Context](https://arxiv.org/abs/2508.11454)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: 研究调查了在情感分析任务中，通过向大型语言模型（LLM）输入结构化补充信息（如JSON格式），能否提升分析效果，发现结构化信息能提升小参数模型的表现，且潜力适用于实际市场营销场景。


<details>
  <summary>Details</summary>
Motivation: 大多数情感分析任务只依据评论文本进行分析，忽略了营销学理论中强调的‘参考点’等补充信息，而这可能影响模型对用户评价的准确理解。该研究意图探索补充信息的引入是否能提升LLM在情感分析中的效果，特别是在资源受限的实际应用环境下。

Method: 作者将大语言模型的输入分为自然语言（NL）和JSON结构化格式两种，并向模型输入带有或不带有补充信息的内容。在Yelp餐饮和夜生活两个类别数据集上，使用轻量级3B参数的模型进行对比实验，无需微调，评估模型性能。

Result: 结构化的JSON格式输入并带有补充信息时，Macro-F1分数在餐饮和夜生活类分别提高了1.6%和4%，RMSE分别降低了16%和9.1%。后续分析排除了模型仅基于标签代理的可能性，确认性能提升源自真实的上下文推理能力。

Conclusion: 结构化提示词设计能有效提升小规模LLM的情感分析表现，使其成为大规模模型落地敏感场景中的可行替代方案。

Abstract: Large language models (LLMs) are now widely used across many fields,
including marketing research. Sentiment analysis, in particular, helps firms
understand consumer preferences. While most NLP studies classify sentiment from
review text alone, marketing theories, such as prospect theory and
expectation--disconfirmation theory, point out that customer evaluations are
shaped not only by the actual experience but also by additional reference
points. This study therefore investigates how the content and format of such
supplementary information affect sentiment analysis using LLMs. We compare
natural language (NL) and JSON-formatted prompts using a lightweight 3B
parameter model suitable for practical marketing applications. Experiments on
two Yelp categories (Restaurant and Nightlife) show that the JSON prompt with
additional information outperforms all baselines without fine-tuning: Macro-F1
rises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it
deployable in resource-constrained edge devices. Furthermore, a follow-up
analysis confirms that performance gains stem from genuine contextual reasoning
rather than label proxying. This work demonstrates that structured prompting
can enable smaller models to achieve competitive performance, offering a
practical alternative to large-scale model deployment.

</details>


### [132] [Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models](https://arxiv.org/abs/2508.11534)
*Monika Jotautaitė,Lucius Caviola,David A. Brewster,Thilo Hagendorff*

Main category: cs.CL

TL;DR: 本文系统地研究了大语言模型(LLM)是否存在物种歧视偏见。通过多种实验发现：LLM能识别但较少谴责物种歧视，在某些道德权衡中更倾向于优先照顾人类。结论认为有必要将非人类动物纳入AI公平和对齐框架，以减少这类偏见。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在实际中的广泛应用，其伦理倾向引发关注。尽管AI领域已重视公平与歧视问题，但对“物种歧视”——即因物种归属产生的歧视——尚缺系统性研究。动机在于探究LLM对人类与非人动物的道德价值判断及其潜在偏见。

Method: 方法包括三个主要部分：(1) 构建了名为SpeciesismBench的1,003项基准测试，评估LLM对物种歧视性陈述的识别和道德评价；(2) 用心理学标准量表，将模型反应与人类参与者进行对比；(3) 设计文本生成任务，考察模型如何阐述或抵制物种歧视性理由。

Result: LLM在基准测试中能可靠识别物种歧视陈述，却很少加以谴责，往往默许其合理性。心理量表结果表明，LLM虽然表现出稍低的显性物种歧视，但在价值权衡中，更多选择拯救一个人类而非多只动物。当比较对象能力相同时，偏见减弱，且对能力较强的动物会优先考虑。在文本生成任务中，LLM常为农场动物遭受伤害提供合理化理由，对非农场动物则拒绝。

Conclusion: 结论认为，LLM折射出现有的人类主流文化观念，对动物剥削的规范态度仍被再现。研究建议：应将非人类动物纳入AI公平与对齐框架，减少这类偏见，以免对AI系统及其影响下的社会加深物种歧视。

Abstract: As large language models (LLMs) become more widely deployed, it is crucial to
examine their ethical tendencies. Building on research on fairness and
discrimination in AI, we investigate whether LLMs exhibit speciesist bias --
discrimination based on species membership -- and how they value non-human
animals. We systematically examine this issue across three paradigms: (1)
SpeciesismBench, a 1,003-item benchmark assessing recognition and moral
evaluation of speciesist statements; (2) established psychological measures
comparing model responses with those of human participants; (3) text-generation
tasks probing elaboration on, or resistance to, speciesist rationalizations. In
our benchmark, LLMs reliably detected speciesist statements but rarely
condemned them, often treating speciesist attitudes as morally acceptable. On
psychological measures, results were mixed: LLMs expressed slightly lower
explicit speciesism than people, yet in direct trade-offs they more often chose
to save one human over multiple animals. A tentative interpretation is that
LLMs may weight cognitive capacity rather than species per se: when capacities
were equal, they showed no species preference, and when an animal was described
as more capable, they tended to prioritize it over a less capable human. In
open-ended text generation tasks, LLMs frequently normalized or rationalized
harm toward farmed animals while refusing to do so for non-farmed animals.
These findings suggest that while LLMs reflect a mixture of progressive and
mainstream human views, they nonetheless reproduce entrenched cultural norms
around animal exploitation. We argue that expanding AI fairness and alignment
frameworks to explicitly include non-human moral patients is essential for
reducing these biases and preventing the entrenchment of speciesist attitudes
in AI systems and the societies they influence.

</details>


### [133] [Language models align with brain regions that represent concepts across modalities](https://arxiv.org/abs/2508.11536)
*Maria Ryskina,Greta Tuckute,Alexander Fung,Ashley Malkin,Evelina Fedorenko*

Main category: cs.CL

TL;DR: 本文探讨了语言模型与大脑表征之间的关系，发现语言模型在对跨模态一致性较高的大脑区域进行信号预测时表现更好，暗示模型可能内部具备跨模态概念表征能力。


<details>
  <summary>Details</summary>
Motivation: 认知科学与神经科学长期以来难以区分大脑中的语言表征和概念意义表征，而该难题同样存在于现代语言模型。作者希望揭示当前语言模型与大脑表征的一致性及其与意义一致性和语言加工的关联。

Method: 作者使用fMRI数据，通过两项神经指标分析语言模型与大脑信号的对齐情况：1）句子处理时的大脑激活水平，用以反映语言加工程度；2）大脑区域对同一概念在不同输入模态（句子、词云、图像）下反应一致性的创新指标。实验比较了只处理语言的模型和语言-视觉模型对这些大脑区域信号的预测能力。

Result: 实验结果显示，无论是语言模型还是语言-视觉模型，在意义一致性高的大脑区域都能更好地预测神经信号，即便这些区域对语言加工不敏感。

Conclusion: 语言模型可能不仅表征语言结构，还具备跨模态的概念意义表征能力，这对理解人工智能与人脑认知的关联具有重要启示。

Abstract: Cognitive science and neuroscience have long faced the challenge of
disentangling representations of language from representations of conceptual
meaning. As the same problem arises in today's language models (LMs), we
investigate the relationship between LM--brain alignment and two neural
metrics: (1) the level of brain activation during processing of sentences,
targeting linguistic processing, and (2) a novel measure of meaning consistency
across input modalities, which quantifies how consistently a brain region
responds to the same concept across paradigms (sentence, word cloud, image)
using an fMRI dataset (Pereira et al., 2018). Our experiments show that both
language-only and language-vision models predict the signal better in more
meaning-consistent areas of the brain, even when these areas are not strongly
sensitive to language processing, suggesting that LMs might internally
represent cross-modal conceptual meaning.

</details>


### [134] [AgentMental: An Interactive Multi-Agent Framework for Explainable and Adaptive Mental Health Assessment](https://arxiv.org/abs/2508.11567)
*Jinpeng Hu,Ao Wang,Qianqian Xie,Hui Ma,Zhuo Li,Dan Guo*

Main category: cs.CL

TL;DR: 本文提出一种多智能体框架用于自动化心理健康评估，通过模拟临床医患问答过程，有效提升评估的智能性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统人工心理健康评估受专业人员短缺限制，现有AI方法多依赖静态文本分析，难以全面把握患者心理状态。迫切需要更动态、互动性更强的自动化评估方法。

Method: 作者设计了多智能体系统，包括提问、充足性评估、评分和记忆更新等角色。引入自适应提问机制，根据用户回答的充分性智能生成针对性追问；采用树状结构记忆，将用户信息分门别类动态记录和更新。

Result: 在DAIC-WOZ数据集上的实验表明，该方法比传统方法表现更佳，在信息提取与症状识别方面更有效。

Conclusion: 多智能体对话与动态记忆机制能显著提升心理健康自动评估的性能，为临床AI评估系统发展提供了有力支撑。

Abstract: Mental health assessment is crucial for early intervention and effective
treatment, yet traditional clinician-based approaches are limited by the
shortage of qualified professionals. Recent advances in artificial intelligence
have sparked growing interest in automated psychological assessment, yet most
existing approaches are constrained by their reliance on static text analysis,
limiting their ability to capture deeper and more informative insights that
emerge through dynamic interaction and iterative questioning. Therefore, in
this paper, we propose a multi-agent framework for mental health evaluation
that simulates clinical doctor-patient dialogues, with specialized agents
assigned to questioning, adequacy evaluation, scoring, and updating. We
introduce an adaptive questioning mechanism in which an evaluation agent
assesses the adequacy of user responses to determine the necessity of
generating targeted follow-up queries to address ambiguity and missing
information. Additionally, we employ a tree-structured memory in which the root
node encodes the user's basic information, while child nodes (e.g., topic and
statement) organize key information according to distinct symptom categories
and interaction turns. This memory is dynamically updated throughout the
interaction to reduce redundant questioning and further enhance the information
extraction and contextual tracking capabilities. Experimental results on the
DAIC-WOZ dataset illustrate the effectiveness of our proposed method, which
achieves better performance than existing approaches.

</details>


### [135] [Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models](https://arxiv.org/abs/2508.11582)
*Qiguang Chen,Dengyun Peng,Jinhao Liu,HuiKang Su,Jiannan Guan,Libo Qin,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文提出了一个名为DR. SAF（动态推理边界自觉框架）的新方法，可以动态调整大语言模型的推理深度，显著提升推理效率并减少冗余。实验显示，该方法在几乎不损失准确率的情况下，大幅提升了效率，适用于对资源有限的场景。


<details>
  <summary>Details</summary>
Motivation: 长链式思维（Long CoT）虽然能提升LLM在复杂任务中的推理能力，但经常导致推理过程中出现大量冗余内容，影响效率，给实时应用带来延迟压力。现有方法通常依赖人为设定的难度先验，未能很好匹配模型自身感知的难度，导致推理过程效率低下。

Method: 提出了动态推理边界自觉框架（DR. SAF），包括三大模块：边界自觉对齐（Boundary Self-Awareness Alignment）、自适应奖励管理（Adaptive Reward Management）和边界保持机制（Boundary Preservation Mechanism），使模型可根据问题复杂度灵活调整推理深度，优化推理过程，在效率和准确性间取得平衡。

Result: 实验表明，DR. SAF方法在准确率几乎无损的情况下，将总响应token数减少了49.27%；token效率提升6.59倍，训练时间缩短到原来的1/5。在极端训练情况下，该方法token效率超过传统基线，并可带来16%以上的准确率提升。

Conclusion: DR. SAF显著提升了大语言模型的推理效率和资源利用率，尤其适合算力受限场景，同时保持甚至提升了准确性，是长链推理任务的有效提升方案。

Abstract: Recent advancements in large language models (LLMs) have greatly improved
their capabilities on complex reasoning tasks through Long Chain-of-Thought
(CoT). However, this approach often results in substantial redundancy,
impairing computational efficiency and causing significant delays in real-time
applications. To improve the efficiency, current methods often rely on
human-defined difficulty priors, which do not align with the LLM's self-awared
difficulty, leading to inefficiencies. In this paper, we introduce the Dynamic
Reasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to
dynamically assess and adjust their reasoning depth in response to problem
complexity. DR. SAF integrates three key components: Boundary Self-Awareness
Alignment, Adaptive Reward Management, and a Boundary Preservation Mechanism.
These components allow models to optimize their reasoning processes, balancing
efficiency and accuracy without compromising performance. Our experimental
results demonstrate that DR. SAF achieves a 49.27% reduction in total response
tokens with minimal loss in accuracy. The framework also delivers a 6.59x gain
in token efficiency and a 5x reduction in training time, making it well-suited
to resource-limited settings. During extreme training, DR. SAF can even surpass
traditional instruction-based models in token efficiency with more than 16%
accuracy improvement.

</details>


### [136] [Representing Speech Through Autoregressive Prediction of Cochlear Tokens](https://arxiv.org/abs/2508.11598)
*Greta Tuckute,Klemen Kotar,Evelina Fedorenko,Daniel L. K. Yamins*

Main category: cs.CL

TL;DR: AuriStream 是一种受生物学启发、模拟人类听觉分层处理的语音编码模型，采用两阶段框架，能高效进行多种语音任务，并具备优秀的音频生成和解释能力。


<details>
  <summary>Details</summary>
Motivation: 当前语音表示学习模型难以全面模拟人类听觉的分层加工机制，限制了对语音结构与语义的理解与建模。因此，旨在提出更接近人类处理音频方式的模型，并提升模型在多种语音任务中的表现。

Method: 采用两阶段架构：第一阶段将原始音频转换成人类耳蜗启发的时频表示，并据此提取离散的 '耳蜗 token'；第二阶段使用自回归序列模型处理这些 token，学习语音的音素、词级表达与语义特点。

Result: AuriStream 能学习有意义的音素和词表示，实现了最先进的词汇语义表征，在多个SUPERB基准语音任务中表现优异。同时，模型具有音频续生成能力，便于可视化与解释预测结果。

Conclusion: 提出了一种具有人类听觉特性、兼具强大表示和生成能力的两阶段语音表示学习模型，对发展更类人的语音任务解决方案具有推动作用。

Abstract: We introduce AuriStream, a biologically inspired model for encoding speech
via a two-stage framework inspired by the human auditory processing hierarchy.
The first stage transforms raw audio into a time-frequency representation based
on the human cochlea, from which we extract discrete \textbf{cochlear tokens}.
The second stage applies an autoregressive sequence model over the cochlear
tokens. AuriStream learns meaningful phoneme and word representations, and
state-of-the-art lexical semantics. AuriStream shows competitive performance on
diverse downstream SUPERB speech tasks. Complementing AuriStream's strong
representational capabilities, it generates continuations of audio which can be
visualized in a spectrogram space and decoded back into audio, providing
insights into the model's predictions. In summary, we present a two-stage
framework for speech representation learning to advance the development of more
human-like models that efficiently handle a range of speech-based tasks.

</details>


### [137] [Dataset Creation for Visual Entailment using Generative AI](https://arxiv.org/abs/2508.11605)
*Rob Reijtenbach,Suzan Verberne,Gijs Wijnholds*

Main category: cs.CL

TL;DR: 本文提出并验证了一种用于视觉蕴涵模型训练的新型合成数据集。通过利用生成模型，用文本数据集SNLI中的前提文本生成图片，并用这些数据训练模型，表现与真实数据相近。


<details>
  <summary>Details</summary>
Motivation: 目前视觉蕴涵领域数据集相对较小且稀疏，人工创建成本高，限制了模型发展，因此亟需一种高效扩充数据的方法。

Method: 以SNLI文本蕴涵数据集的前提句作为Stable Diffusion生成模型的输入，生成与文本语义对应的图片，替换原数据集中的前提文本，构建新的视觉蕴涵合成数据集，并基于CLIP特征训练视觉蕴涵分类模型进行评估。

Result: 用合成数据训练的视觉蕴涵模型在SNLI-VE数据集上的F分数为0.686（真实数据为0.703），在SICK-VTE数据集上的F分数为0.384（真实数据为0.400），只有轻微下降。

Conclusion: 在数据稀缺的场景下，合成数据集能有效替代真实数据，为视觉蕴涵模型训练提供可行解决方案。

Abstract: In this paper we present and validate a new synthetic dataset for training
visual entailment models. Existing datasets for visual entailment are small and
sparse compared to datasets for textual entailment. Manually creating datasets
is labor-intensive. We base our synthetic dataset on the SNLI dataset for
textual entailment. We take the premise text from SNLI as input prompts in a
generative image model, Stable Diffusion, creating an image to replace each
textual premise. We evaluate our dataset both intrinsically and extrinsically.
For extrinsic evaluation, we evaluate the validity of the generated images by
using them as training data for a visual entailment classifier based on CLIP
feature vectors. We find that synthetic training data only leads to a slight
drop in quality on SNLI-VE, with an F-score 0.686 compared to 0.703 when
trained on real data. We also compare the quality of our generated training
data to original training data on another dataset: SICK-VTE. Again, there is
only a slight drop in F-score: from 0.400 to 0.384. These results indicate that
in settings with data sparsity, synthetic data can be a promising solution for
training visual entailment models.

</details>


### [138] [TinyTim: A Family of Language Models for Divergent Generation](https://arxiv.org/abs/2508.11607)
*Christopher J. Agostino*

Main category: cs.CL

TL;DR: 本文提出了TinyTim语言模型家族，专门微调于乔伊斯的《芬尼根的守灵夜》，在生成任务中展现出高词汇多样性和低语义连贯性的特点。


<details>
  <summary>Details</summary>
Motivation: 探索将文学风格（如乔伊斯的《芬尼根的守灵夜》）引入大语言模型后能否拓展模型在创造性和复杂问题解决中的潜力。

Method: 微调大语言模型使其生成风格靠近《芬尼根的守灵夜》，并通过定量实验与基线模型进行对比，评估生成文本的词汇多样性和语义连贯性。

Result: TinyTim V1模型在生成文本时表现出显著高于基线模型的词汇多样性，语义连贯性则较低，形成了一种独特的生成特征。

Conclusion: 将此类具特殊风格的模型视为发散性知识源，能够在更复杂的创造性架构中为自动化发现机制提供支持，适用于多种创新场景。

Abstract: This work introduces TinyTim, a family of large language models fine-tuned on
James Joyce's `Finnegans Wake'. Through quantitative evaluation against
baseline models, we demonstrate that TinyTim V1 produces a statistically
distinct generative profile characterized by high lexical diversity and low
semantic coherence. These findings are interpreted through theories of
creativity and complex problem-solving, arguing that such specialized models
can function as divergent knowledge sources within more extensive creative
architectures, powering automated discovery mechanisms in diverse settings.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [139] [Developing and Validating a High-Throughput Robotic System for the Accelerated Development of Porous Membranes](https://arxiv.org/abs/2508.10973)
*Hongchen Wang,Sima Zeinali Danalou,Jiahao Zhu,Kenneth Sulimro,Chaewon Lim,Smita Basak,Aimee Tai,Usan Siriwardana,Jason Hattrick-Simpers,Jay Werber*

Main category: cs.RO

TL;DR: 本文提出了一种全自动多功能平台，实现了多孔高分子膜的快速、可重复、自动化制备与表征。通过自动化设备，简化并加速了膜制备和性能评价流程，提升了一致性和实验效率。


<details>
  <summary>Details</summary>
Motivation: 多孔高分子膜在分离等领域有广泛应用，但其制备过程往往需要反复试错、效率低，缺乏高通量且可重复的制备与表征方法。本文旨在解决这些问题，实现自动化、高一致性和高通量的膜材料优化。

Method: 开发了集自动配液、自动刮膜、受控浸渍、压缩测试于一体的模块化自动化平台，可精确控制聚合物浓度、环境湿度等关键参数。采用压缩测试作为敏感力学表征方法，通过应力-应变曲线自动分析，快速评估膜的刚度、孔隙率和均匀性，并支持并行处理多个样品。

Result: 以聚砜-PolarClean-水体系为例，自动化平台实验重现实验已知规律，如随聚合物浓度增加，膜刚度和均匀性增加，环境湿度变化影响孔结构和力学响应，验证了平台的精准性与一致性。

Conclusion: 该自动化平台实现了高通量、自动化的多孔高分子膜制备与表征，可作为自驱动实验室的一部分基础设施，为基于数据驱动的膜材料优化提供了可扩展、可重复的技术基础。

Abstract: The development of porous polymeric membranes remains a labor-intensive
process, often requiring extensive trial and error to identify optimal
fabrication parameters. In this study, we present a fully automated platform
for membrane fabrication and characterization via nonsolvent-induced phase
separation (NIPS). The system integrates automated solution preparation, blade
casting, controlled immersion, and compression testing, allowing precise
control over fabrication parameters such as polymer concentration and ambient
humidity. The modular design allows parallel processing and reproducible
handling of samples, reducing experimental time and increasing consistency.
Compression testing is introduced as a sensitive mechanical characterization
method for estimating membrane stiffness and as a proxy to infer porosity and
intra-sample uniformity through automated analysis of stress-strain curves. As
a proof of concept to demonstrate the effectiveness of the system, NIPS was
carried out with polysulfone, the green solvent PolarClean, and water as the
polymer, solvent, and nonsolvent, respectively. Experiments conducted with the
automated system reproduced expected effects of polymer concentration and
ambient humidity on membrane properties, namely increased stiffness and
uniformity with increasing polymer concentration and humidity variations in
pore morphology and mechanical response. The developed automated platform
supports high-throughput experimentation and is well-suited for integration
into self-driving laboratory workflows, offering a scalable and reproducible
foundation for data-driven optimization of porous polymeric membranes through
NIPS.

</details>


### [140] [Robust Online Calibration for UWB-Aided Visual-Inertial Navigation with Bias Correction](https://arxiv.org/abs/2508.10999)
*Yizhi Zhou,Jie Xu,Jiawei Xia,Zechen Hu,Weizi Li,Xuan Wang*

Main category: cs.RO

TL;DR: 该论文提出了一种新颖的面向UWB-aided VINS系统的UWB基站自校准框架，通过显式考虑机器人定位误差和采用Schmidt卡尔曼滤波器，实现更稳健和在线的基站校准。实验显示其精度和鲁棒性优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有UWB基站校准方法对初始化位置依赖大且假设机器人有精确定位，忽略了现实中的定位误差，导致结果不够稳健，实际应用受限。为解决这些痛点，作者提出改进方法。

Method: 方法上，作者建立了一个显式考虑机器人定位不确定性的校准过程，结合了紧耦合的Schmidt Kalman Filter（SKF）进行在线结果优化与鲁棒性提升。该方法能动态修正初始化和实时误差。

Result: 通过仿真和真实场景实验，验证了方法在锚点校准精度和鲁棒性方面优于之前的自动校准方法。

Conclusion: 该方法显著提升了UWB基站校准的实用性和鲁棒性，适用于复杂或带有初始不确定性的实际环境，为UWB辅助导航系统提供更可靠基础。

Abstract: This paper presents a novel robust online calibration framework for
Ultra-Wideband (UWB) anchors in UWB-aided Visual-Inertial Navigation Systems
(VINS). Accurate anchor positioning, a process known as calibration, is crucial
for integrating UWB ranging measurements into state estimation. While several
prior works have demonstrated satisfactory results by using robot-aided systems
to autonomously calibrate UWB systems, there are still some limitations: 1)
these approaches assume accurate robot localization during the initialization
step, ignoring localization errors that can compromise calibration robustness,
and 2) the calibration results are highly sensitive to the initial guess of the
UWB anchors' positions, reducing the practical applicability of these methods
in real-world scenarios. Our approach addresses these challenges by explicitly
incorporating the impact of robot localization uncertainties into the
calibration process, ensuring robust initialization. To further enhance the
robustness of the calibration results against initialization errors, we propose
a tightly-coupled Schmidt Kalman Filter (SKF)-based online refinement method,
making the system suitable for practical applications. Simulations and
real-world experiments validate the improved accuracy and robustness of our
approach.

</details>


### [141] [3D FlowMatch Actor: Unified 3D Policy for Single- and Dual-Arm Manipulation](https://arxiv.org/abs/2508.11002)
*Nikolaos Gkanatsios,Jiahe Xu,Matthew Bronars,Arsalan Mousavian,Tsung-Wei Ke,Katerina Fragkiadaki*

Main category: cs.RO

TL;DR: 本文提出了一种称为3D FlowMatch Actor (3DFA)的新型机器人操作策略架构，通过结合flow matching轨迹预测与3D视觉场景表征，实现基于演示的高效学习。在多个基准测试中取得了大幅超越现有方法的效果，并显著提升训练和推理速度。


<details>
  <summary>Details</summary>
Motivation: 目前3D基于扩散的单臂机器人操作策略存在训练和推理速度慢、资源需求高等问题，并且缺乏对双臂复杂操作和直接轨迹预测的高效解决方案。作者希望开发出既高效又效果更优的新方法，打破现有方法的局限。

Method: 3DFA将flow matching用于机器人端执行器轨迹预测，并利用训练好的3D视觉场景表征，提升策略对复杂环境的理解能力。通过在动作去噪过程中采用3D相对注意力机制，将动作和视觉信息高效结合，优化系统架构和训练流程，显著提高计算效率。

Result: 在双臂PerAct2基准测试中，3DFA相较于次优方法提升了41.4%的绝对性能，并实现了超过30倍的训练与推理速度提升。在真实机器人实验中，3DFA优于参数规模大几千倍和预训练更多的强基线方法。在单臂RLBench 74项任务上，直接预测轨迹取得最新最优成果，无需额外的运动规划。

Conclusion: 3DFA在效率与操作性能上均大幅领先现有3D机器人操作策略，验证了flow matching与3D相对注意力等创新设计的有效性，为多臂和单臂机器人操作任务提供了更高效、更优的解决方案。

Abstract: We present 3D FlowMatch Actor (3DFA), a 3D policy architecture for robot
manipulation that combines flow matching for trajectory prediction with 3D
pretrained visual scene representations for learning from demonstration. 3DFA
leverages 3D relative attention between action and visual tokens during action
denoising, building on prior work in 3D diffusion-based single-arm policy
learning. Through a combination of flow matching and targeted system-level and
architectural optimizations, 3DFA achieves over 30x faster training and
inference than previous 3D diffusion-based policies, without sacrificing
performance. On the bimanual PerAct2 benchmark, it establishes a new state of
the art, outperforming the next-best method by an absolute margin of 41.4%. In
extensive real-world evaluations, it surpasses strong baselines with up to
1000x more parameters and significantly more pretraining. In unimanual
settings, it sets a new state of the art on 74 RLBench tasks by directly
predicting dense end-effector trajectories, eliminating the need for motion
planning. Comprehensive ablation studies underscore the importance of our
design choices for both policy effectiveness and efficiency.

</details>


### [142] [GenFlowRL: Shaping Rewards with Generative Object-Centric Flow in Visual Reinforcement Learning](https://arxiv.org/abs/2508.11049)
*Kelin Yu,Sheng Zhang,Harshit Soora,Furong Huang,Heng Huang,Pratap Tokekar,Ruohan Gao*

Main category: cs.RO

TL;DR: GenFlowRL方法通过从生成的对象-中心流中提取特征，用于奖励塑形，提升了机器人操作任务中的泛化和鲁棒性，在多任务和不同控制主体下表现优异。


<details>
  <summary>Details</summary>
Motivation: 以往基于视频生成的机器人学习方法依赖生成数据的质量，并且在细粒度操作上受限于环境反馈的缺失。此外，收集大规模数据集训练扩散模型存在困难，且视频生成的不确定性限制了策略鲁棒性。为了解决这些问题，需要一种能从多样化演示中学习、同时具备泛化能力的新方法。

Method: 提出了GenFlowRL方法，从跨不同机器人本体的演示数据中训练对象-中心流，并用其提取的低维特征进行奖励塑形，使强化学习能够利用这些特征学习泛化、鲁棒的操作策略。方法能应用于不同类型的操作任务，并支持仿真与真实机器人测试。

Result: 在10个仿真和现实机器人操作任务上，GenFlowRL利用生成的对象-中心流有效提取操作特征，通过奖励塑形显著提升了多样场景下的性能，超越现有方法，体现了强泛化与鲁棒性。

Conclusion: GenFlowRL展示了从生成流学习奖励和特征能极大提升机器人操作学习的泛化与鲁棒性，为机器人在多类型任务和跨主体学习中提供了有效的新范式。

Abstract: Recent advances have shown that video generation models can enhance robot
learning by deriving effective robot actions through inverse dynamics. However,
these methods heavily depend on the quality of generated data and struggle with
fine-grained manipulation due to the lack of environment feedback. While
video-based reinforcement learning improves policy robustness, it remains
constrained by the uncertainty of video generation and the challenges of
collecting large-scale robot datasets for training diffusion models. To address
these limitations, we propose GenFlowRL, which derives shaped rewards from
generated flow trained from diverse cross-embodiment datasets. This enables
learning generalizable and robust policies from diverse demonstrations using
low-dimensional, object-centric features. Experiments on 10 manipulation tasks,
both in simulation and real-world cross-embodiment evaluations, demonstrate
that GenFlowRL effectively leverages manipulation features extracted from
generated object-centric flow, consistently achieving superior performance
across diverse and challenging scenarios. Our Project Page:
https://colinyu1.github.io/genflowrl

</details>


### [143] [Utilizing Vision-Language Models as Action Models for Intent Recognition and Assistance](https://arxiv.org/abs/2508.11093)
*Cesar Alan Contreras,Manolis Chiou,Alireza Rastegarpanah,Michal Szulik,Rustam Stolkin*

Main category: cs.RO

TL;DR: 本文提出了一种结合视觉-语言模型（VLM）和文本语言模型（LLM）增强人机协作中的机器人意图推断能力的方法。通过语义先验过滤无关目标，有效提升机器人辅助任务的精准性。


<details>
  <summary>Details</summary>
Motivation: 提升机器人在人机协作中的意图理解、推理透明性及协助能力，使其更好地理解并辅助用户完成目标任务。

Method: 在原有GUIDER框架基础上，引入YOLO与Segment Anything进行目标检测和实例分割，将检测到目标输入VLM，结合操作员任务指令打分相关性；同时，文本LLM对检测目标标签排序，二者得分加权引导导航与操作，只选取语境相关目标。一旦机器人信心水平超过阈值即自主执行动作，并随时响应操作员意图变化。

Result: 系统能够依据操作员指令高效过滤、权重并选中与任务相关的对象，引导机器人协作动作，提升意图匹配性和响应速度。具体系统效果将在后续实验（Isaac Sim仿真平台，Franka Emika机械臂和Ridgeback底盘）进行实证评估。

Conclusion: 初步方法验证了多模态语义先验可提升机器人对任务相关对象的筛选和定位能力，预期有助于实时、高效的人机交互协作。

Abstract: Human-robot collaboration requires robots to quickly infer user intent,
provide transparent reasoning, and assist users in achieving their goals. Our
recent work introduced GUIDER, our framework for inferring navigation and
manipulation intents. We propose augmenting GUIDER with a vision-language model
(VLM) and a text-only language model (LLM) to form a semantic prior that
filters objects and locations based on the mission prompt. A vision pipeline
(YOLO for object detection and the Segment Anything Model for instance
segmentation) feeds candidate object crops into the VLM, which scores their
relevance given an operator prompt; in addition, the list of detected object
labels is ranked by a text-only LLM. These scores weight the existing
navigation and manipulation layers of GUIDER, selecting context-relevant
targets while suppressing unrelated objects. Once the combined belief exceeds a
threshold, autonomy changes occur, enabling the robot to navigate to the
desired area and retrieve the desired object, while adapting to any changes in
the operator's intent. Future work will evaluate the system on Isaac Sim using
a Franka Emika arm on a Ridgeback base, with a focus on real-time assistance.

</details>


### [144] [Robot Policy Evaluation for Sim-to-Real Transfer: A Benchmarking Perspective](https://arxiv.org/abs/2508.11117)
*Xuning Yang,Clemens Eppner,Jonathan Tremblay,Dieter Fox,Stan Birchfield,Fabio Ramos*

Main category: cs.RO

TL;DR: 本文针对现有基于视觉的机器人模拟基准在模拟到现实（sim-to-real）泛化能力评估方面的不足，提出了新的基准设计建议，以促进通用机器人操作策略的现实世界应用。


<details>
  <summary>Details</summary>
Motivation: 目前的机器人视觉模拟基准推动了操作研究，但在通用策略的实际应用评估上仍有较大差距。由于机器人最终要在现实世界中工作，因此亟需能更好支持模拟到现实迁移和泛化能力评估的基准。

Method: 本文基于对现状的分析，提出三方面的基准改进建议：（1）采用高视觉保真度的仿真，以提高策略从模拟到现实的迁移能力；（2）通过系统增加任务复杂度和情景扰动，全面评估策略的鲁棒性；（3）量化现实表现与仿真表现的一致性，以检测模拟评估的有效性。

Result: 论文提出的改进建议为设计能够更好评估策略泛化能力的机器人基准提供了方向，但摘要中未给出具体实验结果。

Conclusion: 论文指出，目前基准在现实应用评估方面存在不足，并提出了三项针对模拟到现实迁移的关键改进建议，为未来设计更具实际意义的机器人基准提供了参考。

Abstract: Current vision-based robotics simulation benchmarks have significantly
advanced robotic manipulation research. However, robotics is fundamentally a
real-world problem, and evaluation for real-world applications has lagged
behind in evaluating generalist policies. In this paper, we discuss challenges
and desiderata in designing benchmarks for generalist robotic manipulation
policies for the goal of sim-to-real policy transfer. We propose 1) utilizing
high visual-fidelity simulation for improved sim-to-real transfer, 2)
evaluating policies by systematically increasing task complexity and scenario
perturbation to assess robustness, and 3) quantifying performance alignment
between real-world performance and its simulation counterparts.

</details>


### [145] [Geometry-Aware Predictive Safety Filters on Humanoids: From Poisson Safety Functions to CBF Constrained MPC](https://arxiv.org/abs/2508.11129)
*Ryan M. Bena,Gilbert Bahati,Blake Werner,Ryan K. Cosner,Lizhi Yang,Aaron D. Ames*

Main category: cs.RO

TL;DR: 本文提出了一种针对具备非对称形态的腿式机器人，在非结构化并动态变化环境中实现安全自主导航的预测性安全滤波算法。该方法结合了控制势垒函数（CBF）和MPC，并可直接从感知数据中数值合成安全约束，提升了路径规划的安全性与适应性。


<details>
  <summary>Details</summary>
Motivation: 腿式机器人由于自身形态复杂以及环境多变，传统路径规划和安全控制方法难以满足机器人在安全关键场景下的实时性和安全性需求。因此需要新的方法更好地融合机器人实际几何特性与外部感知数据，实现灵活而安全的导航。

Method: 提出一种基于非线性模型预测控制（MPC）的预测性安全滤波器，嵌入基于Poisson安全函数自动生成的控制势垒函数（CBF），能结合最新感知信息动态合成安全约束。同时，将传统静态Poisson方程理论推广为可处理时变环境的参数化移动边值问题，并借助Minkowski集运算将环境映射并提升到能考虑机器人具体形态的配置空间。

Result: 该算法被部署在仿人及四足机器人上，通过多种安全关键场景实验验证。结果显示，无论环境静态或动态，方法均能有效提升规划安全性和适应性，表现出了良好的通用性和实时性。

Conclusion: 通过整合Poisson安全函数和CBF约束，本文方案显著提升了机器人在复杂动态环境下安全自主导航的能力，为未来高度自动化的腿式机器人安全控制提供了有效工具和理论基础。

Abstract: Autonomous navigation through unstructured and dynamically-changing
environments is a complex task that continues to present many challenges for
modern roboticists. In particular, legged robots typically possess manipulable
asymmetric geometries which must be considered during safety-critical
trajectory planning. This work proposes a predictive safety filter: a nonlinear
model predictive control (MPC) algorithm for online trajectory generation with
geometry-aware safety constraints based on control barrier functions (CBFs).
Critically, our method leverages Poisson safety functions to numerically
synthesize CBF constraints directly from perception data. We extend the
theoretical framework for Poisson safety functions to incorporate temporal
changes in the domain by reformulating the static Dirichlet problem for
Poisson's equation as a parameterized moving boundary value problem.
Furthermore, we employ Minkowski set operations to lift the domain into a
configuration space that accounts for robot geometry. Finally, we implement our
real-time predictive safety filter on humanoid and quadruped robots in various
safety-critical scenarios. The results highlight the versatility of Poisson
safety functions, as well as the benefit of CBF constrained model predictive
safety-critical controllers.

</details>


### [146] [Actor-Critic for Continuous Action Chunks: A Reinforcement Learning Framework for Long-Horizon Robotic Manipulation with Sparse Reward](https://arxiv.org/abs/2508.11143)
*Jiarui Yang,Bin Zhu,Jingjing Chen,Yu-Gang Jiang*

Main category: cs.RO

TL;DR: 本文提出了AC3（一种用于连续动作分块的Actor-Critic框架），通过特定的稳定机制，有效解决了稀疏奖励场景下的长时序机器人操作任务。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习在长时序、稀疏奖励的机器人操作任务中表现不佳，特别是在稳定、高效地学习连续动作分块上面临挑战。作者希望设计一种既稳定又数据高效的学习方法。

Method: 提出AC3框架：1）actor部分采用不对称更新，只从成功轨迹学习，确保策略改进的可靠性；2）critic部分利用分段内n步回报和自监督内在奖励机制稳定价值学习，从而缓解稀疏奖励问题。

Result: 在BiGym和RLBench两个基准的25个任务上实验，AC3仅用少量演示和简单模型即可在大多数任务上达到更佳成功率。

Conclusion: AC3方法设计有效，在长时序、稀疏奖励的复杂机器人操作任务上展现了优于现有方法的性能。

Abstract: Existing reinforcement learning (RL) methods struggle with long-horizon
robotic manipulation tasks, particularly those involving sparse rewards. While
action chunking is a promising paradigm for robotic manipulation, using RL to
directly learn continuous action chunks in a stable and data-efficient manner
remains a critical challenge. This paper introduces AC3 (Actor-Critic for
Continuous Chunks), a novel RL framework that learns to generate
high-dimensional, continuous action sequences. To make this learning process
stable and data-efficient, AC3 incorporates targeted stabilization mechanisms
for both the actor and the critic. First, to ensure reliable policy
improvement, the actor is trained with an asymmetric update rule, learning
exclusively from successful trajectories. Second, to enable effective value
learning despite sparse rewards, the critic's update is stabilized using
intra-chunk $n$-step returns and further enriched by a self-supervised module
providing intrinsic rewards at anchor points aligned with each action chunk. We
conducted extensive experiments on 25 tasks from the BiGym and RLBench
benchmarks. Results show that by using only a few demonstrations and a simple
model architecture, AC3 achieves superior success rates on most tasks,
validating its effective design.

</details>


### [147] [Visuomotor Grasping with World Models for Surgical Robots](https://arxiv.org/abs/2508.11200)
*Hongbin Lin,Bin Li,Kwok Wai Samuel Au*

Main category: cs.RO

TL;DR: 该论文提出了一种名为GASv2的视觉运动学习框架，用于外科手术中的自主抓取任务，该方法无需专门的物体识别或特征提取，具备较强泛化能力和鲁棒性，并在实际外科环境中取得良好实验效果。


<details>
  <summary>Details</summary>
Motivation: 自动化外科手术中的抓取操作有助于减轻医生负担并提升效率与安全性，但现有方法主要依赖物体位姿跟踪或手工设计特征，难以适应手术中的新物体、视觉干扰及易变形物体，且采用视觉运动学习时还面临实际手术环境中的多种噪声、精度及安全挑战。

Method: 提出GASv2视觉运动学习框架，采用基于世界模型的架构和外科感知流水线处理视觉输入，结合混合式控制系统保证控制安全性；在仿真中通过领域随机化训练以实现sim-to-real转移，仅用单对内窥镜相机获取输入，并在实际机器人上于模型体和外科组织环境下进行测试。

Result: 该方法在两种实际环境中均取得65%的抓取成功率，能够泛化至未见过的物体和夹爪，并能适应多种干扰，显示出较强的性能、泛化和鲁棒性。

Conclusion: GASv2为外科手术自主抓取任务提供了无需专门物体建模且具较强泛化鲁棒性的解决方案，为未来RAS系统自主化和智能化发展奠定基础。

Abstract: Grasping is a fundamental task in robot-assisted surgery (RAS), and
automating it can reduce surgeon workload while enhancing efficiency, safety,
and consistency beyond teleoperated systems. Most prior approaches rely on
explicit object pose tracking or handcrafted visual features, limiting their
generalization to novel objects, robustness to visual disturbances, and the
ability to handle deformable objects. Visuomotor learning offers a promising
alternative, but deploying it in RAS presents unique challenges, such as low
signal-to-noise ratio in visual observations, demands for high safety and
millimeter-level precision, as well as the complex surgical environment. This
paper addresses three key challenges: (i) sim-to-real transfer of visuomotor
policies to ex vivo surgical scenes, (ii) visuomotor learning using only a
single stereo camera pair -- the standard RAS setup, and (iii) object-agnostic
grasping with a single policy that generalizes to diverse, unseen surgical
objects without retraining or task-specific models. We introduce Grasp Anything
for Surgery V2 (GASv2), a visuomotor learning framework for surgical grasping.
GASv2 leverages a world-model-based architecture and a surgical perception
pipeline for visual observations, combined with a hybrid control system for
safe execution. We train the policy in simulation using domain randomization
for sim-to-real transfer and deploy it on a real robot in both phantom-based
and ex vivo surgical settings, using only a single pair of endoscopic cameras.
Extensive experiments show our policy achieves a 65% success rate in both
settings, generalizes to unseen objects and grippers, and adapts to diverse
disturbances, demonstrating strong performance, generality, and robustness.

</details>


### [148] [Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation](https://arxiv.org/abs/2508.11204)
*Hongbin Lin,Juan Rojas,Kwok Wai Samuel Au*

Main category: cs.RO

TL;DR: 本文提出了一种新的部分可观马尔可夫决策过程（POMDP）建模方式，并结合多组等变增强方法（MEA），有效提升了机器人操作任务中的采样效率。


<details>
  <summary>Details</summary>
Motivation: 现实世界中做基于视觉操作的机器人学习时，采样效率非常关键。过去的研究主要关注各任务物体所有时刻采用同一变换（等距对称），但这在实际任务中限制较大，因此作者希望探索更灵活的非等距对称性，以提升学习效率和泛化能力。

Method: 作者提出将非等距对称性结构纳入POMDP建模，并设计了一种简单且有效的数据增强方法——多组等变增强（MEA），可以在空间和时间维度上独立施加多组变换。他们还引入了一种保持平移动变性的基于体素的视觉表征，并将MEA集成到离线强化学习框架中提升采样效率。

Result: 在两个机器人操作领域做了大量仿真和真实机器人实验，结果证明该方法明显提升了采样效率和任务表现。

Conclusion: 非等距对称性与多组等变增强能够有效拓宽机器人操作任务中的数据利用和泛化能力，为高效视觉-动作学习提供了新思路。

Abstract: Sampling efficiency is critical for deploying visuomotor learning in
real-world robotic manipulation. While task symmetry has emerged as a promising
inductive bias to improve efficiency, most prior work is limited to isometric
symmetries -- applying the same group transformation to all task objects across
all timesteps. In this work, we explore non-isometric symmetries, applying
multiple independent group transformations across spatial and temporal
dimensions to relax these constraints. We introduce a novel formulation of the
partially observable Markov decision process (POMDP) that incorporates the
non-isometric symmetry structures, and propose a simple yet effective data
augmentation method, Multi-Group Equivariance Augmentation (MEA). We integrate
MEA with offline reinforcement learning to enhance sampling efficiency, and
introduce a voxel-based visual representation that preserves translational
equivariance. Extensive simulation and real-robot experiments across two
manipulation domains demonstrate the effectiveness of our approach.

</details>


### [149] [Embodied Edge Intelligence Meets Near Field Communication: Concept, Design, and Verification](https://arxiv.org/abs/2508.11232)
*Guoliang Li,Xibin Jin,Yujie Wan,Chenxuan Liu,Tong Zhang,Shuai Wang,Chengzhong Xu*

Main category: cs.RO

TL;DR: 本文提出结合具身边缘智能（EEI）与近场通信（NFC）的新范式——近场具身边缘智能（NEEI），以提升大模型在机器人中的实时推理性能，同时解决通讯带宽与安全等问题。


<details>
  <summary>Details</summary>
Motivation: 当前具身人工智能需要大模型支持，但计算需求大且需实时响应，单一的边缘计算或通信技术难以满足。因此，本论文旨在结合先进的通信与边缘计算手段，提升整体性能。

Method: 提出NEEI范式，将极大规模天线阵列的NFC硬件与EEI结合，围绕资源优化，分别设计了EEI辅助下的NFC无线友好规划及NFC辅助下的视角引导波束聚焦方法，还提出通过协作导航提高资源利用效率。

Result: 实验结果显示，所提出的方法在多项基准测试中优于现有方案，展现了较强的性能提升。

Conclusion: 结合NFC与EEI的NEEI范式为具身智能提供了新的解决方向，并通过联合优化提升了通信与计算效率，实验验证了其有效性，为未来的具身智能与通信联合设计提供了基础。

Abstract: Realizing embodied artificial intelligence is challenging due to the huge
computation demands of large models (LMs). To support LMs while ensuring
real-time inference, embodied edge intelligence (EEI) is a promising paradigm,
which leverages an LM edge to provide computing powers in close proximity to
embodied robots. Due to embodied data exchange, EEI requires higher spectral
efficiency, enhanced communication security, and reduced inter-user
interference. To meet these requirements, near-field communication (NFC), which
leverages extremely large antenna arrays as its hardware foundation, is an
ideal solution. Therefore, this paper advocates the integration of EEI and NFC,
resulting in a near-field EEI (NEEI) paradigm. However, NEEI also introduces
new challenges that cannot be adequately addressed by isolated EEI or NFC
designs, creating research opportunities for joint optimization of both
functionalities. To this end, we propose radio-friendly embodied planning for
EEI-assisted NFC scenarios and view-guided beam-focusing for NFC-assisted EEI
scenarios. We also elaborate how to realize resource-efficient NEEI through
opportunistic collaborative navigation. Experimental results are provided to
confirm the superiority of the proposed techniques compared with various
benchmarks.

</details>


### [150] [Tactile Robotics: An Outlook](https://arxiv.org/abs/2508.11261)
*Shan Luo,Nathan F. Lepora,Wenzhen Yuan,Kaspar Althoefer,Gordon Cheng,Ravinder Dahiya*

Main category: cs.RO

TL;DR: 本文综述了触觉机器人研究的最新进展、面临的挑战和未来发展方向，强调多领域应用前景。


<details>
  <summary>Details</summary>
Motivation: 实现机器人类比生物系统的触觉感知能力，以满足机器人与人类安全、高效共存和互动的广泛需求。

Method: 回顾并分析了多种触觉传感技术（如压阻式、压电式、电容式、磁性和光学传感器）及其在机器人中的应用，探讨了传感与数据模拟、算法、与视觉等其他模态的集成。同时评估了未来发展的挑战和可行的整体性策略。

Result: 触觉传感技术已取得巨大进展，应用于多种机器人交互场景，并得到了大规模数据模拟和多模态融合的支持。但依然存在集成、实用性和智能性等挑战。

Conclusion: 为推动触觉机器人领域的跨界创新和应用，需采取系统和整体的研究方法，解决传感集成、数据解释和实际应用中的瓶颈。

Abstract: Robotics research has long sought to give robots the ability to perceive the
physical world through touch in an analogous manner to many biological systems.
Developing such tactile capabilities is important for numerous emerging
applications that require robots to co-exist and interact closely with humans.
Consequently, there has been growing interest in tactile sensing, leading to
the development of various technologies, including piezoresistive and
piezoelectric sensors, capacitive sensors, magnetic sensors, and optical
tactile sensors. These diverse approaches utilise different transduction
methods and materials to equip robots with distributed sensing capabilities,
enabling more effective physical interactions. These advances have been
supported in recent years by simulation tools that generate large-scale tactile
datasets to support sensor designs and algorithms to interpret and improve the
utility of tactile data. The integration of tactile sensing with other
modalities, such as vision, as well as with action strategies for active
tactile perception highlights the growing scope of this field. To further the
transformative progress in tactile robotics, a holistic approach is essential.
In this outlook article, we examine several challenges associated with the
current state of the art in tactile robotics and explore potential solutions to
inspire innovations across multiple domains, including manufacturing,
healthcare, recycling and agriculture.

</details>


### [151] [Learning Differentiable Reachability Maps for Optimization-based Humanoid Motion Generation](https://arxiv.org/abs/2508.11275)
*Masaki Murooka,Iori Kumagai,Mitsuharu Morisawa,Fumio Kanehiro*

Main category: cs.RO

TL;DR: 本文提出了一种可微分的可达性地图，用于高效地生成仿人机器人运动方案，通过神经网络或支持向量机学习可达性地图，并将其作为连续优化约束，显著提升了多种仿人机器人运动规划问题的效率。


<details>
  <summary>Details</summary>
Motivation: 仿人机器人在运动生成时，传统的可达性表示方法难以直接应用于连续优化，同时计算开销较大。为降低计算成本并提高优化效率，亟需一种既能精确表达末端执行器可达区域，又能与优化方法无缝集成的新型表现形式。

Method: 作者提出将可达性表示为任务空间上的可微分标量函数，仅在可达区域取正值。该函数通过仿人机器人末端执行器的正向运动学采样得到的姿态数据，借助神经网络或支持向量机进行拟合学习。学习后的可微分可达性地图被直接作为连续优化约束融入运动规划框架中。

Result: 通过将该可微分可达性约束应用于连续优化，能够高效地求解包括步态规划、多接触运动规划及动作操作等在内的多个仿人机器人运动规划任务，展示了优越的效率与应用广度。

Conclusion: 文中方法不仅降低了运动生成中的计算成本，还提高了优化精度，可为复杂仿人机器人任务的高效运动规划提供新思路和实用工具。

Abstract: To reduce the computational cost of humanoid motion generation, we introduce
a new approach to representing robot kinematic reachability: the differentiable
reachability map. This map is a scalar-valued function defined in the task
space that takes positive values only in regions reachable by the robot's
end-effector. A key feature of this representation is that it is continuous and
differentiable with respect to task-space coordinates, enabling its direct use
as constraints in continuous optimization for humanoid motion planning. We
describe a method to learn such differentiable reachability maps from a set of
end-effector poses generated using a robot's kinematic model, using either a
neural network or a support vector machine as the learning model. By
incorporating the learned reachability map as a constraint, we formulate
humanoid motion generation as a continuous optimization problem. We demonstrate
that the proposed approach efficiently solves various motion planning problems,
including footstep planning, multi-contact motion planning, and
loco-manipulation planning for humanoid robots.

</details>


### [152] [Scene Graph-Guided Proactive Replanning for Failure-Resilient Embodied Agent](https://arxiv.org/abs/2508.11286)
*Che Rin Yu,Daewon Chae,Dabin Seo,Sangwon Lee,Hyeongwoo Im,Jinkyu Kim*

Main category: cs.RO

TL;DR: 论文提出了一种前瞻性重规划框架，通过实时比对场景图检测任务中的潜在失败并及时修正，从而提升机器人对环境变化的适应能力，增强自主任务执行的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前多数自主机器人只在任务失败后才进行重规划，缺乏对环境微小变化的前瞻性适应，往往导致执行失败。已有前瞻性方法依赖大量人工规则与监督，灵活性和推广性较差。

Method: 该方法采用RGB-D传感器，从当前观测生成场景图，并与参考演示中的成功场景图进行比对。当检测到偏差时，调用轻量级推理模块，诊断场景不匹配的原因，并调整执行计划，使机器人能主动应对环境变化。

Result: 实验证明，该方法能在失败发生前及时发现语义和空间上的不一致，提前调整计划，有效提升了机器人任务成功率和鲁棒性。

Conclusion: 前瞻性重规划框架相比于传统被动重规划方法，提高了机器人环境适应性和完成任务的成功率，为鲁棒自主系统的实现提供了新途径。

Abstract: When humans perform everyday tasks, we naturally adjust our actions based on
the current state of the environment. For instance, if we intend to put
something into a drawer but notice it is closed, we open it first. However,
many autonomous robots lack this adaptive awareness. They often follow
pre-planned actions that may overlook subtle yet critical changes in the scene,
which can result in actions being executed under outdated assumptions and
eventual failure. While replanning is critical for robust autonomy, most
existing methods respond only after failures occur, when recovery may be
inefficient or infeasible. While proactive replanning holds promise for
preventing failures in advance, current solutions often rely on manually
designed rules and extensive supervision. In this work, we present a proactive
replanning framework that detects and corrects failures at subtask boundaries
by comparing scene graphs constructed from current RGB-D observations against
reference graphs extracted from successful demonstrations. When the current
scene fails to align with reference trajectories, a lightweight reasoning
module is activated to diagnose the mismatch and adjust the plan. Experiments
in the AI2-THOR simulator demonstrate that our approach detects semantic and
spatial mismatches before execution failures occur, significantly improving
task success and robustness.

</details>


### [153] [A Recursive Total Least Squares Solution for Bearing-Only Target Motion Analysis and Circumnavigation](https://arxiv.org/abs/2508.11289)
*Lin Li,Xueming Liu,Zhoujingzi Qiu,Tianjiang Hu,Qingrui Zhang*

Main category: cs.RO

TL;DR: 本文提出了递归型全最小二乘(RTLS)方法和绕行控制器，用于解决仅方位目标运动分析中观测性差和估计算法收敛性不足的问题，并在仿真与实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 仅方位目标运动分析因易于获取方位角而具吸引力，但由于测量模型的非线性和缺乏距离信息，导致系统难以观测且状态估计收敛差。本文旨在克服这些难点，提高跟踪精度与鲁棒性。

Method: 本文提出了一种递归全最小二乘(RTLS)方法，改进了传统的估计算法以降低位置偏差并提升计算效率。同时，设计了绕行控制器，引导移动观测者环绕目标轨道，进一步增强观测性和收敛性。

Result: 通过大量仿真和实验验证，证明所提出方法在目标定位与跟踪精度、系统鲁棒性方面均显著优于伪线性卡尔曼滤波等传统方法。

Conclusion: 本文方法有效提升了仅方位目标运动分析中定位与跟踪的精度和系统稳定性，对实际移动观测场景具有良好应用前景。

Abstract: Bearing-only Target Motion Analysis (TMA) is a promising technique for
passive tracking in various applications as a bearing angle is easy to measure.
Despite its advantages, bearing-only TMA is challenging due to the nonlinearity
of the bearing measurement model and the lack of range information, which
impairs observability and estimator convergence. This paper addresses these
issues by proposing a Recursive Total Least Squares (RTLS) method for online
target localization and tracking using mobile observers. The RTLS approach,
inspired by previous results on Total Least Squares (TLS), mitigates biases in
position estimation and improves computational efficiency compared to
pseudo-linear Kalman filter (PLKF) methods. Additionally, we propose a
circumnavigation controller to enhance system observability and estimator
convergence by guiding the mobile observer in orbit around the target.
Extensive simulations and experiments are performed to demonstrate the
effectiveness and robustness of the proposed method. The proposed algorithm is
also compared with the state-of-the-art approaches, which confirms its superior
performance in terms of both accuracy and stability.

</details>


### [154] [Pedestrian Dead Reckoning using Invariant Extended Kalman Filter](https://arxiv.org/abs/2508.11396)
*Jingran Zhang,Zhengzhang Yan,Yiming Chen,Zeqiang He,Jiahao Chen*

Main category: cs.RO

TL;DR: 本文提出了一种适用于GPS不可用环境下双足机器人行走的低成本惯性行人死 reckoning 方法，并通过仿真实验与真实应用验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在GPS信号不可用的环境下，传统定位方法无法满足双足机器人对定位精度和实时性的需求，因此需要发展新的低成本、高性能的惯性导航方法。

Method: 利用安装在支撑脚上的惯性测量单元（IMU），在脚步静止期间通过伪观测修正惯导系统漂移，并采用基于矩阵李群的InEKF（不变量扩展卡尔曼滤波）算法，详细给出了InEKF的理论基础。方法通过运动捕捉、楼层步行和真实机器人实验进行对比分析。

Result: 实验在运动捕捉、楼层行走和实际机器人平台上分别对比了InEKF与传统EKF的性能，结果表明所提方法精度更高且调参性更好。灵敏度分析显示InEKF相比EKF更加易于调试和优化。

Conclusion: 所提出的惯性导航方法能够有效提升双足机器人在无GPS环境下的定位准确性和实用性，且具备更好的参数可调性，适合实际智能机器人系统应用。

Abstract: This paper presents a cost-effective inertial pedestrian dead reckoning
method for the bipedal robot in the GPS-denied environment. Each time when the
inertial measurement unit (IMU) is on the stance foot, a stationary
pseudo-measurement can be executed to provide innovation to the IMU measurement
based prediction. The matrix Lie group based theoretical development of the
adopted invariant extended Kalman filter (InEKF) is set forth for tutorial
purpose. Three experiments are conducted to compare between InEKF and standard
EKF, including motion capture benchmark experiment, large-scale multi-floor
walking experiment, and bipedal robot experiment, as an effort to show our
method's feasibility in real-world robot system. In addition, a sensitivity
analysis is included to show that InEKF is much easier to tune than EKF.

</details>


### [155] [An Exploratory Study on Crack Detection in Concrete through Human-Robot Collaboration](https://arxiv.org/abs/2508.11404)
*Junyeon Kim,Tianshu Ruan,Cesar Alan Contreras,Manolis Chiou*

Main category: cs.RO

TL;DR: 本论文提出了结合AI视觉裂缝检测与移动机器人（Jackal平台）的核设施结构巡检方案，实验显示协作方式优于传统人工方法。


<details>
  <summary>Details</summary>
Motivation: 核设施结构巡检依赖人工方式存在安全风险高、认知负担重及准确性不足等问题，急需更安全、高效、准确的新方法。AI与机器人技术的进步为此提供了可能。

Method: 研究在移动Jackal机器人平台上集成AI辅助的视觉裂缝检测算法。通过人机协作（HRC）模式开展核设施结构巡检实验，并与传统人工方法进行对比分析。

Result: 实验结果表明，人机协作方式提升了裂缝检测的准确率，显著降低了人工操作负担，具有相较传统巡检更优越的性能。

Conclusion: AI与机器人结合的人机协作巡检模式在核设施结构检测中可有效提升安全性、效率和准确性，优于传统人工巡检方法。

Abstract: Structural inspection in nuclear facilities is vital for maintaining
operational safety and integrity. Traditional methods of manual inspection pose
significant challenges, including safety risks, high cognitive demands, and
potential inaccuracies due to human limitations. Recent advancements in
Artificial Intelligence (AI) and robotic technologies have opened new
possibilities for safer, more efficient, and accurate inspection methodologies.
Specifically, Human-Robot Collaboration (HRC), leveraging robotic platforms
equipped with advanced detection algorithms, promises significant improvements
in inspection outcomes and reductions in human workload. This study explores
the effectiveness of AI-assisted visual crack detection integrated into a
mobile Jackal robot platform. The experiment results indicate that HRC enhances
inspection accuracy and reduces operator workload, resulting in potential
superior performance outcomes compared to traditional manual methods.

</details>


### [156] [Open, Reproducible and Trustworthy Robot-Based Experiments with Virtual Labs and Digital-Twin-Based Execution Tracing](https://arxiv.org/abs/2508.11406)
*Benjamin Alt,Mareike Picklum,Sorin Arion,Franklin Kenghagho Kenfack,Michael Beetz*

Main category: cs.RO

TL;DR: 该论文提出了用于机器人科学实验可重复性与透明度的两个关键工具，包括语义执行追踪框架和云端虚拟研究平台。


<details>
  <summary>Details</summary>
Motivation: 当前机器人执行科学实验时，存在结果难以复现和过程不透明的问题，限制了其作为科学发现参与者的作用。

Method: 方法包括提出语义执行追踪框架——将传感器数据与有语义注释的机器人信念状态日志结合，以及AICOR虚拟研究楼（VRB）——支持机器人任务在云端共享、复现和验证。

Result: 实现了机器人实验的自动化、可追溯和易复现，支持规模化共享与验证。

Conclusion: 这些工具为实现可复现实验和机器人驱动的科学研究奠定了基础，推进了自主系统参与科学发现的进程。

Abstract: We envision a future in which autonomous robots conduct scientific
experiments in ways that are not only precise and repeatable, but also open,
trustworthy, and transparent. To realize this vision, we present two key
contributions: a semantic execution tracing framework that logs sensor data
together with semantically annotated robot belief states, ensuring that
automated experimentation is transparent and replicable; and the AICOR Virtual
Research Building (VRB), a cloud-based platform for sharing, replicating, and
validating robot task executions at scale. Together, these tools enable
reproducible, robot-driven science by integrating deterministic execution,
semantic memory, and open knowledge representation, laying the foundation for
autonomous systems to participate in scientific discovery.

</details>


### [157] [EvoPSF: Online Evolution of Autonomous Driving Models via Planning-State Feedback](https://arxiv.org/abs/2508.11453)
*Jiayue Jin,Lang Qian,Jingyu Zhang,Chuanyu Ju,Liang Song*

Main category: cs.RO

TL;DR: 该论文提出了一种新型在线进化框架EvoPSF，通过实时利用规划状态反馈提升自动驾驶系统的泛化与适应能力。


<details>
  <summary>Details</summary>
Motivation: 目前大多数自动驾驶方法为离线训练，部署后无法适应新环境，导致在真实世界新变动场景下泛化能力差。

Method: 作者提出EvoPSF框架，关注于规划失败通常源自目标级运动预测不准确。方法以规控里的不确定性作为触发信号，结合规划器的注意力机制定位关键目标对象，并对其采用自监督损失函数进行针对性在线更新，提高模型适应性。

Result: 在nuScenes跨区域及数据变异集上实验证明，EvoPSF能在多种复杂环境下稳定提升自动驾驶系统的规划表现。

Conclusion: EvoPSF摆脱了“只训练一次、永久部署”模式，使自动驾驶系统更能应对环境变化，提升了预测精准度与规划稳定性。

Abstract: Recent years have witnessed remarkable progress in autonomous driving, with
systems evolving from modular pipelines to end-to-end architectures. However,
most existing methods are trained offline and lack mechanisms to adapt to new
environments during deployment. As a result, their generalization ability
diminishes when faced with unseen variations in real-world driving scenarios.
In this paper, we break away from the conventional "train once, deploy forever"
paradigm and propose EvoPSF, a novel online Evolution framework for autonomous
driving based on Planning-State Feedback. We argue that planning failures are
primarily caused by inaccurate object-level motion predictions, and such
failures are often reflected in the form of increased planner uncertainty. To
address this, we treat planner uncertainty as a trigger for online evolution,
using it as a diagnostic signal to initiate targeted model updates. Rather than
performing blind updates, we leverage the planner's agent-agent attention to
identify the specific objects that the ego vehicle attends to most, which are
primarily responsible for the planning failures. For these critical objects, we
compute a targeted self-supervised loss by comparing their predicted waypoints
from the prediction module with their actual future positions, selected from
the perception module's outputs with high confidence scores. This loss is then
backpropagated to adapt the model online. As a result, our method improves the
model's robustness to environmental changes, leads to more precise motion
predictions, and therefore enables more accurate and stable planning behaviors.
Experiments on both cross-region and corrupted variants of the nuScenes dataset
demonstrate that EvoPSF consistently improves planning performance under
challenging conditions.

</details>


### [158] [OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal Navigation](https://arxiv.org/abs/2508.11479)
*Tatiana Zemskova,Aleksei Staroverov,Dmitry Yudin,Aleksandr Panov*

Main category: cs.RO

TL;DR: 本文介绍了Open-vocabulary Object Goal Navigation的一个新策略OVSegDT，能够让机器人根据任意目标物体描述导航，并在未知类别上取得领先结果。该方法提升了泛化能力和安全性，降低了训练复杂度和碰撞率。


<details>
  <summary>Details</summary>
Motivation: 现有方法容易过拟合、泛化能力差，并且会出现较多不安全行为如碰撞，导致不能很好地完成自由语义目标导航任务。

Method: 提出了OVSegDT轻量级transformer方案：1）通过语义分支与目标mask编码和辅助分割损失，让目标描述得到空间定位；2）引入熵自适应损失调节机制，根据策略熵动态平衡仿真和强化信号，无需手动阶段切换。

Result: 训练样本消耗减少33%，碰撞降低一倍。仅用RGB输入情况下，在数据集HM3D-OVON上未见物体类别与已见类别表现持平，取得40.1%成功率，20.9%路径效率，超越同类方法。

Conclusion: OVSegDT在提升open-vocabulary导航泛化与安全性、减少训练成本，并在性能上达到了新SOTA，无需深度、里程计等额外输入。

Abstract: Open-vocabulary Object Goal Navigation requires an embodied agent to reach
objects described by free-form language, including categories never seen during
training. Existing end-to-end policies overfit small simulator datasets,
achieving high success on training scenes but failing to generalize and
exhibiting unsafe behaviour (frequent collisions). We introduce OVSegDT, a
lightweight transformer policy that tackles these issues with two synergistic
components. The first component is the semantic branch, which includes an
encoder for the target binary mask and an auxiliary segmentation loss function,
grounding the textual goal and providing precise spatial cues. The second
component consists of a proposed Entropy-Adaptive Loss Modulation, a per-sample
scheduler that continuously balances imitation and reinforcement signals
according to the policy entropy, eliminating brittle manual phase switches.
These additions cut the sample complexity of training by 33%, and reduce
collision count in two times while keeping inference cost low (130M parameters,
RGB-only input). On HM3D-OVON, our model matches the performance on unseen
categories to that on seen ones and establishes state-of-the-art results (40.1%
SR, 20.9% SPL on val unseen) without depth, odometry, or large vision-language
models. Code is available at https://github.com/CognitiveAISystems/OVSegDT.

</details>


### [159] [i2Nav-Robot: A Large-Scale Indoor-Outdoor Robot Dataset for Multi-Sensor Fusion Navigation and Mapping](https://arxiv.org/abs/2508.11485)
*Hailiang Tang,Tisheng Zhang,Liqiang Wang,Xin Ding,Man Yuan,Zhiyu Xiang,Jujin Chen,Yuhan Bian,Shuangyan Liu,Yuqing Wang,Guan Wang,Xiaoji Niu*

Main category: cs.RO

TL;DR: 本文提出了i2Nav-Robot数据集，旨在为无人地面车辆（UGV）的多传感器融合导航与测绘提供高质量大规模数据，涵盖多样化的室内外场景，弥补现有数据集的不足。


<details>
  <summary>Details</summary>
Motivation: 现有UGV相关数据集在传感器配置、时间同步、精确测真值和场景多样性方面存在局限，难以支持更高级的导航与测绘方法发展，因此急需更高质量和多样化的数据支持。

Method: 作者在全向轮机器人上搭载了多种新型传感器（前视与360°固态激光雷达、4D雷达、双目摄像头、里程计、GNSS接收机和IMU），通过硬件在线同步与离线校准保证所有传感器的时间戳精度，采集了包括室外街道、室内停车场等共计约17060米路径的10段大规模数据序列。高精度真值数据经过集成导航与航姿参考系统的后处理获得，空间位置达到厘米级准确率。

Result: i2Nav-Robot数据集在超过十种开源多传感器融合系统上进行了评测，显示出数据质量优异，能支持复杂的场景和室内外混合环境的导航与地图构建。

Conclusion: 提出的i2Nav-Robot数据集在传感器多样性、时间同步和高精度真值等方面具有明显优势，可为多传感器融合导航与测绘系统的开发和评估提供有力的数据支持，为无人地面车辆研究带来价值。

Abstract: Accurate and reliable navigation is crucial for autonomous unmanned ground
vehicle (UGV). However, current UGV datasets fall short in meeting the demands
for advancing navigation and mapping techniques due to limitations in sensor
configuration, time synchronization, ground truth, and scenario diversity. To
address these challenges, we present i2Nav-Robot, a large-scale dataset
designed for multi-sensor fusion navigation and mapping in indoor-outdoor
environments. We integrate multi-modal sensors, including the newest front-view
and 360-degree solid-state LiDARs, 4-dimensional (4D) radar, stereo cameras,
odometer, global navigation satellite system (GNSS) receiver, and inertial
measurement units (IMU) on an omnidirectional wheeled robot. Accurate
timestamps are obtained through both online hardware synchronization and
offline calibration for all sensors. The dataset comprises ten larger-scale
sequences covering diverse UGV operating scenarios, such as outdoor streets,
and indoor parking lots, with a total length of about 17060 meters.
High-frequency ground truth, with centimeter-level accuracy for position, is
derived from post-processing integrated navigation methods using a
navigation-grade IMU. The proposed i2Nav-Robot dataset is evaluated by more
than ten open-sourced multi-sensor fusion systems, and it has proven to have
superior data quality.

</details>


### [160] [Relative Position Matters: Trajectory Prediction and Planning with Polar Representation](https://arxiv.org/abs/2508.11492)
*Bozhou Zhang,Nan Song,Bingzhao Gao,Li Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种基于极坐标系的自动驾驶轨迹预测与规划方法Polaris，在Argoverse 2和nuPlan等基准测试上取得了最优效果。


<details>
  <summary>Details</summary>
Motivation: 现有的轨迹预测方法主要在笛卡尔坐标系下建模，但这种方式无法有效捕捉不同交通元素之间在距离和方向上的相对影响。提升对空间关系的建模能力对于动态环境下的决策尤为重要。

Method: 作者采用极坐标系（半径和角度表示位置），提出Polaris方法，全流程在极坐标系中工作。该方法包含专门用于极坐标空间关系编码和精细建模的模块，以更直观和结构化地建模距离与方向的变化及交通元素间的相对关系。

Result: Polaris方法在Argoverse 2预测和nuPlan规划两个具有挑战性的基准上进行了大量实验，均取得了领先的结果，达到SOTA水平。

Conclusion: 在极坐标系下处理轨迹预测与规划，不仅能更好建模相对关系，还提升了模型性能。Polaris为自动驾驶动态环境下的决策与轨迹生成提供了新思路和有效工具。

Abstract: Trajectory prediction and planning in autonomous driving are highly
challenging due to the complexity of predicting surrounding agents' movements
and planning the ego agent's actions in dynamic environments. Existing methods
encode map and agent positions and decode future trajectories in Cartesian
coordinates. However, modeling the relationships between the ego vehicle and
surrounding traffic elements in Cartesian space can be suboptimal, as it does
not naturally capture the varying influence of different elements based on
their relative distances and directions. To address this limitation, we adopt
the Polar coordinate system, where positions are represented by radius and
angle. This representation provides a more intuitive and effective way to model
spatial changes and relative relationships, especially in terms of distance and
directional influence. Based on this insight, we propose Polaris, a novel
method that operates entirely in Polar coordinates, distinguishing itself from
conventional Cartesian-based approaches. By leveraging the Polar
representation, this method explicitly models distance and direction variations
and captures relative relationships through dedicated encoding and refinement
modules, enabling more structured and spatially aware trajectory prediction and
planning. Extensive experiments on the challenging prediction (Argoverse 2) and
planning benchmarks (nuPlan) demonstrate that Polaris achieves state-of-the-art
performance.

</details>


### [161] [Swarm-in-Blocks: Simplifying Drone Swarm Programming with Block-Based Language](https://arxiv.org/abs/2508.11498)
*Agnes Bressan de Almeida,Joao Aires Correa Fernandes Marsicano*

Main category: cs.RO

TL;DR: Swarm in Blocks是一种基于模块拖拽语言的无人机集群编程高阶接口，旨在让用户以简单、直观的方式编程和管理无人机集群。2023年发布的2.0版本进一步优化了平台体验。


<details>
  <summary>Details</summary>
Motivation: 随着无人机集群在送货、农业、安防等领域应用越来越广泛，管理和编程难度加大，尤其难以被初学者快速掌握。作者希望降低无人机集群管理门槛，让非ROS及编程高手也能便捷使用。

Method: 开发了以Clover平台为基础、以模块拖拽方式（类似Scratch）的集群编程接口，支持循环、条件等程序结构，用户通过拼接代码块完成集群控制逻辑。

Result: Swarm in Blocks显著降低了无人机集群的编程难度，使更多初学者能够入门和操作无人机编队，有助于无人机相关编程的教育普及。2.0版本进一步提升用户体验和函数完善度。

Conclusion: 该平台让无人机集群管理与编程更加友好易用，提升了教育和工程实践的普及性，对无人机应用推广和相关教育具有积极意义。

Abstract: Swarm in Blocks, originally developed for CopterHack 2022, is a high-level
interface that simplifies drone swarm programming using a block-based language.
Building on the Clover platform, this tool enables users to create
functionalities like loops and conditional structures by assembling code
blocks. In 2023, we introduced Swarm in Blocks 2.0, further refining the
platform to address the complexities of swarm management in a user-friendly
way. As drone swarm applications grow in areas like delivery, agriculture, and
surveillance, the challenge of managing them, especially for beginners, has
also increased. The Atena team developed this interface to make swarm handling
accessible without requiring extensive knowledge of ROS or programming. The
block-based approach not only simplifies swarm control but also expands
educational opportunities in programming.

</details>


### [162] [Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media](https://arxiv.org/abs/2508.11503)
*Andrej Orsula,Matthieu Geist,Miguel Olivares-Mendez,Carol Martinez*

Main category: cs.RO

TL;DR: 本文提出了一套完整的端到端流程，将在大规模随机物理仿真环境下训练的强化学习导航策略，成功无调试地迁移到真正在月球类似地形上的无人车，实现跨越传统仿真-现实鸿沟的可靠导航。


<details>
  <summary>Details</summary>
Motivation: 在复杂的行星表面复杂地形上实现自主导航是未来太空探索的重要需求，但受限于轮式机器人与松散介质相互作用的动态复杂性，仿真与现实存在巨大差距，严重制约了基于学习的控制策略实际部署。

Method: 采用大规模并行仿真、随机参数与程序生成环境训练强化学习导航策略，并系统对比多种强化学习算法与动作平滑滤波器。同时，研究了高精度粒子物理微调带来的收益与代价。最后，将训练好的策略直接无调试地迁移到实际月壤模拟场地的无人车上进行测试。

Result: 实验表明，采用程序化多样性训练的智能体，较传统静态场景训练的策略，在现实中的零样本（zero-shot）迁移表现更优。高精度粒子物理微调在低速精度下有细微提升，但计算代价高昂。系统评测识别出适用于现实部署的算法与参数组合。

Conclusion: 本文提出的端到端流程显著提升了学习型导航系统从仿真到现实的可靠性与泛化能力，为未来行星机器人自主导航的实际部署打下基础，迈出了关键一步。

Abstract: Reliable autonomous navigation across the unstructured terrains of distant
planetary surfaces is a critical enabler for future space exploration. However,
the deployment of learning-based controllers is hindered by the inherent
sim-to-real gap, particularly for the complex dynamics of wheel interactions
with granular media. This work presents a complete sim-to-real framework for
developing and validating robust control policies for dynamic waypoint tracking
on such challenging surfaces. We leverage massively parallel simulation to
train reinforcement learning agents across a vast distribution of procedurally
generated environments with randomized physics. These policies are then
transferred zero-shot to a physical wheeled rover operating in a lunar-analogue
facility. Our experiments systematically compare multiple reinforcement
learning algorithms and action smoothing filters to identify the most effective
combinations for real-world deployment. Crucially, we provide strong empirical
evidence that agents trained with procedural diversity achieve superior
zero-shot performance compared to those trained on static scenarios. We also
analyze the trade-offs of fine-tuning with high-fidelity particle physics,
which offers minor gains in low-speed precision at a significant computational
cost. Together, these contributions establish a validated workflow for creating
reliable learning-based navigation systems, marking a critical step towards
deploying autonomous robots in the final frontier.

</details>


### [163] [A Comparative Study of Floating-Base Space Parameterizations for Agile Whole-Body Motion Planning](https://arxiv.org/abs/2508.11520)
*Evangelos Tsiatsianas,Chairi Kiourt,Konstantinos Chatzilygeroudis*

Main category: cs.RO

TL;DR: 本文系统性比较了不同的浮动基参数化方法在腿型和类人机器人敏捷运动轨迹优化中的表现，并提出了一种基于SE(3)切空间的新表示法，有助于规范和优化机器人运动生成。


<details>
  <summary>Details</summary>
Motivation: 过去针对敏捷机器人生成全身运动的轨迹优化方法众多，但缺乏关于浮动基空间参数化选择（即如何表达机器人底座的空间状态）对性能影响的明确指南。复杂的接触动力学行为让对比不同参数化方法显得尤为重要。

Method: 采用直接转录法进行轨迹优化，并在相同优化设置下，系统性评测了数种常见浮动基参数化方法。同时，提出了一种新的基于SE(3)切空间的参数化方法，用于描述机器人浮动基的位姿，使之无需特殊流形优化技巧即可兼容现有数值求解器，实现运动优化。

Result: 实验表明，不同的参数化方法对优化性能具有显著影响。新提出的SE(3)切空间参数化方案在兼容性和数值优化表现上展现出优势，为更敏捷、更大幅度的运动提供了理论和实践基础。

Conclusion: 论文的对比分析和新方法为敏捷机器人整身运动的参数化选择提供了明确建议。新提出的SE(3)切空间方案有效提升了优化易用性与表现，对后续相关研究和工程实现具有指导意义。

Abstract: Automatically generating agile whole-body motions for legged and humanoid
robots remains a fundamental challenge in robotics. While numerous trajectory
optimization approaches have been proposed, there is no clear guideline on how
the choice of floating-base space parameterization affects performance,
especially for agile behaviors involving complex contact dynamics. In this
paper, we present a comparative study of different parameterizations for direct
transcription-based trajectory optimization of agile motions in legged systems.
We systematically evaluate several common choices under identical optimization
settings to ensure a fair comparison. Furthermore, we introduce a novel
formulation based on the tangent space of SE(3) for representing the robot's
floating-base pose, which, to our knowledge, has not received attention from
the literature. This approach enables the use of mature off-the-shelf numerical
solvers without requiring specialized manifold optimization techniques. We hope
that our experiments and analysis will provide meaningful insights for
selecting the appropriate floating-based representation for agile whole-body
motion generation.

</details>


### [164] [MultiPark: Multimodal Parking Transformer with Next-Segment Prediction](https://arxiv.org/abs/2508.11537)
*Han Zheng,Zikang Zhou,Guli Zhang,Zhepei Wang,Kaixuan Wang,Peiliang Li,Shaojie Shen,Ming Yang,Tong Qin*

Main category: cs.RO

TL;DR: 本论文提出了MultiPark模型，能够在车位有限且不规则的空间内实现多样化、鲁棒的自动泊车，取得了领先性能并在真车测试中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 当前利用模仿学习进行自动泊车取得一定进展，但未能考虑泊车行为的多模态性，即同一场景下可能有多种合理泊车路径。同时，模仿学习方法存在因果混淆问题，不易泛化到复杂停车场景中。因此亟需设计能兼顾多模态输出和因果消融的泊车方法。

Method: 提出了基于自回归Transformer的MultiPark框架。采用数据高效的“下一个片段预测”方法以提升空间泛化和时间外推能力，并设计了可学习的、因子分解的泊车查询（分为挂档、纵向、横向）以并行解码多样泊车行为。此外，在损失函数中引入目标中心位姿与自车碰撞等结果导向因素，超越纯模仿损失，缓解因果混淆。

Result: 在真实世界数据集上，MultiPark模型在多种泊车场景下取得了业界最优性能，并通过实际车辆部署，展现出鲁棒且高效的泊车能力。

Conclusion: MultiPark实现了多模态、多路径、泛化能力强的自动泊车方案，有效缓解了模仿学习的局限，能适应各种实际泊车需求，对实际应用有较高价值。

Abstract: Parking accurately and safely in highly constrained spaces remains a critical
challenge. Unlike structured driving environments, parking requires executing
complex maneuvers such as frequent gear shifts and steering saturation. Recent
attempts to employ imitation learning (IL) for parking have achieved promising
results. However, existing works ignore the multimodal nature of parking
behavior in lane-free open space, failing to derive multiple plausible
solutions under the same situation. Notably, IL-based methods encompass
inherent causal confusion, so enabling a neural network to generalize across
diverse parking scenarios is particularly difficult. To address these
challenges, we propose MultiPark, an autoregressive transformer for multimodal
parking. To handle paths filled with abrupt turning points, we introduce a
data-efficient next-segment prediction paradigm, enabling spatial
generalization and temporal extrapolation. Furthermore, we design learnable
parking queries factorized into gear, longitudinal, and lateral components,
parallelly decoding diverse parking behaviors. To mitigate causal confusion in
IL, our method employs target-centric pose and ego-centric collision as
outcome-oriented loss across all modalities beyond pure imitation loss.
Evaluations on real-world datasets demonstrate that MultiPark achieves
state-of-the-art performance across various scenarios. We deploy MultiPark on a
production vehicle, further confirming our approach's robustness in real-world
parking environments.

</details>


### [165] [Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs with Suspended Payloads](https://arxiv.org/abs/2508.11547)
*Martin Jiroušek,Tomáš Báča,Martin Saska*

Main category: cs.RO

TL;DR: 本论文提出了一种仅依赖常规机载传感器（RTK GNSS与IMU）即可对无人机吊挂载荷进行定位与控制的系统，无需额外昂贵的硬件，实现了高效、鲁棒的实际部署。


<details>
  <summary>Details</summary>
Motivation: 现有的无人机吊挂载荷定位方法通常依赖昂贵或复杂的硬件（如运动捕捉系统、额外摄像头或特殊载荷），这限制了其实际应用。为推动实际部署，需要开发只依靠标准机载传感器的方案。

Method: 系统建模了无人机与载荷的耦合动力学，并集成线性Kalman滤波器进行状态估计，结合模型预测轮廓控制规划器（model predictive contouring control planner）与增量式模型预测控制器（incremental MPC）进行控制，在有限的传感与估计条件下保证性能。

Result: 仿真显示，该系统的控制性能可与基于真实地面测量的控制方法媲美，性能损失低于6%，且对载荷参数变化表现出强鲁棒性。实地野外试验验证了该系统基于商用机载硬件的可靠性和可行性。

Conclusion: 所提方法无需额外昂贵设备，仅用标准机载传感器即可实现对空中吊挂载荷有效且鲁棒的估计与控制，具有实际大规模部署的潜力。

Abstract: This paper addresses the problem of tracking the position of a
cable-suspended payload carried by an unmanned aerial vehicle, with a focus on
real-world deployment and minimal hardware requirements. In contrast to many
existing approaches that rely on motion-capture systems, additional onboard
cameras, or instrumented payloads, we propose a framework that uses only
standard onboard sensors--specifically, real-time kinematic global navigation
satellite system measurements and data from the onboard inertial measurement
unit--to estimate and control the payload's position. The system models the
full coupled dynamics of the aerial vehicle and payload, and integrates a
linear Kalman filter for state estimation, a model predictive contouring
control planner, and an incremental model predictive controller. The control
architecture is designed to remain effective despite sensing limitations and
estimation uncertainty. Extensive simulations demonstrate that the proposed
system achieves performance comparable to control based on ground-truth
measurements, with only minor degradation (< 6%). The system also shows strong
robustness to variations in payload parameters. Field experiments further
validate the framework, confirming its practical applicability and reliable
performance in outdoor environments using only off-the-shelf aerial vehicle
hardware.

</details>


### [166] [Nominal Evaluation Of Automatic Multi-Sections Control Potential In Comparison To A Simpler One- Or Two-Sections Alternative With Predictive Spray Switching](https://arxiv.org/abs/2508.11573)
*Mogens Plessen*

Main category: cs.RO

TL;DR: 本文探讨了农业喷雾中自动分段控制（ASC）与更简单的预测喷雾切换方法的比较，提出在无需昂贵传感器的情况下，如何有效降低喷雾重叠并简化实施。


<details>
  <summary>Details</summary>
Motivation: 现代农业喷雾追求高精度，以减少喷洒重叠区域，节省农药与成本。现有的ASC方法依赖于复杂传感器系统与实时数据，但应用条件多变、成本高，因此作者希望找到不依赖复杂传感器、实现低成本的简易替代方法。

Method: 作者设计了基于预测喷雾切换的简化控制方法，并与传统48分段的ASC及2分段、一体化控制三种方案进行对比。还结合两种区域覆盖路径规划与切换逻辑，在10个真实农田案例（包括不规则地块与障碍物）中进行实验评估。

Result: 实验证明，预测切换方案在不使用昂贵传感器的情况下，可以有效减少路径长度，控制覆盖重叠并适用于人工驾驶。尤其在2分段的简化方案下覆盖效果和作业效率有良好平衡。

Conclusion: 文中推荐了无需传感器、成本低廉的简化喷雾切换方法，适合需要路径规划、但不易部署精确传感器的应用场景，有助于农业喷雾自动化的普及和成本控制。

Abstract: Automatic Section Control (ASC) is a long-standing trend for spraying in
agriculture. It promises to minimise spray overlap areas. The core idea is to
(i) switch off spray nozzles on areas that have already been sprayed, and (ii)
to dynamically adjust nozzle flow rates along the boom bar that holds the spray
nozzles when velocities of boom sections vary during turn maneuvers. ASC is not
possible without sensors, in particular for accurate positioning data. Spraying
and the movement of modern wide boom bars are highly dynamic processes. In
addition, many uncertainty factors have an effect such as cross wind drift,
boom height, nozzle clogging in open-field conditions, and so forth. In view of
this complexity, the natural question arises if a simpler alternative exist.
Therefore, an Automatic Multi-Sections Control method is compared to a proposed
simpler one- or two-sections alternative that uses predictive spray switching.
The comparison is provided under nominal conditions. Agricultural spraying is
intrinsically linked to area coverage path planning and spray switching logic.
Combinations of two area coverage path planning and switching logics as well as
three sections-setups are compared. The three sections-setups differ by
controlling 48 sections, 2 sections or controlling all nozzles uniformly with
the same control signal as one single section. Methods are evaluated on 10
diverse real-world field examples, including non-convex field contours,
freeform mainfield lanes and multiple obstacle areas. A preferred method is
suggested that (i) minimises area coverage pathlength, (ii) offers intermediate
overlap, (iii) is suitable for manual driving by following a pre-planned
predictive spray switching logic for an area coverage path plan, and (iv) and
in contrast to ASC can be implemented sensor-free and therefore at low cost.

</details>


### [167] [Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks](https://arxiv.org/abs/2508.11584)
*Jakub Łucki,Jonathan Becktor,Georgios Georgakis,Robert Royce,Shehryar Khattak*

Main category: cs.RO

TL;DR: 本文提出了VPEngine，一种用于机器人平台视觉多任务的高效模块化框架，实现了GPU高效利用和任务并行，显著减少了内存占用和重复计算，提升了推理速度。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的机器人平台上，为不同感知任务部署多个机器学习模型通常会导致计算冗余、内存开销大以及集成复杂。现有做法未能充分利用硬件资源，且难以扩展和维护。本文旨在解决这些问题。

Method: 提出了VPEngine框架，采用共享的基础视觉模型骨干(如DINOv2)进行图像特征提取，并将特征高效共享给多个并行运行的专用任务头（如深度估计、目标检测、语义分割），避免GPU-CPU重复内存传输。基于CUDA MPS实现高效的GPU并发和恒定内存占用，并可在运行时动态调整每个任务的推理频率。

Result: 以DINOv2作为基础模型实现了多个视觉任务头（深度、目标检测、语义分割），在NVIDIA Jetson Orin AGX平台上对TensorRT优化模型的端到端性能测试中，VPEngine实现了≥50Hz的实时推理，且速度最高提升达3倍，GPU利用率显著提高。

Conclusion: VPEngine作为开放源代码框架，极大提升了机器人平台视觉多任务的集成效率与推理速度，同时保持了良好的可扩展性和开发便利性，有助于机器人视觉感知系统的部署与推广。

Abstract: Deploying multiple machine learning models on resource-constrained robotic
platforms for different perception tasks often results in redundant
computations, large memory footprints, and complex integration challenges. In
response, this work presents Visual Perception Engine (VPEngine), a modular
framework designed to enable efficient GPU usage for visual multitasking while
maintaining extensibility and developer accessibility. Our framework
architecture leverages a shared foundation model backbone that extracts image
representations, which are efficiently shared, without any unnecessary GPU-CPU
memory transfers, across multiple specialized task-specific model heads running
in parallel. This design eliminates the computational redundancy inherent in
feature extraction component when deploying traditional sequential models while
enabling dynamic task prioritization based on application demands. We
demonstrate our framework's capabilities through an example implementation
using DINOv2 as the foundation model with multiple task (depth, object
detection and semantic segmentation) heads, achieving up to 3x speedup compared
to sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngine
offers efficient GPU utilization and maintains a constant memory footprint
while allowing per-task inference frequencies to be adjusted dynamically during
runtime. The framework is written in Python and is open source with ROS2 C++
(Humble) bindings for ease of use by the robotics community across diverse
robotic platforms. Our example implementation demonstrates end-to-end real-time
performance at $\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimized
models.

</details>


### [168] [Investigating Sensors and Methods in Grasp State Classification in Agricultural Manipulation](https://arxiv.org/abs/2508.11588)
*Benjamin Walt,Jordan Westphal,Girish Krishnan*

Main category: cs.RO

TL;DR: 该论文提出集成多种传感器（IMU、红外反射、拉力、触觉、RGB相机）于可柔性夹持器，用以高效准确判断抓取状态，并比较不同传感器与分类模型在果蔬采摘中的表现。


<details>
  <summary>Details</summary>
Motivation: 农业采摘环境复杂、遮挡多且果实与植株物理连接，造成对夹持状态识别的挑战，亟需精准、实时反馈以提升采摘效率和准确性。

Method: 在柔性夹爪上集成IMU、红外、拉力、触觉传感器及摄像头，实验对比随机森林与LSTM两类主流分类模型，分别评估每种传感器及其组合对抓取状态（滑脱、失败、成功）的识别贡献与性能。

Result: 在实验室训练、实际番茄植株测试中，随机森林模型实现了100%抓取状态识别准确率，显著优于基线，且发现仅用IMU与拉力传感器组合即可实现有效分类。

Conclusion: 集成选择合适的传感器组合，采用高效分类模型（如随机森林），可极大提升果蔬采摘中夹持状态感知的可靠性和效率，为后续自动纠错与高效采摘提供关键技术基础。

Abstract: Effective and efficient agricultural manipulation and harvesting depend on
accurately understanding the current state of the grasp. The agricultural
environment presents unique challenges due to its complexity, clutter, and
occlusion. Additionally, fruit is physically attached to the plant, requiring
precise separation during harvesting. Selecting appropriate sensors and
modeling techniques is critical for obtaining reliable feedback and correctly
identifying grasp states. This work investigates a set of key sensors, namely
inertial measurement units (IMUs), infrared (IR) reflectance, tension, tactile
sensors, and RGB cameras, integrated into a compliant gripper to classify grasp
states. We evaluate the individual contribution of each sensor and compare the
performance of two widely used classification models: Random Forest and Long
Short-Term Memory (LSTM) networks. Our results demonstrate that a Random Forest
classifier, trained in a controlled lab environment and tested on real cherry
tomato plants, achieved 100% accuracy in identifying slip, grasp failure, and
successful picks, marking a substantial improvement over baseline performance.
Furthermore, we identify a minimal viable sensor combination, namely IMU and
tension sensors that effectively classifies grasp states. This classifier
enables the planning of corrective actions based on real-time feedback, thereby
enhancing the efficiency and reliability of fruit harvesting operations.

</details>
