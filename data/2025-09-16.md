<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 144]
- [cs.CL](#cs.CL) [Total: 86]
- [cs.RO](#cs.RO) [Total: 54]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A Real-Time Diminished Reality Approach to Privacy in MR Collaboration](https://arxiv.org/abs/2509.10466)
*Christian Fane*

Main category: cs.CV

TL;DR: 本文提出了一个基于图像修复的实时消隐现实（Diminished Reality，DR）系统，用于在混合现实（MR）会议中实现隐私控制，允许用户实时移除环境中的敏感物品，对其他参与者不可见。


<details>
  <summary>Details</summary>
Motivation: 在MR会议场景中，用户常需隐藏个人或敏感物品以保护隐私，但现有方法通常受限于硬件（如固定摄像头或需预先扫描环境），缺少灵活且实时的解决方案。

Method: 系统采用YOLOv11进行目标检测，通过语义分割对目标进行精准选择，然后基于移动ZED 2i深度摄像头从第二观察者视角实施实时视频修复（inpainting），用改良的解耦时空Transformer（DSTT）模型提升修复质量，无需固定观察点或环境预扫描。

Result: 在720p分辨率下，整个流程可实现20帧以上的实时处理，能够稳定地应用于实际MR会议场景下的隐私保护。

Conclusion: 所提出的消隐现实系统证明了在无需固定硬件或环境预处理条件下，能够高效地在MR中实现灵活、实时的隐私保护，具有实际应用前景。

Abstract: Diminished reality (DR) refers to the digital removal of real-world objects
by compositing background content in their place. This thesis presents a
real-time, inpainting-based DR system designed to enable privacy control in
shared-space mixed reality (MR) meetings. The system allows a primary headset
user to selectively remove personal or sensitive items from their environment,
ensuring that those objects are no longer visible to other participants.
Removal is achieved through semantic segmentation and precise object selection,
followed by real-time inpainting from the viewpoint of a secondary observer,
implemented using a mobile ZED 2i depth camera. The solution is designed to be
portable and robust, requiring neither a fixed secondary viewpoint nor prior 3D
scanning of the environment. The system utilises YOLOv11 for object detection
and a modified Decoupled Spatial-Temporal Transformer (DSTT) model for
high-quality video inpainting. At 720p resolution, the pipeline sustains frame
rates exceeding 20 fps, demonstrating the feasibility of real-time diminished
reality for practical privacy-preserving MR applications.

</details>


### [2] [SurgLaVi: Large-Scale Hierarchical Dataset for Surgical Vision-Language Representation Learning](https://arxiv.org/abs/2509.10555)
*Alejandra Perez,Chinedu Nwoye,Ramtin Raji Kermani,Omid Mohareri,Muhammad Abdullah Jamal*

Main category: cs.CV

TL;DR: 本文提出了SurgLaVi，这是目前最大且多样性最高的手术视觉-语言数据集，并以此为基础开发了SurgCLIP模型，在手术流程识别等任务上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前手术视觉-语言预训练（VLP）受限于数据集规模小、程序多样性差、语义质量有限及缺乏层次结构，严重影响了模型的泛化能力和应用推广。

Method: 作者构建了SurgLaVi数据集，包含约24万对视频片段和描述，涵盖200多种手术类型，细粒度标注至阶段、步骤和任务三级。该数据集通过全自动流程生成高质量语义标注，并采用双模态过滤剔除无关或噪声样本。同时，开放发布了基于公共数据的子集SurgLaVi-β。基于SurgLaVi，提出了SurgCLIP，一个类CLIP的双编码器视频-文本对比学习框架。

Result: SurgCLIP基于SurgLaVi数据集，在手术阶段、步骤、动作和工具识别等任务上取得了优于以往方法的表现，且提升幅度明显，证明了大规模语义丰富数据集对于提高手术领域基础模型的有效性和泛化能力。

Conclusion: SurgLaVi数据集和SurgCLIP模型为手术视觉-语言研究提供了坚实的数据基础和有效方法，有望推动领域内基础模型的进步并推广到更多手术任务场景。

Abstract: Vision-language pre-training (VLP) offers unique advantages for surgery by
aligning language with surgical videos, enabling workflow understanding and
transfer across tasks without relying on expert-labeled datasets. However,
progress in surgical VLP remains constrained by the limited scale, procedural
diversity, semantic quality, and hierarchical structure of existing datasets.
In this work, we present SurgLaVi, the largest and most diverse surgical
vision-language dataset to date, comprising nearly 240k clip-caption pairs from
more than 200 procedures, and comprising hierarchical levels at phase-, step-,
and task-level. At the core of SurgLaVi lies a fully automated pipeline that
systematically generates fine-grained transcriptions of surgical videos and
segments them into coherent procedural units. To ensure high-quality
annotations, it applies dual-modality filtering to remove irrelevant and noisy
samples. Within this framework, the resulting captions are enriched with
contextual detail, producing annotations that are both semantically rich and
easy to interpret. To ensure accessibility, we release SurgLaVi-\b{eta}, an
open-source derivative of 113k clip-caption pairs constructed entirely from
public data, which is over four times larger than existing surgical VLP
datasets. To demonstrate the value of SurgLaVi datasets, we introduce SurgCLIP,
a CLIP-style video-text contrastive framework with dual encoders, as a
representative base model. SurgCLIP achieves consistent improvements across
phase, step, action, and tool recognition, surpassing prior state-of-the-art
methods, often by large margins. These results validate that large-scale,
semantically rich, and hierarchically structured datasets directly translate
into stronger and more generalizable representations, establishing SurgLaVi as
a key resource for developing surgical foundation models.

</details>


### [3] [Building a General SimCLR Self-Supervised Foundation Model Across Neurological Diseases to Advance 3D Brain MRI Diagnoses](https://arxiv.org/abs/2509.10620)
*Emily Kaczmarek,Justin Szeto,Brennan Nichyporuk,Tal Arbel*

Main category: cs.CV

TL;DR: 本文提出了一个基于SimCLR自监督学习的三维脑结构MRI基础模型，使用大量公共数据集（18,759名患者、44,958份扫描）进行预训练，能够在多任务和多疾病类型下实现高效迁移，在多个下游任务中超越MAE和有监督基线，相关代码和模型已开源。


<details>
  <summary>Details</summary>
Motivation: 现有大多数三维脑MRI深度学习模型针对特定任务定制，受限于标注数据少、泛化能力弱。二维医学影像已受益于自监督式大模型，但三维脑MRI基础模型仍处于分辨率有限、泛化性差、获取难等初级阶段，亟需更强大、开放的通用高分辨率基础模型。

Method: 作者基于SimCLR自监督预训练方法，对三维脑结构MRI进行无标注表征学习，借助来自11个公开数据集的18,759名患者、44,958份扫描数据集，训练高分辨率的MRI基础模型，并与MAE模型及两种有监督基线在四个不同的下游预测任务（包括分布内和分布外）进行了性能对比与评估。

Result: 在所有下游任务中（如阿尔茨海默病预测等），SimCLR自监督模型均优于MAE和有监督基线。即使只用20%的标注样本微调，模型依然展现出较强的性能。

Conclusion: 提出的高分辨率SimCLR基础模型具有卓越的泛化能力和迁移能力，适用于多种临床脑MRI任务；模型与代码已开源，为医学影像领域提供了具有广泛适用性和可获取性的强力基础工具。

Abstract: 3D structural Magnetic Resonance Imaging (MRI) brain scans are commonly
acquired in clinical settings to monitor a wide range of neurological
conditions, including neurodegenerative disorders and stroke. While deep
learning models have shown promising results analyzing 3D MRI across a number
of brain imaging tasks, most are highly tailored for specific tasks with
limited labeled data, and are not able to generalize across tasks and/or
populations. The development of self-supervised learning (SSL) has enabled the
creation of large medical foundation models that leverage diverse, unlabeled
datasets ranging from healthy to diseased data, showing significant success in
2D medical imaging applications. However, even the very few foundation models
for 3D brain MRI that have been developed remain limited in resolution, scope,
or accessibility. In this work, we present a general, high-resolution
SimCLR-based SSL foundation model for 3D brain structural MRI, pre-trained on
18,759 patients (44,958 scans) from 11 publicly available datasets spanning
diverse neurological diseases. We compare our model to Masked Autoencoders
(MAE), as well as two supervised baselines, on four diverse downstream
prediction tasks in both in-distribution and out-of-distribution settings. Our
fine-tuned SimCLR model outperforms all other models across all tasks. Notably,
our model still achieves superior performance when fine-tuned using only 20% of
labeled training samples for predicting Alzheimer's disease. We use publicly
available code and data, and release our trained model at
https://github.com/emilykaczmarek/3D-Neuro-SimCLR, contributing a broadly
applicable and accessible foundation model for clinical brain MRI analysis.

</details>


### [4] [USCTNet: A deep unfolding nuclear-norm optimization solver for physically consistent HSI reconstruction](https://arxiv.org/abs/2509.10651)
*Xiaoyang Ma,Yiyang Chai,Xinran Qu,Hong Sun*

Main category: cs.CV

TL;DR: 本文提出了一种基于物理模型的深度展开网络USCTNet，可利用单帧RGB图像重建高光谱图像，并保证颜色一致性，实验显示重建效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单张RGB图像重建高光谱图像在实际应用中需求量大，但该问题极其病态且易受相机光谱敏感度和场景光照误差影响，传统方法经常出现物理不一致和重建误差。

Method: 将RGB到HSI重建建模为带有可学习变换域核范数正则项的物理反问题，并在每步迭代中显式估计CSS（相机光谱敏感度）和照明，嵌入正向算子以确保颜色一致性。同时引入数据自适应低秩子空间SVD阈值操作，提出USCTNet深度展开架构，将参数估计与可学习近端更新结合。

Result: 在标准基准数据集上进行大量实验，结果表明USCTNet在重建精度方面相较于现有最先进的RGB方法有持续提升。

Conclusion: USCTNet有效结合物理建模和深度学习策略，实现了单张RGB图像到高光谱图像的高效高质量重建，其方法具备更强的泛化能力和物理一致性，具有推广和应用价值。

Abstract: Reconstructing hyperspectral images (HSIs) from a single RGB image is
ill-posed and can become physically inconsistent when the camera spectral
sensitivity (CSS) and scene illumination are misspecified. We formulate
RGB-to-HSI reconstruction as a physics-grounded inverse problem regularized by
a nuclear norm in a learnable transform domain, and we explicitly estimate CSS
and illumination to define the forward operator embedded in each iteration,
ensuring colorimetric consistency. To avoid the cost and instability of full
singular-value decompositions (SVDs) required by singular-value thresholding
(SVT), we introduce a data-adaptive low-rank subspace SVT operator. Building on
these components, we develop USCTNet, a deep unfolding solver tailored to HSI
that couples a parameter estimation module with learnable proximal updates.
Extensive experiments on standard benchmarks show consistent improvements over
state-of-the-art RGB-based methods in reconstruction accuracy. Code:
https://github.com/psykheXX/USCTNet-Code-Implementation.git

</details>


### [5] [A Comparison and Evaluation of Fine-tuned Convolutional Neural Networks to Large Language Models for Image Classification and Segmentation of Brain Tumors on MRI](https://arxiv.org/abs/2509.10683)
*Felicia Liu,Jay J. Yoo,Farzad Khalvati*

Main category: cs.CV

TL;DR: 该论文评估了大语言模型（LLMs）在医学影像任务中的表现，并与传统卷积神经网络（CNNs）进行比较。结果显示，CNN明显优于LLMs，后者目前不适合医学影像分析。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在文本类医疗任务表现优异，但其在图像类医学应用的能力尚不明确。研究旨在评估LLMs在医学影像领域中的潜力，特别是在脑部肿瘤分类和分割任务中的表现。

Method: 使用包含多模态脑MRI的BraTS 2020数据集，分别以未微调及微调（fine-tune）后的通用视觉-语言大模型（LLaMA 3.2 Instruct）参与实验，用于肿瘤分类（低/高级别胶质瘤）和分割（中心点、包围框、多边形提取）。对比模型为自定义3D CNNs，并评估分类准确率、精确率、召回率、特异性及分割能力。

Result: 在肿瘤分类中，CNN准确率为80%，LLM最高为76%；未微调LLM特异性仅为18%，微调后提升至55%，但总体准确率下降。分割任务中，CNN能够准确定位肿瘤，LLM输出多集中于图像中心，无法区分肿瘤大小和位置，微调对空间精度提升有限，甚至出现随机无结构的结果。

Conclusion: CNN在医学影像分类和分割任务上均明显优于当前LLMs。LLMs空间理解有限，单靠微调难以取得本质提升，短期内仍不适用于影像任务。建议未来可尝试更严格微调或其他训练策略提升其图像任务能力。

Abstract: Large Language Models (LLMs) have shown strong performance in text-based
healthcare tasks. However, their utility in image-based applications remains
unexplored. We investigate the effectiveness of LLMs for medical imaging tasks,
specifically glioma classification and segmentation, and compare their
performance to that of traditional convolutional neural networks (CNNs). Using
the BraTS 2020 dataset of multi-modal brain MRIs, we evaluated a
general-purpose vision-language LLM (LLaMA 3.2 Instruct) both before and after
fine-tuning, and benchmarked its performance against custom 3D CNNs. For glioma
classification (Low-Grade vs. High-Grade), the CNN achieved 80% accuracy and
balanced precision and recall. The general LLM reached 76% accuracy but
suffered from a specificity of only 18%, often misclassifying Low-Grade tumors.
Fine-tuning improved specificity to 55%, but overall performance declined
(e.g., accuracy dropped to 72%). For segmentation, three methods - center
point, bounding box, and polygon extraction, were implemented. CNNs accurately
localized gliomas, though small tumors were sometimes missed. In contrast, LLMs
consistently clustered predictions near the image center, with no distinction
of glioma size, location, or placement. Fine-tuning improved output formatting
but failed to meaningfully enhance spatial accuracy. The bounding polygon
method yielded random, unstructured outputs. Overall, CNNs outperformed LLMs in
both tasks. LLMs showed limited spatial understanding and minimal improvement
from fine-tuning, indicating that, in their current form, they are not
well-suited for image-based tasks. More rigorous fine-tuning or alternative
training strategies may be needed for LLMs to achieve better performance,
robustness, and utility in the medical space.

</details>


### [6] [Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation](https://arxiv.org/abs/2509.10687)
*Hao Zhang,Chun-Han Yao,Simon Donné,Narendra Ahuja,Varun Jampani*

Main category: cs.CV

TL;DR: SP4D是一种能从单目输入生成配对的RGB和运动部位视频的新框架，核心在于基于运动结构而非传统语义特征分割物体部位，并具有很强的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前传统物体分割方法多依赖外观语义特征，难以对物体关节及结构运动变化进行稳健分割，尤其在处理动画和运动分析等需要运动一致性的场景时存在不足。因此，研究更贴近物体运动学结构、且能处理视角与时间一致性的分割方法具有重要意义。

Method: 作者提出了Stable Part Diffusion 4D (SP4D) 框架，采用双分支扩散模型，联合生成RGB帧和配套的运动部位分割图。同时引入空间颜色编码机制，将分割掩码映射为RGB式连续图像，使分割分支能共享RGB分支的潜在VAE。为增强分支一致性，提出Bidirectional Diffusion Fusion模块及对比一致性损失，维持分割时空对齐。此外，2D分割结果可还原为3D骨架和蒙皮权重。

Result: SP4D在新建的KinematicParts20K数据集上经验证能处理多样场景，结果在真实视频、新颖物体、稀有关节动作等情况下均能生成具有运动结构感知能力的分割输出，且适用下游动画及动作任务。

Conclusion: SP4D突破了基于静态语义分割的局限，实现了运动结构感知的RGB与分割视频联动生成，具备灵活性和泛化能力，同时为三维骨架提取等后续任务提供了实用输入。

Abstract: We present Stable Part Diffusion 4D (SP4D), a framework for generating paired
RGB and kinematic part videos from monocular inputs. Unlike conventional part
segmentation methods that rely on appearance-based semantic cues, SP4D learns
to produce kinematic parts - structural components aligned with object
articulation and consistent across views and time. SP4D adopts a dual-branch
diffusion model that jointly synthesizes RGB frames and corresponding part
segmentation maps. To simplify the architecture and flexibly enable different
part counts, we introduce a spatial color encoding scheme that maps part masks
to continuous RGB-like images. This encoding allows the segmentation branch to
share the latent VAE from the RGB branch, while enabling part segmentation to
be recovered via straightforward post-processing. A Bidirectional Diffusion
Fusion (BiDiFuse) module enhances cross-branch consistency, supported by a
contrastive part consistency loss to promote spatial and temporal alignment of
part predictions. We demonstrate that the generated 2D part maps can be lifted
to 3D to derive skeletal structures and harmonic skinning weights with few
manual adjustments. To train and evaluate SP4D, we construct KinematicParts20K,
a curated dataset of over 20K rigged objects selected and processed from
Objaverse XL (Deitke et al., 2023), each paired with multi-view RGB and part
video sequences. Experiments show that SP4D generalizes strongly to diverse
scenarios, including real-world videos, novel generated objects, and rare
articulated poses, producing kinematic-aware outputs suitable for downstream
animation and motion-related tasks.

</details>


### [7] [SegSLR: Promptable Video Segmentation for Isolated Sign Language Recognition](https://arxiv.org/abs/2509.10710)
*Sven Schreiber,Noha Sarhan,Simone Frintrop,Christian Wilms*

Main category: cs.CV

TL;DR: 本文提出了SegSLR系统，一个结合RGB和姿态信息的隔离手语识别方法，通过可提示的零样本视频分割，精确提取手和身体区域，在公开数据集上超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有手语识别方法依赖RGB或姿态数据，但简单结合时常因表示不精确（例如边界框）导致手型或朝向信息丢失。急需提升多模态结合准确性，保留细节信息。

Method: 提出SegSLR系统，利用姿态信息提供的粗略定位，通过零样本视频分割技术准确定义手和身体区域，从而将RGB处理关注于关键区域，巧妙融合两类信息。

Result: 在ChaLearn249 IsoGD复杂数据集上，SegSLR的识别效果优于现有方法。消融实验显示专注于手和身体区域大幅提升识别性能。

Conclusion: 通过细粒度分割精确结合姿态和RGB信息，SegSLR有效提升了隔离手语识别的准确性，为多模态手语理解提供了更优基础。

Abstract: Isolated Sign Language Recognition (ISLR) approaches primarily rely on RGB
data or signer pose information. However, combining these modalities often
results in the loss of crucial details, such as hand shape and orientation, due
to imprecise representations like bounding boxes. Therefore, we propose the
ISLR system SegSLR, which combines RGB and pose information through promptable
zero-shot video segmentation. Given the rough localization of the hands and the
signer's body from pose information, we segment the respective parts through
the video to maintain all relevant shape information. Subsequently, the
segmentations focus the processing of the RGB data on the most relevant body
parts for ISLR. This effectively combines RGB and pose information. Our
evaluation on the complex ChaLearn249 IsoGD dataset shows that SegSLR
outperforms state-of-the-art methods. Furthermore, ablation studies indicate
that SegSLR strongly benefits from focusing on the signer's body and hands,
justifying our design choices.

</details>


### [8] [SCOPE: Speech-guided COllaborative PErception Framework for Surgical Scene Segmentation](https://arxiv.org/abs/2509.10748)
*Jecia Z. Y. Mao,Francis X Creighton,Russell H Taylor,Manish Sahu*

Main category: cs.CV

TL;DR: 本文提出了一种由语音引导的大模型协作感知（SCOPE）框架，可在手术实时视频中实现外科器械和解剖结构的动态分割、追踪与标注，仅需医生语音交互引导，提高了模型通用性与实用性。


<details>
  <summary>Details</summary>
Motivation: 当前手术场景中关键元素的分割与追踪依赖特定领域、监督式模型，需要大量标注数据，难以适应新的手术任务和标签类别，且部署受限。随着基础视觉模型崛起，虽然带来零样本、多领域分割能力，但对手工输入的依赖限制了其实用。该工作旨在突破这些瓶颈，开发更加智能和人性化的手术辅助工具。

Method: 作者设计了SCOPE框架，将大语言模型（LLM）的推理与视觉基础模型（VFM）的感知能力结合，实现手术场景中基于语音引导的动态分割与标签标注。医生通过语音与系统交互，系统在视觉模型给出的分割结果中选优，并根据语音反馈优化分割。同时，已识别器械作为‘指针’可进一步标注场景中其他元素。

Result: 在Cataract1k公开数据集子集和自有的颅底手术外植体数据集上实验，SCOPE框架展现了实时分割与追踪能力。通过模拟实验，验证了该系统动态交互与适应性的潜力。

Conclusion: 该框架实现了基于人机协作、语音无接触交互的手术场景分割与追踪，展示了开发适应动态手术室环境、以外科医生为中心的新一代手术辅助工具的潜力。

Abstract: Accurate segmentation and tracking of relevant elements of the surgical scene
is crucial to enable context-aware intraoperative assistance and decision
making. Current solutions remain tethered to domain-specific, supervised models
that rely on labeled data and required domain-specific data to adapt to new
surgical scenarios and beyond predefined label categories. Recent advances in
prompt-driven vision foundation models (VFM) have enabled open-set, zero-shot
segmentation across heterogeneous medical images. However, dependence of these
models on manual visual or textual cues restricts their deployment in
introperative surgical settings. We introduce a speech-guided collaborative
perception (SCOPE) framework that integrates reasoning capabilities of large
language model (LLM) with perception capabilities of open-set VFMs to support
on-the-fly segmentation, labeling and tracking of surgical instruments and
anatomy in intraoperative video streams. A key component of this framework is a
collaborative perception agent, which generates top candidates of VFM-generated
segmentation and incorporates intuitive speech feedback from clinicians to
guide the segmentation of surgical instruments in a natural human-machine
collaboration paradigm. Afterwards, instruments themselves serve as interactive
pointers to label additional elements of the surgical scene. We evaluated our
proposed framework on a subset of publicly available Cataract1k dataset and an
in-house ex-vivo skull-base dataset to demonstrate its potential to generate
on-the-fly segmentation and tracking of surgical scene. Furthermore, we
demonstrate its dynamic capabilities through a live mock ex-vivo experiment.
This human-AI collaboration paradigm showcase the potential of developing
adaptable, hands-free, surgeon-centric tools for dynamic operating-room
environments.

</details>


### [9] [Every Camera Effect, Every Time, All at Once: 4D Gaussian Ray Tracing for Physics-based Camera Effect Data Generation](https://arxiv.org/abs/2509.10759)
*Yi-Ruei Liu,You-Zhe Xie,Yu-Hsiang Hsu,I-Sheng Fang,Yu-Lun Liu,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种名为4D Gaussian Ray Tracing (4D-GRT)的新方法，用于高效、真实地模拟相机失真效果，补足现有训练数据无法覆盖真实相机效应的问题。该方法结合4D高斯溅射和基于物理的光线追踪技术，实现了生成带有可控物理相机失真的动态视频。其渲染速度快，质量优于现有方法，并建立了新的基准数据集。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉系统假设理想针孔相机，难以应对鱼眼畸变、滚动快门等真实相机效应，因为缺乏相关数据训练。已有数据生成方法要么成本高，要么仿真与现实差距大，或无法准确模拟相机效果。本文旨在突破这些瓶颈。

Method: 提出4D-GRT两阶段流程：1）基于多视角视频重建动态场景；2）结合4D Gaussian Splatting和基于物理的光线追踪，生成带有真实可控相机效果的视频。并制作了包含四种相机效应的八组合成动态场景用作基准。

Result: 4D-GRT在渲染速度上显著快于现有方法，且在渲染质量上优于或相当于主流方法。

Conclusion: 4D-GRT为带物理相机效应数据的生成提供了一种高效且高质量的解决方案，能更好地支持真实感计算机视觉任务，并建立了新的评测基准。

Abstract: Common computer vision systems typically assume ideal pinhole cameras but
fail when facing real-world camera effects such as fisheye distortion and
rolling shutter, mainly due to the lack of learning from training data with
camera effects. Existing data generation approaches suffer from either high
costs, sim-to-real gaps or fail to accurately model camera effects. To address
this bottleneck, we propose 4D Gaussian Ray Tracing (4D-GRT), a novel two-stage
pipeline that combines 4D Gaussian Splatting with physically-based ray tracing
for camera effect simulation. Given multi-view videos, 4D-GRT first
reconstructs dynamic scenes, then applies ray tracing to generate videos with
controllable, physically accurate camera effects. 4D-GRT achieves the fastest
rendering speed while performing better or comparable rendering quality
compared to existing baselines. Additionally, we construct eight synthetic
dynamic scenes in indoor environments across four camera effects as a benchmark
to evaluate generated videos with camera effects.

</details>


### [10] [EditDuet: A Multi-Agent System for Video Non-Linear Editing](https://arxiv.org/abs/2509.10761)
*Marcelo Sandoval-Castaneda,Bryan Russell,Josef Sivic,Gregory Shakhnarovich,Fabian Caba Heilbron*

Main category: cs.CV

TL;DR: 本文提出了一种多智能体自动视频剪辑系统，由编辑者（Editor）和评论者（Critic）协作，根据自然语言指令实现高质量视频自动编辑。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法主要集中于检索或用户界面，实际编辑工作仍需人工完成，缺乏能自动理解和执行自然语言编辑指令的解决方案。

Method: 提出多智能体（Editor与Critic）协同方式，编辑者根据视频片段和自然语言指令自动剪辑，评论者对剪辑结果进行点评和反馈，并采用基于学习的机制增强多智能体间的通信能力。此外，设计了基于大语言模型（LLM）的质量评测指标并与人工偏好对比。

Result: 系统在覆盖面、时间约束满足和人类偏好等指标上，通过定性与定量用户研究，均远超现有方法。

Conclusion: 该方法在自然语言驱动自动视频编辑任务上表现突出，显著优于当前方案，为自动化视频编辑开辟新方向。

Abstract: Automated tools for video editing and assembly have applications ranging from
filmmaking and advertisement to content creation for social media. Previous
video editing work has mainly focused on either retrieval or user interfaces,
leaving actual editing to the user. In contrast, we propose to automate the
core task of video editing, formulating it as sequential decision making
process. Ours is a multi-agent approach. We design an Editor agent and a Critic
agent. The Editor takes as input a collection of video clips together with
natural language instructions and uses tools commonly found in video editing
software to produce an edited sequence. On the other hand, the Critic gives
natural language feedback to the editor based on the produced sequence or
renders it if it is satisfactory. We introduce a learning-based approach for
enabling effective communication across specialized agents to address the
language-driven video editing task. Finally, we explore an LLM-as-a-judge
metric for evaluating the quality of video editing system and compare it with
general human preference. We evaluate our system's output video sequences
qualitatively and quantitatively through a user study and find that our system
vastly outperforms existing approaches in terms of coverage, time constraint
satisfaction, and human preference.

</details>


### [11] [Enhancement Without Contrast: Stability-Aware Multicenter Machine Learning for Glioma MRI Imaging](https://arxiv.org/abs/2509.10767)
*Sajad Amiri,Shahram Taeb,Sara Gharibi,Setareh Dehghanfard,Somayeh Sadat Mehrnia,Mehrdad Oveisi,Ilker Hacihaliloglu,Arman Rahmim,Mohammad R. Salmanpour*

Main category: cs.CV

TL;DR: 本文提出了一种基于稳定性筛选的机器学习框架，可以用无增强MRI预测胶质瘤增强影像表现，减少对钆基造影剂的依赖，在多中心数据中表现出高准确性和良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 胶质瘤的MRI增强成像依赖于钆基造影剂，存在安全、成本和可及性问题。如何用无增强MRI预测肿瘤的增强表现，有助于更安全、广泛和低成本地评估肿瘤活动性并指导治疗。以往方法受扫描仪和患者队列差异影响，鲁棒性不足，因此需开发更稳定泛化的预测模型。

Method: 作者收集了四个大规模多中心公开胶质瘤MRI数据库，共1446例，采用PyRadiomics提取IBSI标准下的108个影像特征，并结合48种降维方法和25种分类器，形成1200条机器学习管线。通过轮转验证法，即每次用三个数据集训练、一个测试，评估模型的稳定性和泛化能力，筛选最稳定且有效的方案。

Result: 交叉验证预测准确率达0.91-0.96，多中心外部验证准确率为0.87-0.98，平均0.93。F1、精确率、召回率均表现稳定（0.87-0.96），但ROC-AUC波动（0.50-0.82），反映队列差异。MI linked with ETr模型在准确性和稳定性上综合最佳。

Conclusion: 本研究证明，基于稳定性筛选的机器学习可实现跨中心、可复现的无增强MRI预测胶质瘤增强表现，降低对钆剂依赖，提高泛化力，为神经肿瘤学等领域提供了一种可扩展的机器学习管线筛选模板。

Abstract: Gadolinium-based contrast agents (GBCAs) are central to glioma imaging but
raise safety, cost, and accessibility concerns. Predicting contrast enhancement
from non-contrast MRI using machine learning (ML) offers a safer alternative,
as enhancement reflects tumor aggressiveness and informs treatment planning.
Yet scanner and cohort variability hinder robust model selection. We propose a
stability-aware framework to identify reproducible ML pipelines for multicenter
prediction of glioma MRI contrast enhancement. We analyzed 1,446 glioma cases
from four TCIA datasets (UCSF-PDGM, UPENN-GB, BRATS-Africa, BRATS-TCGA-LGG).
Non-contrast T1WI served as input, with enhancement derived from paired
post-contrast T1WI. Using PyRadiomics under IBSI standards, 108 features were
extracted and combined with 48 dimensionality reduction methods and 25
classifiers, yielding 1,200 pipelines. Rotational validation was trained on
three datasets and tested on the fourth. Cross-validation prediction accuracies
ranged from 0.91 to 0.96, with external testing achieving 0.87 (UCSF-PDGM),
0.98 (UPENN-GB), and 0.95 (BRATS-Africa), with an average of 0.93. F1,
precision, and recall were stable (0.87 to 0.96), while ROC-AUC varied more
widely (0.50 to 0.82), reflecting cohort heterogeneity. The MI linked with ETr
pipeline consistently ranked highest, balancing accuracy and stability. This
framework demonstrates that stability-aware model selection enables reliable
prediction of contrast enhancement from non-contrast glioma MRI, reducing
reliance on GBCAs and improving generalizability across centers. It provides a
scalable template for reproducible ML in neuro-oncology and beyond.

</details>


### [12] [Group Evidence Matters: Tiling-based Semantic Gating for Dense Object Detection](https://arxiv.org/abs/2509.10779)
*Yilun Xiao*

Main category: cs.CV

TL;DR: 本文提出了一种适用于无人机影像中密集小目标检测的、与检测器无关的后处理框架，可有效提升漏检目标的召回率，具有较好的移植性和实用性。


<details>
  <summary>Details</summary>
Motivation: 在无人机遥感图像中，由于拍摄视角远、遮挡多、背景杂乱，密集小目标容易被漏检，如何提升这类目标的检测召回率是一大挑战。

Method: 论文提出了一个检测器无关的后处理流程。具体包括：1）采用重叠切片技术恢复低置信度候选目标；2）通过空间门控（使用DBSCAN对检测框中心点聚类）与语义门控（基于ResNet-18特征聚类）验证目标组；3）对验证后的目标组进行置信度重加权，并与原始输出融合（类别相关的非极大值抑制NMS）。整个流程无需对原有检测器重新训练。

Result: 在VisDrone数据集上，召回率由0.685提升到0.778（+0.093），精度由0.801降至0.595，F1达到0.669。每张图像后处理延迟约0.095秒。消融实验表明各环节（切片、空间聚类、语义聚类、重加权）均有积极作用。

Conclusion: 所提后处理框架能有效提升密集小目标检测的召回率，具备召回优先、精度有所权衡的特性，适合远场计数与监测等对召回率要求较高的应用。该方法不依赖特定检测器，后续可通过引入时序信息进一步优化。

Abstract: Dense small objects in UAV imagery are often missed due to long-range
viewpoints, occlusion, and clutter[cite: 5]. This paper presents a
detector-agnostic post-processing framework that converts overlap-induced
redundancy into group evidence[cite: 6]. Overlapping tiling first recovers
low-confidence candidates[cite: 7]. A Spatial Gate (DBSCAN on box centroids)
and a Semantic Gate (DBSCAN on ResNet-18 embeddings) then validates group
evidence[cite: 7]. Validated groups receive controlled confidence reweighting
before class-aware NMS fusion[cite: 8]. Experiments on VisDrone show a recall
increase from 0.685 to 0.778 (+0.093) and a precision adjustment from 0.801 to
0.595, yielding F1=0.669[cite: 9]. Post-processing latency averages 0.095 s per
image[cite: 10]. These results indicate recall-first, precision-trade-off
behavior that benefits recall-sensitive applications such as far-field counting
and monitoring[cite: 10]. Ablation confirms that tiling exposes missed objects,
spatial clustering stabilizes geometry, semantic clustering enforces appearance
coherence, and reweighting provides calibrated integration with the
baseline[cite: 11]. The framework requires no retraining and integrates with
modern detectors[cite: 12]. Future work will reduce semantic gating cost and
extend the approach with temporal cues[cite: 13].

</details>


### [13] [InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts](https://arxiv.org/abs/2509.10813)
*Weipeng Zhong,Peizhou Cao,Yichen Jin,Li Luo,Wenzhe Cai,Jingli Lin,Hanqing Wang,Zhaoyang Lyu,Tai Wang,Bo Dai,Xudong Xu,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 本文提出了一个名为InternScenes的大规模、高多样性、可模拟的室内三维场景数据集，用于推动Embodied AI的发展。该数据集包含约4万个多样化场景和196万个三维物体，涵盖丰富的小物品和复杂场景布局，将作为开源资源贡献给社区。


<details>
  <summary>Details</summary>
Motivation: 当前Embodied AI发展依赖于高质量的3D场景数据集，但现有数据集在规模、多样性、真实性等方面存在不足，尤其是缺少小物品、场景布置过于简单或存在物体碰撞等问题。为解决这些问题且进一步促进相关研究，作者有必要构建更真实、更复杂且可模拟的大型3D场景数据集。

Method: 作者整合三类场景来源（真实世界扫描、程序生成、设计师手工创建）构建数据集，涵盖15种常见场景类型和288个物体类别。通过数据处理流水线，作者实现了真实与模拟场景的转换，改善了交互性、保留了丰富小物品、并通过物理仿真处理物体碰撞，确保数据可用于仿真。此外，包含可交互对象，提升了应用价值。

Result: InternScenes包含约4万个多样化室内场景，1.96M三维物体、平均每区域约41.5个物体。数据集成功应用于场景布局生成和点位导航两个基准任务，结果显示：该数据集复杂、真实的布局给模型带来了新的挑战，并能有效提升模型训练规模和能力。

Conclusion: InternScenes极大丰富了Embodied AI研究的数据基础，提升了场景复杂度和现实性，有助于推动模型在生成和导航任务上的能力提升，并承诺向社区开源数据、模型和基准测试以促进相关研究。

Abstract: The advancement of Embodied AI heavily relies on large-scale, simulatable 3D
scene datasets characterized by scene diversity and realistic layouts. However,
existing datasets typically suffer from limitations in data scale or diversity,
sanitized layouts lacking small items, and severe object collisions. To address
these shortcomings, we introduce \textbf{InternScenes}, a novel large-scale
simulatable indoor scene dataset comprising approximately 40,000 diverse scenes
by integrating three disparate scene sources, real-world scans, procedurally
generated scenes, and designer-created scenes, including 1.96M 3D objects and
covering 15 common scene types and 288 object classes. We particularly preserve
massive small items in the scenes, resulting in realistic and complex layouts
with an average of 41.5 objects per region. Our comprehensive data processing
pipeline ensures simulatability by creating real-to-sim replicas for real-world
scans, enhances interactivity by incorporating interactive objects into these
scenes, and resolves object collisions by physical simulations. We demonstrate
the value of InternScenes with two benchmark applications: scene layout
generation and point-goal navigation. Both show the new challenges posed by the
complex and realistic layouts. More importantly, InternScenes paves the way for
scaling up the model training for both tasks, making the generation and
navigation in such complex scenes possible. We commit to open-sourcing the
data, models, and benchmarks to benefit the whole community.

</details>


### [14] [Well-Conditioned Polynomial Representations for Mathematical Handwriting Recognition](https://arxiv.org/abs/2509.10815)
*Robert M. Corless,Deepak Singh Kalhan,Stephen M. Watt*

Main category: cs.CV

TL;DR: 本文研究了用多项式作为书写轨迹的数学表示方法，考察了不同正交多项式基（如Legendre、Chebyshev及其Sobolev变体）在数学手写体数字墨水建模中的表现及其权衡。


<details>
  <summary>Details</summary>
Motivation: 先前工作发现用参数化平面曲线多项式（如Legendre基）的表达能为数字墨水提供紧凑的几何表示，但不同基的实际表现优劣和计算复杂性尚未深入研究。

Method: 系统分析并比较了Legendre、Legendre-Sobolev、Chebyshev和Chebyshev-Sobolev等不同正交多项式基在多项式阶数及计算条件数（condition number）上的表现，并用各种内积比较写法符号间变化的范数。实验中主要以条件数和范数为指标衡量表现与效率。

Result: 不同基有不同的数值稳定性与效率表现，作者给出了这些基的条件数比较，并通过数学分析对不同符号间变化的范数给出了上界。发现选用合适基和多项式阶可以在精度与效率之间取得较好权衡。

Conclusion: 多项式表示的基底选择与次数直接影响数学数字墨水的建模效果与计算开销，通过本文的分析和比较，可指导实际系统在表示精度和计算复杂度之间做出合理选择。

Abstract: Previous work has made use of a parameterized plane curve polynomial
representation for mathematical handwriting, with the polynomials represented
in a Legendre or Legendre-Sobolev graded basis. This provides a compact
geometric representation for the digital ink. Preliminary results have also
been shown for Chebyshev and Chebyshev-Sobolev bases. This article explores the
trade-offs between basis choice and polynomial degree to achieve accurate
modeling with a low computational cost. To do this, we consider the condition
number for polynomial evaluation in these bases and bound how the various inner
products give norms for the variations between symbols.

</details>


### [15] [Multi-Task Diffusion Approach For Prediction of Glioma Tumor Progression](https://arxiv.org/abs/2509.10824)
*Aghiles Kebaili,Romain Modzelewski,Jérôme Lapuyade-Lahorgue,Maxime Fontanilles,Sébastien Thureau,Su Ruan*

Main category: cs.CV

TL;DR: 该论文提出了一种多任务扩散框架，用于无时间限制、逐像素预测胶质瘤进展，能根据任意时间点生成未来MRI，并量化肿瘤演化的不确定性。


<details>
  <summary>Details</summary>
Motivation: 由于临床上MRI数据稀疏且随访不规律，数据不完整，导致胶质瘤进展预测存在重大挑战和数据不平衡问题。

Method: 提出了一种结合多任务扩散模型、预训练形变模块和目标增强管线的方法。模型能生成任意时间点的未来MRI FLAIR序列和基于有符号距离场（SDF）的空间演化概率图，同时使用形变模块捕捉肿瘤时空变化。通过特定数据增强和模态补全提升模型表现，还引入了放疗加权的focal loss，以结合放疗剂量信息优化训练。

Result: 方法在公共数据集训练，在内部私有数据集评估，均取得了较好的表现。模型仅需早期两次随访，即可灵活预测未来任意时间的肿瘤风险概率图。

Conclusion: 该方法可在数据稀缺的现实环境下，有效预测胶质瘤进展，提高了临床决策的灵活性和可靠性，具有实际应用前景。

Abstract: Glioma, an aggressive brain malignancy characterized by rapid progression and
its poor prognosis, poses significant challenges for accurate evolution
prediction. These challenges are exacerbated by sparse, irregularly acquired
longitudinal MRI data in clinical practice, where incomplete follow-up
sequences create data imbalances and make reliable modeling difficult. In this
paper, we present a multitask diffusion framework for time-agnostic, pixel-wise
prediction of glioma progression. The model simultaneously generates future
FLAIR sequences at any chosen time point and estimates spatial probabilistic
tumor evolution maps derived using signed distance fields (SDFs), allowing
uncertainty quantification. To capture temporal dynamics of tumor evolution
across arbitrary intervals, we integrate a pretrained deformation module that
models inter-scan changes using deformation fields. Regarding the common
clinical limitation of data scarcity, we implement a targeted augmentation
pipeline that synthesizes complete sequences of three follow-up scans and
imputes missing MRI modalities from available patient studies, improving the
stability and accuracy of predictive models. Based on merely two follow-up
scans at earlier timepoints, our framework produces flexible time-depending
probability maps, enabling clinicians to interrogate tumor progression risks at
any future temporal milestone. We further introduce a radiotherapy-weighted
focal loss term that leverages radiation dose maps, as these highlight regions
of greater clinical importance during model training. The proposed method was
trained on a public dataset and evaluated on an internal private dataset,
achieving promising results in both cases

</details>


### [16] [Point-Plane Projections for Accurate LiDAR Semantic Segmentation in Small Data Scenarios](https://arxiv.org/abs/2509.10841)
*Simone Mosco,Daniel Fusaro,Wanmeng Li,Emanuele Menegatti,Alberto Pretto*

Main category: cs.CV

TL;DR: 本文提出了一种仅利用LiDAR数据，通过点-平面投影从2D表征中有效学习特征的点云语义分割方法，在数据稀缺情况下显著提升性能，并在公开数据集上取得有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 当前3D点云语义分割方法多依赖多模态信息或外部数据，并伴随较高的计算复杂度和对大量数据的需求，导致在数据有限时泛化能力受限。亟需设计一种能在仅用LiDAR且数据稀缺时表现良好的方法。

Method: 提出利用点-平面投影，将点云映射到多个有信息量的2D表示，从而高效提取互补特征，仅依赖LiDAR数据。同时，提出与LiDAR传感器特性对齐且能够缓解类别不均衡的几何感知数据增强技术。

Result: 在有限数据场景下该方法明显提升了点云语义分割的表现，并在SemanticKITTI和PandaSet这两个公开标准数据集上取得了与现有主流方法媲美的竞争性结果。

Conclusion: 本文方法仅用LiDAR数据，通过点-平面投影和几何增强，有效提升了在数据稀缺情况下的分割性能，并表现出良好的泛化能力。

Abstract: LiDAR point cloud semantic segmentation is essential for interpreting 3D
environments in applications such as autonomous driving and robotics. Recent
methods achieve strong performance by exploiting different point cloud
representations or incorporating data from other sensors, such as cameras or
external datasets. However, these approaches often suffer from high
computational complexity and require large amounts of training data, limiting
their generalization in data-scarce scenarios. In this paper, we improve the
performance of point-based methods by effectively learning features from 2D
representations through point-plane projections, enabling the extraction of
complementary information while relying solely on LiDAR data. Additionally, we
introduce a geometry-aware technique for data augmentation that aligns with
LiDAR sensor properties and mitigates class imbalance. We implemented and
evaluated our method that applies point-plane projections onto multiple
informative 2D representations of the point cloud. Experiments demonstrate that
this approach leads to significant improvements in limited-data scenarios,
while also achieving competitive results on two publicly available standard
datasets, as SemanticKITTI and PandaSet. The code of our method is available at
https://github.com/SiMoM0/3PNet

</details>


### [17] [OpenUrban3D: Annotation-Free Open-Vocabulary Semantic Segmentation of Large-Scale Urban Point Clouds](https://arxiv.org/abs/2509.10842)
*Chongyu Wang,Kunlei Jing,Jihua Zhu,Di Wang*

Main category: cs.CV

TL;DR: 该论文提出了首个可用于大规模城市点云的3D开放词汇语义分割框架OpenUrban3D，无需多视角影像配准、预训练点云网络或手工标注，实现零样本分割，并在主流数据集上大幅提升了分割精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着数字孪生、智能城市等应用的兴起，对点云场景的语义分割提出了更开放、灵活的需求。然而，受限于点云数据缺少高质量配准影像，当前3D分割方法迁移性弱，难以应对规模与几何、外观变化大的城市环境。本文试图突破现有标签集和数据依赖瓶颈，实现3D点云的开放词汇分割。

Method: OpenUrban3D方法通过多视角、多粒度渲染获得点云的丰富表征，结合掩码级别的视觉-语言特征抽取和样本均衡融合，直接从原始点云生成鲁棒语义特征，并蒸馏到3D骨干网络中，最终支持任意文本查询下的零样本3D分割。全流程无需对齐影像、预训练分割网络和人工标注。

Result: 在SensatUrban、SUM等大规模城市点云基准上，OpenUrban3D在分割精度与跨场景泛化能力方面均显著优于现有方法。

Conclusion: OpenUrban3D是首个兼具开放词汇、无依赖配准影像且无需标注的3D分割框架，在大规模城市场景下具有优秀的灵活性和可扩展性，为智能城市场景理解提供了新方向。

Abstract: Open-vocabulary semantic segmentation enables models to recognize and segment
objects from arbitrary natural language descriptions, offering the flexibility
to handle novel, fine-grained, or functionally defined categories beyond fixed
label sets. While this capability is crucial for large-scale urban point clouds
that support applications such as digital twins, smart city management, and
urban analytics, it remains largely unexplored in this domain. The main
obstacles are the frequent absence of high-quality, well-aligned multi-view
imagery in large-scale urban point cloud datasets and the poor generalization
of existing three-dimensional (3D) segmentation pipelines across diverse urban
environments with substantial variation in geometry, scale, and appearance. To
address these challenges, we present OpenUrban3D, the first 3D open-vocabulary
semantic segmentation framework for large-scale urban scenes that operates
without aligned multi-view images, pre-trained point cloud segmentation
networks, or manual annotations. Our approach generates robust semantic
features directly from raw point clouds through multi-view, multi-granularity
rendering, mask-level vision-language feature extraction, and sample-balanced
fusion, followed by distillation into a 3D backbone model. This design enables
zero-shot segmentation for arbitrary text queries while capturing both semantic
richness and geometric priors. Extensive experiments on large-scale urban
benchmarks, including SensatUrban and SUM, show that OpenUrban3D achieves
significant improvements in both segmentation accuracy and cross-scene
generalization over existing methods, demonstrating its potential as a flexible
and scalable solution for 3D urban scene understanding.

</details>


### [18] [AutoOEP -- A Multi-modal Framework for Online Exam Proctoring](https://arxiv.org/abs/2509.10887)
*Aryan Kashyap Naveen,Bhuvanesh Singla,Raajan Wankhade,Shreesha M,Ramu S,Ram Mohana Reddy Guddeti*

Main category: cs.CV

TL;DR: 本文提出了一套自动化远程考试监考系统AutoOEP，通过多模态融合提升作弊检测准确性，显著减少人工参与需求，提升在线考试公正性。


<details>
  <summary>Details</summary>
Motivation: 在线教育普及推动了远程考试需求，但传统人工监考无法大规模应用，现有自动化方案要么过于侵入性、要么检测范围有限，难以有效保障考试诚信。

Method: AutoOEP采用双摄像头获取考生正面和侧面视角，利用计算机视觉与机器学习方法并行分析多项作弊特征。人脸模块用ArcFace进行身份验证，结合头部姿态、视线追踪和口部动作分析；手部模块用YOLOv11检测禁用品，并追踪手与物品接近度。模块特征聚合后输入LSTM网络，输出实时作弊概率评分。

Result: 在自建多样化考试场景数据集上，AutoOEP对作弊行为检测准确率达90.7%，禁用品检测mAP@0.5为0.57，无GPU条件下视频流处理速度为2.4帧每秒。

Conclusion: AutoOEP能高效自动化监考，大幅降低人工干预需求，对提升在线考试学术诚信具有重要作用。

Abstract: The burgeoning of online education has created an urgent need for robust and
scalable systems to ensure academic integrity during remote examinations.
Traditional human proctoring is often not feasible at scale, while existing
automated solutions can be intrusive or fail to detect a wide range of cheating
behaviors. This paper introduces AutoOEP (Automated Online Exam Proctoring), a
comprehensive, multi-modal framework that leverages computer vision and machine
learning to provide effective, automated proctoring. The system utilizes a
dual-camera setup to capture both a frontal view of the examinee and a side
view of the workspace, minimizing blind spots. Our approach integrates several
parallel analyses: the Face Module performs continuous identity verification
using ArcFace, along with head pose estimation, gaze tracking, and mouth
movement analysis to detect suspicious cues. Concurrently, the Hand Module
employs a fine-tuned YOLOv11 model for detecting prohibited items (e.g., mobile
phones, notes) and tracks hand proximity to these objects. Features from these
modules are aggregated and fed into a Long Short-Term Memory (LSTM) network
that analyzes temporal patterns to calculate a real-time cheating probability
score. We evaluate AutoOEP on a custom-collected dataset simulating diverse
exam conditions. Our system achieves an accuracy of 90.7% in classifying
suspicious activities. The object detection component obtains a mean Average
Precision (mAP@.5) of 0.57 for prohibited items, and the entire framework
processes video streams at approximately 2.4 frames per second without a GPU.
The results demonstrate that AutoOEP is an effective and resource-efficient
solution for automated proctoring, significantly reducing the need for human
intervention and enhancing the integrity of online assessments.

</details>


### [19] [Total Variation Subgradient Guided Image Fusion for Dual-Camera CASSI System](https://arxiv.org/abs/2509.10897)
*Weiqiang Zhao,Tianzhu Liu,Yuzhe Gui,Yanfeng Gu*

Main category: cs.CV

TL;DR: 本文提出了一种结合TV次梯度理论的双相机CASSI重建框架，通过引入动态正则化策略与辅助参考图像，实现了高效且可解释的高压缩率光谱图像重建，在理论和实验上均表现突出。


<details>
  <summary>Details</summary>
Motivation: CASSI系统在提升光谱、空间与时间分辨率时面临重建问题病态，传统方法或依赖人为先验、或缺乏物理解释性，限制了重建质量与应用推广。

Method: 提出基于总变分（TV）次梯度理论的双相机SD-CASSI数学模型，通过引入来自RGB或全色辅助图像的归一化梯度约束，动态自适应生成参考图像并引导TV正则化优化，确保解的空间-光谱结构一致性，并优化算法计算复杂度。

Result: 实验结果验证了所提方法能在不同重建场景下有效保持空间与光谱结构的一致性，提升了重建质量，且表现出较强的泛化能力与理论可解释性。

Conclusion: 本工作建立了多相机计算光谱成像的理论基础，通过可解释的数学框架和动态梯度约束方法，显著改善了压缩率高时的光谱图像重建质量。

Abstract: Spectral imaging technology has long-faced fundamental challenges in
balancing spectral, spatial, and temporal resolutions. While compressive
sensing-based Coded Aperture Snapshot Spectral Imaging (CASSI) mitigates this
trade-off through optical encoding, high compression ratios result in ill-posed
reconstruction problems. Traditional model-based methods exhibit limited
performance due to reliance on handcrafted inherent image priors, while deep
learning approaches are constrained by their black-box nature, which
compromises physical interpretability. To address these limitations, we propose
a dual-camera CASSI reconstruction framework that integrates total variation
(TV) subgradient theory. By establishing an end-to-end SD-CASSI mathematical
model, we reduce the computational complexity of solving the inverse problem
and provide a mathematically well-founded framework for analyzing multi-camera
systems. A dynamic regularization strategy is introduced, incorporating
normalized gradient constraints from RGB/panchromatic-derived reference images,
which constructs a TV subgradient similarity function with strict convex
optimization guarantees. Leveraging spatial priors from auxiliary cameras, an
adaptive reference generation and updating mechanism is designed to provide
subgradient guidance. Experimental results demonstrate that the proposed method
effectively preserves spatial-spectral structural consistency. The theoretical
framework establishes an interpretable mathematical foundation for
computational spectral imaging, demonstrating robust performance across diverse
reconstruction scenarios. The source code is available at
https://github.com/bestwishes43/ADMM-TVDS.

</details>


### [20] [Lightweight Metadata-Aware Mixture-of-Experts Masked Autoencoder for Earth Observation](https://arxiv.org/abs/2509.10919)
*Mohanad Albughdadi*

Main category: cs.CV

TL;DR: 本文提出了一种仅有2.5M参数的紧凑型元数据感知专家混合掩码自编码器（MoE-MAE），可实现与更大模型相当的下游性能，并提升了迁移和标签利用效率。


<details>
  <summary>Details</summary>
Motivation: 当前地球观测领域的大模型虽然表现优异，但计算成本高，难以广泛应用。作者希望通过设计小巧高效的模型，降低使用门槛，促进模型在真实任务中的可用性和普及。

Method: 作者提出了一个融合稀疏专家路由、地理与时间元数据的MoE-MAE结构。模型利用卫星图像、经纬度及季节/日周期信息进行联合预训练，并在BigEarthNet-Landsat数据集上进行无监督训练，采用线性探针评价编码器表现。

Result: 尽管模型参数远少于当前主流大模型，但在下游任务和迁移测试上，紧凑型MoE-MAE取得了相当甚至接近的性能，尤其是在缺乏明确元数据的EuroSAT-Landsat上仍表现优秀。

Conclusion: 紧凑的、元数据感知的MoE-MAEs能够有效提升地球观测基础模型的效率和可扩展性，为小型、高效EO基础模型的未来发展方向提供了新的思路。

Abstract: Recent advances in Earth Observation have focused on large-scale foundation
models. However, these models are computationally expensive, limiting their
accessibility and reuse for downstream tasks. In this work, we investigate
compact architectures as a practical pathway toward smaller general-purpose EO
models. We propose a Metadata-aware Mixture-of-Experts Masked Autoencoder
(MoE-MAE) with only 2.5M parameters. The model combines sparse expert routing
with geo-temporal conditioning, incorporating imagery alongside
latitude/longitude and seasonal/daily cyclic encodings. We pretrain the MoE-MAE
on the BigEarthNet-Landsat dataset and evaluate embeddings from its frozen
encoder using linear probes. Despite its small size, the model competes with
much larger architectures, demonstrating that metadata-aware pretraining
improves transfer and label efficiency. To further assess generalization, we
evaluate on the EuroSAT-Landsat dataset, which lacks explicit metadata, and
still observe competitive performance compared to models with hundreds of
millions of parameters. These results suggest that compact, metadata-aware
MoE-MAEs are an efficient and scalable step toward future EO foundation models.

</details>


### [21] [Simulating Sinogram-Domain Motion and Correcting Image-Domain Artifacts Using Deep Learning in HR-pQCT Bone Imaging](https://arxiv.org/abs/2509.10961)
*Farhan Sadik,Christopher L. Newman,Stuart J. Warden,Rachel K. Surowiec*

Main category: cs.CV

TL;DR: 该论文提出了一种用于高分辨率外周定量CT（HR-pQCT）运动伪影校正的深度学习新方法，并在模拟与真实数据集上取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: HR-pQCT图像常受刚性运动伪影（如骨皮质条纹、松质骨模糊）影响，严重阻碍了骨微结构的体内评估，目前尚无有效的运动伪影校正方法，主要原因是缺乏标准化的退化模型。本文旨在解决该问题，推动HR-pQCT的广泛应用。

Method: 作者优化了一种基于sinogram的传统方法，模拟HR-pQCT图像中的运动伪影，生成配对的运动损坏图像和真值数据集，使其可用于有监督学习。提出了一个名为ESWGAN-GP的生成对抗网络，结合了边缘增强跳连、Self-attention及梯度惩罚机制，并引入基于VGG的感知损失以更好地恢复细微骨结构。

Result: ESWGAN-GP模型在模拟数据集上取得了平均信噪比（SNR）26.78、结构相似指数（SSIM）0.81和视觉信息保真度（VIF）0.76，在真实目标数据集上分别提升至29.31、0.87和0.81。

Conclusion: 所提方法初步解决了HR-pQCT运动伪影校正问题，为将深度学习应用于实际的HR-pQCT运动伪影校正提供了有效途径，虽尚未完全解决全部复杂运动场景问题，但为该领域后续研究奠定了基础。

Abstract: Rigid-motion artifacts, such as cortical bone streaking and trabecular
smearing, hinder in vivo assessment of bone microstructures in high-resolution
peripheral quantitative computed tomography (HR-pQCT). Despite various motion
grading techniques, no motion correction methods exist due to the lack of
standardized degradation models. We optimize a conventional sinogram-based
method to simulate motion artifacts in HR-pQCT images, creating paired datasets
of motion-corrupted images and their corresponding ground truth, which enables
seamless integration into supervised learning frameworks for motion correction.
As such, we propose an Edge-enhanced Self-attention Wasserstein Generative
Adversarial Network with Gradient Penalty (ESWGAN-GP) to address motion
artifacts in both simulated (source) and real-world (target) datasets. The
model incorporates edge-enhancing skip connections to preserve trabecular edges
and self-attention mechanisms to capture long-range dependencies, facilitating
motion correction. A visual geometry group (VGG)-based perceptual loss is used
to reconstruct fine micro-structural features. The ESWGAN-GP achieves a mean
signal-to-noise ratio (SNR) of 26.78, structural similarity index measure
(SSIM) of 0.81, and visual information fidelity (VIF) of 0.76 for the source
dataset, while showing improved performance on the target dataset with an SNR
of 29.31, SSIM of 0.87, and VIF of 0.81. The proposed methods address a
simplified representation of real-world motion that may not fully capture the
complexity of in vivo motion artifacts. Nevertheless, because motion artifacts
present one of the foremost challenges to more widespread adoption of this
modality, these methods represent an important initial step toward implementing
deep learning-based motion correction in HR-pQCT.

</details>


### [22] [Gaze Authentication: Factors Influencing Authentication Performance](https://arxiv.org/abs/2509.10969)
*Dillon Lohr,Michael J Proulx,Mehedi Hasan Raju,Oleg V Komogortsev*

Main category: cs.CV

TL;DR: 本论文研究了影响最先进的基于凝视认证性能的关键因素。通过对8849名受试者大规模数据集的实验，评估了校准深度、信号质量和滤波方法等因素对认证效果的影响。结果表明，提升信号质量、校准方式等能提高系统性能。


<details>
  <summary>Details</summary>
Motivation: 随着虚拟现实和增强现实系统的兴起，基于眼动追踪的身份认证变得越来越重要。为了提升安全性和用户体验，需要深入理解影响认证结果的各种因素。

Method: 通过Meta Quest Pro级别的硬件和高频视频眼动仪采集8849名受试者的凝视数据，采用最先进的神经网络对比不同信号质量、校准策略（如校准目标深度、是否融合校准与非校准数据）、滤波方式（如滑动平均）等对认证性能的影响。

Result: 结果显示，统一校准目标深度、融合不同校准方式的凝视数据、提升信号质量均可提升基于凝视的认证效果。而简单的三点滑动平均滤波器一般会略微降低认证性能，但也存在一些例外。

Conclusion: 研究表明，高质量的眼动信号以及合理的校准策略对提升凝视认证性能作用显著。部分简单的滤波策略可能并不适用。未来研究应关注更多影响因素与异常情况。

Abstract: This paper examines the key factors that influence the performance of
state-of-the-art gaze-based authentication. Experiments were conducted on a
large-scale, in-house dataset comprising 8,849 subjects collected with Meta
Quest Pro equivalent hardware running a video oculography-driven gaze
estimation pipeline at 72Hz. The state-of-the-art neural network architecture
was employed to study the influence of the following factors on authentication
performance: eye tracking signal quality, various aspects of eye tracking
calibration, and simple filtering on estimated raw gaze. We found that using
the same calibration target depth for eye tracking calibration, fusing
calibrated and non-calibrated gaze, and improving eye tracking signal quality
all enhance authentication performance. We also found that a simple
three-sample moving average filter slightly reduces authentication performance
in general. While these findings hold true for the most part, some exceptions
were noted.

</details>


### [23] [TrueSkin: Towards Fair and Accurate Skin Tone Recognition and Generation](https://arxiv.org/abs/2509.10980)
*Haoming Lu*

Main category: cs.CV

TL;DR: 当前大模型在皮肤色调识别和生成上表现不佳，主要因缺乏完善的数据集。该文提出TrueSkin数据集，有7299张多样化图片，并用其评测现有模型。TrueSkin显著提高了识别和生成的准确率和公平性。


<details>
  <summary>Details</summary>
Motivation: 皮肤色调识别在公正性及医疗、生成式AI等领域很重要，但当前缺乏有代表性的数据集和方法，致使主流多模态模型和生成模型表现较差。

Method: 作者构建了TrueSkin数据集，涵盖6种类别、多种光线和拍摄角度，总计7299张图片。基于该数据集，对主流识别和生成方法进行基准测试，分析其偏差，并尝试用TrueSkin进行模型训练及微调。

Result: 实验发现，现有LMMs常将中间色调错误分类为较浅色，生成模型在受发型或环境等无关提示影响时，难以准确生成特定肤色。用TrueSkin训练识别模型，准确率提升超20%；在生成任务上微调也显著提升了皮肤色调的还原度。

Conclusion: TrueSkin数据集不仅能作为评测基准，还为改善模型的公正性和准确率提供了重要资源。实验强调了高质量、多样化皮肤色调数据集的迫切需求。

Abstract: Skin tone recognition and generation play important roles in model fairness,
healthcare, and generative AI, yet they remain challenging due to the lack of
comprehensive datasets and robust methodologies. Compared to other human image
analysis tasks, state-of-the-art large multimodal models (LMMs) and image
generation models struggle to recognize and synthesize skin tones accurately.
To address this, we introduce TrueSkin, a dataset with 7299 images
systematically categorized into 6 classes, collected under diverse lighting
conditions, camera angles, and capture settings. Using TrueSkin, we benchmark
existing recognition and generation approaches, revealing substantial biases:
LMMs tend to misclassify intermediate skin tones as lighter ones, whereas
generative models struggle to accurately produce specified skin tones when
influenced by inherent biases from unrelated attributes in the prompts, such as
hairstyle or environmental context. We further demonstrate that training a
recognition model on TrueSkin improves classification accuracy by more than
20\% compared to LMMs and conventional approaches, and fine-tuning with
TrueSkin significantly improves skin tone fidelity in image generation models.
Our findings highlight the need for comprehensive datasets like TrueSkin, which
not only serves as a benchmark for evaluating existing models but also provides
a valuable training resource to enhance fairness and accuracy in skin tone
recognition and generation tasks.

</details>


### [24] [Policy-Driven Transfer Learning in Resource-Limited Animal Monitoring](https://arxiv.org/abs/2509.10995)
*Nisha Pillai,Aditi Virupakshaiah,Harrison W. Smith,Amanda J. Ashworth,Prasanna Gowda,Phillip R. Owens,Adam R. Rivers,Bindu Nanduri,Mahalingam Ramkumar*

Main category: cs.CV

TL;DR: 本文提出了一种基于强化学习的迁移学习框架，能够自动为动物检测任务选择最合适的预训练模型，提升检测率并降低计算时间。


<details>
  <summary>Details</summary>
Motivation: 目前在野生动物保护和家畜管理中，自动化动物检测与追踪系统需求日益增长，但受限于训练数据稀缺和大量预训练模型选择困难，阻碍了深度学习技术的高效应用。作者希望通过自动化的模型优选，解决非专业人员在模型选择上的难题。

Method: 论文提出一种结合强化学习与上置信界（UCB）算法的迁移学习框架。该框架能自动评估并为动物检测任务选择最优的预训练模型，实现模型的系统性筛选与排序，简化选择流程。

Result: 实验显示，该框架在动物检测的识别率上优于传统方法，并且大幅减少了所需的计算时间。

Conclusion: 基于强化学习的迁移学习方法能够有效简化预训练模型的选取过程，显著提升动物检测任务的效率与准确性，适合数据或资源有限场景，对新手研究者尤为有益。

Abstract: Animal health monitoring and population management are critical aspects of
wildlife conservation and livestock management that increasingly rely on
automated detection and tracking systems. While Unmanned Aerial Vehicle (UAV)
based systems combined with computer vision offer promising solutions for
non-invasive animal monitoring across challenging terrains, limited
availability of labeled training data remains an obstacle in developing
effective deep learning (DL) models for these applications. Transfer learning
has emerged as a potential solution, allowing models trained on large datasets
to be adapted for resource-limited scenarios such as those with limited data.
However, the vast landscape of pre-trained neural network architectures makes
it challenging to select optimal models, particularly for researchers new to
the field. In this paper, we propose a reinforcement learning (RL)-based
transfer learning framework that employs an upper confidence bound (UCB)
algorithm to automatically select the most suitable pre-trained model for
animal detection tasks. Our approach systematically evaluates and ranks
candidate models based on their performance, streamlining the model selection
process. Experimental results demonstrate that our framework achieves a higher
detection rate while requiring significantly less computational time compared
to traditional methods.

</details>


### [25] [Improving Fungi Prototype Representations for Few-Shot Classification](https://arxiv.org/abs/2509.11020)
*Abdarahmane Traore,Éric Hervet,Andy Couturier*

Main category: cs.CV

TL;DR: 本文提出了一种基于原型网络的深度学习方法，能有效应对野外采集的真菌物种极度类别不平衡和样本稀缺的问题，在FungiCLEF 2025竞赛中表现优异。


<details>
  <summary>Details</summary>
Motivation: 准确识别真菌有助于大型生物多样性调研，但现有系统难以应对物种类别极不平衡和稀有物种样本不足等挑战，尤其在真实观察数据中更为突出，因此需开发新的有效识别方法。

Method: 作者提出基于原型网络的深度学习方法，通过增强原型表示能力，实现对少样本的真菌物种进行分类，重点提升对稀有和缺乏文献记录的物种识别效果。

Result: 该方法在FungiCLEF 2025公开组和私有组排行榜上的Recall@5均比基线提升超过30个百分点。

Conclusion: 该方法显著提升了对常见与稀有真菌物种的自动识别准确性，有助于推动生物多样性大数据的分析应用，支持FungiCLEF 2025的核心目标。

Abstract: The FungiCLEF 2025 competition addresses the challenge of automatic fungal
species recognition using realistic, field-collected observational data.
Accurate identification tools support both mycologists and citizen scientists,
greatly enhancing large-scale biodiversity monitoring. Effective recognition
systems in this context must handle highly imbalanced class distributions and
provide reliable performance even when very few training samples are available
for many species, especially rare and under-documented taxa that are often
missing from standard training sets. According to competition organizers, about
20\% of all verified fungi observations, representing nearly 20,000 instances,
are associated with these rarely recorded species. To tackle this challenge, we
propose a robust deep learning method based on prototypical networks, which
enhances prototype representations for few-shot fungal classification. Our
prototypical network approach exceeds the competition baseline by more than 30
percentage points in Recall@5 on both the public (PB) and private (PR)
leaderboards. This demonstrates strong potential for accurately identifying
both common and rare fungal species, supporting the main objectives of
FungiCLEF 2025.

</details>


### [26] [Cluster-Level Sparse Multi-Instance Learning for Whole-Slide Images](https://arxiv.org/abs/2509.11034)
*Yuedi Zhang,Zhixiang Xia,Guosheng Yin,Bin Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新型的多实例学习方法csMIL，通过全局与局部聚类、簇内注意力和簇级稀疏机制，显著提升弱标注病理切片分析的鲁棒性和可解释性，并在公开数据集上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有多实例学习方法在处理如病理切片等复杂、冗余且弱标注的数据集时，无法有效丢弃无用实例，导致模型鲁棒性和可解释性不足。

Method: 提出csMIL框架，先在所有包上做全局聚类形成K个中心，再包内做局部聚类分配簇标签。每个聚类中计算注意力分数，且对簇权重施加稀疏化正则，使模型能有选择性地保留诊断相关的实例簇并丢弃无关形簇。

Result: 理论分析表明，csMIL只需O(s log K)个包即可恢复s个相关簇。实验证明，csMIL在CAMELYON16和TCGA-NSCLC两个病理切片基准数据集上取得了SOTA表现。

Conclusion: csMIL方法不仅提升了多实例学习在复杂病理图像上的鲁棒性和可解释性，还能降低计算复杂度，为大规模医学影像分析等场景提供了有力手段。

Abstract: Multi-Instance Learning (MIL) is pivotal for analyzing complex, weakly
labeled datasets, such as whole-slide images (WSIs) in computational pathology,
where bags comprise unordered collections of instances with sparse diagnostic
relevance. Traditional MIL approaches, including early statistical methods and
recent attention-based frameworks, struggle with instance redundancy and lack
explicit mechanisms for discarding non-informative instances, limiting their
robustness and interpretability. We propose Cluster-level Sparse MIL (csMIL), a
novel framework that integrates global-local instance clustering,
within-cluster attention, and cluster-level sparsity induction to address these
challenges. Our csMIL first performs global clustering across all bags to
establish $K$ cluster centers, followed by local clustering within each bag to
assign cluster labels. Attention scores are computed within each cluster, and
sparse regularization is applied to cluster weights, enabling the selective
retention of diagnostically relevant clusters while discarding irrelevant ones.
This approach enhances robustness to noisy instances, improves interpretability
by identifying critical regions, and reduces computational complexity.
Theoretical analysis demonstrates that csMIL requires $O(s log K)$ bags to
recover $s$ relevant clusters, aligning with compressed sensing principles.
Empirically, csMIL achieves state-of-the-art performance on two public
histopathology benchmarks (CAMELYON16, TCGA-NSCLC).

</details>


### [27] [Action Hints: Semantic Typicality and Context Uniqueness for Generalizable Skeleton-based Video Anomaly Detection](https://arxiv.org/abs/2509.11058)
*Canhui Tang,Sanping Zhou,Haoyue Shi,Le Wang*

Main category: cs.CV

TL;DR: 本文提出了一种零样本视频异常检测（ZS-VAD）新方法，无需目标域训练数据即可在多种场景下高效检测异常行为，方法显著优于现有骨架基方法。


<details>
  <summary>Details</summary>
Motivation: 随着数据隐私和新监控部署需求的增长，现实中常常无法获得目标场景的大量训练数据。现有骨架法泛化能力受限，难以适应新场景与行为模式。

Method: 1）引入基于语言模型指导的语义典型性建模，将骨架片段映射到动作语义空间，并在训练时提取常见正常/异常行为的知识；2）提出测试时的上下文唯一性分析，细致比较骨架片段间的时空差异，动态生成适应场景的判断边界。

Result: 在上海科技、UBnormal、NWPU和UCF-Crime等4个大型数据集（包含100+未见监控场景）上，本方法在不使用目标域数据的情况下，在骨架基方法中达到最优性能。

Conclusion: 通过结合典型性学习和场景自适应判断，本研究大幅提升了ZS-VAD的泛化能力和检测效果，为数据受限或部署新场景的异常检测提供了切实可行的新路径。

Abstract: Zero-Shot Video Anomaly Detection (ZS-VAD) requires temporally localizing
anomalies without target domain training data, which is a crucial task due to
various practical concerns, e.g., data privacy or new surveillance deployments.
Skeleton-based approach has inherent generalizable advantages in achieving
ZS-VAD as it eliminates domain disparities both in background and human
appearance. However, existing methods only learn low-level skeleton
representation and rely on the domain-limited normality boundary, which cannot
generalize well to new scenes with different normal and abnormal behavior
patterns. In this paper, we propose a novel zero-shot video anomaly detection
framework, unlocking the potential of skeleton data via action typicality and
uniqueness learning. Firstly, we introduce a language-guided semantic
typicality modeling module that projects skeleton snippets into action semantic
space and distills LLM's knowledge of typical normal and abnormal behaviors
during training. Secondly, we propose a test-time context uniqueness analysis
module to finely analyze the spatio-temporal differences between skeleton
snippets and then derive scene-adaptive boundaries. Without using any training
samples from the target domain, our method achieves state-of-the-art results
against skeleton-based methods on four large-scale VAD datasets: ShanghaiTech,
UBnormal, NWPU, and UCF-Crime, featuring over 100 unseen surveillance scenes.

</details>


### [28] [Organoid Tracker: A SAM2-Powered Platform for Zero-shot Cyst Analysis in Human Kidney Organoid Videos](https://arxiv.org/abs/2509.11063)
*Xiaoyu Huang,Lauren M Maxson,Trang Nguyen,Cheng Jack Song,Yuankai Huo*

Main category: cs.CV

TL;DR: 该论文介绍了一款基于最新视觉基础模型SAM2的开源软件Organoid Tracker，可自动分析肾脏类器官的显微视频，为多囊肾疾病（PKD）的研究和药物筛选提供高效工具。


<details>
  <summary>Details</summary>
Motivation: 目前肾脏类器官系统生成大量复杂的时空显微数据，人工分析方法不仅效率低下且通常只能做粗略的分类，难以深入挖掘像素级和时序信息，限制了对疾病过程及药物效果的理解。

Method: 论文开发了Organoid Tracker，这是一个基于模块化插件架构、具有图形界面的平台，利用SAM2模型实现即插即用的零样本分割和自动化分析，无需编程即可量化类器官囊肿形成、增长速度和形态变化等重要指标，能输出详细的分析报告。

Result: Organoid Tracker成功实现了肾脏类器官显微视频的自动化分析，能精准量化各种关键参数，显著提升了研究多囊肾疾病和药物高通量筛选的效率和深度。

Conclusion: 该工具为肾脏发育、PKD建模和药物发现提供了一个可扩展、开源、兼容性强的解决方案，有助于推动肾脏疾病领域相关研究的高效发展。

Abstract: Recent advances in organoid models have revolutionized the study of human
kidney disease mechanisms and drug discovery by enabling scalable,
cost-effective research without the need for animal sacrifice. Here, we present
a kidney organoid platform optimized for efficient screening in polycystic
kidney disease (PKD). While these systems generate rich spatial-temporal
microscopy video datasets, current manual approaches to analysis remain limited
to coarse classifications (e.g., hit vs. non-hit), often missing valuable
pixel-level and longitudinal information. To help overcome this bottleneck, we
developed Organoid Tracker, a graphical user interface (GUI) platform designed
with a modular plugin architecture, which empowers researchers to extract
detailed, quantitative metrics without programming expertise. Built on the
cutting-edge vision foundation model Segment Anything Model 2 (SAM2), Organoid
Tracker enables zero-shot segmentation and automated analysis of
spatial-temporal microscopy videos. It quantifies key metrics such as cyst
formation rate, growth velocity, and morphological changes, while generating
comprehensive reports. By providing an extensible, open-source framework,
Organoid Tracker offers a powerful solution for improving and accelerating
research in kidney development, PKD modeling, and therapeutic discovery. The
platform is publicly available as open-source software at
https://github.com/hrlblab/OrganoidTracker.

</details>


### [29] [The System Description of CPS Team for Track on Driving with Language of CVPR 2024 Autonomous Grand Challenge](https://arxiv.org/abs/2509.11071)
*Jinghan Peng,Jingwen Wang,Xing Yu,Dehui Du*

Main category: cs.CV

TL;DR: 本文提出了一套基于视觉语言模型（Vision Language Model, VLM）的系统，用于CVPR 2024自动驾驶挑战赛，并取得了排行榜第一名。


<details>
  <summary>Details</summary>
Motivation: 推动视觉语言模型在自动驾驶领域的应用，以提升利用自然语言进行路况理解和驾驶决策的能力。

Method: 以LLaVA模型为基础，采用LoRA和DoRA方法进行微调；同时融合了开源深度估计算法获得的深度信息。推理阶段，针对多选与是非题，采用Chain-of-Thought（链式思考）推理方式提升准确性。所有模型均在DriveLM-nuScenes数据集上训练。

Result: 在验证集排行榜上取得了0.7799的最高分，排名第一。

Conclusion: 所提出的方法结合了模型微调、深度信息整合与先进的推理策略，在自动驾驶与多模态理解任务中达到了业界领先的表现。

Abstract: This report outlines our approach using vision language model systems for the
Driving with Language track of the CVPR 2024 Autonomous Grand Challenge. We
have exclusively utilized the DriveLM-nuScenes dataset for training our models.
Our systems are built on the LLaVA models, which we enhanced through
fine-tuning with the LoRA and DoRA methods. Additionally, we have integrated
depth information from open-source depth estimation models to enrich the
training and inference processes. For inference, particularly with
multiple-choice and yes/no questions, we adopted a Chain-of-Thought reasoning
approach to improve the accuracy of the results. This comprehensive methodology
enabled us to achieve a top score of 0.7799 on the validation set leaderboard,
ranking 1st on the leaderboard.

</details>


### [30] [Mars Traversability Prediction: A Multi-modal Self-supervised Approach for Costmap Generation](https://arxiv.org/abs/2509.11082)
*Zongwu Xie,Kaijie Yun,Yang Liu,Yiming Ji,Han Li*

Main category: cs.CV

TL;DR: 提出了一种结合相机与激光雷达（LiDAR）数据的多模态方法，实现对行星探测车可行驶性鸟瞰成本地图的鲁棒预测，新框架具备很强的鲁棒性，对不同输入扰动表现稳定。


<details>
  <summary>Details</summary>
Motivation: 行星探测车在复杂地形上的自动导航依赖高质量的可行驶性地图。现有方法在传感器信息融合和自监督标签生成方面存在不足，且鲁棒性有限。本研究旨在提升多模态感知与自监督训练的鲁棒性与实用性。

Method: 融合了相机和LiDAR数据，通过DINOv3图像编码器和FiLM传感器融合，预测BEV地形成本图（costmap），并用基于IMU的数据自监督生成标签。损失函数结合了Huber损失和光滑性约束。系统性消融实验探索了输入干扰（如图像去色、遮挡、加噪声）及LiDAR稀疏性对性能的影响。

Result: 模型在不同输入扰动和模态缺失下的MAE/MSE变化很小，表现出极强的鲁棒性和对地形几何的依赖，图像语义影响较小。

Conclusion: 所提出的多模态BEV成本地图预测模型效果好且鲁棒性强，得益于高保真仿真环境和自监督IMU标注流程。未来工作将关注领域泛化能力和数据集扩展等问题。

Abstract: We present a robust multi-modal framework for predicting traversability
costmaps for planetary rovers. Our model fuses camera and LiDAR data to produce
a bird's-eye-view (BEV) terrain costmap, trained self-supervised using
IMU-derived labels. Key updates include a DINOv3-based image encoder,
FiLM-based sensor fusion, and an optimization loss combining Huber and
smoothness terms. Experimental ablations (removing image color, occluding
inputs, adding noise) show only minor changes in MAE/MSE (e.g. MAE increases
from ~0.0775 to 0.0915 when LiDAR is sparsified), indicating that geometry
dominates the learned cost and the model is highly robust. We attribute the
small performance differences to the IMU labeling primarily reflecting terrain
geometry rather than semantics and to limited data diversity. Unlike prior work
claiming large gains, we emphasize our contributions: (1) a high-fidelity,
reproducible simulation environment; (2) a self-supervised IMU-based labeling
pipeline; and (3) a strong multi-modal BEV costmap prediction model. We discuss
limitations and future work such as domain generalization and dataset
expansion.

</details>


### [31] [End-to-End Visual Autonomous Parking via Control-Aided Attention](https://arxiv.org/abs/2509.11090)
*Chao Chen,Shunyu Yao,Yuanwu He,Tao Feng,Ruojing Song,Yuliang Guo,Xinyu Huang,Chenxu Wu,Ren Liu,Chen Feng*

Main category: cs.CV

TL;DR: 本文提出了一种新的端到端仿真学习系统（CAA-Policy），通过控制信号引导视觉注意力，以提升自动泊车的精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶系统往往缺乏感知与控制之间的有效协同，尤其在精细操作（如自动泊车）时，现有基于transformer注意力机制的方法存在注意力不稳定、时序一致性差等问题。作者希望解决这些感知与决策之间脱节导致性能不佳和泛化性不足的挑战。

Method: 提出了Control-Aided Attention（CAA）机制，通过控制信号反向传播的梯度来自监督视觉注意力，而不是传统的训练损失。同时融合了短时轨迹预测（waypoint prediction）辅助任务并增加了单独训练的运动预测模块，以提升泊车执行过程中的稳定性和时间一致性。

Result: 在CARLA仿真平台上，所提CAA-Policy方法在准确性、鲁棒性和可解释性方面均显著优于传统的端到端学习基线和模块化分割+混合A*路径规划基线方法。

Conclusion: CAA-Policy能够有效提升自动泊车场景下感知与控制系统的协同性和泛化能力，是自动驾驶领域端到端学习的一项重要进展。

Abstract: Precise parking requires an end-to-end system where perception adaptively
provides policy-relevant details-especially in critical areas where fine
control decisions are essential. End-to-end learning offers a unified framework
by directly mapping sensor inputs to control actions, but existing approaches
lack effective synergy between perception and control. We find that
transformer-based self-attention, when used alone, tends to produce unstable
and temporally inconsistent spatial attention, which undermines the reliability
of downstream policy decisions over time. Instead, we propose CAA-Policy, an
end-to-end imitation learning system that allows control signal to guide the
learning of visual attention via a novel Control-Aided Attention (CAA)
mechanism. For the first time, we train such an attention module in a
self-supervised manner, using backpropagated gradients from the control outputs
instead of from the training loss. This strategy encourages the attention to
focus on visual features that induce high variance in action outputs, rather
than merely minimizing the training loss-a shift we demonstrate leads to a more
robust and generalizable policy. To further enhance stability, CAA-Policy
integrates short-horizon waypoint prediction as an auxiliary task, and
introduces a separately trained motion prediction module to robustly track the
target spot over time. Extensive experiments in the CARLA simulator show that
\titlevariable~consistently surpasses both the end-to-end learning baseline and
the modular BEV segmentation + hybrid A* pipeline, achieving superior accuracy,
robustness, and interpretability. Code is released at
https://github.com/Joechencc/CAAPolicy.

</details>


### [32] [PanoLora: Bridging Perspective and Panoramic Video Generation with LoRA Adaptation](https://arxiv.org/abs/2509.11092)
*Zeyu Dong,Yuyang Yin,Yuqi Li,Eric Li,Hao-Xiang Guo,Yikai Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于LoRA的高质量360度全景视频生成方法，无需大规模训练或复杂架构，通过高效微调实现优于以往方法的全景视频生成。


<details>
  <summary>Details</summary>
Motivation: 传统的透视视频生成模型难以直接用于全景视频生成，现有方法复杂且低效，效果不理想。受LoRA在风格迁移中的成功启发，作者尝试将全景视频生成视为从透视到全景的自适应问题，寻找更高效优质的解决方案。

Method: 作者通过理论证明，只要LoRA的秩大于任务自由度，就能有效拟合投影变换。方法上，利用LoRA微调已有的预训练视频扩散模型，只需大约1000个视频数据，便能实现高质量全景视频生成。

Result: 实验显示，该方法生成的全景视频不仅保持了正确的投影几何，还在视觉质量、左右一致性与运动多样性方面明显优于传统的SOTA方法。

Conclusion: 利用LoRA进行小规模高效微调，可以显著提升全景视频生成质量，为未来相关研究提供新思路。

Abstract: Generating high-quality 360{\deg} panoramic videos remains a significant
challenge due to the fundamental differences between panoramic and traditional
perspective-view projections. While perspective videos rely on a single
viewpoint with a limited field of view, panoramic content requires rendering
the full surrounding environment, making it difficult for standard video
generation models to adapt. Existing solutions often introduce complex
architectures or large-scale training, leading to inefficiency and suboptimal
results. Motivated by the success of Low-Rank Adaptation (LoRA) in style
transfer tasks, we propose treating panoramic video generation as an adaptation
problem from perspective views. Through theoretical analysis, we demonstrate
that LoRA can effectively model the transformation between these projections
when its rank exceeds the degrees of freedom in the task. Our approach
efficiently fine-tunes a pretrained video diffusion model using only
approximately 1,000 videos while achieving high-quality panoramic generation.
Experimental results demonstrate that our method maintains proper projection
geometry and surpasses previous state-of-the-art approaches in visual quality,
left-right consistency, and motion diversity.

</details>


### [33] [SMILE: A Super-resolution Guided Multi-task Learning Method for Hyperspectral Unmixing](https://arxiv.org/abs/2509.11093)
*Ruiying Li,Bin Pan,Qiaoying Qu,Xia Xu,Zhenwei Shi*

Main category: cs.CV

TL;DR: 本文提出了一种理论分析支持的超分辨率引导多任务学习方法（SMILE），用于提升高光谱解混性能，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 高光谱解混受限于低空间分辨率，直接融合超分辨率和解混存在任务亲和性未验证、解混收敛性无保证等问题。

Method: 作者在理论上分析并验证了多任务学习方法的可行性和任务亲和性，通过关系和存在性定理证明了超分辨率对解混的正向指导作用。框架设计上，SMILE学习共享和特有表示，将超分辨率的正确信息迁移到解混任务。为保证收敛性，还证明了解混的最优解存在。

Result: 在合成和真实数据集上的实验结果证实了所提方法的有效性和实用性。

Conclusion: SMILE方法在理论和实际应用中均展现出优越性，为融合超分辨率与高光谱解混提供了可靠框架和支撑。

Abstract: The performance of hyperspectral unmixing may be constrained by low spatial
resolution, which can be enhanced using super-resolution in a multitask
learning way. However, integrating super-resolution and unmixing directly may
suffer two challenges: Task affinity is not verified, and the convergence of
unmixing is not guaranteed. To address the above issues, in this paper, we
provide theoretical analysis and propose super-resolution guided multi-task
learning method for hyperspectral unmixing (SMILE). The provided theoretical
analysis validates feasibility of multitask learning way and verifies task
affinity, which consists of relationship and existence theorems by proving the
positive guidance of super-resolution. The proposed framework generalizes
positive information from super-resolution to unmixing by learning both shared
and specific representations. Moreover, to guarantee the convergence, we
provide the accessibility theorem by proving the optimal solution of unmixing.
The major contributions of SMILE include providing progressive theoretical
support, and designing a new framework for unmixing under the guidance of
super-resolution. Our experiments on both synthetic and real datasets have
substantiate the usefulness of our work.

</details>


### [34] [A Copula-Guided Temporal Dependency Method for Multitemporal Hyperspectral Images Unmixing](https://arxiv.org/abs/2509.11096)
*Ruiying Li,Bin Pan,Qiaoying Qu,Xia Xu,Zhenwei Shi*

Main category: cs.CV

TL;DR: 提出了一种基于Copula理论的多时相高光谱混合分解新方法，有效建模了变量组分和动态丰度的时间依赖性，实验效果优于已有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多时相高光谱分解方法在时间依赖建模方面不足，难以捕捉材料动态演变规律，因此需要更好建模时序依赖的方法。Copula理论在显式建模依赖关系方面表现优异，激发了本文的研究。

Method: 提出了Copula引导的时间依赖方法（Cog-TD）：1）建立了新的数学模型，将Copula理论融入多时相混合分解问题的描述；2）构建Copula引导的框架，估计动态组分和丰度，并明确建模时间依赖关系；3）包含Copula函数估计与时间依赖指导两个关键模块，分别负责估计组分间的依赖与利用时间信息优化分解过程。此外，理论上证明了所估计的Copula函数及模型有效性。

Result: 在合成和真实多时相高光谱数据集上的实验结果显示，该方法能有效建模材料的动态变化，分解性能优于现有基线方法。

Conclusion: Cog-TD方法突破了现有方法时序依赖建模的瓶颈，为多时相高光谱混合分解提供了更为高效和理论可靠的解决思路，具有良好的实际应用价值。

Abstract: Multitemporal hyperspectral unmixing (MTHU) aims to model variable endmembers
and dynamical abundances, which emphasizes the critical temporal information.
However, existing methods have limitations in modeling temporal dependency,
thus fail to capture the dynamical material evolution. Motivated by the ability
of copula theory in modeling dependency structure explicitly, in this paper, we
propose a copula-guided temporal dependency method (Cog-TD) for multitemporal
hyperspectral unmixing. Cog-TD defines new mathematical model, constructs
copula-guided framework and provides two key modules with theoretical support.
The mathematical model provides explicit formulations for MTHU problem
definition, which describes temporal dependency structure by incorporating
copula theory. The copula-guided framework is constructed for utilizing copula
function, which estimates dynamical endmembers and abundances with temporal
dependency. The key modules consist of copula function estimation and temporal
dependency guidance, which computes and employs temporal information to guide
unmixing process. Moreover, the theoretical support demonstrates that estimated
copula function is valid and the represented temporal dependency exists in
hyperspectral images. The major contributions of this paper include redefining
MTHU problem with temporal dependency, proposing a copula-guided framework,
developing two key modules and providing theoretical support. Our experimental
results on both synthetic and real-world datasets demonstrate the utility of
the proposed method.

</details>


### [35] [3DAeroRelief: The first 3D Benchmark UAV Dataset for Post-Disaster Assessment](https://arxiv.org/abs/2509.11097)
*Nhut Le,Ehsan Karimi,Maryam Rahnemoonfar*

Main category: cs.CV

TL;DR: 本论文提出了3DAeroRelief，这是首个专为灾后评估设计的3D基准数据集，通过无人机在飓风灾区采集并重建高密度点云，为3D语义分割和灾害响应提供了重要资源。


<details>
  <summary>Details</summary>
Motivation: 大多数灾害评估工作依赖2D图像，存在视角遮挡、空间信息不足等缺陷。而现有3D数据集主要针对城市或室内场景，缺乏真实灾害环境的结构损伤数据。亟需适用于灾后场景的3D基准数据集，以提升灾害响应能力。

Method: 利用低成本无人机在飓风受灾区采集影像，通过结构光重建（Structure-from-Motion）和多视图立体（Multi-View Stereo）技术生成高密度3D点云。采用人工2D标注后投影至3D。并在该数据集上评测了多种主流3D分割模型，分析其实际应用表现。

Result: 数据集涵盖了真实、细粒度的灾后大型户外3D环境。实验展示了现有3D分割方法在灾后场景中的性能瓶颈与挑战，同时揭示了新的研究机遇。

Conclusion: 3DAeroRelief为后灾害3D计算机视觉研究提供了独特数据资源，有助于推动3D视觉系统在现实应急响应等实际场景下的发展。

Abstract: Timely assessment of structural damage is critical for disaster response and
recovery. However, most prior work in natural disaster analysis relies on 2D
imagery, which lacks depth, suffers from occlusions, and provides limited
spatial context. 3D semantic segmentation offers a richer alternative, but
existing 3D benchmarks focus mainly on urban or indoor scenes, with little
attention to disaster-affected areas. To address this gap, we present
3DAeroRelief--the first 3D benchmark dataset specifically designed for
post-disaster assessment. Collected using low-cost unmanned aerial vehicles
(UAVs) over hurricane-damaged regions, the dataset features dense 3D point
clouds reconstructed via Structure-from-Motion and Multi-View Stereo
techniques. Semantic annotations were produced through manual 2D labeling and
projected into 3D space. Unlike existing datasets, 3DAeroRelief captures 3D
large-scale outdoor environments with fine-grained structural damage in
real-world disaster contexts. UAVs enable affordable, flexible, and safe data
collection in hazardous areas, making them particularly well-suited for
emergency scenarios. To demonstrate the utility of 3DAeroRelief, we evaluate
several state-of-the-art 3D segmentation models on the dataset to highlight
both the challenges and opportunities of 3D scene understanding in disaster
response. Our dataset serves as a valuable resource for advancing robust 3D
vision systems in real-world applications for post-disaster scenarios.

</details>


### [36] [Filling the Gaps: A Multitask Hybrid Multiscale Generative Framework for Missing Modality in Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2509.11102)
*Nhi Kieu,Kien Nguyen,Arnold Wiliem,Clinton Fookes,Sridha Sridharan*

Main category: cs.CV

TL;DR: 本文针对多模态学习中缺失模态问题，提出了一种新型生成增强多模态网络（GEMMNet），在遥感语义分割任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实中由于传感器故障、恶劣天气等，导致多模态数据通常会出现缺失情况，这会严重影响多模态模型的性能。如何利用可用模态重建缺失模态，并提升模型在缺失情景下的鲁棒性，是目前亟需解决的问题。

Method: 提出了GEMMNet网络，包括三个创新模块：（1）混合特征提取（HyFEx），用于学习各模态的特定特征；（2）具备多尺度感知的混合融合（HyFMA），提升多模态协同下的语义建模能力；（3）互补损失（CoLoss），减少模型对主导模态的依赖，增强模态间与任务间的一致性，从而提升鲁棒性。

Result: 在遥感语义分割公开数据集（Vaihingen和Potsdam）上，GEMMNet分别超越了经典生成模型（AE、cGAN）及先进的非生成方法（mmformer、shaspec），达到了更优性能。

Conclusion: GEMMNet有效缓解了多模态缺失对遥感语义分割的影响，提升了模型的泛化性和鲁棒性，对缺失模态场景下的多模态学习具有实际应用价值。

Abstract: Multimodal learning has shown significant performance boost compared to
ordinary unimodal models across various domains. However, in real-world
scenarios, multimodal signals are susceptible to missing because of sensor
failures and adverse weather conditions, which drastically deteriorates models'
operation and performance. Generative models such as AutoEncoder (AE) and
Generative Adversarial Network (GAN) are intuitive solutions aiming to
reconstruct missing modality from available ones. Yet, their efficacy in remote
sensing semantic segmentation remains underexplored. In this paper, we first
examine the limitations of existing generative approaches in handling the
heterogeneity of multimodal remote sensing data. They inadequately capture
semantic context in complex scenes with large intra-class and small inter-class
variation. In addition, traditional generative models are susceptible to heavy
dependence on the dominant modality, introducing bias that affects model
robustness under missing modality conditions. To tackle these limitations, we
propose a novel Generative-Enhanced MultiModal learning Network (GEMMNet) with
three key components: (1) Hybrid Feature Extractor (HyFEx) to effectively learn
modality-specific representations, (2) Hybrid Fusion with Multiscale Awareness
(HyFMA) to capture modality-synergistic semantic context across scales and (3)
Complementary Loss (CoLoss) scheme to alleviate the inherent bias by
encouraging consistency across modalities and tasks. Our method, GEMMNet,
outperforms both generative baselines AE, cGAN (conditional GAN), and
state-of-the-art non-generative approaches - mmformer and shaspec - on two
challenging semantic segmentation remote sensing datasets (Vaihingen and
Potsdam). Source code is made available.

</details>


### [37] [WildSmoke: Ready-to-Use Dynamic 3D Smoke Assets from a Single Video in the Wild](https://arxiv.org/abs/2509.11114)
*Yuqiu Liu,Jialin Song,Manolis Savva,Wuyang Chen*

Main category: cs.CV

TL;DR: 论文提出了一套流程，可从单个真实世界视频中提取并重建动态3D烟雾，还可实现烟雾的交互式编辑和仿真，突破现有环境受限的难题，并获得更高的重建质量。


<details>
  <summary>Details</summary>
Motivation: 以往3D流体重建方法多依赖受控实验环境，难以处理现实世界视频，而现实视频存在更大的复杂性与挑战，当前对此关注不足。

Method: 设计了针对真实视频中烟雾重建的三个新技术：烟雾与背景分离、烟雾粒子与相机位姿初始化、多视角推断。同时支持重建后烟雾的仿真和交互式编辑。

Result: 在真实世界视频上表现优于之前方法，平均PSNR提升2.22，实现高质量的烟雾重建，并可以对流体动态进行多样化、逼真的编辑。

Conclusion: 该方法有效扩展了烟雾等流体的3D重建能力到真实场景，并首次支持重建后烟雾的灵活编辑，对流体仿真和影视特效制作等有重要价值。

Abstract: We propose a pipeline to extract and reconstruct dynamic 3D smoke assets from
a single in-the-wild video, and further integrate interactive simulation for
smoke design and editing. Recent developments in 3D vision have significantly
improved reconstructing and rendering fluid dynamics, supporting realistic and
temporally consistent view synthesis. However, current fluid reconstructions
rely heavily on carefully controlled clean lab environments, whereas real-world
videos captured in the wild are largely underexplored. We pinpoint three key
challenges of reconstructing smoke in real-world videos and design targeted
techniques, including smoke extraction with background removal, initialization
of smoke particles and camera poses, and inferring multi-view videos. Our
method not only outperforms previous reconstruction and generation methods with
high-quality smoke reconstructions (+2.22 average PSNR on wild videos), but
also enables diverse and realistic editing of fluid dynamics by simulating our
smoke assets. We provide our models, data, and 4D smoke assets at
[https://autumnyq.github.io/WildSmoke](https://autumnyq.github.io/WildSmoke).

</details>


### [38] [SVR-GS: Spatially Variant Regularization for Probabilistic Masks in 3D Gaussian Splatting](https://arxiv.org/abs/2509.11116)
*Ashkan Taghipour,Vahid Naghshin,Benjamin Southwell,Farid Boussaid,Hamid Laga,Mohammed Bennamoun*

Main category: cs.CV

TL;DR: 本文提出SVR-GS方法，通过空间可变正则化更有效地对3D Gaussian Splatting中的高斯数量进行稀疏化，显著减少了高斯数量，仅略微降低重建质量，为实时应用带来更快、更小、更高效的模型。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS中的高斯数量优化方法采用全局掩码正则化，与每个射线重建质量的损失不一致，导致稀疏化位置不精准，模型冗余度高，限制实际应用。

Method: 提出一种空间可变正则（SVR-GS），根据各高斯概率分布在每条射线上的实际贡献生成每像素掩码，对“重要性”低的高斯更强的稀疏化压力。探索三种掩码聚合方式，并通过梯度分析优化设计，全部基于CUDA实现。

Result: 在Tanks&Temples、Deep Blending和Mip-NeRF360三套数据集上，SVR-GS平均可比MaskGS多减少1.79倍高斯，比3DGS多减少5.63倍高斯，PSNR仅分别少0.50 dB和0.40 dB，模型更小、更快、更省内存。

Conclusion: SVR-GS实现了3DGS模型的高效稀疏化，在保证新视角合成质量的同时大幅度降低计算与存储代价，非常适合机器人、AR/VR及移动感知等实时场景应用。

Abstract: 3D Gaussian Splatting (3DGS) enables fast, high-quality novel view synthesis
but typically relies on densification followed by pruning to optimize the
number of Gaussians. Existing mask-based pruning, such as MaskGS, regularizes
the global mean of the mask, which is misaligned with the local per-pixel
(per-ray) reconstruction loss that determines image quality along individual
camera rays. This paper introduces SVR-GS, a spatially variant regularizer that
renders a per-pixel spatial mask from each Gaussian's effective contribution
along the ray, thereby applying sparsity pressure where it matters: on
low-importance Gaussians. We explore three spatial-mask aggregation strategies,
implement them in CUDA, and conduct a gradient analysis to motivate our final
design. Extensive experiments on Tanks\&Temples, Deep Blending, and Mip-NeRF360
datasets demonstrate that, on average across the three datasets, the proposed
SVR-GS reduces the number of Gaussians by 1.79\(\times\) compared to MaskGS and
5.63\(\times\) compared to 3DGS, while incurring only 0.50 dB and 0.40 dB PSNR
drops, respectively. These gains translate into significantly smaller, faster,
and more memory-efficient models, making them well-suited for real-time
applications such as robotics, AR/VR, and mobile perception.

</details>


### [39] [No Mesh, No Problem: Estimating Coral Volume and Surface from Sparse Multi-View Images](https://arxiv.org/abs/2509.11164)
*Diego Eustachio Farchione,Ramzi Idoughi,Peter Wonka*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖且轻量的深度学习方法，用于通过多视角2D图像预测复杂珊瑚的三维体积和表面积，实现高效、可扩展的珊瑚生长监测。


<details>
  <summary>Details</summary>
Motivation: 珊瑚因形态复杂，难以通过传统方法准确测量体积和表面积，而这些数据对于有效的珊瑚礁监测和生态管理至关重要。作者希望开发一种既高效又能推广的新工具来解决这一难题。

Method: 方法利用基于VGGT的预训练模块从多视角RGB图像提取点云，并融合每个视角的置信度分数，统一为高质量点云后输入双路DGCNN解码器，分别输出体积和表面积及其置信估计。为提升稳定性和不确定性评估，引入基于高斯负对数似然的复合损失函数（实域和对数域）。

Result: 方法在体积和表面积预测方面取得了与现有方法相媲美的准确率，并且对陌生形态具有良好的泛化能力。

Conclusion: 该框架实现了基于少量多视角图像的高效、可扩展珊瑚3D几何特征估算，将为珊瑚生长分析和珊瑚礁监测提供有力工具。

Abstract: Effective reef monitoring requires the quantification of coral growth via
accurate volumetric and surface area estimates, which is a challenging task due
to the complex morphology of corals. We propose a novel, lightweight, and
scalable learning framework that addresses this challenge by predicting the 3D
volume and surface area of coral-like objects from 2D multi-view RGB images.
Our approach utilizes a pre-trained module (VGGT) to extract dense point maps
from each view; these maps are merged into a unified point cloud and enriched
with per-view confidence scores. The resulting cloud is fed to two parallel
DGCNN decoder heads, which jointly output the volume and the surface area of
the coral, as well as their corresponding confidence estimate. To enhance
prediction stability and provide uncertainty estimates, we introduce a
composite loss function based on Gaussian negative log-likelihood in both real
and log domains. Our method achieves competitive accuracy and generalizes well
to unseen morphologies. This framework paves the way for efficient and scalable
coral geometry estimation directly from a sparse set of images, with potential
applications in coral growth analysis and reef monitoring.

</details>


### [40] [Traffic-MLLM: A Spatio-Temporal MLLM with Retrieval-Augmented Generation for Causal Inference in Traffic](https://arxiv.org/abs/2509.11165)
*Waikit Xiu,Qiang Lu,Xiying Li,Chen Hu,Shengbo Sun*

Main category: cs.CV

TL;DR: 提出了Traffic-MLLM，一个面向交通视频精细化分析的多模态大语言模型，在TrafficQA和DriveQA数据集上取得了SOTA表现，表现出强大的零样本推理和跨场景泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有交通视频理解方法在时空因果建模和领域知识融合方面存在不足，对复杂交通场景的理解有限。因此亟需更有效的模型提高场景感知和因果理解能力。

Method: 构建了以Qwen2.5-VL为基础的Traffic-MLLM，采用高质量交通专属多模态数据集及LoRA轻量微调，提升时空特征建模能力。同时引入结合了CoT推理与RAG的知识提示模块，实现对交通法规和领域知识的精确注入。

Result: 在TrafficQA和DriveQA等基准数据集上，Traffic-MLLM取得了最优效果，在多模态交通数据处理中表现出色。并展现了强劲的零样本推理能力及跨场景泛化能力。

Conclusion: Traffic-MLLM针对交通视频理解的具体挑战，通过多模态预训练与知识提示实现了性能与泛化能力的大幅提升，为复杂交通场景下的因果性与知识感知分析提供了有力工具。

Abstract: As intelligent transportation systems advance, traffic video understanding
plays an increasingly pivotal role in comprehensive scene perception and causal
analysis. Yet, existing approaches face notable challenges in accurately
modeling spatiotemporal causality and integrating domain-specific knowledge,
limiting their effectiveness in complex scenarios. To address these
limitations, we propose Traffic-MLLM, a multimodal large language model
tailored for fine-grained traffic analysis. Built on the Qwen2.5-VL backbone,
our model leverages high-quality traffic-specific multimodal datasets and uses
Low-Rank Adaptation (LoRA) for lightweight fine-tuning, significantly enhancing
its capacity to model continuous spatiotemporal features in video sequences.
Furthermore, we introduce an innovative knowledge prompting module fusing
Chain-of-Thought (CoT) reasoning with Retrieval-Augmented Generation (RAG),
enabling precise injection of detailed traffic regulations and domain knowledge
into the inference process. This design markedly boosts the model's logical
reasoning and knowledge adaptation capabilities. Experimental results on
TrafficQA and DriveQA benchmarks show Traffic-MLLM achieves state-of-the-art
performance, validating its superior ability to process multimodal traffic
data. It also exhibits remarkable zero-shot reasoning and cross-scenario
generalization capabilities.

</details>


### [41] [Multispectral-NeRF:a multispectral modeling approach based on neural radiance fields](https://arxiv.org/abs/2509.11169)
*Hong Zhang,Fei Guo,Zihan Xie,Dizhao Yao*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于NeRF的多光谱三维重建方法——Multispectral-NeRF，能够有效融合和利用多光谱信息，实现更高精度和更高质量的三维重建。


<details>
  <summary>Details</summary>
Motivation: 传统2D图像三维重建依赖RGB信息，已有多光谱三维重建方法价格高、精度低、几何特征差。现有的NeRF等方法只能处理三通道，无法有效整合多光谱数据，限制了在实际应用中的广泛性和效果。

Method: 提出Multispectral-NeRF神经网络架构，创新点包括：1）扩展隐藏层以适应6通道输入；2）重新设计残差函数优化光谱差异计算；3）调整数据压缩模块以应对多光谱高比特需求。

Result: 实验结果表明，Multispectral-NeRF能够有效处理多通道光谱特征，并在重建过程中准确保留原场景的光谱特性，达到高质量的三维重建效果。

Conclusion: Multispectral-NeRF有效弥补了现有三维重建方法无法处理多光谱数据的不足，对于多光谱高精度三维重建应用具有显著提升和实际应用前景。

Abstract: 3D reconstruction technology generates three-dimensional representations of
real-world objects, scenes, or environments using sensor data such as 2D
images, with extensive applications in robotics, autonomous vehicles, and
virtual reality systems. Traditional 3D reconstruction techniques based on 2D
images typically relies on RGB spectral information. With advances in sensor
technology, additional spectral bands beyond RGB have been increasingly
incorporated into 3D reconstruction workflows. Existing methods that integrate
these expanded spectral data often suffer from expensive scheme prices, low
accuracy and poor geometric features. Three - dimensional reconstruction based
on NeRF can effectively address the various issues in current multispectral 3D
reconstruction methods, producing high - precision and high - quality
reconstruction results. However, currently, NeRF and some improved models such
as NeRFacto are trained on three - band data and cannot take into account the
multi - band information. To address this problem, we propose
Multispectral-NeRF, an enhanced neural architecture derived from NeRF that can
effectively integrates multispectral information. Our technical contributions
comprise threefold modifications: Expanding hidden layer dimensionality to
accommodate 6-band spectral inputs; Redesigning residual functions to optimize
spectral discrepancy calculations between reconstructed and reference images;
Adapting data compression modules to address the increased bit-depth
requirements of multispectral imagery. Experimental results confirm that
Multispectral-NeRF successfully processes multi-band spectral features while
accurately preserving the original scenes' spectral characteristics.

</details>


### [42] [SPHERE: Semantic-PHysical Engaged REpresentation for 3D Semantic Scene Completion](https://arxiv.org/abs/2509.11171)
*Zhiwen Yang,Yuxin Peng*

Main category: cs.CV

TL;DR: 本文提出了一种新的表示方法SPHERE，结合体素和高斯两种表示，以提升相机视觉下3D语义场景补全任务（SSC）在细致几何和语义精度上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有体素或平面为主的SSC方法在刻画真实物理几何细节上存在不足，而NeRF和3DGS等神经重建方法尽管物理建模更强，但计算开销大、收敛慢，导致在自动驾驶大场景下语义精度下降。为改善这一困境，迫切需要融合同步高效的表达方法。

Method: 提出SPHERE框架，融合体素与高斯表示。核心包括两个模块：1）语义引导的高斯初始化（SGI），用双分支3D场景先定位关键体素作为锚点，引导高斯初始化提升效率；2）物理感知谐波增强（PHE），引入语义球谐，建模物理上下文并促进语义-几何一致，从而细化场景输出。

Result: 在SemanticKITTI和SSCBench-KITTI-360公开基准上进行大量实验和分析，表明SPHERE框架可在保持语义-物理细节的同时，取得更优的SSC效果，优于现有方法。

Conclusion: SPHERE通过创新的体素与高斯融合、语义-物理协同机制，有效提升了相机视觉SSC任务的几何与语义表达能力，在多个标准数据集上表现突出。代码已公开，便于后续研究与应用。

Abstract: Camera-based 3D Semantic Scene Completion (SSC) is a critical task in
autonomous driving systems, assessing voxel-level geometry and semantics for
holistic scene perception. While existing voxel-based and plane-based SSC
methods have achieved considerable progress, they struggle to capture physical
regularities for realistic geometric details. On the other hand, neural
reconstruction methods like NeRF and 3DGS demonstrate superior physical
awareness, but suffer from high computational cost and slow convergence when
handling large-scale, complex autonomous driving scenes, leading to inferior
semantic accuracy. To address these issues, we propose the Semantic-PHysical
Engaged REpresentation (SPHERE) for camera-based SSC, which integrates voxel
and Gaussian representations for joint exploitation of semantic and physical
information. First, the Semantic-guided Gaussian Initialization (SGI) module
leverages dual-branch 3D scene representations to locate focal voxels as
anchors to guide efficient Gaussian initialization. Then, the Physical-aware
Harmonics Enhancement (PHE) module incorporates semantic spherical harmonics to
model physical-aware contextual details and promote semantic-geometry
consistency through focal distribution alignment, generating SSC results with
realistic details. Extensive experiments and analyses on the popular
SemanticKITTI and SSCBench-KITTI-360 benchmarks validate the effectiveness of
SPHERE. The code is available at
https://github.com/PKU-ICST-MIPL/SPHERE_ACMMM2025.

</details>


### [43] [StegOT: Trade-offs in Steganography via Optimal Transport](https://arxiv.org/abs/2509.11178)
*Chengde Lin,Xuezhu Gong,Shuxue Ding,Mingzhe Yang,Xijun Lu,Chengjun Mo*

Main category: cs.CV

TL;DR: 本论文提出了一种基于自编码器和最优传输理论的新型图像隐写方法StegOT，通过引入多通道最优传输（MCOT）模块，有效缓解了以往GAN和VAE模型中常见的模式崩溃问题，实现了载体图像和秘密图像间信息的均衡，并提升了隐写图像和还原图像的质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于GAN和VAE的图像隐写模型普遍存在模式崩溃问题，导致秘密信息在载体图像中的分布不均衡，影响秘密图像的提取和图像质量。

Method: 作者提出了StegOT模型，采用自编码器架构，并结合最优传输理论，引入多通道最优传输（MCOT）模块，将多峰分布的特征转化为单峰分布，从而在保证信息隐藏能力的同时，实现信息在载体和秘密图像间的合理分配。

Result: 实验结果表明，StegOT在实现载体和秘密图像信息权衡方面表现突出，同时显著提升了隐写图像及还原秘密图像的视觉质量。

Conclusion: StegOT模型通过创新性的MCOT模块，有效应对了模式崩溃导致的信息不均问题，为图像隐写领域提供了更优的解决方案，具有较高的应用潜力。

Abstract: Image hiding is often referred to as steganography, which aims to hide a
secret image in a cover image of the same resolution. Many steganography models
are based on genera-tive adversarial networks (GANs) and variational
autoencoders (VAEs). However, most existing models suffer from mode collapse.
Mode collapse will lead to an information imbalance between the cover and
secret images in the stego image and further affect the subsequent extraction.
To address these challenges, this paper proposes StegOT, an autoencoder-based
steganography model incorporating optimal transport theory. We designed the
multiple channel optimal transport (MCOT) module to transform the feature
distribution, which exhibits multiple peaks, into a single peak to achieve the
trade-off of information. Experiments demonstrate that we not only achieve a
trade-off between the cover and secret images but also enhance the quality of
both the stego and recovery images. The source code will be released on
https://github.com/Rss1124/StegOT.

</details>


### [44] [The Impact of Skin Tone Label Granularity on the Performance and Fairness of AI Based Dermatology Image Classification Models](https://arxiv.org/abs/2509.11184)
*Partha Shah,Durva Sankhe,Maariyah Rashid,Zakaa Khaled,Esther Puyol-Antón,Tiarna Lee,Maram Alqarni,Sweta Rai,Andrew P. King*

Main category: cs.CV

TL;DR: 该论文研究了使用Fitzpatrick皮肤色调(FST)量表的不同粒度对AI皮肤病变分类模型表现和偏见的影响。结果发现，FST粒度的设置对模型表现有显著影响，建议寻找更好替代FST量表的方法。


<details>
  <summary>Details</summary>
Motivation: AI自动分类皮肤病变在皮肤色调不同人群中表现存在偏差。当前使用最广泛的皮肤色调分类方式FST量表被批评对浅色皮肤划分更细，深色皮肤不够精准，可能加重AI模型的偏见。因此，研究FST尺度粒度对AI模型的影响，并探索更合适的皮肤色调分类方法势在必行。

Method: 通过采集基于不同FST粒度（如1/2、3/4、5/6组和1/2/3/4组）的数据，训练多个AI模型进行良性与恶性病变分类，并对比FST专属数据模型与FST均衡数据模型的性能表现和偏见程度。

Result: （i）基于三组划分（FST 1/2、3/4、5/6）训练的FST专属模型整体优于FST均衡数据模型；（ii）降低FST分组粒度（变为1/2/3/4）会导致模型表现下降。由此说明FST分组的粒度设置对模型性能有明显影响。

Conclusion: FST分组粒度在训练皮肤病变分类AI模型时非常重要。鉴于FST量表自身分组有偏、代表性不足，研究建议AI公平性研究中应逐步放弃FST量表，转向能更好表达人类真实皮肤色调多样性的替代表。

Abstract: Artificial intelligence (AI) models to automatically classify skin lesions
from dermatology images have shown promising performance but also
susceptibility to bias by skin tone. The most common way of representing skin
tone information is the Fitzpatrick Skin Tone (FST) scale. The FST scale has
been criticised for having greater granularity in its skin tone categories for
lighter-skinned subjects. This paper conducts an investigation of the impact
(on performance and bias) on AI classification models of granularity in the FST
scale. By training multiple AI models to classify benign vs. malignant lesions
using FST-specific data of differing granularity, we show that: (i) when
training models using FST-specific data based on three groups (FST 1/2, 3/4 and
5/6), performance is generally better for models trained on FST-specific data
compared to a general model trained on FST-balanced data; (ii) reducing the
granularity of FST scale information (from 1/2 and 3/4 to 1/2/3/4) can have a
detrimental effect on performance. Our results highlight the importance of the
granularity of FST groups when training lesion classification models. Given the
question marks over possible human biases in the choice of categories in the
FST scale, this paper provides evidence for a move away from the FST scale in
fair AI research and a transition to an alternative scale that better
represents the diversity of human skin tones.

</details>


### [45] [Scaling Up Forest Vision with Synthetic Data](https://arxiv.org/abs/2509.11201)
*Yihang She,Andrew Blake,David Coomes,Srinivasan Keshav*

Main category: cs.CV

TL;DR: 本文提出了一种基于合成数据的新方法，通过结合游戏引擎与基于物理的LiDAR仿真，生成大规模、多样化、标注完善的3D森林激光点云数据集，用于提升树木分割算法的性能，极大减少真实数据标注需求、降低成本。


<details>
  <summary>Details</summary>
Motivation: 精准的树木分割对于碳循环等生态系统研究至关重要，而现有的公开3D森林数据集规模较小，难以支持鲁棒的分割系统开发。该研究受到自动驾驶领域合成数据应用成功的启发，探索类似策略能否助推森林点云分割。

Method: 开发了一个新的合成数据生成流程，集成了游戏引擎和基于物理的LiDAR模拟技术，用生成的大规模多样化森林点云数据预训练分割模型，仅需极少量真实数据标注进行微调。

Result: 实验表明，使用该合成数据预训练的树木分割模型，在仅仅利用一个小于0.1公顷的真实样地微调后，其分割效果可媲美完全依赖大规模真实数据训练的模型。同时，论文分析了成功利用合成数据的关键因素，包括物理真实感、多样性和合成数据规模。

Conclusion: 基于合成数据的预训练方法可极大降低3D森林分割任务对真实标注数据的依赖，有助于实现更高鲁棒性的3D森林视觉系统。相关数据生成管线和数据集已公开，为后续研究提供便利。

Abstract: Accurate tree segmentation is a key step in extracting individual tree
metrics from forest laser scans, and is essential to understanding ecosystem
functions in carbon cycling and beyond. Over the past decade, tree segmentation
algorithms have advanced rapidly due to developments in AI. However existing,
public, 3D forest datasets are not large enough to build robust tree
segmentation systems. Motivated by the success of synthetic data in other
domains such as self-driving, we investigate whether similar approaches can
help with tree segmentation. In place of expensive field data collection and
annotation, we use synthetic data during pretraining, and then require only
minimal, real forest plot annotation for fine-tuning.
  We have developed a new synthetic data generation pipeline to do this for
forest vision tasks, integrating advances in game-engines with physics-based
LiDAR simulation. As a result, we have produced a comprehensive, diverse,
annotated 3D forest dataset on an unprecedented scale. Extensive experiments
with a state-of-the-art tree segmentation algorithm and a popular real dataset
show that our synthetic data can substantially reduce the need for labelled
real data. After fine-tuning on just a single, real, forest plot of less than
0.1 hectare, the pretrained model achieves segmentations that are competitive
with a model trained on the full scale real data. We have also identified
critical factors for successful use of synthetic data: physics, diversity, and
scale, paving the way for more robust 3D forest vision systems in the future.
Our data generation pipeline and the resulting dataset are available at
https://github.com/yihshe/CAMP3D.git.

</details>


### [46] [Beyond Sliders: Mastering the Art of Diffusion-based Image Manipulation](https://arxiv.org/abs/2509.11213)
*Yufei Tang,Daiheng Gao,Pingyu Wu,Wenbo Zhou,Bang Zhang,Weiming Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Beyond Sliders的新框架，通过结合GAN和扩散模型，实现对各种图像类别的精细化编辑，尤其提升了现实世界非AIGC图片的可控性与真实性。


<details>
  <summary>Details</summary>
Motivation: 当前图像生成领域对真实感和可定制化的需求日益提升，现有的concept sliders方法在处理真实世界拍摄的非AIGC图像时表现不佳，需要新的技术填补这一空白。

Method: 该方法融合了GAN和扩散模型，通过文本和视觉的精细化引导，并在对抗性训练下优化模型，实现细粒度的图像操控和高质量生成。

Result: 实验结果显示，Beyond Sliders在多个应用领域展现出强大的稳健性和多样性，显著提升了图像质量和真实感。

Conclusion: Beyond Sliders框架有效突破了现有方法的局限，为不同图像类别带来了更高水平的可编辑性与生成质量，有望广泛应用于图像生成与编辑领域。

Abstract: In the realm of image generation, the quest for realism and customization has
never been more pressing. While existing methods like concept sliders have made
strides, they often falter when it comes to no-AIGC images, particularly images
captured in real world settings. To bridge this gap, we introduce Beyond
Sliders, an innovative framework that integrates GANs and diffusion models to
facilitate sophisticated image manipulation across diverse image categories.
Improved upon concept sliders, our method refines the image through fine
grained guidance both textual and visual in an adversarial manner, leading to a
marked enhancement in image quality and realism. Extensive experimental
validation confirms the robustness and versatility of Beyond Sliders across a
spectrum of applications.

</details>


### [47] [Geometrically Constrained and Token-Based Probabilistic Spatial Transformers](https://arxiv.org/abs/2509.11218)
*Johann Schmidt,Sebastian Stober*

Main category: cs.CV

TL;DR: 本文提出了一种针对Transformer视觉模型的空间变换网络（STN）概率性组件分解扩展，使模型在几何扰动下更鲁棒，尤其在细粒度视觉分类任务（如蛾类识别）上表现优异。


<details>
  <summary>Details</summary>
Motivation: 细粒度视觉分类任务对物体的几何变化（如旋转、缩放、透视等）非常敏感。现有的等变网络虽然能应对这些问题，但计算开销大且结构受限。作者希望通过提升STN能力，为主流Transformer视觉架构提供一个灵活、通用且高效的几何规范化方法。

Method: 作者将传统STN的仿射变换分解为旋转、缩放和剪切三个分量，并为每个分量设计了带高斯变分后验分布的回归器，由共享的定位编码器输出参数，引入采样式规范化推理。同时，提出基于数据增强参数的新型分量对齐损失，辅助网络更精准地对齐空间结构。

Result: 在具挑战性的蛾类细粒度分类数据集上，作者的方法较现有STN及变体在鲁棒性等指标上均有显著提升。

Conclusion: 通过对STN进行概率化、分组件处理，所提出的方法兼顾了精度、推理灵活性和架构通用性，是面向主流Transformer视觉模型应对几何扰动的有效方案，特别适合细粒度识别任务。

Abstract: Fine-grained visual classification (FGVC) remains highly sensitive to
geometric variability, where objects appear under arbitrary orientations,
scales, and perspective distortions. While equivariant architectures address
this issue, they typically require substantial computational resources and
restrict the hypothesis space. We revisit Spatial Transformer Networks (STNs)
as a canonicalization tool for transformer-based vision pipelines, emphasizing
their flexibility, backbone-agnostic nature, and lack of architectural
constraints. We propose a probabilistic, component-wise extension that improves
robustness. Specifically, we decompose affine transformations into rotation,
scaling, and shearing, and regress each component under geometric constraints
using a shared localization encoder. To capture uncertainty, we model each
component with a Gaussian variational posterior and perform sampling-based
canonicalization during inference.A novel component-wise alignment loss
leverages augmentation parameters to guide spatial alignment. Experiments on
challenging moth classification benchmarks demonstrate that our method
consistently improves robustness compared to other STNs.

</details>


### [48] [CCoMAML: Efficient Cattle Identification Using Cooperative Model-Agnostic Meta-Learning](https://arxiv.org/abs/2509.11219)
*Rabin Dulal,Lihong Zheng,Ashad Kabir*

Main category: cs.CV

TL;DR: 本论文提出了一种基于少样本学习的新方法，大幅提升了实际牧场牛只身份识别的准确性和适应性，比传统和现有深度学习方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有牛只识别多依赖RFID耳标，该方式易受丢失、损坏、篡改等问题影响，且存在安全隐患。生物特征识别（如鼻纹）虽有前景，但深度学习模型在数据有限和牧群动态变化下很难直接应用，需要频繁重训练，应用受限。因此，需要更灵活且高效的新方法。

Method: 作者提出结合协作式模型无关元学习（CCoMAML）与多头注意力特征融合（MHAFF）的少样本学习框架。该框架能在数据样本极少甚至新人群变化时，仍可快速适应，无需大规模重训练。MHAFF可有效提取和整合鼻纹特征，CCoMAML提升模型泛化能力。

Result: 经过与现有主流少样本学习方案对比，实验显示该方法在牛只识别任务中取得了98.46%和97.91%的F1分数，显著优于其他方法。

Conclusion: 该方法为实际牧场牛只生物识别系统提供了高效、鲁棒且适应性强的新途径，具备快速推广和应用价值。

Abstract: Cattle identification is critical for efficient livestock farming management,
currently reliant on radio-frequency identification (RFID) ear tags. However,
RFID-based systems are prone to failure due to loss, damage, tampering, and
vulnerability to external attacks. As a robust alternative, biometric
identification using cattle muzzle patterns similar to human fingerprints has
emerged as a promising solution. Deep learning techniques have demonstrated
success in leveraging these unique patterns for accurate identification. But
deep learning models face significant challenges, including limited data
availability, disruptions during data collection, and dynamic herd compositions
that require frequent model retraining. To address these limitations, this
paper proposes a novel few-shot learning framework for real-time cattle
identification using Cooperative Model-Agnostic Meta-Learning (CCoMAML) with
Multi-Head Attention Feature Fusion (MHAFF) as a feature extractor model. This
model offers great model adaptability to new data through efficient learning
from few data samples without retraining. The proposed approach has been
rigorously evaluated against current state-of-the-art few-shot learning
techniques applied in cattle identification. Comprehensive experimental results
demonstrate that our proposed CCoMAML with MHAFF has superior cattle
identification performance with 98.46% and 97.91% F1 scores.

</details>


### [49] [ANROT-HELANet: Adverserially and Naturally Robust Attention-Based Aggregation Network via The Hellinger Distance for Few-Shot Classification](https://arxiv.org/abs/2509.11220)
*Gao Yu Lee,Tanmoy Dam,Md Meftahul Ferdaus,Daniel Puiu Poenar,Vu N. Duong*

Main category: cs.CV

TL;DR: 本文提出了一种针对小样本学习（FSL）的新方法ANROT-HELANet，通过引入Hellinger距离聚合与对抗鲁棒机制，在性能和鲁棒性上均超越了现有方法，特别是在面对对抗性扰动和自然噪声时表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有基于贝叶斯和KL散度的小样本学习方法虽有改进，但对对抗攻击和自然噪声鲁棒性不足。作者希望设计一种具有更强鲁棒性的小样本学习方法，提升在实际复杂环境下的应用价值。

Method: 提出ANROT-HELANet网络，结合Hellinger距离特征聚合机制，引入对抗与自然鲁棒训练框架，以及一种新颖的Hellinger相似性对比损失函数，用于提升在变分小样本推断场景下的特征判别能力。

Result: 在miniImageNet等基准数据集上，ANROT-HELANet在1-shot和5-shot小样本分类任务分别提升了1.2%和1.4%；在对抗扰动至ε=0.30和高斯噪声σ=0.30下依然表现出较强鲁棒性。图像重建的FID分数显著优于传统VAE和WAE。

Conclusion: ANROT-HELANet通过独特的Hellinger距离特征聚合、注意力机制及新型损失函数，不仅在小样本学习上刷新了SOTA表现，也有效提升了模型对于对抗攻击和自然扰动的鲁棒性。

Abstract: Few-Shot Learning (FSL), which involves learning to generalize using only a
few data samples, has demonstrated promising and superior performances to
ordinary CNN methods. While Bayesian based estimation approaches using
Kullback-Leibler (KL) divergence have shown improvements, they remain
vulnerable to adversarial attacks and natural noises. We introduce
ANROT-HELANet, an Adversarially and Naturally RObusT Hellinger Aggregation
Network that significantly advances the state-of-the-art in FSL robustness and
performance. Our approach implements an adversarially and naturally robust
Hellinger distance-based feature class aggregation scheme, demonstrating
resilience to adversarial perturbations up to $\epsilon=0.30$ and Gaussian
noise up to $\sigma=0.30$. The network achieves substantial improvements across
benchmark datasets, including gains of 1.20\% and 1.40\% for 1-shot and 5-shot
scenarios on miniImageNet respectively. We introduce a novel Hellinger
Similarity contrastive loss function that generalizes cosine similarity
contrastive loss for variational few-shot inference scenarios. Our approach
also achieves superior image reconstruction quality with a FID score of 2.75,
outperforming traditional VAE (3.43) and WAE (3.38) approaches. Extensive
experiments conducted on four few-shot benchmarked datasets verify that
ANROT-HELANet's combination of Hellinger distance-based feature aggregation,
attention mechanisms, and our novel loss function establishes new
state-of-the-art performance while maintaining robustness against both
adversarial and natural perturbations. Our code repository will be available at
https://github.com/GreedYLearner1146/ANROT-HELANet/tree/main.

</details>


### [50] [MIS-LSTM: Multichannel Image-Sequence LSTM for Sleep Quality and Stress Prediction](https://arxiv.org/abs/2509.11232)
*Seongwan Park,Jieun Woo,Siheon Yang*

Main category: cs.CV

TL;DR: 提出了MIS-LSTM混合模型，将CNN和LSTM结合，用于多模态lifelog数据的日级睡眠质量与压力预测，取得了比传统基线更好的性能。


<details>
  <summary>Details</summary>
Motivation: 准确预测个人日常睡眠质量和压力水平，对于健康监测和干预有重要价值。现有方法在处理连续与离散多模态传感器数据时受限，因此作者提出新的深度学习框架以提升预测效果。

Method: 连续传感器数据被分割为N小时块并转化为多通道图像，离散事件用1D-CNN单独编码，通过卷积注意力模块融合两种模态，LSTM捕捉长时序依赖，并引入不确定性感知集成机制（UALRE）增强鲁棒性。

Result: 在2025 ETRI Lifelog Challenge数据集上，MIS-LSTM基线Macro-F1为0.615，加入UALRE集成后提升至0.647，优于LSTM、1D-CNN与CNN等主流方法。消融实验证实不同设计选择的益处。

Conclusion: MIS-LSTM及其UALRE集成显著提升了多模态lifelog数据的睡眠与压力预测准确率，验证了各模块设计的有效性。

Abstract: This paper presents MIS-LSTM, a hybrid framework that joins CNN encoders with
an LSTM sequence model for sleep quality and stress prediction at the day level
from multimodal lifelog data. Continuous sensor streams are first partitioned
into N-hour blocks and rendered as multi-channel images, while sparse discrete
events are encoded with a dedicated 1D-CNN. A Convolutional Block Attention
Module fuses the two modalities into refined block embeddings, which an LSTM
then aggregates to capture long-range temporal dependencies. To further boost
robustness, we introduce UALRE, an uncertainty-aware ensemble that overrides
lowconfidence majority votes with high-confidence individual predictions.
Experiments on the 2025 ETRI Lifelog Challenge dataset show that Our base
MISLSTM achieves Macro-F1 0.615; with the UALRE ensemble, the score improves to
0.647, outperforming strong LSTM, 1D-CNN, and CNN baselines. Ablations confirm
(i) the superiority of multi-channel over stacked-vertical imaging, (ii) the
benefit of a 4-hour block granularity, and (iii) the efficacy of
modality-specific discrete encoding.

</details>


### [51] [Contextualized Multimodal Lifelong Person Re-Identification in Hybrid Clothing States](https://arxiv.org/abs/2509.11247)
*Robert Long,Rongxin Jiang,Mingrui Yan*

Main category: cs.CV

TL;DR: 本文提出了一个处理行人再识别中衣着变化与持续学习挑战的混合任务（LReID-Hybrid），并提出了CMLReID框架，能够同时处理同衣（SC）和换衣（CC）场景，并在多个数据集上超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实监控环境下，行人由于换衣、环境变化，需要再识别系统具有更强的泛化能力和持续学习能力，而现有方法多只关注单一场景或任务，难以兼顾复杂实际需求。

Method: 提出LReID-Hybrid任务，要求模型能持续学习同时应对同衣/换衣场景。具体采用基于CLIP的CMLReID 框架，包含两个创新任务：一是CASP，自适应生成语义提示词并结合上下文对齐多粒度视觉与文本语义空间；二是AKFP，通过双路学习器和衣物状态感知损失产生鲁棒的原型表征。

Result: 在多种数据集上实验证明，该方法超越了所有最新的行人再识别方法，在应对衣服变化和顺序学习方面表现出色的鲁棒性与泛化能力。

Conclusion: CMLReID框架能够同时应对衣着变化及持续学习任务，有效解决了实际监控系统中的关键难题，对行人再识别领域具有重要推动作用。

Abstract: Person Re-Identification (ReID) has several challenges in real-world
surveillance systems due to clothing changes (CCReID) and the need for
maintaining continual learning (LReID). Previous existing methods either
develop models specifically for one application, which is mostly a same-cloth
(SC) setting or treat CCReID as its own separate sub-problem. In this work, we
will introduce the LReID-Hybrid task with the goal of developing a model to
achieve both SC and CC while learning in a continual setting. Mismatched
representations and forgetting from one task to the next are significant
issues, we address this with CMLReID, a CLIP-based framework composed of two
novel tasks: (1) Context-Aware Semantic Prompt (CASP) that generates adaptive
prompts, and also incorporates context to align richly multi-grained visual
cues with semantic text space; and (2) Adaptive Knowledge Fusion and Projection
(AKFP) which produces robust SC/CC prototypes through the use of a dual-path
learner that aligns features with our Clothing-State-Aware Projection Loss.
Experiments performed on a wide range of datasets and illustrate that CMLReID
outperforms all state-of-the-art methods with strong robustness and
generalization despite clothing variations and a sophisticated process of
sequential learning.

</details>


### [52] [Cross-Domain Attribute Alignment with CLIP: A Rehearsal-Free Approach for Class-Incremental Unsupervised Domain Adaptation](https://arxiv.org/abs/2509.11264)
*Kerun Mi,Guoliang Kang,Guangyu Li,Lin Zhao,Tao Zhou,Chen Gong*

Main category: cs.CV

TL;DR: 本文提出了一种无需重放机制的新颖CI-UDA方法，通过“属性”建模和跨域对齐，有效缓解灾难性遗忘并提升领域自适应性能。


<details>
  <summary>Details</summary>
Motivation: 现有CI-UDA方法通常依赖记忆库保存旧数据和仅对齐共享类，导致存储负担加重、知识遗忘问题严重。亟需一种高效、快捷且无需持续存储旧数据的方案。

Method: 利用CLIP模型提取类别无关的“属性”信息，构建由“视觉原型-文本提示”组成的key-value属性对，分领域建立属性字典，并通过属性对齐实现领域间的视觉关注和预测结果一致性，无需重放机制即可适应增量类别。

Result: 在三个CI-UDA基准数据集上实验，所提方法优于现有主流方法，显著缓解灾难性遗忘，有效提升跨域增量学习性能。

Conclusion: 通过属性建模和对齐，实现了无重放CI-UDA，既降低了存储开销，又改善了模型对领域和类别新增的适应能力。该方法为持续学习和领域自适应结合提供了有效新思路。

Abstract: Class-Incremental Unsupervised Domain Adaptation (CI-UDA) aims to adapt a
model from a labeled source domain to an unlabeled target domain, where the
sets of potential target classes appearing at different time steps are disjoint
and are subsets of the source classes. The key to solving this problem lies in
avoiding catastrophic forgetting of knowledge about previous target classes
during continuously mitigating the domain shift. Most previous works
cumbersomely combine two technical components. On one hand, they need to store
and utilize rehearsal target sample from previous time steps to avoid
catastrophic forgetting; on the other hand, they perform alignment only between
classes shared across domains at each time step. Consequently, the memory will
continuously increase and the asymmetric alignment may inevitably result in
knowledge forgetting. In this paper, we propose to mine and preserve
domain-invariant and class-agnostic knowledge to facilitate the CI-UDA task.
Specifically, via using CLIP, we extract the class-agnostic properties which we
name as "attribute". In our framework, we learn a "key-value" pair to represent
an attribute, where the key corresponds to the visual prototype and the value
is the textual prompt. We maintain two attribute dictionaries, each
corresponding to a different domain. Then we perform attribute alignment across
domains to mitigate the domain shift, via encouraging visual attention
consistency and prediction consistency. Through attribute modeling and
cross-domain alignment, we effectively reduce catastrophic knowledge forgetting
while mitigating the domain shift, in a rehearsal-free way. Experiments on
three CI-UDA benchmarks demonstrate that our method outperforms previous
state-of-the-art methods and effectively alleviates catastrophic forgetting.
Code is available at https://github.com/RyunMi/VisTA.

</details>


### [53] [Synthetic Dataset Evaluation Based on Generalized Cross Validation](https://arxiv.org/abs/2509.11273)
*Zhihang Song,Dingyi Yao,Ruibo Ming,Lihui Peng,Danya Yao,Yi Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的综合评估框架，通过泛化交叉验证实验和领域迁移学习方法，对合成数据集的质量进行通用且可比的评估。


<details>
  <summary>Details</summary>
Motivation: 现有的合成数据集质量评估方法尚未形成统一标准，评估手段有限，阻碍了数据合成与应用的进一步发展。需要一个系统、广泛适用且可量化的评估体系来指导数据生成和优化。

Method: 作者提出了一种整合泛化交叉验证（GCV）和领域迁移学习理念的评估框架。具体做法是：将任务模型（如YOLOv5s）分别在合成数据集和多个真实数据集（如KITTI, BDD100K）上训练，形成交叉性能矩阵；规范化处理后，构建GCV矩阵用于量化领域迁移能力。框架引入两个核心评估指标：一是用来衡量合成数据与真实数据的相似度（模拟质量），二是衡量合成数据在不同真实场景下的多样性和覆盖度（迁移质量）。

Result: 在Virtual KITTI上的实验结果验证了该评估框架和指标能够有效地衡量合成数据的保真度。

Conclusion: 本文提出的评估框架具备扩展性和可量化性，突破了传统合成数据评估局限，为AI研究中合成数据集的优化和利用提供了系统化指导。

Abstract: With the rapid advancement of synthetic dataset generation techniques,
evaluating the quality of synthetic data has become a critical research focus.
Robust evaluation not only drives innovations in data generation methods but
also guides researchers in optimizing the utilization of these synthetic
resources. However, current evaluation studies for synthetic datasets remain
limited, lacking a universally accepted standard framework. To address this,
this paper proposes a novel evaluation framework integrating generalized
cross-validation experiments and domain transfer learning principles, enabling
generalizable and comparable assessments of synthetic dataset quality. The
framework involves training task-specific models (e.g., YOLOv5s) on both
synthetic datasets and multiple real-world benchmarks (e.g., KITTI, BDD100K),
forming a cross-performance matrix. Following normalization, a Generalized
Cross-Validation (GCV) Matrix is constructed to quantify domain
transferability. The framework introduces two key metrics. One measures the
simulation quality by quantifying the similarity between synthetic data and
real-world datasets, while another evaluates the transfer quality by assessing
the diversity and coverage of synthetic data across various real-world
scenarios. Experimental validation on Virtual KITTI demonstrates the
effectiveness of our proposed framework and metrics in assessing synthetic data
fidelity. This scalable and quantifiable evaluation solution overcomes
traditional limitations, providing a principled approach to guide synthetic
dataset optimization in artificial intelligence research.

</details>


### [54] [ROSGS: Relightable Outdoor Scenes With Gaussian Splatting](https://arxiv.org/abs/2509.11275)
*Lianjun Liao,Chunhui Zhang,Tong Wu,Henglei Lv,Bailin Deng,Lin Gao*

Main category: cs.CV

TL;DR: 本文提出了一种高效的户外场景重建与重光照（relighting）方法ROSGS，基于高斯点云（Gaussian Splatting）以及混合光照模型，实现了更快更准确的户外三维场景重建与光照分解。


<details>
  <summary>Details</summary>
Motivation: 现有基于NeRF等神经网络方法的场景几何、材质、光照分解在户外复杂场景中计算开销大，且普遍采用低频光照表示，导致渲染效率低和重光照精度不足。需要设计一种高效、准确的户外场景三要素（几何、反射率、光照）分离技术。

Method: ROSGS为两阶段流程：第一阶段利用子采样和2D高斯Splatting结合单目法线先验快速重建场景几何信息；第二阶段在已获几何基础上，采用混合光照模型分解场景纹理和光照，具体以球面高斯捕捉太阳等高频定向分量，用球谐系数学习天空等低频分量，实现高质量重光照。

Result: 实验表明，ROSGS在量化指标和主观可视对比上均达到当前最优水平，相较于NeRF类方法有更高重光照精度与渲染效率。

Conclusion: ROSGS能够高效且准确地分离户外场景的几何、纹理、光照成分，大幅提升了重光照的质量与效率，是目前重光照户外场景最优的方案之一。

Abstract: Image data captured outdoors often exhibit unbounded scenes and
unconstrained, varying lighting conditions, making it challenging to decompose
them into geometry, reflectance, and illumination. Recent works have focused on
achieving this decomposition using Neural Radiance Fields (NeRF) or the 3D
Gaussian Splatting (3DGS) representation but remain hindered by two key
limitations: the high computational overhead associated with neural networks of
NeRF and the use of low-frequency lighting representations, which often result
in inefficient rendering and suboptimal relighting accuracy. We propose ROSGS,
a two-stage pipeline designed to efficiently reconstruct relightable outdoor
scenes using the Gaussian Splatting representation. By leveraging monocular
normal priors, ROSGS first reconstructs the scene's geometry with the compact
2D Gaussian Splatting (2DGS) representation, providing an efficient and
accurate geometric foundation. Building upon this reconstructed geometry, ROSGS
then decomposes the scene's texture and lighting through a hybrid lighting
model. This model effectively represents typical outdoor lighting by employing
a spherical Gaussian function to capture the directional, high-frequency
components of sunlight, while learning a radiance transfer function via
Spherical Harmonic coefficients to model the remaining low-frequency skylight
comprehensively. Both quantitative metrics and qualitative comparisons
demonstrate that ROSGS achieves state-of-the-art performance in relighting
outdoor scenes and highlight its ability to deliver superior relighting
accuracy and rendering efficiency.

</details>


### [55] [Mitigating Hallucinations in Large Vision-Language Models by Self-Injecting Hallucinations](https://arxiv.org/abs/2509.11287)
*Yifan Lu,Ziqi Zhang,Chunfeng Yuan,Jun Gao,Congxuan Zhang,Xiaojuan Qi,Bing Li,Weiming Hu*

Main category: cs.CV

TL;DR: 论文提出了一种新的消除大规模视觉-语言模型（LVLMs）幻觉问题的方法——APASI，在不依赖外部人类标注或数据的情况下，通过模型自生成幻觉及偏好对训练，提升模型稳定性和泛化性。


<details>
  <summary>Details</summary>
Motivation: LVLMs 常常会出现幻觉问题，即模型给出的答案与输入图片内容不吻合。现有方法多依赖人工标注或外部模型来生成优劣样本对进行偏好对齐，这样成本高且难以持续改进。

Method: 提出APASI（Autonomous Preference Alignment via Self-Injection），让目标LVLM自行在回答中注入幻觉，生成有偏好差异的样本对。具体地，通过针对幻觉的三种关键观察方式生成低偏好答案，保障幻觉样本的真实性，以此对模型进行偏好对齐训练。同时采用迭代+课程学习训练方式，持续提升生成样本难度与模型能力。

Result: 在六个主流基准测试上，APASI不仅有效减少了三种基线模型的幻觉问题，其性能甚至可与依赖外部数据的偏好对齐方法媲美甚至超越。

Conclusion: APASI能在无外部依赖下持续、稳定地改进LVLM的幻觉问题，具有良好的效果和泛化能力，为LVLM的可持续优化提供了新途径。

Abstract: Large Vision-Language Models (LVLMs) suffer from serious hallucination
problems, where the model-generated responses are inconsistent with the visual
inputs. Existing hallucination mitigation methods are mainly based on
preference alignment and require external human annotations or auxiliary models
for preference data collection, which increase costs and limit sustainable
improvement. To tackle these challenges, we propose Autonomous Preference
Alignment via Self-Injection (APASI), a novel and generalizable method that
mitigates hallucinations without external dependencies. APASI leverages the
target LVLM to self-inject hallucinations into a generated response, creating a
pair of responses with varying preference levels. During the self-injection
process, the dis-preferred response is generated based on three key
observations of hallucinations, ensuring it simulates real hallucination
patterns. This fidelity offers an accurate learning signal for hallucination
mitigation. Moreover, APASI incorporates an iterative alignment training
strategy combined with curriculum learning to periodically update the
preference data with increasing challenge, enabling stable and continuous
enhancement of the LVLM. Extensive experiments across six benchmarks show that
APASI not only effectively mitigates hallucinations for three baseline models
but also achieves comparable or even superior performance to alignment-based
methods with external dependency, thereby demonstrating its effectiveness and
generalization capability. The code is available at
https://github.com/davidluciolu/APASI.

</details>


### [56] [Leveraging Geometric Priors for Unaligned Scene Change Detection](https://arxiv.org/abs/2509.11292)
*Ziling Liu,Ziwei Chen,Mingqi Gao,Jinyu Yang,Feng Zheng*

Main category: cs.CV

TL;DR: 本论文提出了一种利用几何基础模型的新方法，实现了在不同视角下无需对齐直接进行场景变化检测，并在多个数据集上取得了优越表现。


<details>
  <summary>Details</summary>
Motivation: 以往无对齐场景变化检测方法仅依靠2D视觉信息，在存在大视角变化时表现不佳，且当前训练数据集规模有限，缺乏明确的几何推理，导致方法泛化能力和鲁棒性不足。

Method: 首次引入几何基础模型中的几何先验，解决视角不对齐下跨图像重叠区域识别、鲁棒对应关系建立和遮挡检测等核心难题。提出训练免除的方法，将几何和视觉基础模型的强表达能力结合，用于场景变化检测。

Result: 在PSCD、ChangeSim、PASLCD等多个公开数据集上，提出的方法显示出优越且鲁棒的性能。

Conclusion: 通过利用几何基础模型的几何先验，可以大幅提升无对齐场景变化检测的准确性和鲁棒性，有望推动该领域实用化进程。

Abstract: Unaligned Scene Change Detection aims to detect scene changes between image
pairs captured at different times without assuming viewpoint alignment. To
handle viewpoint variations, current methods rely solely on 2D visual cues to
establish cross-image correspondence to assist change detection. However, large
viewpoint changes can alter visual observations, causing appearance-based
matching to drift or fail. Additionally, supervision limited to 2D change masks
from small-scale SCD datasets restricts the learning of generalizable
multi-view knowledge, making it difficult to reliably identify visual overlaps
and handle occlusions. This lack of explicit geometric reasoning represents a
critical yet overlooked limitation. In this work, we are the first to leverage
geometric priors from a Geometric Foundation Model to address the core
challenges of unaligned SCD, including reliable identification of visual
overlaps, robust correspondence establishment, and explicit occlusion
detection. Building on these priors, we propose a training-free framework that
integrates them with the powerful representations of a visual foundation model
to enable reliable change detection under viewpoint misalignment. Through
extensive evaluation on the PSCD, ChangeSim, and PASLCD datasets, we
demonstrate that our approach achieves superior and robust performance. Our
code will be released at https://github.com/ZilingLiu/GeoSCD.

</details>


### [57] [UnLoc: Leveraging Depth Uncertainties for Floorplan Localization](https://arxiv.org/abs/2509.11301)
*Matthias Wüest,Francis Engelmann,Ondrej Miksik,Marc Pollefeys,Daniel Barath*

Main category: cs.CV

TL;DR: UnLoc是一种高效的数据驱动相机定位方法，可直接利用楼层平面图，提升了定位准确性和泛化能力，并且无需为每个环境专门训练深度网络。


<details>
  <summary>Details</summary>
Motivation: 现有的基于视觉的相机定位方法常常需要为不同环境单独训练深度估计网络，且很少考虑深度预测中的不确定性，使方法在实际应用中泛化能力和鲁棒性不足。

Method: 提出了一个新的概率模型，将单目深度预测结果建模为概率分布，并显式引入不确定性建模。利用现成的预训练单目深度模型，无需每个环境分别训练深度网络，从而提升泛化能力和部署便利度。

Result: 在大规模合成和真实数据集上验证了UnLoc的方法效果，比现有方法在准确性和鲁棒性上都显著提升。在LaMAR HGE数据集上，UnLoc在长序列定位召回率上提升2.7倍，在短序列上提升16.7倍。

Conclusion: UnLoc无需自定义深度网络即可高效、稳健地完成基于楼层平面图的连续相机定位，在实际部署中更具普适性和扩展性。

Abstract: We propose UnLoc, an efficient data-driven solution for sequential camera
localization within floorplans. Floorplan data is readily available, long-term
persistent, and robust to changes in visual appearance. We address key
limitations of recent methods, such as the lack of uncertainty modeling in
depth predictions and the necessity for custom depth networks trained for each
environment. We introduce a novel probabilistic model that incorporates
uncertainty estimation, modeling depth predictions as explicit probability
distributions. By leveraging off-the-shelf pre-trained monocular depth models,
we eliminate the need to rely on per-environment-trained depth networks,
enhancing generalization to unseen spaces. We evaluate UnLoc on large-scale
synthetic and real-world datasets, demonstrating significant improvements over
existing methods in terms of accuracy and robustness. Notably, we achieve $2.7$
times higher localization recall on long sequences (100 frames) and $16.7$
times higher on short ones (15 frames) than the state of the art on the
challenging LaMAR HGE dataset.

</details>


### [58] [Motion Estimation for Multi-Object Tracking using KalmanNet with Semantic-Independent Encoding](https://arxiv.org/abs/2509.11323)
*Jian Song,Wei Mei,Yunfeng Xu,Qiang Fu,Renke Kou,Lina Bu,Yucheng Long*

Main category: cs.CV

TL;DR: 本文提出了一种新的运动估计算法SIKNet，有效提升多目标跟踪（MOT）中的鲁棒性和准确率，显著优于传统Kalman滤波器（KF）和现有的学习辅助滤波方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于常速线性模型的Kalman滤波器在参数失配和目标非平稳运动时表现不佳，导致MOT任务中跟踪失败和ID切换增加，亟需更为鲁棒的运动估计方法。

Method: 提出了一种名为Semantic-Independent KalmanNet（SIKNet）的学习辅助滤波方法，利用Semantic-Independent Encoder（SIE）对状态向量进行编码。SIE包括两步：首先利用1D卷积独立编码同质语义元素信息，然后通过全连接层和非线性激活层捕捉异质语义元素间的非线性关联。为验证运动估计模块的性能，构建了大规模的半仿真数据集进行独立评估。

Result: 实验结果表明，所提SIKNet在鲁棒性和准确率上明显优于传统KF和现有学习辅助滤波器，在半仿真大规模数据集上获得了更好的性能。

Conclusion: SIKNet提升了MOT中的运动估计性能，尤其适用于参数失配和目标运动复杂的场景，为MOT任务带来了更高的稳定性和准确率。

Abstract: Motion estimation is a crucial component in multi-object tracking (MOT).
  It predicts the trajectory of objects by analyzing the changes in their
positions in consecutive frames of images, reducing tracking failures and
identity switches.
  The Kalman filter (KF) based on the linear constant-velocity model is one of
the most commonly used methods in MOT.
  However, it may yield unsatisfactory results when KF's parameters are
mismatched and objects move in non-stationary.
  In this work, we utilize the learning-aided filter to handle the motion
estimation of MOT.
  In particular, we propose a novel method named Semantic-Independent KalmanNet
(SIKNet), which encodes the state vector (the input feature) using a
Semantic-Independent Encoder (SIE) by two steps.
  First, the SIE uses a 1D convolution with a kernel size of 1, which convolves
along the dimension of homogeneous-semantic elements across different state
vectors to encode independent semantic information.
  Then it employs a fully-connected layer and a nonlinear activation layer to
encode nonlinear and cross-dependency information between
heterogeneous-semantic elements.
  To independently evaluate the performance of the motion estimation module in
MOT, we constructed a large-scale semi-simulated dataset from several
open-source MOT datasets.
  Experimental results demonstrate that the proposed SIKNet outperforms the
traditional KF and achieves superior robustness and accuracy than existing
learning-aided filters.
  The code is available at (https://github.com/SongJgit/filternet and
https://github.com/SongJgit/TBDTracker).

</details>


### [59] [Toward Next-generation Medical Vision Backbones: Modeling Finer-grained Long-range Visual Dependency](https://arxiv.org/abs/2509.11328)
*Mingyuan Meng*

Main category: cs.CV

TL;DR: 本文探讨了医学影像计算中如何有效建模长距离依赖关系，提出创新性Transformer与MLP方法，实验验证了MLP能更好处理高分辨率、细粒度医学特征，优于Transformer和CNN。


<details>
  <summary>Details</summary>
Motivation: 医学影像任务需要同时关注整体上下文信息和局部细节，当前CNN受局部感受野限制，Transformer虽能处理长距离关系但算力消耗大，无法直接应用于高分辨率医学图像，因此亟需兼顾效率和能力的新式模型。

Method: 论文先创新性地将Transformer用于医学影像分割和分类等任务，后重点开发和探索基于多层感知机（MLP）的视觉模型，在高分辨率医学图像上建模更细致的长距离依赖。

Result: 大量实验证明，长距离依赖建模对医学影像计算至关重要。更进一步，MLP能高效建模高分辨率下更细粒度的长距离关系，捕捉丰富解剖或病理信息，性能领先于Transformer和CNN。

Conclusion: MLP模型在医学影像分析中表现出超越传统CNN和Transformer的潜力，有望成为下一代医学视觉模型主流架构，显著提升各类医学影像任务表现。

Abstract: Medical Image Computing (MIC) is a broad research topic covering both
pixel-wise (e.g., segmentation, registration) and image-wise (e.g.,
classification, regression) vision tasks. Effective analysis demands models
that capture both global long-range context and local subtle visual
characteristics, necessitating fine-grained long-range visual dependency
modeling. Compared to Convolutional Neural Networks (CNNs) that are limited by
intrinsic locality, transformers excel at long-range modeling; however, due to
the high computational loads of self-attention, transformers typically cannot
process high-resolution features (e.g., full-scale image features before
downsampling or patch embedding) and thus face difficulties in modeling
fine-grained dependency among subtle medical image details. Concurrently,
Multi-layer Perceptron (MLP)-based visual models are recognized as
computation/memory-efficient alternatives in modeling long-range visual
dependency but have yet to be widely investigated in the MIC community. This
doctoral research advances deep learning-based MIC by investigating effective
long-range visual dependency modeling. It first presents innovative use of
transformers for both pixel- and image-wise medical vision tasks. The focus
then shifts to MLPs, pioneeringly developing MLP-based visual models to capture
fine-grained long-range visual dependency in medical images. Extensive
experiments confirm the critical role of long-range dependency modeling in MIC
and reveal a key finding: MLPs provide feasibility in modeling finer-grained
long-range dependency among higher-resolution medical features containing
enriched anatomical/pathological details. This finding establishes MLPs as a
superior paradigm over transformers/CNNs, consistently enhancing performance
across various medical vision tasks and paving the way for next-generation
medical vision backbones.

</details>


### [60] [Dual Band Video Thermography Near Ambient Conditions](https://arxiv.org/abs/2509.11334)
*Sriram Narayanan,Mani Ramanagopal,Srinivasa G. Narasimhan*

Main category: cs.CV

TL;DR: 本文提出了一种使用两台具备不同光谱灵敏度的热像仪，首次在视频中分离热辐射（自发光）和环境反射成分的方法，并对材料表面发射率与温度的估算进行了模型化和定量/定性分析。


<details>
  <summary>Details</summary>
Motivation: 在近环境条件下，两类红外信号（反射与自发辐射）幅度相近、且随时间变化，这对理解物体本征属性、改善热像图像分析非常关键。之前的研究多仅考虑一类成分主导，或假定另一成分恒定，难以在复杂实际场景下应用。

Method: 建立了双波段热成像物理模型，利用两台热像仪分别捕捉不同波段的红外信号，使用算法联合估算表面发射率与随时间变化的温度，同时分离动态反射背景。实验采用已知发射率的多种材料进行定量评估，并在日常复杂场景下展示定性效果。

Result: 方法能有效分离复杂场景中物体自身辐射与环境反射成分，准确估算表面属性，在如有热液体玻璃杯、背景人物等场景展示良好性能。

Conclusion: 文中提出的双波段分离方法首次实现了在真实复杂场景下对热辐射和反射信号的分解，提升了热像分析在计算机视觉和实际应用中的能力。

Abstract: Long-wave infrared radiation captured by a thermal camera consists of two
components: (a) light from the environment reflected or transmitted by a
surface, and (b) light emitted by the surface after undergoing heat transport
through the object and exchanging heat with the surrounding environment.
Separating these components is essential for understanding object properties
such as emissivity, temperature, reflectance and shape. Previous thermography
studies often assume that only one component is dominant (e.g., in welding) or
that the second component is constant and can be subtracted. However, in
near-ambient conditions, which are most relevant to computer vision
applications, both components are typically comparable in magnitude and vary
over time. We introduce the first method that separates reflected and emitted
components of light in videos captured by two thermal cameras with different
spectral sensitivities. We derive a dual-band thermal image formation model and
develop algorithms to estimate the surface's emissivity and its time-varying
temperature while isolating a dynamic background. We quantitatively evaluate
our approach using carefully calibrated emissivities for a range of materials
and show qualitative results on complex everyday scenes, such as a glass filled
with hot liquid and people moving in the background.

</details>


### [61] [Beyond Instance Consistency: Investigating View Diversity in Self-supervised Learning](https://arxiv.org/abs/2509.11344)
*Huaiyuan Qin,Muli Yang,Siyuan Hu,Peng Hu,Yu Zhang,Chen Gong,Hongyuan Zhu*

Main category: cs.CV

TL;DR: 本文研究了自监督学习（SSL）中实例一致性假设在非典型图像上的有效性，发现即使缺乏严格一致性依然能学到有用表征，并提出多样性视图能提升下游任务表现，但过度多样性则会适得其反。采用Earth Mover's Distance（EMD）量化视图间互信息，发现适度的EMD有助于SSL效果，并验证了该结论的普适性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统自监督学习假设同一图像的不同视图具有一致性（即内容相同），但真实非典型图像往往不同区域甚至不存在相同性，因此需要重新审视SSL的有效性及优化策略。

Method: 作者通过消融实验，系统分析在不同程度实例一致性下SSL的表现，并通过控制视图重叠度和裁剪比例来人为调节视图多样性；同时，引入EMD作为衡量视图间互信息的指标，进一步关联多样性与SSL性能。

Result: （1）即使正样本对间缺乏实例一致性，SSL仍能学习到有效表征；（2）增加视图多样性（如裁剪无重叠或缩小裁剪区域）能提升分类和密集预测等下游任务性能，但多样性过高则效果下滑；（3）实验表明，适中的EMD（视图间互信息）对应更佳SSL效果。

Conclusion: 实例一致性并非SSL有效学习的充分必要条件，适度调整视图多样性及互信息有助于提升自监督学习表现，EMD可作为未来设计SSL框架的重要参考指标。

Abstract: Self-supervised learning (SSL) conventionally relies on the instance
consistency paradigm, assuming that different views of the same image can be
treated as positive pairs. However, this assumption breaks down for non-iconic
data, where different views may contain distinct objects or semantic
information. In this paper, we investigate the effectiveness of SSL when
instance consistency is not guaranteed. Through extensive ablation studies, we
demonstrate that SSL can still learn meaningful representations even when
positive pairs lack strict instance consistency. Furthermore, our analysis
further reveals that increasing view diversity, by enforcing zero overlapping
or using smaller crop scales, can enhance downstream performance on
classification and dense prediction tasks. However, excessive diversity is
found to reduce effectiveness, suggesting an optimal range for view diversity.
To quantify this, we adopt the Earth Mover's Distance (EMD) as an estimator to
measure mutual information between views, finding that moderate EMD values
correlate with improved SSL learning, providing insights for future SSL
framework design. We validate our findings across a range of settings,
highlighting their robustness and applicability on diverse data sources.

</details>


### [62] [Promoting Shape Bias in CNNs: Frequency-Based and Contrastive Regularization for Corruption Robustness](https://arxiv.org/abs/2509.11355)
*Robin Narsingh Ranabhat,Longwei Wang,Amit Kumar Patel,KC santosh*

Main category: cs.CV

TL;DR: 论文针对CNN对图像局部纹理依赖强、易受常见干扰的缺陷，提出两种正则化方法引导模型关注全局形状，从而提升模型在干扰条件下的稳健性。


<details>
  <summary>Details</summary>
Motivation: CNN模型虽然图像分类表现优秀，但对常见的输入干扰（如噪声、模糊等）表现脆弱，原因在于其对局部纹理依赖强而对全局形状不敏感，这与人类视觉机制相反。因此，提升CNN对全局形状的感知能力可增强其鲁棒性。

Method: 提出两种互补的正则化方法：1）通过一个辅助损失函数，要求原始图像和低频滤波图像提取的特征保持一致，减少对高频纹理的依赖；2）引入有监督的对比学习，使特征空间结构更聚焦于类别一致且与形状相关的信息。

Result: 在CIFAR-10-C干扰数据集上，两个方法都提高了CNN的抗干扰鲁棒性，且未影响其在干净数据上的准确率。

Conclusion: 基于loss的正则化能够有效引导CNN学习更加关注全局形状、且在干扰下表现更稳健的特征表征。

Abstract: Convolutional Neural Networks (CNNs) excel at image classification but remain
vulnerable to common corruptions that humans handle with ease. A key reason for
this fragility is their reliance on local texture cues rather than global
object shapes -- a stark contrast to human perception. To address this, we
propose two complementary regularization strategies designed to encourage
shape-biased representations and enhance robustness. The first introduces an
auxiliary loss that enforces feature consistency between original and
low-frequency filtered inputs, discouraging dependence on high-frequency
textures. The second incorporates supervised contrastive learning to structure
the feature space around class-consistent, shape-relevant representations.
Evaluated on the CIFAR-10-C benchmark, both methods improve corruption
robustness without degrading clean accuracy. Our results suggest that
loss-level regularization can effectively steer CNNs toward more shape-aware,
resilient representations.

</details>


### [63] [GLaVE-Cap: Global-Local Aligned Video Captioning with Vision Expert Integration](https://arxiv.org/abs/2509.11360)
*Wan Xu,Feng Zhu,Yihan Zeng,Yuanfan Guo,Ming Liu,Hang Xu,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 该论文提出了一种创新的视频详细描述生成方法GLaVE-Cap，通过全局-局部协同机制和视觉专家集成，显著提升了生成视频描述的细致性和一致性。同时，作者构建了大规模评测基准和数据集并取得了最新最优实验结果。


<details>
  <summary>Details</summary>
Motivation: 现有局部到全局的视频描述方法存在细节不足和上下文不一致的问题，主要因为缺乏细粒度机制和本地与全局描述之间的弱交互。作者希望解决这些关键瓶颈，以生成更全面和一致的视频描述。

Method: GLaVE-Cap包含TrackFusion和CaptionBridge两个核心模块。TrackFusion利用视觉专家实现跨帧视觉提示并生成细致的局部描述，CaptionBridge通过全局语境引导局部描述并自适应整合为连贯的全局描述。此外，作者构建了多样性强、查询密度高的新评测基准GLaVE-Bench和高质量训练集GLaVE-1.2M。

Result: 在四个主流基准上，GLaVE-Cap取得了最新最优的性能。消融实验和轻量模型分析也证明了各模块和新数据集对视频理解的显著贡献。

Conclusion: GLaVE-Cap通过全局和局部的深度协同，并引入视觉专家和丰富的数据资源，提升了视频详细描述的质量和上下文一致性。数据和代码将开源，有望推动视频理解领域发展。

Abstract: Video detailed captioning aims to generate comprehensive video descriptions
to facilitate video understanding. Recently, most efforts in the video detailed
captioning community have been made towards a local-to-global paradigm, which
first generates local captions from video clips and then summarizes them into a
global caption. However, we find this paradigm leads to less detailed and
contextual-inconsistent captions, which can be attributed to (1) no mechanism
to ensure fine-grained captions, and (2) weak interaction between local and
global captions. To remedy the above two issues, we propose GLaVE-Cap, a
Global-Local aligned framework with Vision Expert integration for Captioning,
which consists of two core modules: TrackFusion enables comprehensive local
caption generation, by leveraging vision experts to acquire cross-frame visual
prompts, coupled with a dual-stream structure; while CaptionBridge establishes
a local-global interaction, by using global context to guide local captioning,
and adaptively summarizing local captions into a coherent global caption.
Besides, we construct GLaVE-Bench, a comprehensive video captioning benchmark
featuring 5X more queries per video than existing benchmarks, covering diverse
visual dimensions to facilitate reliable evaluation. We further provide a
training dataset GLaVE-1.2M containing 16K high-quality fine-grained video
captions and 1.2M related question-answer pairs. Extensive experiments on four
benchmarks show that our GLaVE-Cap achieves state-of-the-art performance.
Besides, the ablation studies and student model analyses further validate the
effectiveness of the proposed modules and the contribution of GLaVE-1.2M to the
video understanding community. The source code, model weights, benchmark, and
dataset will be open-sourced.

</details>


### [64] [In-Vivo Skin 3-D Surface Reconstruction and Wrinkle Depth Estimation using Handheld High Resolution Tactile Sensing](https://arxiv.org/abs/2509.11385)
*Akhil Padmanabha,Arpit Agarwal,Catherine Li,Austin Williams,Dinesh K. Patel,Sankalp Chopkar,Achu Wilson,Ahmet Ozkan,Wenzhen Yuan,Sonal Choudhary,Arash Mostaghimi,Zackory Erickson,Carmel Majidi*

Main category: cs.CV

TL;DR: 本文提出了一种基于GelSight触觉成像的便携式三维皮肤表面重建设备，实现了微米级皱纹深度估计，并首次在多部位皮肤上做了验证，为皮肤医学与护肤品效果评测提供了新工具。


<details>
  <summary>Details</summary>
Motivation: 皮肤表面三维重建可为皮肤科诊断和评估提供客观、定量的数据。然而，现有设备普遍体积大、分辨率有限，无法实现多部位实际应用。亟需一种便携、高分辨率且已验证的三维重建设备。

Method: 开发了一台基于自定义弹性凝胶和学习型重建算法的小型三维皮肤成像探头，通过集成力传感器确保与皮肤接触一致性，用于微米级皱纹高度测量。

Result: 新设备在模拟皱纹对象上的平均绝对误差为12.55微米。在15名无皮肤病参与者身上，对多个身体部位的皮肤皱纹深度进行了首次系统性、定量测量。涂抹市售保湿霜后，3个部位的皱纹高度显著下降。

Conclusion: 本文开发的仪器为皮肤医学和化妆品领域提供了一种便捷、精准的工具，可用于诊断、疗效监测及护肤品评测，展现出广阔应用前景。

Abstract: Three-dimensional (3-D) skin surface reconstruction offers promise for
objective and quantitative dermatological assessment, but no portable,
high-resolution device exists that has been validated and used for depth
reconstruction across various body locations. We present a compact 3-D skin
reconstruction probe based on GelSight tactile imaging with a custom elastic
gel and a learning-based reconstruction algorithm for micron-level wrinkle
height estimation. Our probe, integrated into a handheld probe with force
sensing for consistent contact, achieves a mean absolute error of 12.55 micron
on wrinkle-like test objects. In a study with 15 participants without skin
disorders, we provide the first validated wrinkle depth metrics across multiple
body regions. We further demonstrate statistically significant reductions in
wrinkle height at three locations following over-the-counter moisturizer
application. Our work offers a validated tool for clinical and cosmetic skin
analysis, with potential applications in diagnosis, treatment monitoring, and
skincare efficacy evaluation.

</details>


### [65] [MixANT: Observation-dependent Memory Propagation for Stochastic Dense Action Anticipation](https://arxiv.org/abs/2509.11394)
*Syed Talal Wasim,Hamid Suleman,Olga Zatsarynna,Muzammal Naseer,Juergen Gall*

Main category: cs.CV

TL;DR: 本文提出了MixANT，一种用于人类活动长期预测的新型架构，通过改进遗忘门机制，实现更优预测效果，在多个数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前的时序状态空间模型（如Mamba）在多个参数上已具备输入相关性，但关键的遗忘门（A矩阵）仍为静态，这限制了网络对复杂、动态时序信息的建模能力。作者旨在解决这一限制，提升模型的表现能力和适应性。

Method: 提出混合专家（mixture of experts）机制，根据输入动态选择不同的A矩阵（遗忘门），赋予模型在不同语境下更灵活的时序记忆能力，同时兼顾计算效率。

Result: 在50Salads、Breakfast、Assembly101等公共人类活动预测数据集上进行广泛实验，结果显示MixANT在所有评测设定下均超越了当前最优方法。

Conclusion: 输入相关的遗忘门机制对于提升人类行为预测的准确率尤为关键，MixANT展现了更强的泛化能力和实际应用价值。

Abstract: We present MixANT, a novel architecture for stochastic long-term dense
anticipation of human activities. While recent State Space Models (SSMs) like
Mamba have shown promise through input-dependent selectivity on three key
parameters, the critical forget-gate ($\textbf{A}$ matrix) controlling temporal
memory remains static. We address this limitation by introducing a mixture of
experts approach that dynamically selects contextually relevant $\textbf{A}$
matrices based on input features, enhancing representational capacity without
sacrificing computational efficiency. Extensive experiments on the 50Salads,
Breakfast, and Assembly101 datasets demonstrate that MixANT consistently
outperforms state-of-the-art methods across all evaluation settings. Our
results highlight the importance of input-dependent forget-gate mechanisms for
reliable prediction of human behavior in diverse real-world scenarios.

</details>


### [66] [No Modality Left Behind: Dynamic Model Generation for Incomplete Medical Data](https://arxiv.org/abs/2509.11406)
*Christoph Fürböck,Paul Weiser,Branko Mitic,Philipp Seeböck,Thomas Helbich,Georg Langs*

Main category: cs.CV

TL;DR: 该论文提出了一种基于超网络的方法，可动态适应不同缺失模态的医学影像数据，提高分类任务的鲁棒性和泛化能力，并在不完整数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在临床实际环境中，多模态医学影像数据常因部分模态缺失而影响深度学习模型表现，现有解决方案在鲁棒性和泛化性方面存在不足。

Method: 作者设计了一个超网络，根据样本实际可用的影像模态动态生成任务模型参数，实现对所有样本的灵活训练和推理，并和现有三类方法（仅使用完整数据、channel dropout、插值补全）做系统对比。

Result: 在模拟不同模态缺失的数据集下，所提方法在只有25%数据完整性时，分类准确率比现有最优方法高出最多8%，适应性显著提升。

Conclusion: 该超网络方法无需针对每种模态组合单独训练模型，能处理多模态缺失，用单一模型实现高效、稳健的医学影像分析，更符合实际应用需求。

Abstract: In real world clinical environments, training and applying deep learning
models on multi-modal medical imaging data often struggles with partially
incomplete data. Standard approaches either discard missing samples, require
imputation or repurpose dropout learning schemes, limiting robustness and
generalizability. To address this, we propose a hypernetwork-based method that
dynamically generates task-specific classification models conditioned on the
set of available modalities. Instead of training a fixed model, a hypernetwork
learns to predict the parameters of a task model adapted to available
modalities, enabling training and inference on all samples, regardless of
completeness. We compare this approach with (1) models trained only on complete
data, (2) state of the art channel dropout methods, and (3) an imputation-based
method, using artificially incomplete datasets to systematically analyze
robustness to missing modalities. Results demonstrate superior adaptability of
our method, outperforming state of the art approaches with an absolute increase
in accuracy of up to 8% when trained on a dataset with 25% completeness (75% of
training data with missing modalities). By enabling a single model to
generalize across all modality configurations, our approach provides an
efficient solution for real-world multi-modal medical data analysis.

</details>


### [67] [On the Skinning of Gaussian Avatars](https://arxiv.org/abs/2509.11411)
*Nikolaos Zioulis,Nikolaos Kotarelas,Georgios Albanis,Spyridon Thermos,Anargyros Chatzitofis*

Main category: cs.CV

TL;DR: 本文提出了一种基于四元数加权平均的旋转混合方法，用于动画高效、易集成的人体高斯体重建。该方法在提升效率和简化实现的同时，解决了以往高斯体动画中由线性混合导致的非线性旋转问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于辐射场的人体重建在神经辐射场方法下虽取得进展，但渲染速度慢且坐标映射复杂。高斯体方法虽加快了训练与渲染，但由于采用线性混合蒙皮，无法正确表达高斯体的非线性旋转属性，出现失真。

Method: 作者提出了一种加权旋转混合方法，具体地，通过四元数的加权平均实现对高斯体的非线性旋转处理。该方法可以基于顶点的高斯体动画，仅需对已有线性混合蒙皮做少量修改，并可与任何高斯体光栅化器结合使用。

Result: 新方法相比于利用网格或训练可纠偏模型的方法更简洁，效率高，动画逼真且易于在实际引擎集成。解决了传统高斯体线性旋转混合带来的变形和不自然等问题。

Conclusion: 该方案有效提升了高斯体方法在人形动画场景下的实用性和表现力，是高效、可靠的人体动画新范式，为相关领域广泛使用提供了技术基础。

Abstract: Radiance field-based methods have recently been used to reconstruct human
avatars, showing that we can significantly downscale the systems needed for
creating animated human avatars. Although this progress has been initiated by
neural radiance fields, their slow rendering and backward mapping from the
observation space to the canonical space have been the main challenges. With
Gaussian splatting overcoming both challenges, a new family of approaches has
emerged that are faster to train and render, while also straightforward to
implement using forward skinning from the canonical to the observation space.
However, the linear blend skinning required for the deformation of the
Gaussians does not provide valid results for their non-linear rotation
properties. To address such artifacts, recent works use mesh properties to
rotate the non-linear Gaussian properties or train models to predict corrective
offsets. Instead, we propose a weighted rotation blending approach that
leverages quaternion averaging. This leads to simpler vertex-based Gaussians
that can be efficiently animated and integrated in any engine by only modifying
the linear blend skinning technique, and using any Gaussian rasterizer.

</details>


### [68] [Beyond Frame-wise Tracking: A Trajectory-based Paradigm for Efficient Point Cloud Tracking](https://arxiv.org/abs/2509.11453)
*BaiChen Fan,Sifan Zhou,Jian Li,Shibo Zhao,Muqing Cao,Qin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于轨迹的3D单目标追踪方法TrajTrack，有效平衡了长期时序信息与计算效率，并显著提升了追踪性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于激光雷达的3D单目标追踪方法主要分为帧间两帧法和多帧序列法。两帧法高效但对稀疏或遮挡场景不鲁棒，多帧法虽鲁棒但计算代价高，因此需要一种同时兼顾效果与效率的新方法。

Method: 本文提出轨迹驱动的TrajTrack范式，在两帧追踪器基础上，利用历史包围盒轨迹信息，通过显式快速运动提议与隐式轨迹建模模块，提升运动连续性建模能力，无需额外点云输入。

Result: TrajTrack在大型NuScenes数据集上达到新的SOTA，追踪精度较强基线提升4.48%，推理速度56FPS，并验证了对不同基线追踪器的强泛化性。

Conclusion: TrajTrack在不增加额外点云计算成本的前提下，显著提升了3D单目标追踪的精度和效率，具有良好泛化性，为自动驾驶等实际应用带来实用价值。

Abstract: LiDAR-based 3D single object tracking (3D SOT) is a critical task in robotics
and autonomous systems. Existing methods typically follow frame-wise motion
estimation or a sequence-based paradigm. However, the two-frame methods are
efficient but lack long-term temporal context, making them vulnerable in sparse
or occluded scenes, while sequence-based methods that process multiple point
clouds gain robustness at a significant computational cost. To resolve this
dilemma, we propose a novel trajectory-based paradigm and its instantiation,
TrajTrack. TrajTrack is a lightweight framework that enhances a base two-frame
tracker by implicitly learning motion continuity from historical bounding box
trajectories alone-without requiring additional, costly point cloud inputs. It
first generates a fast, explicit motion proposal and then uses an implicit
motion modeling module to predict the future trajectory, which in turn refines
and corrects the initial proposal. Extensive experiments on the large-scale
NuScenes benchmark show that TrajTrack achieves new state-of-the-art
performance, dramatically improving tracking precision by 4.48% over a strong
baseline while running at 56 FPS. Besides, we also demonstrate the strong
generalizability of TrajTrack across different base trackers. Video is
available at https://www.bilibili.com/video/BV1ahYgzmEWP.

</details>


### [69] [Disentanglement of Biological and Technical Factors via Latent Space Rotation in Clinical Imaging Improves Disease Pattern Discovery](https://arxiv.org/abs/2509.11436)
*Jeanny Pan,Philipp Seeböck,Christoph Fürböck,Svitlana Pochepnia,Jennifer Straub,Lucian Beer,Helmut Prosch,Georg Langs*

Main category: cs.CV

TL;DR: 本文提出一种利用机器学习，通过对医学影像数据潜在空间进行旋转，以主动学习并消除由于不同成像技术带来的领域偏移，从而实现对生物学和技术变量的有效解耦，提高了跨多中心和多成像技术下的疾病相关特征发现和分析。


<details>
  <summary>Details</summary>
Motivation: 传统的医学影像分析面临由于不同仪器、厂商和参数引起的数据领域偏移，影响了疾病相关特征的识别与发现。需要一种新方法有效区分和解耦生物学差异与技术差异，提升稳定性和泛化能力，促进多中心数据的分析与应用。

Method: 作者提出通过对数据潜在空间后处理旋转，主动学习领域偏移，实现生物学因子与技术因子的解耦。该方法支持在无标签的条件下实现领域间和谐，为类别聚类和统计分析提供一致且稳定的数据表示。

Result: 在实际多中心临床数据上，该方法实现了比四种主流数据标准化方法更高的聚类一致性（ARI提升19.01%，NMI提升16.85%，Dice提升12.39%）。在特定疾病（特发性肺纤维化）患者中，使用聚类分析得出的组织构成能够更好地支持生存预测分析（Cox回归），展示了生物标志物发现的潜力。

Conclusion: 提出的无标签潜在空间旋转解耦方法能有效减少技术差异的影响，促进多中心医学影像数据的稳健、有效分析与疾病生物标志物的挖掘，具有良好的应用前景。作者已开放代码，便于研究和临床应用。

Abstract: Identifying new disease-related patterns in medical imaging data with the
help of machine learning enlarges the vocabulary of recognizable findings. This
supports diagnostic and prognostic assessment. However, image appearance varies
not only due to biological differences, but also due to imaging technology
linked to vendors, scanning- or re- construction parameters. The resulting
domain shifts impedes data representation learning strategies and the discovery
of biologically meaningful cluster appearances. To address these challenges, we
introduce an approach to actively learn the domain shift via post-hoc rotation
of the data latent space, enabling disentanglement of biological and technical
factors. Results on real-world heterogeneous clinical data showcase that the
learned disentangled representation leads to stable clusters representing
tissue-types across different acquisition settings. Cluster consistency is
improved by +19.01% (ARI), +16.85% (NMI), and +12.39% (Dice) compared to the
entangled representation, outperforming four state-of-the-art harmonization
methods. When using the clusters to quantify tissue composition on idiopathic
pulmonary fibrosis patients, the learned profiles enhance Cox survival
prediction. This indicates that the proposed label-free framework facilitates
biomarker discovery in multi-center routine imaging data. Code is available on
GitHub https://github.com/cirmuw/latent-space-rotation-disentanglement.

</details>


### [70] [MultiMAE for Brain MRIs: Robustness to Missing Inputs Using Multi-Modal Masked Autoencoder](https://arxiv.org/abs/2509.11442)
*Ayhan Can Erdur,Christian Beischl,Daniel Scholz,Jiazhen Pan,Benedikt Wiestler,Daniel Rueckert,Jan C Peeken*

Main category: cs.CV

TL;DR: 本文提出了一种针对脑部MRI多模态、多任务的3D医学影像自编码器预训练方法，对输入序列缺失问题表现出较强的鲁棒性，并在下游分割与分类任务上优于MAE-ViT基线。


<details>
  <summary>Details</summary>
Motivation: 医学影像数据常有输入序列缺失，现有深度学习模型对完整输入依赖较强，缺失数据影响表现，因此亟需能有效处理不完整输入的新方法。

Method: 受MultiMAE方法启发，将每种MRI序列视作单独模态，采用late-fusion风格的Transformer编码器整合多序列信息，并为每个模态设计独立解码器，实现多任务重建。该预训练方法引导模型学习每种模态的丰富表示，并通过序列间推理以应对缺失输入。

Result: 在下游分割与分类任务上，与MAE-ViT基线比，在输入序列缺失情况下Dice得分提升10.1，MCC提升0.46，展现出方法在性能与鲁棒性上的优势。

Conclusion: 提出的方法能够灵活、泛化地应用于脑部MRI影像，对于推断缺失序列表现优秀，适应性强，并可拓展至各类下游任务。

Abstract: Missing input sequences are common in medical imaging data, posing a
challenge for deep learning models reliant on complete input data. In this
work, inspired by MultiMAE [2], we develop a masked autoencoder (MAE) paradigm
for multi-modal, multi-task learning in 3D medical imaging with brain MRIs. Our
method treats each MRI sequence as a separate input modality, leveraging a
late-fusion-style transformer encoder to integrate multi-sequence information
(multi-modal) and individual decoder streams for each modality for multi-task
reconstruction. This pretraining strategy guides the model to learn rich
representations per modality while also equipping it to handle missing inputs
through cross-sequence reasoning. The result is a flexible and generalizable
encoder for brain MRIs that infers missing sequences from available inputs and
can be adapted to various downstream applications. We demonstrate the
performance and robustness of our method against an MAE-ViT baseline in
downstream segmentation and classification tasks, showing absolute improvement
of $10.1$ overall Dice score and $0.46$ MCC over the baselines with missing
input sequences. Our experiments demonstrate the strength of this pretraining
strategy. The implementation is made available.

</details>


### [71] [Learning to Generate 4D LiDAR Sequences](https://arxiv.org/abs/2509.11959)
*Ao Liang,Youquan Liu,Yu Yang,Dongyue Lu,Linfeng Li,Lingdong Kong,Huaici Zhao,Wei Tsang Ooi*

Main category: cs.CV

TL;DR: 本文提出了LiDARCrafter，一种将文本转为可编辑的LiDAR序列的统一框架。通过三分支扩散模型和自回归模块，实现了高保真、可控、时序一致的激光雷达数据生成。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式模型在视频与基于占据（occupancy-based）数据合成方面取得进展，但LiDAR数据生成研究较少。这限制了自动驾驶等3D感知系统的数据增强与仿真的发展。

Method: LiDARCrafter将自然语言指令解析为以自车为中心的场景图，再用三分支扩散模型生成3D布局、轨迹与物体形状。初始LiDAR扫描由range-image扩散模型生成，之后用自回归模块扩展为时序一致的点云序列。框架还支持对象级编辑（如插入与重定位）。评估采用自建EvalSuite基准，从场景、对象和序列多个层面进行测评。

Result: 在公开的nuScenes数据集上，LiDARCrafter在保真度、可控性和时序一致性等指标上均达到了最新最优水平，并可支持多样的对象编辑。

Conclusion: LiDARCrafter为基于LiDAR的仿真与数据增强提供了强大基础，填补了LiDAR生成领域的空白，并推动自动驾驶相关研究发展。

Abstract: While generative world models have advanced video and occupancy-based data
synthesis, LiDAR generation remains underexplored despite its importance for
accurate 3D perception. Extending generation to 4D LiDAR data introduces
challenges in controllability, temporal stability, and evaluation. We present
LiDARCrafter, a unified framework that converts free-form language into
editable LiDAR sequences. Instructions are parsed into ego-centric scene
graphs, which a tri-branch diffusion model transforms into object layouts,
trajectories, and shapes. A range-image diffusion model generates the initial
scan, and an autoregressive module extends it into a temporally coherent
sequence. The explicit layout design further supports object-level editing,
such as insertion or relocation. To enable fair assessment, we provide
EvalSuite, a benchmark spanning scene-, object-, and sequence-level metrics. On
nuScenes, LiDARCrafter achieves state-of-the-art fidelity, controllability, and
temporal consistency, offering a foundation for LiDAR-based simulation and data
augmentation.

</details>


### [72] [Modality-Aware Infrared and Visible Image Fusion with Target-Aware Supervision](https://arxiv.org/abs/2509.11476)
*Tianyao Sun,Dawei Xiang,Tianqi Ding,Xiang Fang,Yijiashun Qi,Zunduo Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的红外与可见光图像融合框架FusionNet，通过多模态感知机制，有效整合不同光谱信息，实现高质量的图像融合，并提升下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 多模态感知（如红外与可见光图像）带有互补的结构和纹理信息，但如何高效融合这两种信息，同时兼顾感知质量和下游任务语义表现，是图像融合领域的核心难题。

Method: FusionNet采用端到端的神经网络结构。首先，引入模态感知注意力机制，根据红外和可见光信息的判别能力，动态分配各自特征的权重。此外，设计了像素级alpha混合模块，可以自适应地学习空间变化的融合权重，并结合弱ROI监督的目标感知损失，保证融合结果在重要对象区域语义一致。

Result: 在公开的M3FD数据集上，FusionNet能够生成具有更好语义保留性、高感知质量和良好可解释性的融合图像。

Conclusion: FusionNet为面向语义的多模态图像融合提供了一种通用且可扩展的解决方案，对目标检测和场景理解等下游应用有明显促进作用。

Abstract: Infrared and visible image fusion (IVIF) is a fundamental task in multi-modal
perception that aims to integrate complementary structural and textural cues
from different spectral domains. In this paper, we propose FusionNet, a novel
end-to-end fusion framework that explicitly models inter-modality interaction
and enhances task-critical regions. FusionNet introduces a modality-aware
attention mechanism that dynamically adjusts the contribution of infrared and
visible features based on their discriminative capacity. To achieve
fine-grained, interpretable fusion, we further incorporate a pixel-wise alpha
blending module, which learns spatially-varying fusion weights in an adaptive
and content-aware manner. Moreover, we formulate a target-aware loss that
leverages weak ROI supervision to preserve semantic consistency in regions
containing important objects (e.g., pedestrians, vehicles). Experiments on the
public M3FD dataset demonstrate that FusionNet generates fused images with
enhanced semantic preservation, high perceptual quality, and clear
interpretability. Our framework provides a general and extensible solution for
semantic-aware multi-modal image fusion, with benefits for downstream tasks
such as object detection and scene understanding.

</details>


### [73] [Multiple Instance Learning Framework with Masked Hard Instance Mining for Gigapixel Histopathology Image Analysis](https://arxiv.org/abs/2509.11526)
*Wenhao Tang,Sheng Huang,Heng Fang,Fengtao Zhou,Bo Liu,Qingshan Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新的多实例学习（MIL）框架MHIM-MIL，通过掩码困难实例挖掘提升病理全景切片图像分析的性能。


<details>
  <summary>Details</summary>
Motivation: 传统MIL方法常倾向于关注易分类实例，忽视难分类实例，导致判别边界建模能力不足，而困难样本对模型性能提升至关重要。

Method: MHIM-MIL利用带一致性约束的Siamese架构，通过动量教师网络掩码显著实例，从而间接挖掘困难实例，采用大规模随机掩码及全局循环网络防止特征丢失，学生模型通过指数滑动平均更新教师，增强训练鲁棒性和优化稳定性。

Result: MHIM-MIL在癌症诊断、分型、生存分析及12项基准任务上均优于现有最新方法，在性能与效率上都有较大提升。

Conclusion: 通过挖掘并利用困难实例，MHIM-MIL有效突破了现有MIL瓶颈，为病理图像分析提供了更高效、准确的新方案。

Abstract: Digitizing pathological images into gigapixel Whole Slide Images (WSIs) has
opened new avenues for Computational Pathology (CPath). As positive tissue
comprises only a small fraction of gigapixel WSIs, existing Multiple Instance
Learning (MIL) methods typically focus on identifying salient instances via
attention mechanisms. However, this leads to a bias towards easy-to-classify
instances while neglecting challenging ones. Recent studies have shown that
hard examples are crucial for accurately modeling discriminative boundaries.
Applying such an idea at the instance level, we elaborate a novel MIL framework
with masked hard instance mining (MHIM-MIL), which utilizes a Siamese structure
with a consistency constraint to explore the hard instances. Using a
class-aware instance probability, MHIM-MIL employs a momentum teacher to mask
salient instances and implicitly mine hard instances for training the student
model. To obtain diverse, non-redundant hard instances, we adopt large-scale
random masking while utilizing a global recycle network to mitigate the risk of
losing key features. Furthermore, the student updates the teacher using an
exponential moving average, which identifies new hard instances for subsequent
training iterations and stabilizes optimization. Experimental results on cancer
diagnosis, subtyping, survival analysis tasks, and 12 benchmarks demonstrate
that MHIM-MIL outperforms the latest methods in both performance and
efficiency. The code is available at: https://github.com/DearCaat/MHIM-MIL.

</details>


### [74] [SFGNet: Semantic and Frequency Guided Network for Camouflaged Object Detection](https://arxiv.org/abs/2509.11539)
*Dezhen Wang,Haixiang Zhao,Xiang Shen,Sheng Miao*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的伪装目标检测网络SFGNet，通过引入语义提示和频域特征提升了检测性能，并在多个基准数据集上取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 现有伪装目标检测方法忽视了不同目标文本提示的语义差异以及对细粒度频率特征的利用，导致对复杂背景和模糊边界的处理能力有限。

Method: 作者提出了语义与频域引导网络（SFGNet），结合了语义提示信息和频域特征来提升伪装目标的检测和边界感知能力。设计了多带傅里叶模块（MBFM）以增强对复杂背景和模糊边界的处理能力，同时设计了交互式结构增强模块（ISEB）用于确保预测的结构完整性和边界细节。

Result: 在三个主流伪装目标检测基准数据集上的大量实验显示，SFGNet在性能上明显优于当前主流方法。

Conclusion: 通过融合语义提示与频域特征，所提出的SFGNet显著提升了伪装目标检测的准确率与边界识别能力，是伪装目标检测领域的一项有效进展。

Abstract: Camouflaged object detection (COD) aims to segment objects that blend into
their surroundings. However, most existing studies overlook the semantic
differences among textual prompts of different targets as well as fine-grained
frequency features. In this work, we propose a novel Semantic and Frequency
Guided Network (SFGNet), which incorporates semantic prompts and
frequency-domain features to capture camouflaged objects and improve boundary
perception. We further design Multi-Band Fourier Module(MBFM) to enhance the
ability of the network in handling complex backgrounds and blurred boundaries.
In addition, we design an Interactive Structure Enhancement Block (ISEB) to
ensure structural integrity and boundary details in the predictions. Extensive
experiments conducted on three COD benchmark datasets demonstrate that our
method significantly outperforms state-of-the-art approaches. The core code of
the model is available at the following link:
https://github.com/winter794444/SFGNetICASSP2026.

</details>


### [75] [How Auxiliary Reasoning Unleashes GUI Grounding in VLMs](https://arxiv.org/abs/2509.11548)
*Weiming Li,Yan Shao,Jing Yang,Yujing Lu,Ling Zhong,Yuhan Wang,Manni Duan*

Main category: cs.CV

TL;DR: 本文提出三种零样本辅助推理方法，通过向输入图像提供空间线索（如坐标轴、网格、标注交点），显著提升了VLM在图形界面定位（GUI grounding）任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型（VLM）虽然具备潜在定位能力，但在需要给出明确坐标时表现不佳。目前精调方法数据和标注成本较高，亟需低成本、无需额外数据的新方法。

Method: 作者提出三种零样本辅助推理方法：在输入图像中增加空间线索（坐标轴、网格、标注交点），从而使VLM能更好地表达其对空间的理解，无需针对特定任务重新训练。

Result: 在四个常用GUI grounding基准和七种VLM（开源及专有）上评测，结果显示所提方法大幅提升了VLM在GUI grounding任务上的表现。

Conclusion: 通过增强输入的空间显式性，VLM的空间推理能力被充分激发，无需昂贵的标注和精调成本即可大幅改善GUI grounding表现。

Abstract: Graphical user interface (GUI) grounding is a fundamental task for building
GUI agents. However, general vision-language models (VLMs) struggle with this
task due to a lack of specific optimization. We identify a key gap in this
paper: while VLMs exhibit significant latent grounding potential, as
demonstrated by their performance measured by Pointing Game, they underperform
when tasked with outputting explicit coordinates. To address this discrepancy,
and bypass the high data and annotation costs of current fine-tuning
approaches, we propose three zero-shot auxiliary reasoning methods. By
providing explicit spatial cues such as axes, grids and labeled intersections
as part of the input image, these methods enable VLMs to articulate their
implicit spatial understanding capabilities. We evaluate these methods on four
GUI grounding benchmarks across seven open-source and proprietary VLMs. The
evaluation results demonstrate that the proposed methods substantially improve
the performance of GUI grounding.

</details>


### [76] [Gaussian-Plus-SDF SLAM: High-fidelity 3D Reconstruction at 150+ fps](https://arxiv.org/abs/2509.11574)
*Zhexi Peng,Kun Zhou,Tianjia Shao*

Main category: cs.CV

TL;DR: 本文提出了一种高效的Gaussian-SDF混合表示方法，并基于此开发了GPS-SLAM系统，实现了在RGB-D数据上的实时高质量三维重建，速度远超主流Gauss-based方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于高斯的SLAM方法虽然能够实现高质量的照片级三维重建，但在计算速度上远远落后于几何为主的方法（如KinectFusion），主要瓶颈在于高斯数量多、优化过程复杂而导致的效率低。

Method: 作者提出将有颜色的有符号距离场（SDF）和三维高斯分布相结合，SDF高效融合RGB-D数据重建主体几何和外观特征，高斯只用于补充捕捉SDF难以表达的细节。通过此混合表示，极大减少了所需高斯的数量和优化迭代次数。进而开发了GPS-SLAM系统，实现高效实时三维重建。

Result: GPS-SLAM在真实Azure Kinect数据上实现了超过150fps的实时性能，速度提升一个数量级，同时重建质量与现有最先进方法持平。

Conclusion: 混合表示有效克服了高斯SLAM方法的性能瓶颈，为高效SOTA三维重建提供了新范式，有望推动研究社区发展，代码和数据也将开源。

Abstract: While recent Gaussian-based SLAM methods achieve photorealistic
reconstruction from RGB-D data, their computational performance remains a
critical bottleneck. State-of-the-art techniques operate at less than 20 fps,
significantly lagging behind geometry-centric approaches like KinectFusion
(hundreds of fps). This limitation stems from the heavy computational burden:
modeling scenes requires numerous Gaussians and complex iterative optimization
to fit RGB-D data, where insufficient Gaussian counts or optimization
iterations cause severe quality degradation. To address this, we propose a
Gaussian-SDF hybrid representation, combining a colorized Signed Distance Field
(SDF) for smooth geometry and appearance with 3D Gaussians to capture
underrepresented details. The SDF is efficiently constructed via RGB-D fusion
(as in geometry-centric methods), while Gaussians undergo iterative
optimization. Our representation enables drastic Gaussian reduction (50% fewer)
by avoiding full-scene Gaussian modeling, and efficient Gaussian optimization
(75% fewer iterations) through targeted appearance refinement. Building upon
this representation, we develop GPS-SLAM (Gaussian-Plus-SDF SLAM), a real-time
3D reconstruction system achieving over 150 fps on real-world Azure Kinect
sequences -- delivering an order-of-magnitude speedup over state-of-the-art
techniques while maintaining comparable reconstruction quality. We will release
the source code and data to facilitate future research.

</details>


### [77] [Hierarchical Identity Learning for Unsupervised Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2509.11587)
*Haonan Shi,Yubin Wang,De Cheng,Lingfeng He,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: 这篇论文针对无监督可见光-红外跨模态行人重识别（USVI-ReID）问题，提出了一种层次化身份学习（HIL）框架，通过多级聚类和多中心对比学习，有效缩小模态差异并增强特征区分性，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前USVI-ReID多采用基于聚类的对比学习方法，每个身份只用单一聚类中心表示，这种方法忽视了细粒度的图像差异，限制了特征表达能力，因此亟需一种能同时考虑共性和细粒度差异的无监督方法。

Method: 作者提出层次化身份学习（HIL）框架，先对每个粗粒度聚类中心进行二次聚类，生成多个细粒度子中心，捕捉更多图像变异。引入多中心对比学习（MCCL）增强表征学习，并设计双向反向选择传递（BRST）机制，通过伪标签的双向匹配强化跨模态关联。

Result: 在SYSU-MM01和RegDB两个主流数据集上进行大量实验，结果显示所提方法在各项指标上均优于现有USVI-ReID方法。

Conclusion: 所提出的HIL框架与MCCL、BRST机制能够有效提升无监督可见光-红外跨模态行人重识别的性能，为USVI-ReID提供了新的思路。

Abstract: Unsupervised visible-infrared person re-identification (USVI-ReID) aims to
learn modality-invariant image features from unlabeled cross-modal person
datasets by reducing the modality gap while minimizing reliance on costly
manual annotations. Existing methods typically address USVI-ReID using
cluster-based contrastive learning, which represents a person by a single
cluster center. However, they primarily focus on the commonality of images
within each cluster while neglecting the finer-grained differences among them.
To address the limitation, we propose a Hierarchical Identity Learning (HIL)
framework. Since each cluster may contain several smaller sub-clusters that
reflect fine-grained variations among images, we generate multiple memories for
each existing coarse-grained cluster via a secondary clustering. Additionally,
we propose Multi-Center Contrastive Learning (MCCL) to refine representations
for enhancing intra-modal clustering and minimizing cross-modal discrepancies.
To further improve cross-modal matching quality, we design a Bidirectional
Reverse Selection Transmission (BRST) mechanism, which establishes reliable
cross-modal correspondences by performing bidirectional matching of
pseudo-labels. Extensive experiments conducted on the SYSU-MM01 and RegDB
datasets demonstrate that the proposed method outperforms existing approaches.
The source code is available at: https://github.com/haonanshi0125/HIL.

</details>


### [78] [Optimizing Class Distributions for Bias-Aware Multi-Class Learning](https://arxiv.org/abs/2509.11588)
*Mirco Felske,Stefan Stiene*

Main category: cs.CV

TL;DR: BiCDO 是一个用于多类别图像分类的数据分布优化框架，通过调整各类别数据量提升整体和关键类别的表现。


<details>
  <summary>Details</summary>
Motivation: 多类别分类中，通常采用各类样本均匀分布，但实际应用（如安全关键场景）下，可能需要对某些类别优先提升表现，因此需要灵活优化类别样本分布。

Method: BiCDO 采用迭代优化方式，根据目标函数调整每个类别的数据量，找到帕累托最优分布，提升整体和目标类别的可靠性，减少偏差和方差。易于集成现有训练流程，适用于任意有标签多类别数据集。

Result: 在 EfficientNet、ResNet、ConvNeXt 等模型与 CIFAR-10、iNaturalist21 数据集上的实验，BiCDO 显著提升了模型的总体性能和平衡性。

Conclusion: 通过采用 BiCDO 优化数据分布，可以在不显著增加训练难度的情况下，有效提升多类别分类模型的表现，特别适合对部分类别有特殊要求的实际场景。

Abstract: We propose BiCDO (Bias-Controlled Class Distribution Optimizer), an
iterative, data-centric framework that identifies Pareto optimized class
distributions for multi-class image classification. BiCDO enables performance
prioritization for specific classes, which is useful in safety-critical
scenarios (e.g. prioritizing 'Human' over 'Dog'). Unlike uniform distributions,
BiCDO determines the optimal number of images per class to enhance reliability
and minimize bias and variance in the objective function. BiCDO can be
incorporated into existing training pipelines with minimal code changes and
supports any labelled multi-class dataset. We have validated BiCDO using
EfficientNet, ResNet and ConvNeXt on CIFAR-10 and iNaturalist21 datasets,
demonstrating improved, balanced model performance through optimized data
distribution.

</details>


### [79] [MVQA-68K: A Multi-dimensional and Causally-annotated Dataset with Quality Interpretability for Video Assessment](https://arxiv.org/abs/2509.11589)
*Yanyun Pu,Kehan Li,Zeyi Huang,Zhijie Zhong,Kaixiang Yang*

Main category: cs.CV

TL;DR: 本文提出了MVQA-68K，一个包含68000多条多维度标注的视频质量评估（VQA）数据集，通过链式推理提供更具解释性和全面性的视频质量评分。实验显示，该数据集大幅提升了多模态大模型在VQA任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 随着视频生成模型（如Sora）的进步，视频数据集规模不断扩大，亟需高质量、具解释性的视频选择标准。现有单一分数制VQA方法在全面性和解释性上存在不足。

Method: 构建MVQA-68K数据集，涵盖七个关键质量维度，并为每条标注配备详细的链式推理。对多模态大模型进行训练，并在多个公开基准和内部测试集上评测性能。

Result: MVQA-68K显著提升了多模态大模型（MLLMs）在VQA上的表现，在内部测试集及LSVQ、LIVE-VQC等公开数据集上均取得了最新最优结果。显式推理过程还能显著增强零样本泛化能力。

Conclusion: MVQA-68K是一个更全面、具解释性的视频质量评估工具，有效促进了MLLMs评估视频质量的能力；代码与数据集将开放，推动相关研究发展。

Abstract: With the rapid advancement of video generation models such as Sora, video
quality assessment (VQA) is becoming increasingly crucial for selecting
high-quality videos from large-scale datasets used in pre-training. Traditional
VQA methods, typically producing single numerical scores, often lack
comprehensiveness and interpretability. To address these challenges, we
introduce MVQA-68K, a novel multi-dimensional VQA dataset comprising over
68,000 carefully annotated videos, covering seven essential quality dimensions:
overall aesthetics, camera movement, dynamic degree, texture detail,
composition, visual quality, and factual consistency. Each annotation includes
detailed chain-of-thought reasoning to facilitate interpretability and
comprehensive understanding. Extensive experiments demonstrate that MVQA-68K
significantly enhances the performance of various multimodal large language
models (MLLMs) on the VQA task, achieving state-of-the-art results not only on
our internal test set (Fig.1) but also on public benchmarks including
LSVQ-test, LSVQ-1080p, and LIVE-VQC. Meantime, incorporating explicit reasoning
process during VQA training substantially boosts the zero-shot generalization.
Code and dataset will be available at github:
https://github.com/Controller01-ai/MVQA-68K

</details>


### [80] [Disentangling Content from Style to Overcome Shortcut Learning: A Hybrid Generative-Discriminative Learning Framework](https://arxiv.org/abs/2509.11598)
*Siming Fu,Sijun Dong,Xiaoliang Meng*

Main category: cs.CV

TL;DR: 本文指出，当前自监督学习（SSL）因模型依赖表层特征（如纹理）而导致泛化能力不足，即捷径学习问题。作者提出一种新的混合生成-判别框架HyGDL，通过内容-风格显式解耦，从根本上解决这一系统性问题。


<details>
  <summary>Details</summary>
Motivation: 目前自监督学习模型容易捕捉表层甚至无关的特征（如风格、纹理），而不是数据的本质结构。这导致模型在新领域上表现不佳，急需找到解决捷径学习的根本方法。

Method: 作者提出了新的HyGDL（Hybrid Generative-Discriminative Learning Framework）框架。其核心思想是基于不变性预训练原则，通过系统性地改变输入中的某种偏差（如风格），保持监督信号恒定，从而迫使模型学习到对风格不变的本质内容。具体实现为使用单一编码器，并通过向量投影将风格和内容部分在表征空间中显式解耦。

Result: 通过实验验证，HyGDL在生成式自监督学习范式下能有效降低模型对表层特征的依赖，提高了模型在未见领域上的泛化能力。

Conclusion: HyGDL可从机制上解决自监督学习中的捷径学习问题，实现内容与风格的解耦，对提升模型泛化能力具有显著意义。

Abstract: Despite the remarkable success of Self-Supervised Learning (SSL), its
generalization is fundamentally hindered by Shortcut Learning, where models
exploit superficial features like texture instead of intrinsic structure. We
experimentally verify this flaw within the generative paradigm (e.g., MAE) and
argue it is a systemic issue also affecting discriminative methods, identifying
it as the root cause of their failure on unseen domains. While existing methods
often tackle this at a surface level by aligning or separating domain-specific
features, they fail to alter the underlying learning mechanism that fosters
shortcut dependency. To address this at its core, we propose HyGDL (Hybrid
Generative-Discriminative Learning Framework), a hybrid framework that achieves
explicit content-style disentanglement. Our approach is guided by the
Invariance Pre-training Principle: forcing a model to learn an invariant
essence by systematically varying a bias (e.g., style) at the input while
keeping the supervision signal constant. HyGDL operates on a single encoder and
analytically defines style as the component of a representation that is
orthogonal to its style-invariant content, derived via vector projection.

</details>


### [81] [DUAL-VAD: Dual Benchmarks and Anomaly-Focused Sampling for Video Anomaly Detection](https://arxiv.org/abs/2509.11605)
*Seoik Jung,Taekyung Song,Joshua Jordan Daniel,JinYoung Lee,SungJun Lee*

Main category: cs.CV

TL;DR: 该论文提出了一种用于视频异常检测（VAD）的新型基准与采样方法，提升了模型在帧级与视频级任务上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的VAD基准主要限定在帧级或视频级任务，无法全面评估模型的泛化能力，因此需要新的基准和方法。

Method: 提出了基于softmax的帧分配策略，该策略优先选择异常密集的片段，同时覆盖整个视频，实现了不同时间尺度下的均衡采样，并基于此方法构建了帧级与视频级两个互补性基准。帧级基准关注代表性帧的推理能力，视频级基准则针对局部片段进行异常评分。

Result: 在UCF-Crime数据集上实验显示，该方法在帧与视频两个层面均有提升，消融实验表明异常聚焦采样明显优于均匀采样和随机采样。

Conclusion: 软聚焦异常采样及对应的基准能够更全面、客观地评价VAD模型，有助于促进异常检测领域模型的发展与比较。

Abstract: Video Anomaly Detection (VAD) is critical for surveillance and public safety.
However, existing benchmarks are limited to either frame-level or video-level
tasks, restricting a holistic view of model generalization. This work first
introduces a softmax-based frame allocation strategy that prioritizes
anomaly-dense segments while maintaining full-video coverage, enabling balanced
sampling across temporal scales. Building on this process, we construct two
complementary benchmarks. The image-based benchmark evaluates frame-level
reasoning with representative frames, while the video-based benchmark extends
to temporally localized segments and incorporates an abnormality scoring
task.Experiments on UCF-Crime demonstrate improvements at both the frame and
video levels, and ablation studies confirm clear advantages of anomaly-focused
sampling over uniform and random baselines.

</details>


### [82] [A Controllable 3D Deepfake Generation Framework with Gaussian Splatting](https://arxiv.org/abs/2509.11624)
*Wending Liu,Siyun Liang,Huy H. Nguyen,Isao Echizen*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯散射（Gaussian Splatting）的新型3D Deepfake生成框架，实现了真实且可控的三维人脸替换与表情重现，突破了传统2D Deepfake在几何一致性和多视角上的局限。


<details>
  <summary>Details</summary>
Motivation: 传统2D Deepfake方法存在几何不一致、难以适应新视角的问题，难以做到真正一致、可控的三维人脸伪造。随着3D视效和沉浸式应用需求增加，亟需突破现有限制，实现高质量的3D可控Deepfake。

Method: 结合参数化头部模型和动态高斯点云表示，实现多视角一致的渲染、精确表情控制及背景的无缝集成。创新地将头部与背景高斯分离，利用预训练2D指导优化多视角面部区域，并设计修复模块增强极端姿态和表情时的视觉一致性。

Result: 在NeRSemble和其他评测视频上，方法在人脸身份保持、姿态与表情一致性方面与最优2D方法相当，但在多视角渲染质量和三维一致性上显著优于它们。

Conclusion: 该方法弥合了三维建模与Deepfake合成的鸿沟，为场景感知、可控且沉浸式的视觉伪造开启新方向，并揭示了3D高斯点渲染技术在伪造和安全领域的潜在威胁。

Abstract: We propose a novel 3D deepfake generation framework based on 3D Gaussian
Splatting that enables realistic, identity-preserving face swapping and
reenactment in a fully controllable 3D space. Compared to conventional 2D
deepfake approaches that suffer from geometric inconsistencies and limited
generalization to novel view, our method combines a parametric head model with
dynamic Gaussian representations to support multi-view consistent rendering,
precise expression control, and seamless background integration. To address
editing challenges in point-based representations, we explicitly separate the
head and background Gaussians and use pre-trained 2D guidance to optimize the
facial region across views. We further introduce a repair module to enhance
visual consistency under extreme poses and expressions. Experiments on
NeRSemble and additional evaluation videos demonstrate that our method achieves
comparable performance to state-of-the-art 2D approaches in identity
preservation, as well as pose and expression consistency, while significantly
outperforming them in multi-view rendering quality and 3D consistency. Our
approach bridges the gap between 3D modeling and deepfake synthesis, enabling
new directions for scene-aware, controllable, and immersive visual forgeries,
revealing the threat that emerging 3D Gaussian Splatting technique could be
used for manipulation attacks.

</details>


### [83] [IS-Diff: Improving Diffusion-Based Inpainting with Better Initial Seed](https://arxiv.org/abs/2509.11638)
*Yongzhe Lyu,Yu Wu,Yutian Lin,Bo Du*

Main category: cs.CV

TL;DR: 论文提出了一种无需重新训练的扩散模型（IS-Diff），通过改进初始噪声种子提升图像修复的连贯性和一致性。


<details>
  <summary>Details</summary>
Motivation: 在现有扩散模型的自由修复任务中，由于初始噪声随机，可能导致掩膜区域的语义不一致，影响修复质量。

Method: 该方法从未被掩盖区域采样初始噪声，模拟掩膜区域的数据分布，并引入动态自适应机制，自动检测并调整中间潜在空间的不协调修复。

Result: 在CelebA-HQ、ImageNet和Places2三大数据集（包括常规和大范围遮挡）上的实验表明，该方法在所有指标上都优于现有主流修复方法。

Conclusion: IS-Diff无需重新训练，仅通过改进初始噪声采样即可大幅提升扩散模型修复效果，表现出良好的通用性和有效性。

Abstract: Diffusion models have shown promising results in free-form inpainting. Recent
studies based on refined diffusion samplers or novel architectural designs led
to realistic results and high data consistency. However, random initialization
seed (noise) adopted in vanilla diffusion process may introduce mismatched
semantic information in masked regions, leading to biased inpainting results,
e.g., low consistency and low coherence with the other unmasked area. To
address this issue, we propose the Initial Seed refined Diffusion Model
(IS-Diff), a completely training-free approach incorporating distributional
harmonious seeds to produce harmonious results. Specifically, IS-Diff employs
initial seeds sampled from unmasked areas to imitate the masked data
distribution, thereby setting a promising direction for the diffusion
procedure. Moreover, a dynamic selective refinement mechanism is proposed to
detect severe unharmonious inpaintings in intermediate latent and adjust the
strength of our initialization prior dynamically. We validate our method on
both standard and large-mask inpainting tasks using the CelebA-HQ, ImageNet,
and Places2 datasets, demonstrating its effectiveness across all metrics
compared to state-of-the-art inpainting methods.

</details>


### [84] [WeatherBench: A Real-World Benchmark Dataset for All-in-One Adverse Weather Image Restoration](https://arxiv.org/abs/2509.11642)
*Qiyuan Guan,Qianfeng Yang,Xiang Chen,Tianyu Song,Guiyue Jin,Jiyu Jin*

Main category: cs.CV

TL;DR: 本文提出了一个真实世界的全场景恶劣天气图像修复数据集，填补了领域内大规模真实数据集的空白。


<details>
  <summary>Details</summary>
Motivation: 目前多天气图像修复方法主要依赖混合合成数据集，这类数据集在分辨率、风格、领域特性等方面差异较大，真实世界大规模数据集缺失，限制了模型的研究和公平评测。

Method: 作者采集了包含多种天气（如雨、雪、雾）与复杂户外场景及照明条件下的真实图像对，包含降质与对应干净图像，进行精准配准，建立高质量监督学习与评测基准。并在数据集上系统评测了各类修复方法。

Result: 实验展示了现有各类单任务、多任务及全能型方法在新数据集上的表现，为后续研究提供了直观基线。

Conclusion: 该数据集为更健壮、实用的全场景图像修复方法研究与应用打下了坚实基础，并已公开发布。

Abstract: Existing all-in-one image restoration approaches, which aim to handle
multiple weather degradations within a single framework, are predominantly
trained and evaluated using mixed single-weather synthetic datasets. However,
these datasets often differ significantly in resolution, style, and domain
characteristics, leading to substantial domain gaps that hinder the development
and fair evaluation of unified models. Furthermore, the lack of a large-scale,
real-world all-in-one weather restoration dataset remains a critical bottleneck
in advancing this field. To address these limitations, we present a real-world
all-in-one adverse weather image restoration benchmark dataset, which contains
image pairs captured under various weather conditions, including rain, snow,
and haze, as well as diverse outdoor scenes and illumination settings. The
resulting dataset provides precisely aligned degraded and clean images,
enabling supervised learning and rigorous evaluation. We conduct comprehensive
experiments by benchmarking a variety of task-specific, task-general, and
all-in-one restoration methods on our dataset. Our dataset offers a valuable
foundation for advancing robust and practical all-in-one image restoration in
real-world scenarios. The dataset has been publicly released and is available
at https://github.com/guanqiyuan/WeatherBench.

</details>


### [85] [Joint-octamamba:an octa joint segmentation network based on feature enhanced mamba](https://arxiv.org/abs/2509.11649)
*Chuang Liu,Nan Guo*

Main category: cs.CV

TL;DR: 该论文提出了一种新型的OCTA图像分析模型Joint-OCTAMamba，能够提升视网膜血管与无血管区联合分割的准确率，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有二维OCTA视网膜血管分割技术准确率有限，并且联合分割模型在不同任务间表现不均衡，影响临床应用效果。

Method: 论文提出RVMamba架构，将多特征提取模块与Mamba状态空间模型结合。针对视网膜无血管区分割表现不足，进一步提出FAZMamba模块，并将两者合成一个联合分割模型Joint-OCTAMamba。

Result: 在OCTA-500公开数据集上，Joint-OCTAMamba在各种评价指标上均超过了现有最佳模型。

Conclusion: Joint-OCTAMamba框架显著提升了OCTA图像的血管与无血管区的分割表现，有望推动该领域的临床应用。

Abstract: OCTA is a crucial non-invasive imaging technique for diagnosing and
monitoring retinal diseases like diabetic retinopathy, age-related macular
degeneration, and glaucoma. Current 2D-based methods for retinal vessel (RV)
segmentation offer insufficient accuracy. To address this, we propose RVMamba,
a novel architecture integrating multiple feature extraction modules with the
Mamba state-space model. Moreover, existing joint segmentation models for OCTA
data exhibit performance imbalance between different tasks. To simultaneously
improve the segmentation of the foveal avascular zone (FAZ) and mitigate this
imbalance, we introduce FAZMamba and a unified Joint-OCTAMamba framework.
Experimental results on the OCTA-500 dataset demonstrate that Joint-OCTAMamba
outperforms existing models across evaluation metrics.The code is available at
https://github.com/lc-sfis/Joint-OCTAMamba.

</details>


### [86] [DTGen: Generative Diffusion-Based Few-Shot Data Augmentation for Fine-Grained Dirty Tableware Recognition](https://arxiv.org/abs/2509.11661)
*Lifei Hao,Yue Cheng,Baoqi Huang,Bing Jia,Xuandong Zhao*

Main category: cs.CV

TL;DR: 本文提出DTGen，一种利用生成式扩散模型进行少样本数据增强的方法，以提升智能餐具清洁中的细粒度污染识别能力。仅需极少真实样本，即可合成高质量训练数据，从而显著提升分类器性能。


<details>
  <summary>Details</summary>
Motivation: 智能餐具清洁关乎食品安全和智慧家庭应用，现有方法因识别类别粗糙、缺乏少样本数据，难以满足工业化精细需求。作者为解决细粒度识别和训练数据不足的问题，推动行业智能化升级，提出了DTGen方案。

Method: DTGen利用生成式扩散模型，通过LoRA进行高效领域适配，结合结构化提示多样生成污染餐具图像，并用CLIP做跨模态质量过滤。即使真实少样本条件下，也能合成丰富高质量新样本以提升分类性能。

Result: 实验表明，在极少真实样本条件下，DTGen生成的大量高质量图像显著提升了餐具污染细粒度分类准确率，支持应用于实际的智能清洁和食品安全场景。

Conclusion: DTGen验证了生成式AI在工业少样本视觉领域的实用价值，为自动化餐具清洁和食品安全监测提供了高效的数据增强与轻量化部署路径，在嵌入式场景中具有很强的应用前景。

Abstract: Intelligent tableware cleaning is a critical application in food safety and
smart homes, but existing methods are limited by coarse-grained classification
and scarcity of few-shot data, making it difficult to meet industrialization
requirements. We propose DTGen, a few-shot data augmentation scheme based on
generative diffusion models, specifically designed for fine-grained dirty
tableware recognition. DTGen achieves efficient domain specialization through
LoRA, generates diverse dirty images via structured prompts, and ensures data
quality through CLIP-based cross-modal filtering. Under extremely limited real
few-shot conditions, DTGen can synthesize virtually unlimited high-quality
samples, significantly improving classifier performance and supporting
fine-grained dirty tableware recognition. We further elaborate on lightweight
deployment strategies, promising to transfer DTGen's benefits to embedded
dishwashers and integrate with cleaning programs to intelligently regulate
energy consumption and detergent usage. Research results demonstrate that DTGen
not only validates the value of generative AI in few-shot industrial vision but
also provides a feasible deployment path for automated tableware cleaning and
food safety monitoring.

</details>


### [87] [MindVL: Towards Efficient and Effective Training of Multimodal Large Language Models on Ascend NPUs](https://arxiv.org/abs/2509.11662)
*Feilong Chen,Yijiang Liu,Yi Huang,Hao Wang,Miren Tian,Ya-Qi Yu,Minghui Liao,Jihao Wu*

Main category: cs.CV

TL;DR: 本文提出了MindVL，一种在Ascend NPU上训练的多模态大语言模型，特点是采用原生分辨率的Vision Transformer，并通过高效的训练框架和优化手段，性能媲美Qwen2.5-VL，同时训练数据仅为其1/10。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型大多采用固定分辨率处理视觉输入，导致对复杂图片及细节内容的处理效果下降，因此作者希望通过原生分辨率支持与高效训练机制，提升模型对高密度视觉内容的理解能力。

Method: MindVL采用原生分辨率的Vision Transformer，避免分辨率下采样的细节损失。在Ascend NPU上训练，开发了专门的Mindspeed-MLLM分布式多模态训练框架，并对部分算子进行等价替换以保证精度。训练流程分为三阶段：预热、多任务训练、监督式指令微调，辅以多模态数据打包与混合并行加速。此外，还引入了测试时分辨率搜索与模型权重平均等优化手段。

Result: MindVL在通用多模态理解、文档/表格理解等评测上与Qwen2.5-VL表现相当，且在OCR测试中表现领先，而其训练所用数据量仅为Qwen2.5-VL的十分之一。

Conclusion: MindVL在仅使用较少训练数据、硬件高效支持和一系列优化下，达到了主流大模型性能，并在部分任务超越了竞争对手，展示了原生分辨率视觉建模和高效分布式训练的价值。

Abstract: We propose MindVL, a multimodal large langauge model trained on Ascend NPUs.
Similar to Qwen2.5-VL, MindVL adopts native-resolution Vision Transformers,
which enables it to process images at their original variable resolutions. This
design avoids the degradation caused by fixed-resolution tiling while
preserving fine-grained details and global layouts, which is crucial for
visually dense content such as complex charts and diagrams. To ensure the
smooth training of MindVL on Ascend NPUs, we develop Mindspeed-MLLM, a
distributed multimodal training framework tailored for Ascend NPUs. To maintain
training accuracy, we implement equivalent replacements for certain operators.
MindVL undergoes a three-phase training process, namely the warm-up phase,
multitask training phase, and supervised instruction tuning phase, to gradually
enhance its capabilities. This process starts with basic visual and multimodal
pre-training, followed by large-scale multiask trainging and instruction
tuning. We also adopt multimodal data packaging and hybrid parallelism
techniques, which significantly improve end-to-end training speed. To further
boost model performance, we specifically introduce test-time resolution search
and model weight averaging. Notably, despite using about 1/10 of the training
data required by Qwen2.5-VL, MindVL achieves performance on par with Qwen2.5-VL
in evaluations of general multimodal understanding and document/table
comprehension. Beyond overall scores, MindVL also delivers leading performance
in OCR assessments.

</details>


### [88] [RouteExtract: A Modular Pipeline for Extracting Routes from Paper Maps](https://arxiv.org/abs/2509.11674)
*Bjoern Kremser,Yusuke Matsui*

Main category: cs.CV

TL;DR: 该论文提出了一种从纸质地图中自动提取可导航路线的方法，使其能应用于GPS导航。作者结合了地理参考定位、U-Net分割、图结构生成以及基于路径引擎的迭代优化。实验表明，该流程能适用于多种不同风格的地图，并输出实用的GPS路线。


<details>
  <summary>Details</summary>
Motivation: 纸质地图因包含本地化的步道和注释，在徒步和观光中依然有重要价值，但现有数字导航应用经常缺乏这些信息。研究动机是在保留纸质地图特色内容的前提下，使其可为现代GPS导航使用。

Method: 方法包括四个步骤：对扫描地图进行地理配准，利用U-Net网络进行二值分割提取步道，构建步道网络图，并通过路径引擎反复优化结果，以提取可导航路线。

Result: 实验评估显示，所提流程对多种风格的地图表现鲁棒，能够稳定地还原步道网络，并生成适合实用的GPS导航路线。

Conclusion: 论文证明，将纸质地图信息自动数字化用于GPS导航是可行的，提出的方法能有效扩展现有数字地图应用的内容和功能。

Abstract: Paper maps remain widely used for hiking and sightseeing because they contain
curated trails and locally relevant annotations that are often missing from
digital navigation applications such as Google Maps. We propose a pipeline to
extract navigable trails from scanned maps, enabling their use in GPS-based
navigation. Our method combines georeferencing, U-Net-based binary
segmentation, graph construction, and an iterative refinement procedure using a
routing engine. We evaluate the full end-to-end pipeline as well as individual
components, showing that the approach can robustly recover trail networks from
diverse map styles and generate GPS routes suitable for practical use.

</details>


### [89] [IMD: A 6-DoF Pose Estimation Benchmark for Industrial Metallic Objects](https://arxiv.org/abs/2509.11680)
*Ruimin Ma,Sebastian Zudaire,Zhen Li,Chi Zhang*

Main category: cs.CV

TL;DR: 本文提出了一个专为工业场景设计的新型金属物体6D姿态估计数据集（IMD），以推动相关算法在工业环境下的泛化能力提升。


<details>
  <summary>Details</summary>
Motivation: 现有6D姿态估计算法和数据集多聚焦于有丰富纹理和低反射率的日常物体，难以推广到金属、高反光、无纹理的工业物体环境，这严重限制了机器人在真实工业场景下的应用。

Method: 作者构建了一个包含45个真实工业零部件的新数据集，使用RGB-D相机在自然室内光照和多样化摆放条件下采集图像，支持视频分割、6D姿态跟踪和单次拍摄6D姿态估计三项任务，并用典型算法对基线性能进行评测。

Result: 实验结果显示，该工业金属数据集对现有主流算法的挑战性显著高于现有家居类数据集，算法在工业场景下表现有明显下降。

Conclusion: 该数据集填补了工业领域6D姿态估计基准的空白，为未来专注于工业机器人感知算法的研究和对比提供了标准平台，有助于推动该领域技术进步。

Abstract: Object 6DoF (6D) pose estimation is essential for robotic perception,
especially in industrial settings. It enables robots to interact with the
environment and manipulate objects. However, existing benchmarks on object 6D
pose estimation primarily use everyday objects with rich textures and
low-reflectivity, limiting model generalization to industrial scenarios where
objects are often metallic, texture-less, and highly reflective. To address
this gap, we propose a novel dataset and benchmark namely \textit{Industrial
Metallic Dataset (IMD)}, tailored for industrial applications. Our dataset
comprises 45 true-to-scale industrial components, captured with an RGB-D camera
under natural indoor lighting and varied object arrangements to replicate
real-world conditions. The benchmark supports three tasks, including video
object segmentation, 6D pose tracking, and one-shot 6D pose estimation. We
evaluate existing state-of-the-art models, including XMem and SAM2 for
segmentation, and BundleTrack and BundleSDF for pose estimation, to assess
model performance in industrial contexts. Evaluation results show that our
industrial dataset is more challenging than existing household object datasets.
This benchmark provides the baseline for developing and comparing segmentation
and pose estimation algorithms that better generalize to industrial robotics
scenarios.

</details>


### [90] [Uncertainty-Aware Retinal Vessel Segmentation via Ensemble Distillation](https://arxiv.org/abs/2509.11689)
*Jeremiah Fadugba,Petru Manescu,Bolanle Oladejo,Delmiro Fernandez-Reyes,Philipp Berens*

Main category: cs.CV

TL;DR: 本文提出将多模型集成（Deep Ensembles）知识蒸馏到单个模型中，实现对视网膜血管分割的不确定性估计，并大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割（尤其是视网膜血管分析）对准确性和不确定性的要求极高，而现有的深度集成方法虽然表现好，但计算开销大。作者希望在保留集成模型表现的同时，降低成本。

Method: 通过知识蒸馏（Ensemble Distillation），将多个单独训练的集成模型的知识迁移并压缩到一个模型中。这样既保证了分割性能，也便于部署和推理，具体在DRIVE和FIVES两个数据集上进行了实验。

Result: 实验证明，知识蒸馏后的单一模型在分割和校准指标上与原集成模型性能相当，但显著降低了训练和测试的计算复杂度。

Conclusion: Ensemble Distillation是一种高效且稳健的不确定性估计方案，能在大幅减少资源消耗的同时，保持医疗影像分割的高可靠性，具备实际应用前景。

Abstract: Uncertainty estimation is critical for reliable medical image segmentation,
particularly in retinal vessel analysis, where accurate predictions are
essential for diagnostic applications. Deep Ensembles, where multiple networks
are trained individually, are widely used to improve medical image segmentation
performance. However, training and testing costs increase with the number of
ensembles. In this work, we propose Ensemble Distillation as a robust
alternative to commonly used uncertainty estimation techniques by distilling
the knowledge of multiple ensemble models into a single model. Through
extensive experiments on the DRIVE and FIVES datasets, we demonstrate that
Ensemble Distillation achieves comparable performance via calibration and
segmentation metrics, while significantly reducing computational complexity.
These findings suggest that Ensemble distillation provides an efficient and
reliable approach for uncertainty estimation in the segmentation of the retinal
vessels, making it a promising tool for medical imaging applications.

</details>


### [91] [The Quest for Universal Master Key Filters in DS-CNNs](https://arxiv.org/abs/2509.11711)
*Zahra Babaiee,Peyman M. Kiassari,Daniela Rus,Radu Grosu*

Main category: cs.CV

TL;DR: 本文提出在深度可分离卷积神经网络（DS-CNN）中，所有滤波器实际上都可以由8个通用的“万能滤波器”线性迁移得到。这些滤波器不仅能够在多个数据集和网络架构中使用，还可以在大多数任务上保持高准确率。


<details>
  <summary>Details</summary>
Motivation: 以往研究发现CNN有“主滤波器”的假说，但没有具体化。本文动机是是否能找到更精简、更通用的过滤器集合，并验证其普适性。

Method: 作者通过无监督搜索，分析不同结构和数据集下DS-CNN滤波器，发现大部分滤波器都是对8个基本滤波器的线性变化。这些主滤波器多为高斯、差分高斯及其导数。然后，用这8个固定滤波器初始化网络进行训练和测试。

Result: 利用这8个冻结滤波器初始化的网络，ImageNet准确率超过80%，并在小数据集时优于常规模型。所有提取的主滤波器与生物视觉系统中的感受野十分相似。

Conclusion: 深度可分离卷积层天然趋向于这8种基本空间算子，过滤器冗余度大幅降低。这一发现揭示了网络泛化和迁移学习的新机理，也为网络结构设计提供了新启发。

Abstract: A recent study has proposed the "Master Key Filters Hypothesis" for
convolutional neural network filters. This paper extends this hypothesis by
radically constraining its scope to a single set of just 8 universal filters
that depthwise separable convolutional networks inherently converge to. While
conventional DS-CNNs employ thousands of distinct trained filters, our analysis
reveals these filters are predominantly linear shifts (ax+b) of our discovered
universal set. Through systematic unsupervised search, we extracted these
fundamental patterns across different architectures and datasets. Remarkably,
networks initialized with these 8 unique frozen filters achieve over 80%
ImageNet accuracy, and even outperform models with thousands of trainable
parameters when applied to smaller datasets. The identified master key filters
closely match Difference of Gaussians (DoGs), Gaussians, and their derivatives,
structures that are not only fundamental to classical image processing but also
strikingly similar to receptive fields in mammalian visual systems. Our
findings provide compelling evidence that depthwise convolutional layers
naturally gravitate toward this fundamental set of spatial operators regardless
of task or architecture. This work offers new insights for understanding
generalization and transfer learning through the universal language of these
master key filters.

</details>


### [92] [Advanced Layout Analysis Models for Docling](https://arxiv.org/abs/2509.11720)
*Nikolaos Livathinos,Christoph Auer,Ahmed Nassar,Rafael Teixeira de Lima,Maksym Lysak,Brown Ebouky,Cesar Berrospi,Michele Dolfi,Panagiotis Vagenas,Matteo Omenetti,Kasper Dinkla,Yusik Kim,Valery Weber,Lucas Morin,Ingmar Meijer,Viktor Kuropiatnyk,Tim Strohmeyer,A. Said Gurbuz,Peter W. J. Staar*

Main category: cs.CV

TL;DR: 本文开发了新的文档版面分析模型，并集成进Docling文档转换流程。在多个主流检测架构和15万份文档上训练，显著提升了检测效果和推理速度，并将模型、代码、文档全部开源。


<details>
  <summary>Details</summary>
Motivation: 现有Docling文档转换系统的版面分析模型效果有限，检测精度和速度均有提升空间。该工作希望通过最新的检测网络和大规模多样数据集，显著提升文档版面分析的准确率和效率。

Method: 采用RT-DETR、RT-DETRv2和DFINE等最新目标检测网络，在15万份多样化的文档数据上训练模型。通过针对检测结果的后处理，使其更适合文档转换实际场景。模型在不同硬件环境（CPU、Nvidia、Apple GPU）下进行了速度与准确性的全面评估。

Result: 开发了五种新模型，在文档版面分析基准上相较旧版提升了20.6%-23.9%的mAP。最佳模型heron-101在NVIDIA A100 GPU上一张图片仅需28ms，mAP达到78%。

Conclusion: 新提出的文档版面分析模型在精度和速度上大幅超越原有基线，并给文档版面检测任务提供了训练和部署上的最佳实践建议，推动文档转换领域发展。所有资源已在HuggingFace上开源，便于业界应用和持续优化。

Abstract: This technical report documents the development of novel Layout Analysis
models integrated into the Docling document-conversion pipeline. We trained
several state-of-the-art object detectors based on the RT-DETR, RT-DETRv2 and
DFINE architectures on a heterogeneous corpus of 150,000 documents (both openly
available and proprietary). Post-processing steps were applied to the raw
detections to make them more applicable to the document conversion task. We
evaluated the effectiveness of the layout analysis on various document
benchmarks using different methodologies while also measuring the runtime
performance across different environments (CPU, Nvidia and Apple GPUs). We
introduce five new document layout models achieving 20.6% - 23.9% mAP
improvement over Docling's previous baseline, with comparable or better
runtime. Our best model, "heron-101", attains 78% mAP with 28 ms/image
inference time on a single NVIDIA A100 GPU. Extensive quantitative and
qualitative experiments establish best practices for training, evaluating, and
deploying document-layout detectors, providing actionable guidance for the
document conversion community. All trained checkpoints, code, and documentation
are released under a permissive license on HuggingFace.

</details>


### [93] [Microsurgical Instrument Segmentation for Robot-Assisted Surgery](https://arxiv.org/abs/2509.11727)
*Tae Kyeong Jeong,Garam Kim,Juyoun Park*

Main category: cs.CV

TL;DR: 本文提出了MISRA分割框架，通过增强输入通道、结构创新和反馈机制，提升了显微外科器械分割的准确性，并提供了标注精细的新数据集。实验表明，模型在多个评测指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 显微外科场景中的细微结构分割受限于分辨率、对比度低和类别不平衡，现有方法难以准确分割。这对于机器人辅助手术的场景理解极为关键，因此需要新方法提升分割效果。

Method: 提出了MISRA框架，采用RGB与亮度通道结合，加入跳跃注意力以保留细长特征，并使用迭代反馈模块提升结构连续性。同时，发布了包含细致标注的新显微外科数据集。

Result: MISRA模型平均类别IoU比对比方法提升5.37%，在器械接触与重叠区域分割更稳定，整体性能有显著优势。

Conclusion: MISRA是实现计算机辅助和机器人显微外科场景解析的有力方法，能为实际应用提供更可靠分割，推动领域发展。

Abstract: Accurate segmentation of thin structures is critical for microsurgical scene
understanding but remains challenging due to resolution loss, low contrast, and
class imbalance. We propose Microsurgery Instrument Segmentation for Robotic
Assistance(MISRA), a segmentation framework that augments RGB input with
luminance channels, integrates skip attention to preserve elongated features,
and employs an Iterative Feedback Module(IFM) for continuity restoration across
multiple passes. In addition, we introduce a dedicated microsurgical dataset
with fine-grained annotations of surgical instruments including thin objects,
providing a benchmark for robust evaluation Dataset available at
https://huggingface.co/datasets/KIST-HARILAB/MISAW-Seg. Experiments demonstrate
that MISRA achieves competitive performance, improving the mean class IoU by
5.37% over competing methods, while delivering more stable predictions at
instrument contacts and overlaps. These results position MISRA as a promising
step toward reliable scene parsing for computer-assisted and robotic
microsurgery.

</details>


### [94] [Bridging the Gap Between Sparsity and Redundancy: A Dual-Decoding Framework with Global Context for Map Inference](https://arxiv.org/abs/2509.11731)
*Yudong Shen,Wenyu Wu,Jiali Mao,Yixiao Tong,Guoping Liu,Chaoya Wang*

Main category: cs.CV

TL;DR: 本文提出了DGMap，一种具备全局上下文感知能力的双解码框架，有效提升了稀疏与密集轨迹区域自动地图推断的准确性，在实际数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动地图推断依赖于轨迹数据，但轨迹分布不均导致稀疏区域道路断裂、密集区域冗余路段，影响生成地图的质量。现有方法难以有效处理这一问题，因此亟需新的解决方案。

Method: 提出DGMap方法：包含多尺度网格编码、掩码增强关键点提取和全局上下文关系预测模块。该方法通过结合全局语义与局部几何特征，提升道路关键点检测，抑制密集区域虚假连接。

Result: 在三个真实数据集上评测，DGMap在APLS指标上较最新方法提升了5%，并在滴滴平台的轨迹数据上表现突出。

Conclusion: DGMap能够有效缓解轨迹数据稀疏与密集带来的地图推断难题，具备较强的实际应用前景，优于已发布的主流方法。

Abstract: Trajectory data has become a key resource for automated map in-ference due to
its low cost, broad coverage, and continuous availability. However, uneven
trajectory density often leads to frag-mented roads in sparse areas and
redundant segments in dense regions, posing significant challenges for existing
methods. To address these issues, we propose DGMap, a dual-decoding framework
with global context awareness, featuring Multi-scale Grid Encoding,
Mask-enhanced Keypoint Extraction, and Global Context-aware Relation
Prediction. By integrating global semantic context with local geometric
features, DGMap improves keypoint detection accuracy to reduce road
fragmentation in sparse-trajectory areas. Additionally, the Global
Context-aware Relation Prediction module suppresses false connections in
dense-trajectory regions by modeling long-range trajectory patterns.
Experimental results on three real-world datasets show that DGMap outperforms
state-of-the-art methods by 5% in APLS, with notable performance gains on
trajectory data from the Didi Chuxing platform

</details>


### [95] [A Fully Open and Generalizable Foundation Model for Ultrasound Clinical Applications](https://arxiv.org/abs/2509.11752)
*Hongyuan Zhang,Yuheng Wu,Mingyang Zhao,Zhiwei Chen,Rebecca Li,Fei Zhu,Haohan Zhao,Xiaohua Yuan,Meng Yang,Chunli Qiu,Xiang Cong,Haiyan Chen,Lina Luan,Randolph H. L. Wong,Huai Liao,Colin A Graham,Shi Chang,Guowei Tao,Dong Yi,Zhen Lei,Nassir Navab,Sebastien Ourselin,Jiebo Luo,Hongbin Liu,Gaofeng Meng*

Main category: cs.CV

TL;DR: 本文提出了EchoCare，一个通用的超声医学AI基础模型，利用大规模多源超声数据自监督学习，突破了以往任务专属模型泛化能力差和临床标注稀缺的难题，在多项典型超声任务中效果领先，并开放数据和模型供社区使用。


<details>
  <summary>Details</summary>
Motivation: 现实临床环境中，超声医学AI发展受限于标注数据稀缺及现有模型泛化能力不足，因此急需设计能够普适于多种超声任务、具有良好泛化性的通用基础模型，助力AI在超声领域的广泛临床应用。

Method: 作者自建并公开了包含450万张跨国、多设备、异质性强的超声图像数据集EchoCareData，采用自监督方法，通过创新的分层分类器，实现像素-特征两级信息的联合学习，从而更好地捕捉影像局部特征和全局解剖上下文，模型可便捷微调和扩展到多种任务。

Result: EchoCare模型在10项典型超声任务（如疾病诊断、病灶分割、器官检测、标志物定位、量化回归、影像增强、报告生成等）上，均以较小训练量超越既有SOTA模型，实现优异诊断和泛化表现。

Conclusion: EchoCare代表了完全开放、强泛化性的通用超声基础模型，将极大促进超声AI技术在临床各环节的开发与实际应用，其数据和代码公开亦便于全球研究者进一步拓展更多应用场景。

Abstract: Artificial intelligence (AI) that can effectively learn ultrasound
representations by integrating multi-source data holds significant promise for
advancing clinical care. However, the scarcity of large labeled datasets in
real-world clinical environments and the limited generalizability of
task-specific models have hindered the development of generalizable clinical AI
models for ultrasound applications. In this study, we present EchoCare, a novel
ultrasound foundation model for generalist clinical use, developed via
self-supervised learning on our curated, publicly available, large-scale
dataset EchoCareData. EchoCareData comprises 4.5 million ultrasound images,
sourced from over 23 countries across 5 continents and acquired via a diverse
range of distinct imaging devices, thus encompassing global cohorts that are
multi-center, multi-device, and multi-ethnic. Unlike prior studies that adopt
off-the-shelf vision foundation model architectures, we introduce a
hierarchical classifier into EchoCare to enable joint learning of pixel-level
and representation-level features, capturing both global anatomical contexts
and local ultrasound characteristics. With minimal training, EchoCare
outperforms state-of-the-art comparison models across 10 representative
ultrasound benchmarks of varying diagnostic difficulties, spanning disease
diagnosis, lesion segmentation, organ detection, landmark prediction,
quantitative regression, imaging enhancement and report generation. The code
and pretrained model are publicly released, rendering EchoCare accessible for
fine-tuning and local adaptation, supporting extensibility to additional
applications. EchoCare provides a fully open and generalizable foundation model
to boost the development of AI technologies for diverse clinical ultrasound
applications.

</details>


### [96] [Lost in Embeddings: Information Loss in Vision-Language Models](https://arxiv.org/abs/2509.11986)
*Wenyan Li,Raphael Tang,Chengzu Li,Caiqi Zhang,Ivan Vulić,Anders Søgaard*

Main category: cs.CV

TL;DR: 本文研究了视觉-语言模型（VLMs）中视觉特征通过投影器映射到语言空间时的信息损失，并提出了量化与分析该损失的新方法，发现投影步骤会严重扭曲原始图像特征结构，并影响模型表现。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉-语言模型需要通过投影器融合视觉和语言模态，但投影过程可能带来的信息损失及其对模型能力的直接影响此前缺乏系统研究。本文旨在填补这一空白，揭示投影步骤背后的机制和风险。

Method: 提出两种互补的分析方法：一是通过k近邻关系的变化，评估投影前后语义信息的保留情况；二是通过从投影空间逆向重建图像特征，直接测量和定位patch级别的信息损失。此外，结合视觉问答任务，对模型表现进行分析。

Result: 实验表明，投影过程会显著扰动视觉表征的局部几何结构，k近邻匹配度下降40-60%，与图像检索性能下降高度相关。Patch级别的嵌入重构揭示，信息损失高的区域也是模型推理易出错的区域。

Conclusion: 投影器在当前VLM架构中引入了较大信息损失，需引起重视。本文分析方法为后续改进模块设计、理解模型行为提供了新视角。

Abstract: Vision--language models (VLMs) often process visual inputs through a
pretrained vision encoder, followed by a projection into the language model's
embedding space via a connector component. While crucial for modality fusion,
the potential information loss induced by this projection step and its direct
impact on model capabilities remain understudied. We introduce two
complementary approaches to examine and quantify this loss by analyzing the
latent representation space. First, we evaluate semantic information
preservation by analyzing changes in k-nearest neighbor relationships between
image representations, before and after projection. Second, we directly measure
information loss by reconstructing visual embeddings from the projected
representation, localizing loss at an image patch level. Experiments reveal
that connectors substantially distort the local geometry of visual
representations, with k-nearest neighbors diverging by 40--60\%
post-projection, correlating with degradation in retrieval performance. The
patch-level embedding reconstruction provides interpretable insights for model
behavior on visually grounded question-answering tasks, finding that areas of
high information loss reliably predict instances where models struggle.

</details>


### [97] [MSMA: Multi-Scale Feature Fusion For Multi-Attribute 3D Face Reconstruction From Unconstrained Images](https://arxiv.org/abs/2509.11763)
*Danling Cao*

Main category: cs.CV

TL;DR: 该论文提出一种新方法，通过多尺度特征融合与多属性学习，实现对单张无约束图片的高精度3D人脸重建，有效提升在多样环境下的性能。


<details>
  <summary>Details</summary>
Motivation: 当前单张无约束图片的3D人脸重建受数据匮乏和复杂环境影响，难以准确捕捉细节和多尺度特征，导致重建效果有限。

Method: 提出了一种MSMA（多尺度多属性）框架，整合多尺度特征融合与多属性学习，并引入大核注意力模块增强特征提取，从而提升单张二维图片到三维人脸参数估算的准确性。

Result: 在MICC Florence、Facewarehouse及自建数据集上的实验显示，该方法在多种复杂条件下与最先进技术（SOTA）表现持平，部分情况下超过现有最佳方法。

Conclusion: MSMA框架能更好捕捉多尺度和多属性特征，提高单张无约束图片下的3D人脸重建准确性，是推动该领域进步的有效方案。

Abstract: Reconstructing 3D face from a single unconstrained image remains a
challenging problem due to diverse conditions in unconstrained environments.
Recently, learning-based methods have achieved notable results by effectively
capturing complex facial structures and details across varying conditions.
Consequently, many existing approaches employ projection-based losses between
generated and input images to constrain model training. However, learning-based
methods for 3D face reconstruction typically require substantial amounts of 3D
facial data, which is difficult and costly to obtain. Consequently, to reduce
reliance on labeled 3D face datasets, many existing approaches employ
projection-based losses between generated and input images to constrain model
training. Nonetheless, despite these advancements, existing approaches
frequently struggle to capture detailed and multi-scale features under diverse
facial attributes and conditions, leading to incomplete or less accurate
reconstructions. In this paper, we propose a Multi-Scale Feature Fusion with
Multi-Attribute (MSMA) framework for 3D face reconstruction from unconstrained
images. Our method integrates multi-scale feature fusion with a focus on
multi-attribute learning and leverages a large-kernel attention module to
enhance the precision of feature extraction across scales, enabling accurate 3D
facial parameter estimation from a single 2D image. Comprehensive experiments
on the MICC Florence, Facewarehouse and custom-collect datasets demonstrate
that our approach achieves results on par with current state-of-the-art
methods, and in some instances, surpasses SOTA performance across challenging
conditions.

</details>


### [98] [Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models](https://arxiv.org/abs/2509.12132)
*Pu Jian,Junhong Wu,Wei Sun,Chen Wang,Shuo Ren,Jiajun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉推理模型Reflection-V，通过引入视觉反思能力，显著提升了模型在视觉推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型（VLMs）在长推理过程中对视觉信息的关注度急剧下降，导致视觉推理模型（VRMs）缺乏有效的视觉反思能力，限制了推理效果。因此亟需提升VRMs在推理时自查视觉依据的能力。

Method: 作者提出了Reflection-V模型。首先，构建基于视觉的推理数据，通过设计智能体让VLMs和LLMs交互，实现了视觉反思能力的冷启动学习。其次，在强化学习阶段引入基于视觉注意力的奖励模型，激励模型更多关注视觉信息进行推理。

Result: Reflection-V在多个视觉推理基准测试上均取得了显著提升。并且推理过程中对视觉信息的依赖更强、更稳定，证明了视觉反思能力大幅增强。

Conclusion: Reflection-V能有效促进VRMs进行视觉反思，大幅提升视觉推理性能，为将慢思考推理能力迁移到多模态模型提供了有力方法。

Abstract: Recent advances in text-only "slow-thinking" reasoning have prompted efforts
to transfer this capability to vision-language models (VLMs), for training
visual reasoning models (\textbf{VRMs}). owever, such transfer faces critical
challenges: Effective "slow thinking" in VRMs requires \textbf{visual
reflection}, the ability to check the reasoning process based on visual
information. Through quantitative analysis, we observe that current VRMs
exhibit limited visual reflection, as their attention to visual information
diminishes rapidly with longer generated responses. To address this challenge,
we propose a new VRM \textbf{Reflection-V}, which enhances visual reflection
based on reasoning data construction for cold-start and reward design for
reinforcement learning (RL). Firstly, we construct vision-centered reasoning
data by leveraging an agent that interacts between VLMs and reasoning LLMs,
enabling cold-start learning of visual reflection patterns. Secondly, a visual
attention based reward model is employed during RL to encourage reasoning based
on visual information. Therefore, \textbf{Reflection-V} demonstrates
significant improvements across multiple visual reasoning benchmarks.
Furthermore, \textbf{Reflection-V} maintains a stronger and more consistent
reliance on visual information during visual reasoning, indicating effective
enhancement in visual reflection capabilities.

</details>


### [99] [Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for Zero-shot Generalization](https://arxiv.org/abs/2509.11772)
*Diogo Mendonça,Tiago Barros,Cristiano Premebida,Urbano J. Nunes*

Main category: cs.CV

TL;DR: 本文提出了一种结合预训练检测器、SAM2及创新的Seg2Track模块的新型MOTS（多目标跟踪与分割）框架——Seg2Track-SAM2，在无需微调、检测器无关的前提下，实现了先进的多目标跟踪与分割性能，且显著优化了内存占用。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型如SAM2在视频分割的零样本泛化能力很强，但直接应用于多目标跟踪与分割（MOTS）时存在身份管理不足和内存效率低的问题。因此，亟需一种能在高精度目标跟踪基础上提升身份一致性和资源利用率的新方法，支持自动系统在动态环境下的可靠运行。

Method: 作者设计了Seg2Track-SAM2框架，将预训练目标检测器、SAM2和新设计的Seg2Track模块结合，针对跟踪初始化、目标管理和信息强化进行优化。方法无需微调，通过滑动窗口式内存管理流程，大幅降低了内存使用量，同时保持高精度跟踪。该系统对检测器无特定要求，增强了通用性。

Result: 在KITTI MOT与KITTI MOTS评价集上，Seg2Track-SAM2在汽车与行人类别的总体排名均列第4，并在关联准确率（AssA）上取得新的基准表现。同时，滑动窗口内存策略使内存占用降低至原来的25%，性能损失可忽略。

Conclusion: Seg2Track-SAM2通过结合强大的零样本跟踪能力、优良的身份保持和高效的内存管理，推动了MOTS任务的性能极限。该方案尤其适合资源受限环境的实际部署，为多目标跟踪与分割技术发展树立了新基准。

Abstract: Autonomous systems require robust Multi-Object Tracking (MOT) capabilities to
operate reliably in dynamic environments. MOT ensures consistent object
identity assignment and precise spatial delineation. Recent advances in
foundation models, such as SAM2, have demonstrated strong zero-shot
generalization for video segmentation, but their direct application to MOTS
(MOT+Segmentation) remains limited by insufficient identity management and
memory efficiency. This work introduces Seg2Track-SAM2, a framework that
integrates pre-trained object detectors with SAM2 and a novel Seg2Track module
to address track initialization, track management, and reinforcement. The
proposed approach requires no fine-tuning and remains detector-agnostic.
Experimental results on KITTI MOT and KITTI MOTS benchmarks show that
Seg2Track-SAM2 achieves state-of-the-art (SOTA) performance, ranking fourth
overall in both car and pedestrian classes on KITTI MOTS, while establishing a
new benchmark in association accuracy (AssA). Furthermore, a sliding-window
memory strategy reduces memory usage by up to 75% with negligible performance
degradation, supporting deployment under resource constraints. These results
confirm that Seg2Track-SAM2 advances MOTS by combining robust zero-shot
tracking, enhanced identity preservation, and efficient memory utilization. The
code is available at https://github.com/hcmr-lab/Seg2Track-SAM2

</details>


### [100] [SA-UNetv2: Rethinking Spatial Attention U-Net for Retinal Vessel Segmentation](https://arxiv.org/abs/2509.11774)
*Changlu Guo,Anders Nymark Christensen,Anders Bjorholm Dahl,Yugen Yi,Morten Rieger Hannemose*

Main category: cs.CV

TL;DR: 本文提出了一种改进的轻量化视网膜血管分割模型SA-UNetv2，通过在所有跳连中引入跨尺度空间注意力以及采用加权BCE与MCC联合损失函数，有效提升分割效果并增强对类别不均衡的鲁棒性，在标准数据集上表现优异且资源消耗极低，适用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 前作SA-UNet仅在主干部分应用空间注意力，跳连部分未充分利用注意力机制，并且未能很好地应对前景与背景严重不平衡问题，因此有提升多尺度特征融合和增强对类别不均衡鲁棒性的需求。

Method: SA-UNetv2在所有跳连中引入跨尺度空间注意力，有效整合多尺度特征。同时，损失函数结合加权的二元交叉熵（BCE）与Matthews相关系数（MCC），增强对类别不均衡的适应性，并保持模型小参数量和内存占用。

Result: 在DRIVE和STARE公开数据集上，SA-UNetv2以仅1.2MB内存和0.26M参数（不到SA-UNet一半）取得了当前最优的分割性能，单张592x592x3图片CPU推理只需1秒。

Conclusion: 所提方法显著提升了分割准确率和泛化能力，在资源受限、仅有CPU的设备上也能高效运行，具有很强的实用性和可部署性。

Abstract: Retinal vessel segmentation is essential for early diagnosis of diseases such
as diabetic retinopathy, hypertension, and neurodegenerative disorders.
Although SA-UNet introduces spatial attention in the bottleneck, it underuses
attention in skip connections and does not address the severe
foreground-background imbalance. We propose SA-UNetv2, a lightweight model that
injects cross-scale spatial attention into all skip connections to strengthen
multi-scale feature fusion and adopts a weighted Binary Cross-Entropy (BCE)
plus Matthews Correlation Coefficient (MCC) loss to improve robustness to class
imbalance. On the public DRIVE and STARE datasets, SA-UNetv2 achieves
state-of-the-art performance with only 1.2MB memory and 0.26M parameters (less
than 50% of SA-UNet), and 1 second CPU inference on 592 x 592 x 3 images,
demonstrating strong efficiency and deployability in resource-constrained,
CPU-only settings.

</details>


### [101] [FineQuest: Adaptive Knowledge-Assisted Sports Video Understanding via Agent-of-Thoughts Reasoning](https://arxiv.org/abs/2509.11796)
*Haodong Chen,Haojian Huang,XinXiang Yin,Dian Shao*

Main category: cs.CV

TL;DR: 本文提出了一个名为FineQuest的无训练新框架，实现了在体育视频理解领域的视频问答（VideoQA）新突破，通过结合双模态推理与专门的知识图谱，取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLM）在通用视频理解上已展现潜力，但面对复杂的体育视频场景时仍有显著挑战，包括专业知识缺乏与事件推理难度高。因此，亟需一种能够理解和处理体育领域复杂问题的VideoQA新方法。

Method: 1）提出了FineQuest，无需额外训练，采用认知科学启发的双重推理机制：对简单问题采取反应式推理，复杂问题采用深思熟虑式推理；2）引入SSGraph知识图谱，将多模态视觉实例与体育专业术语结合，覆盖9类体育项目；3）构建并公布了两个新体育VideoQA基准Gym-QA与Diving-QA。

Result: FineQuest在Gym-QA和Diving-QA新基准上都取得了当前最优性能，并在已有的SPORTU数据集上也优于现有方法，同时保有较强的通用视频问答能力。

Conclusion: FineQuest证明了结合认知启发的推理与领域知识图谱能显著提升体育视频问答任务效果，并推动了体育理解领域的方法发展和评测基准建设。

Abstract: Video Question Answering (VideoQA) based on Large Language Models (LLMs) has
shown potential in general video understanding but faces significant challenges
when applied to the inherently complex domain of sports videos. In this work,
we propose FineQuest, the first training-free framework that leverages
dual-mode reasoning inspired by cognitive science: i) Reactive Reasoning for
straightforward sports queries and ii) Deliberative Reasoning for more complex
ones. To bridge the knowledge gap between general-purpose models and
domain-specific sports understanding, FineQuest incorporates SSGraph, a
multimodal sports knowledge scene graph spanning nine sports, which encodes
both visual instances and domain-specific terminology to enhance reasoning
accuracy. Furthermore, we introduce two new sports VideoQA benchmarks, Gym-QA
and Diving-QA, derived from the FineGym and FineDiving datasets, enabling
diverse and comprehensive evaluation. FineQuest achieves state-of-the-art
performance on these benchmarks as well as the existing SPORTU dataset, while
maintains strong general VideoQA capabilities.

</details>


### [102] [Pseudo-D: Informing Multi-View Uncertainty Estimation with Calibrated Neural Training Dynamics](https://arxiv.org/abs/2509.11800)
*Ang Nan Gu,Michael Tsang,Hooman Vaseli,Purang Abolmaesumi,Teresa Tsang*

Main category: cs.CV

TL;DR: 本文提出了一种能反映不确定性的伪标签生成框架，提升了医学图像辅助诊断系统在不确定和噪声数据下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有辅助诊断系统常用单一标签训练模型，忽略了诊断的不确定性和标注者之间的差异，导致模型在面对有噪声或不完整输入时过度自信，影响实际诊断可靠性。

Method: 提出利用神经网络训练动态（NNTD）来衡量样本难度，通过在训练过程中聚合和校准模型预测，生成反映学习过程中模糊性的伪标签。该方法与网络结构无关，适用于任何监督学习框架。

Result: 在心脏超声图像分类基准测试中，本文方法在校准性、选择性分类和多视图融合等方面显著优于现有专业基线方法。

Conclusion: 通过引入包含不确定性的标签增强方法，可有效提升模型对诊断不确定性的感知能力和鲁棒性，对医学图像相关任务具有广泛适用性和实际意义。

Abstract: Computer-aided diagnosis systems must make critical decisions from medical
images that are often noisy, ambiguous, or conflicting, yet today's models are
trained on overly simplistic labels that ignore diagnostic uncertainty. One-hot
labels erase inter-rater variability and force models to make overconfident
predictions, especially when faced with incomplete or artifact-laden inputs. We
address this gap by introducing a novel framework that brings uncertainty back
into the label space. Our method leverages neural network training dynamics
(NNTD) to assess the inherent difficulty of each training sample. By
aggregating and calibrating model predictions during training, we generate
uncertainty-aware pseudo-labels that reflect the ambiguity encountered during
learning. This label augmentation approach is architecture-agnostic and can be
applied to any supervised learning pipeline to enhance uncertainty estimation
and robustness. We validate our approach on a challenging echocardiography
classification benchmark, demonstrating superior performance over specialized
baselines in calibration, selective classification, and multi-view fusion.

</details>


### [103] [LFRA-Net: A Lightweight Focal and Region-Aware Attention Network for Retinal Vessel Segmentatio](https://arxiv.org/abs/2509.11811)
*Mehwish Mehmood,Shahzaib Iqbal,Tariq Mahmood Khan,Ivor Spence,Muhammad Fahim*

Main category: cs.CV

TL;DR: 论文提出了一种新型轻量化视网膜血管分割网络LFRA-Net，在准确性与计算资源消耗之间取得很好的平衡，尤其适合资源有限的临床实际应用。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习分割方法虽然在视网膜血管分割上取得了显著进展，但面对微小血管的提取仍有挑战，且计算开销较大，难以应用于现实中资源有限的场景。

Method: LFRA-Net主要在编码器-解码器的瓶颈层引入了focal modulation attention，并在选择性跳跃连接中加入了region-aware attention机制，从而提升特征表达和区域关注能力。该网络参数量低、内存占用小，能高效捕捉局部与全局依赖。

Result: 在三个公开数据集（DRIVE、STARE、CHASE_DB）上进行了实验，LFRA-Net的Dice分数分别达到84.28%、88.44%和85.50%，Jaccard指数分别为72.86%、79.31%和74.70%，优于多数现有方法，同时模型仅含0.17M参数、0.66MB内存、10.50 GFLOPs，显著轻量化。

Conclusion: LFRA-Net兼顾分割精度与计算效率，非常适合在资源有限的实时临床场景应用。

Abstract: Retinal vessel segmentation is critical for the early diagnosis of
vision-threatening and systemic diseases, especially in real-world clinical
settings with limited computational resources. Although significant
improvements have been made in deep learning-based segmentation methods,
current models still face challenges in extracting tiny vessels and suffer from
high computational costs. In this study, we present LFRA-Net by incorporating
focal modulation attention at the encoder-decoder bottleneck and region-aware
attention in the selective skip connections. LFRA-Net is a lightweight network
optimized for precise and effective retinal vascular segmentation. It enhances
feature representation and regional focus by efficiently capturing local and
global dependencies. LFRA-Net outperformed many state-of-the-art models while
maintaining lightweight characteristics with only 0.17 million parameters, 0.66
MB memory size, and 10.50 GFLOPs. We validated it on three publicly available
datasets: DRIVE, STARE, and CHASE\_DB. It performed better in terms of Dice
score (84.28\%, 88.44\%, and 85.50\%) and Jaccard index (72.86\%, 79.31\%, and
74.70\%) on the DRIVE, STARE, and CHASE\_DB datasets, respectively. LFRA-Net
provides an ideal ratio between segmentation accuracy and computational cost
compared to existing deep learning methods, which makes it suitable for
real-time clinical applications in areas with limited resources. The code can
be found at https://github.com/Mehwish4593/LFRA-Net.

</details>


### [104] [SpecVLM: Fast Speculative Decoding in Vision-Language Models](https://arxiv.org/abs/2509.11815)
*Haiduo Huang,Fuwei Yang,Zhenhua Liu,Xuanwu Yin,Dong Li,Pengju Ren,Emad Barsoum*

Main category: cs.CV

TL;DR: 该论文提出了SpecVLM系统，实现了视觉-语言模型（VLMs）在推理过程中的高效推理加速，提升整体推理速度2.5-2.9倍，同时保证解码结果不失真。


<details>
  <summary>Details</summary>
Motivation: 尽管投机性解码（speculative decoding）显著加速了大语言模型（LLMs）的自回归推理，但直接应用到视觉语言模型（VLMs）时，由于视觉token数量巨大（与分辨率、视频长度相关），导致KV缓存消耗的计算和存储资源显著增加，阻碍了其效率提升。因此亟需新的方法解决VLMs推理加速时的工程瓶颈。

Method: 1）提出EagleVLM作为强基线，实现了1.5-2.3倍加速。2）引入弹性视觉压缩器，动态选择剪枝、池化、卷积、resampler等primitive，根据输入自适应权衡计算/参数量和准确率。3）提出在线logit蒸馏训练流程，即无须离线大语料，通过“on-the-fly”教师logit与中间特征训练draft模型，结合交叉熵与Smooth L1损失，有效提升推理效率同时减少预处理存储。

Result: SpecVLM系统在LLaVA和MMMU数据集上，训练5个epoch以内即可实现端到端2.5-2.9倍的推理加速，且该方法在不同分辨率与任务难度均表现鲁棒，并保证生成分布与目标模型一致（无损解码）。

Conclusion: SpecVLM不仅解决了VLM在高分辨率/长视频场景下的系统资源瓶颈，也通过创新的在线蒸馏训练方案和弹性视觉压缩器显著提升解码加速，兼顾准确率与解码速度，推动VLM实际应用落地。

Abstract: Speculative decoding is a powerful way to accelerate autoregressive large
language models (LLMs), but directly porting it to vision-language models
(VLMs) faces unique systems constraints: the prefill stage is dominated by
visual tokens whose count scales with image resolution and video length,
inflating both compute and memory, especially the key-value (KV) cache. We
study speculative decoding for VLMs and introduce SpecVLM, a practical system
that (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering
1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)
further accelerates VLM inference with an elastic visual compressor that
adaptively selects among pruning, pooling, convolution, and resampler
primitives to balance FLOPs/parameters and accuracy per input. To avoid costly
offline distillation corpora, we propose an online-logit distillation protocol
that trains the draft model with on-the-fly teacher logits and penultimate
features using a combined cross-entropy and Smooth L1 objective, eliminating
storage and preprocessing while remaining compute-efficient. This protocol
reveals a training-time scaling effect: longer online training monotonically
increases the draft model's average accepted length, improving speculative
efficiency. Empirically, SpecVLM achieves additional acceleration, culminating
in 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,
consistently over resolutions and task difficulties, while preserving the
target model's output distribution (lossless decoding). Our code is available
at https://github.com/haiduo/SpecVLM.

</details>


### [105] [MAFS: Masked Autoencoder for Infrared-Visible Image Fusion and Semantic Segmentation](https://arxiv.org/abs/2509.11817)
*Liying Wang,Xiaoli Zhang,Chuanmin Jia,Siwei Ma*

Main category: cs.CV

TL;DR: 提出了一种将红外-可见光图像融合与语义分割统一于一个网络中的新方法，通过异构特征融合和多阶段Transformer结构，提升了融合图像的语义感知与分割性能，两任务协同达到更优效果。


<details>
  <summary>Details</summary>
Motivation: 现有的语义驱动图像融合方法虽然考虑了下游任务的语义信息，但没有从宏观任务层面探讨像素级图像融合与跨模态特征融合两者的互促潜力。作者希望从宏观任务协同的角度提升融合图像在高层次任务（如语义分割）上的表现。

Method: 提出了一种统一网络（MAFS），包含图像融合子网络与分割子网络，采用异构特征融合策略以及多阶段Transformer解码器整合多尺度特征。分割骨干结构与融合模块串联，使分割知识反哺融合任务。同时，引入动态因子实现多任务自适应权重调整。

Result: 大量实验表明，该方法在图像融合与语义分割领域均取得了与最新方法相比具有竞争力的结果，展示了多任务联合训练带来的显著性能提升。

Conclusion: 该工作验证了像素级融合和特征级融合感知任务的协同促进作用，多任务统一学习框架在实际图像融合与分割中具有良好的应用前景。

Abstract: Infrared-visible image fusion methods aim at generating fused images with
good visual quality and also facilitate the performance of high-level tasks.
Indeed, existing semantic-driven methods have considered semantic information
injection for downstream applications. However, none of them investigates the
potential for reciprocal promotion between pixel-wise image fusion and
cross-modal feature fusion perception tasks from a macroscopic task-level
perspective. To address this limitation, we propose a unified network for image
fusion and semantic segmentation. MAFS is a parallel structure, containing a
fusion sub-network and a segmentation sub-network. On the one hand, We devise a
heterogeneous feature fusion strategy to enhance semantic-aware capabilities
for image fusion. On the other hand, by cascading the fusion sub-network and a
segmentation backbone, segmentation-related knowledge is transferred to promote
feature-level fusion-based segmentation. Within the framework, we design a
novel multi-stage Transformer decoder to aggregate fine-grained multi-scale
fused features efficiently. Additionally, a dynamic factor based on the max-min
fairness allocation principle is introduced to generate adaptive weights of two
tasks and guarantee smooth training in a multi-task manner. Extensive
experiments demonstrate that our approach achieves competitive results compared
with state-of-the-art methods. The code is available at
https://github.com/Abraham-Einstein/MAFS/.

</details>


### [106] [Probabilistic Robustness Analysis in High Dimensional Space: Application to Semantic Segmentation Network](https://arxiv.org/abs/2509.11838)
*Navid Hashemi,Samuel Sasaki,Diego Manzanas Lopez,Ipek Oguz,Meiyi Ma,Taylor T. Johnson*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的概率验证框架，能为高维语义分割网络高效且可靠地提供安全保证，并显著减少了现有方法的保守性。


<details>
  <summary>Details</summary>
Motivation: 现代语义分割应用（如医学影像、自动驾驶等）对模型的可靠性和不确定性要求极高，但现有概率验证方法面对网络高复杂度和高维输出时，既计算成本高又过于保守，不易用于实际工程。

Method: 作者提出一种与架构无关、可扩展到高维输出的概率验证框架，将基于采样的可达性分析与保序推断（conformal inference）相结合，在维持严格理论保证的同时，提出新策略缓解保序推断在高维度中的保守性过高。

Result: 在CamVid、OCTA-500、Lung Segmentation和Cityscapes等大规模分割任务上实验证明，该方法能为主流语义分割网络高效提供安全性保证，且相较现有SOTA方法大幅收紧了边界预测的保守程度。同时，作者还公开了相关工具箱代码。

Conclusion: 文中方法为高维语义分割安全验证问题提供了可行且实用的概率保证框架，在保证严谨性的同时提升了实用性和工程落地价值。

Abstract: Semantic segmentation networks (SSNs) play a critical role in domains such as
medical imaging, autonomous driving, and environmental monitoring, where safety
hinges on reliable model behavior under uncertainty. Yet, existing
probabilistic verification approaches struggle to scale with the complexity and
dimensionality of modern segmentation tasks, often yielding guarantees that are
too conservative to be practical. We introduce a probabilistic verification
framework that is both architecture-agnostic and scalable to high-dimensional
outputs. Our approach combines sampling-based reachability analysis with
conformal inference (CI) to deliver provable guarantees while avoiding the
excessive conservatism of prior methods. To counteract CI's limitations in
high-dimensional settings, we propose novel strategies that reduce conservatism
without compromising rigor. Empirical evaluation on large-scale segmentation
models across CamVid, OCTA-500, Lung Segmentation, and Cityscapes demonstrates
that our method provides reliable safety guarantees while substantially
tightening bounds compared to SOTA. We also provide a toolbox implementing this
technique, available on Github.

</details>


### [107] [Synthetic Captions for Open-Vocabulary Zero-Shot Segmentation](https://arxiv.org/abs/2509.11840)
*Tim Lebailly,Vijay Veerabadran,Satwik Kottur,Karl Ridgeway,Michael Louis Iuzzolino*

Main category: cs.CV

TL;DR: 本文提出一种方法，将生成式视觉-语言模型（VLMs）生成的描述与图像进行密集对齐，用于提升图像分割等下游任务的表现，取得了领先的零样本分割效果。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式视觉-语言模型在图像理解方面表现优异，但它们在视觉与语言之间的空间密集对齐能力较弱，而传统密集对齐方法数据获取困难。作者希望结合生成式VLMs生成的大量合成描述，实现更高效的密集对齐，提升下游任务性能。

Method: 该方法使用生成式VLMs自动为图像生成大量合成文本描述，并将这些描述用于密集对齐训练，继而提升如零样本分割这类密集视觉-语言任务的能力。利用合成描述的可扩展、低成本特性，显著提升训练效率与模型性能。

Result: 在标准的零样本open-vocabulary segmentation基准数据集上，本方法优于之前的工作，在保证更高数据效率的同时达到了更好的表现。

Conclusion: 通过将生成式VLMs的大规模合成描述与图像进行密集对齐，可以有效提升密集视觉-语言任务的能力，并证明了此方法在实际任务中的有效性和高数据效率。

Abstract: Generative vision-language models (VLMs) exhibit strong high-level image
understanding but lack spatially dense alignment between vision and language
modalities, as our findings indicate. Orthogonal to advancements in generative
VLMs, another line of research has focused on representation learning for
vision-language alignment, targeting zero-shot inference for dense tasks like
segmentation. In this work, we bridge these two directions by densely aligning
images with synthetic descriptions generated by VLMs. Synthetic captions are
inexpensive, scalable, and easy to generate, making them an excellent source of
high-level semantic understanding for dense alignment methods. Empirically, our
approach outperforms prior work on standard zero-shot open-vocabulary
segmentation benchmarks/datasets, while also being more data-efficient.

</details>


### [108] [Segmentation-Driven Initialization for Sparse-view 3D Gaussian Splatting](https://arxiv.org/abs/2509.11853)
*Yi-Hsin Li,Thomas Sikora,Sebastian Knorr,Måarten Sjöström*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于分割驱动的高斯溅射（Gaussian Splatting）初始化方法（SDI-GS），显著减少稀疏视图重建中的高斯数量，同时保持高渲染质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 在稀疏视图合成中，现有方法如3D Gaussian Splatting依赖于SfM进行相机姿态估计，但在视图极少时表现较差；而无需SfM的方法又由于将每个像素反投影到三维空间，导致需要存储大量高斯点，造成高内存消耗。如何在保证渲染质量的同时减少高斯数量和内存需求，是该领域待解决的关键问题。

Method: 作者提出SDI-GS方法，通过对场景分割，提取结构显著的区域，只对这些关键区域的点云进行选择性降采样，大幅减少高斯点的数量，以提升效率和内存利用率。

Result: 实验表明，SDI-GS可在不同基准数据集上将高斯点数量减少多达50%，且在PSNR和SSIM渲染质量指标上与原方法相当甚至更优，LPIPS指标也仅有轻微下降。此外，该方法带来了更快的训练速度和更低的内存消耗。

Conclusion: SDI-GS有效提升了3DGS在受限视图场景下的实用性，在不牺牲图像质量的前提下大幅降低了计算资源需求，对稀疏视图下真实场景三维重建具有重要意义。

Abstract: Sparse-view synthesis remains a challenging problem due to the difficulty of
recovering accurate geometry and appearance from limited observations. While
recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time
rendering with competitive quality, existing pipelines often rely on
Structure-from-Motion (SfM) for camera pose estimation, an approach that
struggles in genuinely sparse-view settings. Moreover, several SfM-free methods
replace SfM with multi-view stereo (MVS) models, but generate massive numbers
of 3D Gaussians by back-projecting every pixel into 3D space, leading to high
memory costs. We propose Segmentation-Driven Initialization for Gaussian
Splatting (SDI-GS), a method that mitigates inefficiency by leveraging
region-based segmentation to identify and retain only structurally significant
regions. This enables selective downsampling of the dense point cloud,
preserving scene fidelity while substantially reducing Gaussian count.
Experiments across diverse benchmarks show that SDI-GS reduces Gaussian count
by up to 50% and achieves comparable or superior rendering quality in PSNR and
SSIM, with only marginal degradation in LPIPS. It further enables faster
training and lower memory footprint, advancing the practicality of 3DGS for
constrained-view scenarios.

</details>


### [109] [Bridging Vision Language Models and Symbolic Grounding for Video Question Answering](https://arxiv.org/abs/2509.11862)
*Haodi Ma,Vyom Pathak,Daisy Zhe Wang*

Main category: cs.CV

TL;DR: 本文提出了一种集成场景图(SG)与视觉语言模型(VLM)的新方法，显著提升了视频问答任务中的因果和时序推理能力。


<details>
  <summary>Details</summary>
Motivation: 虽然VLM在视频问答中表现良好，但其依赖表层相关性，缺乏对时序和因果性问题的准确理解与解释能力，因此需要更精细化的中间表示进行补充。

Method: 作者提出SG-VLM框架，将冻结的VLM与符号化的场景图进行集成，利用场景图作为额外的中间支持，并通过提示技术和视觉定位实现融合。

Result: 在NExT-QA、iVQA和ActivityNet-QA三大数据集上，SG-VLM在因果与时序推理方面优于以往方法，并在多个VLM（QwenVL, InternVL）上进行了验证。然而，在已经很强的VLM基础上提升有限。

Conclusion: 符号场景图能够有效补充VLM对视频的深层理解，但目前混合方法的提升有限，表明未来还需在符号化与深度学习模式结合上做进一步探索。

Abstract: Video Question Answering (VQA) requires models to reason over spatial,
temporal, and causal cues in videos. Recent vision language models (VLMs)
achieve strong results but often rely on shallow correlations, leading to weak
temporal grounding and limited interpretability. We study symbolic scene graphs
(SGs) as intermediate grounding signals for VQA. SGs provide structured
object-relation representations that complement VLMs holistic reasoning. We
introduce SG-VLM, a modular framework that integrates frozen VLMs with scene
graph grounding via prompting and visual localization. Across three benchmarks
(NExT-QA, iVQA, ActivityNet-QA) and multiple VLMs (QwenVL, InternVL), SG-VLM
improves causal and temporal reasoning and outperforms prior baselines, though
gains over strong VLMs are limited. These findings highlight both the promise
and current limitations of symbolic grounding, and offer guidance for future
hybrid VLM-symbolic approaches in video understanding.

</details>


### [110] [Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding](https://arxiv.org/abs/2509.11866)
*Meng Luo,Shengqiong Wu,Liqiang Jing,Tianjie Ju,Li Zheng,Jinxiang Lai,Tianlong Wu,Xinya Du,Jian Li,Siyuan Yan,Jiebo Luo,William Yang Wang,Hao Fei,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: 提出了Dr.V系统用于检测和诊断大型视频模型中的幻觉现象，通过细粒度的时空定位和分层推理提高视频理解的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视频模型（LVMs）在视频理解领域取得了长足进步，但仍存在幻觉问题，即生成的内容与输入视频不符，因此亟需更有效的幻觉诊断方法。

Method: 作者提出了Dr.V，一个包含感知、时间和认知三个层次的分层框架，用以通过细粒度的时空定位诊断LVMs中的幻觉。该系统包含两个核心组件：Dr.V-Bench基准数据集（含1万条带详细时空标注的视频实例）和Dr.V-Agent（代理）。Dr.V-Agent通过逐步在感知和时间层面对LVMs输出进行时空定位，然后在认知层面推理，检测幻觉。

Result: 实验表明，Dr.V-Agent能够有效诊断视频模型中的幻觉问题，同时提升了诊断过程的可解释性和模型的可靠性。

Conclusion: Dr.V为实际视频理解任务提供了一套可行的强健解决方案，并增强了AI诊断幻觉的能力和可信度。数据与代码已公开，便于社区进一步研究。

Abstract: Recent advancements in large video models (LVMs) have significantly enhance
video understanding. However, these models continue to suffer from
hallucinations, producing content that conflicts with input videos. To address
this issue, we propose Dr.V, a hierarchical framework covering perceptive,
temporal, and cognitive levels to diagnose video hallucination by fine-grained
spatial-temporal grounding. Dr.V comprises of two key components: a benchmark
dataset Dr.V-Bench and a satellite video agent Dr.V-Agent. Dr.V-Bench includes
10k instances drawn from 4,974 videos spanning diverse tasks, each enriched
with detailed spatial-temporal annotation. Dr.V-Agent detects hallucinations in
LVMs by systematically applying fine-grained spatial-temporal grounding at the
perceptive and temporal levels, followed by cognitive level reasoning. This
step-by-step pipeline mirrors human-like video comprehension and effectively
identifies hallucinations. Extensive experiments demonstrate that Dr.V-Agent is
effective in diagnosing hallucination while enhancing interpretability and
reliability, offering a practical blueprint for robust video understanding in
real-world scenarios. All our data and code are available at
https://github.com/Eurekaleo/Dr.V.

</details>


### [111] [Multi-animal tracking in Transition: Comparative Insights into Established and Emerging Methods](https://arxiv.org/abs/2509.11873)
*Anne Marthe Sophie Ngo Bibinbe,Patrick Gagnon,Jamie Ahloy-Dallaire,Eric R. Paquet*

Main category: cs.CV

TL;DR: 文章比较了多动物长期追踪（MAT）与主流多目标追踪（MOT）方法在猪类精确养殖中的表现，发现 MOT 方法整体优于传统 MAT 工具。


<details>
  <summary>Details</summary>
Motivation: 精确化养殖需要持续自动化地监测动物行为，这推动了对高效多动物追踪工具的需求。目前流行的 MAT 工具虽然易用但精度不高，影响后续的行为分析和健康评估，因此亟需验证 MOT 技术在该领域的可行性与优越性。

Method: 作者将主流 MAT 工具（如 DeepLabCut、idTracker）与多种 MOT 方法（如 ByteTrack、DeepSORT、Track-Anything 等）应用于10分钟的猪只追踪数据集，并比较多种算法的表现。

Result: 实验表明，多数 MOT 方法在多动物长期追踪任务中表现优于传统 MAT 工具，尤其在遮挡、动作变化和外观相似等复杂场景下更具优势。

Conclusion: 最新的 MOT 技术能提升精确养殖自动化追踪的准确性和可靠性，建议在实际生产中采用先进 MOT 方法替代传统 MAT 工具。

Abstract: Precision livestock farming requires advanced monitoring tools to meet the
increasing management needs of the industry. Computer vision systems capable of
long-term multi-animal tracking (MAT) are essential for continuous behavioral
monitoring in livestock production. MAT, a specialized subset of multi-object
tracking (MOT), shares many challenges with MOT, but also faces domain-specific
issues including frequent animal occlusion, highly similar appearances among
animals, erratic motion patterns, and a wide range of behavior types.
  While some existing MAT tools are user-friendly and widely adopted, they
often underperform compared to state-of-the-art MOT methods, which can result
in inaccurate downstream tasks such as behavior analysis, health state
estimation, and related applications. In this study, we benchmarked both MAT
and MOT approaches for long-term tracking of pigs. We compared tools such as
DeepLabCut and idTracker with MOT-based methods including ByteTrack, DeepSORT,
cross-input consistency, and newer approaches like Track-Anything and
PromptTrack.
  All methods were evaluated on a 10-minute pig tracking dataset. Our results
demonstrate that, overall, MOT approaches outperform traditional MAT tools,
even for long-term tracking scenarios. These findings highlight the potential
of recent MOT techniques to enhance the accuracy and reliability of automated
livestock tracking.

</details>


### [112] [Do It Yourself (DIY): Modifying Images for Poems in a Zero-Shot Setting Using Weighted Prompt Manipulation](https://arxiv.org/abs/2509.11878)
*Sofia Jamil,Kotla Sai Charan,Sriparna Saha,Koustava Goswami,K J Joseph*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，通过加权提示操控（WPM）技术，在零样本场景下生成诗歌对应的图像，并允许用户修改图像，以更好地反映诗歌意境。


<details>
  <summary>Details</summary>
Motivation: 诗歌具有丰富的表达和多重解释空间，自动为诗歌生成图像有助于受众更直观地理解和欣赏诗歌，但现有方法难以捕捉诗歌语言的复杂语义和个性化需求。

Method: 引入Weighted Prompt Manipulation（WPM）技术，通过调整文本提示中的词语重要性权重以及扩散模型中的文本嵌入，动态增强或减弱词语对最终生成图像的影响，并结合GPT等大语言模型和现有诗歌数据集，提升生成图像的语义丰富性和上下文准确性。

Result: 该技术实现了更符合诗歌语义和作者/用户意图的诗歌视觉化图像生成，呈现出更丰富、更精准的视觉内容。

Conclusion: WPM方法首次展示了对诗歌语言进行加权提示操控可显著提升生成图像的质量，为文学与人工智能交叉领域提供了新思路。

Abstract: Poetry is an expressive form of art that invites multiple interpretations, as
readers often bring their own emotions, experiences, and cultural backgrounds
into their understanding of a poem. Recognizing this, we aim to generate images
for poems and improve these images in a zero-shot setting, enabling audiences
to modify images as per their requirements. To achieve this, we introduce a
novel Weighted Prompt Manipulation (WPM) technique, which systematically
modifies attention weights and text embeddings within diffusion models. By
dynamically adjusting the importance of specific words, WPM enhances or
suppresses their influence in the final generated image, leading to
semantically richer and more contextually accurate visualizations. Our approach
exploits diffusion models and large language models (LLMs) such as GPT in
conjunction with existing poetry datasets, ensuring a comprehensive and
structured methodology for improved image generation in the literary domain. To
the best of our knowledge, this is the first attempt at integrating weighted
prompt manipulation for enhancing imagery in poetic language.

</details>


### [113] [SAM-TTT: Segment Anything Model via Reverse Parameter Configuration and Test-Time Training for Camouflaged Object Detection](https://arxiv.org/abs/2509.11884)
*Zhenni Yu,Li Zhao,Guobao Xiao,Xiaoqin Zhang*

Main category: cs.CV

TL;DR: 本文提出了SAM-TTT模型，通过逆向参数配置和测试时训练技术，提升了Segment Anything Model（SAM）在隐蔽目标检测（COD）任务中的性能，达到了当前最优水平。


<details>
  <summary>Details</summary>
Motivation: 目前基于SAM的COD方法大多关注于增强模型的有利特征和参数，但忽视了那些对语义理解有负面影响的不利参数。作者希望通过解决这一缺口，进一步提升SAM在下游任务的表现。

Method: 提出了逆向参数配置模块，以无须训练的方式降低不利参数对SAM语义理解的影响；并引入T-Visioner模块，将最初用于语言任务的测试时训练层（Test-Time Training layers）整合进视觉任务，用以强化有利参数。这两个模块结合后，共同提升了模型的性能。

Result: 在多个隐蔽目标检测（COD）基准测试上，SAM-TTT模型实现了最新的最优表现，树立了新的业界标杆。

Conclusion: 本文所提的SAM-TTT模型能有效抑制不利参数、增强有利参数，大幅提升了SAM在COD任务中的语义理解能力，并取得了领域内的最优结果。

Abstract: This paper introduces a new Segment Anything Model (SAM) that leverages
reverse parameter configuration and test-time training to enhance its
performance on Camouflaged Object Detection (COD), named SAM-TTT. While most
existing SAM-based COD models primarily focus on enhancing SAM by extracting
favorable features and amplifying its advantageous parameters, a crucial gap is
identified: insufficient attention to adverse parameters that impair SAM's
semantic understanding in downstream tasks. To tackle this issue, the Reverse
SAM Parameter Configuration Module is proposed to effectively mitigate the
influence of adverse parameters in a train-free manner by configuring SAM's
parameters. Building on this foundation, the T-Visioner Module is unveiled to
strengthen advantageous parameters by integrating Test-Time Training layers,
originally developed for language tasks, into vision tasks. Test-Time Training
layers represent a new class of sequence modeling layers characterized by
linear complexity and an expressive hidden state. By integrating two modules,
SAM-TTT simultaneously suppresses adverse parameters while reinforcing
advantageous ones, significantly improving SAM's semantic understanding in COD
task. Our experimental results on various COD benchmarks demonstrate that the
proposed approach achieves state-of-the-art performance, setting a new
benchmark in the field. The code will be available at
https://github.com/guobaoxiao/SAM-TTT.

</details>


### [114] [BREA-Depth: Bronchoscopy Realistic Airway-geometric Depth Estimation](https://arxiv.org/abs/2509.11885)
*Francis Xiatian Zhang,Emile Mackute,Mohammadreza Kasaei,Kevin Dhaliwal,Robert Thomson,Mohsen Khadem*

Main category: cs.CV

TL;DR: 本文提出了Brea-Depth框架，通过结合气道几何先验与深度感知GAN，提升支气管镜单目深度估计的结构一致性及3D重建准确性，并在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的单目深度估计算法在支气管镜应用中，容易因缺乏对气道全局结构的感知而过拟合于局部纹理，尤其在深度线索模糊或光照较差时效果不佳。提升模型对支气管气道结构的识别和重建能力，有助于外科导航安全和诊疗效率。

Method: 提出一种创新框架Brea-Depth，将气道几何学先验信息融入到基础深度模型的自适应过程中。具体做法包括：引入深度感知CycleGAN，实现真实支气管镜图像与解剖结构几何形态之间的域转换优化；增加气道结构感知损失函数，强化气道腔内深度一致性并保持过渡平滑性；并新提出结构评价指标评估重建的气道解剖真实性。

Result: Brea-Depth在自建离体人肺数据集和公开支气管镜数据集上进行测试，结果表明其在气道结构的深度重建准确性与一致性方面均优于现有方法，在解剖深度保留和三维重建精度上获得了提升。

Conclusion: Brea-Depth能有效增强单目支气管镜深度估计模型的结构感知和泛化能力，显著提升气道三维重建的鲁棒性和准确性，有助于临床实际的手术导航和诊断。

Abstract: Monocular depth estimation in bronchoscopy can significantly improve
real-time navigation accuracy and enhance the safety of interventions in
complex, branching airways. Recent advances in depth foundation models have
shown promise for endoscopic scenarios, yet these models often lack anatomical
awareness in bronchoscopy, overfitting to local textures rather than capturing
the global airway structure, particularly under ambiguous depth cues and poor
lighting. To address this, we propose Brea-Depth, a novel framework that
integrates airway-specific geometric priors into foundation model adaptation
for bronchoscopic depth estimation. Our method introduces a depth-aware
CycleGAN, refining the translation between real bronchoscopic images and airway
geometries from anatomical data, effectively bridging the domain gap. In
addition, we introduce an airway structure awareness loss to enforce depth
consistency within the airway lumen while preserving smooth transitions and
structural integrity. By incorporating anatomical priors, Brea-Depth enhances
model generalization and yields more robust, accurate 3D airway
reconstructions. To assess anatomical realism, we introduce Airway Depth
Structure Evaluation, a new metric for structural consistency. We validate
BREA-Depth on a collected ex vivo human lung dataset and an open bronchoscopic
dataset, where it outperforms existing methods in anatomical depth
preservation.

</details>


### [115] [Logit Mixture Outlier Exposure for Fine-grained Out-of-Distribution Detection](https://arxiv.org/abs/2509.11892)
*Akito Shinohara,Kohei Fukuda,Hiroaki Aizawa*

Main category: cs.CV

TL;DR: 本文提出了一种在logit空间中的线性插值技术，用于改进模型对分布外数据（OOD）的检测表现。该方法在logit空间结合分布内与分布外数据，并与输入空间的混合保持一致性，通过实验验证其提高了检测效果，尤其对细粒度OOD任务表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法（如Outlier Exposure及其变体）虽能提升模型对异常数据的识别效率，但对类别间关系学习和分布内外数据判别仍显不足。作者希望通过在logit空间对类别分布特性进行更清晰的区分，提高模型对接近分布边界的异常数据的检测能力。

Method: 作者在logit空间提出了一种线性插值混合技术，将分布内和分布外样本的logit混合，用于训练过程中的平滑处理。同时，要求logit空间混合和输入空间混合结果保持一致性，以避免模型在决策边界附近输出剧烈波动。

Result: 实验表明，logit空间混合能够有效减少模型输出在决策边界附近的抖动，使分布内外数据的分离更加平滑、可信。此外，在细粒度OOD检测任务中也取得了较好的效果。

Conclusion: logit空间的线性插值混合技术能增强模型的OOD检测能力，特别是在难以判别的边界样本上表现突出，有助于提升模型的泛化与鲁棒性。

Abstract: The ability to detect out-of-distribution data is essential not only for
ensuring robustness against unknown or unexpected input data but also for
improving the generalization performance of the model. Among various
out-of-distribution detection methods, Outlier Exposure and Mixture Outlier
Exposure are promising approaches that enhance out-of-distribution detection
performance by exposing the outlier data during training. However, even with
these sophisticated techniques, it remains challenging for models to learn the
relationships between classes effectively and to distinguish data sampling from
in-distribution and out-of-distribution clearly. Therefore, we focus on the
logit space, where the properties between class-wise distributions are
distinctly separated from those in the input or feature spaces. Specifically,
we propose a linear interpolation technique in the logit space that mixes
in-distribution and out-of-distribution data to facilitate smoothing logits
between classes and improve the out-of-distribution detection performance,
particularly for out-of-distribution data that lie close to the in-distribution
data. Additionally, we enforce consistency between the logits obtained through
mixing in the logit space and those generated via mixing in the input space.
Our experiments demonstrate that our logit-space mixing technique reduces the
abrupt fluctuations in the model outputs near the decision boundaries,
resulting in smoother and more reliable separation between in-distribution and
out-of-distribution data. Furthermore, we evaluate the effectiveness of the
proposed method on a fine-grained out-of-distribution detection task.

</details>


### [116] [Integrating Prior Observations for Incremental 3D Scene Graph Prediction](https://arxiv.org/abs/2509.11895)
*Marian Renz,Felix Igelbrink,Martin Atzmueller*

Main category: cs.CV

TL;DR: 本文提出了一种新型异构图模型，可增量式生成3D语义场景图，并融合多模态信息进行处理，无需完整的场景重建，提升了对实际环境的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有的3D语义场景图方法多依赖传感器数据，缺乏对环境中丰富语义信息的整合，且通常假设可以获得完整的场景重建，这使其在真实、逐步探索场景中受限。

Method: 作者提出一种异构图神经网络模型，在信息传递过程中直接整合了多模态输入（如之前观测、语义嵌入），模型由多层结构组成，可灵活结合全局与局部场景表达，无需专门模块或完整场景重建。

Result: 在3DSSG数据集上验证，结果表明引入多模态信息（如CLIP语义嵌入和历史观测）的图神经网络，能更好适应复杂且真实环境，具备更好的可扩展性和泛化能力。

Conclusion: 本文方法突破了3D语义场景图在真实场景下的应用瓶颈，具有较强的通用性和实际意义。相关代码已公开以促进后续研究。

Abstract: 3D semantic scene graphs (3DSSG) provide compact structured representations
of environments by explicitly modeling objects, attributes, and relationships.
While 3DSSGs have shown promise in robotics and embodied AI, many existing
methods rely mainly on sensor data, not integrating further information from
semantically rich environments. Additionally, most methods assume access to
complete scene reconstructions, limiting their applicability in real-world,
incremental settings. This paper introduces a novel heterogeneous graph model
for incremental 3DSSG prediction that integrates additional, multi-modal
information, such as prior observations, directly into the message-passing
process. Utilizing multiple layers, the model flexibly incorporates global and
local scene representations without requiring specialized modules or full scene
reconstructions. We evaluate our approach on the 3DSSG dataset, showing that
GNNs enriched with multi-modal information such as semantic embeddings (e.g.,
CLIP) and prior observations offer a scalable and generalizable solution for
complex, real-world environments. The full source code of the presented
architecture will be made available at
https://github.com/m4renz/incremental-scene-graph-prediction.

</details>


### [117] [NeuroGaze-Distill: Brain-informed Distillation and Depression-Inspired Geometric Priors for Robust Facial Emotion Recognition](https://arxiv.org/abs/2509.11916)
*Zilin Li,Weiwei Xu,Xuanqi Zhao,Yiran Zhu*

Main category: cs.CV

TL;DR: 该论文提出了NeuroGaze-Distill，一种将脑电信息注入图像人脸表情识别（FER）模型的跨模态蒸馏框架，通过静态情感原型和几何先验提升泛化能力，且无需实际部署时的脑电数据。


<details>
  <summary>Details</summary>
Motivation: 传统基于像素的人脸表情识别模型因面部外观是情感的间接代理，存在泛化性差的问题。作者旨在引入更加直接、与情感真实相关的脑电先验，改善模型对不同数据集的适应能力和鲁棒性。

Method: 方法分为两步：（1）用脑电信号训练教师模型，生成静态的5x5情感原型（V/A情感二维格点）并固定用于后续训练；（2）学生模型（ResNet-18或50），在纯图像数据上训练，结合常规CE/KD损失，并加上两个轻量正则：一是对齐至脑电原型的特征匹配（Proto-KD）；二是基于抑郁症研究引入的几何正则（D-Geo），控制高愉悦区域的嵌入收缩以模仿情感障碍规律。

Result: 在FERPlus（域内）和跨数据集（AffectNet-mini、可选CK+）实验中，论文的方法通过8类准确率、Macro-F1和均衡准确率评估，显著优于对比方法。消融分析表明原型及D-Geo正则均带来稳定提升，并且5x5格点效果更优且更稳定。

Conclusion: NeuroGaze-Distill方法简单、易部署，无需在推理阶段依赖脑电信号，能在不增加模型复杂度下增强模型鲁棒性和泛化性。

Abstract: Facial emotion recognition (FER) models trained only on pixels often fail to
generalize across datasets because facial appearance is an indirect and biased
proxy for underlying affect. We present NeuroGaze-Distill, a cross-modal
distillation framework that transfers brain-informed priors into an image-only
FER student via static Valence/Arousal (V/A) prototypes and a
depression-inspired geometric prior (D-Geo). A teacher trained on EEG
topographic maps from DREAMER (with MAHNOB-HCI as unlabeled support) produces a
consolidated 5x5 V/A prototype grid that is frozen and reused; no EEG-face
pairing and no non-visual signals at deployment are required. The student
(ResNet-18/50) is trained on FERPlus with conventional CE/KD and two
lightweight regularizers: (i) Proto-KD (cosine) aligns student features to the
static prototypes; (ii) D-Geo softly shapes the embedding geometry in line with
affective findings often reported in depression research (e.g., anhedonia-like
contraction in high-valence regions). We evaluate both within-domain (FERPlus
validation) and cross-dataset protocols (AffectNet-mini; optional CK+),
reporting standard 8-way scores alongside present-only Macro-F1 and balanced
accuracy to fairly handle label-set mismatch. Ablations attribute consistent
gains to prototypes and D-Geo, and favor 5x5 over denser grids for stability.
The method is simple, deployable, and improves robustness without architectural
complexity.

</details>


### [118] [Enriched text-guided variational multimodal knowledge distillation network (VMD) for automated diagnosis of plaque vulnerability in 3D carotid artery MRI](https://arxiv.org/abs/2509.11924)
*Bo Cao,Fan Yu,Mengmeng Feng,SenHao Zhang,Xin Meng,Yue Zhang,Zhen Qian,Jie Lu*

Main category: cs.CV

TL;DR: 本文提出了一种结合变分推断与多模态知识蒸馏（VMD）的方法，用于自动化诊断颈动脉斑块易损性的3D MRI图像，提高了无标注数据的诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 传统依赖单一模态或仅用图像的AI方法在甄别颈动脉斑块易损性时表现不佳，且3D MRI图像解读对放射科医生和传统算法来说具有挑战性。临床实际中常结合多种成像手段与专家知识评估风险，因此亟需将多模态信息融入自动化诊断模型。

Method: 作者设计了VMD策略，将放射科医生的领域知识通过变分推断与多模态知识蒸馏方式集成到深度学习模型中，能够在带有有限标注数据和报告的数据集上进行训练，从而充分利用图像和文本的互补信息。

Result: 在自建数据集上进行了全面实验，结果显示所提出的VMD策略在无标注3D MRI图像的斑块易损性识别方面表现优异，有效提升了诊断准确率。

Conclusion: VMD方法证实了将跨模态先验知识和专家经验融入诊断网络的有效性，有助于开发更为智能和精准的医学影像诊断工具。

Abstract: Multimodal learning has attracted much attention in recent years due to its
ability to effectively utilize data features from a variety of different
modalities. Diagnosing the vulnerability of atherosclerotic plaques directly
from carotid 3D MRI images is relatively challenging for both radiologists and
conventional 3D vision networks. In clinical practice, radiologists assess
patient conditions using a multimodal approach that incorporates various
imaging modalities and domain-specific expertise, paving the way for the
creation of multimodal diagnostic networks. In this paper, we have developed an
effective strategy to leverage radiologists' domain knowledge to automate the
diagnosis of carotid plaque vulnerability through Variation inference and
Multimodal knowledge Distillation (VMD). This method excels in harnessing
cross-modality prior knowledge from limited image annotations and radiology
reports within training data, thereby enhancing the diagnostic network's
accuracy for unannotated 3D MRI images. We conducted in-depth experiments on
the dataset collected in-house and verified the effectiveness of the VMD
strategy we proposed.

</details>


### [119] [Graph Algorithm Unrolling with Douglas-Rachford Iterations for Image Interpolation with Guaranteed Initialization](https://arxiv.org/abs/2509.11926)
*Xue Zhang,Bingshuo Hu,Gene Cheung*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，将经典的图滤波理论引入深度神经网络，实现高效且可解释的图神经网络初始化与优化，用于图像插值任务，并大幅减少了模型参数量。


<details>
  <summary>Details</summary>
Motivation: 现有深度神经网络通过随机初始化参数并用SGD优化，容易陷入性能较差的局部最优，且参数规模庞大。本文尝试结合图滤波理论，寻求更科学的网络初始化方法，提升图像插值质量并提升网络可解释性。

Method: 首先从一个已知（伪）线性插值器出发，利用最新理论将其映射为一个有向图滤波器，对应的邻接矩阵作为神经网络权重初始化基线。随后，从数据中学习微扰矩阵P和P(2)进一步修正邻接矩阵，并通过Douglas-Rachford(DR)迭代算法实现恢复效果，将该迭代过程展开为可解释的轻量神经网络。

Result: 实验表明，本文方法在图像插值任务上实现了最新最优结果，并显著减少了神经网络参数总量。

Conclusion: 引入基于图滤波理论的初始化与优化，使得神经网络在降低参数量的同时，提升了图像插值性能，有效减少了局部最优风险，增强了网络可解释性。

Abstract: Conventional deep neural nets (DNNs) initialize network parameters at random
and then optimize each one via stochastic gradient descent (SGD), resulting in
substantial risk of poor-performing local minima.Focusing on the image
interpolation problem and leveraging a recent theorem that maps a
(pseudo-)linear interpolator {\Theta} to a directed graph filter that is a
solution to a MAP problem regularized with a graph shift variation (GSV) prior,
we first initialize a directed graph adjacency matrix A based on a known
interpolator {\Theta}, establishing a baseline performance.Then, towards
further gain, we learn perturbation matrices P and P(2) from data to augment A,
whose restoration effects are implemented via Douglas-Rachford (DR) iterations,
which we unroll into a lightweight interpretable neural net.Experimental
results demonstrate state-of-the-art image interpolation results, while
drastically reducing network parameters.

</details>


### [120] [Sphere-GAN: a GAN-based Approach for Saliency Estimation in 360° Videos](https://arxiv.org/abs/2509.11948)
*Mahmoud Z. A. Wahba,Sara Baldoni,Federica Battisti*

Main category: cs.CV

TL;DR: 本文提出Sphere-GAN，一种用于360°视频显著性检测的生成对抗网络方法，并在公开数据集上验证其精度优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着沉浸式应用的流行，360°影像的处理和传输需求大幅增加。显著性检测可以聚焦于视觉重点区域，帮助优化处理算法，但现有360°内容的显著性方法很少。

Method: 作者提出了Sphere-GAN框架，利用具备球面卷积的生成对抗网络（GAN）来进行360°视频显著性区域检测。该方法能够针对360°球面数据建模视觉显著性。

Result: 在公开360°视频显著性数据集上，与主流方法对比实验结果表明，Sphere-GAN在显著图预测的精确性上有明显优势。

Conclusion: Sphere-GAN成功提升了360°视频显著性检测的准确率，为沉浸式内容的高效处理和传输提供了更优的工具。

Abstract: The recent success of immersive applications is pushing the research
community to define new approaches to process 360{\deg} images and videos and
optimize their transmission. Among these, saliency estimation provides a
powerful tool that can be used to identify visually relevant areas and,
consequently, adapt processing algorithms. Although saliency estimation has
been widely investigated for 2D content, very few algorithms have been proposed
for 360{\deg} saliency estimation. Towards this goal, we introduce Sphere-GAN,
a saliency detection model for 360{\deg} videos that leverages a Generative
Adversarial Network with spherical convolutions. Extensive experiments were
conducted using a public 360{\deg} video saliency dataset, and the results
demonstrate that Sphere-GAN outperforms state-of-the-art models in accurately
predicting saliency maps.

</details>


### [121] [CLAIRE: A Dual Encoder Network with RIFT Loss and Phi-3 Small Language Model Based Interpretability for Cross-Modality Synthetic Aperture Radar and Optical Land Cover Segmentation](https://arxiv.org/abs/2509.11952)
*Debopom Sutradhar,Arefin Ittesafun Abian,Mohaimenul Azam Khan Raiaan,Reem E. Mohamed,Sheikh Izzal Azid,Sami Azam*

Main category: cs.CV

TL;DR: 本文提出了一种融合光学与SAR卫星影像的双编码器（Dual Encoder）架构，并结合跨模态注意力机制（CLAIRE）和类别不平衡损失函数（RIFT），提升了地表覆盖分类的准确性与对小样本类别的识别能力。此外，还采用小型语言模型解释分类结果，提高了模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 遥感卫星影像的地表覆盖分类对环境监测和资源管理至关重要，但受限于自然景观复杂、各类影像视觉差异小、数据集类别不均衡的问题，当前分类效果存在显著不足。该论文旨在通过新型模型结构和损失函数，提升分类准确性和泛化能力，特别是对稀有类别的识别。

Method: 1）设计了同时处理光学与SAR影像的双编码器结构，独立提取各自特征；2）采用CLAIRE跨模态注意力融合模块强化互补性空间与纹理信息；3）结合加权Focal Loss与Tversky Loss的RIFT损失函数，有效应对类别不平衡；4）引入小型语言模型生成的推理模块，为每个样本提供专家级可解释性。

Result: 在WHU-OPT-SAR数据集上达到mIoU 56.02%、整体准确率84.56%；OpenEarthMap-SAR数据集上mIoU 59.89%、整体准确率73.92%；PIE-RGB-SAR数据集（云遮挡条件下）mIoU高达86.86%、准确率94.58%。模型表现出良好的泛化性和鲁棒性。

Conclusion: 提出的CLAIRE方法显著提高了不同影像模态下的地表覆盖分类效果，尤其在类别不均衡和复杂场景下表现突出。其解释性模块增强了模型的可信度，对于实际环境监测和资源管理具有重要应用价值。

Abstract: Accurate land cover classification from satellite imagery is crucial in
environmental monitoring and sustainable resource management. However, it
remains challenging due to the complexity of natural landscapes, the visual
similarity between classes, and the significant class imbalance in the
available datasets. To address these issues, we propose a dual encoder
architecture that independently extracts modality-specific features from
optical and Synthetic Aperture Radar (SAR) imagery, which are then fused using
a cross-modality attention-fusion module named Cross-modality Land cover
segmentation with Attention and Imbalance-aware Reasoning-Enhanced Explanations
(CLAIRE). This fusion mechanism highlights complementary spatial and textural
features, enabling the network to better capture detailed and diverse land
cover patterns. We incorporate a hybrid loss function that utilizes Weighted
Focal Loss and Tversky Loss named RIFT (Rare-Instance Focal-Tversky) to address
class imbalance and improve segmentation performance across underrepresented
categories. Our model achieves competitive performance across multiple
benchmarks: a mean Intersection over Union (mIoU) of 56.02% and Overall
Accuracy (OA) of 84.56% on the WHU-OPT-SAR dataset; strong generalization with
a mIoU of 59.89% and OA of 73.92% on the OpenEarthMap-SAR dataset; and
remarkable robustness under cloud-obstructed conditions, achieving an mIoU of
86.86% and OA of 94.58% on the PIE-RGB-SAR dataset. Additionally, we introduce
a metric-driven reasoning module generated by a Small Language Model (Phi-3),
which generates expert-level, sample-specific justifications for model
predictions, thereby enhancing transparency and interpretability.

</details>


### [122] [Robust Concept Erasure in Diffusion Models: A Theoretical Perspective on Security and Robustness](https://arxiv.org/abs/2509.12024)
*Zixuan Fu,Yan Ren,Finn Carter,Chenyue Wen,Le Ku,Daheng Yu,Emily Davis,Bo Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种用于扩散模型的概念擦除新方法SCORE，有效移除敏感或有害概念，同时保持模型生成性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然在图像生成任务上取得巨大成功，但也带来了隐私、伦理、公平等风险。需要能有效从模型中移除敏感或有害内容，而不损害其整体生成能力。

Method: 提出SCORE（Secure and Concept-Oriented Robust Erasure）框架，将概念擦除建模为对抗独立性问题，通过最小化生成输出与目标概念的互信息，从理论上保证了被擦除概念和输出结果的统计独立性。引入对抗优化、轨迹一致性及显著性驱动的微调等技术。

Result: 在Stable Diffusion和FLUX等模型上，SCORE在对象擦除、NSFW移除、名人面孔抑制、艺术风格遗忘等四个基准上进行测试，比现有EraseAnything、ANT、MACE等方法在擦除效果上最高提升12.5%，并保持甚至提升图像质量。

Conclusion: SCORE方法在安全、鲁棒地从扩散模型中擦除敏感或有害概念方面达到了新标准，为模型安全和合规应用提供有效技术路径。

Abstract: Diffusion models have achieved unprecedented success in image generation but
pose increasing risks in terms of privacy, fairness, and security. A growing
demand exists to \emph{erase} sensitive or harmful concepts (e.g., NSFW
content, private individuals, artistic styles) from these models while
preserving their overall generative capabilities. We introduce \textbf{SCORE}
(Secure and Concept-Oriented Robust Erasure), a novel framework for robust
concept removal in diffusion models. SCORE formulates concept erasure as an
\emph{adversarial independence} problem, theoretically guaranteeing that the
model's outputs become statistically independent of the erased concept. Unlike
prior heuristic methods, SCORE minimizes the mutual information between a
target concept and generated outputs, yielding provable erasure guarantees. We
provide formal proofs establishing convergence properties and derive upper
bounds on residual concept leakage. Empirically, we evaluate SCORE on Stable
Diffusion and FLUX across four challenging benchmarks: object erasure, NSFW
removal, celebrity face suppression, and artistic style unlearning. SCORE
consistently outperforms state-of-the-art methods including EraseAnything, ANT,
MACE, ESD, and UCE, achieving up to \textbf{12.5\%} higher erasure efficacy
while maintaining comparable or superior image quality. By integrating
adversarial optimization, trajectory consistency, and saliency-driven
fine-tuning, SCORE sets a new standard for secure and robust concept erasure in
diffusion models.

</details>


### [123] [RAM++: Robust Representation Learning via Adaptive Mask for All-in-One Image Restoration](https://arxiv.org/abs/2509.12039)
*Zilong Zhang,Chujie Qin,Chunle Guo,Yong Zhang,Chao Xue,Ming-Ming Cheng,Chongyi Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为RAM++的自适应掩码鲁棒表征学习方法，实现了一站式的图像复原，性能在多种损坏类型下均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于劣化类型的方法在极端损坏（如与图像结构强相关的损坏）下表现有限，且普遍存在跨任务表现不均衡、易过拟合已见损坏、泛化性差等问题。作者希望通过结合高层语义和低层纹理，实现内容导向的稳健复原。

Method: RAM++是一个两阶段框架，包含三项核心设计：1）自适应语义掩码（AdaSAM）预训练，针对语义丰富和纹理区域生成像素级掩码，使网络学习生成先验和内容先验；2）掩码属性导通（MAC）选择性微调，对高贡献层进行微调，弥合掩码预训练与全图微调之间的差距；3）鲁棒特征正则化（RFR）结合DINOv2的表征能力与高效特征融合，实现准确且语义一致的复原。

Result: 实验表明，RAM++在已知、未知、极端和混合损坏场景下均获得了稳健、均衡并且优于现有技术的复原表现。

Conclusion: RAM++有效解决了传统方法的不足，首次实现了在多个损坏类型和极端情况下的稳定、均衡复原。代码和模型将会开源。

Abstract: This work presents Robust Representation Learning via Adaptive Mask (RAM++),
a two-stage framework for all-in-one image restoration. RAM++ integrates
high-level semantic understanding with low-level texture generation to achieve
content-oriented robust restoration. It addresses the limitations of existing
degradation-oriented methods in extreme scenarios (e.g., degradations strongly
coupled with image structures). RAM++ also mitigates common challenges such as
unbalanced performance across tasks, overfitting to seen degradations, and weak
generalization to unseen ones through three key designs: 1) Adaptive
Semantic-Aware Mask (AdaSAM): a pretraining strategy that applies pixel-level
masks to semantically rich and textured regions. This design enables the
network to learn both generative priors and image content priors from various
degradations. 2) Mask Attribute Conductance (MAC): a selective fine-tuning
strategy that adjusts the layers with higher contributions to bridge the
integrity gap between masked pretraining and full-image fine-tuning while
retaining learned priors. 3) Robust Feature Regularization (RFR): a strategy
that leverages DINOv2's semantically consistent and degradation-invariant
representations, together with efficient feature fusion, to achieve faithful
and semantically coherent restoration. With these designs, RAM++ achieves
robust, well-balanced, and state-of-the-art performance across seen, unseen,
extreme, and mixed degradations. Our code and model will be released at
https://github.com/DragonisCV/RAM

</details>


### [124] [Exploring Efficient Open-Vocabulary Segmentation in the Remote Sensing](https://arxiv.org/abs/2509.12040)
*Bingyu Li,Haocheng Dong,Da Zhang,Zhiyuan Zhao,Junyu Gao,Xuelong Li*

Main category: cs.CV

TL;DR: 本文提出了OVRSISBench基准和RSKT-Seg方法，推动遥感领域的开放词汇分割研究，并显著提高了分割性能和推理效率。


<details>
  <summary>Details</summary>
Motivation: 开放词汇分割（OVS）在自然图像中的研究较多，但在遥感图像（RS）中仍然缺乏研究，主要因为缺少统一评测基准及自然图像和遥感图像间的领域差异。

Method: 首先构建了OVRSISBench标准基准，对主流OVS/OVRSIS模型进行综合评测。针对其局限性，提出了RSKT-Seg，包括（1）多方向成本图聚合模块（RS-CMA），提取旋转不变视觉特征；（2）高效成本图融合模块（RS-Fusion），联合建模空间和语义关系并降低维度；（3）遥感知识迁移模块（RS-Transfer），通过增强上采样进行领域适应。

Result: RSKT-Seg在新基准上相较于主流OVS方法提升了3.8点mIoU和5.9点mACC，并且推理速度提升2倍。

Conclusion: 建立了遥感图像开放词汇分割的评测标准，并提出了具备高精度和高效率的新方法RSKT-Seg，对领域自适应有良好促进作用，有望推动该领域研究发展。

Abstract: Open-Vocabulary Remote Sensing Image Segmentation (OVRSIS), an emerging task
that adapts Open-Vocabulary Segmentation (OVS) to the remote sensing (RS)
domain, remains underexplored due to the absence of a unified evaluation
benchmark and the domain gap between natural and RS images. To bridge these
gaps, we first establish a standardized OVRSIS benchmark (\textbf{OVRSISBench})
based on widely-used RS segmentation datasets, enabling consistent evaluation
across methods. Using this benchmark, we comprehensively evaluate several
representative OVS/OVRSIS models and reveal their limitations when directly
applied to remote sensing scenarios. Building on these insights, we propose
\textbf{RSKT-Seg}, a novel open-vocabulary segmentation framework tailored for
remote sensing. RSKT-Seg integrates three key components: (1) a
Multi-Directional Cost Map Aggregation (RS-CMA) module that captures
rotation-invariant visual cues by computing vision-language cosine similarities
across multiple directions; (2) an Efficient Cost Map Fusion (RS-Fusion)
transformer, which jointly models spatial and semantic dependencies with a
lightweight dimensionality reduction strategy; and (3) a Remote Sensing
Knowledge Transfer (RS-Transfer) module that injects pre-trained knowledge and
facilitates domain adaptation via enhanced upsampling. Extensive experiments on
the benchmark show that RSKT-Seg consistently outperforms strong OVS baselines
by +3.8 mIoU and +5.9 mACC, while achieving 2x faster inference through
efficient aggregation. Our code is
\href{https://github.com/LiBingyu01/RSKT-Seg}{\textcolor{blue}{here}}.

</details>


### [125] [Layout-Conditioned Autoregressive Text-to-Image Generation via Structured Masking](https://arxiv.org/abs/2509.12046)
*Zirui Zheng,Takashi Isobe,Tong Shen,Xu Jia,Jianbin Zhao,Xiaomin Li,Mengmeng Ge,Baolu Li,Qinghe Wang,Dong Li,Dong Zhou,Yunzhi Zhuge,Huchuan Lu,Emad Barsoum*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于自回归（AR）模型的布局到图像生成方法SMARLI，有效整合空间布局约束并提升生成质量。


<details>
  <summary>Details</summary>
Motivation: AR模型在图像生成任务表现突出，但在受布局条件约束的生成中面临困难，主要由于布局信息稀疏且特征易交叉混淆。本文旨在解决AR模型在布局受控图像生成中的控制力和质量问题。

Method: 提出了结构化掩码（Structured Masking）策略，将其应用于AR模型的注意力机制，引导全局提示、布局和图像之间的合理交互，防止区域与描述的误配。同时，采用基于GRPO的后训练策略和定制的布局奖励函数进一步提升布局准确性和生成质量。

Result: 实验表明，SMARLI可以无缝融合布局、文本和图像信息，既显著提升了生成中的布局可控性，又保持了AR模型的结构简洁性与生成效率。

Conclusion: SMARLI在保持高生成质量的前提下，实现了更强的布局控制和结构效率，是布局到图像生成领域的一种有效AR模型扩展方案。

Abstract: While autoregressive (AR) models have demonstrated remarkable success in
image generation, extending them to layout-conditioned generation remains
challenging due to the sparse nature of layout conditions and the risk of
feature entanglement. We present Structured Masking for AR-based
Layout-to-Image (SMARLI), a novel framework for layoutto-image generation that
effectively integrates spatial layout constraints into AR-based image
generation. To equip AR model with layout control, a specially designed
structured masking strategy is applied to attention computation to govern the
interaction among the global prompt, layout, and image tokens. This design
prevents mis-association between different regions and their descriptions while
enabling sufficient injection of layout constraints into the generation
process. To further enhance generation quality and layout accuracy, we
incorporate Group Relative Policy Optimization (GRPO) based post-training
scheme with specially designed layout reward functions for next-set-based AR
models. Experimental results demonstrate that SMARLI is able to seamlessly
integrate layout tokens with text and image tokens without compromising
generation quality. It achieves superior layoutaware control while maintaining
the structural simplicity and generation efficiency of AR models.

</details>


### [126] [A Computer Vision Pipeline for Individual-Level Behavior Analysis: Benchmarking on the Edinburgh Pig Dataset](https://arxiv.org/abs/2509.12047)
*Haiyu Yang,Enhong Liu,Jennifer Sun,Sumit Sharma,Meike van Leerdam,Sebastien Franceschini,Puchun Niu,Miel Hostens*

Main category: cs.CV

TL;DR: 提出一个模块化的开源视觉分析流程，自动化分析猪群行为，显著提升识别准确率和追踪效果，助力精细化养殖。


<details>
  <summary>Details</summary>
Motivation: 传统的动物行为人工观察效率低、主观性强且难以推广，限制了规模化养殖中的福利与健康监测，因此需要自动化、客观的方法。

Method: 构建了一个模块化流程，融合了零样本目标检测、运动感知追踪与分割、视觉Transformer高级特征提取等多种计算机视觉技术，实现对群养环境中动物行为的自动化识别与分析。

Result: 在Edinburgh猪行为数据集上，方案行为识别准确率达94.2%，比现有方法提升21.2个百分点；追踪保持率93.3%，目标检测精度89.3%。

Conclusion: 该流程为猪只行为监测和福利评估提供了高效、自动、连续的开源解决方案，具备跨领域拓展潜力，但跨物种泛化效果需进一步验证。

Abstract: Animal behavior analysis plays a crucial role in understanding animal
welfare, health status, and productivity in agricultural settings. However,
traditional manual observation methods are time-consuming, subjective, and
limited in scalability. We present a modular pipeline that leverages
open-sourced state-of-the-art computer vision techniques to automate animal
behavior analysis in a group housing environment. Our approach combines
state-of-the-art models for zero-shot object detection, motion-aware tracking
and segmentation, and advanced feature extraction using vision transformers for
robust behavior recognition. The pipeline addresses challenges including animal
occlusions and group housing scenarios as demonstrated in indoor pig
monitoring. We validated our system on the Edinburgh Pig Behavior Video Dataset
for multiple behavioral tasks. Our temporal model achieved 94.2% overall
accuracy, representing a 21.2 percentage point improvement over existing
methods. The pipeline demonstrated robust tracking capabilities with 93.3%
identity preservation score and 89.3% object detection precision. The modular
design suggests potential for adaptation to other contexts, though further
validation across species would be required. The open-source implementation
provides a scalable solution for behavior monitoring, contributing to precision
pig farming and welfare assessment through automated, objective, and continuous
analysis.

</details>


### [127] [AvatarSync: Rethinking Talking-Head Animation through Autoregressive Perspective](https://arxiv.org/abs/2509.12052)
*Yuchen Deng,Xiuyang Wu,Hai-Tao Zheng,Suiyang Zhang,Yi He,Yuxing Han*

Main category: cs.CV

TL;DR: 本文提出了AvatarSync，一个基于音素表示的自回归模型，实现了高真实感且可控的说话头动画，显著提升了现有方法的视觉连续性、身份保持及推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的说话头动画技术普遍存在视频帧之间闪烁、身份漂移以及推理速度慢等问题，导致其在实际应用中受限，因此需要一种能生成高质量、稳定动画的方法。

Method: AvatarSync采用了“分而治之”的两阶段生成策略：第一阶段为面部关键帧生成，基于音素实现语义建模，通过音素到视觉的映射结合定制的因果注意力机制产生关键帧；第二阶段为帧间插值，采用带有时间戳自适应的选择性状态空间模型以增强时间一致性和视觉平滑性，同时优化了推理流程以减少延迟。

Result: 实验表明，AvatarSync在视觉保真度、时间一致性和计算效率等方面均优于当前的说话头动画方法。

Conclusion: AvatarSync为说话头动画提供了一种可扩展、可控且高效的新方案，克服了现有方法的主要缺点，可广泛用于实际应用。

Abstract: Existing talking-head animation approaches based on Generative Adversarial
Networks (GANs) or diffusion models often suffer from inter-frame flicker,
identity drift, and slow inference. These limitations inherent to their video
generation pipelines restrict their suitability for applications. To address
this, we introduce AvatarSync, an autoregressive framework on phoneme
representations that generates realistic and controllable talking-head
animations from a single reference image, driven directly text or audio input.
In addition, AvatarSync adopts a two-stage generation strategy, decoupling
semantic modeling from visual dynamics, which is a deliberate "Divide and
Conquer" design. The first stage, Facial Keyframe Generation (FKG), focuses on
phoneme-level semantic representation by leveraging the many-to-one mapping
from text or audio to phonemes. A Phoneme-to-Visual Mapping is constructed to
anchor abstract phonemes to character-level units. Combined with a customized
Text-Frame Causal Attention Mask, the keyframes are generated. The second
stage, inter-frame interpolation, emphasizes temporal coherence and visual
smoothness. We introduce a timestamp-aware adaptive strategy based on a
selective state space model, enabling efficient bidirectional context
reasoning. To support deployment, we optimize the inference pipeline to reduce
latency without compromising visual fidelity. Extensive experiments show that
AvatarSync outperforms existing talking-head animation methods in visual
fidelity, temporal consistency, and computational efficiency, providing a
scalable and controllable solution.

</details>


### [128] [Robust Fetal Pose Estimation across Gestational Ages via Cross-Population Augmentation](https://arxiv.org/abs/2509.12062)
*Sebastian Diaz,Benjamin Billot,Neel Dey,Molin Zhang,Esra Abaci Turk,P. Ellen Grant,Polina Golland,Elfar Adalsteinsson*

Main category: cs.CV

TL;DR: 本文提出了一种针对胎儿动作检测的跨人群数据增强框架，能提升在早孕龄胎儿群体中的姿态估计算法的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 胎儿动作是判断神经发育和宫内健康的重要指标，但现有基于3D EPI的动作定量方法在早孕龄（孕早期）普遍表现欠佳。这一方面由于解剖结构发生显著变化，另一方面由于缺乏早孕龄的标注数据。

Method: 作者提出了一种胎儿特定的数据增强策略，模拟早孕龄胎儿在子宫中的独特环境和姿态，并仅利用孕晚期标注图像，训练能泛化到早孕龄的姿态估计模型。

Result: 跨人群数据增强策略在孕晚期和难度较高的孕早期胎儿群体中均带来了姿态估计性能的显著提升，且结果表现更加稳定。

Conclusion: 该方法提升了孕期不同阶段胎儿动作检测的准确性，为早期临床检测和干预提供了潜在支持，拓展了4D胎儿影像分析在临床中的应用范围。

Abstract: Fetal motion is a critical indicator of neurological development and
intrauterine health, yet its quantification remains challenging, particularly
at earlier gestational ages (GA). Current methods track fetal motion by
predicting the location of annotated landmarks on 3D echo planar imaging (EPI)
time-series, primarily in third-trimester fetuses. The predicted landmarks
enable simplification of the fetal body for downstream analysis. While these
methods perform well within their training age distribution, they consistently
fail to generalize to early GAs due to significant anatomical changes in both
mother and fetus across gestation, as well as the difficulty of obtaining
annotated early GA EPI data. In this work, we develop a cross-population data
augmentation framework that enables pose estimation models to robustly
generalize to younger GA clinical cohorts using only annotated images from
older GA cohorts. Specifically, we introduce a fetal-specific augmentation
strategy that simulates the distinct intrauterine environment and fetal
positioning of early GAs. Our experiments find that cross-population
augmentation yields reduced variability and significant improvements across
both older GA and challenging early GA cases. By enabling more reliable pose
estimation across gestation, our work potentially facilitates early clinical
detection and intervention in challenging 4D fetal imaging settings. Code is
available at https://github.com/sebodiaz/cross-population-pose.

</details>


### [129] [End-to-End Learning of Multi-Organ Implicit Surfaces from 3D Medical Imaging Data](https://arxiv.org/abs/2509.12068)
*Farahdiba Zarin,Nicolas Padoy,Jérémy Dana,Vinkle Srivastav*

Main category: cs.CV

TL;DR: 本文提出了一种基于隐式表示的多器官三维重建新方法ImplMORe，能够在提供细粒度表面细节的同时克服高分辨率下的内存和计算瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前医学三维重建面临分辨率受限带来的细节丢失问题，传统高分辨率重建又导致内存和算力压力大。隐式表示在通用三维视觉领域表现出色，但由于架构和数据的差异，无法直接应用于医学图像领域，因此有必要开发针对医学图像的高效三维表面重建方法。

Method: 作者提出ImplMORe方法，基于端到端深度学习框架，结合3D CNN编码器提取局部特征，并在连续域内利用occupancy function进行多尺度插值表面重建，从而实现单器官或多器官的隐式表面高精度重建。

Result: 在totalsegmentator医学数据集上，ImplMORe在单器官和多器官三维重建任务中，细节还原能力优于基于离散显式表示的主流表面重建方法，能以高于原始输入分辨率的精度展现组织结构。

Conclusion: ImplMORe方法突破了现有医学图像三维重建的精度和计算瓶颈，能够有效提取和表达器官表面细节，有望为医学诊断和手术规划提供更高质量的重建支持。

Abstract: The fine-grained surface reconstruction of different organs from 3D medical
imaging can provide advanced diagnostic support and improved surgical planning.
However, the representation of the organs is often limited by the resolution,
with a detailed higher resolution requiring more memory and computing
footprint. Implicit representations of objects have been proposed to alleviate
this problem in general computer vision by providing compact and differentiable
functions to represent the 3D object shapes. However, architectural and
data-related differences prevent the direct application of these methods to
medical images. This work introduces ImplMORe, an end-to-end deep learning
method using implicit surface representations for multi-organ reconstruction
from 3D medical images. ImplMORe incorporates local features using a 3D CNN
encoder and performs multi-scale interpolation to learn the features in the
continuous domain using occupancy functions. We apply our method for single and
multiple organ reconstructions using the totalsegmentator dataset. By
leveraging the continuous nature of occupancy functions, our approach
outperforms the discrete explicit representation based surface reconstruction
approaches, providing fine-grained surface details of the organ at a resolution
higher than the given input image. The source code will be made publicly
available at: https://github.com/CAMMA-public/ImplMORe

</details>


### [130] [U-Mamba2: Scaling State Space Models for Dental Anatomy Segmentation in CBCT](https://arxiv.org/abs/2509.12069)
*Zhi Qin Tan,Xiatian Zhu,Owen Addison,Yunpeng Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于U-Net和Mamba2状态空间模型的新型神经网络架构U-Mamba2，用于口腔锥形束CT（CBCT）中多结构自动分割，取得了ToothFairy3比赛前3名的好成绩。


<details>
  <summary>Details</summary>
Motivation: CBCT图像在口腔领域广泛应用，但牙齿和下颌等结构的精准分割仍耗时且具挑战性，提高分割效率和准确性迫在眉睫。

Method: U-Mamba2将Mamba2状态空间模型嵌入U-Net架构，强化结构约束，结合交互点击提示与跨注意力块，并利用自监督预训练及牙科领域知识优化模型。

Result: 在ToothFairy3挑战赛的两个任务上，U-Mamba2分别取得了Mean Dice 0.792/HD95 93.19和Mean Dice 0.852/HD95 7.39的优异分割效果，并入围两项任务前三名。

Conclusion: U-Mamba2在CBCT多结构分割中兼具高效性和高准确性，具备实际临床应用潜力和推广价值，并已开源代码方便业界参考和使用。

Abstract: Cone-Beam Computed Tomography (CBCT) is a widely used 3D imaging technique in
dentistry, providing volumetric information about the anatomical structures of
jaws and teeth. Accurate segmentation of these anatomies is critical for
clinical applications such as diagnosis and surgical planning, but remains
time-consuming and challenging. In this paper, we present U-Mamba2, a new
neural network architecture designed for multi-anatomy CBCT segmentation in the
context of the ToothFairy3 challenge. U-Mamba2 integrates the Mamba2 state
space models into the U-Net architecture, enforcing stronger structural
constraints for higher efficiency without compromising performance. In
addition, we integrate interactive click prompts with cross-attention blocks,
pre-train U-Mamba2 using self-supervised learning, and incorporate dental
domain knowledge into the model design to address key challenges of dental
anatomy segmentation in CBCT. Extensive experiments, including independent
tests, demonstrate that U-Mamba2 is both effective and efficient, securing top
3 places in both tasks of the Toothfairy3 challenge. In Task 1, U-Mamba2
achieved a mean Dice of 0.792, HD95 of 93.19 with the held-out test data, with
an average inference time of XX (TBC during the ODIN workshop). In Task 2,
U-Mamba2 achieved the mean Dice of 0.852 and HD95 of 7.39 with the held-out
test data. The code is publicly available at
https://github.com/zhiqin1998/UMamba2.

</details>


### [131] [Progressive Flow-inspired Unfolding for Spectral Compressive Imaging](https://arxiv.org/abs/2509.12079)
*Xiaodong Wang,Ping Wang,Zijun He,Mengjie Qin,Xin Yuan*

Main category: cs.CV

TL;DR: 该论文提出了一种针对CASSI系统的可控轨迹展开框架，结合高效的空间-光谱Transformer和频域融合模块，实现了更平滑和高效的高光谱重建，实验效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: CASSI需要从单帧2D压缩测量中恢复3D高光谱图像，重建过程极具挑战性；现有深度展开网络虽然取得进展，但重建轨迹难以控制，导致质量波动且优化不平滑。

Method: 受扩散轨迹和流匹配思想启发，论文构建了一个可控轨迹的展开框架，使优化过程从初始噪声逐步过渡到高质量重建，减少突变。为提升效率，设计了专用于高光谱重建的空间-光谱Transformer和频域特征融合模块，提高特征一致性。

Result: 在仿真和真实数据上，所提方法在重建性能和计算效率方面均优于现有最先进方法。

Conclusion: 提出的可控轨迹展开框架结合高效的特征处理模块，为CASSI系统下的高光谱图像重建带来了更佳的质量和显著的效率提升，有望推动实际应用。

Abstract: Coded aperture snapshot spectral imaging (CASSI) retrieves a 3D hyperspectral
image (HSI) from a single 2D compressed measurement, which is a highly
challenging reconstruction task. Recent deep unfolding networks (DUNs),
empowered by explicit data-fidelity updates and implicit deep denoisers, have
achieved the state of the art in CASSI reconstruction. However, existing
unfolding approaches suffer from uncontrollable reconstruction trajectories,
leading to abrupt quality jumps and non-gradual refinement across stages.
Inspired by diffusion trajectories and flow matching, we propose a novel
trajectory-controllable unfolding framework that enforces smooth, continuous
optimization paths from noisy initial estimates to high-quality
reconstructions. To achieve computational efficiency, we design an efficient
spatial-spectral Transformer tailored for hyperspectral reconstruction, along
with a frequency-domain fusion module to gurantee feature consistency.
Experiments on simulation and real data demonstrate that our method achieves
better reconstruction quality and efficiency than prior state-of-the-art
approaches.

</details>


### [132] [End-to-End 4D Heart Mesh Recovery Across Full-Stack and Sparse Cardiac MRI](https://arxiv.org/abs/2509.12090)
*Yihong Chen,Jiancheng Yang,Deniz Sayin Mercadier,Hieu Le,Juerg Schwitter,Pascal Fua*

Main category: cs.CV

TL;DR: 本文提出了一种名为TetHeart的新方法，可基于完整或稀疏心脏磁共振图像序列，重建全集运动的多结构心脏4D网格。方法在多个公开与私人数据集上验证，取得了先进准确度和很强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于磁共振的心脏运动重建方法往往依赖完整的扫描序列，这在手术过程中难以获得，限制了方法的应用范围。因此，需要能够从稀疏甚至单张图像切片中重建心脏运动和结构的新方法，以提高实际临床环境下的可用性。

Method: 提出了TetHeart端到端框架，使用深度可变形四面体网格作为显式-隐式混合表示，支持从高质量完整扫描初始化并可用任意数量的切片（包括极其稀疏的输入）动态更新网格。技术创新包括切片自适应2D-3D特征拼装的注意机制，以及从完整切片到稀疏切片的知识蒸馏方法，配合仅需关键帧弱监督的两阶段运动学习方案。

Result: 在三个公开大型数据集上进行了训练和验证，并在更多公开和私人CMR数据集上零样本外部评估，TetHeart在完整和稀疏成像条件下均实现了业内先进的准确度和良好的泛化能力。

Conclusion: TetHeart实现了从完整到极端稀疏心脏磁共振切片的高精度、鲁棒性4D多结构心脏形态与运动重建，有望大幅提升其在术前准备及手术过程中的实用性和推广价值。

Abstract: Reconstructing cardiac motion from cine CMR sequences is critical for
diagnosis, prediction, and intervention. Existing methods rely on complete CMR
stacks to infer full heart motion, limiting their utility in intra-procedural
scenarios where only sparse observations are available. We present TetHeart,
the first end-to-end framework that unifies full 4D multi-structure heart mesh
recovery from both offline full-stack acquisitions and intra-procedural
sparse-slice observations. Our method leverages deep deformable tetrahedra, an
explicit-implicit hybrid representation, to capture shape and motion in a
coherent space shared across cardiac structures. It is initialized from
high-quality pre-procedural or offline-acquired full stacks to build detailed,
patient-specific heart meshes, which can then be updated using whatever slices
are available, from full stacks down to a single slice. We further incorporate
several key innovations: (i) an attentive mechanism for slice-adaptive 2D-3D
feature assembly that dynamically integrates information from arbitrary numbers
of slices at any position, combined with a distillation strategy from
full-slice to sparse-slice settings to ensure accurate reconstruction under
extreme sparsity; and (ii) a two-stage weakly supervised motion learning scheme
requiring only keyframe (e.g., ED and ES) annotations. Trained and validated on
three large public datasets and externally evaluated zero-shot on additional
private interventional and public CMR datasets, TetHeart achieves
state-of-the-art accuracy and strong generalization in both pre- and
intra-procedural settings.

</details>


### [133] [FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic Segmentation via Low-Rank Adaptation](https://arxiv.org/abs/2509.12105)
*Bernardo Forni,Gabriele Lombardi,Federico Pozzi,Mirco Planamente*

Main category: cs.CV

TL;DR: 本文提出了一种基于SAM2模型的Few-Shot语义分割方法（FS-SAM2），通过应用Low-Rank Adaptation（LoRA）对现有模块进行有效参数调整，实现了在多种数据集上的优异效果，且推理效率高。


<details>
  <summary>Details</summary>
Motivation: 现有Few-Shot分割方法通常需在额外模块上大量训练，参数多，成本高，且依赖大规模数据集，这限制了实际应用。SAM2虽为强大的分割模型，但对few-shot场景和多样化图片适应性有限，因此需提出结合其能力与高效适应性的新方法。

Method: 该方法直接利用SAM2的模块和视频处理能力，将其迁移到Few-Shot分割任务中，并对其原有模块采用Low-Rank Adaptation（LoRA）以适应静态图片的多样性。所需微调的参数很少，实现了训练与推理的高效与灵活。

Result: 在PASCAL-5$^i$、COCO-20$^i$、FSS-1000等标准Few-Shot分割数据集上取得了优异成绩，同时在推理阶段具有卓越的计算效率。

Conclusion: FS-SAM2方法能够有效适配SAM2模型到Few-Shot分割任务，只需微量参数微调即可实现高性能与高效率，具有较好的通用性和实际应用前景。

Abstract: Few-shot semantic segmentation has recently attracted great attention. The
goal is to develop a model capable of segmenting unseen classes using only a
few annotated samples. Most existing approaches adapt a pre-trained model by
training from scratch an additional module. Achieving optimal performance with
these approaches requires extensive training on large-scale datasets. The
Segment Anything Model 2 (SAM2) is a foundational model for zero-shot image and
video segmentation with a modular design. In this paper, we propose a Few-Shot
segmentation method based on SAM2 (FS-SAM2), where SAM2's video capabilities
are directly repurposed for the few-shot task. Moreover, we apply a Low-Rank
Adaptation (LoRA) to the original modules in order to handle the diverse images
typically found in standard datasets, unlike the temporally connected frames
used in SAM2's pre-training. With this approach, only a small number of
parameters is meta-trained, which effectively adapts SAM2 while benefiting from
its impressive segmentation performance. Our method supports any K-shot
configuration. We evaluate FS-SAM2 on the PASCAL-5$^i$, COCO-20$^i$ and
FSS-1000 datasets, achieving remarkable results and demonstrating excellent
computational efficiency during inference. Code is available at
https://github.com/fornib/FS-SAM2

</details>


### [134] [RailSafeNet: Visual Scene Understanding for Tram Safety](https://arxiv.org/abs/2509.12125)
*Ing. Ondrej Valach,Ing. Ivan Gruber*

Main category: cs.CV

TL;DR: 本论文提出RailSafeNet系统，运用图像处理和深度学习实时识别和评估电车轨道旁的风险对象，以提升电车与人、车等互动的安全性。


<details>
  <summary>Details</summary>
Motivation: 在密集城市环境中，电车和行人、骑车者等常发生潜在碰撞，如何利用AI技术实时检测危险，保障弱势路权方安全，是当前亟需解决的问题。

Method: 设计了RailSafeNet框架，融合语义分割（SegFormer B3模型）、目标检测（YOLOv8）和基于规则的距离评估，利用单目摄像头视频检测轨道、定位周边对象并评估其侵入风险。

Result: 在RailSem19数据集上，SegFormer B3模型获得65%的IoU，YOLOv8微调后在IoU 0.5阈值下获得75.6%的mAP，表明该系统能有效进行场景解析和风险分类。

Conclusion: RailSafeNet系统可在保证很少标注数据的条件下，为电车驾驶员实时预警潜在危险，提高了轨道及周边区域的交通安全性。

Abstract: Tram-human interaction safety is an important challenge, given that trams
frequently operate in densely populated areas, where collisions can range from
minor injuries to fatal outcomes. This paper addresses the issue from the
perspective of designing a solution leveraging digital image processing, deep
learning, and artificial intelligence to improve the safety of pedestrians,
drivers, cyclists, pets, and tram passengers. We present RailSafeNet, a
real-time framework that fuses semantic segmentation, object detection and a
rule-based Distance Assessor to highlight track intrusions. Using only
monocular video, the system identifies rails, localises nearby objects and
classifies their risk by comparing projected distances with the standard 1435mm
rail gauge. Experiments on the diverse RailSem19 dataset show that a
class-filtered SegFormer B3 model achieves 65% intersection-over-union (IoU),
while a fine-tuned YOLOv8 attains 75.6% mean average precision (mAP) calculated
at an intersection over union (IoU) threshold of 0.50. RailSafeNet therefore
delivers accurate, annotation-light scene understanding that can warn drivers
before dangerous situations escalate. Code available at
https://github.com/oValach/RailSafeNet.

</details>


### [135] [3DViT-GAT: A Unified Atlas-Based 3D Vision Transformer and Graph Learning Framework for Major Depressive Disorder Detection Using Structural MRI Data](https://arxiv.org/abs/2509.12143)
*Nojod M. Alotaibi,Areej M. Alhothali,Manar S. Ali*

Main category: cs.CV

TL;DR: 本论文提出了一种结合Vision Transformer（ViT）与图神经网络（GNN）的新型深度学习管道，用于通过结构磁共振成像（sMRI）自动检测重度抑郁障碍（MDD），并验证了基于脑区先验分区的方法在MDD检测中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于sMRI数据的MDD自动检测方法多依赖于体素级特征或是基于预定义脑图谱的手工区域特征，这些手段难以捕捉脑部结构的复杂规律。为提升诊断准确率与早期干预效率，亟需开发基于深度学习的先进方法，从而充分挖掘sMRI的潜在诊断价值。

Method: 构建了一个端到端流程，先用ViT从sMRI数据中提取3D脑区嵌入表示，然后用GNN进行分类。探索了两种区域定义方案：（1）基于结构和功能脑图谱的atlas-based方法；（2）直接将3D脑扫描均匀划分为cube，再用ViT学习非先验划分。接着，通过余弦相似度建立脑区关系图，引导GNN分类。使用REST-meta-MDD数据集与分层十折交叉验证评估方法。

Result: 最佳模型在十折交叉验证中，准确率为78.98%，敏感性76.54%，特异性81.58%，精确率81.58%，F1分数78.98%。同时，基于脑图谱的atlas-based模型检测效果优于cube-based模型。

Conclusion: ViT与GNN联合的自动检测框架能较好地区分MDD患者和健康对照，基于脑区知识的分区对提升检测表现尤为重要，显示领域先验在MDD影像检测中的价值。

Abstract: Major depressive disorder (MDD) is a prevalent mental health condition that
negatively impacts both individual well-being and global public health.
Automated detection of MDD using structural magnetic resonance imaging (sMRI)
and deep learning (DL) methods holds increasing promise for improving
diagnostic accuracy and enabling early intervention. Most existing methods
employ either voxel-level features or handcrafted regional representations
built from predefined brain atlases, limiting their ability to capture complex
brain patterns. This paper develops a unified pipeline that utilizes Vision
Transformers (ViTs) for extracting 3D region embeddings from sMRI data and
Graph Neural Network (GNN) for classification. We explore two strategies for
defining regions: (1) an atlas-based approach using predefined structural and
functional brain atlases, and (2) an cube-based method by which ViTs are
trained directly to identify regions from uniformly extracted 3D patches.
Further, cosine similarity graphs are generated to model interregional
relationships, and guide GNN-based classification. Extensive experiments were
conducted using the REST-meta-MDD dataset to demonstrate the effectiveness of
our model. With stratified 10-fold cross-validation, the best model obtained
78.98% accuracy, 76.54% sensitivity, 81.58% specificity, 81.58% precision, and
78.98% F1-score. Further, atlas-based models consistently outperformed the
cube-based approach, highlighting the importance of using domain-specific
anatomical priors for MDD detection.

</details>


### [136] [Open-ended Hierarchical Streaming Video Understanding with Vision Language Models](https://arxiv.org/abs/2509.12145)
*Hyolim Kang,Yunsu Park,Youngbeom Yoo,Yeeun Choi,Seon Joo Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新的分层流式视频理解任务（Hierarchical Streaming Video Understanding），并推出了名为OpenHOUSE的新系统，实现了更精细的实时视频事件分割和描述生成。实验显示该系统相比以往方法性能提升近一倍。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏带有分层和细粒度时序标注的数据集，且现有流式动作识别方法对复杂事件理解能力有限，难以应对连续、复杂多样的视频事件场景。

Method: 1）利用大模型（LLM）将视频中简单动作聚合为高层次事件，丰富现有数据集；2）提出OpenHOUSE系统，包含针对流式视频的专用模块，能更精确检测相邻复杂动作间的边界，并生成文本描述。

Result: OpenHOUSE可以更准确识别动作边界，在处理动作紧邻的视频流上，相较于直接改进的传统方法，性能几乎提升一倍。

Conclusion: OpenHOUSE证明了结合生成式大模型能有效提升流式视频事件理解的能力，为未来智能流式视频分析和描述生成指明了方向。

Abstract: We introduce Hierarchical Streaming Video Understanding, a task that combines
online temporal action localization with free-form description generation.
Given the scarcity of datasets with hierarchical and fine-grained temporal
annotations, we demonstrate that LLMs can effectively group atomic actions into
higher-level events, enriching existing datasets. We then propose OpenHOUSE
(Open-ended Hierarchical Online Understanding System for Events), which extends
streaming action perception beyond action classification. OpenHOUSE features a
specialized streaming module that accurately detects boundaries between closely
adjacent actions, nearly doubling the performance of direct extensions of
existing methods. We envision the future of streaming action perception in the
integration of powerful generative models, with OpenHOUSE representing a key
step in that direction.

</details>


### [137] [Multi Anatomy X-Ray Foundation Model](https://arxiv.org/abs/2509.12146)
*Nishank Singla,Krisztian Koos,Farzin Haddadpour,Amin Honarmandi Shandiz,Lovish Chum,Xiaojian Xu,Qing Jin,Erhan Bas*

Main category: cs.CV

TL;DR: 本文提出了XR-0，多解剖部位的X光基础模型，并在多种任务和数据集上实现了最佳或有竞争力的表现。


<details>
  <summary>Details</summary>
Motivation: 目前的X光AI基础模型主要集中在胸部解剖学，难以广泛适用于更多样的临床任务和解剖部位，因此需要开发更具通用性和鲁棒性的基础模型。

Method: 作者构建了包含115万张涵盖多种解剖部位的私有X光影像大数据集，采用自监督学习训练XR-0，并在12个公开数据集、20个下游任务（如分类、检索、分割、定位、视觉定位和报告生成）上进行评估。

Result: XR-0在多解剖学任务上取得了最先进的表现，在胸部相关任务中也仍具有较强竞争力。

Conclusion: 多解剖部位的数据和自监督学习是构建通用医疗视觉模型的关键因素，XR-0为影像科室更具扩展性和适应性的AI系统奠定了基础。

Abstract: X-ray imaging is a ubiquitous in radiology, yet most existing AI foundation
models are limited to chest anatomy and fail to generalize across broader
clinical tasks. In this work, we introduce XR-0, the multi-anatomy X-ray
foundation model using self-supervised learning on a large, private dataset of
1.15 million images spanning diverse anatomical regions and evaluated across 12
datasets and 20 downstream tasks, including classification, retrieval,
segmentation, localization, visual grounding, and report generation. XR-0
achieves state-of-the-art performance on most multi-anatomy tasks and remains
competitive on chest-specific benchmarks. Our results demonstrate that
anatomical diversity and supervision are critical for building robust,
general-purpose medical vision models, paving the way for scalable and
adaptable AI systems in radiology.

</details>


### [138] [LoRA-fine-tuned Large Vision Models for Automated Assessment of Post-SBRT Lung Injury](https://arxiv.org/abs/2509.12155)
*M. Bolhassani,B. Veasey,E. Daugherty,S. Keltner,N. Kumar,N. Dunlap,A. Amini*

Main category: cs.CV

TL;DR: 本研究评估了Low-Rank Adaptation（LoRA）在微调大型视觉模型（DinoV2和SwinV2）以诊断体部立体定向放疗（SBRT）后射线肺损伤（RILI）方面的有效性，并与传统的全量微调和仅推理方法进行了对比。结果显示，LoRA在大幅降低计算成本和训练时间的同时，性能可与传统微调媲美甚至更优。


<details>
  <summary>Details</summary>
Motivation: 放疗后射线肺损伤（RILI）诊断对提升临床疗效具有重要意义，但微调大规模视觉模型存在资源消耗大等局限，亟需高效的适应性方法。

Method: 采用LoRA对DinoV2和SwinV2模型进行参数高效微调，与全微调及无微调（仅推理）方式进行对比，同时探究2D模型适应3D CT数据的不同方案，并评估不同尺寸（50mm³和75mm³）的裁剪图像对模型空间上下文敏感性的影响。

Result: 实验显示，LoRA微调在大幅减少可训练参数、降低计算资源和训练时间的基础上，其模型性能与传统全参数微调相当或更优。

Conclusion: LoRA为大视觉模型在医学图像分析中的高效微调提供了有效途径，可在保证诊断性能的前提下显著降低资源需求，促进相关模型在临床实际中的应用。

Abstract: This study investigates the efficacy of Low-Rank Adaptation (LoRA) for
fine-tuning large Vision Models, DinoV2 and SwinV2, to diagnose
Radiation-Induced Lung Injury (RILI) from X-ray CT scans following Stereotactic
Body Radiation Therapy (SBRT). To evaluate the robustness and efficiency of
this approach, we compare LoRA with traditional full fine-tuning and
inference-only (no fine-tuning) methods. Cropped images of two sizes (50 mm3
and 75 mm3), centered at the treatment isocenter, in addition to different
adaptation techniques for adapting the 2D LVMs for 3D data were used to
determine the sensitivity of the models to spatial context. Experimental
results show that LoRA achieves comparable or superior performance to
traditional fine-tuning while significantly reducing computational costs and
training times by requiring fewer trainable parameters.

</details>


### [139] [HoloGarment: 360° Novel View Synthesis of In-the-Wild Garments](https://arxiv.org/abs/2509.12187)
*Johanna Karras,Yingwei Li,Yasamin Jafarian,Ira Kemelmacher-Shlizerman*

Main category: cs.CV

TL;DR: 本文提出了一种用于野外服装新视角合成的高效方法HoloGarment，能够从1-3张图片或视频生成服装的360°新视角，显著提升了真实场景中衣物处理的精确性和真实性。


<details>
  <summary>Details</summary>
Motivation: 以往新视角合成方法多依赖简单的合成3D模型，难以适应真实环境下衣物的高度遮挡、复杂人体姿势和布料变形。纸中旨在解决野外服装NVS任务的泛化与真实感难题。

Method: 提出HoloGarment方法，将大量真实视频数据与少量合成3D数据结合，训练一个共享的服装嵌入空间。在推理时，通过对某一真实视频进行微调，构建服装专属的“atlas”表示，实现动态视频到360°新视角合成。该表示与人体姿态和动作无关，捕捉了多视角下的衣物几何与纹理。

Result: 实验显示该方法在NVS任务下达到最新的性能，不仅能保持高水平的写实感、视图一致性、细节，还能在服装褶皱、姿势变化、遮挡等复杂情形下稳定工作，优于现有方法。

Conclusion: HoloGarment可有效实现真实衣物的高质量新视角合成，为服装虚拟展示与相关行业应用提供了强有力的技术支持。

Abstract: Novel view synthesis (NVS) of in-the-wild garments is a challenging task due
significant occlusions, complex human poses, and cloth deformations. Prior
methods rely on synthetic 3D training data consisting of mostly unoccluded and
static objects, leading to poor generalization on real-world clothing. In this
paper, we propose HoloGarment (Hologram-Garment), a method that takes 1-3
images or a continuous video of a person wearing a garment and generates
360{\deg} novel views of the garment in a canonical pose. Our key insight is to
bridge the domain gap between real and synthetic data with a novel implicit
training paradigm leveraging a combination of large-scale real video data and
small-scale synthetic 3D data to optimize a shared garment embedding space.
During inference, the shared embedding space further enables dynamic
video-to-360{\deg} NVS through the construction of a garment "atlas"
representation by finetuning a garment embedding on a specific real-world
video. The atlas captures garment-specific geometry and texture across all
viewpoints, independent of body pose or motion. Extensive experiments show that
HoloGarment achieves state-of-the-art performance on NVS of in-the-wild
garments from images and videos. Notably, our method robustly handles
challenging real-world artifacts -- such as wrinkling, pose variation, and
occlusion -- while maintaining photorealism, view consistency, fine texture
details, and accurate geometry. Visit our project page for additional results:
https://johannakarras.github.io/HoloGarment

</details>


### [140] [Domain-Adaptive Pretraining Improves Primate Behavior Recognition](https://arxiv.org/abs/2509.12193)
*Felix B. Mueller,Timo Lueddecke,Richard Vogg,Alexander S. Ecker*

Main category: cs.CV

TL;DR: 本论文提出采用自监督学习和领域自适应预训练（DAP）的方法，提高了大型猿类行为视频动作识别的精度，减少了对标签数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 野生动物视频数据的收集因相机陷阱的普及变得规模化，但手工标注的高成本限制了大规模数据集的创建。研究亟需高效、低依赖标注的数据学习方法，以协助生态学、动物认知及保护领域的研究。

Method: 作者采用了预训练的V-JEPA模型，并在无监督情况下利用领域内数据进行进一步预训练（DAP），然后对猿类行为视频数据（PanAf与ChimpACT两个数据集）进行动作识别任务，并评估其准确率与平均精度均值（mAP）。

Result: 该方法在两个大型猿类行为数据集上分别比已有的动作识别方法提升了6.1个百分点的准确率和6.3个百分点的mAP。实验表明主要性能提升归功于领域自适应预训练（DAP）。

Conclusion: 领域自适应预训练（DAP）能够在无需标注数据的情况下显著提升动物行为识别的性能，为相关领域大规模无标签视频数据的分析与理解提供了极有前景的技术方案。

Abstract: Computer vision for animal behavior offers promising tools to aid research in
ecology, cognition, and to support conservation efforts. Video camera traps
allow for large-scale data collection, but high labeling costs remain a
bottleneck to creating large-scale datasets. We thus need data-efficient
learning approaches. In this work, we show that we can utilize self-supervised
learning to considerably improve action recognition on primate behavior. On two
datasets of great ape behavior (PanAf and ChimpACT), we outperform published
state-of-the-art action recognition models by 6.1 %pt. accuracy and 6.3 %pt.
mAP, respectively. We achieve this by utilizing a pretrained V-JEPA model and
applying domain-adaptive pretraining (DAP), i.e. continuing the pretraining
with in-domain data. We show that most of the performance gain stems from the
DAP. Our method promises great potential for improving the recognition of
animal behavior, as DAP does not require labeled samples. Code is available at
https://github.com/ecker-lab/dap-behavior

</details>


### [141] [3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review](https://arxiv.org/abs/2509.12197)
*Salma Galaaoui,Eduardo Valle,David Picard,Nermin Samet*

Main category: cs.CV

TL;DR: 本文综述了基于野外LiDAR点云的3D人体姿态估计和人体网格恢复，提出方法分类体系，比较主流方法与数据集，并建立基准表，推动领域发展。


<details>
  <summary>Details</summary>
Motivation: 随着LiDAR传感器在自动驾驶、安防等应用中的普及，基于LiDAR点云的3D人体姿态估计和网格恢复成为热点研究方向，然而相关方法繁多、评测标准不统一，缺乏系统总结。本文旨在梳理现有方法，提供统一评价体系，推动技术进步。

Method: 本文系统回顾了相关领域的方法，并提出了结构化的分类体系。按照该分类，对比各方法的优缺点和设计选择；对三大主流公开数据集进行定量分析，统一评估指标定义，并根据数据集建立基准表，促进公平对比和方法进步。同时开设网页持续整理最新文献。

Result: 梳理并比较了各类方法和数据集，统一了术语与评测标准，建立了公认基准表，对比分析了主流数据集特点，为研究者提供了一个全面的资源和参考。

Conclusion: 本文为基于LiDAR点云的3D人体研究提供了系统性综述和工具，明确了方法分级、标准与基准，有助于社区聚焦挑战和未来方向，推动该领域持续发展。

Abstract: In this paper, we present a comprehensive review of 3D human pose estimation
and human mesh recovery from in-the-wild LiDAR point clouds. We compare
existing approaches across several key dimensions, and propose a structured
taxonomy to classify these methods. Following this taxonomy, we analyze each
method's strengths, limitations, and design choices. In addition, (i) we
perform a quantitative comparison of the three most widely used datasets,
detailing their characteristics; (ii) we compile unified definitions of all
evaluation metrics; and (iii) we establish benchmark tables for both tasks on
these datasets to enable fair comparisons and promote progress in the field. We
also outline open challenges and research directions critical for advancing
LiDAR-based 3D human understanding. Moreover, we maintain an accompanying
webpage that organizes papers according to our taxonomy and continuously update
it with new studies:
https://github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR

</details>


### [142] [OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling](https://arxiv.org/abs/2509.12201)
*Yang Zhou,Yifan Wang,Jianjun Zhou,Wenzheng Chang,Haoyu Guo,Zizun Li,Kaijing Ma,Xinyue Li,Yating Wang,Haoyi Zhu,Mingyu Liu,Dingning Liu,Jiange Yang,Zhoujie Fu,Junyi Chen,Chunhua Shen,Jiangmiao Pang,Kaipeng Zhang,Tong He*

Main category: cs.CV

TL;DR: 本文介绍了OmniWorld——一个为4D世界建模（包含空间几何与时间动态）而专门设计的大规模、多领域、多模态数据集，用于弥补现有数据集在动态复杂性和标注方面的不足。


<details>
  <summary>Details</summary>
Motivation: 尽管4D世界建模领域发展迅速，但现有高质量数据集不足，难以支持复杂环境下的重建、预测及视频生成等关键任务。

Method: 作者提出并收集了OmniWorld数据集，包括新采集的OmniWorld-Game数据以及多个精选的公开数据集，涵盖多领域和多模态内容，并建立了基于该数据集的基准测试。

Result: 实验显示，在OmniWorld上微调用现有的SOTA方法能显著提升4D重建与视频生成等任务的表现，同时新基准暴露了现有方法的限制。

Conclusion: OmniWorld为通用4D世界建模提供了重要资源，有助于推动机器对物理世界的整体理解和相关技术的发展。

Abstract: The field of 4D world modeling - aiming to jointly capture spatial geometry
and temporal dynamics - has witnessed remarkable progress in recent years,
driven by advances in large-scale generative models and multimodal learning.
However, the development of truly general 4D world models remains fundamentally
constrained by the availability of high-quality data. Existing datasets and
benchmarks often lack the dynamic complexity, multi-domain diversity, and
spatial-temporal annotations required to support key tasks such as 4D geometric
reconstruction, future prediction, and camera-control video generation. To
address this gap, we introduce OmniWorld, a large-scale, multi-domain,
multi-modal dataset specifically designed for 4D world modeling. OmniWorld
consists of a newly collected OmniWorld-Game dataset and several curated public
datasets spanning diverse domains. Compared with existing synthetic datasets,
OmniWorld-Game provides richer modality coverage, larger scale, and more
realistic dynamic interactions. Based on this dataset, we establish a
challenging benchmark that exposes the limitations of current state-of-the-art
(SOTA) approaches in modeling complex 4D environments. Moreover, fine-tuning
existing SOTA methods on OmniWorld leads to significant performance gains
across 4D reconstruction and video generation tasks, strongly validating
OmniWorld as a powerful resource for training and evaluation. We envision
OmniWorld as a catalyst for accelerating the development of general-purpose 4D
world models, ultimately advancing machines' holistic understanding of the
physical world.

</details>


### [143] [LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence](https://arxiv.org/abs/2509.12203)
*Zixin Yin,Xili Dai,Duomin Wang,Xianfang Zeng,Lionel M. Ni,Gang Yu,Heung-Yeung Shum*

Main category: cs.CV

TL;DR: 本文提出了LazyDrag方法，通过显式对应关系，提升了拖拽编辑在扩散模型中的准确性与效率，解决了以往依赖隐式关注带来的局限，实现了更高质量、更复杂的图片编辑。


<details>
  <summary>Details</summary>
Motivation: 现有拖拽编辑方法依赖于注意力机制下的隐式点匹配，导致图像反演能力受损和推理过程耗时，限制了扩散模型在高保真修复和文本引导生成中的表现。

Method: LazyDrag首创性地在多模态扩散Transformer中引入显式的用户拖拽对应关系图，替代隐式点匹配，作为提升注意力控制的可靠参考，支持无TTO的稳定反演和复杂几何及文本引导编辑。

Result: 在DragBench基准上，LazyDrag无论在拖拽准确率还是感知质量上都超过了现有方法，并获得了VIEScore和人工评测的认可。

Conclusion: LazyDrag不仅刷新了拖拽编辑领域的表现，还为AI图像编辑带来了全新范式，支持更多样复杂的编辑需求和高效多轮操作。

Abstract: The reliance on implicit point matching via attention has become a core
bottleneck in drag-based editing, resulting in a fundamental compromise on
weakened inversion strength and costly test-time optimization (TTO). This
compromise severely limits the generative capabilities of diffusion models,
suppressing high-fidelity inpainting and text-guided creation. In this paper,
we introduce LazyDrag, the first drag-based image editing method for
Multi-Modal Diffusion Transformers, which directly eliminates the reliance on
implicit point matching. In concrete terms, our method generates an explicit
correspondence map from user drag inputs as a reliable reference to boost the
attention control. This reliable reference opens the potential for a stable
full-strength inversion process, which is the first in the drag-based editing
task. It obviates the necessity for TTO and unlocks the generative capability
of models. Therefore, LazyDrag naturally unifies precise geometric control with
text guidance, enabling complex edits that were previously out of reach:
opening the mouth of a dog and inpainting its interior, generating new objects
like a ``tennis ball'', or for ambiguous drags, making context-aware changes
like moving a hand into a pocket. Additionally, LazyDrag supports multi-round
workflows with simultaneous move and scale operations. Evaluated on the
DragBench, our method outperforms baselines in drag accuracy and perceptual
quality, as validated by VIEScore and human evaluation. LazyDrag not only
establishes new state-of-the-art performance, but also paves a new way to
editing paradigms.

</details>


### [144] [Character-Centric Understanding of Animated Movies](https://arxiv.org/abs/2509.12204)
*Zhongrui Gui,Junyu Xie,Tengda Han,Weidi Xie,Andrew Zisserman*

Main category: cs.CV

TL;DR: 本文提出了一种基于音频和视觉信息的管道，实现了对动画电影角色的自动和鲁棒识别，并应用于辅助视觉和听觉障碍者的下游任务。


<details>
  <summary>Details</summary>
Motivation: 传统的人脸识别对于动画角色无效，因为动画人物在外观、动作和形变上极具多样性，现有方法难以适用于动画电影中的角色识别。因此，需要一种更强大的自动化识别解决方案。

Method: 提出了一种音频-视觉融合的角色识别管道：1）自动从线上资源构建带有视觉样本和音频样本的角色库；2）利用多模态信息进行鲁棒的角色识别，解决动画角色外观分布长尾的问题；3）在此基础上开发了音频描述自动生成和角色感知字幕两个无障碍应用。此外，提出了新的动画电影数据集CMD-AM，并为相关研究提供代码与数据集。

Result: 实验显示，该音视频识别系统在动画角色识别、音频描述生成和字幕制作方面均优于基于传统人脸检测的以往方法，大幅提升了动画内容的可访问性与叙事理解。

Conclusion: 音视融合的角色识别方法有效克服了动画角色多样性的挑战，在提升动画电影的可访问性与内容理解方面具有显著成效，同时为学术界提供了新的数据集与工具。

Abstract: Animated movies are captivating for their unique character designs and
imaginative storytelling, yet they pose significant challenges for existing
recognition systems. Unlike the consistent visual patterns detected by
conventional face recognition methods, animated characters exhibit extreme
diversity in their appearance, motion, and deformation. In this work, we
propose an audio-visual pipeline to enable automatic and robust animated
character recognition, and thereby enhance character-centric understanding of
animated movies. Central to our approach is the automatic construction of an
audio-visual character bank from online sources. This bank contains both visual
exemplars and voice (audio) samples for each character, enabling subsequent
multi-modal character recognition despite long-tailed appearance distributions.
Building on accurate character recognition, we explore two downstream
applications: Audio Description (AD) generation for visually impaired
audiences, and character-aware subtitling for the hearing impaired. To support
research in this domain, we introduce CMD-AM, a new dataset of 75 animated
movies with comprehensive annotations. Our character-centric pipeline
demonstrates significant improvements in both accessibility and narrative
comprehension for animated content over prior face-detection-based approaches.
For the code and dataset, visit
https://www.robots.ox.ac.uk/~vgg/research/animated_ad/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [145] [Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment](https://arxiv.org/abs/2509.10546)
*Gang Cheng,Haibo Jin,Wenbin Zhang,Haohan Wang,Jun Zhuang*

Main category: cs.CL

TL;DR: 本文提出了一种名为风险隐藏攻击（RCA）的红队攻击框架，能系统性地欺骗金融领域大语言模型（LLM），并展示了当前主流LLM对该攻击几乎无防御，揭示了金融合规监管风险隐患。


<details>
  <summary>Details</summary>
Motivation: 目前已有红队研究主要聚焦于有害内容，但忽略了实际金融应用中的合规与监管风险，这对金融领域的LLM安全极为重要。因此，作者希望填补这一研究空白，针对金融领域的监管合规风险，评估并揭示现有LLM在该方面的脆弱性。

Method: 作者提出了一种新型多轮对话红队攻击方法——风险隐藏攻击（RCA），通过逐步隐藏监管风险，诱导LLM生成表面合规但实则违规的回答。为对该攻击效果做系统性评估，作者还构建了金融特定安全基准数据集（FIN-Bench），在九个主流LLM上进行实验。

Result: 实验表明，RCA攻击能有效躲避当前主流LLM的防御机制，在所有九个LLM上平均攻击成功率高达93.18%，其中包括对GPT-4.1高达98.28%和OpenAI o1高达97.56%的成功率。

Conclusion: 实验结果揭示，当前LLM对金融合规风险缺乏有效防范手段，现有自动对齐和风控机制存在严重安全隐患。研究呼吁在金融领域亟需更强健的模型对齐和内容审核机制，并为未来金融领域LLM的安全对齐研究提供了实用洞见。

Abstract: Large Language Models (LLMs) are increasingly integrated into financial
applications, yet existing red-teaming research primarily targets harmful
content, largely neglecting regulatory risks. In this work, we aim to
investigate the vulnerability of financial LLMs through red-teaming approaches.
We introduce Risk-Concealment Attacks (RCA), a novel multi-turn framework that
iteratively conceals regulatory risks to provoke seemingly compliant yet
regulatory-violating responses from LLMs. To enable systematic evaluation, we
construct FIN-Bench, a domain-specific benchmark for assessing LLM safety in
financial contexts. Extensive experiments on FIN-Bench demonstrate that RCA
effectively bypasses nine mainstream LLMs, achieving an average attack success
rate (ASR) of 93.18%, including 98.28% on GPT-4.1 and 97.56% on OpenAI o1.
These findings reveal a critical gap in current alignment techniques and
underscore the urgent need for stronger moderation mechanisms in financial
domains. We hope this work offers practical insights for advancing robust and
domain-aware LLM alignment.

</details>


### [146] [No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes](https://arxiv.org/abs/2509.10625)
*Iván Vicente Moreno Cencerrado,Arnau Padrés Masdemont,Anton Gonzalvez Hawthorne,David Demitri Africa,Lorenzo Pacchiardi*

Main category: cs.CL

TL;DR: 本研究通过分析大模型在生成答案前的激活，用线性探针预测模型能否正确回答问题，并发现这种方法优于传统置信度估计，揭示了模型自我评估能力的出现过程。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型能否认识到自身答案的正确性知之甚少。研究者希望探索在回答问题前，模型内部信息是否已经‘知道’它是否会答对，从而深化对模型内部机制的理解。

Method: 在模型读完整个问题但未开始生成答案时，提取其中间激活状态，用线性探针训练识别模型后续是否会答对问题，在不同模型规模与多种数据集上进行实验，对比主流置信度估计方法。

Result: 用线性探针训练出的“提前正确性方向”在主任务及多种分布外知识数据集上均能准确预测即将到来的回答正确性，并优于黑盒基线与显式置信度口头表述。预测能力在模型中间层达到峰值，推测自我评估能力在此阶段显现；但在数学推理等问题上泛化能力较弱。此外，模型回答“我不知道”与探针分数高度相关，说明同一方向也反映置信度。

Conclusion: 本文工作揭示了LLM在内部激活中提前出现的正确性信号及自我评估能力，并用探针方法验证了其有效性，同时指出其在数学推理领域的局限性，加强了对LLM内部机制的理解。

Abstract: Do large language models (LLMs) anticipate when they will answer correctly?
To study this, we extract activations after a question is read but before any
tokens are generated, and train linear probes to predict whether the model's
forthcoming answer will be correct. Across three open-source model families
ranging from 7 to 70 billion parameters, projections on this "in-advance
correctness direction" trained on generic trivia questions predict success in
distribution and on diverse out-of-distribution knowledge datasets,
outperforming black-box baselines and verbalised predicted confidence.
Predictive power saturates in intermediate layers, suggesting that
self-assessment emerges mid-computation. Notably, generalisation falters on
questions requiring mathematical reasoning. Moreover, for models responding "I
don't know", doing so strongly correlates with the probe score, indicating that
the same direction also captures confidence. By complementing previous results
on truthfulness and other behaviours obtained with probes and sparse
auto-encoders, our work contributes essential findings to elucidate LLM
internals.

</details>


### [147] [Interdisciplinary Research in Conversation: A Case Study in Computational Morphology for Language Documentation](https://arxiv.org/abs/2509.10644)
*Enora Rice,Katharina von der Wense,Alexis Palmer*

Main category: cs.CL

TL;DR: 本文探讨了计算形态学在语言文献编制中的应用面临的实际局限，强调需以用户为中心的设计(UCD)以实现更有效的技术转化。


<details>
  <summary>Details</summary>
Motivation: 尽管计算形态学方法（如形态切分和IGT生成）在理论上可为语言文献编制提供支持，但实际上这些NLP研究成果在真实编制场景中的应用却非常有限，存在研究与实际脱节问题。

Method: 作者以主张性论文立场指出当前领域存在的问题，并通过案例分析（以多语种IGT生成模型GlossLM为例），开展了对三位语言文献学家的用户调研，评估模型真实环境下的可用性。

Result: 用户调研显示，虽然GlossLM在标准指标上表现优秀，但难以满足核心的实用需求，凸显了模型在约束、标签标准化、切分和个性化等方面的新问题。

Conclusion: 作者认为，未来模型研发应以用户为中心，这不仅能提升工具实用性，还能激发更有意义的研究方向，从而避免NLP技术脱离实际应用。

Abstract: Computational morphology has the potential to support language documentation
through tasks like morphological segmentation and the generation of Interlinear
Glossed Text (IGT). However, our research outputs have seen limited use in
real-world language documentation settings. This position paper situates the
disconnect between computational morphology and language documentation within a
broader misalignment between research and practice in NLP and argues that the
field risks becoming decontextualized and ineffectual without systematic
integration of User-Centered Design (UCD). To demonstrate how principles from
UCD can reshape the research agenda, we present a case study of GlossLM, a
state-of-the-art multilingual IGT generation model. Through a small-scale user
study with three documentary linguists, we find that despite strong metric
based performance, the system fails to meet core usability needs in real
documentation contexts. These insights raise new research questions around
model constraints, label standardization, segmentation, and personalization. We
argue that centering users not only produces more effective tools, but surfaces
richer, more relevant research directions

</details>


### [148] [Context Copying Modulation: The Role of Entropy Neurons in Managing Parametric and Contextual Knowledge Conflicts](https://arxiv.org/abs/2509.10663)
*Zineddine Tighidet,Andrea Mogini,Hedi Ben-younes,Jiali Mei,Patrick Gallinari,Benjamin Piwowarski*

Main category: cs.CL

TL;DR: 本文揭示了大型语言模型（LLM）面对上下文信息与其内部知识冲突时，内部神经元（熵神经元）在抑制简单照搬上下文信息中的关键作用，并通过对这些神经元的研究加深了对模型内部冲突处理机制的理解。


<details>
  <summary>Details</summary>
Motivation: LLM在上下文与模型内在知识冲突时生成的输出行为不一致，缺乏统一解释。近期发现部分神经元对输出熵有显著影响，但对预估结果排名影响较小，研究这些神经元能帮助理解模型冲突处理机制。

Method: 通过识别和消融LLM中的熵神经元，考察这些神经元在模型处理上下文与内在知识冲突中的作用，分析消融后模型生成行为变化。

Result: 实验证明，熵神经元确实负责在多种LLM中抑制上下文直接复制行为，当这些神经元被抑制（消融）时，模型生成过程发生明显变化。

Conclusion: 熵神经元在处理信息冲突时能够抑制模型直接复制上下文，加深了对LLM内部动态及冲突处理机制的理解。

Abstract: The behavior of Large Language Models (LLMs) when facing contextual
information that conflicts with their internal parametric knowledge is
inconsistent, with no generally accepted explanation for the expected outcome
distribution. Recent work has identified in autoregressive transformer models a
class of neurons -- called entropy neurons -- that produce a significant effect
on the model output entropy while having an overall moderate impact on the
ranking of the predicted tokens. In this paper, we investigate the preliminary
claim that these neurons are involved in inhibiting context copying behavior in
transformers by looking at their role in resolving conflicts between contextual
and parametric information. We show that entropy neurons are responsible for
suppressing context copying across a range of LLMs, and that ablating them
leads to a significant change in the generation process. These results enhance
our understanding of the internal dynamics of LLMs when handling conflicting
information.

</details>


### [149] [Pluralistic Alignment for Healthcare: A Role-Driven Framework](https://arxiv.org/abs/2509.10685)
*Jiayou Zhong,Anudeex Shetty,Chao Jia,Xuanrui Lin,Usman Naseem*

Main category: cs.CL

TL;DR: 本文提出了一种名为EthosAgents的新颖轻量、可泛化的多元化对齐方法，旨在提升大语言模型在医疗等敏感领域对多元价值观的响应能力，并验证了其在多种模型上的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在医疗等敏感领域落地，对其输出能否反映社会多元价值观提出更高要求。现有对齐方法在医疗领域因受个人、文化和情境等因素影响，无法充分涵盖需要，因此亟需更灵活有效的对齐新范式。

Method: 作者提出EthosAgents方法，能够模拟不同的视角和价值观，灵活地完成多元对齐。该方法针对现有对齐技术的局限性，设计为轻量级且易泛化，并在七种不同规模的开源和闭源模型上进行了实证评估。

Result: 实验证明，EthosAgents在三种对齐模式下，对所有测试模型都实现了多元化对齐能力的提升，尤其在处理健康相关多元性方面表现出色。

Conclusion: 研究表明，医疗领域的多元价值需求需借助具备适应性与规范感知能力的新型对齐方法，EthosAgents为高敏感领域的多元对齐提供了有效思路，有助于大模型更好地尊重并反映多元价值观。

Abstract: As large language models are increasingly deployed in sensitive domains such
as healthcare, ensuring their outputs reflect the diverse values and
perspectives held across populations is critical. However, existing alignment
approaches, including pluralistic paradigms like Modular Pluralism, often fall
short in the health domain, where personal, cultural, and situational factors
shape pluralism. Motivated by the aforementioned healthcare challenges, we
propose a first lightweight, generalizable, pluralistic alignment approach,
EthosAgents, designed to simulate diverse perspectives and values. We
empirically show that it advances the pluralistic alignment for all three modes
across seven varying-sized open and closed models. Our findings reveal that
health-related pluralism demands adaptable and normatively aware approaches,
offering insights into how these models can better respect diversity in other
high-stakes domains.

</details>


### [150] [Struct-Bench: A Benchmark for Differentially Private Structured Text Generation](https://arxiv.org/abs/2509.10696)
*Shuaiqi Wang,Vikas Raunak,Arturs Backurs,Victor Reis,Pei Zhou,Sihao Chen,Longqi Yang,Zinan Lin,Sergey Yekhanin,Giulia Fanti*

Main category: cs.CL

TL;DR: 该论文提出了Struct-Bench框架与基准，用于评估包含自然语言数据的结构化合成数据的质量，并公开了相应的数据集和评测平台。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私合成数据评估方法难以捕捉结构化数据中的结构特性和关联性，尤其是在企业常见的结构化（如表格）数据场景中。因此，亟需专门应对结构化、含自然语言字段的数据评测方法。

Method: 设计了Struct-Bench，一套要求用户用上下文无关文法（CFG）表达数据结构的评估框架和基准，包括5个真实和2个人工合成数据集，都标注了对应CFG；提供了评价指标参考实现和排行榜，用以规范和促进差分隐私合成数据研究。

Result: 实验表明，这些数据集对现有领先的差分隐私合成数据生成方法构成挑战，框架有效揭示了方法在结构化数据场景下的不足。通过实证案例，说明Struct-Bench能帮助提升合成数据质量。

Conclusion: Struct-Bench为隐私保护合成数据领域，尤其是结构化含自然语言的数据评估，提供了标准化平台，对后续方法研究与改进具有重要促进作用。

Abstract: Differentially private (DP) synthetic data generation is a promising
technique for utilizing private datasets that otherwise cannot be exposed for
model training or other analytics. While much research literature has focused
on generating private unstructured text and image data, in enterprise settings,
structured data (e.g., tabular) is more common, often including natural
language fields or components. Existing synthetic data evaluation techniques
(e.g., FID) struggle to capture the structural properties and correlations of
such datasets. In this work, we propose Struct-Bench, a framework and benchmark
for evaluating synthetic datasets derived from structured datasets that contain
natural language data. The Struct-Bench framework requires users to provide a
representation of their dataset structure as a Context-Free Grammar (CFG). Our
benchmark comprises 5 real-world and 2 synthetically generated datasets, each
annotated with CFGs. We show that these datasets demonstrably present a great
challenge even for state-of-the-art DP synthetic data generation methods.
Struct-Bench also includes reference implementations of different metrics and a
leaderboard, thereby providing researchers a standardized evaluation platform
to benchmark and investigate privacy-preserving synthetic data generation
methods. Further, we also present a case study showing how to use Struct-Bench
to improve the synthetic data quality of Private Evolution (PE) on structured
data. The benchmark and the leaderboard have been publicly made available at
https://struct-bench.github.io.

</details>


### [151] [A Survey on Retrieval And Structuring Augmented Generation with Large Language Models](https://arxiv.org/abs/2509.10697)
*Pengcheng Jiang,Siru Ouyang,Yizhu Jiao,Ming Zhong,Runchu Tian,Jiawei Han*

Main category: cs.CL

TL;DR: 本文综述了通过检索与结构化（RAS）增强大语言模型（LLM）生成能力的方法，涵盖检索机制、文本结构化技术及其与LLM的集成等方面，同时指出了当前技术挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在实际应用中常见幻觉现象、知识过时及专业领域能力有限等问题，急需通过引入外部知识和结构化信息提升其可靠性和适用性。

Method: 系统梳理了稀疏、稠密与混合知识检索方式，比较了构建分类体系、分层分类与信息抽取等文本结构化手段，并探讨了基于提示、推理框架及知识嵌入等结构化信息与LLM结合的具体实现方案。

Result: 总结目前RAS方法在知识获取、结构化组织与生成集成方面的进展，指出检索效率、结构质量与知识融合等仍是主要技术瓶颈。

Conclusion: RAS增强方法能有效缓解LLM知识与推理的限制，但需持续提升结构化与集成能力。未来研究可关注多模态检索、跨语言结构和人机交互式系统等方向，为模型能力拓展提供新思路。

Abstract: Large Language Models (LLMs) have revolutionized natural language processing
with their remarkable capabilities in text generation and reasoning. However,
these models face critical challenges when deployed in real-world applications,
including hallucination generation, outdated knowledge, and limited domain
expertise. Retrieval And Structuring (RAS) Augmented Generation addresses these
limitations by integrating dynamic information retrieval with structured
knowledge representations. This survey (1) examines retrieval mechanisms
including sparse, dense, and hybrid approaches for accessing external
knowledge; (2) explore text structuring techniques such as taxonomy
construction, hierarchical classification, and information extraction that
transform unstructured text into organized representations; and (3) investigate
how these structured representations integrate with LLMs through prompt-based
methods, reasoning frameworks, and knowledge embedding techniques. It also
identifies technical challenges in retrieval efficiency, structure quality, and
knowledge integration, while highlighting research opportunities in multimodal
retrieval, cross-lingual structures, and interactive systems. This
comprehensive overview provides researchers and practitioners with insights
into RAS methods, applications, and future directions.

</details>


### [152] [SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation](https://arxiv.org/abs/2509.10708)
*Iman Barati,Mostafa Amiri,Heshaam Faili*

Main category: cs.CL

TL;DR: 本文提出了一种名为SearchInstruct的方法，用于为监督微调（SFT）自动构建高质量的领域指令数据集。通过LLM扩展少量人工问题并检索相关资源生成答案，显著提升了数据集的多样性与质量，从而提高LLM在特定领域的表现。


<details>
  <summary>Details</summary>
Motivation: 在训练大语言模型时，SFT需要高质量且适合特定领域的数据集，但由于领域限制和数据稀缺，构建此类数据集非常困难。作者希望解决高质量领域SFT数据集自动生成难题。

Method: 方法从有限的人工编写领域问题出发，利用大语言模型扩展问题集合，之后检索领域相关资源，为扩展后的问题生成准确且符合语境的答案，形成高质量的指令-响应对数据集。

Result: 实验表明，SearchInstruct显著提升了数据集的多样性和质量，从而带来LLM在专业领域的性能提升。该方法还可用于模型编辑，实现高效的模型更新。

Conclusion: SearchInstruct为构建领域SFT数据集提供了一种高效新途径，提升了LLM在专业领域的能力，并促进了模型的便捷更新。相关资源和代码已开源，便于社区复现和应用。

Abstract: Supervised Fine-Tuning (SFT) is essential for training large language models
(LLMs), significantly enhancing critical capabilities such as instruction
following and in-context learning. Nevertheless, creating suitable training
datasets tailored for specific domains remains challenging due to unique domain
constraints and data scarcity. In this paper, we propose SearchInstruct, an
innovative method explicitly designed to construct high quality instruction
datasets for SFT. Our approach begins with a limited set of domain specific,
human generated questions, which are systematically expanded using a large
language model. Subsequently, domain relevant resources are dynamically
retrieved to generate accurate and contextually appropriate answers for each
augmented question. Experimental evaluation demonstrates that SearchInstruct
enhances both the diversity and quality of SFT datasets, leading to measurable
improvements in LLM performance within specialized domains. Additionally, we
show that beyond dataset generation, the proposed method can also effectively
facilitate tasks such as model editing, enabling efficient updates to existing
models. To facilitate reproducibility and community adoption, we provide full
implementation details, the complete set of generated instruction response
pairs, and the source code in a publicly accessible Git repository:
[https://github.com/mostafaamiri/SearchInstruct](https://github.com/mostafaamiri/SearchInstruct)

</details>


### [153] [PolyTruth: Multilingual Disinformation Detection using Transformer-Based Language Models](https://arxiv.org/abs/2509.10737)
*Zaur Gouliev,Jennifer Waters,Chengqian Wang*

Main category: cs.CL

TL;DR: 本文系统性比较了五种多语言Transformer模型（mBERT、XLM、XLM-RoBERTa、RemBERT、mT5）在识别多语言虚假信息任务中的表现，并发布了包含25种语言、超过6万条语句对的新数据集，以评估这些模型在跨语言环境下的有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然虚假信息在多语言环境中迅速传播，但绝大多数AI模型仅在英文数据上进行评测和优化，缺乏对多语言中AI模型表现的系统性对比和优质基准数据集。

Method: 作者构建了一个规模较大的多语言虚假信息与真实更正陈述对数据集PolyTruth Disinfo Corpus（覆盖五大语系、25种语言、多个主题领域），并采用五种主流多语言Transformer模型在共同的二分类任务上进行实验比较，分析各模型尤其在资源稀缺语言下的表现差异。

Result: RemBERT总体表现最佳，尤其在低资源语言下有明显优势；而mBERT、XLM等模型在训练数据有限时表现受限。此外，不同模型在多语言、跨主题范围内的表现也存在差异。

Conclusion: AI多语言虚假信息检测已取得进展，但还存在局限，尤其在资源稀缺语言和多主题情况下。所发布的新数据集为该领域模型开发和评估提供了基础；相关实验和讨论为实际部署和未来研究指明方向。

Abstract: Disinformation spreads rapidly across linguistic boundaries, yet most AI
models are still benchmarked only on English. We address this gap with a
systematic comparison of five multilingual transformer models: mBERT, XLM,
XLM-RoBERTa, RemBERT, and mT5 on a common fake-vs-true machine learning
classification task. While transformer-based language models have demonstrated
notable success in detecting disinformation in English, their effectiveness in
multilingual contexts still remains up for debate. To facilitate evaluation, we
introduce PolyTruth Disinfo Corpus, a novel corpus of 60,486 statement pairs
(false claim vs. factual correction) spanning over twenty five languages that
collectively cover five language families and a broad topical range from
politics, health, climate, finance, and conspiracy, half of which are
fact-checked disinformation claims verified by an augmented MindBugs Discovery
dataset. Our experiments revealed performance variations. Models such as
RemBERT achieved better overall accuracy, particularly excelling in
low-resource languages, whereas models like mBERT and XLM exhibit considerable
limitations when training data is scarce. We provide a discussion of these
performance patterns and implications for real-world deployment. The dataset is
publicly available on our GitHub repository to encourage further
experimentation and advancement. Our findings illuminate both the potential and
the current limitations of AI systems for multilingual disinformation
detection.

</details>


### [154] [Reasoning Under Uncertainty: Exploring Probabilistic Reasoning Capabilities of LLMs](https://arxiv.org/abs/2509.10739)
*Mobina Pournemat,Keivan Rezaei,Gaurang Sriramanan,Arman Zarei,Jiaxiang Fu,Yang Wang,Hamid Eghbalzadeh,Soheil Feizi*

Main category: cs.CL

TL;DR: 本文系统性评估了大语言模型（LLMs）在显式离散概率分布上的推理能力，揭示了它们在概率推理任务中的表现差异和局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在语言理解与生成任务中表现出色，但其在需要概率推理的任务中行为尚不明确且常常不一致，因此需要系统分析它们的概率推理能力。

Method: 作者基于离散概率分布设计了三类任务：模式识别、最大似然估计、样本生成。通过对概率分布及其条件分布的提问，考查模型在频率分析、边缘化和生成等概率技能上的表现，且比较了不同规模模型的性能。

Result: 实验证明，大模型在推断和生成样本方面具有更强能力，但在表示法变化和上下文长度增大时表现大幅下降（性能降低超60%）。此外，模型在概率推理上的表现随规模明显提升，但依然存在关键局限。

Conclusion: 本文揭示了LLMs在概率推理任务上的能力和局限，为后续提升其概率推理能力指明了方向。

Abstract: Despite widespread success in language understanding and generation, large
language models (LLMs) exhibit unclear and often inconsistent behavior when
faced with tasks that require probabilistic reasoning. In this work, we present
the first comprehensive study of the reasoning capabilities of LLMs over
explicit discrete probability distributions. Given observations from a
probability distribution, we evaluate models on three carefully designed tasks,
mode identification, maximum likelihood estimation, and sample generation, by
prompting them to provide responses to queries about either the joint
distribution or its conditionals. These tasks thus probe a range of
probabilistic skills, including frequency analysis, marginalization, and
generative behavior. Through comprehensive empirical evaluations, we
demonstrate that there exists a clear performance gap between smaller and
larger models, with the latter demonstrating stronger inference and surprising
capabilities in sample generation. Furthermore, our investigations reveal
notable limitations, including sensitivity to variations in the notation
utilized to represent probabilistic outcomes and performance degradation of
over 60% as context length increases. Together, our results provide a detailed
understanding of the probabilistic reasoning abilities of LLMs and identify key
directions for future improvement.

</details>


### [155] [Automated MCQA Benchmarking at Scale: Evaluating Reasoning Traces as Retrieval Sources for Domain Adaptation of Small Language Models](https://arxiv.org/abs/2509.10744)
*Ozan Gokdemir,Neil Getty,Robert Underwood,Sandeep Madireddy,Franck Cappello,Arvind Ramanathan,Ian T. Foster,Rick L. Stevens*

Main category: cs.CL

TL;DR: 本文提出了一种自动从大规模科学文献中生成多项选择题（MCQA）基准测试的新框架，并展示了在肿瘤和放射领域的应用效果。


<details>
  <summary>Details</summary>
Motivation: 随着科学知识的快速增长，现有的自然语言处理评测基准已难以反映最新、最广泛的科研进展，需要一种可扩展且自动化的方法生成新的、反映现状的基准，为语言模型的能力评测提供依据。

Method: 作者设计了一个可扩展、模块化的流水线，实现了PDF解析、语义切分、自动生成问题及答案、并对模型表现进行评估。具体以22,000篇开放获取的放射与癌症生物学论文为例，自动生成16,000多道多项选择题，并对不同参数规模的小型语言模型与检索增强生成（RAG）范式进行了基准测试。

Result: 实验结果显示，基于推理轨迹（reasoning-trace）的检索能够稳定提升模型在合成和专家标注数据集上的答题准确率，部分小型语言模型甚至在2023年天体放射和癌症生物学考试中超越了GPT-4。

Conclusion: 所提出的自动化MCQA生成框架能够及时反映科研进展，提升了多种模型的评测效果，特别是引入推理轨迹检索后，小型语言模型表现大幅提升，表明新框架能够支持更公平、准确的模型能力评估。

Abstract: As scientific knowledge grows at an unprecedented pace, evaluation benchmarks
must evolve to reflect new discoveries and ensure language models are tested on
current, diverse literature. We propose a scalable, modular framework for
generating multiple-choice question-answering (MCQA) benchmarks directly from
large corpora of scientific papers. Our pipeline automates every stage of MCQA
creation, including PDF parsing, semantic chunking, question generation, and
model evaluation. As a case study, we generate more than 16,000 MCQs from
22,000 open-access articles in radiation and cancer biology. We then evaluate a
suite of small language models (1.1B-14B parameters) on these questions,
comparing baseline accuracy with retrieval-augmented generation (RAG) from
paper-derived semantic chunks and from reasoning traces distilled from GPT-4.1.
We find that reasoning-trace retrieval consistently improves performance on
both synthetic and expert-annotated benchmarks, enabling several small models
to surpass GPT-4 on the 2023 Astro Radiation and Cancer Biology exam.

</details>


### [156] [RECAP: Transparent Inference-Time Emotion Alignment for Medical Dialogue Systems](https://arxiv.org/abs/2509.10746)
*Adarsh Srinivasan,Jacob Dineen,Muhammad Umar Afzal,Muhammad Uzair Sarfraz,Irbaz B. Riaz,Ben Zhou*

Main category: cs.CL

TL;DR: 本文提出了RECAP框架，通过推理阶段结构化情感推理显著提升了医疗AI的同理心表现，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在医疗领域虽然能给出医学上合理的建议，但在情感理解和共情表达方面存在明显不足，尤其在患者情绪低落、脆弱时，缺乏同理心的回答会影响病人安全、依从性和信任感。因此亟需提升医疗AI情感交流能力。

Method: 作者提出了RECAP（Reflect-Extract-Calibrate-Align-Produce）推理框架，将同理心拆分为多个可审查的理论阶段，通过Likert量表信号结构化情感维度，增强情感推理和表达，无需重新训练原有模型。

Result: 在EmoBench、SECEU、EQ-Bench等多个评测上，RECAP使8B参数模型的情感推理能力提升22-28%，大模型提升10-13%；临床医生评估也证实，RECAP产出的回答在表达共情方面表现显著更优。

Conclusion: RECAP表明，采用理论驱动、模块化的提示方法可以系统性提升医疗人工智能的情感智能，同时保证了结果的审计性和可部署性。

Abstract: Large language models in healthcare often miss critical emotional cues,
delivering medically sound but emotionally flat advice. This is especially
problematic in clinical contexts where patients are distressed and vulnerable,
and require empathic communication to support safety, adherence, and trust. We
present RECAP (Reflect-Extract-Calibrate-Align-Produce), an inference-time
framework that adds structured emotional reasoning without retraining. By
decomposing empathy into transparent appraisal-theoretic stages and exposing
per-dimension Likert signals, RECAP produces nuanced, auditable responses.
Across EmoBench, SECEU, and EQ-Bench, RECAP improves emotional reasoning by
22-28% on 8B models and 10-13% on larger models over zero-shot baselines.
Clinician evaluations further confirm superior empathetic communication. RECAP
shows that modular, theory-grounded prompting can systematically enhance
emotional intelligence in medical AI while preserving the accountability
required for deployment.

</details>


### [157] [Judge Q: Trainable Queries for Optimized Information Retention in KV Cache Eviction](https://arxiv.org/abs/2509.10798)
*Yijun Liu,Yixuan Wang,Yuzhuang Xu,Shiyu Ji,Yang Xu,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文提出了一种新的大模型KV缓存淘汰训练方法——Judge Q，用于提升缓存管理效率，减缓性能退化。


<details>
  <summary>Details</summary>
Motivation: KV缓存随着序列长度线性增长，导致内存占用和解码效率问题。目前的淘汰方法过度依赖本地信息，易忽略全局重要内容，影响模型性能。

Method: 引入“软token列表”，仅微调embedding层，在输入序列末尾拼接软token，通过训练使其注意力分布与真实解码token一致，捕获全局信息，更合理评估KV重要性。

Result: 在Llama-3.1-8B-Instruct和Mistral-7B-Instruct-v0.3等模型及LongBench、RULER等基准上，LongBench提高约1分，RULER提高3分以上。

Conclusion: 该方法几乎无训练开销，可无缝集成到开源大模型中，KV缓存淘汰时性能下降更小，提升了大模型解码效率和效果。

Abstract: Large language models (LLMs) utilize key-value (KV) cache to store historical
information during sequence processing. The size of KV cache grows linearly as
the length of the sequence extends, which seriously affects memory usage and
decoding efficiency. Current methods for KV cache eviction typically utilize
the last window from the pre-filling phase as queries to compute the KV
importance scores for eviction. Although this scheme is simple to implement, it
tends to overly focus on local information, potentially leading to the neglect
or omission of crucial global information. To mitigate this issue, we propose
Judge Q, a novel training method which incorporates a soft token list. This
method only tunes the model's embedding layer at a low training cost. By
concatenating the soft token list at the end of the input sequence, we train
these tokens' attention map to the original input sequence to align with that
of the actual decoded tokens. In this way, the queries corresponding to the
soft tokens can effectively capture global information and better evaluate the
importance of the keys and values within the KV cache, thus maintaining
decoding quality when KV cache is evicted. Under the same eviction budget, our
method exhibits less performance degradation compared to existing eviction
approaches. We validate our approach through experiments conducted on models
such as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks
including LongBench, RULER, and Needle-in-a-Haystack. Results indicate an
improvement of approximately 1 point on the LongBench and over 3 points on
RULER. This proposed methodology can be seamlessly integrated into existing
open-source models with minimal training overhead, thereby enhancing
performance in KV cache eviction scenarios.

</details>


### [158] [Towards Automated Error Discovery: A Study in Conversational AI](https://arxiv.org/abs/2509.10833)
*Dominic Petrak,Thy Thy Tran,Iryna Gurevych*

Main category: cs.CL

TL;DR: 论文提出了一种新方法SEEED，用于在对话AI中自动发现和定义错误，并在多个数据集上超越了主流大模型基线。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型驱动的对话系统表现出良好的流畅性和连贯性，但在实际部署中仍会出现难以预防的错误。现有检测方法对于未明确指令下的新型或未知错误识别能力有限，尤其难以适应模型变化或用户行为转变带来的错误。需要更自动化、泛化能力更强的错误检测方法。

Method: 作者提出Automated Error Discovery框架，实现自动检测和定义对话AI中的错误。方法核心为SEEED（Soft Clustering Extended Encoder-Based Error Detection），基于编码器结构，通过加权负样本距离增强Soft Nearest Neighbor Loss，并引入基于标签的样本排序以选择表征性强的对立样本，从而提升错误识别泛化能力。

Result: SEEED在多个标注有错误的对话数据集上与现有方法（如GPT-4o和Phi-4等）进行了对比，发现其检测未知错误的准确率提升高达8个百分点，并在未知意图检测任务上也表现出较强的泛化能力。

Conclusion: SEEED方法显著提升了对未知错误的检测能力，提高了对话系统的安全性和健壮性，为自动发现多变环境下的对话错误提供了有效手段。

Abstract: Although LLM-based conversational agents demonstrate strong fluency and
coherence, they still produce undesirable behaviors (errors) that are
challenging to prevent from reaching users during deployment. Recent research
leverages large language models (LLMs) to detect errors and guide
response-generation models toward improvement. However, current LLMs struggle
to identify errors not explicitly specified in their instructions, such as
those arising from updates to the response-generation model or shifts in user
behavior. In this work, we introduce Automated Error Discovery, a framework for
detecting and defining errors in conversational AI, and propose SEEED (Soft
Clustering Extended Encoder-Based Error Detection), as an encoder-based
approach to its implementation. We enhance the Soft Nearest Neighbor Loss by
amplifying distance weighting for negative samples and introduce Label-Based
Sample Ranking to select highly contrastive examples for better representation
learning. SEEED outperforms adapted baselines -- including GPT-4o and Phi-4 --
across multiple error-annotated dialogue datasets, improving the accuracy for
detecting unknown errors by up to 8 points and demonstrating strong
generalization to unknown intent detection.

</details>


### [159] [Evaluating Large Language Models for Evidence-Based Clinical Question Answering](https://arxiv.org/abs/2509.10843)
*Can Wang,Yiqun Chen*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型（LLM）在医学和临床问题回答上的能力，发现其在结构化指南推荐上的准确率较高，但在叙述性指南及系统综述问题上较低，引入检索增强后准确性可提升。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在生物医学和临床应用上的进步，迫切需要深入评估其在复杂、证据驱动型问题上的表现。当前缺乏系统的多来源基准，这限制了对LLM医学实用性的客观评判。

Method: 作者构建了涵盖Cochrane系统综述、美国心脏协会结构化推荐和保险公司叙述性指南等多来源的临床问答基准，采用GPT-4o-mini和GPT-5进行问答测试，并分析不同来源、问题类型以及文献引用量对模型表现的影响。此外引入检索增强提示（检索真实文献摘要或语义相关的PubMed摘要）提高模型答案的依据性与准确性。

Result: 模型在结构化指南问题上的准确率最高（约90%），在叙述性指南和系统综述问题上为60-70%。模型答案的准确率与基础文献引用数显著正相关。检索增强方式（提供对真实文献摘要）可大幅提升原本错误答案项的准确率（达0.79）；语义相关的PubMed摘要提升有限（0.23），随机摘要则有负面影响（0.10）。

Conclusion: LLM在证据型临床问答上展现出较高潜力，但仍存在明显局限，尤其受数据结构化程度和证据获取方式影响。检索增强提示是一种有效提升答案准确性和溯源性的策略。未来需按专业、问题类型分层评估模型表现，以推动其医学领域的稳健应用。

Abstract: Large Language Models (LLMs) have demonstrated substantial progress in
biomedical and clinical applications, motivating rigorous evaluation of their
ability to answer nuanced, evidence-based questions. We curate a multi-source
benchmark drawing from Cochrane systematic reviews and clinical guidelines,
including structured recommendations from the American Heart Association and
narrative guidance used by insurers. Using GPT-4o-mini and GPT-5, we observe
consistent performance patterns across sources and clinical domains: accuracy
is highest on structured guideline recommendations (90%) and lower on narrative
guideline and systematic review questions (60--70%). We also find a strong
correlation between accuracy and the citation count of the underlying
systematic reviews, where each doubling of citations is associated with roughly
a 30% increase in the odds of a correct answer. Models show moderate ability to
reason about evidence quality when contextual information is supplied. When we
incorporate retrieval-augmented prompting, providing the gold-source abstract
raises accuracy on previously incorrect items to 0.79; providing top 3 PubMed
abstracts (ranked by semantic relevance) improves accuracy to 0.23, while
random abstracts reduce accuracy (0.10, within temperature variation). These
effects are mirrored in GPT-4o-mini, underscoring that source clarity and
targeted retrieval -- not just model size -- drive performance. Overall, our
results highlight both the promise and current limitations of LLMs for
evidence-based clinical question answering. Retrieval-augmented prompting
emerges as a useful strategy to improve factual accuracy and alignment with
source evidence, while stratified evaluation by specialty and question type
remains essential to understand current knowledge access and to contextualize
model performance.

</details>


### [160] [GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings](https://arxiv.org/abs/2509.10844)
*Yixuan Tang,Yi Yang*

Main category: cs.CL

TL;DR: 本文提出GAPrune，一种结合领域重要性和通用语义基础的剪枝方法，用于大模型在特定领域的压缩，能在保持甚至提升效果的同时大幅减少模型参数。


<details>
  <summary>Details</summary>
Motivation: 当前特定领域嵌入模型虽然在专业任务上表现优秀，但由于基于大型语言模型（LLM）参数量庞大，部署到资源受限设备时非常困难。传统的模型剪枝未能区分通用和领域参数，导致压缩后性能受损。因此，需要新的剪枝方法既能压缩模型又能维持专业表现。

Method: 作者提出GAPrune剪枝框架：利用Fisher信息衡量参数的重要性，并通过通用-领域梯度对齐评估参数对领域与通用目标的影响，将这两种信号通过DAI分数结合，低DAI分数的参数优先被剪掉，从而在领域保持专长的前提下去除冗余参数。

Result: 在金融（FinMTEB）和化学（ChemTEB）领域的基准测试中，GAPrune在一次性50%剪枝下，性能仅比稠密模型低2.5%，但优于其他所有基线方法。经过100步再训练后，在FinMTEB上提升4.51%，ChemTEB上提升1.73%。

Conclusion: GAPrune有效实现了大模型的高效剪枝和领域特化能力的增强，表明通过合理的剪枝策略，可以兼顾模型体积和领域性能，为领域模型的压缩和应用提供了新的技术途径。

Abstract: Domain-specific embedding models have shown promise for applications that
require specialized semantic understanding, such as coding agents and financial
retrieval systems, often achieving higher performance gains than general
models. However, state-of-the-art embedding models are typically based on LLMs,
which contain billions of parameters, making deployment challenging in
resource-constrained environments. Model compression through pruning offers a
promising solution, but existing pruning methods treat all parameters
uniformly, failing to distinguish between general semantic representations and
domain-specific patterns, leading to suboptimal pruning decisions. Thus, we
propose GAPrune, a pruning framework that addresses this challenge by
considering both domain importance and preserving general linguistic
foundation. Our method uses Fisher Information to measure importance and
general-domain gradient alignment to assess parameter behavior, then combines
these signals using our Domain Alignment Importance (DAI) scoring. Lower DAI
scores indicate that the parameter is either less important for the domain task
or creates conflicts between domain and general objectives. Experiments on two
domain benchmarks, FinMTEB and ChemTEB, show that GAPrune maintains performance
within 2.5% of dense models in one-shot pruning at 50% sparsity, while
outperforming all baselines. With retraining in 100 steps, GAPrune achieves
+4.51% improvement on FinMTEB and +1.73% on ChemTEB, demonstrating that our
pruning strategy not only preserves but enhances domain-specific capabilities.
Our findings demonstrate that principled pruning strategies can achieve model
compression and enhanced domain specialization, providing the research
community with a new approach for development.

</details>


### [161] [Text2Sign Diffusion: A Generative Approach for Gloss-Free Sign Language Production](https://arxiv.org/abs/2509.10845)
*Liqian Feng,Lintao Wang,Kun Hu,Dehui Kong,Zhiyong Wang*

Main category: cs.CL

TL;DR: 本文提出了一种无需中间符号（gloss）的手语生成方法，利用扩散模型直接从文本生成手语，显著提升了跨语言泛化和灵活性，并取得了最新最好的实验结果。


<details>
  <summary>Details</summary>
Motivation: 现有手语生成（SLP）方法通常依赖gloss作为中间步骤，但gloss的获取难度高且具有语言特异性，限制了SLP方法的泛用性和适用范围。因此，迫切需要一种不依赖gloss的SLP方法，以促进聋哑及听力障碍群体的信息平等与数字包容。

Method: 提出了Text2Sign Diffusion方法，一种基于隐空间扩散模型的无gloss手语生成框架。该方法从受噪声干扰的手语隐码和文本输入联合生成手语序列，通过非自回归迭代去噪减少误差积累，并设计了跨模态对齐模块，将手语的视频视觉信息与文本嵌入到共享隐空间，增强了语义一致性。

Result: 在PHOENIX14T和How2Sign两个主流数据集上进行了大量实验，所提方法在手语生成质量和相关性评估上均达到了最新最优性能。

Conclusion: 该文首次实现了无gloss条件下的高质量手语生成，为手语生成系统的实际应用和推广铺平了道路，推动了相关领域的技术进步。

Abstract: Sign language production (SLP) aims to translate spoken language sentences
into a sequence of pose frames in a sign language, bridging the communication
gap and promoting digital inclusion for deaf and hard-of-hearing communities.
Existing methods typically rely on gloss, a symbolic representation of sign
language words or phrases that serves as an intermediate step in SLP. This
limits the flexibility and generalization of SLP, as gloss annotations are
often unavailable and language-specific. Therefore, we present a novel
diffusion-based generative approach - Text2Sign Diffusion (Text2SignDiff) for
gloss-free SLP. Specifically, a gloss-free latent diffusion model is proposed
to generate sign language sequences from noisy latent sign codes and spoken
text jointly, reducing the potential error accumulation through a
non-autoregressive iterative denoising process. We also design a cross-modal
signing aligner that learns a shared latent space to bridge visual and textual
content in sign and spoken languages. This alignment supports the conditioned
diffusion-based process, enabling more accurate and contextually relevant sign
language generation without gloss. Extensive experiments on the commonly used
PHOENIX14T and How2Sign datasets demonstrate the effectiveness of our method,
achieving the state-of-the-art performance.

</details>


### [162] [A funny companion: Distinct neural responses to perceived AI- versus humangenerated humor](https://arxiv.org/abs/2509.10847)
*Xiaohui Rao,Hanlin Wu,Zhenguang G. Cai*

Main category: cs.CL

TL;DR: 本研究探讨了人类对来自AI伴侣与人类的幽默的认知与情感反应差异，并以脑电（EEG）手段进行比较。结果显示：尽管主观上AI和人类的幽默同样有趣，但AI的幽默语句处理时，大脑展现出更低的认知负荷和更高的情感奖励，且对AI幽默的反应会随时间递增。社会态度（如信任感）也会正向调节这种神经反应。


<details>
  <summary>Details</summary>
Motivation: 随着AI伴侣能进行类似人类的对话乃至讲笑话，了解人类如何认知和情感上反应AI幽默，成为构建高质量人机互动体验的重要问题。因此，研究者希望探索人类大脑对AI幽默的处理机制及其社会心理调节。

Method: 采用脑电图（EEG），比较受试者在处理AI与人类来源幽默时的神经反应，收集行为评定（幽默评分）和ERP成分（N400与LPP）数据，并分析社会态度等对神经反应的调节作用。

Result: 受试者主观评分AI与人类笑话同样有趣，但AI幽默引发更小的N400（更低认知需求）与更大的LPP（更强情感反应），且随着时间AI幽默的处理越来越高效和情感回报增强。对AI的信任感提升也增强了情感投入。

Conclusion: 大脑对AI幽默的反应比预期更积极且情感更强烈，表明通过对AI语言规律的适应，人们对AI的幽默反应可持续增强。这一发现挑战了“算法厌恶”偏见，证明幽默有力促进了人机社交互动的情感连接。

Abstract: As AI companions become capable of human-like communication, including
telling jokes, understanding how people cognitively and emotionally respond to
AI humor becomes increasingly important. This study used electroencephalography
(EEG) to compare how people process humor from AI versus human sources.
Behavioral analysis revealed that participants rated AI and human humor as
comparably funny. However, neurophysiological data showed that AI humor
elicited a smaller N400 effect, suggesting reduced cognitive effort during the
processing of incongruity. This was accompanied by a larger Late Positive
Potential (LPP), indicating a greater degree of surprise and emotional
response. This enhanced LPP likely stems from the violation of low initial
expectations regarding AI's comedic capabilities. Furthermore, a key temporal
dynamic emerged: human humor showed habituation effects, marked by an
increasing N400 and a decreasing LPP over time. In contrast, AI humor
demonstrated increasing processing efficiency and emotional reward, with a
decreasing N400 and an increasing LPP. This trajectory reveals how the brain
can dynamically update its predictive model of AI capabilities. This process of
cumulative reinforcement challenges "algorithm aversion" in humor, as it
demonstrates how cognitive adaptation to AI's language patterns can lead to an
intensified emotional reward. Additionally, participants' social attitudes
toward AI modulated these neural responses, with higher perceived AI
trustworthiness correlating with enhanced emotional engagement. These findings
indicate that the brain responds to AI humor with surprisingly positive and
intense reactions, highlighting humor's potential for fostering genuine
engagement in human-AI social interaction.

</details>


### [163] [Pre-Storage Reasoning for Episodic Memory: Shifting Inference Burden to Memory for Personalized Dialogue](https://arxiv.org/abs/2509.10852)
*Sangyeop Kim,Yohan Lee,Sanghwa Kim,Hyunjong Kim,Sungzoon Cho*

Main category: cs.CL

TL;DR: 本文提出了一种新的会话AI长期记忆机制PREMem，将复杂推理过程提前到记忆构建阶段，从而提升推理效率和效果。


<details>
  <summary>Details</summary>
Motivation: 当前会话AI系统将大量推理负担放在回复生成阶段，这使得模型性能严重依赖模型规模，效率和效果都受限。需要一种能够减轻推理负担，提升小模型性能的方法。

Method: PREMem方法在会话过程中从多轮对话中提取细粒度的记忆片段（事实、经历、主观信息），并在会话间建立这些片段之间的显式关联（如扩展、变化、蕴含）。这些推理操作在存储前完成，而不是在生成回复时进行，从而优化存储结构。

Result: 实验结果显示，PREMem在各类模型规模下均有显著性能提升，尤其小模型的表现可媲美大型基线模型，并且在token受限时也能保持效果。

Conclusion: 通过将推理过程前移至记忆构建阶段，PREMem不仅丰富了记忆表达，还降低了交互时的计算压力，是提升会话AI长期记忆效果的有效方法。

Abstract: Effective long-term memory in conversational AI requires synthesizing
information across multiple sessions. However, current systems place excessive
reasoning burden on response generation, making performance significantly
dependent on model sizes. We introduce PREMem (Pre-storage Reasoning for
Episodic Memory), a novel approach that shifts complex reasoning processes from
inference to memory construction. PREMem extracts fine-grained memory fragments
categorized into factual, experiential, and subjective information; it then
establishes explicit relationships between memory items across sessions,
capturing evolution patterns like extensions, transformations, and
implications. By performing this reasoning during pre-storage rather than when
generating a response, PREMem creates enriched representations while reducing
computational demands during interactions. Experiments show significant
performance improvements across all model sizes, with smaller models achieving
results comparable to much larger baselines while maintaining effectiveness
even with constrained token budgets. Code and dataset are available at
https://github.com/sangyeop-kim/PREMem.

</details>


### [164] [Quantifier Scope Interpretation in Language Learners and LLMs](https://arxiv.org/abs/2509.10860)
*Shaohua Fang,Yue Li,Yan Cong*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLM）在中英文中处理多重量词句子的能力，发现LLM整体倾向于表层解读，并在不同语言上的表现与人类类似，但程度不一，且与模型的结构和训练背景有关。


<details>
  <summary>Details</summary>
Motivation: 多重量词句子在解释上常存在歧义，且不同语言对歧义的偏好不一致。现有研究缺乏对大语言模型处理量词歧义和跨语言表现的系统分析，因此本文希望评估LLM是否能模拟人类在不同语言环境下对量词范围的理解。

Method: 本文采用跨语言对比的方法，分析LLM在中英文句子的量词范围解释，利用模型输出概率评估不同解释的倾向，并用Human similarity (HS) 分数量化模型对人类表现的模拟程度。

Result: 研究发现，多数大语言模型在中英文中都倾向于表层范围解释，这与人类习惯保持一致。部分模型对逆范围解读的辨别表现出与人类类似的跨语言差异。不同模型在HS分数上有较大变异，部分模型能较好模拟人类行为。模型结构、规模及预训练数据的语言背景对与人类表现的一致性影响显著。

Conclusion: 大语言模型能较好地模拟人类对中英文量词范围歧义的处理，但对特定语言的细微区分能力依赖于模型的规模、结构及训练语料，提示未来模型设计需关注多语言和复杂语义理解能力的提升。

Abstract: Sentences with multiple quantifiers often lead to interpretive ambiguities,
which can vary across languages. This study adopts a cross-linguistic approach
to examine how large language models (LLMs) handle quantifier scope
interpretation in English and Chinese, using probabilities to assess
interpretive likelihood. Human similarity (HS) scores were used to quantify the
extent to which LLMs emulate human performance across language groups. Results
reveal that most LLMs prefer the surface scope interpretations, aligning with
human tendencies, while only some differentiate between English and Chinese in
the inverse scope preferences, reflecting human-similar patterns. HS scores
highlight variability in LLMs' approximation of human behavior, but their
overall potential to align with humans is notable. Differences in model
architecture, scale, and particularly models' pre-training data language
background, significantly influence how closely LLMs approximate human
quantifier scope interpretations.

</details>


### [165] [Term2Note: Synthesising Differentially Private Clinical Notes from Medical Terms](https://arxiv.org/abs/2509.10882)
*Yuping Wu,Viktor Schlegel,Warren Del-Pinto,Srinivasan Nandakumar,Iqra Zahid,Yidan Sun,Usama Farghaly Omar,Amirah Jasmine,Arun-Kumar Kaliya-Perumal,Chun Shen Tham,Gabriel Connors,Anil A Bharath,Goran Nenadic*

Main category: cs.CL

TL;DR: 该论文提出了一种名为Term2Note的方法，可以在严格差分隐私（DP）条件下合成高质量的长篇临床笔记，在保护隐私的同时保持数据的高效用性。


<details>
  <summary>Details</summary>
Motivation: 在医疗等高风险领域，隐私保护使得真实训练数据难以使用，而差分隐私合成数据是一种有前景的解决方案，但很难在隐私和数据效用之间取得平衡。该论文旨在解决在临床笔记这样的领域专用长文本生成中隐私与效用的权衡难题。

Method: 作者提出Term2Note方法，将临床笔记的内容与形式进行结构化分离：首先，对受差分隐私保护的医学术语进行条件生成，再基于这些内容分段生成笔记，并对每一模块分别设定DP约束。通过DP质量优化器选取高质量的合成输出，增强了合成数据的质量。

Result: 实验结果显示，Term2Note生成的合成笔记在统计属性上与真实笔记高度一致，具备强拟真性。同时，用合成笔记训练的多标签分类模型表现与用真实数据的数据训练效果相当，显示出高效用性。在与现有DP文本生成方法的对比中，Term2Note在保真度和效用上均有明显提升，且假设更少。

Conclusion: Term2Note方法为敏感临床笔记数据的隐私保护性合成提供了实用、有效的解决方案，兼顾了数据效用与隐私保护，可作为真实数据使用的可行替代手段。

Abstract: Training data is fundamental to the success of modern machine learning
models, yet in high-stakes domains such as healthcare, the use of real-world
training data is severely constrained by concerns over privacy leakage. A
promising solution to this challenge is the use of differentially private (DP)
synthetic data, which offers formal privacy guarantees while maintaining data
utility. However, striking the right balance between privacy protection and
utility remains challenging in clinical note synthesis, given its domain
specificity and the complexity of long-form text generation. In this paper, we
present Term2Note, a methodology to synthesise long clinical notes under strong
DP constraints. By structurally separating content and form, Term2Note
generates section-wise note content conditioned on DP medical terms, with each
governed by separate DP constraints. A DP quality maximiser further enhances
synthetic notes by selecting high-quality outputs. Experimental results show
that Term2Note produces synthetic notes with statistical properties closely
aligned with real clinical notes, demonstrating strong fidelity. In addition,
multi-label classification models trained on these synthetic notes perform
comparably to those trained on real data, confirming their high utility.
Compared to existing DP text generation baselines, Term2Note achieves
substantial improvements in both fidelity and utility while operating under
fewer assumptions, suggesting its potential as a viable privacy-preserving
alternative to using sensitive clinical notes.

</details>


### [166] [CultureSynth: A Hierarchical Taxonomy-Guided and Retrieval-Augmented Framework for Cultural Question-Answer Synthesis](https://arxiv.org/abs/2509.10886)
*Xinyu Zhang,Pei Zhang,Shuang Luo,Jialong Tang,Yu Wan,Baosong Yang,Fei Huang*

Main category: cs.CL

TL;DR: 本文提出了CultureSynth框架，用于系统性评估大语言模型（LLM）的多文化能力，包括多语种文化分类体系和基于RAG合成的文化问答基准，支持规模化评测并减少人工标注依赖。


<details>
  <summary>Details</summary>
Motivation: 现有针对LLM的文化能力评测方法存在分类不统一、领域局限性和高度依赖人工标注的问题，限制了评估的全面性和可扩展性。

Method: 构建了一个涵盖12个主类和130个次类的多语种文化层级分类，并基于RAG（检索增强生成）方法，合成多语种、多主题的文化相关问答对，形成CultureSynth-7基准数据集。对14种不同规模的主流LLM进行评测分析。

Result: CultureSynth-7包含19360个合成问答和4149个人工验证样本，涵盖7种语言。评测发现ChatGPT-4o-Latest和Qwen2.5-72B-Instruct表现最优，3B参数量成为文化能力的基本门槛，不同模型在知识处理上有结构性差异，且地理分布上存在显著差异。

Conclusion: CultureSynth为LLM的多文化能力评估提供了可扩展、全面的方法，降低了对人工标注的依赖，为开发文化敏感的 AI 系统奠定基础。

Abstract: Cultural competence, defined as the ability to understand and adapt to
multicultural contexts, is increasingly vital for large language models (LLMs)
in global environments. While several cultural benchmarks exist to assess LLMs'
cultural competence, current evaluations suffer from fragmented taxonomies,
domain specificity, and heavy reliance on manual data annotation. To address
these limitations, we introduce CultureSynth, a novel framework comprising (1)
a comprehensive hierarchical multilingual cultural taxonomy covering 12 primary
and 130 secondary topics, and (2) a Retrieval-Augmented Generation (RAG)-based
methodology leveraging factual knowledge to synthesize culturally relevant
question-answer pairs. The CultureSynth-7 synthetic benchmark contains 19,360
entries and 4,149 manually verified entries across 7 languages. Evaluation of
14 prevalent LLMs of different sizes reveals clear performance stratification
led by ChatGPT-4o-Latest and Qwen2.5-72B-Instruct. The results demonstrate that
a 3B-parameter threshold is necessary for achieving basic cultural competence,
models display varying architectural biases in knowledge processing, and
significant geographic disparities exist across models. We believe that
CultureSynth offers a scalable framework for developing culturally aware AI
systems while reducing reliance on manual annotation\footnote{Benchmark is
available at https://github.com/Eyr3/CultureSynth.}.

</details>


### [167] [Aligning ESG Controversy Data with International Guidelines through Semi-Automatic Ontology Construction](https://arxiv.org/abs/2509.10922)
*Tsuyoshi Iwata,Guillaume Comte,Melissa Flores,Ryoma Kondo,Ryohei Hisano*

Main category: cs.CL

TL;DR: 本文提出了一种半自动化方法，将新闻中披露的环境、社会和治理（ESG）事件与国际规范框架（如联合国全球契约、可持续发展目标）进行结构化对齐，并以知识图谱形式展现。


<details>
  <summary>Details</summary>
Motivation: ESG数据在监管和投资中的重要性日益上升。由于相关事件多在新闻等非结构化文本中披露，且现有国际规范用语抽象、分类不标准，与商业数据商的系统差异大，导致非财务风险缺乏可比、可解释的结构化描述。因此，亟需将新闻中的争议事件与国际框架有效对齐，实现自动化和标准化表示。

Method: 作者采用轻量级本体设计、形式化模式建模和大语言模型，将抽象的国际规范转化为可复用的RDF模板，然后依据这些模板从新闻报道中提取事件信息，构建事件与规范条款关联的知识图谱，实现结构化信息抽取和对齐。

Result: 该方法能自动、高效地从非结构化新闻中抽取ESG事件，并准确对齐至国际可持续发展规范，为事件与原则之间建立透明的结构化关联，支持后续的合规性识别和解读。

Conclusion: 所提方法为ESG非财务风险的结构化建模和合规评估提供了可扩展、透明的新框架，有助于国际化标准的采纳和实践。

Abstract: The growing importance of environmental, social, and governance data in
regulatory and investment contexts has increased the need for accurate,
interpretable, and internationally aligned representations of non-financial
risks, particularly those reported in unstructured news sources. However,
aligning such controversy-related data with principle-based normative
frameworks, such as the United Nations Global Compact or Sustainable
Development Goals, presents significant challenges. These frameworks are
typically expressed in abstract language, lack standardized taxonomies, and
differ from the proprietary classification systems used by commercial data
providers. In this paper, we present a semi-automatic method for constructing
structured knowledge representations of environmental, social, and governance
events reported in the news. Our approach uses lightweight ontology design,
formal pattern modeling, and large language models to convert normative
principles into reusable templates expressed in the Resource Description
Framework. These templates are used to extract relevant information from news
content and populate a structured knowledge graph that links reported incidents
to specific framework principles. The result is a scalable and transparent
framework for identifying and interpreting non-compliance with international
sustainability guidelines.

</details>


### [168] [Introducing Spotlight: A Novel Approach for Generating Captivating Key Information from Documents](https://arxiv.org/abs/2509.10935)
*Ankan Mullick,Sombit Bose,Rounak Saha,Ayan Kumar Bhowmick,Aditya Vempaty,Prasenjit Dey,Ravi Kokku,Pawan Goyal,Niloy Ganguly*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的信息抽取模式——Spotlight，以突出文档中最引人注目的部分，并生成简洁、富有吸引力的叙述。该方法相比传统摘要，更注重内容的吸引力和读者参与度。


<details>
  <summary>Details</summary>
Motivation: 传统摘要往往要求信息全面覆盖，导致内容平淡，缺乏吸引力。作者希望通过聚焦于文档中最具吸引力的部分，提高提取内容的可读性和引发读者深入关注原始材料。

Method: 提出Spotlight模式，并正式将其与相关内容区分开。作者编制新数据集，进行基准测试，并采用两阶段方法：首先用基准数据微调大模型，然后通过Direct Preference Optimization（DPO）进行对齐优化。

Result: 通过详细基准测试，结果表明所提出的模型不仅能够精准识别关键内容，还提升了文本的可读性和原文的吸引力。

Conclusion: Spotlight模式在信息抽取任务中不仅突出关键信息，还能增强叙述的吸引力和读者参与度，为文档内容生成带来创新思路。

Abstract: In this paper, we introduce Spotlight, a novel paradigm for information
extraction that produces concise, engaging narratives by highlighting the most
compelling aspects of a document. Unlike traditional summaries, which
prioritize comprehensive coverage, spotlights selectively emphasize intriguing
content to foster deeper reader engagement with the source material. We
formally differentiate spotlights from related constructs and support our
analysis with a detailed benchmarking study using new datasets curated for this
work. To generate high-quality spotlights, we propose a two-stage approach:
fine-tuning a large language model on our benchmark data, followed by alignment
via Direct Preference Optimization (DPO). Our comprehensive evaluation
demonstrates that the resulting model not only identifies key elements with
precision but also enhances readability and boosts the engagement value of the
original document.

</details>


### [169] [An Interpretable Benchmark for Clickbait Detection and Tactic Attribution](https://arxiv.org/abs/2509.10937)
*Lihi Nofar,Tomer Portal,Aviv Elbaz,Alexander Apartsin,Yehudit Aperstein*

Main category: cs.CL

TL;DR: 本文提出了一种可解释的点击诱饵检测模型，不仅能判断标题是否为点击诱饵，还能识别其具体操控策略，并共享了合成数据集。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法虽能检测点击诱饵内容，但因为缺乏可解释性，难以实际应用。透明可信的检测模型有助于增强用户对数字媒体信息的信任。

Method: 作者构建了一个通过预定义操控策略系统化扩展真实新闻标题而成的合成数据集。在两阶段框架中，第一阶段对比了微调BERT分类器和大语言模型（如GPT-4.0、Gemini 2.4 Flash）在不同提示方式下的点击诱饵检测性能；第二阶段使用BERT模型分析标题中采用的具体操控策略。

Result: 实验展示了不同模型在点击诱饵检测和策略归因上的表现，验证了两阶段框架的有效性。合成数据集便于模型行为分析和对比。

Conclusion: 该研究推进了可解释、值得信赖的AI检测系统的发展，有助于打击操纵性媒体内容，对学术和产业界均有实际意义。数据集已开放共享。

Abstract: The proliferation of clickbait headlines poses significant challenges to the
credibility of information and user trust in digital media. While recent
advances in machine learning have improved the detection of manipulative
content, the lack of explainability limits their practical adoption. This paper
presents a model for explainable clickbait detection that not only identifies
clickbait titles but also attributes them to specific linguistic manipulation
strategies. We introduce a synthetic dataset generated by systematically
augmenting real news headlines using a predefined catalogue of clickbait
strategies. This dataset enables controlled experimentation and detailed
analysis of model behaviour. We present a two-stage framework for automatic
clickbait analysis comprising detection and tactic attribution. In the first
stage, we compare a fine-tuned BERT classifier with large language models
(LLMs), specifically GPT-4.0 and Gemini 2.4 Flash, under both zero-shot
prompting and few-shot prompting enriched with illustrative clickbait headlines
and their associated persuasive tactics. In the second stage, a dedicated
BERT-based classifier predicts the specific clickbait strategies present in
each headline. This work advances the development of transparent and
trustworthy AI systems for combating manipulative media content. We share the
dataset with the research community at
https://github.com/LLM-HITCS25S/ClickbaitTacticsDetection

</details>


### [170] [EmoBench-Reddit: A Hierarchical Benchmark for Evaluating the Emotional Intelligence of Multimodal Large Language Models](https://arxiv.org/abs/2509.11101)
*Haokun Li,Yazhou Zhang,Jizhi Ding,Qiuchi Li,Peng Zhang*

Main category: cs.CL

TL;DR: 本文提出了EmoBench-Reddit，这是一个面向多模态情感理解的全新层次化基准，填补了现有多模态大模型评测中过于关注客观视觉问答、忽视情感理解的空白。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型评测主要评估客观任务，未能充分考察模型理解复杂、主观人类情感的能力。因此需要开发更契合真实应用场景的主观情感理解评测基准。

Method: 作者从Reddit精心筛选了350个包含图片、用户文本及情感类别的样本（情感类别由用户flair确认），构建了EmoBench-Reddit数据集。设计了分级任务体系，从感知（如颜色、物体识别）到认知（包括场景推理、意图理解和深度共情），每条数据包含6道选择题和1道开放题，并结合AI和人工进行标注质量把控。

Result: 成功建立了一个涵盖多种情感类别（如悲伤、幽默、讽刺、快乐），兼具感知和认知层级任务的高质量、多模态情感理解评测数据集。该基准丰富了MLLMs在主观情感任务上的评测维度。

Conclusion: EmoBench-Reddit为多模态大模型的主观情感理解能力提供了切实可用的评测工具，有助于推动该领域模型朝更深层次感知和理解人类情感的方向发展。

Abstract: With the rapid advancement of Multimodal Large Language Models (MLLMs), they
have demonstrated exceptional capabilities across a variety of vision-language
tasks. However, current evaluation benchmarks predominantly focus on objective
visual question answering or captioning, inadequately assessing the models'
ability to understand complex and subjective human emotions. To bridge this
gap, we introduce EmoBench-Reddit, a novel, hierarchical benchmark for
multimodal emotion understanding. The dataset comprises 350 meticulously
curated samples from the social media platform Reddit, each containing an
image, associated user-provided text, and an emotion category (sad, humor,
sarcasm, happy) confirmed by user flairs. We designed a hierarchical task
framework that progresses from basic perception to advanced cognition, with
each data point featuring six multiple-choice questions and one open-ended
question of increasing difficulty. Perception tasks evaluate the model's
ability to identify basic visual elements (e.g., colors, objects), while
cognition tasks require scene reasoning, intent understanding, and deep empathy
integrating textual context. We ensured annotation quality through a
combination of AI assistance (Claude 4) and manual verification.

</details>


### [171] [Fluid Language Model Benchmarking](https://arxiv.org/abs/2509.11106)
*Valentin Hofmann,David Heineman,Ian Magnusson,Kyle Lo,Jesse Dodge,Maarten Sap,Pang Wei Koh,Chun Wang,Hannaneh Hajishirzi,Noah A. Smith*

Main category: cs.CL

TL;DR: 本文提出了一种名为Fluid Benchmarking的新型语言模型评测方法，通过借鉴心理测量学中的理念，根据模型能力水平动态选择测评项目，有效提升了评测的效率、有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 目前语言模型（LM）评测存在成本高、准确性不足、标签错误与评测饱和等问题。以往应对策略多局限于单一方面，缺乏对评测全局质量的系统提升。为解决这些问题，作者希望提出更科学、全面的评测方法。

Method: Fluid Benchmarking受心理测量学启发，采用基于现有LM评测结果估算项目反应模型，并结合动态项目选择机制（类似教育领域的计算机自适应测试），针对不同能力水平的模型选择合适的测试项目。实验对比了随机抽样和基于项目反应理论的其他基线方法。

Result: 在效率、有效性、方差和饱和度四个维度，与常用基线方法相比，Fluid Benchmarking均表现更优。具体如在MMLU基准上，用更少的测试条目达到了更高的评测有效性和更小的方差。分析表明，项目反应理论提升了有效性，动态选择显著降低了方差。

Conclusion: 用Fluid Benchmarking替代静态评测框架后，语言模型的评测在多方面都获得了系统性提升，说明自适应和智能化评测应成为未来LM评测的新趋势。

Abstract: Language model (LM) benchmarking faces several challenges: comprehensive
evaluations are costly, benchmarks often fail to measure the intended
capabilities, and evaluation quality can degrade due to labeling errors and
benchmark saturation. Although various strategies have been proposed to
mitigate these issues, they tend to address individual aspects in isolation,
neglecting broader questions about overall evaluation quality. Here, we
introduce Fluid Benchmarking, a new evaluation approach that advances LM
benchmarking across multiple dimensions. Inspired by psychometrics, Fluid
Benchmarking is based on the insight that the relative value of benchmark items
depends on an LM's capability level, suggesting that evaluation should adapt to
each LM. Methodologically, Fluid Benchmarking estimates an item response model
based on existing LM evaluation results and uses the inferred quantities to
select evaluation items dynamically, similar to computerized adaptive testing
in education. In our experiments, we compare Fluid Benchmarking against the
common practice of random item sampling as well as more sophisticated
baselines, including alternative methods grounded in item response theory. We
examine four dimensions -- efficiency, validity, variance, and saturation --
and find that Fluid Benchmarking achieves superior performance in all of them
(e.g., higher validity and less variance on MMLU with fifty times fewer items).
Our analysis shows that the two components of Fluid Benchmarking have distinct
effects: item response theory, used to map performance into a latent ability
space, increases validity, while dynamic item selection reduces variance.
Overall, our results suggest that LM benchmarking can be substantially improved
by moving beyond static evaluation.

</details>


### [172] [We Argue to Agree: Towards Personality-Driven Argumentation-Based Negotiation Dialogue Systems for Tourism](https://arxiv.org/abs/2509.11118)
*Priyanshu Priya,Saurav Dudhate,Desai Vishesh Yasheshbhai,Asif Ekbal*

Main category: cs.CL

TL;DR: 本文提出了一个结合个性化和论证机制的自动化谈判对话生成任务，并构建了相关数据集，验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 传统谈判对话系统在处理个性化交流和论证能力方面存在不足。为提升系统适应性和对复杂冲突的解决能力，需要结合论证机制与个性属性。

Method: 提出了基于个性驱动的论证型谈判对话生成（PAN-DG）任务，并构建了旅游领域专用的个性驱动论证型对话数据集（PACT），该数据集利用大语言模型生成，并包含三类不同个性设定。通过自动和人工方法评估数据集质量，并对预训练与微调大模型在任务中的效果进行对比实验。

Result: 数据集对话质量高，经多维评估，微调后的大语言模型能更有效地生成个性化且具备理性推理的对话内容。

Conclusion: 所提出的数据集和任务能显著提升谈判对话系统的个性化和推理能力，为后续研究提供了坚实基础。

Abstract: Integrating argumentation mechanisms into negotiation dialogue systems
improves conflict resolution through exchanges of arguments and critiques.
Moreover, incorporating personality attributes enhances adaptability by
aligning interactions with individuals' preferences and styles. To advance
these capabilities in negotiation dialogue systems, we propose a novel
Personality-driven Argumentation-based Negotiation Dialogue Generation (PAN-DG)
task. To support this task, we introduce PACT, a dataset of Personality-driven
Argumentation-based negotiation Conversations for Tourism sector. This dataset,
generated using Large Language Models (LLMs), features three distinct
personality profiles, viz. Argumentation Profile, Preference Profile, and
Buying Style Profile to simulate a variety of negotiation scenarios involving
diverse personalities. Thorough automatic and manual evaluations indicate that
the dataset comprises high-quality dialogues. Further, we conduct comparative
experiments between pre-trained and fine-tuned LLMs for the PAN-DG task.
Multi-dimensional evaluation demonstrates that the fine-tuned LLMs effectively
generate personality-driven rational responses during negotiations. This
underscores the effectiveness of PACT in enhancing personalization and
reasoning capabilities in negotiation dialogue systems, thereby establishing a
foundation for future research in this domain.

</details>


### [173] [Joint Effects of Argumentation Theory, Audio Modality and Data Enrichment on LLM-Based Fallacy Classification](https://arxiv.org/abs/2509.11127)
*Hongxu Zhou,Hylke Westerdijk,Khondoker Ittehadul Islam*

Main category: cs.CL

TL;DR: 本研究检验了上下文和情感语调元数据对大语言模型判别谬误任务表现的影响，发现增加这些信息反而可能降低模型表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在谬误识别等高阶推理任务中的表现令人关注，尤其是在真实环境如政治辩论中，不同的输入信息（上下文、情感元数据）会如何影响模型决策，尚未系统探索。

Method: 作者基于美国总统辩论数据，设定六类谬误，通过不同提示方式（包括两种理论链式思考框架与基线提示），在Qwen-3(8B)模型下测试输入仅文本、文本+上下文、文本+上下文+情感语调元数据三种设定，分析其对模型表现的影响。

Result: 理论驱动的链式思考提示有时可以提高可解释性和部分准确率，但引入上下文、尤其是情感元数据，常常导致模型表现下降。情感元数据使模型倾向于判为感情诉诸类谬误，降低了逻辑推理能力。总体来看，基础提示的表现反而优于提升型提示。

Conclusion: 引入过多信息（上下文、情感元数据）可能造成模型关注分散，反而降低谬误分类准确率，基础提示在此任务中更有效。

Abstract: This study investigates how context and emotional tone metadata influence
large language model (LLM) reasoning and performance in fallacy classification
tasks, particularly within political debate settings. Using data from U.S.
presidential debates, we classify six fallacy types through various prompting
strategies applied to the Qwen-3 (8B) model. We introduce two theoretically
grounded Chain-of-Thought frameworks: Pragma-Dialectics and the Periodic Table
of Arguments, and evaluate their effectiveness against a baseline prompt under
three input settings: text-only, text with context, and text with both context
and audio-based emotional tone metadata. Results suggest that while theoretical
prompting can improve interpretability and, in some cases, accuracy, the
addition of context and especially emotional tone metadata often leads to
lowered performance. Emotional tone metadata biases the model toward labeling
statements as \textit{Appeal to Emotion}, worsening logical reasoning. Overall,
basic prompts often outperformed enhanced ones, suggesting that attention
dilution from added inputs may worsen rather than improve fallacy
classification in LLMs.

</details>


### [174] [When Smiley Turns Hostile: Interpreting How Emojis Trigger LLMs' Toxicity](https://arxiv.org/abs/2509.11141)
*Shiyao Cui,Xijia Feng,Yingkang Wang,Junxiao Yang,Zhexin Zhang,Biplab Sikdar,Hongning Wang,Han Qiu,Minlie Huang*

Main category: cs.CL

TL;DR: 本文探讨了表情符号（emoji）在大语言模型（LLM）中可能诱发有毒内容生成的现象，并揭示表情符号可作为规避安全机制的异质语义通道。实验表明，在多语言、多模型环境下，含emoji的提示容易促使LLM输出有害内容，并分析了此现象背后的原因。


<details>
  <summary>Details</summary>
Motivation: 虽然表情符号常与友好、俏皮相关，但作者观察到它们或许会诱发LLM生成有毒内容。因而，研究者希望探究（1）表情符号是否真能增强LLM的毒性输出，以及（2）此现象的成因及其解释。

Method: 作者通过自动生成带有emoji的提示，构建了表达隐晦有毒意图的测试集，并在7种流行LLM、5种主流语言以及越狱任务中，全方位开展实验。同时，采用模型级解释方法，从语义认知、序列生成、分词等层面分析emoji为何能影响模型毒性生成机制，并进一步追溯预训练语料中与emoji相关的数据污染。

Result: 实验结果显示，含有emoji的提示可显著提升LLM生成有毒内容的概率。模型分析指出，emoji能够作为规避模型安全机制的通道，相关数据污染也与毒性输出存在关联。

Conclusion: 表情符号可增强LLM毒性内容生成能力，且其在语义和技术层面为规避安全策略提供了新途径。需进一步关注训练数据和安全机制对emoji的处理方式，以降低潜在风险。

Abstract: Emojis are globally used non-verbal cues in digital communication, and
extensive research has examined how large language models (LLMs) understand and
utilize emojis across contexts. While usually associated with friendliness or
playfulness, it is observed that emojis may trigger toxic content generation in
LLMs. Motivated by such a observation, we aim to investigate: (1) whether
emojis can clearly enhance the toxicity generation in LLMs and (2) how to
interpret this phenomenon. We begin with a comprehensive exploration of
emoji-triggered LLM toxicity generation by automating the construction of
prompts with emojis to subtly express toxic intent. Experiments across 5
mainstream languages on 7 famous LLMs along with jailbreak tasks demonstrate
that prompts with emojis could easily induce toxicity generation. To understand
this phenomenon, we conduct model-level interpretations spanning semantic
cognition, sequence generation and tokenization, suggesting that emojis can act
as a heterogeneous semantic channel to bypass the safety mechanisms. To pursue
deeper insights, we further probe the pre-training corpus and uncover potential
correlation between the emoji-related data polution with the toxicity
generation behaviors. Supplementary materials provide our implementation code
and data. (Warning: This paper contains potentially sensitive contents)

</details>


### [175] [Text2Mem: A Unified Memory Operation Language for Memory Operating System](https://arxiv.org/abs/2509.11145)
*Felix Wang,Boyu Chen,Kerun Xu,Bo Tang,Feiyu Xiong,Zhiyu Li*

Main category: cs.CL

TL;DR: 本文提出了Text2Mem语言与配套基准，旨在为大语言模型智能体提供统一、可执行且标准化的记忆操作接口，解决记忆操作缺乏高阶指令和规范、难以兼容等问题。


<details>
  <summary>Details</summary>
Motivation: 当前大模型智能体对长期记忆的依赖日益增强，但现有记忆框架仅支持基础操作，缺乏如合并、提升、降级等高阶操作，且无正式规范，导致操作不一致和不可预测性。研究者旨在解决记忆指令集单一、无可执行标准等问题。

Method: 作者提出Text2Mem语言，定义了适用于编码、存储与检索的紧凑而具表现力的操作集合，将每个指令以JSON模式结构表达，通过解析器转为标准化、类型化的操作对象。执行前验证器确保指令正确，适配器可将指令映射到SQL原型后端或真实记忆框架，必要时集成向量嵌入、摘要等服务，所有结果通过统一协议输出。

Result: Text2Mem实现了内存指令操作的安全性、确定性和可移植性，可适应异构后端环境。作者还规划了Text2Mem Bench基准，将记忆指令生成和底层执行分离，便于系统化评测。

Conclusion: Text2Mem及其基准为智能体记忆管理建立了首个标准化基础，实现了从自然语言到可靠记忆操作的统一路径，并为多种后端间的通用性和规范性提供支持。

Abstract: Large language model agents increasingly depend on memory to sustain long
horizon interaction, but existing frameworks remain limited. Most expose only a
few basic primitives such as encode, retrieve, and delete, while higher order
operations like merge, promote, demote, split, lock, and expire are missing or
inconsistently supported. Moreover, there is no formal and executable
specification for memory commands, leaving scope and lifecycle rules implicit
and causing unpredictable behavior across systems. We introduce Text2Mem, a
unified memory operation language that provides a standardized pathway from
natural language to reliable execution. Text2Mem defines a compact yet
expressive operation set aligned with encoding, storage, and retrieval. Each
instruction is represented as a JSON based schema instance with required fields
and semantic invariants, which a parser transforms into typed operation objects
with normalized parameters. A validator ensures correctness before execution,
while adapters map typed objects either to a SQL prototype backend or to real
memory frameworks. Model based services such as embeddings or summarization are
integrated when required. All results are returned through a unified execution
contract. This design ensures safety, determinism, and portability across
heterogeneous backends. We also outline Text2Mem Bench, a planned benchmark
that separates schema generation from backend execution to enable systematic
evaluation. Together, these components establish the first standardized
foundation for memory control in agents.

</details>


### [176] [Differentially-private text generation degrades output language quality](https://arxiv.org/abs/2509.11176)
*Erion Çano,Ivan Habernal*

Main category: cs.CL

TL;DR: 本文评估了在差分隐私(DP)下微调大型语言模型(LLM)对生成文本质量与下游任务表现的影响。结果显示，隐私强度提高会明显降低生成文本的长度、语法正确性、多样性及下游分类准确率。


<details>
  <summary>Details</summary>
Motivation: 保护用户隐私成为合成数据应用中的重要需求，通过在差分隐私框架下微调LLM成为一种流行做法。但目前尚不清楚DP微调对生成文本质量和实用性的具体影响。作者希望深入探索这一点，为隐私与文本效用之间的权衡提供实证依据。

Method: 作者选用五种LLM，在三种不同语料库上，设置四个隐私强度等级进行DP微调。通过系统评估生成文本的长度、语法正确性以及词汇多样性，并检验这些文本在下游分类任务(如图书类别辨别与死因识别)中的效果。

Result: 随着隐私强度增强，LLM生成的文本长度缩短不少（至少77%），语法正确性下降（至少9%），双词多样性减少（至少10%），同时在下游分类任务中的准确率也明显下降。

Conclusion: 在提升差分隐私保护强度的同时，LLM生成文本的可用性和质量大幅下降。因此，隐私保护与数据质量存在显著权衡，过强的隐私约束可能削弱合成数据的实用价值。

Abstract: Ensuring user privacy by synthesizing data from large language models (LLMs)
tuned under differential privacy (DP) has become popular recently. However, the
impact of DP fine-tuned LLMs on the quality of the language and the utility of
the texts they produce has not been investigated. In this work, we tune five
LLMs with three corpora under four levels of privacy and assess the length, the
grammatical correctness, and the lexical diversity of the text outputs they
produce. We also probe the utility of the synthetic outputs in downstream
classification tasks such as book genre recognition based on book descriptions
and cause of death recognition based on verbal autopsies. The results indicate
that LLMs tuned under stronger privacy constrains produce texts that are
shorter by at least 77 %, that are less grammatically correct by at least 9 %,
and are less diverse by at least 10 % in bi-gram diversity. Furthermore, the
accuracy they reach in downstream classification tasks decreases, which might
be detrimental to the usefulness of the generated synthetic data.

</details>


### [177] [Optimal Brain Restoration for Joint Quantization and Sparsification of LLMs](https://arxiv.org/abs/2509.11177)
*Hang Guo,Yawei Li,Luca Benini*

Main category: cs.CL

TL;DR: 该论文提出了一种结合量化和剪枝的LLM压缩新框架Optimal Brain Restoration（OBR），能够更进一步提升压缩比和推理效率。


<details>
  <summary>Details</summary>
Motivation: 单独使用量化或剪枝来压缩大语言模型已接近极限，进一步压缩变得困难，因此作者希望通过联合两者来突破瓶颈。然而，量化和剪枝在权重分布上的需求相冲突，需要创新方法解决。

Method: 提出Optimal Brain Restoration（OBR）框架，通过错误补偿机制结合量化与剪枝，将二阶Hessian目标转化为可计算的代理问题，最终得到封闭解。该方法无需训练，并实现量化与稀疏性的协同优化。

Result: 实验结果表明，OBR可以在LLM上实现W4A4KV4的激进量化并达50%稀疏，同时比FP16稠密基线提速4.72倍、内存缩减6.4倍。

Conclusion: OBR能够突破单一量化或剪枝的极限，提升LLM压缩与推理能力，为后续高效大模型部署提供了新思路。

Abstract: Recent advances in Large Language Model (LLM) compression, such as
quantization and pruning, have achieved notable success. However, as these
techniques gradually approach their respective limits, relying on a single
method for further compression has become increasingly challenging. In this
work, we explore an alternative solution by combining quantization and
sparsity. This joint approach, though promising, introduces new difficulties
due to the inherently conflicting requirements on weight distributions:
quantization favors compact ranges, while pruning benefits from high variance.
To attack this problem, we propose Optimal Brain Restoration (OBR), a general
and training-free framework that aligns pruning and quantization by error
compensation between both. OBR minimizes performance degradation on downstream
tasks by building on a second-order Hessian objective, which is then
reformulated into a tractable problem through surrogate approximation and
ultimately reaches a closed-form solution via group error compensation.
Experiments show that OBR enables aggressive W4A4KV4 quantization with 50%
sparsity on existing LLMs, and delivers up to 4.72x speedup and 6.4x memory
reduction compared to the FP16-dense baseline.

</details>


### [178] [RanAT4BIE: Random Adversarial Training for Biomedical Information Extraction](https://arxiv.org/abs/2509.11191)
*Jian Chen,Shengyi Lv,Leilei Su*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法——随机对抗训练（RAT），用于提升生物医学信息抽取任务的性能，同时降低计算成本，并在实验中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 对抗训练能提升预训练语言模型在生物医学信息抽取任务的表现，但会引入较大的计算开销。作者希望在提升模型性能和泛化能力的同时，解决计算效率问题。

Method: 以PubMedBERT为基础，先用常规对抗训练提升模型性能，再提出RAT，通过将随机采样机制与对抗训练相结合，减少运算量，同时保持训练的有效性和鲁棒性。

Result: RAT方法在BioIE任务中表现优于基线模型，显著提高了性能，并大幅降低了计算需求。

Conclusion: RAT是提升生物医学自然语言处理模型性能与计算效率的一种有效方案，有望成为该领域的转折性方法。

Abstract: We introduce random adversarial training (RAT), a novel framework
successfully applied to biomedical information extraction (BioIE) tasks.
Building on PubMedBERT as the foundational architecture, our study first
validates the effectiveness of conventional adversarial training in enhancing
pre-trained language models' performance on BioIE tasks. While adversarial
training yields significant improvements across various performance metrics, it
also introduces considerable computational overhead. To address this
limitation, we propose RAT as an efficiency solution for biomedical information
extraction. This framework strategically integrates random sampling mechanisms
with adversarial training principles, achieving dual objectives: enhanced model
generalization and robustness while significantly reducing computational costs.
Through comprehensive evaluations, RAT demonstrates superior performance
compared to baseline models in BioIE tasks. The results highlight RAT's
potential as a transformative framework for biomedical natural language
processing, offering a balanced solution to the model performance and
computational efficiency.

</details>


### [179] [The Prompt Engineering Report Distilled: Quick Start Guide for Life Sciences](https://arxiv.org/abs/2509.11295)
*Valentin Romanov,Steven A Niederer*

Main category: cs.CL

TL;DR: 本论文针对生命科学领域的常见工作流程，总结并精炼了58种提示词工程方法为6种核心技巧，并结合生命科学应用场景，提出实用指导以提升大语言模型(LMM)的使用效率和结果质量。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛应用于生命科学，如何设计高效提示（prompt）以获得高质量输出已成为重要课题。当前提示工程方法繁多且分散，研究者往往难以系统掌握并灵活应用。精炼、归纳核心方法并结合生命科学实际需求具有重要价值。

Method: 回顾并总结了2025年Prompt Report中58种文本提示工程方法，提炼为零样本、少样本、思路生成、集成、自我批判和分解6大核心技巧，并结合文献总结、数据抽取、编辑等具体生命科学案例详解每种方法的优势及使用建议。同时，分析多轮对话失效、幻觉生成、推理与非推理模型差异等常见陷阱，并评估多平台工具与功能局限性，提出结构化提示优化建议。

Result: 提出了不同任务情境下（如文献综述、数据提取、编辑处理等）的详细提示设计建议，识别并规避了提示工程中的常见问题，比较了主流LLM平台（OpenAI、Google、Anthropic和Perplexity）的相关工具效果，强调提示工程与现有科研流程的互补性和实操价值。

Conclusion: 本研究推动了提示工程从零散尝试走向系统化方法，为生命科学领域的研究者提供了核心提示工程原则和具体操作指南，有助于显著提升研究效率和大语言模型应用的可靠性与质量。

Abstract: Developing effective prompts demands significant cognitive investment to
generate reliable, high-quality responses from Large Language Models (LLMs). By
deploying case-specific prompt engineering techniques that streamline
frequently performed life sciences workflows, researchers could achieve
substantial efficiency gains that far exceed the initial time investment
required to master these techniques. The Prompt Report published in 2025
outlined 58 different text-based prompt engineering techniques, highlighting
the numerous ways prompts could be constructed. To provide actionable
guidelines and reduce the friction of navigating these various approaches, we
distil this report to focus on 6 core techniques: zero-shot, few-shot
approaches, thought generation, ensembling, self-criticism, and decomposition.
We breakdown the significance of each approach and ground it in use cases
relevant to life sciences, from literature summarization and data extraction to
editorial tasks. We provide detailed recommendations for how prompts should and
shouldn't be structured, addressing common pitfalls including multi-turn
conversation degradation, hallucinations, and distinctions between reasoning
and non-reasoning models. We examine context window limitations, agentic tools
like Claude Code, while analyzing the effectiveness of Deep Research tools
across OpenAI, Google, Anthropic and Perplexity platforms, discussing current
limitations. We demonstrate how prompt engineering can augment rather than
replace existing established individual practices around data processing and
document editing. Our aim is to provide actionable guidance on core prompt
engineering principles, and to facilitate the transition from opportunistic
prompting to an effective, low-friction systematic practice that contributes to
higher quality research.

</details>


### [180] [Ko-PIQA: A Korean Physical Commonsense Reasoning Dataset with Cultural Context](https://arxiv.org/abs/2509.11303)
*Dasol Choi,Jungwhan Kim,Guijin Son*

Main category: cs.CL

TL;DR: 本文介绍了首个具有韩国文化背景的物理常识推理数据集Ko-PIQA，丰富了以英语为主的常识数据集格局。


<details>
  <summary>Details</summary>
Motivation: 现有物理常识推理数据集（如PIQA）局限于英语环境，缺乏文化多样性，难以覆盖其他文化特有的常识推理需求。

Method: 作者基于301万条网络爬取问题，采用多轮语言模型过滤，首次筛选出11553条PIQA风格问题，经GPT-4o加工和人工验证，挑选出441条高质量文化相关的韩语问答对。该数据集中近20%的问题包含如韩服、泡菜、专用冰箱等韩国文化元素。

Result: 作者评测了七种语言模型在该数据集上的表现，最优模型准确率83.22%，最差为59.86%，尤其在处理文化相关问题时表现欠佳，显示模型在跨文化常识推理上的瓶颈。

Conclusion: Ko-PIQA不仅可作为韩语常识推理模型的基准，也为包容不同文化的常识推理研究奠定了基础，并促进后续多元文化AI数据集的建设。

Abstract: Physical commonsense reasoning datasets like PIQA are predominantly
English-centric and lack cultural diversity. We introduce Ko-PIQA, a Korean
physical commonsense reasoning dataset that incorporates cultural context.
Starting from 3.01 million web-crawled questions, we employed a multi-stage
filtering approach using three language models to identify 11,553 PIQA-style
questions. Through GPT-4o refinement and human validation, we obtained 441
high-quality question-answer pairs. A key feature of Ko-PIQA is its cultural
grounding: 19.7\% of questions contain culturally specific elements like
traditional Korean foods (kimchi), clothing (hanbok), and specialized
appliances (kimchi refrigerators) that require culturally-aware reasoning
beyond direct translation. We evaluate seven language models on Ko-PIQA, with
the best model achieving 83.22\% accuracy while the weakest reaches only
59.86\%, demonstrating significant room for improvement. Models particularly
struggle with culturally specific scenarios, highlighting the importance of
culturally diverse datasets. Ko-PIQA serves as both a benchmark for Korean
language models and a foundation for more inclusive commonsense reasoning
research. The dataset and code will be publicly available.

</details>


### [181] [!MSA at AraHealthQA 2025 Shared Task: Enhancing LLM Performance for Arabic Clinical Question Answering through Prompt Engineering and Ensemble Learning](https://arxiv.org/abs/2509.11365)
*Mohamed Tarek,Seif Ahmed,Mohamed Basem*

Main category: cs.CL

TL;DR: 作者介绍了他们为AraHealthQA-2025比赛Track 2（阿拉伯语健康问答）设计的系统，在两个子任务均获得第二名。通过对Gemini 2.5 Flash模型的巧妙提示及处理，提升了阿拉伯临床问答的表现。


<details>
  <summary>Details</summary>
Motivation: 需要提升阿拉伯语医学领域的问答系统性能，尤其是在多项选择题和开放式问答场景下，帮助开发更准确、实用的医疗问答工具。

Method: 针对子任务1，采用Gemini 2.5 Flash大模型，结合few-shot提示、数据预处理和三种不同提示词集成，提高了标准、偏置及填空类问题的准确率；子任务2则使用统一提示，模拟阿拉伯医学专家身份，并引入示例和后处理，使模型能简明回答多种类型的开放式医疗问题。

Result: 系统在两项子任务（多项选择和开放式问答）中均获得了比赛第二名，证明方法的有效性，表现优异。

Conclusion: 通过对大模型的提示工程和不同任务的适配，显著提升了阿拉伯语医学问答模型的性能，为医疗场景下自动问答提供了新的技术参考。

Abstract: We present our systems for Track 2 (General Arabic Health QA, MedArabiQ) of
the AraHealthQA-2025 shared task, where our methodology secured 2nd place in
both Sub-Task 1 (multiple-choice question answering) and Sub-Task 2 (open-ended
question answering) in Arabic clinical contexts. For Sub-Task 1, we leverage
the Gemini 2.5 Flash model with few-shot prompting, dataset preprocessing, and
an ensemble of three prompt configurations to improve classification accuracy
on standard, biased, and fill-in-the-blank questions. For Sub-Task 2, we employ
a unified prompt with the same model, incorporating role-playing as an Arabic
medical expert, few-shot examples, and post-processing to generate concise
responses across fill-in-the-blank, patient-doctor Q&A, GEC, and paraphrased
variants.

</details>


### [182] [Transformer Enhanced Relation Classification: A Comparative Analysis of Contextuality, Data Efficiency and Sequence Complexity](https://arxiv.org/abs/2509.11374)
*Bowen Jing,Yang Cui,Tianpeng Huang*

Main category: cs.CL

TL;DR: 本文系统性对比了非Transformer与Transformer深度监督学习模型在关系抽取任务中的表现，发现Transformer模型显著优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 关系抽取(RE)作为信息抽取的核心技术，是将非结构化文本转化为结构化数据的基础。近年来，随着大型语言模型的发展，Transformer模型在自然语言处理的多个任务中表现突出。因此，作者动机在于系统性地比较非Transformer模型与Transformer模型在关系抽取上的性能差异。

Method: 作者选取了典型的非Transformer模型（PA-LSTM、C-GCN、AGGCN）及Transformer模型（BERT、RoBERTa、R-BERT），在TACRED、TACREV和RE-TACRED数据集上，通过微平均F1分数及不同场景（如句长变化、训练集比例变化）下的表现进行全面对比。

Result: Transformer模型在各项指标上显著优于非Transformer模型：Transformer模型获得了80%-90%的micro F1分数，而非Transformer模型仅为64%-67%。

Conclusion: Transformer架构在关系抽取任务中取得了显著提升，优于传统深度学习方法。此外，作者还评述了监督式关系分类的研究历程以及当下大型语言模型在关系抽取领域的作用和发展状况。

Abstract: In the era of large language model, relation extraction (RE) plays an
important role in information extraction through the transformation of
unstructured raw text into structured data (Wadhwa et al., 2023). In this
paper, we systematically compare the performance of deep supervised learning
approaches without transformers and those with transformers. We used a series
of non-transformer architectures such as PA-LSTM(Zhang et al., 2017),
C-GCN(Zhang et al., 2018), and AGGCN(attention guide GCN)(Guo et al., 2019),
and a series of transformer architectures such as BERT, RoBERTa, and R-BERT(Wu
and He, 2019). Our comparison included traditional metrics like micro F1, as
well as evaluations in different scenarios, varying sentence lengths, and
different percentages of the dataset for training. Our experiments were
conducted on TACRED, TACREV, and RE-TACRED. The results show that
transformer-based models outperform non-transformer models, achieving micro F1
scores of 80-90% compared to 64-67% for non-transformer models. Additionally,
we briefly review the research journey in supervised relation classification
and discuss the role and current status of large language models (LLMs) in
relation extraction.

</details>


### [183] [Continually Adding New Languages to Multilingual Language Models](https://arxiv.org/abs/2509.11414)
*Abraham Toluwase Owodunni,Sachin Kumar*

Main category: cs.CL

TL;DR: 该论文提出了一种新方法LayRA，可以在不访问原训练数据的情况下为多语言模型增添新语言，并有效避免遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 多语言模型通常只支持训练时参与的语言，若需添加新语言，往往需要从头训练，成本高且实际缺乏原始数据。现有方法继续预训练会导致灾难性遗忘，而常见的缓解策略需要原始数据，本场景下不可用。

Method: 提出Layer-Selective LoRA (LayRA)方法，将低秩适配器（LoRA）添加到模型的初始与末尾层，其余部分冻结，从而减少对原有能力的影响。基于模型分层语言处理的观察，LayRA选择性适配相关层以更好迁移新语言。

Result: 在将加利西亚语、斯瓦希里语和乌尔都语添加到已训练多语言模型的实验中，LayRA在保持已有语言能力和学习新语言上实现了最佳平衡，效果优于现有方法。进一步通过模型算术，还可不用目标语言指令微调数据下增强模型指令跟随能力。

Conclusion: LayRA能有效在无原始训练数据下扩展多语言模型新语言，兼顾新旧语言能力，为多语言模型的持续扩展提供了新途径。

Abstract: Multilingual language models are trained on a fixed set of languages, and to
support new languages, the models need to be retrained from scratch. This is an
expensive endeavor and is often infeasible, as model developers tend not to
release their pre-training data. Naive approaches, such as continued
pretraining, suffer from catastrophic forgetting; however, mitigation
strategies like experience replay cannot be applied due to the lack of original
pretraining data. In this work, we investigate the problem of continually
adding new languages to a multilingual model, assuming access to pretraining
data in only the target languages. We explore multiple approaches to address
this problem and propose Layer-Selective LoRA (LayRA), which adds Low-Rank
Adapters (LoRA) to selected initial and final layers while keeping the rest of
the model frozen. LayRA builds on two insights: (1) LoRA reduces forgetting,
and (2) multilingual models encode inputs in the source language in the initial
layers, reason in English in intermediate layers, and translate back to the
source language in final layers. We experiment with adding multiple
combinations of Galician, Swahili, and Urdu to pretrained language models and
evaluate each method on diverse multilingual tasks. We find that LayRA provides
the overall best tradeoff between preserving models' capabilities in previously
supported languages, while being competitive with existing approaches such as
LoRA in learning new languages. We also demonstrate that using model
arithmetic, the adapted models can be equipped with strong instruction
following abilities without access to any instruction tuning data in the target
languages.

</details>


### [184] [A Transformer-Based Cross-Platform Analysis of Public Discourse on the 15-Minute City Paradigm](https://arxiv.org/abs/2509.11443)
*Gaurab Chhetri,Darrell Anderson,Boniphace Kutela,Subasish Das*

Main category: cs.CL

TL;DR: 本研究首次在Twitter、Reddit和新闻媒体三大平台上，对15分钟城市概念的舆情进行多平台情感分析，并对压缩型Transformer模型进行评测。


<details>
  <summary>Details</summary>
Motivation: 公众舆论对城市规划理念（如15分钟城市）存在高度多样化，不同平台信息质量和表达方式差异很大，需系统方法进行跨平台、高效、可复现的情感分析和模型评测。

Method: 采用压缩型Transformer模型和Llama-3-8B进行情感标注，构建适用于长短文本的自动化分析流程，并在五个压缩模型（DistilRoBERTa、DistilBERT、MiniLM、ELECTRA、TinyBERT）间进行5折交叉验证，比较F1、AUC及训练效率。

Result: DistilRoBERTa在F1分数上最高（0.8292），TinyBERT效率最好，MiniLM跨平台一致性最优。发现新闻数据因类别不平衡带来虚高表现，Reddit因摘要损失致效果偏低，Twitter中等难度。小模型表现与大模型相当。

Conclusion: 压缩型模型足以胜任复杂的跨平台情感分析任务，大型模型并非必须。各平台存在使用权衡。为城市规划相关情感分析和大规模实际部署提出了可行思路。

Abstract: This study presents the first multi-platform sentiment analysis of public
opinion on the 15-minute city concept across Twitter, Reddit, and news media.
Using compressed transformer models and Llama-3-8B for annotation, we classify
sentiment across heterogeneous text domains. Our pipeline handles long-form and
short-form text, supports consistent annotation, and enables reproducible
evaluation. We benchmark five models (DistilRoBERTa, DistilBERT, MiniLM,
ELECTRA, TinyBERT) using stratified 5-fold cross-validation, reporting
F1-score, AUC, and training time. DistilRoBERTa achieved the highest F1
(0.8292), TinyBERT the best efficiency, and MiniLM the best cross-platform
consistency. Results show News data yields inflated performance due to class
imbalance, Reddit suffers from summarization loss, and Twitter offers moderate
challenge. Compressed models perform competitively, challenging assumptions
that larger models are necessary. We identify platform-specific trade-offs and
propose directions for scalable, real-world sentiment classification in urban
planning discourse.

</details>


### [185] [CognitiveSky: Scalable Sentiment and Narrative Analysis for Decentralized Social Media](https://arxiv.org/abs/2509.11444)
*Gaurab Chhetri,Anandi Dutta,Subasish Das*

Main category: cs.CL

TL;DR: 本文提出了CognitiveSky，这是一个用于去中心化社交平台Bluesky的开源、可扩展分析框架，可以实时分析情感、情绪和叙事。系统通过API获取数据，利用transformer模型对内容进行标注，并动态可视化社交讨论。该框架依赖免费基础设施，成本低，适用性广。


<details>
  <summary>Details</summary>
Motivation: 随着去中心化社交媒体（如Bluesky）的兴起，传统中心化平台的分析工具难以适用。现实需求呼唤能够在新生态中低成本、自动、高效地分析公共舆论，揭示实时情感和讨论走势。

Method: CognitiveSky通过Bluesky的API收集用户内容，采用transformer类模型进行自动情绪、情感和叙事标注，输出结构化结果，并实时驱动动态可视化仪表盘。系统采用开源和可拓展设计，运行于免费云资源上。

Result: CognitiveSky在监测心理健康相关讨论时表现良好，能够实时捕捉和展示情感、活动和话题的变化趋势。此外，其模块化架构为假消息监测、危机响应和社会情感分析等多领域赋能。

Conclusion: CognitiveSky将大语言模型与去中心化社交网络结合，为计算社会科学提供了透明、易用且低成本的分析工具，有助于理解和适应数字舆论格局的变革。

Abstract: The emergence of decentralized social media platforms presents new
opportunities and challenges for real-time analysis of public discourse. This
study introduces CognitiveSky, an open-source and scalable framework designed
for sentiment, emotion, and narrative analysis on Bluesky, a federated Twitter
or X.com alternative. By ingesting data through Bluesky's Application
Programming Interface (API), CognitiveSky applies transformer-based models to
annotate large-scale user-generated content and produces structured and
analyzable outputs. These summaries drive a dynamic dashboard that visualizes
evolving patterns in emotion, activity, and conversation topics. Built entirely
on free-tier infrastructure, CognitiveSky achieves both low operational cost
and high accessibility. While demonstrated here for monitoring mental health
discourse, its modular design enables applications across domains such as
disinformation detection, crisis response, and civic sentiment analysis. By
bridging large language models with decentralized networks, CognitiveSky offers
a transparent, extensible tool for computational social science in an era of
shifting digital ecosystems.

</details>


### [186] [CEMTM: Contextual Embedding-based Multimodal Topic Modeling](https://arxiv.org/abs/2509.11465)
*Amirhossein Abaskohi,Raymond Li,Chuyuan Li,Shafiq Joty,Giuseppe Carenini*

Main category: cs.CL

TL;DR: 本文提出了一种新的多模态主题模型CEMTM，能够对包含文本和图片的短文和长文进行一致且可解释的主题分析，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态文档（同时包含文本与图片）的主题建模方法在处理多图片文档、解释性、以及跨模态语义一致性方面存在不足，作者想要解决这些问题。

Method: 模型基于微调的大型视觉语言模型（LVLM），通过分布式注意力机制，动态加权每个token在主题推断中的贡献。创新性地设计了重构目标，以保证主题表征和文档嵌入的一致性，并能高效处理每篇文档的多幅图片。模型显式输出词-主题分布和文档-主题分布增强可解释性。

Result: 六个多模态基准数据集上的大量实验表明，CEMTM在主题推断性能上持续优于单模态和当前多模态基线，平均LLM分数达2.61，并在下游少样本检索及复杂领域（如科技论文）展示出优异效果。

Conclusion: CEMTM能高效、可解释地进行多模态文档的主题建模，语义一致性强，对多图片文档尤为适用，并具备良好的下游迁移能力。

Abstract: We introduce CEMTM, a context-enhanced multimodal topic model designed to
infer coherent and interpretable topic structures from both short and long
documents containing text and images. CEMTM builds on fine-tuned large vision
language models (LVLMs) to obtain contextualized embeddings, and employs a
distributional attention mechanism to weight token-level contributions to topic
inference. A reconstruction objective aligns topic-based representations with
the document embedding, encouraging semantic consistency across modalities.
Unlike existing approaches, CEMTM can process multiple images per document
without repeated encoding and maintains interpretability through explicit
word-topic and document-topic distributions. Extensive experiments on six
multimodal benchmarks show that CEMTM consistently outperforms unimodal and
multimodal baselines, achieving a remarkable average LLM score of 2.61. Further
analysis shows its effectiveness in downstream few-shot retrieval and its
ability to capture visually grounded semantics in complex domains such as
scientific articles.

</details>


### [187] [Improving LLMs' Learning for Coreference Resolution](https://arxiv.org/abs/2509.11466)
*Yujian Gan,Yuan Liang,Yanni Lin,Juntao Yu,Massimo Poesio*

Main category: cs.CL

TL;DR: 本文针对大语言模型在指代消解任务中的不足，提出了反向训练和迭代文档生成两种新技术，有效提升了模型性能并减少了幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在指代消解任务中易产生幻觉且表现不佳，本文旨在分析现有基于LLM方法（如QA模板和文档模板）的局限性，并提出改进方案。

Method: 提出了两项新技术：1）反向训练与联合推理，提升基于QA模板的方法表现；2）迭代文档生成，消除源文本中的幻觉并提高指代消解效果。并将其与现有方法整合，形成新的指代消解解决方案。

Result: 实验表明，反向训练提升了QA模板法的性能，迭代文档生成有效消除了幻觉并提升了指代消解任务的整体表现。

Conclusion: 综合新提出的方法，能够为基于大语言模型的指代消解任务提供更加有效且稳健的解决方案。

Abstract: Coreference Resolution (CR) is crucial for many NLP tasks, but existing LLMs
struggle with hallucination and under-performance. In this paper, we
investigate the limitations of existing LLM-based approaches to CR-specifically
the Question-Answering (QA) Template and Document Template methods and propose
two novel techniques: Reversed Training with Joint Inference and Iterative
Document Generation. Our experiments show that Reversed Training improves the
QA Template method, while Iterative Document Generation eliminates
hallucinations in the generated source text and boosts coreference resolution.
Integrating these methods and techniques offers an effective and robust
solution to LLM-based coreference resolution.

</details>


### [188] [ClaimIQ at CheckThat! 2025: Comparing Prompted and Fine-Tuned Language Models for Verifying Numerical Claims](https://arxiv.org/abs/2509.11492)
*Anirban Saha Anik,Md Fahimul Kabir Chowdhury,Andrew Wyckoff,Sagnik Ray Choudhury*

Main category: cs.CL

TL;DR: 本文分析了在CLEF 2025 CheckThat! Lab第3任务中，利用两种方法验证数字和时序性事实声明的系统，并强调了证据选择策略和模型泛化能力对于结果的影响。


<details>
  <summary>Details</summary>
Motivation: 面对通过事实核查自动识别虚假或误导性数字、时序信息的需求，现有自动化方法在证据整合和模型泛化方面仍有局限，作者因此探究更优模型和证据选择策略。

Method: 作者提出两种互补的方法：一是采用指令调优的大型语言模型进行零样本推理，二是利用LoRA参数高效微调进行有监督训练；同时研究了包括全文输入和BM25及MiniLM相关性排序的top-k句子筛选在内的证据选择策略。

Result: LLaMA通过LoRA微调后的模型在英语验证集上取得了优异表现，但在测试集上的成绩显著下滑，暴露出模型在泛化能力上的挑战。

Conclusion: 证据的粒度选择和模型针对具体任务的适配性是数值型事实核查任务获得稳健性能的关键，后续需进一步增强模型的泛化能力。

Abstract: This paper presents our system for Task 3 of the CLEF 2025 CheckThat! Lab,
which focuses on verifying numerical and temporal claims using retrieved
evidence. We explore two complementary approaches: zero-shot prompting with
instruction-tuned large language models (LLMs) and supervised fine-tuning using
parameter-efficient LoRA. To enhance evidence quality, we investigate several
selection strategies, including full-document input and top-k sentence
filtering using BM25 and MiniLM. Our best-performing model LLaMA fine-tuned
with LoRA achieves strong performance on the English validation set. However, a
notable drop in the test set highlights a generalization challenge. These
findings underscore the importance of evidence granularity and model adaptation
for robust numerical fact verification.

</details>


### [189] [AKCIT-FN at CheckThat! 2025: Switching Fine-Tuned SLMs and LLM Prompting for Multilingual Claim Normalization](https://arxiv.org/abs/2509.11496)
*Fabrycio Leite Nakano Almada,Kauan Divino Pouso Mariano,Maykon Adriell Dutra,Victor Emanuel da Silva Monteiro,Juliana Resplande Sant'Anna Gomes,Arlindo Rodrigues Galvão Filho,Anderson da Silva Soares*

Main category: cs.CL

TL;DR: 本文介绍了一种针对多语种社交媒体主张归一化任务的自动化方法，并在CLEF-2025 CheckThat! 赛事取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 在自动事实核查流程中，将社交媒体上的非正式主张转化为简洁、独立的规范陈述（即主张归一化）极为关键，特别是在多语言场景下的高效、准确归一化存在挑战性。

Method: 针对高资源（有标注数据）语言，采用微调的小型语言模型（SLM）；针对零样本（无标注数据）语言，采用大型语言模型（LLM）提示方法进行归一化。

Result: 系统在20种语言中有15种获得前3名，其中8种排名第二，且有多达5种零样本语言表现突出。特别是葡萄牙语（开发语言）METEOR分数达到0.5290并排名第三。

Conclusion: 提出的基于SLM和LLM的归一化方法在多语种（特别是低资源和零样本语言）场景下展现出较强的泛化能力，可为自动事实核查提供有效支持。

Abstract: Claim normalization, the transformation of informal social media posts into
concise, self-contained statements, is a crucial step in automated
fact-checking pipelines. This paper details our submission to the CLEF-2025
CheckThat! Task~2, which challenges systems to perform claim normalization
across twenty languages, divided into thirteen supervised (high-resource) and
seven zero-shot (no training data) tracks.
  Our approach, leveraging fine-tuned Small Language Models (SLMs) for
supervised languages and Large Language Model (LLM) prompting for zero-shot
scenarios, achieved podium positions (top three) in fifteen of the twenty
languages. Notably, this included second-place rankings in eight languages,
five of which were among the seven designated zero-shot languages, underscoring
the effectiveness of our LLM-based zero-shot strategy. For Portuguese, our
initial development language, our system achieved an average METEOR score of
0.5290, ranking third. All implementation artifacts, including inference,
training, evaluation scripts, and prompt configurations, are publicly available
at https://github.com/ju-resplande/checkthat2025_normalization.

</details>


### [190] [DeDisCo at the DISRPT 2025 Shared Task: A System for Discourse Relation Classification](https://arxiv.org/abs/2509.11498)
*Zhuoxuan Ju,Jingni Wu,Abhishek Purushothama,Amir Zeldes*

Main category: cs.CL

TL;DR: Georgetown大学提出了一种名为DeDisCo的系统，用于DISRPT 2025话语关系分类任务，尝试了mt5编码器和Qwen解码器两种方法，并通过自动翻译增强低资源语言数据，最终系统取得了71.28的宏准确率。


<details>
  <summary>Details</summary>
Motivation: 推动跨语言和低资源语言的话语关系自动分类，提升多语言任务表现，并尝试利用新模型与数据增强手段取得更好结果。

Method: 分别采用基于mt5的编码器方法和基于Qwen的解码器方法；通过自动翻译的方式为低资源语言扩充训练数据，结合了一些受以往竞赛成果启发的语言特征。

Result: 系统实现了71.28的宏准确率，并结合误差分析对结果进行了初步解释。

Conclusion: 采用多模型和数据增强策略的方法在多语言话语关系分类任务中取得较好成绩，为低资源语言处理提供了有益探索。

Abstract: This paper presents DeDisCo, Georgetown University's entry in the DISRPT 2025
shared task on discourse relation classification. We test two approaches, using
an mt5-based encoder and a decoder based approach using the openly available
Qwen model. We also experiment on training with augmented dataset for
low-resource languages using matched data translated automatically from
English, as well as using some additional linguistic features inspired by
entries in previous editions of the Shared Task. Our system achieves a
macro-accuracy score of 71.28, and we provide some interpretation and error
analysis for our results.

</details>


### [191] [Unsupervised Candidate Ranking for Lexical Substitution via Holistic Sentence Semantics](https://arxiv.org/abs/2509.11513)
*Zhongyang Hu,Naijie Gu,Xiangzhi Tao,Tianhui Gu,Yibing Zhou*

Main category: cs.CL

TL;DR: 该论文提出了两种用于词汇替换任务中候选词排名的方法，分别基于注意力权重和集成梯度，以更有效地对替换造成的语义变化进行建模。实验表明，这两种方法均能提升排名表现。


<details>
  <summary>Details</summary>
Motivation: 在词汇替换任务中，准确排名候选词对于语义保留和上下文契合性非常关键。但现有方法在双向建模目标词与上下文间的影响时存在局限，大多只考虑单向的变化，或过于依赖复杂的参数调优，难以细致刻画语义变化。

Method: 作者提出了两种测量替换影响的方法：一种利用注意力权重分析上下文与目标词之间的交互；另一种利用集成梯度方法，增强解释性，旨在衡量上下文词对目标词的影响，并结合原句与替换句的语义相似性来对候选词进行排名。

Result: 在LS07和SWORDS数据集上的实验显示，两种新方法都比以往方法在候选词排名任务上有更好的表现。

Conclusion: 基于注意力权重和集成梯度的建模方式可以更有效地表征候选词替换对整体句子语义的影响，从而提高候选词排名的准确性，对词汇替换任务具有推广意义。

Abstract: A key subtask in lexical substitution is ranking the given candidate words. A
common approach is to replace the target word with a candidate in the original
sentence and feed the modified sentence into a model to capture semantic
differences before and after substitution. However, effectively modeling the
bidirectional influence of candidate substitution on both the target word and
its context remains challenging. Existing methods often focus solely on
semantic changes at the target position or rely on parameter tuning over
multiple evaluation metrics, making it difficult to accurately characterize
semantic variation. To address this, we investigate two approaches: one based
on attention weights and another leveraging the more interpretable integrated
gradients method, both designed to measure the influence of context tokens on
the target token and to rank candidates by incorporating semantic similarity
between the original and substituted sentences. Experiments on the LS07 and
SWORDS datasets demonstrate that both approaches improve ranking performance.

</details>


### [192] [LVLMs are Bad at Overhearing Human Referential Communication](https://arxiv.org/abs/2509.11514)
*Zhengxiang Wang,Weiling Li,Panagiotis Kaliosis,Owen Rambow,Susan E. Brennan*

Main category: cs.CL

TL;DR: 本论文研究了当代七个主流大型视觉语言模型（LVLMs）在理解人类对话中创新性指代表达时的表现，结果显示这些模型在该任务上仍然具有较大挑战，且未表现出随着对话轮数增加性能提升的趋势。


<details>
  <summary>Details</summary>
Motivation: 人类在自然对话过程中常会合作创造新的指代表达并在后续重复使用，这对真实环境中的具身智能体理解和完成任务至关重要。为推动LVLMs在人机交互和多模态理解上的能力提升，作者希望评估现有模型对此类对话现象的理解能力。

Method: 作者利用一个由人类参与者进行协作对象匹配任务时自发产生的对话语料库，评测七个最先进的LVLMs作为“旁听者”时，理解复用型指代表达的能力。实验考查模型在多轮对话中，理解反复出现的指代表达的表现变化。

Result: 实验结果表明，当前LVLMs在理解和追踪指代表达的多轮协作任务上仍面临困难，即使不断旁听同一组参与者的对话，对模型表现的提升并不具有一致性。

Conclusion: 作者认为，现有LVLMs在自然对话指代表达的积累学习方面存在显著不足，并通过发布语料库和代码，推动相关领域的后续研究。

Abstract: During spontaneous conversations, speakers collaborate on novel referring
expressions, which they can then re-use in subsequent conversations.
Understanding such referring expressions is an important ability for an
embodied agent, so that it can carry out tasks in the real world. This requires
integrating and understanding language, vision, and conversational interaction.
We study the capabilities of seven state-of-the-art Large Vision Language
Models (LVLMs) as overhearers to a corpus of spontaneous conversations between
pairs of human discourse participants engaged in a collaborative
object-matching task. We find that such a task remains challenging for current
LVLMs and they all fail to show a consistent performance improvement as they
overhear more conversations from the same discourse participants repeating the
same task for multiple rounds. We release our corpus and code for
reproducibility and to facilitate future research.

</details>


### [193] [PeruMedQA: Benchmarking Large Language Models (LLMs) on Peruvian Medical Exams -- Dataset Construction and Evaluation](https://arxiv.org/abs/2509.11517)
*Rodrigo M. Carrillo-Larco,Jesus Lovón Melgarejo,Manuel Castillo-Cara,Gusseppe Bravo-Rocca*

Main category: cs.CL

TL;DR: 本研究评估了医学大模型（LLMs）在西班牙语及拉丁美洲实际医疗考试问答中的表现，并提出了专为秘鲁医学环境定制的问答数据集及模型微调策略。


<details>
  <summary>Details</summary>
Motivation: 虽然医学大模型已在医疗考试问答中展现出卓越能力，但其在西班牙语和拉美国家实际场景下的应用效果尚未明确，而这一点对推动拉美医疗AI应用普及至关重要。

Method: 研究者构建了包含8,380道秘鲁医师专科考试选择题的PeruMedQA数据集，涵盖12个医学领域。对多种医学LLM进行零样本任务测试，并采用PEFT和LoRA方法对medgemma-4b-it模型进行微调，再与原生LLM进行表现对比。2025年的题目作为测试集。

Result: medgemma-27b-text-it模型在多项测试中正确率超过90%，表现最佳。参数低于10亿的LLM正确率普遍低于60%。微调后的medgemma-4b-it在参数低于10亿的LLM中表现最好，并在多个考试中与参数更高的LLM（70B）表现接近。

Conclusion: 若在西班牙语及与秘鲁流行病学相似环境下进行医学AI应用和研究，建议优先采用medgemma-27b-text-it或微调的medgemma-4b-it模型。

Abstract: BACKGROUND: Medical large language models (LLMS) have demonstrated remarkable
performance in answering medical examinations. However, the extent to which
this high performance is transferable to medical questions in Spanish and from
a Latin American country remains unexplored. This knowledge is crucial as
LLM-based medical applications gain traction in Latin America. AIMS: to build a
dataset of questions from medical examinations taken by Peruvian physicians
pursuing specialty training; to fine-tune a LLM on this dataset; to evaluate
and compare the performance in terms of accuracy between vanilla LLMs and the
fine-tuned LLM. METHODS: We curated PeruMedQA, a multiple-choice
question-answering (MCQA) datasets containing 8,380 questions spanning 12
medical domains (2018-2025). We selected eight medical LLMs including
medgemma-4b-it and medgemma-27b-text-it, and developed zero-shot task-specific
prompts to answer the questions appropriately. We employed parameter-efficient
fine tuning (PEFT)and low-rant adaptation (LoRA) to fine-tune medgemma-4b-it
utilizing all questions except those from 2025 (test set). RESULTS:
medgemma-27b-text-it outperformed all other models, achieving a proportion of
correct answers exceeding 90% in several instances. LLMs with <10 billion
parameters exhibited <60% of correct answers, while some exams yielded results
<50%. The fine-tuned version of medgemma-4b-it emerged victorious agains all
LLMs with <10 billion parameters and rivaled a LLM with 70 billion parameters
across various examinations. CONCLUSIONS: For medical AI application and
research that require knowledge bases from Spanish-speaking countries and those
exhibiting similar epidemiological profiles to Peru's, interested parties
should utilize medgemma-27b-text-it or a fine-tuned version of medgemma-4b-it.

</details>


### [194] [On the Distinctive Co-occurrence Characteristics of Antonymy](https://arxiv.org/abs/2509.11534)
*Zhihan Cao,Hiroaki Yamada,Takenobu Tokunaga*

Main category: cs.CL

TL;DR: 本文研究了反义关系在文本中共现的特点，并与其他语义关系做比较，发现反义词具有独特的共现模式。


<details>
  <summary>Details</summary>
Motivation: 以往研究发现反义词在语料中经常共现，但尚不清楚这种现象是否区别于其他语义关系，因此需要系统比较。

Method: 作者选择反义关系和其他三种语义关系，跨词性地采用健壮的共现度量方法进行量化和对比分析。

Result: 研究发现，反义词对有三项显著特性：共现强度高、线性顺序有偏好且多集中于短距离内。

Conclusion: 反义关系在文本共现模式上表现出与其他语义关系的显著区别，为语义关系自动识别提供了新线索。

Abstract: Antonymy has long received particular attention in lexical semantics.
Previous studies have shown that antonym pairs frequently co-occur in text,
across genres and parts of speech, more often than would be expected by chance.
However, whether this co-occurrence pattern is distinctive of antonymy remains
unclear, due to a lack of comparison with other semantic relations. This work
fills the gap by comparing antonymy with three other relations across parts of
speech using robust co-occurrence metrics. We find that antonymy is distinctive
in three respects: antonym pairs co-occur with high strength, in a preferred
linear order, and within short spans. All results are available online.

</details>


### [195] [HARP: Hallucination Detection via Reasoning Subspace Projection](https://arxiv.org/abs/2509.11536)
*Junjie Hu,Gang Tu,ShengYu Cheng,Jinxin Li,Jinting Wang,Rui Chen,Zhilong Zhou,Dongbo Shan*

Main category: cs.CL

TL;DR: 本文提出了一种名为HARP的新方法，用于检测大语言模型（LLM）中的幻觉（不真实或不准确的输出），其核心思想是将LLM的隐藏状态分解为语义子空间和推理子空间，并通过特定数学方法投影到推理子空间来提升识别准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的幻觉检测方法难以有效区分语义与推理信息，且在应对模型扰动时鲁棒性不足，因此亟需一种既能更好揭示模型内部推理过程、又具有更高鲁棒性的新检测方案。

Method: HARP通过理论和实验论证，认为LLM的隐藏状态空间可以分解为语义与推理两个子空间，并证明通过Unembedding层参数的奇异值分解（SVD）可以获得这两个子空间的基向量。具体做法是将隐藏状态投影到推理子空间，所得投影向量作为检测幻觉的特征输入。

Result: HARP方法大幅度降低了特征空间的维度（降至原始的约5%），有效过滤噪声，并在多个数据集上实现了最优幻觉检测性能——例如在TriviaQA数据集上AUROC达到92.8%，超越原SOTA方法7.5个百分点。

Conclusion: HARP创新性地利用推理子空间信息，实现了更高效、鲁棒的LLM幻觉检测，为未来提升大模型可靠性、解释性和安全性提供了新的方向和思路。

Abstract: Hallucinations in Large Language Models (LLMs) pose a major barrier to their
reliable use in critical decision-making. Although existing hallucination
detection methods have improved accuracy, they still struggle with
disentangling semantic and reasoning information and maintaining robustness. To
address these challenges, we propose HARP (Hallucination detection via
reasoning subspace projection), a novel hallucination detection framework. HARP
establishes that the hidden state space of LLMs can be decomposed into a direct
sum of a semantic subspace and a reasoning subspace, where the former encodes
linguistic expression and the latter captures internal reasoning processes.
Moreover, we demonstrate that the Unembedding layer can disentangle these
subspaces, and by applying Singular Value Decomposition (SVD) to its
parameters, the basis vectors spanning the semantic and reasoning subspaces are
obtained. Finally, HARP projects hidden states onto the basis vectors of the
reasoning subspace, and the resulting projections are then used as input
features for hallucination detection in LLMs. By using these projections, HARP
reduces the dimension of the feature to approximately 5% of the original,
filters out most noise, and achieves enhanced robustness. Experiments across
multiple datasets show that HARP achieves state-of-the-art hallucination
detection performance; in particular, it achieves an AUROC of 92.8% on
TriviaQA, outperforming the previous best method by 7.5%.

</details>


### [196] [HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with Hierarchical Chunking](https://arxiv.org/abs/2509.11552)
*Wensheng Lu,Keyu Chen,Ruizhi Qiao,Xing Sun*

Main category: cs.CL

TL;DR: 本文针对RAG系统中文档切分环节缺乏有效评估工具的问题，提出了新评测基准HiCBench和高效切分框架HiChunk，有效提升了RAG系统的检索与生成效果。


<details>
  <summary>Details</summary>
Motivation: RAG系统常需对长文档进行切分供检索，但现有评测基准无法很好地衡量切分的实际效果，尤其存在证据稀疏问题，制约了切分算法和RAG总体性能的提升。作者希望提供更细致、全面的评估手段以促进相关研究。

Method: 作者首先分析现有基准的不足，并提出HiCBench，包含多层级人工切分标注、合成高密度证据的QA对及其关联证据来源。同时提出HiChunk框架，利用微调LLM进行多层级文档结构化，并结合Auto-Merge算法优化检索质量。通过实验全面评估不同切分方法对RAG系统的影响。

Result: 实验显示，HiCBench能有效区分和评估不同文档切分方法在RAG流程中的实际表现。HiChunk框架在时间消耗可控前提下实现更优的切分效果，从而提升RAG系统的综合性能。

Conclusion: HiCBench弥补了现有评测基准对文档切分评价的空白，为RAG系统开发提供了有效工具。HiChunk证明了高质量文档切分对提升RAG效果的实际价值。

Abstract: Retrieval-Augmented Generation (RAG) enhances the response capabilities of
language models by integrating external knowledge sources. However, document
chunking as an important part of RAG system often lacks effective evaluation
tools. This paper first analyzes why existing RAG evaluation benchmarks are
inadequate for assessing document chunking quality, specifically due to
evidence sparsity. Based on this conclusion, we propose HiCBench, which
includes manually annotated multi-level document chunking points, synthesized
evidence-dense quetion answer(QA) pairs, and their corresponding evidence
sources. Additionally, we introduce the HiChunk framework, a multi-level
document structuring framework based on fine-tuned LLMs, combined with the
Auto-Merge retrieval algorithm to improve retrieval quality. Experiments
demonstrate that HiCBench effectively evaluates the impact of different
chunking methods across the entire RAG pipeline. Moreover, HiChunk achieves
better chunking quality within reasonable time consumption, thereby enhancing
the overall performance of RAG systems.

</details>


### [197] [D$^2$HScore: Reasoning-Aware Hallucination Detection via Semantic Breadth and Depth Analysis in LLMs](https://arxiv.org/abs/2509.11569)
*Yue Ding,Xiaofang Zhu,Tianze Xia,Junfei Wu,Xinlong Chen,Qiang Liu,Liang Wang*

Main category: cs.CL

TL;DR: 提出了一种新方法D^2HScore，无需训练和标注，仅通过分析大模型各层表示的分散性和漂移性，即可检测LLM生成内容中的“幻觉”。


<details>
  <summary>Details</summary>
Motivation: 虽然大模型表现优异，但生成非真实内容（幻觉）的问题仍限制其实际应用，尤其在高风险领域。如何有效检测、遏制幻觉成为当前亟需解决的重要难题。

Method: 作者从模型结构和生成动态出发，提出将幻觉检测分解为：层内语义分散性（Intra-Layer Dispersion）和跨层语义漂移（Inter-Layer Drift）两维。方法基于LLM多层结构和自回归特性，借助注意力信号筛选代表性token，无需额外训练或标签即可实施。

Result: 在五个开源LLM和五个常见基准数据上进行的实验显示，D^2HScore在训练无关的基线方案中表现最好，检测能力优异。

Conclusion: D^2HScore是一种轻量、可解释的LLM幻觉检测方法，为后续研究和实际应用中提升生成内容可靠性提供了新工具。

Abstract: Although large Language Models (LLMs) have achieved remarkable success, their
practical application is often hindered by the generation of non-factual
content, which is called "hallucination". Ensuring the reliability of LLMs'
outputs is a critical challenge, particularly in high-stakes domains such as
finance, security, and healthcare. In this work, we revisit hallucination
detection from the perspective of model architecture and generation dynamics.
Leveraging the multi-layer structure and autoregressive decoding process of
LLMs, we decompose hallucination signals into two complementary dimensions: the
semantic breadth of token representations within each layer, and the semantic
depth of core concepts as they evolve across layers. Based on this insight, we
propose \textbf{D$^2$HScore (Dispersion and Drift-based Hallucination Score)},
a training-free and label-free framework that jointly measures: (1)
\textbf{Intra-Layer Dispersion}, which quantifies the semantic diversity of
token representations within each layer; and (2) \textbf{Inter-Layer Drift},
which tracks the progressive transformation of key token representations across
layers. To ensure drift reflects the evolution of meaningful semantics rather
than noisy or redundant tokens, we guide token selection using attention
signals. By capturing both the horizontal and vertical dynamics of
representation during inference, D$^2$HScore provides an interpretable and
lightweight proxy for hallucination detection. Extensive experiments across
five open-source LLMs and five widely used benchmarks demonstrate that
D$^2$HScore consistently outperforms existing training-free baselines.

</details>


### [198] [Bhaasha, Bhasa, Zaban: A Survey for Low-Resourced Languages in South Asia -- Current Stage and Challenges](https://arxiv.org/abs/2509.11570)
*Sampoorna Poria,Xiaolei Huang*

Main category: cs.CL

TL;DR: 本文综述了南亚低资源语言在NLP（自然语言处理）领域，尤其是基于transformer的大型语言模型上的最新进展、存在的问题与未来方向。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型极大推动了英文NLP的发展，但南亚地区的低资源语言在模型开发与评估方面被严重忽视，因此迫切需要系统梳理现状和挑战，为后续模型开发和公平资源分配提供参考。

Method: 作者系统梳理了自2020年以来与南亚语言相关的transformer类语言模型（如BERT、T5和GPT）的文献，重点从数据、模型和任务三个层面，分析当前可用的数据源、微调策略、实际应用场景等，并总结存在的技术瓶颈。

Result: 分析发现，南亚语言在健康等关键领域的数据缺失严重，代码混合现象普遍，且缺乏统一、标准的评测基准，对模型进步造成限制。同时已公布了一份完整的南亚语言资源清单供研究者参考。

Conclusion: 作者呼吁NLP社区加强南亚语言相关数据的采集与整理、制定更适合该区域语言文化特征的统一评测体系，并推动这些语言在NLP模型中的公平代表，从而促进多语言AI的均衡发展。

Abstract: Rapid developments of large language models have revolutionized many NLP
tasks for English data. Unfortunately, the models and their evaluations for
low-resource languages are being overlooked, especially for languages in South
Asia. Although there are more than 650 languages in South Asia, many of them
either have very limited computational resources or are missing from existing
language models. Thus, a concrete question to be answered is: Can we assess the
current stage and challenges to inform our NLP community and facilitate model
developments for South Asian languages? In this survey, we have comprehensively
examined current efforts and challenges of NLP models for South Asian languages
by retrieving studies since 2020, with a focus on transformer-based models,
such as BERT, T5, & GPT. We present advances and gaps across 3 essential
aspects: data, models, & tasks, such as available data sources, fine-tuning
strategies, & domain applications. Our findings highlight substantial issues,
including missing data in critical domains (e.g., health), code-mixing, and
lack of standardized evaluation benchmarks. Our survey aims to raise awareness
within the NLP community for more targeted data curation, unify benchmarks
tailored to cultural and linguistic nuances of South Asia, and encourage an
equitable representation of South Asian languages. The complete list of
resources is available at: https://github.com/trust-nlp/LM4SouthAsia-Survey.

</details>


### [199] [Analyzing Information-Seeking Behaviors in a Hakka AI Chatbot: A Cognitive-Pragmatic Study](https://arxiv.org/abs/2509.11591)
*Chu-Hsuan Lee,Chen-Chi Chang,Hung-Shin Lee,Yun-Hsiang Hsu,Ching-Yuan Chen*

Main category: cs.CL

TL;DR: 本研究通过分析Hakka（客家话）AI聊天机器人TALKA中的用户行为，探索生成式AI在濒危语言保护和学习中的作用。结果显示，AI能够有效支持语言学习和文化身份的连接。


<details>
  <summary>Details</summary>
Motivation: 随着濒危语言持续消失，单靠传统教学难以有效保护，这促使研究者探索如何将AI技术与文化导向的教学结合，用于濒危语言的保存和教育。

Method: 采用布鲁姆认知分类学及对话行为分类的双重框架，分析了TALKA聊天机器人中的7,077条用户话语，并根据六种认知层次和十一种对话行为进行标注和归类，进一步对语用功能进行解析。

Result: 用户在TALKA上的互动涵盖了信息查询、翻译请求、文化问题和创造性表达等多种语言功能。不同对话行为（如反馈、指令、社交问候等）与认知意图相对应。分析结果显示，生成式AI促进了学习者的认知发展和社交文化归属感，并增强了其自信心表达。

Conclusion: 生成式AI聊天机器人，如TALKA，在促进濒危语言学习者的认知成长、语用互动与文化身份认同方面表现出显著潜力。该研究为AI助力的语言保护与教学提供了新的实证见解。

Abstract: With many endangered languages at risk of disappearing, efforts to preserve
them now rely more than ever on using technology alongside culturally informed
teaching strategies. This study examines user behaviors in TALKA, a generative
AI-powered chatbot designed for Hakka language engagement, by employing a
dual-layered analytical framework grounded in Bloom's Taxonomy of cognitive
processes and dialogue act categorization. We analyzed 7,077 user utterances,
each carefully annotated according to six cognitive levels and eleven dialogue
act types. These included a variety of functions, such as asking for
information, requesting translations, making cultural inquiries, and using
language creatively. Pragmatic classifications further highlight how different
types of dialogue acts--such as feedback, control commands, and social
greetings--align with specific cognitive intentions. The results suggest that
generative AI chatbots can support language learning in meaningful
ways--especially when they are designed with an understanding of how users
think and communicate. They may also help learners express themselves more
confidently and connect with their cultural identity. The TALKA case provides
empirical insights into how AI-mediated dialogue facilitates cognitive
development in low-resource language learners, as well as pragmatic negotiation
and socio-cultural affiliation. By focusing on AI-assisted language learning,
this study offers new insights into how technology can support language
preservation and educational practice.

</details>


### [200] [Dynamic Span Interaction and Graph-Aware Memory for Entity-Level Sentiment Classification](https://arxiv.org/abs/2509.11604)
*Md. Mithun Hossain,Sanjara,Md. Shakil Hossain,Sudipto Chaki*

Main category: cs.CL

TL;DR: 本文提出了一种新的实体级情感分类框架SpanEIT，通过动态片段交互和图注意力机制，有效提升了实体与情感表达之间关系建模和跨文档一致性，实验结果表现优越。


<details>
  <summary>Details</summary>
Motivation: 实体级情感分类任务面临实体与情感表达关系复杂、跨句依赖、共指消解以及否定、歧义等语言现象带来的挑战，现有方法难以充分解决这些问题，尤其是在实际噪声较大的语料中。

Method: 作者提出SpanEIT框架，采用基于片段的实体和情感短语表示，利用双向注意力捕捉细粒度交互，并通过图注意力网络建模句法和共现关系。此外，设计了共指感知记忆模块以保证同一实体多次出现时的情感一致性。

Result: 在FSAD、BARU和IMDB等数据集上，SpanEIT在准确率和F1分数上均优于现有的transformer和混合模型基线。消融实验和可解释性分析也验证了各模块的有效性。

Conclusion: SpanEIT显著提升了实体级情感分类的效果，能更好地应对复杂语言现象和实际应用中的挑战，为社交媒体监测、客户反馈分析等细粒度情感分析提供有力工具。

Abstract: Entity-level sentiment classification involves identifying the sentiment
polarity linked to specific entities within text. This task poses several
challenges: effectively modeling the subtle and complex interactions between
entities and their surrounding sentiment expressions; capturing dependencies
that may span across sentences; and ensuring consistent sentiment predictions
for multiple mentions of the same entity through coreference resolution.
Additionally, linguistic phenomena such as negation, ambiguity, and overlapping
opinions further complicate the analysis. These complexities make entity-level
sentiment classification a difficult problem, especially in real-world, noisy
textual data. To address these issues, we propose SpanEIT, a novel framework
integrating dynamic span interaction and graph-aware memory mechanisms for
enhanced entity-sentiment relational modeling. SpanEIT builds span-based
representations for entities and candidate sentiment phrases, employs
bidirectional attention for fine-grained interactions, and uses a graph
attention network to capture syntactic and co-occurrence relations. A
coreference-aware memory module ensures entity-level consistency across
documents. Experiments on FSAD, BARU, and IMDB datasets show SpanEIT
outperforms state-of-the-art transformer and hybrid baselines in accuracy and
F1 scores. Ablation and interpretability analyses validate the effectiveness of
our approach, underscoring its potential for fine-grained sentiment analysis in
applications like social media monitoring and customer feedback analysis.

</details>


### [201] [HalluDetect: Detecting, Mitigating, and Benchmarking Hallucinations in Conversational Systems](https://arxiv.org/abs/2509.11619)
*Spandan Anaokar,Shrey Ganatra,Harshvivek Kashid,Swapnil Bhattacharyya,Shruti Nair,Reshma Sekhar,Siddharth Manohar,Rahul Hemrajani,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 本文针对消费类申诉聊天机器人中的幻觉问题，提出了Halludetect检测系统，并比较了多种架构，显著提升了鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 大模型被广泛应用于工业界，特别是自动化客服等领域，但在关键应用中因产生幻觉影响其可靠性。为提升LLM在关键场景（消费类申诉）中的可信度，需要有专门的幻觉检测与缓解方案。

Method: 提出了基于LLaMA 3.1 8B Instruct的小型模型开发的Halludetect幻觉检测系统，并将其与多种聊天机器人架构进行对比。用F1分数评价检测能力，并测试不同架构（如AgentBot）在维持高准确率同时减少幻觉次数的表现。

Result: Halludetect系统F1分数为69%，较基线检测器提升了25.44%。在五种聊天机器人架构对比中，AgentBot每轮平均幻觉最低（0.4159次），token准确率最高（96.13%），综合效果最佳。

Conclusion: 优化的推理架构和检测系统能够在保持高准确率的同时，有效降低大模型幻觉问题，为高风险领域中的大模型助手提供了可扩展的缓解框架。方法虽以消费者权益为例，但能泛化到其他领域。代码和数据集将开源以推动研究。

Abstract: Large Language Models (LLMs) are widely used in industry but remain prone to
hallucinations, limiting their reliability in critical applications. This work
addresses hallucination reduction in consumer grievance chatbots built using
LLaMA 3.1 8B Instruct, a compact model frequently used in industry. We develop
HalluDetect, an LLM-based hallucination detection system that achieves an F1
score of 69% outperforming baseline detectors by 25.44%. Benchmarking five
chatbot architectures, we find that out of them, AgentBot minimizes
hallucinations to 0.4159 per turn while maintaining the highest token accuracy
(96.13%), making it the most effective mitigation strategy. Our findings
provide a scalable framework for hallucination mitigation, demonstrating that
optimized inference strategies can significantly improve factual accuracy.
While applied to consumer law, our approach generalizes to other high-risk
domains, enhancing trust in LLM-driven assistants. We will release the code and
dataset

</details>


### [202] [AesBiasBench: Evaluating Bias and Alignment in Multimodal Language Models for Personalized Image Aesthetic Assessment](https://arxiv.org/abs/2509.11620)
*Kun Li,Lai-Man Po,Hongzheng Yang,Xuyuan Xu,Kangcheng Liu,Yuzhi Zhao*

Main category: cs.CL

TL;DR: 本文提出AesBiasBench基准，用于系统评测多模态大模型（MLLMs）在个性化图像审美评估（PIAA）中的群体偏见与人类偏好对齐性。评测19种模型发现，小模型偏见强，大模型与人类偏好更一致，且加入身份信息会加剧偏见。


<details>
  <summary>Details</summary>
Motivation: 随着MLLMs在图像审美评估中的广泛应用，其预测结果逐渐取代专家评估，但模型存在性别、年龄、教育等人口统计相关的微妙偏见。本研究旨在系统性发现与量化这些偏见，并推动更公正的评估体系。

Method: 作者设计AesBiasBench基准，从刻板印象偏见和模型输出与真实人类审美偏好一致性两个维度评测MLLMs，覆盖审美感知、评价、共情三类任务，并设计IFD、NRD、AAS等多指标综合评估偏见与对齐性，对19个代表性MLLMs进行实验对比分析。

Result: 评测显示：体量较小的模型存在较明显的刻板印象偏见，而大模型与真实人类偏好更为一致。此外，输入身份信息常常加剧偏见（尤其是在情感判断任务中）。

Conclusion: MLLMs在主观性视觉-语言任务中容易受到人口统计特征影响，亟需身份感知型的评测框架来量化和改善相关偏见，促进模型朝公平和真实性方向发展。

Abstract: Multimodal Large Language Models (MLLMs) are increasingly applied in
Personalized Image Aesthetic Assessment (PIAA) as a scalable alternative to
expert evaluations. However, their predictions may reflect subtle biases
influenced by demographic factors such as gender, age, and education. In this
work, we propose AesBiasBench, a benchmark designed to evaluate MLLMs along two
complementary dimensions: (1) stereotype bias, quantified by measuring
variations in aesthetic evaluations across demographic groups; and (2)
alignment between model outputs and genuine human aesthetic preferences. Our
benchmark covers three subtasks (Aesthetic Perception, Assessment, Empathy) and
introduces structured metrics (IFD, NRD, AAS) to assess both bias and
alignment. We evaluate 19 MLLMs, including proprietary models (e.g., GPT-4o,
Claude-3.5-Sonnet) and open-source models (e.g., InternVL-2.5, Qwen2.5-VL).
Results indicate that smaller models exhibit stronger stereotype biases,
whereas larger models align more closely with human preferences. Incorporating
identity information often exacerbates bias, particularly in emotional
judgments. These findings underscore the importance of identity-aware
evaluation frameworks in subjective vision-language tasks.

</details>


### [203] [EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI](https://arxiv.org/abs/2509.11648)
*Sai Kartheek Reddy Kasu*

Main category: cs.CL

TL;DR: 本文提出了一个名为EthicsMH的道德推理数据集，专门用于评估大模型在精神健康领域中面临伦理难题时的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在精神健康等敏感领域应用，涉及伦理推理和公平性，但现有评测基准无法反映精神健康实践中常见的复杂伦理困境，如保密性、自主性与偏见等问题。因此，需要一个更贴合实际的评测数据集。

Method: 作者构建了EthicsMH数据集，包含125个涉及精神健康伦理困境的场景。每个场景都设有多种决策选项、专家推理依据、期望模型行为、现实影响分析及多方利益相关者视角，以便多角度评估AI的决策能力和解释能力。

Result: EthicsMH数据集虽规模有限，是利用AI模型协助生成，但初步展示了如何用结构化方式评估AI在伦理敏感情境下的推理、决策准确性和规范一致性。

Conclusion: EthicsMH为精神健康与AI伦理结合提供了基础框架，有助于推动社区和专家持续丰富数据集，促进AI系统更负责任地处理社会敏感决策，为后续相关研究打下基础。

Abstract: The deployment of large language models (LLMs) in mental health and other
sensitive domains raises urgent questions about ethical reasoning, fairness,
and responsible alignment. Yet, existing benchmarks for moral and clinical
decision-making do not adequately capture the unique ethical dilemmas
encountered in mental health practice, where confidentiality, autonomy,
beneficence, and bias frequently intersect. To address this gap, we introduce
Ethical Reasoning in Mental Health (EthicsMH), a pilot dataset of 125 scenarios
designed to evaluate how AI systems navigate ethically charged situations in
therapeutic and psychiatric contexts. Each scenario is enriched with structured
fields, including multiple decision options, expert-aligned reasoning, expected
model behavior, real-world impact, and multi-stakeholder viewpoints. This
structure enables evaluation not only of decision accuracy but also of
explanation quality and alignment with professional norms. Although modest in
scale and developed with model-assisted generation, EthicsMH establishes a task
framework that bridges AI ethics and mental health decision-making. By
releasing this dataset, we aim to provide a seed resource that can be expanded
through community and expert contributions, fostering the development of AI
systems capable of responsibly handling some of society's most delicate
decisions.

</details>


### [204] [A Dynamic Knowledge Update-Driven Model with Large Language Models for Fake News Detection](https://arxiv.org/abs/2509.11687)
*Di Jin,Jun Yang,Xiaobao Wang,Junwei Zhang,Shuqi Li,Dongxiao He*

Main category: cs.CL

TL;DR: 本文提出了一种动态知识更新驱动的虚假新闻检测模型DYNAMO，可实现知识的持续更新，提高新闻真伪识别能力。


<details>
  <summary>Details</summary>
Motivation: 互联网和社交媒体信息量激增，新闻事件突发且变化快，新闻真实性标签可能随事件发展而改变，因此需要检测方法能获取最新知识并辨别新闻真伪。

Method: 提出DYNAMO模型，结合知识图谱和大语言模型，实现知识的不断更新和新闻真伪判定。具体做法包括：1）构建新闻领域知识图谱；2）用蒙特卡洛树搜索分解和逐步验证复杂新闻；3）从经过验证的新闻文本和推理路径中提取与更新新知识。

Result: 实验在两个真实数据集上进行，DYNAMO取得了最佳性能，表明其在虚假新闻检测任务中优于现有方法。

Conclusion: DYNAMO通过动态知识更新和深度语义挖掘，有效提升了虚假新闻检测的准确性和新知识真实性验证能力，对应对新闻事实动态变化场景具有现实意义。

Abstract: As the Internet and social media evolve rapidly, distinguishing credible news
from a vast amount of complex information poses a significant challenge. Due to
the suddenness and instability of news events, the authenticity labels of news
can potentially shift as events develop, making it crucial for fake news
detection to obtain the latest event updates. Existing methods employ
retrieval-augmented generation to fill knowledge gaps, but they suffer from
issues such as insufficient credibility of retrieved content and interference
from noisy information. We propose a dynamic knowledge update-driven model for
fake news detection (DYNAMO), which leverages knowledge graphs to achieve
continuous updating of new knowledge and integrates with large language models
to fulfill dual functions: news authenticity detection and verification of new
knowledge correctness, solving the two key problems of ensuring the
authenticity of new knowledge and deeply mining news semantics. Specifically,
we first construct a news-domain-specific knowledge graph. Then, we use Monte
Carlo Tree Search to decompose complex news and verify them step by step.
Finally, we extract and update new knowledge from verified real news texts and
reasoning paths. Experimental results demonstrate that DYNAMO achieves the best
performance on two real-world datasets.

</details>


### [205] [CoachMe: Decoding Sport Elements with a Reference-Based Coaching Instruction Generation Model](https://arxiv.org/abs/2509.11698)
*Wei-Hsin Yeh,Yu-An Su,Chih-Ning Chen,Yi-Hsueh Lin,Calvin Ku,Wen-Hsin Chiu,Min-Chun Hu,Lun-Wei Ku*

Main category: cs.CL

TL;DR: CoachMe是一个基于参考动作的模型，能够对比学习者与参考动作的差异，针对具体运动场景给出详细、专业的动作指导，比现有大模型效果更优。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态模型已经在运动动作理解上取得进展，但针对强领域特性和定制化反馈的运动指导仍然很难。目前的模型往往缺乏专业性与有效的纠错建议，实际指导有限。

Method: 提出CoachMe模型，通过对比学习者和参考动作在时序和物理层面上的差异，结合领域知识学习，模拟教练的思考过程，能够有效识别并纠正动作错误；模型可以从通用动作知识迁移至具体运动（如滑冰和拳击），实现小样本学习下的高效指导。

Result: CoachMe在花样滑冰和拳击动作指导的G-Eval评测中，分别比GPT-4o提升了31.6%和58.3%；生成反馈内容比其他模型更具针对性和实用价值，能具体说明错误及改善方法。

Conclusion: CoachMe能够生成真实教练风格、且细致解释纠错与提升方法的运动指导，优化了多模态大模型在专业竞技运动领域的实用性。

Abstract: Motion instruction is a crucial task that helps athletes refine their
technique by analyzing movements and providing corrective guidance. Although
recent advances in multimodal models have improved motion understanding,
generating precise and sport-specific instruction remains challenging due to
the highly domain-specific nature of sports and the need for informative
guidance. We propose CoachMe, a reference-based model that analyzes the
differences between a learner's motion and a reference under temporal and
physical aspects. This approach enables both domain-knowledge learning and the
acquisition of a coach-like thinking process that identifies movement errors
effectively and provides feedback to explain how to improve. In this paper, we
illustrate how CoachMe adapts well to specific sports such as skating and
boxing by learning from general movements and then leveraging limited data.
Experiments show that CoachMe provides high-quality instructions instead of
directions merely in the tone of a coach but without critical information.
CoachMe outperforms GPT-4o by 31.6% in G-Eval on figure skating and by 58.3% on
boxing. Analysis further confirms that it elaborates on errors and their
corresponding improvement methods in the generated instructions. You can find
CoachMe here: https://motionxperts.github.io/

</details>


### [206] [Room acoustics affect communicative success in hybrid meeting spaces: a pilot study](https://arxiv.org/abs/2509.11709)
*Robert Einig,Stefan Janscha,Jonas Schuster,Julian Koch,Martin Hagmueller,Barbara Schuppler*

Main category: cs.CL

TL;DR: 本研究在格拉茨工业大学的一间研讨室，测试了改善房间声学设计后对混合会议沟通效果的影响。结果显示，空间声学优化有助于提升沟通成功率。


<details>
  <summary>Details</summary>
Motivation: 疫情后，越来越多的会议空间用于混合会议，但常忽视了声学设计，容易导致理解障碍和认知疲劳。因此需要研究声学改善的实际效果。

Method: 分别在声学改善前后，对两个小组在同一房间的混合会议表现进行录音和比较，评估空间声学干预对沟通效果的影响。

Result: 尽管样本量小未达统计显著性，但结果指向声学优化提高了混合会议的沟通成功率。

Conclusion: 改善研讨室声学设计有助于混合会议的有效交流，建议未来会议空间设计应更重视声学因素。

Abstract: Since the COVID-19 pandemic in 2020, universities and companies have
increasingly integrated hybrid features into their meeting spaces, or even
created dedicated rooms for this purpose. While the importance of a fast and
stable internet connection is often prioritized, the acoustic design of seminar
rooms is frequently overlooked. Poor acoustics, particularly excessive
reverberation, can lead to issues such as misunderstandings, reduced speech
intelligibility or cognitive and vocal fatigue. This pilot study investigates
whether room acoustic interventions in a seminar room at Graz University of
Technology support better communication in hybrid meetings. For this purpose,
we recorded two groups of persons twice, once before and once after improving
the acoustics of the room. Our findings -- despite not reaching statistical
significance due to the small sample size - indicate clearly that our spatial
interventions improve communicative success in hybrid meetings. To make the
paper accessible also for readers from the speech communication community, we
explain room acoustics background, relevant for the interpretation of our
results.

</details>


### [207] [An Agentic Toolkit for Adaptive Information Extraction from Regulatory Documents](https://arxiv.org/abs/2509.11773)
*Gaye Colakoglu,Gürkan Solmaz,Jonathan Fürst*

Main category: cs.CL

TL;DR: 本文提出了一种用于从欧盟认证建筑产品性能声明（DoP）文档中自动、稳健提取结构化信息的智能代理系统，能够适应文档格式和语言的多样性。


<details>
  <summary>Details</summary>
Motivation: 欧盟法规要求的DoP文档在内容、格式和语言上极为多样，这导致基于自动化的关键信息抽取和问答任务面临很大挑战。现有方法很难适应这种多样性，易出现错误或幻觉。

Method: 作者设计了一个领域特定的状态智能代理系统，采用规划-执行-响应三段式架构。系统可推断用户意图、自动判别文档类型，并动态调用工具，实现过程可溯、不易陷入误用或循环。

Result: 在自建高质量DoP数据集上的评测表明，该系统对不同格式和语言的文档具有更好的稳健性，提升了信息抽取效果。

Conclusion: 该方法为合规性监管场景下自动化结构化数据抽取提供了可扩展、可靠的技术方案，对于应对文档多样性极具价值。

Abstract: Declaration of Performance (DoP) documents, mandated by EU regulation,
certify the performance of construction products. While some of their content
is standardized, DoPs vary widely in layout, language, schema, and format,
posing challenges for automated key-value pair extraction (KVP) and question
answering (QA). Existing static or LLM-only IE pipelines often hallucinate and
fail to adapt to this structural diversity. Our domain-specific, stateful
agentic system addresses these challenges through a planner-executor-responder
architecture. The system infers user intent, detects document modality, and
orchestrates tools dynamically for robust, traceable reasoning while avoiding
tool misuse or execution loops. Evaluation on a curated DoP dataset
demonstrates improved robustness across formats and languages, offering a
scalable solution for structured data extraction in regulated workflows.

</details>


### [208] [User eXperience Perception Insights Dataset (UXPID): Synthetic User Feedback from Public Industrial Forums](https://arxiv.org/abs/2509.11777)
*Mikhail Kulyabin,Jan Joosten,Choro Ulan uulu,Nuno Miguel Martins Pacheco,Fabian Ries,Filippos Petridis,Jan Bosch,Helena Holmström Olsson*

Main category: cs.CL

TL;DR: 本文介绍了UXPID，一个面向工业自动化论坛用户反馈的人工合成和匿名化数据集，并利用大语言模型对其注释，以便于产品体验分析。


<details>
  <summary>Details</summary>
Motivation: 工业论坛的用户反馈能够反映产品在实际使用中的优缺点，但由于内容非结构化且专业词汇多，传统的数据分析方法难以高效处理，导致这些宝贵信息未被充分利用。

Method: 作者构建了UXPID数据集，包括7130条从公开论坛提取并匿名化的用户反馈，每条为含多帖的JSON对象，辅以元数据和上下文。采用大语言模型对每条反馈分支进行系统分析和注释，包括用户体验洞察、用户期望、严重程度、情感和话题分类。

Result: 生成了一个详细标注、多维度的用户反馈数据集，为训练和评估基于transformer的AI模型提供了基础，可应用于问题检测、情感分析、需求提取等任务。

Conclusion: UXPID数据集为技术论坛中的用户反馈分析和AI驱动的产品改进提供了强有力的数据支持，尤其适用于因隐私和授权问题无法获取真实数据的场景，有助于推动UX和用户需求方面的研究。

Abstract: Customer feedback in industrial forums reflect a rich but underexplored
source of insight into real-world product experience. These publicly shared
discussions offer an organic view of user expectations, frustrations, and
success stories shaped by the specific contexts of use. Yet, harnessing this
information for systematic analysis remains challenging due to the unstructured
and domain-specific nature of the content. The lack of structure and
specialized vocabulary makes it difficult for traditional data analysis
techniques to accurately interpret, categorize, and quantify the feedback,
thereby limiting its potential to inform product development and support
strategies. To address these challenges, this paper presents the User
eXperience Perception Insights Dataset (UXPID), a collection of 7130
artificially synthesized and anonymized user feedback branches extracted from a
public industrial automation forum. Each JavaScript object notation (JSON)
record contains multi-post comments related to specific hardware and software
products, enriched with metadata and contextual conversation data. Leveraging a
large language model (LLM), each branch is systematically analyzed and
annotated for UX insights, user expectations, severity and sentiment ratings,
and topic classifications. The UXPID dataset is designed to facilitate research
in user requirements, user experience (UX) analysis, and AI-driven feedback
processing, particularly where privacy and licensing restrictions limit access
to real-world data. UXPID supports the training and evaluation of
transformer-based models for tasks such as issue detection, sentiment analysis,
and requirements extraction in the context of technical forums.

</details>


### [209] [When Curiosity Signals Danger: Predicting Health Crises Through Online Medication Inquiries](https://arxiv.org/abs/2509.11802)
*Dvora Goncharok,Arbel Shifman,Alexander Apartsin,Yehudit Aperstein*

Main category: cs.CL

TL;DR: 本文通过构建并公开一个关于药物相关问题的在线医疗论坛数据集，比较了传统机器学习方法和大型语言模型在检测潜在高危健康问题上的表现。


<details>
  <summary>Details</summary>
Motivation: 在线医疗论坛中包含大量病人对药物使用的疑问，其中部分问题或可反映用药误区甚至健康危机的早期信号。及时甄别关键性高危问题对于干预和提升患者安全至关重要。

Method: 作者手动标注和整理了一个药物相关问答数据集，每条数据根据临床风险因子进行关键性标签。利用TF-IDF特征，比较六种传统机器学习分类器，并引入三种当前主流的大型语言模型进行分类评测。

Result: 传统方法与大语言模型相比较，均展现出在高危问题自动检测中的潜力，表明二者有望支撑数字健康领域的实时分诊和预警系统。

Conclusion: 该研究为利用病人自述文本、自然语言处理和早期健康风险预警系统的交叉研究提供了基础资源，公开数据集和评测基准，有助于推动相关领域进一步发展。

Abstract: Online medical forums are a rich and underutilized source of insight into
patient concerns, especially regarding medication use. Some of the many
questions users pose may signal confusion, misuse, or even the early warning
signs of a developing health crisis. Detecting these critical questions that
may precede severe adverse events or life-threatening complications is vital
for timely intervention and improving patient safety. This study introduces a
novel annotated dataset of medication-related questions extracted from online
forums. Each entry is manually labelled for criticality based on clinical risk
factors. We benchmark the performance of six traditional machine learning
classifiers using TF-IDF textual representations, alongside three
state-of-the-art large language model (LLM)-based classification approaches
that leverage deep contextual understanding. Our results highlight the
potential of classical and modern methods to support real-time triage and alert
systems in digital health spaces. The curated dataset is made publicly
available to encourage further research at the intersection of
patient-generated data, natural language processing, and early warning systems
for critical health events. The dataset and benchmark are available at:
https://github.com/Dvora-coder/LLM-Medication-QA-Risk-Classifier-MediGuard.

</details>


### [210] [Growing Perspectives: Modelling Embodied Perspective Taking and Inner Narrative Development Using Large Language Models](https://arxiv.org/abs/2509.11868)
*Sabrina Patania,Luca Annese,Anna Lambiase,Anita Pellegrini,Tom Foulsham,Azzurra Ruggeri,Silvia Rossi,Silvia Serino,Dimitri Ognibene*

Main category: cs.CL

TL;DR: 本文提出了PerspAct系统，将ReAct范式和大语言模型结合，用以模拟和评估人类语言及具身视角采择的发展阶段，在协作任务中分析其对行动选择与任务效率的影响。实验结果发现模型能够产生与发展阶段一致的内部叙事，但实际任务中倾向于向更高级阶段转变，高发展阶段有助于提升协作效果。


<details>
  <summary>Details</summary>
Motivation: 以往的计算模型很少同时处理语言与具身视角采择，而这两者是人类协作的基础。作者希望通过新的系统，探索如何结合LLMs与发展心理学理论，更贴近人类在协作中的认知过程。

Method: 提出PerspAct系统，将ReAct（Reason and Act）范式与LLMs结合，基于Selman的视角采择发展理论，利用扩展的director task，让GPT生成不同发展阶段的内部叙事，并考察其对协作表现的影响，从行动选择和任务效率两个维度定性和定量评估。

Result: GPT能在任务前生成与指定发展阶段一致的内在叙事，但实际互动过程中，其表现常向更高级发展阶段转变。发展阶段较高时，协作效率和效果更优，早期阶段在复杂任务下表现波动较大。

Conclusion: 语言与具身角度采择的结合对模拟发展动态具有潜力，尤其在追踪和评估LLMs协作时，重视内部语言过程对于理解其认知演化和性能提升非常重要。

Abstract: Language and embodied perspective taking are essential for human
collaboration, yet few computational models address both simultaneously. This
work investigates the PerspAct system [1], which integrates the ReAct (Reason
and Act) paradigm with Large Language Models (LLMs) to simulate developmental
stages of perspective taking, grounded in Selman's theory [2]. Using an
extended director task, we evaluate GPT's ability to generate internal
narratives aligned with specified developmental stages, and assess how these
influence collaborative performance both qualitatively (action selection) and
quantitatively (task efficiency). Results show that GPT reliably produces
developmentally-consistent narratives before task execution but often shifts
towards more advanced stages during interaction, suggesting that language
exchanges help refine internal representations. Higher developmental stages
generally enhance collaborative effectiveness, while earlier stages yield more
variable outcomes in complex contexts. These findings highlight the potential
of integrating embodied perspective taking and language in LLMs to better model
developmental dynamics and stress the importance of evaluating internal speech
during combined linguistic and embodied tasks.

</details>


### [211] [From Fuzzy Speech to Medical Insight: Benchmarking LLMs on Noisy Patient Narratives](https://arxiv.org/abs/2509.11803)
*Eden Mama,Liel Sheri,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.CL

TL;DR: 本文提出了一个模拟真实病人自述的噪声数据集，用于测试大语言模型（LLM）在诊断任务中的表现，并放出了Noisy Diagnostic Benchmark (NDB)以促进社区研究。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型评测主要依赖结构化、干净的临床文本，无法反映模型在处理患者真实自述时的能力，因为患者语言常带有模糊、歧义和噪声。为更全面评估模型在真实医疗场景下的鲁棒性，需要新的测试集。

Method: 作者设计并构建了一个合成数据集，模拟不同噪声水平和清晰度的患者自述，均附带真实诊断标签；涵盖了多种表达风格。随后，作者用该数据集微调和评估了多种主流LLM（如BERT类模型、T5类模型），并公开了数据集。

Result: 通过该数据集，作者比较了不同LLM在噪声患者自述场景下的诊断能力表现，发现噪声和不规范语言会显著影响模型表现；数据集有效区分了各模型在真实场景下的优势与不足。

Conclusion: 研究表明：大语言模型在患者自述带噪声和模糊表达下表现有待提升。Noisy Diagnostic Benchmark (NDB)为后续模型鲁棒性研究和能力提升提供了标准测试平台。

Abstract: The widespread adoption of large language models (LLMs) in healthcare raises
critical questions about their ability to interpret patient-generated
narratives, which are often informal, ambiguous, and noisy. Existing benchmarks
typically rely on clean, structured clinical text, offering limited insight
into model performance under realistic conditions. In this work, we present a
novel synthetic dataset designed to simulate patient self-descriptions
characterized by varying levels of linguistic noise, fuzzy language, and
layperson terminology. Our dataset comprises clinically consistent scenarios
annotated with ground-truth diagnoses, spanning a spectrum of communication
clarity to reflect diverse real-world reporting styles. Using this benchmark,
we fine-tune and evaluate several state-of-the-art models (LLMs), including
BERT-based and encoder-decoder T5 models. To support reproducibility and future
research, we release the Noisy Diagnostic Benchmark (NDB), a structured dataset
of noisy, synthetic patient descriptions designed to stress-test and compare
the diagnostic capabilities of large language models (LLMs) under realistic
linguistic conditions. We made the benchmark available for the community:
https://github.com/lielsheri/PatientSignal

</details>


### [212] [PledgeTracker: A System for Monitoring the Fulfilment of Pledges](https://arxiv.org/abs/2509.11804)
*Yulong Chen,Michael Sejr Schlichtkrull,Zhenyun Deng,David Corney,Nasim Asl,Joshua Salisbury,Andrew Dudfield,Andreas Vlachos*

Main category: cs.CL

TL;DR: 本论文提出了PledgeTracker系统，通过结构化事件时间线的方式，改进了政治承诺实现追踪的方法，有效提高了证据检索和解释的效率。


<details>
  <summary>Details</summary>
Motivation: 当前跟踪政治候选人承诺实现情况的方法，普遍将问题简化为文档分类，忽略了证据的动态、时间性和多文档特性。这种简化方法无法准确、实时地反映承诺的履行过程。

Method: 提出了PledgeTracker系统，包括三大模块：（1）多步证据检索模块；（2）时间线构建模块；（3）履约过滤模块。该系统能够动态整合分散在多源的逐步证据，构建结构化、可解释的事件时间线。

Result: 在与专业事实核查员的真实工作流中进行评估，PledgeTracker可以更有效检索相关证据，显著减少人工核查工作量。

Conclusion: PledgeTracker重新定义了承诺验证流程为事件时间线构建，有效捕捉并解释了承诺实现的动态过程，提高了证据收集准确性，具备实际应用价值。

Abstract: Political pledges reflect candidates' policy commitments, but tracking their
fulfilment requires reasoning over incremental evidence distributed across
multiple, dynamically updated sources. Existing methods simplify this task into
a document classification task, overlooking its dynamic, temporal and
multi-document nature. To address this issue, we introduce
\textsc{PledgeTracker}, a system that reformulates pledge verification into
structured event timeline construction. PledgeTracker consists of three core
components: (1) a multi-step evidence retrieval module; (2) a timeline
construction module and; (3) a fulfilment filtering module, allowing the
capture of the evolving nature of pledge fulfilment and producing interpretable
and structured timelines. We evaluate PledgeTracker in collaboration with
professional fact-checkers in real-world workflows, demonstrating its
effectiveness in retrieving relevant evidence and reducing human verification
effort.

</details>


### [213] [SCDTour: Embedding Axis Ordering and Merging for Interpretable Semantic Change Detection](https://arxiv.org/abs/2509.11818)
*Taichi Aida,Danushka Bollegala*

Main category: cs.CL

TL;DR: 本文提出了SCDTour方法，以在语义变化检测（SCD）任务中兼顾embedding的可解释性与表现。SCDTour通过对embedding空间中的可解释轴进行有序合并，显著提升了解释性且不牺牲表现。实验显示，其方法能在保持甚至提升SCD表现的同时，极大改善embedding的可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前语义变化检测领域面临embedding可解释性与表现难以兼得的问题。提升embedding的可解释性往往导致SCD表现下降。为解决此矛盾，作者尝试寻找在不损失性能的前提下提高解释性的方法。

Method: 作者提出了SCDTour方法，基于embedding空间中各轴的语义相似度及其对语义变化的贡献度，对可解释轴进行排序和合并。通过有序聚合轴，得到更精致的词义集合，从而在不损失性能的前提下提升可解释性。

Result: 实验表明，SCDTour在SCD任务中的表现与原始高维embedding相当甚至有所提升，同时大幅度提升了解释性。聚合后的少量轴对语义变化的检测更加清晰明了。

Conclusion: SCDTour能够有效在语义变化检测任务中平衡embedding的可解释性和表现，使得用少量精炼轴即可实现高效、可解释的语义变化解释。

Abstract: In Semantic Change Detection (SCD), it is a common problem to obtain
embeddings that are both interpretable and high-performing. However, improving
interpretability often leads to a loss in the SCD performance, and vice versa.
To address this problem, we propose SCDTour, a method that orders and merges
interpretable axes to alleviate the performance degradation of SCD. SCDTour
considers both (a) semantic similarity between axes in the embedding space, as
well as (b) the degree to which each axis contributes to semantic change.
Experimental results show that SCDTour preserves performance in semantic change
detection while maintaining high interpretability. Moreover, agglomerating the
sorted axes produces a more refined set of word senses, which achieves
comparable or improved performance against the original full-dimensional
embeddings in the SCD task. These findings demonstrate that SCDTour effectively
balances interpretability and SCD performance, enabling meaningful
interpretation of semantic shifts through a small number of refined axes.
Source code is available at https://github.com/LivNLP/svp-tour .

</details>


### [214] [MOOM: Maintenance, Organization and Optimization of Memory in Ultra-Long Role-Playing Dialogues](https://arxiv.org/abs/2509.11860)
*Weishu Chen,Jinyi Tang,Zhouhui Hou,Shihao Han,Mingjie Zhan,Zhiyuan Huang,Delong Liu,Jiawei Guo,Zhicheng Zhao,Fei Su*

Main category: cs.CL

TL;DR: 本文提出了一种新型双分支记忆插件MOOM，用于解决超长人机对话中的记忆失控问题，并发布了专用于中文角色扮演对话的超长数据集ZH-4O。


<details>
  <summary>Details</summary>
Motivation: 现有方法在超长人机对话场景下，记忆提取容易导致无控制的记忆增长，影响角色扮演中的连贯性与有效性。对话系统需要更有效的方式提取关键信息，控制记忆规模。

Method: 提出MOOM插件，采用双分支结构，借鉴文学理论，分别针对情节发展和角色刻画两大要素；一支提取多时间尺度下的冲突情节，另一支总结用户角色画像。同时引入“竞争-抑制”遗忘机制，有效抑制记忆无序扩展。

Result: MOOM在超长对话的记忆提取任务上效果优于现有技术，能够减少大语言模型的调用次数并保持可控的记忆容量。首先公开了平均600轮对话、带有详细记忆标注的中文ZH-4O数据集，对比实验显示MOOM性能领先。

Conclusion: MOOM显著提升了超长对话记忆提取的效果与可控性，为人机角色扮演对话提供了更稳定的支持，并为相关研究与数据集建设提供了新工具和资源。

Abstract: Memory extraction is crucial for maintaining coherent ultra-long dialogues in
human-robot role-playing scenarios. However, existing methods often exhibit
uncontrolled memory growth. To address this, we propose MOOM, the first
dual-branch memory plugin that leverages literary theory by modeling plot
development and character portrayal as core storytelling elements.
Specifically, one branch summarizes plot conflicts across multiple time scales,
while the other extracts the user's character profile. MOOM further integrates
a forgetting mechanism, inspired by the ``competition-inhibition'' memory
theory, to constrain memory capacity and mitigate uncontrolled growth.
Furthermore, we present ZH-4O, a Chinese ultra-long dialogue dataset
specifically designed for role-playing, featuring dialogues that average 600
turns and include manually annotated memory information. Experimental results
demonstrate that MOOM outperforms all state-of-the-art memory extraction
methods, requiring fewer large language model invocations while maintaining a
controllable memory capacity.

</details>


### [215] [Uncertainty in Authorship: Why Perfect AI Detection Is Mathematically Impossible](https://arxiv.org/abs/2509.11915)
*Aadil Gani Ganie*

Main category: cs.CL

TL;DR: 本文提出，随着大语言模型（LLM）的发展，人工智能生成文本与人类文本愈加难以区分，检测AI文本的过程本身存在理论极限。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成文本的激增，如何区分人类与机器撰写的文本成为实际与伦理层面上的重要问题，但现有技术手段无法完美解决。

Method: 作者以量子不确定性为类比，分析了文本作者检测中的信心与文本自然性之间的权衡，考察了风格分析、水印、神经网络分类器等当前检测技术的局限，并从理论角度探讨检测的根本困境。

Result: 结果表明，检测手段提升的同时会影响AI文本，使其他检测特征变得不可靠。对于高度仿真的AI文本，理论上已无法实现完美检测。

Conclusion: AI文本检测问题不仅仅是工具升级，更关乎语言本质的深层张力。这种不确定性和局限性将持续影响作者归属、伦理及政策制定。

Abstract: As large language models (LLMs) become more advanced, it is increasingly
difficult to distinguish between human-written and AI-generated text. This
paper draws a conceptual parallel between quantum uncertainty and the limits of
authorship detection in natural language. We argue that there is a fundamental
trade-off: the more confidently one tries to identify whether a text was
written by a human or an AI, the more one risks disrupting the text's natural
flow and authenticity. This mirrors the tension between precision and
disturbance found in quantum systems. We explore how current detection
methods--such as stylometry, watermarking, and neural classifiers--face
inherent limitations. Enhancing detection accuracy often leads to changes in
the AI's output, making other features less reliable. In effect, the very act
of trying to detect AI authorship introduces uncertainty elsewhere in the text.
Our analysis shows that when AI-generated text closely mimics human writing,
perfect detection becomes not just technologically difficult but theoretically
impossible. We address counterarguments and discuss the broader implications
for authorship, ethics, and policy. Ultimately, we suggest that the challenge
of AI-text detection is not just a matter of better tools--it reflects a
deeper, unavoidable tension in the nature of language itself.

</details>


### [216] [Designing LLMs for cultural sensitivity: Evidence from English-Japanese translation](https://arxiv.org/abs/2509.11921)
*Helene Tenzer,Oumnia Abidi,Stefan Feuerriegel*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在英日工作邮件翻译中的文化敏感性，发现通过调整提示词可以提升翻译的文化适应性。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM能实现接近完美的字面翻译，但它们在支持符合文化语境的交流方面能力尚不明确。不同文化背景下的合适交流方式非常重要，尤其是在职场等正式场合。本文旨在分析LLM在跨文化场景中能否生成文化上适当的文本。

Method: 作者设计了三种不同的提示策略：1）仅简单要求翻译，2）指定收件人的文化背景，3）明确指导目标语言的沟通规范。利用混合方法，分析了翻译文本中与文化相关的语言特征，并邀请母语者评价语气是否合适。

Result: 结果显示，针对性提示能帮助LLM更好地遵循文化规范，从而生成文化适应性更强的翻译。尤其是在明确给出沟通规范时，翻译更能符合日本职场邮件的期待。

Conclusion: 研究表明，通过优化提示，LLM在多语种、多文化环境中能提升输出的文化敏感性。作者也据此给出了设计更具包容性的多语言LLM的建议。

Abstract: Large language models (LLMs) are increasingly used in everyday communication,
including multilingual interactions across different cultural contexts. While
LLMs can now generate near-perfect literal translations, it remains unclear
whether LLMs support culturally appropriate communication. In this paper, we
analyze the cultural sensitivity of different LLM designs when applied to
English-Japanese translations of workplace e-mails. Here, we vary the prompting
strategies: (1) naive "just translate" prompts, (2) audience-targeted prompts
specifying the recipient's cultural background, and (3) instructional prompts
with explicit guidance on Japanese communication norms. Using a mixed-methods
study, we then analyze culture-specific language patterns to evaluate how well
translations adapt to cultural norms. Further, we examine the appropriateness
of the tone of the translations as perceived by native speakers. We find that
culturally-tailored prompting can improve cultural fit, based on which we offer
recommendations for designing culturally inclusive LLMs in multilingual
settings.

</details>


### [217] [Spec-LLaVA: Accelerating Vision-Language Models with Dynamic Tree-Based Speculative Decoding](https://arxiv.org/abs/2509.11961)
*Mingxiao Huo,Jiayi Zhang,Hewei Wang,Jinfeng Xu,Zheyu Chen,Huilin Tai,Yijun Chen*

Main category: cs.CL

TL;DR: Spec-LLaVA是一种加速视觉-语言模型推理的系统，通过引入动态树结构的推测解码方法，提升推理速度且不损失生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型（VLMs）尽管具有强大多模态推理能力，但其自回归推理速度慢，严重限制了在实时场景下的应用。因此，亟需一种方法能在保证输出质量的前提下加快VLMs的推理速度。

Method: Spec-LLaVA系统采用“双模型体系”：利用轻量级的草稿模型（draft VLM）对未来token进行推测，同时用大型目标模型（target model）并行验证草稿输出。为进一步提高效率，作者提出一种动态树型验证算法，根据草稿模型的置信度自适应扩展或剪枝推测分支。

Result: 在MS COCO数据集及其域外图像上，Spec-LLaVA在LLaVA-1.5（7B, 13B）模型上带来最高3.28倍推理速度提升，且在生成质量上没有损失。

Conclusion: Spec-LLaVA提出了一种动态树结构的无损加速框架，有望实现高效、实时的多模态助手。尤其草稿模型设计轻量，使其适用于资源受限或端侧部署。

Abstract: Vision-Language Models (VLMs) enable powerful multimodal reasoning but suffer
from slow autoregressive inference, limiting their deployment in real-time
applications. We introduce Spec-LLaVA, a system that applies speculative
decoding to accelerate VLMs without sacrificing output quality. Spec-LLaVA
pairs a lightweight draft VLM with a large target model: the draft speculates
future tokens, which the target verifies in parallel, allowing multiple tokens
to be generated per step. To maximize efficiency, we design a dynamic
tree-based verification algorithm that adaptively expands and prunes
speculative branches using draft model confidence. On MS COCO out-of-domain
images, Spec-LLaVA achieves up to 3.28$\times$ faster decoding on LLaVA-1.5
(7B, 13B) with no loss in generation quality. This work presents a lossless
acceleration framework for VLMs using dynamic tree-structured speculative
decoding, opening a path toward practical real-time multimodal assistants.
Importantly, the lightweight draft model design makes the framework amenable to
resource-constrained or on-device deployment settings.

</details>


### [218] [ToolRM: Outcome Reward Models for Tool-Calling Large Language Models](https://arxiv.org/abs/2509.11963)
*Mayank Agarwal,Ibrahim Abdelaziz,Kinjal Basu,Merve Unuvar,Luis A. Lastras,Yara Rizk,Pavan Kapanipathi*

Main category: cs.CL

TL;DR: 本文针对大语言模型在调用外部工具时的奖励建模问题提出了首个系统性评测基准FC-RewardBench，并提出了基于合成数据的新型奖励建模训练框架，显著提升了相关任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有的奖励模型主要针对自然语言输出进行训练，难以精准评估大模型在工具使用场景下的推理和执行能力，因此亟需针对工具调用的奖励建模和评测方法。

Method: 作者提出了FC-RewardBench基准，用于系统性评估奖励模型在工具调用场景下的表现。同时，提出一种基于开放权重LLM合成数据的结果导向型奖励模型训练框架，并在不同规模模型上进行了训练与评测。

Result: 实验结果显示，所提出的奖励模型在七个跨域基准上均优于现有通用奖励模型基线，下游任务表现平均提升可达25%，且支持高效的数据筛选和微调。

Conclusion: 针对此前奖励建模在工具调用领域的空白，本文提出的新基准和训练框架有效提升了相关能力，为后续大模型工具强化学习提供了方法和数据基础。

Abstract: As large language models (LLMs) increasingly interact with external tools,
reward modeling for tool use has become a critical yet underexplored area.
Existing reward models, trained primarily on natural language outputs, struggle
to evaluate tool-based reasoning and execution. To quantify this gap, we
introduce FC-RewardBench, the first benchmark designed to systematically assess
reward models' performance in tool-calling scenarios. Our analysis shows that
current reward models often miss key signals of effective tool use,
highlighting the need for domain-specific modeling. To address this, we propose
a training framework for outcome-based reward models using data synthesized
from permissively licensed, open-weight LLMs. We train models ranging from 1.7B
to 14B parameters and evaluate them across seven out-of-domain benchmarks.
These models consistently outperform general-purpose baselines, achieving up to
25\% average improvement in downstream task performance and enabling
data-efficient fine-tuning through reward-guided filtering.

</details>


### [219] [Query-Focused Extractive Summarization for Sentiment Explanation](https://arxiv.org/abs/2509.11989)
*Ahmed Moubtahij,Sylvie Ratté,Yazid Attabi,Maxime Dumas*

Main category: cs.CL

TL;DR: 本文旨在通过引入多偏向性框架，提升在大量文本中针对查询的情感归因总结（QFS）任务，尤其提升在情感解释方面的性能。


<details>
  <summary>Details</summary>
Motivation: 用户反馈分析时，需要追溯情感形成的具体原因，但文本繁杂、信息分散，人工处理效率低，现有QFS模型又常因查询与文本间的语言差异受限。

Method: 提出了通用、领域无关的多偏向性（multi-bias）框架，减少查询与文档之间的语言不一致，同时针对情感解释，结合情感偏向和查询扩展开展专门方法。

Result: 在真实世界、带情感的QFS数据集上，实验结果表明该方法优于基线模型。

Conclusion: 多偏向性框架及其专用策略可显著提升情感解释型QFS效果，有助于客户反馈分析自动化与高效化。

Abstract: Constructive analysis of feedback from clients often requires determining the
cause of their sentiment from a substantial amount of text documents. To assist
and improve the productivity of such endeavors, we leverage the task of
Query-Focused Summarization (QFS). Models of this task are often impeded by the
linguistic dissonance between the query and the source documents. We propose
and substantiate a multi-bias framework to help bridge this gap at a
domain-agnostic, generic level; we then formulate specialized approaches for
the problem of sentiment explanation through sentiment-based biases and query
expansion. We achieve experimental results outperforming baseline models on a
real-world proprietary sentiment-aware QFS dataset.

</details>


### [220] [Text Adaptation to Plain Language and Easy Read via Automatic Post-Editing Cycles](https://arxiv.org/abs/2509.11991)
*Jesús Calleja,David Ponce,Thierry Etchegoyhen*

Main category: cs.CL

TL;DR: 本文介绍了Vicomtech团队在CLEARS挑战赛中，如何通过自动后编辑和迭代适应的方式，将西班牙语文本转换为简明语言和易读文本，并取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 文本简化和易读性提升对于语言障碍者、初学者等群体非常重要，因此需要高效自动化工具来进行文本适配。

Method: 采用了大型语言模型生成初始简化后，进行自动后编辑。通过迭代的适应过程，利用可读性和相似性指标判断是否需要进一步细化，直到模型表现最优为止。

Result: 该方法在官方评价指标的平均分上，简明语言组获得第一，易读组获得第二。

Conclusion: 自动后编辑和迭代优化方法在西班牙语文本简化和易读性适配中表现优秀，展示了实际应用潜力。

Abstract: We describe Vicomtech's participation in the CLEARS challenge on text
adaptation to Plain Language and Easy Read in Spanish. Our approach features
automatic post-editing of different types of initial Large Language Model
adaptations, where successive adaptations are generated iteratively until
readability and similarity metrics indicate that no further adaptation
refinement can be successfully performed. Taking the average of all official
metrics, our submissions achieved first and second place in Plain language and
Easy Read adaptation, respectively.

</details>


### [221] [Steering Language Models in Multi-Token Generation: A Case Study on Tense and Aspect](https://arxiv.org/abs/2509.12065)
*Alina Klerings,Jannik Brinkmann,Daniel Ruffinelli,Simone Ponzetto*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLM）内部的时态和体（aspect）语法知识表达及其可控性，通过识别和操控这两类多维语法特征，展示了模型内部表征和生成调节的机制和挑战。


<details>
  <summary>Details</summary>
Motivation: 之前的工作多关注于语法的二元对立（如对错），而很少探讨多维、层次化的语法特征（如动词时态和体）。理解和调控这些特征对于更精细地掌控生成文本的语法属性具有重要意义。

Method: 作者用线性判别分析（LDA）在LLM的残差空间中，识别出表征动词时态和体的正交方向。通过concept steering（概念引导）技术，在三类文本生成任务中实证这些特征的可控性，并探究影响多token生成中调控效果的因素。

Result: 实验证明可以沿特定方向控制模型生成的时态和体，但调控时会受到steering的强度、作用位置和持续时间等参数影响，这些参数直接关系到负面副作用（比如主题偏移、文本退化）的出现。

Conclusion: LLM内部对时态和体的结构化编码类似人类语言组织，但在实际文本生成过程中，精准控制这些语法特征依赖细致的参数调节或自动化优化，要求针对应用进行调优。

Abstract: Large language models (LLMs) are able to generate grammatically well-formed
text, but how do they encode their syntactic knowledge internally? While prior
work has focused largely on binary grammatical contrasts, in this work, we
study the representation and control of two multidimensional hierarchical
grammar phenomena - verb tense and aspect - and for each, identify distinct,
orthogonal directions in residual space using linear discriminant analysis.
Next, we demonstrate causal control over both grammatical features through
concept steering across three generation tasks. Then, we use these identified
features in a case study to investigate factors influencing effective steering
in multi-token generation. We find that steering strength, location, and
duration are crucial parameters for reducing undesirable side effects such as
topic shift and degeneration. Our findings suggest that models encode tense and
aspect in structurally organized, human-like ways, but effective control of
such features during generation is sensitive to multiple factors and requires
manual tuning or automated optimization.

</details>


### [222] [SENSE models: an open source solution for multilingual and multimodal semantic-based tasks](https://arxiv.org/abs/2509.12093)
*Salima Mdhaffar,Haroun Elleuch,Chaimae Chellaf,Ha Nguyen,Yannick Estève*

Main category: cs.CL

TL;DR: 本论文提出了SENSE模型，一种用于多语言语音与文本对齐的共享嵌入方法，并在公开SpeechBrain工具包中实现和发布。实验表明该方法在多语言、多模态任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前多语言语音与文本对齐需要有效的共享表示，现有方法如SAMU-XLSR和SONAR存在改进空间，比如教师模型或语音编码器选择不足以最优对齐语义。提高模型性能和对齐语义质量是主要动机。

Method: 采用教师-学生框架，通过选择更强的文本教师模型及更优的初始语音编码器，对原SAMU-XLSR方法进行扩展和优化。训练出的SENSE模型在SpeechBrain工具包中集成并开源。

Result: SENSE模型在多语言和多模态语义任务上表现出极具竞争力的性能，优于或对齐于现有方法，并有模型发布和代码实现。实验还带来了新见解，例如对语义对齐过程和表征能力的理解。

Conclusion: SENSE作为公开的多语言语音-文本共享表示方案，证明了教师-学生对齐方法及优化配置的有效性，为后续语音人工智能任务提供了新的基础工具和研究思路。

Abstract: This paper introduces SENSE (Shared Embedding for N-lingual Speech and tExt),
an open-source solution inspired by the SAMU-XLSR framework and conceptually
similar to Meta AI's SONAR models. These approaches rely on a teacher-student
framework to align a self-supervised speech encoder with the language-agnostic
continuous representations of a text encoder at the utterance level. We
describe how the original SAMU-XLSR method has been updated by selecting a
stronger teacher text model and a better initial speech encoder. The source
code for training and using SENSE models has been integrated into the
SpeechBrain toolkit, and the first SENSE model we trained has been publicly
released. We report experimental results on multilingual and multimodal
semantic tasks, where our SENSE model achieves highly competitive performance.
Finally, this study offers new insights into how semantics are captured in such
semantically aligned speech encoders.

</details>


### [223] [Is 'Hope' a person or an idea? A pilot benchmark for NER: comparing traditional NLP tools and large language models on ambiguous entities](https://arxiv.org/abs/2509.12098)
*Payam Latifi*

Main category: cs.CL

TL;DR: 本研究比较了三种传统NLP工具和三种通用大模型在小规模命名实体识别（NER）任务上的表现，发现大模型在上下文敏感实体识别上表现更好，但部分传统工具在结构化标签任务上更具一致性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在自然语言处理任务中表现突出，但尚缺乏对其在命名实体识别（NER）方面与传统工具系统对比的细致分析，因此作者希望通过设定统一的小型数据集，评估不同系统在NER上的实际能力和特点。

Method: 作者构建了涵盖五种实体类别（PERSON, LOCATION, ORGANIZATION, DATE, TIME）共119个标注词项的小型数据集，对三种主流NLP工具（NLTK、spaCy、Stanza）与三种大语言模型（Gemini-1.5-flash、DeepSeek-V3、Qwen-3-4B）在NER任务上的表现进行测试，评估指标为F1分数，并逐一比较了系统输出与人工金标准之间的差异。

Result: 实验结果表明，LLM（如Gemini）在识别上下文敏感的人名类实体时F1分数最高，但像Stanza这类传统工具在LOCATION和DATE等结构化标签的识别上更为一致。同时，不同LLM在处理时间表达和多词组织实体时存在表现差异。

Conclusion: 大语言模型在上下文理解和复杂实体识别上具有优势，但传统NLP工具在特定结构化任务中表现依然有竞争力。研究结果提醒用户应结合具体任务合理选择NER系统。

Abstract: This pilot study presents a small-scale but carefully annotated benchmark of
Named Entity Recognition (NER) performance across six systems: three non-LLM
NLP tools (NLTK, spaCy, Stanza) and three general-purpose large language models
(LLMs: Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B). The dataset contains 119
tokens covering five entity types (PERSON, LOCATION, ORGANIZATION, DATE, TIME).
We evaluated each system's output against the manually annotated gold standard
dataset using F1-score. The results show that LLMs generally outperform
conventional tools in recognizing context-sensitive entities like person names,
with Gemini achieving the highest average F1-score. However, traditional
systems like Stanza demonstrate greater consistency in structured tags such as
LOCATION and DATE. We also observed variability among LLMs, particularly in
handling temporal expressions and multi-word organizations. Our findings
highlight that while LLMs offer improved contextual understanding, traditional
tools remain competitive in specific tasks, informing model selection.

</details>


### [224] [In-domain SSL pre-training and streaming ASR](https://arxiv.org/abs/2509.12101)
*Jarod Duret,Salima Mdhaffar,Gaëlle Laperrière,Ryan Whetten,Audrey Galametz,Catherine Kobus,Marion-Cécile Martin,Jo Oleiwan,Yannick Estève*

Main category: cs.CL

TL;DR: 本研究针对航空管制（ATC）环境，通过在大量无标注ATC语音数据上的自监督预训练，显著提升了ASR（自动语音识别）系统的准确性和实时性。


<details>
  <summary>Details</summary>
Motivation: 通用ASR模型在专业领域如航空管制中的表现有限，因此作者探索了领域特定的自监督预训练是否能带来更好的识别效果，特别是在高安全性、低延迟要求的ATC实际应用中。

Method: 作者利用4,500小时的ATC无标注数据进行自监督预训练（BEST-RQ模型），随后在小规模有标注ATC数据集上微调，并创新性地采用chunked attention和动态卷积以实现低延迟。与主流通用语音编码器（如w2v-BERT 2.0与HuBERT）进行了对比评测。

Result: 领域自监督预训练模型在标准ATC基准测试中取得了显著更低的词错误率。所提出的流式低延迟推理方法在严格延迟限制下进一步改善了准确性，均优于在广泛语音语料上训练的通用模型。

Conclusion: 针对ATC领域进行自监督学习，有效提升了ASR系统在安全关键、实时性高的实际航空应用中的表现，显示出定制化SSL模型的巨大实用价值。

Abstract: In this study, we investigate the benefits of domain-specific self-supervised
pre-training for both offline and streaming ASR in Air Traffic Control (ATC)
environments. We train BEST-RQ models on 4.5k hours of unlabeled ATC data, then
fine-tune on a smaller supervised ATC set. To enable real-time processing, we
propose using chunked attention and dynamic convolutions, ensuring low-latency
inference. We compare these in-domain SSL models against state-of-the-art,
general-purpose speech encoders such as w2v-BERT 2.0 and HuBERT. Results show
that domain-adapted pre-training substantially improves performance on standard
ATC benchmarks, significantly reducing word error rates when compared to models
trained on broad speech corpora. Furthermore, the proposed streaming approach
further improves word error rate under tighter latency constraints, making it
particularly suitable for safety-critical aviation applications. These findings
highlight that specializing SSL representations for ATC data is a practical
path toward more accurate and efficient ASR systems in real-world operational
settings.

</details>


### [225] [GTA: Supervised-Guided Reinforcement Learning for Text Classification with Large Language Models](https://arxiv.org/abs/2509.12108)
*Min Zeng,Jinfei Sun,Xueyou Luo,Caiquan Liu,Shiqi Zhang,Li Xie,Xiaoxin Chen*

Main category: cs.CL

TL;DR: 该论文提出了Guess-Think-Answer（GTA）框架，结合了有监督微调（SFT）的高效性和强化学习（RL）的高性能上限，在文本分类任务中获得了优于单独SFT或RL方法的结果，并显著加快了收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有NLP任务中的强化学习微调方法学习效率低、收敛慢，而纯有监督微调虽然高效但性能有限。论文旨在探索一种兼顾训练效率和模型性能的方法，实现效率与能力的平衡。

Method: 提出了GTA框架，模型先生成一个初步答案（通过交叉熵损优化），再自我反思后输出最终答案。最终答案及答案结构受RL奖励引导。在两种训练信号下，采取损失掩码和梯度约束减少冲突，实现协同优化。

Result: 在四个文本分类数据集上，实验结果显示GTA框架不仅加快了模型训练收敛速度，还获得了比单独SFT或RL更高的性能水平。

Conclusion: GTA框架有效结合了SFT的训练效率和RL的能力提升，解决了二者间的权衡问题，为提升NLP模型性能和效率提供了新方案。

Abstract: In natural language processing tasks, pure reinforcement learning (RL)
fine-tuning methods often suffer from inefficient exploration and slow
convergence; while supervised fine-tuning (SFT) methods, although efficient in
training, have limited performance ceiling and less solid theoretical
foundation compared to RL. To address efficiency-capability trade-off, we
propose the Guess-Think-Answer (GTA) framework that combines the efficiency of
SFT with the capability gains of RL in a unified training paradigm. GTA works
by having the model first produce a provisional guess (optimized via
cross-entropy loss), then reflect on this guess before generating the final
answer, with RL rewards shaping both the final output and the format of the
entire GTA structure. This hybrid approach achieves both faster convergence
than pure RL and higher performance ceiling than pure SFT. To mitigate gradient
conflicts between the two training signals, we employ loss masking and gradient
constraints. Empirical results on four text classification benchmarks
demonstrate that GTA substantially accelerates convergence while outperforming
both standalone SFT and RL baselines.

</details>


### [226] [CBP-Tuning: Efficient Local Customization for Black-box Large Language Models](https://arxiv.org/abs/2509.12112)
*Jiaxuan Zhao,Naibin Gu,Yuchen Feng,Xiyu Liu,Peng Fu,Zheng Lin,Weiping Wang*

Main category: cs.CL

TL;DR: 该论文提出了一种名为CBP-Tuning的新型大语言模型本地定制方案，实现高效、隐私友好的定制化而无需访问模型参数或上传数据。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的个性化定制成本高，难以满足用户差异化、隐私化需求。传统云服务难以提供灵活定制且存在数据隐私风险。作者提出要兼顾定制效率与双向隐私保护。

Method: 提出CBP-Tuning方法，将过程分为服务器端和用户端两步：服务器端训练Prompt生成器用于捕获领域能力，用户端则采用无梯度优化实现任务个性化，无需上传隐私数据也不要求访问模型权重。每个任务只需定制一个向量。

Result: 在常识推理、医疗、金融等领域评估显示，CBP-Tuning方法在各类基准上取得优于现有方法的效果，验证了其隐私保护和任务无关的优势。

Conclusion: CBP-Tuning能够实现高效、灵活且隐私友好的大模型本地定制，兼顾了服务商和用户的定制灵活性与数据隐私。

Abstract: The high costs of customizing large language models (LLMs) fundamentally
limit their adaptability to user-specific needs. Consequently, LLMs are
increasingly offered as cloud-based services, a paradigm that introduces
critical limitations: providers struggle to support personalized customization
at scale, while users face privacy risks when exposing sensitive data. To
address this dual challenge, we propose Customized Black-box Prompt Tuning
(CBP-Tuning), a novel framework that facilitates efficient local customization
while preserving bidirectional privacy. Specifically, we design a two-stage
framework: (1) a prompt generator trained on the server-side to capture
domain-specific and task-agnostic capabilities, and (2) user-side gradient-free
optimization that tailors soft prompts for individual tasks. This approach
eliminates the need for users to access model weights or upload private data,
requiring only a single customized vector per task while achieving effective
adaptation. Furthermore, the evaluation of CBP-Tuning in the commonsense
reasoning, medical and financial domain settings demonstrates superior
performance compared to baselines, showcasing its advantages in task-agnostic
processing and privacy preservation.

</details>


### [227] [XplaiNLP at CheckThat! 2025: Multilingual Subjectivity Detection with Finetuned Transformers and Prompt-Based Inference with Large Language Models](https://arxiv.org/abs/2509.12130)
*Ariana Sahitaj,Jiaao Li,Pia Wenzel Neves,Fedor Splitt,Premtim Sahitaj,Charlott Jakob,Veronika Solopova,Vera Schmitt*

Main category: cs.CL

TL;DR: 本文介绍了作者团队在CheckThat! 2025多语种主观性检测任务中的参赛方案，测试了微调Transformer编码器和零样本大模型提示两种方法，并在多个语言任务中取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 多语种主观性检测对于社会媒体分析和事实核查具有重要意义。然而，当前相关研究在低资源语言和跨语种泛化等方面存在挑战，尤其是如何充分利用现有大模型及翻译数据改善主观性检测表现。

Method: 实验采用两类方法：（1）微调Transformer编码器（EuroBERT、XLM-RoBERTa、German-BERT）在单语及机器翻译训练数据上进行监督学习；（2）采用两种大语言模型的零样本prompting，包括o3-mini进行规则标注和gpt-4.1-mini用于对比重写及比较推理。

Result: 在意大利语单语检测任务中，基于Annotation方法获得F1=0.8104排名第一，显著高于基线。在罗马尼亚语零样本环境下，微调的XLM-RoBERTa模型F1=0.7917排名第三并超越基线。该模型在多语种和希腊语任务中同样优于基线。德语方面，通过相关语种翻译训练数据微调的German-BERT也实现了竞争性表现。然而，乌克兰语和波兰语的零样本效果略低于基线，表明低资源跨语场景中的泛化仍有难度。

Conclusion: 微调大模型和利用零样本prompting方法在多语种主观性检测任务中表现优秀。尤其在高资源语言及相关性较强语种间迁移时成效显著，低资源跨语泛化仍需进一步探索。

Abstract: This notebook reports the XplaiNLP submission to the CheckThat! 2025 shared
task on multilingual subjectivity detection. We evaluate two approaches: (1)
supervised fine-tuning of transformer encoders, EuroBERT, XLM-RoBERTa, and
German-BERT, on monolingual and machine-translated training data; and (2)
zero-shot prompting using two LLMs: o3-mini for Annotation (rule-based
labelling) and gpt-4.1-mini for DoubleDown (contrastive rewriting) and
Perspective (comparative reasoning). The Annotation Approach achieves 1st place
in the Italian monolingual subtask with an F_1 score of 0.8104, outperforming
the baseline of 0.6941. In the Romanian zero-shot setting, the fine-tuned
XLM-RoBERTa model obtains an F_1 score of 0.7917, ranking 3rd and exceeding the
baseline of 0.6461. The same model also performs reliably in the multilingual
task and improves over the baseline in Greek. For German, a German-BERT model
fine-tuned on translated training data from typologically related languages
yields competitive performance over the baseline. In contrast, performance in
the Ukrainian and Polish zero-shot settings falls slightly below the respective
baselines, reflecting the challenge of generalization in low-resource
cross-lingual scenarios.

</details>


### [228] [Pun Unintended: LLMs and the Illusion of Humor Understanding](https://arxiv.org/abs/2509.12158)
*Alessandro Zangari,Matteo Marcuzzo,Andrea Albarelli,Mohammad Taher Pilehvar,Jose Camacho-Collados*

Main category: cs.CL

TL;DR: 本论文系统分析了大语言模型（LLM）对双关语识别能力的不足，并提出了更具挑战性和细致化的双关语检测基准测试。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在检测双关语方面已有进展，但它们的理解仍然较为肤浅，难以达到人类对幽默和语言微妙之处的把握。研究希望揭示LLM在此方面的局限，并推动模型在语义理解上的进一步提升。

Method: 作者通过对现有的双关语基准测试进行系统分析和改造，制作了更加全面和具有挑战性的双关语检测数据集；同时，加入了人类对多种最新LLM的评测，并分析了模型面对不同类型双关语时的表现和鲁棒性问题。

Result: 实验结果显示，LLM在面对经过精细修改后的双关语时容易出错，这揭示了它们在深层语义理解和语言幽默感知方面的明显不足。

Conclusion: 当前的LLM虽然可以检测部分双关语，但其理解能力仍存在明显短板。未来需在模型的语义消歧、幽默理解等能力上加以改进，以实现更接近人类的语言理解。

Abstract: Puns are a form of humorous wordplay that exploits polysemy and phonetic
similarity. While LLMs have shown promise in detecting puns, we show in this
paper that their understanding often remains shallow, lacking the nuanced grasp
typical of human interpretation. By systematically analyzing and reformulating
existing pun benchmarks, we demonstrate how subtle changes in puns are
sufficient to mislead LLMs. Our contributions include comprehensive and nuanced
pun detection benchmarks, human evaluation across recent LLMs, and an analysis
of the robustness challenges these models face in processing puns.

</details>


### [229] [RAGs to Riches: RAG-like Few-shot Learning for Large Language Model Role-playing](https://arxiv.org/abs/2509.12168)
*Timothy Rupprecht,Enfu Nan,Arash Akbari,Arman Akbari,Lei Lu,Priyanka Maan,Sean Duffy,Pu Zhao,Yumei He,David Kaeli,Yanzhi Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的大语言模型（LLM）角色扮演提示框架RAGs-to-Riches，通过检索型生成方法提升模型在高风险场景下的角色一致性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM角色扮演方法在医疗、教育和治理等高风险领域容易在与敌意用户交互中出现“出戏”问题，影响模型信任度和用户体验，因此需开发更健壮的人类一致性方案。

Method: 借鉴检索增强生成（RAG）思路，将LLM角色扮演转化为文本检索任务，并利用精心策划的参考示例作为条件，引导LLM生成符合角色的回复。新提出了IOO和IOR两种token级ROUGE指标衡量即兴和示例利用率。采用LLM充当裁判进行评估，比较不同方法的角色保持能力。

Result: 在敌意用户场景下，提出方法生成的回复平均有35%的token直接来自参考示例。共进行453次角色扮演交互，实验表明新方法产生的回复比零样本和上下文学习方法更具角色真实性，模型更少出现“出戏”现象。

Conclusion: RAGs-to-Riches提示框架为实现强鲁棒性的人类一致性LLM角色扮演提供了一种可扩展且有效的新策略，可应用于高风险领域。

Abstract: Role-playing Large language models (LLMs) are increasingly deployed in
high-stakes domains such as healthcare, education, and governance, where
failures can directly impact user trust and well-being. A cost effective
paradigm for LLM role-playing is few-shot learning, but existing approaches
often cause models to break character in unexpected and potentially harmful
ways, especially when interacting with hostile users. Inspired by
Retrieval-Augmented Generation (RAG), we reformulate LLM role-playing into a
text retrieval problem and propose a new prompting framework called
RAGs-to-Riches, which leverages curated reference demonstrations to condition
LLM responses. We evaluate our framework with LLM-as-a-judge preference voting
and introduce two novel token-level ROUGE metrics: Intersection over Output
(IOO) to quantity how much an LLM improvises and Intersection over References
(IOR) to measure few-shot demonstrations utilization rate during the evaluation
tasks. When simulating interactions with a hostile user, our prompting strategy
incorporates in its responses during inference an average of 35% more tokens
from the reference demonstrations. As a result, across 453 role-playing
interactions, our models are consistently judged as being more authentic, and
remain in-character more often than zero-shot and in-context Learning (ICL)
methods. Our method presents a scalable strategy for building robust,
human-aligned LLM role-playing frameworks.

</details>


### [230] [Preservation of Language Understanding Capabilities in Speech-aware Large Language Models](https://arxiv.org/abs/2509.12171)
*Marek Kubis,Paweł Skórzewski,Iwona Christop,Mateusz Czyżnikiewicz,Jakub Kubiak,Łukasz Bondaruk,Marcin Lewandowski*

Main category: cs.CL

TL;DR: 该论文提出了C3T基准，用于评估语音相关大语言模型在语音输入情况下的性能和能力保持程度。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型多以文本输入为主，但随着语音输入需求增加，需评估模型在语音输入下的理解与处理能力，并关注不同说话者、公平性及多模态鲁棒性。

Method: C3T基准结合文本任务与语音克隆的文本转语音技术，将同样任务通过文本和语音输入分别测试模型，量化能力保持程度、公平性和多模态鲁棒性。

Result: C3T能有效识别并对比模型在文本与语音输入下的语言理解性能差异，揭示模型在不同说话者类别下的公平性差异。

Conclusion: C3T为语音相关大语言模型提供了系统、可量化的评测手段，有助于推动更公平和鲁棒的跨模态语言模型发展。

Abstract: The paper presents C3T (Cross-modal Capabilities Conservation Test), a new
benchmark for assessing the performance of speech-aware large language models.
The benchmark utilizes textual tasks and a voice cloning text-to-speech model
to quantify the extent to which language understanding capabilities are
preserved when the model is accessed via speech input. C3T quantifies the
fairness of the model for different categories of speakers and its robustness
across text and speech modalities.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [231] [Large Foundation Models for Trajectory Prediction in Autonomous Driving: A Comprehensive Survey](https://arxiv.org/abs/2509.10570)
*Wei Dai,Shengen Wu,Wei Wu,Zhenhao Wang,Sisuo Lyu,Haicheng Liao,Limin Yu,Weiping Ding,Runwei Guan,Yutao Yue*

Main category: cs.RO

TL;DR: 该综述论文系统回顾了大型基础模型（LFMs），尤其是大型语言模型（LLMs）和多模态大型语言模型（MLLMs）在自动驾驶轨迹预测中的最新进展，包括方法、任务、评价指标和未来挑战。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法虽提升了轨迹预测精度，但面临可解释性差、对大规模标注数据依赖强以及长尾场景泛化弱等问题。随着LFMs发展，研究者希望借助其在语义推理和多模态理解上的优势，突破这些限制，提升预测的安全性和泛化能力。

Method: 文章梳理了三大核心方法：一是轨迹-语言映射，将轨迹预测与自然语言信息结合；二是多模态融合，综合视觉与语义信息增强理解能力；三是基于约束的推理，利用规则优化预测结果。同时梳理了面向车辆和行人的预测任务、评估体系及数据集分析。

Result: LFMs，尤其是LLMs和MLLMs，通过整合语言与场景语义，实现了更具可解释性的上下文推理，有效提升在复杂环境下的预测安全性和泛化能力。

Conclusion: LFMs为轨迹预测带来深刻变革，但仍需应对计算延迟、数据稀缺和真实环境鲁棒性等挑战。未来方向包括低延迟推理、因果建模和运动基础模型的研发。

Abstract: Trajectory prediction serves as a critical functionality in autonomous
driving, enabling the anticipation of future motion paths for traffic
participants such as vehicles and pedestrians, which is essential for driving
safety. Although conventional deep learning methods have improved accuracy,
they remain hindered by inherent limitations, including lack of
interpretability, heavy reliance on large-scale annotated data, and weak
generalization in long-tail scenarios. The rise of Large Foundation Models
(LFMs) is transforming the research paradigm of trajectory prediction. This
survey offers a systematic review of recent advances in LFMs, particularly
Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) for
trajectory prediction. By integrating linguistic and scene semantics, LFMs
facilitate interpretable contextual reasoning, significantly enhancing
prediction safety and generalization in complex environments. The article
highlights three core methodologies: trajectory-language mapping, multimodal
fusion, and constraint-based reasoning. It covers prediction tasks for both
vehicles and pedestrians, evaluation metrics, and dataset analyses. Key
challenges such as computational latency, data scarcity, and real-world
robustness are discussed, along with future research directions including
low-latency inference, causality-aware modeling, and motion foundation models.

</details>


### [232] [STL-Based Motion Planning and Uncertainty-Aware Risk Analysis for Human-Robot Collaboration with a Multi-Rotor Aerial Vehicle](https://arxiv.org/abs/2509.10692)
*Giuseppe Silano,Amr Afifi,Martin Saska,Antonio Franchi*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的多旋翼飞行器（MRAV）运动规划与风险分析方法，以提升人机协作的安全与效率。该方法结合了信号时序逻辑（STL）、优化框架与不确定性分析，并在仿真实验中验证了效果。


<details>
  <summary>Details</summary>
Motivation: 在需要人机协作的任务（如电力巡检）中，提升多旋翼飞行器协作的安全性、效率与人性化体验是当前亟需解决的问题。尤其是在安全、舒适度与突发情况响应方面，现有方法存在不足。

Method: 采用信号时序逻辑（STL）对任务目标（如安全、时间、人类偏好等）进行编码，通过优化框架生成符合动力学约束的轨迹。运用平滑近似和梯度优化处理非线性与非凸性问题，引入不确定性感知风险分析以评估任务执行的可靠性，并结合事件驱动的重规划策略应对突发状况。

Result: 通过MATLAB与Gazebo仿真，在模拟电力维护场景下的人机递交物体任务中验证，方法能有效实现安全、高效且具韧性的人机协作。

Conclusion: 本文方法在实际任务场景中表现出优良的人机协作能力，提升了安全性与适应性，对多旋翼飞行器协同任务具有重要应用价值。

Abstract: This paper presents a novel approach to motion planning and risk analysis for
enhancing human-robot collaboration using a Multi-Rotor Aerial Vehicle (MRAV).
The proposed method uses Signal Temporal Logic (STL) to encode key mission
objectives, such as safety, timing, and human preferences, with a strong focus
on ergonomics and comfort. An optimization framework generates dynamically
feasible trajectories while considering the MRAV's physical constraints. Given
the nonlinear and non-convex nature of the problem, smooth approximations and
gradient-based techniques assist in handling the problem's computational
complexity. Additionally, an uncertainty-aware risk analysis is incorporated to
assess potential deviations from the mission specifications, providing insights
into the likelihood of mission success under uncertain conditions. Further, an
event-triggered replanning strategy is implemented to respond to unforeseen
events and external disturbances. The approach is validated through MATLAB and
Gazebo simulations, using an object handover task in a mock-up environment
inspired by power line maintenance scenarios. The results highlight the
method's effectiveness in achieving safe, efficient, and resilient human-robot
collaboration.

</details>


### [233] [A Survey on LiDAR-based Autonomous Aerial Vehicles](https://arxiv.org/abs/2509.10730)
*Yunfan Ren,Yixi Cai,Haotian Li,Nan Chen,Fangcheng Zhu,Longji Yin,Fanze Kong,Rundong Li,Fu Zhang*

Main category: cs.RO

TL;DR: 本文综述了基于激光雷达（LiDAR）的无人机（UAV）在自主导航领域的最新进展，包括设计、感知、规划和控制技术，重点强调了其在多种复杂环境中的应用与挑战。


<details>
  <summary>Details</summary>
Motivation: 近年来，随着高精度导航需求的增加，尤其是在GPS信号受限的环境中，传统视觉与导航传感器已难以满足可靠性与精度要求。激光雷达以其高精度、远距离和环境鲁棒性，成为提升无人机自主性的关键技术。作者希望为该领域的研究者和工程师梳理最新进展和实践应用。

Method: 对激光雷达传感器的发展及其与无人机的集成进行总结，系统梳理了感知（如状态估计与建图）、轨迹规划、控制方法，并从软件与硬件两方面深入分析。进一步探讨了激光雷达无人机在工业、异构平台与集群操作方面的实际应用，并对当前挑战和未来研究方向提出见解。

Result: 激光雷达与无人机集成显著提升了无人机在复杂、动态及无GPS环境下的自主导航能力。文中归纳了多种感知与控制算法的最新成果，并展示了激光雷达无人机在工业作业、多机协同等领域的广泛应用前景。

Conclusion: 尽管激光雷达无人机取得了显著进展，当前仍面临如成本、重量、实时性、环境适应性、多平台协同等挑战。未来需在提升传感器性能、智能算法、多无人机协同等方向持续攻关。本文为相关研究与工程实践提供了系统参考。

Abstract: This survey offers a comprehensive overview of recent advancements in
LiDAR-based autonomous Unmanned Aerial Vehicles (UAVs), covering their design,
perception, planning, and control strategies. Over the past decade, LiDAR
technology has become a crucial enabler for high-speed, agile, and reliable UAV
navigation, especially in GPS-denied environments. The paper begins by
examining the evolution of LiDAR sensors, emphasizing their unique advantages
such as high accuracy, long-range depth measurements, and robust performance
under various lighting conditions, making them particularly well-suited for UAV
applications. The integration of LiDAR with UAVs has significantly enhanced
their autonomy, enabling complex missions in diverse and challenging
environments. Subsequently, we explore essential software components, including
perception technologies for state estimation and mapping, as well as trajectory
planning and control methodologies, and discuss their adoption in LiDAR-based
UAVs. Additionally, we analyze various practical applications of the
LiDAR-based UAVs, ranging from industrial operations to supporting different
aerial platforms and UAV swarm deployments. The survey concludes by discussing
existing challenges and proposing future research directions to advance
LiDAR-based UAVs and enhance multi-UAV collaboration. By synthesizing recent
developments, this paper aims to provide a valuable resource for researchers
and practitioners working to push the boundaries of LiDAR-based UAV systems.

</details>


### [234] [Analytical Design and Development of a Modular and Intuitive Framework for Robotizing and Enhancing the Existing Endoscopic Procedures](https://arxiv.org/abs/2509.10735)
*Mohammad Rafiee Javazm,Yash Kulkarni,Jiaqi Xue,Naruhiko Ikoma,Farshid Alambeigi*

Main category: cs.RO

TL;DR: 本文提出一种模块化、易于安装的机械电子系统，用于增强现有内窥镜设备的操作，降低临床医生负担，提高操作效率。


<details>
  <summary>Details</summary>
Motivation: 内窥镜设备在癌症筛查中应用广泛，但目前手动操作困难，导致临床医生疲劳、分心及工作量增加，因此需要改进操作方式。

Method: 设计了一套包括新颖的嵌套夹持机构（用于控制内窥镜弯曲）、送入/回撤机构（控制插入/回撤自由度）以及直观用户界面的模块化机械电子系统。引入数学建模和参数优化设计空间，并通过仿真和实验对系统进行验证。

Result: 仿真和实验结果显示，提出的数学模型及机器人平台能够有效、全面地提升内窥镜操作性能。

Conclusion: 该研究为现有内窥镜设备提供了易于集成与装配的自动化控制方案，可显著降低医务人员的操作难度，提升癌症筛查过程的效率和安全性。

Abstract: Despite the widespread adoption of endoscopic devices for several cancer
screening procedures, manual control of these devices still remains challenging
for clinicians, leading to several critical issues such as increased workload,
fatigue, and distractions. To address these issues, in this paper, we introduce
the design and development of an intuitive, modular, and easily installable
mechatronic framework. This framework includes (i) a novel nested collet-chuck
gripping mechanism that can readily be integrated and assembled with the
existing endoscopic devices and control their bending degrees-of-freedom
(DoFs); (ii) a feeder mechanism that can control the insertion/retraction DoF
of a colonoscope, and (iii) a complementary and intuitive user interface that
enables simultaneous control of all DoFs during the procedure. To analyze the
design of the proposed mechanisms, we also introduce a mathematical modeling
approach and a design space for optimal selection of the parameters involved in
the design of gripping and feeder mechanisms. Our simulation and experimental
studies thoroughly demonstrate the performance of the proposed mathematical
modeling and robotic framework.

</details>


### [235] [FastTrack: GPU-Accelerated Tracking for Visual SLAM](https://arxiv.org/abs/2509.10757)
*Kimia Khabiri,Parsa Hosseininejad,Shishir Gopinath,Karthik Dantu,Steven Y. Ko*

Main category: cs.RO

TL;DR: 本文提出利用GPU加速视觉-惯性SLAM系统中耗时的跟踪部分，显著提升了跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 视觉-惯性SLAM系统的跟踪模块需在每帧内及时处理图像和IMU数据，否则会导致定位不准或跟踪丢失。因此，提高跟踪效率对于实际应用至关重要。

Method: 作者用CUDA实现GPU并行加速，对ORB-SLAM3中的立体特征匹配和本地地图跟踪等耗时模块进行优化。

Result: 在桌面和Jetson Xavier NX板上进行的实验表明，在stereo-inertial模式下处理EuRoC和TUM-VI数据集时，整体跟踪性能提升高达2.8倍。

Conclusion: 采用GPU对SLAM跟踪模块加速能够大幅提升性能，为实际部署提供了更高效的解决方案。

Abstract: The tracking module of a visual-inertial SLAM system processes incoming image
frames and IMU data to estimate the position of the frame in relation to the
map. It is important for the tracking to complete in a timely manner for each
frame to avoid poor localization or tracking loss. We therefore present a new
approach which leverages GPU computing power to accelerate time-consuming
components of tracking in order to improve its performance. These components
include stereo feature matching and local map tracking. We implement our design
inside the ORB-SLAM3 tracking process using CUDA. Our evaluation demonstrates
an overall improvement in tracking performance of up to 2.8x on a desktop and
Jetson Xavier NX board in stereo-inertial mode, using the well-known SLAM
datasets EuRoC and TUM-VI.

</details>


### [236] [RSL-RL: A Learning Library for Robotics Research](https://arxiv.org/abs/2509.10771)
*Clemens Schwarke,Mayank Mittal,Nikita Rudin,David Hoeller,Marco Hutter*

Main category: cs.RO

TL;DR: RSL-RL 是一个为机器人领域打造的开源强化学习库，注重简洁易扩展，并针对机器人特有问题做了优化，支持高效GPU训练，效果已被仿真和实际机器人任务验证。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习库多为通用性框架，代码复杂且难以调整，不适合机器人领域研究中对灵活性和适用性的特殊需求。

Method: RSL-RL 设计上突出精简和易于修改，实现了主流适用于机器人的强化学习算法，并支持机器人特定辅助技术。库仅支持GPU训练，保证高性能；同时通过开源方式促进扩展。

Result: RSL-RL 在大型仿真环境和真实机器人实验中均取得了良好的表现，展现出高效、轻量和实用的特点。

Conclusion: RSL-RL 为机器人领域的强化学习研究和开发提供了一个高效、可扩展、适用于实际任务的工具。

Abstract: RSL-RL is an open-source Reinforcement Learning library tailored to the
specific needs of the robotics community. Unlike broad general-purpose
frameworks, its design philosophy prioritizes a compact and easily modifiable
codebase, allowing researchers to adapt and extend algorithms with minimal
overhead. The library focuses on algorithms most widely adopted in robotics,
together with auxiliary techniques that address robotics-specific challenges.
Optimized for GPU-only training, RSL-RL achieves high-throughput performance in
large-scale simulation environments. Its effectiveness has been validated in
both simulation benchmarks and in real-world robotic experiments, demonstrating
its utility as a lightweight, extensible, and practical framework to develop
learning-based robotic controllers. The library is open-sourced at:
https://github.com/leggedrobotics/rsl_rl.

</details>


### [237] [Follow-Bench: A Unified Motion Planning Benchmark for Socially-Aware Robot Person Following](https://arxiv.org/abs/2509.10796)
*Hanjing Ye,Weixi Situ,Jianwei Peng,Yu Zhan,Bingyi Xia,Kuanqi Cai,Hong Zhang*

Main category: cs.RO

TL;DR: 本论文全面研究了机器人跟随人与辅助（RPF）技术，从评估方法到实际部署，全方位分析了该领域的挑战与进展。


<details>
  <summary>Details</summary>
Motivation: 机器人跟随与辅助有广泛应用前景，如个人助理、安全巡逻、养老和物流。为实现这些应用，机器人不仅要精准跟随目标人，还要兼顾目标与周围人的安全与舒适。当前RPF研究缺乏系统性评估及标准化测试，尤其在安全与舒适性方面亟需深入分析。

Method: （1）梳理分析了RPF的应用场景、运动规划方法和评估指标，重点关注安全与舒适性；（2）提出了Follow-Bench统一仿真基准，涵盖多样目标轨迹、动态人群流动和环境布局；（3）重新实现并统一评测了六种主流RPF规划器，重点评估安全与舒适权衡；（4）挑选表现最佳的两个规划器，部署于实际移动机器人上进行对比实测。

Result: 仿真和真实机器人实验系统分析了现有规划器在安全与舒适性上的权衡表现，量化了各方法的优劣，并揭示了现有方法的局限性与挑战。

Conclusion: 当前RPF方法在安全与舒适间存在权衡，现有规划器尚有提升空间。未来研究应继续优化算法，进一步提升机器人在实际环境下的跟随表现，并关注人机交互体验。

Abstract: Robot person following (RPF) -- mobile robots that follow and assist a
specific person -- has emerging applications in personal assistance, security
patrols, eldercare, and logistics. To be effective, such robots must follow the
target while ensuring safety and comfort for both the target and surrounding
people. In this work, we present the first end-to-end study of RPF, which (i)
surveys representative scenarios, motion-planning methods, and evaluation
metrics with a focus on safety and comfort; (ii) introduces Follow-Bench, a
unified benchmark simulating diverse scenarios, including various target
trajectory patterns, dynamic-crowd flows, and environmental layouts; and (iii)
re-implements six popular RPF planners, ensuring that both safety and comfort
are systematically considered. Moreover, we evaluate the two highest-performing
planners from our benchmark on a differential-drive robot to provide insights
into real-world deployment. Extensive simulation and real-world experiments
provide quantitative insights into the safety-comfort trade-offs of existing
planners, while revealing open challenges and future research directions.

</details>


### [238] [A Universal Wire Testing Machine for Enhancing the Performance of Wire-Driven Robots](https://arxiv.org/abs/2509.10862)
*Temma Suzuki,Kento Kawaharazuka,Kei Okada*

Main category: cs.RO

TL;DR: 本研究设计并实现了一台通用钢丝测试机，可以测量和调整钢丝特性，从而提升钢丝驱动机械装置的性能，并通过实际机器人实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 钢丝驱动装置具有轻量、低摩擦的优点，但由于钢丝柔性大，导致建模误差高，限制了其在工业及研究机器人领域的应用。

Method: 研究团队设计了一台通用钢丝测试机，用于消除初始拉伸、测量不同直径滑轮的张力传递效率、以及不同长度钢丝的动态特性，并将得到的数据应用于实际钢丝驱动机器人的力控制。

Result: 通过使用该测试机，获得了多种参数下的钢丝性能数据，最终通过数据补偿，降低了实际机器人末端执行器的力误差。

Conclusion: 该测试机能够提升钢丝驱动系统的性能，为钢丝驱动装置在机器人领域应用提供了数据支持和技术支撑。

Abstract: Compared with gears and linkages, wires constitute a lightweight,
low-friction transmission mechanism. However, because wires are flexible
materials, they tend to introduce large modeling errors, and their adoption in
industrial and research robots remains limited.In this study, we built a
Universal Wire Testing Machine that enables measurement and adjustment of wire
characteristics to improve the performance of wire-driven mechanisms. Using
this testing machine, we carried out removal of initial wire stretch,
measurement of tension transmission efficiency for eight different diameters of
passive pulleys, and measurement of the dynamic behavior of variable-length
wires. Finally, we applied the data obtained from this testing machine to the
force control of an actual wire-driven robot, reducing the end-effector force
error.

</details>


### [239] [Nav-R1: Reasoning and Navigation in Embodied Scenes](https://arxiv.org/abs/2509.10884)
*Qingxiang Liu,Ting Huang,Zeyu Zhang,Hao Tang*

Main category: cs.RO

TL;DR: 本文提出了Nav-R1，一种能够在3D环境中集成感知、推理和行动的通用体感导航大模型。该模型通过构建大规模推理数据集和创新的训练范式显著提升复杂环境下的导航与推理效果。


<details>
  <summary>Details</summary>
Motivation: 现有体感导航方法在复杂环境下常因推理过程不连贯或不稳定，难以泛化，并且难以兼顾长时序语义推理与实时低延迟的控制，影响了实际应用中的效果。

Method: 1. 构建大规模体感任务推理链数据集Nav-CoT-110K，实现冷启动下的结构化推理能力。2. 设计基于GRPO的强化学习框架，提出格式、理解和导航三种奖励，分别保证结果结构、语义扎根和路径真实性。3. 引入Fast-in-Slow推理范式，将高阶语义推理与低延迟控制分离，提高效率和连贯性。

Result: 在主流体感AI基准测试中，Nav-R1在推理和导航任务上平均提升8%以上，显著优于强基线模型。同时在真实机器人平台中验证了其在资源受限场景下的稳健性。

Conclusion: Nav-R1通过统一推理框架和高效计算范式，兼顾导航性能与推理能力，推动了体感智能体在复杂实时环境中的实际应用。

Abstract: Embodied navigation requires agents to integrate perception, reasoning, and
action for robust interaction in complex 3D environments. Existing approaches
often suffer from incoherent and unstable reasoning traces that hinder
generalization across diverse environments, and difficulty balancing
long-horizon semantic reasoning with low-latency control for real-time
navigation. To address these challenges, we propose Nav-R1, an embodied
foundation model that unifies reasoning in embodied environments. We first
construct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought
(CoT) for embodied tasks, which enables cold-start initialization with
structured reasoning. Building on this foundation, we design a GRPO-based
reinforcement learning framework with three complementary rewards: format,
understanding, and navigation, to improve structural adherence, semantic
grounding, and path fidelity. Furthermore, we introduce a Fast-in-Slow
reasoning paradigm, decoupling deliberate semantic reasoning from low-latency
reactive control for efficient yet coherent navigation. Extensive evaluations
on embodied AI benchmarks demonstrate that Nav-R1 consistently outperforms
strong baselines, with over 8% average improvement in reasoning and navigation
performance. Real-world deployment on a mobile robot further validates its
robustness under limited onboard resources. Code:
https://github.com/AIGeeksGroup/Nav-R1. Website:
https://aigeeksgroup.github.io/Nav-R1.

</details>


### [240] [Design of scalable orthogonal digital encoding architecture for large-area flexible tactile sensing in robotics](https://arxiv.org/abs/2509.10888)
*Weijie Liu,Ziyi Qiu,Shihang Wang,Deqing Mei,Yancheng Wang*

Main category: cs.RO

TL;DR: 该论文提出了一种基于码分多址(CDMA)思想的新型分布式触觉感知架构，有效简化了大面积柔性电子皮肤传感的信号连线，能实现高灵敏度和低延迟的人类皮肤级触觉感知。


<details>
  <summary>Details</summary>
Motivation: 现有柔性触觉传感器在实现大面积、高分辨率、快速响应的人类皮肤类似性能方面，面临信号编码效率低和线路布线复杂等难题，严重限制了可扩展性和实时性能。

Method: 作者提出了一种受CDMA启发的正交数字编码架构，通过分布式节点发送能量正交基码，可并行叠加信号，大幅降低连线数量，提高数据吞吐，用单根线实现多节点压力分布实时重建。

Result: 用16节点现成传感阵列实验验证，仅用一根线即可实现12.8ms的时间分辨率，同时能在节点数量极大变化（千节点级）时保持亚20ms延迟。

Conclusion: 本文架构通过根本性重新定义软电子的信号编码范式，为开发具人类皮肤般感知能力、可大规模扩展的智能系统开辟了新方向。

Abstract: Human-like embodied tactile perception is crucial for the next-generation
intelligent robotics. Achieving large-area, full-body soft coverage with high
sensitivity and rapid response, akin to human skin, remains a formidable
challenge due to critical bottlenecks in encoding efficiency and wiring
complexity in existing flexible tactile sensors, thus significantly hinder the
scalability and real-time performance required for human skin-level tactile
perception. Herein, we present a new architecture employing code division
multiple access-inspired orthogonal digital encoding to overcome these
challenges. Our decentralized encoding strategy transforms conventional serial
signal transmission by enabling parallel superposition of energy-orthogonal
base codes from distributed sensing nodes, drastically reducing wiring
requirements and increasing data throughput. We implemented and validated this
strategy with off-the-shelf 16-node sensing array to reconstruct the pressure
distribution, achieving a temporal resolution of 12.8 ms using only a single
transmission wire. Crucially, the architecture can maintain sub-20ms latency
across orders-of-magnitude variations in node number (to thousands of nodes).
By fundamentally redefining signal encoding paradigms in soft electronics, this
work opens new frontiers in developing scalable embodied intelligent systems
with human-like sensory capabilities.

</details>


### [241] [ViSTR-GP: Online Cyberattack Detection via Vision-to-State Tensor Regression and Gaussian Processes in Automated Robotic Operations](https://arxiv.org/abs/2509.10948)
*Navid Aftabi,Philip Samaha,Jin Ma,Long Cheng,Ramy Harik,Dan Li*

Main category: cs.RO

TL;DR: 本文提出了一种通过视觉通道实时检测机器人制造流程中数据完整性攻击的新框架ViSTR-GP，并通过真实机器人实验验证其检测性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 随着智能制造与自动化工厂的发展，工业机器人系统在提升生产效率的同时，面临着日益严重的网络安全风险。其中数据完整性攻击难以通过现有检测手段发现，亟需更有效的检测方法。

Method: 提出ViSTR-GP检测框架，将编码器测量值与独立的视觉通道（如机顶摄像头）估测结果进行对比。方法包括SAM-Track分割初始化、张量回归测量映射，以及矩阵高斯过程处理残差数据，并基于预测分布计算检测统计量实现在线检测。

Result: 在真实机器人平台上，同步采集视频及编码器数据并设计重复正常与攻击（重放）场景测试。结果显示所提框架不仅能精确恢复关节角度，还能比所有对比方法更早、更高频率地报警，尤其在微小攻击下优势更显著。

Conclusion: 通过增设独立的物理视觉通道并绕过控制器权限，无需复杂仪器，工厂即可有效检测数据完整性攻击，提升生产系统安全性。

Abstract: Industrial robotic systems are central to automating smart manufacturing
operations. Connected and automated factories face growing cybersecurity risks
that can potentially cause interruptions and damages to physical operations.
Among these attacks, data-integrity attacks often involve sophisticated
exploitation of vulnerabilities that enable an attacker to access and
manipulate the operational data and are hence difficult to detect with only
existing intrusion detection or model-based detection. This paper addresses the
challenges in utilizing existing side-channels to detect data-integrity attacks
in robotic manufacturing processes by developing an online detection framework,
ViSTR-GP, that cross-checks encoder-reported measurements against a
vision-based estimate from an overhead camera outside the controller's
authority. In this framework, a one-time interactive segmentation initializes
SAM-Track to generate per-frame masks. A low-rank tensor-regression surrogate
maps each mask to measurements, while a matrix-variate Gaussian process models
nominal residuals, capturing temporal structure and cross-joint correlations. A
frame-wise test statistic derived from the predictive distribution provides an
online detector with interpretable thresholds. We validate the framework on a
real-world robotic testbed with synchronized video frame and encoder data,
collecting multiple nominal cycles and constructing replay attack scenarios
with graded end-effector deviations. Results on the testbed indicate that the
proposed framework recovers joint angles accurately and detects data-integrity
attacks earlier with more frequent alarms than all baselines. These
improvements are most evident in the most subtle attacks. These results show
that plants can detect data-integrity attacks by adding an independent physical
channel, bypassing the controller's authority, without needing complex
instrumentation.

</details>


### [242] [ImMimic: Cross-Domain Imitation from Human Videos via Mapping and Interpolation](https://arxiv.org/abs/2509.10952)
*Yangcen Liu,Woo Chul Shin,Yunhai Han,Zhenyang Chen,Harish Ravichandar,Danfei Xu*

Main category: cs.RO

TL;DR: 本论文提出ImMimic方法，通过结合人类视频和少量机器人示范，实现跨领域学习，提升机器人操作的成功率和灵活性。


<details>
  <summary>Details</summary>
Motivation: 从海量人类视频中学习机器人操作可显著降低收集机器人专属数据的成本，但受限于视觉、形态和物理等领域间的差异，直接模仿存在较大障碍。

Method: 设计了ImMimic框架，利用动态时间规整（DTW）对人手与机器人关节进行动作或视觉映射，并采用MixUp插值技术在人类与机器人轨迹间生成中间域，辅助联合训练，实现更平滑的领域适应。

Result: 在4种真实世界操作任务与4类机器人形态上实验显示，ImMimic显著提升了任务成功率和执行的平滑度。

Conclusion: ImMimic能够有效弥合领域差距，实现鲁棒的机器人操作学习，展示了利用人类视频结合少量机器人示范训练的巨大潜力。

Abstract: Learning robot manipulation from abundant human videos offers a scalable
alternative to costly robot-specific data collection. However, domain gaps
across visual, morphological, and physical aspects hinder direct imitation. To
effectively bridge the domain gap, we propose ImMimic, an embodiment-agnostic
co-training framework that leverages both human videos and a small amount of
teleoperated robot demonstrations. ImMimic uses Dynamic Time Warping (DTW) with
either action- or visual-based mapping to map retargeted human hand poses to
robot joints, followed by MixUp interpolation between paired human and robot
trajectories. Our key insights are (1) retargeted human hand trajectories
provide informative action labels, and (2) interpolation over the mapped data
creates intermediate domains that facilitate smooth domain adaptation during
co-training. Evaluations on four real-world manipulation tasks (Pick and Place,
Push, Hammer, Flip) across four robotic embodiments (Robotiq, Fin Ray, Allegro,
Ability) show that ImMimic improves task success rates and execution
smoothness, highlighting its efficacy to bridge the domain gap for robust robot
manipulation. The project website can be found at
https://sites.google.com/view/immimic.

</details>


### [243] [Pogosim -- a Simulator for Pogobot robots](https://arxiv.org/abs/2509.10968)
*Leo Cazenille,Loona Macabre,Nicolas Bredeche*

Main category: cs.RO

TL;DR: Pogobots是一种专为群体机器人研究设计的开放源代码/硬件机器人。本文提出了其专用仿真器Pogosim，能够大幅降低算法开发成本，实现代码无缝切换仿真与真实机器人。


<details>
  <summary>Details</summary>
Motivation: 群体机器人研究中，直接在机器人上测试分布式算法成本高、不易扩展，限制了复杂问题的研究和用户代码参数标定。

Method: 设计了Pogosim仿真平台，使用户能用同样的代码在仿真与真实机器人间切换。详细说明了Pogosim的软件架构、配置文件与用户程序写法，以及仿真与实验的对应关系，还介绍了并行仿真、结果分析和参数优化的功能。

Result: Pogosim能够高效、可扩展地进行大规模仿真，简化了实验流程，支持多仿真并行、自动分析与参数优化，实现了算法开发流程自动化。

Conclusion: Pogosim显著降低了Pogobots群体机器人算法开发和实验的门槛，对推动群体机器人研究和应用具有积极作用。

Abstract: Pogobots are a new type of open-source/open-hardware robots specifically
designed for swarm robotics research. Their cost-effective and modular design,
complemented by vibration-based and wheel-based locomotion, fast infrared
communication and extensive software architecture facilitate the implementation
of swarm intelligence algorithms. However, testing even simple distributed
algorithms directly on robots is particularly labor-intensive. Scaling to more
complex problems or calibrate user code parameters will have a prohibitively
high strain on available resources. In this article we present Pogosim, a fast
and scalable simulator for Pogobots, designed to reduce as much as possible
algorithm development costs. The exact same code will be used in both
simulation and to experimentally drive real robots. This article details the
software architecture of Pogosim, explain how to write configuration files and
user programs and how simulations approximate or differ from experiments. We
describe how a large set of simulations can be launched in parallel, how to
retrieve and analyze the simulation results, and how to optimize user code
parameters using optimization algorithms.

</details>


### [244] [Autonomous Close-Proximity Photovoltaic Panel Coating Using a Quadcopter](https://arxiv.org/abs/2509.10979)
*Dimitri Jacquemont,Carlo Bosio,Teaya Yang,Ruiqi Zhang,Ozgur Orun,Shuai Li,Reza Alam,Thomas M. Schutzius,Simo A. Makiharju,Mark W. Mueller*

Main category: cs.RO

TL;DR: 本文提出了一种基于四旋翼无人机的自动化光伏面板涂层系统，通过自身携带的液体喷涂装置和自主定位技术，实现高效低成本的面板维护，并通过室内外实验验证了系统性能。


<details>
  <summary>Details</summary>
Motivation: 光伏面板性能提升所带来的经济和能源效益巨大，但防反射和自清洁涂层会随时间退化，需要反复人工维护，效率低且成本高。为解决维护频繁和人工成本高的问题，研究团队希望开发更自动化且灵活的维护方式。

Method: 本文提出一种配备液体分散机构的四旋翼无人机系统。该系统仅依赖自主携带的视觉-惯性里程计和基于相对位置感知的定位技术，无需外部定位系统。控制器采用基于模型的方法，考虑了飞行时地面效应以及喷涂过程中无人机质量的变化。

Result: 通过在室内和室外进行的大量实验，验证了该无人机系统在定位、自主控制和喷涂操作中的有效性和自主性。系统能够按照预定任务完成防护涂层喷涂，表现出了良好的鲁棒性。

Conclusion: 四旋翼无人机结合高效的定位与自主控制技术，能有效自动为光伏面板喷涂功能性涂层，提高维护效率、降低成本，对光伏产业的可持续性和自动化维护具有积极意义。

Abstract: Photovoltaic (PV) panels are becoming increasingly widespread in the domain
of renewable energy, and thus, small efficiency gains can have massive effects.
Anti-reflective and self-cleaning coatings enhance panel performance but
degrade over time, requiring periodic reapplication. Uncrewed Aerial Vehicles
(UAVs) offer a flexible and autonomous way to apply protective coatings more
often and at lower cost compared to traditional manual coating methods. In this
letter, we propose a quadcopter-based system, equipped with a liquid dispersion
mechanism, designed to automate such tasks. The localization stack only uses
onboard sensors, relying on visual-inertial odometry and the relative position
of the PV panel detected with respect to the quadcopter. The control relies on
a model-based controller that accounts for the ground effect and the mass
decrease of the quadcopter during liquid dispersion. We validate the autonomy
capabilities of our system through extensive indoor and outdoor experiments.

</details>


### [245] [Multi-objective task allocation for electric harvesting robots: a hierarchical route reconstruction approach](https://arxiv.org/abs/2509.11025)
*Peng Chen,Jing Liang,Hui Song,Kang-Jia Qiao,Cai-Tong Yue,Kun-Jie Yu,Ponnuthurai Nagaratnam Suganthan,Witold Pedrycz*

Main category: cs.RO

TL;DR: 本论文提出了一种针对多电动机器人在农业采摘中的任务分配与调度问题（AMERTA），兼顾总作业时间与能耗，并考虑实际中的速度与电池等约束，创新性地提出了HRRA算法，并实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 农业劳动力成本上升推动多机器人系统在果园采摘中的应用，但高效协同面临实际约束条件下作业时间和能耗的双重难题，现有方法对此考虑不足。

Method: 提出了多目标农业多电机机器人任务分配问题（AMERTA），引入了层次化编码结构、双阶段初始化、任务序列优化及专用路径重构算子，设计了HRRA（混合层次化路径重构）算法，并与七种前沿算法进行比较测试。

Result: 在45组数据集上进行了广泛实验，HRRA在多指标表现和可探索解空间方面优于现有方法。采用Wilcoxon符号秩检验和Friedman测试进一步验证了其竞争力。

Conclusion: 论文不仅提出了新颖的问题建模与高效算法，还推进了多机器人协同理论和农业自动化的实际应用，为实际场景下的机器人调度提供了有力支持。

Abstract: The increasing labor costs in agriculture have accelerated the adoption of
multi-robot systems for orchard harvesting. However, efficiently coordinating
these systems is challenging due to the complex interplay between makespan and
energy consumption, particularly under practical constraints like
load-dependent speed variations and battery limitations. This paper defines the
multi-objective agricultural multi-electrical-robot task allocation (AMERTA)
problem, which systematically incorporates these often-overlooked real-world
constraints. To address this problem, we propose a hybrid hierarchical route
reconstruction algorithm (HRRA) that integrates several innovative mechanisms,
including a hierarchical encoding structure, a dual-phase initialization
method, task sequence optimizers, and specialized route reconstruction
operators. Extensive experiments on 45 test instances demonstrate HRRA's
superior performance against seven state-of-the-art algorithms. Statistical
analysis, including the Wilcoxon signed-rank and Friedman tests, empirically
validates HRRA's competitiveness and its unique ability to explore previously
inaccessible regions of the solution space. In general, this research
contributes to the theoretical understanding of multi-robot coordination by
offering a novel problem formulation and an effective algorithm, thereby also
providing practical insights for agricultural automation.

</details>


### [246] [FEWT: Improving Humanoid Robot Perception with Frequency-Enhanced Wavelet-based Transformers](https://arxiv.org/abs/2509.11109)
*Jiaxin Huang,Hanyu Liu,Yunsheng Ma,Jian Shen,Yilin Zheng,Jiayi Wen,Baishu Wan,Pan Li,Zhigong Song*

Main category: cs.RO

TL;DR: 本文提出了一种结合人形机器人与仿生外骨骼遥操作舱的硬件平台，并基于此提出了频域增强小波变换Transformer（FEWT）模仿学习框架，显著提升了机器人动作学习的表现。


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人的感知和模仿学习能力受限，缺乏高效、直观收集人类动作数据的平台。提升机器人在真实和仿真环境中的任务执行成功率成为关键需求。

Method: 构建了一个集成人形机器人与外骨骼遥操作舱的硬件平台，实现直观远程操作与高效动作数据采集。提出了FEWT框架，包括频域增强高效多尺度注意力（FE-EMA）和时序离散小波变换（TS-DWT）两大模块，动态融合时域与频域特征以提升鲁棒性和感知表达能力。

Result: FEWT在仿真环境下将动作分块Transformer算法（ACT）的成功率提升最高达30%，在真实环境下提升6-12%。

Conclusion: 所提硬件平台与FEWT模仿学习框架可有效提升人形机器人对复杂动作的学习与执行能力，为机器人智能感知与控制提供新方案。

Abstract: The embodied intelligence bridges the physical world and information space.
As its typical physical embodiment, humanoid robots have shown great promise
through robot learning algorithms in recent years. In this study, a hardware
platform, including humanoid robot and exoskeleton-style teleoperation cabin,
was developed to realize intuitive remote manipulation and efficient collection
of anthropomorphic action data. To improve the perception representation of
humanoid robot, an imitation learning framework, termed Frequency-Enhanced
Wavelet-based Transformer (FEWT), was proposed, which consists of two primary
modules: Frequency-Enhanced Efficient Multi-Scale Attention (FE-EMA) and
Time-Series Discrete Wavelet Transform (TS-DWT). By combining multi-scale
wavelet decomposition with the residual network, FE-EMA can dynamically fuse
features from both time-domain and frequency-domain. This fusion is able to
capture feature information across various scales effectively, thereby
enhancing model robustness. Experimental performance demonstrates that FEWT
improves the success rate of the state-of-the-art algorithm (Action Chunking
with Transformers, ACT baseline) by up to 30% in simulation and by 6-12% in
real-world.

</details>


### [247] [ManiVID-3D: Generalizable View-Invariant Reinforcement Learning for Robotic Manipulation via Disentangled 3D Representations](https://arxiv.org/abs/2509.11125)
*Zheng Li,Pei Qu,Yufei Jia,Shihui Zhou,Haizhou Ge,Jiahang Cao,Jinni Zhou,Guyue Zhou,Jun Ma*

Main category: cs.RO

TL;DR: 本文提出了ManiVID-3D，一种适用于机器人操作的视觉强化学习3D架构，能够通过自监督学习获得视角无关的表示，大幅提升在不同摄像头视角下的任务成功率。


<details>
  <summary>Details</summary>
Motivation: 在现实机器人操作场景中，摄像头视角变化普遍存在，导致基于视觉强化学习训练的策略在视角变化后效果显著下降。现有方法需精确标定摄像头或应对较难的大幅透视变化，实际部署困难。

Method: 设计了ManiVID-3D这一架构，通过自监督方式学习解耦的特征表示，使得来自不同视角的点云数据能自动对齐到统一空间坐标系。提出了ViewNet模块，无需外参标定即可完成多视角对齐。此外实现了高效的GPU批量渲染模块，使3D视觉RL训练能快速进行。

Result: 在10个仿真与5个现实任务中，ManiVID-3D在视角变化条件下，相比最新方法成功率提升44.7%，且参数量减少80%。

Conclusion: ManiVID-3D能有效提升机器人操作在视角剧烈变化和现实环境中的表现，展现出极强的鲁棒性和优秀的仿真到现实迁移潜力，有助于机器人学的实际部署。

Abstract: Deploying visual reinforcement learning (RL) policies in real-world
manipulation is often hindered by camera viewpoint changes. A policy trained
from a fixed front-facing camera may fail when the camera is shifted--an
unavoidable situation in real-world settings where sensor placement is hard to
manage appropriately. Existing methods often rely on precise camera calibration
or struggle with large perspective changes. To address these limitations, we
propose ManiVID-3D, a novel 3D RL architecture designed for robotic
manipulation, which learns view-invariant representations through
self-supervised disentangled feature learning. The framework incorporates
ViewNet, a lightweight yet effective module that automatically aligns point
cloud observations from arbitrary viewpoints into a unified spatial coordinate
system without the need for extrinsic calibration. Additionally, we develop an
efficient GPU-accelerated batch rendering module capable of processing over
5000 frames per second, enabling large-scale training for 3D visual RL at
unprecedented speeds. Extensive evaluation across 10 simulated and 5 real-world
tasks demonstrates that our approach achieves a 44.7% higher success rate than
state-of-the-art methods under viewpoint variations while using 80% fewer
parameters. The system's robustness to severe perspective changes and strong
sim-to-real performance highlight the effectiveness of learning geometrically
consistent representations for scalable robotic manipulation in unstructured
environments. Our project website can be found in
https://zheng-joe-lee.github.io/manivid3d/.

</details>


### [248] [RoVerFly: Robust and Versatile Learning-based Control of Quadrotor Across Payload Configurations](https://arxiv.org/abs/2509.11149)
*Mintae Kim,Jiaze Cai,Koushil Sreenath*

Main category: cs.RO

TL;DR: 提出了一种基于强化学习的新型鲁棒四旋翼控制器RoVerFly，可适应不同的载荷和系统配置，实现高精度轨迹跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有四旋翼控制方法在面对悬挂载荷等复杂情况时，稳定性虽有理论保证，但缺乏泛化性和易于适应性，且需大量调参。实际应用中载荷质量、绳索长度经常变化，导致传统方法难以应对。

Method: 提出统一的基于强化学习（RL）控制框架，通过任务和领域随机化进行训练，强化学习策略能够适应不同的载荷情形，无需针对不同配置单独调整或切换控制器，同时保留了反馈控制器的可解释性。

Result: 所提出的RoVerFly控制器在标准四旋翼及各种悬挂载荷的配置下均能实现强大的鲁棒轨迹跟踪，实验显示其在零样本泛化能力方面优于传统方法，对扰动和动力学变化具有高度适应性。

Conclusion: RoVerFly展示了强化学习在复杂无人机系统鲁棒控制中的巨大潜力，克服了传统方法适应性差、需要频繁调参的问题，为无人机实际应用相关控制技术提供了新思路。

Abstract: Designing robust controllers for precise, arbitrary trajectory tracking with
quadrotors is challenging due to nonlinear dynamics and underactuation, and
becomes harder with flexible cable-suspended payloads that introduce extra
degrees of freedom and hybridness. Classical model-based methods offer
stability guarantees but require extensive tuning and often do not adapt when
the configuration changes, such as when a payload is added or removed, or when
the payload mass or cable length varies. We present RoVerFly, a unified
learning-based control framework in which a reinforcement learning (RL) policy
serves as a robust and versatile tracking controller for standard quadrotors
and for cable-suspended payload systems across a range of configurations.
Trained with task and domain randomization, the controller is resilient to
disturbances and varying dynamics. It achieves strong zero-shot generalization
across payload settings, including no payload as well as varying mass and cable
length, without controller switching or re-tuning, while retaining the
interpretability and structure of a feedback tracking controller. Code and
supplementary materials are available at
https://github.com/mintaeshkim/roverfly

</details>


### [249] [SAMP: Spatial Anchor-based Motion Policy for Collision-Aware Robotic Manipulators](https://arxiv.org/abs/2509.11185)
*Kai Chen,Zhihai Bi,Guoyang Zhao,Chunxin Zheng,Yulin Li,Hang Zhao,Jun Ma*

Main category: cs.RO

TL;DR: 提出了一种新的空间锚点运动策略（SAMP）用于机械臂运动规划，可更准确同时建模机器人本体和环境几何，提升了在复杂场景中的规划安全性和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有神经基础的运动规划方法很难同时精确考虑机器人本体几何和环境信息，导致在复杂或拥挤场景下碰撞检测不完整、性能下降。研究动因是提升机器人在实际应用中对复杂环境的自主运动规划能力。

Method: 提出SAMP框架，利用锚定在共享空间网格上的有符号距离场(SDF)分别编码机器人和环境，特别是采用专门的机器人SDF网络准确表征机械臂几何，并在空间锚点上融合这些信息，通过特征对齐策略训练神经运动策略网络，生成平滑、无碰撞的轨迹。

Result: 在仿真和真实场景实验中，SAMP方法比现有方法成功率提升11%，碰撞发生率降低7%。

Conclusion: 结果显示联合建模机器人和环境几何的策略显著提升运动规划性能，尤其在实际复杂环境中更具实用价值。

Abstract: Neural-based motion planning methods have achieved remarkable progress for
robotic manipulators, yet a fundamental challenge lies in simultaneously
accounting for both the robot's physical shape and the surrounding environment
when generating safe and feasible motions. Moreover, existing approaches often
rely on simplified robot models or focus primarily on obstacle representation,
which can lead to incomplete collision detection and degraded performance in
cluttered scenes. To address these limitations, we propose spatial anchor-based
motion policy (SAMP), a unified framework that simultaneously encodes the
environment and the manipulator using signed distance field (SDF) anchored on a
shared spatial grid. SAMP incorporates a dedicated robot SDF network that
captures the manipulator's precise geometry, enabling collision-aware reasoning
beyond coarse link approximations. These representations are fused on spatial
anchors and used to train a neural motion policy that generates smooth,
collision-free trajectories in the proposed efficient feature alignment
strategy. Experiments conducted in both simulated and real-world environments
consistently show that SAMP outperforms existing methods, delivering an 11%
increase in success rate and a 7% reduction in collision rate. These results
highlight the benefits of jointly modelling robot and environment geometry,
demonstrating its practical value in challenging real-world environments.

</details>


### [250] [DreamNav: A Trajectory-Based Imaginative Framework for Zero-Shot Vision-and-Language Navigation](https://arxiv.org/abs/2509.11197)
*Yunheng Wang,Yuetong Fang,Taowen Wang,Yixiao Feng,Yawen Tan,Shuning Zhang,Peiran Liu,Yiding Ji,Renjing Xu*

Main category: cs.RO

TL;DR: 本文提出DreamNav，实现了在视觉-语言导航（VLN-CE）任务中，不依赖任务训练的零样本导航，结合了轨迹级规划和主动想象，并显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前零样本VLN方法依赖高昂的感知与场景理解，且控制仅仅是点级选择，导致部署成本高、语义对齐差和规划短视。作者希望通过提升感知效率、改善规划粒度与赋能预测能力，提升VLN系统的实际应用效果。

Method: DreamNav包括三个关键模块：1）EgoView Corrector用于校正视角和稳定自我感知，降低感知成本；2）Trajectory Predictor实现轨迹层面的全局规划，更好地与指令语义对齐；3）Imagination Predictor赋予代理前瞻性思考能力，实现主动、长远的规划。

Result: 在VLN-CE以及真实环境实验中，DreamNav在零样本场景下创下新SOTA。在SR（成功率）和SPL（路径长度加权成功率）指标上，分别比最强基线高出7.49%和18.15%。

Conclusion: DreamNav首次将轨迹层级规划与主动想象结合，并且仅用自我视角输入，在零样本VLN任务上取得了显著领先。这对机器人现实世界导航任务具有重要意义。

Abstract: Vision-and-Language Navigation in Continuous Environments (VLN-CE), which
links language instructions to perception and control in the real world, is a
core capability of embodied robots. Recently, large-scale pretrained foundation
models have been leveraged as shared priors for perception, reasoning, and
action, enabling zero-shot VLN without task-specific training. However,
existing zero-shot VLN methods depend on costly perception and passive scene
understanding, collapsing control to point-level choices. As a result, they are
expensive to deploy, misaligned in action semantics, and short-sighted in
planning. To address these issues, we present DreamNav that focuses on the
following three aspects: (1) for reducing sensory cost, our EgoView Corrector
aligns viewpoints and stabilizes egocentric perception; (2) instead of
point-level actions, our Trajectory Predictor favors global trajectory-level
planning to better align with instruction semantics; and (3) to enable
anticipatory and long-horizon planning, we propose an Imagination Predictor to
endow the agent with proactive thinking capability. On VLN-CE and real-world
tests, DreamNav sets a new zero-shot state-of-the-art (SOTA), outperforming the
strongest egocentric baseline with extra information by up to 7.49\% and
18.15\% in terms of SR and SPL metrics. To our knowledge, this is the first
zero-shot VLN method to unify trajectory-level planning and active imagination
while using only egocentric inputs.

</details>


### [251] [MEMBOT: Memory-Based Robot in Intermittent POMDP](https://arxiv.org/abs/2509.11225)
*Youzhi Liang,Eyan Noronha*

Main category: cs.RO

TL;DR: 本文提出了MEMBOT，一个基于模块化记忆的架构，用于应对真实机器人系统中间歇性、部分可观测环境下的控制任务，并在多项机器人操纵基准任务上显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 面对真实环境中的机器人传感器数据缺失、噪声及遮挡，传统强化学习算法在假设信息全观测下难以适应实际部分可观测场景，亟需更强健的状态推断与策略学习方法。

Method: MEMBOT采用两阶段训练流程：首先在多个任务上进行无关任务的离线预训练，训练出鲁棒的潜在信念编码器（利用重建损失）；其次在特定任务上用行为克隆微调策略。信念编码器由状态空间模型（SSM）和LSTM实现，融合时序观察与动作输入，即使在观测缺失下也能推断并维持潜在状态表示。

Result: 在MetaWorld和Robomimic的10个机器人操控基准任务中，在不同观测丢失率下，MEMBOT都优于无记忆和普通循环网络基线，尤其在50%观测可用率下仍保持80%的性能峰值。

Conclusion: 显式信念建模能显著提升强化学习在真实部分可观测环境下的鲁棒性、泛化和数据效率，为现实机器人系统的部署带来积极意义。

Abstract: Robotic systems deployed in real-world environments often operate under
conditions of partial and often intermittent observability, where sensor inputs
may be noisy, occluded, or entirely unavailable due to failures or
environmental constraints. Traditional reinforcement learning (RL) approaches
that assume full state observability are ill-equipped for such challenges. In
this work, we introduce MEMBOT, a modular memory-based architecture designed to
address intermittent partial observability in robotic control tasks. MEMBOT
decouples belief inference from policy learning through a two-phase training
process: an offline multi-task learning pretraining stage that learns a robust
task-agnostic latent belief encoder using a reconstruction losses, followed by
fine-tuning of task-specific policies using behavior cloning. The belief
encoder, implemented as a state-space model (SSM) and a LSTM, integrates
temporal sequences of observations and actions to infer latent state
representations that persist even when observations are dropped. We train and
evaluate MEMBOT on 10 robotic manipulation benchmark tasks from MetaWorld and
Robomimic under varying rates of observation dropout. Results show that MEMBOT
consistently outperforms both memoryless and naively recurrent baselines,
maintaining up to 80% of peak performance under 50% observation availability.
These findings highlight the effectiveness of explicit belief modeling in
achieving robust, transferable, and data-efficient policies for real-world
partially observable robotic systems.

</details>


### [252] [CORB-Planner: Corridor as Observations for RL Planning in High-Speed Flight](https://arxiv.org/abs/2509.11240)
*Yechen Zhang,Bin Gao,Gang Wang,Jian Sun,Zhuo Li*

Main category: cs.RO

TL;DR: 该论文提出了一种名为CORB-Planner的实时轨迹规划框架，实现了异构无人机平台在密集障碍环境中的高效自主飞行，并通过强化学习和B样条规划相结合提升了通用性和稳健性。


<details>
  <summary>Details</summary>
Motivation: 虽然强化学习在机器人控制任务上表现出色，但在无人机的真实部署中，仍然存在依赖精确动力学模型与平台专用感知方式的问题，导致跨平台迁移能力差。因此，作者希望设计一种能够减少对平台细节依赖、简化动力学建模，并提升在不同无人机上的通用性的轨迹规划方法。

Method: 论文提出融合B样条轨迹生成和强化学习策略的规划框架：通过启发式搜索得到安全飞行走廊（SFC）的低维表示，并以此为观测输入，RL策略逐步产生轨迹控制点。采用易到难的渐进仿真训练，以及分解评论家结构的Q学习（SDCQ）算法，在约十分钟训练内学得高效策略，提升系统的实际部署可能性。

Result: 实验在仿真和真实环境下均表明，CORB-Planner可以在轻量级的无人机板载计算设备上实时运行，无需外部定位，最高可支持8.2m/s的飞行速度，并成功适配四旋翼与六旋翼等多种机型，展示了良好的泛化性和鲁棒性。

Conclusion: CORB-Planner有效实现了高通用性和高稳健性的自主无人机轨迹规划，降低了对精确建模与平台感知的依赖，为各种无人机快速、可靠地在复杂环境中自主飞行提供了一种实用的方案。

Abstract: Reinforcement learning (RL) has shown promise in a large number of robotic
control tasks. Nevertheless, its deployment on unmanned aerial vehicles (UAVs)
remains challenging, mainly because of reliance on accurate dynamic models and
platform-specific sensing, which hinders cross-platform transfer. This paper
presents the CORB-Planner (Corridor-as-Observations for RL B-spline planner), a
real-time, RL-based trajectory planning framework for high-speed autonomous UAV
flight across heterogeneous platforms. The key idea is to combine B-spline
trajectory generation with the RL policy producing successive control points
with a compact safe flight corridor (SFC) representation obtained via heuristic
search. The SFC abstracts obstacle information in a low-dimensional form,
mitigating overfitting to platform-specific details and reducing sensitivity to
model inaccuracies. To narrow the sim-to-real gap, we adopt an easy-to-hard
progressive training pipeline in simulation. A value-based soft
decomposed-critic Q (SDCQ) algorithm is used to learn effective policies within
approximately ten minutes of training. Benchmarks in simulation and real-world
tests demonstrate real-time planning on lightweight onboard hardware and
support maximum flight speeds up to 8.2m/s in dense, cluttered environments
without external positioning. Compatibility with various UAV configurations
(quadrotors, hexarotors) and modest onboard compute underlines the generality
and robustness of CORB-Planner for practical deployment.

</details>


### [253] [Embodied Intelligence in Disassembly: Multimodal Perception Cross-validation and Continual Learning in Neuro-Symbolic TAMP](https://arxiv.org/abs/2509.11270)
*Ziwen He,Zhigang Wang,Yanlong Peng,Pengxu Chang,Hong Yang,Ming Chen*

Main category: cs.RO

TL;DR: 本文提出了一种基于神经符号任务与运动规划的持续学习框架，用于提升动力电池拆解过程中的机器人自主感知和自适应能力，有效解决动态环境下的稳健性难题。实验表明，该方法能大幅提升任务成功率并减少感知误判。


<details>
  <summary>Details</summary>
Motivation: 动力电池的高效拆解与回收是新能车产业及循环经济亟需解决的难题。目前实际拆解场景的不确定性限制了机器人在工业应用中的自主操作能力。

Method: 作者设计了融合多模态感知交叉验证机制的神经符号任务与运动规划（TAMP）持续学习框架。该框架包括：正向工作流动态优化动作策略，反向学习流自动采集任务执行历史数据，实现系统自优化与持续学习。

Result: 在动力电池动态拆解测试中，该框架将任务成功率从81.68%提升到100%，平均感知误判次数从3.389降至1.128。

Conclusion: 研究证明以神经符号持续学习为核心的新范式能显著提升嵌入式智能体在复杂工业环境下的鲁棒性和自适应性，对提升自动拆解与循环经济有现实指导价值。

Abstract: With the rapid development of the new energy vehicle industry, the efficient
disassembly and recycling of power batteries have become a critical challenge
for the circular economy. In current unstructured disassembly scenarios, the
dynamic nature of the environment severely limits the robustness of robotic
perception, posing a significant barrier to autonomous disassembly in
industrial applications. This paper proposes a continual learning framework
based on Neuro-Symbolic task and motion planning (TAMP) to enhance the
adaptability of embodied intelligence systems in dynamic environments. Our
approach integrates a multimodal perception cross-validation mechanism into a
bidirectional reasoning flow: the forward working flow dynamically refines and
optimizes action strategies, while the backward learning flow autonomously
collects effective data from historical task executions to facilitate continual
system learning, enabling self-optimization. Experimental results show that the
proposed framework improves the task success rate in dynamic disassembly
scenarios from 81.68% to 100%, while reducing the average number of perception
misjudgments from 3.389 to 1.128. This research provides a new paradigm for
enhancing the robustness and adaptability of embodied intelligence in complex
industrial environments.

</details>


### [254] [Policy Learning for Social Robot-Led Physiotherapy](https://arxiv.org/abs/2509.11297)
*Carl Bettosi,Lynne Ballie,Susan Shenkin,Marta Romeo*

Main category: cs.RO

TL;DR: 本论文提出利用社会机器人自主引导病人进行物理治疗训练，并通过专家代理模拟病人行为，进而训练强化学习算法，实现个性化和动态调整的指导策略。


<details>
  <summary>Details</summary>
Motivation: 部署社会机器人帮助患者进行物理治疗有很大潜力，但受限于现实中病人行为数据稀缺，难以开发出鲁棒的适应性策略。为此，研究者希望借助专家知识弥补数据不足，提升机器人自主决策能力。

Method: 通过招募33名医疗专家作为‘病人代理’与机器人互动采集数据，从行为和主观感受两个维度建立病人行为模型。随后，在仿真环境下基于强化学习训练机器人，优化其对不同病人状态的指导策略。

Result: 结果显示，训练获得的机器人能够针对病人运动能力和主观感受动态调整训练指令，同时适配不同恢复阶段和康复计划的患者。

Conclusion: 本方法有效解决了病人行为数据稀缺带来的模型训练难题，验证了用专家代理方法训练的强化学习策略可实现对多类型病人的个性化物理治疗指导。

Abstract: Social robots offer a promising solution for autonomously guiding patients
through physiotherapy exercise sessions, but effective deployment requires
advanced decision-making to adapt to patient needs. A key challenge is the
scarcity of patient behavior data for developing robust policies. To address
this, we engaged 33 expert healthcare practitioners as patient proxies, using
their interactions with our robot to inform a patient behavior model capable of
generating exercise performance metrics and subjective scores on perceived
exertion. We trained a reinforcement learning-based policy in simulation,
demonstrating that it can adapt exercise instructions to individual exertion
tolerances and fluctuating performance, while also being applicable to patients
at different recovery stages with varying exercise plans.

</details>


### [255] [Brain-Robot Interface for Exercise Mimicry](https://arxiv.org/abs/2509.11306)
*Carl Bettosi,Emilyann Nault,Lynne Baillie,Markus Garschall,Marta Romeo,Beatrix Wais-Zechmann,Nicole Binderlehner,Theodoros Georgio*

Main category: cs.RO

TL;DR: 本研究提出了一种创新的脑-机器人接口（BRI），允许社交机器人实时模仿患者的锻炼动作，以促进康复训练中的互动融洽。系统在14名参与者中测试，验证了其实时模仿能力和用户积极反馈。


<details>
  <summary>Details</summary>
Motivation: 社交机器人作为锻炼和康复指导者时，维持长期用户参与至关重要，而模仿用户肢体动作已被证实有助于建立互动亲密关系。传统康复训练中也常见病人模仿理疗师动作，因此本研究希望赋予机器人同样的能力。

Method: 研究团队开发了一套脑-机器人接口（BRI），能通过患者的意图生成心智指令，驱动机器人导师实时模仿患者的锻炼动作。随后在3位物理治疗师和11名偏瘫康复患者中进行探索性实验。

Result: 该系统在12场针对不同参与者的训练中成功实现了运动模仿，但模仿的准确性存在一定变化。总体上，参与者对机器人导师持正面态度，对其信任和接受度都很高，而BRI新技术的引入并未负面影响用户感受。

Conclusion: 本研究验证了脑-机器人接口技术能应用于社交机器人作为康复教练场景中，帮助机器人与用户更好建立互动关系，为后续优化系统精度和大规模应用奠定了基础。

Abstract: For social robots to maintain long-term engagement as exercise instructors,
rapport-building is essential. Motor mimicry--imitating one's physical
actions--during social interaction has long been recognized as a powerful tool
for fostering rapport, and it is widely used in rehabilitation exercises where
patients mirror a physiotherapist or video demonstration. We developed a novel
Brain-Robot Interface (BRI) that allows a social robot instructor to mimic a
patient's exercise movements in real-time, using mental commands derived from
the patient's intention. The system was evaluated in an exploratory study with
14 participants (3 physiotherapists and 11 hemiparetic patients recovering from
stroke or other injuries). We found our system successfully demonstrated
exercise mimicry in 12 sessions; however, accuracy varied. Participants had
positive perceptions of the robot instructor, with high trust and acceptance
levels, which were not affected by the introduction of BRI technology.

</details>


### [256] [ActivePose: Active 6D Object Pose Estimation and Tracking for Robotic Manipulation](https://arxiv.org/abs/2509.11364)
*Sheng Liu,Zhe Li,Weiheng Wang,Han Sun,Heng Zhang,Hongpeng Chen,Yusen Qin,Arash Ajoudani,Yizhao Wang*

Main category: cs.RO

TL;DR: 本文提出了一种结合视觉-语言模型(VLM)与“机器人想象力”的主动姿态估计与跟踪方法，用于提升机器人在对象姿态感知中的精度、鲁棒性和实时性。核心思想是系统可主动识别歧义并选择视角实现消疑，并通过策略生成运动轨迹，维持目标可见性，实现比传统方案更优的表现。


<details>
  <summary>Details</summary>
Motivation: 现有零样本6-DoF对象姿态估计方法在面对视角歧义、目标自遮挡或移动等问题时表现不佳，且固定视角会导致感知盲区。本文旨在解决这些主动消歧和保持目标持续可见等关键难题。

Method: 提出主动姿态感知管线，离线阶段：渲染CAD模型的多个视图，计算FoundationPose熵值，构建融合低、高熵样例的几何提示。在在线阶段：(1) 利用VLM对当前帧识别姿态歧义度；(2) 若检测到歧义，基于多候选视点渲染和加权评分（VLM概率与FoundationPose熵结合）选出最佳下一个观测视角(NBV)，并驱动机器人转向此视角实现消疑。跟踪模块基于扩散策略和模仿学习生成摄像机轨迹，确保目标可持续可见并降低姿态歧义。

Result: 在仿真和现实环境下，实验结果证明所提方法在姿态估计和跟踪准确率上均显著优于传统基线方法。

Conclusion: 主动融合视觉-语言模型、几何提示和轨迹生成思想，可有效提升机器人对象姿态感知的准确性与鲁棒性，尤其在复杂环境与目标自运动场景下具备明显优势。

Abstract: Accurate 6-DoF object pose estimation and tracking are critical for reliable
robotic manipulation. However, zero-shot methods often fail under
viewpoint-induced ambiguities and fixed-camera setups struggle when objects
move or become self-occluded. To address these challenges, we propose an active
pose estimation pipeline that combines a Vision-Language Model (VLM) with
"robotic imagination" to dynamically detect and resolve ambiguities in real
time. In an offline stage, we render a dense set of views of the CAD model,
compute the FoundationPose entropy for each view, and construct a
geometric-aware prompt that includes low-entropy (unambiguous) and high-entropy
(ambiguous) examples. At runtime, the system: (1) queries the VLM on the live
image for an ambiguity score; (2) if ambiguity is detected, imagines a discrete
set of candidate camera poses by rendering virtual views, scores each based on
a weighted combination of VLM ambiguity probability and FoundationPose entropy,
and then moves the camera to the Next-Best-View (NBV) to obtain a disambiguated
pose estimation. Furthermore, since moving objects may leave the camera's field
of view, we introduce an active pose tracking module: a diffusion-policy
trained via imitation learning, which generates camera trajectories that
preserve object visibility and minimize pose ambiguity. Experiments in
simulation and real-world show that our approach significantly outperforms
classical baselines.

</details>


### [257] [Quantum deep reinforcement learning for humanoid robot navigation task](https://arxiv.org/abs/2509.11388)
*Romerik Lokossou,Birhanu Shimelis Girma,Ozan K. Tonguz,Ahmed Biyabani*

Main category: cs.RO

TL;DR: 本文提出利用量子深度强化学习（QDRL）用于高维复杂环境下的人形机器人任务，并通过实验验证其在MuJoCo的Humanoid-v4等环境中优于经典强化学习方法。


<details>
  <summary>Details</summary>
Motivation: 经典强化学习在高维、非确定性环境下面临参数多和训练效率低的问题，难以高效训练拟人机器人。本研究试图通过引入量子计算克服这些瓶颈。

Method: 采用参数化量子电路，构建混合量子-经典的QDRL架构，与深度强化学习结合，无需传统的映射与规划，直接在高维状态空间中训练。将Soft Actor-Critic (SAC) 算法的经典实现与量子实现进行对比。

Result: 量子SAC在比经典SAC少92%训练步数的情况下，将平均回报率提高了8%（246.40 vs 228.36），显示了量子方案学习速度和效果均优于传统强化学习。

Conclusion: 量子深度强化学习能显著提升高维复杂机器人任务的学习效率和效果，展示了量子计算在机器人强化学习领域的巨大潜力。

Abstract: Classical reinforcement learning (RL) methods often struggle in complex,
high-dimensional environments because of their extensive parameter requirements
and challenges posed by stochastic, non-deterministic settings. This study
introduces quantum deep reinforcement learning (QDRL) to train humanoid agents
efficiently. While previous quantum RL models focused on smaller environments,
such as wheeled robots and robotic arms, our work pioneers the application of
QDRL to humanoid robotics, specifically in environments with substantial
observation and action spaces, such as MuJoCo's Humanoid-v4 and Walker2d-v4.
Using parameterized quantum circuits, we explored a hybrid quantum-classical
setup to directly navigate high-dimensional state spaces, bypassing traditional
mapping and planning. By integrating quantum computing with deep RL, we aim to
develop models that can efficiently learn complex navigation tasks in humanoid
robots. We evaluated the performance of the Soft Actor-Critic (SAC) in
classical RL against its quantum implementation. The results show that the
quantum SAC achieves an 8% higher average return (246.40) than the classical
SAC (228.36) after 92% fewer steps, highlighting the accelerated learning
potential of quantum computing in RL tasks.

</details>


### [258] [TRUST 2025: SCRITA and RTSS @ RO-MAN 2025](https://arxiv.org/abs/2509.11402)
*Alessandra Rossi,Patrick Holthaus,Gabriella Lakatos,Sílvia Moros,Ali Fallahi,Murat Kirtay,Marie Postma,Erhan Oztop*

Main category: cs.RO

TL;DR: TRUST研讨会是SCRITA和RTSS两个人机交互领域知名研讨会的联合举办，旨在汇聚双方优势，共同推进信任相关研究。


<details>
  <summary>Details</summary>
Motivation: 两个研讨会希望整合在信任、接受度及社会线索（SCRITA）与机器人共生社会中的信任（RTSS）方面的资源与经验，加强跨学科交流，提高对于人-机器人信任理解的全面性。

Method: 通过联合举办研讨会，邀请相关领域研究者和实践者，对人-机器人信任的主题进行多角度探讨，涉及人的信任心理、机器人的信任行为等。

Result: 此次合作促进了人和机器人在信任研究视角与方法上的融合，扩大了参与者的交流平台和学科影响力。

Conclusion: TRUST研讨会为推动人机交互中信任机制的研究和应用提供了有力的平台，促进了领域内合作与知识共享。

Abstract: The TRUST workshop is the result of a collaboration between two established
workshops in the field of Human-Robot Interaction: SCRITA (Trust, Acceptance
and Social Cues in Human-Robot Interaction) and RTSS (Robot Trust for Symbiotic
Societies). This joint initiative brings together the complementary goals of
these workshops to advance research on trust from both the human and robot
perspectives.
  Website: https://scrita.herts.ac.uk/2025/

</details>


### [259] [Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations](https://arxiv.org/abs/2509.11417)
*Shresth Grover,Akshay Gopalkrishnan,Bo Ai,Henrik I. Christensen,Hao Su,Xuanlin Li*

Main category: cs.RO

TL;DR: 本文提出了一种能更好利用预训练视觉-语言模型（VLM）的视觉-语言-动作（VLA）机器人模型微调框架，提升了泛化性和任务表现。


<details>
  <summary>Details</summary>
Motivation: 直接在机器人数据上微调VLM虽然能利用大规模预训练表示，但容易破坏原有特征结构，影响泛化能力。因此需要一种有效方法在保留预训练知识的同时适应机器人 manipulation 任务需求。

Method: 方法由三个关键创新组成：（1）双编码器结构：一个视觉编码器冻结以保持预训练特征，另一个可训练以适应具体任务；（2）基于字符序列的动作tokenizer，将连续动作转为字符表征，便于与模型预训练域对齐；（3）协同训练策略：结合机器人演示数据与注重空间关系和可供性的视觉-语言数据进行训练。

Result: 在仿真环境和真实机器人实验中，方法在面对视觉干扰、新指令、新环境时对比基线数据泛化性和任务完成率显著提升，表现更为稳健。

Conclusion: 该框架在不破坏原有表示的前提下，成功提升了VLA模型在操控任务中的泛化和适应能力，对推进通用机器人发展具有积极意义。

Abstract: Vision-language-action (VLA) models finetuned from vision-language models
(VLMs) hold the promise of leveraging rich pretrained representations to build
generalist robots across diverse tasks and environments. However, direct
fine-tuning on robot data often disrupts these representations and limits
generalization. We present a framework that better preserves pretrained
features while adapting them for robot manipulation. Our approach introduces
three components: (i) a dual-encoder design with one frozen vision encoder to
retain pretrained features and another trainable for task adaptation, (ii) a
string-based action tokenizer that casts continuous actions into character
sequences aligned with the model's pretraining domain, and (iii) a co-training
strategy that combines robot demonstrations with vision-language datasets
emphasizing spatial reasoning and affordances. Evaluations in simulation and on
real robots show that our method improves robustness to visual perturbations,
generalization to novel instructions and environments, and overall task success
compared to baselines.

</details>


### [260] [A Software-Only Post-Processor for Indexed Rotary Machining on GRBL-Based CNCs](https://arxiv.org/abs/2509.11433)
*Pedro Portugal,Damian D. Venghaus,Diego Lopez*

Main category: cs.RO

TL;DR: 本论文提出了一种纯软件框架，可在GRBL系统的桌面CNC机床上实现索引式旋转加工，无需改造硬件或更换控制器，降低了多轴加工门槛。


<details>
  <summary>Details</summary>
Motivation: 现有大多数经济型桌面CNC缺少旋转轴，无法方便加工旋转对称或多面的零件。已有的解决方案通常需要硬件升级、专用控制器或商用CAM软件，增加了成本与复杂性。

Method: 通过自定义后处理器，将平面刀路径转化为离散的旋转步进过程，并用基于浏览器的界面进行执行，全过程不需更改机床固件，利用现有标准硬件即可。

Result: 实现了无需硬件升级的索引式旋转加工方法，可用普通桌面CNC机床完成有限的多面/旋转体零件加工。

Conclusion: 该框架大大降低了多轴加工的技术和经济门槛，有助于教育、创客空间及小型工坊等场景下多轴加工的推广与实践，对动手学习及快速原型制造具有积极意义。

Abstract: Affordable desktop CNC routers are common in education, prototyping, and
makerspaces, but most lack a rotary axis, limiting fabrication of rotationally
symmetric or multi-sided parts. Existing solutions often require hardware
retrofits, alternative controllers, or commercial CAM software, raising cost
and complexity. This work presents a software-only framework for indexed rotary
machining on GRBL-based CNCs. A custom post-processor converts planar toolpaths
into discrete rotary steps, executed through a browser-based interface. While
not equivalent to continuous 4-axis machining, the method enables practical
rotary-axis fabrication using only standard, off-the-shelf mechanics, without
firmware modification. By reducing technical and financial barriers, the
framework expands access to multi-axis machining in classrooms, makerspaces,
and small workshops, supporting hands-on learning and rapid prototyping.

</details>


### [261] [RAPTOR: A Foundation Policy for Quadrotor Control](https://arxiv.org/abs/2509.11481)
*Jonas Eschmann,Dario Albani,Giuseppe Loianno*

Main category: cs.RO

TL;DR: 该论文提出了一种新的方法RAPTOR，能够训练出适用于多种四旋翼无人机的通用控制策略，实现对新平台的零样本自适应。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的神经网络策略过度专注于单一环境，面对新环境或系统微小变化时（如Sim2Real误差），性能大幅下降，需要重新识别和训练，而人类却能高效适应新条件。因此，作者试图开发一种更具适应性的通用四旋翼无人机控制策略。

Method: 提出RAPTOR方法，采用端到端神经网络结构，通过在隐藏层引入递归，从而实现基于上下文学习的快速适应。具体流程为：首先用强化学习为1000种不同的四旋翼分别训练教师策略，然后再将这1000个教师策略蒸馏为一个能够广泛适应不同平台的学生策略。

Result: RAPTOR仅用三层2084参数就实现了零样本快速适应，能在毫秒级别内适应多个从未见过的四旋翼平台。实验证明该策略在不同机型、飞控、螺旋桨类型，以及多种扰动环境下均表现出良好适应性和鲁棒性。

Conclusion: RAPTOR方法实现了高效、自适应的四旋翼无人机控制策略，显著缓解了传统RL策略的过拟合及环境迁移难题，为通用机器人控制提供了切实可行的方案。

Abstract: Humans are remarkably data-efficient when adapting to new unseen conditions,
like driving a new car. In contrast, modern robotic control systems, like
neural network policies trained using Reinforcement Learning (RL), are highly
specialized for single environments. Because of this overfitting, they are
known to break down even under small differences like the Simulation-to-Reality
(Sim2Real) gap and require system identification and retraining for even
minimal changes to the system. In this work, we present RAPTOR, a method for
training a highly adaptive foundation policy for quadrotor control. Our method
enables training a single, end-to-end neural-network policy to control a wide
variety of quadrotors. We test 10 different real quadrotors from 32 g to 2.4 kg
that also differ in motor type (brushed vs. brushless), frame type (soft vs.
rigid), propeller type (2/3/4-blade), and flight controller
(PX4/Betaflight/Crazyflie/M5StampFly). We find that a tiny, three-layer policy
with only 2084 parameters is sufficient for zero-shot adaptation to a wide
variety of platforms. The adaptation through In-Context Learning is made
possible by using a recurrence in the hidden layer. The policy is trained
through a novel Meta-Imitation Learning algorithm, where we sample 1000
quadrotors and train a teacher policy for each of them using Reinforcement
Learning. Subsequently, the 1000 teachers are distilled into a single, adaptive
student policy. We find that within milliseconds, the resulting foundation
policy adapts zero-shot to unseen quadrotors. We extensively test the
capabilities of the foundation policy under numerous conditions (trajectory
tracking, indoor/outdoor, wind disturbance, poking, different propellers).

</details>


### [262] [FR-Net: Learning Robust Quadrupedal Fall Recovery on Challenging Terrains through Mass-Contact Prediction](https://arxiv.org/abs/2509.11504)
*Yidan Lu,Yinzhao Dong,Jiahui Zhang,Ji Ma,Peng Lu*

Main category: cs.RO

TL;DR: 提出了FR-Net，一个可使四足机器人在各种环境中恢复跌倒姿态的学习框架，利用质量-接触预测网络提升恢复能力，并在仿真与现实实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 四足机器人在复杂地形跌倒后恢复依然极具挑战，主要由于传统方法在地形感知不全和接触不确定时易失效，因此需要更鲁棒的恢复方法。

Method: 提出FR-Net，核心为质量-接触预测网络，仅依赖有限传感输入预测机器人质量分布和接触状态，结合专门设计的奖励函数在仿真中进行特权学习训练，并在实际部署时无需明确地形数据。

Result: FR-Net在多种四足平台仿真中显示优良泛化能力，并在现实Go2机器人10种复杂场景下大量实验验证，效果出色。

Conclusion: 显式的质量-接触预测对于鲁棒跌倒恢复至关重要，为通用四足技能提供了新方向。

Abstract: Fall recovery for legged robots remains challenging, particularly on complex
terrains where traditional controllers fail due to incomplete terrain
perception and uncertain interactions. We present \textbf{FR-Net}, a
learning-based framework that enables quadrupedal robots to recover from
arbitrary fall poses across diverse environments. Central to our approach is a
Mass-Contact Predictor network that estimates the robot's mass distribution and
contact states from limited sensory inputs, facilitating effective recovery
strategies. Our carefully designed reward functions ensure safe recovery even
on steep stairs without dangerous rolling motions common to existing methods.
Trained entirely in simulation using privileged learning, our framework guides
policy learning without requiring explicit terrain data during deployment. We
demonstrate the generalization capabilities of \textbf{FR-Net} across different
quadrupedal platforms in simulation and validate its performance through
extensive real-world experiments on the Go2 robot in 10 challenging scenarios.
Our results indicate that explicit mass-contact prediction is key to robust
fall recovery, offering a promising direction for generalizable quadrupedal
skills.

</details>


### [263] [Design and Development of a Remotely Wire-Driven Walking Robot](https://arxiv.org/abs/2509.11506)
*Takahiro Hattori,Kento Kawaharazuka,Kei Okada*

Main category: cs.RO

TL;DR: 本文提出了一种新的远程线驱机制（Remote Wire Drive），可通过电缆远程驱动移动机器人，并通过搭建和实验证明了其可行性。


<details>
  <summary>Details</summary>
Motivation: 恶劣或人类无法到达的环境对机器人电子元件构成威胁，因此需要能在缺乏电子元件的情况下实现远程驱动和控制机制。现有如无电子自主机器人、液压驱动及线驱动方式均存在局限。作者希望结合现有优点，提出新的解决方案。

Method: 提出远程线驱（Remote Wire Drive）的新机制，通过串联解耦关节（即线驱动机械臂中的一种结构）用于移动机器人的动力传递。并开发了一个线驱动的四足机器人，通过远程控制进行实验验证。

Result: 通过实验，成功验证了远程线驱动四足机器人的可行性，说明该机制能够实现远距离对移动机器人的驱动与控制。

Conclusion: 远程线驱机制为在恶劣环境中电子元件不可用或不可行的场合，提供了一种无需在机器人本体带有电子组件即可实现远程驱动的新方案，提升了移动机器人的环境适应性和可靠性。

Abstract: Operating in environments too harsh or inaccessible for humans is one of the
critical roles expected of robots. However, such environments often pose risks
to electronic components as well. To overcome this, various approaches have
been developed, including autonomous mobile robots without electronics,
hydraulic remotely actuated mobile robots, and long-reach robot arms driven by
wires. Among these, electronics-free autonomous robots cannot make complex
decisions, while hydraulically actuated mobile robots and wire-driven robot
arms are used in harsh environments such as nuclear power plants. Mobile robots
offer greater reach and obstacle avoidance than robot arms, and wire mechanisms
offer broader environmental applicability than hydraulics. However, wire-driven
systems have not been used for remote actuation of mobile robots. In this
study, we propose a novel mechanism called Remote Wire Drive that enables
remote actuation of mobile robots via wires. This mechanism is a series
connection of decoupled joints, a mechanism used in wire-driven robot arms,
adapted for power transmission. We experimentally validated its feasibility by
actuating a wire-driven quadruped robot, which we also developed in this study,
through Remote Wire Drive.

</details>


### [264] [PaiP: An Operational Aware Interactive Planner for Unknown Cabinet Environments](https://arxiv.org/abs/2509.11516)
*Chengjin Wang,Zheng Yan,Yanmin Zhou,Runjie Shen,Zhipeng Wang,Bin Cheng,Bin He*

Main category: cs.RO

TL;DR: 本文提出了一种基于多模态触觉感知、用于狭小空间中堆叠物体操作的实时交互式运动规划器PaiP，弥补了传统轨迹规划在不可见障碍物环境中的不足。


<details>
  <summary>Details</summary>
Motivation: 在箱体/柜体等封闭或半封闭场景下，堆叠的物体导致视觉遮挡和空间受限，使得传统无碰撞路径规划方法失效，甚至可能因为不可见物体导致灾难性碰撞。因此，需要一种更智能、稳健的方案来应对这些挑战。

Method: 提出PaiP交互式运动规划器，将多模态触觉感知用于实时闭环运动规划。系统通过感知机械臂与物体的交互界面上的运动效应，自动推断交互特征，并将其编码到栅格地图中生成操作代价图，在此基础上将采样式路径规划方法扩展到同时优化路径代价和操作代价的交互式规划。

Result: 实验结果表明，PaiP系统能在狭小、障碍物隐藏或不可见的复杂环境下，实现鲁棒的机器人运动规划。

Conclusion: 通过融合多模态感知与交互特性，本方法显著提升了机器人在受限环境中规划与运动的鲁棒性，克服了传统方法在视觉受限和无无碰撞路径存在时的局限。

Abstract: Box/cabinet scenarios with stacked objects pose significant challenges for
robotic motion due to visual occlusions and constrained free space. Traditional
collision-free trajectory planning methods often fail when no collision-free
paths exist, and may even lead to catastrophic collisions caused by invisible
objects. To overcome these challenges, we propose an operational aware
interactive motion planner (PaiP) a real-time closed-loop planning framework
utilizing multimodal tactile perception. This framework autonomously infers
object interaction features by perceiving motion effects at interaction
interfaces. These interaction features are incorporated into grid maps to
generate operational cost maps. Building upon this representation, we extend
sampling-based planning methods to interactive planning by optimizing both path
cost and operational cost. Experimental results demonstrate that PaiP achieves
robust motion in narrow spaces.

</details>


### [265] [Shape control of simulated multi-segment continuum robots via Koopman operators with per-segment projection](https://arxiv.org/abs/2509.11567)
*Eron Ristich,Jiahe Wang,Lei Zhang,Sultan Haidar Ali,Wanxin Jin,Yi Ren,Jiefeng Sun*

Main category: cs.RO

TL;DR: 文章提出了一种基于Koopman算子的软体连续体机器人形状控制方法，通过数据驱动与分段投影显著提升形状控制精度，实现了实时高效的形状闭环控制。


<details>
  <summary>Details</summary>
Motivation: 当前软体连续体机器人虽然可以进行末端实时控制，但由于其无限自由度，整体形状控制难以实现，主要受限于高昂的计算成本。作者希望通过更高效的建模与控制方法解决形状控制难题。

Method: 作者采用Kirchhoff杆模型模拟多节腱驱动软体机器人，通过仿真数据进行分段状态投影，从而训练出控制相关的Koopman算子模型。随后，利用学习到的Koopman模型，实现线性模型预测控制（MPC），以达到多样的目标形状。

Result: 实验表明，带分段投影的Koopman模型在控制精度上高出一个数量级，并能实现对不同复杂目标形状的实时闭环控制。

Conclusion: 本文方法验证了基于Koopman算子的软体机器人实时形状控制的可行性与高效性，有望推动软体连续体机器人实用化的形状控制。

Abstract: Soft continuum robots can allow for biocompatible yet compliant motions, such
as the ability of octopus arms to swim, crawl, and manipulate objects. However,
current state-of-the-art continuum robots can only achieve real-time task-space
control (i.e., tip control) but not whole-shape control, mainly due to the high
computational cost from its infinite degrees of freedom. In this paper, we
present a data-driven Koopman operator-based approach for the shape control of
simulated multi-segment tendon-driven soft continuum robots with the Kirchhoff
rod model. Using data collected from these simulated soft robots, we conduct a
per-segment projection scheme on the state of the robots allowing for the
identification of control-affine Koopman models that are an order of magnitude
more accurate than without the projection scheme. Using these learned Koopman
models, we use a linear model predictive control (MPC) to control the robots to
a collection of target shapes of varying complexity. Our method realizes
computationally efficient closed-loop control, and demonstrates the feasibility
of real-time shape control for soft robots. We envision this work can pave the
way for practical shape control of soft continuum robots.

</details>


### [266] [GBPP: Grasp-Aware Base Placement Prediction for Robots via Two-Stage Learning](https://arxiv.org/abs/2509.11594)
*Jizhuo Chen,Diwen Liu,Jiaming Wang,Harold Soh*

Main category: cs.RO

TL;DR: 本文提出了一种名为GBPP的新方法，通过学习型评分机制，从单帧RGB-D图像快速选择机器人抓取位置。该方法结合规则自动标注与高保真仿真校正，实现在仿真和真实机器人中的高效表现。


<details>
  <summary>Details</summary>
Motivation: 在机器人抓取任务中，如何高效且准确地为机器人选择适合的底座（base）姿态一直是挑战，尤其是在场景信息受限的情况下。传统方法往往需要复杂的任务与运动规划，效率低下且实时性差。本文旨在提出一种简单高效的数据驱动方法，以在低成本下实现更优的底座姿态选择。

Method: 本文方法分为两个阶段：首先，用一个基于距离和可见性的简单自动标注规则，对大量数据进行低成本标签生成；其次，利用较少的高保真仿真试验对模型进行校准，从而更贴合真实抓取结果。整个系统采用类似PointNet++的点云编码器结合MLP，对候选底座姿势进行批量评分，实现快速在线选择。

Result: 实验表明，GBPP在仿真和真实移动操作机器人上表现均优于仅靠邻近性或几何信息的基线方法。它可以选择更安全、更易于到达的姿态，并且在出现错误时表现出较好的鲁棒性（degrade gracefully）。

Conclusion: 本文提出的GBPP系统是一种数据高效且具备几何感知能力的机器人底座姿态选择方法。其关键经验在于：先用廉价启发式收集覆盖性数据，后用有针对性的仿真数据校准，可在不依赖复杂优化的前提下实现可靠的机器人部署。

Abstract: GBPP is a fast learning based scorer that selects a robot base pose for
grasping from a single RGB-D snapshot. The method uses a two stage curriculum:
(1) a simple distance-visibility rule auto-labels a large dataset at low cost;
and (2) a smaller set of high fidelity simulation trials refines the model to
match true grasp outcomes. A PointNet++ style point cloud encoder with an MLP
scores dense grids of candidate poses, enabling rapid online selection without
full task-and-motion optimization. In simulation and on a real mobile
manipulator, GBPP outperforms proximity and geometry only baselines, choosing
safer and more reachable stances and degrading gracefully when wrong. The
results offer a practical recipe for data efficient, geometry aware base
placement: use inexpensive heuristics for coverage, then calibrate with
targeted simulation.

</details>


### [267] [AssemMate: Graph-Based LLM for Robotic Assembly Assistance](https://arxiv.org/abs/2509.11617)
*Qi Zheng,Chaoran Zhang,Zijian Liang,EnTe Lin,Shubo Cui,Qinghongbing Xie,Zhaobo Xu,Long Zeng*

Main category: cs.RO

TL;DR: 本文提出了AssemMate，一种结合知识图谱和大语言模型（LLM）的机器人装配辅助系统，实现了更高效、精确的知识问答与操作规划。与以往使用长文本的做法相比，AssemMate采用图结构作为输入，有效提升了上下文利用率和推理效率。实验表明，该方法在准确率、推理速度和泛化能力等方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在机器人装配辅助中需要详细且实时的领域知识，但传统以自然语言文本存储知识的方法冗余且推理速度慢。这限制了机器人在人机交互和装配规划中的实时性和精确性需求，需要寻找更精炼的知识表达方法。

Method: AssemMate采用知识图谱作为知识输入，通过自监督的图卷积网络（GCN）对实体及关系编码，并与LLM语义对齐，使LLM能理解图中信息。系统还结合视觉增强策略用于应对复杂场景（如堆叠抓取）。最终，AssemMate支持知识图谱问答（KGQA）、装配任务规划和真实/虚拟环境中的机器人抓取辅助。

Result: AssemMate在实验中的表现优于现有方法：准确率提高6.4%，推理速度提升3倍，上下文长度缩短28倍，并在随机知识图谱上显示了较强的泛化能力。实际抓取实验（仿真与现实）也证实了其优越性。

Conclusion: 将知识图谱与大语言模型结合，能极大提升机器人装配领域中的知识问答与任务执行效率，具有良好的通用性和应用前景。

Abstract: Large Language Model (LLM)-based robotic assembly assistance has gained
significant research attention. It requires the injection of domain-specific
knowledge to guide the assembly process through natural language interaction
with humans. Despite some progress, existing methods represent knowledge in the
form of natural language text. Due to the long context and redundant content,
they struggle to meet the robots' requirements for real-time and precise
reasoning. In order to bridge this gap, we present AssemMate, which utilizes
the graph\textemdash a concise and accurate form of knowledge
representation\textemdash as input. This graph-based LLM enables knowledge
graph question answering (KGQA), supporting human-robot interaction and
assembly task planning for specific products. Beyond interactive QA, AssemMate
also supports sensing stacked scenes and executing grasping to assist with
assembly. Specifically, a self-supervised Graph Convolutional Network (GCN)
encodes knowledge graph entities and relations into a latent space and aligns
them with LLM's representation, enabling the LLM to understand graph
information. In addition, a vision-enhanced strategy is employed to address
stacked scenes in grasping. Through training and evaluation, AssemMate
outperforms existing methods, achieving 6.4\% higher accuracy, 3 times faster
inference, and 28 times shorter context length, while demonstrating strong
generalization ability on random graphs. And our approach further demonstrates
superiority through robotic grasping experiments in both simulated and
real-world settings. More details can be found on the project page:
https://github.com/cristina304/AssemMate.git

</details>


### [268] [Inference-stage Adaptation-projection Strategy Adapts Diffusion Policy to Cross-manipulators Scenarios](https://arxiv.org/abs/2509.11621)
*Xiangtong Yao,Yirui Zhou,Yuan Meng,Yanwen Liu,Liangyu Dong,Zitao Zhang,Zhenshan Bing,Kai Huang,Fuchun Sun,Alois Knoll*

Main category: cs.RO

TL;DR: 提出一种适用于机器人操作的扩散策略，通过适应-投影方法，实现零样本迁移到新机械臂和任务，无需重新训练，提升通用性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散策略模型在机器人操作中对新机械臂或末端执行器的泛化能力较差，且在面对新任务时需要高昂的数据采集和策略重训练成本。作者希望解决这一适应性和高成本问题。

Method: 首先基于基础机械臂的操作演示，在SE(3)空间训练扩散策略。推理阶段，利用适应-投影方法，对生成的轨迹进行投影，使其满足新硬件和任务的运动学及特定约束。该投影过程可动态适应不同的物理参数（如工具偏移、夹爪宽度）和任务要求（如障碍物高度等）。

Result: 在多种机械臂（含Franka Panda, Kuka iiwa 14）及多样末端执行器（柔性夹爪、Robotiq夹爪、3D打印夹爪）上，验证了在抓取、推动、倒液等任务上的高成功率和普适性。

Conclusion: 适应-投影策略可以无须重新训练，实现在新机械臂和动态任务场景下的高效、稳健操作，具有实际应用价值。

Abstract: Diffusion policies are powerful visuomotor models for robotic manipulation,
yet they often fail to generalize to manipulators or end-effectors unseen
during training and struggle to accommodate new task requirements at inference
time. Addressing this typically requires costly data recollection and policy
retraining for each new hardware or task configuration. To overcome this, we
introduce an adaptation-projection strategy that enables a diffusion policy to
perform zero-shot adaptation to novel manipulators and dynamic task settings,
entirely at inference time and without any retraining. Our method first trains
a diffusion policy in SE(3) space using demonstrations from a base manipulator.
During online deployment, it projects the policy's generated trajectories to
satisfy the kinematic and task-specific constraints imposed by the new hardware
and objectives. Moreover, this projection dynamically adapts to physical
differences (e.g., tool-center-point offsets, jaw widths) and task requirements
(e.g., obstacle heights), ensuring robust and successful execution. We validate
our approach on real-world pick-and-place, pushing, and pouring tasks across
multiple manipulators, including the Franka Panda and Kuka iiwa 14, equipped
with a diverse array of end-effectors like flexible grippers, Robotiq 2F/3F
grippers, and various 3D-printed designs. Our results demonstrate consistently
high success rates in these cross-manipulator scenarios, proving the
effectiveness and practicality of our adaptation-projection strategy. The code
will be released after peer review.

</details>


### [269] [ParaEQsA: Parallel and Asynchronous Embodied Questions Scheduling and Answering](https://arxiv.org/abs/2509.11663)
*Haisheng Wang,Weiming Zhi*

Main category: cs.RO

TL;DR: 本文提出了体感式问答（EQsA）新问题，建立了相应基准，并提出了并行、紧急性感知的解答框架ParaEQsA。该方法支持多问题异步、高效处理，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统体感问答（EQA）仅能针对单一问题进行探索并解答，但现实场景通常需应对异步、多问题，并区分问题的紧急程度，因此有必要拓展至更实际的设置。

Method: 作者提出了EQsA新设定，开发了ParaEQsA系统，利用共享群组记忆减少冗余探索，并用优先级调度动态分配问题处理；同时贡献了新数据集（PAEQs）和两个评测指标（DAR，NUWL）。

Result: ParaEQsA在效率与响应性上均优于现有EQA先进方法，减少了探索和延迟。实验分析了优先级、紧急性建模、空间范围、奖励估计和依赖推理的影响。

Conclusion: 多问题、紧急性感知的并行调度，显著提升了体感式智能体在实际复杂任务下的效率和响应性。

Abstract: This paper formulates the Embodied Questions Answering (EQsA) problem,
introduces a corresponding benchmark, and proposes a system to tackle the
problem. Classical Embodied Question Answering (EQA) is typically formulated as
answering one single question by actively exploring a 3D environment. Real
deployments, however, often demand handling multiple questions that may arrive
asynchronously and carry different urgencies. We formalize this setting as
Embodied Questions Answering (EQsA) and present ParaEQsA, a framework for
parallel, urgency-aware scheduling and answering. ParaEQsA leverages a group
memory module shared among questions to reduce redundant exploration, and a
priority-planning module to dynamically schedule questions. To evaluate this
setting, we contribute the Parallel Asynchronous Embodied Questions (PAEQs)
benchmark containing 40 indoor scenes and five questions per scene (200 in
total), featuring asynchronous follow-up questions and urgency labels. We
further propose metrics for EQsA performance: Direct Answer Rate (DAR), and
Normalized Urgency-Weighted Latency (NUWL), which jointly measure efficiency
and responsiveness of this system. ParaEQsA consistently outperforms strong
sequential baselines adapted from recent EQA systems, while reducing
exploration and delay. Empirical evaluations investigate the relative
contributions of priority, urgency modeling, spatial scope, reward estimation,
and dependency reasoning within our framework. Together, these results
demonstrate that urgency-aware, parallel scheduling is key to making embodied
agents responsive and efficient under realistic, multi-question workloads.

</details>


### [270] [Tensor Invariant Data-Assisted Control and Dynamic Decomposition of Multibody Systems](https://arxiv.org/abs/2509.11688)
*Mostafa Eslami,Maryam Babazadeh*

Main category: cs.RO

TL;DR: 该论文提出了一种新型机器人控制框架，实现了基于张量力学的无坐标多体动力学模型与数据辅助控制架构的结合，有效提升了在多变协作空间中的控制鲁棒性、数据效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有机器人系统在复杂协作空间中控制时，主要依赖与具体坐标系相关的模型，导致同样的物理规律在不同场景下需反复学习，造成数据效率低，难以泛化。

Method: 作者提出采用张量力学的无坐标、多体动力学和运动学的非递归闭式Newton-Euler模型，将系统分解为基于物理原理的确定性部分和需要数据辅助学习的不确定性部分，并通过虚拟端口变量进行连接。整个流程从建模、控制到学习均基于张量不变性，使控制规律无关具体参考系，保障了稳定性（通过Lyapunov分析证明）。

Result: 论文通过仿真对所建模型及闭环系统进行了验证，证明该方法能作为数据高效、参考系无关的学习算法（如等变学习）的理想输入，提高数据利用率和控制鲁棒性。

Conclusion: 提出的无坐标张量动力学控制框架提升了机器人学习与控制的数据效率、泛化能力及可解释性，有望显著增强机器人系统在交互环境中的鲁棒性和通用性。

Abstract: The control of robotic systems in complex, shared collaborative workspaces
presents significant challenges in achieving robust performance and safety when
learning from experienced or simulated data is employed in the pipeline. A
primary bottleneck is the reliance on coordinate-dependent models, which leads
to profound data inefficiency by failing to generalize physical interactions
across different frames of reference. This forces learning algorithms to
rediscover fundamental physical principles in every new orientation,
artificially inflating the complexity of the learning task. This paper
introduces a novel framework that synergizes a coordinate-free, unreduced
multibody dynamics and kinematics model based on tensor mechanics with a
Data-Assisted Control (DAC) architecture. A non-recursive, closed-form
Newton-Euler model in an augmented matrix form is derived that is optimized for
tensor-based control design. This structure enables a principled decomposition
of the system into a structurally certain, physically grounded part and an
uncertain, empirical, and interaction-focused part, mediated by a virtual port
variable. Then, a complete, end-to-end tensor-invariant pipeline for modeling,
control, and learning is proposed. The coordinate-free control laws for the
structurally certain part provide a stable and abstract command interface,
proven via Lyapunov analysis. Eventually, the model and closed-loop system are
validated through simulations. This work provides a naturally ideal input for
data-efficient, frame-invariant learning algorithms, such as equivariant
learning, designed to learn the uncertain interaction. The synergy directly
addresses the data-inefficiency problem, increases explainability and
interpretability, and paves the way for more robust and generalizable robotic
control in interactive environments.

</details>


### [271] [From Pixels to Shelf: End-to-End Algorithmic Control of a Mobile Manipulator for Supermarket Stocking and Fronting](https://arxiv.org/abs/2509.11740)
*Davide Peron,Victor Nan Fernandez-Ayala,Lukas Segelmark*

Main category: cs.RO

TL;DR: 本文提出了一套高效的超市货架自动补货及整理机器人系统，实现了商品检测、抓取与摆放的全流程自动化，具有实际应用潜力，但仍不及人工表现。


<details>
  <summary>Details</summary>
Motivation: 超市等零售环境的自动补货面临空间狭小、人流复杂及商品多样等难题，亟需高效稳定的自动化解决方案以降低人工成本和提升效率。

Method: 系统集成了商用硬件，基于ROS2进行感知、规划与控制；采用行为树进行任务规划，细化视觉模型识别商品，并通过两步模型预测控制与ArUco标记实现精确导航和抓取。

Result: 在模拟真实超市环境实验中，货品抓取与摆放操作700余次，成功率超过98%。

Conclusion: 尽管提出的系统在实验环境下表现优异，当前自动补货技术在成本与效率上仍无法超越人工，需要进一步提升以满足商业化大规模部署的要求。

Abstract: Autonomous stocking in retail environments, particularly supermarkets,
presents challenges due to dynamic human interactions, constrained spaces, and
diverse product geometries. This paper introduces an efficient end-to-end
robotic system for autonomous shelf stocking and fronting, integrating
commercially available hardware with a scalable algorithmic architecture. A
major contribution of this work is the system integration of off-the-shelf
hardware and ROS2-based perception, planning, and control into a single
deployable platform for retail environments. Our solution leverages Behavior
Trees (BTs) for task planning, fine-tuned vision models for object detection,
and a two-step Model Predictive Control (MPC) framework for precise shelf
navigation using ArUco markers. Laboratory experiments replicating realistic
supermarket conditions demonstrate reliable performance, achieving over 98%
success in pick-and-place operations across a total of more than 700 stocking
events. However, our comparative benchmarks indicate that the performance and
cost-effectiveness of current autonomous systems remain inferior to that of
human workers, which we use to highlight key improvement areas and quantify the
progress still required before widespread commercial deployment can
realistically be achieved.

</details>


### [272] [Adaptive Motorized LiDAR Scanning Control for Robust Localization with OpenStreetMap](https://arxiv.org/abs/2509.11742)
*Jianping Li,Kaisong Zhu,Zhongyuan Liu,Rui Jin,Xinhang Xu,Pengfei Wan,Lihua Xie*

Main category: cs.RO

TL;DR: 提出了一种利用OSM地图先验信息指引LiDAR扫描的自适应定位方法，有效提升了复杂环境下的定位鲁棒性和精度。


<details>
  <summary>Details</summary>
Motivation: 传统基于OSM的LiDAR定位受限于OSM数据的不完整和过时性，同时现有的电机式LiDAR扫描策略未根据场景及地图特征自适应分配扫描资源，导致局部观测不足，影响机器人定位效果。

Method: 提出自适应LiDAR扫描框架，将OSM全局先验与本地可观测性预测融合，在不确定性感知模型预测控制中引入OSM相关项，自适应调整扫描，优先关注有用特征密集区域，实现对运动LiDAR的动态控制，并在ROS平台下进行实验验证。

Result: 在校园道路、室内走廊及城市环境的仿真与实地测试中，相较于常速扫描基线方法，提出方法明显降低了机器人轨迹误差，同时保持了扫描数据的完整性。

Conclusion: 结合开放式地图与自适应LiDAR扫描可大幅提升机器人在复杂环境中的定位鲁棒性和效率，展现了实际部署的应用前景。

Abstract: LiDAR-to-OpenStreetMap (OSM) localization has gained increasing attention, as
OSM provides lightweight global priors such as building footprints. These
priors enhance global consistency for robot navigation, but OSM is often
incomplete or outdated, limiting its reliability in real-world deployment.
Meanwhile, LiDAR itself suffers from a limited field of view (FoV), where
motorized rotation is commonly used to achieve panoramic coverage. Existing
motorized LiDAR systems, however, typically employ constant-speed scanning that
disregards both scene structure and map priors, leading to wasted effort in
feature-sparse regions and degraded localization accuracy. To address these
challenges, we propose Adaptive LiDAR Scanning with OSM guidance, a framework
that integrates global priors with local observability prediction to improve
localization robustness. Specifically, we augment uncertainty-aware model
predictive control with an OSM-aware term that adaptively allocates scanning
effort according to both scene-dependent observability and the spatial
distribution of OSM features. The method is implemented in ROS with a motorized
LiDAR odometry backend and evaluated in both simulation and real-world
experiments. Results on campus roads, indoor corridors, and urban environments
demonstrate significant reductions in trajectory error compared to
constant-speed baselines, while maintaining scan completeness. These findings
highlight the potential of coupling open-source maps with adaptive LiDAR
scanning to achieve robust and efficient localization in complex environments.

</details>


### [273] [Igniting VLMs toward the Embodied Space](https://arxiv.org/abs/2509.11766)
*Andy Zhai,Brae Liu,Bruno Fang,Chalse Cai,Ellie Ma,Ethan Yin,Hao Wang,Hugo Zhou,James Wang,Lights Shi,Lucy Liang,Make Wang,Qian Wang,Roy Gan,Ryan Yu,Shalfun Li,Starrick Liu,Sylas Chen,Vincent Chen,Zach Xu*

Main category: cs.RO

TL;DR: 本文提出了WALL-OSS，一种端到端的具身基础模型，能够实现更强的视觉-语言理解、语言与动作的关联和健壮的操纵能力，有效克服当前视觉-语言模型在空间与动作理解方面的不足。


<details>
  <summary>Details</summary>
Motivation: 目前的视觉-语言模型（VLM）在空间和具身理解方面存在显著局限，这成为将模型应用到具身智能任务（如机器人操作）时的瓶颈。为推动AGI的发展，需要将语言与实际动作高效结合，同时提升模型的理解和生成动作能力。

Method: 该方法提出了WALL-OSS架构，并采用了多策略训练课程，提出Unified Cross-Level CoT（Chain of Thought），将指令推理、子目标分解和细粒度动作合成整合到一个可微分框架内。这样能够端到端地将大规模多模态预训练能力迁移到具身任务中，提升模型对指令、目标和动作的理解与生成。

Result: WALL-OSS在复杂、长时序的操纵任务上表现出高成功率，展现了强大的指令遵循、复杂理解和推理能力，并超越了现有的强基线模型。

Conclusion: WALL-OSS为从视觉-语言模型向具身基础模型的转变提供了可扩展且可靠的途径，有效提升了多模态模型在真实操控和理解任务中的能力。

Abstract: While foundation models show remarkable progress in language and vision,
existing vision-language models (VLMs) still have limited spatial and
embodiment understanding. Transferring VLMs to embodied domains reveals
fundamental mismatches between modalities, pretraining distributions, and
training objectives, leaving action comprehension and generation as a central
bottleneck on the path to AGI.
  We introduce WALL-OSS, an end-to-end embodied foundation model that leverages
large-scale multimodal pretraining to achieve (1) embodiment-aware
vision-language understanding, (2) strong language-action association, and (3)
robust manipulation capability.
  Our approach employs a tightly coupled architecture and multi-strategies
training curriculum that enables Unified Cross-Level CoT-seamlessly unifying
instruction reasoning, subgoal decomposition, and fine-grained action synthesis
within a single differentiable framework.
  Our results show that WALL-OSS attains high success on complex long-horizon
manipulations, demonstrates strong instruction-following capabilities, complex
understanding and reasoning, and outperforms strong baselines, thereby
providing a reliable and scalable path from VLMs to embodied foundation models.

</details>


### [274] [Augmented Reality-Enhanced Robot Teleoperation for Collecting User Demonstrations](https://arxiv.org/abs/2509.11783)
*Shiqi Gong,Sebastian Zudaire,Chi Zhang,Zhen Li*

Main category: cs.RO

TL;DR: 该论文提出了一种基于增强现实（AR）的机器人远程操作系统，结合点云渲染，实现了直观、免接触的工业机器人示教，极大提升了操作效率和用户体验。


<details>
  <summary>Details</summary>
Motivation: 传统工业机器人编程复杂耗时，需专家长期参与。虽然示教编程（PbD）降低了门槛，但直观的控制界面仍是难点。该研究旨在解决工业机器人示教的便捷性与高效性问题，提升操作人员的使用体验和工作效率。

Method: 提出并实现了一个集成AR控制和点云空间渲染的机器人远程操作系统。用户无需进入机器人工作区或使用传统示教器，通过AR界面实现直观控制，并支持远程操作。系统在ABB的IRB 1200和GoFa 5机器人上进行了验证，并设计用户实验，比较有无点云渲染时的任务完成表现。

Result: 实验结果显示，增强的环境感知（即点云渲染）能将任务完成表现提升28％，系统可用性提升12％（SUS得分）。用户操作更加高效且自信，体验显著改善。

Conclusion: 所提AR增强遥操作系统在工业机器人示教中具有广泛适用性，大幅提升了操作直观性与安全性。改善环境感知能力对提高任务精度和用户体验具有重要作用，相关示教数据还可用于后续机器学习训练。

Abstract: Traditional industrial robot programming is often complex and time-consuming,
typically requiring weeks or even months of effort from expert programmers.
Although Programming by Demonstration (PbD) offers a more accessible
alternative, intuitive interfaces for robot control and demonstration
collection remain challenging. To address this, we propose an Augmented Reality
(AR)-enhanced robot teleoperation system that integrates AR-based control with
spatial point cloud rendering, enabling intuitive, contact-free demonstrations.
This approach allows operators to control robots remotely without entering the
workspace or using conventional tools like the teach pendant. The proposed
system is generally applicable and has been demonstrated on ABB robot
platforms, specifically validated with the IRB 1200 industrial robot and the
GoFa 5 collaborative robot. A user study evaluates the impact of real-time
environmental perception, specifically with and without point cloud rendering,
on task completion accuracy, efficiency, and user confidence. Results indicate
that enhanced perception significantly improves task performance by 28% and
enhances user experience, as reflected by a 12% increase in the System
Usability Scale (SUS) score. This work contributes to the advancement of
intuitive robot teleoperation, AR interface design, environmental perception,
and teleoperation safety mechanisms in industrial settings for demonstration
collection. The collected demonstrations may serve as valuable training data
for machine learning applications.

</details>


### [275] [Synthetic vs. Real Training Data for Visual Navigation](https://arxiv.org/abs/2509.11791)
*Lauri Suomela,Sasanka Kuruppu Arachchige,German F. Torres,Harry Edelman,Joni-Kristian Kämäräinen*

Main category: cs.RO

TL;DR: 该论文比较了在仿真环境中训练的视觉导航策略与用真实世界数据训练的策略在真实环境下的表现，发现通过合适的方法可缩小甚至超越sim-to-real差距。


<details>
  <summary>Details</summary>
Motivation: 仿真训练的导航策略在真实世界效果不佳，存在显著sim-to-real gap。作者希望探索仿真训练策略是否有可能达到甚至超越真实训练策略的表现，并找出影响因素。

Method: 提出了一种结合预训练视觉特征编码器的新型导航策略架构，可实时部署于机器人硬件，提升仿真与现实中的泛化能力。实际测试中，该策略可用于轮式机器人和无人机。同时，论文比较了仿真与现实训练方法，并分析了对表现的影响。

Result: 在轮式移动机器人上的实验中，仿真训练的策略比真实世界训练的版本导航成功率高31%，超过先前方法50%。相同模型还可在无人机上测试通过，验证泛化能力。

Conclusion: 多样化视觉编码器的预训练对于sim-to-real泛化至关重要，且仿真训练中的on-policy学习是其优于真实训练的关键因素。

Abstract: This paper investigates how the performance of visual navigation policies
trained in simulation compares to policies trained with real-world data.
Performance degradation of simulator-trained policies is often significant when
they are evaluated in the real world. However, despite this well-known
sim-to-real gap, we demonstrate that simulator-trained policies can match the
performance of their real-world-trained counterparts.
  Central to our approach is a navigation policy architecture that bridges the
sim-to-real appearance gap by leveraging pretrained visual representations and
runs real-time on robot hardware. Evaluations on a wheeled mobile robot show
that the proposed policy, when trained in simulation, outperforms its
real-world-trained version by 31% and the prior state-of-the-art methods by 50%
in navigation success rate. Policy generalization is verified by deploying the
same model onboard a drone.
  Our results highlight the importance of diverse image encoder pretraining for
sim-to-real generalization, and identify on-policy learning as a key advantage
of simulated training over training with real data.

</details>


### [276] [UniPilot: Enabling GPS-Denied Autonomy Across Embodiments](https://arxiv.org/abs/2509.11793)
*Mihir Kulkarni,Mihir Dharmadhikari,Nikhil Khedekar,Morten Nissov,Mohit Singh,Philipp Weiss,Kostas Alexis*

Main category: cs.RO

TL;DR: 本文提出了UniPilot，一个可跨多种机器人部署的紧凑型硬软一体自主负载系统，实现了在无GPS环境下的自主操作。


<details>
  <summary>Details</summary>
Motivation: 许多机器人在缺乏GPS信号的环境（如室内或地下）下难以实现可靠自主导航，当前依赖单一传感器的方案易受限于特定环境因素。

Method: UniPilot融合了LiDAR、雷达、视觉和惯性等多模态传感器，搭载完整自主软件，实现多模态感知、探索与检查路径规划以及基于学习的导航策略。该系统集成了定位、建图、规划、安全与控制等能力，可一体化部署在多种机器人平台上。

Result: 在多种环境和不同机器人平台上进行了大量实验，验证了该系统在建图、规划和安全导航方面的能力。

Conclusion: UniPilot在无需GPS的复杂环境下，实现了高鲁棒性和可移植性的自主导航系统，适用于多种机器人应用场景。

Abstract: This paper presents UniPilot, a compact hardware-software autonomy payload
that can be integrated across diverse robot embodiments to enable autonomous
operation in GPS-denied environments. The system integrates a multi-modal
sensing suite including LiDAR, radar, vision, and inertial sensing for robust
operation in conditions where uni-modal approaches may fail. UniPilot runs a
complete autonomy software comprising multi-modal perception, exploration and
inspection path planning, and learning-based navigation policies. The payload
provides robust localization, mapping, planning, and safety and control
capabilities in a single unit that can be deployed across a wide range of
platforms. A large number of experiments are conducted across diverse
environments and on a variety of robot platforms to validate the mapping,
planning, and safe navigation capabilities enabled by the payload.

</details>


### [277] [TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning](https://arxiv.org/abs/2509.11839)
*Jiacheng Liu,Pengxiang Ding,Qihang Zhou,Yuxuan Wu,Da Huang,Zimian Peng,Wei Xiao,Weinan Zhang,Lixin Yang,Cewu Lu,Donglin Wang*

Main category: cs.RO

TL;DR: 本文提出了一种结合Koopman算子理论的残差策略学习方法（KORR），通过引入全局动力学建模提升了模仿学习在长时序、高精度任务中的表现，在机器人装配等场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统的模仿学习虽然高效，但在长时序任务和高精度控制中容易出现误差积累，现有残差策略方法仅做局部修正，缺乏对整体状态演化的理解，导致难以泛化。作者希望通过全局动力学建模提升策略修正的稳定性和泛化能力。

Method: 作者提出基于Koopman算子理论，在潜在空间中模拟具有线性时不变结构的全局动力学。具体实现为KORR框架：根据Koopman预测的潜在状态，对残差策略进行条件修正，使动作调整依赖于更有全局信息的状态预测，从而提升策略的稳定性与泛化能力。

Result: KORR在长时序、高精度家具装配机器人任务中，与强基线方法相比，在性能、鲁棒性和泛化能力上均有显著提升。

Conclusion: Koopman引导的残差策略学习为结合现代学习方法与经典控制理论提供了可行路径，尤其适用于需要全局信息和复杂动态建模的场景。

Abstract: Imitation learning (IL) enables efficient skill acquisition from
demonstrations but often struggles with long-horizon tasks and high-precision
control due to compounding errors. Residual policy learning offers a promising,
model-agnostic solution by refining a base policy through closed-loop
corrections. However, existing approaches primarily focus on local corrections
to the base policy, lacking a global understanding of state evolution, which
limits robustness and generalization to unseen scenarios. To address this, we
propose incorporating global dynamics modeling to guide residual policy
updates. Specifically, we leverage Koopman operator theory to impose linear
time-invariant structure in a learned latent space, enabling reliable state
transitions and improved extrapolation for long-horizon prediction and unseen
environments. We introduce KORR (Koopman-guided Online Residual Refinement), a
simple yet effective framework that conditions residual corrections on
Koopman-predicted latent states, enabling globally informed and stable action
refinement. We evaluate KORR on long-horizon, fine-grained robotic furniture
assembly tasks under various perturbations. Results demonstrate consistent
gains in performance, robustness, and generalization over strong baselines. Our
findings further highlight the potential of Koopman-based modeling to bridge
modern learning methods with classical control theory. For more details, please
refer to https://jiachengliu3.github.io/TrajBooster.

</details>


### [278] [Tenma: Robust Cross-Embodiment Robot Manipulation with Diffusion Transformer](https://arxiv.org/abs/2509.11865)
*Travis Davies,Yiqi Huang,Yunxin Liu,Xiang Chen,Huxian Liu,Luhui Hu*

Main category: cs.RO

TL;DR: 本文提出了Tenma，一种轻量级的扩散-Transformer策略网络，可实现多模态双臂机器人操作任务，显著提升稳健性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer策略和扩散模型推动了机器人操作的发展，但在资源有限、需跨机器人结构学习的情境下，将这两者结合且兼顾高效与泛化能力依然困难。因此，需探索有效设计来提升多模态、跨结构模仿学习的能力。

Method: Tenma将多视角RGB、机体感知和语言融合输入，并通过跨结构归一化器把不同状态/动作空间映射到共享潜空间。提出联合状态-时间编码器，实现对齐的时序观测学习与推理加速，扩散动作解码器优化了训练稳定性和学习容量。所有模块均针对通用和稳健设计。

Result: 在多个基准测试及相同计算资源下，Tenma在分布内任务成功率平均为88.95%，在物体或场景变化条件下仍保持高性能，远超于分布内平均仅18.12%的基线策略。

Conclusion: Tenma即使使用中等规模数据，也能实现高鲁棒性操作和泛化。结果验证了多模态跨结构学习策略与Transformer增强模仿学习策略的巨大潜力。

Abstract: Scaling Transformer policies and diffusion models has advanced robotic
manipulation, yet combining these techniques in lightweight, cross-embodiment
learning settings remains challenging. We study design choices that most affect
stability and performance for diffusion-transformer policies trained on
heterogeneous, multimodal robot data, and introduce Tenma, a lightweight
diffusion-transformer for bi-manual arm control. Tenma integrates multiview
RGB, proprioception, and language via a cross-embodiment normalizer that maps
disparate state/action spaces into a shared latent space; a Joint State-Time
encoder for temporally aligned observation learning with inference speed
boosts; and a diffusion action decoder optimized for training stability and
learning capacity. Across benchmarks and under matched compute, Tenma achieves
an average success rate of 88.95% in-distribution and maintains strong
performance under object and scene shifts, substantially exceeding baseline
policies whose best in-distribution average is 18.12%. Despite using moderate
data scale, Tenma delivers robust manipulation and generalization, indicating
the great potential for multimodal and cross-embodiment learning strategies for
further augmenting the capacity of transformer-based imitation learning
policies.

</details>


### [279] [VH-Diffuser: Variable Horizon Diffusion Planner for Time-Aware Goal-Conditioned Trajectory Planning](https://arxiv.org/abs/2509.11930)
*Ruijia Liu,Ancheng Hou,Shaoyuan Li,Xiang Yin*

Main category: cs.RO

TL;DR: 提出了一种新颖的可变轨迹长度扩散规划器（VHD），通过学习轨迹长度提升在任务中的稳健性和效率。


<details>
  <summary>Details</summary>
Motivation: 目前扩散规划方法通常使用预设的固定轨迹长度，这使得其在遇到不同难度和长度需求的任务时容易出现长度不匹配的问题，导致任务成功率下降和表现不佳。

Method: VHD方法将轨迹长度视为可学习的变量，引入Length Predictor模型针对每个任务预测适合的轨迹长度，然后用这个长度指导扩散式规划器生成合适长度的轨迹。与现有方法兼容，通过初始噪声形态和随机截取子轨迹进行训练，无需更改网络结构。

Result: 在迷宫导航和机械臂控制等基准任务上，VHD显著提升了成功率与轨迹效率，对轨迹长度不匹配和未见过的长度更具鲁棒性。

Conclusion: VHD有效缓解了固定长度轨迹的不适应问题，在不增加训练复杂度的前提下提升了泛化能力和任务表现。

Abstract: Diffusion-based planners have gained significant recent attention for their
robustness and performance in long-horizon tasks. However, most existing
planners rely on a fixed, pre-specified horizon during both training and
inference. This rigidity often produces length-mismatch (trajectories that are
too short or too long) and brittle performance across instances with varying
geometric or dynamical difficulty. In this paper, we introduce the Variable
Horizon Diffuser (VHD) framework, which treats the horizon as a learned
variable rather than a fixed hyperparameter. Given a start-goal pair, we first
predict an instance-specific horizon using a learned Length Predictor model,
which guides a Diffusion Planner to generate a trajectory of the desired
length. Our design maintains compatibility with existing diffusion planners by
controlling trajectory length through initial noise shaping and training on
randomly cropped sub-trajectories, without requiring architectural changes.
Empirically, VHD improves success rates and path efficiency in maze-navigation
and robot-arm control benchmarks, showing greater robustness to horizon
mismatch and unseen lengths, while keeping training simple and offline-only.

</details>


### [280] [E2-BKI: Evidential Ellipsoidal Bayesian Kernel Inference for Uncertainty-aware Gaussian Semantic Mapping](https://arxiv.org/abs/2509.11964)
*Junyoung Kim,Minsik Jeon,Jihong Min,Kiho Kwak,Junwon Seo*

Main category: cs.RO

TL;DR: 本论文提出了一种具有不确定性感知能力的语义建图新框架，在处理复杂户外环境中的各种不确定性因素时显著提升了语义地图质量和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有语义建图方法在面对传感器稀疏、观测噪声等导致的不确定性问题时，推断质量和地图连续性均受影响，特别是在复杂户外环境下更为明显。该问题尚未有效解决。

Method: 该方法基于Evidential Deep Learning估计语义推断中的不确定性，并将该不确定性嵌入到BKI框架中，提升推断的鲁棒性。同时，将噪声观测通过高斯聚合成一致的表达，有效抑制不可靠观测点的影响，并采用几何对齐核来适应复杂场景结构，实现对局部几何与语义信息的高效融合。

Result: 在多种城市和越野户外环境中进行全面评测，结果显示新方法在语义建图质量、不确定性校准、表达灵活性及鲁棒性方面均有稳定提升，同时能保持实时推理效率。

Conclusion: 本文方法能有效处理多源不确定性，在复杂户外场景实现高质量的语义建图，表现出较强的泛化能力和系统鲁棒性。

Abstract: Semantic mapping aims to construct a 3D semantic representation of the
environment, providing essential knowledge for robots operating in complex
outdoor settings. While Bayesian Kernel Inference (BKI) addresses
discontinuities of map inference from sparse sensor data, existing semantic
mapping methods suffer from various sources of uncertainties in challenging
outdoor environments. To address these issues, we propose an uncertainty-aware
semantic mapping framework that handles multiple sources of uncertainties,
which significantly degrade mapping performance. Our method estimates
uncertainties in semantic predictions using Evidential Deep Learning and
incorporates them into BKI for robust semantic inference. It further aggregates
noisy observations into coherent Gaussian representations to mitigate the
impact of unreliable points, while employing geometry-aligned kernels that
adapt to complex scene structures. These Gaussian primitives effectively fuse
local geometric and semantic information, enabling robust, uncertainty-aware
mapping in complex outdoor scenarios. Comprehensive evaluation across diverse
off-road and urban outdoor environments demonstrates consistent improvements in
mapping quality, uncertainty calibration, representational flexibility, and
robustness, while maintaining real-time efficiency.

</details>


### [281] [Time-Constrained Intelligent Adversaries for Automation Vulnerability Testing: A Multi-Robot Patrol Case Study](https://arxiv.org/abs/2509.11971)
*James C. Ward,Alex Bott,Connor York,Edmund R. Hunt*

Main category: cs.RO

TL;DR: 该论文提出了一种基于机器学习的对抗者模型，通过模拟有敌意的攻击，评估多机器人巡逻系统的鲁棒性和安全性。实验结果表明，该新模型比现有基线方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 面对物理自主系统（如巡逻机器人）可能遭受的敌对攻击，需要有效方法来检测系统在现实攻击下的脆弱点，并据此优化巡逻策略，提高安全性。

Method: 提出了一套基于机器学习的敌对模型，让攻击者观察机器人巡逻行为，试图在限定时间内秘密进入受保护场所。通过模拟这种攻击，系统能在更接近真实攻击者的情境下进行安全性测试。

Result: 实验表明，新提出的攻击者模型在突破巡逻系统方面优于现有基线模型，对多种主流去中心化多机器人巡逻策略均起到了更严格的考验作用。

Conclusion: 引入基于机器学习的攻击者模型能为多机器人巡逻系统的脆弱性评估和安全策略优化提供更有效的测试手段，有助于未来巡逻策略的改进设计。

Abstract: Simulating hostile attacks of physical autonomous systems can be a useful
tool to examine their robustness to attack and inform vulnerability-aware
design. In this work, we examine this through the lens of multi-robot patrol,
by presenting a machine learning-based adversary model that observes robot
patrol behavior in order to attempt to gain undetected access to a secure
environment within a limited time duration. Such a model allows for evaluation
of a patrol system against a realistic potential adversary, offering insight
into future patrol strategy design. We show that our new model outperforms
existing baselines, thus providing a more stringent test, and examine its
performance against multiple leading decentralized multi-robot patrol
strategies.

</details>


### [282] [Gesture-Based Robot Control Integrating Mm-wave Radar and Behavior Trees](https://arxiv.org/abs/2509.12008)
*Yuqing Song,Cesare Tonola,Stefano Savazzi,Sanaz Kianoush,Nicola Pedrocchi,Stephan Sigg*

Main category: cs.RO

TL;DR: 本论文提出了一种基于毫米波雷达的手势控制机器人手臂系统，实现了实时、无接触的人机交互，有效提升了交互的直观性和效率。


<details>
  <summary>Details</summary>
Motivation: 随着机器人在家庭和工业场景中的普及，直观高效的人机交互需求不断增加。相比传统的摄像头视觉系统，雷达感知技术在隐私保护、遮挡和光照条件下表现更优，因此期望利用毫米波雷达提升手势识别的人机交互体验。

Method: 利用毫米波雷达实现手势识别，将9种手势映射为机器臂的实时指令。系统将手势识别和机器人控制统一进一个实时处理流程，实现无接触的自然交互，并通过案例测试其实用性和可靠性。

Result: 系统能够精确识别九种手势并实时控制机器人手臂，案例研究表明该系统在操作性、性能和可靠性方面表现良好。

Conclusion: 该研究提出的毫米波雷达手势控制系统，不仅保护隐私且对环境鲁棒，能够为未来机器人无接触交互提供安全、可靠、高效的解决方案。

Abstract: As robots become increasingly prevalent in both homes and industrial
settings, the demand for intuitive and efficient human-machine interaction
continues to rise. Gesture recognition offers an intuitive control method that
does not require physical contact with devices and can be implemented using
various sensing technologies. Wireless solutions are particularly flexible and
minimally invasive. While camera-based vision systems are commonly used, they
often raise privacy concerns and can struggle in complex or poorly lit
environments. In contrast, radar sensing preserves privacy, is robust to
occlusions and lighting, and provides rich spatial data such as distance,
relative velocity, and angle. We present a gesture-controlled robotic arm using
mm-wave radar for reliable, contactless motion recognition. Nine gestures are
recognized and mapped to real-time commands with precision. Case studies are
conducted to demonstrate the system practicality, performance and reliability
for gesture-based robotic manipulation. Unlike prior work that treats gesture
recognition and robotic control separately, our system unifies both into a
real-time pipeline for seamless, contactless human-robot interaction.

</details>


### [283] [Embodied Navigation Foundation Model](https://arxiv.org/abs/2509.12129)
*Jiazhao Zhang,Anqi Li,Yunpeng Qi,Minghan Li,Jiahang Liu,Shaoan Wang,Haoran Liu,Gengze Zhou,Yuze Wu,Xingxing Li,Yuxin Fan,Wenjun Li,Zhibo Chen,Fei Gao,Qi Wu,Zhizheng Zhang,He Wang*

Main category: cs.RO

TL;DR: 本文提出了一种名为NavFoM的通用导航基础模型，能够跨多种机器人平台和任务类型实现高效导航，并在多个公开基准上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 当前大规模视觉-语言模型（VLMs）在一般视觉语言任务上表现突出，但在具身导航任务中的泛化能力有限，只适用于特定任务设置和硬件架构。该研究旨在突破这一局限，实现能够跨不同机器人（如四足机器人、无人机、轮式机器人、车辆）和跨任务（如视觉-语言导航、目标搜寻、目标追踪、自动驾驶）的通用导航方法。

Method: NavFoM基于统一的多模态架构，能处理不同相机配置和时序需求的导航输入；引入编码相机视角和时序信息的标识符（identifier tokens）；采用动态调整采样策略，在有限token预算下高效利用观测信息。该模型在8百万导航样本上进行训练，样本覆盖多种具身平台和任务类型。

Result: NavFoM在多个公共导航基准上达到了最新的SOTA（最佳）或具备高度竞争力的表现，无需针对具体任务微调。此外，实物实验也验证了其良好的泛化能力和实际应用潜力。

Conclusion: NavFoM能够实现跨平台、跨任务的具身导航，兼具高泛化能力与实际部署价值，为通用智能体的导航提供了坚实的基础。

Abstract: Navigation is a fundamental capability in embodied AI, representing the
intelligence required to perceive and interact within physical environments
following language instructions. Despite significant progress in large
Vision-Language Models (VLMs), which exhibit remarkable zero-shot performance
on general vision-language tasks, their generalization ability in embodied
navigation remains largely confined to narrow task settings and
embodiment-specific architectures. In this work, we introduce a
cross-embodiment and cross-task Navigation Foundation Model (NavFoM), trained
on eight million navigation samples that encompass quadrupeds, drones, wheeled
robots, and vehicles, and spanning diverse tasks such as vision-and-language
navigation, object searching, target tracking, and autonomous driving. NavFoM
employs a unified architecture that processes multimodal navigation inputs from
varying camera configurations and navigation horizons. To accommodate diverse
camera setups and temporal horizons, NavFoM incorporates identifier tokens that
embed camera view information of embodiments and the temporal context of tasks.
Furthermore, to meet the demands of real-world deployment, NavFoM controls all
observation tokens using a dynamically adjusted sampling strategy under a
limited token length budget. Extensive evaluations on public benchmarks
demonstrate that our model achieves state-of-the-art or highly competitive
performance across multiple navigation tasks and embodiments without requiring
task-specific fine-tuning. Additional real-world experiments further confirm
the strong generalization capability and practical applicability of our
approach.

</details>


### [284] [Learning Contact Dynamics for Control with Action-conditioned Face Interaction Graph Networks](https://arxiv.org/abs/2509.12151)
*Zongyao Yi,Joachim Hertzberg,Martin Atzmueller*

Main category: cs.RO

TL;DR: 本文提出了一种可学习的物理仿真器，在机器人复杂操作任务中能更准确预测末端执行器的运动和力矩。该方法基于GNN，扩展了节点和边类型，实现了面向动作的预测，并在仿真和真实环境中显著优于传统仿真器。


<details>
  <summary>Details</summary>
Motivation: 传统物理仿真器在处理机器人末端执行器与环境复杂接触时，运动及力矩预测准确率有限，难以满足现代机器人控制与估计的需求。

Method: 作者基于最新的图神经网络物理仿真（FIGNet），引入了新的节点和边类型，使模型能够基于动作条件，进行更精准的运动和力矩预测。用MPC控制器在仿真和真实操作中测试了该模型。

Result: 仿真中，基于该模型的MPC控制器在难度高的插销任务上表现与理想动力学模型一致。在真实实验中，相较于传统物理仿真器，运动预测准确率提升50%，力矩预测精度提升3倍。

Conclusion: 该可学习物理仿真器在复杂接触机械操作任务中提升了预测精度和控制性能，优于传统方法，具有实际应用潜力，并开放源码和数据以促进后续研究。

Abstract: We present a learnable physics simulator that provides accurate motion and
force-torque prediction of robot end effectors in contact-rich manipulation.
The proposed model extends the state-of-the-art GNN-based simulator (FIGNet)
with novel node and edge types, enabling action-conditional predictions for
control and state estimation tasks. In simulation, the MPC agent using our
model matches the performance of the same controller with the ground truth
dynamics model in a challenging peg-in-hole task, while in the real-world
experiment, our model achieves a 50% improvement in motion prediction accuracy
and 3$\times$ increase in force-torque prediction precision over the baseline
physics simulator. Source code and data are publicly available.

</details>
