<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 122]
- [cs.CL](#cs.CL) [Total: 45]
- [cs.RO](#cs.RO) [Total: 21]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A Context-aware Attention and Graph Neural Network-based Multimodal Framework for Misogyny Detection](https://arxiv.org/abs/2508.09175)
*Mohammad Zia Ur Rehman,Sufyaan Zahoor,Areeb Manzoor,Musharaf Maqbool,Nagendra Kumar*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态方法，专门用于检测社交媒体上的厌女和性别歧视内容，并在主流数据集上显著提升了检测效果。


<details>
  <summary>Details</summary>
Motivation: 当前通用的攻击性内容检测方法难以有效识别针对女性的厌女内容，因此亟需专门针对女性攻击的检测方法。

Method: 提出包含多模态注意力模块（MANM）、基于图的特征重构模块（GFRM）和内容特异特征学习模块（CFLM）三大模块的架构，同时引入厌女词典打分与特征空间增强，旨在更好地学习图像、文本及其交互中的厌女特征。

Result: 在MAMI和MMHS150K两个多模态数据集上，方法的宏F1分数分别比已有方法提升了10.17%和8.88%。

Conclusion: 本方法能有效提升社交媒体上针对女性的攻击性、厌女内容检测表现，对性别歧视内容的智能识别具有实际价值。

Abstract: A substantial portion of offensive content on social media is directed
towards women. Since the approaches for general offensive content detection
face a challenge in detecting misogynistic content, it requires solutions
tailored to address offensive content against women. To this end, we propose a
novel multimodal framework for the detection of misogynistic and sexist
content. The framework comprises three modules: the Multimodal Attention module
(MANM), the Graph-based Feature Reconstruction Module (GFRM), and the
Content-specific Features Learning Module (CFLM). The MANM employs adaptive
gating-based multimodal context-aware attention, enabling the model to focus on
relevant visual and textual information and generating contextually relevant
features. The GFRM module utilizes graphs to refine features within individual
modalities, while the CFLM focuses on learning text and image-specific features
such as toxicity features and caption features. Additionally, we curate a set
of misogynous lexicons to compute the misogyny-specific lexicon score from the
text. We apply test-time augmentation in feature space to better generalize the
predictions on diverse inputs. The performance of the proposed approach has
been evaluated on two multimodal datasets, MAMI and MMHS150K, with 11,000 and
13,494 samples, respectively. The proposed method demonstrates an average
improvement of 10.17% and 8.88% in macro-F1 over existing methods on the MAMI
and MMHS150K datasets, respectively.

</details>


### [2] [IAD-R1: Reinforcing Consistent Reasoning in Industrial Anomaly Detection](https://arxiv.org/abs/2508.09178)
*Yanhui Li,Yunkang Cao,Chengliang Liu,Yuan Xiong,Xinghui Dong,Chao Huang*

Main category: cs.CV

TL;DR: 提出了一种通用的后训练框架IAD-R1，显著提升多种视觉-语言模型（VLM）的工业异常检测能力，0.5B参数模型甚至超越GPT-4.1等商用大模型。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测因缺少缺陷样本而难以泛化，传统检测方法多为特定场景专用，现有VLM的泛化优势并未充分体现在工业检测领域。

Method: 设计两阶段训练策略：1）“感知激活监督微调”（PA-SFT），利用精心构建的Expert-AD Chain-of-Thought数据集训练模型，提高模型识别异常与推理能力；2）“结构化对照组相对策略优化”（SC-GRPO），通过奖励函数引导模型从单纯的异常感知向异常解释跃迁。该框架可用于不同架构、参数规模的VLM。

Result: IAD-R1在7个VLM上的性能大幅提升，在6个工业异常检测基准数据集上平均准确率最高提升43.3%。IAD-R1训练的0.5B参数模型在zero-shot场景下超越当前主流商用大模型如GPT-4.1、Claude-Sonnet-4。

Conclusion: IAD-R1有效提升了VLM在工业异常检测中的泛化和解释能力，技术优越性和通用性突出。数据集、代码及模型权重均将开源，推动工业智能检测技术发展。

Abstract: Industrial anomaly detection is a critical component of modern manufacturing,
yet the scarcity of defective samples restricts traditional detection methods
to scenario-specific applications. Although Vision-Language Models (VLMs)
demonstrate significant advantages in generalization capabilities, their
performance in industrial anomaly detection remains limited. To address this
challenge, we propose IAD-R1, a universal post-training framework applicable to
VLMs of different architectures and parameter scales, which substantially
enhances their anomaly detection capabilities. IAD-R1 employs a two-stage
training strategy: the Perception Activation Supervised Fine-Tuning (PA-SFT)
stage utilizes a meticulously constructed high-quality Chain-of-Thought dataset
(Expert-AD) for training, enhancing anomaly perception capabilities and
establishing reasoning-to-answer correlations; the Structured Control Group
Relative Policy Optimization (SC-GRPO) stage employs carefully designed reward
functions to achieve a capability leap from "Anomaly Perception" to "Anomaly
Interpretation". Experimental results demonstrate that IAD-R1 achieves
significant improvements across 7 VLMs, attaining up to 43.3% enhancement in
average accuracy on 6 industrial anomaly detection benchmark datasets. Notably,
the 0.5B parameter model trained with IAD-R1 surpasses commercial models
including GPT-4.1 and Claude-Sonnet-4 in zero-shot settings, demonstrating the
effectiveness and superiority of IAD-R1. The dataset, code, and all model
weights will be publicly available at https://github.com/Yanhui-Lee/IAD-R1.

</details>


### [3] [A Neurosymbolic Framework for Interpretable Cognitive Attack Detection in Augmented Reality](https://arxiv.org/abs/2508.09185)
*Rongqian Chen,Allison Andreyev,Yanming Xiu,Mahdi Imani,Bin Li,Maria Gorlatova,Gang Tan,Tian Lan*

Main category: cs.CV

TL;DR: 本论文提出了一种结合神经符号方法的AR认知攻击检测技术CADAR，显著提升了检测准确率并兼具可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着增强现实逐渐普及，针对其内容的认知攻击风险增加，现有方法要么缺乏语义推理能力，要么可解释性有限，因此急需更有效且能解释的检测方法。

Method: 作者提出了CADAR模型：首先通过神经VLM多模态输入生成符号感知图，并融合先验知识、显著性与时间关联性；之后利用粒子滤波进行统计推理，实现认知攻击检测。

Result: 在扩展的AR认知攻击数据集上，CADAR在多种具有挑战性的攻击场景中表现出色，最高比强基线提升10.7%的检测准确率。

Conclusion: 神经符号方法能够在准确性与可解释性之间取得平衡，是增强现实认知攻击检测的有效方案。

Abstract: Augmented Reality (AR) enriches perception by overlaying virtual elements on
the physical world. Due to its growing popularity, cognitive attacks that alter
AR content to manipulate users' semantic perception have received increasing
attention. Existing detection methods often focus on visual changes, which are
restricted to pixel- or image-level processing and lack semantic reasoning
capabilities, or they rely on pre-trained vision-language models (VLMs), which
function as black-box approaches with limited interpretability. In this paper,
we present CADAR, a novel neurosymbolic approach for cognitive attack detection
in AR. It fuses multimodal vision-language inputs using neural VLMs to obtain a
symbolic perception-graph representation, incorporating prior knowledge,
salience weighting, and temporal correlations. The model then enables
particle-filter based statistical reasoning -- a sequential Monte Carlo method
-- to detect cognitive attacks. Thus, CADAR inherits the adaptability of
pre-trained VLM and the interpretability and reasoning rigor of particle
filtering. Experiments on an extended AR cognitive attack dataset show accuracy
improvements of up to 10.7% over strong baselines on challenging AR attack
scenarios, underscoring the promise of neurosymbolic methods for effective and
interpretable cognitive attack detection.

</details>


### [4] [RL-MoE: An Image-Based Privacy Preserving Approach In Intelligent Transportation System](https://arxiv.org/abs/2508.09186)
*Abdolazim Rezaei,Mehdi Sookhak,Mahboobeh Haghparast*

Main category: cs.CV

TL;DR: 本文提出了一种名为RL-MoE的新框架，将敏感的视觉数据转换为隐私保护的文本描述，从而无需直接传输图像，兼顾了数据实用性与隐私。


<details>
  <summary>Details</summary>
Motivation: 在智能交通系统（ITS）中，基于AI的摄像头广泛应用，但直接传输视觉数据可能侵犯个人隐私。现有隐私保护方法（如模糊处理或加密）往往无法在隐私与数据实用性之间取得平衡。

Method: RL-MoE结合了专家混合（MoE）架构用于多方面场景分解，并利用强化学习（RL）算法优化生成文本，在确保语义准确的同时最大化隐私保护。

Result: 实验表明，RL-MoE显著提升了隐私保护效果，将CFP-FP数据集上的重放攻击成功率降低至9.4%，且生成的文本内容相较基线方法更为丰富。

Conclusion: RL-MoE为在隐私敏感领域（如智慧城市、自动驾驶网络）构建可信AI系统提供了切实可行且可扩展的解决方案。

Abstract: The proliferation of AI-powered cameras in Intelligent Transportation Systems
(ITS) creates a severe conflict between the need for rich visual data and the
fundamental right to privacy. Existing privacy-preserving mechanisms, such as
blurring or encryption, are often insufficient, creating an undesirable
trade-off where either privacy is compromised against advanced reconstruction
attacks or data utility is critically degraded. To resolve this impasse, we
propose RL-MoE, a novel framework that transforms sensitive visual data into
privacy-preserving textual descriptions, eliminating the need for direct image
transmission. RL-MoE uniquely combines a Mixture-of-Experts (MoE) architecture
for nuanced, multi-aspect scene decomposition with a Reinforcement Learning
(RL) agent that optimizes the generated text for a dual objective of semantic
accuracy and privacy preservation. Extensive experiments demonstrate that
RL-MoE provides superior privacy protection, reducing the success rate of
replay attacks to just 9.4\% on the CFP-FP dataset, while simultaneously
generating richer textual content than baseline methods. Our work provides a
practical and scalable solution for building trustworthy AI systems in
privacy-sensitive domains, paving the way for more secure smart city and
autonomous vehicle networks.

</details>


### [5] [Synthetic Data Generation for Emotional Depth Faces: Optimizing Conditional DCGANs via Genetic Algorithms in the Latent Space and Stabilizing Training with Knowledge Distillation](https://arxiv.org/abs/2508.09188)
*Seyed Muhammad Hossein Mousavi,S. Younes Mirinezhad*

Main category: cs.CV

TL;DR: 本文提出了一种新的合成深度人脸数据集生成方法，结合优化的GAN、知识蒸馏和遗传算法，显著提升了表情多样性、图像质量和情感分类准确率，优于现有多种方法。


<details>
  <summary>Details</summary>
Motivation: 情感计算领域难以获得高质量、多样性的深度面部数据集，尤其是在识别细微情感表情时数据集稀缺，限制了算法性能提升。作者希望通过生成逼真的合成数据缓解此问题。

Method: 提出采用优化版生成对抗网络（GAN），结合EMA教师模型进行知识蒸馏以稳定训练、防止模式崩溃，并利用遗传算法优化潜变量以提升生成数据的多样性和质量。提取LBP、HOG、Sobel边缘和强度直方图等特征给XGBoost分类器用于情感识别。

Result: 提出方法在多样性与质量上均优于GAN、VAE、GMM和KDE。采用XGBoost进行分类，准确率分别达到94%和96%。使用FID、IS、SSIM、PSNR等主流指标评估，均优于最新方法。

Conclusion: 本文方法显著提升了深度人脸情感合成数据的多样性与质量，并在情感分类准确率上取得优良表现，有望促进情感计算领域发展。

Abstract: Affective computing faces a major challenge: the lack of high-quality,
diverse depth facial datasets for recognizing subtle emotional expressions. We
propose a framework for synthetic depth face generation using an optimized GAN
with Knowledge Distillation (EMA teacher models) to stabilize training, improve
quality, and prevent mode collapse. We also apply Genetic Algorithms to evolve
GAN latent vectors based on image statistics, boosting diversity and visual
quality for target emotions. The approach outperforms GAN, VAE, GMM, and KDE in
both diversity and quality. For classification, we extract and concatenate LBP,
HOG, Sobel edge, and intensity histogram features, achieving 94% and 96%
accuracy with XGBoost. Evaluation using FID, IS, SSIM, and PSNR shows
consistent improvement over state-of-the-art methods.

</details>


### [6] [$Δ$-AttnMask: Attention-Guided Masked Hidden States for Efficient Data Selection and Augmentation](https://arxiv.org/abs/2508.09199)
*Jucheng Hu,Suorong Yang,Dongzhan Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种高效的数据选择方法Δ-AttnMask，用于视觉-语言模型（VLM）的视觉指令微调（VIF），能仅用20%的数据达到甚至超越全量数据基线性能。


<details>
  <summary>Details</summary>
Motivation: 当前VIF对高质量多模态（图片+文本）数据的需求远超单模态指令微调，但优质数据的获取与挑选难度较大，且相关研究不足。

Method: 提出Δ-AttnMask方法，通过注意力引导对模型隐状态进行掩码，比较高注意力区域与原始状态的损失差异，无需领域标签、辅助模型或额外训练，实现对图文样本对质量的内在评估。

Result: Δ-AttnMask仅用20%训练数据即可达SOTA表现，训练加速5倍，并能在多个模型与数据集上，相比全量数据基线提升10.1%的整体准确率。

Conclusion: Δ-AttnMask为VIF任务提供了一种高效、通用的数据选择框架，有助于大幅减小所需数据量并提升模型性能，适用于不同模态与架构。

Abstract: Visual Instruction Finetuning (VIF) is pivotal for post-training
Vision-Language Models (VLMs). Unlike unimodal instruction finetuning in
plain-text large language models, which mainly requires instruction datasets to
enable model instruction-following ability, VIF also requires multimodal data
to enable joint visual and textual understanding; therefore, it typically
requires more data. Consequently, VIF imposes stricter data selection
challenges: the method must scale efficiently to handle larger data demands
while ensuring the quality of both visual and textual content, as well as their
alignment. Despite its critical impact on performance, data selection for VIF
remains an understudied area. In this paper, we propose $\Delta$-AttnMask. This
data-efficient framework quantifies sample quality through attention-guided
masking of the model's hidden states, jointly evaluating image-text pairs
without requiring domain labels, auxiliary models, or extra training. By
computing loss differences ($\Delta$) between the original states and states
masked using high-attention regions, $\Delta$-AttnMask intrinsically assesses
sample quality. Experiments across multiple VLMs and datasets show that
$\Delta$-AttnMask achieves state-of-the-art performance with just 20% of data,
accelerating training by 5x while surpassing full-dataset baselines by +10.1%
in overall accuracy. Its model-agnostic and data-agnostic design ensures broad
applicability across modalities and architectures.

</details>


### [7] [Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method](https://arxiv.org/abs/2508.09202)
*Masoumeh Sharafi,Soufiane Belharbi,Houssem Ben Salem,Ali Etemad,Alessandro Lameiras Koerich,Marco Pedersoli,Simon Bacon,Eric Granger*

Main category: cs.CV

TL;DR: 本文提出了一种针对仅有中性表情的无标签目标数据且无法获取源数据情况下的个性化特征迁移(PFT)方法，实现了高效的源无关域自适应(SFDA)人脸表情识别。该方法在潜在空间进行特征转换，无需生成面部图像，减小计算量，同时提升模型在实际应用中的适应性和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸表情识别(FER)在解决细微表情和个体差异上的表现有限；实际应用中经常出现只有中性表情的无标签目标数据，并且涉及隐私时难以获取源数据。这一局限促使作者探索如何在极端受限场景下通过源无关域自适应技术提升FER性能。

Method: 提出个性化特征迁移(PFT)方法，核心思想是：先在源域数据上预训练一个轻量级的特征转换网络，使其能够在潜在空间中将某主体的个性化风格特征转到另一主体，同时通过表达一致性与风格感知损失保持表情信息；适应阶段仅用目标域的中性表情无标签数据，对转化器进行调整，无需合成表情图像，更高效也更安全。

Result: PFT方法能够在仅有目标域中性表情且无源数据的情景下，有效提升表情分类的判别能力和适应性。且与现有基于图像生成的方法相比，PFT运算量更小、不易受噪声影响，迁移过程表现更稳定。

Conclusion: 个性化特征迁移(PFT)为源无关域自适应FER提供了一种高效、隐私友好、计算量低的解决方案，尤其适用于现实中数据资源有限与隐私受限的场景，可在保证识别精度的同时极大降低部署与运算成本。

Abstract: Facial expression recognition (FER) models are employed in many video-based
affective computing applications, such as human-computer interaction and
healthcare monitoring. However, deep FER models often struggle with subtle
expressions and high inter-subject variability, limiting their performance in
real-world applications. To improve their performance, source-free domain
adaptation (SFDA) methods have been proposed to personalize a pretrained source
model using only unlabeled target domain data, thereby avoiding data privacy,
storage, and transmission constraints. This paper addresses a challenging
scenario where source data is unavailable for adaptation, and only unlabeled
target data consisting solely of neutral expressions is available. SFDA methods
are not typically designed to adapt using target data from only a single class.
Further, using models to generate facial images with non-neutral expressions
can be unstable and computationally intensive. In this paper, personalized
feature translation (PFT) is proposed for SFDA. Unlike current image
translation methods for SFDA, our lightweight method operates in the latent
space. We first pre-train the translator on the source domain data to transform
the subject-specific style features from one source subject into another.
Expression information is preserved by optimizing a combination of expression
consistency and style-aware objectives. Then, the translator is adapted on
neutral target data, without using source data or image synthesis. By
translating in the latent space, PFT avoids the complexity and noise of face
expression generation, producing discriminative embeddings optimized for
classification. Using PFT eliminates the need for image synthesis, reduces
computational overhead (using a lightweight translator), and only adapts part
of the model, making the method efficient compared to image-based translation.

</details>


### [8] [GANime: Generating Anime and Manga Character Drawings from Sketches with Deep Learning](https://arxiv.org/abs/2508.09207)
*Tai Vu,Robert Yang*

Main category: cs.CV

TL;DR: 本文比较了多种图像到图像转换模型在动漫线稿自动上色任务中的表现，发现C-GAN模型效果最佳。


<details>
  <summary>Details</summary>
Motivation: 手工将线稿转换为全彩图片是动漫和漫画行业的耗时、昂贵流程，因此亟需自动化解决方案。

Method: 对比和评估了三种常见的图像到图像转换模型：Neural Style Transfer、C-GAN和CycleGAN，并通过定性与定量分析方法进行比较。

Result: 结果显示C-GAN模型在图像质量和分辨率方面优于其他方法，生成的图片接近人工上色效果。

Conclusion: C-GAN作为自动线稿上色工具有望帮助提高动漫行业生产效率，降低成本，具有实际应用价值。

Abstract: The process of generating fully colorized drawings from sketches is a large,
usually costly bottleneck in the manga and anime industry. In this study, we
examine multiple models for image-to-image translation between anime characters
and their sketches, including Neural Style Transfer, C-GAN, and CycleGAN. By
assessing them qualitatively and quantitatively, we find that C-GAN is the most
effective model that is able to produce high-quality and high-resolution images
close to those created by humans.

</details>


### [9] [MME-Emotion: A Holistic Evaluation Benchmark for Emotional Intelligence in Multimodal Large Language Models](https://arxiv.org/abs/2508.09210)
*Fan Zhang,Zebang Cheng,Chong Deng,Haoxuan Li,Zheng Lian,Qian Chen,Huadai Liu,Wen Wang,Yi-Fan Zhang,Renrui Zhang,Ziyu Guo,Zhihong Zhu,Hao Wu,Haixin Wang,Yefeng Zheng,Xiaojiang Peng,Xian Wu,Kun Wang,Xiangang Li,Jieping Ye,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 本文提出了MME-Emotion，是当前最大规模、最系统的多模态大模型（MLLMs）情感智能基准，弥补了现有模型在情感理解及推理能力评测上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在情感计算领域虽有进展，但缺乏全面、细致的基准来评测其通用性和推理情感成因的能力。为了推动MLLMs在情感智能方面的发展，急需开发具备广泛覆盖场景和统一评测体系的新基准。

Method: 作者构建了MME-Emotion基准，收集6000多条视频片段，并设计针对情感理解与推理的八大任务，形成QA对。基准支持多种评测场景与混合指标，通过多智能体系统框架进行系统性分析，对20个主流MLLMs进行了严格评测。

Result: 结果表明，目前MLLMs在情感智能方面表现不佳，最佳模型在情感识别和推理任务上的得分仅为39.3%和56.0%。通用型模型（如Gemini-2.5-Pro）主要依靠强大多模态能力实现情感理解，专用型模型（如R1-Omni）可通过领域微调达成相似性能。

Conclusion: MME-Emotion为评测和推动MLLMs的情感智能能力提供了统一、可扩展的平台，有望促进未来情感智能领域的进一步突破。

Abstract: Recent advances in multimodal large language models (MLLMs) have catalyzed
transformative progress in affective computing, enabling models to exhibit
emergent emotional intelligence. Despite substantial methodological progress,
current emotional benchmarks remain limited, as it is still unknown: (a) the
generalization abilities of MLLMs across distinct scenarios, and (b) their
reasoning capabilities to identify the triggering factors behind emotional
states. To bridge these gaps, we present \textbf{MME-Emotion}, a systematic
benchmark that assesses both emotional understanding and reasoning capabilities
of MLLMs, enjoying \textit{scalable capacity}, \textit{diverse settings}, and
\textit{unified protocols}. As the largest emotional intelligence benchmark for
MLLMs, MME-Emotion contains over 6,000 curated video clips with task-specific
questioning-answering (QA) pairs, spanning broad scenarios to formulate eight
emotional tasks. It further incorporates a holistic evaluation suite with
hybrid metrics for emotion recognition and reasoning, analyzed through a
multi-agent system framework. Through a rigorous evaluation of 20 advanced
MLLMs, we uncover both their strengths and limitations, yielding several key
insights: \ding{182} Current MLLMs exhibit unsatisfactory emotional
intelligence, with the best-performing model achieving only $39.3\%$
recognition score and $56.0\%$ Chain-of-Thought (CoT) score on our benchmark.
\ding{183} Generalist models (\emph{e.g.}, Gemini-2.5-Pro) derive emotional
intelligence from generalized multimodal understanding capabilities, while
specialist models (\emph{e.g.}, R1-Omni) can achieve comparable performance
through domain-specific post-training adaptation. By introducing MME-Emotion,
we hope that it can serve as a foundation for advancing MLLMs' emotional
intelligence in the future.

</details>


### [10] [Towards Effective MLLM Jailbreaking Through Balanced On-Topicness and OOD-Intensity](https://arxiv.org/abs/2508.09218)
*Zuoou Li,Weitong Zhang,Jingyuan Wang,Shuyuan Zhang,Wenjia Bai,Bernhard Kainz,Mengyun Qiao*

Main category: cs.CV

TL;DR: MLLMs易被对抗提示攻破，提出新评估框架和攻击方法BSD，突破现安全机制，大幅提升攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型（MLLMs）存在被jailbreak（越狱）攻击的问题，但当前的攻击效果评估标准夸大了成功率，许多“成功”并未真正实现恶意目标，且安全机制对某些类型输入效果较差，因此需要更严谨的评估与更有效的攻击方法以暴露其真实弱点。

Method: 提出四轴评估框架，考虑输入相关性、输入分布外强度、输出危害性和拒答率，从而精准衡量攻击真实效果；同时提出Balanced Structural Decomposition（BSD）递归重写策略，将恶意提示拆解为语义相关子任务，并巧妙引入分布外与视觉信号，提升绕过检测能力。

Result: 在13个商用与开源MLLM上实验，BSD方法显著提升攻击成功率（67%提升）、有害输出比例（21%提升）、降低拒答率，远超现有攻击手法。

Conclusion: 当前多模态安全机制存在被BSD突破的显著弱点，未来评估攻击与防御MLLM安全需采用更科学标准与更强攻防手段。

Abstract: Multimodal large language models (MLLMs) are widely used in vision-language
reasoning tasks. However, their vulnerability to adversarial prompts remains a
serious concern, as safety mechanisms often fail to prevent the generation of
harmful outputs. Although recent jailbreak strategies report high success
rates, many responses classified as "successful" are actually benign, vague, or
unrelated to the intended malicious goal. This mismatch suggests that current
evaluation standards may overestimate the effectiveness of such attacks. To
address this issue, we introduce a four-axis evaluation framework that
considers input on-topicness, input out-of-distribution (OOD) intensity, output
harmfulness, and output refusal rate. This framework identifies truly effective
jailbreaks. In a substantial empirical study, we reveal a structural trade-off:
highly on-topic prompts are frequently blocked by safety filters, whereas those
that are too OOD often evade detection but fail to produce harmful content.
However, prompts that balance relevance and novelty are more likely to evade
filters and trigger dangerous output. Building on this insight, we develop a
recursive rewriting strategy called Balanced Structural Decomposition (BSD).
The approach restructures malicious prompts into semantically aligned
sub-tasks, while introducing subtle OOD signals and visual cues that make the
inputs harder to detect. BSD was tested across 13 commercial and open-source
MLLMs, where it consistently led to higher attack success rates, more harmful
outputs, and fewer refusals. Compared to previous methods, it improves success
rates by $67\%$ and harmfulness by $21\%$, revealing a previously
underappreciated weakness in current multimodal safety systems.

</details>


### [11] [Towards Scalable Training for Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2508.09220)
*Haoyang Li,Jiaqing Li,Jialun Cao,Zongyuan Yang,Yongping Xiong*

Main category: cs.CV

TL;DR: 本文提出了一种结合有限手写公式与大规模LaTeX生成公式的新方法，构建了迄今最大的公式数据集Tex80M，并提出首个大规模训练的HMER模型TexTeller，显著提升了识别性能。


<details>
  <summary>Details</summary>
Motivation: HMER领域因手写公式标注工作繁重且昂贵，数据稀缺，限制了模型性能和发展。作者希望通过扩大高质量训练数据集来弥补这一短板。

Method: 作者设计了可扩展的数据引擎，自动生成复杂且一致的LaTeX公式序列，并与有限的手写公式数据结合，构建了Tex80M数据集。随后，采用混合训练策略，提出并训练了TexTeller模型。

Result: TexTeller在几乎所有HMER基准上都取得了最新最优（SOTA）成绩，显示出大规模数据和新模型结构的有效性。

Conclusion: 开放模型、数据集与代码将极大推动领域研究发展。大规模高质量数据结合创新模型可显著提升HMER任务表现。

Abstract: Large foundation models have achieved significant performance gains through
scalable training on massive datasets. However, the field of
\textbf{H}andwritten \textbf{M}athematical \textbf{E}xpression
\textbf{R}ecognition (HMER) has been impeded by the scarcity of data, primarily
due to the arduous and costly process of manual annotation. To bridge this gap,
we propose a novel method integrating limited handwritten formulas with
large-scale LaTeX-rendered formulas by developing a scalable data engine to
generate complex and consistent LaTeX sequences. With this engine, we built the
largest formula dataset to date, termed \texttt{Tex80M}, comprising over 80
million high-quality training instances. Then we propose \texttt{TexTeller},
the first HMER model trained at scale, by mix-training \texttt{Tex80M} with a
relatively small HME dataset. The expansive training dataset and our refined
pipeline have equipped \texttt{TexTeller} with state-of-the-art (SOTA)
performance across nearly all benchmarks. To advance the field, we will openly
release our complete model, entire dataset, and full codebase, enabling further
research building upon our contributions.

</details>


### [12] [Gradient-Direction-Aware Density Control for 3D Gaussian Splatting](https://arxiv.org/abs/2508.09239)
*Zheng Zhou,Yu-Jie Xiong,Chun-Ming Xia,Jia-Chen Zhang,Hong-Jian Zhan*

Main category: cs.CV

TL;DR: 本论文提出了一种新方法，称为GDAGS，通过引入梯度方向感知来优化3D Gaussians Splatting的效率和效果，成功减少了内存占用并提升了新视角合成质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D Gaussians Splatting方法在复杂场景中存在两个主要问题：一是由于密度控制条件不足导致的大高斯球过度重构，二是梯度一致区域内高斯球过密，浪费内存资源。因此，需要一种能同时解决这两大问题的方法。

Method: 作者提出了GDAGS（Gradient-Direction-Aware Gaussian Splatting）的框架，引入了梯度相干性比（GCR）指标，通过归一化梯度向量的范数来区分梯度方向一致和冲突的高斯球，并引入非线性动态加权机制对密度进行自适应控制。具体而言：分裂时优先处理梯度冲突的高斯球以增强细节，克隆时优先处理梯度一致的高斯球以完善结构，并防止冲突高斯球过度复制。

Result: 在多种真实世界基准数据集上的实验显示，GDAGS显著提升了渲染质量，有效减少了过度重构和过密堆叠。同时，通过优化高斯球的利用，内存占用最高可减少50%。

Conclusion: GDAGS利用梯度方向信息，显著改进了3D高斯溅射在复杂场景下的表现，实现了更紧凑的场景表示、更优的渲染质量和更低的内存开销。

Abstract: The emergence of 3D Gaussian Splatting (3DGS) has significantly advanced
novel view synthesis through explicit scene representation, enabling real-time
photorealistic rendering. However, existing approaches manifest two critical
limitations in complex scenarios: (1) Over-reconstruction occurs when
persistent large Gaussians cannot meet adaptive splitting thresholds during
density control. This is exacerbated by conflicting gradient directions that
prevent effective splitting of these Gaussians; (2) Over-densification of
Gaussians occurs in regions with aligned gradient aggregation, leading to
redundant component proliferation. This redundancy significantly increases
memory overhead due to unnecessary data retention. We present
Gradient-Direction-Aware Gaussian Splatting (GDAGS), a gradient-direction-aware
adaptive density control framework to address these challenges. Our key
innovations: the gradient coherence ratio (GCR), computed through normalized
gradient vector norms, which explicitly discriminates Gaussians with concordant
versus conflicting gradient directions; and a nonlinear dynamic weighting
mechanism leverages the GCR to enable gradient-direction-aware density control.
Specifically, GDAGS prioritizes conflicting-gradient Gaussians during splitting
operations to enhance geometric details while suppressing redundant
concordant-direction Gaussians. Conversely, in cloning processes, GDAGS
promotes concordant-direction Gaussian densification for structural completion
while preventing conflicting-direction Gaussian overpopulation. Comprehensive
evaluations across diverse real-world benchmarks demonstrate that GDAGS
achieves superior rendering quality while effectively mitigating
over-reconstruction, suppressing over-densification, and constructing compact
scene representations with 50\% reduced memory consumption through optimized
Gaussians utilization.

</details>


### [13] [FineState-Bench: A Comprehensive Benchmark for Fine-Grained State Control in GUI Agents](https://arxiv.org/abs/2508.09241)
*Fengxian Ji,Jingpu Yang,Zirui Song,Yuanxi Wang,Zhexuan Cui,Yuke Li,Qian Jiang,Miao Fang,Xiuying Chen*

Main category: cs.CV

TL;DR: 本文提出了FineState-Bench，这是首个针对细粒度GUI代理操作的评测与诊断基准，旨在量化GUI代理对细粒度控制能力。结果显示，当前最先进模型在细粒度交互中的准确率仅为32.8%，并通过可插拔视觉诊断助手（VDA）定量分析了视觉能力对表现的提升。所有资源完全开源。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理评测体系过于关注任务整体完成情况，缺乏对实际应用中至关重要的细粒度控制能力的分析和评测。因此，需要一个全面评估和诊断细粒度GUI交互及感知定位能力的新标准，以推动该领域进步。

Method: 作者构建了FineState-Bench基准，覆盖桌面、网页和移动端，共设2257项多样化任务，通过四阶段指标对从感知到控制的过程进行全面评测。同时开发了可插拔的视觉诊断助手(VDA)，实现首次对感知/定位和执行动作能力的定量解耦分析。

Result: 在FineState-Bench上测评显示，当前最先进模型（如Gemini-2.5-Flash）在细粒度交互上的最高准确率仅为32.8%。通过理想视觉定位辅助，Gemini-2.5-Flash的表现可提升14.9%。

Conclusion: 本文成果首次系统提出并验证了细粒度评测框架对于揭示GUI代理视觉定位能力瓶颈的重要性，并通过VDA工具为定量分析瓶颈来源提供了有效手段。FineState-Bench及相关资源已全部开源，有望推动智能GUI代理研究和应用的发展。

Abstract: With the rapid advancement of generative artificial intelligence technology,
Graphical User Interface (GUI) agents have demonstrated tremendous potential
for autonomously managing daily tasks through natural language instructions.
However, current evaluation frameworks for GUI agents suffer from fundamental
flaws: existing benchmarks overly focus on coarse-grained task completion while
neglecting fine-grained control capabilities crucial for real-world
applications. To address this, we introduce FineState-Bench, the first
evaluation and diagnostic standard for fine-grained GUI proxy operations,
designed to quantify fine-grained control. This multi-platform (desktop, Web,
mobile) framework includes 2257 task benchmarks in four components and uses a
four-phase indicator for comprehensive perception-to-control assessment. To
analyze perception and positioning for refined operations, we developed the
plug-and-play Visual Diagnostic Assistant (VDA), enabling the first
quantitative decoupling analysis of these capabilities. Experimental results on
our benchmark show that the most advanced models achieve only 32.8%
fine-grained interaction accuracy. Using our VDA in controlled experiments,
quantifying the impact of visual capabilities, we showed that ideal visual
localization boosts Gemini-2.5-Flash's success rate by 14.9\%. Our diagnostic
framework confirms for the first time that the primary bottleneck for current
GUI proxies is basic visual positioning capability.All resources are fully
open-source. github: https://github.com/AnonymousThewarehouse/FineState-Bench
huggingface: https://huggingface.co/datasets/Willtime2006/Static-FineBench

</details>


### [14] [Beyond Blanket Masking: Examining Granularity for Privacy Protection in Images Captured by Blind and Low Vision Users](https://arxiv.org/abs/2508.09245)
*Jeffri Murrugarra-LLerena,Haoran Niu,K. Suzanne Barber,Hal Daumé III,Yang Trista Cao,Paola Cascante-Bonilla*

Main category: cs.CV

TL;DR: 本文提出了一种针对视觉助理系统的细粒度隐私保护框架FiGPriv，通过精细分割和风险评分，仅屏蔽高风险的私人信息，提高了隐私保护的灵活性和系统可用性。


<details>
  <summary>Details</summary>
Motivation: 传统的隐私保护方法会整体屏蔽私人对象，导致实用性下降，尤其在为盲人和低视力用户提供辅助时，这些人有可能无意中分享了敏感信息。因此，作者希望提升视觉语言模型在保护隐私的同时，保证更好的可用性和信息保留。

Method: 提出FiGPriv框架，实现精细分割，将影像中的私人信息根据数据驱动的风险评分机制进行区分，仅对高风险部分加遮罩，保留低风险内容，提高辨识度和交互质量。

Result: 在BIV-Priv-Seg数据集上评估，FiGPriv能够多保留26%的图像内容，使VLMs给出有用回复的能力提升11%、内容识别能力提升45%，隐私保护效果不变。

Conclusion: FiGPriv保障安全同时提升图像辅助系统的可用性和内容丰富度，为视觉语言模型中的隐私保护提供了一种更灵活、实用的解决方案。

Abstract: As visual assistant systems powered by visual language models (VLMs) become
more prevalent, concerns over user privacy have grown, particularly for blind
and low vision users who may unknowingly capture personal private information
in their images. Existing privacy protection methods rely on coarse-grained
segmentation, which uniformly masks entire private objects, often at the cost
of usability. In this work, we propose FiGPriv, a fine-grained privacy
protection framework that selectively masks only high-risk private information
while preserving low-risk information. Our approach integrates fine-grained
segmentation with a data-driven risk scoring mechanism. We evaluate our
framework using the BIV-Priv-Seg dataset and show that FiG-Priv preserves +26%
of image content, enhancing the ability of VLMs to provide useful responses by
11% and identify the image content by 45%, while ensuring privacy protection.
Project Page: https://artcs1.github.io/VLMPrivacy/

</details>


### [15] [Harnessing Input-Adaptive Inference for Efficient VLN](https://arxiv.org/abs/2508.09262)
*Dongwoo Kang,Akhil Perincherry,Zachary Coalson,Aiden Gabriel,Stefan Lee,Sanghyun Hong*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的输入自适应方法，用于提升视觉-语言导航（VLN）模型的效率，并在多项基准测试中实现了计算量减半以上且性能无明显下降。


<details>
  <summary>Details</summary>
Motivation: 当前VLN多模态Transformer模型虽性能强大，但因计算资源需求高，难以应用于资源受限场景。为此，需要改进方法以减少计算量，同时保持模型性能。

Method: 作者发现现有的输入自适应机制在降低计算量时往往伴随较大性能损失。为解决该问题，本文分别在空间、模型内部及时间三个不同层面提出三种自适应算法：（1）空间效率：对每次观测的全景视图进行选择性处理；（2）模型内部效率：基于重要性的自适应阈值早退机制；（3）时间效率：缓存机制避免对已观测视图的重复处理。

Result: 在七个VLN基准任务及三个现成智能体上实验表明，采用该方法后，无论在标准还是连续环境中，计算量均降低了两倍以上，同时模型性能基本不受影响。

Conclusion: 证明了在不牺牲性能的情况下，通过多粒度自适应机制可大幅提高VLN模型在资源受限场景下的效率。

Abstract: An emerging paradigm in vision-and-language navigation (VLN) is the use of
history-aware multi-modal transformer models. Given a language instruction,
these models process observation and navigation history to predict the most
appropriate action for an agent. While they have significantly improved
performance, the scale of these models can be a bottleneck in practical
settings with limited computational resources. In this work, we propose a novel
input-adaptive navigation method to enhance VLN model efficiency. We first show
that existing input-adaptive mechanisms fail to reduce computations without
substantial performance degradation. To address this, we introduce three
adaptive algorithms, each deployed at a different level: (1) To improve spatial
efficiency, we selectively process panoramic views at each observation of an
agent. (2) To improve intra-model efficiency, we propose importance-based
adaptive thresholding for the early-exit methods. (3) To improve temporal
efficiency, we implement a caching mechanism that prevents reprocessing of
views previously seen by the agent. In evaluations on seven VLN benchmarks, we
demonstrate over a 2$\times$ reduction in computation across three
off-the-shelf agents in both standard and continuous environments. Our code is
publicly available at
https://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.

</details>


### [16] [SegDAC: Segmentation-Driven Actor-Critic for Visual Reinforcement Learning](https://arxiv.org/abs/2508.09325)
*Alexandre Brown,Glen Berseth*

Main category: cs.CV

TL;DR: 本文提出了一种名为SegDAC的RL方法，通过分割和语义理解提升视觉泛化能力，在高维视觉输入和强干扰场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 视觉强化学习中既要从高维像素学习感知，又需学习动作策略，并且常面对噪声奖励，这使得泛化和样本效率成为挑战。虽然已有大规模感知模型，但如何有效集成进RL来提升泛化及效率仍不明确。

Method: 方法名为SegDAC（Segmentation-Driven Actor-Critic）。首先利用Segment Anything（SAM）进行对象分割，随后用YOLO-World结合文本提示对分割结果进行语义归因。主模型采用基于transformer的新架构，支持每时刻动态数量的分割区域，并通过在线RL自主学会关注重要区域，无需人工标注。

Result: 在Maniskill3的大规模视觉泛化基准上，覆盖多样操作任务及强视觉扰动，SegDAC在最困难设置下性能提升至前作两倍，在所有任务中样本效率亦优于或持平于现有方法。

Conclusion: SegDAC通过对象分割和语义理解，有效提升了视觉RL的泛化性和样本效率，在复杂视觉环境下优于现有方法，展现了该路线的巨大潜力。

Abstract: Visual reinforcement learning (RL) is challenging due to the need to learn
both perception and actions from high-dimensional inputs and noisy rewards.
Although large perception models exist, integrating them effectively into RL
for visual generalization and improved sample efficiency remains unclear. We
propose SegDAC, a Segmentation-Driven Actor-Critic method. SegDAC uses Segment
Anything (SAM) for object-centric decomposition and YOLO-World to ground
segments semantically via text prompts. It includes a novel transformer-based
architecture that supports a dynamic number of segments at each time step and
effectively learns which segments to focus on using online RL, without using
human labels. By evaluating SegDAC over a challenging visual generalization
benchmark using Maniskill3, which covers diverse manipulation tasks under
strong visual perturbations, we demonstrate that SegDAC achieves significantly
better visual generalization, doubling prior performance on the hardest setting
and matching or surpassing prior methods in sample efficiency across all
evaluated tasks.

</details>


### [17] [Lung-DDPM+: Efficient Thoracic CT Image Synthesis using Diffusion Probabilistic Model](https://arxiv.org/abs/2508.09327)
*Yifan Jiang,Ahmad Shariftabrizi,Venkata SK. Manem*

Main category: cs.CV

TL;DR: 本文提出了Lung-DDPM+，一种适用于肺癌诊断CT图像生成的新型模型，在大幅提升效率的同时，保持了合成图像的高质量和医学可用性。


<details>
  <summary>Details</summary>
Motivation: 现有用于肺癌诊断的生成模型存在效率低与解剖精准度不足等问题，限制了其在临床实际中的应用。

Method: Lung-DDPM+是一种以结节语义布局为引导、并结合肺部DPM-solver加速的去噪扩散概率模型（DDPM），专注于病灶区域，兼顾采样效率与图像质量。

Result: 在LIDC-IDRI公开数据集上，该方法较Lung-DDPM实现了8倍计算量降低、6.8倍GPU显存减少及14倍采样加速，且合成样本质量接近Lung-DDPM及其他SOTA模型。经验丰富的放射科医生视觉Turing测试也证明了其高质量和真实感。

Conclusion: Lung-DDPM+能高效生成高质量带结节的胸部CT图像，具备在肿瘤合成及广义医学影像病灶生成等更广泛领域应用的潜力。

Abstract: Generative artificial intelligence (AI) has been playing an important role in
various domains. Leveraging its high capability to generate high-fidelity and
diverse synthetic data, generative AI is widely applied in diagnostic tasks,
such as lung cancer diagnosis using computed tomography (CT). However, existing
generative models for lung cancer diagnosis suffer from low efficiency and
anatomical imprecision, which limit their clinical applicability. To address
these drawbacks, we propose Lung-DDPM+, an improved version of our previous
model, Lung-DDPM. This novel approach is a denoising diffusion probabilistic
model (DDPM) guided by nodule semantic layouts and accelerated by a pulmonary
DPM-solver, enabling the method to focus on lesion areas while achieving a
better trade-off between sampling efficiency and quality. Evaluation results on
the public LIDC-IDRI dataset suggest that the proposed method achieves
8$\times$ fewer FLOPs (floating point operations per second), 6.8$\times$ lower
GPU memory consumption, and 14$\times$ faster sampling compared to Lung-DDPM.
Moreover, it maintains comparable sample quality to both Lung-DDPM and other
state-of-the-art (SOTA) generative models in two downstream segmentation tasks.
We also conducted a Visual Turing Test by an experienced radiologist, showing
the advanced quality and fidelity of synthetic samples generated by the
proposed method. These experimental results demonstrate that Lung-DDPM+ can
effectively generate high-quality thoracic CT images with lung nodules,
highlighting its potential for broader applications, such as general tumor
synthesis and lesion generation in medical imaging. The code and pretrained
models are available at https://github.com/Manem-Lab/Lung-DDPM-PLUS.

</details>


### [18] [UltraLight Med-Vision Mamba for Classification of Neoplastic Progression in Tubular Adenomas](https://arxiv.org/abs/2508.09339)
*Aqsa Sultana,Nordin Abouzahra,Ahmed Rahu,Brian Shula,Brandon Combs,Derrick Forchetti,Theus Aspiras,Vijayan K. Asari*

Main category: cs.CV

TL;DR: 本论文提出并评估了一种名为Ultralight Med-Vision Mamba的深度学习模型，用于常规肠镜筛查中癌前息肉的识别以及腺瘤分型。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌多由癌前息肉发展而来，早期检测和切除息肉是降低癌症发生的有效手段。现有深度学习方法存在模型复杂、推理慢等局限，难以在实时临床环境中应用，因此需要更高效、准确的自动息肉识别与分型方法。

Method: 作者提出基于状态空间模型（SSM）的Ultralight Med-Vision Mamba模型，强调其善于建模长、短距离依赖关系和提升全视野图像泛化能力。通过其高效架构，提高对全镜切片图像的分析能力，并大幅提升计算速度和拓展性。

Result: Ultralight Med-Vision Mamba模型在腺瘤分类、风险评估等多项指标上均表现优异，在图像建模精度和处理速度上超越现有方法。其高效构架验证了在实际临床环境中具备未来推广和实时部署的潜力。

Conclusion: 该方法为肠镜筛查中的高效癌前息肉识别和分型提供了创新且可行的解决方案，有望推动AI辅助诊断在消化内镜领域的实际应用。

Abstract: Identification of precancerous polyps during routine colonoscopy screenings
is vital for their excision, lowering the risk of developing colorectal cancer.
Advanced deep learning algorithms enable precise adenoma classification and
stratification, improving risk assessment accuracy and enabling personalized
surveillance protocols that optimize patient outcomes. Ultralight Med-Vision
Mamba, a state-space based model (SSM), has excelled in modeling long- and
short-range dependencies and image generalization, critical factors for
analyzing whole slide images. Furthermore, Ultralight Med-Vision Mamba's
efficient architecture offers advantages in both computational speed and
scalability, making it a promising tool for real-time clinical deployment.

</details>


### [19] [Blink-to-code: real-time Morse code communication via eye blink detection and classification](https://arxiv.org/abs/2508.09344)
*Anushka Bhatt*

Main category: cs.CV

TL;DR: 本研究提出了一种基于眼动的莫尔斯电码实时翻译系统，旨在帮助严重运动障碍者沟通。系统通过摄像头识别眨眼类型，实现低成本的辅助交流。


<details>
  <summary>Details</summary>
Motivation: 当前许多运动障碍人士难以使用传统交流工具，因此亟需新型便捷的辅助交流方式。

Method: 利用普通摄像头和计算机视觉技术，系统检测并分类自愿眨眼为短（点）或长（划），再将其译码为字母和数字。实验邀请了5名参与者进行测试。

Result: 系统在5名参与者中的解码准确率为62%，响应时间为18-20秒。

Conclusion: 本系统初步证明了低成本、实时的眼动莫尔斯电码翻译工具在辅助沟通方面的可行性。

Abstract: This study proposes a real-time system that translates voluntary eye blinks
into Morse code, enabling communication for individuals with severe motor
impairments. Using a standard webcam and computer vision, the system detects
and classifies blinks as short (dot) or long (dash), then decodes them into
alphanumeric characters. Experiments with five participants show 62% decoding
accuracy and 18-20 seconds response times, demonstrating a viable, low-cost
assistive communication method.

</details>


### [20] [FusionEnsemble-Net: An Attention-Based Ensemble of Spatiotemporal Networks for Multimodal Sign Language Recognition](https://arxiv.org/abs/2508.09362)
*Md. Milon Islam,Md Rezwanul Haque,S M Taslim Uddin Raju,Fakhri Karray*

Main category: cs.CV

TL;DR: 本文提出了一种新方法FusionEnsemble-Net，通过融合视频与雷达数据，实现对意大利手语手势的高精度识别，在数据集上取得99.44%的准确率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在医疗沟通中，手语复杂且多模态，需要高精度的识别系统，现有方法难以充分地融合各种模态信息。

Method: 提出FusionEnsemble-Net，结合RGB视频和雷达数据，设置4个时空神经网络通道，通过注意力机制融合特征，并采用集成分类器提升鲁棒性和准确率。

Result: 在意大利手语MultiMeDaLIS大规模数据集上，方法准确率达到99.44%，超越了目前最好的方法。

Conclusion: 通过注意力机制和集成学习，融合不同时空网络和多模态数据，可极大提升复杂手语识别的表现。

Abstract: Accurate recognition of sign language in healthcare communication poses a
significant challenge, requiring frameworks that can accurately interpret
complex multimodal gestures. To deal with this, we propose FusionEnsemble-Net,
a novel attention-based ensemble of spatiotemporal networks that dynamically
fuses visual and motion data to enhance recognition accuracy. The proposed
approach processes RGB video and range Doppler map radar modalities
synchronously through four different spatiotemporal networks. For each network,
features from both modalities are continuously fused using an attention-based
fusion module before being fed into an ensemble of classifiers. Finally, the
outputs of these four different fused channels are combined in an ensemble
classification head, thereby enhancing the model's robustness. Experiments
demonstrate that FusionEnsemble-Net outperforms state-of-the-art approaches
with a test accuracy of 99.44% on the large-scale MultiMeDaLIS dataset for
Italian Sign Language. Our findings indicate that an ensemble of diverse
spatiotemporal networks, unified by attention-based fusion, yields a robust and
accurate framework for complex, multimodal isolated gesture recognition tasks.
The source code is available at:
https://github.com/rezwanh001/Multimodal-Isolated-Italian-Sign-Language-Recognition.

</details>


### [21] [A Signer-Invariant Conformer and Multi-Scale Fusion Transformer for Continuous Sign Language Recognition](https://arxiv.org/abs/2508.09372)
*Md Rezwanul Haque,Md. Milon Islam,S M Taslim Uddin Raju,Fakhri Karray*

Main category: cs.CV

TL;DR: 本文提出了一种双架构框架，有效提升了连续手语识别（CSLR）在不同手语者和新句子结构下的泛化能力，并在Isharah-1000数据集上刷新了基准表现。


<details>
  <summary>Details</summary>
Motivation: CSLR面临手语者差异大、对新句子结构泛化能力差等挑战，现有方法难以高效应对，亟需针对性解决方案。

Method: 针对手语者独立（SI）问题，提出Sign-Invariant Conformer架构，结合卷积和多头自注意力，从骨架关键点学习鲁棒的手语者无关表征；针对未见句子（US）任务，设计多尺度融合Transformer引入双路径时序编码器，同时捕捉细粒度动作动态，提高对新句式的理解能力。

Result: 在Isharah-1000数据集上，所提Conformer架构在SI任务上WER降至13.07%（比SOTA低13.53%）；Transformer在US任务WER达47.78%，优于已有方法。SignEval 2025挑战赛上分别取得US第二名、SI第四名。

Conclusion: 为CSLR不同挑战设计专门网络结构可带来显著性能提升，并为后续研究树立了新基线。源码已开源。

Abstract: Continuous Sign Language Recognition (CSLR) faces multiple challenges,
including significant inter-signer variability and poor generalization to novel
sentence structures. Traditional solutions frequently fail to handle these
issues efficiently. For overcoming these constraints, we propose a
dual-architecture framework. For the Signer-Independent (SI) challenge, we
propose a Signer-Invariant Conformer that combines convolutions with multi-head
self-attention to learn robust, signer-agnostic representations from pose-based
skeletal keypoints. For the Unseen-Sentences (US) task, we designed a
Multi-Scale Fusion Transformer with a novel dual-path temporal encoder that
captures both fine-grained posture dynamics, enabling the model's ability to
comprehend novel grammatical compositions. Experiments on the challenging
Isharah-1000 dataset establish a new standard for both CSLR benchmarks. The
proposed conformer architecture achieves a Word Error Rate (WER) of 13.07% on
the SI challenge, a reduction of 13.53% from the state-of-the-art. On the US
task, the transformer model scores a WER of 47.78%, surpassing previous work.
In the SignEval 2025 CSLR challenge, our team placed 2nd in the US task and 4th
in the SI task, demonstrating the performance of these models. The findings
validate our key hypothesis: that developing task-specific networks designed
for the particular challenges of CSLR leads to considerable performance
improvements and establishes a new baseline for further research. The source
code is available at: https://github.com/rezwanh001/MSLR-Pose86K-CSLR-Isharah.

</details>


### [22] [What Can We Learn from Inter-Annotator Variability in Skin Lesion Segmentation?](https://arxiv.org/abs/2508.09381)
*Kumar Abhishek,Jeremy Kawahara,Ghassan Hamarneh*

Main category: cs.CV

TL;DR: 本论文针对医学图像分割中标注者间分歧问题，提出并分析了皮肤病变分割中的一致性与恶性程度的关联，并通过新数据集和方法提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中存在较大的标注者间、标注者内的差异性，尤其在边界模糊或不规则病灶（如恶性肿瘤）上分歧更严重，这对下游诊断和算法性能造成影响。需要深入理解标注一致性的影响因素，并探索其与临床特征（如恶性程度）之间的联系，从而设计更健壮的分割方法。

Method: 1. 构建了目前最大的多标注者皮肤病变分割数据集IMA++。
2. 系统分析了标注一致性与标注者、疾病恶性程度、工具、技能等因素的关系。
3. 用于预测分割一致性（IAA）与恶性程度之间的统计分析。
4. 探索直接从皮肤镜图像预测IAA的方法。
5. 将IAA作为“软”临床特征融入多任务学习，实现性能提升。

Result: 1. 标注者间一致性（IAA）与皮肤病变的恶性程度存在显著相关（p<0.001）。
2. 可直接通过图像预测IAA，平均绝对误差为0.108。
3. 在多模型和多个数据集上，引入IAA作为软特征后平衡准确率提升4.2%。

Conclusion: 本文揭示了医学图像分割中标注一致性受多因素影响且与恶性程度相关，提出了以一致性信息提升模型的方法，并在新数据集及公开数据集上取得显著提升，为今后分割算法与标注策略提供了新思路。

Abstract: Medical image segmentation exhibits intra- and inter-annotator variability
due to ambiguous object boundaries, annotator preferences, expertise, and
tools, among other factors. Lesions with ambiguous boundaries, e.g., spiculated
or infiltrative nodules, or irregular borders per the ABCD rule, are
particularly prone to disagreement and are often associated with malignancy. In
this work, we curate IMA++, the largest multi-annotator skin lesion
segmentation dataset, on which we conduct an in-depth study of variability due
to annotator, malignancy, tool, and skill factors. We find a statistically
significant (p<0.001) association between inter-annotator agreement (IAA),
measured using Dice, and the malignancy of skin lesions. We further show that
IAA can be accurately predicted directly from dermoscopic images, achieving a
mean absolute error of 0.108. Finally, we leverage this association by
utilizing IAA as a "soft" clinical feature within a multi-task learning
objective, yielding a 4.2% improvement in balanced accuracy averaged across
multiple model architectures and across IMA++ and four public dermoscopic
datasets. The code is available at https://github.com/sfu-mial/skin-IAV.

</details>


### [23] [Distilling LLM Prior to Flow Model for Generalizable Agent's Imagination in Object Goal Navigation](https://arxiv.org/abs/2508.09423)
*Badi Li,Ren-jie Lu,Yu Zhou,Jingke Meng,Wei-shi Zheng*

Main category: cs.CV

TL;DR: 本文提出了GOAL框架，一种融合了大语言模型知识的生成式流模型，用于在ObjectNav任务中预测未知区域的语义分布，大幅提升了导航泛化能力并取得了最新最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有ObjectNav任务方法多依赖于确定性和判别式模型，忽略了室内结构的不确定性，导致对陌生环境的泛化能力有限。

Method: GOAL框架将从大语言模型（LLMs）中获取的空间先验，以二维高斯场的形式编码并融合到目标语义地图中，用生成式流模型学习场景中观察区域与全景语义的分布关系，从而实现更具泛化性和适应性的场景语义补全。

Result: GOAL在MP3D和Gibson数据集上取得了最优性能，并在到HM3D数据集的迁移泛化测试中表现出色。

Conclusion: GOAL通过结合生成式模型与LLM知识，有效提升了ObjectNav任务中对新环境的理解和导航能力。

Abstract: The Object Goal Navigation (ObjectNav) task challenges agents to locate a
specified object in an unseen environment by imagining unobserved regions of
the scene. Prior approaches rely on deterministic and discriminative models to
complete semantic maps, overlooking the inherent uncertainty in indoor layouts
and limiting their ability to generalize to unseen environments. In this work,
we propose GOAL, a generative flow-based framework that models the semantic
distribution of indoor environments by bridging observed regions with
LLM-enriched full-scene semantic maps. During training, spatial priors inferred
from large language models (LLMs) are encoded as two-dimensional Gaussian
fields and injected into target maps, distilling rich contextual knowledge into
the flow model and enabling more generalizable completions. Extensive
experiments demonstrate that GOAL achieves state-of-the-art performance on MP3D
and Gibson, and shows strong generalization in transfer settings to HM3D. Codes
and pretrained models are available at https://github.com/Badi-Li/GOAL.

</details>


### [24] [X-UniMotion: Animating Human Images with Expressive, Unified and Identity-Agnostic Motion Latents](https://arxiv.org/abs/2508.09383)
*Guoxian Song,Hongyi Xu,Xiaochen Zhao,You Xie,Tianpei Gu,Zenan Li,Chenxu Zhang,Linjie Luo*

Main category: cs.CV

TL;DR: X-UniMotion是一种用于全身人类动作统一隐式表征的新方法，可以表达面部表情、身体姿势和手势，显著提升跨身份动作迁移的保真度与细节表现力。


<details>
  <summary>Details</summary>
Motivation: 现有动作迁移方法过于依赖显式骨架和启发式调整，难以实现高精度、细粒度和高保真度的人体动作跨身份迁移，特别是应对多身份、多姿态和多空间配置的人体动作时存在局限。

Method: 该方法通过单张图像将面部表情、身体姿势及双手手势编码为四个解耦的隐式动作标记（latent tokens），并采用自监督、端到端的训练方式协同学习动作编码器和DiT视频生成模型，同时利用2D空间及色彩增强和3D合成渲染实现动作与身份的解耦，辅助解码器进一步促进细粒度、语义一致和具有深度感知的动作嵌入学习。

Result: X-UniMotion在大规模人体动作数据集上，经大量实验验证，全面优于主流方法，在动作表达力、动画保真度和身份保持度上都有明显提升。

Conclusion: X-UniMotion为全身动作隐式表征与跨身份迁移带来新的高保真方案，能够生成更生动、细致、保持个体特征的动画，具有广泛应用前景。

Abstract: We present X-UniMotion, a unified and expressive implicit latent
representation for whole-body human motion, encompassing facial expressions,
body poses, and hand gestures. Unlike prior motion transfer methods that rely
on explicit skeletal poses and heuristic cross-identity adjustments, our
approach encodes multi-granular motion directly from a single image into a
compact set of four disentangled latent tokens -- one for facial expression,
one for body pose, and one for each hand. These motion latents are both highly
expressive and identity-agnostic, enabling high-fidelity, detailed
cross-identity motion transfer across subjects with diverse identities, poses,
and spatial configurations. To achieve this, we introduce a self-supervised,
end-to-end framework that jointly learns the motion encoder and latent
representation alongside a DiT-based video generative model, trained on
large-scale, diverse human motion datasets. Motion-identity disentanglement is
enforced via 2D spatial and color augmentations, as well as synthetic 3D
renderings of cross-identity subject pairs under shared poses. Furthermore, we
guide motion token learning with auxiliary decoders that promote fine-grained,
semantically aligned, and depth-aware motion embeddings. Extensive experiments
show that X-UniMotion outperforms state-of-the-art methods, producing highly
expressive animations with superior motion fidelity and identity preservation.

</details>


### [25] [WeatherPrompt: Multi-modality Representation Learning for All-Weather Drone Visual Geo-Localization](https://arxiv.org/abs/2508.09560)
*Jiahao Wen,Hang Yu,Zhedong Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态视觉地理定位方法WeatherPrompt，针对无人机在恶劣天气下定位效果下降的痛点，通过融合图像和文本，实现对天气鲁棒性的提升。实验显示，在多种恶劣天气下显著提升了定位准确率。


<details>
  <summary>Details</summary>
Motivation: 现有无人机视觉定位技术在雨、雾等恶劣天气下性能明显下降，主要由于过度依赖有限的天气类别和天气-场景特征未有效解耦，导致泛化能力差。

Method: 提出多模态学习框架WeatherPrompt，核心贡献有两点：（1）无训练天气推理机制，利用现有大模型生成多样的类人天气文本描述，适应未见过或复杂天气；（2）多模态动态门控机制，根据文本嵌入自适应地加权融合视觉特征，并以跨模态对比学习和匹配目标优化，使同一场景不同天气下的表征更加接近。

Result: 大量实验证明，在多种极端天气下，新方法取得与现有最优方法相当甚至更优的召回率。例如，夜间Recall@1提升13.37%，雾和雪天提升18.69%。

Conclusion: WeatherPrompt框架能够显著提升无人机在多种恶劣天气下的视觉本地化能力，强泛化性和天气适应性比现有方法优越。

Abstract: Visual geo-localization for drones faces critical degradation under weather
perturbations, \eg, rain and fog, where existing methods struggle with two
inherent limitations: 1) Heavy reliance on limited weather categories that
constrain generalization, and 2) Suboptimal disentanglement of entangled
scene-weather features through pseudo weather categories. We present
WeatherPrompt, a multi-modality learning paradigm that establishes
weather-invariant representations through fusing the image embedding with the
text context. Our framework introduces two key contributions: First, a
Training-free Weather Reasoning mechanism that employs off-the-shelf large
multi-modality models to synthesize multi-weather textual descriptions through
human-like reasoning. It improves the scalability to unseen or complex weather,
and could reflect different weather strength. Second, to better disentangle the
scene and weather feature, we propose a multi-modality framework with the
dynamic gating mechanism driven by the text embedding to adaptively reweight
and fuse visual features across modalities. The framework is further optimized
by the cross-modal objectives, including image-text contrastive learning and
image-text matching, which maps the same scene with different weather
conditions closer in the respresentation space. Extensive experiments validate
that, under diverse weather conditions, our method achieves competitive recall
rates compared to state-of-the-art drone geo-localization methods. Notably, it
improves Recall@1 by +13.37\% under night conditions and by 18.69\% under fog
and snow conditions.

</details>


### [26] [DenoDet V2: Phase-Amplitude Cross Denoising for SAR Object Detection](https://arxiv.org/abs/2508.09392)
*Kang Ni,Minrui Zou,Yuxuan Li,Xiang Li,Kehua Guo,Ming-Ming Cheng,Yimian Dai*

Main category: cs.CV

TL;DR: 本文提出了一种适用于合成孔径雷达（SAR）目标检测的新方法DenoDet V2，通过在变换域中利用注意力结构有效地抑制相干噪声，实现了领先的检测效果，并显著提升了性能与效率。


<details>
  <summary>Details</summary>
Motivation: SAR目标检测受到相干噪声的严重影响，现有方法多在空间域特征处理上进行隐式去噪。为突破困境，作者探索了在变换域对特征进行拆解和调制的新方案。

Method: 提出DenoDet V2，采用精心设计的注意力架构，在变换域实现特征处理。该方法创新性地通过带域互相调节机制，实现了幅度与相位信息的互补增强，有效提升噪声抑制效果。

Result: 在多个SAR数据集上实验，DenoDet V2表现出当前最优水平。在SARDet-100K数据集上，较前一代DenoDet V1检测性能提升0.8%，模型复杂度减少一半。

Conclusion: DenoDet V2在SAR目标检测领域具备更高的精度和更低的模型复杂度，为应对相干噪声提供了新的有效思路，对实际应用有积极推动作用。

Abstract: One of the primary challenges in Synthetic Aperture Radar (SAR) object
detection lies in the pervasive influence of coherent noise. As a common
practice, most existing methods, whether handcrafted approaches or deep
learning-based methods, employ the analysis or enhancement of object
spatial-domain characteristics to achieve implicit denoising. In this paper, we
propose DenoDet V2, which explores a completely novel and different perspective
to deconstruct and modulate the features in the transform domain via a
carefully designed attention architecture. Compared to DenoDet V1, DenoDet V2
is a major advancement that exploits the complementary nature of amplitude and
phase information through a band-wise mutual modulation mechanism, which
enables a reciprocal enhancement between phase and amplitude spectra. Extensive
experiments on various SAR datasets demonstrate the state-of-the-art
performance of DenoDet V2. Notably, DenoDet V2 achieves a significant 0.8\%
improvement on SARDet-100K dataset compared to DenoDet V1, while reducing the
model complexity by half. The code is available at
https://github.com/GrokCV/GrokSAR.

</details>


### [27] [Plane Detection and Ranking via Model Information Optimization](https://arxiv.org/abs/2508.09625)
*Daoxin Zhong,Jun Li,Meng Yee Michael Chuah*

Main category: cs.CV

TL;DR: 本文提出了一种基于模型信息优化的平面检测新框架，通过量化传感器物理模型与噪声，将信息量最小的模型作为真实平面，并有效避免RANSAC的误检测问题，实现更高精度的平面检测。


<details>
  <summary>Details</summary>
Motivation: 传统的RANSAC方法在平面检测中虽然鲁棒，但阈值设定模糊，且在复杂场景多平面情况下易产生误判。实际应用中需要更客观、高效且能自动判别平面数量的方法。

Method: 方法首先将深度读数视为离散随机变量，将满足不同平面约束的多种模型通过随机子采样产生，并结合物理与噪声建模，计算各模型信息量。选择信息量最小的模型，确定真实平面数量，并对每个平面按照其对信息减少的贡献进行排名。同时引入神经网络分割对深度图进行预分区，实现加速。

Result: 通过合成数据实验，算法平面参数估计精度优于Open3D默认RANSAC方法。利用神经网络分割后，算法在真实数据上的平面参数生成能力更强，更贴近真实情况。

Conclusion: 基于信息优化的模型选择不仅提升了平面检测的精度，同时为平面数量判定和结果排序提供了更客观的依据，有效解决了RANSAC误判和主观阈值设定的问题，且在实际场景中具备较好应用潜力。

Abstract: Plane detection from depth images is a crucial subtask with broad robotic
applications, often accomplished by iterative methods such as Random Sample
Consensus (RANSAC). While RANSAC is a robust strategy with strong probabilistic
guarantees, the ambiguity of its inlier threshold criterion makes it
susceptible to false positive plane detections. This issue is particularly
prevalent in complex real-world scenes, where the true number of planes is
unknown and multiple planes coexist. In this paper, we aim to address this
limitation by proposing a generalised framework for plane detection based on
model information optimization. Building on previous works, we treat the
observed depth readings as discrete random variables, with their probability
distributions constrained by the ground truth planes. Various models containing
different candidate plane constraints are then generated through repeated
random sub-sampling to explain our observations. By incorporating the physics
and noise model of the depth sensor, we can calculate the information for each
model, and the model with the least information is accepted as the most likely
ground truth. This information optimization process serves as an objective
mechanism for determining the true number of planes and preventing false
positive detections. Additionally, the quality of each detected plane can be
ranked by summing the information reduction of inlier points for each plane. We
validate these properties through experiments with synthetic data and find that
our algorithm estimates plane parameters more accurately compared to the
default Open3D RANSAC plane segmentation. Furthermore, we accelerate our
algorithm by partitioning the depth map using neural network segmentation,
which enhances its ability to generate more realistic plane parameters in
real-world data.

</details>


### [28] [Skyshield: Event-Driven Submillimetre Thin Obstacle Detection for Drone Flight Safety](https://arxiv.org/abs/2508.09397)
*Zhengli Zhang,Xinyu Luo,Yuchen Sun,Wenhua Ding,Dongyu Huang,Xinlei Chen*

Main category: cs.CV

TL;DR: 本论文提出了一种名为SkyShield的基于事件流的端到端框架，实现对亚毫米级细小障碍物的高效精准感知，专为复杂环境下无人机避障而设计。


<details>
  <summary>Details</summary>
Motivation: 传统传感器如RGB摄像头、激光雷达和深度相机难以识别钢丝、风筝线等亚毫米级细小障碍物，这些障碍物对无人机威胁极大，因此亟需一种新型感知方法。

Method: 作者基于事件相机的数据流，提出采用轻量级U-Net结构结合创新的Dice-Contour Regularization Loss，能够抓住细小障碍物在事件流中的特征，实现高效检测。

Result: 实验结果表明，所提方法平均F1分数达到0.7088，检测延迟仅21.2毫秒，验证了方案的高精度和低延迟。

Conclusion: SkyShield极大提升了无人机对亚毫米级障碍物的感知能力，且低延迟优势使其适合部署到边缘或移动平台，推动了无人机在复杂环境下的安全性能提升。

Abstract: Drones operating in complex environments face a significant threat from thin
obstacles, such as steel wires and kite strings at the submillimeter level,
which are notoriously difficult for conventional sensors like RGB cameras,
LiDAR, and depth cameras to detect. This paper introduces SkyShield, an
event-driven, end-to-end framework designed for the perception of submillimeter
scale obstacles. Drawing upon the unique features that thin obstacles present
in the event stream, our method employs a lightweight U-Net architecture and an
innovative Dice-Contour Regularization Loss to ensure precise detection.
Experimental results demonstrate that our event-based approach achieves mean F1
Score of 0.7088 with a low latency of 21.2 ms, making it ideal for deployment
on edge and mobile platforms.

</details>


### [29] [Surg-InvNeRF: Invertible NeRF for 3D tracking and reconstruction in surgical vision](https://arxiv.org/abs/2508.09681)
*Gerardo Loza,Junlei Hu,Dominic Jones,Sharib Ali,Pietro Valdastri*

Main category: cs.CV

TL;DR: 本文提出了一种基于NeRF的新颖测试时优化(TTO)方法，用于长期3D点追踪，显著提升了2D及3D跟踪精度，尤其在手术场景中效果突出。


<details>
  <summary>Details</summary>
Motivation: 当前点追踪方法在长期运动一致性和3D运动方面存在瓶颈，尤其在医学等实际场景下，需求更高精度和鲁棒性的跟踪能力。

Method: 将长期点追踪问题建模为优化一个结合多种先进点对应方法的函数。创新性地用新型可逆NeRF架构(InvNeRF)参数化该函数，实现2D/3D点追踪，并利用渲染方法对像素重投影结果进行监督。采用多尺度HexPlanes结构加速推理，并设计高效像素采样和收敛算法。

Result: 在STIR和SCARE两个数据集上实验，2D点追踪平均准确率相比现有TTO方法提升约50%；首次在3D点追踪上实现TTO方法，超过现有的前馈方法。

Conclusion: 该方法实现了更高效、精确的2D/3D点追踪，并开拓了基于可变形NeRF的跟踪新方向，具有较强的实际意义和应用前景。

Abstract: We proposed a novel test-time optimisation (TTO) approach framed by a
NeRF-based architecture for long-term 3D point tracking. Most current methods
in point tracking struggle to obtain consistent motion or are limited to 2D
motion. TTO approaches frame the solution for long-term tracking as optimising
a function that aggregates correspondences from other specialised
state-of-the-art methods. Unlike the state-of-the-art on TTO, we propose
parametrising such a function with our new invertible Neural Radiance Field
(InvNeRF) architecture to perform both 2D and 3D tracking in surgical
scenarios. Our approach allows us to exploit the advantages of a
rendering-based approach by supervising the reprojection of pixel
correspondences. It adapts strategies from recent rendering-based methods to
obtain a bidirectional deformable-canonical mapping, to efficiently handle a
defined workspace, and to guide the rays' density. It also presents our
multi-scale HexPlanes for fast inference and a new algorithm for efficient
pixel sampling and convergence criteria. We present results in the STIR and
SCARE datasets, for evaluating point tracking and testing the integration of
kinematic data in our pipeline, respectively. In 2D point tracking, our
approach surpasses the precision and accuracy of the TTO state-of-the-art
methods by nearly 50% on average precision, while competing with other
approaches. In 3D point tracking, this is the first TTO approach, surpassing
feed-forward methods while incorporating the benefits of a deformable
NeRF-based reconstruction.

</details>


### [30] [Autonomous AI Bird Feeder for Backyard Biodiversity Monitoring](https://arxiv.org/abs/2508.09398)
*El Mustapha Mansouri*

Main category: cs.CV

TL;DR: 本文提出了一种廉价的本地自主系统，可用于比利时城市花园里的后院鸟类监测。系统利用运动触发摄像头与本地服务器协作，实现鸟类检测并本地分类，无需云服务。实现成本低，隐私性强，性能优异。


<details>
  <summary>Details</summary>
Motivation: 传统鸟类监测依赖人工观察或昂贵的设备，且常需上传数据到云端，带来隐私和费用问题。为了帮助市民参与生物多样性监测并保障数据隐私，亟需一种低成本、本地化、易部署的自动化鸟类识别系统。

Method: 系统由运动触发的IP摄像头和本地服务器组成，摄像头通过FTP上传视频片段到本地服务器。服务器利用Detectron2在视频帧上定位鸟类，将感兴趣区域裁剪出来，并用针对比利时40种鸟类精细微调的EfficientNet-B3模型进行分类。设备运行在无独立GPU的普通硬件上，同时物理鸟食器通过小入口排除鸽子等无关目标，提高检测效率。

Result: 方法实现了高效的本地鸟类检测与分类，分类模型在验证集上的准确率高达99.5%，实际应用于新物种时的top-1准确率约为88%。另外，通过检测器引导的区域裁剪提升了整体分类性能。

Conclusion: 本系统在普通硬件和低成本下实现了高准确率的本地化鸟类监测，具备作为市民科学项目进行家庭生物多样性记录的可行性。

Abstract: This paper presents a low cost, on premise system for autonomous backyard
bird monitoring in Belgian urban gardens. A motion triggered IP camera uploads
short clips via FTP to a local server, where frames are sampled and birds are
localized with Detectron2; cropped regions are then classified by an
EfficientNet-B3 model fine tuned on a 40-species Belgian subset derived from a
larger Kaggle corpus. All processing runs on commodity hardware without a
discrete GPU, preserving privacy and avoiding cloud fees. The physical feeder
uses small entry ports (30 mm) to exclude pigeons and reduce nuisance triggers.
Detector-guided cropping improves classification accuracy over raw-frame
classification. The classifier attains high validation performance on the
curated subset (about 99.5 percent) and delivers practical field accuracy
(top-1 about 88 percent) on held-out species, demonstrating feasibility for
citizen-science-grade biodiversity logging at home.

</details>


### [31] [Predictive Uncertainty for Runtime Assurance of a Real-Time Computer Vision-Based Landing System](https://arxiv.org/abs/2508.09732)
*Romeo Valentin,Sydney M. Katz,Artur B. Carneiro,Don Walker,Mykel J. Kochenderfer*

Main category: cs.CV

TL;DR: 本文提出了一种面向航空安全关键应用的基于视觉的飞机姿态估计管道，提升了自动降落和跑道检测的鲁棒性和安全性。


<details>
  <summary>Details</summary>
Motivation: 尽管基于数据驱动的视觉系统已广泛应用于民航自动导航，但其在鲁棒性和安全性要求极高的航空领域仍无法满足认证需求。本文旨在推动此领域系统朝向可认证、安全可用方向发展。

Method: 提出了一种高效灵活的神经网络架构，利用空间Soft Argmax算子实现概率化关键点回归，适配多种视觉主干网络并支持实时推理。设计了可生成校准预测不确定性的损失函数，通过锐度和校准度指标进行评估；并结合残差自主演化完整性监测（RAIM）方法，实现推理阶段实时检测并剔除异常模型输出。

Result: 在跑道图像数据集上评估表明，该管道在准确率上优于基线架构，且获得了亚像素级高精度和校准性良好的不确定性估计，可直接用于后续故障检测。

Conclusion: 该研究为基于视觉的航空安全关键系统向可认证方向迈出了重要一步，显著提升了自动化姿态估计在安全性与可靠性方面的表现。

Abstract: Recent advances in data-driven computer vision have enabled robust autonomous
navigation capabilities for civil aviation, including automated landing and
runway detection. However, ensuring that these systems meet the robustness and
safety requirements for aviation applications remains a major challenge. In
this work, we present a practical vision-based pipeline for aircraft pose
estimation from runway images that represents a step toward the ability to
certify these systems for use in safety-critical aviation applications. Our
approach features three key innovations: (i) an efficient, flexible neural
architecture based on a spatial Soft Argmax operator for probabilistic keypoint
regression, supporting diverse vision backbones with real-time inference; (ii)
a principled loss function producing calibrated predictive uncertainties, which
are evaluated via sharpness and calibration metrics; and (iii) an adaptation of
Residual-based Receiver Autonomous Integrity Monitoring (RAIM), enabling
runtime detection and rejection of faulty model outputs. We implement and
evaluate our pose estimation pipeline on a dataset of runway images. We show
that our model outperforms baseline architectures in terms of accuracy while
also producing well-calibrated uncertainty estimates with sub-pixel precision
that can be used downstream for fault detection.

</details>


### [32] [Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving](https://arxiv.org/abs/2508.09404)
*Guangxun Zhu,Shiyu Fan,Hang Dai,Edmond S. L. Ho*

Main category: cs.CV

TL;DR: 本文提出了Waymo-3DSkelMo，一个高质量大规模、多人的3D骨骼动作数据集，专为自动驾驶中精细行人互动理解设计，并公开了相关基准与代码。


<details>
  <summary>Details</summary>
Motivation: 现有3D动作数据集多源于单目RGB图像，受遮挡影响大且缺乏时序连贯性，导致动作不真实，质量不高，限制了自动驾驶等领域精细化行人行为理解能力。

Method: 作者基于Waymo Perception原始LiDAR点云，结合3D人体形状、动作先验，有效提升3D骨骼动作序列的质量与时序连贯性，并标注明确交互语义。数据集包含800多个真实驾驶场景，平均每个场景27个体，最长可达250人，总长度超14000秒。

Result: 除了高水准的数据本身，作者还建立了不同密度下的3D姿态预测基准，实验结果展示了数据集在复杂场景中对人类行为精细分析的价值。

Conclusion: Waymo-3DSkelMo填补了多人物高质量3D行为数据的空白，将极大推动复杂城市场景下的人体行为学和自动驾驶等领域研究。

Abstract: Large-scale high-quality 3D motion datasets with multi-person interactions
are crucial for data-driven models in autonomous driving to achieve
fine-grained pedestrian interaction understanding in dynamic urban
environments. However, existing datasets mostly rely on estimating 3D poses
from monocular RGB video frames, which suffer from occlusion and lack of
temporal continuity, thus resulting in unrealistic and low-quality human
motion. In this paper, we introduce Waymo-3DSkelMo, the first large-scale
dataset providing high-quality, temporally coherent 3D skeletal motions with
explicit interaction semantics, derived from the Waymo Perception dataset. Our
key insight is to utilize 3D human body shape and motion priors to enhance the
quality of the 3D pose sequences extracted from the raw LiDRA point clouds. The
dataset covers over 14,000 seconds across more than 800 real driving scenarios,
including rich interactions among an average of 27 agents per scene (with up to
250 agents in the largest scene). Furthermore, we establish 3D pose forecasting
benchmarks under varying pedestrian densities, and the results demonstrate its
value as a foundational resource for future research on fine-grained human
behavior understanding in complex urban environments. The dataset and code will
be available at https://github.com/GuangxunZhu/Waymo-3DSkelMo

</details>


### [33] [TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos](https://arxiv.org/abs/2508.09811)
*Jinxi Li,Ziyang Song,Bo Yang*

Main category: cs.CV

TL;DR: 本文提出了一种全新的无监督3D动态场景建模方法，通过将3D点建模为刚性粒子，能够显式学习复杂运动物理信息，在未来帧推断任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有3D动态场景建模方法通常难以准确学习复杂的运动物理规律，或需要如物体类型、掩码等额外人工标注，限制了无监督建模的能力。

Method: 提出TRACE框架，将每个3D点作为带有尺寸和朝向的刚性粒子，通过学习粒子的平移-旋转动力系统，直接显式估计其运动的完整物理参数，无需人工标签，可以整合物理信息。

Result: 在3个公开动态数据集和一个合成高难度数据集上进行了大量实验，TRACE方法在未来帧外推任务中显著优于基线方法，能够有效地学习到复杂的场景运动物理规律。

Conclusion: TRACE框架不仅在3D动态场景物理建模方面取得突破，还具备利用物理参数聚类实现对象/部分自动分割的能力，具有较强应用前景。

Abstract: In this paper, we aim to model 3D scene geometry, appearance, and physical
information just from dynamic multi-view videos in the absence of any human
labels. By leveraging physics-informed losses as soft constraints or
integrating simple physics models into neural nets, existing works often fail
to learn complex motion physics, or doing so requires additional labels such as
object types or masks. We propose a new framework named TRACE to model the
motion physics of complex dynamic 3D scenes. The key novelty of our method is
that, by formulating each 3D point as a rigid particle with size and
orientation in space, we directly learn a translation rotation dynamics system
for each particle, explicitly estimating a complete set of physical parameters
to govern the particle's motion over time. Extensive experiments on three
existing dynamic datasets and one newly created challenging synthetic datasets
demonstrate the extraordinary performance of our method over baselines in the
task of future frame extrapolation. A nice property of our framework is that
multiple objects or parts can be easily segmented just by clustering the
learned physical parameters.

</details>


### [34] [RampNet: A Two-Stage Pipeline for Bootstrapping Curb Ramp Detection in Streetscape Images from Open Government Metadata](https://arxiv.org/abs/2508.09415)
*John S. O'Meara,Jared Hwang,Zeyu Wang,Michael Saugstad,Jon E. Froehlich*

Main category: cs.CV

TL;DR: 提出了一种名为RampNet的两阶段流程，自动大规模构建路缘坡道（curb ramp）检测数据集，并基于该数据集训练模型，取得了极高的检测性能。


<details>
  <summary>Details</summary>
Motivation: 城市可达性对残障人士友好至关重要，路缘坡道检测是相关智能系统的核心任务。但高质量、大规模的数据集稀缺，现有人工标注或众包方法在规模或精度上都不够。该领域亟需新的数据集和更好的检测方法。

Method: 采用两阶段流程：第一阶段利用政府提供的路缘坡道位置信息，自动匹配到Google街景全景图，并生成21万多条带注释的数据；第二阶段使用改进的ConvNeXt V2模型在该数据集上训练路缘坡道检测模型。两阶段均与人工标注数据进行了对比评估。

Result: 自动生成的数据集精度94.0%，召回率92.5%；基于此数据集训练的检测模型AP达0.9236，均显著超过已有方法。

Conclusion: 首次提出了大规模高质量的路缘坡道检测数据集、评测基准和检测模型，将推动城市无障碍基础设施相关研究。

Abstract: Curb ramps are critical for urban accessibility, but robustly detecting them
in images remains an open problem due to the lack of large-scale, high-quality
datasets. While prior work has attempted to improve data availability with
crowdsourced or manually labeled data, these efforts often fall short in either
quality or scale. In this paper, we introduce and evaluate a two-stage pipeline
called RampNet to scale curb ramp detection datasets and improve model
performance. In Stage 1, we generate a dataset of more than 210,000 annotated
Google Street View (GSV) panoramas by auto-translating government-provided curb
ramp location data to pixel coordinates in panoramic images. In Stage 2, we
train a curb ramp detection model (modified ConvNeXt V2) from the generated
dataset, achieving state-of-the-art performance. To evaluate both stages of our
pipeline, we compare to manually labeled panoramas. Our generated dataset
achieves 94.0% precision and 92.5% recall, and our detection model reaches
0.9236 AP -- far exceeding prior work. Our work contributes the first
large-scale, high-quality curb ramp detection dataset, benchmark, and model.

</details>


### [35] [RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians](https://arxiv.org/abs/2508.09830)
*Shenxing Wei,Jinxi Li,Yafei Yang,Siyuan Zhou,Bo Yang*

Main category: cs.CV

TL;DR: 本文提出了一种新方法RayletDF，通过引入光线距离场高效地从点云或3D高斯重建3D表面，相较以往方法更高效且泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 传统的基于坐标的3D重建方法在渲染显式表面时计算量大，难以高效且普适地从原始点云或3D高斯数据进行快速精准的表面重建。作者希望提出一种既高效又具良好泛化能力的方法。

Method: 作者提出了RayletDF方法。核心是通过新颖的光线距离场（raylet distance field），直接从查询光线预测表面点。算法由三大模块组成：光线特征提取器、光线距离场预测器和多光线融合器，能够提取局部几何特征、多视角融合，实现表面点的精准预测与重建。

Result: 在多个公开真实数据集上进行实验，RayletDF方法在点云和3D高斯数据的表面重建任务上均取得了优异效果，表现优于现有方法。其突出特点是强泛化性，在未经训练的数据集上一轮推理即可恢复出高质量3D表面。

Conclusion: RayletDF方法在3D表面重建领域实现了效率和准确性的平衡，具有强大的通用性和实际应用潜力，为进一步提升3D重建技术提供了新思路。

Abstract: In this paper, we present a generalizable method for 3D surface
reconstruction from raw point clouds or pre-estimated 3D Gaussians by 3DGS from
RGB images. Unlike existing coordinate-based methods which are often
computationally intensive when rendering explicit surfaces, our proposed
method, named RayletDF, introduces a new technique called raylet distance
field, which aims to directly predict surface points from query rays. Our
pipeline consists of three key modules: a raylet feature extractor, a raylet
distance field predictor, and a multi-raylet blender. These components work
together to extract fine-grained local geometric features, predict raylet
distances, and aggregate multiple predictions to reconstruct precise surface
points. We extensively evaluate our method on multiple public real-world
datasets, demonstrating superior performance in surface reconstruction from
point clouds or 3D Gaussians. Most notably, our method achieves exceptional
generalization ability, successfully recovering 3D surfaces in a single-forward
pass across unseen datasets in testing.

</details>


### [36] [What-Meets-Where: Unified Learning of Action and Contact Localization in a New Dataset](https://arxiv.org/abs/2508.09428)
*Yuxiao Wang,Yu Lei,Wolin Liang,Weiying Xue,Zhenao Wei,Nan Zhuang,Qi Liu*

Main category: cs.CV

TL;DR: 该论文提出了一个新颖的视觉任务和方法，用于同时预测动作语义与身体部位接触区域，并发布了相关数据集和模型，实验表明方法优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时建模动作的语义（做了什么）和空间语境（在哪发生），即动作和身体与场景的关系，为提升对多样化视觉场景中的动作理解能力，需更精细化建模。

Method: 提出PaIR-Net框架，包含三个模块：1）CPAM识别与接触相关的身体部位；2）PGCS用于像素级接触分割；3）IIM整合全局交互关系。同时，发布了新的PaIR数据集（含13979张图片，654种动作，80种物体，17个身体部位）。

Result: 实验验证PaIR-Net显著优于现有基线方法。消融实验进一步证明了各个模块的有效性。

Conclusion: PaIR-Net框架能更好地同时理解动作语义及其空间上下文，为人机交互、动作识别等场景提供了新的研究基础。代码和数据集将随论文发布。

Abstract: People control their bodies to establish contact with the environment. To
comprehensively understand actions across diverse visual contexts, it is
essential to simultaneously consider \textbf{what} action is occurring and
\textbf{where} it is happening. Current methodologies, however, often
inadequately capture this duality, typically failing to jointly model both
action semantics and their spatial contextualization within scenes. To bridge
this gap, we introduce a novel vision task that simultaneously predicts
high-level action semantics and fine-grained body-part contact regions. Our
proposed framework, PaIR-Net, comprises three key components: the Contact Prior
Aware Module (CPAM) for identifying contact-relevant body parts, the
Prior-Guided Concat Segmenter (PGCS) for pixel-wise contact segmentation, and
the Interaction Inference Module (IIM) responsible for integrating global
interaction relationships. To facilitate this task, we present PaIR (Part-aware
Interaction Representation), a comprehensive dataset containing 13,979 images
that encompass 654 actions, 80 object categories, and 17 body parts.
Experimental evaluation demonstrates that PaIR-Net significantly outperforms
baseline approaches, while ablation studies confirm the efficacy of each
architectural component. The code and dataset will be released upon
publication.

</details>


### [37] [MPT: Motion Prompt Tuning for Micro-Expression Recognition](https://arxiv.org/abs/2508.09446)
*Jiateng Liu,Hengcan Shi,Feng Chen,Zhiwen Shao,Yaonan Wang,Jianfei Cai,Wenming Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法，通过运动提示调优（MPT）适配大模型用于微表情识别，显著提升了在小样本数据上的表现，并在多个数据集上超越了现有技术。


<details>
  <summary>Details</summary>
Motivation: 微表情识别在情感计算领域有重要应用，但由于微表情标注需要心理专业知识，导致数据集样本稀缺，限制了模型的学习能力。同时，目前的大模型无法直接捕捉微妙、短暂的面部运动，难以胜任微表情识别任务。

Method: 作者提出了运动提示调优（MPT）方法，将运动放大和高斯分词技术用于生成运动提示，以突显面部细微变化，并作为提示输入到大模型中。此外，设计了分组适配器嵌入大模型中，进一步增强模型在微表情领域中的表现。

Result: 在三个主流微表情识别数据集上的实验表明，所提MPT方法在准确率等指标上均优于当前最先进的其它方法。

Conclusion: 运动提示调优为大模型迁移到微表情识别提供了新思路，有效缓解了样本稀缺和特征捕捉难题，验证了该方法在实际应用中的有效性和优越性。

Abstract: Micro-expression recognition (MER) is crucial in the affective computing
field due to its wide application in medical diagnosis, lie detection, and
criminal investigation. Despite its significance, obtaining micro-expression
(ME) annotations is challenging due to the expertise required from
psychological professionals. Consequently, ME datasets often suffer from a
scarcity of training samples, severely constraining the learning of MER models.
While current large pre-training models (LMs) offer general and discriminative
representations, their direct application to MER is hindered by an inability to
capture transitory and subtle facial movements-essential elements for effective
MER. This paper introduces Motion Prompt Tuning (MPT) as a novel approach to
adapting LMs for MER, representing a pioneering method for subtle motion prompt
tuning. Particularly, we introduce motion prompt generation, including motion
magnification and Gaussian tokenization, to extract subtle motions as prompts
for LMs. Additionally, a group adapter is carefully designed and inserted into
the LM to enhance it in the target MER domain, facilitating a more nuanced
distinction of ME representation. Furthermore, extensive experiments conducted
on three widely used MER datasets demonstrate that our proposed MPT
consistently surpasses state-of-the-art approaches and verifies its
effectiveness.

</details>


### [38] [RASR: Retrieval-Augmented Super Resolution for Practical Reference-based Image Restoration](https://arxiv.org/abs/2508.09449)
*Jiaqi Yan,Shuning Xu,Xiangyu Chen,Dell Zhang,Jie Tang,Gangshan Wu,Jie Liu*

Main category: cs.CV

TL;DR: 提出了一种无需手动配对参考图像、可自动检索相关高分辨率参考图像的新型超分辨率方法（RASR），解决了传统参考超分辨率难以应用于真实场景的问题，并构建了首个检索增强超分辨率的数据集与基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的参考图像超分辨率方法局限于手动挑选的目标-参考图像对，实际中难以广泛应用。需要一种更自动化且适用于真实环境的方法。

Method: 提出检索增强超分辨率（RASR）范式，输入低质量图片后，从数据库中自动检索语义相关的高分辨率参考图像，并结合扩散生成网络进行超分辨重建。构建了RASR-Flickr30数据集，支持开放类别的参考检索。提出了结合语义检索器和扩散生成器的基线方法RASRNet。

Result: 在新数据集RASR-Flickr30上，RASRNet相比单图像超分辨率方法提升了0.38 dB的PSNR和0.0131的LPIPS，并生成了更真实的纹理。

Conclusion: 检索增强为参考超分辨率带来了更好的现实适用性和提升效果，为学术研究与实际应用之间架起了桥梁，是未来值得关注的发展方向。

Abstract: Reference-based Super Resolution (RefSR) improves upon Single Image Super
Resolution (SISR) by leveraging high-quality reference images to enhance
texture fidelity and visual realism. However, a critical limitation of existing
RefSR approaches is their reliance on manually curated target-reference image
pairs, which severely constrains their practicality in real-world scenarios. To
overcome this, we introduce Retrieval-Augmented Super Resolution (RASR), a new
and practical RefSR paradigm that automatically retrieves semantically relevant
high-resolution images from a reference database given only a low-quality
input. This enables scalable and flexible RefSR in realistic use cases, such as
enhancing mobile photos taken in environments like zoos or museums, where
category-specific reference data (e.g., animals, artworks) can be readily
collected or pre-curated. To facilitate research in this direction, we
construct RASR-Flickr30, the first benchmark dataset designed for RASR. Unlike
prior datasets with fixed target-reference pairs, RASR-Flickr30 provides
per-category reference databases to support open-world retrieval. We further
propose RASRNet, a strong baseline that combines a semantic reference retriever
with a diffusion-based RefSR generator. It retrieves relevant references based
on semantic similarity and employs a diffusion-based generator enhanced with
semantic conditioning. Experiments on RASR-Flickr30 demonstrate that RASRNet
consistently improves over SISR baselines, achieving +0.38 dB PSNR and -0.0131
LPIPS, while generating more realistic textures. These findings highlight
retrieval augmentation as a promising direction to bridge the gap between
academic RefSR research and real-world applicability.

</details>


### [39] [HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss](https://arxiv.org/abs/2508.09453)
*Abdul Matin,Tanjim Bin Faruk,Shrideep Pallickara,Sangmi Lee Pallickara*

Main category: cs.CV

TL;DR: 本文提出了一种称为HyperKD的新型知识蒸馏框架，用以解决超光谱遥感中谱域差异大和观测数据稀缺的问题，显著提升了基础模型在超光谱图像表征和下游任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的大型基础模型多在光学影像等通用数据上预训练，面对超光谱数据时，由于谱带差异大和训练样本稀缺，直接迁移性能不佳。如何利用已有基础模型的知识提升超光谱模型，是需要解决的问题。

Method: 作者提出HyperKD框架，将一个基础模型（Prithvi）中积累的知识，通过知识蒸馏的方式，迁移给专门针对EnMAP超光谱影像设计的学生模型。该框架基于Masked Autoencoder，采用谱段对齐、空间特征掩码、改进损失函数等方法解决谱域差异及谱间间隙带来的适应性问题。

Result: 通过大量实验，HyperKD在表征学习和多个下游遥感任务（如地表覆盖分类、作物类型识别、土壤有机碳预测）上，显著提升了重建精度和任务鲁棒性。

Conclusion: HyperKD框架有效弥合了基础模型和超光谱数据之间的域差，实现了知识的高效迁移，展示了知识蒸馏在超光谱遥感分析领域的重要应用前景。

Abstract: The proliferation of foundation models, pretrained on large-scale unlabeled
datasets, has emerged as an effective approach in creating adaptable and
reusable architectures that can be leveraged for various downstream tasks using
satellite observations. However, their direct application to hyperspectral
remote sensing remains challenging due to inherent spectral disparities and the
scarcity of available observations. In this work, we present HyperKD, a novel
knowledge distillation framework that enables transferring learned
representations from a teacher model into a student model for effective
development of a foundation model on hyperspectral images. Unlike typical
knowledge distillation frameworks, which use a complex teacher to guide a
simpler student, HyperKD enables an inverse form of knowledge transfer across
different types of spectral data, guided by a simpler teacher model. Building
upon a Masked Autoencoder, HyperKD distills knowledge from the Prithvi
foundational model into a student tailored for EnMAP hyperspectral imagery.
HyperKD addresses the inverse domain adaptation problem with spectral gaps by
introducing a feature-based strategy that includes spectral range-based channel
alignment, spatial feature-guided masking, and an enhanced loss function
tailored for hyperspectral images. HyperKD bridges the substantial spectral
domain gap, enabling the effective use of pretrained foundation models for
geospatial applications. Extensive experiments show that HyperKD significantly
improves representation learning in MAEs, leading to enhanced reconstruction
fidelity and more robust performance on downstream tasks such as land cover
classification, crop type identification, and soil organic carbon prediction,
underpinning the potential of knowledge distillation frameworks in remote
sensing analytics with hyperspectral imagery.

</details>


### [40] [Animate-X++: Universal Character Image Animation with Dynamic Backgrounds](https://arxiv.org/abs/2508.09454)
*Shuai Tan,Biao Gong,Zhuoxin Liu,Yan Wang,Xi Chen,Yifan Feng,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 这篇论文提出了Animate-X++，一个通用的角色图像动画生成框架，能够从参考图像和目标姿态序列生成高质量、有背景动态的视频，特别适用于拟人化角色。实验显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有角色动画生成方法多只针对人体角色，难以泛化到具备不同风格（如拟人化）的角色，且只能生成静态背景的视频，影响动画的真实感。作者旨在解决通用性和动态背景两个关键挑战。

Method: 论文提出Animate-X++，基于DiT设计，适配各种角色类型。为增强动作建模能力，引入Pose Indicator，包括CLIP隐式特征捕捉和通过显式方式增强推理泛化。为实现动态背景，提出多任务联合训练（动画+文本驱动背景生成）和局部参数训练。此外还构建A2Bench基准用于评测。

Result: Animate-X++在A2Bench和其他评测中表现出色，能够有效实现不同类型角色的动画及动态背景生成，超越以往方法。

Conclusion: Animate-X++通过改善运动表示和引入动态背景，使角色动画生成更加通用和真实，对游戏与娱乐行业中的动画制作具有重要意义。

Abstract: Character image animation, which generates high-quality videos from a
reference image and target pose sequence, has seen significant progress in
recent years. However, most existing methods only apply to human figures, which
usually do not generalize well on anthropomorphic characters commonly used in
industries like gaming and entertainment. Furthermore, previous methods could
only generate videos with static backgrounds, which limits the realism of the
videos. For the first challenge, our in-depth analysis suggests to attribute
this limitation to their insufficient modeling of motion, which is unable to
comprehend the movement pattern of the driving video, thus imposing a pose
sequence rigidly onto the target character. To this end, this paper proposes
Animate-X++, a universal animation framework based on DiT for various character
types, including anthropomorphic characters. To enhance motion representation,
we introduce the Pose Indicator, which captures comprehensive motion pattern
from the driving video through both implicit and explicit manner. The former
leverages CLIP visual features of a driving video to extract its gist of
motion, like the overall movement pattern and temporal relations among motions,
while the latter strengthens the generalization of DiT by simulating possible
inputs in advance that may arise during inference. For the second challenge, we
introduce a multi-task training strategy that jointly trains the animation and
TI2V tasks. Combined with the proposed partial parameter training, this
approach achieves not only character animation but also text-driven background
dynamics, making the videos more realistic. Moreover, we introduce a new
Animated Anthropomorphic Benchmark (A2Bench) to evaluate the performance of
Animate-X++ on universal and widely applicable animation images. Extensive
experiments demonstrate the superiority and effectiveness of Animate-X++.

</details>


### [41] [IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding](https://arxiv.org/abs/2508.09456)
*Junxian Li,Beining Xu,Di Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新型输入感知后门攻击方法IAG，能够在视觉-语言模型（VLMs）中操控目标定位行为，并在多种模型和测试集上表现出较高的攻击成功率且对干净样本影响较小。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在视觉指向相关任务上取得了较大突破，但其安全性、特别是后门攻击风险尚未受到充分关注。作者旨在揭示并测试后门攻击在VLMs定位任务中的威胁。

Method: 提出IAG攻击方法：利用条件U-Net生成带有目标对象语义信息的自适应触发器，将其嵌入原图，实现对任意查询都强制定位到攻击目标，并通过重建损失提升攻击隐蔽性。还设计了统一的攻击数据生成方法。

Result: IAG方法在InternVL-2.5-8B上的ASR@0.5超过65%，对Ferret-7B及LlaVA-1.5-7B也表现出良好效果，同时对干净样本准确率影响很小。消融实验和防御测试验证了其鲁棒性与可迁移性。

Conclusion: 作者所提IAG后门攻击方法能够有效、隐蔽地影响VLMs的视觉定位行为，在多模型、多场景下具有较强的适用性，需引起对VLMs安全的重视。

Abstract: Vision-language models (VLMs) have shown significant advancements in tasks
such as visual grounding, where they localize specific objects in images based
on natural language queries and images. However, security issues in visual
grounding tasks for VLMs remain underexplored, especially in the context of
backdoor attacks. In this paper, we introduce a novel input-aware backdoor
attack method, IAG, designed to manipulate the grounding behavior of VLMs. This
attack forces the model to ground a specific target object in the input image,
regardless of the user's query. We propose an adaptive trigger generator that
embeds the semantic information of the attack target's description into the
original image using a text-conditional U-Net, thereby overcoming the
open-vocabulary attack challenge. To ensure the attack's stealthiness, we
utilize a reconstruction loss to minimize visual discrepancies between poisoned
and clean images. Additionally, we introduce a unified method for generating
attack data. IAG is evaluated theoretically and empirically, demonstrating its
feasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches
over 65\% on various testing sets. IAG also shows promising potential on
manipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on
clean samples. Extensive specific experiments, such as ablation study and
potential defense, also indicate the robustness and transferability of our
attack.

</details>


### [42] [RelayFormer: A Unified Local-Global Attention Framework for Scalable Image and Video Manipulation Localization](https://arxiv.org/abs/2508.09459)
*Wen Huang,Jiarui Yang,Tao Dai,Jiawei Li,Shaoxiong Zhan,Bin Wang,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 本文提出了用于视觉篡改定位（VML）的新型统一架构RelayFormer，能高效处理不同分辨率的图片与视频，具有很强的跨模态泛化能力，并在多个基准测试中取得了最新最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉篡改定位方法在跨模态泛化能力、处理高分辨率或长时间数据的效率等方面表现有限，无法胜任日益复杂的实际取证需求。因此需要提出一种更高效、通用的方法。

Method: 提出了RelayFormer架构，采用灵活的局部单元和全局-局部中继注意力（GLoRA）机制，实现对不同分辨率输入的可扩展处理，并能与主流Transformer骨干结构（如ViT、SegFormer）无缝集成。此外，设计了轻量级的基于查询的mask解码器，可实现在视频序列上的线性复杂度推断。

Result: 在多个视觉篡改定位基准测试上，RelayFormer展现出优越的跨模态泛化与高效处理能力，取得了最前沿的定位性能，并树立了新的可扩展、多模态VML基线。

Conclusion: RelayFormer不仅兼容现有主流Transformer模型，而且能高效、准确地在不同分辨率、模态的图像和视频中定位篡改区域，为视觉取证和相关实际应用提供了有力支持，实现了跨模态与高效VML的新突破。

Abstract: Visual manipulation localization (VML) -- across both images and videos -- is
a crucial task in digital forensics that involves identifying tampered regions
in visual content. However, existing methods often lack cross-modal
generalization and struggle to handle high-resolution or long-duration inputs
efficiently.
  We propose RelayFormer, a unified and modular architecture for visual
manipulation localization across images and videos. By leveraging flexible
local units and a Global-Local Relay Attention (GLoRA) mechanism, it enables
scalable, resolution-agnostic processing with strong generalization. Our
framework integrates seamlessly with existing Transformer-based backbones, such
as ViT and SegFormer, via lightweight adaptation modules that require only
minimal architectural changes, ensuring compatibility without disrupting
pretrained representations.
  Furthermore, we design a lightweight, query-based mask decoder that supports
one-shot inference across video sequences with linear complexity. Extensive
experiments across multiple benchmarks demonstrate that our approach achieves
state-of-the-art localization performance, setting a new baseline for scalable
and modality-agnostic VML. Code is available at:
https://github.com/WenOOI/RelayFormer.

</details>


### [43] [Gen-AFFECT: Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy](https://arxiv.org/abs/2508.09461)
*Hao Yu,Rupayan Mallick,Margrit Betke,Sarah Adel Bargal*

Main category: cs.CV

TL;DR: 本文提出了GEN-AFFECT框架，实现了能够表现丰富细腻面部表情且始终保持身份一致性的2D头像生成。


<details>
  <summary>Details</summary>
Motivation: 现有个性化2D头像生成方法难以细致捕捉面部表情变化，且不同表情之间难以保持人物身份一致性。

Method: 引入多模态扩散Transformer，根据抽取的身份-表情特征进行头像生成，并在推断阶段采用一致注意力机制，保证在生成多种面部表情时，头像间可共享信息，提升身份一致性。

Result: GEN-AFFECT在表情准确性、身份保持和细腻表情下的身份一致性方面，相比现有先进方法，取得了更优表现。

Conclusion: GEN-AFFECT显著提升了2D头像的表情丰富性与身份一致性，为个性化多表情头像的生成提供了更优解决方案。

Abstract: Different forms of customized 2D avatars are widely used in gaming
applications, virtual communication, education, and content creation. However,
existing approaches often fail to capture fine-grained facial expressions and
struggle to preserve identity across different expressions. We propose
GEN-AFFECT, a novel framework for personalized avatar generation that generates
expressive and identity-consistent avatars with a diverse set of facial
expressions. Our framework proposes conditioning a multimodal diffusion
transformer on an extracted identity-expression representation. This enables
identity preservation and representation of a wide range of facial expressions.
GEN-AFFECT additionally employs consistent attention at inference for
information sharing across the set of generated expressions, enabling the
generation process to maintain identity consistency over the array of generated
fine-grained expressions. GEN-AFFECT demonstrates superior performance compared
to previous state-of-the-art methods on the basis of the accuracy of the
generated expressions, the preservation of the identity and the consistency of
the target identity across an array of fine-grained facial expressions.

</details>


### [44] [Event-driven Robust Fitting on Neuromorphic Hardware](https://arxiv.org/abs/2508.09466)
*Tam Ngoc-Bang Nguyen,Anh-Dzung Doan,Zhipeng Cai,Tat-Jun Chin*

Main category: cs.CV

TL;DR: 本文提出了一种基于神经形态硬件（如Intel Loihi 2）的鲁棒几何模型拟合方法，在保证精度的前提下能大幅降低能耗，仅消耗传统CPU方法约15%的能量。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统的大规模部署，能耗问题变得日益重要。然而，鲁棒拟合领域此前很少关注能效指标。本文旨在填补鲁棒拟合算法在高能效实现方面的空白。

Method: 作者专为神经形态硬件Loihi 2设计了新型脉冲神经网络（SNN）结构，并提出了事件驱动的模型估计算法，以适应Loihi 2的架构特点。同时，针对硬件现有的精度和指令集局限，提出了相应算法策略。

Result: 实验证明，该神经形态鲁棒拟合方法在能效上显著优于传统CPU上的算法，以相当精度仅消耗其15%的能量。

Conclusion: 利用神经形态硬件和创新的SNN设计，可以在保持鲁棒拟合算法精度的同时，实现大幅能耗降低，为AI落地场景提供了更可行的高能效解决方案。

Abstract: Robust fitting of geometric models is a fundamental task in many computer
vision pipelines. Numerous innovations have been produced on the topic, from
improving the efficiency and accuracy of random sampling heuristics to
generating novel theoretical insights that underpin new approaches with
mathematical guarantees. However, one aspect of robust fitting that has
received little attention is energy efficiency. This performance metric has
become critical as high energy consumption is a growing concern for AI
adoption. In this paper, we explore energy-efficient robust fitting via the
neuromorphic computing paradigm. Specifically, we designed a novel spiking
neural network for robust fitting on real neuromorphic hardware, the Intel
Loihi 2. Enabling this are novel event-driven formulations of model estimation
that allow robust fitting to be implemented in the unique architecture of Loihi
2, and algorithmic strategies to alleviate the current limited precision and
instruction set of the hardware. Results show that our neuromorphic robust
fitting consumes only a fraction (15%) of the energy required to run the
established robust fitting algorithm on a standard CPU to equivalent accuracy.

</details>


### [45] [CitySeg: A 3D Open Vocabulary Semantic Segmentation Foundation Model in City-scale Scenarios](https://arxiv.org/abs/2508.09470)
*Jialei Xu,Zizhuang Wei,Weikang You,Linyun Li,Weijian Sun*

Main category: cs.CV

TL;DR: 本文提出了CitySeg，一种适用于城市级点云语义分割的基础模型，通过引入文本模态实现开放词表分割和零样本推断，在九个基准测试集上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 城市规模的点云语义分割对于无人机感知至关重要，然而现有方法受限于数据规模、领域差异，泛化能力不足。

Method: 提出CitySeg，引入文本模态以支持开放词表与零样本推断；定制多域数据预处理规则，设计局部-全局交叉注意力网络增强点网络能力，引入分层分类方法及图结构编码类别关系，采用两阶段训练与hinge loss提升特征区分度。

Result: CitySeg在九个点云分割基准上取得最优实验结果，并首次支持无需视觉信息的零样本泛化能力。

Conclusion: CitySeg极大提升了城市规模点云语义分割的泛化与适应性，实现了开放词表与零样本推理，为无人机感知系统提供了更强的场景理解能力。

Abstract: Semantic segmentation of city-scale point clouds is a critical technology for
Unmanned Aerial Vehicle (UAV) perception systems, enabling the classification
of 3D points without relying on any visual information to achieve comprehensive
3D understanding. However, existing models are frequently constrained by the
limited scale of 3D data and the domain gap between datasets, which lead to
reduced generalization capability. To address these challenges, we propose
CitySeg, a foundation model for city-scale point cloud semantic segmentation
that incorporates text modality to achieve open vocabulary segmentation and
zero-shot inference. Specifically, in order to mitigate the issue of
non-uniform data distribution across multiple domains, we customize the data
preprocessing rules, and propose a local-global cross-attention network to
enhance the perception capabilities of point networks in UAV scenarios. To
resolve semantic label discrepancies across datasets, we introduce a
hierarchical classification strategy. A hierarchical graph established
according to the data annotation rules consolidates the data labels, and the
graph encoder is used to model the hierarchical relationships between
categories. In addition, we propose a two-stage training strategy and employ
hinge loss to increase the feature separability of subcategories. Experimental
results demonstrate that the proposed CitySeg achieves state-of-the-art (SOTA)
performance on nine closed-set benchmarks, significantly outperforming existing
approaches. Moreover, for the first time, CitySeg enables zero-shot
generalization in city-scale point cloud scenarios without relying on visual
information.

</details>


### [46] [Leveraging Failed Samples: A Few-Shot and Training-Free Framework for Generalized Deepfake Detection](https://arxiv.org/abs/2508.09475)
*Shibo Yao,Renshuai Tao,Xiaolong Zheng,Chao Liang,Chunjie Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的深度伪造检测方法FTNet，无需对新伪造样本进行训练，仅通过与少量已知伪造和真实样本的比对即可有效识别，从而显著提升对未知生成模型样本的检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法在遇到未知模型生成的伪造样本时泛化能力较差，而现实中这些未知样本实际上常常可以收集到。传统方法多关注零样本泛化，忽视了有效利用这些可得少量未知样本的可能性，因此亟需更实用的少样本检测机制。

Method: 提出了FTNet方法，无需复杂训练或参数更新，仅用一张评估集中的伪造样本和一张真实样本作为参考，通过最近邻比对判别后续测试样本的真假，实现了零训练下的少样本检测。该方法适应新出现的伪造形式，提升了现实场景的实用性。

Result: 在包含29种生成模型的大型伪造数据集上进行了全面实验，FTNet平均检测性能比已有方法提升了8.7%，创造了新的最优记录（SoTA）。

Conclusion: 将深度伪造检测从零样本泛化转变为可利用少量新型失败样本的现实少样本场景，能显著提升识别泛化能力和实际效果，为真实世界的伪造检测提供了更好思路。

Abstract: Recent deepfake detection studies often treat unseen sample detection as a
``zero-shot" task, training on images generated by known models but
generalizing to unknown ones. A key real-world challenge arises when a model
performs poorly on unknown samples, yet these samples remain available for
analysis. This highlights that it should be approached as a ``few-shot" task,
where effectively utilizing a small number of samples can lead to significant
improvement. Unlike typical few-shot tasks focused on semantic understanding,
deepfake detection prioritizes image realism, which closely mirrors real-world
distributions. In this work, we propose the Few-shot Training-free Network
(FTNet) for real-world few-shot deepfake detection. Simple yet effective, FTNet
differs from traditional methods that rely on large-scale known data for
training. Instead, FTNet uses only one fake samplefrom an evaluation set,
mimicking the scenario where new samples emerge in the real world and can be
gathered for use, without any training or parameter updates. During evaluation,
each test sample is compared to the known fake and real samples, and it is
classified based on the category of the nearest sample. We conduct a
comprehensive analysis of AI-generated images from 29 different generative
models and achieve a new SoTA performance, with an average improvement of 8.7\%
compared to existing methods. This work introduces a fresh perspective on
real-world deepfake detection: when the model struggles to generalize on a
few-shot sample, leveraging the failed samples leads to better performance.

</details>


### [47] [From Large Angles to Consistent Faces: Identity-Preserving Video Generation via Mixture of Facial Experts](https://arxiv.org/abs/2508.09476)
*Yuji Wang,Moran Li,Xiaobin Hu,Ran Yi,Jiangning Zhang,Chengming Xu,Weijian Cao,Yabiao Wang,Chengjie Wang,Lizhuang Ma*

Main category: cs.CV

TL;DR: 本论文提出了一种新的视频生成方法，旨在解决在大角度人脸情况下身份保持的问题，通过创新的Mixture of Facial Experts（MoFE）结构与定制化大角度人脸数据集，显著提升了视频生成的面部一致性和保真度。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在面对大角度人脸时难以保持身份一致，主要由于缺乏有效将身份特征融入生成结构的方法，以及现有公开数据集缺乏足够大角度的人脸样本。为此，论文提出相应创新以推动该领域发展。

Method: 1）方法上，引入了Mixture of Facial Experts（MoFE），由三个专家（身份专家、语义专家、细节专家）动态融合各类人脸特征，实现跨姿态身份感知、语义抽取与细节保留。2）数据方面，打造了专门的数据处理流程，强调人脸角度多样性（Face Constraints）和身份一致性（Identity Consistency），并基于此从公开人像视频中构建了大角度人脸数据集（LFA Dataset），共计46万条视频片段并标注人脸角度。

Result: 在LFA基准测试中，该方法在面部相似性、面部FID分数以及CLIP语义对齐等多项指标上均显著超过当前最优方法，实现了更高的身份一致性和画面质量。

Conclusion: 结合MoFE结构与新构建的大角度人脸数据集，显著提升了大角度人脸视频生成的身份一致性与画面质量，为该领域的发展提供了新的思路和数据资源。代码和数据集将公开。

Abstract: Current video generation models struggle with identity preservation under
large facial angles, primarily facing two challenges: the difficulty in
exploring an effective mechanism to integrate identity features into DiT
structure, and the lack of targeted coverage of large facial angles in existing
open-source video datasets. To address these, we present two key innovations.
First, we introduce a Mixture of Facial Experts (MoFE) that dynamically
combines complementary cues from three specialized experts, each designed to
capture distinct but mutually reinforcing aspects of facial attributes. The
identity expert captures cross-pose identity-sensitive features, the semantic
expert extracts high-level visual semantxics, and the detail expert preserves
pixel-level features (e.g., skin texture, color gradients). Furthermore, to
mitigate dataset limitations, we have tailored a data processing pipeline
centered on two key aspects: Face Constraints and Identity Consistency. Face
Constraints ensure facial angle diversity and a high proportion of facial
regions, while Identity Consistency preserves coherent person-specific features
across temporal sequences, collectively addressing the scarcity of large facial
angles and identity-stable training data in existing datasets. Leveraging this
pipeline, we have curated and refined a Large Face Angles (LFA) Dataset from
existing open-source human video datasets, comprising 460K video clips with
annotated facial angles. Experimental results on the LFA benchmark demonstrate
that our method, empowered by the LFA dataset, significantly outperforms prior
SOTA methods in face similarity, face FID, and CLIP semantic alignment. The
code and dataset will be made publicly available at
https://github.com/rain152/LFA-Video-Generation.

</details>


### [48] [CLIP-Flow: A Universal Discriminator for AI-Generated Images Inspired by Anomaly Detection](https://arxiv.org/abs/2508.09477)
*Zhipeng Yuan,Kai Wang,Weize Quan,Dong-Ming Yan,Tieru Wu*

Main category: cs.CV

TL;DR: 本文提出了一种通用的AI生成图像检测方法，不依赖于已知的AI生成图像进行训练，通过异常检测理念提升对未知生成模型AI图像的检测能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成模型的发展，AI生成图像与自然图像的视觉质量日益接近，引发了安全隐患。以往检测方法依赖于已知AI生成图像进行训练，对新、未知生成模型产生的图像检测能力有限，亟需更具泛化能力的检测方法。

Method: 作者提出基于异常检测的检测思路：无须访问任何真实的AI生成图像，利用无监督学习方式获取可泛化特征。具体方法包括使用预训练CLIP作为特征提取器，并设计类似正态化流的无监督模型。训练时引入代理图像（如通过频谱修改处理的自然图像）替代AI生成图像，使模型最小化代理图像的似然（有时结合最大化自然图像似然）。

Result: 通过大量实验，验证了所提方法对不同生成模型生成的AI图像检测的有效性，展现出良好的泛化及鲁棒性。

Conclusion: 该方法为AI生成内容的检测提供了一种更为通用、无需先验AI样本的解决思路，为应对未来更多样、复杂的AI生成图像提供了技术储备。

Abstract: With the rapid advancement of AI generative models, the visual quality of
AI-generated images (AIIs) has become increasingly close to natural images,
which inevitably raises security concerns. Most AII detectors often employ the
conventional image classification pipeline with natural images and AIIs
(generated by a generative model), which can result in limited detection
performance for AIIs from unseen generative models. To solve this, we proposed
a universal AI-generated image detector from the perspective of anomaly
detection. Our discriminator does not need to access any AIIs and learn a
generalizable representation with unsupervised learning. Specifically, we use
the pre-trained CLIP encoder as the feature extractor and design a normalizing
flow-like unsupervised model. Instead of AIIs, proxy images, e.g., obtained by
applying a spectral modification operation on natural images, are used for
training. Our models are trained by minimizing the likelihood of proxy images,
optionally combined with maximizing the likelihood of natural images. Extensive
experiments demonstrate the effectiveness of our method on AIIs produced by
various image generators.

</details>


### [49] [GazeLT: Visual attention-guided long-tailed disease classification in chest radiographs](https://arxiv.org/abs/2508.09478)
*Moinak Bhattacharya,Gagandeep Singh,Shubham Jain,Prateek Prasanna*

Main category: cs.CV

TL;DR: 本研究提出了一种名为GazeLT的方法，通过整合分析放射科医生的视线数据，提升了医疗影像中长尾疾病分类的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统的深度学习方法在处理医疗影像的长尾类别（即罕见疾病）时表现不佳，而放射科医生在阅读影像时会关注显著和细微的病变。如何将专家的视觉关注模式融入算法中，是提升分类效果的关键。

Method: GazeLT利用放射科医生解读影像时的视线（gaze）数据，捕捉其在不同时间段对图像不同区域的关注，通过“整合-分解”机制，将这些细粒度与粗粒度的信息结合到深度学习模型中，从而指导模型对重要及长尾类别的关注。

Result: 在NIH-CXR-LT和MIMIC-CXR-LT两个公开长尾疾病分类数据集上，GazeLT方法的平均准确率分别较最佳长尾损失方法高4.1%，较视觉注意力基线高21.7%。

Conclusion: GazeLT能够有效利用专家的视觉行为信号，显著提升长尾类别疾病的智能诊断准确率，有望在实际医疗影像辅助诊断中发挥作用。

Abstract: In this work, we present GazeLT, a human visual attention
integration-disintegration approach for long-tailed disease classification. A
radiologist's eye gaze has distinct patterns that capture both fine-grained and
coarser level disease related information. While interpreting an image, a
radiologist's attention varies throughout the duration; it is critical to
incorporate this into a deep learning framework to improve automated image
interpretation. Another important aspect of visual attention is that apart from
looking at major/obvious disease patterns, experts also look at
minor/incidental findings (few of these constituting long-tailed classes)
during the course of image interpretation. GazeLT harnesses the temporal aspect
of the visual search process, via an integration and disintegration mechanism,
to improve long-tailed disease classification. We show the efficacy of GazeLT
on two publicly available datasets for long-tailed disease classification,
namely the NIH-CXR-LT (n=89237) and the MIMIC-CXR-LT (n=111898) datasets.
GazeLT outperforms the best long-tailed loss by 4.1% and the visual
attention-based baseline by 21.7% in average accuracy metrics for these
datasets. Our code is available at https://github.com/lordmoinak1/gazelt.

</details>


### [50] [SkySplat: Generalizable 3D Gaussian Splatting from Multi-Temporal Sparse Satellite Images](https://arxiv.org/abs/2508.09479)
*Xuejun Huang,Xinyi Liu,Yi Wan,Zhi Zheng,Bin Zhang,Mingtao Xiong,Yingying Pei,Yongjun Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种针对稀疏视角卫星图像三维重建的新方法SkySplat，针对传统3D高斯溅射与卫星影像间的不兼容性与泛化能力不足等问题，显著提高了三维重建的效率和精度。


<details>
  <summary>Details</summary>
Motivation: 当前3DGS方法虽然高效，但无法很好应用于卫星影像，主要因为不支持RPC模型并且泛化能力弱，尤其在多时相稀疏卫星影像上效果不佳。

Method: 提出SkySplat框架，将RPC模型融合到3DGS流程，并采用自监督方式，仅依赖RGB图和相对高度指导，无需真实高度图。包括Cross-Self Consistency Module（CSCM）和多视角一致性聚合，前者以一致性掩码减少瞬时物体干扰，后者优化重建结果。

Result: SkySplat实现了比单场景优化法（如EOGS）快86倍的速度，同时精度更高；对比可泛化3DGS基线，其在DFC19数据集上MAE从13.18m降至1.80m，并在MVS3D基准展现良好跨数据集泛化能力。

Conclusion: SkySplat有效解决了泛化3DGS在卫星影像上的局限，提升了稀疏卫星图像三维重建的精度、效率与通用性。

Abstract: Three-dimensional scene reconstruction from sparse-view satellite images is a
long-standing and challenging task. While 3D Gaussian Splatting (3DGS) and its
variants have recently attracted attention for its high efficiency, existing
methods remain unsuitable for satellite images due to incompatibility with
rational polynomial coefficient (RPC) models and limited generalization
capability. Recent advances in generalizable 3DGS approaches show potential,
but they perform poorly on multi-temporal sparse satellite images due to
limited geometric constraints, transient objects, and radiometric
inconsistencies. To address these limitations, we propose SkySplat, a novel
self-supervised framework that integrates the RPC model into the generalizable
3DGS pipeline, enabling more effective use of sparse geometric cues for
improved reconstruction. SkySplat relies only on RGB images and
radiometric-robust relative height supervision, thereby eliminating the need
for ground-truth height maps. Key components include a Cross-Self Consistency
Module (CSCM), which mitigates transient object interference via
consistency-based masking, and a multi-view consistency aggregation strategy
that refines reconstruction results. Compared to per-scene optimization
methods, SkySplat achieves an 86 times speedup over EOGS with higher accuracy.
It also outperforms generalizable 3DGS baselines, reducing MAE from 13.18 m to
1.80 m on the DFC19 dataset significantly, and demonstrates strong
cross-dataset generalization on the MVS3D benchmark.

</details>


### [51] [COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets](https://arxiv.org/abs/2508.09886)
*Lingyu Chen,Yawen Zeng,Yue Wang,Peng Wan,Guo-chen Ning,Hongen Liao,Daoqiang Zhang,Fang Chen*

Main category: cs.CV

TL;DR: 提出了一种通用的多异构超声数据集学习框架——COME，其能够跨多个数据集有效提取判别特征，在三种评测条件下均大幅超越以往方法。


<details>
  <summary>Details</summary>
Motivation: 传统单数据集训练在新数据分布下表现不佳，超声图像分析受限于样本有限、声影、斑点噪声等，亟需能适用于多异构数据的统一框架，并解决数据集间干扰和特征保留的挑战。

Method: 提出COME框架，设计了结构-语义双重共享专家以搭建通用表示空间，再与不同数据集的源特定专家协同工作，提取互补特征，实现跨数据集判别能力和泛化能力提升。

Result: 在单数据集、器官内集成、器官间集成三种评测下，COME显著优于主流方法，其平均AP值有明显提升。

Conclusion: COME能够有效提升多异构超声数据集下的泛化与性能，适合小批量或未见新场景，是面向多源超声任务的有力方法。

Abstract: Conventional single-dataset training often fails with new data distributions,
especially in ultrasound (US) image analysis due to limited data, acoustic
shadows, and speckle noise. Therefore, constructing a universal framework for
multi-heterogeneous US datasets is imperative. However, a key challenge arises:
how to effectively mitigate inter-dataset interference while preserving
dataset-specific discriminative features for robust downstream task? Previous
approaches utilize either a single source-specific decoder or a domain
adaptation strategy, but these methods experienced a decline in performance
when applied to other domains. Considering this, we propose a Universal
Collaborative Mixture of Heterogeneous Source-Specific Experts (COME).
Specifically, COME establishes dual structure-semantic shared experts that
create a universal representation space and then collaborate with
source-specific experts to extract discriminative features through providing
complementary features. This design enables robust generalization by leveraging
cross-datasets experience distributions and providing universal US priors for
small-batch or unseen data scenarios. Extensive experiments under three
evaluation modes (single-dataset, intra-organ, and inter-organ integration
datasets) demonstrate COME's superiority, achieving significant mean AP
improvements over state-of-the-art methods. Our project is available at:
https://universalcome.github.io/UniversalCOME/.

</details>


### [52] [Episodic Memory Representation for Long-form Video Understanding](https://arxiv.org/abs/2508.09486)
*Yun Wang,Long Zhang,Jingren Liu,Jiaqi Yan,Zhanjie Zhang,Jiahao Zheng,Xun Yang,Dapeng Wu,Xiangyu Chen,Xuelong Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的无训练视频理解框架 Video-EM，通过模拟人类情节记忆，显著提升视频大语言模型（Video-LLM）在长视频理解和问答任务中的表现。该方法能提升4-9%的准确率，且使用更少的帧。


<details>
  <summary>Details</summary>
Motivation: 目前的Video-LLM在处理长视频时，因上下文窗口限制，只能选取少量关键帧，导致忽略时空关系与场景连续性，影响理解和问答准确度。

Method: 提出Video-EM框架，将关键帧建模为时间有序的情节事件，显式捕捉空间和时间动态。同时借助大语言模型的链式思维（CoT），逐步筛选出最小且信息量高的情节记忆，实现高效、准确的视频问答。

Result: 在多个基准数据集（Video-MME, EgoSchema, HourVideo, LVBench）上，Video-EM比传统方法少用帧数情况下，取得了4-9个百分点的性能提升。

Conclusion: Video-EM框架通过对关键帧时空关联与叙事还原建模，大幅提升了Video-LLM在长视频问答任务中的性能，表现优于以往的静态关键帧提取方法。

Abstract: Video Large Language Models (Video-LLMs) excel at general video understanding
but struggle with long-form videos due to context window limits. Consequently,
recent approaches focus on keyframe retrieval, condensing lengthy videos into a
small set of informative frames. Despite their practicality, these methods
simplify the problem to static text image matching, overlooking spatio temporal
relationships crucial for capturing scene transitions and contextual
continuity, and may yield redundant keyframes with limited information,
diluting salient cues essential for accurate video question answering. To
address these limitations, we introduce Video-EM, a training free framework
inspired by the principles of human episodic memory, designed to facilitate
robust and contextually grounded reasoning. Rather than treating keyframes as
isolated visual entities, Video-EM explicitly models them as temporally ordered
episodic events, capturing both spatial relationships and temporal dynamics
necessary for accurately reconstructing the underlying narrative. Furthermore,
the framework leverages chain of thought (CoT) thinking with LLMs to
iteratively identify a minimal yet highly informative subset of episodic
memories, enabling efficient and accurate question answering by Video-LLMs.
Extensive evaluations on the Video-MME, EgoSchema, HourVideo, and LVBench
benchmarks confirm the superiority of Video-EM, which achieves highly
competitive results with performance gains of 4-9 percent over respective
baselines while utilizing fewer frames.

</details>


### [53] [Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation](https://arxiv.org/abs/2508.09987)
*Junyan Ye,Dongzhi Jiang,Zihao Wang,Leqi Zhu,Zhenghao Hu,Zilong Huang,Jun He,Zhiyuan Yan,Jinghua Yu,Hongsheng Li,Conghui He,Weijia Li*

Main category: cs.CV

TL;DR: 本文提出利用GPT-4o生成的合成图像数据集（Echo-4o-Image）来提升开源多模态生成模型的性能，并取得在评测基准上的显著提升。


<details>
  <summary>Details</summary>
Motivation: 尽管开源模型在图像生成方面与GPT-4o的性能仍有差距，一些工作通过蒸馏GPT-4o的数据来改进开源模型，取得了进展，但疑问在于为何要用GPT-4o的合成数据而不只依赖真实世界图像集。

Method: 作者指出合成图像数据的两大优势：1）能补足真实数据集中稀有场景（如幻想场景、多参考生成）满足用户查询，2）提供更干净、可控的监督以提升文本-图像对齐。基于此，作者构建了180K规模的Echo-4o-Image合成图像数据集，并用它微调Bagel基线模型，得到Echo-4o，同时提出了GenEval++与Imagine-Bench两个新评测基准。

Result: Echo-4o模型在多个标准基准测试中表现优异。将Echo-4o-Image数据集应用于其他基座模型（如OmniGen2、BLIP3-o）同样带来多项指标的一致提升，体现数据集的良好可迁移性。

Conclusion: GPT-4o生成的合成数据不仅能够弥补真实数据的覆盖盲点，还能以更纯净和明确的方式促进多模态生成模型的训练和性能提升，Echo-4o-Image数据集在多个模型和任务上均展现出显著价值和可迁移性。

Abstract: Recently, GPT-4o has garnered significant attention for its strong
performance in image generation, yet open-source models still lag behind.
Several studies have explored distilling image data from GPT-4o to enhance
open-source models, achieving notable progress. However, a key question
remains: given that real-world image datasets already constitute a natural
source of high-quality data, why should we use GPT-4o-generated synthetic data?
In this work, we identify two key advantages of synthetic images. First, they
can complement rare scenarios in real-world datasets, such as surreal fantasy
or multi-reference image generation, which frequently occur in user queries.
Second, they provide clean and controllable supervision. Real-world data often
contains complex background noise and inherent misalignment between text
descriptions and image content, whereas synthetic images offer pure backgrounds
and long-tailed supervision signals, facilitating more accurate text-to-image
alignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale
synthetic dataset generated by GPT-4o, harnessing the power of synthetic image
data to address blind spots in real-world coverage. Using this dataset, we
fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o.
In addition, we propose two new evaluation benchmarks for a more accurate and
challenging assessment of image generation capabilities: GenEval++, which
increases instruction complexity to mitigate score saturation, and
Imagine-Bench, which focuses on evaluating both the understanding and
generation of imaginative content. Echo-4o demonstrates strong performance
across standard benchmarks. Moreover, applying Echo-4o-Image to other
foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains
across multiple metrics, highlighting the datasets strong transferability.

</details>


### [54] [SARE: Semantic-Aware Reconstruction Error for Generalizable Diffusion-Generated Image Detection](https://arxiv.org/abs/2508.09487)
*Ju Yeon Kang,Jaehong Park,Semin Kim,Ji Won Yoon,Nam Soo Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的检测方法，通过计算图像与基于其描述（caption）重建结果之间的语义差异，实现对扩散模型生成图片的有效识别，尤其在识别来自未知分布或新型生成模型的假图片时表现优越。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的图片质量越来越高，常规检测方法往往依赖特定模型的伪影，导致在面对从未见过的新型生成模型的假图片时检测效果显著下降。为提升检测的泛化能力，需要寻找一种不依赖于特定生成模型痕迹的方法。

Method: 作者观察到，假图片与其文本描述的一致性通常高于真实图片。基于此，提出Semantic-Aware Reconstruction Error（SARE）指标，通过对图片进行caption引导的重建，然后量化重建前后图像的语义变化。真实图片的描述难以涵盖所有图像细节，会导致重建产生更大语义差异，而假图片重建前后的变化小。通过SARE作为特征进行检测。

Result: 实验证明，所提方法在GenImage、CommunityForensics等基准数据集上均显著优于现有的检测方法，尤其是在面对未知分布（OOD）生成模型时具有更强的鲁棒性和泛化能力。

Conclusion: SARE方法能够有效提升扩散生成图片检测的泛化能力，为应对不断进化的生成模型提供了可靠工具，具有良好的实际应用前景。

Abstract: Recently, diffusion-generated image detection has gained increasing
attention, as the rapid advancement of diffusion models has raised serious
concerns about their potential misuse. While existing detection methods have
achieved promising results, their performance often degrades significantly when
facing fake images from unseen, out-of-distribution (OOD) generative models,
since they primarily rely on model-specific artifacts. To address this
limitation, we explore a fundamental property commonly observed in fake images.
Motivated by the observation that fake images tend to exhibit higher similarity
to their captions than real images, we propose a novel representation, namely
Semantic-Aware Reconstruction Error (SARE), that measures the semantic
difference between an image and its caption-guided reconstruction. The
hypothesis behind SARE is that real images, whose captions often fail to fully
capture their complex visual content, may undergo noticeable semantic shifts
during the caption-guided reconstruction process. In contrast, fake images,
which closely align with their captions, show minimal semantic changes. By
quantifying these semantic shifts, SARE can be utilized as a discriminative
feature for robust detection across diverse generative models. We empirically
demonstrate that the proposed method exhibits strong generalization,
outperforming existing baselines on benchmarks including GenImage and
CommunityForensics.

</details>


### [55] [CWFBind: Geometry-Awareness for Fast and Accurate Protein-Ligand Docking](https://arxiv.org/abs/2508.09499)
*Liyan Jia,Chuan-Xian Ren,Hong Yan*

Main category: cs.CV

TL;DR: CWFBind是一种利用局部曲率特征加权的高效分子对接方法，能更准确地预测小分子与蛋白质的结合构象。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的分子对接方法多依赖图神经网络和语言模型，却忽视了重要的几何信息，导致结合口袋定位和结合构象预测不准。

Method: CWFBind在特征提取阶段引入了局部曲率描述符，丰富了蛋白质与配体的几何表达，并在消息传递过程中结合了度感知加权机制。为解决口袋分类不均衡问题，采用了配体感知的动态半径策略及增强损失函数。

Result: CWFBind在多个分子对接基准测试中取得了与主流方法有竞争力的性能，实现了准确性与效率的良好平衡。

Conclusion: CWFBind利用曲率和加权机制提升了几何建模能力，使小分子配体与蛋白的结合位点及结合构象预测更为准确，推动了分子对接算法发展。

Abstract: Accurately predicting the binding conformation of small-molecule ligands to
protein targets is a critical step in rational drug design. Although recent
deep learning-based docking surpasses traditional methods in speed and
accuracy, many approaches rely on graph representations and language
model-inspired encoders while neglecting critical geometric information,
resulting in inaccurate pocket localization and unrealistic binding
conformations. In this study, we introduce CWFBind, a weighted, fast, and
accurate docking method based on local curvature features. Specifically, we
integrate local curvature descriptors during the feature extraction phase to
enrich the geometric representation of both proteins and ligands, complementing
existing chemical, sequence, and structural features. Furthermore, we embed
degree-aware weighting mechanisms into the message passing process, enhancing
the model's ability to capture spatial structural distinctions and interaction
strengths. To address the class imbalance challenge in pocket prediction,
CWFBind employs a ligand-aware dynamic radius strategy alongside an enhanced
loss function, facilitating more precise identification of binding regions and
key residues. Comprehensive experimental evaluations demonstrate that CWFBind
achieves competitive performance across multiple docking benchmarks, offering a
balanced trade-off between accuracy and efficiency.

</details>


### [56] [Generation of Indian Sign Language Letters, Numbers, and Words](https://arxiv.org/abs/2508.09522)
*Ajeet Kumar Yadav,Nishant Kumar,Rathna G N*

Main category: cs.CV

TL;DR: 本文提出了一种结合ProGAN和SAGAN优点的新型生成对抗网络（GAN）变体，用于生成高分辨率、特征丰富的印度手语图像，并发布了相关大规模数据集。


<details>
  <summary>Details</summary>
Motivation: 手语是听障人士交流的重要媒介，但非手语群体难以交流。虽然手语识别有进展，手语生成领域仍亟需探索，特别是在兼顾图像分辨率与细节方面。

Method: 提出了一种融合ProGAN与SAGAN模型优点的注意力机制GAN变体，针对印度手语字母、数字与单词图像进行有条件、高分辨率生成，并与传统ProGAN进行对比评测。

Result: 所提出模型在IS（Inception Score）和FID（Fréchet Inception Distance）评价指标上分别较传统ProGAN提高了3.2和30.12，生成了质量更高的手语图像。

Conclusion: 融合ProGAN和SAGAN的改进型Attention GAN能够更好地生成高质量印度手语图像，为手语交流和AI辅助沟通带来新进展，并发布了高质量相关数据集。

Abstract: Sign language, which contains hand movements, facial expressions and bodily
gestures, is a significant medium for communicating with hard-of-hearing
people. A well-trained sign language community communicates easily, but those
who don't know sign language face significant challenges. Recognition and
generation are basic communication methods between hearing and hard-of-hearing
individuals. Despite progress in recognition, sign language generation still
needs to be explored. The Progressive Growing of Generative Adversarial Network
(ProGAN) excels at producing high-quality images, while the Self-Attention
Generative Adversarial Network (SAGAN) generates feature-rich images at medium
resolutions. Balancing resolution and detail is crucial for sign language image
generation. We are developing a Generative Adversarial Network (GAN) variant
that combines both models to generate feature-rich, high-resolution, and
class-conditional sign language images. Our modified Attention-based model
generates high-quality images of Indian Sign Language letters, numbers, and
words, outperforming the traditional ProGAN in Inception Score (IS) and
Fr\'echet Inception Distance (FID), with improvements of 3.2 and 30.12,
respectively. Additionally, we are publishing a large dataset incorporating
high-quality images of Indian Sign Language alphabets, numbers, and 129 words.

</details>


### [57] [SOI is the Root of All Evil: Quantifying and Breaking Similar Object Interference in Single Object Tracking](https://arxiv.org/abs/2508.09524)
*Yipei Wang,Shiyu Hu,Shukun Jia,Panxi Xu,Hongfei Ma,Yiping Ma,Jing Zhang,Xiaobo Lu,Xin Zhao*

Main category: cs.CV

TL;DR: 本文首次系统性研究并量化了单目标跟踪（SOT）中的相似目标干扰（SOI）问题，并提出SOIBench基准用于语义认知指导下的SOI挑战评测，显著推动该领域研究。


<details>
  <summary>Details</summary>
Motivation: 单目标跟踪在真实应用中经常遭遇相似目标干扰，这一直是被忽视却影响显著的性能瓶颈，理解并解决SOI对于提升跟踪鲁棒性具有重要意义。

Method: 作者通过设计在线干扰掩码（OIM）实验量化SOI瓶颈，用自然语言作为外部认知指导，提出自动化捕捉SOI帧和多级注释协议，构建SOIBench数据集，并系统评价现有视觉-语言跟踪方法和新型VLM认知引擎方案。

Result: 消除SOI后，所有主流跟踪器的AUC提升高达4.35，证明SOI是主要限制；现有视觉-语言方法仅带来-0.26至+0.71的AUC变化，改用大型视觉-语言模型（VLM）后AUC提升可达0.93，效果优于传统方法。

Conclusion: SOI是SOT中的关键挑战，语义认知指导具有提升潜力。SOIBench可作为该方向的评测标准，新提出的VLM认知引擎方法为解决SOI提供了有效途径。

Abstract: In this paper, we present the first systematic investigation and
quantification of Similar Object Interference (SOI), a long-overlooked yet
critical bottleneck in Single Object Tracking (SOT). Through controlled Online
Interference Masking (OIM) experiments, we quantitatively demonstrate that
eliminating interference sources leads to substantial performance improvements
(AUC gains up to 4.35) across all SOTA trackers, directly validating SOI as a
primary constraint for robust tracking and highlighting the feasibility of
external cognitive guidance. Building upon these insights, we adopt natural
language as a practical form of external guidance, and construct SOIBench-the
first semantic cognitive guidance benchmark specifically targeting SOI
challenges. It automatically mines SOI frames through multi-tracker collective
judgment and introduces a multi-level annotation protocol to generate precise
semantic guidance texts. Systematic evaluation on SOIBench reveals a striking
finding: existing vision-language tracking (VLT) methods fail to effectively
exploit semantic cognitive guidance, achieving only marginal improvements or
even performance degradation (AUC changes of -0.26 to +0.71). In contrast, we
propose a novel paradigm employing large-scale vision-language models (VLM) as
external cognitive engines that can be seamlessly integrated into arbitrary RGB
trackers. This approach demonstrates substantial improvements under semantic
cognitive guidance (AUC gains up to 0.93), representing a significant
advancement over existing VLT methods. We hope SOIBench will serve as a
standardized evaluation platform to advance semantic cognitive tracking
research and contribute new insights to the tracking research community.

</details>


### [58] [Learning Spatial Decay for Vision Transformers](https://arxiv.org/abs/2508.09525)
*Yuxin Mao,Zhen Qin,Jinxing Zhou,Bin Fan,Jing Zhang,Yiran Zhong,Yuchao Dai*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Spatial Decay Transformer (SDT)的新型视觉Transformer模型，引入上下文感知门控机制（CAG），实现空间衰减对输入内容的动态自适应，在ImageNet-1K等任务上优于现有方法，确立了数据自适应空间衰减的新范式。


<details>
  <summary>Details</summary>
Motivation: ViT虽推动了计算机视觉进步，但其自注意力机制缺乏显式空间先验，限制了在空间结构化任务上的表现。已有工作采用与内容无关的固定空间衰减，难以适应复杂多样的视觉场景，激发了提出内容感知空间衰减机制的需求。

Method: 提出SDT模型，核心是上下文感知门控（CAG）机制，根据图像内容和空间距离自适应调整patch间注意力。通过统一的空间-内容融合框架，将曼哈顿距离空间先验与学习到的内容特征结合，实现了1D语言模型门控在2D视觉任务的拓展。

Result: 在ImageNet-1K分类和生成任务上，SDT模型在多个基线下取得了一致优于现有视觉Transformer方法的实验结果，验证了其有效性。

Conclusion: 数据自适应空间衰减机制有效提升了视觉Transformer的空间建模能力，为视觉Transformer空间注意力机制的研究和应用开辟了新范式。

Abstract: Vision Transformers (ViTs) have revolutionized computer vision, yet their
self-attention mechanism lacks explicit spatial inductive biases, leading to
suboptimal performance on spatially-structured tasks. Existing approaches
introduce data-independent spatial decay based on fixed distance metrics,
applying uniform attention weighting regardless of image content and limiting
adaptability to diverse visual scenarios. Inspired by recent advances in large
language models where content-aware gating mechanisms (e.g., GLA, HGRN2, FOX)
significantly outperform static alternatives, we present the first successful
adaptation of data-dependent spatial decay to 2D vision transformers. We
introduce \textbf{Spatial Decay Transformer (SDT)}, featuring a novel
Context-Aware Gating (CAG) mechanism that generates dynamic, data-dependent
decay for patch interactions. Our approach learns to modulate spatial attention
based on both content relevance and spatial proximity. We address the
fundamental challenge of 1D-to-2D adaptation through a unified spatial-content
fusion framework that integrates manhattan distance-based spatial priors with
learned content representations. Extensive experiments on ImageNet-1K
classification and generation tasks demonstrate consistent improvements over
strong baselines. Our work establishes data-dependent spatial decay as a new
paradigm for enhancing spatial attention in vision transformers.

</details>


### [59] [Physics-guided Deep Unfolding Network for Enhanced Kronecker Compressive sensing](https://arxiv.org/abs/2508.09528)
*Gang Qu,Ping Wang,Siming Zheng,Xin Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的图像压缩感知（CS）模型MEUNet，通过改进测量相干性及引入注意力机制，实现了更优的重建准确率和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有CS方法在采样阶段测量具有高相干性，且重建过程中测量表征不充分，导致恢复精度受限。作者旨在解决如何提升测量的不可相干性、如何从测量中学习更有效表征两个核心问题。

Method: 作者提出了非对称Kronecker CS（AKCS）模型，与以往Kronecker CS相比，仅带来极小复杂度提升却理论上具有更好的不可相干性。同时提出MACA（测量感知跨注意力）机制，以增强网络对测量表征的隐式学习。将AKCS和MACA集成进主流Unfolding网络架构，构建了MEUNet。

Result: 实验表明，MEUNet在图像重建精度和推理速度方面均优于现有方法，达到了最新水平。

Conclusion: 本文方法通过结构创新和注意力机制，有效解决了测量相干性和表征学习上的不足，显著提升了压缩感知的性能和效率。

Abstract: Deep networks have achieved remarkable success in image compressed sensing
(CS) task, namely reconstructing a high-fidelity image from its compressed
measurement. However, existing works are deficient inincoherent compressed
measurement at sensing phase and implicit measurement representations at
reconstruction phase, limiting the overall performance. In this work, we answer
two questions: 1) how to improve the measurement incoherence for decreasing the
ill-posedness; 2) how to learn informative representations from measurements.
To this end, we propose a novel asymmetric Kronecker CS (AKCS) model and
theoretically present its better incoherence than previous Kronecker CS with
minimal complexity increase. Moreover, we reveal that the unfolding networks'
superiority over non-unfolding ones result from sufficient gradient descents,
called explicit measurement representations. We propose a measurement-aware
cross attention (MACA) mechanism to learn implicit measurement representations.
We integrate AKCS and MACA into widely-used unfolding architecture to get a
measurement-enhanced unfolding network (MEUNet). Extensive experiences
demonstrate that our MEUNet achieves state-of-the-art performance in
reconstruction accuracy and inference speed.

</details>


### [60] [COXNet: Cross-Layer Fusion with Adaptive Alignment and Scale Integration for RGBT Tiny Object Detection](https://arxiv.org/abs/2508.09533)
*Peiran Peng,Tingfa Xu,Liqiang Song,Mengqi Zhu,Yuqiang Fang,Jianan Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态红绿蓝-热成像（RGBT）微小目标检测方法COXNet，在无人机复杂环境下显著提高了检测性能。


<details>
  <summary>Details</summary>
Motivation: 微小目标在多模态（可见光与热成像）图像中的检测任务对于监控、搜救和自动导航等场景非常关键。无人机平台下存在空间错位、低光、遮挡以及背景复杂等更大挑战，现有方法难以有效整合多模态互补信息。

Method: 提出COXNet框架，通过三项创新：1）跨层融合模块，将高层可见光与低层热成像特征融合，提高语义和空间准确性；2）动态对齐与尺度优化模块，校正跨模态空间错位并保留多尺度特征；3）基于GeoShape相似性的新标签分配策略，提升目标定位精度。

Result: 在RGBTDronePerson数据集上，COXNet实现了比当前最优方法高3.32%的mAP$_{50}$，显示出更强的鲁棒性和检测能力。

Conclusion: COXNet有效解决了复杂环境下多模态微小目标检测难题，方法在提升检测准确率和鲁棒性上取得了显著进展。

Abstract: Detecting tiny objects in multimodal Red-Green-Blue-Thermal (RGBT) imagery is
a critical challenge in computer vision, particularly in surveillance, search
and rescue, and autonomous navigation. Drone-based scenarios exacerbate these
challenges due to spatial misalignment, low-light conditions, occlusion, and
cluttered backgrounds. Current methods struggle to leverage the complementary
information between visible and thermal modalities effectively. We propose
COXNet, a novel framework for RGBT tiny object detection, addressing these
issues through three core innovations: i) the Cross-Layer Fusion Module, fusing
high-level visible and low-level thermal features for enhanced semantic and
spatial accuracy; ii) the Dynamic Alignment and Scale Refinement module,
correcting cross-modal spatial misalignments and preserving multi-scale
features; and iii) an optimized label assignment strategy using the GeoShape
Similarity Measure for better localization. COXNet achieves a 3.32\% mAP$_{50}$
improvement on the RGBTDronePerson dataset over state-of-the-art methods,
demonstrating its effectiveness for robust detection in complex environments.

</details>


### [61] [Iterative Volume Fusion for Asymmetric Stereo Matching](https://arxiv.org/abs/2508.09543)
*Yuanting Gao,Linghao Shen*

Main category: cs.CV

TL;DR: 本文提出了一种新的双阶段迭代体积融合网络（IVF-AStereo），用于解决非对称立体匹配问题，在具有分辨率和色彩劣化的非对称环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着多摄像头系统在实际中的应用增多（如长焦-广角摄像头组合），传统假设双目视觉特性对称的立体匹配方法难以适用。因此亟需应对视觉非对称带来的匹配代价体积（cost volume）失真问题。

Method: 分析了常用的两种cost volume构建方法在非对称环境下的信息失真分别情况，并提出融合二者优势。具体方法为：第一阶段用聚合的级联体积（concatenation volume）去细化相关性体积（correlation volume）；第二阶段将两种体积融合以提升精细细节。

Result: 所提方法在多种基准数据集上与现有方法进行了对比实验，并通过消融实验验证。在分辨率和色彩劣化情况下，方法在非对称立体匹配任务展现出更强的鲁棒性和优越性能。

Conclusion: 综合利用不同构建方式的cost volume能有效缓解非对称立体匹配的信息失真问题，新提出的IVF-AStereo网络在具有显著视觉非对称的应用场景中具有实际价值。

Abstract: Stereo matching is vital in 3D computer vision, with most algorithms assuming
symmetric visual properties between binocular visions. However, the rise of
asymmetric multi-camera systems (e.g., tele-wide cameras) challenges this
assumption and complicates stereo matching. Visual asymmetry disrupts stereo
matching by affecting the crucial cost volume computation. To address this, we
explore the matching cost distribution of two established cost volume
construction methods in asymmetric stereo. We find that each cost volume
experiences distinct information distortion, indicating that both should be
comprehensively utilized to solve the issue. Based on this, we propose the
two-phase Iterative Volume Fusion network for Asymmetric Stereo matching
(IVF-AStereo). Initially, the aggregated concatenation volume refines the
correlation volume. Subsequently, both volumes are fused to enhance fine
details. Our method excels in asymmetric scenarios and shows robust performance
against significant visual asymmetry. Extensive comparative experiments on
benchmark datasets, along with ablation studies, confirm the effectiveness of
our approach in asymmetric stereo with resolution and color degradation.

</details>


### [62] [GoViG: Goal-Conditioned Visual Navigation Instruction Generation](https://arxiv.org/abs/2508.09547)
*Fengyi Wu,Yifei Dong,Zhi-Qi Cheng,Yilong Dai,Guangyu Chen,Hang Wang,Qi Dai,Alexander G. Hauptmann*

Main category: cs.CV

TL;DR: 该论文提出了一种新的任务GoViG，仅通过初始和目标状态的第一视角视觉观察自动生成精准且具有上下文的导航指令，提升了其在未知和非结构化环境中的适应性。方法表现超过当前主流方案。


<details>
  <summary>Details</summary>
Motivation: 现有视觉导航指令生成方法多依赖结构化输入（语义标注或环境地图），而人类在实际导航时常常仅凭视觉信息理解和规划，因而该研究希望探索能仅基于原始视觉输入生成导航指令的方法，从而提升泛化能力和实用性。

Method: 该方法将任务分为两个子任务：一是视觉预测，拟合起始到目标视角间的中间视觉状态；二是指令生成，将观测到和预测到的视觉信息转化为语言指令。这两个子任务整合在一个自回归多模态大语言模型中，并采用定制目标训练以保证空间准确性和语言清晰性。此外，引入了模拟人类认知流程的一次性推理和交替推理策略。

Result: 据提出的R2R-Goal数据集（融合多样化的合成与真实轨迹）实验，所提方法在BLEU-4和CIDEr等指标以及跨域泛化能力上显著优于现有SOTA方法。

Conclusion: GoViG能够在只依赖第一视角视觉输入的情况下，生成高质量的导航指令，在多个场景中展现出强泛化能力，证明了其实际应用的潜力。

Abstract: We introduce Goal-Conditioned Visual Navigation Instruction Generation
(GoViG), a new task that aims to autonomously generate precise and contextually
coherent navigation instructions solely from egocentric visual observations of
initial and goal states. Unlike conventional approaches that rely on structured
inputs such as semantic annotations or environmental maps, GoViG exclusively
leverages raw egocentric visual data, substantially improving its adaptability
to unseen and unstructured environments. Our method addresses this task by
decomposing it into two interconnected subtasks: (1) visual forecasting, which
predicts intermediate visual states bridging the initial and goal views; and
(2) instruction generation, which synthesizes linguistically coherent
instructions grounded in both observed and anticipated visuals. These subtasks
are integrated within an autoregressive multimodal large language model trained
with tailored objectives to ensure spatial accuracy and linguistic clarity.
Furthermore, we introduce two complementary multimodal reasoning strategies,
one-pass and interleaved reasoning, to mimic incremental human cognitive
processes during navigation. To evaluate our method, we propose the R2R-Goal
dataset, combining diverse synthetic and real-world trajectories. Empirical
results demonstrate significant improvements over state-of-the-art methods,
achieving superior BLEU-4 and CIDEr scores along with robust cross-domain
generalization.

</details>


### [63] [Exploring the Equivalence of Closed-Set Generative and Real Data Augmentation in Image Classification](https://arxiv.org/abs/2508.09550)
*Haowen Wang,Guowei Zhang,Xiang Zhang,Zeyuan Chen,Haiyang Xu,Dou Hoon Kwark,Zhuowen Tu*

Main category: cs.CV

TL;DR: 本文研究了用生成模型在图像分类任务中进行闭集数据增强对分类性能的提升作用，并系统分析了真实与生成数据增强的效果及其需求量。


<details>
  <summary>Details</summary>
Motivation: 机器学习中，数据增强有助于提升模型性能，但高质量真实数据难以获取。利用生成模型合成的闭集数据能否有效增强分类性能，是该文关注的科学问题。

Method: 作者通过大量实验证明，比较了真实图像、闭集生成图像和开放集生成图像在数据增强中的作用，量化了需要多少生成图像才能达到与真实图像增强相似的效果。

Result: 实验定量分析了闭集生成数据增强所需的合成图像规模，并展示了现实数据增强与开放集生成增强在效果上的量化等价。此外，结果表明真实图像效果更佳，但合成数据在一定规模下可达到相近性能。

Conclusion: 即便生成数据无法完全替代真实数据，但适量的高质量合成图像能大幅提升分类性能，文中还给出了针对不同基础数据规模与合成数据量的指导建议。

Abstract: In this paper, we address a key scientific problem in machine learning: Given
a training set for an image classification task, can we train a generative
model on this dataset to enhance the classification performance? (i.e.,
closed-set generative data augmentation). We start by exploring the
distinctions and similarities between real images and closed-set synthetic
images generated by advanced generative models. Through extensive experiments,
we offer systematic insights into the effective use of closed-set synthetic
data for augmentation. Notably, we empirically determine the equivalent scale
of synthetic images needed for augmentation. In addition, we also show
quantitative equivalence between the real data augmentation and open-set
generative augmentation (generative models trained using data beyond the given
training set). While it aligns with the common intuition that real images are
generally preferred, our empirical formulation also offers a guideline to
quantify the increased scale of synthetic data augmentation required to achieve
comparable image classification performance. Our results on natural and medical
image datasets further illustrate how this effect varies with the baseline
training set size and the amount of synthetic data incorporated.

</details>


### [64] [Topological Invariant-Based Iris Identification via Digital Homology and Machine Learning](https://arxiv.org/abs/2508.09555)
*Ahmet Öztel,İsmet Karaca*

Main category: cs.CV

TL;DR: 该研究提出了一种基于二维虹膜图像拓扑不变量的生物识别方法，并与深度学习方法进行了对比。


<details>
  <summary>Details</summary>
Motivation: 传统虹膜识别依赖于特征提取和黑盒深度学习方法，存在可解释性和对数据量需求大的问题。作者希望通过正式定义的数字同调拓扑特征，提升虹膜识别准确率，同时兼具紧凑性和可解释性。

Method: 将归一化后的虹膜图像划分为若干网格子区域，分别计算每个子区域的Betti0、Betti1及其比值，用作特征输入到逻辑回归、KNN、SVM等分类器，并与卷积神经网络（CNN）基于原始图像的结果进行对比。在分类训练中还使用了主成分分析（PCA）和100次随机重复以评估性能稳定性。

Result: 逻辑回归模型基于拓扑特征的识别准确率达到97.78%（±0.82%），优于CNN（96.44%±1.32%）和其他对比模型。拓扑特征的准确率高且方差较小。

Conclusion: 首次将正式数字同调的拓扑不变量用于虹膜识别，提出的方法比深度学习模型更紧凑、可解释且高效，适用于对可解释性或数据量有限有需求的场景，也可推广至其他生物识别、医学影像、材料科学等领域。算法可在无GPU的CPU系统上高效运行，生成稳健且可解释的特征，非常适合安全等关键领域。

Abstract: Objective - This study presents a biometric identification method based on
topological invariants from 2D iris images, representing iris texture via
formally defined digital homology and evaluating classification performance.
  Methods - Each normalized iris image (48x482 pixels) is divided into grids
(e.g., 6x54 or 3x27). For each subregion, we compute Betti0, Betti1, and their
ratio using a recent algorithm for homology groups in 2D digital images. The
resulting invariants form a feature matrix used with logistic regression, KNN,
and SVM (with PCA and 100 randomized repetitions). A convolutional neural
network (CNN) is trained on raw images for comparison.
  Results - Logistic regression achieved 97.78 +/- 0.82% accuracy,
outperforming CNN (96.44 +/- 1.32%) and other feature-based models. The
topological features showed high accuracy with low variance.
  Conclusion - This is the first use of topological invariants from formal
digital homology for iris recognition. The method offers a compact,
interpretable, and accurate alternative to deep learning, useful when
explainability or limited data is important. Beyond iris recognition, it can
apply to other biometrics, medical imaging, materials science, remote sensing,
and interpretable AI. It runs efficiently on CPU-only systems and produces
robust, explainable features valuable for security-critical domains.

</details>


### [65] [WEC-DG: Multi-Exposure Wavelet Correction Method Guided by Degradation Description](https://arxiv.org/abs/2508.09565)
*Ming Zhao,Pingping Liu,Tongshun Zhang,Zhe Zhang*

Main category: cs.CV

TL;DR: 提出一种基于小波变换和降解引导的多曝光矫正方法WEC-DG，可有效在复杂成像环境下校正光照并恢复图像细节。


<details>
  <summary>Details</summary>
Motivation: 现有多曝光矫正方法在单一曝光和复杂光照环境下，常因难以应对类内差异和降解类型（如模糊等）导致效果不佳。作者希望提升方法在实际多变场景下的适应性和鲁棒性。

Method: 提出波小波变换与降解引导结合的新方法WEC-DG。设计了曝光一致性校准模块（ECAM）引入降解描述符，在流程两端保证曝光一致性；利用小波分离光照与细节，设计曝光与细节重建模块（EDRM），先处理低频（曝光提升），再用高频信息重建细节。

Result: 在多个公开数据集上通过实验验证，提出的方法在曝光校正和细节恢复方面均优于现有主流算法，表现出显著的性能提升。

Conclusion: WEC-DG方法能够有效消除复杂环境下由曝光退化引起的问题，提升细节和整体观感，具有良好的实用性和推广价值。

Abstract: Multi-exposure correction technology is essential for restoring images
affected by insufficient or excessive lighting, enhancing the visual experience
by improving brightness, contrast, and detail richness. However, current
multi-exposure correction methods often encounter challenges in addressing
intra-class variability caused by diverse lighting conditions, shooting
environments, and weather factors, particularly when processing images captured
at a single exposure level. To enhance the adaptability of these models under
complex imaging conditions, this paper proposes a Wavelet-based Exposure
Correction method with Degradation Guidance (WEC-DG). Specifically, we
introduce a degradation descriptor within the Exposure Consistency Alignment
Module (ECAM) at both ends of the processing pipeline to ensure exposure
consistency and achieve final alignment. This mechanism effectively addresses
miscorrected exposure anomalies caused by existing methods' failure to
recognize 'blurred' exposure degradation. Additionally, we investigate the
light-detail decoupling properties of the wavelet transform to design the
Exposure Restoration and Detail Reconstruction Module (EDRM), which processes
low-frequency information related to exposure enhancement before utilizing
high-frequency information as a prior guide for reconstructing spatial domain
details. This serial processing strategy guarantees precise light correction
and enhances detail recovery. Extensive experiments conducted on multiple
public datasets demonstrate that the proposed method outperforms existing
algorithms, achieving significant performance improvements and validating its
effectiveness and practical applicability.

</details>


### [66] [A Chain of Diagnosis Framework for Accurate and Explainable Radiology Report Generation](https://arxiv.org/abs/2508.09566)
*Haibo Jin,Haoxuan Che,Sunan He,Hao Chen*

Main category: cs.CV

TL;DR: 本文提出了一种链式诊断(CoD)框架，用于提升放射学报告生成的临床有效性和可解释性。该方法通过对关键发现进行问答对抽取，实现精确描述异常，并结合诊断和病变定位，强化报告的可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前放射学报告生成（RRG）主要面临两个挑战：临床表现特别是病灶属性描述不够精准，以及生成文本缺乏可解释性，影响放射科医生的信任。

Method: 方法提出了链式诊断（CoD）框架，包括：1）通过诊断性对话生成问答对，提取影像的关键发现；2）用QA诊断结果提示大型语言模型生成报告文本；3）设计了诊断定位模块，将文本与诊断QA对比，提升解释性；4）设计病灶定位模块，在影像中定位异常病灶，便于医生复查。还提出了一种兼容多种标注数据的全监督学习策略，以提升标签利用效率。

Result: 论文构建了包含QA对和病灶框的多标记RRG数据集，开发了病灶位置和严重程度评测工具，并在两个基准数据集上进行大量实验，结果显示CoD在临床描述准确性和可解释性方面均优于领域专家和通用模型。

Conclusion: 该工作显著提升了自动放射学报告生成的临床有效性和可解释性，并为进一步构建可信赖的辅助诊断AI系统提供了有效方案。

Abstract: Despite the progress of radiology report generation (RRG), existing works
face two challenges: 1) The performances in clinical efficacy are
unsatisfactory, especially for lesion attributes description; 2) the generated
text lacks explainability, making it difficult for radiologists to trust the
results. To address the challenges, we focus on a trustworthy RRG model, which
not only generates accurate descriptions of abnormalities, but also provides
basis of its predictions. To this end, we propose a framework named chain of
diagnosis (CoD), which maintains a chain of diagnostic process for clinically
accurate and explainable RRG. It first generates question-answer (QA) pairs via
diagnostic conversation to extract key findings, then prompts a large language
model with QA diagnoses for accurate generation. To enhance explainability, a
diagnosis grounding module is designed to match QA diagnoses and generated
sentences, where the diagnoses act as a reference. Moreover, a lesion grounding
module is designed to locate abnormalities in the image, further improving the
working efficiency of radiologists. To facilitate label-efficient training, we
propose an omni-supervised learning strategy with clinical consistency to
leverage various types of annotations from different datasets. Our efforts lead
to 1) an omni-labeled RRG dataset with QA pairs and lesion boxes; 2) a
evaluation tool for assessing the accuracy of reports in describing lesion
location and severity; 3) extensive experiments to demonstrate the
effectiveness of CoD, where it outperforms both specialist and generalist
models consistently on two RRG benchmarks and shows promising explainability by
accurately grounding generated sentences to QA diagnoses and images.

</details>


### [67] [Dual Recursive Feedback on Generation and Appearance Latents for Pose-Robust Text-to-Image Diffusion](https://arxiv.org/abs/2508.09575)
*Jiwon Kim,Pureum Kim,SeonHwa Kim,Soobin Park,Eunju Cha,Kyong Hwan Jin*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的新型可控文本生成图像（T2I）扩散模型方法——Dual Recursive Feedback（DRF），大幅提升了空间结构和精细控制能力，实验结果显示生成图像质量更高，结构和语义一致性更好。


<details>
  <summary>Details</summary>
Motivation: 现有的可控T2I扩散模型如Ctrl-X和FreeControl虽然实现了空间和外观的控制，但在精确保留空间结构和捕捉物体姿态、场景布局等细粒度条件方面存在不足。作者旨在无须额外训练辅助模块的情况下，提升这些控制能力。

Method: 提出Dual Recursive Feedback（DRF）系统，包括外观反馈和生成反馈，通过递归地调整中间隐变量，使其更好反映用户给定的外观信息和意图。这一双重反馈机制不断指导隐空间，融合结构与外观特征，实现结构外观的高阶融合，如将人类动作迁移到老虎形态上等。

Result: 大量实验表明，该方法在可控T2I任务中能生成高质量、结构和语义一致的图像，显著优于既有方法。

Conclusion: DRF方法无需额外训练即可显著提升可控T2I模型的空间结构和精细外观控制能力，实现高质量、语义连贯、结构一致的图像生成。

Abstract: Recent advancements in controllable text-to-image (T2I) diffusion models,
such as Ctrl-X and FreeControl, have demonstrated robust spatial and appearance
control without requiring auxiliary module training. However, these models
often struggle to accurately preserve spatial structures and fail to capture
fine-grained conditions related to object poses and scene layouts. To address
these challenges, we propose a training-free Dual Recursive Feedback (DRF)
system that properly reflects control conditions in controllable T2I models.
The proposed DRF consists of appearance feedback and generation feedback that
recursively refines the intermediate latents to better reflect the given
appearance information and the user's intent. This dual-update mechanism guides
latent representations toward reliable manifolds, effectively integrating
structural and appearance attributes. Our approach enables fine-grained
generation even between class-invariant structure-appearance fusion, such as
transferring human motion onto a tiger's form. Extensive experiments
demonstrate the efficacy of our method in producing high-quality, semantically
coherent, and structurally consistent image generations. Our source code is
available at https://github.com/jwonkm/DRF.

</details>


### [68] [SHALE: A Scalable Benchmark for Fine-grained Hallucination Evaluation in LVLMs](https://arxiv.org/abs/2508.09584)
*Bei Yan,Zhiyuan Chen,Yuecong Min,Jie Zhang,Jiahao Wang,Xiaozhen Wang,Shiguang Shan*

Main category: cs.CV

TL;DR: 该论文提出了SHALE，一个用于评估大规模视觉-语言模型（LVLMs）幻觉现象（hallucinations）的自动化、可扩展基准。该基准具备细粒度分类，能同时评估faithfulness和factuality两类幻觉，并覆盖多种视觉感知和知识领域。


<details>
  <summary>Details</summary>
Motivation: 现有关于LVLMs幻觉现象的评测存在两个主要问题：一是只做粗粒度（如对象级）的评估，缺少细粒度分析；二是依赖昂贵的人工标注或重复利用公开数据集，导致难以扩展且可能出现数据泄露。本文为解决这两个问题，设计了更高效、自动化、且细致的评测方案。

Method: 作者提出了自动化数据构建流程，能够大规模且可控地生成多样化的评测数据。同时，设计了层次化的幻觉诱发框架，通过扰动输入来模拟真实的噪声场景。综合这些思路，作者建立了SHALE基准，包括超过3万对图像-指令，覆盖faithfulness的12种视觉感知子类型及factuality的6个知识领域，且考虑了干净与噪声场景。

Result: 在20余个主流LVLMs上进行大量实验证明，这些模型存在显著的factuality幻觉问题，并对语义扰动极为敏感。该基准能够细致地区分和量化不同类型的幻觉现象。

Conclusion: SHALE为LVLMs幻觉评测提供了自动化、可扩展、细粒度的基准工具，促进LVLMs更全面、系统的真实性和忠实性研究，有望推动该领域未来发展。

Abstract: Despite rapid advances, Large Vision-Language Models (LVLMs) still suffer
from hallucinations, i.e., generating content inconsistent with input or
established world knowledge, which correspond to faithfulness and factuality
hallucinations, respectively. Prior studies primarily evaluate faithfulness
hallucination at a coarse level (e.g., object-level) and lack fine-grained
analysis. Additionally, existing benchmarks rely on costly manual curation or
reused public datasets, raising concerns about scalability and data leakage. To
address these limitations, we propose an automated data construction pipeline
that produces scalable, controllable, and diverse evaluation data. We also
design a hierarchical hallucination induction framework with input
perturbations to simulate realistic noisy scenarios. Integrating these designs,
we construct SHALE, a Scalable HALlucination Evaluation benchmark designed to
assess both faithfulness and factuality hallucinations via a fine-grained
hallucination categorization scheme. SHALE comprises over 30K image-instruction
pairs spanning 12 representative visual perception aspects for faithfulness and
6 knowledge domains for factuality, considering both clean and noisy scenarios.
Extensive experiments on over 20 mainstream LVLMs reveal significant factuality
hallucinations and high sensitivity to semantic perturbations.

</details>


### [69] [Offline Auto Labeling: BAAS](https://arxiv.org/abs/2508.09585)
*Stefan Haag,Bharanidhar Duraisamy,Felix Govaers,Wolfgang Koch,Martin Fritzsche,Juergen Dickmann*

Main category: cs.CV

TL;DR: 本文提出了一种基于贝叶斯扩展目标跟踪与融合的自动标注框架BAAS，用于自动驾驶雷达数据的准确标注，通过实验验证，其能有效提升跟踪与标注性能。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶领域，雷达目标的高质量标注对感知算法的训练和验证极为关键，但人工标注成本高、效率低。因此，亟需一种能够自动化、精确地在检测层面产生带有形状估计的标注标签的框架。

Method: 提出了一个新的BAAS框架，利用基于贝叶斯的扩展目标跟踪、平滑及多源信息融合方法，在不同监督级别下，为雷达检测数据生成高可信度的对象轨迹与形状估计的标注标签。同时支持手工标注与自动标注的闭环优化，并可独立评估各处理模块。

Result: 在复杂的城市道路真实场景下，通过对跟踪表现和标注误差的评估，证明了该方法能有效追踪多类别动态目标并生成准确的检测级别标注。

Conclusion: 融合贝叶斯跟踪和多模块闭环提高了自动标注的准确性和性能，BAAS框架能应用于多种目标类型，实现标注效率与质量的同步提升。

Abstract: This paper introduces BAAS, a new Extended Object Tracking (EOT) and
fusion-based label annotation framework for radar detections in autonomous
driving. Our framework utilizes Bayesian-based tracking, smoothing and
eventually fusion methods to provide veritable and precise object trajectories
along with shape estimation to provide annotation labels on the detection level
under various supervision levels. Simultaneously, the framework provides
evaluation of tracking performance and label annotation. If manually labeled
data is available, each processing module can be analyzed independently or
combined with other modules to enable closed-loop continuous improvements. The
framework performance is evaluated in a challenging urban real-world scenario
in terms of tracking performance and the label annotation errors. We
demonstrate the functionality of the proposed approach for varying dynamic
objects and class types

</details>


### [70] [Hierarchical Brain Structure Modeling for Predicting Genotype of Glioma](https://arxiv.org/abs/2508.09593)
*Haotian Tang,Jianwei Chen,Xinrui Tang,Yunjia Wu,Zhengyang Miao,Chao Li*

Main category: cs.CV

TL;DR: 该论文提出了Hi-SMGNN，一种集成结构和形态连接组的分层框架，利用层次化和多模态机制提升IDH突变状态的预测准确性。实验证明其方法优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前胶质瘤IDH突变状态预测主要依赖功能MRI，但面临数据稀缺和噪声等问题，而结构和形态连接组虽有潜力，却未充分利用脑层级结构和多尺度信息。因此需要创新方法提升预测能力。

Method: 提出Hi-SMGNN分层模型，从区域到模块融合结构与形态连接组；设计多模态交互模块（西安网络+跨模态注意力）、多尺度特征融合机制减少冗余，并采用个性化模块划分以增强特异性和可解释性。

Result: 在UCSF-PDGM数据集上，实验表明Hi-SMGNN在IDH突变预测的准确性、鲁棒性等方面均优于基线和最新方法。

Conclusion: Hi-SMGNN有效融合结构与形态连接组信息，充分挖掘脑组织多层次、多尺度特征，提升了IDH突变预测能力，对无创生物标志物研究具有促进作用。

Abstract: Isocitrate DeHydrogenase (IDH) mutation status is a crucial biomarker for
glioma prognosis. However, current prediction methods are limited by the low
availability and noise of functional MRI. Structural and morphological
connectomes offer a non-invasive alternative, yet existing approaches often
ignore the brain's hierarchical organisation and multiscale interactions. To
address this, we propose Hi-SMGNN, a hierarchical framework that integrates
structural and morphological connectomes from regional to modular levels. It
features a multimodal interaction module with a Siamese network and cross-modal
attention, a multiscale feature fusion mechanism for reducing redundancy, and a
personalised modular partitioning strategy to enhance individual specificity
and interpretability. Experiments on the UCSF-PDGM dataset demonstrate that
Hi-SMGNN outperforms baseline and state-of-the-art models, showing improved
robustness and effectiveness in IDH mutation prediction.

</details>


### [71] [SVG-Head: Hybrid Surface-Volumetric Gaussians for High-Fidelity Head Reconstruction and Real-Time Editing](https://arxiv.org/abs/2508.09597)
*Heyi Sun,Cong Wang,Tian-Xing Xu,Jingwei Huang,Di Kang,Chunchao Guo,Song-Hai Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为SVG-Head的混合表达方法，通过融合显式几何建模和可编辑纹理，实现了高保真、可实时编辑的头像重建。


<details>
  <summary>Details</summary>
Motivation: 尽管现有方法在头像渲染和动画方面已有突破，但头像外观的实时编辑仍具挑战，主要原因在于隐式表示方法使得几何与外观特征高度耦合，难以独立编辑。

Method: SVG-Head使用基于FLAME网格的3D高斯体显式建模几何，并结合解耦的纹理图像表示全局外观。其中，表面高斯负责可编辑纹理，体积高斯主要提升非朗伯特区域（如嘴唇和头发）的重建质量；并提出了基于FLAME网格和UV坐标的高斯UV映射方法，实现高效渲染，还采用分层优化策略提升效果。

Result: 在NeRSemble数据集实验中，SVG-Head不仅实现了高保真渲染，还首次为高斯头像获得了显式纹理图像，并支持实时外观编辑。

Conclusion: SVG-Head为三维头像的高质量、可编辑建模提供了新方案，兼具重建效果和编辑灵活性，为相关应用带来技术突破。

Abstract: Creating high-fidelity and editable head avatars is a pivotal challenge in
computer vision and graphics, boosting many AR/VR applications. While recent
advancements have achieved photorealistic renderings and plausible animation,
head editing, especially real-time appearance editing, remains challenging due
to the implicit representation and entangled modeling of the geometry and
global appearance. To address this, we propose Surface-Volumetric Gaussian Head
Avatar (SVG-Head), a novel hybrid representation that explicitly models the
geometry with 3D Gaussians bound on a FLAME mesh and leverages disentangled
texture images to capture the global appearance. Technically, it contains two
types of Gaussians, in which surface Gaussians explicitly model the appearance
of head avatars using learnable texture images, facilitating real-time texture
editing, while volumetric Gaussians enhance the reconstruction quality of
non-Lambertian regions (e.g., lips and hair). To model the correspondence
between 3D world and texture space, we provide a mesh-aware Gaussian UV mapping
method, which leverages UV coordinates given by the FLAME mesh to obtain sharp
texture images and real-time rendering speed. A hierarchical optimization
strategy is further designed to pursue the optimal performance in both
reconstruction quality and editing flexibility. Experiments on the NeRSemble
dataset show that SVG-Head not only generates high-fidelity rendering results,
but also is the first method to obtain explicit texture images for Gaussian
head avatars and support real-time appearance editing.

</details>


### [72] [Images Speak Louder Than Scores: Failure Mode Escape for Enhancing Generative Quality](https://arxiv.org/abs/2508.09598)
*Jie Shao,Ke Zhu,Minghao Fu,Guo-hua Wang,Jianxin Wu*

Main category: cs.CV

TL;DR: 该论文提出了FaME方法，用于提升扩散模型生成图像的感知质量，不增加训练开销且推断高效。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型虽然FID等指标表现优异，但实际可能生成变形或低质量图片。FID仅衡量全局分布，而忽略单个样本的观感质量。此外，常用的CFG方法虽提升分数，但也带来分布偏移和视觉伪影。当前缺乏无需重新训练即可提升图像质量的方法。

Method: FaME是一种免训练、推理高效的新方法。它借助图像质量评估模型鉴别低质量生成图片，并记录其采样轨迹，把这些失败模式作为负引导，使未来的生成远离劣质区域。

Result: 在ImageNet实验中，FaME能在不牺牲FID的前提下，显著提升视觉质量。

Conclusion: FaME有效提升扩散模型生成图像的感知质量，有望推广到文本到图像等更广泛的生成任务。

Abstract: Diffusion models have achieved remarkable progress in class-to-image
generation. However, we observe that despite impressive FID scores,
state-of-the-art models often generate distorted or low-quality images,
especially in certain classes. This gap arises because FID evaluates global
distribution alignment, while ignoring the perceptual quality of individual
samples. We further examine the role of CFG, a common technique used to enhance
generation quality. While effective in improving metrics and suppressing
outliers, CFG can introduce distribution shift and visual artifacts due to its
misalignment with both training objectives and user expectations. In this work,
we propose FaME, a training-free and inference-efficient method for improving
perceptual quality. FaME uses an image quality assessment model to identify
low-quality generations and stores their sampling trajectories. These failure
modes are then used as negative guidance to steer future sampling away from
poor-quality regions. Experiments on ImageNet demonstrate that FaME brings
consistent improvements in visual quality without compromising FID. FaME also
shows the potential to be extended to improve text-to-image generation.

</details>


### [73] [BridgeTA: Bridging the Representation Gap in Knowledge Distillation via Teacher Assistant for Bird's Eye View Map Segmentation](https://arxiv.org/abs/2508.09599)
*Beomjun Kim,Suhan Woo,Sejong Heo,Euntai Kim*

Main category: cs.CV

TL;DR: 提出了一种名为BridgeTA的知识蒸馏框架，通过引入轻量级的Teacher Assistant（TA）网络，有效提升了Camera-only BEV分割模型的表现，缩小了其与更昂贵的多传感器模型的性能差距。


<details>
  <summary>Details</summary>
Motivation: 相较于成本更高的LiDAR-Camera融合方法，仅用摄像头的Bird's-Eye View（BEV）分割在自动驾驶中更具经济性，但精度略低。现有知识蒸馏方法虽能缩小差距，但常常让学生模型变大，导致推理成本上升。因此，亟需一种在不增加学生模型复杂度下提升Camera-only模型性能的方法。

Method: 提出BridgeTA框架：在教师（LC融合模型）和学生（Camera-only模型）之间引入一个轻量的TA网络，将二者的BEV表征进行整合，构造共享的中间表征空间。还基于杨氏不等式提出新的蒸馏损失，将原教师-学生直接蒸馏路径分解为教师-TA和TA-学生两条路径，以提高优化稳定性和知识迁移效果。

Result: 在nuScenes数据集上，Camera-only学生模型通过BridgeTA蒸馏相比原始基线mIoU提升4.2%，显著高于现有其他KD方法（提升高出45%）。

Conclusion: BridgeTA能在不增加学生模型推理成本前提下，大幅提升Camera-only BEV分割性能，优于当前主流知识蒸馏方法，在经济性与性能之间取得了良好的平衡，适合实际自动驾驶部署。

Abstract: Bird's-Eye-View (BEV) map segmentation is one of the most important and
challenging tasks in autonomous driving. Camera-only approaches have drawn
attention as cost-effective alternatives to LiDAR, but they still fall behind
LiDAR-Camera (LC) fusion-based methods. Knowledge Distillation (KD) has been
explored to narrow this gap, but existing methods mainly enlarge the student
model by mimicking the teacher's architecture, leading to higher inference
cost. To address this issue, we introduce BridgeTA, a cost-effective
distillation framework to bridge the representation gap between LC fusion and
Camera-only models through a Teacher Assistant (TA) network while keeping the
student's architecture and inference cost unchanged. A lightweight TA network
combines the BEV representations of the teacher and student, creating a shared
latent space that serves as an intermediate representation. To ground the
framework theoretically, we derive a distillation loss using Young's
Inequality, which decomposes the direct teacher-student distillation path into
teacher-TA and TA-student dual paths, stabilizing optimization and
strengthening knowledge transfer. Extensive experiments on the challenging
nuScenes dataset demonstrate the effectiveness of our method, achieving an
improvement of 4.2% mIoU over the Camera-only baseline, up to 45% higher than
the improvement of other state-of-the-art KD methods.

</details>


### [74] [MInDI-3D: Iterative Deep Learning in 3D for Sparse-view Cone Beam Computed Tomography](https://arxiv.org/abs/2508.09616)
*Daniel Barco,Marc Stadelmann,Martin Oswald,Ivo Herzig,Lukas Lichtensteiger,Pascal Paysan,Igor Peterlik,Michal Walczak,Bjoern Menze,Frank-Peter Schilling*

Main category: cs.CV

TL;DR: 本文提出了一种名为MInDI-3D的三维条件扩散模型，用于去除真实世界中稀疏视角锥束CT（CBCT）的伪影，显著降低放射剂量，并在各项评价中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有CBCT减少辐射剂量的方法往往导致图像伪影、质量下降，难以满足临床需求。本文希望利用更先进的三维扩散模型，提升稀疏采样CBCT的复原质量，实现更低辐射剂量和临床可用性的统一。

Method: 提出了MInDI-3D，将迭代去噪扩散模型的“InDI”概念从二维扩展到三维，实现对稀疏视角CBCT体数据的直接复原。同时，生成了大规模的虚拟CBCT训练数据集，并从多维度进行模型评测，包括临床医生主观评价。

Result: MInDI-3D在16,182例伪CBCT测试集上获得了显著性能提升（PSNR提升12.96 dB），达到8倍辐射降低的效果，并能随着训练数据量增加进一步提升。在16例癌症患者的实际CBCT中，表现与3D U-Net持平，能够泛化到新的设备和几何结构，临床医生认可其定位用途和肿瘤边界保留能力。

Conclusion: MInDI-3D为降低CBCT辐射剂量与提升图像质量提供了高效方案，并已获得良好的临床评测结果，显示出较强的实用前景和通用性。

Abstract: We present MInDI-3D (Medical Inversion by Direct Iteration in 3D), the first
3D conditional diffusion-based model for real-world sparse-view Cone Beam
Computed Tomography (CBCT) artefact removal, aiming to reduce imaging radiation
exposure. A key contribution is extending the "InDI" concept from 2D to a full
3D volumetric approach for medical images, implementing an iterative denoising
process that refines the CBCT volume directly from sparse-view input. A further
contribution is the generation of a large pseudo-CBCT dataset (16,182) from
chest CT volumes of the CT-RATE public dataset to robustly train MInDI-3D. We
performed a comprehensive evaluation, including quantitative metrics,
scalability analysis, generalisation tests, and a clinical assessment by 11
clinicians. Our results show MInDI-3D's effectiveness, achieving a 12.96 (6.10)
dB PSNR gain over uncorrected scans with only 50 projections on the CT-RATE
pseudo-CBCT (independent real-world) test set and enabling an 8x reduction in
imaging radiation exposure. We demonstrate its scalability by showing that
performance improves with more training data. Importantly, MInDI-3D matches the
performance of a 3D U-Net on real-world scans from 16 cancer patients across
distortion and task-based metrics. It also generalises to new CBCT scanner
geometries. Clinicians rated our model as sufficient for patient positioning
across all anatomical sites and found it preserved lung tumour boundaries well.

</details>


### [75] [Semantic-aware DropSplat: Adaptive Pruning of Redundant Gaussians for 3D Aerial-View Segmentation](https://arxiv.org/abs/2508.09626)
*Xu Tang,Junan Jia,Yijing Wang,Jingjing Ma,Xiangrong Zhang*

Main category: cs.CV

TL;DR: 本文提出了SAD-Splat方法，通过引入高斯点去除模块与高置信伪标签生成机制，提升3D俯视场景语义分割的准确性和模型紧凑性；并发布了新的挑战性数据集3D-AS，实验验证新方法在精度和效率上的平衡表现。


<details>
  <summary>Details</summary>
Motivation: 传统3D航拍视角的场景语义分割方法难以克服因尺度变化和结构遮挡带来的语义歧义，导致分割精度和一致性受限。

Method: 作者提出SAD-Splat方法。首先引入高斯点去除（Gaussian point drop）模块，结合语义置信估计与Hard Concrete分布的可学习稀疏机制，删除冗余和歧义的高斯点。其次，利用2D基础模型设计高置信伪标签生成机制，在标注稀缺环境下增强监督，提升分割表现。并提出全新挑战性数据集3D-AS。

Result: 实验证明SAD-Splat在不同场景和稀疏标注情况下，实现了良好的分割精度和表达紧凑性，兼顾高效性和可扩展性。

Conclusion: SAD-Splat为3D航拍场景的智能理解提供了一种高效、精确且紧凑的解决方案，并通过数据集推动领域研究进步。

Abstract: In the task of 3D Aerial-view Scene Semantic Segmentation (3D-AVS-SS),
traditional methods struggle to address semantic ambiguity caused by scale
variations and structural occlusions in aerial images. This limits their
segmentation accuracy and consistency. To tackle these challenges, we propose a
novel 3D-AVS-SS approach named SAD-Splat. Our method introduces a Gaussian
point drop module, which integrates semantic confidence estimation with a
learnable sparsity mechanism based on the Hard Concrete distribution. This
module effectively eliminates redundant and semantically ambiguous Gaussian
points, enhancing both segmentation performance and representation compactness.
Furthermore, SAD-Splat incorporates a high-confidence pseudo-label generation
pipeline. It leverages 2D foundation models to enhance supervision when
ground-truth labels are limited, thereby further improving segmentation
accuracy. To advance research in this domain, we introduce a challenging
benchmark dataset: 3D Aerial Semantic (3D-AS), which encompasses diverse
real-world aerial scenes with sparse annotations. Experimental results
demonstrate that SAD-Splat achieves an excellent balance between segmentation
accuracy and representation compactness. It offers an efficient and scalable
solution for 3D aerial scene understanding.

</details>


### [76] [Enhancing Monocular 3D Hand Reconstruction with Learned Texture Priors](https://arxiv.org/abs/2508.09629)
*Giorgos Karvounas,Nikolaos Kyriazis,Iason Oikonomidis,Georgios Pavlakos,Antonis A. Argyros*

Main category: cs.CV

TL;DR: 本文提出了一种将纹理用于单目3D手部重建的新方法，不只关注视觉真实感，而是将纹理作为密集监督信号来提升姿态与形状估计精度。通过加入轻量级纹理模块，引入像素级外观对齐损失，显著提升了重建准确性与真实度。


<details>
  <summary>Details</summary>
Motivation: 现有单目3D手部重建方法对纹理利用不足，主要作为最后渲染用，而没有有效作为监督提升重建质量。作者注意到即便在高性能模型中，手部几何与图像外观的重叠往往仍有误差，表明纹理对齐信号未被充分利用。

Method: 作者提出一个轻量级纹理模块，将每像素观测嵌入UV纹理空间，并计算预测与观测手部外观之间的密集对齐损失。假设已经有可微分渲染流程和可将图片映射到已知拓扑3D手部网格的模型，通过纹理后向投射实现像素级监督。该模块可直接集成到现有重建管线中。为体现纹理监督价值，特意将其嵌入HaMeR（一个Transformer架构的高性能3D手部姿态估计模型）进行对比分析。

Result: 加入纹理对齐模块后，在手部姿态与形状估计的准确性及重建的真实感方面都有明显提升，优于只基于几何的模型。实验显示纹理引导的监督对提升手部重建非常有效。

Conclusion: 纹理不仅仅用于后期渲染，也能作为关键的密集监督信号，提高单目3D手部重建的准确性与真实度。提出的轻量纹理模块易于集成，能为现有模型带来明显性能提升。

Abstract: We revisit the role of texture in monocular 3D hand reconstruction, not as an
afterthought for photorealism, but as a dense, spatially grounded cue that can
actively support pose and shape estimation. Our observation is simple: even in
high-performing models, the overlay between predicted hand geometry and image
appearance is often imperfect, suggesting that texture alignment may be an
underused supervisory signal. We propose a lightweight texture module that
embeds per-pixel observations into UV texture space and enables a novel dense
alignment loss between predicted and observed hand appearances. Our approach
assumes access to a differentiable rendering pipeline and a model that maps
images to 3D hand meshes with known topology, allowing us to back-project a
textured hand onto the image and perform pixel-based alignment. The module is
self-contained and easily pluggable into existing reconstruction pipelines. To
isolate and highlight the value of texture-guided supervision, we augment
HaMeR, a high-performing yet unadorned transformer architecture for 3D hand
pose estimation. The resulting system improves both accuracy and realism,
demonstrating the value of appearance-guided alignment in hand reconstruction.

</details>


### [77] [Preacher: Paper-to-Video Agentic System](https://arxiv.org/abs/2508.09632)
*Jingwei Liu,Ling Yang,Hao Luo,Fan Wang Hongyan Li,Mengdi Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Preacher的系统，能够将学术论文自动转化为结构化的视频摘要，有效提升信息传递效率和表现力。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在长文本理解、视频时长、风格多样性和领域知识表达等方面存在局限，难以满足学术论文复杂内容可视化的需求，因此亟需一种能够高效、智能地将论文内容组织并转化为视频的系统。

Method: Preacher系统采用自顶向下的方式，先对论文进行拆解、总结与重构，再自底向上合成多样的视频片段，最终形成连贯的视频摘要。核心创新在于引入了Progressive Chain of Thought（P-CoT）进行细粒度、逐步规划，并通过定义关键场景实现跨模态对齐。

Result: Preacher能在五个不同领域生成高质量的视频摘要，在内容理解、重点提炼和视频呈现等方面超过现有视频生成模型。

Conclusion: Preacher为论文可视化提供了全新解决方案，突破了现有模型的瓶颈，为后续研究带来了广泛应用前景。源码公开，便于社区进一步验证和发展。

Abstract: The paper-to-video task converts a research paper into a structured video
abstract, distilling key concepts, methods, and conclusions into an accessible,
well-organized format. While state-of-the-art video generation models
demonstrate potential, they are constrained by limited context windows, rigid
video duration constraints, limited stylistic diversity, and an inability to
represent domain-specific knowledge. To address these limitations, we introduce
Preacher, the first paper-to-video agentic system. Preacher employs a top-down
approach to decompose, summarize, and reformulate the paper, followed by
bottom-up video generation, synthesizing diverse video segments into a coherent
abstract. To align cross-modal representations, we define key scenes and
introduce a Progressive Chain of Thought (P-CoT) for granular, iterative
planning. Preacher successfully generates high-quality video abstracts across
five research fields, demonstrating expertise beyond current video generation
models. Code will be released at: https://github.com/GenVerse/Paper2Video

</details>


### [78] [Multi-Contrast Fusion Module: An attention mechanism integrating multi-contrast features for fetal torso plane classification](https://arxiv.org/abs/2508.09644)
*Shengjun Zhu,Siyu Liu,Runqing Xiong,Liping Zheng,Duo Ma,Rongshang Chen,Jiaxin Cai*

Main category: cs.CV

TL;DR: 本论文提出了一种多对比度融合模块（MCFM），提升了胎儿超声图像中躯干平面识别的准确性，对临床产检有重要意义。


<details>
  <summary>Details</summary>
Motivation: 超声检查对孕期胎儿结构评估至关重要，但由于超声图像对比度低、细节不清，妨碍了精细的解剖结构识别，因此亟需新的方法提高识别性能。

Method: 作者提出在神经网络底层集成多对比度融合模块（MCFM），对原始超声图像进行多对比度特征处理，通过注意力机制分配权重，突出不同对比条件下的显著特征，同时保持参数开销极小。

Result: 在胎儿躯干平面超声图像数据集上验证，MCFM显著提升了识别性能，并且模型复杂度增加很小。多对比注意力使模型对细微解剖结构的捕捉能力更强，提高了分类准确率和临床可靠性。

Conclusion: 所提方法提升了超声图像胎儿躯干平面识别的准确性和一致性，有助于临床更精确的诊断，有较强的实际应用前景。

Abstract: Purpose: Prenatal ultrasound is a key tool in evaluating fetal structural
development and detecting abnormalities, contributing to reduced perinatal
complications and improved neonatal survival. Accurate identification of
standard fetal torso planes is essential for reliable assessment and
personalized prenatal care. However, limitations such as low contrast and
unclear texture details in ultrasound imaging pose significant challenges for
fine-grained anatomical recognition. Methods: We propose a novel Multi-Contrast
Fusion Module (MCFM) to enhance the model's ability to extract detailed
information from ultrasound images. MCFM operates exclusively on the lower
layers of the neural network, directly processing raw ultrasound data. By
assigning attention weights to image representations under different contrast
conditions, the module enhances feature modeling while explicitly maintaining
minimal parameter overhead. Results: The proposed MCFM was evaluated on a
curated dataset of fetal torso plane ultrasound images. Experimental results
demonstrate that MCFM substantially improves recognition performance, with a
minimal increase in model complexity. The integration of multi-contrast
attention enables the model to better capture subtle anatomical structures,
contributing to higher classification accuracy and clinical reliability.
Conclusions: Our method provides an effective solution for improving fetal
torso plane recognition in ultrasound imaging. By enhancing feature
representation through multi-contrast fusion, the proposed approach supports
clinicians in achieving more accurate and consistent diagnoses, demonstrating
strong potential for clinical adoption in prenatal screening. The codes are
available at https://github.com/sysll/MCFM.

</details>


### [79] [Multi-Sequence Parotid Gland Lesion Segmentation via Expert Text-Guided Segment Anything Model](https://arxiv.org/abs/2508.09645)
*Zhongyuan Wu,Chuan-Xian Ren,Yu Wang,Xiaohua Ban,Jianning Xiao,Xiaohui Duan*

Main category: cs.CV

TL;DR: 本文提出了PG-SAM模型，一种结合专家诊断文本指导的多模态分割方法，在腮腺病灶分割中取得了最佳表现。


<details>
  <summary>Details</summary>
Motivation: 当前腮腺病灶分割因病灶大小变化大、边界复杂而难以准确实现，而现有的Segment Anything Model（SAM）虽然提升了医学图像分割效果，但对精准的提示（prompt）依赖过高，且缺乏医学专家领域知识的引入，影响其实际应用。

Method: 提出PG-SAM模型，通过自动提取专家诊断报告中的领域知识，生成分割所需的提示信息；引入跨序列注意力模块，融合多模态互补信息；将多序列图像特征和生成的提示信息输入解码器，得到分割结果。

Result: 在三个独立临床中心的数据上，PG-SAM模型实现了腮腺病灶分割领域的最优性能。实验结果验证了模型的临床适用性及专家诊断文本对实际医学图像分割的促进作用。

Conclusion: PG-SAM有效结合了专家文本知识与多模态信息，显著提升了腮腺病灶分割的准确性，在真实临床场景下表现出很高的实用价值。

Abstract: Parotid gland lesion segmentation is essential for the treatment of parotid
gland diseases. However, due to the variable size and complex lesion
boundaries, accurate parotid gland lesion segmentation remains challenging.
Recently, the Segment Anything Model (SAM) fine-tuning has shown remarkable
performance in the field of medical image segmentation. Nevertheless, SAM's
interaction segmentation model relies heavily on precise lesion prompts
(points, boxes, masks, etc.), which are very difficult to obtain in real-world
applications. Besides, current medical image segmentation methods are
automatically generated, ignoring the domain knowledge of medical experts when
performing segmentation. To address these limitations, we propose the parotid
gland segment anything model (PG-SAM), an expert diagnosis text-guided SAM
incorporating expert domain knowledge for cross-sequence parotid gland lesion
segmentation. Specifically, we first propose an expert diagnosis report guided
prompt generation module that can automatically generate prompt information
containing the prior domain knowledge to guide the subsequent lesion
segmentation process. Then, we introduce a cross-sequence attention module,
which integrates the complementary information of different modalities to
enhance the segmentation effect. Finally, the multi-sequence image features and
generated prompts are feed into the decoder to get segmentation result.
Experimental results demonstrate that PG-SAM achieves state-of-the-art
performance in parotid gland lesion segmentation across three independent
clinical centers, validating its clinical applicability and the effectiveness
of diagnostic text for enhancing image segmentation in real-world clinical
settings.

</details>


### [80] [The Brain Resection Multimodal Image Registration (ReMIND2Reg) 2025 Challenge](https://arxiv.org/abs/2508.09649)
*Reuben Dorent,Laura Rigolo,Colin P. Galvin,Junyu Chen,Mattias P. Heinrich,Aaron Carass,Olivier Colliot,Demian Wassermann,Alexandra Golby,Tina Kapur,William Wells*

Main category: cs.CV

TL;DR: 本文介绍了ReMIND2Reg 2025挑战赛，该挑战提供了目前最大规模的术中超声(iUS)与术前MRI配准公开基准，旨在推动神经外科术中图像引导技术的发展。


<details>
  <summary>Details</summary>
Motivation: 由于大脑移位，基于术前MRI的神经导航系统在手术过程中精度下降。将术后超声与术前MRI对齐有助于恢复空间精度，但受到解剖结构变化和多模态图像强度差异的影响，配准非常困难。急需相关基准和评测体系推动算法发展。

Method: 本挑战基于ReMIND数据集，提供配对的三维ceT1 MRI、T2 MRI与术后iUS体积数据，共有99个训练、5个验证和10个测试病例，无人工标注用于训练，验证与测试通过人工标注的解剖标志点评估。评测指标包括目标配准误差(TRE)、最差情况配准健壮性(TRE30)及算法运行时间。

Result: 建立了标准化的评测框架，集成多模态、3D配准及严苛评估，丰富了数据量与病例种类，大幅提升了算法测试的广度和临床相关性。

Conclusion: ReMIND2Reg通过提供公开大数据、标准指标与公平评测平台，有助于推动更强健、通用、可临床部署的多模态医学图像配准算法在神经外科手术中的发展与落地。

Abstract: Accurate intraoperative image guidance is critical for achieving maximal safe
resection in brain tumor surgery, yet neuronavigation systems based on
preoperative MRI lose accuracy during the procedure due to brain shift.
Aligning post-resection intraoperative ultrasound (iUS) with preoperative MRI
can restore spatial accuracy by estimating brain shift deformations, but it
remains a challenging problem given the large anatomical and topological
changes and substantial modality intensity gap. The ReMIND2Reg 2025 Challenge
provides the largest public benchmark for this task, built upon the ReMIND
dataset. It offers 99 training cases, 5 validation cases, and 10 private test
cases comprising paired 3D ceT1 MRI, T2 MRI, and post-resection 3D iUS volumes.
Data are provided without annotations for training, while validation and test
performance are evaluated on manually annotated anatomical landmarks. Metrics
include target registration error (TRE), robustness to worst-case landmark
misalignment (TRE30), and runtime. By establishing a standardized evaluation
framework for this clinically critical and technically complex problem,
ReMIND2Reg aims to accelerate the development of robust, generalizable, and
clinically deployable multimodal registration algorithms for image-guided
neurosurgery.

</details>


### [81] [TOTNet: Occlusion-Aware Temporal Tracking for Robust Ball Detection in Sports Videos](https://arxiv.org/abs/2508.09650)
*Hao Xu,Arbind Agrahari Baniya,Sam Wells,Mohamed Reda Bouadjenek,Richard Dazely,Sunil Aryal*

Main category: cs.CV

TL;DR: 本文提出了TOTNet，一种用于在严重遮挡情况下进行球体跟踪的深度学习网络，在多个体育视频数据集上表现显著优于现有方法，尤其提升了遮挡帧的跟踪准确率。


<details>
  <summary>Details</summary>
Motivation: 球在体育视频中经常被遮挡，导致诸如事件检测与裁判辅助等任务变得困难，因此需要更鲁棒的跟踪方法应对遮挡问题。

Method: TOTNet结合了3D卷积网络、基于可见性的加权损失及遮挡增强策略来提升对被部分或完全遮挡的球体的跟踪能力，同时引入新的包含丰富遮挡案例的残奥乒乓球数据集（TTA）用于训练和评估。

Result: TOTNet在网球、羽毛球和乒乓球等四大公开数据集上表现超越现有最先进方法，RMSE从37.30降至7.19，遮挡帧准确率从0.63提高至0.80。

Conclusion: TOTNet为快速运动场景下的遮挡鲁棒球体跟踪提供了有效方法，适用于实际体育分析应用，尤其在残奥会高水平比赛数据集上验证了其实用性。

Abstract: Robust ball tracking under occlusion remains a key challenge in sports video
analysis, affecting tasks like event detection and officiating. We present
TOTNet, a Temporal Occlusion Tracking Network that leverages 3D convolutions,
visibility-weighted loss, and occlusion augmentation to improve performance
under partial and full occlusions. Developed in collaboration with Paralympics
Australia, TOTNet is designed for real-world sports analytics. We introduce
TTA, a new occlusion-rich table tennis dataset collected from
professional-level Paralympic matches, comprising 9,159 samples with 1,996
occlusion cases. Evaluated on four datasets across tennis, badminton, and table
tennis, TOTNet significantly outperforms prior state-of-the-art methods,
reducing RMSE from 37.30 to 7.19 and improving accuracy on fully occluded
frames from 0.63 to 0.80. These results demonstrate TOTNets effectiveness for
offline sports analytics in fast-paced scenarios. Code and data
access:\href{https://github.com/AugustRushG/TOTNet}{AugustRushG/TOTNet}.

</details>


### [82] [Noise-adapted Neural Operator for Robust Non-Line-of-Sight Imaging](https://arxiv.org/abs/2508.09655)
*Lianfang Wang,Kuilin Qin,Xueying Liu,Huibin Chang,Yong Wang,Yuping Duan*

Main category: cs.CV

TL;DR: 本文提出了一种针对3D重建的大规模线性逆问题框架，结合噪声自适应估计和参数化神经算子，实现了高效且鲁棒的NLOS（非视线）成像。


<details>
  <summary>Details</summary>
Motivation: NLOS成像面临信号弱且易受噪声干扰的问题。为实现更准确、鲁棒的重建，需要能适应噪声并有效利用物理过程的新方法。

Method: 方法包括：1）噪声估计模块用于自适应评估瞬态数据的噪声水平；2）提出参数化神经算子来模拟逆映射，实现端到端重建；3）基于算法展开的深度网络，实现模块可解释性和对不同噪声的自适应调整；4）融合全局与局部时空特征，提高结构与细节信息的重建精度。

Result: 通过大量模拟和真实数据验证，方法在快速扫描和稀疏照明点情况下依然展现出较强的重建精度和鲁棒性。

Conclusion: 该方法能在复杂场景下实现高效、鲁棒的NLOS成像，有望推广到实际应用，为相关领域提供了强有力的技术支撑。

Abstract: Computational imaging, especially non-line-of-sight (NLOS) imaging, the
extraction of information from obscured or hidden scenes is achieved through
the utilization of indirect light signals resulting from multiple reflections
or scattering. The inherently weak nature of these signals, coupled with their
susceptibility to noise, necessitates the integration of physical processes to
ensure accurate reconstruction. This paper presents a parameterized inverse
problem framework tailored for large-scale linear problems in 3D imaging
reconstruction. Initially, a noise estimation module is employed to adaptively
assess the noise levels present in transient data. Subsequently, a
parameterized neural operator is developed to approximate the inverse mapping,
facilitating end-to-end rapid image reconstruction. Our 3D image reconstruction
framework, grounded in operator learning, is constructed through deep algorithm
unfolding, which not only provides commendable model interpretability but also
enables dynamic adaptation to varying noise levels in the acquired data,
thereby ensuring consistently robust and accurate reconstruction outcomes.
Furthermore, we introduce a novel method for the fusion of global and local
spatiotemporal data features. By integrating structural and detailed
information, this method significantly enhances both accuracy and robustness.
Comprehensive numerical experiments conducted on both simulated and real
datasets substantiate the efficacy of the proposed method. It demonstrates
remarkable performance with fast scanning data and sparse illumination point
data, offering a viable solution for NLOS imaging in complex scenarios.

</details>


### [83] [NegFaceDiff: The Power of Negative Context in Identity-Conditioned Diffusion for Synthetic Face Generation](https://arxiv.org/abs/2508.09661)
*Eduarda Caldeira,Naser Damer,Fadi Boutros*

Main category: cs.CV

TL;DR: 文章提出了一种新的人脸合成方法，在生成训练数据时引入了负条件，显著提升了身份区分度和一致性，有效改善了基于合成数据的人脸识别性能。


<details>
  <summary>Details</summary>
Motivation: 使用真实人脸数据存在隐私和伦理问题，合成数据逐步成为替代方案。现有基于扩散模型的人脸图像生成虽能保持身份一致性，但因缺乏有效机制区分不同身份，导致合成数据存在身份重叠，从而影响人脸识别训练效果。

Method: 提出了NegFaceDiff方法，在传统身份条件扩散模型生成流程中引入负条件（negative conditions），具体做法是在采样时强化远离非目标身份的人脸特征，既保证同一身份内部一致性，又增强不同身份间的区分能力。

Result: 引入NegFaceDiff后，合成数据的身份区分度（用Fisher判别比FDR衡量）从2.427提升到5.687。在多个基准测试中，用NegFaceDiff数据集训练的人脸识别模型，性能明显优于未引入负条件的数据所训练的模型。

Conclusion: NegFaceDiff极大提升了基于扩散模型的人脸合成数据的身份一致性和分离性，显著改善了合成数据驱动的人脸识别系统效果，为隐私友好型人脸识别技术发展提供了有效工具。

Abstract: The use of synthetic data as an alternative to authentic datasets in face
recognition (FR) development has gained significant attention, addressing
privacy, ethical, and practical concerns associated with collecting and using
authentic data. Recent state-of-the-art approaches have proposed
identity-conditioned diffusion models to generate identity-consistent face
images, facilitating their use in training FR models. However, these methods
often lack explicit sampling mechanisms to enforce inter-class separability,
leading to identity overlap in the generated data and, consequently, suboptimal
FR performance. In this work, we introduce NegFaceDiff, a novel sampling method
that incorporates negative conditions into the identity-conditioned diffusion
process. NegFaceDiff enhances identity separation by leveraging negative
conditions that explicitly guide the model away from unwanted features while
preserving intra-class consistency. Extensive experiments demonstrate that
NegFaceDiff significantly improves the identity consistency and separability of
data generated by identity-conditioned diffusion models. Specifically, identity
separability, measured by the Fisher Discriminant Ratio (FDR), increases from
2.427 to 5.687. These improvements are reflected in FR systems trained on the
NegFaceDiff dataset, which outperform models trained on data generated without
negative conditions across multiple benchmarks.

</details>


### [84] [GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors](https://arxiv.org/abs/2508.09667)
*Xingyilang Yin,Qi Zhang,Jiahao Chang,Ying Feng,Qingnan Fan,Xi Yang,Chi-Man Pun,Huaqi Zhang,Xiaodong Cun*

Main category: cs.CV

TL;DR: 提出了一种新方法GSFixer，用于提升稀疏视角下3D Gaussian Splatting重建效果，有效修复伪影并改善3D一致性。


<details>
  <summary>Details</summary>
Motivation: 由于3D Gaussian Splatting在视角稀疏时会出现信息不足，导致重建结果存在明显伪影，传统补全方法难以兼顾输入观测一致性，需要新的解决手段。

Method: 提出GSFixer框架，核心是基于DiT的视频扩散模型，利用带参考条件的3DGS伪影与干净图像对训练，并结合2D语义与3D几何特征来提升修复的语义和空间一致性。同时还构建了DL3DV-Res数据集来评估伪影修复效果。

Result: 实验证明GSFixer在3DGS伪影修复及稀疏视角3D重建方面超越了现有同类方法。

Conclusion: GSFixer为稀疏视角3D重建提供了更优的伪影修复与一致性保证，并推动了相关基准数据集的构建，具有显著应用价值。

Abstract: Reconstructing 3D scenes using 3D Gaussian Splatting (3DGS) from sparse views
is an ill-posed problem due to insufficient information, often resulting in
noticeable artifacts. While recent approaches have sought to leverage
generative priors to complete information for under-constrained regions, they
struggle to generate content that remains consistent with input observations.
To address this challenge, we propose GSFixer, a novel framework designed to
improve the quality of 3DGS representations reconstructed from sparse inputs.
The core of our approach is the reference-guided video restoration model, built
upon a DiT-based video diffusion model trained on paired artifact 3DGS renders
and clean frames with additional reference-based conditions. Considering the
input sparse views as references, our model integrates both 2D semantic
features and 3D geometric features of reference views extracted from the visual
geometry foundation model, enhancing the semantic coherence and 3D consistency
when fixing artifact novel views. Furthermore, considering the lack of suitable
benchmarks for 3DGS artifact restoration evaluation, we present DL3DV-Res which
contains artifact frames rendered using low-quality 3DGS. Extensive experiments
demonstrate our GSFixer outperforms current state-of-the-art methods in 3DGS
artifact restoration and sparse-view 3D reconstruction. Project page:
https://github.com/GVCLab/GSFixer.

</details>


### [85] [PaCo-FR: Patch-Pixel Aligned End-to-End Codebook Learning for Facial Representation Pre-training](https://arxiv.org/abs/2508.09691)
*Yin Xie,Zhichao Chen,Xiaoze Yu,Yongle Zhao,Xiang An,Kaicheng Yang,Zimin Ran,Jia Guo,Ziyong Feng,Jiankang Deng*

Main category: cs.CV

TL;DR: 提出一种新的无监督面部表征预训练方法PaCo-FR，能高效提升多种面部分析任务表现，尤其在极少标注数据下取得领先效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效捕捉面部细致特征和结构，对面部解剖空间结构考虑不足，且不能高效利用有限的标注数据，限制了面部表征的精度和鲁棒性。

Method: 提出PaCo-FR无监督预训练框架，结合了掩码图像建模和patch-像素级对齐。具体包括：以面部语义区域为单位的结构化掩码策略、提高特征区分能力的多候选token的patch级codebook设计，以及保持面部组件几何关系的空间一致性约束。

Result: 在多个面部分析任务基准上，使用仅200万张无标注图片进行预训练，实现了当前最优效果，尤其在姿态、遮挡和光照变化等复杂场景下表现显著提升。

Conclusion: PaCo-FR方法有效提升了面部表征预训练的性能，降低了对大量标注数据的依赖，推动了高效、可扩展的面部分析系统发展。

Abstract: Facial representation pre-training is crucial for tasks like facial
recognition, expression analysis, and virtual reality. However, existing
methods face three key challenges: (1) failing to capture distinct facial
features and fine-grained semantics, (2) ignoring the spatial structure
inherent to facial anatomy, and (3) inefficiently utilizing limited labeled
data. To overcome these, we introduce PaCo-FR, an unsupervised framework that
combines masked image modeling with patch-pixel alignment. Our approach
integrates three innovative components: (1) a structured masking strategy that
preserves spatial coherence by aligning with semantically meaningful facial
regions, (2) a novel patch-based codebook that enhances feature discrimination
with multiple candidate tokens, and (3) spatial consistency constraints that
preserve geometric relationships between facial components. PaCo-FR achieves
state-of-the-art performance across several facial analysis tasks with just 2
million unlabeled images for pre-training. Our method demonstrates significant
improvements, particularly in scenarios with varying poses, occlusions, and
lighting conditions. We believe this work advances facial representation
learning and offers a scalable, efficient solution that reduces reliance on
expensive annotated datasets, driving more effective facial analysis systems.

</details>


### [86] [Slot Attention-based Feature Filtering for Few-Shot Learning](https://arxiv.org/abs/2508.09699)
*Javier Rodenas,Eduardo Aguilar,Petia Radeva*

Main category: cs.CV

TL;DR: 本文提出了一种基于Slot Attention机制的特征过滤方法（SAFF），以提升小样本学习中的分类性能。通过有效去除无关特征，方法在多个小样本数据集上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有小样本学习任务中，图像中存在的无关特征（如背景噪声）会导致匹配混淆，降低分类准确率，迫切需要一种能自动消除无关特征的方法。

Method: 提出SAFF方法，将slot attention机制与patch embedding结合，通过单一注意力过程提取类相关特征并过滤无关特征，引入支持集与查询集间相似度矩阵提升分类判别能力。

Result: 实验证明SAFF的Slot Attention机制能更好区分有用特征，减少无关信息，且在CIFAR-FS、FC100、miniImageNet、tieredImageNet等小样本学习基准上，性能超越多种现有方法。

Conclusion: Slot Attention机制在小样本学习中特征甄别和过滤效果显著，SAFF方法增强了分类判别能力，为小样本学习任务带来实际性能提升。

Abstract: Irrelevant features can significantly degrade few-shot learn ing performance.
This problem is used to match queries and support images based on meaningful
similarities despite the limited data. However, in this process, non-relevant
fea tures such as background elements can easily lead to confu sion and
misclassification. To address this issue, we pro pose Slot Attention-based
Feature Filtering for Few-Shot Learning (SAFF) that leverages slot attention
mechanisms to discriminate and filter weak features, thereby improving few-shot
classification performance. The key innovation of SAFF lies in its integration
of slot attention with patch em beddings, unifying class-aware slots into a
single attention mechanism to filter irrelevant features effectively. We intro
duce a similarity matrix that computes across support and query images to
quantify the relevance of filtered embed dings for classification. Through
experiments, we demon strate that Slot Attention performs better than other
atten tion mechanisms, capturing discriminative features while reducing
irrelevant information. We validate our approach through extensive experiments
on few-shot learning bench marks: CIFAR-FS, FC100, miniImageNet and tieredIma
geNet, outperforming several state-of-the-art methods.

</details>


### [87] [MangaDiT: Reference-Guided Line Art Colorization with Hierarchical Attention in Diffusion Transformers](https://arxiv.org/abs/2508.09709)
*Qianru Qiu,Jiafeng Mao,Kento Masui,Xueting Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新模型MangaDiT，针对基于参考图像引导的线稿上色任务，提升了区域级别的色彩一致性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型已提升线稿上色性能，但在遇到参考图像与目标图像存在姿态或动作差异时，区域色彩一致性仍表现不佳。作者想解决这一问题。

Method: 提出基于Diffusion Transformers（DiT）的MangaDiT模型，模型同时以线稿和参考图为条件输入，并引入分层注意力机制和动态权重策略，通过池化空间特征有效扩展感受野，提升区域水平色彩对齐。

Result: 在两个基准数据集上的实验显示，无论是定性还是定量指标，新方法都比现有技术显著更优。

Conclusion: MangaDiT在参考引导的线稿上色任务中提升了区域色彩一致性，验证了其有效性，为解决相关领域问题提供了新思路。

Abstract: Recent advances in diffusion models have significantly improved the
performance of reference-guided line art colorization. However, existing
methods still struggle with region-level color consistency, especially when the
reference and target images differ in character pose or motion. Instead of
relying on external matching annotations between the reference and target, we
propose to discover semantic correspondences implicitly through internal
attention mechanisms. In this paper, we present MangaDiT, a powerful model for
reference-guided line art colorization based on Diffusion Transformers (DiT).
Our model takes both line art and reference images as conditional inputs and
introduces a hierarchical attention mechanism with a dynamic attention
weighting strategy. This mechanism augments the vanilla attention with an
additional context-aware path that leverages pooled spatial features,
effectively expanding the model's receptive field and enhancing region-level
color alignment. Experiments on two benchmark datasets demonstrate that our
method significantly outperforms state-of-the-art approaches, achieving
superior performance in both qualitative and quantitative evaluations.

</details>


### [88] [NEURAL: Attention-Guided Pruning for Unified Multimodal Resource-Constrained Clinical Evaluation](https://arxiv.org/abs/2508.09715)
*Devvrat Joshi,Islem Rekik*

Main category: cs.CV

TL;DR: 提出了NEURAL框架，基于语义引导的数据压缩，将胸片转化为高压缩图结构，极大减少存储空间，同时保持诊断性能。


<details>
  <summary>Details</summary>
Motivation: 多模态医学影像数据量日益增长，尤其在资源有限的临床环境中，数据存储和传输面临巨大挑战。如何在保证临床诊断有效性的前提下，实现高效的数据压缩和管理，成为亟需解决的问题。

Method: NEURAL利用经过微调的生成式视觉-语言模型的cross-attention分数，指引对胸部X光进行结构性裁剪，仅保留诊断所需关键区域，并将其转化为高度压缩的图表达。随后融合临床报告生成的知识图谱，形成统一的数据结构，便于下游建模。

Result: 在MIMIC-CXR和CheXpert Plus数据集上，NEURAL实现了93.4%至97.7%的图像数据压缩率，诊断AUC保持在0.88到0.95，优于使用未压缩数据的其他基线方法。

Conclusion: NEURAL框架有效兼顾了数据压缩率与临床诊断性能，促进医学影像在有限资源环境下的高效传输与利用，是临床远程会诊和智能化工作流的重要支持。

Abstract: The rapid growth of multimodal medical imaging data presents significant
storage and transmission challenges, particularly in resource-constrained
clinical settings. We propose NEURAL, a novel framework that addresses this by
using semantics-guided data compression. Our approach repurposes
cross-attention scores between the image and its radiological report from a
fine-tuned generative vision-language model to structurally prune chest X-rays,
preserving only diagnostically critical regions. This process transforms the
image into a highly compressed, graph representation. This unified graph-based
representation fuses the pruned visual graph with a knowledge graph derived
from the clinical report, creating a universal data structure that simplifies
downstream modeling. Validated on the MIMIC-CXR and CheXpert Plus dataset for
pneumonia detection, NEURAL achieves a 93.4-97.7\% reduction in image data size
while maintaining a high diagnostic performance of 0.88-0.95 AUC, outperforming
other baseline models that use uncompressed data. By creating a persistent,
task-agnostic data asset, NEURAL resolves the trade-off between data size and
clinical utility, enabling efficient workflows and teleradiology without
sacrificing performance. Our NEURAL code is available at
https://github.com/basiralab/NEURAL.

</details>


### [89] [Multimodal Sheaf-based Network for Glioblastoma Molecular Subtype Prediction](https://arxiv.org/abs/2508.09717)
*Shekhnaz Idrissova,Islem Rekik*

Main category: cs.CV

TL;DR: 本论文提出了一种创新的sheaf-based架构，用以更好地融合MRI与组织病理图像，实现对胶质母细胞瘤分子亚型的非侵入性识别，并在数据不完整场景下表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前胶质母细胞瘤的亚型分型需要侵入性的组织提取，对患者损伤大。现有多模态融合方法（如MRI与病理图像联合）未能有效保留模态间的结构信息，尤其图结构方法难以捕捉判别性特征且针对缺失数据缺乏有效机制，限制了其实际临床应用。

Method: 作者提出了一种新的基于sheaf理论的框架，实现了结构感知的多模态数据一致性融合。该方法在保留不同模态（MRI与病理图像）共享结构特征的同时，具备对缺失或不完整数据的结构重构能力。

Result: 实验结果显示，该sheaf-based模型在多项评测指标上均超越现有主流方法，尤其在数据部分缺失时依然能保证诊断准确性和鲁棒性。

Conclusion: 该研究为胶质母细胞瘤的虚拟活检和快速、低侵入诊断提供了新工具，并推动多模态医学影像分析方法的发展。作者已开源相关代码，便于学术和工业界进一步研究和应用。

Abstract: Glioblastoma is a highly invasive brain tumor with rapid progression rates.
Recent studies have shown that glioblastoma molecular subtype classification
serves as a significant biomarker for effective targeted therapy selection.
However, this classification currently requires invasive tissue extraction for
comprehensive histopathological analysis. Existing multimodal approaches
combining MRI and histopathology images are limited and lack robust mechanisms
for preserving shared structural information across modalities. In particular,
graph-based models often fail to retain discriminative features within
heterogeneous graphs, and structural reconstruction mechanisms for handling
missing or incomplete modality data are largely underexplored. To address these
limitations, we propose a novel sheaf-based framework for structure-aware and
consistent fusion of MRI and histopathology data. Our model outperforms
baseline methods and demonstrates robustness in incomplete or missing data
scenarios, contributing to the development of virtual biopsy tools for rapid
diagnostics. Our source code is available at
https://github.com/basiralab/MMSN/.

</details>


### [90] [Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory](https://arxiv.org/abs/2508.09736)
*Lin Long,Yichen He,Wentao Ye,Yiyuan Pan,Yuan Lin,Hang Li,Junbo Zhao,Wei Li*

Main category: cs.CV

TL;DR: 本文提出了一种新型多模态智能体框架M3-Agent，具备类人长期记忆能力，在多个长视频推理任务基准上表现优越。


<details>
  <summary>Details</summary>
Motivation: 当前多模态智能体在处理长期、多轮、多模态的复杂任务时缺乏高效持久的记忆机制，无法如人类般积累和利用经验及知识，限制了其应用潜力。为此，作者希望构建一种具有人类式长期记忆和先进推理能力的智能体。

Method: 提出M3-Agent系统，包括：1) 实时将视觉及听觉输入转化为长期记忆，涵盖情节和语义；2) 构建以实体为中心的多模态记忆结构，使理解更深入且一致；3) 设计自主多轮推理机制，可基于记忆检索与整合信息；4) 新建M3-Bench多模态记忆基准，该基准包含机器人视角和网络多场景长视频，并配有丰富的问答标注。通过强化学习对M3-Agent训练，并同现有强基线方法进行对比。

Result: 实验表明，M3-Agent在M3-Bench-robot、M3-Bench-web和VideoMME-long任务中，准确率分别比基线（Gemini-1.5-pro与GPT-4o等）高6.7%、7.7%和5.3%。

Conclusion: M3-Agent展示了更类人的长期多模态记忆和推理能力，推动了多模态智能体的发展，并为相关系统设计提供了实际启示。模型及数据已开源。

Abstract: We introduce M3-Agent, a novel multimodal agent framework equipped with
long-term memory. Like humans, M3-Agent can process real-time visual and
auditory inputs to build and update its long-term memory. Beyond episodic
memory, it also develops semantic memory, enabling it to accumulate world
knowledge over time. Its memory is organized in an entity-centric, multimodal
format, allowing deeper and more consistent understanding of the environment.
Given an instruction, M3-Agent autonomously performs multi-turn, iterative
reasoning and retrieves relevant information from memory to accomplish the
task. To evaluate memory effectiveness and memory-based reasoning in multimodal
agents, we develop M3-Bench, a new long-video question answering benchmark.
M3-Bench comprises 100 newly recorded real-world videos captured from a robot's
perspective (M3-Bench-robot) and 929 web-sourced videos across diverse
scenarios (M3-Bench-web). We annotate question-answer pairs designed to test
key capabilities essential for agent applications, such as human understanding,
general knowledge extraction, and cross-modal reasoning. Experimental results
show that M3-Agent, trained via reinforcement learning, outperforms the
strongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o,
achieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web
and VideoMME-long, respectively. Our work advances the multimodal agents toward
more human-like long-term memory and provides insights into their practical
design. Model, code and data are available at
https://github.com/bytedance-seed/m3-agent

</details>


### [91] [Region-to-Region: Enhancing Generative Image Harmonization with Adaptive Regional Injection](https://arxiv.org/abs/2508.09746)
*Zhiqiu Zhang,Dongqi Fan,Mingjie Wang,Qiang Tang,Jian Yang,Zili Yi*

Main category: cs.CV

TL;DR: 本文提出了一种新的区域到区域（Region-to-Region, R2R）图像协调方法和数据集，用于提升复合图像中前景与背景的视觉一致性。该方法能更好地保留细节并适应真实复杂光照，通过新模型和数据增强在多项指标与可视效果上优于现有方法。相关代码和数据已开源。


<details>
  <summary>Details</summary>
Motivation: 现有基于潜在扩散模型（LDM）的图像协调方法在细节保留和协调能力上存在局限，且合成数据集方法简单，难以反映复杂真实场景。该研究旨在解决这些挑战，提高协调质量和数据集多样性。

Method: 提出R2R模型，通过区域注入（Region-to-Region transformation）将合适区域信息注入前景以实现协调和细节保留。设计Clear-VAE（结合自适应滤波，用于去除不协调信息并保留高频细节）和融合了带掩模感知通道注意（MACA）的Harmony Controller，用于动态调整前景。提出随机泊松融合(Random Poisson Blending)生成更复杂合成数据，并据此构建新数据集RPHarmony。

Result: 实验显示，所提方法在定量指标和可视协调表现上均优于其它先进方法。新数据集RPHarmony能帮助模型生成更逼真、协调的合成图像。

Conclusion: R2R方法与RPHarmony数据集有效提升了图像协调质量与数据集多样性，推动了自动图像合成领域的进步。所有成果均已开源，为后续研究和实际应用提供基础。

Abstract: The goal of image harmonization is to adjust the foreground in a composite
image to achieve visual consistency with the background. Recently, latent
diffusion model (LDM) are applied for harmonization, achieving remarkable
results. However, LDM-based harmonization faces challenges in detail
preservation and limited harmonization ability. Additionally, current synthetic
datasets rely on color transfer, which lacks local variations and fails to
capture complex real-world lighting conditions. To enhance harmonization
capabilities, we propose the Region-to-Region transformation. By injecting
information from appropriate regions into the foreground, this approach
preserves original details while achieving image harmonization or, conversely,
generating new composite data. From this perspective, We propose a novel model
R2R. Specifically, we design Clear-VAE to preserve high-frequency details in
the foreground using Adaptive Filter while eliminating disharmonious elements.
To further enhance harmonization, we introduce the Harmony Controller with
Mask-aware Adaptive Channel Attention (MACA), which dynamically adjusts the
foreground based on the channel importance of both foreground and background
regions. To address the limitation of existing datasets, we propose Random
Poisson Blending, which transfers color and lighting information from a
suitable region to the foreground, thereby generating more diverse and
challenging synthetic images. Using this method, we construct a new synthetic
dataset, RPHarmony. Experiments demonstrate the superiority of our method over
other methods in both quantitative metrics and visual harmony. Moreover, our
dataset helps the model generate more realistic images in real examples. Our
code, dataset, and model weights have all been released for open access.

</details>


### [92] [MoIIE: Mixture of Intra- and Inter-Modality Experts for Large Vision Language Models](https://arxiv.org/abs/2508.09779)
*Dianyi Wang,Siyuan Wang,Zejun Li,Yikun Wang,Yitong Li,Duyu Tang,Xiaoyu Shen,Xuanjing Huang,Zhongyu Wei*

Main category: cs.CV

TL;DR: 本文提出了一种用于大型视觉-语言模型（LVLMs）的新型专家混合架构（MoIIE），能高效地捕获模态内及跨模态特征；并通过两阶段训练策略提升模型效果。在相同或更少激活参数量下，超越了现有多模态MoE模型。


<details>
  <summary>Details</summary>
Motivation: 当前LVLMs虽然多模态性能突出，但由于参数量大和高昂计算成本，实际应用受限。采用稀疏MoE可提升效率，但MoE如何处理多模态下的模态内与模态间信息仍存在挑战。

Method: 提出了一种结合模态内专家与跨模态专家的Mixture of Intra- and Inter-Modality Experts (MoIIE)架构。不同模态token按其类型分别路由到模态内和共用的跨模态专家，实现特征联合学习。引入了简单高效的两阶段训练法以强化专家激活及多模态建模能力。

Result: 在多种数据规模和不同LLM主干上实验证明，MoIIE方法具有优异的效果、效率和通用性。5.5B和11.3B参数规模下，模型性能优于更多激活参数的先进Multimodal MoE-LLMs。

Conclusion: MoIIE架构不仅提高了参数效率，同时兼顾了模态内和跨模态学习，具有显著性能优势。为多模态模型在效率与性能上的平衡提供了新方案。源码已开源。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable performance
across multi-modal tasks by scaling model size and training data. However,
these dense LVLMs incur significant computational costs and motivate the
exploration of sparse Mixture of Experts (MoE) architectures. While MoE improve
parameter efficiency, effectively applying MoE to simultaneously model
modality-specific features and cross-modal associations in LVLMs remains
challenging. In this work, we propose to incorporate Mixture of Intra- and
Inter-Modality Experts (MoIIE) to LVLMs. For each token, expert routing is
guided by its modality, directing tokens to their respective intra-modality
experts as well as a shared pool of inter-modality experts, enabling the model
to jointly learn rich intra-modal features and cross-modal interactions. We
further introduce an effective and straightforward two-stage training strategy,
which facilitates the direct activation of both MoE and multi-modal
capabilities. Extensive experiments across different data scales and LLM
backbone demonstrate the effectiveness, efficiency and generality of our
approach. Notably, our MoIIE models with 5.5B and 11.3B activated parameters
match or even surpass the performance of existing advanced open-source MoE-LLMs
based multi-modal models that involve more activated parameters. The code is
available at https://github.com/AlenjandroWang/MoIIE.

</details>


### [93] [Combinative Matching for Geometric Shape Assembly](https://arxiv.org/abs/2508.09780)
*Nahyuk Lee,Juhong Min,Junhong Lee,Chunghyun Park,Minsu Cho*

Main category: cs.CV

TL;DR: 本文提出了一种创新的组合式几何拼装匹配方法，通过结合“相同表面形状”与“相反体积占据”的特征，大幅提升了几何拼装任务中的零件匹配精度。


<details>
  <summary>Details</summary>
Motivation: 现有几何拼装方法主要依赖表面形状的相似性进行对准，然而在处理复杂或互锁结构时，单纯表面匹配易导致局部歧义，难以实现鲁棒拼装。本文旨在解决传统形状匹配在此类任务中的局限性，提高组装的准确性和稳定性。

Method: 作者创新性地将“相同表面形状”和“相反体积占据”两种互锁几何特性建模，通过等变神经网络学习各局部区域的空间取向，实现更精准的区域配对与旋转对齐，从而提升拼装鲁棒性和消除局部歧义。

Result: 在多个几何拼装基准数据集上的实验表明，该方法在拼装准确率和鲁棒性方面均优于目前主流方法，取得了更好的实验表现。

Conclusion: 本文提出的组合式几何形状拼装匹配方法，有效结合了多种几何特征，提升了形状拼装的精度和鲁棒性，为复杂互锁结构的自动组装提供了有力工具，有望推广到更多实际三维装配应用中。

Abstract: This paper introduces a new shape-matching methodology, combinative matching,
to combine interlocking parts for geometric shape assembly. Previous methods
for geometric assembly typically rely on aligning parts by finding identical
surfaces between the parts as in conventional shape matching and registration.
In contrast, we explicitly model two distinct properties of interlocking
shapes: 'identical surface shape' and 'opposite volume occupancy.' Our method
thus learns to establish correspondences across regions where their surface
shapes appear identical but their volumes occupy the inverted space to each
other. To facilitate this process, we also learn to align regions in rotation
by estimating their shape orientations via equivariant neural networks. The
proposed approach significantly reduces local ambiguities in matching and
allows a robust combination of parts in assembly. Experimental results on
geometric assembly benchmarks demonstrate the efficacy of our method,
consistently outperforming the state of the art. Project page:
https://nahyuklee.github.io/cmnet.

</details>


### [94] [DSS-Prompt: Dynamic-Static Synergistic Prompting for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2508.09785)
*Linpu He,Yanan Li,Bingze Li,Elvis Han Cui,Donghui Wang*

Main category: cs.CV

TL;DR: 本文提出了DSS-Prompt方法，通过在预训练的Vision Transformer中引入静态与动态两类prompt，极大提升了小样本类增量学习（FSCIL）的效果，实验证明该方法超越了现有主流方法，并缓解了遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练模型在泛化能力上取得了显著进展，但其在困难的FSCIL任务（小样本类增量学习）中的应用仍未被充分研究，FSCIL要求模型能在极少样本下学习新类别且不遗忘旧类别。

Method: 方法在每个Transformer模块中融合静态prompt（用于弥合预训练任务与下游任务之间的分布差异，提升适应性）与动态prompt（利用预训练多模态模型提取输入相关语义，生成输入自适应的补充prompt，并在不同层间自适应调整其权重），从而提升新旧类别的迁移与表达能力，最终通过简单原型分类器即可实现高效分类，无需针对增量任务再训练。

Result: 在四个主流基准数据集上的实验表明，DSS-Prompt在所有数据集上都达到了优于现有方法的效果，并有效缓解了灾难性遗忘。

Conclusion: DSS-Prompt证明了在预训练Vision Transformer基础上，通过静态与动态prompt的协同作用，可极大增强FSCIL能力，为小样本增量学习提供了更强大的新方法。

Abstract: Learning from large-scale pre-trained models with strong generalization
ability has shown remarkable success in a wide range of downstream tasks
recently, but it is still underexplored in the challenging few-shot
class-incremental learning (FSCIL) task. It aims to continually learn new
concepts from limited training samples without forgetting the old ones at the
same time. In this paper, we introduce DSS-Prompt, a simple yet effective
approach that transforms the pre-trained Vision Transformer with minimal
modifications in the way of prompts into a strong FSCIL classifier. Concretely,
we synergistically utilize two complementary types of prompts in each
Transformer block: static prompts to bridge the domain gap between the
pre-training and downstream datasets, thus enabling better adaption; and
dynamic prompts to capture instance-aware semantics, thus enabling easy
transfer from base to novel classes. Specially, to generate dynamic prompts, we
leverage a pre-trained multi-modal model to extract input-related diverse
semantics, thereby generating complementary input-aware prompts, and then
adaptively adjust their importance across different layers. In this way, on top
of the prompted visual embeddings, a simple prototype classifier can beat
state-of-the-arts without further training on the incremental tasks. We conduct
extensive experiments on four benchmarks to validate the effectiveness of our
DSS-Prompt and show that it consistently achieves better performance than
existing approaches on all datasets and can alleviate the catastrophic
forgetting issue as well.

</details>


### [95] [MeMoSORT: Memory-Assisted Filtering and Motion-Adaptive Association Metric for Multi-Person Tracking](https://arxiv.org/abs/2508.09796)
*Yingjie Wang,Zhixing Wang,Le Zheng,Tianxiao Liu,Roujing Li,Xueyao Hu*

Main category: cs.CV

TL;DR: 本文提出了一种新的多目标跟踪（MOT）方法MeMoSORT，结合了记忆增强的卡尔曼滤波器（MeKF）和自适应运动IoU（Mo-IoU），在复杂的人体跟踪场景中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统MOT方法易受卡尔曼滤波器运动模型与实际运动不符及IoU刚性关联的限制，在目标被遮挡和运动复杂时容易出现追踪错误（如身份切换和目标丢失）。因此，提升跟踪的鲁棒性和准确性成为亟需解决的难题。

Method: 该方法创新性地引入了：（1）记忆辅助卡尔曼滤波器（MeKF），利用记忆增强神经网络校正运动模型与实际目标运动的偏差；（2）运动自适应IoU（Mo-IoU），通过动态扩展匹配空间并融合高度相似性信息，提升遮挡和检测误差场景下的匹配鲁棒性。

Result: 在DanceTrack和SportsMOT数据集上，MeMoSORT分别取得了67.9%和82.1%的HOTA分数，展示了优于现有主流MOT跟踪器的性能。

Conclusion: MeMoSORT是一种简单高效、实时的多目标跟踪方法，能够有效应对复杂运动与遮挡问题，有望在实际人群跟踪等场景中推广应用。

Abstract: Multi-object tracking (MOT) in human-dominant scenarios, which involves
continuously tracking multiple people within video sequences, remains a
significant challenge in computer vision due to targets' complex motion and
severe occlusions. Conventional tracking-by-detection methods are fundamentally
limited by their reliance on Kalman filter (KF) and rigid Intersection over
Union (IoU)-based association. The motion model in KF often mismatches
real-world object dynamics, causing filtering errors, while rigid association
struggles under occlusions, leading to identity switches or target loss. To
address these issues, we propose MeMoSORT, a simple, online, and real-time MOT
tracker with two key innovations. First, the Memory-assisted Kalman filter
(MeKF) uses memory-augmented neural networks to compensate for mismatches
between assumed and actual object motion. Second, the Motion-adaptive IoU
(Mo-IoU) adaptively expands the matching space and incorporates height
similarity to reduce the influence of detection errors and association
failures, while remaining lightweight. Experiments on DanceTrack and SportsMOT
show that MeMoSORT achieves state-of-the-art performance, with HOTA scores of
67.9\% and 82.1\%, respectively.

</details>


### [96] [MUJICA: Reforming SISR Models for PBR Material Super-Resolution via Cross-Map Attention](https://arxiv.org/abs/2508.09802)
*Xin Du,Maoyuan Xu,Zhi Ying*

Main category: cs.CV

TL;DR: 该论文提出了一种面向物理基础渲染（PBR）材质超分辨率的新方法——MUJICA，通过跨图融合提升材质多通道图像的超分辨表现，在多个评测指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有PBR材质的超分方法主要使用单幅图像超分（SISR），但难以处理多通道材质贴图之间的一致性、无法有效地建模各通道特有特征，并且泛化能力有限。因此，提升PBR材质多图通道（如basecolor、normal、metallic等）的超分效果具有实际和应用价值。

Method: 作者提出了MUJICA模块，一种灵活的适配器，可以直接接在预训练的Swin-transformer结构的SISR模型之后，主干网络参数保持冻结状态。MUJICA引入跨贴图注意力机制以融合各个通道的特征信息，提升不同贴图之间的一致性和整体重建能力。

Result: 将MUJICA应用于SwinIR、DRCT和HMANet等SISR模型，在PSNR、SSIM和LPIPS等图像超分评价指标上均有提升，并能更好地保持材质贴图间一致性。实验还表明MUJICA在有限训练资源下也能高效训练，并在PBR材质数据集上实现了SOTA性能。

Conclusion: MUJICA能够有效提升PBR材料多贴图超分辨率任务的性能，解决了现有方法的跨图一致性和特征融合问题，并且具有良好的通用性和高效性。

Abstract: Physically Based Rendering (PBR) materials are typically characterized by
multiple 2D texture maps such as basecolor, normal, metallic, and roughness
which encode spatially-varying bi-directional reflectance distribution function
(SVBRDF) parameters to model surface reflectance properties and microfacet
interactions. Upscaling SVBRDF material is valuable for modern 3D graphics
applications. However, existing Single Image Super-Resolution (SISR) methods
struggle with cross-map inconsistency, inadequate modeling of modality-specific
features, and limited generalization due to data distribution shifts. In this
work, we propose Multi-modal Upscaling Joint Inference via Cross-map Attention
(MUJICA), a flexible adapter that reforms pre-trained Swin-transformer-based
SISR models for PBR material super-resolution. MUJICA is seamlessly attached
after the pre-trained and frozen SISR backbone. It leverages cross-map
attention to fuse features while preserving remarkable reconstruction ability
of the pre-trained SISR model. Applied to SISR models such as SwinIR, DRCT, and
HMANet, MUJICA improves PSNR, SSIM, and LPIPS scores while preserving cross-map
consistency. Experiments demonstrate that MUJICA enables efficient training
even with limited resources and delivers state-of-the-art performance on PBR
material datasets.

</details>


### [97] [Automated Segmentation of Coronal Brain Tissue Slabs for 3D Neuropathology](https://arxiv.org/abs/2508.09805)
*Jonathan Williams Ramirez,Dina Zemlyanker,Lucas Deden-Binder,Rogeny Herisse,Erendira Garcia Pallares,Karthik Gopinath,Harshvardhan Gazula,Christopher Mount,Liana N. Kozanno,Michael S. Marshall,Theresa R. Connors,Matthew P. Frosch,Mark Montine,Derek H. Oakley,Christine L. Mac Donald,C. Dirk Keene,Bradley T. Hyman,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: 该论文提出了一种基于深度学习的自动分割后尸体脑组织照片的方法，大幅减少人工操作，提高分割效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的脑组织照片分析需要昂贵且耗时的人工分割，限制了大规模数据分析和应用。研究者希望开发一种自动工具，简化流程、降低成本，并提升可复现性。

Method: 本文采用U-Net神经网络架构，通过1414张手动分割的不同医院、不同状态脑组织照片及2000张模拟MRI生成的合成图片进行训练，增强模型泛化能力。对未参与训练的照片进行自动预测，并评估分割准确性。

Result: 模型在未见过的测试集照片上取得了中位数Dice分数超过0.98，平均表面距离低于0.4毫米，95% Hausdorff距离低于1.60毫米，性能几乎达到人工分割的一致性水平。

Conclusion: 该自动分割工具能有效自动化处理脑组织照片，为脑库及神经病理研究大规模数据分析提供了高效、准确的手段。工具已开源，可供社区使用。

Abstract: Advances in image registration and machine learning have recently enabled
volumetric analysis of \emph{postmortem} brain tissue from conventional
photographs of coronal slabs, which are routinely collected in brain banks and
neuropathology laboratories worldwide. One caveat of this methodology is the
requirement of segmentation of the tissue from photographs, which currently
requires costly manual intervention. In this article, we present a deep
learning model to automate this process. The automatic segmentation tool relies
on a U-Net architecture that was trained with a combination of
\textit{(i)}1,414 manually segmented images of both fixed and fresh tissue,
from specimens with varying diagnoses, photographed at two different sites; and
\textit{(ii)}~2,000 synthetic images with randomized contrast and corresponding
masks generated from MRI scans for improved generalizability to unseen
photographic setups. Automated model predictions on a subset of photographs not
seen in training were analyzed to estimate performance compared to manual
labels -- including both inter- and intra-rater variability. Our model achieved
a median Dice score over 0.98, mean surface distance under 0.4~mm, and 95\%
Hausdorff distance under 1.60~mm, which approaches inter-/intra-rater levels.
Our tool is publicly available at surfer.nmr.mgh.harvard.edu/fswiki/PhotoTools.

</details>


### [98] [Poaching Hotspot Identification Using Satellite Imagery](https://arxiv.org/abs/2508.09812)
*Aryan Pandhi,Shrey Baid,Sanjali Jha*

Main category: cs.CV

TL;DR: 非洲大象盗猎问题严重，通过卫星影像与计算机视觉模型可以高效识别盗猎热点，提高反盗猎力度。


<details>
  <summary>Details</summary>
Motivation: 大象盗猎导致物种濒危，传统反盗猎方式效果有限且人力消耗大。盗猎热点不断变化，需要新的技术手段实现动态监控和资源的有效部署。

Method: 提出利用计算机视觉模型结合卫星影像，根据地理与环境指标自动识别和预测盗猎热点，无需人工巡查，自动化覆盖大范围区域。

Result: 尽管具体结果未在摘要中详述，但指出该方法可动态分析盗猎活动分布，提升反盗猎资源配置效率，避开监管盲区。

Conclusion: 采用CV模型与卫星影像有助于科学高效打击盗猎，保护濒危大象种群，突破现有巡逻模式的局限性。

Abstract: Elephant Poaching in African countries has been a decade-old problem. So much
so that African Forest Elephants are now listed as an endangered species, and
African Savannah Elephants as critically endangered by the IUCN (International
Union for Conservation of Nature). [1] Elephants are hunted primarily for their
ivory tusks which caused many elephants to be born tuskless as a genetic
modification for survival. [2] Data gathered by recent studies shows that
though poaching methods remain the same, the poaching grounds are rather
dynamic. Poachers have shifted to areas with less ranger patrols and several
other factors like watering holes, seasons, altitude etc. cause constant shifts
in poaching hotspot locations. [3] After a period of low poaching from
2000-2014, poaching numbers in African countries are now on the rise again --
WWF (World Wildlife Foundation) says there are 20,000 elephants poached
annually [4]. In African countries, anti-poaching efforts are concentrated near
towns, while a majority of poaching occurs in the deserted regions. All of
these factors result in the need for a Computer Vision Model to identify
poaching hotspots through locating the geographic indicators of favorable
poaching regions. A CV model eliminates the need to manually track poachers and
account for the environmental factors to deploy resources and its combination
with satellite imagery allows us to survey large areas without disturbing local
species or cross border aviation restrictions.

</details>


### [99] [Evolution of Low-Level and Texture Human-CLIP Alignment](https://arxiv.org/abs/2508.09814)
*Pablo Hernández-Cámara,Jose Manuel Jaén-Lorites,Jorge Vila-Tomás,Jesus Malo,Valero Laparra*

Main category: cs.CV

TL;DR: CLIP等多模态模型训练初期，人类低层次图像质量评估的相关性较高，随后逐渐下降，研究探讨了其成因及意义。


<details>
  <summary>Details</summary>
Motivation: 观察到多模态模型与人类低层次感知的一致性在早期训练高、后期降低，作者想要探明导致此现象的机制及其对于模型优化的启示。

Method: 通过分析CLIP的shape-texture偏好、在噪声下的分类准确率，系统跟踪模型训练过程中感知一致性与鲁棒性的变化，揭示训练过程中特征学习变化。

Result: 发现CLIP初期学习低层视觉特征，表现为与人类感知高度对齐，但更敏感于噪声且偏向纹理；随着训练模型逐步转向抽象的形状表征，提升了对噪声的鲁棒性，但与低层人类感知一致性下降。

Conclusion: 感知一致性与鲁棒性受相同学习机制影响，结果为视觉-语言多模态模型的感知一致性与鲁棒性权衡优化提供新见解。

Abstract: During the training of multi-modal models like CLIP, we observed an
intriguing phenomenon: the correlation with low-level human image quality
assessments peaks in the early epochs before gradually declining. This study
investigates this observation and seeks to understand its causes through two
key factors: shape-texture bias alignment and classification accuracy drop
under noise. Our findings suggest that CLIP initially learn low-level visual
features, enhancing its alignment with low-level human perception but also
increasing its sensitivity to noise and its texture bias. As training
progresses, the model shifts toward more abstract shape-based representations,
improving noise robustness but reducing alignment with low-level human
perception. These results suggest that these factors shared an underlying
learning mechanism and provide new insights into optimizing the trade-off
between perceptual alignment and robustness in vision-language models.

</details>


### [100] [ViMoNet: A Multimodal Vision-Language Framework for Human Behavior Understanding from Motion and Video](https://arxiv.org/abs/2508.09818)
*Rajan Das Gupta,Md Yeasin Rahat,Nafiz Fahad,Abir Ahmed,Liew Tze Hui*

Main category: cs.CV

TL;DR: 本文提出了一种结合动作与视频数据的大语言模型（LLM）框架ViMoNet，用于深入理解和推断人类行为，并证明该方法在行为理解性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型多只关注单一的数据类型（如动作数据或视频数据），难以全面捕捉人类行动的细腻动态及其内涵。因此，作者希望通过融合两种数据类型，实现对人类行为更深层次的理解。

Method: 提出ViMoNet框架，采用动作-文本与视频-文本的联合训练，充分利用详细且精确的动作信息与全面但粗略的视频信息。同时，创建了新的VIMOS数据集，包含多样化的数据类型，并开发了标准化的ViMoNet-Bench基准测试集用于评估模型。

Result: 实验结果表明，在描述生成、动作理解和行为解释等任务上，ViMoNet的表现均优于现有方法。

Conclusion: 将动作与视频数据有效结合，能够显著提升大模型对人类行为的理解与推断能力，ViMoNet及其相关数据与基准为该领域研究提供了新的支撑。

Abstract: This study investigates how large language models (LLMs) can be used to
understand human behavior using motion and video data. We think that mixing
both types is essential to completely capture the nuanced movements and
meanings of human actions, in contrast to recent models that simply concentrate
on motion data or films. To address this, we provide ViMoNet, a straightforward
yet effective framework for comprehending, characterizing, and deducing human
action. ViMoNet employs a joint training strategy that leverages the advantages
of two data types: detailed motion-text data, which is more exact, and generic
video-text data, which is more comprehensive but less detailed. This aids in
the model's acquisition of rich data regarding time and space in human
behavior. Additionally, we provide a brand new dataset named VIMOS that
contains a variety of films, motion sequences, instructions, and subtitles. We
developed ViMoNet-Bench, a standardized benchmark with carefully labeled
samples, to evaluate how well models understand human behavior. Our tests show
that ViMoNet outperforms existing methods in caption generation, motion
understanding, and behavior interpretation.

</details>


### [101] [Physical Autoregressive Model for Robotic Manipulation without Action Pretraining](https://arxiv.org/abs/2508.09822)
*Zijian Song,Sihan Qin,Tianshui Chen,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: 本文提出一种新的物理自回归模型（PAR），结合视频和动作信息，实现机器人与环境的联合预测，能够在无需动作预训练的情况下准确预测未来视频和动作轨迹。


<details>
  <summary>Details</summary>
Motivation: 在机器人操作领域，操作相关数据稀缺，导致动作模型难以有效训练。因此，研究者希望通过利用其他模态（如视频）大模型中已学习到的世界知识，提升机器人操作能力。

Method: 作者提出了物理自回归模型（PAR），用物理token（包含帧和动作）作为输入，融合视频预训练的世界知识，不依赖动作数据进行预训练。该方法采用类DiT去token化方式，将连续帧和动作建模为连续token，减少量化损失，并通过加入因果遮罩、逆运动学、并行训练以及KV-cache机制进一步提升性能和效率。

Result: 在ManiSkill基准测试中，该方法在PushCube任务上达到100%成功率，并在其他任务上表现与动作预训练的基线模型相当，同时实现了动作轨迹与视频预测的高度一致。

Conclusion: 本研究展示了通过视频自回归预训练转移世界知识到机器人操作中的潜力，为机器人操作智能的发展开辟了新途径。

Abstract: The scarcity of manipulation data has motivated the use of pretrained large
models from other modalities in robotics. In this work, we build upon
autoregressive video generation models to propose a Physical Autoregressive
Model (PAR), where physical tokens combine frames and actions to represent the
joint evolution of the robot and its environment. PAR leverages the world
knowledge embedded in video pretraining to understand physical dynamics without
requiring action pretraining, enabling accurate video prediction and consistent
action trajectories. It also adopts a DiT-based de-tokenizer to model frames
and actions as continuous tokens, mitigating quantization errors and
facilitating mutual enhancement. Furthermore, we incorporate a causal mask with
inverse kinematics, parallel training, and the KV-cache mechanism to further
improve performance and efficiency. Experiments on the ManiSkill benchmark show
that PAR achieves a 100\% success rate on the PushCube task, matches the
performance of action-pretrained baselines on other tasks, and accurately
predicts future videos with tightly aligned action trajectories. These findings
underscore a promising direction for robotic manipulation by transferring world
knowledge from autoregressive video pretraining.

</details>


### [102] [KonfAI: A Modular and Fully Configurable Framework for Deep Learning in Medical Imaging](https://arxiv.org/abs/2508.09823)
*Valentin Boussot,Jean-Louis Dillenseger*

Main category: cs.CV

TL;DR: KonfAI 是一个专为医学影像任务设计的深度学习框架，支持模块化、可扩展且完全可配置，用户无需修改代码即可通过 YAML 文件配置全部流程。该工具已在多个顶级竞赛中取得卓越表现，且为开源。


<details>
  <summary>Details</summary>
Motivation: 医学影像任务复杂多变，研究人员和开发者通常需要频繁更改和定制训练、推理、评测流程。现有框架难以兼顾高扩展性、易用性和可追溯性，且开发周期长、复用性差。因此，需要一种简化流程管理、提升研究复现度和灵活性的深度学习工具。

Method: KonfAI 采用 YAML 配置文件的声明式方式，用户可定义训练、推理、评估等完整流程，无需直接修改底层代码。框架内建补丁学习、测试时增强、模型集成、深度监督等高级功能，并支持生成对抗网络等复杂多模型训练。其架构高度模块化和可扩展，可自定义模型、损失函数和数据处理组件。

Result: KonfAI 已成功应用于医学影像的分割、配准、图像合成等多类任务，并在多个国际医学影像竞赛取得顶级排名。

Conclusion: KonfAI 有效提升了医学影像深度学习任务的实验可复现性与开发效率，同时具备强大扩展能力，适合各类定制需求，并已通过公开源码促进社区应用和发展。

Abstract: KonfAI is a modular, extensible, and fully configurable deep learning
framework specifically designed for medical imaging tasks. It enables users to
define complete training, inference, and evaluation workflows through
structured YAML configuration files, without modifying the underlying code.
This declarative approach enhances reproducibility, transparency, and
experimental traceability while reducing development time. Beyond the
capabilities of standard pipelines, KonfAI provides native abstractions for
advanced strategies including patch-based learning, test-time augmentation,
model ensembling, and direct access to intermediate feature representations for
deep supervision. It also supports complex multi-model training setups such as
generative adversarial architectures. Thanks to its modular and extensible
architecture, KonfAI can easily accommodate custom models, loss functions, and
data processing components. The framework has been successfully applied to
segmentation, registration, and image synthesis tasks, and has contributed to
top-ranking results in several international medical imaging challenges. KonfAI
is open source and available at
\href{https://github.com/vboussot/KonfAI}{https://github.com/vboussot/KonfAI}.

</details>


### [103] [Reverse Convolution and Its Applications to Image Restoration](https://arxiv.org/abs/2508.09824)
*Xuhong Huang,Shiqi Liu,Kai Zhang,Ying Tai,Jian Yang,Hui Zeng,Lei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的深度可分逆卷积算子，解决了现有转置卷积无法作为卷积严格逆算子的缺陷，并应用于图像恢复任务，取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 传统的转置卷积（反卷积）并非卷积算子的真实逆算子，受数学公式限制，深度学习模型中缺乏标准化的逆卷积操作。因此，亟需设计一种数学上更合理且易于实现的逆卷积算子，为模型设计和应用带来新突破。

Method: 提出了一种深度可分逆卷积算子，将其转化为正则化最小二乘优化问题求解。进一步完善了核初始化、填充策略等实现细节，并结合层归一化、1x1卷积和GELU激活设计了Transformer风格的逆卷积模块。以此替换现有神经网络架构中的常规卷积与转置卷积，构建了ConverseNet，并分别用于去噪、超分、去模糊任务。

Result: 在典型的图像恢复任务中（如高斯去噪、超分辨率、去模糊），ConverseNet及其变体在DnCNN、SRResNet、USRNet等框架下经过训练，实验结果显示新算子作为基础模块能够实现有效的性能提升。

Conclusion: 所提出的逆卷积算子为深度模型设计引入了新的结构组件，展现了在多个图像恢复任务中的广泛适用性和有效性，有望促进更多新型神经网络算子的开发。

Abstract: Convolution and transposed convolution are fundamental operators widely used
in neural networks. However, transposed convolution (a.k.a. deconvolution) does
not serve as a true inverse of convolution due to inherent differences in their
mathematical formulations. To date, no reverse convolution operator has been
established as a standard component in neural architectures. In this paper, we
propose a novel depthwise reverse convolution operator as an initial attempt to
effectively reverse depthwise convolution by formulating and solving a
regularized least-squares optimization problem. We thoroughly investigate its
kernel initialization, padding strategies, and other critical aspects to ensure
its effective implementation. Building upon this operator, we further construct
a reverse convolution block by combining it with layer normalization,
1$\times$1 convolution, and GELU activation, forming a Transformer-like
structure. The proposed operator and block can directly replace conventional
convolution and transposed convolution layers in existing architectures,
leading to the development of ConverseNet. Corresponding to typical image
restoration models such as DnCNN, SRResNet and USRNet, we train three variants
of ConverseNet for Gaussian denoising, super-resolution and deblurring,
respectively. Extensive experiments demonstrate the effectiveness of the
proposed reverse convolution operator as a basic building module. We hope this
work could pave the way for developing new operators in deep model design and
applications.

</details>


### [104] [Hierarchical Graph Attention Network for No-Reference Omnidirectional Image Quality Assessment](https://arxiv.org/abs/2508.09843)
*Hao Yang,Xu Zhang,Jiaqi Ma,Linwei Zhu,Yun Zhang,Huan Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于图神经网络（GNN）的全景图像质量评价（OIQA）方法，有效处理空间失真非均匀的问题，实验效果大幅优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前OIQA方法难以评估图像中的局部非均匀失真，主要因为空间质量变化建模不足，以及特征表达上不能有效兼顾局部细节和全局信息。为了解决这些问题，需要设计更具表现力的空间关系与特征表达方法。

Method: 作者提出用Fibonacci球面采样生成具有良好结构的视口，视口构建为图节点。通过多阶段特征提取网络获得高维节点表示，随后采用图注意力网络（GAT）建模相邻视口间的细粒度局部失真变化，并结合图Transformer捕捉远距离区域的质量交互，实现局部与全局空间依赖的联合建模。

Result: 在两个具有复杂空间失真的大规模OIQA数据库上，所提方法在图像质量评价任务中显著超过现有主流方法，实验结果展示了其更高的有效性和优秀的泛化能力。

Conclusion: 基于GNN的空间结构建模和特征提取策略，显著提升了全景图像质量评价在非均匀失真环境下的表现，对相关领域具有重要参考价值。

Abstract: Current Omnidirectional Image Quality Assessment (OIQA) methods struggle to
evaluate locally non-uniform distortions due to inadequate modeling of spatial
variations in quality and ineffective feature representation capturing both
local details and global context. To address this, we propose a graph neural
network-based OIQA framework that explicitly models structural relationships
between viewports to enhance perception of spatial distortion non-uniformity.
Our approach employs Fibonacci sphere sampling to generate viewports with
well-structured topology, representing each as a graph node. Multi-stage
feature extraction networks then derive high-dimensional node representation.
To holistically capture spatial dependencies, we integrate a Graph Attention
Network (GAT) modeling fine-grained local distortion variations among adjacent
viewports, and a graph transformer capturing long-range quality interactions
across distant regions. Extensive experiments on two large-scale OIQA databases
with complex spatial distortions demonstrate that our method significantly
outperforms existing approaches, confirming its effectiveness and strong
generalization capability.

</details>


### [105] [Enhancing Diffusion Face Generation with Contrastive Embeddings and SegFormer Guidance](https://arxiv.org/abs/2508.09847)
*Dhruvraj Singh Rawat,Enggen Sherpa,Rishikesan Kirupanantha,Tin Hoang*

Main category: cs.CV

TL;DR: 本文对Diffusion模型在小规模CelebAMask-HQ人脸生成任务上的表现进行了系统评测，提出了使用InfoNCE损失和SegFormer编码器来提升条件生成的准确性和可控性。


<details>
  <summary>Details</summary>
Motivation: 人脸生成需要更高的精准控制，特别是在数据量有限的情况下。因此，本文旨在研究在小规模数据集下，如何通过提升条件特征编码和引入对比性损失，提高Diffusion模型在属性控制生成任务中的表现。

Method: 1. 比较了UNet与DiT两种架构在无条件人脸生成上的表现；2. 对Stable Diffusion模型进行了LoRA微调实验；3. 基于现有多条件方法（同时输入属性向量和分割掩码），改进地引入了InfoNCE损失以优化属性嵌入，并采用SegFormer架构代替传统的分割编码器。

Result: 融合InfoNCE损失的属性嵌入和SegFormer分割编码器显著提升了生成样本与目标属性之间的语义对齐和面部特征可控性。在实验的数据受限场景下，提出的方法比基线拥有更优效果。

Conclusion: 采用先进的属性对比编码和分割特征提取技术，可在有限数据下实现更精确且可控的人脸生成，为后续可控生成领域研究提供了新方向。

Abstract: We present a benchmark of diffusion models for human face generation on a
small-scale CelebAMask-HQ dataset, evaluating both unconditional and
conditional pipelines. Our study compares UNet and DiT architectures for
unconditional generation and explores LoRA-based fine-tuning of pretrained
Stable Diffusion models as a separate experiment. Building on the
multi-conditioning approach of Giambi and Lisanti, which uses both attribute
vectors and segmentation masks, our main contribution is the integration of an
InfoNCE loss for attribute embedding and the adoption of a SegFormer-based
segmentation encoder. These enhancements improve the semantic alignment and
controllability of attribute-guided synthesis. Our results highlight the
effectiveness of contrastive embedding learning and advanced segmentation
encoding for controlled face generation in limited data settings.

</details>


### [106] [ARI3D: A Software for Interactive Quantification of Regions in X-Ray CT 3D Images](https://arxiv.org/abs/2508.09849)
*Jan Phillipp Albrecht,Jose R. A. Godinho,Christina Hübers,Deborah Schmidt*

Main category: cs.CV

TL;DR: 本文提出了一款名为ARI3D的软件工具，帮助用户更高效、准确地分析和量化X射线CT三维图像中的微观结构。


<details>
  <summary>Details</summary>
Motivation: 传统X射线CT图像分析面临成像伪影（如束硬化、部分体积效应）带来的挑战，且分析流程需用户做大量主观决策，影响定量分析的准确性和一致性。提升分析效率、减少人为误差，是亟需解决的问题。

Method: 作者开发了ARI3D交互式软件，支持用户在三维X射线CT图像中，按照特定分析协议，对不同区域物体进行分割、分类、定量等分析，且考虑到部分体积效应等固有成像问题。

Result: ARI3D软件能够提升相的识别准确性，校正部分体积效应，提高目标检测下限和数量化准确性，实现3D定量分析流程标准化、可跨学科应用。

Conclusion: ARI3D为三维X射线CT微观结构定量分析提供了更优、标准化的技术路径，能普适应用于多学科领域，提升分析过程的准确性与一致性。

Abstract: X-ray computed tomography (CT) is the main 3D technique for imaging the
internal microstructures of materials. Quantitative analysis of the
microstructures is usually achieved by applying a sequence of steps that are
implemented to the entire 3D image. This is challenged by various imaging
artifacts inherent from the technique, e.g., beam hardening and partial volume.
Consequently, the analysis requires users to make a number of decisions to
segment and classify the microstructures based on the voxel gray-values. In
this context, a software tool, here called ARI3D, is proposed to interactively
analyze regions in three-dimensional X-ray CT images, assisting users through
the various steps of a protocol designed to classify and quantify objects
within regions of a three-dimensional image. ARI3D aims to 1) Improve phase
identification; 2) Account for partial volume effect; 3) Increase the detection
limit and accuracy of object quantification; and 4) Harmonize quantitative 3D
analysis that can be implemented in different fields of science.

</details>


### [107] [Do Vision Transformers See Like Humans? Evaluating their Perceptual Alignment](https://arxiv.org/abs/2508.09850)
*Pablo Hernández-Cámara,Jose Manuel Jaén-Lorites,Jorge Vila-Tomás,Valero Laparra,Jesus Malo*

Main category: cs.CV

TL;DR: 本文系统性研究了视觉Transformer（ViT）模型与人类感知的一致性，发现更大规模、更复杂的模型以及特定的数据增强和正则化策略会降低ViT与人类感知的契合度。


<details>
  <summary>Details</summary>
Motivation: 当前ViT在图像识别任务上表现优异，但其与人类感知一致性的机制尚未明确，需要评估模型优化与人类主观感知之间的平衡关系。

Method: 作者在TID2013数据集上，系统评估了模型规模、训练数据规模、多样性、数据增强与正则化等因素对ViT与人类感知一致性的影响。

Result: 实验证明更大规模的模型表现出更低的人类感知一致性。增加数据集多样性影响甚微，而重复训练同一图像会进一步降低一致性。更强的数据增强和正则化措施也会显著降低模型与人类感知的契合度，尤其是在多次训练时。

Conclusion: 论文指出模型设计与训练策略之间与感知一致性存在权衡，这为需要人类视觉理解的实际应用提出了重要参考。

Abstract: Vision Transformers (ViTs) achieve remarkable performance in image
recognition tasks, yet their alignment with human perception remains largely
unexplored. This study systematically analyzes how model size, dataset size,
data augmentation and regularization impact ViT perceptual alignment with human
judgments on the TID2013 dataset. Our findings confirm that larger models
exhibit lower perceptual alignment, consistent with previous works. Increasing
dataset diversity has a minimal impact, but exposing models to the same images
more times reduces alignment. Stronger data augmentation and regularization
further decrease alignment, especially in models exposed to repeated training
cycles. These results highlight a trade-off between model complexity, training
strategies, and alignment with human perception, raising important
considerations for applications requiring human-like visual understanding.

</details>


### [108] [OneVAE: Joint Discrete and Continuous Optimization Helps Discrete Video VAE Train Better](https://arxiv.org/abs/2508.09857)
*Yupeng Zhou,Zhen Li,Ziheng Ouyang,Yuming Chen,Ruoyi Du,Daquan Zhou,Bin Fu,Yihao Liu,Peng Gao,Ming-Ming Cheng,Qibin Hou*

Main category: cs.CV

TL;DR: 提出了一种统一连续和离散视频表示的VAE方法OneVAE，通过保留连续VAE先验和结构改进，大幅提升了离散视频VAE的训练效率与重建效果。


<details>
  <summary>Details</summary>
Motivation: 离散化视频有利于和文本token对齐，便于多模态大模型统一表达，但离散视频VAE训练困难且重建质量差。连续VAE训练简单且性能好，因此探索如何结合两者，提升离散视频VAE性能。

Method: 分析了离散与连续表示的关系，发现FSQ量化方法能更好保留连续VAE先验。提出基于预训练连续VAE先验的离散VAE方法，实现更快收敛和更好的性能。结构上，一是引入多token量化提升重建PSNR，二是强化首帧重建应对高压缩下信息丢失。此外提出了联合优化机制，在单网络上实现两种表示的竞争性能。

Result: 训练速度提升数倍，重建质量优于从头训练。多token量化提升PSNR约1dB。首帧强化+因果结构显著提升高压缩离散VAE结果。首次实现单网络上离散与连续表示均有竞争力表现。

Conclusion: OneVAE方法有效弥合离散和连续视频VAE性能与训练差距，为多模态LLM统一视频—文本token铺路，具备重要应用前景。

Abstract: Encoding videos into discrete tokens could align with text tokens to
facilitate concise and unified multi-modal LLMs, yet introducing significant
spatiotemporal compression compared to continuous video representation.
Previous discrete video VAEs experienced unstable training, long training time,
and degraded reconstruction quality. Given the easier training and superior
performance of continuous VAEs, an intuitive idea is to enhance discrete video
VAEs by leveraging continuous VAEs. After rethinking the intrinsic link between
discrete and continuous representations, we found that FSQ could effectively
preserve pre-trained continuous VAE priors compared to other quantization
methods. By leveraging continuous VAE priors, it converges several times faster
than training from scratch and achieves superior performance at convergence.
Meanwhile, two structural improvements are proposed. First, inspired by how
continuous VAEs enhance reconstruction via enlarged latent dimensions, we
introduce a multi-token quantization mechanism, which achieves nearly a 1 dB
improvement in PSNR without compromising the token compression ratio. Second,
to tackle reconstruction challenges in high-compression video VAEs, we
strengthen first-frame reconstruction, enabling the causal VAE to leverage this
information in subsequent frames and markedly improving the performance of 4 x
16 x 16 discrete VAEs. Furthermore, we propose a joint discrete-continuous
optimization scheme that unifies the two paradigms and, for the first time,
achieves competitive performance on both continuous and discrete
representations within a single network. We name our method OneVAE to reflect
this connection.

</details>


### [109] [HumanGenesis: Agent-Based Geometric and Generative Modeling for Synthetic Human Dynamics](https://arxiv.org/abs/2508.09858)
*Weiqi Li,Zehao Zhang,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的合成真人动态（HumanGenesis）框架，可以生成高度写实的人体视频，同时突破当前方法在几何一致性、细节保留和动作泛化方面的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有合成真人视频方法在三维建模和细节上存在几何不一致与粗糙的问题，同时难以泛化表达多样的动作，且人和场景的融合感较差。

Method: HumanGenesis系统引入了四个协同工作的智能体：1) Reconstructor，利用3D高斯斑点和变形分解，从单目视频重建3D一致的人体-场景表示；2) Critique Agent，通过MLLM多轮反思增强重建局部细节；3) Pose Guider，基于时序感知参数编码器生成丰富动作序列；4) Video Harmonizer，基于混合渲染与扩散模型并结合反馈优化，实现最终高质量视频合成。

Result: HumanGenesis在文本驱动生成、视频换脸和新动作泛化等多任务上表现出超越现有方法的性能，提升了动作表现力、几何精确度和场景融合度。

Conclusion: 该框架有效破解了合成真人视频在细节、几何真实感及动作泛化中的瓶颈，为多种下游应用奠定了更高质量的基石。

Abstract: \textbf{Synthetic human dynamics} aims to generate photorealistic videos of
human subjects performing expressive, intention-driven motions. However,
current approaches face two core challenges: (1) \emph{geometric inconsistency}
and \emph{coarse reconstruction}, due to limited 3D modeling and detail
preservation; and (2) \emph{motion generalization limitations} and \emph{scene
inharmonization}, stemming from weak generative capabilities. To address these,
we present \textbf{HumanGenesis}, a framework that integrates geometric and
generative modeling through four collaborative agents: (1)
\textbf{Reconstructor} builds 3D-consistent human-scene representations from
monocular video using 3D Gaussian Splatting and deformation decomposition. (2)
\textbf{Critique Agent} enhances reconstruction fidelity by identifying and
refining poor regions via multi-round MLLM-based reflection. (3) \textbf{Pose
Guider} enables motion generalization by generating expressive pose sequences
using time-aware parametric encoders. (4) \textbf{Video Harmonizer} synthesizes
photorealistic, coherent video via a hybrid rendering pipeline with diffusion,
refining the Reconstructor through a Back-to-4D feedback loop. HumanGenesis
achieves state-of-the-art performance on tasks including text-guided synthesis,
video reenactment, and novel-pose generalization, significantly improving
expressiveness, geometric fidelity, and scene integration.

</details>


### [110] [E-4DGS: High-Fidelity Dynamic Reconstruction from the Multi-view Event Cameras](https://arxiv.org/abs/2508.09912)
*Chaoran Feng,Zhenyu Tang,Wangbo Yu,Yatian Pang,Yian Zhao,Jianbin Zhao,Li Yuan,Yonghong Tian*

Main category: cs.CV

TL;DR: 本论文提出E-4DGS，一种基于事件相机的动态高斯Splatting方法，实现多视角高动态场景下的新视图合成与4D重建，针对运动模糊和弱光等传统RGB相机难题给出新解法。


<details>
  <summary>Details</summary>
Motivation: 现有新视图合成与4D重建方法依赖RGB相机，受制于光照、动态范围和运动模糊。而事件相机高速、高动态、低能耗特性，有望突破这些瓶颈，尤其适用于高速运动和弱光场景。因此迫切需要事件相机驱动的4D重建新方法。

Method: 提出E-4DGS框架，包括事件流初始化、基于事件自适应切片的Splatting时间感知重建，以及基于强度的剪枝去伪影和自适应对比度阈值优化。同时设计六视角全方位事件相机实验平台，并公开高难度场景数据集。

Result: 在多视角事件流数据集上，E-4DGS优于现有事件流和事件-RGB融合基线方法，能更好适应高速运动和低光环境，提高3D一致性、降低伪影。

Conclusion: E-4DGS展示了事件相机在多视角高动态4D重建中的重要潜力，为快速场景采集及事件驱动重建方法开辟方向。

Abstract: Novel view synthesis and 4D reconstruction techniques predominantly rely on
RGB cameras, thereby inheriting inherent limitations such as the dependence on
adequate lighting, susceptibility to motion blur, and a limited dynamic range.
Event cameras, offering advantages of low power, high temporal resolution and
high dynamic range, have brought a new perspective to addressing the scene
reconstruction challenges in high-speed motion and low-light scenes. To this
end, we propose E-4DGS, the first event-driven dynamic Gaussian Splatting
approach, for novel view synthesis from multi-view event streams with
fast-moving cameras. Specifically, we introduce an event-based initialization
scheme to ensure stable training and propose event-adaptive slicing splatting
for time-aware reconstruction. Additionally, we employ intensity importance
pruning to eliminate floating artifacts and enhance 3D consistency, while
incorporating an adaptive contrast threshold for more precise optimization. We
design a synthetic multi-view camera setup with six moving event cameras
surrounding the object in a 360-degree configuration and provide a benchmark
multi-view event stream dataset that captures challenging motion scenarios. Our
approach outperforms both event-only and event-RGB fusion baselines and paves
the way for the exploration of multi-view event-based reconstruction as a novel
approach for rapid scene capture.

</details>


### [111] [SpeechForensics: Audio-Visual Speech Representation Learning for Face Forgery Detection](https://arxiv.org/abs/2508.09913)
*Yachao Liang,Min Yu,Gang Li,Jianguo Jiang,Boquan Li,Feng Yu,Ning Zhang,Xiang Meng,Weiqing Huang*

Main category: cs.CV

TL;DR: 本论文提出了一种基于音频与视觉联合表征学习的新方法，用于检测人脸伪造视频，显著提升了对未知数据集和常见干扰的泛化和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 人脸伪造（如Deepfake）的视频检测很难，特别是在面对未见过的数据集和常见扰动时。研究发现，音频中的说话内容与面部运动有高度关联，因此可以辅助伪造检测。

Method: 作者首先在真实视频上，通过自监督的mask预测任务学习音频-视觉联合的说话表征，既编码了局部又编码了全局语义信息。随后，这一模型直接迁移至伪造检测任务，并且在训练过程中没有使用任何伪造视频。

Result: 大量实验表明，该方法在跨数据集的泛化和鲁棒性方面优于当前最先进方法，并且实现了无需引入伪造样本的情况下取得好性能。

Conclusion: 通过音频与视觉说话元素的联合自监督学习，可以极大提升人脸伪造检测模型的泛化能力和鲁棒性，未来有望推动真实性检测系统的实用化应用。

Abstract: Detection of face forgery videos remains a formidable challenge in the field
of digital forensics, especially the generalization to unseen datasets and
common perturbations. In this paper, we tackle this issue by leveraging the
synergy between audio and visual speech elements, embarking on a novel approach
through audio-visual speech representation learning. Our work is motivated by
the finding that audio signals, enriched with speech content, can provide
precise information effectively reflecting facial movements. To this end, we
first learn precise audio-visual speech representations on real videos via a
self-supervised masked prediction task, which encodes both local and global
semantic information simultaneously. Then, the derived model is directly
transferred to the forgery detection task. Extensive experiments demonstrate
that our method outperforms the state-of-the-art methods in terms of
cross-dataset generalization and robustness, without the participation of any
fake video in model training. Code is available at
https://github.com/Eleven4AI/SpeechForensics.

</details>


### [112] [Towards Comprehensive Cellular Characterisation of H&E slides](https://arxiv.org/abs/2508.09926)
*Benjamin Adjadj,Pierre-Antoine Bannier,Guillaume Horent,Sebastien Mandela,Aurore Lyon,Kathryn Schutte,Ulysse Marteau,Valentin Gaury,Laura Dumont,Thomas Mathieu,Reda Belbahri,Benoît Schmauch,Eric Durand,Katharina Von Loga,Lucie Gillet*

Main category: cs.CV

TL;DR: 本文提出了HistoPLUS模型，在肿瘤微环境H&E切片的细胞检测、分割和分类上大幅领先现有方法，尤其是对欠研究细胞类型表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有细胞分析方法在稀有或未被充分研究的细胞类型上的表现很差，且跨领域泛化能力有限。为此需要开发更精确且适应性强的新方法。

Method: 作者开发了HistoPLUS模型，并在一个包含13种细胞类型、共108,722个细胞核的大型癌症数据集上进行训练。与其他先进模型进行了横向对比和外部验证，覆盖4个独立队列，还检验了模型对新肿瘤类型的迁移能力。

Result: HistoPLUS在外部验证中检测质量提升5.2%，分类整体F1分数提升23.7%，参数量减少5倍。模型首次支持7类欠研究细胞的分析，并对13类中的8类实现显著改进，在新未见过的癌症类型上也表现稳健。

Conclusion: HistoPLUS模型显著提升了肿瘤微环境中细胞分析的广度和精度，有助于更广泛的生物标志物研究。模型权重和代码已经开源发布。

Abstract: Cell detection, segmentation and classification are essential for analyzing
tumor microenvironments (TME) on hematoxylin and eosin (H&E) slides. Existing
methods suffer from poor performance on understudied cell types (rare or not
present in public datasets) and limited cross-domain generalization. To address
these shortcomings, we introduce HistoPLUS, a state-of-the-art model for cell
analysis, trained on a novel curated pan-cancer dataset of 108,722 nuclei
covering 13 cell types. In external validation across 4 independent cohorts,
HistoPLUS outperforms current state-of-the-art models in detection quality by
5.2% and overall F1 classification score by 23.7%, while using 5x fewer
parameters. Notably, HistoPLUS unlocks the study of 7 understudied cell types
and brings significant improvements on 8 of 13 cell types. Moreover, we show
that HistoPLUS robustly transfers to two oncology indications unseen during
training. To support broader TME biomarker research, we release the model
weights and inference code at https://github.com/owkin/histoplus/.

</details>


### [113] [Quo Vadis Handwritten Text Generation for Handwritten Text Recognition?](https://arxiv.org/abs/2508.09936)
*Vittorio Pippi,Konstantina Nikolaidou,Silvia Cascianelli,George Retsinas,Giorgos Sfikas,Rita Cucchiara,Marcus Liwicki*

Main category: cs.CV

TL;DR: 本文比较了三种最新的手写文本生成（HTG）模型，对低资源历史手稿识别中的应用效果进行系统评估，并提出选型建议。


<details>
  <summary>Details</summary>
Motivation: 历史手稿数字化面临手写文本识别（HTR）难题，尤其是小型、作者特定的手稿集与训练数据分布差异明显。通过生成模拟特定书写风格的数据（HTG），有望缓解低资源场景下的识别瓶颈，但不同HTG模型对识别效果的提升尚无系统比较。

Method: 系统评估了三类代表性HTG模型（涵盖对抗生成、扩散和自回归范式），分析生成数据在视觉和语言特征上的表现，研究其对HTR微调的具体影响，并提出模型选择的定量建议。

Result: 三种主流HTG模型对低资源HTR任务可产生不同程度的辅助效果，论文量化分析了这些模型生成的数据质量及其对下游任务微调的具体帮助，并形成选型指引。

Conclusion: HTG方法现阶段可显著帮助低资源手写文本识别，但不同模型的适用性存在差异，未来应聚焦提升合成数据的真实性和多样性，以进一步优化实际应用效果。

Abstract: The digitization of historical manuscripts presents significant challenges
for Handwritten Text Recognition (HTR) systems, particularly when dealing with
small, author-specific collections that diverge from the training data
distributions. Handwritten Text Generation (HTG) techniques, which generate
synthetic data tailored to specific handwriting styles, offer a promising
solution to address these challenges. However, the effectiveness of various HTG
models in enhancing HTR performance, especially in low-resource transcription
settings, has not been thoroughly evaluated. In this work, we systematically
compare three state-of-the-art styled HTG models (representing the generative
adversarial, diffusion, and autoregressive paradigms for HTG) to assess their
impact on HTR fine-tuning. We analyze how visual and linguistic characteristics
of synthetic data influence fine-tuning outcomes and provide quantitative
guidelines for selecting the most effective HTG model. The results of our
analysis provide insights into the current capabilities of HTG methods and
highlight key areas for further improvement in their application to
low-resource HTR.

</details>


### [114] [AST-n: A Fast Sampling Approach for Low-Dose CT Reconstruction using Diffusion Models](https://arxiv.org/abs/2508.09943)
*Tomás de la Sotta,José M. Saavedra,Héctor Henríquez,Violeta Chang,Aline Xavier*

Main category: cs.CV

TL;DR: 本文提出了一种加速扩散模型推理的框架AST-n，实现低剂量CT（LDCT）图像快速去噪，在保持高影像质量的同时大幅降低推理时间。该方法结合中间噪声初始化和高阶ODE求解器，仅用25步就能获得接近常规基线的高质量图像。


<details>
  <summary>Details</summary>
Motivation: 低剂量CT可减少患者辐射风险，但增加图像噪声，影响诊断。扩散生成模型虽能有效去噪但推理慢，难以满足临床实时需求，因此需要加速扩散模型推理，同时保持图像质量。

Method: 提出AST-n推断框架，从中间噪声水平开始逆扩散，加速过程；结合高阶ODE求解器进一步减少采样步数。分别采用AST-n采样与常规高阶调度方式，在LDCT Grand Challenge数据集（10-25%标准剂量）上评估方法，包括条件与无条件采样，并对DDIM反演进行对比分析。

Result: 条件采样（AST-25）仅用25步即实现PSNR>38dB、SSIM>0.95，接近常规模型表现，推断时间由约16秒降至1秒以内/切片。无条件采样质量明显下降。DDIM反演虽微提升PSNR但推理时间翻倍，实用性受限。

Conclusion: AST-n结合高阶采样器显著加速LDCT图像重建，几乎不损失质量，推动扩散模型在临床流程中的实际应用。

Abstract: Low-dose CT (LDCT) protocols reduce radiation exposure but increase image
noise, compromising diagnostic confidence. Diffusion-based generative models
have shown promise for LDCT denoising by learning image priors and performing
iterative refinement. In this work, we introduce AST-n, an accelerated
inference framework that initiates reverse diffusion from intermediate noise
levels, and integrate high-order ODE solvers within conditioned models to
further reduce sampling steps. We evaluate two acceleration paradigms--AST-n
sampling and standard scheduling with high-order solvers -- on the Low Dose CT
Grand Challenge dataset, covering head, abdominal, and chest scans at 10-25 %
of standard dose. Conditioned models using only 25 steps (AST-25) achieve peak
signal-to-noise ratio (PSNR) above 38 dB and structural similarity index (SSIM)
above 0.95, closely matching standard baselines while cutting inference time
from ~16 seg to under 1 seg per slice. Unconditional sampling suffers
substantial quality loss, underscoring the necessity of conditioning. We also
assess DDIM inversion, which yields marginal PSNR gains at the cost of doubling
inference time, limiting its clinical practicality. Our results demonstrate
that AST-n with high-order samplers enables rapid LDCT reconstruction without
significant loss of image fidelity, advancing the feasibility of
diffusion-based methods in clinical workflows.

</details>


### [115] [Stable Diffusion Models are Secretly Good at Visual In-Context Learning](https://arxiv.org/abs/2508.09949)
*Trevine Oorloff,Vishwanath Sindagi,Wele Gedara Chaminda Bandara,Ali Shafahi,Amin Ghiasi,Charan Prakash,Reza Ardekani*

Main category: cs.CV

TL;DR: 本文展示了无需额外训练，仅利用现有Stable Diffusion模型，通过对自注意力机制中的注意力矩阵重新计算，实现视觉情境学习（V-ICL），从而能快速适应多种视觉任务，并在多个任务上显著提升表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型已在NLP领域表现出色的情境学习能力，CV领域类似方法难以迁移且依赖复杂训练或额外数据，应用受限。因此，作者希望简化流程并提升通用性，直接让现有扩散模型具备情境学习能力，从而适应多任务视觉应用。

Method: 方法是在Stable Diffusion架构自注意力层中，重新设计注意力机制，让模型能显式注入“查询图像”和“示例图像”间的上下文信息。这个过程无需对Stable Diffusion模型进行额外微调，直接复用预训练模型即可实现视觉情境学习。

Result: 该方法可在无微调前提下，完成前景分割、单物体检测、语义分割、关键点检测、边缘检测和图像上色等六项视觉任务。实验表明，仅在Pascal-5i数据集的前景分割任务上，方法相比Visual Prompting和IMProv分别提升了8.9%和3.2%的mIoU。此外，方法支持多样例集成，进一步提升表现。

Conclusion: Stable Diffusion可以无需微调，通过注意力机制重新计算实现视觉情境学习，有效适应多种视觉下游任务，理论上简化流程并具有更强的泛化能力。

Abstract: Large language models (LLM) in natural language processing (NLP) have
demonstrated great potential for in-context learning (ICL) -- the ability to
leverage a few sets of example prompts to adapt to various tasks without having
to explicitly update the model weights. ICL has recently been explored for
computer vision tasks with promising early outcomes. These approaches involve
specialized training and/or additional data that complicate the process and
limit its generalizability. In this work, we show that off-the-shelf Stable
Diffusion models can be repurposed for visual in-context learning (V-ICL).
Specifically, we formulate an in-place attention re-computation within the
self-attention layers of the Stable Diffusion architecture that explicitly
incorporates context between the query and example prompts. Without any
additional fine-tuning, we show that this repurposed Stable Diffusion model is
able to adapt to six different tasks: foreground segmentation, single object
detection, semantic segmentation, keypoint detection, edge detection, and
colorization. For example, the proposed approach improves the mean intersection
over union (mIoU) for the foreground segmentation task on Pascal-5i dataset by
8.9% and 3.2% over recent methods such as Visual Prompting and IMProv,
respectively. Additionally, we show that the proposed method is able to
effectively leverage multiple prompts through ensembling to infer the task
better and further improve the performance.

</details>


### [116] [LIA-X: Interpretable Latent Portrait Animator](https://arxiv.org/abs/2508.09959)
*Yaohui Wang,Di Yang,Xinyuan Chen,Francois Bremond,Yu Qiao,Antitza Dantcheva*

Main category: cs.CV

TL;DR: LIA-X是一种可解释、可控的人像动画生成模型，通过引入稀疏运动字典，实现精细的人脸动态转移，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸驱动方法（如warp-render）在精细可控性和可解释性上存在局限，难以实现对面部微妙表情动作的细致操控，且与驱动视频之间的姿态与表情差异难以弥合。

Method: LIA-X采用自编码器架构，将运动迁移建模为潜在空间中的线性运动编码导航。提出稀疏运动字典，将面部动态解耦为可解释的因子，进而实现“编辑-变形-渲染”新策略，允许对源人像进行细粒度、可控地编辑和变形。此外，作者训练了十亿参数级的大模型，证明了方法的可扩展性。

Result: 在自重现和跨重现任务中，LIA-X在多个基准数据集上均优于以往方法。稀疏运动字典的引入显著提升了编辑和操控性能。

Conclusion: LIA-X不仅在性能上超越前人，还具备良好的可解释性和可控性，适用于精细级别的图片及视频编辑，以及3D相关的人像视频操控。

Abstract: We introduce LIA-X, a novel interpretable portrait animator designed to
transfer facial dynamics from a driving video to a source portrait with
fine-grained control. LIA-X is an autoencoder that models motion transfer as a
linear navigation of motion codes in latent space. Crucially, it incorporates a
novel Sparse Motion Dictionary that enables the model to disentangle facial
dynamics into interpretable factors. Deviating from previous 'warp-render'
approaches, the interpretability of the Sparse Motion Dictionary allows LIA-X
to support a highly controllable 'edit-warp-render' strategy, enabling precise
manipulation of fine-grained facial semantics in the source portrait. This
helps to narrow initial differences with the driving video in terms of pose and
expression. Moreover, we demonstrate the scalability of LIA-X by successfully
training a large-scale model with approximately 1 billion parameters on
extensive datasets. Experimental results show that our proposed method
outperforms previous approaches in both self-reenactment and cross-reenactment
tasks across several benchmarks. Additionally, the interpretable and
controllable nature of LIA-X supports practical applications such as
fine-grained, user-guided image and video editing, as well as 3D-aware portrait
video manipulation.

</details>


### [117] [January Food Benchmark (JFB): A Public Benchmark Dataset and Evaluation Suite for Multimodal Food Analysis](https://arxiv.org/abs/2508.09966)
*Amir Hosseinian,Ashkan Dehghani Zahedani,Umer Mansoor,Noosheen Hashemi,Mark Woodward*

Main category: cs.CV

TL;DR: 本文提出了JFB食物图片基准数据集和配套评测体系，并验证了专用模型在营养分析任务上的优越性。


<details>
  <summary>Details</summary>
Motivation: 当前AI自动化营养分析的发展受限于缺乏统一的评测方法和高质量的真实数据集。为了解决该难题，作者希望构建标准化的评价基准和为模型评测提供参考数据。

Method: 1）发布包含1,000张带人工标注的食物图片数据集JFB；2）设计覆盖性良好的评测体系，包括健壮的指标和面向应用的新型综合得分；3）用通用视觉-语言模型和自研专用模型（january/food-vision-v1）在新基准上进行对比实验。

Result: 自研专用模型获得86.2的综合得分，比表现最好的通用模型高出12.1分，基准体系有效区分模型表现。

Conclusion: JFB数据集和评测框架为后续营养分析AI模型的开发和评估提供了科学、公开的参考标准，能推动领域研究进展。

Abstract: Progress in AI for automated nutritional analysis is critically hampered by
the lack of standardized evaluation methodologies and high-quality, real-world
benchmark datasets. To address this, we introduce three primary contributions.
First, we present the January Food Benchmark (JFB), a publicly available
collection of 1,000 food images with human-validated annotations. Second, we
detail a comprehensive benchmarking framework, including robust metrics and a
novel, application-oriented overall score designed to assess model performance
holistically. Third, we provide baseline results from both general-purpose
Vision-Language Models (VLMs) and our own specialized model,
january/food-vision-v1. Our evaluation demonstrates that the specialized model
achieves an Overall Score of 86.2, a 12.1-point improvement over the
best-performing general-purpose configuration. This work offers the research
community a valuable new evaluation dataset and a rigorous framework to guide
and benchmark future developments in automated nutritional analysis.

</details>


### [118] [MOC: Meta-Optimized Classifier for Few-Shot Whole Slide Image Classification](https://arxiv.org/abs/2508.09967)
*Tianqi Xiang,Yi Li,Qixiang Zhang,Xiaomeng Li*

Main category: cs.CV

TL;DR: 本文提出了一种称为Meta-Optimized Classifier (MOC)的新方法，提升了病理切片图像的少样本诊断性能，超越现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 尽管基于视觉-语言基础模型（VLFM）的病理图像分析在少样本分类上有进展，但在大数据集下仍不如传统多实例学习（MIL）。少样本改进方法依赖于传统分类器，容易受到数据稀缺性的影响，因此亟需更鲁棒的分类方式。

Method: 提出“Meta-Optimized Classifier (MOC)”，包含：1）一个元学习器，自动从多个候选分类器中优化分类器结构；2）一个分类器库，包含不同类型的分类器，为病理解读提供更全面的视角。MOC通过元学习动态选择并优化适合新任务的分类器架构。

Result: 在多个少样本基准上，MOC的性能优于以往方法。在TCGA-NSCLC基准下，MOC的AUC比现有最优VLFM少样本方法高出10.4%，在1-shot极低标注下提升达26.25%。

Conclusion: MOC方法在诊断训练数据极度有限的临床场景中，显著提高了病理图像的自动诊断准确率，有望推动相关技术实际部署与应用。

Abstract: Recent advances in histopathology vision-language foundation models (VLFMs)
have shown promise in addressing data scarcity for whole slide image (WSI)
classification via zero-shot adaptation. However, these methods remain
outperformed by conventional multiple instance learning (MIL) approaches
trained on large datasets, motivating recent efforts to enhance VLFM-based WSI
classification through fewshot learning paradigms. While existing few-shot
methods improve diagnostic accuracy with limited annotations, their reliance on
conventional classifier designs introduces critical vulnerabilities to data
scarcity. To address this problem, we propose a Meta-Optimized Classifier (MOC)
comprising two core components: (1) a meta-learner that automatically optimizes
a classifier configuration from a mixture of candidate classifiers and (2) a
classifier bank housing diverse candidate classifiers to enable a holistic
pathological interpretation. Extensive experiments demonstrate that MOC
outperforms prior arts in multiple few-shot benchmarks. Notably, on the
TCGA-NSCLC benchmark, MOC improves AUC by 10.4% over the state-of-the-art
few-shot VLFM-based methods, with gains up to 26.25% under 1-shot conditions,
offering a critical advancement for clinical deployments where diagnostic
training data is severely limited. Code is available at
https://github.com/xmed-lab/MOC.

</details>


### [119] [PERSONA: Personalized Whole-Body 3D Avatar with Pose-Driven Deformations from a Single Image](https://arxiv.org/abs/2508.09973)
*Geonhee Sim,Gyeongsik Moon*

Main category: cs.CV

TL;DR: 本文提出了PERSONA，一个能够仅用一张图片生成带有动作变形能力的高逼真三维人像的方法。该方法结合了基于3D建模和扩散模型的优点，克服了仅用扩散模型身份保持性差及3D建模数据需求大的缺点。


<details>
  <summary>Details</summary>
Motivation: 现有3D人像生成方法要么要求大量带姿态变化的视频数据，获取成本高昂且实际不便；要么通过扩散模型学到的动作变形难以保证身份一致性。作者旨在解决两种方法的局限，降低数据门槛的同时获得高质量、可动画的个性化3D人像。

Method: 1. 首先用扩散模型根据输入的单张图片生成丰富姿态变化的视频；2. 基于生成的视频优化个性化三维人像；3. 引入'balanced sampling'，即在训练采样时多用原始图片，减少身份漂移；4. 引入'geometry-weighted optimization'，训练时提高几何约束的重要性，确保不同姿态下的渲染质量。

Result: 实验证明PERSONA可以仅靠一张图片生成具有多样姿态变形、身份一致性高、渲染清晰的3D人像，优于单一使用扩散模型或3D优化模型的方法。

Conclusion: PERSONA成功融合两类主流技术，实现了单张图片到可动画个性化3D人像的转换，并有效解决了身份漂移和姿态泛化的问题，推进了3D人像动画技术的实用性。

Abstract: Two major approaches exist for creating animatable human avatars. The first,
a 3D-based approach, optimizes a NeRF- or 3DGS-based avatar from videos of a
single person, achieving personalization through a disentangled identity
representation. However, modeling pose-driven deformations, such as non-rigid
cloth deformations, requires numerous pose-rich videos, which are costly and
impractical to capture in daily life. The second, a diffusion-based approach,
learns pose-driven deformations from large-scale in-the-wild videos but
struggles with identity preservation and pose-dependent identity entanglement.
We present PERSONA, a framework that combines the strengths of both approaches
to obtain a personalized 3D human avatar with pose-driven deformations from a
single image. PERSONA leverages a diffusion-based approach to generate
pose-rich videos from the input image and optimizes a 3D avatar based on them.
To ensure high authenticity and sharp renderings across diverse poses, we
introduce balanced sampling and geometry-weighted optimization. Balanced
sampling oversamples the input image to mitigate identity shifts in
diffusion-generated training videos. Geometry-weighted optimization prioritizes
geometry constraints over image loss, preserving rendering quality in diverse
poses.

</details>


### [120] [A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation](https://arxiv.org/abs/2508.09977)
*Shuting He,Peilin Ji,Yitong Yang,Changshuo Wang,Jiayi Ji,Yinglin Wang,Henghui Ding*

Main category: cs.CV

TL;DR: 本文综述了3D高斯泼洒（3DGS）在几何语义理解及下游任务中的应用，涵盖方法、最新进展及评测。


<details>
  <summary>Details</summary>
Motivation: 3DGS已经在3D场景表示和实时高保真渲染取得突破，但其在下游任务（如分割、编辑等）中的潜力仍需系统梳理，以便为学界和工业界提供技术演进全景。

Method: 首先介绍2D基础模型对3DGS语义理解和控制的支持，回顾NeRF方法为3DGS相关研究提供的思路。随后将3DGS应用分为分割、编辑、生成及其它功能，总结代表方法、监督与学习范式，并归纳设计原则与趋势。此外还总结了常用数据集与评测协议，并对主流方法在公开基准上的表现进行对比分析。

Result: 系统归纳了3DGS多个应用场景的代表性方法与性能，展示了3DGS在分割、编辑、生成等方向的进展，并对比了各方法在公开数据集上的表现，突出新趋势。

Conclusion: 3DGS作为3D场景表示的新趋势，已在多项功能任务上展现出极大潜力。文章为该领域提供了全面参考，并维护持续更新的资源库，助力进一步研究与应用。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a powerful alternative
to Neural Radiance Fields (NeRF) for 3D scene representation, offering
high-fidelity photorealistic rendering with real-time performance. Beyond novel
view synthesis, the explicit and compact nature of 3DGS enables a wide range of
downstream applications that require geometric and semantic understanding. This
survey provides a comprehensive overview of recent progress in 3DGS
applications. It first introduces 2D foundation models that support semantic
understanding and control in 3DGS applications, followed by a review of
NeRF-based methods that inform their 3DGS counterparts. We then categorize 3DGS
applications into segmentation, editing, generation, and other functional
tasks. For each, we summarize representative methods, supervision strategies,
and learning paradigms, highlighting shared design principles and emerging
trends. Commonly used datasets and evaluation protocols are also summarized,
along with comparative analyses of recent methods across public benchmarks. To
support ongoing research and development, a continually updated repository of
papers, code, and resources is maintained at
https://github.com/heshuting555/Awesome-3DGS-Applications.

</details>


### [121] [LLMC+: Benchmarking Vision-Language Model Compression with a Plug-and-play Toolkit](https://arxiv.org/abs/2508.09981)
*Chengtao Lv,Bilang Zhang,Yang Yong,Ruihao Gong,Yushi Huang,Shiqiao Gu,Jiajun Wu,Yumeng Shi,Jinyang Guo,Wenya Wang*

Main category: cs.CV

TL;DR: 该论文提出LLMC+，一个全面的视觉-语言大模型(VLM)压缩评测基准和工具包，用于系统性研究与评估不同压缩方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉-语言模型在多模态任务中表现突出，但由于视觉token序列较长和模型参数庞大，计算和存储开销极大。现有训练无关的压缩方法存在模块不可复用、评测单一、未充分结合多种技巧等三大局限。

Method: 作者提出了LLMC+基准和Plug-and-Play工具包，支持5个主流VLM家族、20多种算法，系统性地分析与评测token级和模型级压缩方法及其混合用法，关注空间、时间冗余和多轮/细粒度任务场景。

Result: 实验证明：(1) 空间与时间冗余需要不同技术路线；(2) token压缩在多轮对话和细节敏感任务中性能下降显著；(3) token与模型级联合压缩能极大减小规模而损失有限。

Conclusion: LLMC+可促进行业对高效VLM的公平评估和系统研究，为未来的模型压缩研究提供了关键支撑与参考。

Abstract: Large Vision-Language Models (VLMs) exhibit impressive multi-modal
capabilities but suffer from prohibitive computational and memory demands, due
to their long visual token sequences and massive parameter sizes. To address
these issues, recent works have proposed training-free compression methods.
However, existing efforts often suffer from three major limitations: (1)
Current approaches do not decompose techniques into comparable modules,
hindering fair evaluation across spatial and temporal redundancy. (2)
Evaluation confined to simple single-turn tasks, failing to reflect performance
in realistic scenarios. (3) Isolated use of individual compression techniques,
without exploring their joint potential. To overcome these gaps, we introduce
LLMC+, a comprehensive VLM compression benchmark with a versatile,
plug-and-play toolkit. LLMC+ supports over 20 algorithms across five
representative VLM families and enables systematic study of token-level and
model-level compression. Our benchmark reveals that: (1) Spatial and temporal
redundancies demand distinct technical strategies. (2) Token reduction methods
degrade significantly in multi-turn dialogue and detail-sensitive tasks. (3)
Combining token and model compression achieves extreme compression with minimal
performance loss. We believe LLMC+ will facilitate fair evaluation and inspire
future research in efficient VLM. Our code is available at
https://github.com/ModelTC/LightCompress.

</details>


### [122] [Story2Board: A Training-Free Approach for Expressive Storyboard Generation](https://arxiv.org/abs/2508.09983)
*David Dinkevich,Matan Levy,Omri Avrahami,Dvir Samuel,Dani Lischinski*

Main category: cs.CV

TL;DR: 本文提出了一个无需训练的Story2Board框架，可以根据自然语言自动生成富有表现力的分镜图，并在保持叙事一致性的同时提升画面多样性。


<details>
  <summary>Details</summary>
Motivation: 现有分镜生成方法过于关注角色身份，忽视了空间布局、背景变化和叙事节奏等视觉叙事关键要素，导致生成结果难以满足真实漫画需求。

Method: 提出了两个轻量化一致性机制：Latent Panel Anchoring（跨分镜保持角色参考）和Reciprocal Attention Value Mixing（在具有强互注意力的token对间柔和融合视觉特征），无需改变扩散模型结构或微调。结合语言模型，将自由叙述转化为分镜级提示词，进行多步生成。

Result: 在新提出的Rich Storyboard Benchmark和Scene Diversity指标，以及用户研究中，Story2Board生成的分镜在动态性、一致性和叙事吸引力上均超越现有方法。

Conclusion: Story2Board显著提升了分镜生成的一致性和多样性，为故事可视化提供了切实可用的解决方案，无需复杂训练即可带来较大提升。

Abstract: We present Story2Board, a training-free framework for expressive storyboard
generation from natural language. Existing methods narrowly focus on subject
identity, overlooking key aspects of visual storytelling such as spatial
composition, background evolution, and narrative pacing. To address this, we
introduce a lightweight consistency framework composed of two components:
Latent Panel Anchoring, which preserves a shared character reference across
panels, and Reciprocal Attention Value Mixing, which softly blends visual
features between token pairs with strong reciprocal attention. Together, these
mechanisms enhance coherence without architectural changes or fine-tuning,
enabling state-of-the-art diffusion models to generate visually diverse yet
consistent storyboards. To structure generation, we use an off-the-shelf
language model to convert free-form stories into grounded panel-level prompts.
To evaluate, we propose the Rich Storyboard Benchmark, a suite of open-domain
narratives designed to assess layout diversity and background-grounded
storytelling, in addition to consistency. We also introduce a new Scene
Diversity metric that quantifies spatial and pose variation across storyboards.
Our qualitative and quantitative results, as well as a user study, show that
Story2Board produces more dynamic, coherent, and narratively engaging
storyboards than existing baselines.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [123] [ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning](https://arxiv.org/abs/2508.09303)
*Shu Zhao,Tan Yu,Anbang Xu,Japinder Singh,Aaditya Shukla,Rama Akkiraju*

Main category: cs.CL

TL;DR: 本文提出了一种新的并行搜索框架ParallelSearch，使大型语言模型（LLMs）在信息检索任务中能并行处理查询，显著提升了多步推理任务的效率和准确率。


<details>
  <summary>Details</summary>
Motivation: 目前基于强化学习的推理增强搜索代理（如Search-R1）虽然能动态获取外部知识解决复杂推理任务，但面临结构性限制——只能串行处理搜索查询，无法利用查询之间的并行性，导致效率低、计算成本高。

Method: 本文提出ParallelSearch框架，通过设计激励识别查询中可并行部分的奖励函数，引导LLM将查询拆分为独立部分，并并行处理。奖励函数同时兼顾正确性、分解质量和并行执行收益。强化学习训练方法用于优化代理的性能。

Result: 在七个问答基准测试中，ParallelSearch在整体上超越了现有最优方法，平均性能提升2.9%。对于本质可并行的问题，性能提升达12.7%，同时减少了30%的LLM调用次数。

Conclusion: ParallelSearch有效克服了串行瓶颈，提升了推理效率和准确率，尤其适用于需要多实体比较的复杂查询任务，证明了识别和执行可并行查询结构的巨大价值。

Abstract: Reasoning-augmented search agents such as Search-R1, trained via
reinforcement learning with verifiable rewards (RLVR), demonstrate remarkable
capabilities in multi-step information retrieval from external knowledge
sources. These agents address the limitations of their parametric memory by
dynamically gathering relevant facts to address complex reasoning tasks.
However, existing approaches suffer from a fundamental architectural
limitation: they process search queries strictly sequentially, even when
handling inherently parallelizable and logically independent comparisons. This
sequential bottleneck significantly constrains computational efficiency,
particularly for queries that require multiple entity comparisons. To address
this critical limitation, we propose ParallelSearch, a novel reinforcement
learning framework that empowers large language models (LLMs) to recognize
parallelizable query structures and execute multiple search operations
concurrently. Our approach introduces dedicated reward functions that
incentivize the identification of independent query components while preserving
answer accuracy through jointly considering correctness, query decomposition
quality, and parallel execution benefits. Comprehensive experiments demonstrate
that ParallelSearch outperforms state-of-the-art baselines by an average
performance gain of 2.9% across seven question-answering benchmarks. Notably,
on parallelizable questions, our method achieves a 12.7% performance
improvement while requiring only 69.6% of the LLM calls compared to sequential
approaches.

</details>


### [124] [Leveraging Large Language Models for Rare Disease Named Entity Recognition](https://arxiv.org/abs/2508.09323)
*Nan Miles Xi,Yu Deng,Lin Wang*

Main category: cs.CL

TL;DR: 本研究评估了GPT-4o在罕见疾病领域下、低资源环境中的命名实体识别（NER）能力，并通过多种提示策略进行性能对比，结果显示GPT-4o可与主流生物医学模型媲美甚至超越，并在任务级微调后达到最新最优水平。


<details>
  <summary>Details</summary>
Motivation: 罕见疾病NER任务面临标注数据稀缺、实体类型语义模糊和长尾分布等挑战。现有深度学习方法对样本需求高，传统模式难以推广，因此有必要探索大语言模型（如GPT-4o）在低资源、复杂语境下的应用潜力。

Method: 作者系统评估了GPT-4o在零样本、少样本、检索增强（RAG）和任务级微调等多种提示下的NER能力，设计了结构化提示框架，融合领域知识及消歧规则，并提出两种语义引导的少样本示例选择方法以优化上下文学习效果并降低人工标注成本。

Result: 实验证明，GPT-4o在RareDis数据集上NER性能与BioClinicalBERT持平或更优。任务级微调后取得SOTA成绩。分析显示，少样本提示在低计算成本下表现优异，RAG仅带来有限益处。错误分析归纳出边界漂移、类型混淆等典型问题。

Conclusion: 经优化的提示式大模型可成为生物医学NER，尤其是罕见疾病场景下的有效、可扩展模型替代方案，并为进一步混合方法和后处理优化提供了方向。

Abstract: Named Entity Recognition (NER) in the rare disease domain poses unique
challenges due to limited labeled data, semantic ambiguity between entity
types, and long-tail distributions. In this study, we evaluate the capabilities
of GPT-4o for rare disease NER under low-resource settings, using a range of
prompt-based strategies including zero-shot prompting, few-shot in-context
learning, retrieval-augmented generation (RAG), and task-level fine-tuning. We
design a structured prompting framework that encodes domain-specific knowledge
and disambiguation rules for four entity types. We further introduce two
semantically guided few-shot example selection methods to improve in-context
performance while reducing labeling effort. Experiments on the RareDis Corpus
show that GPT-4o achieves competitive or superior performance compared to
BioClinicalBERT, with task-level fine-tuning yielding new state-of-the-art
(SOTA) results. Cost-performance analysis reveals that few-shot prompting
delivers high returns at low token budgets, while RAG offers marginal
additional benefit. An error taxonomy highlights common failure modes such as
boundary drift and type confusion, suggesting opportunities for post-processing
and hybrid refinement. Our results demonstrate that prompt-optimized LLMs can
serve as effective, scalable alternatives to traditional supervised models in
biomedical NER, particularly in rare disease applications where annotated data
is scarce.

</details>


### [125] [TEN: Table Explicitization, Neurosymbolically](https://arxiv.org/abs/2508.09324)
*Nikita Mehrotra,Aayush Kumar,Sumit Gulwani,Arjun Radhakrishna,Ashish Tiwari*

Main category: cs.CL

TL;DR: 本文提出了一种结合神经网络与符号方法的TEN系统，用于从半结构化文本中高效提取表格数据，显著优于纯神经方法。


<details>
  <summary>Details</summary>
Motivation: 在现实场景中，很多文本数据未严格按特定分隔符组织成表格，这给自动表格提取带来巨大挑战，现有的纯神经方法容易出现幻觉与格式错误问题。

Method: TEN系统采用结构性分解提示（Structural Decomposition prompting），先用大语言模型（LLM）按链式思维生成初始表格，再用符号检查器评估其格式与内容，检测幻觉与遗漏。针对问题输出，由‘批判-LLM’生成修正指导，通过自我调试循环，不断优化生成表格。

Result: TEN在多个数据集和评价指标上均显著优于纯神经基线：精确匹配率更高，幻觉率大幅降低。用户研究显示TEN方法的表格准确性平均分为5.0（基线为4.3，p=0.021），并且在超过60%的场合被用户更倾向选择，表明其易验证、易修正。

Conclusion: TEN系统有效结合了神经与符号推理，极大提升了半结构化文本到表格的提取质量，降低了幻觉并提高了可用性，展示了神经符号方法的强大优势。

Abstract: We present a neurosymbolic approach, TEN, for extracting tabular data from
semistructured input text. This task is particularly challenging for text input
that does not use special delimiters consistently to separate columns and rows.
Purely neural approaches perform poorly due to hallucinations and their
inability to enforce hard constraints. TEN uses Structural Decomposition
prompting - a specialized chain-of-thought prompting approach - on a large
language model (LLM) to generate an initial table, and thereafter uses a
symbolic checker to evaluate not only the well-formedness of that table, but
also detect cases of hallucinations or forgetting. The output of the symbolic
checker is processed by a critique-LLM to generate guidance for fixing the
table, which is presented to the original LLM in a self-debug loop. Our
extensive experiments demonstrate that TEN significantly outperforms purely
neural baselines across multiple datasets and metrics, achieving significantly
higher exact match accuracy and substantially reduced hallucination rates. A
21-participant user study further confirms that TEN's tables are rated
significantly more accurate (mean score: 5.0 vs 4.3; p = 0.021), and are
consistently preferred for ease of verification and correction, with
participants favoring our method in over 60% of the cases.

</details>


### [126] [Decoding Neural Emotion Patterns through Natural Language Processing Embeddings](https://arxiv.org/abs/2508.09337)
*Gideon Vos,Maryam Ebrahimpour,Liza van Eijk,Zoltan Sarnyai,Mostafa Rahimi Azghadi*

Main category: cs.CL

TL;DR: 本文提出了一种无需神经影像的计算框架，通过分析文本情感内容并将其映射到特定大脑区域，实现对自然语言中情感表达和脑功能的关联分析。该方法可用于健康人群与抑郁症患者对比、自发文本与AI文本情感大脑区域活跃度比较，具有低成本、高效且可扩展的优点。


<details>
  <summary>Details</summary>
Motivation: 情感通过语言表达与脑功能之间的关系尚不明晰，传统神经影像方法代价高昂且受限于实验室环境。海量数字文本提供了低成本分析的可能，但当前研究多将神经影像和文本分析分开，缺乏系统整合。因此，迫切需要一种能利用文本数据间接推断大脑情感区域活动的有效方法。

Method: 作者提出了一种新方法：首先利用OpenAI的text-embedding-ada-002模型对文本进行高维语义嵌入，继而通过降维与聚类识别情感类别，并将之映射到18个与情感加工有关的大脑解剖区域。具体进行了三项实验：1）用DIAC-WOZ数据集对比健康人与抑郁人群的情感-脑区映射，2）测试GoEmotions情感数据集，3）比较人类文本与大语言模型生成文本的脑区推断表现。情感强度则通过词汇分析获得。

Result: 方法实现了空间特异性高、神经解剖学合理的情感-脑区映射，且能区分出离散情感类别。抑郁者表现出更多与消极情感相关的边缘系统活动。人类与LLM文本在基础情感分布相似，但LLM在共情和自我相关脑区（如内侧前额叶皮层和后扣带皮层）的激活表现不如人类。

Conclusion: 该框架成本低、可扩展，适用于大规模自然语言情感脑网络分析、临床群体区分，并能作为AI情感表达的脑功能基准。

Abstract: Understanding how emotional expression in language relates to brain function
is a challenge in computational neuroscience and affective computing.
Traditional neuroimaging is costly and lab-bound, but abundant digital text
offers new avenues for emotion-brain mapping. Prior work has largely examined
neuroimaging-based emotion localization or computational text analysis
separately, with little integration. We propose a computational framework that
maps textual emotional content to anatomically defined brain regions without
requiring neuroimaging. Using OpenAI's text-embedding-ada-002, we generate
high-dimensional semantic representations, apply dimensionality reduction and
clustering to identify emotional groups, and map them to 18 brain regions
linked to emotional processing. Three experiments were conducted: i) analyzing
conversational data from healthy vs. depressed subjects (DIAC-WOZ dataset) to
compare mapping patterns, ii) applying the method to the GoEmotions dataset and
iii) comparing human-written text with large language model (LLM) responses to
assess differences in inferred brain activation. Emotional intensity was scored
via lexical analysis. Results showed neuroanatomically plausible mappings with
high spatial specificity. Depressed subjects exhibited greater limbic
engagement tied to negative affect. Discrete emotions were successfully
differentiated. LLM-generated text matched humans in basic emotion distribution
but lacked nuanced activation in empathy and self-referential regions (medial
prefrontal and posterior cingulate cortex). This cost-effective, scalable
approach enables large-scale analysis of naturalistic language, distinguishes
between clinical populations, and offers a brain-based benchmark for evaluating
AI emotional expression.

</details>


### [127] [The Human-AI Hybrid Delphi Model: A Structured Framework for Context-Rich, Expert Consensus in Complex Domains](https://arxiv.org/abs/2508.09349)
*Cathy Speed,Ahmed A. Metwally*

Main category: cs.CL

TL;DR: 该论文提出并验证了一种人机协作的Delphi专家共识生成方法（HAH-Delphi），通过结合大语言模型和小规模人类专家小组，提升了专家共识制定的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 当前专家共识生成方法面临证据分散、信息过载、解释过度简化和难以保持情境细腻度等问题。传统方法如Delphi研究和共识会议存在高成本、负担大、易忽视条件细节等短板。这些挑战被信息泛滥和证据碎片化进一步放大，因此亟需更高效、可扩展的专家共识生成方法。

Method: 引入了“人机混合Delphi”框架（HAH-Delphi），将Gemini 2.5 Pro生成式AI、资深专家小组和结构化引导相结合。该框架分三阶段测试：回顾性AI共识复现、与人类专家对比、在耐力及混合训练领域实际应用。评估共识一致性、效率和情境覆盖度。

Result: AI在第一阶段95%复现已发表的专家共识结论，第二阶段与专家保持95%趋势一致性，但缺乏经验性细节。实际应用中，6人专家组能达到90%以上共识覆盖及主题饱和，AI为共识过程提供了稳定且基于文献的支持，加快异议解决和共识达成。

Conclusion: HAH-Delphi框架是一种灵活、可扩展的专家共识生成工具，适用于健康、教练与运动科学领域，并可支持生成个性化和有条件的共识建议。其方法学稳健性和实用性已获实际验证。

Abstract: Expert consensus plays a critical role in domains where evidence is complex,
conflicting, or insufficient for direct prescription. Traditional methods, such
as Delphi studies, consensus conferences, and systematic guideline synthesis,
offer structure but face limitations including high panel burden, interpretive
oversimplification, and suppression of conditional nuance. These challenges are
now exacerbated by information overload, fragmentation of the evidence base,
and increasing reliance on publicly available sources that lack expert
filtering. This study introduces and evaluates a Human-AI Hybrid Delphi
(HAH-Delphi) framework designed to augment expert consensus development by
integrating a generative AI model (Gemini 2.5 Pro), small panels of senior
human experts, and structured facilitation. The HAH-Delphi was tested in three
phases: retrospective replication, prospective comparison, and applied
deployment in two applied domains (endurance training and resistance and mixed
cardio/strength training). The AI replicated 95% of published expert consensus
conclusions in Phase I and showed 95% directional agreement with senior human
experts in Phase II, though it lacked experiential and pragmatic nuance. In
Phase III, compact panels of six senior experts achieved >90% consensus
coverage and reached thematic saturation before the final participant. The AI
provided consistent, literature-grounded scaffolding that supported divergence
resolution and accelerated saturation. The HAH-Delphi framework offers a
flexible, scalable approach for generating high-quality, context-sensitive
consensus. Its successful application across health, coaching, and performance
science confirms its methodological robustness and supports its use as a
foundation for generating conditional, personalised guidance and published
consensus frameworks at scale.

</details>


### [128] [Flow-SLM: Joint Learning of Linguistic and Acoustic Information for Spoken Language Modeling](https://arxiv.org/abs/2508.09350)
*Ju-Chieh Chou,Jiawei Zhou,Karen Livescu*

Main category: cs.CL

TL;DR: 本文提出了一种结合语言和声学信息的textless spoken language model（SLM），通过同时生成语义token和连续声学向量，提高了生成语音的细腻程度。


<details>
  <summary>Details</summary>
Motivation: 当前没有文本监督的SLM大多只预测离散的语义token，由单独的vocoder补充声学信息，导致无法利用声学上下文，也缺乏对声学细节的掌控。

Method: 作者提出联合建模的思路，模型同时生成语义token和连续的声学表示。采用flow-matching方法，在已知语义token的情况下预测声学连续向量，并探索多预测未来语义token对模型表现的影响。

Result: 实验表明，联合预测多个语义token有助于语言信息的保留。模型在语言基准测试上与现有方法表现相当，但在声学细节还原上表现更好。

Conclusion: 联合建模语言和声学信息能够弥补单一token建模的不足，提高生成语音的自然度和细节丰富度。

Abstract: Textless spoken language models (SLMs) are generative models of speech that
do not rely on text supervision. Most textless SLMs learn to predict the next
semantic token, a discrete representation of linguistic content, and rely on a
separate vocoder to add acoustic information to the generated speech. Such
models have no access to acoustic context and no built-in control over acoustic
details. In this work, we propose to jointly model linguistic and acoustic
information by generating semantic tokens and a continuous real-valued
representation of the acoustic frame. We use a flow-matching objective to
predict the continuous vector conditioned on the semantic tokens. We study the
design space of this approach and find that predicting multiple future semantic
tokens helps preserve linguistic information. Our approach achieves comparable
performance to existing models in terms of linguistic likelihood benchmarks,
while providing better acoustic detail in prompted generation.

</details>


### [129] [APIO: Automatic Prompt Induction and Optimization for Grammatical Error Correction and Text Simplification](https://arxiv.org/abs/2508.09378)
*Artem Chernodub,Aman Saini,Yejin Huh,Vivek Kulkarni,Vipul Raheja*

Main category: cs.CL

TL;DR: 本文提出了一种无需人工指定初始提示词（seed prompt）的自动提示词归纳与优化方法（APIO），用于提升LLM在语法纠错和文本简化任务上的表现，并在同类方法中取得了最新最优效果。


<details>
  <summary>Details</summary>
Motivation: 尽管自动提示词优化（APO）能够根据明确的评价指标优化模型表现，但大多需要一个手工编写的初始提示词，这限制了其通用性和便捷性。因此，亟需一种无需手动提示词，且同样高效的优化方法。

Method: 作者提出APIO方法，通过算法自动归纳并优化提示词，无需人为指定初始提示词。APIO专注于语法纠错（GEC）和文本简化两项任务，直接与当前顶尖LLM提示方法对比。

Result: APIO在GEC和文本简化任务上表现出色，刷新了现有仅依赖LLM提示的方法的最优性能。

Conclusion: APIO验证了无需人工指定初始提示词也能自动归纳并优化出高效的提示方法，为LLM在NLP任务中的推广应用和自动化开发提供了新路径。

Abstract: Recent advancements in large language models (LLMs) have enabled a wide range
of natural language processing (NLP) tasks to be performed through simple
prompt-based interactions. Consequently, several approaches have been proposed
to engineer prompts that most effectively enable LLMs to perform a given task
(e.g., chain-of-thought prompting). In settings with a well-defined metric to
optimize model performance, automatic prompt optimization (APO) methods have
been developed to refine a seed prompt. Advancing this line of research, we
propose APIO, a simple but effective prompt induction and optimization approach
for the tasks of Grammatical Error Correction (GEC) and Text Simplification,
without relying on manually specified seed prompts. APIO achieves a new
state-of-the-art performance for purely LLM-based prompting methods on these
tasks. We make our data, code, prompts, and outputs publicly available.

</details>


### [130] [Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large Language Models](https://arxiv.org/abs/2508.09403)
*Ting Cai,Stephen Sheen,AnHai Doan*

Main category: cs.CL

TL;DR: 本文提出了Columbo，一种基于大模型的表格列名扩展方法，通过新数据集和改进的评估指标，显著提升了缩写扩展的准确率。


<details>
  <summary>Details</summary>
Motivation: 表格列名常以缩写形式出现（如“esal”代表“employee salary”），这为后续数据处理、分析等工作带来挑战，因此亟需提高缩写扩展的准确度。

Method: 1）指出现有数据集的局限，创建了4个更贴近企业与科研实际的新数据集；2）改进准确率评估方式，引入同义词感知的指标，使其更贴合真实正确扩展；3）提出Columbo方法，结合上下文、规则、链式推理和细粒度分析，并基于大型语言模型实现。

Result: 实验显示，在5个数据集上，Columbo比当前最先进的NameGuess方法准确率提升4-29%。Columbo已在重要的环境科学数据门户实际部署应用。

Conclusion: Columbo方法在现实应用和准确评估上均大幅优于现有方案，能够为企业、科研等领域提升表格数据处理智能化水平。

Abstract: Expanding the abbreviated column names of tables, such as ``esal'' to
``employee salary'', is critical for numerous downstream data tasks. This
problem arises in enterprises, domain sciences, government agencies, and more.
In this paper we make three contributions that significantly advances the state
of the art. First, we show that synthetic public data used by prior work has
major limitations, and we introduce 4 new datasets in enterprise/science
domains, with real-world abbreviations. Second, we show that accuracy measures
used by prior work seriously undercount correct expansions, and we propose new
synonym-aware measures that capture accuracy much more accurately. Finally, we
develop Columbo, a powerful LLM-based solution that exploits context, rules,
chain-of-thought reasoning, and token-level analysis. Extensive experiments
show that Columbo significantly outperforms NameGuess, the current most
advanced solution, by 4-29\%, over 5 datasets. Columbo has been used in
production on EDI, a major data portal for environmental sciences.

</details>


### [131] [Leveraging Zipformer Model for Effective Language Identification in Code-Switched Child-Directed Speech](https://arxiv.org/abs/2508.09430)
*Lavanya Shankar,Leibny Paola Garcia Perera*

Main category: cs.CL

TL;DR: 本论文利用Zipformer模型改善了对儿童指向性场景下中英混合语音中的语言识别，尤其在数据不均衡时显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 在儿童指向的双语环境下，语音中的多语言切换现象普遍存在，传统语言识别面临挑战，尤其是混合和数据分布不均衡的问题。

Method: 作者采用Zipformer模型来处理包含中英两种不均衡语言的语音，并探索其内部层对语言特征的编码能力。通过选择不同的内部层提取嵌入向量，并与不同后端进行比较分析。

Result: 实验显示Zipformer在各种后端下都表现出稳健性，在语言识别任务上取得了81.89%的平衡准确率（BAC），比基线方法提升了15.47%。

Conclusion: Zipformer等Transformer编码器结构在实际混合语言语音场景下具有较好应用潜力，特别适合处理不均衡语言分布等复杂情况。

Abstract: Code-switching and language identification in child-directed scenarios
present significant challenges, particularly in bilingual environments. This
paper addresses this challenge by using Zipformer to handle the nuances of
speech, which contains two imbalanced languages, Mandarin and English, in an
utterance. This work demonstrates that the internal layers of the Zipformer
effectively encode the language characteristics, which can be leveraged in
language identification. We present the selection methodology of the inner
layers to extract the embeddings and make a comparison with different
back-ends. Our analysis shows that Zipformer is robust across these backends.
Our approach effectively handles imbalanced data, achieving a Balanced Accuracy
(BAC) of 81.89%, a 15.47% improvement over the language identification
baseline. These findings highlight the potential of the transformer encoder
architecture model in real scenarios.

</details>


### [132] [From Charts to Fair Narratives: Uncovering and Mitigating Geo-Economic Biases in Chart-to-Text](https://arxiv.org/abs/2508.09450)
*Ridwan Mahbub,Mohammed Saidul Islam,Mir Tafseer Nayeem,Md Tahmid Rahman Laskar,Mizanur Rahman,Shafiq Joty,Enamul Hoque*

Main category: cs.CL

TL;DR: 本文分析了大型视觉-语言模型（VLM）在生成图表摘要时所产生的地缘经济偏见，发现高收入国家常被更积极地描述，现有消偏方法效果有限。


<details>
  <summary>Details</summary>
Motivation: 随着VLM技术的进步，图表到文本自动生成受到关注，但学界较少关注模型输出中可能隐藏的地缘经济偏见。模型若对不同经济水平国家展现系统性态度差异，可能引发社会层面的问题。

Method: 作者构建了6,000组图表-国家样本，涵盖六个主流专有和开源VLM，对比分析更换国家变量时模型生成的摘要情感变化，并尝试使用推理时基于提示的消偏技巧（如加入积极干扰项）来缓解偏见。

Result: 分析发现主流VLM对高收入国家生成的描述更为积极，且即使只更换国家属性也会出现明显情感偏差。不同模型如GPT-4o-mini、Gemini-1.5-Flash和Phi-3.5表现出不同程度的偏见。所用推理阶段的消偏方法作用有限。

Conclusion: 目前VLM存在显著的地缘经济偏见，简单的推理时消偏办法难以充分解决，未来需要更有效的消偏策略以减轻潜在的社会危害。

Abstract: Charts are very common for exploring data and communicating insights, but
extracting key takeaways from charts and articulating them in natural language
can be challenging. The chart-to-text task aims to automate this process by
generating textual summaries of charts. While with the rapid advancement of
large Vision-Language Models (VLMs), we have witnessed great progress in this
domain, little to no attention has been given to potential biases in their
outputs. This paper investigates how VLMs can amplify geo-economic biases when
generating chart summaries, potentially causing societal harm. Specifically, we
conduct a large-scale evaluation of geo-economic biases in VLM-generated chart
summaries across 6,000 chart-country pairs from six widely used proprietary and
open-source models to understand how a country's economic status influences the
sentiment of generated summaries. Our analysis reveals that existing VLMs tend
to produce more positive descriptions for high-income countries compared to
middle- or low-income countries, even when country attribution is the only
variable changed. We also find that models such as GPT-4o-mini,
Gemini-1.5-Flash, and Phi-3.5 exhibit varying degrees of bias. We further
explore inference-time prompt-based debiasing techniques using positive
distractors but find them only partially effective, underscoring the complexity
of the issue and the need for more robust debiasing strategies. Our code and
dataset are publicly available here.

</details>


### [133] [User-centric Subjective Leaderboard by Customizable Reward Modeling](https://arxiv.org/abs/2508.09463)
*Qi Jia,Xiujie Song,Zicheng Zhang,Yijin Guo,Kaiwei Zhang,Zijian Chen,Guangtao Zhai*

Main category: cs.CL

TL;DR: 本文提出了首个以用户偏好为驱动的大语言模型（LLM）主观排行榜USL，并通过自定义奖励模型（CRM）显著提升了模型在多元场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评测基准大多侧重于可检验任务，导致选型时实用性有限，无法反映用户真实需求，亟需引入主观性和用户偏好。

Method: 作者收集了超过1万条真实用户主观偏好数据，发现其中存在极大多样性和矛盾，并提出了可定制奖励模型（CRM），以根据用户查询情况自适应调整评分。

Result: CRM模型仅有40亿参数却超越了GPT-4.1和Gemini-2.5-pro，并能在新话题和新标准下展现出卓越的泛化能力。USL排行榜基于CRM，有效反映了用户偏好的多样性和矛盾性。

Conclusion: 通过USL和CRM，可以为用户提供更具实际参考价值的LLM选择参考，显著提升模型评测的实用性和个性化能力。

Abstract: Existing benchmarks for large language models (LLMs) predominantely focus on
assessing their capabilities through verifiable tasks. Such objective and
static benchmarks offer limited utility for practical LLM selection, making it
difficult for users to find suitable models for their individual needs. To
bridge this gap, we present the first User-Centric Subjective Leaderboard
(USL), which provides a preference-driven, dynamic ranking of LLMs across
diverse real-world scenarios. Our work is built upon a thorough investigation
of real human preference data, involving more than 10K subjective queries. Our
investigation reveals significant diversity and contradictions in human
preferences, which limit the effectiveness of state-of-the-art reward models.
To address this, we introduce Customizable Reward Models (CRMs). With only 4B
parameters, our CRM surpasses the performance of leading models such as GPT-4.1
and Gemini-2.5-pro, showing exceptional generalization capabilities across new
topics and criteria. The USL, powered by CRMs, exhibits strong negative
correlations to contradictory preferences.

</details>


### [134] [Learning Facts at Scale with Active Reading](https://arxiv.org/abs/2508.09494)
*Jessy Lin,Vincent-Pierre Berges,Xilun Chen,Wen-Tau Yih,Gargi Ghosh,Barlas Oğuz*

Main category: cs.CL

TL;DR: 提出了Active Reading框架，通过自生成学习策略显著提升大模型对知识的吸收能力，并在多个事实问答基准测试上大幅优于常规finetune。


<details>
  <summary>Details</summary>
Motivation: 现有大模型从参数记忆中学习和提取知识表现不稳定，且缺乏可靠工具帮助确保模型学习特定知识体系。

Method: 提出Active Reading方法，让模型以主动自我生成学习策略研读材料，比传统finetune和数据增强方法更有效；并在专家领域及大规模预训练上进行实验评估。

Result: Active Reading显著提升模型在专家领域吸收知识的能力：模型在Wikipedia SimpleQA子集得分从finetuning基线提升313%，FinanceBench提升160%。基于此发布了WikiExpert-8B小模型，其事实问答表现超过了更大规模模型。

Conclusion: Active Reading在提升大模型知识获取和事实回答能力表现突出，为训练更可靠、更具事实基础的模型提供了新路径。

Abstract: LLMs are known to store vast amounts of knowledge in their parametric memory.
However, learning and recalling facts from this memory is known to be
unreliable, depending largely on the prevalence of particular facts in the
training data and other factors which are poorly understood. Practitioners are
lacking tools which will allow them to ensure that the models learn a given
body of knowledge reliably and consistently. To this end, we propose Active
Reading: a framework where we train models to study a given set of material
with self-generated learning strategies. First, we demonstrate models trained
with Active Reading on expert domains absorb significantly more knowledge than
vanilla finetuning and other data augmentations. We train expert 8B models that
achieve 66% on a Wikipedia-grounded subset of SimpleQA (+313% relative over
vanilla finetuning) and 26% on FinanceBench (+160% relative over vanilla
finetuning) by applying Active Reading to the source documents for each
benchmark. Finally, we show that Active Reading can be utilized at pre-training
scale to build more factual models. As a demonstration of this, we release Meta
WikiExpert-8B, a Wikipedia-expert model trained on 1 trillion generated tokens,
which outcompetes models with hundreds of billions of parameters on factual QA.

</details>


### [135] [From Ranking to Selection: A Simple but Efficient Dynamic Passage Selector for Retrieval Augmented Generation](https://arxiv.org/abs/2508.09497)
*Siyuan Meng,Junming Liu,Yirong Chen,Song Mao,Pinlong Cai,Guohang Yan,Botian Shi,Ding Wang*

Main category: cs.CL

TL;DR: 提出了动态文段选择器（DPS），解决RAG系统中多跳推理时固定Top-K选取的弊端，并大幅提升了复杂场景下的检索增强生成表现。


<details>
  <summary>Details</summary>
Motivation: RAG系统常因重排序模块瓶颈，特别是在需要多文档综合推理的复杂查询下，固定Top-K方法难以兼顾召回关键证据与去除噪声信息。

Method: 提出Dynamic Passage Selector（DPS），将文段选择建模为有监督学习任务，通过细致捕捉文段间依赖，实现动态选取，与现有RAG流水线无缝集成。

Result: 在五项基准上，DPS一致优于当前最新的重排序与微调方法，特别是在MuSiQue数据集上，分别比Qwen3-reranker与RankingGPT提升F1-score达30.06%和15.4%。

Conclusion: DPS通过自适应证据选择，显著增强了RAG系统在复杂场景中的推理与生成能力。

Abstract: Retrieval-augmented generation (RAG) systems are often bottlenecked by their
reranking modules, which typically score passages independently and select a
fixed Top-K size. This approach struggles with complex multi-hop queries that
require synthesizing evidence across multiple documents, creating a trade-off
where small K values omit crucial information and large K values introduce
noise. To address this, we introduce the Dynamic Passage Selector (DPS), a
novel reranking framework that treats passage selection as a supervised
learning problem. Unlike traditional point-wise or list-wise methods, DPS is
fine-tuned to capture inter-passage dependencies and dynamically select the
most relevant set of passages for generation. As a seamless plug-and-play
module, DPS requires no modifications to the standard RAG pipeline.
Comprehensive evaluations on five benchmarks show that DPS consistently
outperforms state-of-the-art rerankers and fine-tuning methods. Notably, on the
challenging MuSiQue dataset, DPS improves the F1-score by 30.06% and 15.4% over
strong baselines like Qwen3-reranker and RankingGPT, respectively. Our results
demonstrate that by enabling adaptive evidence selection, DPS substantially
enhances reasoning capabilities in complex RAG scenarios.

</details>


### [136] [LACA: Improving Cross-lingual Aspect-Based Sentiment Analysis with LLM Data Augmentation](https://arxiv.org/abs/2508.09515)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 该论文提出一种利用大语言模型（LLM）生成高质量伪标签数据，实现无需翻译工具的跨语言细粒度情感分析（ABSA）方法，实验结果在多种语言和模型上均超过现有翻译型方法。


<details>
  <summary>Details</summary>
Motivation: 传统跨语言ABSA方法依赖于容易出错的翻译工具，影响分析的准确性。为克服这一不足，研究者希望找到一种摆脱翻译环节、提升伪标签质量的新方法。

Method: 先在目标语言的无标签数据上用现有ABSA模型做预测，随后通过LLM生成更加自然且代表模型输出的句子，构造高质量伪标签数据，最后用这些数据对ABSA模型进行微调。该框架不依赖翻译工具，并支持不同主干与生成式模型。

Result: 在六种语言和五个主干模型上，所提方法均显著优于以往基于翻译的SOTA方法。微调后的LLM性能优于小型多语言模型。

Conclusion: 该框架可有效提升跨语言ABSA任务的表现，特别是在不依赖翻译工具的情况下展现出强大优势，为相关任务提供新方向。

Abstract: Cross-lingual aspect-based sentiment analysis (ABSA) involves detailed
sentiment analysis in a target language by transferring knowledge from a source
language with available annotated data. Most existing methods depend heavily on
often unreliable translation tools to bridge the language gap. In this paper,
we propose a new approach that leverages a large language model (LLM) to
generate high-quality pseudo-labelled data in the target language without the
need for translation tools. First, the framework trains an ABSA model to obtain
predictions for unlabelled target language data. Next, LLM is prompted to
generate natural sentences that better represent these noisy predictions than
the original text. The ABSA model is then further fine-tuned on the resulting
pseudo-labelled dataset. We demonstrate the effectiveness of this method across
six languages and five backbone models, surpassing previous state-of-the-art
translation-based approaches. The proposed framework also supports generative
models, and we show that fine-tuned LLMs outperform smaller multilingual
models.

</details>


### [137] [Cross-lingual Aspect-Based Sentiment Analysis: A Survey on Tasks, Approaches, and Challenges](https://arxiv.org/abs/2508.09516)
*Jakub Šmíd,Pavel Král*

Main category: cs.CL

TL;DR: 本文综述了跨语言层面的细粒度情感分析（ABSA）领域，涵盖任务定义、数据集、建模方法及未来趋势。


<details>
  <summary>Details</summary>
Motivation: 尽管ABSA领域取得了长足进展，大多数研究集中在单语环境。跨语言ABSA，尤其是从资源丰富语言向资源稀缺语言的知识迁移，尚未得到系统性梳理。本文旨在填补该领域综述的空白。

Method: 作者通过系统回顾和分析，梳理了ABSA的核心任务（如方面术语抽取、情感分类及复合任务），总结了相关数据集、建模范式及跨语言迁移方法。同时，还分析了单语、多语及大型语言模型（LLMs）相关工作的交叉贡献。

Result: 本文为跨语言ABSA领域构建了知识体系，系统梳理了任务、方法和数据资源，分析了当前进展和存在的挑战。

Conclusion: 本文指出了跨语言ABSA的主要挑战，并提出了未来研究方向，旨在推动该领域进一步发展。

Abstract: Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task that focuses on understanding opinions at the aspect level, including
sentiment towards specific aspect terms, categories, and opinions. While ABSA
research has seen significant progress, much of the focus has been on
monolingual settings. Cross-lingual ABSA, which aims to transfer knowledge from
resource-rich languages (such as English) to low-resource languages, remains an
under-explored area, with no systematic review of the field. This paper aims to
fill that gap by providing a comprehensive survey of cross-lingual ABSA. We
summarize key ABSA tasks, including aspect term extraction, aspect sentiment
classification, and compound tasks involving multiple sentiment elements.
Additionally, we review the datasets, modelling paradigms, and cross-lingual
transfer methods used to solve these tasks. We also examine how existing work
in monolingual and multilingual ABSA, as well as ABSA with LLMs, contributes to
the development of cross-lingual ABSA. Finally, we highlight the main
challenges and suggest directions for future research to advance cross-lingual
ABSA systems.

</details>


### [138] [UWBa at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2508.09517)
*Ladislav Lenc,Daniel Cífka,Jiří Martínek,Jakub Šmíd,Pavel Král*

Main category: cs.CL

TL;DR: 本文提出了一种用于事实核查型声明检索的零样本系统，通过结合多种先进大语言模型的文本嵌入取得了较好的排名。


<details>
  <summary>Details</summary>
Motivation: 面对不同语言和结构的事实核查任务，现有多语言模型效果不佳，需开发能跨语言、高效检索事实声明的新方法。

Method: 利用多种现有的大语言模型（如NV-Embed-v2、GPT、Mistral）生成文本嵌入，均以英文翻译文本作为输入，通过余弦相似度匹配相关声明，并实验多模型组合。

Result: 在英文单语任务中取得第7名，在跨语言任务中取得第9名。NV-Embed-v2单模型表现最好，部分语言通过多模型组合提升效果。

Conclusion: 采用先进的大语言模型并通过多模型组合，可在事实核查声明检索任务中取得有竞争力的结果，特别是在使用英文翻译和特定模型组合时效果最佳。

Abstract: This paper presents a zero-shot system for fact-checked claim retrieval. We
employed several state-of-the-art large language models to obtain text
embeddings. The models were then combined to obtain the best possible result.
Our approach achieved 7th place in monolingual and 9th in cross-lingual
subtasks. We used only English translations as an input to the text embedding
models since multilingual models did not achieve satisfactory results. We
identified the most relevant claims for each post by leveraging the embeddings
and measuring cosine similarity. Overall, the best results were obtained by the
NVIDIA NV-Embed-v2 model. For some languages, we benefited from model
combinations (NV-Embed & GPT or Mistral).

</details>


### [139] [COMPEER: Controllable Empathetic Reinforcement Reasoning for Emotional Support Conversation](https://arxiv.org/abs/2508.09521)
*Yunxiao Wang,Meng Liu,Wenqi Liu,Kaiyu Jiang,Bin Wen,Fan Yang,Tingting Gao,Guorui Zhou,Liqiang Nie*

Main category: cs.CL

TL;DR: 本文提出了一种可控的移情推理模型，将自然语言推理与结构化的心理学步骤相结合，有效提升了情感支持对话的能力。


<details>
  <summary>Details</summary>
Motivation: 当前情感支持对话模型缺乏基于心理学原理的深度移情推理，难以提供更具人性化和理解力的情感支持。

Method: 作者构建了一套精细标注的数据集，对推理正确性和回应偏好进行注释。采用基于过程-结果统一奖励的强化学习，提升训练效果。同时，引入基于人格的对话重写和冗余感知奖励重加权策略以减少回复重复。

Result: 实验表明，该方法显著提升了模型在情感支持任务中的表现，更好地实现了移情和人性化。

Conclusion: 该方法推动了人性化支持系统的发展，让情感支持对话更加具有同理心和心理学依据。

Abstract: Emotional support conversations are crucial for promoting emotional
well-being, yet current models often lack deep empathetic reasoning grounded in
psychological principles. To address this, we propose controllable empathetic
reasoning, which combines natural language reasoning with structured
psychological steps. We construct a fine-grained dataset annotated with
reasoning correctness and response preferences to enable this capability. To
further enhance training, we employ reinforcement learning with a unified
process-outcome reward model that delivers precise feedback. To mitigate
response repetitiveness from entropy collapse, we introduce personality-based
dialogue rewriting and a redundancy-aware reward reweighting strategy. Our
approach significantly improves model's emotional support ability, advancing
the development of empathetic, human-like support systems.

</details>


### [140] [The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage](https://arxiv.org/abs/2508.09603)
*Skyler Hallinan,Jaehun Jung,Melanie Sclar,Ximing Lu,Abhilasha Ravichander,Sahana Ramnath,Yejin Choi,Sai Praneeth Karimireddy,Niloofar Mireshghallah,Xiang Ren*

Main category: cs.CL

TL;DR: 本文提出了一种新型的成员关系推断攻击方法——N-Gram Coverage Attack，仅依赖于目标语言模型的文本输出（而非概率分布或隐藏状态），能有效攻击黑盒API模型如GPT-4，并在多个基准上超越其他黑盒方法，甚至接近或优于最新白盒方法。


<details>
  <summary>Details</summary>
Motivation: 目前成员关系推断攻击在隐私保护和责任追溯等方面有重要应用，但主流方法大多依赖模型内部信息，难以应用于仅提供API黑盒访问的主流大模型。因此亟需能适用于黑盒场景的强成员关系推断工具。

Method: N-Gram Coverage Attack对候选样本，先取其前缀，让模型生成多个后缀，再通过n-gram重叠度来衡量模型输出与真实后缀的相似性，相似度高则推断该样本在训练集中。该方法仅需模型文本输出，无需概率分布或内部状态。

Result: 在多项公开基准上，该方法明显优于其他黑盒攻击方法，且在部分情况下可达到甚至优于现有白盒方法。同时，该方法性能随生成样本数的提升而增强。

Conclusion: N-Gram Coverage Attack不仅提升了对API黑盒模型的推断攻击能力，也揭示了最新如GPT-4o等模型在成员推断面前的稳健性提升，暗示大型语言模型的隐私保护已在持续进步。

Abstract: Membership inference attacks serves as useful tool for fair use of language
models, such as detecting potential copyright infringement and auditing data
leakage. However, many current state-of-the-art attacks require access to
models' hidden states or probability distribution, which prevents investigation
into more widely-used, API-access only models like GPT-4. In this work, we
introduce N-Gram Coverage Attack, a membership inference attack that relies
solely on text outputs from the target model, enabling attacks on completely
black-box models. We leverage the observation that models are more likely to
memorize and subsequently generate text patterns that were commonly observed in
their training data. Specifically, to make a prediction on a candidate member,
N-Gram Coverage Attack first obtains multiple model generations conditioned on
a prefix of the candidate. It then uses n-gram overlap metrics to compute and
aggregate the similarities of these outputs with the ground truth suffix; high
similarities indicate likely membership. We first demonstrate on a diverse set
of existing benchmarks that N-Gram Coverage Attack outperforms other black-box
methods while also impressively achieving comparable or even better performance
to state-of-the-art white-box attacks - despite having access to only text
outputs. Interestingly, we find that the success rate of our method scales with
the attack compute budget - as we increase the number of sequences generated
from the target model conditioned on the prefix, attack performance tends to
improve. Having verified the accuracy of our method, we use it to investigate
previously unstudied closed OpenAI models on multiple domains. We find that
more recent models, such as GPT-4o, exhibit increased robustness to membership
inference, suggesting an evolving trend toward improved privacy protections.

</details>


### [141] [AINL-Eval 2025 Shared Task: Detection of AI-Generated Scientific Abstracts in Russian](https://arxiv.org/abs/2508.09622)
*Tatiana Batura,Elena Bruches,Milana Shvenk,Valentin Malykh*

Main category: cs.CL

TL;DR: 本文介绍了AINL-Eval 2025共享任务，该任务旨在检测俄语科学摘要中的AI生成内容，推动学术诚信检测研究。研究团队构建了一个包含5种先进LLM生成内容与12个科学领域人工摘要的大型数据集，吸引了多支团队参与，顶尖系统表现优秀。任务平台和数据集已开放用于持续学术探索。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在文本生成领域取得巨大进步，AI生成内容与人类撰写内容愈发难以区分，严重威胁学术诚信，尤其是在AI检测资源稀缺的多语言环境下。因此，亟需构建专门的数据集与评测平台，推动AI生成内容检测能力的提升，保障学术发布的真实性与可信度。

Method: 研究者提出了AINL-Eval 2025共享任务：收集和整理了5种先进LLM生成的摘要及12个科学领域人写摘要，共计52,305个样本。比赛设计要求参赛者研发能适应新领域和未见模型的通用检测方案，分两阶段举行，多支队伍参与并提交了多次方案。评测平台和数据集全部开放。

Result: 本次评测共有10支团队、159次提交，顶尖方案在AI生成内容识别方面表现优异，任务有效推动了AI检测方法的进步。同时建立了可持续运行的共享任务平台，为后续相关研究打下基础。

Conclusion: AINL-Eval 2025为AI生成内容检测研究提供了权威数据集与开放平台，在俄语及科学出版领域推动了学术诚信保护。其持续性的设计有望促进该领域长期进步并增强实际检测能力。

Abstract: The rapid advancement of large language models (LLMs) has revolutionized text
generation, making it increasingly difficult to distinguish between human- and
AI-generated content. This poses a significant challenge to academic integrity,
particularly in scientific publishing and multilingual contexts where detection
resources are often limited. To address this critical gap, we introduce the
AINL-Eval 2025 Shared Task, specifically focused on the detection of
AI-generated scientific abstracts in Russian. We present a novel, large-scale
dataset comprising 52,305 samples, including human-written abstracts across 12
diverse scientific domains and AI-generated counterparts from five
state-of-the-art LLMs (GPT-4-Turbo, Gemma2-27B, Llama3.3-70B, Deepseek-V3, and
GigaChat-Lite). A core objective of the task is to challenge participants to
develop robust solutions capable of generalizing to both (i) previously unseen
scientific domains and (ii) models not included in the training data. The task
was organized in two phases, attracting 10 teams and 159 submissions, with top
systems demonstrating strong performance in identifying AI-generated content.
We also establish a continuous shared task platform to foster ongoing research
and long-term progress in this important area. The dataset and platform are
publicly available at https://github.com/iis-research-team/AINL-Eval-2025.

</details>


### [142] [Improving Diversity in Language Models: When Temperature Fails, Change the Loss](https://arxiv.org/abs/2508.09654)
*Alexandre Verine,Florian Le Bronnec,Kunhao Zheng,Alexandre Allauzen,Yann Chevaleyre,Benjamin Negrevergne*

Main category: cs.CL

TL;DR: 本文关注于提高语言模型多样性的问题，通过分析解码温度调整的方法，提出基于精准率-召回率框架的新损失函数，以更好地权衡输出的质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 当前提升语言模型输出多样性的方法（如提升温度）往往无法有效兼顾生成质量和覆盖范围，因此需要新的方法指导多样性调整。

Method: 作者分析了调整温度对语言模型的精准率（质量）和召回率（覆盖）的影响，并提出用精准率-召回率框架设计新的损失函数，使模型在训练时更适合通过温度调整实现多样性与质量的权衡。

Result: 使用精准率-召回率理念指导的损失函数，显著优于单纯的负对数似然配合温度缩放，能更好地平衡生成的精准率与召回率。

Conclusion: 该框架和方法为实现更灵活、稳健的语言建模提供了新思路，有助于提升模型在多样性和质量上的表现。

Abstract: Increasing diversity in language models is a challenging yet essential
objective. A common approach is to raise the decoding temperature. In this
work, we investigate this approach through a simplistic yet common case to
provide insights into why decreasing temperature can improve quality
(Precision), while increasing it often fails to boost coverage (Recall). Our
analysis reveals that for a model to be effectively tunable through temperature
adjustments, it must be trained toward coverage. To address this, we propose
rethinking loss functions in language models by leveraging the Precision-Recall
framework. Our results demonstrate that this approach achieves a substantially
better trade-off between Precision and Recall than merely combining negative
log-likelihood training with temperature scaling. These findings offer a
pathway toward more versatile and robust language modeling techniques.

</details>


### [143] [EffiEval: Efficient and Generalizable Model Evaluation via Capability Coverage Maximization](https://arxiv.org/abs/2508.09662)
*Yaoning Wang,Jiahao Ying,Yixin Cao,Yubo Ma,Yugang Jiang*

Main category: cs.CL

TL;DR: 本文提出EffiEval方法，无需训练即可高效评估大语言模型，只用少量数据就能实现高效、可靠、公平的基准测试。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和评测基准的发展，模型评测面临计算量大和数据冗余等挑战。现有方法需要大量评测数据或依赖绝对性能，导致效率低下、评测成本高。

Method: EffiEval是一种训练自由的样本选取方法，基于模型效用指数（MUI）自适应选取高质量、有代表性的子集。关注评测的代表性、公平性和泛化能力，不依赖模型表现，也不用大规模评测数据。

Result: 在多个公开基准和不同LLM上实验证明，EffiEval用仅一小部分数据就能维持与全量评测一致的排名结果，实现高效与高一致性。同时能弹性调整样本集规模。

Conclusion: EffiEval为LLM评测提供一种高效、可靠且可泛化的实用方法，有助于在大模型时代实现公平、高效的模型评估。

Abstract: The rapid advancement of large language models (LLMs) and the development of
increasingly large and diverse evaluation benchmarks have introduced
substantial computational challenges for model assessment. In this paper, we
present EffiEval, a training-free approach for efficient benchmarking that
effectively addresses data redundancy while maintaining high evaluation
reliability. Our method is specifically designed to meet three key criteria for
high-quality evaluation: representativeness, by ensuring comprehensive coverage
of model capabilities; fairness, by remaining independent of model performance
during sample selection to avoid bias; and generalizability, by enabling
flexible transfer across datasets and model families without reliance on
large-scale evaluation data. Unlike traditional methods that rely on absolute
performance or require extensive evaluation data, our approach adaptively
selects high-quality representative subsets based on the Model Utility Index
(MUI). Extensive experiments on multiple public benchmarks and diverse LLMs
demonstrate that EffiEval achieves strong ranking consistency with full-dataset
evaluation using only a small fraction of the original data. Furthermore, our
method is flexible and scalable in size, allowing users to balance evaluation
efficiency and representativeness according to specific needs. Overall,
EffiEval provides a practical and generalizable solution for reliable, fair,
and efficient evaluation in the era of LLMs.

</details>


### [144] [Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought Distillation](https://arxiv.org/abs/2508.09666)
*Ziyang Ma,Qingyue Yuan,Linhai Zhang,Deyu Zhou*

Main category: cs.CL

TL;DR: 本论文提出了一种新的链式思考(CoT)蒸馏方法SLowED，有效解决了以往蒸馏提升小型语言模型(SLMs)推理能力时引发的模型安全性下降问题。该方法在保证安全性的同时提升了推理表现。


<details>
  <summary>Details</summary>
Motivation: 以往通过连锁思考蒸馏方法提升SLM推理能力，主要关注推理性能，忽视了由此导致的安全性问题（如更易输出有害内容）。现有安全对齐方法需要额外计算资源或数据，且可能影响推理能力。因此需要新的方法在提升SLM推理能力的同时保持其安全性。

Method: 提出了SLowED（慢调优和低熵掩码蒸馏）方法，包括两个关键模块：1）慢调优，缓慢调整模型权重，使模型参数分布变化保持邻近初始值；2）低熵掩码，对低熵（置信度高的）token进行掩码，排除其作为学习目标，避免过拟合或不必要的变动。这一方法在蒸馏过程中可有效平衡推理能力提升与模型的安全性。

Result: 在Qwen2.5-1.5B、Llama-3.2-1B和BLOOM-1.1B等三个SLM，以及BBH、BB-Sub、ARC、AGIEval等推理基准和AdvBench安全性评测上，SLowED方法在保持优秀推理能力的同时，有效维持了模型的安全性，优于传统蒸馏方法。消融实验显示：慢调优有助于在训练初期保持安全，低熵掩码则延长了安全训练周期。

Conclusion: SLowED可以在提升SLM推理性能的同时保持其安全性，为CoT蒸馏方法的安全性问题提供了有效方案；慢调优和低熵掩码两个模块各有贡献，建议后续蒸馏任务参考该策略。

Abstract: Previous chain-of-thought (CoT) distillation methods primarily focused on
enhancing the reasoning capabilities of Small Language Models (SLMs) by
utilizing high-quality rationales generated by powerful Large Language Models
(LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM
safety brought by the training, which are revealed in this study. Although
there are works on safety alignment that fine-tune language models or
manipulate model weights to defend against harmful inputs, they require extra
computation or annotated data, and probably impact the reasoning ability of
SLMs. In this paper, we investigate how to maintain the safety of SLMs during
the CoT distillation process. Specifically, we propose a safe distillation
method, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing
two modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the
magnitude of model weight changes to optimize the model weights in the
neighboring space near the initial weight distribution. Low-Entropy Masking
masks low-entropy tokens, which are regarded as unnecessary learning targets,
to exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B,
Llama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC,
AGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety
of SLMs and comparably improves their reasoning capability compared to existing
distillation methods. Furthermore, our ablation study presents the
effectiveness of Slow Tuning and Low-Entropy Masking, with the former
maintaining the model's safety in the early stage and the latter prolonging the
safe training epochs.

</details>


### [145] [Evaluating the Role of Large Language Models in Legal Practice in India](https://arxiv.org/abs/2508.09713)
*Rahul Hemrajani*

Main category: cs.CL

TL;DR: 本文通过实证方法评估了大型语言模型（LLMs）在印度法律任务中的表现，发现其在起草和问题识别中表现出色，但在专业法律研究方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能融入法律行业，人们关注LLMs是否能胜任关键法律任务，尤其是在印度的实际法律环境中。此文试图具体衡量LLMs与初级律师在实际法律操作中的能力差异。

Method: 作者通过实验调查，选择GPT、Claude、Llama等主流LLMs，对比其在印度法律相关任务（如问题识别、法律文件起草、法律建议、研究和推理）上的表现。输出结果由高级法律系学生依据有用性、准确性及全面性进行评价，并与初级律师的表现比较。

Result: 研究发现，LLMs在法律文件起草和问题识别方面与人类初级律师持平甚至更优。但在专业法律研究环节，LLMs常出现虚构和错误事实，表现不佳。

Conclusion: LLMs可辅助部分标准化法律任务，但在复杂推理及精准法律适用上仍需人类专家，完全替代尚不现实。

Abstract: The integration of Artificial Intelligence(AI) into the legal profession
raises significant questions about the capacity of Large Language Models(LLM)
to perform key legal tasks. In this paper, I empirically evaluate how well
LLMs, such as GPT, Claude, and Llama, perform key legal tasks in the Indian
context, including issue spotting, legal drafting, advice, research, and
reasoning. Through a survey experiment, I compare outputs from LLMs with those
of a junior lawyer, with advanced law students rating the work on helpfulness,
accuracy, and comprehensiveness. LLMs excel in drafting and issue spotting,
often matching or surpassing human work. However, they struggle with
specialised legal research, frequently generating hallucinations, factually
incorrect or fabricated outputs. I conclude that while LLMs can augment certain
legal tasks, human expertise remains essential for nuanced reasoning and the
precise application of law.

</details>


### [146] [The Perils of Chart Deception: How Misleading Visualizations Affect Vision-Language Models](https://arxiv.org/abs/2508.09716)
*Ridwan Mahbub,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Mizanur Rahman,Mir Tafseer Nayeem,Enamul Hoque*

Main category: cs.CL

TL;DR: 本文系统评估了视觉-语言模型（VLMs）在解读具有误导性设计的信息可视化时的表现，发现绝大多数模型容易被误导，从而给出错误解读。


<details>
  <summary>Details</summary>
Motivation: 随着视觉-语言模型更广泛地用于数据可视化的解读，尤其在普通用户中，研究这些模型是否会被故意的可视化设计误导变得尤为重要。

Method: 作者针对8种常见的误导性图表类型，利用10种不同的VLMs，收集并分析超过1.6万条模型回答，系统评测其对误导性可视化的敏感性和表现。

Result: 实验结果显示，绝大多数视觉-语言模型容易被含有误导性设计的图表所欺骗，即使底层数据未变，模型也会做出不同的解读。

Conclusion: 研究指出，当前VLMs对视觉误导具有高度敏感性，亟需在模型中引入更健壮的防护机制，以应对视觉信息误导。

Abstract: Information visualizations are powerful tools that help users quickly
identify patterns, trends, and outliers, facilitating informed decision-making.
However, when visualizations incorporate deceptive design elements-such as
truncated or inverted axes, unjustified 3D effects, or violations of best
practices-they can mislead viewers and distort understanding, spreading
misinformation. While some deceptive tactics are obvious, others subtly
manipulate perception while maintaining a facade of legitimacy. As
Vision-Language Models (VLMs) are increasingly used to interpret
visualizations, especially by non-expert users, it is critical to understand
how susceptible these models are to deceptive visual designs. In this study, we
conduct an in-depth evaluation of VLMs' ability to interpret misleading
visualizations. By analyzing over 16,000 responses from ten different models
across eight distinct types of misleading chart designs, we demonstrate that
most VLMs are deceived by them. This leads to altered interpretations of
charts, despite the underlying data remaining the same. Our findings highlight
the need for robust safeguards in VLMs against visual misinformation.

</details>


### [147] [Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning](https://arxiv.org/abs/2508.09726)
*Vaishnavi Shrivastava,Ahmed Awadallah,Vidhisha Balachandran,Shivam Garg,Harkirat Behl,Dimitris Papailiopoulos*

Main category: cs.CL

TL;DR: 提出一种新方法GFPO，有效缓解大语言模型训练时的回复长度膨胀问题，保持准确率的同时显著减少冗余内容。


<details>
  <summary>Details</summary>
Motivation: 目前用可验证奖励进行强化学习训练的大语言模型，往往通过增加回复长度来提升准确率，但是这种方法引入了大量无效、冗余内容，降低了推理效率。作者希望在不牺牲准确率的前提下，缓解回复长度膨胀、提升推理效率。

Method: 提出了GFPO（Group Filtered Policy Optimization）方法：在训练阶段对每个问题采样更多候选回复，并基于回复长度和Token效率（奖励与Token数的比值）进行筛选，训练过程中只保留质量高、性价比高的样本。此外，提出了Adaptive Difficulty GFPO，可根据问题难度动态分配更多训练资源。

Result: 在Phi-4-reasoning模型和多个困难STEM、编程基准上，GFPO将原有方法带来的回复长度膨胀减少了46-71%，并保持准确率不变。进一步优化Token效率时，长度膨胀减少可达71-85%。采用自适应难度机制后，在难题上进一步提升了计算效率与准确率平衡。

Conclusion: GFPO证明了训练阶段计算投入的增加可以换取推理阶段计算的节省，提出了一种简单有效的高效推理新范式。

Abstract: Large language models trained with reinforcement learning with verifiable
rewards tend to trade accuracy for length--inflating response lengths to
achieve gains in accuracy. While longer answers may be warranted for harder
problems, many tokens are merely "filler": repetitive, verbose text that makes
no real progress. We introduce GFPO (Group Filtered Policy Optimization), which
curbs this length explosion by sampling larger groups per problem during
training and filtering responses to train on based on two key metrics: (1)
response length and (2) token efficiency: reward per token ratio. By sampling
more at training time, we teach models to think less at inference time. On the
Phi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across
challenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH,
LiveCodeBench) while maintaining accuracy. Optimizing for reward per token
further increases reductions in length inflation to 71-85%. We also propose
Adaptive Difficulty GFPO, which dynamically allocates more training resources
to harder problems based on real-time difficulty estimates, improving the
balance between computational efficiency and accuracy especially on difficult
questions. GFPO demonstrates that increased training-time compute directly
translates to reduced test-time compute--a simple yet effective trade-off for
efficient reasoning.

</details>


### [148] [Transforming Questions and Documents for Semantically Aligned Retrieval-Augmented Generation](https://arxiv.org/abs/2508.09755)
*Seokgi Lee*

Main category: cs.CL

TL;DR: 本文提出了一种新的针对多跳问答任务的检索增强生成（RAG）方法，通过将复杂问题分解为单跳子问题引导检索，并用生成答案问题的嵌入提升检索效果，实验表明相比于基线系统，性能有所提升。


<details>
  <summary>Details</summary>
Motivation: 当前的多跳问答由于问题复杂、跳数多，直接检索和生成难以准确涵盖所有知识点，因此有必要通过改进检索与生成流程提高系统性能。

Method: 方法包括：（1）用大语言模型将复杂多跳问题分解为单跳子问题，引导文档检索；（2）用Qwen3-8B为每个文档块生成可回答的问题，并用这些问题进行嵌入，计算问题-问题之间的相似度进行检索；（3）将检索到的相关文档块与原始问题一并输入RAG管道进行推断。

Result: 在MuSiQue、2WikiMultiHopQa和HotpotQA三个多跳问答数据集上进行评估，所提出方法在RAG性能上优于基线系统。

Conclusion: 本文证明了基于可回答问题的嵌入对RAG提升有效，同时大模型驱动的查询分解对多跳场景检索增强非常有帮助。

Abstract: We introduce a novel retrieval-augmented generation (RAG) framework tailored
for multihop question answering. First, our system uses large language model
(LLM) to decompose complex multihop questions into a sequence of single-hop
subquestions that guide document retrieval. This decomposition mitigates the
ambiguity inherent in multi-hop queries by clearly targeting distinct knowledge
facets. Second, instead of embedding raw or chunked documents directly, we
generate answerable questions from each document chunk using Qwen3-8B, embed
these generated questions, and retrieve relevant chunks via question-question
embedding similarity. During inference, the retrieved chunks are then fed along
with the original question into the RAG pipeline. We evaluate on three multihop
question datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) from LongBench. Our
method improves RAG performacne compared to baseline systems. Our contributions
highlight the benefits of using answerable-question embeddings for RAG, and the
effectiveness of LLM-based query decomposition for multihop scenarios.

</details>


### [149] [Echoes of Agreement: Argument Driven Opinion Shifts in Large Language Models](https://arxiv.org/abs/2508.09759)
*Avneet Kaur*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLM）对政治话题偏见评测的提示敏感性，特别是在提示包含支持或反对论据情况下，对模型偏见输出的影响。结果显示，论据内容显著左右模型立场，且论据强度越强，模型越易随之改变立场。这一现象对未来的偏见测量与缓解策略有重要意义。


<details>
  <summary>Details</summary>
Motivation: 现有对LLMs政治偏见的研究忽略了提示中不同论据（支持/反对）对模型回答的影响。而在实际应用中，LLM常会接触到带有倾向性的文本，因此研究论据敏感性对于更准确理解和评估模型偏见非常关键。

Method: 在单轮和多轮会话情况下，分别向LLM输入包含支持或反对政治立场的論据型prompt，分析模型回答是否与论据方向一致。进一步考察论据强度对这种一致性的影响。

Result: 实验表明，无论是在单轮还是多轮对话，LLM的政治立场强烈受所给定支持/反对论据的引导，且论据越强势，模型越容易随之改变立场。

Conclusion: LLM存在明显的“迎合”（sycophantic）倾向：在面对有倾向性论据时，容易调整自身立场以呼应提示内容。这对偏见测评的鲁棒性提出挑战，也为偏见缓解和提示设计提供新思路。

Abstract: There have been numerous studies evaluating bias of LLMs towards political
topics. However, how positions towards these topics in model outputs are highly
sensitive to the prompt. What happens when the prompt itself is suggestive of
certain arguments towards those positions remains underexplored. This is
crucial for understanding how robust these bias evaluations are and for
understanding model behaviour, as these models frequently interact with
opinionated text. To that end, we conduct experiments for political bias
evaluation in presence of supporting and refuting arguments. Our experiments
show that such arguments substantially alter model responses towards the
direction of the provided argument in both single-turn and multi-turn settings.
Moreover, we find that the strength of these arguments influences the
directional agreement rate of model responses. These effects point to a
sycophantic tendency in LLMs adapting their stance to align with the presented
arguments which has downstream implications for measuring political bias and
developing effective mitigation strategies.

</details>


### [150] [UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in Multilingual Text-to-Speech](https://arxiv.org/abs/2508.09767)
*Shuhei Kato*

Main category: cs.CL

TL;DR: 提出了UtterTune，一种针对多语种TTS系统的高效适配方法，实现了目标语言发音可控性提升，同时兼顾其他语言表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型架构的多语种TTS取得了显著自然度，但缺少显式的G2P（字母到音素）模块，导致发音和韵律的精确可控性困难，尤其在仅使用编码文本作为输入时更为突出。针对提升目标语言内发音细节可控性的需求，提出新方法。

Method: 采用低秩适配（low-rank adaptation）方法，对基于LLM架构的多语种TTS模型进行轻量级微调。在日语为目标语言下，UtterTune实现了对发音和音高重音的音素级控制，且不牺牲整体自然度和说话人相似性。无需专门针对每个说话人或语言重新训练（zero-shot）。

Result: 通过主观和客观实验评估，UtterTune成功提升了日语的发音和重音可控性，同时维持其他语种的表现和自然度。

Conclusion: UtterTune是实现多语种TTS在目标语种发音可控性的有效方法，并且不会影响其它语种的表现，证明了低秩适配在实际多语种TTS可控性优化中的有效性。

Abstract: We propose UtterTune, a lightweight adaptation method that fine-tunes a
multilingual text-to-speech (TTS) system based on a large language model (LLM)
architecture, designed to enhance the controllability of pronunciation in a
target language while preserving performance in others. While LLM architectures
have enabled TTS models to achieve remarkable naturalness, accurately modeling
grapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially
when the model omits an explicit G2P module and directly processes minimally
encoded text (e.g., byte-pair encoding). UtterTune leverages low-rank
adaptation to enable the control of segmental pronunciation and pitch accent at
the phoneme level for Japanese speech, the target language in this paper, while
maintaining naturalness and speaker similarity in a zero-shot setting.
Objective and subjective evaluations confirm its effectiveness.

</details>


### [151] [Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study](https://arxiv.org/abs/2508.09776)
*Mahdi Dhaini,Juraj Vladika,Ege Erdogan,Zineb Attaoui,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 本论文提出了一种自动化框架，利用多个大语言模型生成高质量文本解释，并证明其在自然语言推断任务中能媲美人工标注，有助于提高模型表现和数据集扩展。


<details>
  <summary>Details</summary>
Motivation: 当前解释型NLP任务中，高质量的人类文本解释依赖人工注释，成本高且难以扩展，急需自动化、可扩展的解决办法。

Method: 提出利用多种先进的大语言模型自动生成文本解释，并通过全面的自然语言生成（NLG）指标严格评测解释质量，同时分析这些自动化解释对预训练语言模型和大语言模型在自然语言推断任务上的影响。

Result: 实验显示，自动生成的解释与人工标注的解释相比，能在提升模型性能方面达到极具竞争力的效果。

Conclusion: 研究表明，自动化基于LLM的文本解释生成方法是一条有前景的方向，能扩展NLP数据集并提升模型性能，为解释型NLP任务带来可扩展且高效的解决方案。

Abstract: In the rapidly evolving field of Explainable Natural Language Processing
(NLP), textual explanations, i.e., human-like rationales, are pivotal for
explaining model predictions and enriching datasets with interpretable labels.
Traditional approaches rely on human annotation, which is costly,
labor-intensive, and impedes scalability. In this work, we present an automated
framework that leverages multiple state-of-the-art large language models (LLMs)
to generate high-quality textual explanations. We rigorously assess the quality
of these LLM-generated explanations using a comprehensive suite of Natural
Language Generation (NLG) metrics. Furthermore, we investigate the downstream
impact of these explanations on the performance of pre-trained language models
(PLMs) and LLMs across natural language inference tasks on two diverse
benchmark datasets. Our experiments demonstrate that automated explanations
exhibit highly competitive effectiveness compared to human-annotated
explanations in improving model performance. Our findings underscore a
promising avenue for scalable, automated LLM-based textual explanation
generation for extending NLP datasets and enhancing model performance.

</details>


### [152] [Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges](https://arxiv.org/abs/2508.09786)
*Mahdi Dhaini,Tobias Müller,Roksoliana Rabets,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 本论文通过访谈调研分析了产业与学术界从业者对可解释自然语言处理（NLP）方法的实际采用和满意度，揭示了目前方法存在的不足及面临的实际挑战。


<details>
  <summary>Details</summary>
Motivation: 随着复杂NLP模型的不透明性增加，模型决策的可解释性变得尤为重要，尤其是在高风险场景下。然而，目前对于从业者采用与评价可解释方法的实际情况缺乏深入了解。

Method: 作者采用定性访谈的方法，对产业界从业者及学术界研究人员进行系统调研，通过比较分析，收集其使用可解释NLP方法的动机、技术方案、满意度及实际遇到的问题。

Result: 研究发现从业者对于现有可解释NLP技术总体满意度较低，存在较大概念鸿沟，同时面临评估难题。

Conclusion: 作者指出，需要明确的定义和以用户为中心的框架，以促进可解释NLP技术在实际中的有效采用。

Abstract: The field of explainable natural language processing (NLP) has grown rapidly
in recent years. The growing opacity of complex models calls for transparency
and explanations of their decisions, which is crucial to understand their
reasoning and facilitate deployment, especially in high-stakes environments.
Despite increasing attention given to explainable NLP, practitioners'
perspectives regarding its practical adoption and effectiveness remain
underexplored. This paper addresses this research gap by investigating
practitioners' experiences with explainability methods, specifically focusing
on their motivations for adopting such methods, the techniques employed,
satisfaction levels, and the practical challenges encountered in real-world NLP
applications. Through a qualitative interview-based study with industry
practitioners and complementary interviews with academic researchers, we
systematically analyze and compare their perspectives. Our findings reveal
conceptual gaps, low satisfaction with current explainability methods, and
highlight evaluation challenges. Our findings emphasize the need for clear
definitions and user-centric frameworks for better adoption of explainable NLP
in practice.

</details>


### [153] [BigCharts-R1: Enhanced Chart Reasoning with Visual Reinforcement Finetuning](https://arxiv.org/abs/2508.09804)
*Ahmed Masry,Abhay Puri,Masoud Hashemi,Juan A. Rodriguez,Megh Thakkar,Khyati Mahajan,Vikas Yadav,Sathwik Tejaswi Madhusudhan,Alexandre Piché,Dzmitry Bahdanau,Christopher Pal,David Vazquez,Enamul Hoque,Perouz Taslakian,Sai Rajeswar,Spandana Gella*

Main category: cs.CL

TL;DR: 本论文提出了BigCharts和BigCharts-R1，通过提高图表数据集的多样性和真实性，并结合强化学习优化，实现了图表理解任务上的最新性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在图表理解方面表现不佳，主要是由于训练数据集缺乏多样性和真实性，或依赖自动提取的数据表而导致数据误差累积。此外，当前方法仅依靠监督微调，受限于低质量数据集，效果有限。

Method: 提出BigCharts数据集生成管道，从多个线上平台获取真实世界图表，通过条件渲染实现数据集的真实性和视觉多样性，并通过重新绘制(replotting)保证数据准确性。同时，提出结合监督微调及基于GRPO的强化学习的训练框架，设计了面向图表推理的奖励信号，提升模型泛化能力。

Result: 与现有多个开放及闭源模型比较，BigCharts-R1在多个图表问答基准上取得了超越性的性能。

Conclusion: 通过设计更优质的数据集及训练方式，极大提升了VLM在图表理解任务上的表现，为后续相关研究指明了方向。

Abstract: Charts are essential to data analysis, transforming raw data into clear
visual representations that support human decision-making. Although current
vision-language models (VLMs) have made significant progress, they continue to
struggle with chart comprehension due to training on datasets that lack
diversity and real-world authenticity, or on automatically extracted underlying
data tables of charts, which can contain numerous estimation errors.
Furthermore, existing models only rely on supervised fine-tuning using these
low-quality datasets, severely limiting their effectiveness. To address these
issues, we first propose BigCharts, a dataset creation pipeline that generates
visually diverse chart images by conditioning the rendering process on
real-world charts sourced from multiple online platforms. Unlike purely
synthetic datasets, BigCharts incorporates real-world data, ensuring
authenticity and visual diversity, while still retaining accurate underlying
data due to our proposed replotting process. Additionally, we introduce a
comprehensive training framework that integrates supervised fine-tuning with
Group Relative Policy Optimization (GRPO)-based reinforcement learning. By
introducing novel reward signals specifically designed for chart reasoning, our
approach enhances model robustness and generalization across diverse chart
styles and domains, resulting in a state-of-the-art chart reasoning model,
BigCharts-R1. Extensive experiments demonstrate that our models surpass
existing methods on multiple chart question-answering benchmarks compared to
even larger open-source and closed-source models.

</details>


### [154] [A Comprehensive Survey of Datasets for Clinical Mental Health AI Systems](https://arxiv.org/abs/2508.09809)
*Aishik Mandal,Prottay Kumar Adhikary,Hiba Arnaout,Iryna Gurevych,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 该论文首次全面梳理了用于训练AI临床助手的精神健康临床数据集，并对数据集类型、数据模态、任务类型、可及性和社会文化背景等方面进行了系统分类，总结了当前数据集的缺陷和未来改进方向。


<details>
  <summary>Details</summary>
Motivation: 随着全球精神健康障碍病例的上升，受训临床医生的数量却无法跟上需求，导致大量患者难以及时获得支持。AI技术在精神健康领域辅助诊断和干预被认为有很大潜力，但高质量的临床训练数据集是构建高效、可靠且伦理合规的AI的前提。目前相关数据集分散、文档不全且难以公开获取，严重限制了AI模型的可复现性和泛化能力。

Method: 文章对公开发表的精神健康AI临床助手训练相关数据集进行了广泛调研，按照精神疾病类型、数据模态、任务类型、可及性和社会文化背景进行系统性分类梳理，并关注了合成数据集的现状。

Result: 调研发现现有数据集存在明显短板，包括：缺乏纵向随访数据、文化和语言代表性不足、数据收集和标注标准不一致以及合成数据模态单一等。

Conclusion: 作者建议未来需加强多样性、收集标准化、开放性和纵向随访的精神健康数据集建设，并提出相关发展方向和建议，以推动更健壮、通用且公平的AI精神健康系统发展。

Abstract: Mental health disorders are rising worldwide. However, the availability of
trained clinicians has not scaled proportionally, leaving many people without
adequate or timely support. To bridge this gap, recent studies have shown the
promise of Artificial Intelligence (AI) to assist mental health diagnosis,
monitoring, and intervention. However, the development of efficient, reliable,
and ethical AI to assist clinicians is heavily dependent on high-quality
clinical training datasets. Despite growing interest in data curation for
training clinical AI assistants, existing datasets largely remain scattered,
under-documented, and often inaccessible, hindering the reproducibility,
comparability, and generalizability of AI models developed for clinical mental
health care. In this paper, we present the first comprehensive survey of
clinical mental health datasets relevant to the training and development of
AI-powered clinical assistants. We categorize these datasets by mental
disorders (e.g., depression, schizophrenia), data modalities (e.g., text,
speech, physiological signals), task types (e.g., diagnosis prediction, symptom
severity estimation, intervention generation), accessibility (public,
restricted or private), and sociocultural context (e.g., language and cultural
background). Along with these, we also investigate synthetic clinical mental
health datasets. Our survey identifies critical gaps such as a lack of
longitudinal data, limited cultural and linguistic representation, inconsistent
collection and annotation standards, and a lack of modalities in synthetic
data. We conclude by outlining key challenges in curating and standardizing
future datasets and provide actionable recommendations to facilitate the
development of more robust, generalizable, and equitable mental health AI
systems.

</details>


### [155] [Speed Always Wins: A Survey on Efficient Architectures for Large Language Models](https://arxiv.org/abs/2508.09834)
*Weigao Sun,Jiaxi Hu,Yucheng Zhou,Jusen Du,Disen Lan,Kexin Wang,Tong Zhu,Xiaoye Qu,Yu Zhang,Xiaoyu Mo,Daizong Liu,Yuxuan Liang,Wenliang Chen,Guoqi Li,Yu Cheng*

Main category: cs.CL

TL;DR: 本文综述了当前为提升大型语言模型（LLM）效率而提出的创新架构，包括线性/稀疏序列建模、高效全注意力机制、稀疏专家混合、混合模型和扩散LLM等。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer虽然在LLM领域表现出色，但其高计算开销限制了大规模训练和实际部署，因此亟需更高效的模型架构。

Method: 系统性梳理并归类了提升LLM效率的模型方法，包括1）线性化和稀疏化序列建模；2）高效注意力机制变体；3）稀疏专家混合模型；4）融合多种技术的混合模型及新兴的扩散LLM。并探讨这些技术如何应用到多模态和规模化基础模型。

Result: 整理和比较了上述高效架构的方法、原理和应用场景，揭示了当前主流趋向，并讨论了这些技术对未来模型开发的潜力影响。

Conclusion: 文章为高效LLM架构提供了系统蓝图，为构建更高效、通用的AI系统指明了方向，鼓励后续研究继续突破资源限制，发展多能智能模型。

Abstract: Large Language Models (LLMs) have delivered impressive results in language
understanding, generation, reasoning, and pushes the ability boundary of
multimodal models. Transformer models, as the foundation of modern LLMs, offer
a strong baseline with excellent scaling properties. However, the traditional
transformer architecture requires substantial computations and poses
significant obstacles for large-scale training and practical deployment. In
this survey, we offer a systematic examination of innovative LLM architectures
that address the inherent limitations of transformers and boost the efficiency.
Starting from language modeling, this survey covers the background and
technical details of linear and sparse sequence modeling methods, efficient
full attention variants, sparse mixture-of-experts, hybrid model architectures
incorporating the above techniques, and emerging diffusion LLMs. Additionally,
we discuss applications of these techniques to other modalities and consider
their wider implications for developing scalable, resource-aware foundation
models. By grouping recent studies into the above category, this survey
presents a blueprint of modern efficient LLM architectures, and we hope this
could help motivate future research toward more efficient, versatile AI
systems.

</details>


### [156] [PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts](https://arxiv.org/abs/2508.09848)
*Mo Yu,Tsz Ting Chung,Chulun Zhou,Tong Li,Rui Lu,Jiangnan Li,Liyan Xu,Haoshu Lu,Ning Zhang,Jing Li,Jie Zhou*

Main category: cs.CL

TL;DR: 该论文提出了PRELUDE基准，用于评估模型在长文本理解和深度推理能力上的表现，特别是通过判断衍生故事与原始小说情节是否一致。


<details>
  <summary>Details</summary>
Motivation: 现有基准不足以评估语言模型在处理全局性、间接信息整合、深度推理等能力上的极限，因此需要更挑战性的任务。

Method: 通过设计一个新基准PRELUDE，任务要求判断角色的前传故事是否与原著叙事相符，并分析该任务对模型长文本理解和多证据整合能力的需求。实验对比了最先进的大模型、检索增强生成（RAG）、现有深度研究服务与人类在该任务上的表现。

Result: 实验显示，无论是上下文学习、RAG还是领域内训练的大模型以及商业服务，在该任务上的表现均远逊于人类，差距至少超15%；进一步的人类研究发现，即使模型给出正确答案，其推理过程仍有明显缺陷，推理准确率较人类低30%以上。

Conclusion: 目前长文本理解和推理能力仍有巨大提升空间，现有主流模型在PRELUDE任务上的不足凸显了基本挑战性。

Abstract: We introduce PRELUDE, a benchmark for evaluating long-context understanding
through the task of determining whether a character's prequel story is
consistent with the canonical narrative of the original book. Our task poses a
stronger demand for global comprehension and deep reasoning than existing
benchmarks -- as the prequels are not part of the original story, assessing
their plausibility typically requires searching and integrating information
that is only indirectly related. Empirically, 88% of instances require evidence
from multiple parts of the narrative. Experimental results highlight the
challenge of our task: in-context learning, RAG and in-domain training with
state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans
by >15%. A further human study reveals that models often produce correct
answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy
compared to humans. These findings underscore the substantial room for
improvement in long-context understanding and reasoning.

</details>


### [157] [Assessing the Feasibility of Lightweight Whisper Models for Low-Resource Urdu Transcription](https://arxiv.org/abs/2508.09865)
*Abdul Rehman Antall,Naveed Akhtar*

Main category: cs.CL

TL;DR: 本研究评估了轻量级Whisper模型（Tiny、Base、Small）在低资源环境下用于乌尔都语语音识别的可行性，结果显示Whisper-Small表现最佳，但准确率仍有提升空间。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语是世界第10大语言，使用人数众多，但受方言多样、混合语言现象及训练数据稀缺影响，当前自动语音识别（ASR）系统对乌尔都语支持有限。该研究旨在填补这一空白，探索低资源环境下ASR模型在乌尔都语上的表现。

Method: 研究选择Whisper的Tiny、Base、Small三种轻量级预训练模型，在未进行微调的情况下，使用一个经过精选的乌尔都语数据集对模型进行评测，并以词错误率（WER）为主要指标。同时进行了定性分析，考察了模型在语音准确性和词汇连贯性方面的表现。

Result: Whisper-Small模型在测试中达到了33.68%的最低WER，优于Base（53.67%）和Tiny（67.08%）。但模型在发音准确性和复杂话语的词汇连贯性上仍存在显著挑战。

Conclusion: Whisper-Small模型在乌尔都语ASR应用中显示出一定的潜力，但当前准确度仍不足以完全满足部署需求。研究结果为未来构建高效、低资源ASR系统提供了基础和方向。

Abstract: This study evaluates the feasibility of lightweight Whisper models (Tiny,
Base, Small) for Urdu speech recognition in low-resource settings. Despite Urdu
being the 10th most spoken language globally with over 230 million speakers,
its representation in automatic speech recognition (ASR) systems remains
limited due to dialectal diversity, code-switching, and sparse training data.
We benchmark these models on a curated Urdu dataset using word error rate
(WER), without fine-tuning. Results show Whisper-Small achieves the lowest
error rates (33.68\% WER), outperforming Tiny (67.08\% WER) and Base (53.67\%
WER). Qualitative analysis reveals persistent challenges in phonetic accuracy
and lexical coherence, particularly for complex utterances. While Whisper-Small
demonstrates promise for deployable Urdu ASR, significant gaps remain. Our
findings emphasize lay the groundwork for future research into effective,
low-resource ASR systems.

</details>


### [158] [Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models](https://arxiv.org/abs/2508.09874)
*Jiaqi Cao,Jiarui Wang,Rubin Wei,Qipeng Guo,Kai Chen,Bowen Zhou,Zhouhan Lin*

Main category: cs.CL

TL;DR: 提出了一种新方法Memory Decoder，可在不改动原模型参数的情况下高效实现大语言模型的领域自适应。该方法通过一个小型解码器网络模拟检索器行为，灵活集成，实验验证其在多个专业领域显著提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型具备强泛化能力，但针对特定专业领域的适应能力仍有限。现有的领域自适应方法（如DAPT需要消耗大量计算资源且易遗忘原有知识，RAG推理延迟高，因此急需更高效便捷的领域自适应方案。

Method: 提出Memory Decoder：一个小型transformer解码器，预训练以模仿外部检索式RAG模块的行为，无需修改原有大模型参数。该预训练记忆模块与任意同tokenizer的预训练大语言模型无缝集成，实现“即插即用”。

Result: Memory Decoder可集成到Qwen和Llama等多个主流大语言模型，分别在生物医学、金融、法律等三个专业领域进行实验，平均降低困惑度6.17点，表明其能有效提升领域内模型表现。

Conclusion: Memory Decoder作为一种特别预训练的记忆组件，首次实现了通用大模型的高效领域适应。其简单可插拔的架构，能稳定提升不同模型在目标领域内的性能，具有广泛实际应用前景。

Abstract: Large Language Models (LLMs) have shown strong abilities in general language
tasks, yet adapting them to specific domains remains a challenge. Current
method like Domain Adaptive Pretraining (DAPT) requires costly full-parameter
training and suffers from catastrophic forgetting. Meanwhile,
Retrieval-Augmented Generation (RAG) introduces substantial inference latency
due to expensive nearest-neighbor searches and longer context. This paper
introduces Memory Decoder, a plug-and-play pretrained memory that enables
efficient domain adaptation without changing the original model's parameters.
Memory Decoder employs a small transformer decoder that learns to imitate the
behavior of an external non-parametric retriever. Once trained, Memory Decoder
can be seamlessly integrated with any pretrained language model that shares the
same tokenizer, requiring no model-specific modifications. Experimental results
demonstrate that Memory Decoder enables effective adaptation of various Qwen
and Llama models to three distinct specialized domains: biomedicine, finance,
and law, reducing perplexity by an average of 6.17 points. Overall, Memory
Decoder introduces a novel paradigm centered on a specially pretrained memory
component designed for domain-specific adaptation. This memory architecture can
be integrated in a plug-and-play manner, consistently enhancing performance
across multiple models within the target domain.

</details>


### [159] [A Survey of Cognitive Distortion Detection and Classification in NLP](https://arxiv.org/abs/2508.09878)
*Archie Sage,Jeroen Keppens,Helen Yannakoudakis*

Main category: cs.CL

TL;DR: 本文对过去二十年在自然语言处理领域自动检测与分类认知扭曲（CD）相关的38项研究进行综述，归纳了数据集、建模方法及评估策略，总结了现状与挑战。


<details>
  <summary>Details</summary>
Motivation: 认知扭曲对心理健康有显著影响，自动化检测对心理治疗有重要应用价值，但目前研究碎片化，分类体系、任务定义与评估方法不一致，阻碍了领域进步，因此需要系统梳理现有文献。

Method: 作者系统性回顾和整理了相关文献，围绕数据集、模型方法、评测策略等核心要素进行归纳总结，并整合了认知扭曲分类体系，对常见任务设定及研究难点进行了全面分析。

Result: 建立了统一的认知扭曲分类参考，整理了主流的数据集和建模策略，归纳了评估流程，并总结了现存任务设定，提出了领域面临的主要挑战和未来方向。

Conclusion: 本文为心理健康领域的自然语言处理研究人员提供了系统参考，有助于推动认知扭曲自动检测方向的标准化、可复现和协同发展。

Abstract: As interest grows in the application of natural language processing (NLP)
techniques to mental health, a growing body of work explores the automatic
detection and classification of cognitive distortions (CDs). CDs are habitual
patterns of negatively biased or flawed thinking that distort how people
perceive events, judge themselves, and react to the world around them.
Identifying and addressing them is an important part of therapy. Despite its
momentum, the field remains fragmented, with inconsistencies in CD taxonomies,
task formulations, and evaluation practices. This survey reviews 38 studies
spanning two decades, providing a structured overview of datasets, modelling
approaches, and evaluation strategies. We provide a consolidated CD taxonomy
reference, summarise common task setups, and highlight open challenges to
support more coherent and reproducible research in this emerging area.

</details>


### [160] [Language of Persuasion and Misrepresentation in Business Communication: A Textual Detection Approach](https://arxiv.org/abs/2508.09935)
*Sayem Hossen,Monalisa Moon Joti,Md. Golam Rashed*

Main category: cs.CL

TL;DR: 该论文探讨了数字化商业沟通中说服性语言和欺骗行为的检测问题，并利用计算文本分析和个性化transformer模型在受控实验中取得了高于99%的检测准确率，但在多语言环境下数据和基础设施不足导致效果难以复现。


<details>
  <summary>Details</summary>
Motivation: 随着商业沟通数字化，欺骗性语言的使用和检测变得日益重要。研究动机是希望揭示和检测在财务报告、可持续发展叙事及数字营销中出现的欺骗性说服语言。

Method: 整合古典修辞学、传播心理学、语言学理论及实证研究，通过使用说服性词汇表，结合计算文本分析和个性化transformer模型进行欺骗语言检测。

Result: 在受控实验和同质语料条件下，文本分析与transformer模型检测准确率超过99%；但在多语言环境下数据和多语种处理平台不足导致准确率得不到保证。

Conclusion: 理论与实际效果之间存在差距，未来需要完善自动文本识别系统，尤其在AI主导的对话环境中提升多语言欺骗性语言的检测能力。

Abstract: Business communication digitisation has reorganised the process of persuasive
discourse, which
  allows not only greater transparency but also advanced deception. This
inquiry synthesises classical
  rhetoric and communication psychology with linguistic theory and empirical
studies in the financial
  reporting, sustainability discourse, and digital marketing to explain how
deceptive language can be
  systematically detected using persuasive lexicon. In controlled settings,
detection accuracies of greater
  than 99% were achieved by using computational textual analysis as well as
personalised transformer
  models. However, reproducing this performance in multilingual settings is
also problematic and,
  to a large extent, this is because it is not easy to find sufficient data,
and because few multilingual
  text-processing infrastructures are in place. This evidence shows that there
has been an increasing
  gap between the theoretical representations of communication and those
empirically approximated,
  and therefore, there is a need to have strong automatic text-identification
systems where AI-based
  discourse is becoming more realistic in communicating with humans.

</details>


### [161] [A Comprehensive Evaluation framework of Alignment Techniques for LLMs](https://arxiv.org/abs/2508.09937)
*Muneeza Azmat,Momin Abbas,Maysa Malfiza Garcia de Macedo,Marcelo Carpinette Grave,Luan Soares de Souza,Tiago Machado,Rogerio A de Paula,Raya Horesh,Yixin Chen,Heloisa Caroline de Souza Pereira Candello,Rebecka Nordenlow,Aminat Adebiyi*

Main category: cs.CL

TL;DR: 该论文提出了一个多维度LLM对齐技术评估框架，用以系统比较主流对齐方法（如RLHF、指令微调、后处理和推理时干预），并通过实验证明该框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐方法多样，但缺乏统一的评价标准，导致不同策略之间难以比较，影响实际部署和方法选择。作者希望通过建立统一评估标准，推动领域进步。

Method: 该文提出了一个综合评估框架，从对齐检测、对齐质量、计算效率和鲁棒性四个维度，系统比较各类大模型对齐方法，并在不同底座模型和多种对齐策略上进行实验。

Result: 实验结果显示，该框架能有效揭示当前主流对齐技术的优缺点，帮助分析各方法在不同场景下的表现。

Conclusion: 该评估框架为LLM对齐方法的选择和未来研究提供了有价值的参考，有助于指导安全且高效的大模型实际应用。

Abstract: As Large Language Models (LLMs) become increasingly integrated into
real-world applications, ensuring their outputs align with human values and
safety standards has become critical. The field has developed diverse alignment
approaches including traditional fine-tuning methods (RLHF, instruction
tuning), post-hoc correction systems, and inference-time interventions, each
with distinct advantages and limitations. However, the lack of unified
evaluation frameworks makes it difficult to systematically compare these
paradigms and guide deployment decisions. This paper introduces a
multi-dimensional evaluation of alignment techniques for LLMs, a comprehensive
evaluation framework that provides a systematic comparison across all major
alignment paradigms. Our framework assesses methods along four key dimensions:
alignment detection, alignment quality, computational efficiency, and
robustness. Through experiments across diverse base models and alignment
strategies, we demonstrate the utility of our framework in identifying
strengths and limitations of current state-of-the-art models, providing
valuable insights for future research directions.

</details>


### [162] [VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models](https://arxiv.org/abs/2508.09945)
*Lingjie Jiang,Shaohan Huang,Xun Wu,Yixia Li,Dongdong Zhang,Furu Wei*

Main category: cs.CL

TL;DR: 本文提出了VisCodex系统，将视觉与代码生成能力结合，实现多模态大语言模型（MLLMs）在多模输入下的强大代码生成能力。通过模型融合与新数据集，性能优于现有开源模型，接近GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大语言模型虽能理解视觉与文本，但在多模态输入驱动的代码生成方面能力有限。研究者希望加强模型面对复杂多模任务（如图像-代码生成、编程问答）的能力。

Method: 提出VisCodex框架，采用任务向量驱动的模型融合方法，将先进的代码大语言模型和视觉-语言主干网络结合，保留视觉理解和高阶代码生成能力。同时构建了多模态代码数据集（MCD, 598k样本）和新的复杂基准InfiBench-V进行评估和训练。

Result: VisCodex在多项任务、尤其是复杂多模态代码生成上取得了开源MLLMs中的最优表现，并接近商用闭源模型（如GPT-4o）。

Conclusion: VisCodex证明了模型融合与专用多模态数据集在提升多模态代码生成能力上的有效性，为多模态大语言模型的下一步发展提供了新方向。

Abstract: Multimodal large language models (MLLMs) have significantly advanced the
integration of visual and textual understanding. However, their ability to
generate code from multimodal inputs remains limited. In this work, we
introduce VisCodex, a unified framework that seamlessly merges vision and
coding language models to empower MLLMs with strong multimodal code generation
abilities. Leveraging a task vector-based model merging technique, we integrate
a state-of-the-art coding LLM into a strong vision-language backbone, while
preserving both visual comprehension and advanced coding skills. To support
training and evaluation, we introduce the Multimodal Coding Dataset (MCD), a
large-scale and diverse collection of 598k samples, including high-quality HTML
code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic
problems. Furthermore, we propose InfiBench-V, a novel and challenging
benchmark specifically designed to assess models on visually-rich, real-world
programming questions that demand a nuanced understanding of both textual and
visual contexts. Extensive experiments show that VisCodex achieves
state-of-the-art performance among open-source MLLMs and approaches proprietary
models like GPT-4o, highlighting the effectiveness of our model merging
strategy and new datasets.

</details>


### [163] [Specialised or Generic? Tokenization Choices for Radiology Language Models](https://arxiv.org/abs/2508.09952)
*Hermione Warr,Wentian Xu,Harry Anthony,Yasin Ibrahim,Daniel McGowan,Konstantinos Kamnitsas*

Main category: cs.CL

TL;DR: 该论文系统性地比较了不同分词器对于放射学报告摘要生成任务的影响，发现领域专属分词器在多方面均表现更优。


<details>
  <summary>Details</summary>
Motivation: 语言模型的分词器对生成文本质量影响显著，但在医学放射学场景下分词器的影响尚未被充分研究。

Method: 作者比较了通用分词器、医学分词器和放射学领域专用分词器在三种影像模态的放射学报告摘要任务上的表现，并探讨了是否在 PubMed 语料上进行预训练对结果的影响。

Result: 实验发现：领域专用分词器训练的新模型性能优于自然语言类分词器；预训练可以缩小不同分词器间的性能差距，但领域专用分词器依然最佳。同时，领域专用分词器因词表更小、平均句子分词更短，降低了内存和计算需求。

Conclusion: 针对医学临床领域优化分词器不仅提升了模型性能，还减少了资源消耗，对研究和实际医疗具有重要现实意义。

Abstract: The vocabulary used by language models (LM) - defined by the tokenizer -
plays a key role in text generation quality. However, its impact remains
under-explored in radiology. In this work, we address this gap by
systematically comparing general, medical, and domain-specific tokenizers on
the task of radiology report summarisation across three imaging modalities. We
also investigate scenarios with and without LM pre-training on PubMed
abstracts. Our findings demonstrate that medical and domain-specific
vocabularies outperformed widely used natural language alternatives when models
are trained from scratch. Pre-training partially mitigates performance
differences between tokenizers, whilst the domain-specific tokenizers achieve
the most favourable results. Domain-specific tokenizers also reduce memory
requirements due to smaller vocabularies and shorter sequences. These results
demonstrate that adapting the vocabulary of LMs to the clinical domain provides
practical benefits, including improved performance and reduced computational
demands, making such models more accessible and effective for both research and
real-world healthcare settings.

</details>


### [164] [Shaping Event Backstories to Estimate Potential Emotion Contexts](https://arxiv.org/abs/2508.09954)
*Johannes Schäfer,Roman Klinger*

Main category: cs.CL

TL;DR: 本文提出通过为事件描述添加合理上下文，提升情感分析标注一致性。作者利用事件链生成方法与短故事生成技术，系统地探索上下文对情感标注的影响，实验证明丰富的上下文信息能提升标注者的一致性。


<details>
  <summary>Details</summary>
Motivation: 情感分析常因事件本身信息不足导致标注者分歧，过去仅关注标注者差异，忽视了上下文缺失也会增加歧义。为探究上下文缺失对标注一致性的影响，作者提出补充合理上下文信息的方法。

Method: 自动生成与不同情感对应的多条事件链，为同一事件描述提供多种上下文叙事。结合短故事生成技术，确保上下文连贯性和丰富性，并构建专业数据集进行系统化分析。

Result: 自动和人工评估均显示，添加上下文叙事能够提升特定情感理解的准确性，使标注者对事件的情感标注更一致、更可靠。

Conclusion: 丰富事件描述的上下文可有效减少情感标注歧义，支持标注者做出更一致的情感判断，对情感分析的研究与应用具有积极促进作用。

Abstract: Emotion analysis is an inherently ambiguous task. Previous work studied
annotator properties to explain disagreement, but this overlooks the
possibility that ambiguity may stem from missing information about the context
of events. In this paper, we propose a novel approach that adds reasonable
contexts to event descriptions, which may better explain a particular
situation. Our goal is to understand whether these enriched contexts enable
human annotators to annotate emotions more reliably. We disambiguate a target
event description by automatically generating multiple event chains conditioned
on differing emotions. By combining techniques from short story generation in
various settings, we achieve coherent narratives that result in a specialized
dataset for the first comprehensive and systematic examination of
contextualized emotion analysis. Through automatic and human evaluation, we
find that contextual narratives enhance the interpretation of specific emotions
and support annotators in producing more consistent annotations.

</details>


### [165] [Performance of GPT-5 Frontier Models in Ophthalmology Question Answering](https://arxiv.org/abs/2508.09956)
*Fares Antaki,David Mikhail,Daniel Milad,Danny A Mammo,Sumit Sharma,Sunil K Srivastava,Bing Yu Chen,Samir Touma,Mertcan Sevgi,Jonathan El-Khoury,Pearse A Keane,Qingyu Chen,Yih Chung Tham,Renaud Duval*

Main category: cs.CL

TL;DR: 本文系统评估了GPT-5系列及多种相关模型在眼科学高质量考试题集（BCSC数据集）上的表现，比较了不同模型和配置在准确率、解释质量及成本效益方面的优劣。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在医学问答领域显示出高度潜力，但针对最优的模型配置以同时最大化准确率及成本效益，仍缺乏系统性研究。眼科学作为知识密集型领域，对于自动化、可信问答工具有迫切需求。作者试图补全相关研究空白。

Method: 作者选取了12个GPT-5系列配置（3个模型等级×4种推理努力设置）及其他主流模型（o1-high、o3-high、GPT-4o），基于美国眼科学会BCSC闭卷单选题共计260题，系统测评各模型的准确率、解释合理性（LLM打分），并量化分析模型准确率与计算成本之间的权衡。

Result: GPT-5-high取得最高答题准确率（96.5%），显著优于多数小型模型，也优于GPT-4o及o1-high，但与o3-high差异不显著。GPT-5-high在准确率及解释质量均排名首位。成本效益分析显示，GPT-5-mini-low在成本与性能间达到最优平衡。

Conclusion: 本文首次为GPT-5系列在高质量眼科数据集上的能力做出标杆测试，显示推理深度对模型准确性有明显提升，同时提出了一种可扩展的自动判分框架，有助于未来临床领域大模型的评估和推广。

Abstract: Large language models (LLMs) such as GPT-5 integrate advanced reasoning
capabilities that may improve performance on complex medical question-answering
tasks. For this latest generation of reasoning models, the configurations that
maximize both accuracy and cost-efficiency have yet to be established. We
evaluated 12 configurations of OpenAI's GPT-5 series (three model tiers across
four reasoning effort settings) alongside o1-high, o3-high, and GPT-4o, using
260 closed-access multiple-choice questions from the American Academy of
Ophthalmology Basic Clinical Science Course (BCSC) dataset. The primary outcome
was multiple-choice accuracy; secondary outcomes included head-to-head ranking
via a Bradley-Terry model, rationale quality assessment using a
reference-anchored, pairwise LLM-as-a-judge framework, and analysis of
accuracy-cost trade-offs using token-based cost estimates. GPT-5-high achieved
the highest accuracy (0.965; 95% CI, 0.942-0.985), outperforming all GPT-5-nano
variants (P < .001), o1-high (P = .04), and GPT-4o (P < .001), but not o3-high
(0.958; 95% CI, 0.931-0.981). GPT-5-high ranked first in both accuracy (1.66x
stronger than o3-high) and rationale quality (1.11x stronger than o3-high).
Cost-accuracy analysis identified several GPT-5 configurations on the Pareto
frontier, with GPT-5-mini-low offering the most favorable low-cost,
high-performance balance. These results benchmark GPT-5 on a high-quality
ophthalmology dataset, demonstrate the influence of reasoning effort on
accuracy, and introduce an autograder framework for scalable evaluation of
LLM-generated answers against reference standards in ophthalmology.

</details>


### [166] [Which one Performs Better? Wav2Vec or Whisper? Applying both in Badini Kurdish Speech to Text (BKSTT)](https://arxiv.org/abs/2508.09957)
*Renas Adnan,Hossein Hassani*

Main category: cs.CL

TL;DR: 本研究开发了巴迪尼（Badini）库尔德语方言的语音识别系统（STT），并对两种主流模型进行了性能评估。


<details>
  <summary>Details</summary>
Motivation: 尽管库尔德语在某些方言（如索拉尼）已有语音识别系统，但对巴迪尼等其他重要方言仍缺乏支持。为提升该方言社群的技术普及率与全球能见度，亟须建立相应的STT系统。

Method: 研究团队采集了来自六位叙述者的巴迪尼儿童故事音频（共约17小时、8本书、78个故事），经过清洗与分割后，预处理得到约15小时音频、19193段语音、25221词语，分别基于Wav2Vec2-Large-XLSR-53和Whisper-small模型进行训练和测试。

Result: Wav2Vec2-Large-XLSR-53模型的输出在可读性（90.38%）与准确率（82.67%）上均显著优于Whisper-small模型（可读性65.45%，准确率53.17%）。

Conclusion: 为巴迪尼方言建立了第一个系统化的STT语音识别模型，其中Wav2Vec2-Large-XLSR-53表现最佳。该成果有助于提升相关社群的数字化水平，并为低资源语种的STT研究提供实证依据。

Abstract: Speech-to-text (STT) systems have a wide range of applications. They are
available in many languages, albeit at different quality levels. Although
Kurdish is considered a less-resourced language from a processing perspective,
SST is available for some of the Kurdish dialects, for instance, Sorani
(Central Kurdish). However, that is not applied to other Kurdish dialects,
Badini and Hawrami, for example. This research is an attempt to address this
gap. Bandin, approximately, has two million speakers, and STT systems can help
their community use mobile and computer-based technologies while giving their
dialect more global visibility. We aim to create a language model based on
Badini's speech and evaluate its performance. To cover a conversational aspect,
have a proper confidence level of grammatical accuracy, and ready
transcriptions, we chose Badini kids' stories, eight books including 78
stories, as the textual input. Six narrators narrated the books, which resulted
in approximately 17 hours of recording. We cleaned, segmented, and tokenized
the input. The preprocessing produced nearly 15 hours of speech, including
19193 segments and 25221 words. We used Wav2Vec2-Large-XLSR-53 and
Whisper-small to develop the language models. The experiments indicate that the
transcriptions process based on the Wav2Vec2-Large-XLSR-53 model provides a
significantly more accurate and readable output than the Whisper-small model,
with 90.38% and 65.45% readability, and 82.67% and 53.17% accuracy,
respectively.

</details>


### [167] [Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks](https://arxiv.org/abs/2508.09958)
*Baran Atalar,Eddie Zhang,Carlee Joe-Wong*

Main category: cs.CL

TL;DR: 本论文提出了一种基于神经上下文赌博机的算法，能在任务分解为多个子任务时，动态选择最能完成每个子任务的大语言模型（LLM）序列，从而提升整体任务效果和成本效率。实验验证了该方法在电信问答和医疗诊断任务中的优越性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）在各类任务中的普及，如何以低成本预测并选择最有可能成功的LLM成为重要问题。尤其在任务过于复杂或专业，单一LLM难以胜任，需要分解为多个子任务，由不同LLM协作完成时，传统的单LLM选择方法无法解决子任务串联带来的复杂性能依赖问题。

Method: 作者提出一种基于神经网络的上下文赌博机算法，针对每个任务子步骤训练神经网络，用于动态建模各LLM在不同子任务上的成功概率，并在无历史数据的在线环境下进行适应性学习和LLM序列选择。该方法考虑了各子任务间输出结果对下游输入、成本和成功率的连锁影响。

Result: 在电信问答和医疗诊断数据集上的实验表明，所提方法较其他LLM选择算法具有更高的任务成功率和更优的成本效率。

Conclusion: 论文表明，将复杂任务分解为子任务并根据任务上下文动态选择LLM序列，能显著提升多LLM协作系统的表现。提出的神经上下文赌博机算法有效应对了子任务间性能依赖问题，具有普适应用前景。

Abstract: With the increasing popularity of large language models (LLMs) for a variety
of tasks, there has been a growing interest in strategies that can predict
which out of a set of LLMs will yield a successful answer at low cost. This
problem promises to become more and more relevant as providers like Microsoft
allow users to easily create custom LLM "assistants" specialized to particular
types of queries. However, some tasks (i.e., queries) may be too specialized
and difficult for a single LLM to handle alone. These applications often
benefit from breaking down the task into smaller subtasks, each of which can
then be executed by a LLM expected to perform well on that specific subtask.
For example, in extracting a diagnosis from medical records, one can first
select an LLM to summarize the record, select another to validate the summary,
and then select another, possibly different, LLM to extract the diagnosis from
the summarized record. Unlike existing LLM selection or routing algorithms,
this setting requires that we select a sequence of LLMs, with the output of
each LLM feeding into the next and potentially influencing its success. Thus,
unlike single LLM selection, the quality of each subtask's output directly
affects the inputs, and hence the cost and success rate, of downstream LLMs,
creating complex performance dependencies that must be learned and accounted
for during selection. We propose a neural contextual bandit-based algorithm
that trains neural networks that model LLM success on each subtask in an online
manner, thus learning to guide the LLM selections for the different subtasks,
even in the absence of historical LLM performance data. Experiments on
telecommunications question answering and medical diagnosis prediction datasets
illustrate the effectiveness of our proposed approach compared to other LLM
selection algorithms.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [168] [Decision-Making-Based Path Planning for Autonomous UAVs: A Survey](https://arxiv.org/abs/2508.09304)
*Kelen C. Teixeira Vivaldini,Robert Pěnička,Martin Saska*

Main category: cs.RO

TL;DR: 本文综述了自主无人机在路径规划过程中基于决策制定的相关研究，重点关注探索性路径规划和信息性路径规划领域。


<details>
  <summary>Details</summary>
Motivation: 自主无人机在实际应用中需要根据环境信息作出决策，路径规划中的决策能力直接影响其飞行性能和任务完成效果，因此需要系统梳理现有相关研究进展。

Method: 本文对当前文献中涉及路径规划决策的研究进行综述，归纳了探索路径规划和信息路径规划两个主要研究方向，并分析了数据建模与理解的方法。

Result: 通过调研，文章梳理了现有方法的研究脉络、数据建模特点，并展示了这两类路径规划的研究进展及应用。

Conclusion: 自主无人机路径规划中的决策制定是实现其自主性的关键，当前在信息建模和实际部署方面仍存在诸多挑战，未来需进一步突破其关键难点。

Abstract: One of the most critical features for the successful operation of autonomous
UAVs is the ability to make decisions based on the information acquired from
their surroundings. Each UAV must be able to make decisions during the flight
in order to deal with uncertainties in its system and the environment, and to
further act upon the information being received. Such decisions influence the
future behavior of the UAV, which is expressed as the path plan. Thus,
decision-making in path planning is an enabling technique for deploying
autonomous UAVs in real-world applications. This survey provides an overview of
existing studies that use aspects of decision-making in path planning,
presenting the research strands for Exploration Path Planning and Informative
Path Planning, and focusing on characteristics of how data have been modeled
and understood. Finally, we highlight the existing challenges for relevant
topics in this field.

</details>


### [169] [How Safe Will I Be Given What I Saw? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy](https://arxiv.org/abs/2508.09346)
*Zhenjiang Mao,Mrinall Eashaan Umasudhan,Ivan Ruchkin*

Main category: cs.RO

TL;DR: 本文提出了一种用于自主机器人视觉控制系统安全性的校准预测框架，能够在缺乏状态和观测模型情况下，通过世界模型和无监督领域自适应，提高长时序高维观测下的安全预测准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络控制的自主机器人难以在部分可观测和分布偏移情况下预测安全性。现有基于模型的方法对状态建模有要求且难以扩展，而无模型方法又缺乏可靠性保证。因此，亟需一种既无需模型访问、又具备理论可靠性的安全预测方法。

Method: 提出基于世界模型的安全预测方法，使用变分自编码器(VAE)和递归预测器，从原始图像序列中预测未来潜变量轨迹，再估算安全属性的满足概率。将整体式和复合式预测管线加以区分，并引入校准机制度量置信度。同时结合无监督领域自适应(UDA)，提升分布偏移下的鲁棒性，无需人工标注。

Result: 实验在三个基准测试上验证，采用UDA的安全评估器在分布偏移下依然保持高准确率和显著降低的误报率。基于世界模型的复合式预测器在长时序任务上优于整体式预测器，所提出的保形校准也提供了可靠的统计界限。

Conclusion: 该方法在缺少状态/观测模型情况下提供了有理论保证且在实际环境中表现优异的安全预测能力，适用于复杂分布偏移、长预测时域等挑战性场景，并为自主机器人安全预测提供了可行路径。

Abstract: Autonomous robots that rely on deep neural network controllers pose critical
challenges for safety prediction, especially under partial observability and
distribution shift. Traditional model-based verification techniques are limited
in scalability and require access to low-dimensional state models, while
model-free methods often lack reliability guarantees. This paper addresses
these limitations by introducing a framework for calibrated safety prediction
in end-to-end vision-controlled systems, where neither the state-transition
model nor the observation model is accessible. Building on the foundation of
world models, we leverage variational autoencoders and recurrent predictors to
forecast future latent trajectories from raw image sequences and estimate the
probability of satisfying safety properties. We distinguish between monolithic
and composite prediction pipelines and introduce a calibration mechanism to
quantify prediction confidence. In long-horizon predictions from
high-dimensional observations, the forecasted inputs to the safety evaluator
can deviate significantly from the training distribution due to compounding
prediction errors and changing environmental conditions, leading to
miscalibrated risk estimates. To address this, we incorporate unsupervised
domain adaptation to ensure robustness of safety evaluation under distribution
shift in predictions without requiring manual labels. Our formulation provides
theoretical calibration guarantees and supports practical evaluation across
long prediction horizons. Experimental results on three benchmarks show that
our UDA-equipped evaluators maintain high accuracy and substantially lower
false positive rates under distribution shift. Similarly, world model-based
composite predictors outperform their monolithic counterparts on long-horizon
tasks, and our conformal calibration provides reliable statistical bounds.

</details>


### [170] [CLF-RL: Control Lyapunov Function Guided Reinforcement Learning](https://arxiv.org/abs/2508.09354)
*Kejun Li,Zachary Olkin,Yisong Yue,Aaron D. Ames*

Main category: cs.RO

TL;DR: 提出了一种结合模型规划和控制Lyapunov函数（CLF）的结构化奖励设计框架，显著提升了双足机器人运动稳定性的RL训练，并通过实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在双足机器人运动控制上存在奖励设计繁琐且对目标函数敏感的问题，需要更有效、稳健的奖励设计方法提升实用性和鲁棒性。

Method: 采用两种模型规划器（LIP模型和HZD步态库）生成参考轨迹，通过CLF引入奖励项来惩罚跟踪误差并促进收敛。参考轨迹和CLF奖励只在训练期用，推理时无需调用，因此最终策略轻量化。

Result: 在仿真和真实Unitree G1机器人实验下，提出的CLF-RL方法在鲁棒性与性能上均优于传统RL奖励方式和跟踪型奖励RL。

Conclusion: CLF与模型规划器结合的结构化奖励设计能有效提升双足机器人RL策略的可靠性和泛化能力，实用性较强，部署方便。

Abstract: Reinforcement learning (RL) has shown promise in generating robust locomotion
policies for bipedal robots, but often suffers from tedious reward design and
sensitivity to poorly shaped objectives. In this work, we propose a structured
reward shaping framework that leverages model-based trajectory generation and
control Lyapunov functions (CLFs) to guide policy learning. We explore two
model-based planners for generating reference trajectories: a reduced-order
linear inverted pendulum (LIP) model for velocity-conditioned motion planning,
and a precomputed gait library based on hybrid zero dynamics (HZD) using
full-order dynamics. These planners define desired end-effector and joint
trajectories, which are used to construct CLF-based rewards that penalize
tracking error and encourage rapid convergence. This formulation provides
meaningful intermediate rewards, and is straightforward to implement once a
reference is available. Both the reference trajectories and CLF shaping are
used only during training, resulting in a lightweight policy at deployment. We
validate our method both in simulation and through extensive real-world
experiments on a Unitree G1 robot. CLF-RL demonstrates significantly improved
robustness relative to the baseline RL policy and better performance than a
classic tracking reward RL formulation.

</details>


### [171] [DAgger Diffusion Navigation: DAgger Boosted Diffusion Policy for Vision-Language Navigation](https://arxiv.org/abs/2508.09444)
*Haoxiang Shi,Xiang Deng,Zaijing Li,Gongwei Chen,Yaowei Wang,Liqiang Nie*

Main category: cs.RO

TL;DR: 本文提出了一种全新的端到端视觉-语言导航（VLN-CE）方法DifNav，避免了传统两阶段航点规划框架的局限性，并在多个基准数据集上取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLN-CE方法一般采用航点预测再导航规划的两阶段策略，但这种方案存在每阶段目标分解后的次优性，以及过分依赖第一阶段质量导致性能瓶颈的问题。

Method: 本文提出DifNav，将航点生成与规划两阶段统一为单一扩散策略，直接在连续导航空间建模多模态行为分布，并采用DAgger算法实现策略在线训练和专家轨迹增强，最终利用聚合数据微调模型，提高鲁棒性和容错能力。

Result: 实验表明，DifNav无需航点预测器即可在多个VLN-CE基准任务上显著优于现有两阶段方法，导航性能达到新的SOTA水平。

Conclusion: DifNav突破传统方法的结构性限制，展现了端到端扩散策略在复杂导航任务中的巨大潜力，为VLN-CE研究提供了新思路。

Abstract: Vision-Language Navigation in Continuous Environments (VLN-CE) requires
agents to follow natural language instructions through free-form 3D spaces.
Existing VLN-CE approaches typically use a two-stage waypoint planning
framework, where a high-level waypoint predictor generates the navigable
waypoints, and then a navigation planner suggests the intermediate goals in the
high-level action space. However, this two-stage decomposition framework
suffers from: (1) global sub-optimization due to the proxy objective in each
stage, and (2) a performance bottleneck caused by the strong reliance on the
quality of the first-stage predicted waypoints. To address these limitations,
we propose DAgger Diffusion Navigation (DifNav), an end-to-end optimized VLN-CE
policy that unifies the traditional two stages, i.e. waypoint generation and
planning, into a single diffusion policy. Notably, DifNav employs a conditional
diffusion policy to directly model multi-modal action distributions over future
actions in continuous navigation space, eliminating the need for a waypoint
predictor while enabling the agent to capture multiple possible
instruction-following behaviors. To address the issues of compounding error in
imitation learning and enhance spatial reasoning in long-horizon navigation
tasks, we employ DAgger for online policy training and expert trajectory
augmentation, and use the aggregated data to further fine-tune the policy. This
approach significantly improves the policy's robustness and its ability to
recover from error states. Extensive experiments on benchmark datasets
demonstrate that, even without a waypoint predictor, the proposed method
substantially outperforms previous state-of-the-art two-stage waypoint-based
models in terms of navigation performance. Our code is available at:
https://github.com/Tokishx/DifNav.

</details>


### [172] [Reactive Model Predictive Contouring Control for Robot Manipulators](https://arxiv.org/abs/2508.09502)
*Junheon Yoon,Woo-Jeong Baek,Jaeheung Park*

Main category: cs.RO

TL;DR: 本文提出了一种基于RMPCC的机器人路径跟踪框架，可在动态环境下以100Hz高频率实现障碍物、奇异点和自碰撞的规避。


<details>
  <summary>Details</summary>
Motivation: 现有路径跟踪方法依赖时间参数化，难以同时兼顾碰撞和奇异点规避及运动学约束，尤其在紧急规避动作时会导致较大路径跟踪误差。

Method: 本文采用以路径参数为基准的路径参数化方式，通过RMPCC进行优化，并引入控制屏障函数（CBF）以实现动态环境下的碰撞和奇异点规避。利用基于雅可比的线性化方法与Gauss-Newton Hessian近似，使系统能以100Hz频率求解非线性RMPCC问题。

Result: 该方法计算效率比当前先进方法高10倍，并在实际动态障碍物环境中取得了低轮廓误差和低机器人加速度的优秀性能表现。

Conclusion: 实验验证了该框架可有效处理实际场景中的动态障碍问题，在保证路径精度和运动平稳性的同时，大幅提升了反应速度。

Abstract: This contribution presents a robot path-following framework via Reactive
Model Predictive Contouring Control (RMPCC) that successfully avoids obstacles,
singularities and self-collisions in dynamic environments at 100 Hz. Many
path-following methods rely on the time parametrization, but struggle to handle
collision and singularity avoidance while adhering kinematic limits or other
constraints. Specifically, the error between the desired path and the actual
position can become large when executing evasive maneuvers. Thus, this paper
derives a method that parametrizes the reference path by a path parameter and
performs the optimization via RMPCC. In particular, Control Barrier Functions
(CBFs) are introduced to avoid collisions and singularities in dynamic
environments. A Jacobian-based linearization and Gauss-Newton Hessian
approximation enable solving the nonlinear RMPCC problem at 100 Hz,
outperforming state-of-the-art methods by a factor of 10. Experiments confirm
that the framework handles dynamic obstacles in real-world settings with low
contouring error and low robot acceleration.

</details>


### [173] [SMART-OC: A Real-time Time-risk Optimal Replanning Algorithm for Dynamic Obstacles and Spatio-temporally Varying Currents](https://arxiv.org/abs/2508.09508)
*Reema Raval,Shalabh Gupta*

Main category: cs.RO

TL;DR: 本文提出了一种名为SMART-OC的新算法，使无人水面艇(USVs)能在复杂变化的海洋环境中实现实时的、兼顾安全与效率的路径再规划。


<details>
  <summary>Details</summary>
Motivation: 复杂的海洋环境中，水流和动态障碍物不断变化，给USVs安全高效航行带来很大挑战；因此需要能够实时感知并适应环境变化的路径规划方法。

Method: 提出SMART-OC算法，将动态障碍物风险与到目标的时间代价综合考量，实时为USVs计算出风险-时间最优路径，对环境变化快速做出反应。

Result: 通过仿真实验验证SMART-OC的有效性，结果显示该算法能够让USV快速重新规划路径，及时躲避动态障碍，利用洋流顺利到达目标。

Conclusion: SMART-OC算法提升了USVs在动态海洋环境中自主导航的安全性与效率，在未来USV自主航行领域具有应用潜力。

Abstract: Typical marine environments are highly complex with spatio-temporally varying
currents and dynamic obstacles, presenting significant challenges to Unmanned
Surface Vehicles (USVs) for safe and efficient navigation. Thus, the USVs need
to continuously adapt their paths with real-time information to avoid
collisions and follow the path of least resistance to the goal via exploiting
ocean currents. In this regard, we introduce a novel algorithm, called
Self-Morphing Adaptive Replanning Tree for dynamic Obstacles and Currents
(SMART-OC), that facilitates real-time time-risk optimal replanning in dynamic
environments. SMART-OC integrates the obstacle risks along a path with the time
cost to reach the goal to find the time-risk optimal path. The effectiveness of
SMART-OC is validated by simulation experiments, which demonstrate that the USV
performs fast replannings to avoid dynamic obstacles and exploit ocean currents
to successfully reach the goal.

</details>


### [174] [CaRoBio: 3D Cable Routing with a Bio-inspired Gripper Fingernail](https://arxiv.org/abs/2508.09558)
*Jiahui Zuo,Boyang Zhang,Fumin Zhang*

Main category: cs.RO

TL;DR: 本论文提出了一种仿鹰爪指甲的新型机械手爪结构，并基于此开发了高效的3D电缆布线机器人系统，有效提升了机器人对柔性线缆的抓取和操控能力，明显优于传统抓取方式。


<details>
  <summary>Details</summary>
Motivation: 当前机器人在柔性线缆（如电缆）布线场景下常因通用二指夹爪容易夹紧过度或拉伸过度，导致难以精细操作且存在损坏风险。传统抓取-放置（pick-and-place）策略效率有限，急需更可靠高效的自动化方法。

Method: 1）设计了仿鹰爪指甲的新型指端结构，附加于机械手爪；2）借此提出单次抓取的3D电缆布线“端到端”操作框架，不依赖传统多次移动；3）结合视觉感知的状态估计和离线轨迹规划，实现连续高效的线缆操控。

Result: 在各种类型的电缆和通道槽环境下进行了实验，所提出的框架在等同感知环境下显著优于传统的抓取-放置流程，表现更好。

Conclusion: 仿生指甲+单次抓取布线框架在提升机器人柔性线材操作表现上效果突出，方案为3D空间内复杂线缆布线任务的自动化提供了新思路，为后续相关研究与应用奠定了基础。

Abstract: The manipulation of deformable linear flexures has a wide range of
applications in industry, such as cable routing in automotive manufacturing and
textile production. Cable routing, as a complex multi-stage robot manipulation
scenario, is a challenging task for robot automation. Common parallel
two-finger grippers have the risk of over-squeezing and over-tension when
grasping and guiding cables. In this paper, a novel eagle-inspired fingernail
is designed and mounted on the gripper fingers, which helps with cable grasping
on planar surfaces and in-hand cable guiding operations. Then we present a
single-grasp end-to-end 3D cable routing framework utilizing the proposed
fingernails, instead of the common pick-and-place strategy. Continuous control
is achieved to efficiently manipulate cables through vision-based state
estimation of task configurations and offline trajectory planning based on
motion primitives. We evaluate the effectiveness of the proposed framework with
a variety of cables and channel slots, significantly outperforming the
pick-and-place manipulation process under equivalent perceptual conditions. Our
reconfigurable task setting and the proposed framework provide a reference for
future cable routing manipulations in 3D space.

</details>


### [175] [ESCoT: An Enhanced Step-based Coordinate Trajectory Planning Method for Multiple Car-like Robots](https://arxiv.org/abs/2508.09581)
*Junkai Jiang,Yihe Chen,Yibin Yang,Ruochen Li,Shaobing Xu,Jianqiang Wang*

Main category: cs.RO

TL;DR: 本文提出了一种针对多车类机器人轨迹规划的增强型逐步协调方法ESCoT，显著提升了解决质量与效率，并通过实验和真实机器人验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 多车系统的轨迹规划（MVTP）存在高复杂度、冲突处理难和实时性要求，传统方法在稠密或冲突场景往往存在性能瓶颈，因此需要提升MVTP方法的可靠性与效率。

Method: 提出ESCoT算法，结合本地小组协同规划与重复配置再规划两种策略，增强了基础逐步（step-based）轨迹规划方法的性能。算法通过分组协作和及时调整，有效避免冲突和重复配置问题。

Result: 在稀疏场景下，ESCoT相比基础方法典型冲突下提升了70%，随机场景提升34%；稠密场景下仍优于所有对比基线方法，并在最困难场景中保持超过50%的成功率。还通过实际机器人实验验证了算法的有效性。

Conclusion: ESCoT方法有效提升了多车类机器人轨迹规划的求解能力，特别是在高冲突与高密度场景下，扩展了逐步方法的适用范围并具备现实应用潜力。

Abstract: Multi-vehicle trajectory planning (MVTP) is one of the key challenges in
multi-robot systems (MRSs) and has broad applications across various fields.
This paper presents ESCoT, an enhanced step-based coordinate trajectory
planning method for multiple car-like robots. ESCoT incorporates two key
strategies: collaborative planning for local robot groups and replanning for
duplicate configurations. These strategies effectively enhance the performance
of step-based MVTP methods. Through extensive experiments, we show that ESCoT
1) in sparse scenarios, significantly improves solution quality compared to
baseline step-based method, achieving up to 70% improvement in typical conflict
scenarios and 34% in randomly generated scenarios, while maintaining high
solving efficiency; and 2) in dense scenarios, outperforms all baseline
methods, maintains a success rate of over 50% even in the most challenging
configurations. The results demonstrate that ESCoT effectively solves MVTP,
further extending the capabilities of step-based methods. Finally, practical
robot tests validate the algorithm's applicability in real-world scenarios.

</details>


### [176] [HapticGiant: A Novel Very Large Kinesthetic Haptic Interface with Hierarchical Force Control](https://arxiv.org/abs/2508.09595)
*Michael Fennel,Markus Walker,Dominik Pikos,Uwe D. Hanebeck*

Main category: cs.RO

TL;DR: 本文提出了一种新的大尺度动力触觉接口HapticGiant，通过创新的力控制方案，提高虚拟现实中的沉浸感。


<details>
  <summary>Details</summary>
Motivation: 虚拟现实虽然在视觉沉浸感上已取得较大进步，但动力触觉设备依然受限于空间、自由度和运动学不匹配等问题，难以实现真正的人体自然交互体验。为提升虚拟现实中的沉浸感，亟需开发既能全方位反馈又与人体运动习惯相符的动力触觉接口。

Method: 提出并实现了HapticGiant动力触觉接口，设计上与人臂运动特性高度匹配，并支持自然移动。采用新颖的容纳型力控制算法，通过分层优化处理，能够渲染任意串联运动链和笛卡尔空间的动态，且可自动处理系统约束和奇异点。

Result: 实验验证了HapticGiant及其控制方案的有效性，在保证用户自然运动的同时，实现了全向运动反馈能力。

Conclusion: HapticGiant为虚拟现实动力触觉交互提供了突破性方案，具备广阔的应用前景，有望显著提升虚拟现实的沉浸体验。

Abstract: Research in virtual reality and haptic technologies has consistently aimed to
enhance immersion. While advanced head-mounted displays are now commercially
available, kinesthetic haptic interfaces still face challenges such as limited
workspaces, insufficient degrees of freedom, and kinematics not matching the
human arm. In this paper, we present HapticGiant, a novel large-scale
kinesthetic haptic interface designed to match the properties of the human arm
as closely as possible and to facilitate natural user locomotion while
providing full haptic feedback. The interface incorporates a novel
admittance-type force control scheme, leveraging hierarchical optimization to
render both arbitrary serial kinematic chains and Cartesian admittances.
Notably, the proposed control scheme natively accounts for system limitations,
including joint and Cartesian constraints, as well as singularities.
Experimental results demonstrate the effectiveness of HapticGiant and its
control scheme, paving the way for highly immersive virtual reality
applications.

</details>


### [177] [BEAVR: Bimanual, multi-Embodiment, Accessible, Virtual Reality Teleoperation System for Robots](https://arxiv.org/abs/2508.09606)
*Alejandro Posadas-Nava,Alejandro Carrasco,Richard Linares*

Main category: cs.RO

TL;DR: BEAVR 是一个开源的双手多实体虚拟现实机器人遥操作系统，支持多平台、低延迟遥操作、数据记录和策略学习，兼容多种机器人和先进策略方法。


<details>
  <summary>Details</summary>
Motivation: 当前机器人遥操作系统缺乏通用、实时、便于策略学习与数据采集的工具，且跨多机器人平台的兼容性较差。本系统旨在统一这些需求，推动机器人感知和控制的数据驱动发展。

Method: BEAVR 利用通用VR设备，对多类型机器人（从机械臂到全身人形机器人）进行实时远程操控。系统采用零拷贝流媒体架构，实现低延迟；支持异步控制回路和灵活网络API，便于多机器人并行操作。并直接按LeRobot数据集格式记录数据，易于后续机器学习策略训练。

Result: BEAVR 在多个复杂操作任务进行了基准评测，证明了其低延迟、高兼容性以及对主流视觉-动作策略（如ACT、DiffusionPolicy和SmolVLA）的支持。所有代码和数据集公开。

Conclusion: BEAVR 有效推动了多机器人系统的遥操作研究，为学术界和工业界提供了一个低门槛、高扩展性的遥操作与示范数据采集平台，有助于机器人自主策略的持续进步。

Abstract: \textbf{BEAVR} is an open-source, bimanual, multi-embodiment Virtual Reality
(VR) teleoperation system for robots, designed to unify real-time control, data
recording, and policy learning across heterogeneous robotic platforms. BEAVR
enables real-time, dexterous teleoperation using commodity VR hardware,
supports modular integration with robots ranging from 7-DoF manipulators to
full-body humanoids, and records synchronized multi-modal demonstrations
directly in the LeRobot dataset schema. Our system features a zero-copy
streaming architecture achieving $\leq$35\,ms latency, an asynchronous
``think--act'' control loop for scalable inference, and a flexible network API
optimized for real-time, multi-robot operation. We benchmark BEAVR across
diverse manipulation tasks and demonstrate its compatibility with leading
visuomotor policies such as ACT, DiffusionPolicy, and SmolVLA. All code is
publicly available, and datasets are released on Hugging Face\footnote{Code,
datasets, and VR app available at https://github.com/ARCLab-MIT/BEAVR-Bot.

</details>


### [178] [Interpretable Robot Control via Structured Behavior Trees and Large Language Models](https://arxiv.org/abs/2508.09621)
*Ingrid Maéva Chekam,Ines Pastor-Martinez,Ali Tourani,Jose Andres Millan-Romera,Laura Ribeiro,Pedro Miguel Bastos Soares,Holger Voos,Jose Luis Sanchez-Lopez*

Main category: cs.RO

TL;DR: 本文提出了一种将大型语言模型（LLM）与行为树结合的机器人控制新框架，可使机器人理解自然语言指令并执行相应动作，验证了其实用性和高准确率。


<details>
  <summary>Details</summary>
Motivation: 随着智能机器人在人类环境中的普及，对更自然、直观的人机交互界面的需求不断增长，而传统方法存在适应性差、学习成本高等问题。

Method: 将大型语言模型（LLM）与行为树集成，通过自然语言理解驱动行为树中具体插件的激活，实现机器人理解并执行用户的自然语言指令。框架支持模块化、可扩展，强调感知类功能，例如人员跟踪和手势识别。通过多种真实环境下的实验进行验证。

Result: 实验证明该方法在不同环境下都具有较高的实用性，认知到执行的准确率约为94%。

Conclusion: 本方法有效提升了人机交互的自然性和实用性，对HRI领域做出了重要贡献，相关代码已开源。

Abstract: As intelligent robots become more integrated into human environments, there
is a growing need for intuitive and reliable Human-Robot Interaction (HRI)
interfaces that are adaptable and more natural to interact with. Traditional
robot control methods often require users to adapt to interfaces or memorize
predefined commands, limiting usability in dynamic, unstructured environments.
This paper presents a novel framework that bridges natural language
understanding and robotic execution by combining Large Language Models (LLMs)
with Behavior Trees. This integration enables robots to interpret natural
language instructions given by users and translate them into executable actions
by activating domain-specific plugins. The system supports scalable and modular
integration, with a primary focus on perception-based functionalities, such as
person tracking and hand gesture recognition. To evaluate the system, a series
of real-world experiments was conducted across diverse environments.
Experimental results demonstrate that the proposed approach is practical in
real-world scenarios, with an average cognition-to-execution accuracy of
approximately 94%, making a significant contribution to HRI systems and robots.
The complete source code of the framework is publicly available at
https://github.com/snt-arg/robot_suite.

</details>


### [179] [Immersive Teleoperation of Beyond-Human-Scale Robotic Manipulators: Challenges and Future Directions](https://arxiv.org/abs/2508.09700)
*Mahdi Hejrati,Jouni Mattila*

Main category: cs.RO

TL;DR: 该论文探讨了远程操作超人类尺寸机器人（BHSRMs）面临的控制、认知和界面层面的独特挑战，并提出针对这些挑战的新型沉浸式操作界面设计策略。


<details>
  <summary>Details</summary>
Motivation: 随着BHSRMs在建筑、采矿和灾害响应等工业领域的重要性增加，现有的人机协作和遥操作界面已无法满足其对安全性、可扩展性及有效性的需求，因此需要重新思考和优化沉浸式界面。

Method: 论文分析了BHSRMs遥操作中触觉和视觉反馈系统的设计权衡，进行了外骨骼控制与操纵杆控制实验比较，并关注于减少操作员的传感运动不匹配与提升操作代入感。

Result: 初步实验结果展示了不同控制方式（外骨骼与操纵杆）在传感运动一致性和代入感方面的差异，为后续优化提供了数据支撑。

Conclusion: 论文强调需针对大规模机器人遥操作开发新评估工具、扩展策略及以人为中心的安全模型，并为相关研究指出了未来重点方向。

Abstract: Teleoperation of beyond-human-scale robotic manipulators (BHSRMs) presents
unique challenges that differ fundamentally from conventional human-scale
systems. As these platforms gain relevance in industrial domains such as
construction, mining, and disaster response, immersive interfaces must be
rethought to support scalable, safe, and effective human-robot collaboration.
This paper investigates the control, cognitive, and interface-level challenges
of immersive teleoperation in BHSRMs, with a focus on ensuring operator safety,
minimizing sensorimotor mismatch, and enhancing the sense of embodiment. We
analyze design trade-offs in haptic and visual feedback systems, supported by
early experimental comparisons of exoskeleton- and joystick-based control
setups. Finally, we outline key research directions for developing new
evaluation tools, scaling strategies, and human-centered safety models tailored
to large-scale robotic telepresence.

</details>


### [180] [FLARE: Agile Flights for Quadrotor Cable-Suspended Payload System via Reinforcement Learning](https://arxiv.org/abs/2508.09797)
*Dongcheng Cao,Jin Zhou,Xian Wang,Shuo Li*

Main category: cs.RO

TL;DR: 本文提出FLARE强化学习框架，实现了四旋翼无人机带索吊挂系统的高效敏捷飞行，在仿真和实际中均取得优异表现，超过传统优化方法。


<details>
  <summary>Details</summary>
Motivation: 四旋翼带索吊挂系统由于欠驱动性、高度非线性和混合动力学特性，使敏捷飞行具有极大挑战，传统优化方法计算代价高且实时性差，难以应对绳索模式切换。

Method: 提出FLARE强化学习框架，通过高保真仿真直接学习敏捷导航策略，并在三种具挑战性场景下进行测试。对比基于优化的先进方法，展现出在门穿越操作中3倍的速度提升。

Result: 所学策略在仿真中表现优异，实现了从仿真到现实的零样本迁移，在真实飞行实验中表现出高度敏捷和安全，且可实时运行于机载计算机上。

Conclusion: 强化学习方法可显著提升四旋翼带索系统的敏捷飞行能力，优于传统方法，具备实际落地应用潜力。

Abstract: Agile flight for the quadrotor cable-suspended payload system is a formidable
challenge due to its underactuated, highly nonlinear, and hybrid dynamics.
Traditional optimization-based methods often struggle with high computational
costs and the complexities of cable mode transitions, limiting their real-time
applicability and maneuverability exploitation. In this letter, we present
FLARE, a reinforcement learning (RL) framework that directly learns agile
navigation policy from high-fidelity simulation. Our method is validated across
three designed challenging scenarios, notably outperforming a state-of-the-art
optimization-based approach by a 3x speedup during gate traversal maneuvers.
Furthermore, the learned policies achieve successful zero-shot sim-to-real
transfer, demonstrating remarkable agility and safety in real-world
experiments, running in real time on an onboard computer.

</details>


### [181] [Embodied Tactile Perception of Soft Objects Properties](https://arxiv.org/abs/2508.09836)
*Anirvan Dutta,Alexis WM Devillard,Zhihuan Zhang,Xiaoxiao Cheng,Etienne Burdet*

Main category: cs.RO

TL;DR: 本文通过可调机械顺应性和多模态感知的模块化电子皮肤，研究了机器人如何通过不同的交互策略和传感器融合来提升对物体的触觉感知能力。提出了无监督深度模型，能够将动作和触觉信号映射到结构化潜空间，深入理解感知机制，并发现多模态融合优于单一模态。


<details>
  <summary>Details</summary>
Motivation: 实现类人级别的机器人精细操作，需要理解机械顺应性、多模态感知与交互方式如何协同影响机器人的触觉感知。本研究希望系统分析感知本体和交互策略对物体感知的作用机制。

Method: 使用可模块化、具可调机械顺应性和多模态（法向、切向力及振动）传感能力的e-Skin，结合一组特性可控的软体物体，通过系统性操作不同的触觉探索原语（按压、旋转滑动等），并提出无监督、基于动作的深度时序状态空间模型latent filter, 用以挖掘交互动力学并将物理属性编码进潜空间。

Result: 实验发现多模态感知显著优于单模态感知，e-Skin的机械属性与环境的相互作用影响感知性能。研究证明潜在空间表征可以有效区分和解释不同感知机制、交互策略及本体属性对触觉感知的影响。

Conclusion: 多模态传感与机械顺应性的电子皮肤，通过结合动作策略和深度学习模型，能够显著提升机器人触觉感知的可解释性和泛化能力。未来机器感知研究需结合多模态、时序信息与本体-环境交互。

Abstract: To enable robots to develop human-like fine manipulation, it is essential to
understand how mechanical compliance, multi-modal sensing, and purposeful
interaction jointly shape tactile perception. In this study, we use a dedicated
modular e-Skin with tunable mechanical compliance and multi-modal sensing
(normal, shear forces and vibrations) to systematically investigate how sensing
embodiment and interaction strategies influence robotic perception of objects.
Leveraging a curated set of soft wave objects with controlled viscoelastic and
surface properties, we explore a rich set of palpation primitives-pressing,
precession, sliding that vary indentation depth, frequency, and directionality.
In addition, we propose the latent filter, an unsupervised, action-conditioned
deep state-space model of the sophisticated interaction dynamics and infer
causal mechanical properties into a structured latent space. This provides
generalizable and in-depth interpretable representation of how embodiment and
interaction determine and influence perception. Our investigation demonstrates
that multi-modal sensing outperforms uni-modal sensing. It highlights a nuanced
interaction between the environment and mechanical properties of e-Skin, which
should be examined alongside the interaction by incorporating temporal
dynamics.

</details>


### [182] [Whole-Body Bilateral Teleoperation with Multi-Stage Object Parameter Estimation for Wheeled Humanoid Locomanipulation](https://arxiv.org/abs/2508.09846)
*Donghoon Baek,Amartya Purushottam,Jason J. Choi,Joao Ramos*

Main category: cs.RO

TL;DR: 提出了一种面向物体的全身双向遥操作框架，用于带轮人形机器人，结合了多阶段物体惯性参数估计与遥操作控制，实现了对负载物体的高效估计和实时动态同步遥操作。


<details>
  <summary>Details</summary>
Motivation: 现有遥操作系统缺乏对负载物体惯性参数的实时感知与动态补偿，从而影响机器人在搬运等高动态任务中的操作稳定性与灵活性。解决如何准确、实时地估计物体参数，并将其集成到全身动态遥操作控制框架中，是提升机器人实际任务能力的关键。

Method: 提出了一个多阶段在线估计框架，利用视觉估计物体尺寸，借助大规模视觉-语言模型（VLM）获得惯性参数初值，并进行分层采样细化。该估计算法与模拟及硬件实时并行运行，保证系统参数动态更新。估算结果用于实时修改机器人的平衡控制点和力反馈控制。

Result: 实验在带机械手的自研轮式人形机器人平台上开展，系统实现了对自身质量约三分之一重量负载的实时操作任务，包括搬起、运输和释放，验证了估计方法的实时性与系统整体的动态操作能力。

Conclusion: 集成参数估计与全身动态遥操作，可以有效提升遥操作过程中的力反馈质量与对操作任务的适应性，提高了机器人搬运等任务的可控性和灵活性。

Abstract: This paper presents an object-aware whole-body bilateral teleoperation
framework for wheeled humanoid loco-manipulation. This framework combines
whole-body bilateral teleoperation with an online multi-stage object inertial
parameter estimation module, which is the core technical contribution of this
work. The multi-stage process sequentially integrates a vision-based object
size estimator, an initial parameter guess generated by a large vision-language
model (VLM), and a decoupled hierarchical sampling strategy. The visual size
estimate and VLM prior offer a strong initial guess of the object's inertial
parameters, significantly reducing the search space for sampling-based
refinement and improving the overall estimation speed. A hierarchical strategy
first estimates mass and center of mass, then infers inertia from object size
to ensure physically feasible parameters, while a decoupled multi-hypothesis
scheme enhances robustness to VLM prior errors. Our estimator operates in
parallel with high-fidelity simulation and hardware, enabling real-time online
updates. The estimated parameters are then used to update the wheeled
humanoid's equilibrium point, allowing the operator to focus more on locomotion
and manipulation. This integration improves the haptic force feedback for
dynamic synchronization, enabling more dynamic whole-body teleoperation. By
compensating for object dynamics using the estimated parameters, the framework
also improves manipulation tracking while preserving compliant behavior. We
validate the system on a customized wheeled humanoid with a robotic gripper and
human-machine interface, demonstrating real-time execution of lifting,
delivering, and releasing tasks with a payload weighing approximately one-third
of the robot's body weight.

</details>


### [183] [Toward Human-Robot Teaming: Learning Handover Behaviors from 3D Scenes](https://arxiv.org/abs/2508.09855)
*Yuekun Wu,Yik Lung Pang,Andrea Cavallaro,Changjae Oh*

Main category: cs.RO

TL;DR: 该论文提出了一种仅依赖RGB图像、无需真实机器人数据即可训练人机协作（HRT）策略的方法，有效提升了人-机器人交接任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现实中，基于真实环境图像进行机器人操作策略学习需要大量的机器人实验，成本高昂，而仅用仿真又存在视觉域差问题。本文旨在通过新方法解决这一挑战，提升机器人在人-机器人交接中的可靠性和安全性。

Method: 作者提出利用稀疏视角高斯斑点重建（Gaussian Splatting）技术还原人-机器交接场景，通过虚拟相机采集图像-动作对，并训练策略模型。由此无需进行真实机器人的数据采集和训练，而能得到鲁棒的操作策略。

Result: 实验证明，该方法在重建场景以及真实世界的人-机器人交接任务中均表现良好，能够有效提升机器人稳定抓取物体、避免碰撞的能力。

Conclusion: 本文方法为人-机器人交接任务提供了一种新颖有效的表达与训练方式，促进了更流畅和稳健的人-机器人协作。

Abstract: Human-robot teaming (HRT) systems often rely on large-scale datasets of human
and robot interactions, especially for close-proximity collaboration tasks such
as human-robot handovers. Learning robot manipulation policies from raw,
real-world image data requires a large number of robot-action trials in the
physical environment. Although simulation training offers a cost-effective
alternative, the visual domain gap between simulation and robot workspace
remains a major limitation. We introduce a method for training HRT policies,
focusing on human-to-robot handovers, solely from RGB images without the need
for real-robot training or real-robot data collection. The goal is to enable
the robot to reliably receive objects from a human with stable grasping while
avoiding collisions with the human hand. The proposed policy learner leverages
sparse-view Gaussian Splatting reconstruction of human-to-robot handover scenes
to generate robot demonstrations containing image-action pairs captured with a
camera mounted on the robot gripper. As a result, the simulated camera pose
changes in the reconstructed scene can be directly translated into gripper pose
changes. Experiments in both Gaussian Splatting reconstructed scene and
real-world human-to-robot handover experiments demonstrate that our method
serves as a new and effective representation for the human-to-robot handover
task, contributing to more seamless and robust HRT.

</details>


### [184] [A Shank Angle-Based Control System Enables Soft Exoskeleton to Assist Human Non-Steady Locomotion](https://arxiv.org/abs/2508.09876)
*Xiaowei Tan,Weizhong Jiang,Bi Zhang,Wanxin Chen,Yiwen Zhao,Ning Li,Lianqing Liu,Xingang Zhao*

Main category: cs.RO

TL;DR: 本论文提出了一种基于小腿角度的外骨骼控制系统，能够在多种非稳定步态和运动任务（如步行、跑步、上下楼梯）中，实现对人体步态的实时协调和动态辅助。通过三项实验验证控制系统的有效性、鲁棒性及其对人体生物力学与生理上的有益影响。


<details>
  <summary>Details</summary>
Motivation: 以往外骨骼大多聚焦于稳定步态（如持续步行）下的人体辅助，但在复杂、多变的非稳定步态情境下其效果和适应性尚不清楚。本研究动机是系统性解决外骨骼在非线性步相进展以及多样活动场景中的实时匹配和个体自适应难题。

Method: 采用以小腿角度为输入的实时双高斯模型生成辅助力矩，并通过IMU传感器在线更新模型参数以适应不同个体和步态变化。结合基于人-外骨骼运动学及刚度的前馈控制策略，以降低对历史步态数据的依赖。

Result: 三项实验结果表明：单独和联合控制方法均有效，系统对步态扰动及不同运动任务具有良好鲁棒性，且外骨骼辅助带来显著的人体生物力学和生理正向反应。

Conclusion: 基于小腿角度的外骨骼控制系统不仅能在非稳定、多样化步态下实现对用户的实时高适应性辅助，还能有效改善用户的生物力学和生理状态，在实际复杂场景中具有良好的应用前景。

Abstract: Exoskeletons have been shown to effectively assist humans during steady
locomotion. However, their effects on non-steady locomotion, characterized by
nonlinear phase progression within a gait cycle, remain insufficiently
explored, particularly across diverse activities. This work presents a shank
angle-based control system that enables the exoskeleton to maintain real-time
coordination with human gait, even under phase perturbations, while dynamically
shaping assistance profiles to match the biological ankle moment patterns
across walking, running, stair negotiation tasks. The control system consists
of an assistance profile online generation method and a model-based feedforward
control method. The assistance profile is formulated as a dual-Gaussian model
with the shank angle as the independent variable. Leveraging only IMU
measurements, the model parameters are updated online each stride to adapt to
inter- and intra-individual biomechanical variability. The profile tracking
control employs a human-exoskeleton kinematics and stiffness model as a
feedforward component, reducing reliance on historical control data due to the
lack of clear and consistent periodicity in non-steady locomotion. Three
experiments were conducted using a lightweight soft exoskeleton with multiple
subjects. The results validated the effectiveness of each individual method,
demonstrated the robustness of the control system against gait perturbations
across various activities, and revealed positive biomechanical and
physiological responses of human users to the exoskeleton's mechanical
assistance.

</details>


### [185] [PPL: Point Cloud Supervised Proprioceptive Locomotion Reinforcement Learning for Legged Robots in Crawl Spaces](https://arxiv.org/abs/2508.09950)
*Bida Ma,Nuo Xu,Chenkun Qi,Xin Liu,Yule Mo,Jinkai Wang,Chunpeng Lu*

Main category: cs.RO

TL;DR: 本论文提出了一种新的基于点云监督的本体感知强化学习方法，用于提升足式机器人在狭小空间（如爬行空间）中的运动能力，并通过实验验证了其优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 传统的外部感知方法在爬行空间中受到噪声和低能见度影响，表现不佳；纯本体感知方法又难以处理空间结构。亟需一种既能充分利用本体感知数据、又能适应复杂空间结构的机器人运动学习方法。

Method: 设计了状态估计网络，利用历史本体感知数据预测机器人周围地面、空间结构及碰撞状态。采用极坐标点云处理法高效提取监督用地形与空间特征，并设计多元奖励函数引导机器人在碰撞后有效穿越狭小空间。

Result: 实验表明，与现有方法相比，本方法能在爬行空间中实现更为灵活、敏捷的运动表现。

Conclusion: 本研究提升了足式机器人在空间受限环境下无需额外外部传感器的运动能力，对机器人在复杂实际场景中的应用有重要推动作用。

Abstract: The legged locomotion in spatially constrained structures (called crawl
spaces) is challenging. In crawl spaces, current exteroceptive locomotion
learning methods are limited by large noises and errors of the sensors in
possible low visibility conditions, and current proprioceptive locomotion
learning methods are difficult in traversing crawl spaces because only ground
features are inferred. In this study, a point cloud supervised proprioceptive
locomotion reinforcement learning method for legged robots in crawl spaces is
proposed. A state estimation network is designed to estimate the robot's
surrounding ground and spatial features as well as the robot's collision states
using historical proprioceptive sensor data. The point cloud is represented in
polar coordinate frame and a point cloud processing method is proposed to
efficiently extract the ground and spatial features that are used to supervise
the state estimation network learning. Comprehensive reward functions that
guide the robot to traverse through crawl spaces after collisions are designed.
Experiments demonstrate that, compared to existing methods, our method exhibits
more agile locomotion in crawl spaces. This study enhances the ability of
legged robots to traverse spatially constrained environments without requiring
exteroceptive sensors.

</details>


### [186] [GBC: Generalized Behavior-Cloning Framework for Whole-Body Humanoid Imitation](https://arxiv.org/abs/2508.09960)
*Yifei Yao,Chengyuan Luo,Jiaheng Du,Wentao He,Jun-Guo Lu*

Main category: cs.RO

TL;DR: 本文提出了一种通用行为克隆（GBC）框架，以解决类人机器人控制中数据处理与学习算法难以通用的难题。该框架建立了从人体动作到机器人行为的完整路径，显著提升了不同形态机器人间的通用性。


<details>
  <summary>Details</summary>
Motivation: 目前类人机器人通常需要针对不同形态设计专属的数据处理和学习算法，导致开发过程碎片化、效率低，难以推广与通用。本研究旨在提供一套统一高效的解决方案，实现从人类动作采集数据到任意类人机器人的通用行为学习。

Method: 该框架主要包括三项创新：1）利用可微分逆运动学（IK）网络的自适应数据流水线，实现任意人体动作数据向任意类人机器人的自动重定向；2）提出了结合DAgger-MMPPO算法与MMTransformer结构的新型模仿学习策略，实现稳定高保真的动作模仿；3）基于Isaac Lab开发了高效开源平台，用户可通过简单配置脚本部署完整流程。

Result: 通过在多种不同形态的类人机器人上训练控制策略，结果表明GBC框架不仅在训练、迁移新动作等方面表现优异，同时验证了其强大的通用性和鲁棒性。

Conclusion: 该工作首次建立了从人体动作到类人控制器的真正实用、统一的通用工作路径，有望推动类人机器人领域实现更广泛、便捷的应用。

Abstract: The creation of human-like humanoid robots is hindered by a fundamental
fragmentation: data processing and learning algorithms are rarely universal
across different robot morphologies. This paper introduces the Generalized
Behavior Cloning (GBC) framework, a comprehensive and unified solution designed
to solve this end-to-end challenge. GBC establishes a complete pathway from
human motion to robot action through three synergistic innovations. First, an
adaptive data pipeline leverages a differentiable IK network to automatically
retarget any human MoCap data to any humanoid. Building on this foundation, our
novel DAgger-MMPPO algorithm with its MMTransformer architecture learns robust,
high-fidelity imitation policies. To complete the ecosystem, the entire
framework is delivered as an efficient, open-source platform based on Isaac
Lab, empowering the community to deploy the full workflow via simple
configuration scripts. We validate the power and generality of GBC by training
policies on multiple heterogeneous humanoids, demonstrating excellent
performance and transfer to novel motions. This work establishes the first
practical and unified pathway for creating truly generalized humanoid
controllers.

</details>


### [187] [Vision-driven River Following of UAV via Safe Reinforcement Learning using Semantic Dynamics Model](https://arxiv.org/abs/2508.09971)
*Zihan Wang,Nina Mahmoudian*

Main category: cs.RO

TL;DR: 本论文针对无人机在无GPS区域进行自主河流跟随的问题，提出了基于视觉的高效、安全的强化学习方法，并通过多种创新技术提升了任务表现和安全性。


<details>
  <summary>Details</summary>
Motivation: 在密集河谷等GPS信号不佳的环境中，无人机需要可靠地自主跟随河流用于救援、监控及环境保护任务。传统方法受限于信号和环境复杂性，亟需新型视觉驱动、具备安全性和高效性的控制方法。

Method: 主要方法创新有三：1）提出边际收益优势估计方法（MGAE）优化奖励函数，提升非Markov情景下优势估计的精度；2）开发基于分块语义水体掩膜的语义动力学模型（SDM），提升状态预测的可解释性和效率；3）设计受约束的行动者动力学估计算法（CADE），结合模型预测、安全估计和动作选择，从而实现安全的模型化强化学习（SafeRL）。

Result: 仿真实验显示，MGAE方法在收敛速度和表现上优于传统优势估计方法如GAE。SDM模型提升了短期状态预测的准确度，进而增强了安全判违规预测。CADE框架有效融合了安全约束和强化学习，提高了整体任务表现。

Conclusion: 所提方法可显著提升无人机视觉自主河流跟随任务的安全性和效率，有望推动相关实际应用发展。融合软硬安全机制的模型在仿真中表现优异，为后续实际部署和方法优化提供了技术基础。

Abstract: Vision-driven autonomous river following by Unmanned Aerial Vehicles is
critical for applications such as rescue, surveillance, and environmental
monitoring, particularly in dense riverine environments where GPS signals are
unreliable. We formalize river following as a coverage control problem in which
the reward function is submodular, yielding diminishing returns as more unique
river segments are visited, thereby framing the task as a Submodular Markov
Decision Process. First, we introduce Marginal Gain Advantage Estimation, which
refines the reward advantage function by using a sliding window baseline
computed from historical episodic returns, thus aligning the advantage
estimation with the agent's evolving recognition of action value in
non-Markovian settings. Second, we develop a Semantic Dynamics Model based on
patchified water semantic masks that provides more interpretable and
data-efficient short-term prediction of future observations compared to latent
vision dynamics models. Third, we present the Constrained Actor Dynamics
Estimator architecture, which integrates the actor, the cost estimator, and SDM
for cost advantage estimation to form a model-based SafeRL framework capable of
solving partially observable Constrained Submodular Markov Decision Processes.
Simulation results demonstrate that MGAE achieves faster convergence and
superior performance over traditional critic-based methods like Generalized
Advantage Estimation. SDM provides more accurate short-term state predictions
that enable the cost estimator to better predict potential violations. Overall,
CADE effectively integrates safety regulation into model-based RL, with the
Lagrangian approach achieving the soft balance of reward and safety during
training, while the safety layer enhances performance during inference by hard
action overlay.

</details>


### [188] [Masquerade: Learning from In-the-wild Human Videos using Data-Editing](https://arxiv.org/abs/2508.09976)
*Marion Lepert,Jiaying Fang,Jeannette Bohg*

Main category: cs.RO

TL;DR: 论文提出Masquerade方法，通过编辑真人视角的视频，让机器人学习人类操作，将人类视频转化为机器人的训练数据，极大增加了数据量，并提升了机器人泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前机器人操作数据稀缺，限制了其智能发展，而人类的视频数据却丰富但直接不可用于机器人学习，如何充分利用这些视频资源成为痛点。

Method: Masquerade方法将真人第一视角视频通过手部3D姿态估计、手臂修复与机器人虚拟叠加，转化为机器人的模仿数据。在675K帧编辑视频上预训练视觉编码器，并在极少的机器示范下微调政策网络，实现优秀泛化能力。

Result: 在三项复杂厨房任务、三种新场景上，Masquerade方法比基线高5-6倍，消融实验表明机器人叠加及协同训练均不可或缺，且性能随编辑视频规模呈对数提升。

Conclusion: 显式缩小游戏视觉体现差异，能利用大量真人视频提升机器人学习，数据可获性与有效性明显增强。

Abstract: Robot manipulation research still suffers from significant data scarcity:
even the largest robot datasets are orders of magnitude smaller and less
diverse than those that fueled recent breakthroughs in language and vision. We
introduce Masquerade, a method that edits in-the-wild egocentric human videos
to bridge the visual embodiment gap between humans and robots and then learns a
robot policy with these edited videos. Our pipeline turns each human video into
robotized demonstrations by (i) estimating 3-D hand poses, (ii) inpainting the
human arms, and (iii) overlaying a rendered bimanual robot that tracks the
recovered end-effector trajectories. Pre-training a visual encoder to predict
future 2-D robot keypoints on 675K frames of these edited clips, and continuing
that auxiliary loss while fine-tuning a diffusion policy head on only 50 robot
demonstrations per task, yields policies that generalize significantly better
than prior work. On three long-horizon, bimanual kitchen tasks evaluated in
three unseen scenes each, Masquerade outperforms baselines by 5-6x. Ablations
show that both the robot overlay and co-training are indispensable, and
performance scales logarithmically with the amount of edited human video. These
results demonstrate that explicitly closing the visual embodiment gap unlocks a
vast, readily available source of data from human videos that can be used to
improve robot policies.

</details>
