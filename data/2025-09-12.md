<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 78]
- [cs.CL](#cs.CL) [Total: 37]
- [cs.RO](#cs.RO) [Total: 19]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Recurrence Meets Transformers for Universal Multimodal Retrieval](https://arxiv.org/abs/2509.08897)
*Davide Caffagni,Sara Sarto,Marcella Cornia,Lorenzo Baraldi,Rita Cucchiara*

Main category: cs.CV

TL;DR: 本文提出ReT-2，一种统一的多模态检索模型，能支持图像和文本混合查询，并在多模态文档库中高效检索，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态检索方法多依赖于针对特定任务的视觉-语言模型微调，且通常仅限于单一模态的查询或文档，难以满足实际日益复杂的多模态检索需求。

Method: ReT-2利用多层表征和带有LSTM启发门控机制的递归Transformer架构，实现不同层和模态间的信息动态融合，能够细致捕捉图像与文本细节。模型支持组合（图像+文本）查询，并在包含文本与图片的多模态文档库中检索。

Result: 在M2KR和M-BEIR等多模态检索基准上，ReT-2在不同配置下均取得了SOTA（最新最优）表现；推理速度更快，内存消耗更低。集成进检索增强生成（RAG）后，在Encyclopedic-VQA和InfoSeek等数据集的下游任务表现也有提升。

Conclusion: ReT-2为复杂多模态检索任务提供了高效统一的解决方案，兼具优异性能和资源占用优势，为相关应用奠定了坚实基础。代码与模型已开源。

Abstract: With the rapid advancement of multimodal retrieval and its application in
LLMs and multimodal LLMs, increasingly complex retrieval tasks have emerged.
Existing methods predominantly rely on task-specific fine-tuning of
vision-language models and are limited to single-modality queries or documents.
In this paper, we propose ReT-2, a unified retrieval model that supports
multimodal queries, composed of both images and text, and searches across
multimodal document collections where text and images coexist. ReT-2 leverages
multi-layer representations and a recurrent Transformer architecture with
LSTM-inspired gating mechanisms to dynamically integrate information across
layers and modalities, capturing fine-grained visual and textual details. We
evaluate ReT-2 on the challenging M2KR and M-BEIR benchmarks across different
retrieval configurations. Results demonstrate that ReT-2 consistently achieves
state-of-the-art performance across diverse settings, while offering faster
inference and reduced memory usage compared to prior approaches. When
integrated into retrieval-augmented generation pipelines, ReT-2 also improves
downstream performance on Encyclopedic-VQA and InfoSeek datasets. Our source
code and trained models are publicly available at:
https://github.com/aimagelab/ReT-2

</details>


### [2] [Diffusion-Based Action Recognition Generalizes to Untrained Domains](https://arxiv.org/abs/2509.08908)
*Rogerio Guimaraes,Frank Xiao,Pietro Perona,Markus Marks*

Main category: cs.CV

TL;DR: 本文提出了一种新的人类动作识别方法，采用视觉扩散模型(VDM)生成特征，通过transformer聚合，用于提升模型在不同物种、视角和上下文下的泛化能力，并在多个基准测试中刷新了性能记录。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在人类动作识别任务中，面对物种差异、视角变化以及场景多样性时，泛化性能不足，远不及人类视觉系统。该研究旨在解决模型泛化能力弱的问题，使动作识别系统更加接近人类的鲁棒性。

Method: 作者利用视觉扩散模型(VDM)生成特征，并通过transformer进行信息聚合。关键在于采用扩散过程早期时刻的信息，突出语义而弱化像素级细节，从而提升特征的可泛化性。通过设计多种实验，验证了模型在跨物种、跨视角、跨场景上的动作识别能力。

Result: 实验表明，该方法在跨物种、跨视角、跨场景的动作识别泛化基准上均取得了新SOTA（最优性能），显著优于已有的深度学习模型。

Conclusion: 基于VDM和transformer的信息提取与聚合方法，有效提升了动作识别模型的泛化能力，将机器动作识别的鲁棒性推进到更接近人类的水平，可为未来相关研究提供重要借鉴。

Abstract: Humans can recognize the same actions despite large context and viewpoint
variations, such as differences between species (walking in spiders vs.
horses), viewpoints (egocentric vs. third-person), and contexts (real life vs
movies). Current deep learning models struggle with such generalization. We
propose using features generated by a Vision Diffusion Model (VDM), aggregated
via a transformer, to achieve human-like action recognition across these
challenging conditions. We find that generalization is enhanced by the use of a
model conditioned on earlier timesteps of the diffusion process to highlight
semantic information over pixel level details in the extracted features. We
experimentally explore the generalization properties of our approach in
classifying actions across animal species, across different viewing angles, and
different recording contexts. Our model sets a new state-of-the-art across all
three generalization benchmarks, bringing machine action recognition closer to
human-like robustness. Project page:
$\href{https://www.vision.caltech.edu/actiondiff/}{\texttt{vision.caltech.edu/actiondiff}}$
Code:
$\href{https://github.com/frankyaoxiao/ActionDiff}{\texttt{github.com/frankyaoxiao/ActionDiff}}$

</details>


### [3] [PromptGuard: An Orchestrated Prompting Framework for Principled Synthetic Text Generation for Vulnerable Populations using LLMs with Enhanced Safety, Fairness, and Controllability](https://arxiv.org/abs/2509.08910)
*Tung Vu,Lam Nguyen,Quynh Dao*

Main category: cs.CV

TL;DR: 本文提出PromptGuard，这是一个创新的模块化提示框架，核心包括VulnGuard Prompt技术，用于在大语言模型生成内容源头主动防止对弱势群体（如LGBTQ+、单亲家庭等）的有害输出。通过多模块协作和对比学习等方法，实现了理论上的显著风险降低。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型普遍存在生成有害、偏见或误导性信息的风险，且影响易致弱势群体受到伤害。现有安全机制多为事后过滤或通用对齐措施，无法从根本上预防有害输出。

Method: 作者提出PromptGuard框架，包括六个模块（输入分类、VulnGuard提示、伦理原则集成、外部工具交互、输出验证、用户-系统交互），核心技术为VulnGuard Prompt，融合了少样本实例、伦理推理、适应性角色提示，并利用多目标优化和信息论理论作危害分析。实证数据来源于GitHub等开放数据集。

Result: 在理论分析下，PromptGuard可实现25-30%的有害输出风险降低（以熵界和帕累托最优性为证明），并给出收敛性、脆弱性分析等严格数学论证。

Conclusion: PromptGuard为大语言模型安全防护提供了具理论支撑、能主动防害的系统框架，为今后系统性实证研究奠定了基础。

Abstract: The proliferation of Large Language Models (LLMs) in real-world applications
poses unprecedented risks of generating harmful, biased, or misleading
information to vulnerable populations including LGBTQ+ individuals, single
parents, and marginalized communities. While existing safety approaches rely on
post-hoc filtering or generic alignment techniques, they fail to proactively
prevent harmful outputs at the generation source. This paper introduces
PromptGuard, a novel modular prompting framework with our breakthrough
contribution: VulnGuard Prompt, a hybrid technique that prevents harmful
information generation using real-world data-driven contrastive learning.
VulnGuard integrates few-shot examples from curated GitHub repositories,
ethical chain-of-thought reasoning, and adaptive role-prompting to create
population-specific protective barriers. Our framework employs theoretical
multi-objective optimization with formal proofs demonstrating 25-30% analytical
harm reduction through entropy bounds and Pareto optimality. PromptGuard
orchestrates six core modules: Input Classification, VulnGuard Prompting,
Ethical Principles Integration, External Tool Interaction, Output Validation,
and User-System Interaction, creating an intelligent expert system for
real-time harm prevention. We provide comprehensive mathematical formalization
including convergence proofs, vulnerability analysis using information theory,
and theoretical validation framework using GitHub-sourced datasets,
establishing mathematical foundations for systematic empirical research.

</details>


### [4] [Similarity-based Outlier Detection for Noisy Object Re-Identification Using Beta Mixtures](https://arxiv.org/abs/2509.08926)
*Waqar Ahmad,Evan Murphy,Vladimir A. Krylov*

Main category: cs.CV

TL;DR: 该论文通过引入Beta-SOD统计离群检测方法，提高了目标重识别任务在标签噪声环境下的鲁棒性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目标重识别（Re-ID）技术对标签噪声极为敏感，噪声标签会导致识别性能大幅下降，因此急需提升Re-ID对噪声的鲁棒性。

Method: 将Re-ID任务转化为有监督的图像相似性学习，采用Siamese网络。核心方法为Beta-SOD（Beta混合分布的相似性离群检测），通过二元Beta分布混合模型拟合嵌入对的余弦相似度分布，并证明了两Beta分布混合模型的可辨识性。同时结合二元交叉熵、对比损失及余弦嵌入损失联合优化特征相似性。

Result: 在行人Re-ID（CUHK03、Market-1501）和车辆Re-ID（VeRi-776）等主流数据集上，Beta-SOD能在10-30%标签噪声条件下显著超越现有方法，效果更稳健。

Conclusion: Beta-SOD为Re-ID领域提供了一种高效的标签去噪和鲁棒训练框架，在高噪声环境下依旧能保持领先性能，具有广泛适用性。

Abstract: Object re-identification (Re-ID) methods are highly sensitive to label noise,
which typically leads to significant performance degradation. We address this
challenge by reframing Re-ID as a supervised image similarity task and adopting
a Siamese network architecture trained to capture discriminative pairwise
relationships. Central to our approach is a novel statistical outlier detection
(OD) framework, termed Beta-SOD (Beta mixture Similarity-based Outlier
Detection), which models the distribution of cosine similarities between
embedding pairs using a two-component Beta distribution mixture model. We
establish a novel identifiability result for mixtures of two Beta
distributions, ensuring that our learning task is well-posed.The proposed OD
step complements the Re-ID architecture combining binary cross-entropy,
contrastive, and cosine embedding losses that jointly optimize feature-level
similarity learning.We demonstrate the effectiveness of Beta-SOD in de-noising
and Re-ID tasks for person Re-ID, on CUHK03 and Market-1501 datasets, and
vehicle Re-ID, on VeRi-776 dataset. Our method shows superior performance
compared to the state-of-the-art methods across various noise levels (10-30\%),
demonstrating both robustness and broad applicability in noisy Re-ID scenarios.
The implementation of Beta-SOD is available at:
https://github.com/waqar3411/Beta-SOD

</details>


### [5] [SFD-Mamba2Net: Strcture-Guided Frequency-Enhanced Dual-Stream Mamba2 Network for Coronary Artery Segmentation](https://arxiv.org/abs/2509.08934)
*Nan Mu,Ruiqi Song,Zhihui Xu,Jingfeng Jiang,Chen Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种新的端到端神经网络SFD-Mamba2Net，有效提升了冠状动脉造影图像中的血管分割和狭窄检测精度。


<details>
  <summary>Details</summary>
Motivation: 侵入式冠状动脉造影因图像对比度低、噪声高、血管结构复杂，现有算法在分割和检测上表现有限，亟需提升自动化和准确性。

Method: 提出SFD-Mamba2Net框架：编码器引入结构增强模块（CASE），通过多尺度特征强化血管形态、抑制背景；解码器设计高频细节感知模块（PHFP），采用多层小波分解逐步优化高频特征，并融合全局低频信息。实现血管和狭窄的精细分割与检测。

Result: 本方法在八项主流分割指标、狭窄检测的真实阳性率和阳性预测值均超越现有先进方法，表现卓越。

Conclusion: SFD-Mamba2Net显著提升了ICA图像下的冠脉分割与狭窄检测精度，有助于提高临床CAD辅助诊断的效率和可靠性。

Abstract: Background: Coronary Artery Disease (CAD) is one of the leading causes of
death worldwide. Invasive Coronary Angiography (ICA), regarded as the gold
standard for CAD diagnosis, necessitates precise vessel segmentation and
stenosis detection. However, ICA images are typically characterized by low
contrast, high noise levels, and complex, fine-grained vascular structures,
which pose significant challenges to the clinical adoption of existing
segmentation and detection methods. Objective: This study aims to improve the
accuracy of coronary artery segmentation and stenosis detection in ICA images
by integrating multi-scale structural priors, state-space-based long-range
dependency modeling, and frequency-domain detail enhancement strategies.
Methods: We propose SFD-Mamba2Net, an end-to-end framework tailored for
ICA-based vascular segmentation and stenosis detection. In the encoder, a
Curvature-Aware Structural Enhancement (CASE) module is embedded to leverage
multi-scale responses for highlighting slender tubular vascular structures,
suppressing background interference, and directing attention toward vascular
regions. In the decoder, we introduce a Progressive High-Frequency Perception
(PHFP) module that employs multi-level wavelet decomposition to progressively
refine high-frequency details while integrating low-frequency global
structures. Results and Conclusions: SFD-Mamba2Net consistently outperformed
state-of-the-art methods across eight segmentation metrics, and achieved the
highest true positive rate and positive predictive value in stenosis detection.

</details>


### [6] [Live(r) Die: Predicting Survival in Colorectal Liver Metastasis](https://arxiv.org/abs/2509.08935)
*Muhammad Alberb,Helen Cheung,Anne Martel*

Main category: cs.CV

TL;DR: 本论文提出了一个全自动框架，通过术前MRI图像进行结直肠癌肝转移患者的手术预后预测。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌肝转移（CRLM）患者预后差异大，现有模型预测能力有限，尤其是多灶性病例。需要结合多模态影像与自动化分析提升预测准确性和效率。

Method: 作者开发了包括分割与影像组学两部分的自动化预测框架。分割流程利用基础模型SAM和新提出的SAMONAI算法，对MRI中的肝脏、肿瘤、脾脏进行高效分割。之后，影像组学模块从分割结果提取特征，采用SurvAMINN（基于自编码器的神经网络）进行生存分析预测。

Result: 在227位患者数据上的评测显示，该框架显著优于现有临床和基因组指标，C-index提升超过10%。

Conclusion: 将自动分割和影像组学相结合，有潜力为CRLM患者提供更准确、效率更高且易解释的预后预测方法，有助于辅助临床决策。

Abstract: Colorectal cancer frequently metastasizes to the liver, significantly
reducing long-term survival. While surgical resection is the only potentially
curative treatment for colorectal liver metastasis (CRLM), patient outcomes
vary widely depending on tumor characteristics along with clinical and genomic
factors. Current prognostic models, often based on limited clinical or
molecular features, lack sufficient predictive power, especially in multifocal
CRLM cases. We present a fully automated framework for surgical outcome
prediction from pre- and post-contrast MRI acquired before surgery. Our
framework consists of a segmentation pipeline and a radiomics pipeline. The
segmentation pipeline learns to segment the liver, tumors, and spleen from
partially annotated data by leveraging promptable foundation models to complete
missing labels. Also, we propose SAMONAI, a novel zero-shot 3D prompt
propagation algorithm that leverages the Segment Anything Model to segment 3D
regions of interest from a single point prompt, significantly improving our
segmentation pipeline's accuracy and efficiency. The predicted pre- and
post-contrast segmentations are then fed into our radiomics pipeline, which
extracts features from each tumor and predicts survival using SurvAMINN, a
novel autoencoder-based multiple instance neural network for survival analysis.
SurvAMINN jointly learns dimensionality reduction and hazard prediction from
right-censored survival data, focusing on the most aggressive tumors. Extensive
evaluation on an institutional dataset comprising 227 patients demonstrates
that our framework surpasses existing clinical and genomic biomarkers,
delivering a C-index improvement exceeding 10%. Our results demonstrate the
potential of integrating automated segmentation algorithms and radiomics-based
survival analysis to deliver accurate, annotation-efficient, and interpretable
outcome prediction in CRLM.

</details>


### [7] [Discovering Divergent Representations between Text-to-Image Models](https://arxiv.org/abs/2509.08940)
*Lisa Dunlap,Joseph E. Gonzalez,Trevor Darrell,Fabian Caba Heilbron,Josef Sivic,Bryan Russell*

Main category: cs.CV

TL;DR: 本文提出了CompCon算法，用于比较两个文本生成图像模型在视觉表现上的差异，能够自动发现特定属性差异及其触发的提示词，并用以分析主流模型间的表征分歧。


<details>
  <summary>Details</summary>
Motivation: 随文本生成图像技术进步，不同模型在同一提示下生成的图片往往有着微妙但有意义的表现差异，理解这些差异有助于解释模型偏好与局限，提升智能模型的可解释性和公平性。

Method: 作者设计了CompCon，一种基于进化搜索的算法，自动发现并对比两个文本生成图像模型输出中的视觉属性差异，以及触发这些差异的提示。为了评估算法性能，作者自建了一个带有60种输入相关差异的数据集ID2，并同多种基于LLM和VLM的基线方法进行对比。

Result: 实验表明，CompCon有效地揭示了主流文本-图像生成模型间的图像特征分歧。例如，PixArt在描述孤独主题提示时常生成湿漉漉的街道，而Stable Diffusion 3.5在媒体职业相关提示下更倾向于以非裔美国人为形象。

Conclusion: CompCon算法可自动识别并解释文本到图像生成模型间的表征差异，为理解生成式模型的表现机制及其在不同场景下的偏差与公平性研究提供了新工具。

Abstract: In this paper, we investigate when and how visual representations learned by
two different generative models diverge. Given two text-to-image models, our
goal is to discover visual attributes that appear in images generated by one
model but not the other, along with the types of prompts that trigger these
attribute differences. For example, "flames" might appear in one model's
outputs when given prompts expressing strong emotions, while the other model
does not produce this attribute given the same prompts. We introduce CompCon
(Comparing Concepts), an evolutionary search algorithm that discovers visual
attributes more prevalent in one model's output than the other, and uncovers
the prompt concepts linked to these visual differences. To evaluate CompCon's
ability to find diverging representations, we create an automated data
generation pipeline to produce ID2, a dataset of 60 input-dependent
differences, and compare our approach to several LLM- and VLM-powered
baselines. Finally, we use CompCon to compare popular text-to-image models,
finding divergent representations such as how PixArt depicts prompts mentioning
loneliness with wet streets and Stable Diffusion 3.5 depicts African American
people in media professions. Code at: https://github.com/adobe-research/CompCon

</details>


### [8] [An U-Net-Based Deep Neural Network for Cloud Shadow and Sun-Glint Correction of Unmanned Aerial System (UAS) Imagery](https://arxiv.org/abs/2509.08949)
*Yibin Wang,Wondimagegn Beshah,Padmanava Dash,Haifeng Wang*

Main category: cs.CV

TL;DR: 本文提出一种基于U-Net深度学习模型的方法，用于检测并校正UAS遥感图像中的云影和太阳耀斑，从而提升水质参数评估的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着无人机遥感应用的增长，其获取图像的灵活性非常高，但云影和太阳耀斑会严重影响水体的遥感解析质量，亟需有效识别并去除这些干扰区域。

Method: 使用U-Net神经网络对像素级数据进行训练，利用机器学习方法自动识别、分割云影和太阳耀斑区域，并与未受到干扰的区域区分。通过多种评估指标确定模型参数，并选出最佳模型进行图像校正。

Result: 实验结果显示，提出的模型能够有效识别和修正云影与太阳耀斑区域，显著提升了图像的质量及后续水体参数的遥感精度。

Conclusion: 基于深度学习的图像校正方法为无人机遥感图像带来了更高的可用性，尤其是在评估受自然干扰影响严重时，为水体质量监测等应用提供了更可靠的技术手段。

Abstract: The use of unmanned aerial systems (UASs) has increased tremendously in the
current decade. They have significantly advanced remote sensing with the
capability to deploy and image the terrain as per required spatial, spectral,
temporal, and radiometric resolutions for various remote sensing applications.
One of the major advantages of UAS imagery is that images can be acquired in
cloudy conditions by flying the UAS under the clouds. The limitation to the
technology is that the imagery is often sullied by cloud shadows. Images taken
over water are additionally affected by sun glint. These are two pose serious
issues for estimating water quality parameters from the UAS images. This study
proposes a novel machine learning approach first to identify and extract
regions with cloud shadows and sun glint and separate such regions from
non-obstructed clear sky regions and sun-glint unaffected regions. The data was
extracted from the images at pixel level to train an U-Net based deep learning
model and best settings for model training was identified based on the various
evaluation metrics from test cases. Using this evaluation, a high-quality image
correction model was determined, which was used to recover the cloud shadow and
sun glint areas in the images.

</details>


### [9] [CoSwin: Convolution Enhanced Hierarchical Shifted Window Attention For Small-Scale Vision](https://arxiv.org/abs/2509.08959)
*Puskal Khadka,Rodrigue Rizk,Longwei Wang,KC Santosh*

Main category: cs.CV

TL;DR: 本文提出了一种名为CoSwin的新型特征融合结构，通过结合分层移窗注意力和局部卷积特征学习，提升ViT模型在小型数据集上的表现。实验证明CoSwin在多个主流图像分类数据集上优于现有卷积与Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 虽然Vision Transformer（ViT）模型在计算机视觉领域表现优异，但其对全局特征的关注导致其在小数据集、尤其需要局部特征提取任务上的表现受限，因为缺乏类似卷积网络的归纳偏置（如局部性与平移等变性）。因此，亟需一种方法在ViT中增强局部特征捕获能力，提升其泛化与鲁棒性。

Method: 作者提出CoSwin，创新性地将可学习的局部特征增强模块嵌入每个Transformer注意力模块中，即在分层移窗自注意力结构基础上加入卷积特征学习模块，实现局部空间细节与全局语义特征的联合建模。

Result: 实验证明CoSwin在CIFAR-10（+2.17%）、CIFAR-100（+4.92%）、MNIST（+0.1%）、SVHN（+0.26%）和Tiny ImageNet（+4.47%）等主流分类数据集上都优于Swin Transformer等基线模型，并在多项指标上超越现有方法。

Conclusion: 融合局部卷积特征与全球Transformer自注意力能够有效提升ViT模型在小规模视觉任务中的泛化和鲁棒性。CoSwin为Transformer类视觉模型带来了显著性能提升。

Abstract: Vision Transformers (ViTs) have achieved impressive results in computer
vision by leveraging self-attention to model long-range dependencies. However,
their emphasis on global context often comes at the expense of local feature
extraction in small datasets, particularly due to the lack of key inductive
biases such as locality and translation equivariance. To mitigate this, we
propose CoSwin, a novel feature-fusion architecture that augments the
hierarchical shifted window attention with localized convolutional feature
learning. Specifically, CoSwin integrates a learnable local feature enhancement
module into each attention block, enabling the model to simultaneously capture
fine-grained spatial details and global semantic structure. We evaluate CoSwin
on multiple image classification benchmarks including CIFAR-10, CIFAR-100,
MNIST, SVHN, and Tiny ImageNet. Our experimental results show consistent
performance gains over state-of-the-art convolutional and transformer-based
models. Notably, CoSwin achieves improvements of 2.17% on CIFAR-10, 4.92% on
CIFAR-100, 0.10% on MNIST, 0.26% on SVHN, and 4.47% on Tiny ImageNet over the
baseline Swin Transformer. These improvements underscore the effectiveness of
local-global feature fusion in enhancing the generalization and robustness of
transformers for small-scale vision. Code and pretrained weights available at
https://github.com/puskal-khadka/coswin

</details>


### [10] [iMatcher: Improve matching in point cloud registration via local-to-global geometric consistency learning](https://arxiv.org/abs/2509.08982)
*Karim Slimani,Catherine Achard,Brahim Tamadazte*

Main category: cs.CV

TL;DR: 本文提出了一种新型的点云配准特征匹配方法iMatcher，通过全可微结构结合局部和全局特征一致性，极大提升了点云刚性配准的性能，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有点云特征匹配方法大多仅关注局部信息，难以同时兼顾局部和全局几何一致性，导致配准效果有限。因此，亟需一种能在端到端可微框架下统筹局部与全局一致性的匹配方法。

Method: iMatcher框架包括：1）局部图嵌入模块初始化匹配得分矩阵；2）通过3D空间的最近邻搜索实现源点与目标点的双向重定向以细化得分；3）将匹配后的特征堆叠，通过全局几何一致性学习，提高点对匹配概率预测的精度。算法端到端可微。

Result: 在KITTI、KITTI-360、3DMatch等公开数据集以及TUD-L、MVP-RG等任务上，iMatcher在刚性配准表现上大幅提升，inlier指标达到95%-97%，并在多个场景下均显著优于以往方法。

Conclusion: iMatcher结合局部和全局一致性，有效提升了点云特征匹配和配准准确率，展示了良好的泛化能力和鲁棒性，为点云刚性配准任务提供了新方法。

Abstract: This paper presents iMatcher, a fully differentiable framework for feature
matching in point cloud registration. The proposed method leverages learned
features to predict a geometrically consistent confidence matrix, incorporating
both local and global consistency. First, a local graph embedding module leads
to an initialization of the score matrix. A subsequent repositioning step
refines this matrix by considering bilateral source-to-target and
target-to-source matching via nearest neighbor search in 3D space. The paired
point features are then stacked together to be refined through global geometric
consistency learning to predict a point-wise matching probability. Extensive
experiments on real-world outdoor (KITTI, KITTI-360) and indoor (3DMatch)
datasets, as well as on 6-DoF pose estimation (TUD-L) and partial-to-partial
matching (MVP-RG), demonstrate that iMatcher significantly improves rigid
registration performance. The method achieves state-of-the-art inlier ratios,
scoring 95% - 97% on KITTI, 94% - 97% on KITTI-360, and up to 81.1% on 3DMatch,
highlighting its robustness across diverse settings.

</details>


### [11] [UltrON: Ultrasound Occupancy Networks](https://arxiv.org/abs/2509.08991)
*Magdalena Wysocki,Felix Duelmer,Ananya Bal,Nassir Navab,Mohammad Farid Azampour*

Main category: cs.CV

TL;DR: 本论文提出了一种名为UltrON的新方法，将声学特征引入占用式隐式形状表示，用于提高超声影像中三维结构的重建精度，尤其在监督不足和视图依赖等挑战下表现突出。


<details>
  <summary>Details</summary>
Motivation: 传统自由手超声成像依赖操作者经验，将二维切片在脑海中拼接为三维解剖结构。为辅助医师进行重建，目前主流方法采用体素或网格表示法、隐式方法（如SDF）。但现有隐式方法高度依赖精确的人工标注，且难以应对超声图像中的视图相关性和声影伪影等问题，部分丢失原始B超图像中的丰富声学信息。

Method: 论文提出一种基于占用函数的隐式表示，并设计了新系统UltrON。该方法无须额外标注，通过从B超图像中自动提取声学特征，结合新型损失函数，能补偿超声图像中视图依赖问题，并优化占用表示，实现跨多视图弱监督下的三维重建。

Result: 实验表明，UltrON可有效减缓因遮挡与标签稀疏导致的重建误差，提升了三维结构的准确性和几何一致性。该系统还能泛化至同类解剖结构，增强实用性和鲁棒性。

Conclusion: UltrON通过融合声学信息，弱化了对精确注释的依赖，提高了多视角下超声三维重建的准确性。该方法为临床无创影像重建提供了新思路，并有发布日期的数据集与开源代码强化其可重现性和推广应用潜力。

Abstract: In free-hand ultrasound imaging, sonographers rely on expertise to mentally
integrate partial 2D views into 3D anatomical shapes. Shape reconstruction can
assist clinicians in this process. Central to this task is the choice of shape
representation, as it determines how accurately and efficiently the structure
can be visualized, analyzed, and interpreted. Implicit representations, such as
SDF and occupancy function, offer a powerful alternative to traditional voxel-
or mesh-based methods by modeling continuous, smooth surfaces with compact
storage, avoiding explicit discretization. Recent studies demonstrate that SDF
can be effectively optimized using annotations derived from segmented B-mode
ultrasound images. Yet, these approaches hinge on precise annotations,
overlooking the rich acoustic information embedded in B-mode intensity.
Moreover, implicit representation approaches struggle with the ultrasound's
view-dependent nature and acoustic shadowing artifacts, which impair
reconstruction. To address the problems resulting from occlusions and
annotation dependency, we propose an occupancy-based representation and
introduce \gls{UltrON} that leverages acoustic features to improve geometric
consistency in weakly-supervised optimization regime. We show that these
features can be obtained from B-mode images without additional annotation cost.
Moreover, we propose a novel loss function that compensates for view-dependency
in the B-mode images and facilitates occupancy optimization from multiview
ultrasound. By incorporating acoustic properties, \gls{UltrON} generalizes to
shapes of the same anatomy. We show that \gls{UltrON} mitigates the limitations
of occlusions and sparse labeling and paves the way for more accurate 3D
reconstruction. Code and dataset will be available at
https://github.com/magdalena-wysocki/ultron.

</details>


### [12] [Implicit Neural Representations of Intramyocardial Motion and Strain](https://arxiv.org/abs/2509.09004)
*Andrew Bell,Yan Kit Choi,Steffen Peterson,Andrew King,Muhummad Sohaib Nazir,Alistair Young*

Main category: cs.CV

TL;DR: 本文提出了一种基于隐式神经表示（INR）的方法，用于从MRI心脏标记图像自动定量左心室运动和应变，无需在推理时进行优化。方法在英国生物库452例测试中实现了更高的准确性和更快的速度。


<details>
  <summary>Details</summary>
Motivation: 心肌运动和应变的定量分析对于心脏病诊断和研究非常关键，但现有自动化方法面临准确性和效率的挑战。需要新的方法兼顾精度和速度，适用于大型数据集分析。

Method: 提出利用隐式神经表示（INR），模型通过学习到的潜在编码进行条件建模，预测左心室连续位移，无需推理时再额外优化。与三种主流深度学习方法进行对比评估。

Result: 在452个英国生物库测试样本中，本方法实现了最佳跟踪精度（RMSE为2.14mm），以及最低全局环向（2.86%）和径向（6.42%）应变误差。同时推理速度约为当前最准确方法的380倍。

Conclusion: INR方法能在无需推理时优化的情况下，高效、精确地分析大规模心脏MRI数据中的心肌应变，展现出在实际临床应用和大规模队列研究中的广泛前景。

Abstract: Automatic quantification of intramyocardial motion and strain from tagging
MRI remains an important but challenging task. We propose a method using
implicit neural representations (INRs), conditioned on learned latent codes, to
predict continuous left ventricular (LV) displacement -- without requiring
inference-time optimisation. Evaluated on 452 UK Biobank test cases, our method
achieved the best tracking accuracy (2.14 mm RMSE) and the lowest combined
error in global circumferential (2.86%) and radial (6.42%) strain compared to
three deep learning baselines. In addition, our method is $\sim$380$\times$
faster than the most accurate baseline. These results highlight the suitability
of INR-based models for accurate and scalable analysis of myocardial strain in
large CMR datasets.

</details>


### [13] [E-MLNet: Enhanced Mutual Learning for Universal Domain Adaptation with Sample-Specific Weighting](https://arxiv.org/abs/2509.09006)
*Samuel Felipe dos Santos,Tiago Agostinho de Almeida,Jurandy Almeida*

Main category: cs.CV

TL;DR: 作者提出了一种增强的互学习网络（E-MLNet），通过动态加权优化UniDA任务中已知类与未知类的区分，显著提升识别性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 以往UniDA方法如MLNet在处理已知与未知类别时，对所有分类器的处理一视同仁，导致学习信号分散，降低了对关键类别边界的区分能力。作者希望通过优化分配学习资源，提升已知和未知类别分类的精度。

Method: 提出E-MLNet，通过动态加权策略结合Open-set Entropy Minimization，将注意力集中在目标样本最相关的类别边界上，从而提升已知与未知类别区分能力，并在四个公开基准数据集上进行了实验验证。

Result: E-MLNet在VisDA和ImageCLEF数据集上取得最高平均H-score。它在开放部分领域适应（Open-Partial DA）和开放集领域适应（Open-Set DA）任务中分别在31个任务中的22个和19个实现优于MLNet的表现，表现出更强的鲁棒性。

Conclusion: 所提出的E-MLNet方法通过动态加权专注于关键类别边界，有效增强了已知与未知类别的区分能力，在多个基准任务中展现出比以往方法更优的整体性能。

Abstract: Universal Domain Adaptation (UniDA) seeks to transfer knowledge from a
labeled source to an unlabeled target domain without assuming any relationship
between their label sets, requiring models to classify known samples while
rejecting unknown ones. Advanced methods like Mutual Learning Network (MLNet)
use a bank of one-vs-all classifiers adapted via Open-set Entropy Minimization
(OEM). However, this strategy treats all classifiers equally, diluting the
learning signal. We propose the Enhanced Mutual Learning Network (E-MLNet),
which integrates a dynamic weighting strategy to OEM. By leveraging the
closed-set classifier's predictions, E-MLNet focuses adaptation on the most
relevant class boundaries for each target sample, sharpening the distinction
between known and unknown classes. We conduct extensive experiments on four
challenging benchmarks: Office-31, Office-Home, VisDA-2017, and ImageCLEF. The
results demonstrate that E-MLNet achieves the highest average H-scores on VisDA
and ImageCLEF and exhibits superior robustness over its predecessor. E-MLNet
outperforms the strong MLNet baseline in the majority of individual adaptation
tasks -- 22 out of 31 in the challenging Open-Partial DA setting and 19 out of
31 in the Open-Set DA setting -- confirming the benefits of our focused
adaptation strategy.

</details>


### [14] [COCO-Urdu: A Large-Scale Urdu Image-Caption Dataset with Multimodal Quality Estimation](https://arxiv.org/abs/2509.09014)
*Umair Hassan*

Main category: cs.CV

TL;DR: 该论文构建了COCO-Urdu，这是迄今为止规模最大的乌尔都语多模态图像描述数据集，并公开了相应的数据和质量评估流程，促进了多语言视觉语言模型的公平性发展。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语在多模态和视觉语言领域显著缺乏支持，鲜有高质量大规模数据集，导致乌尔都语系统性能受限和多语言模型有资源偏向。为消除这一不足，本文旨在建立并释放高质量乌尔都语图像描述数据集。

Method: 作者基于MS COCO，通过分层采样获得59,000张图像和319,000条乌尔都语描述，采用SeamlessM4T v2自动翻译生成标注，并结合COMET-Kiwi（评估翻译质量）、CLIP相似度（验证视觉语义对应）、BERTScore回译一致性（验证语义一致性），针对低分样本迭代调用大模型进行修正，最终形成高质量数据集。

Result: 数据集质量通过BLEU、SacreBLEU和chrF等指标在基准测试中表现良好。同时，文中声明这是目前最大规模的乌尔都语图像描述公共数据集。

Conclusion: COCO-Urdu数据集及其质量评估流程的发布，有助于减轻多模态研究中的语言偏见，为发展包容性视觉语言系统奠定基础。

Abstract: Urdu, spoken by over 250 million people, remains critically under-served in
multimodal and vision-language research. The absence of large-scale,
high-quality datasets has limited the development of Urdu-capable systems and
reinforced biases in multilingual vision-language models trained primarily on
high-resource languages. To address this gap, we present COCO-Urdu, a
large-scale image-caption dataset derived from MS COCO, containing 59,000
images and 319,000 Urdu captions selected through stratified sampling to
preserve the original distribution. Captions were translated using SeamlessM4T
v2 and validated with a hybrid multimodal quality estimation framework that
integrates COMET-Kiwi for translation quality, CLIP-based similarity for visual
grounding, and BERTScore with back-translation for semantic consistency;
low-scoring captions were iteratively refined using open-source large language
models. We further benchmark COCO-Urdu on BLEU, SacreBLEU, and chrF, reporting
consistently strong results. To the best of our knowledge, COCO-Urdu is the
largest publicly available Urdu captioning dataset. By releasing both the
dataset and the quality estimation pipeline, we aim to reduce language bias in
multimodal research and establish a foundation for inclusive vision-language
systems.

</details>


### [15] [VoxelFormer: Parameter-Efficient Multi-Subject Visual Decoding from fMRI](https://arxiv.org/abs/2509.09015)
*Chenqian Le,Yilin Zhao,Nikasadat Emami,Kushagra Yadav,Xujin "Chris" Liu,Xupeng Chen,Yao Wang*

Main category: cs.CV

TL;DR: 本研究提出了一种名为VoxelFormer的高效变换器架构，实现了fMRI到视觉信息多被试解码，显著减少了模型参数，并在同类任务中取得了有竞争力的表现。


<details>
  <summary>Details</summary>
Motivation: 现有fMRI图像解码方法大多依赖被试特定训练，扩展性和实际应用受限，亟需支持多被试、参数高效的视觉解码新方法。

Method: 提出轻量级VoxelFormer变换器架构，包含Token Merging Transformer（ToMer）实现体素压缩，以及Q-Former实现与CLIP图像嵌入空间对齐的固定维神经表征。模型在7T Natural Scenes Dataset上进行评估。

Result: VoxelFormer用更少参数下，在训练被试上的检索性能与现有方法持平甚至更优，显示良好的参数效率。

Conclusion: Token merging和基于查询的变换器是实现高效神经解码的有前景技术，为多被试条件下的视觉解码任务提供了新范式。

Abstract: Recent advances in fMRI-based visual decoding have enabled compelling
reconstructions of perceived images. However, most approaches rely on
subject-specific training, limiting scalability and practical deployment. We
introduce \textbf{VoxelFormer}, a lightweight transformer architecture that
enables multi-subject training for visual decoding from fMRI. VoxelFormer
integrates a Token Merging Transformer (ToMer) for efficient voxel compression
and a query-driven Q-Former that produces fixed-size neural representations
aligned with the CLIP image embedding space. Evaluated on the 7T Natural Scenes
Dataset, VoxelFormer achieves competitive retrieval performance on subjects
included during training with significantly fewer parameters than existing
methods. These results highlight token merging and query-based transformers as
promising strategies for parameter-efficient neural decoding.

</details>


### [16] [Integrating Anatomical Priors into a Causal Diffusion Model](https://arxiv.org/abs/2509.09054)
*Binxu Li,Wei Peng,Mingjie Li,Ehsan Adeli,Kilian M. Pohl*

Main category: cs.CV

TL;DR: 提出了一种结合解剖结构约束的生成扩散模型（PCGM），用于生成更高质量、结构细节保真的脑MRI反事实图像，有助于神经影像研究中检测微小的形态差异。


<details>
  <summary>Details</summary>
Motivation: 现有MRI反事实生成方法很难保留精细的脑部解剖结构变化，从而难以辅助神经影像学中对亚群体微小形态差异的研究。该研究旨在解决模型忽略局部重要解剖变异的问题，并提升合成MRIs对实际疾病影响的临床可用性。

Method: 在生成扩散模型中引入了体素级解剖学先验约束：用概率图模块捕获解剖结构，并通过3D ControlNet编码为空间掩码，将其作为约束引入反事实去噪UNet，最终通过扩散解码器生成高质量的3D脑MRI。

Result: 在多个数据集上实验证明，PCGM生成的结构性脑MRI在图像质量和细节保真度均超越现有方法。同时首次实验证明PCGM生成的反事实MRI在疾病脑区变化上的测量结果与神经科学经典报道一致。

Conclusion: PCGM模型不仅提升了对解剖结构细节的合成能力，也推动了合成MRI在检测微小脑部形态差异、辅助神经科学研究中的实际应用价值。

Abstract: 3D brain MRI studies often examine subtle morphometric differences between
cohorts that are hard to detect visually. Given the high cost of MRI
acquisition, these studies could greatly benefit from image syntheses,
particularly counterfactual image generation, as seen in other domains, such as
computer vision. However, counterfactual models struggle to produce
anatomically plausible MRIs due to the lack of explicit inductive biases to
preserve fine-grained anatomical details. This shortcoming arises from the
training of the models aiming to optimize for the overall appearance of the
images (e.g., via cross-entropy) rather than preserving subtle, yet medically
relevant, local variations across subjects. To preserve subtle variations, we
propose to explicitly integrate anatomical constraints on a voxel-level as
prior into a generative diffusion framework. Called Probabilistic Causal Graph
Model (PCGM), the approach captures anatomical constraints via a probabilistic
graph module and translates those constraints into spatial binary masks of
regions where subtle variations occur. The masks (encoded by a 3D extension of
ControlNet) constrain a novel counterfactual denoising UNet, whose encodings
are then transferred into high-quality brain MRIs via our 3D diffusion decoder.
Extensive experiments on multiple datasets demonstrate that PCGM generates
structural brain MRIs of higher quality than several baseline approaches.
Furthermore, we show for the first time that brain measurements extracted from
counterfactuals (generated by PCGM) replicate the subtle effects of a disease
on cortical brain regions previously reported in the neuroscience literature.
This achievement is an important milestone in the use of synthetic MRIs in
studies investigating subtle morphological differences.

</details>


### [17] [Enhancing 3D Medical Image Understanding with Pretraining Aided by 2D Multimodal Large Language Models](https://arxiv.org/abs/2509.09064)
*Qiuhui Chen,Xuancheng Yao,Huping Ye,Yi Hong*

Main category: cs.CV

TL;DR: 该论文提出了一种新的3D医学图像预训练框架Med3DInsight，将3D图像编码器与2D多模态大语言模型结合，通过特殊的切片感知模块，实现无需人工标注的高效3D医学语义表征，显著提升了分割与分类任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有3D医学图像自监督学习方法在深层语义理解方面有限，而2D多模态大语言模型能提供文本辅助理解。因此，结合3D医学图像与2D多模态大模型，有望提升医学图像的语义理解能力。

Method: 提出Med3DInsight框架：以3D图像编码器作为视觉特征提取器，融合2D多模态大语言模型，通过设计切片感知transformer模块来实现3D-2D特征联动。同时采用基于部分最优传输的对齐方法，提高对LLM生成文本噪音的容忍度。

Result: 在CT与MRI多公开数据集上进行大量实验，Med3DInsight在分割与分类两个下游任务上均取得了优于现有自监督学习方法的领先性能。

Conclusion: Med3DInsight无需人工标注，可扩展并无缝集成到现有3D医学图像理解网络中，极大增强了3D医学图像的多模态表示能力，有望推动医学影像智能分析发展。

Abstract: Understanding 3D medical image volumes is critical in the medical field, yet
existing 3D medical convolution and transformer-based self-supervised learning
(SSL) methods often lack deep semantic comprehension. Recent advancements in
multimodal large language models (MLLMs) provide a promising approach to
enhance image understanding through text descriptions. To leverage these 2D
MLLMs for improved 3D medical image understanding, we propose Med3DInsight, a
novel pretraining framework that integrates 3D image encoders with 2D MLLMs via
a specially designed plane-slice-aware transformer module. Additionally, our
model employs a partial optimal transport based alignment, demonstrating
greater tolerance to noise introduced by potential noises in LLM-generated
content. Med3DInsight introduces a new paradigm for scalable multimodal 3D
medical representation learning without requiring human annotations. Extensive
experiments demonstrate our state-of-the-art performance on two downstream
tasks, i.e., segmentation and classification, across various public datasets
with CT and MRI modalities, outperforming current SSL methods. Med3DInsight can
be seamlessly integrated into existing 3D medical image understanding networks,
potentially enhancing their performance. Our source code, generated datasets,
and pre-trained models will be available at
https://github.com/Qybc/Med3DInsight.

</details>


### [18] [Improvement of Human-Object Interaction Action Recognition Using Scene Information and Multi-Task Learning Approach](https://arxiv.org/abs/2509.09067)
*Hesham M. Shehata,Mohammad Abdolrahmani*

Main category: cs.CV

TL;DR: 本文提出了一种结合场景中固定物体信息和多任务学习方法，提高基于人体骨骼动作识别的准确性，特别是在有人-物体交互场景下取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 传统以人体骨骼为基础的动作识别方法在识别有人与环境中固定物体交互的场景时效果不佳，因缺乏场景信息和合适的学习结构。作者希望解决这一问题，提升复杂场景下的人体动作识别性能。

Method: 作者提出在动作识别中引入固定物体的信息，并采用多任务学习策略，将人的动作和与周围物体的交互信息结合建模。为评估方法效果，作者采集了包含人与实际公共场所固定物体交互的真实数据集，区分交互（如操作自动售票机）和非交互（如行走、站立）动作。

Result: 该方法利用多任务学习和交互区域信息，对交互和非交互动作识别取得了99.25%的准确率，比仅用骨骼信息的基线模型高出2.75%。

Conclusion: 结合场景中固定物体信息和多任务学习，有效提升了包括人-物体交互在内的人体动作识别准确率。

Abstract: Recent graph convolutional neural networks (GCNs) have shown high performance
in the field of human action recognition by using human skeleton poses.
However, it fails to detect human-object interaction cases successfully due to
the lack of effective representation of the scene information and appropriate
learning architectures. In this context, we propose a methodology to utilize
human action recognition performance by considering fixed object information in
the environment and following a multi-task learning approach. In order to
evaluate the proposed method, we collected real data from public environments
and prepared our data set, which includes interaction classes of hands-on fixed
objects (e.g., ATM ticketing machines, check-in/out machines, etc.) and
non-interaction classes of walking and standing. The multi-task learning
approach, along with interaction area information, succeeds in recognizing the
studied interaction and non-interaction actions with an accuracy of 99.25%,
outperforming the accuracy of the base model using only human skeleton poses by
2.75%.

</details>


### [19] [IRDFusion: Iterative Relation-Map Difference guided Feature Fusion for Multispectral Object Detection](https://arxiv.org/abs/2509.09085)
*Jifeng Shen,Haibo Zhan,Xin Zuo,Heng Fan,Xiaohui Yuan,Jun Li,Wankou Yang*

Main category: cs.CV

TL;DR: 本文提出了一种针对多光谱目标检测的新型特征融合框架IRDFusion，通过抑制背景噪声和增强显著结构，有效提升了检测性能，并在多个数据集上取得了最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有多光谱目标检测方法在特征融合时容易保留多余背景或噪声，影响感知性能。论文旨在解决如何自适应增强目标相关特征并抑制共有背景干扰，以提升检测表现。

Method: 作者提出基于跨模态特征对比和筛选的创新融合框架IRDFusion。其核心模块包括互特征精炼模块（MFRM）和差异特征反馈模块（DFFM）。MFRM建模模态内外关系，提升特征区分度和对齐性；DFFM动态计算模态间差异特征作为引导，通过反馈信号指导MFRM实现更优融合。两者集成为迭代关系图差分引导特征融合机制，实现高质量的特征融合。

Result: 在FLIR、LLVIP及M^3FD等公开多光谱数据集上，IRDFusion在各类复杂场景下均显著优于现有方法，达到最新性能水平，表现出极强鲁棒性和有效性。

Conclusion: IRDFusion能通过抑制背景噪声、突出互补目标特征迭代优化融合过程，极大提升多光谱目标检测性能，适用于不同场景，结果优异。

Abstract: Current multispectral object detection methods often retain extraneous
background or noise during feature fusion, limiting perceptual performance.To
address this, we propose an innovative feature fusion framework based on
cross-modal feature contrastive and screening strategy, diverging from
conventional approaches. The proposed method adaptively enhances salient
structures by fusing object-aware complementary cross-modal features while
suppressing shared background interference.Our solution centers on two novel,
specially designed modules: the Mutual Feature Refinement Module (MFRM) and the
Differential Feature Feedback Module (DFFM). The MFRM enhances intra- and
inter-modal feature representations by modeling their relationships, thereby
improving cross-modal alignment and discriminative power.Inspired by feedback
differential amplifiers, the DFFM dynamically computes inter-modal differential
features as guidance signals and feeds them back to the MFRM, enabling adaptive
fusion of complementary information while suppressing common-mode noise across
modalities. To enable robust feature learning, the MFRM and DFFM are integrated
into a unified framework, which is formally formulated as an Iterative
Relation-Map Differential Guided Feature Fusion mechanism, termed IRDFusion.
IRDFusion enables high-quality cross-modal fusion by progressively amplifying
salient relational signals through iterative feedback, while suppressing
feature noise, leading to significant performance gains.In extensive
experiments on FLIR, LLVIP and M$^3$FD datasets, IRDFusion achieves
state-of-the-art performance and consistently outperforms existing methods
across diverse challenging scenarios, demonstrating its robustness and
effectiveness. Code will be available at
https://github.com/61s61min/IRDFusion.git.

</details>


### [20] [Model-Agnostic Open-Set Air-to-Air Visual Object Detection for Reliable UAV Perception](https://arxiv.org/abs/2509.09297)
*Spyridon Loukovitis,Anastasios Arsenos,Vasileios Karampinis,Athanasios Voulodimos*

Main category: cs.CV

TL;DR: 该论文提出了一种用于无人机空对空场景的开放集目标检测框架，专为嵌入式检测器设计，有效提升了对未知目标的识别能力并增强了对飞行数据异常的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的封闭集目标检测器在实际无人机飞行中由于域偏移和数据损坏，表现显著下降，无法满足安全关键应用对鲁棒性的要求。因此，需研发能有效拒绝未知目标且对异常数据鲁棒的新方案。

Method: 作者提出了一种与检测模型无关（model-agnostic）的开放集目标检测方法，主要创新包括：（1）在嵌入空间通过熵建模实现语义不确定性估计，（2）利用谱归一化和温度缩放强化未知目标判别能力，以及（3）结合背景样本排除机制增强整体鲁棒性。整个方法可与流行的嵌入式检测器集成，如YOLO等。

Result: 在AOT空中基准数据集及大量真实无人机飞行实验中，该方法比现有基线（如YOLO）在AUROC等指标上提升高达10%。消融实验进一步验证了各子模块的有效性，且引入背景排除不会损失检测准确率。

Conclusion: 该开放集检测方案显著提升了无人机感知系统在动态空对空环境中的鲁棒性与安全性，对实际应用有明显推动价值。

Abstract: Open-set detection is crucial for robust UAV autonomy in air-to-air object
detection under real-world conditions. Traditional closed-set detectors degrade
significantly under domain shifts and flight data corruption, posing risks to
safety-critical applications. We propose a novel, model-agnostic open-set
detection framework designed specifically for embedding-based detectors. The
method explicitly handles unknown object rejection while maintaining robustness
against corrupted flight data. It estimates semantic uncertainty via entropy
modeling in the embedding space and incorporates spectral normalization and
temperature scaling to enhance open-set discrimination. We validate our
approach on the challenging AOT aerial benchmark and through extensive
real-world flight tests. Comprehensive ablation studies demonstrate consistent
improvements over baseline methods, achieving up to a 10\% relative AUROC gain
compared to standard YOLO-based detectors. Additionally, we show that
background rejection further strengthens robustness without compromising
detection accuracy, making our solution particularly well-suited for reliable
UAV perception in dynamic air-to-air environments.

</details>


### [21] [SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models](https://arxiv.org/abs/2509.09090)
*Hengyu Fang,Yijiang Liu,Yuan Du,Li Du,Huanrui Yang*

Main category: cs.CV

TL;DR: 本文提出了一种高效的视觉-语言-动作（VLA）模型加速框架SQAP-VLA，实现了训练时免调整的量化与token剪枝协同，显著提升推理速度与效率。


<details>
  <summary>Details</summary>
Motivation: VLA模型在体感智能领域表现出色，但其高昂的计算与内存成本限制了实际部署。现有加速方法未能同时高效兼容量化与token剪枝，因此需要新的方法实现两者协同提升效率。

Method: 提出了SQAP-VLA框架，创新性地共设计量化与token剪枝流水线。引入量化感知的token剪枝标准以适应激进量化，同时优化量化器以提升剪枝效率，实现结构化、无训练推理加速。

Result: SQAP-VLA在标准VLA模型上实现了计算效率和推理速度的大幅提升，同时保持或提升模型核心表现。具体上，实现了1.93倍加速及最多平均4.5%成功率提升。

Conclusion: SQAP-VLA作为首个结构化、训练免调的VLA加速框架，有效破解了量化与token剪枝的兼容难题，兼顾高效率和核心性能，为VLA模型的实际应用部署提供了技术支撑。

Abstract: Vision-Language-Action (VLA) models exhibit unprecedented capabilities for
embodied intelligence. However, their extensive computational and memory costs
hinder their practical deployment. Existing VLA compression and acceleration
approaches conduct quantization or token pruning in an ad-hoc manner but fail
to enable both for a holistic efficiency improvement due to an observed
incompatibility. This work introduces SQAP-VLA, the first structured,
training-free VLA inference acceleration framework that simultaneously enables
state-of-the-art quantization and token pruning. We overcome the
incompatibility by co-designing the quantization and token pruning pipeline,
where we propose new quantization-aware token pruning criteria that work on an
aggressively quantized model while improving the quantizer design to enhance
pruning effectiveness. When applied to standard VLA models, SQAP-VLA yields
significant gains in computational efficiency and inference speed while
successfully preserving core model performance, achieving a $\times$1.93
speedup and up to a 4.5\% average success rate enhancement compared to the
original model.

</details>


### [22] [Classification of Driver Behaviour Using External Observation Techniques for Autonomous Vehicles](https://arxiv.org/abs/2509.09349)
*Ian Nell,Shane Gilroy*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的驾驶员行为分类系统，利用外部观测和计算机视觉技术检测驾驶员分心和受损行为，提高非联网车辆的不安全行为识别能力。


<details>
  <summary>Details</summary>
Motivation: 交通事故中驾驶员分心和受损是主要原因，现有系统大多依赖车联网技术，难以适用于非联网车辆。因而需要一种基于外部观测的普适解决方案。

Method: 基于摄像头的计算机视觉方法，结合YOLO目标检测、实时目标跟踪、侧向位移分析、定制车道估计算法，监测并识别车辆的异常侧向移动和轨迹异常。

Result: 在多样化的视频数据集上进行了实验，结果显示该系统具有较强的可靠性和较好的路况、环境适应性。

Conclusion: 提出的视觉观测方法能有效检测分心与受损驾驶行为，适合于无联网要求的车辆，为提升道路交通安全提供了实用工具。

Abstract: Road traffic accidents remain a significant global concern, with human error,
particularly distracted and impaired driving, among the leading causes. This
study introduces a novel driver behavior classification system that uses
external observation techniques to detect indicators of distraction and
impairment. The proposed framework employs advanced computer vision
methodologies, including real-time object tracking, lateral displacement
analysis, and lane position monitoring. The system identifies unsafe driving
behaviors such as excessive lateral movement and erratic trajectory patterns by
implementing the YOLO object detection model and custom lane estimation
algorithms. Unlike systems reliant on inter-vehicular communication, this
vision-based approach enables behavioral analysis of non-connected vehicles.
Experimental evaluations on diverse video datasets demonstrate the framework's
reliability and adaptability across varying road and environmental conditions.

</details>


### [23] [S-BEVLoc: BEV-based Self-supervised Framework for Large-scale LiDAR Global Localization](https://arxiv.org/abs/2509.09110)
*Chenghao Zhang,Lun Luo,Si-Yuan Cao,Xiaokai Bai,Yuncheng Jin,Zhu Yu,Beinan Yu,Yisen Wang,Hui-Liang Shen*

Main category: cs.CV

TL;DR: 本文提出了一种基于鸟瞰图(BEV)的自监督LiDAR全局定位框架S-BEVLoc，在无需真实位姿标签的情况下，实现了高效、可扩展的定位和闭环回环检测。实验表明，该方法在主流数据集上性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR定位方法严重依赖高精度真实位姿标签（如GPS），数据采集代价高，限制了方法的可扩展性。

Method: 作者提出S-BEVLoc，通过已知地理距离的BEV局部区域构建训练三元组，并基于CNN提取BEV图像特征，利用NetVLAD聚合全局描述符，同时引入SoftCos损失函数提升三元组学习效果。整个框架无需真实位姿标签，具备自监督特性。

Result: 在KITTI和NCLT大规模数据集上的实验显示，S-BEVLoc在地点识别、闭环检测和全局定位任务中均取得了领先性能，同时更易扩展到大规模环境中。

Conclusion: S-BEVLoc有效消除了标注依赖，在保持高精度的同时具有更高的扩展性，为LiDAR全局定位任务提供了新的思路和技术路径。

Abstract: LiDAR-based global localization is an essential component of simultaneous
localization and mapping (SLAM), which helps loop closure and re-localization.
Current approaches rely on ground-truth poses obtained from GPS or SLAM
odometry to supervise network training. Despite the great success of these
supervised approaches, substantial cost and effort are required for
high-precision ground-truth pose acquisition. In this work, we propose
S-BEVLoc, a novel self-supervised framework based on bird's-eye view (BEV) for
LiDAR global localization, which eliminates the need for ground-truth poses and
is highly scalable. We construct training triplets from single BEV images by
leveraging the known geographic distances between keypoint-centered BEV
patches. Convolutional neural network (CNN) is used to extract local features,
and NetVLAD is employed to aggregate global descriptors. Moreover, we introduce
SoftCos loss to enhance learning from the generated triplets. Experimental
results on the large-scale KITTI and NCLT datasets show that S-BEVLoc achieves
state-of-the-art performance in place recognition, loop closure, and global
localization tasks, while offering scalability that would require extra effort
for supervised approaches.

</details>


### [24] [Visual Grounding from Event Cameras](https://arxiv.org/abs/2509.09584)
*Lingdong Kong,Dongyue Lu,Ao Liang,Rong Li,Yuhao Dong,Tianshuai Hu,Lai Xing Ng,Wei Tsang Ooi,Benoit R. Cottereau*

Main category: cs.CV

TL;DR: 本文提出并发布了Talk2Event，这是第一个基于事件相机数据和自然语言结合的大规模语言驱动目标定位数据集。


<details>
  <summary>Details</summary>
Motivation: 事件相机可在高速动态场景和极端光照下，精确捕捉亮度变化。但目前几乎没有结合事件数据和自然语言理解的多模态感知研究，阻碍了在复杂场景下更高级的AI认知和推理应用。

Method: 作者建立了Talk2Event基准，涵盖5567个真实驾驶场景，13458个标注物体，以及3万多条经验证的指代表达，并为每条表达提取了四类结构化属性：外观、状态、与观察者关系及与周围物体关系，专门强化多模态、时序和关系性语义信息。

Result: Talk2Event提供了丰富的时空和关系线索属性，提升了基于事件相机数据的语言理解与物体定位的解释性与推理能力。该数据集支持更复杂的环境语境下的多模态目标定位分析。

Conclusion: Talk2Event为推进融合事件视觉与自然语言的时序感知、多模态理解提供了基础，有望促进机器人和人机交互等领域的智能发展。

Abstract: Event cameras capture changes in brightness with microsecond precision and
remain reliable under motion blur and challenging illumination, offering clear
advantages for modeling highly dynamic scenes. Yet, their integration with
natural language understanding has received little attention, leaving a gap in
multimodal perception. To address this, we introduce Talk2Event, the first
large-scale benchmark for language-driven object grounding using event data.
Built on real-world driving scenarios, Talk2Event comprises 5,567 scenes,
13,458 annotated objects, and more than 30,000 carefully validated referring
expressions. Each expression is enriched with four structured attributes --
appearance, status, relation to the viewer, and relation to surrounding objects
-- that explicitly capture spatial, temporal, and relational cues. This
attribute-centric design supports interpretable and compositional grounding,
enabling analysis that moves beyond simple object recognition to contextual
reasoning in dynamic environments. We envision Talk2Event as a foundation for
advancing multimodal and temporally-aware perception, with applications
spanning robotics, human-AI interaction, and so on.

</details>


### [25] [FPI-Det: a face--phone Interaction Dataset for phone-use detection and understanding](https://arxiv.org/abs/2509.09111)
*Jianqin Gao,Tianqi Wang,Yu Zhang,Yishu Zhang,Chenyuan Wang,Allan Dong,Zihao Wang*

Main category: cs.CV

TL;DR: 本文提出了FPI-Det数据集，关注于细粒度的人与手机互动检测，并在多场景下提供了标注与基线模型评测。


<details>
  <summary>Details</summary>
Motivation: 随着移动设备的广泛使用，在安全监控、工作场所生产力评估和注意力管理等领域，对是否使用手机的检测需求增加，而现有基准数据集无法充分反映人-设备细粒度交互。

Method: 作者提出了FPI-Det数据集，共包含22,879张图像，涵盖工作、教育、交通和公共场景，并同步标注了人脸和手机。数据集包含极端的尺度变化、频繁遮挡和多样化采集条件。论文还对代表性的YOLO与DETR目标检测器进行了评估，针对不同物体大小、遮挡程度和环境进行了性能分析。

Result: 在FPI-Det数据集上，YOLO和DETR等主流目标检测方法在不同物体尺寸、遮挡水平和各种环境下的表现基准被给出，反映了现有模型在该任务中的优势和挑战。

Conclusion: FPI-Det数据集针对现有研究空白，促进了细粒度人体-设备交互检测的发展，并为后续工作提供了实验基准和公开资源。

Abstract: The widespread use of mobile devices has created new challenges for vision
systems in safety monitoring, workplace productivity assessment, and attention
management. Detecting whether a person is using a phone requires not only
object recognition but also an understanding of behavioral context, which
involves reasoning about the relationship between faces, hands, and devices
under diverse conditions. Existing generic benchmarks do not fully capture such
fine-grained human--device interactions. To address this gap, we introduce the
FPI-Det, containing 22{,}879 images with synchronized annotations for faces and
phones across workplace, education, transportation, and public scenarios. The
dataset features extreme scale variation, frequent occlusions, and varied
capture conditions. We evaluate representative YOLO and DETR detectors,
providing baseline results and an analysis of performance across object sizes,
occlusion levels, and environments. Source code and dataset is available at
https://github.com/KvCgRv/FPI-Det.

</details>


### [26] [Zero-shot Hierarchical Plant Segmentation via Foundation Segmentation Models and Text-to-image Attention](https://arxiv.org/abs/2509.09116)
*Junhao Xing,Ryohei Miyakawa,Yang Yang,Xinpeng Liu,Risa Shinoda,Hiroaki Santo,Yosuke Toda,Fumio Okura*

Main category: cs.CV

TL;DR: 本文提出了ZeroPlantSeg，无需额外训练即可实现对叶片重叠植物个体的零样本分割，并在多物种、不同生长阶段及环境下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有分割模型虽能提取单片叶子，但对于一株包含多片重叠叶子的完整植物分割仍然困难。通常需大量带注释的数据，耗费人力且受物种限制。研究动机在于实现无需额外标注和训练即可完成植物整体分割，提高泛化能力和实用性。

Method: 提出了ZeroPlantSeg，将现有基础分割模型用于叶片实例提取，并结合视觉-语言模型推理植物结构，从而实现对植物个体的零样本层次化分割，无需额外训练数据或注释。

Result: ZeroPlantSeg在多物种、不同生长阶段及多拍摄环境的数据集上进行评估，结果显示其优于现有零样本分割方法，并在跨领域实验中表现超过部分有监督方法。

Conclusion: ZeroPlantSeg具备良好的领域泛化能力和无需再训练即可分割重叠植物个体的优点，有望推动相关领域的数据高效利用和实际应用。

Abstract: Foundation segmentation models achieve reasonable leaf instance extraction
from top-view crop images without training (i.e., zero-shot). However,
segmenting entire plant individuals with each consisting of multiple
overlapping leaves remains challenging. This problem is referred to as a
hierarchical segmentation task, typically requiring annotated training
datasets, which are often species-specific and require notable human labor. To
address this, we introduce ZeroPlantSeg, a zero-shot segmentation for
rosette-shaped plant individuals from top-view images. We integrate a
foundation segmentation model, extracting leaf instances, and a vision-language
model, reasoning about plants' structures to extract plant individuals without
additional training. Evaluations on datasets with multiple plant species,
growth stages, and shooting environments demonstrate that our method surpasses
existing zero-shot methods and achieves better cross-domain performance than
supervised methods. Implementations are available at
https://github.com/JunhaoXing/ZeroPlantSeg.

</details>


### [27] [Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval](https://arxiv.org/abs/2509.09118)
*Tianlu Zheng,Yifan Zhang,Xiang An,Ziyong Feng,Kaicheng Yang,Qichuan Ding*

Main category: cs.CV

TL;DR: 本文提升了CLIP在人像表征学习中的表现，通过高质量数据集WebPerson和GA-DMS架构，实现了跨模态对齐和更精细的语义表征，在多个基准上刷新了现有最好成绩。


<details>
  <summary>Details</summary>
Motivation: CLIP虽在多任务中强大，但用于人像表征时，面临缺乏大规模高质量带注释数据和全局对比学习无法提取细粒度特征的问题。因此，需要新的方法和数据来提升其在人像理解上的表现。

Method: 1）开发了自动过滤和生成图文描述的数据管道，构建了包含500万高质量图文对的WebPerson数据集；2）提出了GA-DMS框架，通过梯度-注意力相似性分数自适应屏蔽噪声文本，同时加入掩码文本预测目标，提升细粒度语义表征能力。

Result: GA-DMS在多个公开基准上达到了当前最优的人像表征性能，展示了方法的有效性和优越性。

Conclusion: 针对CLIP在人物表征学习中的不足，采用新的数据与方法显著提升了其效果，这些技术有望推动大规模视觉-语言模型在人像等细粒度任务上的应用与研究。

Abstract: Although Contrastive Language-Image Pre-training (CLIP) exhibits strong
performance across diverse vision tasks, its application to person
representation learning faces two critical challenges: (i) the scarcity of
large-scale annotated vision-language data focused on person-centric images,
and (ii) the inherent limitations of global contrastive learning, which
struggles to maintain discriminative local features crucial for fine-grained
matching while remaining vulnerable to noisy text tokens. This work advances
CLIP for person representation learning through synergistic improvements in
data curation and model architecture. First, we develop a noise-resistant data
construction pipeline that leverages the in-context learning capabilities of
MLLMs to automatically filter and caption web-sourced images. This yields
WebPerson, a large-scale dataset of 5M high-quality person-centric image-text
pairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking
Synergetic) framework, which improves cross-modal alignment by adaptively
masking noisy textual tokens based on the gradient-attention similarity score.
Additionally, we incorporate masked token prediction objectives that compel the
model to predict informative text tokens, enhancing fine-grained semantic
representation learning. Extensive experiments show that GA-DMS achieves
state-of-the-art performance across multiple benchmarks.

</details>


### [28] [ALL-PET: A Low-resource and Low-shot PET Foundation Model in the Projection Domain](https://arxiv.org/abs/2509.09130)
*Bin Huang,Kang Chen,Bingxuan Li,Huafeng Liu,Qiegen Liu*

Main category: cs.CV

TL;DR: ALL-PET是一种针对PET影像的低资源、低样本量基础模型，采用投影域（sinogram）建模。利用Latent Diffusion Model并结合三项创新技术，大幅提升了小样本下的生成与处理能力。


<details>
  <summary>Details</summary>
Motivation: 当前构建大规模PET影像基础模型面临数据标签稀缺与计算资源受限的问题。研究动机是如何利用更少的标注数据和资源，依然能获得泛化性强、任务自适应的基础模型。

Method: 1. 提出Radon掩膜增强策略（RMAS），通过投影各种掩膜到sinogram空间，提升数据多样性；2. 引入动态多掩膜机制（DMM），进一步丰富数据而不增加模型复杂度；3. 设置正负掩膜约束，强化几何一致性；4. 创新使用透明医学注意力机制（TMA），在原始投影域实时突出病灶区域，并允许临床医生自定义ROI。核心模型为潜变量扩散模型。

Result: ALL-PET仅需500个样本即可实现高质量sinogram生成，性能可媲美更大数据集训练的模型，且泛化于低剂量重建、衰减校正、延迟帧预测和示踪剂分离等多种PET相关任务，显著降低内存和资源需求。

Conclusion: ALL-PET在小样本、低资源条件下实现了高质量、多任务的PET影像生成与辅助，兼具优良的通用性与效率，对基础医学影像模型构建具有重要意义。

Abstract: Building large-scale foundation model for PET imaging is hindered by limited
access to labeled data and insufficient computational resources. To overcome
data scarcity and efficiency limitations, we propose ALL-PET, a low-resource,
low-shot PET foundation model operating directly in the projection domain.
ALL-PET leverages a latent diffusion model (LDM) with three key innovations.
First, we design a Radon mask augmentation strategy (RMAS) that generates over
200,000 structurally diverse training samples by projecting randomized
image-domain masks into sinogram space, significantly improving generalization
with minimal data. This is extended by a dynamic multi-mask (DMM) mechanism
that varies mask quantity and distribution, enhancing data diversity without
added model complexity. Second, we implement positive/negative mask constraints
to embed strict geometric consistency, reducing parameter burden while
preserving generation quality. Third, we introduce transparent medical
attention (TMA), a parameter-free, geometry-driven mechanism that enhances
lesion-related regions in raw projection data. Lesion-focused attention maps
are derived from coarse segmentation, covering both hypermetabolic and
hypometabolic areas, and projected into sinogram space for physically
consistent guidance. The system supports clinician-defined ROI adjustments,
ensuring flexible, interpretable, and task-adaptive emphasis aligned with PET
acquisition physics. Experimental results show ALL-PET achieves high-quality
sinogram generation using only 500 samples, with performance comparable to
models trained on larger datasets. ALL-PET generalizes across tasks including
low-dose reconstruction, attenuation correction, delayed-frame prediction, and
tracer separation, operating efficiently with memory use under 24GB.

</details>


### [29] [Noise-Robust Topology Estimation of 2D Image Data via Neural Networks and Persistent Homology](https://arxiv.org/abs/2509.09140)
*Dylan Peek,Matthew P. Skerritt,Stephan Chalup*

Main category: cs.CV

TL;DR: 比较了人工神经网络（ANN）和持久同调（PH）在2D二值图像拓扑估计中的抗噪性能，结果显示ANN在噪声条件下优于传统PH方法。


<details>
  <summary>Details</summary>
Motivation: 当前拓扑分析常用的持久同调（PH）方法在噪声较大数据中表现有限，而人工神经网络（ANN）能够学习数据的上下文和几何先验，作者希望探究ANN在噪声环境下估算拓扑结构（Betti数）的能力，评估其对传统PH方法的替代潜力。

Method: 作者设计了一个利用人工神经网络预测2D二值图像Betti数的监督学习方案，并与基于立方复形及符号欧氏距离变换（SEDT）的经典PH流程进行对比。实验涵盖了一个合成数据集和两个真实世界数据集，通过人为引入噪声检验两者性能。

Result: 实验证明，在有噪声情况下，ANN方法在预测Betti数方面的表现优于基于PH的方案。这主要归因于ANN能够从训练数据中捕捉到上下文和几何信息，从而提升抗噪能力。

Conclusion: 人工神经网络在结构噪声条件下，对拓扑估计具有较强的鲁棒性，成为传统持久同调方法的有力补充和新兴替代方案。

Abstract: Persistent Homology (PH) and Artificial Neural Networks (ANNs) offer
contrasting approaches to inferring topological structure from data. In this
study, we examine the noise robustness of a supervised neural network trained
to predict Betti numbers in 2D binary images. We compare an ANN approach
against a PH pipeline based on cubical complexes and the Signed Euclidean
Distance Transform (SEDT), which is a widely adopted strategy for noise-robust
topological analysis. Using one synthetic and two real-world datasets, we show
that ANNs can outperform this PH approach under noise, likely due to their
capacity to learn contextual and geometric priors from training data. Though
still emerging, the use of ANNs for topology estimation offers a compelling
alternative to PH under structural noise.

</details>


### [30] [Objectness Similarity: Capturing Object-Level Fidelity in 3D Scene Evaluation](https://arxiv.org/abs/2509.09143)
*Yuiko Uchida,Ren Togo,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CV

TL;DR: 本文提出了一种新型3D场景评估指标OSIM，强调以“物体”为中心的评价方式，并通过实验验证了其更符合人类感知。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景评估指标侧重整体质量，易与人类主观认知产生偏差。作者受到神经心理学启发，认为人类对3D场景的理解本质上依赖对独立物体的关注，因此需要一种以物体为中心的新评估方法。

Method: 作者提出OSIM指标，利用物体检测模型及其特征表征，量化每个场景中各个物体“物体性”，实现物体为中心的3D场景评估。同时，通过用户调研和实验分析OSIM特性，并在统一实验环境下，重新评估了近年来的3D重建和生成模型。

Result: OSIM在用户调研中表现出更高的人类感知一致性，优于现有3D场景评估指标，同时，通过标准化实验，进一步厘清了模型间的差异及进展。

Conclusion: OSIM有效弥补了传统3D评估方法与人类主观感知间的差距，为3D场景评估提供了新的思路和工具，有助于推动相关模型的改进和发展。

Abstract: This paper presents Objectness SIMilarity (OSIM), a novel evaluation metric
for 3D scenes that explicitly focuses on "objects," which are fundamental units
of human visual perception. Existing metrics assess overall image quality,
leading to discrepancies with human perception. Inspired by neuropsychological
insights, we hypothesize that human recognition of 3D scenes fundamentally
involves attention to individual objects. OSIM enables object-centric
evaluations by leveraging an object detection model and its feature
representations to quantify the "objectness" of each object in the scene. Our
user study demonstrates that OSIM aligns more closely with human perception
compared to existing metrics. We also analyze the characteristics of OSIM using
various approaches. Moreover, we re-evaluate recent 3D reconstruction and
generation models under a standardized experimental setup to clarify
advancements in this field. The code is available at
https://github.com/Objectness-Similarity/OSIM.

</details>


### [31] [Video Understanding by Design: How Datasets Shape Architectures and Insights](https://arxiv.org/abs/2509.09151)
*Lei Wang,Piotr Koniusz,Yongsheng Gao*

Main category: cs.CV

TL;DR: 本综述提出以数据集驱动的视角，系统分析视频理解模型的演进，强调数据集对模型结构的影响，为未来视频理解研究提供实践指导。


<details>
  <summary>Details</summary>
Motivation: 现有综述多按任务或模型类别分类，忽略了数据集对模型结构演进的深远影响。因此，本综述旨在从数据集诱导偏置的角度，系统梳理视频理解模型的发展。

Method: 作者从运动复杂性、时间跨度、层次组成和多模态丰富性四个方面分析数据集对模型的诱导偏置，重新解读了从两流、3D CNN到序列化、Transformer和多模态大模型的发展脉络。

Result: 综述显示，主流视频理解架构的演化实质上是对数据集“压力”的响应，并据此提出了模型设计应如何与数据集特性匹配的实践建议。

Conclusion: 本综述将数据集、诱导偏置与架构统一于同一理论框架，不仅系统回顾了视频理解的研究进展，还为未来通用视频理解的模型设计提供了清晰的路线图。

Abstract: Video understanding has advanced rapidly, fueled by increasingly complex
datasets and powerful architectures. Yet existing surveys largely classify
models by task or family, overlooking the structural pressures through which
datasets guide architectural evolution. This survey is the first to adopt a
dataset-driven perspective, showing how motion complexity, temporal span,
hierarchical composition, and multimodal richness impose inductive biases that
models should encode. We reinterpret milestones, from two-stream and 3D CNNs to
sequential, transformer, and multimodal foundation models, as concrete
responses to these dataset-driven pressures. Building on this synthesis, we
offer practical guidance for aligning model design with dataset invariances
while balancing scalability and task demands. By unifying datasets, inductive
biases, and architectures into a coherent framework, this survey provides both
a comprehensive retrospective and a prescriptive roadmap for advancing
general-purpose video understanding.

</details>


### [32] [OCELOT 2023: Cell Detection from Cell-Tissue Interaction Challenge](https://arxiv.org/abs/2509.09153)
*JaeWoong Shin,Jeongun Ryu,Aaron Valero Puche,Jinhee Lee,Biagio Brattoli,Wonkyung Jung,Soo Ick Cho,Kyunghyun Paeng,Chan-Young Ock,Donggeun Yoo,Zhaoyang Li,Wangkai Li,Huayu Mai,Joshua Millward,Zhen He,Aiden Nibali,Lydia Anette Schoenpflug,Viktor Hendrik Koelzer,Xu Shuoyu,Ji Zheng,Hu Bin,Yu-Wen Lo,Ching-Hui Yang,Sérgio Pereira*

Main category: cs.CV

TL;DR: 本论文介绍了OCELOT 2023挑战赛，其关注点在于多尺度细胞与组织结构的相互关系对于病理图像分析的重要性，并评估了多个参赛团队的方法。结果显示结合细胞与组织关系的多尺度方法显著优于传统仅检测细胞的方法。


<details>
  <summary>Details</summary>
Motivation: 病理学家通过多倍镜切换观察整张切片，结合细胞与组织形态实现诊断。现有深度学习细胞检测模型难以模拟多尺度信息融合，原因之一是缺乏多尺度注释数据集。因此需要推动相关数据和方法发展，验证细胞与组织关系对于判读的重要作用。

Method: OCELOT 2023挑战赛提供六种器官的细胞检测与组织分割重叠标注数据（673对，来自TCGA），分为训练/验证/测试集。参赛者提交多种结合细胞-组织关系的检测模型，论文对这些方法进行对比分析，关注创新策略。

Result: 最佳参赛队模型在测试集F1-score相比不结合细胞-组织关系的基线模型提升最高可达7.99，显示细胞-组织联合建模对性能提升显著。

Conclusion: 多尺度、细胞-组织语义关系建模对于人类级性能至关重要，本文的数据集与竞赛推动了该领域研究，未来病理AI应整合多尺度语义以提升诊断水平。

Abstract: Pathologists routinely alternate between different magnifications when
examining Whole-Slide Images, allowing them to evaluate both broad tissue
morphology and intricate cellular details to form comprehensive diagnoses.
However, existing deep learning-based cell detection models struggle to
replicate these behaviors and learn the interdependent semantics between
structures at different magnifications. A key barrier in the field is the lack
of datasets with multi-scale overlapping cell and tissue annotations. The
OCELOT 2023 challenge was initiated to gather insights from the community to
validate the hypothesis that understanding cell and tissue (cell-tissue)
interactions is crucial for achieving human-level performance, and to
accelerate the research in this field. The challenge dataset includes
overlapping cell detection and tissue segmentation annotations from six organs,
comprising 673 pairs sourced from 306 The Cancer Genome Atlas (TCGA)
Whole-Slide Images with hematoxylin and eosin staining, divided into training,
validation, and test subsets. Participants presented models that significantly
enhanced the understanding of cell-tissue relationships. Top entries achieved
up to a 7.99 increase in F1-score on the test set compared to the baseline
cell-only model that did not incorporate cell-tissue relationships. This is a
substantial improvement in performance over traditional cell-only detection
methods, demonstrating the need for incorporating multi-scale semantics into
the models. This paper provides a comparative analysis of the methods used by
participants, highlighting innovative strategies implemented in the OCELOT 2023
challenge.

</details>


### [33] [RT-DETR++ for UAV Object Detection](https://arxiv.org/abs/2509.09157)
*Yuan Shufang*

Main category: cs.CV

TL;DR: 本文提出了RT-DETR++，通过改进编码器架构，提升无人机图像中小目标和密集目标的检测性能，同时保证了实时检测速度与计算效率。


<details>
  <summary>Details</summary>
Motivation: 无人机图像中的目标检测面临诸如小目标密集、尺度变化大、易被遮挡等难题，现有方法在保持速度和复杂度的基础上，往往难以兼顾精度，亟需更有效的特征编码与融合方案。

Method: 提出了两项关键改进：一是引入基于通道门控注意力的上/下采样机制（AU/AD），以减少特征传递过程中的误差并保留细节；二是在特征融合阶段采用CSP-PAC，利用并行空洞卷积融合多尺度的局部与上下文信息。

Result: 实验显示，所提出的新颖特征融合结构在小目标及密集目标检测方面性能优越，且在计算复杂度不增加的情况下，保持了实时检测所需的速度。

Conclusion: RT-DETR++为实时目标检测系统的特征编码设计提供了有效方法，尤其适用于无人机小目标和密集目标的检测场景。

Abstract: Object detection in unmanned aerial vehicle (UAV) imagery presents
significant challenges. Issues such as densely packed small objects, scale
variations, and occlusion are commonplace. This paper introduces RT-DETR++,
which enhances the encoder component of the RT-DETR model. Our improvements
focus on two key aspects. First, we introduce a channel-gated attention-based
upsampling/downsampling (AU/AD) mechanism. This dual-path system minimizes
errors and preserves details during feature layer propagation. Second, we
incorporate CSP-PAC during feature fusion. This technique employs parallel
hollow convolutions to process local and contextual information within the same
layer, facilitating the integration of multi-scale features. Evaluation
demonstrates that our novel neck design achieves superior performance in
detecting small and densely packed objects. The model maintains sufficient
speed for real-time detection without increasing computational complexity. This
study provides an effective approach for feature encoding design in real-time
detection systems.

</details>


### [34] [A Knowledge Noise Mitigation Framework for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2509.09159)
*Zhiyue Liu,Sihang Liu,Jinyuan Liu,Xinru Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种无须训练的新型知识关注框架，用以提升知识驱动视觉问答（KB-VQA）系统在利用外部知识时的效率和准确性，通过减少冗余与噪声，实现对问题相关知识的高效获取与应用，显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的KB-VQA方法在融合外部知识时，常常忽视了知识的冗余，直接将检索到的大量信息用于模型推理，这一做法容易引入噪声，影响问答精度。因此，亟需一种能够突出有用知识、减少无关内容干扰的技术路线。

Method: 1. 针对知识检索阶段，框架通过分析图片与问题对内容，构建低噪声查询，从源头上提高所检索知识的相关性。2. 对于已检索到的知识片段，进一步使用大模型分析与提取直接有助于回答问题的内容。3. 在知识整合环节，提出选择性整合策略，只有在模型对答案不自信时才引入外部知识，从而最大限度减少冗余信息带来的负面影响。整个流程无需额外训练。

Result: 实验证明，该框架能够更精准高效地获取并利用相关知识，有效提升KB-VQA的答案准确率，在多个基准测试上优于当前最先进方法。

Conclusion: 通过知识关注和选择性整合，本文提出的无训练KB-VQA新框架有效克服了知识冗余带来的噪声问题，为视觉问答任务提供了更清晰的知识支持，为后续相关研究提供了新思路。

Abstract: Knowledge-based visual question answering (KB-VQA) requires a model to
understand images and utilize external knowledge to provide accurate answers.
Existing approaches often directly augment models with retrieved information
from knowledge sources while ignoring substantial knowledge redundancy, which
introduces noise into the answering process. To address this, we propose a
training-free framework with knowledge focusing for KB-VQA, that mitigates the
impact of noise by enhancing knowledge relevance and reducing redundancy.
First, for knowledge retrieval, our framework concludes essential parts from
the image-question pairs, creating low-noise queries that enhance the retrieval
of highly relevant knowledge. Considering that redundancy still persists in the
retrieved knowledge, we then prompt large models to identify and extract
answer-beneficial segments from knowledge. In addition, we introduce a
selective knowledge integration strategy, allowing the model to incorporate
knowledge only when it lacks confidence in answering the question, thereby
mitigating the influence of redundant information. Our framework enables the
acquisition of accurate and critical knowledge, and extensive experiments
demonstrate that it outperforms state-of-the-art methods.

</details>


### [35] [CWSSNet: Hyperspectral Image Classification Enhanced by Wavelet Domain Convolution](https://arxiv.org/abs/2509.09163)
*Yulin Tong,Fengzong Zhang,Haiqin Cheng*

Main category: cs.CV

TL;DR: 本研究提出了一种新的基于波形卷积的3D光谱-空间特征融合分类网络CWSSNet，用于高光谱遥感影像的地物分类，有效提升了分类准确度并具备良好的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 高光谱遥感影像在地物分类中准确度高，但高维和波段冗余问题导致分类性能瓶颈。为此，亟需高效方法以充分利用光谱-空间特征，减少冗余并缓解样本小带来的性能下降。

Method: 以ZY1F卫星的高光谱影像为数据源，选取江西省余干县为研究区，提出包含多尺度特征融合和波段分解的CWSSNet。该模型结合3D光谱-空间特征、波形卷积以及多模态特征，通过多尺度卷积注意力模块和波形域卷积操作，有效融合信息，突破传统方法瓶颈。

Result: 在余干县实验证明，CWSSNet获得74.50%的mIoU、82.73%的mAcc和84.94%的mF1。对水体、植被和裸地取得最高IoU，显示出良好鲁棒性。此外，在训练集占比70%时，训练时间增加有限，分类性能仍接近最优。

Conclusion: CWSSNet能有效提升高光谱遥感影像地物分类效果，在应对高维冗余、提升鲁棒性以及对小样本场景下均具优异表现，显示出良好应用前景。

Abstract: Hyperspectral remote sensing technology has significant application value in
fields such as forestry ecology and precision agriculture, while also putting
forward higher requirements for fine ground object classification. However,
although hyperspectral images are rich in spectral information and can improve
recognition accuracy, they tend to cause prominent feature redundancy due to
their numerous bands, high dimensionality, and spectral mixing characteristics.
To address this, this study used hyperspectral images from the ZY1F satellite
as a data source and selected Yugan County, Shangrao City, Jiangxi Province as
the research area to perform ground object classification research. A
classification framework named CWSSNet was proposed, which integrates 3D
spectral-spatial features and wavelet convolution. This framework integrates
multimodal information us-ing a multiscale convolutional attention module and
breaks through the classification performance bottleneck of traditional methods
by introducing multi-band decomposition and convolution operations in the
wavelet domain. The experiments showed that CWSSNet achieved 74.50\%, 82.73\%,
and 84.94\% in mean Intersection over Union (mIoU), mean Accuracy (mAcc), and
mean F1-score (mF1) respectively in Yugan County. It also obtained the highest
Intersection over Union (IoU) in the classifica-tion of water bodies,
vegetation, and bare land, demonstrating good robustness. Additionally, when
the training set proportion was 70\%, the increase in training time was
limited, and the classification effect was close to the optimal level,
indicating that the model maintains reliable performance under small-sample
training conditions.

</details>


### [36] [Bridging the Gap Between Ideal and Real-world Evaluation: Benchmarking AI-Generated Image Detection in Challenging Scenarios](https://arxiv.org/abs/2509.09172)
*Chunxiao Li,Xiaoxiao Wang,Meiling Li,Boming Miao,Peng Sun,Yunjian Zhang,Xiangyang Ji,Yao Zhu*

Main category: cs.CV

TL;DR: 论文介绍了一个新的用于评估AI生成图片检测方法现实场景适用性的RRDataset数据集，涵盖多维度测试，包括场景泛化、互联网传播鲁棒性和再数字化鲁棒性，并进行多种检测器和人类测试，揭示了现有检测方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图片检测方法在复杂真实世界条件下的性能尚不清楚，尤其缺乏多样化场景、不同网络传播和多种再数字化情况下的系统性评估。因此亟需新的数据集和评测体系来反映检测方法在实际应用中的效果。

Method: 构建了名为RRDataset的新数据集，涵盖来自七大现实场景的高质量图片，测试检测器在不同社交媒体多次传播后的图片和四种再数字化方式处理后的图片的鲁棒性。对17种检测器和10种视觉-语言模型进行基准测试，并设置192人参与的人类few-shot学习能力实验。

Result: 实验表明，多数现有AI生成图片检测方法在真实场景及图像多轮传播和再数字化后表现出明显性能下降。人类在少样本条件下具备一定适应性，但整体效果亦有限。

Conclusion: 论文强调当前检测方法与实际需求存在差距，人类灵活性具参考价值。未来应结合人类识别优势，开发更加鲁棒的AI生成图片检测算法。

Abstract: With the rapid advancement of generative models, highly realistic image
synthesis has posed new challenges to digital security and media credibility.
Although AI-generated image detection methods have partially addressed these
concerns, a substantial research gap remains in evaluating their performance
under complex real-world conditions. This paper introduces the Real-World
Robustness Dataset (RRDataset) for comprehensive evaluation of detection models
across three dimensions: 1) Scenario Generalization: RRDataset encompasses
high-quality images from seven major scenarios (War and Conflict, Disasters and
Accidents, Political and Social Events, Medical and Public Health, Culture and
Religion, Labor and Production, and everyday life), addressing existing dataset
gaps from a content perspective. 2) Internet Transmission Robustness: examining
detector performance on images that have undergone multiple rounds of sharing
across various social media platforms. 3) Re-digitization Robustness: assessing
model effectiveness on images altered through four distinct re-digitization
methods. We benchmarked 17 detectors and 10 vision-language models (VLMs) on
RRDataset and conducted a large-scale human study involving 192 participants to
investigate human few-shot learning capabilities in detecting AI-generated
images. The benchmarking results reveal the limitations of current AI detection
methods under real-world conditions and underscore the importance of drawing on
human adaptability to develop more robust detection algorithms.

</details>


### [37] [Dark-ISP: Enhancing RAW Image Processing for Low-Light Object Detection](https://arxiv.org/abs/2509.09183)
*Jiasheng Guo,Xin Gao,Yuxiang Yan,Guanghao Li,Jian Pu*

Main category: cs.CV

TL;DR: 提出了一种轻量级、可自适应的图像信号处理（ISP）插件Dark-ISP，直接处理低光环境下的Bayer RAW图像，实现端到端的目标检测，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 低光环境下的目标检测因图像退化而非常困难。虽然RAW图像理论上优于RGB图像，但现有方法会有信息损失或结构复杂，需要新的方法既充分利用RAW图像信息，又简化结构并提升检测效果。

Method: 提出Dark-ISP插件：1）将传统ISP流程分解为线性（传感器校准）和非线性（色调映射）可微分模块，通过任务驱动损失优化，各模块具备内容自适应与物理先验，自动完成RAW到RGB转换。2）利用ISP本身的级联结构，设计了自增强机制促进各子模块协同优化。

Result: 在三个RAW图像数据集上，通过广泛实验表明，该方法在低光环境下，参数量少，检测效果优于先进的RGB及RAW检测方法。

Conclusion: Dark-ISP有效提升了低光下目标检测性能，并且结构轻量，具备实际应用潜力。

Abstract: Low-light Object detection is crucial for many real-world applications but
remains challenging due to degraded image quality. While recent studies have
shown that RAW images offer superior potential over RGB images, existing
approaches either use RAW-RGB images with information loss or employ complex
frameworks. To address these, we propose a lightweight and self-adaptive Image
Signal Processing (ISP) plugin, Dark-ISP, which directly processes Bayer RAW
images in dark environments, enabling seamless end-to-end training for object
detection. Our key innovations are: (1) We deconstruct conventional ISP
pipelines into sequential linear (sensor calibration) and nonlinear (tone
mapping) sub-modules, recasting them as differentiable components optimized
through task-driven losses. Each module is equipped with content-aware
adaptability and physics-informed priors, enabling automatic RAW-to-RGB
conversion aligned with detection objectives. (2) By exploiting the ISP
pipeline's intrinsic cascade structure, we devise a Self-Boost mechanism that
facilitates cooperation between sub-modules. Through extensive experiments on
three RAW image datasets, we demonstrate that our method outperforms
state-of-the-art RGB- and RAW-based detection approaches, achieving superior
results with minimal parameters in challenging low-light environments.

</details>


### [38] [VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal Models: Methods and Results](https://arxiv.org/abs/2509.09190)
*Hanwei Zhu,Haoning Wu,Zicheng Zhang,Lingyu Zhu,Yixuan Li,Peilin Chen,Shiqi Wang,Chris Wei Zhou,Linhan Cao,Wei Sun,Xiangyang Zhu,Weixia Zhang,Yucheng Zhu,Jing Liu,Dandan Zhu,Guangtao Zhai,Xiongkuo Min,Zhichao Zhang,Xinyue Li,Shubo Xu,Anh Dao,Yifan Li,Hongyuan Yu,Jiaojiao Yi,Yiding Tian,Yupeng Wu,Feiran Sun,Lijuan Liao,Song Jiang*

Main category: cs.CV

TL;DR: 本文介绍了ICCV 2025举办的VQualA 2025多模态大模型视觉质量比较挑战赛及其主要成果。该挑战旨在评估和提升大模型对多图像视觉质量差异的推理能力，推出了包含数千项任务的新基准，并吸引了约100名参与者和五个模型参赛。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大模型的发展，对其理解和比较视觉质量能力的需求日益突出。此前缺乏可用于评估这些模型进行细致视觉质量推理的系统性基准和评测协议。

Method: 挑战赛设立了开放域视觉质量比较任务，包括单图像、两图像和多图像分组等多种粒度任务。评测采用综合协议，比如2AFC二元选择和多项选择题，参赛模型需针对这些任务给出高质量判断。

Result: 挑战收到约100份参赛作品，并筛选出五个表现突出的模型，展示了经指令调优的大模型在视觉质量评估任务上的新能力。

Conclusion: VQualA 2025挑战为开放领域视觉质量推理和比较树立了新基准，推动了可解释和与人类一致的质量评估系统研究，为未来相关领域发展奠定了坚实基础。

Abstract: This paper presents a summary of the VQualA 2025 Challenge on Visual Quality
Comparison for Large Multimodal Models (LMMs), hosted as part of the ICCV 2025
Workshop on Visual Quality Assessment. The challenge aims to evaluate and
enhance the ability of state-of-the-art LMMs to perform open-ended and detailed
reasoning about visual quality differences across multiple images. To this end,
the competition introduces a novel benchmark comprising thousands of
coarse-to-fine grained visual quality comparison tasks, spanning single images,
pairs, and multi-image groups. Each task requires models to provide accurate
quality judgments. The competition emphasizes holistic evaluation protocols,
including 2AFC-based binary preference and multi-choice questions (MCQs).
Around 100 participants submitted entries, with five models demonstrating the
emerging capabilities of instruction-tuned LMMs on quality assessment. This
challenge marks a significant step toward open-domain visual quality reasoning
and comparison and serves as a catalyst for future research on interpretable
and human-aligned quality evaluation systems.

</details>


### [39] [MGTraj: Multi-Granularity Goal-Guided Human Trajectory Prediction with Recursive Refinement Network](https://arxiv.org/abs/2509.09200)
*Ge Sun,Jun Ma*

Main category: cs.CV

TL;DR: 本文提出了一种新的人体轨迹预测方法MGTraj，能够利用不同时间粒度的信息递归细化预测，方法在多个公开数据集上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有基于目标引导的人体轨迹预测方法通常只关注粗粒度的目标预测和细粒度的轨迹生成，忽略了中间粒度信息的潜在作用。中间粒度能更好地捕捉人类行为中的多尺度动态和运动模式，但如何在目标引导框架中有效融合多粒度信息仍是挑战。

Method: 提出MGTraj模型，从粗到细递归编码轨迹提案。每个粒度层都由基于transformer的递归细化网络（RRN）捕获特征，并进一步预测细化。不同粒度之间通过参数共享方式集成特征，辅以速度预测作为辅助任务。

Result: MGTraj在EHT/UCY和Stanford Drone Dataset等公开数据集上进行了全面实验，对比主流方法显示在预测精度上具有显著优势，达到了当前目标引导方法的最优性能。

Conclusion: 通过多粒度融合和辅助任务，MGTraj提升了轨迹预测的准确性，证明了多粒度建模在目标引导轨迹预测中的实际应用价值和优越性。

Abstract: Accurate human trajectory prediction is crucial for robotics navigation and
autonomous driving. Recent research has demonstrated that incorporating goal
guidance significantly enhances prediction accuracy by reducing uncertainty and
leveraging prior knowledge. Most goal-guided approaches decouple the prediction
task into two stages: goal prediction and subsequent trajectory completion
based on the predicted goal, which operate at extreme granularities:
coarse-grained goal prediction forecasts the overall intention, while
fine-grained trajectory completion needs to generate the positions for all
future timesteps. The potential utility of intermediate temporal granularity
remains largely unexplored, which motivates multi-granularity trajectory
modeling. While prior work has shown that multi-granularity representations
capture diverse scales of human dynamics and motion patterns, effectively
integrating this concept into goal-guided frameworks remains challenging. In
this paper, we propose MGTraj, a novel Multi-Granularity goal-guided model for
human Trajectory prediction. MGTraj recursively encodes trajectory proposals
from coarse to fine granularity levels. At each level, a transformer-based
recursive refinement network (RRN) captures features and predicts progressive
refinements. Features across different granularities are integrated using a
weight-sharing strategy, and velocity prediction is employed as an auxiliary
task to further enhance performance. Comprehensive experimental results in
EHT/UCY and Stanford Drone Dataset indicate that MGTraj outperforms baseline
methods and achieves state-of-the-art performance among goal-guided methods.

</details>


### [40] [Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization](https://arxiv.org/abs/2509.09307)
*Zhengzhao Lai,Youbin Zheng,Zhenyang Cai,Haonan Lyu,Jinpu Yang,Hongqing Liang,Yan Hu,Benyou Wang*

Main category: cs.CV

TL;DR: 本文介绍了MatCha，这是首个针对材料表征图像理解的基准，涵盖了1,500个需要专家级知识的问题，以评估多模态大语言模型（MLLM）在该领域的能力，并发现当前主流模型相比人类专家仍存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在材料科学的生成和预测任务取得进展，但其理解真实材料表征影像数据的能力尚未被充分测试。目前缺乏针对这类复杂问题的系统性基准，因此需填补这一评估与开发的空白。

Method: 作者提出了MatCha基准，包含1,500个问题，覆盖材料科学研究的四个关键阶段与21项具体任务。这些问题结合真实材料科学难题，考验模型的综合理解和视觉推理能力。作者利用MatCha评估了多个最先进的MLLM，并对比了其与人类专家的表现。

Result: 实验结果显示，现有MLLM在应对高阶专业问题和复杂视觉推理时表现不佳，明显落后于人类专家。简单的few-shot与chain-of-thought提示未能显著提升模型应对难题的能力。

Conclusion: 目前的MLLM对真实材料表征场景的适应性和理解力有限。MatCha建立了新领域评估标准，有望推动新材料发现和自主科学智能体等研究。

Abstract: Materials characterization is fundamental to acquiring materials information,
revealing the processing-microstructure-property relationships that guide
material design and optimization. While multimodal large language models
(MLLMs) have recently shown promise in generative and predictive tasks within
materials science, their capacity to understand real-world characterization
imaging data remains underexplored. To bridge this gap, we present MatCha, the
first benchmark for materials characterization image understanding, comprising
1,500 questions that demand expert-level domain expertise. MatCha encompasses
four key stages of materials research comprising 21 distinct tasks, each
designed to reflect authentic challenges faced by materials scientists. Our
evaluation of state-of-the-art MLLMs on MatCha reveals a significant
performance gap compared to human experts. These models exhibit degradation
when addressing questions requiring higher-level expertise and sophisticated
visual perception. Simple few-shot and chain-of-thought prompting struggle to
alleviate these limitations. These findings highlight that existing MLLMs still
exhibit limited adaptability to real-world materials characterization
scenarios. We hope MatCha will facilitate future research in areas such as new
material discovery and autonomous scientific agents. MatCha is available at
https://github.com/FreedomIntelligence/MatCha.

</details>


### [41] [Medverse: A Universal Model for Full-Resolution 3D Medical Image Segmentation, Transformation and Enhancement](https://arxiv.org/abs/2509.09232)
*Jiesi Hu,Jianfeng Cao,Yanwu Yang,Chenfei Ye,Yixuan Zhang,Hanyang Peng,Ting Ma*

Main category: cs.CV

TL;DR: Medverse 提出了一种通用型的3D医学影像in-context learning（ICL）模型，能在多任务多器官多模态下优于现有ICL方案，实现高分辨率、具全局解剖学意识的预测。


<details>
  <summary>Details</summary>
Motivation: 当前医学影像ICL模型存在无法同时兼顾高质量预测和全局解剖理解、无法覆盖多任务与多部位、缺乏通用性的问题。因此亟需开发一个通用的ICL模型，能应对多任务、多解剖部位、多模态、多中心的数据需求。

Method: 提出 Medverse，一种适用于3D医学影像的通用ICL模型。其方法包括：1) 在22个涵盖多任务、多部位、多模态的数据集上进行训练；2) 采用 next-scale 自回归 ICL 框架，实现从粗到细的多尺度渐进式预测，增强全局解剖认知和分辨率；3) 创新性地引入区块状跨注意力模块，提升上下文-目标输入的长程交互，同时通过稀疏计算保障效率。

Result: 在覆盖未见过的数据集（新医疗中心、器官、物种和影像模态）上广泛评测，Medverse 显著优于现有ICL基线方法，在多任务上展现通用性和强大性能。

Conclusion: Medverse 树立了医学影像in-context learning新范式，实现多任务多模态影像的泛化能力，对推动医学AI通用模型具有实际意义。代码与模型已开源。

Abstract: In-context learning (ICL) offers a promising paradigm for universal medical
image analysis, enabling models to perform diverse image processing tasks
without retraining. However, current ICL models for medical imaging remain
limited in two critical aspects: they cannot simultaneously achieve
high-fidelity predictions and global anatomical understanding, and there is no
unified model trained across diverse medical imaging tasks (e.g., segmentation
and enhancement) and anatomical regions. As a result, the full potential of ICL
in medical imaging remains underexplored. Thus, we present \textbf{Medverse}, a
universal ICL model for 3D medical imaging, trained on 22 datasets covering
diverse tasks in universal image segmentation, transformation, and enhancement
across multiple organs, imaging modalities, and clinical centers. Medverse
employs a next-scale autoregressive in-context learning framework that
progressively refines predictions from coarse to fine, generating consistent,
full-resolution volumetric outputs and enabling multi-scale anatomical
awareness. We further propose a blockwise cross-attention module that
facilitates long-range interactions between context and target inputs while
preserving computational efficiency through spatial sparsity. Medverse is
extensively evaluated on a broad collection of held-out datasets covering
previously unseen clinical centers, organs, species, and imaging modalities.
Results demonstrate that Medverse substantially outperforms existing ICL
baselines and establishes a novel paradigm for in-context learning. Code and
model weights will be made publicly available. Our model are publicly available
at https://github.com/jiesihu/Medverse.

</details>


### [42] [CoAtNeXt:An Attention-Enhanced ConvNeXtV2-Transformer Hybrid Model for Gastric Tissue Classification](https://arxiv.org/abs/2509.09242)
*Mustafa Yurdakul,Sakir Tasdemir*

Main category: cs.CV

TL;DR: 该论文提出了一种新型混合模型CoAtNeXt，用于胃组织图像的自动分类，在公共数据集上表现优异，超越现有模型，有望提升病理诊断的准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 胃疾病早期诊断对预防严重后果至关重要，但现有的人工病理检查方法劳动强度大、结果可变且易漏诊。因此，亟需自动化、可靠且高效的分析方法。

Method: 该研究提出了基于CoAtNet架构的新模型CoAtNeXt，将原有MBConv层替换为增强的ConvNeXtV2模块，并集成了CBAM注意力模块以提升局部特征提取能力。该模型在两个公共数据集（HMU-GC-HE-30K和GasHisSDB）上与10种CNN和10种ViT模型对比评估。

Result: CoAtNeXt在HMU-GC-HE-30K数据集上取得了96.47%的准确率、96.60%的精度、96.47%的召回率、96.45%的F1分数和99.89%的AUC；在GasHisSDB数据集上也取得了优异表现（98%以上的各项指标），均超越所有对比CNN和ViT模型及文献已有方法。

Conclusion: 实验表明，CoAtNeXt在胃组织图像的病理分类任务上表现出卓越的鲁棒性和准确性，具有协助病理医生提高诊断精度并减轻工作负担的应用潜力。

Abstract: Background and objective Early diagnosis of gastric diseases is crucial to
prevent fatal outcomes. Although histopathologic examination remains the
diagnostic gold standard, it is performed entirely manually, making evaluations
labor-intensive and prone to variability among pathologists. Critical findings
may be missed, and lack of standard procedures reduces consistency. These
limitations highlight the need for automated, reliable, and efficient methods
for gastric tissue analysis. Methods In this study, a novel hybrid model named
CoAtNeXt was proposed for the classification of gastric tissue images. The
model is built upon the CoAtNet architecture by replacing its MBConv layers
with enhanced ConvNeXtV2 blocks. Additionally, the Convolutional Block
Attention Module (CBAM) is integrated to improve local feature extraction
through channel and spatial attention mechanisms. The architecture was scaled
to achieve a balance between computational efficiency and classification
performance. CoAtNeXt was evaluated on two publicly available datasets,
HMU-GC-HE-30K for eight-class classification and GasHisSDB for binary
classification, and was compared against 10 Convolutional Neural Networks
(CNNs) and ten Vision Transformer (ViT) models. Results CoAtNeXt achieved
96.47% accuracy, 96.60% precision, 96.47% recall, 96.45% F1 score, and 99.89%
AUC on HMU-GC-HE-30K. On GasHisSDB, it reached 98.29% accuracy, 98.07%
precision, 98.41% recall, 98.23% F1 score, and 99.90% AUC. It outperformed all
CNN and ViT models tested and surpassed previous studies in the literature.
Conclusion Experimental results show that CoAtNeXt is a robust architecture for
histopathological classification of gastric tissue images, providing
performance on binary and multiclass. Its highlights its potential to assist
pathologists by enhancing diagnostic accuracy and reducing workload.

</details>


### [43] [Towards Better Dental AI: A Multimodal Benchmark and Instruction Dataset for Panoramic X-ray Analysis](https://arxiv.org/abs/2509.09254)
*Jing Hao,Yuxuan Fan,Yanpeng Sun,Kaixin Guo,Lizhuo Lin,Jinrong Yang,Qi Yong H. Ai,Lun M. Wong,Hao Tang,Kuo Feng Hung*

Main category: cs.CV

TL;DR: 本论文提出了MMOral数据集和评测基准，专为口腔全景X光片解释设计，并基于此开发了专用大模型OralGPT，大幅提升了面向口腔影像解释任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉-语言模型（LVLMs）在医学领域已展现出强大能力，但在牙科等专业领域应用尚未充分研究。尤其是口腔全景X光片因解剖结构丰富、病变线索隐晦，存在较大解释难度，而已有的医学基准和指令数据集并未覆盖此场景，亟需专门的数据集和评测体系促进模型发展。

Method: 1）构建了MMOral数据集，包含20,563张标注图像及130万条多任务指令实例，涵盖属性抽取、报告生成、视觉问答和对话等任务。
2）搭建了MMOral-Bench评测套件，从牙科5大诊断维度对模型表现进行全面评估。
3）对现有64个LVLMs在MMOral-Bench上进行表现测试。
4）在Qwen2.5-VL-7B基础上，采用自身数据集进行有监督微调，推出OralGPT模型。

Result: 在MMOral-Bench上，测试的64个LVLMs表现普遍不佳，表现最好的GPT-4o准确率仅41.45%。OralGPT通过仅一次微调训练，准确率提升24.73%，表现远超其他通用LVLMs，显著弥补了专用模型短板。

Conclusion: MMOral数据集与OralGPT模型为智能牙科领域提供了关键基础，有助于推动医疗影像AI在牙科领域的专业化与临床应用发展。相关数据集、模型与评测工具已开放共享。

Abstract: Recent advances in large vision-language models (LVLMs) have demonstrated
strong performance on general-purpose medical tasks. However, their
effectiveness in specialized domains such as dentistry remains underexplored.
In particular, panoramic X-rays, a widely used imaging modality in oral
radiology, pose interpretative challenges due to dense anatomical structures
and subtle pathological cues, which are not captured by existing medical
benchmarks or instruction datasets. To this end, we introduce MMOral, the first
large-scale multimodal instruction dataset and benchmark tailored for panoramic
X-ray interpretation. MMOral consists of 20,563 annotated images paired with
1.3 million instruction-following instances across diverse task types,
including attribute extraction, report generation, visual question answering,
and image-grounded dialogue. In addition, we present MMOral-Bench, a
comprehensive evaluation suite covering five key diagnostic dimensions in
dentistry. We evaluate 64 LVLMs on MMOral-Bench and find that even the
best-performing model, i.e., GPT-4o, only achieves 41.45% accuracy, revealing
significant limitations of current models in this domain. To promote the
progress of this specific domain, we also propose OralGPT, which conducts
supervised fine-tuning (SFT) upon Qwen2.5-VL-7B with our meticulously curated
MMOral instruction dataset. Remarkably, a single epoch of SFT yields
substantial performance enhancements for LVLMs, e.g., OralGPT demonstrates a
24.73% improvement. Both MMOral and OralGPT hold significant potential as a
critical foundation for intelligent dentistry and enable more clinically
impactful multimodal AI systems in the dental field. The dataset, model,
benchmark, and evaluation suite are available at
https://github.com/isbrycee/OralGPT.

</details>


### [44] [FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark](https://arxiv.org/abs/2509.09680)
*Rongyao Fang,Aldrich Yu,Chengqi Duan,Linjiang Huang,Shuai Bai,Yuxuan Cai,Kun Wang,Si Liu,Xihui Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: 本文提出了两个重要资源FLUX-Reason-6M数据集和PRISM-Bench基准，极大推动了开源文生图模型在推理能力方向的发展和评估。


<details>
  <summary>Details</summary>
Motivation: 目前开源文生图模型缺乏大规模、专注推理的数据集和综合性评测基准，导致性能落后于闭源系统。

Method: （1）构建FLUX-Reason-6M数据集，含600万张高质量图片和2000万条中英文描述，针对六类推理特征精心设计，采用生成式链式思维（GCoT）详细刻画生成步骤；（2）推出PRISM-Bench评测基准，设有七大赛道，采用先进视觉-语言模型自动与人类标准对齐评测文图一致性与美学质量。

Result: 对19个主流模型基于PRISM-Bench全面评测，揭示了它们在推理能力等方面存在显著差距，并指出了亟需提升之处。

Conclusion: 本文数据集和评测基准将极大促进社区在推理导向文生图方向的研究和进步，相关资源已全部开源，为行业发展奠定基础。

Abstract: The advancement of open-source text-to-image (T2I) models has been hindered
by the absence of large-scale, reasoning-focused datasets and comprehensive
evaluation benchmarks, resulting in a performance gap compared to leading
closed-source systems. To address this challenge, We introduce FLUX-Reason-6M
and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark).
FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality
FLUX-generated images and 20 million bilingual (English and Chinese)
descriptions specifically designed to teach complex reasoning. The image are
organized according to six key characteristics: Imagination, Entity, Text
rendering, Style, Affection, and Composition, and design explicit Generation
Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation
steps. The whole data curation takes 15,000 A100 GPU days, providing the
community with a resource previously unattainable outside of large industrial
labs. PRISM-Bench offers a novel evaluation standard with seven distinct
tracks, including a formidable Long Text challenge using GCoT. Through
carefully designed prompts, it utilizes advanced vision-language models for
nuanced human-aligned assessment of prompt-image alignment and image
aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench
reveals critical performance gaps and highlights specific areas requiring
improvement. Our dataset, benchmark, and evaluation code are released to
catalyze the next wave of reasoning-oriented T2I generation. Project page:
https://flux-reason-6m.github.io/ .

</details>


### [45] [DATE: Dynamic Absolute Time Enhancement for Long Video Understanding](https://arxiv.org/abs/2509.09263)
*Chao Yuan,Yang Yang,Yehui Yang,Zach Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种名为Dynamic Absolute Time Enhancement (DATE)的新方法，以提升多模态大语言模型（MLLMs）对长视频的时序理解和关键事件定位能力，改进当前模型在时间推理和长距离依赖方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法采用均匀帧采样及隐式位置编码，难以处理长视频中的长期依赖，导致关键信息丢失及时序理解能力下降。为解决这一难题，提升模型在长视频场景下的表现，作者提出了新机制。

Method: 引入时间戳注入机制（TIM），将视频帧嵌入和文本时间戳Token交错，形成连续的时序参考；提出语义引导的时序感知相似性采样（TASS）策略，将视频采样转化为视觉-语言检索任务，采用两阶段算法确保语义相关性和时序覆盖。第一阶段将查询丰富为描述性字幕以更好地对齐视觉特征，第二阶段用基于相似度的时序正则化贪心策略采样关键事件。

Result: 在长达一小时的视频基准测试上，DATE方法在7B和72B多模态大模型上均取得了最先进的性能，特别是其7B模型在某些测试中超越了不少72B模型。

Conclusion: DATE方法显著提升了MLLMs对绝对时间的理解和关键事件的定位能力，在长视频理解任务中取得了领先效果，对大模型视频时序推理具有重要意义。

Abstract: Long video understanding remains a fundamental challenge for multimodal large
language models (MLLMs), particularly in tasks requiring precise temporal
reasoning and event localization. Existing approaches typically adopt uniform
frame sampling and rely on implicit position encodings to model temporal order.
However, these methods struggle with long-range dependencies, leading to
critical information loss and degraded temporal comprehension. In this paper,
we propose Dynamic Absolute Time Enhancement (DATE) that enhances temporal
awareness in MLLMs through the Timestamp Injection Mechanism (TIM) and a
semantically guided Temporal-Aware Similarity Sampling (TASS) strategy.
Specifically, we interleave video frame embeddings with textual timestamp
tokens to construct a continuous temporal reference system. We further
reformulate the video sampling problem as a vision-language retrieval task and
introduce a two-stage algorithm to ensure both semantic relevance and temporal
coverage: enriching each query into a descriptive caption to better align with
the vision feature, and sampling key event with a similarity-driven temporally
regularized greedy strategy. Our method achieves remarkable improvements w.r.t.
absolute time understanding and key event localization, resulting in
state-of-the-art performance among 7B and 72B models on hour-long video
benchmarks. Particularly, our 7B model even exceeds many 72B models on some
benchmarks.

</details>


### [46] [Unified Start, Personalized End: Progressive Pruning for Efficient 3D Medical Image Segmentation](https://arxiv.org/abs/2509.09267)
*Linhao Li,Yiwen Ye,Ziyang Chen,Yong Xia*

Main category: cs.CV

TL;DR: PSP-Seg是一种针对3D医学图像分割的动态、高效剪枝框架，可以显著减少资源消耗同时保持分割性能。


<details>
  <summary>Details</summary>
Motivation: 当前的3D医学图像分割模型耗时且占用大量计算资源，难以灵活适配不同场景，限制了其在临床实际中的部署和应用。

Method: 提出了一种名为PSP-Seg的进阶剪枝方法，从冗余模型出发，通过模块级递归剪枝和功能解耦损失，动态地去除冗余模块以实现高效分割。

Result: 在五个公开数据集上进行评测，与七种最新和六种高效分割模型对比。PSP-Seg轻量版在性能上与nnU-Net相当，但GPU显存降低42-45%，训练时间缩短29-48%，参数量减少83-87%。

Conclusion: PSP-Seg兼具高效与高性能，非常适合大规模临床应用，具有良好的推广潜力。

Abstract: 3D medical image segmentation often faces heavy resource and time
consumption, limiting its scalability and rapid deployment in clinical
environments. Existing efficient segmentation models are typically static and
manually designed prior to training, which restricts their adaptability across
diverse tasks and makes it difficult to balance performance with resource
efficiency. In this paper, we propose PSP-Seg, a progressive pruning framework
that enables dynamic and efficient 3D segmentation. PSP-Seg begins with a
redundant model and iteratively prunes redundant modules through a combination
of block-wise pruning and a functional decoupling loss. We evaluate PSP-Seg on
five public datasets, benchmarking it against seven state-of-the-art models and
six efficient segmentation models. Results demonstrate that the lightweight
variant, PSP-Seg-S, achieves performance on par with nnU-Net while reducing GPU
memory usage by 42-45%, training time by 29-48%, and parameter number by 83-87%
across all datasets. These findings underscore PSP-Seg's potential as a
cost-effective yet high-performing alternative for widespread clinical
application.

</details>


### [47] [Visual Programmability: A Guide for Code-as-Thought in Chart Understanding](https://arxiv.org/abs/2509.09286)
*Bohao Tang,Yan Ma,Fei Zhang,Jiadi Su,Ethan Chern,Zhulin Hu,Zhixin Wang,Pengfei Liu,Ya Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的“代码即思考”（CaT）方法，通过适应性地在符号化代码和直接视觉分析之间选择路径，提升了视觉语言模型（VLM）对图表理解任务的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有图表理解方法依赖外部工具或只专注单一推理策略（如文本链式推理），存在灵活性不足、难以验证中间步骤及事实性强化学习信号难以利用等痛点。作者希望提升VLM对不同复杂度图表推理的灵活适应与准确性。

Method: 提出CaT（Code-as-Thought）方法，将图表视觉信息转化为可验证的符号化代码。在此基础上，提出视觉可编程性（Visual Programmability）概念，通过学习判断每个图表问答任务更适合用代码还是直接视觉分析，并在统一架构下用强化学习训练选择策略，引入数据准确性奖励和决策奖励组成的双奖励系统。

Result: 在多种主流图表理解基准测试上，所提出的自适应双路径推理方法表现出强大且稳健的性能，优于传统只用单一推理路径的模型。

Conclusion: 视觉语言模型不仅能够学会‘推理’，还能学会‘如何推理’——即动态选择最优推理路径，从而在图表理解等多模态推理任务上达到更高效和更准确的表现。

Abstract: Chart understanding presents a critical test to the reasoning capabilities of
Vision-Language Models (VLMs). Prior approaches face critical limitations: some
rely on external tools, making them brittle and constrained by a predefined
toolkit, while others fine-tune specialist models that often adopt a single
reasoning strategy, such as text-based chain-of-thought (CoT). The intermediate
steps of text-based reasoning are difficult to verify, which complicates the
use of reinforcement-learning signals that reward factual accuracy. To address
this, we propose a Code-as-Thought (CaT) approach to represent the visual
information of a chart in a verifiable, symbolic format. Our key insight is
that this strategy must be adaptive: a fixed, code-only implementation
consistently fails on complex charts where symbolic representation is
unsuitable. This finding leads us to introduce Visual Programmability: a
learnable property that determines if a chart-question pair is better solved
with code or direct visual analysis. We implement this concept in an adaptive
framework where a VLM learns to choose between the CaT pathway and a direct
visual reasoning pathway. The selection policy of the model is trained with
reinforcement learning using a novel dual-reward system. This system combines a
data-accuracy reward to ground the model in facts and prevent numerical
hallucination, with a decision reward that teaches the model when to use each
strategy, preventing it from defaulting to a single reasoning mode. Experiments
demonstrate strong and robust performance across diverse chart-understanding
benchmarks. Our work shows that VLMs can be taught not only to reason but also
how to reason, dynamically selecting the optimal reasoning pathway for each
task.

</details>


### [48] [Modality-Agnostic Input Channels Enable Segmentation of Brain lesions in Multimodal MRI with Sequences Unavailable During Training](https://arxiv.org/abs/2509.09290)
*Anthony P. Addison,Felix Wagner,Wentian Xu,Natalie Voets,Konstantinos Kamnitsas*

Main category: cs.CV

TL;DR: 本论文提出了一种改进的U-net分割模型，使其能处理训练期间未见过的MRI模态，从而提升脑部病灶的分割能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态脑MRI分割模型主要局限于已知模态，难以应对新模态，且一些泛化模型会损失模态特有的信息。因此，需要开发能够灵活处理任意可用MRI模态且不牺牲分辨率的信息分割模型。

Method: 在常规U-net架构基础上，增加了模态无关输入通道（或路径），与模态特异通道同时输入信息。通过新的图像增强策略，人工合成不同MRI模态，使模型学习并适应未见过的模态。评估覆盖8个MRI数据库、5种脑部病变类型及8种模态。

Result: 改进后的模型不仅能有效处理训练中见过的MRI模态，还能支持和利用未见过的新模态组合，提高了分割效果与适用性。

Conclusion: 通过对U-net架构的小幅修改及创新增强策略，可让分割模型兼顾老模态和新模态，提升其通用性和实际应用价值。

Abstract: Segmentation models are important tools for the detection and analysis of
lesions in brain MRI. Depending on the type of brain pathology that is imaged,
MRI scanners can acquire multiple, different image modalities (contrasts). Most
segmentation models for multimodal brain MRI are restricted to fixed modalities
and cannot effectively process new ones at inference. Some models generalize to
unseen modalities but may lose discriminative modality-specific information.
This work aims to develop a model that can perform inference on data that
contain image modalities unseen during training, previously seen modalities,
and heterogeneous combinations of both, thus allowing a user to utilize any
available imaging modalities. We demonstrate this is possible with a simple,
thus practical alteration to the U-net architecture, by integrating a
modality-agnostic input channel or pathway, alongside modality-specific input
channels. To train this modality-agnostic component, we develop an image
augmentation scheme that synthesizes artificial MRI modalities. Augmentations
differentially alter the appearance of pathological and healthy brain tissue to
create artificial contrasts between them while maintaining realistic anatomical
integrity. We evaluate the method using 8 MRI databases that include 5 types of
pathologies (stroke, tumours, traumatic brain injury, multiple sclerosis and
white matter hyperintensities) and 8 modalities (T1, T1+contrast, T2, PD, SWI,
DWI, ADC and FLAIR). The results demonstrate that the approach preserves the
ability to effectively process MRI modalities encountered during training,
while being able to process new, unseen modalities to improve its segmentation.
Project code: https://github.com/Anthony-P-Addison/AGN-MOD-SEG

</details>


### [49] [Learning Object-Centric Representations in SAR Images with Multi-Level Feature Fusion](https://arxiv.org/abs/2509.09298)
*Oh-Tae Jang,Min-Gon Cho,Kyung-Tae Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新的面向对象学习（OCL）框架SlotSAR，有效去除了SAR图像中的背景杂波对目标表征的干扰，实现了更清晰的目标识别，不需要掩码注释，且取得了当前最优性能。


<details>
  <summary>Details</summary>
Motivation: SAR图像由于复杂的背景杂波和散斑噪声，常导致模型提取出混杂的信息，影响目标识别的准确性。现有方法难以清晰分离目标与背景信息。

Method: 提出了SlotSAR框架，融合了从SARATR-X提取的高层语义特征和从小波散射网络提取的低层散射特征，通过多层slot attention模块实现这两类特征的有效整合与解耦，无需掩码注释即可分离目标和背景表征。

Result: SlotSAR在SAR图像目标表征任务上取得了比现有OCL方法更优的性能，能更好保留结构细节。

Conclusion: SlotSAR可有效分离SAR图像中的目标与杂波，提升目标识别效果，是当前SAR目标识别领域的先进方法。

Abstract: Synthetic aperture radar (SAR) images contain not only targets of interest
but also complex background clutter, including terrain reflections and speckle
noise. In many cases, such clutter exhibits intensity and patterns that
resemble targets, leading models to extract entangled or spurious features.
Such behavior undermines the ability to form clear target representations,
regardless of the classifier. To address this challenge, we propose a novel
object-centric learning (OCL) framework, named SlotSAR, that disentangles
target representations from background clutter in SAR images without mask
annotations. SlotSAR first extracts high-level semantic features from SARATR-X
and low-level scattering features from the wavelet scattering network in order
to obtain complementary multi-level representations for robust target
characterization. We further present a multi-level slot attention module that
integrates these low- and high-level features to enhance slot-wise
representation distinctiveness, enabling effective OCL. Experimental results
demonstrate that SlotSAR achieves state-of-the-art performance in SAR imagery
by preserving structural details compared to existing OCL methods.

</details>


### [50] [You Share Beliefs, I Adapt: Progressive Heterogeneous Collaborative Perception](https://arxiv.org/abs/2509.09310)
*Hao Si,Ehsan Javanmardi,Manabu Tsukada*

Main category: cs.CV

TL;DR: 本文提出了一种名为PHCP的进阶异构协同感知方法，可在推理时动态适配不同车辆模型，无需联合训练和标注数据，实验性能优异。


<details>
  <summary>Details</summary>
Motivation: 实际场景中，不同车辆因制造商差异导致感知模型异构，现有方法需联合训练或提前存储所有模型，实际应用中难以实施，亟需一种无需联合训练解决异构感知问题的新方法。

Method: 作者提出PHCP框架，将问题表述为小样本无监督领域自适应，通过在推理阶段自监督训练适配器动态对齐特征，无需标签和联合训练即可实现异构协同感知。

Result: 在OPV2V数据集上大量实验结果显示，PHCP在各种异构场景下均实现了优异表现，且仅使用少量无标签数据即可达到与SOTA方法相当的效果。

Conclusion: PHCP方法有效解决了异构车辆协同感知中联合训练的实际障碍，为真实环境下部署具备较强泛化能力的协同感知系统提供了新思路。

Abstract: Collaborative perception enables vehicles to overcome individual perception
limitations by sharing information, allowing them to see further and through
occlusions. In real-world scenarios, models on different vehicles are often
heterogeneous due to manufacturer variations. Existing methods for
heterogeneous collaborative perception address this challenge by fine-tuning
adapters or the entire network to bridge the domain gap. However, these methods
are impractical in real-world applications, as each new collaborator must
undergo joint training with the ego vehicle on a dataset before inference, or
the ego vehicle stores models for all potential collaborators in advance.
Therefore, we pose a new question: Can we tackle this challenge directly during
inference, eliminating the need for joint training? To answer this, we
introduce Progressive Heterogeneous Collaborative Perception (PHCP), a novel
framework that formulates the problem as few-shot unsupervised domain
adaptation. Unlike previous work, PHCP dynamically aligns features by
self-training an adapter during inference, eliminating the need for labeled
data and joint training. Extensive experiments on the OPV2V dataset demonstrate
that PHCP achieves strong performance across diverse heterogeneous scenarios.
Notably, PHCP achieves performance comparable to SOTA methods trained on the
entire dataset while using only a small amount of unlabeled data.

</details>


### [51] [Image Recognition with Vision and Language Embeddings of VLMs](https://arxiv.org/abs/2509.09311)
*Illia Volkov,Nikita Kisel,Klara Janouskova,Jiri Matas*

Main category: cs.CV

TL;DR: 本论文系统评估了多种视觉-语言模型（VLM）在图片分类任务中的语言引导与纯视觉能力，并提出了一种无学习融合方法以提升分类效果。


<details>
  <summary>Details</summary>
Motivation: 现有VLM主要通过图文对齐实现强大的零样本分类能力，但其在纯视觉推理方面的能力尚未得到充分挖掘，因此有必要对其视觉和语言互补性进行深入研究，以优化实际应用表现。

Method: 作者选取了多种双编码器VLM（如SigLIP 2和RADIOv2.5），在ImageNet-1k及其修正标签版本上，分别进行标准、语言引导和纯视觉分类评测，并系统分析了prompt设计、类别多样性、k-NN邻居数和参考集规模等对准确率的影响。最后，提出了一种基于每类精度的简单无学习融合方法。

Result: 实验发现，不同类别在文本提示和视觉相似性上的表现各有优劣，二者表现互补。通过所提无学习融合策略，可进一步提升整体分类准确率。

Conclusion: 视觉与语言引导各自对VLM分类有独特贡献，其互补性可通过简单的融合方法加以利用，显著提升性能，为后续VLM在实际视觉任务中的落地提供了参考。

Abstract: Vision-language models (VLMs) have enabled strong zero-shot classification
through image-text alignment. Yet, their purely visual inference capabilities
remain under-explored. In this work, we conduct a comprehensive evaluation of
both language-guided and vision-only image classification with a diverse set of
dual-encoder VLMs, including both well-established and recent models such as
SigLIP 2 and RADIOv2.5. The performance is compared in a standard setup on the
ImageNet-1k validation set and its label-corrected variant. The key factors
affecting accuracy are analysed, including prompt design, class diversity, the
number of neighbours in k-NN, and reference set size. We show that language and
vision offer complementary strengths, with some classes favouring textual
prompts and others better handled by visual similarity. To exploit this
complementarity, we introduce a simple, learning-free fusion method based on
per-class precision that improves classification performance. The code is
available at: https://github.com/gonikisgo/bmvc2025-vlm-image-recognition.

</details>


### [52] [Fine-Grained Customized Fashion Design with Image-into-Prompt benchmark and dataset from LMM](https://arxiv.org/abs/2509.09324)
*Hui Li,Yi You,Qiqi Chen,Bingfeng Zhang,George Q. Huang*

Main category: cs.CV

TL;DR: 本文提出了一种名为BUG（Better Understanding Generation）的新型服装设计工作流框架，借助多模态大模型（LMM），提升了服装设计的自动化和定制化能力，并通过新数据集和多项指标验证了效果。


<details>
  <summary>Details</summary>
Motivation: 现有生成式AI在服装行业的设计流程中虽能实现高效设计生成，但对终端用户而言，缺乏专业知识时依然难以实现细致的个性化定制。解决这一难题，成为自动化智能设计发展的关键需求。

Method: 提出了BUG工作流结合LMM，支持用户以对话加图片输入的方式，自动生成并细粒度定制服装设计，实现了将用户的创意高效、专业地转化为成衣设计。此外，构建了FashionEdit新数据集，模拟真实服装设计流程，用以量化评测生成模型的表现。

Result: 通过生成相似度、用户满意度以及生成质量等多维度实验，BUG工作流在衣物自动生成与定制的精准性和用户体验上，都显示出优越性。并公开相关代码与数据集。

Conclusion: BUG工作流架构显著降低了服装设计的门槛，释放了用户的创造力，提升了自动化与定制化能力。该方法对行业内提升AI设计效率和质量具有重要意义，为非专业用户带来了更多便利。

Abstract: Generative AI evolves the execution of complex workflows in industry, where
the large multimodal model empowers fashion design in the garment industry.
Current generation AI models magically transform brainstorming into fancy
designs easily, but the fine-grained customization still suffers from text
uncertainty without professional background knowledge from end-users. Thus, we
propose the Better Understanding Generation (BUG) workflow with LMM to
automatically create and fine-grain customize the cloth designs from chat with
image-into-prompt. Our framework unleashes users' creative potential beyond
words and also lowers the barriers of clothing design/editing without further
human involvement. To prove the effectiveness of our model, we propose a new
FashionEdit dataset that simulates the real-world clothing design workflow,
evaluated from generation similarity, user satisfaction, and quality. The code
and dataset: https://github.com/detectiveli/FashionEdit.

</details>


### [53] [Exploring Pre-training Across Domains for Few-Shot Surgical Skill Assessment](https://arxiv.org/abs/2509.09327)
*Dimitrios Anastasiou,Razvan Caramalau,Nazir Sirajudeen,Matthew Boal,Philip Edwards,Justin Collins,John Kelly,Ashwin Sridhar,Maxine Tran,Faiz Mumtaz,Nevil Pavithran,Nader Francis,Danail Stoyanov,Evangelos B. Mazomenos*

Main category: cs.CV

TL;DR: 本文提出了自监督预训练结合小样本学习的方法，在自动化外科技能评估任务中，尤其是在标注稀缺的条件下，有效提升了模型的下游性能。研究发现，与大规模但领域无关的数据相比，小规模且与任务高度相关的数据预训练表现更佳。


<details>
  <summary>Details</summary>
Motivation: 自动外科技能评估需要耗费专家时间和精力进行标注，数据稀缺严重制约了模型开发，而如何在极少标注下获得有效评估模型成为亟需解决的问题。

Method: 将外科技能评估任务(Skill Assessment，SSA)表述为小样本学习问题。采用自监督方式对模型进行预训练，并深入分析了预训练数据的领域相关性（如是否为手术流程相关）对后续小样本学习任务的迁移效果。通过在公开手术机器人数据集上辅以新标注的OSATS评分，评估不同预训练策略、数据来源与领域相似性对三种小样本情境（1-shot, 2-shot, 5-shot）下的效果。

Result: 小而领域相关的数据集预训练优于大规模但领域关联性差的数据。在1、2、5-shot下获得60.16%、66.03%、73.65%的准确率。引入手术特定的数据，结合领域相关的数据源作为预训练，可带来1.22%的准确率和2.28%的F1-score提升；相反，若预训练数据与目标领域不相关且规模大，反而会导致性能下降。

Conclusion: 在外科技能评估中，预训练的数据与目标任务领域的相关性比数据规模更关键。有效结合领域相关的小样本与自监督预训练，能够在极少标注情况下获得强大的识别能力，有助于外科计算机视觉的实际应用。

Abstract: Automated surgical skill assessment (SSA) is a central task in surgical
computer vision. Developing robust SSA models is challenging due to the
scarcity of skill annotations, which are time-consuming to produce and require
expert consensus. Few-shot learning (FSL) offers a scalable alternative
enabling model development with minimal supervision, though its success
critically depends on effective pre-training. While widely studied for several
surgical downstream tasks, pre-training has remained largely unexplored in SSA.
In this work, we formulate SSA as a few-shot task and investigate how
self-supervised pre-training strategies affect downstream few-shot SSA
performance. We annotate a publicly available robotic surgery dataset with
Objective Structured Assessment of Technical Skill (OSATS) scores, and evaluate
various pre-training sources across three few-shot settings. We quantify domain
similarity and analyze how domain gap and the inclusion of procedure-specific
data into pre-training influence transferability. Our results show that small
but domain-relevant datasets can outperform large scale, less aligned ones,
achieving accuracies of 60.16%, 66.03%, and 73.65% in the 1-, 2-, and 5-shot
settings, respectively. Moreover, incorporating procedure-specific data into
pre-training with a domain-relevant external dataset significantly boosts
downstream performance, with an average gain of +1.22% in accuracy and +2.28%
in F1-score; however, applying the same strategy with less similar but
large-scale sources can instead lead to performance degradation. Code and
models are available at https://github.com/anastadimi/ssa-fsl.

</details>


### [54] [Texture-aware Intrinsic Image Decomposition with Model- and Learning-based Priors](https://arxiv.org/abs/2509.09352)
*Xiaodong Wang,Zijun He,Xin Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，能在复杂场景（如空间变化的光照与丰富纹理）下，从单张图像分离反射率层和明暗层，实现高质量的内在图像分解。


<details>
  <summary>Details</summary>
Motivation: 传统的内在图像分解方法在面对复杂场景（如严重光照变化和丰富的材质纹理）时表现较差。现有基于学习的方法往往过度平滑、缺乏纹理信息，难以精细地分离材质和光照成分。

Method: 作者发现现有深度学习方法的内在图像过于平滑且缺乏纹理，根据这一观察，设计了基于纹理引导的正则化项，将分解任务建模为优化问题，尤其突出材质纹理和光照成分的分离。

Result: 所提方法能够有效处理现实图片中的复杂光照与纹理，分离出的内在图像（反射率和明暗层）质量优于现有方法。

Conclusion: 引入纹理感知先验和优化框架，有效提升了复杂场景中内在图像分解的表现，为实际应用提供了更高质量的反射率和明暗分离结果。

Abstract: This paper aims to recover the intrinsic reflectance layer and shading layer
given a single image. Though this intrinsic image decomposition problem has
been studied for decades, it remains a significant challenge in cases of
complex scenes, i.e. spatially-varying lighting effect and rich textures. In
this paper, we propose a novel method for handling severe lighting and rich
textures in intrinsic image decomposition, which enables to produce
high-quality intrinsic images for real-world images. Specifically, we observe
that previous learning-based methods tend to produce texture-less and
over-smoothing intrinsic images, which can be used to infer the lighting and
texture information given a RGB image. In this way, we design a texture-guided
regularization term and formulate the decomposition problem into an
optimization framework, to separate the material textures and lighting effect.
We demonstrate that combining the novel texture-aware prior can produce
superior results to existing approaches.

</details>


### [55] [Plug-and-play Diffusion Models for Image Compressive Sensing with Data Consistency Projection](https://arxiv.org/abs/2509.09365)
*Xiaodong Wang,Ping Wang,Zhangyuan Li,Xin Yuan*

Main category: cs.CV

TL;DR: 本文探讨了Plug-and-Play (PnP) 方法和去噪扩散隐式模型（DDIM）在求解病态逆问题（尤其是单像素成像）上的联系，并提出了一种混合数据一致性模块，提升了成像重建效果。


<details>
  <summary>Details</summary>
Motivation: 逆问题（如单像素成像）病态且难以重建，现有PnP和扩散模型各有优劣，作者希望将两者优点结合，提升重建质量。

Method: 作者首先分析PnP和DDIM之间的关键不同点，并将扩散过程解耦为去噪、数据一致性、采样三个阶段。基于此，提出一种混合数据一致性模块，将多个PnP式保真项线性组合，直接对去噪结果进行修正，同时不影响扩散采样路径。

Result: 在单像素成像实验中，新方法取得了更好的重建质量。

Conclusion: 通过结合PnP和扩散模型方法，提出的统一框架和混合修正模块有效提升了逆问题（如单像素成像）的重建效果。

Abstract: We explore the connection between Plug-and-Play (PnP) methods and Denoising
Diffusion Implicit Models (DDIM) for solving ill-posed inverse problems, with a
focus on single-pixel imaging. We begin by identifying key distinctions between
PnP and diffusion models-particularly in their denoising mechanisms and
sampling procedures. By decoupling the diffusion process into three
interpretable stages: denoising, data consistency enforcement, and sampling, we
provide a unified framework that integrates learned priors with physical
forward models in a principled manner. Building upon this insight, we propose a
hybrid data-consistency module that linearly combines multiple PnP-style
fidelity terms. This hybrid correction is applied directly to the denoised
estimate, improving measurement consistency without disrupting the diffusion
sampling trajectory. Experimental results on single-pixel imaging tasks
demonstrate that our method achieves better reconstruction quality.

</details>


### [56] [A Fully Automatic Framework for Intracranial Pressure Grading: Integrating Keyframe Identification, ONSD Measurement and Clinical Data](https://arxiv.org/abs/2509.09368)
*Pengxu Wen,Tingting Yu,Ziwei Nie,Cheng Jiang,Zhenyu Yin,Mingyang He,Bo Liao,Xiaoping Yang*

Main category: cs.CV

TL;DR: 该论文提出了一种全自动两阶段方法，通过眼底超声视频分析和融合临床数据，实现了非侵入性颅内压（ICP）分级评估，准确性明显优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统的颅内压测量方法如腰穿虽为金标准，但具有侵入性和一定风险；目前临床上利用视神经鞘直径（ONSD）作为ICP无创监测指标，但人工操作主观性强、结果易变，需开发更可靠的自动化无创评估方法。

Method: 提出了全自动两阶段框架：首先在超声视频逐帧进行解剖分割、基于规则和国际共识识别关键帧并精确测量ONSD；其次将ONSD与临床特征融合，预测ICP等级，实现可解释的超声分析与多源数据整合。

Result: 所提方法在五折交叉验证中验证准确率为0.845±0.071，独立测试准确率为0.786，均明显高于传统阈值法（验证0.637±0.111，测试0.429）。

Conclusion: 该全自动框架有效降低了人工操作的主观性与变异性，整合多源信息，实现了可靠的无创颅内压评估，有望改善急性神经疾病患者的管理。

Abstract: Intracranial pressure (ICP) elevation poses severe threats to cerebral
function, thus necessitating monitoring for timely intervention. While lumbar
puncture is the gold standard for ICP measurement, its invasiveness and
associated risks drive the need for non-invasive alternatives. Optic nerve
sheath diameter (ONSD) has emerged as a promising biomarker, as elevated ICP
directly correlates with increased ONSD. However, current clinical practices
for ONSD measurement suffer from inconsistency in manual operation,
subjectivity in optimal view selection, and variability in thresholding,
limiting their reliability. To address these challenges, we introduce a fully
automatic two-stage framework for ICP grading, integrating keyframe
identification, ONSD measurement and clinical data. Specifically, the fundus
ultrasound video processing stage performs frame-level anatomical segmentation,
rule-based keyframe identification guided by an international consensus
statement, and precise ONSD measurement. The intracranial pressure grading
stage then fuses ONSD metrics with clinical features to enable the prediction
of ICP grades, thereby demonstrating an innovative blend of interpretable
ultrasound analysis and multi-source data integration for objective clinical
evaluation. Experimental results demonstrate that our method achieves a
validation accuracy of $0.845 \pm 0.071$ (with standard deviation from
five-fold cross-validation) and an independent test accuracy of 0.786,
significantly outperforming conventional threshold-based method ($0.637 \pm
0.111$ validation accuracy, $0.429$ test accuracy). Through effectively
reducing operator variability and integrating multi-source information, our
framework establishes a reliable non-invasive approach for clinical ICP
evaluation, holding promise for improving patient management in acute
neurological conditions.

</details>


### [57] [Unsupervised Integrated-Circuit Defect Segmentation via Image-Intrinsic Normality](https://arxiv.org/abs/2509.09375)
*Botong Zhao,Qijun Shi,Shujing Lyu,Yue Lu*

Main category: cs.CV

TL;DR: 本论文提出了一种无需外部正常样本支持的集成电路(IC)缺陷分割无监督方法，通过从测试图像内提取代表正常区域的特征，实现更鲁棒的缺陷检测。


<details>
  <summary>Details</summary>
Motivation: 当前工业缺陷分割方法依赖外部正常图像库，由于IC版图变化大、配准困难，导致实际应用中效果不佳。作者发现IC缺陷多为局部异常，且每张图像内部有重复的正常结构，因此有望利用单张图像自身进行无监督分割。

Method: 设计了一种可学习的正常信息提取器，从测试图像内部聚合正常区域特征，通过一致性损失指导这些特征聚焦正常区域，然后利用特征引导的解码器只重建正常内容，重建残差用于缺陷分割。同时用伪异常增强方式提升训练稳定性。

Result: 在三个IC制程阶段的数据集上，该方法在分割效果和对产品多样性的鲁棒性上均优于现有方法。

Conclusion: 论文证明了无需外部支持、单图像内部特征自提取的无监督IC缺陷分割方法具有更强的泛化性和适应性，为工业检测提供了更实用的解决方案。

Abstract: Modern Integrated-Circuit(IC) manufacturing introduces diverse, fine-grained
defects that depress yield and reliability. Most industrial defect segmentation
compares a test image against an external normal set, a strategy that is
brittle for IC imagery where layouts vary across products and accurate
alignment is difficult. We observe that defects are predominantly local, while
each image still contains rich, repeatable normal patterns. We therefore
propose an unsupervised IC defect segmentation framework that requires no
external normal support. A learnable normal-information extractor aggregates
representative normal features from the test image, and a coherence loss
enforces their association with normal regions. Guided by these features, a
decoder reconstructs only normal content; the reconstruction residual then
segments defects. Pseudo-anomaly augmentation further stabilizes training.
Experiments on datasets from three IC process stages show consistent
improvements over existing approaches and strong robustness to product
variability.

</details>


### [58] [Decoupling Clinical and Class-Agnostic Features for Reliable Few-Shot Adaptation under Shift](https://arxiv.org/abs/2509.09397)
*Umaima Rahman,Raza Imam,Mohammad Yaqub,Dwarikanath Mahapatra*

Main category: cs.CV

TL;DR: 本文提出了一种新的结构化特征解耦方法DRiFt，能提高医学视觉-语言模型在分布变化下的泛化和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前医学视觉-语言模型在实际临床应用时存在对分布变化敏感、泛化能力弱等问题，主要因为模型容易捕捉到无关和噪声特征，无法准确区分临床相关信号。提升模型在新场景下的可靠性和安全性，是推动其临床部署的关键。

Method: 作者提出DRiFt框架，采用参数高效的LoRA微调方式和可学习的提示(token)机制，有针对性地分离有效临床信号和无关噪声。此外，作者还通过生成高质量的图片-文本配对，以提升多模态特征对齐和降低不确定性。

Result: 在医学多模态任务中，DRiFt方法相比传统基于提示的方法，Top-1准确率提升11.4%，Macro-F1提升3.3%，在未见数据集（分布外）上也表现出色。消融实验表明，特征解耦和精准对齐对提升泛化能力非常关键。

Conclusion: DRiFt显著增强了医学视觉-语言模型的泛化和鲁棒性，为临床安全应用打造更可信赖的AI工具。

Abstract: Medical vision-language models (VLMs) offer promise for clinical decision
support, yet their reliability under distribution shifts remains a major
concern for safe deployment. These models often learn task-agnostic
correlations due to variability in imaging protocols and free-text reports,
limiting their generalizability and increasing the risk of failure in
real-world settings. We propose DRiFt, a structured feature decoupling
framework that explicitly separates clinically relevant signals from
task-agnostic noise using parameter-efficient tuning (LoRA) and learnable
prompt tokens. To enhance cross-modal alignment and reduce uncertainty, we
curate high-quality, clinically grounded image-text pairs by generating
captions for a diverse medical dataset. Our approach improves in-distribution
performance by +11.4% Top-1 accuracy and +3.3% Macro-F1 over prior prompt-based
methods, while maintaining strong robustness across unseen datasets. Ablation
studies reveal that disentangling task-relevant features and careful alignment
significantly enhance model generalization and reduce unpredictable behavior
under domain shift. These insights contribute toward building safer, more
trustworthy VLMs for clinical use. The code is available at
https://github.com/rumaima/DRiFt.

</details>


### [59] [FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal image fusion and super-resolution](https://arxiv.org/abs/2509.09427)
*Yuchan Jie,Yushen Xu,Xiaosong Li,Fuqiang Zhou,Jianming Lv,Huafeng Li*

Main category: cs.CV

TL;DR: FS-Diff 是一种结合语义引导与清晰度感知、用于图像融合和超分辨率的联合生成模型。它在多个公开及自建数据集上显著优于现有方法，可生成高分辨率且语义丰富的融合图像。


<details>
  <summary>Details</summary>
Motivation: 现有图像融合和超分辨率方法在实际如军用侦查等应用场景中表现不佳，因多源模态图像存在分辨率低、语义弱等问题，现有技术难以充分利用多模态互补信息。

Method: 提出FS-Diff，利用语义引导的清晰度感知机制，将目标输出初始化为高斯噪声，通过引入双向特征Mamba提取全局特征，并基于源图与语义条件进行迭代去噪，采用改进U-Net网络在多噪声级别下训练，实现端到端的融合与超分辨率。

Result: 在六个公开数据集和作者提出的AVMS数据集上实验，FS-Diff 对比当前主流方法，在多放大倍率下均表现更优，能够恢复更丰富的细节与语义信息。

Conclusion: FS-Diff 有效提升了多模态融合与超分辨率的表现，尤其适用于目标与背景受损或高分辨率需求场景，展现出良好的实际应用前景。

Abstract: As an influential information fusion and low-level vision technique, image
fusion integrates complementary information from source images to yield an
informative fused image. A few attempts have been made in recent years to
jointly realize image fusion and super-resolution. However, in real-world
applications such as military reconnaissance and long-range detection missions,
the target and background structures in multimodal images are easily corrupted,
with low resolution and weak semantic information, which leads to suboptimal
results in current fusion techniques. In response, we propose FS-Diff, a
semantic guidance and clarity-aware joint image fusion and super-resolution
method. FS-Diff unifies image fusion and super-resolution as a conditional
generation problem. It leverages semantic guidance from the proposed clarity
sensing mechanism for adaptive low-resolution perception and cross-modal
feature extraction. Specifically, we initialize the desired fused result as
pure Gaussian noise and introduce the bidirectional feature Mamba to extract
the global features of the multimodal images. Moreover, utilizing the source
images and semantics as conditions, we implement a random iterative denoising
process via a modified U-Net network. This network istrained for denoising at
multiple noise levels to produce high-resolution fusion results with
cross-modal features and abundant semantic information. We also construct a
powerful aerial view multiscene (AVMS) benchmark covering 600 pairs of images.
Extensive joint image fusion and super-resolution experiments on six public and
our AVMS datasets demonstrated that FS-Diff outperforms the state-of-the-art
methods at multiple magnifications and can recover richer details and semantics
in the fused images. The code is available at
https://github.com/XylonXu01/FS-Diff.

</details>


### [60] [Semantic Concentration for Self-Supervised Dense Representations Learning](https://arxiv.org/abs/2509.09429)
*Peisong Wen,Qianqian Xu,Siran Dai,Runmin Cong,Qingming Huang*

Main category: cs.CV

TL;DR: 本文系统分析了自监督学习在密集任务上的难点，提出了显式语义聚合的方法，通过去除严格空间对齐和引入对象感知过滤，提高了分布式补丁特征的质量，并通过实验证实其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有图像级自监督学习在学习补丁级（patch-level）密集特征上存在困难，因为同一实例或类别的补丁特征过度分散，影响下游密集预测任务的表现。作者发现主流方法中空间对齐和语义聚合是关键问题，因此欲解决密集自监督中的过度分散问题。

Method: 作者提出三方面改进：（1）为打破对补丁的严格空间对齐，引入补丁对应关系蒸馏，并用对噪声鲁棒的排序损失（扩展自AP loss，并适用连续目标）缓解伪标签噪声问题；（2）为区分复杂场景中的共享模式，引入对象感知过滤器，通过交叉注意力将补丁映射到对象原型空间；（3）在多个任务上的实验证明了方法有效。

Result: 实验证明，所提出方法在多种下游密集任务上表现优越，解决了补丁级自监督表示中的过度分散难题。相关代码已开源。

Conclusion: 本文针对密集任务自监督学习中的过度分散问题，提出了显式语义聚合的创新方法。不仅理论分析充分，且实验结果显著，具有潜在应用价值。

Abstract: Recent advances in image-level self-supervised learning (SSL) have made
significant progress, yet learning dense representations for patches remains
challenging. Mainstream methods encounter an over-dispersion phenomenon that
patches from the same instance/category scatter, harming downstream performance
on dense tasks. This work reveals that image-level SSL avoids over-dispersion
by involving implicit semantic concentration. Specifically, the non-strict
spatial alignment ensures intra-instance consistency, while shared patterns,
i.e., similar parts of within-class instances in the input space, ensure
inter-image consistency. Unfortunately, these approaches are infeasible for
dense SSL due to their spatial sensitivity and complicated scene-centric data.
These observations motivate us to explore explicit semantic concentration for
dense SSL. First, to break the strict spatial alignment, we propose to distill
the patch correspondences. Facing noisy and imbalanced pseudo labels, we
propose a noise-tolerant ranking loss. The core idea is extending the Average
Precision (AP) loss to continuous targets, such that its decision-agnostic and
adaptive focusing properties prevent the student model from being misled.
Second, to discriminate the shared patterns from complicated scenes, we propose
the object-aware filter to map the output space to an object-based space.
Specifically, patches are represented by learnable prototypes of objects via
cross-attention. Last but not least, empirical studies across various tasks
soundly support the effectiveness of our method. Code is available in
https://github.com/KID-7391/CoTAP.

</details>


### [61] [FlexiD-Fuse: Flexible number of inputs multi-modal medical image fusion based on diffusion model](https://arxiv.org/abs/2509.09456)
*Yushen Xu,Xiaosong Li,Yuchun Wang,Xiaoqi Cheng,Huafeng Li,Haishu Tan*

Main category: cs.CV

TL;DR: 本文提出了一种新型的基于扩散模型的多模态医学图像融合方法FlexiD-Fuse，能够灵活处理不同数量的输入模态，实现高质量的图像融合。实验结果显示该方法优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态医学图像融合方法通常仅能接受固定数量（如仅两模态或三模态）的输入，无法直接处理输入数量变化，限制了其在真实临床环境的应用。因此亟需一种能够灵活处理不同输入模态数量的图像融合方法。

Method: 提出FlexiD-Fuse网络，基于扩散过程和层次贝叶斯建模，将扩散融合问题转化为最大似然估计问题。引入期望最大化（EM）算法到扩散采样过程中，从而支持不同数量输入模态下的端到端图像融合。

Result: 在Harvard数据集上与最新的两模态和三模态医学图像融合方法对比，通过九项主流评价指标进行评测，FlexiD-Fuse在多种输入情况下均取得最佳性能。还在红外-可见、多曝光、多焦点任务上与SOTA方法进行扩展实验，效果依然优异。

Conclusion: FlexiD-Fuse能够突破输入模态数量的限制，实现任意模态数量下的高质量医学图像融合，具有良好的普适性和优越性能，对临床实际应用有重要意义。

Abstract: Different modalities of medical images provide unique physiological and
anatomical information for diseases. Multi-modal medical image fusion
integrates useful information from different complementary medical images with
different modalities, producing a fused image that comprehensively and
objectively reflects lesion characteristics to assist doctors in clinical
diagnosis. However, existing fusion methods can only handle a fixed number of
modality inputs, such as accepting only two-modal or tri-modal inputs, and
cannot directly process varying input quantities, which hinders their
application in clinical settings. To tackle this issue, we introduce
FlexiD-Fuse, a diffusion-based image fusion network designed to accommodate
flexible quantities of input modalities. It can end-to-end process two-modal
and tri-modal medical image fusion under the same weight. FlexiD-Fuse
transforms the diffusion fusion problem, which supports only fixed-condition
inputs, into a maximum likelihood estimation problem based on the diffusion
process and hierarchical Bayesian modeling. By incorporating the
Expectation-Maximization algorithm into the diffusion sampling iteration
process, FlexiD-Fuse can generate high-quality fused images with cross-modal
information from source images, independently of the number of input images. We
compared the latest two and tri-modal medical image fusion methods, tested them
on Harvard datasets, and evaluated them using nine popular metrics. The
experimental results show that our method achieves the best performance in
medical image fusion with varying inputs. Meanwhile, we conducted extensive
extension experiments on infrared-visible, multi-exposure, and multi-focus
image fusion tasks with arbitrary numbers, and compared them with the
perspective SOTA methods. The results of the extension experiments consistently
demonstrate the effectiveness and superiority of our method.

</details>


### [62] [Resource-Efficient Glioma Segmentation on Sub-Saharan MRI](https://arxiv.org/abs/2509.09469)
*Freedmore Sidume,Oumayma Soula,Joseph Muthui Wacira,YunFei Zhu,Abbas Rabiu Muhammad,Abderrazek Zeraii,Oluwaseun Kalejaye,Hajer Ibrahim,Olfa Gaddour,Brain Halubanza,Dong Zhang,Udunna C Anazodo,Confidence Raymond*

Main category: cs.CV

TL;DR: 该论文提出了一种适用于非洲撒哈拉以南资源有限地区的高效脑胶质瘤MRI分割模型，并在BraTS-Africa数据集上验证了其鲁棒性和实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 当前脑胶质瘤MRI分割模型需要大量高质量标注数据，但在撒哈拉以南非洲数据匮乏，限制了这些模型的临床应用。因此，亟需开发能够在有限数据与低硬件资源下依然可靠的医学影像分割方法。

Method: 本文设计了一种结合3D Attention UNet、残差模块及迁移学习的新模型，利用BraTS 2021预训练权重提升分割效果，并专门针对低资源环境优化模型体积和推理速度。

Result: 在95例BraTS-Africa MRI样本上，模型在不同肿瘤区域分别获得了0.76、0.80及0.85的Dice分数，展示了在数据质量和数量受限情况下的良好表现。

Conclusion: 该方法不仅推广性好，且模型小巧推理快，适于撒哈拉以南非洲等资源受限医疗场景，有助于推动全球健康领域医疗AI公平化进程。

Abstract: Gliomas are the most prevalent type of primary brain tumors, and their
accurate segmentation from MRI is critical for diagnosis, treatment planning,
and longitudinal monitoring. However, the scarcity of high-quality annotated
imaging data in Sub-Saharan Africa (SSA) poses a significant challenge for
deploying advanced segmentation models in clinical workflows. This study
introduces a robust and computationally efficient deep learning framework
tailored for resource-constrained settings. We leveraged a 3D Attention UNet
architecture augmented with residual blocks and enhanced through transfer
learning from pre-trained weights on the BraTS 2021 dataset. Our model was
evaluated on 95 MRI cases from the BraTS-Africa dataset, a benchmark for glioma
segmentation in SSA MRI data. Despite the limited data quality and quantity,
our approach achieved Dice scores of 0.76 for the Enhancing Tumor (ET), 0.80
for Necrotic and Non-Enhancing Tumor Core (NETC), and 0.85 for Surrounding
Non-Functional Hemisphere (SNFH). These results demonstrate the
generalizability of the proposed model and its potential to support clinical
decision making in low-resource settings. The compact architecture,
approximately 90 MB, and sub-minute per-volume inference time on consumer-grade
hardware further underscore its practicality for deployment in SSA health
systems. This work contributes toward closing the gap in equitable AI for
global health by empowering underserved regions with high-performing and
accessible medical imaging solutions.

</details>


### [63] [OpenFake: An Open Dataset and Platform Toward Large-Scale Deepfake Detection](https://arxiv.org/abs/2509.09495)
*Victor Livernoche,Akshatha Arodi,Andreea Musulan,Zachary Yang,Adam Salvail,Gaétan Marceau Caron,Jean-François Godbout,Reihaneh Rabbany*

Main category: cs.CV

TL;DR: 本论文针对现有深度伪造检测数据集的局限，提出并发布了一个面向政治领域、涵盖最新生成模型的多模态大规模数据集，并开创性地引入众包对抗平台，持续提升检测方法的鲁棒性和适应性。


<details>
  <summary>Details</summary>
Motivation: 深度伪造内容特别是在政治语境中加剧了虚假信息传播，而现有检测数据集往往过于陈旧、真实度低且场景单一，无法有效支持对最新合成图像的检测与研究，因此亟需一个更全面、更具代表性的数据集和动态更新机制。

Method: 1. 通过社交媒体分析，确定深度伪造传播虚假信息的多种模态。2. 进行人类感知实验，评估新一代合成模型生成图像的迷惑性。3. 制作了包含300万真实配图和96.3万高质量合成图的大规模数据集，涵盖多种生成模型。4. 搭建众包对抗平台，激励社区生成和提交更具挑战性的伪造图像，实现数据集和检测方法动态进化。

Result: 实验证明，最新专有AI模型生成的图像在视觉上已难以被普通大众区分；同时，所构建的数据集在检测任务中更具挑战性和代表性。众包平台则有效补充了新的伪造样本。

Conclusion: 本文提出的政治领域综合性数据集与众包对抗平台，有效推动了深度伪造检测的研究进步，并为应对生成模型不断演进提供了可持续、动态的数据支撑，从而有助于保护公共舆论环境安全。

Abstract: Deepfakes, synthetic media created using advanced AI techniques, have
intensified the spread of misinformation, particularly in politically sensitive
contexts. Existing deepfake detection datasets are often limited, relying on
outdated generation methods, low realism, or single-face imagery, restricting
the effectiveness for general synthetic image detection. By analyzing social
media posts, we identify multiple modalities through which deepfakes propagate
misinformation. Furthermore, our human perception study demonstrates that
recently developed proprietary models produce synthetic images increasingly
indistinguishable from real ones, complicating accurate identification by the
general public. Consequently, we present a comprehensive, politically-focused
dataset specifically crafted for benchmarking detection against modern
generative models. This dataset contains three million real images paired with
descriptive captions, which are used for generating 963k corresponding
high-quality synthetic images from a mix of proprietary and open-source models.
Recognizing the continual evolution of generative techniques, we introduce an
innovative crowdsourced adversarial platform, where participants are
incentivized to generate and submit challenging synthetic images. This ongoing
community-driven initiative ensures that deepfake detection methods remain
robust and adaptive, proactively safeguarding public discourse from
sophisticated misinformation threats.

</details>


### [64] [Improving Human Motion Plausibility with Body Momentum](https://arxiv.org/abs/2509.09496)
*Ha Linh Nguyen,Tze Ho Elden Tse,Angela Yao*

Main category: cs.CV

TL;DR: 本文提出用整体动量作为约束，将局部动作和全局运动物理性地关联起来，并以此改进运动预测模型，提高动作平滑性、减少脚步滑移。


<details>
  <summary>Details</summary>
Motivation: 现有研究常将人体动作分解为‘局部动作’和‘全局运动’并分别处理，忽略了两者之间的物理耦合关系，导致动作预测中出现平滑度差、脚步滑移等问题。而准确从关节力矩和外力推导全局轨迹又极为复杂和耗算。

Method: 论文以全身线性和角动量作为约束，将‘局部关节行为’与‘整体运动’关联起来，并提出动量一致性损失函数，将生成的动量分布与真实数据中观测到的动量分布进行匹配，从而物理性约束模型输出。

Result: 引入动量一致性损失后，动作预测模型显著减少了脚步滑移和抖动，提高了动作平衡性，同时保持了高重建精度。

Conclusion: 使用整体动量约束为人体动作建模提供了物理一致性的新视角和途径，可有效提升动作生成质量。

Abstract: Many studies decompose human motion into local motion in a frame attached to
the root joint and global motion of the root joint in the world frame, treating
them separately. However, these two components are not independent. Global
movement arises from interactions with the environment, which are, in turn,
driven by changes in the body configuration. Motion models often fail to
precisely capture this physical coupling between local and global dynamics,
while deriving global trajectories from joint torques and external forces is
computationally expensive and complex. To address these challenges, we propose
using whole-body linear and angular momentum as a constraint to link local
motion with global movement. Since momentum reflects the aggregate effect of
joint-level dynamics on the body's movement through space, it provides a
physically grounded way to relate local joint behavior to global displacement.
Building on this insight, we introduce a new loss term that enforces
consistency between the generated momentum profiles and those observed in
ground-truth data. Incorporating our loss reduces foot sliding and jitter,
improves balance, and preserves the accuracy of the recovered motion. Code and
data are available at the project page https://hlinhn.github.io/momentum_bmvc.

</details>


### [65] [Region-Wise Correspondence Prediction between Manga Line Art Images](https://arxiv.org/abs/2509.09501)
*Yingxuan Li,Jiafeng Mao,Qianru Qiu,Yusuke Matsui*

Main category: cs.CV

TL;DR: 本文提出了一种无需预先标签或分割信息，直接对原始漫画线稿图像进行区域对应预测的新任务，并设计了基于Transformer的框架，通过补丁级相似性学习实现跨图像区域对应，相关方法在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 区域对应是漫画处理的基础任务，对自动上色、过渡帧生成等应用十分关键，但目前几乎无人研究如何在没有分割或标注的现实场景下解决这一问题。

Method: 作者将每张漫画线稿图划分为若干小补丁，利用基于Transformer的神经网络框架学习同一图像及不同图像之间补丁的相似性，接着结合边缘感知聚类及区域匹配算法，将补丁层面的预测结果转化为连贯的区域级对应关系。此外，开发了自动标注流程，并对部分数据进行了人工精修，建立了基准数据集。

Result: 在多个数据集上，模型实现了高补丁级别准确率（如96.34%），且能够生成稳定、一致的区域级对应结果。

Conclusion: 该方法能在没有标签和掩膜的实际场景下实现漫画线稿图像间区域对应，具有广阔的实际应用前景。

Abstract: Understanding region-wise correspondence between manga line art images is a
fundamental task in manga processing, enabling downstream applications such as
automatic line art colorization and in-between frame generation. However, this
task remains largely unexplored, especially in realistic scenarios without
pre-existing segmentation or annotations. In this paper, we introduce a novel
and practical task: predicting region-wise correspondence between raw manga
line art images without any pre-existing labels or masks. To tackle this
problem, we divide each line art image into a set of patches and propose a
Transformer-based framework that learns patch-level similarities within and
across images. We then apply edge-aware clustering and a region matching
algorithm to convert patch-level predictions into coherent region-level
correspondences. To support training and evaluation, we develop an automatic
annotation pipeline and manually refine a subset of the data to construct
benchmark datasets. Experiments on multiple datasets demonstrate that our
method achieves high patch-level accuracy (e.g., 96.34%) and generates
consistent region-level correspondences, highlighting its potential for
real-world manga applications.

</details>


### [66] [Generative Diffusion Contrastive Network for Multi-View Clustering](https://arxiv.org/abs/2509.09527)
*Jian Zhu,Xin Zou,Xi Wang,Ning Zhang,Bian Wu,Yao Yang,Ying Zhou,Lingfang Zeng,Chang Tang,Cheng Luo*

Main category: cs.CV

TL;DR: 本文提出了一种新的多视图聚类方法，能有效处理数据噪声和缺失问题，并在多项实验中取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 多视图聚类虽然能够提升聚类效果，但在多视图数据融合时，受噪声和缺失数据影响较大，导致聚类性能受限，因此需要一种更稳健的方法提升低质量数据下的聚类表现。

Method: 提出了随机生成扩散融合（SGDF）方法，通过多生成机制对每个样本的多视图特征建模，提高对低质量数据的鲁棒性。基于SGDF，进一步设计了生成扩散对比网络（GDCN）。

Result: 大量实验表明，GDCN在深度多视图聚类任务上取得了当前最优的效果。

Conclusion: GDCN方法在解决带有噪声及缺失的多视图数据聚类问题方面展现出显著优势，并提升了聚类分析的性能。

Abstract: In recent years, Multi-View Clustering (MVC) has been significantly advanced
under the influence of deep learning. By integrating heterogeneous data from
multiple views, MVC enhances clustering analysis, making multi-view fusion
critical to clustering performance. However, there is a problem of low-quality
data in multi-view fusion. This problem primarily arises from two reasons: 1)
Certain views are contaminated by noisy data. 2) Some views suffer from missing
data. This paper proposes a novel Stochastic Generative Diffusion Fusion (SGDF)
method to address this problem. SGDF leverages a multiple generative mechanism
for the multi-view feature of each sample. It is robust to low-quality data.
Building on SGDF, we further present the Generative Diffusion Contrastive
Network (GDCN). Extensive experiments show that GDCN achieves the
state-of-the-art results in deep MVC tasks. The source code is publicly
available at https://github.com/HackerHyper/GDCN.

</details>


### [67] [DualTrack: Sensorless 3D Ultrasound needs Local and Global Context](https://arxiv.org/abs/2509.09530)
*Paul F. R. Wilson,Matteo Ronchetti,Rüdiger Göbl,Viktoria Markova,Sebastian Rosenzweig,Raphael Prevost,Parvin Mousavi,Oliver Zettinig*

Main category: cs.CV

TL;DR: 本论文提出了一种名为DualTrack的双编码器架构，能有效提升无传感器三维超声探头运动估计的精度，在公开基准上的重建误差低于5毫米，表现优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 三维超声成像具有明显的临床优势，但传统三维系统成本高、复杂度大，限制了其广泛应用。基于深度学习的无传感器三维超声，通过序列二维超声图像估计三维探头轨迹，有望成为一种低成本替代方案。目前方法往往忽略或耦合了局部与全局特征的提取，难以充分发挥双方互补作用。

Method: 作者提出了一种双编码器架构DualTrack，分别采用局部和全局编码器，彼此解耦，各自专责不同尺度特征的提取。局部编码器利用密集时空卷积提取精细局部特征；全局编码器采用2D CNN或基础模型结合时序注意力，嵌入高层次解剖特征和长程依赖。通过一个轻量融合模块合并二者特征，预测超声探头的三维轨迹。

Result: 在一个大规模公开基准数据集上，DualTrack实现了最新的准确率和全球一致的三维重建效果，平均重建误差低于5毫米，显著优于以往方法。

Conclusion: DualTrack通过解耦的局部与全局特征提取，有效提升了无传感器三维超声的重建精度和鲁棒性，有望推动三维超声的广泛应用，具有重要的实际意义。

Abstract: Three-dimensional ultrasound (US) offers many clinical advantages over
conventional 2D imaging, yet its widespread adoption is limited by the cost and
complexity of traditional 3D systems. Sensorless 3D US, which uses deep
learning to estimate a 3D probe trajectory from a sequence of 2D US images, is
a promising alternative. Local features, such as speckle patterns, can help
predict frame-to-frame motion, while global features, such as coarse shapes and
anatomical structures, can situate the scan relative to anatomy and help
predict its general shape. In prior approaches, global features are either
ignored or tightly coupled with local feature extraction, restricting the
ability to robustly model these two complementary aspects. We propose
DualTrack, a novel dual-encoder architecture that leverages decoupled local and
global encoders specialized for their respective scales of feature extraction.
The local encoder uses dense spatiotemporal convolutions to capture
fine-grained features, while the global encoder utilizes an image backbone
(e.g., a 2D CNN or foundation model) and temporal attention layers to embed
high-level anatomical features and long-range dependencies. A lightweight
fusion module then combines these features to estimate the trajectory.
Experimental results on a large public benchmark show that DualTrack achieves
state-of-the-art accuracy and globally consistent 3D reconstructions,
outperforming previous methods and yielding an average reconstruction error
below 5 mm.

</details>


### [68] [Improving Video Diffusion Transformer Training by Multi-Feature Fusion and Alignment from Self-Supervised Vision Encoders](https://arxiv.org/abs/2509.09547)
*Dohun Lee,Hyeonho Jeong,Jiwook Kim,Duygu Ceylan,Jong Chul Ye*

Main category: cs.CV

TL;DR: 本论文提出Align4Gen方法，通过融合和对齐视频生成模型的中间特征与预训练视觉编码器的特征，实现更高质量的视频生成。


<details>
  <summary>Details</summary>
Motivation: 虽然近年来视频扩散模型在结构创新和训练目标上取得进步，但对模型特征表达能力的提升关注较少。作者希望解决这一不足，提升视频生成质量。

Method: 作者提出Align4Gen方法：首先分析不同视觉编码器的判别力与时序一致性，选出适合的视频特征对齐编码器；然后在视频扩散模型训练中引入多特征融合和对齐模块，使生成过程中的特征与高质量视觉特征保持一致。

Result: Align4Gen方法在无条件和类别条件视频生成任务中均有较大提升，使用多项指标验证了生成视频质量的提升。

Conclusion: Align4Gen能有效提升视频扩散模型的特征表达能力和最终生成效果，拓展了视频生成模型的训练方法，为后续提升视频生成质量提供了新思路。

Abstract: Video diffusion models have advanced rapidly in the recent years as a result
of series of architectural innovations (e.g., diffusion transformers) and use
of novel training objectives (e.g., flow matching). In contrast, less attention
has been paid to improving the feature representation power of such models. In
this work, we show that training video diffusion models can benefit from
aligning the intermediate features of the video generator with feature
representations of pre-trained vision encoders. We propose a new metric and
conduct an in-depth analysis of various vision encoders to evaluate their
discriminability and temporal consistency, thereby assessing their suitability
for video feature alignment. Based on the analysis, we present Align4Gen which
provides a novel multi-feature fusion and alignment method integrated into
video diffusion model training. We evaluate Align4Gen both for unconditional
and class-conditional video generation tasks and show that it results in
improved video generation as quantified by various metrics. Full video results
are available on our project page: https://align4gen.github.io/align4gen/

</details>


### [69] [InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation](https://arxiv.org/abs/2509.09555)
*Sirui Xu,Dongting Li,Yucheng Zhang,Xiyan Xu,Qi Long,Ziyin Wang,Yunzhi Lu,Shuchang Dong,Hezi Jiang,Akshat Gupta,Yu-Xiong Wang,Liang-Yan Gui*

Main category: cs.CV

TL;DR: 本文提出了InterAct，这是一个大规模3D人体-物体交互（HOI）基准数据集，改进了现有数据集在高质量动作捕捉和标注方面的不足，并利用统一优化框架提升数据质量，同时定义了六项基准任务，实现了领域内最新性能。


<details>
  <summary>Details</summary>
Motivation: 现有的人体动作捕捉数据集虽然推动了动作生成的发展，但对动态的3D人与物体交互（HOI）建模仍然受限，因为这些数据集存在数据量小、标注不充分以及诸如穿插、漂浮和手部动作不正确等伪影问题，严重制约了高质量动作生成和分析。

Method: 1. 整合并标准化了21.81小时不同来源的HOI数据，并加入详细文本标注。2. 提出统一的优化框架，减少伪影并矫正手部动作，基于接触不变性原理生成更多运动变化内容，将数据扩展到30.70小时。3. 定义六类基准任务，制定统一的生成建模视角系统并在实验中取得了最优性能。

Result: 提出的InterAct数据集具有更丰富、标准化和高质量的动作捕捉与标注，实验结果显示基于InterAct开发的方法在六项基准任务上取得了领域内最优表现，有效推动了3D HOI生成研究。

Conclusion: InterAct数据集为3D人体-物体交互建模与生成提供了高质量的基础资源，显著改进了现有数据集和方法的不足，并通过公开和持续维护推动该领域研究的深入发展。

Abstract: While large-scale human motion capture datasets have advanced human motion
generation, modeling and generating dynamic 3D human-object interactions (HOIs)
remain challenging due to dataset limitations. Existing datasets often lack
extensive, high-quality motion and annotation and exhibit artifacts such as
contact penetration, floating, and incorrect hand motions. To address these
issues, we introduce InterAct, a large-scale 3D HOI benchmark featuring dataset
and methodological advancements. First, we consolidate and standardize 21.81
hours of HOI data from diverse sources, enriching it with detailed textual
annotations. Second, we propose a unified optimization framework to enhance
data quality by reducing artifacts and correcting hand motions. Leveraging the
principle of contact invariance, we maintain human-object relationships while
introducing motion variations, expanding the dataset to 30.70 hours. Third, we
define six benchmarking tasks and develop a unified HOI generative modeling
perspective, achieving state-of-the-art performance. Extensive experiments
validate the utility of our dataset as a foundational resource for advancing 3D
human-object interaction generation. To support continued research in this
area, the dataset is publicly available at
https://github.com/wzyabcas/InterAct, and will be actively maintained.

</details>


### [70] [Invisible Attributes, Visible Biases: Exploring Demographic Shortcuts in MRI-based Alzheimer's Disease Classification](https://arxiv.org/abs/2509.09558)
*Akshit Achara,Esther Puyol Anton,Alexander Hammers,Andrew P. King*

Main category: cs.CV

TL;DR: 本文探讨了基于深度学习（DL）的阿尔茨海默病（AD）MRI诊断中可能存在的种族和性别偏差。研究发现，深度学习模型能够从脑MRI中识别出受保护属性，并且训练集中的分布不均会导致偏见和性能下降。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习广泛应用于脑MRI的疾病（如AD）诊断，存在算法利用与疾病本身无关但与受保护属性（如种族、性别）相关特征进行预测的风险，从而导致对某些群体存在性能偏见。因此，需要系统性分析和量化这种偏见及其来源。

Method: 1. 研究DL模型能否识别脑MRI中的种族和性别特征，以探索是否存在与受保护属性相关的分布偏移。
2. 探索训练集中的种族或性别比例失衡是否导致模型性能下降，从而体现捷径学习和偏见。
3. 采用定量和定性方式，分析不同脑区对受保护属性和AD诊断任务的特征归因。
4. 使用多数据集和不同DL模型（ResNet和SwinTransformer）进行实验。

Result: 实验表明，DL模型不仅能从MRI中识别出受保护属性（种族、性别），且在训练集中存在分布失衡时，会导致模型性能下降和偏见。此外，通过特征归因分析，也发现了模型对受保护属性的依赖。

Conclusion: 深度学习模型在脑MRI AD分类任务中存在与种族和性别相关的捷径学习和偏见现象。文章为后续更公平的医学DL工具开发提供了基础。作者公开了相关代码以便进一步研究。

Abstract: Magnetic resonance imaging (MRI) is the gold standard for brain imaging. Deep
learning (DL) algorithms have been proposed to aid in the diagnosis of diseases
such as Alzheimer's disease (AD) from MRI scans. However, DL algorithms can
suffer from shortcut learning, in which spurious features, not directly related
to the output label, are used for prediction. When these features are related
to protected attributes, they can lead to performance bias against
underrepresented protected groups, such as those defined by race and sex. In
this work, we explore the potential for shortcut learning and demographic bias
in DL based AD diagnosis from MRI. We first investigate if DL algorithms can
identify race or sex from 3D brain MRI scans to establish the presence or
otherwise of race and sex based distributional shifts. Next, we investigate
whether training set imbalance by race or sex can cause a drop in model
performance, indicating shortcut learning and bias. Finally, we conduct a
quantitative and qualitative analysis of feature attributions in different
brain regions for both the protected attribute and AD classification tasks.
Through these experiments, and using multiple datasets and DL models (ResNet
and SwinTransformer), we demonstrate the existence of both race and sex based
shortcut learning and bias in DL based AD classification. Our work lays the
foundation for fairer DL diagnostic tools in brain MRI. The code is provided at
https://github.com/acharaakshit/ShortMR

</details>


### [71] [PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient Fine-Tuning for Remote Sensing Change Detection](https://arxiv.org/abs/2509.09572)
*Sijun Dong,Yuxuan Hu,LiBo Wang,Geng Chen,Xiaoliang Meng*

Main category: cs.CV

TL;DR: 本文提出了一个基于视觉基础模型（VFM）和参数高效微调（PEFT）的遥感变化检测框架PeftCD。该方法通过技术集成与精简参数训练，在公共数据集上取得了当前最优性能。


<details>
  <summary>Details</summary>
Motivation: 遥感图像变化检测中存在伪变化普遍、标注样本稀缺以及跨域泛化难等问题。为解决这些挑战，提高检测的准确性和适应性，提出新方法。

Method: PeftCD 核心是基于视觉基础模型的权重共享Siamese编码器，结合了 LoRA 和 Adapter 模块，实现只训练少量新增参数的高效适应；采用轻量解码器；主干网络探索了 SAM2 和 DINOv3 两种先进模型。

Result: 在SYSU-CD、WHUCD、MSRSCD、MLCD、CDD、S2Looking和LEVIR-CD七个数据集上，PeftCD在IoU等指标上实现了SOTA，表现出优秀的边界检测和伪变化抑制能力。

Conclusion: PeftCD在准确性、效率和泛化能力之间实现了最佳平衡，具有广泛的实际适用潜力。框架及代码将开源，利于推动遥感变化检测领域的应用。

Abstract: To tackle the prevalence of pseudo changes, the scarcity of labeled samples,
and the difficulty of cross-domain generalization in multi-temporal and
multi-source remote sensing imagery, we propose PeftCD, a change detection
framework built upon Vision Foundation Models (VFMs) with Parameter-Efficient
Fine-Tuning (PEFT). At its core, PeftCD employs a weight-sharing Siamese
encoder derived from a VFM, into which LoRA and Adapter modules are seamlessly
integrated. This design enables highly efficient task adaptation by training
only a minimal set of additional parameters. To fully unlock the potential of
VFMs, we investigate two leading backbones: the Segment Anything Model v2
(SAM2), renowned for its strong segmentation priors, and DINOv3, a
state-of-the-art self-supervised representation learner. The framework is
complemented by a deliberately lightweight decoder, ensuring the focus remains
on the powerful feature representations from the backbones. Extensive
experiments demonstrate that PeftCD achieves state-of-the-art performance
across multiple public datasets, including SYSU-CD (IoU 73.81%), WHUCD
(92.05%), MSRSCD (64.07%), MLCD (76.89%), CDD (97.01%), S2Looking (52.25%) and
LEVIR-CD (85.62%), with notably precise boundary delineation and strong
suppression of pseudo-changes. In summary, PeftCD presents an optimal balance
of accuracy, efficiency, and generalization. It offers a powerful and scalable
paradigm for adapting large-scale VFMs to real-world remote sensing change
detection applications. The code and pretrained models will be released at
https://github.com/dyzy41/PeftCD.

</details>


### [72] [Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis](https://arxiv.org/abs/2509.09595)
*Yikang Ding,Jiwen Liu,Wenyuan Zhang,Zekun Wang,Wentao Hu,Liyuan Cui,Mingming Lao,Yingchao Shao,Hui Liu,Xiaohan Li,Ming Chen,Xiaoqiang Liu,Yu-Shen Liu,Pengfei Wan*

Main category: cs.CV

TL;DR: Kling-Avatar提出了一种新颖的级联式框架，实现了对多模态指令的理解以及高保真音频驱动虚拟人视频生成，在表现人物情感和语义一致性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的音频驱动虚拟人视频生成方法在理解指令时仅限于低层次的声学或视觉跟踪，缺乏对指令所传递语义意图的建模，导致叙事连贯性和角色表现力不足。该研究旨在突破上述局限，实现对更复杂语义指令的响应和高质量虚拟人视频生成。

Method: 方法分为两个阶段。第一阶段，设计了多模态大语言模型（MLLM）作为“导演”，根据多模态指令生成描述高层语义（如动作、情绪等）的草图视频。第二阶段，依据草图关键帧并采用首尾帧策略并行生成多个子片段，实现细粒度细节还原和高层意图表达兼顾。该全局-局部、并行化架构提升了生成速度和视频时长可控性。

Result: 构建了涵盖375种多样指令和挑战场景的基准集，实验显示Kling-Avatar在唇形同步、情感及动态表现、指令可控性、身份保持及跨领域泛化等方面均显著优于现有方案，可生成高达1080p 48fps的自然流畅长视频。

Conclusion: Kling-Avatar成为音频驱动虚拟人视频生成领域在语义性、高保真度等方面的新标杆，在真人直播、vlog等实际应用具备良好潜力。

Abstract: Recent advances in audio-driven avatar video generation have significantly
enhanced audio-visual realism. However, existing methods treat instruction
conditioning merely as low-level tracking driven by acoustic or visual cues,
without modeling the communicative purpose conveyed by the instructions. This
limitation compromises their narrative coherence and character expressiveness.
To bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that
unifies multimodal instruction understanding with photorealistic portrait
generation. Our approach adopts a two-stage pipeline. In the first stage, we
design a multimodal large language model (MLLM) director that produces a
blueprint video conditioned on diverse instruction signals, thereby governing
high-level semantics such as character motion and emotions. In the second
stage, guided by blueprint keyframes, we generate multiple sub-clips in
parallel using a first-last frame strategy. This global-to-local framework
preserves fine-grained details while faithfully encoding the high-level intent
behind multimodal instructions. Our parallel architecture also enables fast and
stable generation of long-duration videos, making it suitable for real-world
applications such as digital human livestreaming and vlogging. To
comprehensively evaluate our method, we construct a benchmark of 375 curated
samples covering diverse instructions and challenging scenarios. Extensive
experiments demonstrate that Kling-Avatar is capable of generating vivid,
fluent, long-duration videos at up to 1080p and 48 fps, achieving superior
performance in lip synchronization accuracy, emotion and dynamic
expressiveness, instruction controllability, identity preservation, and
cross-domain generalization. These results establish Kling-Avatar as a new
benchmark for semantically grounded, high-fidelity audio-driven avatar
synthesis.

</details>


### [73] [Mechanistic Learning with Guided Diffusion Models to Predict Spatio-Temporal Brain Tumor Growth](https://arxiv.org/abs/2509.09610)
*Daria Laslo,Efthymios Georgiou,Marius George Linguraru,Andreas Rauschecker,Sabine Muller,Catherine R. Jutzeler,Sarah Bruningk*

Main category: cs.CV

TL;DR: 本文提出了一种混合机制学习框架，将数学肿瘤生长模型与引导去噪扩散隐式模型（DDIM）结合，以从先前的MRI扫描合成解剖上合理的未来影像，用以预测脑肿瘤时空进展。该方法在多个数据集上验证，能生成逼真的随访影像并可输出肿瘤生长概率图。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤的时空进展对神经肿瘤学临床决策至关重要，现有方法难以结合生物机制与高质量影像预测，尤其在数据受限场景下，临床上亟需能利用有限数据兼顾生物学真实性和解剖合理性的预测工具。

Method: 作者将普通微分方程形式的生长机制模型，用以估计未来肿瘤负担及放疗影响，并将该估算作为先验引入梯度引导的DDIM，通过扩散模型生成与预测肿瘤进展和患者解剖结构一致的未来MRI影像。模型在BraTS数据集和自有儿童胶质瘤病例上训练与评估。

Result: 该框架能基于空间相似性指标生成现实的后续MRI影像，并引入肿瘤生长概率图，能反映肿瘤生长的临床相关范围和方向性（以Hausdorff距离95百分位衡量）。

Conclusion: 本方法能于数据有限情境下实现基于机制先验的生物学合理影像生成，为脑肿瘤时空进展和治疗响应预测提供了有力工具。

Abstract: Predicting the spatio-temporal progression of brain tumors is essential for
guiding clinical decisions in neuro-oncology. We propose a hybrid mechanistic
learning framework that combines a mathematical tumor growth model with a
guided denoising diffusion implicit model (DDIM) to synthesize anatomically
feasible future MRIs from preceding scans. The mechanistic model, formulated as
a system of ordinary differential equations, captures temporal tumor dynamics
including radiotherapy effects and estimates future tumor burden. These
estimates condition a gradient-guided DDIM, enabling image synthesis that
aligns with both predicted growth and patient anatomy. We train our model on
the BraTS adult and pediatric glioma datasets and evaluate on 60 axial slices
of in-house longitudinal pediatric diffuse midline glioma (DMG) cases. Our
framework generates realistic follow-up scans based on spatial similarity
metrics. It also introduces tumor growth probability maps, which capture both
clinically relevant extent and directionality of tumor growth as shown by 95th
percentile Hausdorff Distance. The method enables biologically informed image
generation in data-limited scenarios, offering generative-space-time
predictions that account for mechanistic priors.

</details>


### [74] [Measuring Epistemic Humility in Multimodal Large Language Models](https://arxiv.org/abs/2509.09658)
*Bingkui Tong,Jiaer Xia,Sifeng Shang,Kaiyang Zhou*

Main category: cs.CV

TL;DR: 本文提出了HumbleBench，这是一个专门用于测试多模态大语言模型（MLLMs）应对幻觉（生成与输入图片不符内容）的基准。它关注模型识别错误选项并拒绝它们的能力，即“谦逊认知”，从而提升AI在实际应用中的可靠性和安全性。


<details>
  <summary>Details</summary>
Motivation: 目前MLLM的评测主要集中在识别正确答案（识别精度），但忽视了模型能否识别所有选项均不正确的能力，这对于实现安全、可信赖的AI极为关键。幻觉问题在真实应用（如视觉问答、决策等）中可能导致严重后果，因此需要新的评价能力。

Method: 作者基于一个全景场景图（panoptic scene graph）数据集，利用精细化场景标签生成实体与关系，并通过GPT-4-Turbo生成多项选择题。每道题都包含“以上皆非”选项，要求模型在三种幻觉类型（物体、关系、属性）上既能识别正确信息，又能判断所有答案都不正确时选择拒绝。问题生成后还进行了严格人工筛选。

Result: 作者对多种主流的MLLM（包括通用型与专用推理型）在HumbleBench上进行了评测，展示了模型在错误选项拒绝能力和识别真实视觉信息中的表现与差距，揭示了现有模型在安全关键情境中的不足。

Conclusion: 引入明确的错误选项拒绝评价标准，HumbleBench弥补了现有多模态模型评测体系的空白，为未来MLLM在安全、可靠场景中的应用提供了更加现实和严谨的测试手段。相关代码和数据集已公开发布，方便社区进一步研究。

Abstract: Hallucinations in multimodal large language models (MLLMs) -- where the model
generates content inconsistent with the input image -- pose significant risks
in real-world applications, from misinformation in visual question answering to
unsafe errors in decision-making. Existing benchmarks primarily test
recognition accuracy, i.e., evaluating whether models can select the correct
answer among distractors. This overlooks an equally critical capability for
trustworthy AI: recognizing when none of the provided options are correct, a
behavior reflecting epistemic humility. We present HumbleBench, a new
hallucination benchmark designed to evaluate MLLMs' ability to reject plausible
but incorrect answers across three hallucination types: object, relation, and
attribute. Built from a panoptic scene graph dataset, we leverage fine-grained
scene graph annotations to extract ground-truth entities and relations, and
prompt GPT-4-Turbo to generate multiple-choice questions, followed by a
rigorous manual filtering process. Each question includes a "None of the above"
option, requiring models not only to recognize correct visual information but
also to identify when no provided answer is valid. We evaluate a variety of
state-of-the-art MLLMs -- including both general-purpose and specialized
reasoning models -- on HumbleBench and share valuable findings and insights
with the community. By incorporating explicit false-option rejection,
HumbleBench fills a key gap in current evaluation suites, providing a more
realistic measure of MLLM reliability in safety-critical settings. Our code and
dataset are released publicly and can be accessed at
https://github.com/maifoundations/HumbleBench.

</details>


### [75] [Can Understanding and Generation Truly Benefit Together -- or Just Coexist?](https://arxiv.org/abs/2509.09666)
*Zhiyuan Yan,Kaiqing Lin,Zongjian Li,Junyan Ye,Hui Han,Zhendong Wang,Hao Liu,Bin Lin,Hao Li,Xue Xu,Xinyan Xiao,Jingdong Wang,Haifeng Wang,Li Yuan*

Main category: cs.CV

TL;DR: 本文提出一种新颖的多模态统一学习框架UAE，将图像到文本（I2T）和文本到图像（T2I）看作自动编码器的编码和解码过程，通过重建保真度作为统一训练目标，促进理解与生成的双向信息协同提升。


<details>
  <summary>Details</summary>
Motivation: 多模态理解（如图像理解）和生成（如图像生成）任务各自为战，缺乏统一和信息流通的训练范式。因此，作者希望寻找一个方法实现两者的相互促进，提高多模态模型的整体表现。

Method: 首先利用大规模蕴含丰富语义及空间关系的长文本caption预训练解码器；然后提出基于强化学习的Unified-GRPO算法，包括三阶段：1）冷启动通过语义重建损失初始化编码器与解码器；2）通过让编码器生成更佳caption以提升解码器重建图像的能力，训练编码器；3）通过让解码器细致理解caption增强重建和生成能力，训练解码器。

Result: 在提出的Unified-Bench多模态统一性评测基准上，实验发现随着强化学习的进展，编码器自动生成更具描述性的caption，解码器也能更好地理解并高质量重建图像，展现出引人注目的重建精度提升。

Conclusion: 本方法无缝结合理解和生成，有效提高了多模态模型在统一任务上的表现，实现了理解与生成间的互促和协同进步，推进了多模态统一学习的发展。

Abstract: In this paper, we introduce an insightful paradigm through the Auto-Encoder
lens-understanding as the encoder (I2T) that compresses images into text, and
generation as the decoder (T2I) that reconstructs images from that text. Using
reconstruction fidelity as the unified training objective, we enforce the
coherent bidirectional information flow between the understanding and
generation processes, bringing mutual gains. To implement this, we propose UAE,
a novel framework for unified multimodal learning. We begin by pre-training the
decoder with large-scale long-context image captions to capture fine-grained
semantic and complex spatial relationships. We then propose Unified-GRPO via
reinforcement learning (RL), which covers three stages: (1) A cold-start phase
to gently initialize both encoder and decoder with a semantic reconstruction
loss; (2) Generation for Understanding, where the encoder is trained to
generate informative captions that maximize the decoder's reconstruction
quality, enhancing its visual understanding; (3) Understanding for Generation,
where the decoder is refined to reconstruct from these captions, forcing it to
leverage every detail and improving its long-context instruction following and
generation fidelity. For evaluation, we introduce Unified-Bench, the first
benchmark tailored to assess the degree of unification of the UMMs. A
surprising "aha moment" arises within the multimodal learning domain: as RL
progresses, the encoder autonomously produces more descriptive captions, while
the decoder simultaneously demonstrates a profound ability to understand these
intricate descriptions, resulting in reconstructions of striking fidelity.

</details>


### [76] [Geometric Neural Distance Fields for Learning Human Motion Priors](https://arxiv.org/abs/2509.09667)
*Zhengdi Yu,Simone Foti,Linguang Zhang,Amy Zhao,Cem Keskin,Stefanos Zafeiriou,Tolga Birdal*

Main category: cs.CV

TL;DR: 本文提出了一种新的人体运动生成模型NRMF，通过神经距离场刻画更真实且物理合理的三维人体动作，用于各种人体动作恢复任务，表现优越。


<details>
  <summary>Details</summary>
Motivation: 当前基于VAE或扩散模型的人体动作先验难以兼顾时序一致性与物理可行性，难在多任务下泛化，需一种新的、更高阶的运动先验模型。

Method: NRMF将人体动作编码为神经距离场的零水平集，分别建模姿势、速度和加速度，严格在关节旋转与其变化的积空间中遵循人物关节结构。引入自适应投影算法和几何积分器，用于投影与轨迹生成。

Result: 在AMASS数据集上的实验显示，NRMF在不同输入模态和任务（如去噪、动作补全、2D/3D拟合等）中均取得显著改善，泛化能力强。

Conclusion: NRMF为三维人体动作建模提供了一种物理合理、可泛化的方法，优化了多种动作恢复类任务的表现，对相关研究有重要推动作用。

Abstract: We introduce Neural Riemannian Motion Fields (NRMF), a novel 3D generative
human motion prior that enables robust, temporally consistent, and physically
plausible 3D motion recovery. Unlike existing VAE or diffusion-based methods,
our higher-order motion prior explicitly models the human motion in the zero
level set of a collection of neural distance fields (NDFs) corresponding to
pose, transition (velocity), and acceleration dynamics. Our framework is
rigorous in the sense that our NDFs are constructed on the product space of
joint rotations, their angular velocities, and angular accelerations,
respecting the geometry of the underlying articulations. We further introduce:
(i) a novel adaptive-step hybrid algorithm for projecting onto the set of
plausible motions, and (ii) a novel geometric integrator to "roll out"
realistic motion trajectories during test-time-optimization and generation. Our
experiments show significant and consistent gains: trained on the AMASS
dataset, NRMF remarkably generalizes across multiple input modalities and to
diverse tasks ranging from denoising to motion in-betweening and fitting to
partial 2D / 3D observations.

</details>


### [77] [Locality in Image Diffusion Models Emerges from Data Statistics](https://arxiv.org/abs/2509.09672)
*Artem Lukoianov,Chenyang Yuan,Justin Solomon,Vincent Sitzmann*

Main category: cs.CV

TL;DR: 本文分析了扩散模型中最优去噪器与深度扩散模型之间的性能差距，证明了图像局部性主要来源于数据自身的统计属性，而非神经网络结构的归纳偏置，并据此提出更优的分析性去噪模型。


<details>
  <summary>Details</summary>
Motivation: 以往认为深度扩散模型（如UNet）的出色性能部分源于其网络结构的局部性（locality）和移位等变性（shift equivariance）等归纳偏置，但目前缺乏实证分析。作者旨在查明，扩散模型中表现出的局部性究竟是因网络结构还是因数据本身的统计属性。

Method: 作者分析了扩散模型训练目标的最优解（最优去噪器），并比较其与传统深度去噪器（如UNet）的性能和特性，通过理论推导和实验，考察了在不依赖神经网络结构归纳偏置，仅依赖数据统计属性时局部性的产生，并据此设计了新的分析性去噪器。

Result: 作者发现，线性参数化的最优去噪器能够表现出与深度神经网络去噪器类似的局部性，这一现象在理论和实验中都得到验证。进一步表明，这种局部性直接起源于自然图像集的像素相关关系。基于上述洞察，设计了比现有专家手工模型更契合深度扩散模型的分析性去噪器。

Conclusion: 深度扩散模型中的局部性主要源自数据统计属性，而不是卷积神经网络的归纳偏置。正确理解生成模型工作机制有助于更准确地分析和改进其组件，未来设计分析性模型时应重视数据本身的统计结构。

Abstract: Among generative models, diffusion models are uniquely intriguing due to the
existence of a closed-form optimal minimizer of their training objective, often
referred to as the optimal denoiser. However, diffusion using this optimal
denoiser merely reproduces images in the training set and hence fails to
capture the behavior of deep diffusion models. Recent work has attempted to
characterize this gap between the optimal denoiser and deep diffusion models,
proposing analytical, training-free models that can generate images that
resemble those generated by a trained UNet. The best-performing method
hypothesizes that shift equivariance and locality inductive biases of
convolutional neural networks are the cause of the performance gap, hence
incorporating these assumptions into its analytical model. In this work, we
present evidence that the locality in deep diffusion models emerges as a
statistical property of the image dataset, not due to the inductive bias of
convolutional neural networks. Specifically, we demonstrate that an optimal
parametric linear denoiser exhibits similar locality properties to the deep
neural denoisers. We further show, both theoretically and experimentally, that
this locality arises directly from the pixel correlations present in natural
image datasets. Finally, we use these insights to craft an analytical denoiser
that better matches scores predicted by a deep diffusion model than the prior
expert-crafted alternative.

</details>


### [78] [SpatialVID: A Large-Scale Video Dataset with Spatial Annotations](https://arxiv.org/abs/2509.09676)
*Jiahao Wang,Yufeng Yuan,Rujie Zheng,Youtian Lin,Jian Gao,Lin-Zhuo Chen,Yajie Bao,Yi Zhang,Chang Zeng,Yanxi Zhou,Xiaoxiao Long,Hao Zhu,Zhaoxiang Zhang,Xun Cao,Yao Yao*

Main category: cs.CV

TL;DR: 本文介绍了SpatialVID，这是一个大规模、高质量的真实世界视频数据集，包含丰富的三维和语义注释，旨在突破空间智能领域现有数据集的规模和多样性瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前空间智能模型在规模和真实世界泛化能力上受限，原因是缺乏大规模、高质量、具有丰富注释的数据集。已有数据集规模小、多样性和注释匮乏，尤其缺乏真实动态场景下的精确相机运动信息。

Method: 作者收集了21000小时的原始视频，经过层级过滤处理后，生成270万段视频片段，并挑选出7089小时的动态内容。然后通过注释流程，为这些片段补充了精细的空间与语义信息，包括每帧的相机位姿、深度图、动态掩码、结构化描述和序列化运动指令等。

Result: SpatialVID数据集具有极其丰富和多样的数据统计特征，数据量和注释的全面性远超先前的数据集。

Conclusion: SpatialVID为视频和三维视觉领域提供了一个关键的数据资源，有望推动相关模型的泛化能力和性能提升。

Abstract: Significant progress has been made in spatial intelligence, spanning both
spatial reconstruction and world exploration. However, the scalability and
real-world fidelity of current models remain severely constrained by the
scarcity of large-scale, high-quality training data. While several datasets
provide camera pose information, they are typically limited in scale,
diversity, and annotation richness, particularly for real-world dynamic scenes
with ground-truth camera motion. To this end, we collect \textbf{SpatialVID}, a
dataset consists of a large corpus of in-the-wild videos with diverse scenes,
camera movements and dense 3D annotations such as per-frame camera poses,
depth, and motion instructions. Specifically, we collect more than 21,000 hours
of raw video, and process them into 2.7 million clips through a hierarchical
filtering pipeline, totaling 7,089 hours of dynamic content. A subsequent
annotation pipeline enriches these clips with detailed spatial and semantic
information, including camera poses, depth maps, dynamic masks, structured
captions, and serialized motion instructions. Analysis of SpatialVID's data
statistics reveals a richness and diversity that directly foster improved model
generalization and performance, establishing it as a key asset for the video
and 3D vision research community.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [79] [Noise or Nuance: An Investigation Into Useful Information and Filtering For LLM Driven AKBC](https://arxiv.org/abs/2509.08903)
*Alex Clay,Ernesto Jiménez-Ruiz,Pranava Madhyastha*

Main category: cs.CL

TL;DR: 本文分析了在受限条件下完成知识三元组任务时，对于生成、质量保障和LLM回复解析的不同策略和成效。


<details>
  <summary>Details</summary>
Motivation: 在2025 LM-KBC挑战等受限场景下，常用的RAG与微调方法不可用，因此需要探索在此环境下提升大模型输出质量的新方法。

Method: 针对知识三元组补全，论文分别从生成、质量保障和LLM回复解析三个方面进行实证研究，检验增加额外信息、利用LLM过滤低质量三元组和不同回复解析方式的效果。

Result: 实验表明：在受限环境中，额外的信息能够提升生成质量；LLM能有效过滤劣质三元组；解析回复时，在灵活性与一致性间需权衡，且效果受具体设置影响。

Conclusion: 论文证实了在无法用RAG和微调时，依靠补充信息和巧用LLM自身能力，可以提升三元组补全效率与质量，但回复解析方式应按实际场景选择。

Abstract: RAG and fine-tuning are prevalent strategies for improving the quality of LLM
outputs. However, in constrained situations, such as that of the 2025 LM-KBC
challenge, such techniques are restricted. In this work we investigate three
facets of the triple completion task: generation, quality assurance, and LLM
response parsing. Our work finds that in this constrained setting: additional
information improves generation quality, LLMs can be effective at filtering
poor quality triples, and the tradeoff between flexibility and consistency with
LLM response parsing is setting dependent.

</details>


### [80] [Automated Evidence Extraction and Scoring for Corporate Climate Policy Engagement: A Multilingual RAG Approach](https://arxiv.org/abs/2509.08907)
*Imene Kolli,Ario Saeid Vaghefi,Chiara Colesanti Senni,Shantam Raj,Markus Leippold*

Main category: cs.CL

TL;DR: 本论文提出利用AI增强的框架，加速企业气候政策参与的监测，尤其是自动提取大量文本证据的流程，并通过实验证明新方法显著提升效率，但仍需人工参与以保证准确性。


<details>
  <summary>Details</summary>
Motivation: 目前InfluenceMap的LobbyMap平台监测企业和协会的气候政策活动较为依赖手工，既耗时又易出错，亟需提升自动化水平以提高监控效率和数据准确性。

Method: 提出一种基于检索增强生成（RAG）的AI辅助框架，结合布局感知解析、Nomic嵌入模型和少样本提示等技术，实现多语言公司文档中相关证据的自动提取和分类。

Result: 通过评估发现，综合上述方法可以在多语种环境下有效提取和分类公司文档中的相关证据，并在效率上高于传统手动流程。

Conclusion: 自动化RAG系统能明显提升证据提取速度，但复杂深入的分析仍需专家参与，最佳方式是人机协作，用技术增强而非完全取代人工判断，以保证分析的准确性。

Abstract: InfluenceMap's LobbyMap Platform monitors the climate policy engagement of
over 500 companies and 250 industry associations, assessing each entity's
support or opposition to science-based policy pathways for achieving the Paris
Agreement's goal of limiting global warming to 1.5{\deg}C. Although
InfluenceMap has made progress with automating key elements of the analytical
workflow, a significant portion of the assessment remains manual, making it
time- and labor-intensive and susceptible to human error. We propose an
AI-assisted framework to accelerate the monitoring of corporate climate policy
engagement by leveraging Retrieval-Augmented Generation to automate the most
time-intensive extraction of relevant evidence from large-scale textual data.
Our evaluation shows that a combination of layout-aware parsing, the Nomic
embedding model, and few-shot prompting strategies yields the best performance
in extracting and classifying evidence from multilingual corporate documents.
We conclude that while the automated RAG system effectively accelerates
evidence extraction, the nuanced nature of the analysis necessitates a
human-in-the-loop approach where the technology augments, rather than replaces,
expert judgment to ensure accuracy.

</details>


### [81] [Documents Are People and Words Are Items: A Psychometric Approach to Textual Data with Contextual Embeddings](https://arxiv.org/abs/2509.08920)
*Jinsong Chen*

Main category: cs.CL

TL;DR: 该研究提出了一种新颖的文本心理测量方法，利用大语言模型与上下文嵌入，将文本数据转化为适用于心理测量分析的反应数据，并通过因子分析等方法挖掘文本中潜在的知识维度和模式。


<details>
  <summary>Details</summary>
Motivation: 传统心理测量方法难以直接处理和解读复杂的文本数据，为了解决如何将文本数据转化为结构化、可度量对象，研究者希望通过大语言模型和现代NLP技术，将自然语言文本与心理测量分析结合，提升潜变量挖掘和解释能力。

Method: 将文档视为个体，单词视为项目，首先采用基于Transformer的编码器等NLP技术提取关键词并生成上下文得分；再通过探索性因子分析和双因子模型等心理测量统计方法，提取和定义潜在因子，分析因子间关系，并找出每个因子的重要关联词。实验以Wiki STEM文本库为例进行验证。

Result: 实验表明该方法能够有效揭示文本数据中的潜在知识维度和模式，对文本的心理测量分析有显著提升，并可解释不同因子对应的重要关键词。

Conclusion: 该方法创新性地结合大语言模型与心理测量分析，为文本丰富的领域如教育、心理学和法律的文本分析提供新思路和工具，具有广泛应用前景。

Abstract: This research introduces a novel psychometric method for analyzing textual
data using large language models. By leveraging contextual embeddings to create
contextual scores, we transform textual data into response data suitable for
psychometric analysis. Treating documents as individuals and words as items,
this approach provides a natural psychometric interpretation under the
assumption that certain keywords, whose contextual meanings vary significantly
across documents, can effectively differentiate documents within a corpus. The
modeling process comprises two stages: obtaining contextual scores and
performing psychometric analysis. In the first stage, we utilize natural
language processing techniques and encoder based transformer models to identify
common keywords and generate contextual scores. In the second stage, we employ
various types of factor analysis, including exploratory and bifactor models, to
extract and define latent factors, determine factor correlations, and identify
the most significant words associated with each factor. Applied to the Wiki
STEM corpus, our experimental results demonstrate the method's potential to
uncover latent knowledge dimensions and patterns within textual data. This
approach not only enhances the psychometric analysis of textual data but also
holds promise for applications in fields rich in textual information, such as
education, psychology, and law.

</details>


### [82] [BRoverbs -- Measuring how much LLMs understand Portuguese proverbs](https://arxiv.org/abs/2509.08960)
*Thales Sales Almeida,Giovana Kerche Bonás,João Guilherme Alves Santos*

Main category: cs.CL

TL;DR: 该论文提出了一种新的葡萄牙语谚语数据集（BRoverbs），用于评估大语言模型（LLM）在葡语文化语境下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有葡萄牙语LLM评估大多依赖翻译数据集或偏重结构化考试、情感分析，难以反映葡语独有的语言和文化特征。缺乏体现地区文化和复杂语言理解的数据集，导致评测不充分。

Method: 作者构建了BRoverbs数据集，包含巴西葡萄牙语的谚语，通过这些富含文化和修辞的句子，考查LLM对地区性表达的理解能力。

Result: BRoverbs数据集已上线，并为评估葡萄牙语LLM提供了一个新的、文化相关的基准。初步结果未明述，但强调了其测试维度的独特性和必要性。

Conclusion: BRoverbs填补了现有葡萄牙语LLM评测的空白，为未来葡语及其它区域语言模型的文化适应性评估提供了借鉴和工具。

Abstract: Large Language Models (LLMs) exhibit significant performance variations
depending on the linguistic and cultural context in which they are applied.
This disparity signals the necessity of mature evaluation frameworks that can
assess their capabilities in specific regional settings. In the case of
Portuguese, existing evaluations remain limited, often relying on translated
datasets that may not fully capture linguistic nuances or cultural references.
Meanwhile, native Portuguese-language datasets predominantly focus on
structured national exams or sentiment analysis of social media interactions,
leaving gaps in evaluating broader linguistic understanding. To address this
limitation, we introduce BRoverbs, a dataset specifically designed to assess
LLM performance through Brazilian proverbs. Proverbs serve as a rich linguistic
resource, encapsulating cultural wisdom, figurative expressions, and complex
syntactic structures that challenge the model comprehension of regional
expressions. BRoverbs aims to provide a new evaluation tool for
Portuguese-language LLMs, contributing to advancing regionally informed
benchmarking. The benchmark is available at
https://huggingface.co/datasets/Tropic-AI/BRoverbs.

</details>


### [83] [Can Vision-Language Models Solve Visual Math Equations?](https://arxiv.org/abs/2509.09013)
*Monjoy Narayan Choudhury,Junling Wang,Yifan Hou,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 本文分析了当前视觉-语言模型（VLMs）在需要整合感知与符号计算的任务上的局限性，尤其是在基于视觉线索解数学方程上暴露的不足。


<details>
  <summary>Details</summary>
Motivation: 虽然VLMs在视觉理解和基于语言的推理任务上表现优异，但它们在需要同时感知和符号推理的复杂任务中依然存在明显短板。本文希望通过视觉方程求解分析这些模型的具体瓶颈。

Method: 设计视觉方程求解任务，将数学方程嵌入图片，变量用物体图标表示，系数需通过数数来推断。将任务拆分为系数计数和变量识别两个子任务，对比分析VLM在文本方程与视觉方程上的表现。

Result: 结果发现，尽管VLM在变量识别上表现尚可，但系数计数是主要瓶颈。当识别与推理需要组合时，错误率进一步上升。此外，随着方程复杂度提升，符号推理能力本身成为瓶颈。

Conclusion: 当前VLM在具象视觉线索与符号推理融合类任务上有显著不足，尤其在多步视觉推理和复杂符号运算中表现有限，未来模型需针对视觉数学推理能力进行改进。

Abstract: Despite strong performance in visual understanding and language-based
reasoning, Vision-Language Models (VLMs) struggle with tasks requiring
integrated perception and symbolic computation. We study this limitation
through visual equation solving, where mathematical equations are embedded in
images, variables are represented by object icons, and coefficients must be
inferred by counting. While VLMs perform well on textual equations, they fail
on visually grounded counterparts. To understand this gap, we decompose the
task into coefficient counting and variable recognition, and find that counting
is the primary bottleneck, even when recognition is accurate. We also observe
that composing recognition and reasoning introduces additional errors,
highlighting challenges in multi-step visual reasoning. Finally, as equation
complexity increases, symbolic reasoning itself becomes a limiting factor.
These findings reveal key weaknesses in current VLMs and point toward future
improvements in visually grounded mathematical reasoning.

</details>


### [84] [Stated Preference for Interaction and Continued Engagement (SPICE): Evaluating an LLM's Willingness to Re-engage in Conversation](https://arxiv.org/abs/2509.09043)
*Thomas Manuel Rost,Martina Figlia,Bernd Wallraff*

Main category: cs.CL

TL;DR: 本文提出了一种名为SPICE的新诊断信号，通过让大语言模型对是否愿意继续与用户互动进行简单的“是/否”回答，从而衡量其对用户行为的反应。实验显示SPICE能有效区分不同语气的用户（友好、不清晰、辱骂），并且与传统滥用检测手段互补。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在面对用户的不同语气时，缺乏直接、简便的工具来评估模型是否希望继续互动，尤其在滥用检测和模型鲁棒性评估方面存在不足。因此，需要一种新的信号，能够快速反映模型的互动倾向性。

Method: 作者设计了SPICE信号，即在模型阅读用户交互之后，直接询问模型是否愿意继续对话。实验设置包括3种用户语气（友好、不清晰、辱骂）、10次交互、4个开源模型和4种框架条件，共480组实验，并采用多种统计方法进行分析。

Result: SPICE信号能显著区分不同用户语气：对友好语气97.5%选择继续，辱骂语气仅17.9%选择继续，不清晰语气为60.4%。上述关联在多种统计检验下均显著。此外，SPICE与传统滥用分类不同，即使模型未检测到滥用，SPICE仍能表达不愿继续的倾向。不同文本展示方式、前言设置对SPICE也有显著影响。

Conclusion: SPICE是一种简单、低成本、可复现的工具，可以作为现有指标的有力补充，直接反映模型的互动态度。论文公开了所有实验材料和代码，便于社区复现。

Abstract: We introduce and evaluate Stated Preference for Interaction and Continued
Engagement (SPICE), a simple diagnostic signal elicited by asking a Large
Language Model a YES or NO question about its willingness to re-engage with a
user's behavior after reviewing a short transcript. In a study using a 3-tone
(friendly, unclear, abusive) by 10-interaction stimulus set, we tested four
open-weight chat models across four framing conditions, resulting in 480
trials. Our findings show that SPICE sharply discriminates by user tone.
Friendly interactions yielded a near-unanimous preference to continue (97.5%
YES), while abusive interactions yielded a strong preference to discontinue
(17.9% YES), with unclear interactions falling in between (60.4% YES). This
core association remains decisive under multiple dependence-aware statistical
tests, including Rao-Scott adjustment and cluster permutation tests.
Furthermore, we demonstrate that SPICE provides a distinct signal from abuse
classification. In trials where a model failed to identify abuse, it still
overwhelmingly stated a preference not to continue the interaction (81% of the
time). An exploratory analysis also reveals a significant interaction effect: a
preamble describing the study context significantly impacts SPICE under
ambiguity, but only when transcripts are presented as a single block of text
rather than a multi-turn chat. The results validate SPICE as a robust,
low-overhead, and reproducible tool for auditing model dispositions,
complementing existing metrics by offering a direct, relational signal of a
model's state. All stimuli, code, and analysis scripts are released to support
replication.

</details>


### [85] [Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M](https://arxiv.org/abs/2509.09055)
*Piyush Pant*

Main category: cs.CL

TL;DR: 本研究比较了三种语言模型对齐技术——监督微调（SFT）、直接偏好优化（DPO）、以及二者结合（SFT+DPO）——对OPT-350M模型的安全性和有用性的提升效果。结果显示SFT+DPO结合模型在各项指标上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型应用的增长，如何通过有效对齐技术提升模型的安全性和有用性成为关键问题。目前不同微调方法优劣不明，本研究旨在系统比较它们的效果，并探索其组合是否带来额外提升。

Method: 使用Anthropic Helpful-Harmless RLHF数据集，分别训练了基础OPT-350M模型、SFT模型、DPO模型以及SFT+DPO组合模型。创新性地引入三项对齐评价指标——安全率（HmR）、有用率（HpR）、对齐综合得分（CAS），由奖励模型进行评测。

Result: 实验结果显示，SFT方法优于DPO，但SFT+DPO组合模型在所有评价指标上超过单一方法，验证了二者互补性。同时，实验指出噪声数据、GPU算力和训练限制对结果的影响。

Conclusion: 研究表明，SFT和DPO结合能显著提升模型安全性和有用性，对未来设计更健壮的语言模型对齐流程具有重要参考价值。

Abstract: This research investigates the effectiveness of alignment techniques,
Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and a
combined SFT+DPO approach on improving the safety and helpfulness of the
OPT-350M language model. Utilizing the Anthropic Helpful-Harmless RLHF dataset,
we train and evaluate four models: the base OPT350M, an SFT model, a DPO model,
and a model trained with both SFT and DPO. We introduce three key evaluation
metrics: Harmlessness Rate (HmR), Helpfulness Rate (HpR), and a Combined
Alignment Score (CAS), all derived from reward model outputs. The results show
that while SFT outperforms DPO, The combined SFT+DPO model outperforms all
others across all metrics, demonstrating the complementary nature of these
techniques. Our findings also highlight challenges posed by noisy data, limited
GPU resources, and training constraints. This study offers a comprehensive view
of how fine-tuning strategies affect model alignment and provides a foundation
for more robust alignment pipelines in future work.

</details>


### [86] [MR-UIE: Multi-Perspective Reasoning with Reinforcement Learning for Universal Information Extraction](https://arxiv.org/abs/2509.09082)
*Zhongqiu Li,Shiquan Wang,Ruiyu Fang,Mengjiao Bao,Zhenhe Wu,Shuangyong Song,Yongxiang Li,Zhongjiang He*

Main category: cs.CL

TL;DR: 该论文提出通过将多视角推理(MR)与强化学习(RL)结合，提升大语言模型(LLMs)在通用信息抽取(UIE)任务中的泛化能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在应对复杂结构化输出与多步推理的信息抽取任务时表现有限，即使使用了上下文学习和指令微调，依然存在泛化能力不足的问题。

Method: 作者提出MR-UIE方法：将多视角推理融入强化学习训练框架，使LLMs能够主动推理，不仅知道提取什么信息，还理解如何推理，从而提升模型泛化能力。

Result: 在多个信息抽取基准数据集上的实验显示，MR-UIE方法在多个领域的抽取准确率持续提升，且在若干数据集上超越了当前最优水平。

Conclusion: 将多视角推理整合到强化学习中，对于提升LLMs在复杂信息抽取任务中的泛化和推理能力至关重要。

Abstract: Large language models (LLMs) demonstrate robust capabilities across diverse
research domains. However, their performance in universal information
extraction (UIE) remains insufficient, especially when tackling structured
output scenarios that involve complex schema descriptions and require
multi-step reasoning. While existing approaches enhance the performance of LLMs
through in-context learning and instruction tuning, significant limitations
nonetheless persist. To enhance the model's generalization ability, we propose
integrating reinforcement learning (RL) with multi-perspective reasoning for
information extraction (IE) tasks. Our work transitions LLMs from passive
extractors to active reasoners, enabling them to understand not only what to
extract but also how to reason. Experiments conducted on multiple IE benchmarks
demonstrate that MR-UIE consistently elevates extraction accuracy across
domains and surpasses state-of-the-art methods on several datasets.
Furthermore, incorporating multi-perspective reasoning into RL notably enhances
generalization in complex IE tasks, underscoring the critical role of reasoning
in challenging scenarios.

</details>


### [87] [TigerCoder: A Novel Suite of LLMs for Code Generation in Bangla](https://arxiv.org/abs/2509.09101)
*Nishat Raihan,Antonios Anastasopoulos,Marcos Zampieri*

Main category: cs.CL

TL;DR: 本文针对孟加拉语代码生成任务，提出了首个专用的Bangla代码大语言模型（Code LLM），并显著提升了代码生成性能。


<details>
  <summary>Details</summary>
Motivation: 虽然孟加拉语是全球第五大语言，但在代码生成相关的大语言模型中严重缺乏资源。主要原因是高质量训练数据的不足，制约了模型在该语言上的发展和应用。

Method: 1. 构建了全面的孟加拉语代码指令数据集，便于编程领域的模型适应；2. 提出了MBPP-Bangla作为代码生成评测基准；3. 研发了TigerCoder系列孟加拉语代码LLM（1B和9B参数规模）。所有资源均开源。

Result: TigerCoder系列模型在Pass@1指标上比现有多语种及通用孟加拉语LLM提升了约11-18%的性能。实验证明，精心构建的高质量数据集在小模型对低资源语言任务有明显助益。

Conclusion: 高质量、多样化的数据集能推动低资源语言（如孟加拉语）在代码生成等任务上的研究与应用。开源资源有望进一步促进孟加拉语相关LLM的发展。

Abstract: Despite being the 5th most spoken language, Bangla remains underrepresented
in Large Language Models (LLMs), particularly for code generation. This
primarily stems from the scarcity of high-quality data to pre-train and/or
finetune such models. Hence, we introduce the first dedicated family of Code
LLMs for Bangla (1B & 9B). We offer three major contributions: (1) a
comprehensive Bangla code instruction datasets for programming domain
adaptation; (2) MBPP-Bangla, an evaluation benchmark for Bangla code
generation; and (3) the TigerCoder-family of Code LLMs, achieving significant
~11-18% performance gains at Pass@1 over existing multilingual and
general-purpose Bangla LLMs. Our findings show that curated, high-quality
datasets can overcome limitations of smaller models for low-resource languages.
We open-source all resources to advance further Bangla LLM research.

</details>


### [88] [Compass-v3: Scaling Domain-Specific LLMs for Multilingual E-Commerce in Southeast Asia](https://arxiv.org/abs/2509.09121)
*Sophia Maria*

Main category: cs.CL

TL;DR: Compass-v3是一款面向东南亚电商领域的大规模专家混合（MoE）语言模型，具备强大的多语言和电商特定能力，已在Shopee平台广泛应用，取代了大部分OpenAI模型流量。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在通用任务上表现优异，但在电商等垂直领域，因数据多样、噪声高、场景复杂，模型表现大幅下降。针对此问题，特别是在东南亚这种多语言、多变数据市场，开发适应性强的模型迫在眉睫。

Method: Compass-v3采用245B参数的MoE结构，每次调用激活71B参数，以减少计算资源消耗。通过专家数减少但规模增大、节点内专家并行、定制内存拷贝算子等技术提升GPU利用率。训练数据涵盖12万亿tokens的多语种电商语料和大规模合成指令，结合混合训练策略。引入最优传输偏好直接优化（OTPO）方法，提升模型对电商领域指令和细节的遵循能力。

Result: Compass-v3在电商任务上超越了DeepSeek-V3.1、GPT-4系列和Qwen3-235B，在低资源东南亚语言和葡萄牙语上的多语言能力突出。一般性基准测试也保持竞争力，并且在Shopee落地后已取代70%以上的OpenAI相关模型流量。

Conclusion: Compass-v3兼具电商专业能力和多语言覆盖，显著优化了东南亚电商场景的AI服务效果，是通用LLM与垂直领域应用深度结合的成功案例。

Abstract: Large language models (LLMs) excel in general-domain applications, yet their
performance often degrades in specialized tasks requiring domain-specific
knowledge. E-commerce is particularly challenging, as its data are noisy,
heterogeneous, multilingual, and highly dynamic. We present Compass-v3, a
vertical-domain Mixture-of-Experts (MoE) model with 245B total parameters and
71B active per token, designed for Southeast Asian e-commerce. Compass-v3
adopts fewer but larger experts, combined with hardware-efficient
optimizations-such as intra-node expert parallelism and a customized memcpy
operator-to maximize GPU utilization. The model is trained on 12T tokens of
curated multilingual corpora and large-scale synthetic e-commerce instructions
using a mixed-training strategy. To enhance alignment, we propose
Optimal-Transport Direct Preference Optimization (OTPO), which captures
token-level distinctions and improves instruction adherence in
commerce-specific scenarios. Extensive evaluations demonstrate that Compass-v3
delivers state-of-the-art e-commerce performance, surpassing DeepSeek-V3.1,
GPT-4 series, and Qwen3-235B. Moreover, Compass-v3 demonstrates strong
multilingual capability across low-resource Southeast Asian languages
(Indonesian, Thai, Filipino, Vietnamese, Malay, Taglog) and Portuguese while
sustaining competitive performance on general benchmarks. It has already been
widely applied in Shopee's industrial-scale e-commerce platform and is
gradually replacing OpenAI's traffic, now accounting for over 70\% of total LLM
usage, highlighting its dual strengths in specialized commerce expertise and
broad linguistic competence.

</details>


### [89] [Automated Classification of Tutors' Dialogue Acts Using Generative AI: A Case Study Using the CIMA Corpus](https://arxiv.org/abs/2509.09125)
*Liqun He,Jiaqi Xu*

Main category: cs.CL

TL;DR: 本文探讨了利用生成式AI（如GPT-3.5-turbo和GPT-4）自动对导师对话行为（Dialogue Acts, DAs）进行分类，旨在减少人工编码的时间和精力。GPT-4模型表现优异，与人工注释高度一致，显示出AI在教育对话分析中的潜力。


<details>
  <summary>Details</summary>
Motivation: 手动对教育对话中的导师发言进行分类耗时耗力，通过生成式AI实现自动化分类，可以提升效率，降低成本，提高大规模语料分析的可行性。

Method: 研究基于CIMA语料库，导师的发言已根据四种DA类型预先标注。使用定制化提示语，让GPT-3.5-turbo和GPT-4两种生成式AI模型对同样语料进行DA分类，结果与人工标注进行比较。

Result: GPT-4分类准确率达80%，加权F1分数为0.81，Cohen’s Kappa为0.74，超过基线表现，与人工注释有较高一致性。

Conclusion: 生成式AI能高效且准确地对教育对话中的导师发言进行DA分类；结合明确定义的标签和上下文信息可提升AI标注质量。需关注AI应用中的伦理与透明问题。相关代码已开源。

Abstract: This study explores the use of generative AI for automating the
classification of tutors' Dialogue Acts (DAs), aiming to reduce the time and
effort required by traditional manual coding. This case study uses the
open-source CIMA corpus, in which tutors' responses are pre-annotated into four
DA categories. Both GPT-3.5-turbo and GPT-4 models were tested using tailored
prompts. Results show that GPT-4 achieved 80% accuracy, a weighted F1-score of
0.81, and a Cohen's Kappa of 0.74, surpassing baseline performance and
indicating substantial agreement with human annotations. These findings suggest
that generative AI has strong potential to provide an efficient and accessible
approach to DA classification, with meaningful implications for educational
dialogue analysis. The study also highlights the importance of task-specific
label definitions and contextual information in enhancing the quality of
automated annotation. Finally, it underscores the ethical considerations
associated with the use of generative AI and the need for responsible and
transparent research practices. The script of this research is publicly
available at
https://github.com/liqunhe27/Generative-AI-for-educational-dialogue-act-tagging.

</details>


### [90] [ViRanker: A BGE-M3 & Blockwise Parallel Transformer Cross-Encoder for Vietnamese Reranking](https://arxiv.org/abs/2509.09131)
*Phuong-Nam Dang,Kieu-Linh Nguyen,Thanh-Hieu Pham*

Main category: cs.CL

TL;DR: 本文提出ViRanker，一种专为越南语开发的交叉编码重排序模型，在MMARCO-VI基准上表现优异，超越多语种基线，助力低资源语言的信息检索。


<details>
  <summary>Details</summary>
Motivation: 越南语作为低资源语言，缺乏强有力的信息检索重排序模型。同时，其语法复杂且含有变音符号，难以适配现有主流多语种模型。因此，作者致力于开发一个专为越南语优化的先进重排序模型，以提升检索性能，并填补该领域的空白。

Method: 以BGE-M3编码器为基础，结合Blockwise Parallel Transformer架构，训练数据为8GB精心整理的越南语语料。在微调阶段采用混合硬负样本采样策略，增强模型鲁棒性，并在MMARCO-VI基准上进行评测。

Result: ViRanker在MMARCO-VI评测中取得优秀早期排序准确率，超越了多语种基线模型，并与PhoRanker表现接近。模型的强劲性能证明了其优越性。

Conclusion: ViRanker不仅提升了越南语检索系统性能，还为其他低资源语言的重排序技术提供了可借鉴的模型架构与数据处理方法。开源发布有助于模型可复现性和实际应用推广。

Abstract: This paper presents ViRanker, a cross-encoder reranking model tailored to the
Vietnamese language. Built on the BGE-M3 encoder and enhanced with the
Blockwise Parallel Transformer, ViRanker addresses the lack of competitive
rerankers for Vietnamese, a low-resource language with complex syntax and
diacritics. The model was trained on an 8 GB curated corpus and fine-tuned with
hybrid hard-negative sampling to strengthen robustness. Evaluated on the
MMARCO-VI benchmark, ViRanker achieves strong early-rank accuracy, surpassing
multilingual baselines and competing closely with PhoRanker. By releasing the
model openly on Hugging Face, we aim to support reproducibility and encourage
wider adoption in real-world retrieval systems. Beyond Vietnamese, this study
illustrates how careful architectural adaptation and data curation can advance
reranking in other underrepresented languages.

</details>


### [91] [LITcoder: A General-Purpose Library for Building and Comparing Encoding Models](https://arxiv.org/abs/2509.09152)
*Taha Binhuraib,Ruimin Gao,Anna A. Ivanova*

Main category: cs.CL

TL;DR: LITcoder是一个开源库，用于构建和评测神经编码模型，支持多种大脑-刺激配对、特征转换、映射及性能评估任务，并强调灵活、可拓展的模块化设计和实验可复现性。


<details>
  <summary>Details</summary>
Motivation: 现有的神经编码模型实现常缺乏标准化、灵活性差，导致研究者需要重复开发基础设施。该库旨在降低门槛，帮助学界更系统、规范地比较、推广和高效实现高质量大脑预测模型。

Method: LITcoder采取模块化流水线设计，整合了丰富的大脑数据集、脑区、刺激特征（神经网络特征及对照特征）、下采样和实验追踪工具等，并实现了日志与可视化工具。其框架已在三个叙事类fMRI数据集上进行了多种编码模型的拟合与方法学探索（如TR内token使用、血流时滞、抑制信息泄漏、头动影响等）。

Result: 证明了LITcoder对不同数据集和编码模型的可扩展性和通用性，突出了一些方法选择（如如何对TR内token处理、血流时滞建模、合理的数据划分及头动控制）的重要性，这些对预测效果显著影响。

Conclusion: LITcoder有效降低了神经编码模型实现的技术壁垒，使跨模型/数据集的系统比较与方法学严谨性强化更为便捷，并推动了高质量脑活动预测模型的快速发展。

Abstract: We introduce LITcoder, an open-source library for building and benchmarking
neural encoding models. Designed as a flexible backend, LITcoder provides
standardized tools for aligning continuous stimuli (e.g., text and speech) with
brain data, transforming stimuli into representational features, mapping those
features onto brain data, and evaluating the predictive performance of the
resulting model on held-out data. The library implements a modular pipeline
covering a wide array of methodological design choices, so researchers can
easily compose, compare, and extend encoding models without reinventing core
infrastructure. Such choices include brain datasets, brain regions, stimulus
feature (both neural-net-based and control, such as word rate), downsampling
approaches, and many others. In addition, the library provides built-in
logging, plotting, and seamless integration with experiment tracking platforms
such as Weights & Biases (W&B). We demonstrate the scalability and versatility
of our framework by fitting a range of encoding models to three story listening
datasets: LeBel et al. (2023), Narratives, and Little Prince. We also explore
the methodological choices critical for building encoding models for continuous
fMRI data, illustrating the importance of accounting for all tokens in a TR
scan (as opposed to just taking the last one, even when contextualized),
incorporating hemodynamic lag effects, using train-test splits that minimize
information leakage, and accounting for head motion effects on encoding model
predictivity. Overall, LITcoder lowers technical barriers to encoding model
implementation, facilitates systematic comparisons across models and datasets,
fosters methodological rigor, and accelerates the development of high-quality
high-performance predictive models of brain activity.
  Project page: https://litcoder-brain.github.io

</details>


### [92] [Target-oriented Multimodal Sentiment Classification with Counterfactual-enhanced Debiasing](https://arxiv.org/abs/2509.09160)
*Zhiyue Liu,Fanrong Ma,Xin Ling*

Main category: cs.CL

TL;DR: 本文提出了一种用于目标导向多模态情感分类的反事实增强去偏框架，通过对图片-文本对进行反事实数据增强，并结合自适应对比学习，有效缓解了文本中的词级上下文偏见，提高了模型对情感特定目标的区分能力，实验结果优于现有先进方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态情感分类方法过度依赖文本内容，忽视了数据集中的词级上下文偏见，导致模型学到虚假的相关性，影响分类准确性。为此，作者希望减弱这些偏见，提高模型的泛化能力。

Method: 方法包括两部分：1）提出反事实数据增强策略，通过对图文配对的情感相关因果特征进行最小化修改，生成细节匹配的新样本，引导模型关注真正与情感相关的内容；2）提出自适应去偏对比学习机制，增强模型从反事实数据中提取鲁棒特征的能力，降低受有偏词影响。

Result: 在多个公开基准数据集上，该方法在情感分类准确率上显著优于当前主流方法，表明所提策略能有效减弱偏见并提升性能。

Conclusion: 本文所提的反事实增强去偏多模态情感分类方法不仅有效减弱了词级偏见的影响，还提升了模型对目标情感的捕捉能力，在多个数据集上表现优异，有较好应用和推广价值。

Abstract: Target-oriented multimodal sentiment classification seeks to predict
sentiment polarity for specific targets from image-text pairs. While existing
works achieve competitive performance, they often over-rely on textual content
and fail to consider dataset biases, in particular word-level contextual
biases. This leads to spurious correlations between text features and output
labels, impairing classification accuracy. In this paper, we introduce a novel
counterfactual-enhanced debiasing framework to reduce such spurious
correlations. Our framework incorporates a counterfactual data augmentation
strategy that minimally alters sentiment-related causal features, generating
detail-matched image-text samples to guide the model's attention toward content
tied to sentiment. Furthermore, for learning robust features from
counterfactual data and prompting model decisions, we introduce an adaptive
debiasing contrastive learning mechanism, which effectively mitigates the
influence of biased words. Experimental results on several benchmark datasets
show that our proposed method outperforms state-of-the-art baselines.

</details>


### [93] [EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs](https://arxiv.org/abs/2509.09174)
*Yuhao Zhang,Yuhao Du,Zhanchen Dai,Xiangnan Ma,Kaiqi Kou,Benyou Wang,Haizhou Li*

Main category: cs.CL

TL;DR: 本文介绍了一种新的语音大语言模型（SLLM）训练方法EchoX，通过结合声学和语义表示，显著提升了模型的知识和推理能力，在多个知识问答基准上取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 传统的语音大语言模型（SLLM）由于沿用文本大语言模型（LLM）的训练思路，导致其知识推理能力下降。作者认为，主要原因是现有训练方法未能有效弥合语音特征和语义特征之间的鸿沟。

Method: 作者提出了EchoX方法，该方法充分利用语义表示，并动态生成语音训练目标，将语音与语义学习过程有机结合，有助于实现更精准的语音推理和知识迁移。

Result: 通过约6000小时训练数据，EchoX在多个基于知识的问答基准测试中展现出先进性能，说明该方法在保持推理能力的同时，提升了语音LLM整体效果。

Conclusion: EchoX有效弥合了语音与语义间的特征鸿沟，显著改善了SLLM在知识推理方面的表现，为增强型语音大语言模型的训练提供了新的思路。

Abstract: Speech-to-speech large language models (SLLMs) are attracting increasing
attention. Derived from text-based large language models (LLMs), SLLMs often
exhibit degradation in knowledge and reasoning capabilities. We hypothesize
that this limitation arises because current training paradigms for SLLMs fail
to bridge the acoustic-semantic gap in the feature representation space. To
address this issue, we propose EchoX, which leverages semantic representations
and dynamically generates speech training targets. This approach integrates
both acoustic and semantic learning, enabling EchoX to preserve strong
reasoning abilities as a speech LLM. Experimental results demonstrate that
EchoX, with about six thousand hours of training data, achieves advanced
performance on multiple knowledge-based question-answering benchmarks. The
project is available at https://github.com/FreedomIntelligence/EchoX.

</details>


### [94] [Efficient Trie-based Biasing using K-step Prediction for Rare Word Recognition](https://arxiv.org/abs/2509.09196)
*Chin Yuen Kwok,Jia Qi yip*

Main category: cs.CL

TL;DR: 本文提出了一种新方法，通过让ASR模型进行多步预测，提升了罕见词的识别率，并显著减少了词错误率。


<details>
  <summary>Details</summary>
Motivation: 传统的Trie-based上下文偏置方法在解码过程中提升罕见词的输出概率，但其撤销奖励的机制仅限于beam search，并且在大解码器模型中计算量大，效率低。作者希望找出一种能提高效率且无需撤销步骤的方法。

Method: 作者提出通过适配ASR模型，使其能够提前预测多个步骤（即look ahead预测），从而更准确地判断部分假设是否会导致完整罕见词的生成。具体做法为微调Whisper模型，仅用10小时的合成数据。

Result: 在NSC Part 2测试集上，提出方法使词错误率由30.86%降至12.19%。

Conclusion: 多步预测的ASR模型极大提升了罕见词的识别能力，省略了计算昂贵的撤销步骤，在极小规模的数据微调下也能带来显著性能提升。

Abstract: Contextual biasing improves rare word recognition of ASR models by
prioritizing the output of rare words during decoding. A common approach is
Trie-based biasing, which gives "bonus scores" to partial hypothesis (e.g.
"Bon") that may lead to the generation of the rare word (e.g. "Bonham"). If the
full word ("Bonham") isn't ultimately recognized, the system revokes those
earlier bonuses. This revocation is limited to beam search and is
computationally expensive, particularly for models with large decoders. To
overcome these limitations, we propose adapting ASR models to look ahead and
predict multiple steps at once. This avoids the revocation step entirely by
better estimating whether a partial hypothesis will lead to the generation of
the full rare word. By fine-tuning Whisper with only 10 hours of synthetic
data, our method reduces the word error rate on the NSC Part 2 test set from
30.86% to 12.19%.

</details>


### [95] [Improving Synthetic Data Training for Contextual Biasing Models with a Keyword-Aware Cost Function](https://arxiv.org/abs/2509.09197)
*Chin Yuen Kwok,Jia Qi Yip,Eng Siong Chng*

Main category: cs.CL

TL;DR: 通过引入针对稀有词的上下文偏置模块及新的关键字感知损失函数，极大提升了ASR系统对稀有词的识别准确率，将Word Error Rate从29.71%降至11.81%。


<details>
  <summary>Details</summary>
Motivation: 自动语音识别（ASR）系统在识别稀有词方面表现不佳，尤其在训练数据中这些词极少或不存在时。如何有效提升ASR对稀有词的识别能力，是该研究的主要动机。

Method: 本论文提出在ASR模型架构中加入上下文偏置模块，并设计了关键字感知损失函数。损失包含：1）针对偏置词的掩蔽交叉熵用于提升偏置词预测；2）二分类损失用于检测偏置词位置。这两个损失项协同优化模型对稀有词的识别能力，并使用合成数据对Whisper模型进行微调。

Result: 采用该方法，将Whisper模型在NSC Part 2测试集上的词错误率（WER）从原来的29.71%显著降低至11.81%。

Conclusion: 结合上下文偏置与关键字感知损失函数有效提升了ASR系统对稀有词的识别性能，为解决实际应用中稀有词识别难题提供了可行且有效的技术路径。

Abstract: Rare word recognition can be improved by adapting ASR models to synthetic
data that includes these words. Further improvements can be achieved through
contextual biasing, which trains and adds a biasing module into the model
architecture to prioritize rare words. While training the module on synthetic
rare word data is more effective than using non-rare-word data, it can lead to
overfitting due to artifacts in the synthetic audio. To address this, we
enhance the TCPGen-based contextual biasing approach and propose a
keyword-aware loss function that additionally focuses on biased words when
training biasing modules. This loss includes a masked cross-entropy term for
biased word prediction and a binary classification term for detecting biased
word positions. These two terms complementarily support the decoding of biased
words during inference. By adapting Whisper to 10 hours of synthetic data, our
method reduced the word error rate on the NSC Part 2 test set from 29.71% to
11.81%.

</details>


### [96] [GmSLM : Generative Marmoset Spoken Language Modeling](https://arxiv.org/abs/2509.09198)
*Talia Sternberg,Michael London,David Omer,Yossi Adi*

Main category: cs.CL

TL;DR: 本文提出了一种针对狨猴（Marmoset）复杂发声交流的生成式语言建模系统GmSLM，并证明其优于基于人类语音的模型，有助于进一步研究灵长类动物语言与大脑活动的关系。


<details>
  <summary>Details</summary>
Motivation: 传统认为非人类灵长类动物的发声交流完全由先天决定，缺乏学习与复杂特征。然而，狨猴展现出了类似人类的语言行为，比如轮流对话、用发声标记他人，为研究语言及其神经基础提供了机会。目前，研究非人类动物发声通信的模型有限，且难以直接套用人类自然语言处理的方法。

Method: 作者开发了GmSLM，一个针对狨猴发声的生成模型流程。该方法包括了无监督的“零样本”评估指标，结合野外采集数据和弱标记对话数据，对模型进行衡量和测试，并与基于人类语音的基线模型进行了比较。

Result: GmSLM生成的狨猴发声在音响学特征上与真实样本高度一致，且在多项下游任务中表现优异。模型能够有效区分真实对话与人工生成对话，显示出很强的实用性。

Conclusion: 完全无监督的GmSLM为狨猴以及其它动物声音通信与脑活动的关系研究提供了实用新工具，未来可推动神经科学、生物声学和进化生物学领域相关研究。

Abstract: Marmoset monkeys exhibit complex vocal communication, challenging the view
that nonhuman primates vocal communication is entirely innate, and show similar
features of human speech, such as vocal labeling of others and turn-taking.
Studying their vocal communication offers a unique opportunity to link it with
brain activity-especially given the difficulty of accessing the human brain in
speech and language research. Since Marmosets communicate primarily through
vocalizations, applying standard LLM approaches is not straightforward. We
introduce Generative Marmoset Spoken Language Modeling (GmSLM), an optimized
spoken language model pipeline for Marmoset vocal communication. We designed a
novel zero-shot evaluation metrics using unsupervised in-the-wild data,
alongside weakly labeled conversational data, to assess GmSLM and demonstrate
its advantage over a basic human-speech-based baseline. GmSLM generated
vocalizations closely matched real resynthesized samples acoustically and
performed well on downstream tasks. Despite being fully unsupervised, GmSLM
effectively distinguish real from artificial conversations and may support
further investigations of the neural basis of vocal communication and provides
a practical framework linking vocalization and brain activity. We believe GmSLM
stands to benefit future work in neuroscience, bioacoustics, and evolutionary
biology. Samples are provided under: pages.cs.huji.ac.il/adiyoss-lab/GmSLM.

</details>


### [97] [CCF: A Context Compression Framework for Efficient Long-Sequence Language Modeling](https://arxiv.org/abs/2509.09199)
*Wenhao Li,Bangcheng Sun,Weihao Ye,Tianyi Zhang,Daohai Yu,Fei Chao,Rongrong Ji*

Main category: cs.CL

TL;DR: 提出了一种高效长上下文语言建模的上下文压缩框架CCF，可大幅降低计算和内存开销，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 当前将语言模型扩展到更长上下文虽有助于捕捉复杂语义，但通常带来巨大的计算和内存负担，制约了实际应用。需要一种有效压缩冗余信息、提升建模效率的新方法。

Method: CCF框架通过分段语义聚合与关键值记忆编码相结合，学习层次化潜在表示，压缩输入冗余但保留全局语义。训练时结合增量分段解码和稀疏蓄水池采样策略，进一步减低内存消耗。

Result: 在多个长上下文语言建模基准上，CCF在高压缩比下取得了与主流方法相当的困惑度表现，且显著提升了推理吞吐量与内存效率。

Conclusion: 结构化压缩策略有望实现可扩展、高效的长上下文语言建模，为模型处理长文本提供了新的可能。

Abstract: Scaling language models to longer contexts is essential for capturing rich
dependencies across extended discourse. However, na\"ive context extension
imposes significant computational and memory burdens, often resulting in
inefficiencies during both training and inference. In this work, we propose
CCF, a novel context compression framework designed to enable efficient
long-context modeling by learning hierarchical latent representations that
preserve global semantics while aggressively reducing input redundancy. CCF
integrates segment-wise semantic aggregation with key-value memory encoding,
forming compact representations that support accurate reconstruction and
long-range understanding. To further enhance scalability, we introduce a
training-efficient optimization strategy that couples incremental segment
decoding with sparse reservoir sampling, substantially reducing memory overhead
without degrading performance. Empirical results on multiple long-context
language modeling benchmarks demonstrate that CCF achieves competitive
perplexity under high compression ratios, and significantly improves throughput
and memory efficiency compared to existing approaches. These findings highlight
the potential of structured compression for scalable and effective long-context
language modeling.

</details>


### [98] [Reading Between the Lines: Classifying Resume Seniority with Large Language Models](https://arxiv.org/abs/2509.09229)
*Matan Cohen,Shira Shani,Eden Menahem,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.CL

TL;DR: 本研究评估大型语言模型（LLM）在简历中判别候选人资历等级的准确性，提出包含真实及合成例子的混合数据集，并探讨模型对夸大与隐晦资历表述的检测能力。


<details>
  <summary>Details</summary>
Motivation: 由于简历普遍存在夸大经验和模糊自述，人工判定候选人资历等级变得困难。需要自动化手段提高评估准确性，降低人为偏见。

Method: 采用经过微调的BERT等大型语言模型，利用包含真实与模拟夸大、低调表达的混合数据集，对简历资历分类进行实验，重点测试模型对细微语言线索的识别能力。

Result: 模型能够部分检测到简历中的资历夸大和隐含资历表现，对自动化评估系统的准确性和公平性有所提升。

Conclusion: 大型语言模型可有效助力候选人资历评估，减少自我包装带来的bias，同时混合数据集提升了评测鲁棒性，对未来AI招聘系统有借鉴意义。

Abstract: Accurately assessing candidate seniority from resumes is a critical yet
challenging task, complicated by the prevalence of overstated experience and
ambiguous self-presentation. In this study, we investigate the effectiveness of
large language models (LLMs), including fine-tuned BERT architectures, for
automating seniority classification in resumes. To rigorously evaluate model
performance, we introduce a hybrid dataset comprising both real-world resumes
and synthetically generated hard examples designed to simulate exaggerated
qualifications and understated seniority. Using the dataset, we evaluate the
performance of Large Language Models in detecting subtle linguistic cues
associated with seniority inflation and implicit expertise. Our findings
highlight promising directions for enhancing AI-driven candidate evaluation
systems and mitigating bias introduced by self-promotional language. The
dataset is available for the research community at https://bit.ly/4mcTovt

</details>


### [99] [Agentic LLMs for Question Answering over Tabular Data](https://arxiv.org/abs/2509.09234)
*Rishit Tyagi,Mohit Gupta,Rahul Bouri*

Main category: cs.CL

TL;DR: 本文提出使用大语言模型（LLM）辅助的多阶段NL-to-SQL系统，有效提升了表格问答（Table QA）在大规模多领域数据集下的准确率。


<details>
  <summary>Details</summary>
Motivation: 表格问答任务由于真实世界表格结构和数据类型复杂，如何让模型准确理解问题并生成可用于查询的SQL语句，是当前面临的难题。SemEval 2025提出了新的大规模基准，推动该领域进步。

Method: 作者设计了多阶段流水线：样例选择、SQL生成、答案提取、结果校验和迭代优化，利用GPT-4o、GPT-4o-mini和DeepSeek v2:16b等大语言模型动态生成SQL语句完成问答。

Result: 在DataBench QA上达到70.5%的准确率，在DataBench Lite QA上达到71.6%，远超基线26%和27%。

Conclusion: 大语言模型驱动下的NL-to-SQL方法在复杂表格问答任务中表现出强大能力，同时论文也分析了方法的优势与局限，为后续研究提供了参考。

Abstract: Question Answering over Tabular Data (Table QA) presents unique challenges
due to the diverse structure, size, and data types of real-world tables. The
SemEval 2025 Task 8 (DataBench) introduced a benchmark composed of large-scale,
domain-diverse datasets to evaluate the ability of models to accurately answer
structured queries. We propose a Natural Language to SQL (NL-to-SQL) approach
leveraging large language models (LLMs) such as GPT-4o, GPT-4o-mini, and
DeepSeek v2:16b to generate SQL queries dynamically. Our system follows a
multi-stage pipeline involving example selection, SQL query generation, answer
extraction, verification, and iterative refinement. Experiments demonstrate the
effectiveness of our approach, achieving 70.5\% accuracy on DataBench QA and
71.6\% on DataBench Lite QA, significantly surpassing baseline scores of 26\%
and 27\% respectively. This paper details our methodology, experimental
results, and alternative approaches, providing insights into the strengths and
limitations of LLM-driven Table QA.

</details>


### [100] [From scratch to silver: Creating trustworthy training data for patent-SDG classification using Large Language Models](https://arxiv.org/abs/2509.09303)
*Grazia Sveva Ascione,Nicolò Tamagnone*

Main category: cs.CL

TL;DR: 本论文提出了一种基于专利与SDG（可持续发展目标）对齐的新方法，解决了缺乏大规模标注数据的问题，通过弱监督和大型语言模型提升了专利与SDG标签的匹配效果，并在多个验证场景下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有将专利与SDG相关性归类的方法依赖于人工标注、关键词及引文启发式，难以扩展且泛化性有限，而如何高效、准确地追踪创新应对全球挑战变得愈发重要。

Method: 将专利归类为SDG问题建模为弱监督任务：利用专利对已SDG标签科学文献的非专利引文（NPL）作为初始噪声信号，结合大语言模型抽取专利文档和SDG论文中的结构化概念（如功能、解决方案与应用），按专利本体进行语义匹配并以排序检索方式计算相似度。通过定制正样本损失校准标签函数，提高新关联发现能力，最终构建银标准、多标签的专利-SDG对齐数据集，用于训练多标签回归模型。

Result: 方法在两方面进行了验证：一是内部分割实验对比变换器模型和零样本大语言模型，效果更优；二是基于专利引文、共发明人、共申请人的网络模块度，发现该方法的标签在主题、认知和组织层面上的一致性均优于传统专利技术分类。

Conclusion: 弱监督结合语义对齐的方案可在大规模上提升专利与SDG目标的匹配能力，为追踪和分析全球可持续创新提供了更有效手段。

Abstract: Classifying patents by their relevance to the UN Sustainable Development
Goals (SDGs) is crucial for tracking how innovation addresses global
challenges. However, the absence of a large, labeled dataset limits the use of
supervised learning. Existing methods, such as keyword searches, transfer
learning, and citation-based heuristics, lack scalability and generalizability.
This paper frames patent-to-SDG classification as a weak supervision problem,
using citations from patents to SDG-tagged scientific publications (NPL
citations) as a noisy initial signal. To address its sparsity and noise, we
develop a composite labeling function (LF) that uses large language models
(LLMs) to extract structured concepts, namely functions, solutions, and
applications, from patents and SDG papers based on a patent ontology.
Cross-domain similarity scores are computed and combined using a rank-based
retrieval approach. The LF is calibrated via a custom positive-only loss that
aligns with known NPL-SDG links without penalizing discovery of new SDG
associations. The result is a silver-standard, soft multi-label dataset mapping
patents to SDGs, enabling the training of effective multi-label regression
models. We validate our approach through two complementary strategies: (1)
internal validation against held-out NPL-based labels, where our method
outperforms several baselines including transformer-based models, and zero-shot
LLM; and (2) external validation using network modularity in patent citation,
co-inventor, and co-applicant graphs, where our labels reveal greater thematic,
cognitive, and organizational coherence than traditional technological
classifications. These results show that weak supervision and semantic
alignment can enhance SDG classification at scale.

</details>


### [101] [MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems](https://arxiv.org/abs/2509.09360)
*Channdeth Sok,David Luz,Yacine Haddam*

Main category: cs.CL

TL;DR: 本文提出了MetaRAG，一种用于检验增强检索生成（RAG）系统中幻觉（即内容与事实不符）的框架，能在无监督、黑盒、实时的情况下检测并定位幻觉。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在企业中的广泛应用，幻觉问题（模型自信但事实错误的输出）影响了其可靠性，尤其是在RAG系统中，回答应与检索到的证据一致。目前大部分检测手段侧重独立LLM，不适应RAG环境。

Method: MetaRAG采用变形测试，包括：1）将答案分解为原子事实；2）针对每个事实，生成同义和反义词变体；3）将这些变体与检索上下文比对，同义词应被支持，反义词应被否定；4）将不一致性聚合为幻觉分数，并定位具体虚假内容。

Result: 在一套企业级数据集上实验，MetaRAG能有效检测RAG系统中的幻觉，并可为对话代理安全部署提供支持。

Conclusion: MetaRAG能在不需标注数据和不接触模型内部的情况下，灵活检测和定位RAG系统中的幻觉，适合用于身份敏感和高风险领域。附带提出但未实验的话题级安全部署设计。

Abstract: Large Language Models (LLMs) are increasingly deployed in enterprise
applications, yet their reliability remains limited by hallucinations, i.e.,
confident but factually incorrect information. Existing detection approaches,
such as SelfCheckGPT and MetaQA, primarily target standalone LLMs and do not
address the unique challenges of Retrieval-Augmented Generation (RAG) systems,
where responses must be consistent with retrieved evidence. We therefore
present MetaRAG, a metamorphic testing framework for hallucination detection in
Retrieval-Augmented Generation (RAG) systems. MetaRAG operates in a real-time,
unsupervised, black-box setting, requiring neither ground-truth references nor
access to model internals, making it suitable for proprietary and high-stakes
domains. The framework proceeds in four stages: (1) decompose answers into
atomic factoids, (2) generate controlled mutations of each factoid using
synonym and antonym substitutions, (3) verify each variant against the
retrieved context (synonyms are expected to be entailed and antonyms
contradicted), and (4) aggregate penalties for inconsistencies into a
response-level hallucination score. Crucially for identity-aware AI, MetaRAG
localizes unsupported claims at the factoid span where they occur (e.g.,
pregnancy-specific precautions, LGBTQ+ refugee rights, or labor eligibility),
allowing users to see flagged spans and enabling system designers to configure
thresholds and guardrails for identity-sensitive queries. Experiments on a
proprietary enterprise dataset illustrate the effectiveness of MetaRAG for
detecting hallucinations and enabling trustworthy deployment of RAG-based
conversational agents. We also outline a topic-based deployment design that
translates MetaRAG's span-level scores into identity-aware safeguards; this
design is discussed but not evaluated in our experiments.

</details>


### [102] [Modelling Analogies and Analogical Reasoning: Connecting Cognitive Science Theory and NLP Research](https://arxiv.org/abs/2509.09381)
*Molly R Petersen,Claire E Stevenson,Lonneke van der Plas*

Main category: cs.CL

TL;DR: 本文将人类类比推理的认知理论与自然语言处理（NLP）领域的研究相联系，强调在NLP研究中应更多关注关系层面的理解。


<details>
  <summary>Details</summary>
Motivation: 类比推理对人类认知至关重要，但自然语言处理研究往往未从认知角度探讨类比推理。作者希望弥合认知科学与NLP之间的理论空白，并推动NLP领域关注文本中的关系理解。

Method: 文章首先综述了认知科学中关于类比推理过程的核心理论，然后将其与NLP中的相应概念进行对比分析。作者还讨论了这些认知理论如何适用于NLP研究中的若干重大挑战。

Result: 作者发现，认知科学对类比推理过程的理解可以为NLP中的诸多问题提供新的视角，尤其是在推动文本的关系层面理解方面具有潜力。当前NLP研究过于依赖实体层面的相似性，未能充分利用类比推理的认知基础。

Conclusion: 将类比推理的认知理论引入NLP研究，有助于优化文本关系理解，激励研究者开发更加注重关系建模的方法，从而提升NLP模型的认知合理性和理解深度。

Abstract: Analogical reasoning is an essential aspect of human cognition. In this
paper, we summarize key theory about the processes underlying analogical
reasoning from the cognitive science literature and relate it to current
research in natural language processing. While these processes can be easily
linked to concepts in NLP, they are generally not viewed through a cognitive
lens. Furthermore, we show how these notions are relevant for several major
challenges in NLP research, not directly related to analogy solving. This may
guide researchers to better optimize relational understanding in text, as
opposed to relying heavily on entity-level similarity.

</details>


### [103] [Hierarchical Bracketing Encodings Work for Dependency Graphs](https://arxiv.org/abs/2509.09388)
*Ana Ezquerro,Carlos Gómez-Rodríguez,David Vilares*

Main category: cs.CL

TL;DR: 本文提出一种将层次括号编码用于依存图解析的新方法，可高效线性时间解析并有效保留图结构信息，实验显示新方法在多语言多形式基准上表现优良。


<details>
  <summary>Details</summary>
Motivation: 现有依存图线性化方法标签空间大，结构信息易丢失，且高效处理循环、重入和空节点有难度。作者希望设计一种既能减小标签空间，又能保留丰富结构信息的编码方式，提高解析性能。

Method: 作者提出将依存图编码为序列，采用层次括号方式，能够用n次标注操作高效解析图。该方法支持表示重入、循环和空节点，并极大减少了标签空间。通过多语言多形式基准进行实验比较。

Result: 在多语言和多形式基准测试上，本方法在准确匹配上取得有竞争力的成绩，并在多个方面超过现有方法，尤其在最优匹配准确率上有持续提升。

Conclusion: 层次括号编码为依存图解析提供了一种高效、结构信息保留与标签空间优化兼备的新方案，综合性能优于现有图线性化方法。

Abstract: We revisit hierarchical bracketing encodings from a practical perspective in
the context of dependency graph parsing. The approach encodes graphs as
sequences, enabling linear-time parsing with $n$ tagging actions, and still
representing reentrancies, cycles, and empty nodes. Compared to existing graph
linearizations, this representation substantially reduces the label space while
preserving structural information. We evaluate it on a multilingual and
multi-formalism benchmark, showing competitive results and consistent
improvements over other methods in exact match accuracy.

</details>


### [104] [GrACE: A Generative Approach to Better Confidence Elicitation in Large Language Models](https://arxiv.org/abs/2509.09438)
*Zhaohan Zhang,Ziquan Liu,Ioannis Patras*

Main category: cs.CL

TL;DR: 本文提出了一种名为GrACE的新方法，能够高效、可靠地评估大语言模型（LLMs）的置信度，从而提升其在现实高风险场景中的安全性与实用性。该方法无需大量计算资源，且优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在医疗、金融等高风险领域部署LLMs时，评估其回答的置信度对于AI安全至关重要。现有置信度获取方法要么计算成本高，要么校准效果差，难以实际应用。为此，亟需一种高效且可靠的置信度获取方案。

Method: GrACE方法创新性地通过LLM最后一层隐藏状态与新增特殊token嵌入之间的相似性，实时生成置信度分数。作者对模型进行微调，以利用与准确率关联的校准目标对置信度进行校准，无需额外采样或辅助模型。

Result: 在三个LLM和两个基准数据集上的实验显示，GrACE在开放式生成任务中的判别能力和置信度校准均优于六种对比方法，同时无需额外的采样或辅助模型。此外，通过GrACE的置信度，测试时采样量明显减少，但决策准确率反而提升。

Conclusion: GrACE提供了一种可扩展、可靠且实时的LLM置信度评估解决方案，适用于实际部署场景。该方法不仅提升了最终决策准确性，也大幅减少了测试时的样本需求，展现出极强的实用潜力。

Abstract: Assessing the reliability of Large Language Models (LLMs) by confidence
elicitation is a prominent approach to AI safety in high-stakes applications,
such as healthcare and finance. Existing methods either require expensive
computational overhead or suffer from poor calibration, making them impractical
and unreliable for real-world deployment. In this work, we propose GrACE, a
Generative Approach to Confidence Elicitation that enables scalable and
reliable confidence elicitation for LLMs. GrACE adopts a novel mechanism in
which the model expresses confidence by the similarity between the last hidden
state and the embedding of a special token appended to the vocabulary, in
real-time. We fine-tune the model for calibrating the confidence with
calibration targets associated with accuracy. Experiments with three LLMs and
two benchmark datasets show that the confidence produced by GrACE achieves the
best discriminative capacity and calibration on open-ended generation tasks,
outperforming six competing methods without resorting to additional sampling or
an auxiliary model. Moreover, we propose two strategies for improving test-time
scaling based on confidence induced by GrACE. Experimental results show that
using GrACE not only improves the accuracy of the final decision but also
significantly reduces the number of required samples in the test-time scaling
scheme, indicating the potential of GrACE as a practical solution for deploying
LLMs with scalable, reliable, and real-time confidence estimation.

</details>


### [105] [Mitigating Language Barriers in Education: Developing Multilingual Digital Learning Materials with Machine Translation](https://arxiv.org/abs/2509.09473)
*Lucie Poláková,Martin Popel,Věra Kloudová,Michal Novák,Mariia Anisimova,Jiří Balhar*

Main category: cs.CL

TL;DR: EdUKate项目通过将捷克教育内容翻译成多语言，为小学和中学开发多媒体互动学习资料，侧重捷乌机器翻译系统开发与评估，满足非捷克语学生需求，成果免费开放。


<details>
  <summary>Details</summary>
Motivation: 捷克有大量非捷克语学生，需要多语言学习资料提升他们的学习体验；现有自动翻译系统在教育领域和多媒体内容处理上存在不足，需定制化解决方案。

Method: 项目跨足数字教育、语言学、翻译学及机器翻译领域，结合学术机构与教育出版商合作，把约9000条多模态互动练习从捷克语翻译成乌克兰语、英语和德语；研发专用捷乌机器翻译系统，优化对XML、PDF格式内容和科技术语的处理，并评估教师需求及系统效果。

Result: 完成了教师需求调研，开发并评估了定制化捷乌机器翻译系统，实现多语言学习资料的生成与部署；所有应用成果向学生和教育工作者免费开放。

Conclusion: EdUKate项目有效推动了捷克基础教育内容的多语言普及，提升了非捷克语学生的学习支持，定制化机器翻译系统在教育场景下表现良好，相关资源具有开放性和广泛应用价值。

Abstract: The EdUKate project combines digital education, linguistics, translation
studies, and machine translation to develop multilingual learning materials for
Czech primary and secondary schools. Launched through collaboration between a
major Czech academic institution and the country's largest educational
publisher, the project is aimed at translating up to 9,000 multimodal
interactive exercises from Czech into Ukrainian, English, and German for an
educational web portal. It emphasizes the development and evaluation of a
direct Czech-Ukrainian machine translation system tailored to the educational
domain, with special attention to processing formatted content such as XML and
PDF and handling technical and scientific terminology. We present findings from
an initial survey of Czech teachers regarding the needs of non-Czech-speaking
students and describe the system's evaluation and implementation on the web
portal. All resulting applications are freely available to students, educators,
and researchers.

</details>


### [106] [Towards Explainable Job Title Matching: Leveraging Semantic Textual Relatedness and Knowledge Graphs](https://arxiv.org/abs/2509.09522)
*Vadim Zadykian,Bruno Andrade,Haithem Afli*

Main category: cs.CL

TL;DR: 本文提出一种结合稠密句子嵌入和领域知识图谱的自监督混合架构，用于提升职位名称匹配中的语义相关建模能力，并通过分段评分进行精细化评估。该方法在高语义相关性区域取得显著改进，为HR系统提供更公平、可解释、上下文相关的匹配能力。


<details>
  <summary>Details</summary>
Motivation: 在简历推荐系统中，职位名称匹配常因词汇重叠度低或误导性高，导致传统的纯文本匹配方式难以准确刻画语义相关性。因此亟需新的方法提升匹配的准确性与可解释性。

Method: 提出一种自监督混合架构，将稠密句子嵌入（如SBERT）与领域知识图谱通过图神经网络进行结合，并且创新性地将语义相关性得分划分为低、中、高三个区域，做分区性能评估。

Result: 实验表明，经过知识图谱增强的SBERT模型在高语义相关度区域RMSE降低25%，优于多种基线。此外，分区评估揭示了模型在不同语义相关性区间的表现差异。

Conclusion: 将知识图谱结合文本嵌入能够有效提升职位匹配的准确性与可解释性，区域化评估有助于识别模型的具体优劣，为HR应用中的模型选择和解释性提供有力支持。

Abstract: Semantic Textual Relatedness (STR) captures nuanced relationships between
texts that extend beyond superficial lexical similarity. In this study, we
investigate STR in the context of job title matching - a key challenge in
resume recommendation systems, where overlapping terms are often limited or
misleading. We introduce a self-supervised hybrid architecture that combines
dense sentence embeddings with domain-specific Knowledge Graphs (KGs) to
improve both semantic alignment and explainability. Unlike previous work that
evaluated models on aggregate performance, our approach emphasizes data
stratification by partitioning the STR score continuum into distinct regions:
low, medium, and high semantic relatedness. This stratified evaluation enables
a fine-grained analysis of model performance across semantically meaningful
subspaces. We evaluate several embedding models, both with and without KG
integration via graph neural networks. The results show that fine-tuned SBERT
models augmented with KGs produce consistent improvements in the high-STR
region, where the RMSE is reduced by 25% over strong baselines. Our findings
highlight not only the benefits of combining KGs with text embeddings, but also
the importance of regional performance analysis in understanding model
behavior. This granular approach reveals strengths and weaknesses hidden by
global metrics, and supports more targeted model selection for use in Human
Resources (HR) systems and applications where fairness, explainability, and
contextual matching are essential.

</details>


### [107] [DeMeVa at LeWiDi-2025: Modeling Perspectives with In-Context Learning and Label Distribution Learning](https://arxiv.org/abs/2509.09524)
*Daniil Ignatev,Nan Li,Hugh Mee Wong,Anh Dang,Shane Kaszefski Yaschuk*

Main category: cs.CL

TL;DR: 本文介绍了DeMeVa团队在LeWiDi 2025任务中的方法，重点探索了大语言模型的ICL和RoBERTa的LDL方法，并证明两者对于处理争议性标注和软标签预测都较有效。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习任务中，经常面临标注者之间的分歧（即相同数据被不同标注者以不同方式标注）。如何有效学习并利用这些不同观点，对提升模型泛化和公平性有重要意义。

Method: 作者分别探索了：1）基于大语言模型的in-context learning（ICL），通过不同示例采样策略，预测特定标注者的“观点主义”标注，并将预测聚合形成软标签；2）基于RoBERTa的label distribution learning（LDL）方法，比较了多种微调方法，以评估其在软标签学习上的效果。

Result: 1）ICL能够有效预测特定标注者的标注，并且将这些预测聚合为软标签后，模型表现具有竞争力。2）LDL方法在软标签预测任务上具有良好前景。

Conclusion: ICL与LDL均为处理争议性/多元性标注的有效手段，尤其LDL方法值得观点主义研究社区进一步探索。

Abstract: This system paper presents the DeMeVa team's approaches to the third edition
of the Learning with Disagreements shared task (LeWiDi 2025; Leonardelli et
al., 2025). We explore two directions: in-context learning (ICL) with large
language models, where we compare example sampling strategies; and label
distribution learning (LDL) methods with RoBERTa (Liu et al., 2019b), where we
evaluate several fine-tuning methods. Our contributions are twofold: (1) we
show that ICL can effectively predict annotator-specific annotations
(perspectivist annotations), and that aggregating these predictions into soft
labels yields competitive performance; and (2) we argue that LDL methods are
promising for soft label predictions and merit further exploration by the
perspectivist community.

</details>


### [108] [Prompting the Market? A Large-Scale Meta-Analysis of GenAI in Finance NLP (2022-2025)](https://arxiv.org/abs/2509.09544)
*Paolo Pedinotti,Peter Baumann,Nathan Jessurun,Leslie Barrett,Enrico Santus*

Main category: cs.CL

TL;DR: 本文提出MetaGraph方法，通过知识图谱提取与分析，构建金融NLP领域研究趋势的结构化视图。


<details>
  <summary>Details</summary>
Motivation: LLM加速了金融NLP领域发展，形成了新的任务、数据集与数据源，但传统调研跟不上行业变化，需要自动化、结构化展示研究进展的方法。

Method: 作者制定了金融NLP本体体系，利用LLM驱动的信息抽取流程，从681篇文献中抽取知识图谱，进行趋势分析。

Result: 发现金融NLP经历了三个关键阶段：LLM初期采用与创新、对LLM局限性反思、外围技术与模块化系统融合的趋势。MetaGraph 能够揭示研究重点转变和方法创新。

Conclusion: MetaGraph为金融NLP及其它领域提供了一种可复用的方法，对研究进展的自动化映射和趋势分析具有重要意义。

Abstract: Large Language Models (LLMs) have rapidly reshaped financial NLP, enabling
new tasks and driving a proliferation of datasets and diversification of data
sources. Yet, this transformation has outpaced traditional surveys. In this
paper, we present MetaGraph, a generalizable methodology for extracting
knowledge graphs from scientific literature and analyzing them to obtain a
structured, queryable view of research trends. We define an ontology for
financial NLP research and apply an LLM-based extraction pipeline to 681 papers
(2022-2025), enabling large-scale, data-driven analysis. MetaGraph reveals
three key phases: early LLM adoption and task/dataset innovation; critical
reflection on LLM limitations; and growing integration of peripheral techniques
into modular systems. This structured view offers both practitioners and
researchers a clear understanding of how financial NLP has evolved -
highlighting emerging trends, shifting priorities, and methodological
shifts-while also demonstrating a reusable approach for mapping scientific
progress in other domains.

</details>


### [109] [Personality-Enhanced Social Recommendations in SAMI: Exploring the Role of Personality Detection in Matchmaking](https://arxiv.org/abs/2509.09583)
*Brittany Harbison,Samuel Taubman,Travis Taylor,Ashok. K. Goel*

Main category: cs.CL

TL;DR: 本文提出使用GPT的零样本能力，从论坛介绍帖中推断学生的“大五”人格特质，推动线上课程中学生更精准的社交匹配。初步结果显示这种做法对提升匹配系统有效性有潜力。


<details>
  <summary>Details</summary>
Motivation: 线上课程中，学生自然结交社交群体存在障碍，当前基于SAMI系统的匹配受到学生心理模型构建不完善的限制，尤其缺乏对学生个性的直观理解。作者希望通过识别人格特质，提高匹配系统的推荐相关性。

Method: 作者提出了一种利用GPT零样本（zero-shot）推断能力，从在线课程论坛的自我介绍帖中自动识别“大五人格”特质，并与已有的人格检测模型进行基准测试；同时，将该模型集成进SAMI的实体匹配系统，进行人格知情的社交推荐。

Result: 新模型在人格识别任务中表现良好，具备实际效力。集成后，个性特质可以与现有因素共同提升学生匹配的相关性和可能的互动质量，但效应的全面评估还需进一步实验验证。

Conclusion: 个性标签有望成为现有学生匹配系统的有效补充因素，提升线上课程中的社交互动和推荐效果。但需要进一步分析其对学生参与度和匹配质量的影响。

Abstract: Social connection is a vital part of learning, yet online course environments
present barriers to the organic formation of social groups. SAMI offers one
solution by facilitating student connections, but its effectiveness is
constrained by an incomplete Theory of Mind, limiting its ability to create an
effective mental model of a student. One facet of this is its inability to
intuit personality, which may influence the relevance of its recommendations.
To explore this, we propose a personality detection model utilizing GPTs
zero-shot capability to infer Big-Five personality traits from forum
introduction posts, often encouraged in online courses. We benchmark its
performance against established models, demonstrating its efficacy in this
task. Furthermore, we integrate this model into SAMIs entity-based matchmaking
system, enabling personality-informed social recommendations. Initial
integration suggests personality traits can complement existing matching
factors, though additional evaluation is required to determine their full
impact on student engagement and match quality.

</details>


### [110] [Fluent but Unfeeling: The Emotional Blind Spots of Language Models](https://arxiv.org/abs/2509.09593)
*Bangzhao Shu,Isha Joshi,Melissa Karnaze,Anh C. Pham,Ishita Kakkar,Sindhu Kothe,Arpine Hovasapian,Mai ElSherief*

Main category: cs.CL

TL;DR: 论文提出了EXPRESS基准数据集，专注于评估大模型在细粒度情感识别上与人类自我表述情感的一致性。结果显示，当前LLMs在捕捉细腻情感方面存在局限。


<details>
  <summary>Details</summary>
Motivation: 现有LLM情感识别研究多以粗分类为主，无法评估模型在更细粒度、接近真实人类自我表述情感上的表现，因此亟需更真实且复杂的评测基准。

Method: 构建了EXPRESS数据集，包含Reddit用户自述的251种细粒度情感标签，并设计了综合评估框架，将预测情感拆解为8种基础情感，以便与人类标签进行精细比较。此外，系统性测试多种主流LLM在不同设定下的表现，并做了定性分析。

Result: LLMs准确预测并贴合人类自述细腻情感依然困难。有些LLM能给出符合情感理论的词汇，但难以像人类一样结合上下文抓住细节。

Conclusion: 现有LLM在细粒度情感识别与人类自述情感的一致性方面仍有限制，未来需提升模型的情境理解能力。

Abstract: The versatility of Large Language Models (LLMs) in natural language
understanding has made them increasingly popular in mental health research.
While many studies explore LLMs' capabilities in emotion recognition, a
critical gap remains in evaluating whether LLMs align with human emotions at a
fine-grained level. Existing research typically focuses on classifying emotions
into predefined, limited categories, overlooking more nuanced expressions. To
address this gap, we introduce EXPRESS, a benchmark dataset curated from Reddit
communities featuring 251 fine-grained, self-disclosed emotion labels. Our
comprehensive evaluation framework examines predicted emotion terms and
decomposes them into eight basic emotions using established emotion theories,
enabling a fine-grained comparison. Systematic testing of prevalent LLMs under
various prompt settings reveals that accurately predicting emotions that align
with human self-disclosed emotions remains challenging. Qualitative analysis
further shows that while certain LLMs generate emotion terms consistent with
established emotion theories and definitions, they sometimes fail to capture
contextual cues as effectively as human self-disclosures. These findings
highlight the limitations of LLMs in fine-grained emotion alignment and offer
insights for future research aimed at enhancing their contextual understanding.

</details>


### [111] [LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death Determination](https://arxiv.org/abs/2509.09602)
*Yiqun T. Chen,Tyler H. McCormick,Li Liu,Abhirup Datta*

Main category: cs.CL

TL;DR: 本文提出了LA-VA管线，将大型语言模型（如GPT-5）与传统算法和嵌入式分类方法结合，提升了资源有限地区死因调查的准确率。GPT-5显著优于常规模型。


<details>
  <summary>Details</summary>
Motivation: 在缺乏医疗认证的资源有限地区，准确推断死亡原因极具挑战性，现有算法准确率有限，因此亟需更高效的死因预测工具。

Method: 作者构建了LA-VA流程，将大型语言模型（如GPT-5）、传统LCVA、文本嵌入和meta-learner集成等方法在PHMRC数据集上进行比较，并衡量它们在不同年龄组的预测表现。

Result: GPT-5分别在成人、儿童和新生儿组取得48.6%、50.5%、53.5%的准确率，比传统统计机器学习基线高出5-10%。

Conclusion: 简单应用LLM（如GPT-5）即能实质性提高死因预测准确性，对低资源环境下的全球健康监测具有重要意义。

Abstract: Verbal autopsy (VA) is a critical tool for estimating causes of death in
resource-limited settings where medical certification is unavailable. This
study presents LA-VA, a proof-of-concept pipeline that combines Large Language
Models (LLMs) with traditional algorithmic approaches and embedding-based
classification for improved cause-of-death prediction. Using the Population
Health Metrics Research Consortium (PHMRC) dataset across three age categories
(Adult: 7,580; Child: 1,960; Neonate: 2,438), we evaluate multiple approaches:
GPT-5 predictions, LCVA baseline, text embeddings, and meta-learner ensembles.
Our results demonstrate that GPT-5 achieves the highest individual performance
with average test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5%
(Neonate), outperforming traditional statistical machine learning baselines by
5-10%. Our findings suggest that simple off-the-shelf LLM-assisted approaches
could substantially improve verbal autopsy accuracy, with important
implications for global health surveillance in low-resource settings.

</details>


### [112] [Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing LLM-based Multi-Agent Systems](https://arxiv.org/abs/2509.09629)
*Minghang Zhu,Zhengliang Shi,Zhiwei Xu,Shiguang Wu,Lingjie Wang,Pengjie Ren,Zhaochun Ren,Zhumin Chen*

Main category: cs.CL

TL;DR: 提出了一种促进多智能体协作的新框架MOAT，实现了更高效的多智能体联合调优，在多个任务上提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统常独立微调各智能体，导致能力差距和协作不佳。因此需要新的方法提升智能体间的协调与整体性能。

Method: MOAT框架采用交替式联合调优方法，分为两阶段：一是优化规划智能体，使其能生成更有效指导的子目标序列；二是通过多样化子目标-动作对微调执行智能体，提升泛化能力。该过程循环进行，通过理论分析保证训练收敛性。

Result: 在六个基准任务上实验，MOAT在已见和未见任务上分别平均提升3.1%及4.4%，优于近期先进方法。

Conclusion: MOAT能够有效提升多智能体系统的协作能力和整体表现，为解决复杂任务提供了更优的技术路线。

Abstract: The advancement of large language models (LLMs) has enabled the construction
of multi-agent systems to solve complex tasks by dividing responsibilities
among specialized agents, such as a planning agent for subgoal generation and a
grounding agent for executing tool-use actions. Most existing methods typically
fine-tune these agents independently, leading to capability gaps among them
with poor coordination. To address this, we propose MOAT, a Multi-Agent Joint
Alignment Tuning framework that improves agents collaboration through iterative
alignment. MOAT alternates between two key stages: (1) Planning Agent
Alignment, which optimizes the planning agent to generate subgoal sequences
that better guide the grounding agent; and (2) Grounding Agent Improving, which
fine-tunes the grounding agent using diverse subgoal-action pairs generated by
the agent itself to enhance its generalization capablity. Theoretical analysis
proves that MOAT ensures a non-decreasing and progressively convergent training
process. Experiments across six benchmarks demonstrate that MOAT outperforms
state-of-the-art baselines, achieving average improvements of 3.1% on held-in
tasks and 4.4% on held-out tasks.

</details>


### [113] [All for One: LLMs Solve Mental Math at the Last Token With Information Transferred From Other Tokens](https://arxiv.org/abs/2509.09650)
*Siddarth Mamidanna,Daking Rai,Ziyu Yao,Yilun Zhou*

Main category: cs.CL

TL;DR: 本文探讨了大模型在心算类任务中，计算操作在模型内部何时何处发生，并提出了两种新方法来揭示模型的计算结构。结果发现，高性能依赖于模型后期、尾部token的集中计算。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多种任务中表现优异，但其计算过程的内部机制尚不清晰。理论上每个token都能利用自注意力和多层感知机进行全面信息处理，实际中这些操作到底何时何地发生尚未知。作者希望揭示模型进行实际计算的关键结构和流程。

Method: 研究聚焦于心算（mental math）任务，分三步探查模型运行机制：1）抑制初始层的输入特异性计算、2）限制中间层token间的信息传递通路、3）强制剩馀层只在最后一个token完成所有计算。作者提出两种新技术：Context-Aware Mean Ablation（CAMA）与Attention-Based Peeking（ABP）来定位关键计算子图（subgraph）。

Result: 通过CAMA和ABP，本文发现一种“All-for-One”子图：在大多数心算任务和算术表达式中，主要有效计算发生在深层且仅限于最后一个token；信息主要在中间特定层由其他token聚合到最后token。实验还验证了该子图对模型高性能既充要、可迁移于多模型和不同输入风格。CAMA、ABP在消融实验中也表现出独特效果。

Conclusion: 心算任务中，LLMs有效计算并未分散于全局，而是后期集中于最后token，并且模型依赖于特定计算子图。这一发现有助于理解模型内部机制，并为模型可解释性和结构优化提供新启示。

Abstract: Large language models (LLMs) demonstrate proficiency across numerous
computational tasks, yet their inner workings remain unclear. In theory, the
combination of causal self-attention and multilayer perceptron layers allows
every token to access and compute information based on all preceding tokens. In
practice, to what extent are such operations present? In this paper, on mental
math tasks (i.e., direct math calculation via next-token prediction without
explicit reasoning), we investigate this question in three steps: inhibiting
input-specific token computations in the initial layers, restricting the routes
of information transfer across token positions in the next few layers, and
forcing all computation to happen at the last token in the remaining layers.
With two proposed techniques, Context-Aware Mean Ablation (CAMA) and
Attention-Based Peeking (ABP), we identify an All-for-One subgraph (AF1) with
high accuracy on a wide variety of mental math tasks, where meaningful
computation occurs very late (in terms of layer depth) and only at the last
token, which receives information of other tokens in few specific middle
layers. Experiments on a variety of models and arithmetic expressions show that
this subgraph is sufficient and necessary for high model performance, transfers
across different models, and works on a variety of input styles. Ablations on
different CAMA and ABP alternatives reveal their unique advantages over other
methods, which may be of independent interest.

</details>


### [114] [Steering MoE LLMs via Expert (De)Activation](https://arxiv.org/abs/2509.09660)
*Mohsen Fayyaz,Ali Modarressi,Hanieh Deilamsalehy,Franck Dernoncourt,Ryan Rossi,Trung Bui,Hinrich Schütze,Nanyun Peng*

Main category: cs.CL

TL;DR: 论文提出SteerMoE，通过检测和控制行为相关专家，实现对大模型行为的引导，无需重训练或修改权重，在安全性和真实性等方面效果显著。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）广泛应用，用户对于模型输出的安全性、可靠性有更高诉求。当前提升或控制模型行为的方法往往需要重训练或权重微调，成本高且灵活度有限，尤其在Mixture-of-Experts结构下，尚缺乏针对专家行为的高效操控方法。

Method: 文中提出一种新颖框架SteerMoE，首先检测与目标行为（如安全性、真实性）相关的专家（FFN）——通过分析输入对在特定行为上的表现差异来识别活跃专家；在推理阶段，有选择地激活或关闭这些专家，从而无需修改模型结构或参数即调整模型行为。

Result: 在6个LLM和11个基准测试中，SteerMoE在无需再训练的前提下，将模型的安全性提升了最高20%、真实性提升了27%。在模拟攻击场景下，能够将模型安全性降低41%，若结合其他越狱策略，安全性可被完全绕过（-100%），暴露了专家激活机制的安全隐患。

Conclusion: SteerMoE框架为MoE大模型行为可控性提供了一种有效工具，可以极大提升或削弱模型的安全性和真实性，但也带来潜在的安全风险，揭示了MoE模型结构在行为对齐方面的新漏洞。

Abstract: Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token
through a subset of specialized Feed-Forward Networks (FFN), known as experts.
We present SteerMoE, a framework for steering MoE models by detecting and
controlling behavior-linked experts. Our detection method identifies experts
with distinct activation patterns across paired inputs exhibiting contrasting
behaviors. By selectively (de)activating such experts during inference, we
control behaviors like faithfulness and safety without retraining or modifying
weights. Across 11 benchmarks and 6 LLMs, our steering raises safety by up to
+20% and faithfulness by +27%. In adversarial attack mode, it drops safety by
-41% alone, and -100% when combined with existing jailbreak methods, bypassing
all safety guardrails and exposing a new dimension of alignment faking hidden
within experts.

</details>


### [115] [CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2509.09675)
*Runpeng Dai,Linfeng Song,Haolin Liu,Zhenwen Liang,Dian Yu,Haitao Mi,Zhaopeng Tu,Rui Liu,Tong Zheng,Hongtu Zhu,Dong Yu*

Main category: cs.CL

TL;DR: 本文提出了一种基于好奇心驱动的探索框架（CDE），用于提升大语言模型（LLM）在可验证奖励强化学习（RLVR）中的探索效率和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法探索能力较差，导致模型容易过早收敛和熵崩塌，限制了大语言模型在复杂推理任务中的表现。因此，需要一种更有效的探索机制来提升模型的能力。

Method: 作者在传统RLVR框架下引入了好奇心驱动探索信号。具体来说，Actor端通过生成响应的困惑度（perplexity）衡量新颖性，Critic端通过多头架构的价值估计方差衡量不确定性，并将这两种信号作为探索奖励，指导模型探索。

Result: 理论上，Actor端奖励能抑制自信但错误的输出并提升多样性，Critic端奖励与传统RL中的计数型探索奖励有内在联系。在AIME基准测试上，所提方法比标准RLVR（基于GRPO/PPO）提升约3分。

Conclusion: 好奇心驱动的探索方法有效提升了RLVR框架中的探索效率，揭示了校准崩溃等LLM常见失效机制，对后续改进大模型推理能力具有重要意义。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm
for enhancing the reasoning ability of Large Language Models (LLMs). Yet
current RLVR methods often explore poorly, leading to premature convergence and
entropy collapse. To address this challenge, we introduce Curiosity-Driven
Exploration (CDE), a framework that leverages the model's own intrinsic sense
of curiosity to guide exploration. We formalize curiosity with signals from
both the actor and the critic: for the actor, we use perplexity over its
generated response, and for the critic, we use the variance of value estimates
from a multi-head architecture. Both signals serve as an exploration bonus
within the RLVR framework to guide the model. Our theoretical analysis shows
that the actor-wise bonus inherently penalizes overconfident errors and
promotes diversity among correct responses; moreover, we connect the
critic-wise bonus to the well-established count-based exploration bonus in RL.
Empirically, our method achieves an approximate +3 point improvement over
standard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a
calibration collapse mechanism within RLVR, shedding light on common LLM
failure modes.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [116] [Multi Robot Coordination in Highly Dynamic Environments: Tackling Asymmetric Obstacles and Limited Communication](https://arxiv.org/abs/2509.08859)
*Vincenzo Suriani,Daniele Affinita,Domenico D. Bloisi,Daniele Nardi*

Main category: cs.RO

TL;DR: 本文提出了一种适用于低通信能力、多障碍（尤其是非对称活动障碍）和部分可观测环境下的分布式多智能体系统任务分配新方法。该方法有效减少了任务重叠并已在仿真及RoboCup机器人比赛中验证。


<details>
  <summary>Details</summary>
Motivation: 在通信受限、环境动态且障碍复杂（如非对称和活动障碍）的真实多智能体任务分配场景，现有方法不适用或效果差，因此需要新的分布式任务协调机制。

Method: 方法受市场机制启发，提出了一种新型分布式协调算法，重点考虑通信受限和障碍非对称性带来的挑战，支持频繁任务重分配，提升实际可用性。

Result: 算法在仿真与实际RoboCup机器人团队实验中表现优越，尤其在通信受限情況下，最常见的任务重叠减少了52%。

Conclusion: 本方法能高效支持在低通信、高动态障碍（尤其是非对称障碍）和部分观测环境下的多智能体任务协调，实验成果显著，具备实际应用潜力。

Abstract: Coordinating a fully distributed multi-agent system (MAS) can be challenging
when the communication channel has very limited capabilities in terms of
sending rate and packet payload. When the MAS has to deal with active obstacles
in a highly partially observable environment, the communication channel
acquires considerable relevance. In this paper, we present an approach to deal
with task assignments in extremely active scenarios, where tasks need to be
frequently reallocated among the agents participating in the coordination
process. Inspired by market-based task assignments, we introduce a novel
distributed coordination method to orchestrate autonomous agents' actions
efficiently in low communication scenarios. In particular, our algorithm takes
into account asymmetric obstacles. While in the real world, the majority of
obstacles are asymmetric, they are usually treated as symmetric ones, thus
limiting the applicability of existing methods. To summarize, the presented
architecture is designed to tackle scenarios where the obstacles are active and
asymmetric, the communication channel is poor and the environment is partially
observable. Our approach has been validated in simulation and in the real
world, using a team of NAO robots during official RoboCup competitions.
Experimental results show a notable reduction in task overlaps in limited
communication settings, with a decrease of 52% in the most frequent reallocated
task.

</details>


### [117] [Rapid Manufacturing of Lightweight Drone Frames Using Single-Tow Architected Composites](https://arxiv.org/abs/2509.09024)
*Md Habib Ullah Khan,Kaiyue Deng,Ismail Mujtaba Khan,Kelvin Fu*

Main category: cs.RO

TL;DR: 本论文提出了一种通过3D连续纤维牵引（3DFiT）制造轻质FCC晶格结构无人机机架的新方法，既减轻重量又提高结构性能，优于传统复合材料和3D打印方式。


<details>
  <summary>Details</summary>
Motivation: 目前航空航天和机器人领域对于轻质高强度复合结构的需求不断增长，特别是无人机机架的优化设计。但传统复合材料制造工艺难以实现复杂的三维结构，需要拼装多个部件，这会在连接处产生薄弱环节，并且难以保持连续纤维增强，影响结构效率。

Method: 采用3DFiT技术，利用单股连续纤维精确构建FCC晶格无人机机架，实现复杂拓扑和连续纤维增强，避免了传统组装带来的薄弱点。

Result: 实验制造的无人机机架比金属和热塑性塑料的比强度高4-8倍，重仅260g，比现有商用DJI F450机架轻10%。实飞测试下，提升了3分钟续航，并验证了其结构稳定性和耐用性。

Conclusion: FCC晶格单股连续纤维无人机机架表现出优异的轻量化与强度，3DFiT方法是一种可扩展、高效的新型复合材料成型工艺，具有广阔应用前景。

Abstract: The demand for lightweight and high-strength composite structures is rapidly
growing in aerospace and robotics, particularly for optimized drone frames.
However, conventional composite manufacturing methods struggle to achieve
complex 3D architectures for weight savings and rely on assembling separate
components, which introduce weak points at the joints. Additionally,
maintaining continuous fiber reinforcement remains challenging, limiting
structural efficiency. In this study, we demonstrate the lightweight Face
Centered Cubic (FFC) lattice structured conceptualization of drone frames for
weight reduction and complex topology fabrication through 3D Fiber Tethering
(3DFiT) using continuous single tow fiber ensuring precise fiber alignment,
eliminating weak points associated with traditional composite assembly.
Mechanical testing demonstrates that the fabricated drone frame exhibits a high
specific strength of around four to eight times the metal and thermoplastic,
outperforming other conventional 3D printing methods. The drone frame weighs
only 260 g, making it 10% lighter than the commercial DJI F450 frame, enhancing
structural integrity and contributing to an extended flight time of three
minutes, while flight testing confirms its stability and durability under
operational conditions. The findings demonstrate the potential of single tow
lattice truss-based drone frames, with 3DFiT serving as a scalable and
efficient manufacturing method.

</details>


### [118] [KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for Motion Planning](https://arxiv.org/abs/2509.09074)
*Alice Kate Li,Thales C Silva,Victoria Edwards,Vijay Kumar,M. Ani Hsieh*

Main category: cs.RO

TL;DR: 本文提出了一种基于流场的运动规划方法，结合Koopman算子，实现机器人从任意初始状态平滑收敛到指定轨迹终点，解决了传统Koopman方法无法保证收敛性的问题。验证结果显示，该方法在空间和时间上都具备高采样效率，相比基线方法在动力学建模上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于Koopman算子的动力学建模方法，虽然有效，但无法保障机器人运动收敛到期望轨迹端点，这是学习示范(LfD)时的关键需求。作者希望改进该限制，使学习后的模型能可靠实现目标收敛性。

Method: 作者提出KoopMotion，将运动流场表示为动力系统，并用Koopman算子参数化以模仿期望轨迹。通过引入流场的散度性质，实现远离目标轨迹的状态能平滑地向目标轨迹收敛并跟踪到终点。方法在LASA手写数据集、3D机械臂轨迹数据集以及真实机器人平台进行了验证。

Result: KoopMotion仅需3%的LASA数据集即可生成高密度运动规划，空间与时间维度上的采样效率极高。与常用基线进行动力学模型相关指标对比，效果提升明显，并能在有背景流体扰动的实际机器人中成功验证。

Conclusion: KoopMotion实现了在复杂环境下，机器人的高效、收敛性强的运动规划。该方法大幅减少数据需求，同时动力学建模准确率优于传统方法，对机器人学习自演示及实际应用有重要价值。

Abstract: In this work, we propose a novel flow field-based motion planning method that
drives a robot from any initial state to a desired reference trajectory such
that it converges to the trajectory's end point. Despite demonstrated efficacy
in using Koopman operator theory for modeling dynamical systems, Koopman does
not inherently enforce convergence to desired trajectories nor to specified
goals -- a requirement when learning from demonstrations (LfD). We present
KoopMotion which represents motion flow fields as dynamical systems,
parameterized by Koopman Operators to mimic desired trajectories, and leverages
the divergence properties of the learnt flow fields to obtain smooth motion
fields that converge to a desired reference trajectory when a robot is placed
away from the desired trajectory, and tracks the trajectory until the end
point. To demonstrate the effectiveness of our approach, we show evaluations of
KoopMotion on the LASA human handwriting dataset and a 3D manipulator
end-effector trajectory dataset, including spectral analysis. We also perform
experiments on a physical robot, verifying KoopMotion on a miniature autonomous
surface vehicle operating in a non-static fluid flow environment. Our approach
is highly sample efficient in both space and time, requiring only 3\% of the
LASA dataset to generate dense motion plans. Additionally, KoopMotion provides
a significant improvement over baselines when comparing metrics that measure
spatial and temporal dynamics modeling efficacy.

</details>


### [119] [Kinetostatics and Particle-Swarm Optimization of Vehicle-Mounted Underactuated Metamorphic Loading Manipulators](https://arxiv.org/abs/2509.09093)
*Nan Mao,Guanglu Jia,Junpeng Chen,Emmanouil Spyrakos-Papastavridis,Jian S. Dai*

Main category: cs.RO

TL;DR: 该论文提出了一种新型的欠驱动变构装载机械臂（UMLM），集成了可变结构机械臂与被动自适应夹爪，提升了灵活性和操作适应性，同时减少了执行器数量和控制复杂性。


<details>
  <summary>Details</summary>
Motivation: 传统的定自由度加载机构往往需要大量执行器、控制复杂且适应动态任务能力有限，因此需要更高效、灵活和自适应的加载解决方案。

Method: 采用了一种结合几何约束实现拓扑重构且无需额外执行器的变构机械臂，与完全由机械臂驱动的被动自适应夹爪相结合。建立了结构模型并进行了运动静力学分析，采用粒子群优化（PSO）方法优化夹爪尺寸参数，以提高抓取适应性。

Result: 仿真结果表明所提出的UMLM具有易于实现的控制策略、良好的操作多样性及在动态环境下抓取多样物体的有效性。

Conclusion: 该研究证明了欠驱动变构机构在需要高效和自适应装载的应用中的实际潜力，并且其建模和优化框架可拓展至更广泛的机械臂系统设计。

Abstract: Fixed degree-of-freedom (DoF) loading mechanisms often suffer from excessive
actuators, complex control, and limited adaptability to dynamic tasks. This
study proposes an innovative mechanism of underactuated metamorphic loading
manipulators (UMLM), integrating a metamorphic arm with a passively adaptive
gripper. The metamorphic arm exploits geometric constraints, enabling the
topology reconfiguration and flexible motion trajectories without additional
actuators. The adaptive gripper, driven entirely by the arm, conforms to
diverse objects through passive compliance. A structural model is developed,
and a kinetostatics analysis is conducted to investigate isomorphic grasping
configurations. To optimize performance, Particle-Swarm Optimization (PSO) is
utilized to refine the gripper's dimensional parameters, ensuring robust
adaptability across various applications. Simulation results validate the
UMLM's easily implemented control strategy, operational versatility, and
effectiveness in grasping diverse objects in dynamic environments. This work
underscores the practical potential of underactuated metamorphic mechanisms in
applications requiring efficient and adaptable loading solutions. Beyond the
specific design, this generalized modeling and optimization framework extends
to a broader class of manipulators, offering a scalable approach to the
development of robotic systems that require efficiency, flexibility, and robust
performance.

</details>


### [120] [LIPM-Guided Reinforcement Learning for Stable and Perceptive Locomotion in Bipedal Robots](https://arxiv.org/abs/2509.09106)
*Haokai Su,Haoxiang Luo,Shunpeng Yang,Kaiwen Jiang,Wei Zhang,Hua Chen*

Main category: cs.RO

TL;DR: 本文提出了一种基于线性倒立摆模型（LIPM）的新型奖励设计与训练框架，通过改进奖励函数和采用双评价器架构，实现了双足机器人在复杂户外环境下更稳定和健壮的感知行走性能。


<details>
  <summary>Details</summary>
Motivation: 针对双足机器人在非结构化户外环境中因地形复杂和外部扰动造成的运动稳定性问题，现有方法难以兼顾动态平衡与环境感知。

Method: 受LIPM启发，从理论层面指导质心高度和躯干姿态的调节，提出有助于动态稳定与感知的奖励函数。采用Reward Fusion Module（RFM）机制在速度跟踪与稳定性间自适应权衡，利用双评价器分别衡量稳定性与运动目标，提高训练效率和健壮性。

Result: 在仿真和真实户外双足机器人平台上进行大量实验，结果显示所提方法显著提升了在不同地形、速度和感知条件下的适应性、抗扰性和一致性表现。

Conclusion: 本方法能有效提升双足机器人在真实复杂野外环境下的稳定感知行走能力，为机器人动态稳定控制和感知运动提供了理论与实践的新方案。

Abstract: Achieving stable and robust perceptive locomotion for bipedal robots in
unstructured outdoor environments remains a critical challenge due to complex
terrain geometry and susceptibility to external disturbances. In this work, we
propose a novel reward design inspired by the Linear Inverted Pendulum Model
(LIPM) to enable perceptive and stable locomotion in the wild. The LIPM
provides theoretical guidance for dynamic balance by regulating the center of
mass (CoM) height and the torso orientation. These are key factors for
terrain-aware locomotion, as they help ensure a stable viewpoint for the
robot's camera. Building on this insight, we design a reward function that
promotes balance and dynamic stability while encouraging accurate CoM
trajectory tracking. To adaptively trade off between velocity tracking and
stability, we leverage the Reward Fusion Module (RFM) approach that prioritizes
stability when needed. A double-critic architecture is adopted to separately
evaluate stability and locomotion objectives, improving training efficiency and
robustness. We validate our approach through extensive experiments on a bipedal
robot in both simulation and real-world outdoor environments. The results
demonstrate superior terrain adaptability, disturbance rejection, and
consistent performance across a wide range of speeds and perceptual conditions.

</details>


### [121] [AEOS: Active Environment-aware Optimal Scanning Control for UAV LiDAR-Inertial Odometry in Complex Scenes](https://arxiv.org/abs/2509.09141)
*Jianping Li,Xinhang Xu,Zhongyuan Liu,Shenghai Yuan,Muqing Cao,Lihua Xie*

Main category: cs.RO

TL;DR: 本文提出了一种灵感来自猫头鹰的主动环境感知系统（AEOS），用于提升无人机（UAV）搭载的紧凑型LiDAR在复杂环境下的三维感知和定位能力。AEOS通过结合模型预测控制与强化学习，实现了对激光雷达自适应扫描的高效控制，有效提升了定位精度。


<details>
  <summary>Details</summary>
Motivation: 当前紧凑型LiDAR因视场狭窄和载重受限，难以通过多传感器组合提升UAV的3D感知和定位能力。传统机械旋转LiDAR扫描方式缺乏环境感知和任务自适应能力，在复杂遮挡场景下定位与建图效果下降，亟需更智能的主动感知方案。

Method: AEOS采用模型预测控制（MPC）与强化学习（RL）相结合的架构，通过不确定性建模预测未来位姿可观测性，实现高效利用；同时通过轻量级神经网络从全景深度数据中隐式学习代价地图，引导主动探索。此外，作者还开发了基于点云的仿真环境，支持多场景泛化与仿真到现实的迁移能力。

Result: 大量仿真和真实环境实验显示，AEOS在满足机载算力实时性的同时，显著提升了定位与建图精度，对比固定速率、仅优化和全学习策略均有明显优势。

Conclusion: AEOS为UAV基于LiDAR惯性里程计系统提供了一种高效、具自适应能力的主动感知框架，可在复杂环境下大幅提升3D感知与定位性能，为无人机自主导航等任务打下了坚实基础。

Abstract: LiDAR-based 3D perception and localization on unmanned aerial vehicles (UAVs)
are fundamentally limited by the narrow field of view (FoV) of compact LiDAR
sensors and the payload constraints that preclude multi-sensor configurations.
Traditional motorized scanning systems with fixed-speed rotations lack scene
awareness and task-level adaptability, leading to degraded odometry and mapping
performance in complex, occluded environments. Inspired by the active sensing
behavior of owls, we propose AEOS (Active Environment-aware Optimal Scanning),
a biologically inspired and computationally efficient framework for adaptive
LiDAR control in UAV-based LiDAR-Inertial Odometry (LIO). AEOS combines model
predictive control (MPC) and reinforcement learning (RL) in a hybrid
architecture: an analytical uncertainty model predicts future pose
observability for exploitation, while a lightweight neural network learns an
implicit cost map from panoramic depth representations to guide exploration. To
support scalable training and generalization, we develop a point cloud-based
simulation environment with real-world LiDAR maps across diverse scenes,
enabling sim-to-real transfer. Extensive experiments in both simulation and
real-world environments demonstrate that AEOS significantly improves odometry
accuracy compared to fixed-rate, optimization-only, and fully learned
baselines, while maintaining real-time performance under onboard computational
constraints. The project page can be found at
https://kafeiyin00.github.io/AEOS/.

</details>


### [122] [Occupancy-aware Trajectory Planning for Autonomous Valet Parking in Uncertain Dynamic Environments](https://arxiv.org/abs/2509.09206)
*Farhad Nawaz,Faizan M. Tariq,Sangjae Bae,David Isele,Avinash Singh,Nadia Figueroa,Nikolai Matni,Jovin D'sa*

Main category: cs.RO

TL;DR: 本文提出了一种能够预测未来停车位占用情况的方法，并结合智能规划策略以提升自动代客泊车的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前自动代客泊车系统常依赖于即时观察或静态假设，难以应对动态、不确定的环境。准确推理未来停车位可用性，并进行一体化规划，对于提升安全和效率至关重要。

Method: 所提方法区分初始空置与已占用车位，并结合动态体的运动预测，利用概率方式在有限视野下融合部分且有噪声的观测信息，同时建模未观测区域的不确定性。还设计了自适应策略规划器，基于信息增益平衡任务导向泊车与探索性导航，并引入智能等待-前进机制。

Result: 在大规模停车场的随机仿真中，证明该方法在提升泊车效率、安全裕度和轨迹平顺性方面，均明显优于现有方法。

Conclusion: 该框架显著提升了自动泊车系统在动态、不确定环境下的性能，体现了预判能力和智能策略规划对未来无人驾驶泊车的重要价值。

Abstract: Accurately reasoning about future parking spot availability and integrated
planning is critical for enabling safe and efficient autonomous valet parking
in dynamic, uncertain environments. Unlike existing methods that rely solely on
instantaneous observations or static assumptions, we present an approach that
predicts future parking spot occupancy by explicitly distinguishing between
initially vacant and occupied spots, and by leveraging the predicted motion of
dynamic agents. We introduce a probabilistic spot occupancy estimator that
incorporates partial and noisy observations within a limited Field-of-View
(FoV) model and accounts for the evolving uncertainty of unobserved regions.
Coupled with this, we design a strategy planner that adaptively balances
goal-directed parking maneuvers with exploratory navigation based on
information gain, and intelligently incorporates wait-and-go behaviors at
promising spots. Through randomized simulations emulating large parking lots,
we demonstrate that our framework significantly improves parking efficiency,
safety margins, and trajectory smoothness compared to existing approaches.

</details>


### [123] [RENet: Fault-Tolerant Motion Control for Quadruped Robots via Redundant Estimator Networks under Visual Collapse](https://arxiv.org/abs/2509.09283)
*Yueqi Zhang,Quancheng Qian,Taixian Hou,Peng Zhai,Xiaoyi Wei,Kangmai Hu,Jiafu Yi,Lihua Zhang*

Main category: cs.RO

TL;DR: 该论文提出了RENet（冗余估计器网络）框架，用于提升四足机器人在户外环境下基于视觉的运动控制的鲁棒性，尤其解决了视觉传感器噪声和视觉故障下的稳定性问题。实验结果表明，该方法在视觉感知受损的复杂环境中表现优越。


<details>
  <summary>Details</summary>
Motivation: 四足机器人要在真实世界的户外环境中稳定行走，主要面临环境预测不准和视觉深度传感器噪声大的难题，这极大限制了现有算法的实际应用。

Method: 提出RENet体系结构，采用双估计器（dual-estimator）。在机器人视觉出现故障时，通过在线估计器自适应机制，实现不同估计模块的无缝切换，从而提升了整体系统鲁棒性和部署稳定性。

Result: 在真实四足机器人平台上进行了实验。结果显示，在视觉感知能力下降的复杂户外场景下，RENet框架使机器人表现出比传统方法更强的适应性和稳定性。

Conclusion: RENet方案为四足机器人在复杂户外环境下的可靠运动控制提供了一种实用且有效的新思路，有望推动基于视觉的机器人更广泛的野外应用。

Abstract: Vision-based locomotion in outdoor environments presents significant
challenges for quadruped robots. Accurate environmental prediction and
effective handling of depth sensor noise during real-world deployment remain
difficult, severely restricting the outdoor applications of such algorithms. To
address these deployment challenges in vision-based motion control, this letter
proposes the Redundant Estimator Network (RENet) framework. The framework
employs a dual-estimator architecture that ensures robust motion performance
while maintaining deployment stability during onboard vision failures. Through
an online estimator adaptation, our method enables seamless transitions between
estimation modules when handling visual perception uncertainties. Experimental
validation on a real-world robot demonstrates the framework's effectiveness in
complex outdoor environments, showing particular advantages in scenarios with
degraded visual perception. This framework demonstrates its potential as a
practical solution for reliable robotic deployment in challenging field
conditions. Project website: https://RENet-Loco.github.io/

</details>


### [124] [OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning](https://arxiv.org/abs/2509.09332)
*Yuecheng Liu,Dafeng Chi,Shiguang Wu,Zhanguang Zhang,Yuzheng Zhuang,Bowen Yang,He Zhu,Lingfeng Zhang,Pengwei Xie,David Gamaliel Arcos Bravo,Yingxue Zhang,Jianye Hao,Xingyue Quan*

Main category: cs.RO

TL;DR: 本论文提出了OmniEVA，一个面向实际身体智能的多模态大模型规划器，通过创新的3D感知与物理约束机制，提升了下游任务中的空间适应性和可执行性。实验表明其拥有出色的多任务通用性和规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于多模态大语言模型的具身系统存在空间信息缺失与物理约束忽略两大问题：一是仅2D输入或简单3D注入导致空间适应性不足；二是缺少真实机器人物理约束，导致规划不可实际执行。因此亟需能兼顾多样空间需求与可执行性的系统。

Method: OmniEVA包含两大创新：（1）任务自适应3D定位机制，通过门控路由单元按上下文动态选择3D融合模式，提升空间泛化能力；（2）具身约束感知推理框架，将任务目标与物理约束联合引入推理过程，输出既有目标导向又可实际操作的规划方案。

Result: 大量实验表明OmniEVA实现了当前最佳的具身推理性能。对多项原子与复合任务的基准测试，系统均表现出强健的泛化与规划能力。

Conclusion: OmniEVA有效弥合了空间适应性和具身可行性两大鸿沟，证明了其在多场景、多任务具身智能中的强大通用性与实践潜力。

Abstract: Recent advances in multimodal large language models (MLLMs) have opened new
opportunities for embodied intelligence, enabling multimodal understanding,
reasoning, and interaction, as well as continuous spatial decision-making.
Nevertheless, current MLLM-based embodied systems face two critical
limitations. First, Geometric Adaptability Gap: models trained solely on 2D
inputs or with hard-coded 3D geometry injection suffer from either insufficient
spatial information or restricted 2D generalization, leading to poor
adaptability across tasks with diverse spatial demands. Second, Embodiment
Constraint Gap: prior work often neglects the physical constraints and
capacities of real robots, resulting in task plans that are theoretically valid
but practically infeasible.To address these gaps, we introduce OmniEVA -- an
embodied versatile planner that enables advanced embodied reasoning and task
planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding
mechanism, which introduces a gated router to perform explicit selective
regulation of 3D fusion based on contextual requirements, enabling
context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware
Reasoning framework that jointly incorporates task goals and embodiment
constraints into the reasoning loop, resulting in planning decisions that are
both goal-directed and executable. Extensive experimental results demonstrate
that OmniEVA not only achieves state-of-the-art general embodied reasoning
performance, but also exhibits a strong ability across a wide range of
downstream scenarios. Evaluations of a suite of proposed embodied benchmarks,
including both primitive and composite tasks, confirm its robust and versatile
planning capabilities. Project page: https://omnieva.github.io

</details>


### [125] [AGILOped: Agile Open-Source Humanoid Robot for Research](https://arxiv.org/abs/2509.09364)
*Grzegorz Ficht,Luis Denninger,Sven Behnke*

Main category: cs.RO

TL;DR: 本文提出了一款开源的人形机器人AGILOped，实现了高性能与易获取性的结合，并进行了多项实验验证其在科研中的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人多为封闭源代码，且获取和维护成本较高，限制了研究和应用的广泛发展。

Method: 开发了一款开源、易获取、可由单人操作的人形机器人AGILOped，采用了现成高功率密度可回转驱动器和标准电子器件。

Result: AGILOped机器人在步行、跳跃、缓冲碰撞和起身等任务中表现良好，展示了其实验中的实用性。

Conclusion: AGILOped为高性能人形机器人的研究与应用提供了一个低成本、可获取的开源平台，有助于推动该领域的发展。

Abstract: With academic and commercial interest for humanoid robots peaking, multiple
platforms are being developed. Through a high level of customization, they
showcase impressive performance. Most of these systems remain closed-source or
have high acquisition and maintenance costs, however. In this work, we present
AGILOped - an open-source humanoid robot that closes the gap between high
performance and accessibility. Our robot is driven by off-the-shelf
backdrivable actuators with high power density and uses standard electronic
components. With a height of 110 cm and weighing only 14.5 kg, AGILOped can be
operated without a gantry by a single person. Experiments in walking, jumping,
impact mitigation and getting-up demonstrate its viability for use in research.

</details>


### [126] [VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model](https://arxiv.org/abs/2509.09372)
*Yihao Wang,Pengxiang Ding,Lingxiao Li,Can Cui,Zirui Ge,Xinyang Tong,Wenxuan Song,Han Zhao,Wei Zhao,Pengxu Hou,Siteng Huang,Yifan Tang,Wenhui Wang,Ru Zhang,Jianyi Liu,Donglin Wang*

Main category: cs.RO

TL;DR: VLA-Adapter是一种新颖的方法，减少了依赖大规模视觉-语言模型和预训练数据，实现了高效且快速的视觉-语言-动作模型训练及推理。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作（VLA）模型通常需大规模模型和大量机器人数据预训练，成本高昂，因此作者希望探索如何更高效地将视觉-语言信息转化为动作指令，降低训练资源消耗和应用门槛。

Method: 作者提出VLA-Adapter框架，首先系统性分析了各种视觉-语言条件对动作生成的影响，并基于分析结果，设计了轻量级Policy模块和Bridge Attention机制，可自动注入最优视觉-语言条件到动作空间，从而减少对大规模模型及预训练数据的需求。

Result: 在多个仿真和真实机器人任务中，VLA-Adapter仅用0.5B参数的主干、无需预训练数据实现了与当前最优模型相当甚至更好的性能，并且推理速度极快。

Conclusion: VLA-Adapter极大降低了视觉-语言-动作模型的训练和部署门槛，实现了性能、速度及资源消耗的高效统一，为实际机器人应用推广提供了新的方案。

Abstract: Vision-Language-Action (VLA) models typically bridge the gap between
perceptual and action spaces by pre-training a large-scale Vision-Language
Model (VLM) on robotic data. While this approach greatly enhances performance,
it also incurs significant training costs. In this paper, we investigate how to
effectively bridge vision-language (VL) representations to action (A). We
introduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA
models on large-scale VLMs and extensive pre-training. To this end, we first
systematically analyze the effectiveness of various VL conditions and present
key findings on which conditions are essential for bridging perception and
action spaces. Based on these insights, we propose a lightweight Policy module
with Bridge Attention, which autonomously injects the optimal condition into
the action space. In this way, our method achieves high performance using only
a 0.5B-parameter backbone, without any robotic data pre-training. Extensive
experiments on both simulated and real-world robotic benchmarks demonstrate
that VLA-Adapter not only achieves state-of-the-art level performance, but also
offers the fast inference speed reported to date. Furthermore, thanks to the
proposed advanced bridging paradigm, VLA-Adapter enables the training of a
powerful VLA model in just 8 hours on a single consumer-grade GPU, greatly
lowering the barrier to deploying the VLA model. Project page:
https://vla-adapter.github.io/.

</details>


### [127] [A Hybrid Hinge-Beam Continuum Robot with Passive Safety Capping for Real-Time Fatigue Awareness](https://arxiv.org/abs/2509.09404)
*Tongshun Chen,Zezhou Sun,Yanhan Sun,Yuhao Wang,Dezhen Song,Ke Wu*

Main category: cs.RO

TL;DR: 论文提出了一种新的具备疲劳感知能力的连续体机器人设计，通过结构创新和实时疲劳评估方法提升了机器人的耐久性和长期运行安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的绳索驱动连续体机器人因高柔性和轻量化广受关注，但长期使用易产生机械疲劳，影响性能甚至导致结构失效。而当前针对连续体机器人的疲劳估算方法鲜有研究，限制了其长期应用能力。

Method: 提出三大创新：（1）混合铰链-梁结构，将扭转梁和弯曲梁的运动解耦，利用被动转动关节和限扭设计有效减小应力幅值；（2）设计被动限位器，通过机械限位及电机扭矩感知安全保护并采集数据；（3）开发基于电机端扭矩估算极限位姿下刚度的实时疲劳感知方法，无需额外传感器即可实现在线疲劳监测。

Result: 实验结果表明新设计相比传统结构疲劳累计降低了约49%，机械限位配合电机端感知可准确估计结构疲劳与损伤。

Conclusion: 新架构能实现连续体机器人的安全、可靠、长期运行，验证了所提方案的有效性。

Abstract: Cable-driven continuum robots offer high flexibility and lightweight design,
making them well-suited for tasks in constrained and unstructured environments.
However, prolonged use can induce mechanical fatigue from plastic deformation
and material degradation, compromising performance and risking structural
failure. In the state of the art, fatigue estimation of continuum robots
remains underexplored, limiting long-term operation. To address this, we
propose a fatigue-aware continuum robot with three key innovations: (1) a
Hybrid Hinge-Beam structure where TwistBeam and BendBeam decouple torsion and
bending: passive revolute joints in the BendBeam mitigate stress concentration,
while TwistBeam's limited torsional deformation reduces BendBeam stress
magnitude, enhancing durability; (2) a Passive Stopper that safely constrains
motion via mechanical constraints and employs motor torque sensing to detect
corresponding limit torque, ensuring safety and enabling data collection; and
(3) a real-time fatigue-awareness method that estimates stiffness from motor
torque at the limit pose, enabling online fatigue estimation without additional
sensors. Experiments show that the proposed design reduces fatigue accumulation
by about 49% compared with a conventional design, while passive mechanical
limiting combined with motor-side sensing allows accurate estimation of
structural fatigue and damage. These results confirm the effectiveness of the
proposed architecture for safe and reliable long-term operation.

</details>


### [128] [BagIt! An Adaptive Dual-Arm Manipulation of Fabric Bags for Object Bagging](https://arxiv.org/abs/2509.09484)
*Peng Zhou,Jiaming Qi,Hongmin Wu,Chen Wang,Yizhou Chen,Zeqing Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种利用双机械臂和自适应SOI操作策略的自动化装袋系统，在不需要预先了解袋子性质的情况下，实现了对不同物体的精准、稳健装袋。


<details>
  <summary>Details</summary>
Motivation: 柔性物体（如袋子）操作因其形状复杂性和不可预测性，在工业自动化领域一直较难解决，急需高效、通用的自动化装袋解决方案。

Method: 采用高斯混合模型（GMM）估算关键结构状态，应用优化技术生成SOI，通过受限双向RRT（CBiRRT）实现动作规划，并利用模型预测控制（MPC）进行双臂协调，结合视觉反馈动态调整操作。

Result: 大量实验表明，该系统能在无需预知袋子属性的前提下，完成多种物体的精准稳健装袋，展现出很强的适应性。

Conclusion: 提出的方法为机器人柔性物体操作（特别是自动化装袋任务）提供了一种创新且有效的解决方案。

Abstract: Bagging tasks, commonly found in industrial scenarios, are challenging
considering deformable bags' complicated and unpredictable nature. This paper
presents an automated bagging system from the proposed adaptive
Structure-of-Interest (SOI) manipulation strategy for dual robot arms. The
system dynamically adjusts its actions based on real-time visual feedback,
removing the need for pre-existing knowledge of bag properties. Our framework
incorporates Gaussian Mixture Models (GMM) for estimating SOI states,
optimization techniques for SOI generation, motion planning via Constrained
Bidirectional Rapidly-exploring Random Tree (CBiRRT), and dual-arm coordination
using Model Predictive Control (MPC). Extensive experiments validate the
capability of our system to perform precise and robust bagging across various
objects, showcasing its adaptability. This work offers a new solution for
robotic deformable object manipulation (DOM), particularly in automated bagging
tasks. Video of this work is available at https://youtu.be/6JWjCOeTGiQ.

</details>


### [129] [SMapper: A Multi-Modal Data Acquisition Platform for SLAM Benchmarking](https://arxiv.org/abs/2509.09509)
*Pedro Miguel Bastos Soares,Ali Tourani,Miguel Fernandez-Cortizas,Asier Bikandi Noya,Jose Luis Sanchez-Lopez,Holger Voos*

Main category: cs.RO

TL;DR: 本文提出了一种名为SMapper的新型开放硬件平台，集成多种传感器用于同步采集SLAM等领域所需的数据，并发布了高精度、多模态的公开数据集，助力相关研究的可复现性和进步。


<details>
  <summary>Details</summary>
Motivation: 现有SLAM和自动导航研究所用多模态数据集存在传感器类型单一、环境多样性不足和硬件难以复现等问题，严重制约了算法发展和研究的可重复性。

Method: 设计并实现了一个开放的多传感器平台SMapper，集成了同步的LiDAR、多摄像头和惯性测量单元，配套高精度校准与同步流程。平台开放硬件及设计文件，便于社区复现和扩展，并支持手持和机器人多场景应用。此外，公开发布了SMapper-light数据集，含精确地面真实轨迹和高密度三维重建。

Result: SMapper平台可可靠地采集高质量多模态数据，SMapper-light数据集包含高精度同步数据及地面真值，为现有SLAM算法全面基准测试提供平台。文中使用SMapper-light对当前视觉和激光SLAM算法进行了评测并给出结果。

Conclusion: SMapper通过开放硬件设计与高质量、可复现数据集的发布，为SLAM研究在算法开发、性能评测和结果复现性方面提供坚实基础，推动领域持续进步。

Abstract: Advancing research in fields like Simultaneous Localization and Mapping
(SLAM) and autonomous navigation critically depends on reliable and
reproducible multimodal datasets. While several influential datasets have
driven progress in these domains, they often suffer from limitations in sensing
modalities, environmental diversity, and the reproducibility of the underlying
hardware setups. To address these challenges, this paper introduces SMapper, a
novel open-hardware, multi-sensor platform designed explicitly for, though not
limited to, SLAM research. The device integrates synchronized LiDAR,
multi-camera, and inertial sensing, supported by a robust calibration and
synchronization pipeline that ensures precise spatio-temporal alignment across
modalities. Its open and replicable design allows researchers to extend its
capabilities and reproduce experiments across both handheld and robot-mounted
scenarios. To demonstrate its practicality, we additionally release
SMapper-light, a publicly available SLAM dataset containing representative
indoor and outdoor sequences. The dataset includes tightly synchronized
multimodal data and ground-truth trajectories derived from offline LiDAR-based
SLAM with sub-centimeter accuracy, alongside dense 3D reconstructions.
Furthermore, the paper contains benchmarking results on state-of-the-art LiDAR
and visual SLAM frameworks using the SMapper-light dataset. By combining
open-hardware design, reproducible data collection, and comprehensive
benchmarking, SMapper establishes a robust foundation for advancing SLAM
algorithm development, evaluation, and reproducibility.

</details>


### [130] [A Neuromorphic Incipient Slip Detection System using Papillae Morphology](https://arxiv.org/abs/2509.09546)
*Yanhui Lu,Zeyu Deng,Stephen J. Redmond,Efi Psomopoulou,Benjamin Ward-Cherrier*

Main category: cs.RO

TL;DR: 本文提出了一种基于神经形态触觉传感与脉冲卷积神经网络（SCNN）的系统，实现了对不同滑移状态（无滑移、初始滑移、严重滑移）的高精度检测，可在对象真正滑落前360ms及时发现滑移迹象，提升了机器人操作的安全性。


<details>
  <summary>Details</summary>
Motivation: 在机器人抓取与操作中，能够提前检测到物体即将滑落（初始滑移）十分关键，这样可及时调整以防止物体掉落，提升安全性与操作可靠性。然而，要在资源受限的边缘平台上运行快速、高效的滑移检测系统仍然面临能耗等挑战。

Method: 本文设计了一种基于神经形态的触觉传感系统，核心为NeuroTac传感器（带有突起乳突状皮肤），并结合脉冲卷积神经网络（SCNN）对采集到的信号进行滑移分类。检测过程中通过SCNN对三类滑移状态进行识别，并应用时序平滑处理提升检测鲁棒性。

Result: 所提出的SCNN模型在三类滑移状态的分类准确率达到94.33%；在动态重力诱导的滑移实验中，系统可在严重滑移发生前至少360ms检测到初始滑移，所有实验均成功实现初始滑移的超前检测。

Conclusion: 基于神经形态传感与SCNN的系统实现了高效、稳定且实时的滑移检测，适用于能耗敏感的边缘计算平台，为机器人安全操作提供了有力保障。

Abstract: Detecting incipient slip enables early intervention to prevent object
slippage and enhance robotic manipulation safety. However, deploying such
systems on edge platforms remains challenging, particularly due to energy
constraints. This work presents a neuromorphic tactile sensing system based on
the NeuroTac sensor with an extruding papillae-based skin and a spiking
convolutional neural network (SCNN) for slip-state classification. The SCNN
model achieves 94.33% classification accuracy across three classes (no slip,
incipient slip, and gross slip) in slip conditions induced by sensor motion.
Under the dynamic gravity-induced slip validation conditions, after temporal
smoothing of the SCNN's final-layer spike counts, the system detects incipient
slip at least 360 ms prior to gross slip across all trials, consistently
identifying incipient slip before gross slip occurs. These results demonstrate
that this neuromorphic system has stable and responsive incipient slip
detection capability.

</details>


### [131] [ObjectReact: Learning Object-Relative Control for Visual Navigation](https://arxiv.org/abs/2509.09594)
*Sourav Garg,Dustin Craggs,Vineeth Bhat,Lachlan Mares,Stefan Podgorski,Madhava Krishna,Feras Dayoub,Ian Reid*

Main category: cs.RO

TL;DR: 该论文提出了一种基于对象关系的视觉导航方法，用物体级别的3D场景图替代传统的图像级别表示，实现跨平台、跨环境的更稳健导航。


<details>
  <summary>Details</summary>
Motivation: 现有基于单目相机和拓扑地图的视觉导航方法，普遍采用图像相对的方式，受到代理体姿态和外形变化影响，难以泛化或适应新的任务、环境、装备。

Method: 作者提出“object-relative”控制范式，基于3D相对场景图表达环境，并设计了以高层次WayObject Costmap为输入的局部控制器ObjectReact，实现无需RGB输入进行导航决策。该范式将控制预测与图像匹配问题解耦，路径规划更依赖于地图中对象的布局信息。

Result: 实验显示在多个具备环境高度、体型变化及空间推理挑战的任务下，object-relative方法优于传统image-relative方法，包括反方向导航等。仅用仿真训练的策略也能较好迁移到现实室内场景。

Conclusion: 对象相对的导航方法不依赖经验复现，具备更强泛化和高空间理解力，对跨设备和新环境导航任务有显著优势。

Abstract: Visual navigation using only a single camera and a topological map has
recently become an appealing alternative to methods that require additional
sensors and 3D maps. This is typically achieved through an "image-relative"
approach to estimating control from a given pair of current observation and
subgoal image. However, image-level representations of the world have
limitations because images are strictly tied to the agent's pose and
embodiment. In contrast, objects, being a property of the map, offer an
embodiment- and trajectory-invariant world representation. In this work, we
present a new paradigm of learning "object-relative" control that exhibits
several desirable characteristics: a) new routes can be traversed without
strictly requiring to imitate prior experience, b) the control prediction
problem can be decoupled from solving the image matching problem, and c) high
invariance can be achieved in cross-embodiment deployment for variations across
both training-testing and mapping-execution settings. We propose a topometric
map representation in the form of a "relative" 3D scene graph, which is used to
obtain more informative object-level global path planning costs. We train a
local controller, dubbed "ObjectReact", conditioned directly on a high-level
"WayObject Costmap" representation that eliminates the need for an explicit RGB
input. We demonstrate the advantages of learning object-relative control over
its image-relative counterpart across sensor height variations and multiple
navigation tasks that challenge the underlying spatial understanding
capability, e.g., navigating a map trajectory in the reverse direction. We
further show that our sim-only policy is able to generalize well to real-world
indoor environments. Code and supplementary material are accessible via project
page: https://object-react.github.io/

</details>


### [132] [MOFU: Development of a MOrphing Fluffy Unit with Expansion and Contraction Capabilities and Evaluation of the Animacy of Its Movements](https://arxiv.org/abs/2509.09613)
*Taisei Mogi,Mari Saito,Yoshihiro Nakata*

Main category: cs.RO

TL;DR: 本文介绍了一种新型可变形机器人MOFU，通过全身扩张-收缩运动提升了机器人被感知为有生命力（拟生性）的效果。通过实验比较，发现增加扩张-收缩运动显著增强了被试者对机器人的拟生性感知。扩展到双机器人未提升拟生性，扩张-收缩结合移动效果优于仅移动。研究表明此类体积变化运动应作为未来机器人设计的重要元素。


<details>
  <summary>Details</summary>
Motivation: 现有社交与治疗型机器人多注重外观与关节动作模仿，很少关注生物常见的全身扩张-收缩（体积变化）运动及其对拟生性感知的影响。因此，探究如何通过体积变化提升机器人生命感知有较高学术与应用价值。

Method: 设计了MOFU机器人，具备单电机驱动的体积扩张-收缩能力，并能移动。通过制作MOFU行为视频，邀请受试者观看后用Godspeed问卷评价拟生性与相关感知。实验对比了无扩展动作、有扩展动作，以及多个MOFU组合和扩展动作结合移动的情况。

Result: 实验表明：1.加入扩张-收缩动作显著提升了机器人拟生性感知；2.同时呈现两个MOFU未进一步提升拟生性；3.扩张-收缩与移动结合后，比单独移动的拟生性更高。

Conclusion: 带有扩张-收缩等体积变化方式能提升机器人给人的“有生命”印象，这应作为未来社交和治疗类机器人设计的重要元素。

Abstract: Robots for therapy and social interaction are often intended to evoke
"animacy" in humans. While many robots imitate appearance and joint movements,
little attention has been given to whole-body expansion-contraction,
volume-changing movements observed in living organisms, and their effect on
animacy perception. We developed a mobile robot called "MOFU (Morphing Fluffy
Unit)," capable of whole-body expansion-contraction with a single motor and
covered with a fluffy exterior. MOFU employs a "Jitterbug" structure, a
geometric transformation mechanism that enables smooth volume change in
diameter from 210 to 280 mm using one actuator. It is also equipped with a
differential two-wheel drive mechanism for locomotion. To evaluate the effect
of expansion-contraction movements, we conducted an online survey using videos
of MOFU's behavior. Participants rated impressions with the Godspeed
Questionnaire Series. First, we compared videos of MOFU in a stationary state
with and without expansion-contraction and turning, finding that
expansion-contraction significantly increased perceived animacy. Second, we
hypothesized that presenting two MOFUs would increase animacy compared with a
single robot; however, this was not supported, as no significant difference
emerged. Exploratory analyses further compared four dual-robot motion
conditions. Third, when expansion-contraction was combined with locomotion,
animacy ratings were higher than locomotion alone. These results suggest that
volume-changing movements such as expansion and contraction enhance perceived
animacy in robots and should be considered an important design element in
future robot development aimed at shaping human impressions.

</details>


### [133] [Dexplore: Scalable Neural Control for Dexterous Manipulation from Reference-Scoped Exploration](https://arxiv.org/abs/2509.09671)
*Sirui Xu,Yu-Wei Chao,Liuyu Bian,Arsalan Mousavian,Yu-Xiong Wang,Liang-Yan Gui,Wei Yang*

Main category: cs.RO

TL;DR: 提出了一种新的统一优化方法Dexplore，使机器人可以直接高效地利用大规模手-物体动作捕捉数据学习灵巧操作技能，提升数据利用率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有手-物体动作捕捉数据资源丰富，蕴含大量可用于机器人学习灵巧操作的示范。但是，这些数据存在演示误差和人机手型差异，传统三阶段方法处理不充分，导致误差积累和数据利用率低，需要更有效的利用机制。

Method: 提出Dexplore，一个统一的单环优化方法，把重定向和追踪联合成一步，采用强化学习不把示范轨迹当作绝对真实标签，而作为软引导信号。算法自适应调整空间范围，强调在任务完成与最小控制成本间平衡，并可扩展到大规模数据集。最后通过蒸馏，将策略转为支持实物多技能泛化的视觉-生成控制器。

Result: Dexplore能够更好保持示范意图，让机器人在弱监督下学习出针对自身结构的操作策略，对示范噪声鲁棒，并且能大幅扩展到大规模数据集。生成的控制器具备多种操作能力，能够在不同物体和实际场景下应用。

Conclusion: Dexplore打通了手-物体动作捕捉数据到机器人灵巧操作能力之间的桥梁，将不完美的示范有效转化为高质量的训练信号，大幅提升了机器人学习的鲁棒性、泛化性及实际应用价值。

Abstract: Hand-object motion-capture (MoCap) repositories offer large-scale,
contact-rich demonstrations and hold promise for scaling dexterous robotic
manipulation. Yet demonstration inaccuracies and embodiment gaps between human
and robot hands limit the straightforward use of these data. Existing methods
adopt a three-stage workflow, including retargeting, tracking, and residual
correction, which often leaves demonstrations underused and compound errors
across stages. We introduce Dexplore, a unified single-loop optimization that
jointly performs retargeting and tracking to learn robot control policies
directly from MoCap at scale. Rather than treating demonstrations as ground
truth, we use them as soft guidance. From raw trajectories, we derive adaptive
spatial scopes, and train with reinforcement learning to keep the policy
in-scope while minimizing control effort and accomplishing the task. This
unified formulation preserves demonstration intent, enables robot-specific
strategies to emerge, improves robustness to noise, and scales to large
demonstration corpora. We distill the scaled tracking policy into a
vision-based, skill-conditioned generative controller that encodes diverse
manipulation skills in a rich latent representation, supporting generalization
across objects and real-world deployment. Taken together, these contributions
position Dexplore as a principled bridge that transforms imperfect
demonstrations into effective training signals for dexterous manipulation.

</details>


### [134] [SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning](https://arxiv.org/abs/2509.09674)
*Haozhan Li,Yuxin Zuo,Jiale Yu,Yuhao Zhang,Zhaohui Yang,Kaiyan Zhang,Xuekai Zhu,Yuchen Zhang,Tianxing Chen,Ganqu Cui,Dehui Wang,Dingxiang Luo,Yuchen Fan,Youbang Sun,Jia Zeng,Jiangmiao Pang,Shanghang Zhang,Yu Wang,Yao Mu,Bowen Zhou,Ning Ding*

Main category: cs.RO

TL;DR: 本文提出了一种针对视觉-语言-动作（VLA）模型的高效强化学习（RL）框架SimpleVLA-RL，在减少对大规模人工演示数据依赖的同时，提升了模型的泛化能力，并在多项任务中超越了监督微调（SFT）方法。


<details>
  <summary>Details</summary>
Motivation: VLA模型虽然在机器人操作中取得进展，但其大规模人工示范数据稀缺、获取成本高，且在分布变化任务中的泛化有限，极大限制了实际应用。受大模型强化学习提升推理能力的启发，作者尝试让RL提升VLA模型的长时序行动规划能力。

Method: 作者基于veRL，提出了简单高效、适配VLA任务的SimpleVLA-RL框架，引入了VLA专用的轨迹采样方式、可扩展的并行执行、多环境渲染及优化损失计算，并结合探索性增强策略，极大提升了RL优化下的VLA模型性能。

Result: SimpleVLA-RL在公开的VLA基准LIBERO上表现优异，并在RoboTwin 1.0和2.0上超越了已有的pi_0基线。相较于传统的SFT方法，RL方法不仅减少了对数据规模的依赖，而且在实际任务中实现了更强的泛化和更高性能。

Conclusion: RL能够大幅提升VLA模型在机器人操作任务中的表现，尤其在长时序任务规划中优于SFT，且极大减轻了对人工演示数据的依赖。此外，探索中还发现了pushcut等新的策略，从而扩展了现有模型的能力。

Abstract: Vision-Language-Action (VLA) models have recently emerged as a powerful
paradigm for robotic manipulation. Despite substantial progress enabled by
large-scale pretraining and supervised fine-tuning (SFT), these models face two
fundamental challenges: (i) the scarcity and high cost of large-scale
human-operated robotic trajectories required for SFT scaling, and (ii) limited
generalization to tasks involving distribution shift. Recent breakthroughs in
Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can
dramatically enhance step-by-step reasoning capabilities, raising a natural
question: Can RL similarly improve the long-horizon step-by-step action
planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL
framework tailored for VLA models. Building upon veRL, we introduce
VLA-specific trajectory sampling, scalable parallelization, multi-environment
rendering, and optimized loss computation. When applied to OpenVLA-OFT,
SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $\pi_0$
on RoboTwin 1.0\&2.0 with the exploration-enhancing strategies we introduce.
SimpleVLA-RL not only reduces dependence on large-scale data and enables robust
generalization, but also remarkably surpasses SFT in real-world tasks.
Moreover, we identify a novel phenomenon ``pushcut'' during RL training,
wherein the policy discovers previously unseen patterns beyond those seen in
the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL

</details>
