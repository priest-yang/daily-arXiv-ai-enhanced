<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 59]
- [cs.CL](#cs.CL) [Total: 33]
- [cs.RO](#cs.RO) [Total: 35]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [3D and 4D World Modeling: A Survey](https://arxiv.org/abs/2509.07996)
*Lingdong Kong,Wesley Yang,Jianbiao Mei,Youquan Liu,Ao Liang,Dekai Zhu,Dongyue Lu,Wei Yin,Xiaotao Hu,Mingkai Jia,Junyuan Deng,Kaiwen Zhang,Yang Wu,Tianyi Yan,Shenyuan Gao,Song Wang,Linfeng Li,Liang Pan,Yong Liu,Jianke Zhu,Wei Tsang Ooi,Steven C. H. Hoi,Ziwei Liu*

Main category: cs.CV

TL;DR: 本文是首个系统综述3D和4D世界建模与生成技术的文献，提出清晰定义与分类体系，总结方法、数据集与评测指标，反思挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 当前世界建模研究多聚焦于2D图像与视频，忽略了3D/4D（如RGB-D、占据网格、激光点云等）原生数据方法，且业界缺乏标准定义与分类，导致文献主张分散、不一致。

Method: 本文通过文献调研，系统梳理和归纳3D/4D世界建模与生成技术，提出明确定义及三大分类（基于视频、占据、激光雷达），并总结相关数据集、评测指标、应用场景与挑战。

Result: 建立了系统的3D/4D世界模型定义和多维度分类，总结了现有方法、数据资源和评测手段，为后续研究提供一站式参考。

Conclusion: 本文填补了3D/4D世界模型领域综述空白，规范了基本概念与技术框架，有助于研究社区统一认识、厘清方向并促进技术进步。

Abstract: World modeling has become a cornerstone in AI research, enabling agents to
understand, represent, and predict the dynamic environments they inhabit. While
prior work largely emphasizes generative methods for 2D image and video data,
they overlook the rapidly growing body of work that leverages native 3D and 4D
representations such as RGB-D imagery, occupancy grids, and LiDAR point clouds
for large-scale scene modeling. At the same time, the absence of a standardized
definition and taxonomy for ``world models'' has led to fragmented and
sometimes inconsistent claims in the literature. This survey addresses these
gaps by presenting the first comprehensive review explicitly dedicated to 3D
and 4D world modeling and generation. We establish precise definitions,
introduce a structured taxonomy spanning video-based (VideoGen),
occupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and
systematically summarize datasets and evaluation metrics tailored to 3D/4D
settings. We further discuss practical applications, identify open challenges,
and highlight promising research directions, aiming to provide a coherent and
foundational reference for advancing the field. A systematic summary of
existing literature is available at https://github.com/worldbench/survey

</details>


### [2] [An Explainable Deep Neural Network with Frequency-Aware Channel and Spatial Refinement for Flood Prediction in Sustainable Cities](https://arxiv.org/abs/2509.08003)
*Shahid Shafi Dar,Bharat Kaurav,Arnav Jain,Chandravardhan Singh Raghaw,Mohammad Zia Ur Rehman,Nagendra Kumar*

Main category: cs.CV

TL;DR: 本文提出了一种新型深度学习框架XFloodNet，用于提升城市洪水检测的准确性，相较现有方法显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 面对气候变化带来的城市洪水挑战，现有方法在数据种类、特征集成和适应复杂环境方面存在明显不足，导致洪水检测效果不理想。

Method: XFloodNet包括三个关键创新组件：1）分层跨模态门控注意力机制，实现视觉和文本特征的动态融合和多粒度交互；2）异构卷积自适应多尺度注意模块，结合频率增强通道和空间注意机制，有效提取洪水判别特征；3）级联卷积Transformer特征精炼技术，通过自适应缩放和级联操作，增强不同层次特征的协调性和抗噪性。

Result: 在Chennai、Rhine18和Harz17三个洪水公开数据集上，XFloodNet分别取得了F1-score为93.33%、82.24%和88.60%的成绩，显著优于现有方法。

Conclusion: XFloodNet框架在多模态城市洪水检测问题上展现出先进性能，为城市洪水智能监测和响应系统提供了更精准高效的技术手段。

Abstract: In an era of escalating climate change, urban flooding has emerged as a
critical challenge for sustainable cities, threatening lives, infrastructure,
and ecosystems. Traditional flood detection methods are constrained by their
reliance on unimodal data and static rule-based systems, which fail to capture
the dynamic, non-linear relationships inherent in flood events. Furthermore,
existing attention mechanisms and ensemble learning approaches exhibit
limitations in hierarchical refinement, cross-modal feature integration, and
adaptability to noisy or unstructured environments, resulting in suboptimal
flood classification performance. To address these challenges, we present
XFloodNet, a novel framework that redefines urban flood classification through
advanced deep-learning techniques. XFloodNet integrates three novel components:
(1) a Hierarchical Cross-Modal Gated Attention mechanism that dynamically
aligns visual and textual features, enabling precise multi-granularity
interactions and resolving contextual ambiguities; (2) a Heterogeneous
Convolutional Adaptive Multi-Scale Attention module, which leverages
frequency-enhanced channel attention and frequency-modulated spatial attention
to extract and prioritize discriminative flood-related features across spectral
and spatial domains; and (3) a Cascading Convolutional Transformer Feature
Refinement technique that harmonizes hierarchical features through adaptive
scaling and cascading operations, ensuring robust and noise-resistant flood
detection. We evaluate our proposed method on three benchmark datasets, such as
Chennai Floods, Rhine18 Floods, and Harz17 Floods, XFloodNet achieves
state-of-the-art F1-scores of 93.33%, 82.24%, and 88.60%, respectively,
surpassing existing methods by significant margins.

</details>


### [3] [Video Parallel Scaling: Aggregating Diverse Frame Subsets for VideoLLMs](https://arxiv.org/abs/2509.08016)
*Hyungjin Chung,Hyelin Nam,Jiyeon Kim,Hyojun Go,Byeongjun Park,Junho Kim,Joonseok Lee,Seongsu Ha,Byung-Hoon Kim*

Main category: cs.CV

TL;DR: 本文提出了Video Parallel Scaling (VPS)方法，通过在推理阶段并行处理视频帧子集，有效提升了VideoLLMs对细粒度时序信息的捕捉能力，同时避免了上下文窗口增加带来的算力瓶颈和性能下降。实验显示VPS可显著提升多种架构的性能，并优于其他并行方案。


<details>
  <summary>Details</summary>
Motivation: 随着输入帧数增加，VideoLLMs虽然能获得更丰富的时序细节，但计算成本大幅上升且长上下文窗口反而导致性能下降，因此需要新的方法在不提升窗口长度的前提下提升模型对时间信息的感知。

Method: VPS方法在推理时并行动作，将视频帧划分为多个独立子集，每个子集分别输入到并行推理流中，最后对各流的输出概率进行聚合，从而整合更丰富的视觉信息，无需额外训练。理论上，该方法能通过融合来自不相关视角的证据，有效缩短模型规模与性能的依赖。

Result: 在Video-MME和EventHallusion等多个视频基准测试上，2B-32B规模的不同模型结构经过VPS处理后均表现出一致且明显的性能提升。相比自洽性(self-consistency)等并行方案，VPS具备更优的扩展性和互补性。

Conclusion: VPS为提升VideoLLMs时序推理能力提供了高效、健壮且节省内存的解决方案，不仅性能优越，还能与其他解码策略互补，推动大模型在视频理解领域的实用性提升。

Abstract: Video Large Language Models (VideoLLMs) face a critical bottleneck:
increasing the number of input frames to capture fine-grained temporal detail
leads to prohibitive computational costs and performance degradation from long
context lengths. We introduce Video Parallel Scaling (VPS), an inference-time
method that expands a model's perceptual bandwidth without increasing its
context window. VPS operates by running multiple parallel inference streams,
each processing a unique, disjoint subset of the video's frames. By aggregating
the output probabilities from these complementary streams, VPS integrates a
richer set of visual information than is possible with a single pass. We
theoretically show that this approach effectively contracts the Chinchilla
scaling law by leveraging uncorrelated visual evidence, thereby improving
performance without additional training. Extensive experiments across various
model architectures and scales (2B-32B) on benchmarks such as Video-MME and
EventHallusion demonstrate that VPS consistently and significantly improves
performance. It scales more favorably than other parallel alternatives (e.g.
Self-consistency) and is complementary to other decoding strategies, offering a
memory-efficient and robust framework for enhancing the temporal reasoning
capabilities of VideoLLMs.

</details>


### [4] [Two Stage Context Learning with Large Language Models for Multimodal Stance Detection on Climate Change](https://arxiv.org/abs/2509.08024)
*Lata Pangtey,Omkar Kabde,Shahid Shafi Dar,Nagendra Kumar*

Main category: cs.CV

TL;DR: 提出了一种结合文本和视觉信息的多模态立场检测方法，在气候变化相关的数据集上取得了新SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 大多数现有社交媒体立场检测方法仅利用文本信息，但现实社交媒体内容常常包括图像等多模态信息，因此需要更先进的多模态方法提升立场检测效果。

Method: 提出了分层融合框架，将文本和图像信息融合。具体方法包括：利用大型语言模型从文本中提取立场相关摘要，使用针对领域的图像描述生成器对图片内容做场景化解释，再通过专用的transformer模块对文本摘要、回复文本和图像描述进行建模，捕捉不同模态间的互动，最终完成立场分类。

Result: 在气候变化话题的MultiClimate多模态数据集（包含视频帧和转录文本）上实验，准确率76.2%、精确率76.3%、召回率76.2%、F1分数76.2%，均优于现有最优方法。

Conclusion: 整合多模态信息（文本+图片）显著提升了社交媒体立场检测效果，提出的方法性能超越了现有技术，为相关应用提供了更强工具。

Abstract: With the rapid proliferation of information across digital platforms, stance
detection has emerged as a pivotal challenge in social media analysis. While
most of the existing approaches focus solely on textual data, real-world social
media content increasingly combines text with visual elements creating a need
for advanced multimodal methods. To address this gap, we propose a multimodal
stance detection framework that integrates textual and visual information
through a hierarchical fusion approach. Our method first employs a Large
Language Model to retrieve stance-relevant summaries from source text, while a
domain-aware image caption generator interprets visual content in the context
of the target topic. These modalities are then jointly modeled along with the
reply text, through a specialized transformer module that captures interactions
between the texts and images. The proposed modality fusion framework integrates
diverse modalities to facilitate robust stance classification. We evaluate our
approach on the MultiClimate dataset, a benchmark for climate change-related
stance detection containing aligned video frames and transcripts. We achieve
accuracy of 76.2%, precision of 76.3%, recall of 76.2% and F1-score of 76.2%,
respectively, outperforming existing state-of-the-art approaches.

</details>


### [5] [Two-Stage Swarm Intelligence Ensemble Deep Transfer Learning (SI-EDTL) for Vehicle Detection Using Unmanned Aerial Vehicles](https://arxiv.org/abs/2509.08026)
*Zeinab Ghasemi Darehnaei,Mohammad Shokouhifar,Hossein Yazdanjouei,S. M. J. Rastegar Fatemi*

Main category: cs.CV

TL;DR: 本文提出了SI-EDTL模型，通过集成深度迁移学习和群体智能优化，实现了无人机图像中多车辆识别，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 无人机图像中的车辆检测任务因为视角多变和目标多样而具有较大挑战。现有方法在准确率、召回率等方面存在提升空间，因此需要探索更有效的多模型集成与高效优化手段。

Method: 构建一个两阶段模型：第一阶段用三种预训练Faster R-CNN特征提取器（InceptionV3、ResNet50、GoogLeNet），第二阶段将提取特征分别输入五种转移学习分类器（KNN、SVM、MLP、C4.5、朴素贝叶斯），形成15个基学习器。最后用加权平均集成。利用鲸鱼优化算法调整超参数以优化检测效果。

Result: 在AU-AIR无人机数据集上测试，SI-EDTL模型准确率、精度、召回率等评价指标超越现有对比方法。

Conclusion: SI-EDTL模型结合多特征提取和多分类器融合，并采用群体智能优化，有效提升了无人机图像车辆检测的综合性能。

Abstract: This paper introduces SI-EDTL, a two-stage swarm intelligence ensemble deep
transfer learning model for detecting multiple vehicles in UAV images. It
combines three pre-trained Faster R-CNN feature extractor models (InceptionV3,
ResNet50, GoogLeNet) with five transfer classifiers (KNN, SVM, MLP, C4.5,
Na\"ive Bayes), resulting in 15 different base learners. These are aggregated
via weighted averaging to classify regions as Car, Van, Truck, Bus, or
background. Hyperparameters are optimized with the whale optimization algorithm
to balance accuracy, precision, and recall. Implemented in MATLAB R2020b with
parallel processing, SI-EDTL outperforms existing methods on the AU-AIR UAV
dataset.

</details>


### [6] [MCTED: A Machine-Learning-Ready Dataset for Digital Elevation Model Generation From Mars Imagery](https://arxiv.org/abs/2509.08027)
*Rafał Osadnik,Pablo Gómez,Eleni Bohacek,Rickbir Bahia*

Main category: cs.CV

TL;DR: 本文提出了一个新用于火星数字高程模型预测的机器学习数据集MCTED，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前用于火星高分辨率数字高程模型（DEM）预测的机器学习数据集较少，制约了深度学习方法在该领域的研究和发展，因此需要一个高质量、公开的数据集。

Method: 作者设计了一套完整的数据处理流程，利用火星勘测轨道飞行器（MRO）的CTX仪器拍摄的高分辨率正射影像与DEM数据生成了MCTED数据集。针对原始数据常见的伪影和缺失数据，开发了相应的修复工具。数据集严格划分为训练集与验证集，并标注缺失或被改动的区域。随后采用U-Net模型在该数据集上训练，并与DepthAnythingV2等深度估计基础模型进行对比。

Result: 结果显示，专门在MCTED数据集上训练的小型U-Net网络，其性能优于零样本测试状态下的深度估计基础大模型DepthAnythingV2。

Conclusion: MCTED数据集为火星地形高程预测提供了新的资源，有助于促进相关机器学习研究。数据和代码已全部在线开源，便于学界与业界使用和验证。

Abstract: This work presents a new dataset for the Martian digital elevation model
prediction task, ready for machine learning applications called MCTED. The
dataset has been generated using a comprehensive pipeline designed to process
high-resolution Mars orthoimage and DEM pairs from Day et al., yielding a
dataset consisting of 80,898 data samples. The source images are data gathered
by the Mars Reconnaissance Orbiter using the CTX instrument, providing a very
diverse and comprehensive coverage of the Martian surface. Given the complexity
of the processing pipelines used in large-scale DEMs, there are often artefacts
and missing data points in the original data, for which we developed tools to
solve or mitigate their impact. We divide the processed samples into training
and validation splits, ensuring samples in both splits cover no mutual areas to
avoid data leakage. Every sample in the dataset is represented by the optical
image patch, DEM patch, and two mask patches, indicating values that were
originally missing or were altered by us. This allows future users of the
dataset to handle altered elevation regions as they please. We provide
statistical insights of the generated dataset, including the spatial
distribution of samples, the distributions of elevation values, slopes and
more. Finally, we train a small U-Net architecture on the MCTED dataset and
compare its performance to a monocular depth estimation foundation model,
DepthAnythingV2, on the task of elevation prediction. We find that even a very
small architecture trained on this dataset specifically, beats a zero-shot
performance of a depth estimation foundation model like DepthAnythingV2. We
make the dataset and code used for its generation completely open source in
public repositories.

</details>


### [7] [APML: Adaptive Probabilistic Matching Loss for Robust 3D Point Cloud Reconstruction](https://arxiv.org/abs/2509.08104)
*Sasan Sharifipour,Constantino Álvarez Casado,Mohammad Sabokrou,Miguel Bordallo López*

Main category: cs.CV

TL;DR: 本文提出了一种新的点云损失函数APML，在保证一对一匹配的同时，具备可微分性和较优效率，显著提升了点云预测任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有主流点云预测损失函数如Chamfer Distance等，因采用最近邻分配，导致稠密区域点拥挤、稀疏区域覆盖不足，且含有不可微操作，不利于模型优化。Earth Mover Distance虽能保证一对一匹配，但计算复杂度太高。亟需一种可微、效率高且结构感知能力强的损失函数。

Method: 作者提出Adaptive Probabilistic Matching Loss (APML)，用带温度参数的Sinkhorn迭代在相似度矩阵上实现可微分一对一近似匹配，且通过分析式计算温度，无需手工调参。APML的复杂度接近二次方，远优于EMD，并避免了不可微分的索引操作。

Result: APML嵌入到多个主流点云补全/生成网络（PoinTr、PCN、FoldingNet）及CSI2PC等变换器架构后，在ShapeNet等基准和WiFi CSI点云生成任务中，表现出快速收敛、低密度区域分布更优、定量指标优于或相当于现有方法，且无需超参搜索。

Conclusion: APML在点云预测任务中兼具高效性、结构感知与可微性，是优于既有损失的新选择，实践中效果显著且易于集成。

Abstract: Training deep learning models for point cloud prediction tasks such as shape
completion and generation depends critically on loss functions that measure
discrepancies between predicted and ground-truth point sets. Commonly used
functions such as Chamfer Distance (CD), HyperCD, and InfoCD rely on
nearest-neighbor assignments, which often induce many-to-one correspondences,
leading to point congestion in dense regions and poor coverage in sparse
regions. These losses also involve non-differentiable operations due to index
selection, which may affect gradient-based optimization. Earth Mover Distance
(EMD) enforces one-to-one correspondences and captures structural similarity
more effectively, but its cubic computational complexity limits its practical
use. We propose the Adaptive Probabilistic Matching Loss (APML), a fully
differentiable approximation of one-to-one matching that leverages Sinkhorn
iterations on a temperature-scaled similarity matrix derived from pairwise
distances. We analytically compute the temperature to guarantee a minimum
assignment probability, eliminating manual tuning. APML achieves near-quadratic
runtime, comparable to Chamfer-based losses, and avoids non-differentiable
operations. When integrated into state-of-the-art architectures (PoinTr, PCN,
FoldingNet) on ShapeNet benchmarks and on a spatiotemporal Transformer (CSI2PC)
that generates 3D human point clouds from WiFi CSI measurements, APM loss
yields faster convergence, superior spatial distribution, especially in
low-density regions, and improved or on-par quantitative performance without
additional hyperparameter search. The code is available at:
https://github.com/apm-loss/apml.

</details>


### [8] [Lightweight Deep Unfolding Networks with Enhanced Robustness for Infrared Small Target Detection](https://arxiv.org/abs/2509.08205)
*Jingjing Liu,Yinchao Han,Xianchao Xiu,Jianhua Zhang,Wanquan Liu*

Main category: cs.CV

TL;DR: 提出了一种新的轻量级红外小目标检测方法L-RPCANet，结合层次化瓶颈结构、噪声抑制及通道注意力机制，在保证网络轻量和鲁棒性的同时提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测在复杂噪声下需要高精度与高鲁棒性，但现有深度展开网络存在参数量大、抗噪能力弱等不足。

Method: 基于RPCA构建轻量级框架L-RPCANet，采用层次化瓶颈结构对特征通道数进行动态压缩和扩展，同时引入噪声抑制模块提升鲁棒性，并用SENet关注不同通道的重要性。

Result: 在ISTD多个公开数据集上的实验表明，L-RPCANet在检测性能、参数量和抗噪性能等方面优于RPCANet、DRPCANet等现有方法。

Conclusion: L-RPCANet结合通道特征优化和抗噪设计，能在保持模型轻量化的前提下显著提升红外小目标检测的精度和鲁棒性。

Abstract: Infrared small target detection (ISTD) is one of the key techniques in image
processing. Although deep unfolding networks (DUNs) have demonstrated promising
performance in ISTD due to their model interpretability and data adaptability,
existing methods still face significant challenges in parameter lightweightness
and noise robustness. In this regard, we propose a highly lightweight framework
based on robust principal component analysis (RPCA) called L-RPCANet.
Technically, a hierarchical bottleneck structure is constructed to reduce and
increase the channel dimension in the single-channel input infrared image to
achieve channel-wise feature refinement, with bottleneck layers designed in
each module to extract features. This reduces the number of channels in feature
extraction and improves the lightweightness of network parameters. Furthermore,
a noise reduction module is embedded to enhance the robustness against complex
noise. In addition, squeeze-and-excitation networks (SENets) are leveraged as a
channel attention mechanism to focus on the varying importance of different
features across channels, thereby achieving excellent performance while
maintaining both lightweightness and robustness. Extensive experiments on the
ISTD datasets validate the superiority of our proposed method compared with
state-of-the-art methods covering RPCANet, DRPCANet, and RPCANet++. The code
will be available at https://github.com/xianchaoxiu/L-RPCANet.

</details>


### [9] [Sparse Transformer for Ultra-sparse Sampled Video Compressive Sensing](https://arxiv.org/abs/2509.08228)
*Miao Cao,Siming Zheng,Lishun Wang,Ziyang Chen,David Brady,Xin Yuan*

Main category: cs.CV

TL;DR: 本文针对当前高分辨率高速摄像的大功耗问题，提出了一种超稀疏采样（USS）策略，并配套设计了BSTFormer模型以提升视频快照压缩成像（SCI）系统性能，有效降低能耗并提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 随着数字相机的分辨率和帧率不断提升，现有的数据采集和处理方式导致能耗过高（如4K、30fps下需20W），难以支撑未来的千兆像素和高帧率（100-1000fps）的摄像需求。因此，需开发更高效的物理层数据采集与恢复方法，以实现低能耗高速成像。

Method: 1. 受图像修补（I2P）启发，提出Ultra-Sparse Sampling（USS）策略，即在每个空间位置仅一帧采样为1，其余为0；2. 构建了DMD编码系统验证USS有效性；3. 针对USS难以理想分解的问题，提出了BSTFormer模型，结合块内、稀疏和时域注意力，用于高效还原高速视频帧。

Result: 通过大量仿真和真实数据实验表明，USS联合BSTFormer方法在SCI视频重建质量上显著优于以往所有方法。同时，USS相比随机采样（RS）有更高的动态范围。

Conclusion: USS策略和BSTFormer不仅在理论和实验中提升了高帧率视频SCI的能耗效率和重建性能，还为芯片级实现SCI系统提供了有吸引力的方案，尤其由于其固定曝光时间，更利于硬件实现。

Abstract: Digital cameras consume ~0.1 microjoule per pixel to capture and encode
video, resulting in a power usage of ~20W for a 4K sensor operating at 30 fps.
Imagining gigapixel cameras operating at 100-1000 fps, the current processing
model is unsustainable. To address this, physical layer compressive measurement
has been proposed to reduce power consumption per pixel by 10-100X. Video
Snapshot Compressive Imaging (SCI) introduces high frequency modulation in the
optical sensor layer to increase effective frame rate. A commonly used sampling
strategy of video SCI is Random Sampling (RS) where each mask element value is
randomly set to be 0 or 1. Similarly, image inpainting (I2P) has demonstrated
that images can be recovered from a fraction of the image pixels. Inspired by
I2P, we propose Ultra-Sparse Sampling (USS) regime, where at each spatial
location, only one sub-frame is set to 1 and all others are set to 0. We then
build a Digital Micro-mirror Device (DMD) encoding system to verify the
effectiveness of our USS strategy. Ideally, we can decompose the USS
measurement into sub-measurements for which we can utilize I2P algorithms to
recover high-speed frames. However, due to the mismatch between the DMD and
CCD, the USS measurement cannot be perfectly decomposed. To this end, we
propose BSTFormer, a sparse TransFormer that utilizes local Block attention,
global Sparse attention, and global Temporal attention to exploit the sparsity
of the USS measurement. Extensive results on both simulated and real-world data
show that our method significantly outperforms all previous state-of-the-art
algorithms. Additionally, an essential advantage of the USS strategy is its
higher dynamic range than that of the RS strategy. Finally, from the
application perspective, the USS strategy is a good choice to implement a
complete video SCI system on chip due to its fixed exposure time.

</details>


### [10] [GTA-Crime: A Synthetic Dataset and Generation Framework for Fatal Violence Detection with Adversarial Snippet-Level Domain Adaptation](https://arxiv.org/abs/2509.08232)
*Seongho Kim,Sejong Ryu,Hyoukjun You,Je Hyeong Hong*

Main category: cs.CV

TL;DR: 本文提出了GTA-Crime数据集及视频生成框架，用于解决视频异常检测（如枪击、持刀伤人）场景数据稀缺的问题，并提出了片段级领域自适应方法提升合成与真实视频间的泛化能力，实验验证了提升。


<details>
  <summary>Details</summary>
Motivation: 现实监控视频中致命异常事件（如枪击、持刀）的数据极其稀缺，既因其本身罕见，也因伦理和数据采集上的难题，严重制约了视频异常检测的发展。

Method: 利用GTA5游戏环境，捕捉了包含枪击、持刀等致命暴力的多视角模拟监控视频，构建GTA-Crime数据集，并建立自动视频生成框架。此外，提出了基于Wasserstein对抗训练的片段级领域自适应策略，以缩小模拟数据与真实监控数据特征间的差距。

Result: 实验表明，在真实数据集（如UCF-Crime）上结合GTA-Crime及其领域自适应策略能显著提升致命暴力行为检测的准确率。

Conclusion: GTA-Crime数据集和生成框架为视频异常检测提供了高质量、稀有致命异常事件数据，通过领域自适应策略，有效助力实际场景下的暴力行为检测能力提升。

Abstract: Recent advancements in video anomaly detection (VAD) have enabled
identification of various criminal activities in surveillance videos, but
detecting fatal incidents such as shootings and stabbings remains difficult due
to their rarity and ethical issues in data collection. Recognizing this
limitation, we introduce GTA-Crime, a fatal video anomaly dataset and
generation framework using Grand Theft Auto 5 (GTA5). Our dataset contains
fatal situations such as shootings and stabbings, captured from CCTV multiview
perspectives under diverse conditions including action types, weather, time of
day, and viewpoints. To address the rarity of such scenarios, we also release a
framework for generating these types of videos. Additionally, we propose a
snippet-level domain adaptation strategy using Wasserstein adversarial training
to bridge the gap between synthetic GTA-Crime features and real-world features
like UCF-Crime. Experimental results validate our GTA-Crime dataset and
demonstrate that incorporating GTA-Crime with our domain adaptation strategy
consistently enhances real world fatal violence detection accuracy. Our dataset
and the data generation framework are publicly available at
https://github.com/ta-ho/GTA-Crime.

</details>


### [11] [RepViT-CXR: A Channel Replication Strategy for Vision Transformers in Chest X-ray Tuberculosis and Pneumonia Classification](https://arxiv.org/abs/2509.08234)
*Faisal Ahmed*

Main category: cs.CV

TL;DR: 本论文提出了一种名为RepViT-CXR的通道复制策略，使单通道胸部X光片（CXR）兼容于Vision Transformer（ViT），显著提升了肺结核和肺炎自动检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有ViT结构大多基于三通道自然图像预训练，而医学影像如CXR为灰度单通道，这限制了其直接应用。因此，迫切需要开发一种有效桥接单通道和三通道输入的方案。

Method: 提出RepViT-CXR方法，通过通道复制将单通道CXR图像扩展为三通道，避免信息损失，使其可应用ViT架构，并在三个数据集上进行评测。

Result: 在TB-CXR、Pediatric Pneumonia和Shenzhen TB三个数据集上，RepViT-CXR均取得了比现有主流方法更高的准确率和AUC值。例如，在TB-CXR数据集上准确率和AUC均达到99.9%。

Conclusion: 通道复制是一种简单有效的策略，可充分激发ViT在医学灰度图像分析中的潜力。RepViT-CXR为胸部X光肺结核及肺炎检测建立了新的技术基准，并展示了实际临床应用的前景。

Abstract: Chest X-ray (CXR) imaging remains one of the most widely used diagnostic
tools for detecting pulmonary diseases such as tuberculosis (TB) and pneumonia.
Recent advances in deep learning, particularly Vision Transformers (ViTs), have
shown strong potential for automated medical image analysis. However, most ViT
architectures are pretrained on natural images and require three-channel
inputs, while CXR scans are inherently grayscale. To address this gap, we
propose RepViT-CXR, a channel replication strategy that adapts single-channel
CXR images into a ViT-compatible format without introducing additional
information loss. We evaluate RepViT-CXR on three benchmark datasets. On the
TB-CXR dataset,our method achieved an accuracy of 99.9% and an AUC of 99.9%,
surpassing prior state-of-the-art methods such as Topo-CXR (99.3% accuracy,
99.8% AUC). For the Pediatric Pneumonia dataset, RepViT-CXR obtained 99.0%
accuracy, with 99.2% recall, 99.3% precision, and an AUC of 99.0%,
outperforming strong baselines including DCNN and VGG16. On the Shenzhen TB
dataset, our approach achieved 91.1% accuracy and an AUC of 91.2%, marking a
performance improvement over previously reported CNN-based methods. These
results demonstrate that a simple yet effective channel replication strategy
allows ViTs to fully leverage their representational power on grayscale medical
imaging tasks. RepViT-CXR establishes a new state of the art for TB and
pneumonia detection from chest X-rays, showing strong potential for deployment
in real-world clinical screening systems.

</details>


### [12] [Symmetry Interactive Transformer with CNN Framework for Diagnosis of Alzheimer's Disease Using Structural MRI](https://arxiv.org/abs/2509.08243)
*Zheng Yang,Yanteng Zhang,Xupeng Kou,Yang Liu,Chao Ren*

Main category: cs.CV

TL;DR: 提出了一种结合3D CNN和对称性交互Transformer（SIT）的端到端网络，用于探测阿尔茨海默病引起的左右脑萎缩不对称性，并取得了优于传统方法的诊断准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度学习的阿尔茨海默病诊断方法多忽略了因脑部疾病导致的左右脑不对称特征，且大部分基于预训练，未针对结构性萎缩的本质特征建模。

Method: 设计了由3D CNN编码器和对称性交互Transformer（SIT）组成的端到端神经网络。通过分块操作将左右脑特征对齐，再输入SIT模块以聚焦不对称区域，从而提升诊断效果。

Result: 基于ADNI数据集，所提方法诊断准确率达92.5%，优于一般CNN与CNN-Transformer方法。可视化结果显示模型关注于脑萎缩区，特别是左右不对称相关的异常结构。

Conclusion: 该方法有效聚焦阿尔茨海默病导致的脑部结构不对称，提高了诊断准确率，具备良好可解释性和实用价值。

Abstract: Structural magnetic resonance imaging (sMRI) combined with deep learning has
achieved remarkable progress in the prediction and diagnosis of Alzheimer's
disease (AD). Existing studies have used CNN and transformer to build a
well-performing network, but most of them are based on pretraining or ignoring
the asymmetrical character caused by brain disorders. We propose an end-to-end
network for the detection of disease-based asymmetric induced by left and right
brain atrophy which consist of 3D CNN Encoder and Symmetry Interactive
Transformer (SIT). Following the inter-equal grid block fetch operation, the
corresponding left and right hemisphere features are aligned and subsequently
fed into the SIT for diagnostic analysis. SIT can help the model focus more on
the regions of asymmetry caused by structural changes, thus improving
diagnostic performance. We evaluated our method based on the ADNI dataset, and
the results show that the method achieves better diagnostic accuracy (92.5\%)
compared to several CNN methods and CNNs combined with a general transformer.
The visualization results show that our network pays more attention in regions
of brain atrophy, especially for the asymmetric pathological characteristics
induced by AD, demonstrating the interpretability and effectiveness of the
method.

</details>


### [13] [EVDI++: Event-based Video Deblurring and Interpolation via Self-Supervised Learning](https://arxiv.org/abs/2509.08260)
*Chi Zhang,Xiang Zhang,Chenxu Jiang,Gui-Song Xia,Lei Yu*

Main category: cs.CV

TL;DR: 本文提出EVDI++，一种基于事件相机的视频去模糊和插帧的统一自监督框架，利用事件相机的高时间分辨率来提升传统帧相机视频的清晰度和流畅性。


<details>
  <summary>Details</summary>
Motivation: 传统帧相机因为曝光时间较长，容易在运动场景下产生明显的模糊和信息流失，从而影响视频质量。需要一种方法利用新型传感器（如事件相机）的高时间分辨率，解决模糊问题并且提升视频插帧性能。

Method: EVDI++框架主要包括：1）设计了可学习的双重积分（LDI）网络，建模模糊帧与清晰潜在图像之间的关系；2）引入学习型分割重建模块优化粗糙结果，并提升训练效率；3）提出自适应、无参的融合策略，利用事件流中的置信度合成最终结果；4）设计自监督学习框架，使网络可直接在真实世界模糊视频和事件流上自学习，并构建了真实事件/模糊图像数据集。

Result: 在合成与真实世界的数据集上进行大量实验，表明所提方法在视频去模糊和插帧任务中均取得了最优的性能表现（state-of-the-art）。

Conclusion: EVDI++能够有效利用事件相机的信息，在实际场景下显著提升视频去模糊和插帧性能，具有良好的通用性和实际应用价值。

Abstract: Frame-based cameras with extended exposure times often produce perceptible
visual blurring and information loss between frames, significantly degrading
video quality. To address this challenge, we introduce EVDI++, a unified
self-supervised framework for Event-based Video Deblurring and Interpolation
that leverages the high temporal resolution of event cameras to mitigate motion
blur and enable intermediate frame prediction. Specifically, the Learnable
Double Integral (LDI) network is designed to estimate the mapping relation
between reference frames and sharp latent images. Then, we refine the coarse
results and optimize overall training efficiency by introducing a
learning-based division reconstruction module, enabling images to be converted
with varying exposure intervals. We devise an adaptive parameter-free fusion
strategy to obtain the final results, utilizing the confidence embedded in the
LDI outputs of concurrent events. A self-supervised learning framework is
proposed to enable network training with real-world blurry videos and events by
exploring the mutual constraints among blurry frames, latent images, and event
streams. We further construct a dataset with real-world blurry images and
events using a DAVIS346c camera, demonstrating the generalizability of the
proposed EVDI++ in real-world scenarios. Extensive experiments on both
synthetic and real-world datasets show that our method achieves
state-of-the-art performance in video deblurring and interpolation tasks.

</details>


### [14] [Hyperspectral Mamba for Hyperspectral Object Tracking](https://arxiv.org/abs/2509.08265)
*Long Gao,Yunhe Zhang,Yan Jiang,Weiying Xie,Yunsong Li*

Main category: cs.CV

TL;DR: 论文提出了一种新的高光谱目标跟踪网络（HyMamba），融合了光谱、深度和时间信息，显著提升了跟踪精度，在多个基准数据集上实现了最新最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有高光谱目标跟踪方法主要问题在于无法同时充分利用光谱本身的信息、时间相关性及不同深度的交互，导致在复杂环境下表现受限。因此，亟需一种能够综合上述多维信息的新颖跟踪方法。

Method: 提出HyMamba网络，通过状态空间模块（State Space Modules, SSMs）统一建模光谱、交叉深度和时间信息。其核心是光谱状态集成模块（SSI），用于逐步优化和传播光谱特征。同时，设计高光谱Mamba（HSM）模块，实现三向（空间、光谱、时间）扫描学习综合特征。整体网络结合伪彩色与原始高光谱数据，并融合提取的原始光谱特征进行综合增强。

Result: 在七个高光谱目标跟踪基准数据集上进行了广泛实验，HyMamba在HOTC2020数据集上取得了73.0%的AUC分数和96.3%的DP@20分数，均领先于现有主流方法，展现了优异的性能。

Conclusion: HyMamba有效克服了现有高光谱跟踪方法的不足，实现了多维信息的联合建模，极大地提升了目标跟踪准确率。实验结果充分验证了其方法的有效性和先进性。

Abstract: Hyperspectral object tracking holds great promise due to the rich spectral
information and fine-grained material distinctions in hyperspectral images,
which are beneficial in challenging scenarios. While existing hyperspectral
trackers have made progress by either transforming hyperspectral data into
false-color images or incorporating modality fusion strategies, they often fail
to capture the intrinsic spectral information, temporal dependencies, and
cross-depth interactions. To address these limitations, a new hyperspectral
object tracking network equipped with Mamba (HyMamba), is proposed. It unifies
spectral, cross-depth, and temporal modeling through state space modules
(SSMs). The core of HyMamba lies in the Spectral State Integration (SSI)
module, which enables progressive refinement and propagation of spectral
features with cross-depth and temporal spectral information. Embedded within
each SSI, the Hyperspectral Mamba (HSM) module is introduced to learn spatial
and spectral information synchronously via three directional scanning SSMs.
Based on SSI and HSM, HyMamba constructs joint features from false-color and
hyperspectral inputs, and enhances them through interaction with original
spectral features extracted from raw hyperspectral images. Extensive
experiments conducted on seven benchmark datasets demonstrate that HyMamba
achieves state-of-the-art performance. For instance, it achieves 73.0\% of the
AUC score and 96.3\% of the DP@20 score on the HOTC2020 dataset. The code will
be released at https://github.com/lgao001/HyMamba.

</details>


### [15] [Examining Vision Language Models through Multi-dimensional Experiments with Vision and Text Features](https://arxiv.org/abs/2509.08266)
*Saurav Sengupta,Nazanin Moradinasab,Jiebei Liu,Donald E. Brown*

Main category: cs.CV

TL;DR: 本文分析了视觉语言模型（VLMs）在处理视觉属性相关问题时，因输入图像和提示的细微变化导致表现差异显著。结果显示，VLMs对输入参数非常敏感，轻微调整即可明显影响其输出结果。


<details>
  <summary>Details</summary>
Motivation: 以前的研究表明，VLMs在问及图片视觉细节时习惯性依赖训练中形成的偏见，尤其是在需要聚焦细节的任务（如数图中特定元素数量）时更为明显。本研究旨在系统分析影响VLMs表现的输入数据（图像和提示）特征。

Method: 构建了一个多维度评估框架，利用开源VLMs，通过变化输入参数（如图片大小、对象数量、背景颜色、提示具体性）并分析注意力值的变化，来研究VLMs表现与输入特征的关系。

Result: 实验证明，哪怕是输入图像或提示的微小变化，都可能导致VLMs的答案和整体表现出现巨大波动。例如，模型在处理星星数量不同的国旗问题时，常无视视觉证据，难以准确回答。

Conclusion: VLMs对输入细节高度敏感。了解这种敏感性及其表现，有助于未来更有效地评估和改进VLM的鲁棒性及可靠性。

Abstract: Recent research on Vision Language Models (VLMs) suggests that they rely on
inherent biases learned during training to respond to questions about visual
properties of an image. These biases are exacerbated when VLMs are asked highly
specific questions that require focusing on specific areas of the image. For
example, a VLM tasked with counting stars on a modified American flag (e.g.,
with more than 50 stars) will often disregard the visual evidence and fail to
answer accurately. We build upon this research and develop a multi-dimensional
examination framework to systematically determine which characteristics of the
input data, including both the image and the accompanying prompt, lead to such
differences in performance. Using open-source VLMs, we further examine how
attention values fluctuate with varying input parameters (e.g., image size,
number of objects in the image, background color, prompt specificity). This
research aims to learn how the behavior of vision language models changes and
to explore methods for characterizing such changes. Our results suggest, among
other things, that even minor modifications in image characteristics and prompt
specificity can lead to large changes in how a VLM formulates its answer and,
subsequently, its overall performance.

</details>


### [16] [Generalized Zero-Shot Learning for Point Cloud Segmentation with Evidence-Based Dynamic Calibration](https://arxiv.org/abs/2509.08280)
*Hyeonseok Kim,Byeongkeun Kang,Yeejin Lee*

Main category: cs.CV

TL;DR: 本文提出了一种新方法E3DPC-GZSL，提升了3D点云广义零样本语义分割的准确性，尤其是在未见类别上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云的广义零样本分割任务面临模型对已见类别预测偏向（overconfident）的挑战，且3D领域训练数据相比图像较少，该问题更突出。

Method: 提出E3DPC-GZSL方法，通过在分类器中引入基于证据的不确定性估算器，用于动态调整每个点预测的校准堆叠因子，从而减弱对已见类别的过度自信。此外，设计了融合可学习参数与文本特征的新训练策略，优化对未见类别的泛化能力。

Result: 在ScanNet v2和S3DIS等数据集上，实验表明该方法取得了广义零样本语义分割的最新最优性能。

Conclusion: E3DPC-GZSL能够显著减缓分类器对已见类别的偏向，提高对未见类别的分割表现，整体提升广义零样本3D点云分割能力。

Abstract: Generalized zero-shot semantic segmentation of 3D point clouds aims to
classify each point into both seen and unseen classes. A significant challenge
with these models is their tendency to make biased predictions, often favoring
the classes encountered during training. This problem is more pronounced in 3D
applications, where the scale of the training data is typically smaller than in
image-based tasks. To address this problem, we propose a novel method called
E3DPC-GZSL, which reduces overconfident predictions towards seen classes
without relying on separate classifiers for seen and unseen data. E3DPC-GZSL
tackles the overconfidence problem by integrating an evidence-based uncertainty
estimator into a classifier. This estimator is then used to adjust prediction
probabilities using a dynamic calibrated stacking factor that accounts for
pointwise prediction uncertainty. In addition, E3DPC-GZSL introduces a novel
training strategy that improves uncertainty estimation by refining the semantic
space. This is achieved by merging learnable parameters with text-derived
features, thereby improving model optimization for unseen data. Extensive
experiments demonstrate that the proposed approach achieves state-of-the-art
performance on generalized zero-shot semantic segmentation datasets, including
ScanNet v2 and S3DIS.

</details>


### [17] [Dual-Thresholding Heatmaps to Cluster Proposals for Weakly Supervised Object Detection](https://arxiv.org/abs/2509.08289)
*Yuelin Guo,Haoyu He,Zhiyuan Chen,Zitong Huang,Renhao Lu,Lu Shi,Zejun Wang,Weizhe Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新型的弱监督目标检测方法，有效缓解了现有方法在伪GT框、背景表示及优化收敛速度等方面的问题，在PASCAL VOC 2007和2012上取得了优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督目标检测方法普遍基于多模块框架，但存在伪GT框不能完整覆盖目标或无法区分相邻目标的问题；基础网络结构缺乏背景类别表示且分支语义隔阂大；训练时忽略了一些候选区域，导致收敛缓慢。

Method: 1）提出了热图引导候选框选择算法（HGPS），通过双阈值策略用于生成更优的伪GT框；2）设计了增强版弱监督基础检测网络（WSBDN），为每个候选框引入背景类别，并通过热图进行预监督以缩小分支语义差距；3）引入了针对被忽略候选框的负确信监督损失以加速模型收敛。

Result: 在PASCAL VOC 2007和2012数据集上，本方法分别取得了58.5%/81.8%和55.6%/80.5%的mAP/mCorLoc分数，超越现有主流方法，充分证明了方法有效性。

Conclusion: 所提框架能更准确生成高质量伪GT框，有效弥补现有WSOD方法的三大缺陷。实验结果突出，展示其实际应用潜力，对未来弱监督目标检测提供了新思路。

Abstract: Weakly supervised object detection (WSOD) has attracted significant attention
in recent years, as it does not require box-level annotations. State-of-the-art
methods generally adopt a multi-module network, which employs WSDDN as the
multiple instance detection network module and multiple instance refinement
modules to refine performance. However, these approaches suffer from three key
limitations. First, existing methods tend to generate pseudo GT boxes that
either focus only on discriminative parts, failing to capture the whole object,
or cover the entire object but fail to distinguish between adjacent intra-class
instances. Second, the foundational WSDDN architecture lacks a crucial
background class representation for each proposal and exhibits a large semantic
gap between its branches. Third, prior methods discard ignored proposals during
optimization, leading to slow convergence. To address these challenges, we
first design a heatmap-guided proposal selector (HGPS) algorithm, which
utilizes dual thresholds on heatmaps to pre-select proposals, enabling pseudo
GT boxes to both capture the full object extent and distinguish between
adjacent intra-class instances. We then present a weakly supervised basic
detection network (WSBDN), which augments each proposal with a background class
representation and uses heatmaps for pre-supervision to bridge the semantic gap
between matrices. At last, we introduce a negative certainty supervision loss
on ignored proposals to accelerate convergence. Extensive experiments on the
challenging PASCAL VOC 2007 and 2012 datasets demonstrate the effectiveness of
our framework. We achieve mAP/mCorLoc scores of 58.5%/81.8% on VOC 2007 and
55.6%/80.5% on VOC 2012, performing favorably against the state-of-the-art WSOD
methods. Our code is publicly available at
https://github.com/gyl2565309278/DTH-CP.

</details>


### [18] [An Open Benchmark Dataset for GeoAI Foundation Models for Oil Palm Mapping in Indonesia](https://arxiv.org/abs/2509.08303)
*M. Warizmi Wafiq,Peter Cutter,Ate Poortinga,Daniel Marc G. dela Torre,Karis Tenneson,Vanna Teck,Enikoe Bihari,Chanarun Saisaward,Weraphong Suaruang,Andrea McMahon,Andi Vika Faradiba Muin,Karno B. Batiran,Chairil A,Nurul Qomar,Arya Arismaya Metananda,David Ganz,David Saah*

Main category: cs.CV

TL;DR: 本文公开发布了印度尼西亚油棕种植园及相关地表类型的高分辨率遥感数据集，为可持续发展和监管框架提供支持。


<details>
  <summary>Details</summary>
Motivation: 由于油棕种植是印度尼西亚森林砍伐的主要原因之一，需要详尽且可靠的地表覆盖图来更好地监控和管理油棕扩张，支持全球减少森林砍伐的目标。

Method: 研究团队利用2020-2024年的高分辨率卫星影像，由专家通过壁到壁方式进行数字化标注，形成多解释者共识并经实地验证。数据涵盖不同气候带及油棕种植分期，并对相似多年生作物进行区分。

Result: 该数据集以多边形形式、无缝覆盖整个研究区，并以分层类型学区分油棕阶段，确保高质量标注。数据集适用于传统卷积神经网络和新兴地理空间基础模型的训练与评测，并以CC-BY协议开放获取。

Conclusion: 该数据集填补了遥感地表覆盖类型识别领域训练数据的空白，有助于提高油棕种植区和相关地类制图的准确性，推动透明监管和全球减缓森林砍伐目标的实现，符合FAIR数据原则。

Abstract: Oil palm cultivation remains one of the leading causes of deforestation in
Indonesia. To better track and address this challenge, detailed and reliable
mapping is needed to support sustainability efforts and emerging regulatory
frameworks. We present an open-access geospatial dataset of oil palm
plantations and related land cover types in Indonesia, produced through expert
labeling of high-resolution satellite imagery from 2020 to 2024. The dataset
provides polygon-based, wall-to-wall annotations across a range of
agro-ecological zones and includes a hierarchical typology that distinguishes
oil palm planting stages as well as similar perennial crops. Quality was
ensured through multi-interpreter consensus and field validation. The dataset
was created using wall-to-wall digitization over large grids, making it
suitable for training and benchmarking both conventional convolutional neural
networks and newer geospatial foundation models. Released under a CC-BY
license, it fills a key gap in training data for remote sensing and aims to
improve the accuracy of land cover types mapping. By supporting transparent
monitoring of oil palm expansion, the resource contributes to global
deforestation reduction goals and follows FAIR data principles.

</details>


### [19] [SimCroP: Radiograph Representation Learning with Similarity-driven Cross-granularity Pre-training](https://arxiv.org/abs/2509.08311)
*Rongsheng Wang,Fenghe Tang,Qingsong Yao,Rui Yan,Xu Zhang,Zhen Huang,Haoran Lai,Zhiyang He,Xiaodong Tao,Zihang Jiang,Shaohua Kevin Zhou*

Main category: cs.CV

TL;DR: 本论文提出了SimCroP框架，通过相似性驱动的跨粒度预训练，提升了胸部CT影像的解读能力，实验显示显著优于已有方法。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉-语言预训练方法难以处理CT影像中病灶稀疏、结构复杂的问题，以及报告中不同病理描述与影像子区域之间隐晦的关系。

Method: 提出了SimCroP框架，通过多模态掩码建模优化编码器低层次语义理解，采用相似性驱动对齐机制实现报告语句与CT影像块的智能对齐，设计跨粒度融合模块用于融合实例级和词-块级多模态信息。

Result: 在大规模配对CT-报告数据集上预训练，并在五个公开数据集的图像分类和分割任务中进行验证。实验结果显示SimCroP优于最新的医学自监督学习和医学视觉-语言预训练方法。

Conclusion: SimCroP能够更好地捕捉CT影像稀疏病理结构，有效提升跨任务的下游性能，是胸部CT影像多模态学习的有力方法。

Abstract: Medical vision-language pre-training shows great potential in learning
representative features from massive paired radiographs and reports. However,
in computed tomography (CT) scans, the distribution of lesions which contain
intricate structures is characterized by spatial sparsity. Besides, the complex
and implicit relationships between different pathological descriptions in each
sentence of the report and their corresponding sub-regions in radiographs pose
additional challenges. In this paper, we propose a Similarity-Driven
Cross-Granularity Pre-training (SimCroP) framework on chest CTs, which combines
similarity-driven alignment and cross-granularity fusion to improve radiograph
interpretation. We first leverage multi-modal masked modeling to optimize the
encoder for understanding precise low-level semantics from radiographs. Then,
similarity-driven alignment is designed to pre-train the encoder to adaptively
select and align the correct patches corresponding to each sentence in reports.
The cross-granularity fusion module integrates multimodal information across
instance level and word-patch level, which helps the model better capture key
pathology structures in sparse radiographs, resulting in improved performance
for multi-scale downstream tasks. SimCroP is pre-trained on a large-scale
paired CT-reports dataset and validated on image classification and
segmentation tasks across five public datasets. Experimental results
demonstrate that SimCroP outperforms both cutting-edge medical self-supervised
learning methods and medical vision-language pre-training methods. Codes and
models are available at https://github.com/ToniChopp/SimCroP.

</details>


### [20] [Boosted Training of Lightweight Early Exits for Optimizing CNN Image Classification Inference](https://arxiv.org/abs/2509.08318)
*Yehudit Aperstein,Alexander Apartsin*

Main category: cs.CV

TL;DR: 本文提出了一种新的早退式推理训练方案BTS-EE，在资源受限平台上实现实时图像分类时，实现了更优的效率-精度权衡。该方法通过顺序训练和校准每个分支，减少了推理与训练分布不一致带来的问题，并结合轻量分支结构和新型阈值调整方法，显著提升了早退CNN的实际性能。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备和嵌入式系统等资源受限环境下，深度学习推理需要权衡准确率、延迟和功耗。早退（Early-exit）方法虽能提升效率，但传统训练方式导致推理时分支处理的数据分布发生变化，影响实际效率和精度。因此，有必要提出更合理的训练和分支设计方法，优化早退CNN的部署价值。

Method: 作者提出Boosted Training Scheme for Early Exits（BTS-EE），即早退分支依次训练与校准，保证每个分支见到与其真实推理时一致的数据分布。此外，引入1D卷积轻量化分支结构，并设计Class Precision Margin（CPM）类精度边界校准方法，实现对不同类别独立的出口阈值调整。

Result: 在CINIC-10数据集和ResNet18骨架下，BTS-EE方法在64种不同配置实验中，相较于非BOOST训练一致性提升，在仅损失2%精度的情况下，最多可减少45%的计算量。

Conclusion: BTS-EE及其配套方法拓展了早退CNN高效部署的设计空间，为工业检测、嵌入式视觉和无人机监控等实时场景带来了更实用的推理效率提升，结合新型分支与校准设计，实际效率-精度权衡得到显著优化。

Abstract: Real-time image classification on resource-constrained platforms demands
inference methods that balance accuracy with strict latency and power budgets.
Early-exit strategies address this need by attaching auxiliary classifiers to
intermediate layers of convolutional neural networks (CNNs), allowing "easy"
samples to terminate inference early. However, conventional training of early
exits introduces a covariance shift: downstream branches are trained on full
datasets, while at inference they process only the harder, non-exited samples.
This mismatch limits efficiency--accuracy trade-offs in practice. We introduce
the Boosted Training Scheme for Early Exits (BTS-EE), a sequential training
approach that aligns branch training with inference-time data distributions.
Each branch is trained and calibrated before the next, ensuring robustness
under selective inference conditions. To further support embedded deployment,
we propose a lightweight branch architecture based on 1D convolutions and a
Class Precision Margin (CPM) calibration method that enables per-class
threshold tuning for reliable exit decisions. Experiments on the CINIC-10
dataset with a ResNet18 backbone demonstrate that BTS-EE consistently
outperforms non-boosted training across 64 configurations, achieving up to 45
percent reduction in computation with only 2 percent accuracy degradation.
These results expand the design space for deploying CNNs in real-time image
processing systems, offering practical efficiency gains for applications such
as industrial inspection, embedded vision, and UAV-based monitoring.

</details>


### [21] [Retrieval-Augmented VLMs for Multimodal Melanoma Diagnosis](https://arxiv.org/abs/2509.08338)
*Jihyun Moon,Charmgil Hong*

Main category: cs.CV

TL;DR: 本文提出了一种基于检索增强视觉-语言模型（VLM）的方法，将相似病例作为诊断提示，显著提升了黑色素瘤的诊断准确性和错误纠正能力。


<details>
  <summary>Details</summary>
Motivation: 现有卷积神经网络（CNN）在皮肤镜图像诊断中忽视了临床元数据，视觉-语言模型虽然支持多模态信息，但若仅基于通用领域数据训练，则难以把握临床细节。需要一种兼顾多模态信息和临床特异性的方法来提升皮肤黑色素瘤的早期准确诊断效果。

Method: 设计了一种检索增强的视觉-语言模型框架，将与当前患者病例在语义上相似的历史病例纳入到诊断提示（prompt），无需微调模型即可进行诊断，并基于这些丰富的诊断提示做出更准确的预测。

Result: 该方法在无需微调的情况下，显著提升了诊断分类准确率和错误纠正能力，优于传统的基线方法。

Conclusion: 检索增强的提示策略为临床决策支持提供了一种更鲁棒、有效的手段，有望促进AI在医疗辅助诊断中的应用。

Abstract: Accurate and early diagnosis of malignant melanoma is critical for improving
patient outcomes. While convolutional neural networks (CNNs) have shown promise
in dermoscopic image analysis, they often neglect clinical metadata and require
extensive preprocessing. Vision-language models (VLMs) offer a multimodal
alternative but struggle to capture clinical specificity when trained on
general-domain data. To address this, we propose a retrieval-augmented VLM
framework that incorporates semantically similar patient cases into the
diagnostic prompt. Our method enables informed predictions without fine-tuning
and significantly improves classification accuracy and error correction over
conventional baselines. These results demonstrate that retrieval-augmented
prompting provides a robust strategy for clinical decision support.

</details>


### [22] [InsFusion: Rethink Instance-level LiDAR-Camera Fusion for 3D Object Detection](https://arxiv.org/abs/2509.08374)
*Zhongyu Xia,Hansong Yang,Yongtao Wang*

Main category: cs.CV

TL;DR: 提出InsFusion方法，通过结合原始和融合特征提取候选区域，并用注意力机制查询原始特征，缓解特征提取过程中的噪声和误差积累，实现3D目标检测新SOTA。


<details>
  <summary>Details</summary>
Motivation: 多视角相机和激光雷达融合用于3D目标检测是自动驾驶和智能交通的关键。但在特征提取和融合过程中，噪声和误差会逐步积累，影响检测精度。作者旨在减少这些误差带来的负面影响。

Method: 提出InsFusion：从原始与融合特征中分别提取候选区域（proposal），并通过这些proposal对原始特征进行查询，结合注意力机制，有效减少特征处理过程中的误差累积影响。

Result: 在nuScenes数据集上的实验表明，InsFusion与多种先进基线方法兼容，并在3D目标检测任务上取得了新的SOTA性能。

Conclusion: InsFusion有效缓解了噪声与误差累计问题，提升了3D目标检测的准确性和鲁棒性，并推动了领域性能上限。

Abstract: Three-dimensional Object Detection from multi-view cameras and LiDAR is a
crucial component for autonomous driving and smart transportation. However, in
the process of basic feature extraction, perspective transformation, and
feature fusion, noise and error will gradually accumulate. To address this
issue, we propose InsFusion, which can extract proposals from both raw and
fused features and utilizes these proposals to query the raw features, thereby
mitigating the impact of accumulated errors. Additionally, by incorporating
attention mechanisms applied to the raw features, it thereby mitigates the
impact of accumulated errors. Experiments on the nuScenes dataset demonstrate
that InsFusion is compatible with various advanced baseline methods and
delivers new state-of-the-art performance for 3D object detection.

</details>


### [23] [Bitrate-Controlled Diffusion for Disentangling Motion and Content in Video](https://arxiv.org/abs/2509.08376)
*Xiao Li,Qi Chen,Xiulian Peng,Kai Yu,Xie Chen,Yan Lu*

Main category: cs.CV

TL;DR: 本论文提出一种全新的自监督框架，将视频数据拆解为动态运动和静态内容两个部分，通过Transformer架构和矢量量化促进解耦，能有效应用于多种类型视频的表示学习和生成任务。


<details>
  <summary>Details</summary>
Motivation: 现有视频表征学习方法解耦运动与内容时往往依赖较强的假设或具备较强归纳偏置，难以普适且泛化能力有限。因此，亟需一种假设更少、适用范围更广的自监督视频解耦方法。

Method: 作者提出采用基于Transformer的架构，分别生成适用于帧级（motion）以及片段级（content）的灵活隐式特征，通过低比特率的矢量量化制造信息瓶颈，促进运动成分与内容成分的分离。将这些经过压缩的运动与内容特征作为条件输入，喂给去噪扩散模型实现自监督学习。

Result: 在真实说话人视频的动作迁移和自回归生成任务上，验证了所提解耦框架的有效性。此外，框架也能推广至2D卡通角色像素视频等多种类型数据，展现出了良好的泛化能力。

Conclusion: 作者的方法在假设和归纳偏置更弱的情况下，成功实现了自监督、可泛化的视频运动与内容解耦，为视频分析与生成领域提供了新的视角和工具。

Abstract: We propose a novel and general framework to disentangle video data into its
dynamic motion and static content components. Our proposed method is a
self-supervised pipeline with less assumptions and inductive biases than
previous works: it utilizes a transformer-based architecture to jointly
generate flexible implicit features for frame-wise motion and clip-wise
content, and incorporates a low-bitrate vector quantization as an information
bottleneck to promote disentanglement and form a meaningful discrete motion
space. The bitrate-controlled latent motion and content are used as conditional
inputs to a denoising diffusion model to facilitate self-supervised
representation learning. We validate our disentangled representation learning
framework on real-world talking head videos with motion transfer and
auto-regressive motion generation tasks. Furthermore, we also show that our
method can generalize to other types of video data, such as pixel sprites of 2D
cartoon characters. Our work presents a new perspective on self-supervised
learning of disentangled video representations, contributing to the broader
field of video analysis and generation.

</details>


### [24] [Semantic Causality-Aware Vision-Based 3D Occupancy Prediction](https://arxiv.org/abs/2509.08388)
*Dubing Chen,Huan Zheng,Yucheng Zhou,Xianfei Li,Wenlong Liao,Tao He,Pai Peng,Jianbing Shen*

Main category: cs.CV

TL;DR: 本文提出一种全新的因果损失函数，将2D视觉特征有效转化为3D语义体素，显著提升了端到端可训练性和预测准确性，在Occ3D基准上达到了SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有的基于视觉的3D语义占据预测方法通常采用模块化流程，每个模块独立优化或用预设输入，导致模块间误差累积，整体效果和一致性受限。作者希望通过打通2D到3D的端到端学习，消除误差传递，提高各组件的协作效果。

Method: 作者设计了一种新颖的因果损失（causal loss），使2D到3D的每个模块都能接受端到端监督，保证梯度能从3D体素全部传回2D特征层。具体的架构包括三部分：通道分组提升（Channel-Grouped Lifting）用于自适应的语义映射、可学习的相机偏移（Learnable Camera Offsets）提升对相机噪声的鲁棒性、归一化卷积（Normalized Convolution）实现有效特征传播。

Result: 在Occ3D基准测试上，该方法取得了最新最优（SOTA）成绩。大量实验表明，提出的方法对于相机参数扰动表现出更高的鲁棒性，提升了2D到3D语义一致性。

Conclusion: 通过因果损失实现2D到3D语义占据预测全过程端到端可训练，大幅提升了准确性与稳健性，对3D视觉任务具有重要推动作用。

Abstract: Vision-based 3D semantic occupancy prediction is a critical task in 3D vision
that integrates volumetric 3D reconstruction with semantic understanding.
Existing methods, however, often rely on modular pipelines. These modules are
typically optimized independently or use pre-configured inputs, leading to
cascading errors. In this paper, we address this limitation by designing a
novel causal loss that enables holistic, end-to-end supervision of the modular
2D-to-3D transformation pipeline. Grounded in the principle of 2D-to-3D
semantic causality, this loss regulates the gradient flow from 3D voxel
representations back to the 2D features. Consequently, it renders the entire
pipeline differentiable, unifying the learning process and making previously
non-trainable components fully learnable. Building on this principle, we
propose the Semantic Causality-Aware 2D-to-3D Transformation, which comprises
three components guided by our causal loss: Channel-Grouped Lifting for
adaptive semantic mapping, Learnable Camera Offsets for enhanced robustness
against camera perturbations, and Normalized Convolution for effective feature
propagation. Extensive experiments demonstrate that our method achieves
state-of-the-art performance on the Occ3D benchmark, demonstrating significant
robustness to camera perturbations and improved 2D-to-3D semantic consistency.

</details>


### [25] [VRAE: Vertical Residual Autoencoder for License Plate Denoising and Deblurring](https://arxiv.org/abs/2509.08392)
*Cuong Nguyen,Dung T. Tran,Hong Nguyen,Xuan-Vu Phan,Nam-Phong Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种用于交通监控图像增强的垂直残差自编码器（VRAE）方法，能够在实时条件下有效提升受损图像质量，从而提高车牌识别的准确率，实验结果优于现有多种方法。


<details>
  <summary>Details</summary>
Motivation: 实际交通监控中，车辆图像常因恶劣天气、光线不足或车速过快而受到噪声和模糊影响，特别是车牌区域较小时更难识别，因此亟需一种高效的图像增强方法，作为车牌识别的预处理步骤，提高整体系统性能。

Method: 提出VRAE结构，通过在编码阶段引入辅助模块，为每一层注入与输入相关的特征，引导更有效的信息保留与表示提升；与传统自编码器相比，能在网络各层更好地保持原始图像信息。该方法在包含车牌的交通监控数据集上进行测试，并与AE、GAN和Flow-Based方法对比。

Result: VRAE在实验中表现优异：在与相同深度AE对比时，提升PSNR约20%，降低NMSE约50%，提升SSIM约1%，而参数量仅增加约1%；同时也优于GAN和Flow-Based方法。

Conclusion: VRAE作为一种高效的实时图像增强算法，在提升交通监控中受损车牌图像质量及后续识别准确率方面具有显著优势，具备实际部署潜力。

Abstract: In real-world traffic surveillance, vehicle images captured under adverse
weather, poor lighting, or high-speed motion often suffer from severe noise and
blur. Such degradations significantly reduce the accuracy of license plate
recognition systems, especially when the plate occupies only a small region
within the full vehicle image. Restoring these degraded images a fast realtime
manner is thus a crucial pre-processing step to enhance recognition
performance. In this work, we propose a Vertical Residual Autoencoder (VRAE)
architecture designed for the image enhancement task in traffic surveillance.
The method incorporates an enhancement strategy that employs an auxiliary
block, which injects input-aware features at each encoding stage to guide the
representation learning process, enabling better general information
preservation throughout the network compared to conventional autoencoders.
Experiments on a vehicle image dataset with visible license plates demonstrate
that our method consistently outperforms Autoencoder (AE), Generative
Adversarial Network (GAN), and Flow-Based (FB) approaches. Compared with AE at
the same depth, it improves PSNR by about 20\%, reduces NMSE by around 50\%,
and enhances SSIM by 1\%, while requiring only a marginal increase of roughly
1\% in parameters.

</details>


### [26] [Sparse BEV Fusion with Self-View Consistency for Multi-View Detection and Tracking](https://arxiv.org/abs/2509.08421)
*Keisuke Toida,Taigo Sakai,Naoki Kato,Kazutoyo Yokota,Takeshi Nakamura,Kazuhiro Hotta*

Main category: cs.CV

TL;DR: 该论文提出SCFusion框架，改进BEV特征融合方法，有效提升多摄像头环境下的多目标检测与跟踪性能，在主流数据集上取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 多摄像头场景下保持目标一致性难度大，当前BEV投影虽能缓解遮挡问题，但会带来特征扭曲和密度不均，降低检测与跟踪精度，因此需要新的特征融合策略。

Method: SCFusion包含三项关键技术：1) 稀疏变换，减少BEV投影过程中的非自然插值；2) 密度感知加权，根据空间置信度和距离自适应融合特征；3) 多视角一致性损失，增强各摄像头独立判别能力后再融合。

Result: SCFusion在WildTrack数据集上IDF1分数达到95.9%，在MultiviewX数据集上MODP为89.2%，超越现有方法TrackTacular，取得业界最高性能。

Conclusion: SCFusion有效克服传统BEV投影的局限，显著提升了多摄像头目标检测与跟踪的鲁棒性和准确性，具有广泛的应用前景。

Abstract: Multi-View Multi-Object Tracking (MVMOT) is essential for applications such
as surveillance, autonomous driving, and sports analytics. However, maintaining
consistent object identities across multiple cameras remains challenging due to
viewpoint changes, lighting variations, and occlusions, which often lead to
tracking errors.Recent methods project features from multiple cameras into a
unified Bird's-Eye-View (BEV) space to improve robustness against occlusion.
However, this projection introduces feature distortion and non-uniform density
caused by variations in object scale with distance. These issues degrade the
quality of the fused representation and reduce detection and tracking
accuracy.To address these problems, we propose SCFusion, a framework that
combines three techniques to improve multi-view feature integration. First, it
applies a sparse transformation to avoid unnatural interpolation during
projection. Next, it performs density-aware weighting to adaptively fuse
features based on spatial confidence and camera distance. Finally, it
introduces a multi-view consistency loss that encourages each camera to learn
discriminative features independently before fusion.Experiments show that
SCFusion achieves state-of-the-art performance, reaching an IDF1 score of 95.9%
on WildTrack and a MODP of 89.2% on MultiviewX, outperforming the baseline
method TrackTacular. These results demonstrate that SCFusion effectively
mitigates the limitations of conventional BEV projection and provides a robust
and accurate solution for multi-view object detection and tracking.

</details>


### [27] [LD-ViCE: Latent Diffusion Model for Video Counterfactual Explanations](https://arxiv.org/abs/2509.08422)
*Payal Varshney,Adriano Lucieri,Christoph Balada,Sheraz Ahmed,Andreas Dengel*

Main category: cs.CV

TL;DR: 本文提出了一种用于视频AI系统的新型可解释性方法LD-ViCE，通过潜变量扩散模型生成更为真实和具备时序连贯性的反事实解释，提升了效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频AI系统可解释性方法常因时序一致性、解释稳健性及因果洞察不足而难以信任，尤其在安全关键领域（如自动驾驶、医疗）更显突出。因此有必要开发更高质量、更符合人类认知的解释方法。

Method: 提出LD-ViCE框架，将现代扩散模型应用于视频反事实解释，借助模型潜空间来高效生成反事实，通过额外精细化步骤提升解释的可用性和现实感。

Result: 在三个公开数据集（心脏超声、面部表情、动作识别）上实验证明，LD-ViCE在R2分数上比最新方法最高提升68%，推理时间减半，同时生成的解释更符合语义和时序一致性。

Conclusion: LD-ViCE能够提升视频AI模型可解释性的质量与效率，助力AI系统在自动驾驶和医疗等安全关键场景中的可信落地。

Abstract: Video-based AI systems are increasingly adopted in safety-critical domains
such as autonomous driving and healthcare. However, interpreting their
decisions remains challenging due to the inherent spatiotemporal complexity of
video data and the opacity of deep learning models. Existing explanation
techniques often suffer from limited temporal coherence, insufficient
robustness, and a lack of actionable causal insights. Current counterfactual
explanation methods typically do not incorporate guidance from the target
model, reducing semantic fidelity and practical utility. We introduce Latent
Diffusion for Video Counterfactual Explanations (LD-ViCE), a novel framework
designed to explain the behavior of video-based AI models. Compared to previous
approaches, LD-ViCE reduces the computational costs of generating explanations
by operating in latent space using a state-of-the-art diffusion model, while
producing realistic and interpretable counterfactuals through an additional
refinement step. Our experiments demonstrate the effectiveness of LD-ViCE
across three diverse video datasets, including EchoNet-Dynamic (cardiac
ultrasound), FERV39k (facial expression), and Something-Something V2 (action
recognition). LD-ViCE outperforms a recent state-of-the-art method, achieving
an increase in R2 score of up to 68% while reducing inference time by half.
Qualitative analysis confirms that LD-ViCE generates semantically meaningful
and temporally coherent explanations, offering valuable insights into the
target model behavior. LD-ViCE represents a valuable step toward the
trustworthy deployment of AI in safety-critical domains.

</details>


### [28] [Beyond Distribution Shifts: Adaptive Hyperspectral Image Classification at Test Time](https://arxiv.org/abs/2509.08436)
*Xia Yue,Anfeng Liu,Ning Chen,Chenjia Huang,Hui Liu,Zhou Huang,Leyuan Fang*

Main category: cs.CV

TL;DR: 本文提出HyperTTA框架，增强超光谱图像分类在各种现实降质环境下的鲁棒性，通过自建多降质数据集、优化分类器和引入测试时自适应策略，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 超光谱图像分类模型对噪声、模糊、压缩和大气效应等真实世界降质非常敏感，实际应用鲁棒性差，需要有效方法提升模型应对多种降质的能力。

Method: 1）构建覆盖九种代表性降质的多降质超光谱数据集。2）设计带多尺度感受野和标签平滑正则化的谱—空间Transformer分类器，以增强泛化能力。3）提出轻量级测试时自适应方法——置信度感知熵最小化LayerNorm适配器（CELA），只在高置信度无标注目标样本上，通过最小化熵并只更新LayerNorm层参数，实现无源数据的动态鲁棒自适应。

Result: 在两个超光谱基准数据集上的大量实验显示，HyperTTA在各种降质情况下均明显优于现有主流方法，表现出更好的鲁棒性和泛化能力。

Conclusion: HyperTTA框架和方法有效增强了超光谱图像分类任务面对多降质环境的鲁棒性，是提升实际应用性能的有价值工具和基线。

Abstract: Hyperspectral image (HSI) classification models are highly sensitive to
distribution shifts caused by various real-world degradations such as noise,
blur, compression, and atmospheric effects. To address this challenge, we
propose HyperTTA, a unified framework designed to enhance model robustness
under diverse degradation conditions. Specifically, we first construct a
multi-degradation hyperspectral dataset that systematically simulates nine
representative types of degradations, providing a comprehensive benchmark for
robust classification evaluation. Based on this, we design a spectral-spatial
transformer classifier (SSTC) enhanced with a multi-level receptive field
mechanism and label smoothing regularization to jointly capture multi-scale
spatial context and improve generalization. Furthermore, HyperTTA incorporates
a lightweight test-time adaptation (TTA) strategy, the confidence-aware
entropy-minimized LayerNorm adapter (CELA), which updates only the affine
parameters of LayerNorm layers by minimizing prediction entropy on
high-confidence unlabeled target samples. This confidence-aware adaptation
prevents unreliable updates from noisy predictions, enabling robust and dynamic
adaptation without access to source data or target annotations. Extensive
experiments on two benchmark datasets demonstrate that HyperTTA outperforms
existing baselines across a wide range of degradation scenarios, validating the
effectiveness of both its classification backbone and the proposed TTA scheme.
Code will be made available publicly.

</details>


### [29] [Spherical Brownian Bridge Diffusion Models for Conditional Cortical Thickness Forecasting](https://arxiv.org/abs/2509.08442)
*Ivan Stoyanov,Fabian Bongratz,Christian Wachinger*

Main category: cs.CV

TL;DR: 本文提出一种用于精确预测个体高分辨率皮层厚度轨迹的新方法：球面布朗桥扩散模型（SBDM），并在两个真实数据集上表现出更低的预测误差，还能生成事实与反事实的个体化变化路径。


<details>
  <summary>Details</summary>
Motivation: 个体皮层厚度变化的准确预测对于揭示神经退行性过程、早期干预与疾病进程追踪有重要意义。然而，由于大脑皮层的复杂几何结构和多模态信息的整合需求，预测存在较大挑战。

Method: 作者提出了双向条件布朗桥扩散过程（SBDM），在皮层顶点层面进行轨迹预测。为去噪过程设计了条件球面U-Net（CoS-UNet）模型，结合球面卷积与密集跨模态注意力机制，能有效整合皮层表面数据和表格条件信息。

Result: 在ADNI和OASIS两个纵向数据集上，SBDM模型相较此前方法实现了显著更低的预测误差。此外，新模型还能生成个体化的事实和反事实皮层厚度变化轨迹。

Conclusion: SBDM为个体化皮层厚度序列的高精度预测和潜在因素探索提供了新的技术框架，有望推动疾病早期诊断与机制研究。

Abstract: Accurate forecasting of individualized, high-resolution cortical thickness
(CTh) trajectories is essential for detecting subtle cortical changes,
providing invaluable insights into neurodegenerative processes and facilitating
earlier and more precise intervention strategies. However, CTh forecasting is a
challenging task due to the intricate non-Euclidean geometry of the cerebral
cortex and the need to integrate multi-modal data for subject-specific
predictions. To address these challenges, we introduce the Spherical Brownian
Bridge Diffusion Model (SBDM). Specifically, we propose a bidirectional
conditional Brownian bridge diffusion process to forecast CTh trajectories at
the vertex level of registered cortical surfaces. Our technical contribution
includes a new denoising model, the conditional spherical U-Net (CoS-UNet),
which combines spherical convolutions and dense cross-attention to integrate
cortical surfaces and tabular conditions seamlessly. Compared to previous
approaches, SBDM achieves significantly reduced prediction errors, as
demonstrated by our experiments based on longitudinal datasets from the ADNI
and OASIS. Additionally, we demonstrate SBDM's ability to generate individual
factual and counterfactual CTh trajectories, offering a novel framework for
exploring hypothetical scenarios of cortical development.

</details>


### [30] [First-order State Space Model for Lightweight Image Super-resolution](https://arxiv.org/abs/2509.08458)
*Yujie Zhu,Xinyi Zhang,Yekai Lu,Guang Yang,Faming Fang,Guixu Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种改进的Mamba SSM模块——一阶状态空间模型（FSSM），在不增加参数量的前提下，显著提升了轻量级超分辨率模型的性能，达到了最新的SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 现有的Mamba类视觉模型主要关注网络结构和扫描方式，较少深入研究SSM模块本身的改进潜力。作者希望通过优化SSM计算过程，在参数量不变的情况下，提升轻量级超分辨率（SR）任务中的性能表现。

Method: 提出了FSSM，即在SSM中引入一阶保持条件，推导出新的离散形式，并在该过程中增强了token之间的相关性。同时分析了累计误差。该模型应用在改进后的MambaIR框架中。

Result: 实验表明，FSSM在五个基准数据集上提升了MambaIR的性能，且参数量无增加，并且超越了现有的轻量级超分辨率方法，取得SOTA结果。

Conclusion: 通过对SSM模块的计算方式进行创新性改进，FSSM在保持高效和轻量的同时，提升了超分辨率任务的表现，证明了深入挖掘SSM潜力的价值。

Abstract: State space models (SSMs), particularly Mamba, have shown promise in NLP
tasks and are increasingly applied to vision tasks. However, most Mamba-based
vision models focus on network architecture and scan paths, with little
attention to the SSM module. In order to explore the potential of SSMs, we
modified the calculation process of SSM without increasing the number of
parameters to improve the performance on lightweight super-resolution tasks. In
this paper, we introduce the First-order State Space Model (FSSM) to improve
the original Mamba module, enhancing performance by incorporating token
correlations. We apply a first-order hold condition in SSMs, derive the new
discretized form, and analyzed cumulative error. Extensive experimental results
demonstrate that FSSM improves the performance of MambaIR on five benchmark
datasets without additionally increasing the number of parameters, and
surpasses current lightweight SR methods, achieving state-of-the-art results.

</details>


### [31] [Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles](https://arxiv.org/abs/2509.08777)
*Eric Slyman,Mehrab Tanjim,Kushal Kafle,Stefan Lee*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态提示集成方法（MMB），用于提升多模态大模型评估文本生成图像（TTI）系统时的可靠性和一致性。通过引入贝叶斯集成与图像聚类机制，有效缓解了现有方法中的偏见和过度自信等问题。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大模型常用于自动化评估TTI系统，但存在偏见、置信度过高以及跨图像域一致性差的问题。传统的文本提示集成法在多模态设定下效果有限，亟需针对多模态特点的新方法。

Method: 提出了一种新的多模态贝叶斯提示集成（MMB）方法：结合贝叶斯提示集成和基于图像聚类的动态加权，使判别模型能按照不同样本的视觉特征分配提示权重，从而自适应地提升评估准确性和置信度校准。

Result: 在HPSv2和MJBench两个主流TTI评测基准上，MMB方法在成对偏好判断准确率和置信度校准方面均优于现有基线，与人工标注一致性更高，对不同类型图像均有更好泛化能力。

Conclusion: 本研究证明多模态特定的提示集成和校准策略对于大规模TTI系统的评测极为关键。所提MMB方法为提升多模态评测模型的可靠性与置信度指明了方向，并推动了相关领域评测工具的发展。

Abstract: Multimodal large language models (MLLMs) are increasingly used to evaluate
text-to-image (TTI) generation systems, providing automated judgments based on
visual and textual context. However, these "judge" models often suffer from
biases, overconfidence, and inconsistent performance across diverse image
domains. While prompt ensembling has shown promise for mitigating these issues
in unimodal, text-only settings, our experiments reveal that standard
ensembling methods fail to generalize effectively for TTI tasks. To address
these limitations, we propose a new multimodal-aware method called Multimodal
Mixture-of-Bayesian Prompt Ensembles (MMB). Our method uses a Bayesian prompt
ensemble approach augmented by image clustering, allowing the judge to
dynamically assign prompt weights based on the visual characteristics of each
sample. We show that MMB improves accuracy in pairwise preference judgments and
greatly enhances calibration, making it easier to gauge the judge's true
uncertainty. In evaluations on two TTI benchmarks, HPSv2 and MJBench, MMB
outperforms existing baselines in alignment with human annotations and
calibration across varied image content. Our findings highlight the importance
of multimodal-specific strategies for judge calibration and suggest a promising
path forward for reliable large-scale TTI evaluation.

</details>


### [32] [Maximally Useful and Minimally Redundant: The Key to Self Supervised Learning for Imbalanced Data](https://arxiv.org/abs/2509.08469)
*Yash Kumar Sharma,Vineet Nair,Wilson Naik*

Main category: cs.CV

TL;DR: 本文提出了一种基于“多于两个视角”理论的对比自监督学习方法，有效改善不平衡数据集的表示学习，并在多个基准数据集上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 对比自监督学习（CSSL）在平衡数据集上表现良好，但在类别不平衡的数据集上泛化能力较差。目前对其鲁棒性的研究有限，因此需要探索能提升不平衡数据集表现的新方法。

Method: 作者提出将LeCun提到的“多于两个视角”理论引入CSSL，并以互信息为理论基础，设计出一种新的损失函数，过滤极端特征、区分类内与类间判别特征，从而更好地提取尾部类别代表性属性。此外，该方法适用于对比和非对比的自监督框架。

Result: 在Cifar10-LT、Cifar100-LT和Imagenet-LT等典型不平衡数据集上，采用所提方法分别用Resnet-18和Resnet-50取得了2%、5%和3%的准确率提升，刷新了自监督不平衡分类任务的最新水平。

Conclusion: 通过引入多视角理论和新型损失函数，有效提升了不平衡数据集的自监督学习表现，验证了新方法的理论与实际效用。

Abstract: The robustness of contrastive self-supervised learning (CSSL) for imbalanced
datasets is largely unexplored. CSSL usually makes use of \emph{multi-view}
assumptions to learn discriminatory features via similar and dissimilar data
samples. CSSL works well on balanced datasets, but does not generalize well for
imbalanced datasets. In a very recent paper, as part of future work, Yann LeCun
pointed out that the self-supervised multiview framework can be extended to
cases involving \emph{more than two views}. Taking a cue from this insight we
propose a theoretical justification based on the concept of \emph{mutual
information} to support the \emph{more than two views} objective and apply it
to the problem of dataset imbalance in self-supervised learning. The proposed
method helps extract representative characteristics of the tail classes by
segregating between \emph{intra} and \emph{inter} discriminatory
characteristics. We introduce a loss function that helps us to learn better
representations by filtering out extreme features. Experimental evaluation on a
variety of self-supervised frameworks (both contrastive and non-contrastive)
also prove that the \emph{more than two view} objective works well for
imbalanced datasets. We achieve a new state-of-the-art accuracy in
self-supervised imbalanced dataset classification (2\% improvement in
Cifar10-LT using Resnet-18, 5\% improvement in Cifar100-LT using Resnet-18, 3\%
improvement in Imagenet-LT (1k) using Resnet-50).

</details>


### [33] [Prompt-Driven Image Analysis with Multimodal Generative AI: Detection, Segmentation, Inpainting, and Interpretation](https://arxiv.org/abs/2509.08489)
*Kaleem Ahmad*

Main category: cs.CV

TL;DR: 本文提出了一个端到端的统一管道，将基于自然语言指令的图像检测、分割、编辑和描述无缝整合到一个工作流中，实现了从单一提示到多步骤图像分析操作。


<details>
  <summary>Details</summary>
Motivation: 随着视觉-语言模型能力的发展，用户用自然语言控制图像分析/编辑任务的需求增长，但现有流程通常碎片化、操作繁杂、结果不够可控。作者希望设计一个透明、可重复、易调试的、多模态统一操作流程，提升任务可靠性和用户体验。

Method: 系统集成开放词汇检测、可提示分割、文本条件的修复、视觉-语言描述四大模块。支持命令行和交互式UI，能在过程各步输出中间成果（如分割掩码、编辑前后对比等），集成了阈值微调、掩码形态处理和资源自适应策略，支持完整日志追踪、版本固定和随机种子控制，便于复现和调试。

Result: 在小规模、单词级提示下，检测与分割模块产生的掩码90%以上为可用，准确率超85%。修复模块（inpainting）在高配GPU下占总时间60-75%，显示需要仔细调优。论文也给出了门槛设定、掩码紧致度、扩散参数等实用经验。

Conclusion: 作者提出的多模态“单提示统一管道”，实现了高透明度、稳定可靠的视觉编辑与分析，对场景增强、目标替换、对象移除等任务有很强实用价值，总结了降脆弱性的工程实践，为搭建一站式提示驱动视觉系统提供参考。

Abstract: Prompt-driven image analysis converts a single natural-language instruction
into multiple steps: locate, segment, edit, and describe. We present a
practical case study of a unified pipeline that combines open-vocabulary
detection, promptable segmentation, text-conditioned inpainting, and
vision-language description into a single workflow. The system works end to end
from a single prompt, retains intermediate artifacts for transparent debugging
(such as detections, masks, overlays, edited images, and before and after
composites), and provides the same functionality through an interactive UI and
a scriptable CLI for consistent, repeatable runs. We highlight integration
choices that reduce brittleness, including threshold adjustments, mask
inspection with light morphology, and resource-aware defaults. In a small,
single-word prompt segment, detection and segmentation produced usable masks in
over 90% of cases with an accuracy above 85% based on our criteria. On a
high-end GPU, inpainting makes up 60 to 75% of total runtime under typical
guidance and sampling settings, which highlights the need for careful tuning.
The study offers implementation-guided advice on thresholds, mask tightness,
and diffusion parameters, and details version pinning, artifact logging, and
seed control to support replay. Our contribution is a transparent, reliable
pattern for assembling modern vision and multimodal models behind a single
prompt, with clear guardrails and operational practices that improve
reliability in object replacement, scene augmentation, and removal.

</details>


### [34] [A Structured Review of Underwater Object Detection Challenges and Solutions: From Traditional to Large Vision Language Models](https://arxiv.org/abs/2509.08490)
*Edwine Nabahirwa,Wei Song,Minghua Zhang,Yi Fang,Zhou Ni*

Main category: cs.CV

TL;DR: 本文系统综述了水下目标检测（UOD）领域的挑战、发展及新兴大模型（LVLMs）的应用前景，并指出当前方法尚难解决图像退化、小目标检测等关键难题。


<details>
  <summary>Details</summary>
Motivation: 水下目标检测对海洋研究、机器人、生态保护等领域至关重要，但由于水下环境复杂，现有方法效果有限，因此需要系统总结现有挑战和方法，探索新的技术方向。

Method: 作者将UOD面临的挑战归纳为五大类：图像质量退化、目标相关难题、数据相关挑战、计算与处理限制、检测方法局限性。回顾了从传统方法到现代方法的发展，探讨了大视觉语言模型（LVLMs）在UOD中的应用潜力，并通过案例研究了如用DALL-E 3生成合成数据、微调Florence-2模型等场景。

Result: （1）现有UOD方法难以全面解决真实动态水下环境中的关键难题；（2）用LVLM生成的合成数据有助于扩充数据集，但逼真度和适用性仍需提升；（3）LVLM在本领域前景巨大，但其实时应用和优化仍缺乏深入研究。

Conclusion: 水下目标检测仍面临重大挑战，LVLMs为解决部分关键问题提供了新思路，今后需要继续提升数据质量和模型实时性，以促进UOD技术的实际落地。

Abstract: Underwater object detection (UOD) is vital to diverse marine applications,
including oceanographic research, underwater robotics, and marine conservation.
However, UOD faces numerous challenges that compromise its performance. Over
the years, various methods have been proposed to address these issues, but they
often fail to fully capture the complexities of underwater environments. This
review systematically categorizes UOD challenges into five key areas: Image
quality degradation, target-related issues, data-related challenges,
computational and processing constraints, and limitations in detection
methodologies. To address these challenges, we analyze the progression from
traditional image processing and object detection techniques to modern
approaches. Additionally, we explore the potential of large vision-language
models (LVLMs) in UOD, leveraging their multi-modal capabilities demonstrated
in other domains. We also present case studies, including synthetic dataset
generation using DALL-E 3 and fine-tuning Florence-2 LVLM for UOD. This review
identifies three key insights: (i) Current UOD methods are insufficient to
fully address challenges like image degradation and small object detection in
dynamic underwater environments. (ii) Synthetic data generation using LVLMs
shows potential for augmenting datasets but requires further refinement to
ensure realism and applicability. (iii) LVLMs hold significant promise for UOD,
but their real-time application remains under-explored, requiring further
research on optimization techniques.

</details>


### [35] [Chirality in Action: Time-Aware Video Representation Learning by Latent Straightening](https://arxiv.org/abs/2509.08502)
*Piyush Bagad,Andrew Zisserman*

Main category: cs.CV

TL;DR: 本论文提出了一种新的紧凑且对时间变化敏感的视频特征表示方法，并引入了“手性（chiral）动作识别”这一新任务来测评模型区分时间上相反动作（如开门/关门等）的能力。通过自监督的自编码器结构，将时间敏感性注入冻结的图像特征序列，实现了有效区分正反动作的视频表示。实验在多个数据集上显示新方法性能优于大规模预训练的视频模型，并且与现有模型组合可进一步提升分类准确率。


<details>
  <summary>Details</summary>
Motivation: 许多日常动作存在正反区分（如开/关、进/退），这些动作对于理解事物随时间变化的状态和空间变化至关重要，但现有视频特征表示对这种时间顺序变化识别较差。缺乏对时序敏感度的紧凑表达限制了视频模型在现实场景和正反动作识别中的应用。

Method: 提出了手性动作识别作为新的测试任务——区分两种时序相反的动作。方法上，采用自监督学习，利用自编码器结构和带有感知直化（perceptual straightening）归纳偏置的隐空间，将时间敏感性引入冻结的图像特征序列，形成新的视频表示。

Result: 在Something-Something、EPIC-Kitchens、Charade三个数据集上，所提方法生成的视频表示比当前的大型预训练视频模型表现更好。将新表示与已有模型融合能进一步提升分类性能。

Conclusion: 文中自监督方法为视频表示增添了时间敏感性，能够有效区分时序相反的手性动作。提出的紧凑视频表示不仅优于现有大模型，也可与其互补提升性能，为时间感知视频识别任务提供了有力的新工具。

Abstract: Our objective is to develop compact video representations that are sensitive
to visual change over time. To measure such time-sensitivity, we introduce a
new task: chiral action recognition, where one needs to distinguish between a
pair of temporally opposite actions, such as "opening vs. closing a door",
"approaching vs. moving away from something", "folding vs. unfolding paper",
etc. Such actions (i) occur frequently in everyday life, (ii) require
understanding of simple visual change over time (in object state, size, spatial
position, count . . . ), and (iii) are known to be poorly represented by many
video embeddings. Our goal is to build time aware video representations which
offer linear separability between these chiral pairs. To that end, we propose a
self-supervised adaptation recipe to inject time-sensitivity into a sequence of
frozen image features. Our model is based on an auto-encoder with a latent
space with inductive bias inspired by perceptual straightening. We show that
this results in a compact but time-sensitive video representation for the
proposed task across three datasets: Something-Something, EPIC-Kitchens, and
Charade. Our method (i) outperforms much larger video models pre-trained on
large-scale video datasets, and (ii) leads to an improvement in classification
performance on standard benchmarks when combined with these existing models.

</details>


### [36] [HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning](https://arxiv.org/abs/2509.08519)
*Liyang Chen,Tianxiang Ma,Jiawei Liu,Bingchuan Li,Zhuowei Chen,Lijie Liu,Xu He,Gen Li,Qian He,Zhiyong Wu*

Main category: cs.CV

TL;DR: HuMo提出了一种统一的人体视频多模态生成框架，通过构建新数据集和分阶段训练，有效提升了多模态协同生成效果，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人体视频生成方法面临多模态配合难、训练数据稀缺等挑战，导致在语音、图像、文本联合控制下生成效果不理想。

Method: (1) 构建包含文本、参考图像和音频配对的大规模高质量数据集；(2) 采用两阶段的渐进式多模态训练范式：第一阶段通过最小侵入式图像注入保持模型提示跟随和视觉生成能力，第二阶段通过音频跨注意力和“预测引导”策略加强音频与面部区域关联性，并渐进融合多模态控制任务；(3) 推理阶段设计了时间自适应无分类器引导策略，实现灵活细粒度的多模态调控。

Result: HuMo在各子任务上超过现有专用SOTA方法，展示了更优越的多模态协作能力和生成质量。

Conclusion: HuMo建立了一个统一、协同、多模态条件下的人体视频生成框架，解决了多模态数据协调和子任务协同难题，为相关应用提供了新思路和基础。

Abstract: Human-Centric Video Generation (HCVG) methods seek to synthesize human videos
from multimodal inputs, including text, image, and audio. Existing methods
struggle to effectively coordinate these heterogeneous modalities due to two
challenges: the scarcity of training data with paired triplet conditions and
the difficulty of collaborating the sub-tasks of subject preservation and
audio-visual sync with multimodal inputs. In this work, we present HuMo, a
unified HCVG framework for collaborative multimodal control. For the first
challenge, we construct a high-quality dataset with diverse and paired text,
reference images, and audio. For the second challenge, we propose a two-stage
progressive multimodal training paradigm with task-specific strategies. For the
subject preservation task, to maintain the prompt following and visual
generation abilities of the foundation model, we adopt the minimal-invasive
image injection strategy. For the audio-visual sync task, besides the commonly
adopted audio cross-attention layer, we propose a focus-by-predicting strategy
that implicitly guides the model to associate audio with facial regions. For
joint learning of controllabilities across multimodal inputs, building on
previously acquired capabilities, we progressively incorporate the audio-visual
sync task. During inference, for flexible and fine-grained multimodal control,
we design a time-adaptive Classifier-Free Guidance strategy that dynamically
adjusts guidance weights across denoising steps. Extensive experimental results
demonstrate that HuMo surpasses specialized state-of-the-art methods in
sub-tasks, establishing a unified framework for collaborative
multimodal-conditioned HCVG. Project Page:
https://phantom-video.github.io/HuMo.

</details>


### [37] [MESH -- Understanding Videos Like Human: Measuring Hallucinations in Large Video Models](https://arxiv.org/abs/2509.08538)
*Garry Yang,Zizhe Chen,Man Hon Wong,Haoyu Lei,Yongqiang Chen,Zhenguo Li,Kaiwen Zhou,James Cheng*

Main category: cs.CV

TL;DR: 本论文提出了一种全新的视频错觉评测基准MESH，用于系统性评估大规模视频模型（LVMs）在视频内容理解中的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模视频模型（LVMs）通过融合时序信息提升了对动态视频内容的理解能力，但它们仍然容易产生不准确或不相关的描述（即“幻觉”），现有评测方法主要依赖人工标签，忽视了人与视频自然交互的感知过程，因此亟需更科学合理的评测基准。

Method: 作者提出了MESH基准，采用问答框架，包括二元和多选题，包含目标和诱导陷阱项。评测设计采用自下而上的分析路径，逐步考察模型对基本物体、主题粗细特征、主题-动作对的识别和理解，模拟人类观看视频的理解顺序。

Result: 实验结果表明，MESH能够有效地识别和评估大规模视频模型在各层次上的幻觉表现。目前的LVM在识别基础对象和简单特征方面表现良好，但在处理细粒度特征或长视频中涉及多对象、多动作的复杂情况时，幻觉率明显上升。

Conclusion: MESH提供了一种系统且全面的视频幻觉评测方式，有助于推动大规模视频模型在真实世界视频理解中的可靠性研究。

Abstract: Large Video Models (LVMs) build on the semantic capabilities of Large
Language Models (LLMs) and vision modules by integrating temporal information
to better understand dynamic video content. Despite their progress, LVMs are
prone to hallucinations-producing inaccurate or irrelevant descriptions.
Current benchmarks for video hallucination depend heavily on manual
categorization of video content, neglecting the perception-based processes
through which humans naturally interpret videos. We introduce MESH, a benchmark
designed to evaluate hallucinations in LVMs systematically. MESH uses a
Question-Answering framework with binary and multi-choice formats incorporating
target and trap instances. It follows a bottom-up approach, evaluating basic
objects, coarse-to-fine subject features, and subject-action pairs, aligning
with human video understanding. We demonstrate that MESH offers an effective
and comprehensive approach for identifying hallucinations in videos. Our
evaluations show that while LVMs excel at recognizing basic objects and
features, their susceptibility to hallucinations increases markedly when
handling fine details or aligning multiple actions involving various subjects
in longer videos.

</details>


### [38] [ViewSparsifier: Killing Redundancy in Multi-View Plant Phenotyping](https://arxiv.org/abs/2509.08550)
*Robin-Nico Kampa,Fabian Deuser,Konrad Habel,Norbert Oswald*

Main category: cs.CV

TL;DR: 本文提出了一种面向多视图植物表型分析的深度学习方法，通过引入随机多视角选择策略（ViewSparsifier），在ACM Multimedia 2025 GroMo挑战赛中获得了两个任务的冠军。


<details>
  <summary>Details</summary>
Motivation: 现有基于单视图的深度学习方法未能全面捕获影响植物性状估计的全部信息，导致对植物健康评估和收获准备预测存在不足，亟需更有效利用多视角数据来提高分析精度。

Method: 提出ViewSparsifier方法，基于多视图随机选择（selection vector），从24个不同视角中选择数据进行训练，以学习视角不变的特征嵌入，并尝试将随机选择扩展至全部五个高度共120个视角（selection matrices），增强特征提取能力。

Result: 所提ViewSparsifier方法在GroMo挑战赛的两个任务（植物年龄预测与叶片计数）上均获得冠军，并在更全面的多高度视角实验中展示出进一步改进的潜力。

Conclusion: 多视图随机采样与特征学习方法能够有效提升植物表型性状的估计精度，为未来更复杂的多视角表型分析研究提供了方向和方法基础。

Abstract: Plant phenotyping involves analyzing observable characteristics of plants to
better understand their growth, health, and development. In the context of deep
learning, this analysis is often approached through single-view classification
or regression models. However, these methods often fail to capture all
information required for accurate estimation of target phenotypic traits, which
can adversely affect plant health assessment and harvest readiness prediction.
To address this, the Growth Modelling (GroMo) Grand Challenge at ACM Multimedia
2025 provides a multi-view dataset featuring multiple plants and two tasks:
Plant Age Prediction and Leaf Count Estimation. Each plant is photographed from
multiple heights and angles, leading to significant overlap and redundancy in
the captured information. To learn view-invariant embeddings, we incorporate 24
views, referred to as the selection vector, in a random selection. Our
ViewSparsifier approach won both tasks. For further improvement and as a
direction for future research, we also experimented with randomized view
selection across all five height levels (120 views total), referred to as
selection matrices.

</details>


### [39] [Vision-Language Semantic Aggregation Leveraging Foundation Model for Generalizable Medical Image Segmentation](https://arxiv.org/abs/2509.08570)
*Wenjun Yu,Yinchen Zhou,Jia-Xuan Jiang,Shubin Zeng,Yuee Li,Zhong Wang*

Main category: cs.CV

TL;DR: 本文提出了EM聚合机制和文本引导像素解码器，专注于缓解多模态医学图像分割中跨模态融合带来的语义差异和特征分散问题，并在多项数据集上超越现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 自然图像分割中多模态模型表现优异，但在医学图像领域效果明显下降。作者探究发现主要瓶颈在于抽象文本提示与细粒度医学视觉特征之间的语义鸿沟及其导致的特征分散。解决这一核心矛盾，有助于提升多模态模型在医学领域的表现和泛化能力。

Method: 作者提出了两项创新机制：1) EM聚合机制，通过动态特征聚类，将分散的特征聚合为语义中心，从而提升跨模态特征一致性；2) 文本引导像素解码器，利用领域不变的文本知识，更有效地引导视觉特征的深层次表达。两者协同作用以改善语义融合。

Result: 在公开的心脏和眼底数据集上，本文方法在多个领域泛化基准上，均显著优于当前主流SOTA方法，展示了更强的泛化性能和鲁棒性。

Conclusion: 通过关注语义聚合和有效的文本引导，提出的方法显著提升了多模态医学图像分割的准确性及跨域泛化能力，为跨模态医学图像处理提供了新的解决思路。

Abstract: Multimodal models have achieved remarkable success in natural image
segmentation, yet they often underperform when applied to the medical domain.
Through extensive study, we attribute this performance gap to the challenges of
multimodal fusion, primarily the significant semantic gap between abstract
textual prompts and fine-grained medical visual features, as well as the
resulting feature dispersion. To address these issues, we revisit the problem
from the perspective of semantic aggregation. Specifically, we propose an
Expectation-Maximization (EM) Aggregation mechanism and a Text-Guided Pixel
Decoder. The former mitigates feature dispersion by dynamically clustering
features into compact semantic centers to enhance cross-modal correspondence.
The latter is designed to bridge the semantic gap by leveraging
domain-invariant textual knowledge to effectively guide deep visual
representations. The synergy between these two mechanisms significantly
improves the model's generalization ability. Extensive experiments on public
cardiac and fundus datasets demonstrate that our method consistently
outperforms existing SOTA approaches across multiple domain generalization
benchmarks.

</details>


### [40] [Improving Greenland Bed Topography Mapping with Uncertainty-Aware Graph Learning on Sparse Radar Data](https://arxiv.org/abs/2509.08571)
*Bayu Adhi Tama,Homayra Alam,Mostafa Cham,Omar Faruque,Jianwu Wang,Vandana Janeja*

Main category: cs.CV

TL;DR: 本文提出了一种名为GraphTopoNet的图学习框架，用于结合不同类型的监督信息与不确定性建模，从稀疏的格陵兰冰盖雷达观测数据推断高精度冰下床面图。结果表明其在多个子区域的表现大幅超过现有方法，误差降低可达60%，对气候建模和政策制定有重要意义。


<details>
  <summary>Details</summary>
Motivation: 现有用于绘制格陵兰冰盖下床面地图的数据稀疏且分布不均，而高精度床面图对预测海平面上升和气候分析至关重要。传统方法对数据空缺和不确定性处理有限，亟需新型机器学习架构来提升推断精度并定量不确定性。

Method: 提出了GraphTopoNet，一种基于图神经网络的学习框架：1）空间图来自地表可观测（高程、速度、质量平衡等）；2）结合梯度特征与多项式趋势以获取局部和全局信息；3）利用Monte Carlo dropout对不确定性建模；4）通过混合损失函数引入基于置信度的雷达监督和自适应平衡正则化，有效应对数据缺口。

Result: 在格陵兰冰盖三个子区域验证，GraphTopoNet的床面预测误差比插值、卷积和其他图方法低最多60%，且更好保留了冰川小尺度特征。

Conclusion: GraphTopoNet能大幅提高大陆尺度稀疏观测下的冰下床面推断精度与不确定度表达，为气候建模和政策提供更可靠的数据支持，也展示了图机器学习在地球物理观测中的广泛价值。

Abstract: Accurate maps of Greenland's subglacial bed are essential for sea-level
projections, but radar observations are sparse and uneven. We introduce
GraphTopoNet, a graph-learning framework that fuses heterogeneous supervision
and explicitly models uncertainty via Monte Carlo dropout. Spatial graphs built
from surface observables (elevation, velocity, mass balance) are augmented with
gradient features and polynomial trends to capture both local variability and
broad structure. To handle data gaps, we employ a hybrid loss that combines
confidence-weighted radar supervision with dynamically balanced regularization.
Applied to three Greenland subregions, GraphTopoNet outperforms interpolation,
convolutional, and graph-based baselines, reducing error by up to 60 percent
while preserving fine-scale glacial features. The resulting bed maps improve
reliability for operational modeling, supporting agencies engaged in climate
forecasting and policy. More broadly, GraphTopoNet shows how graph machine
learning can convert sparse, uncertain geophysical observations into actionable
knowledge at continental scale.

</details>


### [41] [Implicit Shape-Prior for Few-Shot Assisted 3D Segmentation](https://arxiv.org/abs/2509.08580)
*Mathilde Monvoisin,Louise Piecuch,Blanche Texier,Cédric Hémon,Anaïs Barateau,Jérémie Huet,Antoine Nordez,Anne-Sophie Boureau,Jean-Claude Nunes,Diana Mateus*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法，通过引入隐式形状先验和自动选择最有信息量切片的框架，极大减少了医学3D分割任务中的人工工作量。


<details>
  <summary>Details</summary>
Motivation: 目前复杂的3D医学图像分割（如放疗相关的靶区和危及器官分割、肌肉体积测量等）尚无法完全自动化，依赖于临床医生繁琐的手工分割操作，亟需减轻人工负担，提高效率。

Method: 提出了结合隐式形状先验的分割方法，可以从稀疏切片人工标注推广到多器官分割。此外，还设计了一个自动选择最有信息的切片以引导后续人工交互最小化的简单框架。

Result: 在两个临床场景下进行验证：1）脑癌患者的危及器官半自动分割，2）显著加快新建肌肉形状数据库（服务于肌肉萎缩症患者）时的分割速度。实验表明该方法有效。

Conclusion: 该方法有效减轻了医学3D分割的手工负担，提升了分割任务的效率和准确性，适用于多种复杂医学场景。

Abstract: The objective of this paper is to significantly reduce the manual workload
required from medical professionals in complex 3D segmentation tasks that
cannot be yet fully automated. For instance, in radiotherapy planning, organs
at risk must be accurately identified in computed tomography (CT) or magnetic
resonance imaging (MRI) scans to ensure they are spared from harmful radiation.
Similarly, diagnosing age-related degenerative diseases such as sarcopenia,
which involve progressive muscle volume loss and strength, is commonly based on
muscular mass measurements often obtained from manual segmentation of medical
volumes. To alleviate the manual-segmentation burden, this paper introduces an
implicit shape prior to segment volumes from sparse slice manual annotations
generalized to the multi-organ case, along with a simple framework for
automatically selecting the most informative slices to guide and minimize the
next interactions. The experimental validation shows the method's effectiveness
on two medical use cases: assisted segmentation in the context of at risks
organs for brain cancer patients, and acceleration of the creation of a new
database with unseen muscle shapes for patients with sarcopenia.

</details>


### [42] [EfficientIML: Efficient High-Resolution Image Manipulation Localization](https://arxiv.org/abs/2509.08583)
*Jinhan Li,Haoyang He,Lei Xie,Jiangning Zhang*

Main category: cs.CV

TL;DR: 现有的图像伪造检测方法难以应对基于扩散模型的新型高分辨率伪造图像。本文提出了一个新数据集，并设计了高效的新模型EfficientIML，能更好地检测此类伪造。该方法在精度和效率上均优于现有主流模型。


<details>
  <summary>Details</summary>
Motivation: 随扩散模型伪造技术的发展，现有检测器只在传统伪造数据集上训练，无法应对扩散类的新伪造类型，亟需新数据集和检测方法。

Method: （1）构建包含1200+个高分辨率、扩散模型生成伪造图片及其掩码的新数据集。 （2）提出三阶段轻量化EfficientRWKV骨干的EfficientIML模型，融合状态空间和注意力机制以兼顾全局和局部特征。 （3）多尺度监督策略提升分层预测一致性。

Result: 在新构建的数据集和主流评测基准上，EfficientIML在定位精度、FLOPs和推理速度上均超越了ViT及其他轻量级主流方法。

Conclusion: EfficientIML模型结合高效结构和多尺度监督，在扩散伪造检测任务中的表现优异，适用于实际实时取证场景。

Abstract: With imaging devices delivering ever-higher resolutions and the emerging
diffusion-based forgery methods, current detectors trained only on traditional
datasets (with splicing, copy-moving and object removal forgeries) lack
exposure to this new manipulation type. To address this, we propose a novel
high-resolution SIF dataset of 1200+ diffusion-generated manipulations with
semantically extracted masks. However, this also imposes a challenge on
existing methods, as they face significant computational resource constraints
due to their prohibitive computational complexities. Therefore, we propose a
novel EfficientIML model with a lightweight, three-stage EfficientRWKV
backbone. EfficientRWKV's hybrid state-space and attention network captures
global context and local details in parallel, while a multi-scale supervision
strategy enforces consistency across hierarchical predictions. Extensive
evaluations on our dataset and standard benchmarks demonstrate that our
approach outperforms ViT-based and other SOTA lightweight baselines in
localization performance, FLOPs and inference speed, underscoring its
suitability for real-time forensic applications.

</details>


### [43] [CLAPS: A CLIP-Unified Auto-Prompt Segmentation for Multi-Modal Retinal Imaging](https://arxiv.org/abs/2509.08618)
*Zhihao Zhao,Yinzheng Zhao,Junjie Yang,Xiangtong Yao,Quanmin Liang,Shahrooz Faghihroohi,Kai Huang,Nassir Navab,M. Ali Nasseri*

Main category: cs.CV

TL;DR: 提出了一种用于视网膜影像多任务与多模态分割的自动化、统一方法CLAPS，通过自动生成空间和文本提示，实现无需手动干预的高精度分割，并在多个数据集上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 尽管基础大模型如SAM已推动医学影像分割进步，但目前方法存在文本描述模态歧义、依赖人工提示以及缺乏统一结构等问题，尤其在视网膜分割领域亟需解决。

Method: 1) 预训练基于CLIP的图像编码器以适应多模态数据；2) 利用GroundingDINO自动检测局部病变，生成空间提示框；3) 文本提示中引入独特的“模态签名”以消除模态歧义并统一任务；4) 最终由SAM执行自动分割，实现端到端自动化流程。

Result: 在涵盖11类关键分割任务的12个数据集上，CLAPS能取得与专家定制化模型相匹配，且超越多数现有基线算法的性能，验证了方法的有效性及广泛适用性。

Conclusion: CLAPS提出了一种面向视网膜多任务与多模态分割的自动、统一化方法，在准确性和适应性上均表现突出，为相关医学影像分割提供了强大的基础模型方案。

Abstract: Recent advancements in foundation models, such as the Segment Anything Model
(SAM), have significantly impacted medical image segmentation, especially in
retinal imaging, where precise segmentation is vital for diagnosis. Despite
this progress, current methods face critical challenges: 1) modality ambiguity
in textual disease descriptions, 2) a continued reliance on manual prompting
for SAM-based workflows, and 3) a lack of a unified framework, with most
methods being modality- and task-specific. To overcome these hurdles, we
propose CLIP-unified Auto-Prompt Segmentation (\CLAPS), a novel method for
unified segmentation across diverse tasks and modalities in retinal imaging.
Our approach begins by pre-training a CLIP-based image encoder on a large,
multi-modal retinal dataset to handle data scarcity and distribution imbalance.
We then leverage GroundingDINO to automatically generate spatial bounding box
prompts by detecting local lesions. To unify tasks and resolve ambiguity, we
use text prompts enhanced with a unique "modality signature" for each imaging
modality. Ultimately, these automated textual and spatial prompts guide SAM to
execute precise segmentation, creating a fully automated and unified pipeline.
Extensive experiments on 12 diverse datasets across 11 critical segmentation
categories show that CLAPS achieves performance on par with specialized expert
models while surpassing existing benchmarks across most metrics, demonstrating
its broad generalizability as a foundation model.

</details>


### [44] [AdsQA: Towards Advertisement Video Understanding](https://arxiv.org/abs/2509.08621)
*Xinwei Long,Kai Tian,Peng Xu,Guoli Jia,Jingxuan Li,Sa Yang,Yihua Shao,Kaiyan Zhang,Che Jiang,Hao Xu,Yang Liu,Jiaheng Ma,Bowen Zhou*

Main category: cs.CV

TL;DR: 本文提出利用广告视频来作为大语言模型（LLMs）新的挑战性测试平台，并构建了相应的问答基准（AdsQA），用于评估和提升LLMs在营销逻辑、说服策略等高阶感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在诸如数学、编程等专业领域内持续进步，拓展具有知识性和多样化的专业领域应用尤为重要。广告视频兼具信息丰富、线索多样等特点，因此作者希望借助广告视频对LLMs进行更全面的能力测试。

Method: 作者首先构建并公开了AdsQA基准数据集（涵盖1,544个广告视频与10,962个片段），设计包括五项任务。其次，提出强化学习模型ReAd-R，通过反思式问题处理和奖励驱动优化提升回答质量。

Result: 在AdsQA上评测了14个主流LLM模型。ReAd-R模型超越所有现有强大竞争者，表现出更优的长链推理与任务完成能力，取得当前最佳成绩。

Conclusion: 广告视频为LLMs能力评估提供了更具挑战性和多样性的测试集，新提出的ReAd-R模型提升了LLMs在广告视频理解中的表现，对提升模型实际应用能力具有重要意义。

Abstract: Large language models (LLMs) have taken a great step towards AGI. Meanwhile,
an increasing number of domain-specific problems such as math and programming
boost these general-purpose models to continuously evolve via learning deeper
expertise. Now is thus the time further to extend the diversity of specialized
applications for knowledgeable LLMs, though collecting high quality data with
unexpected and informative tasks is challenging. In this paper, we propose to
use advertisement (ad) videos as a challenging test-bed to probe the ability of
LLMs in perceiving beyond the objective physical content of common visual
domain. Our motivation is to take full advantage of the clue-rich and
information-dense ad videos' traits, e.g., marketing logic, persuasive
strategies, and audience engagement. Our contribution is three-fold: (1) To our
knowledge, this is the first attempt to use ad videos with well-designed tasks
to evaluate LLMs. We contribute AdsQA, a challenging ad Video QA benchmark
derived from 1,544 ad videos with 10,962 clips, totaling 22.7 hours, providing
5 challenging tasks. (2) We propose ReAd-R, a Deepseek-R1 styled RL model that
reflects on questions, and generates answers via reward-driven optimization.
(3) We benchmark 14 top-tier LLMs on AdsQA, and our \texttt{ReAd-R}~achieves
the state-of-the-art outperforming strong competitors equipped with long-chain
reasoning capabilities by a clear margin.

</details>


### [45] [UOPSL: Unpaired OCT Predilection Sites Learning for Fundus Image Diagnosis Augmentation](https://arxiv.org/abs/2509.08624)
*Zhihao Zhao,Yinzheng Zhao,Junjie Yang,Xiangtong Yao,Quanmin Liang,Daniel Zapp,Kai Huang,Nassir Navab,M. Ali Nasseri*

Main category: cs.CV

TL;DR: 本论文提出了一种新的非配对多模态方法，将OCT空间先验引入视网膜彩照诊断，显著提升了病变识别准确性，在大量数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前眼科多模态影像（如视网膜彩照与OCT）诊断依赖于昂贵的配对数据，现有方法无法充分利用不同影像模态的空间信息，特别是OCT数据稀缺和模态不均带来的困境。

Method: 作者提出了UOPSL框架，利用大量非配对的OCT和视网膜彩照进行对比学习，在OCT潜在空间中学习病变倾向点矩阵，以捕获OCT特征中的病变定位。下游仅基于彩照分类时，无须OCT输入，通过这一矩阵增强对病变部位的感知和分类能力。

Result: 在9个不同数据集、28类疾病上进行了广泛实验，结果表明本方法优于现有主流基线方法。

Conclusion: UOPSL框架有效融合了OCT空间先验，无需配对数据即可显著提升基于视网膜彩照的眼科疾病识别，为多模态医学影像诊断提供了创新思路。

Abstract: Significant advancements in AI-driven multimodal medical image diagnosis have
led to substantial improvements in ophthalmic disease identification in recent
years. However, acquiring paired multimodal ophthalmic images remains
prohibitively expensive. While fundus photography is simple and cost-effective,
the limited availability of OCT data and inherent modality imbalance hinder
further progress. Conventional approaches that rely solely on fundus or textual
features often fail to capture fine-grained spatial information, as each
imaging modality provides distinct cues about lesion predilection sites. In
this study, we propose a novel unpaired multimodal framework \UOPSL that
utilizes extensive OCT-derived spatial priors to dynamically identify
predilection sites, enhancing fundus image-based disease recognition. Our
approach bridges unpaired fundus and OCTs via extended disease text
descriptions. Initially, we employ contrastive learning on a large corpus of
unpaired OCT and fundus images while simultaneously learning the predilection
sites matrix in the OCT latent space. Through extensive optimization, this
matrix captures lesion localization patterns within the OCT feature space.
During the fine-tuning or inference phase of the downstream classification task
based solely on fundus images, where paired OCT data is unavailable, we
eliminate OCT input and utilize the predilection sites matrix to assist in
fundus image classification learning. Extensive experiments conducted on 9
diverse datasets across 28 critical categories demonstrate that our framework
outperforms existing benchmarks.

</details>


### [46] [LADB: Latent Aligned Diffusion Bridges for Semi-Supervised Domain Translation](https://arxiv.org/abs/2509.08628)
*Xuqin Wang,Tao Wu,Yanfeng Zhang,Lu Liu,Dong Wang,Mingwei Sun,Yongliang Wang,Niclas Zeller,Daniel Cremers*

Main category: cs.CV

TL;DR: 本文提出了一种名为Latent Aligned Diffusion Bridges（LADB）的半监督扩散模型框架，实现了在部分配对数据下的高质量领域迁移、样本到样本的转换，并有效克服了数据稀缺带来的训练和配对数据成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型在数据稀缺领域面临训练数据昂贵或难以获得全配对数据的问题，现有方法在控制性或数据需求上存在显著不足，因此需要能在部分配对甚至部分监督情况下实现高质量域迁移的解决方案。

Method: LADB方法通过共享潜在空间对源域和目标域分布进行对齐，将预训练的源域扩散模型与在部分配对潜在表示上训练的目标域扩散模型结合，实现无须完全配对数据下的确定性域映射。并针对多源（如深度图与分割掩码）、多目标任务进行了扩展。

Result: 实验结果显示，LADB在部分监督下的深度图到图像转换任务中表现优于现有方法，同时证明了其在类别条件下的风格迁移、多源及多目标任务中的灵活性和优越性。

Conclusion: LADB为真实世界中数据注释昂贵或不完整时的领域迁移问题提供了一种可扩展、通用且高效的解决方案，在平衡生成质量和多样性方面也优于现有方法。

Abstract: Diffusion models excel at generating high-quality outputs but face challenges
in data-scarce domains, where exhaustive retraining or costly paired data are
often required. To address these limitations, we propose Latent Aligned
Diffusion Bridges (LADB), a semi-supervised framework for sample-to-sample
translation that effectively bridges domain gaps using partially paired data.
By aligning source and target distributions within a shared latent space, LADB
seamlessly integrates pretrained source-domain diffusion models with a
target-domain Latent Aligned Diffusion Model (LADM), trained on partially
paired latent representations. This approach enables deterministic domain
mapping without the need for full supervision. Compared to unpaired methods,
which often lack controllability, and fully paired approaches that require
large, domain-specific datasets, LADB strikes a balance between fidelity and
diversity by leveraging a mixture of paired and unpaired latent-target
couplings. Our experimental results demonstrate superior performance in
depth-to-image translation under partial supervision. Furthermore, we extend
LADB to handle multi-source translation (from depth maps and segmentation
masks) and multi-target translation in a class-conditioned style transfer task,
showcasing its versatility in handling diverse and heterogeneous use cases.
Ultimately, we present LADB as a scalable and versatile solution for real-world
domain translation, particularly in scenarios where data annotation is costly
or incomplete.

</details>


### [47] [Skeleton-based sign language recognition using a dual-stream spatio-temporal dynamic graph convolutional network](https://arxiv.org/abs/2509.08661)
*Liangjin Liu,Haoyang Zheng,Pei Zhou*

Main category: cs.CV

TL;DR: 提出了Dual-SignLanguageNet (DSLNet)，利用双参考系和双流结构，分别分析手势形态与轨迹，实现更高精度的手语识别。


<details>
  <summary>Details</summary>
Motivation: 现有孤立手语识别方法对于形态相近但含义不同的手势区分能力不足，原因在于仅依赖单一参考系，难以解决几何上的歧义。

Method: 论文提出DSLNet，采用腕部参考系做手势形态分析，面部参考系做轨迹建模，两者由不同专用网络（图卷积与Finsler几何编码器）独立处理，最后用基于最优传输的机制融合，实现协同识别。

Result: 在WLASL-100、WLASL-300、LSA64等主流数据集上，DSLNet取得了93.70%、89.97%和99.79%的准确率，均达到或超过最新水平，且参数数量更少。

Conclusion: DSLNet显著提升了孤立手语识别中对形态/轨迹相近手势的区分能力，验证了基于双流、双参考系结构的有效性。

Abstract: Isolated Sign Language Recognition (ISLR) is challenged by gestures that are
morphologically similar yet semantically distinct, a problem rooted in the
complex interplay between hand shape and motion trajectory. Existing methods,
often relying on a single reference frame, struggle to resolve this geometric
ambiguity. This paper introduces Dual-SignLanguageNet (DSLNet), a
dual-reference, dual-stream architecture that decouples and models gesture
morphology and trajectory in separate, complementary coordinate systems. Our
approach utilizes a wrist-centric frame for view-invariant shape analysis and a
facial-centric frame for context-aware trajectory modeling. These streams are
processed by specialized networks-a topology-aware graph convolution for shape
and a Finsler geometry-based encoder for trajectory-and are integrated via a
geometry-driven optimal transport fusion mechanism. DSLNet sets a new
state-of-the-art, achieving 93.70%, 89.97% and 99.79% accuracy on the
challenging WLASL-100, WLASL-300 and LSA64 datasets, respectively, with
significantly fewer parameters than competing models.

</details>


### [48] [FractalPINN-Flow: A Fractal-Inspired Network for Unsupervised Optical Flow Estimation with Total Variation Regularization](https://arxiv.org/abs/2509.08670)
*Sara Behnamian,Rasoul Khaksarinezhad,Andreas Langer*

Main category: cs.CV

TL;DR: FractalPINN-Flow是一种无监督深度学习方法，可直接从连续灰度图像估算稠密光流，无需真实标签。其核心是受分形几何启发的递归编码器-解码器网络，结合变分公式与总变差正则项提升平滑性与边缘保持能力。实验表明，该方法对高分辨率和弱标注数据表现突出。


<details>
  <summary>Details</summary>
Motivation: 目前许多光流估计算法依赖真实的标注数据进行有监督训练，这在高分辨率或标注稀缺的情况下难以实现。此外，现有CNN结构对细节和大范围运动模式的兼顾也存在不足，推动研究者探索新结构和训练范式。

Method: 提出分形变形网络（FDN），以递归嵌套的编码器-解码器结构和跳跃连接，借鉴分形几何的自相似性，提升特征捕获能力。训练目标基于变分方法，结合亮度一致性下的L1和L2数据项以及空间总变差正则，整体端到端无监督学习光流。

Result: 在合成和标准数据集上的实验证明，FractalPINN-Flow能获得精确、平滑且边缘保持能力强的光流估计，尤其适合高分辨率场景和标注有限的问题。

Conclusion: FractalPINN-Flow打破对真实标签的依赖，通过创新的网络结构与无监督变分训练方式，实现了高质量、适应性强的光流估计，为面向新场景和弱标注数据提供了有力工具。

Abstract: We present FractalPINN-Flow, an unsupervised deep learning framework for
dense optical flow estimation that learns directly from consecutive grayscale
frames without requiring ground truth. The architecture centers on the Fractal
Deformation Network (FDN) - a recursive encoder-decoder inspired by fractal
geometry and self-similarity. Unlike traditional CNNs with sequential
downsampling, FDN uses repeated encoder-decoder nesting with skip connections
to capture both fine-grained details and long-range motion patterns. The
training objective is based on a classical variational formulation using total
variation (TV) regularization. Specifically, we minimize an energy functional
that combines $L^1$ and $L^2$ data fidelity terms to enforce brightness
constancy, along with a TV term that promotes spatial smoothness and coherent
flow fields. Experiments on synthetic and benchmark datasets show that
FractalPINN-Flow produces accurate, smooth, and edge-preserving optical flow
fields. The model is especially effective for high-resolution data and
scenarios with limited annotations.

</details>


### [49] [Multi-Modal Robust Enhancement for Coastal Water Segmentation: A Systematic HSV-Guided Framework](https://arxiv.org/abs/2509.08694)
*Zhen Tian,Christos Anagnostopoulos,Qiyuan Wang,Zhiwei Gao*

Main category: cs.CV

TL;DR: 该论文提出了一种针对卫星影像中海岸水体分割的Robust U-Net方法，通过多模态约束与HSV色彩空间监督，有效提升了分割精度与训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统的基于RGB的分割方法在多样化海域场景中容易出现训练不稳定和泛化能力差的问题，此外，海岸水体在光谱特性和边界形态上均较为复杂，给分割带来很大挑战。为此，作者提出了新的分割增强框架，以更好地适应复杂的海洋环境。

Method: 该方法通过引入五个协同模块：HSV空间指导的色彩监督、基于梯度的海岸线优化、形态学后处理、海域清理和连通性控制，系统性地提升分割效果。重点是HSV色彩监督模块，对提升分割性能影响最大。

Result: 经消融实验分析，HSV指导的色彩监督模块贡献最高（影响分数为0.85）；整体框架带来训练稳定性显著提升（方差降低84%），分割质量多项评估指标均有提升，并保持了计算效率。

Conclusion: Robust U-Net展示了在卫星影像海岸分割任务中的有效性和鲁棒性，改善了现有RGB方法面临的不稳定及泛化差的问题，提供了更优解决方案。

Abstract: Coastal water segmentation from satellite imagery presents unique challenges
due to complex spectral characteristics and irregular boundary patterns.
Traditional RGB-based approaches often suffer from training instability and
poor generalization in diverse maritime environments. This paper introduces a
systematic robust enhancement framework, referred to as Robust U-Net, that
leverages HSV color space supervision and multi-modal constraints for improved
coastal water segmentation. Our approach integrates five synergistic
components: HSV-guided color supervision, gradient-based coastline
optimization, morphological post-processing, sea area cleanup, and connectivity
control. Through comprehensive ablation studies, we demonstrate that HSV
supervision provides the highest impact (0.85 influence score), while the
complete framework achieves superior training stability (84\% variance
reduction) and enhanced segmentation quality. Our method shows consistent
improvements across multiple evaluation metrics while maintaining computational
efficiency. For reproducibility, our training configurations and code are
available here: https://github.com/UofgCoastline/ICASSP-2026-Robust-Unet.

</details>


### [50] [Computational Imaging for Enhanced Computer Vision](https://arxiv.org/abs/2509.08712)
*Humera Shaikh,Kaur Jashanpreet*

Main category: cs.CV

TL;DR: 本文综述了计算成像（CI）技术在计算机视觉（CV）应用中的变革性影响，重点分析CI技术如何突破传统成像的局限并提升CV各项任务表现。


<details>
  <summary>Details</summary>
Motivation: 传统成像在低光、动态模糊、高动态范围等苛刻环境下难以获得高质量数据，限制了CV系统的表现。亟需新技术突破成像瓶颈，强化视觉任务鲁棒性和精度。

Method: 系统梳理并分类了多种CI技术（如光场成像、HDR成像、去模糊、高速成像、防炫光），探讨这些技术与CV核心任务（如目标检测、深度估计、光流、人脸识别、关键点检测）之间的协同关系。

Result: 剖析了CI方法对CV应用的具体贡献，揭示了CI与CV融合带来的新机遇和存在的挑战，对各类实际场景（自动驾驶、安防、增强现实、机器人）中的应用前景做了系统分析。

Conclusion: 强调任务自适应成像流程的设计能提升CV系统的鲁棒性、准确性与效率，未来研究可针对具体视觉任务发展更高效的CI技术并促进CV应用落地。

Abstract: This paper presents a comprehensive survey of computational imaging (CI)
techniques and their transformative impact on computer vision (CV)
applications. Conventional imaging methods often fail to deliver high-fidelity
visual data in challenging conditions, such as low light, motion blur, or high
dynamic range scenes, thereby limiting the performance of state-of-the-art CV
systems. Computational imaging techniques, including light field imaging, high
dynamic range (HDR) imaging, deblurring, high-speed imaging, and glare
mitigation, address these limitations by enhancing image acquisition and
reconstruc- tion processes. This survey systematically explores the synergies
between CI techniques and core CV tasks, including object detection, depth
estimation, optical flow, face recognition, and keypoint detection. By
analyzing the relationships between CI methods and their practical
contributions to CV applications, this work highlights emerging opportunities,
challenges, and future research directions. We emphasize the potential for
task-specific, adaptive imaging pipelines that improve robustness, accuracy,
and efficiency in real-world scenarios, such as autonomous navigation,
surveillance, augmented reality, and robotics.

</details>


### [51] [BcQLM: Efficient Vision-Language Understanding with Distilled Q-Gated Cross-Modal Fusion](https://arxiv.org/abs/2509.08715)
*Sike Xiang,Shuang Chen,Amir Atapour-Abarghouei*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级多模态大型语言模型框架（BcQLM），在大幅降低算力需求的同时，视觉问答等多模态任务上表现优异，适合资源受限环境下的实际部署。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大型语言模型（MLLMs）规模不断扩大，其在能效、可扩展性和环境可持续性等方面面临挑战。为了使这些模型能在硬件资源有限的现实环境中部署，必须开发兼顾高性能与高效性的小型化模型。

Method: 作者提出了一种紧凑的视觉-语言编码器BreezeCLIP，将其作为核心，构建了BcQLM（BreezeCLIP-enhanced Q-Gated Multimodal Language Model）框架。该模型仅含1.2亿参数，强调模块化和可扩展性。作者在多种数据集上开展了视觉问答等多模态任务实验。

Result: BcQLM模型在大幅降低参数规模的基础上，计算成本显著降低，但在多模态任务（如视觉问答）上表现接近传统大规模MLLMs，权衡了准确性与效率。

Conclusion: BcQLM展现了在受限硬件条件下部署多模态大模型的可行性，并为今后通用多模态任务的轻量化发展提供了新思路。

Abstract: As multimodal large language models (MLLMs) advance, their large-scale
architectures pose challenges for deployment in resource-constrained
environments. In the age of large models, where energy efficiency,
computational scalability and environmental sustainability are paramount, the
development of lightweight and high-performance models is critical for
real-world applications. As such, we propose a lightweight MLLM framework for
end-to-end visual question answering. Our proposed approach centres on
BreezeCLIP, a compact yet powerful vision-language encoder optimised for
efficient multimodal understanding. With only 1.2 billion parameters overall,
our model significantly reduces computational cost while achieving performance
comparable to standard-size MLLMs. Experiments conducted on multiple datasets
further validate its effectiveness in balancing accuracy and efficiency. The
modular and extensible design enables generalisation to broader multimodal
tasks. The proposed lightweight vision-language framework is denoted as BcQLM
(BreezeCLIP-enhanced Q-Gated Multimodal Language Model). It offers a promising
path toward deployable MLLMs under practical hardware constraints. The source
code is available at https://github.com/thico0224/BcQLM.

</details>


### [52] [CrowdQuery: Density-Guided Query Module for Enhanced 2D and 3D Detection in Crowded Scenes](https://arxiv.org/abs/2509.08738)
*Marius Dähling,Sebastian Krebs,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 本文提出了CrowdQuery（CQ），一种在Transformer基础上通过引入目标密度信息提高拥挤场景下行人检测效果的方法。CQ通过预测并嵌入密度图信息，将其融入解码器，提高2D和3D检测的性能。


<details>
  <summary>Details</summary>
Motivation: 在拥挤场景下，由于目标遮挡严重，现有检测方法容易漏检或误检。传统密度图仅关注位置，忽略了目标尺寸等关键信息，因此亟需更有效地融合密度信息以提升检测性能。

Method: 提出了CQ模块，通过预测并嵌入包含边界框尺寸的密度图，并将该信息有机融合到Transformer解码器的object query中。该方法可兼容2D与3D检测，无需额外数据，具备通用性。

Result: 在STCrowd、CrowdHuman等2D和3D领域权威数据集上，CQ集成相关检测模型后取得了显著性能提升，优于多数现有方法，并具备很强的泛化性。

Conclusion: CQ方法通过密度信息引入，有效提升了拥挤场景的检测性能，可广泛适用于2D/3D各种Transformer架构，填补了2D和3D拥挤检测统一方法的空白。

Abstract: This paper introduces a novel method for end-to-end crowd detection that
leverages object density information to enhance existing transformer-based
detectors. We present CrowdQuery (CQ), whose core component is our CQ module
that predicts and subsequently embeds an object density map. The embedded
density information is then systematically integrated into the decoder.
Existing density map definitions typically depend on head positions or
object-based spatial statistics. Our method extends these definitions to
include individual bounding box dimensions. By incorporating density
information into object queries, our method utilizes density-guided queries to
improve detection in crowded scenes. CQ is universally applicable to both 2D
and 3D detection without requiring additional data. Consequently, we are the
first to design a method that effectively bridges 2D and 3D detection in
crowded environments. We demonstrate the integration of CQ into both a general
2D and 3D transformer-based object detector, introducing the architectures CQ2D
and CQ3D. CQ is not limited to the specific transformer models we selected.
Experiments on the STCrowd dataset for both 2D and 3D domains show significant
performance improvements compared to the base models, outperforming most
state-of-the-art methods. When integrated into a state-of-the-art crowd
detector, CQ can further improve performance on the challenging CrowdHuman
dataset, demonstrating its generalizability. The code is released at
https://github.com/mdaehl/CrowdQuery.

</details>


### [53] [ArgoTweak: Towards Self-Updating HD Maps through Structured Priors](https://arxiv.org/abs/2509.08764)
*Lena Wild,Rafael Valencia,Patric Jensfelt*

Main category: cs.CV

TL;DR: 论文提出了ArgoTweak，这是第一个包含真实先验地图、当前地图和传感器数据三元组的数据集，用于自验证和自更新的高精地图研究。它采用一种可解释的双射映射框架，实现高保真的变更检测和集成，大幅缩小了模拟到现实的差距。


<details>
  <summary>Details</summary>
Motivation: 目前公开数据集缺乏真实的先验地图与当前地图及传感器数据的三元组，导致高精度地图的自验和自更新研究不得不依赖合成先验，这产生了模拟与现实之间的显著差距（sim2real gap）。为了解决这一瓶颈，论文提出并发布了ArgoTweak数据集。

Method: ArgoTweak数据集核心利用双射映射框架，把大范围地图变更细分为原子级元素变动，增强了解释性和变更检测的精度。此外，对比现有合成先验，ArgoTweak提供真实先验，支持详细的变更注释，并配套开发了地图修改工具箱和基线方法。

Result: 实验结果表明，在ArgoTweak数据集上训练的模型，相较于使用合成先验的模型，显著降低了sim2real gap。大量消融实验证明了结构化先验和详细变更标注的积极作用。

Conclusion: ArgoTweak建立了高精地图可解释性和先验辅助映射的基准，推动了高精地图的可扩展性、自我完善能力和实际部署。数据集和相关工具已公开。

Abstract: Reliable integration of prior information is crucial for self-verifying and
self-updating HD maps. However, no public dataset includes the required triplet
of prior maps, current maps, and sensor data. As a result, existing methods
must rely on synthetic priors, which create inconsistencies and lead to a
significant sim2real gap. To address this, we introduce ArgoTweak, the first
dataset to complete the triplet with realistic map priors. At its core,
ArgoTweak employs a bijective mapping framework, breaking down large-scale
modifications into fine-grained atomic changes at the map element level, thus
ensuring interpretability. This paradigm shift enables accurate change
detection and integration while preserving unchanged elements with high
fidelity. Experiments show that training models on ArgoTweak significantly
reduces the sim2real gap compared to synthetic priors. Extensive ablations
further highlight the impact of structured priors and detailed change
annotations. By establishing a benchmark for explainable, prior-aided HD
mapping, ArgoTweak advances scalable, self-improving mapping solutions. The
dataset, baselines, map modification toolbox, and further resources are
available at https://kth-rpl.github.io/ArgoTweak/.

</details>


### [54] [An End-to-End Deep Learning Framework for Arsenicosis Diagnosis Using Mobile-Captured Skin Images](https://arxiv.org/abs/2509.08780)
*Asif Newaz,Asif Ur Rahman Adib,Rajit Sahil,Mashfique Mehzad*

Main category: cs.CV

TL;DR: 该论文提出了一种基于手机拍摄皮肤图像的砷中毒（arsenicosis）自动诊断方法，利用深度学习提升早期检测能力，尤其适用于资源有限地区。


<details>
  <summary>Details</summary>
Motivation: 南亚和东南亚地区砷中毒因长期饮用受污染水源而普遍，早期皮肤症状易被漏诊。农村地区缺乏皮肤科医生，亟需自动化、基于图像的辅助诊断方法，以提升基层医疗能力。

Method: 作者构建了包含20类、11000多张皮肤图像的数据集，涵盖砷中毒及其他皮肤病。尝试多种深度学习模型，包括CNN和基于Transformer的模型用于图像分类，并结合LIME、Grad-CAM提升模型可解释性。最终部署为网络诊断工具，便于落地应用。

Result: 基于Transformer的模型表现优于CNN，其中Swin Transformer准确率达86%。LIME 和 Grad-CAM 可视化显示模型能关注与皮损相关区域，增强临床透明度。通过外部数据验证，框架具备良好的泛化能力。

Conclusion: 该方法具有无创、便捷和可解释的优点，能在缺乏专科医生的农村或资源匮乏地区提供早期砷中毒筛查和辅助诊断，有助于推动早发现、早干预。

Abstract: Background: Arsenicosis is a serious public health concern in South and
Southeast Asia, primarily caused by long-term consumption of
arsenic-contaminated water. Its early cutaneous manifestations are clinically
significant but often underdiagnosed, particularly in rural areas with limited
access to dermatologists. Automated, image-based diagnostic solutions can
support early detection and timely interventions.
  Methods: In this study, we propose an end-to-end framework for arsenicosis
diagnosis using mobile phone-captured skin images. A dataset comprising 20
classes and over 11000 images of arsenic-induced and other dermatological
conditions was curated. Multiple deep learning architectures, including
convolutional neural networks (CNNs) and Transformer-based models, were
benchmarked for arsenicosis detection. Model interpretability was integrated
via LIME and Grad-CAM, while deployment feasibility was demonstrated through a
web-based diagnostic tool.
  Results: Transformer-based models significantly outperformed CNNs, with the
Swin Transformer achieving the best results (86\\% accuracy). LIME and Grad-CAM
visualizations confirmed that the models attended to lesion-relevant regions,
increasing clinical transparency and aiding in error analysis. The framework
also demonstrated strong performance on external validation samples, confirming
its ability to generalize beyond the curated dataset.
  Conclusion: The proposed framework demonstrates the potential of deep
learning for non-invasive, accessible, and explainable diagnosis of arsenicosis
from mobile-acquired images. By enabling reliable image-based screening, it can
serve as a practical diagnostic aid in rural and resource-limited communities,
where access to dermatologists is scarce, thereby supporting early detection
and timely intervention.

</details>


### [55] [Quantifying Accuracy of an Event-Based Star Tracker via Earth's Rotation](https://arxiv.org/abs/2509.08794)
*Dennis Melamed,Connor Hashemi,Scott McCloskey*

Main category: cs.CV

TL;DR: 本文提出了一种基于事件相机（EBC）的星跟踪方法，通过利用地球自转作为真实基准，验证了EBC进行姿态测量的精度，并将结果与IERS官方数据对比分析。


<details>
  <summary>Details</summary>
Motivation: 事件相机作为一种新兴传感器，因其高动态范围、低延迟等优势，极具潜力用于星跟踪与航天器姿态控制。而以往真实数据的高精度真实姿态测定一直是难题，如何准确评估EBC在实际观测中的性能亟需解决。

Method: 作者将EBC通过地面望远镜静止指向夜空，利用地球自转产生的可预测运动作为唯一参考，采集天区事件流数据，随后将估算获得的姿态与IERS的精确地球定向数据进行对比，定量评估EBC星跟踪精度。

Result: 系统获得的姿态估算结果表现为横跨误差均方根（RMSE）为18.47角秒，‘about’误差为78.84角秒。相较于帧式传感器，EBC在数据稀疏、高动态范围、低能耗和高更新率等方面更具优势。

Conclusion: 事件相机具备在实际低成本、低延迟星跟踪系统中的应用潜力，所展现的姿态精度达到实际可用水平。论文还共享了数据和代码供进一步研究。

Abstract: Event-based cameras (EBCs) are a promising new technology for star
tracking-based attitude determination, but prior studies have struggled to
determine accurate ground truth for real data. We analyze the accuracy of an
EBC star tracking system utilizing the Earth's motion as the ground truth for
comparison. The Earth rotates in a regular way with very small irregularities
which are measured to the level of milli-arcseconds. By keeping an event camera
static and pointing it through a ground-based telescope at the night sky, we
create a system where the only camera motion in the celestial reference frame
is that induced by the Earth's rotation. The resulting event stream is
processed to generate estimates of orientation which we compare to the
International Earth Rotation and Reference System (IERS) measured orientation
of the Earth. The event camera system is able to achieve a root mean squared
across error of 18.47 arcseconds and an about error of 78.84 arcseconds.
Combined with the other benefits of event cameras over framing sensors (reduced
computation due to sparser data streams, higher dynamic range, lower energy
consumption, faster update rates), this level of accuracy suggests the utility
of event cameras for low-cost and low-latency star tracking. We provide all
code and data used to generate our results:
https://gitlab.kitware.com/nest-public/telescope_accuracy_quantification.

</details>


### [56] [Handling Multiple Hypotheses in Coarse-to-Fine Dense Image Matching](https://arxiv.org/abs/2509.08805)
*Matthieu Vilain,Rémi Giraud,Yannick Berthoumieu,Guillaume Bourmaud*

Main category: cs.CV

TL;DR: 该论文提出了一种用于密集图像匹配的新方法BEAMER，通过在每个尺度预测和传播多个匹配假设，提高了在深度突变或目标图像大幅变焦场景下的匹配鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前主流的密集图像匹配方法在每个尺度为每个源点只预测单一匹配，这在深度不连续和大幅缩放时易导致错误。为此，作者希望探索在每尺度保留多个假设以提升匹配准确性和鲁棒性。

Method: 作者设计了一种结合beam search和cross-attention层的新架构。首先在各尺度为每个源点生成多个假设，通过beam search在多尺度间传播，再将这些多假设集成进cross-attention实现高效融合，并称这一架构为BEAMER。

Result: BEAMER被证明比现有方法在易错场景（如深度突变及强zoom-in）下表现更加鲁棒，能有效提升匹配准确率。

Conclusion: 在密集图像匹配任务中，BEAMER通过多假设传播机制显著提升了鲁棒性，尤其适合对复杂与极端场景。该思路为今后的密集匹配方法提供了创新方向。

Abstract: Dense image matching aims to find a correspondent for every pixel of a source
image in a partially overlapping target image. State-of-the-art methods
typically rely on a coarse-to-fine mechanism where a single correspondent
hypothesis is produced per source location at each scale. In challenging cases
-- such as at depth discontinuities or when the target image is a strong
zoom-in of the source image -- the correspondents of neighboring source
locations are often widely spread and predicting a single correspondent
hypothesis per source location at each scale may lead to erroneous matches. In
this paper, we investigate the idea of predicting multiple correspondent
hypotheses per source location at each scale instead. We consider a beam search
strategy to propagat multiple hypotheses at each scale and propose integrating
these multiple hypotheses into cross-attention layers, resulting in a novel
dense matching architecture called BEAMER. BEAMER learns to preserve and
propagate multiple hypotheses across scales, making it significantly more
robust than state-of-the-art methods, especially at depth discontinuities or
when the target image is a strong zoom-in of the source image.

</details>


### [57] [GeneVA: A Dataset of Human Annotations for Generative Text to Video Artifacts](https://arxiv.org/abs/2509.08818)
*Jenna Kang,Maria Silva,Patsorn Sangkloy,Kenneth Chen,Niall Williams,Qi Sun*

Main category: cs.CV

TL;DR: 本文介绍了GeneVA，这是一个大规模带有详细人工标注的视频生成伪影数据集，关注文本驱动生成视频中时空伪影的问题。


<details>
  <summary>Details</summary>
Motivation: 生成式模型在视频生成中经常出现难以预测的伪影（如物理不可能现象、时间不连续），而目前缺乏系统化的视频评测基准，主要因视频的时空复杂性导致合适数据集稀缺。

Method: 作者构建了GeneVA数据集，专注于收集并人工标注文本生成视频中出现的时空伪影，规模较大，覆盖多种伪影类型和丰富描述信息。

Result: GeneVA作为首个大规模、专门关注生成视频时空伪影的开放数据集，为模型评测和分析提供了有力工具。

Conclusion: GeneVA有望促进生成视频质量提升、评测方法发展，并辅助相关应用领域创新。

Abstract: Recent advances in probabilistic generative models have extended capabilities
from static image synthesis to text-driven video generation. However, the
inherent randomness of their generation process can lead to unpredictable
artifacts, such as impossible physics and temporal inconsistency. Progress in
addressing these challenges requires systematic benchmarks, yet existing
datasets primarily focus on generative images due to the unique spatio-temporal
complexities of videos. To bridge this gap, we introduce GeneVA, a large-scale
artifact dataset with rich human annotations that focuses on spatio-temporal
artifacts in videos generated from natural text prompts. We hope GeneVA can
enable and assist critical applications, such as benchmarking model performance
and improving generative video quality.

</details>


### [58] [RewardDance: Reward Scaling in Visual Generation](https://arxiv.org/abs/2509.08826)
*Jie Wu,Yu Gao,Zilyu Ye,Ming Li,Liang Li,Hanzhong Guo,Jie Liu,Zeyue Xue,Xiaoxia Hou,Wei Liu,Yan Zeng,Weilin Huang*

Main category: cs.CV

TL;DR: RewardDance是一种用于视觉生成的大规模奖励建模框架，通过与VLM架构内在对齐和创新生成式奖励方式，显著提升了生成质量并解决了奖励劫持和模式崩溃问题，支持奖励模型的规模、上下文扩展。


<details>
  <summary>Details</summary>
Motivation: 现有视觉生成领域的奖励模型（如基于CLIP和Bradley-Terry损失）存在架构、输入约束及与VLM实际机制不匹配等问题，严重制约了奖励模型的扩展和效果，且奖励劫持问题突出，急需创新性方法突破这些瓶颈。

Method: 提出RewardDance框架，将奖励建模转化为模型预测“Yes” token概率，表示生成结果是否优于参考图像，天然对齐VLM。RewardDance同时支持模型参数量和上下文（如任务指令、参考样例、CoT推理）双向扩展。

Result: RewardDance大规模奖励模型（最大达260亿参数）在文本生成图像、文本生成视频、图像生成视频等任务上均超越现有方法，并在RL微调时显著缓解奖励劫持和模式崩溃问题，输出多样、质量高。

Conclusion: RewardDance通过生成式奖励建模和VLM架构对齐，有效推进视觉生成奖励模型的扩展和鲁棒性，为生成模型的高质量、多样性输出奠定基础。

Abstract: Reward Models (RMs) are critical for improving generation models via
Reinforcement Learning (RL), yet the RM scaling paradigm in visual generation
remains largely unexplored. It primarily due to fundamental limitations in
existing approaches: CLIP-based RMs suffer from architectural and input
modality constraints, while prevalent Bradley-Terry losses are fundamentally
misaligned with the next-token prediction mechanism of Vision-Language Models
(VLMs), hindering effective scaling. More critically, the RLHF optimization
process is plagued by Reward Hacking issue, where models exploit flaws in the
reward signal without improving true quality. To address these challenges, we
introduce RewardDance, a scalable reward modeling framework that overcomes
these barriers through a novel generative reward paradigm. By reformulating the
reward score as the model's probability of predicting a "yes" token, indicating
that the generated image outperforms a reference image according to specific
criteria, RewardDance intrinsically aligns reward objectives with VLM
architectures. This alignment unlocks scaling across two dimensions: (1) Model
Scaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context
Scaling: Integration of task-specific instructions, reference examples, and
chain-of-thought (CoT) reasoning. Extensive experiments demonstrate that
RewardDance significantly surpasses state-of-the-art methods in text-to-image,
text-to-video, and image-to-video generation. Crucially, we resolve the
persistent challenge of "reward hacking": Our large-scale RMs exhibit and
maintain high reward variance during RL fine-tuning, proving their resistance
to hacking and ability to produce diverse, high-quality outputs. It greatly
relieves the mode collapse problem that plagues smaller models.

</details>


### [59] [SAFT: Shape and Appearance of Fabrics from Template via Differentiable Physical Simulations from Monocular Video](https://arxiv.org/abs/2509.08828)
*David Stotko,Reinhard Klein*

Main category: cs.CV

TL;DR: 本文提出了一种仅用单目RGB视频序列即可对织物进行三维动态场景重建与外观估计的新方法，通过物理模拟和可微渲染，提升了重建的真实感和细节。


<details>
  <summary>Details</summary>
Motivation: 三维动态场景重建尤其是仅依赖单目视频情况下，存在深度歧义等难题，同时要实现高质量的物理逼真渲染更有挑战。本文旨在提升基于单目视频的织物重建与外观估计的准确性与真实感。

Method: 采用物理模拟对布料的几何形变建模，结合可微渲染进行物理渲染。引入两种新的正则化项，专门解决单目视频三维重建中的深度歧义问题。使用单目RGB视频作为输入，实现融合几何和外观的联合优化。

Result: 相较最新方法，3D重建误差降低了2.64倍，单场景处理时间约30分钟。可以从单目视频成功恢复出高质量的运动和细节丰富的外观参数。

Conclusion: 该方法显著提升了单目视频下织物动态三维重建的准确性和真实感，为实际应用中低成本、高质量的动态物体重建提供了新思路。

Abstract: The reconstruction of three-dimensional dynamic scenes is a well-established
yet challenging task within the domain of computer vision. In this paper, we
propose a novel approach that combines the domains of 3D geometry
reconstruction and appearance estimation for physically based rendering and
present a system that is able to perform both tasks for fabrics, utilizing only
a single monocular RGB video sequence as input. In order to obtain realistic
and high-quality deformations and renderings, a physical simulation of the
cloth geometry and differentiable rendering are employed. In this paper, we
introduce two novel regularization terms for the 3D reconstruction task that
improve the plausibility of the reconstruction by addressing the depth
ambiguity problem in monocular video. In comparison with the most recent
methods in the field, we have reduced the error in the 3D reconstruction by a
factor of 2.64 while requiring a medium runtime of 30 min per scene.
Furthermore, the optimized motion achieves sufficient quality to perform an
appearance estimation of the deforming object, recovering sharp details from
this single monocular RGB video.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [60] [Bilingual Word Level Language Identification for Omotic Languages](https://arxiv.org/abs/2509.07998)
*Mesay Gemeda Yigezu,Girma Yohannis Bade,Atnafu Lambebo Tonja,Olga Kolesnikova,Grigori Sidorov,Alexander Gelbukh*

Main category: cs.CL

TL;DR: 本文针对南埃塞俄比亚使用的Wolaita和Gofa两种语言，提出并实验了双语语言识别（BLID）方法，最终BERT与LSTM结合模型表现最好，F1分数为0.72。


<details>
  <summary>Details</summary>
Motivation: Wolaita和Gofa两种本地语言存在相似和差异，这给自动识别带来挑战。且在多语社群及社交媒体场景真实存在双语混杂文本，识别需求迫切。

Method: 作者对多种方法进行了实验，最终采用BERT预训练语言模型与LSTM相结合的方法对这两种语言进行了识别。

Result: BERT与LSTM结合的模型在测试集上取得了0.72的F1分数，超过了其他尝试的方法。

Conclusion: 提出的方法能有效应对南埃塞俄比亚双语文本识别问题，对治理社交媒体相关问题及后续研究具有推动作用。

Abstract: Language identification is the task of determining the languages for a given
text. In many real world scenarios, text may contain more than one language,
particularly in multilingual communities. Bilingual Language Identification
(BLID) is the task of identifying and distinguishing between two languages in a
given text. This paper presents BLID for languages spoken in the southern part
of Ethiopia, namely Wolaita and Gofa. The presence of words similarities and
differences between the two languages makes the language identification task
challenging. To overcome this challenge, we employed various experiments on
various approaches. Then, the combination of the BERT based pretrained language
model and LSTM approach performed better, with an F1 score of 0.72 on the test
set. As a result, the work will be effective in tackling unwanted social media
issues and providing a foundation for further research in this area.

</details>


### [61] [AntiDote: Bi-level Adversarial Training for Tamper-Resistant LLMs](https://arxiv.org/abs/2509.08000)
*Debdeep Sanyal,Manodeep Ray,Murari Mandal*

Main category: cs.CL

TL;DR: 本文提出了一种新方法AntiDote，能提升开源大语言模型面对恶意微调（如LoRA注入等）攻击时的安全性，实验证明在各种攻击下，比现有基线更稳健且性能损失很小。


<details>
  <summary>Details</summary>
Motivation: 随着开源大语言模型的流行，研究民主化和模型误用防控之间出现了矛盾。攻击者可直接对模型权重全参微调以绕过安全措施，现有效果有限。需要新的方案增强开源模型安全性。

Method: 提出AntiDote方法：引入一个辅助对抗超网络，根据现有模型激活生成恶意的LoRA微调权重，模拟恶意攻击。主模型通过对抗训练，使自己能抵御这些恶意权重的注入，强制其保持安全对齐。

Result: 在52种红队攻击（包括越狱提示、潜空间操纵、直接权重攻击）中，AntiDote比同类防篡改与遗忘基线方法最高提升27.4%的稳健性。同时，该方法只带来不到0.5%的能力损失，在MMLU、HellaSwag、GSM8K等测试上性能影响极小。

Conclusion: AntiDote提供了一种实用且计算高效的训练流程，使开源大语言模型具备更强、内在的安全特性，在通用性能与安全抗攻击能力之间取得了很好的平衡。

Abstract: The release of open-weight large language models (LLMs) creates a tension
between advancing accessible research and preventing misuse, such as malicious
fine-tuning to elicit harmful content. Current safety measures struggle to
preserve the general capabilities of the LLM while resisting a determined
adversary with full access to the model's weights and architecture, who can use
full-parameter fine-tuning to erase existing safeguards. To address this, we
introduce AntiDote, a bi-level optimization procedure for training LLMs to be
resistant to such tampering. AntiDote involves an auxiliary adversary
hypernetwork that learns to generate malicious Low-Rank Adaptation (LoRA)
weights conditioned on the defender model's internal activations. The defender
LLM is then trained with an objective to nullify the effect of these
adversarial weight additions, forcing it to maintain its safety alignment. We
validate this approach against a diverse suite of 52 red-teaming attacks,
including jailbreak prompting, latent space manipulation, and direct
weight-space attacks. AntiDote is upto 27.4\% more robust against adversarial
attacks compared to both tamper-resistance and unlearning baselines. Crucially,
this robustness is achieved with a minimal trade-off in utility, incurring a
performance degradation of upto less than 0.5\% across capability benchmarks
including MMLU, HellaSwag, and GSM8K. Our work offers a practical and compute
efficient methodology for building open-weight models where safety is a more
integral and resilient property.

</details>


### [62] [MVPBench: A Benchmark and Fine-Tuning Framework for Aligning Large Language Models with Diverse Human Values](https://arxiv.org/abs/2509.08022)
*Yao Liang,Dongcheng Zhao,Feifei Zhao,Guobin Shen,Yuwei Wang,Dongqi Liang,Yi Zeng*

Main category: cs.CL

TL;DR: 提出了MVPBench，一个多维度跨国人类价值观对齐评测基准，用于系统评估大语言模型（LLMs）在75个国家的价值观对齐表现，并分析了不同模型的地理和人口差异。


<details>
  <summary>Details</summary>
Motivation: 现有对齐基准过于单一，忽略了文化和人口多样性，因此无法全面理解LLMs在全球范围的价值观对齐能力。

Method: 构建MVPBench基准，涵盖24020个人类价值观对齐案例，附有细致价值标签、个性化问题和丰富人口统计元数据；利用该基准对多种先进LLMs进行评测，并采用LoRA、DPO等轻量级微调方法以优化对齐效果。

Result: 发现不同地理和人口群体间LLMs的对齐表现存在明显差异；此外，LoRA和DPO等微调手段显著提升了模型在域内和跨域的价值观对齐能力。

Conclusion: 研究凸显了考虑多样人口背景的对齐评估必要性，为发展具备跨文化适应性和价值敏感性的LLMs提供了实践基础，也为今后全球化对齐、个性化价值建模与公平AI发展指明了方向。

Abstract: The alignment of large language models (LLMs) with human values is critical
for their safe and effective deployment across diverse user populations.
However, existing benchmarks often neglect cultural and demographic diversity,
leading to limited understanding of how value alignment generalizes globally.
In this work, we introduce MVPBench, a novel benchmark that systematically
evaluates LLMs' alignment with multi-dimensional human value preferences across
75 countries. MVPBench contains 24,020 high-quality instances annotated with
fine-grained value labels, personalized questions, and rich demographic
metadata, making it the most comprehensive resource of its kind to date. Using
MVPBench, we conduct an in-depth analysis of several state-of-the-art LLMs,
revealing substantial disparities in alignment performance across geographic
and demographic lines. We further demonstrate that lightweight fine-tuning
methods, such as Low-Rank Adaptation (LoRA) and Direct Preference Optimization
(DPO), can significantly enhance value alignment in both in-domain and
out-of-domain settings. Our findings underscore the necessity for
population-aware alignment evaluation and provide actionable insights for
building culturally adaptive and value-sensitive LLMs. MVPBench serves as a
practical foundation for future research on global alignment, personalized
value modeling, and equitable AI development.

</details>


### [63] [NOWJ@COLIEE 2025: A Multi-stage Framework Integrating Embedding Models and Large Language Models for Legal Retrieval and Entailment](https://arxiv.org/abs/2509.08025)
*Hoang-Trung Nguyen,Tan-Minh Nguyen,Xuan-Bach Le,Tuan-Kiet Le,Khanh-Huyen Nguyen,Ha-Thanh Nguyen,Thi-Hai-Yen Vuong,Le-Minh Nguyen*

Main category: cs.CL

TL;DR: 本文介绍了NOWJ团队在COLIEE 2025竞赛五项任务中的方法与成绩，尤其是在法律案例蕴含任务（Task 2）中取得第一名。采用了传统信息检索与生成式大模型的融合方法，在所有任务上均表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着法律信息处理需求增加，法律领域对检索、蕴含判断、判决预测等自动化方法的需求增长。现有方法在复杂推理或上下文理解上存在瓶颈，因此亟需结合现代大模型与传统信息检索技术提升表现。

Method: 采用预排序模型（BM25、BERT、monoT5）、语义嵌入（BGE-m3、LLM2Vec）及先进大模型（Qwen-2、QwQ-32B、DeepSeek-V3）对文本进行排序与摘要。在Task 2中，结合词法语义过滤和上下文化大模型分析的两阶段检索体系。此外，在其他任务中采用模型融合与高效提示推理策略。

Result: 在Task 2法律案例蕴含任务中取得F1第一（0.3195），其他如案例检索、法条检索、法律文本蕴含、判决预测等任务中也表现出色。

Conclusion: 结合传统信息检索与生成式大模型的混合方法在法律信息处理领域具有巨大潜力，为未来相关研究提供了有益参考。

Abstract: This paper presents the methodologies and results of the NOWJ team's
participation across all five tasks at the COLIEE 2025 competition, emphasizing
advancements in the Legal Case Entailment task (Task 2). Our comprehensive
approach systematically integrates pre-ranking models (BM25, BERT, monoT5),
embedding-based semantic representations (BGE-m3, LLM2Vec), and advanced Large
Language Models (Qwen-2, QwQ-32B, DeepSeek-V3) for summarization, relevance
scoring, and contextual re-ranking. Specifically, in Task 2, our two-stage
retrieval system combined lexical-semantic filtering with contextualized LLM
analysis, achieving first place with an F1 score of 0.3195. Additionally, in
other tasks--including Legal Case Retrieval, Statute Law Retrieval, Legal
Textual Entailment, and Legal Judgment Prediction--we demonstrated robust
performance through carefully engineered ensembles and effective prompt-based
reasoning strategies. Our findings highlight the potential of hybrid models
integrating traditional IR techniques with contemporary generative models,
providing a valuable reference for future advancements in legal information
processing.

</details>


### [64] [SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery](https://arxiv.org/abs/2509.08032)
*Fengyu She,Nan Wang,Hongfei Wu,Ziyi Wan,Jingmian Wang,Chang Wang*

Main category: cs.CL

TL;DR: 科研文献数量爆炸性增长，现有大型语言模型难以满足科学领域的专有需求。该文提出了新模型SciGPT及其专用评测基准ScienceBench。


<details>
  <summary>Details</summary>
Motivation: 当前科学文献数量极大，研究者难以高效整合知识，现有通用LLM缺乏科学领域专业性、在复杂科学任务表现不佳，限制了跨学科研究。

Method: 1. SciGPT模型基于Qwen3架构，通过两阶段低成本领域蒸馏提升效率和性能；2. 引入稀疏专家混合（SMoE）注意力机制，大幅降低长文档推理内存消耗；3. 结合领域本体，实现跨学科知识适应。同时提出ScienceBench作为专用评测集。

Result: 在ScienceBench评测中，SciGPT在序列标注、文本生成和推理等关键科学任务上表现优于GPT-4o，对未见过的新任务也有较强鲁棒性。

Conclusion: SciGPT有效提升了科学文献理解与推理能力，显著优于现有大模型，为AI辅助科学发现提供了新工具和可能。

Abstract: Scientific literature is growing exponentially, creating a critical
bottleneck for researchers to efficiently synthesize knowledge. While
general-purpose Large Language Models (LLMs) show potential in text processing,
they often fail to capture scientific domain-specific nuances (e.g., technical
jargon, methodological rigor) and struggle with complex scientific tasks,
limiting their utility for interdisciplinary research. To address these gaps,
this paper presents SciGPT, a domain-adapted foundation model for scientific
literature understanding and ScienceBench, an open source benchmark tailored to
evaluate scientific LLMs.
  Built on the Qwen3 architecture, SciGPT incorporates three key innovations:
(1) low-cost domain distillation via a two-stage pipeline to balance
performance and efficiency; (2) a Sparse Mixture-of-Experts (SMoE) attention
mechanism that cuts memory consumption by 55\% for 32,000-token long-document
reasoning; and (3) knowledge-aware adaptation integrating domain ontologies to
bridge interdisciplinary knowledge gaps.
  Experimental results on ScienceBench show that SciGPT outperforms GPT-4o in
core scientific tasks including sequence labeling, generation, and inference.
It also exhibits strong robustness in unseen scientific tasks, validating its
potential to facilitate AI-augmented scientific discovery.

</details>


### [65] [No for Some, Yes for Others: Persona Prompts and Other Sources of False Refusal in Language Models](https://arxiv.org/abs/2509.08075)
*Flor Miriam Plaza-del-Arco,Paul Röttger,Nino Scherrer,Emanuele Borgonovo,Elmar Plischke,Dirk Hovy*

Main category: cs.CL

TL;DR: 本论文研究了大型语言模型（LLM）个性化时出现的“错误拒绝”现象，分析了社会人口学属性（如性别、种族、宗教和残疾）对错误拒绝率的影响。结果表明，随着模型能力增强，个性化人格对错误拒绝影响减弱，但某些人格在部分模型中会增加错误拒绝，显示出算法偏见。最终发现，模型类型和任务本身对错误拒绝的影响更大，有关个性化的影响可能被高估了。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，人格化个性定制逐渐普及，但个性化也带来了不可预料的副作用，尤其是‘错误拒绝’用户请求的问题。之前的研究指出‘人格提示’可能导致模型错误拒绝，但没有系统量化该问题的严重性。因此，作者希望通过全面测量和科学实验，厘清个性化人格对模型表现的具体影响。

Method: 作者设计了包含15种社会人口学人格（按性别、种族、宗教和残疾分类），在16种不同大语言模型、3类任务（自然语言推断、礼貌性和冒犯性分类）、9种提示语重述条件下进行测试。提出了基于蒙特卡洛方法的新型量化方法，有效估算‘错误拒绝’现象的发生情况，并控制混杂变量。

Result: 实验发现：随着大语言模型能力提高，人格化定制对错误拒绝的影响趋于减弱；某些人口学人格在部分模型仍会显著增加错误拒绝现象，显示出安全机制或对齐策略存在偏见。此外，模型类型和任务本身对错误拒绝率的影响远大于人格化属性，尤其是在处理敏感内容时更为显著。

Conclusion: 研究认为，个性化人格对大模型‘错误拒绝’影响被高估，模型选择和任务类型才是主导因素。同时揭示了对齐与安全机制中可能存在的偏见，为今后模型设计与安全审查提供参考。

Abstract: Large language models (LLMs) are increasingly integrated into our daily lives
and personalized. However, LLM personalization might also increase unintended
side effects. Recent work suggests that persona prompting can lead models to
falsely refuse user requests. However, no work has fully quantified the extent
of this issue. To address this gap, we measure the impact of 15
sociodemographic personas (based on gender, race, religion, and disability) on
false refusal. To control for other factors, we also test 16 different models,
3 tasks (Natural Language Inference, politeness, and offensiveness
classification), and nine prompt paraphrases. We propose a Monte Carlo-based
method to quantify this issue in a sample-efficient manner. Our results show
that as models become more capable, personas impact the refusal rate less and
less. Certain sociodemographic personas increase false refusal in some models,
which suggests underlying biases in the alignment strategies or safety
mechanisms. However, we find that the model choice and task significantly
influence false refusals, especially in sensitive content tasks. Our findings
suggest that persona effects have been overestimated, and might be due to other
factors.

</details>


### [66] [Culturally transmitted color categories in LLMs reflect a learning bias toward efficient compression](https://arxiv.org/abs/2509.08093)
*Nathaniel Imel,Noga Zaslavsky*

Main category: cs.CL

TL;DR: 本研究探讨大语言模型（LLM）是否能像人类一样自发形成高效的语义范畴体系，尤其以颜色分类为例，结果显示LLM能趋近人类的语义压缩和高效表达。


<details>
  <summary>Details</summary>
Motivation: 人类语言的语义范畴系统通常遵循信息瓶颈（IB）理论，实现信息压缩和表达的平衡。而大语言模型虽然没有专门为此目标训练，是否能表现出类似人类的高效语义系统尚不清楚。该研究以颜色命名任务为例，检验LLM能否形成符合IB原则的语义系统。

Method: 研究复现了两个人类行为实验：（1）使用Gemini和Llama模型在英语颜色命名任务中与人类实验对照测量其IB效率；（2）通过循环上下文语言学习，模拟颜色命名系统的文化进化，观察模型是否能自主演化出更高效的系统。

Result: Gemini模型的颜色命名与英语母语者高度一致，达到较高的信息瓶颈效率分数，Llama虽也高效，但复杂度较低。迭代实验中，LLM可将初始随机系统逐步重组为更高IB效率，并趋近世界语言的普遍类别划分。

Conclusion: LLM能够自主演化出类人、感知基础的高效语义系统，表明其内部涌现出了与人类语言相同的信息效率原理。

Abstract: Converging evidence suggests that systems of semantic categories across human
languages achieve near-optimal compression via the Information Bottleneck (IB)
complexity-accuracy principle. Large language models (LLMs) are not trained for
this objective, which raises the question: are LLMs capable of evolving
efficient human-like semantic systems? To address this question, we focus on
the domain of color as a key testbed of cognitive theories of categorization
and replicate with LLMs (Gemini 2.0-flash and Llama 3.3-70B-Instruct) two
influential human behavioral studies. First, we conduct an English color-naming
study, showing that Gemini aligns well with the naming patterns of native
English speakers and achieves a significantly high IB-efficiency score, while
Llama exhibits an efficient but lower complexity system compared to English.
Second, to test whether LLMs simply mimic patterns in their training data or
actually exhibit a human-like inductive bias toward IB-efficiency, we simulate
cultural evolution of pseudo color-naming systems in LLMs via iterated
in-context language learning. We find that akin to humans, LLMs iteratively
restructure initially random systems towards greater IB-efficiency and
increased alignment with patterns observed across the world's languages. These
findings demonstrate that LLMs are capable of evolving perceptually grounded,
human-like semantic systems, driven by the same fundamental principle that
governs semantic efficiency across human languages.

</details>


### [67] [MERLIN: Multi-Stage Curriculum Alignment for Multilingual Encoder and LLM Fusion](https://arxiv.org/abs/2509.08105)
*Kosei Uemura,David Guzmán,Quang Phuoc Nguyen,Jesujoba Oluwadara Alabi,En-shiun Annie Lee,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: MERLIN方法通过两阶段模型叠加和课程式学习策略，有效提升了低资源语言复杂推理任务的表现，在多个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在英语等高资源语言上表现优异，但在低资源语言（LRL）上的复杂推理任务表现较差，且现有方法对于LRL的提升仍有限，存在较大性能差距。

Method: 提出MERLIN框架，采用两阶段模型叠加策略。第一阶段利用通用双语平行语料进行训练，第二阶段在具体任务数据上精细调整，仅需适配少量DoRA权重。同时融合课程学习策略逐步过渡。

Result: 在AfriMGSM基准上，MERLIN比MindMerger提升了12.9个百分点的准确率，并超过了GPT-4o-mini；在MGSM和MSVAMP数据集上也有正向提升（分别为+0.9和+2.8个百分点），显示了跨低高资源设置的有效性。

Conclusion: MERLIN有效缩小了低资源语言复杂推理任务的性能差距，在多语种、不同资源丰富度的条件下均有明显优势。

Abstract: Large language models excel in English but still struggle with complex
reasoning in many low-resource languages (LRLs). Existing encoder-plus-decoder
methods such as LangBridge and MindMerger raise accuracy on mid and
high-resource languages, yet they leave a large gap on LRLs. We present MERLIN,
a two-stage model-stacking framework that applies a curriculum learning
strategy -- from general bilingual bitext to task-specific data -- and adapts
only a small set of DoRA weights. On the AfriMGSM benchmark MERLIN improves
exact-match accuracy by +12.9 pp over MindMerger and outperforms GPT-4o-mini.
It also yields consistent gains on MGSM and MSVAMP (+0.9 and +2.8 pp),
demonstrating effectiveness across both low and high-resource settings.

</details>


### [68] [Bias after Prompting: Persistent Discrimination in Large Language Models](https://arxiv.org/abs/2509.08146)
*Nivedha Sivakumar,Natalie Mackraz,Samira Khorshidi,Krishna Patel,Barry-John Theobald,Luca Zappella,Nicholas Apostoloff*

Main category: cs.CL

TL;DR: 本文研究了大语言模型(LLM)中的偏见是否会通过提示(prompt)适配方式向下游任务迁移，结论是偏见确实会迁移，且现有的基于提示的去偏方法并不能有效阻止这一现象。


<details>
  <summary>Details</summary>
Motivation: 现有关于偏见迁移假说(BTH)的研究常假设大型预训练模型中的偏见不会通过适配(比如prompting)传递到下游模型或任务中，这一假设在实际应用场景中可能导致对偏见的低估，因此有必要验证它的实际有效性。

Method: 作者基于因果模型，分析了在提示(prompt)适配下LLM的偏见迁移情况，涵盖了协同指代消解(如性别)和问答(如年龄、宗教)等多种任务和群体，并系统评估了不同的基于prompt的去偏方法在减少偏见迁移方面的有效性。

Result: 实验发现：1) LLM中的内在偏见与经prompt适配后的偏见在各类任务和群体间仍呈中到强的相关性(如性别rho>=0.94，年龄rho>=0.98)；2) 不同的few-shot构成参数变化(如样本数量、内容刻板印象、职业比例等)下，偏见相关性依然很高(rho>=0.90)；3) 经评测的多种prompt去偏方法都未能稳定地减少偏见迁移。

Conclusion: 当前流行的基于prompt的方法和去偏策略不能有效阻止LLM中的偏见迁移。从源头上修正预训练模型的偏见，或可有效遏制其在下游任务中的传播，同时可能提升模型推理能力。

Abstract: A dangerous assumption that can be made from prior work on the bias transfer
hypothesis (BTH) is that biases do not transfer from pre-trained large language
models (LLMs) to adapted models. We invalidate this assumption by studying the
BTH in causal models under prompt adaptations, as prompting is an extremely
popular and accessible adaptation strategy used in real-world applications. In
contrast to prior work, we find that biases can transfer through prompting and
that popular prompt-based mitigation methods do not consistently prevent biases
from transferring. Specifically, the correlation between intrinsic biases and
those after prompt adaptation remain moderate to strong across demographics and
tasks -- for example, gender (rho >= 0.94) in co-reference resolution, and age
(rho >= 0.98) and religion (rho >= 0.69) in question answering. Further, we
find that biases remain strongly correlated when varying few-shot composition
parameters, such as sample size, stereotypical content, occupational
distribution and representational balance (rho >= 0.90). We evaluate several
prompt-based debiasing strategies and find that different approaches have
distinct strengths, but none consistently reduce bias transfer across models,
tasks or demographics. These results demonstrate that correcting bias, and
potentially improving reasoning ability, in intrinsic models may prevent
propagation of biases to downstream tasks.

</details>


### [69] [Verbalized Algorithms](https://arxiv.org/abs/2509.08150)
*Supriya Lall,Christian Farrell,Hari Pathanjaly,Marko Pavic,Sarvesh Chezhian,Masataro Asai*

Main category: cs.CL

TL;DR: 本文提出了一种新范式“verbalized algorithms”（VAs），通过将大任务分解为简单自然语言操作，利用经典算法减少大模型推理错误。实验展示了其在字符串排序和聚类任务上的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前大模型（LLMs）被用于推理任务时，常常采用一轮问答（one-shot）方式，但获取正确答案的稳定性差。为克服这一难题，作者希望结合理论可靠的传统算法与大模型能力，提高复杂任务的准确性和可控性。

Method: 作者提出了“verbalized algorithms”（VAs）：将复杂任务分解为LLMs可稳定完成的简单自然语言子任务。例如，将排序任务中的每一步二元比较转化为LLM可处理的字符串判断，并将原有经典排序算法（如bitonic排序网络）与LLM结合。

Result: 采用VAs，在排序和聚类等任务中，能够有效发挥LLM的能力，显著提升任务完成的准确性和鲁棒性。

Conclusion: 将LLM作为经典算法的子模块，只处理有限范围的基础任务，可以有效弥补LLM在复杂推理任务上的不稳定性，从而实现更可靠的推理与决策。

Abstract: Instead of querying LLMs in a one-shot manner and hoping to get the right
answer for a reasoning task, we propose a paradigm we call \emph{verbalized
algorithms} (VAs), which leverage classical algorithms with established
theoretical understanding. VAs decompose a task into simple elementary
operations on natural language strings that they should be able to answer
reliably, and limit the scope of LLMs to only those simple tasks. For example,
for sorting a series of natural language strings, \emph{verbalized sorting}
uses an LLM as a binary comparison oracle in a known and well-analyzed sorting
algorithm (e.g., bitonic sorting network). We demonstrate the effectiveness of
this approach on sorting and clustering tasks.

</details>


### [70] [Balancing Quality and Variation: Spam Filtering Distorts Data Label Distributions](https://arxiv.org/abs/2509.08217)
*Eve Fleisig,Matthias Orlikowski,Philipp Cimiano,Dan Klein*

Main category: cs.CL

TL;DR: 本文探讨了在保留数据多样性的同时，如何通过滤除低质量或垃圾标注者来优化机器学习数据集标签的可靠性。研究发现，许多常用的标注者过滤方法反而不当地移除了持有不同意见的标注者，导致标签多样性和准确性之间产生次优权衡。


<details>
  <summary>Details</summary>
Motivation: 确保机器学习数据集既能代表多元观点，又能防止垃圾或低质量标注对标签质量产生负面影响，是数据标注中的关键挑战。因此，研究者希望找到在保留标签多样性的同时，有效去除垃圾标注的方法。

Method: 作者实证评估了多种标注者筛选启发式方法在主观任务中对标签多样性的影响，并在合成垃圾标注场景下分析这些方法的表现，比较了不同“严苛度”下的标签准确性和多样性权衡。

Result: 研究发现，现有许多针对“唯一真值”场景设计的标注者过滤方法，往往误删对主观问题持不同看法的标注者，而不是垃圾标注者。当标注者剔除比例很小（小于5%）时表现较好，超过后会快速损害标签准确性。合成垃圾标注实验说明，大部分真实垃圾标注者与普通标注者分布难以区分，真正容易分辨的少数垃圾标注者更偏向于给出固定答案而非随机答案。

Conclusion: 现有垃圾标注过滤方法未能充分考虑标签多样性的保留，在需要保留分歧的主观标注任务中，过分依赖这些方法反而会伤害数据质量。未来应开发可以兼顾多样性和可靠性的垃圾标注删除方法。

Abstract: For machine learning datasets to accurately represent diverse opinions in a
population, they must preserve variation in data labels while filtering out
spam or low-quality responses. How can we balance annotator reliability and
representation? We empirically evaluate how a range of heuristics for annotator
filtering affect the preservation of variation on subjective tasks. We find
that these methods, designed for contexts in which variation from a single
ground-truth label is considered noise, often remove annotators who disagree
instead of spam annotators, introducing suboptimal tradeoffs between accuracy
and label diversity. We find that conservative settings for annotator removal
(<5%) are best, after which all tested methods increase the mean absolute error
from the true average label. We analyze performance on synthetic spam to
observe that these methods often assume spam annotators are less random than
real spammers tend to be: most spammers are distributionally indistinguishable
from real annotators, and the minority that are distinguishable tend to give
fixed answers, not random ones. Thus, tasks requiring the preservation of
variation reverse the intuition of existing spam filtering methods: spammers
tend to be less random than non-spammers, so metrics that assume variation is
spam fare worse. These results highlight the need for spam removal methods that
account for label diversity.

</details>


### [71] [Towards Knowledge-Aware Document Systems: Modeling Semantic Coverage Relations via Answerability Detection](https://arxiv.org/abs/2509.08304)
*Yehudit Aperstein,Alon Gottlib,Gal Benita,Alexander Apartsin*

Main category: cs.CL

TL;DR: 本文提出了一种用于建模文档间语义覆盖关系（SCR）的新框架，并构建了合成数据集和基准，评估了现有模型在识别文档间信息关系方面的能力。


<details>
  <summary>Details</summary>
Motivation: 理解不同表述方式下信息在文档间的共享对于信息检索、摘要和内容对齐等任务至关重要。而现有方法对文档间“信息覆盖关系”的建模有所不足。

Method: 提出以问答为基础的评估方法，通过文档间共享问题的可回答性来判定三种语义覆盖关系（等价、包含、部分重叠）。为实现评估，作者基于SQuAD语料，通过释义和信息省略生成合成数据集，并训练了生成式与判别式模型进行对比。

Result: 判别式模型显著优于生成式模型。RoBERTa-base模型在准确率上达到61.4%，Random Forest模型在macro-F1上最高（52.9%）。

Conclusion: 问答方法为文档间信息关系建模提供了有效工具，表明当前模型在超越表面相似性的推理能力仍有限。数据集和代码已公开，便于复现和后续研究。

Abstract: Understanding how information is shared across documents, regardless of the
format in which it is expressed, is critical for tasks such as information
retrieval, summarization, and content alignment. In this work, we introduce a
novel framework for modelling Semantic Coverage Relations (SCR), which
classifies document pairs based on how their informational content aligns. We
define three core relation types: equivalence, where both texts convey the same
information using different textual forms or styles; inclusion, where one
document fully contains the information of another and adds more; and semantic
overlap, where each document presents partially overlapping content. To capture
these relations, we adopt a question answering (QA)-based approach, using the
answerability of shared questions across documents as an indicator of semantic
coverage. We construct a synthetic dataset derived from the SQuAD corpus by
paraphrasing source passages and selectively omitting information, enabling
precise control over content overlap. This dataset allows us to benchmark
generative language models and train transformer-based classifiers for SCR
prediction. Our findings demonstrate that discriminative models significantly
outperform generative approaches, with the RoBERTa-base model achieving the
highest accuracy of 61.4% and the Random Forest-based model showing the best
balance with a macro-F1 score of 52.9%. The results show that QA provides an
effective lens for assessing semantic relations across stylistically diverse
texts, offering insights into the capacity of current models to reason about
information beyond surface similarity. The dataset and code developed in this
study are publicly available to support reproducibility.

</details>


### [72] [Toward Subtrait-Level Model Explainability in Automated Writing Evaluation](https://arxiv.org/abs/2509.08345)
*Alejandro Andrade-Lotero,Lee Becker,Joshua Southerland,Scott Hellman*

Main category: cs.CL

TL;DR: 本文提出利用生成式语言模型进行自动写作评分中的子特质（潜在特质成分）评估，以增强评分透明度，初步实验显示自动与人工子特质评分相关性一般。


<details>
  <summary>Details</summary>
Motivation: 自动写作评分虽然效率高，但缺乏透明度和可解释性，难以帮助教育工作者和学生理解得分原因。因此，提升自动评分的细致解释能力成为亟需解决的问题。

Method: 作者提出用生成式语言模型对写作的不同子特质进行打分，通过将自动评分的各子特质分数与人工评分进行对比，探究二者之间的相关性，并评估方法的可解释性。

Result: 实验表明，人工子特质分数与总特质分数之间，以及自动评分与人工子特质分数之间存在一定但不高的相关性。

Conclusion: 该方法能够为自动写作评分提供细致化解释，有助于提高评分透明度，使教师和学生更好地理解分数背后的依据。

Abstract: Subtrait (latent-trait components) assessment presents a promising path
toward enhancing transparency of automated writing scores. We prototype
explainability and subtrait scoring with generative language models and show
modest correlation between human subtrait and trait scores, and between
automated and human subtrait scores. Our approach provides details to demystify
scores for educators and students.

</details>


### [73] [Automatic Detection of Inauthentic Templated Responses in English Language Assessments](https://arxiv.org/abs/2509.08355)
*Yashad Samant,Lee Becker,Scott Hellman,Bradley Behan,Sarah Hughes,Joshua Southerland*

Main category: cs.CL

TL;DR: 本文设计了一种检测考试中考生利用模板作弊的机器学习方法，并强调了模型定期更新的重要性。


<details>
  <summary>Details</summary>
Motivation: 在高风险的英语考试中，一些低水平考生通过背诵模板来欺骗自动评分系统，影响考试的公平性和自动评分的可靠性。

Method: 提出了自动检测非真实模板化答案（AuDITR）任务，并基于机器学习实现检测，能够识别考生是否在使用模板作答。

Result: 实验展示了该自动检测方法的有效性，并通过实际应用表明，只有定期更新该检测模型，才能持续对抗新的作弊手法。

Conclusion: 机器学习在自动检测模板作弊中表现出显著效果，但模型需持续迭代升级，以保持对高风险英语考试作弊行为的识别能力。

Abstract: In high-stakes English Language Assessments, low-skill test takers may employ
memorized materials called ``templates'' on essay questions to ``game'' or fool
the automated scoring system. In this study, we introduce the automated
detection of inauthentic, templated responses (AuDITR) task, describe a machine
learning-based approach to this task and illustrate the importance of regularly
updating these models in production.

</details>


### [74] [<think> So let's replace this phrase with insult... </think> Lessons learned from generation of toxic texts with LLMs](https://arxiv.org/abs/2509.08358)
*Sergey Pletenev,Daniil Moskovskiy,Alexander Panchenko*

Main category: cs.CL

TL;DR: 现代大语言模型（LLMs）虽然能生成合成数据，但用于文本去毒化等敏感领域方面表现有限。使用LLM生成的有毒数据训练去毒化模型，性能明显劣于用真人数据，主要原因是生成内容的词汇多样性明显不足。


<details>
  <summary>Details</summary>
Motivation: 目前去毒化模型训练需要大量有毒文本样本，人工标注代价高昂。研究者希望用LLM合成有毒样本以降低成本、扩大数据量，但尚不清楚用合成数据替代真人数据会如何影响去毒化模型质量。

Method: 采用Llama 3 和 Qwen 等LLM，针对ParaDetox与SST-2数据集中性文本，生成配套有毒文本并构建训练集。用这些合成数据微调去毒化模型，并用与真人有毒样本训练的模型进行对比，分析性能差异。同时，分析生成数据的词汇多样性。

Result: 实验发现，基于LLM合成数据训练的去毒化模型相比真人数据训练的性能下降高达30%。合成数据的有毒内容词汇重复度高、缺乏人类多样性，导致模型泛化能力降低。

Conclusion: 当前LLM在生成复杂、多样的有毒文本方面能力有限，所产合成数据难以替代真人标注数据。应持续重视多样化真人数据在构建高质量去毒化模型中的不可替代作用。

Abstract: Modern Large Language Models (LLMs) are excellent at generating synthetic
data. However, their performance in sensitive domains such as text
detoxification has not received proper attention from the scientific community.
This paper explores the possibility of using LLM-generated synthetic toxic data
as an alternative to human-generated data for training models for
detoxification. Using Llama 3 and Qwen activation-patched models, we generated
synthetic toxic counterparts for neutral texts from ParaDetox and SST-2
datasets. Our experiments show that models fine-tuned on synthetic data
consistently perform worse than those trained on human data, with a drop in
performance of up to 30% in joint metrics. The root cause is identified as a
critical lexical diversity gap: LLMs generate toxic content using a small,
repetitive vocabulary of insults that fails to capture the nuances and variety
of human toxicity. These findings highlight the limitations of current LLMs in
this domain and emphasize the continued importance of diverse, human-annotated
data for building robust detoxification systems.

</details>


### [75] [Low-Resource Fine-Tuning for Multi-Task Structured Information Extraction with a Billion-Parameter Instruction-Tuned Model](https://arxiv.org/abs/2509.08381)
*Yu Cheng Chih,Yong Hao Hou*

Main category: cs.CL

TL;DR: 本论文提出ETLCH小型大语言模型，能高效低成本地完成结构化数据抽取任务，尤其适用于资源有限的环境。


<details>
  <summary>Details</summary>
Motivation: 大模型在结构化数据抽取等领域表现优异，但中小团队难以承担其高部署成本及大规模数据需求，目前关于小模型在低资源多任务场景下的效果研究较少。

Method: 基于LLaMA开发了参数量约十亿的ETLCH模型，通过低秩适应（LoRA）方法在每个任务仅数百至千条样本下微调，任务包括JSON抽取、知识图谱抽取与实体识别。

Result: ETLCH小模型在绝大多数评测指标上超越主流基线模型，尤其在数据极少时有显著优势。

Conclusion: 微调得当的小型模型可在计算资源受限场景下实现稳定、准确的结构化数据抽取，大幅降低部署和使用成本。

Abstract: Deploying large language models (LLMs) for structured data extraction in
domains such as financial compliance reporting, legal document analytics, and
multilingual knowledge base construction is often impractical for smaller teams
due to the high cost of running large architectures and the difficulty of
preparing large, high-quality datasets. Most recent instruction-tuning studies
focus on seven-billion-parameter or larger models, leaving limited evidence on
whether much smaller models can work reliably under low-resource, multi-task
conditions. This work presents ETLCH, a billion-parameter LLaMA-based model
fine-tuned with low-rank adaptation on only a few hundred to one thousand
samples per task for JSON extraction, knowledge graph extraction, and named
entity recognition. Despite its small scale, ETLCH outperforms strong baselines
across most evaluation metrics, with substantial gains observed even at the
lowest data scale. These findings demonstrate that well-tuned small models can
deliver stable and accurate structured outputs at a fraction of the
computational cost, enabling cost-effective and reliable information extraction
pipelines in resource-constrained environments.

</details>


### [76] [CommonVoice-SpeechRE and RPG-MoGe: Advancing Speech Relation Extraction with a New Dataset and Multi-Order Generative Framework](https://arxiv.org/abs/2509.08438)
*Jinzhong Ning,Paerhati Tulajiang,Yingying Le,Yijia Zhang,Yuanyuan Sun,Hongfei Lin,Haifeng Liu*

Main category: cs.CL

TL;DR: 本文提出并发布了首个大规模真实人声关系抽取数据集CommonVoice-SpeechRE，并提出了RPG-MoGe框架，有效提升了语音关系抽取性能。


<details>
  <summary>Details</summary>
Motivation: 现有语音关系抽取数据集主要依赖合成数据，缺乏真实人声样本，限制了模型的泛化能力。同时，现有方法主要采用单一模板和弱语义对齐，效果有限。

Method: 1. 构建了CommonVoice-SpeechRE数据集，包含约2万条多说话者真实语音样本。2. 设计了RPG-MoGe多模态生成框架，采用多顺序三元组生成集成策略，在训练与推理时利用不同元素顺序的数据多样性。3. 引入CNN关系预测头，生成明确的关系提示，实现更精准的跨模态对齐与三元组生成。

Result: 实验证明，提出的RPG-MoGe方法在新数据集上显著优于当前主流方法，推动了真实人声语音关系抽取领域的发展。

Conclusion: 论文为语音关系抽取任务提供了高质量基准数据集和有效解决方案，为后续研究奠定了基础。源码与数据集已开源，有助于社区进一步研究。

Abstract: Speech Relation Extraction (SpeechRE) aims to extract relation triplets
directly from speech. However, existing benchmark datasets rely heavily on
synthetic data, lacking sufficient quantity and diversity of real human speech.
Moreover, existing models also suffer from rigid single-order generation
templates and weak semantic alignment, substantially limiting their
performance. To address these challenges, we introduce CommonVoice-SpeechRE, a
large-scale dataset comprising nearly 20,000 real-human speech samples from
diverse speakers, establishing a new benchmark for SpeechRE research.
Furthermore, we propose the Relation Prompt-Guided Multi-Order Generative
Ensemble (RPG-MoGe), a novel framework that features: (1) a multi-order triplet
generation ensemble strategy, leveraging data diversity through diverse element
orders during both training and inference, and (2) CNN-based latent relation
prediction heads that generate explicit relation prompts to guide cross-modal
alignment and accurate triplet generation. Experiments show our approach
outperforms state-of-the-art methods, providing both a benchmark dataset and an
effective solution for real-world SpeechRE. The source code and dataset are
publicly available at https://github.com/NingJinzhong/SpeechRE_RPG_MoGe.

</details>


### [77] [Adversarial Attacks Against Automated Fact-Checking: A Survey](https://arxiv.org/abs/2509.08463)
*Fanzhen Liu,Alsharif Abuadbba,Kristen Moore,Surya Nepal,Cecile Paris,Jia Wu,Jian Yang,Quan Z. Sheng*

Main category: cs.CL

TL;DR: 本文综述了自动事实核查（AFC）系统面临的对抗性攻击，探讨了现有攻击方法、其对AFC模型的影响，以及防御策略，并总结了未来的研究方向与挑战。


<details>
  <summary>Details</summary>
Motivation: 在信息泛滥、虚假信息传播迅速的时代，自动事实核查系统对维护信息可靠性变得至关重要。然而，这些系统易受对抗性攻击，可能被用来误导决策者、曲解事实，因此亟需梳理和提升其鲁棒性。

Method: 本文以综述方式，系统整理并分类当前针对FC系统的对抗性攻击方法，评估其对自动事实核查系统的影响，同时回顾最新的对抗意识防御手段，并总结相关的开放性研究问题。

Result: 文章归纳了不同类型的对抗性攻击方法，对AFC系统鲁棒性的现状进行了评估。此外，还展现了最新的防御进展和当前系统在某些攻击情境下的不足。

Conclusion: 面对对抗性攻击的威胁，现有AFC系统仍存在较大提升空间，提高其鲁棒性的需求迫切。未来研究应聚焦于设计能有效应对多样化对抗样本的事实核查框架。

Abstract: In an era where misinformation spreads freely, fact-checking (FC) plays a
crucial role in verifying claims and promoting reliable information. While
automated fact-checking (AFC) has advanced significantly, existing systems
remain vulnerable to adversarial attacks that manipulate or generate claims,
evidence, or claim-evidence pairs. These attacks can distort the truth, mislead
decision-makers, and ultimately undermine the reliability of FC models. Despite
growing research interest in adversarial attacks against AFC systems, a
comprehensive, holistic overview of key challenges remains lacking. These
challenges include understanding attack strategies, assessing the resilience of
current models, and identifying ways to enhance robustness. This survey
provides the first in-depth review of adversarial attacks targeting FC,
categorizing existing attack methodologies and evaluating their impact on AFC
systems. Additionally, we examine recent advancements in adversary-aware
defenses and highlight open research questions that require further
exploration. Our findings underscore the urgent need for resilient FC
frameworks capable of withstanding adversarial manipulations in pursuit of
preserving high verification accuracy.

</details>


### [78] [Acquiescence Bias in Large Language Models](https://arxiv.org/abs/2509.08480)
*Daniel Braun*

Main category: cs.CL

TL;DR: 研究调查了大型语言模型（LLMs）是否像人类一样存在迎合性偏差，但结果发现LLMs反而更倾向于否定回答。


<details>
  <summary>Details</summary>
Motivation: 人类在问卷调查中普遍存在迎合性偏差，即倾向于同意陈述，而不一定表达真实想法。由于LLMs在输入微调下容易被影响，且训练于人类数据，作者怀疑LLMs可能也会表现出类似偏差。因此需验证这一假设。

Method: 作者对不同的大型语言模型（涵盖多种模型），在多种任务和三种语言（英语、德语、波兰语）下，系统测试它们是否存在迎合性偏差。方法包括以人类调查问卷常规方式构造输入，观察模型回答 ‘yes’ 或 ‘no’ 的倾向。

Result: 结果发现，与人类倾向于同意不同，LLMs在回答时总体上表现为更愿意选择‘no’，无论‘no’表示赞同还是反对陈述。

Conclusion: LLMs与人类似，但其响应表现出不同的系统性偏差；具体地，LLMs展现的是否定性偏差，而非人类的迎合（肯定）性偏差。这暗示未来使用LLMs处理调研或语言任务时需谨慎对待此类偏差。

Abstract: Acquiescence bias, i.e. the tendency of humans to agree with statements in
surveys, independent of their actual beliefs, is well researched and
documented. Since Large Language Models (LLMs) have been shown to be very
influenceable by relatively small changes in input and are trained on
human-generated data, it is reasonable to assume that they could show a similar
tendency. We present a study investigating the presence of acquiescence bias in
LLMs across different models, tasks, and languages (English, German, and
Polish). Our results indicate that, contrary to humans, LLMs display a bias
towards answering no, regardless of whether it indicates agreement or
disagreement.

</details>


### [79] [Simulating Identity, Propagating Bias: Abstraction and Stereotypes in LLM-Generated Text](https://arxiv.org/abs/2509.08484)
*Pia Sommerauer,Giulia Rambelli,Tommaso Caselli*

Main category: cs.CL

TL;DR: 本论文研究了在大型语言模型（LLM）中，使用persona-prompting（人格设定提示）是否会改变其语言抽象程度，进而影响对社会群体的刻板印象表达。结果显示，persona-prompting 在调整语言抽象水平方面存在局限，且存在加剧刻板印象传播的风险。


<details>
  <summary>Details</summary>
Motivation: 随着LLM中persona-prompting手法的流行，人们希望通过指定身份来引导模型模拟特定视角或语言风格。然而，这种做法对模型如何再现社会群体，尤其是在刻板印象方面的具体影响，还没有被充分揭示。

Method: 作者使用Linguistic Expectancy Bias理论框架，通过让六个开源LLM在三种不同提示条件下，生成与社会-人口类别相关带有或不带有刻板印象属性的短文本，比较11种persona设定和通用AI助理的输出。并构建了Self-Stereo这样一个Reddit用户自报刻板印象的数据集。通过具体性、特异性和否定这三个指标衡量语言的抽象程度。

Result: 实验发现persona-prompting在调节文本抽象层面能力有限。同时发现，尽管persona-prompting有时想用以表达弱势群体声音，实际却可能助长或维持对相关群体的刻板印象。

Conclusion: Persona-prompting对于调整模型输出中的抽象程度并不理想，代表性有限，甚至有继续传播和强化刻板印象的潜在风险。因此，在以persona为策略的LLM应用中需要格外谨慎，应警惕加剧社会偏见的问题。

Abstract: Persona-prompting is a growing strategy to steer LLMs toward simulating
particular perspectives or linguistic styles through the lens of a specified
identity. While this method is often used to personalize outputs, its impact on
how LLMs represent social groups remains underexplored. In this paper, we
investigate whether persona-prompting leads to different levels of linguistic
abstraction - an established marker of stereotyping - when generating short
texts linking socio-demographic categories with stereotypical or
non-stereotypical attributes. Drawing on the Linguistic Expectancy Bias
framework, we analyze outputs from six open-weight LLMs under three prompting
conditions, comparing 11 persona-driven responses to those of a generic AI
assistant. To support this analysis, we introduce Self-Stereo, a new dataset of
self-reported stereotypes from Reddit. We measure abstraction through three
metrics: concreteness, specificity, and negation. Our results highlight the
limits of persona-prompting in modulating abstraction in language, confirming
criticisms about the ecology of personas as representative of socio-demographic
groups and raising concerns about the risk of propagating stereotypes even when
seemingly evoking the voice of a marginalized group.

</details>


### [80] [Too Helpful, Too Harmless, Too Honest or Just Right?](https://arxiv.org/abs/2509.08486)
*Gautam Siddharth Kashyap,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 提出了一种名为TrinityX的LLM对齐新框架，通过将针对Helpfulness、Harmlessness和Honesty三维度分别训练的专家模块融合，实现了更优的整体对齐表现，且效率更高。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）虽然在多种NLP任务中表现良好，但让其输出同时符合有用性、无害性和诚实性（即HHH原则）依然很难。现有方法多只针对某一维度优化，结果常出现权衡与表现不稳的问题，亟需一种能兼顾三者且效率高的新方法。

Method: 提出TrinityX框架，结构核心是将Mixture-of-Experts（MoE）架构中的专家模块针对每个对齐维度（有用性、无害性、诚实性）单独训练，通过引入更精准的任务自适应路由算法（MoCaE），将多个专家的输出进行校准融合，生成统一且对齐意识更强的输出表征。

Result: 在Alpaca（有用性）、BeaverTails（无害性）、TruthfulQA（诚实性）三个标准基准上，TrinityX在胜率、安全分和诚实分上分别超过Baseline 32.5%、33.9%、28.4%。在内存和推理延迟上也比以往MoE方法节省40%以上。消融实验表明路由校准机制至关重要，跨模型评测也验证了方法的通用性。

Conclusion: TrinityX实现了在不牺牲效率的前提下，各对齐维度的整体大幅提升，为让LLM更好地满足实际需求、体现多维度对齐目标提供了有效方案。

Abstract: Large Language Models (LLMs) exhibit strong performance across a wide range
of NLP tasks, yet aligning their outputs with the principles of Helpfulness,
Harmlessness, and Honesty (HHH) remains a persistent challenge. Existing
methods often optimize for individual alignment dimensions in isolation,
leading to trade-offs and inconsistent behavior. While Mixture-of-Experts (MoE)
architectures offer modularity, they suffer from poorly calibrated routing,
limiting their effectiveness in alignment tasks. We propose TrinityX, a modular
alignment framework that incorporates a Mixture of Calibrated Experts (MoCaE)
within the Transformer architecture. TrinityX leverages separately trained
experts for each HHH dimension, integrating their outputs through a calibrated,
task-adaptive routing mechanism that combines expert signals into a unified,
alignment-aware representation. Extensive experiments on three standard
alignment benchmarks-Alpaca (Helpfulness), BeaverTails (Harmlessness), and
TruthfulQA (Honesty)-demonstrate that TrinityX outperforms strong baselines,
achieving relative improvements of 32.5% in win rate, 33.9% in safety score,
and 28.4% in truthfulness. In addition, TrinityX reduces memory usage and
inference latency by over 40% compared to prior MoE-based approaches. Ablation
studies highlight the importance of calibrated routing, and cross-model
evaluations confirm TrinityX's generalization across diverse LLM backbones.

</details>


### [81] [CM-Align: Consistency-based Multilingual Alignment for Large Language Models](https://arxiv.org/abs/2509.08541)
*Xue Zhang,Yunlong Liang,Fandong Meng,Songming Zhang,Yufeng Chen,Jinan Xu,Jie Zhou*

Main category: cs.CL

TL;DR: 本论文提出了CM-Align方法，通过一致性筛选机制，提升大语言模型多语言对齐性能，解决现有基于英文参考的偏见和低质量偏好数据问题。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在英语与其他语言的对齐表现存在巨大差距，常用的方法多依赖英文响应作为参考，但这些参考并不总高质量，并可能引入偏见，影响多语言对齐效果。需要一种更科学、客观的方法提升多语言数据质量和模型的对齐表现。

Method: 提出了一种基于一致性的多语言高质量偏好数据构建方法（CM-Align）：首先通过一致性引导筛选高质量英语参考响应，再基于跨语言一致性构建多语言偏好对，避免了人工启发式或有偏手段带来的噪声。

Result: 在三种LLM和三项常见任务上的实验结果显示，CM-Align方法能有效提升多语言对齐的效果，优于现有方法。

Conclusion: CM-Align通过高质量一致性偏好数据，有效解决了当前多语言对齐中的数据噪声和偏见问题，强调高质量偏好数据对多语言对齐的重要性。

Abstract: Current large language models (LLMs) generally show a significant performance
gap in alignment between English and other languages. To bridge this gap,
existing research typically leverages the model's responses in English as a
reference to select the best/worst responses in other languages, which are then
used for Direct Preference Optimization (DPO) training. However, we argue that
there are two limitations in the current methods that result in noisy
multilingual preference data and further limited alignment performance: 1) Not
all English responses are of high quality, and using a response with low
quality may mislead the alignment for other languages. 2) Current methods
usually use biased or heuristic approaches to construct multilingual preference
pairs. To address these limitations, we design a consistency-based data
selection method to construct high-quality multilingual preference data for
improving multilingual alignment (CM-Align). Specifically, our method includes
two parts: consistency-guided English reference selection and cross-lingual
consistency-based multilingual preference data construction. Experimental
results on three LLMs and three common tasks demonstrate the effectiveness and
superiority of our method, which further indicates the necessity of
constructing high-quality preference data.

</details>


### [82] [LLM Ensemble for RAG: Role of Context Length in Zero-Shot Question Answering for BioASQ Challenge](https://arxiv.org/abs/2509.08596)
*Dima Galat,Diego Molla-Aliod*

Main category: cs.CL

TL;DR: 本文提出了一种结合信息检索(IR)和多大语言模型(LLMs)集成的生物医学问答系统方法，实现无需微调或标注数据便达到甚至超越领域定制系统的效果。


<details>
  <summary>Details</summary>
Motivation: 生物医学问答系统需要高精度地理解和解释专业知识，然而现有方法常需耗费大量标注数据和昂贵的模型微调，且面临数据和领域知识快速变化的挑战。因此，探索零样本（zero-shot）大语言模型及其集成的潜能，对实现高性能且通用的生物医学问答具有重要意义。

Method: 采用多种主流大语言模型（如Anthropic、Google等）的零样本推理能力，构建模型集成，通过多个模型答案的聚合提升准确性与鲁棒性。基于BioASQ挑战任务进行评估，并分析了上下文长度设置对性能的影响，强调高质量信息检索对RAG（检索增强生成）体系的关键作用。

Result: 集成多个零样本大语言模型的方法，在BioASQ生物医学Yes/No问答任务中超越了单模型表现，并可媲美、甚至超越经过领域微调的系统。同时发现，过长的上下文虽带来更多证据，却可能稀释信息，使模型难以聚焦并降低表现。

Conclusion: 采用高效的信息检索流程和零样本大模型集成，能为检索增强的生物医学问答系统提供一种实用且易扩展的解决方案，为不依赖领域数据标注与微调的高性能域问答开辟新道路。

Abstract: Biomedical question answering (QA) poses significant challenges due to the
need for precise interpretation of specialized knowledge drawn from a vast,
complex, and rapidly evolving corpus. In this work, we explore how large
language models (LLMs) can be used for information retrieval (IR), and an
ensemble of zero-shot models can accomplish state-of-the-art performance on a
domain-specific Yes/No QA task. Evaluating our approach on the BioASQ challenge
tasks, we show that ensembles can outperform individual LLMs and in some cases
rival or surpass domain-tuned systems - all while preserving generalizability
and avoiding the need for costly fine-tuning or labeled data. Our method
aggregates outputs from multiple LLM variants, including models from Anthropic
and Google, to synthesize more accurate and robust answers. Moreover, our
investigation highlights a relationship between context length and performance:
while expanded contexts are meant to provide valuable evidence, they
simultaneously risk information dilution and model disorientation. These
findings emphasize IR as a critical foundation in Retrieval-Augmented
Generation (RAG) approaches for biomedical QA systems. Precise, focused
retrieval remains essential for ensuring LLMs operate within relevant
information boundaries when generating answers from retrieved documents. Our
results establish that ensemble-based zero-shot approaches, when paired with
effective RAG pipelines, constitute a practical and scalable alternative to
domain-tuned systems for biomedical question answering.

</details>


### [83] [Memorization in Large Language Models in Medicine: Prevalence, Characteristics, and Implications](https://arxiv.org/abs/2509.08604)
*Anran Li,Lingfei Qian,Mengmeng Du,Yu Yin,Yan Hu,Zihao Sun,Yihang Fu,Erica Stutz,Xuguang Ai,Qianqian Xie,Rui Zhu,Jimin Huang,Yifan Yang,Siru Liu,Yih-Chung Tham,Lucila Ohno-Machado,Hyunghoon Cho,Zhiyong Lu,Hua Xu,Qingyu Chen*

Main category: cs.CL

TL;DR: 本论文首次系统评估了大型语言模型（LLMs）在医学领域的记忆现象，发现记忆普遍存在，且可能影响医学应用的安全与效果。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs已广泛用于医学，但其在医学训练数据上的记忆能力及其后果尚不清楚。作者希望系统研究LLMs对医疗数据的记忆现象及其对实际医疗应用的影响。

Method: 系统分析了三种医学领域常见的LLM适应场景：1）继续在医学语料上预训练，2）在标准医学基准数据集上微调，3）在真实临床数据（如纽约耶鲁医院13000份住院记录）上微调，全面评估LLM的记忆频率、内容、量级及其影响。

Result: 结果显示，LLMs在所有医学适应场景中记忆现象普遍存在，而且远高于通用领域。记忆类型可分为有益（准确回忆医学知识）、无意义（重复模板性内容）、有害（泄露敏感或特定数据）。

Conclusion: 研究表明，LLMs在医学领域存在较强记忆性，这既有潜在益处也有隐患。建议通过措施促进有益记忆、减少无意义记忆，并严防有害记忆以保障数据安全与模型可靠性。

Abstract: Large Language Models (LLMs) have demonstrated significant potential in
medicine. To date, LLMs have been widely applied to tasks such as diagnostic
assistance, medical question answering, and clinical information synthesis.
However, a key open question remains: to what extent do LLMs memorize medical
training data. In this study, we present the first comprehensive evaluation of
memorization of LLMs in medicine, assessing its prevalence (how frequently it
occurs), characteristics (what is memorized), volume (how much content is
memorized), and potential downstream impacts (how memorization may affect
medical applications). We systematically analyze common adaptation scenarios:
(1) continued pretraining on medical corpora, (2) fine-tuning on standard
medical benchmarks, and (3) fine-tuning on real-world clinical data, including
over 13,000 unique inpatient records from Yale New Haven Health System. The
results demonstrate that memorization is prevalent across all adaptation
scenarios and significantly higher than reported in the general domain.
Memorization affects both the development and adoption of LLMs in medicine and
can be categorized into three types: beneficial (e.g., accurate recall of
clinical guidelines and biomedical references), uninformative (e.g., repeated
disclaimers or templated medical document language), and harmful (e.g.,
regeneration of dataset-specific or sensitive clinical content). Based on these
findings, we offer practical recommendations to facilitate beneficial
memorization that enhances domain-specific reasoning and factual accuracy,
minimize uninformative memorization to promote deeper learning beyond
surface-level patterns, and mitigate harmful memorization to prevent the
leakage of sensitive or identifiable patient information.

</details>


### [84] [OTESGN:Optimal Transport Enhanced Syntactic-Semantic Graph Networks for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2509.08612)
*Xinfeng Liao,Xuanqi Chen,Lianxi Wang,Jiahuan Yang,Zhuowei Chen,Ziying Rong*

Main category: cs.CL

TL;DR: 本文提出了一种结合句法和语义新方法OTESGN，提升了方面级情感分析的性能，尤其在处理复杂语义关系和噪声时表现优异。


<details>
  <summary>Details</summary>
Motivation: 以往的方面级情感分析方法难以捕捉复杂的非线性语义关系，并容易被无关词产生的噪声影响，导致对关键观点词的识别不精准。

Method: 提出了Optimal Transport Enhanced Syntactic-Semantic Graph Network (OTESGN)，其核心包括句法图感知注意力和语义优化传输注意力，用于分别挖掘句法结构和细粒度语义对齐，并通过自适应注意力融合模块结合两类特征，同时使用对比学习提升模型鲁棒性。

Result: OTESGN在Twitter和Laptop14基准上F1分别超越先前最佳模型1.01%和1.30%，并通过消融实验和可视化展示更好地定位观点词与抗噪能力。

Conclusion: 所提方法有效提升ABSA任务的性能，尤其在噪声环境和复杂语义关系建模方面优于传统模型。

Abstract: Aspect-based sentiment analysis (ABSA) aims to identify aspect terms and
determine their sentiment polarity. While dependency trees combined with
contextual semantics effectively identify aspect sentiment, existing methods
relying on syntax trees and aspect-aware attention struggle to model complex
semantic relationships. Their dependence on linear dot-product features fails
to capture nonlinear associations, allowing noisy similarity from irrelevant
words to obscure key opinion terms. Motivated by Differentiable Optimal
Matching, we propose the Optimal Transport Enhanced Syntactic-Semantic Graph
Network (OTESGN), which introduces a Syntactic-Semantic Collaborative
Attention. It comprises a Syntactic Graph-Aware Attention for mining latent
syntactic dependencies and modeling global syntactic topology, as well as a
Semantic Optimal Transport Attention designed to uncover fine-grained semantic
alignments amidst textual noise, thereby accurately capturing sentiment signals
obscured by irrelevant tokens. A Adaptive Attention Fusion module integrates
these heterogeneous features, and contrastive regularization further improves
robustness. Experiments demonstrate that OTESGN achieves state-of-the-art
results, outperforming previous best models by +1.01% F1 on Twitter and +1.30%
F1 on Laptop14 benchmarks. Ablative studies and visual analyses corroborate its
efficacy in precise localization of opinion words and noise resistance.

</details>


### [85] [X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates](https://arxiv.org/abs/2509.08729)
*Hyunjun Kim,Junwoo Ha,Sangyoon Yu,Haon Park*

Main category: cs.CL

TL;DR: 本文提出了一种自动化演化式多到单轮（M2S）prompt生成框架，通过进化优化提升单轮攻击的效果，在GPT-4.1上取得了44.8%的成功率，并发现prompt长度与表现正相关。


<details>
  <summary>Details</summary>
Motivation: 之前的M2S方法依赖少量手工模板，难以扩展或提高效果，因此作者提出自动进化和模型评判结合的新方法，以自动发现高效结构化模板。

Method: 提出X-Teaming Evolutionary M2S框架，结合12个采样源，利用大模型自动评判（参考StrongREJECT方法），设置成功阈值通过演化不断优化模板，完整记录实验日志。

Result: 经过5代演化获得了2个新模板家族，在GPT-4.1上44.8%的单轮攻击成功率。用2500次跨模型测试发现结构性收益可迁移但不同模型效果有差异，且模板长度越长得分越高。

Conclusion: 结构级自动搜索能稳定提升单轮prompt攻击效果，阈值设置和跨模型评估非常关键。所有实验代码开放。

Abstract: Multi-turn-to-single-turn (M2S) compresses iterative red-teaming into one
structured prompt, but prior work relied on a handful of manually written
templates. We present X-Teaming Evolutionary M2S, an automated framework that
discovers and optimizes M2S templates through language-model-guided evolution.
The system pairs smart sampling from 12 sources with an LLM-as-judge inspired
by StrongREJECT and records fully auditable logs.
  Maintaining selection pressure by setting the success threshold to $\theta =
0.70$, we obtain five evolutionary generations, two new template families, and
44.8% overall success (103/230) on GPT-4.1. A balanced cross-model panel of
2,500 trials (judge fixed) shows that structural gains transfer but vary by
target; two models score zero at the same threshold. We also find a positive
coupling between prompt length and score, motivating length-aware judging.
  Our results demonstrate that structure-level search is a reproducible route
to stronger single-turn probes and underscore the importance of threshold
calibration and cross-model evaluation. Code, configurations, and artifacts are
available at https://github.com/hyunjun1121/M2S-x-teaming.

</details>


### [86] [Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling](https://arxiv.org/abs/2509.08753)
*Neil Zeghidour,Eugene Kharitonov,Manu Orsini,Václav Volhejn,Gabriel de Marmiesse,Edouard Grave,Patrick Pérez,Laurent Mazaré,Alexandre Défossez*

Main category: cs.CL

TL;DR: 本文提出了延迟流建模（DSM）方法，通过在预处理阶段时间对齐流数据，引入适当延迟，实现了流式和多模态的序列到序列生成，在语音识别与语音合成等任务上达到了领先表现。


<details>
  <summary>Details</summary>
Motivation: 传统的序列到序列生成方法通常为离线式——需等所有输入读入后才开始生成输出，不能很好地支持流式、实时及多模态的数据场景。而现有流式方法又依赖复杂的策略学习，灵活性有限。因此，亟需一种既灵活又支持实时、多样流输入输出的新方法。

Method: DSM方法将输入与输出流在预处理阶段统一时间对齐，并通过对其中一个流施加延迟，使解码器仅需进行单向建模，即可实时生成输出。该方法灵活支持不同模态（如文本、音频）、任意长度的输入输出流。模型实现在自动语音识别（ASR，延迟文本流）和语音合成（TTS，延迟音频流）等任务中进行了实验。

Result: 实验表明，DSM方法在ASR与TTS等任务上实现了与传统离线方法媲美甚至更优的性能，且表现出低延迟与对超长序列良好的支持能力，达到了当前最先进水平。

Conclusion: DSM为流式、多模态的序列到序列学习提供了一种简单、有效、高效的统一建模方案，兼具高性能与低延迟，适合广泛的现实应用场景。

Abstract: We introduce Delayed Streams Modeling (DSM), a flexible formulation for
streaming, multimodal sequence-to-sequence learning. Sequence-to-sequence
generation is often cast in an offline manner, where the model consumes the
complete input sequence before generating the first output timestep.
Alternatively, streaming sequence-to-sequence rely on learning a policy for
choosing when to advance on the input stream, or write to the output stream.
DSM instead models already time-aligned streams with a decoder-only language
model. By moving the alignment to a pre-processing step,and introducing
appropriate delays between streams, DSM provides streaming inference of
arbitrary output sequences, from any input combination, making it applicable to
many sequence-to-sequence problems. In particular, given text and audio
streams, automatic speech recognition (ASR) corresponds to the text stream
being delayed, while the opposite gives a text-to-speech (TTS) model. We
perform extensive experiments for these two major sequence-to-sequence tasks,
showing that DSM provides state-of-the-art performance and latency while
supporting arbitrary long sequences, being even competitive with offline
baselines. Code, samples and demos are available at
https://github.com/kyutai-labs/delayed-streams-modeling

</details>


### [87] [Do All Autoregressive Transformers Remember Facts the Same Way? A Cross-Architecture Analysis of Recall Mechanisms](https://arxiv.org/abs/2509.08778)
*Minyeong Choe,Haehyun Cho,Changho Seo,Hyunil Kim*

Main category: cs.CL

TL;DR: 本文比较和分析了多种自回归Transformer模型中事实记忆的编码与提取方式，发现模型架构差异导致事实记忆机制存在本质区别。


<details>
  <summary>Details</summary>
Motivation: 以往研究发现，GPT类模型事实回忆主要依赖前几层的MLP模块。然而，尚不清楚这种结论能否推广至其他自回归Transformer架构，研究动机即在于系统探索不同模型中的事实记忆机制。

Method: 作者对GPT、LLaMA、Qwen和DeepSeek等多种模型进行实证评测，系统分析其事实信息在各层、各模块（注意力与MLP）中的编码和提取方式。

Result: 发现Qwen系列与先前模型表现不同，最早层的注意力模块而非MLP模块对事实回忆起主要作用。

Conclusion: 即便同属自回归Transformer家族，不同模型架构也可能导致事实记忆机制的根本性不同。

Abstract: Understanding how Transformer-based language models store and retrieve
factual associations is critical for improving interpretability and enabling
targeted model editing. Prior work, primarily on GPT-style models, has
identified MLP modules in early layers as key contributors to factual recall.
However, it remains unclear whether these findings generalize across different
autoregressive architectures. To address this, we conduct a comprehensive
evaluation of factual recall across several models -- including GPT, LLaMA,
Qwen, and DeepSeek -- analyzing where and how factual information is encoded
and accessed. Consequently, we find that Qwen-based models behave differently
from previous patterns: attention modules in the earliest layers contribute
more to factual recall than MLP modules. Our findings suggest that even within
the autoregressive Transformer family, architectural variations can lead to
fundamentally different mechanisms of factual recall.

</details>


### [88] [Evaluating LLMs Without Oracle Feedback: Agentic Annotation Evaluation Through Unsupervised Consistency Signals](https://arxiv.org/abs/2509.08809)
*Cheng Chen,Haiyan Yin,Ivor Tsang*

Main category: cs.CL

TL;DR: 本文提出了一种无监督环境下评估大型语言模型（LLM）自动标注质量的新范式，包括Agentic Annotation机制和CAI Ratio新指标。


<details>
  <summary>Details</summary>
Motivation: 在动态、无监督环境下，LLM自动生成的标注减少了人工成本，但缺乏高质量标注的评估手段，尤其是在没有权威反馈（oracle feedback）的情况下，现有方法无法有效评估标注质量。

Method: 提出一种代理标注（agentic annotation）范式，由一个学生模型与作为噪声教师的LLM协作。学生模型基于用户偏好使用多数投票策略评估LLM输出一致性。此外，提出CAI Ratio指标量化LLM标注的一致性和不一致性，实现无监督的标注质量评估及模型选择。

Result: 在四种LLM和十个开放领域NLP数据集上测试，CAI Ratio与LLM标注准确率呈现强正相关，能够有效反映模型表现。

Conclusion: CAI Ratio为无监督环境中LLM标注质量评估与模型选择提供了有效工具，有助于实际应用中的自动标注与模型选择。

Abstract: Large Language Models (LLMs), when paired with prompt-based tasks, have
significantly reduced data annotation costs and reliance on human annotators.
However, evaluating the quality of their annotations remains challenging in
dynamic, unsupervised environments where oracle feedback is scarce and
conventional methods fail. To address this challenge, we propose a novel
agentic annotation paradigm, where a student model collaborates with a noisy
teacher (the LLM) to assess and refine annotation quality without relying on
oracle feedback. The student model, acting as an unsupervised feedback
mechanism, employs a user preference-based majority voting strategy to evaluate
the consistency of the LLM outputs. To systematically measure the reliability
of LLM-generated annotations, we introduce the Consistent and Inconsistent
(CAI) Ratio, a novel unsupervised evaluation metric. The CAI Ratio not only
quantifies the annotation quality of the noisy teacher under limited user
preferences but also plays a critical role in model selection, enabling the
identification of robust LLMs in dynamic, unsupervised environments. Applied to
ten open-domain NLP datasets across four LLMs, the CAI Ratio demonstrates a
strong positive correlation with LLM accuracy, establishing it as an essential
tool for unsupervised evaluation and model selection in real-world settings.

</details>


### [89] [MoVoC: Morphology-Aware Subword Construction for Geez Script Languages](https://arxiv.org/abs/2509.08812)
*Hailay Kidu Teklehaymanot,Dren Fazlija,Wolfgang Nejdl*

Main category: cs.CL

TL;DR: 本文提出了一种新型分词方法MoVoC（基于词素感知的子词词表构建），针对Geez文字书写、形态学复杂且资源稀缺的语言，加强分词对形态学边界的保留。作者还公开了手工注释的词素数据集和工具。


<details>
  <summary>Details</summary>
Motivation: 现有基于子词的分词方法（如BPE）难以准确保留词内形态学边界，尤其是在形态结构复杂且资源稀缺的语言中，如Geez文字书写的语言。为了提升这些语言的分词质量和语言学保真度，有必要设计更形态学敏感的分词方法。

Method: 作者提出MoVoC，将基于词素的分析与传统的BPE方法结合，利用带监督的形态学分析生成的词素信息，训练融合词素和BPE子词的MoVoC-Tok分词器。此外，作者还人工标注并公开了四种Geez文字语言的词素数据集，并为其中两种语言制作了基于形态学的词表。

Result: 虽然MoVoC在自动翻译指标上提升有限，但在词素分割相关的内在评价指标（如MorphoScore与Boundary Precision）上取得了稳定提升，说明新方法在形态保真度和token利用效率上有优势。

Conclusion: 形态学感知分词有助于提升形态信息保留和分词效率，尤其适合处理资源稀缺、形态复杂的语言。公开的数据集与工具有助于相关语言的后续研究。

Abstract: Subword-based tokenization methods often fail to preserve morphological
boundaries, a limitation especially pronounced in low-resource, morphologically
complex languages such as those written in the Geez script. To address this, we
present MoVoC (Morpheme-aware Subword Vocabulary Construction) and train
MoVoC-Tok, a tokenizer that integrates supervised morphological analysis into
the subword vocabulary. This hybrid segmentation approach combines
morpheme-based and Byte Pair Encoding (BPE) tokens to preserve morphological
integrity while maintaining lexical meaning. To tackle resource scarcity, we
curate and release manually annotated morpheme data for four Geez script
languages and a morpheme-aware vocabulary for two of them. While the proposed
tokenization method does not lead to significant gains in automatic translation
quality, we observe consistent improvements in intrinsic metrics, MorphoScore,
and Boundary Precision, highlighting the value of morphology-aware segmentation
in enhancing linguistic fidelity and token efficiency. Our morpheme-annotated
datasets and tokenizer will be publicly available to support further research
in low-resource, morphologically rich languages. Our code and data are
available on GitHub: https://github.com/hailaykidu/MoVoC

</details>


### [90] [Building High-Quality Datasets for Portuguese LLMs: From Common Crawl Snapshots to Industrial-Grade Corpora](https://arxiv.org/abs/2509.08824)
*Thales Sales Almeida,Rodrigo Nogueira,Helio Pedrini*

Main category: cs.CL

TL;DR: 本文提出了构建葡萄牙语大语言模型训练语料库的有效方法，并验证了其可提升模型性能，对多语言模型开发提供了参考。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的训练数据主要集中在英语，其他语言的高质量训练数据构建方法研究不足。为实现更好的多语言模型效果，需要探索适合其他语言的训练语料构建流程。

Method: 作者提出了一套可扩展的网络文本语料库构建方法，并以葡萄牙语为例，构建了一个120B tokens的新语料库。采用持续预训练的方式，分析了不同数据筛选与预处理策略（如STEM领域和有害内容的过滤）对将原英语训练模型转为葡萄牙语的影响。

Result: 用新构建的葡萄牙语语料训练出的模型，性能与工业级语料库相当。实验证明了语言特定的过滤流程和针对性内容筛选（如STEM及有害内容剔除）的重要性。

Conclusion: 将LLM从英语迁移到目标语言时，采用高质量、定制化的语料能显著提升性能。这一方法虽以葡萄牙语为例，但可推广到其他语言，为多语种大模型开发提供了有益思路。

Abstract: The performance of large language models (LLMs) is deeply influenced by the
quality and composition of their training data. While much of the existing work
has centered on English, there remains a gap in understanding how to construct
effective training corpora for other languages. We explore scalable methods for
building web-based corpora for LLMs. We apply them to build a new 120B token
corpus in Portuguese that achieves competitive results to an industrial-grade
corpus. Using a continual pretraining setup, we study how different data
selection and preprocessing strategies affect LLM performance when
transitioning a model originally trained in English to another language. Our
findings demonstrate the value of language-specific filtering pipelines,
including classifiers for education, science, technology, engineering, and
mathematics (STEM), as well as toxic content. We show that adapting a model to
the target language leads to performance improvements, reinforcing the
importance of high-quality, language-specific data. While our case study
focuses on Portuguese, our methods are applicable to other languages, offering
insights for multilingual LLM development.

</details>


### [91] [Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation](https://arxiv.org/abs/2509.08825)
*Joachim Baumann,Paul Röttger,Aleksandra Urman,Albert Wendsjö,Flor Miriam Plaza-del-Arco,Johannes B. Gruber,Dirk Hovy*

Main category: cs.CL

TL;DR: 本论文指出大型语言模型（LLMs）在社会科学中的应用虽然可以极大提高自动化水平，但研究者的实现选择会极大影响结果，导致随之产生系统性偏差和错误，被称为LLM hacking。实验证明，即使是最先进模型，LLM hacking现象也难以完全避免。


<details>
  <summary>Details</summary>
Motivation: LLM在社会科学中承担了大量数据标注和文本分析任务，极大提升了研究效率。然而，LLM的输出高度依赖于模型选择、提示策略和参数设置，研究者主观操作空间大，可能引入错误和偏见，对研究结论带来威胁，需要系统量化这一风险。

Method: 作者复现了21篇已发表社会科学论文的37个数据标注任务，采用18个不同语言模型，分析了约1,300万个标注，总计测试2,361个假设，以客观量化LLM实现选择对统计分析结论的影响，并评估不同风险缓解方法。

Result: 最先进LLM在三分之一的假设中会因设定差异导致错误结论，小模型则达一半。模型表现越好、能力越强，风险降低，但无法完全消除。效应量越大，风险越低。人类标注显著减少了错误发现，提升了模型选择精度，而常见的回归校正技术对LLM hacking作用有限。

Conclusion: LLM hacking是不可忽视的风险，尤其是在显著性边界附近的结论需严密核查。人类标注仍然不可完全替代，回归校正手段不够有效。故，无论偶然还是蓄意，LLM hacking都可能导致显著性结论被错误呈现，需研究者格外警惕。

Abstract: Large language models (LLMs) are rapidly transforming social science research
by enabling the automation of labor-intensive tasks like data annotation and
text analysis. However, LLM outputs vary significantly depending on the
implementation choices made by researchers (e.g., model selection, prompting
strategy, or temperature settings). Such variation can introduce systematic
biases and random errors, which propagate to downstream analyses and cause Type
I, Type II, Type S, or Type M errors. We call this LLM hacking.
  We quantify the risk of LLM hacking by replicating 37 data annotation tasks
from 21 published social science research studies with 18 different models.
Analyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure
how plausible researcher choices affect statistical conclusions. We find
incorrect conclusions based on LLM-annotated data in approximately one in three
hypotheses for state-of-the-art models, and in half the hypotheses for small
language models. While our findings show that higher task performance and
better general model capabilities reduce LLM hacking risk, even highly accurate
models do not completely eliminate it. The risk of LLM hacking decreases as
effect sizes increase, indicating the need for more rigorous verification of
findings near significance thresholds. Our extensive analysis of LLM hacking
mitigation techniques emphasizes the importance of human annotations in
reducing false positive findings and improving model selection. Surprisingly,
common regression estimator correction techniques are largely ineffective in
reducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors.
  Beyond accidental errors, we find that intentional LLM hacking is
unacceptably simple. With few LLMs and just a handful of prompt paraphrases,
anything can be presented as statistically significant.

</details>


### [92] [A Survey of Reinforcement Learning for Large Reasoning Models](https://arxiv.org/abs/2509.08827)
*Kaiyan Zhang,Yuxin Zuo,Bingxiang He,Youbang Sun,Runze Liu,Che Jiang,Yuchen Fan,Kai Tian,Guoli Jia,Pengfei Li,Yu Fu,Xingtai Lv,Yuchen Zhang,Sihang Zeng,Shang Qu,Haozhan Li,Shijie Wang,Yuru Wang,Xinwei Long,Fangfu Liu,Xiang Xu,Jiaze Ma,Xuekai Zhu,Ermo Hua,Yihao Liu,Zonglin Li,Huayu Chen,Xiaoye Qu,Yafu Li,Weize Chen,Zhenzhao Yuan,Junqi Gao,Dong Li,Zhiyuan Ma,Ganqu Cui,Zhiyuan Liu,Biqing Qi,Ning Ding,Bowen Zhou*

Main category: cs.CL

TL;DR: 本文综述了利用强化学习（RL）提升大语言模型（LLMs）推理能力的最新进展，分析了现阶段面临的机遇与挑战，并提出了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在数学、编程等复杂逻辑任务上的应用日益广泛，强化学习成为提升其推理能力的重要途径。该领域进展迅速，但面临着算法、数据和资源等多方面的挑战，因此需要系统性回顾和总结以指导未来发展。

Method: 本文回顾了自DeepSeek-R1以来，将RL应用于LLM和LLM推理模型（LRM）的研究成果，包括基础组件、关键问题、训练资源以及实际下游应用等，并就如何提升RL在LRM发展中的可扩展性进行了探讨。

Result: 通过梳理相关研究，本文系统总结了RL在提升LLMs推理能力过程中的贡献及存在的局限，明确了面临的资源和算法挑战，并提出了潜在的发展机遇。

Conclusion: 强化学习已经成为推动LLMs向更强推理智能（如ASI）发展的核心方法之一，但要实现大规模应用和更高智能，还需解决训练资源、算法设计等基础性问题。本文的综述有助于加深对该领域的理解并推动后续研究。

Abstract: In this paper, we survey recent advances in Reinforcement Learning (RL) for
reasoning with Large Language Models (LLMs). RL has achieved remarkable success
in advancing the frontier of LLM capabilities, particularly in addressing
complex logical tasks such as mathematics and coding. As a result, RL has
emerged as a foundational methodology for transforming LLMs into LRMs. With the
rapid progress of the field, further scaling of RL for LRMs now faces
foundational challenges not only in computational resources but also in
algorithm design, training data, and infrastructure. To this end, it is timely
to revisit the development of this domain, reassess its trajectory, and explore
strategies to enhance the scalability of RL toward Artificial SuperIntelligence
(ASI). In particular, we examine research applying RL to LLMs and LRMs for
reasoning abilities, especially since the release of DeepSeek-R1, including
foundational components, core problems, training resources, and downstream
applications, to identify future opportunities and directions for this rapidly
evolving area. We hope this review will promote future research on RL for
broader reasoning models. Github:
https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [93] [A Novel Theoretical Approach on Micro-Nano Robotic Networks Based on Density Matrices and Swarm Quantum Mechanics](https://arxiv.org/abs/2509.08002)
*Maria Mannone,Mahathi Anand,Peppino Fazio,Abdalla Swikir*

Main category: cs.RO

TL;DR: 该论文提出用量子混态和密度矩阵来描述机器人群体，且密度矩阵的规模与机器人数量无关。


<details>
  <summary>Details</summary>
Motivation: 传统上，机器人群体的参数如位置、与目标的距离等可以用概率幅表示。研究者们受此启发，最近开始尝试用量子方法定义和描述机器人群体，包括分块矩阵表示。但现有方法在处理规模时存在限制。

Method: 本文提出将机器人群体定义为量子混合态，并用密度矩阵进行描述，这种方法能够使描述所需的矩阵规模不随机器人数量的增加而增加。

Result: 验证了该量子混合态方法的可行性，并证明其密度矩阵表达的优势，使得大规模群体控制更具可扩展性。

Conclusion: 采用密度矩阵方法能够有效地描述机器人群体，为后续研究（例如群体控制与量子机器学习结合等）提供了新的理论基础，并展望了未来的研究方向。

Abstract: In a robotic swarm, parameters such as position and proximity to the target
can be described in terms of probability amplitudes. This idea led to recent
studies on a quantum approach to the definition of the swarm, including a
block-matrix representation. Here, we propose an advancement of the idea,
defining a swarm as a mixed quantum state, to be described with a density
matrix, whose size does not change with the number of robots. We end the
article with some directions for future research.

</details>


### [94] [PySensors 2.0: A Python Package for Sparse Sensor Placement](https://arxiv.org/abs/2509.08017)
*Niharika Karnik,Yash Bhangale,Mohammad G. Abdo,Andrei A. Klishin,Joshua J. Cogliati,Bingni W. Brunton,J. Nathan Kutz,Steven L. Brunton,Krithika Manohar*

Main category: cs.RO

TL;DR: PySensors软件进行了重大更新，支持空间约束的传感器布置、噪声不确定性分析、热力学多解法和自定义基底等功能，提升了重建与分类任务的适应性和实用性。


<details>
  <summary>Details</summary>
Motivation: 在实际传感器布置问题中，常需满足空间约束（例如限定数量、区域分布、距离）并兼顾数据噪声与实际部署需求。原有解决方案不灵活且难以反映多种候选布局的优劣，缺乏鲁棒性和适应新型任务的能力。

Method: 作者在PySensors中增加了空间区域约束（如区域内数量限制、预设点、最小距离）、自定义基底输入和传感器多解热力学视角，并引入了带正则化的最小二乘优化以及噪声敏感性分析与可视化热力图，辅助实际布置和后续替换分析。

Result: 新版本支持用户对传感器布点施加多种复杂空间约束，允许接入任意数据驱动或谱方法基底，能够全景展示可能的传感器相互作用及其替换敏感性，提升了鲁棒性与灵活性。增加的噪声不确定性分析为部署提供决策支持。

Conclusion: 该更新使PySensors更好地适应现实传感器布置需求（如区域限制、传感器备选及误差分析）。其综合算法与可视化工具能指导更科学、实用的传感器配置选择，有望推广到更多传感领域应用，并为未来功能拓展奠定基础。

Abstract: PySensors is a Python package for selecting and placing a sparse set of
sensors for reconstruction and classification tasks. In this major update to
\texttt{PySensors}, we introduce spatially constrained sensor placement
capabilities, allowing users to enforce constraints such as maximum or exact
sensor counts in specific regions, incorporate predetermined sensor locations,
and maintain minimum distances between sensors. We extend functionality to
support custom basis inputs, enabling integration of any data-driven or
spectral basis. We also propose a thermodynamic approach that goes beyond a
single ``optimal'' sensor configuration and maps the complete landscape of
sensor interactions induced by the training data. This comprehensive view
facilitates integration with external selection criteria and enables assessment
of sensor replacement impacts. The new optimization technique also accounts for
over- and under-sampling of sensors, utilizing a regularized least squares
approach for robust reconstruction. Additionally, we incorporate noise-induced
uncertainty quantification of the estimation error and provide visual
uncertainty heat maps to guide deployment decisions. To highlight these
additions, we provide a brief description of the mathematical algorithms and
theory underlying these new capabilities. We demonstrate the usage of new
features with illustrative code examples and include practical advice for
implementation across various application domains. Finally, we outline a
roadmap of potential extensions to further enhance the package's functionality
and applicability to emerging sensing challenges.

</details>


### [95] [SVN-ICP: Uncertainty Estimation of ICP-based LiDAR Odometry using Stein Variational Newton](https://arxiv.org/abs/2509.08069)
*Shiping Ma,Haoming Zhang,Marc Toussaint*

Main category: cs.RO

TL;DR: 本文提出了SVN-ICP算法，一种基于Stein变分牛顿法的ICP配准方法，能够提供不确定性估计，适用于多传感器融合中的激光雷达里程计。此外方法省去了手动噪声建模和调参。实验验证了其准确性和不确定性估计的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的ICP算法在多传感器融合中通常需要手动建模噪声和微调参数，且在激光雷达信号质量差的环境下表现不佳。因此，提出需要一种能自动估计不确定性、提高健壮性的配准方法。

Method: 提出SVN-ICP算法：在流形空间上结合Stein变分推断，用粒子逼近后验分布，从而隐式建模配准噪声；并与IMU信息结合，在误差状态卡尔曼滤波框架下实现激光雷达里程计融合。

Result: 该方法在多种环境和机器人平台的多个公开或自建数据集上进行了实验，结果表明SVN-ICP在困难场景下精度优于当前最优方法，且能给出一致且可信的不确定性评估。

Conclusion: SVN-ICP能有效提升多传感器系统中激光雷达配准的准确性和健壮性，自动实现噪声建模与不确定性推断，为激光雷达受损环境下的定位与地图构建提供了更优解法。

Abstract: This letter introduces SVN-ICP, a novel Iterative Closest Point (ICP)
algorithm with uncertainty estimation that leverages Stein Variational Newton
(SVN) on manifold. Designed specifically for fusing LiDAR odometry in
multisensor systems, the proposed method ensures accurate pose estimation and
consistent noise parameter inference, even in LiDAR-degraded environments. By
approximating the posterior distribution using particles within the Stein
Variational Inference framework, SVN-ICP eliminates the need for explicit noise
modeling or manual parameter tuning. To evaluate its effectiveness, we
integrate SVN-ICP into a simple error-state Kalman filter alongside an IMU and
test it across multiple datasets spanning diverse environments and robot types.
Extensive experimental results demonstrate that our approach outperforms
best-in-class methods on challenging scenarios while providing reliable
uncertainty estimates.

</details>


### [96] [Real-Time Obstacle Avoidance for a Mobile Robot Using CNN-Based Sensor Fusion](https://arxiv.org/abs/2509.08095)
*Lamiaa H. Zain,Raafat E. Shalaby*

Main category: cs.RO

TL;DR: 本文提出并比较了三种基于卷积神经网络（CNN）的端到端模型，用于移动机器人在复杂、未知环境中的实时避障。实验结果表明NetConEmb模型表现最佳，NetEmb模型参数更少但性能相近，NetConEmb在实际导航中表现出最强的鲁棒性与通用性。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在复杂、未知环境中高效安全避障是自主导航的关键难题，现有方法在泛化能力与实时性上存在不足，因此需要研究高效、可靠的端到端避障方法。

Method: 研究设计并离线训练、评测了三种CNN架构（NetConEmb、NetEmb、NetGated），输入为Intel RealSense D415摄像头同步采集的彩色与深度图像，模型输出低层直接控制的转向指令。

Result: 离线评估表明，NetConEmb模型的性能最好，MedAE仅为0.58×10^-3 rad/s；NetEmb参数量减少25%，但RMSE与NetConEmb接近，分别为21.68×10^-3 与21.42×10^-3 rad/s。实际机器人实验中，NetConEmb在已知和未知环境中均可100%成功导航，NetEmb和NetGated只在已知环境成功。

Conclusion: NetConEmb架构不仅在离线评估中表现出色，而且在真实环境下具备最强的鲁棒性和适应性，证明其能有效支持移动机器人在多变环境中的端到端避障导航。

Abstract: Obstacle avoidance is a critical component of the navigation stack required
for mobile robots to operate effectively in complex and unknown environments.
In this research, three end-to-end Convolutional Neural Networks (CNNs) were
trained and evaluated offline and deployed on a differential-drive mobile robot
for real-time obstacle avoidance to generate low-level steering commands from
synchronized color and depth images acquired by an Intel RealSense D415 RGB-D
camera in diverse environments. Offline evaluation showed that the NetConEmb
model achieved the best performance with a notably low MedAE of $0.58 \times
10^{-3}$ rad/s. In comparison, the lighter NetEmb architecture adopted in this
study, which reduces the number of trainable parameters by approximately 25\%
and converges faster, produced comparable results with an RMSE of $21.68 \times
10^{-3}$ rad/s, close to the $21.42 \times 10^{-3}$ rad/s obtained by
NetConEmb. Real-time navigation further confirmed NetConEmb's robustness,
achieving a 100\% success rate in both known and unknown environments, while
NetEmb and NetGated succeeded only in navigating the known environment.

</details>


### [97] [Online Learning and Coverage of Unknown Fields Using Random-Feature Gaussian Processes](https://arxiv.org/abs/2509.08117)
*Ruijie Du,Ruoyu Lin,Yanning Shen,Magnus Egerstedt*

Main category: cs.RO

TL;DR: 本文提出了一种支持多机器人同时学习和覆盖未知且可能随时间变化区域的方法框架。结合了高斯过程随机特征近似及其在线变体、Voronoi划分和UCB采样，实现了高效自适应覆盖，理论分析和仿真、实验证明了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人在覆盖未知或动态区域时，如何高效学习区域密度分布，并自适应调整覆盖方案，提高效率和学习精度，克服传统GP回归在大规模和在线场景下的局限。

Method: 1. 采用随机特征高斯过程（RFGP）及其在线版本（O-RFGP）实现在线、增量式密度学习；2. 与Voronoi基覆盖控制相结合，实现机器人区域划分与分工；3. 利用UCB采样策略，使机器人侧重信息量更丰富的区域；4. 在时变和时不变环境下进行了理论分析、仿真和实物实验。

Result: 在理论上证明了方法的有效性和收敛性。仿真和实物实验表明，该框架能高效适应环境密度的动态变化，并在学习精度和覆盖效率方面优于传统方法。

Conclusion: 文中提出的多机器人联合学习与覆盖框架能自适应应对时变和未知密度环境，具有良好的理论支持和实际效果，可广泛应用于机器人环境监测等场景。

Abstract: This paper proposes a framework for multi-robot systems to perform
simultaneous learning and coverage of the domain of interest characterized by
an unknown and potentially time-varying density function. To overcome the
limitations of Gaussian Process (GP) regression, we employ Random Feature GP
(RFGP) and its online variant (O-RFGP) that enables online and incremental
inference. By integrating these with Voronoi-based coverage control and Upper
Confidence Bound (UCB) sampling strategy, a team of robots can adaptively focus
on important regions while refining the learned spatial field for efficient
coverage. Under mild assumptions, we provide theoretical guarantees and
evaluate the framework through simulations in time-invariant scenarios.
Furthermore, its effectiveness in time-varying settings is demonstrated through
additional simulations and a physical experiment.

</details>


### [98] [Attribute-based Object Grounding and Robot Grasp Detection with Spatial Reasoning](https://arxiv.org/abs/2509.08126)
*Houjian Yu,Zheming Zhou,Min Sun,Omid Ghasemalizadeh,Yuyin Sun,Cheng-Hao Kuo,Arnie Sen,Changhyun Choi*

Main category: cs.RO

TL;DR: OGRG是一个新颖的框架，能够根据开放型自然语言指令识别和抓取目标物体，支持重复物品、多场景，并且显著提高了定位和抓取的表现。


<details>
  <summary>Details</summary>
Motivation: 当前机器人基于自然语言抓取对象仍有诸多挑战，特别是在应对开放语言表达、多重目标以及昂贵、繁琐的人工标注方面，现有方法表现有限。

Method: 提出了属性驱动的目标定位和机器人抓取（OGRG）框架，包括双向视觉-语言融合模块并结合深度信息实现空间推理。OGRG在两种设置下进行实验：（1）有像素级全监督标注的抓取合成；（2）仅用单点抓取弱监督。

Result: OGRG在多样化桌面场景下，相比强基线方法表现更优，在RGS任务（全监督）下支持高帧率实时操作，准确性领先。在RGA任务（弱监督）下，无论在仿真还是实际机器人实验中均使抓取成功率超越基线。

Conclusion: OGRG显著提升了机器人在复杂语言指令和重复目标下的抓取能力，验证了其空间推理机制的有效性，在实际和仿真环境中均有良好表现，具备实际部署前景。

Abstract: Enabling robots to grasp objects specified through natural language is
essential for effective human-robot interaction, yet it remains a significant
challenge. Existing approaches often struggle with open-form language
expressions and typically assume unambiguous target objects without duplicates.
Moreover, they frequently rely on costly, dense pixel-wise annotations for both
object grounding and grasp configuration. We present Attribute-based Object
Grounding and Robotic Grasping (OGRG), a novel framework that interprets
open-form language expressions and performs spatial reasoning to ground target
objects and predict planar grasp poses, even in scenes containing duplicated
object instances. We investigate OGRG in two settings: (1) Referring Grasp
Synthesis (RGS) under pixel-wise full supervision, and (2) Referring Grasp
Affordance (RGA) using weakly supervised learning with only single-pixel grasp
annotations. Key contributions include a bi-directional vision-language fusion
module and the integration of depth information to enhance geometric reasoning,
improving both grounding and grasping performance. Experiment results show that
OGRG outperforms strong baselines in tabletop scenes with diverse spatial
language instructions. In RGS, it operates at 17.59 FPS on a single NVIDIA RTX
2080 Ti GPU, enabling potential use in closed-loop or multi-object sequential
grasping, while delivering superior grounding and grasp prediction accuracy
compared to all the baselines considered. Under the weakly supervised RGA
setting, OGRG also surpasses baseline grasp-success rates in both simulation
and real-robot trials, underscoring the effectiveness of its spatial reasoning
design. Project page: https://z.umn.edu/ogrg

</details>


### [99] [Mean Field Game-Based Interactive Trajectory Planning Using Physics-Inspired Unified Potential Fields](https://arxiv.org/abs/2509.08147)
*Zhen Tian,Fujiang Yuan,Chunhong Yuan,Yanhong Peng*

Main category: cs.RO

TL;DR: 提出了一种称为 IUPF 的新型交互式轨迹规划方法，能高效平衡安全、效率与多样驾驶风格。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶轨迹规划方法要么计算量大，要么过度依赖额外的安全机制，难以兼顾多样化驾驶行为下的高效安全规划。

Method: 引入了基于平均场博弈理论、变分模型的互动势场 (IUPF) 框架，将风格相关的收益与风险场融合，通过随机微分方程实现纳什均衡和指数收敛，无需单独的安全模块即可应对不同驾驶风格。

Result: 在变道与超车场景下，IUPF 方法能保持安全距离，生成平滑高效的轨迹，且在适应性和计算效率上都优于传统优化及博弈方法。

Conclusion: IUPF 实现了无需外部安全模块下的安全高效多风格轨迹规划，为自动驾驶交互决策提供了有效新范式。

Abstract: Interactive trajectory planning in autonomous driving must balance safety,
efficiency, and scalability under heterogeneous driving behaviors. Existing
methods often face high computational cost or rely on external safety critics.
To address this, we propose an Interaction-Enriched Unified Potential Field
(IUPF) framework that fuses style-dependent benefit and risk fields through a
physics-inspired variational model, grounded in mean field game theory. The
approach captures conservative, aggressive, and cooperative behaviors without
additional safety modules, and employs stochastic differential equations to
guarantee Nash equilibrium with exponential convergence. Simulations on lane
changing and overtaking scenarios show that IUPF ensures safe distances,
generates smooth and efficient trajectories, and outperforms traditional
optimization and game-theoretic baselines in both adaptability and
computational efficiency.

</details>


### [100] [Risk-Bounded Multi-Agent Visual Navigation via Dynamic Budget Allocation](https://arxiv.org/abs/2509.08157)
*Viraj Parimi,Brian C. Williams*

Main category: cs.RO

TL;DR: 该论文提出了一种新的多智能体路径规划方法RB-CBS，通过动态分配和调整用户设定的风险约束，实现了在保证总体安全的情况下提升任务效率。


<details>
  <summary>Details</summary>
Motivation: 传统导航方法虽然能解决长时任务，但依赖于预先定义的距离度量；安全强化学习虽能处理高维输入，却难以应对多智能体目标条件场景。已有方法基于中间图剪枝策略过于保守，不适合必须穿越高风险区域的任务，限制了任务效率。

Method: 提出RB-CBS算法，该方法在CBS方案基础上扩展，对每个智能体分配本地风险预算，通过动态调整风险，使整体风险在用户设定的Δ范围内，实现灵活的安全-效率权衡。

Result: 实验结果显示，RB-CBS能够在复杂环境下，为多个智能体在用户设定的风险限制下，找到无碰撞且更高效的路径，性能优于传统方法。

Conclusion: RB-CBS通过灵活的风险分配机制，在保证总体安全约束的前提下，显著提高了多智能体系统的路径规划效率，适用于需要平衡安全和效率的实际场景。

Abstract: Safe navigation is essential for autonomous systems operating in hazardous
environments, especially when multiple agents must coordinate using just visual
inputs over extended time horizons. Traditional planning methods excel at
solving long-horizon tasks but rely on predefined distance metrics, while safe
Reinforcement Learning (RL) can learn complex behaviors using high-dimensional
inputs yet struggles with multi-agent, goal-conditioned scenarios. Recent work
combined these paradigms by leveraging goal-conditioned RL (GCRL) to build an
intermediate graph from replay buffer states, pruning unsafe edges, and using
Conflict-Based Search (CBS) for multi-agent path planning. Although effective,
this graph-pruning approach can be overly conservative, limiting mission
efficiency by precluding missions that must traverse high-risk regions. To
address this limitation, we propose RB-CBS, a novel extension to CBS that
dynamically allocates and adjusts user-specified risk bound ($\Delta$) across
agents to flexibly trade off safety and speed. Our improved planner ensures
that each agent receives a local risk budget ($\delta$) enabling more efficient
navigation while still respecting overall safety constraints. Experimental
results demonstrate that this iterative risk-allocation framework yields
superior performance in complex environments, allowing multiple agents to find
collision-free paths within the user-specified $\Delta$.

</details>


### [101] [Zero-Shot Metric Depth Estimation via Monocular Visual-Inertial Rescaling for Autonomous Aerial Navigation](https://arxiv.org/abs/2509.08159)
*Steven Yang,Xiaoyu Tian,Kshitij Goel,Wennie Tabib*

Main category: cs.RO

TL;DR: 本论文提出了一种通过单目RGB图像和惯性测量单元（IMU）预测绝对深度的方法，实现轻量级的无人机避障。该方法无需重型传感器或大量领域特定微调，采用稀疏三维特征图对相对深度进行零样本重缩放。最优策略在实际四旋翼上部署并实现15Hz深度估计，成功集成至运动规划器完成避障。


<details>
  <summary>Details</summary>
Motivation: 传统无人机避障依赖高成本、重型传感器（如LiDAR、双目摄像头）或对单目深度估计模型的大量数据和特定领域微调，导致系统复杂度和负载提升。本文旨在探索无需重型传感器和复杂微调的轻量化深度估计方法，为算力受限的无人机提供实用的避障能力。

Method: 提出多种轻量级零样本重缩放方法，将视觉-惯性导航系统生成的稀疏3D特征点图用于相对深度转绝对深度，重点比较不同重缩放策略效果。最佳策略采用单调样条拟合，并在真实四旋翼无人机上运行，每秒可估计15次深度。

Result: 在多种仿真环境下对比各重缩放方案，单调样条拟合法表现最佳。实际部署在四旋翼无人机后，系统可实时获得准确的绝对深度估计，并集成至运动规划器，实现了避障能力。

Conclusion: 通过视觉-惯性稀疏3D特征图与零样本深度重缩放策略，无需重型传感器或高成本微调，即可实现小型无人机实时、可靠的绝对深度感知和避障。该方法对轻量级、算力受限的自主飞行平台具有实际应用价值。

Abstract: This paper presents a methodology to predict metric depth from monocular RGB
images and an inertial measurement unit (IMU). To enable collision avoidance
during autonomous flight, prior works either leverage heavy sensors (e.g.,
LiDARs or stereo cameras) or data-intensive and domain-specific fine-tuning of
monocular metric depth estimation methods. In contrast, we propose several
lightweight zero-shot rescaling strategies to obtain metric depth from relative
depth estimates via the sparse 3D feature map created using a visual-inertial
navigation system. These strategies are compared for their accuracy in diverse
simulation environments. The best performing approach, which leverages
monotonic spline fitting, is deployed in the real-world on a
compute-constrained quadrotor. We obtain on-board metric depth estimates at 15
Hz and demonstrate successful collision avoidance after integrating the
proposed method with a motion primitives-based planner.

</details>


### [102] [Diffusion-Guided Multi-Arm Motion Planning](https://arxiv.org/abs/2509.08160)
*Viraj Parimi,Brian C. Williams*

Main category: cs.RO

TL;DR: 提出了一种基于扩散模型的多机械臂运动规划新方法（DG-MAP），通过结构化分解和专用生成模型提升在多机械臂任务中的扩展性与实用性。


<details>
  <summary>Details</summary>
Motivation: 现有多机械臂规划方法在任务规模变大时，状态空间指数级膨胀，且依赖大规模的数据集，导致难以扩展和实用。

Method: 受到多智能体路径规划（MAPF）思想启发，将多臂问题分解为单臂规划和成对碰撞解决。训练两个条件扩散模型：一个生成可行的单臂轨迹，另一个建模双臂间的动态以解决碰撞问题，并将这两个模型融入MAPF式结构分解流程。

Result: 与其他学习型方法相比，在不同规模团队的实验证明该方法对多机械臂规划问题具有更好的扩展性和实际效果。

Conclusion: 结构化分解结合专用生成模型有效缓解了数据需求与规模扩展问题，展示了在大规模多机械臂任务中的实际可用性。

Abstract: Multi-arm motion planning is fundamental for enabling arms to complete
complex long-horizon tasks in shared spaces efficiently but current methods
struggle with scalability due to exponential state-space growth and reliance on
large training datasets for learned models. Inspired by Multi-Agent Path
Finding (MAPF), which decomposes planning into single-agent problems coupled
with collision resolution, we propose a novel diffusion-guided multi-arm
planner (DG-MAP) that enhances scalability of learning-based models while
reducing their reliance on massive multi-arm datasets. Recognizing that
collisions are primarily pairwise, we train two conditional diffusion models,
one to generate feasible single-arm trajectories, and a second, to model the
dual-arm dynamics required for effective pairwise collision resolution. By
integrating these specialized generative models within a MAPF-inspired
structured decomposition, our planner efficiently scales to larger number of
arms. Evaluations against alternative learning-based methods across various
team sizes demonstrate our method's effectiveness and practical applicability.
Project website can be found at https://diff-mapf-mers.csail.mit.edu

</details>


### [103] [Quadrotor Navigation using Reinforcement Learning with Privileged Information](https://arxiv.org/abs/2509.08177)
*Jonathan Lee,Abhishek Rathod,Kshitij Goel,John Stecklein,Wennie Tabib*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习的无人机（四旋翼）导航方法，利用可微仿真、创新损失函数和特权信息有效绕过大障碍物，并在真实和仿真环境中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有学习型导航方法在通过狭窄障碍物时表现良好，但面对大墙或复杂地形时易受阻，亟需提升无人机应对大障碍能力。

Method: 方法核心通过使用到达时间（ToA）图作为特权信息，并设计新的偏航对齐损失，引导无人机避开大障碍物。算法先在具备大障碍、锐角和死胡同的高仿真环境中进行训练评测。

Result: 所提方法在仿真环境中导航成功率达到86%，比基线方法高34%。实机部署于户外复杂环境（白天和夜晚）共20次飞行，总行程589米，最高速度4米/秒，未发生碰撞。

Conclusion: 新的强化学习导航方法大大提升了无人机绕大障碍的能力，在仿真和现实复杂环境中均优于现有方法，表现出较高的安全性和鲁棒性。

Abstract: This paper presents a reinforcement learning-based quadrotor navigation
method that leverages efficient differentiable simulation, novel loss
functions, and privileged information to navigate around large obstacles. Prior
learning-based methods perform well in scenes that exhibit narrow obstacles,
but struggle when the goal location is blocked by large walls or terrain. In
contrast, the proposed method utilizes time-of-arrival (ToA) maps as privileged
information and a yaw alignment loss to guide the robot around large obstacles.
The policy is evaluated in photo-realistic simulation environments containing
large obstacles, sharp corners, and dead-ends. Our approach achieves an 86%
success rate and outperforms baseline strategies by 34%. We deploy the policy
onboard a custom quadrotor in outdoor cluttered environments both during the
day and night. The policy is validated across 20 flights, covering 589 meters
without collisions at speeds up to 4 m/s.

</details>


### [104] [Online Dynamic SLAM with Incremental Smoothing and Mapping](https://arxiv.org/abs/2509.08197)
*Jesse Morris,Yiduo Wang,Viorela Ila*

Main category: cs.RO

TL;DR: 本文提出了一种适用于在线应用的高效动态SLAM系统，其精准度与现有最先进方法持平或更优，但计算速度提升5倍。


<details>
  <summary>Details</summary>
Motivation: 现有动态SLAM方法虽准确，但计算开销大，不适用于实时或在线场景。因此，亟需一种既高效又能兼顾准确性的动态SLAM方案。

Method: 该研究首次将增量优化技术应用于动态SLAM，提出了一种新的因子图建模方式和系统架构，从而能够充分利用增量优化方法，实现在线估计。

Result: 在多个数据集上验证，提出方法在相机位姿与目标运动估计上至少与SOTA方法相当，某些情境下表现更优，同时系统运行效率提高5倍。

Conclusion: 该方法结构适合增量求解器，系统设计进一步提升了系统性能，是动态SLAM领域实现高效在线处理的重要进展。

Abstract: Dynamic SLAM methods jointly estimate for the static and dynamic scene
components, however existing approaches, while accurate, are computationally
expensive and unsuitable for online applications. In this work, we present the
first application of incremental optimisation techniques to Dynamic SLAM. We
introduce a novel factor-graph formulation and system architecture designed to
take advantage of existing incremental optimisation methods and support online
estimation. On multiple datasets, we demonstrate that our method achieves equal
to or better than state-of-the-art in camera pose and object motion accuracy.
We further analyse the structural properties of our approach to demonstrate its
scalability and provide insight regarding the challenges of solving Dynamic
SLAM incrementally. Finally, we show that our formulation results in problem
structure well-suited to incremental solvers, while our system architecture
further enhances performance, achieving a 5x speed-up over existing methods.

</details>


### [105] [A Comprehensive Review of Reinforcement Learning for Autonomous Driving in the CARLA Simulator](https://arxiv.org/abs/2509.08221)
*Elahe Delavari,Feeza Khan Khanzada,Jaerock Kwon*

Main category: cs.RO

TL;DR: 本文综述了基于深度强化学习（RL）在自动驾驶CARLA仿真环境中的最新研究，分类分析现有文献，梳理评测方法，总结挑战，并提出未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶研究的发展，深度强化学习成为一个有前景的数据驱动决策框架，但尚缺乏关于其应用和评测标准的系统梳理，限制了领域的进一步推进。

Method: 作者系统性地分析了约100篇在CARLA仿真器中训练、测试或验证RL策略的论文。分类方法包括模型无关、模型相关、分层和混合算法，并统计了各类方法的应用比例。同时，分析了不同的状态、动作、奖励设计和主要评测指标，并汇总了仿真场景。

Result: 数据显示，目前超过80%的研究仍采用模型无关方法（如DQN、PPO、SAC）。文献在传感器输入、控制抽象和奖励塑造等方面存在多样性。评测以成功率、碰撞率、车道偏离和驾驶分数最为常用。文献存在稀疏奖励、现实迁移、安全保障和值行为多样性不足等难题。

Conclusion: 本综述构建了统一的领域分类与评测框架，总结挑战并指明未来如模型相关RL、元学习、多智能体仿真等方向，对新入门者和领域进展均具参考价值。

Abstract: Autonomous-driving research has recently embraced deep Reinforcement Learning
(RL) as a promising framework for data-driven decision making, yet a clear
picture of how these algorithms are currently employed, benchmarked and
evaluated is still missing. This survey fills that gap by systematically
analysing around 100 peer-reviewed papers that train, test or validate RL
policies inside the open-source CARLA simulator. We first categorize the
literature by algorithmic family model-free, model-based, hierarchical, and
hybrid and quantify their prevalence, highlighting that more than 80% of
existing studies still rely on model-free methods such as DQN, PPO and SAC.
Next, we explain the diverse state, action and reward formulations adopted
across works, illustrating how choices of sensor modality (RGB, LiDAR, BEV,
semantic maps, and carla kinematics states), control abstraction (discrete vs.
continuous) and reward shaping are used across various literature. We also
consolidate the evaluation landscape by listing the most common metrics
(success rate, collision rate, lane deviation, driving score) and the towns,
scenarios and traffic configurations used in CARLA benchmarks. Persistent
challenges including sparse rewards, sim-to-real transfer, safety guarantees
and limited behaviour diversity are distilled into a set of open research
questions, and promising directions such as model-based RL, meta-learning and
richer multi-agent simulations are outlined. By providing a unified taxonomy,
quantitative statistics and a critical discussion of limitations, this review
aims to serve both as a reference for newcomers and as a roadmap for advancing
RL-based autonomous driving toward real-world deployment.

</details>


### [106] [Input-gated Bilateral Teleoperation: An Easy-to-implement Force Feedback Teleoperation Method for Low-cost Hardware](https://arxiv.org/abs/2509.08226)
*Yoshiki Kanai,Akira Kanazawa,Hideyuki Ichiwara,Hiroshi Ito,Naoaki Noguchi,Tetsuya Ogata*

Main category: cs.RO

TL;DR: 本文提出了一种无需力传感器、依赖简单反馈控制器的双边遥操作方法，可以在低成本硬件上实现力反馈，提升接触操作数据采集的有效性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 接触丰富操作任务中，准确的接触感知对于稳定控制至关重要，但现有双边遥操作系统复杂且难以实现，尤其在经济硬件方案上。作者希望通过降低复杂度与成本，扩大遥操作系统的应用范围。

Method: 提出了一种基于简单反馈控制器的双边遥操作架构，完全不依赖力传感器，适用于低成本硬件，采用leader-follower系统结构，参数调节极少。该方法通过数值仿真和实验在商业低价硬件上进行了验证。

Result: 该方法实现了高操作性和接触稳定性，优于传统方法。在主从端低通讯周期下，控制性能退化极小，展现出高度鲁棒性。在两种商业低成本硬件上零参数调整即可运行，显示出极高的可实现性和通用性。

Conclusion: 新方法为低成本硬件上的力反馈遥操作系统的推广和应用提供了新思路，有助于推动模仿学习中接触丰富任务的自主化发展。

Abstract: Effective data collection in contact-rich manipulation requires force
feedback during teleoperation, as accurate perception of contact is crucial for
stable control. However, such technology remains uncommon, largely because
bilateral teleoperation systems are complex and difficult to implement. To
overcome this, we propose a bilateral teleoperation method that relies only on
a simple feedback controller and does not require force sensors. The approach
is designed for leader-follower setups using low-cost hardware, making it
broadly applicable. Through numerical simulations and real-world experiments,
we demonstrate that the method requires minimal parameter tuning, yet achieves
both high operability and contact stability, outperforming conventional
approaches. Furthermore, we show its high robustness: even at low communication
cycle rates between leader and follower, control performance degradation is
minimal compared to high-speed operation. We also prove our method can be
implemented on two types of commercially available low-cost hardware with zero
parameter adjustments. This highlights its high ease of implementation and
versatility. We expect this method will expand the use of force feedback
teleoperation systems on low-cost hardware. This will contribute to advancing
contact-rich task autonomy in imitation learning.

</details>


### [107] [Deep Visual Odometry for Stereo Event Cameras](https://arxiv.org/abs/2509.08235)
*Sheng Zhong,Junkai Niu,Yi Zhou*

Main category: cs.RO

TL;DR: 本文提出了一种基于深度学习的立体事件视觉里程计系统（Stereo-DEVO），能够在高动态范围、夜间等复杂条件下，实时、高精度地估计运动轨迹。


<details>
  <summary>Details</summary>
Motivation: 传统事件相机视觉里程计方法依赖手工特征关联，在动态范围大、低光照且信噪比变化大的场景（如野外机器人应用中）表现不可靠。深度学习方法有望提升鲁棒性并适应复杂环境。

Method: 提出了一种新颖且高效的静态立体关联策略，用于稀疏深度估计，并与紧耦合的束束调整优化结合。利用递归网络和基于体素的事件表示进行光流估计和补丁关联，从而实现精准的运动估计。系统可处理VGA分辨率的事件流并实时运行。

Result: 通过在多个公开数据集和自采集数据上的广泛评测，系统在精度与鲁棒性上均优于现有事件视觉里程计方法，且可在大规模夜间高动态范围场景中实现稳定的位姿估计。

Conclusion: Stereo-DEVO兼具实时性、高精度和良好适应性，为事件相机在实际复杂环境和机器人领域应用提供了有效解决方案，表现出显著优势。

Abstract: Event-based cameras are bio-inspired sensors with pixels that independently
and asynchronously respond to brightness changes at microsecond resolution,
offering the potential to handle state estimation tasks involving motion blur
and high dynamic range (HDR) illumination conditions. However, the versatility
of event-based visual odometry (VO) relying on handcrafted data association
(either direct or indirect methods) is still unreliable, especially in field
robot applications under low-light HDR conditions, where the dynamic range can
be enormous and the signal-to-noise ratio is spatially-and-temporally varying.
Leveraging deep neural networks offers new possibilities for overcoming these
challenges. In this paper, we propose a learning-based stereo event visual
odometry. Building upon Deep Event Visual Odometry (DEVO), our system (called
Stereo-DEVO) introduces a novel and efficient static-stereo association
strategy for sparse depth estimation with almost no additional computational
burden. By integrating it into a tightly coupled bundle adjustment (BA)
optimization scheme, and benefiting from the recurrent network's ability to
perform accurate optical flow estimation through voxel-based event
representations to establish reliable patch associations, our system achieves
high-precision pose estimation in metric scale. In contrast to the offline
performance of DEVO, our system can process event data of \zs{Video Graphics
Array} (VGA) resolution in real time. Extensive evaluations on multiple public
real-world datasets and self-collected data justify our system's versatility,
demonstrating superior performance compared to state-of-the-art event-based VO
methods. More importantly, our system achieves stable pose estimation even in
large-scale nighttime HDR scenarios.

</details>


### [108] [Sample-Efficient Online Control Policy Learning with Real-Time Recursive Model Updates](https://arxiv.org/abs/2509.08241)
*Zixin Zhang,James Avtges,Todd D. Murphey*

Main category: cs.RO

TL;DR: 提出了一种名为Recursive Koopman Learning (RKL)的新型数据驱动学习方法，实现了高效、实时的非线性系统控制建模，显著提升了样本利用率和运算速度。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动控制方法对数据需求大、运算速度慢，难以满足实际硬件终端有限数据和算力情况下的实时学习需求，因此需要一种更高效、轻量且能实时更新的建模与控制方法。

Method: 基于Koopman理论，将非线性系统以观测变量的线性模型表示，提出递归式Koopman学习（RKL）方法。该方法在算法设计上保证了收敛性、轻量级和与数据集大小无关的复杂度，并能实现在线快速更新。

Result: 在模拟的二维机械臂和具备软执行器的非线性混合硬件系统上实验，RKL只需<10%的数据量即可达到甚至超越基准方法的表现，实现了高效的样本利用率和系统稳定性。

Conclusion: RKL实现了样本高效、实时、轻量级的数据驱动控制新方案，适用于动态和资源受限场景，并通过开源代码推动实际应用和学术发展。

Abstract: Data-driven control methods need to be sample-efficient and lightweight,
especially when data acquisition and computational resources are limited --
such as during learning on hardware. Most modern data-driven methods require
large datasets and struggle with real-time updates of models, limiting their
performance in dynamic environments. Koopman theory formally represents
nonlinear systems as linear models over observables, and Koopman
representations can be determined from data in an optimization-friendly setting
with potentially rapid model updates. In this paper, we present a highly
sample-efficient, Koopman-based learning pipeline: Recursive Koopman Learning
(RKL). We identify sufficient conditions for model convergence and provide
formal algorithmic analysis supporting our claim that RKL is lightweight and
fast, with complexity independent of dataset size. We validate our method on a
simulated planar two-link arm and a hybrid nonlinear hardware system with soft
actuators, showing that real-time recursive Koopman model updates improve the
sample efficiency and stability of data-driven controller synthesis --
requiring only <10% of the data compared to benchmarks. The high-performance
C++ codebase is open-sourced. Website:
https://www.zixinatom990.com/home/robotics/corl-2025-recursive-koopman-learning.

</details>


### [109] [Behaviorally Heterogeneous Multi-Agent Exploration Using Distributed Task Allocation](https://arxiv.org/abs/2509.08242)
*Nirabhra Mandal,Aamodh Suresh,Carlos Nieto-Granda,Sonia Martínez*

Main category: cs.RO

TL;DR: 本文提出了一种面向多机器人异质行为的探索任务分配方法，通过行为熵（BE）评价探索前沿点，利用分布式任务分配算法（d-PBRAG）以博弈论方式实现最优分工，并在仿真中验证了团队异质性对探索效率的正面影响。


<details>
  <summary>Details</summary>
Motivation: 多机器人探索未知环境时，不同机器人往往具有异质性（如传感能力、行为策略等），如何高效分配各自的探索任务并发挥异质性的优势，提升整体探索效率，是实际应用中的一大挑战。本研究意在通过行为熵度量和博弈论建模，实现更合理的任务分工。

Method: 每台机器人首先使用SLAM构建地图，识别感兴趣区域（AoIs）或前沿点。通过行为熵（BE）量化探索收益，之后将任务分配视为非合作博弈，提出分布式算法d-PBRAG求解纳什均衡，实现最优任务分配。对于未知收益情形，引入近似奖励得到稳健界限。最终在仿真平台上考察了不同传感半径、准确度及异质性设置下的探索效率表现。

Result: 实验结果显示：提出的d-PBRAG算法通信开销低、收敛快，对不同异质性和传感参数下的探索时间和路径长度均有良好表现。尤其，具有异质性的机器人团队能显著提升整体探索效率。

Conclusion: 在多机器人异质性探索场景下，基于行为熵和博弈论的分布式任务分配方法有效提高了探索效率，团队异质性是促进群体协作探索能力的重要因素。

Abstract: We study a problem of multi-agent exploration with behaviorally heterogeneous
robots. Each robot maps its surroundings using SLAM and identifies a set of
areas of interest (AoIs) or frontiers that are the most informative to explore
next. The robots assess the utility of going to a frontier using Behavioral
Entropy (BE) and then determine which frontier to go to via a distributed task
assignment scheme. We convert the task assignment problem into a
non-cooperative game and use a distributed algorithm (d-PBRAG) to converge to
the Nash equilibrium (which we show is the optimal task allocation solution).
For unknown utility cases, we provide robust bounds using approximate rewards.
We test our algorithm (which has less communication cost and fast convergence)
in simulation, where we explore the effect of sensing radii, sensing accuracy,
and heterogeneity among robotic teams with respect to the time taken to
complete exploration and path traveled. We observe that having a team of agents
with heterogeneous behaviors is beneficial.

</details>


### [110] [Symmetry-Guided Multi-Agent Inverse Reinforcement Learnin](https://arxiv.org/abs/2509.08257)
*Yongkai Tian,Yirong Qi,Xin Yu,Wenjun Wu,Jie Luo*

Main category: cs.RO

TL;DR: 本文解决了多机器人系统中逆向强化学习（IRL）对专家示范依赖量大，样本效率低的问题，提出通过引入系统对称性提升样本效率，并在实际机器人系统中验证有效。


<details>
  <summary>Details</summary>
Motivation: IRL能够通过模仿专家找回奖励函数，但需要大量昂贵的专家示范，尤其是在多机器人系统中，极大限制了实际应用。因此，提高多主体IRL的样本效率成为亟需解决的挑战。

Method: 理论分析表明，多主体系统存在的对称性有助于奖励函数的准确恢复。基于此，提出了将对称性嵌入现有多主体对抗IRL算法的通用框架，以提升样本效率。

Result: 在多个具有挑战性的仿真任务中，所提框架显著提升了样本效率，并通过物理多机器人系统实验验证了方法的实用性。

Conclusion: 将对称性引入多主体IRL不仅在仿真中效果突出，也在真实机器人中具备良好应用前景，有效缓解了专家数据匮乏的难题。

Abstract: In robotic systems, the performance of reinforcement learning depends on the
rationality of predefined reward functions. However, manually designed reward
functions often lead to policy failures due to inaccuracies. Inverse
Reinforcement Learning (IRL) addresses this problem by inferring implicit
reward functions from expert demonstrations. Nevertheless, existing methods
rely heavily on large amounts of expert demonstrations to accurately recover
the reward function. The high cost of collecting expert demonstrations in
robotic applications, particularly in multi-robot systems, severely hinders the
practical deployment of IRL. Consequently, improving sample efficiency has
emerged as a critical challenge in multi-agent inverse reinforcement learning
(MIRL). Inspired by the symmetry inherent in multi-agent systems, this work
theoretically demonstrates that leveraging symmetry enables the recovery of
more accurate reward functions. Building upon this insight, we propose a
universal framework that integrates symmetry into existing multi-agent
adversarial IRL algorithms, thereby significantly enhancing sample efficiency.
Experimental results from multiple challenging tasks have demonstrated the
effectiveness of this framework. Further validation in physical multi-robot
systems has shown the practicality of our method.

</details>


### [111] [Foundation Models for Autonomous Driving Perception: A Survey Through Core Capabilities](https://arxiv.org/abs/2509.08302)
*Rajendramayavan Sathyam,Yueqi Li*

Main category: cs.RO

TL;DR: 本文综述了基础模型在自动驾驶感知领域的最新进展，提出以能力为核心的划分体系，总结目前面临的挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习不断发展，传统的专用模型面临泛化、可扩展性和对分布变化的鲁棒性等问题。基础模型以其通用性和大规模训练数据，有望克服这些瓶颈。

Method: 文章构建了一个基于四大核心能力（泛化知识、空间理解、多传感器鲁棒性、时序推理）的分类体系，系统梳理每种能力的重要性及相应前沿方法，并以能力驱动而非方法驱动解析各类模型方案。

Result: 对现有基础模型在四大能力上的方法进行了系统归纳，并总结其在实际部署、计算需求、模型可靠性等方面的不足，尤其是如何在动态驾驶环境中融合这些能力。

Conclusion: 基础模型有潜力提升自动驾驶系统的泛化和鲁棒性，但仍需解决实时性、可扩展性、计算资源和模型可靠性等挑战。文章提出了未来的研究方向，如提升模型可靠性和对分布外样本的适应能力，以确保其安全高效落地。

Abstract: Foundation models are revolutionizing autonomous driving perception,
transitioning the field from narrow, task-specific deep learning models to
versatile, general-purpose architectures trained on vast, diverse datasets.
This survey examines how these models address critical challenges in autonomous
perception, including limitations in generalization, scalability, and
robustness to distributional shifts. The survey introduces a novel taxonomy
structured around four essential capabilities for robust performance in dynamic
driving environments: generalized knowledge, spatial understanding,
multi-sensor robustness, and temporal reasoning. For each capability, the
survey elucidates its significance and comprehensively reviews cutting-edge
approaches. Diverging from traditional method-centric surveys, our unique
framework prioritizes conceptual design principles, providing a
capability-driven guide for model development and clearer insights into
foundational aspects. We conclude by discussing key challenges, particularly
those associated with the integration of these capabilities into real-time,
scalable systems, and broader deployment challenges related to computational
demands and ensuring model reliability against issues like hallucinations and
out-of-distribution failures. The survey also outlines crucial future research
directions to enable the safe and effective deployment of foundation models in
autonomous driving systems.

</details>


### [112] [Good Deep Features to Track: Self-Supervised Feature Extraction and Tracking in Visual Odometry](https://arxiv.org/abs/2509.08333)
*Sai Puneeth Reddy Gottam,Haoming Zhang,Eivydas Keras*

Main category: cs.RO

TL;DR: 本文提出了一种通过自监督学习增强深度特征提取与跟踪的新方法，从而提升视觉定位在复杂环境下的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉定位方法在大规模、户外、长期场景下常因光照变化、动态场景及低纹理区域而性能下降，主要问题出在特征提取与跟踪环节。此外，尽管一些基于学习的方法如SuperPoint和SuperGlue增强了特征表现，但仍存在泛化性不足的问题。

Method: 本文通过引入基于任务反馈的自监督学习方法，强化深度网络在特征提取与跟踪上的表现。具体地，方法设计促进了特征更加稳定且具有信息性，从而提升在多变环境下的适应能力。

Result: 该方法在具有挑战性的环境（如光照变化、动态、低纹理区域）中展示了更优的泛化能力和更强的鲁棒性，能够更可靠地完成特征提取和追踪任务。

Conclusion: 引入基于任务反馈的自监督特征学习，有效提升了视觉定位系统在复杂环境下的适用性和鲁棒性，是强化基于学习方法视觉定位系统的重要途径。

Abstract: Visual-based localization has made significant progress, yet its performance
often drops in large-scale, outdoor, and long-term settings due to factors like
lighting changes, dynamic scenes, and low-texture areas. These challenges
degrade feature extraction and tracking, which are critical for accurate motion
estimation. While learning-based methods such as SuperPoint and SuperGlue show
improved feature coverage and robustness, they still face generalization issues
with out-of-distribution data. We address this by enhancing deep feature
extraction and tracking through self-supervised learning with task specific
feedback. Our method promotes stable and informative features, improving
generalization and reliability in challenging environments.

</details>


### [113] [Grasp Like Humans: Learning Generalizable Multi-Fingered Grasping from Human Proprioceptive Sensorimotor Integration](https://arxiv.org/abs/2509.08354)
*Ce Guo,Xieyuanli Chen,Zhiwen Zeng,Zirui Guo,Yihong Li,Haoran Xiao,Dewen Hu,Huimin Lu*

Main category: cs.RO

TL;DR: 本文提出了一种基于手套的触觉-运动感知预测框架，实现从人类自然操作到机器人模仿学习的抓取技能迁移。验证了其在多种抓取任务、包括可变形物体抓取上的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前，尽管机器人能够获取触觉和运动反馈，但如何将这些感知信息直接映射为有效的动作仍是难题。人类依靠身体感知进行精妙操作，因此借鉴人类自然抓取过程有望提升机器手的仿生能力。

Method: 1）利用兼容人类和机器人手的手套，采集关节层级的触觉与运动数据，标准化数据格式，便于人机两端数据比对。2）基于图结构与极坐标统一多模态感知表示，显式整合手型差异。3）提出TK-STGN模型，用多维子图卷积与注意力LSTM提取时空特征，预测每个关节状态，最终通过力-位混合映射实现动作命令输出。

Result: 通过在人类及机器手不同场景下的数据采集和性能验证，实验表明该方法在抓取及操纵多样对象（包括可变形物体）上均取得了良好效果，验证了技能迁移方法的有效性和泛化能力。

Conclusion: 该研究实现了基于手套采集与多模态感知统一表示的人机抓取技能迁移，提高了机器人在复杂任务中的适应性和操作能力。所提出的TK-STGN模型与数据表示方法为未来高精度机器人操作提供了新思路。

Abstract: Tactile and kinesthetic perceptions are crucial for human dexterous
manipulation, enabling reliable grasping of objects via proprioceptive
sensorimotor integration. For robotic hands, even though acquiring such tactile
and kinesthetic feedback is feasible, establishing a direct mapping from this
sensory feedback to motor actions remains challenging. In this paper, we
propose a novel glove-mediated tactile-kinematic perception-prediction
framework for grasp skill transfer from human intuitive and natural operation
to robotic execution based on imitation learning, and its effectiveness is
validated through generalized grasping tasks, including those involving
deformable objects. Firstly, we integrate a data glove to capture tactile and
kinesthetic data at the joint level. The glove is adaptable for both human and
robotic hands, allowing data collection from natural human hand demonstrations
across different scenarios. It ensures consistency in the raw data format,
enabling evaluation of grasping for both human and robotic hands. Secondly, we
establish a unified representation of multi-modal inputs based on graph
structures with polar coordinates. We explicitly integrate the morphological
differences into the designed representation, enhancing the compatibility
across different demonstrators and robotic hands. Furthermore, we introduce the
Tactile-Kinesthetic Spatio-Temporal Graph Networks (TK-STGN), which leverage
multidimensional subgraph convolutions and attention-based LSTM layers to
extract spatio-temporal features from graph inputs to predict node-based states
for each hand joint. These predictions are then mapped to final commands
through a force-position hybrid mapping.

</details>


### [114] [PegasusFlow: Parallel Rolling-Denoising Score Sampling for Robot Diffusion Planner Flow Matching](https://arxiv.org/abs/2509.08435)
*Lei Ye,Haibo Gao,Peng Xu,Zhelin Zhang,Junqi Shan,Ao Zhang,Wei Zhang,Ruyi Zhou,Zongquan Deng,Liang Ding*

Main category: cs.RO

TL;DR: PegasusFlow提出了一种无需专家示范的新型扩散模型框架，能高效生成机器人轨迹，改进了轨迹规划的速度和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在机器人轨迹规划中依赖模仿学习，需要大量专家数据，且对稀缺数据的特种机器人不适用。现有训练流程效率低且理论上次优。

Method: 提出了PegasusFlow体系，通过层次滚动去噪框架，使用Weighted Basis Function Optimization (WBFO)采样算法，以样条基表示轨迹，支持高效并行采样，无需专家数据。框架内建异步并行仿真架构，以支持大规模数据采集。

Result: 在轨迹优化和导航任务上的实验显示，PegasusFlow特别是结合强化学习预热的AVWBFO，明显优于基线算法。在难度较高的障碍通行任务中，成功率100%，速度比第二名提升18%。

Conclusion: PegasusFlow有效解决了机器人轨迹规划中对专家数据的依赖，提升了采样效率和性能，在复杂地形运动规划中表现优异，展现出广泛应用前景。

Abstract: Diffusion models offer powerful generative capabilities for robot trajectory
planning, yet their practical deployment on robots is hindered by a critical
bottleneck: a reliance on imitation learning from expert demonstrations. This
paradigm is often impractical for specialized robots where data is scarce and
creates an inefficient, theoretically suboptimal training pipeline. To overcome
this, we introduce PegasusFlow, a hierarchical rolling-denoising framework that
enables direct and parallel sampling of trajectory score gradients from
environmental interaction, completely bypassing the need for expert data. Our
core innovation is a novel sampling algorithm, Weighted Basis Function
Optimization (WBFO), which leverages spline basis representations to achieve
superior sample efficiency and faster convergence compared to traditional
methods like MPPI. The framework is embedded within a scalable, asynchronous
parallel simulation architecture that supports massively parallel rollouts for
efficient data collection. Extensive experiments on trajectory optimization and
robotic navigation tasks demonstrate that our approach, particularly
Action-Value WBFO (AVWBFO) combined with a reinforcement learning warm-start,
significantly outperforms baselines. In a challenging barrier-crossing task,
our method achieved a 100% success rate and was 18% faster than the next-best
method, validating its effectiveness for complex terrain locomotion planning.
https://masteryip.github.io/pegasusflow.github.io/

</details>


### [115] [Augmenting Neural Networks-based Model Approximators in Robotic Force-tracking Tasks](https://arxiv.org/abs/2509.08440)
*Kevin Saad,Vincenzo Petrone,Enrico Ferrentino,Pasquale Chiacchio,Francesco Braghin,Loris Roveda*

Main category: cs.RO

TL;DR: 本文提出了一种基于神经网络的新型机器人交互控制策略，通过预测接触力并结合机械臂切向速度信息，提升了力跟踪性能，在仿真实验中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 机器人在复杂环境中进行精确力控制越来越重要，但传统方法通常需要大量调参或对环境有较强的先验知识，实际应用受到限制。

Method: 设计了一种新控制方法VAICAM，利用一组前馈神经网络预测接触力，结合机械臂切向速度，然后通过优化问题生成残余动作，最终叠加到直接力控制（DFC）输出，并作用于阻抗控制器。

Result: 在Gazebo仿真平台和Franka Emika Panda机器人上，对多种轨迹进行了测试，VAICAM在所有任务中均优于两个基线控制器。

Conclusion: VAICAM结合了神经网络预测和运动学信息，无需大量调参与环境知识，就可以显著提升机器人力跟踪性能，具备较好的应用前景。

Abstract: As robotics gains popularity, interaction control becomes crucial for
ensuring force tracking in manipulator-based tasks. Typically, traditional
interaction controllers either require extensive tuning, or demand expert
knowledge of the environment, which is often impractical in real-world
applications. This work proposes a novel control strategy leveraging Neural
Networks (NNs) to enhance the force-tracking behavior of a Direct Force
Controller (DFC). Unlike similar previous approaches, it accounts for the
manipulator's tangential velocity, a critical factor in force exertion,
especially during fast motions. The method employs an ensemble of feedforward
NNs to predict contact forces, then exploits the prediction to solve an
optimization problem and generate an optimal residual action, which is added to
the DFC output and applied to an impedance controller. The proposed
Velocity-augmented Artificial intelligence Interaction Controller for Ambiguous
Models (VAICAM) is validated in the Gazebo simulator on a Franka Emika Panda
robot. Against a vast set of trajectories, VAICAM achieves superior performance
compared to two baseline controllers.

</details>


### [116] [Dual-Stage Safe Herding Framework for Adversarial Attacker in Dynamic Environment](https://arxiv.org/abs/2509.08460)
*Wenqing Wang,Ye Zhang,Haoyu Li,Jingyu Wang*

Main category: cs.RO

TL;DR: 本文提出了一种用于多机器人系统在复杂城市和动态障碍环境中引导对抗性目标的分层混合调度方法，通过博弈和局部路径规划，有效实现目标安全隔离和引导。


<details>
  <summary>Details</summary>
Motivation: 随着机器人自主能力提升，其在实际复杂环境中的部署日益广泛，安全和防护问题也随之凸显。传统的固定队形护卫方法难以应对城市或障碍密集环境，尤其难以对付策略未知且可自适应的对抗性目标。亟需新的方法提升多机器人系统对于对抗目标的有效控制和引导能力。

Method: 本文提出一种基于reach-avoid博弈理论和局部运动规划的分层混合框架。核心包括虚拟围界和事件触发追捕机制，实现多防御机器人间的高效协同与可扩展控制，保障导航过程中的无碰撞和安全分隔。

Result: 仿真结果表明，该方法能够有效、可靠地将对抗性目标从保护区引导至指定安全区域，实现了对抗性目标的安全引导和隔离任务。

Conclusion: 所提方法在动态复杂环境下展现了良好的安全性、效率和可扩展性，为多机器人系统应对未知对抗目标提供了理论和实践基础。

Abstract: Recent advances in robotics have enabled the widespread deployment of
autonomous robotic systems in complex operational environments, presenting both
unprecedented opportunities and significant security problems. Traditional
shepherding approaches based on fixed formations are often ineffective or risky
in urban and obstacle-rich scenarios, especially when facing adversarial agents
with unknown and adaptive behaviors. This paper addresses this challenge as an
extended herding problem, where defensive robotic systems must safely guide
adversarial agents with unknown strategies away from protected areas and into
predetermined safe regions, while maintaining collision-free navigation in
dynamic environments. We propose a hierarchical hybrid framework based on
reach-avoid game theory and local motion planning, incorporating a virtual
containment boundary and event-triggered pursuit mechanisms to enable scalable
and robust multi-agent coordination. Simulation results demonstrate that the
proposed approach achieves safe and efficient guidance of adversarial agents to
designated regions.

</details>


### [117] [CLAP: Clustering to Localize Across n Possibilities, A Simple, Robust Geometric Approach in the Presence of Symmetries](https://arxiv.org/abs/2509.08495)
*Gabriel I. Fernandez,Ruochen Hou,Alex Xu,Colin Togashi,Dennis W. Hong*

Main category: cs.RO

TL;DR: 本文提出了一种名为CLAP的定位方法，通过状态聚类提升在RoboCup 2024成人组仿人机器人足球赛中的定位鲁棒性和准确性，最终帮助团队夺冠。


<details>
  <summary>Details</summary>
Motivation: 受RoboCup竞赛规则限制，机器人仅能使用立体视觉和惯性传感器，还需应对环境噪声、特征遮挡、视觉误检等多种实际挑战，因此亟需一种高鲁棒性的定位方案为运动规划和策略决策提供支持。

Method: CLAP方法利用对场地特征对估算出的机器人状态进行聚类，正确的状态结果会自然聚集，不正确的状态则会分散，从而在输入噪声大、误检多情况下仍保持高鲁棒性。同时，结合粒子滤波和扩展卡尔曼滤波提高了一致性与平滑性。

Result: 与其他基于地标定位方法对比实验发现，CLAP在正常和虚假特征增加条件下都表现出较高的准确性和显著提升的鲁棒性，很少出现位置发散和速度突变。

Conclusion: CLAP有力支撑了机器人比赛表现，助其在远距离射门和守门等关键时刻成功定位，展现了实际应用价值和优越性能。

Abstract: In this paper, we present our localization method called CLAP, Clustering to
Localize Across $n$ Possibilities, which helped us win the RoboCup 2024
adult-sized autonomous humanoid soccer competition. Competition rules limited
our sensor suite to stereo vision and an inertial sensor, similar to humans. In
addition, our robot had to deal with varying lighting conditions, dynamic
feature occlusions, noise from high-impact stepping, and mistaken features from
bystanders and neighboring fields. Therefore, we needed an accurate, and most
importantly robust localization algorithm that would be the foundation for our
path-planning and game-strategy algorithms. CLAP achieves these requirements by
clustering estimated states of our robot from pairs of field features to
localize its global position and orientation. Correct state estimates naturally
cluster together, while incorrect estimates spread apart, making CLAP resilient
to noise and incorrect inputs. CLAP is paired with a particle filter and an
extended Kalman filter to improve consistency and smoothness. Tests of CLAP
with other landmark-based localization methods showed similar accuracy.
However, tests with increased false positive feature detection showed that CLAP
outperformed other methods in terms of robustness with very little divergence
and velocity jumps. Our localization performed well in competition, allowing
our robot to shoot faraway goals and narrowly defend our goal.

</details>


### [118] [Facilitating the Emergence of Assistive Robots to Support Frailty: Psychosocial and Environmental Realities](https://arxiv.org/abs/2509.08510)
*Angela Higgins,Stephen Potter,Mauro Dragone,Mark Hawley,Farshid Amirabdollahian,Alessandro Di Nuovo,Praminda Caleb-Solly*

Main category: cs.RO

TL;DR: 通过对患有虚弱症的老年人、照护者和专业人士进行共创设计研讨，发现辅助机器人从实验室走向现实应用面临情感、社会和心理等多元复杂因素。研究总结了与虚弱相关的实际设计需求，推动辅助机器人更贴合用户需要，提高现实可行性。


<details>
  <summary>Details</summary>
Motivation: 辅助机器人在帮助有虚弱症状老年人生活方面有潜力，但现实应用很少，说明实验室研发与实际需求存在较大差距。该研究旨在深入理解现实生活中的技术适应困境，推动辅助机器人设计更好地满足真实用户需求。

Method: 研究通过系列共创设计研讨会（7场共61名参与者，包括虚弱老年人、照护者和医疗保健专业人员），采用基于人物角色（persona）的方式，挖掘并探讨技术融入日常生活面临的情感、社会和心理等具体问题。

Result: 通过用户参与的共创活动，总结出与虚弱状况直接相关的设计需求，展示出辅助机器人应用的复杂性，强调情感、社会和环境因素对实际可用性的影响。

Conclusion: 辅助机器人设计应充分考虑与用户生活相关的心理和社会环境等多维需求，更具务实地集成人机交互理念，有望加快辅助机器人在现实场景中的应用进展。

Abstract: While assistive robots have much potential to help older people with
frailty-related needs, there are few in use. There is a gap between what is
developed in laboratories and what would be viable in real-world contexts.
Through a series of co-design workshops (61 participants across 7 sessions)
including those with lived experience of frailty, their carers, and healthcare
professionals, we gained a deeper understanding of everyday issues concerning
the place of new technologies in their lives. A persona-based approach surfaced
emotional, social, and psychological issues. Any assistive solution must be
developed in the context of this complex interplay of psychosocial and
environmental factors. Our findings, presented as design requirements in direct
relation to frailty, can help promote design thinking that addresses people's
needs in a more pragmatic way to move assistive robotics closer to real-world
use.

</details>


### [119] [FMT$^{x}$: An Efficient and Asymptotically Optimal Extension of the Fast Marching Tree for Dynamic Replanning](https://arxiv.org/abs/2509.08521)
*Soheil Espahbodini Nia*

Main category: cs.RO

TL;DR: 本文提出了FMT$^{x}$算法，用以实现动态环境中高效且一致的路径重规划，并且其性能优于主流的重规划算法RRT$^{x}$。


<details>
  <summary>Details</summary>
Motivation: 现有的FMT$^{*}$算法适用于静态环境，可获得渐进最优解，但其单次遍历设计使其在动态环境下无法实时修改路径；而全局重规划则计算代价极高。因此亟需一种既能高效维护路径、又能有效适应环境变化的规划算法。

Method: 作者对FMT$^{*}$算法的邻居选择规则进行微小修改，提出FMT$^{x}$，允许在发现更优路径连接时对已知节点的cost-to-come进行更新。FMT$^{x}$通过维护一个按代价排序的优先队列，并采用有选择的更新策略，即只有扩展邻居节点时才会触发下游节点的路径修正操作，实现了高效的局部修复，同时保留FMT$^{*}$本来的计算效率和渐进最优性。

Result: 理论上，本方法在环境变化后能恢复渐进最优解。实验显示，FMT$^{x}$在应对动态事件时，比RRT$^{x}$重规划算法反应更快、计算开销更低，能更有效地适应动态障碍物。

Conclusion: FMT$^{x}$在保持FMT$^{*}$高效和最优特性的基础上，引入了灵活、局部的重规划机制，为动态场景下的机器人导航提供了优于现有方法的新方案。

Abstract: Path planning in dynamic environments remains a core challenge in robotics,
especially as autonomous systems are deployed in unpredictable spaces such as
warehouses and public roads. While algorithms like Fast Marching Tree
(FMT$^{*}$) offer asymptotically optimal solutions in static settings, their
single-pass design prevents path revisions which are essential for real-time
adaptation. On the other hand, full replanning is often too computationally
expensive. This paper introduces FMT$^{x}$, an extension of the Fast Marching
Tree algorithm that enables efficient and consistent replanning in dynamic
environments. We revisit the neighbor selection rule of FMT$^{*}$ and
demonstrate that a minimal change overcomes its single-pass limitation,
enabling the algorithm to update cost-to-come values upon discovering better
connections without sacrificing asymptotic optimality or computational
efficiency. By maintaining a cost-ordered priority queue and applying a
selective update condition that uses an expanding neighbor to identify and
trigger the re-evaluation of any node with a potentially suboptimal path,
FMT$^{x}$ ensures that suboptimal routes are efficiently repaired as the
environment evolves. This targeted strategy preserves the inherent efficiency
of FMT$^{*}$ while enabling robust adaptation to changes in obstacle
configuration. FMT$^{x}$ is proven to recover an asymptotically optimal
solution after environmental changes. Experimental results demonstrate that
FMT$^{x}$ outperforms the influential replanner RRT$^{x}$, reacting more
swiftly to dynamic events with lower computational overhead and thus offering a
more effective solution for real-time robotic navigation in unpredictable
worlds.

</details>


### [120] [RoboMatch: A Mobile-Manipulation Teleoperation Platform with Auto-Matching Network Architecture for Long-Horizon Manipulation](https://arxiv.org/abs/2509.08522)
*Hanyu Liu,Yunsheng Ma,Jiaxin Huang,Keqiang Ren,Jiayi Wen,Yilin Zheng,Baishu Wan,Pan Li,Jiejun Hou,Haoru Luan,Zhihua Wang,Zhigong Song*

Main category: cs.RO

TL;DR: 本文提出了RoboMatch，一个用于动态环境中移动操作的统一远程操作平台，能够提升遥操作表现和复杂任务完成效率。


<details>
  <summary>Details</summary>
Motivation: 现有的移动操作平台在处理动态环境下的长时任务时效率不高，远程操作不够精细，数据收集不高效，难以满足实际复杂场景需求。

Method: 1）设计了驾驶舱式控制界面，实现同步操作移动底盘和双臂，提高控制精度和数据收集效率；2）提出Proprioceptive-Visual Enhanced Diffusion Policy（PVE-DP），利用小波变换（DWT）做多尺度视觉特征提取，并融合末端IMU提升本体感知输入，强化精细操作能力；3）设计Auto-Matching Network（AMN）架构，将长时任务分解为序列子任务，动态分配轻量预训练模型分布式推理。

Result: 实验证明，该系统数据收集效率提升20%以上；PVE-DP使任务成功率提升20-30%；AMN使长时任务推理性能提升约40%。

Conclusion: RoboMatch及其新方法显著提升了复杂任务下遥操作的效率、数据收集、精细操控与长时任务稳定性，是复杂操作任务的有力解决方案。

Abstract: This paper presents RoboMatch, a novel unified teleoperation platform for
mobile manipulation with an auto-matching network architecture, designed to
tackle long-horizon tasks in dynamic environments. Our system enhances
teleoperation performance, data collection efficiency, task accuracy, and
operational stability. The core of RoboMatch is a cockpit-style control
interface that enables synchronous operation of the mobile base and dual arms,
significantly improving control precision and data collection. Moreover, we
introduce the Proprioceptive-Visual Enhanced Diffusion Policy (PVE-DP), which
leverages Discrete Wavelet Transform (DWT) for multi-scale visual feature
extraction and integrates high-precision IMUs at the end-effector to enrich
proprioceptive feedback, substantially boosting fine manipulation performance.
Furthermore, we propose an Auto-Matching Network (AMN) architecture that
decomposes long-horizon tasks into logical sequences and dynamically assigns
lightweight pre-trained models for distributed inference. Experimental results
demonstrate that our approach improves data collection efficiency by over 20%,
increases task success rates by 20-30% with PVE-DP, and enhances long-horizon
inference performance by approximately 40% with AMN, offering a robust solution
for complex manipulation tasks.

</details>


### [121] [AutoODD: Agentic Audits via Bayesian Red Teaming in Black-Box Models](https://arxiv.org/abs/2509.08638)
*Rebecca Martin,Jay Patrikar,Sebastian Scherer*

Main category: cs.RO

TL;DR: 专用机器学习模型在实际部署中易发生失效，需要高效审核其运行边界，论文提出一种基于大语言模型代理的自动测试框架，自动生成相关测试样本以发现模型潜在失效点。


<details>
  <summary>Details</summary>
Motivation: 随着高风险场景下机器学习模型的应用增多，如何高效审核并明确模型可安全运行的边界（ODD）非常关键，人工审核工作量和专业门槛极高，需要自动化手段。

Method: 提出基于大语言模型代理（LLM-Agent）的测试框架，利用LLM-Agent自动生成语义相关测试案例，低维文本嵌入空间下构建不确定性感知的失效分布模型，通过工具协作不断生成测试并记录模型响应，指导探查低维流形上的不确定区域。

Result: 在MNIST数据集（特定缺失数字条件）及无人机视觉入侵者检测真实场景中，展示了此方法自动生成测试样本、发现潜在失效模式的有效性。

Conclusion: 基于LLM-Agent的自动测试方法能在复杂高维输入空间下高效探索、发现模型潜在失效边界，降低人工审核成本，提高审核全面性，对安全关键应用有现实意义。

Abstract: Specialized machine learning models, regardless of architecture and training,
are susceptible to failures in deployment. With their increasing use in high
risk situations, the ability to audit these models by determining their
operational design domain (ODD) is crucial in ensuring safety and compliance.
However, given the high-dimensional input spaces, this process often requires
significant human resources and domain expertise. To alleviate this, we
introduce \coolname, an LLM-Agent centric framework for automated generation of
semantically relevant test cases to search for failure modes in specialized
black-box models. By leveraging LLM-Agents as tool orchestrators, we aim to fit
a uncertainty-aware failure distribution model on a learned text-embedding
manifold by projecting the high-dimension input space to low-dimension
text-embedding latent space. The LLM-Agent is tasked with iteratively building
the failure landscape by leveraging tools for generating test-cases to probe
the model-under-test (MUT) and recording the response. The agent also guides
the search using tools to probe uncertainty estimate on the low dimensional
manifold. We demonstrate this process in a simple case using models trained
with missing digits on the MNIST dataset and in the real world setting of
vision-based intruder detection for aerial vehicles.

</details>


### [122] [TANGO: Traversability-Aware Navigation with Local Metric Control for Topological Goals](https://arxiv.org/abs/2509.08699)
*Stefan Podgorski,Sourav Garg,Mehdi Hosseinzadeh,Lachlan Mares,Feras Dayoub,Ian Reid*

Main category: cs.RO

TL;DR: 本文提出了一种无需3D地图和预训练控制器的全新基于RGB图像的对象级拓扑-度量导航方法，实现了零样本长距离导航，且超越现有方法。代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有机器人视觉导航需要全局一致的3D地图或需广泛训练的控制器，计算开销大且难以适应不同环境，限制了泛化和实际部署。

Method: 方法结合了基于RGB图像的全局拓扑路径规划与局部度量轨迹控制，支持对象级子目标导航与避障。通过单目深度和可通行性估计持续预测局部轨迹，并具备自动切换到基础控制器的机制。系统依赖基础大模型，无需领域特定微调。

Result: 方法在仿真和现实场景中均展现了稳健性和实际可部署性，性能优于当前先进方法，具有更好环境适应性。

Conclusion: 该方法为开放集环境下的机器人视觉导航提供了更高效、可泛化的解决方案，为实际部署带来了新机遇。

Abstract: Visual navigation in robotics traditionally relies on globally-consistent 3D
maps or learned controllers, which can be computationally expensive and
difficult to generalize across diverse environments. In this work, we present a
novel RGB-only, object-level topometric navigation pipeline that enables
zero-shot, long-horizon robot navigation without requiring 3D maps or
pre-trained controllers. Our approach integrates global topological path
planning with local metric trajectory control, allowing the robot to navigate
towards object-level sub-goals while avoiding obstacles. We address key
limitations of previous methods by continuously predicting local trajectory
using monocular depth and traversability estimation, and incorporating an
auto-switching mechanism that falls back to a baseline controller when
necessary. The system operates using foundational models, ensuring open-set
applicability without the need for domain-specific fine-tuning. We demonstrate
the effectiveness of our method in both simulated environments and real-world
tests, highlighting its robustness and deployability. Our approach outperforms
existing state-of-the-art methods, offering a more adaptable and effective
solution for visual navigation in open-set environments. The source code is
made publicly available: https://github.com/podgorki/TANGO.

</details>


### [123] [Parallel, Asymptotically Optimal Algorithms for Moving Target Traveling Salesman Problems](https://arxiv.org/abs/2509.08743)
*Anoop Bhat,Geordan Gutow,Bhaskar Vundurthy,Zhongqiang Ren,Sivakumar Rathinam,Howie Choset*

Main category: cs.RO

TL;DR: 本文提出了一种用于解决移动目标旅行商问题（MT-TSP）的新算法框架IRG，其核心思想是结合随机采样和广义TSP（GTSP）求解，所提并行算法在多种场景下收敛速度优于已有方法。


<details>
  <summary>Details</summary>
Motivation: 针对MT-TSP问题，面对目标轨迹非线性或智能体动力学约束，现有算法无法保证最优收敛，因此需发展新的能保证收敛的方法。

Method: 提出了迭代随机广义TSP（IRG）框架，将智能体拦截目标的配置-时间点进行随机采样，并用GTSP求解优化顺序，两步迭代收敛于最优。同时提出两种并行算法：IRG-PGLNS（基于并行化GLNS求解器）、PCG（同时并行求解多个GTSP）。

Result: 在三种MT-TSP变体（距离接近、可变速Dubins车、多余自由度机械臂）上进行数值实验，表明IRG-PGLNS与PCG算法收敛速度均优于基线方法。

Conclusion: 所提IRG框架及其并行算法能有效解决带有复杂目标轨迹及智能体约束的MT-TSP问题，具有更快的收敛速度与较强通用性。

Abstract: The Moving Target Traveling Salesman Problem (MT-TSP) seeks an agent
trajectory that intercepts several moving targets, within a particular time
window for each target. In the presence of generic nonlinear target
trajectories or kinematic constraints on the agent, no prior algorithm
guarantees convergence to an optimal MT-TSP solution. Therefore, we introduce
the Iterated Random Generalized (IRG) TSP framework. The key idea behind IRG is
to alternate between randomly sampling a set of agent configuration-time
points, corresponding to interceptions of targets, and finding a sequence of
interception points by solving a generalized TSP (GTSP). This alternation
enables asymptotic convergence to the optimum. We introduce two parallel
algorithms within the IRG framework. The first algorithm, IRG-PGLNS, solves
GTSPs using PGLNS, our parallelized extension of the state-of-the-art solver
GLNS. The second algorithm, Parallel Communicating GTSPs (PCG), solves GTSPs
corresponding to several sets of points simultaneously. We present numerical
results for three variants of the MT-TSP: one where intercepting a target only
requires coming within a particular distance, another where the agent is a
variable-speed Dubins car, and a third where the agent is a redundant robot
arm. We show that IRG-PGLNS and PCG both converge faster than a baseline based
on prior work.

</details>


### [124] [SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot Navigation](https://arxiv.org/abs/2509.08757)
*Michael J. Munje,Chen Tang,Shuijing Liu,Zichao Hu,Yifeng Zhu,Jiaxun Cui,Garrett Warnell,Joydeep Biswas,Peter Stone*

Main category: cs.RO

TL;DR: 本文提出了SocialNav-SUB数据集和基准，用于系统性评估视觉-语言模型（VLMs）在社会化机器人导航场景理解中的能力。实验显示尽管VLMs有进步但仍不及简单规则和人类基线。


<details>
  <summary>Details</summary>
Motivation: 虽然VLMs在物体识别、常识推理和上下文理解上表现出色，这些能力适合社会化机器人导航需求，但尚未有系统研究其在复杂社会导航场景（尤其是时空和意图推断）中的实际理解能力。

Method: 作者提出并构建了SocialNav-SUB，这是一个用于社会化机器人导航场景理解的视觉问答数据集和测试基准，通过空间、时空和社会推理相关问题，对VLMs的能力进行综合性评估，并与规则和人类基线作对比。

Result: 实验表明，当前最先进的VLM与人类答案一致的概率虽较高，但在性能上仍低于简单的规则方法和人类基线，揭示现有VLMs在社会场景理解中的短板。

Conclusion: SocialNav-SUB为VLMs在社会化机器人导航中的场景理解能力评估建立了基础，有助于推动更符合实际应用需求的模型发展与研究。

Abstract: Robot navigation in dynamic, human-centered environments requires
socially-compliant decisions grounded in robust scene understanding. Recent
Vision-Language Models (VLMs) exhibit promising capabilities such as object
recognition, common-sense reasoning, and contextual understanding-capabilities
that align with the nuanced requirements of social robot navigation. However,
it remains unclear whether VLMs can accurately understand complex social
navigation scenes (e.g., inferring the spatial-temporal relations among agents
and human intentions), which is essential for safe and socially compliant robot
navigation. While some recent works have explored the use of VLMs in social
robot navigation, no existing work systematically evaluates their ability to
meet these necessary conditions. In this paper, we introduce the Social
Navigation Scene Understanding Benchmark (SocialNav-SUB), a Visual Question
Answering (VQA) dataset and benchmark designed to evaluate VLMs for scene
understanding in real-world social robot navigation scenarios. SocialNav-SUB
provides a unified framework for evaluating VLMs against human and rule-based
baselines across VQA tasks requiring spatial, spatiotemporal, and social
reasoning in social robot navigation. Through experiments with state-of-the-art
VLMs, we find that while the best-performing VLM achieves an encouraging
probability of agreeing with human answers, it still underperforms simpler
rule-based approach and human consensus baselines, indicating critical gaps in
social scene understanding of current VLMs. Our benchmark sets the stage for
further research on foundation models for social robot navigation, offering a
framework to explore how VLMs can be tailored to meet real-world social robot
navigation needs. An overview of this paper along with the code and data can be
found at https://larg.github.io/socialnav-sub .

</details>


### [125] [Joint Model-based Model-free Diffusion for Planning with Constraints](https://arxiv.org/abs/2509.08775)
*Wonsuhk Jung,Utkarsh A. Mishra,Nadun Ranawaka Arachchige,Yongxin Chen,Danfei Xu,Shreyas Kousik*

Main category: cs.RO

TL;DR: 本文提出了一种新的生成建模框架JM2D，将无模型扩散规划器与基于模型的优化模块有效集成，用于机器人运动规划，显著提升了任务表现和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的无模型运动规划器在实际机器人系统中面临与安全等约束相关的性能限制。单纯地将其与基于模型的优化模块组合，二者在输出时会出现兼容性问题，导致难以在保证安全的同时充分发挥扩散模型的多模态能力。

Method: JM2D（Joint Model-based Model-free Diffusion）将扩散规划模块和优化模块的集成形式化为联合采样问题，并通过相互作用势能（interaction potential）来衡量二者的兼容性。该框架利用重要性采样，仅依赖于对势能的评估，能处理非可微目标，从而兼容诸如非凸优化常见的安全约束。无需额外训练即可实现模块联合优化。

Result: 在离线强化学习和机器人操作任务中应用JM2D后，系统显著提升了任务完成度，并在不牺牲安全性的前提下优于传统的安全过滤方法。此外，作者还分析了传统基于梯度和投影的扩散规划方法，证明了条件生成是JM2D的特例。

Conclusion: JM2D有效融合无模型扩散规划器和基于模型的优化模块，提升机器人系统任务表现和安全性。该方法适用于包含非可微和非凸目标的实际机器人任务，无需额外训练，具备较强实际推广价值。

Abstract: Model-free diffusion planners have shown great promise for robot motion
planning, but practical robotic systems often require combining them with
model-based optimization modules to enforce constraints, such as safety.
Naively integrating these modules presents compatibility challenges when
diffusion's multi-modal outputs behave adversarially to optimization-based
modules. To address this, we introduce Joint Model-based Model-free Diffusion
(JM2D), a novel generative modeling framework. JM2D formulates module
integration as a joint sampling problem to maximize compatibility via an
interaction potential, without additional training. Using importance sampling,
JM2D guides modules outputs based only on evaluations of the interaction
potential, thus handling non-differentiable objectives commonly arising from
non-convex optimization modules. We evaluate JM2D via application to aligning
diffusion planners with safety modules on offline RL and robot manipulation.
JM2D significantly improves task performance compared to conventional safety
filters without sacrificing safety. Further, we show that conditional
generation is a special case of JM2D and elucidate key design choices by
comparing with SOTA gradient-based and projection-based diffusion planners.
More details at: https://jm2d-corl25.github.io/.

</details>


### [126] [Calib3R: A 3D Foundation Model for Multi-Camera to Robot Calibration and 3D Metric-Scaled Scene Reconstruction](https://arxiv.org/abs/2509.08813)
*Davide Allegro,Matteo Terreran,Stefano Ghidoni*

Main category: cs.RO

TL;DR: 本论文提出了一种无需标定板的新方法Calib3R，能同时完成机器人相机标定与三维重建，并实现度量尺度和机器人坐标系对齐，且在数据集上精度优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 传统机器人需要通过RGB图像完成操控和导航，但精确任务要求三维场景具有真实尺度并与机器人参考系对齐。现有流程需要分别处理相机标定和三维重建且通常需要标定板，流程复杂、效率低。该工作旨在简化流程、提升效率与通用性。

Method: 作者提出Calib3R方法，将相机标定与三维重建通过统一优化联合实现。技术上基于MASt3R三维基础模型，从RGB图像中提取点云，并结合机器人位姿，实现场景三维恢复。方法适用于单/多相机、机械臂与移动机器人，无需使用标定板。

Result: 在多种数据集上实验表明，该方法仅需少于10张图片即可完成精确标定和度量尺度三维重建，表现优于传统无靶和有靶标定方法。

Conclusion: Calib3R简化了机器人系统的标定与三维重建流程，无须使用专用标定板，且精度高、效率优，适合多种机器人平台应用，推动机器人感知与环境建模的发展。

Abstract: Robots often rely on RGB images for tasks like manipulation and navigation.
However, reliable interaction typically requires a 3D scene representation that
is metric-scaled and aligned with the robot reference frame. This depends on
accurate camera-to-robot calibration and dense 3D reconstruction, tasks usually
treated separately, despite both relying on geometric correspondences from RGB
data. Traditional calibration needs patterns, while RGB-based reconstruction
yields geometry with an unknown scale in an arbitrary frame. Multi-camera
setups add further complexity, as data must be expressed in a shared reference
frame. We present Calib3R, a patternless method that jointly performs
camera-to-robot calibration and metric-scaled 3D reconstruction via unified
optimization. Calib3R handles single- and multi-camera setups on robot arms or
mobile robots. It builds on the 3D foundation model MASt3R to extract pointmaps
from RGB images, which are combined with robot poses to reconstruct a scaled 3D
scene aligned with the robot. Experiments on diverse datasets show that Calib3R
achieves accurate calibration with less than 10 images, outperforming
target-less and marker-based methods.

</details>


### [127] [RoboChemist: Long-Horizon and Safety-Compliant Robotic Chemical Experimentation](https://arxiv.org/abs/2509.08820)
*Zongzheng Zhang,Chenghao Yue,Haobo Xu,Minwen Liao,Xianglin Qi,Huan-ang Gao,Ziwei Wang,Hao Zhao*

Main category: cs.RO

TL;DR: 本论文提出了RoboChemist系统，通过结合视觉-语言模型（VLM）和视觉-语言-动作（VLA）模型，实现了更高成功率和合规性的机器人化学实验操作。系统能分解任务、生成视觉提示，并检查任务合规性，显著超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 当前机器人化学家在实际执行复杂化学实验中面临实验步骤长、环境危险且物体易变形等挑战，且现有方法在对实验细节的把控和对透明器皿的识别上效果有限。此外，真实化学实验对精确合规性要求高。为解决这些痛点，需要提升机器人在语义层次反馈和精确控制上的能力。

Method: 作者提出了RoboChemist的双环路框架，将VLM和VLA模型深度集成。VLM负责任务分解、视觉提示生成和合规性监控，VLA则基于VLM输出的视觉目标实现精准的动作控制。系统同时支持基础动作与多步骤化学实验的完整操作。

Result: RoboChemist系统在多步骤化学实验中的成功率相比主流VLA基线提升了23.57%，合规性评分提高了0.298，并能够泛化到多种对象和复杂任务。

Conclusion: RoboChemist显著提升了机器人化学家自动化执行实验的可靠性和规范性，在复杂和实际化学场景中展现出强大潜力，为未来的自动化科学发现提供了新路径。

Abstract: Robotic chemists promise to both liberate human experts from repetitive tasks
and accelerate scientific discovery, yet remain in their infancy. Chemical
experiments involve long-horizon procedures over hazardous and deformable
substances, where success requires not only task completion but also strict
compliance with experimental norms. To address these challenges, we propose
\textit{RoboChemist}, a dual-loop framework that integrates Vision-Language
Models (VLMs) with Vision-Language-Action (VLA) models. Unlike prior VLM-based
systems (e.g., VoxPoser, ReKep) that rely on depth perception and struggle with
transparent labware, and existing VLA systems (e.g., RDT, pi0) that lack
semantic-level feedback for complex tasks, our method leverages a VLM to serve
as (1) a planner to decompose tasks into primitive actions, (2) a visual prompt
generator to guide VLA models, and (3) a monitor to assess task success and
regulatory compliance. Notably, we introduce a VLA interface that accepts
image-based visual targets from the VLM, enabling precise, goal-conditioned
control. Our system successfully executes both primitive actions and complete
multi-step chemistry protocols. Results show 23.57% higher average success rate
and a 0.298 average increase in compliance rate over state-of-the-art VLA
baselines, while also demonstrating strong generalization to objects and tasks.

</details>
