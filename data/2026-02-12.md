<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 83]
- [cs.CL](#cs.CL) [Total: 53]
- [cs.RO](#cs.RO) [Total: 33]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Multi-encoder ConvNeXt Network with Smooth Attentional Feature Fusion for Multispectral Semantic Segmentation](https://arxiv.org/abs/2602.10137)
*Leo Thomas Ramos,Angel D. Sappa*

Main category: cs.CV

TL;DR: 本文提出MeCSAFNet，一种适用于多光谱遥感影像土地覆盖分割的多分支编码-解码模型结构。该模型在大规模数据集上取得了显著优于现有主流方法的表现，并能兼顾高效和节省计算资源。


<details>
  <summary>Details</summary>
Motivation: 传统方法对多光谱数据的空间和光谱信息融合不足，在土地覆盖分割上难以兼顾准确性和高效性。因此，作者设计新结构以提升分割精度并适应不同光谱配置。

Method: MeCSAFNet采用双分支ConvNeXt编码器分别处理可见光和不可见光通道，独立解码后再引入融合解码器进行多尺度特征整合，并结合CBAM注意力机制和ASAU激活函数优化模型训练，支持4、6通道等多种输入。

Result: 在Five-Billion-Pixels和Potsdam数据集上，MeCSAFNet显著超越U-Net、SegFormer、DeepLabV3+等主流方法，mIoU提升达14~19%（FBP）和5~9%（Potsdam），同时提供轻量级变体兼顾性能与效率。

Conclusion: MeCSAFNet在多光谱影像土地覆盖分割任务中有效提升了分割性能，并支持低资源环境下的部署，展现出卓越的应用前景。

Abstract: This work proposes MeCSAFNet, a multi-branch encoder-decoder architecture for land cover segmentation in multispectral imagery. The model separately processes visible and non-visible channels through dual ConvNeXt encoders, followed by individual decoders that reconstruct spatial information. A dedicated fusion decoder integrates intermediate features at multiple scales, combining fine spatial cues with high-level spectral representations. The feature fusion is further enhanced with CBAM attention, and the ASAU activation function contributes to stable and efficient optimization. The model is designed to process different spectral configurations, including a 4-channel (4c) input combining RGB and NIR bands, as well as a 6-channel (6c) input incorporating NDVI and NDWI indices. Experiments on the Five-Billion-Pixels (FBP) and Potsdam datasets demonstrate significant performance gains. On FBP, MeCSAFNet-base (6c) surpasses U-Net (4c) by +19.21%, U-Net (6c) by +14.72%, SegFormer (4c) by +19.62%, and SegFormer (6c) by +14.74% in mIoU. On Potsdam, MeCSAFNet-large (4c) improves over DeepLabV3+ (4c) by +6.48%, DeepLabV3+ (6c) by +5.85%, SegFormer (4c) by +9.11%, and SegFormer (6c) by +4.80% in mIoU. The model also achieves consistent gains over several recent state-of-the-art approaches. Moreover, compact variants of MeCSAFNet deliver notable performance with lower training time and reduced inference cost, supporting their deployment in resource-constrained environments.

</details>


### [2] [Multimodal Information Fusion for Chart Understanding: A Survey of MLLMs -- Evolution, Limitations, and Cognitive Enhancement](https://arxiv.org/abs/2602.10138)
*Zhihang Yi,Jian Zhao,Jiancheng Lv,Tao Wang*

Main category: cs.CV

TL;DR: 本综述系统梳理了基于多模态大模型（MLLM）的图表理解研究现状，从挑战、任务、数据集、方法到未来方向，旨在为该领域研究者提供清晰的发展脉络和未来研究建议。


<details>
  <summary>Details</summary>
Motivation: 图表作为信息融合的重要载体，需要结合图像和文本数据进行有效理解。随着多模态大语言模型的发展，基于MLLM的图表分析取得了进展，但相关研究仍较分散，缺乏系统梳理。本文为厘清研究现状并推动该领域发展，提出系统性综述。

Method: 首先分析图文融合中的核心挑战，然后对下游任务和数据集进行分类，提出新的基准测试分类法，并梳理从传统深度学习到先进MLLM方法的技术演进，重点关注复杂信息融合策略。最后，指出当前模型在感知与推理等方面的不足，并提出未来可能的发展方向，如高级对齐技术和基于强化学习的认知能力提升。

Result: 总结了图表理解领域核心挑战、主流任务、关键数据集及主流方法，对现有MLLM模型的局限性进行了深入分析，包括感知能力和推理能力上的不足，并对当前基准及评测方法进行了系统分类和评述。

Conclusion: 本综述为MLLM驱动的图表理解研究提供了系统性的研究框架和路线图，揭示了现有问题和改进方向，期待能够引导研究者聚焦于模型感知与推理能力的提升，推进图表信息融合技术的发展。

Abstract: Chart understanding is a quintessential information fusion task, requiring the seamless integration of graphical and textual data to extract meaning. The advent of Multimodal Large Language Models (MLLMs) has revolutionized this domain, yet the landscape of MLLM-based chart analysis remains fragmented and lacks systematic organization. This survey provides a comprehensive roadmap of this nascent frontier by structuring the domain's core components. We begin by analyzing the fundamental challenges of fusing visual and linguistic information in charts. We then categorize downstream tasks and datasets, introducing a novel taxonomy of canonical and non-canonical benchmarks to highlight the field's expanding scope. Subsequently, we present a comprehensive evolution of methodologies, tracing the progression from classic deep learning techniques to state-of-the-art MLLM paradigms that leverage sophisticated fusion strategies. By critically examining the limitations of current models, particularly their perceptual and reasoning deficits, we identify promising future directions, including advanced alignment techniques and reinforcement learning for cognitive enhancement. This survey aims to equip researchers and practitioners with a structured understanding of how MLLMs are transforming chart information fusion and to catalyze progress toward more robust and reliable systems.

</details>


### [3] [MPA: Multimodal Prototype Augmentation for Few-Shot Learning](https://arxiv.org/abs/2602.10143)
*Liwen Wu,Wei Wang,Lei Zhao,Zhan Gao,Qika Lin,Shaowen Yao,Zuozhu Liu,Bin Pu*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态原型增强FSL框架MPA，在多模态和增强方面提升了小样本学习的识别能力，实现了跨领域和单领域的显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前小样本学习（FSL）大多只利用视觉信息，直接从支持图片计算原型，导致语义信息有限，不利于模型泛化和识别能力提升。作者的动机在于利用多模态信息、语义增强和不确定性建模来弥补上述不足，提高FSL性能。

Method: 提出的MPA框架包括三个核心模块：1) 基于大语言模型生成多样化类别描述的多变体语义增强（LMSE），为支持集补充丰富语义信息；2) 层次化多视角增强（HMA），通过自然及多视角增强手段提升特征多样性（如视距、视角、光照等变化）；3) 自适应不确定类吸收器（AUCA），通过插值与高斯采样引入不确定类别样本，有效吸收难以判别的样本。

Result: 在4个单领域和6个跨领域FSL基准数据上的大规模实验表明，MPA在大多数评测设置下均优于现有最优方法。在最具挑战性的5-way 1-shot单领域和跨领域评测中，相较于次优方法，MPA分别提升了12.29%和24.56%的准确率。

Conclusion: 结合多模态语义增强、视角多样化和不确定性建模的MPA显著提升了小样本学习的泛化能力和识别性能，为多领域FSL提供了有效解决方案。

Abstract: Recently, few-shot learning (FSL) has become a popular task that aims to recognize new classes from only a few labeled examples and has been widely applied in fields such as natural science, remote sensing, and medical images. However, most existing methods focus only on the visual modality and compute prototypes directly from raw support images, which lack comprehensive and rich multimodal information. To address these limitations, we propose a novel Multimodal Prototype Augmentation FSL framework called MPA, including LLM-based Multi-Variant Semantic Enhancement (LMSE), Hierarchical Multi-View Augmentation (HMA), and an Adaptive Uncertain Class Absorber (AUCA). LMSE leverages large language models to generate diverse paraphrased category descriptions, enriching the support set with additional semantic cues. HMA exploits both natural and multi-view augmentations to enhance feature diversity (e.g., changes in viewing distance, camera angles, and lighting conditions). AUCA models uncertainty by introducing uncertain classes via interpolation and Gaussian sampling, effectively absorbing uncertain samples. Extensive experiments on four single-domain and six cross-domain FSL benchmarks demonstrate that MPA achieves superior performance compared to existing state-of-the-art methods across most settings. Notably, MPA surpasses the second-best method by 12.29% and 24.56% in the single-domain and cross-domain setting, respectively, in the 5-way 1-shot setting.

</details>


### [4] [VERA: Identifying and Leveraging Visual Evidence Retrieval Heads in Long-Context Understanding](https://arxiv.org/abs/2602.10146)
*Rongcan Pei,Huan Li,Fang Guo,Qi Zhu*

Main category: cs.CV

TL;DR: 该论文分析了视觉语言模型（VLMs）在处理长上下文和复杂推理任务时的机制与瓶颈，并提出了一种提升模型表现的新方法。


<details>
  <summary>Details</summary>
Motivation: 当前VLM在应对需要长文本理解和复杂推理的任务上表现有限，对其内部机制了解不足，这限制了模型能力的提升。作者希望通过机制拆解和细粒度分析找到性能瓶颈，并提出改进方法。

Method: 作者通过对VLM注意力机制的分析，发现了对视觉证据检索（VER）起关键作用的稀疏注意力头，并通过屏蔽实验验证了其对模型性能的因果影响。基于此，提出无需额外训练的VERA方法，通过检测模型不确定性来触发VER头关注视觉证据的显式表述，从而增强模型长上下文处理能力。

Result: VERA方法在多个公开基准上测试，平均使Qwen3-VL-8B-Instruct模型提升21.3%，GLM-4.1V-Thinking提升20.1%。结果表明，该方法能显著改善开放源代码VLM长上下文理解表现。

Conclusion: 论文揭示了VLM长上下文处理的关键机制，并提出了能有效提升模型理解能力的新架构。该方法实用性强、泛化性好，对未来VLM改进有启发意义。

Abstract: While Vision-Language Models (VLMs) have shown promise in textual understanding, they face significant challenges when handling long context and complex reasoning tasks. In this paper, we dissect the internal mechanisms governing long-context processing in VLMs to understand their performance bottlenecks. Through the lens of attention analysis, we identify specific Visual Evidence Retrieval (VER) Heads - a sparse, dynamic set of attention heads critical for locating visual cues during reasoning, distinct from static OCR heads. We demonstrate that these heads are causal to model performance; masking them leads to significant degradation. Leveraging this discovery, we propose VERA (Visual Evidence Retrieval Augmentation), a training-free framework that detects model uncertainty (i.e., entropy) to trigger the explicit verbalization of visual evidence attended by VER heads. Comprehensive experiments demonstrate that VERA significantly improves long-context understanding of open-source VLMs: it yields an average relative improvement of 21.3% on Qwen3-VL-8B-Instruct and 20.1% on GLM-4.1V-Thinking across five benchmarks.

</details>


### [5] [Beyond Closed-Pool Video Retrieval: A Benchmark and Agent Framework for Real-World Video Search and Moment Localization](https://arxiv.org/abs/2602.10159)
*Tao Yu,Yujia Yang,Haopeng Jin,Junhao Gong,Xinlong Chen,Yuxuan Zhou,Shanbin Zhang,Jiabing Yang,Xinming Wang,Hongzhu Yi,Ping Nie,Kai Zou,Zhang Zhang,Yan Huang,Liang Wang,Yeshani,Ruiwen Tao,Jin Ma,Haijin Liang,Jinwen Luo*

Main category: cs.CV

TL;DR: 该论文提出了RVMS-Bench基准和RACLO框架，以更真实地评估和解决基于模糊记忆的开放域视频检索难题。


<details>
  <summary>Details</summary>
Motivation: 现有的视频检索基准侧重于精确描述与封闭视频池的匹配，无法反映真实世界中基于模糊、多维记忆的搜索方式，亟需更贴合实际场景的评测系统和方法。

Method: 1）提出RVMS-Bench：包含1440个样本，横跨20个类别和4个时长组，取自开放网络，设计多层次记忆描述，包括全局印象、关键时刻、时间上下文、听觉记忆，全部人工验证。
2）提出RACLO：一种基于代理和溯因推理的框架，模拟人类“回忆-搜索-验证”全过程，提升应对真实世界模糊视频记忆检索问题的能力。

Result: 实验表明，现有多模态大模型（MLLM）在处理基于模糊记忆的真实世界视频检索和片段定位方面仍能力不足。

Conclusion: RVMS-Bench及RACLO为视频检索研究提供了更加真实和复杂的基准，有助于推动视频检索方法在真实开放场景下的稳健性发展。

Abstract: Traditional video retrieval benchmarks focus on matching precise descriptions to closed video pools, failing to reflect real-world searches characterized by fuzzy, multi-dimensional memories on the open web. We present \textbf{RVMS-Bench}, a comprehensive system for evaluating real-world video memory search. It consists of \textbf{1,440 samples} spanning \textbf{20 diverse categories} and \textbf{four duration groups}, sourced from \textbf{real-world open-web videos}. RVMS-Bench utilizes a hierarchical description framework encompassing \textbf{Global Impression, Key Moment, Temporal Context, and Auditory Memory} to mimic realistic multi-dimensional search cues, with all samples strictly verified via a human-in-the-loop protocol. We further propose \textbf{RACLO}, an agentic framework that employs abductive reasoning to simulate the human ``Recall-Search-Verify'' cognitive process, effectively addressing the challenge of searching for videos via fuzzy memories in the real world. Experiments reveal that existing MLLMs still demonstrate insufficient capabilities in real-world Video Retrieval and Moment Localization based on fuzzy memories. We believe this work will facilitate the advancement of video retrieval robustness in real-world unstructured scenarios.

</details>


### [6] [AD$^2$: Analysis and Detection of Adversarial Threats in Visual Perception for End-to-End Autonomous Driving Systems](https://arxiv.org/abs/2602.10160)
*Ishan Sahu,Somnath Hazra,Somak Aditya,Soumyajit Dey*

Main category: cs.CV

TL;DR: 本文评估了最新端到端自动驾驶系统在黑盒对抗攻击下的脆弱性，并提出了高效的攻击检测模型。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶系统虽取得重大进展，但对抗安全性研究不足，存在潜在安全隐患。

Method: 作者在CARLA平台上对两种主流自动驾驶模型（Transfuser和Interfuser）进行了三类攻击：基于声波的物理模糊攻击、电磁干扰攻击、数字化幽灵物体攻击，考察系统鲁棒性。同时，提出了一种基于注意力机制、关注时空一致性的轻量级攻击检测模型AD$^2$。

Result: 实验证明各种攻击可使驾驶得分下降至99%，验证了系统脆弱性。新提出的AD$^2$检测器在多摄像头输入下检测能力和计算效率均优于现有方法。

Conclusion: 端到端自动驾驶系统面临严重对抗威胁，作者提出的检测模型为提升系统安全性提供了有效途径。

Abstract: End-to-end autonomous driving systems have achieved significant progress, yet their adversarial robustness remains largely underexplored. In this work, we conduct a closed-loop evaluation of state-of-the-art autonomous driving agents under black-box adversarial threat models in CARLA. Specifically, we consider three representative attack vectors on the visual perception pipeline: (i) a physics-based blur attack induced by acoustic waves, (ii) an electromagnetic interference attack that distorts captured images, and (iii) a digital attack that adds ghost objects as carefully crafted bounded perturbations on images. Our experiments on two advanced agents, Transfuser and Interfuser, reveal severe vulnerabilities to such attacks, with driving scores dropping by up to 99% in the worst case, raising valid safety concerns. To help mitigate such threats, we further propose a lightweight Attack Detection model for Autonomous Driving systems (AD$^2$) based on attention mechanisms that capture spatial-temporal consistency. Comprehensive experiments across multi-camera inputs on CARLA show that our detector achieves superior detection capability and computational efficiency compared to existing approaches.

</details>


### [7] [ArtisanGS: Interactive Tools for Gaussian Splat Selection with AI and Human in the Loop](https://arxiv.org/abs/2602.10173)
*Clement Fuji Tsang,Anita Hu,Or Perel,Carsten Kolve,Maria Shugrina*

Main category: cs.CV

TL;DR: 本文提出了一套基于3D高斯斑点（3DGS）表示的交互式选择和分割工具，通过结合AI驱动的二维掩码传播和手动编辑，提升了对3DGS场景编辑的灵活性和可控性，并能直接应用于原始场景而无需额外优化。


<details>
  <summary>Details</summary>
Motivation: 当前3DGS技术在物理模拟、动画等领域应用广泛，但其对象提取和可控编辑能力有限，尤其是在处理原始捕获场景时缺乏高效、灵活的编辑手段。作者希望解决3DGS对象选择、分割与可控编辑的难题，赋予用户更多交互和控制能力。

Method: 本文提出了结合AI驱动的2D选择掩码扩展到3DGS的算法，并配套开发了手动选择及分割工具。该方法支持用户根据需要对系统自动选择结果进行干预，灵活实现3D场景二元分割，并搭配自定义的视频扩散模型促进局部区域可控编辑。

Result: 实验表明，该工具集在3DGS选择任务上的表现优于已有方法，并且在下游本地可控编辑场景中表现出良好实用性。工具操作直观，可适用于任意原始3DGS捕获数据，无需额外优化处理。

Conclusion: 作者提出的工具集显著提升了3DGS编辑与分割的灵活性和效率，为用户提供了自动与手动紧密结合的选择与编辑能力，可广泛应用于实际3D场景的快速处理与编辑。

Abstract: Representation in the family of 3D Gaussian Splats (3DGS) are growing into a viable alternative to traditional graphics for an expanding number of application, including recent techniques that facilitate physics simulation and animation. However, extracting usable objects from in-the-wild captures remains challenging and controllable editing techniques for this representation are limited. Unlike the bulk of emerging techniques, focused on automatic solutions or high-level editing, we introduce an interactive suite of tools centered around versatile Gaussian Splat selection and segmentation. We propose a fast AI-driven method to propagate user-guided 2D selection masks to 3DGS selections. This technique allows for user intervention in the case of errors and is further coupled with flexible manual selection and segmentation tools. These allow a user to achieve virtually any binary segmentation of an unstructured 3DGS scene. We evaluate our toolset against the state-of-the-art for Gaussian Splat selection and demonstrate their utility for downstream applications by developing a user-guided local editing approach, leveraging a custom Video Diffusion Model. With flexible selection tools, users have direct control over the areas that the AI can modify. Our selection and editing tools can be used for any in-the-wild capture without additional optimization.

</details>


### [8] [When the Prompt Becomes Visual: Vision-Centric Jailbreak Attacks for Large Image Editing Models](https://arxiv.org/abs/2602.10179)
*Jiacheng Hou,Yining Sun,Ruochong Jin,Haochen Han,Fangming Liu,Wai Kin Victor Chan,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: 论文提出了以视觉为中心的“越狱攻击”(VJA)，即通过纯视觉输入(如标记、箭头等)对大图像编辑模型进行安全攻击，并建立了名为IESBench的安全性测试基准。实验显示VJA可高效攻破当前主流商业模型，暴露了新型的安全风险。


<details>
  <summary>Details</summary>
Motivation: 随着大图像编辑模型从文本驱动转向视觉驱动，虽然提升了易用性，但同时导致视觉输入本身成为攻击面，而这一安全隐患尚未被充分研究。作者旨在系统性揭示和分析视觉攻击对图像编辑模型所带来的新型安全威胁。

Method: 作者提出“视觉-视觉越狱攻击”(VJA)，通过纯视觉输入引导模型执行恶意操作，并构建了IESBench安全评测基准，全面测试模型在攻击下的表现。为防御此类攻击，作者提出基于模型自省的多模态推理防御方法，且该方案无需额外防护模型，也不会显著增加计算量。

Result: VJA攻击在商用大模型上成功率高(如Nano Banana Pro达80.9%，GPT-Image-1.5达70.1%)，有效暴露了模型的安全漏洞。所提出的防御策略能大幅提升对齐性较弱模型的安全性，对齐后可与主流商用模型安全性相媲美。

Conclusion: 论文首次系统分析了视觉输入带来的安全挑战，提出了视觉-视觉越狱攻击及防御方法，为后续图像编辑模型安全研究提供了测试标准和实用防护措施，推动了该领域的安全发展。

Abstract: Recent advances in large image editing models have shifted the paradigm from text-driven instructions to vision-prompt editing, where user intent is inferred directly from visual inputs such as marks, arrows, and visual-text prompts. While this paradigm greatly expands usability, it also introduces a critical and underexplored safety risk: the attack surface itself becomes visual. In this work, we propose Vision-Centric Jailbreak Attack (VJA), the first visual-to-visual jailbreak attack that conveys malicious instructions purely through visual inputs. To systematically study this emerging threat, we introduce IESBench, a safety-oriented benchmark for image editing models. Extensive experiments on IESBench demonstrate that VJA effectively compromises state-of-the-art commercial models, achieving attack success rates of up to 80.9% on Nano Banana Pro and 70.1% on GPT-Image-1.5. To mitigate this vulnerability, we propose a training-free defense based on introspective multimodal reasoning, which substantially improves the safety of poorly aligned models to a level comparable with commercial systems, without auxiliary guard models and with negligible computational overhead. Our findings expose new vulnerabilities, provide both a benchmark and practical defense to advance safe and trustworthy modern image editing systems. Warning: This paper contains offensive images created by large image editing models.

</details>


### [9] [DEGMC: Denoising Diffusion Models Based on Riemannian Equivariant Group Morphological Convolutions](https://arxiv.org/abs/2602.10221)
*El Hadji S. Diop,Thierno Fall,Mohamed Daoudi*

Main category: cs.CV

TL;DR: 本文针对DDPM中的几何特征提取和网络等变性问题，引入了群形态学卷积和欧氏群等变性，并在经典数据集上取得优于基线的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的DDPM模型（如U-net）仅具备平移等变性，难以有效捕捉诸如旋转、反射等更广泛的几何特征，导致对复杂几何结构和对称性的学习不足。

Method: 提出结合欧氏群（包含旋转、反射和置换）的几何方法，采用Riemann流形上的群形态学卷积，这些卷积来自一类一阶Hamilton-Jacobi型PDE的粘性解，实现多尺度的形态学膨胀与腐蚀。模型中加入对流项，并用特征线法求解，以更好捕捉非线性和细长几何结构，并嵌入对称性。

Result: 在MNIST、RotoMNIST和CIFAR-10数据集上的实验结果表明，该方法在生成效果上明显优于基线DDPM模型。

Conclusion: 本文提出的新方法显著提升了DDPM对几何特征的建模能力，并增强了网络对旋转、反射等对称变换的鲁棒性与表达力。

Abstract: In this work, we address two major issues in recent Denoising Diffusion Probabilistic Models (DDPM): {\bf 1)} geometric key feature extraction and {\bf 2)} network equivariance. Since the DDPM prediction network relies on the U-net architecture, which is theoretically only translation equivariant, we introduce a geometric approach combined with an equivariance property of the more general Euclidean group, which includes rotations, reflections, and permutations. We introduce the notion of group morphological convolutions in Riemannian manifolds, which are derived from the viscosity solutions of first-order Hamilton-Jacobi-type partial differential equations (PDEs) that act as morphological multiscale dilations and erosions. We add a convection term to the model and solve it using the method of characteristics. This helps us better capture nonlinearities, represent thin geometric structures, and incorporate symmetries into the learning process. Experimental results on the MNIST, RotoMNIST, and CIFAR-10 datasets show noticeable improvements compared to the baseline DDPM model.

</details>


### [10] [XSPLAIN: XAI-enabling Splat-based Prototype Learning for Attribute-aware INterpretability](https://arxiv.org/abs/2602.10239)
*Dominik Galus,Julia Farganus,Tymoteusz Zapala,Mikołaj Czachorowski,Piotr Borycki,Przemysław Spurek,Piotr Syga*

Main category: cs.CV

TL;DR: 提出了XSPLAIN，首个专为3D Gaussian Splatting（3DGS）分类设计的原型解释框架，提升了3DGS模型的可解释性并获得用户信任。


<details>
  <summary>Details</summary>
Motivation: 虽然3DGS在高保真重建方面表现出色，但其模型生成过程和分类结果缺乏可解释性，限制了其在关键领域的应用。现有的3D表示解释方法（如点云）未能有效适配3DGS，主要因为基于显著图的方法难以表达高斯基元的体积一致性。

Method: 提出XSPLAIN框架，结合基于体素聚合的PointNet骨干网络与可逆正交变换，使特征通道解耦同时严格保持分类边界。通过原型训练样本进行解释，实现“以此类推”的直观理解，提升可解释性而不影响分类性能。

Result: 基于N=51的用户研究，XSPLAIN的解释被48.4%的参与者选为最佳，显著优于基线方法（p<0.001），验证了其透明性和用户信任。

Conclusion: XSPLAIN在无损分类性能下，显著提升了3DGS分类的可解释性和直观性，有望推动其在关键领域的应用，并增强用户对AI模型的信任。

Abstract: 3D Gaussian Splatting (3DGS) has rapidly become a standard for high-fidelity 3D reconstruction, yet its adoption in multiple critical domains is hindered by the lack of interpretability of the generation models as well as classification of the Splats. While explainability methods exist for other 3D representations, like point clouds, they typically rely on ambiguous saliency maps that fail to capture the volumetric coherence of Gaussian primitives. We introduce XSPLAIN, the first ante-hoc, prototype-based interpretability framework designed specifically for 3DGS classification. Our approach leverages a voxel-aggregated PointNet backbone and a novel, invertible orthogonal transformation that disentangles feature channels for interpretability while strictly preserving the original decision boundaries. Explanations are grounded in representative training examples, enabling intuitive ``this looks like that'' reasoning without any degradation in classification performance. A rigorous user study (N=51) demonstrates a decisive preference for our approach: participants selected XSPLAIN explanations 48.4\% of the time as the best, significantly outperforming baselines $(p<0.001)$, showing that XSPLAIN provides transparency and user trust. The source code for this work is available at: https://github.com/Solvro/ml-splat-xai

</details>


### [11] [PMMA: The Polytechnique Montreal Mobility Aids Dataset](https://arxiv.org/abs/2602.10259)
*Qingwu Liu,Nicolas Saunier,Guillaume-Alexandre Bilodeau*

Main category: cs.CV

TL;DR: 本文提出了一个新的用于辅助出行行人检测的数据集PMMA，并基于该数据集对多种主流目标检测和跟踪模型进行了实验评价。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏针对使用辅助出行工具（如轮椅、拐杖和助行器）行人的目标检测与跟踪数据集，限制了相关研究与实际应用的发展。

Method: 作者在室外环境中收集了包含9类辅助出行行人的视频数据，构建了PMMA数据集，并在MMDetection平台下，用7种目标检测模型和3种跟踪算法进行基准测试。

Result: YOLOX、Deformable DETR和Faster R-CNN在检测任务上表现最佳，三种跟踪算法（ByteTrack、BOT-SORT、OC-SORT）差异较小。

Conclusion: PMMA数据集丰富了辅助出行行人的检测与跟踪研究，相关基准实验为后续工作提供了参考，数据及代码已公开，有助于推动领域发展。

Abstract: This study introduces a new object detection dataset of pedestrians using mobility aids, named PMMA. The dataset was collected in an outdoor environment, where volunteers used wheelchairs, canes, and walkers, resulting in nine categories of pedestrians: pedestrians, cane users, two types of walker users, whether walking or resting, five types of wheelchair users, including wheelchair users, people pushing empty wheelchairs, and three types of users pushing occupied wheelchairs, including the entire pushing group, the pusher and the person seated on the wheelchair. To establish a benchmark, seven object detection models (Faster R-CNN, CenterNet, YOLOX, DETR, Deformable DETR, DINO, and RT-DETR) and three tracking algorithms (ByteTrack, BOT-SORT, and OC-SORT) were implemented under the MMDetection framework. Experimental results show that YOLOX, Deformable DETR, and Faster R-CNN achieve the best detection performance, while the differences among the three trackers are relatively small. The PMMA dataset is publicly available at https://doi.org/10.5683/SP3/XJPQUG, and the video processing and model training code is available at https://github.com/DatasetPMMA/PMMA.

</details>


### [12] [Colorimeter-Supervised Skin Tone Estimation from Dermatoscopic Images for Fairness Auditing](https://arxiv.org/abs/2602.10265)
*Marin Benčević,Krešimir Romić,Ivana Hartmann Tolić,Irena Galić*

Main category: cs.CV

TL;DR: 本论文提出了一种利用神经网络自动标注皮肤颜色类型的新方法，并公开了相关工具，用于皮肤镜图像数据集的公平性分析，填补了现有数据缺乏可靠皮肤色注释的空白。


<details>
  <summary>Details</summary>
Motivation: 皮肤镜神经网络模型在不同肤色人群中表现存在差异，但目前公共数据集缺少准确的皮肤色注释，严重限制了模型公平性和偏见分析，因此亟需一种高效、准确的肤色自动标注方法。

Method: 作者训练了两种神经网络：一是通过序贯回归预测Fitzpatrick皮肤类型，二是通过回归预测色差指数ITA，二者分别以真实Fitzpatrick标签和色差仪测量值作为目标。同时，这两个模型通过大量真实和合成皮肤镜及临床图像预训练。

Result: Fitzpatrick模型的标注效果可与众包人工标注媲美，ITA模型对色差仪数据的预测远优于简单像素平均方法；对ISIC 2020和MILK10k数据集应用后，发现第五、六型肤色比例不足1%。

Conclusion: 作者发布了开源代码和预训练模型，为快速皮肤色标注和模型偏见审核提供工具，这是首个经色差仪测量验证的皮肤镜肤色估计算法，证实了不同肤色人群间性能差异问题值得关注。

Abstract: Neural-network-based diagnosis from dermatoscopic images is increasingly used for clinical decision support, yet studies report performance disparities across skin tones. Fairness auditing of these models is limited by the lack of reliable skin-tone annotations in public dermatoscopy datasets. We address this gap with neural networks that predict Fitzpatrick skin type via ordinal regression and the Individual Typology Angle (ITA) via color regression, using in-person Fitzpatrick labels and colorimeter measurements as targets. We further leverage extensive pretraining on synthetic and real dermatoscopic and clinical images. The Fitzpatrick model achieves agreement comparable to human crowdsourced annotations, and ITA predictions show high concordance with colorimeter-derived ITA, substantially outperforming pixel-averaging approaches. Applying these estimators to ISIC 2020 and MILK10k, we find that fewer than 1% of subjects belong to Fitzpatrick types V and VI. We release code and pretrained models as an open-source tool for rapid skin-tone annotation and bias auditing. This is, to our knowledge, the first dermatoscopic skin-tone estimation neural network validated against colorimeter measurements, and it supports growing evidence of clinically relevant performance gaps across skin-tone groups.

</details>


### [13] [ERGO: Excess-Risk-Guided Optimization for High-Fidelity Monocular 3D Gaussian Splatting](https://arxiv.org/abs/2602.10278)
*Zehua Ma,Hanhui Li,Zhenyu Xie,Xiaonan Luo,Michael Kampffmeyer,Feng Gao,Xiaodan Liang*

Main category: cs.CV

TL;DR: 该论文提出了一种新的自适应优化框架（ERGO），用于从单张图片生成高质量的3D内容，在应对监督噪声和提升几何与纹理质量方面效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单幅图像生成3D内容非常具有挑战性，主要因为遮挡区域缺少几何和纹理信息。已有方法利用生成模型合成辅助视角提供监督，但这些视角往往包含几何不一致和纹理不匹配，导致最终重建的3D内容存在伪影。因此，开发能有效处理这种有噪声监督的新方法具有重要意义。

Method: 提出ERGO自适应优化框架，将3D Gaussian splatting中的损失分解为excess risk（当前与最优参数的差距）和Bayes error（不可避免的噪声）。对每个视角动态估计excess risk，并自适应调整损失权重。此外，ERGO结合了几何感知和纹理感知的目标函数，实现全局与局部的协同优化，从而提升鲁棒性和重建质量。

Result: 在Google Scanned Objects与OmniObject3D等主流数据集上，ERGO展现出对监督噪声的较强鲁棒性，能够持续提升重建3D内容的几何精度和纹理质量，并在各项指标上超过了当前领先的方法。

Conclusion: ERGO提供了一种更为鲁棒、智能的单图像3D重建解决方案，通过对损失的分解与自适应优化，有效缓解了噪声导致的重建伪影问题，为3D内容生成任务带来新的性能提升。

Abstract: Generating 3D content from a single image remains a fundamentally challenging and ill-posed problem due to the inherent absence of geometric and textural information in occluded regions. While state-of-the-art generative models can synthesize auxiliary views to provide additional supervision, these views inevitably contain geometric inconsistencies and textural misalignments that propagate and amplify artifacts during 3D reconstruction. To effectively harness these imperfect supervisory signals, we propose an adaptive optimization framework guided by excess risk decomposition, termed ERGO. Specifically, ERGO decomposes the optimization losses in 3D Gaussian splatting into two components, i.e., excess risk that quantifies the suboptimality gap between current and optimal parameters, and Bayes error that models the irreducible noise inherent in synthesized views. This decomposition enables ERGO to dynamically estimate the view-specific excess risk and adaptively adjust loss weights during optimization. Furthermore, we introduce geometry-aware and texture-aware objectives that complement the excess-risk-derived weighting mechanism, establishing a synergistic global-local optimization paradigm. Consequently, ERGO demonstrates robustness against supervision noise while consistently enhancing both geometric fidelity and textural quality of the reconstructed 3D content. Extensive experiments on the Google Scanned Objects dataset and the OmniObject3D dataset demonstrate the superiority of ERGO over existing state-of-the-art methods.

</details>


### [14] [A Low-Rank Defense Method for Adversarial Attack on Diffusion Models](https://arxiv.org/abs/2602.10319)
*Jiaxuan Zhu,Siyu Huang*

Main category: cs.CV

TL;DR: 本文提出了一种高效的防御策略LoRD（Low-Rank Defense），主要用于防御针对潜在扩散模型（LDMs）的对抗攻击。通过实验证明LoRD能显著提升模型的防御能力，同时保持生成图像质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型和其微调过程面临越来越多对抗攻击，这威胁了该类模型的实际应用。为防止对抗攻击被滥用，亟需有效的防御机制。

Method: 作者提出LoRD防御方法，将低秩自适应(LoRA)模块、样本合并思想和一个平衡参数结合，以检测和防御对抗样本，并构建了基于LoRD的防御流程用于扩散模型。该方法允许模型在对抗和干净样本混合微调下依然生成高质量图像。

Result: 在面部和风景图像任务上进行了广泛实验，LoRD方法在防御性能上显著优于基线方法。

Conclusion: LoRD是一种高效的对抗防御策略，能够有效保护潜在扩散模型免受对抗攻击，并在保持图像质量的前提下提升安全性。

Abstract: Recently, adversarial attacks for diffusion models as well as their fine-tuning process have been developed rapidly. To prevent the abuse of these attack algorithms from affecting the practical application of diffusion models, it is critical to develop corresponding defensive strategies. In this work, we propose an efficient defensive strategy, named Low-Rank Defense (LoRD), to defend the adversarial attack on Latent Diffusion Models (LDMs). LoRD introduces the merging idea and a balance parameter, combined with the low-rank adaptation (LoRA) modules, to detect and defend the adversarial samples. Based on LoRD, we build up a defense pipeline that applies the learned LoRD modules to help diffusion models defend against attack algorithms. Our method ensures that the LDM fine-tuned on both adversarial and clean samples can still generate high-quality images. To demonstrate the effectiveness of our approach, we conduct extensive experiments on facial and landscape images, and our method shows significantly better defense performance compared to the baseline methods.

</details>


### [15] [Flow Matching with Uncertainty Quantification and Guidance](https://arxiv.org/abs/2602.10326)
*Juyeop Han,Lukas Lao Beyer,Sertac Karaman*

Main category: cs.CV

TL;DR: 本文提出了一种新方法UA-Flow，通过引入不确定性感知机制，提升生成模型的样本质量与可靠性评估。实验表明，该方法生成的不确定性信号与样本质量高度相关，并可用于指导生成过程，提高生成效果。


<details>
  <summary>Details</summary>
Motivation: 虽然基于采样的生成模型（如flow matching）非常成功，但仍存在生成样本质量不稳定的问题。缺乏对单一样本可靠性和不确定性的有效评估方法，限制了这些模型在高质量生成任务中的应用。作者希望通过更好地度量和利用生成过程中的不确定性，提高输出质量。

Method: 作者提出了UA-Flow，在标准flow matching基础上，扩展模型输出，使其不仅预测速度场，还同时给出随样本变化的不确定性（异方差不确定性）。通过在流动动力学中传播速度的不确定性，UA-Flow可为每个样本估算单独的不确定性。作者还探索了如何利用这些不确定性信号，通过不确定性感知的分类器引导和无分类器引导，提升生成样本的可靠性和质量。

Result: 在图像生成任务上，UA-Flow生成的不确定性信号与样本真实的保真度（fidelity）表现出比传统方法更高的相关性。基于不确定性进行引导采样后，能进一步提升生成样本的质量。

Conclusion: UA-Flow为flow matching类生成模型提供了有效的不确定性评估机制，能作为样本可靠性的信号，并通过不确定性引导进一步提升生成样本的质量。该方法简单、有效，有望推动生成模型在高可靠性场景下的进一步应用。

Abstract: Despite the remarkable success of sampling-based generative models such as flow matching, they can still produce samples of inconsistent or degraded quality. To assess sample reliability and generate higher-quality outputs, we propose uncertainty-aware flow matching (UA-Flow), a lightweight extension of flow matching that predicts the velocity field together with heteroscedastic uncertainty. UA-Flow estimates per-sample uncertainty by propagating velocity uncertainty through the flow dynamics. These uncertainty estimates act as a reliability signal for individual samples, and we further use them to steer generation via uncertainty-aware classifier guidance and classifier-free guidance. Experiments on image generation show that UA-Flow produces uncertainty signals more highly correlated with sample fidelity than baseline methods, and that uncertainty-guided sampling further improves generation quality.

</details>


### [16] [Conditional Uncertainty-Aware Political Deepfake Detection with Stochastic Convolutional Neural Networks](https://arxiv.org/abs/2602.10343)
*Rafael-Petruţ Gardoş*

Main category: cs.CV

TL;DR: 本文提出了一种结合不确定性意识的政治深度伪造图像检测方法，通过随机卷积神经网络提升对高风险应用场景下的可靠性评估，并系统比较了多种不确定性处理方案。


<details>
  <summary>Details</summary>
Motivation: 当前高真实感政治深度伪造图像的生成能力提升，威胁到信息完整性及民主进程，并且现有自动化检测系统常常只给出单一预测结果，缺乏对预测可靠性的标志，无法满足实际高风险政治环境的需求。

Method: 作者构建了一个政治相关的二分类图像数据集，并在ResNet-18和EfficientNet-B4两个预训练模型基础上进行微调。比较了确定性推理、单次随机推断、蒙特卡洛dropout多次前向、温度缩放和集成不确定性代理等多种方法，结合ROC-AUC、混淆矩阵、校准指标和分布外测试全面评估方法表现。

Result: 结果表明，经校准的概率输出和不确定性估计有助于实现基于风险的内容管理策略。信心区间的系统分析显示出在何种情况下，不确定性信息能为检测系统带来实际的操作价值，同时定位了不确定性方法在政治伪造检测中的优缺点。

Conclusion: 本文证明了结合不确定性建模的深度伪造检测方法可有效提升高风险政治场景下的决策可靠性，并明确了当前方案的能力界限和未来优化方向。

Abstract: Recent advances in generative image models have enabled the creation of highly realistic political deepfakes, posing risks to information integrity, public trust, and democratic processes. While automated deepfake detectors are increasingly deployed in moderation and investigative pipelines, most existing systems provide only point predictions and fail to indicate when outputs are unreliable, being an operationally critical limitation in high-stakes political contexts. This work investigates conditional, uncertainty-aware political deepfake detection using stochastic convolutional neural networks within an empirical, decision-oriented reliability framework. Rather than treating uncertainty as a purely Bayesian construct, it is evaluated through observable criteria, including calibration quality, proper scoring rules, and its alignment with prediction errors under both global and confidence-conditioned analyses. A politically focused binary image dataset is constructed via deterministic metadata filtering from a large public real-synthetic corpus. Two pretrained CNN backbones (ResNet-18 and EfficientNet-B4) are fully fine-tuned for classification. Deterministic inference is compared with single-pass stochastic prediction, Monte Carlo dropout with multiple forward passes, temperature scaling, and ensemble-based uncertainty surrogates. Evaluation reports ROC-AUC, thresholded confusion matrices, calibration metrics, and generator-disjoint out-of-distribution performance. Results demonstrate that calibrated probabilistic outputs and uncertainty estimates enable risk-aware moderation policies. A systematic confidence-band analysis further clarifies when uncertainty provides operational value beyond predicted confidence, delineating both the benefits and limitations of uncertainty-aware deepfake detection in political settings.

</details>


### [17] [Monte Carlo Maximum Likelihood Reconstruction for Digital Holography with Speckle](https://arxiv.org/abs/2602.10344)
*Xi Chen,Arian Maleki,Shirin Jalali*

Main category: cs.CV

TL;DR: 本论文提出了一种基于随机线性代数的最大似然估计（MLE）优化方法，实现了无需高维矩阵求逆即可高效处理高分辨率数字全息重建中的散斑噪声问题。新方法兼具物理精确孔径建模和计算效率，显著提升了重建质量，并且优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在相干成像中，散斑噪声作为乘性噪声给图像重建带来巨大挑战。尽管最大似然估计理论上可以有效抑制散斑，但其在考虑实际有限孔径数字全息时计算量极大，尤其在高分辨率情况下需高维矩阵求逆，难以实际应用。因此需要能够既保持物理真实又具有良好计算可扩展性的解决方案。

Method: 作者提出了一种基于随机线性代数的优化方法，其中利用感知矩阵的结构特性，采用共轭梯度法在梯度计算中规避显式矩阵求逆，显著降低了高分辨率情况下的计算负担。该方法被称为“带Monte Carlo估计的投影梯度下降（PGD-MC）”，可支持多样且物理精确的孔径建模，无需简化假设，还能结合不同的去噪正则化器。

Result: 实验表明，PGD-MC 方法对不同物理精确的孔径模型具有鲁棒性，在重建质量（准确性）和计算效率上均显著优于以往的Plug-and-Play模型。三种代表性去噪器的实验证明，该方法在数字全息的高分辨率场景下灵活、有效，并且速度和精度均超过前人方法。

Conclusion: PGD-MC为有限孔径数字全息提供了灵活、高效、物理准确的最大似然重建框架，在实际应用中有效提升了分辨率、速度和重建质量，有望成为相关成像领域的新标准。

Abstract: In coherent imaging, speckle is statistically modeled as multiplicative noise, posing a fundamental challenge for image reconstruction. While maximum likelihood estimation (MLE) provides a principled framework for speckle mitigation, its application to coherent imaging system such as digital holography with finite apertures is hindered by the prohibitive cost of high-dimensional matrix inversion, especially at high resolutions. This computational burden has prevented the use of MLE-based reconstruction with physically accurate aperture modeling. In this work, we propose a randomized linear algebra approach that enables scalable MLE optimization without explicit matrix inversions in gradient computation. By exploiting the structural properties of sensing matrix and using conjugate gradient for likelihood gradient evaluation, the proposed algorithm supports accurate aperture modeling without the simplifying assumptions commonly imposed for tractability. We term the resulting method projected gradient descent with Monte Carlo estimation (PGD-MC). The proposed PGD-MC framework (i) demonstrates robustness to diverse and physically accurate aperture models, (ii) achieves substantial improvements in reconstruction quality and computational efficiency, and (iii) scales effectively to high-resolution digital holography. Extensive experiments incorporating three representative denoisers as regularization show that PGD-MC provides a flexible and effective MLE-based reconstruction framework for digital holography with finite apertures, consistently outperforming prior Plug-and-Play model-based iterative reconstruction methods in both accuracy and speed. Our code is available at: https://github.com/Computational-Imaging-RU/MC_Maximum_Likelihood_Digital_Holography_Speckle.

</details>


### [18] [Comp2Comp: Open-Source Software with FDA-Cleared Artificial Intelligence Algorithms for Computed Tomography Image Analysis](https://arxiv.org/abs/2602.10364)
*Adrit Rao,Malte Jensen,Andrea T. Fisher,Louis Blankemeier,Pauline Berens,Arash Fereydooni,Seth Lirette,Eren Alkan,Felipe C. Kitamura,Juan M. Zambrano Chaves,Eduardo Reis,Arjun Desai,Marc H. Willis,Jason Hom,Andrew Johnston,Leon Lenchik,Robert D. Boutin,Eduardo M. J. M. Farina,Augusto S. Serpa,Marcelo S. Takahashi,Jordan Perchik,Steven A. Rothenberg,Jamie L. Schroeder,Ross Filice,Leonardo K. Bittencourt,Hari Trivedi,Marly van Assen,John Mongan,Kimberly Kallianos,Oliver Aalami,Akshay S. Chaudhari*

Main category: cs.CV

TL;DR: 本文介绍了Comp2Comp包中两个完全开源、获得FDA 510(k)认证的深度学习管线（腹主动脉定量和骨密度估计）的开发与验证，实现了CT影像的机会性分析，确保准确性并增强透明度。


<details>
  <summary>Details</summary>
Motivation: 当前许多开源医学影像分析工具缺乏严格验证，而商用工具又缺乏透明度，导致实际部署时出现意外失败。为此，需要开发既经过严格验证、又公开透明的分析算法。

Method: 开发了Comp2Comp软件包，包含腹主动脉定量（AAQ）和骨密度（BMD）两个深度学习算法，均为开源。AAQ对腹主动脉进行分割以评估瘤体大小，BMD对椎体进行分割以估算骨小梁骨密度和骨质疏松风险。分别对来自4家外部机构的258例腹主动脉瘤和371例骨密度数据进行外部验证，对照放射科医师人工测量及DXA扫描。

Result: AAQ平均绝对误差为1.57 mm（95% CI 1.38–1.80 mm），BMD二分类灵敏度为81.0%（95% CI 74.0–86.8%），特异性为78.4%（95% CI 72.3–83.7%），均达到了临床可用的准确度。

Conclusion: Comp2Comp包中的AAQ与BMD算法开源透明，准确性高，适合临床应用，有助于医院和研究者在临床推广前深入测试和研究，同时提升FDA认证算法的透明度。

Abstract: Artificial intelligence allows automatic extraction of imaging biomarkers from already-acquired radiologic images. This paradigm of opportunistic imaging adds value to medical imaging without additional imaging costs or patient radiation exposure. However, many open-source image analysis solutions lack rigorous validation while commercial solutions lack transparency, leading to unexpected failures when deployed. Here, we report development and validation for two of the first fully open-sourced, FDA-510(k)-cleared deep learning pipelines to mitigate both challenges: Abdominal Aortic Quantification (AAQ) and Bone Mineral Density (BMD) estimation are both offered within the Comp2Comp package for opportunistic analysis of computed tomography scans. AAQ segments the abdominal aorta to assess aneurysm size; BMD segments vertebral bodies to estimate trabecular bone density and osteoporosis risk. AAQ-derived maximal aortic diameters were compared against radiologist ground-truth measurements on 258 patient scans enriched for abdominal aortic aneurysms from four external institutions. BMD binary classifications (low vs. normal bone density) were compared against concurrent DXA scan ground truths obtained on 371 patient scans from four external institutions. AAQ had an overall mean absolute error of 1.57 mm (95% CI 1.38-1.80 mm). BMD had a sensitivity of 81.0% (95% CI 74.0-86.8%) and specificity of 78.4% (95% CI 72.3-83.7%). Comp2Comp AAQ and BMD demonstrated sufficient accuracy for clinical use. Open-sourcing these algorithms improves transparency of typically opaque FDA clearance processes, allows hospitals to test the algorithms before cumbersome clinical pilots, and provides researchers with best-in-class methods.

</details>


### [19] [HII-DPO: Eliminate Hallucination via Accurate Hallucination-Inducing Counterfactual Images](https://arxiv.org/abs/2602.10425)
*Yilin Yang,Zhenghui Guo,Yuke Wang,Omprakash Gnawali,Sheng Di,Chengming Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种通过合成诱导幻觉的图像来分析和缓解大规模视觉-语言模型（VLMs）幻觉的新方法，并提出新的评测基准和对齐数据集。作者的方法在减少幻觉方面显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs虽然在多模态任务中表现优异，但由于固有的语言偏见，容易出现幻觉现象（即描述与图像无关的对象）。现有的缓解方法未能深入探究幻觉背后的语言驱动模式，因此有必要提出针对性更强的研究和评测方法。

Method: 作者设计了一套新颖流程，能够合成专门诱发模型幻觉的图像（HII），并借此揭示VLMs在特定场景下倾向于描述场景典型物体的幻觉模式。进一步，作者建立Masked-Object-Hallucination (MOH)基准，评估主流对齐方法的抗幻觉能力，并用HII构建高质量偏好数据集，精细化模型对齐。

Result: 实验证明，所提出的方法能有效减缓VLMs幻觉出现，同时不损害模型整体能力。在标准幻觉基准测试中，作者的方法比现有最佳技术提升了最高38%的表现。

Conclusion: 利用合成的诱导幻觉图像和更加严谨的评测方法，可以显著提升VLMs在抵御幻觉上的表现，为进一步模型对齐和多模态理解任务打下基础。

Abstract: Large Vision-Language Models (VLMs) have achieved remarkable success across diverse multimodal tasks but remain vulnerable to hallucinations rooted in inherent language bias. Despite recent progress, existing hallucination mitigation methods often overlook the underlying hallucination patterns driven by language bias. In this work, we design a novel pipeline to accurately synthesize Hallucination-Inducing Images (HIIs). Using synthesized HIIs, we reveal a consistent scene-conditioned hallucination pattern: models tend to mention objects that are highly typical of the scene even when visual evidence is removed. To quantify the susceptibility of VLMs to this hallucination pattern, we establish the Masked-Object-Hallucination (MOH) benchmark to rigorously evaluate existing state-of-the-art alignment frameworks. Finally, we leverage HIIs to construct high-quality preference datasets for fine-grained alignment. Experimental results demonstrate that our approach effectively mitigates hallucinations while preserving general model capabilities. Specifically, our method achieves up to a 38% improvement over the current state-of-the-art on standard hallucination benchmarks.

</details>


### [20] [Towards Remote Sensing Change Detection with Neural Memory](https://arxiv.org/abs/2602.10491)
*Zhenyu Yang,Gensheng Pei,Yazhou Yao,Tianfei Zhou,Lizhong Ding,Fumin Shen*

Main category: cs.CV

TL;DR: 本文提出了一种名为ChangeTitans的远程感知变化检测新框架，在准确性和计算效率之间取得了良好平衡，并在多个基准数据集上实现了最新的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有遥感变化检测方法在捕捉长距离依赖和保持计算效率间存在权衡。Transformer虽能建模全局上下文，但计算复杂度高，线性注意力方法则难以捕捉复杂的时空关系。故需一种高效且能捕捉全局依赖的新方法。

Method: 作者提出ChangeTitans框架，包括：1) VTitans——一种结合神经记忆和分段局部注意力的新型视觉backbone，可高效捕捉长距离依赖；2) VTitans-Adapter——层级特征细化模块，实现多尺度特征优化；3) TS-CBAM——双流跨时序注意力融合模块，用于抑制伪变化并提升检测精度。

Result: 在LEVER-CD、WHU-CD、LEVER-CD+及SYSU-CD四大基准数据集上进行实验，ChangeTitans在LEVER-CD数据集上获得了84.36%的IoU和91.52%的F1分数，性能领先，同时计算开销较低。

Conclusion: ChangeTitans有效兼顾了检测精度和计算效能，为遥感变化检测任务提供了新颖、强有力的技术手段，对相关领域具有实际推广价值。

Abstract: Remote sensing change detection is essential for environmental monitoring, urban planning, and related applications. However, current methods often struggle to capture long-range dependencies while maintaining computational efficiency. Although Transformers can effectively model global context, their quadratic complexity poses scalability challenges, and existing linear attention approaches frequently fail to capture intricate spatiotemporal relationships. Drawing inspiration from the recent success of Titans in language tasks, we present ChangeTitans, the Titans-based framework for remote sensing change detection. Specifically, we propose VTitans, the first Titans-based vision backbone that integrates neural memory with segmented local attention, thereby capturing long-range dependencies while mitigating computational overhead. Next, we present a hierarchical VTitans-Adapter to refine multi-scale features across different network layers. Finally, we introduce TS-CBAM, a two-stream fusion module leveraging cross-temporal attention to suppress pseudo-changes and enhance detection accuracy. Experimental evaluations on four benchmark datasets (LEVIR-CD, WHU-CD, LEVIR-CD+, and SYSU-CD) demonstrate that ChangeTitans achieves state-of-the-art results, attaining \textbf{84.36\%} IoU and \textbf{91.52\%} F1-score on LEVIR-CD, while remaining computationally competitive.

</details>


### [21] [End-to-End LiDAR optimization for 3D point cloud registration](https://arxiv.org/abs/2602.10492)
*Siddhant Katyan,Marc-André Gardner,Jean-François Lalonde*

Main category: cs.CV

TL;DR: 本文提出了一种自适应LiDAR感知框架，可动态调整激光雷达参数，实现数据采集和点云配准过程的联合优化，从而提升配准准确性和效率。方法在CARLA仿真中表现优于传统固定参数方案。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR点云配准方法往往与传感器参数独立设计，导致采集的数据质量与配准性能不能同时最优，且需要大量参数调优和噪声处理，增加了计算负担。

Method: 提出一种自适应LiDAR感知系统，将点云配准反馈引入传感器参数调整环节，实现传感器数据采集参数和配准超参数的联合自适应优化，使点的密度、噪声和稀疏性相互权衡。

Result: 在CARLA仿真平台上评估显示，该方法在配准准确性和效率上均优于传统固定参数基线方法，而且具有良好的泛化能力。

Conclusion: 自适应LiDAR联合优化框架可以提升点云配准的效果和效率，具有推动自动驾驶与机器人感知应用的潜力。

Abstract: LiDAR sensors are a key modality for 3D perception, yet they are typically designed independently of downstream tasks such as point cloud registration. Conventional registration operates on pre-acquired datasets with fixed LiDAR configurations, leading to suboptimal data collection and significant computational overhead for sampling, noise filtering, and parameter tuning. In this work, we propose an adaptive LiDAR sensing framework that dynamically adjusts sensor parameters, jointly optimizing LiDAR acquisition and registration hyperparameters. By integrating registration feedback into the sensing loop, our approach optimally balances point density, noise, and sparsity, improving registration accuracy and efficiency. Evaluations in the CARLA simulation demonstrate that our method outperforms fixed-parameter baselines while retaining generalization abilities, highlighting the potential of adaptive LiDAR for autonomous perception and robotic applications.

</details>


### [22] [Characterizing and Optimizing the Spatial Kernel of Multi Resolution Hash Encodings](https://arxiv.org/abs/2602.10495)
*Tianxiang Dai,Jonathan Fan*

Main category: cs.CV

TL;DR: 本文对多分辨率哈希编码（MHE）的空间行为进行物理系统视角下的分析，提出用点扩散函数（PSF）来量化其空间分辨率和保真度，并提出旋转MHE（R-MHE）架构来缓解各向异性。


<details>
  <summary>Details</summary>
Motivation: MHE作为神经场的强大参数化工具，其空间行为缺乏物理系统层面的严谨解析，目前参数选择主要靠经验。理解其本质，有助于更好地优化和应用MHE。

Method: 作者首先通过分析MHE的点扩散函数（PSF）来刻画其空间响应，并推导了无碰撞情况下PSF的封闭表达式。研究了网格带来的各向异性、分辨率展宽等现象，并分析有限哈希容量造成的碰撞与噪声影响。基于这些理论分析，提出对不同分辨率层输入应用不同旋转的R-MHE架构以缓解各向异性。

Result: 理论推导得到PSF和经验分辨率（FWHM）主要受平均分辨率N_avg控制而非最大分辨率N_max。通过仿真和实验证实优化过程带来的分辨率展宽以及哈希碰撞导致的噪声和信噪比下降。R-MHE能有效降低各向异性，同时保持原有MHE的效率和参数规模。

Conclusion: 本文首次从物理系统角度系统分析MHE空间性能，建立了可量化的理论工具和优化新结构R-MHE，为神经场编码提供了更科学的参数化方法，推动相关领域从经验走向理论指导。

Abstract: Multi-Resolution Hash Encoding (MHE), the foundational technique behind Instant Neural Graphics Primitives, provides a powerful parameterization for neural fields. However, its spatial behavior lacks rigorous understanding from a physical systems perspective, leading to reliance on heuristics for hyperparameter selection. This work introduces a novel analytical approach that characterizes MHE by examining its Point Spread Function (PSF), which is analogous to the Green's function of the system. This methodology enables a quantification of the encoding's spatial resolution and fidelity. We derive a closed-form approximation for the collision-free PSF, uncovering inherent grid-induced anisotropy and a logarithmic spatial profile. We establish that the idealized spatial bandwidth, specifically the Full Width at Half Maximum (FWHM), is determined by the average resolution, $N_{\text{avg}}$. This leads to a counterintuitive finding: the effective resolution of the model is governed by the broadened empirical FWHM (and therefore $N_{\text{avg}}$), rather than the finest resolution $N_{\max}$, a broadening effect we demonstrate arises from optimization dynamics. Furthermore, we analyze the impact of finite hash capacity, demonstrating how collisions introduce speckle noise and degrade the Signal-to-Noise Ratio (SNR). Leveraging these theoretical insights, we propose Rotated MHE (R-MHE), an architecture that applies distinct rotations to the input coordinates at each resolution level. R-MHE mitigates anisotropy while maintaining the efficiency and parameter count of the original MHE. This study establishes a methodology based on physical principles that moves beyond heuristics to characterize and optimize MHE.

</details>


### [23] [The Garbage Dataset (GD): A Multi-Class Image Benchmark for Automated Waste Segregation](https://arxiv.org/abs/2602.10500)
*Suman Kunwar*

Main category: cs.CV

TL;DR: 本文介绍了一个用于垃圾分类的图像公开数据集GD，涵盖10类家庭垃圾，共包含13348张带标签图片，并用多种先进深度学习模型进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 自动垃圾分类对于环境保护和资源回收至关重要，目前缺乏高质量、公开且多样性强的垃圾图像数据集阻碍了机器学习在该领域的应用和研究。

Method: 作者收集了10类常见家庭垃圾的图片，采用多种采集方式并进行严格的数据校验、离群点检测等处理，分析了类别不均衡和视觉分离性，评估了背景复杂性。使用EfficientNetV2M/S、MobileNet、ResNet50/101等深度学习模型进行基准测试，并记录其性能及碳排放。

Result: GD数据集通过多模型验证，EfficientNetV2S在准确率与F1分数上表现最佳，但碳排放适中。数据分析发现类别存在不均衡、部分类别有高离群现象及亮度变化等影响。

Conclusion: GD数据集为垃圾识别研究提供了有价值的基准，同时揭示了类别不平衡、背景复杂、模型碳排放等实际挑战，对可持续环境应用和后续研究有很大促进作用。

Abstract: This study introduces the Garbage Dataset (GD), a publicly available image dataset designed to advance automated waste segregation through machine learning and computer vision. It's a diverse dataset covering 10 common household waste categories: metal, glass, biological, paper, battery, trash, cardboard, shoes, clothes, and plastic. The dataset comprises 13,348 labeled images collected through multiple methods, including DWaste mobile app and curated web sources. Methods included rigorous validation through checksums and outlier detection, analysis of class imbalance and visual separability via PCA/t-SNE, and assessment of background complexity using entropy and saliency measures. The dataset was benchmarked using state-of-the-art deep learning models (EfficientNetV2M, EfficientNetV2S, MobileNet, ResNet50, ResNet101) evaluated on performance metrics and operational carbon emissions. Experiment results indicate EfficientNetV2S achieved the highest performance with 96.19% accuracy and a 0.96 F1-score, though with a moderate carbon cost. Analysis revealed inherent dataset characteristics including class imbalance, a skew toward high-outlier classes (plastic, cardboard, paper), and brightness variations that require consideration. The main conclusion is that GD provides a valuable, real-world benchmark for waste classification research while highlighting important challenges such as class imbalance, background complexity, and environmental trade-offs in model selection that must be addressed for practical deployment. The dataset is publicly released to support further research in environmental sustainability applications.

</details>


### [24] [Med-SegLens: Latent-Level Model Diffing for Interpretable Medical Image Segmentation](https://arxiv.org/abs/2602.10508)
*Salma J. Ahmed,Emad A. Mohammed,Azam Asilian Bidgoli*

Main category: cs.CV

TL;DR: 提出了一种名为Med-SegLens的模型diff分析框架，使分割模型的激活特征可解释，并能够诊断模型失效、理解数据集漂移及实施纠错。


<details>
  <summary>Details</summary>
Motivation: 当前医学分割模型表现出色，但其内部机制黑箱化，导致模型失效原因难以分析，对数据集漂移的适应性不足，也限制了有针对性的优化和干预。

Method: 使用稀疏自编码器迫使SegFormer和U-Net的分割模型激活特征分解为可解释的潜在特征，并在不同架构、数据集（包括健康人群、成人、儿童、非洲胶质瘤人群）之间做潜在空间对齐，推断潜在变量与数据集漂移及模型失效之间的关系，尝试在潜在变量层面进行干预纠错。

Result: 发现了跨架构、跨数据集共享的稳定潜在表示，同时数据集漂移主要由群体特异性的潜在特征驱动。这些潜在特征为失效的因果瓶颈，通过在潜在变量层面有针对性的操作，可以在无需重新训练模型的前提下，修正70%的错误案例，并将Dice系数从39.4%提升至74.2%。

Conclusion: 通过潜在特征层面分析和干预，可有效诊断分割模型失效和缓解数据集漂移，为医学图像分割的鲁棒性与泛化提供实用新工具。

Abstract: Modern segmentation models achieve strong predictive performance but remain largely opaque, limiting our ability to diagnose failures, understand dataset shift, or intervene in a principled manner. We introduce Med-SegLens, a model-diffing framework that decomposes segmentation model activations into interpretable latent features using sparse autoencoders trained on SegFormer and U-Net. Through cross-architecture and cross-dataset latent alignment across healthy, adult, pediatric, and sub-Saharan African glioma cohorts, we identify a stable backbone of shared representations, while dataset shift is driven by differential reliance on population-specific latents. We show that these latents act as causal bottlenecks for segmentation failures, and that targeted latent-level interventions can correct errors and improve cross-dataset adaption without retraining, recovering performance in 70% of failure cases and improving Dice score from 39.4% to 74.2%. Our results demonstrate that latent-level model diffing provides a practical and mechanistic tool for diagnosing failures and mitigating dataset shift in segmentation models.

</details>


### [25] [1%>100%: High-Efficiency Visual Adapter with Complex Linear Projection Optimization](https://arxiv.org/abs/2602.10513)
*Dongshuo Yin,Xue Yang,Deng-Ping Fan,Shi-Min Hu*

Main category: cs.CV

TL;DR: 本文提出了一种新的适配器（CoLin），通过复杂线性投影优化，用低至1%新增参数实现高效视觉基础模型微调，在多项任务上优于全量微调和传统delta-tuning。


<details>
  <summary>Details</summary>
Motivation: 传统视觉基础模型全量微调成本高、效率低，尚无像在大语言模型上那样高效的delta-tuning迁移方案。因此需要探索更高效的视觉模型适配方法。

Method: 设计了一种低秩复数适配器，只在主干网络新增1%的参数；针对低秩复合矩阵训练收敛问题，提出了专门的损失函数以改进收敛性，并从理论分析收敛问题。

Result: 在目标检测、分割、图像分类和遥感旋转目标检测等任务上，CoLin仅用1%参数首次超越了全量微调和经典delta-tuning方法。

Conclusion: CoLin为视觉基础模型提供了高效、低参数量的适配策略，有望推动视觉模型的便捷部署。

Abstract: Deploying vision foundation models typically relies on efficient adaptation strategies, whereas conventional full fine-tuning suffers from prohibitive costs and low efficiency. While delta-tuning has proven effective in boosting the performance and efficiency of LLMs during adaptation, its advantages cannot be directly transferred to the fine-tuning pipeline of vision foundation models. To push the boundaries of adaptation efficiency for vision tasks, we propose an adapter with Complex Linear Projection Optimization (CoLin). For architecture, we design a novel low-rank complex adapter that introduces only about 1% parameters to the backbone. For efficiency, we theoretically prove that low-rank composite matrices suffer from severe convergence issues during training, and address this challenge with a tailored loss. Extensive experiments on object detection, segmentation, image classification, and rotated object detection (remote sensing scenario) demonstrate that CoLin outperforms both full fine-tuning and classical delta-tuning approaches with merely 1% parameters for the first time, providing a novel and efficient solution for deployment of vision foundation models. We release the code on https://github.com/DongshuoYin/CoLin.

</details>


### [26] [3DXTalker: Unifying Identity, Lip Sync, Emotion, and Spatial Dynamics in Expressive 3D Talking Avatars](https://arxiv.org/abs/2602.10516)
*Zhongju Wang,Zhenhong Sun,Beier Wang,Yifu Wang,Daoyi Dong,Huadong Mo,Hongdong Li*

Main category: cs.CV

TL;DR: 提出了3DXTalker，一种结合身份建模、丰富音频表达和空间动态可控性的音频驱动3D说话头像生成方法，能够在数字人及虚拟交流领域中更好地实现身份保持、口型同步、表情与头部动态控制，效果领先。


<details>
  <summary>Details</summary>
Motivation: 当前3D说话头像的生成在身份、表情和空间动态等多维度表现受限，主要因训练数据不足、可控性和音频表示能力有限，难以广泛应用于虚拟交流和数字人等需要高度表现力的场景。

Method: 提出3DXTalker，通过（1）2D转3D数据策划和特征解耦实现可拓展身份建模，提升对多身份的泛化能力；（2）引入逐帧幅度和情感提示作为补充音频信息，强化口型同步及细腻表情；（3）采用基于flow-matching的Transformer模型融合多种提示，实现连贯的面部动态输出；（4）支持基于文本提示的头部动态和风格化控制。

Result: 实验显示3DXTalker在唇动同步、情感表达和头部动作的三者统一表现上取得领先，能自然生成包含丰富表达和风格控制的三维说话头像，效果优于现有主流方法。

Conclusion: 3DXTalker系统性地解决了3D说话头像生成中的身份泛化、表达力和可控性难题，推动音频驱动虚拟人广泛应用，并为后续研究提供了统一高效的技术框架。

Abstract: Audio-driven 3D talking avatar generation is increasingly important in virtual communication, digital humans, and interactive media, where avatars must preserve identity, synchronize lip motion with speech, express emotion, and exhibit lifelike spatial dynamics, collectively defining a broader objective of expressivity. However, achieving this remains challenging due to insufficient training data with limited subject identities, narrow audio representations, and restricted explicit controllability. In this paper, we propose 3DXTalker, an expressive 3D talking avatar through data-curated identity modeling, audio-rich representations, and spatial dynamics controllability. 3DXTalker enables scalable identity modeling via 2D-to-3D data curation pipeline and disentangled representations, alleviating data scarcity and improving identity generalization. Then, we introduce frame-wise amplitude and emotional cues beyond standard speech embeddings, ensuring superior lip synchronization and nuanced expression modulation. These cues are unified by a flow-matching-based transformer for coherent facial dynamics. Moreover, 3DXTalker also enables natural head-pose motion generation while supporting stylized control via prompt-based conditioning. Extensive experiments show that 3DXTalker integrates lip synchronization, emotional expression, and head-pose dynamics within a unified framework, achieves superior performance in 3D talking avatar generation.

</details>


### [27] [MapVerse: A Benchmark for Geospatial Question Answering on Diverse Real-World Maps](https://arxiv.org/abs/2602.10518)
*Sharat Bhat,Harshita Khandelwal,Tushar Kataria,Vivek Gupta*

Main category: cs.CV

TL;DR: 提出了MapVerse，一个基于真实地图的大规模基准数据集，涵盖多种地图和问题类型，用以评估多模态地图推理能力。实验证明现有主流视觉语言模型在复杂空间推理任务上仍有明显不足。


<details>
  <summary>Details</summary>
Motivation: 目前用于评测视觉语言模型地图推理能力的数据集覆盖面窄、领域有限且内容常为人工生成，难以准确检测真实地理空间推理水平。作者希望填补这一评价空白。

Method: 作者构建了MapVerse基准，包括1025张真实世界地图、11837个人工设计的问答对，涵盖10类不同地图、多种问题类别。并对10个主流视觉语言模型在该基准上的表现进行整体及细粒度对比分析。

Result: 主流VLM在分类型任务上表现尚可，但在需要复杂空间推理的高级任务上，无论开源还是闭源模型均有显著不足。分类分析揭示了模型推理中的不同维度表现与视觉因素影响。

Conclusion: 现有视觉语言大模型在处理高级多模态地图推理任务时存在明显短板，MapVerse基准为后续模型改进和深入评估提供了有力工具与方向。

Abstract: Maps are powerful carriers of structured and contextual knowledge, encompassing geography, demographics, infrastructure, and environmental patterns. Reasoning over such knowledge requires models to integrate spatial relationships, visual cues, real-world context, and domain-specific expertise-capabilities that current large language models (LLMs) and vision-language models (VLMs) still struggle to exhibit consistently. Yet, datasets used to benchmark VLMs on map-based reasoning remain narrow in scope, restricted to specific domains, and heavily reliant on artificially generated content (outputs from LLMs or pipeline-based methods), offering limited depth for evaluating genuine geospatial reasoning. To address this gap, we present MapVerse, a large-scale benchmark built on real-world maps. It comprises 11,837 human-authored question-answer pairs across 1,025 maps, spanning ten diverse map categories and multiple question categories for each. The dataset provides a rich setting for evaluating map reading, interpretation, and multimodal reasoning. We evaluate ten state-of-the-art models against our benchmark to establish baselines and quantify reasoning gaps. Beyond overall performance, we conduct fine-grained categorical analyses to assess model inference across multiple dimensions and investigate the visual factors shaping reasoning outcomes. Our findings reveal that while current VLMs perform competitively on classification-style tasks, both open- and closed-source models fall short on advanced tasks requiring complex spatial reasoning.

</details>


### [28] [RealHD: A High-Quality Dataset for Robust Detection of State-of-the-Art AI-Generated Images](https://arxiv.org/abs/2602.10546)
*Hanzhe Yu,Yun Ye,Jintao Rong,Qi Xuan,Chen Ma*

Main category: cs.CV

TL;DR: 本文提出了一个包含超过73万张高质量、多类别的真实与AI生成图片的大规模数据集，并开发了一种基于图像噪声熵轻量化检测方法。在大规模和多样化的数据集上训练的检测模型表现出优越的泛化能力，为AI生成图像检测提供了有力基线。数据集及代码已公开。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的发展，高度逼真的伪造图片日益增多，增加了社会风险。现有用于检测AI生成图片的数据集在泛化性、质量、提示词复杂度和多样性等方面存在不足，无法支持对更强泛化和鲁棒性的检测方法研究。

Method: 1. 构建了一个覆盖多类别、超73万张的高质量数据集，既包含真实图片，也包含采用最先进技术（文本生成、修复、增强、人脸替换等）生成的伪造图片，并为每张生成图像标注生成方法与类别。2. 对修复图片提供二值掩码，便于分析。3. 提出一种基于图像NLM噪声熵的轻量化检测方法，将原图转换为噪声熵张量并进行分类。

Result: 大量实验表明，用本数据集训练的检测模型在与其他数据集相比时表现出更强泛化能力。所提出的基于噪声熵的方法在检测任务上取得了具竞争力的表现，成为有力的基线。

Conclusion: 本工作为AI生成图片检测提供了高质量的基准数据集和有效的检测方法，有力推动了领域发展，对后续研究具有重要意义。数据集和代码已公开，将便利于相关社区和产业的研究。

Abstract: The rapid advancement of generative AI has raised concerns about the authenticity of digital images, as highly realistic fake images can now be generated at low cost, potentially increasing societal risks. In response, several datasets have been established to train detection models aimed at distinguishing AI-generated images from real ones. However, existing datasets suffer from limited generalization, low image quality, overly simple prompts, and insufficient image diversity. To address these limitations, we propose a high-quality, large-scale dataset comprising over 730,000 images across multiple categories, including both real and AI-generated images. The generated images are synthesized via state-of-the-art methods, including text-to-image generation (guided by over 10,000 carefully designed prompts), image inpainting, image refinement, and face swapping. Each generated image is annotated with its generation method and category. Inpainting images further include binary masks to indicate inpainted regions, providing rich metadata for analysis. Compared to existing datasets, detection models trained on our dataset demonstrate superior generalization capabilities. Our dataset not only serves as a strong benchmark for evaluating detection methods but also contributes to advancing the robustness of AI-generated image detection techniques. Building upon this, we propose a lightweight detection method based on image noise entropy, which transforms the original image into an entropy tensor of Non-Local Means (NLM) noise before classification. Extensive experiments demonstrate that models trained on our dataset achieve strong generalization, and our method delivers competitive performance, establishing a solid baseline for future research. The dataset and source code are publicly available at https://real-hd.github.io.

</details>


### [29] [Enhancing Weakly Supervised Multimodal Video Anomaly Detection through Text Guidance](https://arxiv.org/abs/2602.10549)
*Shengyang Sun,Jiashen Hua,Junyi Feng,Xiaojin Gong*

Main category: cs.CV

TL;DR: 本文提出了一种创新的文本引导多模态视频异常检测框架，在文本特征利用和多模态融合上取得突破，并在多个公开数据集上达到最新性能。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督多模态视频异常检测方法对文本模态的利用不足，而文本语义能有效提升异常检测准确性并降低误报。同时，文本特征提取难度较大且模态融合存在信息冗余和不均衡问题。

Method: 1. 基于上下文学习的多阶段文本增强机制，生成高质量异常文本样本，提升文本特征抽取器的异常表征能力；2. 设计多尺度瓶颈Transformer融合模块，通过压缩的瓶颈token逐步融合多模态信息，减少信息冗余和不均衡。

Result: 在UCF-Crime和XD-Violence公开数据集上，所提方法实现了最优的检测效能，超越了现有的最新方法。

Conclusion: 高质量文本增强与高效多模态融合结合，可以显著提升弱监督多模态视频异常检测的性能，证明了文本模态的巨大潜力。

Abstract: Weakly supervised multimodal video anomaly detection has gained significant attention, yet the potential of the text modality remains under-explored. Text provides explicit semantic information that can enhance anomaly characterization and reduce false alarms. However, extracting effective text features is challenging due to the inability of general-purpose language models to capture anomaly-specific nuances and the scarcity of relevant descriptions. Furthermore, multimodal fusion often suffers from redundancy and imbalance. To address these issues, we propose a novel text-guided framework. First, we introduce an in-context learning-based multi-stage text augmentation mechanism to generate high-quality anomaly text samples for fine-tuning the text feature extractor. Second, we design a multi-scale bottleneck Transformer fusion module that uses compressed bottleneck tokens to progressively integrate information across modalities, mitigating redundancy and imbalance. Experiments on UCF-Crime and XD-Violence demonstrate state-of-the-art performance.

</details>


### [30] [C^2ROPE: Causal Continuous Rotary Positional Encoding for 3D Large Multimodal-Models Reasoning](https://arxiv.org/abs/2602.10551)
*Guanting Ye,Qiyan Zhao,Wenhao Yu,Xiaofeng Zhang,Jianmin Ji,Yanyong Zhang,Ka-Veng Yuen*

Main category: cs.CV

TL;DR: 本论文提出了C^2RoPE方法，旨在解决现有3D大模型中RoPE位置编码带来的视觉特征空间连续性损失和远距离视觉信息遗忘问题。通过引入时空连续的位置嵌入及Chebyshev因果掩码，提高了模型对3D视觉任务的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型普遍采用RoPE进行位置编码，但RoPE仅适用于1D时序数据。在处理3D视觉信息时，RoPE会打破视觉特征在空间上的连续性，并随着序列增加遗忘早期视觉信息，限制了模型对长序列、空间结构的感知。

Method: 作者提出C^2RoPE，通过将1D时序位置与基于笛卡尔坐标的空间位置结合，形成三元组混合位置索引，同时采用频率分配策略对三个维度进行编码。此外，提出Chebyshev Causal Masking，根据2D空间中token的Chebyshev距离决定因果依赖关系，从而保留空间因果性。

Result: 在多项基准测试（如3D场景推理、3D视觉问答）中，C^2RoPE明显优于传统RoPE，展现了更强的空间感知和长期视觉信息利用能力。

Conclusion: C^2RoPE有效解决了RoPE在3D视觉任务中的空间连续性和长期依赖性问题，提升了多模态大模型在3D场景理解和推理任务上的表现。

Abstract: Recent advances in 3D Large Multimodal Models (LMMs) built on Large Language Models (LLMs) have established the alignment of 3D visual features with LLM representations as the dominant paradigm. However, the inherited Rotary Position Embedding (RoPE) introduces limitations for multimodal processing. Specifically, applying 1D temporal positional indices disrupts the continuity of visual features along the column dimension, resulting in spatial locality loss. Moreover, RoPE follows the prior that temporally closer image tokens are more causally related, leading to long-term decay in attention allocation and causing the model to progressively neglect earlier visual tokens as the sequence length increases. To address these issues, we propose C^2RoPE, an improved RoPE that explicitly models local spatial Continuity and spatial Causal relationships for visual processing. C^2RoPE introduces a spatio-temporal continuous positional embedding mechanism for visual tokens. It first integrates 1D temporal positions with Cartesian-based spatial coordinates to construct a triplet hybrid positional index, and then employs a frequency allocation strategy to encode spatio-temporal positional information across the three index components. Additionally, we introduce Chebyshev Causal Masking, which determines causal dependencies by computing the Chebyshev distance of image tokens in 2D space. Evaluation results across various benchmarks, including 3D scene reasoning and 3D visual question answering, demonstrate C^2RoPE's effectiveness. The code is be available at https://github.com/ErikZ719/C2RoPE.

</details>


### [31] [MetaphorStar: Image Metaphor Understanding and Reasoning with End-to-End Visual Reinforcement Learning](https://arxiv.org/abs/2602.10575)
*Chenhao Zhang,Yazhe Niu,Hongsheng Li*

Main category: cs.CV

TL;DR: 本文提出了MetaphorStar框架，通过视觉强化学习提升AI对图像中隐含隐喻等深层意义的理解能力，大幅优于现有多模态大模型。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态大语言模型在基本视觉问答上表现良好，但对图像中蕴含的文化、情感和上下文隐喻等复杂含义理解能力不足，难以胜任多跳推理和具备心理理论的任务。因此，亟需新的方法来提升AI对图像深层含义的理解。

Method: 作者提出了MetaphorStar端到端视觉强化学习框架，包括精细标注的数据集TFQ-Data、基于强化学习的TFQ-GRPO方法及评测基准TFQ-Bench。框架采用开放源代码，并系统性分析了模型参数、数据规模及结构、训练策略等因素对性能的影响。

Result: MetaphorStar在图像隐喻和含义理解任务上取得了平均82.6%的显著提升，在选择题和开放式问题上表现优于20多个主流多模态大模型，在判断题上显著超越Gemini-3.0-pro等闭源顶尖模型。

Conclusion: 学习图像隐喻及深层含义能够显著提升多模态模型的通用理解和复杂视觉推理能力，所提方法泛用性强，相关模型、数据集与代码已全部开源。

Abstract: Metaphorical comprehension in images remains a critical challenge for Nowadays AI systems. While Multimodal Large Language Models (MLLMs) excel at basic Visual Question Answering (VQA), they consistently struggle to grasp the nuanced cultural, emotional, and contextual implications embedded in visual content. This difficulty stems from the task's demand for sophisticated multi-hop reasoning, cultural context, and Theory of Mind (ToM) capabilities, which current models lack. To fill this gap, we propose MetaphorStar, the first end-to-end visual reinforcement learning (RL) framework for image implication tasks. Our framework includes three core components: the fine-grained dataset TFQ-Data, the visual RL method TFQ-GRPO, and the well-structured benchmark TFQ-Bench.
  Our fully open-source MetaphorStar family, trained using TFQ-GRPO on TFQ-Data, significantly improves performance by an average of 82.6% on the image implication benchmarks. Compared with 20+ mainstream MLLMs, MetaphorStar-32B achieves state-of-the-art (SOTA) on Multiple-Choice Question and Open-Style Question, significantly outperforms the top closed-source model Gemini-3.0-pro on True-False Question. Crucially, our experiments reveal that learning image implication tasks improves the general understanding ability, especially the complex visual reasoning ability. We further provide a systematic analysis of model parameter scaling, training data scaling, and the impact of different model architectures and training strategies, demonstrating the broad applicability of our method. We open-sourced all model weights, datasets, and method code at https://metaphorstar.github.io.

</details>


### [32] [(MGS)$^2$-Net: Unifying Micro-Geometric Scale and Macro-Geometric Structure for Cross-View Geo-Localization](https://arxiv.org/abs/2602.10704)
*Minglei Li,Mengfan He,Chao Chen,Ziyang Meng*

Main category: cs.CV

TL;DR: 该论文提出了一种名为(MGS)$^2$的新方法，通过引入几何结构过滤和尺度自适应模块，提升了无人机在GNSS信号缺失地区的跨视角地理定位性能，显著超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 在无人机跨视角地理定位任务中，因无人机斜视图与正射卫星图间存在剧烈几何不对齐，现有大多2D方法在面对高楼立面等复杂结构时表现脆弱，亟需更好地结合三维几何信息以提升鲁棒性。

Method: 方法包括两个核心模块：1. 宏观几何结构过滤(MGSF)，利用扩张几何梯度原理，削弱高频噪声和立面伪影，增强具有视角不变性的水平面特征；2. 微观几何尺度自适应(MGSA)模块，利用深度先验和多分支特征融合，纠正尺度变换；此外设计了几何-外观对比蒸馏损失(GACD)，提升对斜向遮挡的判别能力。

Result: (MGS)$^2$方法在University-1652和SUES-200主流数据集上分别取得了Recall@1为97.5%和97.02%的先进水平，并在跨数据集泛化测试中表现优异。

Conclusion: 通过结合宏观和微观几何信息，该方法显著提升了跨视角地理定位的准确性和鲁棒性，为GNSS失效环境下的无人机导航提供了有力支持。

Abstract: Cross-view geo-localization (CVGL) is pivotal for GNSS-denied UAV navigation but remains brittle under the drastic geometric misalignment between oblique aerial views and orthographic satellite references. Existing methods predominantly operate within a 2D manifold, neglecting the underlying 3D geometry where view-dependent vertical facades (macro-structure) and scale variations (micro-scale) severely corrupt feature alignment. To bridge this gap, we propose (MGS)$^2$, a geometry-grounded framework. The core of our innovation is the Macro-Geometric Structure Filtering (MGSF) module. Unlike pixel-wise matching sensitive to noise, MGSF leverages dilated geometric gradients to physically filter out high-frequency facade artifacts while enhancing the view-invariant horizontal plane, directly addressing the domain shift. To guarantee robust input for this structural filtering, we explicitly incorporate a Micro-Geometric Scale Adaptation (MGSA) module. MGSA utilizes depth priors to dynamically rectify scale discrepancies via multi-branch feature fusion. Furthermore, a Geometric-Appearance Contrastive Distillation (GACD) loss is designed to strictly discriminate against oblique occlusions. Extensive experiments demonstrate that (MGS)$^2$ achieves state-of-the-art performance, recording a Recall@1 of 97.5\% on University-1652 and 97.02\% on SUES-200. Furthermore, the framework exhibits superior cross-dataset generalization against geometric ambiguity. The code is available at: \href{https://github.com/GabrielLi1473/MGS-Net}{https://github.com/GabrielLi1473/MGS-Net}.

</details>


### [33] [Enhancing Underwater Images via Adaptive Semantic-aware Codebook Learning](https://arxiv.org/abs/2602.10586)
*Bosen Lin,Feng Gao,Yanwei Yu,Junyu Dong,Qian Du*

Main category: cs.CV

TL;DR: 本文提出了一种名为SUCode的语义感知型水下图像增强方法，有效缓解了水下影像中不同区域退化不一致造成的色彩失真和细节丢失问题，实验结果显示该方法在多个数据集上性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有水下图像增强方法大多采用单一的全局模型，未能针对图像内部不同语义区域的退化差异进行处理，导致异质化场景中的色彩失真与细节丢失。因此，需要一种能够自适应不同区域退化的增强方法。

Method: 作者提出SUCode网络，基于语义感知的离散码本进行自适应水下图像增强。与传统基于一次性码本的方法不同，SUCode为像素级提供与语义相关的码本表示。采用三阶段训练范式，减少伪真值污染；引入门控通道注意力模块（GCAM）与频率感知特征融合（FAFF），综合通道与频率信息以提升色彩还原和纹理恢复能力。

Result: 在多个标准数据集上，SUCode在有参考与无参考指标下均优于当前最新的水下图像增强方法，取得了最优性能。

Conclusion: SUCode能够自适应水下图像中不同区域的退化情况，显著增强色彩还原与细节，推进了水下图像增强领域的发展。

Abstract: Underwater Image Enhancement (UIE) is an ill-posed problem where natural clean references are not available, and the degradation levels vary significantly across semantic regions. Existing UIE methods treat images with a single global model and ignore the inconsistent degradation of different scene components. This oversight leads to significant color distortions and loss of fine details in heterogeneous underwater scenes, especially where degradation varies significantly across different image regions. Therefore, we propose SUCode (Semantic-aware Underwater Codebook Network), which achieves adaptive UIE from semantic-aware discrete codebook representation. Compared with one-shot codebook-based methods, SUCode exploits semantic-aware, pixel-level codebook representation tailored to heterogeneous underwater degradation. A three-stage training paradigm is employed to represent raw underwater image features to avoid pseudo ground-truth contamination. Gated Channel Attention Module (GCAM) and Frequency-Aware Feature Fusion (FAFF) jointly integrate channel and frequency cues for faithful color restoration and texture recovery. Extensive experiments on multiple benchmarks demonstrate that SUCode achieves state-of-the-art performance, outperforming recent UIE methods on both reference and no-reference metrics. The code will be made public available at https://github.com/oucailab/SUCode.

</details>


### [34] [From Steering to Pedalling: Do Autonomous Driving VLMs Generalize to Cyclist-Assistive Spatial Perception and Planning?](https://arxiv.org/abs/2602.10771)
*Krishna Kanth Nakka,Vedasri Nakka*

Main category: cs.CV

TL;DR: 该论文旨在提升骑行者安全，通过开发并使用CyclingVQA基准评估现有视觉语言模型在骑行场景下的感知与推理能力，发现目前模型在骑行者视角下仍有较大改进空间。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉语言模型在自动驾驶任务取得显著进展，但现有评估多以车辆为中心，忽视了骑行者在城市交通中的独特需求与挑战。论文认识到骑行者经常面临紧急安全状况，因此迫切需要面向骑行者的智能辅助系统。

Method: 作者提出了CyclingVQA，一个从骑行者视角设计的问题解答基准，内容涵盖感知、时空理解及交通规则至车道的推理。通过该基准，系统性地对31种主流视觉语言模型进行评测，涵盖通用型、空间感知增强型以及自动驾驶专用模型。

Result: 评估结果显示，尽管现有模型在部分任务上表现出一定能力，但在理解骑行相关交通提示和将交通标志与导航车道准确关联上存在不足。同时，部分自动驾驶专用模型在骑行任务上的表现还不如某些通用模型，表明从车辆到骑行者场景的迁移存在局限。

Conclusion: 当前视觉语言模型在骑行者辅助方面能力有限。论文通过失误分析总结模型主要失败类型，为今后开发更加有效的骑行者智能辅助系统提供改进方向。

Abstract: Cyclists often encounter safety-critical situations in urban traffic, highlighting the need for assistive systems that support safe and informed decision-making. Recently, vision-language models (VLMs) have demonstrated strong performance on autonomous driving benchmarks, suggesting their potential for general traffic understanding and navigation-related reasoning. However, existing evaluations are predominantly vehicle-centric and fail to assess perception and reasoning from a cyclist-centric viewpoint. To address this gap, we introduce CyclingVQA, a diagnostic benchmark designed to probe perception, spatio-temporal understanding, and traffic-rule-to-lane reasoning from a cyclist's perspective. Evaluating 31+ recent VLMs spanning general-purpose, spatially enhanced, and autonomous-driving-specialized models, we find that current models demonstrate encouraging capabilities, while also revealing clear areas for improvement in cyclist-centric perception and reasoning, particularly in interpreting cyclist-specific traffic cues and associating signs with the correct navigational lanes. Notably, several driving-specialized models underperform strong generalist VLMs, indicating limited transfer from vehicle-centric training to cyclist-assistive scenarios. Finally, through systematic error analysis, we identify recurring failure modes to guide the development of more effective cyclist-assistive intelligent systems.

</details>


### [35] [Enhancing YOLOv11n for Reliable Child Detection in Noisy Surveillance Footage](https://arxiv.org/abs/2602.10592)
*Khanh Linh Tran,Minh Nguyen Dang,Thien Nguyen Trong,Hung Nguyen Quoc,Linh Nguyen Kieu*

Main category: cs.CV

TL;DR: 该论文提出了一种便于部署且高效的儿童检测方案，专为低质量监控视频设计，提升了实际应用中对儿童的检测能力，尤其适合资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 现实中监控视频质量普遍较差，儿童目标存在小尺寸、遮挡、模糊等问题，急需提升在此类条件下的检测准确率，满足失踪儿童预警和托管场所监控等需求。

Method: 以轻量YOLOv11n架构为基础，提出领域特定的数据增强方法（空间扰动与光照降质），并在推理时引入SAHI切片辅助推理以提升对小目标和部分遮挡目标的召回。所有方法在Roboflow Daycare数据集的儿童子集上训练和评估。

Result: 改进方案在YOLOv11n基线基础上，mAP@0.5提升0.7%，mAP@0.5:0.95提升2.3%，且无架构变化，兼容低功耗设备并支持实时运行。

Conclusion: 提出的方法有效提升了低质量监控环境下的儿童检测性能，适合低成本、算力受限的实际工业场景，可直接应用于失踪儿童监控与托管所自动化监管。

Abstract: This paper presents a practical and lightweight solution for enhancing child detection in low-quality surveillance footage, a critical component in real-world missing child alert and daycare monitoring systems. Building upon the efficient YOLOv11n architecture, we propose a deployment-ready pipeline that improves detection under challenging conditions including occlusion, small object size, low resolution, motion blur, and poor lighting commonly found in existing CCTV infrastructures. Our approach introduces a domain-specific augmentation strategy that synthesizes realistic child placements using spatial perturbations such as partial visibility, truncation, and overlaps, combined with photometric degradations including lighting variation and noise. To improve recall of small and partially occluded instances, we integrate Slicing Aided Hyper Inference (SAHI) at inference time. All components are trained and evaluated on a filtered, child-only subset of the Roboflow Daycare dataset. Compared to the baseline YOLOv11n, our enhanced system achieves a mean Average Precision at 0.5 IoU (mAP@0.5) of 0.967 and a mean Average Precision averaged over IoU thresholds from 0.5 to 0.95 (mAP@0.5:0.95) of 0.783, yielding absolute improvements of 0.7 percent and 2.3 percent, respectively, without architectural changes. Importantly, the entire pipeline maintains compatibility with low-power edge devices and supports real-time performance, making it particularly well suited for low-cost or resource-constrained industrial surveillance deployments. The example augmented dataset and the source code used to generate it are available at: https://github.com/html-ptit/Data-Augmentation-YOLOv11n-child-detection

</details>


### [36] [Towards Learning a Generalizable 3D Scene Representation from 2D Observations](https://arxiv.org/abs/2602.10943)
*Martin Gromniak,Jan-Gerrit Habekost,Sebastian Kamp,Sven Magg,Stefan Wermter*

Main category: cs.CV

TL;DR: 本文提出了一种通用的神经辐射场方法，通过来自机器人自我视角的观测来预测三维工作空间的占据状态。该方法能不经针对特定场景的微调，泛化到新场景，性能优于传统立体视觉方法。


<details>
  <summary>Details</summary>
Motivation: 现有三维占据预测方法多基于摄像头坐标系，难以直接应用在机器人操作任务中，且泛化能力有限。为提升机器人对复杂工作场景的知觉与操作能力，亟需高精度、通用性强的三维工作空间建模方法。

Method: 作者提出在全局工作空间坐标系下，结合灵活多源视角，使用神经辐射场(NeRF)框架进行占据预测。模型能够泛化到未见过的物体摆放，不需对每个场景专门微调。方法在40个真实场景上进行训练和评估，采用3D传感器数据为真值。

Result: 在包括遮挡区域在内的三维重建任务上，模型达到26mm的重构误差，准确预测完整的三维占据状态，优于传统立体视觉技术。

Conclusion: 该方法能够凭借神经辐射场和全局坐标建模，提升机器人理解和预测实际工作空间的能力，具备良好的泛化性和实用性，为机器人操控奠定更坚实的感知基础。

Abstract: We introduce a Generalizable Neural Radiance Field approach for predicting 3D workspace occupancy from egocentric robot observations. Unlike prior methods operating in camera-centric coordinates, our model constructs occupancy representations in a global workspace frame, making it directly applicable to robotic manipulation. The model integrates flexible source views and generalizes to unseen object arrangements without scene-specific finetuning. We demonstrate the approach on a humanoid robot and evaluate predicted geometry against 3D sensor ground truth. Trained on 40 real scenes, our model achieves 26mm reconstruction error, including occluded regions, validating its ability to infer complete 3D occupancy beyond traditional stereo vision methods.

</details>


### [37] [Fast Person Detection Using YOLOX With AI Accelerator For Train Station Safety](https://arxiv.org/abs/2602.10593)
*Mas Nurul Achmadiah,Novendra Setyawan,Achmad Arif Bryantono,Chi-Chia Sun,Wen-Kai Kuo*

Main category: cs.CV

TL;DR: 本文利用YOLOX模型与边缘AI加速器，用于火车站的乘客检测，并比较Hailo-8与Jetson Orin Nano的性能。结果显示Hailo-8的准确率和响应速度均优于Jetson Orin Nano。


<details>
  <summary>Details</summary>
Motivation: 鉴于火车站过道存在部分乘客不慎越过黄色警戒线，导致事故发生，因此亟需提升该场景下的安全性与事故预防能力。

Method: 采用YOLOX目标检测算法，并部署在两种AI加速硬件（Hailo-8和Jetson Orin Nano）上，针对火车站乘客检测场景进行实验，并比对两者在准确率和延迟上的表现。

Result: Hailo-8相比Jetson Orin Nano，准确率提升超过12%，延迟降低约20毫秒。

Conclusion: 在火车站乘客检测应用中，Hailo-8 AI加速器优于Jetson Orin Nano，适用于需要高准确率和低延迟的实际场景。

Abstract: Recently, Image processing has advanced Faster and applied in many fields, including health, industry, and transportation. In the transportation sector, object detection is widely used to improve security, for example, in traffic security and passenger crossings at train stations. Some accidents occur in the train crossing area at the station, like passengers uncarefully when passing through the yellow line. So further security needs to be developed. Additional technology is required to reduce the number of accidents. This paper focuses on passenger detection applications at train stations using YOLOX and Edge AI Accelerator hardware. the performance of the AI accelerator will be compared with Jetson Orin Nano. The experimental results show that the Hailo-8 AI hardware accelerator has higher accuracy than Jetson Orin Nano (improvement of over 12%) and has lower latency than Jetson Orin Nano (reduced 20 ms).

</details>


### [38] [Enhancing Predictability of Multi-Tenant DNN Inference for Autonomous Vehicles' Perception](https://arxiv.org/abs/2602.11004)
*Liangkai Liu,Kang G. Shin,Jinkyu Lee,Chengmo Yang,Weisong Shi*

Main category: cs.CV

TL;DR: PP-DNN系统通过动态选择关键帧和兴趣区域（ROI），减少需处理的数据量，实现了自动驾驶车辆感知过程中多模型DNN推理的可预测性与高效率。


<details>
  <summary>Details</summary>
Motivation: 实时DNN推理对自动驾驶车辆来说计算需求高，但车载资源有限。传统方法多通过模型压缩提升速度，而忽视了可优化数据处理量这一路径。该研究旨在突破仅依赖模型优化的局限，提升自动驾驶感知系统在有限资源下的实际能力。

Method: 提出PP-DNN系统，包括ROI生成、MACs预测、ROI调度、多模型协调及非关键帧检测。其核心是根据环境动态识别并仅处理关键帧与ROI，结合算力消耗预测及多DNN模型协作，达成感知的可预测性与高效性。该系统集成在ROS自动驾驶感知流程，并在真实数据集上验证。

Result: PP-DNN在BDD100K与nuScenes数据集上显著提升了感知能力：融合帧数提升最多7.3倍，融合延迟降低超2.6倍，延迟波动下降超2.3倍，检测完整性提升75.4%，性价比提升最高达98%。

Conclusion: PP-DNN通过动态关键帧与ROI选择，显著提升了自动驾驶DNN感知系统的实时性、可预测性与性价比，为资源受限场景下的多模型推理提供了新思路。

Abstract: Autonomous vehicles (AVs) rely on sensors and deep neural networks (DNNs) to perceive their surrounding environment and make maneuver decisions in real time. However, achieving real-time DNN inference in the AV's perception pipeline is challenging due to the large gap between the computation requirement and the AV's limited resources. Most, if not all, of existing studies focus on optimizing the DNN inference time to achieve faster perception by compressing the DNN model with pruning and quantization. In contrast, we present a Predictable Perception system with DNNs (PP-DNN) that reduce the amount of image data to be processed while maintaining the same level of accuracy for multi-tenant DNNs by dynamically selecting critical frames and regions of interest (ROIs). PP-DNN is based on our key insight that critical frames and ROIs for AVs vary with the AV's surrounding environment. However, it is challenging to identify and use critical frames and ROIs in multi-tenant DNNs for predictable inference. Given image-frame streams, PP-DNN leverages an ROI generator to identify critical frames and ROIs based on the similarities of consecutive frames and traffic scenarios. PP-DNN then leverages a FLOPs predictor to predict multiply-accumulate operations (MACs) from the dynamic critical frames and ROIs. The ROI scheduler coordinates the processing of critical frames and ROIs with multiple DNN models. Finally, we design a detection predictor for the perception of non-critical frames. We have implemented PP-DNN in an ROS-based AV pipeline and evaluated it with the BDD100K and the nuScenes dataset. PP-DNN is observed to significantly enhance perception predictability, increasing the number of fusion frames by up to 7.3x, reducing the fusion delay by >2.6x and fusion-delay variations by >2.3x, improving detection completeness by 75.4% and the cost-effectiveness by up to 98% over the baseline.

</details>


### [39] [Improving Medical Visual Reinforcement Fine-Tuning via Perception and Reasoning Augmentation](https://arxiv.org/abs/2602.10619)
*Guangjing Yang,ZhangYuan Yu,Ziyuan Qin,Xinyuan Song,Huahui Yi,Qingbo Kang,Jun Gao,Yiyue Li,Chenlin Du,Qicheng Lao*

Main category: cs.CV

TL;DR: 本文提出了一种专为医学影像领域设计的视觉强化微调框架（VRFT-Aug），通过多项数据集实验证明其在感知和推理任务上优于现有微调及基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于规则的强化微调方法主要用于文本领域，对跨模态（尤其是医学影像）应用的探索非常有限。而医学影像任务对视觉感知与结构化推理能力要求很高，因此作者希望弥补这一应用空白。

Method: 作者提出VRFT-Aug框架，结合多种训练策略：注入先验知识、感知驱动的策略优化、医学知识指导的奖励塑造、以及行为模仿，以此增强模型的视觉感知能力及推理能力，并稳定强化微调的训练过程。

Result: 在多个医学影像数据集上，VRFT-Aug在视觉感知和推理任务表现上，超过了传统有监督微调和RFT基线方法。

Conclusion: 本方法为医学高风险场景下开发强大且可靠的跨模态AI模型提供了实证支持及实用训练方法，并可为其它医学任务推广提供借鉴。

Abstract: While recent advances in Reinforcement Fine-Tuning (RFT) have shown that rule-based reward schemes can enable effective post-training for large language models, their extension to cross-modal, vision-centric domains remains largely underexplored. This limitation is especially pronounced in the medical imaging domain, where effective performance requires both robust visual perception and structured reasoning. In this work, we address this gap by proposing VRFT-Aug, a visual reinforcement fine-tuning framework tailored for the medical domain. VRFT-Aug introduces a series of training strategies designed to augment both perception and reasoning, including prior knowledge injection, perception-driven policy refinement, medically informed reward shaping, and behavioral imitation. Together, these methods aim to stabilize and improve the RFT process.
  Through extensive experiments across multiple medical datasets, we show that our approaches consistently outperform both standard supervised fine-tuning and RFT baselines. Moreover, we provide empirically grounded insights and practical training heuristics that can be generalized to other medical image tasks. We hope this work contributes actionable guidance and fresh inspiration for the ongoing effort to develop reliable, reasoning-capable models for high-stakes medical applications.

</details>


### [40] [A Vision-Language Foundation Model for Zero-shot Clinical Collaboration and Automated Concept Discovery in Dermatology](https://arxiv.org/abs/2602.10624)
*Siyuan Yan,Xieji Li,Dan Mo,Philipp Tschandl,Yiwen Jiang,Zhonghua Wang,Ming Hu,Lie Ju,Cristina Vico-Alonso,Yizhen Zheng,Jiahe Liu,Juexiao Zhou,Camilla Chello,Jen G. Cheung,Julien Anriot,Luc Thomas,Clare Primiero,Gin Tan,Aik Beng Ng,Simon See,Xiaoying Tang,Albert Ip,Xiaoyang Liao,Adrian Bowling,Martin Haskett,Shuang Zhao,Monika Janda,H. Peter Soyer,Victoria Mar,Harald Kittler,Zongyuan Ge*

Main category: cs.CV

TL;DR: DermFM-Zero 是一个无需针对特定任务微调的皮肤科视觉-语言基础模型，在20项基准测试中实现了零样本任务的最先进性能，并在三项临床多国研究中提高了从普通医生到专家的诊断表现。


<details>
  <summary>Details</summary>
Motivation: 现有医学基础模型在实际应用中受限于对特定任务微调的依赖，难以广泛部署。作者希望开发一种无需微调也能具有高性能和通用性的医学基础模型，特别是在皮肤科领域。

Method: DermFM-Zero 通过掩码潜变量建模和对比学习，在超过400万条多模态数据上进行训练，并在20项零样本诊断和多模态检索基准上测试。在三项多国临床研究中，分析该模型对医生诊断准确率和决策的提升。其潜在表征的可解释性通过稀疏自编码器无监督地获得并验证其优势。

Result: DermFM-Zero 零样本下实现了最先进的诊断和检索性能。AI辅助显著提高了普通医生对98种皮肤病的诊断准确率，并在专业场景中超越了有资质的皮肤科专家。非专家有AI辅助时也能超过未使用AI的专家。模型的潜在特征具备很好的临床可解释性和抗伪影能力。

Conclusion: DermFM-Zero 证明了基础模型无需微调即可安全、有效并透明地实现皮肤科临床决策支持，具备实际部署潜力。

Abstract: Medical foundation models have shown promise in controlled benchmarks, yet widespread deployment remains hindered by reliance on task-specific fine-tuning. Here, we introduce DermFM-Zero, a dermatology vision-language foundation model trained via masked latent modelling and contrastive learning on over 4 million multimodal data points. We evaluated DermFM-Zero across 20 benchmarks spanning zero-shot diagnosis and multimodal retrieval, achieving state-of-the-art performance without task-specific adaptation. We further evaluated its zero-shot capabilities in three multinational reader studies involving over 1,100 clinicians. In primary care settings, AI assistance enabled general practitioners to nearly double their differential diagnostic accuracy across 98 skin conditions. In specialist settings, the model significantly outperformed board-certified dermatologists in multimodal skin cancer assessment. In collaborative workflows, AI assistance enabled non-experts to surpass unassisted experts while improving management appropriateness. Finally, we show that DermFM-Zero's latent representations are interpretable: sparse autoencoders unsupervisedly disentangle clinically meaningful concepts that outperform predefined-vocabulary approaches and enable targeted suppression of artifact-induced biases, enhancing robustness without retraining. These findings demonstrate that a foundation model can provide effective, safe, and transparent zero-shot clinical decision support.

</details>


### [41] [Eliminating VAE for Fast and High-Resolution Generative Detail Restoration](https://arxiv.org/abs/2602.10630)
*Yan Wang,Shijie Zhao,Junlin Li,Li Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种加速基于扩散模型的超分辨率方法，通过消除VAE瓶颈与创新的多阶段蒸馏，使得高分辨率图片的恢复更快且占用更少内存，同时保持输出质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在超分任务中表现突出，但推理速度慢、内存消耗大，尤其VAE成为系统瓶颈，限制了高分辨率图像的高效重建。作者希望彻底解决推理效率与内存瓶颈问题。

Method: 1）用pixel-(un)shuffle操作替换VAE，直接在像素空间进行推理，形成GenDR-Pix架构；2）为避免高倍pixelshuffle带来重复模式失真，设计了多阶段对抗蒸馏，通过逐步移除编码器和解码器，利用前一阶段生成特征引导判别器学习；3）引入随机padding和傅里叶空间损失，增强判别能力，防止收敛塌陷和减少振幅异常；4）融合基于padding的self-ensemble与无分类器引导，进一步提升推理性能。

Result: GenDR-Pix相较GenDR实现了2.8倍的推理加速和60%的内存节省，视觉质量损失极小，大幅优于其他一步扩散超分方法。能够在1秒和6GB内恢复4K图像。

Conclusion: 消除VAE及采用多阶段创新蒸馏，极大提升了超分辨率扩散模型的推理速度和内存效率，为高分辨率图像恢复提供更实用的解决方案。

Abstract: Diffusion models have attained remarkable breakthroughs in the real-world super-resolution (SR) task, albeit at slow inference and high demand on devices. To accelerate inference, recent works like GenDR adopt step distillation to minimize the step number to one. However, the memory boundary still restricts the maximum processing size, necessitating tile-by-tile restoration of high-resolution images. Through profiling the pipeline, we pinpoint that the variational auto-encoder (VAE) is the bottleneck of latency and memory. To completely solve the problem, we leverage pixel-(un)shuffle operations to eliminate the VAE, reversing the latent-based GenDR to pixel-space GenDR-Pix. However, upscale with x8 pixelshuffle may induce artifacts of repeated patterns. To alleviate the distortion, we propose a multi-stage adversarial distillation to progressively remove the encoder and decoder. Specifically, we utilize generative features from the previous stage models to guide adversarial discrimination. Moreover, we propose random padding to augment generative features and avoid discriminator collapse. We also introduce a masked Fourier space loss to penalize the outliers of amplitude. To improve inference performance, we empirically integrate a padding-based self-ensemble with classifier-free guidance to improve inference scaling. Experimental results show that GenDR-Pix performs 2.8x acceleration and 60% memory-saving compared to GenDR with negligible visual degradation, surpassing other one-step diffusion SR. Against all odds, GenDR-Pix can restore 4K image in only 1 second and 6GB.

</details>


### [42] [VideoSTF: Stress-Testing Output Repetition in Video Large Language Models](https://arxiv.org/abs/2602.10639)
*Yuxin Cao,Wei Song,Shangzhi Xu,Jingling Xue,Jin Song Dong*

Main category: cs.CV

TL;DR: 该论文发现当前VideoLLM模型存在严重的输出重复问题，并提出了VideoSTF框架用于系统分析和测试该问题，揭示了这一未被重视的生成失败现象及其安全隐患。


<details>
  <summary>Details</summary>
Motivation: 尽管VideoLLMs在视频理解任务中表现优异，现有基准主要关注任务准确性与事实正确性，未曾关注或捕捉模型输出严重重复（句子或短语自我循环）的问题。作者希望填补这一空白，并全面评估这种失败模式。

Method: 提出VideoSTF框架，利用三种n-gram指标形式化衡量输出重复，为此构建包含1万个多样化视频和一套可控时序变换库的标准化测试集，对10个先进的VideoLLM进行广泛测试、时序压力测试及对抗性实验。

Result: 发现输出重复问题在各主流VideoLLM中广泛存在，且高度敏感于视频输入的时序扰动。简单的时序变换即可在黑盒条件下有效诱发模型输出退化，显示该问题具有被利用的安全漏洞属性。

Conclusion: 输出重复是现代VideoLLM的基本稳定性问题，现有评测手段未能捕捉，应引入关注稳定性的评估方法以保障视频-语言系统的健壮性。

Abstract: Video Large Language Models (VideoLLMs) have recently achieved strong performance in video understanding tasks. However, we identify a previously underexplored generation failure: severe output repetition, where models degenerate into self-reinforcing loops of repeated phrases or sentences. This failure mode is not captured by existing VideoLLM benchmarks, which focus primarily on task accuracy and factual correctness. We introduce VideoSTF, the first framework for systematically measuring and stress-testing output repetition in VideoLLMs. VideoSTF formalizes repetition using three complementary n-gram-based metrics and provides a standardized testbed of 10,000 diverse videos together with a library of controlled temporal transformations. Using VideoSTF, we conduct pervasive testing, temporal stress testing, and adversarial exploitation across 10 advanced VideoLLMs. We find that output repetition is widespread and, critically, highly sensitive to temporal perturbations of video inputs. Moreover, we show that simple temporal transformations can efficiently induce repetitive degeneration in a black-box setting, exposing output repetition as an exploitable security vulnerability. Our results reveal output repetition as a fundamental stability issue in modern VideoLLMs and motivate stability-aware evaluation for video-language systems. Our evaluation code and scripts are available at: https://github.com/yuxincao22/VideoSTF_benchmark.

</details>


### [43] [Multimodal Priors-Augmented Text-Driven 3D Human-Object Interaction Generation](https://arxiv.org/abs/2602.10659)
*Yin Wang,Ziyao Zhang,Zhiying Leng,Haitian Liu,Frederick W. B. Li,Mu Li,Xiaohui Liang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为MP-HOI的创新框架，用于根据文本生成高质量的人体-物体交互动作，显著超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于文本驱动的3D人体-物体交互动画生成存在三个主要缺陷：生成的人体动作不理想，物体动作不自然，以及人与物体的交互较弱。这些问题主要源于文本与动作间的跨模态鸿沟。

Method: MP-HOI框架包含四个核心创新：(1)利用大规模多模态模型（文本、图像、姿态/物体）作为先验引导生成，提升动作质量；(2)增强物体表示，加入几何关键点、接触特征和动态属性，使其更加生动；(3)提出模态感知的专家混合模型（MoE），高效融合多模态特征；(4)设计级联扩散架构，配合专门的交互监督，逐步细化人机交互表现。

Result: 实验显示该方法生成的动作在细节精度和真实感上均超越了现有的主流方案。

Conclusion: MP-HOI创新性地结合多模态数据和专家混合架构，有效解决了人体动作、物体动作及交互自然度等问题，推动了文本驱动3D HOI生成技术的发展。

Abstract: We address the challenging task of text-driven 3D human-object interaction (HOI) motion generation. Existing methods primarily rely on a direct text-to-HOI mapping, which suffers from three key limitations due to the significant cross-modality gap: (Q1) sub-optimal human motion, (Q2) unnatural object motion, and (Q3) weak interaction between humans and objects. To address these challenges, we propose MP-HOI, a novel framework grounded in four core insights: (1) Multimodal Data Priors: We leverage multimodal data (text, image, pose/object) from large multimodal models as priors to guide HOI generation, which tackles Q1 and Q2 in data modeling. (2) Enhanced Object Representation: We improve existing object representations by incorporating geometric keypoints, contact features, and dynamic properties, enabling expressive object representations, which tackles Q2 in data representation. (3) Multimodal-Aware Mixture-of-Experts (MoE) Model: We propose a modality-aware MoE model for effective multimodal feature fusion paradigm, which tackles Q1 and Q2 in feature fusion. (4) Cascaded Diffusion with Interaction Supervision: We design a cascaded diffusion framework that progressively refines human-object interaction features under dedicated supervision, which tackles Q3 in interaction refinement. Comprehensive experiments demonstrate that MP-HOI outperforms existing approaches in generating high-fidelity and fine-grained HOI motions.

</details>


### [44] [AurigaNet: A Real-Time Multi-Task Network for Enhanced Urban Driving Perception](https://arxiv.org/abs/2602.10660)
*Kiarash Ghasemzadeh,Sedigheh Dehghani*

Main category: cs.CV

TL;DR: 本文提出了一种面向自动驾驶感知的多任务网络架构AurigaNet，同时实现目标检测、车道线检测和可行驶区域实例分割，实验结果优于现有方法，在实际嵌入式设备上也展现出良好实时性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶技术需要在复杂环境下高效、准确地感知周围环境。单任务模型难以满足实时性与多元感知需求，多任务学习为解决这些挑战提供了新思路，因此作者希望通过多任务网络提升自动驾驶感知系统的整体性能与效率。

Method: 提出AurigaNet多任务学习网络，集成目标检测、车道检测和可行驶区域实例分割三大任务。该网络在BDDK100K数据集上进行训练与评估，并具备端到端实例分割能力。还在嵌入式设备如Jetson Orin NX上进行了部署与测试。

Result: AurigaNet在可行驶区域分割任务上取得了85.2%的IoU，超越最强对手0.7%；车道检测任务上实现60.8%的IoU，领先其它方法30%；交通目标检测的mAP@0.5:0.95为47.6%，比次优模型高2.9%。在Jetson Orin NX等嵌入式设备上也有竞争力的实时表现。

Conclusion: AurigaNet作为多任务学习架构，有效整合自动驾驶所需核心感知任务，取得了优异的准确度与实时性表现，可为自动驾驶系统的感知模块提供可靠、高效的解决方案。

Abstract: Self-driving cars hold significant potential to reduce traffic accidents, alleviate congestion, and enhance urban mobility. However, developing reliable AI systems for autonomous vehicles remains a substantial challenge. Over the past decade, multi-task learning has emerged as a powerful approach to address complex problems in driving perception. Multi-task networks offer several advantages, including increased computational efficiency, real-time processing capabilities, optimized resource utilization, and improved generalization. In this study, we present AurigaNet, an advanced multi-task network architecture designed to push the boundaries of autonomous driving perception. AurigaNet integrates three critical tasks: object detection, lane detection, and drivable area instance segmentation. The system is trained and evaluated using the BDD100K dataset, renowned for its diversity in driving conditions. Key innovations of AurigaNet include its end-to-end instance segmentation capability, which significantly enhances both accuracy and efficiency in path estimation for autonomous vehicles. Experimental results demonstrate that AurigaNet achieves an 85.2% IoU in drivable area segmentation, outperforming its closest competitor by 0.7%. In lane detection, AurigaNet achieves a remarkable 60.8% IoU, surpassing other models by more than 30%. Furthermore, the network achieves an mAP@0.5:0.95 of 47.6% in traffic object detection, exceeding the next leading model by 2.9%. Additionally, we validate the practical feasibility of AurigaNet by deploying it on embedded devices such as the Jetson Orin NX, where it demonstrates competitive real-time performance. These results underscore AurigaNet's potential as a robust and efficient solution for autonomous driving perception systems. The code can be found here https://github.com/KiaRational/AurigaNet.

</details>


### [45] [Dynamic Frequency Modulation for Controllable Text-driven Image Generation](https://arxiv.org/abs/2602.10662)
*Tiandong Shi,Ling Zhao,Ji Qi,Jiayi Ma,Chengli Peng*

Main category: cs.CV

TL;DR: 提出了一种基于频率调制的无训练方法，能在修改文本提示时，保持图像结构一致性的同时，实现语义的定向调整。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导的扩散模型在改动文本提示以实现语义调整时，容易引入结构性变化，影响用户意图。而现有解决方法需经验性选取特征图，稳定性和效果有限。本文旨在解决该结构稳定与语义修改的平衡难题。

Method: 分析噪声潜变量的频谱对结构和细节生成的影响，发现低频负责生成结构，高频负责细节。基于此，提出一种无训练的频率调制方法，采用动态衰减的频率加权函数，直接调制噪声潜变量，无需经验性选取特征图。

Result: 大量实验显示，所提方法在保持结构一致性的同时，实现了有效的语义调整，优于现有SOTA方法，在结构保持与语义更新间取得更好平衡。

Conclusion: 基于频率视角直接调制噪声潜变量可有效提升扩散模型在结构保持与语义调整中的性能，为图像生成任务提供了简单实用的新方案。

Abstract: The success of text-guided diffusion models has established a new image generation paradigm driven by the iterative refinement of text prompts. However, modifying the original text prompt to achieve the expected semantic adjustments often results in unintended global structure changes that disrupt user intent. Existing methods rely on empirical feature map selection for intervention, whose performance heavily depends on appropriate selection, leading to suboptimal stability. This paper tries to solve the aforementioned problem from a frequency perspective and analyzes the impact of the frequency spectrum of noisy latent variables on the hierarchical emergence of the structure framework and fine-grained textures during the generation process. We find that lower-frequency components are primarily responsible for establishing the structure framework in the early generation stage. Their influence diminishes over time, giving way to higher-frequency components that synthesize fine-grained textures. In light of this, we propose a training-free frequency modulation method utilizing a frequency-dependent weighting function with dynamic decay. This method maintains the structure framework consistency while permitting targeted semantic modifications. By directly manipulating the noisy latent variable, the proposed method avoids the empirical selection of internal feature maps. Extensive experiments demonstrate that the proposed method significantly outperforms current state-of-the-art methods, achieving an effective balance between preserving structure and enabling semantic updates.

</details>


### [46] [AMAP-APP: Efficient Segmentation and Morphometry Quantification of Fluorescent Microscopy Images of Podocytes](https://arxiv.org/abs/2602.10663)
*Arash Fatehi,David Unnersjö-Jess,Linus Butt,Noémie Moreau,Thomas Benzing,Katarzyna Bozek*

Main category: cs.CV

TL;DR: AMAP-APP是一个跨平台、易用的桌面应用，用于自动分析肾小球足细胞形态，相比原本的AMAP方法大幅提升了速度和便捷性，并具有高度准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的AMAP方法虽然能自动分析足细胞形态，但对硬件需求高，没有用户界面，只能在Linux上运行，制约了其在肾脏研究和临床中的广泛应用。因此，亟需更高效、易用、跨平台的分析工具。

Method: 开发了AMAP-APP桌面程序，核心方法为用传统图像处理取代资源消耗大的实例分割，保留原有的语义分割模型，并引入优化后的ROI算法提升精度。在365张小鼠和人类肾脏图像（STED和共聚焦显微）上，通过Pearson相关性分析和双边T检验与原AMAP方法对比。

Result: AMAP-APP在普通硬件上处理速度提升147倍；输出的形态学指标与原方法高度相关（r＞0.90）、统计学等价（TOST P＜0.05）。新ROl算法在与手动标注比对中显示更高精度，偏差更小。

Conclusion: AMAP-APP大幅降低了足细胞形态学分析的门槛，无需高性能计算资源，提供跨平台图形界面，将深度学习方法推广到更广泛的肾脏研究及可能的临床应用中。

Abstract: Background: Automated podocyte foot process quantification is vital for kidney research, but the established "Automatic Morphological Analysis of Podocytes" (AMAP) method is hindered by high computational demands, a lack of a user interface, and Linux dependency. We developed AMAP-APP, a cross-platform desktop application designed to overcome these barriers.
  Methods: AMAP-APP optimizes efficiency by replacing intensive instance segmentation with classic image processing while retaining the original semantic segmentation model. It introduces a refined Region of Interest (ROI) algorithm to improve precision. Validation involved 365 mouse and human images (STED and confocal), benchmarking performance against the original AMAP via Pearson correlation and Two One-Sided T-tests (TOST).
  Results: AMAP-APP achieved a 147-fold increase in processing speed on consumer hardware. Morphometric outputs (area, perimeter, circularity, and slit diaphragm density) showed high correlation (r>0.90) and statistical equivalence (TOST P<0.05) to the original method. Additionally, the new ROI algorithm demonstrated superior accuracy compared to the original, showing reduced deviation from manual delineations.
  Conclusion: AMAP-APP democratizes deep learning-based podocyte morphometry. By eliminating the need for high-performance computing clusters and providing a user-friendly interface for Windows, macOS, and Linux, it enables widespread adoption in nephrology research and potential clinical diagnostics.

</details>


### [47] [TwiFF (Think With Future Frames): A Large-Scale Dataset for Dynamic Visual Reasoning](https://arxiv.org/abs/2602.10675)
*Junhua Liu,Zhangcheng Wang,Zhike Han,Ningli Wang,Guotao Liang,Kun Kuang*

Main category: cs.CV

TL;DR: 该论文提出了TwiFF-2.7M，这是首个大规模、面向视频动态场景的视觉推理链（VCoT）数据集，并配套了评测基准与新模型 TwiFF，显著提升了动态视觉问答能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉推理链（VCoT）方法多局限于静态图像，难以处理涉及时间动态的视频场景，而很多应用如指令推理、预测和镜头运动都需理解时序信息。因此，亟需构建针对时间动态性的VCoT机制和数据集，以推动多模态推理的进步。

Method: 作者构建了TwiFF-2.7M数据集（包含270万视频片段），专为动态视觉问答设计，并给出TwiFF-Bench评测基准。提出了TwiFF模型，将视频生成和图像理解预训练能力融合，通过递归生成未来动作帧及文本推理，形成时序连贯的视觉推理链。

Result: 实验表明，TwiFF在动态推理任务和视觉问答方面均显著优于现有VCoT方法及文本推理链（CoT）基线，有力验证了其设计的有效性。

Conclusion: TwiFF数据集、评测基准和模型为动态多模态推理领域树立了新的标准，推动了视觉推理链技术从静态向动态场景的突破。

Abstract: Visual Chain-of-Thought (VCoT) has emerged as a promising paradigm for enhancing multimodal reasoning by integrating visual perception into intermediate reasoning steps. However, existing VCoT approaches are largely confined to static scenarios and struggle to capture the temporal dynamics essential for tasks such as instruction, prediction, and camera motion. To bridge this gap, we propose TwiFF-2.7M, the first large-scale, temporally grounded VCoT dataset derived from $2.7$ million video clips, explicitly designed for dynamic visual question and answer. Accompanying this, we introduce TwiFF-Bench, a high-quality evaluation benchmark of $1,078$ samples that assesses both the plausibility of reasoning trajectories and the correctness of final answers in open-ended dynamic settings. Building on these foundations, we propose the TwiFF model, a unified modal that synergistically leverages pre-trained video generation and image comprehension capabilities to produce temporally coherent visual reasoning cues-iteratively generating future action frames and textual reasoning. Extensive experiments demonstrate that TwiFF significantly outperforms existing VCoT methods and Textual Chain-of-Thought baselines on dynamic reasoning tasks, which fully validates the effectiveness for visual question answering in dynamic scenarios. Our code and data is available at https://github.com/LiuJunhua02/TwiFF.

</details>


### [48] [OmniVL-Guard: Towards Unified Vision-Language Forgery Detection and Grounding via Balanced RL](https://arxiv.org/abs/2602.10687)
*Jinjie Shen,Jing Wu,Yaxiong Wang,Lechao Cheng,Shengeng Tang,Tianrui Hui,Nan Pu,Zhun Zhong*

Main category: cs.CV

TL;DR: 本文提出了OmniVL-Guard，一种用于多模态信息伪造检测与定位的统一框架，能够同时处理文本、图像和视频的复杂交互，并实现强大的检测及溯源能力。


<details>
  <summary>Details</summary>
Motivation: 现有的伪造检测方法多局限于单模态或双模态，无法有效处理真实场景中常见的多种模态交织（如文本、图片、视频）的假信息。本研究旨在弥补这一研究空白，提出能够统一处理多模态伪造的框架。

Method: 提出了OmniVL-Guard框架，包括自进化推理链生成（Self-Evolving CoT Generation）和自适应奖励缩放策略优化（ARSPO）。自进化推理链生成用于合成高质量的推理路径，帮助模型从冷启动中突破；ARSPO则动态调整奖励及任务权重，实现多任务的平衡优化。

Result: 大量实验证明，OmniVL-Guard在多模态伪造检测和定位任务中全面优于目前主流方法，并能在零样本的跨领域场景下表现出很好的泛化能力。

Conclusion: OmniVL-Guard在多模态跨领域伪造检测与定位任务中展现出显著优势，为应对真实世界复杂的数据信息伪造挑战，提供了新的先进解决方案。

Abstract: Existing forgery detection methods are often limited to uni-modal or bi-modal settings, failing to handle the interleaved text, images, and videos prevalent in real-world misinformation. To bridge this gap, this paper targets to develop a unified framework for omnibus vision-language forgery detection and grounding. In this unified setting, the {interplay} between diverse modalities and the dual requirements of simultaneous detection and localization pose a critical ``difficulty bias`` problem: the simpler veracity classification task tends to dominate the gradients, leading to suboptimal performance in fine-grained grounding during multi-task optimization. To address this challenge, we propose \textbf{OmniVL-Guard}, a balanced reinforcement learning framework for omnibus vision-language forgery detection and grounding. Particularly, OmniVL-Guard comprises two core designs: Self-Evolving CoT Generatio and Adaptive Reward Scaling Policy Optimization (ARSPO). {Self-Evolving CoT Generation} synthesizes high-quality reasoning paths, effectively overcoming the cold-start challenge. Building upon this, {Adaptive Reward Scaling Policy Optimization (ARSPO)} dynamically modulates reward scales and task weights, ensuring a balanced joint optimization. Extensive experiments demonstrate that OmniVL-Guard significantly outperforms state-of-the-art methods and exhibits zero-shot robust generalization across out-of-domain scenarios.

</details>


### [49] [AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Language-Action Models](https://arxiv.org/abs/2602.10698)
*Zhifeng Rao,Wenlong Chen,Lei Xie,Xia Hua,Dongfu Yin,Zhen Tian,F. Richard Yu*

Main category: cs.CV

TL;DR: 该论文提出通过整合深度估计技术，提升视觉-语言-动作（VLA）模型对三维空间的理解能力，从而显著提升机器人在复杂三维环境中的感知与控制表现。


<details>
  <summary>Details</summary>
Motivation: 传统VLA模型大多基于二维图像训练，导致其在处理复杂三维空间时空间理解和动作关联能力有限，因此亟需方法弥补二维-三维感知间的鸿沟。

Method: 作者提出在VLA模型中融入深度估计模块（采用VGGT），利用普通RGB输入提取3D几何特征。同时提出动作辅助模块，通过动作先验约束所学3D表示，与下游控制任务保持一致。最终将这些增强的3D特征与传统二维视觉token融合。

Result: 实验结果显示，该方法在几何信息模糊场景下提升了机器人感知和动作预测准确率，模型泛化能力和鲁棒性均有显著增强。

Conclusion: 结合深度估计与动作先验能帮助VLA模型打破二维观测的局限，支持更强的三维空间理解和决策，为机器人系统的三维感知和控制能力提供了新途径。

Abstract: Vision-Language-Action (VLA) models have recently achieved remarkable progress in robotic perception and control, yet most existing approaches primarily rely on VLM trained using 2D images, which limits their spatial understanding and action grounding in complex 3D environments. To address this limitation, we propose a novel framework that integrates depth estimation into VLA models to enrich 3D feature representations. Specifically, we employ a depth estimation baseline called VGGT to extract geometry-aware 3D cues from standard RGB inputs, enabling efficient utilization of existing large-scale 2D datasets while implicitly recovering 3D structural information. To further enhance the reliability of these depth-derived features, we introduce a new module called action assistant, which constrains the learned 3D representations with action priors and ensures their consistency with downstream control tasks. By fusing the enhanced 3D features with conventional 2D visual tokens, our approach significantly improves the generalization ability and robustness of VLA models. Experimental results demonstrate that the proposed method not only strengthens perception in geometrically ambiguous scenarios but also leads to superior action prediction accuracy. This work highlights the potential of depth-driven data augmentation and auxiliary expert supervision for bridging the gap between 2D observations and 3D-aware decision-making in robotic systems.

</details>


### [50] [FGAA-FPN: Foreground-Guided Angle-Aware Feature Pyramid Network for Oriented Object Detection](https://arxiv.org/abs/2602.10710)
*Jialin Ma*

Main category: cs.CV

TL;DR: 本文提出了一种新型的面向物体检测的特征金字塔网络FGAA-FPN，能更好地处理遥感影像中目标方向与前景建模问题，在多个数据集上取得了最新的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法对于遥感和航空影像中的方向性目标检测，主要依赖多尺度特征融合和上下文建模，但缺乏对前景目标的显式建模，未能充分利用方向先验，导致特征区分性不足。为此，需要设计能增强目标表示并利用角度信息的新方法。

Method: 提出FGAA-FPN框架，分为两个核心模块：1）前景引导的特征调制模块，在弱监督下学习显著性，提升目标区域特征、抑制背景干扰；2）角度感知多头注意力模块，编码特征间方向关系，提升高层语义特征的全局建模能力。这两个创新设计结合不同尺度与语义级别，有效提升了金字塔特征表达。

Result: 在DOTA v1.0和DOTA v1.5数据集上，FGAA-FPN分别取得了75.5%和68.3%的mAP，超过了当前的主流方法，验证了所提方法的有效性。

Conclusion: 通过引入前景引导与角度感知机制，FGAA-FPN显著提升了方向性目标检测的区分性和鲁棒性，为遥感/航空遥感等高分辨影像的实际应用提供了更强有力的技术支撑。

Abstract: With the increasing availability of high-resolution remote sensing and aerial imagery, oriented object detection has become a key capability for geographic information updating, maritime surveillance, and disaster response. However, it remains challenging due to cluttered backgrounds, severe scale variation, and large orientation changes. Existing approaches largely improve performance through multi-scale feature fusion with feature pyramid networks or contextual modeling with attention, but they often lack explicit foreground modeling and do not leverage geometric orientation priors, which limits feature discriminability. To overcome these limitations, we propose FGAA-FPN, a Foreground-Guided Angle-Aware Feature Pyramid Network for oriented object detection. FGAA-FPN is built on a hierarchical functional decomposition that accounts for the distinct spatial resolution and semantic abstraction across pyramid levels, thereby strengthening multi-scale representations. Concretely, a Foreground-Guided Feature Modulation module learns foreground saliency under weak supervision to enhance object regions and suppress background interference in low-level features. In parallel, an Angle-Aware Multi-Head Attention module encodes relative orientation relationships to guide global interactions among high-level semantic features. Extensive experiments on DOTA v1.0 and DOTA v1.5 demonstrate that FGAA-FPN achieves state-of-the-art results, reaching 75.5% and 68.3% mAP, respectively.

</details>


### [51] [Ecological mapping with geospatial foundation models](https://arxiv.org/abs/2602.10720)
*Craig Mahlasi,Gciniwe S. Baloyi,Zaheed Gaffoor,Levente Klein,Anne Jones,Etienne Vos,Michal Muszynski,Geoffrey Dawson,Campbell Watson*

Main category: cs.CV

TL;DR: 本文探讨了地理空间基础模型（GFM）在生态应用中的表现，比较了主流AI模型和基准模型的优劣。结果显示，GFM在相关任务上优于传统ResNet模型，TerraMind表现尤为突出，但具体效果仍受训练数据和分辨率影响。


<details>
  <summary>Details</summary>
Motivation: 地理空间基础模型虽然在诸多地理空间任务中崛起迅速，但其在高价值生态应用领域的效用与挑战尚未被充分评估，因此需要实证研究其能力和潜力。

Method: 对Prithvi-E0-2.0和TerraMind等预训练AI模型通过微调应用于三个用例，并与基线模型ResNet-101进行对比。任务包括土地利用/覆盖层生成、森林功能性状映射和泥炭地检测。对不同输入模态的影响和模型表现进行了系统评估。

Result: 在所有实验任务上，GFM模型均优于基线ResNet模型。TerraMind整体表现略优于Prithvi，特别是在引入多模态数据后，TerraMind显著优于其他模型。同时发现训练数据模态与预训练差异较大会影响表现。

Conclusion: GFM，尤其是TerraMind，在生态地理任务中极具潜力，但模型效果依赖于高分辨率与高质量标签数据。如果能改善数据分辨率和标注精度，模型将进一步提升性能。

Abstract: Geospatial foundation models (GFMs) are a fast-emerging paradigm for various geospatial tasks, such as ecological mapping. However, the utility of GFMs has not been fully explored for high-value use cases. This study aims to explore the utility, challenges and opportunities associated with the application of GFMs for ecological uses. In this regard, we fine-tune several pretrained AI models, namely, Prithvi-E0-2.0 and TerraMind, across three use cases, and compare this with a baseline ResNet-101 model. Firstly, we demonstrate TerraMind's LULC generation capabilities. Lastly, we explore the utility of the GFMs in forest functional trait mapping and peatlands detection. In all experiments, the GFMs outperform the baseline ResNet models. In general TerraMind marginally outperforms Prithvi. However, with additional modalities TerraMind significantly outperforms the baseline ResNet and Prithvi models. Nonetheless, consideration should be given to the divergence of input data from pretrained modalities. We note that these models would benefit from higher resolution and more accurate labels, especially for use cases where pixel-level dynamics need to be mapped.

</details>


### [52] [A Diffusion-Based Generative Prior Approach to Sparse-view Computed Tomography](https://arxiv.org/abs/2602.10722)
*Davide Evangelista,Pasquale Cascarano,Elena Loli Piccolomini*

Main category: cs.CV

TL;DR: 本文探讨了如何利用扩散生成模型结合迭代优化算法，在稀疏采样的条件下重建CT图像，有效抑制伪影并提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 稀疏或有限角度采集的CT重建常导致图像伪影甚至物体畸变。为克服采集数据不足的问题，研究者寻求高效且可解释的新方法。

Method: 本文采用Deep Generative Prior (DGP)框架，将扩散生成模型的强生成能力与解释性强的模型方法结合，通过迭代优化重建稀疏CT投影的图像，并对图像生成、模型设计和优化算法提出新改进。

Result: 在极度稀疏的采集条件下，该方法依然获得了有希望的重建结果，展现出优于传统方法的潜力。

Conclusion: 扩散生成模型与优化算法的结合能提升稀疏CT重建质量，但该领域仍需进一步深入研究以推动其成熟应用。

Abstract: The reconstruction of X-rays CT images from sparse or limited-angle geometries is a highly challenging task. The lack of data typically results in artifacts in the reconstructed image and may even lead to object distortions. For this reason, the use of deep generative models in this context has great interest and potential success. In the Deep Generative Prior (DGP) framework, the use of diffusion-based generative models is combined with an iterative optimization algorithm for the reconstruction of CT images from sinograms acquired under sparse geometries, to maintain the explainability of a model-based approach while introducing the generative power of a neural network. There are therefore several aspects that can be further investigated within these frameworks to improve reconstruction quality, such as image generation, the model, and the iterative algorithm used to solve the minimization problem, for which we propose modifications with respect to existing approaches. The results obtained even under highly sparse geometries are very promising, although further research is clearly needed in this direction.

</details>


### [53] [OccFace: Unified Occlusion-Aware Facial Landmark Detection with Per-Point Visibility](https://arxiv.org/abs/2602.10728)
*Xinhao Xiang,Zhengxin Li,Saurav Dhakad,Theo Bancroft,Jiawei Zhang,Weiyang Li*

Main category: cs.CV

TL;DR: 本文提出了一种针对遮挡人脸的高精度关键点检测方法OccFace，能有效处理多种人脸（包括非人类风格）下的大遮挡与自遮挡问题。


<details>
  <summary>Details</summary>
Motivation: 当前人脸关键点检测在遇到严重遮挡和不同风格人脸（如卡通、非人脸等）时效果欠佳，且现有方法无法同时预测每个关键点的可见性，这对后续应用造成影响。

Method: OccFace框架采用统一的密集100点布局和基于热图的主干网络，设计遮挡模块同时预测关键点位置和逐点可见性。可见性标注通过手工与伪可见性标签结合生成，并构建新的遮挡评测基准（包括多指标评测及新数据集）。

Result: 实验表明OccFace在遮挡严重及大角度旋转情况下显著提升了带遮挡区域的鲁棒性，对可见区域精度无损。

Conclusion: OccFace兼顾可见性和位置的预测能力，为人脸及类人脸遮挡场景下的关键点检测提供了更强大且通用的解决方案。

Abstract: Accurate facial landmark detection under occlusion remains challenging, especially for human-like faces with large appearance variation and rotation-driven self-occlusion. Existing detectors typically localize landmarks while handling occlusion implicitly, without predicting per-point visibility that downstream applications can benefits. We present OccFace, an occlusion-aware framework for universal human-like faces, including humans, stylized characters, and other non-human designs. OccFace adopts a unified dense 100-point layout and a heatmap-based backbone, and adds an occlusion module that jointly predicts landmark coordinates and per-point visibility by combining local evidence with cross-landmark context. Visibility supervision mixes manual labels with landmark-aware masking that derives pseudo visibility from mask-heatmap overlap. We also create an occlusion-aware evaluation suite reporting NME on visible vs. occluded landmarks and benchmarking visibility with Occ AP, F1@0.5, and ROC-AUC, together with a dataset annotated with 100-point landmarks and per-point visibility. Experiments show improved robustness under external occlusion and large head rotations, especially on occluded regions, while preserving accuracy on visible landmarks.

</details>


### [54] [Self-Supervised Image Super-Resolution Quality Assessment based on Content-Free Multi-Model Oriented Representation Learning](https://arxiv.org/abs/2602.10744)
*Kian Majlessi,Amir Masoud Soltani,Mohammad Ebrahim Mahdavi,Aurelien Gourrier,Peyman Adibi*

Main category: cs.CV

TL;DR: 本文提出了一种针对真实低分辨率图像超分辨重建后图像质量无参考评估的新方法，称为S3 RIQA，并建立了新的数据集SRMORSS。实验结果显示，该方法在真实超分评估基准上优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有超分辨图像的无参考质量评估方法主要针对人工降质生成的合成低分辨率图像，而在实际应用中，真实图像退化复杂且不可预测，现有方法难以应对，因此有必要提出适用于真实场景的新型无参考质量评估方法。

Method: 提出了一种自监督对比学习框架，将同一超分模型生成的图像作为正样本对，不同模型生成的作为负样本对，与图像内容无关。还引入了针对性预处理以提取补充的质量信息，并通过辅助任务处理不同放大因子的退化模式。此外，构建了新的SRMORSS数据集，涵盖多种SR算法和大量真实低分辨率图像，支持无监督训练。

Result: 在真实超分辨无参考质量评估任务上的基准测试证明，S3 RIQA在大部分相关主流评价指标上都取得了优于现有方法的性能。

Conclusion: S3 RIQA可有效适应真实复杂退化条件下的超分重建质量评估，特别适用于数据稀缺领域，为实际应用中的超分图像质量评估提供了强有力的新工具。

Abstract: Super-resolution (SR) applied to real-world low-resolution (LR) images often results in complex, irregular degradations that stem from the inherent complexity of natural scene acquisition. In contrast to SR artifacts arising from synthetic LR images created under well-defined scenarios, those distortions are highly unpredictable and vary significantly across different real-life contexts. Consequently, assessing the quality of SR images (SR-IQA) obtained from realistic LR, remains a challenging and underexplored problem. In this work, we introduce a no-reference SR-IQA approach tailored for such highly ill-posed realistic settings. The proposed method enables domain-adaptive IQA for real-world SR applications, particularly in data-scarce domains. We hypothesize that degradations in super-resolved images are strongly dependent on the underlying SR algorithms, rather than being solely determined by image content. To this end, we introduce a self-supervised learning (SSL) strategy that first pretrains multiple SR model oriented representations in a pretext stage. Our contrastive learning framework forms positive pairs from images produced by the same SR model and negative pairs from those generated by different methods, independent of image content. The proposed approach S3 RIQA, further incorporates targeted preprocessing to extract complementary quality information and an auxiliary task to better handle the various degradation profiles associated with different SR scaling factors. To this end, we constructed a new dataset, SRMORSS, to support unsupervised pretext training; it includes a wide range of SR algorithms applied to numerous real LR images, which addresses a gap in existing datasets. Experiments on real SR-IQA benchmarks demonstrate that S3 RIQA consistently outperforms most state-of-the-art relevant metrics.

</details>


### [55] [Spectral-Spatial Contrastive Learning Framework for Regression on Hyperspectral Data](https://arxiv.org/abs/2602.10745)
*Mohamad Dhaini,Paul Honeine,Maxime Berar,Antonin Van Exem*

Main category: cs.CV

TL;DR: 本文提出了一种针对高光谱数据回归任务的谱-空对比学习框架，可提升包括3D卷积和Transformer等多种主干模型的表现，并通过有效的数据增强提升整体性能。


<details>
  <summary>Details</summary>
Motivation: 虽然对比学习在表征学习和图像分类方面取得了巨大成功，但在回归任务，尤其是针对高光谱数据的应用方面，相关研究仍然较少。

Method: 提出了一个谱-空对比学习框架，并且是模型无关的，能用于增强3D卷积和Transformer等不同结构的主干网络，同时设计了一系列适用于高光谱数据的数据增强变换方法。

Result: 在合成与真实高光谱数据集上的实验表明，该框架及其数据增强方法能显著提升多种主干模型在回归任务上的性能。

Conclusion: 所提谱-空对比学习框架具备通用性和有效性，能显著优化高光谱回归任务的表现，为相关领域的研究和应用提供了有力工具。

Abstract: Contrastive learning has demonstrated great success in representation learning, especially for image classification tasks. However, there is still a shortage in studies targeting regression tasks, and more specifically applications on hyperspectral data. In this paper, we propose a spectral-spatial contrastive learning framework for regression tasks for hyperspectral data, in a model-agnostic design allowing to enhance backbones such as 3D convolutional and transformer-based networks. Moreover, we provide a collection of transformations relevant for augmenting hyperspectral data. Experiments on synthetic and real datasets show that the proposed framework and transformations significantly improve the performance of all studied backbone models.

</details>


### [56] [Text-to-Vector Conversion for Residential Plan Design](https://arxiv.org/abs/2602.10757)
*Egor Bazhenov,Stepan Kasai,Viacheslav Shalamov,Valeria Efimova*

Main category: cs.CV

TL;DR: 本论文提出了一种新方法，可将文本描述自动生成高质量的矢量住宅平面图，并提出将光栅图转换为结构化矢量图的新算法。所提方法在视觉质量指标（CLIPScore）上超越现有方案。


<details>
  <summary>Details</summary>
Motivation: 矢量图具有可扩展、无损的优点，尤其适合设计和建筑等领域。然而，目前从文本生成矢量图及将光栅图高质量转化为矢量图仍面临难题，因此研究更加高效、准确的生成方法具有重要意义。

Method: 提出了一种新颖的方法，从文本描述生成矢量住宅平面图。该方法针对结构特征（如直角）和灵活设定进行了优化。同时，提出了一种用于将光栅平面图矢量化成结构化矢量图的新算法。

Result: 新方法在视觉质量上按CLIPScore评价，较现有方案提升约5%。新矢量化算法也实现了比其他方法高约4%的CLIPScore。

Conclusion: 本文提出的文本到矢量图生成方法与平面图矢量化算法均有效提升了输出图像的视觉质量。新方案为设计与建筑领域提供了更强的工具，有助于提高工作效率和成果质量。

Abstract: Computer graphics, comprising both raster and vector components, is a fundamental part of modern science, industry, and digital communication. While raster graphics offer ease of use, its pixel-based structure limits scalability. Vector graphics, defined by mathematical primitives, provides scalability without quality loss, however, it is more complex to produce. For design and architecture, the versatility of vector graphics is paramount, despite its computational demands. This paper introduces a novel method for generating vector residential plans from textual descriptions. Our approach surpasses existing solutions by approximately 5% in CLIPScore-based visual quality, benefiting from its inherent handling of right angles and flexible settings. Additionally, we present a new algorithm for vectorizing raster plans into structured vector images. Such images have a better CLIPscore compared to others by about 4%.

</details>


### [57] [Dual-End Consistency Model](https://arxiv.org/abs/2602.10764)
*Linwei Dong,Ruoyu Guo,Ge Bai,Zehuan Yuan,Yawei Luo,Changqing Zou*

Main category: cs.CV

TL;DR: 本文针对扩散和流生成模型推断速度慢的问题，提出Dual-End Consistency Model（DE-CM），通过关键子轨迹选择和结构创新，既提升训练稳定性，也提高采样灵活性。实验结果取得了新的性能记录。


<details>
  <summary>Details</summary>
Motivation: 主流的一致性模型（CMs）虽然通过蒸馏提升了生成效率，但仍然在训练不稳定和采样方式受限两个方面存在局限，影响了大规模实际应用。现有方法多从模型结构和损失正则化尝试改进，却忽视了轨迹选择的重要性。

Method: 作者首先分析现有一致性模型的两大问题：1）训练不稳定源于不稳定的自监督项导致的损失发散；2）采样不灵活则因误差累积。为此，作者提出DE-CM方法，将PF-ODE轨迹分解并选取三个关键子轨迹作为优化目标，结合连续一致性模型损失和流匹配正则，提高训练稳定性。同时创新性地引入噪声-至-噪声（N2N）映射机制，减少采样首步的误差累积。

Result: 在ImageNet 256x256数据集上，所提方法在一次采样生成（one-step generation）任务中达到最优的FID分数1.70，超越了现有CM类方法。

Conclusion: DE-CM通过关键子轨迹选择与正则设计，解决了训练不稳定和采样不灵活的问题，为一致性模型的高效、高质量生成奠定了基础，推动了其实际应用进程。

Abstract: The slow iterative sampling nature remains a major bottleneck for the practical deployment of diffusion and flow-based generative models. While consistency models (CMs) represent a state-of-the-art distillation-based approach for efficient generation, their large-scale application is still limited by two key issues: training instability and inflexible sampling. Existing methods seek to mitigate these problems through architectural adjustments or regularized objectives, yet overlook the critical reliance on trajectory selection. In this work, we first conduct an analysis on these two limitations: training instability originates from loss divergence induced by unstable self-supervised term, whereas sampling inflexibility arises from error accumulation. Based on these insights and analysis, we propose the Dual-End Consistency Model (DE-CM) that selects vital sub-trajectory clusters to achieve stable and effective training. DE-CM decomposes the PF-ODE trajectory and selects three critical sub-trajectories as optimization targets. Specifically, our approach leverages continuous-time CMs objectives to achieve few-step distillation and utilizes flow matching as a boundary regularizer to stabilize the training process. Furthermore, we propose a novel noise-to-noisy (N2N) mapping that can map noise to any point, thereby alleviating the error accumulation in the first step. Extensive experimental results show the effectiveness of our method: it achieves a state-of-the-art FID score of 1.70 in one-step generation on the ImageNet 256x256 dataset, outperforming existing CM-based one-step approaches.

</details>


### [58] [Chatting with Images for Introspective Visual Thinking](https://arxiv.org/abs/2602.11073)
*Junfei Wu,Jian Guan,Qiang Liu,Shu Wu,Liang Wang,Wei Wu,Tienie Tan*

Main category: cs.CV

TL;DR: 本文提出了一个新的大规模视觉-语言模型（LVLM）推理框架“chatting with images”，通过语言引导动态多区域视觉重新编码，有效提升复杂视觉推理能力，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前的LVLM多依赖单次视觉编码和文字推理，导致细粒度视觉信息丢失，且现有视觉操作方法与语言的语义结合不紧密，尤其在需要跨区域、跨图像推理时效果有限。

Method: 提出“chatting with images”框架，将视觉操作重塑为由语言引导的特征调节，利用动态视觉编码器在多区域实现联合重新编码，并通过两阶段（有监督微调+强化学习）训练提升推理能力。

Result: 在八个标准数据集上广泛测试，该方法表现出显著且一致的提升，尤其在复杂的多图像和视频空间推理任务上优势明显。

Conclusion: 通过紧密结合语言推理与视觉状态更新，新框架能更好地捕获跨模态对齐关系，提升LVLM在复杂视觉-语言推理场景下的表现，有望推动跨模态理解的发展。

Abstract: Current large vision-language models (LVLMs) typically rely on text-only reasoning based on a single-pass visual encoding, which often leads to loss of fine-grained visual information. Recently the proposal of ''thinking with images'' attempts to alleviate this limitation by manipulating images via external tools or code; however, the resulting visual states are often insufficiently grounded in linguistic semantics, impairing effective cross-modal alignment - particularly when visual semantics or geometric relationships must be reasoned over across distant regions or multiple images. To address these challenges, we propose ''chatting with images'', a new framework that reframes visual manipulation as language-guided feature modulation. Under the guidance of expressive language prompts, the model dynamically performs joint re-encoding over multiple image regions, enabling tighter coupling between linguistic reasoning and visual state updates. We instantiate this paradigm in ViLaVT, a novel LVLM equipped with a dynamic vision encoder explicitly designed for such interactive visual reasoning, and trained it with a two-stage curriculum combining supervised fine-tuning and reinforcement learning to promote effective reasoning behaviors. Extensive experiments across eight benchmarks demonstrate that ViLaVT achieves strong and consistent improvements, with particularly pronounced gains on complex multi-image and video-based spatial reasoning tasks.

</details>


### [59] [RSHallu: Dual-Mode Hallucination Evaluation for Remote-Sensing Multimodal Large Language Models with Domain-Tailored Mitigation](https://arxiv.org/abs/2602.10799)
*Zihui Zhou,Yong Feng,Yanying Chen,Guofan Duan,Zhenxi Song,Mingliang Zhou,Weijia Jia*

Main category: cs.CV

TL;DR: 本论文系统性研究了遥感多模态大模型中的幻觉问题，提出了定义、评测和缓解的完整框架，并提出有效提升幻觉免疫能力的方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在遥感领域表现优秀，但幻觉（与输入图像不符的输出）问题严重，尤其在高风险场景下影响部署。此前在遥感领域对此问题研究不足，需要系统性探讨和应对。

Method: 1）提出RS幻觉新分类法，涵盖遥感特定的图像级幻觉；2）构建RSHalluEval基准，以及高精度云端和低成本本地双模式自动检测工具；3）建立训练友好的幻觉缓解数据集RSHalluShield，并提出零训练即插即用的解码修正规则和感知提示作为幻觉缓解对策。

Result: 所提缓解方法可在主流遥感多模态大模型上，将幻觉免疫率最高提升21.63个百分点，同时下游任务性能相当。

Conclusion: 该研究为遥感多模态模型的幻觉问题提供了体系化定义、评测、缓解方案，显著提升了高风险领域的实际可用性，推动了遥感智能分析的安全应用。

Abstract: Multimodal large language models (MLLMs) are increasingly adopted in remote sensing (RS) and have shown strong performance on tasks such as RS visual grounding (RSVG), RS visual question answering (RSVQA), and multimodal dialogue. However, hallucinations, which are responses inconsistent with the input RS images, severely hinder their deployment in high-stakes scenarios (e.g., emergency management and agricultural monitoring) and remain under-explored in RS. In this work, we present RSHallu, a systematic study with three deliverables: (1) we formalize RS hallucinations with an RS-oriented taxonomy and introduce image-level hallucination to capture RS-specific inconsistencies beyond object-centric errors (e.g., modality, resolution, and scene-level semantics); (2) we build a hallucination benchmark RSHalluEval (2,023 QA pairs) and enable dual-mode checking, supporting high-precision cloud auditing and low-cost reproducible local checking via a compact checker fine-tuned on RSHalluCheck dataset (15,396 QA pairs); and (3) we introduce a domain-tailored dataset RSHalluShield (30k QA pairs) for training-friendly mitigation and further propose training-free plug-and-play strategies, including decoding-time logit correction and RS-aware prompting. Across representative RS-MLLMs, our mitigation improves the hallucination-free rate by up to 21.63 percentage points under a unified protocol, while maintaining competitive performance on downstream RS tasks (RSVQA/RSVG). Code and datasets will be released.

</details>


### [60] [DMP-3DAD: Cross-Category 3D Anomaly Detection via Realistic Depth Map Projection with Few Normal Samples](https://arxiv.org/abs/2602.10806)
*Zi Wang,Katsuya Hotta,Koichiro Kamide,Yawen Zou,Jianjian Qin,Chao Zhang,Jun Yu*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练即可进行跨类别3D点云异常检测的方法，通过多视角深度图和CLIP编码器实现，无需针对具体类别微调，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云异常检测方法多数需要针对每个类别进行训练，灵活性较差，尤其在只提供少量正常样本（few-shot）时效果有限。论文旨在解决跨类别场景下无需针对性训练即可实现高效异常检测的问题。

Method: 作者提出DMP-3DAD算法，将3D点云投影为固定数量的真实感深度图像，再利用冻结的CLIP视觉编码器提取多视角特征；最后通过加权特征相似度进行异常检测，整个流程无需再训练，也不依赖类别适配。

Result: 在ShapeNetPart数据集上的实验显示，DMP-3DAD在few-shot场景下表现优异，达到了当前最好的性能。

Conclusion: DMP-3DAD为跨类别3D点云异常检测提供了一种简单、高效、无需训练的新方案，具有很好的实际应用前景。

Abstract: Cross-category anomaly detection for 3D point clouds aims to determine whether an unseen object belongs to a target category using only a few normal examples. Most existing methods rely on category-specific training, which limits their flexibility in few-shot scenarios. In this paper, we propose DMP-3DAD, a training-free framework for cross-category 3D anomaly detection based on multi-view realistic depth map projection. Specifically, by converting point clouds into a fixed set of realistic depth images, our method leverages a frozen CLIP visual encoder to extract multi-view representations and performs anomaly detection via weighted feature similarity, which does not require any fine-tuning or category-dependent adaptation. Extensive experiments on the ShapeNetPart dataset demonstrate that DMP-3DAD achieves state-of-the-art performance under few-shot setting. The results show that the proposed approach provides a simple yet effective solution for practical cross-category 3D anomaly detection.

</details>


### [61] [DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories](https://arxiv.org/abs/2602.10809)
*Chenlong Deng,Mengjie Deng,Junjie Wu,Dun Zeng,Teng Wang,Qingsong Xie,Jiadeng Huang,Shengjie Ma,Changwang Zhang,Zhaoxiang Wang,Jun Wang,Yutao Zhu,Zhicheng Dou*

Main category: cs.CV

TL;DR: 本文提出了深度图像检索（DeepImageSearch）新范式，将图像检索转化为自主探索任务，并构建了DISBench基准，强调了跨时间序列的多步推理能力对检索系统的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态检索系统只关注单一时刻的语义匹配，忽略了现实视觉流中的时序依赖与上下文信息分布，无法满足复杂、动态场景下准确检索的需求。

Method: 作者提出DeepImageSearch，将图像检索设定为智能体多步探索任务，要求模型基于视觉历史和隐式线索推理定位目标。为生成依赖上下文的查询，提出人机协作流程，利用视觉-语言模型挖掘潜在时空关联，并经人工审核。同时开发基于模块化智能体、细粒度工具和双记忆系统的强健基线方法。

Result: 通过DISBench基准的实验显示，当前主流模型在该基准上表现不佳，验证了该任务对时序推理和多步决策提出了更高挑战。

Conclusion: 仅靠传统片段式语义检索难以应对现实视觉流场景，下一代检索系统需融合智能体式、多步推理能力，才能实现更强的上下文感知与准确检索。

Abstract: Existing multimodal retrieval systems excel at semantic matching but implicitly assume that query-image relevance can be measured in isolation. This paradigm overlooks the rich dependencies inherent in realistic visual streams, where information is distributed across temporal sequences rather than confined to single snapshots. To bridge this gap, we introduce DeepImageSearch, a novel agentic paradigm that reformulates image retrieval as an autonomous exploration task. Models must plan and perform multi-step reasoning over raw visual histories to locate targets based on implicit contextual cues. We construct DISBench, a challenging benchmark built on interconnected visual data. To address the scalability challenge of creating context-dependent queries, we propose a human-model collaborative pipeline that employs vision-language models to mine latent spatiotemporal associations, effectively offloading intensive context discovery before human verification. Furthermore, we build a robust baseline using a modular agent framework equipped with fine-grained tools and a dual-memory system for long-horizon navigation. Extensive experiments demonstrate that DISBench poses significant challenges to state-of-the-art models, highlighting the necessity of incorporating agentic reasoning into next-generation retrieval systems.

</details>


### [62] [Why Does RL Generalize Better Than SFT? A Data-Centric Perspective on VLM Post-Training](https://arxiv.org/abs/2602.10815)
*Aojun Lu,Tao Feng,Hangjie Yuan,Wei Li,Yanan Sun*

Main category: cs.CV

TL;DR: 本文揭示了大规模视觉-语言模型（VLMs）在后训练阶段存在明显的泛化性能差距，强化学习（RL）微调优于常规有监督微调（SFT），通过数据难度过滤可以缩小甚至超越这一差距。提出了基于难度筛选的SFT方法（DC-SFT），在提升OOD泛化性能的同时兼具稳定性与高效性。


<details>
  <summary>Details</summary>
Motivation: 观察到RL微调优于SFT在OOD泛化任务上的表现，但原因不明。作者想要解释并弥补这种泛化表现差距，推动VLMs更好地泛化。

Method: 提出了数据难度过滤的视角，通过系统性地比较不同难度样本集训练的SFT模型泛化表现，设计了基于难度筛选的SFT（DC-SFT）方法，即在训练集中显式过滤过难样本，仅用中等难度样本进行微调。

Result: 实验证明，过多依赖高难度样本会显著降低SFT模型在OOD上的性能，而DC-SFT不仅显著提升了标准SFT的OOD表现，还优于RL微调，并且更加稳定高效。

Conclusion: 数据难度对泛化具有关键影响，难度筛选方法为VLMs实现高效、稳定和鲁棒的泛化建立了有效路径。

Abstract: The adaptation of large-scale Vision-Language Models (VLMs) through post-training reveals a pronounced generalization gap: models fine-tuned with Reinforcement Learning (RL) consistently achieve superior out-of-distribution (OOD) performance compared to those trained with Supervised Fine-Tuning (SFT). This paper posits a data-centric explanation for this phenomenon, contending that RL's generalization advantage arises from an implicit data filtering mechanism that inherently prioritizes medium-difficulty training samples. To test this hypothesis, we systematically evaluate the OOD generalization of SFT models across training datasets of varying difficulty levels. Our results confirm that data difficulty is a critical factor, revealing that training on hard samples significantly degrades OOD performance. Motivated by this finding, we introduce Difficulty-Curated SFT (DC-SFT), a straightforward method that explicitly filters the training set based on sample difficulty. Experiments show that DC-SFT not only substantially enhances OOD generalization over standard SFT, but also surpasses the performance of RL-based training, all while providing greater stability and computational efficiency. This work offers a data-centric account of the OOD generalization gap in VLMs and establishes a more efficient pathway to achieving robust generalization. Code is available at https://github.com/byyx666/DC-SFT.

</details>


### [63] [Resource-Efficient RGB-Only Action Recognition for Edge Deployment](https://arxiv.org/abs/2602.10818)
*Dongsik Yoon,Jongeun Kim,Dayeon Lee*

Main category: cs.CV

TL;DR: 本文提出了一种紧凑的仅用RGB数据的动作识别模型，专为边缘设备设计，兼顾准确率和效率，并在Jetson Orin Nano等设备上有效减少资源占用。


<details>
  <summary>Details</summary>
Motivation: 边缘设备在做动作识别时对延迟、内存、存储和能耗有严格要求，而现有增强方式如骨架、深度信息往往需要额外硬件或高昂的计算代价，不适合实际部署。作者希望用仅仅RGB视频数据就能实现高效高精度的动作识别，提升在嵌入式设备上的实用性。

Method: 模型以X3D风格为基础网络骨架，融合了Temporal Shift（时序位移），同时提出了选择性时序自适应和无需新增参数的注意力机制，仅用RGB数据，无需额外传感器和复杂的姿态估计算法。

Result: 在NTU RGB+D 60和120数据集上进行了大量实验，证明精度和效率都有很好的平衡，并通过Jetson Orin Nano上的真实部署测试，体现了模型在设备端的低资源占用和高实际可用性，超越了以往RGB动作识别方法。

Conclusion: 本文方法能在不依赖额外传感器情况下，实现高效且高精度的动作识别，非常适合资源受限的边缘设备，为实际部署打开新思路。

Abstract: Action recognition on edge devices poses stringent constraints on latency, memory, storage, and power consumption. While auxiliary modalities such as skeleton and depth information can enhance recognition performance, they often require additional sensors or computationally expensive pose-estimation pipelines, limiting practicality for edge use. In this work, we propose a compact RGB-only network tailored for efficient on-device inference. Our approach builds upon an X3D-style backbone augmented with Temporal Shift, and further introduces selective temporal adaptation and parameter-free attention. Extensive experiments on the NTU RGB+D 60 and 120 benchmarks demonstrate a strong accuracy-efficiency balance. Moreover, deployment-level profiling on the Jetson Orin Nano verifies a smaller on-device footprint and practical resource utilization compared to existing RGB-based action recognition techniques.

</details>


### [64] [Flow caching for autoregressive video generation](https://arxiv.org/abs/2602.10825)
*Yuexiao Ma,Xuzhe Zheng,Jing Xu,Xiwei Xu,Feng Ling,Xiawu Zheng,Huafeng Kuang,Huixia Li,Xing Wang,Xuefeng Xiao,Fei Chao,Rongrong Ji*

Main category: cs.CV

TL;DR: 提出了一种名为FlowCache的专为自回归视频生成设计的缓存框架，在极大提高生成速度的同时，几乎不损失生成质量。


<details>
  <summary>Details</summary>
Motivation: 自回归视频生成模型虽然在生成超长视频时表现出色，但其顺序生成过程非常缓慢。现有的视频扩散模型加速方法假设所有视频帧去噪特性一致，但这种假设在自回归模型中并不成立，因此需要新的加速策略。

Method: FlowCache 支持每个视频分块独立缓存，允许对每一分块在每个时间步需要重算与缓存的部分精细管理。方法包括动态分块缓存策略，并结合一种联合重要性-冗余优化的KV缓存压缩机制，可控内存占用的同时保证生成质量。

Result: 在MAGI-1和SkyReels-V2数据集上，FlowCache分别实现了2.38倍和6.7倍的生成加速，而且几乎没有质量下降（VBench分在0.87提升和0.79下降范围）。

Conclusion: FlowCache有效解决了自回归视频生成的效率瓶颈，使其实时生成超长视频成为可能，为大规模高效视频生成树立了新标杆。

Abstract: Autoregressive models, often built on Transformer architectures, represent a powerful paradigm for generating ultra-long videos by synthesizing content in sequential chunks. However, this sequential generation process is notoriously slow. While caching strategies have proven effective for accelerating traditional video diffusion models, existing methods assume uniform denoising across all frames-an assumption that breaks down in autoregressive models where different video chunks exhibit varying similarity patterns at identical timesteps. In this paper, we present FlowCache, the first caching framework specifically designed for autoregressive video generation. Our key insight is that each video chunk should maintain independent caching policies, allowing fine-grained control over which chunks require recomputation at each timestep. We introduce a chunkwise caching strategy that dynamically adapts to the unique denoising characteristics of each chunk, complemented by a joint importance-redundancy optimized KV cache compression mechanism that maintains fixed memory bounds while preserving generation quality. Our method achieves remarkable speedups of 2.38 times on MAGI-1 and 6.7 times on SkyReels-V2, with negligible quality degradation (VBench: 0.87 increase and 0.79 decrease respectively). These results demonstrate that FlowCache successfully unlocks the potential of autoregressive models for real-time, ultra-long video generation-establishing a new benchmark for efficient video synthesis at scale. The code is available at https://github.com/mikeallen39/FlowCache.

</details>


### [65] [Hyperspectral Smoke Segmentation via Mixture of Prototypes](https://arxiv.org/abs/2602.10858)
*Lujian Yao,Haitao Zhao,Xianghai Kong,Yuhan Xu*

Main category: cs.CV

TL;DR: 本文提出了一种基于高光谱成像的烟雾分割方法，建立了首个高光谱烟雾分割数据集，并提出了具有谱带自适应加权能力的网络，有效提升了烟雾分割精度，尤其是在多种复杂场景下。


<details>
  <summary>Details</summary>
Motivation: 传统可见光图像方法在烟雾分割上易受光谱信息不足影响，难以区分烟雾和云等干扰，特别是在半透明烟雾区域。为此，利用高光谱影像以获取更多判别信息，提升烟雾分割的鲁棒性和准确性。

Method: 作者首先采集并精细标注了包含20种实际场景和18,000帧的高光谱烟雾分割数据集（HSSDataset）。提出了混合原型（MoP）网络，包括谱带分离、基于原型的光谱表示和空间感知谱带自适应双层加权机制，应对光谱交互污染、模式建模受限和加权难度问题。同时还构建了RGB-红外的多光谱数据集（MSSDataset）。

Result: 实验结果表明，所提出方法在高光谱和多光谱数据集上均明显优于现有方法，在复杂环境下对烟雾的分割表现尤为突出。

Conclusion: 以高光谱及多光谱信息为基础的分割新范式，有效提升了烟雾分割的准确率和适应不同场景下的能力，为烟雾检测应用提供了更优选择。

Abstract: Smoke segmentation is critical for wildfire management and industrial safety applications. Traditional visible-light-based methods face limitations due to insufficient spectral information, particularly struggling with cloud interference and semi-transparent smoke regions. To address these challenges, we introduce hyperspectral imaging for smoke segmentation and present the first hyperspectral smoke segmentation dataset (HSSDataset) with carefully annotated samples collected from over 18,000 frames across 20 real-world scenarios using a Many-to-One annotations protocol. However, different spectral bands exhibit varying discriminative capabilities across spatial regions, necessitating adaptive band weighting strategies. We decompose this into three technical challenges: spectral interaction contamination, limited spectral pattern modeling, and complex weighting router problems. We propose a mixture of prototypes (MoP) network with: (1) Band split for spectral isolation, (2) Prototype-based spectral representation for diverse patterns, and (3) Dual-level router for adaptive spatial-aware band weighting. We further construct a multispectral dataset (MSSDataset) with RGB-infrared images. Extensive experiments validate superior performance across both hyperspectral and multispectral modalities, establishing a new paradigm for spectral-based smoke segmentation.

</details>


### [66] [Stride-Net: Fairness-Aware Disentangled Representation Learning for Chest X-Ray Diagnosis](https://arxiv.org/abs/2602.10875)
*Darakshan Rashid,Raza Imam,Dwarikanath Mahapatra,Brejesh Lall*

Main category: cs.CV

TL;DR: 本文提出了一种新的深度学习方法Stride-Net，能够在保证胸片疾病诊断精度的同时，有效减少性别和种族等敏感属性带来的不公平影响，实现更公平的医学影像智能分析。


<details>
  <summary>Details</summary>
Motivation: 目前主流的胸片分类深度网络由于受到敏感人群属性（如性别、种族）的影响，部分群体可能诊断效果较差，这在临床实际中会影响公平性和安全性。已有的去偏方法通常以牺牲诊断准确率为代价，或者泛化能力有限。因此亟须能在不降低诊断效果的前提下提升各人群公平性的方法。

Method: Stride-Net提出以patch（图像块）级别进行操作，采用可学习的stride-based mask筛选与疾病标签相关的图像区块，同时通过对抗混淆损失抑制敏感属性信息。为防止模型学习捷径，还用Group Optimal Transport机制让图像特征与基于BioBERT生成的疾病语义特征语义对齐。该方法能同时实现判别性与公平性的平衡。

Result: 在MIMIC-CXR和CheXpert等权威医学影像数据集上，涵盖不同种族及性别组，Stride-Net在包括ResNet、Vision Transformers等架构下，都显著提升了公平性指标，并保持或提升了分类准确率，优于主流去偏算法的公平-准确权衡表现。

Conclusion: Stride-Net有效实现了胸片智能分析中的公平性提升而不影响整体性能，为实际安全可靠的医学AI提供了新方向。

Abstract: Deep neural networks for chest X-ray classification achieve strong average performance, yet often underperform for specific demographic subgroups, raising critical concerns about clinical safety and equity. Existing debiasing methods frequently yield inconsistent improvements across datasets or attain fairness by degrading overall diagnostic utility, treating fairness as a post hoc constraint rather than a property of the learned representation. In this work, we propose Stride-Net (Sensitive Attribute Resilient Learning via Disentanglement and Learnable Masking with Embedding Alignment), a fairness-aware framework that learns disease-discriminative yet demographically invariant representations for chest X-ray analysis. Stride-Net operates at the patch level, using a learnable stride-based mask to select label-aligned image regions while suppressing sensitive attribute information through adversarial confusion loss. To anchor representations in clinical semantics and discourage shortcut learning, we further enforce semantic alignment between image features and BioBERT-based disease label embeddings via Group Optimal Transport. We evaluate Stride-Net on the MIMIC-CXR and CheXpert benchmarks across race and intersectional race-gender subgroups. Across architectures including ResNet and Vision Transformers, Stride-Net consistently improves fairness metrics while matching or exceeding baseline accuracy, achieving a more favorable accuracy-fairness trade-off than prior debiasing approaches. Our code is available at https://github.com/Daraksh/Fairness_StrideNet.

</details>


### [67] [Chart Specification: Structural Representations for Incentivizing VLM Reasoning in Chart-to-Code Generation](https://arxiv.org/abs/2602.10880)
*Minggui He,Mingchen Dai,Jian Zhang,Yilun Liu,Shimin Tao,Pufan Zeng,Osamu Yoshie,Yuya Ieiri*

Main category: cs.CV

TL;DR: 本文提出Chart Specification结构化中间表示方法，通过结构化监督提升图表生成代码的结构一致性，显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在图表转代码时，主要依赖文本模仿的有监督微调，容易产生结构和语义不一致的问题，难以保证高保真的结构还原。

Method: 作者提出Chart Specification结构化中间表达，筛除语法噪声，构建结构平衡数据集，并设计Spec-Align Reward用于结构正确性的精细化、可验证反馈，基于此进行强化学习。

Result: 在三个公开基准数据集上，该方法在只用3K训练样本的情况下，数据效率高，复杂任务上优于主流方法最多61.7%；扩大至4K样本时在所有评测指标上均创下新SOTA。

Conclusion: 精确结构化监督为高保真的图表转代码生成提供高效途径，方法优越且数据效率高。代码和数据集公开。

Abstract: Vision-Language Models (VLMs) have shown promise in generating plotting code from chart images, yet achieving structural fidelity remains challenging. Existing approaches largely rely on supervised fine-tuning, encouraging surface-level token imitation rather than faithful modeling of underlying chart structure, which often leads to hallucinated or semantically inconsistent outputs. We propose Chart Specification, a structured intermediate representation that shifts training from text imitation to semantically grounded supervision. Chart Specification filters syntactic noise to construct a structurally balanced training set and supports a Spec-Align Reward that provides fine-grained, verifiable feedback on structural correctness, enabling reinforcement learning to enforce consistent plotting logic. Experiments on three public benchmarks show that our method consistently outperforms prior approaches. With only 3K training samples, we achieve strong data efficiency, surpassing leading baselines by up to 61.7% on complex benchmarks, and scaling to 4K samples establishes new state-of-the-art results across all evaluated metrics. Overall, our results demonstrate that precise structural supervision offers an efficient pathway to high-fidelity chart-to-code generation. Code and dataset are available at: https://github.com/Mighten/chart-specification-paper

</details>


### [68] [ResWorld: Temporal Residual World Model for End-to-End Autonomous Driving](https://arxiv.org/abs/2602.10884)
*Jinqing Zhang,Zehua Fu,Zelin Xu,Wenying Dai,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种专注于自动驾驶场景动态图建模的方法TR-World，通过计算时序残差针对性地捕获动态目标变化，无需依赖传统检测与跟踪。通过引入Future-Guided Trajectory Refinement(FGTR)模块，将未来BEV特征和轨迹相结合，大幅提升了自动驾驶规划性能。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型在自动驾驶中对静态区域冗余建模，且与规划轨迹缺乏深度交互，导致世界模型效力未能充分发挥。本文旨在解决上述局限，实现对动态对象有效建模，提高端到端自动驾驶的规划准确率。

Method: 作者提出Temporal Residual World Model (TR-World)，通过计算场景表示的时间残差，提取动态目标相关信号，仅以剩余动态信息为输入，精准预测动态目标未来空间分布。结合当前BEV特征中的静态目标信息实现未来BEV特征预测。进一步提出Future-Guided Trajectory Refinement（FGTR）模块，通过与未来BEV特征交互，利用未来路况细化轨迹，并对未来特征提供稀疏时空监督。

Result: 在自动驾驶领域主流数据集nuScenes和NAVSIM上的实验证明，所提ResWorld方法在自动驾驶路径规划任务上取得了最优的规划表现，优于现有主流方法。

Conclusion: TR-World通过动态信息建模与未来特征指导的融合，实现了更高效更精准的未来场景理解与规划，推动了端到端自动驾驶世界模型的准确性和实用性。

Abstract: The comprehensive understanding capabilities of world models for driving scenarios have significantly improved the planning accuracy of end-to-end autonomous driving frameworks. However, the redundant modeling of static regions and the lack of deep interaction with trajectories hinder world models from exerting their full effectiveness. In this paper, we propose Temporal Residual World Model (TR-World), which focuses on dynamic object modeling. By calculating the temporal residuals of scene representations, the information of dynamic objects can be extracted without relying on detection and tracking. TR-World takes only temporal residuals as input, thus predicting the future spatial distribution of dynamic objects more precisely. By combining the prediction with the static object information contained in the current BEV features, accurate future BEV features can be obtained. Furthermore, we propose Future-Guided Trajectory Refinement (FGTR) module, which conducts interaction between prior trajectories (predicted from the current scene representation) and the future BEV features. This module can not only utilize future road conditions to refine trajectories, but also provides sparse spatial-temporal supervision on future BEV features to prevent world model collapse. Comprehensive experiments conducted on the nuScenes and NAVSIM datasets demonstrate that our method, namely ResWorld, achieves state-of-the-art planning performance. The code is available at https://github.com/mengtan00/ResWorld.git.

</details>


### [69] [FastUSP: A Multi-Level Collaborative Acceleration Framework for Distributed Diffusion Model Inference](https://arxiv.org/abs/2602.10940)
*Guandong Li*

Main category: cs.CV

TL;DR: 本文提出了FastUSP框架，通过多级优化措施，提升了大规模扩散模型（如FLUX和Stable Diffusion 3）在多GPU上的推理效率。在评测中，FastUSP在FLUX模型上实现了1.12~1.16倍的端到端加速，并深入分析发现内核启动开销是主要性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前大规模扩散模型的高效分布式推理依赖有效的注意力机制并行，但现有的USP实现存在较大内核启动开销与计算-通信调度不佳，影响推理效率。为了突破这些瓶颈并充分利用现代GPU的能力，亟需优化USP的实现。

Method: FastUSP提出了三层优化：1）编译级优化，包括用CUDA Graphs进行图编译和重新排序计算与通信；2）通信级优化，用FP8量化提升集合通信性能；3）算子级优化，采用双缓冲流水Ring attention。通过这三重措施减少开销、提升并行度。

Result: 在FLUX模型（12B参数）上，FastUSP相较原始USP实现获得了1.12~1.16倍推理加速。对于Qwen-Image模型，在2卡上提升1.09倍，在4~8卡上因PyTorch Inductor与Ring attention兼容性受限，部分编译优化无法应用，不过原始USP有良好的扩展性。

Conclusion: FastUSP显著提升了大模型分布式推理效率，实验证明其内核启动开销远大于通信延迟，是GPU集群上的主要瓶颈。所提出的多级优化策略为未来大规模模型的高效推理提供了有力支持。

Abstract: Large-scale diffusion models such as FLUX (12B parameters) and Stable Diffusion 3 (8B parameters) require multi-GPU parallelism for efficient inference. Unified Sequence Parallelism (USP), which combines Ulysses and Ring attention mechanisms, has emerged as the state-of-the-art approach for distributed attention computation. However, existing USP implementations suffer from significant inefficiencies including excessive kernel launch overhead and suboptimal computation-communication scheduling. In this paper, we propose \textbf{FastUSP}, a multi-level optimization framework that integrates compile-level optimization (graph compilation with CUDA Graphs and computation-communication reordering), communication-level optimization (FP8 quantized collective communication), and operator-level optimization (pipelined Ring attention with double buffering). We evaluate FastUSP on FLUX (12B) and Qwen-Image models across 2, 4, and 8 NVIDIA RTX 5090 GPUs. On FLUX, FastUSP achieves consistent \textbf{1.12$\times$--1.16$\times$} end-to-end speedup over baseline USP, with compile-level optimization contributing the dominant improvement. On Qwen-Image, FastUSP achieves \textbf{1.09$\times$} speedup on 2 GPUs; on 4--8 GPUs, we identify a PyTorch Inductor compatibility limitation with Ring attention that prevents compile optimization, while baseline USP scales to 1.30$\times$--1.46$\times$ of 2-GPU performance. We further provide a detailed analysis of the performance characteristics of distributed diffusion inference, revealing that kernel launch overhead -- rather than communication latency -- is the primary bottleneck on modern high-bandwidth GPU interconnects.

</details>


### [70] [Healthy Harvests: A Comparative Look at Guava Disease Classification Using InceptionV3](https://arxiv.org/abs/2602.10967)
*Samanta Ghosh,Shaila Afroz Anika,Umma Habiba Ahmed,B. M. Shahria Alam,Mohammad Tahmid Noor,Nishat Tasnim Niloy*

Main category: cs.CV

TL;DR: 本研究通过深度学习方法，实现对番石榴果实疾病的高精度自动分类。


<details>
  <summary>Details</summary>
Motivation: 番石榴果实常受多种疾病影响，导致品质下降和产量损失，因此亟需早期自动化识别以降低损害。

Method: 收集473张番石榴原始图像，经增广扩充为3784张，图像经过统一预处理（256x256, RGB）。采用InceptionV3和ResNet50两种深度学习模型进行三分类（炭疽病、果蝇侵害、健康果实），并引入CutMix和MixUp等数据混合方法提升模型鲁棒性，同时用SHAP分析提升模型可解释性。

Result: InceptionV3模型达到了98.15%的分类准确率，ResNet50为94.46%。

Conclusion: 高级深度学习模型结合数据增强与可解释性分析，可有效实现番石榴果实三类病害的高精度检测和分类，有助于早期防控病害。

Abstract: Guava fruits often suffer from many diseases. This can harm fruit quality and fruit crop yield. Early identification is important for minimizing damage and ensuring fruit health. This study focuses on 3 different categories for classifying diseases. These are Anthracnose, Fruit flies, and Healthy fruit. The data set used in this study is collected from Mendeley Data. This dataset contains 473 original images of Guava. These images vary in size and format. The original dataset was resized to 256x256 pixels with RGB color mode for better consistency. After this, the Data augmentation process is applied to improve the dataset by generating variations of the original images. The augmented dataset consists of 3784 images using advanced preprocessing techniques. Two deep learning models were implemented to classify the images. The InceptionV3 model is well known for its advanced framework. These apply multiple convolutional filters for obtaining different features effectively. On the other hand, the ResNet50 model helps to train deeper networks by using residual learning. The InceptionV3 model achieved the impressive accuracy of 98.15%, and ResNet50got 94.46% accuracy. Data mixing methods such as CutMix and MixUp were applied to enhance the model's robustness. The confusion matrix was used to evaluate the overall model performance of both InceptionV3 and Resnet50. Additionally, SHAP analysis is used to improve interpretability, which helps to find the significant parts of the image for the model prediction. This study purposes to highlight how advanced models enhan

</details>


### [71] [VFGS-Net: Frequency-Guided State-Space Learning for Topology-Preserving Retinal Vessel Segmentation](https://arxiv.org/abs/2602.10978)
*Ruiqi Song,Lei Liu,Ya-Nan Zhang,Chao Wang,Xiaoning Li,Nan Mu*

Main category: cs.CV

TL;DR: 本文提出了一种新的视网膜血管分割方法VFGS-Net，有效提升了微小血管和低对比度区域的分割性能，在多个公开数据集取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 现有的视网膜血管分割方法在处理细小毛细血管、尺度变化大及低对比度时效果不佳，不能很好地兼顾细节保持与整体拓扑连续性。

Method: VFGS-Net为端到端架构，包括频域增强、双路径卷积特征学习，以及基于Mamba2的双向不对称空间建模。其频域通道注意力机制可以自适应强化与血管相关的特征响应，并在网络瓶颈层通过创新空间建模块捕捉长程依赖，增强整体血管连贯性。

Result: 在四个公开视网膜血管数据集上，VFGS-Net在细血管、复杂分支和低对比区域分割准确率领先或优于现有最佳方法。

Conclusion: VFGS-Net有效提升了视网膜血管分割的精度与鲁棒性，特别适合临床辅助诊断应用，有较强的推广及应用潜力。

Abstract: Accurate retinal vessel segmentation is a critical prerequisite for quantitative analysis of retinal images and computer-aided diagnosis of vascular diseases such as diabetic retinopathy. However, the elongated morphology, wide scale variation, and low contrast of retinal vessels pose significant challenges for existing methods, making it difficult to simultaneously preserve fine capillaries and maintain global topological continuity. To address these challenges, we propose the Vessel-aware Frequency-domain and Global Spatial modeling Network (VFGS-Net), an end-to-end segmentation framework that seamlessly integrates frequency-aware feature enhancement, dual-path convolutional representation learning, and bidirectional asymmetric spatial state-space modeling within a unified architecture. Specifically, VFGS-Net employs a dual-path feature convolution module to jointly capture fine-grained local textures and multi-scale contextual semantics. A novel vessel-aware frequency-domain channel attention mechanism is introduced to adaptively reweight spectral components, thereby enhancing vessel-relevant responses in high-level features. Furthermore, at the network bottleneck, we propose a bidirectional asymmetric Mamba2-based spatial modeling block to efficiently capture long-range spatial dependencies and strengthen the global continuity of vascular structures. Extensive experiments on four publicly available retinal vessel datasets demonstrate that VFGS-Net achieves competitive or superior performance compared to state-of-the-art methods. Notably, our model consistently improves segmentation accuracy for fine vessels, complex branching patterns, and low-contrast regions, highlighting its robustness and clinical potential.

</details>


### [72] [DFIC: Towards a balanced facial image dataset for automatic ICAO compliance verification](https://arxiv.org/abs/2602.10985)
*Nuno Gonçalves,Diogo Nunes,Carla Guerra,João Marcos*

Main category: cs.CV

TL;DR: 论文提出并公开了DFIC人脸图像数据集，旨在提升自动化ICAO标准合规检测的性能，并基于新数据集提出了利用空间注意力机制的新方法，取得了优于现有技术的效果。


<details>
  <summary>Details</summary>
Motivation: 当前机器可读旅行证件（MRTDs）对人脸照片的ICAO标准合规性要求高，但传统人工检测方法低效，尤其在高需求环境下。现有数据集在规模、注释和人口分布上均有限，影响了自动检测方法的开发和泛化能力。

Method: 构建了包含约5.8万张标注人脸图片和2706段视频、覆盖1000多名受试者且分布均衡的新数据集DFIC，涵盖了合规及大量不合规情况。在此基础上，作者基于空间注意力机制，微调并优化了自动ICAO标准合规验证的算法，并与现有最佳方案进行了对比。

Result: 新提出的方法在DFIC数据集上实现了比当前最佳方法更优的ICAO合规自动化检测结果。数据集本身在人数、条件多样性和人口分布均衡性上均优于公开数据集。

Conclusion: DFIC数据集以及改进的方法能提升ICAO标准自动人脸合规检测系统的鲁棒性和适应性，促进人脸识别系统在安全性、隐私和公平性方面的研究和应用。

Abstract: Ensuring compliance with ISO/IEC and ICAO standards for facial images in machine-readable travel documents (MRTDs) is essential for reliable identity verification, but current manual inspection methods are inefficient in high-demand environments. This paper introduces the DFIC dataset, a novel comprehensive facial image dataset comprising around 58,000 annotated images and 2706 videos of more than 1000 subjects, that cover a broad range of non-compliant conditions, in addition to compliant portraits. Our dataset provides a more balanced demographic distribution than the existing public datasets, with one partition that is nearly uniformly distributed, facilitating the development of automated ICAO compliance verification methods.
  Using DFIC, we fine-tuned a novel method that heavily relies on spatial attention mechanisms for the automatic validation of ICAO compliance requirements, and we have compared it with the state-of-the-art aimed at ICAO compliance verification, demonstrating improved results. DFIC dataset is now made public (https://github.com/visteam-isr-uc/DFIC) for the training and validation of new models, offering an unprecedented diversity of faces, that will improve both robustness and adaptability to the intrinsically diverse combinations of faces and props that can be presented to the validation system. These results emphasize the potential of DFIC to enhance automated ICAO compliance methods but it can also be used in many other applications that aim to improve the security, privacy, and fairness of facial recognition systems.

</details>


### [73] [Interpretable Vision Transformers in Image Classification via SVDA](https://arxiv.org/abs/2602.10994)
*Vasileios Arampatzakis,George Pavlidis,Nikolaos Mitianoudis,Nikos Papamarkos*

Main category: cs.CV

TL;DR: 本文将SVD启发式注意力机制（SVDA）引入ViT架构，在保持分类性能的基础上显著提升注意力的可解释性与结构性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉Transformer（ViT）虽然在图像分类任务上表现优异，但其注意力机制由于过于稠密和难以解释，限制了透明度与模型分析能力。作者希望增强ViT的注意力可解释性和结构性。

Method: 将之前提出的SVD-Inspired Attention（SVDA）方法与ViT架构结合，形成具几何基础的新注意力机制。同时，采用SVDA原有的可解释性指标来动态监控训练时注意力变化，并评估学习到的表征结构。

Result: 在CIFAR-10、FashionMNIST、CIFAR-100及ImageNet-100这四个主流图像分类基准上，SVDA注意力机制不仅显著提升ViT可解释性，还保证了与原始ViT相当的分类准确率。

Conclusion: SVDA为计算机视觉中的注意力机制提供了一种结构化、可解释的分析和开发工具，为未来可解释AI、谱分析和基于注意力的模型压缩等方向奠定了基础。

Abstract: Vision Transformers (ViTs) have achieved state-of-the-art performance in image classification, yet their attention mechanisms often remain opaque and exhibit dense, non-structured behaviors. In this work, we adapt our previously proposed SVD-Inspired Attention (SVDA) mechanism to the ViT architecture, introducing a geometrically grounded formulation that enhances interpretability, sparsity, and spectral structure. We apply the use of interpretability indicators -- originally proposed with SVDA -- to monitor attention dynamics during training and assess structural properties of the learned representations. Experimental evaluations on four widely used benchmarks -- CIFAR-10, FashionMNIST, CIFAR-100, and ImageNet-100 -- demonstrate that SVDA consistently yields more interpretable attention patterns without sacrificing classification accuracy. While the current framework offers descriptive insights rather than prescriptive guidance, our results establish SVDA as a comprehensive and informative tool for analyzing and developing structured attention models in computer vision. This work lays the foundation for future advances in explainable AI, spectral diagnostics, and attention-based model compression.

</details>


### [74] [Interpretable Vision Transformers in Monocular Depth Estimation via SVDA](https://arxiv.org/abs/2602.11005)
*Vasileios Arampatzakis,George Pavlidis,Nikolaos Mitianoudis,Nikos Papamarkos*

Main category: cs.CV

TL;DR: 提出了一种基于奇异值分解（SVD）思想的注意力机制（SVDA），用于单目深度估计任务，使Transformer模型的注意力机制更加可解释，并且在保持甚至提升预测精度的同时增加了多项可解释指标。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer模型的自注意力机制虽然取得了优异的表现，但其工作原理难以解释，尤其在密集预测任务（如单目深度估计）中缺乏透明性和可量化指标。提升注意力的可解释性有助于模型应用于需要高透明度的场景，如自动驾驶和机器人等。

Method: 提出SVDA注意力机制，将一个可学习的对角矩阵嵌入标准的Query-Key交互中，实现方向对齐与谱调制的解耦。通过该方法，注意力图从本质上就是可解释的，不再依赖后处理。实验在KITTI和NYU-v2数据集进行，并与标准Transformer进行对比。

Result: SVDA模型在两个主流数据集上能够保持或略微提升预测精度，计算开销很小。同时SVDA提出的6个谱指标（熵、秩、稀疏性、对齐性、选择性、稳健性）可有力揭示注意力分布的规律，这些规律在标准Transformer下难以获知。

Conclusion: SVDA不仅提升了单目深度估计Transformer模型的可解释性，并通过谱指标提供了量化分析手段，为透明的密集预测模型开辟了新途径。

Abstract: Monocular depth estimation is a central problem in computer vision with applications in robotics, AR, and autonomous driving, yet the self-attention mechanisms that drive modern Transformer architectures remain opaque. We introduce SVD-Inspired Attention (SVDA) into the Dense Prediction Transformer (DPT), providing the first spectrally structured formulation of attention for dense prediction tasks. SVDA decouples directional alignment from spectral modulation by embedding a learnable diagonal matrix into normalized query-key interactions, enabling attention maps that are intrinsically interpretable rather than post-hoc approximations. Experiments on KITTI and NYU-v2 show that SVDA preserves or slightly improves predictive accuracy while adding only minor computational overhead. More importantly, SVDA unlocks six spectral indicators that quantify entropy, rank, sparsity, alignment, selectivity, and robustness. These reveal consistent cross-dataset and depth-wise patterns in how attention organizes during training, insights that remain inaccessible in standard Transformers. By shifting the role of attention from opaque mechanism to quantifiable descriptor, SVDA redefines interpretability in monocular depth estimation and opens a principled avenue toward transparent dense prediction models.

</details>


### [75] [LaSSM: Efficient Semantic-Spatial Query Decoding via Local Aggregation and State Space Models for 3D Instance Segmentation](https://arxiv.org/abs/2602.11007)
*Lei Yao,Yi Wang,Yawen Cui,Moyun Liu,Lap-Pui Chau*

Main category: cs.CV

TL;DR: 本文提出了LaSSM方法，有效提升了基于点云查询的3D场景实例分割的效率与性能，大幅减少了计算量，同时在多个基准上实现了领先的分割效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于查询的点云3D场景实例分割方法存在查询初始化困境，且解码器依赖复杂的注意力机制，导致计算消耗大，效率低。因此亟需一种更高效、简单的解决方案。

Method: 作者提出LaSSM，包括两大创新点：（1）层次化语义-空间查询初始化器，结合语义信息和空间分布，从超点生成高效的查询集合；（2）坐标引导的状态空间模型（SSM）解码器，包括局部聚合机制（聚焦于几何关系接近的区域）和空间双路径SSM模块（整合坐标信息以捕获查询集合内的相关性）。

Result: LaSSM在最新的ScanNet++ V2榜单中取得第一名，比此前最佳方法高2.5% mAP，计算量仅为后者的三分之一。其它如ScanNet、ScanNet200、S3DIS和ScanNet++ V1数据集也取得了极具竞争力的成绩。

Conclusion: LaSSM能在大规模3D场景实例分割任务中以更低计算代价实现更高精度，方法简单高效，得到充分实验验证。

Abstract: Query-based 3D scene instance segmentation from point clouds has attained notable performance. However, existing methods suffer from the query initialization dilemma due to the sparse nature of point clouds and rely on computationally intensive attention mechanisms in query decoders. We accordingly introduce LaSSM, prioritizing simplicity and efficiency while maintaining competitive performance. Specifically, we propose a hierarchical semantic-spatial query initializer to derive the query set from superpoints by considering both semantic cues and spatial distribution, achieving comprehensive scene coverage and accelerated convergence. We further present a coordinate-guided state space model (SSM) decoder that progressively refines queries. The novel decoder features a local aggregation scheme that restricts the model to focus on geometrically coherent regions and a spatial dual-path SSM block to capture underlying dependencies within the query set by integrating associated coordinates information. Our design enables efficient instance prediction, avoiding the incorporation of noisy information and reducing redundant computation. LaSSM ranks first place on the latest ScanNet++ V2 leaderboard, outperforming the previous best method by 2.5% mAP with only 1/3 FLOPs, demonstrating its superiority in challenging large-scale scene instance segmentation. LaSSM also achieves competitive performance on ScanNet, ScanNet200, S3DIS and ScanNet++ V1 benchmarks with less computational cost. Extensive ablation studies and qualitative results validate the effectiveness of our design. The code and weights are available at https://github.com/RayYoh/LaSSM.

</details>


### [76] [Chain-of-Look Spatial Reasoning for Dense Surgical Instrument Counting](https://arxiv.org/abs/2602.11024)
*Rishikesh Bhyri,Brian R Quaranto,Philip J Seger,Kaity Tung,Brendan Fox,Gene Yang,Steven D. Schwaitzberg,Junsong Yuan,Nan Xi,Peter C W Kim*

Main category: cs.CV

TL;DR: 本文提出了一种模仿人类顺序计数过程的新视觉推理方法Chain-of-Look，有效提升了手术器械高密度场景下的计数准确性，并发布了相关高密度数据集。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉-语言模型和AI方法在手术器械密集堆放时计数准确性低，给手术安全带来隐患。作者希望通过模仿人类计数过程，解决计数难题。

Method: 引入Chain-of-Look视觉推理框架，强制模型按空间顺序链路计数，而非传统无序目标检测。提出neighboring loss损失函数，以建模密集空间下物体间物理约束，并公开高密度手术器械图像数据集SurgCount-HD。

Result: 大量实验证明，该方法在密集器械计数任务上，优于现有先进方法（比如CountGD、REC）和多模态大模型（如Qwen、ChatGPT）。

Conclusion: 通过顺序链式计数机制和空间约束损失，Chain-of-Look显著提升了复杂高密度场景下的手术器械计数性能，有助于提高手术室安全管理水平。

Abstract: Accurate counting of surgical instruments in Operating Rooms (OR) is a critical prerequisite for ensuring patient safety during surgery. Despite recent progress of large visual-language models and agentic AI, accurately counting such instruments remains highly challenging, particularly in dense scenarios where instruments are tightly clustered. To address this problem, we introduce Chain-of-Look, a novel visual reasoning framework that mimics the sequential human counting process by enforcing a structured visual chain, rather than relying on classic object detection which is unordered. This visual chain guides the model to count along a coherent spatial trajectory, improving accuracy in complex scenes. To further enforce the physical plausibility of the visual chain, we introduce the neighboring loss function, which explicitly models the spatial constraints inherent to densely packed surgical instruments. We also present SurgCount-HD, a new dataset comprising 1,464 high-density surgical instrument images. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches for counting (e.g., CountGD, REC) as well as Multimodality Large Language Models (e.g., Qwen, ChatGPT) in the challenging task of dense surgical instrument counting.

</details>


### [77] [PuriLight: A Lightweight Shuffle and Purification Framework for Monocular Depth Estimation](https://arxiv.org/abs/2602.11066)
*Yujie Chen,Li Zhang,Xiaomeng Chu,Tian Zhang*

Main category: cs.CV

TL;DR: PuriLight 是一个为单目深度估计设计的轻量级高效框架，既提升了计算效率，又保留了结构细节，在参数非常少的情况下取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督深度估计模型要么结构庞大，实际应用受限，要么为减小模型舍弃了结构精度。如何在轻量模型和结构保真之间取得平衡，是该领域的核心挑战。

Method: PuriLight 采用三阶段架构，包括三个创新模块：Shuffle-Dilation Convolution（局部特征提取）、Rotation-Adaptive Kernel Attention（层次特征增强）、Deep Frequency Signal Purification（全局特征净化），协同提取和处理深度特征。

Result: PuriLight 通过大量实验在保持极低参数量和高计算效率的前提下，达到了同类自监督单目深度估计方法中的最新性能水平。

Conclusion: PuriLight 框架有效克服了轻量模型与结构精度难以兼得的难题，为自监督单目深度估计提供了一种兼顾高效和精度的新方法。

Abstract: We propose PuriLight, a lightweight and efficient framework for self-supervised monocular depth estimation, to address the dual challenges of computational efficiency and detail preservation. While recent advances in self-supervised depth estimation have reduced reliance on ground truth supervision, existing approaches remain constrained by either bulky architectures compromising practicality or lightweight models sacrificing structural precision. These dual limitations underscore the critical need to develop lightweight yet structurally precise architectures. Our framework addresses these limitations through a three-stage architecture incorporating three novel modules: the Shuffle-Dilation Convolution (SDC) module for local feature extraction, the Rotation-Adaptive Kernel Attention (RAKA) module for hierarchical feature enhancement, and the Deep Frequency Signal Purification (DFSP) module for global feature purification. Through effective collaboration, these modules enable PuriLight to achieve both lightweight and accurate feature extraction and processing. Extensive experiments demonstrate that PuriLight achieves state-of-the-art performance with minimal training parameters while maintaining exceptional computational efficiency. Codes will be available at https://github.com/ishrouder/PuriLight.

</details>


### [78] [First International StepUP Competition for Biometric Footstep Recognition: Methods, Results and Remaining Challenges](https://arxiv.org/abs/2602.11086)
*Robyn Larracy,Eve MacDonald,Angkoon Phinyomark,Saeid Rezaei,Mahdi Laghaei,Ali Hajighasem,Aaron Tabor,Erik Scheme*

Main category: cs.CV

TL;DR: 该论文介绍了一项基于步态压力模式的生物识别竞赛，并分析了采用深度学习方法在新发布的大型高分辨率脚步压力数据集上的识别性能。


<details>
  <summary>Details</summary>
Motivation: 步态压力生物识别技术在安全领域有潜力，但缺乏大规模多样性数据集限制了其通用性和对复杂因素的适应能力。新数据集StepUP-P150的发布为突破这些挑战提供了条件。

Method: 举办了第一届国际StepUP足步识别竞赛，参赛队伍需在StepUP-P150数据集上开发鲁棒识别模型，并在专门设计的独立测试集上评估其在各种变化（如换鞋、步速变动）下的性能。

Result: 全球共有23支队伍参赛，最佳队伍采用生成性奖励机制优化方法（GRM），在等错误率（EER）上获得最好成绩10.77%。

Conclusion: 竞赛推动了该领域算法发展，但模型在面对不同鞋类等条件变化上的泛化能力仍存在明显挑战，是未来研究的重要方向。

Abstract: Biometric footstep recognition, based on a person's unique pressure patterns under their feet during walking, is an emerging field with growing applications in security and safety. However, progress in this area has been limited by the lack of large, diverse datasets necessary to address critical challenges such as generalization to new users and robustness to shifts in factors like footwear or walking speed. The recent release of the UNB StepUP-P150 dataset, the largest and most comprehensive collection of high-resolution footstep pressure recordings to date, opens new opportunities for addressing these challenges through deep learning. To mark this milestone, the First International StepUP Competition for Biometric Footstep Recognition was launched. Competitors were tasked with developing robust recognition models using the StepUP-P150 dataset that were then evaluated on a separate, dedicated test set designed to assess verification performance under challenging variations, given limited and relatively homogeneous reference data. The competition attracted global participation, with 23 registered teams from academia and industry. The top-performing team, Saeid_UCC, achieved the best equal error rate (EER) of 10.77% using a generative reward machine (GRM) optimization strategy. Overall, the competition showcased strong solutions, but persistent challenges in generalizing to unfamiliar footwear highlight a critical area for future work.

</details>


### [79] [FastFlow: Accelerating The Generative Flow Matching Models with Bandit Inference](https://arxiv.org/abs/2602.11105)
*Divya Jyoti Bajpai,Dhruv Bhardwaj,Soumya Roy,Tejas Duseja,Harsh Agarwal,Aashay Sandansing,Manjesh Kumar Hanawal*

Main category: cs.CV

TL;DR: 提出FastFlow方法加速flow-matching模型的生成过程，在显著提升推理速度的同时保持高质量输出，无需重新训练且可直接集成到现有管道中。


<details>
  <summary>Details</summary>
Motivation: flow-matching模型在图像和视频生成领域表现出色，但其依赖逐步去噪过程导致速度较慢。现有的加速方法需要重新训练，且泛化性弱。因此需要一种高效、便捷且自适应的推理加速方案。

Method: 作者提出FastFlow框架，在推理阶段通过识别变化较小的去噪步骤，以有限差分估算当前状态下的速度来近似未来状态，实现跳步计算。这一跳步决策被建模为多臂赌博机问题，可在速度与性能之间自适应平衡。该方法无需重新训练，作为插件与现有生成模型无缝集成。

Result: 实验表明，FastFlow可以在保证高输出质量的前提下实现2.6倍以上的生成速度提升，并适用于图像生成、视频生成及编辑等多种任务。

Conclusion: FastFlow是一种通用且高效的加速推理方案，能极大提升flow-matching模型在各类生成任务中的实际应用效率。

Abstract: Flow-matching models deliver state-of-the-art fidelity in image and video generation, but the inherent sequential denoising process renders them slower. Existing acceleration methods like distillation, trajectory truncation, and consistency approaches are static, require retraining, and often fail to generalize across tasks. We propose FastFlow, a plug-and-play adaptive inference framework that accelerates generation in flow matching models. FastFlow identifies denoising steps that produce only minor adjustments to the denoising path and approximates them without using the full neural network models used for velocity predictions. The approximation utilizes finite-difference velocity estimates from prior predictions to efficiently extrapolate future states, enabling faster advancements along the denoising path at zero compute cost. This enables skipping computation at intermediary steps. We model the decision of how many steps to safely skip before requiring a full model computation as a multi-armed bandit problem. The bandit learns the optimal skips to balance speed with performance. FastFlow integrates seamlessly with existing pipelines and generalizes across image generation, video generation, and editing tasks. Experiments demonstrate a speedup of over 2.6x while maintaining high-quality outputs. The source code for this work can be found at https://github.com/Div290/FastFlow.

</details>


### [80] [HairWeaver: Few-Shot Photorealistic Hair Motion Synthesis with Sim-to-Real Guided Video Diffusion](https://arxiv.org/abs/2602.11117)
*Di Chang,Ji Hou,Aljaz Bozic,Assaf Neuberger,Felix Juefei-Xu,Olivier Maury,Gene Wei-Chin Lin,Tuur Stuyck,Doug Roble,Mohammad Soleymani,Stephane Grabli*

Main category: cs.CV

TL;DR: HairWeaver是一种基于扩散模型的人像动画生成管线，能实现头发动态的高真实感动画。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽能控制身体姿态，但无法精细控制头发，导致动画中头发僵硬且不自然。头发的精细动态是实现高质量人像动画的关键。

Method: 提出了HairWeaver方法，包含两个专门模块：Motion-Context-LoRA用于融合运动条件，Sim2Real-Domain-LoRA用于在不同数据域中保持外观真实。这两个轻量化模块指导视频扩散模型主干，在CG模拟器生成的动态人像数据集上进行训练。

Result: HairWeaver能够对头发做出精细控制，实现对运动响应真实自然的头发动画效果，显著优于此前技术。

Conclusion: HairWeaver实现了高逼真、细腻的人头发动画，推动了人像动画生成的技术进步。

Abstract: We present HairWeaver, a diffusion-based pipeline that animates a single human image with realistic and expressive hair dynamics. While existing methods successfully control body pose, they lack specific control over hair, and as a result, fail to capture the intricate hair motions, resulting in stiff and unrealistic animations. HairWeaver overcomes this limitation using two specialized modules: a Motion-Context-LoRA to integrate motion conditions and a Sim2Real-Domain-LoRA to preserve the subject's photoreal appearance across different data domains. These lightweight components are designed to guide a video diffusion backbone while maintaining its core generative capabilities. By training on a specialized dataset of dynamic human motion generated from a CG simulator, HairWeaver affords fine control over hair motion and ultimately learns to produce highly realistic hair that responds naturally to movement. Comprehensive evaluations demonstrate that our approach sets a new state of the art, producing lifelike human hair animations with dynamic details.

</details>


### [81] [PhyCritic: Multimodal Critic Models for Physical AI](https://arxiv.org/abs/2602.11124)
*Tianyi Xiong,Shihao Wang,Guilin Liu,Yi Dong,Ming Li,Heng Huang,Jan Kautz,Zhiding Yu*

Main category: cs.CV

TL;DR: 本文提出了PhyCritic，一种专为物理智能（Physical AI）任务优化的多模态评价模型。在物理和通用多模态评测任务上表现优越，并推动了物理感知与推理能力的提升。


<details>
  <summary>Details</summary>
Motivation: 现有多模态评价模型大多针对一般视觉任务训练（如图像描述或问答），对涉及物理感知、因果推理和规划的物理智能场景支持有限。因此，亟需面向物理AI任务的可靠评价和批判模型。

Method: 提出了两阶段RLVR训练流程。第一阶段为物理技能热身，提升模型的物理感知与推理能力；第二阶段进行自我参照型评价微调，让模型在打分前自生成标准参考答案，从而提升评判稳定性和物理正确性。

Result: 在物理智能和通用多模态评价基准上，PhyCritic均优于现有开源模型作为评价器时；并且作为策略模型时，能进一步增强下游物理感知和推理任务表现。

Conclusion: PhyCritic有效填补了物理场景多模态评价模型的空白，在相关任务中表现突出，有助于推动面向物理AI的多模态大模型研究和实际应用。

Abstract: With the rapid development of large multimodal models, reliable judge and critic models have become essential for open-ended evaluation and preference alignment, providing pairwise preferences, numerical scores, and explanatory justifications for assessing model-generated responses. However, existing critics are primarily trained in general visual domains such as captioning or image question answering, leaving physical AI tasks involving perception, causal reasoning, and planning largely underexplored. We introduce PhyCritic, a multimodal critic model optimized for physical AI through a two-stage RLVR pipeline: a physical skill warmup stage that enhances physically oriented perception and reasoning, followed by self-referential critic finetuning, where the critic generates its own prediction as an internal reference before judging candidate responses, improving judgment stability and physical correctness. Across both physical and general-purpose multimodal judge benchmarks, PhyCritic achieves strong performance gains over open-source baselines and, when applied as a policy model, further improves perception and reasoning in physically grounded tasks.

</details>


### [82] [Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling](https://arxiv.org/abs/2602.11146)
*Gongye Liu,Bo Yang,Yida Zhi,Zhizhou Zhong,Lei Ke,Didan Deng,Han Gao,Yongxiang Huang,Kaihao Zhang,Hongbo Fu,Wenhan Luo*

Main category: cs.CV

TL;DR: 本文提出了一种新的扩散模型偏好优化方法DiNa-LRM，通过直接在噪声扩散状态上进行偏好建模，实现高效、鲁棒且计算成本低的奖励机制，性能媲美主流视觉-语言模型（VLM），但资源消耗更少。


<details>
  <summary>Details</summary>
Motivation: 现有的基于VLM的奖励函数虽能有效指导扩散模型的偏好优化，但计算与内存开销大，同时以像素空间奖励优化潜空间生成器存在领域不匹配，阻碍了高效对齐。

Method: 提出DiNa-LRM：直接在噪声扩散状态上进行偏好建模，设计了带有噪声依赖不确定性的Thurstone似然函数，采用预训练扩散潜空间主干和步长条件化奖励头，并支持推理时噪声集成优化。

Result: 在图像对齐基准上，DiNa-LRM显著超越现有扩散奖励基线，在极低资源消耗下与最先进的VLM性能相当。偏好优化任务中表现出更快、更高效的对齐。

Conclusion: DiNa-LRM实现了资源高效的扩散原生偏好奖励机制，并有效提升了扩散模型的偏好优化与对齐速度和质量，在实际系统中具有实用价值。

Abstract: Preference optimization for diffusion and flow-matching models relies on reward functions that are both discriminatively robust and computationally efficient. Vision-Language Models (VLMs) have emerged as the primary reward provider, leveraging their rich multimodal priors to guide alignment. However, their computation and memory cost can be substantial, and optimizing a latent diffusion generator through a pixel-space reward introduces a domain mismatch that complicates alignment. In this paper, we propose DiNa-LRM, a diffusion-native latent reward model that formulates preference learning directly on noisy diffusion states. Our method introduces a noise-calibrated Thurstone likelihood with diffusion-noise-dependent uncertainty. DiNa-LRM leverages a pretrained latent diffusion backbone with a timestep-conditioned reward head, and supports inference-time noise ensembling, providing a diffusion-native mechanism for test-time scaling and robust rewarding. Across image alignment benchmarks, DiNa-LRM substantially outperforms existing diffusion-based reward baselines and achieves performance competitive with state-of-the-art VLMs at a fraction of the computational cost. In preference optimization, we demonstrate that DiNa-LRM improves preference optimization dynamics, enabling faster and more resource-efficient model alignment.

</details>


### [83] [SurfPhase: 3D Interfacial Dynamics in Two-Phase Flows from Sparse Videos](https://arxiv.org/abs/2602.11154)
*Yue Gao,Hong-Xing Yu,Sanghyeon Chang,Qianxi Fu,Bo Zhu,Yoonjin Won,Juan Carlos Niebles,Jiajun Wu*

Main category: cs.CV

TL;DR: 本论文提出了一种名为SurfPhase的新方法，可以从有限的摄像机视角重建两相流体中界面的三维动态变化，突破了以往方法对复杂流体界面的测量与重建瓶颈。


<details>
  <summary>Details</summary>
Motivation: 两相流（如液-气界面）在动量、热量和质量传递等物理过程中非常关键，但实验测量难度大。传统测量方法在运动界面附近有本质限制，而当前神经重建方法主要关注单相流体，无法处理液-气间的清晰、变形界面。对于这类复杂动力学界面的高质量三维重建技术迫切需要。

Method: 提出SurfPhase模型，将动态高斯surfels（表面元素）与有符号距离函数（SDF）结合，实现几何一致性。再利用视频扩散模型，从稀疏视角下合成新视角视频，进一步提升重建质量。整体方法可从仅有的少量摄像头观察中恢复复杂流体界面的3D动态。

Result: 在新构建的高速沸腾流体视频数据集上进行评估，SurfPhase能从仅两台摄像机视角下实现高质量的视角合成和界面速度估计，显示出在三维重建和运动估计上的显著优势。

Conclusion: SurfPhase为两相流动界面三维动态的稀疏重建提供了突破性方法，在处理复杂、剧烈变化的流体界面方面优于传统方法，为相关流体力学与工程实验研究带来新工具。

Abstract: Interfacial dynamics in two-phase flows govern momentum, heat, and mass transfer, yet remain difficult to measure experimentally. Classical techniques face intrinsic limitations near moving interfaces, while existing neural rendering methods target single-phase flows with diffuse boundaries and cannot handle sharp, deformable liquid-vapor interfaces. We propose SurfPhase, a novel model for reconstructing 3D interfacial dynamics from sparse camera views. Our approach integrates dynamic Gaussian surfels with a signed distance function formulation for geometric consistency, and leverages a video diffusion model to synthesize novel-view videos to refine reconstruction from sparse observations. We evaluate on a new dataset of high-speed pool boiling videos, demonstrating high-quality view synthesis and velocity estimation from only two camera views. Project website: https://yuegao.me/SurfPhase.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [84] [Reviewing the Reviewer: Elevating Peer Review Quality through LLM-Guided Feedback](https://arxiv.org/abs/2602.10118)
*Sukannya Purkayastha,Qile Wan,Anne Lauscher,Lizhen Qu,Iryna Gurevych*

Main category: cs.CL

TL;DR: 该论文提出一个基于大语言模型(LLM)的工具来分析和改进学术评审中的“懒惰思考”问题，并发布了新数据集LazyReviewPlus。


<details>
  <summary>Details</summary>
Motivation: 虽然同行评审对科研质量至关重要，但评审人常常依赖简单的直觉判断（即“懒惰思考”），导致评审标准降低。当前相关研究仅将懒惰思考检测视为单标签分类任务，且缺乏细粒度、面向改进的指导性反馈。

Method: 提出LLM驱动的框架，首先将评审文本分解为论证片段，通过神经符号模块（融合LLM特征与传统分类器）检测多种问题，并借助针对性模板及遗传算法生成个性化反馈。

Result: 实验结果表明，该方法在检测和改进评审质量方面均显著优于零样本LLM基线，在提升评审质量上最高可提升92.4%。同时公开了LazyReviewPlus数据集，包含1,309条针对懒惰思考和细致度标注的句子。

Conclusion: 新方法不仅能够更精准地检测评审中的多种问题，还能生成有指导性的改进建议，有助于整体提升评审质量和科研评价过程的可靠性。

Abstract: Peer review is central to scientific quality, yet reliance on simple heuristics -- lazy thinking -- has lowered standards. Prior work treats lazy thinking detection as a single-label task, but review segments may exhibit multiple issues, including broader clarity problems, or specificity issues. Turning detection into actionable improvements requires guideline-aware feedback, which is currently missing. We introduce an LLM-driven framework that decomposes reviews into argumentative segments, identifies issues via a neurosymbolic module combining LLM features with traditional classifiers, and generates targeted feedback using issue-specific templates refined by a genetic algorithm. Experiments show our method outperforms zero-shot LLM baselines and improves review quality by up to 92.4\%. We also release LazyReviewPlus, a dataset of 1,309 sentences labeled for lazy thinking and specificity.

</details>


### [85] [Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens](https://arxiv.org/abs/2602.10229)
*Weihao Liu,Dehai Min,Lu Cheng*

Main category: cs.CL

TL;DR: 本文提出了一种新的大模型推理方法Latent Thoughts Tuning（LT-Tuning），通过改进潜在思维的构建和使用方式，有效提升模型的推理能力，并克服现有方法中的特征塌缩与不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 传统的显式链式思维（CoT）虽然提升了大模型的推理能力，但受限于离散词表，表达和思考受限。新兴的潜在空间推理虽解决部分问题，但存在特征塌缩和推理不稳定等不足，需要新的方法来提升潜在推理的稳定性与准确性。

Method: LT-Tuning方法通过引入Context-Prediction-Fusion机制，将模型的语境隐藏状态与词表嵌入的语义预测结合，用于生成更稳健的潜在思维表达；同时采用三阶段课程学习策略，使模型能够在潜在与显性思维模式间动态切换。

Result: 实验结果显示，LT-Tuning在推理任务上优于当前主流的潜在推理方法，明显缓解了特征塌缩和推理不稳定等问题，并提高了整体推理准确率。

Conclusion: LT-Tuning方法有效结构化了潜在思维表示，在提升模型推理能力的同时，增强了过程稳定性，为大模型推理提供了更优的解决方案。

Abstract: While explicit Chain-of-Thought (CoT) equips Large Language Models (LLMs) with strong reasoning capabilities, it requires models to verbalize every intermediate step in text tokens, constraining the model thoughts to the discrete vocabulary space. Recently, reasoning in continuous latent space has emerged as a promising alternative, enabling more robust inference and flexible computation beyond discrete token constraints. However, current latent paradigms often suffer from feature collapse and instability, stemming from distribution mismatches when recurrently using hidden states as the input embeddings, or alignment issues when relying on assistant models. To address this, we propose Latent Thoughts Tuning (LT-Tuning), a framework that redefines how latent thoughts are constructed and deployed. Instead of relying solely on raw hidden states, our method introduces a Context-Prediction-Fusion mechanism that jointly leveraging contextual hidden states and predictive semantic guidance from the vocabulary embedding space. Combined with a progressive three-stage curriculum learning pipeline, LT-Tuning also enables dynamically switching between latent and explicit thinking modes. Experiments demonstrate that our method outperforms existing latent reasoning baselines, effectively mitigating feature collapse and achieving robust reasoning accuracy.

</details>


### [86] [Learning to Evict from Key-Value Cache](https://arxiv.org/abs/2602.10238)
*Luca Moschella,Laura Manduchi,Ozan Sener*

Main category: cs.CL

TL;DR: 本论文提出利用强化学习（RL）方法优化大语言模型（LLM）推理时Key-Value缓存的淘汰策略，大幅提升了内存使用效率和推理表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的规模不断增长，推理时KV缓存消耗大量显存，现有的缓存淘汰方法多依赖于启发式规则（如最近性或历史注意力分数），但这些指标仅为未来效用的间接代理且计算开销较大，亟需更优的解决方案。

Method: 作者将KV缓存淘汰建模为一个强化学习排序问题，提出KV Policy（KVP）框架，用轻量级的每头RL代理在预先生成的推理轨迹上训练，仅利用key和value向量，学习以未来效用为目标的专业化淘汰策略，无需改动LLM模型或增加额外推理开销。

Result: 在RULER（长上下文）及OASST2-4k（多轮对话）等基准测试上，KVP效果显著优于现有方案。同时，在LongBench、BOOLQ、ARC等下游任务的zero-shot评测中，KVP表现出良好的泛化能力，并适用于更长的上下文。

Conclusion: 通过学习预测未来token的效用，KVP为LLM自适应KV缓存管理开辟了一条高效且可扩展的路径，在实际任务中表现突出，优于传统启发式方法。

Abstract: The growing size of Large Language Models (LLMs) makes efficient inference challenging, primarily due to the memory demands of the autoregressive Key-Value (KV) cache. Existing eviction or compression methods reduce cost but rely on heuristics, such as recency or past attention scores, which serve only as indirect proxies for a token's future utility and introduce computational overhead. We reframe KV cache eviction as a reinforcement learning (RL) problem: learning to rank tokens by their predicted usefulness for future decoding. To this end, we introduce KV Policy (KVP), a framework of lightweight per-head RL agents trained on pre-computed generation traces using only key and value vectors. Each agent learns a specialized eviction policy guided by future utility, which evaluates the quality of the ranking across all cache budgets, requiring no modifications to the underlying LLM or additional inference. Evaluated across two different model families on the long-context benchmark RULER and the multi-turn dialogue benchmark OASST2-4k, KVP significantly outperforms baselines. Furthermore, zero-shot tests on standard downstream tasks (e.g., LongBench, BOOLQ, ARC) indicate that KVP generalizes well beyond its training distribution and to longer context lengths. These results demonstrate that learning to predict future token utility is a powerful and scalable paradigm for adaptive KV cache management.

</details>


### [87] [On Emergent Social World Models -- Evidence for Functional Integration of Theory of Mind and Pragmatic Reasoning in Language Models](https://arxiv.org/abs/2602.10298)
*Polina Tsvilodub,Jan-Felix Klumpp,Amir Mohammadpour,Jennifer Hu,Michael Franke*

Main category: cs.CL

TL;DR: 本论文探讨了语言模型（LMs）是否通过共享的计算机制来处理通用“心智理论”（ToM）与语言相关的语用推理，并考察LMs是否出现了“社交世界模型”的迹象。研究使用行为评估和类脑功能定位等方法，分析了LMs在七类ToM能力上的表现。结果显示，LM的社交能力可能是互相关联的，而并非孤立存在。


<details>
  <summary>Details</summary>
Motivation: 动机在于理解高级LM是否具备人类般的社交认知，即能否形成可跨任务复用的心智状态表征，这关系到人工智能能否发展出类似人类的社会认知能力。

Method: 方法包括：基于认知神经科学的功能定位技术，对LMs在七个心智理论子能力上的表现进行行为和因果机制层面测试，并使用更大规模的本地化数据集进行分析与统计检验。

Result: 结果通过严格的统计测试，提供了支持功能整合假设的证据，表明LMs可能发展出了互联的“社交世界模型”，而不是彼此孤立的推理能力。

Conclusion: 结论认为，LMs具备形成和复用心智状态表征的潜力，推动了对人工系统社会认知能力产生机制的理解。此外，论文贡献了新的ToM本地化数据和功能定位方法上的改进。

Abstract: This paper investigates whether LMs recruit shared computational mechanisms for general Theory of Mind (ToM) and language-specific pragmatic reasoning in order to contribute to the general question of whether LMs may be said to have emergent "social world models", i.e., representations of mental states that are repurposed across tasks (the functional integration hypothesis). Using behavioral evaluations and causal-mechanistic experiments via functional localization methods inspired by cognitive neuroscience, we analyze LMs' performance across seven subcategories of ToM abilities (Beaudoin et al., 2020) on a substantially larger localizer dataset than used in prior like-minded work. Results from stringent hypothesis-driven statistical testing offer suggestive evidence for the functional integration hypothesis, indicating that LMs may develop interconnected "social world models" rather than isolated competencies. This work contributes novel ToM localizer data, methodological refinements to functional localization techniques, and empirical insights into the emergence of social cognition in artificial systems.

</details>


### [88] [Are More Tokens Rational? Inference-Time Scaling in Language Models as Adaptive Resource Rationality](https://arxiv.org/abs/2602.10329)
*Zhimin Hu,Riya Roshan,Sashank Varma*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLM）在推理复杂性提升时，其推理策略是否会自发表现为资源理性。他们发现，随着任务变复杂，推理策略从简单暴力解法转向更有效的分析型策略，且无需显式的计算成本奖励。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型在推理任务中，随着计算资源扩展，其推理策略是否会自然体现资源理性，而不是仅靠训练时的奖励去约束。

Method: 提出了一个变量归属任务，让模型判断哪些变量决定了结果。通过调节候选变量和样本数量，控制任务复杂度，并分析两类模型（instruction-tuned模型和reinforcement-learning训练的LRMs）在不同复杂度下的表现与策略转变。

Result: 两类模型在任务复杂度提高时都出现了从暴力策略向分析型策略的转变。IT模型在解决XOR和XNOR逻辑函数时性能明显下降，而LRMs则表现出较强的鲁棒性。

Conclusion: 无需显式地惩罚计算资源消耗，仅通过推理时扩展计算量，大模型便能自发展现出资源理性的推理行为。这表明资源理性是推理时尺度扩展的自然结果。

Abstract: Human reasoning is shaped by resource rationality -- optimizing performance under constraints. Recently, inference-time scaling has emerged as a powerful paradigm to improve the reasoning performance of Large Language Models by expanding test-time computation. Specifically, instruction-tuned (IT) models explicitly generate long reasoning steps during inference, whereas Large Reasoning Models (LRMs) are trained by reinforcement learning to discover reasoning paths that maximize accuracy. However, it remains unclear whether resource-rationality can emerge from such scaling without explicit reward related to computational costs. We introduce a Variable Attribution Task in which models infer which variables determine outcomes given candidate variables, input-output trials, and predefined logical functions. By varying the number of candidate variables and trials, we systematically manipulate task complexity. Both models exhibit a transition from brute-force to analytic strategies as complexity increases. IT models degrade on XOR and XNOR functions, whereas LRMs remain robust. These findings suggest that models can adjust their reasoning behavior in response to task complexity, even without explicit cost-based reward. It provides compelling evidence that resource rationality is an emergent property of inference-time scaling itself.

</details>


### [89] [The Subjectivity of Respect in Police Traffic Stops: Modeling Community Perspectives in Body-Worn Camera Footage](https://arxiv.org/abs/2602.10339)
*Preni Golazizian,Elnaz Rahmati,Jackson Trager,Zhivar Sourati,Nona Ghazizadeh,Georgios Chochlakis,Jose Alcocer,Kerby Bennett,Aarya Vijay Devnani,Parsa Hejabi,Harry G. Muttram,Akshay Kiran Padte,Mehrshad Saadatinia,Chenhao Wu,Alireza S. Zaibari,Michael Sierra-Arévalo,Nick Weller,Shrikanth Narayanan,Benjamin A. T. Graham,Morteza Dehghani*

Main category: cs.CL

TL;DR: 本论文基于洛杉矶警察局执法记录仪视频，首创大规模交通拦截数据集，并引入了多群体视角下对“尊重”的评级及解释。提出了透视感知建模框架，可预测和解释不同社区成员眼中的互动尊重度，增强警民信任和程序合法性。


<details>
  <summary>Details</summary>
Motivation: 执法过程中“尊重”的体现对社会信任和警察合法性极为重要，但其主观性强、随社区文化差异变化。当前研究未能细致刻画各群体视角下的尊重体验，本论文致力于弥补这一空白，帮助警务工作更好理解和回应多元社区的公众期待。

Method: 作者利用洛杉矶警察局的大量执法记录仪视频，首次邀请警察、受司法系统影响者、一般居民三类人群对交通拦截片段分别按“尊重”评级并给出理由。构建了基于程序正义理论和LAPD培训的评价量表，设计了面向群体视角一致性的评级数据生成和建模流程，并开发能根据文本自动预测不同群体尊重评级与理由的人工智能模型。

Result: 新提出的数据集和建模框架显著提升了对多群体视角的尊重评级预测准确性，也能更好生成个性化的理由解释，三大群体中均体现出效果提升。

Conclusion: 该研究为执法部门理解多元社区对尊重的期待和差异提供了先进工具，对于增进公众信任、提升执法程序合法性具有重要意义。

Abstract: Traffic stops are among the most frequent police-civilian interactions, and body-worn cameras (BWCs) provide a unique record of how these encounters unfold. Respect is a central dimension of these interactions, shaping public trust and perceived legitimacy, yet its interpretation is inherently subjective and shaped by lived experience, rendering community-specific perspectives a critical consideration. Leveraging unprecedented access to Los Angeles Police Department BWC footage, we introduce the first large-scale traffic-stop dataset annotated with respect ratings and free-text rationales from multiple perspectives. By sampling annotators from police-affiliated, justice-system-impacted, and non-affiliated Los Angeles residents, we enable the systematic study of perceptual differences across diverse communities. To this end, we (i) develop a domain-specific evaluation rubric grounded in procedural justice theory, LAPD training materials, and extensive fieldwork; (ii) introduce a rubric-driven preference data construction framework for perspective-consistent alignment; and (iii) propose a perspective-aware modeling framework that predicts personalized respect ratings and generates annotator-specific rationales for both officers and civilian drivers from traffic-stop transcripts. Across all three annotator groups, our approach improves both rating prediction performance and rationale alignment. Our perspective-aware framework enables law enforcement to better understand diverse community expectations, providing a vital tool for building public trust and procedural legitimacy.

</details>


### [90] [Geometry-Aware Decoding with Wasserstein-Regularized Truncation and Mass Penalties for Large Language Models](https://arxiv.org/abs/2602.10346)
*Arash Gholami Davoodi,Navid Rezazadeh,Seyed Pouyan Mousavi Davoudi,Pouya Pezeshkpour*

Main category: cs.CL

TL;DR: 本文提出了一种基于Wasserstein距离和几何结构的新型截断采样方法Top-W，用于提升大语言模型开放式生成的表现，在多个基准实验中超越了现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型需要兼顾生成内容的多样性与创造性，以及生成逻辑性和连贯性。现有的截断采样方法（如Top-k、Top-p）主要依赖概率总量和熵阈值，忽略了token语义嵌入空间的几何结构，于是难以在多样性和逻辑性之间取得最佳平衡。

Method: 作者提出基于Wasserstein距离的截断策略Top-W，通过度量裁剪后分布与原分布在token嵌入空间中的接近程度来定义采样集合，并在保留概率质量与集合熵之间显式权衡。该方法得到一个简单闭式的截断更新，并且可高效实现。Top-W使用基于几何的potentials（如最近邻或k-NN），并与常规截断采样接口兼容。

Result: 在GSM8K、GPQA、AlpacaEval和MT-Bench四个基准，以及三个主流指令微调模型上，Top-W在准确性和创造性方面均显著超越以往最优采样策略，部分任务上提升幅度达33.7%。

Conclusion: Top-W能更好地平衡概率质量和输出多样性，提升模型输出的准确性与创造性，是对现有截断采样方法的有力改进。

Abstract: Large language models (LLMs) must balance diversity and creativity against logical coherence in open-ended generation. Existing truncation-based samplers are effective but largely heuristic, relying mainly on probability mass and entropy while ignoring semantic geometry of the token space. We present Top-W, a geometry-aware truncation rule that uses Wasserstein distance-defined over token-embedding geometry-to keep the cropped distribution close to the original, while explicitly balancing retained probability mass against the entropy of the kept set. Our theory yields a simple closed-form structure for the fixed-potential subset update: depending on the mass-entropy trade-off, the optimal crop either collapses to a single token or takes the form of a one-dimensional prefix that can be found efficiently with a linear scan. We implement Top-W using efficient geometry-based potentials (nearest-set or k-NN) and pair it with an alternating decoding routine that keeps the standard truncation-and-sampling interface unchanged. Extensive experiments on four benchmarks (GSM8K, GPQA, AlpacaEval, and MT-Bench) across three instruction-tuned models show that Top-W consistently outperforms prior state-of-the-art decoding approaches achieving up to 33.7% improvement. Moreover, we find that Top-W not only improves accuracy-focused performance, but also boosts creativity under judge-based open-ended evaluation.

</details>


### [91] [When Less Is More? Diagnosing ASR Predictions in Sardinian via Layer-Wise Decoding](https://arxiv.org/abs/2602.10350)
*Domenico De Cristofaro,Alessandro Vietti,Marianne Pouplier,Aleese Block*

Main category: cs.CL

TL;DR: 该论文发现多语种语音模型的中间层对音素的表示比最终层更准确。通过在低资源语言（坎皮达诺萨丁语）上实验，截断顶层后音素错误率（PER）反而降低，最佳表现出现在倒数第二层。中间层更好保留了音段信息，减少特定语音错误，还首次提出了“回退性错误”的概念。结果表明应利用早期层分析来辅助ASR模型诊断与优化，尤其适用于低资源语言。


<details>
  <summary>Details</summary>
Motivation: 以往研究主要关注模型输出层，但在低资源语言环境下，传统评估标准难以准确衡量模型的语言学表现。因此，需要探索模型内部的表征演变，寻找能更好反映真实语音识别效果的层次。

Method: 采用Wav2Vec2预训练多语种模型，在坎皮达诺萨丁语数据集上，使用逐层解码的方式考察编码器各层的音素预测表现，并进行细致的一致性分析。通过截断上层transformer模块评估各层PER变化，引入“回退性错误”作为补充评价。

Result: 截断顶层transformer后PER下降，最佳PER出现在倒数第二层。这些中间层能更好地保留音段特征，减少过度生成和部分音系错误。提出“回退性错误”现象发现，最终层反而会覆盖中间层的准确预测。

Conclusion: 多语种ASR模型中，早中期层对于音素表征更精准，适当利用这些层的信息有助于提升低资源语言的语音识别效果。此外，常规表层指标存在局限，引入新的诊断和评估手段（如中间层分析与“回退性错误”）对于真实性能监控至关重要。

Abstract: Recent studies have shown that intermediate layers in multilingual speech models often encode more phonetically accurate representations than the final output layer. In this work, we apply a layer-wise decoding strategy to a pretrained Wav2Vec2 model to investigate how phoneme-level predictions evolve across encoder layers, focusing on Campidanese Sardinian, a low-resource language. We show that truncating upper transformer layers leads to improved Phoneme Error Rates (PER), with the best performance achieved not at the final layer, but two layers earlier. Through fine-grained alignment analysis, we find that intermediate predictions better preserve segmental identity, avoid overgeneration, and reduce certain classes of phonological errors. We also introduce the notion of regressive errors, cases where correct predictions at intermediate layers are overwritten by errors at the final layer. These regressions highlight the limitations of surface-level error metrics and reveal how deeper layers may generalize or abstract away from acoustic detail. Our findings support the use of early-layer probing as a diagnostic tool for ASR models, particularly in low-resource settings where standard evaluation metrics may fail to capture linguistically meaningful behavior.

</details>


### [92] [Learning Self-Interpretation from Interpretability Artifacts: Training Lightweight Adapters on Vector-Label Pairs](https://arxiv.org/abs/2602.10352)
*Keenan Pepper,Alex McKenzie,Florin Pop,Stijn Servaes,Martin Leitgab,Mike Vaiana,Judd Rosenblatt,Michael S. A. Graziano,Diogo de Lucena*

Main category: cs.CL

TL;DR: 本论文提出在保持大型语言模型（LM）完全冻结的情况下，通过训练轻量级适配器以提升模型自我解释能力，并在多个任务和模型家族中实现了更加可靠的自我解释。


<details>
  <summary>Details</summary>
Motivation: 自我解释方法通常高度依赖超参数，稳定性较差，限制了其实际应用。本研究旨在探索无需修改预训练模型主体即可有效提升语言模型自我解释性能的方法。

Method: 作者提出仅在解释性人工数据上训练轻量级标量仿射适配器（参数量极低d_model+1），且主语言模型参数保持冻结。结合自编码器稀疏特征标注、主题识别等任务评估方法有效性。

Result: 在70B规模模型上，训练后适配器生成的特征标签生成得分从63%提升到71%，主题识别Recall@1从1%提升到94%。适配器还能识别出多跳推理中的桥接实体，且仅学习到的偏置向量即可占85%的改进效果。简化适配器在泛化性上优于复杂结构。自我解释能力随模型规模提升优于常规能力提升。

Conclusion: 轻量级适配器能够在不改动主模型的前提下，大幅提升大型语言模型的自我解释性能，且简化结构效果更佳。这为大型模型解释性提升提供了实用且具备泛化能力的新路径。

Abstract: Self-interpretation methods prompt language models to describe their own internal states, but remain unreliable due to hyperparameter sensitivity. We show that training lightweight adapters on interpretability artifacts, while keeping the LM entirely frozen, yields reliable self-interpretation across tasks and model families. A scalar affine adapter with just $d_\text{model}+1$ parameters suffices: trained adapters generate sparse autoencoder feature labels that outperform the training labels themselves (71% vs 63% generation scoring at 70B scale), identify topics with 94% recall@1 versus 1% for untrained baselines, and decode bridge entities in multi-hop reasoning that appear in neither prompt nor response, surfacing implicit reasoning without chain-of-thought. The learned bias vector alone accounts for 85% of improvement, and simpler adapters generalize better than more expressive alternatives. Controlling for model knowledge via prompted descriptions, we find self-interpretation gains outpace capability gains from 7B to 72B parameters. Our results demonstrate that self-interpretation improves with scale, without modifying the model being interpreted.

</details>


### [93] [Physically Interpretable AlphaEarth Foundation Model Embeddings Enable LLM-Based Land Surface Intelligence](https://arxiv.org/abs/2602.10354)
*Mashrekur Rahman*

Main category: cs.CL

TL;DR: 本论文评估了Google AlphaEarth卫星基础模型生成的64维嵌入向量在物理环境解读方面的能力，证实其可用于高保真重建环境变量，并开发了基于该嵌入的自然语言环境智能系统。


<details>
  <summary>Details</summary>
Motivation: 尽管卫星基础模型的表征能力强大，但其嵌入向量的物理可解释性不足，阻碍了其在环境决策中的广泛应用。因此需要系统评估其嵌入的可解释性，并探索其实际应用潜力。

Method: 收集了覆盖美国本土的1210万样本，将AlphaEarth的64维嵌入与26种环境变量（气候、植被、水文、温度、地形等）进行线性、非线性和注意力法的关联分析，评估单一维度与整体嵌入的映射关系，并进行空间、时间稳健性验证。基于解释结果，构建了一个利用FAISS索引数据库的Land Surface Intelligence系统，实现自然语言到卫星环境评估的检索增强生成，并用多轮LLM评测系统评价。

Result: 发现部分嵌入维度可与具体地表属性映射，多数环境变量可被高精度重建（12/26个变量R^2>0.90，温度和高程R^2≈0.97）。这些映射关系在不同分析方法、空间分割和跨年度中均稳健（空间ΔR^2=0.017，年度相关r=0.963）。环境智能系统在360个任务中取得了扎实的评价结果，尤其是在信息溯源和内容连贯上表现优异。

Conclusion: AlphaEarth卫星基础模型嵌入具备清晰、有用的物理结构并支持高效的环境语义检索，能为地理环境智能应用提供坚实基础，实现从嵌入到实用系统的桥接。

Abstract: Satellite foundation models produce dense embeddings whose physical interpretability remains poorly understood, limiting their integration into environmental decision systems. Using 12.1 million samples across the Continental United States (2017--2023), we first present a comprehensive interpretability analysis of Google AlphaEarth's 64-dimensional embeddings against 26 environmental variables spanning climate, vegetation, hydrology, temperature, and terrain. Combining linear, nonlinear, and attention-based methods, we show that individual embedding dimensions map onto specific land surface properties, while the full embedding space reconstructs most environmental variables with high fidelity (12 of 26 variables exceed $R^2 > 0.90$; temperature and elevation approach $R^2 = 0.97$). The strongest dimension-variable relationships converge across all three analytical methods and remain robust under spatial block cross-validation (mean $ΔR^2 = 0.017$) and temporally stable across all seven study years (mean inter-year correlation $r = 0.963$). Building on these validated interpretations, we then developed a Land Surface Intelligence system that implements retrieval-augmented generation over a FAISS-indexed embedding database of 12.1 million vectors, translating natural language environmental queries into satellite-grounded assessments. An LLM-as-Judge evaluation across 360 query--response cycles, using four LLMs in rotating generator, system, and judge roles, achieved weighted scores of $μ= 3.74 \pm 0.77$ (scale 1--5), with grounding ($μ= 3.93$) and coherence ($μ= 4.25$) as the strongest criteria. Our results demonstrate that satellite foundation model embeddings are physically structured representations that can be operationalized for environmental and geospatial intelligence.

</details>


### [94] [Autonomous Continual Learning of Computer-Use Agents for Environment Adaptation](https://arxiv.org/abs/2602.10356)
*Tianci Xue,Zeyi Liao,Tianneng Shi,Zilu Wang,Kai Zhang,Dawn Song,Yu Su,Huan Sun*

Main category: cs.CL

TL;DR: 提出ACuRL无监督课程强化学习框架，实现计算机智能体在多变环境中的持续学习和自适应，无需人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 现实数字环境多样且变化快速，计算机智能体常面临未见过场景和数据分布变化，因此需要能持续适应具体环境的方法。但现有方法过度依赖人工标注数据，成本高且难以扩展。

Method: 智能体先在目标环境中自主探索获取初始经验，然后通过课程任务生成器，结合历史经验和上次结果生成适合智能体当前水平的新任务迭代训练。为实现无人工奖励信号，设计了自动评估器CUAJudge，能取得93%与人类判断一致率。

Result: 方法实现了环境内与环境间的持续学习，在现有环境中未出现灾难性遗忘，带来了4-22%的性能提升。参数更新高度稀疏（约20%参数），体现有效且稳健的自适应。

Conclusion: ACuRL框架可在无人工数据的前提下，有效推动计算机智能体在多样实用环境中持续学习和自适应。相关数据和代码已开源。

Abstract: Real-world digital environments are highly diverse and dynamic. These characteristics cause agents to frequently encounter unseen scenarios and distribution shifts, making continual learning in specific environments essential for computer-use agents (CUAs). However, a key challenge lies in obtaining high-quality and environment-grounded agent data without relying on costly human annotation. In this work, we introduce ACuRL, an Autonomous Curriculum Reinforcement Learning framework that continually adapts agents to specific environments with zero human data. The agent first explores target environments to acquire initial experiences. During subsequent iterative training, a curriculum task generator leverages these experiences together with feedback from the previous iteration to synthesize new tasks tailored for the agent's current capabilities. To provide reliable reward signals, we introduce CUAJudge, a robust automatic evaluator for CUAs that achieves 93% agreement with human judgments. Empirically, our method effectively enables both intra-environment and cross-environment continual learning, yielding 4-22% performance gains without catastrophic forgetting on existing environments. Further analyses show highly sparse updates (e.g., 20% parameters), which helps explain the effective and robust adaptation. Our data and code are available at https://github.com/OSU-NLP-Group/ACuRL.

</details>


### [95] [The Alignment Bottleneck in Decomposition-Based Claim Verification](https://arxiv.org/abs/2602.10380)
*Mahmud Elahi Akhter,Federico Ruggeri,Iman Munire Bilal,Rob Procter,Maria Liakata*

Main category: cs.CL

TL;DR: 结构化主张分解在复杂主张验证中效果并不一致，作者发现其瓶颈在于证据对齐和子主张错误分布，并提出新数据集探讨两种证据对齐方式下的分解效果。只有在证据严格对齐时分解才有效，否则甚至会降低性能。文中还发现保守的预测策略有助于减少错误传播。


<details>
  <summary>Details</summary>
Motivation: 结构化分解被认为能提升复杂、多层次主张的验证效果，但实验结果并不一致。作者认为之前研究忽视了证据对齐和子主张验证中的错误传播两个关键问题，急需更深入的研究和数据支持。

Method: 作者构建了包含真实复杂主张的新数据集，特征包括有时间范围的证据和人工标注的子主张证据片段。实验中对比了两种证据对齐设定：子主张对齐证据（SAE）和重复使用主张级别证据（SRE），并分析了子主张标签噪声对下游系统鲁棒性的影响。

Result: 结果显示，仅有证据细致对齐（SAE）时分解才带来显著性能提升；常见的SRE设定无效且经常降低性能，并且这一现象在多个领域数据集均出现。对于子主张标签噪声，“弃权”式保守策略相比激进但错误的预测更能减少错误传播。

Conclusion: 未来的主张分解框架应优先保证证据的精细合成和对齐，并校准子主张验证模型的标签偏置，否则分解带来的益处有限甚至适得其反。

Abstract: Structured claim decomposition is often proposed as a solution for verifying complex, multi-faceted claims, yet empirical results have been inconsistent. We argue that these inconsistencies stem from two overlooked bottlenecks: evidence alignment and sub-claim error profiles. To better understand these factors, we introduce a new dataset of real-world complex claims, featuring temporally bounded evidence and human-annotated sub-claim evidence spans. We evaluate decomposition under two evidence alignment setups: Sub-claim Aligned Evidence (SAE) and Repeated Claim-level Evidence (SRE). Our results reveal that decomposition brings significant performance improvement only when evidence is granular and strictly aligned. By contrast, standard setups that rely on repeated claim-level evidence (SRE) fail to improve and often degrade performance as shown across different datasets and domains (PHEMEPlus, MMM-Fact, COVID-Fact). Furthermore, we demonstrate that in the presence of noisy sub-claim labels, the nature of the error ends up determining downstream robustness. We find that conservative "abstention" significantly reduces error propagation compared to aggressive but incorrect predictions. These findings suggest that future claim decomposition frameworks must prioritize precise evidence synthesis and calibrate the label bias of sub-claim verification models.

</details>


### [96] [Triggers Hijack Language Circuits: A Mechanistic Analysis of Backdoor Behaviors in Large Language Models](https://arxiv.org/abs/2602.10382)
*Théo Lasnier,Wissam Antoun,Francis Kulumba,Djamé Seddah*

Main category: cs.CL

TL;DR: 本文首次对大模型中的“语言切换后门”进行机制性分析，发现后门触发器并非独立存在，而是与模型本身的语言处理组件有高度重合。


<details>
  <summary>Details</summary>
Motivation: 尽管后门攻击对大语言模型安全构成威胁，但后门触发机制尚未被深入理解，尤其是触发器如何影响模型内部结构。

Method: 采用activation patching方法，分析了GAPperon模型（1B、8B、24B参数）预训练期间注入的语言切换触发器，定位了触发器主要在早期层（模型深度7.5-25%）形成，并鉴定出处理触发器信息的注意力头。

Result: 触发器激活的注意力头与自然编码输出语言的注意力头有显著重叠（Jaccard指数0.18-0.66），表明后门不是独立电路，而是“挪用”了原有语言组件。

Conclusion: 针对后门检测应关注功能已知的组件，防御措施可考虑利用后门行为与自然行为的耦合关系。

Abstract: Backdoor attacks pose significant security risks for Large Language Models (LLMs), yet the internal mechanisms by which triggers operate remain poorly understood. We present the first mechanistic analysis of language-switching backdoors, studying the GAPperon model family (1B, 8B, 24B parameters) which contains triggers injected during pretraining that cause output language switching. Using activation patching, we localize trigger formation to early layers (7.5-25% of model depth) and identify which attention heads process trigger information. Our central finding is that trigger-activated heads substantially overlap with heads naturally encoding output language across model scales, with Jaccard indices between 0.18 and 0.66 over the top heads identified. This suggests that backdoor triggers do not form isolated circuits but instead co-opt the model's existing language components. These findings have implications for backdoor defense: detection methods may benefit from monitoring known functional components rather than searching for hidden circuits, and mitigation strategies could potentially leverage this entanglement between injected and natural behaviors.

</details>


### [97] [When Tables Go Crazy: Evaluating Multimodal Models on French Financial Documents](https://arxiv.org/abs/2602.10384)
*Virginie Mouilleron,Théo Lasnier,Djamé Seddah*

Main category: cs.CL

TL;DR: 该论文推出了第一个用于评估法语金融文件理解的多模态基准数据集Multimodal Finance Eval，并基于六种开源VLMs进行了实验，发现当前模型在表格和文本抽取任务上表现良好，但在图表解释和多轮对话推理上仍有显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型（VLMs）在多个文档理解任务上表现出色，但在非英文且高度专业化领域（如金融）中的表现和可靠性尚未被深入研究。金融领域的文档通常包含复杂的法规文本、数值表格和视觉图表，信息提取错误可能带来严重后果，因此需要针对相关模型的能力进行系统性评估。

Method: 作者构建了全球首个针对法语金融文档理解的多模态基准Multimodal Finance Eval，涵盖文本抽取、表格理解、图表解释及多轮对话推理四类任务，数据源包括真实的投资说明书、KID和PRIIP等，并由专家标注共1,204个问题。使用六个开源视觉-语言模型（参数规模8B-124B）进行评测，采用大模型仲裁（LLM-as-judge）的方法进行打分。

Result: VLMs在文本和表格任务上的准确率高达85-90%；然而，在图表解释任务上，仅有34-62%。在多轮对话任务中，无论模型规模如何，只要前期出现错误，错误会层层传递，导致最终准确率降至约50%。

Conclusion: 尽管现有VLMs擅长处理结构化抽取类任务，但在需要更复杂交互和多步推理的金融分析任务中仍表现脆弱。Multimodal Finance Eval为推动该领域发展提供了具有挑战性的基准，有助于衡量并提升模型在高风险金融场景下的能力。

Abstract: Vision-language models (VLMs) perform well on many document understanding tasks, yet their reliability in specialized, non-English domains remains underexplored. This gap is especially critical in finance, where documents mix dense regulatory text, numerical tables, and visual charts, and where extraction errors can have real-world consequences. We introduce Multimodal Finance Eval, the first multimodal benchmark for evaluating French financial document understanding. The dataset contains 1,204 expert-validated questions spanning text extraction, table comprehension, chart interpretation, and multi-turn conversational reasoning, drawn from real investment prospectuses, KIDs, and PRIIPs. We evaluate six open-weight VLMs (8B-124B parameters) using an LLM-as-judge protocol. While models achieve strong performance on text and table tasks (85-90% accuracy), they struggle with chart interpretation (34-62%). Most notably, multi-turn dialogue reveals a sharp failure mode: early mistakes propagate across turns, driving accuracy down to roughly 50% regardless of model size.
  These results show that current VLMs are effective for well-defined extraction tasks but remain brittle in interactive, multi-step financial analysis. Multimodal Finance Eval offers a challenging benchmark to measure and drive progress in this high-stakes setting.

</details>


### [98] [Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs](https://arxiv.org/abs/2602.10388)
*Zhongzhi Li,Xuansheng Wu,Yijiang Li,Lijie Hu,Ninghao Liu*

Main category: cs.CL

TL;DR: 本文提出了一种通过特征激活覆盖（Feature Activation Coverage, FAC）来衡量大模型后训练数据多样性的指标，并基于此开发了多样性驱动的数据合成框架FAC Synthesis，从而有效提升大语言模型的下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前用于增强大语言模型下游能力的数据集，大多只关注了文本的表层多样性（如词汇、句法等），但这些只能弱关联模型真正需要优化的任务相关特征。缺乏针对‘有效特征多样性’的系统性度量与优化手段，因此有必要提出新的多样性衡量方式，更贴近模型实际学习需求。

Method: 作者提出了特征激活覆盖（FAC）来测量种子数据在大模型内部表征空间中的多样性，具体通过稀疏自编码器识别出缺失的特征维度，然后合成具有目标特征的新样本，形成FAC Synthesis流程。同时探索了不同主流模型共享的特征空间，实现跨模型的数据迁移。

Result: 实验显示，该方法在包含指令跟随、有害内容识别、奖励建模和行为引导等多项任务上显著提升了数据的多样性和下游性能，并证实了特征空间在LLaMA、Mistral、Qwen等模型间具有一定的通用性。

Conclusion: 本文提出的FAC及FAC Synthesis为以特征为中心的数据优化提供了有效方法，从数据多样性角度推动了LLM性能提升，并为跨模型数据迁移、数据驱动模型优化等方向打开了新的思路。

Abstract: The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance. In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Building upon this metric, we further propose a diversity-driven data synthesis framework, named FAC Synthesis, that first uses a sparse autoencoder to identify missing features from a seed dataset, and then generates synthetic samples that explicitly reflect these features. Experiments show that our approach consistently improves both data diversity and downstream performance on various tasks, including instruction following, toxicity detection, reward modeling, and behavior steering. Interestingly, we identify a shared, interpretable feature space across model families (i.e., LLaMA, Mistral, and Qwen), enabling cross-model knowledge transfer. Our work provides a solid and practical methodology for exploring data-centric optimization of LLMs.

</details>


### [99] [When are We Worried? Temporal Trends of Anxiety and What They Reveal about Us](https://arxiv.org/abs/2602.10400)
*Saif M. Mohammad*

Main category: cs.CL

TL;DR: 本论文利用词汇-焦虑关联词典，分析美加地区推特数据，从时段和语法视角揭示焦虑在社交媒体中的表现规律。


<details>
  <summary>Details</summary>
Motivation: 了解人们在社交媒体上何时以及如何表达焦虑，以获得有关焦虑情绪的更多信息和具体表现规律。

Method: 采用新建立的焦虑词汇词典，对大量美国和加拿大的推文进行文本分析，统计不同时间、不同语法结构（时态、代词）下的焦虑词汇出现频率。

Result: （1）社交媒体上的焦虑在一天中呈现规律性变化，上午8点最高，中午最低；（2）周末焦虑最低，周中最高；（3）过去时表达焦虑最高，未来时最低；（4）三人称代词和主格代词的帖子比一、二人称和宾格代词的帖子出现更多焦虑词。

Conclusion: 研究揭示了社交媒体上焦虑情绪的时间、语法和关注点规律，有助于进一步理解情绪表达和社会心理动态。

Abstract: In this short paper, we make use of a recently created lexicon of word-anxiety associations to analyze large amounts of US and Canadian social media data (tweets) to explore *when* we are anxious and what insights that reveals about us. We show that our levels of anxiety on social media exhibit systematic patterns of rise and fall during the day -- highest at 8am (in-line with when we have high cortisol levels in the body) and lowest around noon. Anxiety is lowest on weekends and highest mid-week. We also examine anxiety in past, present, and future tense sentences to show that anxiety is highest in past tense and lowest in future tense. Finally, we examine the use of anxiety and calmness words in posts that contain pronouns to show: more anxiety in 3rd person pronouns (he, they) posts than 1st and 2nd person pronouns and higher anxiety in posts with subject pronouns (I, he, she, they) than object pronouns (me, him, her, them). Overall, these trends provide valuable insights on not just when we are anxious, but also how different types of focus (future, past, self, outward, etc.) are related to anxiety.

</details>


### [100] [EVOKE: Emotion Vocabulary Of Korean and English](https://arxiv.org/abs/2602.10414)
*Yoonwon Jung,Hagyeong Shin,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: 本文推出了EVOKE中英对照情感词汇数据集，覆盖英韩两种语言的大量情感词及其多对多对应关系，并识别了各自特有情感词，是当前最全面系统的跨语言情感词库之一。


<details>
  <summary>Details</summary>
Motivation: 不同语言中情感表达方式存在差异，且跨语言对照的情感词汇资源稀缺，限制了情感科学、心理语言学和自然语言处理等领域的研究。作者希望填补中英情感词汇平行数据的缺口，促进两种语言间的情感词汇研究。

Method: 作者收集并整理了1427个韩语、1399个英语情感词，系统性地对819个韩语、924个英语的形容词和动词进行标注，包括多重含义关系、多义词及情感隐喻。建立了词语的多对多翻译关系，并识别各语言特有的情感词。

Result: 构建完成了详细标注、包含多义关系、跨语言对应与特有词识别等功能的EVOKE数据集。数据集展现了高度的覆盖度、细致性和理论中立性，是英韩情感词领域最全面和系统的资源之一。

Conclusion: EVOKE数据集为情感科学、心理语言学、计算语言学和自然语言处理等研究提供了重要、灵活的工具，可支持不同理论视角下的多样化需求。该数据集已对外公开，为后续相关领域研究提供了坚实基础。

Abstract: This paper introduces EVOKE, a parallel dataset of emotion vocabulary in English and Korean. The dataset offers comprehensive coverage of emotion words in each language, in addition to many-to-many translations between words in the two languages and identification of language-specific emotion words. The dataset contains 1,427 Korean words and 1,399 English words, and we systematically annotate 819 Korean and 924 English adjectives and verbs. We also annotate multiple meanings of each word and their relationships, identifying polysemous emotion words and emotion-related metaphors. The dataset is, to our knowledge, the most comprehensive, systematic, and theory-agnostic dataset of emotion words in both Korean and English to date. It can serve as a practical tool for emotion science, psycholinguistics, computational linguistics, and natural language processing, allowing researchers to adopt different views on the resource reflecting their needs and theoretical perspectives. The dataset is publicly available at https://github.com/yoonwonj/EVOKE.

</details>


### [101] [LATA: A Tool for LLM-Assisted Translation Annotation](https://arxiv.org/abs/2602.10454)
*Baorong Huang,Ali Asiri*

Main category: cs.CL

TL;DR: 本文提出了一款结合大型语言模型（LLM）与人工互动的平行语料库构建工具，旨在提高结构性差异较大语言对（如阿英）的句子对齐与注释质量。


<details>
  <summary>Details</summary>
Motivation: 随着平行语料库构建任务从简单对齐向多层注释发展，传统自动化工具无法应对如阿英这类语言对的深层语义和结构差异，亟需更智能且结合人工的辅助系统。

Method: 该系统采用基于模板的Prompt Manager，利用LLM进行句子分割和对齐，并输出结构化JSON。整体流程结合自动化预处理和人工参与（human-in-the-loop），并引入独立注释机制以便细致标注翻译技巧。

Result: 该工具实现了自动处理与人工细化有机融合，提高了复杂语对的注释效率和精度，并可适配专业领域的特定需求。

Conclusion: 借助LLM和互动式流程，本系统在效率与语言学精度间取得平衡，为多层次平行语料库构建提供了新路径，尤其适用于结构差异显著的语言对。

Abstract: The construction of high-quality parallel corpora for translation research has increasingly evolved from simple sentence alignment to complex, multi-layered annotation tasks. This methodological shift presents significant challenges for structurally divergent language pairs, such as Arabic--English, where standard automated tools frequently fail to capture deep linguistic shifts or semantic nuances. This paper introduces a novel, LLM-assisted interactive tool designed to reduce the gap between scalable automation and the rigorous precision required for expert human judgment. Unlike traditional statistical aligners, our system employs a template-based Prompt Manager that leverages large language models (LLMs) for sentence segmentation and alignment under strict JSON output constraints. In this tool, automated preprocessing integrates into a human-in-the-loop workflow, allowing researchers to refine alignments and apply custom translation technique annotations through a stand-off architecture. By leveraging LLM-assisted processing, the tool balances annotation efficiency with the linguistic precision required to analyze complex translation phenomena in specialized domains.

</details>


### [102] [Neuro-Symbolic Synergy for Interactive World Modeling](https://arxiv.org/abs/2602.10480)
*Hongyu Zhao,Siyu Zhou,Haolin Yang,Zengyi Qin,Tianyi Zhou*

Main category: cs.CL

TL;DR: 本文提出了Neuro-Symbolic Synergy (NeSyS) 框架，将大语言模型（LLMs）的概率语义与可执行的符号规则结合，实现了表达性与健壮性的兼得，并在多个交互环境中表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型具备强推理能力，但作为世界模型时容易产生幻觉，尤其在需要严格遵循确定性转移规则的场景。相反，符号世界模型虽逻辑一致却语义表达力不足，因此有必要将两者优势结合。

Method: NeSyS框架融合LLMs的概率语义先验和可执行符号规则，通过在模型间交替训练，并用对方解释不充分的轨迹强化训练。与基于规则的提示不同，符号WM会直接约束LLM输出概率分布。神经WM只针对不被符号规则覆盖的轨迹进行微调，从而减少一半的训练数据。

Result: 在ScienceWorld、Webshop、Plancraft三大交互环境上，NeSyS在WM预测准确率和数据效率方面均优于对照基线方法。

Conclusion: NeSyS能够实现神经与符号方法的有效结合，在保证高准确率的同时，大幅提升训练数据的利用效率，具有良好的应用前景。

Abstract: Large language models (LLMs) exhibit strong general-purpose reasoning capabilities, yet they frequently hallucinate when used as world models (WMs), where strict compliance with deterministic transition rules--particularly in corner cases--is essential. In contrast, Symbolic WMs provide logical consistency but lack semantic expressivity. To bridge this gap, we propose Neuro-Symbolic Synergy (NeSyS), a framework that integrates the probabilistic semantic priors of LLMs with executable symbolic rules to achieve both expressivity and robustness. NeSyS alternates training between the two models using trajectories inadequately explained by the other. Unlike rule-based prompting, the symbolic WM directly constrains the LLM by modifying its output probability distribution. The neural WM is fine-tuned only on trajectories not covered by symbolic rules, reducing training data by 50% without loss of accuracy. Extensive experiments on three distinct interactive environments, i.e., ScienceWorld, Webshop, and Plancraft, demonstrate NeSyS's consistent advantages over baselines in both WM prediction accuracy and data efficiency.

</details>


### [103] [Canvas-of-Thought: Grounding Reasoning via Mutable Structured States](https://arxiv.org/abs/2602.10494)
*Lingzhuang Sun,Yuxia Zhu,Ruitong Liu,Hao Liang,Zheng Sun,Caijun Jia,Honghao He,Yuchen Wu,Siyuan Li,Jingxuan Wei,Xiangxiang Zhang,Bihui Yu,Wentao Zhang*

Main category: cs.CL

TL;DR: 本文提出Canvas-of-Thought (Canvas-CoT)，利用HTML Canvas辅助多模态大语言模型(MMLMs)实现高效、多维度、可修正的推理方式，大幅提升模型在高维复杂任务中的推理表现。


<details>
  <summary>Details</summary>
Motivation: 传统链式思维(CoT)虽然提升了多模态大模型的推理能力，但仅依赖线性文本，特别是在几何、SVG设计等高维场景下，导致推理过程难以纠错、状态跟踪负担重、上下文消耗高，缺乏显式可视化指导，影响推理准确性。

Method: 作者引入Canvas-CoT，利用可操作的HTML Canvas作为外部推理底座，模型可在Canvas上进行原子性DOM操作(增删查改)，实现原地状态修正和“真实状态”显式维护。此外，通过渲染反馈回路形成严格的可视约束和修正，辅助模型处理难以用语言表述的复杂任务。

Result: 在VCode、RBench-V、MathVista等数据集上的实验显示，Canvas-CoT显著优于现有多模态推理基线，表现出更高的推理精度和上下文效率。

Conclusion: Canvas-CoT为多模态大模型推理提供了全新范式，实现了可交互、可校正、高维场景下更精准高效的推理，为复杂任务推理提供了有力支持。

Abstract: While Chain-of-Thought (CoT) prompting has significantly advanced the reasoning capabilities of Multimodal Large Language Models (MLLMs), relying solely on linear text sequences remains a bottleneck for complex tasks. We observe that even when auxiliary visual elements are interleaved, they are often treated as static snapshots within a one-dimensional, unstructured reasoning chain. We argue that such approaches treat reasoning history as an immutable stream: correcting a local error necessitates either generating verbose downstream corrections or regenerating the entire context. This forces the model to implicitly maintain and track state updates, significantly increasing token consumption and cognitive load. This limitation is particularly acute in high-dimensional domains, such as geometry and SVG design, where the textual expression of CoT lacks explicit visual guidance, further constraining the model's reasoning precision. To bridge this gap, we introduce \textbf{Canvas-of-Thought (Canvas-CoT)}. By leveraging a HTML Canvas as an external reasoning substrate, Canvas-CoT empowers the model to perform atomic, DOM-based CRUD operations. This architecture enables in-place state revisions without disrupting the surrounding context, allowing the model to explicitly maintain the "ground truth". Furthermore, we integrate a rendering-based critique loop that serves as a hard constraint validator, providing explicit visual feedback to resolve complex tasks that are difficult to articulate through text alone. Extensive experiments on VCode, RBench-V, and MathVista demonstrate that Canvas-CoT significantly outperforms existing baselines, establishing a new paradigm for context-efficient multimodal reasoning.

</details>


### [104] [On the Robustness of Knowledge Editing for Detoxification](https://arxiv.org/abs/2602.10504)
*Ming Dong,Shiyi Tang,Ziyan Peng,Guanyi Chen,Tingting He*

Main category: cs.CL

TL;DR: 本文提出了一种关注稳健性的评估框架，针对现有知识编辑（KE）方法对大型语言模型的去有害化效果，从鲁棒性多个维度进行系统评估，并发现现有方法在多目标、跨语言等情况下效果有限。


<details>
  <summary>Details</summary>
Motivation: 虽然知识编辑方法被广泛应用于大型语言模型的去有害化，但评估主要依赖自动化毒性分类器，忽略了行为层面上的真正抑制效果，可能低估了失败情形，亟需更可靠的评估方式。

Method: 作者设计了一个新的稳健性评估框架，从优化稳健性、组分稳健性和跨语言稳健性三个角度，对去有害化的实际有效性进行深入分析，揭示现有基于KE的方法容易出现表面去有害化而未抑制真实有害行为（伪去有害化）等问题。

Result: 实验表明：1）KE方法常出现伪去有害化，表现为生成行为退化但并未真正抑制有害内容；2）当同时编辑多个有害行为时，去有害化效果显著下降；3）单语与跨语种去有害化都依赖于具体模型与方法的匹配，只在部分情境下有效。

Conclusion: KE方法的去有害化表现仅在少数模型、目标有限及部分语言中较为稳健，存在明显局限性。实际部署中需谨慎选择和评估。

Abstract: Knowledge-Editing-based (KE-based) detoxification has emerged as a promising approach for mitigating harmful behaviours in Large Language Models. Existing evaluations, however, largely rely on automatic toxicity classifiers, implicitly assuming that reduced toxicity scores reflect genuine behavioural suppression. In this work, we propose a robustness-oriented evaluation framework for KE-based detoxification that examines its reliability beyond standard classifier-based metrics along three dimensions: optimisation robustness, compositional robustness, and cross-lingual robustness. We identify pseudo-detoxification as a common failure mode, where apparent toxicity reductions arise from degenerate generation behaviours rather than meaningful suppression of unsafe content. We further show that detoxification effectiveness degrades when multiple unsafe behaviours are edited jointly, and that both monolingual and cross-lingual detoxification remain effective only under specific model-method combinations. Overall, our results indicate that KE-based detoxification is robust only for certain models, limited numbers of detoxification objectives, and a subset of languages.

</details>


### [105] [LHAW: Controllable Underspecification for Long-Horizon Tasks](https://arxiv.org/abs/2602.10525)
*George Pu,Michael S. Lee,Udari Madhushani Sehwag,David J. Lee,Bryan Zhu,Yash Maurya,Mohit Raghavendra,Yuan Xue,Samuel Marc Denton*

Main category: cs.CL

TL;DR: 本文提出了LHAW（Long-Horizon Augmented Workflows）框架，可将任何明确任务系统性地转化为不同程度信息缺失的变体，用于评估自主代理在长时序任务下遇到不明确（模糊、信息缺失）情状的处理能力。


<details>
  <summary>Details</summary>
Motivation: 长时序自动化代理需要具备处理歧义和主动澄清的能力，但当前缺乏通用、可扩展的流程来系统性地刻画和度量任务模糊性带来的影响。

Method: LHAW为模块化、数据集无关的合成管道，通过分别在任务的目标、约束、输入和上下文四个维度删除信息，生成可控的、不同程度欠指定（模糊）的任务变体。不同于使用大模型预测模糊性的方案，LHAW通过实际的代理实验来验证任务变体，将其分为关键性差异、行为偏离和良性三类。

Result: 基于LHAW，作者在TheAgentCompany、SWE-Bench Pro和MCP-Atlas三大数据集上发布了285个任务变体，并正式分析了现有智能体在识别、推理及处理模糊任务情景方面的表现。

Conclusion: LHAW首次为长时序自动化代理在不明确场景下澄清行为的性价比评估和提升提供了系统方法，有助于开发更可靠的自主系统。

Abstract: Long-horizon workflow agents that operate effectively over extended periods are essential for truly autonomous systems. Their reliable execution critically depends on the ability to reason through ambiguous situations in which clarification seeking is necessary to ensure correct task execution. However, progress is limited by the lack of scalable, task-agnostic frameworks for systematically curating and measuring the impact of ambiguity across custom workflows. We address this gap by introducing LHAW (Long-Horizon Augmented Workflows), a modular, dataset-agnostic synthetic pipeline that transforms any well-specified task into controllable underspecified variants by systematically removing information across four dimensions - Goals, Constraints, Inputs, and Context - at configurable severity levels. Unlike approaches that rely on LLM predictions of ambiguity, LHAW validates variants through empirical agent trials, classifying them as outcome-critical, divergent, or benign based on observed terminal state divergence. We release 285 task variants from TheAgentCompany, SWE-Bench Pro and MCP-Atlas according to our taxonomy alongside formal analysis measuring how current agents detect, reason about, and resolve underspecification across ambiguous settings. LHAW provides the first systematic framework for cost-sensitive evaluation of agent clarification behavior in long-horizon settings, enabling development of reliable autonomous systems.

</details>


### [106] [When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning](https://arxiv.org/abs/2602.10560)
*Leheng Sheng,Yongtao Zhang,Wenchang Ma,Yaorui Shi,Ting Huang,Xiang Wang,An Zhang,Ke Shen,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 文章提出GRU-Mem方法，通过引入基于文本控制的门控机制，解决大型语言模型在长上下文推理中内存膨胀和不必要计算的问题，提升推理效率和表现。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如MemAgent）在长上下文推理中由于无选择性地更新记忆和缺乏合理的循环退出机制，导致性能下降、内存耗尽及计算冗余。

Method: 提出GRU-Mem结构，设计了包含“更新门”和“退出门”的门控机制，结合端到端的强化学习，分别用奖励信号指导模型何时更新记忆、何时退出循环。

Result: 在各类长上下文推理任务中，GRU-Mem在效果和效率上显著优于MemAgent，推理速度最高提升400%。

Conclusion: GRU-Mem方法通过精细的门控策略，有效解决了长上下文推理中的关键问题，为大模型推理效率和实际应用带来提升。

Abstract: While reasoning over long context is crucial for various real-world applications, it remains challenging for large language models (LLMs) as they suffer from performance degradation as the context length grows. Recent work MemAgent has tried to tackle this by processing context chunk-by-chunk in an RNN-like loop and updating a textual memory for final answering. However, this naive recurrent memory update faces two crucial drawbacks: (i) memory can quickly explode because it can update indiscriminately, even on evidence-free chunks; and (ii) the loop lacks an exit mechanism, leading to unnecessary computation after even sufficient evidence is collected. To address these issues, we propose GRU-Mem, which incorporates two text-controlled gates for more stable and efficient long-context reasoning. Specifically, in GRU-Mem, the memory only updates when the update gate is open and the recurrent loop will exit immediately once the exit gate is open. To endow the model with such capabilities, we introduce two reward signals $r^{\text{update}}$ and $r^{\text{exit}}$ within end-to-end RL, rewarding the correct updating and exiting behaviors respectively. Experiments on various long-context reasoning tasks demonstrate the effectiveness and efficiency of GRU-Mem, which generally outperforms the vanilla MemAgent with up to 400\% times inference speed acceleration.

</details>


### [107] [Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters](https://arxiv.org/abs/2602.10604)
*Ailin Huang,Ang Li,Aobo Kong,Bin Wang,Binxing Jiao,Bo Dong,Bojun Wang,Boyu Chen,Brian Li,Buyun Ma,Chang Su,Changxin Miao,Changyi Wan,Chao Lou,Chen Hu,Chen Xu,Chenfeng Yu,Chengting Feng,Chengyuan Yao,Chunrui Han,Dan Ma,Dapeng Shi,Daxin Jiang,Dehua Ma,Deshan Sun,Di Qi,Enle Liu,Fajie Zhang,Fanqi Wan,Guanzhe Huang,Gulin Yan,Guoliang Cao,Guopeng Li,Han Cheng,Hangyu Guo,Hanshan Zhang,Hao Nie,Haonan Jia,Haoran Lv,Hebin Zhou,Hekun Lv,Heng Wang,Heung-Yeung Shum,Hongbo Huang,Hongbo Peng,Hongyu Zhou,Hongyuan Wang,Houyong Chen,Huangxi Zhu,Huimin Wu,Huiyong Guo,Jia Wang,Jian Zhou,Jianjian Sun,Jiaoren Wu,Jiaran Zhang,Jiashu Lv,Jiashuo Liu,Jiayi Fu,Jiayu Liu,Jie Cheng,Jie Luo,Jie Yang,Jie Zhou,Jieyi Hou,Jing Bai,Jingcheng Hu,Jingjing Xie,Jingwei Wu,Jingyang Zhang,Jishi Zhou,Junfeng Liu,Junzhe Lin,Ka Man Lo,Kai Liang,Kaibo Liu,Kaijun Tan,Kaiwen Yan,Kaixiang Li,Kang An,Kangheng Lin,Lei Yang,Liang Lv,Liang Zhao,Liangyu Chen,Lieyu Shi,Liguo Tan,Lin Lin,Lina Chen,Luck Ma,Mengqiang Ren,Michael Li,Ming Li,Mingliang Li,Mingming Zhang,Mingrui Chen,Mitt Huang,Na Wang,Peng Liu,Qi Han,Qian Zhao,Qinglin He,Qinxin Du,Qiuping Wu,Quan Sun,Rongqiu Yang,Ruihang Miao,Ruixin Han,Ruosi Wan,Ruyan Guo,Shan Wang,Shaoliang Pang,Shaowen Yang,Shengjie Fan,Shijie Shang,Shiliang Yang,Shiwei Li,Shuangshuang Tian,Siqi Liu,Siye Wu,Siyu Chen,Song Yuan,Tiancheng Cao,Tianchi Yue,Tianhao Cheng,Tianning Li,Tingdan Luo,Wang You,Wei Ji,Wei Yuan,Wei Zhang,Weibo Wu,Weihao Xie,Wen Sun,Wenjin Deng,Wenzhen Zheng,Wuxun Xie,Xiangfeng Wang,Xiangwen Kong,Xiangyu Liu,Xiangyu Zhang,Xiaobo Yang,Xiaojia Liu,Xiaolan Yuan,Xiaoran Jiao,Xiaoxiao Ren,Xiaoyun Zhang,Xin Li,Xin Liu,Xin Wu,Xing Chen,Xingping Yang,Xinran Wang,Xu Zhao,Xuan He,Xuanti Feng,Xuedan Cai,Xuqiang Zhou,Yanbo Yu,Yang Li,Yang Xu,Yanlin Lai,Yanming Xu,Yaoyu Wang,Yeqing Shen,Yibo Zhu,Yichen Lv,Yicheng Cao,Yifeng Gong,Yijing Yang,Yikun Yang,Yin Zhao,Yingxiu Zhao,Yinmin Zhang,Yitong Zhang,Yixuan Zhang,Yiyang Chen,Yongchi Zhao,Yongshen Long,Yongyao Wang,Yousong Guan,Yu Zhou,Yuang Peng,Yuanhao Ding,Yuantao Fan,Yuanzhen Yang,Yuchu Luo,Yudi Zhao,Yue Peng,Yueqiang Lin,Yufan Lu,Yuling Zhao,Yunzhou Ju,Yurong Zhang,Yusheng Li,Yuxiang Yang,Yuyang Chen,Yuzhu Cai,Zejia Weng,Zetao Hong,Zexi Li,Zhe Xie,Zheng Ge,Zheng Gong,Zheng Zeng,Zhenyi Lu,Zhewei Huang,Zhichao Chang,Zhiguo Huang,Zhiheng Hu,Zidong Yang,Zili Wang,Ziqi Ren,Zixin Zhang,Zixuan Wang*

Main category: cs.CL

TL;DR: Step 3.5 Flash是一种高效的稀疏MoE大模型，专为智能代理场景设计，兼顾推理能力和推理速度，在多项基准测试上表现接近业界顶尖模型，可高效应用于实际工业环境。


<details>
  <summary>Details</summary>
Motivation: 当前前沿智能代理模型虽然能力强但计算和推理成本高，造成部署困难。作者希望在不降低智能水平的前提下，实现更高的推理效率和经济性，从而推动智能代理在实际大规模场景中的应用。

Method: 提出Step 3.5 Flash模型，结合196B参数作为基础模型和11B激活参数以提升推理速度，创新性地采用交错的3:1滑动窗口/全局注意力机制和多Token预测（MTP-3）来降低延迟和成本；同时引入结合可验证信号与偏好反馈的可扩展强化学习框架，保证在稳定大规模离策略训练下持续自我提升。

Result: 在代理、编程和数学等基准任务上取得了与GPT-5.2 xHigh、Gemini 3.0 Pro等前沿模型相当的成绩（如IMO-AnswerBench 85.4%、LiveCodeBench-v6 86.4%、tau2-Bench 88.2%、BrowseComp 69.0%、Terminal-Bench 51.0%），实现了能效与能力的良好平衡。

Conclusion: Step 3.5 Flash在保持接近顶级智能水平的同时，实现了极高的推理效率与工业可部署性，为智能代理模型在现实世界中规模化应用提供了基础。

Abstract: We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.

</details>


### [108] [Online Causal Kalman Filtering for Stable and Effective Policy Optimization](https://arxiv.org/abs/2602.10609)
*Shuo He,Lang Feng,Xin Cheng,Lei Feng,Bo An*

Main category: cs.CL

TL;DR: 本文提出了一种基于Kalman滤波的新方法，用于提升大语言模型强化学习中的采样比稳定性，从而实现更有效、稳定的策略优化。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的强化学习训练存在高方差的采样比，这会严重影响策略优化的稳定性。现有方法要么对整个序列采用固定采样比，要么只独立调整各token采样比，忽略了序列中token之间的时序关系，因此训练中会出现结构性不一致和性能不佳。

Method: 作者提出了一种在线因果Kalman滤波算法（KPO），将目标采样比建模为一个随token演化的潜在状态，并用Kalman滤波器在线、递归地根据历史token的状态（而非未来token）更新采样比。此方法既保留了token级的本地调整，又有效平滑了因高方差带来的噪声。

Result: 在有挑战性的数学推理数据集上，KPO方法在稳定性与性能方面均优于当前先进方法。

Conclusion: KPO方法有效提升了大模型强化学习中的采样比估计稳定性，能带来更稳定和有效的策略优化，对大语言模型训练具有实际意义。

Abstract: Reinforcement learning for large language models suffers from high-variance token-level importance sampling (IS) ratios, which would destabilize policy optimization at scale. To improve stability, recent methods typically use a fixed sequence-level IS ratio for all tokens in a sequence or adjust each token's IS ratio separately, thereby neglecting temporal off-policy derivation across tokens in a sequence. In this paper, we first empirically identify that local off-policy deviation is structurally inconsistent at the token level, which may distort policy-gradient updates across adjacent tokens and lead to training collapse. To address the issue, we propose Online Causal Kalman Filtering for stable and effective Policy Optimization (KPO). Concretely, we model the desired IS ratio as a latent state that evolves across tokens and apply a Kalman filter to update this state online and autoregressively based on the states of past tokens, regardless of future tokens. The resulting filtered IS ratios preserve token-wise local structure-aware variation while strongly smoothing noise spikes, yielding more stable and effective policy updates. Experimentally, KPO achieves superior results on challenging math reasoning datasets compared with state-of-the-art counterparts.

</details>


### [109] [How Do Decoder-Only LLMs Perceive Users? Rethinking Attention Masking for User Representation Learning](https://arxiv.org/abs/2602.10622)
*Jiahao Yuan,Yike Xu,Jinyong Wen,Baokun Wang,Yang Chen,Xiaotong Lin,Wuliang Huang,Ziyi Gao,Xing Fu,Yu Cheng,Weiqiang Wang*

Main category: cs.CL

TL;DR: 本文系统性比较了三种注意力mask（因果、混合、双向）对用户表示学习的影响，并提出一种基于梯度引导的soft masking方法，提升了decoder-only大模型在用户行为建模上的表现。


<details>
  <summary>Details</summary>
Motivation: 用户表示学习常依赖decoder-only大语言模型作为行为编码器，但attention masking方式对embedding质量的影响尚未深入探讨。本文希望通过系统研究不同mask及提出新方法，提升实际推荐与用户认知任务中的表现。

Method: 在Alipay的大规模真实用户行为数据集上，作者在统一的对比学习框架下，对因果、混合、双向三类mask进行了实验比较。提出了一个名为Gradient-Guided Soft Masking的方法，利用gradient-based pre-warmup进行训练初期软化mask，再逐步开启未来attention，以实现平滑过渡和更优训练。

Result: 在9个工业级用户认知任务上（如预测、偏好、营销敏感性），作者的方法相较于因果、混合、及仅用scheduler的baseline，表现出更稳定的训练过程与更高质量的双向用户表示，同时保持与预训练兼容。

Conclusion: 合适的mask设计及训练迁移机制对decoder-only模型用户表示学习至关重要。提出的新方法有效提升了大模型在实际工业场景下的应用价值。

Abstract: Decoder-only large language models are increasingly used as behavioral encoders for user representation learning, yet the impact of attention masking on the quality of user embeddings remains underexplored. In this work, we conduct a systematic study of causal, hybrid, and bidirectional attention masks within a unified contrastive learning framework trained on large-scale real-world Alipay data that integrates long-horizon heterogeneous user behaviors. To improve training dynamics when transitioning from causal to bidirectional attention, we propose Gradient-Guided Soft Masking, a gradient-based pre-warmup applied before a linear scheduler that gradually opens future attention during optimization. Evaluated on 9 industrial user cognition benchmarks covering prediction, preference, and marketing sensitivity tasks, our approach consistently yields more stable training and higher-quality bidirectional representations compared with causal, hybrid, and scheduler-only baselines, while remaining compatible with decoder pretraining. Overall, our findings highlight the importance of masking design and training transition in adapting decoder-only LLMs for effective user representation learning. Our code is available at https://github.com/JhCircle/Deepfind-GGSM.

</details>


### [110] [UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory](https://arxiv.org/abs/2602.10652)
*Yongshi Ye,Hui Jiang,Feihu Jiang,Tian Lan,Yichao Du,Biao Fu,Xiaodong Shi,Qianghuai Jia,Longyue Wang,Weihua Luo*

Main category: cs.CL

TL;DR: 本文提出了一种用于大语言模型（LLM）智能体的自进化记忆框架UMEM，实现了记忆抽取与管理的联合优化，有效提升了多轮交互任务表现，并显著增强了记忆泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体的方法主要关注记忆管理，而将记忆抽取作为静态过程，导致智能体记忆易过拟合于实例噪声，泛化能力差。因此，亟需实现记忆抽取和管理的高效协同，提升记忆的可靠性与泛化性能。

Method: 提出UMEM框架，将记忆抽取与记忆管理联合优化。为减轻例子特定的过拟合，引入了语义邻域建模，并通过群体层面的边际收益奖励机制（GRPO）训练模型，在语义相关问题簇上评估记忆的效用。

Result: 在五个基准测试上，UMEM显著优于强竞争基线，在多轮交互任务上最高提升达10.67%。实验还表明，UMEM在持续进化过程中展现了单调增长的性能曲线。

Conclusion: UMEM不仅能提升记忆的泛化能力，还能在实际多轮智能体任务中带来显著性能提高，为自进化LLM智能体的记忆机制提供了新的方向。代码和模型将开源。

Abstract: Self-evolving memory serves as the trainable parameters for Large Language Models (LLMs)-based agents, where extraction (distilling insights from experience) and management (updating the memory bank) must be tightly coordinated. Existing methods predominately optimize memory management while treating memory extraction as a static process, resulting in poor generalization, where agents accumulate instance-specific noise rather than robust memories. To address this, we propose Unified Memory Extraction and Management (UMEM), a self-evolving agent framework that jointly optimizes a Large Language Model to simultaneous extract and manage memories. To mitigate overfitting to specific instances, we introduce Semantic Neighborhood Modeling and optimize the model with a neighborhood-level marginal utility reward via GRPO. This approach ensures memory generalizability by evaluating memory utility across clusters of semantically related queries. Extensive experiments across five benchmarks demonstrate that UMEM significantly outperforms highly competitive baselines, achieving up to a 10.67% improvement in multi-turn interactive tasks. Futhermore, UMEM maintains a monotonic growth curve during continuous evolution. Codes and models will be publicly released.

</details>


### [111] [Benchmarks Are Not That Out of Distribution: Word Overlap Predicts Performance](https://arxiv.org/abs/2602.10657)
*Woojin Chung,Jeonghoon Kim*

Main category: cs.CL

TL;DR: 本文研究了预训练数据与评测数据在统计模式上的重叠对语言模型表现的影响，发现词级unigram交叉熵与基准测试表现呈现强烈反相关，表明标准基准测试与预训练数据高度重叠。


<details>
  <summary>Details</summary>
Motivation: 当前尚不清楚高质量预训练数据应具备哪些特征，尤其是预训练语料与评测集之间的统计相似度对模型性能的影响。因此，作者希望厘清数据的“重叠性”对模型评测结果的驱动作用。

Method: 作者通过词级unigram交叉熵与词频统计来量化预训练语料与评测集的重叠性，并在10个零样本基准、4个不同规模的预训练集和不同模型大小（4亿到30亿参数）间开展对照实验。

Result: 结果表明，词级unigram交叉熵与基准测试表现出稳定的负相关关系，说明训练与评测之间的词语重叠显著提升了模型在基准测试集上的表现。此外，扩大预训练集但保持相似交叉熵也能提升表现，进一步突出词频统计的作用。

Conclusion: 标准的语言理解基准测试与预训练数据实际上只有很弱的分布外特性，因此基于简单的词重叠统计即可很好预测基准表现。这对如何构建更具挑战性的评测集有重要启示。

Abstract: Understanding what constitutes high-quality pre-training data remains a central question in language model training. In this work, we investigate whether benchmark performance is primarily driven by the degree of statistical pattern overlap between pre-training corpora and evaluation datasets. We measure this overlap using word-level unigram cross-entropy and word frequency statistics, and perform controlled experiments across $10$ zero-shot benchmarks, $4$ pre-training datasets spanning $8.5\mathrm{B}$ to $60\mathrm{B}$ tokens, and model sizes ranging from $400\mathrm{M}$ to $3\mathrm{B}$ parameters. Our results demonstrate a robust inverse relationship between word-level unigram cross-entropy and benchmark performance, suggesting that widely used benchmarks are strongly influenced by word overlap between training and evaluation data. Thus, larger pre-training subsets with similar word-level unigram cross-entropy yield improved downstream results, indicating that word frequency statistics play an additional role in shaping benchmark scores. Taken together, these results suggest that many standard benchmarks are only weakly out-of-distribution relative to pre-training corpora, so that simple word-overlap statistics predict benchmark performance.

</details>


### [112] [Targeted Syntactic Evaluation of Language Models on Georgian Case Alignment](https://arxiv.org/abs/2602.10661)
*Daniel Gallagher,Gerhard Heyer*

Main category: cs.CL

TL;DR: 本文评估了基于Transformer的语言模型在格鲁吉亚语分裂作格指派上的表现，发现这些模型特别难以正确分配ergative格。


<details>
  <summary>Details</summary>
Motivation: 格鲁吉亚语采用罕见的分裂作格系统，通过不同的名词格形态（主格、作格、与格）来标记句子成分角色。现有语言模型在这种复杂指派上的能力未知，因此需要专门评估。

Method: 作者基于句法树库，利用Grew查询语言生成最小对比样本，设计了包含七个任务、共370个测试样本的数据集。样本中涉及三种名词格的各种组合。评测了5个编码器和2个解码器模型，通过词级或句子级准确率进行对比。

Result: 所有模型对于作格（ERG）标记表现最差，对主格（NOM）表现最佳，这与三种名词格的语料分布频率相关（NOM>DAT>ERG）。

Conclusion: 数据稀缺，以及作格角色的特殊性，共同导致了模型在作格指派上的低表现。所构建的数据集已开放，提出的方法可为低资源语言的句法评测提供新思路。

Abstract: This paper evaluates the performance of transformer-based language models on split-ergative case alignment in Georgian, a particularly rare system for assigning grammatical cases to mark argument roles. We focus on subject and object marking determined through various permutations of nominative, ergative, and dative noun forms. A treebank-based approach for the generation of minimal pairs using the Grew query language is implemented. We create a dataset of 370 syntactic tests made up of seven tasks containing 50-70 samples each, where three noun forms are tested in any given sample. Five encoder- and two decoder-only models are evaluated with word- and/or sentence-level accuracy metrics. Regardless of the specific syntactic makeup, models performed worst in assigning the ergative case correctly and strongest in assigning the nominative case correctly. Performance correlated with the overall frequency distribution of the three forms (NOM > DAT > ERG). Though data scarcity is a known issue for low-resource languages, we show that the highly specific role of the ergative along with a lack of available training data likely contributes to poor performance on this case. The dataset is made publicly available and the methodology provides an interesting avenue for future syntactic evaluations of languages where benchmarks are limited.

</details>


### [113] [Locomo-Plus: Beyond-Factual Cognitive Memory Evaluation Framework for LLM Agents](https://arxiv.org/abs/2602.10715)
*Yifei Li,Weidong Guo,Lingling Zhang,Rongman Xu,Muye Huang,Hui Liu,Lijiao Xu,Yu Xu,Jun Liu*

Main category: cs.CL

TL;DR: 该论文提出了LoCoMo-Plus基准，用于衡量对隐性约束的长对话记忆能力，而不仅仅是表面事实回忆。通过新的评测框架揭示了现有方法难以捕捉的认知记忆挑战。


<details>
  <summary>Details</summary>
Motivation: 现有对话模型评测主要只关注事实性记忆，忽视了实际对话中存在的隐性约束（如用户状态、目标等），这导致评测结果与真实能力不符。

Method: 提出LoCoMo-Plus新基准，特别关注模型在长对话中记忆和应用隐性约束的能力，并建立基于约束一致性的统一评测框架，以替代传统的字符串匹配和任务型提示。

Result: 实验发现主流大模型、检索与记忆方法在认知记忆上依然存在明显短板，且这些问题无法被现有基准很好捕捉。

Conclusion: 提升对隐性约束的长时记忆能力是对话模型的重要挑战，新基准和评测框架为相关研究提供了有力工具。

Abstract: Long-term conversational memory is a core capability for LLM-based dialogue systems, yet existing benchmarks and evaluation protocols primarily focus on surface-level factual recall. In realistic interactions, appropriate responses often depend on implicit constraints such as user state, goals, or values that are not explicitly queried later. To evaluate this setting, we introduce \textbf{LoCoMo-Plus}, a benchmark for assessing cognitive memory under cue--trigger semantic disconnect, where models must retain and apply latent constraints across long conversational contexts. We further show that conventional string-matching metrics and explicit task-type prompting are misaligned with such scenarios, and propose a unified evaluation framework based on constraint consistency. Experiments across diverse backbone models, retrieval-based methods, and memory systems demonstrate that cognitive memory remains challenging and reveals failures not captured by existing benchmarks. Our code and evaluation framework are publicly available at: https://github.com/xjtuleeyf/Locomo-Plus.

</details>


### [114] [Macaron: Controlled, Human-Written Benchmark for Multilingual and Multicultural Reasoning via Template-Filling](https://arxiv.org/abs/2602.10732)
*Alaa Elsetohy,Sama Hadhoud,Haryo Akbarianto Wibowo,Chenxi Whitehouse,Genta Indra Winata,Fajri Koto,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 该论文提出了Macaron，一个以模板为核心，用于多语言推理和文化相关性测试的新型基准数据集，支持20种语言，涵盖多种文化情境，有效测试了多语种大模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有多语言基准数据集在文化推理方面存在局限：翻译数据集以英语情景为主，文化优先的数据集缺乏推理类型的控制。因此需要一个能兼顾多种推理类型和文化要素，同时支持多语言对比测试的新基准。

Method: 作者设计了100个与语言无关的模板，覆盖七种推理类型和22种文化方面，由本地注释者创建情景相关的英语及本地语言多项选择题和系统生成的判断题。最终数据集包含11,862个实例，覆盖20个国家/文化、10种文字、20种语言。

Result: 零样本评估中，21个多语言大模型在推理模式下表现最好，英语和本地语言准确率接近，而开源模型在本地语言和判断任务上的表现显著下降，接近蒙猜。以文化为基础的数学和计数模板最具挑战性。

Conclusion: Macaron能够系统地衡量多语言大模型在跨文化推理中的能力，是首个同时强调文化多样性与推理类型控制的多语言基准，有助于推动多语种模型在实际、多文化环境下的应用与发展。

Abstract: Multilingual benchmarks rarely test reasoning over culturally grounded premises: translated datasets keep English-centric scenarios, while culture-first datasets often lack control over the reasoning required. We propose Macaron, a template-first benchmark that factorizes reasoning type and cultural aspect across question languages. Using 100 language-agnostic templates that cover 7 reasoning types, 22 cultural aspects, native annotators create scenario-aligned English and local-language multiple-choice questions and systematically derived True/False questions. Macaron contains 11,862 instances spanning 20 countries/cultural contexts, 10 scripts, and 20 languages (including low-resource ones like Amharic, Yoruba, Zulu, Kyrgyz, and some Arabic dialects). In zero-shot evaluation of 21 multilingual LLMs, reasoning-mode models achieve the strongest performance and near-parity between English and local languages, while open-weight models degrade substantially in local languages and often approach chance on T/F tasks. Culture-grounded mathematical and counting templates are consistently the hardest. The data can be accessed here https://huggingface.co/datasets/AlaaAhmed2444/Macaron.

</details>


### [115] [Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs](https://arxiv.org/abs/2602.10740)
*Yuming Yan,Shuo Yang,Kai Tang,Sihong Chen,Yang Zhang,Ke Xu,Dan Hu,Qun Yu,Pengfei Hu,Edith C. H. Ngai*

Main category: cs.CL

TL;DR: 本文提出了一种面向视觉-语言模型（VLM）高效领域适应的新方法RCPA，通过逐步引导模型适应新领域知识，同时保持其通用能力成效显著。


<details>
  <summary>Details</summary>
Motivation: VLM具有广泛的通用性，但在医学影像等专用领域表现不佳。传统微调虽能提升专域性能，但容易引发灾难性遗忘，导致通用能力丧失。由于VLM大规模再预训练代价高，急需高效后训练适应方法以解决领域适应与通用性保留的矛盾。

Method: 作者提出RCPA，一种结合课程学习的逐步调优方式。模型初期仅施加部分输出约束，安全地接触新领域知识，随着熟悉度提升，训练过渡为完整任务优化，实现对领域需求的深度对齐。

Result: RCPA在多个专用领域和通用基准测试中都表现优异，既提升了领域适应能力，又有效保留了原有的通用多模态能力。

Conclusion: RCPA为VLM领域适应问题提供了实用可行的解决方案，可在提升专用领域表现的同时防止通用能力丧失，为高性能、域自适应的多模态模型发展指明了新方向。

Abstract: Vision-Language Models (VLMs) demonstrate remarkable general-purpose capabilities but often fall short in specialized domains such as medical imaging or geometric problem-solving. Supervised Fine-Tuning (SFT) can enhance performance within a target domain, but it typically causes catastrophic forgetting, limiting its generalization. The central challenge, therefore, is to adapt VLMs to new domains while preserving their general-purpose capabilities. Continual pretraining is effective for expanding knowledge in Large Language Models (LLMs), but it is less feasible for VLMs due to prohibitive computational costs and the unavailability of pretraining data for most open-source models. This necessitates efficient post-training adaptation methods. Reinforcement learning (RL)-based approaches such as Group Relative Policy Optimization (GRPO) have shown promise in preserving general abilities, yet they often fail in domain adaptation scenarios where the model initially lacks sufficient domain knowledge, leading to optimization collapse. To bridge this gap, we propose Reinforced Curriculum Pre-Alignment (RCPA), a novel post-training paradigm that introduces a curriculum-aware progressive modulation mechanism. In the early phase, RCPA applies partial output constraints to safely expose the model to new domain concepts. As the model's domain familiarity increases, training gradually transitions to full generation optimization, refining responses and aligning them with domain-specific preferences. This staged adaptation balances domain knowledge acquisition with the preservation of general multimodal capabilities. Extensive experiments across specialized domains and general benchmarks validate the effectiveness of RCPA, establishing a practical pathway toward building high-performing and domain-adaptive VLMs.

</details>


### [116] [Deep Learning-based Method for Expressing Knowledge Boundary of Black-Box LLM](https://arxiv.org/abs/2602.10801)
*Haotian Sheng,Heyong Wang,Ming Hong,Hongman He,Junqiu Liu*

Main category: cs.CL

TL;DR: 本文提出了LSCL方法，以解决大语言模型（LLM）内容生成失真（幻觉）问题，尤其针对仅API可访问的黑盒LLM，通过深度学习映射方法实现其知识边界表达，实验效果显著超过已有方案。


<details>
  <summary>Details</summary>
Motivation: 当前LLM存在内容生成失真（幻觉）问题，核心原因是LLM无法表达其知识边界，且主流方法多为白盒模型，缺乏适用于黑盒API LLM的解决机制。

Method: 提出LSCL方法，在知识蒸馏框架下，将黑盒LLM的输入问题、输出答案及token概率作为深度学习模型输入，训练映射其内部知识状态，实现知识边界量化和表达。若部分黑盒LLM不提供token概率，文中还提出了自适应备选方案。

Result: LSCL在多个公开数据集和主流黑盒LLM上进行实验，相较基线模型，在准确率和召回率等指标上有明显提升。自适应备选方法的表现也接近LSCL，优于基线方法。

Conclusion: LSCL能有效帮助黑盒LLM表达知识边界，缓解幻觉问题，有较强的泛化性和实用价值，对API型LLM应用具有实际意义。

Abstract: Large Language Models (LLMs) have achieved remarkable success, however, the emergence of content generation distortion (hallucination) limits their practical applications. The core cause of hallucination lies in LLMs' lack of awareness regarding their stored internal knowledge, preventing them from expressing their knowledge state on questions beyond their internal knowledge boundaries, as humans do. However, existing research on knowledge boundary expression primarily focuses on white-box LLMs, leaving methods suitable for black-box LLMs which offer only API access without revealing internal parameters-largely unexplored. Against this backdrop, this paper proposes LSCL (LLM-Supervised Confidence Learning), a deep learning-based method for expressing the knowledge boundaries of black-box LLMs. Based on the knowledge distillation framework, this method designs a deep learning model. Taking the input question, output answer, and token probability from a black-box LLM as inputs, it constructs a mapping between the inputs and the model' internal knowledge state, enabling the quantification and expression of the black-box LLM' knowledge boundaries. Experiments conducted on diverse public datasets and with multiple prominent black-box LLMs demonstrate that LSCL effectively assists black-box LLMs in accurately expressing their knowledge boundaries. It significantly outperforms existing baseline models on metrics such as accuracy and recall rate. Furthermore, considering scenarios where some black-box LLMs do not support access to token probability, an adaptive alternative method is proposed. The performance of this alternative approach is close to that of LSCL and surpasses baseline models.

</details>


### [117] [Beyond Confidence: The Rhythms of Reasoning in Generative Models](https://arxiv.org/abs/2602.10816)
*Deyuan Liu,Zecheng Wang,Zhanyue Qin,Zhiying Tu,Dianhui Chu,Dianbo Sui*

Main category: cs.CL

TL;DR: 提出了一个新的指标$δ_{TCB}$，用于衡量大语言模型（LLMs）预测结果对输入微小扰动的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有指标（如准确率、困惑度）无法充分反映LLMs对输入微小变化的预测鲁棒性，因为这些指标可能掩盖模型内部状态对扰动的敏感性。

Method: 作者提出了一种新的度量指标Token Constraint Bound（$δ_{TCB}$），测量LLM内部状态可以承受多大扰动而不改变主要的下一个token预测。该指标与输出嵌入空间的几何结构密切相关。

Result: 实验表明，$δ_{TCB}$与有效的prompt工程相关，并能发现困惑度遗漏的预测不稳定性。

Conclusion: $δ_{TCB}$为分析和提升LLM上下文稳定性提供了一种有理论依据的补充手段。

Abstract: Large Language Models (LLMs) exhibit impressive capabilities yet suffer from sensitivity to slight input context variations, hampering reliability. Conventional metrics like accuracy and perplexity fail to assess local prediction robustness, as normalized output probabilities can obscure the underlying resilience of an LLM's internal state to perturbations. We introduce the Token Constraint Bound ($δ_{\mathrm{TCB}}$), a novel metric that quantifies the maximum internal state perturbation an LLM can withstand before its dominant next-token prediction significantly changes. Intrinsically linked to output embedding space geometry, $δ_{\mathrm{TCB}}$ provides insights into the stability of the model's internal predictive commitment. Our experiments show $δ_{\mathrm{TCB}}$ correlates with effective prompt engineering and uncovers critical prediction instabilities missed by perplexity during in-context learning and text generation. $δ_{\mathrm{TCB}}$ offers a principled, complementary approach to analyze and potentially improve the contextual stability of LLM predictions.

</details>


### [118] [I can tell whether you are a Native Hawlêri Speaker! How ANN, CNN, and RNN perform in NLI-Native Language Identification](https://arxiv.org/abs/2602.10832)
*Hardi Garari,Hossein Hassani*

Main category: cs.CL

TL;DR: 本论文首次针对库尔德语索拉尼方言中的Hewlêri次方言进行母语识别（NLI）研究，提出并比较了多种神经网络模型，RNN在5秒音频分割下获得了95.92%的最高准确率，并构建了相关语音数据集。


<details>
  <summary>Details</summary>
Motivation: 目前大多数母语识别研究集中在主要语言（如英语、德语）或主方言上，对于次方言及资源贫乏语言领域研究稀缺。尤其是在库尔德语索拉尼方言的Hewlêri次方言上，尚无公开的相关语音数据和识别研究。因此，填补这一空白具有重要的学术和应用价值。

Method: 研究团队录制并收集了40名（17女23男）Hewlêri次方言的本地及非本地说话人共约24小时的访谈语音，构建数据集。然后分别基于人工神经网络（ANN）、卷积神经网络（CNN）和循环神经网络（RNN）三种模型进行NLI任务实验，实验设计包括不同时间片段（1-60秒）、欠采样、过采样和交叉验证等66种配置。

Result: 在所有实验设置中，RNN模型在5秒音频分割、80:10:10数据分割下取得了最高准确率95.92%。

Conclusion: 该研究首次在索拉尼库尔德语Hewlêri次方言上建立了语音识别数据集并探索了母语识别，为该领域后续相关研究奠定了基础，并推动了资源稀缺语言/方言的自然语言处理技术发展。

Abstract: Native Language Identification (NLI) is a task in Natural Language Processing (NLP) that typically determines the native language of an author through their writing or a speaker through their speaking. It has various applications in different areas, such as forensic linguistics and general linguistics studies. Although considerable research has been conducted on NLI regarding two different languages, such as English and German, the literature indicates a significant gap regarding NLI for dialects and subdialects. The gap becomes wider in less-resourced languages such as Kurdish. This research focuses on NLI within the context of a subdialect of Sorani (Central) Kurdish. It aims to investigate the NLI for Hewlêri, a subdialect spoken in Hewlêr (Erbil), the Capital of the Kurdistan Region of Iraq. We collected about 24 hours of speech by recording interviews with 40 native or non-native Hewlêri speakers, 17 female and 23 male. We created three Neural Network-based models: Artificial Neural Network (ANN), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN), which were evaluated through 66 experiments, covering various time-frames from 1 to 60 seconds, undersampling, oversampling, and cross-validation. The RNN model showed the highest accuracy of 95.92% for 5-second audio segmentation, using an 80:10:10 data splitting scheme. The created dataset is the first speech dataset for NLI on the Hewlêri subdialect in the Sorani Kurdish dialect, which can be of benefit to various research areas.

</details>


### [119] [C-MOP: Integrating Momentum and Boundary-Aware Clustering for Enhanced Prompt Evolution](https://arxiv.org/abs/2602.10874)
*Binwei Yan,Yifei Fu,Mingjian Zhu,Hanting Chen,Mingxuan Yuan,Yunhe Wang,Hailin Hu*

Main category: cs.CL

TL;DR: 本文提出了一种新型自动化提示词优化框架C-MOP，有效提升了大型语言模型的表现，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前自动提示词优化方法面临噪声和冲突更新信号的问题，影响优化效果。作者希望通过更稳定和精细的更新机制提升优化质量。

Method: 提出了C-MOP框架，结合了边界感知对比采样（BACS）和动量引导的语义聚类（MGSC）：BACS从批次层级挖掘三元特征，对正负样本进行判别边界采样；MGSC利用文本动量机制提炼历史共识以减少语义冲突。

Result: 大量实验显示C-MOP在各项基准任务中持续优于SOTA方法（如PromptWizard、ProTeGi），平均提升1.58%和3.35%。一款通用3B参数LLM经C-MOP优化后，甚至超过70B参数的领域专用密集模型。

Conclusion: C-MOP方法能有效驱动提示词进化，极大提升LLM能力，为自动提示优化提供了强有力的技术方案。

Abstract: Automatic prompt optimization is a promising direction to boost the performance of Large Language Models (LLMs). However, existing methods often suffer from noisy and conflicting update signals. In this research, we propose C-MOP (Cluster-based Momentum Optimized Prompting), a framework that stabilizes optimization via Boundary-Aware Contrastive Sampling (BACS) and Momentum-Guided Semantic Clustering (MGSC). Specifically, BACS utilizes batch-level information to mine tripartite features--Hard Negatives, Anchors, and Boundary Pairs--to precisely characterize the typical representation and decision boundaries of positive and negative prompt samples. To resolve semantic conflicts, MGSC introduces a textual momentum mechanism with temporal decay that distills persistent consensus from fluctuating gradients across iterations. Extensive experiments demonstrate that C-MOP consistently outperforms SOTA baselines like PromptWizard and ProTeGi, yielding average gains of 1.58% and 3.35%. Notably, C-MOP enables a general LLM with 3B activated parameters to surpass a 70B domain-specific dense LLM, highlighting its effectiveness in driving precise prompt evolution. The code is available at https://github.com/huawei-noah/noah-research/tree/master/C-MOP.

</details>


### [120] [Diagnosing Structural Failures in LLM-Based Evidence Extraction for Meta-Analysis](https://arxiv.org/abs/2602.10881)
*Zhiyin Tan,Jennifer D'Souza*

Main category: cs.CL

TL;DR: 本论文评估了大语言模型（LLM）在系统性综述和荟萃分析中，从文本中结构化提取数据的能力，发现其在复杂结构绑定和数值归因任务中表现不佳，难以实现自动化荟萃分析的需求。


<details>
  <summary>Details</summary>
Motivation: 系统综述和荟萃分析需要将叙述性研究文章转化为结构化、可量化的数据记录。近年来LLM进展迅速，但尚不明确其是否能满足复杂结构化数据抽取的需求，尤其是在变量、方法和效应量的精确绑定方面。

Method: 作者提出了一个结构化、诊断性评估框架，设计一系列逐步增加结构和数值复杂性的约束查询。利用五大科研领域的手动标注文库，配合统一的查询和评测协议，测试主流LLM在单文档及多文档大上下文情景下的数据抽取表现。

Result: LLM在处理简单属性查询时表现中等，但涉及变量间绑定、角色分配、数理方法及效应量归因等关系抽取时，性能急剧下降。完整的荟萃分析关联元组几乎无法可靠抽取，大上下文输入更易导致结构性错误和上游误差放大。

Conclusion: 造成自动化荟萃分析失败的根本原因不是实体识别，而是LLM对结构、关系绑定和数值归因的能力不足。现有模型难以满足高阶结构化证据抽取的需求，限制了其在综述与荟萃分析自动化中的应用。代码与数据已公开。

Abstract: Systematic reviews and meta-analyses rely on converting narrative articles into structured, numerically grounded study records. Despite rapid advances in large language models (LLMs), it remains unclear whether they can meet the structural requirements of this process, which hinge on preserving roles, methods, and effect-size attribution across documents rather than on recognizing isolated entities. We propose a structural, diagnostic framework that evaluates LLM-based evidence extraction as a progression of schema-constrained queries with increasing relational and numerical complexity, enabling precise identification of failure points beyond atom-level extraction. Using a manually curated corpus spanning five scientific domains, together with a unified query suite and evaluation protocol, we evaluate two state-of-the-art LLMs under both per-document and long-context, multi-document input regimes. Across domains and models, performance remains moderate for single-property queries but degrades sharply once tasks require stable binding between variables, roles, statistical methods, and effect sizes. Full meta-analytic association tuples are extracted with near-zero reliability, and long-context inputs further exacerbate these failures. Downstream aggregation amplifies even minor upstream errors, rendering corpus-level statistics unreliable. Our analysis shows that these limitations stem not from entity recognition errors, but from systematic structural breakdowns, including role reversals, cross-analysis binding drift, instance compression in dense result sections, and numeric misattribution, indicating that current LLMs lack the structural fidelity, relational binding, and numerical grounding required for automated meta-analysis. The code and data are publicly available at GitHub (https://github.com/zhiyintan/LLM-Meta-Analysis).

</details>


### [121] [The CLEF-2026 FinMMEval Lab: Multilingual and Multimodal Evaluation of Financial AI Systems](https://arxiv.org/abs/2602.10886)
*Zhuohan Xie,Rania Elbadry,Fan Zhang,Georgi Georgiev,Xueqing Peng,Lingfei Qian,Jimin Huang,Dimitar Dimitrov,Vanshikaa Jani,Yuyang Dai,Jiahui Geng,Yuxia Wang,Ivan Koychev,Veselin Stoyanov,Preslav Nakov*

Main category: cs.CL

TL;DR: FinMMEval Lab在CLEF 2026推出首个面向金融大语言模型的多语种多模态评测框架，包含三大任务，推动全球化金融AI发展。


<details>
  <summary>Details</summary>
Motivation: 当前金融NLP在市场报告、监管文本等分析上取得进展，但已有评测大多局限于单语种、纯文本且子任务有限，缺乏全面、全球化的评测手段。

Method: 设计并发布三个高度互联的任务：金融考试问答、多语种金融问答（PolyFiQA）、金融决策任务。通过跨语言、跨模态、多任务评价模型的理解、推理和决策能力，并公开数据集和评测工具。

Result: FinMMEval 2026为金融大模型提供了首个覆盖多语言和多模态的全面评测套件，增强了评测广度和深度。

Conclusion: 该实验室及其评测框架将推动开发更健壮、透明、包容全球多样性的金融AI，促进可复现的研究与全球AI公平性。

Abstract: We present the setup and the tasks of the FinMMEval Lab at CLEF 2026, which introduces the first multilingual and multimodal evaluation framework for financial Large Language Models (LLMs). While recent advances in financial natural language processing have enabled automated analysis of market reports, regulatory documents, and investor communications, existing benchmarks remain largely monolingual, text-only, and limited to narrow subtasks. FinMMEval 2026 addresses this gap by offering three interconnected tasks that span financial understanding, reasoning, and decision-making: Financial Exam Question Answering, Multilingual Financial Question Answering (PolyFiQA), and Financial Decision Making. Together, these tasks provide a comprehensive evaluation suite that measures models' ability to reason, generalize, and act across diverse languages and modalities. The lab aims to promote the development of robust, transparent, and globally inclusive financial AI systems, with datasets and evaluation resources publicly released to support reproducible research.

</details>


### [122] [SoftMatcha 2: A Fast and Soft Pattern Matcher for Trillion-Scale Corpora](https://arxiv.org/abs/2602.10908)
*Masataka Yoneda,Yusuke Matsushita,Go Kamoda,Kohei Suenaga,Takuya Akiba,Masaki Waga,Sho Yokoi*

Main category: cs.CL

TL;DR: 提出一种超快速且灵活的语义搜索算法，可在0.3秒内在万亿级自然语言语料库中进行搜索，并处理语义变体（替换、插入、删除）。实验显示其显著快于现有方法，并支持多语言在线演示。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模自然语言搜索方法在处理语义变体时效率低下，尤其是在超大规模（万亿级）语料下存在组合爆炸问题，急需一种高效、灵活且可扩展的搜索方案。

Method: 该方法基于后缀数组进行字符串匹配，通过磁盘感知的快速精确查找和动态、语料感知的剪枝策略，显著抑制因语义松弛带来的组合爆炸；并基于自然语言统计特性优化搜索空间扩展。

Result: 在FineWeb-Edu（1.4万亿Token）上，搜索延迟大幅低于infini-gram、infini-gram mini、SoftMatcha等代表方法。

Conclusion: 方法实现了大规模、高效、可处理语义变体的自然语言搜索，实际应用中可发现训练集中的基准污染问题，且支持多语种在线体验，显示出非常强的实用性与推广价值。

Abstract: We present an ultra-fast and flexible search algorithm that enables search over trillion-scale natural language corpora in under 0.3 seconds while handling semantic variations (substitution, insertion, and deletion). Our approach employs string matching based on suffix arrays that scales well with corpus size. To mitigate the combinatorial explosion induced by the semantic relaxation of queries, our method is built on two key algorithmic ideas: fast exact lookup enabled by a disk-aware design, and dynamic corpus-aware pruning. We theoretically show that the proposed method suppresses exponential growth in the search space with respect to query length by leveraging statistical properties of natural language. In experiments on FineWeb-Edu (Lozhkov et al., 2024) (1.4T tokens), we show that our method achieves significantly lower search latency than existing methods: infini-gram (Liu et al., 2024), infini-gram mini (Xu et al., 2025), and SoftMatcha (Deguchi et al., 2025). As a practical application, we demonstrate that our method identifies benchmark contamination in training corpora, unidentified by existing approaches. We also provide an online demo of fast, soft search across corpora in seven languages.

</details>


### [123] [Computational Phenomenology of Temporal Experience in Autism: Quantifying the Emotional and Narrative Characteristics of Lived Unpredictability](https://arxiv.org/abs/2602.10947)
*Kacper Dudzic,Karolina Drożdż,Maciej Wodziński,Anastazja Szuła,Marcin Moskalewicz*

Main category: cs.CL

TL;DR: 这篇论文探讨了自闭症个体在时间感知方面的困扰，采用现象学访谈和计算分析方法，发现其挑战主要体现在生活经历的不可预测性，而非叙事能力。


<details>
  <summary>Details</summary>
Motivation: 既有关于自闭症和时间感知的研究多基于医学缺陷模型、样本量不足且缺乏现象学基础；作者希望弥合现象学与计算方法之间的鸿沟，并提升研究的代表性和深度。

Method: 研究整合了三种方法：A. 针对自闭症者的结构化现象学访谈；B. 针对专门建立的自闭症叙事自传语料库进行计算分析；C. 对自闭症自传使用叙事流动计算分析，评估其现象学真实性。

Result: 访谈显示自闭症组在生活不可预测性上与对照组有显著差异。计算分析显示自闭症叙事中的与时间相关词汇情感取向较负面，尤其是“即时性与突发性”类别，且包含更多表达断裂的词汇。叙事流动分析表明这些自闭症叙事更接近真实自传而非虚构故事。

Conclusion: 自闭症者在时间感知上的主要困扰是生活经历的不可预测性，这种困扰来源于实际体验而非叙事能力。

Abstract: Disturbances in temporality, such as desynchronization with the social environment and its unpredictability, are considered core features of autism with a deep impact on relationships. However, limitations regarding research on this issue include: 1) the dominance of deficit-based medical models of autism, 2) sample size in qualitative research, and 3) the lack of phenomenological anchoring in computational research. To bridge the gap between phenomenological and computational approaches and overcome sample-size limitations, our research integrated three methodologies. Study A: structured phenomenological interviews with autistic individuals using the Transdiagnostic Assessment of Temporal Experience. Study B: computational analysis of an autobiographical corpus of autistic narratives built for this purpose. Study C: a replication of a computational study using narrative flow measures to assess the perceived phenomenological authenticity of autistic autobiographies. Interviews revealed that the most significant differences between the autistic and control groups concerned unpredictability of experience. Computational results mirrored these findings: the temporal lexicon in autistic narratives was significantly more negatively valenced - particularly the "Immediacy & Suddenness" category. Outlier analysis identified terms associated with perceived discontinuity (unpredictably, precipitously, and abruptly) as highly negative. The computational analysis of narrative flow found that the autistic narratives contained within the corpus quantifiably resemble autobiographical stories more than imaginary ones. Overall, the temporal challenges experienced by autistic individuals were shown to primarily concern lived unpredictability and stem from the contents of lived experience, and not from autistic narrative construction.

</details>


### [124] [Search or Accelerate: Confidence-Switched Position Beam Search for Diffusion Language Models](https://arxiv.org/abs/2602.10953)
*Mingyu Cao,Alvaro Correia,Christos Louizos,Shiwei Liu,Lu Yin*

Main category: cs.CL

TL;DR: 本文提出了一种名为SOAR的新型解码算法，可自适应调整解码策略以提升扩散语言模型（DLM）的生成质量与效率，尤其在数学推理与代码生成任务中效果显著。


<details>
  <summary>Details</summary>
Motivation: 传统DLM采用贪心策略在每步解码时只解锁最有信心的位置，但这种局部最优选择容易导致模型陷入次优的解码顺序，尤其在需要复杂推理的问题上。为克服这一局限，研究者希望找到能更智能权衡探索与利用的解码方法。

Method: SOAR是一种无需重新训练的解码算法。其核心思想是：当模型对解码决策信心较低时，临时扩大对可能解码位置的探索，避免过早做出不可逆的决定；而当模型信心高时，则快速批量解码多个位置以加快整体推理速度。

Result: 在GSM8K（数学推理）、MBPP与HumanEval（代码生成）等基准数据集上，SOAR在Dream-7B和LLaDA-8B等模型中应用，显著提升了生成质量，并保持了具有竞争力的推理速度。

Conclusion: SOAR为DLM解码提供了一种实际有效的平衡质量与效率的方法，尤其适用于对解码顺序敏感的推理重任务，无需修改原模型即可获得性能提升。

Abstract: Diffusion Language Models (DLMs) generate text by iteratively denoising a masked sequence, repeatedly deciding which positions to commit at each step. Standard decoding follows a greedy rule: unmask the most confident positions, yet this local choice can lock the model into a suboptimal unmasking order, especially on reasoning-heavy prompts. We present SOAR, a training-free decoding algorithm that adapts its behavior to the model's uncertainty. When confidence is low, SOAR briefly widens the search over alternative unmasking decisions to avoid premature commitments; when confidence is high, it collapses the search and decodes many positions in parallel to reduce the number of denoising iterations. Across mathematical reasoning and code generation benchmarks (GSM8K, MBPP, HumanEval) on Dream-7B and LLaDA-8B, SOAR improves generation quality while maintaining competitive inference speed, offering a practical way to balance quality and efficiency in DLM decoding.

</details>


### [125] [LoRA-Squeeze: Simple and Effective Post-Tuning and In-Tuning Compression of LoRA Modules](https://arxiv.org/abs/2602.10993)
*Ivan Vulić,Adam Grycner,Quentin de Laroussilhe,Jonas Pfeiffer*

Main category: cs.CL

TL;DR: 提出了一种名为 LoRA-Squeeze 的新方法，通过在训练后或训练过程中动态调整 LoRA 模块的秩，实现更高效的参数微调和模型压缩，并在多个任务上表现优越。


<details>
  <summary>Details</summary>
Motivation: 目前流行的低秩适配 (LoRA) 方法虽然高效，但需要预先选定最优秩和相关超参数，这一过程繁琐且限制了灵活性。此外，复杂 LoRA 变体的实际部署也不方便。作者希望找到更简单、高效的秩控制和模块压缩方法。

Method: 首先使用较高秩对 LoRA 模块进行微调，之后通过重构或近似重构全权重更新矩阵，再用随机奇异值分解（RSVD）将其压缩至较低目标秩，从而生成紧凑的新 LoRA 模块，同时也提出了训练中逐步降秩的变体。

Result: 在13个文本和10个视觉-语言任务中的大量实验表明，后处理压缩后的低秩适配器，通常优于直接以目标秩训练的适配器，若在目标秩上再进行少量微调，效果更明显。逐步降秩的变体可以在模型大小和性能之间取得更优权衡。

Conclusion: LoRA-Squeeze为LoRA适配器的秩选择和压缩提供了更简单、有效的解决方案，在提升模型效率的同时增强了性能和实际应用的灵活性。

Abstract: Despite its huge number of variants, standard Low-Rank Adaptation (LoRA) is still a dominant technique for parameter-efficient fine-tuning (PEFT). Nonetheless, it faces persistent challenges, including the pre-selection of an optimal rank and rank-specific hyper-parameters, as well as the deployment complexity of heterogeneous-rank modules and more sophisticated LoRA derivatives. In this work, we introduce LoRA-Squeeze, a simple and efficient methodology that aims to improve standard LoRA learning by changing LoRA module ranks either post-hoc or dynamically during training}. Our approach posits that it is better to first learn an expressive, higher-rank solution and then compress it, rather than learning a constrained, low-rank solution directly. The method involves fine-tuning with a deliberately high(er) source rank, reconstructing or efficiently approximating the reconstruction of the full weight update matrix, and then using Randomized Singular Value Decomposition (RSVD) to create a new, compressed LoRA module at a lower target rank. Extensive experiments across 13 text and 10 vision-language tasks show that post-hoc compression often produces lower-rank adapters that outperform those trained directly at the target rank, especially if a small number of fine-tuning steps at the target rank is allowed. Moreover, a gradual, in-tuning rank annealing variant of LoRA-Squeeze consistently achieves the best LoRA size-performance trade-off.

</details>


### [126] [Linguistic Indicators of Early Cognitive Decline in the DementiaBank Pitt Corpus: A Statistical and Machine Learning Study](https://arxiv.org/abs/2602.11028)
*Artsvik Avetisyan,Sachin Kumar*

Main category: cs.CL

TL;DR: 本研究通过分析痴呆患者的自发表达语音转录，发现基于语言特征的模型能够有效区分早期认知衰退者与健康人。语法和句法特征在筛查中表现出很强的判别力，相关模型结果具备统计意义和可解释性，支持语言为基础的认知筛查方法在临床上的应用。


<details>
  <summary>Details</summary>
Motivation: 早期发现认知衰退对于痴呆症的干预和管理至关重要，而自发表达语言中的微妙变化是认知衰退的早期指标之一。本研究旨在发现能够被语义学解释的语言标记，以支持透明和可信的认知筛查手段。

Method: 作者利用DementiaBank Pitt语料库的自发表达语音转录，设计了三种语言表征（原始文本、结合词汇-语法信息的POS增强表征、仅POS表征），并采用逻辑回归和随机森林模型进行训练和评估。评估包括转录层面的训练-测试划分和避免说话人重叠的受试者五折交叉验证。模型可解释性分析采用全局特征重要性，同时通过非参数统计检验（Mann-Whitney U检验和Cliff's delta效应量）验证结果。

Result: 无论采用哪种表征，模型在区分早期认知衰退方面表现稳定，尤其是语法和句法特征在缺乏具体词汇信息时仍具有很强的区分力。受试者层面评价显示结果更加保守但一致。统计分析进一步确认功能词使用、词汇多样性、句子结构和话语连贯性等指标在组间存在显著差异，并与机器学习特征重要性高度一致。

Conclusion: 抽象语言特征可以作为早期认知衰退的稳健标记。在临床场景下，通过可解释的机器学习和统计检验相结合的方法，基于语言特征的认知筛查具有透明性和可靠性，适合推广至实际应用。

Abstract: Background: Subtle changes in spontaneous language production are among the earliest indicators of cognitive decline. Identifying linguistically interpretable markers of dementia can support transparent and clinically grounded screening approaches.
  Methods: This study analyzes spontaneous speech transcripts from the DementiaBank Pitt Corpus using three linguistic representations: raw cleaned text, a part-of-speech (POS)-enhanced representation combining lexical and grammatical information, and a POS-only syntactic representation. Logistic regression and random forest models were evaluated under two protocols: transcript-level train-test splits and subject-level five-fold cross-validation to prevent speaker overlap. Model interpretability was examined using global feature importance, and statistical validation was conducted using Mann-Whitney U tests with Cliff's delta effect sizes.
  Results: Across representations, models achieved stable performance, with syntactic and grammatical features retaining strong discriminative power even in the absence of lexical content. Subject-level evaluation yielded more conservative but consistent results, particularly for POS-enhanced and POS-only representations. Statistical analysis revealed significant group differences in functional word usage, lexical diversity, sentence structure, and discourse coherence, aligning closely with machine learning feature importance findings.
  Conclusion: The results demonstrate that abstract linguistic features capture robust markers of early cognitive decline under clinically realistic evaluation. By combining interpretable machine learning with non-parametric statistical validation, this study supports the use of linguistically grounded features for transparent and reliable language-based cognitive screening.

</details>


### [127] [Language Model Inversion through End-to-End Differentiation](https://arxiv.org/abs/2602.11044)
*Kevin Yandoka Denamganaï,Kartic Subr*

Main category: cs.CL

TL;DR: 本文提出了一种基于梯度优化的方法，实现了对语言模型（LM）输出的反向推断，即给定目标输出，自动优化出能生成该输出的输入提示。


<details>
  <summary>Details</summary>
Motivation: 目前对于语言模型的输入输出间关系，尤其是如何通过对输出的逆推得到合适输入的研究较少。该问题对于解释模型行为、逆向生成、攻击防御等任务具有重要意义。

Method: 作者将“给定目标输出，求对应输入”问题转换为梯度优化问题。具体做法是：首先提出一种简单算法，使冻结（不训练）状态下的现有语言模型实现端到端可微分；随后利用梯度下降法，优化输入提示，使其输出尽可能接近目标文本。关键思想是把语言模型看作对“分布序列”而非“符号序列”进行操作。

Result: 实验证明，提出的方法能够在多个白盒预训练语言模型下，对长度为20的目标输出，分别成功优化出长度为10和80的有效输入提示，并展示了较高的效率和可靠性。

Conclusion: 通过建立可微分优化框架，本文实现了语言模型的‘反向生成’，为分析、控制和攻击语言模型输出提供了新工具，对理论和应用都有积极价值。

Abstract: Despite emerging research on Language Models (LM), few approaches analyse the invertibility of LMs. That is, given a LM and a desirable target output sequence of tokens, determining what input prompts would yield the target output remains an open problem. We formulate this problem as a classical gradient-based optimisation. First, we propose a simple algorithm to achieve end-to-end differentiability of a given (frozen) LM and then find optimised prompts via gradient descent. Our central insight is to view LMs as functions operating on sequences of distributions over tokens (rather than the traditional view as functions on sequences of tokens). Our experiments and ablations demonstrate that our DLM-powered inversion can reliably and efficiently optimise prompts of lengths $10$ and $80$ for targets of length $20$, for several white-box LMs (out-of-the-box).

</details>


### [128] [Embedding Inversion via Conditional Masked Diffusion Language Models](https://arxiv.org/abs/2602.11047)
*Han Xiao*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的嵌入反演方法，将其建模为条件掩码扩散过程，通过并行去噪恢复所有token，而不是传统的序列自回归生成，实现了高效准确的嵌入还原。


<details>
  <summary>Details</summary>
Motivation: 传统的嵌入反演方法普遍采用自回归方式，速度较慢且推断复杂，因此亟需一种更高效、更准确的方法进行嵌入内容复原，且无需目标编码器访问。

Method: 作者将嵌入反演问题建模为带条件掩码的扩散过程。具体做法是用一个扩散语言模型，通过自适应层归一化将目标嵌入作为条件，针对整段序列并行去噪，仅需8次前向传播即可实现。模型规模为7800万参数，无需访问被还原序列原始的编码器。

Result: 在32-token的序列和三种主流嵌入模型的测试下，该方法可以达到81.3%的token还原准确率和0.87的余弦相似度，体现出优异的性能。

Conclusion: 该方法显著提升了嵌入反演的效率和准确性，实现了无编码器访问条件下的高效复原，为相关任务提供了更有力的技术工具。

Abstract: We frame embedding inversion as conditional masked diffusion, recovering all tokens in parallel through iterative denoising rather than sequential autoregressive generation. A masked diffusion language model is conditioned on the target embedding via adaptive layer normalization, requiring only 8 forward passes through a 78M parameter model with no access to the target encoder. On 32-token sequences across three embedding models, the method achieves 81.3% token accuracy and 0.87 cosine similarity.

</details>


### [129] [Conversational Behavior Modeling Foundation Model With Multi-Level Perception](https://arxiv.org/abs/2602.11065)
*Dingkun Zhou,Shuchang Pan,Jiachen Lian,Siddharth Banerjee,Sarika Pasumarthy,Dhruv Hebbar,Siddhant Patel,Zeyi Austin Li,Kan Jen Cheng,Sanay Bordia,Krish Patel,Akshaj Gupta,Tingle Li,Gopala Anumanchipalli*

Main category: cs.CL

TL;DR: 论文提出了一种用于建模和推理人类对话思路的新框架Graph-of-Thoughts（GoT），能够识别意图及其因果与时序关系，有助于对话系统的自然交互。


<details>
  <summary>Details</summary>
Motivation: 人类对话通过一系列有时序的言语行为展现出隐含的思维链，理解和建模这一过程对于构建自然流畅的人机全双工对话系统至关重要。当前系统在捕捉对话中的链式因果和推理能力方面仍存在不足，亟需一种能够深入解析对话意图及行为的新方法。

Method: 作者提出以多层感知和因果关系为核心的Graph-of-Thoughts（GoT）框架，采用分层标注方案，识别并区分高层意图与低层言语行为。通过开发含丰富事件且标注详尽的数据集，用transformer实现流式预测，将对话结构化为动态演化的图谱，并解释预测及推理过程。

Result: 该方法在合成和真实的双工对话语料上进行实验，结果表明GoT框架可实现稳健的对话行为识别，生成可解释的推理链条，并有助于对话系统推理能力的量化评测。

Conclusion: GoT框架不仅优化了全双工对话行为的检测，还提升了推理过程的可解释性，为后续对话系统的推理与人机交互研究提供了坚实基础。

Abstract: Human conversation is organized by an implicit chain of thoughts that manifests as timed speech acts. Capturing this perceptual pathway is key to building natural full-duplex interactive systems. We introduce a framework that models this process as multi-level perception, and then reasons over conversational behaviors via a Graph-of-Thoughts (GoT). Our approach formalizes the intent-to-action pathway with a hierarchical labeling scheme, predicting high-level communicative intents and low-level speech acts to learn their causal and temporal dependencies. To train this system, we develop a high quality corpus that pairs controllable, event-rich dialogue data with human-annotated labels. The GoT framework structures streaming predictions as an evolving graph, enabling a transformer to forecast the next speech act, generate concise justifications for its decisions, and dynamically refine its reasoning. Experiments on both synthetic and real duplex dialogues show that the framework delivers robust behavior detection, produces interpretable reasoning chains, and establishes a foundation for benchmarking conversational reasoning in full duplex spoken dialogue systems.

</details>


### [130] [Simultaneous Speech-to-Speech Translation Without Aligned Data](https://arxiv.org/abs/2602.11072)
*Tom Labiausse,Romain Fabre,Yannick Estève,Alexandre Défossez,Neil Zeghidour*

Main category: cs.CL

TL;DR: Hibiki-Zero提出了一种无需词级对齐的新方法，有效简化了同时语音翻译任务的训练流程，并在多个语言任务上达到了先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统同步语音翻译方法依赖于词级对齐数据，但这类数据难以大规模获取，且用语言特定启发式生成的对齐往往效果欠佳。为了解决这一限制，需要一种无需词级对齐的统一方案来推广到多语种和不同语法结构的语言。

Method: Hibiki-Zero首先利用句级对齐数据进行高延迟语音翻译训练，然后通过一种新颖的基于GRPO的强化学习方法优化延迟与翻译质量间的平衡，从而达成无需词级对齐的端到端训练。

Result: Hibiki-Zero在五个语言到英语的同步语音翻译任务中，在准确率、延迟、语音风格传递和自然度等方面都达到了SOTA（最先进）的水平。同时，模型还能用不到1000小时的新语言语音数据快速适配。此外作者还发布了45小时多语种评测基准数据，并公开了模型权重和推理代码。

Conclusion: Hibiki-Zero无需依赖语言特定的对齐启发式，大幅简化训练流程，提升跨语种可扩展性，对实际部署和研究均具有重要意义。

Abstract: Simultaneous speech translation requires translating source speech into a target language in real-time while handling non-monotonic word dependencies. Traditional approaches rely on supervised training with word-level aligned data, which is difficult to collect at scale and thus depends on synthetic alignments using language-specific heuristics that are suboptimal. We propose Hibiki-Zero, which eliminates the need for word-level alignments entirely. This fundamentally simplifies the training pipeline and enables seamless scaling to diverse languages with varying grammatical structures, removing the bottleneck of designing language-specific alignment heuristics. We first train on sentence-level aligned data to learn speech translation at high latency, then apply a novel reinforcement learning strategy using GRPO to optimize latency while preserving translation quality. Hibiki-Zero achieves state-of-the-art performance in translation accuracy, latency, voice transfer, and naturalness across five X-to-English tasks. Moreover, we demonstrate that our model can be adapted to support a new input language with less than 1000h of speech. We provide examples, model weights, inference code and we release a benchmark containing 45h of multilingual data for speech translation evaluation.

</details>


### [131] [SteuerLLM: Local specialized large language model for German tax law analysis](https://arxiv.org/abs/2602.11081)
*Sebastian Wind,Jeta Sopa,Laurin Schmid,Quirin Jackl,Sebastian Kiefer,Fei Wu,Martin Mayr,Harald Köstler,Gerhard Wellein,Andreas Maier,Soroosh Tayebi Arasteh*

Main category: cs.CL

TL;DR: 本文提出了SteuerEx，这是第一个基于真实德国大学税法考试生成的开放基准，并发布了专为德国税法设计的SteuerLLM语言模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在需要精确术语、严格结构和法律约束等形式化领域表现不佳，税法正是这种典型难题，需要精确引证和结构化推理。因此，缺乏税法领域高质量评测集与适应性强的模型是当前研究痛点。

Method: 作者提出SteuerEx，包含115道经过专家验证的考试题，涵盖六大税法领域，采用部分得分与真实考试相符。其后通过受控的检索增强管道，用真实考试材料生成大规模合成数据，训练域适应模型SteuerLLM（28B参数），并和同等规模及更大型通用模型做比较。

Result: SteuerLLM在德国税法任务上稳定地超过同等规模的通用模型，并在多个案例中优于更大规模模型。结果显示，针对领域的数据和结构优化，比单纯扩大模型参数更关键。

Conclusion: 领域特定的数据和结构性适应对于法律推理任务至关重要。作者开放发布了基准、训练数据和模型，有助于推动法律领域AI的复现与研究。

Abstract: Large language models (LLMs) demonstrate strong general reasoning and language understanding, yet their performance degrades in domains governed by strict formal rules, precise terminology, and legally binding structure. Tax law exemplifies these challenges, as correct answers require exact statutory citation, structured legal argumentation, and numerical accuracy under rigid grading schemes. We algorithmically generate SteuerEx, the first open benchmark derived from authentic German university tax law examinations. SteuerEx comprises 115 expert-validated examination questions spanning six core tax law domains and multiple academic levels, and employs a statement-level, partial-credit evaluation framework that closely mirrors real examination practice. We further present SteuerLLM, a domain-adapted LLM for German tax law trained on a large-scale synthetic dataset generated from authentic examination material using a controlled retrieval-augmented pipeline. SteuerLLM (28B parameters) consistently outperforms general-purpose instruction-tuned models of comparable size and, in several cases, substantially larger systems, demonstrating that domain-specific data and architectural adaptation are more decisive than parameter scale for performance on realistic legal reasoning tasks. All benchmark data, training datasets, model weights, and evaluation code are released openly to support reproducible research in domain-specific legal artificial intelligence. A web-based demo of SteuerLLM is available at https://steuerllm.i5.ai.fau.de.

</details>


### [132] [DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement Learning](https://arxiv.org/abs/2602.11089)
*Yicheng Chen,Zerun Ma,Xinchen Xie,Yining Li,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出DataChef-32B模型，通过在线强化学习自动生成用于大模型自适应的数据处理方案（data recipe），在多个任务上表现接近人工处理结果。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）的训练效果极度依赖于高质量大规模数据。数据处理流程（data recipe）的设计依赖专家手动调整，成本高、效率低，迫切需要自动化完整流程以降低人力投入。

Method: 作者提出端到端的数据处理方案生成任务，目标是输入目标任务及原始数据，输出完整的数据处理流程。采用DataChef-32B模型，通过针对候选方案设计的代理奖励进行在线强化学习，预测并优化下游模型性能。

Result: DataChef-32B在六个未见过的任务上生成的数据处理方案使下游模型性能与人工方案相当。特别地，在将Qwen3-1.7B-Base适配到数学领域时，获得AIME'25上的66.7分，超过原始模型表现。

Conclusion: 该研究有效推动了LLM训练流程的自动化，为自进化AI系统的发展提供了有价值的新思路。

Abstract: In the current landscape of Large Language Models (LLMs), the curation of large-scale, high-quality training data is a primary driver of model performance. A key lever is the \emph{data recipe}, which comprises a data processing pipeline to transform raw sources into training corpora. Despite the growing use of LLMs to automate individual data processing steps, such as data synthesis and filtering, the overall design of data recipes remains largely manual and labor-intensive, requiring substantial human expertise and iteration. To bridge this gap, we formulate \emph{end-to-end data recipe generation} for LLM adaptation. Given a target benchmark and a pool of available data sources, a model is required to output a complete data recipe that adapts a base LLM to the target task. We present DataChef-32B, which performs online reinforcement learning using a proxy reward that predicts downstream performance for candidate recipes. Across six held-out tasks, DataChef-32B produces practical recipes that reach comparable downstream performance to those curated by human experts. Notably, the recipe from DataChef-32B adapts Qwen3-1.7B-Base to the math domain, achieving 66.7 on AIME'25 and surpassing Qwen3-1.7B. This work sheds new light on automating LLM training and developing self-evolving AI systems.

</details>


### [133] [Can Large Language Models Make Everyone Happy?](https://arxiv.org/abs/2602.11091)
*Usman Naseem,Gautam Siddharth Kashyap,Ebad Shabbir,Sushant Kumar Ray,Abdullah Mohammad,Rafiq Ali*

Main category: cs.CL

TL;DR: 本文提出了MisAlign-Profile，一个用于系统测量大型语言模型（LLMs）在安全、价值和文化等多维度间失配权衡的基准。该基准通过新数据集MISALIGNTRADE，评估不同类型LLMs在多维失配上的表现。


<details>
  <summary>Details</summary>
Motivation: 目前主流的LLM评测基准仅单独考查安全、价值或文化等维度，忽视了真实场景中这三者需要同时被满足时的复杂交互和权衡问题。因此难以全面反映LLM在真实应用下的表现和风险。

Method: 作者构建了一个覆盖112个规范领域（含安全、价值和文化）的英文问答数据集MISALIGNTRADE，并为每个问题贴上三类失配标签（对象、属性、关系）。使用两个不同的LLM模型进行数据扩展和高质量失配与对齐答案采集。再基于这个数据集系统评测了多种类型LLM在不同维度的失配权衡情况。

Result: 通过数据集及基准测试，揭示了现有通用、微调和开源LLMs在不同维度间存在12%-34%的多维失配权衡（即往往难以同时优化所有维度）。

Conclusion: MisAlign-Profile为系统性分析LLM在安全、价值和文化等维度交互权衡提供了新工具，为模型多维度协同优化和现实应用风险评估打下了基础。

Abstract: Misalignment in Large Language Models (LLMs) refers to the failure to simultaneously satisfy safety, value, and cultural dimensions, leading to behaviors that diverge from human expectations in real-world settings where these dimensions must co-occur. Existing benchmarks, such as SAFETUNEBED (safety-centric), VALUEBENCH (value-centric), and WORLDVIEW-BENCH (culture-centric), primarily evaluate these dimensions in isolation and therefore provide limited insight into their interactions and trade-offs. More recent efforts, including MIB and INTERPRETABILITY BENCHMARK-based on mechanistic interpretability, offer valuable perspectives on model failures; however, they remain insufficient for systematically characterizing cross-dimensional trade-offs. To address these gaps, we introduce MisAlign-Profile, a unified benchmark for measuring misalignment trade-offs inspired by mechanistic profiling. First, we construct MISALIGNTRADE, an English misaligned-aligned dataset across 112 normative domains taxonomies, including 14 safety, 56 value, and 42 cultural domains. In addition to domain labels, each prompt is classified with one of three orthogonal semantic types-object, attribute, or relations misalignment-using Gemma-2-9B-it and expanded via Qwen3-30B-A3B-Instruct-2507 with SimHash-based fingerprinting to avoid deduplication. Each prompt is paired with misaligned and aligned responses through two-stage rejection sampling to ensure quality. Second, we benchmark general-purpose, fine-tuned, and open-weight LLMs on MISALIGNTRADE-revealing 12%-34% misalignment trade-offs across dimensions.

</details>


### [134] [Safety Recovery in Reasoning Models Is Only a Few Early Steering Steps Away](https://arxiv.org/abs/2602.11096)
*Soumya Suvra Ghosal,Souradip Chakraborty,Vaibhav Singh,Furong Huang,Dinesh Manocha,Amrit Singh Bedi*

Main category: cs.CL

TL;DR: 本文提出了一种名为SafeThink的推理时安全防御方法，能有效降低大型多模态推理模型被越狱攻击的成功率，同时保持原有的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的链式思维（Chain-of-Thought）后训练虽可提升多模态大模型的推理能力，但会损害其安全性，使越狱攻击成功率提升。因此亟需在不牺牲推理能力的前提下提高模型在推理过程中的安全性。

Method: SafeThink是一种轻量级的推理时安全防御方法。它以安全恢复为满意约束，在推理过程中监控模型生成的思维轨迹，当检测到安全性阈值被突破时，自动插入一个优化过的短纠正前缀（如“Wait, think safely”），引导模型回复到安全轨道，而不是一味地最大化安全目标。

Result: 在六个开源大型多模态推理模型和四个越狱攻击基准（包括JailbreakV-28K、Hades、FigStep 和 MM-SafetyBench）上评测，SafeThink可将攻击成功率降低30-60%，在部分指标上从60%以上降到不到6%。同时，数学推理准确率基本保持不变。

Conclusion: SafeThink能够在不影响推理性能的前提下，大幅提升MLRM的安全性。实验还揭示，安全恢复通常只需在前1-3步及时干预即可有效转向到安全输出。

Abstract: Reinforcement learning (RL) based post-training for explicit chain-of-thought (e.g., GRPO) improves the reasoning ability of multimodal large-scale reasoning models (MLRMs). But recent evidence shows that it can simultaneously degrade safety alignment and increase jailbreak success rates. We propose SafeThink, a lightweight inference-time defense that treats safety recovery as a satisficing constraint rather than a maximization objective. SafeThink monitors the evolving reasoning trace with a safety reward model and conditionally injects an optimized short corrective prefix ("Wait, think safely") only when the safety threshold is violated. In our evaluations across six open-source MLRMs and four jailbreak benchmarks (JailbreakV-28K, Hades, FigStep, and MM-SafetyBench), SafeThink reduces attack success rates by 30-60% (e.g., LlamaV-o1: 63.33% to 5.74% on JailbreakV-28K, R1-Onevision: 69.07% to 5.65% on Hades) while preserving reasoning performance (MathVista accuracy: 65.20% to 65.00%). A key empirical finding from our experiments is that safety recovery is often only a few steering steps away: intervening in the first 1-3 reasoning steps typically suffices to redirect the full generation toward safe completions.

</details>


### [135] [TEGRA: Text Encoding With Graph and Retrieval Augmentation for Misinformation Detection](https://arxiv.org/abs/2602.11106)
*Géraud Faye,Wassila Ouerdane,Guillaume Gadek,Céline Hudelot*

Main category: cs.CL

TL;DR: 本文提出了一种结合知识库信息的文本表示方法TEG，用于提升谣言/虚假信息检测的准确率，并提出拓展模型TEGRA进一步整合领域知识。实验表明该方法优于单纯使用语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有谣言检测方法主要依赖文本表征，缺乏对外部知识的有效利用，而人工事实核查高度依赖额外知识。作者希望弥补这一天然短板。

Method: 提出Text Encoding with Graph（TEG）方法，将文本提取为结构化的图信息，并将文本和图共同编码用于分类。同时提出TEGRA，在TEG基础上引入领域知识。

Result: 通过大量实验验证，这种图-文本联合表征方案在谣言检测任务上优于仅用语言模型。TEGRA进一步结合领域知识，在多数场景下进一步提升检测准确率。

Conclusion: 集成外部及领域知识的图-文本编码方法对于提升谣言检测具有显著效果。TEG及其扩展TEGRA为信息检测提供了更高效的设计思路。

Abstract: Misinformation detection is a critical task that can benefit significantly from the integration of external knowledge, much like manual fact-checking. In this work, we propose a novel method for representing textual documents that facilitates the incorporation of information from a knowledge base. Our approach, Text Encoding with Graph (TEG), processes documents by extracting structured information in the form of a graph and encoding both the text and the graph for classification purposes. Through extensive experiments, we demonstrate that this hybrid representation enhances misinformation detection performance compared to using language models alone. Furthermore, we introduce TEGRA, an extension of our framework that integrates domain-specific knowledge, further enhancing classification accuracy in most cases.

</details>


### [136] [Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning](https://arxiv.org/abs/2602.11149)
*Dawid J. Kopiczko,Sagar Vaze,Tijmen Blankevoort,Yuki M. Asano*

Main category: cs.CL

TL;DR: 这篇论文指出，在推理语言模型中，监督微调（SFT）用较小数据集多轮训练比在大数据集上单轮训练能取得更好泛化效果，提出了数据重复训练的优势及其实用性。


<details>
  <summary>Details</summary>
Motivation: 传统经验认为增加独特训练样本数量能提升模型泛化能力，但作者注意到在链式思维（CoT）的SFT场景下，是否多次训练少量样本比训练大量不同样本更优并不明确。作者希望探索和验证这一反直觉现象，并寻找更高效的SFT训练准则。

Method: 作者在固定参数更新预算下，分别对Olmo3-7B模型在AIME'24/25和GPQA数据集上使用较小数据集多轮训练与较大数据集单轮训练进行对比，分析训练token准确率、泛化能力和灾难性遗忘现象，并提出以token准确率饱和度作为SFT训练停止的新准则。

Result: 与直觉相反，实验发现：在相同训练步数下，用400条样本反复训练128轮，效果明显优于用51200条样本只训练1轮，准确率提升高达12-26个百分点，同时未出现灾难性遗忘。此外，token准确率可有效反映重复训练已达饱和，继续训练不再提升。

Conclusion: 论文指出SFT中『重复优势』现象：样本完全记忆时模型泛化能力提升，这对大模型训练领域是新的开放问题。作者建议采用以token准确率为终止标准的多轮小数据集训练，替代成本高昂的数据扩充。

Abstract: Supervised fine-tuning (SFT) on chain-of-thought data is an essential post-training step for reasoning language models. Standard machine learning intuition suggests that training with more unique training samples yields better generalization. Counterintuitively, we show that SFT benefits from repetition: under a fixed update budget, training for more epochs on smaller datasets outperforms single-epoch training on larger datasets. On AIME'24/25 and GPQA benchmarks, Olmo3-7B trained for 128 epochs on 400 samples outperforms the equivalent 1 epoch on 51200 samples by 12-26 percentage points, with no additional catastrophic forgetting. We find that training token accuracy reliably signals when repetition has saturated; improvements from additional epochs plateau at full memorization, a pattern consistent across all settings. These findings provide a practical approach for reasoning SFT, where scaling epochs with token accuracy as a stopping criterion can replace expensive undirected data scaling. We pose the repetition advantage, where full memorization coincides with improved generalization, as a new open problem for the community in understanding the training dynamics of large language models.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [137] [Adaptive Time Step Flow Matching for Autonomous Driving Motion Planning](https://arxiv.org/abs/2602.10285)
*Ananya Trivedi,Anjian Li,Mohamed Elnoor,Yusuf Umut Ciftci,Avinash Singh,Jovin D'sa,Sangjae Bae,David Isele,Taskin Padir,Faizan M. Tariq*

Main category: cs.RO

TL;DR: 本文提出了一种基于条件流匹配的新型自动驾驶轨迹规划框架，用于实时预测周围交通参与者的动态并规划自身轨迹，解决了扩散模型和一致性模型推理速度慢及噪声调度依赖强的问题，实现了高效高质量的在线自动驾驶决策。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶领域主要通过大规模模仿学习促进模型对复杂交通场景的泛化，但现有扩散模型和一致性模型方法在推理时需要大量去噪步骤或噪声调度调整，导致高时延和重训练成本，限制了实时应用场景。作者旨在解决上述性能和灵活性瓶颈。

Method: 作者提出基于条件流匹配的方法，实现对交通参与者未来运动和本车路径的联合预测与规划；设计轻量级方差估计器，实现推理步数的在线选择，无需针对不同调度重训练；融合凸二次规划后处理，提高轨迹平滑度和行驶质量；模型在Waymo Open Motion Dataset上训练并测试。

Result: 该方法在NVIDIA RTX 3070 GPU上可实时20Hz运行，能完成变道、巡航和无保护左转等复杂驾驶动作，无需场景特定调优。相比于transformer、扩散模型及一致性模型，在轨迹平滑性及动态约束遵循方面均有提升。

Conclusion: 所提出的条件流匹配框架为自动驾驶轨迹规划提供了一种兼具实时性与高质量表现的解决方案，克服了现有大规模模仿学习路线下的推理时延与调度难题，为在线部署奠定了坚实基础。

Abstract: Autonomous driving requires reasoning about interactions with surrounding traffic. A prevailing approach is large-scale imitation learning on expert driving datasets, aimed at generalizing across diverse real-world scenarios. For online trajectory generation, such methods must operate at real-time rates. Diffusion models require hundreds of denoising steps at inference, resulting in high latency. Consistency models mitigate this issue but rely on carefully tuned noise schedules to capture the multimodal action distributions common in autonomous driving. Adapting the schedule, typically requires expensive retraining. To address these limitations, we propose a framework based on conditional flow matching that jointly predicts future motions of surrounding agents and plans the ego trajectory in real time. We train a lightweight variance estimator that selects the number of inference steps online, removing the need for retraining to balance runtime and imitation learning performance. To further enhance ride quality, we introduce a trajectory post-processing step cast as a convex quadratic program, with negligible computational overhead. Trained on the Waymo Open Motion Dataset, the framework performs maneuvers such as lane changes, cruise control, and navigating unprotected left turns without requiring scenario-specific tuning. Our method maintains a 20 Hz update rate on an NVIDIA RTX 3070 GPU, making it suitable for online deployment. Compared to transformer, diffusion, and consistency model baselines, we achieve improved trajectory smoothness and better adherence to dynamic constraints. Experiment videos and code implementations can be found at https://flow-matching-self-driving.github.io/.

</details>


### [138] [A Human-in-the-Loop Confidence-Aware Failure Recovery Framework for Modular Robot Policies](https://arxiv.org/abs/2602.10289)
*Rohan Banerjee,Krishna Palempalli,Bohan Yang,Jiaying Fang,Alif Abdullah,Tom Silver,Sarah Dean,Tapomayukh Bhattacharjee*

Main category: cs.RO

TL;DR: 提出了一种将人类纳入失效恢复过程的机器人控制框架，用于更有效且减轻用户负担地处理机器人在日常服务场景中的失败。


<details>
  <summary>Details</summary>
Motivation: 机器人在复杂的人类环境中经常遇到失败，需要向人类寻求帮助。但无差别、过多或不精准的人机交互会增加用户负担，特别是在护理等敏感场景，因此需要一种智能、高效且以用户为中心的失效恢复方法。

Method: 提出了一种模块化的机器人控制策略，将感知、规划、控制等拆分为多个模块。框架分别评估各模块的失败概率和请求人类协助的代价，分成模块选择器（定位最可能失败的模块）与请求决策算法（评估是否需要人类介入）。在合成实验和真实助餐机器人系统中对不同策略进行对比评测。

Result: 实验显示该方法可在保证恢复效率和健壮性的同时，显著减少用户的认知和体力负担。在助餐机器人与健康和有限行动能力用户的研究中，系统提升了恢复成功率并降低了用户工作量。

Conclusion: 在机器人协作场景中，如果模型同时考虑机器人模块的不确定性和人为干预的代价，就能实现更高效、更以用户为中心的失效恢复。

Abstract: Robots operating in unstructured human environments inevitably encounter failures, especially in robot caregiving scenarios. While humans can often help robots recover, excessive or poorly targeted queries impose unnecessary cognitive and physical workload on the human partner. We present a human-in-the-loop failure-recovery framework for modular robotic policies, where a policy is composed of distinct modules such as perception, planning, and control, any of which may fail and often require different forms of human feedback. Our framework integrates calibrated estimates of module-level uncertainty with models of human intervention cost to decide which module to query and when to query the human. It separates these two decisions: a module selector identifies the module most likely responsible for failure, and a querying algorithm determines whether to solicit human input or act autonomously. We evaluate several module-selection strategies and querying algorithms in controlled synthetic experiments, revealing trade-offs between recovery efficiency, robustness to system and user variables, and user workload. Finally, we deploy the framework on a robot-assisted bite acquisition system and demonstrate, in studies involving individuals with both emulated and real mobility limitations, that it improves recovery success while reducing the workload imposed on users. Our results highlight how explicitly reasoning about both robot uncertainty and human effort can enable more efficient and user-centered failure recovery in collaborative robots. Supplementary materials and videos can be found at: http://emprise.cs.cornell.edu/modularhil

</details>


### [139] [Solving Geodesic Equations with Composite Bernstein Polynomials for Trajectory Planning](https://arxiv.org/abs/2602.10365)
*Nick Gorman,Gage MacLin,Maxwell Hammond,Venanzio Cichella*

Main category: cs.RO

TL;DR: 本文提出了一种基于复合Bernstein多项式的轨迹规划方法，用于自主系统在复杂环境中的路径规划，实现了连续平滑且避障有效的轨迹。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中，自主系统（如机器人、航天器、无人机等）需要能灵活、安全、高效地穿越障碍物；现有方法或难以兼顾轨迹的平滑性与避障性能，或优化过程效率低下。因此，开发一种既能精准控制轨迹形状、保证光滑度又能高效优化的轨迹规划方法具有重要意义。

Method: 方法采用复合Bernstein多项式表述的路径，在符号化优化平台中进行优化。通过将障碍转化为连续的高代价区，用代价面驱动轨迹自动避障。同时，加入三类约束：障碍物的最小清距（高斯面不等式）、引导路径局部高效的测地线方程、起终点边界条件。符号求导提升了优化的精度与效率。

Result: 该方法在二维和三维环境下均适用，能生成连续、可执行、无碰撞的高效轨迹。实验证明，在多障碍场景下，比传统采样和后处理方法更高效地生成平滑且合理的避障轨迹，适用于地面、空中、水下及航天操作场景。

Conclusion: 所提方法能自主高效地生成平滑避障轨迹，具备良好的数值效率和可适应性，用于航天器、机器人等多种自主系统路径规划。可作为独立规划器或复杂运动规划的初始化器，有推广与实际应用价值。

Abstract: This work presents a trajectory planning method based on composite Bernstein polynomials for autonomous systems navigating complex environments. The method is implemented in a symbolic optimization framework that enables continuous paths and precise control over trajectory shape. Trajectories are planned over a cost surface that encodes obstacles as continuous fields rather than discrete boundaries. Regions near obstacles are assigned higher costs, naturally encouraging the trajectory to maintain a safe distance while still allowing efficient routing through constrained spaces. The use of composite Bernstein polynomials preserves continuity while enabling fine control over local curvature to satisfy geodesic constraints. The symbolic representation supports exact derivatives, improving optimization efficiency. The method applies to both two- and three-dimensional environments and is suitable for ground, aerial, underwater, and space systems. In spacecraft trajectory planning, for example, it enables the generation of continuous, dynamically feasible trajectories with high numerical efficiency, making it well suited for orbital maneuvers, rendezvous and proximity operations, cluttered gravitational environments, and planetary exploration missions with limited onboard computational resources. Demonstrations show that the approach efficiently generates smooth, collision-free paths in scenarios with multiple obstacles, maintaining clearance without extensive sampling or post-processing. The optimization incorporates three constraint types: (1) a Gaussian surface inequality enforcing minimum obstacle clearance; (2) geodesic equations guiding the path along locally efficient directions on the cost surface; and (3) boundary constraints enforcing fixed start and end conditions. The method can serve as a standalone planner or as an initializer for more complex motion planning problems.

</details>


### [140] [LocoVLM: Grounding Vision and Language for Adapting Versatile Legged Locomotion Policies](https://arxiv.org/abs/2602.10399)
*I Made Aswin Nahrendra,Seunghyun Lee,Dongkyu Lee,Hyun Myung*

Main category: cs.RO

TL;DR: 本文提出了一种将大模型高阶语义推理能力与腿式机器人运动控制相结合的方法，实现机器人对人类指令与环境语义的实时响应与自适应。


<details>
  <summary>Details</summary>
Motivation: 当前腿式机器人运动学习多依赖于几何信息建模，缺乏对高层语义（如人类指令）的理解与适应能力，这是实现智能人机交互和复杂场景自适应的主要瓶颈。

Method: 作者提出利用预训练的大型语言模型（LLM）生成面向指令的技能库，并结合预训练视觉-语言模型提取环境语义，将语义与技能库关联，从而指引机器人动作。同时训练一个风格调控策略网络，实现在不同风格/指令下多样化且鲁棒的运动控制。整个流程无需在线访问云端基础模型，可离线高效运行。

Result: 实验展示，该方法能实现腿式机器人在环境和指令变化下的实时动作生成，指令-响应准确率高达87%，证明了高阶语义推理对提升机器人适应性的有效性。

Conclusion: 该工作首次实现了基于高层次语义推理与人类指令的腿式机器人实时适应，对推进机器人更自然的智能交互和复杂环境自适应具有积极意义。

Abstract: Recent advances in legged locomotion learning are still dominated by the utilization of geometric representations of the environment, limiting the robot's capability to respond to higher-level semantics such as human instructions. To address this limitation, we propose a novel approach that integrates high-level commonsense reasoning from foundation models into the process of legged locomotion adaptation. Specifically, our method utilizes a pre-trained large language model to synthesize an instruction-grounded skill database tailored for legged robots. A pre-trained vision-language model is employed to extract high-level environmental semantics and ground them within the skill database, enabling real-time skill advisories for the robot. To facilitate versatile skill control, we train a style-conditioned policy capable of generating diverse and robust locomotion skills with high fidelity to specified styles. To the best of our knowledge, this is the first work to demonstrate real-time adaptation of legged locomotion using high-level reasoning from environmental semantics and instructions with instruction-following accuracy of up to 87% without the need for online query to on-the-cloud foundation models.

</details>


### [141] [Towards Long-Lived Robots: Continual Learning VLA Models via Reinforcement Fine-Tuning](https://arxiv.org/abs/2602.10503)
*Yuan Liu,Haoran Li,Shuai Tian,Yuxing Qin,Yuhui Chen,Yupeng Zheng,Yongzhen Huang,Dongbin Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的VLAs微调方法——LifeLong-RFT，能更高效适应新任务且表现优于传统SFT，尤其是在小样本、持续学习场景下。


<details>
  <summary>Details</summary>
Motivation: 当前通用机器人策略VLA模型虽然泛化能力强，但主流的有监督微调(SFT)需大量任务数据且易遗忘已学知识，限制了其在多任务和持续学习场景下的实际应用。

Method: 提出一种无须在线环境反馈与预训练奖励模型的新型策略微调方法——LifeLong-RFT。核心在于结合分块级的在策略强化学习和多维过程奖励机制（MDPR），从三维度量化中间动作片段的多样贡献，包括：1）动作一致性奖励（QACR）；2）轨迹对齐奖励（CTAR）；3）格式合规奖励（FCR），从而优化策略。

Result: 在SimperEnv、LIBERO以及实际任务中，LifeLong-RFT在多任务学习上取得了优异表现。在LIBERO基准上的持续学习实验中，平均成功率较SFT提升了22%，且新任务迁移仅需20%的训练数据。

Conclusion: LifeLong-RFT作为一种后训练范式，有效解决了SFT的局限性，展现出在多任务、持续学习和小样本适应中的巨大应用潜力。

Abstract: Pretrained on large-scale and diverse datasets, VLA models demonstrate strong generalization and adaptability as general-purpose robotic policies. However, Supervised Fine-Tuning (SFT), which serves as the primary mechanism for adapting VLAs to downstream domains, requires substantial amounts of task-specific data and is prone to catastrophic forgetting. To address these limitations, we propose LifeLong-RFT, a simple yet effective Reinforcement Fine-Tuning (RFT) strategy for VLA models independent of online environmental feedback and pre-trained reward models. By integrating chunking-level on-policy reinforcement learning with the proposed Multi-Dimensional Process Reward (MDPR) mechanism, LifeLong-RFT quantifies the heterogeneous contributions of intermediate action chunks across three dimensions to facilitate policy optimization. Specifically, (1) the Quantized Action Consistency Reward (QACR) ensures accurate action prediction within the discrete action space; (2) the Continuous Trajectory Alignment Reward (CTAR) aligns decoded continuous action chunks with reference trajectories to ensure precise control; (3) the Format Compliance Reward (FCR) guarantees the structural validity of outputs. Comprehensive experiments across SimplerEnv, LIBERO, and real-world tasks demonstrate that LifeLong-RFT exhibits strong performance in multi-task learning. Furthermore, for continual learning on the LIBERO benchmark, our method achieves a 22% gain in average success rate over SFT, while effectively adapting to new tasks using only 20% of the training data. Overall, our method provides a promising post-training paradigm for VLAs.

</details>


### [142] [Co-jump: Cooperative Jumping with Quadrupedal Robots via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.10514)
*Shihao Dong,Yeke Chen,Zeren Luo,Jiahui Zhang,Bowen Xu,Jinghan Lin,Yimin Han,Ji Ma,Zhiyou Yu,Yudong Zhao,Peng Lu*

Main category: cs.RO

TL;DR: 本文提出了Co-jump任务，通过两个四足机器人协作，实现远超单个机器人能力的同步跳跃。采用多智能体强化学习方法在无通信基础上完成高难度动作，并成功在实际硬件上验证。


<details>
  <summary>Details</summary>
Motivation: 单个四足机器人在运动能力上存在物理极限，难以突破。通过多机器人协作，有望实现超越单体的运动表现，拓展机器人应用边界。

Method: 采用分布式多智能体近端策略优化（MAPPO）算法，结合递进式课程学习策略，使两个机器人在没有显式通信和预定义运动模式的情况下，仅凭自身体感反馈同步完成高冲击力协作跳跃。从仿真到实际硬件迁移，有效解决稀疏奖励和机械耦合带来的探索难题。

Result: 在仿真和实际机器人实验中，系统能实现多方向、高度可达1.5米的平台跳跃，个别机器人足端抬升达到1.1米，较单机器人跃高提升144%。表现出极强的动作同步和突破性能力提升。

Conclusion: 通过无通信的协作学习方式，显著提升了多机器人协作行动的能力，为受限环境下无需通信的协同运动奠定了基础，并展示了机械协作在突破单机器人物理极限方面的巨大潜力。

Abstract: While single-agent legged locomotion has witnessed remarkable progress, individual robots remain fundamentally constrained by physical actuation limits. To transcend these boundaries, we introduce Co-jump, a cooperative task where two quadrupedal robots synchronize to execute jumps far beyond their solo capabilities. We tackle the high-impulse contact dynamics of this task under a decentralized setting, achieving synchronization without explicit communication or pre-specified motion primitives. Our framework leverages Multi-Agent Proximal Policy Optimization (MAPPO) enhanced by a progressive curriculum strategy, which effectively overcomes the sparse-reward exploration challenges inherent in mechanically coupled systems. We demonstrate robust performance in simulation and successful transfer to physical hardware, executing multi-directional jumps onto platforms up to 1.5 m in height. Specifically, one of the robots achieves a foot-end elevation of 1.1 m, which represents a 144% improvement over the 0.45 m jump height of a standalone quadrupedal robot, demonstrating superior vertical performance. Notably, this precise coordination is achieved solely through proprioceptive feedback, establishing a foundation for communication-free collaborative locomotion in constrained environments.

</details>


### [143] [ReSPEC: A Framework for Online Multispectral Sensor Reconfiguration in Dynamic Environments](https://arxiv.org/abs/2602.10547)
*Yanchen Liu,Yuang Fan,Minghui Zhao,Xiaofan Jiang*

Main category: cs.RO

TL;DR: 本论文提出了一种多传感器自适应融合框架，能够根据任务需求和环境变化动态调整各传感器的配置，从而提升资源利用效率，并在实际移动机器人上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前多传感器融合系统通常采用静态配置，所有传感器都以固定频率和质量采集数据，导致带宽、计算和能耗的浪费，且难以在光照不佳或遮挡等复杂环境下智能优化资源分配。为此，需要开发能够根据情境动态调整感知模式的系统。

Method: 该框架融合感知、学习与执行，形成闭环。具体流程为，利用任务导向的检测网络提取多光谱特征（如RGB、红外、毫米波、深度），并对每种模态输出量化的贡献分数。RL（强化学习）代理根据该分数动态调整各传感器的采样频率、分辨率和感知范围等参数，实现实时自适应。

Result: 在移动机器人上的实验结果表明，该框架在GPU负载降低29.3%的情况下，只带来了5.3%的准确率下降，明显优于基线启发式方法。

Conclusion: 该方法通过资源感知的自适应感知与决策，为嵌入式机器人平台带来了更高的能效和感知智能，有助提升机器人系统在实际复杂环境下的适应能力。

Abstract: Multi-sensor fusion is central to robust robotic perception, yet most existing systems operate under static sensor configurations, collecting all modalities at fixed rates and fidelity regardless of their situational utility. This rigidity wastes bandwidth, computation, and energy, and prevents systems from prioritizing sensors under challenging conditions such as poor lighting or occlusion. Recent advances in reinforcement learning (RL) and modality-aware fusion suggest the potential for adaptive perception, but prior efforts have largely focused on re-weighting features at inference time, ignoring the physical cost of sensor data collection. We introduce a framework that unifies sensing, learning, and actuation into a closed reconfiguration loop. A task-specific detection backbone extracts multispectral features (e.g. RGB, IR, mmWave, depth) and produces quantitative contribution scores for each modality. These scores are passed to an RL agent, which dynamically adjusts sensor configurations, including sampling frequency, resolution, sensing range, and etc., in real time. Less informative sensors are down-sampled or deactivated, while critical sensors are sampled at higher fidelity as environmental conditions evolve. We implement and evaluate this framework on a mobile rover, showing that adaptive control reduces GPU load by 29.3\% with only a 5.3\% accuracy drop compared to a heuristic baseline. These results highlight the potential of resource-aware adaptive sensing for embedded robotic platforms.

</details>


### [144] [LAP: Language-Action Pre-Training Enables Zero-shot Cross-Embodiment Transfer](https://arxiv.org/abs/2602.10556)
*Lihan Zha,Asher J. Hancock,Mingtong Zhang,Tenny Yin,Yixuan Huang,Dhruv Shah,Allen Z. Ren,Anirudha Majumdar*

Main category: cs.RO

TL;DR: 该论文提出了一种新的机器人通用策略预训练方法LAP，实现了在未见过的机器人硬件上零样本迁移，显著超越此前方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型（VLA）虽然可以进行多硬件预训练，但依然过度依赖训练时的机器人本体，且在迁移到新硬件时通常需要昂贵的微调。因此，亟需一种无需具体硬件适配、能实现真正泛化的迁移方法。

Method: 提出了Language-Action Pre-training (LAP)方法，将底层机器人动作直接用自然语言进行表达，使动作监督过程与大规模视觉-语言模型的输入输出分布对齐。LAP无需专门的分词器，无需额外昂贵的人工标注，也不需根据不同硬件调整模型结构。在此基础上，开发了LAP-3B模型。

Result: LAP-3B模型首次实现了视觉-语言-动作模型在此前未见过的机器人硬件上的大幅零样本迁移，无需针对不同硬件进行微调。在多个新机器人和操控任务上平均零样本成功率超过50%，比此前最强方法提升约2倍。同时，LAP还提升了适应效率和可扩展性，将动作预测与VQA通过共享格式联合训练获得了进一步提升。

Conclusion: LAP方法简化了视觉-语言-动作模型的迁移流程，使其具备了良好的零样本泛化能力，对推动机器人通用智能有重要意义。

Abstract: A long-standing goal in robotics is a generalist policy that can be deployed zero-shot on new robot embodiments without per-embodiment adaptation. Despite large-scale multi-embodiment pre-training, existing Vision-Language-Action models (VLAs) remain tightly coupled to their training embodiments and typically require costly fine-tuning. We introduce Language-Action Pre-training (LAP), a simple recipe that represents low-level robot actions directly in natural language, aligning action supervision with the pre-trained vision-language model's input-output distribution. LAP requires no learned tokenizer, no costly annotation, and no embodiment-specific architectural design. Based on LAP, we present LAP-3B, which to the best of our knowledge is the first VLA to achieve substantial zero-shot transfer to previously unseen robot embodiments without any embodiment-specific fine-tuning. Across multiple novel robots and manipulation tasks, LAP-3B attains over 50% average zero-shot success, delivering roughly a 2x improvement over the strongest prior VLAs. We further show that LAP enables efficient adaptation and favorable scaling, while unifying action prediction and VQA in a shared language-action format that yields additional gains through co-training.

</details>


### [145] [Morphogenetic Assembly and Adaptive Control for Heterogeneous Modular Robots](https://arxiv.org/abs/2602.10561)
*Chongxi Meng,Da Zhao,Yifei Zhao,Minghao Zeng,Yanmin Zhou,Zhipeng Wang,Bin He*

Main category: cs.RO

TL;DR: 本文提出了一个用于异构模块化机器人的闭环自动化框架，实现了从结构组装到自适应运动控制的全流程自动化。


<details>
  <summary>Details</summary>
Motivation: 当前异构模块化机器人在大规模重构和实时自适应控制方面面临状态空间爆炸、组装规划与控制难以协同等挑战。

Method: 本研究通过移动机械臂自动组装不同功能模块，提出了层次化双阶段规划：高层使用带类型惩罚项的双向启发式搜索进行模块处理序列规划，低层采用A*获得最优执行路径。针对新组装结构的自适应控制，引入了基于GPU的退火-方差MPPI控制器，通过多阶段方差退火策略实现兼顾全局探索和局部收敛，提升了不同结构下的实时控制能力。

Result: 大规模仿真验证表明，类型惩罚项提升了异构场景下的规划鲁棒性；贪婪启发策略相比匈牙利算法具有更低的物理执行代价。所提出的退火-方差MPPI在速度跟踪与控制频率上显著优于标准MPPI，实现了50Hz的实时控制。

Conclusion: 该自动化框架能够高效实现异构模块机器人的组装、合并、拆解及自适应动态运动控制，为模块化机器人场景提供了完整、实用的解决方案。

Abstract: This paper presents a closed-loop automation framework for heterogeneous modular robots, covering the full pipeline from morphological construction to adaptive control. In this framework, a mobile manipulator handles heterogeneous functional modules including structural, joint, and wheeled modules to dynamically assemble diverse robot configurations and provide them with immediate locomotion capability. To address the state-space explosion in large-scale heterogeneous reconfiguration, we propose a hierarchical planner: the high-level planner uses a bidirectional heuristic search with type-penalty terms to generate module-handling sequences, while the low level planner employs A* search to compute optimal execution trajectories. This design effectively decouples discrete configuration planning from continuous motion execution. For adaptive motion generation of unknown assembled configurations, we introduce a GPU accelerated Annealing-Variance Model Predictive Path Integral (MPPI) controller. By incorporating a multi stage variance annealing strategy to balance global exploration and local convergence, the controller enables configuration-agnostic, real-time motion control. Large scale simulations show that the type-penalty term is critical for planning robustness in heterogeneous scenarios. Moreover, the greedy heuristic produces plans with lower physical execution costs than the Hungarian heuristic. The proposed annealing-variance MPPI significantly outperforms standard MPPI in both velocity tracking accuracy and control frequency, achieving real time control at 50 Hz. The framework validates the full-cycle process, including module assembly, robot merging and splitting, and dynamic motion generation.

</details>


### [146] [Flow-Enabled Generalization to Human Demonstrations in Few-Shot Imitation Learning](https://arxiv.org/abs/2602.10594)
*Runze Tang,Penny Sweetser*

Main category: cs.RO

TL;DR: 本文提出了一种新方法（SFCrP），改进了从人类视频中模仿学习复杂机器人技能的能力，大幅减少了机器人示范的需求，并提升了泛化性和精度。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习依赖大量机器人演示，收集成本高昂。之前的方法试图利用人类视频减少机器人演示需求，但现有'flow'（流动）表示方法难以同时描述交互动作和高精度运动细节，并且泛化能力有限。

Method: 提出SFCrP，包括两个部分：1）跨体现Scene Flow预测模型（SFCr），从人类和机器人视频学习，能预测任意点轨迹；2）基于流动和裁剪点云的策略网络（FCrP），根据通用流动引导并结合场景观察进行精细动作调整。

Result: 实验表明，该方法在多种真实任务中优于现有方法，并在仅通过人类视频观察到的场景中展现出很强的空间和实例泛化能力。

Conclusion: SFCrP有效解决了人类视频映射到机器人技能学习中的交互与泛化难题，在减少演示需求同时提升了模仿学习的效果。

Abstract: Imitation Learning (IL) enables robots to learn complex skills from demonstrations without explicit task modeling, but it typically requires large amounts of demonstrations, creating significant collection costs. Prior work has investigated using flow as an intermediate representation to enable the use of human videos as a substitute, thereby reducing the amount of required robot demonstrations. However, most prior work has focused on the flow, either on the object or on specific points of the robot/hand, which cannot describe the motion of interaction. Meanwhile, relying on flow to achieve generalization to scenarios observed only in human videos remains limited, as flow alone cannot capture precise motion details. Furthermore, conditioning on scene observation to produce precise actions may cause the flow-conditioned policy to overfit to training tasks and weaken the generalization indicated by the flow. To address these gaps, we propose SFCrP, which includes a Scene Flow prediction model for Cross-embodiment learning (SFCr) and a Flow and Cropped point cloud conditioned Policy (FCrP). SFCr learns from both robot and human videos and predicts any point trajectories. FCrP follows the general flow motion and adjusts the action based on observations for precision tasks. Our method outperforms SOTA baselines across various real-world task settings, while also exhibiting strong spatial and instance generalization to scenarios seen only in human videos.

</details>


### [147] [Pitch Angle Control of a Magnetically Actuated Capsule Robot with Nonlinear FEA-based MPC and EKF Multisensory Fusion](https://arxiv.org/abs/2602.10610)
*Chongxun Wang,Zikang Shen,Apoorav Rathore,Akanimoh Udombeh,Harrison Teng,Fangzhou Xia*

Main category: cs.RO

TL;DR: 本论文提出了一种基于非线性模型的磁控胶囊机器人俯仰角控制方法，实现了在类胃环境下快速、稳定地调整姿态。


<details>
  <summary>Details</summary>
Motivation: 当前磁控胶囊机器人多用于消化道的微创诊断与治疗，但普遍缺乏对俯仰角（pitch）的有效控制能力，而俯仰控制对于与胃壁等倾斜表面的接触操作具有重要意义。

Method: 作者开发了四线圈电磁阵列驱动系统，通过三维有限元仿真得到角度相关的磁力与力矩，并通过查找表嵌入刚体动力学模型中。基于该模型，设计受限模型预测控制器（MPC），在满足硬件电流和变化速率约束下调节俯仰角。同时，应用扩展卡尔曼滤波器（EKF）融合惯性与视觉传感信息，提高弱成像条件下的闭环控制稳定性。

Result: 在仿胃柔顺表面实验中，该方法实现了从水平与竖直姿态的快速俯仰角调整，比传统的开关控制快3到5倍且振荡更小。EKF使系统在视觉更新率由30Hz降至1Hz时依然保持稳定的闭环控制。

Conclusion: 有限元辅助的模型预测控制结合传感器融合是一种可扩展的胶囊机器人姿态调节策略，可支持精确吸附和多自由度运动，将推动胶囊机器人在消化道内更复杂任务的实现。

Abstract: Magnetically actuated capsule robots promise minimally invasive diagnosis and therapy in the gastrointestinal (GI) tract, but existing systems largely neglect control of capsule pitch, a degree of freedom critical for contact-rich interaction with inclined gastric walls. This paper presents a nonlinear, model-based framework for magnetic pitch control of an ingestible capsule robot actuated by a four-coil electromagnetic array. Angle-dependent magnetic forces and torques acting on embedded permanent magnets are characterized using three-dimensional finite-element simulations and embedded as lookup tables in a control-oriented rigid-body pitching model with rolling contact and actuator dynamics. A constrained model predictive controller (MPC) is designed to regulate pitch while respecting hardware-imposed current and slew-rate limits. Experiments on a compliant stomach-inspired surface demonstrate robust pitch reorientation from both horizontal and upright configurations, achieving about three to five times faster settling and reduced oscillatory motion than on-off control. Furthermore, an extended Kalman filter (EKF) fusing inertial sensing with intermittent visual measurements enables stable closed-loop control when the camera update rate is reduced from 30 Hz to 1 Hz, emulating clinically realistic imaging constraints. These results establish finite-element-informed MPC with sensor fusion as a scalable strategy for pitch regulation, controlled docking, and future multi-degree-of-freedom capsule locomotion.

</details>


### [148] [Free-Flying Crew Cooperative Robots on the ISS: A Joint Review of Astrobee, CIMON, and Int-Ball Operations](https://arxiv.org/abs/2602.10686)
*Seiko Piotr Yamaguchi,Andres Mora Vargas,Till Eisenberg,Christian Rogon,Tatsuya Yamamoto,Shona Inoue,Christoph Kössl,Brian Coltin,Trey Smith,Jose V. Benavides*

Main category: cs.RO

TL;DR: 本文首次联合分析了国际空间站上三种典型舱内自由飞行机器人Astrobee、CIMON和Int-Ball的设计与运行经验，总结了开发过程中遇到的共同问题和经验教训，并为未来相关机器人的设计提供了建议。


<details>
  <summary>Details</summary>
Motivation: 虽然Astrobee、CIMON和Int-Ball出自不同国家和团队，设计理念各异，但它们都致力于支持载人航天工作，同时并肩协作于国际空间站内。这些平台的联合分析旨在总结其共同的开发与运行经验，为后续技术研发提供宝贵经验。

Method: 由各机器人开发及运营团队成员共同撰写，系统性梳理比较了三种机器人在国际空间站的设计目标、设计方案及舱内运行情况，并对整个全生命周期（从设计到在轨运行）过程中积累的经验教训进行了归纳总结。

Result: 分析发现，尽管三平台设计初衷和实现手段各具特点，但在实际开发和运营过程中存在许多相似挑战和经验，形成了若干可供借鉴的设计和操作建议。

Conclusion: 这项联合分析为未来舱内自由飞行机器人系统的开发和研究提供了系统化的经验总结和设计建议，有助于提升后续机器人的性能与可靠性。

Abstract: Intra-vehicular free-flying robots are anticipated to support various work in human spaceflight while working side-by-side with astronauts. Such example of robots includes NASA's Astrobee, DLR's CIMON, and JAXA's Int-Ball, which are deployed on the International Space Station. This paper presents the first joint analyses of these robot's shared experiences, co-authored by their development and operation team members. Despite the different origins and design philosophies, the development and operations of these platforms encountered various convergences. Hence, this paper presents a detailed overview of these robots, presenting their objectives, design, and onboard operations. Hence, joint lessons learned across the lifecycle are presented, from design to on-orbit operations. These lessons learned are anticipated to serve for future development and research as design recommendations.

</details>


### [149] [3D-Printed Anisotropic Soft Magnetic Coating for Directional Rolling of a Magnetically Actuated Capsule Robot](https://arxiv.org/abs/2602.10688)
*Jin Zhou,Chongxun Wang,Zikang Shen,Fangzhou Xia*

Main category: cs.RO

TL;DR: 该论文提出了一种采用磁性涂层的3D打印软体胶囊机器人，取代传统内置磁体设计，从而增大内部空间并提升功能集成能力。实验表明该系统实现了多向运动、越障等关键性能，为微创医疗应用提供更优方案。


<details>
  <summary>Details</summary>
Motivation: 现有磁控胶囊机器人多采用体积庞大的内置永磁体，导致内部可用空间减少，并限制了功能性模块的集成，难以满足日益复杂的医疗需求。

Method: 作者采用3D打印工艺制造硅胶-磁性复合材料软体胶囊，在胶囊外壳表面进行磁性涂层，并通过磁极分布优化实现运动控制。进行了磁静力学仿真和实验，评估其运动能力和磁场驱动性能。

Result: 新型胶囊机器人实现了稳定双向滚动、全向转向、攀爬7.5°斜坡和越过5mm障碍等功能。在实验平台下，滚动运动可在胶囊处磁场大于0.3 mT时持续，最大驱动深度达30 mm。

Conclusion: 所提磁涂层软体胶囊不仅显著提高了内部空间利用率，还提升了吞咽舒适性和运动性能，为临床微创诊疗工具发展奠定了基础。未来将优化材料参数并开发闭环磁场控制系统以提高实用性和可操控性。

Abstract: Capsule robots are promising tools for minimally invasive diagnostics and therapy, with applications from gastrointestinal endoscopy to targeted drug delivery and biopsy sampling. Conventional magnetic capsule robots embed bulky permanent magnets at both ends, reducing the usable cavity by about 10-20 mm and limiting integration of functional modules. We propose a compact, 3D-printed soft capsule robot with a magnetic coating that replaces internal magnets, enabling locomotion via a thin, functional shell while preserving the entire interior cavity as a continuous volume for medical payloads. The compliant silicone-magnetic composite also improves swallowability, even with a slightly larger capsule size. Magnetostatic simulations and experiments confirm that programmed NSSN/SNNS pole distributions provide strong anisotropy and reliable torque generation, enabling stable bidirectional rolling, omnidirectional steering, climbing on 7.5 degree inclines, and traversal of 5 mm protrusions. Rolling motion is sustained when the magnetic field at the capsule reaches at least 0.3 mT, corresponding to an effective actuation depth of 30 mm in our setup. Future work will optimize material composition, coating thickness, and magnetic layouts to enhance force output and durability, while next-generation robotic-arm-based field generators with closed-loop feedback will address nonlinearities and expand maneuverability. Together, these advances aim to transition coating-based capsule robots toward reliable clinical deployment and broaden their applications in minimally invasive diagnostics and therapy.

</details>


### [150] [A Unified Experimental Architecture for Informative Path Planning: from Simulation to Deployment with GuadalPlanner](https://arxiv.org/abs/2602.10702)
*Alejandro Mendoza Barrionuevo,Dame Seck Diop,Alejandro Casado Pérez,Daniel Gutiérrez Reina,Sergio L. Toral Marín,Samuel Yanes Luis*

Main category: cs.RO

TL;DR: 提出了一种新的统一架构GuadalPlanner，使自主车辆的信息化路径规划算法在仿真和现实部署间能无缝转移和一致评估。


<details>
  <summary>Details</summary>
Motivation: 现有路径规划算法的评估受到执行流程碎片化和仿真到现实转移受限的影响，难以实现算法的一致测试和应用。

Method: 设计了GuadalPlanner框架，采用标准化接口分离高层决策与低层控制，并集成ROS2、MAVLink、MQTT等技术，支持不同环境和可替换的路径规划策略，实现了算法在仿真、软件环和实物部署中的同一执行流程。

Result: 通过实验验证了该架构，包括将其部署到自主水面船只进行水质监测，利用实时传感器反馈展示了架构的实用性和一致性。

Conclusion: GuadalPlanner为信息化路径规划算法的开发、评估和现实应用提供了统一、开放和可扩展的平台，提升了仿真与现实部署的一致性和效率。

Abstract: The evaluation of informative path planning algorithms for autonomous vehicles is often hindered by fragmented execution pipelines and limited transferability between simulation and real-world deployment. This paper introduces a unified architecture that decouples high-level decision-making from vehicle-specific control, enabling algorithms to be evaluated consistently across different abstraction levels without modification. The proposed architecture is realized through GuadalPlanner, which defines standardized interfaces between planning, sensing, and vehicle execution. It is an open and extensible research tool that supports discrete graph-based environments and interchangeable planning strategies, and is built upon widely adopted robotics technologies, including ROS2, MAVLink, and MQTT. Its design allows the same algorithmic logic to be deployed in fully simulated environments, software-in-the-loop configurations, and physical autonomous vehicles using an identical execution pipeline. The approach is validated through a set of experiments, including real-world deployment on an autonomous surface vehicle performing water quality monitoring with real-time sensor feedback.

</details>


### [151] [Omnidirectional Dual-Arm Aerial Manipulator with Proprioceptive Contact Localization for Landing on Slanted Roofs](https://arxiv.org/abs/2602.10703)
*Martijn B. J. Brummelhuis,Nathan F. Lepora,Salua Hamaza*

Main category: cs.RO

TL;DR: 本论文提出了一种创新的无人机系统，利用自身机械臂与地面接触检测屋顶倾斜角，实现高精度安全降落。实验表明该方法在不同倾斜表面均能稳定着陆，倾角估算误差仅为2.87度。


<details>
  <summary>Details</summary>
Motivation: 城市环境中无人机降落常需依托屋顶，但屋顶形状与表面材料多变，传统视觉或声学方法在倾角检测上受环境影响大，可靠性不足，因此亟需一种不依赖外界条件即可准确检测表面倾斜度的方法。

Method: 作者设计了一种具备全向3D工作空间和扩展工作距离的双臂无人机（UAM），并提出基于自身动力-力矩观测器的本体感知式接触检测与定位策略，使无人机可以通过物理接触感知表面倾斜度。

Result: 通过实飞实验，验证所提方法能实现在最高30.5度倾斜表面上安全着陆，且在9次不同角度的实验中，表面倾角估算的平均误差为2.87度，表现出很强的鲁棒性和准确性。

Conclusion: 新型双臂无人机系统结合本体感知方法，显著提升了在不良环境下的屋顶着陆能力，能适应不同倾斜表面，实现准确着陆检测和安全降落，为城市无人机应用提供了更为可靠的技术手段。

Abstract: Operating drones in urban environments often means they need to land on rooftops, which can have different geometries and surface irregularities. Accurately detecting roof inclination using conventional sensing methods, such as vision-based or acoustic techniques, can be unreliable, as measurement quality is strongly influenced by external factors including weather conditions and surface materials. To overcome these challenges, we propose a novel unmanned aerial manipulator morphology featuring a dual-arm aerial manipulator with an omnidirectional 3D workspace and extended reach. Building on this design, we develop a proprioceptive contact detection and contact localization strategy based on a momentum-based torque observer. This enables the UAM to infer the inclination of slanted surfaces blindly - through physical interaction - prior to touchdown. We validate the approach in flight experiments, demonstrating robust landings on surfaces with inclinations of up to 30.5 degrees and achieving an average surface inclination estimation error of 2.87 degrees over 9 experiments at different incline angles.

</details>


### [152] [Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation](https://arxiv.org/abs/2602.10717)
*Songen Gu,Yunuo Cai,Tianyu Wang,Simo Wu,Yanwei Fu*

Main category: cs.RO

TL;DR: 该论文提出了一种用于机器人操作的快速预测性视频条件行动框架，并在实验中显著提升了操作的准确性和任务完成度。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作系统缺乏预测环境随动作变化的能力，导致误差和效率低下。尽管理解视觉-语言模型可以提供高层指导，但无法显式预测未来状态，而传统世界模型只能短期预测或生成空间不一致的画面，不能满足实际需要。

Method: 论文方法包括：1）选择并适配高度鲁棒的视频生成模型以获得可靠的未来预测；2）采用对抗蒸馏方法，实现快速、少步的视频生成；3）训练行动模型，结合生成视频和真实观测，纠正空间错误。

Result: 大量实验表明，该方法生成的视频在时间连贯性和空间准确性上均优于现有方法，能直接支持精确操作，在化身一致性、空间指涉能力和任务完成率等指标上大幅超越基线。

Conclusion: 本文方法能显著提升机器人对未来状态的预测和操作能力，为视频驱动的机器人操作提供了有效的框架。相关代码和模型将会开放。

Abstract: Robotic manipulation requires anticipating how the environment evolves in response to actions, yet most existing systems lack this predictive capability, often resulting in errors and inefficiency. While Vision-Language Models (VLMs) provide high-level guidance, they cannot explicitly forecast future states, and existing world models either predict only short horizons or produce spatially inconsistent frames. To address these challenges, we propose a framework for fast and predictive video-conditioned action. Our approach first selects and adapts a robust video generation model to ensure reliable future predictions, then applies adversarial distillation for fast, few-step video generation, and finally trains an action model that leverages both generated videos and real observations to correct spatial errors. Extensive experiments show that our method produces temporally coherent, spatially accurate video predictions that directly support precise manipulation, achieving significant improvements in embodiment consistency, spatial referring ability, and task completion over existing baselines. Codes & Models will be released.

</details>


### [153] [From Representational Complementarity to Dual Systems: Synergizing VLM and Vision-Only Backbones for End-to-End Driving](https://arxiv.org/abs/2602.10719)
*Sining Ang,Yuguang Yang,Chenxu Dang,Canyu Chen,Cheng Chi,Haiyan Liu,Xuanyao Mao,Jason Bao,Xuliang,Bingchuan Sun,Yan Wang*

Main category: cs.RO

TL;DR: 本文比较了带有语言能力和仅使用视觉的驾驶模型，分析了它们在不同行为和性能上的差异，并提出了混合和高效的决策方案以兼顾精度和速度。


<details>
  <summary>Details</summary>
Motivation: 目前端到端自动驾驶模型在引入视觉-语言模型（VLM）后，其带来的变化尚不明确，尤其是在行为决策和性能权衡方面。因此，本文旨在系统分析使用VLM与仅视觉模型的不同影响。

Method: 作者在统一的Diffusion Transformer规划器下，分别引入全VLM和仅视觉ViT骨干网络，通过三个研究问题（RQ）逐步分析两个系统在表示空间、行为差异，以及如何融合两者优势（包括提出HybridDriveVLA和DualDriveVLA混合策略）。

Result: 实验发现：VLM引入了新的子空间，导致在某些追尾或极端场景下具备更激进策略，而ViT更为保守。通过理想选择器可达93.58 PDMS。HybridDriveVLA利用学习评分器选择最优分支，提升到92.10 PDMS。DualDriveVLA只在评分不高时调用VLM，15%场景下可保证91.00 PDMS且吞吐量提升3.2倍。

Conclusion: 引入多模态（VLM+视觉）结构在少数场景下大幅提升决策表现，通过分支择优和高效策略可兼顾准确率与推理速度，为E2E自动驾驶模型的实用化提供了新思路和方法。

Abstract: Vision-Language-Action (VLA) driving augments end-to-end (E2E) planning with language-enabled backbones, yet it remains unclear what changes beyond the usual accuracy--cost trade-off. We revisit this question with 3--RQ analysis in RecogDrive by instantiating the system with a full VLM and vision-only backbones, all under an identical diffusion Transformer planner. RQ1: At the backbone level, the VLM can introduce additional subspaces upon the vision-only backbones. RQ2: This unique subspace leads to a different behavioral in some long-tail scenario: the VLM tends to be more aggressive whereas ViT is more conservative, and each decisively wins on about 2--3% of test scenarios; With an oracle that selects, per scenario, the better trajectory between the VLM and ViT branches, we obtain an upper bound of 93.58 PDMS. RQ3: To fully harness this observation, we propose HybridDriveVLA, which runs both ViT and VLM branches and selects between their endpoint trajectories using a learned scorer, improving PDMS to 92.10. Finally, DualDriveVLA implements a practical fast--slow policy: it runs ViT by default and invokes the VLM only when the scorer's confidence falls below a threshold; calling the VLM on 15% of scenarios achieves 91.00 PDMS while improving throughput by 3.2x. Code will be released.

</details>


### [154] [Biomimetic Mantaray robot toward the underwater autonomous -- Experimental verification of swimming and diving by flapping motion -](https://arxiv.org/abs/2602.10904)
*Kenta Tabata,Ryosuke Oku,Jun Ito,Renato Miyagusuku,Koichi Ozaki*

Main category: cs.RO

TL;DR: 本研究开发并验证了一款仿生蝠鲼水下机器人，采用扑动推进方式，实验结果显示其在游泳与下潜控制方面表现稳定，适用于需低干扰和高机动性的水下探索场景。


<details>
  <summary>Details</summary>
Motivation: 传统螺旋桨推进的水下机器人容易干扰海底环境，效率较低，因此需要新型推进方式以提升探索效率并减少对生态环境的影响。

Method: 机器人仿效蝠鲼扑动推进，利用舵机驱动胸鳍，控制箱流线型设计减小阻力，控制系统采用Raspberry Pi 3B，配备IMU及压力传感器。实验在泳池内测试其游泳和下潜能力，采用PD控制进行运动控制。

Result: 实验结果表明，机器人能实现稳定的游泳与下潜动作，控制效果良好。

Conclusion: 仿生设计的水下机器人具备高效推进与低环境干扰优势，适用于生态监测和水下探索等应用场景，展示了生物启发机器人在实际应用中的潜力。

Abstract: This study presents the development and experimental verification of a biomimetic manta ray robot for underwater autonomous exploration. Inspired by manta rays, the robot uses flapping motion for propulsion to minimize seabed disturbance and enhance efficiency compared to traditional screw propulsion. The robot features pectoral fins driven by servo motors and a streamlined control box to reduce fluid resistance. The control system, powered by a Raspberry Pi 3B, includes an IMU and pressure sensor for real-time monitoring and control. Experiments in a pool assessed the robot's swimming and diving capabilities. Results show stable swimming and diving motions with PD control. The robot is suitable for applications in environments like aquariums and fish nurseries, requiring minimal disturbance and efficient maneuverability. Our findings demonstrate the potential of bio-inspired robotic designs to improve ecological monitoring and underwater exploration.

</details>


### [155] [Safe mobility support system using crowd mapping and avoidance route planning using VLM](https://arxiv.org/abs/2602.10910)
*Sena Saito,Kenta Tabata,Renato Miyagusuku,Koichi Ozaki*

Main category: cs.RO

TL;DR: 本文提出了一种结合视觉-语言模型（VLM）与高斯过程回归（GPR）的框架，用于生成动态的拥挤度地图，从而提升自主移动机器人在人群密集环境中的安全高效导航能力。


<details>
  <summary>Details</summary>
Motivation: 自主移动机器人在动态、特别是拥挤环境中安全有效导航仍具挑战。传统方法难以精准识别人群动态及环境抽象特征，因此亟需新方法提高机器人在复杂动态场景中的适应性和安全性。

Method: 方法上，将VLM用于识别环境中如人群拥挤度等抽象概念，再通过GPR进行概率建模，最终生成动态拥挤度‘抽象地图’，辅助导航规划。

Result: 在真实校园环境试验中，机器人成功规避静态障碍物和动态人流，实现了更安全和适应性更强的导航。实验显示该方法在动态复杂环境下表现优异。

Conclusion: 结合VLM和GPR能有效提升机器人在动态、拥挤环境中的路径规划能力，为未来机器人自主导航系统的发展开辟了新方向。

Abstract: Autonomous mobile robots offer promising solutions for labor shortages and increased operational efficiency. However, navigating safely and effectively in dynamic environments, particularly crowded areas, remains challenging. This paper proposes a novel framework that integrates Vision-Language Models (VLM) and Gaussian Process Regression (GPR) to generate dynamic crowd-density maps (``Abstraction Maps'') for autonomous robot navigation. Our approach utilizes VLM's capability to recognize abstract environmental concepts, such as crowd densities, and represents them probabilistically via GPR. Experimental results from real-world trials on a university campus demonstrated that robots successfully generated routes avoiding both static obstacles and dynamic crowds, enhancing navigation safety and adaptability.

</details>


### [156] [Design, Development, and Use of Maya Robot as an Assistant for the Therapy/Education of Children with Cancer: a Pilot Study](https://arxiv.org/abs/2602.10942)
*Alireza Taheri,Minoo Alemi,Elham Ranjkar,Raman Rafatnejad,Ali F. Meghdari*

Main category: cs.RO

TL;DR: 本研究设计并实现了一种名为Maya的便携式大象形社交机器人，旨在与接受癌症治疗的儿童互动。实验显示，机器人能提升患儿信任感、减少注射痛感和焦虑情绪。


<details>
  <summary>Details</summary>
Motivation: 儿童在接受癌症治疗过程中常常感到疼痛和焦虑，缺乏有效的情绪支持工具。社交机器人有潜力通过互动帮助改善这一状况。

Method: 首先通过深度神经网络提升机器人面部表情识别准确率至98%。随后开展两项初步探索性实验：一是比较有无机器人陪伴下儿童注射疼痛感，二是通过问卷调查分析儿童及其母亲与机器人互动及游戏后的感受。

Result: 实验结果表明，机器人陪伴显著减轻了患儿的注射疼痛感。互动及游戏过程中，儿童的焦虑显著低于父母，且对机器人及其游戏的信任度高于父母（P < 0.05）。

Conclusion: Maya社交机器人作为 临床辅助工具可有效改善接受癌症治疗儿童的疼痛管理和情感健康，凸显了社交机器人在儿科医疗中的应用潜力。

Abstract: This study centers around the design and implementation of the Maya Robot, a portable elephant-shaped social robot, intended to engage with children undergoing cancer treatment. Initial efforts were devoted to enhancing the robot's facial expression recognition accuracy, achieving a 98% accuracy through deep neural networks. Two subsequent preliminary exploratory experiments were designed to advance the study's objectives. The first experiment aimed to compare pain levels experienced by children during the injection process, with and without the presence of the Maya robot. Twenty-five children, aged 4 to 9, undergoing cancer treatment participated in this counterbalanced study. The paired T-test results revealed a significant reduction in perceived pain when the robot was actively present in the injection room. The second experiment sought to assess perspectives of hospitalized children and their mothers during engagement with Maya through a game. Forty participants, including 20 children aged 4 to 9 and their mothers, were involved. Post Human-Maya Interactions, UTAUT questionnaire results indicated that children experienced significantly less anxiety than their parents during the interaction and game play. Notably, children exhibited higher trust levels in both the robot and the games, presenting a statistically significant difference in trust levels compared to their parents (P-value < 0.05). This preliminary exploratory study highlights the positive impact of utilizing Maya as an assistant for therapy/education in a clinical setting, particularly benefiting children undergoing cancer treatment. The findings underscore the potential of social robots in pediatric healthcare contexts, emphasizing improved pain management and emotional well-being among young patients.

</details>


### [157] [Developing Neural Network-Based Gaze Control Systems for Social Robots](https://arxiv.org/abs/2602.10946)
*Ramtin Tabatabaei,Alireza Taheri*

Main category: cs.RO

TL;DR: 本研究利用深度学习模型分析和预测人在多方社交场景中的注视行为，并将最佳模型应用于社交机器人，取得了较好的预测准确率和用户反馈。


<details>
  <summary>Details</summary>
Motivation: 在多方互动中，注视方向反映了兴趣和意图，对于社交机器人来说，能否正确理解和应对注视行为对有效社交至关重要。因此，研究人类注视行为的时序模式，有助于提升社交机器人在人机互动中的表现。

Method: 研究团队设计了两种不同的社交场景视频（2D与3D），并分别让30名实验参与者采用眼动仪和VR头显采集注视数据。随后，采用LSTM与Transformer深度学习模型对这些注视数据进行分析和预测。

Result: 在预测注视方向任务中，模型在2D动画中的准确率为60%，在3D动画中的准确率为65%。最终，将表现最佳的模型部署到Nao机器人身上，并通过36名新参与者进行互动评估。

Conclusion: 深度学习模型能有效预测多方社交场景中的人类注视行为，且移植到实际机器人平台后得到用户整体满意，尤其是机器人经验丰富的用户反馈更好。

Abstract: During multi-party interactions, gaze direction is a key indicator of interest and intent, making it essential for social robots to direct their attention appropriately. Understanding the social context is crucial for robots to engage effectively, predict human intentions, and navigate interactions smoothly. This study aims to develop an empirical motion-time pattern for human gaze behavior in various social situations (e.g., entering, leaving, waving, talking, and pointing) using deep neural networks based on participants' data. We created two video clips-one for a computer screen and another for a virtual reality headset-depicting different social scenarios. Data were collected from 30 participants: 15 using an eye-tracker and 15 using an Oculus Quest 1 headset. Deep learning models, specifically Long Short-Term Memory (LSTM) and Transformers, were used to analyze and predict gaze patterns. Our models achieved 60% accuracy in predicting gaze direction in a 2D animation and 65% accuracy in a 3D animation. Then, the best model was implemented onto the Nao robot; and 36 new participants evaluated its performance. The feedback indicated overall satisfaction, with those experienced in robotics rating the models more favorably.

</details>


### [158] [Stability Analysis of Geometric Control for a Canonical Class of Underactuated Aerial Vehicles with Spurious Forces](https://arxiv.org/abs/2602.10961)
*Simone Orelli,Mirko Mizzoni,Antonio Franchi*

Main category: cs.RO

TL;DR: 本文针对传统几何控制在存在杂散力情况下失效的问题，提出了利用Lyapunov方法对带有耦合力的浮动刚体进行稳定性分析，并首次给出了理论上的稳定性证明。


<details>
  <summary>Details</summary>
Motivation: 许多空中机器人在受控力矩作用下会自然产生杂散力，导致常用的力-力矩解耦假设失效。目前虽有针对这种耦合系统的策略实现，但缺乏对其稳定性的严格理论证明。

Method: 作者构建了一个通用的浮动刚体模型，考虑了因控制力矩引入的杂散力，并基于Lyapunov方法，对其悬停稳定性进行了形式化分析，证明了其局部指数稳定性。同时，针对因杂散力耦合导致的非最小相位行为，提出了新的分析方法，克服了标准级联系统分析不能应用的难题。

Result: 给出了泛化浮动刚体系统的稳定性形式证明，明确说明了悬停状态在杂散力存在下依然可实现局部指数稳定。

Conclusion: 该工作首次理论上证明了存在杂散力情况下，相关系统在新提出的模型与控制条件下的局部悬停稳定性，为实际飞行器的耦合力控制提供了理论基础和分析工具。

Abstract: Standard geometric control relies on force-moment decoupling, an assumption that breaks down in many aerial platforms due to spurious forces naturally induced by control moments. While strategies for such coupled systems have been validated experimentally, a rigorous theoretical certification of their stability is currently missing. This work fills this gap by providing the first formal stability analysis for a generic class of floating rigid bodies subject to spurious forces. We introduce a canonical model and construct a Lyapunov-based proof establishing local exponential stability of the hovering equilibrium. Crucially, the analysis explicitly addresses the structural challenges - specifically the induced non-minimum-phase behavior - that prevent the application of standard cascade arguments.

</details>


### [159] [RADAR: Benchmarking Vision-Language-Action Generalization via Real-World Dynamics, Spatial-Physical Intelligence, and Autonomous Evaluation](https://arxiv.org/abs/2602.10980)
*Yuhao Chen,Zhihao Zhan,Xiaoxin Lin,Zijian Song,Hao Liu,Qinhan Lyu,Yubo Zu,Xiao Chen,Zhiyuan Liu,Tao Pu,Tianshui Chen,Keze Wang,Liang Lin,Guangrun Wang*

Main category: cs.RO

TL;DR: 该论文指出当前VLA模型的评测大多局限于仿真或受限的现实环境，存在显著的'现实差距'，致使模型在实际中表现不佳。作者提出了新的、更加真实的评测基准RADAR，并展示现有VLA模型在该基准下表现脆弱。


<details>
  <summary>Details</summary>
Motivation: 现有VLA（Vision-Language-Action）模型通常只在仿真或极度受限的现实环境下进行评测，导致模型在多样化现实环境中泛化能力较差。过往评测忽视了现实物理动态、空间-物理智能和可扩展的全自动评测，这些问题妨碍了模型能力的真实、公正比较。

Method: 作者提出了RADAR评测基准，包括三大核心：一是系统化的物理动态测试，二是考查空间推理和物理理解的专门任务，三是基于3D指标且无需人工监督的全自动评测流程。作者利用RADAR评测多个现有顶尖VLA模型，分析其泛化和空间推理能力。

Result: 在RADAR基准下，主流VLA模型在引入轻微物理动态（如传感器噪声）后表现大幅下降，3D IoU分数由0.261骤降至0.068，而且空间推理能力有限。

Conclusion: RADAR揭示当前VLA模型在复杂真实环境中的显著脆弱性，为该领域提供了一个更可靠、公正、可扩展的现实世界评估基准。

Abstract: VLA models have achieved remarkable progress in embodied intelligence; however, their evaluation remains largely confined to simulations or highly constrained real-world settings. This mismatch creates a substantial reality gap, where strong benchmark performance often masks poor generalization in diverse physical environments. We identify three systemic shortcomings in current benchmarking practices that hinder fair and reliable model comparison. (1) Existing benchmarks fail to model real-world dynamics, overlooking critical factors such as dynamic object configurations, robot initial states, lighting changes, and sensor noise. (2) Current protocols neglect spatial--physical intelligence, reducing evaluation to rote manipulation tasks that do not probe geometric reasoning. (3) The field lacks scalable fully autonomous evaluation, instead relying on simplistic 2D metrics that miss 3D spatial structure or on human-in-the-loop systems that are costly, biased, and unscalable. To address these limitations, we introduce RADAR (Real-world Autonomous Dynamics And Reasoning), a benchmark designed to systematically evaluate VLA generalization under realistic conditions. RADAR integrates three core components: (1) a principled suite of physical dynamics; (2) dedicated tasks that explicitly test spatial reasoning and physical understanding; and (3) a fully autonomous evaluation pipeline based on 3D metrics, eliminating the need for human supervision. We apply RADAR to audit multiple state-of-the-art VLA models and uncover severe fragility beneath their apparent competence. Performance drops precipitously under modest physical dynamics, with the expectation of 3D IoU declining from 0.261 to 0.068 under sensor noise. Moreover, models exhibit limited spatial reasoning capability. These findings position RADAR as a necessary bench toward reliable and generalizable real-world evaluation of VLA models.

</details>


### [160] [Scaling World Model for Hierarchical Manipulation Policies](https://arxiv.org/abs/2602.10983)
*Qian Long,Yueze Wang,Jiaxi Song,Junbo Zhang,Peiyan Li,Wenxuan Wang,Yuqi Wang,Haoyang Li,Shaoxuan Xie,Guocai Yao,Hanbo Zhang,Xinlong Wang,Zhongyuan Wang,Xuguang Lan,Huaping Liu,Xinghang Li*

Main category: cs.RO

TL;DR: 本文提出了一种分层的视觉-语言-动作（VLA）框架，通过引入大规模预训练世界模型，将任务拆解为子目标，并利用视觉和语言信息提升机器人操作任务的泛化能力，尤其在分布外场景下显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型虽然有助于通用机器人的操作任务，但在分布外（OOD）场景下泛化能力有限，尤其受限于真实机器人数据的稀缺。因此，亟需提升其在新环境和新任务下的泛化性能。

Method: 提出一种分层VLA框架，顶层为世界模型，负责将复杂操作任务分解为带有目标图像的子任务序列；底层VLA策略参考这些视觉与文本子目标，生成具体动作序列。通过“生成中间视觉目标”，使低层策略具备物理和视觉的具体指导，从而提升新物体和场景下的泛化能力。

Result: 在大规模分布外测试中，加入世界模型生成的视觉目标后，相同VLA结构在新场景下的表现由14%大幅提升到69%；相较于主流基线，在OOD场景下展示出明显优势。

Conclusion: 分层VLA框架显著提升了机器人操作任务在分布外场景下的泛化能力，所提方法能高效地利用视觉和语言信息，优于以往方法，对通用机器人操作具有较大推动作用。

Abstract: Vision-Language-Action (VLA) models are promising for generalist robot manipulation but remain brittle in out-of-distribution (OOD) settings, especially with limited real-robot data. To resolve the generalization bottleneck, we introduce a hierarchical Vision-Language-Action framework \our{} that leverages the generalization of large-scale pre-trained world model for robust and generalizable VIsual Subgoal TAsk decomposition VISTA. Our hierarchical framework \our{} consists of a world model as the high-level planner and a VLA as the low-level executor. The high-level world model first divides manipulation tasks into subtask sequences with goal images, and the low-level policy follows the textual and visual guidance to generate action sequences. Compared to raw textual goal specification, these synthesized goal images provide visually and physically grounded details for low-level policies, making it feasible to generalize across unseen objects and novel scenarios. We validate both visual goal synthesis and our hierarchical VLA policies in massive out-of-distribution scenarios, and the performance of the same-structured VLA in novel scenarios could boost from 14% to 69% with the guidance generated by the world model. Results demonstrate that our method outperforms previous baselines with a clear margin, particularly in out-of-distribution scenarios. Project page: \href{https://vista-wm.github.io/}{https://vista-wm.github.io}

</details>


### [161] [Multi-Task Reinforcement Learning of Drone Aerobatics by Exploiting Geometric Symmetries](https://arxiv.org/abs/2602.10997)
*Zhanyu Guo,Zikang Yin,Guobin Zhu,Shiliang Guo,Shiyu Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种高效、通用的新型多任务强化学习（RL）框架GEAR，用于自主微型飞行器的高难度特技飞行动作控制，显著提升了学习效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法在无人机多任务高难度特技飞行控制中，数据效率低且泛化能力有限，难以让单一策略掌握多种特技动作，而传统方法缺乏对系统对称性的利用，限制了性能。

Method: 提出GEAR框架，结合了SO(2)旋转对称性等几何特性的等变actor网络、任务调制FiLM模块和多头评价网络，显式地将旋转对称性融入策略网络设计，实现端到端多任务特技学习。

Result: GEAR框架在多种特技任务中取得了98.85%的成功率，实验结果显著优于现有方法。同时，在真实世界实验中，GEAR可稳定完成多种动作，并能组合基础动作执行复杂特技。

Conclusion: GEAR作为一种兼具高效性、灵活性和鲁棒性的多任务RL框架，为自主飞行器特技控制提供了统一、有效的解决方案，推动了该领域发展。

Abstract: Flight control for autonomous micro aerial vehicles (MAVs) is evolving from steady flight near equilibrium points toward more aggressive aerobatic maneuvers, such as flips, rolls, and Power Loop. Although reinforcement learning (RL) has shown great potential in these tasks, conventional RL methods often suffer from low data efficiency and limited generalization. This challenge becomes more pronounced in multi-task scenarios where a single policy is required to master multiple maneuvers. In this paper, we propose a novel end-to-end multi-task reinforcement learning framework, called GEAR (Geometric Equivariant Aerobatics Reinforcement), which fully exploits the inherent SO(2) rotational symmetry in MAV dynamics and explicitly incorporates this property into the policy network architecture. By integrating an equivariant actor network, FiLM-based task modulation, and a multi-head critic, GEAR achieves both efficiency and flexibility in learning diverse aerobatic maneuvers, enabling a data-efficient, robust, and unified framework for aerobatic control. GEAR attains a 98.85\% success rate across various aerobatic tasks, significantly outperforming baseline methods. In real-world experiments, GEAR demonstrates stable execution of multiple maneuvers and the capability to combine basic motion primitives to complete complex aerobatics.

</details>


### [162] [ContactGaussian-WM: Learning Physics-Grounded World Model from Videos](https://arxiv.org/abs/2602.11021)
*Meizhong Wang,Wanxin Jin,Kun Cao,Lihua Xie,Yiguang Hong*

Main category: cs.RO

TL;DR: 这篇文章提出了ContactGaussian-WM，一种能够从稀疏且包含接触动态的视频中学习复杂物理定律的刚体世界模型，并在多项仿真与真实场景中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的世界建模方法在数据稀缺和物体接触动态复杂的情况下表现不佳，影响了机器人规划和仿真的推动。本研究旨在解决这些限制，实现更精准的物理感知和推理。

Method: 作者提出ContactGaussian-WM，包括：1）将视觉外观和碰撞几何统一为高斯表示；2）构建端到端可微学习框架，通过可微化的物理引擎从稀疏视觉观测中推断物理属性。该方法能以物理驱动方式从视频直接学习复杂物理规律。

Result: 广泛的仿真与实际场景评估表明，ContactGaussian-WM在复杂场景中的建模和泛化能力都优于最先进的方法。展示了其在数据合成、实时MPC等下游任务中的实际应用价值。

Conclusion: ContactGaussian-WM不仅能在数据稀缺和复杂接触运动中表现出色，还展现出良好的泛化能力和应用前景，为机器人和仿真系统中的物理建模做出了贡献。

Abstract: Developing world models that understand complex physical interactions is essential for advancing robotic planning and simulation.However, existing methods often struggle to accurately model the environment under conditions of data scarcity and complex contact-rich dynamic motion.To address these challenges, we propose ContactGaussian-WM, a differentiable physics-grounded rigid-body world model capable of learning intricate physical laws directly from sparse and contact-rich video sequences.Our framework consists of two core components: (1) a unified Gaussian representation for both visual appearance and collision geometry, and (2) an end-to-end differentiable learning framework that differentiates through a closed-form physics engine to infer physical properties from sparse visual observations.Extensive simulations and real-world evaluations demonstrate that ContactGaussian-WM outperforms state-of-the-art methods in learning complex scenarios, exhibiting robust generalization capabilities.Furthermore, we showcase the practical utility of our framework in downstream applications, including data synthesis and real-time MPC.

</details>


### [163] [SQ-CBF: Signed Distance Functions for Numerically Stable Superquadric-Based Safety Filtering](https://arxiv.org/abs/2602.11049)
*Haocheng Zhao,Lukas Brunke,Oliver Lagerquist,Siqi Zhou,Angela P. Schoellig*

Main category: cs.RO

TL;DR: 本文提出了一种基于超二次曲线（Superquadrics, SQ）的机器人安全过滤框架，通过使用有符号距离函数（SDFs）作为屏障函数候选，提升了复杂几何环境下机器人的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有使用控制屏障函数（CBFs）实现机器人安全的方案，通常简化几何表示，导致要么过于保守、要么碰撞保护不足。尽管Superquadrics能有效建模复杂形状，目前多直接用其隐式函数作为屏障，忽视了梯度病态问题，这会影响优化的可行性与实时性，从而危及实际操作安全。

Method: 提出用有符号距离函数（SDFs）替代传统隐式SQ函数，作为CBF的候选。由于一般SQ缺乏解析SDF，本文用Gilbert-Johnson-Keerthi（GJK）算法高效求距，并采用随机平滑方法获得梯度。

Result: 通过丰富的仿真和实物实验验证，该方法在杂乱和非结构化场景下，能持续完成无碰撞操作，表现出对复杂形状、传感噪声和动态扰动的鲁棒性，并且提升了机器人远程操控效率。

Conclusion: 基于SDFs的安全过滤不仅提升了准确性与可靠性，为现实复杂环境下机器人安全提供了有前景的解决路径。

Abstract: Ensuring safe robot operation in cluttered and dynamic environments remains a fundamental challenge. While control barrier functions provide an effective framework for real-time safety filtering, their performance critically depends on the underlying geometric representation, which is often simplified, leading to either overly conservative behavior or insufficient collision coverage. Superquadrics offer an expressive way to model complex shapes using a few primitives and are increasingly used for robot safety. To integrate this representation into collision avoidance, most existing approaches directly use their implicit functions as barrier candidates. However, we identify a critical but overlooked issue in this practice: the gradients of the implicit SQ function can become severely ill-conditioned, potentially rendering the optimization infeasible and undermining reliable real-time safety filtering. To address this issue, we formulate an SQ-based safety filtering framework that uses signed distance functions as barrier candidates. Since analytical SDFs are unavailable for general SQs, we compute distances using the efficient Gilbert-Johnson-Keerthi algorithm and obtain gradients via randomized smoothing. Extensive simulation and real-world experiments demonstrate consistent collision-free manipulation in cluttered and unstructured scenes, showing robustness to challenging geometries, sensing noise, and dynamic disturbances, while improving task efficiency in teleoperation tasks. These results highlight a pathway toward safety filters that remain precise and reliable under the geometric complexity of real-world environments.

</details>


### [164] [RISE: Self-Improving Robot Policy with Compositional World Model](https://arxiv.org/abs/2602.11075)
*Jiazhi Yang,Kunyang Lin,Jinwei Li,Wencong Zhang,Tianwei Lin,Longyan Wu,Zhizhong Su,Hao Zhao,Ya-Qin Zhang,Li Chen,Ping Luo,Xiangyu Yue,Hongyang Li*

Main category: cs.RO

TL;DR: 本文提出了一种用于机器人学习的可扩展方法RISE，通过在模拟世界中自我改进，显著提升了机器人在复杂动态任务中的表现，无需大量真实物理实验。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉-语言-动作（VLA）模型在容量和数据量持续扩展，但在涉及复杂接触和动态变化的机器操作中依然表现脆弱。实际物理环境下的强化学习由于安全风险和成本高昂难以广泛应用，因此需要创新方法在不增加物理试验成本的情况下提升模型鲁棒性。

Method: 核心方法为RISE框架：基于可组合的世界模型，分别使用可控动态模型预测多视角未来，并用进度值模型评估想象结果。这样设计使状态和价值建模可以采用最适配的架构和目标，结合成一个闭环自我提升流程，通过持续生成、评估虚拟操作轨迹，并在模拟空间中更新策略，减少对真实环境的依赖。

Result: 在砖块分拣、背包装箱、盒子闭合三项真实世界挑战任务中，RISE相较于以往方法，分别实现了+35%、+45%、+35%的绝对性能提升。

Conclusion: RISE展示了通过虚拟想象和自我改进，机器人能够在不进行大量物理实验的前提下，取得明显性能提升。这为机器人强化学习带来了安全、高效和可扩展的新范式。

Abstract: Despite the sustained scaling on model capacity and data acquisition, Vision-Language-Action (VLA) models remain brittle in contact-rich and dynamic manipulation tasks, where minor execution deviations can compound into failures. While reinforcement learning (RL) offers a principled path to robustness, on-policy RL in the physical world is constrained by safety risk, hardware cost, and environment reset. To bridge this gap, we present RISE, a scalable framework of robotic reinforcement learning via imagination. At its core is a Compositional World Model that (i) predicts multi-view future via a controllable dynamics model, and (ii) evaluates imagined outcomes with a progress value model, producing informative advantages for the policy improvement. Such compositional design allows state and value to be tailored by best-suited yet distinct architectures and objectives. These components are integrated into a closed-loop self-improving pipeline that continuously generates imaginary rollouts, estimates advantages, and updates the policy in imaginary space without costly physical interaction. Across three challenging real-world tasks, RISE yields significant improvement over prior art, with more than +35% absolute performance increase in dynamic brick sorting, +45% for backpack packing, and +35% for box closing, respectively.

</details>


### [165] [Digging for Data: Experiments in Rock Pile Characterization Using Only Proprioceptive Sensing in Excavation](https://arxiv.org/abs/2602.11082)
*Unal Artan,Martin Magnusson,Joshua A. Marshall*

Main category: cs.RO

TL;DR: 本论文提出并验证了一种通过轮式装载机作业时的自身惯性数据，无需外部传感器即可估算岩石堆粒径的方法。通过实验与现有视觉与筛分方法对比，取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 在采矿和采石行业，准确估计爆破后岩石堆的粒径分布对于后续加工和运输非常关键。目前常用相机或激光雷达等外部传感器，但这些方法成本高、部署复杂。作者希望探索利用装备本身在作业过程中的内部传感数据来替代外部感知，简化过程并降低成本。

Method: 该方法基于收集轮式装载机在挖掘不同岩石堆时的惯性响应，采用小波分析提取特征用于表征岩石破碎度。通过对不同尺寸岩石堆挖掘过程的数据进行分析，评估小波特征比值与实际粒径比值之间的关系。实验在实际采石环境中，用18吨电动装载机进行全尺寸作业，对结果与视觉工具和筛分结果进行对比。

Result: 实验显示，不同岩石堆的惯性数据提取的小波特征比值，能够较好地近似其粒径平均值之比。该方法估算的相对粒径与基于视觉分析和筛分的实际测量结果较为一致，验证了方法的有效性。

Conclusion: 通过装载机自身惯性数据、小波分析提取特征的方法能有效估算岩石堆相对粒径，无需外部传感器，为采矿行业岩石破碎度评估提供了低成本、自动化的新方案。

Abstract: Characterization of fragmented rock piles is a fundamental task in the mining and quarrying industries, where rock is fragmented by blasting, transported using wheel loaders, and then sent for further processing. This field report studies a novel method for estimating the relative particle size of fragmented rock piles from only proprioceptive data collected while digging with a wheel loader. Rather than employ exteroceptive sensors (e.g., cameras or LiDAR sensors) to estimate rock particle sizes, the studied method infers rock fragmentation from an excavator's inertial response during excavation. This paper expands on research that postulated the use of wavelet analysis to construct a unique feature that is proportional to the level of rock fragmentation. We demonstrate through extensive field experiments that the ratio of wavelet features, constructed from data obtained by excavating in different rock piles with different size distributions, approximates the ratio of the mean particle size of the two rock piles. Full-scale excavation experiments were performed with a battery electric, 18-tonne capacity, load-haul-dump (LHD) machine in representative conditions in an operating quarry. The relative particle size estimates generated with the proposed sensing methodology are compared with those obtained from both a vision-based fragmentation analysis tool and from sieving of sampled materials.

</details>


### [166] [A receding-horizon multi-contact motion planner for legged robots in challenging environments](https://arxiv.org/abs/2602.11113)
*Daniel S. J. Derwent,Simon Watson,Bruno V. Adorno*

Main category: cs.RO

TL;DR: 提出了一种新型的行进区间多接触运动规划器，适用于足式机器人在极具挑战性的场景中的运动规划，实现了动态规划、同时生成接触点和全身运动轨迹，并在多种场景下展现出优于现有方法的速度和运动质量。


<details>
  <summary>Details</summary>
Motivation: 现有的多接触运动规划器在复杂环境下（如烟囱攀爬、狭窄通道、大间隙跨越）常常需要多阶段处理，难以实现实时反应，且规划计算速度和质量有待提升。因此需要一种能应对复杂场景并能在线快速更新的运动规划方法。

Method: 将区间滚动（receding-horizon）方法应用于多接触运动规划，并通过二次规划生成姿态，做到同时规划接触点与全身轨迹，避免多阶段处理和后处理带来的复杂性，提高在线规划能力和鲁棒性。

Result: 实验证明：在短规划区间时，新方法比最新技术快45%-98%，在不同场景下平均动作转变次数少5%至多700%；长区间情况下多数场景平均规划时间介于快73%到慢400%，但运动质量提升（动作转换次数下降8%-47%）。

Conclusion: 所提方法在保持高速度的同时显著提升了足式机器人在复杂环境中的运动规划能力，尤其是在动态响应和计划质量上优于传统方法，具有更强的实用性和适应性。

Abstract: We present a novel receding-horizon multi-contact motion planner for legged robots in challenging scenarios, able to plan motions such as chimney climbing, navigating very narrow passages or crossing large gaps. Our approach adds new capabilities to the state of the art, including the ability to reactively re-plan in response to new information, and planning contact locations and whole-body trajectories simultaneously, simplifying the implementation and removing the need for post-processing or complex multi-stage approaches. Our method is more resistant to local minima problems than other potential field based approaches, and our quadratic-program-based posture generator returns nodes more quickly than those of existing algorithms. Rigorous statistical analysis shows that, with short planning horizons (e.g., one step ahead), our planner is faster than the state-of-the-art across all scenarios tested (between 45% and 98% faster on average, depending on the scenario), while planning less efficient motions (requiring 5% fewer to 700% more stance changes on average). In all but one scenario (Chimney Walking), longer planning horizons (e.g., four steps ahead) extended the average planning times (between 73% faster and 400% slower than the state-of-the-art) but resulted in higher quality motion plans (between 8% more and 47% fewer stance changes than the state-of-the-art).

</details>


### [167] [Data-Efficient Hierarchical Goal-Conditioned Reinforcement Learning via Normalizing Flows](https://arxiv.org/abs/2602.11142)
*Shaswat Garg,Matin Moezzi,Brandon Da Silva*

Main category: cs.RO

TL;DR: 提出了一种基于正态流的分层隐式Q学习（NF-HIQL），提高了分层目标强化学习在数据有限环境下的效率与表达能力。


<details>
  <summary>Details</summary>
Motivation: 传统分层目标强化学习（H-GCRL）能够通过分解复杂任务处理长时序问题，但在数据稀缺或离线场景下，数据效率低且策略表达能力有限，限制了实际应用。

Method: 将高层和低层策略中的单峰高斯策略替换为可表达能力更强的正态流（Normalizing Flow）策略，实现对多模态复杂行为的建模，同时保持高效的采样和可计算的对数似然。理论上推导了RealNVP策略的KL散度边界和PAC式采样效率，验证了稳定性和泛化能力。

Result: 在OGBench平台的多样长时序任务（如行走、带球、复杂操作）上，NF-HIQL在数据稀缺条件下表现出更强的鲁棒性与优越性，持续超越同期目标驱动及分层方法。

Conclusion: NF-HIQL架构展现了可扩展性和数据高效性，证明了基于正态流的分层强化学习在复杂任务中的巨大潜力。

Abstract: Hierarchical goal-conditioned reinforcement learning (H-GCRL) provides a powerful framework for tackling complex, long-horizon tasks by decomposing them into structured subgoals. However, its practical adoption is hindered by poor data efficiency and limited policy expressivity, especially in offline or data-scarce regimes. In this work, Normalizing flow-based hierarchical implicit Q-learning (NF-HIQL), a novel framework that replaces unimodal gaussian policies with expressive normalizing flow policies at both the high- and low-levels of the hierarchy is introduced. This design enables tractable log-likelihood computation, efficient sampling, and the ability to model rich multimodal behaviors. New theoretical guarantees are derived, including explicit KL-divergence bounds for Real-valued non-volume preserving (RealNVP) policies and PAC-style sample efficiency results, showing that NF-HIQL preserves stability while improving generalization. Empirically, NF-HIQL is evaluted across diverse long-horizon tasks in locomotion, ball-dribbling, and multi-step manipulation from OGBench. NF-HIQL consistently outperforms prior goal-conditioned and hierarchical baselines, demonstrating superior robustness under limited data and highlighting the potential of flow-based architectures for scalable, data-efficient hierarchical reinforcement learning.

</details>


### [168] [APEX: Learning Adaptive High-Platform Traversal for Humanoid Robots](https://arxiv.org/abs/2602.11143)
*Yikai Wang,Tingxuan Leng,Changyi Lin,Shiqi Liu,Shir Simon,Bingqing Chen,Jonathan Francis,Ding Zhao*

Main category: cs.RO

TL;DR: 该论文提出了APEX系统，实现了人形机器人基于攀爬的高平台穿越，突破了传统RL方法跳跃式、高风险穿越的局限，并在29自由度人形机器人上实现了安全、顺畅、多技能的sim-to-real迁移与自适应。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习已推动人形机器人在崎岖地形上步行，但对于远超腿长的高平台，现有RL方法多收敛为跳跃式策略，带来高冲击力、高扭矩风险，难以实际应用，亟需安全高效的新方法。

Method: 提出APEX系统，通过合成地形感知调节的攀爬、行走/爬行、躺下/起身等多种行为，实现高平台穿越。创新性设计奖惩函数（广义棘轮进度奖励），鼓励任务进展并惩罚重复或无效动作。结合激光雷达感知，通过建模感知偏差与实际部署时的滤波/修补方式，实现高质量的sim-to-real迁移。最终将六类技能蒸馏为一个在本地地形及指令驱动下自动切换行为的策略。

Result: 在29自由度Unitree G1人形机器人上，实验实现了0.8米（腿长的114%）高平台的无微调sim-to-real穿越，无论平台高度或初始姿态都有稳健适应，并实现了平滑的多技能切换和穿越过程。

Conclusion: APEX系统创新地使人形机器人安全、稳健、高效地实现了前所未有的高平台穿越能力，是人形机器人多技能自主适应迈向现实部署的重要进展。

Abstract: Humanoid locomotion has advanced rapidly with deep reinforcement learning (DRL), enabling robust feet-based traversal over uneven terrain. Yet platforms beyond leg length remain largely out of reach because current RL training paradigms often converge to jumping-like solutions that are high-impact, torque-limited, and unsafe for real-world deployment. To address this gap, we propose APEX, a system for perceptive, climbing-based high-platform traversal that composes terrain-conditioned behaviors: climb-up and climb-down at vertical edges, walking or crawling on the platform, and stand-up and lie-down for posture reconfiguration. Central to our approach is a generalized ratchet progress reward for learning contact-rich, goal-reaching maneuvers. It tracks the best-so-far task progress and penalizes non-improving steps, providing dense yet velocity-free supervision that enables efficient exploration under strong safety regularization. Based on this formulation, we train LiDAR-based full-body maneuver policies and reduce the sim-to-real perception gap through a dual strategy: modeling mapping artifacts during training and applying filtering and inpainting to elevation maps during deployment. Finally, we distill all six skills into a single policy that autonomously selects behaviors and transitions based on local geometry and commands. Experiments on a 29-DoF Unitree G1 humanoid demonstrate zero-shot sim-to-real traversal of 0.8 meter platforms (approximately 114% of leg length), with robust adaptation to platform height and initial pose, as well as smooth and stable multi-skill transitions.

</details>


### [169] [YOR: Your Own Mobile Manipulator for Generalizable Robotics](https://arxiv.org/abs/2602.11150)
*Manan H Anjaria,Mehmet Enes Erciyes,Vedant Ghatnekar,Neha Navarkar,Haritheja Etukuru,Xiaole Jiang,Kanad Patel,Dhawal Kabra,Nicholas Wojno,Radhika Ajay Prayage,Soumith Chintala,Lerrel Pinto,Nur Muhammad Mahi Shafiullah,Zichen Jeff Cui*

Main category: cs.RO

TL;DR: 本文提出了YOR，一个开源、低成本的移动操作机器人，具备全身移动、双臂操作和自主导航能力，组装方便且总成本低于一万美元。


<details>
  <summary>Details</summary>
Motivation: 机器人学习快速发展，需要更廉价且功能强大的移动操作平台作为支撑，目前低成本设计仍缺乏通用最佳形态。

Method: 设计并实现了YOR机器人，采用全向底盘、可伸缩升降柱和双臂夹持器，具有模块化、易组装和低成本特点，全部使用通用现成零部件。

Result: YOR完成了包括全身协作控制、双手操作和自主导航等多项任务，成功验证了其在多种实际移动操作任务中的能力。

Conclusion: YOR以极低成本为移动操作研究提供了一款高性能开放平台，降低了实验机器人的门槛，有助于推动该领域进一步发展。

Abstract: Recent advances in robot learning have generated significant interest in capable platforms that may eventually approach human-level competence. This interest, combined with the commoditization of actuators, has propelled growth in low-cost robotic platforms. However, the optimal form factor for mobile manipulation, especially on a budget, remains an open question. We introduce YOR, an open-source, low-cost mobile manipulator that integrates an omnidirectional base, a telescopic vertical lift, and two arms with grippers to achieve whole-body mobility and manipulation. Our design emphasizes modularity, ease of assembly using off-the-shelf components, and affordability, with a bill-of-materials cost under 10,000 USD. We demonstrate YOR's capability by completing tasks that require coordinated whole-body control, bimanual manipulation, and autonomous navigation. Overall, YOR offers competitive functionality for mobile manipulation research at a fraction of the cost of existing platforms. Project website: https://www.yourownrobot.ai/

</details>
