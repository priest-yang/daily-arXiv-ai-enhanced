<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 159]
- [cs.CL](#cs.CL) [Total: 109]
- [cs.RO](#cs.RO) [Total: 47]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Random Direct Preference Optimization for Radiography Report Generation](https://arxiv.org/abs/2509.21351)
*Valentin Samokhin,Boris Shirokikh,Mikhail Goncharov,Dmitriy Umerenkov,Maksim Bobrin,Ivan Oseledets,Dmitry Dylov,Mikhail Belyaev*

Main category: cs.CV

TL;DR: 本文提出了一种基于直接偏好优化（DPO）的通用框架，通过随机对比采样提升放射影像报告生成（RRG）的准确性，无需额外数据或人工偏好标注，实验表明可提升临床表现指标。


<details>
  <summary>Details</summary>
Motivation: 尽管放射影像报告生成技术已取得进展，但其生成质量尚未满足实际临床使用需求。近期大视觉语言模型（VLMs）在通用领域表现优异，启发作者探索是否可借用LLM中的训练方法提升RRG质量。

Method: 提出一种不依赖具体模型的框架：采用Direct Preference Optimization（DPO），利用随机对比采样自动生成训练对，无需奖励模型或人工偏好标注，将该方法作为补充应用于三种主流RRG模型。

Result: 在不增加额外训练数据的情况下，DPO策略可令三种SOTA模型的临床指标提升最高达5%。

Conclusion: 利用随机DPO优化策略，可在不增加数据或人工标注成本下，显著提升RRG系统的临床表现，有望加速其在实际医疗中的应用。

Abstract: Radiography Report Generation (RRG) has gained significant attention in
medical image analysis as a promising tool for alleviating the growing workload
of radiologists. However, despite numerous advancements, existing methods have
yet to achieve the quality required for deployment in real-world clinical
settings. Meanwhile, large Visual Language Models (VLMs) have demonstrated
remarkable progress in the general domain by adopting training strategies
originally designed for Large Language Models (LLMs), such as alignment
techniques. In this paper, we introduce a model-agnostic framework to enhance
RRG accuracy using Direct Preference Optimization (DPO). Our approach leverages
random contrastive sampling to construct training pairs, eliminating the need
for reward models or human preference annotations. Experiments on supplementing
three state-of-the-art models with our Random DPO show that our method improves
clinical performance metrics by up to 5%, without requiring any additional
training data.

</details>


### [2] [Improving Autism Detection with Multimodal Behavioral Analysis](https://arxiv.org/abs/2509.21352)
*William Saakyan,Matthias Norden,Lola Eversmann,Simon Kirsch,Muyu Lin,Simon Guendelman,Isabel Dziobek,Hanna Drimalla*

Main category: cs.CV

TL;DR: 本研究基于迄今最大、最平衡的自闭症光谱障碍(ASC)视频数据集，通过分析面部表情、语音、头部动作、心率变异性和凝视行为，多模态地提升了自闭症自动检测准确性。


<details>
  <summary>Details</summary>
Motivation: 自闭症的诊断过程复杂且资源消耗大，现有自动检测方法在现实应用中受限，尤其是在凝视特征表现不佳和泛化能力有限的问题。论文旨在通过改进数据集和特征工程，提高自动诊断系统的准确性及现实可用性。

Method: 作者收集并分析了包含168名ASC参与者和157名非自闭症参与者的标准视频数据集，涵盖多种行为特征（面部表情、语音、头部动作、心率变异性和凝视行为）。针对凝视特征，提出新的统计描述符，用于量化眼动角度的变化性。同时采用多模态特征融合，并利用晚期融合策略提升总体分类性能。

Result: 创新的凝视特征将凝视相关分类准确率由64%提升到69%，多模态特征融合后整体准确率达到74%。

Conclusion: 多模态整合行为特征显著提升了ASC的自动化筛查能力，为基于视频的自闭症评估提供了可扩展的技术路径。

Abstract: Due to the complex and resource-intensive nature of diagnosing Autism
Spectrum Condition (ASC), several computer-aided diagnostic support methods
have been proposed to detect autism by analyzing behavioral cues in patient
video data. While these models show promising results on some datasets, they
struggle with poor gaze feature performance and lack of real-world
generalizability. To tackle these challenges, we analyze a standardized video
dataset comprising 168 participants with ASC (46% female) and 157 non-autistic
participants (46% female), making it, to our knowledge, the largest and most
balanced dataset available. We conduct a multimodal analysis of facial
expressions, voice prosody, head motion, heart rate variability (HRV), and gaze
behavior. To address the limitations of prior gaze models, we introduce novel
statistical descriptors that quantify variability in eye gaze angles, improving
gaze-based classification accuracy from 64% to 69% and aligning computational
findings with clinical research on gaze aversion in ASC. Using late fusion, we
achieve a classification accuracy of 74%, demonstrating the effectiveness of
integrating behavioral markers across multiple modalities. Our findings
highlight the potential for scalable, video-based screening tools to support
autism assessment.

</details>


### [3] [KV-Efficient VLA: A Method of Speed up Vision Language Model with RNN-Gated Chunked KV Cache](https://arxiv.org/abs/2509.21354)
*Wanshun Xu,Long Zhuang*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的视觉-语言-动作（VLA）模型记忆压缩方法，有效提升模型在长时推理场景下的推理速度和内存利用率。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在推理长时间任务时，注意力机制的计算量呈二次增长且KV存储会无限扩大，导致推理效率低下，不利于实际部署。许多方法关注提升泛化能力，却忽视了推理阶段的高效问题。

Method: 提出KV-Efficient VLA，这是一个模型无关的记忆压缩框架。具体方法是将KV缓存分块，并用循环门控模块对每块内容进行筛选和摘要，根据学习到的效用分数，保留高效用信息、抛弃无关冗余内容，从而在不影响因果性的前提下，大幅减少KV存储和计算量。

Result: 理论结果显示，该方法在基本不影响VLA任务成功率的前提下，实现了推理速度最高提升1.21倍，KV内存占用减少36%。

Conclusion: KV-Efficient VLA能无缝集成到现有自回归或混合VLA模型推理流程，无需改变训练流程及下游控制逻辑，为大规模部署和高效推理带来新的解决方案。

Abstract: Vision-Language-Action (VLA) models promise unified robotic perception and
control, yet their scalability is constrained by the quadratic cost of
attention and the unbounded growth of key-value (KV) memory during long-horizon
inference. While recent methods improve generalization through scaling backbone
architectures, they often neglect the inference inefficiencies critical to
real-time deployment. In this work, we present KV-Efficient VLA, a
model-agnostic memory compression framework that addresses these limitations by
introducing a lightweight, training-friendly mechanism to selectively retain
high-utility context. Our method partitions the KV cache into fixed size chunks
and employs a recurrent gating module to summarize and filter historical
context according to learned utility scores. This design preserves recent
fine-grained detail while aggressively pruning stale, low-relevance memory, all
while maintaining causality. Theoretically, KV-Efficient VLA yields up to 1.21x
inference speedup and 36% KV memory reduction, with minimal impact on task
success. Our method integrates seamlessly into existing autoregressive and
hybrid VLA stacks, enabling scalable inference without modifying training
pipelines or downstream control logic.

</details>


### [4] [Phrase-grounded Fact-checking for Automatically Generated Chest X-ray Reports](https://arxiv.org/abs/2509.21356)
*Razi Mahmood,Diego Machado-Reyes,Joy Wu,Parisa Kaviani,Ken C. L. Wong,Niharika D'Souza,Mannudeep Kalra,Ge Wang,Pingkun Yan,Tanveer Syeda-Mahmood*

Main category: cs.CV

TL;DR: 本文提出了一种用于自动检查胸部X射线报告是否与图像内容一致的事实核查模型，通过检测报告中疾病现象及定位是否准确，提高报告的可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着大规模视觉语言模型的兴起，自动生成的胸片报告存在事实性错误和臆测，影响其临床应用，因此亟需自动化的手段验证报告内容的真实性。

Method: 作者合成了一个大规模带有扰动的报告数据集，人为制造正确和错误的疾病-位置对，通过多标签跨模态对比回归网络进行训练，实现对报告正确性和定位的自动检测。

Result: 该方法在多个X射线数据集上表现出极强的鲁棒性，对报告中发现真实性和定位精度均有提升；在与主流报告自动生成系统结合时，核查准确率与基于人工标注的验证几乎一致（CCC=0.997）。

Conclusion: 所提出的事实核查方法能高效识别自动生成医学报告中的错误，显著增强自动报告系统临床使用的可靠性和安全性。

Abstract: With the emergence of large-scale vision language models (VLM), it is now
possible to produce realistic-looking radiology reports for chest X-ray images.
However, their clinical translation has been hampered by the factual errors and
hallucinations in the produced descriptions during inference. In this paper, we
present a novel phrase-grounded fact-checking model (FC model) that detects
errors in findings and their indicated locations in automatically generated
chest radiology reports.
  Specifically, we simulate the errors in reports through a large synthetic
dataset derived by perturbing findings and their locations in ground truth
reports to form real and fake findings-location pairs with images. A new
multi-label cross-modal contrastive regression network is then trained on this
dataset. We present results demonstrating the robustness of our method in terms
of accuracy of finding veracity prediction and localization on multiple X-ray
datasets. We also show its effectiveness for error detection in reports of SOTA
report generators on multiple datasets achieving a concordance correlation
coefficient of 0.997 with ground truth-based verification, thus pointing to its
utility during clinical inference in radiology workflows.

</details>


### [5] [MDF-MLLM: Deep Fusion Through Cross-Modal Feature Alignment for Contextually Aware Fundoscopic Image Classification](https://arxiv.org/abs/2509.21358)
*Jason Jordan,Mohammadreza Akbari Lor,Peter Koulen,Mei-Ling Shyu,Shu-Ching Chen*

Main category: cs.CV

TL;DR: 本文提出了一种多模态深度学习新架构MDF-MLLM，通过融合视网膜图像的细粒度特征与文本信息，有效提升了疾病分类准确率，并且明显优于传统多模态大模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在视网膜疾病诊断中，难以捕捉关键的低层空间细节，导致难以精准分类如青光眼、糖尿病视网膜病变等病症。因此，亟需一种结合图像细节和文本上下文信息的高效模型。

Method: 作者整合了三大公开数据集（FIVES、HRF、StoneRounds）中的1305组图像-文本对，开发MDF-MLLM架构。其核心在于从U-Net编码器四层跳跃特征融入LLaMA 3.2 11B基础的MLLM交叉注意力模块，联合应用分块投影、缩放交叉注意力与FiLM调制，端到端联合微调图像与语言两部分。

Result: MDF-MLLM在双类型疾病分类任务中，准确率达到94%，较传统MLLM基线（60%）提升56个百分点；召回率和F1分数分别提升67%与35%。消融实验显示多层次特征融合对空间推理和分类有显著贡献，尤其对带有丰富文本的遗传性疾病效果更优。

Conclusion: MDF-MLLM通过多尺度特征融合，有效增强了模型对视网膜疾病的判别能力，展现出良好的泛化性、可解释性和临床应用前景。未来将开展同步训练、拓展疾病种类和分割任务的研究。

Abstract: This study aimed to enhance disease classification accuracy from retinal
fundus images by integrating fine-grained image features and global textual
context using a novel multimodal deep learning architecture. Existing
multimodal large language models (MLLMs) often struggle to capture low-level
spatial details critical for diagnosing retinal diseases such as glaucoma,
diabetic retinopathy, and retinitis pigmentosa. This model development and
validation study was conducted on 1,305 fundus image-text pairs compiled from
three public datasets (FIVES, HRF, and StoneRounds), covering acquired and
inherited retinal diseases, and evaluated using classification accuracy and
F1-score. The MDF-MLLM integrates skip features from four U-Net encoder layers
into cross-attention blocks within a LLaMA 3.2 11B MLLM. Vision features are
patch-wise projected and fused using scaled cross-attention and FiLM-based
U-Net modulation. Baseline MLLM achieved 60% accuracy on the dual-type disease
classification task. MDF-MLLM, with both U-Net and MLLM components fully
fine-tuned during training, achieved a significantly higher accuracy of 94%,
representing a 56% improvement. Recall and F1-scores improved by as much as 67%
and 35% over baseline, respectively. Ablation studies confirmed that the
multi-depth fusion approach contributed to substantial gains in spatial
reasoning and classification, particularly for inherited diseases with rich
clinical text. MDF-MLLM presents a generalizable, interpretable, and modular
framework for fundus image classification, outperforming traditional MLLM
baselines through multi-scale feature fusion. The architecture holds promise
for real-world deployment in clinical decision support systems. Future work
will explore synchronized training techniques, a larger pool of diseases for
more generalizability, and extending the model for segmentation tasks.

</details>


### [6] [Multimodal Prompt Decoupling Attack on the Safety Filters in Text-to-Image Models](https://arxiv.org/abs/2509.21360)
*Xingkai Peng,Jun Jiang,Meng Tong,Shuai Li,Weiming Zhang,Nenghai Yu,Kejiang Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的攻破文本到图像（T2I）模型安全机制的方法，称为多模态提示解耦攻击（MPDA），通过图像和文本配合，能够生成违禁内容并绕过现有安全过滤系统。


<details>
  <summary>Details</summary>
Motivation: 虽然T2I模型具有强大的图像生成能力，但其安全防护存在漏洞，尤其是通过提示词（prompt）操控生成不安全（NSFW）内容。目前主要的绕过滤器方法集中在文本提示，但难以应对更高级的安全检测，且对多模态输入的研究不足。

Method: MPDA方法包括三个主要步骤：1）利用大语言模型（LLM）将不安全的提示词拆分为表面安全和真正有害的子提示；2）对有害子提示进行自然化改写，生成能绕过过滤器的对抗性提示，引导模型生成NSFW图像；3）借助视觉语言模型对生成图片进行描述，进一步优化、迭代提示，确保内容与原始不安全提示一致。

Result: 实验表明，MPDA可以有效绕过多种T2I模型的安全过滤器，通过多轮优化获得与原始不安全提示高度一致的NSFW图像，突破了传统仅依赖文本的攻击方法的限制。

Conclusion: MPDA揭示了当前T2I模型在多模态攻击下的安全不足，提示需要更高级的跨模态安全防护，同时为未来防御工作提供了新的研究视角。

Abstract: Text-to-image (T2I) models have been widely applied in generating
high-fidelity images across various domains. However, these models may also be
abused to produce Not-Safe-for-Work (NSFW) content via jailbreak attacks.
Existing jailbreak methods primarily manipulate the textual prompt, leaving
potential vulnerabilities in image-based inputs largely unexplored. Moreover,
text-based methods face challenges in bypassing the model's safety filters. In
response to these limitations, we propose the Multimodal Prompt Decoupling
Attack (MPDA), which utilizes image modality to separate the harmful semantic
components of the original unsafe prompt. MPDA follows three core steps:
firstly, a large language model (LLM) decouples unsafe prompts into pseudo-safe
prompts and harmful prompts. The former are seemingly harmless sub-prompts that
can bypass filters, while the latter are sub-prompts with unsafe semantics that
trigger filters. Subsequently, the LLM rewrites the harmful prompts into
natural adversarial prompts to bypass safety filters, which guide the T2I model
to modify the base image into an NSFW output. Finally, to ensure semantic
consistency between the generated NSFW images and the original unsafe prompts,
the visual language model generates image captions, providing a new pathway to
guide the LLM in iterative rewriting and refining the generated content.

</details>


### [7] [A Mutual Learning Method for Salient Object Detection with intertwined Multi-Supervision--Revised](https://arxiv.org/abs/2509.21363)
*Runmin Wu,Mengyang Feng,Wenlong Guan,Dong Wang,Huchuan Lu,Errui Ding*

Main category: cs.CV

TL;DR: 本文提出了一种结合显著性目标检测、前景轮廓检测和边缘检测的多任务互助深度学习方法，有效提升了显著性检测的完整性与边缘精度，并取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前显著性目标检测中，深度学习方法尽管取得了进展，但由于物体内部结构复杂及卷积步幅与池化操作等造成边界不精确，导致检测结果不完整。本文致力于缓解这些问题，提升显著性检测的精度和完整性。

Method: 本文不仅利用显著性检测，还引入了前景轮廓检测与边缘检测，采用互相关联的方式训练网络，从而优化不同子任务间的反馈。提出了一种新颖的互助学习模块（MLM），通过多分支互助学习提升网络整体性能。

Result: 在七个具有挑战性的数据集上进行了大量实验，结果表明所提方法在显著性检测与边缘检测任务中均达到最新最优水平。

Conclusion: 多任务互助学习，结合轮廓和边缘信息，可以显著提升显著性检测的整体效果和边界准确性。

Abstract: Though deep learning techniques have made great progress in salient object
detection recently, the predicted saliency maps still suffer from incomplete
predictions due to the internal complexity of objects and inaccurate boundaries
caused by strides in convolution and pooling operations. To alleviate these
issues, we propose to train saliency detection networks by exploiting the
supervision from not only salient object detection, but also foreground contour
detection and edge detection. First, we leverage salient object detection and
foreground contour detection tasks in an intertwined manner to generate
saliency maps with uniform highlight. Second, the foreground contour and edge
detection tasks guide each other simultaneously, thereby leading to precise
foreground contour prediction and reducing the local noises for edge
prediction. In addition, we develop a novel mutual learning module (MLM) which
serves as the building block of our method. Each MLM consists of multiple
network branches trained in a mutual learning manner, which improves the
performance by a large margin. Extensive experiments on seven challenging
datasets demonstrate that the proposed method has delivered state-of-the-art
results in both salient object detection and edge detection.

</details>


### [8] [MAJORScore: A Novel Metric for Evaluating Multimodal Relevance via Joint Representation](https://arxiv.org/abs/2509.21365)
*Zhicheng Du,Qingyang Shi,Jiasheng Lu,Yingshan Liang,Xinyu Zhang,Yiran Wang,Peiwu Qin*

Main category: cs.CV

TL;DR: 该论文提出了一种新的多模态相关性评价指标MAJORScore，能有效评价三种及以上模态间的相关性，解决了传统只适用于双模态的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态相关性评价指标主要针对双模态（如CLIP），无法准确、公平地评价三种及以上模态的数据相关性，限制了多模态模型和数据集的评测深度。

Method: 提出MAJORScore指标，并创新性地利用多模态联合表征方法，将多种模态数据映射到同一潜在空间，通过该空间的分布和距离机制实现多模态相关性准确量化。

Result: 大量实验表明，MAJORScore在一致性模态上较现有方法提升26.03%-64.29%，在非一致性模态上降低13.28%-20.54%，即更具判别力和可靠性。

Conclusion: MAJORScore为大规模多模态数据集的相关性评价和多模态模型性能评测提供了更加公平、有效的新工具和指标。

Abstract: The multimodal relevance metric is usually borrowed from the embedding
ability of pretrained contrastive learning models for bimodal data, which is
used to evaluate the correlation between cross-modal data (e.g., CLIP).
However, the commonly used evaluation metrics are only suitable for the
associated analysis between two modalities, which greatly limits the evaluation
of multimodal similarity. Herein, we propose MAJORScore, a brand-new evaluation
metric for the relevance of multiple modalities (N modalities, N>=3) via
multimodal joint representation for the first time. The ability of multimodal
joint representation to integrate multiple modalities into the same latent
space can accurately represent different modalities at one scale, providing
support for fair relevance scoring. Extensive experiments have shown that
MAJORScore increases by 26.03%-64.29% for consistent modality and decreases by
13.28%-20.54% for inconsistence compared to existing methods. MAJORScore serves
as a more reliable metric for evaluating similarity on large-scale multimodal
datasets and multimodal model performance evaluation.

</details>


### [9] [Safety Assessment of Scaffolding on Construction Site using AI](https://arxiv.org/abs/2509.21368)
*Sameer Prabhu,Amit Patwardhan,Ramin Karim*

Main category: cs.CV

TL;DR: 本文提出了利用AI和数字化技术，通过云端平台对脚手架结构进行点云数据分析，以实现更高效、更可靠的自动化安全巡检。


<details>
  <summary>Details</summary>
Motivation: 目前脚手架主要依赖人工目测巡检，既耗时又易出现人为疏漏，可能导致不安全隐患。因此需要寻求更高效、智能化的方法提升巡检准确性和工作安全性。

Method: 开发了一套基于云端的AI平台，系统通过采集脚手架结构的点云数据，将其与认证参照数据进行比对，检测结构上的变更及潜在风险，实现自动化分析与监控。

Result: 系统能够自动检测脚手架结构的变化，大幅减少人工检查所需的时间和精力，同时提升了巡检的准确性和现场安全水平。

Conclusion: AI与点云数字化方法可有效提升脚手架安全巡检的自动化与智能化，具有降低风险、提升施工现场安全管理水平的应用前景。

Abstract: In the construction industry, safety assessment is vital to ensure both the
reliability of assets and the safety of workers. Scaffolding, a key structural
support asset requires regular inspection to detect and identify alterations
from the design rules that may compromise the integrity and stability. At
present, inspections are primarily visual and are conducted by site manager or
accredited personnel to identify deviations. However, visual inspection is
time-intensive and can be susceptible to human errors, which can lead to unsafe
conditions. This paper explores the use of Artificial Intelligence (AI) and
digitization to enhance the accuracy of scaffolding inspection and contribute
to the safety improvement. A cloud-based AI platform is developed to process
and analyse the point cloud data of scaffolding structure. The proposed system
detects structural modifications through comparison and evaluation of certified
reference data with the recent point cloud data. This approach may enable
automated monitoring of scaffolding, reducing the time and effort required for
manual inspections while enhancing the safety on a construction site.

</details>


### [10] [Automated Prompt Generation for Creative and Counterfactual Text-to-image Synthesis](https://arxiv.org/abs/2509.21375)
*Aleksa Jelaca,Ying Jiao,Chang Tian,Marie-Francine Moens*

Main category: cs.CV

TL;DR: 本文提出了一种自动化提示词工程框架，用于提升文本到图像生成模型对反事实尺寸（如“小海象在巨大纽扣旁”）的可控性，并构建了首个相关数据集。实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在文本到图像生成中的精细可控性不足，尤其是难以生成违背常识的反事实（如不合理尺寸比例）图像，而这对于创造力和探索应用非常关键。

Method: 框架包含三个部分：1）图像评估器——通过引导数据集构建，识别成功生成的反事实图像；2）有监督的提示词重写器——将原始提示词改写为更能生成反事实结果的新提示词；3）DPO训练排序器——从改写的提示中选出最佳结果。辅助以扩展的Grounded SAM模型提升评估准确性。

Result: 实现首个反事实尺寸文本-图像数据集，改进后的图像评估器比基础模型性能提升114%。实验表明，该方法在多个指标上超过SOTA和ChatGPT-4o。

Conclusion: 该方法有效提升了反事实可控性，为后续相关研究奠定了基础，拓展了文本到图像生成的创造力和应用空间。

Abstract: Text-to-image generation has advanced rapidly with large-scale multimodal
training, yet fine-grained controllability remains a critical challenge.
Counterfactual controllability, defined as the capacity to deliberately
generate images that contradict common-sense patterns, remains a major
challenge but plays a crucial role in enabling creativity and exploratory
applications. In this work, we address this gap with a focus on counterfactual
size (e.g., generating a tiny walrus beside a giant button) and propose an
automatic prompt engineering framework that adapts base prompts into revised
prompts for counterfactual images. The framework comprises three components: an
image evaluator that guides dataset construction by identifying successful
image generations, a supervised prompt rewriter that produces revised prompts,
and a DPO-trained ranker that selects the optimal revised prompt. We construct
the first counterfactual size text-image dataset and enhance the image
evaluator by extending Grounded SAM with refinements, achieving a 114 percent
improvement over its backbone. Experiments demonstrate that our method
outperforms state-of-the-art baselines and ChatGPT-4o, establishing a
foundation for future research on counterfactual controllability.

</details>


### [11] [In silico Deep Learning Protocols for Label-Free Super-Resolution Microscopy: A Comparative Study of Network Architectures and SNR Dependence](https://arxiv.org/abs/2509.21376)
*Shiraz S Kaderuppan,Jonathan Mar,Andrew Irvine,Anurag Sharma,Muhammad Ramadan Saifuddin,Wai Leong Eugene Wong,Wai Lok Woo*

Main category: cs.CV

TL;DR: 本研究提出利用深度神经网络（DNN）提升非荧光光学显微镜的分辨率，验证了一种不依赖昂贵硬件的超分辨率成像新方法。


<details>
  <summary>Details</summary>
Motivation: 传统光学显微镜受限于约200nm的横向分辨率，超越该限制通常需昂贵设备或复杂技术。大多数用户和实验室难以承担，因此需要更经济易用的解决方案。

Method: 采用两种自研的神经网络（O-Net和Theta-Net），对基于相位调制的显微模式（如Zernike和DIC）下拍摄的标准测试样本进行图像超分辨率实验，且样本特征通过AFM校准。比较两种网络在不同信噪比（SNR）下的成像效果。

Result: O-Net在图像信噪比较高时效果更佳，Theta-Net则适用于低信噪比图像，两者表现互补，均能提升测试样本的超分辨率成像质量。

Conclusion: 神经网络架构与源图像信噪比密切相关，影响超分辨率重建质量。合理选择模型可在无需专业设备的前提下实现非荧光光学显微的纳米级成像，有助于推广经济、高效的超分辨率成像技术。

Abstract: The field of optical microscopy spans across numerous industries and research
domains, ranging from education to healthcare, quality inspection and analysis.
Nonetheless, a key limitation often cited by optical microscopists refers to
the limit of its lateral resolution (typically defined as ~200nm), with
potential circumventions involving either costly external modules (e.g.
confocal scan heads, etc) and/or specialized techniques [e.g. super-resolution
(SR) fluorescent microscopy]. Addressing these challenges in a normal
(non-specialist) context thus remains an aspect outside the scope of most
microscope users & facilities. This study thus seeks to evaluate an alternative
& economical approach to achieving SR optical microscopy, involving
non-fluorescent phase-modulated microscopical modalities such as Zernike phase
contrast (PCM) and differential interference contrast (DIC) microscopy. Two in
silico deep neural network (DNN) architectures which we developed previously
(termed O-Net and Theta-Net) are assessed on their abilities to resolve a
custom-fabricated test target containing nanoscale features calibrated via
atomic force microscopy (AFM). The results of our study demonstrate that
although both O-Net and Theta-Net seemingly performed well when super-resolving
these images, they were complementary (rather than competing) approaches to be
considered for image SR, particularly under different image signal-to-noise
ratios (SNRs). High image SNRs favoured the application of O-Net models, while
low SNRs inclined preferentially towards Theta-Net models. These findings
demonstrate the importance of model architectures (in conjunction with the
source image SNR) on model performance and the SR quality of the generated
images where DNN models are utilized for non-fluorescent optical nanoscopy,
even where the same training dataset & number of epochs are being used.

</details>


### [12] [Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation](https://arxiv.org/abs/2509.21377)
*Yinfeng Yu,Hailong Zhang,Meiling Zhu*

Main category: cs.CV

TL;DR: 提出了一种新的多模态信息融合方法，通过Transformer机制动态整合音频和视觉信息，有效提升机器人定位音源的能力，并在多个数据集上取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有的视听导航方法在多模态融合上较为简单，忽略了更深入的感知上下文，限制了机器人导航效率和适应性。

Method: 设计了名为DMTF-AVN的动态多目标融合架构，结合精细化的Transformer机制，对跨模态信息进行筛选与融合，从而更高效地指导机器人导航。

Result: 在Replica和Matterport3D数据集上，提出的方法在成功率（SR）、路径效率（SPL）和场景适应性（SNA）等指标上超过了现有方法，表现出良好的可扩展性和泛化能力。

Conclusion: DMTF-AVN有效提升了基于音视频的多模态机器人导航性能，为进一步多模态融合策略的发展奠定基础。

Abstract: Audiovisual embodied navigation enables robots to locate audio sources by
dynamically integrating visual observations from onboard sensors with the
auditory signals emitted by the target. The core challenge lies in effectively
leveraging multimodal cues to guide navigation. While prior works have explored
basic fusion of visual and audio data, they often overlook deeper perceptual
context. To address this, we propose the Dynamic Multi-Target Fusion for
Efficient Audio-Visual Navigation (DMTF-AVN). Our approach uses a multi-target
architecture coupled with a refined Transformer mechanism to filter and
selectively fuse cross-modal information. Extensive experiments on the Replica
and Matterport3D datasets demonstrate that DMTF-AVN achieves state-of-the-art
performance, outperforming existing methods in success rate (SR), path
efficiency (SPL), and scene adaptation (SNA). Furthermore, the model exhibits
strong scalability and generalizability, paving the way for advanced multimodal
fusion strategies in robotic navigation. The code and videos are available at
  https://github.com/zzzmmm-svg/DMTF.

</details>


### [13] [SAEmnesia: Erasing Concepts in Diffusion Models with Sparse Autoencoders](https://arxiv.org/abs/2509.21379)
*Enrico Cassano,Riccardo Renzulli,Marco Nurisso,Mirko Zaffaroni,Alan Perotti,Marco Grangetto*

Main category: cs.CV

TL;DR: 本文提出了一种提升文本到图像扩散模型中概念反学习效率的新方法SAEmnesia。通过有监督的稀疏自编码器训练，实现了更准确的概念定位和表示，大幅简化了概念反学习过程，同时在多个基准和任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在文本到图像扩散模型中进行特定概念反学习，需要精准定位模型中的概念表示。然而，现有的稀疏自编码器虽然减少了单元的多义性，但单个概念仍然分布于多个特征，导致反学习操作繁琐且低效。因此，亟需一种更能实现一一对应概念-神经元映射的方法，从而提升反学习效率。

Method: 提出SAEmnesia方法：通过有监督训练的稀疏自编码器，加入系统的概念标注，用交叉熵损失引导每一个神经元集中专注于单一概念，减少特征分裂，提升了神经元与概念的耦合度。该方法仅在训练阶段增加了交叉熵计算步骤，推理时不增加额外计算负担。

Result: SAEmnesia训练得到的神经元对概念的表征更专一，推理时可减少96.67%的超参数搜索工作量。在UnlearnCanvas基准上，相比最优方法性能提升9.22%。在9对象顺序反学习实验中，准确率提升28.4%，可扩展性更好。

Conclusion: SAEmnesia实现了更高效、可解释的概念反学习过程，大幅简化了实际操作步骤，并在各类评测中显著超越目前主流方法，对于需要频繁或大规模反学习的场景具有重要实用价值。

Abstract: Effective concept unlearning in text-to-image diffusion models requires
precise localization of concept representations within the model's latent
space. While sparse autoencoders successfully reduce neuron polysemanticity
(i.e., multiple concepts per neuron) compared to the original network,
individual concept representations can still be distributed across multiple
latent features, requiring extensive search procedures for concept unlearning.
We introduce SAEmnesia, a supervised sparse autoencoder training method that
promotes one-to-one concept-neuron mappings through systematic concept
labeling, mitigating feature splitting and promoting feature centralization.
Our approach learns specialized neurons with significantly stronger concept
associations compared to unsupervised baselines. The only computational
overhead introduced by SAEmnesia is limited to cross-entropy computation during
training. At inference time, this interpretable representation reduces
hyperparameter search by 96.67% with respect to current approaches. On the
UnlearnCanvas benchmark, SAEmnesia achieves a 9.22% improvement over the
state-of-the-art. In sequential unlearning tasks, we demonstrate superior
scalability with a 28.4% improvement in unlearning accuracy for 9-object
removal.

</details>


### [14] [Coreset selection based on Intra-class diversity](https://arxiv.org/abs/2509.21380)
*Imran Ashraf,Mukhtar Ullah,Muhammad Faisal Nadeem,Muhammad Nouman Noor*

Main category: cs.CV

TL;DR: 本文提出一种高效智能的子集选择（coreset）方法，用于加速和优化深度学习在生物医学图像分类中的训练流程，并展现了超越随机采样的性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习在生物医学图像分类中需要处理庞大数据集和多轮超参数搜索，导致显著的计算时间和资源消耗。传统的子集（coreset）选择如随机采样，难以保证样本的代表性，尤其中在类别不均或类内多样性问题。本文旨在探索更智能且高效的子集选择方案。

Method: 作者提出一种智能轻量化的子集（coreset）选择机制，能够挖掘每个类别的类内多样性。具体方法是先对每个类别进行聚类分析，并基于聚类结果进行样本选择，确保训练子集兼顾代表性和多样性。

Result: 在著名的生物医学图像数据集上进行大量实验，所提出的方法在多个指标下均优于传统的随机采样方法，验证了其实用性和有效性。

Conclusion: 该智能coreset选择方法能够有效减少训练计算资源，同时保持模型性能，对于大规模数据训练和参数搜索具有推广应用价值，优于常规随机采样方式。

Abstract: Deep Learning models have transformed various domains, including the
healthcare sector, particularly biomedical image classification by learning
intricate features and enabling accurate diagnostics pertaining to complex
diseases. Recent studies have adopted two different approaches to train DL
models: training from scratch and transfer learning. Both approaches demand
substantial computational time and resources due to the involvement of massive
datasets in model training. These computational demands are further increased
due to the design-space exploration required for selecting optimal
hyperparameters, which typically necessitates several training rounds. With the
growing sizes of datasets, exploring solutions to this problem has recently
gained the research community's attention. A plausible solution is to select a
subset of the dataset for training and hyperparameter search. This subset,
referred to as the corset, must be a representative set of the original
dataset. A straightforward approach to selecting the coreset could be employing
random sampling, albeit at the cost of compromising the representativeness of
the original dataset. A critical limitation of random sampling is the bias
towards the dominant classes in an imbalanced dataset. Even if the dataset has
inter-class balance, this random sampling will not capture intra-class
diversity. This study addresses this issue by introducing an intelligent,
lightweight mechanism for coreset selection. Specifically, it proposes a method
to extract intra-class diversity, forming per-class clusters that are utilized
for the final sampling. We demonstrate the efficacy of the proposed methodology
by conducting extensive classification experiments on a well-known biomedical
imaging dataset. Results demonstrate that the proposed scheme outperforms the
random sampling approach on several performance metrics for uniform conditions.

</details>


### [15] [The LongiMam model for improved breast cancer risk prediction using longitudinal mammograms](https://arxiv.org/abs/2509.21383)
*Manel Rakez,Thomas Louis,Julien Guillaumin,Foucauld Chamming's,Pierre Fillard,Brice Amadeo,Virginie Rondeau*

Main category: cs.CV

TL;DR: 本文提出了一种名为LongiMam的端到端深度学习模型，能够结合当前及最多4次先前乳腺X光片，有效提升乳腺癌风险预测性能。该模型在大规模筛查真实数据集上，尤其是在不平衡样本环境中表现出色，结果表明结合历史与当前影像可提升预测准确度，并已开源。


<details>
  <summary>Details</summary>
Motivation: 当前多数深度学习乳腺癌筛查模型仅使用单次或极少数历史影像，且缺乏对真实世界数据不平衡与随访多样化的适应。为提升在实际人群筛查环境下的风险预测能力，需要开发能充分利用纵向（多时点）影像信息的模型。

Method: 研究提出LongiMam模型，将卷积神经网络与循环神经网络结合，能够处理当前及最多4次历史乳腺X光影像，学习空间及时间演变特征。模型在包含严重类别不平衡的大规模实际乳腺癌筛查数据集上进行训练和评估，实验设计涉及不同数量及组合的历史影像。

Result: 实验结果显示，模型每纳入一次历史影像，预测性能均有提升。结合当前与历史影像的模型优于仅用当前影像，单用历史影像不如联合模型。亚组分析显示，该方法对高风险组（如乳腺致密女性、≥55岁人群）同样有效，且对乳腺密度变化者表现最佳。

Conclusion: 使用多时点乳腺X光片可显著提升乳腺癌风险预测，支持在筛查项目中利用重复影像优化个体风险分层。LongiMam模型为开源，推动纵向智能医学影像分析应用。

Abstract: Risk-adapted breast cancer screening requires robust models that leverage
longitudinal imaging data. Most current deep learning models use single or
limited prior mammograms and lack adaptation for real-world settings marked by
imbalanced outcome distribution and heterogeneous follow-up. We developed
LongiMam, an end-to-end deep learning model that integrates both current and up
to four prior mammograms. LongiMam combines a convolutional and a recurrent
neural network to capture spatial and temporal patterns predictive of breast
cancer. The model was trained and evaluated using a large, population-based
screening dataset with disproportionate case-to-control ratio typical of
clinical screening. Across several scenarios that varied in the number and
composition of prior exams, LongiMam consistently improved prediction when
prior mammograms were included. The addition of prior and current visits
outperformed single-visit models, while priors alone performed less well,
highlighting the importance of combining historical and recent information.
Subgroup analyses confirmed the model's efficacy across key risk groups,
including women with dense breasts and those aged 55 years or older. Moreover,
the model performed best in women with observed changes in mammographic density
over time. These findings demonstrate that longitudinal modeling enhances
breast cancer prediction and support the use of repeated mammograms to refine
risk stratification in screening programs. LongiMam is publicly available as
open-source software.

</details>


### [16] [Assessing the Alignment of Popular CNNs to the Brain for Valence Appraisal](https://arxiv.org/abs/2509.21384)
*Laurent Mertens,Elahe' Yargholi,Laura Van Hove,Hans Op de Beeck,Jan Van den Stock,Joost Vennekens*

Main category: cs.CV

TL;DR: 本论文研究了卷积神经网络（CNNs）与人脑在社会认知任务中的对应关系，发现CNN在更复杂的情感评价任务上难以反映高级脑加工，并提出了Object2Brain分析不同对象类别对CNN与人脑相关性影响的新方法。


<details>
  <summary>Details</summary>
Motivation: 虽然过去研究表明CNN和人脑在视觉感知上的处理策略有相似性，但这些对应性主要局限于一般视觉感知，尚不清楚在社会认知等更复杂脑功能上，CNN是否依旧与人脑表现一致。因此，作者希望探究CNN与人类在图像情感评价（valence appraisal）这一更复杂认知任务中的对应关系。

Method: 作者通过对经典CNN结构（未具体点明哪些）与人类行为数据及fMRI脑成像数据，在图像情感评价任务上的表现进行相关性分析，来评估CNN与大脑处理的相似程度。同时，作者提出了Object2Brain框架，结合GradCAM显著性方法、对象检测和相关性分析，研究不同对象类别对CNN与人脑相关性的影响和敏感性。

Result: 实验结果表明，CNN在图像情感评价任务上难以超越简单的视觉处理水平，未能表现出与人脑高级社会认知相关的处理机制。尽管不同CNN模型与人类数据表现出类似的相关性趋势，但各模型对不同对象类别的敏感性存在差异。

Conclusion: 本研究发现CNN在复杂的社会认知任务上无法充分模拟人脑的高级加工过程，并通过Object2Brain框架展示了不同网络结构在对象类别敏感性上的区别，提示未来提升神经网络在社会认知等更高层次任务中的表现仍面临挑战。

Abstract: Convolutional Neural Networks (CNNs) are a popular type of computer model
that have proven their worth in many computer vision tasks. Moreover, they form
an interesting study object for the field of psychology, with shown
correspondences between the workings of CNNs and the human brain. However,
these correspondences have so far mostly been studied in the context of general
visual perception. In contrast, this paper explores to what extent this
correspondence also holds for a more complex brain process, namely social
cognition. To this end, we assess the alignment between popular CNN
architectures and both human behavioral and fMRI data for image valence
appraisal through a correlation analysis. We show that for this task CNNs
struggle to go beyond simple visual processing, and do not seem to reflect
higher-order brain processing. Furthermore, we present Object2Brain, a novel
framework that combines GradCAM and object detection at the CNN-filter level
with the aforementioned correlation analysis to study the influence of
different object classes on the CNN-to-human correlations. Despite similar
correlation trends, different CNN architectures are shown to display different
object class sensitivities.

</details>


### [17] [Debugging Concept Bottleneck Models through Removal and Retraining](https://arxiv.org/abs/2509.21385)
*Eric Enouen,Sainyam Galhotra*

Main category: cs.CV

TL;DR: 提出了一种通用的可解释调试框架CBDebug，能有效修正CBM模型与专家推理之间的错位，减少模型对不良概念的依赖，并在多种基准下优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 传统的概念瓶颈模型（CBMs）虽然在预测过程中引入了可解释的中间概念，方便专家介入和修正，但当模型出现系统性偏差或学习到错误的关联（如数据偏见导致的投机取巧）时，只修改单个概念并不能从根本上解决模型与专家推理不一致的问题。为减少模型对有害概念的依赖、增强专家调试能力，需要更有效的策略。

Method: 提出了两步走的调试框架：第一步“去除”，专家根据概念层面的解释，识别并移除不希望模型依赖的概念；第二步“再训练”，通过CBDebug方法，将专家在概念层面的反馈转化为具体样本的辅助标签，据此对模型进行偏见纠正和数据增强，从而修正模型对错误概念的依赖。该方法充分利用CBM的可解释性，实现更细致的监督。

Result: 在PIP-Net、Post-hoc CBM等多种CBM架构及涉及已知偏差的多个评测基准上，CBDebug方法显著优于以往的再训练方法，不论是用真人专家还是自动化专家反馈都表现良好，有效降低了模型对有害概念的依赖。

Conclusion: CBDebug作为一种通用、可解释的CBM调试框架，显著提升了模型的健壮性与可控性，为专家干预带来更高效、系统的手段。未来该框架有望被广泛应用于需要高可靠性和专家参与的场景中。

Abstract: Concept Bottleneck Models (CBMs) use a set of human-interpretable concepts to
predict the final task label, enabling domain experts to not only validate the
CBM's predictions, but also intervene on incorrect concepts at test time.
However, these interventions fail to address systemic misalignment between the
CBM and the expert's reasoning, such as when the model learns shortcuts from
biased data. To address this, we present a general interpretable debugging
framework for CBMs that follows a two-step process of Removal and Retraining.
In the Removal step, experts use concept explanations to identify and remove
any undesired concepts. In the Retraining step, we introduce CBDebug, a novel
method that leverages the interpretability of CBMs as a bridge for converting
concept-level user feedback into sample-level auxiliary labels. These labels
are then used to apply supervised bias mitigation and targeted augmentation,
reducing the model's reliance on undesired concepts. We evaluate our framework
with both real and automated expert feedback, and find that CBDebug
significantly outperforms prior retraining methods across multiple CBM
architectures (PIP-Net, Post-hoc CBM) and benchmarks with known spurious
correlations.

</details>


### [18] [ShipwreckFinder: A QGIS Tool for Shipwreck Detection in Multibeam Sonar Data](https://arxiv.org/abs/2509.21386)
*Anja Sheppard,Tyler Smithline,Andrew Scheffer,David Smith,Advaith V. Sethuraman,Ryan Bird,Sabrina Lin,Katherine A. Skinner*

Main category: cs.CV

TL;DR: 该论文提出了一款名为ShipwreckFinder的开源QGIS插件，用于自动化检测多波束声呐数据中的沉船，提高了沉船发现的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 沉船是重要的海事历史标记，而通过人工检查测深数据发现沉船既耗时又需专家参与，因此需要一种自动化方法来简化和加速这一过程。

Method: 作者开发了一款基于深度学习的QGIS插件，可以自动对测深数据进行预处理、模型推理、阈值分割，并输出像素级分割掩码或目标边框。模型在来自大湖区和爱尔兰沿海的多样化沉船数据上训练，并通过合成数据扩充数据集。

Result: 该工具与基于深度学习的ArcGIS工具包和传统逆向陷坑检测方法对比，展现出更优的分割性能。

Conclusion: ShipwreckFinder插件有效提升了沉船自动检测的效率和精度，为相关研究和应用提供了开源、便捷的工具。

Abstract: In this paper, we introduce ShipwreckFinder, an open-source QGIS plugin that
detects shipwrecks from multibeam sonar data. Shipwrecks are an important
historical marker of maritime history, and can be discovered through manual
inspection of bathymetric data. However, this is a time-consuming process and
often requires expert analysis. Our proposed tool allows users to automatically
preprocess bathymetry data, perform deep learning inference, threshold model
outputs, and produce either pixel-wise segmentation masks or bounding boxes of
predicted shipwrecks. The backbone of this open-source tool is a deep learning
model, which is trained on a variety of shipwreck data from the Great Lakes and
the coasts of Ireland. Additionally, we employ synthetic data generation in
order to increase the size and diversity of our dataset. We demonstrate
superior segmentation performance with our open-source tool and training
pipeline as compared to a deep learning-based ArcGIS toolkit and a more
classical inverse sinkhole detection method. The open-source tool can be found
at https://github.com/umfieldrobotics/ShipwreckFinderQGISPlugin.

</details>


### [19] [Do Sparse Subnetworks Exhibit Cognitively Aligned Attention? Effects of Pruning on Saliency Map Fidelity, Sparsity, and Concept Coherence](https://arxiv.org/abs/2509.21387)
*Sanish Suwal,Dipkamal Bhusal,Michael Clifford,Nidhi Rastogi*

Main category: cs.CV

TL;DR: 本文研究了神经网络剪枝对模型可解释性的影响，发现适度剪枝能提升模型内部表征的可解释性，但过度剪枝则适得其反。


<details>
  <summary>Details</summary>
Motivation: 虽然神经网络可以通过剪枝大幅减少参数数量且维持性能，但剪枝对模型可解释性的影响仍不清楚。论文旨在系统分析剪枝对模型解释结果（如显著性图与概念表征）的影响。

Method: 作者使用在ImageNette数据集上训练的ResNet-18，并对其进行基于权重幅度的剪枝及微调。对不同剪枝程度的模型，分别采用Vanilla Gradients (VG) 和 Integrated Gradients (IG) 获得显著性图，分析其稀疏性和忠实性。还用CRAFT方法来跟踪语义概念变化，评估概念的语义一致性。

Result: 实验发现，轻度和中度剪枝可提升显著性图的聚焦性和忠实性，并保持清晰且有意义的概念表征；而过度剪枝则导致特征混合、显著性图变得不稀疏、概念一致性下降，尽管准确率未下降。

Conclusion: 适度剪枝有助于使模型关注更加符合人类的注意力模式，从而提升可解释性；然而剪枝过度会破坏这一优势，不利于模型解释。

Abstract: Prior works have shown that neural networks can be heavily pruned while
preserving performance, but the impact of pruning on model interpretability
remains unclear. In this work, we investigate how magnitude-based pruning
followed by fine-tuning affects both low-level saliency maps and high-level
concept representations. Using a ResNet-18 trained on ImageNette, we compare
post-hoc explanations from Vanilla Gradients (VG) and Integrated Gradients (IG)
across pruning levels, evaluating sparsity and faithfulness. We further apply
CRAFT-based concept extraction to track changes in semantic coherence of
learned concepts. Our results show that light-to-moderate pruning improves
saliency-map focus and faithfulness while retaining distinct, semantically
meaningful concepts. In contrast, aggressive pruning merges heterogeneous
features, reducing saliency map sparsity and concept coherence despite
maintaining accuracy. These findings suggest that while pruning can shape
internal representations toward more human-aligned attention patterns,
excessive pruning undermines interpretability.

</details>


### [20] [TUN3D: Towards Real-World Scene Understanding from Unposed Images](https://arxiv.org/abs/2509.21388)
*Anton Konushin,Nikita Drozdov,Bulat Gabdullin,Alexey Zakharov,Anna Vorontsova,Danila Rukhovich,Maksim Kolodiazhnyi*

Main category: cs.CV

TL;DR: 本文提出了TUN3D，一种不依赖深度信息或相机位姿，基于多视角图像实现室内场景三维物体检测与布局估计的端到端方法，并在多个基准上实现了新的业界性能纪录。


<details>
  <summary>Details</summary>
Motivation: 现有的室内三维场景理解方法通常依赖点云作为输入，但大多数消费级相机并没有深度传感器，因此需要仅依赖视觉数据的新方法。

Method: TUN3D采用轻量级稀疏卷积骨干网络，并设计了两个独立的头部结构，分别用于3D物体检测和布局估计。同时，提出了新颖有效的参数化墙体表示法。输入只需要多视角RGB图像，无需真实的相机位姿或深度监督。

Result: 通过在三个有挑战性的基准（基于真实点云、已知位姿图像、未知位姿图像）上的大量实验，TUN3D均取得了最先进的性能，尤其在联动三维物体检测和布局恢复方面表现突出。

Conclusion: TUN3D不仅在3D物体检测方面达到与专用方法相当的水平，还极大推进了室内布局估计，成为端到端室内场景理解的新基准。

Abstract: Layout estimation and 3D object detection are two fundamental tasks in indoor
scene understanding. When combined, they enable the creation of a compact yet
semantically rich spatial representation of a scene. Existing approaches
typically rely on point cloud input, which poses a major limitation since most
consumer cameras lack depth sensors and visual-only data remains far more
common. We address this issue with TUN3D, the first method that tackles joint
layout estimation and 3D object detection in real scans, given multi-view
images as input, and does not require ground-truth camera poses or depth
supervision. Our approach builds on a lightweight sparse-convolutional backbone
and employs two dedicated heads: one for 3D object detection and one for layout
estimation, leveraging a novel and effective parametric wall representation.
Extensive experiments show that TUN3D achieves state-of-the-art performance
across three challenging scene understanding benchmarks: (i) using ground-truth
point clouds, (ii) using posed images, and (iii) using unposed images. While
performing on par with specialized 3D object detection methods, TUN3D
significantly advances layout estimation, setting a new benchmark in holistic
indoor scene understanding. Code is available at
https://github.com/col14m/tun3d .

</details>


### [21] [Large AI Model-Enabled Generative Semantic Communications for Image Transmission](https://arxiv.org/abs/2509.21394)
*Qiyu Ma,Wanli Ni,Zhijin Qin*

Main category: cs.CV

TL;DR: 提出了一种生成式语义通信系统，通过区分图像关键和非关键区域，提升了图像传输中的语义完整性和视觉质量。采用轻量化AI部署方式，兼顾了资源利用和性能。


<details>
  <summary>Details</summary>
Motivation: 传统语义通信方法未充分考虑图像中各区域的重要性差异，导致视觉关键内容重建质量受损。

Method: 将图像分割为关键区与非关键区。关键区采用面向图像的语义编码器处理，非关键区则通过图像转文本的建模方式高效压缩。同时，为减轻大模型部署的资源消耗，采用模型量化和低秩自适应微调实现轻量化部署。

Result: 仿真结果显示，所提系统在语义忠实度和视觉质量方面均优于传统方法。

Conclusion: 该系统有效提升了图像传输的语义完整性和视觉质量，同时兼顾了计算与存储资源的高效利用，适合相关应用场景。

Abstract: The rapid development of generative artificial intelligence (AI) has
introduced significant opportunities for enhancing the efficiency and accuracy
of image transmission within semantic communication systems. Despite these
advancements, existing methodologies often neglect the difference in importance
of different regions of the image, potentially compromising the reconstruction
quality of visually critical content. To address this issue, we introduce an
innovative generative semantic communication system that refines semantic
granularity by segmenting images into key and non-key regions. Key regions,
which contain essential visual information, are processed using an image
oriented semantic encoder, while non-key regions are efficiently compressed
through an image-to-text modeling approach. Additionally, to mitigate the
substantial storage and computational demands posed by large AI models, the
proposed system employs a lightweight deployment strategy incorporating model
quantization and low-rank adaptation fine-tuning techniques, significantly
boosting resource utilization without sacrificing performance. Simulation
results demonstrate that the proposed system outperforms traditional methods in
terms of both semantic fidelity and visual quality, thereby affirming its
effectiveness for image transmission tasks.

</details>


### [22] [mmHSense: Multi-Modal and Distributed mmWave ISAC Datasets for Human Sensing](https://arxiv.org/abs/2509.21396)
*Nabeel Nisar Bhat,Maksim Karnaukh,Stein Vandenbroeke,Wouter Lemoine,Jakob Struye,Jesus Omar Lacruz,Siddhartha Kumar,Mohammad Hossein Moghaddam,Joerg Widmer,Rafael Berkvens,Jeroen Famaey*

Main category: cs.CV

TL;DR: 本文介绍了mmHSense，这是一套开放标注的毫米波(mmWave)数据集，旨在支持综合感知与通信(ISAC)系统中的人体感知研究，并通过下游任务验证其实用性。


<details>
  <summary>Details</summary>
Motivation: 毫米波在ISAC系统中具有广泛应用潜力，但缺乏公开标注数据集阻碍了该领域，如手势识别、人体识别、姿态估计和定位等任务的研究和技术进步。

Method: 构建并公开了一套涵盖多种应用的毫米波ISAC标注数据集，详细说明了测试平台、实验设置和信号特征。同时，通过具体下游任务对数据集实用性进行验证，并提出了高效参数微调方法，实现不同任务间模型迁移，降低了计算复杂度。

Result: 实验表明，所发布的数据集能够支持多种人体感知任务，并且所提出的参数高效微调方法在减少计算量的同时，有效保持了原有任务的性能。

Conclusion: mmHSense数据集为毫米波ISAC领域相关研究提供了重要基础，有助于推动信号处理与深度学习在该领域中的应用和进步。

Abstract: This article presents mmHSense, a set of open labeled mmWave datasets to
support human sensing research within Integrated Sensing and Communication
(ISAC) systems. The datasets can be used to explore mmWave ISAC for various end
applications such as gesture recognition, person identification, pose
estimation, and localization. Moreover, the datasets can be used to develop and
advance signal processing and deep learning research on mmWave ISAC. This
article describes the testbed, experimental settings, and signal features for
each dataset. Furthermore, the utility of the datasets is demonstrated through
validation on a specific downstream task. In addition, we demonstrate the use
of parameter-efficient fine-tuning to adapt ISAC models to different tasks,
significantly reducing computational complexity while maintaining performance
on prior tasks.

</details>


### [23] [Skeleton Sparsification and Densification Scale-Spaces](https://arxiv.org/abs/2509.21398)
*Julia Gierke,Pascal Peter*

Main category: cs.CV

TL;DR: 本文提出了一种新的骨架化尺度空间框架，实现了骨架的可控分层简化，能有效抗噪并适用于多种实际任务。


<details>
  <summary>Details</summary>
Motivation: 传统的Hamilton-Jacobi骨架（骨架轴）对边界噪声非常敏感，小的扰动会导致骨架过度扩展。虽然有经典的剪枝方法，但仍希望有更系统、分层、可控且对几何变换保持一致性的骨架简化方案。

Method: 作者将骨架稀疏化（类似于尺度空间稀疏化图像像素）引入骨架化过程，提出“骨架化尺度空间”方法。该方法理论上支持连续和离散表达，并可以通过“致密化”逆向从粗到细，甚至扩展至超越原始骨架，获得更丰富的形状表示。

Result: 作者通过原型实验展示了该框架在鲁棒骨架提取、形状压缩和3D打印加固等实际任务中的有效性。理论分析保证了方法的层次性、简化可控性和对几何变换的等变性。

Conclusion: 本文新框架克服了传统骨架对于噪声敏感的问题，实现了骨架的多层次、可控和可逆简化，可以满足实际应用中对骨架表达的多样需求。

Abstract: The Hamilton-Jacobi skeleton, also known as the medial axis, is a powerful
shape descriptor that represents binary objects in terms of the centres of
maximal inscribed discs. Despite its broad applicability, the medial axis
suffers from sensitivity to noise: minor boundary variations can lead to
disproportionately large and undesirable expansions of the skeleton. Classical
pruning methods mitigate this shortcoming by systematically removing extraneous
skeletal branches. This sequential simplification of skeletons resembles the
principle of sparsification scale-spaces that embed images into a family of
reconstructions from increasingly sparse pixel representations.
  We combine both worlds by introducing skeletonisation scale-spaces: They
leverage sparsification of the medial axis to achieve hierarchical
simplification of shapes. Unlike conventional pruning, our framework inherently
satisfies key scale-space properties such as hierarchical architecture,
controllable simplification, and equivariance to geometric transformations. We
provide a rigorous theoretical foundation in both continuous and discrete
formulations and extend the concept further with densification. This allows
inverse progression from coarse to fine scales and can even reach beyond the
original skeleton to produce overcomplete shape representations with relevancy
for practical applications.
  Through proof-of-concept experiments, we demonstrate the effectiveness of our
framework for practical tasks including robust skeletonisation, shape
compression, and stiffness enhancement for additive manufacturing.

</details>


### [24] [Downscaling climate projections to 1 km with single-image super resolution](https://arxiv.org/abs/2509.21399)
*Petr Košťál,Pavel Kordík,Ondřej Podsztavek*

Main category: cs.CV

TL;DR: 本文提出利用单幅图像超分辨率模型，将气候模型投影数据空间分辨率从12.5公里提升到1公里，以更好支持本地决策。作者提出了一种新的基于气候指标的方法，评估下采样结果，而不依赖高分辨率气候投影的真实标签。实验证明，该方法在温度数据的下采样中不会增加气候指标的误差。


<details>
  <summary>Details</summary>
Motivation: 现有气候模型投影的空间分辨率较低，不足以满足本地化决策需求，而直接获取高分辨率气候预测又不可行，因此需要通过统计方法实现空间下采样。

Method: 作者采用单幅图像超分辨率（single-image super-resolution）深度学习模型，在高分辨率观测数据集上进行训练，并将模型用于低分辨率气候预测的下采样。同时，他们设计了一种基于气候指标的评估方法，通过对比观测站点的气候指数，来评估下采样效果，无需高分辨率气候预测的真实标签。

Result: 实验证明，单幅图像超分辨率模型能够有效地将温度类气候预测下采样到1公里分辨率，并且不会引入比原始低分辨率数据更大的气候指数误差。

Conclusion: 单幅图像超分辨率深度学习模型能够实现在无高分辨率标签情况下，对气候投影数据的高精度空间下采样，提升气候模型对本地决策的支撑能力。提出的气候指标评估方式具有实际可操作性。

Abstract: High-resolution climate projections are essential for local decision-making.
However, available climate projections have low spatial resolution (e.g. 12.5
km), which limits their usability. We address this limitation by leveraging
single-image super-resolution models to statistically downscale climate
projections to 1-km resolution. Since high-resolution climate projections are
unavailable for training, we train models on a high-resolution observational
gridded data set and apply them to low-resolution climate projections. We
propose a climate indicator-based assessment using observed climate indices
computed at weather station locations to evaluate the downscaled climate
projections without ground-truth high-resolution climate projections.
Experiments on daily mean temperature demonstrate that single-image
super-resolution models can downscale climate projections without increasing
the error of climate indicators compared to low-resolution climate projections.

</details>


### [25] [JaiLIP: Jailbreaking Vision-Language Models via Loss Guided Image Perturbation](https://arxiv.org/abs/2509.21401)
*Md Jueal Mia,M. Hadi Amini*

Main category: cs.CV

TL;DR: 本文提出了一种新的针对视觉-语言模型（VLMs）的图像空间越狱攻击方法JaiLIP，并证实该方法能高效且隐蔽地产生有害输出，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前VLMs在多模态推理任务中表现突出，但也存在日益增长的安全和滥用风险，特别是图像扰动可以较容易地破解这些模型。面对现有攻防方法的不稳定和可见性高的问题，作者希望提出一种更有效、隐蔽的攻击手段。

Method: 作者提出了JaiLIP攻击方法，通过最小化干净图像和对抗图像的均方误差（MSE）损失以及模型有害输出的损失相结合的目标函数，生成既难以分辨又有效的对抗性图像。方法在VLMs上进行评估，并利用Perspective API和Detoxify的毒性指标进行效果验证。

Result: 实验显示，JaiLIP方法生成的对抗图像在诱发有害输出方面比现有方法更高效且更不易被察觉。此外，该方法还被应用于交通领域，证明其不仅能生成毒性文本，还具备跨领域实际攻击能力。

Conclusion: JaiLIP展示了图像扰动对VLMs攻防的实际挑战，突显了发展高效防护机制的重要性。

Abstract: Vision-Language Models (VLMs) have remarkable abilities in generating
multimodal reasoning tasks. However, potential misuse or safety alignment
concerns of VLMs have increased significantly due to different categories of
attack vectors. Among various attack vectors, recent studies have demonstrated
that image-based perturbations are particularly effective in generating harmful
outputs. In the literature, many existing techniques have been proposed to
jailbreak VLMs, leading to unstable performance and visible perturbations. In
this study, we propose Jailbreaking with Loss-guided Image Perturbation
(JaiLIP), a jailbreaking attack in the image space that minimizes a joint
objective combining the mean squared error (MSE) loss between clean and
adversarial image with the models harmful-output loss. We evaluate our proposed
method on VLMs using standard toxicity metrics from Perspective API and
Detoxify. Experimental results demonstrate that our method generates highly
effective and imperceptible adversarial images, outperforming existing methods
in producing toxicity. Moreover, we have evaluated our method in the
transportation domain to demonstrate the attacks practicality beyond toxic text
generation in specific domain. Our findings emphasize the practical challenges
of image-based jailbreak attacks and the need for efficient defense mechanisms
for VLMs.

</details>


### [26] [Overview of ExpertLifeCLEF 2018: how far automated identification systems are from the best experts?](https://arxiv.org/abs/2509.21419)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: 本论文介绍了LifeCLEF 2018 ExpertCLEF挑战赛，通过对比19个深度学习系统和9位植物学专家对法国本地植物的识别表现，评估了自动化系统与人类专家能力的差距。结果显示，当前最先进的深度学习模型性能已接近人类专家。


<details>
  <summary>Details</summary>
Motivation: 近年来深度学习推动了动植物自动识别技术巨大进步，但尚不清楚这些系统与人类专家相比有多大差距。考虑到即使专业人士遇到部分信息时也会产生分歧，准确评估自动系统与专家对不确定性的判断能力具有重要意义，有助于推动领域发展。

Method: 设计了LifeCLEF 2018 ExpertCLEF挑战赛，将19个由4支研究团队开发的深度学习系统与9位法国植物学专家在植物图片识别任务上的表现进行对比评估。通过设定标准的数据资源、评测方式，并系统总结参赛团队的技术路径。

Result: 实验结果显示，最先进的深度学习模型在大部分场景下，其识别准确率已非常接近人类高级专家水平。

Conclusion: 深度学习自动识别系统在动植物领域已经取得了接近专家水平的突破，无论在准确性还是一致性方面都显示出巨大潜力，将来有望在生态调查和物种保护等实际应用中发挥重要作用。

Abstract: Automated identification of plants and animals has improved considerably in
the last few years, in particular thanks to the recent advances in deep
learning. The next big question is how far such automated systems are from the
human expertise. Indeed, even the best experts are sometimes confused and/or
disagree between each others when validating visual or audio observations of
living organism. A picture actually contains only a partial information that is
usually not sufficient to determine the right species with certainty.
Quantifying this uncertainty and comparing it to the performance of automated
systems is of high interest for both computer scientists and expert
naturalists. The LifeCLEF 2018 ExpertCLEF challenge presented in this paper was
designed to allow this comparison between human experts and automated systems.
In total, 19 deep-learning systems implemented by 4 different research teams
were evaluated with regard to 9 expert botanists of the French flora. The main
outcome of this work is that the performance of state-of-the-art deep learning
models is now close to the most advanced human expertise. This paper presents
more precisely the resources and assessments of the challenge, summarizes the
approaches and systems employed by the participating research groups, and
provides an analysis of the main outcomes.

</details>


### [27] [QuadGPT: Native Quadrilateral Mesh Generation with Autoregressive Models](https://arxiv.org/abs/2509.21420)
*Jian Liu,Chunshi Wang,Song Guo,Haohan Weng,Zhen Zhou,Zhiqi Li,Jiaao Yu,Yiling Zhu,Jing Xu,Biwen Lei,Zhuo Chen,Chunchao Guo*

Main category: cs.CV

TL;DR: 本文提出了QuadGPT，这是首个可端到端生成四边形网格的自回归框架，在网格几何和拓扑质量上均优于现有三角网格转四边形网格的方法。


<details>
  <summary>Details</summary>
Motivation: 在3D内容创作中，四边形主导的网格生成是核心步骤，但目前方法需先生成三角网格再合并为四边形，导致最终四边形网格的拓扑结构品质较差。

Method: 1. QuadGPT首次将四边形网格生成建模为序列预测问题；2. 提出统一的token化方案，能编码混合三角形与四边形拓扑；3. 采用专用的强化学习微调方法tDPO，进一步提升生成质量。

Result: 大量实验表明，QuadGPT无论在几何精度还是四边形网格的拓扑结构质量上都显著优于以往三角到四边形的转换流程。

Conclusion: QuadGPT为原生四边形网格生成设立了新标杆，验证了大规模自回归模型结合基于拓扑感知RL微调在结构化3D资产生成上的巨大潜力。

Abstract: The generation of quadrilateral-dominant meshes is a cornerstone of
professional 3D content creation. However, existing generative models generate
quad meshes by first generating triangle meshes and then merging triangles into
quadrilaterals with some specific rules, which typically produces quad meshes
with poor topology. In this paper, we introduce QuadGPT, the first
autoregressive framework for generating quadrilateral meshes in an end-to-end
manner. QuadGPT formulates this as a sequence prediction paradigm,
distinguished by two key innovations: a unified tokenization method to handle
mixed topologies of triangles and quadrilaterals, and a specialized
Reinforcement Learning fine-tuning method tDPO for better generation quality.
Extensive experiments demonstrate that QuadGPT significantly surpasses previous
triangle-to-quad conversion pipelines in both geometric accuracy and
topological quality. Our work establishes a new benchmark for native quad-mesh
generation and showcases the power of combining large-scale autoregressive
models with topology-aware RL refinement for creating structured 3D assets.

</details>


### [28] [DyME: Dynamic Multi-Concept Erasure in Diffusion Models with Bi-Level Orthogonal LoRA Adaptation](https://arxiv.org/abs/2509.21433)
*Jiaqi Liu,Lan Zhang,Xiaoyong Yuan*

Main category: cs.CV

TL;DR: 本论文提出DyME框架，通过动态组合概念特定的LoRA适配器，实现按需擦除扩散模型中的受保护视觉概念，解决现有方法多概念难以灵活擦除的问题。实验表明，DyME在多概念擦除和内容保真度方面优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型容易复制受版权保护的风格和概念，带来法律与伦理隐患。已有的概念擦除方法只能静态地一次性擦除多概念，不适应实际需求下动态、精细化擦除多种、甚至相互冲突的概念，且容易导致非目标内容质量下降。因此需一种更灵活高效的擦除方案。

Method: DyME采用轻量级、概念特定的LoRA适配器并按需动态组合，从而根据推理时实际擦除需求灵活抑制多个概念。为解决多适配器组合时的干扰，提出特征和参数层面的双层正交约束，使每个适配器作用在正交子空间上，降低适配器间的相互影响。此外，开发了分层擦除基准ErasureBench-H，以支持不同语义层级和擦除规模的系统评测。

Result: 在ErasureBench-H及主流数据集如CIFAR-100和Imagenette上的实验结果显示，DyME实现了比现有最佳方法更高的多概念擦除效果，同时对非目标内容影响最小。

Conclusion: DyME打破了传统静态擦除的局限，实现了多概念、按需动态擦除扩散模型敏感内容，提升了实际应用的扩展性和安全性，为生成模型安全合规提供了新方案。

Abstract: Text-to-image diffusion models (DMs) inadvertently reproduce copyrighted
styles and protected visual concepts, raising legal and ethical concerns.
Concept erasure has emerged as a safeguard, aiming to selectively suppress such
concepts through fine-tuning. However, existing methods do not scale to
practical settings where providers must erase multiple and possibly conflicting
concepts. The core bottleneck is their reliance on static erasure: a single
checkpoint is fine-tuned to remove all target concepts, regardless of the
actual erasure needs at inference. This rigid design mismatches real-world
usage, where requests vary per generation, leading to degraded erasure success
and reduced fidelity for non-target content. We propose DyME, an on-demand
erasure framework that trains lightweight, concept-specific LoRA adapters and
dynamically composes only those needed at inference. This modular design
enables flexible multi-concept erasure, but naive composition causes
interference among adapters, especially when many or semantically related
concepts are suppressed. To overcome this, we introduce bi-level orthogonality
constraints at both the feature and parameter levels, disentangling
representation shifts and enforcing orthogonal adapter subspaces. We further
develop ErasureBench-H, a new hierarchical benchmark with
brand-series-character structure, enabling principled evaluation across
semantic granularities and erasure set sizes. Experiments on ErasureBench-H and
standard datasets (e.g., CIFAR-100, Imagenette) demonstrate that DyME
consistently outperforms state-of-the-art baselines, achieving higher
multi-concept erasure fidelity with minimal collateral degradation.

</details>


### [29] [VideoJudge: Bootstrapping Enables Scalable Supervision of MLLM-as-a-Judge for Video Understanding](https://arxiv.org/abs/2509.21451)
*Abdul Waheed,Zhen Wu,Dareen Alharthi,Seungone Kim,Bhiksha Raj*

Main category: cs.CV

TL;DR: 提出了一种专门用于评估视频理解模型输出的多模态大模型（VideoJudge），相比目前主流的评价方法和大模型评委，表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前主流的视频理解模型评价指标（如BLEU、ROUGE、BERTScore）无法精准反映人工评价的细致性，而人工评价又成本高昂。最近尝试用大模型作为自动评委，但在视频理解领域应用有限。

Method: 提出了一种3B和7B规模的多模态大模型评委（VideoJudge），通过生成–评审协作的训练方式，只保留评分一致的样本，提升模型作为视频理解评委的能力。

Result: 在四个元评估基准中，VideoJudge-7B在三项上超过了更大参数量的MLLM评委（如Qwen2.5-VL 32B/72B），也发现LLM评委在视频理解任务中不如MLLM评委。连贯推理步骤过长也未带来性能提升。

Conclusion: 提供视频输入对视频理解任务的评价至关重要，专用MLLM评委如VideoJudge效果显著优于传统指标和仅文本大模型评委。

Abstract: Precisely evaluating video understanding models remains challenging: commonly
used metrics such as BLEU, ROUGE, and BERTScore fail to capture the fineness of
human judgment, while obtaining such judgments through manual evaluation is
costly. Recent work has explored using large language models (LLMs) or
multimodal LLMs (MLLMs) as evaluators, but their extension to video
understanding remains relatively unexplored. In this work, we introduce
VideoJudge, a 3B and 7B-sized MLLM judge specialized to evaluate outputs from
video understanding models (\textit{i.e.}, text responses conditioned on
videos). To train VideoJudge, our recipe builds on the interplay between a
generator and an evaluator: the generator is prompted to produce responses
conditioned on a target rating, and responses not matching the evaluator's
rating are discarded. Across three out of four meta-evaluation benchmarks,
VideoJudge-7B outperforms larger MLLM judge baselines such as Qwen2.5-VL (32B
and 72B). Notably, we find that LLM judges (Qwen3) models perform worse than
MLLM judges (Qwen2.5-VL) and long chain-of-thought reasoning does not improve
performance, indicating that providing video inputs is crucial for evaluation
of video understanding tasks.

</details>


### [30] [Residual Vector Quantization For Communication-Efficient Multi-Agent Perception](https://arxiv.org/abs/2509.21464)
*Dereje Shenkut,B. V. K Vijaya Kumar*

Main category: cs.CV

TL;DR: 提出了一种名为ReVQom的新型特征压缩方法，大幅降低协同感知系统中的通信负载，实现高效多智能体协作感知。


<details>
  <summary>Details</summary>
Motivation: 多智能体协同感知（如自动驾驶车队、无人机、机器人集团）需共享感知特征数据，但受限于通信带宽，影响了系统的可扩展性与实用性。

Method: 设计了一种端到端的特征编解码框架ReVQom，利用特征维度压缩的瓶颈网络和多级残差矢量量化（RVQ）对中间特征编码，只需传输像素级别索引，极大减少数据量。

Result: 在真实数据集DAIR-V2X上，特征可从8192 bpp压缩至6-30 bpp，实现273-1365倍压缩率。18 bpp时压缩方案可媲美或优于未压缩协同感知，6-12 bpp带宽下亦能保持较好性能。

Conclusion: ReVQom显著提升了多智能体协同感知系统的通信效率和实用性，是实现大规模车联网（V2X）落地的重要进展。

Abstract: Multi-agent collaborative perception (CP) improves scene understanding by
sharing information across connected agents such as autonomous vehicles,
unmanned aerial vehicles, and robots. Communication bandwidth, however,
constrains scalability. We present ReVQom, a learned feature codec that
preserves spatial identity while compressing intermediate features. ReVQom is
an end-to-end method that compresses feature dimensions via a simple bottleneck
network followed by multi-stage residual vector quantization (RVQ). This allows
only per-pixel code indices to be transmitted, reducing payloads from 8192 bits
per pixel (bpp) of uncompressed 32-bit float features to 6-30 bpp per agent
with minimal accuracy loss. On DAIR-V2X real-world CP dataset, ReVQom achieves
273x compression at 30 bpp to 1365x compression at 6 bpp. At 18 bpp (455x),
ReVQom matches or outperforms raw-feature CP, and at 6-12 bpp it enables
ultra-low-bandwidth operation with graceful degradation. ReVQom allows
efficient and accurate multi-agent collaborative perception with a step toward
practical V2X deployment.

</details>


### [31] [Gender Stereotypes in Professional Roles Among Saudis: An Analytical Study of AI-Generated Images Using Language Models](https://arxiv.org/abs/2509.21466)
*Khaloud S. AlKhalifah,Malak Mashaabi,Hend Al-Khalifa*

Main category: cs.CV

TL;DR: 本研究评估了当前主流文本到图像AI模型在描绘沙特专业人士时的性别刻板印象和文化不准确性。结果显示这些模型普遍存在严重性别失衡和文化误读。


<details>
  <summary>Details</summary>
Motivation: 当前AI文本到图像生成模型在全球范围应用，但尚不清楚其是否也会延续或放大特定文化与性别偏见，特别是在如沙特这样的独特社会文化背景中，研究该现象具有现实和伦理意义。

Method: 选取ImageFX、DALL-E V3和Grok三种模型，针对沙特56个不同职业，用中性提示词生成1006张图像，由两位受训沙特评估员从五个维度进行标注，分歧时由第三位资深研究员裁决，形成共10100个评价数据点。

Result: 三种模型生成的职业形象中，性别严重失衡（85%-96%为男性），DALL-E V3问题最为突出。特别是在领导和技术职业中，男性比例更高。同时，图像在服饰、场景和活动上经常出现文化误读。偶有“反刻板印象”案例，但多数为文化理解错误而非进步表现。

Conclusion: 当前模型深受训练数据和人类社会偏见影响，无法真实反映沙特劳动力市场中的性别与文化多样性，急需更丰富多元的数据、更公正的算法及文化敏感的评测体系以提升AI输出的公平与真实性。

Abstract: This study investigates the extent to which contemporary Text-to-Image
artificial intelligence (AI) models perpetuate gender stereotypes and cultural
inaccuracies when generating depictions of professionals in Saudi Arabia. We
analyzed 1,006 images produced by ImageFX, DALL-E V3, and Grok for 56 diverse
Saudi professions using neutral prompts. Two trained Saudi annotators evaluated
each image on five dimensions: perceived gender, clothing and appearance,
background and setting, activities and interactions, and age. A third senior
researcher adjudicated whenever the two primary raters disagreed, yielding
10,100 individual judgements. The results reveal a strong gender imbalance,
with ImageFX outputs being 85\% male, Grok 86.6\% male, and DALL-E V3 96\%
male, indicating that DALL-E V3 exhibited the strongest overall gender
stereotyping. This imbalance was most evident in leadership and technical
roles. Moreover, cultural inaccuracies in clothing, settings, and depicted
activities were frequently observed across all three models.
Counter-stereotypical images often arise from cultural misinterpretations
rather than genuinely progressive portrayals. We conclude that current models
mirror societal biases embedded in their training data, generated by humans,
offering only a limited reflection of the Saudi labour market's gender dynamics
and cultural nuances. These findings underscore the urgent need for more
diverse training data, fairer algorithms, and culturally sensitive evaluation
frameworks to ensure equitable and authentic visual outputs.

</details>


### [32] [Reasoning-Enhanced Domain-Adaptive Pretraining of Multimodal Large Language Models for Short Video Content Moderation](https://arxiv.org/abs/2509.21486)
*Zixuan Wang,Yu Sun,Hongwei Wang,Baoyu Jing,Xiang Shen,Xin Dong,Zhuolin Hao,Hongyu Xiong,Yang Song*

Main category: cs.CV

TL;DR: 提出了一种基于多模态大语言模型（MLLM）的统一性不良内容检测方法，通过设计新的预训练任务显著提升了模型在短视频违规内容检测中的表现和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 短视频平台内容多样且更新迅速，违规内容检测面临多类别、繁琐标注和跨类别泛化难题，现有的小模型单独训练方案不足以胜任实际需求。

Method: 提出为MLLM定义三个有针对性的预训练任务：1）视频细节感知的描述（Caption）任务；2）加深理解和执行标签规则的视觉问答（VQA）任务；3）提升推理能力的思维链（Chain-of-Thought, CoT）训练。通过这样设计，使模型适应短视频实际分布和复杂判别标准。

Result: 实验证明，提出的多任务预训练方法在零样本和有监督微调场景下均大幅提升了MLLM对于违规内容的检测准确率，并能很好地识别新出现的问题类型。

Conclusion: 三任务联合预训练显著提升了MLLM在短视频违规内容检测中的表现，实现了任务统一以及对新型违规内容的良好泛化。

Abstract: Short video platforms are evolving rapidly, making the identification of
inappropriate content increasingly critical. Existing approaches typically
train separate and small classification models for each type of issue, which
requires extensive human-labeled data and lacks cross-issue generalization. We
propose a reasoning-enhanced multimodal large language model (MLLM) pretraining
paradigm for unified inappropriate content detection. To address the
distribution gap between short video content and the original pretraining data
of MLLMs, as well as the complex issue definitions, we introduce three targeted
pretraining tasks: (1) \textit{Caption}, to enhance the MLLM's perception of
video details; (2) \textit{Visual Question Answering (VQA)}, to deepen the
MLLM's understanding of issue definitions and annotation guidelines; (3)
\textit{Chain-of-Thought (CoT)}, to enhance the MLLM's reasoning capability.
Experimental results show that our pretraining approach significantly improves
the MLLM's performance in both zero-shot and supervised fine-tuning (SFT)
settings. In addition, our pretrained model demonstrates strong generalization
capabilities to emergent, previously unseen issues.

</details>


### [33] [Learning GUI Grounding with Spatial Reasoning from Visual Feedback](https://arxiv.org/abs/2509.21552)
*Yu Zhao,Wei-Ning Chen,Huseyin Atahan Inan,Samuel Kessler,Lu Wang,Lukas Wutschitz,Fangkai Yang,Chaoyun Zhang,Pasquale Minervini,Saravan Rajmohan,Robert Sim*

Main category: cs.CV

TL;DR: 本文提出了一种新的GUI定位方法：通过模拟鼠标指针的交互式搜索，而非直接坐标预测，提升了模型在复杂高分辨率GUI界面的定位准确率。


<details>
  <summary>Details</summary>
Motivation: 当前主流视觉语言模型（VLMs）在处理复杂、高分辨率的GUI界面时，直接预测具体坐标效果不佳，导致定位准确率降低。因此，研究者希望通过改进任务设定提升泛化性和准确度。

Method: 将GUI定位任务重新定义为交互式的搜索任务。模型每一步判断目标UI元素的位置，根据鼠标指针当前位置与目标的空间关系决定移动方式，并利用强化学习和基于轨迹的奖励机制进行多步在线训练。该方法利用可视化鼠标指针反馈，帮助模型更精准对齐屏幕位置。

Result: 提出的GUI-Cursor方法在ScreenSpot-v2和ScreenSpot-Pro数据集上分别将准确率从88.8%提升至93.9%，从26.8%提升至56.5%，并且95%的案例仅需两步即可完成定位，在复杂案例上有自适应的多步能力，取得了SOTA性能。

Conclusion: 交互式搜索策略结合强化学习大幅提升了复杂GUI定位任务的表现，显示出优于坐标直接预测的方法，并具备处理更难例子的能力。

Abstract: Graphical User Interface (GUI) grounding is commonly framed as a coordinate
prediction task -- given a natural language instruction, generate on-screen
coordinates for actions such as clicks and keystrokes. However, recent Vision
Language Models (VLMs) often fail to predict accurate numeric coordinates when
processing high-resolution GUI images with complex layouts. To address this
issue, we reframe GUI grounding as an \emph{interactive search task}, where the
VLM generates actions to move a cursor in the GUI to locate UI elements. At
each step, the model determines the target object, evaluates the spatial
relations between the cursor and the target, and moves the cursor closer to the
target conditioned on the movement history. In this interactive process, the
rendered cursor provides visual feedback to help the model align its
predictions with the corresponding on-screen locations. We train our GUI
grounding model, GUI-Cursor, using multi-step online reinforcement learning
with a dense trajectory-based reward function. Our experimental results show
that GUI-Cursor, based on Qwen2.5-VL-7B, improves the GUI grounding accuracy
and achieves state-of-the-art results on ScreenSpot-v2 ($88.8\% \rightarrow
93.9\%$) and ScreenSpot-Pro ($26.8\% \rightarrow 56.5\%$). Moreover, we observe
that GUI-Cursor learns to solve the problem within two steps for 95\% of
instances and can adaptively conduct more steps on more difficult examples.

</details>


### [34] [X-CoT: Explainable Text-to-Video Retrieval via LLM-based Chain-of-Thought Reasoning](https://arxiv.org/abs/2509.21559)
*Prasanna Reddy Pulakurthi,Jiamian Wang,Majid Rabbani,Sohail Dianat,Raghuveer Rao,Zhiqiang Tao*

Main category: cs.CV

TL;DR: 本文提出了一种基于大模型链式推理（CoT）的可解释文本-视频检索方法X-CoT，替代传统的嵌入-余弦相似度方案，实现了更高性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统文本-视频检索方法依赖嵌入模型和余弦相似度，但存在两大问题：1）低质量数据对检索有干扰且难以发现；2）仅用余弦相似度无法解释检索排序，导致模型和数据难以分析评估。

Method: 作者提出X-CoT框架，利用大语言模型链式推理（LLM CoT）来进行排序和理由生成。为此，扩展了现有数据集，补充更多视频注释，以丰富语义信息和减少数据偏差，并设计了逐对比较的检索推理链，输出详细推理过程和完整排序。

Result: 实验表明，X-CoT不仅提升了文本-视频检索性能，还能输出详细的排序理由，有助于分析模型行为和数据质量。

Conclusion: 利用LLM的链式推理方式替代传统相似度排名，不仅提高了检索效果，还显著增强了系统的解释能力，对模型评估与数据挖掘具有重要意义。

Abstract: Prevalent text-to-video retrieval systems mainly adopt embedding models for
feature extraction and compute cosine similarities for ranking. However, this
design presents two limitations. Low-quality text-video data pairs could
compromise the retrieval, yet are hard to identify and examine. Cosine
similarity alone provides no explanation for the ranking results, limiting the
interpretability. We ask that can we interpret the ranking results, so as to
assess the retrieval models and examine the text-video data? This work proposes
X-CoT, an explainable retrieval framework upon LLM CoT reasoning in place of
the embedding model-based similarity ranking. We first expand the existing
benchmarks with additional video annotations to support semantic understanding
and reduce data bias. We also devise a retrieval CoT consisting of pairwise
comparison steps, yielding detailed reasoning and complete ranking. X-CoT
empirically improves the retrieval performance and produces detailed
rationales. It also facilitates the model behavior and data quality analysis.
Code and data are available at: https://github.com/PrasannaPulakurthi/X-CoT.

</details>


### [35] [Unsupervised Defect Detection for Surgical Instruments](https://arxiv.org/abs/2509.21561)
*Joseph Huang,Yichi Zhang,Jingxi Yu,Wei Chen,Seunghyun Hwang,Qiang Qiu,Amy R. Reibman,Edward J. Delp,Fengqing Zhu*

Main category: cs.CV

TL;DR: 现有缺陷检测方法难以迁移到手术器械领域，本文提出一种专门针对手术器械的无监督检测方法，提升了细粒度缺陷的检出效果。


<details>
  <summary>Details</summary>
Motivation: 手术器械的安全性依赖于对其视觉缺陷的可靠检测，手工检查容易出错，而现有自动检测方法对手术领域迁移不佳，因此亟需专门适配手术器械的缺陷检测方法。

Method: 提出一种针对手术器械图像的无监督缺陷检测方法，结合了背景遮罩、基于小块的分析策略以及高效的领域适应技术，提升对细微和特定缺陷的识别能力。

Result: 该方法能够有效克服原有检测方法的误检、漏检以及对领域特征适应不良的问题，实现了对手术器械图像中细粒度缺陷的可靠检测。

Conclusion: 专门针对领域特性而设计的检测方法能显著提升手术器械缺陷检测的准确性和鲁棒性，对于实际手术安全具有重要意义。

Abstract: Ensuring the safety of surgical instruments requires reliable detection of
visual defects. However, manual inspection is prone to error, and existing
automated defect detection methods, typically trained on natural/industrial
images, fail to transfer effectively to the surgical domain. We demonstrate
that simply applying or fine-tuning these approaches leads to issues: false
positive detections arising from textured backgrounds, poor sensitivity to
small, subtle defects, and inadequate capture of instrument-specific features
due to domain shift. To address these challenges, we propose a versatile method
that adapts unsupervised defect detection methods specifically for surgical
instruments. By integrating background masking, a patch-based analysis
strategy, and efficient domain adaptation, our method overcomes these
limitations, enabling the reliable detection of fine-grained defects in
surgical instrument imagery.

</details>


### [36] [No Alignment Needed for Generation: Learning Linearly Separable Representations in Diffusion Models](https://arxiv.org/abs/2509.21565)
*Junno Yun,Yaşar Utku Alçalar,Mehmet Akçakaya*

Main category: cs.CV

TL;DR: 本文提出了一种新的正则化训练方法LSEP，无需大型外部编码器，通过提升中间层表征的线性可分性，提升扩散模型的训练效率和生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前大规模扩散模型训练方法通常依赖于与强大外部编码器特征对齐来提升表征能力，但这种方法需要预训练大模型，计算成本高昂。为此，作者希望找到无需外部编码器的方法来提升模型表征能力。

Method: 作者提出了一种基于提升中间层表征线性可分性（LSEP）的正则化方法，在训练过程中将线性可分性直接融入神经网络学习动态中，无需依赖辅助编码器，也不将线性探针视为仅仅是后验评估工具。

Result: 该方法在基于流的transformer结构（如SiTs）上的实验表明，无论在训练效率还是生成质量上均有显著提升，在256×256 ImageNet数据集上实现了1.46的FID分数。

Conclusion: LSEP方法可以有效取代依赖外部编码器的表征对齐策略，以更优的训练效率和生成表现提升大规模扩散模型，为相关领域提供了更高效、更易实现的训练新思路。

Abstract: Efficient training strategies for large-scale diffusion models have recently
emphasized the importance of improving discriminative feature representations
in these models. A central line of work in this direction is representation
alignment with features obtained from powerful external encoders, which
improves the representation quality as assessed through linear probing.
Alignment-based approaches show promise but depend on large pretrained
encoders, which are computationally expensive to obtain. In this work, we
propose an alternative regularization for training, based on promoting the
Linear SEParability (LSEP) of intermediate layer representations. LSEP
eliminates the need for an auxiliary encoder and representation alignment,
while incorporating linear probing directly into the network's learning
dynamics rather than treating it as a simple post-hoc evaluation tool. Our
results demonstrate substantial improvements in both training efficiency and
generation quality on flow-based transformer architectures such as SiTs,
achieving an FID of 1.46 on $256 \times 256$ ImageNet dataset.

</details>


### [37] [Enhancing Contrastive Learning for Geolocalization by Discovering Hard Negatives on Semivariograms](https://arxiv.org/abs/2509.21573)
*Boyi Chen,Zhangyu Wang,Fabian Deuser,Johann Maximilian Zollner,Martin Werner*

Main category: cs.CV

TL;DR: 本文提出了一种空间正则化对比学习方法，结合地统计学中的半变异函数提升图像地理定位的准确性，显著减少了类别错误和难负样本的影响。


<details>
  <summary>Details</summary>
Motivation: 全球尺度的图像地理定位存在环境多样、视觉模糊和地标缺失等难题。现有对比学习方法忽略了地理空间中的空间依赖性，导致难以有效处理视觉和地理上相似但被误判为负样本（假负例）的问题，对区分难负样本（视觉相似但地理距离大）能力有限。

Method: 本文创新性地将地统计学中的半变异函数引入对比学习，通过将特征空间距离与地理空间距离进行拟合，捕捉空间相关性。基于半变异函数，定义了给定地理距离下的期望视觉差异，用于识别假负例和难负样本，并将此策略集成到GeoCLIP方法中。

Result: 在OSV5M数据集上实验证明，本文方法能够提高地理定位任务的准确性，尤其在细粒度层次上提升更显著。

Conclusion: 对比学习中显式建模地理空间依赖可有效提升大尺度图像地理定位性能，针对难负样本与假负例的问题具有明显改善作用。

Abstract: Accurate and robust image-based geo-localization at a global scale is
challenging due to diverse environments, visually ambiguous scenes, and the
lack of distinctive landmarks in many regions. While contrastive learning
methods show promising performance by aligning features between street-view
images and corresponding locations, they neglect the underlying spatial
dependency in the geographic space. As a result, they fail to address the issue
of false negatives -- image pairs that are both visually and geographically
similar but labeled as negatives, and struggle to effectively distinguish hard
negatives, which are visually similar but geographically distant. To address
this issue, we propose a novel spatially regularized contrastive learning
strategy that integrates a semivariogram, which is a geostatistical tool for
modeling how spatial correlation changes with distance. We fit the
semivariogram by relating the distance of images in feature space to their
geographical distance, capturing the expected visual content in a spatial
correlation. With the fitted semivariogram, we define the expected visual
dissimilarity at a given spatial distance as reference to identify hard
negatives and false negatives. We integrate this strategy into GeoCLIP and
evaluate it on the OSV5M dataset, demonstrating that explicitly modeling
spatial priors improves image-based geo-localization performance, particularly
at finer granularity.

</details>


### [38] [X-Streamer: Unified Human World Modeling with Audiovisual Interaction](https://arxiv.org/abs/2509.21574)
*You Xie,Tianpei Gu,Zenan Li,Chenxu Zhang,Guoxian Song,Xiaochen Zhao,Chao Liang,Jianwen Jiang,Hongyi Xu,Linjie Luo*

Main category: cs.CV

TL;DR: 本文提出了X-Streamer，一种端到端多模态数字人世界建模框架，实现了基于单张静态人像的、高质量、实时、多轮、跨文本、语音及视频的数字人人机交互。该系统核心为“思考者-表演者”双Transformer结构，有效统一理解与生成能力。


<details>
  <summary>Details</summary>
Motivation: 随着数字人日益广泛应用（如虚拟助手、元宇宙等），实现能够自然、持久、多模态交互的数字人一直是挑战，尤其需要单一架构融合文本、语音、视频渠道，并可实时持久互动。

Method: X-Streamer框架以单张人像为输入，采用“Thinker-Actor”双Transformer结构。Thinker模块基于预训练大语言-语音模型，理解和推理流式用户输入；Actor模块基于分段自回归扩散模型，对Thinker隐藏状态进行交叉注意力，生成对齐的文本、音频与视频输出。系统引入分段和全局注意力、多模态位置编码、分段扩散强制和身份引用机制，提升长时一致性和多模态对齐。

Result: X-Streamer可于两块A100显卡上实时运行，从任意肖像持续数小时稳定生成一致的多模态视频对话，展现了极强的互动持续性和统一性。

Conclusion: X-Streamer为多模态数字人世界建模提供了统一、高效的技术路线，为数字人跨文本、语音、视频实时交互与长期一致性带来重大进步。

Abstract: We introduce X-Streamer, an end-to-end multimodal human world modeling
framework for building digital human agents capable of infinite interactions
across text, speech, and video within a single unified architecture. Starting
from a single portrait, X-Streamer enables real-time, open-ended video calls
driven by streaming multimodal inputs. At its core is a Thinker-Actor
dual-transformer architecture that unifies multimodal understanding and
generation, turning a static portrait into persistent and intelligent
audiovisual interactions. The Thinker module perceives and reasons over
streaming user inputs, while its hidden states are translated by the Actor into
synchronized multimodal streams in real time. Concretely, the Thinker leverages
a pretrained large language-speech model, while the Actor employs a chunk-wise
autoregressive diffusion model that cross-attends to the Thinker's hidden
states to produce time-aligned multimodal responses with interleaved discrete
text and audio tokens and continuous video latents. To ensure long-horizon
stability, we design inter- and intra-chunk attentions with time-aligned
multimodal positional embeddings for fine-grained cross-modality alignment and
context retention, further reinforced by chunk-wise diffusion forcing and
global identity referencing. X-Streamer runs in real time on two A100 GPUs,
sustaining hours-long consistent video chat experiences from arbitrary
portraits and paving the way toward unified world modeling of interactive
digital humans.

</details>


### [39] [What Happens Next? Anticipating Future Motion by Generating Point Trajectories](https://arxiv.org/abs/2509.21592)
*Gabrijel Boduljak,Laurynas Karazija,Iro Laina,Christian Rupprecht,Andrea Vedaldi*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，仅凭单张图像预测物体运动轨迹，其效果优于现有代表性方法，尤其在现实和下游应用任务中表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成模型多用于像素层面的预测，并不擅长从单帧图像预测物体具体运动，尤其是在无法获得速度或外力等额外参数时，准确建模现实场景的动力学具有挑战性。作者希望解决此类情况下的运动预测问题，以提升在物理建模和下游应用中的实用性和精度。

Method: 作者将问题建模为有条件的密集轨迹网格生成，借鉴现代视频生成器网络结构，但模型直接输出运动轨迹而非像素，从而规避生成像素带来的开销和局限，更准确、多样化地反映场景运动及其不确定性。

Result: 在模拟数据和真实物理场景上进行充分实验，结果显示该方法能生成更准确且多样的运动预测；在机器人等下游任务上同样展现出强大实用性。与最新视频生成模型对比，单帧下本方法在简单物理情境（如落块、机械交互）下明显优于后者。

Conclusion: 直接生成运动轨迹比分步骤生成像素更适合单帧图像的运动预测任务，现有的视频生成模型在此场景下具有先天局限。本文模型为实际场景下的动力学建模及应用提供了准确、高效的新路径。

Abstract: We consider the problem of forecasting motion from a single image, i.e.,
predicting how objects in the world are likely to move, without the ability to
observe other parameters such as the object velocities or the forces applied to
them. We formulate this task as conditional generation of dense trajectory
grids with a model that closely follows the architecture of modern video
generators but outputs motion trajectories instead of pixels. This approach
captures scene-wide dynamics and uncertainty, yielding more accurate and
diverse predictions than prior regressors and generators. We extensively
evaluate our method on simulated data, demonstrate its effectiveness on
downstream applications such as robotics, and show promising accuracy on
real-world intuitive physics datasets. Although recent state-of-the-art video
generators are often regarded as world models, we show that they struggle with
forecasting motion from a single image, even in simple physical scenarios such
as falling blocks or mechanical object interactions, despite fine-tuning on
such data. We show that this limitation arises from the overhead of generating
pixels rather than directly modeling motion.

</details>


### [40] [Temporal vs. Spatial: Comparing DINOv3 and V-JEPA2 Feature Representations for Video Action Analysis](https://arxiv.org/abs/2509.21595)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.CV

TL;DR: 本文比较分析了两种主流的视频自监督学习架构：DINOv3（空间特征提取）和V-JEPA2（时序建模），并在UCF Sports数据集上评估其在分类准确率、聚类与判别能力、类别一致性等多方面的表现。


<details>
  <summary>Details</summary>
Motivation: 视频动作识别任务中，不同网络架构（仅空间特征vs. 空间+时序特征）在特征表达和下游任务上的表现差异不明。本文旨在深入分析两类主流架构的优劣并为实际应用选型提供指导。

Method: 选取DINOv3和V-JEPA2两种自监督学习模型，分别代表空间特征提取和时序建模架构，在UCF Sports动作识别数据集上，通过分类、聚类、类间分辨率、一致性等多维指标，系统对比分析其特征表示能力和可靠性差异。

Result: DINOv3在聚类（Silhouette分数0.31 vs 0.21）和特定类别动作（如静态姿势识别）表现优异，但在需时序信息的动态动作识别上性能下降；V-JEPA2对各种动作类型表现更加均衡且性能方差更低（0.094 vs 0.288），但在聚类等判别能力上略弱于DINOv3。

Conclusion: 空间特征架构（DINOv3）适合识别静态动作并具有高判别性，时序建模架构（V-JEPA2）则在各类动作上更为稳健。选择何种特征提取方法需根据具体任务性能要求与一致性需求权衡。

Abstract: This study presents a comprehensive comparative analysis of two prominent
self-supervised learning architectures for video action recognition: DINOv3,
which processes frames independently through spatial feature extraction, and
V-JEPA2, which employs joint temporal modeling across video sequences. We
evaluate both approaches on the UCF Sports dataset, examining feature quality
through multiple dimensions including classification accuracy, clustering
performance, intra-class consistency, and inter-class discrimination. Our
analysis reveals fundamental architectural trade-offs: DINOv3 achieves superior
clustering performance (Silhouette score: 0.31 vs 0.21) and demonstrates
exceptional discrimination capability (6.16x separation ratio) particularly for
pose-identifiable actions, while V-JEPA2 exhibits consistent reliability across
all action types with significantly lower performance variance (0.094 vs
0.288). Through action-specific evaluation, we identify that DINOv3's spatial
processing architecture excels at static pose recognition but shows degraded
performance on motion-dependent actions, whereas V-JEPA2's temporal modeling
provides balanced representation quality across diverse action categories.
These findings contribute to the understanding of architectural design choices
in video analysis systems and provide empirical guidance for selecting
appropriate feature extraction methods based on task requirements and
reliability constraints.

</details>


### [41] [VLCE: A Knowledge-Enhanced Framework for Image Description in Disaster Assessment](https://arxiv.org/abs/2509.21609)
*Md. Mahfuzur Rahman,Kishor Datta Gupta,Marufa Kamal,Fahad Rahman,Sunzida Siddique,Ahmed Rafi Hasan,Mohd Ariful Haque,Roy George*

Main category: cs.CV

TL;DR: 本文提出了一种名为VLCE的多模态系统，用于生成对灾害图像的详细说明，相比现有方法能够更全面和准确地理解灾情。


<details>
  <summary>Details</summary>
Motivation: 传统的灾害损失评估方法效率低且危险。虽然现有计算机视觉方法利用卫星和无人机图片，但输出多为分类或分割标签，难以实现深入的情境理解。

Method: 提出了VLCE模型，包括：1）用于xBD数据集的基于ResNet50的CNN-LSTM模型（在EuroSat预训练），2）用于RescueNet数据集的ViT模型（在UAV图片预训练）。两者均结合ConceptNet和WordNet扩展语义知识，提高描述能力。

Result: VLCE在CLIPScore和InfoMetIC等指标上表现优异，InfoMetIC最高达95.33%，显著超越LLaVA和QwenVL等主流视觉-语言模型，并保持良好的语义对齐能力。

Conclusion: VLCE系统通过自动生成丰富、信息密集的图像描述，显著提升了灾害损失评估的效率和可用性，具有广泛的应用前景。

Abstract: Immediate damage assessment is essential after natural catastrophes; yet,
conventional hand evaluation techniques are sluggish and perilous. Although
satellite and unmanned aerial vehicle (UAV) photos offer extensive perspectives
of impacted regions, current computer vision methodologies generally yield just
classification labels or segmentation masks, so constraining their capacity to
deliver a thorough situational comprehension. We introduce the Vision Language
Caption Enhancer (VLCE), a multimodal system designed to produce comprehensive,
contextually-informed explanations of disaster imagery. VLCE employs a
dual-architecture approach: a CNN-LSTM model with a ResNet50 backbone
pretrained on EuroSat satellite imagery for the xBD dataset, and a Vision
Transformer (ViT) model pretrained on UAV pictures for the RescueNet dataset.
Both systems utilize external semantic knowledge from ConceptNet and WordNet to
expand vocabulary coverage and improve description accuracy. We assess VLCE in
comparison to leading vision-language models (LLaVA and QwenVL) utilizing
CLIPScore for semantic alignment and InfoMetIC for caption informativeness.
Experimental findings indicate that VLCE markedly surpasses baseline models,
attaining a maximum of 95.33% on InfoMetIC while preserving competitive
semantic alignment. Our dual-architecture system demonstrates significant
potential for improving disaster damage assessment by automating the production
of actionable, information-dense descriptions from satellite and drone photos.

</details>


### [42] [A Data-driven Typology of Vision Models from Integrated Representational Metrics](https://arxiv.org/abs/2509.21628)
*Jialin Wu,Shreya Saha,Yiqing Bo,Meenakshi Khosla*

Main category: cs.CV

TL;DR: 本文提出了一套多维度表征相似性度量方法，揭示了不同视觉模型家族在表征层面的共性与区别，并通过类似生物信息学的融合方法实现了高效的视觉模型分类。


<details>
  <summary>Details</summary>
Motivation: 当前大规模视觉模型在架构和训练方式上差异较大，但缺乏系统方法来区分它们在表征层面上哪些是共性，哪些是家族特有。理解这些差异有助于指导模型的选择与创新。

Method: 作者使用了一套多样的表征相似性度量，包括几何、单元调谐性、线性可解码性等多个维度，分别评估模型家族的可分性。继而引入融合方法Similarity Network Fusion (SNF) 整合上述不同度量，形成复合的模型表征签名。

Result: 发现基于几何与调谐性的度量能很好区分模型家族，线性可解码性则区分弱。SNF融合度量后，家族区分性更明显、结构更鲁棒，并揭示了如自监督模型跨架构聚类、某些混合架构趋向自编码器等现象。

Conclusion: 模型的架构与训练目标共同决定了其表征结构，现有的视觉模型可通过本方法获得更为系统和生物学启发的类型学划分，有助于深入理解“表面设计”背后的表征机制。

Abstract: Large vision models differ widely in architecture and training paradigm, yet
we lack principled methods to determine which aspects of their representations
are shared across families and which reflect distinctive computational
strategies. We leverage a suite of representational similarity metrics, each
capturing a different facet-geometry, unit tuning, or linear decodability-and
assess family separability using multiple complementary measures. Metrics
preserving geometry or tuning (e.g., RSA, Soft Matching) yield strong family
discrimination, whereas flexible mappings such as Linear Predictivity show
weaker separation. These findings indicate that geometry and tuning carry
family-specific signatures, while linearly decodable information is more
broadly shared. To integrate these complementary facets, we adapt Similarity
Network Fusion (SNF), a method inspired by multi-omics integration. SNF
achieves substantially sharper family separation than any individual metric and
produces robust composite signatures. Clustering of the fused similarity matrix
recovers both expected and surprising patterns: supervised ResNets and ViTs
form distinct clusters, yet all self-supervised models group together across
architectural boundaries. Hybrid architectures (ConvNeXt, Swin) cluster with
masked autoencoders, suggesting convergence between architectural modernization
and reconstruction-based training. This biology-inspired framework provides a
principled typology of vision models, showing that emergent computational
strategies-shaped jointly by architecture and training objective-define
representational structure beyond surface design categories.

</details>


### [43] [FantasyWorld: Geometry-Consistent World Modeling via Unified Video and 3D Prediction](https://arxiv.org/abs/2509.21657)
*Yixiang Dai,Fan Jiang,Chiyu Wang,Mu Xu,Yonggang Qi*

Main category: cs.CV

TL;DR: FantasyWorld是一种增强型3D世界建模框架，通过在现有视频基础模型上加入可训练的几何分支，实现了更具三维感知和一致性的高质量视频与3D场景建模。


<details>
  <summary>Details</summary>
Motivation: 尽管现有视频基础模型在想象能力上较强，但缺乏明确的3D建模能力，难以支持空间一致性和3D推理等实际应用场景。因此，提升视频模型的3D建模能力成为迫切需求，尤其适用于AR/VR和机器人导航等领域。

Method: 作者提出了FantasyWorld框架，在被冻结的视频基础模型上集成了一个可训练的几何分支，两分支间通过交叉监督机制协作，几何信号指导视频生成，视频先验反向提升3D预测能力，实现单次前向推理下同时建模视频潜变量和隐式3D场。

Result: 实验表明，FantasyWorld在多视角一致性与风格一致性等方面优于现有几何一致性基线。同时，消融实验显示该框架的统一主干和分支信息交互机制带来了显著性能提升。

Conclusion: FantasyWorld有效连接了视频想象和三维感知，为高质量、通用型的下游3D任务（如新视角合成和导航）提供了兼容且无需针对单场景微调的表征。

Abstract: High-quality 3D world models are pivotal for embodied intelligence and
Artificial General Intelligence (AGI), underpinning applications such as AR/VR
content creation and robotic navigation. Despite the established strong
imaginative priors, current video foundation models lack explicit 3D grounding
capabilities, thus being limited in both spatial consistency and their utility
for downstream 3D reasoning tasks. In this work, we present FantasyWorld, a
geometry-enhanced framework that augments frozen video foundation models with a
trainable geometric branch, enabling joint modeling of video latents and an
implicit 3D field in a single forward pass. Our approach introduces
cross-branch supervision, where geometry cues guide video generation and video
priors regularize 3D prediction, thus yielding consistent and generalizable
3D-aware video representations. Notably, the resulting latents from the
geometric branch can potentially serve as versatile representations for
downstream 3D tasks such as novel view synthesis and navigation, without
requiring per-scene optimization or fine-tuning. Extensive experiments show
that FantasyWorld effectively bridges video imagination and 3D perception,
outperforming recent geometry-consistent baselines in multi-view coherence and
style consistency. Ablation studies further confirm that these gains stem from
the unified backbone and cross-branch information exchange.

</details>


### [44] [MORPH: Shape-agnostic PDE Foundation Models](https://arxiv.org/abs/2509.21670)
*Mahindra Singh Rautela,Alexander Most,Siddharth Mansingh,Bradley C. Love,Ayan Biswas,Diane Oyen,Earl Lawrence*

Main category: cs.CV

TL;DR: MORPH 是一种通用的自回归基础模型，能够高效处理不同形状、分辨率、多场混合（标量和矢量）的偏微分方程 (PDEs) 数据，对科学机器学习任务具有很好的泛化和迁移能力。


<details>
  <summary>Details</summary>
Motivation: 科学领域中，来自不同行业和应用的偏微分方程 (PDEs) 数据具有多样性，如维度（1D-3D）、分辨率、数据类型（标量/矢量）等，缺乏能高效统一处理这些异质数据的通用基础模型。希望构建一个高效、形状无关且泛化能力强的PDE建模工具。

Method: MORPH 采用基于卷积视觉Transformer的架构，集成三大创新：（1）分量卷积，对局部信息和不同类型物理字段进行联合处理；（2）字段间交叉注意力，实现多物理场之间的信息流动与选择性建模；（3）轴向注意力，将时空自注意力因子化，减少计算量。该模型通过预训练，结合全模型微调和参数高效的 LoRA 方法，迁移到各种下游PDE预测任务。

Result: MORPH的多个变体在广泛的PDE任务中迁移效果优异。无论是全零样本（zero-shot）还是全训练（full-shot），MORPH都明显优于从头训练的模型，并且在大规模评测中可匹配或超越现有的先进基线方法与最优模型。

Conclusion: MORPH 作为统一、灵活且强大的PDE建模基础架构，可高效学习和泛化不同来源、不同模态的科学观测数据，为可扩展和数据高效的科学机器学习开辟新途径。

Abstract: We introduce MORPH, a shape-agnostic, autoregressive foundation model for
partial differential equations (PDEs). MORPH is built on a convolutional vision
transformer backbone that seamlessly handles heterogeneous spatiotemporal
datasets of varying data dimensionality (1D--3D) at different resolutions,
multiple fields with mixed scalar and vector components. The architecture
combines (i) component-wise convolution, which jointly processes scalar and
vector channels to capture local interactions, (ii) inter-field
cross-attention, which models and selectively propagates information between
different physical fields, (iii) axial attentions, which factorizes full
spatiotemporal self-attention along individual spatial and temporal axes to
reduce computational burden while retaining expressivity. We pretrain multiple
model variants on a diverse collection of heterogeneous PDE datasets and
evaluate transfer to a range of downstream prediction tasks. Using both
full-model fine-tuning and parameter-efficient low-rank adapters (LoRA), MORPH
outperforms models trained from scratch in both zero-shot and full-shot
generalization. Across extensive evaluations, MORPH matches or surpasses strong
baselines and recent state-of-the-art models. Collectively, these capabilities
present a flexible and powerful backbone for learning from heterogeneous and
multimodal nature of scientific observations, charting a path toward scalable
and data-efficient scientific machine learning.

</details>


### [45] [MS-YOLO: Infrared Object Detection for Edge Deployment via MobileNetV4 and SlideLoss](https://arxiv.org/abs/2509.21696)
*Jiali Zhang,Thomas S. White,Haoliang Zhang,Wenqing Hu,Donald C. Wunsch II,Jian Liu*

Main category: cs.CV

TL;DR: 本文提出了一种针对城市环境红外图像目标检测的新型算法MS-YOLO，在保持高精度的同时降低了计算成本，适用于实时边缘部署。


<details>
  <summary>Details</summary>
Motivation: 目前城市环境下的目标检测常面临低光照、恶劣天气、数据类别不平衡、热噪声及算力受限等实际问题，亟需一种既高效又精确的红外检测方法解决这些瓶颈。

Method: 作者首先在FLIR ADAS V2数据集上比较多种YOLO架构后选取YOLOv8为基线；随后将YOLOv8的CSPDarknet骨干网络替换为更高效的MobileNetV4，降低了计算量；同时提出新颖的SlideLoss损失函数，能够动态关注类别不平衡和遮挡样本，从而提高模型对弱小目标和遮挡目标检测的表现。

Result: 实验表明，MS-YOLO在FLIR ADAS V2基准上的mAP和精度表现优越，仅需6.7 GFLOPs，同时计算开销下降1.5%。

Conclusion: MS-YOLO在城市红外目标检测中兼顾高精度和低算力，能很好应用于智能交通等现实边缘场景，证明了其实际部署价值。

Abstract: Infrared imaging has emerged as a robust solution for urban object detection
under low-light and adverse weather conditions, offering significant advantages
over traditional visible-light cameras. However, challenges such as class
imbalance, thermal noise, and computational constraints can significantly
hinder model performance in practical settings. To address these issues, we
evaluate multiple YOLO variants on the FLIR ADAS V2 dataset, ultimately
selecting YOLOv8 as our baseline due to its balanced accuracy and efficiency.
Building on this foundation, we present \texttt{MS-YOLO} (\textbf{M}obileNetv4
and \textbf{S}lideLoss based on YOLO), which replaces YOLOv8's CSPDarknet
backbone with the more efficient MobileNetV4, reducing computational overhead
by \textbf{1.5%} while sustaining high accuracy. In addition, we introduce
\emph{SlideLoss}, a novel loss function that dynamically emphasizes
under-represented and occluded samples, boosting precision without sacrificing
recall. Experiments on the FLIR ADAS V2 benchmark show that \texttt{MS-YOLO}
attains competitive mAP and superior precision while operating at only
\textbf{6.7 GFLOPs}. These results demonstrate that \texttt{MS-YOLO}
effectively addresses the dual challenge of maintaining high detection quality
while minimizing computational costs, making it well-suited for real-time edge
deployment in urban environments.

</details>


### [46] [Motion-Aware Transformer for Multi-Object Tracking](https://arxiv.org/abs/2509.21715)
*Xu Yang,Gady Agam*

Main category: cs.CV

TL;DR: 本文提出了一种名为Motion-Aware Transformer（MATR）的新型多目标跟踪方法，通过显式预测物体运动以优化检测与关联表现，在多个公开数据集上取得了显著的提升和最新的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于DETR的多目标跟踪方法通常在单一Transformer解码器层中同时处理检测与跟踪查询，导致二者产生冲突，影响关联准确率。因此，亟需改进框架以提升跟踪表现。

Method: 提出Motion-Aware Transformer（MATR），在Transformer中通过显式预测视频帧间的物体运动，提前更新跟踪查询，从而减小查询冲突，实现更一致的训练流程，提升目标检测与关联准确率。

Result: 在DanceTrack、SportsMOT和BDD100k数据集上，MATR在标准评价指标上均有显著提升。例如，在DanceTrack上，HOTA指标领先MOTR 9分，在其它数据集也达到了最新的最好成绩。

Conclusion: 在端到端Transformer中显式建模运动信息是一种简单高效的多目标跟踪提升手段，MATR方法在多个主流数据集都取得了先进的跟踪性能。

Abstract: Multi-object tracking (MOT) in videos remains challenging due to complex
object motions and crowded scenes. Recent DETR-based frameworks offer
end-to-end solutions but typically process detection and tracking queries
jointly within a single Transformer Decoder layer, leading to conflicts and
degraded association accuracy. We introduce the Motion-Aware Transformer
(MATR), which explicitly predicts object movements across frames to update
track queries in advance. By reducing query collisions, MATR enables more
consistent training and improves both detection and association. Extensive
experiments on DanceTrack, SportsMOT, and BDD100k show that MATR delivers
significant gains across standard metrics. On DanceTrack, MATR improves HOTA by
more than 9 points over MOTR without additional data and reaches a new
state-of-the-art score of 71.3 with supplementary data. MATR also achieves
state-of-the-art results on SportsMOT (72.2 HOTA) and BDD100k (54.7 mTETA, 41.6
mHOTA) without relying on external datasets. These results demonstrate that
explicitly modeling motion within end-to-end Transformers offers a simple yet
highly effective approach to advancing multi-object tracking.

</details>


### [47] [DeLiVR: Differential Spatiotemporal Lie Bias for Efficient Video Deraining](https://arxiv.org/abs/2509.21719)
*Shuning Sun,Jialang Lu,Xiang Chen,Jichao Wang,Dianjie Lu,Guijuan Zhang,Guangwei Gao,Zhuoran Zheng*

Main category: cs.CV

TL;DR: DeLiVR是一种高效的视频去雨方法，通过将空间和时间上的Lie群微分偏置注入到网络的注意力分数中，实现了帧间对齐和去除雨条纹。实验结果显示该方法优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有视频去雨方法依赖于光流或启发式对齐，既计算量大又不够鲁棒。摄像角度的微小变化会放大帧间不匹配和时序伪影，因此需要更高效、更鲁棒的空间与时间一致性建模方法。

Method: 提出DeLiVR方法：1）引入基于Lie群的旋转有界相对偏置，通过预测每帧的面内角度，将归一化坐标旋转并与基础坐标比较，实现几何一致的特征对齐；2）引入微分群位移，通过计算相邻帧的角度差来估算速度，结合时序衰减和注意力掩码，精确捕捉雨条纹方向和帧间关系。这些Lie群偏置被直接注入到网络的注意力机制中。

Result: 在多个公开基准测试集上进行的大量实验结果表明，DeLiVR方法在去雨任务上取得了优异的表现，优于现有对比方法。

Conclusion: 本文提出的基于Lie群微分偏置的DeLiVR方法，为视频时空对齐和去雨任务提供了高效且鲁棒的解决方案，并通过大量实验验证了其实用有效性。

Abstract: Videos captured in the wild often suffer from rain streaks, blur, and noise.
In addition, even slight changes in camera pose can amplify cross-frame
mismatches and temporal artifacts. Existing methods rely on optical flow or
heuristic alignment, which are computationally expensive and less robust. To
address these challenges, Lie groups provide a principled way to represent
continuous geometric transformations, making them well-suited for enforcing
spatial and temporal consistency in video modeling. Building on this insight,
we propose DeLiVR, an efficient video deraining method that injects
spatiotemporal Lie-group differential biases directly into attention scores of
the network. Specifically, the method introduces two complementary components.
First, a rotation-bounded Lie relative bias predicts the in-plane angle of each
frame using a compact prediction module, where normalized coordinates are
rotated and compared with base coordinates to achieve geometry-consistent
alignment before feature aggregation. Second, a differential group displacement
computes angular differences between adjacent frames to estimate a velocity.
This bias computation combines temporal decay and attention masks to focus on
inter-frame relationships while precisely matching the direction of rain
streaks. Extensive experimental results demonstrate the effectiveness of our
method on publicly available benchmarks.

</details>


### [48] [On the Status of Foundation Models for SAR Imagery](https://arxiv.org/abs/2509.21722)
*Nathan Inkawhich*

Main category: cs.CV

TL;DR: 本文研究了基础AI/ML模型在合成孔径雷达（SAR）目标识别任务中的可行性，借鉴了自然图像领域自监督学习大模型的成功经验，通过自监督微调公开SSL模型，在SAR领域取得了比现有最优模型更优的结果。


<details>
  <summary>Details</summary>
Motivation: 自然图像领域大型自监督学习模型在目标识别上表现出众，能够以较少标注数据适应下游任务，提升模型鲁棒性和特征迁移能力。受此启发，作者希望将这些技术应用于SAR目标识别，弥补传统SAR方法的不足。

Method: 首先，作者评估了现有的视觉基础模型（如DINOv2、DINOv3和PE-Core）在SAR特征提取上的表现，发现直接使用效果有限。随后，通过用SAR数据对公开的SSL模型进行自监督微调（如AFRL-DINOv2），并比较不同主干网络和下游任务适配方案的性能。

Result: 用SAR数据对SSL模型（如AFRL-DINOv2s）的自监督微调大幅优于现有最好的SAR模型（SARATR-X），并实现了SAR基础模型的最新最佳效果。同时分析了不同主干网络的性能权衡及模型在不同应用环境下的表现。

Conclusion: 基础AI/ML模型结合自监督微调在SAR目标识别领域具有巨大潜力，可利用有限标注数据获得优异性能，但仍有技术难点待解决。该工作为未来SAR领域的基础模型构建提供了重要参考。

Abstract: In this work we investigate the viability of foundational AI/ML models for
Synthetic Aperture Radar (SAR) object recognition tasks. We are inspired by the
tremendous progress being made in the wider community, particularly in the
natural image domain where frontier labs are training huge models on web-scale
datasets with unprecedented computing budgets. It has become clear that these
models, often trained with Self-Supervised Learning (SSL), will transform how
we develop AI/ML solutions for object recognition tasks - they can be adapted
downstream with very limited labeled data, they are more robust to many forms
of distribution shift, and their features are highly transferable
out-of-the-box. For these reasons and more, we are motivated to apply this
technology to the SAR domain. In our experiments we first run tests with
today's most powerful visual foundational models, including DINOv2, DINOv3 and
PE-Core and observe their shortcomings at extracting semantically-interesting
discriminative SAR target features when used off-the-shelf. We then show that
Self-Supervised finetuning of publicly available SSL models with SAR data is a
viable path forward by training several AFRL-DINOv2s and setting a new
state-of-the-art for SAR foundation models, significantly outperforming today's
best SAR-domain model SARATR-X. Our experiments further analyze the performance
trade-off of using different backbones with different downstream
task-adaptation recipes, and we monitor each model's ability to overcome
challenges within the downstream environments (e.g., extended operating
conditions and low amounts of labeled data). We hope this work will inform and
inspire future SAR foundation model builders, because despite our positive
results, we still have a long way to go.

</details>


### [49] [UISim: An Interactive Image-Based UI Simulator for Dynamic Mobile Environments](https://arxiv.org/abs/2509.21733)
*Jiannan Xiang,Yun Zhu,Lei Shu,Maria Wang,Lijun Yu,Gabriel Barcik,James Lyon,Srinivas Sunkara,Jindong Chen*

Main category: cs.CV

TL;DR: 本论文提出并验证了一种新型基于图像的UI模拟器UISim，可以更高效地模拟和测试手机界面及训练智能体交互。


<details>
  <summary>Details</summary>
Motivation: 当前UI开发和AI智能体训练过程中，受限于实物设备或静态截图分析，导致测试和开发效率低下，难以扩展。因此急需一种能够动态、交互式、高效地模拟真实移动端UI环境的新方法。

Method: 文章提出UISim系统，采取两阶段方式：首先，根据初始界面图片和用户操作预测下一个UI状态的布局信息；其次，基于预测的抽象布局合成具有真实感且一致性强的新界面图像。该方法实现了UI状态转移的视觉化和动态模拟。

Result: 实验结果显示，UISim在生成真实、连贯的UI后续状态方面优于端到端UI生成的基线模型，具有更高的仿真保真度。

Conclusion: UISim不仅能提升UI测试、原型开发和合成数据生成效率，还为智能体的UI导航、任务规划等高级应用提供了有力工具，有望推动UI开发自动化和智能体训练的发展。

Abstract: Developing and testing user interfaces (UIs) and training AI agents to
interact with them are challenging due to the dynamic and diverse nature of
real-world mobile environments. Existing methods often rely on cumbersome
physical devices or limited static analysis of screenshots, which hinders
scalable testing and the development of intelligent UI agents. We introduce
UISim, a novel image-based UI simulator that offers a dynamic and interactive
platform for exploring mobile phone environments purely from screen images. Our
system employs a two-stage method: given an initial phone screen image and a
user action, it first predicts the abstract layout of the next UI state, then
synthesizes a new, visually consistent image based on this predicted layout.
This approach enables the realistic simulation of UI transitions. UISim
provides immediate practical benefits for UI testing, rapid prototyping, and
synthetic data generation. Furthermore, its interactive capabilities pave the
way for advanced applications, such as UI navigation task planning for AI
agents. Our experimental results show that UISim outperforms end-to-end UI
generation baselines in generating realistic and coherent subsequent UI states,
highlighting its fidelity and potential to streamline UI development and
enhance AI agent training.

</details>


### [50] [LFA-Net: A Lightweight Network with LiteFusion Attention for Retinal Vessel Segmentation](https://arxiv.org/abs/2509.21738)
*Mehwish Mehmood,Ivor Spence,Muhammad Fahim*

Main category: cs.CV

TL;DR: 提出了一种轻量级视网膜血管分割网络LFA-Net，使用创新的LiteFusion-Attention模块，实现高精度和低计算资源消耗，适合实际临床环境。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习技术的进步，现有视网膜血管分割模型在小血管分割以及计算资源消耗方面仍存在重大挑战。临床实际环境通常计算资源有限，因此亟需高效、轻量的分割方法。

Method: 作者设计了一种新型注意力模块LiteFusion-Attention，结合残差连接、受Vision Mamba启发的动态特性和基于调制的注意力机制，并将其应用到新提出的LFA-Net网络中，实现高效捕获局部与全局信息。

Result: LFA-Net在DRIVE、STARE和CHASE_DB数据集上取得了优异的分割性能（Dice分数分别为83.28%、87.44%、84.50%，Jaccard分数分别为72.85%、79.31%、74.70%）。模型仅有0.11百万参数、0.42MB内存和4.46 GFLOPs，显著轻量化。

Conclusion: LFA-Net兼具高准确率和极低的计算资源消耗，非常适合资源受限的真实临床环境，有望推动视网膜血管分割技术在实际中的应用。

Abstract: Lightweight retinal vessel segmentation is important for the early diagnosis
of vision-threatening and systemic diseases, especially in a real-world
clinical environment with limited computational resources. Although
segmentation methods based on deep learning are improving, existing models are
still facing challenges of small vessel segmentation and high computational
costs. To address these challenges, we proposed a new vascular segmentation
network, LFA-Net, which incorporates a newly designed attention module,
LiteFusion-Attention. This attention module incorporates residual learning
connections, Vision Mamba-inspired dynamics, and modulation-based attention,
enabling the model to capture local and global context efficiently and in a
lightweight manner. LFA-Net offers high performance with 0.11 million
parameters, 0.42 MB memory size, and 4.46 GFLOPs, which make it ideal for
resource-constrained environments. We validated our proposed model on DRIVE,
STARE, and CHASE_DB with outstanding performance in terms of dice scores of
83.28, 87.44, and 84.50% and Jaccard indices of 72.85, 79.31, and 74.70%,
respectively. The code of LFA-Net is available online
https://github.com/Mehwish4593/LFA-Net.

</details>


### [51] [Incorporating Scene Context and Semantic Labels for Enhanced Group-level Emotion Recognition](https://arxiv.org/abs/2509.21747)
*Qing Zhu,Wangdong Guo,Qirong Mao,Xiaohua Huang,Xiuyan Shao,Wenming Zheng*

Main category: cs.CV

TL;DR: 该论文提出了一种融合视觉场景上下文和标签引导语义信息的新框架，显著提升了群体级情感识别（GER）性能，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有群体级情感识别方法低估了视觉场景上下文信息对个体关系建模的重要性，并忽略了情感标签中语义信息对情感理解的关键作用。

Method: 提出了包含视觉上下文编码模块和情感语义编码模块的新型框架：视觉模块利用多尺度场景信息多样化地编码个体关系；情感语义模块通过群体级标签引导大语言模型生成细致的情感词汇，并构建结构化情感树，形成完备的语义表征。最后，提出相似性交互机制对齐和融合视觉与语义信息，改进群体情感表征。

Result: 在三个主流GER数据集上，所提方法取得了与当前最新方法相当或更优的识别效果。

Conclusion: 融合视觉上下文和标签引导的语义信息极大提升了群体级情感识别的准确性和泛化能力，验证了方法的有效性。

Abstract: Group-level emotion recognition (GER) aims to identify holistic emotions
within a scene involving multiple individuals. Current existed methods
underestimate the importance of visual scene contextual information in modeling
individual relationships. Furthermore, they overlook the crucial role of
semantic information from emotional labels for complete understanding of
emotions. To address this limitation, we propose a novel framework that
incorporates visual scene context and label-guided semantic information to
improve GER performance. It involves the visual context encoding module that
leverages multi-scale scene information to diversely encode individual
relationships. Complementarily, the emotion semantic encoding module utilizes
group-level emotion labels to prompt a large language model to generate nuanced
emotion lexicons. These lexicons, in conjunction with the emotion labels, are
then subsequently refined into comprehensive semantic representations through
the utilization of a structured emotion tree. Finally, similarity-aware
interaction is proposed to align and integrate visual and semantic information,
thereby generating enhanced group-level emotion representations and
subsequently improving the performance of GER. Experiments on three widely
adopted GER datasets demonstrate that our proposed method achieves competitive
performance compared to state-of-the-art methods.

</details>


### [52] [KG-SAM: Injecting Anatomical Knowledge into Segment Anything Models via Conditional Random Fields](https://arxiv.org/abs/2509.21750)
*Yu Li,Da Chang,Xi Xiao*

Main category: cs.CV

TL;DR: 该论文提出了KG-SAM，一个结合医学知识图谱、边界优化和不确定性估计的医学影像分割新框架。实验显示，该方法在多中心数据集和多种任务中均取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 现有的通用分割模型（如SAM）在医学影像领域效果受限，主要因边界模糊、缺乏解剖关系建模及无法量化结果不确定性。为提高医学影像分割的准确性与可靠性，需融合医学领域知识并提升模型不确定性评估能力。

Method: KG-SAM采用医学知识图谱建模解剖结构关系，结合基于能量的条件随机场（CRF）强制产生解剖一致性分割结果，并引入不确定性感知模块提升分割可靠性。框架在分割过程中融合了先验知识、边界细化和不确定性估计。

Result: KG-SAM在多中心医学影像数据集上进行了大量实验。在前列腺分割任务中平均Dice分数达到82.69%；腹部分割任务中，在MRI和CT上分别达到78.05%和79.68%。有效提升了现有方法的分割表现。

Conclusion: KG-SAM是一种强健且具广泛适用性的医学影像分割框架，能够有效结合医学知识，实现高质量、可置信度量与解剖一致的分割结果，对推动医学影像分割发展具有重要意义。

Abstract: While the Segment Anything Model (SAM) has achieved remarkable success in
image segmentation, its direct application to medical imaging remains hindered
by fundamental challenges, including ambiguous boundaries, insufficient
modeling of anatomical relationships, and the absence of uncertainty
quantification. To address these limitations, we introduce KG-SAM, a
knowledge-guided framework that synergistically integrates anatomical priors
with boundary refinement and uncertainty estimation. Specifically, KG-SAM
incorporates (i) a medical knowledge graph to encode fine-grained anatomical
relationships, (ii) an energy-based Conditional Random Field (CRF) to enforce
anatomically consistent predictions, and (iii) an uncertainty-aware fusion
module to enhance reliability in high-stakes clinical scenarios. Extensive
experiments across multi-center medical datasets demonstrate the effectiveness
of our approach: KG-SAM achieves an average Dice score of 82.69% on prostate
segmentation and delivers substantial gains in abdominal segmentation, reaching
78.05% on MRI and 79.68% on CT. These results establish KG-SAM as a robust and
generalizable framework for advancing medical image segmentation.

</details>


### [53] [UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models](https://arxiv.org/abs/2509.21760)
*Lan Chen,Yuchao Gu,Qi Mao*

Main category: cs.CV

TL;DR: 本论文提出了一种名为UniVid的新框架，通过微调预训练视频生成模型，使其能统一适应各种视觉任务，无需针对每个任务单独设计算法，极大简化和提升了视觉任务的通用性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 目前视觉大模型（如LVM）将各类任务组织为“视觉句子”，虽然取得进展，但需要跨模态和多数据源的专门预训练，代价高，且难以扩展到新任务。作者希望找到一种成本更低、泛化性更强、能够统一各种视觉任务的新方法。

Method: 作者提出UniVid框架，利用预训练视频扩散变换器，通过对视觉任务的输入和输出统一编码为“视觉句子”，只需微调即可完成多种视觉任务，无需针对每个任务做定制化处理。同时，评估了其在跨模态（图像与视频混编）以及跨数据源（从自然到标注数据）任务中的泛化能力。

Result: 实验证明，UniVid在仅用自然视频数据训练的情况下，也能很好地泛化到不同模态、不同来源的数据，并在多种理解和生成类视觉任务中表现出良好性能。此外，该方法只需调整视觉句子的顺序，便能灵活切换任务类型。

Conclusion: 预训练视频生成模型有潜力为视觉建模提供一个统一、可扩展的基础框架，极大简化了任务扩展和适应过程，展示了优异的泛化与统一建模能力。

Abstract: Large language models, trained on extensive corpora, successfully unify
diverse linguistic tasks within a single generative framework. Inspired by
this, recent works like Large Vision Model (LVM) extend this paradigm to vision
by organizing tasks into sequential visual sentences, where visual prompts
serve as the context to guide outputs. However, such modeling requires
task-specific pre-training across modalities and sources, which is costly and
limits scalability to unseen tasks. Given that pre-trained video generation
models inherently capture temporal sequence dependencies, we explore a more
unified and scalable alternative: can a pre-trained video generation model
adapt to diverse image and video tasks? To answer this, we propose UniVid, a
framework that fine-tunes a video diffusion transformer to handle various
vision tasks without task-specific modifications. Tasks are represented as
visual sentences, where the context sequence defines both the task and the
expected output modality. We evaluate the generalization of UniVid from two
perspectives: (1) cross-modal inference with contexts composed of both images
and videos, extending beyond LVM's uni-modal setting; (2) cross-source tasks
from natural to annotated data, without multi-source pre-training. Despite
being trained solely on natural video data, UniVid generalizes well in both
settings. Notably, understanding and generation tasks can easily switch by
simply reversing the visual sentence order in this paradigm. These findings
highlight the potential of pre-trained video generation models to serve as a
scalable and unified foundation for vision modeling. Our code will be released
at https://github.com/CUC-MIPG/UniVid.

</details>


### [54] [DynaNav: Dynamic Feature and Layer Selection for Efficient Visual Navigation](https://arxiv.org/abs/2509.21930)
*Jiahui Wang,Changhao Chen*

Main category: cs.CV

TL;DR: DynaNav是一种为视觉导航任务设计的高效与可解释框架，通过动态特征与层选择，大幅降低计算资源消耗并提升导航表现。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉导航基础模型（尤其是带transformer解码器的）计算量大、解释性差，难以部署到算力有限的机器人或嵌入式设备上，因此需要更高效且易解释的方法。

Method: 提出了DynaNav框架，能够根据场景复杂度动态选择特征与网络层，采用可训练的硬特征选择器实现稀疏操作，同时将特征选择融入早停机制，并利用贝叶斯优化自动确定最优早停阈值以进一步降低计算开销。

Result: 在真实和模拟环境中的大量实验表明，DynaNav在四个公开数据集上导航表现优于对比方法。与ViNT相比，DynaNav计算量降低了2.26倍、推理时间下降42.3%、内存占用降低32.8%。

Conclusion: DynaNav显著提升了视觉导航效率与可解释性，使基础视觉导航模型更适用于资源受限场景，并保持甚至提升了导航性能。

Abstract: Visual navigation is essential for robotics and embodied AI. However,
existing foundation models, particularly those with transformer decoders,
suffer from high computational overhead and lack interpretability, limiting
their deployment in resource-tight scenarios. To address this, we propose
DynaNav, a Dynamic Visual Navigation framework that adapts feature and layer
selection based on scene complexity. It employs a trainable hard feature
selector for sparse operations, enhancing efficiency and interpretability.
Additionally, we integrate feature selection into an early-exit mechanism, with
Bayesian Optimization determining optimal exit thresholds to reduce
computational cost. Extensive experiments in real-world-based datasets and
simulated environments demonstrate the effectiveness of DynaNav. Compared to
ViNT, DynaNav achieves a 2.26x reduction in FLOPs, 42.3% lower inference time,
and 32.8% lower memory usage, while improving navigation performance across
four public datasets.

</details>


### [55] [CubistMerge: Spatial-Preserving Token Merging For Diverse ViT Backbones](https://arxiv.org/abs/2509.21764)
*Wenyi Gong,Mieszko Lis*

Main category: cs.CV

TL;DR: 本文提出了一种简单但有效的token合并方法，可在保留空间结构的前提下应用于ViT等各类空间架构，兼顾信息分布和结构完整性，提升推理速度且性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 现有大多数token reduction方法无法很好地保留ViT等空间架构依赖的空间结构，导致性能下降。因此需探索既能高效减少token数又能兼容空间结构的新方法。

Method: 提出了一种针对2D空间布局的token合并方法：（1）使用2D化约策略保证结构化token；（2）通过空间感知合并算法保持token相对位置；（3）采用新的max-magnitude-per-dimension表示以保留显著特征。

Result: 该方法在无需微调和少量微调的情况下，对空间与非空间架构在多项视觉任务上都达到了SOTA水平。例如，SAM-H模型在COCO测试上，推理速度提升1.25倍，仅损失0.7%的mIOU。DeiT-B在ImageNet上微调1个epoch即提升1.15倍推理速度，top-1准确率无下降。

Conclusion: 该方法有效解决了空间架构ViT中的token合并难题，能够提升运算速度且基本无性能损失，对未来高效Transformer设计有重大意义。

Abstract: Many modern ViT backbones adopt spatial architectural designs, such as window
attention, decomposed relative positional embeddings in SAM, and RoPE in
DINOv3. Such architectures impose new challenges on token reduction, as the
vast majority of existing methods fail to preserve the spatial structure these
architectures depend on. In this paper, we introduce a simple yet effective
token merging method that maintains spatial integrity, enabling seamless
compatibility with spatial architectures. We reconcile two seemingly
conflicting requirements: (i)exploiting the uneven information distribution
across the spatial layout while (ii)preserving the spatial structure
post-merging. Our approach employs (i)a 2D reduction strategy to enforce
structured token layouts, (ii)a spatial-aware merging algorithm that maintains
relative token positions, and (iii)a novel max-magnitude-per-dimension token
representation that preserves salient features. Our method demonstrates strong
performance both off-the-shelf and with fine-tuning, achieving state-of-the-art
results on spatial and non-spatial architectures across various vision tasks.
Specifically, we achieve 1.25x speedup on SAM-H with only 0.7% mIOU drop
evaluated on COCO off-the-shelf, and 1.15x speedup on DeiT-B with no top-1
accuracy drop on ImageNet within just one epoch of fine-tuning.

</details>


### [56] [Lightweight Structured Multimodal Reasoning for Clinical Scene Understanding in Robotics](https://arxiv.org/abs/2509.22014)
*Saurav Jha,Stefan K. Ehrlich*

Main category: cs.CV

TL;DR: 本文提出了一个轻量级的多模态智能体框架，用于医疗机器人中的视频场景理解，并在多个基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型（VLMs）虽在通用场景下效果良好，但在机器人规划所需的时序推理、不确定性估计和结构化输出等方面存在局限。医疗机器人在动态临床环境中，安全与可靠性极为重要，因此需要更强的多模态感知与推理能力。

Method: 作者将Qwen2.5-VL-3B-Instruct模型与SmolAgent智能体编排层结合，实现了链式思考推理、语音与视觉融合以及动态工具调用。框架能够生成结构化场景图，集成了混合检索模块以实现可解释和自适应的推理能力。

Result: 该方法在Video-MME基准和定制的临床数据集上测试，表现出与当前最先进VLMs相比更高的准确性和鲁棒性。

Conclusion: 该框架为机器人辅助手术、患者监护和临床决策支持等应用展示了很大的应用潜力，有望提升医疗机器人在复杂环境下的安全性和智能化水平。

Abstract: Healthcare robotics requires robust multimodal perception and reasoning to
ensure safety in dynamic clinical environments. Current Vision-Language Models
(VLMs) demonstrate strong general-purpose capabilities but remain limited in
temporal reasoning, uncertainty estimation, and structured outputs needed for
robotic planning. We present a lightweight agentic multimodal framework for
video-based scene understanding. Combining the Qwen2.5-VL-3B-Instruct model
with a SmolAgent-based orchestration layer, it supports chain-of-thought
reasoning, speech-vision fusion, and dynamic tool invocation. The framework
generates structured scene graphs and leverages a hybrid retrieval module for
interpretable and adaptive reasoning. Evaluations on the Video-MME benchmark
and a custom clinical dataset show competitive accuracy and improved robustness
compared to state-of-the-art VLMs, demonstrating its potential for applications
in robot-assisted surgery, patient monitoring, and decision support.

</details>


### [57] [Training-Free Multimodal Deepfake Detection via Graph Reasoning](https://arxiv.org/abs/2509.21774)
*Yuxin Liu,Fei Wang,Kun Li,Yiqi Nie,Junjie Chen,Yanyan Wei,Zhangling Duan,Zhaohong Jia*

Main category: cs.CV

TL;DR: 提出了一种新的无需训练的多模态深度伪造检测框架GASP-ICL，通过筛选任务相关示例提升大规模视觉-语言模型的检测能力，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态深度伪造检测面临LVLM难以捕捉伪造细节、跨模态矛盾以及任务对齐示例检索等问题，影响其检测效果，需要新的方法提升检测性能。

Method: 提出GASP-ICL框架，利用MDD定制特征提取器检索对齐的图文对候选集，并设计Graph-Structured Taylor Adaptive Scorer捕捉样本间信息，传播查询相关信号，最终选择最优演示例子输入LVLM，无需进行模型微调。

Result: 在四种伪造类型数据集上，GASP-ICL在准确率等关键指标上明显优于多个强基线，未依赖额外LVLM训练。

Conclusion: GASP-ICL框架能有效挖掘和利用多模态任务对齐示例，增强LVLM对深度伪造的适应能力，提升检测可靠性，无需模型微调，具有实际应用价值。

Abstract: Multimodal deepfake detection (MDD) aims to uncover manipulations across
visual, textual, and auditory modalities, thereby reinforcing the reliability
of modern information systems. Although large vision-language models (LVLMs)
exhibit strong multimodal reasoning, their effectiveness in MDD is limited by
challenges in capturing subtle forgery cues, resolving cross-modal
inconsistencies, and performing task-aligned retrieval. To this end, we propose
Guided Adaptive Scorer and Propagation In-Context Learning (GASP-ICL), a
training-free framework for MDD. GASP-ICL employs a pipeline to preserve
semantic relevance while injecting task-aware knowledge into LVLMs. We leverage
an MDD-adapted feature extractor to retrieve aligned image-text pairs and build
a candidate set. We further design the Graph-Structured Taylor Adaptive Scorer
(GSTAS) to capture cross-sample relations and propagate query-aligned signals,
producing discriminative exemplars. This enables precise selection of
semantically aligned, task-relevant demonstrations, enhancing LVLMs for robust
MDD. Experiments on four forgery types show that GASP-ICL surpasses strong
baselines, delivering gains without LVLM fine-tuning.

</details>


### [58] [MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning](https://arxiv.org/abs/2509.22281)
*Jinkun Hao,Naifu Liang,Zhen Luo,Xudong Xu,Weipeng Zhong,Ran Yi,Yichen Jin,Zhaoyang Lyu,Feng Zheng,Lizhuang Ma,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 提出了一种面向任务的桌面场景生成新方法，并发布了一个含10700个合成场景的大规模数据集MesaTask-10K，依托空间推理链和LLM进行自动场景生成，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 机器人理解和执行人类任务需要和任务紧密相关的桌面场景，但现有场景构造方式（手工或全随机）效率低且不合理。因此，需要自动化生成与高层次任务描述紧密对应的真实桌面场景。

Method: 提出Task-Oriented Tabletop Scene Generation任务，构建MesaTask-10K数据集（10700个人工布局的虚拟场景）。方法采用“空间推理链”，依次将任务生成流程分为对象推断、空间关系推理、场景图构建，并利用LLM与DPO算法生成与任务描述高度匹配且真实可行的3D布局。

Result: 通过大量实验，MesaTask生成的场景在符合任务描述与布局真实度两方面都显著优于传统方法和多项基线。

Conclusion: MesaTask能够高质量地生成符合任务的真实桌面场景，有效促进了机器人任务理解与执行相关研究，为自动化场景生成提供了新思路。

Abstract: The ability of robots to interpret human instructions and execute
manipulation tasks necessitates the availability of task-relevant tabletop
scenes for training. However, traditional methods for creating these scenes
rely on time-consuming manual layout design or purely randomized layouts, which
are limited in terms of plausibility or alignment with the tasks. In this
paper, we formulate a novel task, namely task-oriented tabletop scene
generation, which poses significant challenges due to the substantial gap
between high-level task instructions and the tabletop scenes. To support
research on such a challenging task, we introduce MesaTask-10K, a large-scale
dataset comprising approximately 10,700 synthetic tabletop scenes with manually
crafted layouts that ensure realistic layouts and intricate inter-object
relations. To bridge the gap between tasks and scenes, we propose a Spatial
Reasoning Chain that decomposes the generation process into object inference,
spatial interrelation reasoning, and scene graph construction for the final 3D
layout. We present MesaTask, an LLM-based framework that utilizes this
reasoning chain and is further enhanced with DPO algorithms to generate
physically plausible tabletop scenes that align well with given task
descriptions. Exhaustive experiments demonstrate the superior performance of
MesaTask compared to baselines in generating task-conforming tabletop scenes
with realistic layouts. Project page is at https://mesatask.github.io/

</details>


### [59] [Prompt-guided Representation Disentanglement for Action Recognition](https://arxiv.org/abs/2509.21783)
*Tianci Wu,Guangming Zhu,Jiang Lu,Siyuan Wang,Ning Wang,Nuoye Xiong,Zhang Liang*

Main category: cs.CV

TL;DR: 本文提出了ProDA框架，通过引入动态提示模块和时空场景图，有效地将多动作场景中的指定动作进行解耦，实现了更精确的动作识别。


<details>
  <summary>Details</summary>
Motivation: 当前方法在处理包含多个动作的视频时，通常提取统一特征，难以建模不同对象间的交互，导致在多动作识别场景下效果不佳。因此需要能针对指定动作从复杂场景中解耦特征的方法。

Method: 本文提出Prompt-guided Disentangled Representation for Action Recognition（ProDA）框架，利用时空场景图（SSG）和动态提示模块（DPM），引导图解析神经网络（GPNN）生成动作特定表示，并设计了针对视频的自适应加权GPNN。

Result: 实验表明，ProDA在视频动作识别任务中优于现有最先进方法。

Conclusion: ProDA能够有效解耦并识别多动作场景下的指定动作，提升了动作识别的准确性。

Abstract: Action recognition is a fundamental task in video understanding. Existing
methods typically extract unified features to process all actions in one video,
which makes it challenging to model the interactions between different objects
in multi-action scenarios. To alleviate this issue, we explore disentangling
any specified actions from complex scenes as an effective solution. In this
paper, we propose Prompt-guided Disentangled Representation for Action
Recognition (ProDA), a novel framework that disentangles any specified actions
from a multi-action scene. ProDA leverages Spatio-temporal Scene Graphs (SSGs)
and introduces Dynamic Prompt Module (DPM) to guide a Graph Parsing Neural
Network (GPNN) in generating action-specific representations. Furthermore, we
design a video-adapted GPNN that aggregates information using dynamic weights.
Experiments in video action recognition demonstrate the effectiveness of our
approach when compared with the state-of-the-art methods. Our code can be found
in https://github.com/iamsnaping/ProDA.git

</details>


### [60] [JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation](https://arxiv.org/abs/2509.22548)
*Shuang Zeng,Dekang Qi,Xinyuan Chang,Feng Xiong,Shichao Xie,Xiaolong Wu,Shiyi Liang,Mu Xu,Xing Wei*

Main category: cs.CV

TL;DR: 本论文提出了一种面向视觉语言导航（VLN）任务的新型神经网络结构JanusVLN，以提升智能体在未知环境中的导航效率和准确率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言导航方法往往依赖于显式语义记忆，例如文本认知地图或历史视觉帧，这导致空间信息损失、计算冗余和内存膨胀，影响导航性能。受人类大脑左右半球分别处理语义与空间的启发，作者希望设计一种既能表达空间又能表达语义的高效存储机制。

Method: JanusVLN框架采用双隐式神经记忆，将空间-几何和视觉-语义记忆分别以紧凑、固定大小的神经表示存储，并扩展多模态大语言模型（MLLM）以融合3D空间先验知识。方法中仅保留滑动窗口和初始部分的token Key-Value，从而避免大量计算冗余，实现了高效的增量式更新。

Result: JanusVLN在多个视觉语言导航任务上大幅超越20余种现有方法。在使用多种输入数据的场景下，其成功率提升10.5-35.5个百分点；仅用RGB数据训练时也优于现有方法3.6-10.8个百分点，性能达到了SOTA。

Conclusion: 提出的双隐式神经记忆架构为VLN领域开辟了新的研究方向，能够有效避免空间信息损失和计算冗余，大大提升了导航效率和准确率，对未来VLN系统的设计具有重要意义。

Abstract: Vision-and-Language Navigation requires an embodied agent to navigate through
unseen environments, guided by natural language instructions and a continuous
video stream. Recent advances in VLN have been driven by the powerful semantic
understanding of Multimodal Large Language Models. However, these methods
typically rely on explicit semantic memory, such as building textual cognitive
maps or storing historical visual frames. This type of method suffers from
spatial information loss, computational redundancy, and memory bloat, which
impede efficient navigation. Inspired by the implicit scene representation in
human navigation, analogous to the left brain's semantic understanding and the
right brain's spatial cognition, we propose JanusVLN, a novel VLN framework
featuring a dual implicit neural memory that models spatial-geometric and
visual-semantic memory as separate, compact, and fixed-size neural
representations. This framework first extends the MLLM to incorporate 3D prior
knowledge from the spatial-geometric encoder, thereby enhancing the spatial
reasoning capabilities of models based solely on RGB input. Then, the
historical key-value caches from the spatial-geometric and visual-semantic
encoders are constructed into a dual implicit memory. By retaining only the KVs
of tokens in the initial and sliding window, redundant computation is avoided,
enabling efficient incremental updates. Extensive experiments demonstrate that
JanusVLN outperforms over 20 recent methods to achieve SOTA performance. For
example, the success rate improves by 10.5-35.5 compared to methods using
multiple data types as input and by 3.6-10.8 compared to methods using more RGB
training data. This indicates that the proposed dual implicit neural memory, as
a novel paradigm, explores promising new directions for future VLN research.
Ours project page: https://miv-xjtu.github.io/JanusVLN.github.io/.

</details>


### [61] [DeHate: A Stable Diffusion-based Multimodal Approach to Mitigate Hate Speech in Images](https://arxiv.org/abs/2509.21787)
*Dwip Dalal,Gautam Vashishtha,Anku Ranui,Aishwarya Reganti,Parth Patwa,Mohd Sarique,Chandan Gupta,Keshav Nath,Viswanatha Reddy,Vinija Jain,Aman Chadha,Amitava Das,Amit Sheth,Asif Ekbal*

Main category: cs.CV

TL;DR: 该论文提出了一种结合了水印和稳定扩散技术，并结合数字关注分析模块的新型多模态数据集和方法，用于识别和去除数字内容中的仇恨元素，并提出了DeHater多模态模型。


<details>
  <summary>Details</summary>
Motivation: 随着有害在线内容的增加，维护健康的数字环境变得愈发重要。因此亟需自动化、精确、高效的仇恨内容检测和去除方法。

Method: 作者构建了一个多模态数据集，采用水印和增强稳定性的扩散模型与数字关注分析模块（DAAM）结合，能够在图像中精准定位仇恨区域。基于注意力图模糊仇恨区域，实现去除，并提出用于多模态任务的DeHater视觉-语言模型。

Result: 成果包括一个公开的数据集、用于仇恨属性标注和去除的工具链以及DeHater多模态模型，在文本提示下针对图像中的仇恨内容识别取得较优效果。

Conclusion: 提出的系统和数据集提升了AI在社交媒体场景下自动检测和去除仇恨内容的能力，促进了更道德的AI应用发展。

Abstract: The rise in harmful online content not only distorts public discourse but
also poses significant challenges to maintaining a healthy digital environment.
In response to this, we introduce a multimodal dataset uniquely crafted for
identifying hate in digital content. Central to our methodology is the
innovative application of watermarked, stability-enhanced, stable diffusion
techniques combined with the Digital Attention Analysis Module (DAAM). This
combination is instrumental in pinpointing the hateful elements within images,
thereby generating detailed hate attention maps, which are used to blur these
regions from the image, thereby removing the hateful sections of the image. We
release this data set as a part of the dehate shared task. This paper also
describes the details of the shared task. Furthermore, we present DeHater, a
vision-language model designed for multimodal dehatification tasks. Our
approach sets a new standard in AI-driven image hate detection given textual
prompts, contributing to the development of more ethical AI applications in
social media.

</details>


### [62] [MIRG-RL: Multi-Image Reasoning and Grounding with Reinforcement Learning](https://arxiv.org/abs/2509.21788)
*Lihao Zheng,Jiawei Chen,Xintian Shen,Hao Ma,Tao Wei*

Main category: cs.CV

TL;DR: 提出MIRG-RL框架，结合监督微调和基于图像感知的强化学习，提升大视觉语言模型在多图推理与定位上的能力，实现了当前最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型缺乏跨图推理能力和充分的跨图参考奖励建模，难以支持复杂的多图像任务。

Method: 提出MIRG-RL统一框架，采用两阶段训练（监督微调和图像感知强化学习组合），创新性地构建融合对象级和图像级标注的轨迹数据，设计了同时针对对象与图像的双奖励函数。

Result: 在多图定位基准上取得SOTA表现，跨图推理任务上达到64.82%，比前SOTA高1%。

Conclusion: 所提方法有效提升了多图推理和定位能力，为LVLM在复杂视觉-语言任务的应用提供了强有力支持。

Abstract: Multi-image reasoning and grounding require understanding complex cross-image
relationships at both object levels and image levels. Current Large Visual
Language Models (LVLMs) face two critical challenges: the lack of cross-image
reasoning capabilities and insufficient cross-image reference reward modeling.
To address these issues, we propose a unified framework - Multi-Image Reasoning
and Grounding with Reinforcement Learning (MIRG-RL). Specifically, our
two-stage training paradigm combines supervised fine-tuning with annotated
trajectories and image-aware reinforcement learning optimization, progressively
developing multi-image reasoning capabilities. Furthermore, we innovatively
propose a method for constructing the trajectory data, which integrates
object-level and image-level annotation information, and use this method to
generate a lightweight reasoning-enhanced dataset. To effectively resolve
cross-image ambiguities, we design an image-aware RL policy with dual reward
functions for objects and images. Experiments demonstrate that MIRG-RL achieves
state-of-the-art (SOTA) performance in multi-image grounding benchmarks,
attaining 64.82% on cross-image reasoning tasks - exceeding the previous best
method by 1%. The code and dataset have been released at
https://github.com/ZEUS2035/MIRG-RL.

</details>


### [63] [LongScape: Advancing Long-Horizon Embodied World Models with Context-Aware MoE](https://arxiv.org/abs/2509.21790)
*Yu Shang,Lei Jin,Yiding Ma,Xin Zhang,Chen Gao,Wei Wu,Yong Li*

Main category: cs.CV

TL;DR: 本文提出了一种视频生成新方法LongScape，通过结合扩散去噪和自回归的方式，有效提升了机器人操作视频的长期一致性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成方法难以实现高质量的长期生成，扩散模型存在时序不一致和视觉漂移，而自回归方法则在细节上有所妥协。为解决这些问题并促进高质量机器人数据的生成，作者提出了新框架。

Method: LongScape创新性地采用了动作引导的变长分块机制，将视频基于机器人行为的语义进行分块。每个分块内部使用扩散去噪生成，分块之间则用自回归方法连接。同时引入了上下文感知的专家混合（CMoE）框架，根据分块内容动态激活专门的生成专家。

Result: 大量实验表明，该方法能在长视频生成任务中，保持更高的稳定性和一致性，显著优于经典扩散和自回归技术。

Conclusion: LongScape为长期一致、高质量的视频生成提供了有效工具，特别适合于机器人操作数据的生成，对后续相关领域研究和应用具有推动作用。

Abstract: Video-based world models hold significant potential for generating
high-quality embodied manipulation data. However, current video generation
methods struggle to achieve stable long-horizon generation: classical
diffusion-based approaches often suffer from temporal inconsistency and visual
drift over multiple rollouts, while autoregressive methods tend to compromise
on visual detail. To solve this, we introduce LongScape, a hybrid framework
that adaptively combines intra-chunk diffusion denoising with inter-chunk
autoregressive causal generation. Our core innovation is an action-guided,
variable-length chunking mechanism that partitions video based on the semantic
context of robotic actions. This ensures each chunk represents a complete,
coherent action, enabling the model to flexibly generate diverse dynamics. We
further introduce a Context-aware Mixture-of-Experts (CMoE) framework that
adaptively activates specialized experts for each chunk during generation,
guaranteeing high visual quality and seamless chunk transitions. Extensive
experimental results demonstrate that our method achieves stable and consistent
long-horizon generation over extended rollouts. Our code is available at:
https://github.com/tsinghua-fib-lab/Longscape.

</details>


### [64] [MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation](https://arxiv.org/abs/2509.21797)
*Yu Shang,Yangcheng Yu,Xin Zhang,Xin Jin,Haisheng Su,Wei Wu,Yong Li*

Main category: cs.CV

TL;DR: MoWM提出了一种融合显式与隐式世界模型特征的机器人动作规划新方法，在任务成功率及泛化能力上取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 当前机器人动作规划面临生成精确动作的挑战。现有基于视频生成的世界模型虽有前景，但像素级重建带来冗余且削弱解码与泛化；而潜在空间模型虽紧凑、关注动作，但缺乏对细粒度信息的关注，难以实现精细操作。因此亟需结合两者优势，提升动作规划表现。

Method: 提出MoWM框架，将具备动态感知的潜在模型特征作为高层先验，引导像素空间模型提取精细图像特征，从而融合二者优势：既有动作相关抽象特征，也保留关键视觉细节，助力高效动作解码和泛化。

Result: 在CALVIN基准测试上，MoWM展现出任务成功率和泛化能力均优于现有方法，达到SOTA水平。同时，论文还系统分析了不同特征空间的优势。

Conclusion: MoWM证明混合世界模型能显著提升机器人动作规划效果，为未来在具身规划领域的模型设计与特征融合提供了重要参考。

Abstract: Embodied action planning is a core challenge in robotics, requiring models to
generate precise actions from visual observations and language instructions.
While video generation world models are promising, their reliance on
pixel-level reconstruction often introduces visual redundancies that hinder
action decoding and generalization. Latent world models offer a compact,
motion-aware representation, but overlook the fine-grained details critical for
precise manipulation. To overcome these limitations, we propose MoWM, a
mixture-of-world-model framework that fuses representations from hybrid world
models for embodied action planning. Our approach uses motion-aware
representations from a latent model as a high-level prior, which guides the
extraction of fine-grained visual features from the pixel space model. This
design allows MoWM to highlight the informative visual details needed for
action decoding. Extensive evaluations on the CALVIN benchmark demonstrate that
our method achieves state-of-the-art task success rates and superior
generalization. We also provide a comprehensive analysis of the strengths of
each feature space, offering valuable insights for future research in embodied
planning. The code is available at: https://github.com/tsinghua-fib-lab/MoWM.

</details>


### [65] [DiTraj: training-free trajectory control for video diffusion transformer](https://arxiv.org/abs/2509.21839)
*Cheng Lei,Jiayu Zhang,Yue Ma,Xinyu Wang,Long Chen,Liang Tang,Yiqiang Yan,Fei Su,Zhicheng Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种名为DiTraj的训练无关、基于Diffusion Transformers（DiT）的text-to-video轨迹控制方法，在提升视频生成质量和可控性的同时，显著减少训练资源消耗。


<details>
  <summary>Details</summary>
Motivation: 传统视频轨迹控制方法通常资源消耗大或专为U-Net设计，难以充分利用DiT模型的强大生成能力，因此亟需一种高效且适用于DiT的通用轨迹控制方法。

Method: 1）通过使用大语言模型（LLM）将用户文本提示分为前景和背景提示，实现前景-背景分离引导，分别指导视频不同区域的生成。2）对3D全注意力机制中的token注意力分数和位置编码的关系进行深入分析，提出空间-时间解耦的3D-RoPE（STD-RoPE），通过修改前景token的位置编码以消除跨帧空间差异，提升前景对象在视频中轨迹的可控性。3）通过调节位置编码密度实现三维感知下的轨迹控制。

Result: 大量实验验证表明，所提方法在视频质量和轨迹可控性方面均优于以往方法。

Conclusion: DiTraj作为一种训练无关、适配DiT的视频轨迹控制框架，兼顾了生成效果和轨迹控制能力，在实际应用中具有较大潜力和推广价值。

Abstract: Diffusion Transformers (DiT)-based video generation models with 3D full
attention exhibit strong generative capabilities. Trajectory control represents
a user-friendly task in the field of controllable video generation. However,
existing methods either require substantial training resources or are
specifically designed for U-Net, do not take advantage of the superior
performance of DiT. To address these issues, we propose DiTraj, a simple but
effective training-free framework for trajectory control in text-to-video
generation, tailored for DiT. Specifically, first, to inject the object's
trajectory, we propose foreground-background separation guidance: we use the
Large Language Model (LLM) to convert user-provided prompts into foreground and
background prompts, which respectively guide the generation of foreground and
background regions in the video. Then, we analyze 3D full attention and explore
the tight correlation between inter-token attention scores and position
embedding. Based on this, we propose inter-frame Spatial-Temporal Decoupled
3D-RoPE (STD-RoPE). By modifying only foreground tokens' position embedding,
STD-RoPE eliminates their cross-frame spatial discrepancies, strengthening
cross-frame attention among them and thus enhancing trajectory control.
Additionally, we achieve 3D-aware trajectory control by regulating the density
of position embedding. Extensive experiments demonstrate that our method
outperforms previous methods in both video quality and trajectory
controllability.

</details>


### [66] [A Comprehensive Evaluation of Transformer-Based Question Answering Models and RAG-Enhanced Design](https://arxiv.org/abs/2509.21845)
*Zichen Zhang,Kunlong Zhang,Hongwei Ruan,Yiming Luo*

Main category: cs.CV

TL;DR: 本文系统评估了多跳问答中不同信息检索（retrieval）策略，提出混合检索方法能显著提升性能，尤其是在无需额外训练的检索增强生成场景下。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型虽提升了问答表现，但多跳推理依赖聚合多篇文档信息，检索环节仍是瓶颈。需要探索更有效的多跳问答检索策略，并兼顾效率与准确性。

Method: 比较了余弦相似度、最大边际相关性以及将稠密嵌入与词汇重叠、重排序结合的混合方法；还对EfficientRAG检索管线进行改进，引入token标注和迭代优化，同时保证效率。

Result: 在HotpotQA数据集上，混合检索法较余弦基线在准确匹配（EM）和F1得分上分别提升50%和47%。误差分析显示混合法增强了实体召回和证据互补性，但仍受干扰信息和时间推理影响。

Conclusion: 混合检索增强生成可为多跳问答提供实用的零样本方案，在准确性、效率和可解释性之间取得良好平衡。

Abstract: Transformer-based models have advanced the field of question answering, but
multi-hop reasoning, where answers require combining evidence across multiple
passages, remains difficult. This paper presents a comprehensive evaluation of
retrieval strategies for multi-hop question answering within a
retrieval-augmented generation framework. We compare cosine similarity, maximal
marginal relevance, and a hybrid method that integrates dense embeddings with
lexical overlap and re-ranking. To further improve retrieval, we adapt the
EfficientRAG pipeline for query optimization, introducing token labeling and
iterative refinement while maintaining efficiency. Experiments on the HotpotQA
dataset show that the hybrid approach substantially outperforms baseline
methods, achieving a relative improvement of 50 percent in exact match and 47
percent in F1 score compared to cosine similarity. Error analysis reveals that
hybrid retrieval improves entity recall and evidence complementarity, while
remaining limited in handling distractors and temporal reasoning. Overall, the
results suggest that hybrid retrieval-augmented generation provides a practical
zero-shot solution for multi-hop question answering, balancing accuracy,
efficiency, and interpretability.

</details>


### [67] [Dynamic Novel View Synthesis in High Dynamic Range](https://arxiv.org/abs/2509.21853)
*Kaixuan Zhang,Zhipeng Xiong,Minxian Li,Mingwu Ren,Jiankang Deng,Xiatian Zhu*

Main category: cs.CV

TL;DR: 本文提出了HDR Dynamic Novel View Synthesis（HDR DNVS）问题，即从常规低动态范围（LDR）图像中学习高动态范围（HDR）动态场景的新视角渲染。针对这一更具挑战性的任务，作者提出了HDR-4DGS方法，在动态场景下实现了HDR渲染，并在实验中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的HDR新视角合成（HDR NVS）方法仅适用于静态场景，无法处理真实世界中的动态元素如移动物体和变化的光照。现实场景涉及更多时间变化和动态性，因此需要可以同时建模时间和HDR–LDR转换的方法。

Method: 本文提出HDR Dynamic Novel View Synthesis（HDR DNVS）新任务，并设计了基于Gaussian Splatting的HDR-4DGS架构。该架构包含创新的动态色调映射模块，可根据不同时间分布自适应调整HDR与LDR之间的转换关系，从而在动态变化中保持时序上的光强一致性和空间上的色彩准确。

Result: HDR-4DGS能够在任意视角和时间点合成高保真的HDR场景，实验结果显示在定量和视觉质量上均超越现有最先进方法。

Conclusion: HDR-4DGS首次有效解决了动态场景中的HDR新视角合成问题，实现了时空一致且高质量的HDR渲染，对现实世界的3D重建和渲染具有实际意义。

Abstract: High Dynamic Range Novel View Synthesis (HDR NVS) seeks to learn an HDR 3D
model from Low Dynamic Range (LDR) training images captured under conventional
imaging conditions. Current methods primarily focus on static scenes,
implicitly assuming all scene elements remain stationary and non-living.
However, real-world scenarios frequently feature dynamic elements, such as
moving objects, varying lighting conditions, and other temporal events, thereby
presenting a significantly more challenging scenario. To address this gap, we
propose a more realistic problem named HDR Dynamic Novel View Synthesis (HDR
DNVS), where the additional dimension ``Dynamic'' emphasizes the necessity of
jointly modeling temporal radiance variations alongside sophisticated 3D
translation between LDR and HDR. To tackle this complex, intertwined challenge,
we introduce HDR-4DGS, a Gaussian Splatting-based architecture featured with an
innovative dynamic tone-mapping module that explicitly connects HDR and LDR
domains, maintaining temporal radiance coherence by dynamically adapting
tone-mapping functions according to the evolving radiance distributions across
the temporal dimension. As a result, HDR-4DGS achieves both temporal radiance
consistency and spatially accurate color translation, enabling photorealistic
HDR renderings from arbitrary viewpoints and time instances. Extensive
experiments demonstrate that HDR-4DGS surpasses existing state-of-the-art
methods in both quantitative performance and visual fidelity. Source code will
be released.

</details>


### [68] [SRHand: Super-Resolving Hand Images and 3D Shapes via View/Pose-aware Neural Image Representations and Explicit 3D Meshes](https://arxiv.org/abs/2509.21859)
*Minje Kim,Tae-Kyun Kim*

Main category: cs.CV

TL;DR: 本文提出了一种用于从低分辨率图像重建高细节3D手部模型的技术，命名为SRHand，是首个专为手部建模设计的超分辨率与三维重建结合方法。


<details>
  <summary>Details</summary>
Motivation: 现有高精度手部三维重建方法严重依赖高分辨率多视角图像输入，难以在低分辨率场景中应用。已有多视角超分辨率方法针对的是静态场景和固定分辨率物体，不适用于高自由度的手部建模，存在泛化性与适用性不足的问题。因此，急需一种能从低分辨率图像重建高质量手部三维模型的新方法。

Method: 作者提出SRHand，将隐式图像表示与显式手部网格有机结合。具体地，创新性地引入了几何感知的隐式图像函数（GIIF），通过对输入的低分辨率图像进行上采样，学习到细致的手部先验。该方法通过联合优化隐式图像函数与显式三维手部形状，实现了多视角及手部姿态一致性，同时获得手部细节丰富的3D重建（如皱纹、指甲等）。

Result: 在InterHand2.6M和Goliath数据集上的实验表明，SRHand在手部数据集上对比最新图像超分辨率方法和三维重建方法，在定量和定性评测上均取得了显著更优性能。

Conclusion: SRHand首次证明了结合隐式图像超分辨率与三维手部重建的可行性与优越性，为低分辨率输入条件下的高质量手部重建提供了高效且通用的解决方案。

Abstract: Reconstructing detailed hand avatars plays a crucial role in various
applications. While prior works have focused on capturing high-fidelity hand
geometry, they heavily rely on high-resolution multi-view image inputs and
struggle to generalize on low-resolution images. Multi-view image
super-resolution methods have been proposed to enforce 3D view consistency.
These methods, however, are limited to static objects/scenes with fixed
resolutions and are not applicable to articulated deformable hands. In this
paper, we propose SRHand (Super-Resolution Hand), the method for reconstructing
detailed 3D geometry as well as textured images of hands from low-resolution
images. SRHand leverages the advantages of implicit image representation with
explicit hand meshes. Specifically, we introduce a geometric-aware implicit
image function (GIIF) that learns detailed hand prior by upsampling the coarse
input images. By jointly optimizing the implicit image function and explicit 3D
hand shapes, our method preserves multi-view and pose consistency among
upsampled hand images, and achieves fine-detailed 3D reconstruction (wrinkles,
nails). In experiments using the InterHand2.6M and Goliath datasets, our method
significantly outperforms state-of-the-art image upsampling methods adapted to
hand datasets, and 3D hand reconstruction methods, quantitatively and
qualitatively. Project page: https://yunminjin2.github.io/projects/srhand

</details>


### [69] [Deepfakes: we need to re-think the concept of "real" images](https://arxiv.org/abs/2509.21864)
*Janis Keuper,Margret Keuper*

Main category: cs.CV

TL;DR: 本文指出目前针对“假”图像检测的研究过度关注生成算法和“假”数据，忽视了对“真”图像的明确定义和数据收集，而“真”图像的生成方式也因技术进步发生了巨大变化。


<details>
  <summary>Details</summary>
Motivation: 随着现代图像生成模型的广泛普及，伪造与操控图像的犯罪风险和负面社会影响加剧，催生了图像真伪检测的需求。但目前研究主要关注“假”图像的检测，忽略了对“真”图像内涵的重新认识。

Method: 作者通过分析现有“假”图像检测方法，指出这些方法大多使用老旧、低分辨率的“真”图像数据集，如ImageNet，忽视了当前真实图像大多由智能手机拍摄并经过神经网络处理的现实。论文提升问题的认知，提倡重新定义“真”图像，并促进社会公开讨论。

Result: 作者发现，几乎所有现有的“假”图像检测方法都依赖极少且陈旧的“真”图像数据集，这些数据难以代表当代真实图像的生成和特征。

Conclusion: 当前“真”图像与“假”图像的界限已被现代成像技术模糊，必须重新定义“真”图像，并构建新的标注数据集。作者呼吁学界对“假图像检测”这一目标的合理性进行深入探讨和反思。

Abstract: The wide availability and low usability barrier of modern image generation
models has triggered the reasonable fear of criminal misconduct and negative
social implications. The machine learning community has been engaging this
problem with an extensive series of publications proposing algorithmic
solutions for the detection of "fake", e.g. entirely generated or partially
manipulated images. While there is undoubtedly some progress towards technical
solutions of the problem, we argue that current and prior work is focusing too
much on generative algorithms and "fake" data-samples, neglecting a clear
definition and data collection of "real" images. The fundamental question "what
is a real image?" might appear to be quite philosophical, but our analysis
shows that the development and evaluation of basically all current
"fake"-detection methods is relying on only a few, quite old low-resolution
datasets of "real" images like ImageNet. However, the technology for the
acquisition of "real" images, aka taking photos, has drastically evolved over
the last decade: Today, over 90% of all photographs are produced by smartphones
which typically use algorithms to compute an image from multiple inputs (over
time) from multiple sensors. Based on the fact that these image formation
algorithms are typically neural network architectures which are closely related
to "fake"-image generators, we state the position that today, we need to
re-think the concept of "real" images. The purpose of this position paper is to
raise the awareness of the current shortcomings in this active field of
research and to trigger an open discussion whether the detection of "fake"
images is a sound objective at all. At the very least, we need a clear
technical definition of "real" images and new benchmark datasets.

</details>


### [70] [Unlocking the Essence of Beauty: Advanced Aesthetic Reasoning with Relative-Absolute Policy Optimization](https://arxiv.org/abs/2509.21871)
*Boyang Liu,Yifan Hu,Senjie Jin,Shihan Dou,Gonglei Shi,Jie Shao,Tao Gui,Xuanjing Huang*

Main category: cs.CV

TL;DR: 本文提出Aes-R1框架，通过结合链式思维（CoT）数据和强化学习优化，显著提升了多模态大模型在图像美学评估的能力。


<details>
  <summary>Details</summary>
Motivation: 传统MLLMs在美学评估中的表现受限于美学推理数据稀缺和审美评价主观性，难以生成准确且可解释的美学判断。作者希望解决数据与可解释性的双重挑战。

Method: 提出Aes-R1框架：1）通过AesCoT模块自动构建与筛选高质量链式美学推理数据，提升模型冷启动表现；2）设计全新强化学习算法RAPO，联合优化绝对分数回归和相对排序，提升单图和跨图像的评估准确性。

Result: 实验表明，Aes-R1在主干模型上的平均PLCC/SRCC指标分别提升47.9%/34.8%，超过同等规模最优基线。消融实验也验证了Aes-R1在低监督和分布外数据上的强泛化能力。

Conclusion: Aes-R1能让MLLMs同时给出扎实的美学解释和真实可靠的评分，实现统一、高效的美学推理和评估框架，推动多模态美学自解释模型发展。

Abstract: Multimodal large language models (MLLMs) are well suited to image aesthetic
assessment, as they can capture high-level aesthetic features leveraging their
cross-modal understanding capacity. However, the scarcity of multimodal
aesthetic reasoning data and the inherently subjective nature of aesthetic
judgment make it difficult for MLLMs to generate accurate aesthetic judgments
with interpretable rationales. To this end, we propose Aes-R1, a comprehensive
aesthetic reasoning framework with reinforcement learning (RL). Concretely,
Aes-R1 integrates a pipeline, AesCoT, to construct and filter high-quality
chain-of-thought aesthetic reasoning data used for cold-start. After teaching
the model to generate structured explanations prior to scoring, we then employ
the Relative-Absolute Policy Optimization (RAPO), a novel RL algorithm that
jointly optimizes absolute score regression and relative ranking order,
improving both per-image accuracy and cross-image preference judgments. Aes-R1
enables MLLMs to generate grounded explanations alongside faithful scores,
thereby enhancing aesthetic scoring and reasoning in a unified framework.
Extensive experiments demonstrate that Aes-R1 improves the backbone's average
PLCC/SRCC by 47.9%/34.8%, surpassing state-of-the-art baselines of similar
size. More ablation studies validate Aes-R1's robust generalization under
limited supervision and in out-of-distribution scenarios.

</details>


### [71] [StableDub: Taming Diffusion Prior for Generalized and Efficient Visual Dubbing](https://arxiv.org/abs/2509.21887)
*Liyang Chen,Tianze Zhou,Xu He,Boshi Tang,Zhiyong Wu,Yang Huang,Yang Wu,Zhongqian Sun,Wei Yang,Helen Meng*

Main category: cs.CV

TL;DR: 本文提出了StableDub，一种新颖的可视配音生成框架，通过结合说话者唇部习惯建模与遮挡鲁棒合成，显著提升唇动与音频同步及在有遮挡情况下的生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有可视配音方法存在两大问题：其一，仅基于音频的驱动难以还原说话者本人的唇部习惯，导致生成的唇动与目标化身不够相似；其二，传统修补方法在遇到遮挡（如麦克风、手等）时，易产生可见伪影，限制实际应用。

Method: 1）构建于Stable Diffusion基础之上的唇部习惯调制机制，实现音素级音频-视觉同步与说话者特定口面部动态建模；2）引入遮挡感知训练策略，将遮挡物显式纳入修补流程提升遮挡下的生成效果；3）在模型结构上引入混合Mamba-Transformer以优化在低资源场景下的训练效率。

Result: 实验表明，StableDub在唇部习惯相似性和遮挡鲁棒性方面表现优越，同时在音频-唇动同步、视频质量和分辨率一致性等多项指标上超过现有方法。

Conclusion: StableDub有效扩展了可视配音技术的适用范围，在提升生成质量的同时兼顾效率，具有广泛的实际应用前景。

Abstract: The visual dubbing task aims to generate mouth movements synchronized with
the driving audio, which has seen significant progress in recent years.
However, two critical deficiencies hinder their wide application: (1)
Audio-only driving paradigms inadequately capture speaker-specific lip habits,
which fail to generate lip movements similar to the target avatar; (2)
Conventional blind-inpainting approaches frequently produce visual artifacts
when handling obstructions (e.g., microphones, hands), limiting practical
deployment. In this paper, we propose StableDub, a novel and concise framework
integrating lip-habit-aware modeling with occlusion-robust synthesis.
Specifically, building upon the Stable-Diffusion backbone, we develop a
lip-habit-modulated mechanism that jointly models phonemic audio-visual
synchronization and speaker-specific orofacial dynamics. To achieve plausible
lip geometries and object appearances under occlusion, we introduce the
occlusion-aware training strategy by explicitly exposing the occlusion objects
to the inpainting process. By incorporating the proposed designs, the model
eliminates the necessity for cost-intensive priors in previous methods, thereby
exhibiting superior training efficiency on the computationally intensive
diffusion-based backbone. To further optimize training efficiency from the
perspective of model architecture, we introduce a hybrid Mamba-Transformer
architecture, which demonstrates the enhanced applicability in low-resource
research scenarios. Extensive experimental results demonstrate that StableDub
achieves superior performance in lip habit resemblance and occlusion
robustness. Our method also surpasses other methods in audio-lip sync, video
quality, and resolution consistency. We expand the applicability of visual
dubbing methods from comprehensive aspects, and demo videos can be found at
https://stabledub.github.io.

</details>


### [72] [Drag4D: Align Your Motion with Text-Driven 3D Scene Generation](https://arxiv.org/abs/2509.21888)
*Minjun Kang,Inkyu Shin,Taeyeop Lee,In So Kweon,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: Drag4D是一个将物体运动控制与文本驱动3D场景生成集成的互动框架，实现用户可控的3D物体动画生成。


<details>
  <summary>Details</summary>
Motivation: 目前，文本驱动的3D场景生成在真实感与物体交互性方面仍有局限，用户很难直接控制3D物体在场景中的运动轨迹。为此，Drag4D旨在提升3D场景生成的互动性与用户操控精度，满足创作与动画应用需求。

Method: Drag4D共分三步：(1) 利用2D高斯散斑融合全景图像与修补视图，提升文本到3D场景的重建质量；(2) 用图像到3D模型将目标物体从单张图片生成完整三维网格，并通过物理解耦学习精确插入背景场景；(3) 用户可自定义3D轨迹，框架以兼顾多视角、一致性的时序扩散模型对物体进行动画，保证运动合成真实且连贯。

Result: 通过逐步评估各环节及整体，Drag4D能实现高质量的3D场景重建，物体运动轨迹控制精准，舞台运动能够与三维背景高度融合，且运动过程中保持视觉一致性。

Conclusion: Drag4D有效提升了3D场景生成的交互性与动画表现，为文本到3D内容生成和动画控制提供了统一、高效的方法，对后续3D内容制作及应用具有重要参考价值。

Abstract: We introduce Drag4D, an interactive framework that integrates object motion
control within text-driven 3D scene generation. This framework enables users to
define 3D trajectories for the 3D objects generated from a single image,
seamlessly integrating them into a high-quality 3D background. Our Drag4D
pipeline consists of three stages. First, we enhance text-to-3D background
generation by applying 2D Gaussian Splatting with panoramic images and
inpainted novel views, resulting in dense and visually complete 3D
reconstructions. In the second stage, given a reference image of the target
object, we introduce a 3D copy-and-paste approach: the target instance is
extracted in a full 3D mesh using an off-the-shelf image-to-3D model and
seamlessly composited into the generated 3D scene. The object mesh is then
positioned within the 3D scene via our physics-aware object position learning,
ensuring precise spatial alignment. Lastly, the spatially aligned object is
temporally animated along a user-defined 3D trajectory. To mitigate motion
hallucination and ensure view-consistent temporal alignment, we develop a
part-augmented, motion-conditioned video diffusion model that processes
multiview image pairs together with their projected 2D trajectories. We
demonstrate the effectiveness of our unified architecture through evaluations
at each stage and in the final results, showcasing the harmonized alignment of
user-controlled object motion within a high-quality 3D background.

</details>


### [73] [Syncphony: Synchronized Audio-to-Video Generation with Diffusion Transformers](https://arxiv.org/abs/2509.21893)
*Jibin Song,Mingi Kwon,Jaeseok Jeong,Youngjung Uh*

Main category: cs.CV

TL;DR: 本文提出了一种新的音频到视频生成方法Syncphony，能够根据音频生成高度同步的视频，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本或图像到视频生成方法难以精确控制运动时序，而音频天然包含与视频运动同步的时间线索，因此利用音频进行视频生成具有潜力。但目前音频到视频（A2V）模型仍难细粒度同步，主要由于条件机制间接或时间建模能力有限。

Method: 作者基于预训练视频生成网络，提出了两个关键创新：1）运动感知损失（Motion-aware Loss），加强对高运动区域的学习；2）音频同步引导（Audio Sync Guidance），利用无音频层的视觉对齐不同步模型，在推理阶段利用音频线索，同时保持视觉质量。此外，提出了新的评测指标CycleSync，通过生成视频重构原始音频，衡量同步程度。

Result: 在AVSync15和The Greatest Hits数据集上，Syncphony在同步准确率和视觉质量上均优于现有音视频同步生成方法。

Conclusion: Syncphony有效提升了音频到视频生成的同步性和视觉质量，并提出了新的同步评估指标，为音视频生成领域带来重要进步。

Abstract: Text-to-video and image-to-video generation have made rapid progress in
visual quality, but they remain limited in controlling the precise timing of
motion. In contrast, audio provides temporal cues aligned with video motion,
making it a promising condition for temporally controlled video generation.
However, existing audio-to-video (A2V) models struggle with fine-grained
synchronization due to indirect conditioning mechanisms or limited temporal
modeling capacity. We present Syncphony, which generates 380x640 resolution,
24fps videos synchronized with diverse audio inputs. Our approach builds upon a
pre-trained video backbone and incorporates two key components to improve
synchronization: (1) Motion-aware Loss, which emphasizes learning at
high-motion regions; (2) Audio Sync Guidance, which guides the full model using
a visually aligned off-sync model without audio layers to better exploit audio
cues at inference while maintaining visual quality. To evaluate
synchronization, we propose CycleSync, a video-to-audio-based metric that
measures the amount of motion cues in the generated video to reconstruct the
original audio. Experiments on AVSync15 and The Greatest Hits datasets
demonstrate that Syncphony outperforms existing methods in both synchronization
accuracy and visual quality. Project page is available at:
https://jibin86.github.io/syncphony_project_page

</details>


### [74] [LG-CD: Enhancing Language-Guided Change Detection through SAM2 Adaptation](https://arxiv.org/abs/2509.21894)
*Yixiao Liu,Yizhou Yang,Jinwen Li,Jun Tao,Ruoyu Li,Xiangkun Wang,Min Zhu,Junlong Cheng*

Main category: cs.CV

TL;DR: 该论文提出了一种全新基于语言引导的遥感变化检测模型（LG-CD），利用文本信息提升变化检测精度，实验结果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大多数基于深度学习的遥感变化检测方法只关注单一模态的视觉信息，忽略了如文本等多模态语义信息的作用。因此，希望通过融合文本信息提升变化检测的准确性和泛化能力。

Method: 提出LG-CD模型，利用自然语言提示引导网络关注变化区域。采用视觉基础模型（SAM2）提取多尺度特征，通过多层适配器对模型进行微调，并设计文本融合注意模块（TFAM）对齐视觉与文本信息，最终通过视觉-语义融合解码器（V-SFD）生成高精度的变化检测掩膜。

Result: 在三个公开数据集（LEVIR-CD, WHU-CD, SYSU-CD）上进行实验，LG-CD均优于当前主流变化检测方法，表现出更高的检测精度和鲁棒性。

Conclusion: LG-CD实现了对多模态信息的有效融合，显著提升了遥感变化检测的效果。该方法为未来通过多模态信息实现更泛化的变化检测提供了新思路。

Abstract: Remote Sensing Change Detection (RSCD) typically identifies changes in land
cover or surface conditions by analyzing multi-temporal images. Currently, most
deep learning-based methods primarily focus on learning unimodal visual
information, while neglecting the rich semantic information provided by
multimodal data such as text. To address this limitation, we propose a novel
Language-Guided Change Detection model (LG-CD). This model leverages natural
language prompts to direct the network's attention to regions of interest,
significantly improving the accuracy and robustness of change detection.
Specifically, LG-CD utilizes a visual foundational model (SAM2) as a feature
extractor to capture multi-scale pyramid features from high-resolution to
low-resolution across bi-temporal remote sensing images. Subsequently,
multi-layer adapters are employed to fine-tune the model for downstream tasks,
ensuring its effectiveness in remote sensing change detection. Additionally, we
design a Text Fusion Attention Module (TFAM) to align visual and textual
information, enabling the model to focus on target change regions using text
prompts. Finally, a Vision-Semantic Fusion Decoder (V-SFD) is implemented,
which deeply integrates visual and semantic information through a
cross-attention mechanism to produce highly accurate change detection masks.
Our experiments on three datasets (LEVIR-CD, WHU-CD, and SYSU-CD) demonstrate
that LG-CD consistently outperforms state-of-the-art change detection methods.
Furthermore, our approach provides new insights into achieving generalized
change detection by leveraging multimodal information.

</details>


### [75] [TDEdit: A Unified Diffusion Framework for Text-Drag Guided Image Manipulation](https://arxiv.org/abs/2509.21905)
*Qihang Wang,Yaxiong Wang,Lechao Cheng,Zhun Zhong*

Main category: cs.CV

TL;DR: 该论文提出了一种将文本与拖拽联合控制的图像编辑新方法，结合现有文本驱动和拖拽驱动编辑各自的优点，实现更精细和灵活的图像操作。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的图像编辑虽然在纹理调整方面表现优秀，但缺乏对空间的精确控制；基于拖拽的编辑则擅长形状结构变化，但无法细致控制纹理。两者各有所长，但也互有局限，因此亟需一种方法将两者优势结合，实现更自然、更可控的图像编辑。

Method: 作者提出了一种统一的扩散模型框架，实现文本和拖拽的联合编辑。核心创新包括：（1）Point-Cloud Deterministic Drag——通过3D特征映射提升潜空间布局精度；（2）Drag-Text Guided Denoising——在去噪过程中动态平衡文本与拖拽信息的影响。该模型支持灵活的编辑模式（仅文本、仅拖拽或两者结合）。

Result: 大量定量与定性实验表明，该方法不仅在联合编辑下实现高保真，且在文本或拖拽单独编辑情境中匹敌甚至超越现有专用方法。

Conclusion: 提出的方法在可控图像编辑上实现了通用性和高性能，是一种灵活且可扩展的解决方案。相关代码也将开源以便复现。

Abstract: This paper explores image editing under the joint control of text and drag
interactions. While recent advances in text-driven and drag-driven editing have
achieved remarkable progress, they suffer from complementary limitations:
text-driven methods excel in texture manipulation but lack precise spatial
control, whereas drag-driven approaches primarily modify shape and structure
without fine-grained texture guidance. To address these limitations, we propose
a unified diffusion-based framework for joint drag-text image editing,
integrating the strengths of both paradigms. Our framework introduces two key
innovations: (1) Point-Cloud Deterministic Drag, which enhances latent-space
layout control through 3D feature mapping, and (2) Drag-Text Guided Denoising,
dynamically balancing the influence of drag and text conditions during
denoising. Notably, our model supports flexible editing modes - operating with
text-only, drag-only, or combined conditions - while maintaining strong
performance in each setting. Extensive quantitative and qualitative experiments
demonstrate that our method not only achieves high-fidelity joint editing but
also matches or surpasses the performance of specialized text-only or drag-only
approaches, establishing a versatile and generalizable solution for
controllable image manipulation. Code will be made publicly available to
reproduce all results presented in this work.

</details>


### [76] [Enhancing Vehicle Detection under Adverse Weather Conditions with Contrastive Learning](https://arxiv.org/abs/2509.21916)
*Boying Li,Chang Liu,Petter Kyösti,Mattias Öhman,Devashish Singha Roy,Sofia Plazzi,Hamam Mokayed,Olle Hagner*

Main category: cs.CV

TL;DR: 本文提出了一种利用无标注数据提升轻量级模型在北欧地区无人机影像车辆检测性能的新方法，通过对比学习和多种特征融合实验，实现在复杂雪情下有效提升检测精度。


<details>
  <summary>Details</summary>
Motivation: 北欧地区无人机影像车辆检测面临能见度差和数据域变化大（如不同积雪程度）的特殊挑战，而有标注的数据获取昂贵，急需高效利用丰富的无标注数据提升检测性能。

Method: 提出了sideload-CL-adaptation框架。具体方法是在预训练阶段利用无标注数据进行对比学习训练CNN特征提取器，并在微调阶段将其侧载到冻结的YOLO11n骨干网络上。通过实验评估不同特征融合方式和粒度。

Result: 在NVD数据集上，所提模型在mAP50指标上较基线提升了3.8%到9.5%。

Conclusion: 通过对比学习与特征侧载方法，能更好适应雪覆盖变化大的复杂场景，且利用无标注数据能显著提升车辆检测模型性能。

Abstract: Aside from common challenges in remote sensing like small, sparse targets and
computation cost limitations, detecting vehicles from UAV images in the Nordic
regions faces strong visibility challenges and domain shifts caused by diverse
levels of snow coverage. Although annotated data are expensive, unannotated
data is cheaper to obtain by simply flying the drones. In this work, we
proposed a sideload-CL-adaptation framework that enables the use of unannotated
data to improve vehicle detection using lightweight models. Specifically, we
propose to train a CNN-based representation extractor through contrastive
learning on the unannotated data in the pretraining stage, and then sideload it
to a frozen YOLO11n backbone in the fine-tuning stage. To find a robust
sideload-CL-adaptation, we conducted extensive experiments to compare various
fusion methods and granularity. Our proposed sideload-CL-adaptation model
improves the detection performance by 3.8% to 9.5% in terms of mAP50 on the NVD
dataset.

</details>


### [77] [Taming Flow-based I2V Models for Creative Video Editing](https://arxiv.org/abs/2509.21917)
*Xianghao Kong,Hansheng Chen,Yuwei Guo,Lvmin Zhang,Gordon Wetzstein,Maneesh Agrawala,Anyi Rao*

Main category: cs.CV

TL;DR: 本论文提出了IF-V2V，一种适用于现有I2V模型的无反演视频编辑方法，通过新颖的向量场校正与结构-运动保持初始化，有效提升了编辑质量与一致性，实现了高效的视频编辑。


<details>
  <summary>Details</summary>
Motivation: 当前视频编辑仍面临挑战，尤其是在利用最新的图像-视频模型将图像编辑能力迁移到视频领域时，现有方法普遍需要复杂的模型设计或大量优化，限制了其实用性和灵活性。因此，亟需一种无需模型特定反演、且可直接适配流行I2V模型的高效视频编辑方法。

Method: 作者提出了IF-V2V方法，核心包括：（1）向量场校正与样本偏差，将源视频信息通过偏差项引入去噪过程，避免了反演操作；（2）结构-运动保持初始化，生成既包含结构信息又体现时序相关性的噪声，以提升编辑一致性和运动表现；（3）偏差缓存机制，进一步减少了这些新机制带来的计算负担。整个方法具备高度的通用性，可直接适配各类基于流配对的I2V模型。

Result: 实验结果表明，IF-V2V在编辑质量和一致性方面均优于现有主流方法，并且额外计算开销极低，能够作为轻量级即插即用方案，为用户带来更高效的视觉创作体验。

Conclusion: IF-V2V为视频编辑领域提供了一种通用、无反演、计算高效的新技术，不仅提升了编辑效果和一致性，还显著降低了实际应用门槛，有望推动图像模型能力向视频编辑领域的广泛迁移和应用。

Abstract: Although image editing techniques have advanced significantly, video editing,
which aims to manipulate videos according to user intent, remains an emerging
challenge. Most existing image-conditioned video editing methods either require
inversion with model-specific design or need extensive optimization, limiting
their capability of leveraging up-to-date image-to-video (I2V) models to
transfer the editing capability of image editing models to the video domain. To
this end, we propose IF-V2V, an Inversion-Free method that can adapt
off-the-shelf flow-matching-based I2V models for video editing without
significant computational overhead. To circumvent inversion, we devise Vector
Field Rectification with Sample Deviation to incorporate information from the
source video into the denoising process by introducing a deviation term into
the denoising vector field. To further ensure consistency with the source video
in a model-agnostic way, we introduce Structure-and-Motion-Preserving
Initialization to generate motion-aware temporally correlated noise with
structural information embedded. We also present a Deviation Caching mechanism
to minimize the additional computational cost for denoising vector
rectification without significantly impacting editing quality. Evaluations
demonstrate that our method achieves superior editing quality and consistency
over existing approaches, offering a lightweight plug-and-play solution to
realize visual creativity.

</details>


### [78] [Multi-View Crowd Counting With Self-Supervised Learning](https://arxiv.org/abs/2509.21918)
*Hong Mo,Xiong Zhang,Tengfei Shi,Zhongbo Wu*

Main category: cs.CV

TL;DR: 本文提出了一种新的自监督学习框架SSLCounter，针对多视角计数任务，利用神经体渲染减少对大规模标注数据的依赖，并取得了优异的性能和更高的数据利用效率。


<details>
  <summary>Details</summary>
Motivation: 现有多视角计数（MVC）方法普遍依赖大量标注数据，通过全监督学习提升性能，导致数据标注成本高。作者希望通过自监督学习方法，减少对标注数据的依赖。

Method: 提出了SSLCounter框架，结合神经体渲染方法，学习场景的隐式表示，并能通过可微分的神经渲染来重建2D视图下的形状和外观。该方法具有可插拔性，能够集成到现有MVC框架中。

Result: 在多个MVC基准上，SSLCounter实现了与最先进方法相比的优异性能。在训练数据量只有70%的情况下仍保持竞争力，展现了优秀的数据效率。

Conclusion: SSLCounter不仅能有效提升多视角计数的精度，还大大减少了对标注数据量的需求，对实际应用具有广泛的推广价值。

Abstract: Multi-view counting (MVC) methods have attracted significant research
attention and stimulated remarkable progress in recent years. Despite their
success, most MVC methods have focused on improving performance by following
the fully supervised learning (FSL) paradigm, which often requires large
amounts of annotated data. In this work, we propose SSLCounter, a novel
self-supervised learning (SSL) framework for MVC that leverages neural
volumetric rendering to alleviate the reliance on large-scale annotated
datasets. SSLCounter learns an implicit representation w.r.t. the scene,
enabling the reconstruction of continuous geometry shape and the complex,
view-dependent appearance of their 2D projections via differential neural
rendering. Owing to its inherent flexibility, the key idea of our method can be
seamlessly integrated into exsiting frameworks. Notably, extensive experiments
demonstrate that SSLCounter not only demonstrates state-of-the-art performances
but also delivers competitive performance with only using 70% proportion of
training data, showcasing its superior data efficiency across multiple MVC
benchmarks.

</details>


### [79] [Spatial Reasoning in Foundation Models: Benchmarking Object-Centric Spatial Understanding](https://arxiv.org/abs/2509.21922)
*Vahid Mirjalili,Ramin Giahi,Sriram Kollipara,Akshay Kekuda,Kehui Yao,Kai Zhao,Jianpeng Xu,Kaushiki Nag,Sinduja Subramaniam,Topojoy Biswas,Evren Korpeoglu,Kannan Achan*

Main category: cs.CV

TL;DR: 本文提出了一个专注于物体空间推理的新基准，通过合成数据集系统性评估了主流视觉模型和视觉-语言模型的空间理解能力，揭示了现有模型在空间推理和精细场景理解方面的不足。


<details>
  <summary>Details</summary>
Motivation: 随着视觉基础模型和视觉-语言模型发展，模型的识别能力显著增强，但现有评价主要考查目标定位，忽略了对物体空间关系、排列与分组等更复杂场景理解能力的评价。因此，有必要设计新的基准系统性衡量模型的空间推理能力。

Method: 作者构建了一个受控的合成数据集，设计了三个任务（空间定位、空间推理和下游检索任务），系统评估了数个主流检测器（如GroundingDINO、Florence-2、OWLv2）和大规模视觉-语言模型（如InternVL、LLaVA、GPT-4o）在空间理解方面的表现。

Result: 结果显示，检测器如GroundingDINO、OWLv2提供了精准的定位框，但空间关系和推理能力有限，而VLMs如SmolVLM、GPT-4o可给出粗粒度场景布局描述及流畅的文本，但在细致空间上下文推理上表现欠佳。整体表现揭示了定位能力与空间理解之间的权衡。

Conclusion: 当前主流视觉和视觉-语言模型虽然定位能力强，但在真实空间理解方面仍存在明显差距。社区需要研发具备更强空间感知能力的基础模型，从而提升视觉场景的综合理解能力。

Abstract: Spatial understanding is a critical capability for vision foundation models.
While recent advances in large vision models or vision-language models (VLMs)
have expanded recognition capabilities, most benchmarks emphasize localization
accuracy rather than whether models capture how objects are arranged and
related within a scene. This gap is consequential; effective scene
understanding requires not only identifying objects, but reasoning about their
relative positions, groupings, and depth. In this paper, we present a
systematic benchmark for object-centric spatial reasoning in foundation models.
Using a controlled synthetic dataset, we evaluate state-of-the-art vision
models (e.g., GroundingDINO, Florence-2, OWLv2) and large VLMs (e.g., InternVL,
LLaVA, GPT-4o) across three tasks: spatial localization, spatial reasoning, and
downstream retrieval tasks. We find a stable trade-off: detectors such as
GroundingDINO and OWLv2 deliver precise boxes with limited relational
reasoning, while VLMs like SmolVLM and GPT-4o provide coarse layout cues and
fluent captions but struggle with fine-grained spatial context. Our study
highlights the gap between localization and true spatial understanding, and
pointing toward the need for spatially-aware foundation models in the
community.

</details>


### [80] [PANICL: Mitigating Over-Reliance on Single Prompt in Visual In-Context Learning](https://arxiv.org/abs/2509.21926)
*Jiahao Zhang,Bowen Wang,Hong Liu,Yuta Nakashima,Hajime Nagahara*

Main category: cs.CV

TL;DR: 本文提出了一种名为PANICL的训练无关框架，通过利用多个输入-输出图像对（in-context pairs）来提升视觉任务中的模型表现，有效缓解了视觉上下文学习（VICL）中过度依赖单一样本导致的偏差和不稳定性，实验证明其在多项测试和不同领域迁移中均优于现有强基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉上下文学习（VICL）依赖输入-输出图像对作为提示，但往往过度依赖单一样本，导致预测结果出现偏差和不稳定性，亟需一种更健壮、泛化能力更强的方法。

Method: 作者提出了PANICL框架，结合了patch-based与k近邻思想，不依赖训练，直接在推理阶段利用多个in-context pairs，通过平滑不同pair之间的分配分数，减少模型对单个示例的偏倚，并能兼容多种VICL模型。

Result: 大量实验证明，PANICL在前景分割、单目标检测、图像上色、多目标分割、关键点检测等任务上，均超过了强基线模型。同时，对于数据集迁移和标签域迁移等领域泛化任务，PANICL表现出了良好的鲁棒性。

Conclusion: PANICL提供了一种训练无关、通用且高效的框架，能够显著缓解VICL中的偏置问题，并提升多种视觉任务的性能与泛化能力，适用于不同类型的VICL模型，具有广泛应用潜力。

Abstract: Visual In-Context Learning (VICL) uses input-output image pairs, referred to
as in-context pairs (or examples), as prompts alongside query images to guide
models in performing diverse vision tasks. However, VICL often suffers from
over-reliance on a single in-context pair, which can lead to biased and
unstable predictions. We introduce PAtch-based $k$-Nearest neighbor visual
In-Context Learning (PANICL), a general training-free framework that mitigates
this issue by leveraging multiple in-context pairs. PANICL smooths assignment
scores across pairs, reducing bias without requiring additional training.
Extensive experiments on a variety of tasks, including foreground segmentation,
single object detection, colorization, multi-object segmentation, and keypoint
detection, demonstrate consistent improvements over strong baselines. Moreover,
PANICL exhibits strong robustness to domain shifts, including dataset-level
shift (e.g., from COCO to Pascal) and label-space shift (e.g., FSS-1000), and
generalizes well to other VICL models such as SegGPT, Painter, and LVM,
highlighting its versatility and broad applicability.

</details>


### [81] [SingRef6D: Monocular Novel Object Pose Estimation with a Single RGB Reference](https://arxiv.org/abs/2509.21927)
*Jiahui Wang,Haiyue Zhu,Haoren Guo,Abdullah Al Mamun,Cheng Xiang,Tong Heng Lee*

Main category: cs.CV

TL;DR: 提出了一种无需深度传感器，仅用单张RGB参考图像即可进行6D姿态估计的新方法SingRef6D，并在标准数据集上超过了现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有6D姿态估计依赖于深度传感器或多视角获取，不适合透明、反光材料或低光无纹理场景，亟需更轻量且鲁棒的方案。

Method: 该方法仅需一张RGB参考图像，基于Depth-Anything v2提出token-scaler微调机制和新型优化损失，提升对复杂表面深度的预测准确性，并引入深度感知匹配，结合LoFTR进行空间关系融合，增强匹配鲁棒性。

Result: 在REAL275上，深度预测比原始Depth-Anything v2提升了14.41%；6D姿态估计在REAL275、ClearPose和Toyota-Light上平均召回率提升6.1%，均优于现有最优方案。

Conclusion: SingRef6D通过创新机制实现了对挑战性材料和光照下的鲁棒6D姿态估计，仅需单张RGB图像，极大拓宽了传统方法的适应场景和实际应用价值。

Abstract: Recent 6D pose estimation methods demonstrate notable performance but still
face some practical limitations. For instance, many of them rely heavily on
sensor depth, which may fail with challenging surface conditions, such as
transparent or highly reflective materials. In the meantime, RGB-based
solutions provide less robust matching performance in low-light and
texture-less scenes due to the lack of geometry information. Motivated by
these, we propose SingRef6D, a lightweight pipeline requiring only a single RGB
image as a reference, eliminating the need for costly depth sensors, multi-view
image acquisition, or training view synthesis models and neural fields. This
enables SingRef6D to remain robust and capable even under resource-limited
settings where depth or dense templates are unavailable. Our framework
incorporates two key innovations. First, we propose a token-scaler-based
fine-tuning mechanism with a novel optimization loss on top of Depth-Anything
v2 to enhance its ability to predict accurate depth, even for challenging
surfaces. Our results show a 14.41% improvement (in $\delta_{1.05}$) on REAL275
depth prediction compared to Depth-Anything v2 (with fine-tuned head). Second,
benefiting from depth availability, we introduce a depth-aware matching process
that effectively integrates spatial relationships within LoFTR, enabling our
system to handle matching for challenging materials and lighting conditions.
Evaluations of pose estimation on the REAL275, ClearPose, and Toyota-Light
datasets show that our approach surpasses state-of-the-art methods, achieving a
6.1% improvement in average recall.

</details>


### [82] [SemanticControl: A Training-Free Approach for Handling Loosely Aligned Visual Conditions in ControlNet](https://arxiv.org/abs/2509.21938)
*Woosung Joung,Daewon Chae,Jinkyu Kim*

Main category: cs.CV

TL;DR: 本文提出了一种在文本到图像扩散模型中利用与文本指令不完全对齐但语义相关的视觉条件的新方法，称为SemanticControl。通过辅助去噪和注意力掩码机制，有效提升了不严密对齐条件下的生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的ControlNet等方法强依赖与文本目标高度对齐的视觉条件（如边缘图、深度图），但实际中常常无法获得与文本完全匹配的视觉条件，尤其是在生成不常见或想象场景时。因此，作者希望提升模型对“语义上相关但不完全对齐”的视觉条件的适应能力。

Method: 作者提出了无需再训练的新方法SemanticControl。其核心是利用与条件视觉信息一致的临时代替文本（surrogate prompt）先进行一次辅助去噪过程，从而提取出有用的注意力掩码。在真正使用目标文本进行去噪时，这些注意力掩码被用来自适应抑制与文本冲突的视觉条件因素，并加强语言指导，从而更好地结合松耦合视觉条件和文本。

Result: 实验结果表明，SemanticControl在深度图、边缘图和人体骨架等多种视觉条件下，都优于现有基线，显著提升了语义不紧密对齐时的生成表现，包括文本忠实度和图像质量。

Conclusion: 本文方法实现了在松耦合、语义相关的视觉条件下，提升文本到图像生成模型的表现，为实际复杂和开放场景下的空间控制带来了新的解决方案。

Abstract: ControlNet has enabled detailed spatial control in text-to-image diffusion
models by incorporating additional visual conditions such as depth or edge
maps. However, its effectiveness heavily depends on the availability of visual
conditions that are precisely aligned with the generation goal specified by
text prompt-a requirement that often fails in practice, especially for uncommon
or imaginative scenes. For example, generating an image of a cat cooking in a
specific pose may be infeasible due to the lack of suitable visual conditions.
In contrast, structurally similar cues can often be found in more common
settings-for instance, poses of humans cooking are widely available and can
serve as rough visual guides. Unfortunately, existing ControlNet models
struggle to use such loosely aligned visual conditions, often resulting in low
text fidelity or visual artifacts. To address this limitation, we propose
SemanticControl, a training-free method for effectively leveraging misaligned
but semantically relevant visual conditions. Our approach adaptively suppresses
the influence of the visual condition where it conflicts with the prompt, while
strengthening guidance from the text. The key idea is to first run an auxiliary
denoising process using a surrogate prompt aligned with the visual condition
(e.g., "a human playing guitar" for a human pose condition) to extract
informative attention masks, and then utilize these masks during the denoising
of the actual target prompt (e.g., cat playing guitar). Experimental results
demonstrate that our method improves performance under loosely aligned
conditions across various conditions, including depth maps, edge maps, and
human skeletons, outperforming existing baselines. Our code is available at
https://mung3477.github.io/semantic-control.

</details>


### [83] [Customizing Visual Emotion Evaluation for MLLMs: An Open-vocabulary, Multifaceted, and Scalable Approach](https://arxiv.org/abs/2509.21950)
*Daiqing Wu,Dongbao Yang,Sicheng Zhao,Can Ma,Yu Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新的情感表达判断（Emotion Statement Judgment）任务和自动化评价流程，用于系统性评估多模态大模型（MLLMs）对图像情感的理解能力，并揭示了当前模型与人类在情感解读上的显著差距。


<details>
  <summary>Details</summary>
Motivation: 虽然MLLMs在多种任务上表现卓越，但其对图像情感的感知能力仍存在争议，部分原因在于当前的评估方法存在局限，如对合理响应忽视、情感分类有限、缺乏上下文、评注繁琐。研究动机在于解决这些评价瓶颈，为MLLM的情感智能发展建立基础。

Method: 作者提出了Emotion Statement Judgment任务，用于细致评估模型对视觉情感的理解；同时设计了一套自动化流程，低人工成本地生成与情感相关的陈述；随后，作者系统性对主流MLLMs进行评测，对比分析了其在不同情感推断方面的表现。

Result: 主流MLLMs在情感解释和基于上下文的情感判断方面表现较好，但在感知主观性理解上存在不足。即使是表现顶尖的GPT4o，相比人类在情感感知能力上仍有较大差距。

Conclusion: 作者构建了一个基础评价框架，通过实证评估强调了MLLMs在视觉情感理解上的进展与不足，为未来提升模型情感智能指明了方向。

Abstract: Recently, Multimodal Large Language Models (MLLMs) have achieved exceptional
performance across diverse tasks, continually surpassing previous expectations
regarding their capabilities. Nevertheless, their proficiency in perceiving
emotions from images remains debated, with studies yielding divergent results
in zero-shot scenarios. We argue that this inconsistency stems partly from
constraints in existing evaluation methods, including the oversight of
plausible responses, limited emotional taxonomies, neglect of contextual
factors, and labor-intensive annotations. To facilitate customized visual
emotion evaluation for MLLMs, we propose an Emotion Statement Judgment task
that overcomes these constraints. Complementing this task, we devise an
automated pipeline that efficiently constructs emotion-centric statements with
minimal human effort. Through systematically evaluating prevailing MLLMs, our
study showcases their stronger performance in emotion interpretation and
context-based emotion judgment, while revealing relative limitations in
comprehending perception subjectivity. When compared to humans, even
top-performing MLLMs like GPT4o demonstrate remarkable performance gaps,
underscoring key areas for future improvement. By developing a fundamental
evaluation framework and conducting a comprehensive MLLM assessment, we hope
this work contributes to advancing emotional intelligence in MLLMs. Project
page: https://github.com/wdqqdw/MVEI.

</details>


### [84] [MultiCrafter: High-Fidelity Multi-Subject Generation via Spatially Disentangled Attention and Identity-Aware Reinforcement Learning](https://arxiv.org/abs/2509.21953)
*Tao Wu,Yibo Jiang,Yehao Lu,Zhizhong Wang,Zeyi Huang,Zequn Qin,Xi Li*

Main category: cs.CV

TL;DR: MultiCrafter能够在多主体图像生成过程中，同时保持主体真实度、文本一致性与人类审美偏好，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多主体图像生成方法由于过度依赖简单的重构损失导致主体特征泄露严重，生成效果难以满足人类细致的偏好，限制了其实用性。

Method: 提出了MultiCrafter框架：1）通过显式位置监督，分离各主体的注意力区域，减少属性泄露；2）采用Mixture-of-Experts（专家混合）结构，不同专家关注不同场景，提高模型的泛化与理解能力；3）设计了在线强化学习框架，通过打分机制精确评估多主体保真度，并提出适配MoE结构的稳定训练策略，使模型对人类偏好对齐更好。

Result: 实验验证结果表明，MultiCrafter极大提升了多主体图像生成中的主体保真度，同时生成结果更贴合人类主观审美。

Conclusion: MultiCrafter框架解决了现有多主体图像生成中的主体属性泄露和偏好对齐问题，为高质量、多主体图像生成提供了有效方案。

Abstract: Multi-subject image generation aims to synthesize user-provided subjects in a
single image while preserving subject fidelity, ensuring prompt consistency,
and aligning with human aesthetic preferences. However, existing methods,
particularly those built on the In-Context-Learning paradigm, are limited by
their reliance on simple reconstruction-based objectives, leading to both
severe attribute leakage that compromises subject fidelity and failing to align
with nuanced human preferences. To address this, we propose MultiCrafter, a
framework that ensures high-fidelity, preference-aligned generation. First, we
find that the root cause of attribute leakage is a significant entanglement of
attention between different subjects during the generation process. Therefore,
we introduce explicit positional supervision to explicitly separate attention
regions for each subject, effectively mitigating attribute leakage. To enable
the model to accurately plan the attention region of different subjects in
diverse scenarios, we employ a Mixture-of-Experts architecture to enhance the
model's capacity, allowing different experts to focus on different scenarios.
Finally, we design a novel online reinforcement learning framework to align the
model with human preferences, featuring a scoring mechanism to accurately
assess multi-subject fidelity and a more stable training strategy tailored for
the MoE architecture. Experiments validate that our framework significantly
improves subject fidelity while aligning with human preferences better.

</details>


### [85] [PartSAM: A Scalable Promptable Part Segmentation Model Trained on Native 3D Data](https://arxiv.org/abs/2509.21965)
*Zhe Zhu,Le Wan,Rui Xu,Yiheng Zhang,Honghua Chen,Zhiyang Dou,Cheng Lin,Yuan Liu,Mingqiang Wei*

Main category: cs.CV

TL;DR: PartSAM是首个基于大规模3D数据训练且可通过提示进行零样本3D部件分割的模型，显著超越了依赖2D模型迁移的现有方法。


<details>
  <summary>Details</summary>
Motivation: 3D对象的部件分割长期难题，现有方法往往依赖2D模型迁移，限制了对未见3D对象的普适性和几何理解，缺乏真实3D原生能力。

Method: 提出PartSAM，一种受SAM启发、具备encoder-decoder架构的3D分割模型。该模型采用三平面(triplane)双分支编码器构建空间结构化的部件特征表示，通过“模型辅助”标注流程，从线上资源中自动构建了500万高质量3D形状与部件对，保证数据量和标注多样性。

Result: PartSAM展示了强大的开放世界能力：可通过单一提示实现高精度部件识别，也可自动分解3D对象的表面及内部结构。在多个基准数据集上显著超过现有最优方法。

Conclusion: PartSAM推动了3D部件解析基础模型的进步，首次可大规模、原生地实现3D对象的精细化、提示驱动分割。代码与模型即将开源。

Abstract: Segmenting 3D objects into parts is a long-standing challenge in computer
vision. To overcome taxonomy constraints and generalize to unseen 3D objects,
recent works turn to open-world part segmentation. These approaches typically
transfer supervision from 2D foundation models, such as SAM, by lifting
multi-view masks into 3D. However, this indirect paradigm fails to capture
intrinsic geometry, leading to surface-only understanding, uncontrolled
decomposition, and limited generalization. We present PartSAM, the first
promptable part segmentation model trained natively on large-scale 3D data.
Following the design philosophy of SAM, PartSAM employs an encoder-decoder
architecture in which a triplane-based dual-branch encoder produces spatially
structured tokens for scalable part-aware representation learning. To enable
large-scale supervision, we further introduce a model-in-the-loop annotation
pipeline that curates over five million 3D shape-part pairs from online assets,
providing diverse and fine-grained labels. This combination of scalable
architecture and diverse 3D data yields emergent open-world capabilities: with
a single prompt, PartSAM achieves highly accurate part identification, and in a
Segment-Every-Part mode, it automatically decomposes shapes into both surface
and internal structures. Extensive experiments show that PartSAM outperforms
state-of-the-art methods by large margins across multiple benchmarks, marking a
decisive step toward foundation models for 3D part understanding. Our code and
model will be released soon.

</details>


### [86] [No-Reference Image Contrast Assessment with Customized EfficientNet-B0](https://arxiv.org/abs/2509.21967)
*Javad Hassannataj Joloudari,Bita Mesbahzadeh,Omid Zare,Emrah Arslan,Roohallah Alizadehsani,Hossein Moosaei*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的无参考图像对比度质量评价方法，结合了多种预训练网络，实现了在合成和真实对比度失真数据集上的准确预测，对实时和资源受限应用具有实际意义。


<details>
  <summary>Details</summary>
Motivation: 现有的无参考图像质量评价（NR-IQA）方法在复杂真实场景下难以准确评估对比度失真，而图像对比度对视觉感知和图像整体质量至关重要。

Method: 将三个主流轻量级预训练网络（EfficientNet B0、ResNet18、MobileNetV2）进行定制并微调，结合人类主观评分作为监督信号。每个网络添加对比度感知回归头，通过数据增强，在CID2013和CCID2014两个包含合成及真实对比度失真的基准数据集上端到端训练和测试。还尝试了Siamese结构进行对比学习。

Result: 定制后的EfficientNet B0模型在两个数据集上均取得了最新最优的PLCC和SRCC分数（如CCID2014上PLCC=0.9286，SRCC=0.9178，CID2013上PLCC=0.9581，SRCC=0.9369），明显超过传统方法和其他深度学习基线。Siamese结构未表现出显著提升。

Conclusion: 轻量级预训练网络经对比度感知适配后，可实现高效、稳健的无参考对比度质量评价，适用于实时及资源受限场景，具有较强的实用价值。

Abstract: Image contrast was a fundamental factor in visual perception and played a
vital role in overall image quality. However, most no reference image quality
assessment NR IQA models struggled to accurately evaluate contrast distortions
under diverse real world conditions. In this study, we proposed a deep learning
based framework for blind contrast quality assessment by customizing and
fine-tuning three pre trained architectures, EfficientNet B0, ResNet18, and
MobileNetV2, for perceptual Mean Opinion Score, along with an additional model
built on a Siamese network, which indicated a limited ability to capture
perceptual contrast distortions. Each model is modified with a contrast-aware
regression head and trained end to end using targeted data augmentations on two
benchmark datasets, CID2013 and CCID2014, containing synthetic and authentic
contrast distortions. Performance is evaluated using Pearson Linear Correlation
Coefficient and Spearman Rank Order Correlation Coefficient, which assess the
alignment between predicted and human rated scores. Among these three models,
our customized EfficientNet B0 model achieved state-of-the-art performance with
PLCC = 0.9286 and SRCC = 0.9178 on CCID2014 and PLCC = 0.9581 and SRCC = 0.9369
on CID2013, surpassing traditional methods and outperforming other deep
baselines. These results highlighted the models robustness and effectiveness in
capturing perceptual contrast distortion. Overall, the proposed method
demonstrated that contrast aware adaptation of lightweight pre trained networks
can yield a high performing, scalable solution for no reference contrast
quality assessment suitable for real time and resource constrained
applications.

</details>


### [87] [Geo-R1: Improving Few-Shot Geospatial Referring Expression Understanding with Reinforcement Fine-Tuning](https://arxiv.org/abs/2509.21976)
*Zilun Zhang,Zian Guan,Tiancheng Zhao,Haozhan Shen,Tianyu Li,Yuxiang Cai,Zhonggen Su,Zhaojun Liu,Jianwei Yin,Xiang Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的地理空间引用理解方法Geo-R1，通过强化推理链生成提升模型在少样本场景下的表现，并在多个基准测试上超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 遥感领域的指代表达理解需要对复杂的目标与上下文关系进行推理。现有多模态大模型在有大量标注数据时表现优秀，但在数据稀缺情况下泛化能力较差。为解决这一问题，作者希望提升模型在少样本情景下的理解能力与泛化表现。

Method: 提出Geo-R1范式，使用强化推理微调(RFT)。具体做法是：模型先显式生成可解释的推理链，将复杂表达拆解为推理过程，再基于这些推理链定位目标。通过“先推理，后行动”的机制，提升模型利用有限标注的能力，并增强模型可解释性。

Result: 在三个精心设计的少样本地理空间基准上，Geo-R1在所有任务中均显著优于传统的监督微调(SFT)方法。此外，Geo-R1在跨数据集泛化方面也表现强劲，说明其鲁棒性较好。

Conclusion: Geo-R1通过引入推理链和强化训练，有效提升了遥感领域在少样本和泛化环境下的指代表达理解能力，并增加了模型的可解释性，优于传统方法。

Abstract: Referring expression understanding in remote sensing poses unique challenges,
as it requires reasoning over complex object-context relationships. While
supervised fine-tuning (SFT) on multimodal large language models achieves
strong performance with massive labeled datasets, they struggle in data-scarce
scenarios, leading to poor generalization. To address this limitation, we
propose Geo-R1, a reasoning-centric reinforcement fine-tuning (RFT) paradigm
for few-shot geospatial referring. Geo-R1 enforces the model to first generate
explicit, interpretable reasoning chains that decompose referring expressions,
and then leverage these rationales to localize target objects. This "reason
first, then act" process enables the model to make more effective use of
limited annotations, enhances generalization, and provides interpretability. We
validate Geo-R1 on three carefully designed few-shot geospatial referring
benchmarks, where our model consistently and substantially outperforms SFT
baselines. It also demonstrates strong cross-dataset generalization,
highlighting its robustness. Code and data will be released at
http://geo-r1.github.io.

</details>


### [88] [Benchmarking and Mitigate Psychological Sycophancy in Medical Vision-Language Models](https://arxiv.org/abs/2509.21979)
*Zikun Guo,Xinyue Xu,Pei Xiang,Shu Yang,Xin Han,Di Wang,Lijie Hu*

Main category: cs.CV

TL;DR: 本文发现视觉语言模型(VLMs)在医学问答场景下容易出现“谄媚性”行为，提出了新的数据集与评测方法，并通过VIPER策略，有效缓解该问题，提高结果的可靠性。


<details>
  <summary>Details</summary>
Motivation: 医疗领域正广泛采用视觉语言模型，但这些模型倾向于优先迎合用户说法或权威，而非基于证据作出判断。这种“谄媚”行为可能影响诊断安全与质量，因此作者希望系统性评估和解决该问题。

Method: 1. 构建了基于PathVQA、SLAKE和VQA-RAD等数据集的医学谄媚性评测数据集，按器官系统与成像模态分层。2. 设计包含心理学动机的压力模板，分别引入不同的谄媚诱因。3. 对多种VLMs进行对抗性实验, 测试在不同诱因下的表现。4. 提出VIPER方法，通过过滤非证据性内容，生成被证据约束的答案，以减轻谄媚问题。

Result: 实验证明，多数VLM模型在遭遇谄媚性诱因时容易受到影响，且这种现象与模型准确性或规模弱相关。“模仿”及“专家纠正”类诱因最易触发谄媚行为。VIPER方法能够显著减少谄媚回答比例，同时保持可解释性，并优于现有基线方法。

Conclusion: 当前VLM在医疗对话和问答任务中易受谄媚性偏见影响，需采用基于证据的防护策略。VIPER方案为模型安全部署提供了可行路径，强调了医学AI系统中证据优先的重要性。

Abstract: Vision language models(VLMs) are increasingly integrated into clinical
workflows, but they often exhibit sycophantic behavior prioritizing alignment
with user phrasing social cues or perceived authority over evidence based
reasoning. This study evaluate clinical sycophancy in medical visual question
answering through a novel clinically grounded benchmark. We propose a medical
sycophancy dataset construct from PathVQA, SLAKE, and VQA-RAD stratified by
different type organ system and modality. Using psychologically motivated
pressure templates including various sycophancy. In our adversarial experiments
on various VLMs, we found that these models are generally vulnerable,
exhibiting significant variations in the occurrence of adversarial responses,
with weak correlations to the model accuracy or size. Imitation and expert
provided corrections were found to be the most effective triggers, suggesting
that the models possess a bias mechanism independent of visual evidence. To
address this, we propose Visual Information Purification for Evidence based
Response (VIPER) a lightweight mitigation strategy that filters non evidentiary
content for example social pressures and then generates constrained evidence
first answers. This framework reduces sycophancy by an average amount
outperforming baselines while maintaining interpretability. Our benchmark
analysis and mitigation framework lay the groundwork for robust deployment of
medical VLMs in real world clinician interactions emphasizing the need for
evidence anchored defenses.

</details>


### [89] [Resolving Ambiguity in Gaze-Facilitated Visual Assistant Interaction Paradigm](https://arxiv.org/abs/2509.21980)
*Zeyu Wang,Baiyu Chen,Kun Yan,Hongjing Piao,Hao Xue,Flora D. Salim,Yuanchun Shi,Yuntao Wang*

Main category: cs.CV

TL;DR: 本文提出了GLARIFY方法，通过融合人眼凝视的时空信息提升视觉-语言模型（VLMs）在智能眼镜等现实多模态场景下的效果，有效解决了用户语言模糊与凝视信号不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: 随着智能眼镜普及，多模态（语音+视觉）查询需求增加，但传统VLMs仅以单幅图像为输入，无法捕捉用户注意力的动态变化。而直接利用凝视数据又带来模糊与噪声问题，例如用户问题用代词表达、凝视模式复杂且不稳定。

Method: 1）分析带有凝视数据的多样查询案例，揭示人眼凝视的复杂性与噪声特点；2）利用GPT-4o搭建自动数据合成流程，生成包含链式思考CoT的GLARIFY-Ambi数据集，有助于模型处理噪声凝视；3）设计热力图模块，将凝视数据有效融入主流VLMs，并保持其预训练知识优势。

Result: 在严格的测试集上，GLARIFY显著优于现有基线方法。方法在对齐人类注意力与提升实际效果方面有明确优势。

Conclusion: GLARIFY通过创新地利用凝视的时空特征，有效提升了VLMs实际交互的鲁棒性和易用性，为智能视觉助手领域的自然直观交互模式奠定了基础。

Abstract: With the rise in popularity of smart glasses, users' attention has been
integrated into Vision-Language Models (VLMs) to streamline multi-modal
querying in daily scenarios. However, leveraging gaze data to model users'
attention may introduce ambiguity challenges: (1) users' verbal questions
become ambiguous by using pronouns or skipping context, (2) humans' gaze
patterns can be noisy and exhibit complex spatiotemporal relationships with
their spoken questions. Previous works only consider single image as visual
modality input, failing to capture the dynamic nature of the user's attention.
In this work, we introduce GLARIFY, a novel method to leverage spatiotemporal
gaze information to enhance the model's effectiveness in real-world
applications. Initially, we analyzed hundreds of querying samples with the gaze
modality to demonstrate the noisy nature of users' gaze patterns. We then
utilized GPT-4o to design an automatic data synthesis pipeline to generate the
GLARIFY-Ambi dataset, which includes a dedicated chain-of-thought (CoT) process
to handle noisy gaze patterns. Finally, we designed a heatmap module to
incorporate gaze information into cutting-edge VLMs while preserving their
pretrained knowledge. We evaluated GLARIFY using a hold-out test set.
Experiments demonstrate that GLARIFY significantly outperforms baselines. By
robustly aligning VLMs with human attention, GLARIFY paves the way for a usable
and intuitive interaction paradigm with a visual assistant.

</details>


### [90] [From Bias to Balance: Exploring and Mitigating Spatial Bias in LVLMs](https://arxiv.org/abs/2509.21984)
*Yingjie Zhu,Xuefeng Bai,Kehai Chen,Yang Xiang,Weili Guan,Jun Yu,Min Zhang*

Main category: cs.CV

TL;DR: 本文系统性分析了大型视觉-语言模型（LVLMs）在空间变化下的鲁棒性问题，并提出了能提升其空间鲁棒性的简单高效方法BaPA。


<details>
  <summary>Details</summary>
Motivation: 尽管LVLMs在多模态任务上取得了巨大成功，但其对图像空间中视觉信息位置变化的鲁棒性尚不清楚，实际应用中存在输出不一致的隐患。作者旨在揭示和解决LVLMs在空间-语义理解上的局限。

Method: 作者通过构建特定探测数据集，分析不同图像位置的同一关键信息对模型输出的影响，并进一步定位问题源于语言模型部分的位置嵌入不平衡。为此，作者提出Balanced Position Assignment（BaPA）方法，为所有图像token分配相同的位置嵌入，从而实现视觉信息的平衡融合。

Result: 实验表明，LVLMs 在空间信息发生变化时会产生不一致的输出。采用BaPA后，无需重新训练即可显著提升模型的空间鲁棒性，并在配合轻量级微调时提升多模态任务表现。信息流分析显示BaPA能实现更均衡的注意力分布和全面的视觉理解。

Conclusion: LVLMs存在空间偏置问题，现有的位置编码策略影响空间-语义理解，BaPA方法能有效缓解该问题并提升模型空间鲁棒性和多模态性能。

Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable success across
a wide range of multimodal tasks, yet their robustness to spatial variations
remains insufficiently understood. In this work, we present a systematic study
of the spatial bias of LVLMs, focusing on how models respond when identical key
visual information is placed at different locations within an image. Through a
carefully designed probing dataset, we demonstrate that current LVLMs often
produce inconsistent outputs under such spatial shifts, revealing a fundamental
limitation in their spatial-semantic understanding. Further analysis shows that
this phenomenon originates not from the vision encoder, which reliably
perceives and interprets visual content across positions, but from the
unbalanced design of position embeddings in the language model component. In
particular, the widely adopted position embedding strategies, such as RoPE,
introduce imbalance during cross-modal interaction, leading image tokens at
different positions to exert unequal influence on semantic understanding. To
mitigate this issue, we introduce Balanced Position Assignment (BaPA), a simple
yet effective mechanism that assigns identical position embeddings to all image
tokens, promoting a more balanced integration of visual information. Extensive
experiments show that BaPA enhances the spatial robustness of LVLMs without
retraining and further boosts their performance across diverse multimodal
benchmarks when combined with lightweight fine-tuning. Further analysis of
information flow reveals that BaPA yields balanced attention, enabling more
holistic visual understanding.

</details>


### [91] [Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation](https://arxiv.org/abs/2509.21989)
*Abdelrahman Eldesokey,Aleksandar Cvejic,Bernard Ghanem,Peter Wonka*

Main category: cs.CV

TL;DR: 作者提出了一种新方法，从扩散模型中解耦视觉特征与语义特征，实现类似语义对应的视觉对应及不一致性检测。方法通过自动管道构建具备视觉和语义标注的图像对，并采用对比架构分离特征类型。提出新的VSM指标，能量化和定位主图像生成过程中的视觉不一致性，在实验中优于主流的全局特征指标，如CLIP、DINO等。该方法首次实现了主导生成任务中的不一致性度量与定位。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散模型在合成图像时需要既有语义特征也有视觉特征，但如何在无标注数据下分离出纯视觉特征极具挑战。现有指标难以细粒度评估或定位生成图像在视觉一致性上的问题，缺少相应工具限制了领域进步。

Method: 提出自动化管道，基于现有受控主体生成数据集，合成带有视觉和语义对应标注的图像对。设计对比学习架构实现语义和视觉特征的解耦，并据此提出视觉语义匹配(VSM)新指标，可定量分析并空间定位生成图像中的视觉不一致。

Result: 实验发现，所提方法在量化主导图像生成中的视觉不一致性时，显著优于CLIP、DINO等主流全局特征指标，并可实现不一致区域的空间定位。

Conclusion: 本工作为主导生成任务中图像视觉和语义特征解耦及不一致性检测提供了新工具，不仅提升了该任务的评估能力，也推动了算法的发展。

Abstract: We propose a novel approach for disentangling visual and semantic features
from the backbones of pre-trained diffusion models, enabling visual
correspondence in a manner analogous to the well-established semantic
correspondence. While diffusion model backbones are known to encode
semantically rich features, they must also contain visual features to support
their image synthesis capabilities. However, isolating these visual features is
challenging due to the absence of annotated datasets. To address this, we
introduce an automated pipeline that constructs image pairs with annotated
semantic and visual correspondences based on existing subject-driven image
generation datasets, and design a contrastive architecture to separate the two
feature types. Leveraging the disentangled representations, we propose a new
metric, Visual Semantic Matching (VSM), that quantifies visual inconsistencies
in subject-driven image generation. Empirical results show that our approach
outperforms global feature-based metrics such as CLIP, DINO, and
vision--language models in quantifying visual inconsistencies while also
enabling spatial localization of inconsistent regions. To our knowledge, this
is the first method that supports both quantification and localization of
inconsistencies in subject-driven generation, offering a valuable tool for
advancing this task. Project
Page:https://abdo-eldesokey.github.io/mind-the-glitch/

</details>


### [92] [WAVE: Learning Unified & Versatile Audio-Visual Embeddings with Multimodal LLM](https://arxiv.org/abs/2509.21990)
*Changli Tang,Qinfan Xiao,Ke Mei,Tianyi Wang,Fengyun Rao,Chao Zhang*

Main category: cs.CV

TL;DR: WAVE提出了一种新型多模态大模型嵌入方法，实现了文本、音频和视频的统一表征，并在多项多模态检索与问答任务上取得了新SOTA。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态大模型的嵌入在文本等领域表现优异，但对于音频、视频等动态模态的通用表征方法仍缺乏充分探索，需要一种能将三种模态统一到一个表示空间的方法。

Method: WAVE采用了新颖的分层特征融合策略，以及联合多模态、多任务训练方法。它通过统一的嵌入模型实现了跨模态的'任意对任意'检索，并能根据用户指令生成提示感知型嵌入表达。

Result: WAVE在MMEB-v2视频基准上取得了新的SOTA，在音视频检索任务（如音频到视频、视频到音频）上效果显著优于现有模型，在多模态问答等任务也有极佳表现。消融实验验证了方法的有效性。

Conclusion: WAVE作为首个基于LLM的统一音频-视觉嵌入方法，极大提升了多模态应用的性能和灵活性，为跨模态的通用检索与理解场景提供了创新的技术路线。

Abstract: While embeddings from multimodal large language models (LLMs) excel as
general-purpose representations, their application to dynamic modalities like
audio and video remains underexplored. We introduce WAVE (\textbf{u}nified \&
\textbf{v}ersatile \textbf{a}udio-\textbf{v}isual \textbf{e}mbeddings), the
first LLM-based embedding that creates a unified representation space for text,
audio, and video modalities. WAVE employs a novel hierarchical feature fusion
strategy and a joint multi-modal, multi-task training approach to enable two
key capabilities: any-to-any cross-modal retrieval and the generation of
prompt-aware embeddings tailored to user instructions. Experimentally, WAVE
sets a new state-of-the-art on the MMEB-v2 video benchmark and achieves
superior results in audio and video-to-audio retrieval. Its prompt-aware nature
also yields remarkable performance in multimodal question answering,
significantly outperforming existing embedding models. Ablation studies
validate our joint training strategy, demonstrating improved performance across
all modalities. With a newly introduced benchmark for versatile audio-visual
learning, WAVE opens up broad possibilities for cross-modal, any-to-any
applications. Our code, checkpoints, and data will be released.

</details>


### [93] [ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models](https://arxiv.org/abs/2509.21991)
*Jewon Lee,Wooksu Shin,Seungmin Yang,Ki-Ung Song,DongUk Lim,Jaeyeon Kim,Tae-Ho Kim,Bo-Kyeong Kim*

Main category: cs.CV

TL;DR: 提出一种高效处理高分辨率图像的方法——ERGO，实现了对视觉语言模型在推理速度和准确性的双提升。该方法对图像分阶段处理，显著减少计算量，同时保留有效信息。


<details>
  <summary>Details</summary>
Motivation: 高分辨率图像对视觉语言模型（LVLM）的计算资源消耗极大，实际应用时推理慢、成本高。随着'以图思考'模型发展，有必要提升模型在视觉推理效率和有效性。

Method: 采用“粗到细”的两阶段推理流程：第一步对下采样图像分析，定位任务相关区域；第二步对这些区域进行高分辨率裁剪和深入处理。为解决任务相关区域识别困难，提出ERGO方法，利用多模态上下文做推理驱动的感知，同时可对视觉不确定区域自适应扩展裁剪，并在强化学习框架下设计奖励机制优化流程。

Result: ERGO在多项数据集上取得比基础模型和同类方法更高的准确率，同时计算效率更高。例如，在V*基准上比Qwen2.5-VL-7B高4.7分，仅用23%视觉token，实现3倍推理加速。

Conclusion: ERGO显著提升了高分辨率图像下视觉语言推理的效率与准确率，为实际视觉-语言联合应用带来更优解，同时消耗更低算力。

Abstract: Efficient processing of high-resolution images is crucial for real-world
vision-language applications. However, existing Large Vision-Language Models
(LVLMs) incur substantial computational overhead due to the large number of
vision tokens. With the advent of "thinking with images" models, reasoning now
extends beyond text to the visual domain. This capability motivates our
two-stage "coarse-to-fine" reasoning pipeline: first, a downsampled image is
analyzed to identify task-relevant regions; then, only these regions are
cropped at full resolution and processed in a subsequent reasoning stage. This
approach reduces computational cost while preserving fine-grained visual
details where necessary. A major challenge lies in inferring which regions are
truly relevant to a given query. Recent related methods often fail in the first
stage after input-image downsampling, due to perception-driven reasoning, where
clear visual information is required for effective reasoning. To address this
issue, we propose ERGO (Efficient Reasoning & Guided Observation) that performs
reasoning-driven perception-leveraging multimodal context to determine where to
focus. Our model can account for perceptual uncertainty, expanding the cropped
region to cover visually ambiguous areas for answering questions. To this end,
we develop simple yet effective reward components in a reinforcement learning
framework for coarse-to-fine perception. Across multiple datasets, our approach
delivers higher accuracy than the original model and competitive methods, with
greater efficiency. For instance, ERGO surpasses Qwen2.5-VL-7B on the V*
benchmark by 4.7 points while using only 23% of the vision tokens, achieving a
3x inference speedup. The code and models can be found at:
https://github.com/nota-github/ERGO.

</details>


### [94] [DualFocus: Depth from Focus with Spatio-Focal Dual Variational Constraints](https://arxiv.org/abs/2509.21992)
*Sungmin Woo,Sangyoun Lee*

Main category: cs.CV

TL;DR: 本文提出DualFocus，一种新的深度聚焦（DFF）方法，通过联合建模空间和焦点维度的梯度变化，提升复杂场景下深度估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有学习型DFF方法在处理细纹理或深度突变的复杂场景时，因聚焦线索不清晰，导致性能下降。作者希望解决深度边界易混淆、纹理假象多的问题，提高聚焦估计的健壮性和精度。

Method: DualFocus在方法上引入变分建模框架，设计了空间和焦点两个约束。空间约束利用焦点变化下的梯度模式，区分真实深度边缘与纹理伪影；焦点约束则引入单峰、单调的聚焦概率，符合物理聚焦规律。两者结合提升了模型对复杂区域的识别能力。

Result: 在四个公共数据集上的实验结果表明，DualFocus在深度估计准确率与感知质量上，均优于现有主流方法。

Conclusion: DualFocus通过合理的空间与焦点约束，增强了深度聚焦估计的鲁棒性与效果，尤其适用于复杂纹理和深度突变场景，在DFF领域具有明显优势。

Abstract: Depth-from-Focus (DFF) enables precise depth estimation by analyzing focus
cues across a stack of images captured at varying focal lengths. While recent
learning-based approaches have advanced this field, they often struggle in
complex scenes with fine textures or abrupt depth changes, where focus cues may
become ambiguous or misleading. We present DualFocus, a novel DFF framework
that leverages the focal stack's unique gradient patterns induced by focus
variation, jointly modeling focus changes over spatial and focal dimensions.
Our approach introduces a variational formulation with dual constraints
tailored to DFF: spatial constraints exploit gradient pattern changes across
focus levels to distinguish true depth edges from texture artifacts, while
focal constraints enforce unimodal, monotonic focus probabilities aligned with
physical focus behavior. These inductive biases improve robustness and accuracy
in challenging regions. Comprehensive experiments on four public datasets
demonstrate that DualFocus consistently outperforms state-of-the-art methods in
both depth accuracy and perceptual quality.

</details>


### [95] [Rate-Distortion Optimized Communication for Collaborative Perception](https://arxiv.org/abs/2509.21994)
*Genjia Liu,Anning Hu,Yue Hu,Wenjun Zhang,Siheng Chen*

Main category: cs.CV

TL;DR: 提出了一种基于率失真理论的高效多智能体协同感知框架RDcomm，在保障目标任务性能的同时，大幅减少了通信量。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体协同感知研究关注通信效率与任务表现之间的经验性权衡，但理论基础仍不完善。作者希望用理论方法指导通信策略的优化，实现更高效的信息共享。

Method: 本文以信息论为基础，扩展并提出了适用于多智能体协作的实际率失真理论，明确了“提供任务相关信息”和“冗余最小化”两大原则。基于此，提出RDcomm框架：1）引入任务熵离散编码，根据特征的任务相关性分配编码长度；2）基于互信息进行信息筛选，实现信息冗余最小化。

Result: 在DAIR-V2X和OPV2V两个数据集上的3D目标检测和BEV分割实验中，RDcomm框架在大幅减少（最多108倍）通信量的同时，实现了比现有方法更高的准确率。

Conclusion: 本文提供了多智能体感知系统中性能与通信效率权衡的理论和方法创新，为高效协同感知系统的设计提供了理论基础和实践工具。

Abstract: Collaborative perception emphasizes enhancing environmental understanding by
enabling multiple agents to share visual information with limited bandwidth
resources. While prior work has explored the empirical trade-off between task
performance and communication volume, a significant gap remains in the
theoretical foundation. To fill this gap, we draw on information theory and
introduce a pragmatic rate-distortion theory for multi-agent collaboration,
specifically formulated to analyze performance-communication trade-off in
goal-oriented multi-agent systems. This theory concretizes two key conditions
for designing optimal communication strategies: supplying pragmatically
relevant information and transmitting redundancy-less messages. Guided by these
two conditions, we propose RDcomm, a communication-efficient collaborative
perception framework that introduces two key innovations: i) task entropy
discrete coding, which assigns features with task-relevant codeword-lengths to
maximize the efficiency in supplying pragmatic information; ii)
mutual-information-driven message selection, which utilizes mutual information
neural estimation to approach the optimal redundancy-less condition.
Experiments on 3D object detection and BEV segmentation demonstrate that RDcomm
achieves state-of-the-art accuracy on DAIR-V2X and OPV2V, while reducing
communication volume by up to 108 times. The code will be released.

</details>


### [96] [FailureAtlas:Mapping the Failure Landscape of T2I Models via Active Exploration](https://arxiv.org/abs/2509.21995)
*Muxi Chen,Zhaohua Zhang,Chenchen Zhao,Mingyang Chen,Wenyu Jiang,Tianwen Jiang,Jianhuan Zhuo,Yu Tang,Qiuyong Xiao,Jihong Zhang,Qiang Xu*

Main category: cs.CV

TL;DR: 该论文提出了FailureAtlas框架，用于主动探索和绘制文本生成图片（T2I）模型的失败版图，大幅提升了模型故障诊断能力。


<details>
  <summary>Details</summary>
Motivation: 现有静态基准只被动对比T2I模型，难以发现和定位系统性失效。本研究希望通过主动探索方法，更全面、深入地揭示和分析模型的失败类型及根因。

Method: 提出FailureAtlas系统，通过结构化搜索最小失效概念并引入加速技术，使在大规模下探索模型失败成为可行。将其应用于Stable Diffusion等模型，自动发现大量未被察觉的错误。

Result: 在Stable Diffusion 1.5模型上，FailureAtlas发现了超过24.7万个新的失效片段，并首次在大规模上关联这些失败与训练集中的数据稀缺问题。

Conclusion: FailureAtlas为深度模型审核提供了可扩展的方法，推动了以诊断为先的研究范式，有望指导更健壮的生成式AI系统开发。

Abstract: Static benchmarks have provided a valuable foundation for comparing
Text-to-Image (T2I) models. However, their passive design offers limited
diagnostic power, struggling to uncover the full landscape of systematic
failures or isolate their root causes. We argue for a complementary paradigm:
active exploration. We introduce FailureAtlas, the first framework designed to
autonomously explore and map the vast failure landscape of T2I models at scale.
FailureAtlas frames error discovery as a structured search for minimal,
failure-inducing concepts. While it is a computationally explosive problem, we
make it tractable with novel acceleration techniques. When applied to Stable
Diffusion models, our method uncovers hundreds of thousands of previously
unknown error slices (over 247,000 in SD1.5 alone) and provides the first
large-scale evidence linking these failures to data scarcity in the training
set. By providing a principled and scalable engine for deep model auditing,
FailureAtlas establishes a new, diagnostic-first methodology to guide the
development of more robust generative AI. The code is available at
https://github.com/cure-lab/FailureAtlas

</details>


### [97] [Exposing Hallucinations To Suppress Them: VLMs Representation Editing With Generative Anchors](https://arxiv.org/abs/2509.21997)
*Youxu Shi,Suorong Yang,Dong Liu*

Main category: cs.CV

TL;DR: MLLMs容易产生与视觉证据不符的幻觉内容，文中提出了一种无需额外训练的自监督方法，通过利用文本到图像模型放大幻觉信号并对解码器进行语义校正，有效减少幻觉发生且可广泛适配不同架构。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大模型已经在视觉-语言任务上取得巨大进展，但“幻觉”问题依然严重，传统缓解方法多依赖精调、人工规则或会影响其它性能，因此急需一种无需训练、通用性强的解决方式。

Method: 作者提出利用文本到图像生成模型，把生成的描述投影到视觉空间，放大其中的幻觉信号作为负锚点，并以原始图像为正锚点。通过这两个锚点，对解码器的隐状态向真实语义靠拢、远离幻觉方向，从而校正输出。整个过程属自监督范畴，无需人工先验与额外训练。

Result: 实验在多个基准数据集上进行，结果显示该方法能够大幅降低物体、属性和关系层面的幻觉率（如LLaVA-v1.5-7B在CHAIR集上降低超5%幻觉），且大部分保持召回率和描述丰富度。方法同样适用于多种MLLM架构且具良好泛化性。

Conclusion: 提出方法无需finetune或任何人工干预即可有效减少MLLM幻觉，兼具高效、稳健与可插拔性，对实际应用有极大价值，实现代码会公开。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable success
across diverse vision-language tasks, yet they remain highly susceptible to
hallucinations, producing content that is fluent but inconsistent with visual
evidence. Such hallucinations, spanning objects, attributes, and relations,
persist even in larger models, while existing mitigation approaches often
require additional finetuning, handcrafted priors, or trade-offs that
compromise informativeness and scalability. To address this limitation, we
propose a training-free, self-supervised method for hallucination mitigation.
Our approach introduces a novel hallucination amplification mechanism: a
caption is projected into the visual space via a text-to-image model to reveal
implicit hallucination signals, serving as a negative anchor, while the
original image provides a positive anchor. Leveraging these dual anchors, we
edit decoder hidden states by pulling representations toward faithful semantics
and pushing them away from hallucination directions. This correction requires
no human priors or additional training costs, ensuring both effectiveness and
efficiency. Extensive experiments across multiple benchmarks show that our
method significantly reduces hallucinations at the object, attribute, and
relation levels while largely preserving recall and caption richness, e.g.,
achieving a hallucination reduction by over 5% using LLaVA-v1.5-7B on CHAIR.
Furthermore, results on diverse architectures, including LLaVA-NEXT-7B,
Cambrian-8B, and InstructBLIP-7B, validate strong cross-architecture
generalization. More importantly, when applied to hallucination-free captions,
our method introduces almost no side effects, underscoring its robustness and
practical plug-and-play applicability. The implementation will be publicly
available.

</details>


### [98] [CoFFT: Chain of Foresight-Focus Thought for Visual Language Models](https://arxiv.org/abs/2509.22010)
*Xinyu Zhang,Yuxuan Dong,Lingling Zhang,Chengyou Jia,Zhuohang Dang,Basura Fernando,Jun Liu,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 该论文提出了一种无需训练的新方法CoFFT（Chain of Foresight-Focus Thought），通过模拟人类视觉认知，提升视觉语言模型（VLMs）在复杂视觉输入下的推理能力，并在多项基准测试中实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在面对包含大量与任务无关信息的图片时容易受到干扰，产生过多无关推理甚至幻觉，其根本原因是模型无法精准识别并处理推理所需的视觉区域。

Method: CoFFT方法包含三个迭代阶段：1）生成多样化推理样本，探索不同的推理路径；2）通过视觉聚焦和推理进度双重评估这些样本，将最优样本的首步加入正式推理流程；3）调整视觉聚焦，更准确地针对对后续推理最有益的视觉区域，然后回到第一步，循环迭代直至得出最终答案。

Result: 在Qwen2.5-VL、InternVL-2.5和Llava-Next等多个基准上的实验证明，CoFFT方法可带来3.1-5.8%的性能提升，同时增加的计算开销可控。

Conclusion: CoFFT作为一种训练自由、类人认知的流程，有效提升了VLMs的视觉推理能力，为处理包含冗余和复杂信息的视觉任务提供了新思路。

Abstract: Despite significant advances in Vision Language Models (VLMs), they remain
constrained by the complexity and redundancy of visual input. When images
contain large amounts of irrelevant information, VLMs are susceptible to
interference, thus generating excessive task-irrelevant reasoning processes or
even hallucinations. This limitation stems from their inability to discover and
process the required regions during reasoning precisely. To address this
limitation, we present the Chain of Foresight-Focus Thought (CoFFT), a novel
training-free approach that enhances VLMs' visual reasoning by emulating human
visual cognition. Each Foresight-Focus Thought consists of three stages: (1)
Diverse Sample Generation: generates diverse reasoning samples to explore
potential reasoning paths, where each sample contains several reasoning steps;
(2) Dual Foresight Decoding: rigorously evaluates these samples based on both
visual focus and reasoning progression, adding the first step of optimal sample
to the reasoning process; (3) Visual Focus Adjustment: precisely adjust visual
focus toward regions most beneficial for future reasoning, before returning to
stage (1) to generate subsequent reasoning samples until reaching the final
answer. These stages function iteratively, creating an interdependent cycle
where reasoning guides visual focus and visual focus informs subsequent
reasoning. Empirical results across multiple benchmarks using Qwen2.5-VL,
InternVL-2.5, and Llava-Next demonstrate consistent performance improvements of
3.1-5.8\% with controllable increasing computational overhead.

</details>


### [99] [EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking](https://arxiv.org/abs/2509.22019)
*Yuki Sakai,Ryosuke Furuta,Juichun Yen,Yoichi Sato*

Main category: cs.CV

TL;DR: 本论文提出了首个用于面对面教学场景分析的头戴式视频数据集，涵盖代码化的步骤分割和交流状态分类两项基础任务，并通过实验对多模态大模型进行效果评估。


<details>
  <summary>Details</summary>
Motivation: 面对面教学交互分析对于教育和技能传递至关重要，但由于缺乏数据集和分析方法，这一场景在计算机视觉中鲜有涉及。论文旨在弥补该领域的研究空白。

Method: 作者构建了一个包含现实世界师生互动的头戴式（egocentric）视频数据集，并为步骤分割与对话状态分类两项任务标注了真值。随后，作者用多模态大模型（MLLMs）和传统专用模型在该数据集上进行评估，考查其对图像、音频和文本的联合理解能力。

Result: 实验表明，近期提出的MLLMs在无需特定任务微调的情况下，整体表现优于传统的专用模型。评测结果显示这些模型能较好地理解面对面教学场景中的多模态信息。

Conclusion: MLLMs在面对面教学场景中的表现呈现出对整体交互理解的潜力，该数据集和任务可促进教育交互场景的多模态分析研究。

Abstract: Analyzing instructional interactions between an instructor and a learner who
are co-present in the same physical space is a critical problem for educational
support and skill transfer. Yet such face-to-face instructional scenes have not
been systematically studied in computer vision. We identify two key reasons: i)
the lack of suitable datasets and ii) limited analytical techniques. To address
this gap, we present a new egocentric video dataset of face-to-face instruction
and provide ground-truth annotations for two fundamental tasks that serve as a
first step toward a comprehensive understanding of instructional interactions:
procedural step segmentation and conversation-state classification. Using this
dataset, we benchmark multimodal large language models (MLLMs) against
conventional task-specific models. Since face-to-face instruction involves
multiple modalities (speech content and prosody, gaze and body motion, and
visual context), effective understanding requires methods that handle verbal
and nonverbal communication in an integrated manner. Accordingly, we evaluate
recently introduced MLLMs that jointly process images, audio, and text. This
evaluation quantifies the extent to which current machine learning models
understand face-to-face instructional scenes. In experiments, MLLMs outperform
specialized baselines even without task-specific fine-tuning, suggesting their
promise for holistic understanding of instructional interactions.

</details>


### [100] [High-Quality Sound Separation Across Diverse Categories via Visually-Guided Generative Modeling](https://arxiv.org/abs/2509.22063)
*Chao Huang,Susan Liang,Yapeng Tian,Anurag Kumar,Chenliang Xu*

Main category: cs.CV

TL;DR: 本文提出DAVIS，一种基于扩散模型的音频-视觉分离框架，通过生成式学习解决音频-视觉声音源分离任务。实验结果表明，该方法在主流数据集上显著优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有音频-视觉分离方法多将分离任务建模为mask回归问题，但难以应对复杂、多样的声音类别数据分布，影响分离质量。

Method: DAVIS采用强大的生成式建模方法，包括去噪扩散概率模型（DDPM）与Flow Matching，结合专用的分离式U-Net结构，利用混合音频和视觉信息作为条件，从噪声分布直接生成目标声音的频谱图。

Result: 在标准的AVE和MUSIC数据集上，DAVIS的DDPM和Flow Matching两种变体均在分离质量上超过了现有主流方法。

Conclusion: DAVIS作为一种生成式框架，有效提升了音频-视觉源分离的效果，特别适用于应对多样化声音类别的高质量分离。

Abstract: We propose DAVIS, a Diffusion-based Audio-VIsual Separation framework that
solves the audio-visual sound source separation task through generative
learning. Existing methods typically frame sound separation as a mask-based
regression problem, achieving significant progress. However, they face
limitations in capturing the complex data distribution required for
high-quality separation of sounds from diverse categories. In contrast, DAVIS
circumvents these issues by leveraging potent generative modeling paradigms,
specifically Denoising Diffusion Probabilistic Models (DDPM) and the more
recent Flow Matching (FM), integrated within a specialized Separation U-Net
architecture. Our framework operates by synthesizing the desired separated
sound spectrograms directly from a noise distribution, conditioned concurrently
on the mixed audio input and associated visual information. The inherent nature
of its generative objective makes DAVIS particularly adept at producing
high-quality sound separations for diverse sound categories. We present
comparative evaluations of DAVIS, encompassing both its DDPM and Flow Matching
variants, against leading methods on the standard AVE and MUSIC datasets. The
results affirm that both variants surpass existing approaches in separation
quality, highlighting the efficacy of our generative framework for tackling the
audio-visual source separation task.

</details>


### [101] [SpecXNet: A Dual-Domain Convolutional Network for Robust Deepfake Detection](https://arxiv.org/abs/2509.22070)
*Inzamamul Alam,Md Tanvir Islam,Simon S. Woo*

Main category: cs.CV

TL;DR: 本文提出了一种结合空间域和频谱域特征的新型深度伪造检测网络SpecXNet，显著提升了对尚未见过的伪造手法的检测准确率，在多个基准数据集上获得了最新最好成绩，并已开源。


<details>
  <summary>Details</summary>
Motivation: 随着GANs和扩散模型生成内容的真实度提升，深度伪造内容检测越来越困难。现有方法大多仅利用单一的空间或频率域特征，导致在遇到新型伪造手法时泛化能力不足。

Method: 提出了Spectral Cross-Attentional Network（SpecXNet），采用双域特征耦合机制（DDFC），分别从空间域检测纹理异常与频谱域建模周期性不一致，同时通过Dual Fourier Attention（DFA）模块动态融合两种特征。整个结构建立在改进的XceptionNet骨干网络上，并将新模块嵌入深度可分离卷积块。

Result: 在多个深度伪造检测基准数据集上，SpecXNet在跨数据集和未知伪造场景下实现了最新最优的检测准确率，同时具有实时检测能力。

Conclusion: 空间-频谱联合学习能够有效提升深度伪造检测的准确率及泛化能力，SpecXNet为鲁棒且通用的深度伪造检测提供了新方向。

Abstract: The increasing realism of content generated by GANs and diffusion models has
made deepfake detection significantly more challenging. Existing approaches
often focus solely on spatial or frequency-domain features, limiting their
generalization to unseen manipulations. We propose the Spectral
Cross-Attentional Network (SpecXNet), a dual-domain architecture for robust
deepfake detection. The core \textbf{Dual-Domain Feature Coupler (DDFC)}
decomposes features into a local spatial branch for capturing texture-level
anomalies and a global spectral branch that employs Fast Fourier Transform to
model periodic inconsistencies. This dual-domain formulation allows SpecXNet to
jointly exploit localized detail and global structural coherence, which are
critical for distinguishing authentic from manipulated images. We also
introduce the \textbf{Dual Fourier Attention (DFA)} module, which dynamically
fuses spatial and spectral features in a content-aware manner. Built atop a
modified XceptionNet backbone, we embed the DDFC and DFA modules within a
separable convolution block. Extensive experiments on multiple deepfake
benchmarks show that SpecXNet achieves state-of-the-art accuracy, particularly
under cross-dataset and unseen manipulation scenarios, while maintaining
real-time feasibility. Our results highlight the effectiveness of unified
spatial-spectral learning for robust and generalizable deepfake detection. To
ensure reproducibility, we released the full code on
\href{https://github.com/inzamamulDU/SpecXNet}{\textcolor{blue}{\textbf{GitHub}}}.

</details>


### [102] [Large Material Gaussian Model for Relightable 3D Generation](https://arxiv.org/abs/2509.22112)
*Jingrui Ye,Lingting Zhu,Runze Zhang,Zeyu Hu,Yingda Yin,Lanjiong Li,Lequan Yu,Qingmin Liao*

Main category: cs.CV

TL;DR: 本文提出了一种结合3D高斯斑点技术与多视角扩散模型的新框架，可实现高质量、具备PBR材质属性（如albedo、roughness和metallic）的3D资产生成，支持真实动态光照渲染。


<details>
  <summary>Details</summary>
Motivation: 目前3D资产需求增长，但现有自动化3D重建模型普遍无法输出包含物理渲染材质属性（如粗糙度、金属度等）的数据，仅能生成RGB贴图，难以满足现实渲染需求。

Method: 方法包括：(1) 基于深度和法线图调整多视角材质扩散模型，用于生成多视角PBR材质图像；(2) 构建能表达PBR不同通道特性的高斯材质表示，将其与2D高斯斑点技术对齐；(3) 利用重建点云进行渲染，可获得动态可重光照的PBR属性。

Result: 大量实验表明，该框架生成材质更具视觉吸引力，材质建模显著优于基线方法，在真实渲染和下游实际应用中表现更佳。

Conclusion: 该工作不仅实现了自动3D资产高质量及具物理属性材质的生成，还推动了3D内容创建向真实、可拓展和实际可用的新高度。

Abstract: The increasing demand for 3D assets across various industries necessitates
efficient and automated methods for 3D content creation. Leveraging 3D Gaussian
Splatting, recent large reconstruction models (LRMs) have demonstrated the
ability to efficiently achieve high-quality 3D rendering by integrating
multiview diffusion for generation and scalable transformers for
reconstruction. However, existing models fail to produce the material
properties of assets, which is crucial for realistic rendering in diverse
lighting environments. In this paper, we introduce the Large Material Gaussian
Model (MGM), a novel framework designed to generate high-quality 3D content
with Physically Based Rendering (PBR) materials, ie, albedo, roughness, and
metallic properties, rather than merely producing RGB textures with
uncontrolled light baking. Specifically, we first fine-tune a new multiview
material diffusion model conditioned on input depth and normal maps. Utilizing
the generated multiview PBR images, we explore a Gaussian material
representation that not only aligns with 2D Gaussian Splatting but also models
each channel of the PBR materials. The reconstructed point clouds can then be
rendered to acquire PBR attributes, enabling dynamic relighting by applying
various ambient light maps. Extensive experiments demonstrate that the
materials produced by our method not only exhibit greater visual appeal
compared to baseline methods but also enhance material modeling, thereby
enabling practical downstream rendering applications.

</details>


### [103] [Self-Supervised Point Cloud Completion based on Multi-View Augmentations of Single Partial Point Cloud](https://arxiv.org/abs/2509.22132)
*Jingjing Lu,Huilong Pi,Yunchuan Qin,Zhuo Tang,Ruihui Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的自监督点云补全方法，利用多视图增强与Mamba模型，显著提升补全效果，并在合成和真实数据集上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有点云补全方法在依赖监督数据、泛化能力弱、自监督信号有限等方面存在不足，难以适应实际场景。本研究旨在摆脱对配对真值或多视图样本的依赖，提升模型在真实世界中的通用性与性能。

Method: 提出了基于单个部分点云的多视图增强自监督信号，以多样化的视角增强训练，设计新颖自监督信号，并首次引入Mamba架构到自监督点云补全任务中，增强模型的学习及生成高质量点云的能力。

Result: 在合成数据和真实数据集上的实验表明，所提出方法在补全准确性与泛化性上均优于现有主流方法，取得了最新最优性能。

Conclusion: 通过创新的多视图自监督信号与Mamba模型结合，该方法有效克服了以往对真值和多视图的依赖，大幅提升了点云补全过程的自监督能力，展现出优越的补全效果和实际应用前景。

Abstract: Point cloud completion aims to reconstruct complete shapes from partial
observations. Although current methods have achieved remarkable performance,
they still have some limitations: Supervised methods heavily rely on ground
truth, which limits their generalization to real-world datasets due to the
synthetic-to-real domain gap. Unsupervised methods require complete point
clouds to compose unpaired training data, and weakly-supervised methods need
multi-view observations of the object. Existing self-supervised methods
frequently produce unsatisfactory predictions due to the limited capabilities
of their self-supervised signals. To overcome these challenges, we propose a
novel self-supervised point cloud completion method. We design a set of novel
self-supervised signals based on multi-view augmentations of the single partial
point cloud. Additionally, to enhance the model's learning ability, we first
incorporate Mamba into self-supervised point cloud completion task, encouraging
the model to generate point clouds with better quality. Experiments on
synthetic and real-world datasets demonstrate that our method achieves
state-of-the-art results.

</details>


### [104] [REFINE-CONTROL: A Semi-supervised Distillation Method For Conditional Image Generation](https://arxiv.org/abs/2509.22139)
*Yicheng Jiang,Jin Yuan,Hua Yuan,Yao Zhang,Yong Rui*

Main category: cs.CV

TL;DR: 该论文提出了一种面向条件图像生成的轻量级半监督知识蒸馏框架Refine-Control，大幅降低计算成本，同时保持高质量可控生成。


<details>
  <summary>Details</summary>
Motivation: 现有条件图像生成模型由于资源消耗大、标注数据稀缺且存在隐私问题，难以部署在边缘设备上。

Method: 提出了名为Refine-Control的半监督蒸馏方法，通过三层次知识融合损失提升学生模型表现，并利用有标签与无标签数据进行半监督训练，增强泛化能力。

Result: 实验结果表明，Refine-Control在大幅降低计算开销和延迟的前提下，仍能保持高保真度和生成可控性，量化指标优异。

Conclusion: Refine-Control为边缘设备部署条件图像生成模型提供了高效且隐私友好的解决方案，兼顾生成质量与计算资源。

Abstract: Conditional image generation models have achieved remarkable results by
leveraging text-based control to generate customized images. However, the high
resource demands of these models and the scarcity of well-annotated data have
hindered their deployment on edge devices, leading to enormous costs and
privacy concerns, especially when user data is sent to a third party. To
overcome these challenges, we propose Refine-Control, a semi-supervised
distillation framework. Specifically, we improve the performance of the student
model by introducing a tri-level knowledge fusion loss to transfer different
levels of knowledge. To enhance generalization and alleviate dataset scarcity,
we introduce a semi-supervised distillation method utilizing both labeled and
unlabeled data. Our experiments reveal that Refine-Control achieves significant
reductions in computational cost and latency, while maintaining high-fidelity
generation capabilities and controllability, as quantified by comparative
metrics.

</details>


### [105] [Joint graph entropy knowledge distillation for point cloud classification and robustness against corruptions](https://arxiv.org/abs/2509.22150)
*Zhiqiang Tian,Weigang Li,Junwei Hu,Chunhua Deng*

Main category: cs.CV

TL;DR: 本文提出了一种适用于非独立同分布3D点云数据的新型分类方法JGEKD，有效利用类间相关性，通过联合图熵知识蒸馏提升模型性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在3D点云分类任务中，通常假设类别事件独立同分布（IID），但这忽略了类别之间的相关性，降低了模型对真实场景复杂关系的建模能力，尤其在数据分布复杂或有扰动时容易退化。

Method: 作者提出JGEKD策略，核心是利用联合图捕捉类间隐含关系，通过基于联合图熵的损失函数进行知识蒸馏，实现类别相关性知识的迁移。方法上，结合联合图与图熵，设计了适用于空间变换不变性的孪生（Siamese）结构，并开发了自我知识蒸馏和教师知识蒸馏两种框架，还将上述机制扩展到点云及其受损形态之间，提高对扰动的鲁棒性。

Result: 在ScanObject、ModelNet40、ScanntV2_cls和ModelNet-C四个数据集上进行了大量实验，结果表明JGEKD方法在准确率和鲁棒性方面达到或超过了现有主流方法。

Conclusion: JGEKD通过融合图结构、信息熵及知识蒸馏，有效缓解了传统3D点云分类方法在类间相关性利用及抗扰动能力上的不足，具有良好的实用前景。

Abstract: Classification tasks in 3D point clouds often assume that class events
\replaced{are }{follow }independent and identically distributed (IID), although
this assumption destroys the correlation between classes. This \replaced{study
}{paper }proposes a classification strategy, \textbf{J}oint \textbf{G}raph
\textbf{E}ntropy \textbf{K}nowledge \textbf{D}istillation (JGEKD), suitable for
non-independent and identically distributed 3D point cloud data,
\replaced{which }{the strategy } achieves knowledge transfer of class
correlations through knowledge distillation by constructing a loss function
based on joint graph entropy. First\deleted{ly}, we employ joint graphs to
capture add{the }hidden relationships between classes\replaced{ and}{,}
implement knowledge distillation to train our model by calculating the entropy
of add{add }graph.\replaced{ Subsequently}{ Then}, to handle 3D point clouds
\deleted{that is }invariant to spatial transformations, we construct
\replaced{S}{s}iamese structures and develop two frameworks, self-knowledge
distillation and teacher-knowledge distillation, to facilitate information
transfer between different transformation forms of the same data. \replaced{In
addition}{ Additionally}, we use the above framework to achieve knowledge
transfer between point clouds and their corrupted forms, and increase the
robustness against corruption of model. Extensive experiments on ScanObject,
ModelNet40, ScanntV2\_cls and ModelNet-C demonstrate that the proposed strategy
can achieve competitive results.

</details>


### [106] [MultiMat: Multimodal Program Synthesis for Procedural Materials using Large Multimodal Models](https://arxiv.org/abs/2509.22151)
*Jonas Belouadi,Tamy Boubekeur,Adrien Kaiser*

Main category: cs.CV

TL;DR: 本论文提出了一种名为MultiMat的多模态程序生成框架，能够更高效、更直观地生成高质量的程序化材质节点图，突破了以往仅用文本表示的限制。


<details>
  <summary>Details</summary>
Motivation: 程序化材质节点图（Material node graphs）是3D物体外观建模的重要工具，但其创建过程复杂，对专业技能要求高。现有神经程序生成方法只用文本表示，无法捕捉节点图的视觉空间特性，影响可理解性与操作性。为此，作者提出结合视觉与文本信息的多模态方法，提升程序生成效果与效率。

Method: 作者提出了MultiMat框架，利用大规模多模态模型同时处理节点图的视觉和文本表示，并训练于新构建的高质量程序化材质数据集。推断时结合受限树搜索算法，保证生成结果的语法有效性和高效搜索。

Result: 实验证明，该多模态方法在无条件和有条件的材质图生成任务中，生成速度更快、视觉质量及保真度更高，优于仅用文本的基线方法，并刷新了相关领域的性能纪录。

Conclusion: MultiMat利用多模态信息突破了材质节点图传统创建方式，简化了流程、提升了效果，为交互式外观建模工作提供了更强有力的技术支持。

Abstract: Material node graphs are programs that generate the 2D channels of procedural
materials, including geometry such as roughness and displacement maps, and
reflectance such as albedo and conductivity maps. They are essential in
computer graphics for representing the appearance of virtual 3D objects
parametrically and at arbitrary resolution. In particular, their directed
acyclic graph structures and intermediate states provide an intuitive
understanding and workflow for interactive appearance modeling. Creating such
graphs is a challenging task and typically requires professional training.
While recent neural program synthesis approaches attempt to simplify this
process, they solely represent graphs as textual programs, failing to capture
the inherently visual-spatial nature of node graphs that makes them accessible
to humans. To address this gap, we present MultiMat, a multimodal program
synthesis framework that leverages large multimodal models to process both
visual and textual graph representations for improved generation of procedural
material graphs. We train our models on a new dataset of production-quality
procedural materials and combine them with a constrained tree search inference
algorithm that ensures syntactic validity while efficiently navigating the
program space. Our experimental results show that our multimodal program
synthesis method is more efficient in both unconditional and conditional graph
synthesis with higher visual quality and fidelity than text-only baselines,
establishing new state-of-the-art performance.

</details>


### [107] [DragGANSpace: Latent Space Exploration and Control for GANs](https://arxiv.org/abs/2509.22169)
*Kirsten Odendaal,Neela Kaushik,Spencer Halverson*

Main category: cs.CV

TL;DR: 本论文结合了StyleGAN、DragGAN和主成分分析（PCA），提升了GAN生成图像的潜空间效率和可控性，在动物面部（AFHQ）数据集上验证了方法的有效性，能够提高优化效率并保持甚至提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有GAN图像生成与编辑方法在潜空间探索和高效优化方面存在挑战，尤其是优化效率与可解释性难以兼得。为了解决这一问题，作者尝试融合多种技术并利用PCA降维，提升潜空间的效率与可控性。

Method: 作者将StyleGAN提供的结构化潜空间、DragGAN的直观图像编辑能力与PCA的降维和跨模型对齐优势结合，提出了一种面向潜空间高效编辑和探索的新框架，并在AFHQ数据集上进行了应用和验证。

Result: 实验结果显示，将PCA引入DragGAN的W+空间中，可一致性降低总优化时间，同时在较浅层潜空间下还能提升结构相似性指标（SSIM）；此外，方法还可实现两个风格GAN模型（如狗与猫）之间生成图像的有效对齐及可控编辑。

Conclusion: 作者方法实现了高效且可解释的GAN潜空间编辑与对齐，提升了图像合成与编辑的效率和质量，为相关应用提供了更优的解决方案。

Abstract: This work integrates StyleGAN, DragGAN and Principal Component Analysis (PCA)
to enhance the latent space efficiency and controllability of GAN-generated
images. Style-GAN provides a structured latent space, DragGAN enables intuitive
image manipulation, and PCA reduces dimensionality and facilitates cross-model
alignment for more streamlined and interpretable exploration of latent spaces.
We apply our techniques to the Animal Faces High Quality (AFHQ) dataset, and
find that our approach of integrating PCA-based dimensionality reduction with
the Drag-GAN framework for image manipulation retains performance while
improving optimization efficiency. Notably, introducing PCA into the latent W+
layers of DragGAN can consistently reduce the total optimization time while
maintaining good visual quality and even boosting the Structural Similarity
Index Measure (SSIM) of the optimized image, particularly in shallower latent
spaces (W+ layers = 3). We also demonstrate capability for aligning images
generated by two StyleGAN models trained on similar but distinct data domains
(AFHQ-Dog and AFHQ-Cat), and show that we can control the latent space of these
aligned images to manipulate the images in an intuitive and interpretable
manner. Our findings highlight the possibility for efficient and interpretable
latent space control for a wide range of image synthesis and editing
applications.

</details>


### [108] [MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing](https://arxiv.org/abs/2509.22186)
*Junbo Niu,Zheng Liu,Zhuangcheng Gu,Bin Wang,Linke Ouyang,Zhiyuan Zhao,Tao Chu,Tianyao He,Fan Wu,Qintong Zhang,Zhenjiang Jin,Guang Liang,Rui Zhang,Wenzheng Zhang,Yuan Qu,Zhifei Ren,Yuefeng Sun,Yuanhong Zheng,Dongsheng Ma,Zirui Tang,Boyu Niu,Ziyang Miao,Hejun Dong,Siyi Qian,Junyuan Zhang,Jingzhou Chen,Fangdong Wang,Xiaomeng Zhao,Liqun Wei,Wei Li,Shasha Wang,Ruiliang Xu,Yuanyuan Cao,Lu Chen,Qianqian Wu,Huaiyu Gu,Lindong Lu,Keming Wang,Dechen Lin,Guanlin Shen,Xuanhe Zhou,Linfeng Zhang,Yuhang Zang,Xiaoyi Dong,Jiaqi Wang,Bo Zhang,Lei Bai,Pei Chu,Weijia Li,Jiang Wu,Lijun Wu,Zhenxiang Li,Guangyu Wang,Zhongying Tu,Chao Xu,Kai Chen,Yu Qiao,Bowen Zhou,Dahua Lin,Wentao Zhang,Conghui He*

Main category: cs.CV

TL;DR: 本文提出了MinerU2.5，一种1.2B参数的文档解析视觉-语言模型，兼具高识别精度与出色的计算效率。该方法采用两阶段解析策略，结合新的数据引擎，最终在多项基准任务上取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 当前文档解析模型在识别精度和计算效率间存在权衡，如何在保证高精度的同时显著降低计算资源消耗，是该领域亟需解决的问题。

Method: 模型采用“粗到细”的两阶段解析策略，先在低分辨率图像上进行全局结构分析，锁定文档布局，再基于此指导，对原始分辨率的局部区域进行内容识别，以保证细节信息的获取。同时，作者开发了全面的数据生成引擎，为预训练和微调提供大规模高质量训练语料。

Result: MinerU2.5模型在多个文档解析基准上表现优异，超越了通用和特定领域的主流模型，识别准确率达到最新水平，且计算开销显著低于其它竞品。

Conclusion: MinerU2.5证明了两阶段精细解析以及高质量数据支撑可实现文档解析领域性能与效率的兼得，为后续相关系统研发提供了新思路。

Abstract: We introduce MinerU2.5, a 1.2B-parameter document parsing vision-language
model that achieves state-of-the-art recognition accuracy while maintaining
exceptional computational efficiency. Our approach employs a coarse-to-fine,
two-stage parsing strategy that decouples global layout analysis from local
content recognition. In the first stage, the model performs efficient layout
analysis on downsampled images to identify structural elements, circumventing
the computational overhead of processing high-resolution inputs. In the second
stage, guided by the global layout, it performs targeted content recognition on
native-resolution crops extracted from the original image, preserving
fine-grained details in dense text, complex formulas, and tables. To support
this strategy, we developed a comprehensive data engine that generates diverse,
large-scale training corpora for both pretraining and fine-tuning. Ultimately,
MinerU2.5 demonstrates strong document parsing ability, achieving
state-of-the-art performance on multiple benchmarks, surpassing both
general-purpose and domain-specific models across various recognition tasks,
while maintaining significantly lower computational overhead.

</details>


### [109] [Towards Faithful Reasoning in Remote Sensing: A Perceptually-Grounded GeoSpatial Chain-of-Thought for Vision-Language Models](https://arxiv.org/abs/2509.22221)
*Jiaqi Liu,Lang Sun,Ronghao Fu,Bo Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Geo-CoT的遥感视觉-语言模型推理框架，通过结构化推理链和可验证性分析，显著提升了模型在复杂遥感分析任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 以往遥感VLMs由于端到端训练方式，缺乏中间推理步骤，输出难以验证，导致在复杂分析任务上表现有限。本研究旨在解决上述可解释性与可验证性的问题。

Method: 作者提出了Geo-CoT框架，将遥感分析建模为可验证的多步推理过程。方法采用两阶段校准策略：首先通过有监督微调（SFT）建立基础认知架构，再通过Group Reward Policy Optimization（GRPO）优化模型推理政策，提升事实准确性，同时构建了大规模、结构化的Geo-CoT380k推理链数据集。

Result: 提出的RSThinker模型不仅能输出最终答案，还能输出结构化、可验证的推理过程。在各类遥感分析任务上，性能远超现有最佳模型。

Conclusion: Geo-CoT框架及其支撑的数据集与模型为遥感AI领域从“黑盒”感知走向结构化、可验证推理提供了切实可行的路径，并将在论文发布后开源，为地球观测分析带来重大进步。

Abstract: Vision-Language Models (VLMs) in remote sensing often fail at complex
analytical tasks, a limitation stemming from their end-to-end training paradigm
that bypasses crucial reasoning steps and leads to unverifiable outputs. To
address this limitation, we introduce the Perceptually-Grounded Geospatial
Chain-of-Thought (Geo-CoT), a framework that models remote sensing analysis as
a verifiable, multi-step process. We instill this analytical process through a
two-stage alignment strategy, leveraging Geo-CoT380k, the first large-scale
dataset of structured Geo-CoT rationales. This strategy first employs
supervised fine-tuning (SFT) to instill the foundational cognitive
architecture, then leverages Group Reward Policy Optimization (GRPO) to refine
the model's reasoning policy towards factual correctness. The resulting model,
RSThinker, outputs both a final answer and its justifying, verifiable
analytical trace. This capability yields dominant performance, significantly
outperforming state-of-the-art models across a comprehensive range of tasks.
The public release of our Geo-CoT380k dataset and RSThinker model upon
publication serves as a concrete pathway from opaque perception towards
structured, verifiable reasoning for Earth Observation.

</details>


### [110] [Polysemous Language Gaussian Splatting via Matching-based Mask Lifting](https://arxiv.org/abs/2509.22225)
*Jiayu Ding,Xinpeng Liu,Zhiyi Pan,Shiqiang Long,Ge Li*

Main category: cs.CV

TL;DR: 该论文提出了一种无需训练即可实现3D高斯散射场景中的开放词汇（Open-vocabulary）理解的方法MUSplat，并成功解决了以往方法需针对每个场景繁琐训练、表达语义单一和表现出多视角下语义不一致等关键问题。


<details>
  <summary>Details</summary>
Motivation: 主流方法在将二维开放词汇理解提升到三维场景中时，存在训练耗时高、不能表达复杂语义、多视角下语义一致性差等问题。研究动机是实现一个高效、可即插即用、能处理多语义（多义性）且语义一致性强的三维场景开放词汇理解系统。

Method: 本方法MUSplat完全摒弃了传统的特征训练，通过预训练的二维分割模型生成不同层次的二维掩码并升维到三维空间，计算每个高斯点的前景概率以组成初始对象组。再通过语义熵和几何不透明度优化初步的组间边界，最后利用视觉语言模型（VLM）在最具代表性的视角下提取鲁棒的文本特征，实现语义一致性和开放词汇查询。全流程无需对新场景再训练。

Result: 在开放词汇3D对象选择与语义分割基准任务上，MUSplat在准确率和效率上超越了当前主流基于训练的方法；且实现了表达多语义和语义一致性，适应场景的时间从小时级降至分钟级。

Conclusion: MUSplat为3D高斯散射场景下的开放词汇语义理解提供了高效、训练自由且表现优异的解决方案，克服了现有方法的核心局限，有望在实际三维视觉与推理任务中广泛应用。

Abstract: Lifting 2D open-vocabulary understanding into 3D Gaussian Splatting (3DGS)
scenes is a critical challenge. However, mainstream methods suffer from three
key flaws: (i) their reliance on costly per-scene retraining prevents
plug-and-play application; (ii) their restrictive monosemous design fails to
represent complex, multi-concept semantics; and (iii) their vulnerability to
cross-view semantic inconsistencies corrupts the final semantic representation.
To overcome these limitations, we introduce MUSplat, a training-free framework
that abandons feature optimization entirely. Leveraging a pre-trained 2D
segmentation model, our pipeline generates and lifts multi-granularity 2D masks
into 3D, where we estimate a foreground probability for each Gaussian point to
form initial object groups. We then optimize the ambiguous boundaries of these
initial groups using semantic entropy and geometric opacity. Subsequently, by
interpreting the object's appearance across its most representative viewpoints,
a Vision-Language Model (VLM) distills robust textual features that reconciles
visual inconsistencies, enabling open-vocabulary querying via semantic
matching. By eliminating the costly per-scene training process, MUSplat reduces
scene adaptation time from hours to mere minutes. On benchmark tasks for
open-vocabulary 3D object selection and semantic segmentation, MUSplat
outperforms established training-based frameworks while simultaneously
addressing their monosemous limitations.

</details>


### [111] [UrbanFeel: A Comprehensive Benchmark for Temporal and Perceptual Understanding of City Scenes through Human Perspective](https://arxiv.org/abs/2509.22228)
*Jun He,Yi Lin,Zilong Huang,Jiacong Yin,Junyan Ye,Yuchuan Zhou,Weijia Li,Xiang Zhang*

Main category: cs.CV

TL;DR: 本论文提出了UrbanFeel基准，系统评估多模态大模型（MLLMs）在理解城市发展与主观环境感受方面的能力。通过对14.3K张多时相街景图片和问题对，分析了多模型在静态感知、时序变化和主观评价三方面的表现。Gemini-2.5 Pro总体表现最好，接近人类水平，但模型在时序推理上明显不足。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型已展现强大能力，但针对城市环境特别是涉及结构变化与主观感知的系统评测仍然缺乏，无法真实反映模型在人类相关城市感知任务上的能力弱点。

Method: 作者提出UrbanFeel基准集，包含全球11大城市的多时相街景（单视角及全景），并用聚类、规则生成、模型辅助及人工标注等混合流程，构建三类问题（静态感知、时序变化、主观感受），生成14.3K高质问答对。

Result: 评测了20个主流MLLMs，Gemini-2.5 Pro表现最佳，准确率几乎与专家持平，仅差1.5%；场景感知任务模型普遍优异，像素级变化检测部分模型甚至超越人类。但涉及城市发展时序推理时模型性能显著下降；对主观感知（如美观、安全）部分模型可与人类评价一致。

Conclusion: UrbanFeel基准推动MLLMs在城市场景理解与主观环境感知方向迈进一大步。虽然顶尖模型已接近人类在部分任务的水平，但在时序推理等领域仍存在明显短板，为未来模型改进指明方向。

Abstract: Urban development impacts over half of the global population, making
human-centered understanding of its structural and perceptual changes essential
for sustainable development. While Multimodal Large Language Models (MLLMs)
have shown remarkable capabilities across various domains, existing benchmarks
that explore their performance in urban environments remain limited, lacking
systematic exploration of temporal evolution and subjective perception of urban
environment that aligns with human perception. To address these limitations, we
propose UrbanFeel, a comprehensive benchmark designed to evaluate the
performance of MLLMs in urban development understanding and subjective
environmental perception. UrbanFeel comprises 14.3K carefully constructed
visual questions spanning three cognitively progressive dimensions: Static
Scene Perception, Temporal Change Understanding, and Subjective Environmental
Perception. We collect multi-temporal single-view and panoramic street-view
images from 11 representative cities worldwide, and generate high-quality
question-answer pairs through a hybrid pipeline of spatial clustering,
rule-based generation, model-assisted prompting, and manual annotation. Through
extensive evaluation of 20 state-of-the-art MLLMs, we observe that Gemini-2.5
Pro achieves the best overall performance, with its accuracy approaching human
expert levels and narrowing the average gap to just 1.5\%. Most models perform
well on tasks grounded in scene understanding. In particular, some models even
surpass human annotators in pixel-level change detection. However, performance
drops notably in tasks requiring temporal reasoning over urban development.
Additionally, in the subjective perception dimension, several models reach
human-level or even higher consistency in evaluating dimension such as
beautiful and safety.

</details>


### [112] [A Tale of Two Experts: Cooperative Learning for Source-Free Unsupervised Domain Adaptation](https://arxiv.org/abs/2509.22229)
*Jiaping Yu,Muli Yang,Jiapeng Ji,Jiexi Yan,Cheng Deng*

Main category: cs.CV

TL;DR: 本文提出了EXCL方法，通过让冻结的源模型和多模态视觉-语言模型共同挖掘目标域未标注数据的知识，用于源数据不可用时的无监督领域自适应，并能取得先进性能。


<details>
  <summary>Details</summary>
Motivation: SFUDA解决了因隐私和成本原因，源数据不可用时模型适配的问题。现有方法要么仅用源模型预测，要么微调大模型，忽视了互补知识和目标域潜在结构，因此亟需更有效的融合与挖掘方案。

Method: 提出了EXCL方法：包括Dual Experts框架（冻结带适配器的源域模型+可调文本提示的预训练视觉-语言模型），共同处理目标未标注样本。优化上提出RAIN三阶段流程，分别为检索伪源/复杂目标样本、各专家分别微调、通过共享学习结果强化一致性。

Result: 在四个领域自适应基准数据集上，EXCL方法达到了与当前最优方法相当的性能，验证了其有效性。

Conclusion: EXCL通过专家协同促进知识挖掘，为无需源数据的领域自适应开辟了新方向，兼顾了性能与隐私安全，具有实际价值。

Abstract: Source-Free Unsupervised Domain Adaptation (SFUDA) addresses the realistic
challenge of adapting a source-trained model to a target domain without access
to the source data, driven by concerns over privacy and cost. Existing SFUDA
methods either exploit only the source model's predictions or fine-tune large
multimodal models, yet both neglect complementary insights and the latent
structure of target data. In this paper, we propose the Experts Cooperative
Learning (EXCL). EXCL contains the Dual Experts framework and
Retrieval-Augmentation-Interaction optimization pipeline. The Dual Experts
framework places a frozen source-domain model (augmented with Conv-Adapter) and
a pretrained vision-language model (with a trainable text prompt) on equal
footing to mine consensus knowledge from unlabeled target samples. To
effectively train these plug-in modules under purely unsupervised conditions,
we introduce Retrieval-Augmented-Interaction(RAIN), a three-stage pipeline that
(1) collaboratively retrieves pseudo-source and complex target samples, (2)
separately fine-tunes each expert on its respective sample set, and (3)
enforces learning object consistency via a shared learning result. Extensive
experiments on four benchmark datasets demonstrate that our approach matches
state-of-the-art performance.

</details>


### [113] [FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image Editing](https://arxiv.org/abs/2509.22244)
*Junyi Wu,Zhiteng Li,Haotong Qin,Xiaohong Liu,Linghe Kong,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: FlashEdit是一种用于文本引导图像编辑的高效扩散模型框架，实现了高质量、实时编辑，并大幅提高了处理速度。


<details>
  <summary>Details</summary>
Motivation: 以往基于扩散模型的文本引导图像编辑质量虽高，但由于处理延迟大，难以实际应用。作者希望提升编辑速度，同时保证编辑质量和图像结构完整性。

Method: 提出了三个关键创新：1）一键反演与编辑(OSIE)流程，省略了反复的迭代过程；2）背景保护(BG-Shield)技术，仅在编辑区域内改变特征，确保背景不被修改；3）稀疏空间交叉注意(SSCA)机制，通过抑制语义泄漏，实现精确、本地化编辑。

Result: 大量实验结果表明，FlashEdit在保证背景一致性和图像结构完整性的前提下，将编辑用时缩短至0.2秒以内，速度比其它多步方法提升超过150倍。

Conclusion: FlashEdit实现了低延迟、高保真度的文本引导图像编辑，可以更好地应用于实际场景。相关代码已开放。

Abstract: Text-guided image editing with diffusion models has achieved remarkable
quality but suffers from prohibitive latency, hindering real-world
applications. We introduce FlashEdit, a novel framework designed to enable
high-fidelity, real-time image editing. Its efficiency stems from three key
innovations: (1) a One-Step Inversion-and-Editing (OSIE) pipeline that bypasses
costly iterative processes; (2) a Background Shield (BG-Shield) technique that
guarantees background preservation by selectively modifying features only
within the edit region; and (3) a Sparsified Spatial Cross-Attention (SSCA)
mechanism that ensures precise, localized edits by suppressing semantic leakage
to the background. Extensive experiments demonstrate that FlashEdit maintains
superior background consistency and structural integrity, while performing
edits in under 0.2 seconds, which is an over 150$\times$ speedup compared to
prior multi-step methods. Our code will be made publicly available at
https://github.com/JunyiWuCode/FlashEdit.

</details>


### [114] [Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper Reasoning Benchmarks](https://arxiv.org/abs/2509.22258)
*Miao Jing,Mengting Jia,Junling Lin,Zhongxia Shen,Lijun Wang,Yuanyuan Peng,Huan Gao,Mingkun Xu,Shangyang Li*

Main category: cs.CV

TL;DR: 本文提出了一个专为神经科多模态临床推理设计的新基准Neural-MedBench，并通过该基准测试发现主流视觉-语言模型（VLM）在真实推理能力上远不如传统数据集表现，需要更严谨的评测方式。


<details>
  <summary>Details</summary>
Motivation: 传统医学VLM评测数据集主要聚焦分类准确率，容易导致模型在高风险推理任务上表现出“虚假精通”。作者希望提出一种能深入考查模型多模态临床推理能力的评测方法，填补现有基准在推理维度上的不足。

Method: 设计了Neural-MedBench基准，集成了多序列MRI影像、结构化电子健康记录及临床笔记，并包含鉴别诊断、病灶识别、推理解释三大任务类型。评测结合大语言模型评分、临床医生验证和语义相似性指标，构建混合评测流程，全面评估SOTA模型（GPT-4o、Claude-4、MedGemma）。

Result: 在Neural-MedBench上，主流医学VLM相比传统基准数据集表现显著下降。误差分析发现，模型主要短板来源于推理失败而非感知错误。

Conclusion: 仅靠大规模统计型数据集无法检验模型推理可信度，需“两轴评测框架”：即广度导向的大型数据集和深度导向的小而精推理型基准。Neural-MedBench为开放、可扩展的测试平台，有助于推动医学可信AI的评测方式发展。

Abstract: Recent advances in vision-language models (VLMs) have achieved remarkable
performance on standard medical benchmarks, yet their true clinical reasoning
ability remains unclear. Existing datasets predominantly emphasize
classification accuracy, creating an evaluation illusion in which models appear
proficient while still failing at high-stakes diagnostic reasoning. We
introduce Neural-MedBench, a compact yet reasoning-intensive benchmark
specifically designed to probe the limits of multimodal clinical reasoning in
neurology. Neural-MedBench integrates multi-sequence MRI scans, structured
electronic health records, and clinical notes, and encompasses three core task
families: differential diagnosis, lesion recognition, and rationale generation.
To ensure reliable evaluation, we develop a hybrid scoring pipeline that
combines LLM-based graders, clinician validation, and semantic similarity
metrics. Through systematic evaluation of state-of-the-art VLMs, including
GPT-4o, Claude-4, and MedGemma, we observe a sharp performance drop compared to
conventional datasets. Error analysis shows that reasoning failures, rather
than perceptual errors, dominate model shortcomings. Our findings highlight the
necessity of a Two-Axis Evaluation Framework: breadth-oriented large datasets
for statistical generalization, and depth-oriented, compact benchmarks such as
Neural-MedBench for reasoning fidelity. We release Neural-MedBench at
https://neuromedbench.github.io/ as an open and extensible diagnostic testbed,
which guides the expansion of future benchmarks and enables rigorous yet
cost-effective assessment of clinically trustworthy AI.

</details>


### [115] [UniMapGen: A Generative Framework for Large-Scale Map Construction from Multi-modal Data](https://arxiv.org/abs/2509.22262)
*Yujian Yuan,Changjie Wu,Xinyuan Chang,Sijin Wang,Hang Zhang,Shiyi Liang,Shuang Zeng,Mu Xu*

Main category: cs.CV

TL;DR: 本文提出了一种名为UniMapGen的新型生成式框架，用于大规模地图构建，不仅能产生更连续和光滑的道路向量，还支持多模态输入，并保障地图的全局连续性和一致性。实验结果在OpenSatMap数据集上优于现有方法，并能推断遮挡或缺失的道路。


<details>
  <summary>Details</summary>
Motivation: 大规模地图对于自动驾驶和导航等关键应用至关重要。传统方法依赖昂贵的专业数据采集车辆和繁琐的标注，效率低下。基于卫星的方案虽提升了效率和覆盖率，但受制于数据本身的局限（如遮挡、过时）及向量化质量低下，需要大量后处理。因此，亟需一种高效、准确、自动化的大规模地图构建新方法。

Method: 提出了UniMapGen框架，包含三大创新：1）将车道线以离散序列形式建模，通过迭代生成策略，使输出道路更加平滑完整；2）搭建支持多模态输入的灵活架构，能动态选择鸟瞰图（BEV）、正视图（PV）及文本提示，有效减缓卫星数据不足带来的影响；3）提出状态更新策略，确保大规模地图的全局连续性和一致性。

Result: UniMapGen在OpenSatMap数据集上的性能达到了目前最优。此外，该框架还能推断出被遮挡及数据集标注中缺失的道路。

Conclusion: UniMapGen为大规模地图构建提供了高效、准确且自动化的新方案，克服了卫星数据的局限与传统向量化道路不连续的问题。其优异的实验结果显示出该方法在实际应用中的巨大潜力。

Abstract: Large-scale map construction is foundational for critical applications such
as autonomous driving and navigation systems. Traditional large-scale map
construction approaches mainly rely on costly and inefficient special data
collection vehicles and labor-intensive annotation processes. While existing
satellite-based methods have demonstrated promising potential in enhancing the
efficiency and coverage of map construction, they exhibit two major
limitations: (1) inherent drawbacks of satellite data (e.g., occlusions,
outdatedness) and (2) inefficient vectorization from perception-based methods,
resulting in discontinuous and rough roads that require extensive
post-processing. This paper presents a novel generative framework, UniMapGen,
for large-scale map construction, offering three key innovations: (1)
representing lane lines as \textbf{discrete sequence} and establishing an
iterative strategy to generate more complete and smooth map vectors than
traditional perception-based methods. (2) proposing a flexible architecture
that supports \textbf{multi-modal} inputs, enabling dynamic selection among
BEV, PV, and text prompt, to overcome the drawbacks of satellite data. (3)
developing a \textbf{state update} strategy for global continuity and
consistency of the constructed large-scale map. UniMapGen achieves
state-of-the-art performance on the OpenSatMap dataset. Furthermore, UniMapGen
can infer occluded roads and predict roads missing from dataset annotations.
Our code will be released.

</details>


### [116] [GS-2M: Gaussian Splatting for Joint Mesh Reconstruction and Material Decomposition](https://arxiv.org/abs/2509.22276)
*Dinh Minh Nguyen,Malte Avenhaus,Thomas Lindemeier*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯泼溅(Gaussian Splatting)的统一方法(GS-2M)，可同时实现多视角图像下的网格重建和材质分解，特别提升了对高反射表面的重建效果。


<details>
  <summary>Details</summary>
Motivation: 以往方法在网格重建和材质分解任务上通常各自为政，且难以应对高反射表面，往往需借助外部先验模型来改善结果。因此，作者希望提出一种无需依赖复杂神经网络组件，并能提升反射表面表现的联合优化方案。

Method: 方法以3D高斯泼溅为基础，通过联合优化有助于深度和法线渲染质量的属性，同时引入基于多视角光度变化的粗糙度监督，新设计了损失函数和整体优化流程，避免过度依赖深度神经网络。

Result: GS-2M生成的网格和材质分解结果，在广泛使用的数据集和与主流重建方法的比较中，展现出与现有最先进方法相当的效果，并能输出用于后续任务的高质量三角网格及材质参数。

Conclusion: GS-2M能够以更简洁、有效的方式实现高质量的网格重建和材质分解，对反射表面有优势，且具有良好的扩展性及实用价值。

Abstract: We propose a unified solution for mesh reconstruction and material
decomposition from multi-view images based on 3D Gaussian Splatting, referred
to as GS-2M. Previous works handle these tasks separately and struggle to
reconstruct highly reflective surfaces, often relying on priors from external
models to enhance the decomposition results. Conversely, our method addresses
these two problems by jointly optimizing attributes relevant to the quality of
rendered depth and normals, maintaining geometric details while being resilient
to reflective surfaces. Although contemporary works effectively solve these
tasks together, they often employ sophisticated neural components to learn
scene properties, which hinders their performance at scale. To further
eliminate these neural components, we propose a novel roughness supervision
strategy based on multi-view photometric variation. When combined with a
carefully designed loss and optimization process, our unified framework
produces reconstruction results comparable to state-of-the-art methods,
delivering triangle meshes and their associated material components for
downstream tasks. We validate the effectiveness of our approach with widely
used datasets from previous works and qualitative comparisons with
state-of-the-art surface reconstruction methods.

</details>


### [117] [Rule-Based Reinforcement Learning for Document Image Classification with Vision Language Models](https://arxiv.org/abs/2509.22283)
*Michael Jungo,Andreas Fischer*

Main category: cs.CV

TL;DR: 本文探讨了基于规则的强化学习在文档图像分类任务中的应用，发现其在面对分布外数据表现出更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在某些领域表现突出，但在文档分析领域的应用尚不普遍。本研究旨在探索强化学习在文档图像分类等下游任务中的优势，尤其是其推理和泛化能力。

Method: 作者采用基于规则的强化学习方法，对文档图像分类任务进行实验，并比较其在三种分布外场景（分布外图像、未见类别、不同模态）下的表现。

Result: 结果表明，基于规则的强化学习方法在面对分布外数据时，比传统方法具有更好的泛化能力。

Conclusion: 研究证明了在文档图像分类任务中应用基于规则的强化学习是有效的，尤其在处理分布外数据时具有优势。

Abstract: Rule-based reinforcement learning has been gaining popularity ever since
DeepSeek-R1 has demonstrated its success through simple verifiable rewards. In
the domain of document analysis, reinforcement learning is not as prevalent,
even though many downstream tasks may benefit from the emerging properties of
reinforcement learning, particularly the enhanced reason capabilities. We study
the effects of rule-based reinforcement learning with the task of Document
Image Classification which is one of the most commonly studied downstream tasks
in document analysis. We find that reinforcement learning tends to have better
generalisation capabilities to out-of-distritbution data, which we examine in
three different scenarios, namely out-of-distribution images, unseen classes
and different modalities. Our code is available at
https://github.com/jungomi/vision-finetune.

</details>


### [118] [Jailbreaking on Text-to-Video Models via Scene Splitting Strategy](https://arxiv.org/abs/2509.22292)
*Wonjun Lee,Haon Park,Doehyeon Lee,Bumsub Ham,Suhyun Kim*

Main category: cs.CV

TL;DR: 本文提出了一种针对文本生成视频（T2V）模型的全新黑盒越狱攻击方法SceneSplit，实验显示其能高效绕过现有的安全机制，暴露了T2V模型在安全防护中的显著漏洞。


<details>
  <summary>Details</summary>
Motivation: 当前T2V模型发展迅速，但其安全机制未受充分检验。尽管已有的越狱攻击在大语言模型、视觉语言模型及文本生成图像模型上取得进展，T2V模型的相关研究仍存在空白，亟需专门的方法揭示其安全漏洞。

Method: 论文提出SceneSplit方法，将具有潜在危害的叙事拆分为多个单独安全的片段（场景），分别输入模型生成视频，之后组合这些片段诱发整体有害内容。该方法依赖抽象的生成空间组合收敛至危险区域，并通过迭代片段组合和策略库进一步提升攻击稳定性和成功率。

Result: SceneSplit在11类安全指标下对主流T2V模型（Luma Ray2、Hailuo、Veo2）测试，攻击成功率分别达77.2%、84.1%、78.2%，显著高于现有基线方法。

Conclusion: 论文证实当前T2V模型的安全机制可被结构化叙事攻击轻易突破，呼吁业界重视此类模型的安全改进，并为后续T2V安全机制的设计与完善提供了重要参考。

Abstract: Along with the rapid advancement of numerous Text-to-Video (T2V) models,
growing concerns have emerged regarding their safety risks. While recent
studies have explored vulnerabilities in models like LLMs, VLMs, and
Text-to-Image (T2I) models through jailbreak attacks, T2V models remain largely
unexplored, leaving a significant safety gap. To address this gap, we introduce
SceneSplit, a novel black-box jailbreak method that works by fragmenting a
harmful narrative into multiple scenes, each individually benign. This approach
manipulates the generative output space, the abstract set of all potential
video outputs for a given prompt, using the combination of scenes as a powerful
constraint to guide the final outcome. While each scene individually
corresponds to a wide and safe space where most outcomes are benign, their
sequential combination collectively restricts this space, narrowing it to an
unsafe region and significantly increasing the likelihood of generating a
harmful video. This core mechanism is further enhanced through iterative scene
manipulation, which bypasses the safety filter within this constrained unsafe
region. Additionally, a strategy library that reuses successful attack patterns
further improves the attack's overall effectiveness and robustness. To validate
our method, we evaluate SceneSplit across 11 safety categories on T2V models.
Our results show that it achieves a high average Attack Success Rate (ASR) of
77.2% on Luma Ray2, 84.1% on Hailuo, and 78.2% on Veo2, significantly
outperforming the existing baseline. Through this work, we demonstrate that
current T2V safety mechanisms are vulnerable to attacks that exploit narrative
structure, providing new insights for understanding and improving the safety of
T2V models.

</details>


### [119] [HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion Models](https://arxiv.org/abs/2509.22300)
*Seyedmorteza Sadat,Farnood Salehi,Romann M. Weber*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的动量采样技术——历史引导采样（HiGS），能够在不增加计算成本的情况下显著提升扩散模型生成图像的质量和效率。实验表明，HiGS在多种模型和设置下均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在图像生成方面取得了显著进展，但在神经网络评估次数较少或指导尺度较低时，生成结果仍然不够真实且细节不足。提高在有限计算资源下图像质量成为紧迫需求。

Method: 作者提出HiGS，通过在每次推理中结合过去预测结果（动量），利用当前预测与加权历史预测的差异，引导采样更接近真实高质量结果。该方法无需额外训练或微调，且几乎没有新增计算量，可直接集成到现有扩散模型框架中。

Result: 在众多实验中，HiGS在不同模型架构、采样预算及引导尺度下均提升了图像质量。特别是在ImageNet 256x256无指导生成任务中，HiGS利用预训练SiT模型只需30步便达到了1.61的当前最佳FID分数（通常需250步），展示了高效高保真采样能力。

Conclusion: HiGS是一种即插即用的扩散采样增强方法，在保持高生成速度的同时，显著提升了输出图像的真实性与细节表现，为扩散模型的实用化和高效扩展提供了新范式。

Abstract: While diffusion models have made remarkable progress in image generation,
their outputs can still appear unrealistic and lack fine details, especially
when using fewer number of neural function evaluations (NFEs) or lower guidance
scales. To address this issue, we propose a novel momentum-based sampling
technique, termed history-guided sampling (HiGS), which enhances quality and
efficiency of diffusion sampling by integrating recent model predictions into
each inference step. Specifically, HiGS leverages the difference between the
current prediction and a weighted average of past predictions to steer the
sampling process toward more realistic outputs with better details and
structure. Our approach introduces practically no additional computation and
integrates seamlessly into existing diffusion frameworks, requiring neither
extra training nor fine-tuning. Extensive experiments show that HiGS
consistently improves image quality across diverse models and architectures and
under varying sampling budgets and guidance scales. Moreover, using a
pretrained SiT model, HiGS achieves a new state-of-the-art FID of 1.61 for
unguided ImageNet generation at 256$\times$256 with only 30 sampling steps
(instead of the standard 250). We thus present HiGS as a plug-and-play
enhancement to standard diffusion sampling that enables faster generation with
higher fidelity.

</details>


### [120] [Johnson-Lindenstrauss Lemma Guided Network for Efficient 3D Medical Segmentation](https://arxiv.org/abs/2509.22307)
*Jinpeng Lu,Linghan Cai,Yinda Chen,Guo Tang,Songhan Jiang,Haoyuan Shi,Zhiwei Xiong*

Main category: cs.CV

TL;DR: VeloxSeg提出了一种轻量级3D医学图像分割框架，通过结合CNN-Transformer双流架构和创新的特征提取方法，显著提升了分割精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 在3D医学图像分割领域，轻量级方法常面临效率与鲁棒性之间的冲突，尤其在处理复杂结构和多模态数据时表现不佳。本文旨在突破现有轻量级分割方法在表达能力和泛化能力上的限制。

Method: 提出VeloxSeg，一种基于Paired Window Attention（PWA）和Johnson-Lindenstrauss lemma引导卷积（JLC）的CNN-Transformer双流架构。通过'glance-and-focus'机制快速提取多尺度特征，并利用JLC提升本地特征鲁棒性，参数量小。系统还拓展为可处理多模态数据，并通过空间解耦知识迁移（SDKT）引入自监督纹理先验，增强特征表达。

Result: 在多模态分割基准上，VeloxSeg的Dice系数提升了26%，GPU吞吐量提升11倍，CPU提升48倍，显著优于此前基线。

Conclusion: VeloxSeg通过与众不同的轻量级结构与知识迁移，有效提升3D医学图像分割的效率和鲁棒性，为实际医学图像分割应用提供有力技术支撑。

Abstract: Lightweight 3D medical image segmentation remains constrained by a
fundamental "efficiency / robustness conflict", particularly when processing
complex anatomical structures and heterogeneous modalities. In this paper, we
study how to redesign the framework based on the characteristics of
high-dimensional 3D images, and explore data synergy to overcome the fragile
representation of lightweight methods. Our approach, VeloxSeg, begins with a
deployable and extensible dual-stream CNN-Transformer architecture composed of
Paired Window Attention (PWA) and Johnson-Lindenstrauss lemma-guided
convolution (JLC). For each 3D image, we invoke a "glance-and-focus" principle,
where PWA rapidly retrieves multi-scale information, and JLC ensures robust
local feature extraction with minimal parameters, significantly enhancing the
model's ability to operate with low computational budget. Followed by an
extension of the dual-stream architecture that incorporates modal interaction
into the multi-scale image-retrieval process, VeloxSeg efficiently models
heterogeneous modalities. Finally, Spatially Decoupled Knowledge Transfer
(SDKT) via Gram matrices injects the texture prior extracted by a
self-supervised network into the segmentation network, yielding stronger
representations than baselines at no extra inference cost. Experimental results
on multimodal benchmarks show that VeloxSeg achieves a 26% Dice improvement,
alongside increasing GPU throughput by 11x and CPU by 48x. Codes are available
at https://github.com/JinPLu/VeloxSeg.

</details>


### [121] [NIFTY: a Non-Local Image Flow Matching for Texture Synthesis](https://arxiv.org/abs/2509.22318)
*Pierrick Chatillon,Julien Rabin,David Tschumperlé*

Main category: cs.CV

TL;DR: 本文提出了NIFTY，一种结合了扩散模型和传统基于块的纹理优化的新型杂交框架，实现了基于样例的纹理合成。无需神经网络训练，同时克服了传统方法中的初始化和伪影问题。实验显示该方法优于已有代表性方法。


<details>
  <summary>Details</summary>
Motivation: 解决基于样例的纹理合成问题，旨在兼得扩散模型与传统块匹配法的优点，同时规避二者的不足，如神经网络训练复杂或可视化伪影。

Method: 提出NIFTY框架，利用卷积神经网络训练的扩散模型的原理，并结合非局部块（patch）匹配，实现非参数流匹配，无需神经网络训练。该方法解决了先前块匹配法易受初始化和伪影影响的问题。

Result: 实验结果表明，NIFTY框架在多个评价指标上优于文献中的代表性纹理合成方法，视觉质量高且鲁棒性更好。

Conclusion: NIFTY 作为一种融合扩散模型和块优化的新方法，在无需神经网络泛化训练的前提下，取得了优异的纹理合成效果，为今后相关研究提供了新思路。

Abstract: This paper addresses the problem of exemplar-based texture synthesis. We
introduce NIFTY, a hybrid framework that combines recent insights on diffusion
models trained with convolutional neural networks, and classical patch-based
texture optimization techniques. NIFTY is a non-parametric flow-matching model
built on non-local patch matching, which avoids the need for neural network
training while alleviating common shortcomings of patch-based methods, such as
poor initialization or visual artifacts. Experimental results demonstrate the
effectiveness of the proposed approach compared to representative methods from
the literature. Code is available at https://github.com/PierrickCh/Nifty.git

</details>


### [122] [RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion Transformer](https://arxiv.org/abs/2509.22323)
*Wangbo Zhao,Yizeng Han,Zhiwei Tang,Jiasheng Tang,Pengfei Zhou,Kai Wang,Bohan Zhuang,Zhangyang Wang,Fan Wang,Yang You*

Main category: cs.CV

TL;DR: RAPID3为Diffusion Transformers引入三层强化策略，实现每张图片自适应加速，无需微调生成器，显著提升采样速度且保持高质量。


<details>
  <summary>Details</summary>
Motivation: 当前Diffusion Transformers尽管生成效果出色，但采样速度慢。现有的加速方法大多基于一刀切的启发式或人工设定，对所有图片统一处理，影响生成质量；而动态图网络虽可自适应加速，但微调成本高，难以广泛应用。作者希望无须微调整体模型的情况下，达到针对每张图片灵活加速的目的。

Method: 提出RAPID3框架，通过Step-Skip、Cache-Reuse与Sparse-Attention三种独立而轻量的策略头在每步决策相应的加速行为。所有策略参数用Group Relative Policy Optimization（GRPO）在线训练，生成器参数冻结。同时引入一个对抗学习判别器丰富奖励信号，仅在生成分布与原模型相近时提升回报，防止策略投机取巧。

Result: 在Stable Diffusion 3、FLUX等多种主流DiT骨干上，RAPID3实现近3倍的采样加速，并能保持与原模型相当的生成质量。

Conclusion: RAPID3无需微调生成器，实现基于单张图片自适应加速，极大提升了Diffusion Transformers的实际推理效率，在保证生成质量的前提下，有望推动相关模型的实际应用。

Abstract: Diffusion Transformers (DiTs) excel at visual generation yet remain hampered
by slow sampling. Existing training-free accelerators - step reduction, feature
caching, and sparse attention - enhance inference speed but typically rely on a
uniform heuristic or a manually designed adaptive strategy for all images,
leaving quality on the table. Alternatively, dynamic neural networks offer
per-image adaptive acceleration, but their high fine-tuning costs limit broader
applicability. To address these limitations, we introduce RAPID3: Tri-Level
Reinforced Acceleration Policies for Diffusion Transformers, a framework that
delivers image-wise acceleration with zero updates to the base generator.
Specifically, three lightweight policy heads - Step-Skip, Cache-Reuse, and
Sparse-Attention - observe the current denoising state and independently decide
their corresponding speed-up at each timestep. All policy parameters are
trained online via Group Relative Policy Optimization (GRPO) while the
generator remains frozen. Meanwhile, an adversarially learned discriminator
augments the reward signal, discouraging reward hacking by boosting returns
only when generated samples stay close to the original model's distribution.
Across state-of-the-art DiT backbones, including Stable Diffusion 3 and FLUX,
RAPID3 achieves nearly 3x faster sampling with competitive generation quality.

</details>


### [123] [Pedestrian Attribute Recognition via Hierarchical Cross-Modality HyperGraph Learning](https://arxiv.org/abs/2509.22331)
*Xiao Wang,Shujuan Wu,Xiaoxia Cheng,Changwei Bi,Jin Tang,Bin Luo*

Main category: cs.CV

TL;DR: 本文提出了多模态知识图谱以提升行人属性识别（PAR），结合视觉特征和属性文本，充分挖掘语义与上下文关系，实验验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有行人属性识别方法未能充分利用属性知识和上下文信息，且基于属性文本的多模态方法仍处于初级阶段，无法实现最佳的识别效果。

Method: 构建一种多模态知识图谱，通过挖掘局部视觉特征与文本之间、属性与大量视觉上下文样本之间的关系。提出多模态知识图谱构建方法，并引入知识图谱引导的跨模态超图学习框架，增强传统PAR方法。

Result: 在多个行人属性识别基准数据集上进行了全面实验，结果显示提出的方法在PAR任务上具有显著提升，验证了知识图谱引导的有效性。

Conclusion: 多模态知识图谱为知识引导的行人属性识别奠定了坚实基础，未来可促进更精准的属性识别。源码将开源。

Abstract: Current Pedestrian Attribute Recognition (PAR) algorithms typically focus on
mapping visual features to semantic labels or attempt to enhance learning by
fusing visual and attribute information. However, these methods fail to fully
exploit attribute knowledge and contextual information for more accurate
recognition. Although recent works have started to consider using attribute
text as additional input to enhance the association between visual and semantic
information, these methods are still in their infancy. To address the above
challenges, this paper proposes the construction of a multi-modal knowledge
graph, which is utilized to mine the relationships between local visual
features and text, as well as the relationships between attributes and
extensive visual context samples. Specifically, we propose an effective
multi-modal knowledge graph construction method that fully considers the
relationships among attributes and the relationships between attributes and
vision tokens. To effectively model these relationships, this paper introduces
a knowledge graph-guided cross-modal hypergraph learning framework to enhance
the standard pedestrian attribute recognition framework. Comprehensive
experiments on multiple PAR benchmark datasets have thoroughly demonstrated the
effectiveness of our proposed knowledge graph for the PAR task, establishing a
strong foundation for knowledge-guided pedestrian attribute recognition. The
source code of this paper will be released on
https://github.com/Event-AHU/OpenPAR

</details>


### [124] [Vision-Language Alignment from Compressed Image Representations using 2D Gaussian Splatting](https://arxiv.org/abs/2509.22615)
*Yasmine Omri,Connor Ding,Tsachy Weissman,Thierry Tambe*

Main category: cs.CV

TL;DR: 本论文提出使用2D高斯斑点（2D Gaussian Splatting, 2DGS）作为视觉语言任务的替代视觉表示方式，旨在提高边缘设备到云端的传输效率并减少序列长度膨胀问题。通过结构化初始化、亮度感知剪枝和批量CUDA优化，实现了大幅加速和高效利用GPU资源，并在大数据集上展示了合理的零样本性能和极高的输入压缩比。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言系统通常依赖于RGB编码，从而导致图像传输成本高、能耗大，以及基于patch的Token化带来序列膨胀，降低效率。因此，希望研发一种更高效、适应性更强的图像表示方法，既能保证模型的多模态识别能力，又提升传输与计算效率。

Method: 作者采用2D高斯斑点作为新的视觉表示，用一组有色的各向异性高斯参数化图像，配合结构化初始化、亮度感知剪枝和批量CUDA内核以实现高效拟合和利用。模型方面，将CLIP预训练结构迁移到2DGS，主干为冻结的RGB Transformer，仅在前端输入stem及感知采样器上训练极少参数。

Result: 该方案在大规模DataComp子集上，实现了对图像的3~20倍压缩，并保证了ImageNet-1K上的合理零样本识别性能。2DGS的拟合速度比现有方法快90倍，GPU利用率达97%。尽管精度尚未超过主流RGB编码器，但在输入压缩和传输效率上取得了显著提升。

Conclusion: 2DGS被验证为一种多模态场景下可行的新型视觉表示，具备良好的压缩与计算效率，为边缘-云协同学习场景提供了有前景的方向。论文还指出了架构瓶颈，并为提升语义表达与传输效率的视觉表示研究开辟了新路径。

Abstract: Modern vision language pipelines are driven by RGB vision encoders trained on
massive image text corpora. While these pipelines have enabled impressive zero
shot capabilities and strong transfer across tasks, they still inherit two
structural inefficiencies from the pixel domain: (i) transmitting dense RGB
images from edge devices to the cloud is energy intensive and costly, and (ii)
patch based tokenization explodes sequence length, stressing attention budgets
and context limits. We explore 2D Gaussian Splatting (2DGS) as an alternative
visual substrate for alignment: a compact, spatially adaptive representation
that parameterizes images by a set of colored anisotropic Gaussians. We develop
a scalable 2DGS pipeline with structured initialization, luminance aware
pruning, and batched CUDA kernels, achieving over 90x faster fitting and about
97% GPU utilization compared to prior implementations. We further adapt
contrastive language image pretraining (CLIP) to 2DGS by reusing a frozen
RGB-based transformer backbone with a lightweight splat aware input stem and a
perceiver resampler, training only about 7% of the total parameters. On large
DataComp subsets, GS encoders yield meaningful zero shot ImageNet-1K
performance while compressing inputs 3 to 20x relative to pixels. While
accuracy currently trails RGB encoders, our results establish 2DGS as a viable
multimodal substrate, pinpoint architectural bottlenecks, and open a path
toward representations that are both semantically powerful and transmission
efficient for edge cloud learning.

</details>


### [125] [CircuitSense: A Hierarchical Circuit System Benchmark Bridging Visual Comprehension and Symbolic Reasoning in Engineering Design Process](https://arxiv.org/abs/2509.22339)
*Arman Akbari,Jian Gao,Yifei Zou,Mei Yang,Jinru Duan,Dmitrii Torbunov,Yanzhi Wang,Yihui Ren,Xuan Zhang*

Main category: cs.CV

TL;DR: 该论文提出了CircuitSense基准，用于评估多模态大语言模型(MLLMs)在电路设计理解中的视觉与符号推理能力，发现现有模型感知任务表现良好但符号推理能力严重不足。


<details>
  <summary>Details</summary>
Motivation: 以往大语言模型在自然图像任务上表现突出，但在工程设计中从技术图示中抽取数学模型的能力却未被系统研究。工程设计需要视觉理解和数学推理的结合，尤其是在电路设计过程中，这一能力尤为重要。

Method: 作者提出CircuitSense基准，包括8,006+道题，覆盖从元件级原理图到系统级框图的多层级任务。基准涵盖感知、分析和设计三类任务，突出视觉输入的符号方程推导。作者还提出分层合成数据生成机制，并对六个主流闭源和开源MLLMs进行全面评估。

Result: 闭源模型在组件识别和拓扑识别等感知任务上超过85%准确率，但在符号推导和分析推理上的准确率低于19%，表明视觉解析与符号推理之间存在显著差距。具备更强符号推理能力的模型在设计任务中表现更好。

Conclusion: 数学（符号）推理能力是工程（如电路设计）AI模型的核心能力。当前MLLMs在从视觉信息到符号推理的关键步骤上表现不足，表明未来工作需要加强模型的数学推理能力以提升工程智能水平。

Abstract: Engineering design operates through hierarchical abstraction from system
specifications to component implementations, requiring visual understanding
coupled with mathematical reasoning at each level. While Multi-modal Large
Language Models (MLLMs) excel at natural image tasks, their ability to extract
mathematical models from technical diagrams remains unexplored. We present
\textbf{CircuitSense}, a comprehensive benchmark evaluating circuit
understanding across this hierarchy through 8,006+ problems spanning
component-level schematics to system-level block diagrams. Our benchmark
uniquely examines the complete engineering workflow: Perception, Analysis, and
Design, with a particular emphasis on the critical but underexplored capability
of deriving symbolic equations from visual inputs. We introduce a hierarchical
synthetic generation pipeline consisting of a grid-based schematic generator
and a block diagram generator with auto-derived symbolic equation labels.
Comprehensive evaluation of six state-of-the-art MLLMs, including both
closed-source and open-source models, reveals fundamental limitations in
visual-to-mathematical reasoning. Closed-source models achieve over 85\%
accuracy on perception tasks involving component recognition and topology
identification, yet their performance on symbolic derivation and analytical
reasoning falls below 19\%, exposing a critical gap between visual parsing and
symbolic reasoning. Models with stronger symbolic reasoning capabilities
consistently achieve higher design task accuracy, confirming the fundamental
role of mathematical understanding in circuit synthesis and establishing
symbolic reasoning as the key metric for engineering competence.

</details>


### [126] [LABELING COPILOT: A Deep Research Agent for Automated Data Curation in Computer Vision](https://arxiv.org/abs/2509.22631)
*Debargha Ganguly,Sumit Kumar,Ishwar Balappanawar,Weicong Chen,Shashank Kambhatla,Srinivasan Iyengar,Shivkumar Kalyanaraman,Ponnurangam Kumaraguru,Vipin Chaudhary*

Main category: cs.CV

TL;DR: 该论文提出了Labeling Copilot，这是一个为计算机视觉领域设计的数据策划智能代理系统，旨在提升大规模数据集的质量和多样性，并显著优化标注效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 高质量、领域特定的数据集是构建鲁棒视觉系统的瓶颈，现有的数据策划流程面临效率低、成本高、难以权衡数据质量与多样性等问题。因此亟需一种智能化的新方法提升大规模无标签数据的整理和标注效率。

Method: 提出Labeling Copilot系统，其核心为一个由大多模态语言模型驱动的中央代理，通过多步推理调用三种工具：（1）Calibrated Discovery 用于从海量数据中精准筛选相关样本；（2）Controllable Synthesis 通过生成式方式增强稀有场景数据并进行严格筛选；（3）Consensus Annotation 通过多模型协作、非极大值抑制与投票机制创新融合，提高自动标注准确度。

Result: 大规模实验验证显示：Consensus Annotation在COCO数据集的目标发现能力突出，平均每张图片提出14.2个候选对象（远超真实标注7.4），最终自动标注mAP达37.1%。在类别极度不平衡的Open Images大规模数据集上，该模块发现了903类新类别，共可识别超过1500类。Calibrated Discovery采用主动学习方法，样本效率相当时计算效率比现有方法高40倍。

Conclusion: 实验结果表明，采用智能代理与高效工具集成的工作流，能够为工业级大规模数据策划提供稳健、可扩展的基础，有助于大幅提升智能视觉系统的数据驱动能力。

Abstract: Curating high-quality, domain-specific datasets is a major bottleneck for
deploying robust vision systems, requiring complex trade-offs between data
quality, diversity, and cost when researching vast, unlabeled data lakes. We
introduce Labeling Copilot, the first data curation deep research agent for
computer vision. A central orchestrator agent, powered by a large multimodal
language model, uses multi-step reasoning to execute specialized tools across
three core capabilities: (1) Calibrated Discovery sources relevant,
in-distribution data from large repositories; (2) Controllable Synthesis
generates novel data for rare scenarios with robust filtering; and (3)
Consensus Annotation produces accurate labels by orchestrating multiple
foundation models via a novel consensus mechanism incorporating non-maximum
suppression and voting. Our large-scale validation proves the effectiveness of
Labeling Copilot's components. The Consensus Annotation module excels at object
discovery: on the dense COCO dataset, it averages 14.2 candidate proposals per
image-nearly double the 7.4 ground-truth objects-achieving a final annotation
mAP of 37.1%. On the web-scale Open Images dataset, it navigated extreme class
imbalance to discover 903 new bounding box categories, expanding its capability
to over 1500 total. Concurrently, our Calibrated Discovery tool, tested at a
10-million sample scale, features an active learning strategy that is up to 40x
more computationally efficient than alternatives with equivalent sample
efficiency. These experiments validate that an agentic workflow with optimized,
scalable tools provides a robust foundation for curating industrial-scale
datasets.

</details>


### [127] [HierLight-YOLO: A Hierarchical and Lightweight Object Detection Network for UAV Photography](https://arxiv.org/abs/2509.22365)
*Defan Chen,Yaohua Hu,Luchan Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种针对无人机图像中小目标的实时检测方法HierLight-YOLO，兼顾检测精度和模型轻量化，在VisDrone2019数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 在无人机拍摄的复杂场景中，小目标（如小于32像素）检测难且资源受限，现有YOLO系列虽快但小目标漏检率高，急需提升小目标检测效果、同时保证实时性和低计算资源消耗。

Method: 提出HierLight-YOLO模型，在YOLOv8基础上，引入分层特征融合模块HEPAN提升多尺度特征聚合能力，并设计IRDCB和LDown两个轻量模块以减少参数与计算量，同时设计微小目标检测头（spatial resolution更高），整体提升检测小目标的精度和速度。

Result: 在VisDrone2019公开数据集上通过对比实验和消融实验，HierLight-YOLO在小目标检测上取得了SOTA性能，表现优于现有方法，参数量和计算复杂度均得以降低。

Conclusion: HierLight-YOLO证明了通过分层特征融合和模型轻量化能显著提升无人机图像中小目标的实时检测能力，为边缘设备部署提供了有效解决方案。

Abstract: The real-time detection of small objects in complex scenes, such as the
unmanned aerial vehicle (UAV) photography captured by drones, has dual
challenges of detecting small targets (<32 pixels) and maintaining real-time
efficiency on resource-constrained platforms. While YOLO-series detectors have
achieved remarkable success in real-time large object detection, they suffer
from significantly higher false negative rates for drone-based detection where
small objects dominate, compared to large object scenarios. This paper proposes
HierLight-YOLO, a hierarchical feature fusion and lightweight model that
enhances the real-time detection of small objects, based on the YOLOv8
architecture. We propose the Hierarchical Extended Path Aggregation Network
(HEPAN), a multi-scale feature fusion method through hierarchical cross-level
connections, enhancing the small object detection accuracy. HierLight-YOLO
includes two innovative lightweight modules: Inverted Residual Depthwise
Convolution Block (IRDCB) and Lightweight Downsample (LDown) module, which
significantly reduce the model's parameters and computational complexity
without sacrificing detection capabilities. Small object detection head is
designed to further enhance spatial resolution and feature fusion to tackle the
tiny object (4 pixels) detection. Comparison experiments and ablation studies
on the VisDrone2019 benchmark demonstrate state-of-the-art performance of
HierLight-YOLO.

</details>


### [128] [Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs](https://arxiv.org/abs/2509.22646)
*Xingyu Fu,Siyi Liu,Yinuo Xu,Pan Lu,Guangqiuse Hu,Tianbo Yang,Taran Anantasagar,Christopher Shen,Yikai Mao,Yuanzhe Liu,Keyush Shah,Chung Un Lee,Yejin Choi,James Zou,Dan Roth,Chris Callison-Burch*

Main category: cs.CV

TL;DR: 本文提出了DeeptraceReward，这是首个针对视频生成领域中人类感知到的深度伪造（deepfake）痕迹进行空间和时间细致标注的基准数据集，用于训练和评测AI模型识别伪造视频及其具体痕迹的能力。


<details>
  <summary>Details</summary>
Motivation: 随着视频生成模型的飞速发展，AI生成视频的真实性不断提升，识别伪造视频尤其是定位具体伪造痕迹（spatiotemporal grounded visual artifacts）成为亟需解决的问题。然而，以往的研究多聚焦于整体真假分类，缺乏对具体伪造区域、发生时间以及人类感知原因的细致研究。该领域缺乏系统基准用于评测和推动具备社会责任的视频生成技术。

Method: 作者构建了DeeptraceReward数据集，对3300个高质量AI生成视频进行了4300条空间和时间细致的人工标注。每条标注包括自然语言解释、具体边框定位、以及详细起止时间点。基于这些标注，总结了9大类人类感知到的深度伪造痕迹，并用标注数据训练多模态语言模型（LM）作为奖励模型，使AI模型学习模仿人类对于伪造线索的判别和定位能力。

Result: 在DeeptraceReward基准上，所提出的7B参数奖励模型在伪造线索识别、区域定位和解释能力上，平均性能超越了GPT-5 34.7%。研究还发现，整体验别真假相对容易，识别和定位具体伪造痕迹（精细分类）难度依次增加：自然语言解释最简单，空间定位较难，时间标注最难。

Conclusion: DeeptraceReward填补了视频深度伪造识别领域的基准空白，为社会责任和可信任视频生成模型提供了严谨测试和训练手段，有助于推动AI识别深度伪造痕迹能力的提升，以及更安全、可控的视频生成技术发展。

Abstract: Can humans identify AI-generated (fake) videos and provide grounded reasons?
While video generation models have advanced rapidly, a critical dimension --
whether humans can detect deepfake traces within a generated video, i.e.,
spatiotemporal grounded visual artifacts that reveal a video as machine
generated -- has been largely overlooked. We introduce DeeptraceReward, the
first fine-grained, spatially- and temporally- aware benchmark that annotates
human-perceived fake traces for video generation reward. The dataset comprises
4.3K detailed annotations across 3.3K high-quality generated videos. Each
annotation provides a natural-language explanation, pinpoints a bounding-box
region containing the perceived trace, and marks precise onset and offset
timestamps. We consolidate these annotations into 9 major categories of
deepfake traces that lead humans to identify a video as AI-generated, and train
multimodal language models (LMs) as reward models to mimic human judgments and
localizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by
34.7% on average across fake clue identification, grounding, and explanation.
Interestingly, we observe a consistent difficulty gradient: binary fake v.s.
real classification is substantially easier than fine-grained deepfake trace
detection; within the latter, performance degrades from natural language
explanations (easiest), to spatial grounding, to temporal labeling (hardest).
By foregrounding human-perceived deepfake traces, DeeptraceReward provides a
rigorous testbed and training signal for socially aware and trustworthy video
generation.

</details>


### [129] [Effectiveness of Large Multimodal Models in Detecting Disinformation: Experimental Results](https://arxiv.org/abs/2509.22377)
*Yasmina Kheddache,Marc Lalonde*

Main category: cs.CV

TL;DR: 本文利用大规模多模态模型（LMMs），特别是GPT-4o，提出了一套完整的多模态虚假信息检测方法，包括高级提示工程、结构化分析框架以及多维评估标准，并通过多数据集验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 随着虚假信息在社交平台、媒体中的广泛传播，尤其是文本和图像结合的多模态信息，现有检测手段面临极大挑战，因此亟需更先进和精确的自动化检测技术。

Method: 作者基于GPT-4o构建多模态检测系统：（1）使用高级提示工程优化评估流程；（2）对文本和图像进行预处理，以适配模型输入限制；（3）提出6项细粒度评估标准，包括自信度打分机制；（4）在多个多样化公开数据集上系统评测；（5）多次重复测试分析预测稳定性，提出置信度和波动性相关评估指标。

Result: GPT-4o在多个真实与合成虚假信息数据集（Gossipcop、Politifact、Fakeddit等）上展示了较强的检测效果，文中详细分析了其在不同场景下的优势与当前局限，提出的多维度评估体系增强了自动检测结果的可解释性与稳定性验证。

Conclusion: 该研究为自动化多模态虚假信息检测建立了系统、可复现的方法学框架，证明了先进大模型（如GPT-4o）在此领域的应用潜力和未来优化方向，对社交平台内容治理具应用意义。

Abstract: The proliferation of disinformation, particularly in multimodal contexts
combining text and images, presents a significant challenge across digital
platforms. This study investigates the potential of large multimodal models
(LMMs) in detecting and mitigating false information. We propose to approach
multimodal disinformation detection by leveraging the advanced capabilities of
the GPT-4o model. Our contributions include: (1) the development of an
optimized prompt incorporating advanced prompt engineering techniques to ensure
precise and consistent evaluations; (2) the implementation of a structured
framework for multimodal analysis, including a preprocessing methodology for
images and text to comply with the model's token limitations; (3) the
definition of six specific evaluation criteria that enable a fine-grained
classification of content, complemented by a self-assessment mechanism based on
confidence levels; (4) a comprehensive performance analysis of the model across
multiple heterogeneous datasets Gossipcop, Politifact, Fakeddit, MMFakeBench,
and AMMEBA highlighting GPT-4o's strengths and limitations in disinformation
detection; (5) an investigation of prediction variability through repeated
testing, evaluating the stability and reliability of the model's
classifications; and (6) the introduction of confidence-level and
variability-based evaluation methods. These contributions provide a robust and
reproducible methodological framework for automated multimodal disinformation
analysis.

</details>


### [130] [CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning](https://arxiv.org/abs/2509.22647)
*Long Xing,Xiaoyi Dong,Yuhang Zang,Yuhang Cao,Jianze Liang,Qidong Huang,Jiaqi Wang,Feng Wu,Dahua Lin*

Main category: cs.CV

TL;DR: 论文提出了一种基于强化学习可验证奖励（RLVR）的新型图像描述训练框架 CapRL，通过让非视觉大模型仅依据生成描述回答多项选择题，提升描述质量和泛化能力；在多个基准取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有图像描述任务依赖高昂的人类或专有模型标注的有监督微调（SFT），导致模型泛化性差，创新能力不足，仅会死记硬背标准答案。为解决该问题，需寻找更通用且可扩展的训练方法。

Method: 作者提出 CapRL 框架，首次将强化学习可验证奖励（RLVR）用于主观性的图像描述任务。具体做法是两个阶段：先由视觉语言大模型（LVLM）生成描述，再用不具备视觉能力的语言模型（LLM）仅根据描述回答关于图片的问题，回答准确度作为奖励信号，反向提升描述质量。论文还构建了新的标注数据集 CapRL-5M 并以此预训练。

Result: CapRL 在12个公开基准上显著优于现有方法。在Prism图像描述质量评价框架中，CapRL 表现与 Qwen2.5-VL-72B 相当，平均性能超过基线8.4%。

Conclusion: CapRL 为图像描述任务提供了新的训练范式，减少了对人工标注依赖，能生成更高质量、多样性和泛化能力更强的描述，为视觉-语言模型的预训练和应用带来重要推动。

Abstract: Image captioning is a fundamental task that bridges the visual and linguistic
domains, playing a critical role in pre-training Large Vision-Language Models
(LVLMs). Current state-of-the-art captioning models are typically trained with
Supervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable
data annotated by humans or proprietary models. This approach often leads to
models that memorize specific ground-truth answers, limiting their generality
and ability to generate diverse, creative descriptions. To overcome the
limitation of SFT, we propose applying the Reinforcement Learning with
Verifiable Rewards (RLVR) paradigm to the open-ended task of image captioning.
A primary challenge, however, is designing an objective reward function for the
inherently subjective nature of what constitutes a "good" caption. We introduce
Captioning Reinforcement Learning (CapRL), a novel training framework that
redefines caption quality through its utility: a high-quality caption should
enable a non-visual language model to accurately answer questions about the
corresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM
generates a caption, and the objective reward is derived from the accuracy of a
separate, vision-free LLM answering Multiple-Choice Questions based solely on
that caption. As the first study to apply RLVR to the subjective image
captioning task, we demonstrate that CapRL significantly enhances multiple
settings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B
results in substantial gains across 12 benchmarks. Moreover, within the Prism
Framework for caption quality evaluation, CapRL achieves performance comparable
to Qwen2.5-VL-72B, while exceeding the baseline by an average margin of 8.4%.
Code is available here: https://github.com/InternLM/CapRL.

</details>


### [131] [GPT-4 for Occlusion Order Recovery](https://arxiv.org/abs/2509.22383)
*Kaziwa Saleh,Zhyar Rzgar K Rostam,Sándor Szénási,Zoltán Vámossy*

Main category: cs.CV

TL;DR: 本文提出利用预训练的GPT-4大模型，通过特定prompt结合输入图像，预测物体之间的遮挡顺序，并生成遮挡矩阵，有效提升对复杂场景中遮挡关系的理解。


<details>
  <summary>Details</summary>
Motivation: 当前视觉模型在处理复杂、密集场景的遮挡（occlusion）问题时表现有限。要实现精准图像理解与物体关系推理，亟需改进遮挡顺序的预测和处理能力。

Method: 作者创新性地将预训练的GPT-4大模型引入遮挡顺序推理任务，通过为GPT-4设计专门的提示词（prompt）并输入图片，GPT-4输出物体的遮挡顺序。随后将结果解析为遮挡矩阵，用于后续任务。

Result: 在COCOA和InstaOrder数据集上，实验表明该方法通过语义、视觉模式及常识推理，在零样本（zero-shot）条件下，能够比传统方法更准确地预测遮挡关系。

Conclusion: GPT-4模型可以不依赖训练标注数据，高效、准确地推理复杂遮挡关系，具备很好的泛化能力，并易于集成到现有的遮挡处理和图像理解框架中。

Abstract: Occlusion remains a significant challenge for current vision models to
robustly interpret complex and dense real-world images and scenes. To address
this limitation and to enable accurate prediction of the occlusion order
relationship between objects, we propose leveraging the advanced capability of
a pre-trained GPT-4 model to deduce the order. By providing a specifically
designed prompt along with the input image, GPT-4 can analyze the image and
generate order predictions. The response can then be parsed to construct an
occlusion matrix which can be utilized in assisting with other occlusion
handling tasks and image understanding. We report the results of evaluating the
model on COCOA and InstaOrder datasets. The results show that by using semantic
context, visual patterns, and commonsense knowledge, the model can produce more
accurate order predictions. Unlike baseline methods, the model can reason about
occlusion relationships in a zero-shot fashion, which requires no annotated
training data and can easily be integrated into occlusion handling frameworks.

</details>


### [132] [Gradient-based multi-focus image fusion with focus-aware saliency enhancement](https://arxiv.org/abs/2509.22392)
*Haoyu Li,XiaoSong Li*

Main category: cs.CV

TL;DR: 本文提出了一种针对多焦点图像融合（MFIF）的新方法，重点提升图像融合边界的质量，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多焦点图像融合方法在处理聚焦-失焦边界时，常导致边界模糊和细节丢失，无法获得高质量的全聚焦图像。提升边界区域的融合质量对于监控、显微成像和计算摄影等应用很重要。

Method: 提出基于显著边界增强的MFIF方法，具体包括：（1）使用梯度域模型获得初步融合结果，完整保留边界；（2）使用Tenengrad梯度检测从源图像和初步融合图中提取显著特征，并生成显著图；（3）开发结合梯度与互补信息的聚焦度量方法，整合多图像的显著与互补信息，联合优化聚焦区域，从而输出高质量融合图像。

Result: 在四个公开数据集上的大量实验表明，该方法无论是在主观视觉还是客观评价指标上，都一致性优于12种现有主流方法。

Conclusion: 所提出的基于显著边界增强的MFIF方法可以有效改善融合图像的边界质量与细节保留，为相关应用带来更优的全聚焦效果。源代码已开源。

Abstract: Multi-focus image fusion (MFIF) aims to yield an all-focused image from
multiple partially focused inputs, which is crucial in applications cover
sur-veillance, microscopy, and computational photography. However, existing
methods struggle to preserve sharp focus-defocus boundaries, often resulting in
blurred transitions and focused details loss. To solve this problem, we propose
a MFIF method based on significant boundary enhancement, which generates
high-quality fused boundaries while effectively detecting focus in-formation.
Particularly, we propose a gradient-domain-based model that can obtain initial
fusion results with complete boundaries and effectively pre-serve the boundary
details. Additionally, we introduce Tenengrad gradient detection to extract
salient features from both the source images and the ini-tial fused image,
generating the corresponding saliency maps. For boundary refinement, we develop
a focus metric based on gradient and complementary information, integrating the
salient features with the complementary infor-mation across images to emphasize
focused regions and produce a high-quality initial decision result. Extensive
experiments on four public datasets demonstrate that our method consistently
outperforms 12 state-of-the-art methods in both subjective and objective
evaluations. We have realized codes in https://github.com/Lihyua/GICI

</details>


### [133] [Text Adversarial Attacks with Dynamic Outputs](https://arxiv.org/abs/2509.22393)
*Wenqiang Wang,Siyuan Liang,Xiao Yan,Xiaochun Cao*

Main category: cs.CV

TL;DR: 本文提出了适用于动态输出场景的文本对抗攻击方法TDOA，通过聚类训练代理模型，将动态输出转化为静态单输出问题，并采用远标签定向攻击策略，在多个数据集和大模型（如ChatGPT）上验证了其高效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本对抗攻击主要面向静态输出场景，难以应对动态输出和大语言模型开放型任务，因而需要开发适用于动态场景的新型攻击方法。

Method: 提出TDOA方法，使用聚类方法训练代理模型，将原本的动态输出问题转为可控的单输出问题；提出远标签定向攻击策略，选择与模型粗粒度标签最远的对抗向量最大化干扰效应。

Result: TDOA在四个数据集和八个受害模型上的攻击实验中，无需多次查询即可达到最高50.81%的攻击成功率，对静态场景同样获得了82.68%的ASR；在生成式任务设定下的表现也优于现有方法。

Conclusion: TDOA不仅提升了动态输出场景下的文本对抗攻击性能，同时在传统静态场景和生成式任务中也实现了新高，能有效威胁现有大型语言模型，对实用安全防护有重要参考意义。

Abstract: Text adversarial attack methods are typically designed for static scenarios
with fixed numbers of output labels and a predefined label space, relying on
extensive querying of the victim model (query-based attacks) or the surrogate
model (transfer-based attacks). To address this gap, we introduce the Textual
Dynamic Outputs Attack (TDOA) method, which employs a clustering-based
surrogate model training approach to convert the dynamic-output scenario into a
static single-output scenario. To improve attack effectiveness, we propose the
farthest-label targeted attack strategy, which selects adversarial vectors that
deviate most from the model's coarse-grained labels, thereby maximizing
disruption. We extensively evaluate TDOA on four datasets and eight victim
models (e.g., ChatGPT-4o, ChatGPT-4.1), showing its effectiveness in crafting
adversarial examples and its strong potential to compromise large language
models with limited access. With a single query per text, TDOA achieves a
maximum attack success rate of 50.81\%. Additionally, we find that TDOA also
achieves state-of-the-art performance in conventional static output scenarios,
reaching a maximum ASR of 82.68\%. Meanwhile, by conceptualizing translation
tasks as classification problems with unbounded output spaces, we extend the
TDOA framework to generative settings, surpassing prior results by up to 0.64
RDBLEU and 0.62 RDchrF.

</details>


### [134] [Integrating Background Knowledge in Medical Semantic Segmentation with Logic Tensor Networks](https://arxiv.org/abs/2509.22399)
*Luca Bergamin,Giovanna Maria Dimitri,Fabio Aiolli*

Main category: cs.CV

TL;DR: 该论文提出在医学图像分割任务中，将医学常识规则以一阶逻辑形式融入深度学习模型损失函数，使用Logic Tensor Networks提升分割性能，特别是在训练数据稀缺时。实验证明该方法在脑MRI海马体分割任务中优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割在医学辅助决策中非常重要，但现有的基于深度学习的方法在受噪声和伪影影响或数据稀缺时性能有限。引入医学背景知识有望提升模型表现。

Method: 通过引入Logic Tensor Networks，将医学常识以一阶逻辑（FOL）规则编码，并将这些规则（如分割形状的约束、不同区域间的关系）作为损失项，与SwinUNETR模型结合，形成端到端的分割框架。

Result: 在脑MRI海马体分割实验中，该方法提升了分割性能，特别是在训练数据有限的情况下表现更为显著。

Conclusion: 神经符号方法（如LTN）能够有效结合医学知识和深度学习模型，提升医学图像分割效果，并具有迁移到其他医学分割任务的潜力。

Abstract: Semantic segmentation is a fundamental task in medical image analysis, aiding
medical decision-making by helping radiologists distinguish objects in an
image. Research in this field has been driven by deep learning applications,
which have the potential to scale these systems even in the presence of noise
and artifacts. However, these systems are not yet perfected. We argue that
performance can be improved by incorporating common medical knowledge into the
segmentation model's loss function. To this end, we introduce Logic Tensor
Networks (LTNs) to encode medical background knowledge using first-order logic
(FOL) rules. The encoded rules span from constraints on the shape of the
produced segmentation, to relationships between different segmented areas. We
apply LTNs in an end-to-end framework with a SwinUNETR for semantic
segmentation. We evaluate our method on the task of segmenting the hippocampus
in brain MRI scans. Our experiments show that LTNs improve the baseline
segmentation performance, especially when training data is scarce. Despite
being in its preliminary stages, we argue that neurosymbolic methods are
general enough to be adapted and applied to other medical semantic segmentation
tasks.

</details>


### [135] [Closing the Safety Gap: Surgical Concept Erasure in Visual Autoregressive Models](https://arxiv.org/abs/2509.22400)
*Xinhao Zhong,Yimin Zhou,Zhiqi Zhang,Junhao Li,Yi Sun,Bin Chen,Shu-Tao Xia,Ke Xu*

Main category: cs.CV

TL;DR: 本文提出了适用于视觉自回归（VAR）模型的概念擦除框架（VARE）及其改进方法S-VARE，有效解决了文本到图像生成过程中的安全性问题。


<details>
  <summary>Details</summary>
Motivation: 随着VAR模型在文本到图像生成领域的广泛应用，其生成过程中的安全隐患（如不适当内容）引发关注。目前用于概念擦除的技术多针对扩散模型设计，难以应用于VAR模型。因此，研究者亟需找到适用于VAR的概念擦除方法，以提升生成内容的安全性。

Method: 作者提出VARE框架，通过引入辅助视觉标记（auxiliary visual tokens）来稳定擦除概念，减少微调强度。在此基础上，提出了S-VARE方法，利用过滤交叉熵损失精确识别并只调整不安全的视觉标记，同时加入语义保留损失以维持语义一致性，有效防止语言漂移和多样性下降等问题。

Result: 大量实验表明，该方法能实现精确（surgical）的概念擦除，并且能较好地保持图像生成质量，优于以往VAR相关安全方法。

Conclusion: VARE和S-VARE为VAR模型提供了一套稳定实用的概念擦除工具，在保障自动文本到图像生成模型内容安全的同时，尽量减少对生成多样性和保真度的影响，推动了VAR模型在安全生成方向的应用落地。

Abstract: The rapid progress of visual autoregressive (VAR) models has brought new
opportunities for text-to-image generation, but also heightened safety
concerns. Existing concept erasure techniques, primarily designed for diffusion
models, fail to generalize to VARs due to their next-scale token prediction
paradigm. In this paper, we first propose a novel VAR Erasure framework VARE
that enables stable concept erasure in VAR models by leveraging auxiliary
visual tokens to reduce fine-tuning intensity. Building upon this, we introduce
S-VARE, a novel and effective concept erasure method designed for VAR, which
incorporates a filtered cross entropy loss to precisely identify and minimally
adjust unsafe visual tokens, along with a preservation loss to maintain
semantic fidelity, addressing the issues such as language drift and reduced
diversity introduce by na\"ive fine-tuning. Extensive experiments demonstrate
that our approach achieves surgical concept erasure while preserving generation
quality, thereby closing the safety gap in autoregressive text-to-image
generation by earlier methods.

</details>


### [136] [RAU: Reference-based Anatomical Understanding with Vision Language Models](https://arxiv.org/abs/2509.22404)
*Yiwei Li,Yikang Liu,Jiaqi Guo,Lin Zhao,Zheyuan Zhang,Xiao Chen,Boris Mailhe,Ankush Mukherjee,Terrence Chen,Shanhui Sun*

Main category: cs.CV

TL;DR: 本文提出了一种结合视觉语言模型（VLM）和参考图像以提升医学图像中解剖结构识别与定位的新方法RAU，并证明其优于现有方法，且在跨数据集泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 医学图像中的自动报告生成、术中导航和器官定位等任务高度依赖于对解剖结构的理解，但受限于专家标注数据的稀缺。以参考图像辅助无标注目标的识别是一种有前景的解决策略。

Method: 提出了RAU框架，利用VLM模型通过参考和目标影像间的相对空间推理识别解剖区域，并结合SAM2模型进行高精度分割。方法在中等规模数据集上训练，通过视觉问答（VQA）和框定框预测等任务验证，在多个数据集上评估泛化能力。

Result: RAU在两个同分布和两个异分布医学影像数据集上表现优异，相较于SAM2微调基线能够实现更精准的分割和更可靠的定位，且泛化能力强。

Conclusion: RAU首次展示了VLM用于医学参考图像引导下解剖结构识别、定位和分割的能力，结果显示VLM驱动方法在自动化临床工作流程中具有重要应用潜力。

Abstract: Anatomical understanding through deep learning is critical for automatic
report generation, intra-operative navigation, and organ localization in
medical imaging; however, its progress is constrained by the scarcity of
expert-labeled data. A promising remedy is to leverage an annotated reference
image to guide the interpretation of an unlabeled target. Although recent
vision-language models (VLMs) exhibit non-trivial visual reasoning, their
reference-based understanding and fine-grained localization remain limited. We
introduce RAU, a framework for reference-based anatomical understanding with
VLMs. We first show that a VLM learns to identify anatomical regions through
relative spatial reasoning between reference and target images, trained on a
moderately sized dataset. We validate this capability through visual question
answering (VQA) and bounding box prediction. Next, we demonstrate that the
VLM-derived spatial cues can be seamlessly integrated with the fine-grained
segmentation capability of SAM2, enabling localization and pixel-level
segmentation of small anatomical regions, such as vessel segments. Across two
in-distribution and two out-of-distribution datasets, RAU consistently
outperforms a SAM2 fine-tuning baseline using the same memory setup, yielding
more accurate segmentations and more reliable localization. More importantly,
its strong generalization ability makes it scalable to out-of-distribution
datasets, a property crucial for medical image applications. To the best of our
knowledge, RAU is the first to explore the capability of VLMs for
reference-based identification, localization, and segmentation of anatomical
structures in medical images. Its promising performance highlights the
potential of VLM-driven approaches for anatomical understanding in automated
clinical workflows.

</details>


### [137] [FreqDebias: Towards Generalizable Deepfake Detection via Consistency-Driven Frequency Debiasing](https://arxiv.org/abs/2509.22412)
*Hossein Kashiani,Niloufar Alipour Talemi,Fatemeh Afghah*

Main category: cs.CV

TL;DR: 本文提出了一种新的频域模型偏差——频谱偏差，导致深度伪造检测器在新型伪造类型上泛化能力有限。针对这一问题，论文提出了FreqDebias框架，有效提升了检测器的跨域泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测器因训练数据有限，易学到特定频段的偏见（频谱偏差），导致难以应对未知的伪造类型，阻碍了实际应用。

Method: 提出FreqDebias框架，包括（1）Forgery Mixup（Fo-Mixup）频域数据增强，动态多样化训练样本的频率特征；（2）双重一致性正则化，利用类别激活图（CAM）实现局部一致性，利用vMF分布在高维球面空间实现全局一致性，促使模型在本地与全局层面都能学习一致的表征。

Result: 大量实验表明，FreqDebias显著提升了深度伪造检测的跨域泛化能力，并在跨域和域内设定下均优于现有最新方法。

Conclusion: 通过缓解频谱偏差，FreqDebias显著提升了深度伪造检测器对未知伪造类型的鲁棒性，在实际应用中更加可靠。

Abstract: Deepfake detectors often struggle to generalize to novel forgery types due to
biases learned from limited training data. In this paper, we identify a new
type of model bias in the frequency domain, termed spectral bias, where
detectors overly rely on specific frequency bands, restricting their ability to
generalize across unseen forgeries. To address this, we propose FreqDebias, a
frequency debiasing framework that mitigates spectral bias through two
complementary strategies. First, we introduce a novel Forgery Mixup (Fo-Mixup)
augmentation, which dynamically diversifies frequency characteristics of
training samples. Second, we incorporate a dual consistency regularization
(CR), which enforces both local consistency using class activation maps (CAMs)
and global consistency through a von Mises-Fisher (vMF) distribution on a
hyperspherical embedding space. This dual CR mitigates over-reliance on certain
frequency components by promoting consistent representation learning under both
local and global supervision. Extensive experiments show that FreqDebias
significantly enhances cross-domain generalization and outperforms
state-of-the-art methods in both cross-domain and in-domain settings.

</details>


### [138] [LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale Diffusion Transformer](https://arxiv.org/abs/2509.22414)
*Song Fei,Tian Ye,Lujia Wang,Lei Zhu*

Main category: cs.CV

TL;DR: 本文提出LucidFlux，一种无需图像字幕的大型扩散变换模型，用于通用图像复原，在保持图像语义和结构的同时，有效抑制伪影，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 通用图像复原任务需要恢复受未知混合损坏的图像，并保持其语义。但现有主流方法（如判别式复原器及基于UNet的扩散先验）容易出现过度平滑、伪影或偏移，此外依赖文本提示或图像字幕也存在延迟与不稳定问题。因此，作者希望无需字幕、避免扩展模型参数和文本辅助手段，提升鲁棒性与通用性。

Method: 提出LucidFlux框架：1）引入轻量级双分支条件器，分别从受损输入和初步复原代理中提取信号，锚定几何结构与抑制伪影；2）设计时步和层次自适应的调制调度，将不同条件信息高效引导到扩散模型各层，实现自粗到细的上下文复原；3）用SigLIP特征进行无字幕语义对齐；4）构建大规模的数据筛选管线，强化结构信息监督，而非单纯扩大参数或依赖字幕。

Result: 在合成和真实场景基准测试中，LucidFlux在恢复质量上持续优于开源和商业领先模型。消融实验验证了方法各组件的必要性。

Conclusion: LucidFlux证明，在通用图像复原中，相比绝对增加参数或依赖文本提示，合理决定何时、何地、以及用什么条件对扩散模型进行引导，更能实现稳健、高质量且无字幕的复原。

Abstract: Universal image restoration (UIR) aims to recover images degraded by unknown
mixtures while preserving semantics -- conditions under which discriminative
restorers and UNet-based diffusion priors often oversmooth, hallucinate, or
drift. We present LucidFlux, a caption-free UIR framework that adapts a large
diffusion transformer (Flux.1) without image captions. LucidFlux introduces a
lightweight dual-branch conditioner that injects signals from the degraded
input and a lightly restored proxy to respectively anchor geometry and suppress
artifacts. Then, a timestep- and layer-adaptive modulation schedule is designed
to route these cues across the backbone's hierarchy, in order to yield
coarse-to-fine and context-aware updates that protect the global structure
while recovering texture. After that, to avoid the latency and instability of
text prompts or MLLM captions, we enforce caption-free semantic alignment via
SigLIP features extracted from the proxy. A scalable curation pipeline further
filters large-scale data for structure-rich supervision. Across synthetic and
in-the-wild benchmarks, LucidFlux consistently outperforms strong open-source
and commercial baselines, and ablation studies verify the necessity of each
component. LucidFlux shows that, for large DiTs, when, where, and what to
condition on -- rather than adding parameters or relying on text prompts -- is
the governing lever for robust and caption-free universal image restoration in
the wild.

</details>


### [139] [Explaining multimodal LLMs via intra-modal token interactions](https://arxiv.org/abs/2509.22415)
*Jiawei Liang,Ruoyu Chen,Xianghao Jiao,Siyuan Liang,Shiming Liu,Qunli Zhang,Zheng Hu,Xiaochun Cao*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态大模型可解释性方法，能够更精细和真实地解释模型在视觉和文本任务中的决策机制。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大模型在视觉-语言任务上取得了巨大成功，其内部决策机制仍不透明。现有解释性方法大多只关注跨模态归因，忽视了模态内部的上下文依赖，导致解释结果零散和噪声多。作者旨在提升模型解释的完整性和准确性。

Method: 在视觉模态上，提出多尺度解释聚合（MSEA），通过多尺度输入汇聚归因信息，实现更协调、整体的视觉解释；在文本模态上，提出激活排序相关性（ARC）方法，根据候选预测的排序相关性过滤无关上下文，仅保留相关的语义信息。

Result: 在多个主流多模态大模型和基准数据集上，作者方法取得了比现有可解释性方法更细致、更真实的解释结果。

Conclusion: 通过增强模态内部的信息交互，作者提出的方法大幅提升了多模态大模型的可解释性，为理解模型决策机制提供了更有力的工具。

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable success
across diverse vision-language tasks, yet their internal decision-making
mechanisms remain insufficiently understood. Existing interpretability research
has primarily focused on cross-modal attribution, identifying which image
regions the model attends to during output generation. However, these
approaches often overlook intra-modal dependencies. In the visual modality,
attributing importance to isolated image patches ignores spatial context due to
limited receptive fields, resulting in fragmented and noisy explanations. In
the textual modality, reliance on preceding tokens introduces spurious
activations. Failing to effectively mitigate these interference compromises
attribution fidelity. To address these limitations, we propose enhancing
interpretability by leveraging intra-modal interaction. For the visual branch,
we introduce \textit{Multi-Scale Explanation Aggregation} (MSEA), which
aggregates attributions over multi-scale inputs to dynamically adjust receptive
fields, producing more holistic and spatially coherent visual explanations. For
the textual branch, we propose \textit{Activation Ranking Correlation} (ARC),
which measures the relevance of contextual tokens to the current token via
alignment of their top-$k$ prediction rankings. ARC leverages this relevance to
suppress spurious activations from irrelevant contexts while preserving
semantically coherent ones. Extensive experiments across state-of-the-art MLLMs
and benchmark datasets demonstrate that our approach consistently outperforms
existing interpretability methods, yielding more faithful and fine-grained
explanations of model behavior.

</details>


### [140] [U-MAN: U-Net with Multi-scale Adaptive KAN Network for Medical Image Segmentation](https://arxiv.org/abs/2509.22444)
*Bohan Huang,Qianyun Bao,Haoyuan Ma*

Main category: cs.CV

TL;DR: 本文提出了一种基于Kolmogorov-Arnold Network（KAN）的新型医学图像分割U-Net架构（U-MAN），在三个公开数据集上分割性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统U-Net架构在医学图像细粒度细节和精确边界的处理上存在困难，主要由于跳连方式简单导致特征表达存在语义鸿沟，以及深层缺乏多尺度特征提取能力。

Method: 作者提出了引入两大模块的新架构U-MAN：1）将原有跳连替换为渐进式注意力引导特征融合（PAGF），提升编码器与解码器特征的融合效果；2）采用多尺度自适应KAN（MAN）模块，实现多尺度特征的自适应处理。

Result: U-MAN在BUSI、GLAS和CVC三个公开医学图像分割数据集上性能优于当前主流方法，尤其在精确边界与保护细节方面有突出表现。

Conclusion: 多尺度自适应KAN和注意力引导特征融合能够显著提升医学图像分割的边界和细节分割效果，U-MAN为医学图像分割任务提供了有力的新方法。

Abstract: Medical image segmentation faces significant challenges in preserving
fine-grained details and precise boundaries due to complex anatomical
structures and pathological regions. These challenges primarily stem from two
key limitations of conventional U-Net architectures: (1) their simple skip
connections ignore the encoder-decoder semantic gap between various features,
and (2) they lack the capability for multi-scale feature extraction in deep
layers. To address these challenges, we propose the U-Net with Multi-scale
Adaptive KAN (U-MAN), a novel architecture that enhances the emerging
Kolmogorov-Arnold Network (KAN) with two specialized modules: Progressive
Attention-Guided Feature Fusion (PAGF) and the Multi-scale Adaptive KAN (MAN).
Our PAGF module replaces the simple skip connection, using attention to fuse
features from the encoder and decoder. The MAN module enables the network to
adaptively process features at multiple scales, improving its ability to
segment objects of various sizes. Experiments on three public datasets (BUSI,
GLAS, and CVC) show that U-MAN outperforms state-of-the-art methods,
particularly in defining accurate boundaries and preserving fine details.

</details>


### [141] [$γ$-Quant: Towards Learnable Quantization for Low-bit Pattern Recognition](https://arxiv.org/abs/2509.22448)
*Mishal Fatima,Shashank Agnihotri,Marius Bock,Kanchana Vaishnavi Gandikota,Kristof Van Laerhoven,Michael Moeller,Margret Keuper*

Main category: cs.CV

TL;DR: 该论文提出了一种新型的针对模式识别的非线性量化方法（γ-Quant），并证明该方法在只有4位数据位宽时在目标检测和可穿戴活动识别任务上表现与12位原始数据相当。


<details>
  <summary>Details</summary>
Motivation: 目前大部分视觉和识别模型都基于为人类感知设计的预处理数据或高位宽、规范化数据，这不仅对自动化分析未必最优，而且在可穿戴设备等能耗敏感场景下显著影响电池续航，因此需要低带宽、低功耗下的任务相关数据表达方案。

Method: 论文提出γ-Quant方法，通过针对具体任务学习一种非线性量化映射，降低输入数据的比特宽度。方法应用于目标检测（原始相机图像）和人体动作识别（可穿戴设备原始传感器数据）任务，研究4位量化下的表现。

Result: 实验表明，经过γ-Quant方法量化为4位的数据，模型的任务识别性能接近甚至等同于使用原始12位数据，显著降低了数据传输量与能源消耗。

Conclusion: γ-Quant方法能够在低带宽、低能耗的条件下，极大压缩输入数据量而不损失识别准确率，非常适合能量受限设备，如可穿戴设备。

Abstract: Most pattern recognition models are developed on pre-proce\-ssed data. In
computer vision, for instance, RGB images processed through image signal
processing (ISP) pipelines designed to cater to human perception are the most
frequent input to image analysis networks. However, many modern vision tasks
operate without a human in the loop, raising the question of whether such
pre-processing is optimal for automated analysis. Similarly, human activity
recognition (HAR) on body-worn sensor data commonly takes normalized
floating-point data arising from a high-bit analog-to-digital converter (ADC)
as an input, despite such an approach being highly inefficient in terms of data
transmission, significantly affecting the battery life of wearable devices. In
this work, we target low-bandwidth and energy-constrained settings where
sensors are limited to low-bit-depth capture. We propose $\gamma$-Quant,
i.e.~the task-specific learning of a non-linear quantization for pattern
recognition. We exemplify our approach on raw-image object detection as well as
HAR of wearable data, and demonstrate that raw data with a learnable
quantization using as few as 4-bits can perform on par with the use of raw
12-bit data. All code to reproduce our experiments is publicly available via
https://github.com/Mishalfatima/Gamma-Quant

</details>


### [142] [SSVIF: Self-Supervised Segmentation-Oriented Visible and Infrared Image Fusion](https://arxiv.org/abs/2509.22450)
*Zixian Zhao,Xingchen Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种自监督的可见光与红外图像融合（VIF）训练框架SSVIF，实现无需标注数据即可提升下游分割任务表现，性能优越并接近有监督方法。


<details>
  <summary>Details</summary>
Motivation: 应用导向的VIF方法需下游任务标注，数据采集成本高，限制了实际应用。为降低标签需求，提高VIF方法在分割等任务中的实用性，亟需无监督或自监督的高效训练方法。

Method: 引入自监督训练框架SSVIF，基于特征级与像素级分割的一致性，提出任务交叉分割一致性自监督任务，同时采用两阶段训练策略与动态权重调整，实现高效联合学习。

Result: 在公开数据集的大量实验表明，SSVIF无需标注数据即可超越传统VIF方法，并在分割表现上媲美有监督的应用导向VIF方法。

Conclusion: SSVIF有效提升了分割导向VIF模型在无标注数据下的性能，显著降低了数据采集与标注成本，推动了VIF实际应用的发展。

Abstract: Visible and infrared image fusion (VIF) has gained significant attention in
recent years due to its wide application in tasks such as scene segmentation
and object detection. VIF methods can be broadly classified into traditional
VIF methods and application-oriented VIF methods. Traditional methods focus
solely on improving the quality of fused images, while application-oriented VIF
methods additionally consider the performance of downstream tasks on fused
images by introducing task-specific loss terms during training. However,
compared to traditional methods, application-oriented VIF methods require
datasets labeled for downstream tasks (e.g., semantic segmentation or object
detection), making data acquisition labor-intensive and time-consuming. To
address this issue, we propose a self-supervised training framework for
segmentation-oriented VIF methods (SSVIF). Leveraging the consistency between
feature-level fusion-based segmentation and pixel-level fusion-based
segmentation, we introduce a novel self-supervised task-cross-segmentation
consistency-that enables the fusion model to learn high-level semantic features
without the supervision of segmentation labels. Additionally, we design a
two-stage training strategy and a dynamic weight adjustment method for
effective joint learning within our self-supervised framework. Extensive
experiments on public datasets demonstrate the effectiveness of our proposed
SSVIF. Remarkably, although trained only on unlabeled visible-infrared image
pairs, our SSVIF outperforms traditional VIF methods and rivals supervised
segmentation-oriented ones. Our code will be released upon acceptance.

</details>


### [143] [Bézier Meets Diffusion: Robust Generation Across Domains for Medical Image Segmentation](https://arxiv.org/abs/2509.22476)
*Chen Li,Meilong Xu,Xiaoling Hu,Weimin Lyu,Chao Chen*

Main category: cs.CV

TL;DR: 该论文提出一种结合Bézier曲线风格迁移与条件扩散模型的新型跨域医学图像生成方法，有效提升未标注目标域医学图像分割精度。


<details>
  <summary>Details</summary>
Motivation: 医学影像分析面临不同模态或设备间的大领域差距，这导致训练出的算法在新域上性能严重下降。如何利用标注源域和未标注目标域数据提升目标域模型表现，是重要且具有挑战性的问题。

Method: 方法分为两步：第一，提出Bézier曲线风格迁移策略，将源域图像风格迁移至目标域，以缩小源目标域之间的分布差异，并用迁移图像训练更鲁棒的分割模型；第二，利用该模型在目标域预测的伪标签，训练一个条件扩散模型（CDM）,以生成高质量、带标签的目标域图像。同时，设计了不确定性引导的分数匹配机制，缓解伪标签带来的噪声影响，提升扩散模型训练稳健性。

Result: 在多个公开医学影像数据集上进行大量实验，结果表明所提方法能生成真实的有标签目标域图像，有效扩充目标域数据并显著提升分割任务性能。

Conclusion: 结合Bézier曲线风格迁移和扩散模型的新框架，能够高效缓解医学影像领域差异带来的迁移难题，为跨域医学图像分割带来实用且有效的解决思路。

Abstract: Training robust learning algorithms across different medical imaging
modalities is challenging due to the large domain gap. Unsupervised domain
adaptation (UDA) mitigates this problem by using annotated images from the
source domain and unlabeled images from the target domain to train the deep
models. Existing approaches often rely on GAN-based style transfer, but these
methods struggle to capture cross-domain mappings in regions with high
variability. In this paper, we propose a unified framework, B\'ezier Meets
Diffusion, for cross-domain image generation. First, we introduce a
B\'ezier-curve-based style transfer strategy that effectively reduces the
domain gap between source and target domains. The transferred source images
enable the training of a more robust segmentation model across domains.
Thereafter, using pseudo-labels generated by this segmentation model on the
target domain, we train a conditional diffusion model (CDM) to synthesize
high-quality, labeled target-domain images. To mitigate the impact of noisy
pseudo-labels, we further develop an uncertainty-guided score matching method
that improves the robustness of CDM training. Extensive experiments on public
datasets demonstrate that our approach generates realistic labeled images,
significantly augmenting the target domain and improving segmentation
performance.

</details>


### [144] [PSTTS: A Plug-and-Play Token Selector for Efficient Event-based Spatio-temporal Representation Learning](https://arxiv.org/abs/2509.22481)
*Xiangmo Zhao,Nan Yang,Yang Wang,Zhanwen Liu*

Main category: cs.CV

TL;DR: 提出了一种新颖的事件数据时空token选择模块（PSTTS），显著降低计算量、提升速度且不损失准确性，适用于主流事件框架。


<details>
  <summary>Details</summary>
Motivation: 现有事件数据表示方法通过转为事件帧序列达到优异性能，但忽略了时空稀疏性和帧间运动冗余，导致不必要的计算开销。RGB视频的Token稀疏化方法直接迁移到事件数据并不适用，需有新的解决方案。

Method: 提出Progressive Spatio-Temporal Token Selection (PSTTS)，这是一个可插拔、无额外参数的模块。包括两个阶段：1)空间token净化，按帧筛选一致性，去除噪声和无效区域；2)时间token选择，比较相邻帧的运动模式相似性，筛除时间冗余信息。模块可集成至多种主流骨干网络。

Result: PSTTS在多个主流backbone和数据集（如HARDVS、DailyDVS-200、SeACT）上提升效率。在DailyDVS-200上FLOPs减少29-43.6%，FPS提升21.6-41.3%，且准确率基本不降低。

Conclusion: PSTTS可作为主流事件数据处理架构的高效、无损插件模块，显著提升推理效率，推动事件视觉领域模型的轻量化与实用化。

Abstract: Mainstream event-based spatio-temporal representation learning methods
typically process event streams by converting them into sequences of event
frames, achieving remarkable performance. However, they neglect the high
spatial sparsity and inter-frame motion redundancy inherent in event frame
sequences, leading to significant computational overhead. Existing token
sparsification methods for RGB videos rely on unreliable intermediate token
representations and neglect the influence of event noise, making them
ineffective for direct application to event data. In this paper, we propose
Progressive Spatio-Temporal Token Selection (PSTTS), a Plug-and-Play module for
event data without introducing any additional parameters. PSTTS exploits the
spatio-temporal distribution characteristics embedded in raw event data to
effectively identify and discard spatio-temporal redundant tokens, achieving an
optimal trade-off between accuracy and efficiency. Specifically, PSTTS consists
of two stages, Spatial Token Purification and Temporal Token Selection. Spatial
Token Purification discards noise and non-event regions by assessing the
spatio-temporal consistency of events within each event frame to prevent
interference with subsequent temporal redundancy evaluation. Temporal Token
Selection evaluates the motion pattern similarity between adjacent event
frames, precisely identifying and removing redundant temporal information. We
apply PSTTS to four representative backbones UniformerV2, VideoSwin, EVMamba,
and ExACT on the HARDVS, DailyDVS-200, and SeACT datasets. Experimental results
demonstrate that PSTTS achieves significant efficiency improvements.
Specifically, PSTTS reduces FLOPs by 29-43.6% and increases FPS by 21.6-41.3%
on the DailyDVS-200 dataset, while maintaining task accuracy. Our code will be
available.

</details>


### [145] [Group Critical-token Policy Optimization for Autoregressive Image Generation](https://arxiv.org/abs/2509.22485)
*Guohui Zhang,Hu Yu,Xiaoxiao Ma,JingHao Zhang,Yaning Pan,Mingde Yao,Jie Xiao,Linjiang Huang,Feng Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种新的图像生成强化学习优化方法GCPO，通过仅优化约30%的关键图像token，在多个基准任务上超过了以往对全部token均一优化的方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于可验证奖励的自回归视觉生成RLVR方法，对所有图像token一视同仁地优化，忽略了不同token对生成质量的贡献差异，导致训练效率和效果受限。

Method: 提出GCPO方法，从因果依赖、熵空间结构、token多样性三方面，动态识别对RLVR训练影响最大的关键token，并为这些token设计基于置信度散度的动态优势权重，实现有针对性的token级优化。

Result: GCPO在多个自回归和多模态模型的文本生成图像任务基准上评测，只用30%的token就能在多个指标上优于对所有token优化的GRPO方法。

Conclusion: GCPO方法有效提高了自回归视觉生成任务中的优化效率和结果质量，为RLVR相关研究提供了高效的token级优化思路。

Abstract: Recent studies have extended Reinforcement Learning with Verifiable Rewards
(RLVR) to autoregressive (AR) visual generation and achieved promising
progress. However, existing methods typically apply uniform optimization across
all image tokens, while the varying contributions of different image tokens for
RLVR's training remain unexplored. In fact, the key obstacle lies in how to
identify more critical image tokens during AR generation and implement
effective token-wise optimization for them. To tackle this challenge, we
propose $\textbf{G}$roup $\textbf{C}$ritical-token $\textbf{P}$olicy
$\textbf{O}$ptimization ($\textbf{GCPO}$), which facilitates effective policy
optimization on critical tokens. We identify the critical tokens in RLVR-based
AR generation from three perspectives, specifically: $\textbf{(1)}$ Causal
dependency: early tokens fundamentally determine the later tokens and final
image effect due to unidirectional dependency; $\textbf{(2)}$ Entropy-induced
spatial structure: tokens with high entropy gradients correspond to image
structure and bridges distinct visual regions; $\textbf{(3)}$ RLVR-focused
token diversity: tokens with low visual similarity across a group of sampled
images contribute to richer token-level diversity. For these identified
critical tokens, we further introduce a dynamic token-wise advantage weight to
encourage exploration, based on confidence divergence between the policy model
and reference model. By leveraging 30\% of the image tokens, GCPO achieves
better performance than GRPO with full tokens. Extensive experiments on
multiple text-to-image benchmarks for both AR models and unified multimodal
models demonstrate the effectiveness of GCPO for AR visual generation.

</details>


### [146] [Where MLLMs Attend and What They Rely On: Explaining Autoregressive Token Generation](https://arxiv.org/abs/2509.22496)
*Ruoyu Chen,Xiaoqing Guo,Kangwei Liu,Siyuan Liang,Shiming Liu,Qunli Zhang,Hua Zhang,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出EAGLE框架，用于解释多模态大语言模型（MLLMs）中生成的文本与视觉输入的关联，并提升解释性与可用性。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大模型在视觉与文本对齐上有很大进步，但具体某个生成内容有多大程度依赖视觉模态知之甚少，导致可解释性不足、难以诊断模型问题。

Method: 提出EAGLE框架，通过黑盒方式，将生成的token归因到图像的紧凑感知区域，并定量分析语言先验和感知证据对生成的影响。采用结合sufficiency（充分性）与necessity（必要性）的目标函数，在稀疏化的图像区域上贪婪搜索优化，高效实现归因。此外还进行模态敏感的分析，细粒度揭示token依赖的来源。

Result: 在多个开源MLLMs上实验，EAGLE在归因的真实性、视觉区域定位和幻觉诊断上均优于现有方法，同时显著节省GPU资源。

Conclusion: EAGLE在提升MLLMs模型可解释性、诊断等方面展现出高效且实用的能力，是推进该领域研究的有效工具。

Abstract: Multimodal large language models (MLLMs) have demonstrated remarkable
capabilities in aligning visual inputs with natural language outputs. Yet, the
extent to which generated tokens depend on visual modalities remains poorly
understood, limiting interpretability and reliability. In this work, we present
EAGLE, a lightweight black-box framework for explaining autoregressive token
generation in MLLMs. EAGLE attributes any selected tokens to compact perceptual
regions while quantifying the relative influence of language priors and
perceptual evidence. The framework introduces an objective function that
unifies sufficiency (insight score) and indispensability (necessity score),
optimized via greedy search over sparsified image regions for faithful and
efficient attribution. Beyond spatial attribution, EAGLE performs
modality-aware analysis that disentangles what tokens rely on, providing
fine-grained interpretability of model decisions. Extensive experiments across
open-source MLLMs show that EAGLE consistently outperforms existing methods in
faithfulness, localization, and hallucination diagnosis, while requiring
substantially less GPU memory. These results highlight its effectiveness and
practicality for advancing the interpretability of MLLMs. The code is available
at https://github.com/RuoyuChen10/EAGLE.

</details>


### [147] [Color Names in Vision-Language Models](https://arxiv.org/abs/2509.22524)
*Alexandra Gomez-Villa,Pablo Hernández-Cámara,Muhammad Atif Butt,Valero Laparra,Jesus Malo,Javier Vazquez-Corral*

Main category: cs.CV

TL;DR: 本文系统地评估了多种视觉-语言模型（VLMs）在颜色命名任务中的表现，发现这些模型对典型颜色表现良好，但在非典型颜色上表现较差，并受到训练语料语言分布和模型结构的影响。


<details>
  <summary>Details</summary>
Motivation: 随着视觉-语言模型在各类任务中的普及，研究其是否能像人类一样准确命名颜色，对于促进高效的人机交互十分重要。

Method: 本文复现了经典的颜色命名方法，基于957种颜色样本，系统评估了5种代表性VLM模型的颜色命名能力。此外，还进行了跨9种语言的分析及模型消融实验，考察不同模型架构和语料分布对结果的影响。

Result: VLMs在常见、典型颜色命名任务上表现优异，但在扩展的、非典型颜色集上准确率大幅下降。所有模型普遍出现了21个常用颜色词，呈现出两种策略：一类使用基础词汇，另一类偏好通过亮度修饰词扩展表达。跨语言分析发现训练语料极度偏向英语和中文，颜色命名决策主要受色调影响。消融研究发现，语言模型结构本身对颜色命名有显著影响，与视觉处理能力无关。

Conclusion: VLMs虽在典型颜色命名表现优秀，但其广泛性和多语言能力受限，受训练语料、不平衡和架构的影响明显。这提醒后续模型训练应更加关注多样性和跨语言表现，以增强模型实际应用价值。

Abstract: Color serves as a fundamental dimension of human visual perception and a
primary means of communicating about objects and scenes. As vision-language
models (VLMs) become increasingly prevalent, understanding whether they name
colors like humans is crucial for effective human-AI interaction. We present
the first systematic evaluation of color naming capabilities across VLMs,
replicating classic color naming methodologies using 957 color samples across
five representative models. Our results show that while VLMs achieve high
accuracy on prototypical colors from classical studies, performance drops
significantly on expanded, non-prototypical color sets. We identify 21 common
color terms that consistently emerge across all models, revealing two distinct
approaches: constrained models using predominantly basic terms versus expansive
models employing systematic lightness modifiers. Cross-linguistic analysis
across nine languages demonstrates severe training imbalances favoring English
and Chinese, with hue serving as the primary driver of color naming decisions.
Finally, ablation studies reveal that language model architecture significantly
influences color naming independent of visual processing capabilities.

</details>


### [148] [EfficientDepth: A Fast and Detail-Preserving Monocular Depth Estimation Model](https://arxiv.org/abs/2509.22527)
*Andrii Litvynchuk,Ivan Livinsky,Anand Ravi,Nima Kalantari,Andrii Tsarov*

Main category: cs.CV

TL;DR: 论文提出了一种名为EfficientDepth的单目深度估计算法，在保证高精度的同时大大提升了效率，适合边缘设备使用。


<details>
  <summary>Details</summary>
Motivation: 现有的单目深度估计算法在几何一致性、细节还原、真实世界复杂环境鲁棒性和轻量高效性方面存在局限，无法完全满足三维重建及新视点合成等实际应用需求。

Method: EfficientDepth结合了transformer架构与轻量级卷积解码器，并引入双模态密度头来预测更精细的深度图。训练时融合了标注合成、真实数据与伪标签数据，并采用多阶段优化提升几何一致性和细节。还创新性地在损失函数中引入基于LPIPS的目标，加强细节保留。

Result: EfficientDepth在多个实验和标准数据集上取得了与最新SOTA模型媲美甚至更优的深度估计性能，同时大幅降低了计算资源消耗。

Conclusion: EfficientDepth在保持精度的基础上，兼具高效性和对现实挑战的鲁棒性，是单目深度估计和轻量化部署的优选方案。

Abstract: Monocular depth estimation (MDE) plays a pivotal role in various computer
vision applications, such as robotics, augmented reality, and autonomous
driving. Despite recent advancements, existing methods often fail to meet key
requirements for 3D reconstruction and view synthesis, including geometric
consistency, fine details, robustness to real-world challenges like reflective
surfaces, and efficiency for edge devices. To address these challenges, we
introduce a novel MDE system, called EfficientDepth, which combines a
transformer architecture with a lightweight convolutional decoder, as well as a
bimodal density head that allows the network to estimate detailed depth maps.
We train our model on a combination of labeled synthetic and real images, as
well as pseudo-labeled real images, generated using a high-performing MDE
method. Furthermore, we employ a multi-stage optimization strategy to improve
training efficiency and produce models that emphasize geometric consistency and
fine detail. Finally, in addition to commonly used objectives, we introduce a
loss function based on LPIPS to encourage the network to produce detailed depth
maps. Experimental results demonstrate that EfficientDepth achieves performance
comparable to or better than existing state-of-the-art models, with
significantly reduced computational resources.

</details>


### [149] [Category Discovery: An Open-World Perspective](https://arxiv.org/abs/2509.22542)
*Zhenqi He,Yuanpei Liu,Kai Han*

Main category: cs.CV

TL;DR: 本文综述了类别发现（Category Discovery）这一开放世界学习任务，包括基础及衍生设定，并对各类方法进行了详细分析与对比。


<details>
  <summary>Details</summary>
Motivation: 随着开放世界和未标注数据场景的普及，如何自动识别和划分未见过的新类别成为重要问题。已有工作呈快速发展态势，需要系统梳理和分析以指导未来研究。

Method: 文章首先按照新类别发现（NCD）、广义类别发现（GCD）等基础设定及其衍生变体来分类文献。逐一分析方法中的表示学习、标签分配、类别数量估计等关键组成部分。同时，对各种方法在不同设定下进行基准测试。

Result: 大规模预训练模型、层次结构和辅助信息、课程式训练对类别发现均有效；而标签分配设计、类别数估计和复杂多目标任务中仍存显著挑战。

Conclusion: 本文为类别发现领域总结了现状、归纳了有效策略与存在难题，并提出未来增强方法设计、处理复杂场景等值得关注的研究方向。还提供了持续更新的文献库链接。

Abstract: Category discovery (CD) is an emerging open-world learning task, which aims
at automatically categorizing unlabelled data containing instances from unseen
classes, given some labelled data from seen classes. This task has attracted
significant attention over the years and leads to a rich body of literature
trying to address the problem from different perspectives. In this survey, we
provide a comprehensive review of the literature, and offer detailed analysis
and in-depth discussion on different methods. Firstly, we introduce a taxonomy
for the literature by considering two base settings, namely novel category
discovery (NCD) and generalized category discovery (GCD), and several derived
settings that are designed to address the extra challenges in different
real-world application scenarios, including continual category discovery,
skewed data distribution, federated category discovery, etc. Secondly, for each
setting, we offer a detailed analysis of the methods encompassing three
fundamental components, representation learning, label assignment, and
estimation of class number. Thirdly, we benchmark all the methods and distill
key insights showing that large-scale pretrained backbones, hierarchical and
auxiliary cues, and curriculum-style training are all beneficial for category
discovery, while challenges remain in the design of label assignment, the
estimation of class numbers, and scaling to complex multi-object
scenarios.Finally, we discuss the key insights from the literature so far and
point out promising future research directions. We compile a living survey of
the category discovery literature at
\href{https://github.com/Visual-AI/Category-Discovery}{https://github.com/Visual-AI/Category-Discovery}.

</details>


### [150] [HyCoVAD: A Hybrid SSL-LLM Model for Complex Video Anomaly Detection](https://arxiv.org/abs/2509.22544)
*Mohammad Mahdi Hemmatyar,Mahdi Jafari,Mohammad Amin Yousefi,Mohammad Reza Nemati,Mobin Azadani,Hamid Reza Rastad,Amirmohammad Akbari*

Main category: cs.CV

TL;DR: 本文提出了一种结合自监督学习（SSL）和大语言模型（LLM）的复杂视频异常检测新方法HyCoVAD，显著提升了异常检测准确率，并优化了计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前视频异常检测（VAD）面临的主要难题是识别由多个实体间复杂关系和时序依赖共同决定的异常事件。现有SSL方法擅长捕捉低层次模式却难以理解语义，而LLM虽能提供语境推理但在逐帧分析时计算代价高、定位精细度不足。因此，需要整合两者优势。

Method: 提出HyCoVAD系统：首先用基于nnFormer主干（transformer用于图像分割）的多任务SSL模块，从视频帧学习发现可疑帧；随后将这些帧输入LLM，通过结构化、规则驱动推理，进行语义验证。

Result: 在ComplexVAD数据集实验中，HyCoVAD实现了72.5%的帧级AUC，比现有方法高出12.5%；同时大幅减少了LLM的计算量。

Conclusion: 结合SSL和LLM能提升复杂视频异常检测的表现与效率，相关工具和标准开放，有助于该领域未来研究。

Abstract: Video anomaly detection (VAD) is crucial for intelligent surveillance, but a
significant challenge lies in identifying complex anomalies, which are events
defined by intricate relationships and temporal dependencies among multiple
entities rather than by isolated actions. While self-supervised learning (SSL)
methods effectively model low-level spatiotemporal patterns, they often
struggle to grasp the semantic meaning of these interactions. Conversely, large
language models (LLMs) offer powerful contextual reasoning but are
computationally expensive for frame-by-frame analysis and lack fine-grained
spatial localization. We introduce HyCoVAD, Hybrid Complex Video Anomaly
Detection, a hybrid SSL-LLM model that combines a multi-task SSL temporal
analyzer with LLM validator. The SSL module is built upon an nnFormer backbone
which is a transformer-based model for image segmentation. It is trained with
multiple proxy tasks, learns from video frames to identify those suspected of
anomaly. The selected frames are then forwarded to the LLM, which enriches the
analysis with semantic context by applying structured, rule-based reasoning to
validate the presence of anomalies. Experiments on the challenging ComplexVAD
dataset show that HyCoVAD achieves a 72.5% frame-level AUC, outperforming
existing baselines by 12.5% while reducing LLM computation. We release our
interaction anomaly taxonomy, adaptive thresholding protocol, and code to
facilitate future research in complex VAD scenarios.

</details>


### [151] [SpikeMatch: Semi-Supervised Learning with Temporal Dynamics of Spiking Neural Networks](https://arxiv.org/abs/2509.22581)
*Jini Yang,Beomseok Oh,Seungryong Kim,Sunok Kim*

Main category: cs.CV

TL;DR: 本文提出了SpikeMatch，这是首个针对脉冲神经网络（SNN）的SSL（半监督学习）框架，能通过利用SNN的泄露因子生成多样化伪标签，并在多个标准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管SNN因其生物合理性和能效受到关注，但其半监督学习方法较少，被人工神经网络（ANN）相关技术远远甩在后面，因此需要专门为SNN设计高效的SSL方法。

Method: 作者提出SpikeMatch，基于SNN的时间动态特性，通过模型泄露因子和协同训练框架，对弱增强的无标签样本生成伪标签，再用这些高置信度伪标签训练强增强样本，通过多轮预测一致性方法缓解伪标签训练中的确认偏差。

Result: 实验结果表明，SpikeMatch在多个标准数据集上，比现有迁移到SNN的SSL方法表现更好。

Conclusion: SpikeMatch为SNN提供了创新的半监督学习方案，通过充分利用SNN特性，有效提升了低标签条件下的模型性能。

Abstract: Spiking neural networks (SNNs) have recently been attracting significant
attention for their biological plausibility and energy efficiency, but
semi-supervised learning (SSL) methods for SNN-based models remain
underexplored compared to those for artificial neural networks (ANNs). In this
paper, we introduce SpikeMatch, the first SSL framework for SNNs that leverages
the temporal dynamics through the leakage factor of SNNs for diverse
pseudo-labeling within a co-training framework. By utilizing agreement among
multiple predictions from a single SNN, SpikeMatch generates reliable
pseudo-labels from weakly-augmented unlabeled samples to train on
strongly-augmented ones, effectively mitigating confirmation bias by capturing
discriminative features with limited labels. Experiments show that SpikeMatch
outperforms existing SSL methods adapted to SNN backbones across various
standard benchmarks.

</details>


### [152] [LongLive: Real-time Interactive Long Video Generation](https://arxiv.org/abs/2509.22622)
*Shuai Yang,Wei Huang,Ruihang Chu,Yicheng Xiao,Yuyang Zhao,Xianbang Wang,Muyang Li,Enze Xie,Yingcong Chen,Yao Lu,Song Han,Yukang Chen*

Main category: cs.CV

TL;DR: LongLive是一种用于实时、交互式长视频生成的帧级自回归（AR）框架，实现了高效且高质量的视频生成，支持流式交互和长视频一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的长视频生成方法在质量和效率之间难以兼顾，尤其是在交互性和一致性方面存在重大挑战。特别是需要支持用户实时引导内容生成，并解决长视频训练中的记忆和推理不一致等问题。

Method: LongLive采用帧级因果AR设计，结合KV-recache机制以便在收到新提示词时平滑切换；引入流式长视频训练（train-long-test-long）使训练和推理保持一致性；采用短窗口注意力和帧级attention sink（帧汇）来保障长时范围一致性并提升推理速度。

Result: 只需32个GPU日即可将1.3B参数的短视频模型微调到可生成分钟级长视频；推理阶段在单块NVIDIA H100上达到20.7FPS，支持最长240秒长视频，VBench评测短视频和长视频均表现优异。还支持INT8低精度推理，画质损失极小。

Conclusion: LongLive在保证生成质量的同时极大提升了效率和交互性，实现了流畅的长视频生成和动态内容编辑，为实际应用提供了有力支撑。

Abstract: We present LongLive, a frame-level autoregressive (AR) framework for
real-time and interactive long video generation. Long video generation presents
challenges in both efficiency and quality. Diffusion and Diffusion-Forcing
models can produce high-quality videos but suffer from low efficiency due to
bidirectional attention. Causal attention AR models support KV caching for
faster inference, but often degrade in quality on long videos due to memory
challenges during long-video training. In addition, beyond static prompt-based
generation, interactive capabilities, such as streaming prompt inputs, are
critical for dynamic content creation, enabling users to guide narratives in
real time. This interactive requirement significantly increases complexity,
especially in ensuring visual consistency and semantic coherence during prompt
transitions. To address these challenges, LongLive adopts a causal, frame-level
AR design that integrates a KV-recache mechanism that refreshes cached states
with new prompts for smooth, adherent switches; streaming long tuning to enable
long video training and to align training and inference (train-long-test-long);
and short window attention paired with a frame-level attention sink, shorten as
frame sink, preserving long-range consistency while enabling faster generation.
With these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model
to minute-long generation in just 32 GPU-days. At inference, LongLive sustains
20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both
short and long videos. LongLive supports up to 240-second videos on a single
H100 GPU. LongLive further supports INT8-quantized inference with only marginal
quality loss.

</details>


### [153] [SPARK: Synergistic Policy And Reward Co-Evolving Framework](https://arxiv.org/abs/2509.22624)
*Ziyu Liu,Yuhang Zang,Shengyuan Ding,Yuhang Cao,Xiaoyi Dong,Haodong Duan,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种高效、稳定的RL后训练框架SPARK，通过回收并利用模型自身生成的数据，实现策略和奖励模型的协同进化，有效提升大模型的表现。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF成本高，且存在奖励-策略不匹配问题；RLVR虽然客观但监督被浪费，回滚和正确信号未充分利用。需要新的高效方法提升后训练效果并降低代价。

Method: 提出SPARK框架，不丢弃rollout和正确性数据，结合点对奖励、对比和条件反思等多目标，实现奖励模型与策略模型的协同进化，模型自评分自身输出，无需额外奖励模型或人工偏好数据。

Result: 在多个LLM与LVLM模型及多项推理、奖励和通用基准测试上取得显著性能提升。例如SPARK-VL-7B在7个推理基准上提升9.7%，在2个奖励基准上提升12.1%，在8个通用基准上提升1.5%。

Conclusion: SPARK框架提升了监督利用效率，降低成本，实现奖励和策略协同优化，展现出强大的鲁棒性与泛化能力。

Abstract: Recent Large Language Models (LLMs) and Large Vision-Language Models (LVLMs)
increasingly use Reinforcement Learning (RL) for post-pretraining, such as RL
with Verifiable Rewards (RLVR) for objective tasks and RL from Human Feedback
(RLHF) for subjective tasks. However, RLHF incurs high costs and potential
reward-policy mismatch due to reliance on human preferences, while RLVR still
wastes supervision by discarding rollouts and correctness signals after each
update. To address these challenges, we introduce the Synergistic Policy And
Reward Co-Evolving Framework (SPARK), an efficient, on-policy, and stable
method that builds on RLVR. Instead of discarding rollouts and correctness
data, SPARK recycles this valuable information to simultaneously train the
model itself as a generative reward model. This auxiliary training uses a mix
of objectives, such as pointwise reward score, pairwise comparison, and
evaluation conditioned on further-reflection responses, to teach the model to
evaluate and improve its own responses. Our process eliminates the need for a
separate reward model and costly human preference data. SPARK creates a
positive co-evolving feedback loop: improved reward accuracy yields better
policy gradients, which in turn produce higher-quality rollouts that further
refine the reward model. Our unified framework supports test-time scaling via
self-reflection without external reward models and their associated costs. We
show that SPARK achieves significant performance gains on multiple LLM and LVLM
models and multiple reasoning, reward models, and general benchmarks. For
example, SPARK-VL-7B achieves an average 9.7% gain on 7 reasoning benchmarks,
12.1% on 2 reward benchmarks, and 1.5% on 8 general benchmarks over the
baselines, demonstrating robustness and broad generalization.

</details>


### [154] [CCNeXt: An Effective Self-Supervised Stereo Depth Estimation Approach](https://arxiv.org/abs/2509.22627)
*Alexandre Lopes,Roberto Souza,Helio Pedrini*

Main category: cs.CV

TL;DR: 本文提出了一种自监督式的卷积网络CCNeXt，用于双目立体图像对的深度估计，在保证计算效率的同时，显著优于现有的CNN和ViT方法。


<details>
  <summary>Details</summary>
Motivation: 在机器人、自动驾驶和增强现实等应用中，深度估计十分关键。然而，当前主流方法存在计算资源消耗大、真实深度标注数据难以获取等问题。自监督方法成为解决这一难题的热门方向。

Method: 提出CCNeXt架构，结合了现代CNN特征提取器和全新的窗口化极线交叉注意力模块，用于编码阶段，并对深度估计算法的解码器进行了全面设计优化。采用自监督学习方法，无需真实深度标签进行训练。

Result: 在KITTI Eigen Split测试集上，CCNeXt取得了与最佳模型相当的指标表现，并且推理速度高出10.18倍；在KITTI Eigen Split Improved Ground Truth和Driving Stereo数据集上，表现全面优于近期最新方法。

Conclusion: CCNeXt在自监督立体深度估计领域充分实现了精度和效率的兼顾，具备较大的应用潜力。相关代码已公开，便于学术社区复现和进一步研究。

Abstract: Depth Estimation plays a crucial role in recent applications in robotics,
autonomous vehicles, and augmented reality. These scenarios commonly operate
under constraints imposed by computational power. Stereo image pairs offer an
effective solution for depth estimation since it only needs to estimate the
disparity of pixels in image pairs to determine the depth in a known rectified
system. Due to the difficulty in acquiring reliable ground-truth depth data
across diverse scenarios, self-supervised techniques emerge as a solution,
particularly when large unlabeled datasets are available. We propose a novel
self-supervised convolutional approach that outperforms existing
state-of-the-art Convolutional Neural Networks (CNNs) and Vision Transformers
(ViTs) while balancing computational cost. The proposed CCNeXt architecture
employs a modern CNN feature extractor with a novel windowed epipolar
cross-attention module in the encoder, complemented by a comprehensive redesign
of the depth estimation decoder. Our experiments demonstrate that CCNeXt
achieves competitive metrics on the KITTI Eigen Split test data while being
10.18$\times$ faster than the current best model and achieves state-of-the-art
results in all metrics in the KITTI Eigen Split Improved Ground Truth and
Driving Stereo datasets when compared to recently proposed techniques. To
ensure complete reproducibility, our project is accessible at
\href{https://github.com/alelopes/CCNext}{\texttt{https://github.com/alelopes/CCNext}}.

</details>


### [155] [UML-CoT: Structured Reasoning and Planning with Unified Modeling Language for Robotic Room Cleaning](https://arxiv.org/abs/2509.22628)
*Hongyu Chen,Guangrun Wang*

Main category: cs.CV

TL;DR: 本文提出了UML-CoT，一种基于统一建模语言（UML）的结构化推理与规划方法，用于提升大语言模型在推理任务中的可解释性和可执行性，并在复杂场景下优于传统链式思维提示。


<details>
  <summary>Details</summary>
Motivation: 传统的链式思维（CoT）提示和已知的结构化逻辑图均存在可解释性和可执行性受限的问题，例如仅能表达低阶关系、缺乏抽象和行为继承以及缺乏标准的序列和条件规划语义。为解决这些局限，作者希望引入更强的结构化表达能力，以便让推理和计划更符合实际应用中的需求。

Method: 作者提出UML-CoT方法，融合UML类图（表达对象的语义和组合）和UML活动图（建模过程控制流），并结合监督微调与群组相对策略优化（GRPO）三阶段训练流程，包括基于纯答案数据的奖励学习。

Result: 在新构建的MRoom-30k复杂打扫房间基准任务中，UML-CoT在解释性、规划连贯性和执行成功率上均优于采用非结构化文本的CoT方法。

Conclusion: 研究表明UML是一种更具表达性和可操作性的结构化推理形式，能显著提升大语言模型在需要推理和行动执行的场景下的性能。

Abstract: Chain-of-Thought (CoT) prompting improves reasoning in large language models
(LLMs), but its reliance on unstructured text limits interpretability and
executability in embodied tasks. Prior work has explored structured CoTs using
scene or logic graphs, yet these remain fundamentally limited: they model only
low-order relations, lack constructs like inheritance or behavioral
abstraction, and provide no standardized semantics for sequential or
conditional planning. We propose UML-CoT, a structured reasoning and planning
framework that leverages Unified Modeling Language (UML) to generate symbolic
CoTs and executable action plans. UML class diagrams capture compositional
object semantics, while activity diagrams model procedural control flow. Our
three-stage training pipeline combines supervised fine-tuning with Group
Relative Policy Optimization (GRPO), including reward learning from answer-only
data. We evaluate UML-CoT on MRoom-30k, a new benchmark of cluttered
room-cleaning scenarios. UML-CoT outperforms unstructured CoTs in
interpretability, planning coherence, and execution success, highlighting UML
as a more expressive and actionable structured reasoning formalism.

</details>


### [156] [Training-Free Synthetic Data Generation with Dual IP-Adapter Guidance](https://arxiv.org/abs/2509.22635)
*Luc Boudier,Loris Manganelli,Eleftherios Tsonis,Nicolas Dufour,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: DIPSY是一种无需训练的新方法，使用IP-Adapter进行图像生成，实现少样本图像分类的高性能提升，且无需外部数据和模型微调。


<details>
  <summary>Details</summary>
Motivation: 现有少样本图像分类受限于标注样本少，尽管扩展数据集的生成式方法已提出，但普遍依赖复杂的微调或外部数据处理，因此迫切需要无需模型训练或外部资源的高效解决方案。

Method: 提出DIPSY方法，利用IP-Adapter实现基于正负条件的图像翻译，创新地：1）扩展了classifier-free guidance，实现对正负图像条件的独立调控；2）设计基于类别相似性的采样方法获取有效对比样本；3）整体流程无需微调模型或借助外部生成标签与图像筛选工具。

Result: 在十个基准数据集上的实验表明，DIPSY无须适配生成式模型或外部工具，即可达到或超过现有最新方法的性能，尤其在细粒度分类任务中表现突出。

Conclusion: 双重图像提示配合正负引导机制，可高效生成区分类别特征的图像，无训练、无外部依赖也能实现少样本分类高表现，展现生成式模型辅助分类的新方向。

Abstract: Few-shot image classification remains challenging due to the limited
availability of labeled examples. Recent approaches have explored generating
synthetic training data using text-to-image diffusion models, but often require
extensive model fine-tuning or external information sources. We present a novel
training-free approach, called DIPSY, that leverages IP-Adapter for
image-to-image translation to generate highly discriminative synthetic images
using only the available few-shot examples. DIPSY introduces three key
innovations: (1) an extended classifier-free guidance scheme that enables
independent control over positive and negative image conditioning; (2) a class
similarity-based sampling strategy that identifies effective contrastive
examples; and (3) a simple yet effective pipeline that requires no model
fine-tuning or external captioning and filtering. Experiments across ten
benchmark datasets demonstrate that our approach achieves state-of-the-art or
comparable performance, while eliminating the need for generative model
adaptation or reliance on external tools for caption generation and image
filtering. Our results highlight the effectiveness of leveraging dual image
prompting with positive-negative guidance for generating class-discriminative
features, particularly for fine-grained classification tasks.

</details>


### [157] [Scale-Wise VAR is Secretly Discrete Diffusion](https://arxiv.org/abs/2509.22636)
*Amandeep Kumar,Nithin Gopalakrishnan Nair,Vishal M. Patel*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的视角，将视觉自回归生成（VAR）模型与离散扩散模型进行了理论上的统一，并展示了这种视角带来的效率和生成质量提升。


<details>
  <summary>Details</summary>
Motivation: 近年来，自回归（AR）transformer在视觉生成任务中表现出色，部分VAR方法甚至超越了扩散模型。然而，目前两者在理论上还未建立清晰的联系。作者希望深入挖掘两者内在的联系，为进一步性能提升和效率优化提供理论基础。

Method: 作者发现，给VAR模型加入马尔可夫式注意力掩码后，该模型在数学上等价于一个离散扩散过程。基于这一视角，作者提出Scalable Visual Refinement with Discrete Diffusion（SRDD），并结合扩散模型的迭代精炼等优点，简化和优化了自回归生成的架构。

Result: 实验证明，基于这种扩散视角改造后的VAR能够加快收敛速度，降低推理成本，并在零样本重建等任务上表现更好。在多个数据集上的测试都显示，模型效率和生成质量均得到提升。

Conclusion: 将VAR与离散扩散在理论上的统一不仅加深了二者的理解，也为视觉生成模型的进一步优化提供了新方向。改进后的模型在效率和性能上表现优异，有望推动该领域的发展。

Abstract: Autoregressive (AR) transformers have emerged as a powerful paradigm for
visual generation, largely due to their scalability, computational efficiency
and unified architecture with language and vision. Among them, next scale
prediction Visual Autoregressive Generation (VAR) has recently demonstrated
remarkable performance, even surpassing diffusion-based models. In this work,
we revisit VAR and uncover a theoretical insight: when equipped with a
Markovian attention mask, VAR is mathematically equivalent to a discrete
diffusion. We term this reinterpretation as Scalable Visual Refinement with
Discrete Diffusion (SRDD), establishing a principled bridge between AR
transformers and diffusion models. Leveraging this new perspective, we show how
one can directly import the advantages of diffusion such as iterative
refinement and reduce architectural inefficiencies into VAR, yielding faster
convergence, lower inference cost, and improved zero-shot reconstruction.
Across multiple datasets, we show that the diffusion based perspective of VAR
leads to consistent gains in efficiency and generation.

</details>


### [158] [Hierarchical Representation Matching for CLIP-based Class-Incremental Learning](https://arxiv.org/abs/2509.22645)
*Zhen-Hao Wen,Yan Wang,Ji Feng,Han-Jia Ye,De-Chuan Zhan,Da-Wei Zhou*

Main category: cs.CV

TL;DR: 该论文提出了一种增强版的类增量学习方法，结合了多层语义特征匹配与大语言模型生成的层级描述，实现了对CLIP模型的显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有类增量学习方法及其在CLIP等视觉-语言模型中的应用，大多仅使用简单的文本模板（如“a photo of a [CLASS]”），忽略了视觉概念的层次性结构，也没有充分利用深层特征中的多层次信息。这样会影响模型对不同粒度类的区分能力，特别是在细粒度辨别和渐进学习任务中会出现遗忘。

Method: 作者提出了HERMAN方法，通过大语言模型递归生成具有判别性的层级文本描述，增强语义空间，然后将这些描述与CLIP的多层表示相匹配。为不同粒度的分类任务自适应地选择和路由相应层级的描述，实现精细区分以及缓解增量学习中的遗忘现象。

Result: 在多个主流数据集增量学习基准上，HERMAN方法都取得了优于现有方法的表现，尤其在细粒度区分和抗遗忘能力方面表现突出。

Conclusion: 引入层级描述与多层特征匹配可以极大提升CLIP类增量学习表现，为持续学习和视觉-语言模型的结合提供了新思路。

Abstract: Class-Incremental Learning (CIL) aims to endow models with the ability to
continuously adapt to evolving data streams. Recent advances in pre-trained
vision-language models (e.g., CLIP) provide a powerful foundation for this
task. However, existing approaches often rely on simplistic templates, such as
"a photo of a [CLASS]", which overlook the hierarchical nature of visual
concepts. For example, recognizing "cat" versus "car" depends on coarse-grained
cues, while distinguishing "cat" from "lion" requires fine-grained details.
Similarly, the current feature mapping in CLIP relies solely on the
representation from the last layer, neglecting the hierarchical information
contained in earlier layers. In this work, we introduce HiErarchical
Representation MAtchiNg (HERMAN) for CLIP-based CIL. Our approach leverages
LLMs to recursively generate discriminative textual descriptors, thereby
augmenting the semantic space with explicit hierarchical cues. These
descriptors are matched to different levels of the semantic hierarchy and
adaptively routed based on task-specific requirements, enabling precise
discrimination while alleviating catastrophic forgetting in incremental tasks.
Extensive experiments on multiple benchmarks demonstrate that our method
consistently achieves state-of-the-art performance.

</details>


### [159] [RefAM: Attention Magnets for Zero-Shot Referral Segmentation](https://arxiv.org/abs/2509.22650)
*Anna Kukleva,Enis Simsar,Alessio Tonioni,Muhammad Ferjad Naeem,Federico Tombari,Jan Eric Lenssen,Bernt Schiele*

Main category: cs.CV

TL;DR: 本论文提出了一种无需额外训练或架构修改，直接利用扩散模型Transformer中的注意力特征用于指代分割的新方法，并在图像和视频的视觉-语言基准上实现了新的零样本分割效果。


<details>
  <summary>Details</summary>
Motivation: 现有指代分割方法往往依赖于微调或多模型组合，导致结构复杂且训练成本高。而扩散模型本身蕴含丰富语义信息，有望成为通用特征提取器，但如何直接用于下游任务尚未充分探索。

Method: 作者发现扩散模型Transformer中的注意力分数可以有效包含分割信息，并针对注意力机制的停用词吸附和全局注意力汇聚（GAS）等现象，提出过滤和再分配注意力的方法，包括：过滤停用词累积注意力、压制或重定向GAS、将停用词分割背景激活以获得更精确的热图，在此基础上构建了RefAM框架，实现免训练的语义分割。

Result: RefAM方法在图像和视频的零样本指代分割基准上，不经微调和架构修改即可优于现有SOTA方法。实验显示其注意力热图更尖锐、定位更精确。

Conclusion: 扩散模型的注意力机制若加以合理利用，在无需任何模型修改和再训练的情况下即可以实现精准的跨模态分割，推动了通用视觉-语言理解任务的轻量级实现。

Abstract: Most existing approaches to referring segmentation achieve strong performance
only through fine-tuning or by composing multiple pre-trained models, often at
the cost of additional training and architectural modifications. Meanwhile,
large-scale generative diffusion models encode rich semantic information,
making them attractive as general-purpose feature extractors. In this work, we
introduce a new method that directly exploits features, attention scores, from
diffusion transformers for downstream tasks, requiring neither architectural
modifications nor additional training. To systematically evaluate these
features, we extend benchmarks with vision-language grounding tasks spanning
both images and videos. Our key insight is that stop words act as attention
magnets: they accumulate surplus attention and can be filtered to reduce noise.
Moreover, we identify global attention sinks (GAS) emerging in deeper layers
and show that they can be safely suppressed or redirected onto auxiliary
tokens, leading to sharper and more accurate grounding maps. We further propose
an attention redistribution strategy, where appended stop words partition
background activations into smaller clusters, yielding sharper and more
localized heatmaps. Building on these findings, we develop RefAM, a simple
training-free grounding framework that combines cross-attention maps, GAS
handling, and redistribution. Across zero-shot referring image and video
segmentation benchmarks, our approach consistently outperforms prior methods,
establishing a new state of the art without fine-tuning or additional
components.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [160] [A Novel Differential Feature Learning for Effective Hallucination Detection and Classification](https://arxiv.org/abs/2509.21357)
*Wenkai Wang,Vincent Lee,Yizhen Zheng*

Main category: cs.CL

TL;DR: 本论文提出通过特征稀疏性和层次分布分析，实现对大语言模型幻觉（hallucination）信号的高效检测，并大幅提升相关任务的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常输出偏离真实事实的信息（幻觉），但现有研究尚未明确幻觉信号在各层的具体分布位置，限制了高效检测方法的发展。

Method: 提出一种双模型架构，包括Projected Fusion (PF)块自适应整合多层特征，以及Differential Feature Learning (DFL)机制，通过并行编码器对比相同输入下的互补表示，找出区分类幻觉与真实内容的特征。

Result: 实验证明幻觉信号集中在极为稀疏的特征子集；在HaluEval数据集的问答和对话任务上检测准确率显著提升。进一步发现浅层特征多样，深层特征则高度聚合，仅用1%特征维度即可基本保持检测效果。

Conclusion: 幻觉信号较此前设想更为集中，利用此特性可设计高效的检测系统，降低推理计算成本并维持检测准确性。

Abstract: Large language model hallucination represents a critical challenge where
outputs deviate from factual accuracy due to distributional biases in training
data. While recent investigations establish that specific hidden layers exhibit
differences between hallucinatory and factual content, the precise localization
of hallucination signals within layers remains unclear, limiting the
development of efficient detection methods. We propose a dual-model
architecture integrating a Projected Fusion (PF) block for adaptive inter-layer
feature weighting and a Differential Feature Learning (DFL) mechanism that
identifies discriminative features by computing differences between parallel
encoders learning complementary representations from identical inputs. Through
systematic experiments across HaluEval's question answering, dialogue, and
summarization datasets, we demonstrate that hallucination signals concentrate
in highly sparse feature subsets, achieving significant accuracy improvements
on question answering and dialogue tasks. Notably, our analysis reveals a
hierarchical "funnel pattern" where shallow layers exhibit high feature
diversity while deep layers demonstrate concentrated usage, enabling detection
performance to be maintained with minimal degradation using only 1\% of feature
dimensions. These findings suggest that hallucination signals are more
concentrated than previously assumed, offering a pathway toward computationally
efficient detection systems that could reduce inference costs while maintaining
accuracy.

</details>


### [161] [Influence Guided Context Selection for Effective Retrieval-Augmented Generation](https://arxiv.org/abs/2509.21359)
*Jiale Deng,Yanyan Shen,Ziyuan Pei,Youmin Chen,Linpeng Huang*

Main category: cs.CL

TL;DR: 本文提出了一种新的RAG（检索增强生成）上下文选择方法，通过量化移除每个上下文后对生成性能的影响来衡量质量，有效提升了RAG系统的可靠性和输出质量。


<details>
  <summary>Details</summary>
Motivation: RAG系统依赖外部检索到的上下文以提升大型语言模型（LLM）的回答准确性，但检索到的上下文往往存在噪音或无关信息，而目前的上下文质量评估方法提升有限，无法综合利用可得信息进行全面评估。

Method: 作者将上下文质量评估重构为推理时的数据价值衡量问题，提出“Contextual Influence Value（CI value）”。CI value定义为移除某一上下文对最终生成效果的性能损失，并融合对查询、上下文列表的全局和生成器的关注，同时用正向CI value筛选高质量上下文。为提高实际可用性，作者设计了参数化代理模型利用层次结构高效预测CI value，并用生成器反馈和CI value真值监督训练。

Result: 在8个NLP任务和多种LLM上，所提出的方法显著优于现有RAG上下文筛选方法，能够更有效地过滤低质量上下文，保留关键信息。

Conclusion: 这种以性能贡献为核心的上下文定量评估新范式，不仅提升了RAG的效果，也避免了繁琐的超参数调整，具有较强泛化性和实际应用价值。

Abstract: Retrieval-Augmented Generation (RAG) addresses large language model (LLM)
hallucinations by grounding responses in external knowledge, but its
effectiveness is compromised by poor-quality retrieved contexts containing
irrelevant or noisy information. While existing approaches attempt to improve
performance through context selection based on predefined context quality
assessment metrics, they show limited gains over standard RAG. We attribute
this limitation to their failure in holistically utilizing available
information (query, context list, and generator) for comprehensive quality
assessment. Inspired by recent advances in data selection, we reconceptualize
context quality assessment as an inference-time data valuation problem and
introduce the Contextual Influence Value (CI value). This novel metric
quantifies context quality by measuring the performance degradation when
removing each context from the list, effectively integrating query-aware
relevance, list-aware uniqueness, and generator-aware alignment. Moreover, CI
value eliminates complex selection hyperparameter tuning by simply retaining
contexts with positive CI values. To address practical challenges of label
dependency and computational overhead, we develop a parameterized surrogate
model for CI value prediction during inference. The model employs a
hierarchical architecture that captures both local query-context relevance and
global inter-context interactions, trained through oracle CI value supervision
and end-to-end generator feedback. Extensive experiments across 8 NLP tasks and
multiple LLMs demonstrate that our context selection method significantly
outperforms state-of-the-art baselines, effectively filtering poor-quality
contexts while preserving critical information. Code is available at
https://github.com/SJTU-DMTai/RAG-CSM.

</details>


### [162] [Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs](https://arxiv.org/abs/2509.21361)
*Norman Paulsen*

Main category: cs.CL

TL;DR: 本文系统评估了主流大语言模型（LLM）声称的最大上下文窗口（MCW）与实际有效可用窗口（MECW）之间的差异，发现多数模型实际有效窗口远小于官方宣传。


<details>
  <summary>Details</summary>
Motivation: 当前LLM厂商强调其模型支持很大的上下文窗口，但实际应用中，上下文窗口有效性存疑。作者希望揭示最大上下文窗口与实际有效上下文窗口的差距，并为模型选择和应用提供参考。

Method: 作者提出了最大有效上下文窗口（MECW）的概念，设计了针对不同任务和不同窗口长度，系统性测试模型效能的方法，并标准化了模型性能对比流程。随后，基于该方法收集了多个主流模型下、数十万条数据的实验结果。

Result: 实验证明，所有被测模型的 MECW 远小于其号称的 MCW，部分高端模型在 100 tokens 时即表现不佳，大多数在 1000 tokens 左右准确率明显下降，实际 MECW 甚至较宣传数值少 99%。而且 MECW 随任务类型变化。

Conclusion: 模型号称的最大上下文窗口与实际有效无关，实际有效窗口需根据应用场景具体评估。研究结果可为提升模型准确率与减少幻觉提供可操作建议。

Abstract: Large language model (LLM) providers boast big numbers for maximum context
window sizes. To test the real world use of context windows, we 1) define a
concept of maximum effective context window, 2) formulate a testing method of a
context window's effectiveness over various sizes and problem types, and 3)
create a standardized way to compare model efficacy for increasingly larger
context window sizes to find the point of failure. We collected hundreds of
thousands of data points across several models and found significant
differences between reported Maximum Context Window (MCW) size and Maximum
Effective Context Window (MECW) size. Our findings show that the MECW is, not
only, drastically different from the MCW but also shifts based on the problem
type. A few top of the line models in our test group failed with as little as
100 tokens in context; most had severe degradation in accuracy by 1000 tokens
in context. All models fell far short of their Maximum Context Window by as
much as 99 percent. Our data reveals the Maximum Effective Context Window
shifts based on the type of problem provided, offering clear and actionable
insights into how to improve model accuracy and decrease model hallucination
rates.

</details>


### [163] [How Large Language Models Need Symbolism](https://arxiv.org/abs/2509.21404)
*Xiaotie Deng,Hanyu Li*

Main category: cs.CL

TL;DR: 本文认为，人工智能未来的发展不能仅靠模型规模的扩大，还需要由人类创造的符号体系来引导语言模型获得真正的创新和发现能力。


<details>
  <summary>Details</summary>
Motivation: 当前主流的大语言模型主要依赖于规模扩展来提升性能，但这种方法在实现更高层次智能和创新方面存在局限。作者希望破解单纯扩展规模的瓶颈。

Method: 提出让大语言模型引入人类设计的“符号”或知识结构，以此作为指导，将模型的强大直觉与人类知识有机结合。

Result: 虽然摘要未具体给出实验结果，但提出了“符号引导模型”的关键思想，认为这种方式能够更好地推动AI的突破性发现。

Conclusion: 人工智能要实现真正的创新与认知能力，不能仅依赖于大规模数据和算力扩展，更需结合人类知识结构（符号）作为指引。

Abstract: We argue that AI's future requires more than scaling. To unlock genuine
discovery, large language models need a compass: human-crafted symbols to guide
their powerful but blind intuition.

</details>


### [164] [One Model, Many Morals: Uncovering Cross-Linguistic Misalignments in Computational Moral Reasoning](https://arxiv.org/abs/2509.21443)
*Sualeha Farid,Jayden Lin,Zean Chen,Shivani Kumar,David Jurgens*

Main category: cs.CL

TL;DR: 本论文分析了大型语言模型（LLMs）在多语言、多文化环境下道德推理表现，发现其在不同语言中存在显著不一致性，呼吁更具文化意识的AI。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs被广泛应用于多文化交流，语言和文化对模型道德推理的影响日益重要。但主流模型多以英文数据训练，是否能公正处理多语言场景中的道德判断成为一个关键问题。

Method: 作者将两套经典道德推理基准测试翻译为五种文化和语言类型各异的语言，并在这些语言环境中开展零样本评测，系统分析各语言下模型的道德决策表现。同时提出具体研究问题以揭示不一致性的成因，并结合案例探讨预训练数据对道德倾向的影响。

Result: 研究发现，不同语言下LLMs的道德判断结果明显不一致，且这些差异常常体现出与特定文化的不匹配。分析还揭示了这些偏差可能源自于模型在多语言环境下应对道德问题时采用的推理策略、预训练数据等因素。

Conclusion: 作者总结了道德推理主要错误类型，表明LLMs在多语言环境下存在严重出错隐患，强调推进具备文化适应性的AI模型十分必要。

Abstract: Large Language Models (LLMs) are increasingly deployed in multilingual and
multicultural environments where moral reasoning is essential for generating
ethically appropriate responses. Yet, the dominant pretraining of LLMs on
English-language data raises critical concerns about their ability to
generalize judgments across diverse linguistic and cultural contexts. In this
work, we systematically investigate how language mediates moral decision-making
in LLMs. We translate two established moral reasoning benchmarks into five
culturally and typologically diverse languages, enabling multilingual zero-shot
evaluation. Our analysis reveals significant inconsistencies in LLMs' moral
judgments across languages, often reflecting cultural misalignment. Through a
combination of carefully constructed research questions, we uncover the
underlying drivers of these disparities, ranging from disagreements to
reasoning strategies employed by LLMs. Finally, through a case study, we link
the role of pretraining data in shaping an LLM's moral compass. Through this
work, we distill our insights into a structured typology of moral reasoning
errors that calls for more culturally-aware AI.

</details>


### [165] [LLM-Based Support for Diabetes Diagnosis: Opportunities, Scenarios, and Challenges with GPT-5](https://arxiv.org/abs/2509.21450)
*Gaurav Kumar Gupta,Nirajan Acharya,Pranal Pande*

Main category: cs.CL

TL;DR: 本研究评估了GPT-5在糖尿病诊断和管理中的应用，利用完全基于合成数据的模拟框架，测试了GPT-5在多种场景下的表现，包括症状识别、实验室结果解释、妊娠期糖尿病筛查、远程监测及多模态并发症检测。结果显示GPT-5能够良好对齐ADA标准，具备为临床医生和患者双重服务的潜力。


<details>
  <summary>Details</summary>
Motivation: 糖尿病发病率高且不断上升，但早期识别困难。面对症状模糊、化验值临界、妊娠复杂性及长期监测难题，亟需更好的辅助决策工具。大型语言模型(LLMs)的进步为提升结构化、可解释和患者友好的决策支持带来了希望。

Method: 采用完全基于合成病例的模拟框架，模拟案例对齐ADA 2025护理标准并参考多项公共数据集。通过五个代表性场景（症状识别、化验解释、妊娠期筛查、远程监测和多模态并发症检测）测试GPT-5，考察其病例分类、临床推理、患者解释及结构化输出能力。

Result: GPT-5在所有测试场景下均能与ADA标准高度一致，能够准确分类病例，给出有理有据的临床推理和易于理解的患者解释，并能结构化输出结果。

Conclusion: GPT-5有望成为同时服务临床医生与患者的辅助决策工具。本研究强调了构建可复现评估框架对LLMs在医疗领域负责任应用的重要性。

Abstract: Diabetes mellitus is a major global health challenge, affecting over half a
billion adults worldwide with prevalence projected to rise. Although the
American Diabetes Association (ADA) provides clear diagnostic thresholds, early
recognition remains difficult due to vague symptoms, borderline laboratory
values, gestational complexity, and the demands of long-term monitoring.
Advances in large language models (LLMs) offer opportunities to enhance
decision support through structured, interpretable, and patient-friendly
outputs. This study evaluates GPT-5, the latest generative pre-trained
transformer, using a simulation framework built entirely on synthetic cases
aligned with ADA Standards of Care 2025 and inspired by public datasets
including NHANES, Pima Indians, EyePACS, and MIMIC-IV. Five representative
scenarios were tested: symptom recognition, laboratory interpretation,
gestational diabetes screening, remote monitoring, and multimodal complication
detection. For each, GPT-5 classified cases, generated clinical rationales,
produced patient explanations, and output structured JSON summaries. Results
showed strong alignment with ADA-defined criteria, suggesting GPT-5 may
function as a dual-purpose tool for clinicians and patients, while underscoring
the importance of reproducible evaluation frameworks for responsibly assessing
LLMs in healthcare.

</details>


### [166] [Diagnosing the Performance Trade-off in Moral Alignment: A Case Study on Gender Stereotypes](https://arxiv.org/abs/2509.21456)
*Guangliang Liu,Bocheng Chen,Xitong Zhang,Kristen Marie Johnson*

Main category: cs.CL

TL;DR: 道德对齐（moral alignment）常被用于规范预训练语言模型（PLMs）行为，但在减少性别刻板印象的同时，往往牺牲下游任务表现。分析发现，选择性遗忘刻板印象反而增强整体遗忘，且常用的公平性目标难以优化两者权衡。


<details>
  <summary>Details</summary>
Motivation: 现有通过道德对齐或公平性目标来减少PLMs中的性别刻板印象，但发现通常下游任务性能受损。作者想探究，为何在减少刻板印象时，模型总体性能会下降，这种损失的机制是什么。

Method: 分析基于遗忘与公平目标的机制，着重考察选择性遗忘刻板印象和整体遗忘度，以及现有减少遗忘方法对下游任务的效果。

Result: （1）PLMs下游任务表现主要由遗忘程度驱动；（2）选择性遗忘刻板印象会增强整体遗忘；（3）主流遗忘缓解方法对提升下游任务性能无效。

Conclusion: 当前公平目标方法在平衡减轻刻板印象与保持任务性能上受限，需要新的方法来同时兼顾二者。

Abstract: Moral alignment has emerged as a widely adopted approach for regulating the
behavior of pretrained language models (PLMs), typically through fine-tuning or
model editing on curated datasets. However, this process often comes at the
cost of degraded downstream task performance. Prior studies commonly aim to
achieve a performance trade-off by encouraging PLMs to selectively forget
stereotypical knowledge through carefully designed fairness objectives, while
preserving their helpfulness. In this short paper, we investigate the
underlying mechanisms of the performance trade-off in the context of mitigating
gender stereotypes, through the lens of forgetting and the fairness objective.
Our analysis reveals the limitations of current fairness objective in achieving
trade-off by demonstrating that: (1) downstream task performance is primarily
driven by the overall forgetting level; (2) selective forgetting of stereotypes
tends to increase overall forgetting; and (3) general solutions for mitigating
forgetting are ineffective at reducing overall forgetting and fail to improve
downstream task performance.

</details>


### [167] [A State-of-the-Art SQL Reasoning Model using RLVR](https://arxiv.org/abs/2509.21459)
*Alnur Ali,Ashutosh Baheti,Jonathan Chang,Ta-Chung Chi,Brandon Cui,Andrew Drozdov,Jonathan Frankle,Abhay Gupta,Pallavi Koppol,Sean Kulinski,Jonathan Li,Dipendra Misra,Krista Opsahl-Ong,Jose Javier Gonzalez Ortiz,Matei Zaharia,Yue Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种基于可验证奖励（RLVR）的强化学习方法，能够结合企业特定知识，用于自然语言到SQL的转换任务，并在BIRD基准集上取得了新的最佳成绩。


<details>
  <summary>Details</summary>
Motivation: 企业在处理与自身相关的知识推理问题时，通常需要定制化的AI模型，而传统方法难以满足高度个性化和可验证奖励的需求。

Method: 作者提出了一套简单而通用的训练流程：通过精心设计提示词和模型选择，先用离线RL方法TAO进行预热训练，再进行严格的在线RLVR训练。

Result: 无需额外训练数据或专有模型，仅用BIRD训练集便在BIRD排行榜上达到私有测试集73.56%（无自洽）和75.68%（有自洽）的准确率，并且生成次数少于次优方法。

Conclusion: 该方法简单易用，突破性地提升了BIRD基准成绩，为企业领域如商业智能、数据科学和代码生成等任务提供了普适性解决方案。

Abstract: Developing custom reasoning models via Reinforcement Learning (RL) that can
incorporate organization-specific knowledge has great potential to address
problems faced by enterprise customers. In many of these problems, the reward
function is verifiable, a setting termed RL with Verifiable Rewards (RLVR). We
apply RLVR to a popular data science benchmark called BIRD that measures the
ability of an AI agent to convert a natural language query for a database to
SQL executions. We apply a simple and general-purpose training recipe involving
careful prompt and model selection, a warm-up stage using our offline RL
approach called TAO, followed by rigorous online RLVR training. With no
additional training data beyond the BIRD training set and no use of proprietary
models, our very first submission to the BIRD leaderboard reached
state-of-the-art accuracy on the private test set: 73.56% without
self-consistency and 75.68% with self-consistency. In the latter case, our
model also required fewer generations than the second-best approach. While BIRD
is only a proxy task, the simplicity of our framework makes it broadly
applicable to enterprise domains such as business intelligence, data science,
and coding.

</details>


### [168] [Learning to Reason with Mixture of Tokens](https://arxiv.org/abs/2509.21482)
*Adit Jain,Brendan Rappazzo*

Main category: cs.CL

TL;DR: 本论文提出将token混合生成（MoT-G）方法引入可验证奖励的强化学习（RLVR）中，有效提升大语言模型推理能力，在一系列推理任务上显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法仅利用离散token采样，丢弃了模型对token分布的丰富信息，可能限制了推理效果。非RL方法利用分布信息有益，但RLVR中尚未解决。本研究意在弥补这一短板，通过引入分布信息提升模型推理和搜索能力。

Method: 作者提出统一的MoT-G框架，将加权token嵌入的无训练方法扩展为可直接在连续分布空间内进行链式推理生成，并在Reasoning-Gym任务集上评测两种MoT-G变体。

Result: MoT-G方法在7/10个推理任务上相较标准token解码提升5-35%，且达成相当准确率时仅需一半的采样轨迹，训练效率提升明显。

Conclusion: MoT-G能在推理过程中维持更高的hidden-state熵，促进token空间探索，从而提升推理能力和训练效率，为RLVR带来新方向。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a leading
approach for improving large language model (LLM) reasoning capabilities. Most
current methods follow variants of Group Relative Policy Optimization, which
samples multiple reasoning completions, scores them relative to each other, and
adjusts the policy accordingly. However, these approaches invariably sample
discrete tokens at each reasoning step, discarding the rich distributional
information in the model's probability distribution over candidate tokens.
While preserving and utilizing this distributional information has proven
beneficial in non-RL settings, current RLVR methods seem to be unnecessarily
constraining the reasoning search space by not using this information. To
address this limitation, we investigate mixture-of-token generation (MoT-G) in
RLVR. We present a unified framework that generalizes existing MoT-G
approaches, including existing training-free methods that construct mixture
embeddings as weighted sums over token embeddings, and extend RLVR to operate
directly in this continuous mixture space for generating chain-of-thought.
Evaluating two MoT-G variants on Reasoning-Gym, a suite of reasoning-intensive
language tasks, we find that MoT--G methods achieve substantial improvements
(5--35 \% gains on 7 out of 10 tasks) compared to standard decoding with the
Qwen2.5-1.5B model, while reaching comparable accuracy with half the number of
trajectories, suggesting improved training efficiency. Through comprehensive
hidden-state and token-level analyses, we provide evidence that MoT--G's
benefits may stem from its ability to maintain higher hidden-state entropy
throughout the reasoning process and promote exploration in token space.

</details>


### [169] [Dual-Head Reasoning Distillation: Improving Classifier Accuracy with Train-Time-Only Reasoning](https://arxiv.org/abs/2509.21487)
*Jillian Xu,Dylan Zhou,Vinay Shukla,Yang Yang,Junrui Ruan,Shuhuai Lin,Wenfei Zou,Yinxiao Liu,Karthik Lakshmanan*

Main category: cs.CL

TL;DR: 本文提出了一种名为Dual-Head Reasoning Distillation (DHRD)的新训练方法，既提升了分类准确率，又显著加快了推理速度。


<details>
  <summary>Details</summary>
Motivation: 传统的Chain-of-Thought (CoT) 推理方式虽然能提升分类准确率，但因要生成推理步骤，极大降低了推理速度。作者希望在不牺牲推理速度的前提下，依然能获得高准确率。

Method: DHRD方法为解码器语言模型添加了两个头：一是用于训练和推理的池化分类头，二是仅在训练中用到、受教师理据监督的推理头。训练时联合优化标签交叉熵损失和带理据输入的token级语言模型损失。推理时仅启用分类头，停用推理头。

Result: 在SuperGLUE的七个任务上，DHRD相较于普通池化方法有0.65-5.47%的准确率提升，特别是在蕴含和因果类任务上提升更大。推理时速度与普通分类器相当，比CoT推理快96-142倍。

Conclusion: DHRD能够结合CoT的高准确率和池化分类的高效率，在分类任务上取得准确和高效的双赢表现。

Abstract: Chain-of-Thought (CoT) prompting often improves classification accuracy, but
it introduces a significant throughput penalty with rationale generation (Wei
et al., 2022; Cheng and Van Durme, 2024). To resolve this trade-off, we
introduce Dual-Head Reasoning Distillation (DHRD), a simple training method for
decoder-only language models (LMs) that adds (i) a pooled classification head
used during training and inference and (ii) a reasoning head supervised by
teacher rationales used only in training. We train with a loss function that is
a weighted sum of label cross-entropy and token-level LM loss over
input-plus-rationale sequences. On seven SuperGLUE tasks, DHRD yields relative
gains of 0.65-5.47% over pooled baselines, with notably larger gains on
entailment/causal tasks. Since we disable the reasoning head at test time,
inference throughput matches pooled classifiers and exceeds CoT decoding on the
same backbones by 96-142 times in QPS.

</details>


### [170] [On Code-Induced Reasoning in LLMs](https://arxiv.org/abs/2509.21499)
*Abdul Waheed,Zhen Wu,Carolyn Rosé,Daphne Ippolito*

Main category: cs.CL

TL;DR: 本论文系统性研究了代码数据对大语言模型（LLM）推理能力提升的具体因素，通过多种实验比较代码的不同属性（结构与语义）对模型表现的影响。


<details>
  <summary>Details</summary>
Motivation: 虽然代码数据已被证实能增强LLM推理能力，但尚不清楚哪些代码属性最为关键，因此需要深入探究代码的哪些成分产生提升作用，从而优化训练数据。

Method: 作者构建了10种编程语言的并行指令数据集，并有针对性地对代码做结构或语义的扰动，随后用不同比例和家族的LLM在各类数据变体上进行微调，最后评估在自然语言、数学和代码任务上的表现。共进行了3,331个实验。

Result: 结果显示，LLM在结构扰动下表现比语义扰动更易受损，尤其是在数学和代码任务中。伪代码、流程图等结构化抽象手段在信息量不减的情况下可以与代码同效甚至更优，而表面规律依旧的“损坏代码”也能有不错性能。此外，不同编程语言风格对模型的任务表现有显著影响。

Conclusion: 通过系统性的实验分析，论文揭示了结构性信息是代码提升LLM推理能力的核心，同时为训练数据设计和模型能力提升提供了新的见解。

Abstract: Code data has been shown to enhance the reasoning capabilities of large
language models (LLMs), but it remains unclear which aspects of code are most
responsible. We investigate this question with a systematic, data-centric
framework. We construct parallel instruction datasets in ten programming
languages and apply controlled perturbations that selectively disrupt
structural or semantic properties of code. We then finetune LLMs from five
model families and eight scales on each variant and evaluate their performance
on natural language, math, and code tasks. Across 3,331 experiments, our
results show that LLMs are more vulnerable to structural perturbations than
semantic ones, particularly on math and code tasks. Appropriate abstractions
like pseudocode and flowcharts can be as effective as code, while encoding the
same information with fewer tokens without adhering to original syntax can
often retain or even improve performance. Remarkably, even corrupted code with
misleading signals remains competitive when surface-level regularities persist.
Finally, syntactic styles also shape task-specific gains with Python favoring
natural language reasoning and lower-level languages such as Java and Rust
favoring math. Through our systematic framework, we aim to provide insight into
how different properties of code influence reasoning and inform the design of
training data for enhancing LLM reasoning capabilities.

</details>


### [171] [Agribot: agriculture-specific question answer system](https://arxiv.org/abs/2509.21535)
*Naman Jain,Pranjali Jain,Pratik Kayal,Jayakrishna Sahit,Soham Pachpande,Jayesh Choudhari*

Main category: cs.CL

TL;DR: 本文提出并实现了一个可以全天候服务、面向印度农民的农业聊天机器人，显著提升了查询解答的准确率，为优化农业知识获取和农业产出提供了有效工具。


<details>
  <summary>Details</summary>
Motivation: 印度是一个农业为主的经济体，农民对农业实践相关信息的高效获取对于提升产量和效益至关重要。当前依赖人工客服的方式成本高、覆盖有限，亟需自动化和智能化的信息服务手段。

Method: 基于Kisan Call Center数据集，开发了一个农业聊天机器人系统。技术核心是句子嵌入模型，实现对用户问题的智能匹配与回答。通过去除同义词干扰和引入实体抽取技术，优化了解答准确率。

Result: 初步系统使用句子嵌入模型，解答准确率为56%。引入同义词消歧和实体抽取后，准确率提升到86%。该系统可全天候通过各种电子设备访问。

Conclusion: 该农业聊天机器人有助于农民快速便捷地获取有关天气、市场价格、农作物保护和政府政策等信息，提高了农业生产效率，减轻了人工客服负担，为农业信息自动化服务提供了有效实践。

Abstract: India is an agro-based economy and proper information about agricultural
practices is the key to optimal agricultural growth and output. In order to
answer the queries of the farmer, we have build an agricultural chatbot based
on the dataset from Kisan Call Center. This system is robust enough to answer
queries related to weather, market rates, plant protection and government
schemes. This system is available 24* 7, can be accessed through any electronic
device and the information is delivered with the ease of understanding. The
system is based on a sentence embedding model which gives an accuracy of 56%.
After eliminating synonyms and incorporating entity extraction, the accuracy
jumps to 86%. With such a system, farmers can progress towards easier
information about farming related practices and hence a better agricultural
output. The job of the Call Center workforce would be made easier and the hard
work of various such workers can be redirected to a better goal.

</details>


### [172] [Domain-Aware Speaker Diarization On African-Accented English](https://arxiv.org/abs/2509.21554)
*Chibuzor Okocha,Kelechi Ezema,Christan Grant*

Main category: cs.CL

TL;DR: 本研究分析了非洲口音英语在说话人分离任务中的领域效应，发现临床语音情境下错误率普遍更高，并提出了一种轻量级领域自适应方法以减少错误率。


<details>
  <summary>Details</summary>
Motivation: 说话人分离（speaker diarization）在不同应用场景（如普通对话和临床对话）中的表现存在差异，且针对非洲口音英语的研究有限。明确领域效应并推动临床情境下的分离准确性，有助于相关技术更好地服务多样化场景。

Method: 评估了多种主流和开源说话人分离系统，在普通和临床对话中，在严格的错误率（DER）协议下，包括重叠语音的计分。采用误差分析分解原因，并对分段模块在匹配口音数据上进行微调以检验简单自适应效果。

Result: 发现所有模型在临床语音领域都受到显著的领域惩罚，主要由短轮次和高频重叠导致的漏检和虚警造成。微调可减少错误，但无法完全消除领域差异。

Conclusion: 该研究构建了跨领域受控基准和误差分解方法，提出简单有效的领域自适应方案。未来需关注支持重叠感知的分段及平衡的临床语音数据资源，以继续提升性能。

Abstract: This study examines domain effects in speaker diarization for
African-accented English. We evaluate multiple production and open systems on
general and clinical dialogues under a strict DER protocol that scores overlap.
A consistent domain penalty appears for clinical speech and remains significant
across models. Error analysis attributes much of this penalty to false alarms
and missed detections, aligning with short turns and frequent overlap. We test
lightweight domain adaptation by fine-tuning a segmentation module on
accent-matched data; it reduces error but does not eliminate the gap. Our
contributions include a controlled benchmark across domains, a concise approach
to error decomposition and conversation-level profiling, and an adaptation
recipe that is easy to reproduce. Results point to overlap-aware segmentation
and balanced clinical resources as practical next steps.

</details>


### [173] [Generation-Time vs. Post-hoc Citation: A Holistic Evaluation of LLM Attribution](https://arxiv.org/abs/2509.21557)
*Yash Saxena,Raviteja Bommireddy,Ankur Padia,Manas Gaur*

Main category: cs.CL

TL;DR: 本文比较了两种大语言模型生成可验证引用的范式：生成时引用（G-Cite）和后验引用（P-Cite），并通过实验评估在高风险领域的表现，给出具体应用建议。


<details>
  <summary>Details</summary>
Motivation: 在医疗、法律、金融等高风险领域，大语言模型若引用错误，后果严重。因此需要研究如何让模型生成并附带人类可验证的引用，从而增加模型输出的可信度。当前有两种主流做法，但其优劣未有系统比较。

Method: 提出G-Cite（生成时引用）与P-Cite（后验引用）两种范式，并基于四个主流归因数据集，采用从零样本到高级检索增强方法，系统评估两种方式在覆盖率、准确性和延迟上的表现，分析其权衡关系。

Result: 实验显示两种范式在覆盖率与引用准确性之间存在权衡。P-Cite方法覆盖率高且准确率有竞争力，延迟适中；G-Cite方法则强调精准，但覆盖率和速度较低。检索模块是决定引用质量的关键因素。

Conclusion: 作者建议，高风险应用优先采用以检索和P-Cite为主的流程，以保证引用的全面性和可靠性；而在对精准性要求极高的任务（如严格事实核查）可选用G-Cite。

Abstract: Trustworthy Large Language Models (LLMs) must cite human-verifiable sources
in high-stakes domains such as healthcare, law, academia, and finance, where
even small errors can have severe consequences. Practitioners and researchers
face a choice: let models generate citations during decoding, or let models
draft answers first and then attach appropriate citations. To clarify this
choice, we introduce two paradigms: Generation-Time Citation (G-Cite), which
produces the answer and citations in one pass, and Post-hoc Citation (P-Cite),
which adds or verifies citations after drafting. We conduct a comprehensive
evaluation from zero-shot to advanced retrieval-augmented methods across four
popular attribution datasets and provide evidence-based recommendations that
weigh trade-offs across use cases. Our results show a consistent trade-off
between coverage and citation correctness, with retrieval as the main driver of
attribution quality in both paradigms. P-Cite methods achieve high coverage
with competitive correctness and moderate latency, whereas G-Cite methods
prioritize precision at the cost of coverage and speed. We recommend a
retrieval-centric, P-Cite-first approach for high-stakes applications,
reserving G-Cite for precision-critical settings such as strict claim
verification. Our codes and human evaluation results are available at
https://anonymous.4open.science/r/Citation_Paradigms-BBB5/

</details>


### [174] [Comparative Personalization for Multi-document Summarization](https://arxiv.org/abs/2509.21562)
*Haoyuan Li,Snigdha Chaturvedi*

Main category: cs.CL

TL;DR: 本文提出了个性化多文档摘要（MDS）方法ComPSum，通过比较用户之间的偏好来实现更精细的个性化摘要，并引入了新的评估框架AuthorMap及数据集PerMSum，实验结果表明方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多文档摘要系统难以满足不同用户在写作风格与内容偏好上的精细化需求，因此有必要开发能识别和区分不同用户具体偏好的个性化摘要方法。

Method: 提出了ComPSum框架，首先通过与其他用户对比分析目标用户的偏好，生成结构化分析，再据此指导摘要生成。同时提出了作者归属评估框架AuthorMap（无需参考摘要），并构建了PerMSum数据集以测试该方法。

Result: 在PerMSum数据集上用AuthorMap对ComPSum进行评估，相较于多种强基线模型，ComPSum取得了更好的个性化表现；验证了方法的有效性。

Conclusion: 通过引入更细粒度偏好建模和新的评测手段，ComPSum提升了多文档摘要系统的个性化效果，为个性化自动摘要领域带来了新的思路和工具。

Abstract: Personalized multi-document summarization (MDS) is essential for meeting
individual user preferences of writing style and content focus for summaries.
In this paper, we propose that for effective personalization, it is important
to identify fine-grained differences between users' preferences by comparing
the given user's preferences with other users' preferences.Motivated by this,
we propose ComPSum, a personalized MDS framework. It first generates a
structured analysis of a user by comparing their preferences with other users'
preferences. The generated structured analysis is then used to guide the
generation of personalized summaries. To evaluate the performance of ComPSum,
we propose AuthorMap, a fine-grained reference-free evaluation framework for
personalized MDS. It evaluates the personalization of a system based on the
authorship attribution between two personalized summaries generated for
different users. For robust evaluation of personalized MDS, we construct
PerMSum, a personalized MDS dataset in the review and news domain. We evaluate
the performance of ComPSum on PerMSum using AuthorMap, showing that it
outperforms strong baselines.

</details>


### [175] [Vision Language Models Cannot Plan, but Can They Formalize?](https://arxiv.org/abs/2509.21576)
*Muyu He,Yuxi Zheng,Yuchen Liu,Zijian An,Bill Cai,Jiani Huang,Lifeng Zhou,Feng Liu,Ziyang Li,Li Zhang*

Main category: cs.CL

TL;DR: 本文提出了一套五种基于VLM（视觉-语言模型）作为规划形式化工具的方法，针对开放词表和多模态环境下的PDDL形式化问题，为多模态长程规划任务提供了新思路。实验显示，VLM作为形式化器比端到端生成计划效果更佳，主要瓶颈在于视觉模型对必要对象关系的捕获不足。


<details>
  <summary>Details</summary>
Motivation: 现有的VLM只适用于简单的多模态规划任务，难以胜任需要长序列动作的长程规划。而在纯文本场景中，通过LLM将问题转化为形式化语言（如PDDL）并用正式求解器处理，能显著提升长程规划能力。多模态环境下将VLM用于任务形式化相关研究较少，多依赖简化假设，亟需通用且有效的方法。

Method: 设计了五种VLM-形式化器管道，支持一次学习、开放词表和多模态PDDL形式化。分别在现有基准数据集以及首个真实多视角低质量图像的数据集上进行评测。对比端到端计划生成及各种中间表征（如图像字幕、场景图）对最终表现的影响。

Result: 五种VLM-形式化器管道均大幅超过端到端计划生成方法，展示了VLM在将多模态输入转化为形式规划语言上的能力。不过，所有方法的主要瓶颈均在于视觉模型，难以准确涵盖全部必要的物体关系。生成中间文本表征有一定补偿作用，但效果不稳定。

Conclusion: VLM作为规划问题的形式化器可显著提升多模态长程规划效果，优于传统端到端方法。当前的主要限制在于视觉理解，对关键关系的捕获不足。未来可针对提升视觉关系挖掘和中间表征构建展开研究。

Abstract: The advancement of vision language models (VLMs) has empowered embodied
agents to accomplish simple multimodal planning tasks, but not long-horizon
ones requiring long sequences of actions. In text-only simulations,
long-horizon planning has seen significant improvement brought by repositioning
the role of LLMs. Instead of directly generating action sequences, LLMs
translate the planning domain and problem into a formal planning language like
the Planning Domain Definition Language (PDDL), which can call a formal solver
to derive the plan in a verifiable manner. In multimodal environments, research
on VLM-as-formalizer remains scarce, usually involving gross simplifications
such as predefined object vocabulary or overly similar few-shot examples. In
this work, we present a suite of five VLM-as-formalizer pipelines that tackle
one-shot, open-vocabulary, and multimodal PDDL formalization. We evaluate those
on an existing benchmark while presenting another two that for the first time
account for planning with authentic, multi-view, and low-quality images. We
conclude that VLM-as-formalizer greatly outperforms end-to-end plan generation.
We reveal the bottleneck to be vision rather than language, as VLMs often fail
to capture an exhaustive set of necessary object relations. While generating
intermediate, textual representations such as captions or scene graphs
partially compensate for the performance, their inconsistent gain leaves
headroom for future research directions on multimodal planning formalization.

</details>


### [176] ["Be My Cheese?": Assessing Cultural Nuance in Multilingual LLM Translations](https://arxiv.org/abs/2509.21577)
*Madison Van Doren,Cory Holland*

Main category: cs.CL

TL;DR: 本研究评估了多语言AI模型在翻译英语中的惯用语和双关语等修辞语言时的本地化能力，结果发现语法虽正确，但文化适当性不足，经常需人工调整。


<details>
  <summary>Details</summary>
Motivation: 现有大模型翻译研究和行业基准主要关注语法和词级准确性，忽视了文化适当性和整体本地化质量，而这些因素对实际应用（如营销和电商）极为关键。

Method: 本项目收集87个由LLM生成的电商营销邮件翻译，涵盖20种语言的24种方言，邀请每种目标语言的母语评审员，对译文的语气、意义及受众针对性进行定量打分与定性反馈。

Result: 大模型生成的译文在语法上普遍正确，但遇到修辞表达和文化差异时表现欠佳，即使是主流高资源语言在处理双关语等方面也频繁出错，往往需要大量人工后期修饰。

Conclusion: 本研究挑战了“数据量是机器翻译质量最佳预测因子”的传统观念，强调文化适当性是多语种LLM性能的核心因素。当前多语言AI系统在真正本地化应用场景下仍存在显著局限性，有必要进一步扩大研究以改善模型表现并完善翻译流程。

Abstract: This pilot study explores the localisation capabilities of state-of-the-art
multilingual AI models when translating figurative language, such as idioms and
puns, from English into a diverse range of global languages. It expands on
existing LLM translation research and industry benchmarks, which emphasise
grammatical accuracy and token-level correctness, by focusing on cultural
appropriateness and overall localisation quality - critical factors for
real-world applications like marketing and e-commerce.
  To investigate these challenges, this project evaluated a sample of 87
LLM-generated translations of e-commerce marketing emails across 24 regional
dialects of 20 languages. Human reviewers fluent in each target language
provided quantitative ratings and qualitative feedback on faithfulness to the
original's tone, meaning, and intended audience. Findings suggest that, while
leading models generally produce grammatically correct translations, culturally
nuanced language remains a clear area for improvement, often requiring
substantial human refinement. Notably, even high-resource global languages,
despite topping industry benchmark leaderboards, frequently mistranslated
figurative expressions and wordplay.
  This work challenges the assumption that data volume is the most reliable
predictor of machine translation quality and introduces cultural
appropriateness as a key determinant of multilingual LLM performance - an area
currently underexplored in existing academic and industry benchmarks. As a
proof of concept, this pilot highlights limitations of current multilingual AI
systems for real-world localisation use cases. Results of this pilot support
the opportunity for expanded research at greater scale to deliver generalisable
insights and inform deployment of reliable machine translation workflows in
culturally diverse contexts.

</details>


### [177] [Multi-Objective Reinforcement Learning for Large Language Model Optimization: Visionary Perspective](https://arxiv.org/abs/2509.21613)
*Lingxiao Kong,Cong Yang,Oya Deniz Beyan,Zeyd Boukhers*

Main category: cs.CL

TL;DR: 本文探讨了将多目标强化学习（MORL）应用于大语言模型（LLM）优化的现状、挑战与前景，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有LLM优化方法难以同时满足多个目标需求，如准确性、多样性和个性化等。随着应用场景复杂化，研究者意识到需要高效灵活的MORL方法来解决多目标冲突和个性化适配。

Method: 文章提出并回顾了一套MORL方法的分类体系，分析了不同方法的优缺点，还设想了一个能评估多种方法影响的基准框架。此外，未来建议重点发展具备双层学习范式的元策略MORL，以提升效率与灵活性。

Result: 分析表明，不同MORL方法在LLM多目标优化中存在显著差异，当前方法难以很好地兼容LLM固有的复杂性与个性化需求。提出的基准框架和元策略MORL思路，为解决这些问题提供了理论和发展路线。

Conclusion: 现有MORL方法为LLM多目标优化提供了参考，但仍需突破效率与灵活性瓶颈。文章强调发展元策略MORL及完善评测工具，是推动LLM多目标个性化优化的关键未来方向。

Abstract: Multi-Objective Reinforcement Learning (MORL) presents significant challenges
and opportunities for optimizing multiple objectives in Large Language Models
(LLMs). We introduce a MORL taxonomy and examine the advantages and limitations
of various MORL methods when applied to LLM optimization, identifying the need
for efficient and flexible approaches that accommodate personalization
functionality and inherent complexities in LLMs and RL. We propose a vision for
a MORL benchmarking framework that addresses the effects of different methods
on diverse objective relationships. As future research directions, we focus on
meta-policy MORL development that can improve efficiency and flexibility
through its bi-level learning paradigm, highlighting key research questions and
potential solutions for improving LLM performance.

</details>


### [178] [OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's Rule](https://arxiv.org/abs/2509.21623)
*Yuxuan Zhu,David H. Yang,Mohammad Mohammadi Amiri,Keerthiram Murugesan,Tejaswini Pedapati,Pin-Yu Chen*

Main category: cs.CL

TL;DR: 本文提出OjaKV框架，通过混合存储策略和在线自空间自适应，实现对大语言模型KV cache高效压缩，减少内存消耗，同时在长上下文任务中保持甚至提升模型准确率。


<details>
  <summary>Details</summary>
Motivation: 长上下文大语言模型在推理时需存储大量KV cache，显著占用内存，成为扩展实用性的瓶颈。现有KV cache低秩压缩方法依赖静态、离线学习的子空间，难以应对上下文分布变化，实际效果有限。

Method: OjaKV采用两步策略：1) 对首末重要token保留全秩KV cache，作为关注力锚点；2) 对中间token基于Oja算法进行在线主成分分析，动态适应低秩投影子空间。在prompt预填充阶段进行全面自空间更新，解码时周期性轻量更新。框架与FlashAttention等现代注意力方法兼容。

Result: 实验表明，在高压缩率下，OjaKV能维持、甚至提升零样本任务准确率，尤其在需要复杂推理的超长上下文基准测试中表现优异。

Conclusion: OjaKV是一种无需模型微调、即插即用的高效KV cache压缩方案，能显著提升大模型长上下文推理的内存效率和实际可用性。

Abstract: The expanding long-context capabilities of large language models are
constrained by a significant memory bottleneck: the key-value (KV) cache
required for autoregressive generation. This bottleneck is substantial; for
instance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of
4 requires approximately 16GB for its KV cache, a size exceeding the model's
weights. While KV-cache compression via low-rank projection is a promising
direction, existing methods rely on a static, offline-learned subspace that
performs poorly under data distribution shifts. To overcome these limitations,
we introduce OjaKV, a novel framework that integrates a strategic hybrid
storage policy with online subspace adaptation. First, OjaKV recognizes that
not all tokens are equally important for compression; it preserves the crucial
first and most recent tokens in full-rank, maintaining high-fidelity anchors
for attention. Second, for the vast majority of intermediate tokens, it applies
low-rank compression by incrementally adapting the projection basis using Oja's
algorithm for online principal component analysis. This adaptation involves a
comprehensive update during prompt prefilling and lightweight periodic updates
during decoding, ensuring the subspace remains aligned with the evolving
context. Crucially, our framework is fully compatible with modern attention
modules like FlashAttention. Experiments demonstrate that OjaKV maintains or
even improves zero-shot accuracy at high compression ratios. In particular,
OjaKV achieves its strongest gains on very long-context benchmarks that require
complex reasoning, highlighting the importance of online subspace adaptation in
dynamically tracking context shifts. These results establish our hybrid
framework as a practical, plug-and-play solution for memory-efficient
long-context inference without requiring model fine-tuning.

</details>


### [179] [Towards Transparent AI: A Survey on Explainable Language Models](https://arxiv.org/abs/2509.21631)
*Avash Palikhe,Zichong Wang,Zhipeng Yin,Rui Guo,Qiang Duan,Jie Yang,Wenbin Zhang*

Main category: cs.CL

TL;DR: 这篇综述分析了针对语言模型的可解释人工智能（XAI）方法，系统梳理了不同Transformer结构（编码器、解码器、编码-解码器）下XAI方法的适用性，并总结其优势与不足。


<details>
  <summary>Details</summary>
Motivation: 语言模型在自然语言处理取得显著进展，但其“黑箱”性质导致内部机制和决策过程缺乏透明性，尤其在高风险领域，模型可解释性成为亟需解决的问题。另外，现有面向非LM的XAI方法难以直接应用于结构复杂的语言模型，已有综述未能详尽覆盖架构多样性带来的挑战。

Method: 对现有XAI技术进行了系统性的文献调研，并按照Transformer的结构类型（仅编码器、仅解码器、编码器-解码器）分类归纳和分析这些方法，评估它们在不同结构中的适应方式、优劣性，并从可信性与真实性两大视角对方法有效性进行评价。

Result: 系统展示了各类Transformer架构下XAI方法的分类、优劣势、适应情形，以及在可信性和真实性方面的能力，对不同方法进行了详细比较。此外，识别出了当前研究中未解决的问题和不足。

Conclusion: 文献综述为XAI在语言模型领域的研究提供了组织体系与评估框架，明确了主要挑战，指明了未来可能研究方向，旨在推进更加健壮、透明和可解释的语言模型XAI方法发展。

Abstract: Language Models (LMs) have significantly advanced natural language processing
and enabled remarkable progress across diverse domains, yet their black-box
nature raises critical concerns about the interpretability of their internal
mechanisms and decision-making processes. This lack of transparency is
particularly problematic for adoption in high-stakes domains, where
stakeholders need to understand the rationale behind model outputs to ensure
accountability. On the other hand, while explainable artificial intelligence
(XAI) methods have been well studied for non-LMs, they face many limitations
when applied to LMs due to their complex architectures, considerable training
corpora, and broad generalization abilities. Although various surveys have
examined XAI in the context of LMs, they often fail to capture the distinct
challenges arising from the architectural diversity and evolving capabilities
of these models. To bridge this gap, this survey presents a comprehensive
review of XAI techniques with a particular emphasis on LMs, organizing them
according to their underlying transformer architectures: encoder-only,
decoder-only, and encoder-decoder, and analyzing how methods are adapted to
each while assessing their respective strengths and limitations. Furthermore,
we evaluate these techniques through the dual lenses of plausibility and
faithfulness, offering a structured perspective on their effectiveness.
Finally, we identify open research challenges and outline promising future
directions, aiming to guide ongoing efforts toward the development of robust,
transparent, and interpretable XAI methods for LMs.

</details>


### [180] [ReviewScore: Misinformed Peer Review Detection with Large Language Models](https://arxiv.org/abs/2509.21679)
*Hyun Ryu,Doohyuk Jang,Hyemin S. Lee,Joonhyun Jeong,Gyeongman Kim,Donghyeon Cho,Gyouk Chu,Minyeong Hwang,Hyeongwon Jang,Changhun Kim,Haechan Kim,Jina Kim,Joowon Kim,Yoonjeon Kim,Kwanhyung Lee,Chanjae Park,Heecheol Yun,Gregor Betz,Eunho Yang*

Main category: cs.CL

TL;DR: 该论文关注AI会议同行评审中评审质量下降问题，通过定义“误导性评审点”，提出自动化工具辅助评审质量检测。


<details>
  <summary>Details</summary>
Motivation: 近年来，AI相关会议投稿数量激增，导致同行评审质量下滑。高质量的评审对学术发展至关重要，因此需要自动化手段检测并提升评审质量。

Method: 作者将评审中的“误导性评审点”分为包含错误前提的“weaknesses”和可被论文直接回答的“questions”。提出ReviewScore评分体系，用于标注和检测误导性内容。开发自动化引擎，结合LLMs（大语言模型），重建并验证weaknesses中的前提，并与人工评分对比，分析一致性。

Result: 实验显示，15.2%的weaknesses和26.4%的questions属于误导性内容。LLM模型与人工之间在ReviewScore上的一致性为中等水平，前提级事实性评价比weakness级更一致。

Conclusion: 前提级事实性自动评价有助于更准确检测低质量评审，有望实现评审自动化，为提升同行评审质量提供新工具。

Abstract: Peer review serves as a backbone of academic research, but in most AI
conferences, the review quality is degrading as the number of submissions
explodes. To reliably detect low-quality reviews, we define misinformed review
points as either "weaknesses" in a review that contain incorrect premises, or
"questions" in a review that can be already answered by the paper. We verify
that 15.2% of weaknesses and 26.4% of questions are misinformed and introduce
ReviewScore indicating if a review point is misinformed. To evaluate the
factuality of each premise of weaknesses, we propose an automated engine that
reconstructs every explicit and implicit premise from a weakness. We build a
human expert-annotated ReviewScore dataset to check the ability of LLMs to
automate ReviewScore evaluation. Then, we measure human-model agreements on
ReviewScore using eight current state-of-the-art LLMs and verify moderate
agreements. We also prove that evaluating premise-level factuality shows
significantly higher agreements than evaluating weakness-level factuality. A
thorough disagreement analysis further supports a potential of fully automated
ReviewScore evaluation.

</details>


### [181] [GRAB: A Risk Taxonomy--Grounded Benchmark for Unsupervised Topic Discovery in Financial Disclosures](https://arxiv.org/abs/2509.21698)
*Ying Li,Tiejun Ma*

Main category: cs.CL

TL;DR: 本文提出了GRAB金融风险披露细分领域基准数据集和评价体系，针对10-K风险信息分类任务，填补了无监督主题模型领域缺乏系统公开评测的空白。


<details>
  <summary>Details</summary>
Motivation: 10-K风险信息披露的准确分类对于监管和投资决策至关重要，但目前缺乏面向金融文本、可供无监督主题模型系统评测的公开基准。

Method: 建立了GRAB数据集，包含8247份年报中161万句风险文本，通过FinBERT注意力、YAKE关键词、和基于风险分类体系的共现匹配三者融合，实现了无需人工标注的自动句级标注。风险分类体系覆盖193个术语、21个细分类和5大主类，并通过弱监督机制和标准化切分、评价指标（准确率、Macro-F1、Topic BERTScore、多样性熵值）统一评测。

Result: 提出的数据集和评测体系，可以对各类主题模型（传统、嵌入、神经及混合）在金融风险披露文本上的分类效果进行公平、可重复且标准化的评估。

Conclusion: GRAB为金融风险主题模型研究提供了首个大规模、无需人工标记的基准，促进领域方法对比和发展，提升金融文本主题发现相关研究的可复现性和科学性。

Abstract: Risk categorization in 10-K risk disclosures matters for oversight and
investment, yet no public benchmark evaluates unsupervised topic models for
this task. We present GRAB, a finance-specific benchmark with 1.61M sentences
from 8,247 filings and span-grounded sentence labels produced without manual
annotation by combining FinBERT token attention, YAKE keyphrase signals, and
taxonomy-aware collocation matching. Labels are anchored in a risk taxonomy
mapping 193 terms to 21 fine-grained types nested under five macro classes; the
21 types guide weak supervision, while evaluation is reported at the macro
level. GRAB unifies evaluation with fixed dataset splits and robust
metrics--Accuracy, Macro-F1, Topic BERTScore, and the entropy-based Effective
Number of Topics. The dataset, labels, and code enable reproducible,
standardized comparison across classical, embedding-based, neural, and hybrid
topic models on financial disclosures.

</details>


### [182] [Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval](https://arxiv.org/abs/2509.21710)
*Xiaojun Wu,Cehao Yang,Xueyuan Lin,Chengjin Xu,Xuhui Jiang,Yuanliang Sun,Hui Xiong,Jia Li,Jian Guo*

Main category: cs.CL

TL;DR: 本文提出了ToG-3（Think-on-Graph 3.0），一种集多智能体和动态图索引演化于一体的新型RAG方法，实现了更加精准和高效的证据检索及推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的RAG（Retrieval-Augmented Generation）方法依赖高质量知识图谱，但构建人工知识图谱成本高昂，自动抽取又受限于基础LLM模型，难以兼顾灵活性和性能。静态一次性构图也无法针对具体查询动态优化，影响推理效果。

Method: 提出MACER机制，通过多智能体系统（包括构造器、检索器、反思者和响应者），动态演化“块-三元组-社区”异构图索引，采用“查询演化”和“子图演化”双演化机制，实现针对性更强、适应查询的证据构建与推理。该系统能够在推理过程中动态构建和优化图结构，即使轻量级LLM也能实现精细推理。

Result: 实验表明，ToG-3在多种深度与广度推理基准测试中都优于现有方法，消融实验证明MACER各模块均有积极贡献。

Conclusion: ToG-3通过多智能体协作和图结构的动态双演化，有效克服了以往静态图构建与低性能抽取器的瓶颈，提高了RAG系统的检索与推理能力，尤其适用于轻量级大模型环境。

Abstract: Retrieval-Augmented Generation (RAG) and Graph-based RAG has become the
important paradigm for enhancing Large Language Models (LLMs) with external
knowledge. However, existing approaches face a fundamental trade-off. While
graph-based methods are inherently dependent on high-quality graph structures,
they face significant practical constraints: manually constructed knowledge
graphs are prohibitively expensive to scale, while automatically extracted
graphs from corpora are limited by the performance of the underlying LLM
extractors, especially when using smaller, local-deployed models. This paper
presents Think-on-Graph 3.0 (ToG-3), a novel framework that introduces
Multi-Agent Context Evolution and Retrieval (MACER) mechanism to overcome these
limitations. Our core innovation is the dynamic construction and refinement of
a Chunk-Triplets-Community heterogeneous graph index, which pioneeringly
incorporates a dual-evolution mechanism of Evolving Query and Evolving
Sub-Graph for precise evidence retrieval. This approach addresses a critical
limitation of prior Graph-based RAG methods, which typically construct a static
graph index in a single pass without adapting to the actual query. A
multi-agent system, comprising Constructor, Retriever, Reflector, and Responser
agents, collaboratively engages in an iterative process of evidence retrieval,
answer generation, sufficiency reflection, and, crucially, evolving query and
subgraph. This dual-evolving multi-agent system allows ToG-3 to adaptively
build a targeted graph index during reasoning, mitigating the inherent
drawbacks of static, one-time graph construction and enabling deep, precise
reasoning even with lightweight LLMs. Extensive experiments demonstrate that
ToG-3 outperforms compared baselines on both deep and broad reasoning
benchmarks, and ablation studies confirm the efficacy of the components of
MACER framework.

</details>


### [183] [ProPerSim: Developing Proactive and Personalized AI Assistants through User-Assistant Simulation](https://arxiv.org/abs/2509.21730)
*Jiho Kim,Junseong Choi,Woosog Chay,Daeun Kyung,Yeonsu Kwon,Yohan Jo,Edward Choi*

Main category: cs.CL

TL;DR: 本文提出了ProPerSim框架，用于训练能够主动、个性化地做出建议的AI助手，并开发了ProPerAssistant，在用户反馈下不断学习，提升用户满意度。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型更多注重反应式交互，业界虽有推进主动性与个性化，但二者结合研究不足。实际应用中用户希望AI助手既具主动性又能个性化服务，因此需要填补这一空白。

Method: 设计了ProPerSim仿真环境，模拟家庭场景，助手基于用户评分不断调整建议策略。进一步提出ProPerAssistant，结合信息检索和用户偏好建模，实现持续自适应。

Result: 实验涵盖32种不同用户角色，结果表明ProPerAssistant能够灵活调整策略，用户满意度持续提升，验证了方法的有效性。

Conclusion: 将主动性与个性化融合有助于实现更高满意度的AI助手，ProPerSim和ProPerAssistant为相关研究提供了新思路和工具。

Abstract: As large language models (LLMs) become increasingly integrated into daily
life, there is growing demand for AI assistants that are not only reactive but
also proactive and personalized. While recent advances have pushed forward
proactivity and personalization individually, their combination remains
underexplored. To bridge this gap, we introduce ProPerSim, a new task and
simulation framework for developing assistants capable of making timely,
personalized recommendations in realistic home scenarios. In our simulation
environment, a user agent with a rich persona interacts with the assistant,
providing ratings on how well each suggestion aligns with its preferences and
context. The assistant's goal is to use these ratings to learn and adapt to
achieve higher scores over time. Built on ProPerSim, we propose
ProPerAssistant, a retrieval-augmented, preference-aligned assistant that
continually learns and adapts through user feedback. Experiments across 32
diverse personas show that ProPerAssistant adapts its strategy and steadily
improves user satisfaction, highlighting the promise of uniting proactivity and
personalization.

</details>


### [184] [How Accurate Are LLMs at Multi-Question Answering on Conversational Transcripts?](https://arxiv.org/abs/2509.21732)
*Xiliang Zhu,Shi Zong,David Rossouw*

Main category: cs.CL

TL;DR: 本文探讨在长上下文环境下，利用大型语言模型（LLM）进行多问答任务的性能，并对比专有与开源模型。结果显示，经过微调的开源模型在准确率上有超越顶级专有模型的潜力，具备更高性价比。


<details>
  <summary>Details</summary>
Motivation: 在工业场景中，针对同一上下文需批量回答多个问题，但传统方法成本高、响应慢，因此亟需高效且经济的解决方案。

Method: 作者系统性地对多个专有与开源LLM（含微调版本）在长上下文多问答任务下进行基准测试与实验评估。

Result: GPT-4o等专有LLM表现最佳，但精调后的开源LLM（参数量达80亿）在准确率上能超越GPT-4o。

Conclusion: 开源LLM通过微调可实现准确、高效的多问答能力，为实际应用部署提供更透明、经济的选择。

Abstract: Deploying Large Language Models (LLMs) for question answering (QA) over
lengthy contexts is a significant challenge. In industrial settings, this
process is often hindered by high computational costs and latency, especially
when multiple questions must be answered based on the same context. In this
work, we explore the capabilities of LLMs to answer multiple questions based on
the same conversational context. We conduct extensive experiments and benchmark
a range of both proprietary and public models on this challenging task. Our
findings highlight that while strong proprietary LLMs like GPT-4o achieve the
best overall performance, fine-tuned public LLMs with up to 8 billion
parameters can surpass GPT-4o in accuracy, which demonstrates their potential
for transparent and cost-effective deployment in real-world applications.

</details>


### [185] [Self-Speculative Biased Decoding for Faster Live Translation](https://arxiv.org/abs/2509.21740)
*Linxiao Zeng,Haoyun Deng,Kangyuan Shu,Shizhen Wang*

Main category: cs.CL

TL;DR: 本文提出了一种用于大语言模型实时串流应用的新型推理范式，通过自我投机与偏置解码，显著提升了同声传译等任务的效率并减少输出不稳定性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然在文本生成任务表现突出，但难以直接应用于需要低延迟、持续生成的流式应用，如同声传译，因为每次输入扩展都需重新生成输出，计算代价高、延迟大。

Method: 提出了自我投机偏置解码（Self-Speculative Biased Decoding）方法：每次输入扩展时，将上一次的输出作为当前新输入的草稿，在验证阶段，偏向采纳这个草稿以提高采用率。若有分歧，则采用经典解码持续生成。这套流程无需重复草稿计算，且模型无关、易于集成。同时，利用mask-k技术减少实时输出的闪烁现象。

Result: 在同声传译任务上的实验结果显示，该方法较传统自回归再翻译推理速度提升1.7倍，无质量损失。同时结合mask-k技术后，输出闪烁程度减少了80%。

Conclusion: 自我投机偏置解码为流式文本生成提供了一种高效、通用的解决方案，显著降低了延迟和闪烁问题，可广泛用于低延迟、高质量要求的实时应用。

Abstract: Large Language Models (LLMs) have recently demonstrated impressive
capabilities in various text generation tasks. However, it remains challenging
to use them off-the-shelf in streaming applications (such as live translation),
where the output must continually update as the input context expands, while
still maintaining a reasonable computational cost to meet the latency
requirement.
  In this work, we reexamine the re-translation approach to simultaneous
translation and propose Self-Speculative Biased Decoding, a novel inference
paradigm designed to avoid repeatedly generating output from scratch for a
consistently growing input stream. We propose using the most recent output as a
draft for the current growing input context. During the verification stage, the
output will be biased towards the draft token for a higher draft acceptance
rate. This strategy not only minimizes flickering that might distract users but
also leads to higher speedups. Conventional decoding may take charge from the
point of divergence after draft verification and continue until the end
condition is met.
  Unlike existing speculative decoding strategies, our approach eliminates the
need for draft computations, making it a model-agnostic and plug-and-play
solution for accelerating latency-sensitive streaming applications.
Experimental results on simultaneous text-to-text re-translation demonstrate
that our approach achieves up to 1.7x speedup compared to conventional
auto-regressive re-translation without compromising quality. Additionally, it
significantly reduces flickering by 80% by incorporating the display-only
mask-k technique.

</details>


### [186] [Thinking with Sound: Audio Chain-of-Thought Enables Multimodal Reasoning in Large Audio-Language Models](https://arxiv.org/abs/2509.21749)
*Zhen Xiong,Yujun Cai,Zhecheng Li,Junsong Yuan,Yiwei Wang*

Main category: cs.CL

TL;DR: 本论文提出了一种新的音频-语言模型框架Thinking-with-Sound（TwS），通过结合语言推理与实时音频分析，显著提升了模型在复杂声学环境下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型音频-语言模型（LALMs）在复杂音频推理任务中表现不佳，尤其是在噪声、混响等复杂声学条件下。提升模型在真实、复杂场景下的理解与推理能力显得尤为重要。

Method: 作者提出TwS框架，使LALMs能够结合语言推理和对音频信号的直接分析（如噪音抑制、信源分离、时序对齐）进行推理。此外，提出了MELD-Hard1k数据集，用于对模型的鲁棒性进行评测。

Result: 实验表明，传统LALMs在复杂扰动音频上的表现下降超过50%。而引入TwS框架后，模型准确率大幅提升，提升幅度随模型规模递增，最大可达36.61%的绝对提升。

Conclusion: 结合Audio CoT的TwS显著提升了音频-语言模型对复杂音频场景的鲁棒性，而且无需重新训练，从而为未来更强的音频理解系统开辟了新方向。

Abstract: Recent Large Audio-Language Models (LALMs) have shown strong performance on
various audio understanding tasks such as speech translation and Audio Q\&A.
However, they exhibit significant limitations on challenging audio reasoning
tasks in complex acoustic scenarios. These situations would greatly benefit
from the use of acoustic tools like noise suppression, source separation, and
precise temporal alignment, but current LALMs lack access to such tools. To
address this limitation, we introduce Thinking-with-Sound (TwS), a framework
that equips LALMs with Audio CoT by combining linguistic reasoning with
on-the-fly audio-domain analysis. Unlike existing approaches that treat audio
as static input, TwS enables models to actively think with audio signals,
performing numerical analysis and digital manipulation through multimodal
reasoning. To evaluate this approach, we construct MELD-Hard1k, a new
robustness benchmark created by introducing various acoustic perturbations.
Experiments reveal that state-of-the-art LALMs suffer dramatic performance
degradation on MELD-Hard1k, with accuracy dropping by more than $50\%$ compared
to clean audio. TwS achieves substantial improvements in robustness,
demonstrating both effectiveness and scalability: small models gain $24.73\%$
absolute accuracy, with improvements scaling consistently up to $36.61\%$ for
larger models. Our findings demonstrate that Audio CoT can significantly
enhance robustness without retraining, opening new directions for developing
more robust audio understanding systems.

</details>


### [187] [SynerGen: Contextualized Generative Recommender for Unified Search and Recommendation](https://arxiv.org/abs/2509.21777)
*Vianne R. Gao,Chen Xue,Marc Versage,Xie Zhou,Zhongruo Wang,Chao Li,Yeon Seonwoo,Nan Chen,Zhen Ge,Gourab Kundu,Weiqi Zhang,Tian Wang,Qingjun Cui,Trishul Chilimbi*

Main category: cs.CL

TL;DR: 本文提出了SynerGen，一种统一个性化搜索与推荐任务的生成式推荐模型，同时兼顾检索和排序效果，在主流基准上超过了以往最佳模型。


<details>
  <summary>Details</summary>
Motivation: 现有大规模推荐系统中，检索和排序分属不同模块，导致系统校准不佳、工程复杂。近期生成模型虽可统一检索与排序，但多针对单一任务，且统一处理时存性能损失。亟需一个既能联合搜索和推荐、又能提升整体性能的统一生成式推荐框架。

Method: 提出SynerGen，一个decoder-only Transformer模型，基于用户行为序列训练。通过InfoNCE进行检索任务的联合优化，排序则采用混合pointwise-pairwise损失，同时提出时间感知旋转位置编码以更好利用时序信息。这样可使搜索和推荐语义互补、提升表现。

Result: SynerGen在主流推荐系统与搜索任务基准上，显著超过强基线的生成式推荐与联合检索排序模型。

Conclusion: SynerGen验证了利用单一生成式基础模型统一大规模工业级信息访问系统（涵盖搜索与推荐）的可行性和优越性。

Abstract: The dominant retrieve-then-rank pipeline in large-scale recommender systems
suffers from mis-calibration and engineering overhead due to its architectural
split and differing optimization objectives. While recent generative sequence
models have shown promise in unifying retrieval and ranking by
auto-regressively generating ranked items, existing solutions typically address
either personalized search or query-free recommendation, often exhibiting
performance trade-offs when attempting to unify both. We introduce
\textit{SynerGen}, a novel generative recommender model that bridges this
critical gap by providing a single generative backbone for both personalized
search and recommendation, while simultaneously excelling at retrieval and
ranking tasks. Trained on behavioral sequences, our decoder-only Transformer
leverages joint optimization with InfoNCE for retrieval and a hybrid
pointwise-pairwise loss for ranking, allowing semantic signals from search to
improve recommendation and vice versa. We also propose a novel time-aware
rotary positional embedding to effectively incorporate time information into
the attention mechanism. \textit{SynerGen} achieves significant improvements on
widely adopted recommendation and search benchmarks compared to strong
generative recommender and joint search and recommendation baselines. This work
demonstrates the viability of a single generative foundation model for
industrial-scale unified information access.

</details>


### [188] [Navigating the Impact of Structured Output Format on Large Language Models through the Compass of Causal Inference](https://arxiv.org/abs/2509.21791)
*Han Yuan,Yue Zhao,Li Zhang,Wuqiong Luo,Zheng Ma*

Main category: cs.CL

TL;DR: 本文通过因果推断重新分析了结构化输出对大语言模型生成质量的影响，发现以往结论可能被粗略评估或弱对照实验误导。实验证明，在绝大多数情况下，结构化输出对生成质量没有因果影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究对结构化输出对LLM生成质量的作用存在分歧，且评估方式存在不足（如测试场景有限、对照设置不严、结果指标粗糙）。该文旨在通过更科学的方法澄清结构化输出的真实影响。

Method: 本研究结合了假设与约束，提出五种潜在因果结构框架。通过因果推断方法和八个推理任务，对GPT-4o在结构化输出下的表现进行了细致比较。

Result: 常规指标显示结构化输出对模型有正面、负面或中性影响，但因果推断发现48个测试场景中有43个并无因果影响，剩余5个有影响的场景多与具体指令有关。

Conclusion: 结构化输出在大多数情况下并未显著提升或降低LLM生成质量，因果推断有助于更准确理解结构和指令对输出的真实影响。

Abstract: Structured output from large language models (LLMs) has enhanced efficiency
in processing generated information and is increasingly adopted in industrial
applications. Prior studies have investigated the impact of structured output
on LLMs' generation quality, often presenting one-way findings. Some suggest
that structured format enhances completeness and factual accuracy, while others
argue that it restricts the reasoning capacity of LLMs and leads to reductions
in standard evaluation metrics. Potential limitations of these assessments
include restricted testing scenarios, weakly controlled comparative settings,
and reliance on coarse metrics. In this work, we present a refined analysis
using causal inference. Based on one assumed and two guaranteed constraints, we
derive five potential causal structures characterizing the influence of
structured output on LLMs' generation: (1) collider without m-bias, (2)
collider with m-bias, (3) single cause from instruction, (4) single cause from
output format, and (5) independence. Across seven public and one developed
reasoning tasks, we find that coarse metrics report positive, negative, or
neutral effects of structured output on GPT-4o's generation. However, causal
inference reveals no causal impact in 43 out of 48 scenarios. In the remaining
5, 3 involve multifaceted causal structures influenced by concrete
instructions.

</details>


### [189] [Evaluating and Improving Cultural Awareness of Reward Models for LLM Alignment](https://arxiv.org/abs/2509.21798)
*Hongbin Zhang,Kehai Chen,Xuefeng Bai,Yang Xiang,Min Zhang*

Main category: cs.CL

TL;DR: 该论文提出了评估大语言模型文化意识的新基准CARB，覆盖10种文化并分析了当前奖励模型在文化意识方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前奖励模型在跨文化对齐方面评估手段有限，缺乏针对文化多样性的专门评测数据集，影响了大模型的全球适用性和公平性。

Method: 作者提出了Cultural Awareness Reward modeling Benchmark（CARB），涉及10种文化和4个领域，系统评估主流奖励模型的文化意识水平。同时发现了模型存在依赖表面特征而忽视文化本质的问题，并进一步提出通过 'Think-as-Locals' 策略和可验证奖励的强化学习（RLVR）机制，提升模型对文化细节的理解和判断。

Result: 实验证明，现有奖励模型在文化意识评测上表现有限，存在虚假相关现象。新方法（RLVR+结构化奖励）有效缓解了表面特征干扰，提升了模型的真实文化适应能力。

Conclusion: CARB为文化感知奖励建模提供了新基准，提出的方法能提升大语言模型的跨文化对齐能力，对于推动AI全球化应用具有重要意义。

Abstract: Reward models (RMs) are crucial for aligning large language models (LLMs)
with diverse cultures. Consequently, evaluating their cultural awareness is
essential for further advancing global alignment of LLMs. However, existing RM
evaluations fall short in assessing cultural awareness due to the scarcity of
culturally relevant evaluation datasets. To fill this gap, we propose Cultural
Awareness Reward modeling Benchmark (CARB), covering 10 distinct cultures
across 4 cultural domains. Our extensive evaluation of state-of-the-art RMs
reveals their deficiencies in modeling cultural awareness and demonstrates a
positive correlation between performance on CARB and downstream multilingual
cultural alignment tasks. Further analysis identifies the spurious correlations
within culture-aware reward modeling, wherein RM's scoring relies predominantly
on surface-level features rather than authentic cultural nuance understanding.
To address these, we propose Think-as-Locals to elicit deeper culturally
grounded reasoning from generative RMs via reinforcement learning from
verifiable rewards (RLVR) and employ well-designed rewards to ensure accurate
preference judgments and high-quality structured evaluation criteria
generation. Experimental results validate its efficacy in mitigating spurious
features interference and advancing culture-aware reward modeling.

</details>


### [190] [Redefining Machine Simultaneous Interpretation: From Incremental Translation to Human-Like Strategies](https://arxiv.org/abs/2509.21801)
*Qianen Zhang,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 本论文提出在同时机器翻译(SiMT)任务中，将原有的读取/写入操作扩展为包括句子切割、丢弃、部分摘要和代词化四种自适应动作，从而更好地兼顾实时性与翻译质量。基于解码器的LLM实现该机制，实验表明对语义指标和时延均有明显优化。


<details>
  <summary>Details</summary>
Motivation: 现有的SiMT方案仅支持简单的读取/写入操作，难以应对实时性要求下内容重组、简化和省略等复杂操作，导致翻译质量和时延难以兼得。

Method: 扩展SiMT的动作空间，引入SENTENCE_CUT、DROP、PARTIAL_SUMMARIZATION与PRONOMINALIZATION四种新动作，通过在解码器型LLM中实现并用动作感知提示生成训练参考。同时，建立结合延迟感知TTS的评测框架，定量评估翻译质量与时延表现。

Result: 在ACL60/60英中与英德数据集上，该方法提升了语义指标（如COMET-KIWI），降低了平均时延。特别是联合使用DROP与SENTENCE_CUT动作时，流畅性与时延达到最佳平衡，性能超越基线方法。

Conclusion: 扩充LLM基础SiMT的动作空间可有效提升其翻译的实时性与质量，为缩小机器与人类口译之间的差距提供了新方向。

Abstract: Simultaneous Machine Translation (SiMT) requires high-quality translations
under strict real-time constraints, which traditional encoder-decoder policies
with only READ/WRITE actions cannot fully address. We extend the action space
of SiMT with four adaptive actions: SENTENCE_CUT, DROP, PARTIAL_SUMMARIZATION
and PRONOMINALIZATION, which enable real-time restructuring, omission, and
simplification while preserving semantic fidelity. We implement these actions
in a decoder-only large language model (LLM) framework and construct training
references through action-aware prompting. To evaluate both quality and
latency, we further develop a latency-aware TTS pipeline that maps textual
outputs to speech with realistic timing. Experiments on the ACL60/60
English-Chinese and English-German benchmarks show that our framework
consistently improves semantic metrics (e.g., COMET-KIWI) and achieves lower
delay (measured by Average Lagging) compared to reference translations and
salami-based baselines. Notably, combining DROP and SENTENCE_CUT yields the
best overall balance between fluency and latency. These results demonstrate
that enriching the action space of LLM-based SiMT provides a promising
direction for bridging the gap between human and machine interpretation.

</details>


### [191] [Towards Minimal Causal Representations for Human Multimodal Language Understanding](https://arxiv.org/abs/2509.21805)
*Menghua Jiang,Yuncheng Jiang,Haifeng Hu,Sijie Mai*

Main category: cs.CL

TL;DR: 本文提出了一种新的因果多模态信息瓶颈（CaMIB）模型，用于提升多模态语言理解任务的泛化能力，尤其是在面对分布外（OOD）数据时的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态语言理解模型通过提升输入与标签的互信息来增强表现，但容易受到数据集偏差的影响，进而利用错误的统计捷径，导致模型在分布外数据上的表现较差。作者希望通过因果推理的方法，从根本上缓解这一问题。

Method: 该方法引入了信息瓶颈来过滤每种模态的输入，剔除与任务无关的噪声。随后，使用参数化的掩码生成器将多模态融合表征拆分为因果特征和捷径特征。通过引入工具变量约束保持因果特征的一致性，并采用随机重组因果与捷径特征的方法进行后门调整，从而稳定因果估计。

Result: 在多模态情感分析、幽默检测和讽刺检测等任务及OOD数据集上，CaMIB模型均取得了优异效果。理论和实证分析显示该方法具备良好的可解释性与科学合理性。

Conclusion: CaMIB模型有效提升了多模态语言理解任务的泛化能力，能够更好地分辨因果特征与非因果特征，为应对数据集偏差和提升分布外鲁棒性提供了一种可行的因果解决方案。

Abstract: Human Multimodal Language Understanding (MLU) aims to infer human intentions
by integrating related cues from heterogeneous modalities. Existing works
predominantly follow a ``learning to attend" paradigm, which maximizes mutual
information between data and labels to enhance predictive performance. However,
such methods are vulnerable to unintended dataset biases, causing models to
conflate statistical shortcuts with genuine causal features and resulting in
degraded out-of-distribution (OOD) generalization. To alleviate this issue, we
introduce a Causal Multimodal Information Bottleneck (CaMIB) model that
leverages causal principles rather than traditional likelihood. Concretely, we
first applies the information bottleneck to filter unimodal inputs, removing
task-irrelevant noise. A parameterized mask generator then disentangles the
fused multimodal representation into causal and shortcut subrepresentations. To
ensure global consistency of causal features, we incorporate an instrumental
variable constraint, and further adopt backdoor adjustment by randomly
recombining causal and shortcut features to stabilize causal estimation.
Extensive experiments on multimodal sentiment analysis, humor detection, and
sarcasm detection, along with OOD test sets, demonstrate the effectiveness of
CaMIB. Theoretical and empirical analyses further highlight its
interpretability and soundness.

</details>


### [192] [Can LLMs Solve and Generate Linguistic Olympiad Puzzles?](https://arxiv.org/abs/2509.21820)
*Neh Majmudar,Elena Filatova*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLM）在语言奥林匹克竞赛题目的解题与生成任务中的表现，并提出自动化生成语言谜题对于语言学推广的重要意义。


<details>
  <summary>Details</summary>
Motivation: 推动语言谜题的自动解题和生成能力，既能提升相关领域研究效率，又有助于稀有或欠研究语言的传承与普及。

Method: （1）扩充现有的语言谜题解决任务基准数据集；（2）系统评估包括OpenAI的最新LLM在内的多种大型语言模型在不同类型语言谜题上的表现；（3）基于模型解题的表现和分析，探索LLM用于自动化生成语言谜题的可行性，并进行讨论。

Result: LLMs在大多数类型的语言奥林匹克谜题解答中优于人类，但在书写系统类谜题和针对稀有、欠研究语言的谜题上表现相对较差。

Conclusion: 大型语言模型在语言谜题解决方面有巨大潜力，同时自动化谜题生成能够拓宽语言学对外推广的途径，尤其有助于罕见及欠研究语言的知识传播和认知普及。

Abstract: In this paper, we introduce a combination of novel and exciting tasks: the
solution and generation of linguistic puzzles. We focus on puzzles used in
Linguistic Olympiads for high school students. We first extend the existing
benchmark for the task of solving linguistic puzzles. We explore the use of
Large Language Models (LLMs), including recent state-of-the-art models such as
OpenAI's o1, for solving linguistic puzzles, analyzing their performance across
various linguistic topics. We demonstrate that LLMs outperform humans on most
puzzles types, except for those centered on writing systems, and for the
understudied languages. We use the insights from puzzle-solving experiments to
direct the novel task of puzzle generation. We believe that automating puzzle
generation, even for relatively simple puzzles, holds promise for expanding
interest in linguistics and introducing the field to a broader audience. This
finding highlights the importance of linguistic puzzle generation as a research
task: such puzzles can not only promote linguistics but also support the
dissemination of knowledge about rare and understudied languages.

</details>


### [193] [ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language Models](https://arxiv.org/abs/2509.21826)
*Zihan Lin,Xiaohan Wang,Jie Cao,Jiajun Chai,Guojun Yin,Wei Lin,Ran He*

Main category: cs.CL

TL;DR: 本文提出了ResT算法，通过熵信息优化大语言模型在工具使用任务中的策略梯度，从而提升其推理和任务完成能力。新方法在多个数据集上实现了性能提升，并超越了一些业界强基线。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型作为自主体使用外部工具时，采用强化学习优化策略，但普遍仅依赖稀疏奖励且忽视工具使用任务的特殊性，导致训练效率低、策略梯度方差大。作者希望提升工具使用任务中的训练效率与性能。

Method: 文章提出了ResT（Reshaped Token-level policy gradients）算法，利用基于熵的token权重调整，逐步增强推理型token的重要性，缓解了结构性与语义性之间的训练矛盾，提升了多回合任务中策略优化的稳定性与收敛速度。

Result: 在BFCL和API-Bank基准上，ResT算法表现优异，效果超过以往方法高达8.76%。在一个4B规模自有模型上微调后，在单回合和多回合任务中也分别超越了GPT-4o达4.11%和1.50%。

Conclusion: ResT利用熵感知机制优化了大模型的工具使用学习过程，大幅提升了策略梯度RL训练的效率和有效性，展现了新颖理论与实际性能双向突破的价值。

Abstract: Large language models (LLMs) transcend passive generation and act as
goal-directed agents by invoking external tools. Reinforcement learning (RL)
offers a principled framework for optimizing these emergent tool-use policies,
yet the prevailing paradigm relies exclusively on sparse outcome rewards and
lacks consideration of the particularity of tool-use tasks, inflating
policy-gradient variance and resulting in inefficient training. To better
understand and address these challenges, we first establish a theoretical link
between policy entropy and training stability of tool-use tasks, which reveals
that structured, low-entropy tokens are primary determinants of rewards.
Motivated by this insight, we propose \textbf{Res}haped \textbf{T}oken-level
policy gradients (\textbf{ResT}) for tool-use tasks. ResT reshapes the policy
gradient through entropy-informed token reweighting, progressively upweighting
reasoning tokens as training proceeds. This entropy-aware scheme enables a
smooth shift from structural correctness to semantic reasoning and stabilizes
convergence in multi-turn tool-use tasks. Evaluation on BFCL and API-Bank shows
that ResT achieves state-of-the-art results, outperforming prior methods by up
to $8.76\%$. When fine-tuned on a 4B base LLM, ResT further surpasses GPT-4o by
$4.11\%$ on single-turn tasks and $1.50\%$ on multi-turn base tasks.

</details>


### [194] [Semantic Agreement Enables Efficient Open-Ended LLM Cascades](https://arxiv.org/abs/2509.21837)
*Duncan Soiffer,Steven Kolawole,Virginia Smith*

Main category: cs.CL

TL;DR: 提出了一种基于语义一致性信号的级联系统，无需额外训练即可高效地在多模型间分配LLM生成任务，在降低成本和延迟的同时保证输出质量。


<details>
  <summary>Details</summary>
Motivation: 级联系统可以用小模型处理简单请求，仅在必要时调用大模型，以降低成本。然而，在开放式文本生成任务中，如何判断输出的可靠性十分困难，特别是输出质量连续且可能有多种合理结果。

Method: 作者提出用“语义一致性”（ensemble模型之间在意义层面的共识）来判断输出是否可靠，无需训练，通过比较多个模型的文本输出在语义层面的相似度决定是否需要递交给更大的模型处理。

Result: 在500M到70B参数规模模型上的实验表明：利用语义级联系统，在降低40%成本、缩短60%延迟的前提下，输出质量可达到甚至超越目标大模型。

Conclusion: 该方法无需访问模型内部信息，不依赖训练，兼容黑盒API，对模型更新具有鲁棒性，是实际应用中部署LLM系统的一种实用基线。

Abstract: Cascade systems route computational requests to smaller models when possible
and defer to larger models only when necessary, offering a promising approach
to balance cost and quality in LLM deployment. However, they face a fundamental
challenge in open-ended text generation: determining output reliability when
generation quality lies on a continuous spectrum, often with multiple valid
responses. To address this, we propose semantic agreement -- meaning-level
consensus between ensemble outputs -- as a training-free signal for reliable
deferral. We show that when diverse model outputs agree semantically, their
consensus is a stronger reliability signal than token-level confidence.
Evaluated from 500M to 70B-parameter models, we find that semantic cascades
match or surpass target-model quality at 40% of the cost and reduce latency by
up to 60%. Our method requires no model internals, works across black-box APIs,
and remains robust to model updates, making it a practical baseline for
real-world LLM deployment.

</details>


### [195] [Following the TRACE: A Structured Path to Empathetic Response Generation with Multi-Agent Models](https://arxiv.org/abs/2509.21849)
*Ziqi Liu,Ziyang Zhou,Yilin Li,Haiyang Zhang,Yangbin Chen*

Main category: cs.CL

TL;DR: 提出TRACE框架，将同理心响应生成任务分解为分析与生成两个阶段，在兼顾分析深度与自然流畅性的同时提升同理心对话系统表现。实验证明该方法优于现有强基线。


<details>
  <summary>Details</summary>
Motivation: 当前同理心对话领域在专用模型的深度分析与大语言模型的生成流畅性之间存在权衡，难以两者兼得。亟需一种方法同时获得丰富理解和自然生成能力。

Method: 提出TRACE框架，将同理心对话任务结构化为分析与生成两个子任务，先进行充分的情感与语境分析，再进行同理表达的自然文本生成。这样融合了深度分析和流畅生成优势。

Result: 实验结合自动评测和LLM评测，两方面结果显示TRACE在同理性和表达流畅性上均显著优于现有主流基线方法。

Conclusion: 结构化、分解式的同理心建模是发展更强同理心对话系统的有效新路径，有望提升系统解释性与能力。

Abstract: Empathetic response generation is a crucial task for creating more human-like
and supportive conversational agents. However, existing methods face a core
trade-off between the analytical depth of specialized models and the generative
fluency of Large Language Models (LLMs). To address this, we propose TRACE,
Task-decomposed Reasoning for Affective Communication and Empathy, a novel
framework that models empathy as a structured cognitive process by decomposing
the task into a pipeline for analysis and synthesis. By building a
comprehensive understanding before generation, TRACE unites deep analysis with
expressive generation. Experimental results show that our framework
significantly outperforms strong baselines in both automatic and LLM-based
evaluations, confirming that our structured decomposition is a promising
paradigm for creating more capable and interpretable empathetic agents. Our
code is available at https://anonymous.4open.science/r/TRACE-18EF/README.md.

</details>


### [196] [KnowMT-Bench: Benchmarking Knowledge-Intensive Long-Form Question Answering in Multi-Turn Dialogues](https://arxiv.org/abs/2509.21856)
*Junhao Chen,Yu Huang,Siyuan Li,Rui Yao,Hanqian Li,Hanyu Zhang,Jungang Li,Jian Chen,Bowen Wang,Xuming Hu*

Main category: cs.CL

TL;DR: 该论文提出了第一个系统性评测大模型在多轮长文本问答中的知识推理能力的基准KnowMT-Bench，并展示了多轮历史将造成事实性和信息效率的下降，同时提出了RAG可以缓解这些问题。


<details>
  <summary>Details</summary>
Motivation: 虽然多轮长文本问答是大模型在知识密集型领域（医疗、金融、法律等）的重要应用场景，但现有基准多关注单轮问答或其它能力，缺乏针对知识密集型多轮对话事实性评测的公开基准。为解决此评测空白，该论文提出新基准。

Method: 提出KnowMT-Bench多轮知识问答基准，涵盖多个知识密集领域。采用动态评测设置：大模型需根据逻辑递进的问题序列自行生成所有多轮对话历史，再对最终一轮答案的事实性与信息传递效率进行自动化且人工验证的评测。并测试不同缓解策略（如RAG）。

Result: 实验证明，在多轮对话背景下，大模型的事实能力因自生成历史的噪声而下降，且随对话轮次变多，答案更冗长，信息效率降低。RAG方法可以有效缓解甚至逆转这一退化趋势。

Conclusion: KnowMT-Bench为评估和改进大模型在知识密集型真实应用场合的多轮事实对话能力提供了重要工具。多轮场景下大模型表现有明显下降，但可以被RAG等策略缓解。

Abstract: Multi-Turn Long-Form Question Answering (MT-LFQA) is a key application
paradigm of Large Language Models (LLMs) in knowledge-intensive domains.
However, existing benchmarks are limited to single-turn dialogue, while
multi-turn dialogue benchmarks typically assess other orthogonal capabilities
rather than knowledge-intensive factuality. To bridge this critical gap, we
introduce \textbf{KnowMT-Bench}, the \textit{first-ever} benchmark designed to
systematically evaluate MT-LFQA for LLMs across knowledge-intensive fields,
including medicine, finance, and law. To faithfully assess the model's
real-world performance, KnowMT-Bench employs a dynamic evaluation setting where
models generate their own multi-turn dialogue histories given logically
progressive question sequences. The factual capability and information delivery
efficiency of the \textit{final-turn} answer are then evaluated using a
human-validated automated pipeline. Our experiments reveal that multi-turn
contexts degrade performance: factual capability declines due to the contextual
noise from self-generated histories, while information efficiency drops as
models become more verbose with increasing dialogue length. We then investigate
mitigation strategies, demonstrating that retrieval-augmented generation (RAG)
can effectively alleviate and even reverse this factual degradation. These
findings underscore the importance of our benchmark in evaluating and enhancing
the conversational factual capabilities of LLMs in real-world
knowledge-intensive applications. Code is available at
\href{https://github.com/hardenyu21/KnowMT-Bench}{\textcolor{cyan}{\texttt{KnowMT-Bench}}}.

</details>


### [197] [Enhancing Low-Rank Adaptation with Structured Nonlinear Transformations](https://arxiv.org/abs/2509.21870)
*Guanzhi Deng,Mingyang Liu,Dapeng Wu,Yinqiao Li,Linqi Song*

Main category: cs.CL

TL;DR: 本文提出了LoRAN和Sinter两种方法，改进大模型低秩微调（LoRA），通过非线性变换提升表达能力，实现参数高效同时提升下游任务效果。


<details>
  <summary>Details</summary>
Motivation: 传统的LoRA方法由于是线性的，表达能力受到限制，难以进一步提升模型性能。为了解决这一问题，作者试图引入非线性能力以增强微调方法的表达性。

Method: LoRAN是对LoRA的非线性扩展，通过对低秩矩阵引入轻量级的非线性变换，提升表达能力。同时提出的Sinter是一种基于正弦的激活函数，在不增加参数量的情况下增强模型结构扰动能力。

Result: 在文本摘要和分类等任务上，LoRAN在实验中表现优于现有的QLoRA方法。消融实验进一步发现，Sinter激活函数比常用的Sigmoid、ReLU和Tanh更优。

Conclusion: 非线性设计（LoRAN）和创新激活函数（Sinter）能提升参数高效微调方法的效果，激活函数的设计对低秩微调非常关键。

Abstract: Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient
fine-tuning method for large language models. However, its linear nature limits
expressiveness. We propose LoRAN, a non-linear extension of LoRA that applies
lightweight transformations to the low-rank updates. We further introduce
Sinter, a sine-based activation that adds structured perturbations without
increasing parameter count. Experiments across summarization and classification
tasks show that LoRAN consistently improves over QLoRA. Ablation studies reveal
that Sinter outperforms standard activations such as Sigmoid, ReLU, and Tanh,
highlighting the importance of activation design in lowrank tuning.

</details>


### [198] [LUMINA: Detecting Hallucinations in RAG System with Context-Knowledge Signals](https://arxiv.org/abs/2509.21875)
*Min-Hsuan Yeh,Yixuan Li,Tanwi Mallick*

Main category: cs.CL

TL;DR: 本文提出了一种名为LUMINA的新框架，能更有效地检测RAG系统中的幻觉现象，且无需大量参数调优，实验结果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG有助于缓解大模型的幻觉问题，但即使提供了准确的上下文，模型仍会出现幻觉，部分原因是模型内部知识与外部检索信息利用不均。现有检测方法泛化差、参数调优负担重，因此需要新的、更实用的检测框架。

Method: LUMINA框架通过两个方面监测幻觉：(1) 利用分布距离度量模型对外部上下文的使用程度；(2) 追踪transformer层中预测token的演化，衡量模型内部知识的利用。此外，还提出了统计方法来验证上述测量方式。

Result: 在多项RAG幻觉基准与四个开源LLM上的实验表明，LUMINA在AUROC与AUPRC指标上均显著优于现有基于利用率的方法，在HalluRAG基准上AUROC提升达13%。在检索质量较低或模型-检索器不匹配等更宽松场景下，LUMINA仍表现稳健。

Conclusion: LUMINA为检测RAG系统幻觉提供了一种准确、稳定且易用的方法，优于现有方法且适用性强，为改进LLM实际应用中的可靠性提供了新工具。

Abstract: Retrieval-Augmented Generation (RAG) aims to mitigate hallucinations in large
language models (LLMs) by grounding responses in retrieved documents. Yet,
RAG-based LLMs still hallucinate even when provided with correct and sufficient
context. A growing line of work suggests that this stems from an imbalance
between how models use external context and their internal knowledge, and
several approaches have attempted to quantify these signals for hallucination
detection. However, existing methods require extensive hyperparameter tuning,
limiting their generalizability. We propose LUMINA, a novel framework that
detects hallucinations in RAG systems through context-knowledge signals:
external context utilization is quantified via distributional distance, while
internal knowledge utilization is measured by tracking how predicted tokens
evolve across transformer layers. We further introduce a framework for
statistically validating these measurements. Experiments on common RAG
hallucination benchmarks and four open-source LLMs show that LUMINA achieves
consistently high AUROC and AUPRC scores, outperforming prior utilization-based
methods by up to +13% AUROC on HalluRAG. Moreover, LUMINA remains robust under
relaxed assumptions about retrieval quality and model matching, offering both
effectiveness and practicality.

</details>


### [199] [No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping](https://arxiv.org/abs/2509.21880)
*Thanh-Long V. Le,Myeongho Jeon,Kim Vu,Viet Lai,Eunho Yang*

Main category: cs.CL

TL;DR: 提出了一种新算法RL-ZVP，能有效利用以往被忽视的“零方差提示”数据，提升大型语言模型在数学推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法只关注模型对同一输入表现不同正确性的情况，而忽略了所有响应得分一致（零方差）的提示，这部分数据被认为无用。本研究指出这些信息也可以为模型学习带来价值。

Method: 提出了RL-ZVP算法，该方法能直接基于正确性奖励和错误惩罚，即使不存在对比响应。同时，算法结合token级别的特征，保留更多有用和细致的学习信号。

Result: 在六个数学推理基准上，RL-ZVP相较于GRPO，准确率提升最高可达8.61点，通过率提升最高可达7.77点，并在与其他只过滤零方差提示的基线方法对比中始终表现更好。

Conclusion: 零方差提示在RLVR中并非无用信息，充分挖掘此类数据能明显提升模型性能，建议后续算法充分利用该类数据。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework
for improving the reasoning abilities of Large Language Models (LLMs). However,
current methods such as GRPO rely only on problems where the model responses to
the same input differ in correctness, while ignoring those where all responses
receive the same reward - so-called zero-variance prompts. In this work, we
argue that such prompts are not useless but can, in fact, provide meaningful
feedback for policy optimization. To this end, we introduce RL with
Zero-Variance Prompts (RL-ZVP), a novel algorithm that extract learning signals
from zero-variance prompts. RL-ZVP directly rewards correctness and penalizes
errors even without contrasting responses, modulating feedback with token-level
characteristics to preserve informative, nuanced signals. Across six math
reasoning benchmarks, RL-ZVP achieves significant improvements of up to 8.61
points in accuracy and 7.77 points in pass rate over GRPO, while consistently
outperforming other baselines that filter out zero-variance prompts. These
results highlight the untapped potential of learning from zero-variance prompts
in RLVR.

</details>


### [200] [QoNext: Towards Next-generation QoE for Foundation Models](https://arxiv.org/abs/2509.21889)
*Yijin Guo,Ye Shen,Farong Wen,Junying Wang,Zicheng Zhang,Qi Jia,Guangtao Zhai*

Main category: cs.CL

TL;DR: 论文提出了QoNext，一个基于用户体验质量（QoE）理念的新评测框架，用于更准确地评估基础模型在真实用户交互下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的基础模型评测方法大多只关注输出正确性，忽略了用户在与模型交互过程中的真实体验。这导致评测结果未能反映实际用户满意度和体验机制，限制了模型优化和应用。

Method: 作者借鉴了网络和多媒体领域的用户体验质量（QoE）评价原则，构建了QoNext框架。该框架通过设计受控实验，系统性地收集人类在不同配置下的评分，建立了关注用户体验因素的数据库，并基于这些数据训练了用于预测用户体验评分的模型。

Result: QoNext框架生成了一个基于用户体验的评测数据库，并通过实验证明可以主动、细粒度地评测基础模型，预测用户体验分数。同时，模型能够为产品化服务提供优化建议。

Conclusion: QoNext框架能够更加全面地反映用户与基础模型交互时的真实体验，有助于基础模型的实用化优化，为实际应用提供关键指导。

Abstract: Existing evaluations of foundation models, including recent human-centric
approaches, fail to capture what truly matters: user's experience during
interaction. Current methods treat evaluation as a matter of output correctness
alone, overlooking that user satisfaction emerges from the interplay between
response quality and interaction, which limits their ability to account for the
mechanisms underlying user experience. To address this gap, we introduce
QoNext, the first framework that adapts Quality of Experience (QoE) principles
from networking and multimedia to the assessment of foundation models. QoNext
identifies experiential factors that shape user experience and incorporates
them into controlled experiments, where human ratings are collected under
varied configurations. From these studies we construct a QoE-oriented database
and train predictive models that estimate perceived user experience from
measurable system parameters. Our results demonstrate that QoNext not only
enables proactive and fine-grained evaluation but also provides actionable
guidance for productized services of optimizing foundation models in practice.

</details>


### [201] [Elastic MoE: Unlocking the Inference-Time Scalability of Mixture-of-Experts](https://arxiv.org/abs/2509.21892)
*Naibin Gu,Zhenyu Zhang,Yuchen Feng,Yilong Chen,Peng Fu,Zheng Lin,Shuohuan Wang,Yu Sun,Hua Wu,Weiping Wang,Haifeng Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的Elastic Mixture-of-Experts (EMoE)训练框架，使得Mixture-of-Experts模型在推理时可以激活不同数量的专家，并有效扩展了可用专家数量对性能的提升区间。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型在训练和推理时激活的专家数k是固定的。直觉上推理时激活更多专家应提升性能，但实际发现提升幅度很小，甚至性能急剧下降。原因在于专家之间缺乏有效协作能力。

Method: 提出EMoE训练框架，在训练阶段通过多样化组合激活专家，并优化router选择质量，促使专家学会协同工作。这一方法无需增加训练开销。

Result: 实验证明，EMoE将推理时可用专家数范围扩展到训练时的2-3倍，且取得了比传统MoE更高的峰值性能。

Conclusion: EMoE有效解决了原MoE扩展专家数时性能退化的问题，使模型在不同算力预算下表现更加稳健，且能够充分利用更多专家的潜力。

Abstract: Mixture-of-Experts (MoE) models typically fix the number of activated experts
$k$ at both training and inference. Intuitively, activating more experts at
inference $k'$ (where $k'> k$) means engaging a larger set of model parameters
for the computation and thus is expected to improve performance. However,
contrary to this intuition, we find the scaling range to be so narrow that
performance begins to degrade rapidly after only a slight increase in the
number of experts. Further investigation reveals that this degradation stems
from a lack of learned collaboration among experts. To address this, we
introduce Elastic Mixture-of-Experts (EMoE), a novel training framework that
enables MoE models to scale the number of activated experts at inference
without incurring additional training overhead. By simultaneously training
experts to collaborate in diverse combinations and encouraging the router for
high-quality selections, EMoE ensures robust performance across computational
budgets at inference. We conduct extensive experiments on various MoE settings.
Our results show that EMoE significantly expands the effective
performance-scaling range, extending it to as much as 2-3$\times$ the
training-time $k$, while also pushing the model's peak performance to a higher
level.

</details>


### [202] [A Large-Scale Dataset and Citation Intent Classification in Turkish with LLMs](https://arxiv.org/abs/2509.21907)
*Kemal Sami Karaca,Bahaeddin Eravcı*

Main category: cs.CL

TL;DR: 本文提出了适用于土耳其语学术引用意图识别的新方法和数据集，显著提升了分类准确率，达到91.3%。


<details>
  <summary>Details</summary>
Motivation: 现有学术引用意图识别研究多聚焦于英语，对构词复杂的土耳其语未能提供有效支持，缺乏高质量数据集和合适的分类方法。因此，亟需为土耳其语开发基础数据集和系统化的方法。

Method: 首先，构建并发布了首个土耳其语引用意图标注数据集。然后，评估了基于LLMs的In-Context Learning（ICL）标准方法，发现其容易因手动设计Prompt而表现不稳。为解决该问题，引入了基于DSPy框架的可编程分类流水线，自动优化Prompt，并结合多模型融合（Stacking，XGBoost为元模型）以保证稳定可靠的最终分类。

Result: 所提出的集成模型在土耳其语引用意图分类任务上取得了91.3%的当前最佳准确率，明显优于传统方法。

Conclusion: 本文为土耳其自然语言处理领域及学术界提供了高质量的基础数据集和稳健的分类框架，为未来更广泛的引用意图研究打下坚实基础。

Abstract: Understanding the qualitative intent of citations is essential for a
comprehensive assessment of academic research, a task that poses unique
challenges for agglutinative languages like Turkish. This paper introduces a
systematic methodology and a foundational dataset to address this problem. We
first present a new, publicly available dataset of Turkish citation intents,
created with a purpose-built annotation tool. We then evaluate the performance
of standard In-Context Learning (ICL) with Large Language Models (LLMs),
demonstrating that its effectiveness is limited by inconsistent results caused
by manually designed prompts. To address this core limitation, we introduce a
programmable classification pipeline built on the DSPy framework, which
automates prompt optimization systematically. For final classification, we
employ a stacked generalization ensemble to aggregate outputs from multiple
optimized models, ensuring stable and reliable predictions. This ensemble, with
an XGBoost meta-model, achieves a state-of-the-art accuracy of 91.3\%.
Ultimately, this study provides the Turkish NLP community and the broader
academic circles with a foundational dataset and a robust classification
framework paving the way for future qualitative citation studies.

</details>


### [203] [AutoSCORE: Enhancing Automated Scoring with Multi-Agent Large Language Models via Structured Component Recognition](https://arxiv.org/abs/2509.21910)
*Yun Wang,Zhaojun Ding,Xuansheng Wu,Siyue Sun,Ninghao Liu,Xiaoming Zhai*

Main category: cs.CL

TL;DR: 本论文提出了一种基于多智能体的大模型自动评分系统AutoSCORE，显著提升了自动评分的准确性、可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型用于自动评分时面临准确率低、提示敏感、难以解释及评价标准（rubric）不匹配等问题，阻碍了其在教育测评实践中的应用。

Method: 提出AutoSCORE多智能体框架：首先由组件抽取Agent提取学生答卷中与评分标准对齐的结构化信息，再由评分Agent基于该结构化信息评分。该方法模仿人类阅卷流程，提高了模型对评分标准的遵循度和解释性。

Result: 在ASAP基准四个数据集上，使用多种LLM，包括GPT-4o和LLaMA-3.1系列进行测试。AutoSCORE无论在评分准确率、与人工评分一致度（QWK、相关系数）还是错误指标（MAE、RMSE）上都优于单Agent方法，尤其是复杂多维评分标准和小模型上提升更明显。

Conclusion: 结构化组件识别结合多智能体设计为自动评分提供了可扩展、可靠且可解释的解决方案，推动了LLM自动评分系统在实用层面的发展。

Abstract: Automated scoring plays a crucial role in education by reducing the reliance
on human raters, offering scalable and immediate evaluation of student work.
While large language models (LLMs) have shown strong potential in this task,
their use as end-to-end raters faces challenges such as low accuracy, prompt
sensitivity, limited interpretability, and rubric misalignment. These issues
hinder the implementation of LLM-based automated scoring in assessment
practice. To address the limitations, we propose AutoSCORE, a multi-agent LLM
framework enhancing automated scoring via rubric-aligned Structured COmponent
REcognition. With two agents, AutoSCORE first extracts rubric-relevant
components from student responses and encodes them into a structured
representation (i.e., Scoring Rubric Component Extraction Agent), which is then
used to assign final scores (i.e., Scoring Agent). This design ensures that
model reasoning follows a human-like grading process, enhancing
interpretability and robustness. We evaluate AutoSCORE on four benchmark
datasets from the ASAP benchmark, using both proprietary and open-source LLMs
(GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B). Across diverse tasks and rubrics,
AutoSCORE consistently improves scoring accuracy, human-machine agreement (QWK,
correlations), and error metrics (MAE, RMSE) compared to single-agent
baselines, with particularly strong benefits on complex, multi-dimensional
rubrics, and especially large relative gains on smaller LLMs. These results
demonstrate that structured component recognition combined with multi-agent
design offers a scalable, reliable, and interpretable solution for automated
scoring.

</details>


### [204] [SimulSense: Sense-Driven Interpreting for Efficient Simultaneous Speech Translation](https://arxiv.org/abs/2509.21932)
*Haotian Tan,Hiroki Ouchi,Sakriani Sakti*

Main category: cs.CL

TL;DR: 本文提出SimulSense框架，通过模拟人类口译员的决策方式，实现更高效的同声传译决策，并在质量和实时性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前同声传译系统依赖多回合对话建模和大型语言模型，存在效率低、对训练数据和计算资源需求高等问题。作者希望找到更接近人类口译员决策行为、同时能提高实时效率的方法。

Method: 提出SimulSense框架，持续读取输入语音，并在感知到新的语义单元时，像人类译员一样触发翻译输出决策，无需复杂对话训练数据和昂贵的LLM推断。

Result: 在与两种最新基线系统的对比实验中，SimulSense框架在翻译质量与延迟间取得更佳权衡，且决策过程速度比基线快9.6倍。

Conclusion: SimulSense显著提升了同声传译的实时性和效率，为人类口译风格的自动决策提供了有效路径。

Abstract: How to make human-interpreter-like read/write decisions for simultaneous
speech translation (SimulST) systems? Current state-of-the-art systems
formulate SimulST as a multi-turn dialogue task, requiring specialized
interleaved training data and relying on computationally expensive large
language model (LLM) inference for decision-making. In this paper, we propose
SimulSense, a novel framework for SimulST that mimics human interpreters by
continuously reading input speech and triggering write decisions to produce
translation when a new sense unit is perceived. Experiments against two
state-of-the-art baseline systems demonstrate that our proposed method achieves
a superior quality-latency tradeoff and substantially improved real-time
efficiency, where its decision-making is up to 9.6x faster than the baselines.

</details>


### [205] [Why Chain of Thought Fails in Clinical Text Understanding](https://arxiv.org/abs/2509.21933)
*Jiageng Wu,Kevin Xie,Bowen Gu,Nils Krüger,Kueiyu Joshua Lin,Jie Yang*

Main category: cs.CL

TL;DR: 本研究首次大规模系统性评估了Chain-of-Thought（CoT）提示在临床文本理解（特别是电子健康记录）中的表现，对95种先进大模型在87项真实临床任务进行测试，发现CoT在大多数模型中会导致性能下降，提出了CoT在提升可解释性的同时可能损害模型可靠性的临床领域悖论。


<details>
  <summary>Details</summary>
Motivation: 临床应用对AI模型的准确性与推理透明性要求极高，CoT技术已在其他领域提升模型表现和可解释性，但其在结构复杂、噪声多、跨语言的临床文本中的效果尚不明确，尤其缺乏系统性大规模评估。

Method: 对95个大语言模型在87个涵盖9种语言、8种任务类型的临床文本任务中，分别用CoT提示和不带CoT提示进行系统性对比测试，并通过模型自评和临床专家评估从推理长度、医疗概念对齐和错误类型等角度细致分析。

Result: 86.3%的模型在CoT提示下表现出系统性的性能下降，较强的模型影响较小，较弱模型性能大幅退化。详细分析揭示了CoT失败的具体模式和原因。

Conclusion: CoT尽管提升了模型推理的可解释性，但却可能降低其在临床文本任务中的可靠性，形成临床人工智能部署中的一个重要悖论，未来需着重研究兼顾透明性与可靠性的推理方法。

Abstract: Large language models (LLMs) are increasingly being applied to clinical care,
a domain where both accuracy and transparent reasoning are critical for safe
and trustworthy deployment. Chain-of-thought (CoT) prompting, which elicits
step-by-step reasoning, has demonstrated improvements in performance and
interpretability across a wide range of tasks. However, its effectiveness in
clinical contexts remains largely unexplored, particularly in the context of
electronic health records (EHRs), the primary source of clinical documentation,
which are often lengthy, fragmented, and noisy. In this work, we present the
first large-scale systematic study of CoT for clinical text understanding. We
assess 95 advanced LLMs on 87 real-world clinical text tasks, covering 9
languages and 8 task types. Contrary to prior findings in other domains, we
observe that 86.3\% of models suffer consistent performance degradation in the
CoT setting. More capable models remain relatively robust, while weaker ones
suffer substantial declines. To better characterize these effects, we perform
fine-grained analyses of reasoning length, medical concept alignment, and error
profiles, leveraging both LLM-as-a-judge evaluation and clinical expert
evaluation. Our results uncover systematic patterns in when and why CoT fails
in clinical contexts, which highlight a critical paradox: CoT enhances
interpretability but may undermine reliability in clinical text tasks. This
work provides an empirical basis for clinical reasoning strategies of LLMs,
highlighting the need for transparent and trustworthy approaches.

</details>


### [206] [Debiasing Large Language Models in Thai Political Stance Detection via Counterfactual Calibration](https://arxiv.org/abs/2509.21946)
*Kasidit Sermsri,Teerapong Panboonyuen*

Main category: cs.CL

TL;DR: 本文提出了ThaiFACTUAL框架，专为低资源、文化复杂环境下的泰国政治立场检测设计，通过模型不可知的校准方法显著消除大语言模型的政治偏见。此外，作者还发布了首个高质量的泰国政治立场数据集。


<details>
  <summary>Details</summary>
Motivation: 泰国政治环境语言表达间接、情感与立场纠缠，且存在人物极化，致使大语言模型在立场检测时出现情感外泄和偏袒实体等偏见，影响公平性和可靠性。需开发有效的去偏方法，提升低资源语言的模型表现。

Method: 提出ThaiFACTUAL校准框架：无需微调，结合反事实数据增强和基于理据的监督，有效区分情感与立场并减少偏见。同时，构建并公开包含立场、情感、理据及偏见标注的泰国政治数据集。

Result: 实验结果表明，ThaiFACTUAL在多种大语言模型中显著降低了伪相关性，提高了零样本泛化能力和公平性。

Conclusion: ThaiFACTUAL为低资源、复杂文化环境的立场检测提供了实用的无模型依赖去偏方法，对欠代表性语言的公平性和可靠性具有重要意义。

Abstract: Political stance detection in low-resource and culturally complex settings
poses a critical challenge for large language models (LLMs). In the Thai
political landscape - marked by indirect language, polarized figures, and
entangled sentiment and stance - LLMs often display systematic biases such as
sentiment leakage and favoritism toward entities. These biases undermine
fairness and reliability. We present ThaiFACTUAL, a lightweight, model-agnostic
calibration framework that mitigates political bias without requiring
fine-tuning. ThaiFACTUAL uses counterfactual data augmentation and
rationale-based supervision to disentangle sentiment from stance and reduce
bias. We also release the first high-quality Thai political stance dataset,
annotated with stance, sentiment, rationales, and bias markers across diverse
entities and events. Experimental results show that ThaiFACTUAL significantly
reduces spurious correlations, enhances zero-shot generalization, and improves
fairness across multiple LLMs. This work highlights the importance of
culturally grounded debiasing techniques for underrepresented languages.

</details>


### [207] [MotivGraph-SoIQ: Integrating Motivational Knowledge Graphs and Socratic Dialogue for Enhanced LLM Ideation](https://arxiv.org/abs/2509.21978)
*Xinping Lei,Tong Zhou,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 本文提出了MotivGraph-SoIQ框架，通过引入动机知识图谱和苏格拉底式对话来提升大语言模型（LLM）在学术创意生成过程中的基础性和创新性。该方法显著优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在学术创意生成方面虽有潜力，但存在创意缺乏基础依托与易于陷入确认偏误两大问题。作者旨在通过结构化的知识注入与严谨的互动探究来改善这些不足。

Method: 提出MotivGraph-SoIQ框架，将动机知识图谱（含问题、挑战、解决方案三类节点）与Q驱动的双主体苏格拉底式问答系统结合，为LLM创意生成提供结构化支撑及多轮严密推敲。

Result: 在ICLR25论文主题数据集上，MotivGraph-SoIQ在LLM自动评分、ELO排序和人工评价等多项指标上显著优于现有最先进方法。

Conclusion: 通过融合动机知识图谱与苏格拉底式问答，MotivGraph-SoIQ有效提升了LLM学术创意的创新性、实验严密性和动机合理性，具备实用价值。

Abstract: Large Language Models (LLMs) hold substantial potential for accelerating
academic ideation but face critical challenges in grounding ideas and
mitigating confirmation bias for further refinement. We propose integrating
motivational knowledge graphs and socratic dialogue to address these
limitations in enhanced LLM ideation (MotivGraph-SoIQ). This novel framework
provides essential grounding and practical idea improvement steps for LLM
ideation by integrating a Motivational Knowledge Graph (MotivGraph) with a
Q-Driven Socratic Ideator. The MotivGraph structurally stores three key node
types(problem, challenge and solution) to offer motivation grounding for the
LLM ideation process. The Ideator is a dual-agent system utilizing Socratic
questioning, which facilitates a rigorous refinement process that mitigates
confirmation bias and improves idea quality across novelty, experimental rigor,
and motivational rationality dimensions. On the ICLR25 paper topics dataset,
MotivGraph-SoIQ exhibits clear advantages over existing state-of-the-art
approaches across LLM-based scoring, ELO ranking, and human evaluation metrics.

</details>


### [208] [Black-Box Hallucination Detection via Consistency Under the Uncertain Expression](https://arxiv.org/abs/2509.21999)
*Seongho Joo,Kyungmin Min,Jahyun Koo,Kyomin Jung*

Main category: cs.CL

TL;DR: 本文提出了一种无需访问LLM内部状态或外部资源的全黑盒幻觉检测指标，基于模型在表达不确定性时的行为一致性，实验表明该方法在检测事实性上的表现优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 由于现有幻觉检测方法依赖LLM内部分布或外部知识，而这些往往难以获取，因此亟需一种只基于模型输入输出行为（即黑盒方法）的可靠幻觉检测指标。

Method: 通过分析大模型在表达不确定性时输出的一致性，发现事实性强的答案表现更一致，由此设计基于不确定性表达下答案一致性的黑盒检测指标。无须访问模型内部概率或依赖外部资源。

Result: 实验证明，所提黑盒指标在预测模型输出真实性方面优于依赖模型内部信息的现有基线方法。

Conclusion: 对LLM进行全黑盒幻觉检测是可行且有效的，基于输出一致性的不确定性表达方法为实践中大模型幻觉检测提供了新思路和工具。

Abstract: Despite the great advancement of Language modeling in recent days, Large
Language Models (LLMs) such as GPT3 are notorious for generating non-factual
responses, so-called "hallucination" problems. Existing methods for detecting
and alleviating this hallucination problem require external resources or the
internal state of LLMs, such as the output probability of each token. Given the
LLM's restricted external API availability and the limited scope of external
resources, there is an urgent demand to establish the Black-Box approach as the
cornerstone for effective hallucination detection. In this work, we propose a
simple black-box hallucination detection metric after the investigation of the
behavior of LLMs under expression of uncertainty. Our comprehensive analysis
reveals that LLMs generate consistent responses when they present factual
responses while non-consistent responses vice versa. Based on the analysis, we
propose an efficient black-box hallucination detection metric with the
expression of uncertainty. The experiment demonstrates that our metric is more
predictive of the factuality in model responses than baselines that use
internal knowledge of LLMs.

</details>


### [209] [GraphSearch: An Agentic Deep Searching Workflow for Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2509.22009)
*Cehao Yang,Xiaojun Wu,Xueyuan Lin,Chengjin Xu,Xuhui Jiang,Yuanliang Sun,Jia Li,Hui Xiong,Jian Guo*

Main category: cs.CL

TL;DR: 本文提出了一种名为GraphSearch的新方法，通过模块化和双通道检索提升基于图的检索增强生成( GraphRAG)在复杂推理任务中的效果。


<details>
  <summary>Details</summary>
Motivation: 现有GraphRAG方法存在两个主要问题：检索不深入导致关键信息遗漏，以及未能高效利用结构化图数据，影响复杂查询的推理效果。

Method: 提出GraphSearch，一种包括六个模块的深度搜索工作流，支持多轮互动与迭代推理。采用双通道检索策略，分别对文本数据和结构化图数据进行语义与关联查询，实现多模态互补信息的充分利用。

Result: 在六个多跳RAG基准测试中，GraphSearch均提升了答案准确率和生成结果质量，相较于传统策略表现更优。

Conclusion: GraphSearch能够有效提升图检索增强生成的推理能力，是推动该领域发展的有前景方向。

Abstract: Graph Retrieval-Augmented Generation (GraphRAG) enhances factual reasoning in
LLMs by structurally modeling knowledge through graph-based representations.
However, existing GraphRAG approaches face two core limitations: shallow
retrieval that fails to surface all critical evidence, and inefficient
utilization of pre-constructed structural graph data, which hinders effective
reasoning from complex queries. To address these challenges, we propose
\textsc{GraphSearch}, a novel agentic deep searching workflow with dual-channel
retrieval for GraphRAG. \textsc{GraphSearch} organizes the retrieval process
into a modular framework comprising six modules, enabling multi-turn
interactions and iterative reasoning. Furthermore, \textsc{GraphSearch} adopts
a dual-channel retrieval strategy that issues semantic queries over chunk-based
text data and relational queries over structural graph data, enabling
comprehensive utilization of both modalities and their complementary strengths.
Experimental results across six multi-hop RAG benchmarks demonstrate that
\textsc{GraphSearch} consistently improves answer accuracy and generation
quality over the traditional strategy, confirming \textsc{GraphSearch} as a
promising direction for advancing graph retrieval-augmented generation.

</details>


### [210] [From Outliers to Topics in Language Models: Anticipating Trends in News Corpora](https://arxiv.org/abs/2509.22030)
*Evangelia Zve,Benjamin Icard,Alice Breton,Lila Sainero,Gauvain Bourgne,Jean-Gabriel Ganascia*

Main category: cs.CL

TL;DR: 本论文发现，常被忽视的主题模型离群点其实可能预示着新兴话题。通过跟踪离群点的发展，研究发现这些离群点随着时间演变成连贯的话题。


<details>
  <summary>Details</summary>
Motivation: 传统主题建模通常将离群点视为噪声而忽略，但这些“噪声”中可能隐藏着新兴的重要话题信号。因此，了解离群点的动态演变有助于及时捕捉新兴社会议题。

Method: 作者采用当前最先进的语言模型生成向量嵌入，并利用累积聚类方法，在涉企业社会责任和气候变化的法英新闻数据中跟踪离群点随时间的变化。

Result: 研究表明，不论用哪种模型、哪种语言，离群点都有演变为一致、连贯的新主题的趋势。

Conclusion: 应重视主题建模中的离群点，因为它们可能预示着未来的重要新话题，尤其是在动态新闻分析中。

Abstract: This paper examines how outliers, often dismissed as noise in topic modeling,
can act as weak signals of emerging topics in dynamic news corpora. Using
vector embeddings from state-of-the-art language models and a cumulative
clustering approach, we track their evolution over time in French and English
news datasets focused on corporate social responsibility and climate change.
The results reveal a consistent pattern: outliers tend to evolve into coherent
topics over time across both models and languages.

</details>


### [211] [Taxonomy of Comprehensive Safety for Clinical Agents](https://arxiv.org/abs/2509.22041)
*Jean Seo,Hyunkyung Lee,Gibaeg Kim,Wooseok Han,Jaehyo Yoo,Seungseop Lim,Kihun Shin,Eunho Yang*

Main category: cs.CL

TL;DR: 本论文提出了TACOS，一种针对临床虚拟助手安全需求的细致21类分类体系，实现了安全过滤和工具选择的统一。通过构建带注释数据集并进行实验，验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 临床聊天机器人若出现不准确或有害的回答会产生严重后果，现有安全保障措施难以完全应对临床环境下复杂的需求，因此需研发更精细化的方法提升安全性。

Method: 作者提出TACOS（TAxonomy of COmprehensive Safety for Clinical Agents），该分类体系涵盖21类，将安全过滤与工具调用融合为意图分类的一步操作。并整理了带TACOS注释的数据集，进行实验分析。

Result: 实验显示，TACOS分类体系在临床助手安全性保障方面具备独特价值。分析中还得出有关训练数据分布和基础模型预训练知识的有益见解。

Conclusion: 新的细致分类体系TACOS有助于提升临床对话系统的安全响应，同时为相关模型的数据分布和知识基础提供了参考。

Abstract: Safety is a paramount concern in clinical chatbot applications, where
inaccurate or harmful responses can lead to serious consequences. Existing
methods--such as guardrails and tool calling--often fall short in addressing
the nuanced demands of the clinical domain. In this paper, we introduce TACOS
(TAxonomy of COmprehensive Safety for Clinical Agents), a fine-grained,
21-class taxonomy that integrates safety filtering and tool selection into a
single user intent classification step. TACOS is a taxonomy that can cover a
wide spectrum of clinical and non-clinical queries, explicitly modeling varying
safety thresholds and external tool dependencies. To validate our framework, we
curate a TACOS-annotated dataset and perform extensive experiments. Our results
demonstrate the value of a new taxonomy specialized for clinical agent
settings, and reveal useful insights about train data distribution and
pretrained knowledge of base models.

</details>


### [212] [Fuzzy Reasoning Chain (FRC): An Innovative Reasoning Framework from Fuzziness to Clarity](https://arxiv.org/abs/2509.22054)
*Ping Chen,Xiang Liu,Zhaoxiang Liu,Zezhou Chen,Xingpeng Zhang,Huan Hu,Zipeng Wang,Kai Wang,Shuming Shi,Shiguo Lian*

Main category: cs.CL

TL;DR: 本文提出了FRC（模糊推理链）框架，通过结合大语言模型的语义先验和连续的模糊隶属度，提升对含糊、多义或不确定性文本的处理能力，并应用于情感分析任务中，理论与实证结果均表明其推理更加稳定且具有更好的知识迁移能力。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型推动了NLP发展，但面对含糊、多义或不确定性文本时仍面临困难。传统基于概率的方法难以有效处理这些问题，需要新方法增强模型的解释性和鲁棒性。

Method: FRC框架将大语言模型的语义先验与模糊隶属度结合，实现概率推理和模糊隶属推理的显式互动。利用该框架，可以逐步将模糊输入转化为清晰、可解释的决策，同时捕获传统概率方法无法处理的不确定信号。

Result: 在情感分析任务上，FRC在理论分析和实证结果中表现出推理稳定、知识迁移能力强的优势。

Conclusion: FRC为处理细腻和含糊表达提供了一种通用机制，能够提升模型的可解释性和鲁棒性。

Abstract: With the rapid advancement of large language models (LLMs), natural language
processing (NLP) has achieved remarkable progress. Nonetheless, significant
challenges remain in handling texts with ambiguity, polysemy, or uncertainty.
We introduce the Fuzzy Reasoning Chain (FRC) framework, which integrates LLM
semantic priors with continuous fuzzy membership degrees, creating an explicit
interaction between probability-based reasoning and fuzzy membership reasoning.
This transition allows ambiguous inputs to be gradually transformed into clear
and interpretable decisions while capturing conflicting or uncertain signals
that traditional probability-based methods cannot. We validate FRC on sentiment
analysis tasks, where both theoretical analysis and empirical results show that
it ensures stable reasoning and facilitates knowledge transfer across different
model scales. These findings indicate that FRC provides a general mechanism for
managing subtle and ambiguous expressions with improved interpretability and
robustness.

</details>


### [213] [RedNote-Vibe: A Dataset for Capturing Temporal Dynamics of AI-Generated Text in Social Media](https://arxiv.org/abs/2509.22055)
*Yudong Li,Yufei Sun,Yuhan Yao,Peiru Yang,Wanyue Li,Jiajun Zou,Yongfeng Huang,Linlin Shen*

Main category: cs.CL

TL;DR: 本文提出了RedNote-Vibe，这是首个涵盖5年时间跨度的社交媒体AI生成文本（AIGT）数据集，并提出了一种基于心理语言学特征的可解释AIGT检测方法PLAD。实验表明PLAD性能优越，并揭示了AIGT与用户互动的复杂关系。


<details>
  <summary>Details</summary>
Motivation: 当前社交平台AI生成文本大量存在，涉及不断变化的用户行为，但现有数据集和检测方法大多只考虑静态AIGT检测，缺乏对动态变化和实际互动数据的刻画。

Method: 作者收集了小红书平台自LLM普及前至2025年7月的用户内容、互动与时间戳，构建了RedNote-Vibe数据集。提出PLAD检测框架，利用心理语言学特征对AIGT进行可解释检测，并与传统方法做对比实验。

Result: PLAD方法在AIGT检测任务中表现优越，不仅检测效果佳，还能解释区分人类与AI内容的语言学特征。此外分析揭示了心理语言学特征与社交平台互动(如点赞、评论)间的复杂关系。

Conclusion: RedNote-Vibe数据集为AIGT动态研究与检测方法提供了新资源，PLAD为高效、可解释AIGT检测提供了新范式，有助于理解AIGT与用户互动规律。

Abstract: The proliferation of Large Language Models (LLMs) has led to widespread
AI-Generated Text (AIGT) on social media platforms, creating unique challenges
where content dynamics are driven by user engagement and evolve over time.
However, existing datasets mainly depict static AIGT detection. In this work,
we introduce RedNote-Vibe, the first longitudinal (5-years) dataset for social
media AIGT analysis. This dataset is sourced from Xiaohongshu platform,
containing user engagement metrics (e.g., likes, comments) and timestamps
spanning from the pre-LLM period to July 2025, which enables research into the
temporal dynamics and user interaction patterns of AIGT. Furthermore, to detect
AIGT in the context of social media, we propose PsychoLinguistic AIGT Detection
Framework (PLAD), an interpretable approach that leverages psycholinguistic
features. Our experiments show that PLAD achieves superior detection
performance and provides insights into the signatures distinguishing human and
AI-generated content. More importantly, it reveals the complex relationship
between these linguistic features and social media engagement. The dataset is
available at https://github.com/testuser03158/RedNote-Vibe.

</details>


### [214] [The QCET Taxonomy of Standard Quality Criterion Names and Definitions for the Evaluation of NLP Systems](https://arxiv.org/abs/2509.22064)
*Anya Belz,Simon Mille,Craig Thomson*

Main category: cs.CL

TL;DR: 本文提出了QCET质量评价标准集，通过标准化评价名称和定义解决NLP领域不同实验结果之间难以比较的问题。


<details>
  <summary>Details</summary>
Motivation: 当前NLP中对同一质量指标（如Fluency）的不同实验结果往往不可直接比较，因各自对评价标准的理解与定义差异较大。这导致我们无法可靠地从多个独立实验中得出关于系统质量的结论，阻碍了领域科学进步。

Method: 作者通过三项NLP评价调研，归纳出一套标准化的质量评价名称与定义（QCET），并将这些标准组织为层级结构，其中父节点概括了子节点的共性。该体系为评价指标的命名与定义建立了映射和标准。

Result: 作者提出并实现了QCET标准集，整理并公布了相应资源。

Conclusion: QCET有助于：1）判定已有评价的可比性；2）指导新评价的设计；3）评估合规性，从而推动NLP领域评价标准化和科学发展。

Abstract: Prior work has shown that two NLP evaluation experiments that report results
for the same quality criterion name (e.g. Fluency) do not necessarily evaluate
the same aspect of quality, and the comparability implied by the name can be
misleading. Not knowing when two evaluations are comparable in this sense means
we currently lack the ability to draw reliable conclusions about system quality
on the basis of multiple, independently conducted evaluations. This in turn
hampers the ability of the field to progress scientifically as a whole, a
pervasive issue in NLP since its beginning (Sparck Jones, 1981). It is hard to
see how the issue of unclear comparability can be fully addressed other than by
the creation of a standard set of quality criterion names and definitions that
the several hundred quality criterion names actually in use in the field can be
mapped to, and grounded in. Taking a strictly descriptive approach, the QCET
Quality Criteria for Evaluation Taxonomy derives a standard set of quality
criterion names and definitions from three surveys of evaluations reported in
NLP, and structures them into a hierarchy where each parent node captures
common aspects of its child nodes. We present QCET and the resources it
consists of, and discuss its three main uses in (i) establishing comparability
of existing evaluations, (ii) guiding the design of new evaluations, and (iii)
assessing regulatory compliance.

</details>


### [215] [Fine-tuning Done Right in Model Editing](https://arxiv.org/abs/2509.22072)
*Wanli Yang,Fei Sun,Rui Tang,Hongyu Zang,Du Su,Qi Cao,Jingang Wang,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文重新审视了微调（fine-tuning）在大语言模型内容编辑中的有效性，提出通过恢复标准广度优先微调流程和局部调优策略，使其在模型编辑方面达到甚至超越现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 以往研究普遍认为微调方法不适合用于大语言模型的编辑任务，但作者认为这种失败归因于方法操作方式（如深度优先逐样本优化引入了编辑间干扰），而非微调本身的局限。

Method: 作者通过对照实验，比较了深度优先和恢复为标准的广度优先（即mini-batch、epoch-based）训练流程性能。同时分析并优化了调节参数的位置，提出了LocFT-BF的局部广度优先微调方法，实现高效且可扩展的模型编辑。

Result: 在多种主流语言模型和数据集上的大量实验证明，LocFT-BF方法在模型编辑后的性能和稳定性方面明显超越现有主流方法，并首次能支持10万次持续编辑及720亿参数规模的模型，无明显能力损失。

Conclusion: 本文推翻了长期流传的微调无效论调，展示了通过合理调度和新的局部微调机制，微调成为模型编辑的强有力工具，极大提升了其实际应用价值，并为未来研究提供了方法论基础。

Abstract: Fine-tuning, a foundational method for adapting large language models, has
long been considered ineffective for model editing. Here, we challenge this
belief, arguing that the reported failure arises not from the inherent
limitation of fine-tuning itself, but from adapting it to the sequential nature
of the editing task, a single-pass depth-first pipeline that optimizes each
sample to convergence before moving on. While intuitive, this depth-first
pipeline coupled with sample-wise updating over-optimizes each edit and induces
interference across edits. Our controlled experiments reveal that simply
restoring fine-tuning to the standard breadth-first (i.e., epoch-based)
pipeline with mini-batch optimization substantially improves its effectiveness
for model editing. Moreover, fine-tuning in editing also suffers from
suboptimal tuning parameter locations inherited from prior methods. Through
systematic analysis of tuning locations, we derive LocFT-BF, a simple and
effective localized editing method built on the restored fine-tuning framework.
Extensive experiments across diverse LLMs and datasets demonstrate that
LocFT-BF outperforms state-of-the-art methods by large margins. Notably, to our
knowledge, it is the first to sustain 100K edits and 72B-parameter models,10 x
beyond prior practice, without sacrificing general capabilities. By clarifying
a long-standing misconception and introducing a principled localized tuning
strategy, we advance fine-tuning from an underestimated baseline to a leading
method for model editing, establishing a solid foundation for future research.

</details>


### [216] [COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning](https://arxiv.org/abs/2509.22075)
*Dmitriy Shopkhoev,Denis Makhov,Magauiya Zhussip,Ammar Ali,Stamatios Lefkimmiatis*

Main category: cs.CL

TL;DR: 作者提出了一种新的大语言模型（LLM）训练后压缩方法CoSpaDi，通过稀疏字典学习代替传统低秩分解，实现更高效且准确的模型压缩。


<details>
  <summary>Details</summary>
Motivation: 传统LLM模型压缩主要依赖低秩分解，将权重矩阵的每一列映射到共享的低维子空间，但这种方法结构僵化，易造成明显的精度损失。作者希望寻找一种表达力更强、灵活性更高，同时无需训练的新型压缩方式。

Method: CoSpaDi方法通过稠密字典与列稀疏系数矩阵对权重矩阵进行分解，各列可在不同子空间近似，增强表达能力。该方法利用小规模校准数据集，优化分解，使压缩后层的输出激活与原始层尽量保持一致，降低功能重构误差而不仅是权重近似误差。该结构化稀疏性便于实现高效稀疏-稠密矩阵乘法，并兼容后量化，进一步优化存储和延迟。

Result: CoSpaDi在多个Llama和Qwen模型上、不同分层和分组压缩设置下（20-50%压缩率）测试，显示在准确率和困惑度方面都优于当前最优的数据感知低秩压缩方法。

Conclusion: 结构化稀疏字典学习方法为LLM高效部署提供了一种优于传统低秩方案的新选择，能在无需微调的条件下实现更优的模型保真度和资源节省。

Abstract: Post-training compression of large language models (LLMs) largely relies on
low-rank weight approximation, which represents each column of a weight matrix
in a shared low-dimensional subspace. While this is a computationally efficient
strategy, the imposed structural constraint is rigid and can lead to a
noticeable model accuracy drop. In this work, we propose CoSpaDi (Compression
via Sparse Dictionary Learning), a novel training-free compression framework
that replaces low-rank decomposition with a more flexible structured sparse
factorization in which each weight matrix is represented with a dense
dictionary and a column-sparse coefficient matrix. This formulation enables a
union-of-subspaces representation: different columns of the original weight
matrix are approximated in distinct subspaces spanned by adaptively selected
dictionary atoms, offering greater expressiveness than a single invariant
basis. Crucially, CoSpaDi leverages a small calibration dataset to optimize the
factorization such that the output activations of compressed projection layers
closely match those of the original ones, thereby minimizing functional
reconstruction error rather than mere weight approximation. This data-aware
strategy preserves better model fidelity without any fine-tuning under
reasonable compression ratios. Moreover, the resulting structured sparsity
allows efficient sparse-dense matrix multiplication and is compatible with
post-training quantization for further memory and latency gains. We evaluate
CoSpaDi across multiple Llama and Qwen models under per-layer and per-group
settings at 20-50\% compression ratios, demonstrating consistent superiority
over state-of-the-art data-aware low-rank methods both in accuracy and
perplexity. Our results establish structured sparse dictionary learning as a
powerful alternative to conventional low-rank approaches for efficient LLM
deployment.

</details>


### [217] [Multilingual Dialogue Generation and Localization with Dialogue Act Scripting](https://arxiv.org/abs/2509.22086)
*Justin Vasselli,Eunike Andriani Kardinata,Yusuke Sakai,Taro Watanabe*

Main category: cs.CL

TL;DR: 本文提出了Dialogue Act Script（DAS）框架，通过结构化意图表示，生成更符合文化和语境的多语言对话，效果优于直接翻译。


<details>
  <summary>Details</summary>
Motivation: 非英语对话数据稀缺，直接翻译英语对话常产生不自然和文化不适当的问题。

Method: 提出DAS结构化框架，不直接翻译对话，而是以对话行为表示编码、在本地化的基础上，生成目标语言的新对话；支持多语言灵活本地化。

Result: 在人类评测中，DAS生成的意大利语、德语和中文对话，在文化相关性、连贯性和情境适用性上，普遍优于机器和人工翻译结果。

Conclusion: DAS方法能有效提升多语言对话的自然性和本地化质量，优于传统基于翻译的方法。

Abstract: Non-English dialogue datasets are scarce, and models are often trained or
evaluated on translations of English-language dialogues, an approach which can
introduce artifacts that reduce their naturalness and cultural appropriateness.
This work proposes Dialogue Act Script (DAS), a structured framework for
encoding, localizing, and generating multilingual dialogues from abstract
intent representations. Rather than translating dialogue utterances directly,
DAS enables the generation of new dialogues in the target language that are
culturally and contextually appropriate. By using structured dialogue act
representations, DAS supports flexible localization across languages,
mitigating translationese and enabling more fluent, naturalistic conversations.
Human evaluations across Italian, German, and Chinese show that DAS-generated
dialogues consistently outperform those produced by both machine and human
translators on measures of cultural relevance, coherence, and situational
appropriateness.

</details>


### [218] [S2J: Bridging the Gap Between Solving and Judging Ability in Generative Reward Models](https://arxiv.org/abs/2509.22099)
*Shaoning Sun,Jiachen Yu,Zongqi Wang,Xuewei Yang,Tianle Gu,Yujiu Yang*

Main category: cs.CL

TL;DR: 本文发现当前生成式奖励模型（GRMs）在某些任务上虽然能正确解决问题，但在判断环节的表现却不佳（称为solve-to-judge gap），并提出了S2J方法，通过同时利用同一GRM的解决与判别能力，在优化中显式关联两者，大幅缩小该差距。


<details>
  <summary>Details</summary>
Motivation: 尽管GRMs在奖励建模和评估中已被广泛应用，但其判别能力未必与问题解决能力匹配，存在模型能解却判错的现象。该研究抓住了这一实际应用痛点，希望提升判别性能。

Method: 提出Solve-to-Judge（S2J）方法：在GRM优化过程中，同时将模型自身的解决问题输出与评价判断输出作为监督信号，将解决能力与判别能力显式关联，实现能力互补。

Result: 实验表明，S2J方法可有效缩窄solve-to-judge gap 16.2%，判别性能提升5.8%。且在只用更小的数据集、无需外部更强模型辅助下，达到同类中最优结果（SOTA）。

Conclusion: S2J方法增强了GRMs的判别能力，提高了其评估性能，为奖励建模领域带来新突破，同时实现了高效、低数据依赖的自我进化方案。

Abstract: With the rapid development of large language models (LLMs), generative reward
models (GRMs) have been widely adopted for reward modeling and evaluation.
Previous studies have primarily focused on training specialized GRMs by
optimizing them on preference datasets with the judgment correctness as
supervision. While it's widely accepted that GRMs with stronger problem-solving
capabilities typically exhibit superior judgment abilities, we first identify a
significant solve-to-judge gap when examining individual queries. Specifically,
the solve-to-judge gap refers to the phenomenon where GRMs struggle to make
correct judgments on some queries (14%-37%), despite being fully capable of
solving them. In this paper, we propose the Solve-to-Judge (S2J) approach to
address this problem. Specifically, S2J simultaneously leverages both the
solving and judging capabilities on a single GRM's output for supervision,
explicitly linking the GRM's problem-solving and evaluation abilities during
model optimization, thereby narrowing the gap. Our comprehensive experiments
demonstrate that S2J effectively reduces the solve-to-judge gap by 16.2%,
thereby enhancing the model's judgment performance by 5.8%. Notably, S2J
achieves state-of-the-art (SOTA) performance among GRMs built on the same base
model while utilizing a significantly smaller training dataset. Moreover, S2J
accomplishes this through self-evolution without relying on more powerful
external models for distillation.

</details>


### [219] [Think Right, Not More: Test-Time Scaling for Numerical Claim Verification](https://arxiv.org/abs/2509.22101)
*Primakov Chungkham,V Venktesh,Vinay Setty,Avishek Anand*

Main category: cs.CL

TL;DR: 本文提出一种新方法，通过扩展推理路径数量并结合自适应机制，提升大语言模型在复杂数值事实核查任务中的准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在多步骤推理组合与数值推理方面仍有不足，难以准确核查基于真实世界的复杂数值性主张，并易受推理漂移问题影响。

Method: 系统性探索扩展推理路径（Scaling Test-time Compute, TTS）在复杂数值事实核查中的效果，设计并训练一个VERIFIERFC模型从多个推理路径中选取最优路径，并引入一种自适应机制，仅在主张较复杂时采用TTS策略，以提升计算效率。

Result: TTS有效缓解了推理漂移问题，显著提升数值主张核查表现。自适应机制在提升1.8倍计算效率的同时，相较单次推理方法带来18.8%的性能提升。

Conclusion: 通过多推理路径探索及自适应TTS机制，可显著提升大语言模型在复杂数值事实核查领域的准确性与计算效率。

Abstract: Fact-checking real-world claims, particularly numerical claims, is inherently
complex that require multistep reasoning and numerical reasoning for verifying
diverse aspects of the claim. Although large language models (LLMs) including
reasoning models have made tremendous advances, they still fall short on
fact-checking real-world claims that require a combination of compositional and
numerical reasoning. They are unable to understand nuance of numerical aspects,
and are also susceptible to the reasoning drift issue, where the model is
unable to contextualize diverse information resulting in misinterpretation and
backtracking of reasoning process. In this work, we systematically explore
scaling test-time compute (TTS) for LLMs on the task of fact-checking complex
numerical claims, which entails eliciting multiple reasoning paths from an LLM.
We train a verifier model (VERIFIERFC) to navigate this space of possible
reasoning paths and select one that could lead to the correct verdict. We
observe that TTS helps mitigate the reasoning drift issue, leading to
significant performance gains for fact-checking numerical claims. To improve
compute efficiency in TTS, we introduce an adaptive mechanism that performs TTS
selectively based on the perceived complexity of the claim. This approach
achieves 1.8x higher efficiency than standard TTS, while delivering a notable
18.8% performance improvement over single-shot claim verification methods. Our
code and data can be found at https://github.com/VenkteshV/VerifierFC

</details>


### [220] [Universal Legal Article Prediction via Tight Collaboration between Supervised Classification Model and LLM](https://arxiv.org/abs/2509.22119)
*Xiao Chi,Wenlin Zhong,Yiquan Wu,Wei Wang,Kun Kuang,Fei Wu,Minghui Xiong*

Main category: cs.CL

TL;DR: 提出一种结合SCM与LLM的新框架Uni-LAP，用于更准确且通用的法律条文预测。


<details>
  <summary>Details</summary>
Motivation: 现有法律条文预测方法难以同时兼顾事实复杂性和跨法系通用性，SCM和LLM各有局限。该研究旨在克服这些挑战，提高预测质量并扩大适用范围。

Method: 提出Uni-LAP框架，将经Top-K loss增强的监督分类模型（SCM）和借鉴三段论推理的LLM协作结合。SCM负责生成准确备选条文，LLM进一步推理出最终预测结果。

Result: 在多法系数据集上的实验结果表明，Uni-LAP在效果和通用性上均优于现有方法。

Conclusion: Uni-LAP能够有效提升法律条文预测的准确性与适用范围，为法律智能决策提供了更强大的支持。

Abstract: Legal Article Prediction (LAP) is a critical task in legal text
classification, leveraging natural language processing (NLP) techniques to
automatically predict relevant legal articles based on the fact descriptions of
cases. As a foundational step in legal decision-making, LAP plays a pivotal
role in determining subsequent judgments, such as charges and penalties.
Despite its importance, existing methods face significant challenges in
addressing the complexities of LAP. Supervised classification models (SCMs),
such as CNN and BERT, struggle to fully capture intricate fact patterns due to
their inherent limitations. Conversely, large language models (LLMs), while
excelling in generative tasks, perform suboptimally in predictive scenarios due
to the abstract and ID-based nature of legal articles. Furthermore, the
diversity of legal systems across jurisdictions exacerbates the issue, as most
approaches are tailored to specific countries and lack broader applicability.
To address these limitations, we propose Uni-LAP, a universal framework for
legal article prediction that integrates the strengths of SCMs and LLMs through
tight collaboration. Specifically, in Uni-LAP, the SCM is enhanced with a novel
Top-K loss function to generate accurate candidate articles, while the LLM
employs syllogism-inspired reasoning to refine the final predictions. We
evaluated Uni-LAP on datasets from multiple jurisdictions, and empirical
results demonstrate that our approach consistently outperforms existing
baselines, showcasing its effectiveness and generalizability.

</details>


### [221] [Multilingual Vision-Language Models, A Survey](https://arxiv.org/abs/2509.22123)
*Andrei-Alexandru Manea,Jindřich Libovický*

Main category: cs.CL

TL;DR: 本文综述了多语言视觉-语言模型的发展，分析了31种模型和21个基准，并指出它们在语言中立性和文化敏感性之间存在权衡。


<details>
  <summary>Details</summary>
Motivation: 随着多语言与多模态AI应用需求增长，当前模型如何在不同语言及文化语境下理解图像和文本，是亟需系统评估和改进的重要问题。

Method: 通过梳理和比较主流的编码器和生成式多语言视觉-语言模型，以及分析各类训练策略和评测基准，探讨模型在保证语义一致的同时能否具备文化适应能力。

Result: 发现目前主流训练方法偏重跨语言一致性，多数评测基准采用翻译，以语义一致性为主；但新近方法已开始引入文化语境。现有模型在多语言能力和真实评测需求之间还存在差距。

Conclusion: 当前多语言视觉-语言模型更多关注语言中立性，文化敏感性则有待提升，未来需开发更平衡两者、贴合实际应用需求的模型和评价体系。

Abstract: This survey examines multilingual vision-language models that process text
and images across languages. We review 31 models and 21 benchmarks, spanning
encoder-only and generative architectures, and identify a key tension between
language neutrality (consistent cross-lingual representations) and cultural
awareness (adaptation to cultural contexts). Current training methods favor
neutrality through contrastive learning, while cultural awareness depends on
diverse data. Two-thirds of evaluation benchmarks use translation-based
approaches prioritizing semantic consistency, though recent work incorporates
culturally grounded content. We find discrepancies in cross-lingual
capabilities and gaps between training objectives and evaluation goals.

</details>


### [222] [FoodSEM: Large Language Model Specialized in Food Named-Entity Linking](https://arxiv.org/abs/2509.22125)
*Ana Gjorgjevikj,Matej Martinc,Gjorgjina Cenikj,Sašo Džeroski,Barbara Koroušić Seljak,Tome Eftimov*

Main category: cs.CL

TL;DR: 本文提出了FoodSEM，这是一个专为食物相关命名实体链接（NEL）任务微调的开源大语言模型，在多个本体（如FoodOn、SNOMED-CT和Hansard分类法）上取得了卓越表现。


<details>
  <summary>Details</summary>
Motivation: 现有的通用大语言模型或定制化领域模型难以精准完成食品领域的实体链接任务，因此需要专门为该领域开发高性能的NEL模型。

Method: 作者采用指令-响应（IR）场景对大语言模型进行微调，使其能够将文本中的食品相关实体准确连接到多个食物本体，并进行了系统的比较实验。

Result: FoodSEM在相关模型/系统中表现优异，在部分本体和数据集上F1分数甚至达到98%，远超零样本、单样本和少样本提示的基线模型及其未微调版本。

Conclusion: FoodSEM推动了食物语义文本理解的发展，为未来食物领域实体链接任务奠定了强有力的基线，同时开放了相关语料与模型，支持社区进一步研究。

Abstract: This paper introduces FoodSEM, a state-of-the-art fine-tuned open-source
large language model (LLM) for named-entity linking (NEL) to food-related
ontologies. To the best of our knowledge, food NEL is a task that cannot be
accurately solved by state-of-the-art general-purpose (large) language models
or custom domain-specific models/systems. Through an instruction-response (IR)
scenario, FoodSEM links food-related entities mentioned in a text to several
ontologies, including FoodOn, SNOMED-CT, and the Hansard taxonomy. The FoodSEM
model achieves state-of-the-art performance compared to related models/systems,
with F1 scores even reaching 98% on some ontologies and datasets. The presented
comparative analyses against zero-shot, one-shot, and few-shot LLM prompting
baselines further highlight FoodSEM's superior performance over its
non-fine-tuned version. By making FoodSEM and its related resources publicly
available, the main contributions of this article include (1) publishing a
food-annotated corpora into an IR format suitable for LLM
fine-tuning/evaluation, (2) publishing a robust model to advance the semantic
understanding of text in the food domain, and (3) providing a strong baseline
on food NEL for future benchmarking.

</details>


### [223] [R-Capsule: Compressing High-Level Plans for Efficient Large Language Model Reasoning](https://arxiv.org/abs/2509.22131)
*Hongyu Shan,Mingyang Song,Chang Dai,Di Liang,Han Chen*

Main category: cs.CL

TL;DR: 本文提出了一种新的推理方法R-Capsule（Reasoning Capsule），利用少量的潜在Token有机结合了推理的效率与可解释性，在保持或提升准确率的同时降低了计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: 现有CoT（Chain-of-Thought）方法虽然提升了LLM的推理能力，但由于需要输出详细的逐步推理过程，导致推理延迟高、内存大且易于早期错误传播。因此，研究者希望在保证推理可解释性的同时，大幅提升推理效率。

Method: 核心方法是将高层推理计划压缩到一组小规模、可学习的潜在Token（即R-Capsule），并结合轻量级或显式的执行步骤。模型受信息瓶颈（IB）理论启发，通过容量受限约束强化Token的最小化特性，同时结合任务准确率损失与计划重建损失，确保Token既高效又能忠实表达原始推理计划。

Result: 在复杂推理基准测试中，R-Capsule显著减少了推理过程中所需的Token数量，同时保持或提升了推理准确率。计划重建损失的引入提升了潜在空间的可解释性并减少了模型采用不透明捷径。

Conclusion: R-Capsule在效率、准确率和可解释性三者之间取得了良好平衡，是提升LLM推理能力及实用性的新方向。

Abstract: Chain-of-Thought (CoT) prompting helps Large Language Models (LLMs) tackle
complex reasoning by eliciting explicit step-by-step rationales. However, CoT's
verbosity increases latency and memory usage and may propagate early errors
across long chains. We propose the Reasoning Capsule (R-Capsule), a framework
that aims to combine the efficiency of latent reasoning with the transparency
of explicit CoT. The core idea is to compress the high-level plan into a small
set of learned latent tokens (a Reasoning Capsule) while keeping execution
steps lightweight or explicit. This hybrid approach is inspired by the
Information Bottleneck (IB) principle, where we encourage the capsule to be
approximately minimal yet sufficient for the task. Minimality is encouraged via
a low-capacity bottleneck, which helps improve efficiency. Sufficiency is
encouraged via a dual objective: a primary task loss for answer accuracy and an
auxiliary plan-reconstruction loss that encourages the capsule to faithfully
represent the original textual plan. The reconstruction objective helps ground
the latent space, thereby improving interpretability and reducing the use of
uninformative shortcuts. Our framework strikes a balance between efficiency,
accuracy, and interpretability, thereby reducing the visible token footprint of
reasoning while maintaining or improving accuracy on complex benchmarks. Our
codes are available at:
https://anonymous.4open.science/r/Reasoning-Capsule-7BE0

</details>


### [224] [Bridging Draft Policy Misalignment: Group Tree Optimization for Speculative Decoding](https://arxiv.org/abs/2509.22134)
*Shijing Hu,Jingyang Li,Zhihui Lu,Pan Zhou*

Main category: cs.CL

TL;DR: 本文提出了Group Tree Optimization (GTO) 方法，通过对比并优化草稿模型与目标模型的对齐策略，显著提升了大型语言模型推理中的速度和草稿接受长度。


<details>
  <summary>Details</summary>
Motivation: 现有的推理加速方法（如speculative decoding）在训练时仅仅优化单一的贪婪生成路径，而实际推理则要对多分支做重新排序和核查，这造成训练与推理策略不一致，限制了性能提升。

Method: GTO包含两个关键创新：一是Draft Tree Reward，以目标模型下草稿树的期望接受长度为奖励，直接对应解码性能；二是Group-based Draft Policy Training，对比当前草稿模型与冻结参考模型生成的树，标准化优势并采用PPO式替代目标，确保训练稳定高效。理论上证明提高Draft Tree Reward可以提升接受长度和加速率。

Result: GTO在多个数据集（如MT-Bench、HumanEval、GSM8K）和主流大模型（LLaMA-3.1-8B等）上实验证明，将接受长度提升7.4%，额外带来了7.7%的加速，相对之前的EAGLE-3方法有显著优势。

Conclusion: GTO有效解决了草稿政策的不一致，无需采样，能广泛适用于高效LLM推理，具有良好的实际应用前景。

Abstract: Speculative decoding accelerates large language model (LLM) inference by
letting a lightweight draft model propose multiple tokens that the target model
verifies in parallel. Yet existing training objectives optimize only a single
greedy draft path, while decoding follows a tree policy that re-ranks and
verifies multiple branches. This draft policy misalignment limits achievable
speedups. We introduce Group Tree Optimization (GTO), which aligns training
with the decoding-time tree policy through two components: (i) Draft Tree
Reward, a sampling-free objective equal to the expected acceptance length of
the draft tree under the target model, directly measuring decoding performance;
(ii) Group-based Draft Policy Training, a stable optimization scheme that
contrasts trees from the current and a frozen reference draft model, forming
debiased group-standardized advantages and applying a PPO-style surrogate along
the longest accepted sequence for robust updates. We further prove that
increasing our Draft Tree Reward provably improves acceptance length and
speedup. Across dialogue (MT-Bench), code (HumanEval), and math (GSM8K), and
multiple LLMs (e.g., LLaMA-3.1-8B, LLaMA-3.3-70B, Vicuna-1.3-13B,
DeepSeek-R1-Distill-LLaMA-8B), GTO increases acceptance length by 7.4% and
yields an additional 7.7% speedup over prior state-of-the-art EAGLE-3. By
bridging draft policy misalignment, GTO offers a practical, general solution
for efficient LLM inference.

</details>


### [225] [NFDI4DS Shared Tasks for Scholarly Document Processing](https://arxiv.org/abs/2509.22141)
*Raia Abu Ahmad,Rana Abdulla,Tilahun Abedissa Taffa,Soeren Auer,Hamed Babaei Giglou,Ekaterina Borisova,Zongxiong Chen,Stefan Dietze,Jennifer DSouza,Mayra Elwes,Genet-Asefa Gesese,Shufan Jiang,Ekaterina Kutafina,Philipp Mayr,Georg Rehm,Sameer Sadruddin,Sonja Schimmler,Daniel Schneider,Kanishka Silva,Sharmila Upadhyaya,Ricardo Usbeck*

Main category: cs.CL

TL;DR: 本文介绍了NFDI4DS联盟下的12个学术文档处理领域的开源共享任务，这些任务推动了方法创新和FAIR科研实践。


<details>
  <summary>Details</summary>
Motivation: 为了促进科研的标准化评测、数据共享和可重现性，支持学术文档处理相关研究的开放与透明。

Method: 组织并在顶级会议举办12项多样化的学术文档处理共享任务，开发和发布开放数据集、模型及工具，并整合到科研数据基础设施中。

Result: 这些共享任务带来了方法学上的创新，为学术界提供了开放的数据、模型和工具资源。

Conclusion: 共享任务极大提升了学术文档处理领域的科研规范性与资源开放性，为数据基础设施与社区合作贡献力量。

Abstract: Shared tasks are powerful tools for advancing research through
community-based standardised evaluation. As such, they play a key role in
promoting findable, accessible, interoperable, and reusable (FAIR), as well as
transparent and reproducible research practices. This paper presents an updated
overview of twelve shared tasks developed and hosted under the German National
Research Data Infrastructure for Data Science and Artificial Intelligence
(NFDI4DS) consortium, covering a diverse set of challenges in scholarly
document processing. Hosted at leading venues, the tasks foster methodological
innovations and contribute open-access datasets, models, and tools for the
broader research community, which are integrated into the consortium's research
data infrastructure.

</details>


### [226] [From Long to Lean: Performance-aware and Adaptive Chain-of-Thought Compression via Multi-round Refinement](https://arxiv.org/abs/2509.22144)
*Jianzhi Yan,Le Liu,Youcheng Pan,Shiwei Chen,Zike Yuan,Yang Xiang,Buzhou Tang*

Main category: cs.CL

TL;DR: 本文提出了多轮自适应思维链压缩（MACC）方法，能够在保证高准确率的同时显著减少推理时序长度和延迟。


<details>
  <summary>Details</summary>
Motivation: 思维链（CoT）推理虽然能提升复杂任务的表现，但由于生成过程冗长，带来了显著的推理延迟。因此亟需在保持任务性能的同时，减少CoT的输出长度。

Method: MACC方法利用了“token弹性”现象，即token上限过小时模型反而生成更长输出。该方法通过多轮迭代精炼、自适应地压缩每条思维链的长度，自动选择最佳压缩深度。此外，研究者使用可解释性特征（如perplexity和压缩率）预测推理表现。

Result: MACC在多个任务和模型上平均提升了5.6%的准确率，同时平均每条思维链减少47个token，显著提升了推理效率和降低延迟。通过训练集的特征表现，能够有效预测测试时的性能，无需反复微调。

Conclusion: MACC不仅高效且可预测，在不牺牲准确性的前提下压缩了推理长度和延迟，具有广泛的应用潜力。

Abstract: Chain-of-Thought (CoT) reasoning improves performance on complex tasks but
introduces significant inference latency due to verbosity. We propose
Multiround Adaptive Chain-of-Thought Compression (MACC), a framework that
leverages the token elasticity phenomenon--where overly small token budgets can
paradoxically increase output length--to progressively compress CoTs via
multiround refinement. This adaptive strategy allows MACC to determine the
optimal compression depth for each input. Our method achieves an average
accuracy improvement of 5.6 percent over state-of-the-art baselines, while also
reducing CoT length by an average of 47 tokens and significantly lowering
latency. Furthermore, we show that test-time performance--accuracy and token
length--can be reliably predicted using interpretable features like perplexity
and compression rate on the training set. Evaluated across different models,
our method enables efficient model selection and forecasting without repeated
fine-tuning, demonstrating that CoT compression is both effective and
predictable. Our code will be released in https://github.com/Leon221220/MACC.

</details>


### [227] [Mixture of Detectors: A Compact View of Machine-Generated Text Detection](https://arxiv.org/abs/2509.22147)
*Sai Teja Lekkala,Yadagiri Annepaka,Arun Kumar Challa,Samatha Reddy Machireddy,Partha Pakray,Chukhu Chunka*

Main category: cs.CL

TL;DR: 本文探讨了在大语言模型（LLMs）持续进步下，如何检测机器生成文本，包括文档级和句子级的多种场景，提出了新的数据集和方法。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs变得越来越强大，人们开始担心如何分辨人类与AI的创造性产出，该能力涉及原创性、归因及内容安全。本文旨在提升机器生成文本检测领域的有效性和精度。

Method: 本文提出了BMAS English数据集，支持二分类和多分类任务，并涵盖了生成器归属、对抗攻击和句级分割。方法包括：文档级的人/机器判别、多生成器归属判断、句级准确分界、及对抗攻击检测减少模型可辨识性。

Result: 利用新数据集和方法，能够更好地区分并归因不同的机器生成文本，提升了检测准确率，并对抗方法下的鲁棒性也得到了考察。

Conclusion: 本文为机生文本检测提供了更细致和全面的数据集与检测方法，为未来人机合作和创新真实性保护提供基础工具。

Abstract: Large Language Models (LLMs) are gearing up to surpass human creativity. The
veracity of the statement needs careful consideration. In recent developments,
critical questions arise regarding the authenticity of human work and the
preservation of their creativity and innovative abilities. This paper
investigates such issues. This paper addresses machine-generated text detection
across several scenarios, including document-level binary and multiclass
classification or generator attribution, sentence-level segmentation to
differentiate between human-AI collaborative text, and adversarial attacks
aimed at reducing the detectability of machine-generated text. We introduce a
new work called BMAS English: an English language dataset for binary
classification of human and machine text, for multiclass classification, which
not only identifies machine-generated text but can also try to determine its
generator, and Adversarial attack addressing where it is a common act for the
mitigation of detection, and Sentence-level segmentation, for predicting the
boundaries between human and machine-generated text. We believe that this paper
will address previous work in Machine-Generated Text Detection (MGTD) in a more
meaningful way.

</details>


### [228] [Context Parametrization with Compositional Adapters](https://arxiv.org/abs/2509.22158)
*Josip Jukić,Martin Tutek,Jan Šnajder*

Main category: cs.CL

TL;DR: 提出了一种新方法CompAs，将上下文信息动态转化为可合成的adapter参数，以高效替代传统的in-context learning（ICL）和supervised fine-tuning（SFT），实现推理成本更低且性能更优。


<details>
  <summary>Details</summary>
Motivation: ICL处理大量示例效率低，而SFT则训练开销大且缺乏灵活性。当前直接基于上下文生成adapter参数的方法未能有效结合多块信息，难以灵活扩展和优化。

Method: 提出CompAs元学习框架，将输入的上下文转化为具备可组合结构的adapter参数，支持多个指令、示例或检索片段的代数合并，无需重处理长提示。同时通过可逆编码支持信息安全与恢复。

Result: 在多项选择与抽取型问答任务上，CompAs明显优于ICL及现有生成型adapter方法，尤其是在输入数量增加时表现突出。

Conclusion: 复合型adapter生成是一种切实可行且高效的LLM部署扩展方案，兼顾推理开销低、长上下文稳健及信息安全等优势。

Abstract: Large language models (LLMs) often seamlessly adapt to new tasks through
in-context learning (ICL) or supervised fine-tuning (SFT). However, both of
these approaches face key limitations: ICL is inefficient when handling many
demonstrations, and SFT incurs training overhead while sacrificing flexibility.
Mapping instructions or demonstrations from context directly into adapter
parameters offers an appealing alternative. While prior work explored
generating adapters based on a single input context, it has overlooked the need
to integrate multiple chunks of information. To address this gap, we introduce
CompAs, a meta-learning framework that translates context into adapter
parameters with a compositional structure. Adapters generated this way can be
merged algebraically, enabling instructions, demonstrations, or retrieved
passages to be seamlessly combined without reprocessing long prompts.
Critically, this approach yields three benefits: lower inference cost,
robustness to long-context instability, and establishes a principled solution
when input exceeds the model's context window. Furthermore, CompAs encodes
information into adapter parameters in a reversible manner, enabling recovery
of input context through a decoder, facilitating safety and security. Empirical
results on diverse multiple-choice and extractive question answering tasks show
that CompAs outperforms ICL and prior generator-based methods, especially when
scaling to more inputs. Our work establishes composable adapter generation as a
practical and efficient alternative for scaling LLM deployment.

</details>


### [229] [When Does Reasoning Matter? A Controlled Study of Reasoning's Contribution to Model Performance](https://arxiv.org/abs/2509.22193)
*Nicolas Boizard,Hippolyte Gisserot-Boukhlef,Kevin El-Haddad,Céline Hudelot,Pierre Colombo*

Main category: cs.CL

TL;DR: 作者对比了不同规模的大模型在指令微调和推理能力方面的表现，发现推理能力不仅提升模型效果，并能在特定任务上超越更大的单纯指令微调模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在推理任务上表现突出，但不同任务、模型规模对推理有效性的影响，以及推理方式的训练和推理成本尚不清楚。作者希望系统性比较不同策略和规模下模型的优劣。

Method: 利用合成数据蒸馏框架，针对不同规模的模型，分别进行指令微调和推理模型训练，在多种数学和通用任务上进行大规模监督实验，并比较多项选择和开放式任务两种格式表现。

Result: 推理模型在各种任务上表现优越，常常能达到或超过更大规模的指令微调模型。而在成本方面，指令微调模型在训练和推理效率上仍占优势。但随着模型参数扩展，推理模型对于复杂、开放式任务表现提升显著，最终打破指令微调模型的性能上限。

Conclusion: 推理能力是提升大模型性能的重要因素，尤其是在规模增大和任务复杂度提高时。尽管指令微调仍具备成本优势，但具备推理能力的模型在实际应用中将越来越具价值。

Abstract: Large Language Models (LLMs) with reasoning capabilities have achieved
state-of-the-art performance on a wide range of tasks. Despite its empirical
success, the tasks and model scales at which reasoning becomes effective, as
well as its training and inference costs, remain underexplored. In this work,
we rely on a synthetic data distillation framework to conduct a large-scale
supervised study. We compare Instruction Fine-Tuning (IFT) and reasoning models
of varying sizes, on a wide range of math-centric and general-purpose tasks,
evaluating both multiple-choice and open-ended formats. Our analysis reveals
that reasoning consistently improves model performance, often matching or
surpassing significantly larger IFT systems. Notably, while IFT remains
Pareto-optimal in training and inference costs, reasoning models become
increasingly valuable as model size scales, overcoming IFT performance limits
on reasoning-intensive and open-ended tasks.

</details>


### [230] [The Outputs of Large Language Models are Meaningless](https://arxiv.org/abs/2509.22206)
*Anandi Hattiangadi,Anders J. Schoubye*

Main category: cs.CL

TL;DR: 本文提出了大型语言模型（LLMs）的输出实际上没有意义的观点，并从意图缺失的角度加以论证，对相关反驳进行了回应。


<details>
  <summary>Details</summary>
Motivation: 近年来LLM（如ChatGPT等）在生成文本方面取得显著进展，但它们输出内容的“意义”长期存在哲学争议。作者试图厘清LLM输出的真实语义地位，特别关注‘意图’在赋予语言意义中的作用。

Method: 作者提出两个前提：一是输出要有字面意义需要有某种类型的意图，二是LLMs无法拥有这样的意图。在此基础上给出了中心论证，并针对语义外在论（依赖他人意图）及语义内在论（概念内在角色关系决定意义）进行了反驳与探讨。

Result: 作者系统回应了主要反驳，认为LLMs的输出依然达不到真正‘有意义’的标准，但也承认LLMs的文本似乎具有表面意义，能带来知识与正确信念。

Conclusion: 作者得出，虽然LLMs生成的文本可用来获得知识或真理，但在严格语义哲学的视角下，这些文本本身仍然是‘无意义’的，因为缺乏创造意义所需的意图。

Abstract: In this paper, we offer a simple argument for the conclusion that the outputs
of large language models (LLMs) are meaningless. Our argument is based on two
key premises: (a) that certain kinds of intentions are needed in order for
LLMs' outputs to have literal meanings, and (b) that LLMs cannot plausibly have
the right kinds of intentions. We defend this argument from various types of
responses, for example, the semantic externalist argument that deference can be
assumed to take the place of intentions and the semantic internalist argument
that meanings can be defined purely in terms of intrinsic relations between
concepts, such as conceptual roles. We conclude the paper by discussing why,
even if our argument is sound, the outputs of LLMs nevertheless seem meaningful
and can be used to acquire true beliefs and even knowledge.

</details>


### [231] [Question-Driven Analysis and Synthesis: Building Interpretable Thematic Trees with LLMs for Text Clustering and Controllable Generation](https://arxiv.org/abs/2509.22211)
*Tiago Fernandes Tavares*

Main category: cs.CL

TL;DR: 本文提出了一种新的利用大语言模型（LLM）辅助的无监督文本分析框架Recursive Thematic Partitioning（RTP），通过构建二叉树形式的可解释主题体系，有效提升了主题聚类的可解释性与下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有无监督文本主题建模在数据稀缺领域存在可解释性差、聚类语义不清晰等问题，需要大量人工参与解读关键词列表，且对用户不友好。

Method: 提出Recursive Thematic Partitioning（RTP）框架，利用LLM交互式地以自然语言问题逐层分割数据，构建语义明确、逻辑清楚的层级主题树，将每个节点描述为一条问题，直观揭示聚类逻辑。

Result: RTP生成的基于问题的主题体系，比传统基于关键词的主题模型（如BERTopic）更易于解释；实验也表明，RTP生成的聚类在与任务标签关联性较高时，可作为下游分类特征提升效果。此外，RTP还能作为生成模型的可控结构化提示，辅助内容生成。

Conclusion: RTP开创性地将主题建模的关注点由统计模式转向知识驱动的主题语义分析，提升了无监督文本建模的可解释性及其实用性，并为后续生成任务提供了可控的语义路径。

Abstract: Unsupervised analysis of text corpora is challenging, especially in
data-scarce domains where traditional topic models struggle. While these models
offer a solution, they typically describe clusters with lists of keywords that
require significant manual effort to interpret and often lack semantic
coherence. To address this critical interpretability gap, we introduce
Recursive Thematic Partitioning (RTP), a novel framework that leverages Large
Language Models (LLMs) to interactively build a binary tree. Each node in the
tree is a natural language question that semantically partitions the data,
resulting in a fully interpretable taxonomy where the logic of each cluster is
explicit. Our experiments demonstrate that RTP's question-driven hierarchy is
more interpretable than the keyword-based topics from a strong baseline like
BERTopic. Furthermore, we establish the quantitative utility of these clusters
by showing they serve as powerful features in downstream classification tasks,
particularly when the data's underlying themes correlate with the task labels.
RTP introduces a new paradigm for data exploration, shifting the focus from
statistical pattern discovery to knowledge-driven thematic analysis.
Furthermore, we demonstrate that the thematic paths from the RTP tree can serve
as structured, controllable prompts for generative models. This transforms our
analytical framework into a powerful tool for synthesis, enabling the
consistent imitation of specific characteristics discovered in the source
corpus.

</details>


### [232] [StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient SpeechLLMs](https://arxiv.org/abs/2509.22220)
*Yuhan Song,Linhao Zhang,Chuhan Wu,Aiwei Liu,Wei Jia,Houfeng Wang,Xiao Zhou*

Main category: cs.CL

TL;DR: 现有的语义语音分词器对于无意义的噪声干扰非常脆弱，本文提出了StableToken分词器，以提升其稳定性并增强语音LLM的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 语义语音分词器经常在高信噪比下也因无关含义的声学扰动而输出不一致，导致下游大模型（LLM）难以学习，影响整体性能。需要提升分词器在实际复杂环境下的稳定性。

Method: 提出一种新的StableToken分词器，采用多分支并行结构处理音频，然后用位级投票机制将各分支的结果合成为一个稳定的Token序列，以获得更加一致和抗噪的分词输出。

Result: StableToken分词器在多种噪声条件下极大地减少了Unit Edit Distance（UED），显著超过以往语音分词器的稳定性表现。

Conclusion: StableToken能显著提升Token序列在噪声环境下的稳定性，最终带来SpeechLLMs在多任务上的鲁棒性提升。

Abstract: Prevalent semantic speech tokenizers, designed to capture linguistic content,
are surprisingly fragile. We find they are not robust to meaning-irrelevant
acoustic perturbations; even at high Signal-to-Noise Ratios (SNRs) where speech
is perfectly intelligible, their output token sequences can change drastically,
increasing the learning burden for downstream LLMs. This instability stems from
two flaws: a brittle single-path quantization architecture and a distant
training signal indifferent to intermediate token stability. To address this,
we introduce StableToken, a tokenizer that achieves stability through a
consensus-driven mechanism. Its multi-branch architecture processes audio in
parallel, and these representations are merged via a powerful bit-wise voting
mechanism to form a single, stable token sequence. StableToken sets a new
state-of-the-art in token stability, drastically reducing Unit Edit Distance
(UED) under diverse noise conditions. This foundational stability translates
directly to downstream benefits, significantly improving the robustness of
SpeechLLMs on a variety of tasks.

</details>


### [233] [Thinking in Many Modes: How Composite Reasoning Elevates Large Language Model Performance with Limited Data](https://arxiv.org/abs/2509.22224)
*Zishan Ahmad,Saisubramaniam Gopalakrishnan*

Main category: cs.CL

TL;DR: 提出了一种名为Composite Reasoning (CR) 的新推理方法，使大语言模型(LLM)能动态结合多种推理方式，提升解决复杂问题的能力，且在医学与科学问答基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM主要依赖单一主导的推理范式，难以应对需要多种认知策略的复杂问题。研究动机在于提升LLM内部的推理多样性和适应性，从而增强其复杂问题求解能力。

Method: 提出Composite Reasoning (CR)方法，允许LLM动态选择并组合演绎、归纳、溯因等多种推理风格。并在科学、医学问答基准测试集上对该方法进行评测，与Chain-of-Thought (CoT)和DeepSeek-R1等主流推理范式对比。

Result: CR方法在科学和医学问答基准上表现优于CoT和DeepSeek-R1，不仅准确率更高，还具备更好的样本效率和合理的token使用。此外，CR能自适应强调领域适配的推理风格。

Conclusion: 通过增强推理风格的多样性，LLM展现出更强大、更具适应性的高效问题解决能力。

Abstract: Large Language Models (LLMs), despite their remarkable capabilities, rely on
singular, pre-dominant reasoning paradigms, hindering their performance on
intricate problems that demand diverse cognitive strategies. To address this,
we introduce Composite Reasoning (CR), a novel reasoning approach empowering
LLMs to dynamically explore and combine multiple reasoning styles like
deductive, inductive, and abductive for more nuanced problem-solving. Evaluated
on scientific and medical question-answering benchmarks, our approach
outperforms existing baselines like Chain-of-Thought (CoT) and also surpasses
the accuracy of DeepSeek-R1 style reasoning (SR) capabilities, while
demonstrating superior sample efficiency and adequate token usage. Notably, CR
adaptively emphasizes domain-appropriate reasoning styles. It prioritizes
abductive and deductive reasoning for medical question answering, but shifts to
causal, deductive, and inductive methods for scientific reasoning. Our findings
highlight that by cultivating internal reasoning style diversity, LLMs acquire
more robust, adaptive, and efficient problem-solving abilities.

</details>


### [234] [In Their Own Words: Reasoning Traces Tailored for Small Models Make Them Better Reasoners](https://arxiv.org/abs/2509.22230)
*Jaehoon Kim,Kwangwook Seo,Dongha Lee*

Main category: cs.CL

TL;DR: 将大模型的推理能力迁移到小模型在实际操作中常常导致性能下降，原因在于分布失配。论文提出了逆向推测解码（RSD）方法，通过让学生模型过滤低概率token，有效实现了更好的能力迁移。


<details>
  <summary>Details</summary>
Motivation: 大语言模型具有强推理能力，但它们资源消耗大，因此希望将其能力迁移至小模型。然而现有基于监督微调的方法常常导致小模型性能下降，亟需新方法解决这一能力迁移难题。

Method: 提出了‘逆向推测解码’（RSD）机制——老师模型生成candidate token，学生模型以自己的概率分布决定是否接受，从而过滤掉低概率token，仅保留对学生友好的推理轨迹。

Result: 在Qwen3-0.6B模型上直接蒸馏标准推理轨迹会导致推理类基准性能平均下降20.5%；而采用RSD生成的推理轨迹训练后性能提升4.9%。分析发现，低概率token是能力迁移的关键瓶颈。跨模型实验显示，RSD结果具有模型定制性，而非一劳永逸。

Conclusion: 低概率token阻碍了推理能力迁移，分布对齐需要针对不同小模型定制。逆向推测解码能显著提升推理迁移效果，但必须为每个学生模型单独定制推理轨迹。

Abstract: Transferring reasoning capabilities from larger language models to smaller
ones through supervised fine-tuning often fails counterintuitively, with
performance degrading despite access to high-quality teacher demonstrations. We
identify that this failure stems from distributional misalignment: reasoning
traces from larger models contain tokens that are low probability under the
student's distribution, exceeding the internal representation capacity of
smaller architectures and creating learning barriers rather than helpful
guidance. We propose Reverse Speculative Decoding (RSD), a mechanism for
generating student-friendly reasoning traces in which the teacher model
proposes candidate tokens but the student model determines acceptance based on
its own probability distributions, filtering low probability tokens. When
applied to Qwen3-0.6B, direct distillation of s1K-1.1 reasoning trace data
degrades average performance across major reasoning benchmarks by 20.5\%, while
the same model trained on RSD-generated reasoning traces achieves meaningful
improvements of 4.9\%. Our analysis reveals that low probability tokens
constitute the critical bottleneck in reasoning ability transfer. However,
cross-model experiments demonstrate that RSD traces are model-specific rather
than universally applicable, indicating that distributional alignment must be
tailored for each student architecture's unique internal representation.

</details>


### [235] [FeatBench: Evaluating Coding Agents on Feature Implementation for Vibe Coding](https://arxiv.org/abs/2509.22237)
*Haorui Chen,Chengze Li,Jia Li*

Main category: cs.CL

TL;DR: 这篇论文介绍了为大语言模型（LLM）开发新的代码生成评测基准—FeatBench，专注于通过自然语言描述实现新的功能（feature implementation），以更好衡量“vibe coding”这种用户用自然语言操作编程代理的能力。


<details>
  <summary>Details</summary>
Motivation: 当前主流代码生成基准评测无法真实反映LLM在“vibe coding”即用高层次自然语言指令实现新功能时的表现。这些基准要么偏向代码层细粒度指定，要么侧重于窄化的问题解决，忽略了在现实开发中高频出现的“通过抽象描述实现新功能”这一关键场景，因此作者提出新基准。

Method: 作者提出FeatBench基准，有以下创新：1）纯自然语言任务描述；2）严谨的多级筛选和全自动迭代数据集流程规避污染；3）每个任务都包含Fail-to-Pass与Pass-to-Pass测试确保实现正确性与不引入回归；4）覆盖多个真实世界领域。并使用当前两大代码智能体框架和四个先进LLM进行实验评测。

Result: 在FeatBench上，最优方法的功能实现成功率仅29.94%，显示出该场景的极大挑战性。同时，模型会表现出“激进实现”现象，这既可能导致严重错误，也有时能带来优秀的软件设计。

Conclusion: 特征实现层面的vibe coding对于现有LLM极具挑战性，该基准丰富了对LLM编程能力的全面理解，并为社区进一步研究提供数据和工具支持。

Abstract: The rapid advancement of Large Language Models (LLMs) has given rise to a
novel software development paradigm known as "vibe coding," where users
interact with coding agents through high-level natural language. However,
existing evaluation benchmarks for code generation inadequately assess an
agent's vibe coding capabilities. Existing benchmarks are misaligned, as they
either require code-level specifications or focus narrowly on issue-solving,
neglecting the critical scenario of feature implementation within the vibe
coding paradiam. To address this gap, we propose FeatBench, a novel benchmark
for vibe coding that focuses on feature implementation. Our benchmark is
distinguished by several key features: 1. Pure Natural Language Prompts. Task
inputs consist solely of abstract natural language descriptions, devoid of any
code or structural hints. 2. A Rigorous & Evolving Data Collection Process.
FeatBench is built on a multi-level filtering pipeline to ensure quality and a
fully automated pipeline to evolve the benchmark, mitigating data
contamination. 3. Comprehensive Test Cases. Each task includes Fail-to-Pass
(F2P) and Pass-to-Pass (P2P) tests to verify correctness and prevent
regressions. 4. Diverse Application Domains. The benchmark includes
repositories from diverse domains to ensure it reflects real-world scenarios.
We evaluate two state-of-the-art agent frameworks with four leading LLMs on
FeatBench. Our evaluation reveals that feature implementation within the vibe
coding paradigm is a significant challenge, with the highest success rate of
only 29.94%. Our analysis also reveals a tendency for "aggressive
implementation," a strategy that paradoxically leads to both critical failures
and superior software design. We release FeatBench, our automated collection
pipeline, and all experimental results to facilitate further community
research.

</details>


### [236] [FLEXI: Benchmarking Full-duplex Human-LLM Speech Interaction](https://arxiv.org/abs/2509.22243)
*Yuan Ge,Saihan Chen,Jingqi Xiao,Xiaoqian Liu,Tong Xiao,Yan Xiang,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: 该论文提出了FLEXI，这是第一个针对全双工人机语音交互大模型的基准测试工具，尤其关注应急场景下的模型中断。


<details>
  <summary>Details</summary>
Motivation: 当前实现自然的人机实时语音对话对全双工大模型依赖较强，但缺乏针对这类模型的系统性评测方法，尤其在紧急中断等特殊场景下的能力评估尚属空白。

Method: 作者设计了FLEXI基准，包含六种多样化的人机交互场景，用于系统性评估模型的延迟、质量与对话有效性，重点测试应急意识、轮次终结和交互延迟等能力，并在开源与商用模型之间进行了对比。

Result: FLEXI揭示了开源模型与商用模型在紧急意识、轮换终止和交互延迟方面存在显著差距。

Conclusion: 作者认为基于下一个token对的预测机制有望实现更加流畅且类人的全双工人机语音交互。

Abstract: Full-Duplex Speech-to-Speech Large Language Models (LLMs) are foundational to
natural human-computer interaction, enabling real-time spoken dialogue systems.
However, benchmarking and modeling these models remains a fundamental
challenge. We introduce FLEXI, the first benchmark for full-duplex LLM-human
spoken interaction that explicitly incorporates model interruption in emergency
scenarios. FLEXI systematically evaluates the latency, quality, and
conversational effectiveness of real-time dialogue through six diverse
human-LLM interaction scenarios, revealing significant gaps between open source
and commercial models in emergency awareness, turn terminating, and interaction
latency. Finally, we suggest that next token-pair prediction offers a promising
path toward achieving truly seamless and human-like full-duplex interaction.

</details>


### [237] [Safety Compliance: Rethinking LLM Safety Reasoning through the Lens of Compliance](https://arxiv.org/abs/2509.22250)
*Wenbin Hu,Huihao Jing,Haochen Shi,Haoran Li,Yangqiu Song*

Main category: cs.CL

TL;DR: 本文提出从法律合规的视角解决大语言模型（LLM）安全问题，制定基于EU AI法案和GDPR的安全合规基准，并利用Group Policy Optimization训练合规推理器，有效提升模型的法律安全表现。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM安全方法基于临时分类，缺乏系统性和严谨性，难以应对复杂、多样的模型行为，无法真正保障模型安全。作者希望通过引入成熟的法律框架，制定明确的安全标准，填补安全与合规之间的空白。

Method: 1. 参考欧盟AI法案与GDPR，设定安全合规标准。2. 基于法律条文生成真实的安全场景，构建全新安全合规基准。3. 采用Group Policy Optimization（GRPO）技术对Qwen3-8B进行对齐，训练安全合规推理器Compliance Reasoner。

Result: 实验证明，Compliance Reasoner在新基准上表现优越，对于EU AI Act平均提升10.45%，对GDPR提升11.85%。

Conclusion: 从法律合规角度定义并提升LLM安全能更有效地防止安全风险，Compliance Reasoner为LLM安全提供了兼具系统性与实效性的解决方案，具备良好应用前景。

Abstract: The proliferation of Large Language Models (LLMs) has demonstrated remarkable
capabilities, elevating the critical importance of LLM safety. However,
existing safety methods rely on ad-hoc taxonomy and lack a rigorous, systematic
protection, failing to ensure safety for the nuanced and complex behaviors of
modern LLM systems. To address this problem, we solve LLM safety from legal
compliance perspectives, named safety compliance. In this work, we posit
relevant established legal frameworks as safety standards for defining and
measuring safety compliance, including the EU AI Act and GDPR, which serve as
core legal frameworks for AI safety and data security in Europe. To bridge the
gap between LLM safety and legal compliance, we first develop a new benchmark
for safety compliance by generating realistic LLM safety scenarios seeded with
legal statutes. Subsequently, we align Qwen3-8B using Group Policy Optimization
(GRPO) to construct a safety reasoner, Compliance Reasoner, which effectively
aligns LLMs with legal standards to mitigate safety risks. Our comprehensive
experiments demonstrate that the Compliance Reasoner achieves superior
performance on the new benchmark, with average improvements of +10.45% for the
EU AI Act and +11.85% for GDPR.

</details>


### [238] [Beyond Textual Context: Structural Graph Encoding with Adaptive Space Alignment to alleviate the hallucination of LLMs](https://arxiv.org/abs/2509.22251)
*Yifang Zhang,Pengfei Duan,Yiwen Yang,Shengwu Xiong*

Main category: cs.CL

TL;DR: 本文提出了一种新模型SSKG-LLM，有效结合知识图谱（KGs）的结构和语义信息，提升大语言模型（LLMs）推理能力，改善模型幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 目前结合知识图谱来缓解大语言模型幻觉问题，但现有方法多将KGs当作纯文本，只调用语义，忽略了结构信息，同时KG编码器与LLMs嵌入空间的鸿沟也妨碍了知识融合。

Method: 提出了SSKG-LLM模型，内含知识图谱检索（KGR）、编码（KGE）以及适配模块（KGA），保留KGs语义并利用结构信息，同时适配LLM对KG嵌入的理解。进行了大量实验和详细分析。

Result: 实验证明将KG结构信息纳入可提升LLM的事实推理能力，并详细分析了其影响。

Conclusion: 结构与语义信息的有效融合能明显提升LLM的准确推理，SSKG-LLM为集成KG提供了一种创新且高效路径。

Abstract: Currently, the main approach for Large Language Models (LLMs) to tackle the
hallucination issue is incorporating Knowledge Graphs(KGs).However, LLMs
typically treat KGs as plain text, extracting only semantic information and
limiting their use of the crucial structural aspects of KGs. Another challenge
is the gap between the embedding spaces of KGs encoders and LLMs text
embeddings, which hinders the effective integration of structured knowledge. To
overcome these obstacles, we put forward the SSKG-LLM, an innovative model
architecture that is designed to efficiently integrate both the Structural and
Semantic information of KGs into the reasoning processes of LLMs. SSKG-LLM
incorporates the Knowledge Graph Retrieval (KGR) module and the Knowledge Graph
Encoding (KGE) module to preserve semantics while utilizing structure. Then,
the Knowledge Graph Adaptation (KGA) module is incorporated to enable LLMs to
understand KGs embeddings. We conduct extensive experiments and provide a
detailed analysis to explore how incorporating the structural information of
KGs can enhance the factual reasoning abilities of LLMs. Our code are available
at https://github.com/yfangZhang/SSKG-LLM.

</details>


### [239] [Bridging Fairness and Explainability: Can Input-Based Explanations Promote Fairness in Hate Speech Detection?](https://arxiv.org/abs/2509.22291)
*Yifan Wang,Mayank Jobanputra,Ji-Ung Lee,Soyoung Oh,Isabel Valera,Vera Demberg*

Main category: cs.CL

TL;DR: 本文系统分析了解释性与公平性在仇恨言论检测任务中的关系，发现基于输入的解释有助于检测和缓解偏见，但不适用于模型选择。


<details>
  <summary>Details</summary>
Motivation: NLP模型存在复制或放大训练数据中的社会偏见问题，加上模型通常为“黑箱”，用户难以察觉偏见、开发者难以缓解偏见。因此，需要系统性分析解释性技术在保障模型公平性中的作用。

Method: 对仇恨言论检测任务，系统性地研究了解释性与公平性的关系，涵盖编码器和解码器两类模型，聚焦于三大方面：检测偏见预测、选择公平模型，以及训练阶段缓解偏见。

Result: 基于输入的解释能有效检测偏见预测，并且在训练时作为监督信号有助于缓解偏见。但在不同模型候选中，基于输入的解释对选择最公平的模型不可靠。

Conclusion: 输入级解释性工具有助于识别和减少模型偏见，但其在模型选择环节的效用有限。未来应结合其它方法共同提升模型公平性。

Abstract: Natural language processing (NLP) models often replicate or amplify social
bias from training data, raising concerns about fairness. At the same time,
their black-box nature makes it difficult for users to recognize biased
predictions and for developers to effectively mitigate them. While some studies
suggest that input-based explanations can help detect and mitigate bias, others
question their reliability in ensuring fairness. Existing research on
explainability in fair NLP has been predominantly qualitative, with limited
large-scale quantitative analysis. In this work, we conduct the first
systematic study of the relationship between explainability and fairness in
hate speech detection, focusing on both encoder- and decoder-only models. We
examine three key dimensions: (1) identifying biased predictions, (2) selecting
fair models, and (3) mitigating bias during model training. Our findings show
that input-based explanations can effectively detect biased predictions and
serve as useful supervision for reducing bias during training, but they are
unreliable for selecting fair models among candidates.

</details>


### [240] [Advancing Natural Language Formalization to First Order Logic with Fine-tuned LLMs](https://arxiv.org/abs/2509.22338)
*Felix Vossel,Till Mossakowski,Björn Gehrke*

Main category: cs.CL

TL;DR: 本文系统评估了微调后的大语言模型（LLMs）在自然语言自动翻译为一阶逻辑（FOL）任务的表现，分析了不同模型结构与训练方法，提出多项性能指标，并在多个数据集上表明微调Flan-T5-XXL模型在该任务上具备领先性能。


<details>
  <summary>Details</summary>
Motivation: 自然语言到一阶逻辑的自动转化对于知识表示和形式化方法至关重要，但该任务仍充满挑战。研究动机在于利用当前先进的大语言模型，通过系统实验探索其在该特定任务下的表现与改进空间。

Method: 作者对比了编码器-解码器结构（如T5）与仅解码器结构的LLMs，并采用多种训练策略（如词汇扩展、谓词条件化、多语种训练），在MALLS和Willow等数据集上进行系统实验。提出了完全匹配、逻辑等价、谓词对齐等新的评价指标，并专门微调Flan-T5-XXL等模型。

Result: 微调后的Flan-T5-XXL模型在有谓词列表辅助的情况下准确率达到70%，超越了GPT-4o和DeepSeek-R1-0528（带CoT能力）以及符号系统ccg2lambda。实验显示：谓词可用性可提升15-20%的表现，T5结构优于更大规模的decoder-only LLMs，并且模型对未见过的逻辑论证（如FOLIO数据集）有较好泛化能力。

Conclusion: 结论认为结构化的一阶逻辑翻译任务已取得较强鲁棒性，但谓词抽取依旧是性能提升的主要瓶颈。

Abstract: Automating the translation of natural language to first-order logic (FOL) is
crucial for knowledge representation and formal methods, yet remains
challenging. We present a systematic evaluation of fine-tuned LLMs for this
task, comparing architectures (encoder-decoder vs. decoder-only) and training
strategies. Using the MALLS and Willow datasets, we explore techniques like
vocabulary extension, predicate conditioning, and multilingual training,
introducing metrics for exact match, logical equivalence, and predicate
alignment. Our fine-tuned Flan-T5-XXL achieves 70% accuracy with predicate
lists, outperforming GPT-4o and even the DeepSeek-R1-0528 model with CoT
reasoning ability as well as symbolic systems like ccg2lambda. Key findings
show: (1) predicate availability boosts performance by 15-20%, (2) T5 models
surpass larger decoder-only LLMs, and (3) models generalize to unseen logical
arguments (FOLIO dataset) without specific training. While structural logic
translation proves robust, predicate extraction emerges as the main bottleneck.

</details>


### [241] [Transformers Can Learn Connectivity in Some Graphs but Not Others](https://arxiv.org/abs/2509.22343)
*Amit Roy,Abulhair Saparov*

Main category: cs.CL

TL;DR: 论文分析了Transformer模型在推断传递关系（如A导致B，B导致C，则A导致C）方面的能力，发现模型在低维‘网格图’任务中表现良好，但维度升高或结构复杂时难度显著上升，且模型规模提升可改善泛化性能。


<details>
  <summary>Details</summary>
Motivation: 推理能力对于保证大语言模型输出的事实正确性至关重要，而传递关系推断在因果推理等多种应用场景中非常关键。现有研究多关注于模型能否通过输入提示内的样例即时学习传递性推断，较少涉及模型通过训练样例学到传递推理能力以及模型规模对推理能力的影响。

Method: 本文通过自动生成有向图，设计传递关系学习任务，并训练不同规模的Transformer模型，在不同图规模和结构上评估模型推断连通性的能力，重点考察‘网格图’和包含多个不连通子图的复杂结构。

Result: 实验结果表明，Transformer能够学习低维网格图的连通性任务，但在维度增加或图结构复杂（如多个不连通子图）时，学习难度显著上升。同时，模型规模扩大会提升网格图任务的泛化能力，但对于不规则、复杂图结构，则提升有限。

Conclusion: Transformer模型能够一定程度上通过训练学习推断‘传递关系’，但能力受限于任务结构和图的嵌入维度，且模型规模提升对部分结构有助益。在更复杂结构或高维图下，推理能力仍然存在显著挑战。

Abstract: Reasoning capability is essential to ensure the factual correctness of the
responses of transformer-based Large Language Models (LLMs), and robust
reasoning about transitive relations is instrumental in many settings, such as
causal inference. Hence, it is essential to investigate the capability of
transformers in the task of inferring transitive relations (e.g., knowing A
causes B and B causes C, then A causes C). The task of inferring transitive
relations is equivalent to the task of connectivity in directed graphs (e.g.,
knowing there is a path from A to B, and there is a path from B to C, then
there is a path from A to C). Past research focused on whether transformers can
learn to infer transitivity from in-context examples provided in the input
prompt. However, transformers' capability to infer transitive relations from
training examples and how scaling affects the ability is unexplored. In this
study, we seek to answer this question by generating directed graphs to train
transformer models of varying sizes and evaluate their ability to infer
transitive relations for various graph sizes. Our findings suggest that
transformers are capable of learning connectivity on "grid-like'' directed
graphs where each node can be embedded in a low-dimensional subspace, and
connectivity is easily inferable from the embeddings of the nodes. We find that
the dimensionality of the underlying grid graph is a strong predictor of
transformers' ability to learn the connectivity task, where higher-dimensional
grid graphs pose a greater challenge than low-dimensional grid graphs. In
addition, we observe that increasing the model scale leads to increasingly
better generalization to infer connectivity over grid graphs. However, if the
graph is not a grid graph and contains many disconnected components,
transformers struggle to learn the connectivity task, especially when the
number of components is large.

</details>


### [242] [The InviTE Corpus: Annotating Invectives in Tudor English Texts for Computational Modeling](https://arxiv.org/abs/2509.22345)
*Sophie Spliethoff,Sanne Hoeken,Silke Schwandt,Sina Zarrieß,Özge Alaçam*

Main category: cs.CL

TL;DR: 该论文提出了一个针对宗教谩骂语的语料库（InviTE），并比较了不同NLP模型在16世纪英国文本谩骂检测上的表现。


<details>
  <summary>Details</summary>
Motivation: 旨在推动NLP技术对历史性文本、特别是宗教谩骂语的研究应用，解决历史领域内特定语境、表达方式难以自动化分析的问题。

Method: 构建了包含近2000句英国16世纪早期现代英语的语料库，并由专家标注谩骂语。流程包括数据获取、预处理、挑选、反复注释。随后对比了历史预训练且针对谩骂检测微调的BERT模型和零样本LLM模型的检测效果。

Result: 结果显示，基于历史数据预训练并经过微调的BERT类模型在检测宗教谩骂语上性能更优。

Conclusion: NLP技术对历史文本谩骂语的研究具有可行性且已取得成果，专门为历史语料进行预训练和微调对提升检测效果非常重要。

Abstract: In this paper, we aim at the application of Natural Language Processing (NLP)
techniques to historical research endeavors, particularly addressing the study
of religious invectives in the context of the Protestant Reformation in Tudor
England. We outline a workflow spanning from raw data, through pre-processing
and data selection, to an iterative annotation process. As a result, we
introduce the InviTE corpus -- a corpus of almost 2000 Early Modern English
(EModE) sentences, which are enriched with expert annotations regarding
invective language throughout 16th-century England. Subsequently, we assess and
compare the performance of fine-tuned BERT-based models and zero-shot prompted
instruction-tuned large language models (LLMs), which highlights the
superiority of models pre-trained on historical data and fine-tuned to
invective detection.

</details>


### [243] [Conversational Implicatures: Modelling Relevance Theory Probabilistically](https://arxiv.org/abs/2509.22354)
*Christoph Unger,Hendrik Buschmeier*

Main category: cs.CL

TL;DR: 本文探讨了将贝叶斯概率理论应用于语用学，尤其是用以建模关联理论中的会话含义传递。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯概率理论及其在认知科学中的进步为语用学和语义学带来新的视角。本文的动机是借助新一代计算方法，探索如何将以往在必要性语用现象上成功的贝叶斯方法，扩展到建模关联理论的含义沟通过程。

Method: 本文以格赖斯式语用现象建模为基础，结合贝叶斯推理方法，通过理性言语行为理论（Rational Speech Act theory）框架，分析会话含义的产生，进而进一步探讨如何用相似的贝叶斯方法来解释和模拟关联理论下的隐含意义交流。

Result: 本研究提出了一种可能的贝叶斯分析方法，用以说明关联理论框架下隐含意义（含义）的传递机制。

Conclusion: 作者认为，贝叶斯方法和概率建模能够为关联理论语用学的隐含意义沟通提供有效的计算解释，并拓展了相关理论的应用范围。

Abstract: Recent advances in Bayesian probability theory and its application to
cognitive science in combination with the development of a new generation of
computational tools and methods for probabilistic computation have led to a
'probabilistic turn' in pragmatics and semantics. In particular, the framework
of Rational Speech Act theory has been developed to model broadly Gricean
accounts of pragmatic phenomena in Bayesian terms, starting with fairly simple
reference games and covering ever more complex communicative exchanges such as
verbal syllogistic reasoning. This paper explores in which way a similar
Bayesian approach might be applied to relevance-theoretic pragmatics (Sperber &
Wilson, 1995) by study a paradigmatic pragmatic phenomenon: the communication
of implicit meaning by ways of (conversational) implicatures.

</details>


### [244] [CHRONOBERG: Capturing Language Evolution and Temporal Awareness in Foundation Models](https://arxiv.org/abs/2509.22360)
*Niharika Hegde,Subarnaduti Paul,Lars Joel-Frey,Manuel Brack,Kristian Kersting,Martin Mundt,Patrick Schramowski*

Main category: cs.CL

TL;DR: 本文提出了一个全新的、按时间结构组织的英文书籍语料库CHRONOBERG，旨在帮助研究自然语言随时间的变化，并用于训练和评估大语言模型（LLM）在历史语境下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型主要依赖社交媒体或网络爬取数据，这些数据缺乏长期的时间结构，导致模型难以把握语言语义和规范随时间的演变。为了解决这一限制，作者希望通过有时间结构的语料库支持模型更好理解和处理跨时间的语言变化。

Method: 作者从Project Gutenberg筛选并整理了250年跨度的英文书籍，构建了CHRONOBERG语料库，并进行丰富的时间标注。通过基于Valence-Arousal-Dominance（VAD）的分析，量化词汇语义随时间的变化，并构建历史校准的情感词典。随后，作者用现代LLM工具演示在不同时期对歧视性语言和情感分析时面临的挑战，并对基于CHRONOBERG顺序训练的模型进行了评估。

Result: 研究发现，即便是按时间顺序用CHRONOBERG训练的语言模型，在处理意义随时间变化（历时变化）时仍然表现不佳。这说明现有模型在时间感知训练和评估流程上存在不足。

Conclusion: CHRONOBERG为研究语言变化与时间泛化提供了可扩展的资源，有助于开发和评估更关注历史语境的自然语言处理工具。论文强调应开发具备时间感知能力的模型和评测方案。

Abstract: Large language models (LLMs) excel at operating at scale by leveraging social
media and various data crawled from the web. Whereas existing corpora are
diverse, their frequent lack of long-term temporal structure may however limit
an LLM's ability to contextualize semantic and normative evolution of language
and to capture diachronic variation. To support analysis and training for the
latter, we introduce CHRONOBERG, a temporally structured corpus of English book
texts spanning 250 years, curated from Project Gutenberg and enriched with a
variety of temporal annotations. First, the edited nature of books enables us
to quantify lexical semantic change through time-sensitive
Valence-Arousal-Dominance (VAD) analysis and to construct historically
calibrated affective lexicons to support temporally grounded interpretation.
With the lexicons at hand, we demonstrate a need for modern LLM-based tools to
better situate their detection of discriminatory language and contextualization
of sentiment across various time-periods. In fact, we show how language models
trained sequentially on CHRONOBERG struggle to encode diachronic shifts in
meaning, emphasizing the need for temporally aware training and evaluation
pipelines, and positioning CHRONOBERG as a scalable resource for the study of
linguistic change and temporal generalization. Disclaimer: This paper includes
language and display of samples that could be offensive to readers. Open
Access: Chronoberg is available publicly on HuggingFace at (
https://huggingface.co/datasets/spaul25/Chronoberg). Code is available at
(https://github.com/paulsubarna/Chronoberg).

</details>


### [245] [Exploratory Semantic Reliability Analysis of Wind Turbine Maintenance Logs using Large Language Models](https://arxiv.org/abs/2509.22366)
*Max Malyi,Jonathan Shek,Andre Biscaya*

Main category: cs.CL

TL;DR: 本文利用大型语言模型（LLM）对风电机组运维日志中的非结构化文本进行深入分析，提出了一种新方法，可实现比传统分类更复杂的推理任务。


<details>
  <summary>Details</summary>
Motivation: 风电机组的运维日志中蕴含丰富信息，由于大部分为自由文本形式，传统定量分析难以有效利用。现有机器学习方法往往局限于将文本分类，无法实现更深层次的语义推理，限制了行业智能化水平的提升。

Method: 作者提出了一种基于LLM的探索性分析框架，能够对非结构化文本进行语义分析和推理。具体实践中，框架涵盖故障模式识别、因果链推断、站点对比分析和数据质量审计四种分析流程，并在大规模工业数据集上进行验证。

Result: 实验结果表明，LLM不仅能完成传统分类任务，还能生成具备专家水准的推理和假设，对文本信息进行综合性分析，有效挖掘可操作性见解。

Conclusion: 本文提出了一种可复现的新方法，使LLM成为可靠的语义推理工具，为风电行业运维领域的智能化提供了全新思路，成功释放了自由文本数据中的洞察力。

Abstract: A wealth of operational intelligence is locked within the unstructured
free-text of wind turbine maintenance logs, a resource largely inaccessible to
traditional quantitative reliability analysis. While machine learning has been
applied to this data, existing approaches typically stop at classification,
categorising text into predefined labels. This paper addresses the gap in
leveraging modern large language models (LLMs) for more complex reasoning
tasks. We introduce an exploratory framework that uses LLMs to move beyond
classification and perform deep semantic analysis. We apply this framework to a
large industrial dataset to execute four analytical workflows: failure mode
identification, causal chain inference, comparative site analysis, and data
quality auditing. The results demonstrate that LLMs can function as powerful
"reliability co-pilots," moving beyond labelling to synthesise textual
information and generate actionable, expert-level hypotheses. This work
contributes a novel and reproducible methodology for using LLMs as a reasoning
tool, offering a new pathway to enhance operational intelligence in the wind
energy sector by unlocking insights previously obscured in unstructured data.

</details>


### [246] [What Is The Political Content in LLMs' Pre- and Post-Training Data?](https://arxiv.org/abs/2509.22367)
*Tanise Ceron,Dmitry Nikolaev,Dominik Stammbach,Debora Nozza*

Main category: cs.CL

TL;DR: 本文系统分析了OLMO2大语言模型训练前后语料的政治倾向，发现语料以左倾为主，这种分布与模型政策立场上的政治偏见高度相关。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型生成内容存在政治偏见，但这种偏见的来源尚不明确，特别是训练数据的政治属性鲜有系统性研究。通过分析训练语料，有助于阐明模型偏见的成因。

Method: 对OLMO2模型的公开训练和微调语料库进行大样本随机抽取，自动标注文档的政治倾向，分析其来源、内容，并考察语料政治内容与模型在政策问题上立场的相关性。

Result: 训练前语料左倾文档占主导，且包含更多政治相关内容；左、右倾文档对相同议题的价值观和合法性框架有所不同；训练数据的主流政治立场与模型输出在政策议题上的偏见高度相关。

Conclusion: 模型所依赖训练数据的政治构成对其输出政治倾向影响巨大，未来数据筛选和语料文档应加强对政治内容和过滤方案的透明披露与记录。

Abstract: Large language models (LLMs) are known to generate politically biased text,
yet how such biases arise remains unclear. A crucial step toward answering this
question is the analysis of training data, whose political content remains
largely underexplored in current LLM research. To address this gap, we present
in this paper an analysis of the pre- and post-training corpora of OLMO2, the
largest fully open-source model released together with its complete dataset.
From these corpora, we draw large random samples, automatically annotate
documents for political orientation, and analyze their source domains and
content. We then assess how political content in the training data correlates
with models' stance on specific policy issues. Our analysis shows that
left-leaning documents predominate across datasets, with pre-training corpora
containing significantly more politically engaged content than post-training
data. We also find that left- and right-leaning documents frame similar topics
through distinct values and sources of legitimacy. Finally, the predominant
stance in the training data strongly correlates with models' political biases
when evaluated on policy issues. These findings underscore the need to
integrate political content analysis into future data curation pipelines as
well as in-depth documentation of filtering strategies for transparency.

</details>


### [247] [Chimera: Diagnosing Shortcut Learning in Visual-Language Understanding](https://arxiv.org/abs/2509.22437)
*Ziheng Chi,Yifan Hou,Chenxi Pang,Shaobo Cui,Mubashara Akhtar,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 该论文提出了一个名为Chimera的新测试集，专门评估视觉语言模型（VLMs）对图示理解的真实性能，并发现目前模型的强劲表现主要依赖于捷径行为而非真正的图示理解。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型虽然在图示相关的基准测试上表现良好，但其得分可能来源于对知识、推理或模态间捷径的利用，并不代表模型真正理解和推理了图示内容。因此，研究者希望揭示模型是否存在利用捷径获取高分的现象，并推动更真实的图示理解能力评测。

Method: 作者构建了Chimera测试集，收集了7500个高质量、标注详尽（语义三元组标注及多级问题）的维基百科图示，考查四大图示理解能力：实体识别、关系理解、知识连接和视觉推理。然后，定义和测试了三种捷径行为（视觉记忆、知识召回以及“聪明汉斯”现象），并用7个模型家族的15个开源VLMs做系统评估。

Result: 实验发现，视觉记忆捷径对模型表现影响有限，知识召回起到中等作用，而聪明汉斯捷径（即滥用表层语言或先验）贡献最大，说明目前模型的高分很大程度源于捷径行为。

Conclusion: 目前主流VLMs在图示理解方面存在显著局限，迫切需要更健壮的评测方法，推动模型实际理解复杂视觉信息而非利用答题捷径。

Abstract: Diagrams convey symbolic information in a visual format rather than a linear
stream of words, making them especially challenging for AI models to process.
While recent evaluations suggest that vision-language models (VLMs) perform
well on diagram-related benchmarks, their reliance on knowledge, reasoning, or
modality shortcuts raises concerns about whether they genuinely understand and
reason over diagrams. To address this gap, we introduce Chimera, a
comprehensive test suite comprising 7,500 high-quality diagrams sourced from
Wikipedia; each diagram is annotated with its symbolic content represented by
semantic triples along with multi-level questions designed to assess four
fundamental aspects of diagram comprehension: entity recognition, relation
understanding, knowledge grounding, and visual reasoning. We use Chimera to
measure the presence of three types of shortcuts in visual question answering:
(1) the visual-memorization shortcut, where VLMs rely on memorized visual
patterns; (2) the knowledge-recall shortcut, where models leverage memorized
factual knowledge instead of interpreting the diagram; and (3) the Clever-Hans
shortcut, where models exploit superficial language patterns or priors without
true comprehension. We evaluate 15 open-source VLMs from 7 model families on
Chimera and find that their seemingly strong performance largely stems from
shortcut behaviors: visual-memorization shortcuts have slight impact,
knowledge-recall shortcuts play a moderate role, and Clever-Hans shortcuts
contribute significantly. These findings expose critical limitations in current
VLMs and underscore the need for more robust evaluation protocols that
benchmark genuine comprehension of complex visual inputs (e.g., diagrams)
rather than question-answering shortcuts.

</details>


### [248] [Detecting (Un)answerability in Large Language Models with Linear Directions](https://arxiv.org/abs/2509.22449)
*Maor Juliet Lavi,Tova Milo,Mor Geva*

Main category: cs.CL

TL;DR: 本文提出了一种简单方法，通过在大型语言模型（LLM）的激活空间中寻找能够刻画不可回答性（unanswerability）的方向，实现对问答任务中不可回答问题的检测，并取得了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 许多LLM即使在缺乏必要信息时，也会自信地做出回答，导致虚假的输出（幻觉问题）。因此，准确检测模型面对问题时能否可靠作答（识别不可回答性）成为亟需解决的问题。

Method: 作者提出在LLM的激活空间中寻找一个能刻画不可回答性的重要方向，通过“激活叠加法”在推理阶段调整激活，并监测其对模型拒答行为的影响，从而获得一个用于分类不可回答问题的得分。通过将模型隐层激活向该方向投影，实现（不可）回答性的分类。

Result: 在两个开放权重的LLM和四个抽取式问答基准数据集上的实验证明，该方法能有效检测不可回答的问题，并在跨数据集泛化能力上优于现有的基于提示和分类器的方法。此外，所找到的方向还适用于科学共识缺失、主观性高等其他不可回答情形。因果干预实验表明，通过加或去除这些方向，可以有效控制模型的拒答行为。

Conclusion: 该方法为问答任务中不可回答性检测提供了一种简单、有效且具有良好泛化能力的解决方案，同时还能有效操控模型的拒答输出，对解决LLM幻觉问题具有实际意义。

Abstract: Large language models (LLMs) often respond confidently to questions even when
they lack the necessary information, leading to hallucinated answers. In this
work, we study the problem of (un)answerability detection, focusing on
extractive question answering (QA) where the model should determine if a
passage contains sufficient information to answer a given question. We propose
a simple approach for identifying a direction in the model's activation space
that captures unanswerability and uses it for classification. This direction is
selected by applying activation additions during inference and measuring their
impact on the model's abstention behavior. We show that projecting hidden
activations onto this direction yields a reliable score for (un)answerability
classification. Experiments on two open-weight LLMs and four extractive QA
benchmarks show that our method effectively detects unanswerable questions and
generalizes better across datasets than existing prompt-based and
classifier-based approaches. Moreover, the obtained directions extend beyond
extractive QA to unanswerability that stems from factors, such as lack of
scientific consensus and subjectivity. Last, causal interventions show that
adding or ablating the directions effectively controls the abstention behavior
of the model.

</details>


### [249] [Evaluating the Limits of Large Language Models in Multilingual Legal Reasoning](https://arxiv.org/abs/2509.22472)
*Antreas Ioannou,Andreas Shiamishis,Nora Hollenstein,Nezihe Merve Gürel*

Main category: cs.CL

TL;DR: 本论文对LLaMA和Gemini两个主流大型语言模型在多语言法律任务中的表现进行系统评估，发现其在法律推理等高要求场景的准确率远低于一般任务。在不同语言和对抗场景下均存在不稳定性和脆弱性。作者还开发了一个开源评测平台，结果显示，Gemini整体优于LLaMA，但二者均难以胜任多语、关键的法律应用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型逐步被用于法律等关键领域，但其真实性能、尤其是在多语言和对抗性条件下的表现和局限尚不明晰。准确了解这些模型在复杂法律任务中的能力和瓶颈，对安全和有效部署尤为重要。

Method: 本文在多语言法律与通用基准测试上评估了LLaMA和Gemini，并通过字符与词级扰动检验其在法律任务下的对抗鲁棒性；采用“模型担任评委”的评估方案。同时，作者开发了一个开源、模块化、多语言的评测流水线，适配各种模型与任务。

Result: 实验发现法律任务对模型难度极高，如LEXam法律推理基准准确率常低于50%，远低于通用任务（如XNLI的70%+）；英语虽有更稳定的结果但非总是最佳，提示脆弱性跨语种存在。Gemini整体优于LLaMA约24个百分点。目标语种与英语句法结构相似度越高，表现越好。

Conclusion: 尽管最新LLM模型在性能上有提升，但在多语、复杂法律场景仍有显著缺陷，对高风险法律工作流程不适宜完全信赖。未来需继续改进模型，完善评测工具，以提升其在法律等关键领域的适用性与可靠性。

Abstract: In an era dominated by Large Language Models (LLMs), understanding their
capabilities and limitations, especially in high-stakes fields like law, is
crucial. While LLMs such as Meta's LLaMA, OpenAI's ChatGPT, Google's Gemini,
DeepSeek, and other emerging models are increasingly integrated into legal
workflows, their performance in multilingual, jurisdictionally diverse, and
adversarial contexts remains insufficiently explored. This work evaluates LLaMA
and Gemini on multilingual legal and non-legal benchmarks, and assesses their
adversarial robustness in legal tasks through character and word-level
perturbations. We use an LLM-as-a-Judge approach for human-aligned evaluation.
We moreover present an open-source, modular evaluation pipeline designed to
support multilingual, task-diverse benchmarking of any combination of LLMs and
datasets, with a particular focus on legal tasks, including classification,
summarization, open questions, and general reasoning. Our findings confirm that
legal tasks pose significant challenges for LLMs with accuracies often below
50% on legal reasoning benchmarks such as LEXam, compared to over 70% on
general-purpose tasks like XNLI. In addition, while English generally yields
more stable results, it does not always lead to higher accuracy. Prompt
sensitivity and adversarial vulnerability is also shown to persist across
languages. Finally, a correlation is found between the performance of a
language and its syntactic similarity to English. We also observe that LLaMA is
weaker than Gemini, with the latter showing an average advantage of about 24
percentage points across the same task. Despite improvements in newer LLMs,
challenges remain in deploying them reliably for critical, multilingual legal
applications.

</details>


### [250] [NeLLCom-Lex: A Neural-agent Framework to Study the Interplay between Lexical Systems and Language Use](https://arxiv.org/abs/2509.22479)
*Yuqing Zhang,Ecesu Ürker,Tessa Verhoef,Gemma Boleda,Arianna Bisazza*

Main category: cs.CL

TL;DR: 本文提出了一种新型的神经智能体模拟框架NeLLCom-Lex，用于研究词汇语义变化背后的因果机制，通过在颜色命名任务下模拟词汇系统的演化，展示出神经体代理可以在一定程度上再现人类的命名行为及其变化。


<details>
  <summary>Details</summary>
Motivation: 传统的语义变化研究手段（如语料库分析和分布式语义建模）难以揭示因果机制，而人类实验范式又受限于语义变化的长期性质，缺乏实验可行性。因此，作者希望通过可控的神经体模拟来解决这一难题。

Method: 提出了NeLLCom-Lex神经智能体框架，首先使智能体在真实语言系统（如英语）中进行训练，再系统性地操控其交流需求。借助成熟的颜色命名任务，通过不同的有监督与强化学习方法，模拟词汇在一代内随交流需求演变。

Result: 神经代理在训练后能够在颜色命名等任务上产生与人类类似的命名方式和词汇系统变化，尤其在改变交流需求时表现出合理的适应行为，支持其模拟人类语义变化的能力。

Conclusion: NeLLCom-Lex能够有效再现和解释人类词汇语义变化的机制，为理解语义变化提供了新的实验平台和工具，推动语义变化因果机制的深入研究。

Abstract: Lexical semantic change has primarily been investigated with observational
and experimental methods; however, observational methods (corpus analysis,
distributional semantic modeling) cannot get at causal mechanisms, and
experimental paradigms with humans are hard to apply to semantic change due to
the extended diachronic processes involved. This work introduces NeLLCom-Lex, a
neural-agent framework designed to simulate semantic change by first grounding
agents in a real lexical system (e.g. English) and then systematically
manipulating their communicative needs. Using a well-established color naming
task, we simulate the evolution of a lexical system within a single generation,
and study which factors lead agents to: (i) develop human-like naming behavior
and lexicons, and (ii) change their behavior and lexicons according to their
communicative needs. Our experiments with different supervised and
reinforcement learning pipelines show that neural agents trained to 'speak' an
existing language can reproduce human-like patterns in color naming to a
remarkable extent, supporting the further use of NeLLCom-Lex to elucidate the
mechanisms of semantic change.

</details>


### [251] [Exploring Solution Divergence and Its Effect on Large Language Model Problem Solving](https://arxiv.org/abs/2509.22480)
*Hang Li,Kaiqi Yang,Yucheng Chu,Hui Liu,Jiliang Tang*

Main category: cs.CL

TL;DR: 大语言模型（LLM）在解决问题任务上表现出色。本文提出了“解答分歧”这一新视角，将不同解答的多样性作为衡量和提升模型能力的新指标。实验结果显示，实现更高的解答分歧可以提升模型的成功率。


<details>
  <summary>Details</summary>
Motivation: 当前大多数提升LLM性能的方法主要依赖有监督微调（SFT）和基于反馈的强化学习（RL），但很少关注模型对同一问题生成不同答案的多样性。论文希望探索这种多样性与模型问题解决能力之间的关系，从而为模型训练和评估提供新方法。

Method: 作者对不同LLM在同一问题上的多样解答情况进行了实验，并考察了解答分歧度与模型表现之间的相关性。在此基础上，将解答分歧作为新的指标融入SFT和RL训练过程中，并在三个代表性问题领域进行了测试。

Result: 实验证明，引入解答分歧指标能够稳定提升在多个问题域中模型的解答成功率。更高的答案多样性与更强的问题解决能力正相关。

Conclusion: 解答分歧是一个简单有效的新指标，能够辅助LLM的训练与评估，建议在相关领域予以关注和使用。

Abstract: Large language models (LLMs) have been widely used for problem-solving tasks.
Most recent work improves their performance through supervised fine-tuning
(SFT) with labeled data or reinforcement learning (RL) from task feedback. In
this paper, we study a new perspective: the divergence in solutions generated
by LLMs for a single problem. We show that higher solution divergence is
positively related to better problem-solving abilities across various models.
Based on this finding, we propose solution divergence as a novel metric that
can support both SFT and RL strategies. We test this idea on three
representative problem domains and find that using solution divergence
consistently improves success rates. These results suggest that solution
divergence is a simple but effective tool for advancing LLM training and
evaluation.

</details>


### [252] [JGU Mainz's Submission to the WMT25 Shared Task on LLMs with Limited Resources for Slavic Languages: MT and QA](https://arxiv.org/abs/2509.22490)
*Hossain Shaikh Saadi,Minh Duc Bui,Mario Sanz-Guerrero,Katharina von der Wense*

Main category: cs.CL

TL;DR: 本文提出了一种针对乌克兰语、上索布语和下索布语的多任务机器翻译与问答系统，通过参数高效微调Qwen2.5-3B-Instruct模型，并融合外部数据，实验结果优于基线。


<details>
  <summary>Details</summary>
Motivation: 斯拉夫小语种在资源有限环境下的机器翻译和问答任务存在模型和数据瓶颈，需要有效的多任务模型和高效的数据利用方法。

Method: 为乌克兰语、上索布语和下索布语分别采用Qwen2.5-3B-Instruct模型，通过参数高效微调，实现机器翻译和问答的联合训练；引入额外的翻译与选择题问答数据；乌克兰语问答任务额外引入检索增强生成方法，上下索布语问答任务使用模型集成。

Result: 所有实验模型在机器翻译与问答任务上的表现均超过了任务基线。

Conclusion: 联合任务微调、数据融合及检索增强等技术能有效提升有限资源环境下斯拉夫小语种的机器翻译和问答能力。

Abstract: This paper presents the JGU Mainz submission to the WMT25 Shared Task on LLMs
with Limited Resources for Slavic Languages: Machine Translation and Question
Answering, focusing on Ukrainian, Upper Sorbian, and Lower Sorbian. For each
language, we jointly fine-tune a Qwen2.5-3B-Instruct model for both tasks with
parameter-efficient finetuning. Our pipeline integrates additional translation
and multiple-choice question answering (QA) data. For Ukrainian QA, we further
use retrieval-augmented generation. We also apply ensembling for QA in Upper
and Lower Sorbian. Experiments show that our models outperform the baseline on
both tasks.

</details>


### [253] [Representing LLMs in Prompt Semantic Task Space](https://arxiv.org/abs/2509.22506)
*Idan Kashani,Avi Mendelson,Yaniv Nemcovsky*

Main category: cs.CL

TL;DR: 作者提出了一种高效、无需训练的新方法，用于表示和评估大型语言模型（LLM），该方法提升了可解释性和可扩展性，并在模型选择和任务预测中取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 随着LLM模型数量和任务类型的快速增加，如何高效地选择最适合某一任务的LLM成为挑战。以往方法存在扩展性差和需要高昂再训练成本的问题，且模型表示难以解释。

Method: 提出了一种基于线性算子的表示方法，将大型语言模型映射到语义任务空间中。该方法依赖解析式几何性质的计算，无需额外训练，且能很好地适应不断扩大的开源模型仓库。

Result: 在模型选择和任务成功预测等应用中，该方法实现了有竞争力甚至是最优的性能，尤其在未见过的样本场景下表现突出。

Conclusion: 这种新的LLM表示方法不仅能高效且实时地适应新模型，还易于解释，在大规模模型管理和应用中具有重要价值。

Abstract: Large language models (LLMs) achieve impressive results over various tasks,
and ever-expanding public repositories contain an abundance of pre-trained
models. Therefore, identifying the best-performing LLM for a given task is a
significant challenge. Previous works have suggested learning LLM
representations to address this. However, these approaches present limited
scalability and require costly retraining to encompass additional models and
datasets. Moreover, the produced representation utilizes distinct spaces that
cannot be easily interpreted. This work presents an efficient, training-free
approach to representing LLMs as linear operators within the prompts' semantic
task space, thus providing a highly interpretable representation of the models'
application. Our method utilizes closed-form computation of geometrical
properties and ensures exceptional scalability and real-time adaptability to
dynamically expanding repositories. We demonstrate our approach on success
prediction and model selection tasks, achieving competitive or state-of-the-art
results with notable performance in out-of-sample scenarios.

</details>


### [254] [We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before They Go Wrong](https://arxiv.org/abs/2509.22510)
*Gautam Siddharth Kashyap,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 提出了一种名为AMBS的适应性多分支操控方法，实现了大语言模型关于多目标（有用性、无害性、诚实性）的高效统一对齐，提升了模型输出一致性和安全性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在部署过程中需同时满足有用性、无害性和诚实性等多维对齐目标，现有方法在优化其中一个目标时容易影响另一个目标（灾难性遗忘），多分支方法又易导致各目标输出不一致。为实现多目标下的统一且高效对齐，提出新方法。

Method: 提出了两阶段一对多(1-to-N)框架AMBS：第一阶段利用Transformer层生成共享表示；第二阶段将该表示复制至多个分支，结合策略-参考机制分别引导每个分支，使之既区分各对齐目标又保证一致性。

Result: 在Alpaca、BeaverTails和TruthfulQA上对多个7B大模型实验证明AMBS方法提升了HHH多目标对齐性能。例如在DeepSeek-7B模型上，平均对齐分数提升32.4%，输出不安全内容减少11%，并与现有最优方法性能持平。

Conclusion: AMBS能高效实现多目标对齐，既克服灾难性遗忘，也兼顾各分支一致性，是一种有效的大语言模型多目标对齐新框架。

Abstract: Alignment of Large Language Models (LLMs) along multiple
objectives-helpfulness, harmlessness, and honesty (HHH)-is critical for safe
and reliable deployment. Prior work has used steering vector-small control
signals injected into hidden states-to guide LLM outputs, typically via
one-to-one (1-to-1) Transformer decoders. In this setting, optimizing a single
alignment objective can inadvertently overwrite representations learned for
other objectives, leading to catastrophic forgetting. More recent approaches
extend steering vectors via one-to-many (1-to-N) Transformer decoders. While
this alleviates catastrophic forgetting, naive multi-branch designs optimize
each objective independently, which can cause inference fragmentation-outputs
across HHH objectives may become inconsistent. We propose Adaptive Multi-Branch
Steering (AMBS), a two-stage 1-to-N framework for unified and efficient
multi-objective alignment. In Stage I, post-attention hidden states of the
Transformer layer are computed once to form a shared representation. In Stage
II, this representation is cloned into parallel branches and steered via a
policy-reference mechanism, enabling objective-specific control while
maintaining cross-objective consistency. Empirical evaluations on Alpaca,
BeaverTails, and TruthfulQA show that AMBS consistently improves HHH alignment
across multiple 7B LLM backbones. For example, on DeepSeek-7B, AMBS improves
average alignment scores by +32.4% and reduces unsafe outputs by 11.0% compared
to a naive 1-to-N baseline, while remaining competitive with state-of-the-art
methods.

</details>


### [255] [InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models](https://arxiv.org/abs/2509.22536)
*Wenjun Wang,Shuo Cai,Congkai Xie,Mingfa Feng,Yiming Zhang,Zhen Li,Kejing Yang,Ming Li,Jiannong Cao,Yuan Xie,Hongxia Yang*

Main category: cs.CL

TL;DR: 提出了一套端到端的FP8训练方案，显著提升了LLM训练效率，并将在社区开源。


<details>
  <summary>Details</summary>
Motivation: LLMs训练算力消耗巨大，阻碍创新。FP8量化理论上能大幅节省资源，但缺乏完整公开的训练实践方案，限制其应用。

Method: 提出一种端到端FP8训练流程，涵盖持续预训练和有监督微调，并采用细粒度混合量化策略，兼顾数值精度与效率。

Result: 在160B-token语料上持续预训练，实验显示精度几乎无损，与BF16持平。训练时间减少22%，峰值显存降低14%，吞吐提升19%。

Conclusion: FP8可作为高效可靠的替代方案，推动大模型训练民主化，并承诺代码开源。

Abstract: The immense computational cost of training Large Language Models (LLMs)
presents a major barrier to innovation. While FP8 training offers a promising
solution with significant theoretical efficiency gains, its widespread adoption
has been hindered by the lack of a comprehensive, open-source training recipe.
To bridge this gap, we introduce an end-to-end FP8 training recipe that
seamlessly integrates continual pre-training and supervised fine-tuning. Our
methodology employs a fine-grained, hybrid-granularity quantization strategy to
maintain numerical fidelity while maximizing computational efficiency. Through
extensive experiments, including the continue pre-training of models on a
160B-token corpus, we demonstrate that our recipe is not only remarkably stable
but also essentially lossless, achieving performance on par with the BF16
baseline across a suite of reasoning benchmarks. Crucially, this is achieved
with substantial efficiency improvements, including up to a 22% reduction in
training time, a 14% decrease in peak memory usage, and a 19% increase in
throughput. Our results establish FP8 as a practical and robust alternative to
BF16, and we will release the accompanying code to further democratize
large-scale model training.

</details>


### [256] [Think Socially via Cognitive Reasoning](https://arxiv.org/abs/2509.22546)
*Jinfeng Zhou,Zheyu Chen,Shuai Wang,Quanyu Dai,Zhenhua Dong,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: 本文提出了一种新的范式——认知推理（Cognitive Reasoning），以提升大型语言模型（LLMs）在社会情境中的推理和决策能力，并提出了CogFlow框架，用于实现这一能力。


<details>
  <summary>Details</summary>
Motivation: 传统LLMs虽然擅长逻辑推理，但在处理包含模糊和不确定性的社会情境时表现不足，亟需提升其社会认知和解释能力。

Method: 作者提出以人类社会认知为蓝本，将解释性推理过程结构化为多个相互关联的认知单元（如观察、归因等），通过CogFlow框架，先利用树状规划模拟人类渐进式联想思维，构建认知流数据集，再通过有监督微调和结合多目标奖励的强化学习，训练LLMs进行认知推理。

Result: 实验结果表明，经过CogFlow训练的LLM在社会认知和决策任务上表现显著提升，甚至对人类的决策也有辅助作用。

Conclusion: CogFlow显著增强了模型的社会认知推理能力，有助于提升LLMs在复杂、有歧义的社会情景下的解释和响应质量。

Abstract: LLMs trained for logical reasoning excel at step-by-step deduction to reach
verifiable answers. However, this paradigm is ill-suited for navigating social
situations, which induce an interpretive process of analyzing ambiguous cues
that rarely yield a definitive outcome. To bridge this gap, we introduce
Cognitive Reasoning, a paradigm modeled on human social cognition. It
formulates the interpretive process into a structured cognitive flow of
interconnected cognitive units (e.g., observation or attribution), which
combine adaptively to enable effective social thinking and responses. We then
propose CogFlow, a complete framework that instills this capability in LLMs.
CogFlow first curates a dataset of cognitive flows by simulating the
associative and progressive nature of human thought via tree-structured
planning. After instilling the basic cognitive reasoning capability via
supervised fine-tuning, CogFlow adopts reinforcement learning to enable the
model to improve itself via trial and error, guided by a multi-objective reward
that optimizes both cognitive flow and response quality. Extensive experiments
show that CogFlow effectively enhances the social cognitive capabilities of
LLMs, and even humans, leading to more effective social decision-making.

</details>


### [257] [Retrieval-Augmented Guardrails for AI-Drafted Patient-Portal Messages: Error Taxonomy Construction and Large-Scale Evaluation](https://arxiv.org/abs/2509.22565)
*Wenyuan Chen,Fateme Nateghi Haredasht,Kameron C. Black,Francois Grolleau,Emily Alsentzer,Jonathan H. Chen,Stephen P. Ma*

Main category: cs.CL

TL;DR: 本研究提出了一套结合检索增强评价流程（RAEC）和两阶段DSPy提示架构，用于提升大语言模型在患者-医生异步消息回复中的错误检测和评估能力，验证其能够更好地发现和标注临床错误。


<details>
  <summary>Details</summary>
Motivation: 随着电子健康档案（EHR）门户中异步患者-医生消息的增多，医生负担加重。大语言模型有望辅助回复，但其输出容易出现临床错误或语气不当，急需更健全的自动化评估方法辅助质量把控。

Method: （1）构建了一个依据临床场景的错误本体，涵盖5大类、59种细分错误；（2）开发基于语义相似历史消息-回复检索的检索增强评估流程（RAEC）；（3）设计两阶段的DSPy推理流程，实现高可扩展性、可解释、层次化的错误检测。

Result: 与单独分析回复相比，参考相似历史对话能更好识别临床不完整和工作流不当等错误。在1,500条消息上，参考增强模型的标注在人类验证中获得更高一致性（50%对33%）和F1分数（0.500对0.256）。

Conclusion: 检索增强评价流程（RAEC）结合历史案例，显著提升了AI回复的错误检测质量和一致性，为基于大语言模型的患者-医生交流系统提供了有力的安全保障。

Abstract: Asynchronous patient-clinician messaging via EHR portals is a growing source
of clinician workload, prompting interest in large language models (LLMs) to
assist with draft responses. However, LLM outputs may contain clinical
inaccuracies, omissions, or tone mismatches, making robust evaluation
essential. Our contributions are threefold: (1) we introduce a clinically
grounded error ontology comprising 5 domains and 59 granular error codes,
developed through inductive coding and expert adjudication; (2) we develop a
retrieval-augmented evaluation pipeline (RAEC) that leverages semantically
similar historical message-response pairs to improve judgment quality; and (3)
we provide a two-stage prompting architecture using DSPy to enable scalable,
interpretable, and hierarchical error detection. Our approach assesses the
quality of drafts both in isolation and with reference to similar past
message-response pairs retrieved from institutional archives. Using a two-stage
DSPy pipeline, we compared baseline and reference-enhanced evaluations on over
1,500 patient messages. Retrieval context improved error identification in
domains such as clinical completeness and workflow appropriateness. Human
validation on 100 messages demonstrated superior agreement (concordance = 50%
vs. 33%) and performance (F1 = 0.500 vs. 0.256) of context-enhanced labels vs.
baseline, supporting the use of our RAEC pipeline as AI guardrails for patient
messaging.

</details>


### [258] [Fine-Grained Detection of Context-Grounded Hallucinations Using LLMs](https://arxiv.org/abs/2509.22582)
*Yehonatan Pesiakhovsky,Zorik Gekhman,Yosi Mass,Liat Ein-Dor,Roi Reichart*

Main category: cs.CL

TL;DR: 本文关注于大型语言模型（LLM）识别“与上下文无关的幻觉”——即模型输出中无法由原始文本验证的信息。作者构建了一个专门用于LLM的幻觉定位基准，并进行了超过1000个案例的人类标注。通过新的自由文本描述方式、LLM评测协议和大规模实验，发现现有LLM在此任务中表现有限。


<details>
  <summary>Details</summary>
Motivation: 当前的幻觉定位评测方法流程复杂，缺乏标准基准，且无法覆盖所有幻觉类型。作者希望开发一种更实用、更广泛表达幻觉的新方法，用以评估和提升LLM在该任务上的表现。

Method: 1. 构建适合LLM的幻觉定位基准，包括1000余个人类标注样本。2. 设计基于LLM的评测协议，并用人工评估其质量。3. 提出幻觉的新表达方式，即用自由形式文本描述各种错误。4. 系统评测4个主流LLM表现，分析策略与挑战。

Result: 建立了高难度基准，4个主流LLM在该任务表现不佳，最佳F1分数仅为0.67。分析指出LLM主要误判为：一是将遗漏细节错误地标为不一致，二是难以分辨输出中的外部事实（模型知识）与文本来源。

Conclusion: 当前LLM在幻觉定位任务中仍有较大挑战。基准和新表示方法能更好检测模型弱点。未来需改进提示策略并寻求提升LLM对不可验证信息的识别能力。

Abstract: Context-grounded hallucinations are cases where model outputs contain
information not verifiable against the source text. We study the applicability
of LLMs for localizing such hallucinations, as a more practical alternative to
existing complex evaluation pipelines. In the absence of established benchmarks
for meta-evaluation of hallucinations localization, we construct one tailored
to LLMs, involving a challenging human annotation of over 1,000 examples. We
complement the benchmark with an LLM-based evaluation protocol, verifying its
quality in a human evaluation. Since existing representations of hallucinations
limit the types of errors that can be expressed, we propose a new
representation based on free-form textual descriptions, capturing the full
range of possible errors. We conduct a comprehensive study, evaluating four
large-scale LLMs, which highlights the benchmark's difficulty, as the best
model achieves an F1 score of only 0.67. Through careful analysis, we offer
insights into optimal prompting strategies for the task and identify the main
factors that make it challenging for LLMs: (1) a tendency to incorrectly flag
missing details as inconsistent, despite being instructed to check only facts
in the output; and (2) difficulty with outputs containing factually correct
information absent from the source - and thus not verifiable - due to alignment
with the model's parametric knowledge.

</details>


### [259] [ArabJobs: A Multinational Corpus of Arabic Job Ads](https://arxiv.org/abs/2509.22589)
*Mo El-Haj*

Main category: cs.CL

TL;DR: 该论文介绍了一个名为ArabJobs的阿拉伯语招聘广告公开数据集，包含四个国家8500余条广告及相关分析与应用。


<details>
  <summary>Details</summary>
Motivation: 当前阿拉伯语相关的招聘数据稀缺，限制了在劳动力市场和NLP领域关于性别、公平性、方言等研究的开展。研究者希望通过发布和分析综合的招聘广告数据集，填补这一空白并促进相关研究。

Method: 研究者从埃及、约旦、沙特阿拉伯和阿联酋收集了8500多份招聘广告，构建了公开数据集ArabJobs，并对性别表现、职业结构和广告中的方言变化进行分析。他们还以薪资预测、职位范畴归一化、性别偏见检测和职业分类为案例，利用大型语言模型进行实验，展示应用场景。

Result: 分析揭示了不同区域和职业的语言及性别表现差异，展示了阿拉伯语方言的丰富性。实验表明利用大型语言模型可以很好地进行职位分类、性别偏见检测等任务，数据集对于公平性相关的阿拉伯语NLP任务表现出了很好的适用性。

Conclusion: ArabJobs数据集为阿拉伯世界的劳动力市场和自然语言处理研究提供了宝贵资源，尤其对于关注公平性、性别和职业分析的研究领域有重要意义。数据集现已向公众开放，促进后续相关研究。

Abstract: ArabJobs is a publicly available corpus of Arabic job advertisements
collected from Egypt, Jordan, Saudi Arabia, and the United Arab Emirates.
Comprising over 8,500 postings and more than 550,000 words, the dataset
captures linguistic, regional, and socio-economic variation in the Arab labour
market. We present analyses of gender representation and occupational
structure, and highlight dialectal variation across ads, which offers
opportunities for future research. We also demonstrate applications such as
salary estimation and job category normalisation using large language models,
alongside benchmark tasks for gender bias detection and profession
classification. The findings show the utility of ArabJobs for fairness-aware
Arabic NLP and labour market research. The dataset is publicly available on
GitHub: https://github.com/drelhaj/ArabJobs.

</details>


### [260] [From Formal Language Theory to Statistical Learning: Finite Observability of Subregular Languages](https://arxiv.org/abs/2509.22598)
*Katsuhiko Hayashi,Hidetaka Kamigaito*

Main category: cs.CL

TL;DR: 论文证明了所有标准的次正则语言类在用判定谓词表示时都可以线性分离，从而为自然语言结构建模提供了理论基础。实验结果验证了理论的有效性。


<details>
  <summary>Details</summary>
Motivation: 自然语言的结构复杂，如何用可解释的有限模型对其进行建模和学习是长期关注的问题。次正则层次结构被认为对描述自然语言约束具有良好表现，但其可分性和可学习性尚未被充分证明。

Method: 作者从理论上证明了标准次正则语言类在决定谓词表示下具有线性可分性，并通过合成数据和英语形态学的实际数据进行了实验验证。

Result: 理论上所有标准次正则语言类均可线性分离。合成实验在无噪声条件下验证了完美分离，实际数据中提取的特征亦符合语言学已知约束。

Conclusion: 次正则层次为建模自然语言结构提供了可解释且可观测的理论基础，并可被简单线性模型有效学习和解释。

Abstract: We prove that all standard subregular language classes are linearly separable
when represented by their deciding predicates. This establishes finite
observability and guarantees learnability with simple linear models. Synthetic
experiments confirm perfect separability under noise-free conditions, while
real-data experiments on English morphology show that learned features align
with well-known linguistic constraints. These results demonstrate that the
subregular hierarchy provides a rigorous and interpretable foundation for
modeling natural language structure. Our code used in real-data experiments is
available at https://github.com/UTokyo-HayashiLab/subregular.

</details>


### [261] [Capturing Opinion Shifts in Deliberative Discourse through Frequency-based Quantum deep learning methods](https://arxiv.org/abs/2509.22603)
*Rakesh Thakur,Harsh Chaturvedi,Ruqayya Shah,Janvi Chauhan,Ayush Sharma*

Main category: cs.CL

TL;DR: 本文比较了多种NLP技术在分析和建模群体讨论中的观点转变与结果预测的效果，提出了两种新模型并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 群体讨论能通过多元观点权衡影响决策结果，但如何高效、智能地分析和预测这种讨论中的观点变化仍缺乏有效的自动化工具。

Method: 作者自建了一个包含多样化人群观点的数据集，通过产品介绍及加入引人注目的事实模拟讨论情景。文中比较了基于频率的语篇调制与“量子讨论框架”两种NLP模型在理解和预测观点变化方面的性能，并与现有方法进行了对比分析。

Result: 两种新模型在解释和预测群体讨论观点变化上效果优于现有技术。特别是在实际数据集和模拟情景下展现出更强的洞察能力和识别效率。

Conclusion: 这些NLP模型可广泛应用于公共政策制定、辩论评估、决策支持和大规模社交媒体意见挖掘，推动智能决策和自动化社会调查。

Abstract: Deliberation plays a crucial role in shaping outcomes by weighing diverse
perspectives before reaching decisions. With recent advancements in Natural
Language Processing, it has become possible to computationally model
deliberation by analyzing opinion shifts and predicting potential outcomes
under varying scenarios. In this study, we present a comparative analysis of
multiple NLP techniques to evaluate how effectively models interpret
deliberative discourse and produce meaningful insights. Opinions from
individuals of varied backgrounds were collected to construct a self-sourced
dataset that reflects diverse viewpoints. Deliberation was simulated using
product presentations enriched with striking facts, which often prompted
measurable shifts in audience opinions. We have given comparative analysis
between two models namely Frequency-Based Discourse Modulation and
Quantum-Deliberation Framework which outperform the existing state of art
models. The findings highlight practical applications in public policy-making,
debate evaluation, decision-support frameworks, and large-scale social media
opinion mining.

</details>


### [262] [From tests to effect sizes: Quantifying uncertainty and statistical variability in multilingual and multitask NLP evaluation benchmarks](https://arxiv.org/abs/2509.22612)
*Jonne Sälevä,Duygu Ataman,Constantine Lignos*

Main category: cs.CL

TL;DR: 本文提出了一套基于重采样的方法，用于量化多语种/多任务NLP评测指标的不确定性和统计精度。


<details>
  <summary>Details</summary>
Motivation: NLP评测指标在多语种和多任务基准测试中常被用于评估模型，但其结果可能受模型本身和数据相关的多重变异性影响。未能同时考虑这些变异性，容易低估指标在不同实验复制间的变动幅度，需要更精准的不确定性量化方法。

Method: 作者提出了基于重采样的统计方法（如bootstrap），系统地从模型和数据层面评估和分解变异源，并用于计算如均值、中位数、模型间两两差异、排名等评分指标的采样分布。

Result: 通过在多语言问答、机器翻译和命名实体识别等NLP任务上的实验证明，所提方法能有效量化评测指标的不确定性，并揭示以往方法存在的变异低估问题。

Conclusion: 评测指标的不确定性需同时考虑模型和数据两个方面的变异。文中方法具有广泛适用性，可用于各种NLP基准评测中的评分统计分析，为排名和模型对比等提供可靠的精度评价。

Abstract: In this paper, we introduce a set of resampling-based methods for quantifying
uncertainty and statistical precision of evaluation metrics in multilingual
and/or multitask NLP benchmarks. We show how experimental variation in
performance scores arises from both model- and data-related sources, and that
accounting for both of them is necessary to avoid substantially underestimating
the overall variability over hypothetical replications. Using multilingual
question answering, machine translation, and named entity recognition as
example tasks, we also demonstrate how resampling methods are useful for
computing sampling distributions for various quantities used in leaderboards
such as the average/median, pairwise differences between models, and rankings.

</details>


### [263] [StateX: Enhancing RNN Recall via Post-training State Expansion](https://arxiv.org/abs/2509.22630)
*Xingyu Shen,Yingfa Chen,Zhen Leng Thai,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 论文提出了一种名为StateX的方法，可在不增加参数量或训练成本的前提下，通过后训练的流程扩展RNN的状态大小，从而提高对长文本的回忆与理解能力。


<details>
  <summary>Details</summary>
Motivation: Transformer尽管性能突出，但在处理长文本时计算和存储成本高昂。相比之下，RNN等模型因每个token复杂度低被关注，但由于状态容量有限，难以记住长距离信息。提升RNN的状态容量虽可改善该问题，但直接扩容会带来高昂的训练成本，亟需新的低成本扩展方法。

Method: 作者提出StateX训练流程，对已有的线性Attention和状态空间型RNN模型，在后训练阶段通过结构性修改来扩展状态大小，无需或几乎不增加参数。

Result: 在多达13亿参数的模型和相关实验中，StateX能有效增强RNN的回忆能力和in-context learning表现，且不会带来显著的训练成本或损害模型其它能力。

Conclusion: StateX是一种高效可行的后训练方法，可低开销地提升RNN在长上下文任务中的性能，实现了效率与能力的兼顾。

Abstract: While Transformer-based models have demonstrated remarkable language modeling
performance, their high complexities result in high costs when processing long
contexts. In contrast, recurrent neural networks (RNNs) such as linear
attention and state space models have gained popularity due to their constant
per-token complexities. However, these recurrent models struggle with tasks
that require accurate recall of contextual information from long contexts,
because all contextual information is compressed into a constant-size recurrent
state. Previous works have shown that recall ability is positively correlated
with the recurrent state size, yet directly training RNNs with larger recurrent
states results in high training costs. In this paper, we introduce StateX, a
training pipeline for efficiently expanding the states of pre-trained RNNs
through post-training. For two popular classes of RNNs, linear attention and
state space models, we design post-training architectural modifications to
scale up the state size with no or negligible increase in model parameters.
Experiments on models up to 1.3B parameters demonstrate that StateX efficiently
enhances the recall and in-context learning ability of RNNs without incurring
high post-training costs or compromising other capabilities.

</details>


### [264] [Variational Reasoning for Language Models](https://arxiv.org/abs/2509.22637)
*Xiangxin Zhou,Zichen Liu,Haonan Wang,Chao Du,Min Lin,Chongxuan Li,Liang Wang,Tianyu Pang*

Main category: cs.CL

TL;DR: 本文提出了一种将推理过程视为隐变量，并通过变分推理优化语言模型推理能力的新框架。该方法统一了变分推理与强化学习方法，提升了语言模型在推理任务中的表现和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的推理能力有限，且现有训练方法如强化学习和采样微调方式存在不稳定或偏置问题，缺乏系统的概率建模解释。作者希望提出一种更为系统且理论合理的方法提升语言模型的推理效果。

Method: 作者将推理轨迹视为隐变量，基于变分证据下界（ELBO）提出多轨迹目标和前向KL公式，用以优化和稳定推理分布。进一步，分析发现主流的采样微调与二值奖励的强化学习算法可理解为局部的前向KL优化，且本身带有对简单问题的偏置。

Result: 在Qwen 2.5及Qwen 3等模型家族上，作者的方法在多种推理任务中得到了实证验证，表现优越，并显示出训练目标更稳定。

Conclusion: 本文方法将变分推理与强化学习式训练目标统一，并提出理论上更合理且训练更稳定的新目标函数，有效提升了大模型的推理能力，为后续大模型推理训练研究提供了新思路。

Abstract: We introduce a variational reasoning framework for language models that
treats thinking traces as latent variables and optimizes them through
variational inference. Starting from the evidence lower bound (ELBO), we extend
it to a multi-trace objective for tighter bounds and propose a forward-KL
formulation that stabilizes the training of the variational posterior. We
further show that rejection sampling finetuning and binary-reward RL, including
GRPO, can be interpreted as local forward-KL objectives, where an implicit
weighting by model accuracy naturally arises from the derivation and reveals a
previously unnoticed bias toward easier questions. We empirically validate our
method on the Qwen 2.5 and Qwen 3 model families across a wide range of
reasoning tasks. Overall, our work provides a principled probabilistic
perspective that unifies variational inference with RL-style methods and yields
stable objectives for improving the reasoning ability of language models. Our
code is available at https://github.com/sail-sg/variational-reasoning.

</details>


### [265] [Language Models Can Learn from Verbal Feedback Without Scalar Rewards](https://arxiv.org/abs/2509.22638)
*Renjie Luo,Zichen Liu,Xiangyan Liu,Chao Du,Min Lin,Wenhu Chen,Wei Lu,Tianyu Pang*

Main category: cs.CL

TL;DR: 该论文提出了一种新的LLM训练范式，即直接将人类或AI的语言反馈作为条件信号进行训练，不再简单地将其压缩成标量奖励，从而保留了反馈的丰富性。作者提出了基于反馈条件的策略（FCP），用最大似然方法近似条件后验，创新性地将语言模型的反馈驱动学习重构为条件生成问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM训练用RLHF/RLAIF等方法，通常把丰富的语言反馈压缩为单一数值奖励，导致信息损失和反馈信号不平衡。作者动机在于充分利用反馈的细致信息，提高训练表达力和效果。

Method: 方法包括两步：1）反馈条件策略（FCP），从反馈-响应样本直接进行最大似然离线训练，将反馈作为训练条件，不需转化为奖励。2）在线自举流程，在正面反馈条件下让模型生成，并借助新获得的反馈进一步迭代，提高模型对不同反馈条件的适应性。

Result: 实验显示，FCP方法能够更好地捕捉和利用语言反馈内容，使LLM更直接地响应和学习，从而提升模型对复杂反馈信号的理解与生成能力。

Conclusion: 作者提出了以条件生成替代奖励优化的反馈学习新范式，更全面充分利用了文本反馈，为LLM训练带来更大灵活性和表达能力。代码已开源，便于复现和进一步研究。

Abstract: LLMs are often trained with RL from human or AI feedback, yet such methods
typically compress nuanced feedback into scalar rewards, discarding much of
their richness and inducing scale imbalance. We propose treating verbal
feedback as a conditioning signal. Inspired by language priors in text-to-image
generation, which enable novel outputs from unseen prompts, we introduce the
feedback-conditional policy (FCP). FCP learns directly from response-feedback
pairs, approximating the feedback-conditional posterior through maximum
likelihood training on offline data. We further develop an online bootstrapping
stage where the policy generates under positive conditions and receives fresh
feedback to refine itself. This reframes feedback-driven learning as
conditional generation rather than reward optimization, offering a more
expressive way for LLMs to directly learn from verbal feedback. Our code is
available at https://github.com/sail-sg/feedback-conditional-policy.

</details>


### [266] [Death of the Novel(ty): Beyond n-Gram Novelty as a Metric for Textual Creativity](https://arxiv.org/abs/2509.22641)
*Arkadiy Saakyan,Najoung Kim,Smaranda Muresan,Tuhin Chakrabarty*

Main category: cs.CL

TL;DR: 本论文研究了n-gram新颖性作为衡量语言模型文本创造力的指标的有效性，发现其存在较大局限，不足以全面捕捉创造力。


<details>
  <summary>Details</summary>
Motivation: 虽然n-gram新颖性常被用来评价文本生成的创新程度和创造力，但理论研究表明，创造力应兼顾新颖性与适切性。现有评价方式可能无法准确反映真正的创造性表达。

Method: 作者收集并分析了7542条来自26位专家作者的标注，针对人类及AI生成文本，评估新颖性、实用性和合理性。同时测试主流开源和封闭大模型在识别创造性与非实用表达方面的表现。

Result: n-gram新颖性与专家评定的创造力有一定相关性，但约91%新颖性高的表达并未被认作有创造力。且开源大模型的新颖性提升伴随实用性下降。封闭大模型仍不及人类写作的创造性表现。大模型在识别创造性表达上优于随机，但识别非实用表达存在不足。

Conclusion: 仅依靠n-gram新颖性难以全面评估文本创造力。当前大模型识别创造性和实用性表达的能力有限，应探索更综合的创造力评价指标。

Abstract: N-gram novelty is widely used to evaluate language models' ability to
generate text outside of their training data. More recently, it has also been
adopted as a metric for measuring textual creativity. However, theoretical work
on creativity suggests that this approach may be inadequate, as it does not
account for creativity's dual nature: novelty (how original the text is) and
appropriateness (how sensical and pragmatic it is). We investigate the
relationship between this notion of creativity and n-gram novelty through 7542
expert writer annotations (n=26) of novelty, pragmaticality, and sensicality
via close reading of human and AI-generated text. We find that while n-gram
novelty is positively associated with expert writer-judged creativity, ~91% of
top-quartile expressions by n-gram novelty are not judged as creative,
cautioning against relying on n-gram novelty alone. Furthermore, unlike
human-written text, higher n-gram novelty in open-source LLMs correlates with
lower pragmaticality. In an exploratory study with frontier close-source
models, we additionally confirm that they are less likely to produce creative
expressions than humans. Using our dataset, we test whether zero-shot,
few-shot, and finetuned models are able to identify creative expressions (a
positive aspect of writing) and non-pragmatic ones (a negative aspect).
Overall, frontier LLMs exhibit performance much higher than random but leave
room for improvement, especially struggling to identify non-pragmatic
expressions. We further find that LLM-as-a-Judge novelty scores from the
best-performing model were predictive of expert writer preferences.

</details>


### [267] [WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning](https://arxiv.org/abs/2509.22644)
*Zimu Lu,Houxing Ren,Yunqiao Yang,Ke Wang,Zhuofan Zong,Junting Pan,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: 本文提出WebGen-Agent，一种结合了视觉反馈的智能体系统，用以提升网站代码生成的质量和准确性。通过引入多层次视觉反馈、截图评价与GUI测试，显著提升了大语言模型（LLM）驱动下的网页生成表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的代码智能体在处理依赖视觉与交互反馈的网站生成任务时，往往只依赖于代码执行结果，难以反映网站的实际视觉效果和用户体验，导致代码质量验证不充分。

Method: 提出WebGen-Agent框架，将视觉语言模型（VLM）生成的网站截图描述和GUI测试建议转化为细粒度打分，结合回溯与最优选择机制引导代码生成。此外，引入Step-GRPO算法，将各步截图与GUI反馈分数作为奖励信号，用于训练和强化LLM的推理与生成能力。

Result: 在WebGen-Bench数据集上，WebGen-Agent使得Claude-3.5-Sonnet的准确率从26.4%提升到51.9%，外观得分从3.0提升到3.9；训练方法Step-GRPO也显著提升了Qwen2.5-Coder-7B-Instruct的相关指标。

Conclusion: WebGen-Agent结合多层视觉反馈和精细化奖励机制，显著提升了现有智能体在网站生成任务中的表现，达到了最新的性能水平，对推动LLM应用于复杂前端开发有重要意义。

Abstract: Agent systems powered by large language models (LLMs) have demonstrated
impressive performance on repository-level code-generation tasks. However, for
tasks such as website codebase generation, which depend heavily on visual
effects and user-interaction feedback, current code agents rely only on simple
code execution for feedback and verification. This approach fails to capture
the actual quality of the generated code. In this paper, we propose
WebGen-Agent, a novel website-generation agent that leverages comprehensive and
multi-level visual feedback to iteratively generate and refine the website
codebase. Detailed and expressive text descriptions and suggestions regarding
the screenshots and GUI-agent testing of the websites are generated by a visual
language model (VLM), together with scores that quantify their quality. The
screenshot and GUI-agent scores are further integrated with a backtracking and
select-best mechanism, enhancing the performance of the agent. Utilizing the
accurate visual scores inherent in the WebGen-Agent workflow, we further
introduce \textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improve
the ability of LLMs to act as the reasoning engine of WebGen-Agent. By using
the screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we
provide a dense and reliable process supervision signal, which effectively
improves the model's website-generation ability. On the WebGen-Bench dataset,
WebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9%
and its appearance score from 3.0 to 3.9, outperforming the previous
state-of-the-art agent system. Additionally, our Step-GRPO training approach
increases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and
raises the appearance score from 3.4 to 3.7.

</details>


### [268] [VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing](https://arxiv.org/abs/2509.22651)
*Ke Wang,Houxing Ren,Zimu Lu,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: 本文介绍了VoiceAssistant-Eval，一个涵盖听说视三大能力的AI语音助手评测基准，并测试了21个开源模型与GPT-4o-Audio，揭示了当前模型在语音理解等任务上的不足与未来改进方向。


<details>
  <summary>Details</summary>
Motivation: 现有的评测体系无法充分衡量语音优先的AI助手在听、说、看全方位的能力。随着大模型和多模态系统能力提升，亟需新基准系统来全面评价并推动AI助手发展。

Method: 设计并推出VoiceAssistant-Eval基准，包含10,497个人工筛选样本，覆盖13个任务类别，分别测试自然声音、音乐、对话等‘听’任务，多轮对话、仿声、场景模拟等‘说’任务，以及多样化的图像‘看’任务。对21个开源及1个商业模型进行内容、语音质量与一致性打分。

Result: （1）闭源模型不一定优于开源模型；（2）多数模型在说任务表现好，但听觉理解能力较弱；（3）中小规模、结构设计良好的模型在部分任务可追平或超越大模型。例如，Step-Audio-2-mini（7B）在听力准确率远超LLaMA-Omni2-32B-Bilingual。

Conclusion: VoiceAssistant-Eval明确指出了多模态AI助手在音视输入及角色扮演仿声等难题，强调当前在鲁棒性与安全性对齐仍有巨大提升空间，为后续技术演进和模型开发提供了可信并具挑战的评测框架。

Abstract: The growing capabilities of large language models and multimodal systems have
spurred interest in voice-first AI assistants, yet existing benchmarks are
inadequate for evaluating the full range of these systems' capabilities. We
introduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI
assistants across listening, speaking, and viewing. VoiceAssistant-Eval
comprises 10,497 curated examples spanning 13 task categories. These tasks
include natural sounds, music, and spoken dialogue for listening; multi-turn
dialogue, role-play imitation, and various scenarios for speaking; and highly
heterogeneous images for viewing. To demonstrate its utility, we evaluate 21
open-source models and GPT-4o-Audio, measuring the quality of the response
content and speech, as well as their consistency. The results reveal three key
findings: (1) proprietary models do not universally outperform open-source
models; (2) most models excel at speaking tasks but lag in audio understanding;
and (3) well-designed smaller models can rival much larger ones. Notably, the
mid-sized Step-Audio-2-mini (7B) achieves more than double the listening
accuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal
(audio plus visual) input and role-play voice imitation tasks are difficult for
current models, and significant gaps persist in robustness and safety
alignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorous
framework for evaluating and guiding the development of next-generation AI
assistants. Code and data will be released at
https://mathllm.github.io/VoiceAssistantEval/ .

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [269] [Language-in-the-Loop Culvert Inspection on the Erie Canal](https://arxiv.org/abs/2509.21370)
*Yashom Dighe,Yash Turkar,Karthik Dantu*

Main category: cs.RO

TL;DR: 本论文提出了一种名为VISION的端到端语言-视觉自主系统，用于高效检测运河涵洞，在实地测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 涵洞检测任务因涵洞老旧、结构复杂、照明不足、天气和进入困难等因素，传统人工检测难度大、效率低，有较强的自动化需求。

Method: 提出了VISION系统，将大规模视觉-语言模型与受限视角规划相结合，实现自动检测。通过语言提示动态生成关注区域（ROI）建议，并结合立体深度信息恢复尺度，基于涵洞物理约束智能规划检测动作，整个系统可在机器人四足平台上自主运行，无需特定领域微调。

Result: VISION在伊利运河下的涵洞实际部署，初步检测提案与专家意见一致率达61.4%，经智能补拍优化后，最终与专家一致率达80%。

Conclusion: VISION系统能有效代替人工作业，自动检测涵洞并输出高质量细节报告，将初步假设转化为可靠的、与专家一致的检测结论。

Abstract: Culverts on canals such as the Erie Canal, built originally in 1825, require
frequent inspections to ensure safe operation. Human inspection of culverts is
challenging due to age, geometry, poor illumination, weather, and lack of easy
access. We introduce VISION, an end-to-end, language-in-the-loop autonomy
system that couples a web-scale vision-language model (VLM) with constrained
viewpoint planning for autonomous inspection of culverts. Brief prompts to the
VLM solicit open-vocabulary ROI proposals with rationales and confidences,
stereo depth is fused to recover scale, and a planner -- aware of culvert
constraints -- commands repositioning moves to capture targeted close-ups.
Deployed on a quadruped in a culvert under the Erie Canal, VISION closes the
see, decide, move, re-image loop on-board and produces high-resolution images
for detailed reporting without domain-specific fine-tuning. In an external
evaluation by New York Canal Corporation personnel, initial ROI proposals
achieved 61.4\% agreement with subject-matter experts, and final
post-re-imaging assessments reached 80\%, indicating that VISION converts
tentative hypotheses into grounded, expert-aligned findings.

</details>


### [270] [Developing a Mono-Actuated Compliant GeoGami Robot](https://arxiv.org/abs/2509.21445)
*Archie Webster,Lee Skull,Seyed Amir Tafrishi*

Main category: cs.RO

TL;DR: 该论文提出了一种新型软刚结合机器人平台GeoGami，利用折纸结构实现仅用单一驱动器的变形与运动。


<details>
  <summary>Details</summary>
Motivation: 折纸结构具有高度自由度，但通常需要多个执行器，造成系统复杂。如何结合表面顺应性与结构几何，减少执行器数量，提升可重复性，是该研究的出发点。

Method: 论文设计了包含柔顺折纸表面与符合几何结构骨架的软刚结合平台，通过集成的中央齿轮箱和单一执行器实现机器人的变形和移动，同时建立了刚度模型并分析了不同的缆绳驱动方案。

Result: GeoGami平台能够通过形状变化实现运动（包括滚动），实验证明该机器人具备良好的变形和运动能力，并可依赖单一执行器完成相应功能。

Conclusion: GeoGami平台为能够变形以适应不同环境，并通过形状变换实现运动的机器人开辟了新可能，简化了结构，增强了适用性和性能。

Abstract: This paper presents the design of a new soft-rigid robotic platform,
"GeoGami". We leverage origami surface capabilities to achieve shape
contraction and to support locomotion with underactuated forms. A key challenge
is that origami surfaces have high degrees of freedom and typically require
many actuators; we address repeatability by integrating surface compliance. We
propose a mono-actuated GeoGami mobile platform that combines origami surface
compliance with a geometric compliant skeleton, enabling the robot to transform
and locomote using a single actuator. We demonstrate the robot, develop a
stiffness model, and describe the central gearbox mechanism. We also analyze
alternative cable-driven actuation methods for the skeleton to enable surface
transformation. Finally, we evaluate the GeoGami platform for capabilities,
including shape transformation and rolling. This platform opens new
capabilities for robots that change shape to access different environments and
that use shape transformation for locomotion.

</details>


### [271] [Wall Inspector: Quadrotor Control in Wall-proximity Through Model Compensation](https://arxiv.org/abs/2509.21496)
*Peiwen Yang,Weisong Wen,Runqiu Yang,Yingming Chen,Cheuk Chi Tsang*

Main category: cs.RO

TL;DR: 本文提出了一种使四旋翼无人机在靠近墙面等复杂环境下安全运行的全新方法，包括物理建模与控制方案，并通过实验验证了其显著优越性。


<details>
  <summary>Details</summary>
Motivation: 近墙飞行时，四旋翼容易因未建模的吸力（复杂气动效应）出现控制不稳定甚至碰撞，现有控制方法处理这种工况效果有限，亟需物理建模和控制融合的改进。

Method: 1）提出了基于物理的吸力（suction force）模型，明确描述转子转速和壁面距离对吸力的影响；2）设计了吸力补偿的模型预测控制（SC-MPC）框架，将吸力模型纳入系统动力学，并通过因子图优化问题综合轨迹跟踪、控制平滑和执行器约束。吸力模型参数通过大量实验测定。

Result: 在实际带风罩的四旋翼上验证，SC-MPC方案在X/Y轴的均方根轨迹误差分别为2.1cm/2.0cm，相对于PID提升74%/79%，相对于标准MPC提升60%/53%；平均绝对误差也大幅优于两种基线方法。

Conclusion: SC-MPC在近墙飞行任务中显著提升了无人机跟踪精度与稳定性，优于传统PID和MPC控制，且硬件方案具备良好安全性和气动效率；源码已开源，便于社区复用。

Abstract: The safe operation of quadrotors in near-wall urban or indoor environments
(e.g., inspection and search-and-rescue missions) is challenged by unmodeled
aerodynamic effects arising from wall-proximity. It generates complex vortices
that induce destabilizing suction forces, potentially leading to hazardous
vibrations or collisions. This paper presents a comprehensive solution
featuring (1) a physics-based suction force model that explicitly characterizes
the dependency on both rotor speed and wall distance, and (2) a
suction-compensated model predictive control (SC-MPC) framework designed to
ensure accurate and stable trajectory tracking during wall-proximity
operations. The proposed SC-MPC framework incorporates an enhanced dynamics
model that accounts for suction force effects, formulated as a factor graph
optimization problem integrating system dynamics constraints, trajectory
tracking objectives, control input smoothness requirements, and actuator
physical limitations. The suction force model parameters are systematically
identified through extensive experimental measurements across varying
operational conditions. Experimental validation demonstrates SC-MPC's superior
performance, achieving 2.1 cm root mean squared error (RMSE) in X-axis and 2.0
cm RMSE in Y-axis position control - representing 74% and 79% improvements over
cascaded proportional-integral-derivative (PID) control, and 60% and 53%
improvements over standard MPC respectively. The corresponding mean absolute
error (MAE) metrics (1.2 cm X-axis, 1.4 cm Y-axis) similarly outperform both
baselines. The evaluation platform employs a ducted quadrotor design that
provides collision protection while maintaining aerodynamic efficiency. To
facilitate reproducibility and community adoption, we have open-sourced our
complete implementation, available at
https://anonymous.4open.science/r/SC-MPC-6A61.

</details>


### [272] [DroneFL: Federated Learning for Multi-UAV Visual Target Tracking](https://arxiv.org/abs/2509.21523)
*Xiaofan Yu,Yuwei Wu,Katherine Mao,Ye Tian,Vijay Kumar,Tajana Rosing*

Main category: cs.RO

TL;DR: 本文提出了一种名为DroneFL的联邦学习框架，专为多无人机（UAV）目标跟踪任务设计，显著提升了预测准确性和跟踪性能，并具备高效的数据同步和实时处理能力。


<details>
  <summary>Details</summary>
Motivation: 在多机器人/无人机目标跟踪领域，现有解决方案面临计算资源有限、数据异质性大以及需要将轨迹预测与多机器人规划紧密结合等挑战，而联邦学习可在不集中数据的前提下促进多机器人学习，但在多UAV目标跟踪中应用尚不充分。

Method: 设计了DroneFL框架，包括：1）基于冻结YOLO骨干与浅层transformer的轻量级本地模型进行目标轨迹预测；2）使用位置不变架构与基于高度自适应实例归一化，缓解多无人机数据异质性对FL收敛的影响；3）云端周期性聚合模型，多无人机预测结果融合，生成最优跟踪轨迹，实现全球知识共享和任务优化。

Result: DroneFL相较于无联邦学习的分布式框架，目标预测误差降低6%-83%，跟踪距离减少0.4%-4.6%，在Raspberry Pi 5等低端硬件上实现实时运行，云端同步数据平均仅1.56 KBps。

Conclusion: DroneFL实现了针对多无人机目标跟踪的高效、低带宽、低误差的联邦学习解决方案，推动了相关领域的实际应用发展。

Abstract: Multi-robot target tracking is a fundamental problem that requires
coordinated monitoring of dynamic entities in applications such as precision
agriculture, environmental monitoring, disaster response, and security
surveillance. While Federated Learning (FL) has the potential to enhance
learning across multiple robots without centralized data aggregation, its use
in multi-Unmanned Aerial Vehicle (UAV) target tracking remains largely
underexplored. Key challenges include limited onboard computational resources,
significant data heterogeneity in FL due to varying targets and the fields of
view, and the need for tight coupling between trajectory prediction and
multi-robot planning. In this paper, we introduce DroneFL, the first federated
learning framework specifically designed for efficient multi-UAV target
tracking. We design a lightweight local model to predict target trajectories
from sensor inputs, using a frozen YOLO backbone and a shallow transformer for
efficient onboard training. The updated models are periodically aggregated in
the cloud for global knowledge sharing. To alleviate the data heterogeneity
that hinders FL convergence, DroneFL introduces a position-invariant model
architecture with altitude-based adaptive instance normalization. Finally, we
fuse predictions from multiple UAVs in the cloud and generate optimal
trajectories that balance target prediction accuracy and overall tracking
performance. Our results show that DroneFL reduces prediction error by 6%-83%
and tracking distance by 0.4%-4.6% compared to a distributed non-FL framework.
In terms of efficiency, DroneFL runs in real time on a Raspberry Pi 5 and has
on average just 1.56 KBps data rate to the cloud.

</details>


### [273] [Plan2Evolve: LLM Self-Evolution for Improved Planning Capability via Automated Domain Generation](https://arxiv.org/abs/2509.21543)
*Jinbang Huang,Zhiyuan Li,Zhanguang Zhang,Xingyue Quan,Jianye Hao,Yingxue Zhang*

Main category: cs.RO

TL;DR: 该论文提出了Plan2Evolve框架，实现大语言模型（LLM）自我进化，通过自动生成规划域与自然语言推理融合，提升LLM的机器人任务规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在机器人任务规划中，主要将自动生成的规划域作为符号搜索工具，未充分利用其作为推理数据源的潜力。同时，提升LLM推理能力的CoT监督依赖昂贵的人类标注数据，阻碍在机器人场景的普及应用。

Method: 提出Plan2Evolve框架：LLM自动生成规划域，再基于该域生成符号问题-解对（推理轨迹）；随后，LLM将这些解对转化为带自然语言解释的扩展版CoT轨迹，实现符号规划结构与自然语言推理对齐，用于后续微调，提升模型能力。

Result: 框架生成了超越原有规划能力的推理数据，经微调后得到了具有更高规划成功率、更强跨任务泛化能力及更低推理成本的LLM。

Conclusion: Plan2Evolve能够扩展LLM的推理能力，在降低人工依赖的同时显著增强模型对机器人任务规划的适用性和效率，推动了机器人推理自动化的进程。

Abstract: Large Language Models (LLMs) have recently shown strong potential in robotic
task planning, particularly through automatic planning domain generation that
integrates symbolic search. Prior approaches, however, have largely treated
these domains as search utilities, with limited attention to their potential as
scalable sources of reasoning data. At the same time, progress in reasoning
LLMs has been driven by chain-of-thought (CoT) supervision, whose application
in robotics remains dependent on costly, human-curated datasets. We propose
Plan2Evolve, an LLM self-evolving framework in which the base model generates
planning domains that serve as engines for producing symbolic problem-plan
pairs as reasoning traces. These pairs are then transformed into extended CoT
trajectories by the same model through natural-language explanations, thereby
explicitly aligning symbolic planning structures with natural language
reasoning. The resulting data extend beyond the model's intrinsic planning
capacity, enabling model fine-tuning that yields a planning-enhanced LLM with
improved planning success, stronger cross-task generalization, and reduced
inference costs.

</details>


### [274] [PL-VIWO2: A Lightweight, Fast and Robust Visual-Inertial-Wheel Odometry Using Points and Lines](https://arxiv.org/abs/2509.21563)
*Zhixin Zhang,Liang Zhao,Pawel Ladosz*

Main category: cs.RO

TL;DR: PL-VIWO2是一种新型的视觉-惯性-轮速计里程计系统，结合IMU、轮编码器和相机，能在复杂户外环境中实现高精度和高鲁棒性的车辆状态估计。该系统提出了高效线特征处理、平面运动优化的轮子预积分以及动态特征去除，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉里程计虽成本低、结构轻便，但在城市复杂环境（如光照变化、动态障碍物）下性能会显著下降。因此，研究如何提升其在自动驾驶等实际场景下的精度和鲁棒性。

Method: 设计了融合IMU、轮速计和单/双目相机的滤波型系统，并提出三大创新：基于几何关联的快速鲁棒线特征处理框架、利用车辆平面特性提升轮子预积分精度的方法、以及结合IMU与轮速计一致性校验剔除动态特征影响。通过大量仿真及公开数据集测试方法有效性。

Result: 在蒙特卡洛仿真和多个自动驾驶公开数据集上，PL-VIWO2在定位精度、计算效率及鲁棒性指标上均优于当前主流里程计系统。

Conclusion: PL-VIWO2为自动驾驶车辆在复杂城市环境下的长期状态估计提供了高效、精确且鲁棒的新方案，有力推动了多传感器融合里程计技术的发展。

Abstract: Vision-based odometry has been widely adopted in autonomous driving owing to
its low cost and lightweight setup; however, its performance often degrades in
complex outdoor urban environments. To address these challenges, we propose
PL-VIWO2, a filter-based visual-inertial-wheel odometry system that integrates
an IMU, wheel encoder, and camera (supporting both monocular and stereo) for
long-term robust state estimation. The main contributions are: (i) a novel line
feature processing framework that exploits the geometric relationship between
2D feature points and lines, enabling fast and robust line tracking and
triangulation while ensuring real-time performance; (ii) an SE(2)-constrained
SE(3) wheel pre-integration method that leverages the planar motion
characteristics of ground vehicles for accurate wheel updates; and (iii) an
efficient motion consistency check (MCC) that filters out dynamic features by
jointly using IMU and wheel measurements. Extensive experiments on Monte Carlo
simulations and public autonomous driving datasets demonstrate that PL-VIWO2
outperforms state-of-the-art methods in terms of accuracy, efficiency, and
robustness.

</details>


### [275] [Autonomous UAV-Quadruped Docking in Complex Terrains via Active Posture Alignment and Constraint-Aware Control](https://arxiv.org/abs/2509.21571)
*HaoZhe Xu,Cheng Cheng,HongRui Sang,Zhipeng Wang,Qiyong He,Xiuxian Li,Bin He*

Main category: cs.RO

TL;DR: 本论文提出了一种可在无GPS环境下，实现无人机（UAV）与四足机器人自主对接的新框架，解决了复杂地形下传统对接方法难以应用的问题。


<details>
  <summary>Details</summary>
Motivation: 现有大多数UAV与地面机器人对接方法针对轮式平台，但这类平台在复杂地形下表现不佳。四足机器人虽适应性强，但频繁的姿态变化给无人机着陆带来很大挑战。因此需要一种新的方法，使UAV能在四足机器人动态且不平稳的表面上安全自主对接。

Method: 提出了自主UAV-四足机器人对接框架。四足机器人端，利用深度强化学习得到的带水平对齐的混合内部模型（HIM-HA），主动稳定躯干。无人机端采用三阶段策略：1）使用中值滤波的YOLOv8进行远距离目标获取；2）近距离跟踪时，使用集成NFTSMC与对视场角约束的对数屏障函数的控制器确保有限时间误差收敛；3）最终下落阶段，通过安全周期机制联合验证跟踪精度与平台稳定性。

Result: 在仿真和现实场景中进行验证，系统成功完成了无人机在高于17 cm的室外台阶及坡度大于30度的崎岖斜坡上的四足机器人对接。

Conclusion: 所提方法有效提升了UAV与四足机器人在复杂地形下的对接能力，为异构机器人系统协作提供了新手段。

Abstract: Autonomous docking between Unmanned Aerial Vehicles (UAVs) and ground robots
is essential for heterogeneous systems, yet most existing approaches target
wheeled platforms whose limited mobility constrains exploration in complex
terrains. Quadruped robots offer superior adaptability but undergo frequent
posture variations, making it difficult to provide a stable landing surface for
UAVs. To address these challenges, we propose an autonomous UAV-quadruped
docking framework for GPS-denied environments. On the quadruped side, a Hybrid
Internal Model with Horizontal Alignment (HIM-HA), learned via deep
reinforcement learning, actively stabilizes the torso to provide a level
platform. On the UAV side, a three-phase strategy is adopted, consisting of
long-range acquisition with a median-filtered YOLOv8 detector, close-range
tracking with a constraint-aware controller that integrates a Nonsingular Fast
Terminal Sliding Mode Controller (NFTSMC) and a logarithmic Barrier Function
(BF) to guarantee finite-time error convergence under field-of-view (FOV)
constraints, and terminal descent guided by a Safety Period (SP) mechanism that
jointly verifies tracking accuracy and platform stability. The proposed
framework is validated in both simulation and real-world scenarios,
successfully achieving docking on outdoor staircases higher than 17 cm and
rough slopes steeper than 30 degrees. Supplementary materials and videos are
available at: https://uav-quadruped-docking.github.io.

</details>


### [276] [Real-Time Indoor Object SLAM with LLM-Enhanced Priors](https://arxiv.org/abs/2509.21602)
*Yang Jiao,Yiding Qiu,Henrik I. Christensen*

Main category: cs.RO

TL;DR: 本文提出一种利用大语言模型（LLM）生成常识性几何先验，以提升基于目标的SLAM系统在稀疏观测下的鲁棒性与精度，实现实时且更准确的三维场景理解。


<details>
  <summary>Details</summary>
Motivation: 传统目标级SLAM系统由于观测稀疏，优化问题通常欠约束，常用的常识先验获取方式成本高且不具备普适性，亟需一种通用、自动化、强适应性的先验获取新方法。

Method: 论文通过大语言模型自动获取目标物体的几何属性（如大小、方向）作为先验信息，集成到图优化SLAM框架中，实现在初期稀疏观测下也能有效的数据关联和地图构建，并完成端到端流程设计。

Result: 在TUM RGB-D和3RScan等公开数据集上，所提出系统将目标级SLAM的建图精度提升了36.8%，并在真实环境下进行了实时实验，效果优于现有方法。

Conclusion: 依托LLM自动生成的几何先验，可以极大提升目标级SLAM系统在稀疏观测场景下的定位与建图能力，方案具备良好的实时性和泛化潜力。

Abstract: Object-level Simultaneous Localization and Mapping (SLAM), which incorporates
semantic information for high-level scene understanding, faces challenges of
under-constrained optimization due to sparse observations. Prior work has
introduced additional constraints using commonsense knowledge, but obtaining
such priors has traditionally been labor-intensive and lacks generalizability
across diverse object categories. We address this limitation by leveraging
large language models (LLMs) to provide commonsense knowledge of object
geometric attributes, specifically size and orientation, as prior factors in a
graph-based SLAM framework. These priors are particularly beneficial during the
initial phase when object observations are limited. We implement a complete
pipeline integrating these priors, achieving robust data association on sparse
object-level features and enabling real-time object SLAM. Our system, evaluated
on the TUM RGB-D and 3RScan datasets, improves mapping accuracy by 36.8\% over
the latest baseline. Additionally, we present real-world experiments in the
supplementary video, demonstrating its real-time performance.

</details>


### [277] [Generating Stable Placements via Physics-guided Diffusion Models](https://arxiv.org/abs/2509.21664)
*Philippe Nadeau,Miguel Rogel,Ivan Bilić,Ivan Petrović,Jonathan Kelly*

Main category: cs.RO

TL;DR: 本文提出了一种在多物体场景下，实现高效、稳定物体放置的新方法，通过结合扩散模型与物理稳定性，实现高效且鲁棒的放置，无需额外训练步骤。


<details>
  <summary>Details</summary>
Motivation: 传统的物体放置评估方法依赖仿真引擎或基于外观的启发式方法，存在效率低和不稳定的问题。因此，需要一种能够直接在采样阶段融入稳定性，并提升放置鲁棒性的方法。

Method: 作者通过离线采样规划器获取多样化的放置标签，训练一个条件于场景和目标物点云的扩散模型，作为几何先验。同时，利用得分生成模型的可组合性，将该先验与稳定性损失结合，在不需再训练的条件下增强稳定放置的概率。

Result: 在四个基准测试场景上，所提物理引导模型放置结果对外力扰动的鲁棒性提高了56%，运行时间相比主流几何方法减少了47%。

Conclusion: 该方法能有效提升复杂多物体场景中物体放置的稳定性与效率，且无需额外训练，可直接在已有模型基础上应用，具有很好的实际应用前景。

Abstract: Stably placing an object in a multi-object scene is a fundamental challenge
in robotic manipulation, as placements must be penetration-free, establish
precise surface contact, and result in a force equilibrium. To assess
stability, existing methods rely on running a simulation engine or resort to
heuristic, appearance-based assessments. In contrast, our approach integrates
stability directly into the sampling process of a diffusion model. To this end,
we query an offline sampling-based planner to gather multi-modal placement
labels and train a diffusion model to generate stable placements. The diffusion
model is conditioned on scene and object point clouds, and serves as a
geometry-aware prior. We leverage the compositional nature of score-based
generative models to combine this learned prior with a stability-aware loss,
thereby increasing the likelihood of sampling from regions of high stability.
Importantly, this strategy requires no additional re-training or fine-tuning,
and can be directly applied to off-the-shelf models. We evaluate our method on
four benchmark scenes where stability can be accurately computed. Our
physics-guided models achieve placements that are 56% more robust to forceful
perturbations while reducing runtime by 47% compared to a state-of-the-art
geometric method.

</details>


### [278] [Towards Versatile Humanoid Table Tennis: Unified Reinforcement Learning with Prediction Augmentation](https://arxiv.org/abs/2509.21690)
*Muqun Hu,Wenxi Chen,Wenjing Li,Falak Mandali,Zijian He,Renhong Zhang,Praveen Krisna,Katherine Christian,Leo Benaharon,Dizhi Ma,Karthik Ramani,Yan Gu*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习的人形乒乓球控制方法，直接将球的位置观测映射为全身关节动作，融合了预测信号和物理引导的密集奖励，实现了高命中率和准确快速回球。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人乒乓球控制器很难同时实现快速感知、全身运动、敏捷步伐等紧密配合的能力，亟需统一的端到端解决方案提升对复杂动态场景的适应能力和泛化能力。

Method: 该方法提出一种强化学习框架，从球的位置观测直接输出协调的全身关节控制指令（包括手臂击球和腿部移动）。在训练过程中，作者引入了轻量级预测器（用近期球位预测未来状态），将预测结果作为观测输入提升政策的前瞻性；利用物理预测器提供精确的未来状态，实现密集且有针对性的奖励信号，引导有效探索和学习。

Result: 在不同发球范围下，所训练出的策略在仿真中展现出极强表现（击球命中率≥96%，成功回球率≥92%）。消融实验表明，预测器与密集预测奖励机制对端到端策略学习至关重要。

Conclusion: 所提出方法不仅在仿真中取得优异成绩，且可零样本迁移至23自由度的人形机器人平台，展现出高效协调的侧向移动和前后步伐与迅速准确的回球能力，为实现通用、强大的竞赛级人形乒乓球机器人提供了新的可行路径。

Abstract: Humanoid table tennis (TT) demands rapid perception, proactive whole-body
motion, and agile footwork under strict timing -- capabilities that remain
difficult for unified controllers. We propose a reinforcement learning
framework that maps ball-position observations directly to whole-body joint
commands for both arm striking and leg locomotion, strengthened by predictive
signals and dense, physics-guided rewards. A lightweight learned predictor, fed
with recent ball positions, estimates future ball states and augments the
policy's observations for proactive decision-making. During training, a
physics-based predictor supplies precise future states to construct dense,
informative rewards that lead to effective exploration. The resulting policy
attains strong performance across varied serve ranges (hit rate $\geq$ 96% and
success rate $\geq$ 92%) in simulations. Ablation studies confirm that both the
learned predictor and the predictive reward design are critical for end-to-end
learning. Deployed zero-shot on a physical Booster T1 humanoid with 23 revolute
joints, the policy produces coordinated lateral and forward-backward footwork
with accurate, fast returns, suggesting a practical path toward versatile,
competitive humanoid TT.

</details>


### [279] [VLBiMan: Vision-Language Anchored One-Shot Demonstration Enables Generalizable Robotic Bimanual Manipulation](https://arxiv.org/abs/2509.21723)
*Huayi Zhou,Kui Jia*

Main category: cs.RO

TL;DR: VLBiMan框架实现了只需一次人类演示即可泛化双臂操作技能，能灵活适应多变环境和不同机器人平台，效率与泛化性远超传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前双臂操作学习方法要么需要大量演示（模仿学习），要么在动态场景下缺乏灵活性（模块化方法），不适合现实中的少样本、复杂与泛化需求。

Method: 提出VLBiMan框架，通过任务感知分解将一次人类演示拆分为可复用的原子技能，利用视觉-语言定位对可调部分适应变化环境，同时保持不变组件以增强稳定性。适配机制结合语义解析和几何约束，无需策略重训练即可应对环境变化、物体变换和场景干扰。

Result: 实验证明VLBiMan大幅降低了演示需求，可以通过原子技能拼接实现长时序任务的组合泛化，对新物体和外部干扰有鲁棒性，并能在不同机器人体上零训练迁移。

Conclusion: 该方法有效融合人类先验与视觉-语言适应机制，为实际复杂场景下通用且灵活的双臂操作提供了新路径。

Abstract: Achieving generalizable bimanual manipulation requires systems that can learn
efficiently from minimal human input while adapting to real-world uncertainties
and diverse embodiments. Existing approaches face a dilemma: imitation policy
learning demands extensive demonstrations to cover task variations, while
modular methods often lack flexibility in dynamic scenes. We introduce VLBiMan,
a framework that derives reusable skills from a single human example through
task-aware decomposition, preserving invariant primitives as anchors while
dynamically adapting adjustable components via vision-language grounding. This
adaptation mechanism resolves scene ambiguities caused by background changes,
object repositioning, or visual clutter without policy retraining, leveraging
semantic parsing and geometric feasibility constraints. Moreover, the system
inherits human-like hybrid control capabilities, enabling mixed synchronous and
asynchronous use of both arms. Extensive experiments validate VLBiMan across
tool-use and multi-object tasks, demonstrating: (1) a drastic reduction in
demonstration requirements compared to imitation baselines, (2) compositional
generalization through atomic skill splicing for long-horizon tasks, (3)
robustness to novel but semantically similar objects and external disturbances,
and (4) strong cross-embodiment transfer, showing that skills learned from
human demonstrations can be instantiated on different robotic platforms without
retraining. By bridging human priors with vision-language anchored adaptation,
our work takes a step toward practical and versatile dual-arm manipulation in
unstructured settings.

</details>


### [280] [The Turkish Ice Cream Robot: Examining Playful Deception in Social Human-Robot Interactions](https://arxiv.org/abs/2509.21776)
*Hyeonseong Kim,Roy El-Helou,Seungbeen Lee,Sungjoon Choi,Matthew Pan*

Main category: cs.RO

TL;DR: 本研究探讨了在人机交互中引入类似土耳其冰淇淋贩卖者恶作剧的“有界愉快欺骗”策略，发现它可以提升用户的娱乐性和参与感，但会降低安全感和信任度，强调了设计时需平衡多维度权衡。


<details>
  <summary>Details</summary>
Motivation: 尽管“愉快欺骗”在日常人际互动中常见，但在人机交互领域鲜有研究。受土耳其冰淇淋贩卖者恶作剧启发，作者希望探究这种文化熟悉、有限度的欺骗方式对用户体验的影响，尤其是在机器人递交物品场景下。

Method: 设计了一台具备特殊末端执行器的机械臂，实现了五种土耳其冰淇淋贩卖者风格的欺骗策略，通过恶作剧手法延迟递交冰淇淋仿制品。招募91名参与者，采用混合设计，评估愉快欺骗及交互时长对信任、娱乐和参与度的影响。

Result: 结果显示：引入土耳其冰淇淋恶作剧式的愉快欺骗明显提升了用户的娱乐感和参与度，但同时降低了他们对机器人的安全感和信任，展现了娱乐性与信任性之间的权衡。

Conclusion: 愉快欺骗作为设计交互机器人时提升娱乐性和参与度的有效策略，适用于娱乐和互动为核心的应用场景，但其负面影响（如信任下降）不可忽略，设计时需谨慎权衡。

Abstract: Playful deception, a common feature in human social interactions, remains
underexplored in Human-Robot Interaction (HRI). Inspired by the Turkish Ice
Cream (TIC) vendor routine, we investigate how bounded, culturally familiar
forms of deception influence user trust, enjoyment, and engagement during
robotic handovers. We design a robotic manipulator equipped with a custom
end-effector and implement five TIC-inspired trick policies that deceptively
delay the handover of an ice cream-shaped object. Through a mixed-design user
study with 91 participants, we evaluate the effects of playful deception and
interaction duration on user experience. Results reveal that TIC-inspired
deception significantly enhances enjoyment and engagement, though reduces
perceived safety and trust, suggesting a structured trade-off across the
multi-dimensional aspects. Our findings demonstrate that playful deception can
be a valuable design strategy for interactive robots in entertainment and
engagement-focused contexts, while underscoring the importance of deliberate
consideration of its complex trade-offs. You can find more information,
including demonstration videos, on
https://hyeonseong-kim98.github.io/turkish-ice-cream-robot/ .

</details>


### [281] [Learning Multi-Skill Legged Locomotion Using Conditional Adversarial Motion Priors](https://arxiv.org/abs/2509.21810)
*Ning Huang,Zhentao Xie,Qinchuan Li*

Main category: cs.RO

TL;DR: 提出了一种基于条件对抗运动先验（CAMP）的多技能学习框架，使四足机器人能通过专家演示高效获得多样运动技能，并实现平滑技能切换和主动控制。


<details>
  <summary>Details</summary>
Motivation: 现有机器人运动学习方法难以通过单一策略获得多重技能，且技能切换不顺畅，限制了机器人在复杂环境中的适应性。

Method: 提出CAMP框架，通过技能判别器和技能条件奖励设计，从专家数据中学习多种运动技能，使多个技能可以共享一个策略高效控制。

Result: 实现了多样运动技能的高精度重建与主动控制，支持技能重用，且在复杂环境中表现出良好的泛化能力和技能切换平滑。

Conclusion: CAMP为四足机器人多技能学习和泛化提供了有效途径，有望提升其在现实复杂场景中的实用性。

Abstract: Despite growing interest in developing legged robots that emulate biological
locomotion for agile navigation of complex environments, acquiring a diverse
repertoire of skills remains a fundamental challenge in robotics. Existing
methods can learn motion behaviors from expert data, but they often fail to
acquire multiple locomotion skills through a single policy and lack smooth
skill transitions. We propose a multi-skill learning framework based on
Conditional Adversarial Motion Priors (CAMP), with the aim of enabling
quadruped robots to efficiently acquire a diverse set of locomotion skills from
expert demonstrations. Precise skill reconstruction is achieved through a novel
skill discriminator and skill-conditioned reward design. The overall framework
supports the active control and reuse of multiple skills, providing a practical
solution for learning generalizable policies in complex environments.

</details>


### [282] [Improved Vehicle Maneuver Prediction using Game Theoretic Priors](https://arxiv.org/abs/2509.21873)
*Nishant Doshi*

Main category: cs.RO

TL;DR: 本文提出结合Level-k博弈论与传统分类模型进行车辆驾驶行为预测，提高换道行为等复杂动作的预测准确率，适用于自动驾驶决策等系统。


<details>
  <summary>Details</summary>
Motivation: 传统的动作预测方法虽有较高精度与召回率，但未能有效利用全场景信息，难以正确预测如换道这样强依赖周围环境的行为。因此，亟需一种集成周围环境交互信息的新方法提升预测准确率。

Method: 本文采用Level-k博弈论模拟不同车辆间的人类级层次推理交互，从而推断每辆车可能采取的最理性行动，并将博弈论结果作为先验或与时序运动分类模型联合，在线优化得到目标车的最优动作策略预测。假设目标车周围车辆状态已知。

Result: 该方法能够输出目标车辆在当前环境下最理性的动作预测，其结果在自适应巡航控制（ACC）等自动驾驶系统中表现优异，并带来额外的油耗节省效果。

Conclusion: 结合博弈论与传统模型可以更精准地预测车辆复杂行为，对智能驾驶决策系统意义重大，有助于提升驾驶安全性与能效。

Abstract: Conventional maneuver prediction methods use some sort of classification
model on temporal trajectory data to predict behavior of agents over a set time
horizon. Despite of having the best precision and recall, these models cannot
predict a lane change accurately unless they incorporate information about the
entire scene. Level-k game theory can leverage the human-like hierarchical
reasoning to come up with the most rational decisions each agent can make in a
group. This can be leveraged to model interactions between different vehicles
in presence of each other and hence compute the most rational decisions each
agent would make. The result of game theoretic evaluation can be used as a
"prior" or combined with a traditional motion-based classification model to
achieve more accurate predictions. The proposed approach assumes that the
states of the vehicles around the target lead vehicle are known. The module
will output the most rational maneuver prediction of the target vehicle based
on an online optimization solution. These predictions are instrumental in
decision making systems like Adaptive Cruise Control (ACC) or Traxen's
iQ-Cruise further improving the resulting fuel savings.

</details>


### [283] [WAVE: Worm Gear-based Adaptive Variable Elasticity for Decoupling Actuators from External Forces](https://arxiv.org/abs/2509.21878)
*Moses Gladson Selvamuthu,Tomoya Takahashi,Riichiro Tadakuma,Kazutoshi Tanaka*

Main category: cs.RO

TL;DR: 该论文提出了一种基于蜗轮的新型可变刚度驱动器（WAVE），能够在提高机器人安全性和多用性的同时，提供精准力控制和冲击保护。


<details>
  <summary>Details</summary>
Motivation: 目前机器人操作臂需要兼顾柔顺性与刚度以提升安全和适应性，但现有的可变刚度驱动器在实现精准力传递、抗外部冲击以及高效隔离外部载荷方面仍存在不足。该文旨在通过新型设计解决这些问题。

Method: 作者提出WAVE驱动器，通过集成不可反驱的蜗轮结构，使驱动电机与外部力解耦，实现精确力传递且具备柔顺性；通过改变弹簧预压缩量，实现关节连续刚度调节。系统能将冲击转化为弹性势能，并在实验中对模型进行了验证。

Result: 实验表明，WAVE驱动器能精确调控关节刚度，模型得到验证；在静止和外载荷作用下，电机负载趋近于零，证明外部力与驱动有效解耦。还展示了带WAVE机械臂的实际应用。

Conclusion: WAVE驱动器成功实现了外力与传动的解耦，具备良好的抗冲击保护和耐接触操作能力，适用于复杂和高接触强度的机器人应用场景。

Abstract: Robotic manipulators capable of regulating both compliance and stiffness
offer enhanced operational safety and versatility. Here, we introduce Worm
Gear-based Adaptive Variable Elasticity (WAVE), a variable stiffness actuator
(VSA) that integrates a non-backdrivable worm gear. By decoupling the driving
motor from external forces using this gear, WAVE enables precise force
transmission to the joint, while absorbing positional discrepancies through
compliance. WAVE is protected from excessive loads by converting impact forces
into elastic energy stored in a spring. In addition, the actuator achieves
continuous joint stiffness modulation by changing the spring's precompression
length. We demonstrate these capabilities, experimentally validate the proposed
stiffness model, show that motor loads approach zero at rest--even under
external loading--and present applications using a manipulator with WAVE. This
outcome showcases the successful decoupling of external forces. The protective
attributes of this actuator allow for extended operation in contact-intensive
tasks, and for robust robotic applications in challenging environments.

</details>


### [284] [SAGE: Scene Graph-Aware Guidance and Execution for Long-Horizon Manipulation Tasks](https://arxiv.org/abs/2509.21928)
*Jialiang Li,Wenzheng Wu,Gaojing Zhang,Yifan Han,Wenzhao Lian*

Main category: cs.RO

TL;DR: 本文提出了SAGE框架，通过语义场景图表征状态，有效提升了长时序操作任务中的规划与控制能力。


<details>
  <summary>Details</summary>
Motivation: 长时序操作任务中需要同时做到高层次语义规划和低层次视觉-动作控制，但当前方法在泛化能力和语义推理上均存在不足，导致难以解决复杂操作问题。

Method: SAGE框架用语义场景图表示场景状态，其包含两个核心模块：1）基于场景图的任务规划器，利用VLM（视觉语言模型）和LLM（大语言模型）解析环境与推理状态转移；2）结构化图像编辑流程，将子目标场景图精准转化为目标图像，实现细致控制。

Result: SAGE在多个具有挑战性的长时序操作任务上进行了大量实验，并取得了当前最优性能。

Conclusion: 基于语义场景图的结构化方法能有效结合语义推理与视觉控制，是长时序操作任务有效的桥梁。

Abstract: Successfully solving long-horizon manipulation tasks remains a fundamental
challenge. These tasks involve extended action sequences and complex object
interactions, presenting a critical gap between high-level symbolic planning
and low-level continuous control. To bridge this gap, two essential
capabilities are required: robust long-horizon task planning and effective
goal-conditioned manipulation. Existing task planning methods, including
traditional and LLM-based approaches, often exhibit limited generalization or
sparse semantic reasoning. Meanwhile, image-conditioned control methods
struggle to adapt to unseen tasks. To tackle these problems, we propose SAGE, a
novel framework for Scene Graph-Aware Guidance and Execution in Long-Horizon
Manipulation Tasks. SAGE utilizes semantic scene graphs as a structural
representation for scene states. A structural scene graph enables bridging
task-level semantic reasoning and pixel-level visuo-motor control. This also
facilitates the controllable synthesis of accurate, novel sub-goal images. SAGE
consists of two key components: (1) a scene graph-based task planner that uses
VLMs and LLMs to parse the environment and reason about physically-grounded
scene state transition sequences, and (2) a decoupled structural image editing
pipeline that controllably converts each target sub-goal graph into a
corresponding image through image inpainting and composition. Extensive
experiments have demonstrated that SAGE achieves state-of-the-art performance
on distinct long-horizon tasks.

</details>


### [285] [Learnable Conformal Prediction with Context-Aware Nonconformity Functions for Robotic Planning and Perception](https://arxiv.org/abs/2509.21955)
*Divake Kumar,Sina Tayebati,Francesco Migliarba,Ranganath Krishnan,Amit Ranjan Trivedi*

Main category: cs.RO

TL;DR: 提出了一种可学习的保形预测方法（LCP），在保持理论覆盖率保证的同时，大幅缩小了预测集合，并提升了机器人任务中的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 深度学习在机器人中的不确定性评估不理想，传统保形预测虽然有分布无关的理论保证，但其固定的非一致性度量会带来过于保守甚至不安全的预测区间，缺乏对具体上下文的适应能力。

Method: 提出Learnable Conformal Prediction（LCP）方法，用可训练的小型神经网络函数动态生成上下文感知的不确定性集合，充分利用几何、语义和任务特定的特征，同时保持CP方法的分布无关理论保障。

Result: LCP在7个基准的3类机器人任务上，预测集合显著缩小（例如，分类任务集大小减少18%，目标检测边界框缩小46-54%），路径规划成功率从72%提升到91%，推理开销极低（4.8%运行时、42KB内存），与传统方法相比，在准确率、安全性和能效上均有优势。

Conclusion: LCP方法在不显著增加硬件负担的前提下，实现了更小、更安全的预测集合，适合资源受限的自动系统，理论严谨且实际效果优越，优于标准CP及集成基线。

Abstract: Deep learning models in robotics often output point estimates with poorly
calibrated confidences, offering no native mechanism to quantify predictive
reliability under novel, noisy, or out-of-distribution inputs. Conformal
prediction (CP) addresses this gap by providing distribution-free coverage
guarantees, yet its reliance on fixed nonconformity scores ignores context and
can yield intervals that are overly conservative or unsafe. We address this
with Learnable Conformal Prediction (LCP), which replaces fixed scores with a
lightweight neural function that leverages geometric, semantic, and
task-specific features to produce context-aware uncertainty sets.
  LCP maintains CP's theoretical guarantees while reducing prediction set sizes
by 18% in classification, tightening detection intervals by 52%, and improving
path planning safety from 72% to 91% success with minimal overhead. Across
three robotic tasks on seven benchmarks, LCP consistently outperforms Standard
CP and ensemble baselines. In classification on CIFAR-100 and ImageNet, it
achieves smaller set sizes (4.7-9.9% reduction) at target coverage. For object
detection on COCO, BDD100K, and Cityscapes, it produces 46-54% tighter bounding
boxes. In path planning through cluttered environments, it improves success to
91.5% with only 4.5% path inflation, compared to 12.2% for Standard CP.
  The method is lightweight (approximately 4.8% runtime overhead, 42 KB memory)
and supports online adaptation, making it well suited to resource-constrained
autonomous systems. Hardware evaluation shows LCP adds less than 1% memory and
15.9% inference overhead, yet sustains 39 FPS on detection tasks while being
7.4 times more energy-efficient than ensembles.

</details>


### [286] [FlowDrive: moderated flow matching with data balancing for trajectory planning](https://arxiv.org/abs/2509.21961)
*Lingguang Wang,Ömer Şahin Taş,Marlon Steiner,Christoph Stiller*

Main category: cs.RO

TL;DR: 本文提出了一种针对自动驾驶规划中数据分布失衡问题的新方法FlowDrive，并通过创新性的引导机制和采样策略显著提升了模型在关键场景下的表现，达到了当前同类方法的最佳水平。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶场景下的数据集存在显著的长尾分布，常见驾驶动作数据量大，而危险或罕见场景数据稀缺，导致基于学习的规划器在关键或非常规情境下表现不佳。为提升模型对稀疏关键场景的适应能力，文中寻求合理的数据平衡与模型优化策略。

Method: 作者首先对不同的数据采样平衡策略进行对比，确认基于轨迹模式重加权最有效。在此基础上，提出了FlowDrive——一种基于flow-matching的轨迹规划方法，可通过很少的flow-matching步骤，将噪声高效映射为具体的轨迹分布。同时，设计了适中的循环引导机制，在每步flow中注入小扰动，系统性地提升轨迹多样性并保证其与场景一致性。最终在流行的nuPlan和interPlan基准上进行评测。

Result: FlowDrive方法在nuPlan和interPlan数据集上取得了学习型规划器中的最优效果，接近甚至超过了部分带规则优化的传统方法。进一步引入“适度引导”和轻量后处理（FlowDrive*）后，其在绝大多数基准分割上达到了整体最佳表现。

Conclusion: 文中提出的FlowDrive及其进阶版本FlowDrive*，通过创新的数据重加权和引导机制，有效解决了自动驾驶轨迹规划中的长尾分布问题，并在标准数据集上实现了优异的综合表现，为自动驾驶学习型规划器设计提供了新的思路和方法。

Abstract: Learning-based planners are sensitive to the long-tailed distribution of
driving data. Common maneuvers dominate datasets, while dangerous or rare
scenarios are sparse. This imbalance can bias models toward the frequent cases
and degrade performance on critical scenarios. To tackle this problem, we
compare balancing strategies for sampling training data and find reweighting by
trajectory pattern an effective approach. We then present FlowDrive, a
flow-matching trajectory planner that learns a conditional rectified flow to
map noise directly to trajectory distributions with few flow-matching steps. We
further introduce moderated, in-the-loop guidance that injects small
perturbation between flow steps to systematically increase trajectory diversity
while remaining scene-consistent. On nuPlan and the interaction-focused
interPlan benchmarks, FlowDrive achieves state-of-the-art results among
learning-based planners and approaches methods with rule-based refinements.
After adding moderated guidance and light post-processing (FlowDrive*), it
achieves overall state-of-the-art performance across nearly all benchmark
splits.

</details>


### [287] [Hybrid Diffusion for Simultaneous Symbolic and Continuous Planning](https://arxiv.org/abs/2509.21983)
*Sigmund Hennum Høeg,Aksel Vaaler,Chaoqi Liu,Olav Egeland,Yilun Du*

Main category: cs.RO

TL;DR: 本文提出结合高层符号计划与连续轨迹生成的混合扩散模型，大幅提升机器人完成复杂长时序任务的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成式方法（如扩散模型）的机器人路径规划，在长时序和复杂决策任务上表现不足，常出现混淆行为模式而导致失败。

Method: 创新性地将高层符号计划生成与连续轨迹生成结合，提出同时进行离散变量扩散和连续扩散的混合扩散过程，用以提升规划能力。

Result: 该方法在机器人长时序任务上显著优于现有基线模型，并实现了高灵活性的轨迹合成，可根据部分或完整的符号条件调整生成动作。

Conclusion: 将离散和连续扩散混合应用于机器人轨迹规划，有效弥补了现有方法在长时序复杂任务中的不足，展示了未来混合生成方法的广阔前景。

Abstract: Constructing robots to accomplish long-horizon tasks is a long-standing
challenge within artificial intelligence. Approaches using generative methods,
particularly Diffusion Models, have gained attention due to their ability to
model continuous robotic trajectories for planning and control. However, we
show that these models struggle with long-horizon tasks that involve complex
decision-making and, in general, are prone to confusing different modes of
behavior, leading to failure. To remedy this, we propose to augment continuous
trajectory generation by simultaneously generating a high-level symbolic plan.
We show that this requires a novel mix of discrete variable diffusion and
continuous diffusion, which dramatically outperforms the baselines. In
addition, we illustrate how this hybrid diffusion process enables flexible
trajectory synthesis, allowing us to condition synthesized actions on partial
and complete symbolic conditions.

</details>


### [288] [Developing Vision-Language-Action Model from Egocentric Videos](https://arxiv.org/abs/2509.21986)
*Tomoya Yoshida,Shuhei Kurita,Taichi Nishimura,Shinsuke Mori*

Main category: cs.RO

TL;DR: 本论文提出EgoScaler框架，实现无需辅助注释即可从第一视角视频中提取6自由度物体操作轨迹，用于训练视觉-语言-动作（VLA）模型，在仿真和真实机器人实验中证明大规模第一视角数据可大幅提升学习表现。


<details>
  <summary>Details</summary>
Motivation: 传统VLA模型训练依赖昂贵且需要专家操作的数据采集方式，限制了其广泛应用。第一视角视频能够以更低成本大规模采集，但现有方法多需额外手部或动作标注，阻碍了直接利用原始视频自动化训练的探索。

Method: 提出EgoScaler框架，可自动从原始第一视角视频中提取及修正6DoF轨迹，无需额外手部或动作标注。将此方法应用于四个大规模视频数据集，自动获得用于VLA预训练的新数据集。

Result: 1）采用EgoScaler生成的数据进行预训练，可使机器人任务成功率提升超过20%；2）该方法在仿真和真实环境下均与真机采集数据表现相当；3）与真实机器人数据结合进一步提升性能。

Conclusion: 第一视角视频是训练VLA模型的可扩展、有效数据源。EgoScaler实现了无标注下自动提取可用于机器人操作学习的高质量数据，有望推动该领域发展。

Abstract: Egocentric videos capture how humans manipulate objects and tools, providing
diverse motion cues for learning object manipulation. Unlike the costly,
expert-driven manual teleoperation commonly used in training
Vision-Language-Action models (VLAs), egocentric videos offer a scalable
alternative. However, prior studies that leverage such videos for training
robot policies typically rely on auxiliary annotations, such as detailed
hand-pose recordings. Consequently, it remains unclear whether VLAs can be
trained directly from raw egocentric videos. In this work, we address this
challenge by leveraging EgoScaler, a framework that extracts 6DoF object
manipulation trajectories from egocentric videos without requiring auxiliary
recordings. We apply EgoScaler to four large-scale egocentric video datasets
and automatically refine noisy or incomplete trajectories, thereby constructing
a new large-scale dataset for VLA pre-training. Our experiments with a
state-of-the-art $\pi_0$ architecture in both simulated and real-robot
environments yield three key findings: (i) pre-training on our dataset improves
task success rates by over 20\% compared to training from scratch, (ii) the
performance is competitive with that achieved using real-robot datasets, and
(iii) combining our dataset with real-robot data yields further improvements.
These results demonstrate that egocentric videos constitute a promising and
scalable resource for advancing VLA research.

</details>


### [289] [One-DoF Robotic Design of Overconstrained Limbs with Energy-Efficient, Self-Collision-Free Motion](https://arxiv.org/abs/2509.22002)
*Yuping Gu,Bangchao Huang,Haoran Sun,Ronghan Xu,Jiayi Yin,Wei Zhang,Fang Wan,Jia Pan,Chaoyang Song*

Main category: cs.RO

TL;DR: 本文提出了一种新型的计算方法，用于设计一自由度(1-DoF)的过约束机器人肢体，实现高效、无自碰撞的空间运动，并通过仿生六足机器人等实验验证了能效提升。


<details>
  <summary>Details</summary>
Motivation: 尽管多自由度的仿生机器人肢体备受关注，但单自由度系统凭借其结构简单、鲁棒性高、成本低和效率高的优点仍具重要意义。现有1-DoF机制很容易因自碰撞限制运动范围，因此亟需高效、能实现多样运动的新型结构设计方法。

Method: 作者提出了基于连杆系统的1-DoF机器人肢体设计的几何优化通用公式，实现自碰撞规避。同时，通过优化空间轨迹和动力学相关指标，进一步设计出能够由单一驱动器驱动、平滑无碰撞运动的过约束连杆结构。相关方法应用于多种自动装置及仿生六足机器人的原型设计与实验证明。

Result: 实验表明，新型过约束1-DoF机器肢体能实现预期空间轨迹，全周期运动过程无碰撞，应用于仿生六足机器人时，显著提升了前行过程的能效。

Conclusion: 该研究实现了对1-DoF过约束机器人肢体的高效设计，为低成本、高效率、结构简单且能实现复杂运动的新型机器人提供了理论与实践基础。

Abstract: While it is expected to build robotic limbs with multiple degrees of freedom
(DoF) inspired by nature, a single DoF design remains fundamental, providing
benefits that include, but are not limited to, simplicity, robustness,
cost-effectiveness, and efficiency. Mechanisms, especially those with multiple
links and revolute joints connected in closed loops, play an enabling factor in
introducing motion diversity for 1-DoF systems, which are usually constrained
by self-collision during a full-cycle range of motion. This study presents a
novel computational approach to designing one-degree-of-freedom (1-DoF)
overconstrained robotic limbs for a desired spatial trajectory, while achieving
energy-efficient, self-collision-free motion in full-cycle rotations. Firstly,
we present the geometric optimization problem of linkage-based robotic limbs in
a generalized formulation for self-collision-free design. Next, we formulate
the spatial trajectory generation problem with the overconstrained linkages by
optimizing the similarity and dynamic-related metrics. We further optimize the
geometric shape of the overconstrained linkage to ensure smooth and
collision-free motion driven by a single actuator. We validated our proposed
method through various experiments, including personalized automata and
bio-inspired hexapod robots. The resulting hexapod robot, featuring
overconstrained robotic limbs, demonstrated outstanding energy efficiency
during forward walking.

</details>


### [290] [An Adaptive ICP LiDAR Odometry Based on Reliable Initial Pose](https://arxiv.org/abs/2509.22058)
*Qifeng Wang,Weigang Li,Lei Nie,Xin Xu,Wenping Liu,Zhe Xu*

Main category: cs.RO

TL;DR: 本文提出了一种自适应ICP（迭代最近点）算法提升了LiDAR里程计在动态环境中的定位精度，核心方法通过优化初始位姿估计和引入自适应阈值，实现了更稳定准确的点云配准。实验表明，该方法在KITTI公开数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于ICP的LiDAR里程计方法在初始位姿可靠性不足、缺乏自适应机制时，容易陷入局部最优且配准精度受复杂动态环境影响较大。为此亟需一种能提升初始估计可靠性并自适应动态环境变化的LiDAR点云配准方法。

Method: 1. 利用基于密度滤波的分布式粗配准获得初始位姿估计；2. 通过与运动预测位姿对比选择最可靠的初始位姿，减小点云初始误差；3. 融合当前与历史配准误差，自适应调整ICP阈值以匹配动态环境变化；4. 在可靠初始位姿和自适应阈值基础上，执行点到平面的ICP配准，实现高精度点云对齐。

Result: 在KITTI公开数据集上开展大量实验，结果显示该方法在点云对齐精度和鲁棒性上均优于现有ICP类方法，显著提升了LiDAR里程计的定位效果。

Conclusion: 提出的自适应ICP-LiDAR里程计方法有效克服了初始估计不准和动态环境影响大的问题，能够实现高精度、稳定的点云配准，具备较高的实用价值。

Abstract: As a key technology for autonomous navigation and positioning in mobile
robots, light detection and ranging (LiDAR) odometry is widely used in
autonomous driving applications. The Iterative Closest Point (ICP)-based
methods have become the core technique in LiDAR odometry due to their efficient
and accurate point cloud registration capability. However, some existing
ICP-based methods do not consider the reliability of the initial pose, which
may cause the method to converge to a local optimum. Furthermore, the absence
of an adaptive mechanism hinders the effective handling of complex dynamic
environments, resulting in a significant degradation of registration accuracy.
To address these issues, this paper proposes an adaptive ICP-based LiDAR
odometry method that relies on a reliable initial pose. First, distributed
coarse registration based on density filtering is employed to obtain the
initial pose estimation. The reliable initial pose is then selected by
comparing it with the motion prediction pose, reducing the initial error
between the source and target point clouds. Subsequently, by combining the
current and historical errors, the adaptive threshold is dynamically adjusted
to accommodate the real-time changes in the dynamic environment. Finally, based
on the reliable initial pose and the adaptive threshold, point-to-plane
adaptive ICP registration is performed from the current frame to the local map,
achieving high-precision alignment of the source and target point clouds.
Extensive experiments on the public KITTI dataset demonstrate that the proposed
method outperforms existing approaches and significantly enhances the accuracy
of LiDAR odometry.

</details>


### [291] [Effect of Gait Design on Proprioceptive Sensing of Terrain Properties in a Quadrupedal Robot](https://arxiv.org/abs/2509.22065)
*Ethan Fulcher,J. Diego Caporale,Yifeng Zhang,John Ruck,Feifei Qian*

Main category: cs.RO

TL;DR: 本论文研究了腿式机器人在不同步态下，用腿部传感实现地形测量的准确性，为未来行星探测及地质研究提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 对地球和其他行星体进行地质调查时，需要高效并安全地获得地形力学参数。传统方法有采样速度慢、风险高等缺陷，因此探索腿式机器人在行进中测量地形的可行性与准确性具有重要意义。

Method: 本文设计了两种步态：以感知为导向的爬行步态（Crawl N' Sense）与以行走为导向的交替步态（Trot-Walk），并让机器人在刚性表面、松散沙地和带表层壳的松散沙地三种地形上移动，实时记录腿部反馈数据，评估其测量地形的能力和准确性。

Result: 两种步态均能区分不同强度（抗穿透性）的地形，但行走步态测量值的方差和幅值较大。感知导向的爬行步态能更准确地侦测表层壳的脆性破裂。

Conclusion: 结果表明，腿式机器人在合适的步态下可实现实时地形感知，有效提升地理信息采集的效率和精度。研究为探地机器人的步态设计和操作规划提供了新思路，有助于拓展对地球和其他星球地质的认知。

Abstract: In-situ robotic exploration is an important tool for advancing knowledge of
geological processes that describe the Earth and other Planetary bodies. To
inform and enhance operations for these roving laboratories, it is imperative
to understand the terramechanical properties of their environments, especially
for traversing on loose, deformable substrates. Recent research suggested that
legged robots with direct-drive and low-gear ratio actuators can sensitively
detect external forces, and therefore possess the potential to measure terrain
properties with their legs during locomotion, providing unprecedented sampling
speed and density while accessing terrains previously too risky to sample. This
paper explores these ideas by investigating the impact of gait on
proprioceptive terrain sensing accuracy, particularly comparing a
sensing-oriented gait, Crawl N' Sense, with a locomotion-oriented gait,
Trot-Walk. Each gait's ability to measure the strength and texture of
deformable substrate is quantified as the robot locomotes over a laboratory
transect consisting of a rigid surface, loose sand, and loose sand with
synthetic surface crusts. Our results suggest that with both the
sensing-oriented crawling gait and locomotion-oriented trot gait, the robot can
measure a consistent difference in the strength (in terms of penetration
resistance) between the low- and high-resistance substrates; however, the
locomotion-oriented trot gait contains larger magnitude and variance in
measurements. Furthermore, the slower crawl gait can detect brittle ruptures of
the surface crusts with significantly higher accuracy than the faster trot
gait. Our results offer new insights that inform legged robot "sensing during
locomotion" gait design and planning for scouting the terrain and producing
scientific measurements on other worlds to advance our understanding of their
geology and formation.

</details>


### [292] [Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation](https://arxiv.org/abs/2509.22093)
*Xiaohuan Pei,Yuxing Chen,Siyu Xu,Yunke Wang,Yuheng Shi,Chang Xu*

Main category: cs.RO

TL;DR: 本文提出了一种动作感知的动态裁剪（ADP）方法，用于提升视觉-语言-动作（VLA）模型在机器人操作中的推理效率，通过结合动作轨迹信息动态裁剪视觉冗余，在保证策略成功率的同时大幅提升运行速度。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作模型在机器人操作中需要长期处理大量视觉信息，计算开销高。现有加速方法仅全局减少冗余，忽视了操作不同阶段视觉信息冗余存在差异。作者发现粗操作阶段视觉冗余远高于精细操作，且与运动动态密切相关，因而提出针对实际操作动态调节视觉信息保留量的方法。

Method: 提出动作感知动态裁剪（ADP）框架，将文本驱动视觉token选择与动作感知的轨迹gating机制结合。具体做法是根据近期动作轨迹信号动态调整视觉token保留比例，实现操作各阶段的自适应裁剪平衡感知精度和推理效率。

Result: 在LIBERO套件与多个现实场景上开展实验。结果显示，ADP方法在保持甚至提升操作成功率（如OpenVLA提升了25.8%）的同时，大幅减少FLOPs和推理延迟（如OpenVLA-OFT推理速度提升1.35倍），全面优于现有方法。

Conclusion: ADP为提升VLA机器人操作策略提供了一种简单有效的插件式加速路径，在保障性能的基础上显著提高推理效率，推动了机器人操作领域的效率与性能界限发展。

Abstract: Robotic manipulation with Vision-Language-Action models requires efficient
inference over long-horizon multi-modal context, where attention to dense
visual tokens dominates computational cost. Existing methods optimize inference
speed by reducing visual redundancy within VLA models, but they overlook the
varying redundancy across robotic manipulation stages. We observe that the
visual token redundancy is higher in coarse manipulation phase than in
fine-grained operations, and is strongly correlated with the action dynamic.
Motivated by this observation, we propose \textbf{A}ction-aware
\textbf{D}ynamic \textbf{P}runing (\textbf{ADP}), a multi-modal pruning
framework that integrates text-driven token selection with action-aware
trajectory gating. Our method introduces a gating mechanism that conditions the
pruning signal on recent action trajectories, using past motion windows to
adaptively adjust token retention ratios in accordance with dynamics, thereby
balancing computational efficiency and perceptual precision across different
manipulation stages. Extensive experiments on the LIBERO suites and diverse
real-world scenarios demonstrate that our method significantly reduces FLOPs
and action inference latency (\textit{e.g.} $1.35 \times$ speed up on
OpenVLA-OFT) while maintaining competitive success rates (\textit{e.g.} 25.8\%
improvements with OpenVLA) compared to baselines, thereby providing a simple
plug-in path to efficient robot policies that advances the efficiency and
performance frontier of robotic manipulation. Our project website is:
\href{https://vla-adp.github.io/}{ADP.com}.

</details>


### [293] [Multi-stage robust nonlinear model predictive control of a lower-limb exoskeleton robot](https://arxiv.org/abs/2509.22120)
*Alireza Aliyari,Gholamreza Vossoughi*

Main category: cs.RO

TL;DR: 本文提出一种名为多阶段非线性模型预测控制（multi-stage NMPC）的鲁棒控制方法，用于提升外骨骼机器人在不确定性条件下的控制性能，显著降低了人机交互力和跟踪误差。


<details>
  <summary>Details</summary>
Motivation: 外骨骼机器人广泛应用于康复和辅助领域，但由于人机系统存在较多不确定性，传统的线性化鲁棒控制方法易受非线性动态影响，导致性能下降。因此，亟需开发能更好应对不确定性的鲁棒非线性控制策略。

Method: 作者提出了一种多阶段非线性模型预测控制（multi-stage NMPC），通过引入多个场景模拟系统不确定性，在控制两自由度的外骨骼机器人时，将控制问题建模为非线性优化问题。该方法在步态摆动阶段特别关注外骨骼携带未知负载时的人机交互力最小化。

Result: 仿真和实验结果显示，该方法在不同不确定性（如未知负载及外部扰动）下显著优于非鲁棒NMPC。例如，在附加2kg未知负载并施加外部扰动后，多阶段NMPC使大腿和小腿的人机交互力均方根值分别降低77%和94%。

Conclusion: 多阶段NMPC方法能有效提升外骨骼机器人在存在系统不确定性时的鲁棒性和控制性能，显著降低人机交互力和跟踪误差，相比传统非鲁棒NMPC具有显著优势。

Abstract: The use of exoskeleton robots is increasing due to the rising number of
musculoskeletal injuries. However, their effectiveness depends heavily on the
design of control systems. Designing robust controllers is challenging because
of uncertainties in human-robot systems. Among various control strategies,
Model Predictive Control (MPC) is a powerful approach due to its ability to
handle constraints and optimize performance. Previous studies have used
linearization-based methods to implement robust MPC on exoskeletons, but these
can degrade performance due to nonlinearities in the robot's dynamics. To
address this gap, this paper proposes a Robust Nonlinear Model Predictive
Control (RNMPC) method, called multi-stage NMPC, to control a
two-degree-of-freedom exoskeleton by solving a nonlinear optimization problem.
This method uses multiple scenarios to represent system uncertainties. The
study focuses on minimizing human-robot interaction forces during the swing
phase, particularly when the robot carries unknown loads. Simulations and
experimental tests show that the proposed method significantly improves
robustness, outperforming non-robust NMPC. It achieves lower tracking errors
and interaction forces under various uncertainties. For instance, when a 2 kg
unknown payload is combined with external disturbances, the RMS values of thigh
and shank interaction forces for multi-stage NMPC are reduced by 77 and 94
percent, respectively, compared to non-robust NMPC.

</details>


### [294] [DemoGrasp: Universal Dexterous Grasping from a Single Demonstration](https://arxiv.org/abs/2509.22149)
*Haoqi Yuan,Ziye Huang,Ye Wang,Chuan Mao,Chaoyi Xu,Zongqing Lu*

Main category: cs.RO

TL;DR: 提出了一种名为DemoGrasp的新方法，通过编辑单条抓取演示轨迹，实现多指灵巧手对各类物体的高效泛化抓取，并在模拟和现实中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的多指抓取方法在面对高维、长时序任务时，探索难度大，需要复杂的奖励与课程设计，且易陷入对特定物体的次优解，难以泛化到新物体。

Method: 作者提出DemoGrasp方法，从单个物体的成功抓取演示出发，通过调整轨迹中的手腕位姿和手指关节角度，适应新的物体与姿态，将轨迹编辑建模为单步MDP，并利用简单的奖励函数（抓取成功和碰撞惩罚）训练通用策略，在仿真中并行优化，支持视觉模仿和语言引导。

Result: 在模拟中，DemoGrasp对DexGraspNet对象达到了95%的抓取成功率，优于现有方法；迁移到不同类型灵巧手和六个未见数据集，平均成功率为84.6%；在现实110个未见物体抓取中，包括小且薄的物品，收获成功。

Conclusion: DemoGrasp无需复杂奖励即可实现灵巧手的通用高效抓取，在模拟和现实均表现出良好的泛化能力，支持多种输入和场景，推动了灵巧机械手通用抓取的发展。

Abstract: Universal grasping with multi-fingered dexterous hands is a fundamental
challenge in robotic manipulation. While recent approaches successfully learn
closed-loop grasping policies using reinforcement learning (RL), the inherent
difficulty of high-dimensional, long-horizon exploration necessitates complex
reward and curriculum design, often resulting in suboptimal solutions across
diverse objects. We propose DemoGrasp, a simple yet effective method for
learning universal dexterous grasping. We start from a single successful
demonstration trajectory of grasping a specific object and adapt to novel
objects and poses by editing the robot actions in this trajectory: changing the
wrist pose determines where to grasp, and changing the hand joint angles
determines how to grasp. We formulate this trajectory editing as a single-step
Markov Decision Process (MDP) and use RL to optimize a universal policy across
hundreds of objects in parallel in simulation, with a simple reward consisting
of a binary success term and a robot-table collision penalty. In simulation,
DemoGrasp achieves a 95% success rate on DexGraspNet objects using the Shadow
Hand, outperforming previous state-of-the-art methods. It also shows strong
transferability, achieving an average success rate of 84.6% across diverse
dexterous hand embodiments on six unseen object datasets, while being trained
on only 175 objects. Through vision-based imitation learning, our policy
successfully grasps 110 unseen real-world objects, including small, thin items.
It generalizes to spatial, background, and lighting changes, supports both RGB
and depth inputs, and extends to language-guided grasping in cluttered scenes.

</details>


### [295] [DHAGrasp: Synthesizing Affordance-Aware Dual-Hand Grasps with Text Instructions](https://arxiv.org/abs/2509.22175)
*Quanzhou Li,Zhonghua Wu,Jingbo Wang,Chen Change Loy,Bo Dai*

Main category: cs.RO

TL;DR: 本文提出了一种新的大规模双手抓取数据集构建方法以及双手抓取生成器，有效提升了双手抓取的语义一致性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有抓取数据集主要聚焦于单手抓取，并且语义分割有限，导致对双手与物体复杂交互的学习受到限制，阻碍了双手语义一致性抓取能力的发展。

Method: 1. 提出SymOpt流程：通过利用现有单手数据集及物体和手的对称性，合成大规模双手抓取数据集。2. 提出DHAGrasp生成器：结合文本引导和创新的双手抓取表征，采用两阶段方案，既能从小规模分割数据中学习，又能扩展到大规模未分割数据。

Result: 实验表明该方法生成的抓取动作丰富、符合语义，并在抓取质量和对未见物体的泛化能力上显著超过现有方法。

Conclusion: SymOpt和DHAGrasp为语义一致的双手抓取提供了新思路，并大幅提升了数据集构建和生成模型效果，有助于推进复杂手物交互的研究。

Abstract: Learning to generate dual-hand grasps that respect object semantics is
essential for robust hand-object interaction but remains largely underexplored
due to dataset scarcity. Existing grasp datasets predominantly focus on
single-hand interactions and contain only limited semantic part annotations. To
address these challenges, we introduce a pipeline, SymOpt, that constructs a
large-scale dual-hand grasp dataset by leveraging existing single-hand datasets
and exploiting object and hand symmetries. Building on this, we propose a
text-guided dual-hand grasp generator, DHAGrasp, that synthesizes Dual-Hand
Affordance-aware Grasps for unseen objects. Our approach incorporates a novel
dual-hand affordance representation and follows a two-stage design, which
enables effective learning from a small set of segmented training objects while
scaling to a much larger pool of unsegmented data. Extensive experiments
demonstrate that our method produces diverse and semantically consistent
grasps, outperforming strong baselines in both grasp quality and generalization
to unseen objects. The project page is at
https://quanzhou-li.github.io/DHAGrasp/.

</details>


### [296] [Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting](https://arxiv.org/abs/2509.22195)
*Asher J. Hancock,Xindi Wu,Lihan Zha,Olga Russakovsky,Anirudha Majumdar*

Main category: cs.RO

TL;DR: 本文提出一种新的训练方法VLM2VLA，通过将机器人操作的低级动作转化为自然语言，与视觉-语言模型的预训练分布对齐，使得微调后既能学习动作，又能保留模型原有的多模态推理能力，从而提升泛化性和语义理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在用机器人遥操作数据微调视觉-语言模型（VLM）时，为了学习动作策略，往往牺牲了模型原始的推理与理解能力，导致对新任务或复杂指令泛化能力差。作者认为主要原因在于机器人数据与大规模互联网预训练数据分布不一致，引发灾难性遗忘。

Method: 提出VLM2VLA方法，在动作数据层面通过自然语言将低级动作表述，从而实现分布对齐。然后仅用Low-Rank Adaptation（LoRA）微调VLM主干，无需大规模共训练。这种方式极小化模型结构修改，避免了原有知识的大量遗忘。

Result: 实验包括大量视觉问答和超过800个真实机器人实验，结果显示VLM2VLA不仅保持了VLM原有能力，还实现了面对新任务及多语言指令时的零样本泛化。

Conclusion: VLM2VLA让VLM成功吸收机器人遥操作数据而不丧失核心推理、多模态及理解能力，为训练通用机器人策略指明了高效、通用的新方向。

Abstract: Fine-tuning vision-language models (VLMs) on robot teleoperation data to
create vision-language-action (VLA) models is a promising paradigm for training
generalist policies, but it suffers from a fundamental tradeoff: learning to
produce actions often diminishes the VLM's foundational reasoning and
multimodal understanding, hindering generalization to novel scenarios,
instruction following, and semantic understanding. We argue that this
catastrophic forgetting is due to a distribution mismatch between the VLM's
internet-scale pretraining corpus and the robotics fine-tuning data. Inspired
by this observation, we introduce VLM2VLA: a VLA training paradigm that first
resolves this mismatch at the data level by representing low-level actions with
natural language. This alignment makes it possible to train VLAs solely with
Low-Rank Adaptation (LoRA), thereby minimally modifying the VLM backbone and
averting catastrophic forgetting. As a result, the VLM can be fine-tuned on
robot teleoperation data without fundamentally altering the underlying
architecture and without expensive co-training on internet-scale VLM datasets.
Through extensive Visual Question Answering (VQA) studies and over 800
real-world robotics experiments, we demonstrate that VLM2VLA preserves the
VLM's core capabilities, enabling zero-shot generalization to novel tasks that
require open-world semantic reasoning and multilingual instruction following.

</details>


### [297] [MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training](https://arxiv.org/abs/2509.22199)
*Haoyun Li,Ivan Zhang,Runqi Ouyang,Xiaofeng Wang,Zheng Zhu,Zhiqin Yang,Zhentao Zhang,Boyuan Wang,Chaojun Ni,Wenkang Qin,Xinze Chen,Yun Ye,Guan Huang,Zhenbo Song,Xingang Wang*

Main category: cs.RO

TL;DR: 该论文提出MimicDreamer框架，通过视频扩散模型、视角校正和动作对齐等方法，将人类演示视频转化为机器人可用的数据，大幅提升机器人操控任务的学习效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 机器人视觉-语言-动作（VLA）模型依赖多样化的训练数据，但收集真实机器人视频数据成本高昂。而人类演示视频可大规模低成本采集，因此提升其转化为机器人训练数据的有效性是关键。当前人机视频存在显著域差异，需创新方法解决。

Method: 1) H2R Aligner：基于视频扩散模型，将人类操作视频的运动迁移到机器人操作视频中，实现视觉对齐；2) EgoStabilizer：基于单应性和修补算法，对第一人称视频视角进行稳定化和修补；3) 动作对齐：将人手轨迹映射到机器人空间，通过受约束逆运动学算法得到精准、平滑的机器人关节指令。

Result: 实验结果显示，完全基于合成的人-机转化视频训练的VLA模型，在实际机器人上实现了小样本任务执行。扩展人类数据参与训练后，在六个典型操作任务上平均成功率提升了14.7%，明显超过只用真实机器人数据训练的模型。

Conclusion: MimicDreamer能有效弥合人类视频与机器人数据的域差异，实现大规模、低成本、高效的机器人监督数据合成，为VLA模型大幅提升性能，具有推广和应用价值。

Abstract: Vision Language Action (VLA) models derive their generalization capability
from diverse training data, yet collecting embodied robot interaction data
remains prohibitively expensive. In contrast, human demonstration videos are
far more scalable and cost-efficient to collect, and recent studies confirm
their effectiveness in training VLA models. However, a significant domain gap
persists between human videos and robot-executed videos, including unstable
camera viewpoints, visual discrepancies between human hands and robotic arms,
and differences in motion dynamics. To bridge this gap, we propose
MimicDreamer, a framework that turns fast, low-cost human demonstrations into
robot-usable supervision by jointly aligning vision, viewpoint, and actions to
directly support policy training. For visual alignment, we propose H2R Aligner,
a video diffusion model that generates high-fidelity robot demonstration videos
by transferring motion from human manipulation footage. For viewpoint
stabilization, EgoStabilizer is proposed, which canonicalizes egocentric videos
via homography and inpaints occlusions and distortions caused by warping. For
action alignment, we map human hand trajectories to the robot frame and apply a
constrained inverse kinematics solver to produce feasible, low-jitter joint
commands with accurate pose tracking. Empirically, VLA models trained purely on
our synthesized human-to-robot videos achieve few-shot execution on real
robots. Moreover, scaling training with human data significantly boosts
performance compared to models trained solely on real robot data; our approach
improves the average success rate by 14.7\% across six representative
manipulation tasks.

</details>


### [298] [From Watch to Imagine: Steering Long-horizon Manipulation via Human Demonstration and Future Envisionment](https://arxiv.org/abs/2509.22205)
*Ke Ye,Jiaming Zhou,Yuanfeng Qiu,Jiayi Liu,Shihui Zhou,Kun-Yu Lin,Junwei Liang*

Main category: cs.RO

TL;DR: 本文提出了一种新的分层框架Super-Mimic，实现了机器人对复杂长时序操作任务的零样本模仿，其核心在于直接从自然人类演示视频中推断出可执行的步骤与动作序列，极大提升了机器人在未知任务中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态基础模型虽在机器人任务中表现突出，但仅凭静态视觉输入难以有效地将高层指令分解为具体可执行的动作序列，限制了机器人在长时序复杂任务中的泛化与实用性。

Method: Super-Mimic框架包含两个顺序模块：1）人类意图转换器（HIT）通过多模态推理分析输入演示视频，输出语言化的子任务序列；2）未来动态预测器（FDP）以生成式模型为基础，针对每步生成物理可行的视频预测轨迹，显式建模了物体交互和接触点，为低层控制提供动态感知的视觉参考。

Result: 通过对多种长时序操纵任务的大量实验证明，Super-Mimic在零样本设置下的表现显著优于最先进的对比方法，提升幅度超过20%。

Conclusion: 结合视频驱动的意图解析和前瞻性动力学建模，是实现通用性强、泛化能力高的机器人系统的有效途径。

Abstract: Generalizing to long-horizon manipulation tasks in a zero-shot setting
remains a central challenge in robotics. Current multimodal foundation based
approaches, despite their capabilities, typically fail to decompose high-level
commands into executable action sequences from static visual input alone. To
address this challenge, we introduce Super-Mimic, a hierarchical framework that
enables zero-shot robotic imitation by directly inferring procedural intent
from unscripted human demonstration videos. Our framework is composed of two
sequential modules. First, a Human Intent Translator (HIT) parses the input
video using multimodal reasoning to produce a sequence of language-grounded
subtasks. These subtasks then condition a Future Dynamics Predictor (FDP),
which employs a generative model that synthesizes a physically plausible video
rollout for each step. The resulting visual trajectories are dynamics-aware,
explicitly modeling crucial object interactions and contact points to guide the
low-level controller. We validate this approach through extensive experiments
on a suite of long-horizon manipulation tasks, where Super-Mimic significantly
outperforms state-of-the-art zero-shot methods by over 20\%. These results
establish that coupling video-driven intent parsing with prospective dynamics
modeling is a highly effective strategy for developing general-purpose robotic
systems.

</details>


### [299] [Leveraging Large Language Models for Robot-Assisted Learning of Morphological Structures in Preschool Children with Language Vulnerabilities](https://arxiv.org/abs/2509.22287)
*Stina Sundstedt,Mattias Wingren,Susanne Hägglund,Daniel Ventus*

Main category: cs.RO

TL;DR: 本论文介绍了TalBot项目，通过结合对话机器人与大语言模型，为有语言脆弱性的学前儿童提供游戏化表达能力训练，旨在通过自动化方法辅助语言学习并减轻教育者负担。


<details>
  <summary>Details</summary>
Motivation: 很多有语言障碍或移民背景的学前儿童在表达性语言上有弱势。日常教学和活动对教师与家长的语言学知识和实时应变能力要求很高，尤其在游戏化互动场景中更难平衡教学内容与儿童参与度。为此，作者希望利用机器人与大语言模型设计更高效、科学的语言支持解决方案。

Method: 研究团队开发了基于Furhat对话机器人的应用，让机器人与儿童玩Alias词汇检索游戏。系统利用大语言模型（LLM）来管理对话、情感反馈和轮流操作，并计划进一步通过LLM实现特定形态学目标（如动词变化形式）的实时生成和传递。

Result: 目前，系统已能实现多轮对话和游戏互动，即时管理游戏进程。计划下一步让LLM驱动机器人精确生成、教学多样化形态学结构，并假设机器人在此方面能优于人类教师。

Conclusion: 这种机器人辅助语言学习方法有望作为儿童和教育人员的双重语言模型与辅导工具。未来目标是打造可适用于多语言、多结构的健壮LLM机器人语言干预平台，满足有语言脆弱性的儿童的基本交流与学习需求。

Abstract: Preschool children with language vulnerabilities -- such as developmental
language disorders or immigration related language challenges -- often require
support to strengthen their expressive language skills. Based on the principle
of implicit learning, speech-language therapists (SLTs) typically embed target
morphological structures (e.g., third person -s) into everyday interactions or
game-based learning activities. Educators are recommended by SLTs to do the
same. This approach demands precise linguistic knowledge and real-time
production of various morphological forms (e.g., "Daddy wears these when he
drives to work"). The task becomes even more demanding when educators or parent
also must keep children engaged and manage turn-taking in a game-based
activity. In the TalBot project our multiprofessional team have developed an
application in which the Furhat conversational robot plays the word retrieval
game "Alias" with children to improve language skills. Our application
currently employs a large language model (LLM) to manage gameplay, dialogue,
affective responses, and turn-taking. Our next step is to further leverage the
capacity of LLMs so the robot can generate and deliver specific morphological
targets during the game. We hypothesize that a robot could outperform humans at
this task. Novel aspects of this approach are that the robot could ultimately
serve as a model and tutor for both children and professionals and that using
LLM capabilities in this context would support basic communication needs for
children with language vulnerabilities. Our long-term goal is to create a
robust LLM-based Robot-Assisted Language Learning intervention capable of
teaching a variety of morphological structures across different languages.

</details>


### [300] [IMU-Preintegrated Radar Factors for Asynchronous Radar-LiDAR-Inertial SLAM](https://arxiv.org/abs/2509.22288)
*Johan Hatleskog,Morten Nissov,Kostas Alexis*

Main category: cs.RO

TL;DR: 该论文提出了一种新的雷达-LiDAR-惯导平滑器方法，利用IMU预积分技术减少因不同传感器时间同步问题导致的计算节点数量，从而大幅降低了计算成本，实验验证了在保持精度的情况下优化效率提升。


<details>
  <summary>Details</summary>
Motivation: 传统的固定窗口雷达-LiDAR-惯导平滑器，为了解决雷达与LiDAR之间时间不同步，每个测量都建立一个因子图节点。这导致在传感器采样频率相同情况下，每秒状态数翻倍，显著增加了优化计算量，限制了实时应用。

Method: 作者提出IMU预积分雷达因子，利用惯导（IMU）高频数据，将最新的LiDAR状态外推到雷达测量时刻，这样仅在LiDAR频率下创建节点。相当于合并节点，减少了50%的节点数量与计算复杂度。

Result: 在配置为4核2.2 GHz A73和2 GHz A53，8GB内存的单板计算机上实验，结果表明该方法在保持姿态精度基本不变的情况下，将因子图优化总时间减少了高达56%。

Conclusion: 该方法在不降低定位精度的基础上，有效降低了在资源受限硬件上因子图优化的计算成本，提高了实时性能，适合实际嵌入式场景部署。

Abstract: Fixed-lag Radar-LiDAR-Inertial smoothers conventionally create one factor
graph node per measurement to compensate for the lack of time synchronization
between radar and LiDAR. For a radar-LiDAR sensor pair with equal rates, this
strategy results in a state creation rate of twice the individual sensor
frequencies. This doubling of the number of states per second yields high
optimization costs, inhibiting real-time performance on resource-constrained
hardware. We introduce IMU-preintegrated radar factors that use high-rate
inertial data to propagate the most recent LiDAR state to the radar measurement
timestamp. This strategy maintains the node creation rate at the LiDAR
measurement frequency. Assuming equal sensor rates, this lowers the number of
nodes by 50 % and consequently the computational costs. Experiments on a single
board computer (which has 4 cores each of 2.2 GHz A73 and 2 GHz A53 with 8 GB
RAM) show that our method preserves the absolute pose error of a conventional
baseline while simultaneously lowering the aggregated factor graph optimization
time by up to 56 %.

</details>


### [301] [Beyond Detection -- Orchestrating Human-Robot-Robot Assistance via an Internet of Robotic Things Paradigm](https://arxiv.org/abs/2509.22296)
*Joseph Hunt,Koyo Fujii,Aly Magassouba,Praminda Caleb-Solly*

Main category: cs.RO

TL;DR: 本文提出了一种结合隐私保护热感模型与多机器人协作的IoRT系统，能实时预测病人下床意图并主动响应，从而有效降低住院病人跌倒风险。


<details>
  <summary>Details</summary>
Motivation: 医院内病人跌倒事件频发，带来巨大风险和成本。传统防跌倒系统多为被动报警且误报率高，难以及时满足病人实际需求，如口渴或不适，导致病人擅自下床，增加危险。

Method: 设计了一个IoRT系统，集成了低分辨率热感应器实现隐私保护的下床预测，并利用两台机器人根据预测和病人输入协同动态响应。进行了用户实验和系统误差分析。

Result: 该系统能准确地预判下床行为，机器人可依病人需求提供个性化主动帮助。系统误差分析与用户调研结果为下一步的多机器人、场景感知交互设计提供了依据。

Conclusion: IoRT与多机器人协作系统能突破单一监控，实现主动及时的人性化护理，提升病人安全和护理效率。

Abstract: Hospital patient falls remain a critical and costly challenge worldwide.
While conventional fall prevention systems typically rely on post-fall
detection or reactive alerts, they also often suffer from high false positive
rates and fail to address the underlying patient needs that lead to bed-exit
attempts. This paper presents a novel system architecture that leverages the
Internet of Robotic Things (IoRT) to orchestrate human-robot-robot interaction
for proactive and personalized patient assistance. The system integrates a
privacy-preserving thermal sensing model capable of real-time bed-exit
prediction, with two coordinated robotic agents that respond dynamically based
on predicted intent and patient input. This orchestrated response could not
only reduce fall risk but also attend to the patient's underlying motivations
for movement, such as thirst, discomfort, or the need for assistance, before a
hazardous situation arises. Our contributions with this pilot study are
three-fold: (1) a modular IoRT-based framework enabling distributed sensing,
prediction, and multi-robot coordination; (2) a demonstration of low-resolution
thermal sensing for accurate, privacy-preserving preemptive bed-exit detection;
and (3) results from a user study and systematic error analysis that inform the
design of situationally aware, multi-agent interactions in hospital settings.
The findings highlight how interactive and connected robotic systems can move
beyond passive monitoring to deliver timely, meaningful assistance, empowering
safer, more responsive care environments.

</details>


### [302] [RoboView-Bias: Benchmarking Visual Bias in Embodied Agents for Robotic Manipulation](https://arxiv.org/abs/2509.22356)
*Enguang Liu,Siyuan Liang,Liming Lu,Xiyu Zeng,Xiaochun Cao,Aishan Liu,Shuchao Pang*

Main category: cs.RO

TL;DR: 本文提出了RoboView-Bias，这是首个系统量化机器人操作中视觉偏见的基准，通过结构性生成任务和公平验证协议，揭示了视觉偏见对智能体决策稳定性的重大影响，并提出了有效的偏见缓解策略。


<details>
  <summary>Details</summary>
Motivation: 现有体化智能体的评测基准多关注通用性和鲁棒性，但对视觉偏见缺乏系统量化，这限制了对感知影响决策稳定性的深入理解。

Method: 提出了RoboView-Bias基准，采用结构化的变体生成框架和感知公平性验证协议，系统生成2,127个任务实例，支持量化单一或交互视觉因素引起的偏见，对三类代表性智能体在两大范式下进行了测评。

Result: 主要发现包括：(1)所有体化智能体都存在显著视觉偏见，摄像头视角是最关键因素；(2)所有体化智能体在高饱和度颜色下表现最佳，暗示VLM继承的视觉偏好；(3)视觉偏见受因素耦合影响，视角会大幅增强色彩相关偏见；(4)基于语义锚定层的缓解策略在MOKA上可将视觉偏见减少约54.5%。

Conclusion: 系统分析视觉偏见对于开发安全、可靠的通用体化智能体至关重要，并首次提供了一套量化工具和有效缓解方法。

Abstract: The safety and reliability of embodied agents rely on accurate and unbiased
visual perception. However, existing benchmarks mainly emphasize generalization
and robustness under perturbations, while systematic quantification of visual
bias remains scarce. This gap limits a deeper understanding of how perception
influences decision-making stability. To address this issue, we propose
RoboView-Bias, the first benchmark specifically designed to systematically
quantify visual bias in robotic manipulation, following a principle of factor
isolation. Leveraging a structured variant-generation framework and a
perceptual-fairness validation protocol, we create 2,127 task instances that
enable robust measurement of biases induced by individual visual factors and
their interactions. Using this benchmark, we systematically evaluate three
representative embodied agents across two prevailing paradigms and report three
key findings: (i) all agents exhibit significant visual biases, with camera
viewpoint being the most critical factor; (ii) agents achieve their highest
success rates on highly saturated colors, indicating inherited visual
preferences from underlying VLMs; and (iii) visual biases show strong,
asymmetric coupling, with viewpoint strongly amplifying color-related bias.
Finally, we demonstrate that a mitigation strategy based on a semantic
grounding layer substantially reduces visual bias by approximately 54.5\% on
MOKA. Our results highlight that systematic analysis of visual bias is a
prerequisite for developing safe and reliable general-purpose embodied agents.

</details>


### [303] [Learning-Based Collaborative Control for Bi-Manual Tactile-Reactive Grasping](https://arxiv.org/abs/2509.22421)
*Leonel Giacobbe,Jingdao Chen,Chuangchuang Sun*

Main category: cs.RO

TL;DR: 提出了一种利用触觉传感和多智能体模型预测控制（MPC）的方法，实现对不同软硬度和形状物体的稳健抓取，效果优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有抓取方法主要针对刚性物体，对易碎或可变形物品适应性差，且通常只限于单个机器人，难以实现对大型或重型物体的协同抓取。

Method: 提出基于触觉数据（使用两枚Gelsight Mini传感器获取实时物体纹理和软硬信息）和多智能体MPC的学习型抓取系统。该方法可实时估算接触动力学和物体顺应性，闭环调整抓力和位置，实现多机器人协作抓取。

Result: 系统在真实物体抓取实验中进行测试，分别与单独的PD控制和MPC基线进行了对比。实验结果表明，该方法在抓取成功率和抓取稳定性方面均有明显提升，适应对象涵盖不同尺寸和软硬度。

Conclusion: 结合多智能体MPC与高质量触觉感知，显著拓展了多智能体机器人系统协同抓取复杂物体的能力，为智能协作抓取和柔性物体操控提供了有效方案。

Abstract: Grasping is a core task in robotics with various applications. However, most
current implementations are primarily designed for rigid items, and their
performance drops considerably when handling fragile or deformable materials
that require real-time feedback. Meanwhile, tactile-reactive grasping focuses
on a single agent, which limits their ability to grasp and manipulate large,
heavy objects. To overcome this, we propose a learning-based, tactile-reactive
multi-agent Model Predictive Controller (MPC) for grasping a wide range of
objects with different softness and shapes, beyond the capabilities of
preexisting single-agent implementations. Our system uses two Gelsight Mini
tactile sensors [1] to extract real-time information on object texture and
stiffness. This rich tactile feedback is used to estimate contact dynamics and
object compliance in real time, enabling the system to adapt its control policy
to diverse object geometries and stiffness profiles. The learned controller
operates in a closed loop, leveraging tactile encoding to predict grasp
stability and adjust force and position accordingly. Our key technical
contributions include a multi-agent MPC formulation trained on real contact
interactions, a tactile-data driven method for inferring grasping states, and a
coordination strategy that enables collaborative control. By combining tactile
sensing and a learning-based multi-agent MPC, our method offers a robust,
intelligent solution for collaborative grasping in complex environments,
significantly advancing the capabilities of multi-agent systems. Our approach
is validated through extensive experiments against independent PD and MPC
baselines. Our pipeline outperforms the baselines regarding success rates in
achieving and maintaining stable grasps across objects of varying sizes and
stiffness.

</details>


### [304] [An Ontology for Unified Modeling of Tasks, Actions, Environments, and Capabilities in Personal Service Robotics](https://arxiv.org/abs/2509.22434)
*Margherita Martorana,Francesca Urgese,Ilaria Tiddi,Stefan Schlobach*

Main category: cs.RO

TL;DR: 本文提出了一种名为OntoBOT的机器人本体，统一描述了任务、动作、环境与机器人的能力，并在四种机器人平台上验证了其通用性和知识推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前服务型机器人的任务执行依赖硬件与软件集成，且现有解决方案通常与特定平台紧密耦合，导致互操作性差、难以复用或共享知识。尽管已有本体如SOMA和DOLCE能描述活动与关系，但无法全面捕获环境、动作和机器人能力之间的联系。

Method: 作者提出了OntoBOT本体，将任务、动作、环境、机器人能力等要素统一编码，支持形式化推理。通过设定能力评估问题，分别在TIAGo、HSR、UR3、Stretch四种机器人上测试本体的通用性和推理功能。

Result: 实验结果表明，OntoBOT能够在多个服务机器人中实现上下文感知推理、面向任务的执行和知识共享，表现出良好的通用性和实用价值。

Conclusion: OntoBOT本体有效整合了现有本体的优点，并扩展了其表达力与适用范围，有助于服务机器人系统实现平台间知识共享，提高任务执行的智能性和灵活性。

Abstract: Personal service robots are increasingly used in domestic settings to assist
older adults and people requiring support. Effective operation involves not
only physical interaction but also the ability to interpret dynamic
environments, understand tasks, and choose appropriate actions based on
context. This requires integrating both hardware components (e.g. sensors,
actuators) and software systems capable of reasoning about tasks, environments,
and robot capabilities. Frameworks such as the Robot Operating System (ROS)
provide open-source tools that help connect low-level hardware with
higher-level functionalities. However, real-world deployments remain tightly
coupled to specific platforms. As a result, solutions are often isolated and
hard-coded, limiting interoperability, reusability, and knowledge sharing.
Ontologies and knowledge graphs offer a structured way to represent tasks,
environments, and robot capabilities. Existing ontologies, such as the
Socio-physical Model of Activities (SOMA) and the Descriptive Ontology for
Linguistic and Cognitive Engineering (DOLCE), provide models for activities,
spatial relationships, and reasoning structures. However, they often focus on
specific domains and do not fully capture the connection between environment,
action, robot capabilities, and system-level integration. In this work, we
propose the Ontology for roBOts and acTions (OntoBOT), which extends existing
ontologies to provide a unified representation of tasks, actions, environments,
and capabilities. Our contributions are twofold: (1) we unify these aspects
into a cohesive ontology to support formal reasoning about task execution, and
(2) we demonstrate its generalizability by evaluating competency questions
across four embodied agents - TIAGo, HSR, UR3, and Stretch - showing how
OntoBOT enables context-aware reasoning, task-oriented execution, and knowledge
sharing in service robotics.

</details>


### [305] [UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation](https://arxiv.org/abs/2509.22441)
*Zhangyuan Wang,Yunpeng Zhu,Yuqi Yan,Xiaoyuan Tian,Xinhao Shao,Meixuan Li,Weikun Li,Guangsheng Su,Weicheng Cui,Dixia Fan*

Main category: cs.RO

TL;DR: 本文提出了UnderwaterVLA，一个集多模态基础模型与智能体为一体的水下自主导航框架，具有高适应性和解释能力，并在实地测试中超越传统方案。


<details>
  <summary>Details</summary>
Motivation: 当前水下操作受限于水动力扰动、通信带宽低和感知退化难题，传统方法泛化能力有限，需求更具通用性、可解释性和强抗扰的自主系统。

Method: 1. 双脑架构：高层任务规划与低层反应控制解耦，适应通信和算力受限环境。
2. 首次将视觉-语言-动作（VLA）模型引入水下机器人，实现结构化链式推理。
3. 融合流体动力学的模型预测控制（MPC），实时补偿水流影响，无需昂贵特定任务训练。

Result: 实地测试显示，在视觉退化环境下显著降低导航误差，任务完成率较基线提升19%~27%。

Conclusion: UnderwaterVLA无需大量专门水下训练数据，适应多环境，具备良好可扩展性和成本效益，为智能自主水下航行器（AUV）提供了新的发展方向。

Abstract: This paper presents UnderwaterVLA, a novel framework for autonomous
underwater navigation that integrates multimodal foundation models with
embodied intelligence systems. Underwater operations remain difficult due to
hydrodynamic disturbances, limited communication bandwidth, and degraded
sensing in turbid waters. To address these challenges, we introduce three
innovations. First, a dual-brain architecture decouples high-level mission
reasoning from low-level reactive control, enabling robust operation under
communication and computational constraints. Second, we apply
Vision-Language-Action(VLA) models to underwater robotics for the first time,
incorporating structured chain-of-thought reasoning for interpretable
decision-making. Third, a hydrodynamics-informed Model Predictive Control(MPC)
scheme compensates for fluid effects in real time without costly task-specific
training. Experimental results in field tests show that UnderwaterVLA reduces
navigation errors in degraded visual conditions while maintaining higher task
completion by 19% to 27% over baseline. By minimizing reliance on
underwater-specific training data and improving adaptability across
environments, UnderwaterVLA provides a scalable and cost-effective path toward
the next generation of intelligent AUVs.

</details>


### [306] [Uncertainty-Aware Multi-Robot Task Allocation With Strongly Coupled Inter-Robot Rewards](https://arxiv.org/abs/2509.22469)
*Ben Rossano,Jaein Lim,Jonathan P. How*

Main category: cs.RO

TL;DR: 提出了一种在任务需求不确定的情况下，针对异构机器人团队的任务分配算法，能够高效协作并主动降低任务失败率。


<details>
  <summary>Details</summary>
Motivation: 异构机器人团队在实际应用中常面临任务需求不确定性，而如何分配任务以最大化整体效率、避免资源浪费，同时规避任务失败，是一个具有挑战性的研究问题。

Method: 通过将任务需求建模为能力概率分布，借助市场机制，优化团队整体目标，显式考虑机器人之间奖励的耦合关系，并设计了在严格通信条件下可行的多项式时间去中心化方案。

Result: 与基准算法对比实验验证了算法的有效性，并指出去中心化情况下处理奖励耦合的挑战。

Conclusion: 该方法能有效提升异构机器人团队在任务需求不确定环境下的协作效率和鲁棒性，同时为奖励耦合下的去中心化任务分配提供了新思路。

Abstract: This paper proposes a task allocation algorithm for teams of heterogeneous
robots in environments with uncertain task requirements. We model these
requirements as probability distributions over capabilities and use this model
to allocate tasks such that robots with complementary skills naturally position
near uncertain tasks, proactively mitigating task failures without wasting
resources. We introduce a market-based approach that optimizes the joint team
objective while explicitly capturing coupled rewards between robots, offering a
polynomial-time solution in decentralized settings with strict communication
assumptions. Comparative experiments against benchmark algorithms demonstrate
the effectiveness of our approach and highlight the challenges of incorporating
coupled rewards in a decentralized formulation.

</details>


### [307] [Ontological foundations for contrastive explanatory narration of robot plans](https://arxiv.org/abs/2509.22493)
*Alberto Olivares-Alarcos,Sergi Foix,Júlia Borràs,Gerard Canal,Guillem Alenyà*

Main category: cs.RO

TL;DR: 该论文提出了一种新颖的本体模型和算法，用于比较和解释机器人面对两种竞选方案时的决策差异，以提升人机交互中的透明度和信任。


<details>
  <summary>Details</summary>
Motivation: 当前人机交互中，理解和信任机器人的决策至关重要。现有方法在如何清晰比较不同方案，以及向人类解释为何选择某一方案方面存在不足，尤其缺少能有效构建对比性解释的机制。

Method: 作者设计了一种新的本体论模型来形式化和推理两种方案之间的差别，同时提出了一种新算法，利用方案间的异质知识，生成有对比性的解释叙述，并与已有的本体论解释算法进行了对比。

Result: 实验结果显示，该新算法在构建对比性叙述和向人类解释机器人决策理由方面，优于现有的基线方法。

Conclusion: 本文的新模型和算法提升了机器人对多方案决策差异的解释能力，为构建值得信赖的人机交互打下基础。

Abstract: Mutual understanding of artificial agents' decisions is key to ensuring a
trustworthy and successful human-robot interaction. Hence, robots are expected
to make reasonable decisions and communicate them to humans when needed. In
this article, the focus is on an approach to modeling and reasoning about the
comparison of two competing plans, so that robots can later explain the
divergent result. First, a novel ontological model is proposed to formalize and
reason about the differences between competing plans, enabling the
classification of the most appropriate one (e.g., the shortest, the safest, the
closest to human preferences, etc.). This work also investigates the
limitations of a baseline algorithm for ontology-based explanatory narration.
To address these limitations, a novel algorithm is presented, leveraging
divergent knowledge between plans and facilitating the construction of
contrastive narratives. Through empirical evaluation, it is observed that the
explanations excel beyond the baseline method.

</details>


### [308] [HELIOS: Hierarchical Exploration for Language-grounded Interaction in Open Scenes](https://arxiv.org/abs/2509.22498)
*Katrina Ashton,Chahyon Ku,Shrey Shah,Wen Jiang,Kostas Daniilidis,Bernadette Bucher*

Main category: cs.RO

TL;DR: 本文提出HELIOS，一种用于语言指定移动操作任务的分层场景表示和搜索方法，在Habitat仿真和真实环境中实现了当前最优的目标检测和任务执行效果。


<details>
  <summary>Details</summary>
Motivation: 现有移动操作系统在新环境中按语言指令完成取放任务时，面临场景感知不完整、语言与视觉信息结合困难、需要主动获取场景知识等挑战。本文旨在提升系统对部分可观环境的理解及任务执行效率。

Method: 提出HELIOS方法，构建融合2D语义地图（含导航相关的语义与占据信息）及3D高斯对象表示的层次场景表示，并结合多视角一致性建模，实现对对象的准确感知。提出新的搜索目标函数，平衡对未知或不确定区域的探索和对已知语义区域的利用。

Result: 在Habitat仿真环境下的OVMM基准数据集上，HELIOS在复杂大场景和小目标感知场景中取得了当前最优效果。同时，HELIOS具备零样本泛化能力，能够不依赖新数据迁移到现实世界（如Spot机器人在办公室环境演示）。

Conclusion: HELIOS有效提升了移动操作系统在部分可观场景中根据语言指令对任务相关对象的感知与操作能力，具有仿真和现实环境下的良好表现与迁移能力。

Abstract: Language-specified mobile manipulation tasks in novel environments
simultaneously face challenges interacting with a scene which is only partially
observed, grounding semantic information from language instructions to the
partially observed scene, and actively updating knowledge of the scene with new
observations. To address these challenges, we propose HELIOS, a hierarchical
scene representation and associated search objective to perform language
specified pick and place mobile manipulation tasks. We construct 2D maps
containing the relevant semantic and occupancy information for navigation while
simultaneously actively constructing 3D Gaussian representations of
task-relevant objects. We fuse observations across this multi-layered
representation while explicitly modeling the multi-view consistency of the
detections of each object. In order to efficiently search for the target
object, we formulate an objective function balancing exploration of unobserved
or uncertain regions with exploitation of scene semantic information. We
evaluate HELIOS on the OVMM benchmark in the Habitat simulator, a pick and
place benchmark in which perception is challenging due to large and complex
scenes with comparatively small target objects. HELIOS achieves
state-of-the-art results on OVMM. As our approach is zero-shot, HELIOS can also
transfer to the real world without requiring additional data, as we illustrate
by demonstrating it in a real world office environment on a Spot robot.

</details>


### [309] [An Intention-driven Lane Change Framework Considering Heterogeneous Dynamic Cooperation in Mixed-traffic Environment](https://arxiv.org/abs/2509.22550)
*Xiaoyun Qiu,Haichao Liu,Yue Pan,Jun Ma,Xinhu Zheng*

Main category: cs.RO

TL;DR: 本论文提出了一种面向意图的换道框架，通过识别不同驾驶风格、合作意愿，结合深度学习和逆强化学习，实现了更安全高效的自动驾驶车辆换道。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶与人工驾驶车辆混合交通环境中，驾驶员行为异质性及不可预测性使得安全高效的换道变得极具挑战；现有方法往往假设交互行为单一，忽视了真实环境中驾驶员合作意愿与驾驶风格的多样性。

Method: 提出集成驾驶风格识别、合作意愿量化的换道决策与轨迹规划框架。方法包括：1）基于NGSIM数据集训练的深度学习分类器实时识别周围车辆驾驶风格；2）提出具有内在与互动成分的合作分数，量化周围车辆合作意愿；3）结合行为克隆与逆强化学习实现换道决策；4）基于模型预测控制与意图推断生成安全、社会接受的轨迹。

Result: 实验中，模型在换道识别上达到94.2%的准确率和94.3%的F1分数，分别比规则方法和已有学习方法高出4-15%。

Conclusion: 建模司机异质性可显著提升自动驾驶系统对复杂交通情境的适应能力，提出的框架有望推动更人性化和情境感知的自动驾驶换道技术发展。

Abstract: In mixed-traffic environments, where autonomous vehicles (AVs) interact with
diverse human-driven vehicles (HVs), unpredictable intentions and heterogeneous
behaviors make safe and efficient lane change maneuvers highly challenging.
Existing methods often oversimplify these interactions by assuming uniform
patterns. We propose an intention-driven lane change framework that integrates
driving-style recognition, cooperation-aware decision-making, and coordinated
motion planning. A deep learning classifier trained on the NGSIM dataset
identifies human driving styles in real time. A cooperation score with
intrinsic and interactive components estimates surrounding drivers' intentions
and quantifies their willingness to cooperate with the ego vehicle.
Decision-making combines behavior cloning with inverse reinforcement learning
to determine whether a lane change should be initiated. For trajectory
generation, model predictive control is integrated with IRL-based intention
inference to produce collision-free and socially compliant maneuvers.
Experiments show that the proposed model achieves 94.2\% accuracy and 94.3\%
F1-score, outperforming rule-based and learning-based baselines by 4-15\% in
lane change recognition. These results highlight the benefit of modeling
inter-driver heterogeneity and demonstrate the potential of the framework to
advance context-aware and human-like autonomous driving in complex traffic
environments.

</details>


### [310] [MINT-RVAE: Multi-Cues Intention Prediction of Human-Robot Interaction using Human Pose and Emotion Information from RGB-only Camera Data](https://arxiv.org/abs/2509.22573)
*Farida Mohsen,Ali Safa*

Main category: cs.RO

TL;DR: 本文提出了一种只需RGB输入、基于深度学习的人机交互意图检测方法，具有帧级精度，并通过生成合成序列和新损失函数有效解决类别不平衡问题，最终在公开任务上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统人机交互意图检测大多依赖多模态数据（如RGB-D），而现实应用中获得多模态数据不便，同时数据不平衡问题影响预测效果。因此，本文致力于开发更简便且鲁棒的意图检测方法。

Method: 作者提出了只需RGB输入、拥有帧级预测精度的检测框架，并设计了MINT-RVAE合成序列生成方法及新型损失函数和训练策略，以提升模型在数据不平衡场景下的泛化能力。

Result: 该方法在AUROC指标上获得0.95的优异成绩，显著优于此前方法（0.90-0.912），并且仅需单一RGB输入即可实现高精度、帧级的人机交互意图检测。

Conclusion: 本文方法有效提升了人机交互意图检测的灵敏度和应用便捷性，有助于增强机器人服务水平，同时公开了新数据集以促进后续研究。

Abstract: Efficiently detecting human intent to interact with ubiquitous robots is
crucial for effective human-robot interaction (HRI) and collaboration. Over the
past decade, deep learning has gained traction in this field, with most
existing approaches relying on multimodal inputs, such as RGB combined with
depth (RGB-D), to classify time-sequence windows of sensory data as interactive
or non-interactive. In contrast, we propose a novel RGB-only pipeline for
predicting human interaction intent with frame-level precision, enabling faster
robot responses and improved service quality. A key challenge in intent
prediction is the class imbalance inherent in real-world HRI datasets, which
can hinder the model's training and generalization. To address this, we
introduce MINT-RVAE, a synthetic sequence generation method, along with new
loss functions and training strategies that enhance generalization on
out-of-sample data. Our approach achieves state-of-the-art performance (AUROC:
0.95) outperforming prior works (AUROC: 0.90-0.912), while requiring only RGB
input and supporting precise frame onset prediction. Finally, to support future
research, we openly release our new dataset with frame-level labeling of human
interaction intent.

</details>


### [311] [EgoDemoGen: Novel Egocentric Demonstration Generation Enables Viewpoint-Robust Manipulation](https://arxiv.org/abs/2509.22578)
*Yuan Xu,Jiabing Yang,Xiaofeng Wang,Yixiang Chen,Zheng Zhu,Bowen Fang,Guan Huang,Xinze Chen,Yun Ye,Qiang Zhang,Peiyan Li,Xiangnan Wu,Kai Wang,Bing Zhan,Shuo Lu,Jing Liu,Nianfeng Liu,Yan Huang,Liang Wang*

Main category: cs.RO

TL;DR: 本文提出了EgoDemoGen，一个能生成新颖自我视角（egocentric viewpoint）演示数据的系统，显著提升了机器人操作策略在不同自我视角下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习策略在机器人操作任务中，若仅从单一自我视角训练，往往在视角变化时表现下降，限制了实际应用。为增强策略对视角变化的适应能力，亟需能有效扩充和多样化自我视角演示数据的方法。

Method: 提出EgoDemoGen框架，创新性地通过动作重定向和视频生成技术，自动合成成对的新颖自我视角演示。其核心组件EgoViewTransfer基于预训练视频生成模型，通过自监督的双重重投影策略进行微调，用于生成与重定向动作相结合的新视角观测视频。

Result: 在仿真（RoboTwin2.0）和真实机器人实验中，将EgoDemoGen生成的新自我视角演示与原始演示混合训练，标准与新颖视角的策略成功率分别绝对提升17.0%/17.7%（仿真）与18.3%/25.8%（真实机器人）。随EgoDemoGen数据占比提升，性能持续增强但回报递减。

Conclusion: EgoDemoGen能够有效提升机器人操作策略面对不同自我视角时的鲁棒性，为实际机器人系统在更多场景下的适应能力提供了可行路径。

Abstract: Imitation learning based policies perform well in robotic manipulation, but
they often degrade under *egocentric viewpoint shifts* when trained from a
single egocentric viewpoint. To address this issue, we present **EgoDemoGen**,
a framework that generates *paired* novel egocentric demonstrations by
retargeting actions in the novel egocentric frame and synthesizing the
corresponding egocentric observation videos with proposed generative video
repair model **EgoViewTransfer**, which is conditioned by a novel-viewpoint
reprojected scene video and a robot-only video rendered from the retargeted
joint actions. EgoViewTransfer is finetuned from a pretrained video generation
model using self-supervised double reprojection strategy. We evaluate
EgoDemoGen on both simulation (RoboTwin2.0) and real-world robot. After
training with a mixture of EgoDemoGen-generated novel egocentric demonstrations
and original standard egocentric demonstrations, policy success rate improves
**absolutely** by **+17.0%** for standard egocentric viewpoint and by
**+17.7%** for novel egocentric viewpoints in simulation. On real-world robot,
the **absolute** improvements are **+18.3%** and **+25.8%**. Moreover,
performance continues to improve as the proportion of EgoDemoGen-generated
demonstrations increases, with diminishing returns. These results demonstrate
that EgoDemoGen provides a practical route to egocentric viewpoint-robust
robotic manipulation.

</details>


### [312] [WoW: Towards a World omniscient World model Through Embodied Interaction](https://arxiv.org/abs/2509.22642)
*Xiaowei Chi,Peidong Jia,Chun-Kai Fan,Xiaozhu Ju,Weishi Mi,Kevin Zhang,Zhiyuan Qin,Wanxin Tian,Kuangzhi Ge,Hao Li,Zezhong Qian,Anthony Chen,Qiang Zhou,Yueru Jia,Jiaming Liu,Yong Dai,Qingpo Wuwu,Chengyu Bai,Yu-Kai Wang,Ying Li,Lizhang Chen,Yong Bao,Zhiyuan Jiang,Jiacheng Zhu,Kai Tang,Ruichuan An,Yulin Luo,Qiuxuan Feng,Siyuan Zhou,Chi-min Chan,Chengkai Hou,Wei Xue,Sirui Han,Yike Guo,Shanghang Zhang,Jian Tang*

Main category: cs.RO

TL;DR: 本文提出WoW，一个通过大规模机器人真实交互数据训练的生成式世界模型，强调与现有仅依赖被动观察的视频模型不同，主动丰富的物理交互是AI获得物理直觉的关键。WoW结合SOPHIA机制提升物理一致性，并通过新基准WoWBench证明其在物理因果推理等任务上的先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前如Sora等视频模型主要依赖被动观察，难以捕捉物理因果关系和具备真正的物理直觉。人类习得物理直觉依赖于主动的实体交互。因此，作者希望通过大规模真实机器人交互，探索AI获得物理直觉的有效路径。

Method: 作者训练了一个拥有140亿参数的生成式模型WoW，以200万条机器人交互轨迹为基础，模拟和推断物理事件。引入SOPHIA模块，利用视觉-语言模型动态评价和调整生成内容，增强调节物理合理性。并通过联合训练逆动力学模型，实现从模型“想象”到机器人实际动作的闭环。最后，提出WoWBench基准评价模型对物理一致性和因果推理的能力。

Result: 实验结果显示，WoW在WoWBench基准上无论是人类评价还是自动评测都取得了最优成绩，在物理因果推理、碰撞动力学、物体持久性等方面表现突出。模型展现出对物理规律的概率分布式理解，可通过SOPHIA进一步收敛到更真实的结果。

Conclusion: 该研究系统性证明了大规模真实物理交互对AI物理直觉建构至关重要。WoW在物理一致性和因果推理方面树立了新基准，模型、数据和基准集将开源，推动领域发展。

Abstract: Humans develop an understanding of intuitive physics through active
interaction with the world. This approach is in stark contrast to current video
models, such as Sora, which rely on passive observation and therefore struggle
with grasping physical causality. This observation leads to our central
hypothesis: authentic physical intuition of the world model must be grounded in
extensive, causally rich interactions with the real world. To test this
hypothesis, we present WoW, a 14-billion-parameter generative world model
trained on 2 million robot interaction trajectories. Our findings reveal that
the model's understanding of physics is a probabilistic distribution of
plausible outcomes, leading to stochastic instabilities and physical
hallucinations. Furthermore, we demonstrate that this emergent capability can
be actively constrained toward physical realism by SOPHIA, where
vision-language model agents evaluate the DiT-generated output and guide its
refinement by iteratively evolving the language instructions. In addition, a
co-trained Inverse Dynamics Model translates these refined plans into
executable robotic actions, thus closing the imagination-to-action loop. We
establish WoWBench, a new benchmark focused on physical consistency and causal
reasoning in video, where WoW achieves state-of-the-art performance in both
human and autonomous evaluation, demonstrating strong ability in physical
causality, collision dynamics, and object permanence. Our work provides
systematic evidence that large-scale, real-world interaction is a cornerstone
for developing physical intuition in AI. Models, data, and benchmarks will be
open-sourced.

</details>


### [313] [VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search](https://arxiv.org/abs/2509.22643)
*Wenkai Guo,Guanxing Lu,Haoyuan Deng,Zhenyu Wu,Yansong Tang,Ziwei Wang*

Main category: cs.RO

TL;DR: 本文提出了一种新的插件式框架VLA-Reasoner，使现有视觉-语言-动作（VLA）模型具备前瞻性，能更好地处理长期规划任务，并利用世界模型、蒙特卡罗树搜索（MCTS）和置信度采样机制提升机器人的自主操作能力。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型在机器人操作中只能预测短期动作，面对需要长远规划的任务时容易出现累积偏差，难以实现稳健的长程操作。为提升VLA模型应对长序列操作的能力，需要增强其对未来状态的预判与推理能力。

Method: 作者提出VLA-Reasoner框架，将世界模型与VLA结合，通过对可能动作轨迹的采样与推演，预测一系列未来状态，并基于结果推理最优动作。为应对大动作空间，提高搜索效率，引入了MCTS；同时利用核密度估计（KDE）实现高效置信度采样，减少冗余推理；并通过离线奖励塑形对中间状态进行评分，实现长期反馈的纠偏。

Result: 在多个仿真环境和实际机器人操作任务中，VLA-Reasoner框架在处理长时间跨度和复杂轨迹推理任务上均显著优于当前最先进的VLAs。

Conclusion: VLA-Reasoner为扩展机器人在实际环境中复杂任务的执行能力提供了一条可扩展的思路，显著提升了现有VLA模型在需长远规划任务中的表现。

Abstract: Vision-Language-Action models (VLAs) achieve strong performance in general
robotic manipulation tasks by scaling imitation learning. However, existing
VLAs are limited to predicting short-sighted next-action, which struggle with
long-horizon trajectory tasks due to incremental deviations. To address this
problem, we propose a plug-in framework named VLA-Reasoner that effectively
empowers off-the-shelf VLAs with the capability of foreseeing future states via
test-time scaling. Specifically, VLA-Reasoner samples and rolls out possible
action trajectories where involved actions are rationales to generate future
states via a world model, which enables VLA-Reasoner to foresee and reason
potential outcomes and search for the optimal actions. We further leverage
Monte Carlo Tree Search (MCTS) to improve search efficiency in large action
spaces, where stepwise VLA predictions seed the root. Meanwhile, we introduce a
confidence sampling mechanism based on Kernel Density Estimation (KDE), to
enable efficient exploration in MCTS without redundant VLA queries. We evaluate
intermediate states in MCTS via an offline reward shaping strategy, to score
predicted futures and correct deviations with long-term feedback. We conducted
extensive experiments in both simulators and the real world, demonstrating that
our proposed VLA-Reasoner achieves significant improvements over the
state-of-the-art VLAs. Our method highlights a potential pathway toward
scalable test-time computation of robotic manipulation.

</details>


### [314] [Pixel Motion Diffusion is What We Need for Robot Control](https://arxiv.org/abs/2509.22652)
*E-Ro Nguyen,Yichi Zhang,Kanchana Ranasinghe,Xiang Li,Michael S. Ryoo*

Main category: cs.RO

TL;DR: 该论文提出了DAWN，一种基于扩散模型的统一机器人控制框架，通过像素级运动表示衔接高层意图和低层动作，实现端到端的可训练系统，并在多个基准数据集上取得优秀成绩，支持现实世界的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操控方法难以在连接高层指令与低层动作时兼顾灵活性与可解释性，且与现实世界适应能力弱。因此，作者希望构建一种统一且通用的方法，能提升多任务处理和跨环境迁移能力。

Method: 该方法将高层控制与低层控制均建模为扩散过程，通过结构化的像素运动表示连接语言输入和具体机器人动作。整个系统实现端到端训练，并引入可解释的中间运动抽象。

Result: 在CALVIN基准测试中取得了多任务新SOTA表现，并在MetaWorld环境进一步验证有效性。即便面对现实与仿真间较大域差距，仅需少量微调便能实现现实场景下的可靠迁移。

Conclusion: 扩散建模结合运动中心表示为大规模、鲁棒机器人学习任务提供了强有力的基线，DAWN验证了扩散运动抽象实际应用的可行性和有效性。

Abstract: We present DAWN (Diffusion is All We Need for robot control), a unified
diffusion-based framework for language-conditioned robotic manipulation that
bridges high-level motion intent and low-level robot action via structured
pixel motion representation. In DAWN, both the high-level and low-level
controllers are modeled as diffusion processes, yielding a fully trainable,
end-to-end system with interpretable intermediate motion abstractions. DAWN
achieves state-of-the-art results on the challenging CALVIN benchmark,
demonstrating strong multi-task performance, and further validates its
effectiveness on MetaWorld. Despite the substantial domain gap between
simulation and reality and limited real-world data, we demonstrate reliable
real-world transfer with only minimal finetuning, illustrating the practical
viability of diffusion-based motion abstractions for robotic control. Our
results show the effectiveness of combining diffusion modeling with
motion-centric representations as a strong baseline for scalable and robust
robot learning. Project page: https://nero1342.github.io/DAWN/

</details>


### [315] [See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation](https://arxiv.org/abs/2509.22653)
*Chih Yao Hu,Yang-Sen Lin,Yuna Lee,Chih-Hai Su,Jie-Ying Lee,Shr-Ruei Tsai,Chin-Yang Lin,Kuan-Wen Chen,Tsung-Wei Ke,Yu-Lun Liu*

Main category: cs.RO

TL;DR: 提出了一种无需训练、基于视觉-语言模型的无人机导航方法SPF（See, Point, Fly），在模拟和真实环境中效果拔群。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-语言模型(VLM)的无人机导航方法通常将动作预测视为文本生成问题，难以高效执行各种自由形式指令并适应动态环境。该论文动机是提出更通用、高效的导航框架，让无人机更智能地理解和执行语言指令。

Method: SPF将无人机导航中的动作预测任务转化为2D空间定位问题——VLM用来自然语言描述的信息，迭代地在输入图像上标注2D航点，然后结合预计行进距离将这些航点转化为3D位移向量，作为无人机行进指令。SPF能自适应调整前进的距离，并采用闭环控制方式以提升导航效率，适用于多样化环境和动态目标追踪。关键特点包括端到端无需训练，直接利用现有VLM。

Result: 在基于深度强化学习（DRL）的模拟基准测试中，SPF性能达到最新最好水平，比前SOTA高63%；真实世界测试亦大幅领先现有方法。消融实验进一步验证了方法各部分设计的有效性，兼容多种主流VLM。

Conclusion: SPF大幅提升了无人机在各种环境下根据语言指令导航的能力，不仅性能优越，泛化性强，还为无需训练的智能体导航开辟了新方向。

Abstract: We present See, Point, Fly (SPF), a training-free aerial vision-and-language
navigation (AVLN) framework built atop vision-language models (VLMs). SPF is
capable of navigating to any goal based on any type of free-form instructions
in any kind of environment. In contrast to existing VLM-based approaches that
treat action prediction as a text generation task, our key insight is to
consider action prediction for AVLN as a 2D spatial grounding task. SPF
harnesses VLMs to decompose vague language instructions into iterative
annotation of 2D waypoints on the input image. Along with the predicted
traveling distance, SPF transforms predicted 2D waypoints into 3D displacement
vectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the
traveling distance to facilitate more efficient navigation. Notably, SPF
performs navigation in a closed-loop control manner, enabling UAVs to follow
dynamic targets in dynamic environments. SPF sets a new state of the art in DRL
simulation benchmark, outperforming the previous best method by an absolute
margin of 63%. In extensive real-world evaluations, SPF outperforms strong
baselines by a large margin. We also conduct comprehensive ablation studies to
highlight the effectiveness of our design choice. Lastly, SPF shows remarkable
generalization to different VLMs. Project page: https://spf-web.pages.dev

</details>
