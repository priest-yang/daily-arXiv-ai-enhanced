<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 64]
- [cs.CL](#cs.CL) [Total: 52]
- [cs.RO](#cs.RO) [Total: 22]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Australian Supermarket Object Set (ASOS): A Benchmark Dataset of Physical Objects and 3D Models for Robotics and Computer Vision](https://arxiv.org/abs/2509.09720)
*Akansel Cosgun,Lachlan Chumbley,Benjamin J. Meyer*

Main category: cs.CV

TL;DR: 该论文提出了一个名为ASOS的超市物品三维模型数据集，包含50种普通日常用品的高质量三维纹理网格，用于机器人和计算机视觉基准测试。


<details>
  <summary>Details</summary>
Motivation: 目前现有数据集多依赖合成模型或特殊物体，获取和应用受限，缺乏真实且易获得的常见物品三维数据集，限制了机器人视觉等应用的真实环境评测。

Method: ASOS收录了澳大利亚大型连锁超市可购的10大类共50种商品，覆盖多样形状、尺寸和重量。使用结构光-运动技术结合高分辨率成像，生成封闭高质量三维纹理网格。

Result: 获得了一个具广泛代表性的高质量三维日用品数据集，物品易于采购和复现，真实反映实际应用场景。

Conclusion: ASOS数据集提升了基准测试的现实意义，对目标检测、姿态估计和机器人应用具有价值，为相关研究和开发提供了便利和标准。

Abstract: This paper introduces the Australian Supermarket Object Set (ASOS), a
comprehensive dataset comprising 50 readily available supermarket items with
high-quality 3D textured meshes designed for benchmarking in robotics and
computer vision applications. Unlike existing datasets that rely on synthetic
models or specialized objects with limited accessibility, ASOS provides a
cost-effective collection of common household items that can be sourced from a
major Australian supermarket chain. The dataset spans 10 distinct categories
with diverse shapes, sizes, and weights. 3D meshes are acquired by a
structure-from-motion techniques with high-resolution imaging to generate
watertight meshes. The dataset's emphasis on accessibility and real-world
applicability makes it valuable for benchmarking object detection, pose
estimation, and robotics applications.

</details>


### [2] [A Multimodal RAG Framework for Housing Damage Assessment: Collaborative Optimization of Image Encoding and Policy Vector Retrieval](https://arxiv.org/abs/2509.09721)
*Jiayi Miao,Dingxin Lu,Zhuqi Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态检索增强生成（MM-RAG）框架，用于灾后房屋损毁评估，在检索和分类上表现优异。


<details>
  <summary>Details</summary>
Motivation: 灾后房屋损毁评估对于保险理赔和资源规划至关重要，现有方法在处理图像与文本信息的融合上存在不足。

Method: 该方法在传统RAG架构基础上，加入了由ResNet和Transformer组成的视觉编码器和用于文本向量化及索引构建的BERT检索器，实现了图像和文本的双分支编码。模型通过跨模态交互模块进行语义对齐，并通过模态注意力门控机制动态调整生成过程中视觉证据与文本先验信息的贡献。整个框架进行端到端训练，联合对比损失、检索损失和生成损失，实现在协同学习下的图像理解和策略匹配。

Result: 实验结果显示，模型在损毁程度分类和检索精度上优于现有方法，Top-1检索准确率提升了9.6%。

Conclusion: 多模态、端到端、任务协同学习的结构显著提升了灾后评估的准确性和检索效率，在相关领域具有良好的应用前景。

Abstract: After natural disasters, accurate evaluations of damage to housing are
important for insurance claims response and planning of resources. In this
work, we introduce a novel multimodal retrieval-augmented generation (MM-RAG)
framework. On top of classical RAG architecture, we further the framework to
devise a two-branch multimodal encoder structure that the image branch employs
a visual encoder composed of ResNet and Transformer to extract the
characteristic of building damage after disaster, and the text branch harnesses
a BERT retriever for the text vectorization of posts as well as insurance
policies and for the construction of a retrievable restoration index. To impose
cross-modal semantic alignment, the model integrates a cross-modal interaction
module to bridge the semantic representation between image and text via
multi-head attention. Meanwhile, in the generation module, the introduced modal
attention gating mechanism dynamically controls the role of visual evidence and
text prior information during generation. The entire framework takes end-to-end
training, and combines the comparison loss, the retrieval loss and the
generation loss to form multi-task optimization objectives, and achieves image
understanding and policy matching in collaborative learning. The results
demonstrate superior performance in retrieval accuracy and classification index
on damage severity, where the Top-1 retrieval accuracy has been improved by
9.6%.

</details>


### [3] [Improving MLLM Historical Record Extraction with Test-Time Image](https://arxiv.org/abs/2509.09722)
*Taylor Archibald,Tony Martinez*

Main category: cs.CV

TL;DR: 本论文提出了一种新的集成框架，通过对历史噪声文档图像进行多种增强处理后，使用大语言模型（如Gemini 2.0 Flash）自动转录，并借助自定义对齐算法融合结果，提高文本提取的稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 历史文档存在极大噪声，自动化文本提取难以确保准确和可靠，目前主流大语言模型单次推理表现不稳定，需要提高转录结果稳定性和置信度。

Method: 将每张文档图片进行多种图像增强，分别送入LLM转录，随后利用自定义的Needleman Wunsch风格对齐算法整合多个转录结果，获得最终的共识转录和信心分数。

Result: 在622份宾夕法尼亚死亡记录数据集上，相较于单模型单次推理，集成框架的转录准确度提升了4个百分点；在增强方式中，填充和模糊有助于提升准确率，网格扭曲更适合区分高低信心的案例。

Conclusion: 该方法简单、可扩展，能够直接应用于其他文档集和转录模型，显著提升了历史文档自动转录的鲁棒性和可靠性。

Abstract: We present a novel ensemble framework that stabilizes LLM based text
extraction from noisy historical documents. We transcribe multiple augmented
variants of each image with Gemini 2.0 Flash and fuse these outputs with a
custom Needleman Wunsch style aligner that yields both a consensus
transcription and a confidence score. We present a new dataset of 622
Pennsylvania death records, and demonstrate our method improves transcription
accuracy by 4 percentage points relative to a single shot baseline. We find
that padding and blurring are the most useful for improving accuracy, while
grid warp perturbations are best for separating high and low confidence cases.
The approach is simple, scalable, and immediately deployable to other document
collections and transcription models.

</details>


### [4] [MITS: A Large-Scale Multimodal Benchmark Dataset for Intelligent Traffic Surveillance](https://arxiv.org/abs/2509.09730)
*Kaikai Zhao,Zhaoxiang Liu,Peng Wang,Xin Wang,Zhicheng Ma,Yajun Xu,Wenjing Zhang,Yibing Nan,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: 该论文提出了首个专为智能交通监控领域设计的大规模多模态数据集（MITS），并证明其能大幅提升主流多模态大模型在交通监控任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前通用型多模态大模型在图像-文本类任务上已取得突破，但受限于缺乏专用数据集，在智能交通监控（ITS）领域表现有限。为推动ITS领域发展，需要专门的大规模多模态数据资源。

Method: 作者构建了MITS数据集，包含17万多张来自真实交通摄像头的图像，细致标注了24种ITS对象和事件类别。并系统生成高质量图像描述及500万条指令式视觉QA对，覆盖目标识别、计数、定位、背景分析、事件推理等关键任务。

Result: 实验证明，用MITS微调主流LMM（如LLaVA-1.5/1.6、Qwen2-VL/2.5-VL）后，在ITS相关任务上性能提升显著。例如LLaVA-1.5的指标由0.494提升到0.905，Qwen2.5-VL由0.732提升到0.930。

Conclusion: MITS填补了交通监控多模态数据集的空白，极大提升了大模型在该领域的应用效果。该数据集与代码、模型已开源，将助力智能交通和多模态AI领域的研究与发展。

Abstract: General-domain large multimodal models (LMMs) have achieved significant
advances in various image-text tasks. However, their performance in the
Intelligent Traffic Surveillance (ITS) domain remains limited due to the
absence of dedicated multimodal datasets. To address this gap, we introduce
MITS (Multimodal Intelligent Traffic Surveillance), the first large-scale
multimodal benchmark dataset specifically designed for ITS. MITS includes
170,400 independently collected real-world ITS images sourced from traffic
surveillance cameras, annotated with eight main categories and 24 subcategories
of ITS-specific objects and events under diverse environmental conditions.
Additionally, through a systematic data generation pipeline, we generate
high-quality image captions and 5 million instruction-following visual
question-answer pairs, addressing five critical ITS tasks: object and event
recognition, object counting, object localization, background analysis, and
event reasoning. To demonstrate MITS's effectiveness, we fine-tune mainstream
LMMs on this dataset, enabling the development of ITS-specific applications.
Experimental results show that MITS significantly improves LMM performance in
ITS applications, increasing LLaVA-1.5's performance from 0.494 to 0.905
(+83.2%), LLaVA-1.6's from 0.678 to 0.921 (+35.8%), Qwen2-VL's from 0.584 to
0.926 (+58.6%), and Qwen2.5-VL's from 0.732 to 0.930 (+27.0%). We release the
dataset, code, and models as open-source, providing high-value resources to
advance both ITS and LMM research.

</details>


### [5] [Decomposing Visual Classification: Assessing Tree-Based Reasoning in VLMs](https://arxiv.org/abs/2509.09732)
*Sary Elmansoury,Islam Mesabah,Gerrit Großmann,Peter Neigel,Raj Bhalwankar,Daniel Kondermann,Sebastian J. Vollmer*

Main category: cs.CV

TL;DR: 本论文探讨了结构化树状推理能否提升视觉语言模型（VLMs）在细粒度和大层次标签空间下的表现，结果发现效果有限。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在零样本分类上表现出色，但在细粒度任务和复杂标签结构下的表现尚未充分研究，作者希望通过结构化推理提升模型的解释性和表现。

Method: 作者提出了一种将分类任务分解为可解释决策的树状推理框架，并在细粒度（GTSRB）和粗粒度（CIFAR-10）数据集上进行了评估。同时，探索使用大模型生成的类别和图像描述优化树状提示。

Result: 模型在理解树状知识方面达到了98.2%的准确率，但树状推理在实际分类性能上稳定低于标准的零样本提示。加入LLM生成的类别和描述后，两种方法的效果均有所提升。

Conclusion: 结构化树状推理对于提升VLMs解释性有一定作用，但在视觉分类准确率方面存在局限，未来可用于设计更可解释的VLM系统。

Abstract: Vision language models (VLMs) excel at zero-shot visual classification, but
their performance on fine-grained tasks and large hierarchical label spaces is
understudied. This paper investigates whether structured, tree-based reasoning
can enhance VLM performance. We introduce a framework that decomposes
classification into interpretable decisions using decision trees and evaluates
it on fine-grained (GTSRB) and coarse-grained (CIFAR-10) datasets. Although the
model achieves 98.2% accuracy in understanding the tree knowledge, tree-based
reasoning consistently underperforms standard zero-shot prompting. We also
explore enhancing the tree prompts with LLM-generated classes and image
descriptions to improve alignment. The added description enhances the
performance of the tree-based and zero-shot methods. Our findings highlight
limitations of structured reasoning in visual classification and offer insights
for designing more interpretable VLM systems.

</details>


### [6] [World Modeling with Probabilistic Structure Integration](https://arxiv.org/abs/2509.09737)
*Klemen Kotar,Wanhee Lee,Rahul Venkatesh,Honglin Chen,Daniel Bear,Jared Watrous,Simon Kim,Khai Loong Aw,Lilian Naing Chen,Stefan Stojanov,Kevin Feigelis,Imran Thobani,Alex Durango,Khaled Jedoui,Atlas Kazemian,Dan Yamins*

Main category: cs.CV

TL;DR: 本文提出了一种名为Probabilistic Structure Integration（PSI）的新型概率结构集成系统，实现了对世界模型的灵活控制与提示，并在训练与推理中循环利用学习到的结构信息。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型在灵活控制与条件建模方面存在局限，难以从数据中自动提取并融合丰富的语义结构信号。PSI旨在解决这一问题，提高模型的泛化与可控性。

Method: PSI分为三个循环步骤：1）概率预测，构建能够随机访问的自回归概率图模型Psi，全面刻画数据变量间的条件依赖关系；2）结构提取，通过因果推断从Psi中零样本挖掘、提取低维且有意义的中间结构特征；3）结构集成，将挖掘到的新结构作为新的token类型，加入后续模型训练与推理，为任务提供额外的控制手段。

Result: 在1.4万亿互联网视频数据token上训练Psi后，系统在视频预测、理解等多项推理任务上表现优异，并实现了光流、深度和物体分割等自监督结构的最优提取与融合。

Conclusion: PSI有效提升了概率模型对复杂数据的建模能力与灵活控制能力，为世界模型提供了类似LLM泛化提示语言式的新机制，对视频等多模态任务表现出极大潜力。

Abstract: We present Probabilistic Structure Integration (PSI), a system for learning
richly controllable and flexibly promptable world models from data. PSI
consists of a three-step cycle. The first step, Probabilistic prediction,
involves building a probabilistic graphical model Psi of the data, in the form
of a random-access autoregressive sequence model. Psi supports a complete set
of learned conditional distributions describing the dependence of any variables
in the data on any other set of variables. In step 2, Structure extraction, we
show how to extract underlying low-dimensional properties in the data,
corresponding to a diverse set of meaningful "intermediate structures", in a
zero-shot fashion via causal inference on Psi. Step 3, Integration, completes
the cycle by converting these structures into new token types that are then
continually mixed back into the training diet as conditioning signals and
prediction targets. Each such cycle augments the capabilities of Psi, both
allowing it to model the underlying data better, and creating new control
handles -- akin to an LLM-like universal prompting language. We train an
instance of Psi on 1.4 trillion tokens of internet video data; we use it to
perform a variety of useful video prediction and understanding inferences; we
extract state-of-the-art optical flow, self-supervised depth and object
segmentation; and we use these structures to support a full cycle of predictive
improvements.

</details>


### [7] [Images in Motion?: A First Look into Video Leakage in Collaborative Deep Learning](https://arxiv.org/abs/2509.09742)
*Md Fazle Rasul,Alanood Alqobaisi,Bruhadeshwar Bezawada,Indrakshi Ray*

Main category: cs.CV

TL;DR: 本文首次系统性分析了在联邦学习（FL）中，视频数据通过梯度反演攻击泄露的风险。研究发现，使用特征提取器的视频分类方法对攻击有一定抵抗力，但安全隐患仍然存在。攻击者可通过图像超分辨率提升被反演的帧质量，实现更有效的视频重建，证实视频数据泄露威胁确实存在。


<details>
  <summary>Details</summary>
Motivation: 虽然针对图像、文本和表格数据的梯度反演攻击已有相关研究，但针对视频数据的研究仍属空白。随着FL在视频相关领域的应用普及，分析视频数据在FL架构下被攻击和泄露的风险及原理，具有重要的现实和理论意义。

Method: 论文选取了两种常用的视频分类方式：一是使用预训练特征提取器，二是利用简单变换对原始视频帧直接处理。分别针对这些方式，评估了梯度反演攻击下的数据泄露风险。此外，作者还尝试将高分辨率重建技术应用于攻击反演的帧，从而提高被攻击视频的可用性；并在不同攻防信息获取条件下进行实验验证。

Result: 结果显示，使用特征提取器提升了防御梯度反演攻击的能力，但如果分类器结构较为简单，依然会存在数据泄露。通过超分辨率技术可以进一步提升泄露视频的质量。相关实验在攻击者获取零帧、一帧或多帧参考数据三种环境下均得到证实。

Conclusion: 视频数据在FL环境下，依然面临通过梯度反演攻击泄露的威胁。特征提取器模型可提升防御能力但不能完全杜绝风险。该领域值得持续深入研究以完善相关安全防护措施。

Abstract: Federated learning (FL) allows multiple entities to train a shared model
collaboratively. Its core, privacy-preserving principle is that participants
only exchange model updates, such as gradients, and never their raw, sensitive
data. This approach is fundamental for applications in domains where privacy
and confidentiality are important. However, the security of this very mechanism
is threatened by gradient inversion attacks, which can reverse-engineer private
training data directly from the shared gradients, defeating the purpose of FL.
While the impact of these attacks is known for image, text, and tabular data,
their effect on video data remains an unexamined area of research. This paper
presents the first analysis of video data leakage in FL using gradient
inversion attacks. We evaluate two common video classification approaches: one
employing pre-trained feature extractors and another that processes raw video
frames with simple transformations. Our initial results indicate that the use
of feature extractors offers greater resilience against gradient inversion
attacks. We also demonstrate that image super-resolution techniques can enhance
the frames extracted through gradient inversion attacks, enabling attackers to
reconstruct higher-quality videos. Our experiments validate this across
scenarios where the attacker has access to zero, one, or more reference frames
from the target environment. We find that although feature extractors make
attacks more challenging, leakage is still possible if the classifier lacks
sufficient complexity. We, therefore, conclude that video data leakage in FL is
a viable threat, and the conditions under which it occurs warrant further
investigation.

</details>


### [8] [A Co-Training Semi-Supervised Framework Using Faster R-CNN and YOLO Networks for Object Detection in Densely Packed Retail Images](https://arxiv.org/abs/2509.09750)
*Hossein Yazdanjouei,Arash Mansouri,Mohammad Shokouhifar*

Main category: cs.CV

TL;DR: 本文提出了一种半监督协同训练框架，结合Faster R-CNN和YOLO，并引入分类集成和超参数优化，用于解决密集零售场景中的目标检测问题，显著减少标注成本并提升检测准确率。


<details>
  <summary>Details</summary>
Motivation: 在零售环境中，商品排列密集、重叠遮挡严重，同时标注数据获取昂贵且样本有限，导致现有目标检测方法难以高效应用。因此亟需一种能在少量标注数据下高效工作的检测框架。

Method: 框架结合了Faster R-CNN（ResNet骨干，精准定位）和YOLO（Darknet骨干，提供全局上下文 ），利用互相交换伪标签提升对遮挡和重叠商品的检测能力；分类环节采用XGBoost、随机森林和SVM的集成以增强鲁棒性，并通过元启发式算法优化各模型超参数。

Result: 在SKU-110k数据集上进行实验，所提方法在检测准确率、鲁棒性和效率上远超基线，表现出良好的扩展性和实用性。

Conclusion: 该框架能够显著降低人工标注依赖，适应零售场景中频繁变动的商品与布局，具有自动库存跟踪、商品监控和结账系统等实际应用潜力。

Abstract: This study proposes a semi-supervised co-training framework for object
detection in densely packed retail environments, where limited labeled data and
complex conditions pose major challenges. The framework combines Faster R-CNN
(utilizing a ResNet backbone) for precise localization with YOLO (employing a
Darknet backbone) for global context, enabling mutual pseudo-label exchange
that improves accuracy in scenes with occlusion and overlapping objects. To
strengthen classification, it employs an ensemble of XGBoost, Random Forest,
and SVM, utilizing diverse feature representations for higher robustness.
Hyperparameters are optimized using a metaheuristic-driven algorithm, enhancing
precision and efficiency across models. By minimizing reliance on manual
labeling, the approach reduces annotation costs and adapts effectively to
frequent product and layout changes common in retail. Experiments on the
SKU-110k dataset demonstrate strong performance, highlighting the scalability
and practicality of the proposed framework for real-world retail applications
such as automated inventory tracking, product monitoring, and checkout systems.

</details>


### [9] [Purge-Gate: Backpropagation-Free Test-Time Adaptation for Point Clouds Classification via Token Purging](https://arxiv.org/abs/2509.09785)
*Moslem Yazdanpanah,Ali Bahri,Mehrdad Noori,Sahar Dastani,Gustavo Adolfo Vargas Hakim,David Osowiechi,Ismail Ben Ayed,Christian Desrosiers*

Main category: cs.CV

TL;DR: 本文提出了一种名为Token Purging（PG）的新型无反向传播测试时自适应（TTA）方法，用于提升3D点云分类在分布偏移下的鲁棒性。PG方法在送入注意力层前剔除受域偏移影响较大的token，实现更优的自适应性能。两种PG变体（PG-SP和PG-SF）在多个数据集上均优于当前最优无反向传播TTA方法，并显著提升了效率和准确率。


<details>
  <summary>Details</summary>
Motivation: 3D点云分类模型在实际应用中经常因为数据分布偏移而导致性能下降。目前的测试时自适应（TTA）方法需反向传播、更新参数或者效率较低，亟需高效且无需反向传播的新方案。

Method: 提出Token Purging (PG)方法，在特征进入注意力层前剔除受分布偏移影响大的token，有PG-SP（利用源域统计）和PG-SF（完全无源域，依赖CLS-token）两种版本。无需参数更新，且方法高效。

Result: 在ModelNet40-C、ShapeNet-C、ScanObjectNN-C等数据集上，PG-SP平均准确率比当前最优无反向传播方法高10.3%；PG-SF在无源自适应场景下刷新了性能纪录。PG方法比基线快12.4倍、节省5.5倍内存。

Conclusion: Token Purging（PG）方法能在不需反向传播的前提下，有效提升3D点云分类模型在分布偏移下的性能，同时大幅提升运行效率，适用于实际部署场景。

Abstract: Test-time adaptation (TTA) is crucial for mitigating performance degradation
caused by distribution shifts in 3D point cloud classification. In this work,
we introduce Token Purging (PG), a novel backpropagation-free approach that
removes tokens highly affected by domain shifts before they reach attention
layers. Unlike existing TTA methods, PG operates at the token level, ensuring
robust adaptation without iterative updates. We propose two variants: PG-SP,
which leverages source statistics, and PG-SF, a fully source-free version
relying on CLS-token-driven adaptation. Extensive evaluations on ModelNet40-C,
ShapeNet-C, and ScanObjectNN-C demonstrate that PG-SP achieves an average of
+10.3\% higher accuracy than state-of-the-art backpropagation-free methods,
while PG-SF sets new benchmarks for source-free adaptation. Moreover, PG is
12.4 times faster and 5.5 times more memory efficient than our baseline, making
it suitable for real-world deployment. Code is available at
\hyperlink{https://github.com/MosyMosy/Purge-Gate}{https://github.com/MosyMosy/Purge-Gate}

</details>


### [10] [Fine-Grained Cross-View Localization via Local Feature Matching and Monocular Depth Priors](https://arxiv.org/abs/2509.09792)
*Zimin Xia,Chenghao Xu,Alexandre Alahi*

Main category: cs.CV

TL;DR: 本文提出了一种精确且具有高度可解释性的细粒度跨视角定位方法，可将地面图像与参考航拍图像进行匹配，从而估算地面图像的三自由度位姿。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将地面图像变换为俯视图后再与航拍图像对齐，过程中由于透视变换或高度信息被压缩会导致特征信息损失，影响定位精度。解决这一问题亟需提升跨视角之间的对齐质量。

Method: 作者的方法直接在地面和航拍图像之间建立局部特征对应关系，仅将匹配到的关键点利用单目深度先验提升到BEV空间。方法支持使用度量型深度和相对深度，并通过尺度感知的Procrustes对齐优化相机位姿估计。

Result: 实验显示，该方法在仅有相机位姿弱监督的情况下，学习了准确的特征对应关系，在跨地区、未知方向等复杂场景下，实现了优于以往方法的定位性能。此外，该方法适用于多种深度模型，无需微调。

Conclusion: 该方法兼具高精度、本地化性能强和灵活性，适用于实际应用，尤其在跨视角、复杂环境下具有显著优势。

Abstract: We propose an accurate and highly interpretable fine-grained cross-view
localization method that estimates the 3 Degrees of Freedom pose of a
ground-level image by matching its local features with a reference aerial
image. Previous methods typically transform the ground image into a bird's-eye
view (BEV) representation and then align it with the aerial image for
localization. However, this transformation often leads to information loss due
to perspective distortion or compression of height information, thereby
degrading alignment quality with the aerial view. In contrast, our method
directly establishes correspondences between ground and aerial images and lifts
only the matched keypoints to BEV space using monocular depth prior. Notably,
modern depth predictors can provide reliable metric depth when the test samples
are similar to the training data. When the depth distribution differs, they
still produce consistent relative depth, i.e., depth accurate up to an unknown
scale. Our method supports both metric and relative depth. It employs a
scale-aware Procrustes alignment to estimate the camera pose from the
correspondences and optionally recover the scale when using relative depth.
Experimental results demonstrate that, with only weak supervision on camera
pose, our method learns accurate local feature correspondences and achieves
superior localization performance under challenging conditions, such as
cross-area generalization and unknown orientation. Moreover, our method is
compatible with various relative depth models without requiring per-model
finetuning. This flexibility, combined with strong localization performance,
makes it well-suited for real-world deployment.

</details>


### [11] [Early Detection of Visual Impairments at Home Using a Smartphone Red-Eye Reflex Test](https://arxiv.org/abs/2509.09808)
*Judith Massmann,Alexander Lichtenstein,Francisco M. López*

Main category: cs.CV

TL;DR: 本研究提出并评估了一款基于移动设备的儿童视觉筛查应用KidsVisionCheck，通过AI分析红光反射图像，实现快速、便携的视觉问题检测。


<details>
  <summary>Details</summary>
Motivation: 传统Bruckner 检查依赖专业设备和眼科医生，局限了儿童视觉疾病早期筛查的广泛性和便捷性。随着智能手机和人工智能技术的发展，将专业视觉筛查转向手机平台具有现实和迫切需求。

Method: 开发了KidsVisionCheck应用，利用深度神经网络对经眼科医生标注的儿童瞳孔红光反射图像进行训练和预测。系统在收集的新数据集上进行了初步实验，并评估其准确率及最佳拍摄条件。

Result: 模型在未见过的测试数据上达到90%的准确率，且无需专业医疗设备，即可高效检测视觉异常。同时，研究明确了数据采集的优化条件，能为用户即时反馈。

Conclusion: 本工作首次实现了基于手机的儿童视觉筛查，验证了AI在该场景下的高效可靠性，为全球范围内普及儿童视力早筛和早期干预提供了可行的新途径。

Abstract: Numerous visual impairments can be detected in red-eye reflex images from
young children. The so-called Bruckner test is traditionally performed by
ophthalmologists in clinical settings. Thanks to the recent technological
advances in smartphones and artificial intelligence, it is now possible to
recreate the Bruckner test using a mobile device. In this paper, we present a
first study conducted during the development of KidsVisionCheck, a free
application that can perform vision screening with a mobile device using
red-eye reflex images. The underlying model relies on deep neural networks
trained on children's pupil images collected and labeled by an ophthalmologist.
With an accuracy of 90% on unseen test data, our model provides highly reliable
performance without the necessity of specialist equipment. Furthermore, we can
identify the optimal conditions for data collection, which can in turn be used
to provide immediate feedback to the users. In summary, this work marks a first
step toward accessible pediatric vision screenings and early intervention for
vision abnormalities worldwide.

</details>


### [12] [DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception](https://arxiv.org/abs/2509.09828)
*Tim Broedermannn,Christos Sakaridis,Luigi Piccinelli,Wim Abbeloos,Luc Van Gool*

Main category: cs.CV

TL;DR: 本文提出了一种深度引导的多模态融合方法DGFusion，通过结合激光雷达（lidar）和图像等多传感器数据，有效提升了自动驾驶场景下的语义感知能力，并在MUSES和DELIVER数据集上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 当前主流的多传感器融合方法往往忽略了输入空间范围内各位置的差异，无法应对复杂、多变的感知场景。因此，作者希望利用深度信息（如lidar深度）提升多模态融合的适应性和鲁棒性。

Method: 作者提出了DGFusion网络，将多模态分割视为多任务问题，利用lidar深度信息作为输入和学习目标。其中加入了辅助深度预测头（auxiliary depth head），提取深度敏感特征并编码为空间相关的深度token，驱动跨模态融合。设计了全局token和局部深度token，动态调整不同区域的传感器融合权重。此外，还引入鲁棒损失函数，以弥补lidar数据在恶劣条件下的稀疏和噪声影响。

Result: DGFusion在挑战性的MUSES和DELIVER数据集上，在语义与全景分割任务上均取得了当前最优的表现。

Conclusion: 通过深度感知增强的融合机制，DGFusion能够实现空间自适应、多模态协同的高鲁棒性能，为自动驾驶场景下多传感器融合感知提供了新思路。

Abstract: Robust semantic perception for autonomous vehicles relies on effectively
combining multiple sensors with complementary strengths and weaknesses.
State-of-the-art sensor fusion approaches to semantic perception often treat
sensor data uniformly across the spatial extent of the input, which hinders
performance when faced with challenging conditions. By contrast, we propose a
novel depth-guided multimodal fusion method that upgrades condition-aware
fusion by integrating depth information. Our network, DGFusion, poses
multimodal segmentation as a multi-task problem, utilizing the lidar
measurements, which are typically available in outdoor sensor suites, both as
one of the model's inputs and as ground truth for learning depth. Our
corresponding auxiliary depth head helps to learn depth-aware features, which
are encoded into spatially varying local depth tokens that condition our
attentive cross-modal fusion. Together with a global condition token, these
local depth tokens dynamically adapt sensor fusion to the spatially varying
reliability of each sensor across the scene, which largely depends on depth. In
addition, we propose a robust loss for our depth, which is essential for
learning from lidar inputs that are typically sparse and noisy in adverse
conditions. Our method achieves state-of-the-art panoptic and semantic
segmentation performance on the challenging MUSES and DELIVER datasets. Code
and models will be available at https://github.com/timbroed/DGFusion

</details>


### [13] [Patch-based Automatic Rosacea Detection Using the ResNet Deep Learning Framework](https://arxiv.org/abs/2509.09841)
*Chengyu Yang,Rishik Reddy Yesgari,Chengjun Liu*

Main category: cs.CV

TL;DR: 本文提出基于ResNet-18的补丁（patch）式自动玫瑰痤疮检测方法，通过从人脸图像中提取不同位置和大小的图像补丁，提升了检测准确率，同时保护了患者隐私。


<details>
  <summary>Details</summary>
Motivation: 玫瑰痤疮需要早期、精准检测以改善治疗效果，现有基于全脸图像的自动检测方法存在准确率和隐私保护的不足。

Method: 从面部图像中提取不同大小、不同位置的图像补丁，利用ResNet-18深度学习模型对这些补丁进行训练和检测；同时评估局部信息对模型表现的影响，并和全图像方法进行对比。

Result: 多个基于图像补丁的检测方法在准确率和敏感性上优于全图像方法，并且能利用局部信息提升模型的鲁棒性和可解释性。

Conclusion: 只利用局部人脸补丁的检测方法不仅在性能上优于全图像方法，还能天然保护患者隐私，为自动皮肤病诊断提供了实用的新思路。

Abstract: Rosacea, which is a chronic inflammatory skin condition that manifests with
facial redness, papules, and visible blood vessels, often requirs precise and
early detection for significantly improving treatment effectiveness. This paper
presents new patch-based automatic rosacea detection strategies using the
ResNet-18 deep learning framework. The contributions of the proposed strategies
come from the following aspects. First, various image pateches are extracted
from the facial images of people in different sizes, shapes, and locations.
Second, a number of investigation studies are carried out to evaluate how the
localized visual information influences the deep learing model performance.
Third, thorough experiments are implemented to reveal that several patch-based
automatic rosacea detection strategies achieve competitive or superior accuracy
and sensitivity than the full-image based methods. And finally, the proposed
patch-based strategies, which use only localized patches, inherently preserve
patient privacy by excluding any identifiable facial features from the data.
The experimental results indicate that the proposed patch-based strategies
guide the deep learning model to focus on clinically relevant regions, enhance
robustness and interpretability, and protect patient privacy. As a result, the
proposed strategies offer practical insights for improving automated
dermatological diagnostics.

</details>


### [14] [Privacy-Preserving Automated Rosacea Detection Based on Medically Inspired Region of Interest Selection](https://arxiv.org/abs/2509.09844)
*Chengyu Yang,Rishik Reddy Yesgari,Chengjun Liu*

Main category: cs.CV

TL;DR: 本论文提出了一种隐私保护的基于合成数据的玫瑰痤疮自动检测方法，该方法利用中心面部红斑分布的临床先验，并在实际测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 玫瑰痤疮易被漏诊，传统自动识别方法受限于症状分布离散、标注数据稀缺以及隐私困扰（涉及人脸图像）。因此，迫切需要兼顾隐私与精度的自动识别新方法。

Method: 作者设计了两步法：1）根据面部红色通道强度稳定分布，构建固定的红斑相关区域掩膜，仅关注面颊、鼻、额部等疾病相关区域，排除身份敏感信息；2）在掩膜处理后生成的合成图像上训练ResNet-18深度学习模型。

Result: 基于掩膜的合成数据训练的ResNet-18模型，在真实世界测试集上，准确率、召回率和F1分数均显著优于使用全脸图像的基线方法。

Conclusion: 结合临床先验与合成数据不仅能实现高效及隐私保护的玫瑰痤疮AI识别，对远程医疗及大规模筛查具有实际应用前景和伦理优势。

Abstract: Rosacea is a common but underdiagnosed inflammatory skin condition that
primarily affects the central face and presents with subtle redness, pustules,
and visible blood vessels. Automated detection remains challenging due to the
diffuse nature of symptoms, the scarcity of labeled datasets, and privacy
concerns associated with using identifiable facial images. A novel
privacy-preserving automated rosacea detection method inspired by clinical
priors and trained entirely on synthetic data is presented in this paper.
Specifically, the proposed method, which leverages the observation that rosacea
manifests predominantly through central facial erythema, first constructs a
fixed redness-informed mask by selecting regions with consistently high red
channel intensity across facial images. The mask thus is able to focus on
diagnostically relevant areas such as the cheeks, nose, and forehead and
exclude identity-revealing features. Second, the ResNet-18 deep learning
method, which is trained on the masked synthetic images, achieves superior
performance over the full-face baselines with notable gains in terms of
accuracy, recall and F1 score when evaluated using the real-world test data.
The experimental results demonstrate that the synthetic data and clinical
priors can jointly enable accurate and ethical dermatological AI systems,
especially for privacy sensitive applications in telemedicine and large-scale
screening.

</details>


### [15] [Investigating the Impact of Various Loss Functions and Learnable Wiener Filter for Laparoscopic Image Desmoking](https://arxiv.org/abs/2509.09849)
*Chengyu Yang,Chengjun Liu*

Main category: cs.CV

TL;DR: 本论文通过系统性的消融实验，评估了ULW（U-Net+可学习Wiener滤波+复合损失）框架各组件在腹腔镜去烟雾任务中的作用与必要性。


<details>
  <summary>Details</summary>
Motivation: ULW方法近期在腹腔镜图像去烟雾领域提出，但其各组件（U-Net骨干、可学习Wiener滤波、复合损失的不同项）对整体性能的具体贡献尚未明确。作者希望通过细致消融实验定量和定性地分析各部分的有效性和必要性。

Method: 采用消融研究法，分别移除可学习Wiener滤波、分别使用复合损失中的不同项（MSE、SSIM loss、感知损失），在公开腹腔镜成对数据集上，对SSIM、PSNR、MSE、CIEDE-2000等指标进行测试，并做视觉对比。

Result: 系统分析展示了各个组件和不同损失项对模型性能的具体影响，量化了可学习Wiener滤波和各损失项各自对SSIM、PSNR、MSE、色彩精准度等的提升作用。

Conclusion: 各组件均对ULW框架性能有正向且不可或缺的贡献，提供了对去烟雾模型框架设计的实证参考。

Abstract: To rigorously assess the effectiveness and necessity of individual components
within the recently proposed ULW framework for laparoscopic image desmoking,
this paper presents a comprehensive ablation study. The ULW approach combines a
U-Net based backbone with a compound loss function that comprises mean squared
error (MSE), structural similarity index (SSIM) loss, and perceptual loss. The
framework also incorporates a differentiable, learnable Wiener filter module.
In this study, each component is systematically ablated to evaluate its
specific contribution to the overall performance of the whole framework. The
analysis includes: (1) removal of the learnable Wiener filter, (2) selective
use of individual loss terms from the composite loss function. All variants are
benchmarked on a publicly available paired laparoscopic images dataset using
quantitative metrics (SSIM, PSNR, MSE and CIEDE-2000) alongside qualitative
visual comparisons.

</details>


### [16] [WAVE-DETR Multi-Modal Visible and Acoustic Real-Life Drone Detector](https://arxiv.org/abs/2509.09859)
*Razvan Stefanescu,Ethan Oh,Ruben Vazquez,Chris Mesterharm,Constantin Serban,Ritu Chadha*

Main category: cs.CV

TL;DR: 提出了一种结合可见光和声音信号的新型无人机检测方法WAVE-DETR，显著提升了无人机在各种环境下的检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有无人机检测方法主要依赖视觉信号，面对复杂噪声环境（如小型无人机、光照变化等）性能有限；本文提出引入声学信息以提升对无人机的鲁棒检测能力。

Method: 方法基于Deformable DETR和Wav2Vec2架构，将RGB图像与声音信号特征融合，设计了门控机制、线性层、MLP和交叉注意力四种特征融合方式，综合利用同步采集的图像与音频数据提升检测性能。

Result: 在公开和自建的无人机数据集（含7,500多对同步图像和音频）上，门控融合方法对小型无人机检测的mAP提升了11.1%至15.3%；中大型无人机检测mAP也提升了3.27%至5.84%。

Conclusion: 融合声学与视觉特征可显著提升无人机检测的鲁棒性和准确率，尤其在小型无人机检测上优势明显。

Abstract: We introduce a multi-modal WAVE-DETR drone detector combining visible RGB and
acoustic signals for robust real-life UAV object detection. Our approach fuses
visual and acoustic features in a unified object detector model relying on the
Deformable DETR and Wav2Vec2 architectures, achieving strong performance under
challenging environmental conditions. Our work leverage the existing
Drone-vs-Bird dataset and the newly generated ARDrone dataset containing more
than 7,500 synchronized images and audio segments. We show how the acoustic
information is used to improve the performance of the Deformable DETR object
detector on the real ARDrone dataset. We developed, trained and tested four
different fusion configurations based on a gated mechanism, linear layer, MLP
and cross attention. The Wav2Vec2 acoustic embeddings are fused with the multi
resolution feature mappings of the Deformable DETR and enhance the object
detection performance over all drones dimensions. The best performer is the
gated fusion approach, which improves the mAP of the Deformable DETR object
detector on our in-distribution and out-of-distribution ARDrone datasets by
11.1% to 15.3% for small drones across all IoU thresholds between 0.5 and 0.9.
The mAP scores for medium and large drones are also enhanced, with overall
gains across all drone sizes ranging from 3.27% to 5.84%.

</details>


### [17] [Surrogate Supervision for Robust and Generalizable Deformable Image Registration](https://arxiv.org/abs/2509.09869)
*Yihao Liu,Junyu Chen,Lianrui Zuo,Shuwen Wei,Brian D. Boyd,Carmen Andreescu,Olusola Ajilore,Warren D. Taylor,Aaron Carass,Bennett A. Landman*

Main category: cs.CV

TL;DR: 本文提出了一种可提升深度学习配准网络鲁棒性与泛化能力的新型训练范式“代理监督（surrogate supervision）”方法，并在多种医学图像配准任务取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习的可变形图像配准方法在准确性上表现良好，但其对输入数据变化（如伪影、视野不匹配或模态差异）十分敏感，鲁棒性和泛化性有限。因此，需要一种新方法以提升模型在多样化医学影像场景下的稳定性与适应性。

Method: 提出'代理监督'训练范式，将输入域与监督域解耦：通过对代理影像进行空间变换，仅在相似性易量化的代理数据域计算监督信号。该框架支持利用异构输入训练，同时确保监督质量。验证用例包括抗伪影脑部MR配准、无遮罩肺部CT配准、多模态MR配准。

Result: 在不同任务中，代理监督方法显著增强了模型对输入变化（包括不均匀场、视野不一致和模态差异）的鲁棒性，同时在高质量数据上维持了卓越性能。

Conclusion: 代理监督作为一种通用、有效且无须增加模型复杂度的训练框架，可显著提升基于深度学习的医学图像配准模型的鲁棒性与泛化能力，推动其在多种复杂、生物医学成像场景下的实际应用。

Abstract: Objective: Deep learning-based deformable image registration has achieved
strong accuracy, but remains sensitive to variations in input image
characteristics such as artifacts, field-of-view mismatch, or modality
difference. We aim to develop a general training paradigm that improves the
robustness and generalizability of registration networks. Methods: We introduce
surrogate supervision, which decouples the input domain from the supervision
domain by applying estimated spatial transformations to surrogate images. This
allows training on heterogeneous inputs while ensuring supervision is computed
in domains where similarity is well defined. We evaluate the framework through
three representative applications: artifact-robust brain MR registration,
mask-agnostic lung CT registration, and multi-modal MR registration. Results:
Across tasks, surrogate supervision demonstrated strong resilience to input
variations including inhomogeneity field, inconsistent field-of-view, and
modality differences, while maintaining high performance on well-curated data.
Conclusions: Surrogate supervision provides a principled framework for training
robust and generalizable deep learning-based registration models without
increasing complexity. Significance: Surrogate supervision offers a practical
pathway to more robust and generalizable medical image registration, enabling
broader applicability in diverse biomedical imaging scenarios.

</details>


### [18] [An Autoencoder and Vision Transformer-based Interpretability Analysis of the Differences in Automated Staging of Second and Third Molars](https://arxiv.org/abs/2509.09911)
*Barkin Buyukcakir,Jannick De Tobel,Patrick Thevissen,Dirk Vandermeulen,Peter Claes*

Main category: cs.CV

TL;DR: 该论文提出了一种结合卷积自编码器（AE）和视觉Transformer（ViT）的新框架，提升了法医学中牙齿年龄估算任务的性能和可解释性，尤其针对下颚第二和第三磨牙的自动分期问题。


<details>
  <summary>Details</summary>
Motivation: 在高风险的法医学应用中，深度学习模型因其“黑箱”特性难以被广泛采纳。本研究旨在通过提升模型性能和透明度，促进其在如牙齿年龄估算等领域的实用落地。

Method: 提出将卷积自编码器与ViT相结合的框架，通过分析AE的潜在空间和图像重建辅助诊断，并与单独ViT模型进行性能对比。

Result: 新框架将37号磨牙的分类准确率从0.712提升至0.815，38号磨牙从0.462提升至0.543；同时揭示38号磨牙高类内变异性是性能受限的主要因素。

Conclusion: 单一可解释性方法（如注意力图）不足以发现数据问题，采用多元诊断和提升性能的方法可更有效支持法医学专家决策，提高自动化牙齿年龄估算的可靠性。

Abstract: The practical adoption of deep learning in high-stakes forensic applications,
such as dental age estimation, is often limited by the 'black box' nature of
the models. This study introduces a framework designed to enhance both
performance and transparency in this context. We use a notable performance
disparity in the automated staging of mandibular second (tooth 37) and third
(tooth 38) molars as a case study. The proposed framework, which combines a
convolutional autoencoder (AE) with a Vision Transformer (ViT), improves
classification accuracy for both teeth over a baseline ViT, increasing from
0.712 to 0.815 for tooth 37 and from 0.462 to 0.543 for tooth 38. Beyond
improving performance, the framework provides multi-faceted diagnostic
insights. Analysis of the AE's latent space metrics and image reconstructions
indicates that the remaining performance gap is data-centric, suggesting high
intra-class morphological variability in the tooth 38 dataset is a primary
limiting factor. This work highlights the insufficiency of relying on a single
mode of interpretability, such as attention maps, which can appear anatomically
plausible yet fail to identify underlying data issues. By offering a
methodology that both enhances accuracy and provides evidence for why a model
may be uncertain, this framework serves as a more robust tool to support expert
decision-making in forensic age estimation.

</details>


### [19] [SCoDA: Self-supervised Continual Domain Adaptation](https://arxiv.org/abs/2509.09935)
*Chirayu Agrawal,Snehasis Mukherjee*

Main category: cs.CV

TL;DR: 提出了一种新的源数据不可用的领域自适应方法（SCoDA），用自监督预训练替代传统有监督，并采用几何流形对齐，实验验证优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的源数据不可用领域自适应方法依赖于有监督预训练，并采用特征匹配，忽略了源模型潜在流形的重要几何信息，影响了适应效果。

Method: 1）用自监督学习（SSL）预训练教师模型，避免对有监督数据的依赖；2）在学生模型训练中，结合实例级特征匹配与空间相似性损失，同时通过对学生参数的指数滑动平均（EMA）更新教师，防止灾难性遗忘；3）提出几何流形对齐的思想并应用于该任务。

Result: 在多个领域自适应基准数据集上，SCoDA实验结果明显优于现有最新SFDA方法。

Conclusion: SCoDA通过自监督预训练和几何流形对齐，有效提升了源数据不可用领域自适应的效果，为该领域提供了更强大的方法和新思路。

Abstract: Source-Free Domain Adaptation (SFDA) addresses the challenge of adapting a
model to a target domain without access to the data of the source domain.
Prevailing methods typically start with a source model pre-trained with full
supervision and distill the knowledge by aligning instance-level features.
However, these approaches, relying on cosine similarity over L2-normalized
feature vectors, inadvertently discard crucial geometric information about the
latent manifold of the source model. We introduce Self-supervised Continual
Domain Adaptation (SCoDA) to address these limitations. We make two key
departures from standard practice: first, we avoid the reliance on supervised
pre-training by initializing the proposed framework with a teacher model
pre-trained entirely via self-supervision (SSL). Second, we adapt the principle
of geometric manifold alignment to the SFDA setting. The student is trained
with a composite objective combining instance-level feature matching with a
Space Similarity Loss. To combat catastrophic forgetting, the teacher's
parameters are updated via an Exponential Moving Average (EMA) of the student's
parameters. Extensive experiments on benchmark datasets demonstrate that SCoDA
significantly outperforms state-of-the-art SFDA methods.

</details>


### [20] [Segment Anything for Cell Tracking](https://arxiv.org/abs/2509.09943)
*Zhu Chen,Mert Edgü,Er Jin,Johannes Stegmaier*

Main category: cs.CV

TL;DR: 本文提出了一种基于SAM2大模型的零样本细胞追踪方法，无需人工标注数据或针对特定数据集微调，在多种显微镜数据上表现出良好泛化性。


<details>
  <summary>Details</summary>
Motivation: 细胞追踪及有丝分裂事件检测是生物医学研究中的核心任务，但目前方法依赖人工标注数据集且泛化能力有限，数据标注昂贵且难以适应多样的显微镜数据。

Method: 方法通过将通用分割大模型Segment Anything 2（SAM2）集成到细胞追踪流程，实现了完全无监督的细胞追踪与分裂检测，无需依赖特定训练数据、无需微调。

Result: 在2D和大规模3D时间序列显微镜视频上，该方法达到了与现有方法相当的准确率，无需针对新数据集进行适配。

Conclusion: 提出的零样本追踪方法实现了不同显微镜数据上的自动泛化，有效降低了人工和数据集适配成本，提升了细胞追踪的实用性。

Abstract: Tracking cells and detecting mitotic events in time-lapse microscopy image
sequences is a crucial task in biomedical research. However, it remains highly
challenging due to dividing objects, low signal-tonoise ratios, indistinct
boundaries, dense clusters, and the visually similar appearance of individual
cells. Existing deep learning-based methods rely on manually labeled datasets
for training, which is both costly and time-consuming. Moreover, their
generalizability to unseen datasets remains limited due to the vast diversity
of microscopy data. To overcome these limitations, we propose a zero-shot cell
tracking framework by integrating Segment Anything 2 (SAM2), a large foundation
model designed for general image and video segmentation, into the tracking
pipeline. As a fully-unsupervised approach, our method does not depend on or
inherit biases from any specific training dataset, allowing it to generalize
across diverse microscopy datasets without finetuning. Our approach achieves
competitive accuracy in both 2D and large-scale 3D time-lapse microscopy videos
while eliminating the need for dataset-specific adaptation.

</details>


### [21] [Online 3D Multi-Camera Perception through Robust 2D Tracking and Depth-based Late Aggregation](https://arxiv.org/abs/2509.09946)
*Vu-Minh Le,Thao-Anh Tran,Duc Huy Do,Xuan Canh Do,Huong Ninh,Hai Tran*

Main category: cs.CV

TL;DR: 本文提出了一种将现有2D多摄像头目标跟踪系统扩展到3D空间的新方法，并在AI City Challenge 2025 3D多目标多摄像头追踪数据集上取得了第3名的成绩。


<details>
  <summary>Details</summary>
Motivation: 随着对大规模自动监控需求的增长，提升多目标多摄像头（MTMC）系统在3D空间的感知能力变得非常重要。然而，现有大多数系统仅限于2D空间，实现3D跟踪通常需要重新开发全部2D跟踪模块，因此需要一种能简便扩展到3D的新方案。

Method: 该方法基于深度信息将2D多摄像头跟踪目标重建为点云，通过聚类与偏航角（yaw）优化恢复目标的3D包围盒。在数据关联方面，提出了一种增强型在线机制，利用目标局部ID的一致性实现全局ID的连续分配。

Result: 该方法在2025 AI City Challenge 3D MTMC数据集上进行了评测，取得了排行榜第3名的成绩。

Conclusion: 本文所提出的方法能够高效地将现有2D多摄像头跟踪系统扩展到3D空间，无需完全推翻原有系统，为大规模3D环境的自动监控提供了实用的技术路线。

Abstract: Multi-Target Multi-Camera Tracking (MTMC) is an essential computer vision
task for automating large-scale surveillance. With camera calibration and depth
information, the targets in the scene can be projected into 3D space, offering
unparalleled levels of automatic perception of a 3D environment. However,
tracking in the 3D space requires replacing all 2D tracking components from the
ground up, which may be infeasible for existing MTMC systems. In this paper, we
present an approach for extending any online 2D multi-camera tracking system
into 3D space by utilizing depth information to reconstruct a target in
point-cloud space, and recovering its 3D box through clustering and yaw
refinement following tracking. We also introduced an enhanced online data
association mechanism that leverages the target's local ID consistency to
assign global IDs across frames. The proposed framework is evaluated on the
2025 AI City Challenge's 3D MTMC dataset, achieving 3rd place on the
leaderboard.

</details>


### [22] [Zero-Shot Referring Expression Comprehension via Visual-Language True/False Verification](https://arxiv.org/abs/2509.09958)
*Jeffrey Liu,Rongbin Hu*

Main category: cs.CV

TL;DR: 该论文提出一种无需特定训练的零样本指代表达理解（REC）方法，通过视觉语言模型验证候选框，实现与或优于现有有监督方法的表现。


<details>
  <summary>Details</summary>
Motivation: 当前REC任务一般依赖特定训练的模型，存在泛化能力差、训练代价高等问题，作者旨在探索能否设计一种无需专门任务训练、可直接应用的强大零样本方案。

Method: 作者将REC问题转化为针对候选框的视觉-语言逐盒验证任务。首先利用COCO-clean的YOLO-World检测器生成候选框，然后用通用视觉-语言大模型（VLM）对每个区域独立判断True/False，无需对REC做专门训练或微调。该流程避免跨框干扰，支持拒答和多框命中。

Result: 在RefCOCO、RefCOCO+和RefCOCOg三个数据集上，该方法不仅超越了零样本GroundingDINO基线，而且优于在REC上训练的GroundingDINO及其增强版本。控实验验证了逐框验证流程比选择式提示显著更强，且结论在开源VLM同样成立。

Conclusion: 论文证明任务流程设计对零样本场景下的REC表现影响巨大，甚至可超过传统的任务特定预训练，推动了泛化REC方法的发展。

Abstract: Referring Expression Comprehension (REC) is usually addressed with
task-trained grounding models. We show that a zero-shot workflow, without any
REC-specific training, can achieve competitive or superior performance. Our
approach reformulates REC as box-wise visual-language verification: given
proposals from a COCO-clean generic detector (YOLO-World), a general-purpose
VLM independently answers True/False queries for each region. This simple
procedure reduces cross-box interference, supports abstention and multiple
matches, and requires no fine-tuning. On RefCOCO, RefCOCO+, and RefCOCOg, our
method not only surpasses a zero-shot GroundingDINO baseline but also exceeds
reported results for GroundingDINO trained on REC and GroundingDINO+CRG.
Controlled studies with identical proposals confirm that verification
significantly outperforms selection-based prompting, and results hold with open
VLMs. Overall, we show that workflow design, rather than task-specific
pretraining, drives strong zero-shot REC performance.

</details>


### [23] [Augment to Segment: Tackling Pixel-Level Imbalance in Wheat Disease and Pest Segmentation](https://arxiv.org/abs/2509.09961)
*Tianqi Wei,Xin Yu,Zhi Chen,Scott Chapman,Zi Huang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为RPCP（随机投影拷贝粘贴）的数据增强方法，有效解决了小麦叶片病害与虫害分割任务中的极端像素类别不平衡问题，从而显著提升了虫害类别分割的性能。


<details>
  <summary>Details</summary>
Motivation: 在小麦叶片病害和虫害的分割任务中，虫害区域像素占比极小，导致分割网络过拟合于常见类别，难以准确区分稀有虫害类别，这极大影响了分割整体表现。

Method: 提出RPCP增强技术，从有标注的训练图片中提取稀有虫害区域，通过随机几何变换增加多样性，再粘贴到图像的合适区域，避免与病斑或已有损伤重叠，并使用随机投影滤波器融合贴片和背景，提升自然度。

Result: 实验结果表明，RPCP方法大幅提升了虫害类的分割性能，同时维持甚至略微提升了其他类别的分割准确率。

Conclusion: 面向极端像素不平衡问题，通过专门的数据增强可以有效提升稀有类别的分割表现。RPCP方法是一种简单高效的农业图像分割增强方案。

Abstract: Accurate segmentation of foliar diseases and insect damage in wheat is
crucial for effective crop management and disease control. However, the insect
damage typically occupies only a tiny fraction of annotated pixels. This
extreme pixel-level imbalance poses a significant challenge to the segmentation
performance, which can result in overfitting to common classes and insufficient
learning of rare classes, thereby impairing overall performance. In this paper,
we propose a Random Projected Copy-and-Paste (RPCP) augmentation technique to
address the pixel imbalance problem. Specifically, we extract rare
insect-damage patches from annotated training images and apply random geometric
transformations to simulate variations. The transformed patches are then pasted
in appropriate regions while avoiding overlaps with lesions or existing damaged
regions. In addition, we apply a random projection filter to the pasted
regions, refining local features and ensuring a natural blend with the new
background. Experiments show that our method substantially improves
segmentation performance on the insect damage class, while maintaining or even
slightly enhancing accuracy on other categories. Our results highlight the
effectiveness of targeted augmentation in mitigating extreme pixel imbalance,
offering a straightforward yet effective solution for agricultural segmentation
problems.

</details>


### [24] [An HMM-based framework for identity-aware long-term multi-object tracking from sparse and uncertain identification: use case on long-term tracking in livestock](https://arxiv.org/abs/2509.09962)
*Anne Marthe Sophie Ngo Bibinbe,Chiron Bang,Patrick Gagnon,Jamie Ahloy-Dallaire,Eric R. Paquet*

Main category: cs.CV

TL;DR: 本文提出了一种结合不确定身份和跟踪的隐马尔可夫模型（HMM）框架，提升了在长时间视频和多目标追踪中的表现，尤其适用于可获得零星身份信息的实际场景，比如牲畜管理。


<details>
  <summary>Details</summary>
Motivation: 现有多目标跟踪技术在长时间视频分析中易出现身份切换问题，长期跟踪表现下降，难以满足现实需求。但在一些实际应用中（如牲畜跟踪），可以通过特定装置零星获取个体身份信息。作者旨在解决长期多目标追踪中的身份不确定和性能下降问题。

Method: 作者提出以隐马尔可夫模型（HMM）为核心的新框架，将不确定的身份数据整合到传统跟踪过程中。框架可利用零星获得的真实身份来优化跟踪表现，并基于主流跟踪器ByteTrack进行实验扩展。

Result: 在10分钟猪群跟踪数据集上，该HMM框架结合偶发身份信息有显著提升（F1分数优于原有方法），对于身份信息频度变化表现出较好鲁棒性。同时，在MOT17和MOT20两个公开评测集上，利用ByteTrack和FairMOT验证了框架的通用性和有效提升。

Conclusion: 结合不确定身份信息与基于HMM的追踪框架，不仅可提升主流长时多目标追踪方法的精度与鲁棒性，还能广泛适用于现实场景。相关代码及数据集已开源，便于后续研究、复现及实际应用。

Abstract: The need for long-term multi-object tracking (MOT) is growing due to the
demand for analyzing individual behaviors in videos that span several minutes.
Unfortunately, due to identity switches between objects, the tracking
performance of existing MOT approaches decreases over time, making them
difficult to apply for long-term tracking. However, in many real-world
applications, such as in the livestock sector, it is possible to obtain
sporadic identifications for some of the animals from sources like feeders. To
address the challenges of long-term MOT, we propose a new framework that
combines both uncertain identities and tracking using a Hidden Markov Model
(HMM) formulation. In addition to providing real-world identities to animals,
our HMM framework improves the F1 score of ByteTrack, a leading MOT approach
even with re-identification, on a 10 minute pig tracking dataset with 21
identifications at the pen's feeding station. We also show that our approach is
robust to the uncertainty of identifications, with performance increasing as
identities are provided more frequently. The improved performance of our HMM
framework was also validated on the MOT17 and MOT20 benchmark datasets using
both ByteTrack and FairMOT. The code for this new HMM framework and the new
10-minute pig tracking video dataset are available at:
https://github.com/ngobibibnbe/uncertain-identity-aware-tracking

</details>


### [25] [Event Camera Guided Visual Media Restoration & 3D Reconstruction: A Survey](https://arxiv.org/abs/2509.09971)
*Aupendu Kar,Vishnu Raj,Guan-Ming Su*

Main category: cs.CV

TL;DR: 本文综述了事件相机和传统帧相机融合在视觉增强和三维重建领域的研究进展，特别关注深度学习方法在恢复和增强图像/视频质量方面的应用。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有低延迟、低能耗和超高采样率，但其输出的事件流与传统视频数据有很大不同，因此如何结合二者优势，实现更强的视觉重建和增强效果，成为重要研究方向。

Method: 系统回顾了基于深度学习的事件流与帧融合方法，围绕时域（如帧插值、去模糊）和空域（如超分辨率、低光增强、高动态范围提升、伪影消除）两大类任务，梳理最新研究进展。同时探讨了三维重建中事件驱动融合的发展，并整理了公开数据集，便于重复性研究与评测。

Result: 涵盖了众多提升视觉质量方法的最新成果，介绍了多种数据集和应用案例，展现了在复杂场景下事件流与帧流结合带来的显着性能提升。

Conclusion: 事件相机与传统帧相机结合，借助深度学习在视觉增强和三维重建领域展现巨大潜力，为后续相关研究提供了总结、现状梳理和未来启发。

Abstract: Event camera sensors are bio-inspired sensors which asynchronously capture
per-pixel brightness changes and output a stream of events encoding the
polarity, location and time of these changes. These systems are witnessing
rapid advancements as an emerging field, driven by their low latency, reduced
power consumption, and ultra-high capture rates. This survey explores the
evolution of fusing event-stream captured with traditional frame-based capture,
highlighting how this synergy significantly benefits various video restoration
and 3D reconstruction tasks. The paper systematically reviews major deep
learning contributions to image/video enhancement and restoration, focusing on
two dimensions: temporal enhancement (such as frame interpolation and motion
deblurring) and spatial enhancement (including super-resolution, low-light and
HDR enhancement, and artifact reduction). This paper also explores how the 3D
reconstruction domain evolves with the advancement of event driven fusion.
Diverse topics are covered, with in-depth discussions on recent works for
improving visual quality under challenging conditions. Additionally, the survey
compiles a comprehensive list of openly available datasets, enabling
reproducible research and benchmarking. By consolidating recent progress and
insights, this survey aims to inspire further research into leveraging event
camera systems, especially in combination with deep learning, for advanced
visual media restoration and enhancement.

</details>


### [26] [ISTASTrack: Bridging ANN and SNN via ISTA Adapter for RGB-Event Tracking](https://arxiv.org/abs/2509.09977)
*Siying Liu,Zikai Wang,Hanle Zheng,Yifan Hu,Xilin Wang,Qingkai Yang,Jibin Wu,Hao Guo,Lei Deng*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的RGB-Event视觉跟踪方法ISTASTrack，将传统人工神经网络（ANN）与脉冲神经网络（SNN）结合，利用Transformer结构，有效融合两类异构特征，实现了对动态目标的高效、鲁棒跟踪，取得了当前最优性能，并兼具能效优势。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-Event视觉跟踪方法难以充分利用事件流的稀疏和异步特性，经典ANN在特征融合方面存在瓶颈，而混合ANN-SNN架构尚未有效解决异构特征互通交互问题，影响融合效果与跟踪性能。

Method: 作者提出了基于Transformer的ANN-SNN双分支架构：一支使用视觉Transformer提取RGB图像的空间上下文，一支使用脉冲Transformer捕获事件流的时空动态。核心创新点是基于稀疏表示理论和ISTA算法设计的适配器，支持两分支特征的双向互动融合，并配备时序下采样注意力，解决SNN多步与ANN单步特征对齐问题。

Result: 在FE240hz、VisEvent、COESOT和FELT等RGB-Event跟踪基准上一致获得SOTA性能，且能耗低、效率高，具备工程落地优势。

Conclusion: ISTASTrack验证了ANN-SNN混合架构和创新融合机制在RGB-Event视觉跟踪中的有效性和实用性，为提升目标跟踪的鲁棒性和能效提供了新范式。

Abstract: RGB-Event tracking has become a promising trend in visual object tracking to
leverage the complementary strengths of both RGB images and dynamic spike
events for improved performance. However, existing artificial neural networks
(ANNs) struggle to fully exploit the sparse and asynchronous nature of event
streams. Recent efforts toward hybrid architectures combining ANNs and spiking
neural networks (SNNs) have emerged as a promising solution in RGB-Event
perception, yet effectively fusing features across heterogeneous paradigms
remains a challenge. In this work, we propose ISTASTrack, the first
transformer-based \textbf{A}NN-\textbf{S}NN hybrid \textbf{Track}er equipped
with \textbf{ISTA} adapters for RGB-Event tracking. The two-branch model
employs a vision transformer to extract spatial context from RGB inputs and a
spiking transformer to capture spatio-temporal dynamics from event streams. To
bridge the modality and paradigm gap between ANN and SNN features, we
systematically design a model-based ISTA adapter for bidirectional feature
interaction between the two branches, derived from sparse representation theory
by unfolding the iterative shrinkage thresholding algorithm. Additionally, we
incorporate a temporal downsampling attention module within the adapter to
align multi-step SNN features with single-step ANN features in the latent
space, improving temporal fusion. Experimental results on RGB-Event tracking
benchmarks, such as FE240hz, VisEvent, COESOT, and FELT, have demonstrated that
ISTASTrack achieves state-of-the-art performance while maintaining high energy
efficiency, highlighting the effectiveness and practicality of hybrid ANN-SNN
designs for robust visual tracking. The code is publicly available at
https://github.com/lsying009/ISTASTrack.git.

</details>


### [27] [FLARE-SSM: Deep State Space Models with Influence-Balanced Loss for 72-Hour Solar Flare Prediction](https://arxiv.org/abs/2509.09988)
*Yusuke Takagi,Shunya Nagashima,Komei Sugiura*

Main category: cs.CV

TL;DR: 本文提出了一种基于多重深度状态空间模型和新型损失函数（FLARE loss）的太阳耀斑分类预测模型，在对抗类别不平衡的情况下提高了72小时内最大耀斑等级预测的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前太阳耀斑预测精度不足，特别是在不同耀斑等级分布极度不均衡的情况下，严重影响实际预警应用效果。为保障关键基础设施的安全，需要提升预测方法对类别不平衡数据的处理能力和总体性能。

Method: 作者提出采用多种深度状态空间模型相结合进行太阳耀斑分类预测，并引入全新损失函数——FLARE loss，使模型在面对类别极度不平衡数据时，能够增强少数类的预测表现，从而提升整体准确性和可靠性。

Result: 在覆盖整个11年太阳活动周期的多波段太阳图像数据集上实验，提出的方法在Gandin-Murphy-Gerrity评分和True Skill Statistic等关键指标上优于现有基线方法。

Conclusion: 基于多深度状态空间模型和FLARE loss的新方法能够显著提升不同耀斑等级的分类预测准确性和可靠性，尤其改善了在类别不平衡条件下的模型实用性，对实际空间天气预警具有潜在应用价值。

Abstract: Accurate and reliable solar flare predictions are essential to mitigate
potential impacts on critical infrastructure. However, the current performance
of solar flare forecasting is insufficient. In this study, we address the task
of predicting the class of the largest solar flare expected to occur within the
next 72 hours. Existing methods often fail to adequately address the severe
class imbalance across flare classes. To address this issue, we propose a solar
flare prediction model based on multiple deep state space models. In addition,
we introduce the frequency & local-boundary-aware reliability loss (FLARE loss)
to improve predictive performance and reliability under class imbalance.
Experiments were conducted on a multi-wavelength solar image dataset covering a
full 11-year solar activity cycle. As a result, our method outperformed
baseline approaches in terms of both the Gandin-Murphy-Gerrity score and the
true skill statistic, which are standard metrics in terms of the performance
and reliability.

</details>


### [28] [TUNI: Real-time RGB-T Semantic Segmentation with Unified Multi-Modal Feature Extraction and Cross-Modal Feature Fusion](https://arxiv.org/abs/2509.10005)
*Xiaodong Guo,Tong Liu,Yike Li,Zi'ang Lin,Zhihong Deng*

Main category: cs.CV

TL;DR: 本文提出TUNI模型，通过统一式编码器实现RGB与热红外图像的高效语义分割，在保持高精度的同时提升实时性能。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-T语义分割模型普遍采用以RGB图像预训练的编码器分别处理RGB和热红外图像，并通过额外模块融合多模态特征，这种做法存在热红外特征提取效果有限、跨模态融合不理想以及模型推理效率低下等问题，亟需更有效和高效的解决方案。

Method: 提出TUNI模型，通过堆叠多个块构建RGB-T编码器，实现多模态特征提取与融合一体化，并结合大规模RGB及伪热数据联合预训练，提升融合与表示能力。模型缩减了热分支，结构更紧凑。并设计RGB-T local模块，利用自适应余弦相似性加强局部跨模态特征融合。

Result: TUNI模型在FMB、PST900、CART等数据集上取得与最新方法相竞争的性能，参数量和计算量更低。在Jetson Orin NX上达到27 FPS的推理速度，展现出良好的实时部署能力。

Conclusion: TUNI通过统一编码与简化结构，实现了高效的RGB-T语义分割，兼顾准确率、模型紧凑性与推理速度，对自动化平台环境感知具有实际应用价值。

Abstract: RGB-thermal (RGB-T) semantic segmentation improves the environmental
perception of autonomous platforms in challenging conditions. Prevailing models
employ encoders pre-trained on RGB images to extract features from both RGB and
infrared inputs, and design additional modules to achieve cross-modal feature
fusion. This results in limited thermal feature extraction and suboptimal
cross-modal fusion, while the redundant encoders further compromises the
model's real-time efficiency. To address the above issues, we propose TUNI,
with an RGB-T encoder consisting of multiple stacked blocks that simultaneously
perform multi-modal feature extraction and cross-modal fusion. By leveraging
large-scale pre-training with RGB and pseudo-thermal data, the RGB-T encoder
learns to integrate feature extraction and fusion in a unified manner. By
slimming down the thermal branch, the encoder achieves a more compact
architecture. Moreover, we introduce an RGB-T local module to strengthen the
encoder's capacity for cross-modal local feature fusion. The RGB-T local module
employs adaptive cosine similarity to selectively emphasize salient consistent
and distinct local features across RGB-T modalities. Experimental results show
that TUNI achieves competitive performance with state-of-the-art models on FMB,
PST900 and CART, with fewer parameters and lower computational cost. Meanwhile,
it achieves an inference speed of 27 FPS on a Jetson Orin NX, demonstrating its
real-time capability in deployment. Codes are available at
https://github.com/xiaodonguo/TUNI.

</details>


### [29] [Efficient and Accurate Downfacing Visual Inertial Odometry](https://arxiv.org/abs/2509.10021)
*Jonas Kühne,Christian Vogt,Michele Magno,Luca Benini*

Main category: cs.CV

TL;DR: 本文提出了一种高效且精确的视觉惯性测程（VIO）方案，适用于微型和纳米级无人机，在超低功耗RISC-V SoC上实现，并大幅提升了性能与准确度。


<details>
  <summary>Details</summary>
Motivation: 当前高精度VIO系统多运行于高性能计算平台，难以应用于微控制器等资源受限的设备。因此，急需一种能够在低功耗硬件上高效运行的VIO方案，以支持微型无人机等对体积和能耗有较高要求的场景。

Method: 设计了一套针对超低功耗RISC-V SoC优化和量化的VIO流水线，融合先进的特征检测和跟踪方法（SuperPoint、PX4FLOW、ORB）并采用刚体运动模型以降低平面运动估计误差。将主流特征跟踪算法集成并实现实时运行，进行了量化优化，评估了算法精度与计算资源消耗。

Result: 经实机验证，优化后的方案在GAP9 SoC上，搭配ORB特征跟踪，可将RMSE平均降低3.65倍。PX4FLOW在移动速度低于24像素/帧时，运算用时更低且精度与ORB相当。

Conclusion: 该方案兼顾了高精度与低功耗，缩小了传统高精度VIO与微控制器端轻量级实现之间的差距，适用于资源受限场景下的实时定位需求。

Abstract: Visual Inertial Odometry (VIO) is a widely used computer vision method that
determines an agent's movement through a camera and an IMU sensor. This paper
presents an efficient and accurate VIO pipeline optimized for applications on
micro- and nano-UAVs. The proposed design incorporates state-of-the-art feature
detection and tracking methods (SuperPoint, PX4FLOW, ORB), all optimized and
quantized for emerging RISC-V-based ultra-low-power parallel systems on chips
(SoCs). Furthermore, by employing a rigid body motion model, the pipeline
reduces estimation errors and achieves improved accuracy in planar motion
scenarios. The pipeline's suitability for real-time VIO is assessed on an
ultra-low-power SoC in terms of compute requirements and tracking accuracy
after quantization. The pipeline, including the three feature tracking methods,
was implemented on the SoC for real-world validation. This design bridges the
gap between high-accuracy VIO pipelines that are traditionally run on
computationally powerful systems and lightweight implementations suitable for
microcontrollers. The optimized pipeline on the GAP9 low-power SoC demonstrates
an average reduction in RMSE of up to a factor of 3.65x over the baseline
pipeline when using the ORB feature tracker. The analysis of the computational
complexity of the feature trackers further shows that PX4FLOW achieves on-par
tracking accuracy with ORB at a lower runtime for movement speeds below 24
pixels/frame.

</details>


### [30] [Few-Part-Shot Font Generation](https://arxiv.org/abs/2509.10006)
*Masaki Akiba,Shumpei Takezaki,Daichi Haraguchi,Seiichi Uchida*

Main category: cs.CV

TL;DR: 本文提出了一种新的少量部分设计元素驱动的字体生成模型，可以仅用部分字符形状就设计全新的完整字体。


<details>
  <summary>Details</summary>
Motivation: 传统的少量样本字体生成方法通常需要完整字符作为输入，而现实中获取完整样本难度较大，作者希望用更少的信息（只需部分字符形状）高效生成新字体，降低设计门槛。

Method: 只需提供部分设计元素（部分字符形状）作为输入，模型通过推断将其扩展为整套字体。模型关注设计细节如何影响整体字形结构。

Result: 实验证明，该方法能高效生成拥有一致风格的完整字体，同时揭示了部分设计细节对整体结构的影响。

Conclusion: 提出的方法显著提高字体生成的效率，无需完整字符输入即可实现高质量字体设计，也为理解字体设计的局部与整体关系提供了新见解。

Abstract: This paper proposes a novel model of few-part-shot font generation, which
designs an entire font based on a set of partial design elements, i.e., partial
shapes. Unlike conventional few-shot font generation, which requires entire
character shapes for a couple of character classes, our approach only needs
partial shapes as input. The proposed model not only improves the efficiency of
font creation but also provides insights into how partial design details
influence the entire structure of the individual characters.

</details>


### [31] [Hierarchical MLANet: Multi-level Attention for 3D Face Reconstruction From Single Images](https://arxiv.org/abs/2509.10024)
*Danling Cao*

Main category: cs.CV

TL;DR: 本文提出了一种基于多层次注意力机制的层次化卷积神经网络MLANet，用于从单张真实世界2D人脸图像重建3D人脸模型，并在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 2D转3D人脸重建因其在现实应用中的广阔前景引发广泛关注，但缺乏带标注的真实数据集和实际环境的复杂性使任务极具挑战。作者旨在突破这一难题，提高2D in-the-wild图像的人脸3D重建准确性和鲁棒性。

Method: 提出MLANet网络，采用预训练的层次化主干网络，并在2D特征提取不同阶段引入多层次注意力机制。训练时结合3DMM参数、区分可微渲染器，采用半监督端到端训练方式。

Result: 在AFLW2000-3D和MICC Florence数据集上进行了广泛实验，包括对比实验和消融实验。在3D人脸重建与3D人脸对齐任务上，MLANet在定量和定性评测中均表现优异，超过了现有方法。

Conclusion: MLANet有效提升了单张2D in-the-wild人脸图像的3D重建精度和对齐能力，证明了多层次注意力和半监督方法的有效性。

Abstract: Recovering 3D face models from 2D in-the-wild images has gained considerable
attention in the computer vision community due to its wide range of potential
applications. However, the lack of ground-truth labeled datasets and the
complexity of real-world environments remain significant challenges. In this
chapter, we propose a convolutional neural network-based approach, the
Hierarchical Multi-Level Attention Network (MLANet), for reconstructing 3D face
models from single in-the-wild images. Our model predicts detailed facial
geometry, texture, pose, and illumination parameters from a single image.
Specifically, we employ a pre-trained hierarchical backbone network and
introduce multi-level attention mechanisms at different stages of 2D face image
feature extraction. A semi-supervised training strategy is employed,
incorporating 3D Morphable Model (3DMM) parameters from publicly available
datasets along with a differentiable renderer, enabling an end-to-end training
process. Extensive experiments, including both comparative and ablation
studies, were conducted on two benchmark datasets, AFLW2000-3D and MICC
Florence, focusing on 3D face reconstruction and 3D face alignment tasks. The
effectiveness of the proposed method was evaluated both quantitatively and
qualitatively.

</details>


### [32] [LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA](https://arxiv.org/abs/2509.10026)
*Jing Huang,Zhiya Tan,Shutao Gong,Fanwei Zeng,Jianshu Li*

Main category: cs.CV

TL;DR: 本文提出了一种新型多语言视觉CoT（Chain-of-Thought）推理框架LaV-CoT，显著提升了多语言视觉问答的能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型在多语言视觉问答方面虽有进步，但现有方法主要依赖文本推理，缺乏对多语言多模态推理的支持，难以满足实际应用需求。

Method: LaV-CoT框架采用可解释的多阶段推理流程，包括带边界框的文本摘要、语言识别、空间目标级描述、逐步逻辑推理，并设计了自动化数据整理流程生成多语言CoT标注。训练方法结合了监督微调（SFT）和新颖的语言感知分组相对策略优化（GRPO），并采用涵盖多方面的可验证奖励（如语言一致性、结构准确性、语义对齐）进行优化。

Result: 在公开多语言视觉问答数据集上，LaV-CoT比同规模开源基线提升最高约9.5%准确率，超越2倍参数规模模型约2.6%。此外，性能超过GPT-4o-0513和Gemini-2.5-flash等先进闭源模型。在线A/B测试进一步验证其在真实工业场景下的有效性。

Conclusion: LaV-CoT为多语言视觉问答任务提供了更强与更可解释的推理能力，具备产业级应用前景，并用数据和实验验证了其优越性。

Abstract: As large vision language models (VLMs) advance, their capabilities in
multilingual visual question answering (mVQA) have significantly improved.
Chain-of-thought (CoT) reasoning has been proven to enhance interpretability
and complex reasoning. However, most existing approaches rely primarily on
textual CoT and provide limited support for multilingual multimodal reasoning,
constraining their deployment in real-world applications. To address this gap,
we introduce \textbf{LaV-CoT}, the first Language-aware Visual CoT framework
with Multi-Aspect Reward Optimization. LaV-CoT incorporates an interpretable
multi-stage reasoning pipeline consisting of Text Summary with Bounding Box
(BBox), Language Identification, Spatial Object-level Captioning, and
Step-by-step Logical Reasoning. Following this reasoning pipeline, we design an
automated data curation method that generates multilingual CoT annotations
through iterative generation, correction, and refinement, enabling scalable and
high-quality training data. To improve reasoning and generalization, LaV-CoT
adopts a two-stage training paradigm combining Supervised Fine-Tuning (SFT)
with Language-aware Group Relative Policy Optimization (GRPO), guided by
verifiable multi-aspect rewards including language consistency, structural
accuracy, and semantic alignment. Extensive evaluations on public datasets
including MMMB, Multilingual MMBench, and MTVQA show that LaV-CoT achieves up
to \(\sim\)9.5\% accuracy improvements over open-source baselines of similar
size and even surpasses models with 2$\times$ larger scales by \(\sim\)2.6\%.
Moreover, LaV-CoT outperforms advanced proprietary models such as GPT-4o-0513
and Gemini-2.5-flash. We further conducted an online A/B test to validate our
method on real-world data, highlighting its effectiveness for industrial
deployment. Our code is available at this link:
\href{https://github.com/HJNVR/LaV-CoT}

</details>


### [33] [Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation](https://arxiv.org/abs/2509.10058)
*Sung-Lin Tsai,Bo-Lun Huang,Yu Ting Shen,Cheng Yu Yeo,Chiang Tseng,Bo-Kai Ruan,Wen-Sheng Lien,Hong-Han Shuai*

Main category: cs.CV

TL;DR: 提出了一种训练无关的方法，用于提高文本生成图像任务中的颜色对齐精度，特别针对复杂和歧义颜色词。方法利用大型语言模型消解歧义，并通过文本嵌入空间进行颜色指导。无需额外训练或参考图像，实现了更高的颜色准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本生成图像扩散模型在处理细致、复杂的颜色描述时表现不佳，常出现与人类意图不符的颜色呈现。现有方法如交叉注意力调整、参考图像或微调，均未能有效消除歧义性的颜色描述。本工作旨在解决复杂和歧义颜色词带来的生成失配问题。

Method: 本方法无需再训练，先用大型语言模型消解文本中颜色相关词语的歧义，再根据CIELAB色彩空间中颜色词的空间关系，直接对文本嵌入进行颜色混合操作，对生成模型提供更精确的颜色指导。

Result: 实验结果表明，该框架在提升颜色对齐准确性的同时并未降低图像质量，明显缩小了文本语义和视觉生成之间的颜色差距。

Conclusion: 该方法有效提升了文本到图像生成中的颜色准确性，不依赖额外训练或参考图像，为复杂色彩描述的视觉生成提供了更强的技术支撑。

Abstract: Accurate color alignment in text-to-image (T2I) generation is critical for
applications such as fashion, product visualization, and interior design, yet
current diffusion models struggle with nuanced and compound color terms (e.g.,
Tiffany blue, lime green, hot pink), often producing images that are misaligned
with human intent. Existing approaches rely on cross-attention manipulation,
reference images, or fine-tuning but fail to systematically resolve ambiguous
color descriptions. To precisely render colors under prompt ambiguity, we
propose a training-free framework that enhances color fidelity by leveraging a
large language model (LLM) to disambiguate color-related prompts and guiding
color blending operations directly in the text embedding space. Our method
first employs a large language model (LLM) to resolve ambiguous color terms in
the text prompt, and then refines the text embeddings based on the spatial
relationships of the resulting color terms in the CIELAB color space. Unlike
prior methods, our approach improves color accuracy without requiring
additional training or external reference images. Experimental results
demonstrate that our framework improves color alignment without compromising
image quality, bridging the gap between text semantics and visual generation.

</details>


### [34] [Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery: Benchmarking, Analysis, and Exploration](https://arxiv.org/abs/2509.10059)
*Yue Zhou,Litong Feng,Mengcheng Lan,Xue Yang,Qingyun Li,Yiping Ke,Xue Jiang,Wayne Zhang*

Main category: cs.CV

TL;DR: 本文提出了AVI-Math数据集，用于评测无人机遥感图像下的多模态数学推理能力，发现当前主流视觉-语言模型（VLMs）在该任务上表现较差。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型虽能处理多模态任务，但其在无人机视角下的高阶数学推理能力尚未被系统评测。遥感领域尤其依赖精确的数学计算与空间推理能力，针对这一应用痛点，作者提出新的基准数据集。

Method: 构建AVI-Math数据集，包含3,773道问题，涵盖几何、逻辑和代数等6大数学类别、20个细分主题，多角度、多高度的无人机图像保证了数据的丰富性和真实性。对14个主流VLMs进行了基准测试，并探索了链式思考提示（Chain-of-Thought）和微调等改进方法。

Result: 实验表明，现有VLMs尽管在其他多模态基准任务中表现出色，但在AVI-Math上的数学推理表现较差。链式思考与微调方法在一定程度上提升了模型的推理能力。

Conclusion: 当前VLMs在无人机遥感图像下的复杂数学推理能力有限。AVI-Math基准为促进相关研究和提升模型在实际无人机应用中的可靠性提供了新方向。

Abstract: Mathematical reasoning is critical for tasks such as precise distance and
area computations, trajectory estimations, and spatial analysis in unmanned
aerial vehicle (UAV) based remote sensing, yet current vision-language models
(VLMs) have not been adequately tested in this domain. To address this gap, we
introduce AVI-Math, the first benchmark to rigorously evaluate multimodal
mathematical reasoning in aerial vehicle imagery, moving beyond simple counting
tasks to include domain-specific knowledge in areas such as geometry, logic,
and algebra. The dataset comprises 3,773 high-quality vehicle-related questions
captured from UAV views, covering 6 mathematical subjects and 20 topics. The
data, collected at varying altitudes and from multiple UAV angles, reflects
real-world UAV scenarios, ensuring the diversity and complexity of the
constructed mathematical problems. In this paper, we benchmark 14 prominent
VLMs through a comprehensive evaluation and demonstrate that, despite their
success on previous multimodal benchmarks, these models struggle with the
reasoning tasks in AVI-Math. Our detailed analysis highlights significant
limitations in the mathematical reasoning capabilities of current VLMs and
suggests avenues for future research. Furthermore, we explore the use of
Chain-of-Thought prompting and fine-tuning techniques, which show promise in
addressing the reasoning challenges in AVI-Math. Our findings not only expose
the limitations of VLMs in mathematical reasoning but also offer valuable
insights for advancing UAV-based trustworthy VLMs in real-world applications.
The code, and datasets will be released at
https://github.com/VisionXLab/avi-math

</details>


### [35] [BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird's-Eye View with Deformable Attention and Sparse Goal Proposals](https://arxiv.org/abs/2509.10080)
*Minsang Kong,Myeongjun Kim,Sang Gu Kang,Sang Hun Lee*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的自动驾驶轨迹预测方法BEVTraj，无需依赖预建高清地图即可实现高效、准确的轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 当前主流方法高度依赖预建高清地图，但这些地图区域有限且难以适应环境变化，而基于实时局部建图的方法容易遗漏细节或产生误差，影响预测精度。因此，需要一种既能准确感知环境，又能适应变化的新方法。

Method: BEVTraj基于鸟瞰视角，直接处理实时传感器数据。方法采用可变形注意力机制从密集BEV特征中高效提取上下文信息，并引入Sparse Goal Candidate Proposal(SGCP)模块，能够端到端输出轨迹预测，无需额外后处理。

Result: 实验结果显示，BEVTraj在去除对高清地图依赖的情况下，预测性能与现有最优模型相当，同时表现出更高的灵活性和泛化能力。

Conclusion: BEVTraj为自动驾驶轨迹预测提供了一条无需地图依赖的新路径，兼具高性能与适应性的优势。

Abstract: In autonomous driving, trajectory prediction is essential for ensuring safe
and efficient navigation. To improve prediction accuracy, recent approaches
often rely on pre-built high-definition (HD) maps or real-time local map
construction modules to incorporate static environmental information. However,
pre-built HD maps are limited to specific regions and cannot adapt to transient
changes. In addition, local map construction modules, which recognize only
predefined elements, may fail to capture critical scene details or introduce
errors that degrade prediction performance. To overcome these limitations, we
propose Bird's-Eye View Trajectory Prediction (BEVTraj), a novel trajectory
prediction framework that operates directly in the bird's-eye view (BEV) space
utilizing real-time sensor data without relying on any pre-built maps. The
BEVTraj leverages deformable attention to efficiently extract relevant context
from dense BEV features. Furthermore, we introduce a Sparse Goal Candidate
Proposal (SGCP) module, which enables full end-to-end prediction without
requiring any post-processing steps. Extensive experiments demonstrate that the
BEVTraj achieves performance comparable to state-of-the-art HD map-based models
while offering greater flexibility by eliminating the dependency on pre-built
maps. The source code is available at https://github.com/Kongminsang/bevtraj.

</details>


### [36] [Leveraging Multi-View Weak Supervision for Occlusion-Aware Multi-Human Parsing](https://arxiv.org/abs/2509.10093)
*Laura Bragagnolo,Matteo Terreran,Leonardo Barcellona,Stefano Ghidoni*

Main category: cs.CV

TL;DR: 该论文提出利用多视角信息提升多人解析（multi-human parsing）在遮挡场景下的表现，通过半自动标注策略与多视角一致性损失，在遮挡条件下显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有最先进的多人解析方法在公开数据集上取得了不错的成绩，但在分割互相遮挡的人体时存在明显困难。作者认为，不同视角下遮挡人体可能变得可分离，因此多视角信息可用于提升遮挡下的人体解析效果。

Method: 设计了一种新颖的训练框架，将多视角知识融入训练，采用了对人体实例的弱监督以及多视角一致性损失函数。同时，针对缺乏合适数据集的问题，提出了基于多视角RGB+D数据和三维人体骨架的半自动标注策略，生成高质量的人体实例分割掩码。

Result: 实验证明，该方法在遮挡场景下相较于基础模型，最多可以带来4.2%的相对性能提升。

Conclusion: 利用多视角信息和新型标注机制能够有效提升多人解析模型在人体遮挡条件下的表现，对于精细化人体理解具有实际应用价值。

Abstract: Multi-human parsing is the task of segmenting human body parts while
associating each part to the person it belongs to, combining instance-level and
part-level information for fine-grained human understanding. In this work, we
demonstrate that, while state-of-the-art approaches achieved notable results on
public datasets, they struggle considerably in segmenting people with
overlapping bodies. From the intuition that overlapping people may appear
separated from a different point of view, we propose a novel training framework
exploiting multi-view information to improve multi-human parsing models under
occlusions. Our method integrates such knowledge during the training process,
introducing a novel approach based on weak supervision on human instances and a
multi-view consistency loss. Given the lack of suitable datasets in the
literature, we propose a semi-automatic annotation strategy to generate human
instance segmentation masks from multi-view RGB+D data and 3D human skeletons.
The experiments demonstrate that the approach can achieve up to a 4.20\%
relative improvement on human parsing over the baseline model in occlusion
scenarios.

</details>


### [37] [VARCO-VISION-2.0 Technical Report](https://arxiv.org/abs/2509.10105)
*Young-rok Cha,Jeongho Ju,SunYoung Park,Jong-Hyeon Lee,Younghyun Yu,Youngjune Kim*

Main category: cs.CV

TL;DR: 本文推出了VARCO-VISION-2.0，一款韩英双语开源视觉-语言大模型，支持多图像与文档、图表、表格等复杂输入，并具备布局感知OCR能力。提供14B和1.7B两个版本，分别面向云端和端侧设备。模型在多模态对齐、语言能力和安全性上均有提升，在OpenCompass VLM排行榜上表现突出。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言大模型在多语言、多图像复杂场景、布局感知OCR等方面存在不足，且缺乏面向端侧和开源的高质量韩英双语VLM。研究动机在于满足这些应用场景需求，推动多语言VLM实用化。

Method: 采用分阶段课程训练和高效显存利用技术，提升模型多模态对齐能力且保留语言能力，通过偏好优化提升安全性。模型支持预测文字及其空间位置（布局感知OCR），并针对复杂输入如文档、表格、图表优化。

Result: 模型在多个基准测试中表现优异，具备良好空间识别能力和双语表现。14B版本在OpenCompass VLM排行榜上取得同等规模模型第八名，1.7B版本适合设备端部署。

Conclusion: VARCO-VISION-2.0推动了韩英双语VLM的发展及其实际应用，且通过开源两个版本（14B和1.7B），促进学术和产业界在多模态双语AI领域的创新与落地。

Abstract: We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model
(VLM) for Korean and English with improved capabilities compared to the
previous model VARCO-VISION-14B. The model supports multi-image understanding
for complex inputs such as documents, charts, and tables, and delivers
layoutaware OCR by predicting both textual content and its spatial location.
Trained with a four-stage curriculum with memory-efficient techniques, the
model achieves enhanced multimodal alignment, while preserving core language
abilities and improving safety via preference optimization. Extensive benchmark
evaluations demonstrate strong spatial grounding and competitive results for
both languages, with the 14B model achieving 8th place on the OpenCompass VLM
leaderboard among models of comparable scale. Alongside the 14B-scale model, we
release a 1.7B version optimized for on-device deployment. We believe these
models advance the development of bilingual VLMs and their practical
applications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a
full-scale 14B model and a lightweight 1.7B model.

</details>


### [38] [A Lightweight Ensemble-Based Face Image Quality Assessment Method with Correlation-Aware Loss](https://arxiv.org/abs/2509.10114)
*MohammadAli Hamidi,Hadi Amirpour,Luigi Atzori,Christian Timmerer*

Main category: cs.CV

TL;DR: 提出了一种高效轻量的人脸图像质量评估方法，结合了两种小型CNN和相关感知损失，在准确率和计算效率间取得了良好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有无参考图像质量评估方法无法有效捕捉人脸特有的降质现象，而最新的人脸图像质量评估方法又常常计算量大，难以实际应用。因此，亟需一种同时兼顾准确度和效率的人脸图像质量评估方法。

Method: 采用MobileNetV3-Small和ShuffleNetV2两种小型卷积神经网络，通过预测级简单平均融合结果。同时，引入MSECorrLoss损失函数，将均方误差和皮尔逊相关正则项结合，以增强模型与人类感知的一致性。

Result: 在VQualA FIQA基准上，模型取得了SRCC 0.9829和PLCC 0.9894的优异成绩，同时保持在竞赛要求的计算效率限制范围之内。

Conclusion: 该方法能够高效准确地评估实际应用场景下的人脸图像质量，兼顾精度与效率，适合真实场景部署。

Abstract: Face image quality assessment (FIQA) plays a critical role in face
recognition and verification systems, especially in uncontrolled, real-world
environments. Although several methods have been proposed, general-purpose
no-reference image quality assessment techniques often fail to capture
face-specific degradations. Meanwhile, state-of-the-art FIQA models tend to be
computationally intensive, limiting their practical applicability. We propose a
lightweight and efficient method for FIQA, designed for the perceptual
evaluation of face images in the wild. Our approach integrates an ensemble of
two compact convolutional neural networks, MobileNetV3-Small and ShuffleNetV2,
with prediction-level fusion via simple averaging. To enhance alignment with
human perceptual judgments, we employ a correlation-aware loss (MSECorrLoss),
combining mean squared error (MSE) with a Pearson correlation regularizer. Our
method achieves a strong balance between accuracy and computational cost,
making it suitable for real-world deployment. Experiments on the VQualA FIQA
benchmark demonstrate that our model achieves a Spearman rank correlation
coefficient (SRCC) of 0.9829 and a Pearson linear correlation coefficient
(PLCC) of 0.9894, remaining within competition efficiency constraints.

</details>


### [39] [Realism Control One-step Diffusion for Real-World Image Super-Resolution](https://arxiv.org/abs/2509.10122)
*Zongliang Wu,Siming Zheng,Peng-Tao Jiang,Xin Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的一步扩散框架（RCOD），通过灵活控制真实感与保真度，实现了更高效和高质量的真实图像超分辨率重建，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的一步扩散超分辨模型虽然提高了推理效率，但无法像多步扩散那样灵活地在保真与真实感之间进行权衡，导致结果在不同场景下表现有限。这个问题源于一步方法通常只依据单一定义的时间步训练，缺乏调控机制。

Method: 作者提出了RCOD（Realism Controlled One-step Diffusion）框架，包括三大创新：1）通过潜域分组显式控制噪声预测阶段保真-真实感平衡，2）引入与分组策略对齐的退化感知采样方法以提升权衡控制力，3）用视觉提示注入模块以退化感知视觉Token替换文本提示，从而提升恢复准确度与语义一致性。这些改进基于微小训练修改与原始数据即可实现。

Result: 实验表明，RCOD在真实图像超分辨率任务上无论在定量指标还是视觉质量上均明显优于最新的一步扩散方法，并在推理阶段可灵活调控输出的真实感。

Conclusion: RCOD通过一系列创新解决了一步扩散方法在真实图像超分领域的灵活性和表现不足的问题，在保证高效率的同时提升了保真度与真实感，具有良好的应用潜力。代码即将开源。

Abstract: Pre-trained diffusion models have shown great potential in real-world image
super-resolution (Real-ISR) tasks by enabling high-resolution reconstructions.
While one-step diffusion (OSD) methods significantly improve efficiency
compared to traditional multi-step approaches, they still have limitations in
balancing fidelity and realism across diverse scenarios. Since the OSDs for SR
are usually trained or distilled by a single timestep, they lack flexible
control mechanisms to adaptively prioritize these competing objectives, which
are inherently manageable in multi-step methods through adjusting sampling
steps. To address this challenge, we propose a Realism Controlled One-step
Diffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping
strategy that enables explicit control over fidelity-realism trade-offs during
the noise prediction phase with minimal training paradigm modifications and
original training data. A degradation-aware sampling strategy is also
introduced to align distillation regularization with the grouping strategy and
enhance the controlling of trade-offs. Moreover, a visual prompt injection
module is used to replace conventional text prompts with degradation-aware
visual tokens, enhancing both restoration accuracy and semantic consistency.
Our method achieves superior fidelity and perceptual quality while maintaining
computational efficiency. Extensive experiments demonstrate that RCOD
outperforms state-of-the-art OSD methods in both quantitative metrics and
visual qualities, with flexible realism control capabilities in the inference
stage. The code will be released.

</details>


### [40] [Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment](https://arxiv.org/abs/2509.10134)
*Rini Smita Thakur,Rajeev Ranjan Dwivedi,Vinod K Kurmi*

Main category: cs.CV

TL;DR: 本文提出了一种名为Grad-CL的全新源无关领域自适应方法，有效提升了视盘和视杯的分割性能，尤其在跨域眼底图像数据上表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有分割模型在从一个数据集迁移到另一个成像协议或条件下常常性能显著下降。需要无需源数据、仅用无标签目标数据提升分割模型的自适应能力，以满足实际应用需求。

Method: Grad-CL结合了梯度引导的伪标签细化模块和基于余弦相似度的对比学习。第一阶段通过梯度机制提取类特异性特征，实现伪标签的不确定性衡量和原型估计，从而优化伪标签。第二阶段采用对比损失增强视盘与视杯特征的判别性，提升分割效果。

Result: 在多个具有挑战性的跨域眼底图像数据集上，大量实验表明Grad-CL优于最新的无监督和源数据不可用的领域自适应方法，在分割精度及边界识别上取得最优。

Conclusion: Grad-CL实现了无需访问源数据的高效领域自适应视盘/视杯分割，对实际临床应用和更广泛的领域自适应问题具有重要意义。

Abstract: Accurate segmentation of the optic disc and cup is critical for the early
diagnosis and management of ocular diseases such as glaucoma. However,
segmentation models trained on one dataset often suffer significant performance
degradation when applied to target data acquired under different imaging
protocols or conditions. To address this challenge, we propose
\textbf{Grad-CL}, a novel source-free domain adaptation framework that
leverages a pre-trained source model and unlabeled target data to robustly
adapt segmentation performance without requiring access to the original source
data. Grad-CL combines a gradient-guided pseudolabel refinement module with a
cosine similarity-based contrastive learning strategy. In the first stage,
salient class-specific features are extracted via a gradient-based mechanism,
enabling more accurate uncertainty quantification and robust prototype
estimation for refining noisy pseudolabels. In the second stage, a contrastive
loss based on cosine similarity is employed to explicitly enforce inter-class
separability between the gradient-informed features of the optic cup and disc.
Extensive experiments on challenging cross-domain fundus imaging datasets
demonstrate that Grad-CL outperforms state-of-the-art unsupervised and
source-free domain adaptation methods, achieving superior segmentation accuracy
and improved boundary delineation. Project and code are available at
https://visdomlab.github.io/GCL/.

</details>


### [41] [Scalable Training for Vector-Quantized Networks with 100% Codebook Utilization](https://arxiv.org/abs/2509.10140)
*Yifan Chang,Jie Qin,Limeng Qiao,Xiaofeng Wang,Zheng Zhu,Lin Ma,Xingang Wang*

Main category: cs.CV

TL;DR: 本文分析了图像生成中向量量化（VQ）核心技术面临的不稳定性问题，提出了VQBridge方法，有效提升了VQ网络的码本利用率与重构表现。


<details>
  <summary>Details</summary>
Motivation: 当前图像生成领域常用的离散分词器依赖VQ，但其训练存在估计偏差、梯度稀疏等问题，导致码本利用率低和重构性能不佳。因此，亟需提升VQ稳定性与效率的解决方案。

Method: 作者提出VQBridge方法，通过基于映射函数的高效投影器，将码向量优化为压缩-处理-恢复三阶段流程，并结合学习退火，实现VQ网络在扩展或温度变化下高码本利用率。该方法适配多种码本配置，并与现有生成模型如LlamaGen结合。

Result: 实验显示，FVQ（即满利用码本的VQBridge VQN）在多种码本设置下均可达到100%码本利用率，即使在超大262k码本规模下也有效。其重构性能超过现有技术，支持更大码本、更高通道数与更长训练，一致提升结果。与LlamaGen集成后，图像生成结果超过视觉自回归方法0.5 rFID和扩散模型0.2 rFID。

Conclusion: VQBridge方法能够有效解决VQ训练不稳定和码本利用率低的问题，大幅提升图像生成分词器品质，对大规模自回归图像生成具有重要意义。

Abstract: Vector quantization (VQ) is a key component in discrete tokenizers for image
generation, but its training is often unstable due to straight-through
estimation bias, one-step-behind updates, and sparse codebook gradients, which
lead to suboptimal reconstruction performance and low codebook usage. In this
work, we analyze these fundamental challenges and provide a simple yet
effective solution. To maintain high codebook usage in VQ networks (VQN) during
learning annealing and codebook size expansion, we propose VQBridge, a robust,
scalable, and efficient projector based on the map function method. VQBridge
optimizes code vectors through a compress-process-recover pipeline, enabling
stable and effective codebook training. By combining VQBridge with learning
annealing, our VQN achieves full (100%) codebook usage across diverse codebook
configurations, which we refer to as FVQ (FullVQ). Through extensive
experiments, we demonstrate that FVQ is effective, scalable, and generalizable:
it attains 100% codebook usage even with a 262k-codebook, achieves
state-of-the-art reconstruction performance, consistently improves with larger
codebooks, higher vector channels, or longer training, and remains effective
across different VQ variants. Moreover, when integrated with LlamaGen, FVQ
significantly enhances image generation performance, surpassing visual
autoregressive models (VAR) by 0.5 and diffusion models (DiT) by 0.2 rFID,
highlighting the importance of high-quality tokenizers for strong
autoregressive image generation.

</details>


### [42] [LayerLock: Non-collapsing Representation Learning with Progressive Freezing](https://arxiv.org/abs/2509.10156)
*Goker Erdogan,Nikhil Parthasarathy,Catalin Ionescu,Drew Hudson,Alexander Lerchner,Andrew Zisserman,Mehdi Sajjadi,Joao Carreira*

Main category: cs.CV

TL;DR: 提出了一种通过逐步冻结视觉Transformer层的自监督视觉表征学习方法LayerLock，能加速训练并提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统的自监督视觉表征学习如MAE，在训练过程中不同深度的ViT层收敛速度不同，训练整体效率和学习效果受限。如何提升训练效率并避免表征崩溃，是该研究关注的问题。

Method: 该方法通过分析MAE模型训练时ViT层的收敛顺序，制定显式的层冻结调度。训练时按计划逐步冻结已有层，使学习重心逐步由像素空间过渡到特征空间。该策略同时能防止表征坍塌问题。

Result: LayerLock在大模型（参数量高达4B）上进行了验证，在4DS感知数据集套件上，其效果优于传统的非潜变量掩码预测方法。

Conclusion: LayerLock是一种简单高效的自监督学习方法，通过层冻结策略提升了训练速度和模型表现，为大规模视觉模型自监督训练提供新思路。

Abstract: We introduce LayerLock, a simple yet effective approach for self-supervised
visual representation learning, that gradually transitions from pixel to latent
prediction through progressive layer freezing. First, we make the observation
that during training of video masked-autoencoding (MAE) models, ViT layers
converge in the order of their depth: shallower layers converge early, deeper
layers converge late. We then show that this observation can be exploited to
accelerate standard MAE by progressively freezing the model according to an
explicit schedule, throughout training. Furthermore, this same schedule can be
used in a simple and scalable approach to latent prediction that does not
suffer from "representation collapse". We apply our proposed approach,
LayerLock, to large models of up to 4B parameters with results surpassing those
of non-latent masked prediction on the 4DS perception suite.

</details>


### [43] [On the Geometric Accuracy of Implicit and Primitive-based Representations Derived from View Rendering Constraints](https://arxiv.org/abs/2509.10241)
*Elias De Smijter,Renaud Detry,Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: 本研究系统性比较了隐式和显式新视角合成方法在太空环境下三维重建的表现，特别分析了外观嵌入的作用。结果发现，外观嵌入提升了照片保真度，但对几何精度提升有限。凸斑点方法相比高斯斑点，在精简表达和去杂乱方面更优。


<details>
  <summary>Details</summary>
Motivation: 太空机器人任务对三维对象重建的几何精度要求极高，而现有新视角合成方法多侧重外观，还未充分系统对比隐式和显式方法在几何上的表现，尤其是外观嵌入对关键几何任务（如碰撞规避）的实际价值尚不明确。

Method: 作者使用SPEED+数据集，对K-Planes、高斯斑点和凸斑点三种方法进行对比，分析应用外观嵌入后，在照明变化建模和几何精度上的提升。同时比较了两种斑点技术在表达上的紧凑性和噪点杂乱度。

Result: 外观嵌入确实提升了外观保真度，尤其能减少显式方法所需的基础元件数量，但并未显著提升三维重建的几何精度。凸斑点方法表现为结构更紧凑、去杂乱性更强，优于高斯斑点。

Conclusion: 外观嵌入主要对视觉效果有益，无法带来几何精度优势。凸斑点更适用于需要紧凑表达和高安全性的太空机器人场景。研究明确了外观嵌入在几何重建任务中的局限性，并揭示了表达效率与重建质量之间的折中关系。

Abstract: We present the first systematic comparison of implicit and explicit Novel
View Synthesis methods for space-based 3D object reconstruction, evaluating the
role of appearance embeddings. While embeddings improve photometric fidelity by
modeling lighting variation, we show they do not translate into meaningful
gains in geometric accuracy - a critical requirement for space robotics
applications. Using the SPEED+ dataset, we compare K-Planes, Gaussian
Splatting, and Convex Splatting, and demonstrate that embeddings primarily
reduce the number of primitives needed for explicit methods rather than
enhancing geometric fidelity. Moreover, convex splatting achieves more compact
and clutter-free representations than Gaussian splatting, offering advantages
for safety-critical applications such as interaction and collision avoidance.
Our findings clarify the limits of appearance embeddings for geometry-centric
tasks and highlight trade-offs between reconstruction quality and
representation efficiency in space scenarios.

</details>


### [44] [GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection](https://arxiv.org/abs/2509.10250)
*Haozhen Yan,Yan Hong,Suning Lang,Jiahui Zhan,Yikun Ji,Yujie Gao,Jun Lan,Huijia Zhu,Weiqiang Wang,Jianfu Zhang*

Main category: cs.CV

TL;DR: 本论文提出了GAMMA，一个新颖的训练框架，通过多样化的图像操作和多任务学习结构，有效提升了AI生成图像检测模型的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管现有AI生成图像检测器在已知分布内表现良好，但对新型、未知生成模型的检测性能较差，主要原因是现有方法过分依赖于特定模型的风格或压缩特征。因此，亟需开发具有更好泛化能力的检测方法。

Method: 提出了GAMMA框架，采用多样的图像处理（如基于修补的操作和保持语义的扰动），配合多任务监督（双分割头和一个分类头），实现对不同生成模型领域下像素级别的源归因。同时，引入逆跨注意力机制，使分割头能够修正分类分支中的偏置表征。

Result: 在GenImage基准测试中，GAMMA取得了最先进的泛化检测性能，准确率提高了5.8%。此外，也在最新的大型生成模型（如GPT-4o）上表现出很强的鲁棒性。

Conclusion: GAMMA通过降低领域偏置和提升语义对齐能力，使AI图像检测器在面对全新或未知的生成模型时，表现出更强泛化能力和更高准确率，为AI生成内容的安全检测提供了新方向。

Abstract: With generative models becoming increasingly sophisticated and diverse,
detecting AI-generated images has become increasingly challenging. While
existing AI-genereted Image detectors achieve promising performance on
in-distribution generated images, their generalization to unseen generative
models remains limited. This limitation is largely attributed to their reliance
on generation-specific artifacts, such as stylistic priors and compression
patterns. To address these limitations, we propose GAMMA, a novel training
framework designed to reduce domain bias and enhance semantic alignment. GAMMA
introduces diverse manipulation strategies, such as inpainting-based
manipulation and semantics-preserving perturbations, to ensure consistency
between manipulated and authentic content. We employ multi-task supervision
with dual segmentation heads and a classification head, enabling pixel-level
source attribution across diverse generative domains. In addition, a reverse
cross-attention mechanism is introduced to allow the segmentation heads to
guide and correct biased representations in the classification branch. Our
method achieves state-of-the-art generalization performance on the GenImage
benchmark, imporving accuracy by 5.8%, but also maintains strong robustness on
newly released generative model such as GPT-4o.

</details>


### [45] [Robustness and Diagnostic Performance of Super-Resolution Fetal Brain MRI](https://arxiv.org/abs/2509.10257)
*Ema Masterl,Tina Vipotnik Vesnaver,Žiga Špiclin*

Main category: cs.CV

TL;DR: 本研究比较了三种先进的超分辨率重建（SRR）方法对胎儿脑MRI的重建与诊断影响。结果发现NeSVoR方法重建成功率最高，尽管各方法的体积测量结果存在差异，但对诊断准确性影响不大。


<details>
  <summary>Details</summary>
Motivation: 胎儿脑MRI受限于运动伪影，需快速二维多视角采集，这导致图像分辨率低且三维结构信息不完整。现有SRR技术虽可提高清晰度，但其在病理病例中的表现及对后续诊断的影响尚不清楚。

Method: 对140例胎儿脑MRI（含健康对照与脑积水病理病例），分别应用三种SRR方法（NiftyMIC、SVRTK、NeSVoR）进行高分辨率三维重建，并使用BoUNTi算法进行脑结构分割，系统评估了重建图像质量、成功率、体积测量一致性及诊断分类性能。

Result: NeSVoR重建成功率最高且最稳定（大于90%），不同SRR方法在体积测量上有显著差异，但对脑积水诊断的分类性能没有明显影响。

Conclusion: NeSVoR在健康及病理胎儿脑MRI高分辨率重建中表现出高鲁棒性，各SRR方法导致的体积差异不会影响后续诊断，说明诊断任务对SRR选择具有较强适应性。

Abstract: Fetal brain MRI relies on rapid multi-view 2D slice acquisitions to reduce
motion artifacts caused by fetal movement. However, these stacks are typically
low resolution, may suffer from motion corruption, and do not adequately
capture 3D anatomy. Super-resolution reconstruction (SRR) methods aim to
address these limitations by combining slice-to-volume registration and
super-resolution techniques to generate high-resolution (HR) 3D volumes. While
several SRR methods have been proposed, their comparative performance -
particularly in pathological cases - and their influence on downstream
volumetric analysis and diagnostic tasks remain underexplored. In this study,
we applied three state-of-the-art SRR method - NiftyMIC, SVRTK, and NeSVoR - to
140 fetal brain MRI scans, including both healthy controls (HC) and
pathological cases (PC) with ventriculomegaly (VM). Each HR reconstruction was
segmented using the BoUNTi algorithm to extract volumes of nine principal brain
structures. We evaluated visual quality, SRR success rates, volumetric
measurement agreement, and diagnostic classification performance. NeSVoR
demonstrated the highest and most consistent reconstruction success rate (>90%)
across both HC and PC groups. Although significant differences in volumetric
estimates were observed between SRR methods, classification performance for VM
was not affected by the choice of SRR method. These findings highlight NeSVoR's
robustness and the resilience of diagnostic performance despite SRR-induced
volumetric variability.

</details>


### [46] [Mask Consistency Regularization in Object Removal](https://arxiv.org/abs/2509.10259)
*Hua Yuan,Jin Yuan,Yicheng Jiang,Yao Zhang,Xin Geng,Yong Rui*

Main category: cs.CV

TL;DR: 本文提出了一种新的训练策略用于提高图像中目标移除任务的效果，显著减少了由于掩码导致的幻觉生成和掩码形状偏差。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在图像修复领域取得了进展，但现有方法在目标移除时，经常会在掩码区域生成无关或错误内容（掩码幻觉）以及按照掩码形状填充对象（掩码形状偏差）。解决这两个问题能够提升修复图像的自然度和一致性。

Method: 提出了掩码一致性正则化（MCR）训练策略。在训练过程中加入了两种掩码扰动：膨胀（dilation）和重塑（reshape），通过这些扰动分支输出与原始掩码输出之间的约束，迫使模型输出更加符合周围内容且避免掩码形状偏差。

Result: 实验表明，MCR显著降低了模型在目标移除任务上的幻觉现象和掩码形状偏差，提升了修复效果和上下文一致性。

Conclusion: MCR通过特殊的掩码扰动训练机制，有效提升了目标移除模型的适应性和生成质量，解决了原有模型中的两大常见问题。

Abstract: Object removal, a challenging task within image inpainting, involves
seamlessly filling the removed region with content that matches the surrounding
context. Despite advancements in diffusion models, current methods still face
two critical challenges. The first is mask hallucination, where the model
generates irrelevant or spurious content inside the masked region, and the
second is mask-shape bias, where the model fills the masked area with an object
that mimics the mask's shape rather than surrounding content. To address these
issues, we propose Mask Consistency Regularization (MCR), a novel training
strategy designed specifically for object removal tasks. During training, our
approach introduces two mask perturbations: dilation and reshape, enforcing
consistency between the outputs of these perturbed branches and the original
mask. The dilated masks help align the model's output with the surrounding
content, while reshaped masks encourage the model to break the mask-shape bias.
This combination of strategies enables MCR to produce more robust and
contextually coherent inpainting results. Our experiments demonstrate that MCR
significantly reduces hallucinations and mask-shape bias, leading to improved
performance in object removal.

</details>


### [47] [MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation](https://arxiv.org/abs/2509.10260)
*Jia Wang,Jie Hu,Xiaoqi Ma,Hanghang Ma,Yanbing Zeng,Xiaoming Wei*

Main category: cs.CV

TL;DR: 本文提出了MagicMirror框架用于系统性评估文本生成图像（T2I）中的物理瑕疵（如结构性缺陷），并发现现有T2I模型普遍存在明显伪影问题。


<details>
  <summary>Details</summary>
Motivation: 虽然T2I生成技术在按照指令生成和美学表现上取得了显著进展，但图像中持续存在的物理性瑕疵破坏了感知质量，现有基准缺乏细粒度和系统性的伪影评估框架。

Method: 作者提出了MagicMirror框架，首先建立了图像伪影的详细分类体系，并据此人工标注了包含34万张生成图像、具备细粒度伪影标签的大规模数据集MagicData340K。基于此数据集，训练了一个视觉-语言模型MagicAssessor，用于自动评估和标注伪影。同时，设计了创新性数据采样方法和多级奖励机制，以解决类别不平衡和奖赏作弊等问题。最终利用MagicAssessor，构建了自动化的基准评测平台MagicBench。

Result: 用MagicBench评估当前主流T2I模型后发现，包括GPT-image-1在内的顶尖模型依然普遍存在显著的图像伪影。

Conclusion: 图像伪影问题依旧是T2I领域发展的主要瓶颈，消除伪影应成为未来研究的关键方向。

Abstract: Text-to-image (T2I) generation has achieved remarkable progress in
instruction following and aesthetics. However, a persistent challenge is the
prevalence of physical artifacts, such as anatomical and structural flaws,
which severely degrade perceptual quality and limit application. Given the
diversity and complexity of these artifacts, a systematic and fine-grained
evaluation framework is required, which is lacking in current benchmarks. To
fill this gap, we introduce MagicMirror, a comprehensive framework for
artifacts assessment. We first establish a detailed taxonomy of generated image
artifacts. Guided by this taxonomy, we manually annotate MagicData340K, the
first human-annotated large-scale dataset of 340K generated images with
fine-grained artifact labels. Building on this dataset, we train MagicAssessor,
a Vision-Language Model (VLM) that provides detailed assessments and
corresponding labels. To overcome challenges like class imbalance and reward
hacking, we design a novel data sampling strategy and a multi-level reward
system for Group Relative Policy Optimization (GRPO). Finally, we leverage
MagicAssessor to construct MagicBench, an automated benchmark for evaluating
the image artifacts of current T2I models. Our evaluation with MagicBench
reveals that despite their widespread adoption, even top-tier models like
GPT-image-1 are consistently plagued by significant artifacts, highlighting
artifact reduction as a critical frontier for future T2I development. Project
page: https://wj-inf.github.io/MagicMirror-page/.

</details>


### [48] [SignClip: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion](https://arxiv.org/abs/2509.10266)
*Wenfang Wu,Tingting Yuan,Yupeng Li,Daling Wang,Xiaoming Fu*

Main category: cs.CV

TL;DR: 论文提出SignClip框架，将手势与口型等非手动信号有效融合，并通过分层对比学习提升手语翻译精度，取得当前最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有手语翻译方法忽视了口型等非手动信息，而这些信息在消歧和传递语义中具有关键作用，因此亟需发展融合手势与口型的高效手语翻译框架。

Method: 提出SignClip新框架，将手势（空间动作）与口型（唇动）特征进行融合，并设计了分层（多级）对比学习机制，实现手语与唇动、视觉与文本多模态间的语义一致性对齐。

Result: 在PHOENIX14T和How2Sign两个主流数据集上，SignClip均优于最新方法SpaMo，例如在PHOENIX14T无Gloss注释条件下，BLEU-4由24.32提升至24.71，ROUGE由46.57提升至48.38，证明方法有效。

Conclusion: 将手势和口型联合建模能显著提升手语翻译。SignClip方法拓展了手语翻译方法的视角，推动了多模态融合在该领域的研究进展。

Abstract: Sign language translation (SLT) aims to translate natural language from sign
language videos, serving as a vital bridge for inclusive communication. While
recent advances leverage powerful visual backbones and large language models,
most approaches mainly focus on manual signals (hand gestures) and tend to
overlook non-manual cues like mouthing. In fact, mouthing conveys essential
linguistic information in sign languages and plays a crucial role in
disambiguating visually similar signs. In this paper, we propose SignClip, a
novel framework to improve the accuracy of sign language translation. It fuses
manual and non-manual cues, specifically spatial gesture and lip movement
features. Besides, SignClip introduces a hierarchical contrastive learning
framework with multi-level alignment objectives, ensuring semantic consistency
across sign-lip and visual-text modalities. Extensive experiments on two
benchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our
approach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip
surpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from
24.32 to 24.71, and ROUGE from 46.57 to 48.38.

</details>


### [49] [Detecting Text Manipulation in Images using Vision Language Models](https://arxiv.org/abs/2509.10278)
*Vidit Vidit,Pavel Korshunov,Amir Mohammadi,Christophe Ecabert,Ketan Kotwal,Sébastien Marcel*

Main category: cs.CV

TL;DR: 本论文评估了大型视觉语言模型（VLMs/LVLMs）在文本篡改检测任务中的表现，发现开源模型仍落后于闭源模型，且现有模型存在泛化能力不足的问题，特别是在现实场景文本和模拟证件数据集上的检测任务中。


<details>
  <summary>Details</summary>
Motivation: 虽然近年来VLMs在图像篡改检测中表现优秀，但对文本篡改检测的研究明显不足，因此本论文试图填补这一空白，系统分析VLMs在文本篡改检测方面的能力。

Method: 作者对比了闭源与开源VLMs在多个文本篡改数据集上的表现，并特别测试了面向图像篡改检测训练的VLMs在文本篡改检测上的泛化能力，还构建基准包含真实世界场景文本和模仿真实证件篡改的ID卡。

Result: 开源模型逐步接近闭源模型（如GPT-4o），但检测精度仍有差距。图像篡改检测专用的VLM在文本篡改检测任务上表现出较差的泛化性，面对现实场景和ID卡等复杂场景时效果有限。

Conclusion: 当前开源VLMs在文本篡改检测方面尚未达到闭源模型的水平，且现有模型难以有效泛化到多样化、复杂的篡改场景，未来需进一步提升VLMs的文本篡改检测能力及泛化能力。

Abstract: Recent works have shown the effectiveness of Large Vision Language Models
(VLMs or LVLMs) in image manipulation detection. However, text manipulation
detection is largely missing in these studies. We bridge this knowledge gap by
analyzing closed- and open-source VLMs on different text manipulation datasets.
Our results suggest that open-source models are getting closer, but still
behind closed-source ones like GPT- 4o. Additionally, we benchmark image
manipulation detection-specific VLMs for text manipulation detection and show
that they suffer from the generalization problem. We benchmark VLMs for
manipulations done on in-the-wild scene texts and on fantasy ID cards, where
the latter mimic a challenging real-world misuse.

</details>


### [50] [MCL-AD: Multimodal Collaboration Learning for Zero-Shot 3D Anomaly Detection](https://arxiv.org/abs/2509.10282)
*Gang Li,Tianjiao Chen,Mingle Zhou,Min Li,Delong Han,Jin Wan*

Main category: cs.CV

TL;DR: 该论文提出了一种多模态协作学习方法MCL-AD，通过结合点云、RGB图像和文本语义，实现了零样本3D异常检测并取得了业内领先的效果。


<details>
  <summary>Details</summary>
Motivation: 现有零样本3D异常检测方法多仅利用点云，忽略了RGB图像和文本等其他模态带来的丰富语义信息，限制了检测效果。

Method: 提出MCL-AD框架，融合点云、RGB图像和文本语义。引入多模态提示学习机制（MPLM），包含与对象无关的文本提示和多模态对比损失，强化模态内和模态间的表示。还设计了协同调制机制（CMM），通过联合调制RGB图像分支和点云分支，充分利用各自的互补特征。

Result: 大量实验显示MCL-AD在零样本3D异常检测任务上取得了当前最优的性能。

Conclusion: 多模态协作学习显著提升了零样本3D异常检测性能，MCL-AD方法为相关任务带来了有效的新策略。

Abstract: Zero-shot 3D (ZS-3D) anomaly detection aims to identify defects in 3D objects
without relying on labeled training data, making it especially valuable in
scenarios constrained by data scarcity, privacy, or high annotation cost.
However, most existing methods focus exclusively on point clouds, neglecting
the rich semantic cues available from complementary modalities such as RGB
images and texts priors. This paper introduces MCL-AD, a novel framework that
leverages multimodal collaboration learning across point clouds, RGB images,
and texts semantics to achieve superior zero-shot 3D anomaly detection.
Specifically, we propose a Multimodal Prompt Learning Mechanism (MPLM) that
enhances the intra-modal representation capability and inter-modal
collaborative learning by introducing an object-agnostic decoupled text prompt
and a multimodal contrastive loss. In addition, a collaborative modulation
mechanism (CMM) is proposed to fully leverage the complementary representations
of point clouds and RGB images by jointly modulating the RGB image-guided and
point cloud-guided branches. Extensive experiments demonstrate that the
proposed MCL-AD framework achieves state-of-the-art performance in ZS-3D
anomaly detection.

</details>


### [51] [Adversarial robustness through Lipschitz-Guided Stochastic Depth in Neural Networks](https://arxiv.org/abs/2509.10298)
*Laith Nayal,Mahmoud Mousatat,Bader Rasheed*

Main category: cs.CV

TL;DR: 本文提出了一种新型的Lipschitz引导型随机深度（DropPath）方法，通过让丢弃概率随网络深度增加来控制Lipschitz常数，实现对深层的有效正则化。该方法能提升神经网络在攻击下的鲁棒性，并保持较高准确率和计算效率。实验证明这种方法在多个对抗攻击下提升鲁棒性，同时减少了计算量。


<details>
  <summary>Details</summary>
Motivation: 当前视觉领域的深度神经网络和视觉Transformer虽然有卓越性能，但对对抗扰动极为脆弱，现有防御方法往往计算开销大或缺乏严格理论保证。需要寻找兼顾鲁棒性、精度和计算效率的新方案。

Method: 作者提出了一种基于Lipschitz常数引导的深度依赖DropPath策略：网络越深，DropPath概率越大，从而有效调控Lipschitz常数。与传统的线性DropPath不同，新方法实现了更合理的层正则化。

Result: 在CIFAR-10数据集和ViT-Tiny模型上，新方法在保持接近基线的原始准确率同时，大幅提升了在FGSM、PGD-20和AutoAttack等多种攻击下的鲁棒性，且显著减少了计算量（FLOPs）。对比线性DropPath和基线，新方法优势明显。

Conclusion: Lipschitz引导的随机深度方法兼顾了鲁棒性、准确率和效率，为深度神经网络防御对抗攻击提供了有效可行的新思路，适用于实际需求较高的场景。

Abstract: Deep neural networks and Vision Transformers achieve state-of-the-art
performance in computer vision but are highly vulnerable to adversarial
perturbations. Standard defenses often incur high computational cost or lack
formal guarantees. We propose a Lipschitz-guided stochastic depth (DropPath)
method, where drop probabilities increase with depth to control the effective
Lipschitz constant of the network. This approach regularizes deeper layers,
improving robustness while preserving clean accuracy and reducing computation.
Experiments on CIFAR-10 with ViT-Tiny show that our custom depth-dependent
schedule maintains near-baseline clean accuracy, enhances robustness under
FGSM, PGD-20, and AutoAttack, and significantly reduces FLOPs compared to
baseline and linear DropPath schedules.

</details>


### [52] [A Stochastic Birth-and-Death Approach for Street Furniture Geolocation in Urban Environments](https://arxiv.org/abs/2509.10310)
*Evan Murphy,Marco Viola,Vladimir A. Krylov*

Main category: cs.CV

TL;DR: 该论文提出了一种基于能量图的概率框架，用于城市环境中街道家具（如路灯等公共设施）的精确地理定位，并通过引入随机生灭优化算法提升了资产配置推断的准确性。


<details>
  <summary>Details</summary>
Motivation: 在复杂的城市环境下，准确定位街道家具对于城市管理和维护至关重要，但由于地理环境复杂、障碍物多、数据融合难等因素，现有方法面临定位精度和可扩展性的挑战。

Method: 作者提出利用能量图表示目标空间概率分布的方式，将地理位置信息地图库（如GIS、道路图、约束条件）无缝整合到优化过程中，通过随机生灭优化算法寻找资产的最优分布。

Result: 在都柏林市中心路灯设施的真实地理数据集上进行仿真实验，验证了所提方法的可扩展性和准确性。

Conclusion: 基于能量图和随机生灭优化的方案能有效提升复杂城市环境下街道家具的地理定位精度，并具备实际部署潜力，相关实现已在GitHub开源。

Abstract: In this paper we address the problem of precise geolocation of street
furniture in complex urban environments, which is a critical task for effective
monitoring and maintenance of public infrastructure by local authorities and
private stakeholders. To this end, we propose a probabilistic framework based
on energy maps that encode the spatial likelihood of object locations.
Representing the energy in a map-based geopositioned format allows the
optimisation process to seamlessly integrate external geospatial information,
such as GIS layers, road maps, or placement constraints, which improves
contextual awareness and localisation accuracy. A stochastic birth-and-death
optimisation algorithm is introduced to infer the most probable configuration
of assets. We evaluate our approach using a realistic simulation informed by a
geolocated dataset of street lighting infrastructure in Dublin city centre,
demonstrating its potential for scalable and accurate urban asset mapping. The
implementation of the algorithm will be made available in the GitHub repository
https://github.com/EMurphy0108/SBD_Street_Furniture.

</details>


### [53] [Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching](https://arxiv.org/abs/2509.10312)
*Zhixin Zheng,Xinyu Wang,Chang Zou,Shaobo Wang,Linfeng Zhang*

Main category: cs.CV

TL;DR: 本文提出ClusCa方法，通过空间聚类极大加速扩散Transformer，在保证生成质量的前提下减少计算量，并且无需重新训练即可应用到各种扩散Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散Transformer在高质量生成的同时，计算开销巨大。之前的特征缓存方法只利用了时序相似性，未考虑空间相似性，因此难以实现更有效率的加速。

Method: 提出Cluster-Driven Feature Caching (ClusCa)。具体做法是在每个时间步对所有token做空间聚类，只计算每个cluster中的一个token，再将该token的信息传播到同cluster的其他tokens，从而大幅减少token数量（可降低90%以上）。本方法与现有特征缓存方法正交，可以互补提升效率。

Result: 在DiT、FLUX和HunyuanVideo等主流扩散Transformer上，ClusCa显著加速模型推理。比如在FLUX上能达到4.96倍加速，同时ImageReward指标高达99.49%，比原始模型还高0.51%。

Conclusion: ClusCa能大幅提升扩散Transformer的推理效率，且对生成质量无负面影响。方法无需重新训练，能直接应用于不同扩散模型，具有很强的实用价值和推广性。

Abstract: Diffusion transformers have gained significant attention in recent years for
their ability to generate high-quality images and videos, yet still suffer from
a huge computational cost due to their iterative denoising process. Recently,
feature caching has been introduced to accelerate diffusion transformers by
caching the feature computation in previous timesteps and reusing it in the
following timesteps, which leverage the temporal similarity of diffusion models
while ignoring the similarity in the spatial dimension. In this paper, we
introduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and
complementary perspective for previous feature caching. Specifically, ClusCa
performs spatial clustering on tokens in each timestep, computes only one token
in each cluster and propagates their information to all the other tokens, which
is able to reduce the number of tokens by over 90%. Extensive experiments on
DiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image
and text-to-video generation. Besides, it can be directly applied to any
diffusion transformer without requirements for training. For instance, ClusCa
achieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing
the original model by 0.51%. The code is available at
https://github.com/Shenyi-Z/Cache4Diffusion.

</details>


### [54] [I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic Segmentation](https://arxiv.org/abs/2509.10334)
*Jordan Sassoon,Michal Szczepanski,Martyna Poreba*

Main category: cs.CV

TL;DR: 本文提出了一种名为I-Segmenter的全整型ViT语义分割框架，实现了端到端的整数运算，有效提升了ViT模型在资源受限设备上的部署效率。


<details>
  <summary>Details</summary>
Motivation: 现有的ViT用于语义分割尽管效果强，但由于高存储和计算需求，难以在边缘设备上部署。低精度量化是提升效率的常用手段，但ViT在量化下易因误差累积导致模型准确率大降，因此急需一种既高效又鲁棒的整数化方案。

Method: 作者基于Segmenter架构，将网络中的浮点操作系统性地替换为整型运算，提出了新激活函数λ-ShiftGELU，缓解均匀量化对长尾分布激活值的影响。同时，移除L2归一化、用最近邻上采样取代双线性插值，保证整个计算流的整数化。训练和推理过程全程整数化。

Result: I-Segmenter在多个实验中可与FP32模型准确率接近（平均损失仅5.1%），模型体积缩小至3.8倍，推理速度提升1.2倍（在优化后实现）。即使只用一张标注图片做一次性PTQ，依然能取得有竞争力的准确率。

Conclusion: I-Segmenter实现了首个全整型ViT分割方案，显著减少了资源占用且保持了较高准确率，为ViT落地边缘设备提供了现实可行的途径。

Abstract: Vision Transformers (ViTs) have recently achieved strong results in semantic
segmentation, yet their deployment on resource-constrained devices remains
limited due to their high memory footprint and computational cost. Quantization
offers an effective strategy to improve efficiency, but ViT-based segmentation
models are notoriously fragile under low precision, as quantization errors
accumulate across deep encoder-decoder pipelines. We introduce I-Segmenter, the
first fully integer-only ViT segmentation framework. Building on the Segmenter
architecture, I-Segmenter systematically replaces floating-point operations
with integer-only counterparts. To further stabilize both training and
inference, we propose $\lambda$-ShiftGELU, a novel activation function that
mitigates the limitations of uniform quantization in handling long-tailed
activation distributions. In addition, we remove the L2 normalization layer and
replace bilinear interpolation in the decoder with nearest neighbor upsampling,
ensuring integer-only execution throughout the computational graph. Extensive
experiments show that I-Segmenter achieves accuracy within a reasonable margin
of its FP32 baseline (5.1 % on average), while reducing model size by up to
3.8x and enabling up to 1.2x faster inference with optimized runtimes. Notably,
even in one-shot PTQ with a single calibration image, I-Segmenter delivers
competitive accuracy, underscoring its practicality for real-world deployment.

</details>


### [55] [GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT](https://arxiv.org/abs/2509.10341)
*Botond Fazekas,Thomas Pinetz,Guilherme Aresta,Taha Emre,Hrvoje Bogunovic*

Main category: cs.CV

TL;DR: 该论文提出了一种名为GARD的深度学习方法，对OCT视网膜图像进行去散斑降噪，显著优于现有传统和深度学习降噪方法，能更好地保持解剖结构细节。


<details>
  <summary>Details</summary>
Motivation: OCT（光学相干断层成像）图像常受散斑噪声影响，难以准确解读，现有降噪方法在降噪与结构保存之间难以平衡，需要更优方法。

Method: 提出GARD方法，基于扩散概率模型，创新性采用伽马分布模拟噪声特性，并引入“降噪保真项”利用预处理低噪声图像引导去噪，结合高效推断框架以提升速度。

Result: 在OCT成对高、低噪声数据集上验证，GARD在PSNR、SSIM、MSE等指标上均明显优于传统与最新深度学习降噪方法，并且定性结果显示边缘更锐利、解剖细节保存更好。

Conclusion: GARD方法能更真实反映散斑噪声统计特性并兼顾降噪与结构保真度，适宜OCT医学图像去噪，有望改善临床诊断基础图像质量。

Abstract: Optical Coherence Tomography (OCT) is a vital imaging modality for diagnosing
and monitoring retinal diseases. However, OCT images are inherently degraded by
speckle noise, which obscures fine details and hinders accurate interpretation.
While numerous denoising methods exist, many struggle to balance noise
reduction with the preservation of crucial anatomical structures. This paper
introduces GARD (Gamma-based Anatomical Restoration and Denoising), a novel
deep learning approach for OCT image despeckling that leverages the strengths
of diffusion probabilistic models. Unlike conventional diffusion models that
assume Gaussian noise, GARD employs a Denoising Diffusion Gamma Model to more
accurately reflect the statistical properties of speckle. Furthermore, we
introduce a Noise-Reduced Fidelity Term that utilizes a pre-processed,
less-noisy image to guide the denoising process. This crucial addition prevents
the reintroduction of high-frequency noise. We accelerate the inference process
by adapting the Denoising Diffusion Implicit Model framework to our Gamma-based
model. Experiments on a dataset with paired noisy and less-noisy OCT B-scans
demonstrate that GARD significantly outperforms traditional denoising methods
and state-of-the-art deep learning models in terms of PSNR, SSIM, and MSE.
Qualitative results confirm that GARD produces sharper edges and better
preserves fine anatomical details.

</details>


### [56] [GLAM: Geometry-Guided Local Alignment for Multi-View VLP in Mammography](https://arxiv.org/abs/2509.10344)
*Yuexi Du,Lihui Chen,Nicha C. Dvornek*

Main category: cs.CV

TL;DR: 论文提出了GLAM方法，通过几何引导的多视图对齐提升乳腺X光影像的VLM预训练效果，实现对多视图详细特征和对应关系的学习，结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的乳腺X光基础视觉语言模型大多直接从自然图像领域迁移，忽略了乳腺X光的特有多视图关系，且多视图信息未被充分建模，导致预测效果不理想。

Method: 提出GLAM方法，将多视图几何知识融入VLM预训练。模型通过全球及局部、视觉-视觉与视觉-语言的对比学习，显式对齐不同视图的局部特征，学习细粒度图像信息。预训练于大型乳腺X光数据集EMBED，使用几何引导实现视图间对齐。

Result: GLAM方法在多个公开乳腺X光数据集和不同实验设定下均明显优于现有基准模型。

Conclusion: 将多视图几何关系融入预训练能显著提升VLM在乳腺X光领域的性能，有望助力乳腺癌筛查的自动化与准确性。

Abstract: Mammography screening is an essential tool for early detection of breast
cancer. The speed and accuracy of mammography interpretation have the potential
to be improved with deep learning methods. However, the development of a
foundation visual language model (VLM) is hindered by limited data and domain
differences between natural and medical images. Existing mammography VLMs,
adapted from natural images, often ignore domain-specific characteristics, such
as multi-view relationships in mammography. Unlike radiologists who analyze
both views together to process ipsilateral correspondence, current methods
treat them as independent images or do not properly model the multi-view
correspondence learning, losing critical geometric context and resulting in
suboptimal prediction. We propose GLAM: Global and Local Alignment for
Multi-view mammography for VLM pretraining using geometry guidance. By
leveraging the prior knowledge about the multi-view imaging process of
mammograms, our model learns local cross-view alignments and fine-grained local
features through joint global and local, visual-visual, and visual-language
contrastive learning. Pretrained on EMBED [14], one of the largest open
mammography datasets, our model outperforms baselines across multiple datasets
under different settings.

</details>


### [57] [Towards Understanding Visual Grounding in Visual Language Models](https://arxiv.org/abs/2509.10345)
*Georgios Pantazopoulos,Eda B. Özyiğit*

Main category: cs.CV

TL;DR: 本文综述了视觉基础（Visual Grounding）在现代通用视觉-语言模型（VLMs）中的关键角色及相关研究进展。


<details>
  <summary>Details</summary>
Motivation: 视觉基础对于促进VLMs更好地理解和处理复杂的视觉与语言任务至关重要。实现精确的视觉定位有助于提升如细粒度问答、图像描述、实体指代等多种应用的表现。

Method: 文章系统回顾了VLMs中用于视觉基础的主要技术路线，包括模型核心组件、训练与评估方法，以及多模态生成的基准测试和评价指标。

Result: 概述了视觉基础相关方法在实际应用中的表现，重点分析了其多模态推理、链式思考（chain-of-thought）能力与视觉基础之间的交互。

Conclusion: 指出了当前视觉基础研究面临的主要挑战，并提出了未来有前景的研究方向，如提升模型推理能力、改进评价体系以及深度融合视觉与语言信息。

Abstract: Visual grounding refers to the ability of a model to identify a region within
some visual input that matches a textual description. Consequently, a model
equipped with visual grounding capabilities can target a wide range of
applications in various domains, including referring expression comprehension,
answering questions pertinent to fine-grained details in images or videos,
caption visual context by explicitly referring to entities, as well as low and
high-level control in simulated and real environments. In this survey paper, we
review representative works across the key areas of research on modern
general-purpose vision language models (VLMs). We first outline the importance
of grounding in VLMs, then delineate the core components of the contemporary
paradigm for developing grounded models, and examine their practical
applications, including benchmarks and evaluation metrics for grounded
multimodal generation. We also discuss the multifaceted interrelations among
visual grounding, multimodal chain-of-thought, and reasoning in VLMs. Finally,
we analyse the challenges inherent to visual grounding and suggest promising
directions for future research.

</details>


### [58] [Immunizing Images from Text to Image Editing via Adversarial Cross-Attention](https://arxiv.org/abs/2509.10359)
*Matteo Trippodo,Federico Becattini,Lorenzo Seidenari*

Main category: cs.CV

TL;DR: 本文提出了一种针对文本引导的图像编辑方法的新型对抗性攻击方式，可以在不被察觉的情况下大幅降低编辑效果，并介绍了两种新的效果评估方法。


<details>
  <summary>Details</summary>
Motivation: 随着基于文本的图像编辑方法在视觉内容精细操作中的应用增多，这类方法也面临针对性的对抗攻击威胁，因此有必要探索新的攻击方法并评估其影响。

Method: 提出了一种名为Attention Attack的新型攻击方式，通过自动生成的图像描述（caption）替换原始编辑提示，扰乱图像与文本之间的跨模态注意力机制；无需了解具体编辑方法或提示信息。还提出了两种全新的评估策略：Caption Similarity 用于衡量编辑前后语义一致性，语义IoU用于量化空间布局扰动。

Result: 在TEDBench++测试集上实验结果表明，所提出的攻击方法在用户不可察觉的情况下，显著降低了文本图像编辑的表现。

Conclusion: Attention Attack在不显著改变图像视觉感知的前提下，有效扰乱了文本与图像的对齐，为理解和防御此类对抗攻击提供了新视角，也为评估编辑健壮性提出了新方法。

Abstract: Recent advances in text-based image editing have enabled fine-grained
manipulation of visual content guided by natural language. However, such
methods are susceptible to adversarial attacks. In this work, we propose a
novel attack that targets the visual component of editing methods. We introduce
Attention Attack, which disrupts the cross-attention between a textual prompt
and the visual representation of the image by using an automatically generated
caption of the source image as a proxy for the edit prompt. This breaks the
alignment between the contents of the image and their textual description,
without requiring knowledge of the editing method or the editing prompt.
Reflecting on the reliability of existing metrics for immunization success, we
propose two novel evaluation strategies: Caption Similarity, which quantifies
semantic consistency between original and adversarial edits, and semantic
Intersection over Union (IoU), which measures spatial layout disruption via
segmentation masks. Experiments conducted on the TEDBench++ benchmark
demonstrate that our attack significantly degrades editing performance while
remaining imperceptible.

</details>


### [59] [Efficient Learned Image Compression Through Knowledge Distillation](https://arxiv.org/abs/2509.10366)
*Fabien Allemand,Attilio Fiandrotti,Sumanta Chaudhuri,Alaa Eddine Mazouz*

Main category: cs.CV

TL;DR: 本论文提出利用知识蒸馏技术，降低神经网络图像压缩对计算资源的要求，使其更适合资源受限平台，实现高效的端到端图像压缩。


<details>
  <summary>Details</summary>
Motivation: 尽管基于神经网络的图像压缩优于传统编解码器，但其高计算成本阻碍了在主流和实时场景下的应用，因此需要在不大幅降低性能的前提下减少资源消耗。

Method: 采用知识蒸馏的方法，小型神经网络在大型复杂模型（教师网络）的指导下进行训练，从而兼顾压缩性能与计算效率。同时在不同架构规模、图像质量/比特率权衡等方面进行了实验验证。

Result: 研究发现知识蒸馏可有效应用于各类架构尺寸的图像压缩模型，有助于节省处理和能耗，并在多种场景下达到较优的性能；提出并实验了新的模型配置与超参数设置。

Conclusion: 知识蒸馏能够在保证图像压缩质量的同时大幅降低模型所需资源，为高效图像压缩提供了新途径，并为未来研究如不同教师网络、损失函数及transformer模型的蒸馏应用奠定基础。

Abstract: Learned image compression sits at the intersection of machine learning and
image processing. With advances in deep learning, neural network-based
compression methods have emerged. In this process, an encoder maps the image to
a low-dimensional latent space, which is then quantized, entropy-coded into a
binary bitstream, and transmitted to the receiver. At the receiver end, the
bitstream is entropy-decoded, and a decoder reconstructs an approximation of
the original image. Recent research suggests that these models consistently
outperform conventional codecs. However, they require significant processing
power, making them unsuitable for real-time use on resource-constrained
platforms, which hinders their deployment in mainstream applications. This
study aims to reduce the resource requirements of neural networks used for
image compression by leveraging knowledge distillation, a training paradigm
where smaller neural networks, partially trained on the outputs of larger, more
complex models, can achieve better performance than when trained independently.
Our work demonstrates that knowledge distillation can be effectively applied to
image compression tasks: i) across various architecture sizes, ii) to achieve
different image quality/bit rate tradeoffs, and iii) to save processing and
energy resources. This approach introduces new settings and hyperparameters,
and future research could explore the impact of different teacher models, as
well as alternative loss functions. Knowledge distillation could also be
extended to transformer-based models. The code is publicly available at:
https://github.com/FABallemand/PRIM .

</details>


### [60] [Ordinality of Visible-Thermal Image Intensities for Intrinsic Image Decomposition](https://arxiv.org/abs/2509.10388)
*Zeqing Leo Yuan,Mani Ramanagopal,Aswin C. Sankaranarayanan,Srinivasa G. Narasimhan*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的新方法，仅依靠一对可见光与热成像图像来分解图像的反射和阴影成分。该方法利用热成像反映未反射光被吸收入热的物理原理，实现了密集的自监督学习，分解效果优于近期基于学习的方法。


<details>
  <summary>Details</summary>
Motivation: 传统的固有图像分解依赖于合成数据或有限的人工标签，真实场景的高质量标注极其稀缺，特别是户外。为了解决缺数据和泛化能力差的问题，作者希望寻找一种无需大量标注、注重物理原理的新方法。

Method: 作者提出一种基于可见光及热成像图像对的图像分解方法。利用未被反射的光会以热的形式被吸收并被热像仪探测到的原理，将可见光与热成像强度顺序与反射、阴影顺序相关联，作为密集自监督信号训练神经网络，实现无需训练集或注释的分解。

Result: 作者在已知的反射率和阴影条件下，分别用自然光和人工光源进行定量评测，并在多种户外场景开展定性实验，结果表明该方法效果优于当前主流的学习方法。

Conclusion: 该研究不仅提升了固有图像分解的性能，还为大规模真实场景的有序监督提供了可行道路，绕开了以往必须人工标注的瓶颈，具有良好的应用前景和扩展性。

Abstract: Decomposing an image into its intrinsic photometric factors--shading and
reflectance--is a long-standing challenge due to the lack of extensive
ground-truth data for real-world scenes. Recent methods rely on synthetic data
or sparse annotations for limited indoor and even fewer outdoor scenes. We
introduce a novel training-free approach for intrinsic image decomposition
using only a pair of visible and thermal images. We leverage the principle that
light not reflected from an opaque surface is absorbed and detected as heat by
a thermal camera. This allows us to relate the ordinalities between visible and
thermal image intensities to the ordinalities of shading and reflectance, which
can densely self-supervise an optimizing neural network to recover shading and
reflectance. We perform quantitative evaluations with known reflectance and
shading under natural and artificial lighting, and qualitative experiments
across diverse outdoor scenes. The results demonstrate superior performance
over recent learning-based models and point toward a scalable path to curating
real-world ordinal supervision, previously infeasible via manual labeling.

</details>


### [61] [Compressed Video Quality Enhancement: Classifying and Benchmarking over Standards](https://arxiv.org/abs/2509.10407)
*Xiem HoangVan,Dang BuiDinh,Sang NguyenQuang,Wen-Hsiao Peng*

Main category: cs.CV

TL;DR: 本论文针对压缩视频质量增强（CVQE）领域，提出了新的方法分类体系、统一评测框架以及当前方法的系统性分析，旨在为CVQE研究和实际应用提供基础和指导。


<details>
  <summary>Details</summary>
Motivation: 压缩视频常因有损编码出现质量损失，现有深度学习方法虽有进展，但相关综述存在方法与标准及压缩伪影关联不清、架构范式横向分析不足、评测标准不完善等问题，亟需系统梳理与综合评测。

Method: 1. 提出基于架构范式、编码标准及压缩域特征的新型CVQE方法分类法；2. 构建统一评测框架，接纳多种主流压缩协议及标准测试序列，实现多准则客观评价；3. 系统性分析现有主流方法在还原性能与计算复杂度间的权衡。

Result: 论文建立了首个覆盖架构与压缩标准的分类体系，统一了多标准评测流程，并梳理和比较了方法间的实际性能及算力开销，为后续研究提供数据参考与方向。

Conclusion: 此综述为CVQE领域提供了评测和方法选型基础，有助于促进方法公平对比、科学选型及未来创新方向的明确。

Abstract: Compressed video quality enhancement (CVQE) is crucial for improving user
experience with lossy video codecs like H.264/AVC, H.265/HEVC, and H.266/VVC.
While deep learning based CVQE has driven significant progress, existing
surveys still suffer from limitations: lack of systematic classification
linking methods to specific standards and artifacts, insufficient comparative
analysis of architectural paradigms across coding types, and underdeveloped
benchmarking practices. To address these gaps, this paper presents three key
contributions. First, it introduces a novel taxonomy classifying CVQE methods
across architectural paradigms, coding standards, and compressed-domain feature
utilization. Second, it proposes a unified benchmarking framework integrating
modern compression protocols and standard test sequences for fair
multi-criteria evaluation. Third, it provides a systematic analysis of the
critical trade-offs between reconstruction performance and computational
complexity observed in state-of-the-art methods and highlighting promising
directions for future research. This comprehensive review aims to establish a
foundation for consistent assessment and informed model selection in CVQE
research and deployment.

</details>


### [62] [Multimodal SAM-adapter for Semantic Segmentation](https://arxiv.org/abs/2509.10408)
*Iacopo Curti,Pierluigi Zama Ramirez,Alioscia Petrelli,Luigi Di Stefano*

Main category: cs.CV

TL;DR: 本文提出了一种名为MM SAM-adapter的新型多模态语义分割框架，在复杂环境中实现了更鲁棒的分割表现，并在多个基准数据集上取得了最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在光照差、遮挡、恶劣天气等复杂条件下表现不佳，仅利用RGB信息有一定局限性。多模态方法通过引入其他传感器数据（如LiDAR、红外等）提高了鲁棒性，但如何有效结合多模态信息仍具挑战性。

Method: 提出MM SAM-adapter框架，通过一个适配器网络将融合后的多模态特征注入到Segment Anything Model (SAM) 的RGB特征中，只在辅助模态能提供有效补充时才选择性融合，从而在保持RGB特征强泛化能力的同时，充分利用补充信息。

Result: 在DeLiVER、FMB和MUSES三个具有挑战性的基准数据集上，MM SAM-adapter均取得了优于现有方法的性能。在将DeLiVER和FMB进一步划分为RGB-easy和RGB-hard子集的实验中，该方法在各种有利和不利条件下的表现也均优于对比方法。

Conclusion: MM SAM-adapter能有效增强语义分割模型在复杂、恶劣环境下的鲁棒性，实现多模态信息的高效平衡利用，为多模态语义分割任务提供了新思路和工具。

Abstract: Semantic segmentation, a key task in computer vision with broad applications
in autonomous driving, medical imaging, and robotics, has advanced
substantially with deep learning. Nevertheless, current approaches remain
vulnerable to challenging conditions such as poor lighting, occlusions, and
adverse weather. To address these limitations, multimodal methods that
integrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged,
providing complementary information that enhances robustness. In this work, we
present MM SAM-adapter, a novel framework that extends the capabilities of the
Segment Anything Model (SAM) for multimodal semantic segmentation. The proposed
method employs an adapter network that injects fused multimodal features into
SAM's rich RGB features. This design enables the model to retain the strong
generalization ability of RGB features while selectively incorporating
auxiliary modalities only when they contribute additional cues. As a result, MM
SAM-adapter achieves a balanced and efficient use of multimodal information. We
evaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES,
where MM SAM-adapter delivers state-of-the-art performance. To further analyze
modality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hard
subsets. Results consistently demonstrate that our framework outperforms
competing methods in both favorable and adverse conditions, highlighting the
effectiveness of multimodal adaptation for robust scene understanding. The code
is available at the following link:
https://github.com/iacopo97/Multimodal-SAM-Adapter.

</details>


### [63] [InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis](https://arxiv.org/abs/2509.10441)
*Tao Han,Wanghan Xu,Junchao Gong,Xiaoyu Yue,Song Guo,Luping Zhou,Lei Bai*

Main category: cs.CV

TL;DR: 本文提出了InfGen方法，可基于固定尺寸潜变量生成任意分辨率的图像，极大提升了高分辨率图像生成的速度。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在高分辨率图像生成时计算需求随分辨率激增，导致生成4K图像需时超过100秒，严重影响实用性。

Method: 作者提出在潜变量扩散模型基础上，利用固定潜变量作为内容表示，通过一种新的一步式生成器（取代传统的VAE解码器）直接解码生成任意分辨率图像，无需重新训练扩散模型。该方法降低了计算复杂度，并可广泛应用于同一潜空间的扩散模型。

Result: 实验表明，InfGen可提升多种模型的分辨率生成上限，将4K图像生成时间从100秒缩短至10秒以内。

Conclusion: InfGen有效提升了扩散模型在任意高分辨率图像生成场景中的效率和实用性，为图像生成带来更一致和流畅的体验。

Abstract: Arbitrary resolution image generation provides a consistent visual experience
across devices, having extensive applications for producers and consumers.
Current diffusion models increase computational demand quadratically with
resolution, causing 4K image generation delays over 100 seconds. To solve this,
we explore the second generation upon the latent diffusion models, where the
fixed latent generated by diffusion models is regarded as the content
representation and we propose to decode arbitrary resolution images with a
compact generated latent using a one-step generator. Thus, we present the
\textbf{InfGen}, replacing the VAE decoder with the new generator, for
generating images at any resolution from a fixed-size latent without retraining
the diffusion models, which simplifies the process, reducing computational
complexity and can be applied to any model using the same latent space.
Experiments show InfGen is capable of improving many models into the arbitrary
high-resolution era while cutting 4K image generation time to under 10 seconds.

</details>


### [64] [SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and Adaptability Across Alzheimer's Prediction Tasks and Datasets](https://arxiv.org/abs/2509.10453)
*Emily Kaczmarek,Justin Szeto,Brennan Nichyporuk,Tal Arbel*

Main category: cs.CV

TL;DR: 本研究提出了一种新的自监督学习（SSL）方法，对阿尔茨海默病（AD）预测任务实现了更优表现，超越了传统监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的AD预测模型受到标注数据稀缺、跨数据集泛化能力弱以及对输入扫描数量和时间间隔变化不敏感等限制。作者希望通过改进SSL方法提升模型的适应性和泛化性。

Method: 作者将三种先进的时序自监督学习方法应用于3D脑MRI影像分析，并提出新扩展以支持可变长度输入和更强的空间特征学习。聚合了四个公开数据集共3161名患者用于预训练，通过时序顺序预测和对比学习提升模型能力。

Result: 该SSL模型在6/7项阿尔茨海默诊断相关下游任务中实现了优于监督学习的性能，展现了在不同任务、输入图片数和时间间隔变化下的良好适应性与泛化性。

Conclusion: 所提出的SSL方法适用于多种阿尔茨海默病临床预测任务，具备强健的泛化能力，有望在多样化实际场景中广泛应用。

Abstract: Alzheimer's disease is a progressive, neurodegenerative disorder that causes
memory loss and cognitive decline. While there has been extensive research in
applying deep learning models to Alzheimer's prediction tasks, these models
remain limited by lack of available labeled data, poor generalization across
datasets, and inflexibility to varying numbers of input scans and time
intervals between scans. In this study, we adapt three state-of-the-art
temporal self-supervised learning (SSL) approaches for 3D brain MRI analysis,
and add novel extensions designed to handle variable-length inputs and learn
robust spatial features. We aggregate four publicly available datasets
comprising 3,161 patients for pre-training, and show the performance of our
model across multiple Alzheimer's prediction tasks including diagnosis
classification, conversion detection, and future conversion prediction.
Importantly, our SSL model implemented with temporal order prediction and
contrastive learning outperforms supervised learning on six out of seven
downstream tasks. It demonstrates adaptability and generalizability across
tasks and number of input images with varying time intervals, highlighting its
capacity for robust performance across clinical applications. We release our
code and model publicly at https://github.com/emilykaczmarek/SSL-AD.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [65] [Structured Information Matters: Explainable ICD Coding with Patient-Level Knowledge Graphs](https://arxiv.org/abs/2509.09699)
*Mingyang Li,Viktor Schlegel,Tingting Mu,Warren Del-Pinto,Goran Nenadic*

Main category: cs.CL

TL;DR: 本文提出利用文档级知识图谱（KG）表征临床文档输入，从而提升自动化ICD（国际疾病分类）编码的效果。该方法压缩了文本内容同时保持信息完整性，在主流基准数据集上提升了准确率并提高了模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 临床文本到标准词汇（如ICD编码）的映射对于医学研究、医院管理和病患护理至关重要。但手动编码耗时又困难，自动编码面临高维和长尾分布等挑战。同时，虽然编码输出侧利用了许多外部知识，而输入侧还未充分利用知识型资源。

Method: 作者通过构建和利用临床文档的知识图谱（KG），以结构化方式压缩并表征输入文档（仅23%原文长度但保留90%信息），然后将其融入当前最优ICD编码框架PLM-ICD进行实验评估。

Result: 通过引入文档级知识图谱，实验在主流数据集Macro-F1分数提升最高3.20%，同时训练效率得到改善，并且方法提升了编码结果的可解释性。

Conclusion: 文档级知识图谱作为输入表征，不仅能高效压缩文档信息，还显著提升自动ICD编码的准确率和解释性，为临床文本结构化提供了新思路。

Abstract: Mapping clinical documents to standardised clinical vocabularies is an
important task, as it provides structured data for information retrieval and
analysis, which is essential to clinical research, hospital administration and
improving patient care. However, manual coding is both difficult and
time-consuming, making it impractical at scale. Automated coding can
potentially alleviate this burden, improving the availability and accuracy of
structured clinical data. The task is difficult to automate, as it requires
mapping to high-dimensional and long-tailed target spaces, such as the
International Classification of Diseases (ICD). While external knowledge
sources have been readily utilised to enhance output code representation, the
use of external resources for representing the input documents has been
underexplored. In this work, we compute a structured representation of the
input documents, making use of document-level knowledge graphs (KGs) that
provide a comprehensive structured view of a patient's condition. The resulting
knowledge graph efficiently represents the patient-centred input documents with
23\% of the original text while retaining 90\% of the information. We assess
the effectiveness of this graph for automated ICD-9 coding by integrating it
into the state-of-the-art ICD coding architecture PLM-ICD. Our experiments
yield improved Macro-F1 scores by up to 3.20\% on popular benchmarks, while
improving training efficiency. We attribute this improvement to different types
of entities and relationships in the KG, and demonstrate the improved
explainability potential of the approach over the text-only baseline.

</details>


### [66] [Cross-Layer Attention Probing for Fine-Grained Hallucination Detection](https://arxiv.org/abs/2509.09700)
*Malavika Suresh,Rahaf Aljundi,Ikechukwu Nkisi-Orji,Nirmalie Wiratunga*

Main category: cs.CL

TL;DR: 本文提出了一种新的激活探测技术CLAP（Cross-Layer Attention Probing），能够有效检测大语言模型（LLM）在生成文本时的幻觉（错误信息），相比传统方法有更好的检测精度。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛应用，其生成不准确文本（幻觉问题）成为亟需应对的可靠性难题。如何检测并减少幻觉，是提升模型可信度的关键。

Method: 作者提出CLAP方法，将LLM的所有层的残差流激活作为联合序列进行处理，通过跨层注意机制探查激活信号，以发现和区分幻觉生成。

Result: 在五个大语言模型和三项任务上实证评估显示，CLAP在贪婪解码和高温采样结果中的幻觉检测能力优于现有基线，能够精细地区分同一提示下采样回复中的幻觉与否，且即使在分布外数据上也保持高可靠性。

Conclusion: 利用CLAP，可以先检测、再针对性减缓幻觉现象，从而比直接干预方法有效提升大语言模型整体可靠性。

Abstract: With the large-scale adoption of Large Language Models (LLMs) in various
applications, there is a growing reliability concern due to their tendency to
generate inaccurate text, i.e. hallucinations. In this work, we propose
Cross-Layer Attention Probing (CLAP), a novel activation probing technique for
hallucination detection, which processes the LLM activations across the entire
residual stream as a joint sequence. Our empirical evaluations using five LLMs
and three tasks show that CLAP improves hallucination detection compared to
baselines on both greedy decoded responses as well as responses sampled at
higher temperatures, thus enabling fine-grained detection, i.e. the ability to
disambiguate hallucinations and non-hallucinations among different sampled
responses to a given prompt. This allows us to propose a detect-then-mitigate
strategy using CLAP to reduce hallucinations and improve LLM reliability
compared to direct mitigation approaches. Finally, we show that CLAP maintains
high reliability even when applied out-of-distribution.

</details>


### [67] [Optimal Multi-Task Learning at Regularization Horizon for Speech Translation Task](https://arxiv.org/abs/2509.09701)
*JungHo Jung,Junhyun Lee*

Main category: cs.CL

TL;DR: 本论文提出通过多任务学习(MTL)和不同的正则化方法来提升端到端语音翻译系统的性能，最终在MuST-C数据集上取得了接近SOTA的表现。


<details>
  <summary>Details</summary>
Motivation: 端到端语音到文本翻译系统受限于配对语音文本数据的稀缺性，研究者希望借助MT领域的大量双语文本数据，通过多任务学习缓解数据不足带来的性能瓶颈。

Method: 作者将多任务学习（MTL）从正则化的角度进行建模，综合引入三类正则化方法：1) 不同模态间的一致性正则化（consistency regularization）；2) 同一模态下的R-drop正则化；3) 多任务设置中MT损失权重作为额外正则化源。并提出了“正则化地平线”这一高维超参数调优空间。

Result: 实验表明，在正则化地平线空间内调优相关超参数，可在MuST-C数据集上达到近乎最优的端到端语音翻译效果。

Conclusion: 多源正则化在MTL端到端语音翻译系统中具有显著提升作用；合理选择超参数可进一步释放模型潜力，从而推近SOTA性能。

Abstract: End-to-end speech-to-text translation typically suffers from the scarcity of
paired speech-text data. One way to overcome this shortcoming is to utilize the
bitext data from the Machine Translation (MT) task and perform Multi-Task
Learning (MTL). In this paper, we formulate MTL from a regularization
perspective and explore how sequences can be regularized within and across
modalities. By thoroughly investigating the effect of consistency
regularization (different modality) and R-drop (same modality), we show how
they respectively contribute to the total regularization. We also demonstrate
that the coefficient of MT loss serves as another source of regularization in
the MTL setting. With these three sources of regularization, we introduce the
optimal regularization contour in the high-dimensional space, called the
regularization horizon. Experiments show that tuning the hyperparameters within
the regularization horizon achieves near state-of-the-art performance on the
MuST-C dataset.

</details>


### [68] [Creativity Benchmark: A benchmark for marketing creativity for LLM models](https://arxiv.org/abs/2509.09702)
*Ninad Bhat,Kieran Browne,Pip Bingemann*

Main category: cs.CL

TL;DR: 本文提出了一个专为市场营销创意设计的大型语言模型评测框架，全面分析了不同模型在品牌创意上的表现，强调了人类专家评价的重要性和多样性方法的必要性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在各行业应用的增加，准确有效地评估其在实际业务场景（如市场营销创意）下的表现成为必要，现有自动化评估方法无法替代人类专业视角。

Method: 作者设计了针对12个品类、100个品牌的三种创意提示（Insights, Ideas, Wild Ideas），收集了678位创意从业者对近1.1万组匿名模型输出的两两偏好评选，并用Bradley-Terry模型进行统计分析。此外，以余弦距离等量化模型间输出的多样性，并比较了多种『模型判官』设置与人类排名的相关性。

Result: 各大模型在品牌及创意提示类型上的表现差距很小（胜率差异仅约0.61），没有单一模型全面领先。模型输出的多样性存在，自动化模型判官与人类评价结果相关性弱且表现不稳定，传统创意评价方法只能部分迁移到品牌任务中。

Conclusion: 在品牌创意场景下，自动化评价工具无法替代人类专家的评价，且考虑结果多样性的方法非常重要，建议受控于人类的、多元化的评估流程。

Abstract: We introduce Creativity Benchmark, an evaluation framework for large language
models (LLMs) in marketing creativity. The benchmark covers 100 brands (12
categories) and three prompt types (Insights, Ideas, Wild Ideas). Human
pairwise preferences from 678 practising creatives over 11,012 anonymised
comparisons, analysed with Bradley-Terry models, show tightly clustered
performance with no model dominating across brands or prompt types: the
top-bottom spread is $\Delta\theta \approx 0.45$, which implies a head-to-head
win probability of $0.61$; the highest-rated model beats the lowest only about
$61\%$ of the time. We also analyse model diversity using cosine distances to
capture intra- and inter-model variation and sensitivity to prompt reframing.
Comparing three LLM-as-judge setups with human rankings reveals weak,
inconsistent correlations and judge-specific biases, underscoring that
automated judges cannot substitute for human evaluation. Conventional
creativity tests also transfer only partially to brand-constrained tasks.
Overall, the results highlight the need for expert human evaluation and
diversity-aware workflows.

</details>


### [69] [CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor](https://arxiv.org/abs/2509.09703)
*Zhenhua Xu,Xixiang Zhao,Xubin Yue,Shengwei Tian,Changting Lin,Meng Han*

Main category: cs.CL

TL;DR: 该论文提出了一种新颖的大语言模型（LLM）指纹嵌入方法CTCC，通过对话多轮上下文相关性进行编码，实现模型所有权验证，在隐蔽性和鲁棒性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛应用，模型被盗和非法分发的风险加剧，知识产权保护成为重要问题。现有指纹方法在隐蔽性、鲁棒性和通用性之间存在权衡限制，因此亟需一种更加稳健且难以检测的模型指纹解决方案。

Method: CTCC采用基于规则驱动的指纹框架，利用多轮对话中的上下文关联（如反事实关系）编码指纹，而非传统的单词级或单轮触发，从而可在黑盒场景下验证指纹并减少误报和信息泄露风险。即使部分触发器暴露，也能持续构建并验证所有权。

Result: 通过多个LLM架构的大量实验，CTCC在隐蔽性和鲁棒性方面均优于以往指纹方法，能实现更强的密码隐藏与抗攻击能力。

Conclusion: CTCC被证实为在实际LLM部署场景中可靠且实用的模型所有权验证方案，有效提升了模型知识产权保护能力。

Abstract: The widespread deployment of large language models (LLMs) has intensified
concerns around intellectual property (IP) protection, as model theft and
unauthorized redistribution become increasingly feasible. To address this,
model fingerprinting aims to embed verifiable ownership traces into LLMs.
However, existing methods face inherent trade-offs between stealthness,
robustness, and generalizability, being either detectable via distributional
shifts, vulnerable to adversarial modifications, or easily invalidated once the
fingerprint is revealed. In this work, we introduce CTCC, a novel rule-driven
fingerprinting framework that encodes contextual correlations across multiple
dialogue turns, such as counterfactual, rather than relying on token-level or
single-turn triggers. CTCC enables fingerprint verification under black-box
access while mitigating false positives and fingerprint leakage, supporting
continuous construction under a shared semantic rule even if partial triggers
are exposed. Extensive experiments across multiple LLM architectures
demonstrate that CTCC consistently achieves stronger stealth and robustness
than prior work. Our findings position CTCC as a reliable and practical
solution for ownership verification in real-world LLM deployment scenarios. Our
code and data are publicly available at <https://github.com/Xuzhenhua55/CTCC>.

</details>


### [70] [Temporal Preferences in Language Models for Long-Horizon Assistance](https://arxiv.org/abs/2509.09704)
*Ali Mazyaki,Mohammad Naghizadeh,Samaneh Ranjkhah Zonouzaghi,Hossein Setareh*

Main category: cs.CL

TL;DR: 这篇论文研究了大语言模型（LMs）在跨期选择任务中的时间取向（未来导向与当下导向）偏好，并提出可量化的操作性指标，验证了模型偏好的可操纵性。同时探讨了AI个性化校准和社会部署的可能方向。


<details>
  <summary>Details</summary>
Motivation: 人类在跨期选择时常体现出不同的时间偏好，在AI和大语言模型广泛应用于决策推荐时，理解和操控这些模型的时间取向对个性化决策支持和符合用户长期目标至关重要。

Method: 采用改编自人类心理学实验的时间权衡任务，对多个大语言模型进行测试，并与人类决策数据进行基准对比。引入操作性指标“时间取向可操控性（MTO）”，量化模型在不同提示下偏好的可变性。还考察了模型对于身份与地域的个性化能力。

Result: 注重推理能力的模型（如DeepSeek-Reasoner、grok-3-mini）在被未来导向提示引导时更倾向做出“延迟满足”决策，但在跨身份、地域个性化方面表现有限。这些模型作为AI决策体时也倾向内化未来导向。

Conclusion: 合理引导下，大语言模型的时间偏好具备一定的可操控性。设计AI助手时应充分考虑不同用户的长期目标和多样化诉求，需进一步研究AI的个性化校准和社会情境适应能力。

Abstract: We study whether language models (LMs) exhibit future- versus
present-oriented preferences in intertemporal choice and whether those
preferences can be systematically manipulated. Using adapted human experimental
protocols, we evaluate multiple LMs on time-tradeoff tasks and benchmark them
against a sample of human decision makers. We introduce an operational metric,
the Manipulability of Time Orientation (MTO), defined as the change in an LM's
revealed time preference between future- and present-oriented prompts. In our
tests, reasoning-focused models (e.g., DeepSeek-Reasoner and grok-3-mini)
choose later options under future-oriented prompts but only partially
personalize decisions across identities or geographies. Moreover, models that
correctly reason about time orientation internalize a future orientation for
themselves as AI decision makers. We discuss design implications for AI
assistants that should align with heterogeneous, long-horizon goals and outline
a research agenda on personalized contextual calibration and socially aware
deployment.

</details>


### [71] [The Non-Determinism of Small LLMs: Evidence of Low Answer Consistency in Repetition Trials of Standard Multiple-Choice Benchmarks](https://arxiv.org/abs/2509.09705)
*Claudio Pinhanez,Paulo Cavalin,Cassia Sanctos,Marcelo Grave,Yago Primerano*

Main category: cs.CL

TL;DR: 本文分析了小规模LLM（参数量2B-8B）在多次重复回答同一问题时的一致性，并对比了不同规模、Finetune与否及推理参数的影响，提出了分析工具，并发现不同模型间一致性差异明显，模型规模和温度对一致性影响较大。


<details>
  <summary>Details</summary>
Motivation: 目前大多数关注点在LLM回答准确性，但对于多次询问同一问题时小模型答案是否一致性关注较少。本研究希望通过量化一致性、探讨影响因素，为小模型实际部署提供参考。

Method: 对多个开源LLM（小规模和中型）在MMLU-Redux和MedQA的选择题上，重复问同一问题10次，并对比不同推理温度、模型规模、是否微调等对一致性的影响。同时提出新的分析与可视化工具来辅助研究。

Result: 小模型在低温度下，多次一致回答的比例通常在50%-80%之间，且准确性与一致性有较高相关。中型模型则表现出更高的一致性。不同模型间一致性差距较大。

Conclusion: 小模型在回答一致性上存在较大提升空间，模型规模和推理温度等因素显著影响一致性；想实现“一致且准确”时需在一致性和准确率间权衡。新的分析工具有助于社区深入理解此类问题。

Abstract: This work explores the consistency of small LLMs (2B-8B parameters) in
answering multiple times the same question. We present a study on known,
open-source LLMs responding to 10 repetitions of questions from the
multiple-choice benchmarks MMLU-Redux and MedQA, considering different
inference temperatures, small vs. medium models (50B-80B), finetuned vs. base
models, and other parameters. We also look into the effects of requiring
multi-trial answer consistency on accuracy and the trade-offs involved in
deciding which model best provides both of them. To support those studies, we
propose some new analytical and graphical tools. Results show that the number
of questions which can be answered consistently vary considerably among models
but are typically in the 50%-80% range for small models at low inference
temperatures. Also, accuracy among consistent answers seems to reasonably
correlate with overall accuracy. Results for medium-sized models seem to
indicate much higher levels of answer consistency.

</details>


### [72] [Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal](https://arxiv.org/abs/2509.09708)
*Nirmalendu Prakash,Yeo Wei Jie,Amir Abdullah,Ranjan Satapathy,Erik Cambria,Roy Ka Wei Lee*

Main category: cs.CL

TL;DR: 本文研究了大模型在面对有害提示时拒绝响应的内部机制，通过稀疏自编码器分析模型的潜在特征，并发现了可以导致模型由拒绝转向顺从的关键特征集合。


<details>
  <summary>Details</summary>
Motivation: 虽然微调后的大语言模型能拒绝执行有害任务，但其内部因果机制尚不清楚。理解这些机制有助于提升模型安全性与可控性。

Method: 作者针对Gemma-2-2B-IT和LLaMA-3.1-8B-IT，训练稀疏自编码器捕获残差流激活，并在有害提示下，在潜在空间搜索通过特征消融能令模型从拒绝转向顺从的特征集合。具体流程为：1) 找到拒绝响应方向并筛选相关特征；2) 采用贪心法缩减特征集；3) 用分解机建模特征间非线性交互关系。

Result: 提出的流程找到了能破解模型拒绝行为的关键特征集合，并发现部分特征具有冗余性，仅在其它特征被抑制时才激活。

Conclusion: 该研究揭示了安全行为的细粒度可审计性和可控性，表明通过操作可解释潜在空间，可实现对模型安全机制的定向干预。

Abstract: Refusal on harmful prompts is a key safety behaviour in instruction-tuned
large language models (LLMs), yet the internal causes of this behaviour remain
poorly understood. We study two public instruction-tuned models, Gemma-2-2B-IT
and LLaMA-3.1-8B-IT, using sparse autoencoders (SAEs) trained on
residual-stream activations. Given a harmful prompt, we search the SAE latent
space for feature sets whose ablation flips the model from refusal to
compliance, demonstrating causal influence and creating a jailbreak. Our search
proceeds in three stages: (1) Refusal Direction: find a refusal-mediating
direction and collect SAE features near that direction; (2) Greedy Filtering:
prune to a minimal set; and (3) Interaction Discovery: fit a factorization
machine (FM) that captures nonlinear interactions among the remaining active
features and the minimal set. This pipeline yields a broad set of
jailbreak-critical features, offering insight into the mechanistic basis of
refusal. Moreover, we find evidence of redundant features that remain dormant
unless earlier features are suppressed. Our findings highlight the potential
for fine-grained auditing and targeted intervention in safety behaviours by
manipulating the interpretable latent space.

</details>


### [73] [Assisting Research Proposal Writing with Large Language Models: Evaluation and Refinement](https://arxiv.org/abs/2509.09709)
*Jing Ren,Weiqi Wang*

Main category: cs.CL

TL;DR: 本文针对ChatGPT等大型语言模型在学术写作中产生错误或虚假引用以及内容评判主观性强的问题，提出了两个量化评估指标与一种迭代提示方法，提升写作质量并减少引用问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在学术写作中使用越来越广泛，但其可能生成不正确或虚假的文献引用，带来伦理风险。现有对内容质量的评估依赖人工判断，主观且耗时，缺乏客观量化方法。因此，亟需开发更客观、自动化的评估体系，提升模型输出的可靠性。

Method: 论文提出了两项核心评估指标：内容质量和引用有效性，并以此为基础设计了迭代式提示方法。通过根据上述指标评分，反复优化模型的写作表现，从而提升内容以及引用的准确性。

Result: 实验表明，所提的评估指标能为ChatGPT学术写作表现提供客观的量化评估。迭代提示显著提高了内容质量，同时大幅减少了不准确和虚假的引用。

Conclusion: 这种基于量化指标和迭代优化的方法，有效提升了大型语言模型在学术写作中的表现，并能应对引用造假等伦理风险，适合在学术相关领域推广应用。

Abstract: Large language models (LLMs) like ChatGPT are increasingly used in academic
writing, yet issues such as incorrect or fabricated references raise ethical
concerns. Moreover, current content quality evaluations often rely on
subjective human judgment, which is labor-intensive and lacks objectivity,
potentially compromising the consistency and reliability. In this study, to
provide a quantitative evaluation and enhance research proposal writing
capabilities of LLMs, we propose two key evaluation metrics--content quality
and reference validity--and an iterative prompting method based on the scores
derived from these two metrics. Our extensive experiments show that the
proposed metrics provide an objective, quantitative framework for assessing
ChatGPT's writing performance. Additionally, iterative prompting significantly
enhances content quality while reducing reference inaccuracies and
fabrications, addressing critical ethical challenges in academic contexts.

</details>


### [74] [Generating Individual Travel Diaries Using Large Language Models Informed by Census and Land-Use Data](https://arxiv.org/abs/2509.09710)
*Sepehr Golrokh Amin,Devin Rhoads,Fatemeh Fakhrmoosavi,Nicholas E. Lownes,John N. Ivan*

Main category: cs.CL

TL;DR: 本研究提出了一种利用大语言模型（LLM）生成个体出行日记的方法，并通过多个现实度指标与经典方法进行对比验证。结果显示，LLM能够以接近甚至优于传统模型的质量，零样本（zero-shot）生成高代表性的合成出行日记。


<details>
  <summary>Details</summary>
Motivation: 现有基于代理的交通模型需要大量昂贵且私有的出行调查数据，数据采集受限。研究动机在于探索是否能用开源数据和大语言模型替代传统数据密集型方法，降低模拟门槛并提升泛化能力。

Method: 方法分为两步：首先，基于美国社区调查（ACS）和智能位置数据库（SLD）的开源数据，随机生成代表性个体画像（personas）；其次，直接通过LLM提示语合成出行日记。论文还提出了包含四个评分的日记真实性指标体系，并基于康涅狄格州真实出行日记进行验证。使用Jensen-Shannon Divergence衡量合成与真实分布的相似性。

Result: LLM生成的日记在整体真实性得分（平均为0.485）与经典方法（0.455）相当，且在出行目的与分布一致性上表现更好，但在出行次数和活动时长的数值精度上略逊于经典模型。总体代表性检验显示LLM优于经典模型（0.612 vs 0.435）。

Conclusion: LLM可用于合成高质量的个体出行日记，具备零样本生成能力，为未来合成日记评价体系提供了新思路。LLM方法突破了对大规模调查数据的依赖，有望提升交通微观模拟的效率与广泛适用性。

Abstract: This study introduces a Large Language Model (LLM) scheme for generating
individual travel diaries in agent-based transportation models. While
traditional approaches rely on large quantities of proprietary household travel
surveys, the method presented in this study generates personas stochastically
from open-source American Community Survey (ACS) and Smart Location Database
(SLD) data, then synthesizes diaries through direct prompting. This study
features a novel one-to-cohort realism score: a composite of four metrics (Trip
Count Score, Interval Score, Purpose Score, and Mode Score) validated against
the Connecticut Statewide Transportation Study (CSTS) diaries, matched across
demographic variables. The validation utilizes Jensen-Shannon Divergence to
measure distributional similarities between generated and real diaries. When
compared to diaries generated with classical methods (Negative Binomial for
trip generation; Multinomial Logit for mode/purpose) calibrated on the
validation set, LLM-generated diaries achieve comparable overall realism (LLM
mean: 0.485 vs. 0.455). The LLM excels in determining trip purpose and
demonstrates greater consistency (narrower realism score distribution), while
classical models lead in numerical estimates of trip count and activity
duration. Aggregate validation confirms the LLM's statistical
representativeness (LLM mean: 0.612 vs. 0.435), demonstrating LLM's zero-shot
viability and establishing a quantifiable metric of diary realism for future
synthetic diary evaluation systems.

</details>


### [75] [Psychiatry-Bench: A Multi-Task Benchmark for LLMs in Psychiatry](https://arxiv.org/abs/2509.09711)
*Aya E. Fouda,Abdelrahamn A. Hassan,Radwa J. Hanafy,Mohammed E. Fouda*

Main category: cs.CL

TL;DR: 本研究提出了PsychiatryBench，这是一个基于权威精神病学教材和案例书的高质量基准，用于全面评估大语言模型（LLM）在精神病学多任务的表现。通过对多个主流模型进行测试，发现LLM在临床一致性和安全性方面仍有较大差距，特别是在多轮随访和管理任务上。


<details>
  <summary>Details</summary>
Motivation: 现有用于评估LLM在精神科应用的资源主要依赖于小型访谈语料、社交媒体数据或合成对话，缺乏权威性，不能反映真实的临床推理复杂度，限制了模型在精神健康领域的应用和发展。

Method: 作者设计了PsychiatryBench基准，内容全部来源于专家认证的精神病学教材和案例书，涵盖诊断推理、治疗方案、长期随访、管理计划、临床路径、案例分析等十一类任务，共计超5300个专家标注项目。并用多个前沿LLM和开源医学模型，通过常规指标及“LLM自动评分”框架进行评测。

Result: 评测显示，当前LLM在精神科相关任务上临床一致性和安全性不足，特别是在需要多轮对话和涉及时序决策的任务上表现不佳，暴露出明显短板。

Conclusion: PsychiatryBench为高风险精神健康相关LLM模型的基准评测和能力提升提供了权威工具，有助于推动专门化模型训练和更严格的评价方法发展，为精神科LLM的临床应用奠定基础。

Abstract: Large language models (LLMs) hold great promise in enhancing psychiatric
practice, from improving diagnostic accuracy to streamlining clinical
documentation and therapeutic support. However, existing evaluation resources
heavily rely on small clinical interview corpora, social media posts, or
synthetic dialogues, which limits their clinical validity and fails to capture
the full complexity of psychiatric reasoning. In this work, we introduce
PsychiatryBench, a rigorously curated benchmark grounded exclusively in
authoritative, expert-validated psychiatric textbooks and casebooks.
PsychiatryBench comprises eleven distinct question-answering tasks ranging from
diagnostic reasoning and treatment planning to longitudinal follow-up,
management planning, clinical approach, sequential case analysis, and
multiple-choice/extended matching formats totaling over 5,300 expert-annotated
items. We evaluate a diverse set of frontier LLMs (including Google Gemini,
DeepSeek, LLaMA 3, and QWQ-32) alongside leading open-source medical models
(e.g., OpenBiloLLM, MedGemma) using both conventional metrics and an
"LLM-as-judge" similarity scoring framework. Our results reveal substantial
gaps in clinical consistency and safety, particularly in multi-turn follow-up
and management tasks, underscoring the need for specialized model tuning and
more robust evaluation paradigms. PsychiatryBench offers a modular, extensible
platform for benchmarking and improving LLM performance in high-stakes mental
health applications.

</details>


### [76] [The Thinking Therapist: Training Large Language Models to Deliver Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio Policy Optimization](https://arxiv.org/abs/2509.09712)
*Talha Tahir*

Main category: cs.CL

TL;DR: 本研究评估了两种后训练方法和显式链式思考（COT）推理步骤对小型开源LLM进行ACT治疗能力的影响。结果显示，ORPO方法优于传统SFT和Instruct，对SFT则COT有明确提升。


<details>
  <summary>Details</summary>
Motivation: ACT是一种认知行为疗法，越来越多证据显示其对多种精神疾病有效。随着大语言模型在心理健康领域的应用，如何让小型LLM更好地执行ACT，是推动AI心理干预实践的重要课题。

Method: 作者以50组由Mistral-Large生成的ACT合成对话为数据集，使用SFT和ORPO两种方式（各自搭配是否加入COT推理）对Llama-3.2-3b-Instruct模型进行训练。之后，利用ACT-Fidelity Measure和Therapist Empathy Scale进行量化评估，评分由精调过的LLM Judge完成。

Result: ORPO方法在ACT忠实性（χ^2=185.15, p<.001）和同理心（χ^2=140.37, p<.001）上均显著优于SFT和Instruct。COT推理只提升了SFT训练模型的表现（ACT-FM平均提升2.68分，p<.001），对ORPO与Instruct则无帮助。

Conclusion: 偏好对齐的策略优化（ORPO）能有效培养小型LLM的ACT能力，COT推理对仅靠模仿训练模型的提升明显，但对于已掌握流程的模型价值有限。显式推理的使用需依赖于具体训练范式。

Abstract: Acceptance and Commitment Therapy (ACT) is a third-wave cognitive behavioral
therapy with emerging evidence of efficacy in several psychiatric conditions.
This study investigates the impact of post-training methodology and explicit
reasoning on the ability of a small open-weight large language model (LLM) to
deliver ACT. Using 50 sets of synthetic ACT transcripts generated by
Mistral-Large, we trained Llama-3.2-3b-Instruct with two distinct approaches,
supervised fine-tuning (SFT) and odds ratio policy optimization (ORPO), each
with and without an explicit chain-of-thought (COT) reasoning step. Performance
was evaluated by comparing these four post-trained variants against the base
Instruct model. These models were benchmarked in simulated therapy sessions,
with performance quantitatively assessed on the ACT Fidelity Measure (ACT-FM)
and the Therapist Empathy Scale (TES) by an LLM judge that had been fine-tuned
on human evaluations. Our findings demonstrate that the ORPO-trained models
significantly outperformed both their SFT and Instruct counterparts on ACT
fidelity ($\chi^2(5) = 185.15, p < .001$) and therapeutic empathy ($\chi^2(5) =
140.37, p < .001$). The effect of COT was conditional as it provided a
significant benefit to SFT models, improving ACT-FM scores by an average of
2.68 points ($p < .001$), while offering no discernible advantage to the
superior ORPO or instruct-tuned variants. We posit that the superiority of ORPO
stems from its ability to learn the therapeutic `process' over imitating
`content,' a key aspect of ACT, while COT acts as a necessary scaffold for
models trained only via imitation. This study establishes that
preference-aligned policy optimization can effectively instill ACT competencies
in small LLMs, and that the utility of explicit reasoning is highly dependent
on the underlying training paradigm.

</details>


### [77] [HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering](https://arxiv.org/abs/2509.09713)
*Duolin Sun,Dan Yang,Yue Shen,Yihan Jiao,Zhehao Tan,Jie Feng,Lianzhen Zhong,Jian Wang,Peng Wei,Jinjie Gu*

Main category: cs.CL

TL;DR: 本文提出了HANRAG，一种基于启发式的新型RAG框架，通过查询路由、子问题分解及噪声过滤显著提升了多跳问答性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于RAG的方法虽能提升问答与对话生成系统，但在处理多跳复杂查询时面临检索迭代过度与噪声积累等问题，影响了系统的准确性与效率。

Method: 提出HANRAG框架，引入启发式revelator模块，实现查询分路、复杂查询分解为子问题，并在检索内容中主动过滤噪声，以提升系统适应性和鲁噪声能力。

Result: 在多个单跳和多跳问答任务基准上，HANRAG均优于现有主流RAG方法，展现出显著性能提升。

Conclusion: HANRAG框架通过路由、分解与噪声过滤机制，有效解决了多跳查询检索不足与噪声积累问题，增强了问答系统的泛化与鲁棒能力。

Abstract: The Retrieval-Augmented Generation (RAG) approach enhances question-answering
systems and dialogue generation tasks by integrating information retrieval (IR)
technologies with large language models (LLMs). This strategy, which retrieves
information from external knowledge bases to bolster the response capabilities
of generative models, has achieved certain successes. However, current RAG
methods still face numerous challenges when dealing with multi-hop queries. For
instance, some approaches overly rely on iterative retrieval, wasting too many
retrieval steps on compound queries. Additionally, using the original complex
query for retrieval may fail to capture content relevant to specific
sub-queries, resulting in noisy retrieved content. If the noise is not managed,
it can lead to the problem of noise accumulation. To address these issues, we
introduce HANRAG, a novel heuristic-based framework designed to efficiently
tackle problems of varying complexity. Driven by a powerful revelator, HANRAG
routes queries, decomposes them into sub-queries, and filters noise from
retrieved documents. This enhances the system's adaptability and noise
resistance, making it highly capable of handling diverse queries. We compare
the proposed framework against other leading industry methods across various
benchmarks. The results demonstrate that our framework obtains superior
performance in both single-hop and multi-hop question-answering tasks.

</details>


### [78] [How Small Transformation Expose the Weakness of Semantic Similarity Measures](https://arxiv.org/abs/2509.09714)
*Serge Lionel Nikiema,Albérick Euraste Djire,Abdoul Aziz Bonkoungou,Micheline Bénédicte Moumoula,Jordan Samhi,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.CL

TL;DR: 本文系统评估了18种语义相似度测量方法在处理不同语义关系时的有效性，包括LLM、嵌入与传统方法。结果显示主流嵌入方法存在显著误判，而LLM方法在区分语义差异方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 语义相似度测量对于代码搜索、API推荐、自动代码审查等软件工程任务至关重要。目前主流方法包括大语言模型和嵌入技术，但其是否真正理解语义关系尚有争议。

Method: 作者搭建了系统性的测试框架，通过对文本和代码进行受控修改，全面评估了18种语义相似度方法（包括词基、嵌入、LLM和结构感知方法）在不同语义关系下的表现。

Result: 结果发现，许多嵌入方法会误判语义相反的内容为高相似度（最高达99.9%），某些Transformer方法甚至认为反义词比同义词更相似。改用余弦相似度可提升24-66%的表现。LLM方法可有效区分语义不同的内容，能给出较低相似度分数（0.00-0.29），而嵌入法往往会给这类内容打出较高分（0.82-0.99）。

Conclusion: 主流嵌入方法在语义理解上存在明显缺陷，距离度量选择影响较大；相比之下，LLM方法更适合于需要深度语义理解的场景。

Abstract: This research examines how well different methods measure semantic
similarity, which is important for various software engineering applications
such as code search, API recommendations, automated code reviews, and
refactoring tools. While large language models are increasingly used for these
similarity assessments, questions remain about whether they truly understand
semantic relationships or merely recognize surface patterns.
  The study tested 18 different similarity measurement approaches, including
word-based methods, embedding techniques, LLM-based systems, and
structure-aware algorithms. The researchers created a systematic testing
framework that applies controlled changes to text and code to evaluate how well
each method handles different types of semantic relationships.
  The results revealed significant issues with commonly used metrics. Some
embedding-based methods incorrectly identified semantic opposites as similar up
to 99.9 percent of the time, while certain transformer-based approaches
occasionally rated opposite meanings as more similar than synonymous ones. The
study found that embedding methods' poor performance often stemmed from how
they calculate distances; switching from Euclidean distance to cosine
similarity improved results by 24 to 66 percent. LLM-based approaches performed
better at distinguishing semantic differences, producing low similarity scores
(0.00 to 0.29) for genuinely different meanings, compared to embedding methods
that incorrectly assigned high scores (0.82 to 0.99) to dissimilar content.

</details>


### [79] [Investigating Symbolic Triggers of Hallucination in Gemma Models Across HaluEval and TruthfulQA](https://arxiv.org/abs/2509.09715)
*Naveen Lamba,Sanju Tiwari,Manas Gaur*

Main category: cs.CL

TL;DR: 该研究分析了大语言模型（LLMs）内在的幻觉（hallucination）易感性，揭示了符号性属性是导致模型幻觉的关键因素，并且即使随着模型规模增大，这一问题依然显著存在。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM幻觉问题被广泛关注，但模型为何本质上容易发生幻觉，其内在脆弱属性尚未被系统识别与研究。本文旨在厘清LLM内部导致幻觉的关键机制。

Method: 作者利用HaluEval和TruthfulQA两个数据集，将问答格式转换为多种变体任务，对比不同格式下模型表现，定位导致幻觉出现的符号属性（如修饰词和命名实体），衡量Gemma系列（2-2B, 2-9B, 2-27B）在此类任务下的幻觉比例。

Result: Gemma系列中，2-2B型号平均幻觉率为79.0%，2-9B为73.6%，2-27B为63.9%，表明模型规模扩大可减少幻觉，但某些符号属性如修饰词（84.76%-94.98%）、命名实体（83.87%-93.96%）幻觉率仍极高。

Conclusion: LLMs在处理带有强烈符号属性的输入（尤其是修饰词、命名实体）时，幻觉现象十分顽固，且模型规模提升未能根本解决这一弱点，彰显其内部机制存在固有脆弱性。

Abstract: Hallucination in Large Language Models (LLMs) is a well studied problem.
However, the properties that make LLM intrinsically vulnerable to
hallucinations have not been identified and studied. This research identifies
and characterizes the key properties, allowing us to pinpoint vulnerabilities
within the model's internal mechanisms. To solidify on these properties, we
utilized two established datasets, HaluEval and TruthfulQA and convert their
existing format of question answering into various other formats to narrow down
these properties as the reason for the hallucinations. Our findings reveal that
hallucination percentages across symbolic properties are notably high for
Gemma-2-2B, averaging 79.0% across tasks and datasets. With increased model
scale, hallucination drops to 73.6% for Gemma-2-9B and 63.9% for Gemma-2-27B,
reflecting a 15 percentage point reduction overall. Although the hallucination
rate decreases as the model size increases, a substantial amount of
hallucination caused by symbolic properties still persists. This is especially
evident for modifiers (ranging from 84.76% to 94.98%) and named entities
(ranging from 83.87% to 93.96%) across all Gemma models and both datasets.
These findings indicate that symbolic elements continue to confuse the models,
pointing to a fundamental weakness in how these LLMs process such
inputs--regardless of their scale.

</details>


### [80] [ALIGNS: Unlocking nomological networks in psychological measurement through a large language model](https://arxiv.org/abs/2509.09723)
*Kai R. Larsen,Sen Yan,Roland Müller,Lan Sang,Mikko Rönkkö,Ravi Starzl,Donald Edmondson*

Main category: cs.CL

TL;DR: 该论文提出了ALIGNS，一种基于大型语言模型的系统，能够自动构建庞大的理论关系网，以提升心理学测量工具的效度研究，并免费开放。


<details>
  <summary>Details</summary>
Motivation: 现有心理测量工具的效度验证依赖构建nomological networks（理论关系网），但这一过程繁琐、难以扩展，导致实际应用中存在严重局限，影响临床与政策决策。

Method: 通过训练大型语言模型（LLM），利用验证过的问卷量表数据，实现自动化、大规模生成覆盖心理学、医学、社会政策等领域的理论关系网。

Result: ALIGNS生成了涵盖55万个指标的三大关系网。多项评估显示该系统具分类准确性，在NIH PROMIS量表、儿童气质测量等任务上显现有效的新发现，专家认可其实用性。

Conclusion: ALIGNS创新性地用大模型解决了测量效度验证中的基础难题，不仅补充了传统手段，还具备高效性与可获得性，推动相关领域理论和实际测量的发展。

Abstract: Psychological measurement is critical to many disciplines. Despite advances
in measurement, building nomological networks, theoretical maps of how concepts
and measures relate to establish validity, remains a challenge 70 years after
Cronbach and Meehl proposed them as fundamental to validation. This limitation
has practical consequences: clinical trials may fail to detect treatment
effects, and public policy may target the wrong outcomes. We introduce Analysis
of Latent Indicators to Generate Nomological Structures (ALIGNS), a large
language model-based system trained with validated questionnaire measures.
ALIGNS provides three comprehensive nomological networks containing over
550,000 indicators across psychology, medicine, social policy, and other
fields. This represents the first application of large language models to solve
a foundational problem in measurement validation. We report classification
accuracy tests used to develop the model, as well as three evaluations. In the
first evaluation, the widely used NIH PROMIS anxiety and depression instruments
are shown to converge into a single dimension of emotional distress. The second
evaluation examines child temperament measures and identifies four potential
dimensions not captured by current frameworks, and questions one existing
dimension. The third evaluation, an applicability check, engages expert
psychometricians who assess the system's importance, accessibility, and
suitability. ALIGNS is freely available at nomologicalnetwork.org,
complementing traditional validation methods with large-scale nomological
analysis.

</details>


### [81] [DiTTO-LLM: Framework for Discovering Topic-based Technology Opportunities via Large Language Model](https://arxiv.org/abs/2509.09724)
*Wonyoung Kim,Sujeong Seo,Juhyun Lee*

Main category: cs.CL

TL;DR: 本文提出了一种基于时序关系与大语言模型的新框架，用于从专利数据中自动识别新兴技术机会，在人工智能领域实验有效。


<details>
  <summary>Details</summary>
Motivation: 技术机会的发现对于技术、产业和创新至关重要。随着专利数量与技术复杂度剧增，传统方法难以及时、准确地识别新兴技术机会，需要新的自动化分析方法。

Method: 提出了以时间序列为基础的框架，首先从专利数据集中抽取文本，再通过大语言模型提取和映射主题，挖掘技术间的关系，并利用聊天式语言模型通过设计提问，辅助发现新技术机会。最后，通过分析主题随时间的变化来识别技术机会。整个流程在美国专利局人工智能专利数据集上进行了测试。

Result: 实验结果显示，人工智能技术正朝着支持日常可及性的方向不断演化，说明框架可以挖掘出相关并具有前瞻性的技术机会。

Conclusion: 提出的时序主题分析结合大语言模型的方法在识别人工智能领域的新兴技术机会方面表现良好，为技术前沿及创新发展趋势的判定提供了有效工具。

Abstract: Technology opportunities are critical information that serve as a foundation
for advancements in technology, industry, and innovation. This paper proposes a
framework based on the temporal relationships between technologies to identify
emerging technology opportunities. The proposed framework begins by extracting
text from a patent dataset, followed by mapping text-based topics to discover
inter-technology relationships. Technology opportunities are then identified by
tracking changes in these topics over time. To enhance efficiency, the
framework leverages a large language model to extract topics and employs a
prompt for a chat-based language model to support the discovery of technology
opportunities. The framework was evaluated using an artificial intelligence
patent dataset provided by the United States Patent and Trademark Office. The
experimental results suggest that artificial intelligence technology is
evolving into forms that facilitate everyday accessibility. This approach
demonstrates the potential of the proposed framework to identify future
technology opportunities.

</details>


### [82] [BIBERT-Pipe on Biomedical Nested Named Entity Linking at BioASQ 2025](https://arxiv.org/abs/2509.09725)
*Chunyu Li,Xindi Zheng,Siqi Liu*

Main category: cs.CL

TL;DR: 本文提出了一种高效轻量的生物医学多语言嵌套实体链接系统，针对现实中英语和俄语文本中的复杂实体提及问题，在BioNNE 2025比赛中获得了多语言赛道第三名。


<details>
  <summary>Details</summary>
Motivation: 当前的生物医学实体链接任务多以单语言、扁平实体数据集为基准，忽视了实际应用中嵌套、多语言实体提及的复杂性。研究动机在于弥补该领域的研究空白，使EL系统更贴近真实场景。

Method: 提出了轻量级pipeline方法，仅对三个与任务对齐的组件进行修改：1）两阶段检索-排序机制，排序阶段进行领域特定微调；2）在排序阶段通过可学习的特殊标记显式包裹实体边界；3）自动利用三种不同来源扩增排序训练集。

Result: 提出的BIBERT-Pipe系统在BioNNE 2025多语言赛道排行榜上排名第三，显示了方法的有效性和竞争力。

Conclusion: 在不更改主模型主体结构的前提下，仅通过少量组件调整，即可应对多语言、嵌套实体识别场景，提升系统鲁棒性与适用性，对推动实际生物医学文本处理具有参考价值。

Abstract: Entity linking (EL) for biomedical text is typically benchmarked on
English-only corpora with flat mentions, leaving the more realistic scenario of
nested and multilingual mentions largely unexplored. We present our system for
the BioNNE 2025 Multilingual Biomedical Nested Named Entity Linking shared task
(English & Russian), closing this gap with a lightweight pipeline that keeps
the original EL model intact and modifies only three task-aligned components:
Two-stage retrieval-ranking. We leverage the same base encoder model in both
stages: the retrieval stage uses the original pre-trained model, while the
ranking stage applies domain-specific fine-tuning. Boundary cues. In the
ranking stage, we wrap each mention with learnable [Ms] / [Me] tags, providing
the encoder with an explicit, language-agnostic span before robustness to
overlap and nesting. Dataset augmentation. We also automatically expand the
ranking training corpus with three complementary data sources, enhancing
coverage without extra manual annotation. On the BioNNE 2025 leaderboard, our
two stage system, bilingual bert (BIBERT-Pipe), ranks third in the multilingual
track, demonstrating the effectiveness and competitiveness of these minimal yet
principled modifications. Code are publicly available at
https://github.com/Kaggle-Competitions-Code/BioNNE-L.

</details>


### [83] [Natural Language Translation of Formal Proofs through Informalization of Proof Steps and Recursive Summarization along Proof Structure](https://arxiv.org/abs/2509.09726)
*Seiji Hattori,Takuya Matsuzaki,Makoto Fujiwara*

Main category: cs.CL

TL;DR: 本论文提出了一种利用大型语言模型（LLM）将机器可验证的形式化证明自动翻译成高可读性的自然语言证明的方法。


<details>
  <summary>Details</summary>
Motivation: 自动化形式化证明生成虽然保障了正确性，但表达晦涩不易理解。将其转换为自然语言有助于增进理解、教学和沟通。因此如何借助LLM高质量地自然语言化成为研究动机。

Method: 该方法采用LLM对形式化证明的每步进行非形式化转述和摘要（informalization & summarization），并将结果重组成易于人类阅读的自然语言证明。作者在实际案例和Lean证明库上验证了方法效果。

Result: 结果表明，生成的自然语言证明与原始教科书形式的自然语言证明在可读性和准确性上具有可比性，方法同样适用于Lean现有的正式证明库。

Conclusion: 利用LLM，可以有效将晦涩的机器形式化证明自然语言化，达到了高可读性和准确性，有望推动自动化证明的广泛理解和应用。

Abstract: This paper proposes a natural language translation method for
machine-verifiable formal proofs that leverages the informalization
(verbalization of formal language proof steps) and summarization capabilities
of LLMs. For evaluation, it was applied to formal proof data created in
accordance with natural language proofs taken from an undergraduate-level
textbook, and the quality of the generated natural language proofs was analyzed
in comparison with the original natural language proofs. Furthermore, we will
demonstrate that this method can output highly readable and accurate natural
language proofs by applying it to existing formal proof library of the Lean
proof assistant.

</details>


### [84] [A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs](https://arxiv.org/abs/2509.09727)
*Andy Zhu,Yingjun Du*

Main category: cs.CL

TL;DR: 本文提出了一种多智能体的角色促发框架，以提升大型语言模型在金融问答任务中的表现，尤其是在复杂推理和专业知识方面，实验表明其方法显著提升了回答准确率，并具有较高的性价比。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在金融教育问答中的推理和专业性表现有限，金融领域问题需要多步定量推理、领域术语理解和真实场景把握，因此迫切需要新方法来应对金融问答的特殊挑战。

Method: 作者设计了由基础生成器(Base Generator)、证据检索器(Evidence Retriever)和专家审查员(Expert Reviewer)构成的多智能体框架。该系统利用RAG技术从6本金融教材检索背景知识，通过专家角色的批判性审查提升回答质量，所有操作在单轮（一轮）内完成。

Result: 在Study.com平台的3,532道专家级金融教育题上，论文方法比零样本思维链(Chain-of-Thought)基础方案提升了6.6-8.3%的准确率，使用Gemini-2.0-Flash模型时表现最佳。此外，该方法也使得轻量版GPT-4o-mini模型表现接近于微调版FinGPT-mt_Llama3-8B_LoRA。

Conclusion: 多智能体角色促发和批判式精炼方法能以较低成本大幅提升金融领域问答的准确性，为未来金融专用大模型的研究和应用提供了有效思路。

Abstract: Question answering (QA) plays a central role in financial education, yet
existing large language model (LLM) approaches often fail to capture the
nuanced and specialized reasoning required for financial problem-solving. The
financial domain demands multistep quantitative reasoning, familiarity with
domain-specific terminology, and comprehension of real-world scenarios. We
present a multi-agent framework that leverages role-based prompting to enhance
performance on domain-specific QA. Our framework comprises a Base Generator, an
Evidence Retriever, and an Expert Reviewer agent that work in a single-pass
iteration to produce a refined answer. We evaluated our framework on a set of
3,532 expert-designed finance education questions from Study.com, an online
learning platform. We leverage retrieval-augmented generation (RAG) for
contextual evidence from 6 finance textbooks and prompting strategies for a
domain-expert reviewer. Our experiments indicate that critique-based refinement
improves answer accuracy by 6.6-8.3% over zero-shot Chain-of-Thought baselines,
with the highest performance from Gemini-2.0-Flash. Furthermore, our method
enables GPT-4o-mini to achieve performance comparable to the finance-tuned
FinGPT-mt_Llama3-8B_LoRA. Our results show a cost-effective approach to
enhancing financial QA and offer insights for further research in multi-agent
financial LLM systems.

</details>


### [85] [A meta-analysis on the performance of machine-learning based language models for sentiment analysis](https://arxiv.org/abs/2509.09728)
*Elena Rohde,Jonas Klingwort,Christian Borgs*

Main category: cs.CL

TL;DR: 本文对基于Twitter数据的情感分析机器学习模型进行元分析，评估其总体性能并探讨影响因素。结果表明通常报告的总体准确率为0.80，但这一指标可能因类别不平衡而产生误导。作者建议采用标准化的模型性能报告方式。


<details>
  <summary>Details</summary>
Motivation: 当前众多基于Twitter情感分析的机器学习研究，各自报告的模型性能存在差异，难以统一比较。因此，本文试图系统总结这一领域已发表研究中的模型表现，并分析研究特征对性能的影响。

Method: 遵循PRISMA指南，系统检索数据库，筛选出20篇文献共195个实验，提取12个特征，主要以总体准确率为指标，采用双反正弦转换与三级随机效应模型进行统计分析。

Result: AIC最优模型的平均总体准确率为0.80（95%置信区间为0.76到0.84）。但准确率作为指标容易受类别不均衡和类别数量影响。

Conclusion: 建议未来研究报告标准化，包括对测试集混淆矩阵的披露，以便不同研究间公平比较。总体准确率虽常用，但容易产生误导，应引入归一化或更多绩效指标。

Abstract: This paper presents a meta-analysis evaluating ML performance in sentiment
analysis for Twitter data. The study aims to estimate the average performance,
assess heterogeneity between and within studies, and analyze how study
characteristics influence model performance. Using PRISMA guidelines, we
searched academic databases and selected 195 trials from 20 studies with 12
study features. Overall accuracy, the most reported performance metric, was
analyzed using double arcsine transformation and a three-level random effects
model. The average overall accuracy of the AIC-optimized model was 0.80 [0.76,
0.84]. This paper provides two key insights: 1) Overall accuracy is widely used
but often misleading due to its sensitivity to class imbalance and the number
of sentiment classes, highlighting the need for normalization. 2) Standardized
reporting of model performance, including reporting confusion matrices for
independent test sets, is essential for reliable comparisons of ML classifiers
across studies, which seems far from common practice.

</details>


### [86] [MultimodalHugs: Enabling Sign Language Processing in Hugging Face](https://arxiv.org/abs/2509.09729)
*Gerard Sant,Zifan Jiang,Carlos Escolano,Amit Moryossef,Mathias Müller,Rico Sennrich,Sarah Ebling*

Main category: cs.CL

TL;DR: 本文提出了MultimodalHugs，一个基于Hugging Face的新框架，旨在提升手语处理（SLP）领域的实验可复现性和灵活性，支持多样的数据模态与任务。


<details>
  <summary>Details</summary>
Motivation: 手语处理领域缺乏像Hugging Face这样既高效又易复现的实验工具，现有工具难以灵活支持手语等多模态的数据，导致研究难以公平对比和重复。

Method: 作者在Hugging Face基础上开发了MultimodalHugs框架。该框架通过引入抽象层，支持多模态数据（如手势姿态、字符像素等）和多样任务，并结合了Hugging Face生态的优点。

Result: 通过定量实验，作者展示了MultimodalHugs在处理手语中的姿态估计数据、字符像素数据等多模态数据方面的能力，并通过问卷调查表明，SLP研究者认可现有工具的不足。

Conclusion: MultimodalHugs不仅提升了手语处理的研究效率和可复现性，还为除手语外的多模态任务提供了通用支持，扩展了Hugging Face工具的适用范畴。

Abstract: In recent years, sign language processing (SLP) has gained importance in the
general field of Natural Language Processing. However, compared to research on
spoken languages, SLP research is hindered by complex ad-hoc code,
inadvertently leading to low reproducibility and unfair comparisons. Existing
tools that are built for fast and reproducible experimentation, such as Hugging
Face, are not flexible enough to seamlessly integrate sign language
experiments. This view is confirmed by a survey we conducted among SLP
researchers.
  To address these challenges, we introduce MultimodalHugs, a framework built
on top of Hugging Face that enables more diverse data modalities and tasks,
while inheriting the well-known advantages of the Hugging Face ecosystem. Even
though sign languages are our primary focus, MultimodalHugs adds a layer of
abstraction that makes it more widely applicable to other use cases that do not
fit one of the standard templates of Hugging Face. We provide quantitative
experiments to illustrate how MultimodalHugs can accommodate diverse modalities
such as pose estimation data for sign languages, or pixel data for text
characters.

</details>


### [87] [Benchmarking Vision-Language Models on Chinese Ancient Documents: From OCR to Knowledge Reasoning](https://arxiv.org/abs/2509.09731)
*Haiyang Yu,Yuchuan Wu,Fan Shi,Lei Liao,Jinghui Lu,Xiaodong Ge,Han Wang,Minghan Zhuo,Xuecheng Wu,Xiang Fei,Hao Feng,Guozhi Tang,An-Lan Wang,Hanshen Zhu,Yangfan He,Quanhuan Liang,Liyuan Meng,Chao Feng,Can Huang,Jingqun Tang,Bin Li*

Main category: cs.CL

TL;DR: 本文提出了AncientDoc，这是首个针对中国古籍的基准数据集，用于评测视觉-语言模型（VLMs）在古籍文献理解中的表现，并涵盖OCR、翻译、问答等多种任务。


<details>
  <summary>Details</summary>
Motivation: 当前古籍文献数字化与理解面临挑战，传统方法只做图像扫描，而现有中英文文档基准集难以覆盖古籍（尤其是繁体或异体字及历史文风等复杂内容）。因此需要一个专门的基准来推动视觉-语言模型在中国古籍场景下的进步。

Method: 作者构建了AncientDoc基准，包含五大任务（页面级OCR、白话翻译、推理型问答、知识型问答、文变体问答），涵盖14种文献类型、100多本书、约3000页内容。评测方法包括多指标自动评分和人工标注辅助的LLM打分。

Result: 基于AncientDoc，对主流视觉-语言模型的表现进行了系统评估，分析了模型在古籍文献结构化理解、翻译和知识推理等方向上的优劣揭示了目前模型的不足。

Conclusion: AncientDoc弥补了中文古籍文档基准的空白，为促进视觉-语言模型在文献数字化与知识挖掘领域提供了重要基础和评测平台，有助于推动相关模型及技术发展。

Abstract: Chinese ancient documents, invaluable carriers of millennia of Chinese
history and culture, hold rich knowledge across diverse fields but face
challenges in digitization and understanding, i.e., traditional methods only
scan images, while current Vision-Language Models (VLMs) struggle with their
visual and linguistic complexity. Existing document benchmarks focus on English
printed texts or simplified Chinese, leaving a gap for evaluating VLMs on
ancient Chinese documents. To address this, we present AncientDoc, the first
benchmark for Chinese ancient documents, designed to assess VLMs from OCR to
knowledge reasoning. AncientDoc includes five tasks (page-level OCR, vernacular
translation, reasoning-based QA, knowledge-based QA, linguistic variant QA) and
covers 14 document types, over 100 books, and about 3,000 pages. Based on
AncientDoc, we evaluate mainstream VLMs using multiple metrics, supplemented by
a human-aligned large language model for scoring.

</details>


### [88] [MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools](https://arxiv.org/abs/2509.09734)
*Zikang Guo,Benfeng Xu,Chiwei Zhu,Wentao Hong,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TL;DR: 提出了MCP-AgentBench，这是一个针对MCP协议下语言智能体与工具集成能力的系统化评测基准，用于更准确反映现实中的智能体表现。


<details>
  <summary>Details</summary>
Motivation: 传统评测未能准确反映基于MCP协议的智能体实际性能，导致对其能力认知偏差，也难以区分不同系统的优劣，需要更贴近实际应用的新评测方法。

Method: 构建了包含33个运行服务器和188个工具的MCP测试平台，设计了覆盖6大类别、600个任务查询的问题集，引入基于任务完成效果的新型MCP-Eval评测方法。

Result: 对主流语言智能体进行了大规模实证评测，获得了一系列基础性洞见，证明了基准的有效性和区分能力。

Conclusion: MCP-AgentBench为研究社区提供了标准化、可靠的评测框架，有助于推动基于MCP协议的智能体系统发展与落地。

Abstract: The Model Context Protocol (MCP) is rapidly emerging as a pivotal open
standard, designed to enhance agent-tool integration and interoperability, and
is positioned to unlock a new era of powerful, interconnected, and genuinely
utilitarian agentic AI. However, despite MCP's growing adoption, existing
benchmarks often fail to capture real-world agent performance within this new
paradigm, leading to a distorted perception of their true operational value and
an inability to reliably differentiate proficiencies. To bridge this critical
evaluation gap, we introduce MCP-AgentBench -- a comprehensive benchmark
specifically engineered to rigorously assess language agent capabilities in
MCP-mediated tool interactions. Core contributions of MCP-AgentBench include:
the establishment of a robust MCP testbed comprising 33 operational servers
with 188 distinct tools; the development of a benchmark featuring 600
systematically designed queries distributed across 6 distinct categories of
varying interaction complexity; and the introduction of MCP-Eval, a novel
outcome-oriented evaluation methodology prioritizing real-world task success.
Through extensive empirical evaluation of leading language agents, we provide
foundational insights. MCP-AgentBench aims to equip the research community with
a standardized and reliable framework to build, validate, and advance agents
capable of fully leveraging MCP's transformative benefits, thereby accelerating
progress toward truly capable and interoperable AI systems.

</details>


### [89] [Discrimination by LLMs: Cross-lingual Bias Assessment and Mitigation in Decision-Making and Summarisation](https://arxiv.org/abs/2509.09735)
*Willem Huijzer,Jieying Chen*

Main category: cs.CL

TL;DR: 本研究分析了大型语言模型（LLM）在决策和文本摘要任务中的背景、性别和年龄偏见，并探讨了偏见的跨语言传播及其缓解方法。发现GPT-3.5和GPT-4o决策时存在明显偏见，新提出的提示词缓解措施可减轻偏见但无法彻底消除。


<details>
  <summary>Details</summary>
Motivation: 随着LLM广泛应用，社会对其带来的不平等和信息偏见问题日益关注。明确LLM在实际任务中的具体偏见及其传播和缓解机制，有助于推动更为公平、负责任的AI研发和应用。

Method: 基于Tamkin等人（2023）数据集，翻译为荷兰语，对决策任务和摘要任务分别设计了151,200和176,400个独特提示，考察多种人口变量、指令、显著性水平和语言，主要在GPT-3.5和GPT-4o上进行实验分析。

Result: 结果表明，两款模型在决策任务中明显偏向女性、年轻群体及特定背景（如非裔美国人），而在摘要任务中偏见较小，仅在GPT-3.5英文摘要中有明显年龄差异。英荷两语的偏见模式整体相似，但部分人口类别存在差异。缓解性提示指令能减轻偏见，最优方案平均缩小27%的偏见差距。GPT-4o在英文任务上的偏见显著低于GPT-3.5。

Conclusion: LLM在具体任务中存在人口学偏见，跨语言传播普遍但有细节差异。目前提示词缓解策略取得一定成效，但无法彻底消除偏见。建议在推广LLM应用时谨慎并持续评估偏见，进一步研发更有效的缓解措施以实现更负责任的AI应用。

Abstract: The rapid integration of Large Language Models (LLMs) into various domains
raises concerns about societal inequalities and information bias. This study
examines biases in LLMs related to background, gender, and age, with a focus on
their impact on decision-making and summarization tasks. Additionally, the
research examines the cross-lingual propagation of these biases and evaluates
the effectiveness of prompt-instructed mitigation strategies. Using an adapted
version of the dataset by Tamkin et al. (2023) translated into Dutch, we
created 151,200 unique prompts for the decision task and 176,400 for the
summarisation task. Various demographic variables, instructions, salience
levels, and languages were tested on GPT-3.5 and GPT-4o. Our analysis revealed
that both models were significantly biased during decision-making, favouring
female gender, younger ages, and certain backgrounds such as the
African-American background. In contrast, the summarisation task showed minimal
evidence of bias, though significant age-related differences emerged for
GPT-3.5 in English. Cross-lingual analysis showed that bias patterns were
broadly similar between English and Dutch, though notable differences were
observed across specific demographic categories. The newly proposed mitigation
instructions, while unable to eliminate biases completely, demonstrated
potential in reducing them. The most effective instruction achieved a 27\% mean
reduction in the gap between the most and least favorable demographics.
Notably, contrary to GPT-3.5, GPT-4o displayed reduced biases for all prompts
in English, indicating the specific potential for prompt-based mitigation
within newer models. This research underscores the importance of cautious
adoption of LLMs and context-specific bias testing, highlighting the need for
continued development of effective mitigation strategies to ensure responsible
deployment of AI.

</details>


### [90] [HEFT: A Coarse-to-Fine Hierarchy for Enhancing the Efficiency and Accuracy of Language Model Reasoning](https://arxiv.org/abs/2509.09801)
*Brennen Hill*

Main category: cs.CL

TL;DR: 提出了一种新颖的层次化高效微调方法HEFT，结合权重和表示层的PEFT方法，在较少训练周期和计算资源下显著提升了大模型在推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）在专用推理任务上的微调受限于算力开销，主流的参数高效微调（PEFT）方法分为权重空间和表示空间两类，各有优劣，尚未有创新性结合两者优势的尝试。

Method: 提出HEFT分层微调策略，首先用LoRA在权重空间做粗粒度调整，然后用ReFT对内部激活进行精细调整，在Llama-2-7B模型和BoolQ推理数据集上进行了对比实验。

Result: HEFT仅微调3个epoch就取得85.17%的准确率，显著超越单独用LoRA（85.05%，20 epoch）或ReFT（83.36%，20 epoch）训练的模型。

Conclusion: 将不同PEFT方法有机结合是提升模型推理能力的有效算法创新，可以在降低算力消耗的同时，大幅提升大型模型复杂任务的适应效果。

Abstract: The adaptation of large language models (LLMs) to specialized reasoning tasks
is fundamentally constrained by computational resources. Parameter-Efficient
Fine-Tuning (PEFT) methods have emerged as a powerful solution, yet the
landscape of these techniques is diverse, with distinct methods operating in
either the model's weight space or its representation space. This paper
investigates the hypothesis that a synergistic combination of these paradigms
can unlock superior performance and efficiency. We introduce HEFT (Hierarchical
Efficient Fine-Tuning), a novel hierarchical adaptation strategy that composes
two distinct PEFT methods in a coarse-to-fine manner: first, a broad,
foundational adaptation in the weight space using Low-Rank Adaptation (LoRA),
followed by a precise, surgical refinement of internal activations using
Representation Fine-Tuning (ReFT). We evaluate this approach by fine-tuning a
Llama-2-7B model on the BoolQ benchmark, a challenging dataset for inferential
reasoning. Our results reveal a profound synergistic effect. A model fine-tuned
for only three epochs with our HEFT strategy achieves an accuracy of 85.17\%,
exceeding the performance of models trained for 20 epochs with either LoRA-only
(85.05\%) or ReFT-only (83.36\%) methodologies. This work demonstrates that the
thoughtful composition of PEFT methods is a potent algorithmic innovation,
offering a more efficient and effective path toward advancing the reasoning
capabilities of language models. By achieving superior results with a fraction
of the computational budget, our findings present a principled approach to
overcoming the obstacles inherent in adapting large-scale models for complex
cognitive tasks.

</details>


### [91] [Pragmatic Frames Evoked by Gestures: A FrameNet Brasil Approach to Multimodality in Turn Organization](https://arxiv.org/abs/2509.09804)
*Helen de Andrade Abreu,Tiago Timponi Torrent,Ely Edison da Silva Matos*

Main category: cs.CL

TL;DR: 本文提出了一套框架，用于通过分析语言与肢体互动手势的相关性来建模多模态对话的话轮组织，并通过丰富语义框架注释的数据集，引入用于对话组织的语用框架。研究证明手势在面对面对话中在转接、维持等组织话轮中的重要作用，并首次记录一些未被系统文献记载的手势变体。


<details>
  <summary>Details</summary>
Motivation: 当前关于对话话轮组织的研究主要限于部分领域，尤其缺乏能够通过机器学习分析对话转接中手势作用的多模态标注数据集，因此作者希望通过对真实场景数据的手势与语言行为进行注释和分析，推动多模态对话理解。

Method: 作者开发了注释方法，将Frame2多模态数据集扩展为含有基于语用框架的手势注释的数据集，然后分析电视纪录片中的真实对话数据，系统归纳了手势在组织对话过程中的表现及其与语言的关系。

Result: 研究结果确认了面对面对话中手势在传递、获得与保持话轮中的关键作用，并观察到一些先前未被文献记载的手势变体。通过语用框架的注释，揭示了认知层面手势运用的多样性。

Conclusion: 作者认为，对话中组织话轮的肢体手势源自语用框架和认知过程，数据集和分析方法有助于深入理解人类认知与语言互动，也为多模态对话建模及相关机器学习研究提供了重要基础。

Abstract: This paper proposes a framework for modeling multimodal conversational turn
organization via the proposition of correlations between language and
interactive gestures, based on analysis as to how pragmatic frames are
conceptualized and evoked by communicators. As a means to provide evidence for
the analysis, we developed an annotation methodology to enrich a multimodal
dataset (annotated for semantic frames) with pragmatic frames modeling
conversational turn organization. Although conversational turn organization has
been studied by researchers from diverse fields, the specific strategies,
especially gestures used by communicators, had not yet been encoded in a
dataset that can be used for machine learning. To fill this gap, we enriched
the Frame2 dataset with annotations of gestures used for turn organization. The
Frame2 dataset features 10 episodes from the Brazilian TV series Pedro Pelo
Mundo annotated for semantic frames evoked in both video and text. This dataset
allowed us to closely observe how communicators use interactive gestures
outside a laboratory, in settings, to our knowledge, not previously recorded in
related literature. Our results have confirmed that communicators involved in
face-to-face conversation make use of gestures as a tool for passing, taking
and keeping conversational turns, and also revealed variations of some gestures
that had not been documented before. We propose that the use of these gestures
arises from the conceptualization of pragmatic frames, involving mental spaces,
blending and conceptual metaphors. In addition, our data demonstrate that the
annotation of pragmatic frames contributes to a deeper understanding of human
cognition and language.

</details>


### [92] [Topic-Guided Reinforcement Learning with LLMs for Enhancing Multi-Document Summarization](https://arxiv.org/abs/2509.09852)
*Chuyuan Li,Austin Xu,Shafiq Joty,Giuseppe Carenini*

Main category: cs.CL

TL;DR: 本文提出了一种基于主题引导的强化学习方法，有效提升了多文档摘要的内容选择和主题对齐能力。


<details>
  <summary>Details</summary>
Motivation: 当前多文档摘要任务在整合多源信息时，常常面临内容连贯性和主题相关性不足的问题。尽管大语言模型在单文档摘要上表现优异，但在多文档摘要任务中的效果仍未达到理想水平。

Method: 首先通过在输入中显式加入主题标签，提升生成摘要的信息量；在此基础上，引入新颖的主题奖励，在Group Relative Policy Optimization（GRPO）框架下，衡量生成摘要与原文档的主题对齐程度，从而指导模型优化。

Result: 在Multi-News和Multi-XScience两个数据集上的实验结果显示，所提方法在多个指标上均优于强基线，验证了主题引导策略的有效性。

Conclusion: 引入主题引导奖励和强化学习框架能够显著提升多文档摘要的内容选取与主题相关性，对于提升生成摘要的整体质量具有重要意义。

Abstract: A key challenge in Multi-Document Summarization (MDS) is effectively
integrating information from multiple sources while maintaining coherence and
topical relevance. While Large Language Models have shown impressive results in
single-document summarization, their performance on MDS still leaves room for
improvement. In this paper, we propose a topic-guided reinforcement learning
approach to improve content selection in MDS. We first show that explicitly
prompting models with topic labels enhances the informativeness of the
generated summaries. Building on this insight, we propose a novel topic reward
within the Group Relative Policy Optimization (GRPO) framework to measure topic
alignment between the generated summary and source documents. Experimental
results on the Multi-News and Multi-XScience datasets demonstrate that our
method consistently outperforms strong baselines, highlighting the
effectiveness of leveraging topical cues in MDS.

</details>


### [93] [Emulating Public Opinion: A Proof-of-Concept of AI-Generated Synthetic Survey Responses for the Chilean Case](https://arxiv.org/abs/2509.09871)
*Bastián González-Bustamante,Nando Verelst,Carla Cisternas*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型（LLMs）作为合成调查受访者在调查研究中的应用潜力，通过与实际公共意见调查数据对比，检验其在还原整体答案分布和减少测量误差方面的能力。结果表明，LLMs在部分题项表现出高准确性，但在解释公众意见细致区别上仍有难度。


<details>
  <summary>Details</summary>
Motivation: 现有调查方法常受测量误差、样本失衡等问题影响，研究者希望通过利用LLMs生成合成受访者答案，降低成本并改善数据代表性。然而，目前尚不清楚这些模型能否准确还原群体答案分布，并存在社会偏见再现等风险。

Method: 作者基于一份智利公众意见概率性调查数据，构建了128组提问指令和模型组合，通过GPT、Llama、Qwen等多种LLM，生成近19万份合成受访者资料。用准确率、查准率、查全率、F1分数等指标，对模型与真实人类回答的一致性及关键社会人口属性偏差进行综合评估，并进行荟萃分析。

Result: 1）LLMs在信任类题目上表现优异（F1和准确率均超过0.90）。2）GPT-4o、GPT-4o-mini与Llama 4 Maverick在该任务上表现接近。3）与45-59岁群体的人类答案一致性最高。整体来看，LLM合成样本大体还原了概率样本调查结果，但在具体题项和人群上差异仍然明显。

Conclusion: LLMs生成的合成调查样本能较好模拟人类受访者群体答题分布，对调查研究有辅助价值。但当前模型在公众意见细腻度把握、算法偏差控制上存在挑战，需要进一步校准和更细致的分布检验，才能确保方法可靠减少错误。

Abstract: Large Language Models (LLMs) offer promising avenues for methodological and
applied innovations in survey research by using synthetic respondents to
emulate human answers and behaviour, potentially mitigating measurement and
representation errors. However, the extent to which LLMs recover aggregate item
distributions remains uncertain and downstream applications risk reproducing
social stereotypes and biases inherited from training data. We evaluate the
reliability of LLM-generated synthetic survey responses against ground-truth
human responses from a Chilean public opinion probabilistic survey.
Specifically, we benchmark 128 prompt-model-question triplets, generating
189,696 synthetic profiles, and pool performance metrics (i.e., accuracy,
precision, recall, and F1-score) in a meta-analysis across 128
question-subsample pairs to test for biases along key sociodemographic
dimensions. The evaluation spans OpenAI's GPT family and o-series reasoning
models, as well as Llama and Qwen checkpoints. Three results stand out. First,
synthetic responses achieve excellent performance on trust items (F1-score and
accuracy > 0.90). Second, GPT-4o, GPT-4o-mini and Llama 4 Maverick perform
comparably on this task. Third, synthetic-human alignment is highest among
respondents aged 45-59. Overall, LLM-based synthetic samples approximate
responses from a probabilistic sample, though with substantial item-level
heterogeneity. Capturing the full nuance of public opinion remains challenging
and requires careful calibration and additional distributional tests to ensure
algorithmic fidelity and reduce errors.

</details>


### [94] [Large Language Models Meet Legal Artificial Intelligence: A Survey](https://arxiv.org/abs/2509.09969)
*Zhitian Hou,Zihan Ye,Nanli Zeng,Tianyong Hao,Kun Zeng*

Main category: cs.CL

TL;DR: 本文综述了法律领域大模型（LLMs）的发展与应用，并对主流法律大模型、框架、评测基准和数据集进行了系统梳理，同时分析了挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 近年来，法律大模型极大提升了法律人工智能（Legal AI）的效率与准确性。为推动法律领域LLM相关研究与应用，有必要对现有工作进行系统化整理和评述。

Method: 作者综述了16个法律大模型系列、47个基于LLM的法律任务框架，收集了15个评测基准和29个相关数据集。此外，对该领域面临的挑战进行分析，探讨未来研究方向。

Result: 系统总结了法律领域LLM相关资源，归纳其现状、优势与不足，并梳理了法律任务中的主流框架和数据支持，指出了当前主要挑战。

Conclusion: 本文为法律大模型领域初学者及研究者提供了系统入门资料，并期待激励更多后续研究与探索。所有资源已公开共享。

Abstract: Large Language Models (LLMs) have significantly advanced the development of
Legal Artificial Intelligence (Legal AI) in recent years, enhancing the
efficiency and accuracy of legal tasks. To advance research and applications of
LLM-based approaches in legal domain, this paper provides a comprehensive
review of 16 legal LLMs series and 47 LLM-based frameworks for legal tasks, and
also gather 15 benchmarks and 29 datasets to evaluate different legal
capabilities. Additionally, we analyse the challenges and discuss future
directions for LLM-based approaches in the legal domain. We hope this paper
provides a systematic introduction for beginners and encourages future research
in this field. Resources are available at
https://github.com/ZhitianHou/LLMs4LegalAI.

</details>


### [95] [CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China](https://arxiv.org/abs/2509.09990)
*Guixian Xu,Zeli Su,Ziyin Zhang,Jianing Liu,XU Han,Ting Zhang,Yushuang Dong*

Main category: cs.CL

TL;DR: 本论文提出了中文少数民族语言标题生成数据集CMHG，包含藏语、维吾尔语和蒙古语，共20万条目，并提供了高质量的母语者标注测试集。


<details>
  <summary>Details</summary>
Motivation: 由于藏语、维吾尔语、蒙古语等中国少数民族语言存在独特书写体系，与国际标准差异较大，导致相关语料库稀缺，尤其是在需要有标签数据的任务（如标题生成）中更为突出。因此，急需填补这类数据资源空白。

Method: 构建了中文少数民族标题生成数据集（CMHG），具体包括：藏语数据10万条，维吾尔语与蒙古语各5万条。同时，组织母语者标注，制作了高质量的测试集，旨在作为后续研究权威基准。

Result: 成功创建了三种少数民族语言的标题生成数据集及高质量测试集，为此领域提供了重要基础。

Conclusion: 该数据集将为中文少数民族语言的标题生成任务和相关基准建设提供宝贵资源，有助于推动该领域的研究与应用。

Abstract: Minority languages in China, such as Tibetan, Uyghur, and Traditional
Mongolian, face significant challenges due to their unique writing systems,
which differ from international standards. This discrepancy has led to a severe
lack of relevant corpora, particularly for supervised tasks like headline
generation. To address this gap, we introduce a novel dataset, Chinese Minority
Headline Generation (CMHG), which includes 100,000 entries for Tibetan, and
50,000 entries each for Uyghur and Mongolian, specifically curated for headline
generation tasks. Additionally, we propose a high-quality test set annotated by
native speakers, designed to serve as a benchmark for future research in this
domain. We hope this dataset will become a valuable resource for advancing
headline generation in Chinese minority languages and contribute to the
development of related benchmarks.

</details>


### [96] [Unsupervised Hallucination Detection by Inspecting Reasoning Processes](https://arxiv.org/abs/2509.10004)
*Ponhvoan Srey,Xiaobao Wu,Anh Tuan Luu*

Main category: cs.CL

TL;DR: 该论文提出了一种全新的无需监督的幻觉内容检测框架IRIS，通过利用大模型内部表征和答案不确定性，提高了检测精度和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督幻觉内容检测方法通常依赖与事实正确性无关的代理信号，导致检测结果容易受到非事实相关因素影响，影响了在不同数据集和场景下的泛化能力。为此，作者希望设计一种更直接关联事实正确性的检测机制。

Method: IRIS框架主要有两大创新：第一，利用大模型验证陈述真实性时生成的上下文嵌入作为特征；第二，将模型输出的置信度作为真伪的软伪标签。在无需人工标注的情况下，利用这些高关联特征训练检测器。

Result: 实验显示，IRIS在多个指标上均超越了现有的无监督幻觉检测方法，并且对训练数据量的要求极低，计算开销小。

Conclusion: IRIS提供了一种有效、低成本、无需人工标注的幻觉内容检测方案，具备优良的实时性和在不同场景下的泛化能力，是无监督幻觉检测领域的有力进展。

Abstract: Unsupervised hallucination detection aims to identify hallucinated content
generated by large language models (LLMs) without relying on labeled data.
While unsupervised methods have gained popularity by eliminating
labor-intensive human annotations, they frequently rely on proxy signals
unrelated to factual correctness. This misalignment biases detection probes
toward superficial or non-truth-related aspects, limiting generalizability
across datasets and scenarios. To overcome these limitations, we propose IRIS,
an unsupervised hallucination detection framework, leveraging internal
representations intrinsic to factual correctness. IRIS prompts the LLM to
carefully verify the truthfulness of a given statement, and obtain its
contextualized embedding as informative features for training. Meanwhile, the
uncertainty of each response is considered a soft pseudolabel for truthfulness.
Experimental results demonstrate that IRIS consistently outperforms existing
unsupervised methods. Our approach is fully unsupervised, computationally low
cost, and works well even with few training data, making it suitable for
real-time detection.

</details>


### [97] [Multi-Intent Recognition in Dialogue Understanding: A Comparison Between Smaller Open-Source LLMs](https://arxiv.org/abs/2509.10010)
*Adnan Ahmad,Philine Kowol,Stefan Hillmann,Sebastian Möller*

Main category: cs.CL

TL;DR: 本文系统分析了开源大语言模型在多标签意图分类任务上的表现，发现Mistral-7B-v0.1在少样本设置下表现最佳，但BERT模型仍优于所有生成式LLM。


<details>
  <summary>Details</summary>
Motivation: 动机是评估可在消费级硬件运行的开源大语言模型在多标签对话系统意图分类中的实际效能，填补当前主流模型评测多聚焦于闭源或单标签场景的空白。

Method: 使用MultiWOZ 2.1对话数据集，选取LLama2-7B-hf、Mistral-7B-v0.1和Yi-6B三种开源LLM，在每个模型中采用20-shot提示，并将基于instruction的微调方法与BertForSequenceClassification监督学习基线作对比，评估指标包括准确率、精确率、召回率、F1分数、推理速度、显存占用等。

Result: Mistral-7B-v0.1在14类意图中的11类F分数表现最佳（加权F1=0.50），Humming Loss最低、Jaccard相似度最高，但BERT模型整体性能优于最好的few-shot生成式LLM。

Conclusion: 虽然部分开源LLM（如Mistral-7B-v0.1）在少样本多标签分类任务显示出较好潜力，但在多意图对话检测方面，监督学习的BERT模型仍更优。该研究为小规模开源LLM在任务型对话系统自然语言理解中的应用提供了实用参考。

Abstract: In this paper, we provide an extensive analysis of multi-label intent
classification using Large Language Models (LLMs) that are open-source,
publicly available, and can be run in consumer hardware. We use the MultiWOZ
2.1 dataset, a benchmark in the dialogue system domain, to investigate the
efficacy of three popular open-source pre-trained LLMs, namely LLama2-7B-hf,
Mistral-7B-v0.1, and Yi-6B. We perform the classification task in a few-shot
setup, giving 20 examples in the prompt with some instructions. Our approach
focuses on the differences in performance of these models across several
performance metrics by methodically assessing these models on multi-label
intent classification tasks. Additionally, we compare the performance of the
instruction-based fine-tuning approach with supervised learning using the
smaller transformer model BertForSequenceClassification as a baseline. To
evaluate the performance of the models, we use evaluation metrics like
accuracy, precision, and recall as well as micro, macro, and weighted F1 score.
We also report the inference time, VRAM requirements, etc. The Mistral-7B-v0.1
outperforms two other generative models on 11 intent classes out of 14 in terms
of F-Score, with a weighted average of 0.50. It also has relatively lower
Humming Loss and higher Jaccard Similarity, making it the winning model in the
few-shot setting. We find BERT based supervised classifier having superior
performance compared to the best performing few-shot generative LLM. The study
provides a framework for small open-source LLMs in detecting complex
multi-intent dialogues, enhancing the Natural Language Understanding aspect of
task-oriented chatbots.

</details>


### [98] [Linguistic trajectories of bipolar disorder on social media](https://arxiv.org/abs/2509.10035)
*Laurin Plank,Armin Zlomuzica*

Main category: cs.CL

TL;DR: 该论文通过分析社交媒体上的语言，比较了躁郁症（BD）、单相抑郁症（UD）和健康用户（HC）在诊断前后长达24年的语言轨迹，发现BD患者在急性期和慢性期均有显著的语言变化，并揭示了这些变化具有季节性周期性特征。


<details>
  <summary>Details</summary>
Motivation: 传统的情感障碍评估依赖临床访谈，规模有限，难以实现大样本、长时程的监测。社交媒体语言为高频且纵向的数据提供了分析情感障碍的新手段，但如何定位诊断时间点并系统观察语言变化仍未充分探索。

Method: 作者提出了一种方法确定用户的诊断时间，并收集并分析了诊断前3年至诊断后21年间社交媒体的语言数据。对比分析了躁郁症、单相抑郁症患者以及无相关障碍用户的语言轨迹。利用自然语言处理等技术，量化多种精神健康相关的语言特征。

Result: 结果显示，躁郁症患者在诊断期间及之后的多年中，语言表现出明显的情绪波动、精神共病、药物滥用、住院、身体共病、非常规思想内容及思维混乱等特征。重要发现之一是BD患者在诊断后长期（约20年）的语言存在反复的情绪相关变化，呈现12个月的周期性。此外，女性用户中周期性现象更为突出。

Conclusion: 本研究证明了躁郁症在急性与慢性阶段都有语言变化，且这些特征可通过社交媒体长期追踪和量化，推动了基于社交媒体的精神健康大规模监测的研究进展。

Abstract: Language provides valuable markers of affective disorders such as bipolar
disorder (BD), yet clinical assessments remain limited in scale. In response,
analyses of social media (SM) language have gained prominence due to their high
temporal resolution and longitudinal scope. Here, we introduce a method to
determine the timing of users' diagnoses and apply it to study language
trajectories from 3 years before to 21 years after BD diagnosis - contrasted
with uses reporting unipolar depression (UD) and non-affected users (HC). We
show that BD diagnosis is accompanied by pervasive linguistic alterations
reflecting mood disturbance, psychiatric comorbidity, substance abuse,
hospitalization, medical comorbidities, unusual thought content, and
disorganized thought. We further observe recurring mood-related language
changes across two decades after the diagnosis, with a pronounced 12-month
periodicity suggestive of seasonal mood episodes. Finally, trend-level evidence
suggests an increased periodicity in users estimated to be female. In sum, our
findings provide evidence for language alterations in the acute and chronic
phase of BD. This validates and extends recent efforts leveraging SM for
scalable monitoring of mental health.

</details>


### [99] [!MSA at BAREC Shared Task 2025: Ensembling Arabic Transformers for Readability Assessment](https://arxiv.org/abs/2509.10040)
*Mohamed Basem,Mohamed Younes,Seif Ahmed,Abdelrahman Moustafa*

Main category: cs.CL

TL;DR: 本文提出了一种用于阿拉伯语可读性细粒度评估的系统，并在BAREC 2025竞赛的六个赛道中全部获得第一名，展现了极佳的性能。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语可读性评估任务面临类别不均衡和数据稀缺等挑战，需要更强鲁棒性和准确性的系统来提升评测水平。

Method: 采用了四个互补的Transformer模型（AraBERTv2、AraELECTRA、MARBERT、CAMeLBERT），各自利用不同的损失函数精调，并通过置信度加权融合；为解决类别不均衡和样本稀缺，应用了加权训练、高级预处理、用最强模型对SAMER语料重新标注，以及利用Gemini 2.5 Flash生成约1万条稀有类别的合成数据，同时采用后处理纠正预测分布偏差。

Result: 该系统在句子级和文档级的QWK（加权Kappa系数）均超过87%，具体为87.5%和87.4%，后处理增加了6.3%的QWK提升。

Conclusion: 多模型多损失的融合、置信度加权和智能数据增强显著提升了阿拉伯语可读性预测的准确性和鲁棒性，取得了领先效果。

Abstract: We present MSAs winning system for the BAREC 2025 Shared Task on fine-grained
Arabic readability assessment, achieving first place in six of six tracks. Our
approach is a confidence-weighted ensemble of four complementary transformer
models (AraBERTv2, AraELECTRA, MARBERT, and CAMeLBERT) each fine-tuned with
distinct loss functions to capture diverse readability signals. To tackle
severe class imbalance and data scarcity, we applied weighted training,
advanced preprocessing, SAMER corpus relabeling with our strongest model, and
synthetic data generation via Gemini 2.5 Flash, adding about 10,000 rare-level
samples. A targeted post-processing step corrected prediction distribution
skew, delivering a 6.3 percent Quadratic Weighted Kappa (QWK) gain. Our system
reached 87.5 percent QWK at the sentence level and 87.4 percent at the document
level, demonstrating the power of model and loss diversity, confidence-informed
fusion, and intelligent augmentation for robust Arabic readability prediction.

</details>


### [100] [Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models](https://arxiv.org/abs/2509.10078)
*Dongmin Choi,Woojung Song,Jongwook Han,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 该论文对比了传统心理学问卷与情境有效性问卷在衡量大语言模型（LLM）性格和价值观方面的差异，发现传统问卷在LLM应用中存在明显问题。


<details>
  <summary>Details</summary>
Motivation: 现有的心理学问卷如BFI和PVQ常被用来评估LLM的性格特质和价值观，但这些问卷本用于人类，其生态效度（即真实反映实际应用场景的能力）受到质疑，因此有必要系统比较不同问卷在LLM上的效果。

Method: 作者同时采用传统的心理学问卷与更加契合用户实际查询场景的情境有效性问卷，对LLM的性格和价值观表现进行全面比较分析。

Result: 分析显示，传统问卷测出的LLM特性与情境有效性问卷有很大不同，不能真实反映LLM在用户场景下的表现，并揭示传统问卷存在题目数量少、测量不稳定、刻画虚假稳定性等问题，同时对人格化提示的LLM给出夸大画像。

Conclusion: 论文建议不要用传统心理学问卷评估LLM的心理特性，提醒相关研究需谨慎选择工具。代码将在发表后公开。

Abstract: Researchers have applied established psychometric questionnaires (e.g., BFI,
PVQ) to measure the personality traits and values reflected in the responses of
Large Language Models (LLMs). However, concerns have been raised about applying
these human-designed questionnaires to LLMs. One such concern is their lack of
ecological validity--the extent to which survey questions adequately reflect
and resemble real-world contexts in which LLMs generate texts in response to
user queries. However, it remains unclear how established questionnaires and
ecologically valid questionnaires differ in their outcomes, and what insights
these differences may provide. In this paper, we conduct a comprehensive
comparative analysis of the two types of questionnaires. Our analysis reveals
that established questionnaires (1) yield substantially different profiles of
LLMs from ecologically valid ones, deviating from the psychological
characteristics expressed in the context of user queries, (2) suffer from
insufficient items for stable measurement, (3) create misleading impressions
that LLMs possess stable constructs, and (4) yield exaggerated profiles for
persona-prompted LLMs. Overall, our work cautions against the use of
established psychological questionnaires for LLMs. Our code will be released
upon publication.

</details>


### [101] [Querying Climate Knowledge: Semantic Retrieval for Scientific Discovery](https://arxiv.org/abs/2509.10087)
*Mustapha Adamu,Qi Zhang,Huitong Pan,Longin Jan Latecki,Eduard C. Dragut*

Main category: cs.CL

TL;DR: 本文提出了一个专门针对气候科学文献的知识图谱（KG），支持结构化、语义化检索，提升气候领域信息访问和利用效率，并展示了其在实际科研场景中的价值。


<details>
  <summary>Details</summary>
Motivation: 气候科学文献庞大且复杂，研究人员难以在模型、数据集、地区和变量等维度高效检索和获取相关知识。

Method: 构建了基于气候领域文献的知识图谱，支持语义化结构化查询，并与大语言模型结合，用于RAG（检索增强生成）系统中；通过 Cypher 查询语言演示了实际应用。

Result: 知识图谱能够准确回答如模型验证区域、常用数据集与遥相关类型配对等复杂科研问题，提升资料检索与知识发现效率。

Conclusion: 该知识图谱不仅仅是数据整理工具，更为气候研究、模型开发等研究者提供了高效、可靠、可溯源的知识服务，推动了实际科研工作的深入。

Abstract: The growing complexity and volume of climate science literature make it
increasingly difficult for researchers to find relevant information across
models, datasets, regions, and variables. This paper introduces a
domain-specific Knowledge Graph (KG) built from climate publications and
broader scientific texts, aimed at improving how climate knowledge is accessed
and used. Unlike keyword based search, our KG supports structured, semantic
queries that help researchers discover precise connections such as which models
have been validated in specific regions or which datasets are commonly used
with certain teleconnection patterns. We demonstrate how the KG answers such
questions using Cypher queries, and outline its integration with large language
models in RAG systems to improve transparency and reliability in
climate-related question answering. This work moves beyond KG construction to
show its real world value for climate researchers, model developers, and others
who rely on accurate, contextual scientific information.

</details>


### [102] [Arabic Large Language Models for Medical Text Generation](https://arxiv.org/abs/2509.10095)
*Abdulrahman Allam,Seif Ahmed,Ali Hamdi,Ammar Mohammed*

Main category: cs.CL

TL;DR: 本文提出了一种针对阿拉伯语医疗文本生成的医院管理系统，通过微调大语言模型提升了对患者的实时医疗建议能力，尤其适用于低资源语言环境。


<details>
  <summary>Details</summary>
Motivation: 现有医院管理系统在应对实时、准确医疗建议时，在输入不规范及小语种支持方面存在明显不足，制约了全球医疗服务能力。

Method: 研究收集并清洗自社交媒体的大型阿拉伯语医患对话数据，处理多种阿拉伯方言，并基于Mistral-7B、LLaMA-2-7B、GPT-2 Medium等语言模型进行微调，比对不同模型生成医疗文本的效果。

Result: 微调后的Mistral-7B模型在BERT评分（精度68.5%、召回率69.08%、F1值68.5%）上表现最佳，生成的医疗建议更连贯、相关性更高，显著优于对比模型。

Conclusion: 微调生成式大模型能够大幅提升医疗建议生成质量，为多语言、多文化环境下的医院管理系统提供了一种可扩展、适应性强的AI解决方案，有望改善全球医疗服务。

Abstract: Efficient hospital management systems (HMS) are critical worldwide to address
challenges such as overcrowding, limited resources, and poor availability of
urgent health care. Existing methods often lack the ability to provide
accurate, real-time medical advice, particularly for irregular inputs and
underrepresented languages. To overcome these limitations, this study proposes
an approach that fine-tunes large language models (LLMs) for Arabic medical
text generation. The system is designed to assist patients by providing
accurate medical advice, diagnoses, drug recommendations, and treatment plans
based on user input. The research methodology required the collection of a
unique dataset from social media platforms, capturing real-world medical
conversations between patients and doctors. The dataset, which includes patient
complaints together with medical advice, was properly cleaned and preprocessed
to account for multiple Arabic dialects. Fine-tuning state-of-the-art
generative models, such as Mistral-7B-Instruct-v0.2, LLaMA-2-7B, and GPT-2
Medium, optimized the system's ability to generate reliable medical text.
Results from evaluations indicate that the fine-tuned Mistral-7B model
outperformed the other models, achieving average BERT (Bidirectional Encoder
Representations from Transformers) Score values in precision, recall, and
F1-scores of 68.5\%, 69.08\%, and 68.5\%, respectively. Comparative
benchmarking and qualitative assessments validate the system's ability to
produce coherent and relevant medical replies to informal input. This study
highlights the potential of generative artificial intelligence (AI) in
advancing HMS, offering a scalable and adaptable solution for global healthcare
challenges, especially in linguistically and culturally diverse environments.

</details>


### [103] [Scaling Arabic Medical Chatbots Using Synthetic Data: Enhancing Generative AI with Synthetic Patient Records](https://arxiv.org/abs/2509.10108)
*Abdulrahman Allam,Seif Ahmed,Ali Hamdi,Khaled Shaban*

Main category: cs.CL

TL;DR: 本文提出了一种可扩展的阿拉伯语医学聊天机器人数据增强方法，通过生成高质量的合成问答数据，有效提升了大模型在小语种医疗领域的表现。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语领域缺乏大规模、高质量注释的数据集，限制了医学聊天机器人的开发和大语言模型的泛化能力。作者希望通过数据增强解决这一瓶颈问题。

Method: 使用ChatGPT-4o和Gemini 2.5 Pro大模型，根据原始2万条真实患者-医生对话，生成8万条结构和语义一致的医学问答；合成数据经过语义过滤和人工验证后与原数据结合，用于微调五种大语言模型，并通过自动指标（BERTScore）和专家人工评估进行性能验证。

Result: 扩充后的数据训练出的模型（特别是用ChatGPT-4o生成的数据）其F1分数更高，幻觉更少，在定量和定性评估中均优于其他方案，显示合成数据增强的有效性。

Conclusion: 合成数据增强是一种切实可行的方式，有助于提升低资源语种（如阿拉伯语）医疗NLP系统效果，为研发高质量、可扩展的阿拉伯语智能医疗助手奠定基础。

Abstract: The development of medical chatbots in Arabic is significantly constrained by
the scarcity of large-scale, high-quality annotated datasets. While prior
efforts compiled a dataset of 20,000 Arabic patient-doctor interactions from
social media to fine-tune large language models (LLMs), model scalability and
generalization remained limited. In this study, we propose a scalable synthetic
data augmentation strategy to expand the training corpus to 100,000 records.
Using advanced generative AI systems ChatGPT-4o and Gemini 2.5 Pro we generated
80,000 contextually relevant and medically coherent synthetic question-answer
pairs grounded in the structure of the original dataset. These synthetic
samples were semantically filtered, manually validated, and integrated into the
training pipeline. We fine-tuned five LLMs, including Mistral-7B and AraGPT2,
and evaluated their performance using BERTScore metrics and expert-driven
qualitative assessments. To further analyze the effectiveness of synthetic
sources, we conducted an ablation study comparing ChatGPT-4o and
Gemini-generated data independently. The results showed that ChatGPT-4o data
consistently led to higher F1-scores and fewer hallucinations across all
models. Overall, our findings demonstrate the viability of synthetic
augmentation as a practical solution for enhancing domain-specific language
models in-low resource medical NLP, paving the way for more inclusive,
scalable, and accurate Arabic healthcare chatbot systems.

</details>


### [104] [Prominence-aware automatic speech recognition for conversational speech](https://arxiv.org/abs/2509.10116)
*Julian Linke,Barbara Schuppler*

Main category: cs.CL

TL;DR: 本文提出了结合重音检测与语音识别的奥地利德语对话体自适应ASR系统，证明Transformer模型能有效编码韵律信息，实现了对单词及其重音层次的共同转录。


<details>
  <summary>Details</summary>
Motivation: 在自然对话中，韵律重音信息对于理解说话者意图和语句结构尤为关键，但传统的ASR系统并未充分利用。这项工作旨在提升ASR系统对韵律特征的感知能力，丰富其应用场景，如语言学研究和更自然的对话系统。

Method: 作者首先微调了wav2vec2模型来实现单词级重音分类，并利用该重音检测器为大规模语料库自动标注韵律重音。随后，基于这些标注，训练了可以同时转录词和重音层次的新型ASR系统。模型采用Transformer结构，专注于韵律信息编码。

Result: 结合重音信息后，ASR识别精度与基线系统持平，但在词序识别正确的语句上，实现了85.53%的重音检测准确率。

Conclusion: Transformer模型能够有效整合韵律信息，提升ASR系统的功能，实现了韵律增强型ASR的新突破，未来可用于语言学研究和重音感知的对话系统。

Abstract: This paper investigates prominence-aware automatic speech recognition (ASR)
by combining prominence detection and speech recognition for conversational
Austrian German. First, prominence detectors were developed by fine-tuning
wav2vec2 models to classify word-level prominence. The detector was then used
to automatically annotate prosodic prominence in a large corpus. Based on those
annotations, we trained novel prominence-aware ASR systems that simultaneously
transcribe words and their prominence levels. The integration of prominence
information did not change performance compared to our baseline ASR system,
while reaching a prominence detection accuracy of 85.53% for utterances where
the recognized word sequence was correct. This paper shows that
transformer-based models can effectively encode prosodic information and
represents a novel contribution to prosody-enhanced ASR, with potential
applications for linguistic research and prosody-informed dialogue systems.

</details>


### [105] [Population-Aligned Persona Generation for LLM-based Social Simulation](https://arxiv.org/abs/2509.10127)
*Zhengyu Hu,Zheyuan Xiao,Max Xiong,Yuxuan Lei,Tianfu Wang,Jianxun Lian,Kaize Ding,Ziang Xiao,Nicholas Jing Yuan,Xing Xie*

Main category: cs.CL

TL;DR: 本文提出了一种系统性方法，用于生成与真实人口分布对齐的高质量persona集合，以提升大语言模型驱动的社会模拟的真实性和代表性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型社会模拟研究往往忽视个体角色（persona）生成的复杂性及其潜在偏见，缺乏能真实反映实际人口多样性的persona集合。这限制了社会模拟的科学性与适用性。

Method: 方法包括三步：1）用LLM从长时段社交媒体数据中生成叙述型persona；2）通过严格的质量评估筛除低拟真角色；3）应用重要性采样使整体persona集在心理测量分布（如大五人格）上与人口基准对齐。此外，提出可针对具体研究场景的子人口，利用任务自适应模块进一步调整persona集合。

Result: 实验证明，该方法能够显著降低群体层面的偏差，并支持更加精确和灵活的大规模社会模拟，适用于多样的科研和政策应用场景。

Conclusion: 论文表明，基于所提系统性框架生成的persona集合，可以有效提升LLM驱动社会模拟的代表性和现实相关性，解决了社会计算中重要但此前被忽视的persona偏见问题。

Abstract: Recent advances in large language models (LLMs) have enabled human-like
social simulations at unprecedented scale and fidelity, offering new
opportunities for computational social science. A key challenge, however, is
the construction of persona sets that authentically represent the diversity and
distribution of real-world populations. Most existing LLM-based social
simulation studies focus primarily on designing agentic frameworks and
simulation environments, often overlooking the complexities of persona
generation and the potential biases introduced by unrepresentative persona
sets. In this paper, we propose a systematic framework for synthesizing
high-quality, population-aligned persona sets for LLM-driven social simulation.
Our approach begins by leveraging LLMs to generate narrative personas from
long-term social media data, followed by rigorous quality assessment to filter
out low-fidelity profiles. We then apply importance sampling to achieve global
alignment with reference psychometric distributions, such as the Big Five
personality traits. To address the needs of specific simulation contexts, we
further introduce a task-specific module that adapts the globally aligned
persona set to targeted subpopulations. Extensive experiments demonstrate that
our method significantly reduces population-level bias and enables accurate,
flexible social simulation for a wide range of research and policy
applications.

</details>


### [106] [Towards Reliable and Interpretable Document Question Answering via VLMs](https://arxiv.org/abs/2509.10129)
*Alessio Chen,Simone Giovannini,Andrea Gemelli,Fabio Coppini,Simone Marinai*

Main category: cs.CL

TL;DR: 本文提出了一种名为DocExplainerV0的新模块，用于提升视觉-语言模型（VLMs）在文档信息抽取任务中答案定位的能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在提取复杂文档的文本信息方面表现良好，但在文档中准确定位答案的能力有限，影响了解释性和实际应用。

Method: 提出了一种即插即用的边界框预测模块DocExplainerV0，将答案生成与空间定位解耦，并能够应用于现有（特别是不能微调的）VLMs。

Result: 通过系统性评估，文章量化了文本准确性与空间定位之间的差距，指出即使答案正确，也常常无法可靠定位。

Conclusion: 作者提出的评测框架暴露了当前VLM在空间定位方面的不足，并为更具解释性和鲁棒性的文档信息抽取模型研究提供基准。

Abstract: Vision-Language Models (VLMs) have shown strong capabilities in document
understanding, particularly in identifying and extracting textual information
from complex documents. Despite this, accurately localizing answers within
documents remains a major challenge, limiting both interpretability and
real-world applicability. To address this, we introduce
\textit{DocExplainerV0}, a plug-and-play bounding-box prediction module that
decouples answer generation from spatial localization. This design makes it
applicable to existing VLMs, including proprietary systems where fine-tuning is
not feasible. Through systematic evaluation, we provide quantitative insights
into the gap between textual accuracy and spatial grounding, showing that
correct answers often lack reliable localization. Our standardized framework
highlights these shortcomings and establishes a benchmark for future research
toward more interpretable and robust document information extraction VLMs.

</details>


### [107] [Benchmark of stylistic variation in LLM-generated texts](https://arxiv.org/abs/2509.10179)
*Jiří Milička,Anna Marklová,Václav Cvrček*

Main category: cs.CL

TL;DR: 本研究对比分析了人类写作文本与大语言模型（LLM）生成文本在文体变异上的差异，重点揭示了LLM与人类在多维度分析中的系统性差异，并提出了一套评测基准。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型被广泛用于生成文本，研究其生成文本与真实人类文本之间在文体、文体维度上的差异变得尤为重要，以了解模型能力局限并指导未来模型改进。

Method: 本研究采用Biber的多维度分析方法（MDA），对比分析了由人类创作文本与基于LLM生成的AI-Brown语料库（与BE-21语料库可比），并在捷克语上利用AI-Koditex语料库和捷克多维度模型做了类似分析。研究覆盖了16个前沿模型，比较了基础模型与指令调优模型的表现差异。

Result: 结果表明，LLM生成文本在某些维度上与人类文本存在显著且系统的差异，并且模型的调优程度（如指令微调）会影响这些差异。研究建立了可解释的基准指标，对各模型进行排序和对比。

Conclusion: 本研究为全面评估LLM生成文本与人类文本之间的文体差异提供了方法和基准，推动了模型评测的系统化和透明化，并为多语种与未来模型改进提供了可操作框架。

Abstract: This study investigates the register variation in texts written by humans and
comparable texts produced by large language models (LLMs). Biber's
multidimensional analysis (MDA) is applied to a sample of human-written texts
and AI-created texts generated to be their counterparts to find the dimensions
of variation in which LLMs differ most significantly and most systematically
from humans. As textual material, a new LLM-generated corpus AI-Brown is used,
which is comparable to BE-21 (a Brown family corpus representing contemporary
British English). Since all languages except English are underrepresented in
the training data of frontier LLMs, similar analysis is replicated on Czech
using AI-Koditex corpus and Czech multidimensional model. Examined were 16
frontier models in various settings and prompts, with emphasis placed on the
difference between base models and instruction-tuned models. Based on this, a
benchmark is created through which models can be compared with each other and
ranked in interpretable dimensions.

</details>


### [108] [Incongruent Positivity: When Miscalibrated Positivity Undermines Online Supportive Conversations](https://arxiv.org/abs/2509.10184)
*Leen Almajed,Abeer ALdayel*

Main category: cs.CL

TL;DR: 本文研究了在情感支持对话中不恰当的积极回应（即“不协调的积极性”）现象，发现大模型（LLMs）尤其在高冲击场景下更易表现出不切实际和轻视情绪的积极倾向；提出了利用情感细颗粒度标注和新分类方法以更好检测并缓解该现象。


<details>
  <summary>Details</summary>
Motivation: 虽然积极回应通常被视为支持性沟通中的正面行为，但过度、失衡或不合时宜的积极反应可能导致被认为是轻视、最小化或假乐观，进而影响沟通质量。理解和纠正 LLMs 在此类对话中的表现，对于提升智能对话系统的情感敏感性与用户信任度至关重要。

Method: 作者收集了 Reddit 真实用户助手对话，并用大模型在相同语境下生成补充回应。按情感强度（轻度与重度，如关系紧张与悲伤焦虑）对对话进行分层。分析 LLMs 在不同场景下的积极倾向，并微调 LLMs 以综合强、弱情感反应。构建并训练包含 DeBERTa 和 MentalBERT 的弱监督多标签分类器以检测积极类型。

Result: 实验显示，LLMs 尤其在高情感强度的（如悲伤、焦虑）对话场景下，更常出现不协调的积极反应。弱监督分类器在区分不同积极类型和场景（轻度/重度）下有较好提升。

Conclusion: 工作强调，支持性对话不能仅依靠通用的积极回应，还需关注积极情绪与情感承认的一致性。提出的分析与方法有助于将 LLMs 调整至更符合情感期待，推动构建更能赢得信任、具情境感知的在线支持对话系统。

Abstract: In emotionally supportive conversations, well-intended positivity can
sometimes misfire, leading to responses that feel dismissive, minimizing, or
unrealistically optimistic. We examine this phenomenon of incongruent
positivity as miscalibrated expressions of positive support in both human and
LLM generated responses. To this end, we collected real user-assistant
dialogues from Reddit across a range of emotional intensities and generated
additional responses using large language models for the same context. We
categorize these conversations by intensity into two levels: Mild, which covers
relationship tension and general advice, and Severe, which covers grief and
anxiety conversations. This level of categorization enables a comparative
analysis of how supportive responses vary across lower and higher stakes
contexts. Our analysis reveals that LLMs are more prone to unrealistic
positivity through dismissive and minimizing tone, particularly in high-stakes
contexts. To further study the underlying dimensions of this phenomenon, we
finetune LLMs on datasets with strong and weak emotional reactions. Moreover,
we developed a weakly supervised multilabel classifier ensemble (DeBERTa and
MentalBERT) that shows improved detection of incongruent positivity types
across two sorts of concerns (Mild and Severe). Our findings shed light on the
need to move beyond merely generating generic positive responses and instead
study the congruent support measures to balance positive affect with emotional
acknowledgment. This approach offers insights into aligning large language
models with affective expectations in the online supportive dialogue, paving
the way toward context-aware and trust preserving online conversation systems.

</details>


### [109] [Beyond Token Limits: Assessing Language Model Performance on Long Text Classification](https://arxiv.org/abs/2509.10199)
*Miklós Sebők,Viktor Kovács,Martin Bánóczy,Daniel Møller Eriksen,Nathalie Neptune,Philippe Roussille*

Main category: cs.CL

TL;DR: 该论文比较了主流大语言模型在处理超长文本多分类任务上的表现，发现针对长文本设计的长文本模型并无明显优势。


<details>
  <summary>Details</summary>
Motivation: 主流的大语言模型（如BERT及其变体）在输入长度上存在限制，难以应对如法律文本这类超长文档的分类任务。因此，亟需探索适用于超长文本分类的有效模型与方法。

Method: 论文采用XLM-RoBERTa、Longformer、GPT-3.5、GPT-4等模型，在比较议程项目（Comparative Agendas Project）21类政策主题的多分类任务中进行实验，涵盖5种语言，比对不同模型在长文本输入下的表现。

Result: 实验结果显示，为长文本专门预训练的Longformer并未表现出预期优势。GPT模型与性能最优的开源模型相比，后者在长文本多分类任务上有一定领先。进一步分析指出，类别间支持度和内容重叠对长文本分类性能有重要影响。

Conclusion: 现有专为长文本设计的模型（如Longformer）在实际多分类任务中并不一定具备优势，未来模型改进需考虑类别支持与内容交叠对长文本任务的实际影响。

Abstract: The most widely used large language models in the social sciences (such as
BERT, and its derivatives, e.g. RoBERTa) have a limitation on the input text
length that they can process to produce predictions. This is a particularly
pressing issue for some classification tasks, where the aim is to handle long
input texts. One such area deals with laws and draft laws (bills), which can
have a length of multiple hundred pages and, therefore, are not particularly
amenable for processing with models that can only handle e.g. 512 tokens. In
this paper, we show results from experiments covering 5 languages with
XLM-RoBERTa, Longformer, GPT-3.5, GPT-4 models for the multiclass
classification task of the Comparative Agendas Project, which has a codebook of
21 policy topic labels from education to health care. Results show no
particular advantage for the Longformer model, pre-trained specifically for the
purposes of handling long inputs. The comparison between the GPT variants and
the best-performing open model yielded an edge for the latter. An analysis of
class-level factors points to the importance of support and substance overlaps
between specific categories when it comes to performance on long text inputs.

</details>


### [110] [SI-FACT: Mitigating Knowledge Conflict via Self-Improving Faithfulness-Aware Contrastive Tuning](https://arxiv.org/abs/2509.10208)
*Shengqiang Fu*

Main category: cs.CL

TL;DR: 本文提出了一种自我改进的对比学习框架（SI FACT），用于提升大模型在知识密集任务中的响应忠实度，实验证明该方法可以显著提升模型基于上下文的召回率，同时减少对内部记忆的依赖。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在面对知识冲突时，往往倾向于依赖自身的参数化知识而忽略给定的上下文，导致生成响应缺乏忠实性。针对这一问题，亟需提升模型对外部上下文的依赖和忠实程度。

Method: 提出自我指导的对比学习方法，让基础的大模型自动生成高质量、结构化的对比学习数据（含锚点样本、等价正样本、模拟不忠实情况的负样本），有效降低人工标注成本。然后通过对比学习拉近忠实响应距离、远离不忠实响应。

Result: 在ECARE KRE和COSE KRE两个知识冲突评测基准上，基于Llama3 8B Instruct的SI FACT模型相比最优基线方法将基于上下文的召回率提升了6.2%，且显著减少了对内部知识的依赖。

Conclusion: SI FACT方法在提升大语言模型的上下文忠实性方面表现出强效和高效性，为构建更主动和可信赖的语言模型提供了切实可行的方式。

Abstract: Large Language Models often generate unfaithful responses in knowledge
intensive tasks due to knowledge conflict,that is,a preference for relying on
internal parametric knowledge rather than the provided context.To address this
issue,we propose a novel self improving framework,Self Improving Faithfulness
Aware Contrastive Tuning.The framework uses a self instruct mechanism that
allows the base LLM to automatically generate high quality,structured
contrastive learning data,including anchor samples,semantically equivalent
positive samples,and negative samples simulating unfaithful scenarios.This
approach significantly reduces the cost of manual
annotation.Subsequently,contrastive learning is applied to train the
model,enabling it to pull faithful responses closer and push unfaithful
responses farther apart in the representation space.Experiments on knowledge
conflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACT
model based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2%
over the best baseline method,while significantly reducing dependence on
internal memory.The results indicate that SI FACT provides strong effectiveness
and high data efficiency in enhancing the contextual faithfulness of
LLMs,offering a practical pathway toward building more proactive and
trustworthy language models.

</details>


### [111] [Dropping Experts, Recombining Neurons: Retraining-Free Pruning for Sparse Mixture-of-Experts LLMs](https://arxiv.org/abs/2509.10377)
*Yixiao Zhou,Ziyu Zhao,Dongzhou Cheng,zhiliang wu,Jie Gui,Yi Yang,Fei Wu,Yu Cheng,Hehe Fan*

Main category: cs.CL

TL;DR: 提出了一种新的SMoE压缩框架DERN，通过神经元级处理有效减少大模型部署的内存消耗且无需额外训练，同时性能提升显著。


<details>
  <summary>Details</summary>
Motivation: SMoE模型虽提高了计算效率，但所有专家参数都要加载，带来高内存开销，难以实际部署。以往方法对专家粗粒度处理，忽视了神经元层面的冗余和冲突。

Method: 提出三步优化流程：1）通过路由器统计先裁剪冗余专家；2）将裁剪的专家分解为神经元级片段，并归于最兼容的保留专家；3）合并片段，紧致模型结构，全部过程无需额外训练。

Result: 在Mixtral、Qwen、DeepSeek等SMoE大模型上测试，DERN在50%专家稀疏的条件下，常识推理和MMLU指标性能提升超5%，专家数量和内存大幅减少。

Conclusion: DERN方法无须重训练，能高效压缩减少SMoE模型内存，提升实际部署友好性并保持甚至提升原有性能。

Abstract: Sparse Mixture-of-Experts (SMoE) architectures are widely used in large
language models (LLMs) due to their computational efficiency. However, though
only a few experts are activated for each token, SMoE still requires loading
all expert parameters, leading to high memory usage and challenges in
deployment. Previous work has tried to reduce the overhead by pruning and
merging experts, but primarily focused on expert-level operations, leaving
neuron-level structure underexplored. We propose DERN (Dropping Experts,
Recombining Neurons), a task-agnostic and retraining-free framework for expert
pruning and reconstruction. We observe that experts are often misaligned and
contain semantic conflicts at the neuron level, which poses challenges for
direct merging. To solve this, DERN works in three steps: it first prunes
redundant experts using router statistics; then it decomposes them into
neuron-level expert segments, assigning each segment to its most compatible
retained expert; and finally, it merges segments within each retained expert to
build a compact representation. Experiments on Mixtral, Qwen, and DeepSeek SMoE
models show that DERN improves performance by more than 5% on commonsense
reasoning and MMLU benchmarks under 50% expert sparsity, without extra
training. It also greatly reduces the number of experts and memory usage,
making SMoE LLMs easier to deploy in practice.

</details>


### [112] [Is In-Context Learning Learning?](https://arxiv.org/abs/2509.10414)
*Adrian de Wynter*

Main category: cs.CL

TL;DR: 本文对大规模语言模型的 in-context learning（ICL）机制进行了系统分析，发现其虽为有效的学习范式，但在泛化到新任务方面能力有限，且高度依赖于提示示例的分布与表达。


<details>
  <summary>Details</summary>
Motivation: ICL被广泛认为能让模型通过少量样例快速适应新任务，但对其是否真正实现了“学习”仍有争议，尤其是ICL并不显式编码观测，只依赖于已有知识与示例。该文旨在澄清ICL是否属于学习，并探究其机制和局限。

Method: 作者通过大规模实验，系统性地剖析了ICL，重点考察记忆、预训练、分布移位、提示风格/措辞等因素对ICL表现的影响，并对提示示例（exemplar）数量、分布及输入特征进行消融和归因分析。

Result: ICL确为有效的学习范式，但其泛化能力受限。在示例数量充足时，准确率对分布、模型、提示风格及输入语言特征几乎不敏感。模型主要根据提示中的规律进行推断，分布敏感性在某些提示风格（如chain-of-thought）下尤为明显。

Conclusion: ICL通过自回归机制实现的是一种“临时编码”，在形式上属于学习，但这种机制对泛用性和健壮性有限，不足以支撑全方位的任务泛化。

Abstract: In-context learning (ICL) allows some autoregressive models to solve tasks
via next-token prediction and without needing further training. This has led to
claims about these model's ability to solve (learn) unseen tasks with only a
few shots (exemplars) in the prompt. However, deduction does not always imply
learning, as ICL does not explicitly encode a given observation. Instead, the
models rely on their prior knowledge and the exemplars given, if any. We argue
that, mathematically, ICL does constitute learning, but its full
characterisation requires empirical work. We then carry out a large-scale
analysis of ICL ablating out or accounting for memorisation, pretraining,
distributional shifts, and prompting style and phrasing. We find that ICL is an
effective learning paradigm, but limited in its ability to learn and generalise
to unseen tasks. We note that, in the limit where exemplars become more
numerous, accuracy is insensitive to exemplar distribution, model, prompt
style, and the input's linguistic features. Instead, it deduces patterns from
regularities in the prompt, which leads to distributional sensitivity,
especially in prompting styles such as chain-of-thought. Given the varied
accuracies on formally similar tasks, we conclude that autoregression's ad-hoc
encoding is not a robust mechanism, and suggests limited all-purpose
generalisability.

</details>


### [113] [Long Context Automated Essay Scoring with Language Models](https://arxiv.org/abs/2509.10417)
*Christopher Ormerod,Gitit Kehat*

Main category: cs.CL

TL;DR: 本文探讨了变换器模型在自动作文评分中的文本长度限制问题，并评估了多种可处理长文本的模型在实际评分任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前主流的变换器结构存在输入文本最大长度的硬性限制，而高年级学生的作文常常超出这个长度。以往裁剪文本的方式，导致模型无法充分把握全文脉络和组织结构，影响评分有效性。为解决这一问题，作者希望找到能处理更长文本的有效方法。

Method: 作者选取了五种结构上经过改进以处理长文本的模型（XLNet、Longformer、ModernBERT、Mamba和Llama），并在标准作文评分数据集Kaggle ASAP 2.0上进行了微调和评测。

Result: 通过对比实验，作者比较了这些长文本模型在自动作文评分任务上的表现，并分析它们能否克服标准transformer长度限制带来的评分缺陷。

Conclusion: 研究指出结构改进的长文本模型在自动作文评分中优于传统方法，更有效地捕捉了组织结构等需长距离依赖的信息，有助于提高评分的有效性和可靠性。

Abstract: Transformer-based language models are architecturally constrained to process
text of a fixed maximum length. Essays written by higher-grade students
frequently exceed the maximum allowed length for many popular open-source
models. A common approach to addressing this issue when using these models for
Automated Essay Scoring is to truncate the input text. This raises serious
validity concerns as it undermines the model's ability to fully capture and
evaluate organizational elements of the scoring rubric, which requires long
contexts to assess. In this study, we evaluate several models that incorporate
architectural modifications of the standard transformer architecture to
overcome these length limitations using the Kaggle ASAP 2.0 dataset. The models
considered in this study include fine-tuned versions of XLNet, Longformer,
ModernBERT, Mamba, and Llama models.

</details>


### [114] [RefactorCoderQA: Benchmarking LLMs for Multi-Domain Coding Question Solutions in Cloud and Edge Deployment](https://arxiv.org/abs/2509.10436)
*Shadikur Rahman,Aroosa Hameed,Gautam Srivastava,Syed Muhammad Danish*

Main category: cs.CL

TL;DR: 本文提出了一种云-边协同的多智能体LLM架构，用于提升大型语言模型的推理与问题求解能力，并以RefactorCoderQA新基准验证其实用性和性能。


<details>
  <summary>Details</summary>
Motivation: 目前LLM在复杂和多领域编码任务上的推理与解决方案能力受限，现有编码基准测试也缺乏系统性和领域覆盖，亟需新的架构和评测方法以推进LLM实际效用。

Method: 架构设计为三模型协作：GuideLLM（边缘侧引导）、SolverLLM（云端主力生成代码）、JudgeLLM（自动评价器），支持结构化多智能体协同推理；为评估，构建了覆盖多个技术领域（包括软件工程、数据科学、机器学习和NLP）的RefactorCoderQA基准，利用真实Stack Overflow编码挑战进行测试。

Result: 经充分实验，作者微调的RefactorCoder-MoE模型取得76.84%的整体准确率，显著超越现有开源、商用主流模型。此外，人工评估确认了解决方案的可解释性、准确性和实用性，系统层面还评测了吞吐量和延迟等性能指标。

Conclusion: 提出的协同架构和基准提升了LLM多领域代码任务的实用性和有效性，对模型的实际部署与性能评估具有明确指导意义。

Abstract: To optimize the reasoning and problem-solving capabilities of Large Language
Models (LLMs), we propose a novel cloud-edge collaborative architecture that
enables a structured, multi-agent prompting framework. This framework comprises
three specialized components: GuideLLM, a lightweight model deployed at the
edge to provide methodological guidance; SolverLLM, a more powerful model
hosted in the cloud responsible for generating code solutions; and JudgeLLM, an
automated evaluator for assessing solution correctness and quality. To evaluate
and demonstrate the effectiveness of this architecture in realistic settings,
we introduce RefactorCoderQA, a comprehensive benchmark designed to evaluate
and enhance the performance of Large Language Models (LLMs) across multi-domain
coding tasks. Motivated by the limitations of existing benchmarks,
RefactorCoderQA systematically covers various technical domains, including
Software Engineering, Data Science, Machine Learning, and Natural Language
Processing, using authentic coding challenges from Stack Overflow. Extensive
experiments reveal that our fine-tuned model, RefactorCoder-MoE, achieves
state-of-the-art performance, significantly outperforming leading open-source
and commercial baselines with an overall accuracy of 76.84%. Human evaluations
further validate the interpretability, accuracy, and practical relevance of the
generated solutions. In addition, we evaluate system-level metrics, such as
throughput and latency, to gain deeper insights into the performance
characteristics and trade-offs of the proposed architecture.

</details>


### [115] [DeepDive: Advancing Deep Search Agents with Knowledge Graphs and Multi-Turn RL](https://arxiv.org/abs/2509.10446)
*Rui Lu,Zhenyu Hou,Zihan Wang,Hanchen Zhang,Xiao Liu,Yujiang Li,Shi Feng,Jie Tang,Yuxiao Dong*

Main category: cs.CL

TL;DR: DeepDive提出了一种通过自动生成复杂问题和多轮强化学习训练来提升大模型利用浏览工具进行深度搜索能力的方法，并取得了领先的开源实验证明。


<details>
  <summary>Details</summary>
Motivation: 当前开源大语言模型（LLM）在结合浏览工具时，在复杂任务和长链条推理上表现不佳，主要原因包括推理能力有限以及缺乏难度足够的大规模监督数据。论文旨在解决这两大关键挑战，提升开源LLM作为深度搜索智能体的潜力。

Method: 1）自动从开放知识图谱中合成复杂难找的问题，用于生成更具挑战性的训练数据；2）通过端到端多轮强化学习（multi-turn RL）方式训练模型，以增强其长程推理和利用浏览器工具的能力。

Result: DeepDive-32B在主流开放基准BrowseComp上表现出领先能力，超过了WebSailor、DeepSeek-R1-Browse和Search-o1等开源系统。同时实验证明，多轮RL训练能够显著提升模型深度搜索与推理能力，推动不同基准测试上的整体性能。

Conclusion: DeepDive方案有效提升了大模型结合浏览工具的深度搜索能力，为开源LLM应用于复杂现实任务奠定基础。论文还表明多轮强化学习训练在深度搜索中的核心作用。所有代码和数据均已开源，便于社区复现和改进。

Abstract: Augmenting large language models (LLMs) with browsing tools substantially
improves their potential as deep search agents to solve complex, real-world
tasks. Yet, open LLMs still perform poorly in such settings due to limited
long-horizon reasoning capacity with browsing tools and the lack of
sufficiently difficult supervised data. To address these challenges, we present
DeepDive to advance deep search agents. First, we propose a strategy to
automatically synthesize complex, difficult, and hard-to-find questions from
open knowledge graphs. Second, we apply end-to-end multi-turn reinforcement
learning (RL) to enhance LLMs' long-horizon reasoning with deep search.
Experiments show that DeepDive-32B achieves a new open-source competitive
result on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and
Search-o1. We demonstrate that multi-turn RL training improves deep search
ability and significantly contributes to the performance improvements across
multiple benchmarks. We observe that DeepDive enables test-time scaling of tool
calls and parallel sampling. All datasets, models, and code are publicly
available at https://github.com/THUDM/DeepDive.

</details>


### [116] [WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers](https://arxiv.org/abs/2509.10452)
*Akshat Pandey,Karun Kumar,Raphael Tang*

Main category: cs.CL

TL;DR: 提出WhisTLE方法，实现对预训练ASR模型仅用文本的深度监督适应，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 虽然诸如Whisper的预训练ASR模型在通用任务上表现良好，但对于新领域中出现的未见词汇和表达方式，仍需进行领域适应。而在实际应用中，语音数据往往难以获取，因此急需一种只需文本的领域适应方案。

Method: 作者提出WhisTLE方法，针对编码器-解码器结构的ASR模型。首先，利用变分自编码器（VAE）模型学习如何将文本映射到编码器输出的隐空间；随后，用这种基于文本的隐向量对原有解码器进行微调。可选地，将该方法与文本转语音（TTS）数据结合训练。推理时恢复原始编码器，不增加额外计算负担。

Result: 在4个领域外数据集和4种ASR模型上，WhisTLE方法结合TTS时，相较于单独TTS适应，词错误率（WER）相对减少12.3%，并在32个测试场景中的27个超过所有其他非WhisTLE基线。

Conclusion: WhisTLE方法实现在无语音数据条件下，通过文本进行ASR模型的有效领域适应，在多个基准与模型上展现出优异效果。

Abstract: Pretrained automatic speech recognition (ASR) models such as Whisper perform
well but still need domain adaptation to handle unseen vocabulary and parlance.
In many real-world settings, collecting speech data is impractical,
necessitating text-only adaptation. We propose WhisTLE, a deeply supervised,
text-only adaptation method for pretrained encoder-decoder ASR models. WhisTLE
trains a variational autoencoder (VAE) to model encoder outputs from text and
fine-tunes the decoder using the learned text-to-latent encoder, optionally
combined with text-to-speech (TTS) adaptation. At inference, the original
encoder is restored, incurring no extra runtime cost. Across four out-of-domain
datasets and four ASR models, WhisTLE with TTS reduces word error rate (WER) by
12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines
in 27 of 32 scenarios.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [117] [MimicDroid: In-Context Learning for Humanoid Robot Manipulation from Human Play Videos](https://arxiv.org/abs/2509.09769)
*Rutav Shah,Shuijing Liu,Qi Wang,Zhenyu Jiang,Sateesh Kumar,Mingyo Seo,Roberto Martín-Martín,Yuke Zhu*

Main category: cs.RO

TL;DR: 本论文提出了MimicDroid，一个利用无标注人类操作视频实现人形机器人高效模仿和在新任务中的快速适应的系统，大幅提升了机器人泛化与学习效率。


<details>
  <summary>Details</summary>
Motivation: 传统机器人依赖人工遥操作数据训练，收集成本高、规模有限，严重制约机器人泛化能力和实际落地。

Method: 作者采用‘人类操作视频’（无标签、连续人类与环境交互视频）作为训练数据，提出MimicDroid方法：从视频中提取相似操作轨迹对，通过一个轨迹条件预测另一个轨迹动作以进行ICL（in-context learning），并通过人手腕姿态重定向与随机遮挡增强提升模型鲁棒性和通用性。

Result: 作者开发了难度递增的仿真基准，并在真实环境中验证。MimicDroid在各类测试中都显著优于现有方法，现实世界成功率提升近一倍。

Conclusion: MimicDroid证明了利用大规模人类日常操作视频进行机器人训练的可行性和优越性，为高效、普适的人形机器人自学能力奠定了基础。

Abstract: We aim to enable humanoid robots to efficiently solve new manipulation tasks
from a few video examples. In-context learning (ICL) is a promising framework
for achieving this goal due to its test-time data efficiency and rapid
adaptability. However, current ICL methods rely on labor-intensive teleoperated
data for training, which restricts scalability. We propose using human play
videos -- continuous, unlabeled videos of people interacting freely with their
environment -- as a scalable and diverse training data source. We introduce
MimicDroid, which enables humanoids to perform ICL using human play videos as
the only training data. MimicDroid extracts trajectory pairs with similar
manipulation behaviors and trains the policy to predict the actions of one
trajectory conditioned on the other. Through this process, the model acquired
ICL capabilities for adapting to novel objects and environments at test time.
To bridge the embodiment gap, MimicDroid first retargets human wrist poses
estimated from RGB videos to the humanoid, leveraging kinematic similarity. It
also applies random patch masking during training to reduce overfitting to
human-specific cues and improve robustness to visual differences. To evaluate
few-shot learning for humanoids, we introduce an open-source simulation
benchmark with increasing levels of generalization difficulty. MimicDroid
outperformed state-of-the-art methods and achieved nearly twofold higher
success rates in the real world. Additional materials can be found on:
ut-austin-rpl.github.io/MimicDroid

</details>


### [118] [MIMo grows! Simulating body and sensory development in a multimodal infant model](https://arxiv.org/abs/2509.09805)
*Francisco M. López,Miles Lenz,Marco G. Fedozzi,Arthur Aubret,Jochen Triesch*

Main category: cs.RO

TL;DR: MIMo v2 是一款针对0至24个月婴儿感知运动发展的模拟平台，具有更真实的身体和感官变化建模能力。


<details>
  <summary>Details</summary>
Motivation: 传统的发育机器人和模拟平台通常只针对某一固定年龄阶段，无法真实反映婴儿随年龄变化的能力和约束。作者希望开发一个能随着“年龄”变化的模型，提升感知运动发展的研究精度。

Method: 提出MIMo v2多模态婴儿模型，引入了可生长的身体（机械力量随年龄增强）、视网膜式视觉和视觉清晰度发展、感觉-运动延迟（模拟信号传递速度有限），以及逆运动学模块、随机环境生成器，并兼容多种第三方模拟和学习库。

Result: MIMo v2显著提升了婴儿感知-运动发展建模的真实感，可更全面地仿真各年龄段婴儿的动作与感知过程。

Conclusion: MIMo v2提供了更接近真实婴儿发育过程的机器人和模拟平台工具，有望推动感知-运动发展相关研究，成为相关领域的有力工具。

Abstract: Infancy is characterized by rapid body growth and an explosive change of
sensory and motor abilities. However, developmental robots and simulation
platforms are typically designed in the image of a specific age, which limits
their ability to capture the changing abilities and constraints of developing
infants. To address this issue, we present MIMo v2, a new version of the
multimodal infant model. It includes a growing body with increasing actuation
strength covering the age range from birth to 24 months. It also features
foveated vision with developing visual acuity as well as sensorimotor delays
modeling finite signal transmission speeds to and from an infant's brain.
Further enhancements of this MIMo version include an inverse kinematics module,
a random environment generator and updated compatiblity with third-party
simulation and learning libraries. Overall, this new MIMo version permits
increased realism when modeling various aspects of sensorimotor development.
The code is available on the official repository
(https://github.com/trieschlab/MIMo).

</details>


### [119] [Using the Pepper Robot to Support Sign Language Communication](https://arxiv.org/abs/2509.09889)
*Giulia Botta,Marco Botta,Cristina Gena,Alessandro Mazzei,Massimo Donini,Alberto Lillo*

Main category: cs.RO

TL;DR: 本研究评估了商业社交机器人Pepper是否能用意大利手语（LIS）进行有效沟通，发现Pepper可以较好表达部分孤立手语词汇，但在完整句子表达上存在不足。


<details>
  <summary>Details</summary>
Motivation: 由于社交机器人在公共和辅助场所的应用日益广泛，但针对聋人用户、尤其是手语交流的可达性研究极少，作者期望借助Pepper机器人提升聋人与机器人之间的包容性互动。

Method: 研究团队与一位聋人学生及其LIS专家翻译合作，为Pepper编程实现52个LIS词汇，通过手动动画与基于MATLAB的逆运动学方法实现。然后邀请12名LIS流利者参与视频识别问卷和开放式问题测试Pepper的手语表达能力。

Result: 结果显示，多数孤立手语动作能被受试者正确识别，但完整句子的识别率明显较低，原因包括Pepper机器人动作表达受限和时序问题。

Conclusion: 目前市售机器人如Pepper具备部分手语传播能力，初步可实现更具包容性的互动设计。未来应结合多模态表达（如屏幕辅助、虚拟手语角色）和聋人深度参与设计，以优化机器人人机交互的表达性和可用性。

Abstract: Social robots are increasingly experimented in public and assistive settings,
but their accessibility for Deaf users remains quite underexplored. Italian
Sign Language (LIS) is a fully-fledged natural language that relies on complex
manual and non-manual components. Enabling robots to communicate using LIS
could foster more inclusive human robot interaction, especially in social
environments such as hospitals, airports, or educational settings. This study
investigates whether a commercial social robot, Pepper, can produce
intelligible LIS signs and short signed LIS sentences. With the help of a Deaf
student and his interpreter, an expert in LIS, we co-designed and implemented
52 LIS signs on Pepper using either manual animation techniques or a MATLAB
based inverse kinematics solver. We conducted a exploratory user study
involving 12 participants proficient in LIS, both Deaf and hearing.
Participants completed a questionnaire featuring 15 single-choice video-based
sign recognition tasks and 2 open-ended questions on short signed sentences.
Results shows that the majority of isolated signs were recognized correctly,
although full sentence recognition was significantly lower due to Pepper's
limited articulation and temporal constraints. Our findings demonstrate that
even commercially available social robots like Pepper can perform a subset of
LIS signs intelligibly, offering some opportunities for a more inclusive
interaction design. Future developments should address multi-modal enhancements
(e.g., screen-based support or expressive avatars) and involve Deaf users in
participatory design to refine robot expressivity and usability.

</details>


### [120] [Self-Augmented Robot Trajectory: Efficient Imitation Learning via Safe Self-augmentation with Demonstrator-annotated Precision](https://arxiv.org/abs/2509.09893)
*Hanbit Oh,Masaki Murooka,Tomohiro Motoda,Ryoichi Nakajo,Yukiyasu Domae*

Main category: cs.RO

TL;DR: 本文提出了一种名为SART的模仿学习方法，仅需一次人工示范，通过机器人自主轨迹扩增，提高数据采集效率，实现更高的学习成功率，并兼顾安全性。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习依赖大量人工示范或随机探索，数据获取成本高。随机探索又缺乏安全保障，易导致碰撞，影响任务推进和安全，增加人工重置环境的负担。作者希望在保证安全和效率的前提下，极大减少人工参与。

Method: SART方法包含两阶段：第一阶段，人工仅进行一次示范并用球形边界标注关键路径点，然后重置环境一次；第二阶段，机器人自动在受限边界范围内生成多样的、无碰撞的轨迹，并与原始示范连接，从而进行自主数据扩充。这样极大提高了数据集多样性和质量，同时保障安全。

Result: 通过仿真及现实世界的操作测试，SART方法相比只依赖人工示范训练的策略，显著提升了任务成功率。

Conclusion: SART极大改善了模仿学习的数据采集效率，降低了人工参与并确保操作安全，为实际机器人任务提供了更有效的数据采集与学习方式。

Abstract: Imitation learning is a promising paradigm for training robot agents;
however, standard approaches typically require substantial data acquisition --
via numerous demonstrations or random exploration -- to ensure reliable
performance. Although exploration reduces human effort, it lacks safety
guarantees and often results in frequent collisions -- particularly in
clearance-limited tasks (e.g., peg-in-hole) -- thereby, necessitating manual
environmental resets and imposing additional human burden. This study proposes
Self-Augmented Robot Trajectory (SART), a framework that enables policy
learning from a single human demonstration, while safely expanding the dataset
through autonomous augmentation. SART consists of two stages: (1) human
teaching only once, where a single demonstration is provided and precision
boundaries -- represented as spheres around key waypoints -- are annotated,
followed by one environment reset; (2) robot self-augmentation, where the robot
generates diverse, collision-free trajectories within these boundaries and
reconnects to the original demonstration. This design improves the data
collection efficiency by minimizing human effort while ensuring safety.
Extensive evaluations in simulation and real-world manipulation tasks show that
SART achieves substantially higher success rates than policies trained solely
on human-collected demonstrations. Video results available at
https://sites.google.com/view/sart-il .

</details>


### [121] [Detection of Anomalous Behavior in Robot Systems Based on Machine Learning](https://arxiv.org/abs/2509.09953)
*Mahfuzul I. Nissan,Sharmin Aktar*

Main category: cs.RO

TL;DR: 本文提出基于机器学习的日志异常检测方法，以提升机器人系统的安全与可靠性。通过在两个机器人场景中的实验，分别对多种机器学习模型的性能进行了对比。


<details>
  <summary>Details</summary>
Motivation: 尽管机器人系统经过严格设计，但其仍可能因故障而引发安全问题。现有方法难以及时有效检测未知异常，因此亟需自动化、高效的异常检测机制。

Method: 研究收集了两种仿真环境（无人机与Pioneer机器人）的系统日志，并用Logistic回归、支持向量机（SVM）与自编码器等机器学习模型进行异常检测性能对比评估。

Result: 在无人机场景下，Logistic回归性能最佳；在Pioneer机器人场景下，自编码器效果显著优于其他方法。不同场景下，最佳模型选择不同。

Conclusion: 自编码器对复杂异常检测展现出优势。不同机器人平台对异常检测模型的需求具有差异；多模型对比方法有助于找到最佳解决方案。

Abstract: Ensuring the safe and reliable operation of robotic systems is paramount to
prevent potential disasters and safeguard human well-being. Despite rigorous
design and engineering practices, these systems can still experience
malfunctions, leading to safety risks. In this study, we present a machine
learning-based approach for detecting anomalies in system logs to enhance the
safety and reliability of robotic systems. We collected logs from two distinct
scenarios using CoppeliaSim and comparatively evaluated several machine
learning models, including Logistic Regression (LR), Support Vector Machine
(SVM), and an Autoencoder. Our system was evaluated in a quadcopter context
(Context 1) and a Pioneer robot context (Context 2). Results showed that while
LR demonstrated superior performance in Context 1, the Autoencoder model proved
to be the most effective in Context 2. This highlights that the optimal model
choice is context-dependent, likely due to the varying complexity of anomalies
across different robotic platforms. This research underscores the value of a
comparative approach and demonstrates the particular strengths of autoencoders
for detecting complex anomalies in robotic systems.

</details>


### [122] [Gaussian path model library for intuitive robot motion programming by demonstration](https://arxiv.org/abs/2509.10007)
*Samuli Soutukorva,Markku Suomalainen,Martin Kollingbaum,Tapio Heikkilä*

Main category: cs.RO

TL;DR: 这篇论文提出了一种系统，可以从教学数据中生成高斯路径模型，并进一步用这些模型对人类的路径演示进行分类。还介绍了通过几何分析示范数据对已有模型进行修改的方法。


<details>
  <summary>Details</summary>
Motivation: 机器人运动编程需要直观且高效地将人类的演示路径转化为机器人可以理解和执行的路径模式。现有方法可能缺乏灵活性和直观性，因此需要一种自动化且可修改的路径建模系统。

Method: 利用人类演示的路径数据，生成形状对应的高斯路径模型，构建包含多种路径形状的高斯路径模型库。再用这些模型对新的演示样本进行分类，以实现直观的机器人运动编程。还提出了一种通过几何分析进一步修改和完善已有高斯路径模型的方法。

Result: 系统能够从不同的人类演示中自动生成并分类多样的路径模型，且能够通过几何示范对已有路径模型进行有效的修改和调整。

Conclusion: 该方法提升了机器人运动路径建模的直观性和灵活性，使得机器人能够更好地学习和适应多样的人类演示路径，并能通过简单示范不断优化已有路径模型。

Abstract: This paper presents a system for generating Gaussian path models from
teaching data representing the path shape. In addition, methods for using these
path models to classify human demonstrations of paths are introduced. By
generating a library of multiple Gaussian path models of various shapes, human
demonstrations can be used for intuitive robot motion programming. A method for
modifying existing Gaussian path models by demonstration through geometric
analysis is also presented.

</details>


### [123] [Towards simulation-based optimization of compliant fingers for high-speed connector assembly](https://arxiv.org/abs/2509.10012)
*Richard Matthias Hartisch,Alexander Rother,Jörg Krüger,Kevin Haninger*

Main category: cs.RO

TL;DR: 本文提出了一种基于仿真的顺应性机构设计工具，可根据具体操作任务目标（如成功率）优化软体机械手指的结构参数，从而提升其机械顺应性与稳健性，并通过实物实验验证了设计工具的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前软体机器人结构（如柔性手指）的设计参数通常依靠硬件反复试验或简化模型获得，时间消耗大且难以应对复杂操作任务。有效的机械顺应性设计对于提高机器人的任务成功率和安全性非常重要。针对动态接触和摩擦建模仿真的进步，作者希望开发一个任务驱动的结构设计工具，提升参数选择的效率与科学性。

Method: 作者提出一种用动态仿真（包含接触和摩擦建模）的设计工具，根据具体任务（如插入任务中的成功率）优化柔性手指的结构参数。在仿真中综合考虑了柔性机构的几何和刚度等因素，并在机器人实物上进行了NIST任务板测试，验证优化参数的物理效果。

Result: 优化设计参数后，手指的容差范围提升显著，最宽可扩大2.29倍，可补偿最大8.6mm的工件变化。此外，不同任务对刚度的最优选择有所差异，有的任务高刚度最佳，有的则相反。

Conclusion: 基于任务目标的仿真设计工具能有效提升柔性手指的稳健性和可容忍工件变化范围，但最优参数依赖任务类型，说明有必要开发能结合具体应用和动力学特性的设计工具。

Abstract: Mechanical compliance is a key design parameter for dynamic contact-rich
manipulation, affecting task success and safety robustness over contact
geometry variation. Design of soft robotic structures, such as compliant
fingers, requires choosing design parameters which affect geometry and
stiffness, and therefore manipulation performance and robustness. Today, these
parameters are chosen through either hardware iteration, which takes
significant development time, or simplified models (e.g. planar), which can't
address complex manipulation task objectives. Improvements in dynamic
simulation, especially with contact and friction modeling, present a potential
design tool for mechanical compliance. We propose a simulation-based design
tool for compliant mechanisms which allows design with respect to task-level
objectives, such as success rate. This is applied to optimize design parameters
of a structured compliant finger to reduce failure cases inside a tolerance
window in insertion tasks. The improvement in robustness is then validated on a
real robot using tasks from the benchmark NIST task board. The finger stiffness
affects the tolerance window: optimized parameters can increase tolerable
ranges by a factor of 2.29, with workpiece variation up to 8.6 mm being
compensated. However, the trends remain task-specific. In some tasks, the
highest stiffness yields the widest tolerable range, whereas in others the
opposite is observed, motivating need for design tools which can consider
application-specific geometry and dynamics.

</details>


### [124] [Design and Evaluation of Two Spherical Systems for Mobile 3D Mapping](https://arxiv.org/abs/2509.10032)
*Marawan Khalil,Fabian Arzberger,Andreas Nüchter*

Main category: cs.RO

TL;DR: 该论文提出并比较了两种球形机器人地图测绘系统（分别为无驱动和摆锤驱动），采用固态激光雷达及LIO算法进行高动态环境下的测绘性能分析，发现主流LIO算法在此类机器人高动态运动下表现出精度下降和地图漂移问题。


<details>
  <summary>Details</summary>
Motivation: 球形机器人因具备良好的保护壳和全方位运动能力，非常适用于危险或狭小环境的测绘，但现有导航与建图算法主要针对传统移动平台，尚未验证其在高动态球形平台上的性能。为此，作者设计实验以评估这些算法在球形机器人上的表现。

Method: 设计了两种球形移动机器人测绘系统：一种为轻量化、无驱动结构，另一种为基于内部摆锤驱动的有驱动设计。两者均搭载Livox Mid-360固态激光雷达，运行主流LIO算法，并选用资源受限的硬件平台。在实际环境中采集测绘数据，并通过与真实地图对比评估系统精度。

Result: 结果显示，在球形机器人运动过程中，由于机器人带来的高动态运动，主流LIO算法的表现明显下降，产生全局不一致的地图，并且有时会导致不可恢复的漂移误差。

Conclusion: 现有LIO算法难以直接高效应用在球形机器人这样高动态的平台上，需要针对球形机器人运动特性进一步优化建图与定位算法，以满足其在特殊环境下的高精度测绘需求。

Abstract: Spherical robots offer unique advantages for mapping applications in
hazardous or confined environments, thanks to their protective shells and
omnidirectional mobility. This work presents two complementary spherical
mapping systems: a lightweight, non-actuated design and an actuated variant
featuring internal pendulum-driven locomotion. Both systems are equipped with a
Livox Mid-360 solid-state LiDAR sensor and run LiDAR-Inertial Odometry (LIO)
algorithms on resource-constrained hardware. We assess the mapping accuracy of
these systems by comparing the resulting 3D point-clouds from the LIO
algorithms to a ground truth map. The results indicate that the performance of
state-of-the-art LIO algorithms deteriorates due to the high dynamic movement
introduced by the spherical locomotion, leading to globally inconsistent maps
and sometimes unrecoverable drift.

</details>


### [125] [TwinTac: A Wide-Range, Highly Sensitive Tactile Sensor with Real-to-Sim Digital Twin Sensor Model](https://arxiv.org/abs/2509.10063)
*Xiyan Huang,Zhe Xu,Chenxi Xiao*

Main category: cs.RO

TL;DR: 本文提出了TwinTac系统，通过开发物理触觉传感器及其数字孪生模型，使机器人能够在强化学习过程中借助高质量触觉模拟数据提升技能获取。


<details>
  <summary>Details</summary>
Motivation: 目前强化学习常依赖仿真来高效生成交互数据，但缺乏触觉传感器的仿真模型，导致触觉感知数据难以用于策略学习，限制了触觉驱动的机器人技能发展。

Method: 设计了高灵敏、宽量程的物理触觉传感器，开发了其数字孪生，通过有限元仿真与真实传感器同步采集数据，并用神经网络将仿真数据映射为真实传感器响应，实现从仿真到现实的跨域学习模型。

Result: 实验证明数字孪生与物理传感器输出高度一致，且在物体分类任务中，通过数字孪生生成的模拟数据能够有效提升真实世界任务的准确率。

Conclusion: TwinTac有效弥补了触觉传感器在仿真中的空白，为机器人跨域技能学习提供了数据支持，在实际与仿真数据结合方面展现出巨大潜力。

Abstract: Robot skill acquisition processes driven by reinforcement learning often rely
on simulations to efficiently generate large-scale interaction data. However,
the absence of simulation models for tactile sensors has hindered the use of
tactile sensing in such skill learning processes, limiting the development of
effective policies driven by tactile perception. To bridge this gap, we present
TwinTac, a system that combines the design of a physical tactile sensor with
its digital twin model. Our hardware sensor is designed for high sensitivity
and a wide measurement range, enabling high quality sensing data essential for
object interaction tasks. Building upon the hardware sensor, we develop the
digital twin model using a real-to-sim approach. This involves collecting
synchronized cross-domain data, including finite element method results and the
physical sensor's outputs, and then training neural networks to map simulated
data to real sensor responses. Through experimental evaluation, we
characterized the sensitivity of the physical sensor and demonstrated the
consistency of the digital twin in replicating the physical sensor's output.
Furthermore, by conducting an object classification task, we showed that
simulation data generated by our digital twin sensor can effectively augment
real-world data, leading to improved accuracy. These results highlight
TwinTac's potential to bridge the gap in cross-domain learning tasks.

</details>


### [126] [Prespecified-Performance Kinematic Tracking Control for Aerial Manipulation](https://arxiv.org/abs/2509.10065)
*Hauzi Cao,Jiahao Shen,Zhengzhen Li,Qinquan Ren,Shiyu Zhao*

Main category: cs.RO

TL;DR: 本文针对空中机器人末端执行器的运动学跟踪控制问题，提出了一种能保证在预设时间内完成跟踪的新型控制框架，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的运动学跟踪方法（如PD控制、基于跟踪误差反馈等）无法保证末端执行器在设定时间内完成轨迹跟踪，难以满足有时间约束的实际任务需求。因此，亟需一种可以显式保证时间性能的跟踪控制方法。

Method: 提出了由两部分组成的控制框架：（1）基于用户预设轨迹的末端执行器跟踪控制，确保跟踪时间和误差满足任务要求；（2）基于二次规划的参考分配算法，同时考虑四旋翼平台和Delta机械臂的物理约束，合理分配跟踪参考，避免物理极限被突破。

Result: 通过三组实验对所提方法进行验证，实验结果表明该方法能有效保证末端执行器在预设时间内到达目标位置，并且跟踪误差始终被限制在性能包络内，物理约束均被满足。

Conclusion: 所提方法对比现有方法，在满足时间和性能约束方面具有明显优势，适用于有严格时间与性能需求的空中操作系统，为空中机器人运动学运动控制提供了有效的新思路。

Abstract: This paper studies the kinematic tracking control problem for aerial
manipulators. Existing kinematic tracking control methods, which typically
employ proportional-derivative feedback or tracking-error-based feedback
strategies, may fail to achieve tracking objectives within specified time
constraints. To address this limitation, we propose a novel control framework
comprising two key components: end-effector tracking control based on a
user-defined preset trajectory and quadratic programming-based reference
allocation. Compared with state-of-the-art approaches, the proposed method has
several attractive features. First, it ensures that the end-effector reaches
the desired position within a preset time while keeping the tracking error
within a performance envelope that reflects task requirements. Second,
quadratic programming is employed to allocate the references of the quadcopter
base and the Delta arm, while considering the physical constraints of the
aerial manipulator, thus preventing solutions that may violate physical
limitations. The proposed approach is validated through three experiments.
Experimental results demonstrate the effectiveness of the proposed algorithm
and its capability to guarantee that the target position is reached within the
preset time.

</details>


### [127] [HHI-Assist: A Dataset and Benchmark of Human-Human Interaction in Physical Assistance Scenario](https://arxiv.org/abs/2509.10096)
*Saeed Saadatnejad,Reyhaneh Hosseininejad,Jose Barreiros,Katherine M. Tsui,Alexandre Alahi*

Main category: cs.RO

TL;DR: 本论文提出了一个用于辅助机器人的新型人机交互动作预测方法，并发布了相关数据集和代码，显著提升了机器人在辅助任务中的动作预测能力。


<details>
  <summary>Details</summary>
Motivation: 随着劳动力短缺与人口老龄化问题加剧，对辅助机器人在照护任务中的需求日益增加。为了实现安全、灵活的辅助，机器人需能准确预测人在物理交互中的动作。然而，辅助场景多变和人机耦合动态复杂，使得动作预测成为一大挑战。

Method: 作者提出了两大技术贡献：（1）建立了HHI-Assist数据集，收集了丰富的辅助任务下人-人物理交互的动作捕捉数据；（2）基于条件变换器（Transformer）的去噪扩散模型，实现了对交互体（照护者与接受者）动作的高效预测。该方法能够捕捉双方耦合动态，比传统基线模型表现更佳，并在新场景中展示出优良的泛化能力。

Result: 提出的模型在复杂的物理交互场景下，相比传统方法在动作预测上有明显提升，并能有效泛化到之前未见过的情景。

Conclusion: 通过提供高质量数据集和创新的动作预测模型，论文推动了基于交互的动作理解与预测研究。相关成果为辅助机器人制定更智能的行动策略提供了支持，有望大幅提升机器人辅助照护实践的效率和安全性。

Abstract: The increasing labor shortage and aging population underline the need for
assistive robots to support human care recipients. To enable safe and
responsive assistance, robots require accurate human motion prediction in
physical interaction scenarios. However, this remains a challenging task due to
the variability of assistive settings and the complexity of coupled dynamics in
physical interactions. In this work, we address these challenges through two
key contributions: (1) HHI-Assist, a dataset comprising motion capture clips of
human-human interactions in assistive tasks; and (2) a conditional
Transformer-based denoising diffusion model for predicting the poses of
interacting agents. Our model effectively captures the coupled dynamics between
caregivers and care receivers, demonstrating improvements over baselines and
strong generalization to unseen scenarios. By advancing interaction-aware
motion prediction and introducing a new dataset, our work has the potential to
significantly enhance robotic assistance policies. The dataset and code are
available at: https://sites.google.com/view/hhi-assist/home

</details>


### [128] [Efficient Learning-Based Control of a Legged Robot in Lunar Gravity](https://arxiv.org/abs/2509.10128)
*Philip Arm,Oliver Fischer,Joseph Church,Adrian Fuhrer,Hendrik Kolvenbach,Marco Hutter*

Main category: cs.RO

TL;DR: 该论文提出了一种基于强化学习的四足机器人能耗优化控制方法，可跨不同重力环境（如月球、地球和类超地球）减少能耗，并在实际实验中验证了在月球重力下的有效性。


<details>
  <summary>Details</summary>
Motivation: 星球探索机器人的能耗和热量管理非常受限，而机器人的控制方法需要在不同重力环境下有效迁移并实现高能效，因此有必要研发能跨不同重力环境的低功耗控制方法。

Method: 采用强化学习方法，并提出了根据重力变化自适应缩放的能耗优化奖励函数，分别设计实现了步态控制器和姿态控制器，同时设计恒力弹簧卸载系统实现月球重力环境的实验验证。

Result: 所提控制器在地球重力下步态消耗23.4W，比基线方法能耗降低23%；在月球重力实验中步态消耗12.2W，比未优化方法降低36%。控制器能随重力环境变化有效自适应并减少能耗。

Conclusion: 该方法为开发跨多重力环境的四足机器人低功耗控制器提供了通用可扩展的解决方案，并在真实与模拟环境下均显示显著节能效果。

Abstract: Legged robots are promising candidates for exploring challenging areas on
low-gravity bodies such as the Moon, Mars, or asteroids, thanks to their
advanced mobility on unstructured terrain. However, as planetary robots' power
and thermal budgets are highly restricted, these robots need energy-efficient
control approaches that easily transfer to multiple gravity environments. In
this work, we introduce a reinforcement learning-based control approach for
legged robots with gravity-scaled power-optimized reward functions. We use our
approach to develop and validate a locomotion controller and a base pose
controller in gravity environments from lunar gravity (1.62 m/s2) to a
hypothetical super-Earth (19.62 m/s2). Our approach successfully scales across
these gravity levels for locomotion and base pose control with the
gravity-scaled reward functions. The power-optimized locomotion controller
reached a power consumption for locomotion of 23.4 W in Earth gravity on a
15.65 kg robot at 0.4 m/s, a 23 % improvement over the baseline policy.
Additionally, we designed a constant-force spring offload system that allowed
us to conduct real-world experiments on legged locomotion in lunar gravity. In
lunar gravity, the power-optimized control policy reached 12.2 W, 36 % less
than a baseline controller which is not optimized for power efficiency. Our
method provides a scalable approach to developing power-efficient locomotion
controllers for legged robots across multiple gravity levels.

</details>


### [129] [CaR1: A Multi-Modal Baseline for BEV Vehicle Segmentation via Camera-Radar Fusion](https://arxiv.org/abs/2509.10139)
*Santiago Montiel-Marín,Angel Llamazares,Miguel Antunes-García,Fabio Sánchez-García,Luis M. Bergasa*

Main category: cs.RO

TL;DR: 本论文提出了CaR1，一种用于鸟瞰图（BEV）车辆分割的新型摄像头与雷达融合架构，在nuScenes数据集上的表现接近当前最先进的方法。


<details>
  <summary>Details</summary>
Motivation: 摄像头虽然能够提供丰富的语义信息，但深度估计不可靠；雷达虽然点云稀疏，但位置和运动信息可靠。两者互补，有望成为成本低、鲁棒性强的自动驾驶感知方案，替代基于激光雷达的系统。

Method: 基于BEVFusion框架，提出了一种网格化雷达特征编码方法，将雷达点云离散为结构化的鸟瞰图特征，并设计了一种自适应融合机制，动态平衡不同传感器的贡献。

Result: 在nuScenes数据集上，车辆分割的IoU指标达到57.6，与最先进方法持平，表明该融合方法性能优越。

Conclusion: CaR1结合了摄像头和雷达的优势，实现了低成本、高鲁棒性的车辆分割性能，为自动驾驶提供了有竞争力的新方案。

Abstract: Camera-radar fusion offers a robust and cost-effective alternative to
LiDAR-based autonomous driving systems by combining complementary sensing
capabilities: cameras provide rich semantic cues but unreliable depth, while
radar delivers sparse yet reliable position and motion information. We
introduce CaR1, a novel camera-radar fusion architecture for BEV vehicle
segmentation. Built upon BEVFusion, our approach incorporates a grid-wise radar
encoding that discretizes point clouds into structured BEV features and an
adaptive fusion mechanism that dynamically balances sensor contributions.
Experiments on nuScenes demonstrate competitive segmentation performance (57.6
IoU), on par with state-of-the-art methods. Code is publicly available
\href{https://www.github.com/santimontiel/car1}{online}.

</details>


### [130] [DiffAero: A GPU-Accelerated Differentiable Simulation Framework for Efficient Quadrotor Policy Learning](https://arxiv.org/abs/2509.10247)
*Xinhong Zhang,Runqing Wang,Yunfan Ren,Jian Sun,Hao Fang,Jie Chen,Gang Wang*

Main category: cs.RO

TL;DR: 本文提出了DiffAero，一种轻量级、GPU加速、全可微分的四旋翼仿真系统，可大幅提升飞行控制策略的学习效率与速度。


<details>
  <summary>Details</summary>
Motivation: 当前四旋翼仿真平台在模拟效率、扩展性以及与深度学习、微分算法兼容性等方面存在瓶颈，特别是CPU-GPU间数据传输成为性能瓶颈，阻碍了高效的策略学习。

Method: DiffAero实现了基于GPU的物理和渲染全过程并行，无需频繁的数据传输，并集成多种动力学模型、定制化传感器（IMU、深度相机、LiDAR）及多样飞行任务，支持环境级和智能体级的并行，形成完整、统一且GPU原生的训练接口。

Result: DiffAero带来了数量级的仿真吞吐率提升，允许在消费级硬件上高效训练四旋翼控制策略。基准测试与实机飞行实验表明，它可在数小时内学习到鲁棒控制策略，并适用于各类可微分或混合智能算法研究。

Conclusion: DiffAero不仅具备高性能仿真能力，还拓展了四旋翼智能体策略学习与控制算法研究的平台边界，对相关领域具有重要的研究和应用价值。

Abstract: This letter introduces DiffAero, a lightweight, GPU-accelerated, and fully
differentiable simulation framework designed for efficient quadrotor control
policy learning. DiffAero supports both environment-level and agent-level
parallelism and integrates multiple dynamics models, customizable sensor stacks
(IMU, depth camera, and LiDAR), and diverse flight tasks within a unified,
GPU-native training interface. By fully parallelizing both physics and
rendering on the GPU, DiffAero eliminates CPU-GPU data transfer bottlenecks and
delivers orders-of-magnitude improvements in simulation throughput. In contrast
to existing simulators, DiffAero not only provides high-performance simulation
but also serves as a research platform for exploring differentiable and hybrid
learning algorithms. Extensive benchmarks and real-world flight experiments
demonstrate that DiffAero and hybrid learning algorithms combined can learn
robust flight policies in hours on consumer-grade hardware. The code is
available at https://github.com/flyingbitac/diffaero.

</details>


### [131] [GundamQ: Multi-Scale Spatio-Temporal Representation Learning for Robust Robot Path Planning](https://arxiv.org/abs/2509.10305)
*Yutong Shen,Ruizhe Xia,Bokai Yan,Shunqi zhang,Pengrui Xiang,Sicheng He,Yixin Xu*

Main category: cs.RO

TL;DR: 本文提出了一种面向机器人路径规划的多尺度时空Q网络GundamQ, 通过提升环境感知与决策能力，有效提升了动态环境下的路径规划表现。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习路径规划方法在动态不确定环境中存在多尺度时序建模不足和探索-利用效率低下的问题，这限制了其适应性与路径质量。

Method: 提出GundamQ框架，包括：(1)时空感知模块，分层提取多粒度空间特征与多尺度时序依赖，加强动态场景中的环境感知；(2)自适应策略优化模块，在平衡探索与利用的同时，通过约束策略更新来优化路径的平滑度与碰撞概率。

Result: 在动态环境实验中，GundamQ的成功率提升了15.3%，路径质量整体提升了21.7%，明显优于现有最先进方法。

Conclusion: GundamQ能更好地适应动态环境，有效提升机器人路径规划的表现，为相关应用带来显著改进。

Abstract: In dynamic and uncertain environments, robotic path planning demands accurate
spatiotemporal environment understanding combined with robust decision-making
under partial observability. However, current deep reinforcement learning-based
path planning methods face two fundamental limitations: (1) insufficient
modeling of multi-scale temporal dependencies, resulting in suboptimal
adaptability in dynamic scenarios, and (2) inefficient exploration-exploitation
balance, leading to degraded path quality. To address these challenges, we
propose GundamQ: A Multi-Scale Spatiotemporal Q-Network for Robotic Path
Planning. The framework comprises two key modules: (i) the Spatiotemporal
Perception module, which hierarchically extracts multi-granularity spatial
features and multi-scale temporal dependencies ranging from instantaneous to
extended time horizons, thereby improving perception accuracy in dynamic
environments; and (ii) the Adaptive Policy Optimization module, which balances
exploration and exploitation during training while optimizing for smoothness
and collision probability through constrained policy updates. Experiments in
dynamic environments demonstrate that GundamQ achieves a 15.3\% improvement in
success rate and a 21.7\% increase in overall path quality, significantly
outperforming existing state-of-the-art methods.

</details>


### [132] [Robot guide with multi-agent control and automatic scenario generation with LLM](https://arxiv.org/abs/2509.10317)
*Elizaveta D. Moskovskaya,Anton D. Moscowsky*

Main category: cs.RO

TL;DR: 本文提出了一种结合多智能体资源管理系统与大语言模型自动生成行为场景的混合控制架构，用于拟人型导览机器人，以提升其行为的自然性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有导览机器人行为场景通常需手动配置，导致灵活性和自然性不足，同时难以规模化和自动化。

Method: 采用两阶段生成流程：首先利用大语言模型生成风格化叙事文本，然后在文本中集成非语言动作标签；通过多智能体系统实现并行行为的协调和冲突解决。

Result: 通过试验结果表明，该方案能有效提升机器人行为的自动化与自然性，具备规模化潜力。

Conclusion: 提出的方法在社会机器人控制系统的自动化和扩展性方面表现优越，为提升机器人交互体验提供了有效解决思路。

Abstract: The work describes the development of a hybrid control architecture for an
anthropomorphic tour guide robot, combining a multi-agent resource management
system with automatic behavior scenario generation based on large language
models. The proposed approach aims to overcome the limitations of traditional
systems, which rely on manual tuning of behavior scenarios. These limitations
include manual configuration, low flexibility, and lack of naturalness in robot
behavior. The process of preparing tour scenarios is implemented through a
two-stage generation: first, a stylized narrative is created, then non-verbal
action tags are integrated into the text. The multi-agent system ensures
coordination and conflict resolution during the execution of parallel actions,
as well as maintaining default behavior after the completion of main
operations, contributing to more natural robot behavior. The results obtained
from the trial demonstrate the potential of the proposed approach for
automating and scaling social robot control systems.

</details>


### [133] [Acetrans: An Autonomous Corridor-Based and Efficient UAV Suspended Transport System](https://arxiv.org/abs/2509.10349)
*Weiyan Lu,Huizhe Li,Yuhao Fang,Zhexuan Zhou,Junda Wu,Yude Li,Youmin Gong,Jie Mei*

Main category: cs.RO

TL;DR: 本文提出了一种名为Acetrans的无人机悬挂运输系统，通过统一的感知、规划与控制方法，显著提升了复杂环境下无人机运输的安全性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有的无人机悬挂运输系统存在对缆绳-载荷动态感知不可靠、大规模环境规划低效、无法保证全身安全等问题。作者旨在解决这些在实际应用中的关键瓶颈。

Method: 提出了基于LiDAR-IMU融合的模块，实现对载荷姿态和缆绳形态的实时估计；创新性地开发了MACIRI算法以提升大尺度场景下的安全路径生成能力；引入时空联合的轨迹优化方案，针对动力学可行与安全性双重目标；最后采用加入缆绳弯曲约束的非线性模型预测控制（NMPC）保障整体执行过程安全稳健。

Result: 在仿真与实物实验中，Acetrans系统在感知精度、规划效率和控制安全性等关键指标上，相较于最新方法实现了显著提升。

Conclusion: Acetrans系统有效解决了无人机复杂环境悬挂运输中的核心挑战，有望推动其在实际物流与救援场景的广泛应用。

Abstract: Unmanned aerial vehicles (UAVs) with suspended payloads offer significant
advantages for aerial transportation in complex and cluttered environments.
However, existing systems face critical limitations, including unreliable
perception of the cable-payload dynamics, inefficient planning in large-scale
environments, and the inability to guarantee whole-body safety under cable
bending and external disturbances. This paper presents Acetrans, an Autonomous,
Corridor-based, and Efficient UAV suspended transport system that addresses
these challenges through a unified perception, planning, and control framework.
A LiDAR-IMU fusion module is proposed to jointly estimate both payload pose and
cable shape under taut and bent modes, enabling robust whole-body state
estimation and real-time filtering of cable point clouds. To enhance planning
scalability, we introduce the Multi-size-Aware Configuration-space Iterative
Regional Inflation (MACIRI) algorithm, which generates safe flight corridors
while accounting for varying UAV and payload geometries. A spatio-temporal,
corridor-constrained trajectory optimization scheme is then developed to ensure
dynamically feasible and collision-free trajectories. Finally, a nonlinear
model predictive controller (NMPC) augmented with cable-bending constraints
provides robust whole-body safety during execution. Simulation and experimental
results validate the effectiveness of Acetrans, demonstrating substantial
improvements in perception accuracy, planning efficiency, and control safety
compared to state-of-the-art methods.

</details>


### [134] [Self-supervised Learning Of Visual Pose Estimation Without Pose Labels By Classifying LED States](https://arxiv.org/abs/2509.10405)
*Nicholas Carlotti,Mirko Nava,Alessandro Giusti*

Main category: cs.RO

TL;DR: 提出了一种不依赖位姿标签或机器人外形先验，仅需LED灯状态信息和标定图片，即可在单目RGB图像下实现地面机器人相对位姿估计的方法。


<details>
  <summary>Details</summary>
Motivation: 传统单目相对位姿估计依赖于昂贵标注、外部基础设施或机器人的结构、外观先验，限制了实际应用，该工作试图通过弱化先验与标注要求，降低部署门槛。

Method: 训练时假设机器人带有多个LED，已知每个LED的状态和估算视向、以及一个带有已知目标距离的标定图片。收集两机器人在随机运动过程中的图像数据，通过图像预测LED状态作为训练任务，间接引导模型学会机器人在图像中的位置、距离和相对夹角。推理时LED状态未知也不影响位姿估计。

Result: 无需位姿标签和机器人外观先验即可获得与需要强监督或CAD模型的现有方法竞争的表现；能泛化到不同场景，并支持多机器人位姿估计。

Conclusion: 本方法大大降低了单目机器人位姿估计的训练与使用门槛，具备良好泛化性和实用前景。

Abstract: We introduce a model for monocular RGB relative pose estimation of a ground
robot that trains from scratch without pose labels nor prior knowledge about
the robot's shape or appearance. At training time, we assume: (i) a robot
fitted with multiple LEDs, whose states are independent and known at each
frame; (ii) knowledge of the approximate viewing direction of each LED; and
(iii) availability of a calibration image with a known target distance, to
address the ambiguity of monocular depth estimation. Training data is collected
by a pair of robots moving randomly without needing external infrastructure or
human supervision. Our model trains on the task of predicting from an image the
state of each LED on the robot. In doing so, it learns to predict the position
of the robot in the image, its distance, and its relative bearing. At inference
time, the state of the LEDs is unknown, can be arbitrary, and does not affect
the pose estimation performance. Quantitative experiments indicate that our
approach: is competitive with SoA approaches that require supervision from pose
labels or a CAD model of the robot; generalizes to different domains; and
handles multi-robot pose estimation.

</details>


### [135] [TASC: Task-Aware Shared Control for Teleoperated Manipulation](https://arxiv.org/abs/2509.10416)
*Ze Fu,Pinhao Song,Yutong Hu,Renaud Detry*

Main category: cs.RO

TL;DR: TASC 是一种面向任务感知的共享控制框架，能够在无需预定义知识的情况下，通过视觉输入推理用户意图并辅助机器人操控，实现对多样任务的泛化支持。


<details>
  <summary>Details</summary>
Motivation: 现有遥操作机械手的共享控制方案往往依赖于预定义的对象或任务知识，难以泛化到日常操作中的新任务和新对象。该论文旨在解决如何理解用户意图，以及如何将控制帮助泛化到更多不同对象与任务的挑战。

Method: TASC 通过视觉输入自动构建开放词汇交互图，表示对象之间的功能关系，并结合视觉-语言模型预测的空间约束推理用户的任务意图。在此基础上，提出了一种新的共享控制策略，能够在抓取和物体交互期间为用户在旋转控制上提供辅助，从而提升操作效率，降低用户输入负担。

Result: 在仿真和现实环境中的实验表明，TASC 相比已有方法能够提高任务效率，并显著减少用户操作输入。同时，TASC 能够在无需额外训练的情况下实现对新任务和新对象的零样本泛化，是第一个支持这类能力的共享控制框架。

Conclusion: TASC 框架实现了面向任务、开放词汇、可泛化的共享控制，推动了遥操作机器人在实际日常任务中的应用潜力。论文已开源相关代码。

Abstract: We present TASC, a Task-Aware Shared Control framework for teleoperated
manipulation that infers task-level user intent and provides assistance
throughout the task. To support everyday tasks without predefined knowledge,
TASC constructs an open-vocabulary interaction graph from visual input to
represent functional object relationships, and infers user intent accordingly.
A shared control policy then provides rotation assistance during both grasping
and object interaction, guided by spatial constraints predicted by a
vision-language model. Our method addresses two key challenges in
general-purpose, long-horizon shared control: (1) understanding and inferring
task-level user intent, and (2) generalizing assistance across diverse objects
and tasks. Experiments in both simulation and the real world demonstrate that
TASC improves task efficiency and reduces user input effort compared to prior
methods. To the best of our knowledge, this is the first shared control
framework that supports everyday manipulation tasks with zero-shot
generalization. The code that supports our experiments is publicly available at
https://github.com/fitz0401/tasc.

</details>


### [136] [DECAMP: Towards Scene-Consistent Multi-Agent Motion Prediction with Disentangled Context-Aware Pre-Training](https://arxiv.org/abs/2509.10426)
*Jianxin Shi,Zengqi Peng,Xiaolong Chen,Tianyu Wo,Jun Ma*

Main category: cs.RO

TL;DR: 该论文提出了一种用于多智能体轨迹预测的可分解上下文感知预训练框架DECAMP，在自动驾驶场景下，提升了多智能体运动预测的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法在多智能体预测和标注数据稀缺场景下表现不佳，受限于表示学习和任务目标的纠缠，难以获得可解释、高效的动态意图建模。为此，作者旨在探索一种新的预训练策略，提升多智能体场景下的预测性能。

Method: 所提DECAMP框架将行为模式学习与潜在特征重建解耦，并设计上下文感知的表示学习及协同空间-运动预训练任务，实现结构和意图的联合建模。与传统方法不同，该框架优先关注可解释的动态行为，并提升对场景的综合表达能力。

Result: 在Argoverse 2基准测试集上，DECAMP展现出优越的多智能体运动预测性能，明显优于现有方法，证明了所提框架在实际任务中的有效性。

Conclusion: DECAMP是首个应用于自动驾驶多智能体运动预测的上下文自编码框架，通过结构化的预训练流程，有效提升了预测性能，未来代码和模型将会开源。

Abstract: Trajectory prediction is a critical component of autonomous driving,
essential for ensuring both safety and efficiency on the road. However,
traditional approaches often struggle with the scarcity of labeled data and
exhibit suboptimal performance in multi-agent prediction scenarios. To address
these challenges, we introduce a disentangled context-aware pre-training
framework for multi-agent motion prediction, named DECAMP. Unlike existing
methods that entangle representation learning with pretext tasks, our framework
decouples behavior pattern learning from latent feature reconstruction,
prioritizing interpretable dynamics and thereby enhancing scene representation
for downstream prediction. Additionally, our framework incorporates
context-aware representation learning alongside collaborative spatial-motion
pretext tasks, which enables joint optimization of structural and intentional
reasoning while capturing the underlying dynamic intentions. Our experiments on
the Argoverse 2 benchmark showcase the superior performance of our method, and
the results attained underscore its effectiveness in multi-agent motion
forecasting. To the best of our knowledge, this is the first context
autoencoder framework for multi-agent motion forecasting in autonomous driving.
The code and models will be made publicly available.

</details>


### [137] [Coordinated Motion Planning of a Wearable Multi-Limb System for Enhanced Human-Robot Interaction](https://arxiv.org/abs/2509.10444)
*Chaerim Moon,Joohyung Kim*

Main category: cs.RO

TL;DR: 本论文提出了一种新型的运动规划方法，用以减少可穿戴型超数量机器人手臂（SRLs）在操作过程中对人体产生的外部力矩，从而提升人机交互舒适性。


<details>
  <summary>Details</summary>
Motivation: 随着超数量机器人手臂（SRLs）增强人体能力的应用增多，其工作时对人体产生的外部扭矩会增加人体的肌肉负荷，从而引发舒适性、安全性等一系列问题。因此，研究如何降低SRLs对人体的负担具有重要意义。

Method: 本文提出了一种运动规划层，能够通过优化轨迹，在限定角加速度与位置偏差的条件下，修改原始运动轨迹，从而降低操作过程中产生的力矩。作者在仿真环境下，结合简化的人体与机械臂系统模型，对所提出方法进行了性能验证。

Result: 仿真结果表明，该运动规划方法能在保证轨迹完成的基础上，有效减少SRLs对人体施加的外部力矩。

Conclusion: 引入运动规划层以约束角加速度和位置偏差能够提升SRLs的人机交互体验，为可穿戴机器人设计提供了新的思路。

Abstract: Supernumerary Robotic Limbs (SRLs) can enhance human capability within close
proximity. However, as a wearable device, the generated moment from its
operation acts on the human body as an external torque. When the moments
increase, more muscle units are activated for balancing, and it can result in
reduced muscular null space. Therefore, this paper suggests a concept of a
motion planning layer that reduces the generated moment for enhanced
Human-Robot Interaction. It modifies given trajectories with desirable angular
acceleration and position deviation limits. Its performance to reduce the
moment is demonstrated through the simulation, which uses simplified human and
robotic system models.

</details>


### [138] [GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation](https://arxiv.org/abs/2509.10454)
*Hang Yin,Haoyu Wei,Xiuwei Xu,Wenxuan Guo,Jie Zhou,Jiwen Lu*

Main category: cs.RO

TL;DR: 本文提出了一种不需要训练的视觉-语言导航（VLN）框架，通过将指令分解为空间约束并通过约束优化确定机器人路径，实现了更强的泛化能力和导航效率。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本VLN方法主要针对离散环境或涉及无监督训练，不易泛化到真实场景。为了解决这一问题，作者希望提出一个可直接应用于连续环境并且无需训练的通用导航框架。

Method: 作者将导航任务建模为图约束优化，将人类文本指令分解为空间约束组成的有向无环图。通过构建空间约束库和利用约束求解器，确定机器人的路径和目标点，并针对无解或多解的情况引入导航树和回溯机制。

Result: 在标准VLN基准测试上，该方法明显提升了成功率和导航效率，相比最新零样本方法效果更好。实验证明该框架能有效泛化到新的环境和指令集。

Conclusion: 该训练自由的VLN框架不但提升了零样本导航的效率和成功率，而且能够很好泛化到实际新场景，为更稳健和自主的导航系统提供了新思路。

Abstract: In this paper, we propose a training-free framework for vision-and-language
navigation (VLN). Existing zero-shot VLN methods are mainly designed for
discrete environments or involve unsupervised training in continuous simulator
environments, which makes it challenging to generalize and deploy them in
real-world scenarios. To achieve a training-free framework in continuous
environments, our framework formulates navigation guidance as graph constraint
optimization by decomposing instructions into explicit spatial constraints. The
constraint-driven paradigm decodes spatial semantics through constraint
solving, enabling zero-shot adaptation to unseen environments. Specifically, we
construct a spatial constraint library covering all types of spatial
relationship mentioned in VLN instructions. The human instruction is decomposed
into a directed acyclic graph, with waypoint nodes, object nodes and edges,
which are used as queries to retrieve the library to build the graph
constraints. The graph constraint optimization is solved by the constraint
solver to determine the positions of waypoints, obtaining the robot's
navigation path and final goal. To handle cases of no solution or multiple
solutions, we construct a navigation tree and the backtracking mechanism.
Extensive experiments on standard benchmarks demonstrate significant
improvements in success rate and navigation efficiency compared to
state-of-the-art zero-shot VLN methods. We further conduct real-world
experiments to show that our framework can effectively generalize to new
environments and instruction sets, paving the way for a more robust and
autonomous navigation framework.

</details>
