<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 98]
- [cs.CL](#cs.CL) [Total: 47]
- [cs.RO](#cs.RO) [Total: 23]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization](https://arxiv.org/abs/2508.20181)
*Alberto Compagnoni,Davide Caffagni,Nicholas Moratelli,Lorenzo Baraldi,Marcella Cornia,Rita Cucchiara*

Main category: cs.CV

TL;DR: 本文提出CHAIR-DPO方法，通过利用CHAIR指标和Direct Preference Optimization，减少多模态大模型生成幻觉内容的问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型（MLLMs）虽然在多任务和多领域取得了优秀表现，但在视觉任务中经常产生“幻觉”（即生成与图像内容无关的内容），影响实用性。为解决这一长期存在的问题，作者想提升模型对齐能力，使其更倾向于生成符合视觉输入的答案。

Method: 与以往依赖复杂且需合成偏好数据或专有模型的方法不同，作者采用CHAIR指标（本用于描述图像描述任务中的幻觉情况），比较模型输出答案对的“优劣”（区分无幻觉与有幻觉的输出），并用Direct Preference Optimization（DPO）微调现有MLLM，实现奖励驱动的微调过程。此方法命名为CHAIR-DPO。

Result: CHAIR-DPO在多个幻觉基准测试中显著减少了幻觉内容的生成，证明基于CHAIR奖励的微调方法有效。

Conclusion: 基于CHAIR的DPO微调方法，可以有效降低多模态大模型的幻觉生成问题。论文公开了源码和模型，推动领域发展。

Abstract: Multimodal Large Language Models (MLLMs) emerge as a unified interface to
address a multitude of tasks, ranging from NLP to computer vision. Despite
showcasing state-of-the-art results in many benchmarks, a long-standing issue
is the tendency of MLLMs to hallucinate, that is to generate answers to the
user's query that are not reflected in the visual input. In this paper, we
address the problem of hallucinations as an alignment problem, seeking to steer
the MLLM so that it prefers generating content without hallucinations. In
contrast to recent approaches that require complicated pipelines to build
synthetic preference data for alignment training, often relying on proprietary
models, we capitalize on the well-known CHAIR metric, originally proposed to
gauge the degree of hallucinations in image captioning. Given a pair of
generated answers, we leverage CHAIR to distinguish winner and loser options
(i.e., non-hallucinated and hallucinated samples) and fine-tune off-the-shelf
MLLMs via Direct Preference Optimization (DPO). The resulting method, which we
refer to as CHAIR-DPO, effectively diminishes the amount of hallucinated
answers on several hallucination benchmarks, demonstrating the effectiveness of
fine-tuning the MLLM with a CHAIR-based reward. Source code and trained models
are publicly available at https://github.com/aimagelab/CHAIR-DPO.

</details>


### [2] [SDiFL: Stable Diffusion-Driven Framework for Image Forgery Localization](https://arxiv.org/abs/2508.20182)
*Yang Su,Shunquan Tan,Jiwu Huang*

Main category: cs.CV

TL;DR: 本文利用Stable Diffusion（SD）等多模态大模型，构建更高效且准确的图像伪造定位系统，相较于现有方法提升了性能，且无需大量昂贵标注。


<details>
  <summary>Details</summary>
Motivation: 当前先进的多模态大模型推动图像伪造技术迅速发展，现有依赖人工标注的图像伪造定位方法难以应对新型伪造，因此亟需更高效、适应性强的伪造检测手段。

Method: 作者首次将Stable Diffusion的大模型生成与感知能力结合进图像取证，提出一种新框架：将高通滤波提取的图像伪造残差作为特殊模态，融合进SD3的潜在空间进行训练，从而加强对伪造区域的感知，并保留原图丰富的语义信息。

Result: 该方法在多个主流数据集上对比现有最优模型，伪造定位性能提升高达12%。此外，对于真实文档伪造与自然场景伪造等未见过的数据也表现出色。

Conclusion: 所提多模态SD3框架显著提高了图像伪造定位准确率，具有良好的泛化能力和实用价值。

Abstract: Driven by the new generation of multi-modal large models, such as Stable
Diffusion (SD), image manipulation technologies have advanced rapidly, posing
significant challenges to image forensics. However, existing image forgery
localization methods, which heavily rely on labor-intensive and costly
annotated data, are struggling to keep pace with these emerging image
manipulation technologies. To address these challenges, we are the first to
integrate both image generation and powerful perceptual capabilities of SD into
an image forensic framework, enabling more efficient and accurate forgery
localization. First, we theoretically show that the multi-modal architecture of
SD can be conditioned on forgery-related information, enabling the model to
inherently output forgery localization results. Then, building on this
foundation, we specifically leverage the multimodal framework of Stable
DiffusionV3 (SD3) to enhance forgery localization performance.We leverage the
multi-modal processing capabilities of SD3 in the latent space by treating
image forgery residuals -- high-frequency signals extracted using specific
highpass filters -- as an explicit modality. This modality is fused into the
latent space during training to enhance forgery localization performance.
Notably, our method fully preserves the latent features extracted by SD3,
thereby retaining the rich semantic information of the input image.
Experimental results show that our framework achieves up to 12% improvements in
performance on widely used benchmarking datasets compared to current
state-of-the-art image forgery localization models. Encouragingly, the model
demonstrates strong performance on forensic tasks involving real-world document
forgery images and natural scene forging images, even when such data were
entirely unseen during training.

</details>


### [3] [Grounding Multimodal Large Language Models with Quantitative Skin Attributes: A Retrieval Study](https://arxiv.org/abs/2508.20188)
*Max Torop,Masih Eskandar,Nicholas Kurtansky,Jinyang Liu,Jochen Weber,Octavia Camps,Veronica Rotemberg,Jennifer Dy,Kivanc Kose*

Main category: cs.CV

TL;DR: 本文提出通过结合多模态大语言模型（MLLMs）和定量属性分析提升皮肤病AI诊断的可解释性，并在SLICE-3D数据集上通过属性检索任务验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 目前AI模型虽能有效检测皮肤疾病，但由于缺乏可解释性，难以实际应用。提高模型预测的可解释性，是模型落地的关键。

Method: 作者结合了多模态大语言模型（MLLMs）以及与皮损外观相关的定量属性（如病变面积）评价模型。通过微调MLLM，使模型能从图像预测这些属性，并通过基于内容的图像检索（以属性为基础）进行评估。

Result: 实验表明，MLLM的嵌入空间可以通过微调有效关联到具体的定量属性，且这一方法在SLICE-3D数据集的属性检索任务中表现良好。

Conclusion: 将MLLM与定量属性结合可显著提高皮肤疾病AI诊断模型的可解释性，为其临床应用提供了有力支撑。

Abstract: Artificial Intelligence models have demonstrated significant success in
diagnosing skin diseases, including cancer, showing the potential to assist
clinicians in their analysis. However, the interpretability of model
predictions must be significantly improved before they can be used in practice.
To this end, we explore the combination of two promising approaches: Multimodal
Large Language Models (MLLMs) and quantitative attribute usage. MLLMs offer a
potential avenue for increased interpretability, providing reasoning for
diagnosis in natural language through an interactive format. Separately, a
number of quantitative attributes that are related to lesion appearance (e.g.,
lesion area) have recently been found predictive of malignancy with high
accuracy. Predictions grounded as a function of such concepts have the
potential for improved interpretability. We provide evidence that MLLM
embedding spaces can be grounded in such attributes, through fine-tuning to
predict their values from images. Concretely, we evaluate this grounding in the
embedding space through an attribute-specific content-based image retrieval
case study using the SLICE-3D dataset.

</details>


### [4] [Enhancing Automatic Modulation Recognition With a Reconstruction-Driven Vision Transformer Under Limited Labels](https://arxiv.org/abs/2508.20193)
*Hossein Ahmadi,Banafsheh Saffari*

Main category: cs.CV

TL;DR: 本文提出了一种结合监督、自监督和重构目标的Vision Transformer（ViT）统一框架，实现了高效和泛化性强的自动调制识别（AMR），在低标注样本条件下性能优越。


<details>
  <summary>Details</summary>
Motivation: 自动调制识别在认知无线电、频谱监测和安全通信等场景非常关键，但现有方法依赖大量标注数据或复杂多阶段训练，实际应用中难以扩展和泛化。为了解决这些局限性，作者希望构建一个通用、高效且对标注需求较低的AMR方案。

Method: 作者设计了基于ViT的统一模型，包括ViT编码器、卷积解码器和线性分类器。模型将信号数据通过增强后进行重构学习，同时引入部分标签进行微调，通过端到端联合优化，自监督分支帮助模型掌握精细的IQ信号结构，提升表征能力。

Result: 在RML2018.01A数据集上，所提方法在低标注（仅15-20%标签）情况下，准确率接近ResNet，且在不同信噪比下依然保持了优异表现，显著优于监督CNN和ViT基线。

Conclusion: 该框架简单、通用且对标注数据依赖低，为自动调制识别提供了一种有效的解决方案，有望推动其实际部署和应用。

Abstract: Automatic modulation recognition (AMR) is critical for cognitive radio,
spectrum monitoring, and secure wireless communication. However, existing
solutions often rely on large labeled datasets or multi-stage training
pipelines, which limit scalability and generalization in practice. We propose a
unified Vision Transformer (ViT) framework that integrates supervised,
self-supervised, and reconstruction objectives. The model combines a ViT
encoder, a lightweight convolutional decoder, and a linear classifier; the
reconstruction branch maps augmented signals back to their originals, anchoring
the encoder to fine-grained I/Q structure. This strategy promotes robust,
discriminative feature learning during pretraining, while partial label
supervision in fine-tuning enables effective classification with limited
labels. On the RML2018.01A dataset, our approach outperforms supervised CNN and
ViT baselines in low-label regimes, approaches ResNet-level accuracy with only
15-20% labeled data, and maintains strong performance across varying SNR
levels. Overall, the framework provides a simple, generalizable, and
label-efficient solution for AMR.

</details>


### [5] [InfinityHuman: Towards Long-Term Audio-Driven Human](https://arxiv.org/abs/2508.20210)
*Xiaodi Li,Pan Xie,Yi Ren,Qijun Gan,Chen Zhang,Fangyuan Kong,Xiang Yin,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种新的音频驱动的人体动画方法InfinityHuman，实现了高分辨率、长时长、外观一致且手部动作自然的视频生成，在手部动作和身份保持方面刷新了SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动人体动画方法在高分辨率、长时长视频生成中，容易出现身份漂移、色彩变化、场景不稳定和手部动作失真等问题，难以同时保证外观一致性和自然手势。

Method: 提出了一个粗到细的生成框架：首先生成与音频同步的中间表示，再用姿态引导的Refiner逐步细化成高分辨率视频。关键点在于姿态序列被解耦于外观，Refiner利用稳定的姿态和初始帧作为视觉锚点减少漂移，改进口型同步。此外，引入手部奖励机制，利用高质量手部动作数据提升手势真实感和语义准确性。

Result: 在EMTD和HDTF数据集上，InfinityHuman在视频质量、身份保持、手部动作准确率和口型同步等指标上均达到或超过最新水平。消融实验进一步验证了各模块的有效性。

Conclusion: InfinityHuman能够显著提升音频驱动人体动画生成的视频质量、身份一致性与手部动作自然度，是当前领域的先进方法。代码即将开源。

Abstract: Audio-driven human animation has attracted wide attention thanks to its
practical applications. However, critical challenges remain in generating
high-resolution, long-duration videos with consistent appearance and natural
hand motions. Existing methods extend videos using overlapping motion frames
but suffer from error accumulation, leading to identity drift, color shifts,
and scene instability. Additionally, hand movements are poorly modeled,
resulting in noticeable distortions and misalignment with the audio. In this
work, we propose InfinityHuman, a coarse-to-fine framework that first generates
audio-synchronized representations, then progressively refines them into
high-resolution, long-duration videos using a pose-guided refiner. Since pose
sequences are decoupled from appearance and resist temporal degradation, our
pose-guided refiner employs stable poses and the initial frame as a visual
anchor to reduce drift and improve lip synchronization. Moreover, to enhance
semantic accuracy and gesture realism, we introduce a hand-specific reward
mechanism trained with high-quality hand motion data. Experiments on the EMTD
and HDTF datasets show that InfinityHuman achieves state-of-the-art performance
in video quality, identity preservation, hand accuracy, and lip-sync. Ablation
studies further confirm the effectiveness of each module. Code will be made
public.

</details>


### [6] [Spherical Vision Transformers for Audio-Visual Saliency Prediction in 360-Degree Videos](https://arxiv.org/abs/2508.20221)
*Mert Cokelek,Halit Ozsoy,Nevrez Imamoglu,Cagri Ozcinar,Inci Ayhan,Erkut Erdem,Aykut Erdem*

Main category: cs.CV

TL;DR: 提出了适用于360度全景视频的视觉显著性预测新模型，综合考虑了音频和视频信息，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前360度全景视频在虚拟现实中广泛应用，加入空间音频后极大提升了沉浸体验，但在音视频综合下进行显著性预测的数据集和有效模型仍较为稀缺。

Method: 1. 构建了新的YT360-EyeTracking数据集，涵盖81个带空间音频的全景视频及多种观察条件。2. 提出基于vision transformer的SalViT360模型，专门设计了适应球面几何的时空注意力机制。3. 在SalViT360基础上，进一步引入结合音频特征的transformer adapter，形成SalViT360-AV模型，实现音视频融合显著性预测。4. 在多个基准数据集上评估模型性能。

Result: SalViT360与SalViT360-AV在所提出和已有数据集上均显著优于目前主流方法，尤其在融合空间音频信息后，预测精度有明显提升。

Conclusion: 360度全景视频的显著性预测需充分结合视觉与空间音频信息，新提出的模型在该领域达到了最优效果，同时为后续相关研究提供了数据集和开源代码。

Abstract: Omnidirectional videos (ODVs) are redefining viewer experiences in virtual
reality (VR) by offering an unprecedented full field-of-view (FOV). This study
extends the domain of saliency prediction to 360-degree environments,
addressing the complexities of spherical distortion and the integration of
spatial audio. Contextually, ODVs have transformed user experience by adding a
spatial audio dimension that aligns sound direction with the viewer's
perspective in spherical scenes. Motivated by the lack of comprehensive
datasets for 360-degree audio-visual saliency prediction, our study curates
YT360-EyeTracking, a new dataset of 81 ODVs, each observed under varying
audio-visual conditions. Our goal is to explore how to utilize audio-visual
cues to effectively predict visual saliency in 360-degree videos. Towards this
aim, we propose two novel saliency prediction models: SalViT360, a
vision-transformer-based framework for ODVs equipped with spherical
geometry-aware spatio-temporal attention layers, and SalViT360-AV, which
further incorporates transformer adapters conditioned on audio input. Our
results on a number of benchmark datasets, including our YT360-EyeTracking,
demonstrate that SalViT360 and SalViT360-AV significantly outperform existing
methods in predicting viewer attention in 360-degree scenes. Interpreting these
results, we suggest that integrating spatial audio cues in the model
architecture is crucial for accurate saliency prediction in omnidirectional
videos. Code and dataset will be available at
https://cyberiada.github.io/SalViT360.

</details>


### [7] [A Novel Framework for Automated Explain Vision Model Using Vision-Language Models](https://arxiv.org/abs/2508.20227)
*Phu-Vinh Nguyen,Tan-Hanh Pham,Chris Ngo,Truong Son Hy*

Main category: cs.CV

TL;DR: 本文提出了一种结合样本级和数据集级解释的视觉模型可解释性分析流程，利用视觉-语言模型，帮助开发者高效理解和改进视觉模型。


<details>
  <summary>Details</summary>
Motivation: 当前视觉模型的研究多关注于提升性能指标如准确率、IoU和mAP，但对于模型可解释性关注较少，主要原因是xAI方法应用复杂且多只解释单个样本，缺乏对模型整体行为的分析。理解模型在整个数据集上的表现对于避免偏见和发现模型趋势至关重要。

Method: 本文提出基于视觉-语言模型的分析流程，能够在样本级和数据集级解释视觉模型行为。该流程可低门槛发现模型的失败案例，并提炼模型的行为模式和趋势。

Result: 该流程能够辅助开发者快速发现和理解视觉模型的失误与特点，同时帮助将可解释性分析与视觉模型开发深度结合。

Conclusion: 通过所提流程，视觉模型开发者能够更加系统性地进行xAI分析，提升模型透明度和可控性，从而促进图像分析领域的发展。

Abstract: The development of many vision models mainly focuses on improving their
performance using metrics such as accuracy, IoU, and mAP, with less attention
to explainability due to the complexity of applying xAI methods to provide a
meaningful explanation of trained models. Although many existing xAI methods
aim to explain vision models sample-by-sample, methods explaining the general
behavior of vision models, which can only be captured after running on a large
dataset, are still underexplored. Furthermore, understanding the behavior of
vision models on general images can be very important to prevent biased
judgments and help identify the model's trends and patterns. With the
application of Vision-Language Models, this paper proposes a pipeline to
explain vision models at both the sample and dataset levels. The proposed
pipeline can be used to discover failure cases and gain insights into vision
models with minimal effort, thereby integrating vision model development with
xAI analysis to advance image analysis.

</details>


### [8] [ATMS-KD: Adaptive Temperature and Mixed Sample Knowledge Distillation for a Lightweight Residual CNN in Agricultural Embedded Systems](https://arxiv.org/abs/2508.20232)
*Mohamed Ohamouddou,Said Ohamouddou,Abdellatif El Afia,Rafik Lasri*

Main category: cs.CV

TL;DR: 该论文提出了ATMS-KD（一种融合自适应温度与混合样本的数据增强的知识蒸馏框架），旨在为农业场景下开发高效轻量级CNN模型。新方法在玫瑰花图片成熟度分类任务中显著提升小模型精度，超过多数已有方法。


<details>
  <summary>Details</summary>
Motivation: 资源受限的农业环境需要推理快速且参数量小的CNN模型，在保证高识别率同时降低计算成本。作者发现现有知识蒸馏等方案存在精度与轻量化模型迁移困难，因此提出新蒸馏框架以提升效果。

Method: 将MobileNetV3 Large作为教师模型，将三种参数规模的残差学生模型（1.3M, 2.4M, 3.8M参数）作为学生，融合自适应温度调节机制（AT）与混合样本蒸馏（MS），在摩洛哥玫瑰田采集的实际农田植物数据集上进行迁移和训练。与直接训练和11种蒸馏方法比较。

Result: 在Damascena玫瑰成熟度分类任务中，所有学生模型通过ATMS-KD蒸馏后验证准确率均超过96.7%，比直接训练提升1%左右。最小的学生模型在精度上比排名第二的蒸馏方法高1.6个百分点（97.11% vs 95.51%），同时具有最低推理时延。知识保留率均超99%。

Conclusion: 所提ATMS-KD框架能高效提升轻量级CNN模型在农业场景下的性能与知识迁移效果，方法优于现有主流蒸馏技术，并非常适合资源受限环境实际部署。

Abstract: This study proposes ATMS-KD (Adaptive Temperature and Mixed-Sample Knowledge
Distillation), a novel framework for developing lightweight CNN models suitable
for resource-constrained agricultural environments. The framework combines
adaptive temperature scheduling with mixed-sample augmentation to transfer
knowledge from a MobileNetV3 Large teacher model (5.7\,M parameters) to
lightweight residual CNN students. Three student configurations were evaluated:
Compact (1.3\,M parameters), Standard (2.4\,M parameters), and Enhanced (3.8\,M
parameters). The dataset used in this study consists of images of \textit{Rosa
damascena} (Damask rose) collected from agricultural fields in the Dades Oasis,
southeastern Morocco, providing a realistic benchmark for agricultural computer
vision applications under diverse environmental conditions. Experimental
evaluation on the Damascena rose maturity classification dataset demonstrated
significant improvements over direct training methods. All student models
achieved validation accuracies exceeding 96.7\% with ATMS-KD compared to
95--96\% with direct training. The framework outperformed eleven established
knowledge distillation methods, achieving 97.11\% accuracy with the compact
model -- a 1.60 percentage point improvement over the second-best approach
while maintaining the lowest inference latency of 72.19\,ms. Knowledge
retention rates exceeded 99\% for all configurations, demonstrating effective
knowledge transfer regardless of student model capacity.

</details>


### [9] [Linking heterogeneous microstructure informatics with expert characterization knowledge through customized and hybrid vision-language representations for industrial qualification](https://arxiv.org/abs/2508.20243)
*Mutahar Safdar,Gentry Wood,Max Zimmermann,Guy Lamouche,Priti Wanjara,Yaoyao Fiona Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种结合微观结构信息学与专家知识的新型视觉-语言表征框架，实现对增材制造复杂材料的高效零样本合格判定。


<details>
  <summary>Details</summary>
Motivation: 当前先进材料（特别是通过非传统增材制造工艺生产的异质性结构）在工业制造中的快速、可重复性检测仍然存在瓶颈，传统方法难以满足具有代表性和泛化性的要求。

Method: 作者集成了深度语义分割与预训练多模态模型（如CLIP和FLAVA），将微观结构图像和专家文本评估统一编码到共享表征空间，并定制了融合正负专家示例图像及其描述的相似性判定方法，实现零样本（即未见过的新结构）分类。同时采用Z值归一化，对单模和跨模态的相似度分数进行数据集驱动调整，提升对合格/不合格样本的区分效果。

Result: 在增材制造金属基复合材料样本测试下，框架有效区分了合格与缺陷样本。对比发现FLAVA模型在视觉敏感性优于CLIP模型，后者在文本标准一致性方面表现更优。

Conclusion: 该方法实现了无需任务专属模型重训练的人机协同决策，提高了检测流程的可追溯性与可解释性，推动了工程信息学领域基于数据和知识语义互操作的高效材料资格判定体系的发展。

Abstract: Rapid and reliable qualification of advanced materials remains a bottleneck
in industrial manufacturing, particularly for heterogeneous structures produced
via non-conventional additive manufacturing processes. This study introduces a
novel framework that links microstructure informatics with a range of expert
characterization knowledge using customized and hybrid vision-language
representations (VLRs). By integrating deep semantic segmentation with
pre-trained multi-modal models (CLIP and FLAVA), we encode both visual
microstructural data and textual expert assessments into shared
representations. To overcome limitations in general-purpose embeddings, we
develop a customized similarity-based representation that incorporates both
positive and negative references from expert-annotated images and their
associated textual descriptions. This allows zero-shot classification of
previously unseen microstructures through a net similarity scoring approach.
Validation on an additively manufactured metal matrix composite dataset
demonstrates the framework's ability to distinguish between acceptable and
defective samples across a range of characterization criteria. Comparative
analysis reveals that FLAVA model offers higher visual sensitivity, while the
CLIP model provides consistent alignment with the textual criteria. Z-score
normalization adjusts raw unimodal and cross-modal similarity scores based on
their local dataset-driven distributions, enabling more effective alignment and
classification in the hybrid vision-language framework. The proposed method
enhances traceability and interpretability in qualification pipelines by
enabling human-in-the-loop decision-making without task-specific model
retraining. By advancing semantic interoperability between raw data and expert
knowledge, this work contributes toward scalable and domain-adaptable
qualification strategies in engineering informatics.

</details>


### [10] [MedNet-PVS: A MedNeXt-Based Deep Learning Model for Automated Segmentation of Perivascular Spaces](https://arxiv.org/abs/2508.20256)
*Zhen Xuen Brandon Low,Rory Zhang,Hang Min,William Pham,Lucy Vivash,Jasmine Moses,Miranda Lynch,Karina Dorfman,Cassandra Marotta,Shaun Koh,Jacob Bunyamin,Ella Rowsthorn,Alex Jarema,Himashi Peiris,Zhaolin Chen,Sandy R. Shultz,David K. Wright,Dexiao Kong,Sharon L. Naismith,Terence J. O'Brien,Ying Xia,Meng Law,Benjamin Sinclair*

Main category: cs.CV

TL;DR: 该论文提出采用MedNeXt-L-k5深度学习网络实现脑部MR图像中扩大的脑周围间隙（PVS）自动分割，在T2加权数据上达到极高的Dice分数，但在异质多源T1加权数据上的泛化能力有限。与主流nnU-Net模型比，未体现出显著优势。


<details>
  <summary>Details</summary>
Motivation: PVS被认为是脑小血管病、阿尔茨海默症、中风和老年性神经退行性变的重要影像生物标志。然而，手动分割耗时且可重复性一般，现有自动深度学习方法泛化能力不足。因此，需开发更高效、泛化性更好的自动分割模型。

Method: 采用MedNeXt-L-k5（融合Transformer机制的3D编码-解码卷积网络）进行PVS自动分割。训练了两个模型，分别用同质的T2w（200例，HCP数据）和异质的T1w（40例，7个不同研究/6台扫描仪数据）。模型性能通过5折交叉验证及留一中心验证进行评估，并与nnU-Net模型对比。

Result: 在T2w的HCP-Aging数据中，MedNeXt-L-k5模型白质Dice分数高达0.88±0.06, 达到人类标注一致性上限并创文献新高；T1w上性能明显下降（Dice=0.58±0.09）；异质样本LOSOCV时，Dice分数进一步降低（白质0.38±0.16，基底节0.35±0.12）。与nnU-Net相比无显著精度优势。

Conclusion: MedNeXt-L-k5可高效自动分割同质MRI数据中的PVS，在多源异构数据上的泛化能力受限。用于PVS分割时，Transformer相关全局注意力机制未显示出优于传统UNet架构的优势。

Abstract: Enlarged perivascular spaces (PVS) are increasingly recognized as biomarkers
of cerebral small vessel disease, Alzheimer's disease, stroke, and
aging-related neurodegeneration. However, manual segmentation of PVS is
time-consuming and subject to moderate inter-rater reliability, while existing
automated deep learning models have moderate performance and typically fail to
generalize across diverse clinical and research MRI datasets. We adapted
MedNeXt-L-k5, a Transformer-inspired 3D encoder-decoder convolutional network,
for automated PVS segmentation. Two models were trained: one using a
homogeneous dataset of 200 T2-weighted (T2w) MRI scans from the Human
Connectome Project-Aging (HCP-Aging) dataset and another using 40 heterogeneous
T1-weighted (T1w) MRI volumes from seven studies across six scanners. Model
performance was evaluated using internal 5-fold cross validation (5FCV) and
leave-one-site-out cross validation (LOSOCV). MedNeXt-L-k5 models trained on
the T2w images of the HCP-Aging dataset achieved voxel-level Dice scores of
0.88+/-0.06 (white matter, WM), comparable to the reported inter-rater
reliability of that dataset, and the highest yet reported in the literature.
The same models trained on the T1w images of the HCP-Aging dataset achieved a
substantially lower Dice score of 0.58+/-0.09 (WM). Under LOSOCV, the model had
voxel-level Dice scores of 0.38+/-0.16 (WM) and 0.35+/-0.12 (BG), and
cluster-level Dice scores of 0.61+/-0.19 (WM) and 0.62+/-0.21 (BG).
MedNeXt-L-k5 provides an efficient solution for automated PVS segmentation
across diverse T1w and T2w MRI datasets. MedNeXt-L-k5 did not outperform the
nnU-Net, indicating that the attention-based mechanisms present in
transformer-inspired models to provide global context are not required for high
accuracy in PVS segmentation.

</details>


### [11] [Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation](https://arxiv.org/abs/2508.20265)
*Zhixiang Chi,Yanan Wu,Li Gu,Huan Liu,Ziqiang Wang,Yang Zhang,Yang Wang,Konstantinos N. Plataniotis*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练、基于反馈的自适应框架，通过引入输出级别的空间一致性反馈，提升了CLIP开放词汇分割任务中的表现，可作为插件模块集成进多种现有方法，并在多个基准上提升性能。


<details>
  <summary>Details</summary>
Motivation: CLIP虽然在图文对齐方面表现优秀，但在开放词汇分割任务中，由于定位能力差，分割效果有限。现有方法通过修改中间注意力来提升空间一致性，但这种一致性无法持续传递到最终输出，并且中间注意力与文本语义没有直接交互，限制了CLIP的潜力。

Method: 作者提出了一种训练自由、反馈驱动的自适应框架，将输出层的patch级对应关系反馈到中间注意力层。具体包括注意力隔离、基于置信度的剪枝实现稀疏自适应，以及多模型的适应集成。该方法作为插件无缝集成至多种主流backbone（ViT-B/L/H）和注意力类型（Q-K、自身、代理等），无需训练即可提升空间一致性和语义一致性。

Result: 该框架可与4种主流方法和3种backbone（ViT-B、ViT-L、ViT-H）集成，在包括MAE、SAM和DINO等多种类型注意力下进行了验证，在8个基准数据集上均表现出一致的性能提升。

Conclusion: 提出的方法能够有效增强内部表征与最终输出之间的语义一致性，提升CLIP及其衍生方法在开放词汇分割中的表现，具有通用性和插件化优势。

Abstract: CLIP exhibits strong visual-textual alignment but struggle with
open-vocabulary segmentation due to poor localization. Prior methods enhance
spatial coherence by modifying intermediate attention. But, this coherence
isn't consistently propagated to the final output due to subsequent operations
such as projections. Additionally, intermediate attention lacks direct
interaction with text representations, such semantic discrepancy limits the
full potential of CLIP.
  In this work, we propose a training-free, feedback-driven self-adaptive
framework that adapts output-based patch-level correspondences back to the
intermediate attention. The output predictions, being the culmination of the
model's processing, encapsulate the most comprehensive visual and textual
semantics about each patch. Our approach enhances semantic consistency between
internal representations and final predictions by leveraging the model's
outputs as a stronger spatial coherence prior. We design key modules, including
attention isolation, confidence-based pruning for sparse adaptation, and
adaptation ensemble, to effectively feedback the output coherence cues. Our
method functions as a plug-in module, seamlessly integrating into four
state-of-the-art approaches with three backbones (ViT-B, ViT-L, ViT-H). We
further validate our framework across multiple attention types (Q-K, self-self,
and Proxy augmented with MAE, SAM, and DINO). Our approach consistently
improves their performance across eight benchmarks.

</details>


### [12] [How Multimodal LLMs Solve Image Tasks: A Lens on Visual Grounding, Task Reasoning, and Answer Decoding](https://arxiv.org/abs/2508.20279)
*Zhuoran Yu,Yong Jae Lee*

Main category: cs.CV

TL;DR: 本文提出了一种探查多模态大模型（MLLM）内部信息处理机制的新框架。通过分层分析和线性分类器，对各层视觉与文本输入的处理过程进行系统性研究。结果揭示MLLMs分层结构具有一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大模型在视觉-语言任务上表现优异，其内部如何处理视觉和文本信息仍不清楚。本文旨在揭示MLLM内部不同层的功能分工和信息流转机制。

Method: 作者提出了一套探查框架：在每一层抽取token embedding，并通过线性分类器预测细粒度视觉类别。采用三种受控提示变体测试各层对词汇、语义否定和输出格式变化的敏感性，进而分析不同层功能。

Result: 在多个主流MLLM（如LLaVA-1.5、LLaVA-Next-LLaMA-3、Qwen2-VL）上，发现早期层执行视觉定位，中层进行词汇整合与语义推理，末层则准备特定任务输出。该分层结构对视觉分词、微调数据等变化具鲁棒性，但分层具体分配会受基础大模型架构影响。

Conclusion: 本文统一了MLLM分层功能认知，并提出了一套轻量、模型无关的分析方法，为后续多模态表征机制的研究和优化提供了理论指导。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated strong performance
across a wide range of vision-language tasks, yet their internal processing
dynamics remain underexplored. In this work, we introduce a probing framework
to systematically analyze how MLLMs process visual and textual inputs across
layers. We train linear classifiers to predict fine-grained visual categories
(e.g., dog breeds) from token embeddings extracted at each layer, using a
standardized anchor question. To uncover the functional roles of different
layers, we evaluate these probes under three types of controlled prompt
variations: (1) lexical variants that test sensitivity to surface-level
changes, (2) semantic negation variants that flip the expected answer by
modifying the visual concept in the prompt, and (3) output format variants that
preserve reasoning but alter the answer format. Applying our framework to
LLaVA-1.5, LLaVA-Next-LLaMA-3, and Qwen2-VL, we identify a consistent
stage-wise structure in which early layers perform visual grounding, middle
layers support lexical integration and semantic reasoning, and final layers
prepare task-specific outputs. We further show that while the overall
stage-wise structure remains stable across variations in visual tokenization,
instruction tuning data, and pretraining corpus, the specific layer allocation
to each stage shifts notably with changes in the base LLM architecture. Our
findings provide a unified perspective on the layer-wise organization of MLLMs
and offer a lightweight, model-agnostic approach for analyzing multimodal
representation dynamics.

</details>


### [13] [Disentangling Latent Embeddings with Sparse Linear Concept Subspaces (SLiCS)](https://arxiv.org/abs/2508.20322)
*Zhi Li,Hau Phan,Matthew Emigh,Austin J. Brockmeier*

Main category: cs.CV

TL;DR: 该论文提出通过稀疏线性子空间（SLiCS）对视觉-语言嵌入空间（如CLIP）进行解耦，实现更精细的概念筛选与检索。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉-语言嵌入网络（如CLIP）虽然具备强大的语义表达能力，但其嵌入空间内容混杂，难以直接按照具体概念解耦，为下游任务带来限制。因此，作者希望将复杂场景中关于内容的语义信息分离，从而更精准地利用这些信息。

Method: 作者提出了一种有监督的字典学习方法，将嵌入分解为多个与不同概念相关的子空间。每个子空间通过稀疏、非负的原子组合表示，并通过分组结构化字典学习和交替优化算法进行训练。利用文本-图像共嵌入，进一步进行零样本多标签推断。

Result: 所提出的SLiCS方法显著提升了基于概念过滤的图像检索的准确性，并可扩展应用到TiTok自动编码器和DINOv2等自监督模型的高压缩嵌入，获得了定量和定性的优异表现。

Conclusion: 作者的方法SLiCS可以更精细地从嵌入中提取概念相关内容，有助于多标签图像检索和条件生成任务，优于直接使用原始嵌入，提升了相关下游应用的表现。

Abstract: Vision-language co-embedding networks, such as CLIP, provide a latent
embedding space with semantic information that is useful for downstream tasks.
We hypothesize that the embedding space can be disentangled to separate the
information on the content of complex scenes by decomposing the embedding into
multiple concept-specific component vectors that lie in different subspaces. We
propose a supervised dictionary learning approach to estimate a linear
synthesis model consisting of sparse, non-negative combinations of groups of
vectors in the dictionary (atoms), whose group-wise activity matches the
multi-label information. Each concept-specific component is a non-negative
combination of atoms associated to a label. The group-structured dictionary is
optimized through a novel alternating optimization with guaranteed convergence.
Exploiting the text co-embeddings, we detail how semantically meaningful
descriptions can be found based on text embeddings of words best approximated
by a concept's group of atoms, and unsupervised dictionary learning can exploit
zero-shot classification of training set images using the text embeddings of
concept labels to provide instance-wise multi-labels. We show that the
disentangled embeddings provided by our sparse linear concept subspaces (SLiCS)
enable concept-filtered image retrieval (and conditional generation using
image-to-prompt) that is more precise. We also apply SLiCS to highly-compressed
autoencoder embeddings from TiTok and the latent embedding from self-supervised
DINOv2. Quantitative and qualitative results highlight the improved precision
of the concept-filtered image retrieval for all embeddings.

</details>


### [14] [MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models](https://arxiv.org/abs/2508.20345)
*Xiao Li,Yanfan Zhu,Ruining Deng,Wei-Qi Wei,Yu Wang,Shilin Zhao,Yaohong Wang,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: 本文提出了MedFoundationHub，一个面向医疗视觉-语言模型(VLMs)的GUI工具包，以安全、易用的方式推动医学AI模型在医疗环境中的应用，并评估了当前主流VLMs在病理任务中的表现及其局限。


<details>
  <summary>Details</summary>
Motivation: 现有医学VLMs有较大潜力，但在医疗环境中应用时面临隐私泄露、数据安全等严重问题，医疗组织需要安全有效的部署解决方案。

Method: 开发了MedFoundationHub，实现三大功能：（1）医生友好的无编程GUI界面可选用不同VLMs；（2）支持工程师按需部署、无缝集成主流开源模型；（3）通过离线、Docker编排的方式保障隐私安全，仅需本地工作站和单卡GPU。并由病理专家评估了5种主流医学VLMs在肠、肾病例上的表现。

Result: 临床专家对不同模型在特定案例中的表现进行了1015次评分，发现这些模型存在答非所问、推理模糊、医学术语不一致等共性问题。

Conclusion: MedFoundationHub为医学VLMs的安全部署与使用提供了便捷工具，但现有主流模型在细致临床任务中表现仍有限，需进一步提升精度和医学适用性。

Abstract: Recent advances in medical vision-language models (VLMs) open up remarkable
opportunities for clinical applications such as automated report generation,
copilots for physicians, and uncertainty quantification. However, despite their
promise, medical VLMs introduce serious security concerns, most notably risks
of Protected Health Information (PHI) exposure, data leakage, and vulnerability
to cyberthreats - which are especially critical in hospital environments. Even
when adopted for research or non-clinical purposes, healthcare organizations
must exercise caution and implement safeguards. To address these challenges, we
present MedFoundationHub, a graphical user interface (GUI) toolkit that: (1)
enables physicians to manually select and use different models without
programming expertise, (2) supports engineers in efficiently deploying medical
VLMs in a plug-and-play fashion, with seamless integration of Hugging Face
open-source models, and (3) ensures privacy-preserving inference through
Docker-orchestrated, operating system agnostic deployment. MedFoundationHub
requires only an offline local workstation equipped with a single NVIDIA A6000
GPU, making it both secure and accessible within the typical resources of
academic research labs. To evaluate current capabilities, we engaged
board-certified pathologists to deploy and assess five state-of-the-art VLMs
(Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and
LLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases,
yielding 1015 clinician-model scoring events. These assessments revealed
recurring limitations, including off-target answers, vague reasoning, and
inconsistent pathology terminology.

</details>


### [15] [Enhancing Mamba Decoder with Bidirectional Interaction in Multi-Task Dense Prediction](https://arxiv.org/abs/2508.20376)
*Mang Cao,Sanping Zhou,Yizhe Li,Ye Deng,Wenli Huang,Le Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的双向交互Mamba（BIM）方法，通过创新扫描机制在多任务致密预测任务中实现更高效跨任务交互。实验结果表现优异。


<details>
  <summary>Details</summary>
Motivation: 多任务致密预测需要充足的跨任务交互来提升性能，但交互越充分，计算开销就越大，现有方法难以兼顾交互性和效率。

Method: 提出了Bidirectional Interaction Scan（BI-Scan）机制，实现任务特定表达与双向顺序交互，结合任务优先和位置优先扫描方式，在保证线性复杂度的同时高效保留关键信息。同时引入Multi-Scale Scan（MS-Scan）实现多粒度场景建模，进一步丰富跨任务特征交互。

Result: 在两个具有挑战性的基准数据集NYUD-V2和PASCAL-Context上的大量实验表明，所提BIM方法在精度和效率方面均优于现有最新方法。

Conclusion: BIM通过创新交互与扫描机制，有效提升多任务致密预测的性能，为相关领域提供了新的高效解决方案。

Abstract: Sufficient cross-task interaction is crucial for success in multi-task dense
prediction. However, sufficient interaction often results in high computational
complexity, forcing existing methods to face the trade-off between interaction
completeness and computational efficiency. To address this limitation, this
work proposes a Bidirectional Interaction Mamba (BIM), which incorporates novel
scanning mechanisms to adapt the Mamba modeling approach for multi-task dense
prediction. On the one hand, we introduce a novel Bidirectional Interaction
Scan (BI-Scan) mechanism, which constructs task-specific representations as
bidirectional sequences during interaction. By integrating task-first and
position-first scanning modes within a unified linear complexity architecture,
BI-Scan efficiently preserves critical cross-task information. On the other
hand, we employ a Multi-Scale Scan~(MS-Scan) mechanism to achieve
multi-granularity scene modeling. This design not only meets the diverse
granularity requirements of various tasks but also enhances nuanced cross-task
feature interactions. Extensive experiments on two challenging benchmarks,
\emph{i.e.}, NYUD-V2 and PASCAL-Context, show the superiority of our BIM vs its
state-of-the-art competitors.

</details>


### [16] [Audio-Guided Visual Editing with Complex Multi-Modal Prompts](https://arxiv.org/abs/2508.20379)
*Hyeonyu Kim,Seokhoon Jeong,Seonghee Han,Chanhyuk Choi,Taehwan Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的音频引导视觉编辑框架，能在无需额外训练的条件下，利用多模态（文本和音频）提示完成复杂的图像编辑任务。


<details>
  <summary>Details</summary>
Motivation: 传统的扩散模型视觉编辑方法依赖文本指导，难以解决许多复杂场景，而仅用文本无法充分表达用户意图，因此需要引入音频等非文本提示以丰富编辑表达能力。

Method: 作者利用零样本能力强的预训练多模态编码器，将多样音频信息引入视觉编辑，并提出了新的方法减少音频编码器与扩散模型提示编码器空间之间的不一致。同时，针对多重和多模态编辑提示，提出了分离噪声分支和自适应区域选择机制。

Result: 系列实验证明，本文方法能在复杂编辑任务中显著超越仅基于文本的方案，充分利用音频信息提升编辑效果。

Conclusion: 通过无须额外训练的音频和文本联合指导，本文提出的框架有效提升了扩散模型编辑在复杂场景下的表现，具备较强的实用和泛化能力。

Abstract: Visual editing with diffusion models has made significant progress but often
struggles with complex scenarios that textual guidance alone could not
adequately describe, highlighting the need for additional non-text editing
prompts. In this work, we introduce a novel audio-guided visual editing
framework that can handle complex editing tasks with multiple text and audio
prompts without requiring additional training. Existing audio-guided visual
editing methods often necessitate training on specific datasets to align audio
with text, limiting their generalization to real-world situations. We leverage
a pre-trained multi-modal encoder with strong zero-shot capabilities and
integrate diverse audio into visual editing tasks, by alleviating the
discrepancy between the audio encoder space and the diffusion model's prompt
encoder space. Additionally, we propose a novel approach to handle complex
scenarios with multiple and multi-modal editing prompts through our separate
noise branching and adaptive patch selection. Our comprehensive experiments on
diverse editing tasks demonstrate that our framework excels in handling
complicated editing scenarios by incorporating rich information from audio,
where text-only approaches fail.

</details>


### [17] [More Reliable Pseudo-labels, Better Performance: A Generalized Approach to Single Positive Multi-label Learning](https://arxiv.org/abs/2508.20381)
*Luong Tran,Thieu Vo,Anh Nguyen,Sang Dinh,Van Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种用于多标签学习的新方法，能有效利用极少的标注（每张图片仅有一个正标签）实现高精度分类，并取得了最新最好成绩。


<details>
  <summary>Details</summary>
Motivation: 多标签学习通常需要每张图片多个标签的完整标注，成本高昂。为减少标注工作量，研究者探索从部分标注甚至单一正标签的情况下进行有效学习。现有方案在处理缺失标签时容易引入错误（如误判负标签），且伪标签策略易带来噪声，因此亟需更鲁棒的训练方法。

Method: 作者提出了一种新的损失函数GPR Loss，用以在利用多种伪标签时减缓噪声影响；同时设计了DAMP策略，动态生成高质量伪标签。两者结合，形成了AEVLP视觉-语言自适应伪标签框架。

Result: 在四个公开多标签数据集上进行实验证明，所提方法在多个评价指标上取得了最优的分类性能，显著优于现有同类方法。

Conclusion: AEVLP框架通过创新的损失函数与伪标签生成方式，在极低标注条件下实现了鲁棒、高效的多标签分类，展示了强大的实际应用潜力。

Abstract: Multi-label learning is a challenging computer vision task that requires
assigning multiple categories to each image. However, fully annotating
large-scale datasets is often impractical due to high costs and effort,
motivating the study of learning from partially annotated data. In the extreme
case of Single Positive Multi-Label Learning (SPML), each image is provided
with only one positive label, while all other labels remain unannotated.
Traditional SPML methods that treat missing labels as unknown or negative tend
to yield inaccuracies and false negatives, and integrating various
pseudo-labeling strategies can introduce additional noise. To address these
challenges, we propose the Generalized Pseudo-Label Robust Loss (GPR Loss), a
novel loss function that effectively learns from diverse pseudo-labels while
mitigating noise. Complementing this, we introduce a simple yet effective
Dynamic Augmented Multi-focus Pseudo-labeling (DAMP) technique. Together, these
contributions form the Adaptive and Efficient Vision-Language Pseudo-Labeling
(AEVLP) framework. Extensive experiments on four benchmark datasets demonstrate
that our framework significantly advances multi-label classification, achieving
state-of-the-art results.

</details>


### [18] [Ultra-Low-Latency Spiking Neural Networks with Temporal-Dependent Integrate-and-Fire Neuron Model for Objects Detection](https://arxiv.org/abs/2508.20392)
*Chengjun Zhang,Yuhao Zhang,Jie Yang,Mohamad Sawan*

Main category: cs.CV

TL;DR: 本文提出了一种改进的脉冲神经网络（SNN）结构，通过引入时间相关的Integrate-and-Fire（tdIF）神经元，显著提升了SNN在视觉检测任务中的精度和速度，实现了只需极低时步（最多5步）即达到最先进的性能。


<details>
  <summary>Details</summary>
Motivation: SNN 以其低能耗和快速推理能力在视觉感知领域具有广阔应用前景，但现有从ANN到SNN的转换方法在分类任务表现优秀、在视觉检测等更复杂任务上则效果不佳。主要问题是时序信息利用不足及异质化脉冲模式带来的残余膜电位影响。

Method: 本文提出delay-spike方法缓解残余膜电位问题，并创新性地设计了时序依赖的Integrate-and-Fire（tdIF）神经元，使神经元能根据时步的时序动态调整脉冲累积和发放，改善了脉冲的时序表达能力，同时保持低能耗。

Result: 在目标检测和车道线检测两个视觉检测任务上，tdIF方法在极低时步（5步内）下取得了超越当前主流ANN-SNN转换方法的性能。该方法实现了更精确的特征表达和更高效的推理速度。

Conclusion: 提出的tdIF神经元结构和延迟脉冲方法有效提升了SNN在视觉检测任务中的表现，实现了极低时步下的高性能和低延迟，为SNN在实际视觉系统应用提供了切实可行的技术基础。

Abstract: Spiking Neural Networks (SNNs), inspired by the brain, are characterized by
minimal power consumption and swift inference capabilities on neuromorphic
hardware, and have been widely applied to various visual perception tasks.
Current ANN-SNN conversion methods have achieved excellent results in
classification tasks with ultra-low time-steps, but their performance in visual
detection tasks remains suboptimal. In this paper, we propose a delay-spike
approach to mitigate the issue of residual membrane potential caused by
heterogeneous spiking patterns. Furthermore, we propose a novel
temporal-dependent Integrate-and-Fire (tdIF) neuron architecture for SNNs. This
enables Integrate-and-fire (IF) neurons to dynamically adjust their
accumulation and firing behaviors based on the temporal order of time-steps.
Our method enables spikes to exhibit distinct temporal properties, rather than
relying solely on frequency-based representations. Moreover, the tdIF neuron
maintains energy consumption on par with traditional IF neuron. We demonstrate
that our method achieves more precise feature representation with lower
time-steps, enabling high performance and ultra-low latency in visual detection
tasks. In this study, we conduct extensive evaluation of the tdIF method across
two critical vision tasks: object detection and lane line detection. The
results demonstrate that the proposed method surpasses current ANN-SNN
conversion approaches, achieving state-of-the-art performance with ultra-low
latency (within 5 time-steps).

</details>


### [19] [Graph-Based Uncertainty Modeling and Multimodal Fusion for Salient Object Detection](https://arxiv.org/abs/2508.20415)
*Yuqi Xiong,Wuzhen Shi,Yang Wen,Ruhan Liu*

Main category: cs.CV

TL;DR: 论文提出了一种新的显著目标检测网络DUP-MCRNet，通过动态不确定性传播和多模态协同推理，有效提升了复杂场景中的细节还原、边缘清晰度和跨模态信息融合表现。


<details>
  <summary>Details</summary>
Motivation: 现有显著目标检测方法在复杂场景下容易丢失细节、边缘模糊，以及单一模态信息融合不足，导致检测精度和鲁棒性下降。因此，作者致力于解决上述问题，提升小结构和边缘区域的检测，并增强对不同场景下多模态信息的有效融合与抑制冗余/干扰信息能力。

Method: 1) 设计动态不确定性图卷积模块，通过基于空间语义距离构建稀疏图，层间传播不确定性，并结合通道自适应交互，提高小结构和边缘检测精度。
2) 提出多模态协同融合策略，采用可学习权重对RGB、深度和边缘特征的注意力图进行自适应加权融合，实现跨模态间的动态互补与一致性增强。
3) 采用多尺度损失、跨尺度一致性约束、不确定性引导监督机制联合优化像素级与区域级检测表现。

Result: 在多个主流显著目标检测数据集上，DUP-MCRNet在边缘清晰度及复杂背景下的鲁棒性等方面均优于现有方法，取得了更高的检测性能。

Conclusion: DUP-MCRNet通过动态不确定性传播和多模态协同推理，有效提升了复杂场景下的显著目标检测效果，特别是在细节恢复和复杂背景适应性方面表现突出。

Abstract: In view of the problems that existing salient object detection (SOD) methods
are prone to losing details, blurring edges, and insufficient fusion of
single-modal information in complex scenes, this paper proposes a dynamic
uncertainty propagation and multimodal collaborative reasoning network
(DUP-MCRNet). Firstly, a dynamic uncertainty graph convolution module (DUGC) is
designed to propagate uncertainty between layers through a sparse graph
constructed based on spatial semantic distance, and combined with channel
adaptive interaction, it effectively improves the detection accuracy of small
structures and edge regions. Secondly, a multimodal collaborative fusion
strategy (MCF) is proposed, which uses learnable modality gating weights to
weightedly fuse the attention maps of RGB, depth, and edge features. It can
dynamically adjust the importance of each modality according to different
scenes, effectively suppress redundant or interfering information, and
strengthen the semantic complementarity and consistency between
cross-modalities, thereby improving the ability to identify salient regions
under occlusion, weak texture or background interference. Finally, the
detection performance at the pixel level and region level is optimized through
multi-scale BCE and IoU loss, cross-scale consistency constraints, and
uncertainty-guided supervision mechanisms. Extensive experiments show that
DUP-MCRNet outperforms various SOD methods on most common benchmark datasets,
especially in terms of edge clarity and robustness to complex backgrounds. Our
code is publicly available at https://github.com/YukiBear426/DUP-MCRNet.

</details>


### [20] [MSMVD: Exploiting Multi-scale Image Features via Multi-scale BEV Features for Multi-view Pedestrian Detection](https://arxiv.org/abs/2508.20447)
*Taiga Yamane,Satoshi Suzuki,Ryo Masumura,Shota Orihashi,Tomohiro Tanaka,Mana Ihori,Naoki Makishima,Naotaka Kawata*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的多尺度多视角行人检测方法（MSMVD），通过利用多尺度图像特征提升鸟瞰图中的检测性能，在标准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多视角行人检测任务中，现有端到端方法对图像中行人尺度变化（过大或过小，或不同视角尺度差异大）处理不佳，主要是因为这些方法没有利用多尺度图像特征。

Method: 作者提出了MSMVD方法：首先从各视角图像中提取多尺度特征，然后按尺度投影到BEV空间，得到相应多尺度BEV特征；利用特征金字塔网络整合不同尺度信息，实现不同尺度和视角下行人的高精度检测。

Result: 大量实验表明，MSMVD方法利用多尺度BEV特征后，检测性能大幅提升，并且在GMVD数据集上MODA指标超过以往最优方法4.5个百分点。

Conclusion: 通过结合多尺度图像特征并在BEV空间进行高效整合，MSMVD有效提升了多视角下不同尺度行人的检测精度，在多视角行人检测领域具有较大应用前景。

Abstract: Multi-View Pedestrian Detection (MVPD) aims to detect pedestrians in the form
of a bird's eye view (BEV) from multi-view images. In MVPD, end-to-end
trainable deep learning methods have progressed greatly. However, they often
struggle to detect pedestrians with consistently small or large scales in views
or with vastly different scales between views. This is because they do not
exploit multi-scale image features to generate the BEV feature and detect
pedestrians. To overcome this problem, we propose a novel MVPD method, called
Multi-Scale Multi-View Detection (MSMVD). MSMVD generates multi-scale BEV
features by projecting multi-scale image features extracted from individual
views into the BEV space, scale-by-scale. Each of these BEV features inherits
the properties of its corresponding scale image features from multiple views.
Therefore, these BEV features help the precise detection of pedestrians with
consistently small or large scales in views. Then, MSMVD combines information
at different scales of multiple views by processing the multi-scale BEV
features using a feature pyramid network. This improves the detection of
pedestrians with vastly different scales between views. Extensive experiments
demonstrate that exploiting multi-scale image features via multi-scale BEV
features greatly improves the detection performance, and MSMVD outperforms the
previous highest MODA by $4.5$ points on the GMVD dataset.

</details>


### [21] [SKGE-SWIN: End-To-End Autonomous Vehicle Waypoint Prediction and Navigation Using Skip Stage Swin Transformer](https://arxiv.org/abs/2508.20762)
*Fachri Najm Noer Kartiman,Rasim,Yaya Wihardi,Nurul Hasanah,Oskar Natan,Bambang Wahono,Taufik Ibnu Salim*

Main category: cs.CV

TL;DR: 提出了一种新的用于自动驾驶的端到端模型SKGE-Swin，通过Swin Transformer及跨阶段跳连机制提升像素级上下文感知能力，并在复杂场景中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶模型在处理复杂环境与像素级语义关联时存在局限，难以充分提取全局及多层次特征。论文旨在解决特征提取能力不足问题，提高车辆对环境复杂语义的理解。

Method: SKGE-Swin架构结合了Swin Transformer的Shifted Window多头自注意力（SW-MSA）和skip-stage机制，一方面扩大了模型在不同尺度上对全局特征的感知范围，另一方面通过跳连方式保留早期特征，提升上下文信息流传递能力。

Result: 在CARLA平台进行对抗场景测试，实验结果显示该模型的Driving Score高于相关现有方法。并通过消融实验验证跳连和Swin Transformer模块分别对模型的性能提升有显著作用。

Conclusion: SKGE-Swin在端到端自动驾驶任务中能够更好地理解复杂场景，提高整体驾驶表现，并验证了创新结构对模型有效性的贡献。

Abstract: Focusing on the development of an end-to-end autonomous vehicle model with
pixel-to-pixel context awareness, this research proposes the SKGE-Swin
architecture. This architecture utilizes the Swin Transformer with a skip-stage
mechanism to broaden feature representation globally and at various network
levels. This approach enables the model to extract information from distant
pixels by leveraging the Swin Transformer's Shifted Window-based Multi-head
Self-Attention (SW-MSA) mechanism and to retain critical information from the
initial to the final stages of feature extraction, thereby enhancing its
capability to comprehend complex patterns in the vehicle's surroundings. The
model is evaluated on the CARLA platform using adversarial scenarios to
simulate real-world conditions. Experimental results demonstrate that the
SKGE-Swin architecture achieves a superior Driving Score compared to previous
methods. Furthermore, an ablation study will be conducted to evaluate the
contribution of each architectural component, including the influence of skip
connections and the use of the Swin Transformer, in improving model
performance.

</details>


### [22] [A Spatial-Frequency Aware Multi-Scale Fusion Network for Real-Time Deepfake Detection](https://arxiv.org/abs/2508.20449)
*Libo Lv,Tianyi Wang,Mengxiao Huang,Ruixia Liu,Yinglong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级且高效的实时Deepfake检测网络SFMFNet，能够在保持高准确性的同时大幅降低计算资源消耗，适合实际应用。


<details>
  <summary>Details</summary>
Motivation: 现有Deepfake检测方法虽在标准测试集上表现优异，但过于复杂、计算量大，不适用于需要实时处理的场景，例如视频会议与社交媒体内容审核。需要一种高效又能泛化的检测方法。

Method: 提出了空间-频率混合感知模块，通过门控机制融合空间纹理与频率伪迹，增强对微小篡改的敏感性。引入token选择性跨层注意力增强多尺度特征交互，并设计了残差增强模糊池化结构，在下采样过程中保留关键信息。

Result: 在多个基准数据集上进行实验证明，SFMFNet兼顾检测准确度和效率，在运算消耗低的情况下表现优越，具备良好的泛化能力。

Conclusion: SFMFNet为实时Deepfake检测提供了实际可用的解决方案，推动相关应用落地，有望在安全、内容审核等实际场景中得到有效应用。

Abstract: With the rapid advancement of real-time deepfake generation techniques,
forged content is becoming increasingly realistic and widespread across
applications like video conferencing and social media. Although
state-of-the-art detectors achieve high accuracy on standard benchmarks, their
heavy computational cost hinders real-time deployment in practical
applications. To address this, we propose the Spatial-Frequency Aware
Multi-Scale Fusion Network (SFMFNet), a lightweight yet effective architecture
for real-time deepfake detection. We design a spatial-frequency hybrid aware
module that jointly leverages spatial textures and frequency artifacts through
a gated mechanism, enhancing sensitivity to subtle manipulations. A
token-selective cross attention mechanism enables efficient multi-level feature
interaction, while a residual-enhanced blur pooling structure helps retain key
semantic cues during downsampling. Experiments on several benchmark datasets
show that SFMFNet achieves a favorable balance between accuracy and efficiency,
with strong generalization and practical value for real-time applications.

</details>


### [23] [To New Beginnings: A Survey of Unified Perception in Autonomous Vehicle Software](https://arxiv.org/abs/2508.20892)
*Loïc Stratil,Felix Fent,Esteban Rivera,Markus Lienkamp*

Main category: cs.CV

TL;DR: 本文综述了自动驾驶领域中融合任务的感知方法，对现有方法进行了系统梳理与分类，提出了统一感知的三种范式，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统的自动驾驶感知系统采用模块化流水线，分解为检测、跟踪和预测等子任务，虽具可解释性，但存在误差累积和任务间协同不足的问题。因此，需要一种能够提升鲁棒性、效率和上下文推理性的统一感知新范式。

Method: 提出了统一感知的概念，并构建了系统的分类体系，将方法按任务整合、跟踪表述和表示流动等维度分类，进一步界定为早期、后期和完全统一感知三大范式，并对已有方法的架构、训练策略、使用数据集及开源情况进行了系统综述。

Result: 系统回顾和梳理了当前统一感知领域的主要方法和进展，提出的分类与框架有助于理解领域全貌，并整合了以往分散的研究。

Conclusion: 建立了首个全面的统一感知研究框架，为今后更强鲁棒性、泛化性和可解释性的自动驾驶感知研究指明了方向。

Abstract: Autonomous vehicle perception typically relies on modular pipelines that
decompose the task into detection, tracking, and prediction. While
interpretable, these pipelines suffer from error accumulation and limited
inter-task synergy. Unified perception has emerged as a promising paradigm that
integrates these sub-tasks within a shared architecture, potentially improving
robustness, contextual reasoning, and efficiency while retaining interpretable
outputs. In this survey, we provide a comprehensive overview of unified
perception, introducing a holistic and systemic taxonomy that categorizes
methods along task integration, tracking formulation, and representation flow.
We define three paradigms -Early, Late, and Full Unified Perception- and
systematically review existing methods, their architectures, training
strategies, datasets used, and open-source availability, while highlighting
future research directions. This work establishes the first comprehensive
framework for understanding and advancing unified perception, consolidates
fragmented efforts, and guides future research toward more robust,
generalizable, and interpretable perception.

</details>


### [24] [Dual-Model Weight Selection and Self-Knowledge Distillation for Medical Image Classification](https://arxiv.org/abs/2508.20461)
*Ayaka Tsutsumi,Guang Li,Ren Togo,Takahiro Ogawa,Satoshi Kondo,Miki Haseyama*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的医学图像分类方法，通过双模型权重选择和自知识蒸馏（SKD）相结合，实现轻量化模型在有限计算资源下的高性能医学图像分类。


<details>
  <summary>Details</summary>
Motivation: 大型医学图像分类模型在实际医疗环境中常受限于计算资源，难以部署。为实现与大型模型相当性能的轻量模型，同时保证效率，论文探索了模型蒸馏和权重迁移的新策略。

Method: 方法包括：1）用大规模预训练模型的权重初始化两个轻量级模型，实现有效知识迁移；2）对这两个模型应用自知识蒸馏（SKD）；3）采用多种初始权重配置，并在目标分类任务上进行微调。该策略兼顾了性能与计算代价。

Result: 在公开数据集（胸部X光、肺部CT、脑部MRI）上，方法展现了优于现有方法的分类效果和鲁棒性。

Conclusion: 结合双模型权重选择及自知识蒸馏后，轻量级模型能更好保留关键信息，突破了传统方法在信息保留和性能上的局限，适合实际低资源医疗环境。

Abstract: We propose a novel medical image classification method that integrates
dual-model weight selection with self-knowledge distillation (SKD). In
real-world medical settings, deploying large-scale models is often limited by
computational resource constraints, which pose significant challenges for their
practical implementation. Thus, developing lightweight models that achieve
comparable performance to large-scale models while maintaining computational
efficiency is crucial. To address this, we employ a dual-model weight selection
strategy that initializes two lightweight models with weights derived from a
large pretrained model, enabling effective knowledge transfer. Next, SKD is
applied to these selected models, allowing the use of a broad range of initial
weight configurations without imposing additional excessive computational cost,
followed by fine-tuning for the target classification tasks. By combining
dual-model weight selection with self-knowledge distillation, our method
overcomes the limitations of conventional approaches, which often fail to
retain critical information in compact models. Extensive experiments on
publicly available datasets-chest X-ray images, lung computed tomography scans,
and brain magnetic resonance imaging scans-demonstrate the superior performance
and robustness of our approach compared to existing methods.

</details>


### [25] [COMETH: Convex Optimization for Multiview Estimation and Tracking of Humans](https://arxiv.org/abs/2508.20920)
*Enrico Martini,Ho Jin Choi,Nadia Figueroa,Nicola Bombieri*

Main category: cs.CV

TL;DR: 本文提出了一种名为COMETH的轻量级多视角人体姿态融合算法，能够在边缘设备上实时实现高精度的人体活动监测，兼顾计算效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 当前工业5.0时代对人体活动监测需求日益增长，而多摄像头集中式方法虽然精度高，但面临高计算和带宽需求，不利于实时与大规模部署。边缘计算虽能缓解资源压力，却导致精度和一致性下降，因此亟需一种兼具效率与精度的新方法。

Method: 作者提出COMETH算法，将运动学与生物力学约束结合以提升关节定位准确性，利用基于凸优化的逆运动学进行空间融合，并设计状态观测器增强时间一致性。该方法在边缘设备上实现多视角实时数据融合。

Result: 在公开数据集和实际工业场景测试中，COMETH在定位、检测和追踪精度方面均优于现有最新方法。

Conclusion: COMETH算法实现了高效、可扩展且准确的人体运动跟踪，适用于工业和安全关键场景，并已开放源码。

Abstract: In the era of Industry 5.0, monitoring human activity is essential for
ensuring both ergonomic safety and overall well-being. While multi-camera
centralized setups improve pose estimation accuracy, they often suffer from
high computational costs and bandwidth requirements, limiting scalability and
real-time applicability. Distributing processing across edge devices can reduce
network bandwidth and computational load. On the other hand, the constrained
resources of edge devices lead to accuracy degradation, and the distribution of
computation leads to temporal and spatial inconsistencies. We address this
challenge by proposing COMETH (Convex Optimization for Multiview Estimation and
Tracking of Humans), a lightweight algorithm for real-time multi-view human
pose fusion that relies on three concepts: it integrates kinematic and
biomechanical constraints to increase the joint positioning accuracy; it
employs convex optimization-based inverse kinematics for spatial fusion; and it
implements a state observer to improve temporal consistency. We evaluate COMETH
on both public and industrial datasets, where it outperforms state-of-the-art
methods in localization, detection, and tracking accuracy. The proposed fusion
pipeline enables accurate and scalable human motion tracking, making it
well-suited for industrial and safety-critical applications. The code is
publicly available at https://github.com/PARCO-LAB/COMETH.

</details>


### [26] [Re-Densification Meets Cross-Scale Propagation: Real-Time Compression of LiDAR Point Clouds](https://arxiv.org/abs/2508.20466)
*Pengpeng Yu,Haoran Li,Dingquan Li,Runqing Jiang,Jing Wang,Liang Lin,Yulan Guo*

Main category: cs.CV

TL;DR: 本论文提出了一种高效的LiDAR点云预测编码方法，通过重新稠密化和跨尺度特征传播，有效提升压缩效率和编码解码速度，取得了当前最佳的实验结果。


<details>
  <summary>Details</summary>
Motivation: 高精度LiDAR点云数据在存储和传输时开销极大，现有压缩方法受极端稀疏性的影响，难以高效建模上下文，导致压缩性能和速度受限。论文旨在通过设计新的特征处理模块，提高点云压缩效率和实时性。

Method: 提出包含两个轻量级模块的框架：1)‘几何重新稠密化模块’，将稀疏几何特征稠密化、提取丰富特征后，再稀疏化用于预测编码，兼顾特征提取与计算效率；2)‘跨尺度特征传播模块’，利用不同分辨率层级的占用信息指引特征传播，减少冗余特征提取并丰富信息。二者结合形成紧凑高效的特征表征。

Result: 在KITTI数据集上进行实验，提出的方法在12位量化条件下实现了编码和解码各26帧每秒的实时性能，并达到了当前最优的压缩比。

Conclusion: 通过重新稠密化和跨尺度特征传播，有效提升了点云压缩的上下文建模能力和整体效率，方法在压缩率及实时性方面均优于现有方案，具备实际落地和推广价值。

Abstract: LiDAR point clouds are fundamental to various applications, yet
high-precision scans incur substantial storage and transmission overhead.
Existing methods typically convert unordered points into hierarchical octree or
voxel structures for dense-to-sparse predictive coding. However, the extreme
sparsity of geometric details hinders efficient context modeling, thereby
limiting their compression performance and speed. To address this challenge, we
propose to generate compact features for efficient predictive coding. Our
framework comprises two lightweight modules. First, the Geometry
Re-Densification Module re-densifies encoded sparse geometry, extracts features
at denser scale, and then re-sparsifies the features for predictive coding.
This module avoids costly computation on highly sparse details while
maintaining a lightweight prediction head. Second, the Cross-scale Feature
Propagation Module leverages occupancy cues from multiple resolution levels to
guide hierarchical feature propagation. This design facilitates information
sharing across scales, thereby reducing redundant feature extraction and
providing enriched features for the Geometry Re-Densification Module. By
integrating these two modules, our method yields a compact feature
representation that provides efficient context modeling and accelerates the
coding process. Experiments on the KITTI dataset demonstrate state-of-the-art
compression ratios and real-time performance, achieving 26 FPS for both
encoding and decoding at 12-bit quantization. Code is available at
https://github.com/pengpeng-yu/FastPCC.

</details>


### [27] [CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification](https://arxiv.org/abs/2508.21046)
*Wei Li,Renshan Zhang,Rui Shao,Jie He,Liqiang Nie*

Main category: cs.CV

TL;DR: CogVLA 是一种新提出的认知对齐视觉-语言-行动（VLA）模型框架，通过指令驱动的路由和稀疏化，实现了训练和推理的高效化，同时提升了性能。其结构包括三阶段逐步压缩与信息筛选，有效降低计算消耗并在多个任务上达到了SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型大多依赖大规模的视觉-语言模型（VLMs）后训练，带来高昂的计算成本，极大限制了实际的大规模部署和扩展。研究动机在于：在不损失性能或行为准确性的前提下，如何显著提升模型效率，实现计算资源友好型的VLA系统。

Method: CogVLA提出了三阶段架构：（1）EFA-Routing：将指令信息注入视觉编码器，使其有针对性地聚合和压缩视觉token，形成指令感知的潜在表征；（2）LFP-Routing：基于紧凑视觉编码，将动作意图注入语言模型，按token级剪除与指令无关的视觉token，实现稀疏化处理；（3）V-L-A Coupled Attention：为保障压缩后的输入仍能支持高质量行为生成，设计了视觉-语言因果注意力和动作并行解码的结合机制。

Result: 在LIBERO基准与实际机器人任务上，CogVLA分别达到97.4%和70.0%的成功率，优于OpenVLA（SOTA）。此外，训练成本降低2.5倍，推理延迟下降2.8倍。

Conclusion: CogVLA显著提升了VLA模型在资源利用和任务表现的平衡，打破了高性能与高计算消耗的固有矛盾，为下一代智能机器人系统提供新范式。相关实现已开源，推动研究社区发展。

Abstract: Recent Vision-Language-Action (VLA) models built on pre-trained
Vision-Language Models (VLMs) require extensive post-training, resulting in
high computational overhead that limits scalability and deployment.We propose
CogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages
instruction-driven routing and sparsification to improve both efficiency and
performance. CogVLA draws inspiration from human multimodal coordination and
introduces a 3-stage progressive architecture. 1) Encoder-FiLM based
Aggregation Routing (EFA-Routing) injects instruction information into the
vision encoder to selectively aggregate and compress dual-stream visual tokens,
forming a instruction-aware latent representation. 2) Building upon this
compact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing)
introduces action intent into the language model by pruning
instruction-irrelevant visually grounded tokens, thereby achieving token-level
sparsity. 3) To ensure that compressed perception inputs can still support
accurate and coherent action generation, we introduce V-L-A Coupled Attention
(CAtten), which combines causal vision-language attention with bidirectional
action parallel decoding. Extensive experiments on the LIBERO benchmark and
real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art
performance with success rates of 97.4% and 70.0%, respectively, while reducing
training costs by 2.5-fold and decreasing inference latency by 2.8-fold
compared to OpenVLA. CogVLA is open-sourced and publicly available at
https://github.com/JiuTian-VL/CogVLA.

</details>


### [28] [Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation](https://arxiv.org/abs/2508.20470)
*Xiaochuan Li,Guoguang Du,Runze Zhang,Liang Jin,Qi Jia,Lihua Lu,Zhenhua Guo,Yaqian Zhao,Haiyang Liu,Tianqi Wang,Changsheng Li,Xiaoli Gong,Rengang Li,Baoyu Fan*

Main category: cs.CV

TL;DR: 本文提出了一种利用多视角视频提升3D资产生成的方法，并推出了大规模标注视频数据集Droplet3D-4M以及支持图像和文本输入的生成模型Droplet3D。结果表明该方法在空间一致性和语义合理性方面优于传统3D方案。


<details>
  <summary>Details</summary>
Motivation: 目前3D领域的数据远少于文本、图像或视频，导致基于大数据训练的生成模型难以取得类似突破。作者希望借助包含常识先验和多视角信息的视频数据，缓解3D数据稀缺带来的瓶颈。

Method: 1）构建Droplet3D-4M，这是首个大规模、多视角标注的视频数据集；2）训练Droplet3D生成模型，可接受图像或文本输入，用视频数据进行监督，促进3D内容生成的空间一致性和语义合理性。

Result: 实验证明：该方法在生成3D内容的空间一致性和语义合理性方面有效，并表现出可扩展至更复杂场景生成的潜力，优于现有主流3D生成方案。

Conclusion: 视频中蕴含的常识和多视角信息可显著提升3D生成质量。本文的开放数据集和代码为3D内容生成领域带来了新的方向和工具，有助于促进相关研究和应用发展。

Abstract: Scaling laws have validated the success and promise of large-data-trained
models in creative generation across text, image, and video domains. However,
this paradigm faces data scarcity in the 3D domain, as there is far less of it
available on the internet compared to the aforementioned modalities.
Fortunately, there exist adequate videos that inherently contain commonsense
priors, offering an alternative supervisory signal to mitigate the
generalization bottleneck caused by limited native 3D data. On the one hand,
videos capturing multiple views of an object or scene provide a spatial
consistency prior for 3D generation. On the other hand, the rich semantic
information contained within the videos enables the generated content to be
more faithful to the text prompts and semantically plausible. This paper
explores how to apply the video modality in 3D asset generation, spanning
datasets to models. We introduce Droplet3D-4M, the first large-scale video
dataset with multi-view level annotations, and train Droplet3D, a generative
model supporting both image and dense text input. Extensive experiments
validate the effectiveness of our approach, demonstrating its ability to
produce spatially consistent and semantically plausible content. Moreover, in
contrast to the prevailing 3D solutions, our approach exhibits the potential
for extension to scene-level applications. This indicates that the commonsense
priors from the videos significantly facilitate 3D creation. We have
open-sourced all resources including the dataset, code, technical framework,
and model weights: https://dropletx.github.io/.

</details>


### [29] [Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation](https://arxiv.org/abs/2508.20471)
*Jiusi Li,Jackson Jiang,Jinyu Miao,Miao Long,Tuopu Wen,Peijin Jia,Shengxiang Liu,Chunlei Yu,Maolin Liu,Yuzhan Cai,Kun Jiang,Mengmeng Yang,Diange Yang*

Main category: cs.CV

TL;DR: 本文提出了一种用于自动驾驶场景中对视频目标物体进行高保真、精准编辑的新方法G^2Editor。


<details>
  <summary>Details</summary>
Motivation: 收集自动驾驶系统所需的边缘案例开销大且危险，因此需要更有效的数据多样化方法。现有通过3D高斯点渲染或生成模型编辑视频的方法要么视觉真实感不足，要么姿态控制不精确。

Method: G^2Editor结合三维高斯表示，将编辑对象的空间先验引入去噪流程，保证物体姿态精确控制及场景一致性；通过三维包围盒布局还原被遮挡区域，并通过分层特征条件化生成精细外观。

Result: 在Waymo Open Dataset上，G^2Editor在目标物体的重定位、插入和删除任务中，同时提升了姿态可控性和视觉质量，性能优于现有方法，并促进了数据驱动下游任务表现。

Conclusion: G^2Editor为自动驾驶视频场景编辑提供了统一、有效且高保真度的技术支持，兼具实用性和先进性。

Abstract: Corner cases are crucial for training and validating autonomous driving
systems, yet collecting them from the real world is often costly and hazardous.
Editing objects within captured sensor data offers an effective alternative for
generating diverse scenarios, commonly achieved through 3D Gaussian Splatting
or image generative models. However, these approaches often suffer from limited
visual fidelity or imprecise pose control. To address these issues, we propose
G^2Editor, a framework designed for photorealistic and precise object editing
in driving videos. Our method leverages a 3D Gaussian representation of the
edited object as a dense prior, injected into the denoising process to ensure
accurate pose control and spatial consistency. A scene-level 3D bounding box
layout is employed to reconstruct occluded areas of non-target objects.
Furthermore, to guide the appearance details of the edited object, we
incorporate hierarchical fine-grained features as additional conditions during
generation. Experiments on the Waymo Open Dataset demonstrate that G^2Editor
effectively supports object repositioning, insertion, and deletion within a
unified framework, outperforming existing methods in both pose controllability
and visual quality, while also benefiting downstream data-driven tasks.

</details>


### [30] [Enhancing Corpus Callosum Segmentation in Fetal MRI via Pathology-Informed Domain Randomization](https://arxiv.org/abs/2508.20475)
*Marina Grifell i Plana,Vladyslav Zalevskyi,Léa Schmidt,Yvan Gomez,Thomas Sanchez,Vincent Dunet,Mériam Koob,Vanessa Siffredi,Meritxell Bach Cuadra*

Main category: cs.CV

TL;DR: 研究提出了针对胎儿脑部罕见病变（如胼胝体发育障碍）数据稀缺问题的一种基于病理知识的数据合成和分割方法，有效提升了病变脑的分割表现和相关生物标志物的评估准确性。


<details>
  <summary>Details</summary>
Motivation: 针对胼胝体发育障碍等罕见胎儿脑部疾病，由于其数据稀缺，深度学习分割模型缺乏泛化能力，影响生物标志物的准确提取和神经发育评估。

Method: 提出一种基于病理知识的领域随机化（domain randomization）数据合成方法，将CCD（胼胝体发育障碍）相关的结构异常作为先验知识嵌入合成数据生成流程，实现不需要真实病理标注也能进行病变分割的训练。

Result: 方法在包含248名健康胎儿、26例CCD和47例其他脑病变的数据集上验证，在CCD病例上分割评估获得显著提升，同时健康与其他病理样本的分割表现保持稳定。生物标志物估算误差（如胼胝体长度LCC）在健康和CCD两类中均显著降低，且分割结构的拓扑一致性明显增强。

Conclusion: 将领域内的解剖学知识引入到合成数据流程能有效缓解稀缺数据问题，提高罕见但临床意义重大的脑结构异常分析的准确性和可用性。

Abstract: Accurate fetal brain segmentation is crucial for extracting biomarkers and
assessing neurodevelopment, especially in conditions such as corpus callosum
dysgenesis (CCD), which can induce drastic anatomical changes. However, the
rarity of CCD severely limits annotated data, hindering the generalization of
deep learning models. To address this, we propose a pathology-informed domain
randomization strategy that embeds prior knowledge of CCD manifestations into a
synthetic data generation pipeline. By simulating diverse brain alterations
from healthy data alone, our approach enables robust segmentation without
requiring pathological annotations.
  We validate our method on a cohort comprising 248 healthy fetuses, 26 with
CCD, and 47 with other brain pathologies, achieving substantial improvements on
CCD cases while maintaining performance on both healthy fetuses and those with
other pathologies. From the predicted segmentations, we derive clinically
relevant biomarkers, such as corpus callosum length (LCC) and volume, and show
their utility in distinguishing CCD subtypes. Our pathology-informed
augmentation reduces the LCC estimation error from 1.89 mm to 0.80 mm in
healthy cases and from 10.9 mm to 0.7 mm in CCD cases. Beyond these
quantitative gains, our approach yields segmentations with improved topological
consistency relative to available ground truth, enabling more reliable
shape-based analyses. Overall, this work demonstrates that incorporating
domain-specific anatomical priors into synthetic data pipelines can effectively
mitigate data scarcity and enhance analysis of rare but clinically significant
malformations.

</details>


### [31] [Towards Inclusive Communication: A Unified LLM-Based Framework for Sign Language, Lip Movements, and Audio Understanding](https://arxiv.org/abs/2508.20476)
*Jeong Hun Yeo,Hyeongseop Rha,Sungjune Park,Junil Won,Yong Man Ro*

Main category: cs.CV

TL;DR: 本论文提出了首个能够统一处理手语、唇部动作和音频多模态输入，实现口语文本生成的框架，不仅方法先进，还优于单一任务的最新模型。


<details>
  <summary>Details</summary>
Motivation: 尽管手语识别(SLT)、视觉语音识别(VSR)等无音频交流技术取得进步，但各自独立发展，缺乏有效的多模态整合手段，限制了对听障甚至多种交流场景的适用性。

Method: 作者设计并实现了一个统一的、与模态无关的神经网络架构，可以处理手语、唇部动作和音频的任意组合输入，并探索了多模态间的协同作用（特别关注唇部动作作为手语理解中的非手动作提示）。

Result: 该统一模型在SLT、VSR、ASR、AVSR等多项任务上达到了与专用SOTA模型相当或更优的表现。实验还显示，将唇部动作明确建模为独立模态，能显著提升手语翻译性能。

Conclusion: 多模态整合不仅可提升识别与翻译系统性能，同时揭示了不同模态间的协同机制，推动了更高效、更包容的无障碍人机通信技术发展。

Abstract: Audio is the primary modality for human communication and has driven the
success of Automatic Speech Recognition (ASR) technologies. However, such
systems remain inherently inaccessible to individuals who are deaf or hard of
hearing. Visual alternatives such as sign language and lip reading offer
effective substitutes, and recent advances in Sign Language Translation (SLT)
and Visual Speech Recognition (VSR) have improved audio-less communication.
Yet, these modalities have largely been studied in isolation, and their
integration within a unified framework remains underexplored. In this paper, we
introduce the first unified framework capable of handling diverse combinations
of sign language, lip movements, and audio for spoken-language text generation.
We focus on three main objectives: (i) designing a unified, modality-agnostic
architecture capable of effectively processing heterogeneous inputs; (ii)
exploring the underexamined synergy among modalities, particularly the role of
lip movements as non-manual cues in sign language comprehension; and (iii)
achieving performance on par with or superior to state-of-the-art models
specialized for individual tasks. Building on this framework, we achieve
performance on par with or better than task-specific state-of-the-art models
across SLT, VSR, ASR, and AVSR. Furthermore, our analysis reveals that
explicitly modeling lip movements as a separate modality significantly improves
SLT performance.

</details>


### [32] [Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding](https://arxiv.org/abs/2508.20478)
*Yuan Xie,Tianshui Chen,Zheng Ge,Lionel Ni*

Main category: cs.CV

TL;DR: 提出了一种用于长视频理解的多轮推理系统Video-MTR，不依赖外部视觉语言模型，通过多次迭代选择关键视频片段，提升理解准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 长视频理解需要处理长时序依赖和多事件，现有方法依赖静态推理或外部VLM，难以端到端训练，导致性能受限。

Method: 提出了Video-MTR框架，通过多轮推理机制，结合历次处理结果及当前问题，逐步选择和分析关键视频片段。设计了新颖的门控双层奖励系统，兼顾答案正确性和帧-查询关联，用于优化分段选择和问题理解，无需外部VLM，实现端到端训练。

Result: 在VideoMME、MLVU和EgoSchema等基准上，通过实验结果表明Video-MTR无论在准确率还是效率上均优于现有方法。

Conclusion: Video-MTR推动了长视频理解技术的发展，提供了高效、准确且可端到端优化的新范式。

Abstract: Long-form video understanding, characterized by long-range temporal
dependencies and multiple events, remains a challenge. Existing methods often
rely on static reasoning or external visual-language models (VLMs), which face
issues like complexity and sub-optimal performance due to the lack of
end-to-end training. In this paper, we propose Video-MTR, a reinforced
multi-turn reasoning framework designed to enable iterative key video segment
selection and question comprehension. Unlike traditional video reasoning
pipeline, which generate predictions in a single turn, Video-MTR performs
reasoning in multiple turns, selecting video segments progressively based on
the evolving understanding of previously processed segments and the current
question. This iterative process allows for a more refined and contextually
aware analysis of the video. To ensure intermediate reasoning process, we
introduce a novel gated bi-level reward system, combining trajectory-level
rewards based on answer correctness and turn-level rewards emphasizing
frame-query relevance. This system optimizes both video segment selection and
question comprehension, eliminating the need for external VLMs and allowing
end-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU,
and EgoSchema demonstrate that Video-MTR outperforms existing methods in both
accuracy and efficiency, advancing the state-of-the-art in long video
understanding.

</details>


### [33] [Adaptive Dual Uncertainty Optimization: Boosting Monocular 3D Object Detection under Test-Time Shifts](https://arxiv.org/abs/2508.20488)
*Zixuan Hu,Dongxiao Li,Xinzhu Ma,Shixiang Tang,Xiaotong Li,Wenhan Yang,Ling-Yu Duan*

Main category: cs.CV

TL;DR: 本文提出了一种名为DUO的双重不确定性优化框架，旨在通过联合优化语义和几何不确定性，实现更强鲁棒性的单目3D物体检测，尤其应对跨域情景下的性能下降。实验结果显示，该方法优于现有TTA方法。


<details>
  <summary>Details</summary>
Motivation: 单目3D物体检测在自动驾驶等任务中至关重要，但在现实环境和传感器变化带来的域偏移下性能大幅下降。现有测试时自适应(TTA)方法忽略了这一任务中本质的语义和几何双重不确定性，因此亟需更有针对性的适应方法。

Method: 作者提出了DUO框架，从凸优化的视角，引入了一种新的焦点损失结构及其无监督版本，用以在无标签情况下对高不确定性目标进行动态权衡。同时设计了一种语义感知的法向场约束，利用明显语义区域的信息提升几何鲁棒性，形成语义与几何互补的优化机制。

Result: 大量实验证明，DUO框架在多个数据集及不同类型的域偏移场景下均超越了现有TTA方法，明显提升了单目3D检测的鲁棒性和泛化能力。

Conclusion: DUO实现了对单目3D检测中的语义与几何不确定性的联合优化，为跨域适应带来显著效果。双分支互补的策略有效提高了空间和语义感知能力，为任务提供了更高的安全性和可靠性。

Abstract: Accurate monocular 3D object detection (M3OD) is pivotal for safety-critical
applications like autonomous driving, yet its reliability deteriorates
significantly under real-world domain shifts caused by environmental or sensor
variations. To address these shifts, Test-Time Adaptation (TTA) methods have
emerged, enabling models to adapt to target distributions during inference.
While prior TTA approaches recognize the positive correlation between low
uncertainty and high generalization ability, they fail to address the dual
uncertainty inherent to M3OD: semantic uncertainty (ambiguous class
predictions) and geometric uncertainty (unstable spatial localization). To
bridge this gap, we propose Dual Uncertainty Optimization (DUO), the first TTA
framework designed to jointly minimize both uncertainties for robust M3OD.
Through a convex optimization lens, we introduce an innovative convex structure
of the focal loss and further derive a novel unsupervised version, enabling
label-agnostic uncertainty weighting and balanced learning for high-uncertainty
objects. In parallel, we design a semantic-aware normal field constraint that
preserves geometric coherence in regions with clear semantic cues, reducing
uncertainty from the unstable 3D representation. This dual-branch mechanism
forms a complementary loop: enhanced spatial perception improves semantic
classification, and robust semantic predictions further refine spatial
understanding. Extensive experiments demonstrate the superiority of DUO over
existing methods across various datasets and domain shift types.

</details>


### [34] [CaddieSet: A Golf Swing Dataset with Human Joint Features and Ball Information](https://arxiv.org/abs/2508.20491)
*Seunghyeon Jung,Seoyoung Hong,Jiwoo Jeong,Seungwon Jeong,Jaerim Choi,Hoki Kim,Woojin Lee*

Main category: cs.CV

TL;DR: 本文提出了CaddieSet数据集，结合挥杆关节信息与球的各种数据，通过分阶段视频分析，以及专家定义的关键指标，实现挥杆姿势与球轨迹的量化关联，为高尔夫挥杆分析和打击精准度提升提供数据支持与解释性模型。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习研究虽关注了高尔夫击球精准度，但缺乏对挥杆姿势与球轨迹之间定量关系的建立，难以为球手提供有效挥杆改进建议。为此，作者希望通过构建定量数据集和解释性指标解决这一不足。

Method: 作者提出了CaddieSet新数据集，采集单次击球中的关节与球数据，并基于计算机视觉将挥杆视频分割为8个阶段。同时，结合高尔夫领域专家知识，设计了15个影响挥杆的关键指标，通过这些特征建立解释性模型，分析挥杆对击球结果的影响。

Result: 通过多组基准实验，验证了CaddieSet数据集用于球轨迹预测的可行性。尤其在可解释模型方面，证明了利用关节特征的挥杆反馈结果与领域知识高度一致。

Conclusion: CaddieSet的数据和基于关键特征的模型能够为高尔夫挥杆分析提供更多新见解，并促进学术界和运动产业的相关研究与实践应用。

Abstract: Recent advances in deep learning have led to more studies to enhance golfers'
shot precision. However, these existing studies have not quantitatively
established the relationship between swing posture and ball trajectory,
limiting their ability to provide golfers with the necessary insights for swing
improvement. In this paper, we propose a new dataset called CaddieSet, which
includes joint information and various ball information from a single shot.
CaddieSet extracts joint information from a single swing video by segmenting it
into eight swing phases using a computer vision-based approach. Furthermore,
based on expert golf domain knowledge, we define 15 key metrics that influence
a golf swing, enabling the interpretation of swing outcomes through
swing-related features. Through experiments, we demonstrated the feasibility of
CaddieSet for predicting ball trajectories using various benchmarks. In
particular, we focus on interpretable models among several benchmarks and
verify that swing feedback using our joint features is quantitatively
consistent with established domain knowledge. This work is expected to offer
new insight into golf swing analysis for both academia and the sports industry.

</details>


### [35] [IAENet: An Importance-Aware Ensemble Model for 3D Point Cloud-Based Anomaly Detection](https://arxiv.org/abs/2508.20492)
*Xuanming Cao,Chengyu Tao,Yifeng Cheng,Juan Du*

Main category: cs.CV

TL;DR: 本文提出了一种结合2D图像与3D点云专家模型、用于工业表面异常检测的新型集成网络IAENet，有效提升了检测性能并降低了误报率。


<details>
  <summary>Details</summary>
Motivation: 虽然2D方法在表面异常检测中表现优异，但3D点云检测因缺乏功能强大的预训练骨干模型发展缓慢，难以充分利用丰富几何信息。作者认为当前瓶颈即在此。

Method: 提出IAENet，将2D预训练模型（专家）和3D模型集成，并创新性地引入了Importance-Aware Fusion（IAF）模块，该模块通过动态评估各来源的重要性对异常分数加权，同时设计了能显式优化IAF的loss函数，兼顾专家知识融合与个体优势保留。

Result: 在MVTec 3D-AD数据集上，IAENet取得了新的SOTA表现，尤其在降低误报率方面表现突出。

Conclusion: IAENet不仅桥接了2D与3D在表面异常检测上的能力，实现了更优检测和更低误报，也为工业实际应用提供了高效可行的新方案。

Abstract: Surface anomaly detection is pivotal for ensuring product quality in
industrial manufacturing. While 2D image-based methods have achieved remarkable
success, 3D point cloud-based detection remains underexplored despite its
richer geometric cues. We argue that the key bottleneck is the absence of
powerful pretrained foundation backbones in 3D comparable to those in 2D. To
bridge this gap, we propose Importance-Aware Ensemble Network (IAENet), an
ensemble framework that synergizes 2D pretrained expert with 3D expert models.
However, naively fusing predictions from disparate sources is non-trivial:
existing strategies can be affected by a poorly performing modality and thus
degrade overall accuracy. To address this challenge, We introduce an novel
Importance-Aware Fusion (IAF) module that dynamically assesses the contribution
of each source and reweights their anomaly scores. Furthermore, we devise
critical loss functions that explicitly guide the optimization of IAF, enabling
it to combine the collective knowledge of the source experts but also preserve
their unique strengths, thereby enhancing the overall performance of anomaly
detection. Extensive experiments on MVTec 3D-AD demonstrate that our IAENet
achieves a new state-of-the-art with a markedly lower false positive rate,
underscoring its practical value for industrial deployment.

</details>


### [36] [Describe, Don't Dictate: Semantic Image Editing with Natural Language Intent](https://arxiv.org/abs/2508.20505)
*En Ci,Shanyan Guan,Yanhao Ge,Yilin Zhang,Wei Li,Zhenyu Zhang,Jian Yang,Ying Tai*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于描述性提示的图像编辑框架DescriptiveEdit，通过引入参考图像，将图像编辑任务转化为类似文本生成图像的问题，从而提升了编辑的准确性与一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成取得了进展，但语义图像编辑仍面临重构误差及数据集规模和质量受限的问题。

Method: 作者提出将基于指令的图像编辑重构为‘基于参考图像的文本到图像生成’任务，设计了Cross-Attentive UNet模型，通过attention bridges融合参考图像特征，实现无需反演或架构大改的高质量图像编辑。

Result: 在Emu Edit基准上实验结果显示，DescriptiveEdit提升了图像编辑的准确性与一致性，对现有文本到图像模型和扩展如ControlNet等具有良好兼容性与扩展性。

Conclusion: DescriptiveEdit方法有效克服了以往在图像编辑中的精度和数据受限问题，提升了编辑效果，为大规模应用和集成带来了新的可能性。

Abstract: Despite the progress in text-to-image generation, semantic image editing
remains a challenge. Inversion-based algorithms unavoidably introduce
reconstruction errors, while instruction-based models mainly suffer from
limited dataset quality and scale. To address these problems, we propose a
descriptive-prompt-based editing framework, named DescriptiveEdit. The core
idea is to re-frame `instruction-based image editing' as `reference-image-based
text-to-image generation', which preserves the generative power of well-trained
Text-to-Image models without architectural modifications or inversion.
Specifically, taking the reference image and a prompt as input, we introduce a
Cross-Attentive UNet, which newly adds attention bridges to inject reference
image features into the prompt-to-edit-image generation process. Owing to its
text-to-image nature, DescriptiveEdit overcomes limitations in instruction
dataset quality, integrates seamlessly with ControlNet, IP-Adapter, and other
extensions, and is more scalable. Experiments on the Emu Edit benchmark show it
improves editing accuracy and consistency.

</details>


### [37] [DCFS: Continual Test-Time Adaptation via Dual Consistency of Feature and Sample](https://arxiv.org/abs/2508.20516)
*Wenting Yin,Han Sun,Xinru Meng,Ningzhong Liu,Huiyu Zhou*

Main category: cs.CV

TL;DR: 本论文提出了一种新的持续测试时自适应（CTTA）方法DCFS，通过双路径特征一致性和置信感知样本学习，有效提升无源数据条件下的模型适应能力，并在多组数据集上验证了优越表现。


<details>
  <summary>Details</summary>
Motivation: 现有CTTA方法在无源数据情况下过度依赖目标域特征，容易引入学习偏差且伪标签误差累积。作者欲解决伪标签质量不高和无源域数据下模型适应性不足的问题。

Method: 提出了DCFS框架：1）使用双分类器将目标数据特征解耦为语义相关特征和域相关特征，通过保持子特征与整体特征间一致性，多角度捕捉数据特点；2）为每个样本设定自适应阈值，计算置信分数，采用损失加权的自监督学习，减少伪标签噪声和误差积累问题。

Result: 在CIFAR10-C、CIFAR100-C及ImageNet-C等多个数据集上进行了大量实验，验证了该方法在CTTA场景下的持续稳定效能提升。

Conclusion: DCFS能够在无源数据的持续测试时自适应问题中，有效提取目标域特征、降低伪标签噪声和误差积累，展现出优于现有方法的稳健性能。

Abstract: Continual test-time adaptation aims to continuously adapt a pre-trained model
to a stream of target domain data without accessing source data. Without access
to source domain data, the model focuses solely on the feature characteristics
of the target data. Relying exclusively on these features can lead to confusion
and introduce learning biases. Currently, many existing methods generate
pseudo-labels via model predictions. However, the quality of pseudo-labels
cannot be guaranteed and the problem of error accumulation must be solved. To
address these challenges, we propose DCFS, a novel CTTA framework that
introduces dual-path feature consistency and confidence-aware sample learning.
This framework disentangles the whole feature representation of the target data
into semantic-related feature and domain-related feature using dual classifiers
to learn distinct feature representations. By maintaining consistency between
the sub-features and the whole feature, the model can comprehensively capture
data features from multiple perspectives. Additionally, to ensure that the
whole feature information of the target domain samples is not overlooked, we
set a adaptive threshold and calculate a confidence score for each sample to
carry out loss weighted self-supervised learning, effectively reducing the
noise of pseudo-labels and alleviating the problem of error accumulation. The
efficacy of our proposed method is validated through extensive experimentation
across various datasets, including CIFAR10-C, CIFAR100-C, and ImageNet-C,
demonstrating consistent performance in continual test-time adaptation
scenarios.

</details>


### [38] [Adam SLAM - the last mile of camera calibration with 3DGS](https://arxiv.org/abs/2508.20526)
*Matthieu Gendrin,Stéphane Pateux,Xiaoran Jiang,Théo Ladune,Luce Morin*

Main category: cs.CV

TL;DR: 本文提出利用3DGS模型通过对新视角合成颜色损失的反向传播，微调摄像机标定参数，有效提升新视角渲染的质量。


<details>
  <summary>Details</summary>
Motivation: 摄像机标定质量对新视角合成结果至关重要，微小的标定误差都可能显著影响重建质量。然而，真实场景中缺乏标定的真实标签，只能通过新视角渲染的效果间接评估标定的优劣。提升标定质量对高质量新视角生成具有重要意义。

Method: 利用3DGS模型，将新视角颜色损失对摄像机参数反向传播，从而在训练过程中不断微调摄像机标定参数，提高标定精度。

Result: 该方法在3DGS参考数据集上，单独采用新的标定方式即可将PSNR平均提升0.4 dB。

Conclusion: 通过这种基于3DGS和颜色损失反向传播的新型标定微调方法，可以有效提升新视角合成质量，尤其适用于如Mip-NeRF 360等对视角合成质量极为敏感的基准场景，尽管这种方法可能较为耗时。

Abstract: The quality of the camera calibration is of major importance for evaluating
progresses in novel view synthesis, as a 1-pixel error on the calibration has a
significant impact on the reconstruction quality. While there is no ground
truth for real scenes, the quality of the calibration is assessed by the
quality of the novel view synthesis. This paper proposes to use a 3DGS model to
fine tune calibration by backpropagation of novel view color loss with respect
to the cameras parameters. The new calibration alone brings an average
improvement of 0.4 dB PSNR on the dataset used as reference by 3DGS. The fine
tuning may be long and its suitability depends on the criticity of training
time, but for calibration of reference scenes, such as Mip-NeRF 360, the stake
of novel view quality is the most important.

</details>


### [39] [Learning What is Worth Learning: Active and Sequential Domain Adaptation for Multi-modal Gross Tumor Volume Segmentation](https://arxiv.org/abs/2508.20528)
*Jingyun Yang,Guoqing Zhang,Jingge Wang,Yang Li*

Main category: cs.CV

TL;DR: 本文提出了一种用于动态多模态样本选择的主动顺序领域自适应（ADA）新框架，用于提升医学影像中肿瘤体积的分割精度，尤其是在标注数据有限的情况下降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 肿瘤体积分割在肿瘤放射治疗规划中至关重要。然而，标注医学影像数据代价高昂，获取高质量标签样本十分困难。现有的主动领域自适应方法虽然可以一定程度减少标注需求，但容易发生负迁移，且很少关注多模态数据的样本选择。解决上述问题对于提升医学影像分割任务具有重要意义。

Method: 作者提出一个主动且顺序的领域自适应框架，设计了基于样本的信息性和代表性的新型多模态样本查询策略，对最有价值的样本优先标注和训练。该方法能够动态适应不同的多模态医学数据分布，避免传统方法一次性选择样本所带来的负迁移风险。

Result: 在多种肿瘤体积分割任务中进行实验证明，该方法在准确性和效率上均优于现有最先进的主动领域自适应方法，显著提升了分割性能。

Conclusion: 所提方法有效降低了医学图像分割任务中的标注成本，并提升了模型的泛化能力，为主动学习和多模态领域自适应提供了新的思路。

Abstract: Accurate gross tumor volume segmentation on multi-modal medical data is
critical for radiotherapy planning in nasopharyngeal carcinoma and
glioblastoma. Recent advances in deep neural networks have brought promising
results in medical image segmentation, leading to an increasing demand for
labeled data. Since labeling medical images is time-consuming and
labor-intensive, active learning has emerged as a solution to reduce annotation
costs by selecting the most informative samples to label and adapting
high-performance models with as few labeled samples as possible. Previous
active domain adaptation (ADA) methods seek to minimize sample redundancy by
selecting samples that are farthest from the source domain. However, such
one-off selection can easily cause negative transfer, and access to source
medical data is often limited. Moreover, the query strategy for multi-modal
medical data remains unexplored. In this work, we propose an active and
sequential domain adaptation framework for dynamic multi-modal sample selection
in ADA. We derive a query strategy to prioritize labeling and training on the
most valuable samples based on their informativeness and representativeness.
Empirical validation on diverse gross tumor volume segmentation tasks
demonstrates that our method achieves favorable segmentation performance,
significantly outperforming state-of-the-art ADA methods. Code is available at
the git repository: \href{https://github.com/Hiyoochan/mmActS}{mmActS}.

</details>


### [40] [Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for Unsupervised 3D Object Detection](https://arxiv.org/abs/2508.20530)
*Mingqian Ji,Jian Yang,Shanshan Zhang*

Main category: cs.CV

TL;DR: 该论文提出利用数据级融合，将RGB图像与LiDAR数据在早期集成，从而提升无监督3D目标检测性能。通过引入视觉基础模型、双向融合与高效去噪方法，大幅优化伪标签质量，实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于LiDAR的3D目标检测高度依赖人工标注，成本高且效率低。近期无监督3D目标检测尝试融合RGB图像提升伪标签，但传统标签层级融合忽略了两种模态数据的互补性，提升有限。因此，急需更深层次的数据融合方式以进一步提升伪标签质量和检测性能。

Method: 作者提出数据级融合框架，将RGB图像与LiDAR在初始阶段集成。具体做法包括：1）利用视觉基础模型实现图像实例分割和深度估计，2）设计双向融合方法，实现2D像素与3D点云信息的互补映射，3）提出局部半径与全局统计两级去噪策略，抑制深度误差与分割异常，4）基于数据级融合引入动态自进化策略，利用密集表示迭代优化伪标签。

Result: 在nuScenes数据集上的实验表明，所提方法训练的无监督检测器在nuScenes验证基准上达到了28.4%的mAP，显著优于此前的主流方法。

Conclusion: 本方法通过创新的数据级融合和自进化机制，极大提升了无监督3D目标检测效果，为降低高质量3D标注依赖提供了新思路。

Abstract: Existing LiDAR-based 3D object detectors typically rely on manually annotated
labels for training to achieve good performance. However, obtaining
high-quality 3D labels is time-consuming and labor-intensive. To address this
issue, recent works explore unsupervised 3D object detection by introducing RGB
images as an auxiliary modal to assist pseudo-box generation. However, these
methods simply integrate pseudo-boxes generated by LiDAR point clouds and RGB
images. Yet, such a label-level fusion strategy brings limited improvements to
the quality of pseudo-boxes, as it overlooks the complementary nature in terms
of LiDAR and RGB image data. To overcome the above limitations, we propose a
novel data-level fusion framework that integrates RGB images and LiDAR data at
an early stage. Specifically, we utilize vision foundation models for instance
segmentation and depth estimation on images and introduce a bi-directional
fusion method, where real points acquire category labels from the 2D space,
while 2D pixels are projected onto 3D to enhance real point density. To
mitigate noise from depth and segmentation estimations, we propose a local and
global filtering method, which applies local radius filtering to suppress depth
estimation errors and global statistical filtering to remove
segmentation-induced outliers. Furthermore, we propose a data-level fusion
based dynamic self-evolution strategy, which iteratively refines pseudo-boxes
under a dense representation, significantly improving localization accuracy.
Extensive experiments on the nuScenes dataset demonstrate that the detector
trained by our method significantly outperforms that trained by previous
state-of-the-art methods with 28.4$\%$ mAP on the nuScenes validation
benchmark.

</details>


### [41] [Digital Scale: Open-Source On-Device BMI Estimation from Smartphone Camera Images Trained on a Large-Scale Real-World Dataset](https://arxiv.org/abs/2508.20534)
*Frederik Rajiv Manichand,Robin Deuber,Robert Jakob,Steve Swerling,Jamie Rosen,Elgar Fleisch,Patrick Langer*

Main category: cs.CV

TL;DR: 本研究提出了一种基于深度学习的BMI估算方法，通过大规模手机图像数据集WayBED进行训练，实现了目前文献中最低的MAPE，并在多数据集上表现出强大的泛化能力，支持移动设备部署并开源全部代码。


<details>
  <summary>Details</summary>
Motivation: 在远程医疗或紧急情况下，传统体重测量手段不可用时，利用摄像头图像估算BMI可实现快速体重评估。但现有视觉方法数据集较小，限制了模型表现和泛化能力。本研究旨在拓展数据集规模并提升估算精度。

Method: 采集并构建了包含84,963张图像的WayBED数据集，开发了一个自动图像过滤流程，通过姿态聚类和行人人体检测清洗掉低质量图像，从而得到71,322张高质量图片。随后用这些数据训练深度学习模型进行BMI预测，并在多个数据集上测试泛化性。同时，该技术还通过CLAID框架部署在安卓终端，并开源所有代码和工具。

Result: 在WayBED测试集上BMI估算达到了7.9%的MAPE（为目前文献最低），在从未见过的VisualBodyToBMI数据集上MAPE为13%，与主流方法持平。对该数据集微调后，MAPE进一步降低至8.56%，为目前最低值。支持移动端实时部署。

Conclusion: 本研究极大提升了基于计算机视觉的BMI估算精度与泛化性，对推进远程健康监测和实际场景应用具有重要意义，并通过开源代码推动领域发展。

Abstract: Estimating Body Mass Index (BMI) from camera images with machine learning
models enables rapid weight assessment when traditional methods are unavailable
or impractical, such as in telehealth or emergency scenarios. Existing computer
vision approaches have been limited to datasets of up to 14,500 images. In this
study, we present a deep learning-based BMI estimation method trained on our
WayBED dataset, a large proprietary collection of 84,963 smartphone images from
25,353 individuals. We introduce an automatic filtering method that uses
posture clustering and person detection to curate the dataset by removing
low-quality images, such as those with atypical postures or incomplete views.
This process retained 71,322 high-quality images suitable for training. We
achieve a Mean Absolute Percentage Error (MAPE) of 7.9% on our hold-out test
set (WayBED data) using full-body images, the lowest value in the published
literature to the best of our knowledge. Further, we achieve a MAPE of 13% on
the completely unseen~(during training) VisualBodyToBMI dataset, comparable
with state-of-the-art approaches trained on it, demonstrating robust
generalization. Lastly, we fine-tune our model on VisualBodyToBMI and achieve a
MAPE of 8.56%, the lowest reported value on this dataset so far. We deploy the
full pipeline, including image filtering and BMI estimation, on Android devices
using the CLAID framework. We release our complete code for model training,
filtering, and the CLAID package for mobile deployment as open-source
contributions.

</details>


### [42] [Domain Adaptation Techniques for Natural and Medical Image Classification](https://arxiv.org/abs/2508.20537)
*Ahmad Chaddad,Yihang Wu,Reem Kateb,Christian Desrosiers*

Main category: cs.CV

TL;DR: 本文系统评估了7种领域自适应（DA）技术在5个自然图像和8个医学图像数据集中的表现，包括常规分布外、动态数据流和有限训练样本等场景，揭示了DA技术在医学和自然图像上的效用，特别强调了DSAN算法的突出表现。


<details>
  <summary>Details</summary>
Motivation: 大多数DA技术在自然图像上的研究较多，而医学图像研究较少。同时，主流数据集可能导致性能偏差。为全面理解DA的适用性和优势，作者系统比较了多种DA技术在医学和自然图像场景下的有效性。

Method: 作者在5个自然数据集和8个医学数据集上，针对不同场景（分布外测试、动态数据流、训练样本有限）使用7种主流DA技术共进行557组仿真实验，详细对比分析其性能，尤其关注了DSAN算法，并采用Resnet50做底座测试。

Result: DSAN算法在COVID-19数据集上使用Resnet50获得了91.2%的分类准确率，在动态数据流场景下较基线提升了6.7%，在COVID-19和皮肤癌数据集上表现出极好的可解释性。

Conclusion: 研究证明了领域自适应技术在医学图像分析中的有效性，DSAN算法在多场景下表现优异。此项工作加深了对DA方法在医学数据适配上的理解，并为未来医学影像领域模型适应性设计提供了有价值的参考。

Abstract: Domain adaptation (DA) techniques have the potential in machine learning to
alleviate distribution differences between training and test sets by leveraging
information from source domains. In image classification, most advances in DA
have been made using natural images rather than medical data, which are harder
to work with. Moreover, even for natural images, the use of mainstream datasets
can lead to performance bias. {With the aim of better understanding the
benefits of DA for both natural and medical images, this study performs 557
simulation studies using seven widely-used DA techniques for image
classification in five natural and eight medical datasets that cover various
scenarios, such as out-of-distribution, dynamic data streams, and limited
training samples.} Our experiments yield detailed results and insightful
observations highlighting the performance and medical applicability of these
techniques. Notably, our results have shown the outstanding performance of the
Deep Subdomain Adaptation Network (DSAN) algorithm. This algorithm achieved
feasible classification accuracy (91.2\%) in the COVID-19 dataset using
Resnet50 and showed an important accuracy improvement in the dynamic data
stream DA scenario (+6.7\%) compared to the baseline. Our results also
demonstrate that DSAN exhibits remarkable level of explainability when
evaluated on COVID-19 and skin cancer datasets. These results contribute to the
understanding of DA techniques and offer valuable insight into the effective
adaptation of models to medical data.

</details>


### [43] [Contrastive Learning through Auxiliary Branch for Video Object Detection](https://arxiv.org/abs/2508.20551)
*Lucas Rakotoarivony*

Main category: cs.CV

TL;DR: 该论文提出了一种简洁有效的视频目标检测方法CLAB，通过对比学习辅助分支提升检测器的特征表达能力，无需额外推理计算与后处理即可获得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 视频目标检测比静态图像目标检测更具挑战性，因视频中存在运动模糊、遮挡及形变等问题。现有方法虽能提升性能但计算量大。该研究旨在无需增加推理计算复杂度的前提下提升检测鲁棒性。

Method: 提出对比学习辅助分支（CLAB）。具体做法为：1）在主检测网络骨干旁增加辅助分支并采用对比损失训练，以强化特征学习能力；2）动态调整损失权重，训练初期侧重辅助特征分支，后期逐步侧重检测主任务。

Result: 在ImageNet VID数据集上，CLAB结合ResNet-101和ResNeXt-101骨干网络，分别达到84.0%和85.2%的mAP，实现了在CNN模型中领先的性能，无需复杂后处理。

Conclusion: CLAB方法为视频目标检测提供了新思路，以极低的推理成本提升了鲁棒性与准确性，且实验验证了该方法在现有CNN架构下显著有效。

Abstract: Video object detection is a challenging task because videos often suffer from
image deterioration such as motion blur, occlusion, and deformable shapes,
making it significantly more difficult than detecting objects in still images.
Prior approaches have improved video object detection performance by employing
feature aggregation and complex post-processing techniques, though at the cost
of increased computational demands. To improve robustness to image degradation
without additional computational load during inference, we introduce a
straightforward yet effective Contrastive Learning through Auxiliary Branch
(CLAB) method. First, we implement a constrastive auxiliary branch using a
contrastive loss to enhance the feature representation capability of the video
object detector's backbone. Next, we propose a dynamic loss weighting strategy
that emphasizes auxiliary feature learning early in training while gradually
prioritizing the detection task as training converges. We validate our approach
through comprehensive experiments and ablation studies, demonstrating
consistent performance gains. Without bells and whistles, CLAB reaches a
performance of 84.0% mAP and 85.2% mAP with ResNet-101 and ResNeXt-101,
respectively, on the ImageNet VID dataset, thus achieving state-of-the-art
performance for CNN-based models without requiring additional post-processing
methods.

</details>


### [44] [Towards Mechanistic Defenses Against Typographic Attacks in CLIP](https://arxiv.org/abs/2508.20570)
*Lorenz Hufe,Constantin Venhoff,Maximilian Dreyer,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.CV

TL;DR: 本文分析了CLIP视觉编码器在排版攻击（typographic attacks）下的表现，提出了一种无需微调即可提升模型鲁棒性的防御方法，并公开了一系列更能抵抗此类攻击的CLIP模型。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型如CLIP容易受排版攻击的影响，即攻击者通过在图片中植入文本，诱导模型产生错误分类或生成恶意内容，甚至导致系统被攻破。提升模型对这类攻击的鲁棒性具有重要的现实意义，尤其是在安全关键场景中。

Method: 研究通过定位CLIP模型高层中对排版信息敏感的注意力头，将它们识别为“排版电路”。防御方法则是选择性地移除这些注意力头，无需模型微调，直接改变推理流程，提高鲁棒性。

Result: 在排版变体ImageNet-100任务中，防御方法可提升目标任务性能最高19.6%，同时标准任务的准确率下降不足1%。与需要微调的现有方法相比，该方法在不经训练的情况下仍具有竞争力。

Conclusion: 提出的训练无关防御策略可大幅增强CLIP对排版攻击的抵抗力，所提供的“dyslexic CLIP”模型在需防范文本操纵风险的应用中可直接替代标准模型，保障多模态系统安全。

Abstract: Typographic attacks exploit multi-modal systems by injecting text into
images, leading to targeted misclassifications, malicious content generation
and even Vision-Language Model jailbreaks. In this work, we analyze how CLIP
vision encoders behave under typographic attacks, locating specialized
attention heads in the latter half of the model's layers that causally extract
and transmit typographic information to the cls token. Building on these
insights, we introduce a method to defend CLIP models against typographic
attacks by selectively ablating a typographic circuit, consisting of attention
heads. Without requiring finetuning, our method improves performance by up to
19.6% on a typographic variant of ImageNet-100, while reducing standard
ImageNet-100 accuracy by less than 1%. Notably, our training-free approach
remains competitive with current state-of-the-art typographic defenses that
rely on finetuning. To this end, we release a family of dyslexic CLIP models
which are significantly more robust against typographic attacks. These models
serve as suitable drop-in replacements for a broad range of safety-critical
applications, where the risks of text-based manipulation outweigh the utility
of text recognition.

</details>


### [45] [GLaRE: A Graph-based Landmark Region Embedding Network for Emotion Recognition](https://arxiv.org/abs/2508.20579)
*Debasis Maji,Debaditya Barman*

Main category: cs.CV

TL;DR: 本文提出了一种基于图神经网络的面部表情识别方法，通过对面部关键点区域进行嵌入，显著提升了在主流数据集上的识别效果。


<details>
  <summary>Details</summary>
Motivation: 传统面部表情识别方法在面对遮挡、表情多样性及可解释性等问题时表现不佳，亟需新的方法提升其性能和解释能力。

Method: 作者提出了GLaRE网络，结合3D面部对齐提取关键点，并通过层次化粗化方法构建商图，将面部区域结构有效建模并简化复杂度，实现区域级的图嵌入学习。

Result: 方法在AffectNet数据集上取得64.89%的准确率，在FERG数据集上取得94.24%的准确率，均优于现有部分主流方法。消融实验表明，引入基于商图的区域嵌入显著提升了预测效果。

Conclusion: GLaRE方法有效利用面部区域结构信息，提升了表情识别的准确率和可解释性，为面向现实应用的表情识别提供了有力支持。

Abstract: Facial expression recognition (FER) is a crucial task in computer vision with
wide range of applications including human computer interaction, surveillance,
and assistive technologies. However, challenges such as occlusion, expression
variability, and lack of interpretability hinder the performance of traditional
FER systems. Graph Neural Networks (GNNs) offer a powerful alternative by
modeling relational dependencies between facial landmarks, enabling structured
and interpretable learning. In this paper, we propose GLaRE, a novel
Graph-based Landmark Region Embedding network for emotion recognition. Facial
landmarks are extracted using 3D facial alignment, and a quotient graph is
constructed via hierarchical coarsening to preserve spatial structure while
reducing complexity. Our method achieves 64.89 percentage accuracy on AffectNet
and 94.24 percentage on FERG, outperforming several existing baselines.
Additionally, ablation studies have demonstrated that region-level embeddings
from quotient graphs have contributed to improved prediction performance.

</details>


### [46] [FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models](https://arxiv.org/abs/2508.20586)
*Zheng Chong,Yanwei Lei,Shiyue Zhang,Zhuandi He,Zhen Wang,Xujie Zhang,Xiao Dong,Yiling Wu,Dongmei Jiang,Xiaodan Liang*

Main category: cs.CV

TL;DR: 本文提出了FastFit，一种高效支持多参考服装和配饰虚拟试穿的扩散模型架构，通过缓存参考特征极大提升推理效率，并公布了新多参考数据集DressCode-MR。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿方法无法支持多参考服装与配饰组合，且在去噪过程中重复计算参考特征，导致效率低下，阻碍了实际应用。

Method: 提出了FastFit框架，采用可缓存的扩散模型架构，引入半注意力机制与类别嵌入，彻底解耦了参考特征编码与去噪过程，仅需一次计算并复用参考特征，显著提高效率。

Result: FastFit在VITON-HD、DressCode和自建DressCode-MR等数据集上，推理速度比同类方法快3.5倍，且在效果一致性等评测指标上超越现有技术。

Conclusion: FastFit不仅提升了多参考虚拟试穿的效率与表现，还为该领域研究提供了新的大规模数据集，有助于推动虚拟试穿技术的落地和发展。

Abstract: Despite its great potential, virtual try-on technology is hindered from
real-world application by two major challenges: the inability of current
methods to support multi-reference outfit compositions (including garments and
accessories), and their significant inefficiency caused by the redundant
re-computation of reference features in each denoising step. To address these
challenges, we propose FastFit, a high-speed multi-reference virtual try-on
framework based on a novel cacheable diffusion architecture. By employing a
Semi-Attention mechanism and substituting traditional timestep embeddings with
class embeddings for reference items, our model fully decouples reference
feature encoding from the denoising process with negligible parameter overhead.
This allows reference features to be computed only once and losslessly reused
across all steps, fundamentally breaking the efficiency bottleneck and
achieving an average 3.5x speedup over comparable methods. Furthermore, to
facilitate research on complex, multi-reference virtual try-on, we introduce
DressCode-MR, a new large-scale dataset. It comprises 28,179 sets of
high-quality, paired images covering five key categories (tops, bottoms,
dresses, shoes, and bags), constructed through a pipeline of expert models and
human feedback refinement. Extensive experiments on the VITON-HD, DressCode,
and our DressCode-MR datasets show that FastFit surpasses state-of-the-art
methods on key fidelity metrics while offering its significant advantage in
inference efficiency.

</details>


### [47] [UTA-Sign: Unsupervised Thermal Video Augmentation via Event-Assisted Traffic Signage Sketching](https://arxiv.org/abs/2508.20594)
*Yuqi Han,Songqian Zhang,Weijian Su,Ke Li,Jiayu Yang,Jinli Suo,Qiang Zhang*

Main category: cs.CV

TL;DR: 本文提出了UTA-Sign方法，通过结合热成像和神经形态视觉（事件相机）技术，提升低光照环境下交通标识（如车牌和路障指示器）的检测与识别能力。


<details>
  <summary>Details</summary>
Motivation: 热成像相机虽擅长在低光下环境感知，但难以区分由相同材料制成的标识，可能导致自动驾驶系统语义理解安全隐患。而事件相机能有效捕捉高速、低照环境下的变化信息，但采样不均。两者互为补充，融合后可弥补各自缺陷。

Method: 提出一种无监督的热-事件视频增强方案，利用双提升机制融合热成像帧和事件信号。具体做法是热成像帧作为参考提供准确的时序运动线索，对齐变化不均的事件信号，事件信号则为热成像帧补充细微的标识内容。

Result: 在真实场景数据集上进行验证，方法能够提升交通标识描绘质量，并在感知层面提高了检测精度。

Conclusion: 融合热成像和事件相机的增强方法，有效提高了夜间等低光环境下交通标识的识别和感知表现，有潜力提升自主驾驶系统安全性。

Abstract: The thermal camera excels at perceiving outdoor environments under low-light
conditions, making it ideal for applications such as nighttime autonomous
driving and unmanned navigation. However, thermal cameras encounter challenges
when capturing signage from objects made of similar materials, which can pose
safety risks for accurately understanding semantics in autonomous driving
systems. In contrast, the neuromorphic vision camera, also known as an event
camera, detects changes in light intensity asynchronously and has proven
effective in high-speed, low-light traffic environments. Recognizing the
complementary characteristics of these two modalities, this paper proposes
UTA-Sign, an unsupervised thermal-event video augmentation for traffic signage
in low-illumination environments, targeting elements such as license plates and
roadblock indicators. To address the signage blind spots of thermal imaging and
the non-uniform sampling of event cameras, we developed a dual-boosting
mechanism that fuses thermal frames and event signals for consistent signage
representation over time. The proposed method utilizes thermal frames to
provide accurate motion cues as temporal references for aligning the uneven
event signals. At the same time, event signals contribute subtle signage
content to the raw thermal frames, enhancing the overall understanding of the
environment. The proposed method is validated on datasets collected from
real-world scenarios, demonstrating superior quality in traffic signage
sketching and improved detection accuracy at the perceptual level.

</details>


### [48] [Disruptive Attacks on Face Swapping via Low-Frequency Perceptual Perturbations](https://arxiv.org/abs/2508.20595)
*Mengxiao Huang,Minglei Shu,Shuwang Zhou,Zhaoyang Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于低频感知扰动的主动防御方法，能有效破坏深度伪造（如换脸）生成效果，同时保持图像视觉真实性。新方法显著降低了换脸攻击的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的深度伪造检测方法多为被动检测，难以及时阻止深伪内容的生成和传播，对隐私和社会安全造成威胁。本文旨在通过主动干预，防止或减少深伪内容在生成过程中的自然度和有效性。

Method: 提出一种基于低频扰动的主动防御框架，通过结合频域与空间域特征，在人脸图片中引入低频扰动文理，影响深伪生成模型。框架采用离散小波变换（DWT）提取低频特征，并通过编码器、扰动生成器和解码器结构生成扰动，插入到原始图片中。这样可影响深度伪造模型自身的生成过程。

Result: 在CelebA-HQ和LFW数据集上实验表明，提出的方法显著降低了换脸等深伪生成的自然度与准确性，提高了防御攻击的成功率，同时保证了图片的视觉质量基本不受影响。

Conclusion: 所提基于低频感知扰动的主动防御方法可有效干扰深度伪造生成，提供比传统被动检测更优的安全防护范式，对未来深伪安全领域具有借鉴意义。

Abstract: Deepfake technology, driven by Generative Adversarial Networks (GANs), poses
significant risks to privacy and societal security. Existing detection methods
are predominantly passive, focusing on post-event analysis without preventing
attacks. To address this, we propose an active defense method based on
low-frequency perceptual perturbations to disrupt face swapping manipulation,
reducing the performance and naturalness of generated content. Unlike prior
approaches that used low-frequency perturbations to impact classification
accuracy,our method directly targets the generative process of deepfake
techniques. We combine frequency and spatial domain features to strengthen
defenses. By introducing artifacts through low-frequency perturbations while
preserving high-frequency details, we ensure the output remains visually
plausible. Additionally, we design a complete architecture featuring an
encoder, a perturbation generator, and a decoder, leveraging discrete wavelet
transform (DWT) to extract low-frequency components and generate perturbations
that disrupt facial manipulation models. Experiments on CelebA-HQ and LFW
demonstrate significant reductions in face-swapping effectiveness, improved
defense success rates, and preservation of visual quality.

</details>


### [49] [Embracing Aleatoric Uncertainty: Generating Diverse 3D Human Motion](https://arxiv.org/abs/2508.20604)
*Zheng Qin,Yabing Wang,Minghui Yang,Sanping Zhou,Ming Yang,Le Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的文本到3D人体动作生成方法Diverse-T2M，重点提升生成动作的多样性，并保证文本语义一致性。该方法通过引入噪声信号和潜在空间采样，克服了现有方法难以生成多样化动作的挑战。实验表明，Diverse-T2M在保证与文本一致性的同时，显著提升了动作的多样性。


<details>
  <summary>Details</summary>
Motivation: 尽管现有方法能高质量地根据文本生成3D人体动作，但生成动作往往缺乏多样性。本文关注如何在保证文本一致性的同时，提高动作输出的多样性。

Method: 提出Diverse-T2M方法，主要有两点创新：第一，在基于Transformer的方法中引入噪声信号，作为多样性信息的载体，显式建模生成过程中的不确定性；第二，将文本映射到潜在空间的连续表征，结合潜在空间采样机制，为动作生成引入随机性。

Result: 在主流数据集HumanML3D和KIT-ML上实验表明，该方法在保持文本一致性的同时，生成动作的多样性显著提升，性能达到甚至超过当前最优方法。

Conclusion: Diverse-T2M方法成功提升了文本驱动3D人体动作生成的多样性且不损失语义一致性，为相关任务提供了更优的生成范式。

Abstract: Generating 3D human motions from text is a challenging yet valuable task. The
key aspects of this task are ensuring text-motion consistency and achieving
generation diversity. Although recent advancements have enabled the generation
of precise and high-quality human motions from text, achieving diversity in the
generated motions remains a significant challenge. In this paper, we aim to
overcome the above challenge by designing a simple yet effective text-to-motion
generation method, \textit{i.e.}, Diverse-T2M. Our method introduces
uncertainty into the generation process, enabling the generation of highly
diverse motions while preserving the semantic consistency of the text.
Specifically, we propose a novel perspective that utilizes noise signals as
carriers of diversity information in transformer-based methods, facilitating a
explicit modeling of uncertainty. Moreover, we construct a latent space where
text is projected into a continuous representation, instead of a rigid
one-to-one mapping, and integrate a latent space sampler to introduce
stochastic sampling into the generation process, thereby enhancing the
diversity and uncertainty of the outputs. Our results on text-to-motion
generation benchmark datasets~(HumanML3D and KIT-ML) demonstrate that our
method significantly enhances diversity while maintaining state-of-the-art
performance in text consistency.

</details>


### [50] [Optimization-Based Calibration for Intravascular Ultrasound Volume Reconstruction](https://arxiv.org/abs/2508.20605)
*Karl-Philippe Beaudet,Sidaty El Hadramy,Philippe C Cattin,Juan Verde,Stéphane Cotin*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D打印仿体的优化校准方法，实现术中三维血管内超声（IVUS）数据与术前CT图像的精确配准，从而提升肝脏手术导航的准确性。方法在猪肝体内实验中取得了可靠的校准和配准精度。


<details>
  <summary>Details</summary>
Motivation: 术中超声图像在肝脏手术中由于视野有限和解剖结构复杂而难以解读，因此需要更好地将术前和术中的数据关联起来以实现精准手术导航。

Method: 开发了一种利用3D打印仿体的优化校准流程，校准跟踪的三维IVUS数据，并实现其与术前CT图像的准确配准。在体猪肝图像上对方法进行了验证。

Result: 校准误差达0.88至1.80毫米，3D IVUS与对应CT图像之间的配准误差为3.40至5.71毫米，证明方法具有很高的准确性和可靠性。

Conclusion: 本方法能够为肝脏手术中超声图像与术前CT图像的融合配准提供精确且可靠的解决方案，提升了术中导航的水平，具有临床推广应用的前景。

Abstract: Intraoperative ultrasound images are inherently challenging to interpret in
liver surgery due to the limited field of view and complex anatomical
structures. Bridging the gap between preoperative and intraoperative data is
crucial for effective surgical guidance. 3D IntraVascular UltraSound (IVUS)
offers a potential solution by enabling the reconstruction of the entire organ,
which facilitates registration between preoperative computed tomography (CT)
scans and intraoperative IVUS images. In this work, we propose an
optimization-based calibration method using a 3D-printed phantom for accurate
3D Intravascular Ultrasound volume reconstruction. Our approach ensures precise
alignment of tracked IVUS data with preoperative CT images, improving
intraoperative navigation. We validated our method using in vivo swine liver
images, achieving a calibration error from 0.88 to 1.80 mm and a registration
error from 3.40 to 5.71 mm between the 3D IVUS data and the corresponding CT
scan. Our method provides a reliable and accurate means of calibration and
volume reconstruction. It can be used to register intraoperative ultrasound
images with preoperative CT images in the context of liver surgery, and enhance
intraoperative guidance.

</details>


### [51] [Physics Informed Generative Models for Magnetic Field Images](https://arxiv.org/abs/2508.20612)
*Aye Phyu Phyu Aung,Lucas Lum,Zhansen Shi,Wen Qiu,Bernice Zee,JM Chin,Yeow Kheng Lim,J. Senthilnath*

Main category: cs.CV

TL;DR: 这篇论文提出了一种基于物理约束的扩散生成模型（PI-GenMFI），能够合成半导体磁场成像（MFI）数据，以辅助缺陷检测和定位。


<details>
  <summary>Details</summary>
Motivation: 半导体制造过程缺陷检测对提高产品质量与良率至关重要。虽然X射线成像准确但耗时且占用内存大，MFI能更高效地定位目标区域辅助X射线扫描。然而，受限于专有数据集的稀缺，MFI缺乏足够样本训练机器学习模型，成为实际推广的难点。

Method: 文中提出了一种物理信息引导的生成模型（PI-GenMFI），将物理约束结合进扩散模型以生成合成MFI图像，专注于模拟最常见的缺陷类型——电源短路。生成的合成MFI图像用于训练缺陷定位相关的机器学习算法。此外，将该模型与当前主流的生成模型（如变分自编码器和扩散模型）做对比，并通过领域专家评价与图像、信号处理常用指标进行定性和定量分析。

Result: PI-GenMFI生成的合成MFI样本在图像质量、信号特性等评价指标上取得优异结果，并通过领域专家的主观评定获得认同。与SOTA方法对比，其在缺陷区域定位效率上表现出显著优化潜力。

Conclusion: 该方法有望缓解MFI数据稀缺问题，为训练高效实用的缺陷检测机器学习模型搭建了坚实基础，有助于推动半导体制造检测效率和智能化水平。

Abstract: In semiconductor manufacturing, defect detection and localization are
critical to ensuring product quality and yield. While X-ray imaging is a
reliable non-destructive testing method, it is memory-intensive and
time-consuming for large-scale scanning, Magnetic Field Imaging (MFI) offers a
more efficient means to localize regions of interest (ROI) for targeted X-ray
scanning. However, the limited availability of MFI datasets due to proprietary
concerns presents a significant bottleneck for training machine learning (ML)
models using MFI. To address this challenge, we consider an ML-driven approach
leveraging diffusion models with two physical constraints. We propose Physics
Informed Generative Models for Magnetic Field Images (PI-GenMFI) to generate
synthetic MFI samples by integrating specific physical information. We generate
MFI images for the most common defect types: power shorts. These synthetic
images will serve as training data for ML algorithms designed to localize
defect areas efficiently. To evaluate generated MFIs, we compare our model to
SOTA generative models from both variational autoencoder (VAE) and diffusion
methods. We present a domain expert evaluation to assess the generated samples.
In addition, we present qualitative and quantitative evaluation using various
metrics used for image generation and signal processing, showing promising
results to optimize the defect localization process.

</details>


### [52] [Revisiting the Privacy Risks of Split Inference: A GAN-Based Data Reconstruction Attack via Progressive Feature Optimization](https://arxiv.org/abs/2508.20613)
*Yixiang Qiu,Yanhan Liu,Hongyao Yu,Hao Fang,Bin Chen,Shu-Tao Xia,Ke Xu*

Main category: cs.CV

TL;DR: 本文提出了一种基于GAN的新型数据重建攻击（DRA）框架，并引入分层渐进特征优化和L1球约束，显著提升了在分割推理中从中间特征还原原始数据的能力。


<details>
  <summary>Details</summary>
Motivation: 现有DRA方法主要仅针对浅层模型有效，且受限于无法充分利用语义先验，导致重建质量和适应性不佳。深度网络与分割推理（SI）的广泛应用暴露出新的隐私风险，需有更强大的攻击方法来衡量SI的安全性。

Method: 提出了一种GAN（生成对抗网络）为基础的DRA框架，引入渐进特征优化（PFO），将生成器分解为层次化模块，逐步优化中间表征提升重建质量。同时，为改善优化稳定性和图像真实感，增加L1球约束以规范重建过程。

Result: 大量实验表明，所提方法在高分辨率场景、分布外数据集和更深、更复杂的神经网络架构上，均显著优于已有DRA攻击手段，能更高保真度重建输入数据。

Conclusion: 该方法提升了基于中间特征的数据重建攻击能力，揭示了SI范式在深层网络和高分辨率应用下的严重隐私威胁，为评估和加强相关隐私防护措施提供了新的基准。

Abstract: The growing complexity of Deep Neural Networks (DNNs) has led to the adoption
of Split Inference (SI), a collaborative paradigm that partitions computation
between edge devices and the cloud to reduce latency and protect user privacy.
However, recent advances in Data Reconstruction Attacks (DRAs) reveal that
intermediate features exchanged in SI can be exploited to recover sensitive
input data, posing significant privacy risks. Existing DRAs are typically
effective only on shallow models and fail to fully leverage semantic priors,
limiting their reconstruction quality and generalizability across datasets and
model architectures. In this paper, we propose a novel GAN-based DRA framework
with Progressive Feature Optimization (PFO), which decomposes the generator
into hierarchical blocks and incrementally refines intermediate representations
to enhance the semantic fidelity of reconstructed images. To stabilize the
optimization and improve image realism, we introduce an L1-ball constraint
during reconstruction. Extensive experiments show that our method outperforms
prior attacks by a large margin, especially in high-resolution scenarios,
out-of-distribution settings, and against deeper and more complex DNNs.

</details>


### [53] [EmoCAST: Emotional Talking Portrait via Emotive Text Description](https://arxiv.org/abs/2508.20615)
*Yiguo Jiang,Xiaodong Cun,Yong Zhang,Yudian Zheng,Fan Tang,Chi-Man Pun*

Main category: cs.CV

TL;DR: 本文提出了EmoCAST框架，实现了高质量、情感丰富且音频同步的说话人头像合成视频，在灵活性、自然性和表达质量方面均超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的情感说话头像合成在控制灵活性、动作自然性和表情质量等方面存在明显不足，且现有数据集多为实验室收集，影响实际应用。因此作者希望突破这些限制。

Method: 作者提出了基于扩散模型的EmoCAST框架，主要包括两个模块：（1）结合文本引导的情感建模模块，通过情感提示改善空间信息和表情理解；（2）引入情感音频注意力模块，更好地交互音频与情感，提升面部动作的情感表达。同时，建立了带有丰富情感文本描述的数据集，并提出了情感感知采样训练策略和渐进式功能训练方法提升表现。

Result: EmoCAST在生成情感表达丰富、自然且音频同步的说话人头像视频方面取得了当前最优的结果。

Conclusion: EmoCAST有效提升了自动说话人头像的情感表达能力和生成质量，为实际应用如虚拟人、娱乐和远程交流等场景提供了更有力的技术支持。

Abstract: Emotional talking head synthesis aims to generate talking portrait videos
with vivid expressions. Existing methods still exhibit limitations in control
flexibility, motion naturalness, and expression quality. Moreover, currently
available datasets are primarily collected in lab settings, further
exacerbating these shortcomings. Consequently, these limitations substantially
hinder practical applications in real-world scenarios. To address these
challenges, we propose EmoCAST, a diffusion-based framework with two key
modules for precise text-driven emotional synthesis. In appearance modeling,
emotional prompts are integrated through a text-guided decoupled emotive
module, enhancing the spatial knowledge to improve emotion comprehension. To
improve the relationship between audio and emotion, we introduce an emotive
audio attention module to capture the interplay between controlled emotion and
driving audio, generating emotion-aware features to guide more precise facial
motion synthesis. Additionally, we construct an emotional talking head dataset
with comprehensive emotive text descriptions to optimize the framework's
performance. Based on the proposed dataset, we propose an emotion-aware
sampling training strategy and a progressive functional training strategy that
further improve the model's ability to capture nuanced expressive features and
achieve accurate lip-synchronization. Overall, EmoCAST achieves
state-of-the-art performance in generating realistic, emotionally expressive,
and audio-synchronized talking-head videos. Project Page:
https://github.com/GVCLab/EmoCAST

</details>


### [54] [Mask-Guided Multi-Channel SwinUNETR Framework for Robust MRI Classification](https://arxiv.org/abs/2508.20621)
*Smriti Joshi,Lidia Garrucho,Richard Osuala,Oliver Diaz,Karim Lekadir*

Main category: cs.CV

TL;DR: 本文提出了一种基于SwinUNETR的深度学习框架，用于乳腺MRI影像中乳腺癌的检测和分类，在国际多中心挑战赛中获得第二名，显示出其在临床乳腺MRI解读中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是女性癌症相关死亡的主要原因之一，早期检测对改善预后至关重要。由于高危或致密乳腺女性的X线乳腺摄影效果有限，MRI成为更敏感的筛查工具。为推动基于AI的乳腺癌诊断方案，ODELIA联盟组织了多中心挑战，需要开发既具有鲁棒性又能泛化的自动分析工具。

Method: 论文构建了一个基于SwinUNETR的深度学习模型，结合了乳腺区掩膜、丰富的数据增强和集成学习策略。模型在六个欧洲中心采集、包含511组不同厂商和场强下的乳腺MRI数据集上进行训练和测试，每个样本标注左、右乳腺无病变、良性或恶性。

Result: 所提方法在ODELIA联盟主办的挑战赛中取得第二名，显示了较高的诊断准确性和模型泛化能力。论文还公开了全部代码以促进后续研究。

Conclusion: 基于SwinUNETR的深度学习方法能有效提升乳腺MRI的自动诊断和分类效果，对未来辅助临床乳腺癌筛查具有实际应用前景。

Abstract: Breast cancer is one of the leading causes of cancer-related mortality in
women, and early detection is essential for improving outcomes. Magnetic
resonance imaging (MRI) is a highly sensitive tool for breast cancer detection,
particularly in women at high risk or with dense breast tissue, where
mammography is less effective. The ODELIA consortium organized a multi-center
challenge to foster AI-based solutions for breast cancer diagnosis and
classification. The dataset included 511 studies from six European centers,
acquired on scanners from multiple vendors at both 1.5 T and 3 T. Each study
was labeled for the left and right breast as no lesion, benign lesion, or
malignant lesion. We developed a SwinUNETR-based deep learning framework that
incorporates breast region masking, extensive data augmentation, and ensemble
learning to improve robustness and generalizability. Our method achieved second
place on the challenge leaderboard, highlighting its potential to support
clinical breast MRI interpretation. We publicly share our codebase at
https://github.com/smriti-joshi/bcnaim-odelia-challenge.git.

</details>


### [55] [AvatarBack: Back-Head Generation for Complete 3D Avatars from Front-View Images](https://arxiv.org/abs/2508.20623)
*Shiqi Xin,Xiaolin Zhang,Yanbin Liu,Peng Zhang,Caifeng Shan*

Main category: cs.CV

TL;DR: 该论文提出了AvatarBack框架，专门为解决3D人头像后脑区域重建不足的问题，通过额外的生成和空间对齐技术实现完整高保真的3D头像。


<details>
  <summary>Details</summary>
Motivation: 当前Gaussian Splatting方法对头像重建虽有突破，但普遍依赖正面视角图像，导致头像后脑区域重建较差，出现几何不一致、模糊和真实感不足等问题，限制了整体重建质量。

Method: 提出AvatarBack框架，包括主体特异性生成器（SSG）和自适应空间对齐策略（ASA）：SSG利用生成式先验，从稀疏正面图像生成与身份一致且可信的后脑伪视图，为训练提供多视角监督；ASA通过可学习的变换矩阵进行精确的几何对齐，解决合成视图与3D高斯表示之间的姿态与坐标差异。

Result: 在NeRSemble和K-hairstyle等数据集上实验，使用几何、光度和GPT-4o感知指标评估，AvatarBack明显提升了后脑重建质量，并且前脸区域的保真度未受影响。

Conclusion: AvatarBack不仅改善了后脑区域的重建保真度，还保持了整体头像的视觉真实感与可动性，为3D人头像高质量全方位重建提供了新思路。

Abstract: Recent advances in Gaussian Splatting have significantly boosted the
reconstruction of head avatars, enabling high-quality facial modeling by
representing an 3D avatar as a collection of 3D Gaussians. However, existing
methods predominantly rely on frontal-view images, leaving the back-head poorly
constructed. This leads to geometric inconsistencies, structural blurring, and
reduced realism in the rear regions, ultimately limiting the fidelity of
reconstructed avatars. To address this challenge, we propose AvatarBack, a
novel plug-and-play framework specifically designed to reconstruct complete and
consistent 3D Gaussian avatars by explicitly modeling the missing back-head
regions. AvatarBack integrates two core technical innovations,i.e., the
Subject-specific Generator (SSG) and the Adaptive Spatial Alignment Strategy
(ASA). The former leverages a generative prior to synthesize
identity-consistent, plausible back-view pseudo-images from sparse frontal
inputs, providing robust multi-view supervision. To achieve precise geometric
alignment between these synthetic views and the 3D Gaussian representation, the
later employs learnable transformation matrices optimized during training,
effectively resolving inherent pose and coordinate discrepancies. Extensive
experiments on NeRSemble and K-hairstyle datasets, evaluated using geometric,
photometric, and GPT-4o-based perceptual metrics, demonstrate that AvatarBack
significantly enhances back-head reconstruction quality while preserving
frontal fidelity. Moreover, the reconstructed avatars maintain consistent
visual realism under diverse motions and remain fully animatable.

</details>


### [56] [Improving Alignment in LVLMs with Debiased Self-Judgment](https://arxiv.org/abs/2508.20655)
*Sihan Yang,Chenhang Cui,Zihao Zhao,Yiyang Zhou,Weilong Yan,Ying Wei,Huaxiu Yao*

Main category: cs.CV

TL;DR: 本文提出了一种创新方法，通过模型自身生成去偏置自我评估分数，实现大视觉-语言模型（LVLMs）中视觉与语言模态的更好对齐，显著减少幻觉现象，提高模型安全性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型及视觉-语言模型快速发展，但视觉和语言模态的有效对齐仍具有挑战，常导致生成内容与视觉输入不一致的幻觉问题，进而引发安全隐患。当前主流对齐方法依赖外部数据或人工标注，不具备高扩展性且成本高昂。

Method: 提出模型内部自洽的自我评估机制，不依赖外部资源，生成去偏置的自我判断分数，并用于优化解码策略和偏好微调流程，以自主提升视觉-语言对齐能力。

Result: 实验结果表明，该方法在减少幻觉、提高安全性以及整体能力方面，显著优于传统对齐方法。

Conclusion: 模型内生的去偏置自我评估机制为视觉-语言模型对齐问题提供了更高效、低成本和更优性能的新解决方案。

Abstract: The rapid advancements in Large Language Models (LLMs) and Large
Visual-Language Models (LVLMs) have opened up new opportunities for integrating
visual and linguistic modalities. However, effectively aligning these
modalities remains challenging, often leading to hallucinations--where
generated outputs are not grounded in the visual input--and raising safety
concerns across various domains. Existing alignment methods, such as
instruction tuning and preference tuning, often rely on external datasets,
human annotations, or complex post-processing, which limit scalability and
increase costs. To address these challenges, we propose a novel approach that
generates the debiased self-judgment score, a self-evaluation metric created
internally by the model without relying on external resources. This enables the
model to autonomously improve alignment. Our method enhances both decoding
strategies and preference tuning processes, resulting in reduced
hallucinations, enhanced safety, and improved overall capability. Empirical
results show that our approach significantly outperforms traditional methods,
offering a more effective solution for aligning LVLMs.

</details>


### [57] [ArtFace: Towards Historical Portrait Face Identification via Model Adaptation](https://arxiv.org/abs/2508.20626)
*Francois Poh,Anjith George,Sébastien Marcel*

Main category: cs.CV

TL;DR: 本文旨在提升历史绘画中人物识别的自动化水平，通过结合和微调基础模型，实现比传统人脸识别方法更好的效果。


<details>
  <summary>Details</summary>
Motivation: 历史绘画中确定人物身份对于艺术史研究至关重要，但现有方法主观性强且数据有限。传统人脸识别在照片上有效，但在绘画中因风格变异和领域差异效果差，因此亟需更强模型提升准确度。

Method: 作者利用基础模型（foundation models），对其进行微调，并将其生成的人脸嵌入与传统人脸识别模型的嵌入结合，形成更强的识别方法，对历史绘画中的人物进行识别测试。

Result: 实验结果显示，该方法在历史绘画的人脸识别任务上，相较于当前最先进的传统方法取得了显著提升。

Conclusion: 基础模型可以有效弥合传统人脸识别方法在艺术作品上出现的性能缺口，为艺术史人物鉴定带来有价值的技术支持。

Abstract: Identifying sitters in historical paintings is a key task for art historians,
offering insight into their lives and how they chose to be seen. However, the
process is often subjective and limited by the lack of data and stylistic
variations. Automated facial recognition is capable of handling challenging
conditions and can assist, but while traditional facial recognition models
perform well on photographs, they struggle with paintings due to domain shift
and high intra-class variation. Artistic factors such as style, skill, intent,
and influence from other works further complicate recognition. In this work, we
investigate the potential of foundation models to improve facial recognition in
artworks. By fine-tuning foundation models and integrating their embeddings
with those from conventional facial recognition networks, we demonstrate
notable improvements over current state-of-the-art methods. Our results show
that foundation models can bridge the gap where traditional methods are
ineffective. Paper page at https://www.idiap.ch/paper/artface/

</details>


### [58] [MobileCLIP2: Improving Multi-Modal Reinforced Training](https://arxiv.org/abs/2508.20691)
*Fartash Faghri,Pavan Kumar Anasosalu Vasu,Cem Koc,Vaishaal Shankar,Alexander Toshev,Oncel Tuzel,Hadi Pouransari*

Main category: cs.CV

TL;DR: 本文提出了MobileCLIP2，一种高效且性能卓越的图文基础模型，提升了在低延迟和模型小型化前提下的零样本图像识别能力，并开源了模型和数据生成工具。


<details>
  <summary>Details</summary>
Motivation: 随着CLIP等图文基础模型广泛应用，有必要在保证高准确率前提下降低计算资源消耗和延迟，以便部署在移动或资源受限设备上，MobileCLIP就是这一方向的成果。本工作旨在进一步提升MobileCLIP的训练效率和泛化能力。

Method: 在MobileCLIP的基础上，本研究提出改进的多模态强化训练方法：（1）采用在DFN数据集上训练的更优CLIP教师模型集合；（2）采用在DFN及多样高质量图文数据集上训练和微调过的图像描述生成器教师；（3）通过消融实验优化知识蒸馏对比损失的温度参数，细致探索合成描述多样性和多教师协作训练的增益。同时，开放分布式数据生成代码，便于灵活扩展训练数据。

Result: 新模型MobileCLIP2在ImageNet-1k上达到了新的零样本准确率SOTA：MobileCLIP2-B比前代MobileCLIP-B提升2.2%；MobileCLIP2-S4仅用一半参数即可匹配SigLIP-SO400M/14准确率，延迟也大幅降低。相比DFN ViT-L/14，速度提升2.5倍。

Conclusion: MobileCLIP2展示了轻量级、低延迟图文模型在保证准确率的基础上具有更广泛的应用前景，相关模型和数据生成代码已开源，有助于后续更多图文基础模型的快速迭代和落地应用。

Abstract: Foundation image-text models such as CLIP with zero-shot capabilities enable
a wide array of applications. MobileCLIP is a recent family of image-text
models at 3-15ms latency and 50-150M parameters with state-of-the-art zero-shot
accuracy. The main ingredients in MobileCLIP were its low-latency and light
architectures and a novel multi-modal reinforced training that made knowledge
distillation from multiple caption-generators and CLIP teachers efficient,
scalable, and reproducible. In this paper, we improve the multi-modal
reinforced training of MobileCLIP through: 1) better CLIP teacher ensembles
trained on the DFN dataset, 2) improved captioner teachers trained on the DFN
dataset and fine-tuned on a diverse selection of high-quality image-caption
datasets. We discover new insights through ablations such as the importance of
temperature tuning in contrastive knowledge distillation, the effectiveness of
caption-generator fine-tuning for caption diversity, and the additive
improvement from combining synthetic captions generated by multiple models. We
train a new family of models called MobileCLIP2 and achieve state-of-the-art
ImageNet-1k zero-shot accuracies at low latencies. In particular, we observe
2.2% improvement in ImageNet-1k accuracy for MobileCLIP2-B compared with
MobileCLIP-B architecture. Notably, MobileCLIP2-S4 matches the zero-shot
accuracy of SigLIP-SO400M/14 on ImageNet-1k while being 2$\times$ smaller and
improves on DFN ViT-L/14 at 2.5$\times$ lower latency. We release our
pretrained models (https://github.com/apple/ml-mobileclip) and the data
generation code (https://github.com/apple/ml-mobileclip-dr). The data
generation code makes it easy to create new reinforced datasets with arbitrary
teachers using distributed scalable processing.

</details>


### [59] [CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models](https://arxiv.org/abs/2508.20640)
*Ayan Banerjee,Fernando Vilariño,Josep Lladós*

Main category: cs.CV

TL;DR: 本文提出了一种名为CraftGraffiti的AI系统，实现将人脸以涂鸦风格高度变换但仍保持身份特征，解决了极端风格转换易导致人物不可识别的难题。系统结合了风格与身份保持机制，并在实际和主观测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 极富艺术性的涂鸦等风格转换过程中，轻微的面部特征变形就可能导致身份丧失，难以保持文化与个人特征的真实性。为促进创意AI的真实表达，亟需在风格自由和身份识别性之间找到平衡。

Method: 系统基于LoRA微调的扩散模型，实现文本引导的风格转移；引入人脸一致性自注意力机制，把显式身份特征嵌入进注意力层，以增强面部特征保存；采用CLIP引导实现姿态自定义，无需关键点检测；提出“先风格、后身份”范式，并以理论和实验论证其有效性。

Result: 在量化评测中，CraftGraffiti在面部特征一致性和美学评分方面均表现突出，同时在人类主观偏好中胜出。定性分析和在Cruilla艺术节现场部署，进一步证明了其实用性和创意影响力。

Conclusion: CraftGraffiti为AI辅助艺术中的身份保护提供了系统化方案，推动在创意自由与可识别性两者的融合，为AI艺术应用领域带来理论和实际价值。

Abstract: Preserving facial identity under extreme stylistic transformation remains a
major challenge in generative art. In graffiti, a high-contrast, abstract
medium, subtle distortions to the eyes, nose, or mouth can erase the subject's
recognizability, undermining both personal and cultural authenticity. We
present CraftGraffiti, an end-to-end text-guided graffiti generation framework
designed with facial feature preservation as a primary objective. Given an
input image and a style and pose descriptive prompt, CraftGraffiti first
applies graffiti style transfer via LoRA-fine-tuned pretrained diffusion
transformer, then enforces identity fidelity through a face-consistent
self-attention mechanism that augments attention layers with explicit identity
embeddings. Pose customization is achieved without keypoints, using CLIP-guided
prompt extension to enable dynamic re-posing while retaining facial coherence.
We formally justify and empirically validate the "style-first, identity-after"
paradigm, showing it reduces attribute drift compared to the reverse order.
Quantitative results demonstrate competitive facial feature consistency and
state-of-the-art aesthetic and human preference scores, while qualitative
analyses and a live deployment at the Cruilla Festival highlight the system's
real-world creative impact. CraftGraffiti advances the goal of
identity-respectful AI-assisted artistry, offering a principled approach for
blending stylistic freedom with recognizability in creative AI applications.

</details>


### [60] [ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering](https://arxiv.org/abs/2508.21010)
*Paritosh Parmar,Eric Peh,Basura Fernando*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的模块化视频问答框架，通过将因果推理与答案生成显式解耦，实现更透明、可解释的因果型视频理解，对现有方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有Causal-Why视频问答模型依赖黑盒式、难以解释的单一流程，难以进行高阶推理，且往往依赖浅层启发式方法，解释性和泛化能力有限。作者旨在通过更透明和模块化的方法提升模型的因果推理能力和可解释性。

Method: 提出两阶段架构：第一阶段为因果链提取器（CCE），从视频-问题对中生成自然语言因果链，作为解释性中间表示；第二阶段为因果链驱动回答器（CCDA），基于因果链生成答案。此外，为解决缺乏推理标注的问题，采用大语言模型从现有数据集中自动生成高质量因果链，并提出新的因果性定向评价指标CauCo。

Result: 在三个大规模基准数据集上实验表明，该方法不仅超越现有最优模型，还在可解释性、用户信任和泛化性方面带来显著提升。

Conclusion: CCE可以作为通用的因果推理引擎应用于不同领域，所提方法实现了因果视频问答领域性能和可解释性的双重突破。

Abstract: Existing Causal-Why Video Question Answering (VideoQA) models often struggle
with higher-order reasoning, relying on opaque, monolithic pipelines that
entangle video understanding, causal inference, and answer generation. These
black-box approaches offer limited interpretability and tend to depend on
shallow heuristics. We propose a novel, modular framework that explicitly
decouples causal reasoning from answer generation, introducing natural language
causal chains as interpretable intermediate representations. Inspired by human
cognitive models, these structured cause-effect sequences bridge low-level
video content with high-level causal reasoning, enabling transparent and
logically coherent inference. Our two-stage architecture comprises a Causal
Chain Extractor (CCE) that generates causal chains from video-question pairs,
and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in
these chains. To address the lack of annotated reasoning traces, we introduce a
scalable method for generating high-quality causal chains from existing
datasets using large language models. We also propose CauCo, a new evaluation
metric for causality-oriented captioning. Experiments on three large-scale
benchmarks demonstrate that our approach not only outperforms state-of-the-art
models, but also yields substantial gains in explainability, user trust, and
generalization -- positioning the CCE as a reusable causal reasoning engine
across diverse domains. Project page:
https://paritoshparmar.github.io/chainreaction/

</details>


### [61] ["Humor, Art, or Misinformation?": A Multimodal Dataset for Intent-Aware Synthetic Image Detection](https://arxiv.org/abs/2508.20670)
*Anastasios Skoularikis,Stefanos-Iordanis Papadopoulos,Symeon Papadopoulos,Panagiotis C. Petrantonakis*

Main category: cs.CV

TL;DR: 提出了S-HArM多模态意图识别数据集，从Twitter/X和Reddit采集近万组配对数据，用于对AI生成图像的意图（幽默/讽刺、艺术、虚假信息）进行分类，并系统评估了多种数据生成和多模态模型方法，发现推断意图任务仍较具挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的AI合成内容检测技术忽略了生成内容背后的意图，而意图（如幽默、艺术、还是误导）对于理解和管理AI内容极为重要，因此需要能识别意图的数据集和方法。

Method: 1）建立S-HArM多模态意图识别数据集，标注9,576组图片-文本对；2）采用三种提示策略（图像、描述、多模态引导）利用Stable Diffusion合成大规模训练数据；3）比较多种多模态模型（如模态融合、对比学习、重建网络、注意力机制及大型视觉语言模型）的意图识别能力。

Result: 训练结果显示，采用图像引导和多模态引导生成的数据集训练的模型在真实网络数据上的泛化能力更强，因为能更好地保留视觉上下文；但总体识别性能仍有限。

Conclusion: 推断AI生成内容的意图是高度复杂但重要的任务，现有模型面临显著挑战，需要为意图识别设计专门的新型多模态架构。

Abstract: Recent advances in multimodal AI have enabled progress in detecting synthetic
and out-of-context content. However, existing efforts largely overlook the
intent behind AI-generated images. To fill this gap, we introduce S-HArM, a
multimodal dataset for intent-aware classification, comprising 9,576 "in the
wild" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art,
or Misinformation. Additionally, we explore three prompting strategies
(image-guided, description-guided, and multimodally-guided) to construct a
large-scale synthetic training dataset with Stable Diffusion. We conduct an
extensive comparative study including modality fusion, contrastive learning,
reconstruction networks, attention mechanisms, and large vision-language
models. Our results show that models trained on image- and multimodally-guided
data generalize better to "in the wild" content, due to preserved visual
context. However, overall performance remains limited, highlighting the
complexity of inferring intent and the need for specialized architectures.

</details>


### [62] [Learned Rate Control for Frame-Level Adaptive Neural Video Compression via Dynamic Neural Network](https://arxiv.org/abs/2508.20709)
*Chenhao Zhang,Wei Gao*

Main category: cs.CV

TL;DR: 该论文提出了一种动态神经视频压缩框架，能够在可变比特率场景下实现精确率控，并在主流数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管神经视频压缩在性能上已取得显著进展，但受限于学习型编解码器的缺陷，现有方法难以实现精确的码率控制。在实际应用中，如流媒体传输、存储等，对灵活且精准的码率控制有很大需求，因此需要新的方法提升可变码率下的压缩性能。

Method: 作者提出Dynamic-Route Autoencoder（DRA），利用多条编解码路径实现不同的计算复杂度和率失真权衡。通过引入Rate Control Agent，对每条路径的比特率进行预测和动态调节，实时选择合适的路径以逼近目标比特率。同时，采用Joint-Routes Optimization对多条路径进行联合训练，从而兼顾整体的率失真性能。

Result: 在HEVC和UVG数据集上进行了大规模实验。结果显示，所提方法相比最新方法平均BD-Rate降低14.8%，BD-PSNR提升0.47dB，且平均码率误差仅1.66%。

Conclusion: 该动态视频压缩框架不仅显著提升了可变比特率下的压缩效果和准确度，还优化了率-失真-复杂度的整体表现，适用于各种需灵活码率控制的应用场景。

Abstract: Neural Video Compression (NVC) has achieved remarkable performance in recent
years. However, precise rate control remains a challenge due to the inherent
limitations of learning-based codecs. To solve this issue, we propose a dynamic
video compression framework designed for variable bitrate scenarios. First, to
achieve variable bitrate implementation, we propose the Dynamic-Route
Autoencoder with variable coding routes, each occupying partial computational
complexity of the whole network and navigating to a distinct RD trade-off.
Second, to approach the target bitrate, the Rate Control Agent estimates the
bitrate of each route and adjusts the coding route of DRA at run time. To
encompass a broad spectrum of variable bitrates while preserving overall RD
performance, we employ the Joint-Routes Optimization strategy, achieving
collaborative training of various routes. Extensive experiments on the HEVC and
UVG datasets show that the proposed method achieves an average BD-Rate
reduction of 14.8% and BD-PSNR gain of 0.47dB over state-of-the-art methods
while maintaining an average bitrate error of 1.66%, achieving
Rate-Distortion-Complexity Optimization (RDCO) for various bitrate and
bitrate-constrained applications. Our code is available at
https://git.openi.org.cn/OpenAICoding/DynamicDVC.

</details>


### [63] [CardioMorphNet: Cardiac Motion Prediction Using a Shape-Guided Bayesian Recurrent Deep Network](https://arxiv.org/abs/2508.20734)
*Reza Akbari Movahed,Abuzar Rezaee,Arezoo Zakeri,Colin Berry,Edmond S. L. Ho,Ali Gooya*

Main category: cs.CV

TL;DR: 本文提出了一种名为CardioMorphNet的新型3D心脏运动估计方法，基于循环贝叶斯深度学习框架，并在大型公开数据集验证中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的心脏运动估计方法主要依赖基于图像强度的配准损失，可能忽略解剖结构区域，导致对心脏运动建模不准确，需要更关注解剖结构且具有不确定性评估的新方法。

Method: 提出CardioMorphNet框架：1）循环变分自编码器模型捕捉心脏周期的时空依赖性；2）双后验模型分别做双心室分割和运动估计；3）摒弃传统强度相似性损失，利用贝叶斯推理，反复对分割图进行配准；4）能输出运动场的不确定性分析。

Result: 在UK Biobank数据集中，CardioMorphNet以配准后分割掩膜与真实掩膜的相似度为标准，验证优于多项最新心脏运动估计方法。同时，本方法在心脏区域的运动场估计中有更低的不确定性值，表现更可信。

Conclusion: CardioMorphNet新方法在关注解剖结构的心脏运动估计和不确定性评估方面，较现有方法有显著提升，对临床心脏功能评估具有潜在应用价值。

Abstract: Accurate cardiac motion estimation from cine cardiac magnetic resonance (CMR)
images is vital for assessing cardiac function and detecting its abnormalities.
Existing methods often struggle to capture heart motion accurately because they
rely on intensity-based image registration similarity losses that may overlook
cardiac anatomical regions. To address this, we propose CardioMorphNet, a
recurrent Bayesian deep learning framework for 3D cardiac shape-guided
deformable registration using short-axis (SAX) CMR images. It employs a
recurrent variational autoencoder to model spatio-temporal dependencies over
the cardiac cycle and two posterior models for bi-ventricular segmentation and
motion estimation. The derived loss function from the Bayesian formulation
guides the framework to focus on anatomical regions by recursively registering
segmentation maps without using intensity-based image registration similarity
loss, while leveraging sequential SAX volumes and spatio-temporal features. The
Bayesian modelling also enables computation of uncertainty maps for the
estimated motion fields. Validated on the UK Biobank dataset by comparing
warped mask shapes with ground truth masks, CardioMorphNet demonstrates
superior performance in cardiac motion estimation, outperforming
state-of-the-art methods. Uncertainty assessment shows that it also yields
lower uncertainty values for estimated motion fields in the cardiac region
compared with other probabilistic-based cardiac registration methods,
indicating higher confidence in its predictions.

</details>


### [64] [Mix, Align, Distil: Reliable Cross-Domain Atypical Mitosis Classification](https://arxiv.org/abs/2508.20745)
*Kaustubh Atey,Sameer Anand Jha,Gouranga Bala,Amit Sethi*

Main category: cs.CV

TL;DR: 本论文提出了一种训练流程，显著提升了在不同域（如扫描仪、染色及采集方式变化）下的非典型有丝分裂体（AMFs）分类的鲁棒性，取得了MIDOG 2025竞赛中较高的准确率和AUC。


<details>
  <summary>Details</summary>
Motivation: 非典型有丝分裂体是重要的组织病理学标志物，但在不同域条件（如设备和染色差异）下的一致识别极具挑战。因此，研究如何实现域泛化、稳定、高效的AMF自动分类方法具有重要意义。

Method: 提出了一种集成方法，包括：1）通过在骨干网络早期和中期插入风格扰动，增强特征多样性；2）利用弱域标签（如扫描仪、来源、物种、肿瘤）通过辅助对齐损失，对注意力特征进行跨域对齐；3）结合带温度缩放KL散度的指数移动平均教师网络蒸馏，提升预测稳定性。

Result: 方法在组织方发布的初步排行榜上，获得了0.8762的平衡准确率、0.8873的敏感性、0.8651的特异性和0.9499的ROC AUC，以及测试时几乎无额外负担的推理速度。

Conclusion: 该方法只依赖粗粒度的域元数据，实现了高效、强大且平衡的跨域AMF分类，成为MIDOG 2025挑战中具有竞争力的解决方案。

Abstract: Atypical mitotic figures (AMFs) are important histopathological markers yet
remain challenging to identify consistently, particularly under domain shift
stemming from scanner, stain, and acquisition differences. We present a simple
training-time recipe for domain-robust AMF classification in MIDOG 2025 Task 2.
The approach (i) increases feature diversity via style perturbations inserted
at early and mid backbone stages, (ii) aligns attention-refined features across
sites using weak domain labels (Scanner, Origin, Species, Tumor) through an
auxiliary alignment loss, and (iii) stabilizes predictions by distilling from
an exponential moving average (EMA) teacher with temperature-scaled KL
divergence. On the organizer-run preliminary leaderboard for atypical mitosis
classification, our submission attains balanced accuracy of 0.8762, sensitivity
of 0.8873, specificity of 0.8651, and ROC AUC of 0.9499. The method incurs
negligible inference-time overhead, relies only on coarse domain metadata, and
delivers strong, balanced performance, positioning it as a competitive
submission for the MIDOG 2025 challenge.

</details>


### [65] [Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning](https://arxiv.org/abs/2508.20751)
*Yibin Wang,Zhimin Li,Yuhang Zang,Yujie Zhou,Jiazi Bu,Chunyu Wang,Qinglin Lu,Cheng Jin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 本文提出了Pref-GRPO，一种基于成对偏好奖励的GRPO方法，以应对现有点对点奖励模型在文本到图像（T2I）生成中的奖励欺骗问题，实现更稳定的训练，并引入了细致化的T2I基准UniGenBench。


<details>
  <summary>Details</summary>
Motivation: 目前T2I生成中存在奖励欺骗现象，即使用点对点奖励模型评分时，微小的分数差被放大，导致模型过度优化于无关的细节，最终使生成过程不稳定。同时，现有T2I评测基准粗糙，难以全面反映模型能力。

Method: 提出Pref-GRPO方法，将优化目标从单纯的分数最大化转变为更注重偏好拟合，通过成对比较图像并用胜率作为奖励信号，提升训练稳定性。同时，构建了细致化的UniGenBench基准，包含600个提示，细分为主题与评测标准，并利用MLLM辅助进行自动化评估。

Result: 实验表明Pref-GRPO能够更准确地区分图像质量细微差异，训练过程更为稳定，有效缓解了奖励欺骗，还能通过新基准全面发现不同模型的优劣；UniGenBench展示了更多模型评测细节和能力短板。

Conclusion: Pref-GRPO方法提升了T2I生成的可靠性并抑制了奖励欺骗现象；新提出的UniGenBench基准能更全面细致地评估T2I模型，为未来相关研究提供了可靠工具与新思路。

Abstract: Recent advancements highlight the importance of GRPO-based reinforcement
learning methods and benchmarking in enhancing text-to-image (T2I) generation.
However, current methods using pointwise reward models (RM) for scoring
generated images are susceptible to reward hacking. We reveal that this happens
when minimal score differences between images are amplified after
normalization, creating illusory advantages that drive the model to
over-optimize for trivial gains, ultimately destabilizing the image generation
process. To address this, we propose Pref-GRPO, a pairwise preference
reward-based GRPO method that shifts the optimization objective from score
maximization to preference fitting, ensuring more stable training. In
Pref-GRPO, images are pairwise compared within each group using preference RM,
and the win rate is used as the reward signal. Extensive experiments
demonstrate that PREF-GRPO differentiates subtle image quality differences,
providing more stable advantages and mitigating reward hacking. Additionally,
existing T2I benchmarks are limited by coarse evaluation criteria, hindering
comprehensive model assessment. To solve this, we introduce UniGenBench, a
unified T2I benchmark comprising 600 prompts across 5 main themes and 20
subthemes. It evaluates semantic consistency through 10 primary and 27
sub-criteria, leveraging MLLM for benchmark construction and evaluation. Our
benchmarks uncover the strengths and weaknesses of both open and closed-source
T2I models and validate the effectiveness of Pref-GRPO.

</details>


### [66] [${C}^{3}$-GS: Learning Context-aware, Cross-dimension, Cross-scale Feature for Generalizable Gaussian Splatting](https://arxiv.org/abs/2508.20754)
*Yuxi Hu,Jun Zhang,Kuangyi Chen,Zhe Zhang,Friedrich Fraundorfer*

Main category: cs.CV

TL;DR: 该论文提出了一种名为C3-GS的通用高斯斑点新方法，可在无需单场景优化的情况下实现高质量新视角合成，尤其在稀疏输入下效果突出。


<details>
  <summary>Details</summary>
Motivation: 现有的高斯斑点通用方法在特征编码上存在不足，难以从稀疏视图中预测一致且具区分性的高斯参数，进而影响几何重建准确性。本文旨在通过提升特征表示能力，增强不同视角间的一致性及判别性。

Method: 作者提出C3-GS框架，将三种轻量级模块（分别针对上下文、跨维度和跨尺度约束）整合进渲染流程中，以提升特征融合能力。网络采用前馈结构，直接预测每像素的高斯参数，无需额外监督。

Result: 在多个公开基准数据集上，C3-GS展现了优异的渲染质量和泛化能力，超越了现有同类方法。

Conclusion: C3-GS通过创新的特征融合与约束机制，显著提升了稀疏视图下的新视角合成表现，并具备良好的泛化能力，推进了高斯斑点相关方法的发展。

Abstract: Generalizable Gaussian Splatting aims to synthesize novel views for unseen
scenes without per-scene optimization. In particular, recent advancements
utilize feed-forward networks to predict per-pixel Gaussian parameters,
enabling high-quality synthesis from sparse input views. However, existing
approaches fall short in encoding discriminative, multi-view consistent
features for Gaussian predictions, which struggle to construct accurate
geometry with sparse views. To address this, we propose $\mathbf{C}^{3}$-GS, a
framework that enhances feature learning by incorporating context-aware,
cross-dimension, and cross-scale constraints. Our architecture integrates three
lightweight modules into a unified rendering pipeline, improving feature fusion
and enabling photorealistic synthesis without requiring additional supervision.
Extensive experiments on benchmark datasets validate that $\mathbf{C}^{3}$-GS
achieves state-of-the-art rendering quality and generalization ability. Code is
available at: https://github.com/YuhsiHu/C3-GS.

</details>


### [67] [SeqVLM: Proposal-Guided Multi-View Sequences Reasoning via VLM for Zero-Shot 3D Visual Grounding](https://arxiv.org/abs/2508.20758)
*Jiawen Lin,Shiran Bian,Yihang Zhu,Wenbin Tan,Yachao Zhang,Yuan Xie,Yanyun Qu*

Main category: cs.CV

TL;DR: 本文提出了一种新的零样本3D视觉定位（3DVG）方法SeqVLM，通过融合多视角场景图像与空间信息，实现更精准的三维对象定位。实验表明，SeqVLM在ScanRefer和Nr3D数据集上均取得了当前最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有3DVG方法依赖于有监督训练，受限于特定场景，限制了其在真实应用中的泛化能力。零样本3DVG更具实际意义，但受限于单视角推理及上下文丢失，导致性能不足。为解决这些问题，论文设计了新的方案。

Method: 方法包括三步：1）利用3D语义分割网络生成候选实例并通过语义过滤保留相关对象；2）应用多视角投影，将3D点云候选体投影到多张真实场景图片上，从而保留空间关系和上下文细节；3）提出动态调度机制，对多视角-查询组合迭代处理，通过VLM（视觉-语言模型）跨模态推理能力识别目标。

Result: 在ScanRefer和Nr3D两个公开数据集上，SeqVLM的Acc@0.25分别达到55.6%和53.2%，较以往零样本方法提升了4.0%和5.2%。

Conclusion: SeqVLM有效提升了零样本3DVG的精度和泛化能力，为其在实际场景中应用迈出了关键一步。

Abstract: 3D Visual Grounding (3DVG) aims to localize objects in 3D scenes using
natural language descriptions. Although supervised methods achieve higher
accuracy in constrained settings, zero-shot 3DVG holds greater promise for
real-world applications since eliminating scene-specific training requirements.
However, existing zero-shot methods face challenges of spatial-limited
reasoning due to reliance on single-view localization, and contextual omissions
or detail degradation. To address these issues, we propose SeqVLM, a novel
zero-shot 3DVG framework that leverages multi-view real-world scene images with
spatial information for target object reasoning. Specifically, SeqVLM first
generates 3D instance proposals via a 3D semantic segmentation network and
refines them through semantic filtering, retaining only semantic-relevant
candidates. A proposal-guided multi-view projection strategy then projects
these candidate proposals onto real scene image sequences, preserving spatial
relationships and contextual details in the conversion process of 3D point
cloud to images. Furthermore, to mitigate VLM computational overload, we
implement a dynamic scheduling mechanism that iteratively processes
sequances-query prompts, leveraging VLM's cross-modal reasoning capabilities to
identify textually specified objects. Experiments on the ScanRefer and Nr3D
benchmarks demonstrate state-of-the-art performance, achieving Acc@0.25 scores
of 55.6% and 53.2%, surpassing previous zero-shot methods by 4.0% and 5.2%,
respectively, which advance 3DVG toward greater generalization and real-world
applicability. The code is available at https://github.com/JiawLin/SeqVLM.

</details>


### [68] [Occlusion Robustness of CLIP for Military Vehicle Classification](https://arxiv.org/abs/2508.20760)
*Jan Erik van Woerden,Gertjan Burghouts,Lotte Nijskens,Alma M. Liezenga,Sabina van Rooij,Frank Ruis,Hugo J. Kuijf*

Main category: cs.CV

TL;DR: 本文研究了CLIP视觉-语言模型在军事环境中面对图像遮挡时的鲁棒性，发现Transformer结构优于CNN，遮挡形式和训练策略会显著影响模型在高遮挡下的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管CLIP等视觉-语言模型因具备零样本分类能力，对标注稀缺的军事应用具有优势，但其在军事环境中常见的遮挡和低信噪比等恶劣条件下的鲁棒性尚缺乏评估。该研究旨在填补这一领域的空白。

Method: 作者使用自制的包含18类军用车辆的图像数据集，通过不同比例和类型的遮挡，评估各种CLIP变体模型在遮挡条件下的分类性能，并以归一化曲线下面积（NAUC）作为评估指标。对比了基于Transformer和CNN的CLIP模型，以及线性探针和backbone微调两种训练方式。

Result: 1) 基于Transformer的CLIP模型始终优于基于CNN的模型；2) 分布式细粒度遮挡对性能损害大于大片连续遮挡；3) 线性探针模型的性能在遮挡约35%时急剧下降，而经过backbone微调后，性能下滑临界点可推迟到60%以上的遮挡。

Conclusion: 研究强调了在训练时引入针对遮挡的数据增强的重要性，以及需要进一步研究CLIP对局部遮挡的敏感性和模型结构的鲁棒性，以实现实际军事部署。

Abstract: Vision-language models (VLMs) like CLIP enable zero-shot classification by
aligning images and text in a shared embedding space, offering advantages for
defense applications with scarce labeled data. However, CLIP's robustness in
challenging military environments, with partial occlusion and degraded
signal-to-noise ratio (SNR), remains underexplored. We investigate CLIP
variants' robustness to occlusion using a custom dataset of 18 military vehicle
classes and evaluate using Normalized Area Under the Curve (NAUC) across
occlusion percentages. Four key insights emerge: (1) Transformer-based CLIP
models consistently outperform CNNs, (2) fine-grained, dispersed occlusions
degrade performance more than larger contiguous occlusions, (3) despite
improved accuracy, performance of linear-probed models sharply drops at around
35% occlusion, (4) by finetuning the model's backbone, this performance drop
occurs at more than 60% occlusion. These results underscore the importance of
occlusion-specific augmentations during training and the need for further
exploration into patch-level sensitivity and architectural resilience for
real-world deployment of CLIP.

</details>


### [69] [Looking Beyond the Obvious: A Survey on Abstract Concept Recognition for Video Understanding](https://arxiv.org/abs/2508.20765)
*Gowreesh Mago,Pascal Mettes,Stevan Rudinac*

Main category: cs.CV

TL;DR: 本文综述了视频内容中抽象概念理解的研究进展，强调基础模型（foundation models）对解决此挑战的重要性，并梳理了相关任务与数据集的发展历程。


<details>
  <summary>Details</summary>
Motivation: 虽然机器对具体可见实体（如物体、动作、事件、场景）的理解已经取得显著进展，但与人类相比，机器还难以理解如正义、自由、团结等抽象概念。这种抽象概念的识别能力对模型实现更与人类思维一致至关重要。

Method: 本文采用综述方式，分析和比较了历史上用于视频中抽象概念识别的各种任务与数据集，并探讨基础模型技术相较以往更适用于此类问题。文中强调吸取以往的经验，避免重复造轮子。

Result: 梳理了历年来学界在视频抽象概念理解方面的多种研究方案，总结出社区不断尝试利用最新工具解决相关挑战的趋势。

Conclusion: 本文呼吁结合基础模型与社区多年共识，进一步研究视频抽象概念的理解课题，以促进模型朝着更接近人类认知和价值观的方向发展。

Abstract: The automatic understanding of video content is advancing rapidly. Empowered
by deeper neural networks and large datasets, machines are increasingly capable
of understanding what is concretely visible in video frames, whether it be
objects, actions, events, or scenes. In comparison, humans retain a unique
ability to also look beyond concrete entities and recognize abstract concepts
like justice, freedom, and togetherness. Abstract concept recognition forms a
crucial open challenge in video understanding, where reasoning on multiple
semantic levels based on contextual information is key. In this paper, we argue
that the recent advances in foundation models make for an ideal setting to
address abstract concept understanding in videos. Automated understanding of
high-level abstract concepts is imperative as it enables models to be more
aligned with human reasoning and values. In this survey, we study different
tasks and datasets used to understand abstract concepts in video content. We
observe that, periodically and over a long period, researchers have attempted
to solve these tasks, making the best use of the tools available at their
disposal. We advocate that drawing on decades of community experience will help
us shed light on this important open grand challenge and avoid ``re-inventing
the wheel'' as we start revisiting it in the era of multi-modal foundation
models.

</details>


### [70] [Safer Skin Lesion Classification with Global Class Activation Probability Map Evaluation and SafeML](https://arxiv.org/abs/2508.20776)
*Kuniko Paxton,Koorosh Aslansefat,Amila Akagić,Dhavalkumar Thakker,Yiannis Papadopoulos*

Main category: cs.CV

TL;DR: 本文提出了一种新的皮肤病变分类可解释性方法，能增强模型诊断透明度与安全性，并在ISIC数据集上进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 尽管皮肤病变分类的AI模型诊断准确率很高，但由于缺乏可解释性和可信度，实际应用中仍存在不信任问题。现有可解释方法如LIME和CAM存在不足，无法可靠地用于医疗决策。

Method: 作者提出了全类激活概率图评估（Global Class Activation Probabilistic Map Evaluation）方法，从概率角度在像素级别分析所有类别的激活区域，并实现统一可视化。同时，结合SafeML技术，对可能的误诊进行检测和警告。

Result: 该方法在ISIC皮肤病变公开数据集上，通过MobileNetV2和Vision Transformer架构进行了评估，展现出提高误诊检测能力和可靠性的效果。

Conclusion: 新方法不仅提升了模型解释力，还能发出误诊预警，提高了AI辅助诊断的可信度和患者安全性。

Abstract: Recent advancements in skin lesion classification models have significantly
improved accuracy, with some models even surpassing dermatologists' diagnostic
performance. However, in medical practice, distrust in AI models remains a
challenge. Beyond high accuracy, trustworthy, explainable diagnoses are
essential. Existing explainability methods have reliability issues, with
LIME-based methods suffering from inconsistency, while CAM-based methods
failing to consider all classes. To address these limitations, we propose
Global Class Activation Probabilistic Map Evaluation, a method that analyses
all classes' activation probability maps probabilistically and at a pixel
level. By visualizing the diagnostic process in a unified manner, it helps
reduce the risk of misdiagnosis. Furthermore, the application of SafeML
enhances the detection of false diagnoses and issues warnings to doctors and
patients as needed, improving diagnostic reliability and ultimately patient
safety. We evaluated our method using the ISIC datasets with MobileNetV2 and
Vision Transformers.

</details>


### [71] [Evaluating Compositional Generalisation in VLMs and Diffusion Models](https://arxiv.org/abs/2508.20783)
*Beth Pearson,Bilal Boulbarss,Michael Wray,Martha Lewis*

Main category: cs.CV

TL;DR: 本文比较了基于扩散模型的Diffusion Classifier与CLIP和ViLT在视觉-语言模型中的组合推理能力，发现扩散模型在某些任务上有优势，但所有模型在关系组合任务上表现较差。


<details>
  <summary>Details</summary>
Motivation: 目前的视觉-语言模型（如CLIP）虽然在多模态理解上取得进展，但在组合已知成分生成新意义（compositional generalisation）方面存在严重短板，尤其体现在对象与属性、关系的精确绑定上。本文旨在验证扩散模型在该领域的提升潜力。

Method: 作者将Diffusion Classifier与CLIP、ViLT三种模型，在对象-属性及对象-关系的组合任务上进行对比，分别在零样本学习（ZSL）和泛化零样本学习（GZSL）两种设置下进行测试；同时分析CLIP的embedding以理解表现差异原因。

Result: Diffusion Classifier与ViLT在对象-属性的绑定任务上表现较好；但在关系组合的泛化零样本任务上，所有模型均表现不佳。CLIP的嵌入显示表示关系概念过于相似，影响区分能力。

Conclusion: 尽管扩散模型在组合推理方面具备一定提升，但对关系推理的泛化仍然是所有主流视觉-语言模型面临的主要挑战，需要进一步研究以提升模型的关系理解能力。

Abstract: A fundamental aspect of the semantics of natural language is that novel
meanings can be formed from the composition of previously known parts.
Vision-language models (VLMs) have made significant progress in recent years,
however, there is evidence that they are unable to perform this kind of
composition. For example, given an image of a red cube and a blue cylinder, a
VLM such as CLIP is likely to incorrectly label the image as a red cylinder or
a blue cube, indicating it represents the image as a `bag-of-words' and fails
to capture compositional semantics. Diffusion models have recently gained
significant attention for their impressive generative abilities, and zero-shot
classifiers based on diffusion models have been shown to perform competitively
with CLIP in certain compositional tasks. In this work we explore whether the
generative Diffusion Classifier has improved compositional generalisation
abilities compared to discriminative models. We assess three models --
Diffusion Classifier, CLIP, and ViLT -- on their ability to bind objects with
attributes and relations in both zero-shot learning (ZSL) and generalised
zero-shot learning (GZSL) settings. Our results show that the Diffusion
Classifier and ViLT perform well at concept binding tasks, but that all models
struggle significantly with the relational GZSL task, underscoring the broader
challenges VLMs face with relational reasoning. Analysis of CLIP embeddings
suggests that the difficulty may stem from overly similar representations of
relational concepts such as left and right. Code and dataset are available at:
https://github.com/otmive/diffusion_classifier_clip

</details>


### [72] [Surfel-based 3D Registration with Equivariant SE(3) Features](https://arxiv.org/abs/2508.20789)
*Xueyang Kang,Hang Zhao,Kourosh Khoshelham,Patrick Vandewalle*

Main category: cs.CV

TL;DR: 本文提出了一种基于surfel的新型姿态学习回归方法，通过显式的SE(3)等变特征实现更健壮的点云配准，在真实扫描数据上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有点云配准方法（无论是传统还是深度学习方法）都忽略了点云的朝向和不确定性，导致模型对噪声及激烈旋转敏感，需要大量带增强变换的训练数据，因此有必要设计更加鲁棒且高效的方法。

Method: 作者提出一种基于surfel的回归架构：首先从Lidar点云利用虚拟相机参数初始化surfel；接着，设计SE(3)等变卷积核学习点的位置与旋转信息特征，并通过等变卷积编码器、交叉注意力机制、全连接解码器及Huber损失函数，实现源点云与目标点云之间的相对变换预测。

Result: 实验证明，在室内和室外真实点云数据集上，该方法在配准准确性和鲁棒性方面都优于最新的相关方法。

Conclusion: 引入surfel与SE(3)等变特征以及特定的模型结构（如交叉注意力）后，可以极大提升点云配准任务在实际条件下的效果，减小对数据增强的依赖，效果更稳定、可靠。

Abstract: Point cloud registration is crucial for ensuring 3D alignment consistency of
multiple local point clouds in 3D reconstruction for remote sensing or digital
heritage. While various point cloud-based registration methods exist, both
non-learning and learning-based, they ignore point orientations and point
uncertainties, making the model susceptible to noisy input and aggressive
rotations of the input point cloud like orthogonal transformation; thus, it
necessitates extensive training point clouds with transformation augmentations.
To address these issues, we propose a novel surfel-based pose learning
regression approach. Our method can initialize surfels from Lidar point cloud
using virtual perspective camera parameters, and learns explicit
$\mathbf{SE(3)}$ equivariant features, including both position and rotation
through $\mathbf{SE(3)}$ equivariant convolutional kernels to predict relative
transformation between source and target scans. The model comprises an
equivariant convolutional encoder, a cross-attention mechanism for similarity
computation, a fully-connected decoder, and a non-linear Huber loss.
Experimental results on indoor and outdoor datasets demonstrate our model
superiority and robust performance on real point-cloud scans compared to
state-of-the-art methods.

</details>


### [73] [Adapting Foundation Model for Dental Caries Detection with Dual-View Co-Training](https://arxiv.org/abs/2508.20813)
*Tao Luo,Han Wu,Tong Yang,Dinggang Shen,Zhiming Cui*

Main category: cs.CV

TL;DR: 本文提出了一种针对全景牙科X光片的龋齿检测新方法（DVCTNet），通过融合全局与局部信息，显著提升了检测准确率，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前龋齿自动检测方法精度不高，主要由于X光片对比度细微、病变形态多样。临床上，牙医通常结合全景与局部检查，因此亟需一种能同时利用全局和局部信息的检测方法。

Method: 作者提出DVCTNet，模拟牙医诊断流程，包含两个视角：全景（全局）和单牙（局部）。分别用两个视觉基础模型对全局与局部图片预训练，通过“门控跨视角注意力模块”动态融合双视角特征，并将融合特征反馈到检测模型用于最终判断。此外，使用公开数据集和自行标注的高精度数据集进行评估。

Result: 实验在两个数据集上均表明，DVCTNet在准确率上明显优于现有SOTA方法，验证了其有效性与泛化能力。

Conclusion: DVCTNet能够显著提升龋齿检测效果，具备实际临床应用潜力。相关代码与高质量标注数据已开源，有助于推动牙科AI领域发展。

Abstract: Accurate dental caries detection from panoramic X-rays plays a pivotal role
in preventing lesion progression. However, current detection methods often
yield suboptimal accuracy due to subtle contrast variations and diverse lesion
morphology of dental caries. In this work, inspired by the clinical workflow
where dentists systematically combine whole-image screening with detailed
tooth-level inspection, we present DVCTNet, a novel Dual-View Co-Training
network for accurate dental caries detection. Our DVCTNet starts with employing
automated tooth detection to establish two complementary views: a global view
from panoramic X-ray images and a local view from cropped tooth images. We then
pretrain two vision foundation models separately on the two views. The
global-view foundation model serves as the detection backbone, generating
region proposals and global features, while the local-view model extracts
detailed features from corresponding cropped tooth patches matched by the
region proposals. To effectively integrate information from both views, we
introduce a Gated Cross-View Attention (GCV-Atten) module that dynamically
fuses dual-view features, enhancing the detection pipeline by integrating the
fused features back into the detection model for final caries detection. To
rigorously evaluate our DVCTNet, we test it on a public dataset and further
validate its performance on a newly curated, high-precision dental caries
detection dataset, annotated using both intra-oral images and panoramic X-rays
for double verification. Experimental results demonstrate DVCTNet's superior
performance against existing state-of-the-art (SOTA) methods on both datasets,
indicating the clinical applicability of our method. Our code and labeled
dataset are available at https://github.com/ShanghaiTech-IMPACT/DVCTNet.

</details>


### [74] [FusionCounting: Robust visible-infrared image fusion guided by crowd counting via multi-task learning](https://arxiv.org/abs/2508.20817)
*He Li,Xinyu Liu,Weihang Kong,Xingchen Zhang*

Main category: cs.CV

TL;DR: 论文提出了FusionCounting，一个结合了可见光-红外图像融合（VIF）和人群计数的多任务学习框架，双向提升图像融合质量和密集场景下的人群计数性能。


<details>
  <summary>Details</summary>
Motivation: 目前VIF方法大多只关注融合图像的主观质量，而少有结合下游任务（如目标检测、语义分割）以赋予融合图像更多语义指导的信息。然而，语义分割标注成本高，目标检测在拥挤场景下存在遮挡与重叠的问题。此外，尚未有将图像融合与人群计数统一到一个框架的工作。作者旨在解决密集场景下的任务难点，同时简化标注和增强任务间的信息互补性。

Method: 提出了FusionCounting框架，在图像融合过程中引入了人群计数任务，实现了多任务协同优化。密集场景下采用人群计数替代对象检测或语义分割，利用较为简单的点标注来获取密度信息。为协调任务贡献，引入了动态损失权重策略，并结合对抗训练提升鲁棒性。

Result: 在公开数据集上进行实验，结果表明该框架实现了更高质量的图像融合效果，同时在人群计数任务上也优于现有方法。提升不仅体现在主观图像质量，还在于下游任务性能。

Conclusion: 将图像融合与人群计数结合，能够实现互利共赢，提升二者性能。所提方法在提升融合鲁棒性和下游任务表现方面具备实用意义，为后续多任务融合方法提供了新的思路。

Abstract: Most visible and infrared image fusion (VIF) methods focus primarily on
optimizing fused image quality. Recent studies have begun incorporating
downstream tasks, such as semantic segmentation and object detection, to
provide semantic guidance for VIF. However, semantic segmentation requires
extensive annotations, while object detection, despite reducing annotation
efforts compared with segmentation, faces challenges in highly crowded scenes
due to overlapping bounding boxes and occlusion. Moreover, although RGB-T crowd
counting has gained increasing attention in recent years, no studies have
integrated VIF and crowd counting into a unified framework. To address these
challenges, we propose FusionCounting, a novel multi-task learning framework
that integrates crowd counting into the VIF process. Crowd counting provides a
direct quantitative measure of population density with minimal annotation,
making it particularly suitable for dense scenes. Our framework leverages both
input images and population density information in a mutually beneficial
multi-task design. To accelerate convergence and balance tasks contributions,
we introduce a dynamic loss function weighting strategy. Furthermore, we
incorporate adversarial training to enhance the robustness of both VIF and
crowd counting, improving the model's stability and resilience to adversarial
attacks. Experimental results on public datasets demonstrate that
FusionCounting not only enhances image fusion quality but also achieves
superior crowd counting performance.

</details>


### [75] [Estimating 2D Keypoints of Surgical Tools Using Vision-Language Models with Low-Rank Adaptation](https://arxiv.org/abs/2508.20830)
*Krit Duangprom,Tryphon Lambrou,Binod Bhattarai*

Main category: cs.CV

TL;DR: 本文提出了一种新的2D手术工具关键点检测流程，通过微调视觉语言模型（VLMs），实现了比传统方法更优的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的CNN或Transformer方法在医学小数据集上容易过拟合，限制了其泛化能力，因此需要能在低数据量情况下表现良好的方法。

Method: 通过LoRA（Low Rank Adjusting）技术对VLM进行微调，并精心设计提示词，建立指令调优数据集，实现视觉特征与关键点语义描述的对齐。

Result: 仅经过两轮微调，所提出的方法就超过了基线模型，在低资源情况下表现出色，显示了LoRA的有效性。

Conclusion: 该方法不仅提升了关键点检测的精度，还为3D手术手部与工具姿态估计等未来应用奠定了基础。

Abstract: This paper presents a novel pipeline for 2D keypoint estima- tion of surgical
tools by leveraging Vision Language Models (VLMs) fine- tuned using a low rank
adjusting (LoRA) technique. Unlike traditional Convolutional Neural Network
(CNN) or Transformer-based approaches, which often suffer from overfitting in
small-scale medical datasets, our method harnesses the generalization
capabilities of pre-trained VLMs. We carefully design prompts to create an
instruction-tuning dataset and use them to align visual features with semantic
keypoint descriptions. Experimental results show that with only two epochs of
fine tuning, the adapted VLM outperforms the baseline models, demonstrating the
ef- fectiveness of LoRA in low-resource scenarios. This approach not only
improves keypoint detection performance, but also paves the way for future work
in 3D surgical hands and tools pose estimation.

</details>


### [76] [PointDGRWKV: Generalizing RWKV-like Architecture to Unseen Domains for Point Cloud Classification](https://arxiv.org/abs/2508.20835)
*Hao Yang,Qianyu Zhou,Haijia Sun,Xiangtai Li,Xuequan Lu,Lizhuang Ma,Shuicheng Yan*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于RWKV架构的点云域泛化分类方法PointDGRWKV，通过改进RWKV以适应点云数据并提升对未知域的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的点云分类在面临新域（Domain Generalization, DG）时，常因模型结构局限（如卷积的感受野有限、Transformer等长依赖能力不足、计算复杂度高）而泛化性不佳。RWKV作为新兴架构，具备良好的全局感受野、长依赖建模能力和线性复杂度，但直接用于点云DG任务存在空间失真和注意力漂移等新问题，亟需针对性改进。

Method: 本文提出PointDGRWKV方法，针对RWKV架构在点云DG任务中遇到的（1）空间失真和（2）跨域注意力漂移，设计了两大模块：1）自适应几何Token Shift（AGTS）用于提高对局部几何结构的建模能力；2）跨域Key特征分布对齐（CDFD）用于抑制注意力漂移，增强跨域鲁棒性。

Result: 在多个主流点云分类DG基准上，PointDGRWKV取得了比现有方案更优异的性能，展示出出色的泛化能力。

Conclusion: 本文工作首次展示了RWKV架构在点云域泛化分类中的潜力，并通过创新性方法解决了其直接应用中的两个关键难题，为点云域泛化任务提供了一种高效且强鲁棒性的方案。

Abstract: Domain Generalization (DG) has been recently explored to enhance the
generalizability of Point Cloud Classification (PCC) models toward unseen
domains. Prior works are based on convolutional networks, Transformer or Mamba
architectures, either suffering from limited receptive fields or high
computational cost, or insufficient long-range dependency modeling. RWKV, as an
emerging architecture, possesses superior linear complexity, global receptive
fields, and long-range dependency. In this paper, we present the first work
that studies the generalizability of RWKV models in DG PCC. We find that
directly applying RWKV to DG PCC encounters two significant challenges: RWKV's
fixed direction token shift methods, like Q-Shift, introduce spatial
distortions when applied to unstructured point clouds, weakening local
geometric modeling and reducing robustness. In addition, the Bi-WKV attention
in RWKV amplifies slight cross-domain differences in key distributions through
exponential weighting, leading to attention shifts and degraded generalization.
To this end, we propose PointDGRWKV, the first RWKV-based framework tailored
for DG PCC. It introduces two key modules to enhance spatial modeling and
cross-domain robustness, while maintaining RWKV's linear efficiency. In
particular, we present Adaptive Geometric Token Shift to model local
neighborhood structures to improve geometric context awareness. In addition,
Cross-Domain key feature Distribution Alignment is designed to mitigate
attention drift by aligning key feature distributions across domains. Extensive
experiments on multiple benchmarks demonstrate that PointDGRWKV achieves
state-of-the-art performance on DG PCC.

</details>


### [77] [PathMR: Multimodal Visual Reasoning for Interpretable Pathology Diagnosis](https://arxiv.org/abs/2508.20851)
*Ye Zhang,Yu Zhou,Jingwen Qi,Yongbing Zhang,Simon Puettmann,Finn Wichmann,Larissa Pereira Ferreira,Lara Sichward,Julius Keyl,Sylvia Hartmann,Shuo Zhao,Hongxiao Wang,Xiaowei Xu,Jianxu Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为PathMR的细胞级多模态视觉推理框架，通过结合图像分割和文本解释提升了病理影像AI诊断的可解释性与可靠性，并在多个数据集上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 虽然深度学习提升了自动化病理诊断的效率和一致性，但模型决策缺乏透明性和可追溯解释，限制了其临床应用。为解决这一问题，近期出现了能够同时生成分割掩码和文本解释的多模态视觉推理架构。

Method: 作者提出PathMR框架，针对输入的病理影像和文本查询，生成专家级诊断解释并预测细胞分布模式。该框架不仅提供图像级别的分割，还能输出与医学专家风格一致的诊断文本。方法在PathGen及新构建的GADVR两个数据集上进行了评价。

Result: 在上述两个数据集上，PathMR在文本生成质量、分割精度和跨模态对齐等方面均优于现有的可视化推理方法。

Conclusion: PathMR大幅提升了AI病理诊断的可解释性和透明化，展示了推动AI临床应用的潜力。代码也已开源，促进社区应用和研究。

Abstract: Deep learning based automated pathological diagnosis has markedly improved
diagnostic efficiency and reduced variability between observers, yet its
clinical adoption remains limited by opaque model decisions and a lack of
traceable rationale. To address this, recent multimodal visual reasoning
architectures provide a unified framework that generates segmentation masks at
the pixel level alongside semantically aligned textual explanations. By
localizing lesion regions and producing expert style diagnostic narratives,
these models deliver the transparent and interpretable insights necessary for
dependable AI assisted pathology. Building on these advancements, we propose
PathMR, a cell-level Multimodal visual Reasoning framework for Pathological
image analysis. Given a pathological image and a textual query, PathMR
generates expert-level diagnostic explanations while simultaneously predicting
cell distribution patterns. To benchmark its performance, we evaluated our
approach on the publicly available PathGen dataset as well as on our newly
developed GADVR dataset. Extensive experiments on these two datasets
demonstrate that PathMR consistently outperforms state-of-the-art visual
reasoning methods in text generation quality, segmentation accuracy, and
cross-modal alignment. These results highlight the potential of PathMR for
improving interpretability in AI-driven pathological diagnosis. The code will
be publicly available in https://github.com/zhangye-zoe/PathMR.

</details>


### [78] [Deep Learning Framework for Early Detection of Pancreatic Cancer Using Multi-Modal Medical Imaging Analysis](https://arxiv.org/abs/2508.20877)
*Dennis Slobodzian,Karissa Tilbury,Amir Kordijazi*

Main category: cs.CV

TL;DR: 本研究提出并验证了一种用于胰腺导管腺癌（PDAC）早期检测的深度学习框架，利用自体荧光和二次谐波成像分析医学影像，实现癌症诊断准确率超过90%。


<details>
  <summary>Details</summary>
Motivation: PDAC 是最致命的癌症之一，五年生存率低于10%，主要因为发现晚，因此急需更早期、准确的检测方法以提升患者预后。

Method: 研究分析了40例患者的双模态影像（自体荧光和二次谐波成像），对比六种深度学习架构（传统CNN与现代ViT），最终采用了经过修改和优化的 ResNet，冻结预训练层，并在训练时使用类别权重，来应对样本数量有限与类别不平衡。

Result: 最终的优化模型（基于ResNet）在癌症检测上取得了90%以上的准确率，远超现有人工分析方法，展示了深度学习方法在医疗影像分析中的潜力。

Conclusion: 该工作搭建了一个可用于临床的自动化PDAC检测流程，不仅有助于提高病理学家的工作效率，还为深度学习应用于小样本医学影像领域提供了方法论基础，可进一步扩展到其他癌症类型。

Abstract: Pacreatic ductal adenocarcinoma (PDAC) remains one of the most lethal forms
of cancer, with a five-year survival rate below 10% primarily due to late
detection. This research develops and validates a deep learning framework for
early PDAC detection through analysis of dual-modality imaging:
autofluorescence and second harmonic generation (SHG). We analyzed 40 unique
patient samples to create a specialized neural network capable of
distinguishing between normal, fibrotic, and cancerous tissue. Our methodology
evaluated six distinct deep learning architectures, comparing traditional
Convolutional Neural Networks (CNNs) with modern Vision Transformers (ViTs).
Through systematic experimentation, we identified and overcome significant
challenges in medical image analysis, including limited dataset size and class
imbalance. The final optimized framework, based on a modified ResNet
architecture with frozen pre-trained layers and class-weighted training,
achieved over 90% accuracy in cancer detection. This represents a significant
improvement over current manual analysis methods an demonstrates potential for
clinical deployment. This work establishes a robust pipeline for automated PDAC
detection that can augment pathologists' capabilities while providing a
foundation for future expansion to other cancer types. The developed
methodology also offers valuable insights for applying deep learning to
limited-size medical imaging datasets, a common challenge in clinical
applications.

</details>


### [79] [Understanding and evaluating computer vision models through the lens of counterfactuals](https://arxiv.org/abs/2508.20881)
*Pushkar Shukla*

Main category: cs.CV

TL;DR: 本论文提出了一系列基于反事实推理的方法，用于解释、审查并减轻视觉分类器和生成模型中的偏见，从而提升人工智能的可解释性和公平性。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型，尤其是视觉分类器和生成模型，面临可解释性不足和社会偏见等问题，亟需系统化工具对模型决策中的因果关系和偏见进行分析和修正。

Method: 本文针对判别模型和生成模型分别提出多种方法：（1）CAVLI将LIME和TCAV结合，通过概念层面分析和热力图量化模型决策对人类解释性概念的依赖；（2）ASAC引入对受保护属性的对抗性反事实扰动，并采用课程学习优化模型偏见和准确性；（3）TIBET提出可扩展流程评估文本-图像生成模型对提示词敏感的偏见；（4）BiasConnect构建因果图诊断交互性、多重偏见；（5）InterMit通过因果敏感性分数以及用户定义公平目标，以无训练方式缓解交互偏见。

Result: 这些方法有效发现并量化了如背景等无关因素对模型决策的影响，提升了模型对关键属性（如性别、种族、年龄）偏见的检测与消除能力，并在理论与实证上优化了分类与生成任务的公平性和准确性。

Conclusion: 反事实推理可作为统一视角，跨判别与生成模型实现可解释性、公平性与因果性分析，并为大规模、社会责任导向的模型偏见评估与缓解建立了理论和方法基础。

Abstract: Counterfactual reasoning -- the practice of asking ``what if'' by varying
inputs and observing changes in model behavior -- has become central to
interpretable and fair AI. This thesis develops frameworks that use
counterfactuals to explain, audit, and mitigate bias in vision classifiers and
generative models. By systematically altering semantically meaningful
attributes while holding others fixed, these methods uncover spurious
correlations, probe causal dependencies, and help build more robust systems.
  The first part addresses vision classifiers. CAVLI integrates attribution
(LIME) with concept-level analysis (TCAV) to quantify how strongly decisions
rely on human-interpretable concepts. With localized heatmaps and a Concept
Dependency Score, CAVLI shows when models depend on irrelevant cues like
backgrounds. Extending this, ASAC introduces adversarial counterfactuals that
perturb protected attributes while preserving semantics. Through curriculum
learning, ASAC fine-tunes biased models for improved fairness and accuracy
while avoiding stereotype-laden artifacts.
  The second part targets generative Text-to-Image (TTI) models. TIBET provides
a scalable pipeline for evaluating prompt-sensitive biases by varying
identity-related terms, enabling causal auditing of how race, gender, and age
affect image generation. To capture interactions, BiasConnect builds causal
graphs diagnosing intersectional biases. Finally, InterMit offers a modular,
training-free algorithm that mitigates intersectional bias via causal
sensitivity scores and user-defined fairness goals.
  Together, these contributions show counterfactuals as a unifying lens for
interpretability, fairness, and causality in both discriminative and generative
models, establishing principled, scalable methods for socially responsible bias
evaluation and mitigation.

</details>


### [80] [Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation](https://arxiv.org/abs/2508.20909)
*Yifan Gao,Haoyue Li,Feng Yuan,Xiaosong Wang,Xin Gao*

Main category: cs.CV

TL;DR: 本文提出了Dino U-Net，一种结合DINOv3视觉基础模型的UNet结构，在医疗图像分割任务中取得了领先性能。通过创新特征融合与精度保持模块，有效提升了医学图像分割的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 虽然大型自然图像基础模型为迁移到医疗图像分割提供了新思路，但如何有效地将其泛化特征适用于临床高精度场景仍有挑战。本文动机在于充分释放基础模型在医学分割中的潜力，解决模型特征转移和精度损失问题。

Method: 作者提出Dino U-Net，一种编码-解码架构。编码器基于冻结的DINOv3主干，利用专用适配器将高层语义特征与低层空间细节融合。为处理降维时特征损失，引入了FAPM（精度感知投影模块）以优化特征传递至解码器。

Result: 在七个公开医疗图像分割数据集上实验，Dino U-Net在各类成像模式上均超越现有方法，取得了最新最优表现。随着主干模型参数量的增加，分割精度持续提升，并展现较好的可扩展性及参数效率。

Conclusion: 结果表明基于大型视觉基础模型的高质量稠密特征，通过有效结构与特征融合策略，能大幅提升医疗图像分割的精度与效率，为该领域带来参数高效且可扩展的新方法。

Abstract: Foundation models pre-trained on large-scale natural image datasets offer a
powerful paradigm for medical image segmentation. However, effectively
transferring their learned representations for precise clinical applications
remains a challenge. In this work, we propose Dino U-Net, a novel
encoder-decoder architecture designed to exploit the high-fidelity dense
features of the DINOv3 vision foundation model. Our architecture introduces an
encoder built upon a frozen DINOv3 backbone, which employs a specialized
adapter to fuse the model's rich semantic features with low-level spatial
details. To preserve the quality of these representations during dimensionality
reduction, we design a new fidelity-aware projection module (FAPM) that
effectively refines and projects the features for the decoder. We conducted
extensive experiments on seven diverse public medical image segmentation
datasets. Our results show that Dino U-Net achieves state-of-the-art
performance, consistently outperforming previous methods across various imaging
modalities. Our framework proves to be highly scalable, with segmentation
accuracy consistently improving as the backbone model size increases up to the
7-billion-parameter variant. The findings demonstrate that leveraging the
superior, dense-pretrained features from a general-purpose foundation model
provides a highly effective and parameter-efficient approach to advance the
accuracy of medical image segmentation. The code is available at
https://github.com/yifangao112/DinoUNet.

</details>


### [81] [Classifying Mitotic Figures in the MIDOG25 Challenge with Deep Ensemble Learning and Rule Based Refinement](https://arxiv.org/abs/2508.20919)
*Sara Krauss,Ellena Spieß,Daniel Hieber,Frank Kramer,Johannes Schobel,Dominik Müller*

Main category: cs.CV

TL;DR: 本文提出了一种基于ConvNeXtBase集成模型结合规则修正模块（RBR）的方法，用于区分肿瘤病理图像中的正常与非典型有丝分裂像，取得了较高准确率。


<details>
  <summary>Details</summary>
Motivation: 手动标注和区分肿瘤组织中的非典型与正常有丝分裂像既耗时又主观，因此需要自动化、客观的方法提升分型效率和准确性。

Method: 作者使用ConvNeXtBase深度学习模型集成，并利用AUCMEDI进行训练，随后加入规则修正模块（RBR），以改善模型结果的特定指标。

Result: 在MIDOG25测试集上，深度集成模型取得了84.02%的平衡准确率。规则修正模块提升了特异性，但牺牲了敏感性和整体性能。

Conclusion: 深度集成模型在非典型有丝分裂像分类任务中表现优异，规则修正可微调特定指标，但仍需更多研究完善。

Abstract: Mitotic figures (MFs) are relevant biomarkers in tumor grading.
Differentiating atypical MFs (AMFs) from normal MFs (NMFs) remains difficult,
as manual annotation is time-consuming and subjective. In this work an ensemble
of ConvNeXtBase models was trained with AUCMEDI and extend with a rule-based
refinement (RBR) module. On the MIDOG25 preliminary test set, the ensemble
achieved a balanced accuracy of 84.02%. While the RBR increased specificity, it
reduced sensitivity and overall performance. The results show that deep
ensembles perform well for AMF classification. RBR can increase specific
metrics but requires further research.

</details>


### [82] [Olive Tree Satellite Image Segmentation Based On SAM and Multi-Phase Refinement](https://arxiv.org/abs/2508.20954)
*Amir Jmal,Chaima Chtourou,Mahdi Louati,Abdelaziz Kallel,Houda Khmila*

Main category: cs.CV

TL;DR: 本论文提出了一种结合前沿基础模型与分割技术的方法，极大提升了卫星图像中橄榄树的识别与分割精度，最终准确率达98%。


<details>
  <summary>Details</summary>
Motivation: 全球气候变化加剧，导致橄榄树资源面临威胁，因此通过遥感技术及早发现异常、维持橄榄多样性对于有效管理至关重要。

Method: 本研究结合了Segment Anything Model（SAM）与字段中橄榄树的排列校正，以及可学习的形状与尺寸约束，对卫星图像中的橄榄树进行识别和分割。

Result: 通过改进的分割方法，最终准确率从SAM原始的82%提升至98%。

Conclusion: 集成基础模型、空间排列校正与可学习约束，能极大提升橄榄树遥感识别分割的准确性，对橄榄多样性管理有重要意义。

Abstract: In the context of proven climate change, maintaining olive biodiversity
through early anomaly detection and treatment using remote sensing technology
is crucial, offering effective management solutions. This paper presents an
innovative approach to olive tree segmentation from satellite images. By
leveraging foundational models and advanced segmentation techniques, the study
integrates the Segment Anything Model (SAM) to accurately identify and segment
olive trees in agricultural plots. The methodology includes SAM segmentation
and corrections based on trees alignement in the field and a learanble
constraint about the shape and the size. Our approach achieved a 98\% accuracy
rate, significantly surpassing the initial SAM performance of 82\%.

</details>


### [83] [E-ConvNeXt: A Lightweight and Efficient ConvNeXt Variant with Cross-Stage Partial Connections](https://arxiv.org/abs/2508.20955)
*Fang Wang,Huitao Li,Wenhan Chao,Zheng Zhuo,Yiran Ji,Chang Peng,Yupeng Sun*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的轻量化网络E-ConvNeXt，在保持高准确率的同时大幅降低了模型的参数量和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 当前多数高性能网络没有针对轻量级应用场景进行设计，导致其难以广泛适用。因此需要针对轻量化和高效能做网络结构优化。

Method: 以ConvNeXt为基础，引入了Cross Stage Partial Network（CSPNet）机制，并优化了Stem和Block结构，用通道注意力代替了Layer Scale，整体显著减小了模型复杂度。

Result: 在ImageNet分类实验中，E-ConvNeXt-mini在0.9GFLOPs下达到78.3%的Top-1准确率，E-ConvNeXt-small在3.1GFLOPs下达到81.9%的Top-1准确率。在目标检测任务迁移学习测试上，也展示了良好的泛化能力。

Conclusion: E-ConvNeXt在多种复杂度配置下都能保持高准确性，实现了优越的精度-效率平衡，并在分类与检测任务均表现出色，适用于轻量级应用场景。

Abstract: Many high-performance networks were not designed with lightweight application
scenarios in mind from the outset, which has greatly restricted their scope of
application. This paper takes ConvNeXt as the research object and significantly
reduces the parameter scale and network complexity of ConvNeXt by integrating
the Cross Stage Partial Connections mechanism and a series of optimized
designs. The new network is named E-ConvNeXt, which can maintain high accuracy
performance under different complexity configurations. The three core
innovations of E-ConvNeXt are : (1) integrating the Cross Stage Partial Network
(CSPNet) with ConvNeXt and adjusting the network structure, which reduces the
model's network complexity by up to 80%; (2) Optimizing the Stem and Block
structures to enhance the model's feature expression capability and operational
efficiency; (3) Replacing Layer Scale with channel attention. Experimental
validation on ImageNet classification demonstrates E-ConvNeXt's superior
accuracy-efficiency balance: E-ConvNeXt-mini reaches 78.3% Top-1 accuracy at
0.9GFLOPs. E-ConvNeXt-small reaches 81.9% Top-1 accuracy at 3.1GFLOPs. Transfer
learning tests on object detection tasks further confirm its generalization
capability.

</details>


### [84] [DrivingGaussian++: Towards Realistic Reconstruction and Editable Simulation for Surrounding Dynamic Driving Scenes](https://arxiv.org/abs/2508.20965)
*Yajiao Xiong,Xiaoyu Zhou,Yongtao Wan,Deqing Sun,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: DrivingGaussian++是一个可用于自动驾驶场景真实重建和可控编辑的高效方法，能够生成多样、真实的动态场景，并支持训练时无需编辑。


<details>
  <summary>Details</summary>
Motivation: 当前动态自动驾驶场景的三维重建与可控编辑面临精度有限和灵活性不足的问题，尤其是在复现物体运动和环境多样性方面。

Method: 该方法用增量式3D高斯模型重建静态背景，用动态高斯图重建运动物体，同时集成激光雷达（LiDAR）先验提升细节和一致性，并利用多视图图像和深度信息实现无需训练的可控编辑。此外，结合大语言模型（LLMs），自动生成动态物体轨迹并优化其真实感。

Result: 在动态场景的重建与光真实感环视合成方面超越现有方法，并支持包括纹理、天气、物体操控等各种场景编辑，生成多视角、多样化的动态场景。

Conclusion: DrivingGaussian++能高效、一致地进行真实动态场景重建和可控编辑，尤其增强了场景的多样性和生成质量。

Abstract: We present DrivingGaussian++, an efficient and effective framework for
realistic reconstructing and controllable editing of surrounding dynamic
autonomous driving scenes. DrivingGaussian++ models the static background using
incremental 3D Gaussians and reconstructs moving objects with a composite
dynamic Gaussian graph, ensuring accurate positions and occlusions. By
integrating a LiDAR prior, it achieves detailed and consistent scene
reconstruction, outperforming existing methods in dynamic scene reconstruction
and photorealistic surround-view synthesis. DrivingGaussian++ supports
training-free controllable editing for dynamic driving scenes, including
texture modification, weather simulation, and object manipulation, leveraging
multi-view images and depth priors. By integrating large language models (LLMs)
and controllable editing, our method can automatically generate dynamic object
motion trajectories and enhance their realism during the optimization process.
DrivingGaussian++ demonstrates consistent and realistic editing results and
generates dynamic multi-view driving scenarios, while significantly enhancing
scene diversity. More results and code can be found at the project site:
https://xiong-creator.github.io/DrivingGaussian_plus.github.io

</details>


### [85] [Webly-Supervised Image Manipulation Localization via Category-Aware Auto-Annotation](https://arxiv.org/abs/2508.20987)
*Chenfan Qu,Yiwu Zhong,Bin Li,Lianwen Jin*

Main category: cs.CV

TL;DR: 本文提出了一种新的以网络数据为基础的图像篡改区域定位方法，构建了规模庞大且高质量的像素级标注数据集MIMLv2，并提出了提升训练质量的新指标和方法，显著提高了现有图像篡改检测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于图像编辑工具的图像篡改已对社会信息安全造成严重挑战，然而，精确定位图像中的篡改区域仍然困难，主要障碍在于高质量、标注精确的数据集严重匮乏且采集成本高昂。本文旨在通过新的方法和大规模数据集缓解数据稀缺问题，从而推动图像篡改定位技术的发展。

Method: 1）利用网络上手工伪造图片作为数据来源；2）借助辅助任务和约束，提出CAAAv2方法，实现对篡改区域的自动、准确像素级标注；3）创新性提出QES指标，筛除低质量标注；4）综合生成MIMLv2大规模、高质量数据集；5）通过Object Jitter技术增强训练示例多样性；6）以此为基础提出Web-IML模型，更有效利用网络级别监督数据进行篡改定位。

Result: MIMLv2数据集拥有24万多张像素级手工篡改图像，是目前最大标注质量最高的数据集之一，规模为IMD20等老数据集的120倍；Web-IML模型在多个真实场景伪造基准数据上的表现显著超越现有方法，最高性能提升达31%，相较SOTA方法TruFor平均IoU提升24.1个百分点。

Conclusion: 通过充分挖掘和利用网络广泛存在的伪造图片及自动标注机制，显著缓解了图像篡改定位的数据稀缺问题，推动领域技术进步，实现了实际应用中更强、更泛化的伪造区域检测能力。相关数据集与代码将公开发布，便于学术和工业界进一步研究与落地。

Abstract: Images manipulated using image editing tools can mislead viewers and pose
significant risks to social security. However, accurately localizing the
manipulated regions within an image remains a challenging problem. One of the
main barriers in this area is the high cost of data acquisition and the severe
lack of high-quality annotated datasets. To address this challenge, we
introduce novel methods that mitigate data scarcity by leveraging readily
available web data. We utilize a large collection of manually forged images
from the web, as well as automatically generated annotations derived from a
simpler auxiliary task, constrained image manipulation localization.
Specifically, we introduce a new paradigm CAAAv2, which automatically and
accurately annotates manipulated regions at the pixel level. To further improve
annotation quality, we propose a novel metric, QES, which filters out
unreliable annotations. Through CAAA v2 and QES, we construct MIMLv2, a
large-scale, diverse, and high-quality dataset containing 246,212 manually
forged images with pixel-level mask annotations. This is over 120x larger than
existing handcrafted datasets like IMD20. Additionally, we introduce Object
Jitter, a technique that further enhances model training by generating
high-quality manipulation artifacts. Building on these advances, we develop a
new model, Web-IML, designed to effectively leverage web-scale supervision for
the image manipulation localization task. Extensive experiments demonstrate
that our approach substantially alleviates the data scarcity problem and
significantly improves the performance of various models on multiple real-world
forgery benchmarks. With the proposed web supervision, Web-IML achieves a
striking performance gain of 31% and surpasses previous SOTA TruFor by 24.1
average IoU points. The dataset and code will be made publicly available at
https://github.com/qcf-568/MIML.

</details>


### [86] [ExpertSim: Fast Particle Detector Simulation Using Mixture-of-Generative-Experts](https://arxiv.org/abs/2508.20991)
*Patryk Będkowski,Jan Dubiński,Filip Szatkowski,Kamil Deja,Przemysław Rokita,Tomasz Trzciński*

Main category: cs.CV

TL;DR: 该论文提出了一种基于深度学习的粒子探测器响应仿真方法ExpertSim，采用生成专家混合模型解决传统统计蒙特卡洛方法计算量大、效率低的问题，并在ALICE实验中的零度量热器模拟实验中验证了其效率和准确性。


<details>
  <summary>Details</summary>
Motivation: CERN大型强子对撞机中的粒子碰撞模拟严重依赖于传统统计蒙特卡洛方法，计算开销巨大，急需更高效的模拟方法。尤其在ALICE实验中，数据分布复杂，常规生成模型难以刻画全部细节，因此需要针对性方法提高模拟效率和精度。

Method: 提出ExpertSim方法，核心为生成专家混合网络架构（Mixture-of-Generative-Experts），针对不同类型的数据由不同的生成专家负责仿真，从而细致拟合数据分布，提高模拟准确性和效率。

Result: ExpertSim在ALICE实验零度量热器响应数据上的应用表明，其比传统蒙特卡洛方法在仿真速度上有显著提升，同时在模拟精度上也有提高。

Conclusion: ExpertSim为高能物理实验中的高效率、高精度检测器响应模拟提供了有前景的替代方案，有助于缓解CERN计算资源紧张的问题。

Abstract: Simulating detector responses is a crucial part of understanding the inner
workings of particle collisions in the Large Hadron Collider at CERN. Such
simulations are currently performed with statistical Monte Carlo methods, which
are computationally expensive and put a significant strain on CERN's
computational grid. Therefore, recent proposals advocate for generative machine
learning methods to enable more efficient simulations. However, the
distribution of the data varies significantly across the simulations, which is
hard to capture with out-of-the-box methods. In this study, we present
ExpertSim - a deep learning simulation approach tailored for the Zero Degree
Calorimeter in the ALICE experiment. Our method utilizes a
Mixture-of-Generative-Experts architecture, where each expert specializes in
simulating a different subset of the data. This allows for a more precise and
efficient generation process, as each expert focuses on a specific aspect of
the calorimeter response. ExpertSim not only improves accuracy, but also
provides a significant speedup compared to the traditional Monte-Carlo methods,
offering a promising solution for high-efficiency detector simulations in
particle physics experiments at CERN. We make the code available at
https://github.com/patrick-bedkowski/expertsim-mix-of-generative-experts.

</details>


### [87] [POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models](https://arxiv.org/abs/2508.21019)
*Jiaxiang Cheng,Bing Ma,Xuhua Ren,Hongyi Jin,Kai Yu,Peng Zhang,Wenyue Li,Yuan Zhou,Tianxiang Zheng,Qinglin Lu*

Main category: cs.CV

TL;DR: POSE是一种新的视频扩散模型蒸馏框架，极大提升了采样效率，实现了高质量视频的一步生成。


<details>
  <summary>Details</summary>
Motivation: 现有加速方法未能有效建模视频帧的时序一致性，且不支持大规模模型单步蒸馏，导致高效高质量生成受限。

Method: POSE采用两阶段流程：（1）稳定预热，适应单步生成器在不同信噪比下的高质量轨迹，提升单步映射质量；（2）统一自对抗均衡，通过自对抗蒸馏在高斯噪音空间内实现稳定训练，令单步生成更接近真实视频。对于条件生成，还引入条件对抗一致性以提升语义和帧一致性。

Result: 在VBench-I2V上，POSE在语义对齐、时序一致性和帧质量方面平均超越其他方法7.15%；模型推理延迟由1000秒降至10秒（提升100倍），同时保持有竞争力的性能。

Conclusion: POSE显著提升了大规模视频扩散模型的采样效率与生成质量，有效解决了现有方法的局限性，为高效高质量视频生成提供了新方案。

Abstract: The field of video diffusion generation faces critical bottlenecks in
sampling efficiency, especially for large-scale models and long sequences.
Existing video acceleration methods adopt image-based techniques but suffer
from fundamental limitations: they neither model the temporal coherence of
video frames nor provide single-step distillation for large-scale video models.
To bridge this gap, we propose POSE (Phased One-Step Equilibrium), a
distillation framework that reduces the sampling steps of large-scale video
diffusion models, enabling the generation of high-quality videos in a single
step. POSE employs a carefully designed two-phase process to distill video
models:(i) stability priming: a warm-up mechanism to stabilize adversarial
distillation that adapts the high-quality trajectory of the one-step generator
from high to low signal-to-noise ratio regimes, optimizing the video quality of
single-step mappings near the endpoints of flow trajectories. (ii) unified
adversarial equilibrium: a flexible self-adversarial distillation mechanism
that promotes stable single-step adversarial training towards a Nash
equilibrium within the Gaussian noise space, generating realistic single-step
videos close to real videos. For conditional video generation, we propose (iii)
conditional adversarial consistency, a method to improve both semantic
consistency and frame consistency between conditional frames and generated
frames. Comprehensive experiments demonstrate that POSE outperforms other
acceleration methods on VBench-I2V by average 7.15% in semantic alignment,
temporal conference and frame quality, reducing the latency of the pre-trained
model by 100$\times$, from 1000 seconds to 10 seconds, while maintaining
competitive performance.

</details>


### [88] [Reusing Computation in Text-to-Image Diffusion for Efficient Generation of Image Sets](https://arxiv.org/abs/2508.21032)
*Dale Decatur,Thibault Groueix,Wang Yifan,Rana Hanocka,Vladimir Kim,Matheus Gadelha*

Main category: cs.CV

TL;DR: 本文针对文本生成图像扩散模型推理计算开销高的问题，提出通过同类prompt共享部分扩散步骤的计算，以提升整体推理效率和图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有工作多聚焦于单次推理流程的优化，而在处理大量相关prompt时存在许多重复计算，提升效率和降低成本具有重要的实际意义。

Method: 提出无需重新训练的方案：首先基于语义相似性聚类prompt，对相似prompt的生成流程，在早期扩散步骤共享计算分支，仅在后期细化生成差异；同时结合UnClip的prior策略优化扩散步骤分配；该方法能无缝对接现有模型与pipeline。

Result: 实验显示，在基于图像embedding条件训练的扩散模型下，该方法显著降低了计算消耗，同时提升了生成图像质量，并能良好扩展到大批量prompt集合。

Conclusion: 本文提出的方法有效缓解大规模文本生成图像场景的环境与财务负担，具有良好的实用性和可扩展性，为相关任务提供高效解决方案。

Abstract: Text-to-image diffusion models enable high-quality image generation but are
computationally expensive. While prior work optimizes per-inference efficiency,
we explore an orthogonal approach: reducing redundancy across correlated
prompts. Our method leverages the coarse-to-fine nature of diffusion models,
where early denoising steps capture shared structures among similar prompts. We
propose a training-free approach that clusters prompts based on semantic
similarity and shares computation in early diffusion steps. Experiments show
that for models trained conditioned on image embeddings, our approach
significantly reduces compute cost while improving image quality. By leveraging
UnClip's text-to-image prior, we enhance diffusion step allocation for greater
efficiency. Our method seamlessly integrates with existing pipelines, scales
with prompt sets, and reduces the environmental and financial burden of
large-scale text-to-image generation. Project page:
https://ddecatur.github.io/hierarchical-diffusion/

</details>


### [89] [Mitosis detection in domain shift scenarios: a Mamba-based approach](https://arxiv.org/abs/2508.21033)
*Gennaro Percannella,Mattia Sarno,Francesco Tortorella,Mario Vento*

Main category: cs.CV

TL;DR: 本文提出了一种基于Mamba架构的有丝分裂检测方法，以解决组织病理图像在域转移情况下检测准确率下降的问题。


<details>
  <summary>Details</summary>
Motivation: 有丝分裂检测对肿瘤评估至关重要，但现有机器学习算法在遇到与训练域不同的新图像时表现明显下降，因此需要提升模型在域转移下的泛化能力。

Method: 作者提出利用Mamba为基础的VM-UNet架构，并结合染色情况增强（stain augmentation）的方法，提升模型对域转移的鲁棒性。该方案应用于MItosis DOmain Generalization (MIDOG) 挑战赛，并在MIDOG++数据集上进行了初步实验。

Result: 实验结果显示，该方法在MIDOG++数据集上仍有较大的提升空间，说明目前性能尚未达到理想状态。

Conclusion: 虽然Mamba-based方法具有潜力提升组织病理图像有丝分裂检测在域转移情况下的表现，但目前仍需进一步优化和改进，以真正提升其实用性。

Abstract: Mitosis detection in histopathology images plays a key role in tumor
assessment. Although machine learning algorithms could be exploited for aiding
physicians in accurately performing such a task, these algorithms suffer from
significative performance drop when evaluated on images coming from domains
that are different from the training ones. In this work, we propose a
Mamba-based approach for mitosis detection under domain shift, inspired by the
promising performance demonstrated by Mamba in medical imaging segmentation
tasks. Specifically, our approach exploits a VM-UNet architecture for carrying
out the addressed task, as well as stain augmentation operations for further
improving model robustness against domain shift. Our approach has been
submitted to the track 1 of the MItosis DOmain Generalization (MIDOG)
challenge. Preliminary experiments, conducted on the MIDOG++ dataset, show
large room for improvement for the proposed method.

</details>


### [90] [A multi-task neural network for atypical mitosis recognition under domain shift](https://arxiv.org/abs/2508.21035)
*Gennaro Percannella,Mattia Sarno,Francesco Tortorella,Mario Vento*

Main category: cs.CV

TL;DR: 本文提出利用多任务学习方法提升病理图像中非典型有丝分裂体识别的鲁棒性，有效缓解了领域变化带来的模型性能下降。


<details>
  <summary>Details</summary>
Motivation: 自动识别病理图像中的非典型有丝分裂体有助于准确评估肿瘤的侵袭性，但现有机器学习方法在不同数据领域下性能容易急剧下滑，因此需要提升其泛化能力。

Method: 采用多任务学习，通过引入与主要分类任务相关的辅助任务，引导模型更好聚焦于待识别对象本身，减少背景等领域相关干扰，从而增强模型的领域适应性。该方法在MIDOG大赛第二赛道中提出，并在多个数据集上进行了评测。

Result: 在MIDOG 2025 Atypical Training Set、Ami-Br 数据集及MIDOG25 challenge初步测试集上，所提方法取得了有前景的初步性能。

Conclusion: 多任务学习结合辅助任务能够有效减轻病理图像领域变换对非典型有丝分裂体识别性能的影响，有望提升相关医疗自动化工具的泛化能力。

Abstract: Recognizing atypical mitotic figures in histopathology images allows
physicians to correctly assess tumor aggressiveness. Although machine learning
models could be exploited for automatically performing such a task, under
domain shift these models suffer from significative performance drops. In this
work, an approach based on multi-task learning is proposed for addressing this
problem. By exploiting auxiliary tasks, correlated to the main classification
task, the proposed approach, submitted to the track 2 of the MItosis DOmain
Generalization (MIDOG) challenge, aims to aid the model to focus only on the
object to classify, ignoring the domain varying background of the image. The
proposed approach shows promising performance in a preliminary evaluation
conducted on three distinct datasets, i.e., the MIDOG 2025 Atypical Training
Set, the Ami-Br dataset, as well as the preliminary test set of the MIDOG25
challenge.

</details>


### [91] [FW-GAN: Frequency-Driven Handwriting Synthesis with Wave-Modulated MLP Generator](https://arxiv.org/abs/2508.21040)
*Huynh Tong Dang Khoa,Dang Hoai Nam,Vo Nguyen Le Duy*

Main category: cs.CV

TL;DR: 本论文提出FW-GAN，一种能从单个样本生成逼真、风格一致手写体的合成方法，解决了手写识别中样本稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 现有手写体合成方法多数基于卷积结构，难以捕捉长距离依赖和复杂笔画模式，而且普遍忽视了频域信息对风格和结构细节的刻画作用。为提升生成数据的质量和多样性，亟需改进网络结构和判别方式。

Method: 提出FW-GAN框架，使用phase-aware Wave-MLP生成器提升空间关系和风格细节的建模能力，并引入以高频成分为指导的判别器提升生成样本的真实性。同时设计了Frequency Distribution Loss，使得合成手写体的频域分布与真实数据一致。

Result: 在越南语和英语手写体数据集上实验表明，FW-GAN能生成高质量且风格一致的手写体图片，对低资源手写识别领域具有实用价值。

Conclusion: FW-GAN提供了一种有效的手写体数据增强工具，促进了手写体识别模型在样本稀缺场景下的性能提升。

Abstract: Labeled handwriting data is often scarce, limiting the effectiveness of
recognition systems that require diverse, style-consistent training samples.
Handwriting synthesis offers a promising solution by generating artificial data
to augment training. However, current methods face two major limitations.
First, most are built on conventional convolutional architectures, which
struggle to model long-range dependencies and complex stroke patterns. Second,
they largely ignore the crucial role of frequency information, which is
essential for capturing fine-grained stylistic and structural details in
handwriting. To address these challenges, we propose FW-GAN, a one-shot
handwriting synthesis framework that generates realistic, writer-consistent
text from a single example. Our generator integrates a phase-aware Wave-MLP to
better capture spatial relationships while preserving subtle stylistic cues. We
further introduce a frequency-guided discriminator that leverages
high-frequency components to enhance the authenticity detection of generated
samples. Additionally, we introduce a novel Frequency Distribution Loss that
aligns the frequency characteristics of synthetic and real handwriting, thereby
enhancing visual fidelity. Experiments on Vietnamese and English handwriting
datasets demonstrate that FW-GAN generates high-quality, style-consistent
handwriting, making it a valuable tool for augmenting data in low-resource
handwriting recognition (HTR) pipelines. Official implementation is available
at https://github.com/DAIR-Group/FW-GAN

</details>


### [92] [MMG-Vid: Maximizing Marginal Gains at Segment-level and Token-level for Efficient Video LLMs](https://arxiv.org/abs/2508.21044)
*Junpeng Ma,Qizhe Zhang,Ming Lu,Zhibin Wang,Qiang Zhou,Jun Song,Shanghang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种高效的视频视觉大语言模型（VLLMs）视觉token剪枝框架MMG-Vid，极大减少计算量且几乎无性能损失。


<details>
  <summary>Details</summary>
Motivation: 现有VLLM在实际应用中因视觉token过多带来巨大计算压力，且现有token剪枝方法未考虑视频的动态特性与时序依赖。作者希望通过更智能的token分配与剪枝机制提升推理效率。

Method: MMG-Vid方法分为两阶段：首先，根据帧相似性将视频划分为多个段，并动态为每个段分配token预算以最大化边际收益；其次，提出时序引导的DPC算法，建模帧间独特性和帧内多样性，进一步优化每个token。整个方法无需训练即可使用。

Result: MMG-Vid能在保持超过99.5%原始性能的前提下，削减75%的视觉tokens，并在LLaVA-OneVision-7B模型prefilling阶段实现3.9倍加速。

Conclusion: MMG-Vid显著提高了VLLM在视频理解中的推理效率且几乎无性能损失，具有极高的实际应用价值。

Abstract: Video Large Language Models (VLLMs) excel in video understanding, but their
excessive visual tokens pose a significant computational challenge for
real-world applications. Current methods aim to enhance inference efficiency by
visual token pruning. However, they do not consider the dynamic characteristics
and temporal dependencies of video frames, as they perceive video understanding
as a multi-frame task. To address these challenges, we propose MMG-Vid, a novel
training-free visual token pruning framework that removes redundancy by
Maximizing Marginal Gains at both segment-level and token-level. Specifically,
we first divide the video into segments based on frame similarity, and then
dynamically allocate the token budget for each segment to maximize the marginal
gain of each segment. Subsequently, we propose a temporal-guided DPC algorithm
that jointly models inter-frame uniqueness and intra-frame diversity, thereby
maximizing the marginal gain of each token. By combining both stages, MMG-Vid
can maximize the utilization of the limited token budget, significantly
improving efficiency while maintaining strong performance. Extensive
experiments demonstrate that MMG-Vid can maintain over 99.5% of the original
performance, while effectively reducing 75% visual tokens and accelerating the
prefilling stage by 3.9x on LLaVA-OneVision-7B. Code will be released soon.

</details>


### [93] [Veritas: Generalizable Deepfake Detection via Pattern-Aware Reasoning](https://arxiv.org/abs/2508.21048)
*Hao Tan,Jun Lan,Zichang Tan,Ajian Liu,Chuanbiao Song,Senyuan Shi,Huijia Zhu,Weiqiang Wang,Jun Wan,Zhen Lei*

Main category: cs.CV

TL;DR: 该论文提出了一个名为HydraFake的新深度伪造数据集，并基于此开发了Veritas多模态大语言模型检测器，有效提升了在现实场景下深度伪造检测的实用性与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测基准与工业实际存在差距，包括训练样本同质化和测试图片质量低，导致检测器在真实场景下效果不理想。为弥补这一差距，作者希望模拟实际应用的复杂性，提升检测器的泛化能力。

Method: 1. 构建HydraFake数据集，涵盖多种深度伪造技术和真实伪造样本，设计分层泛化测试，覆盖未见过的模型架构、新型伪造技术和数据域。2. 基于MLLM提出Veritas检测器，通过pattern-aware reasoning（如规划与自我反思）模拟人类取证推理流程。3. 设计两阶段训练流程，将深度伪造推理能力有效融入现有MLLM模型。

Result: 1. 以HydraFake为评测基准，传统检测器在跨模型泛化能力表现不错，但对新型伪造和新数据域表现不佳。2. Veritas在多种OOD（out-of-domain）场景下显著优于以往方法，并能给出更具解释性的检测结果。

Conclusion: HydraFake数据集和Veritas检测器显著提升了深度伪造检测在复杂真实世界环境下的性能和可信度，为实际应用提供了更为可靠的解决方案。

Abstract: Deepfake detection remains a formidable challenge due to the complex and
evolving nature of fake content in real-world scenarios. However, existing
academic benchmarks suffer from severe discrepancies from industrial practice,
typically featuring homogeneous training sources and low-quality testing
images, which hinder the practical deployments of current detectors. To
mitigate this gap, we introduce HydraFake, a dataset that simulates real-world
challenges with hierarchical generalization testing. Specifically, HydraFake
involves diversified deepfake techniques and in-the-wild forgeries, along with
rigorous training and evaluation protocol, covering unseen model architectures,
emerging forgery techniques and novel data domains. Building on this resource,
we propose Veritas, a multi-modal large language model (MLLM) based deepfake
detector. Different from vanilla chain-of-thought (CoT), we introduce
pattern-aware reasoning that involves critical reasoning patterns such as
"planning" and "self-reflection" to emulate human forensic process. We further
propose a two-stage training pipeline to seamlessly internalize such deepfake
reasoning capacities into current MLLMs. Experiments on HydraFake dataset
reveal that although previous detectors show great generalization on
cross-model scenarios, they fall short on unseen forgeries and data domains.
Our Veritas achieves significant gains across different OOD scenarios, and is
capable of delivering transparent and faithful detection outputs.

</details>


### [94] [FakeParts: a New Family of AI-Generated DeepFakes](https://arxiv.org/abs/2508.21052)
*Gaetan Brison,Soobash Daiboo,Samy Aimeur,Awais Hussain Sani,Xi Wang,Gianni Franchi,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: 本文提出了“FakeParts”这一新型深度伪造类别，其特征为只对视频的部分区域或片段进行细微但隐蔽的篡改。作者还发布了首个用于部分深度伪造检测的大型数据集FakePartsBench，并通过实验显示，无论是人类还是现有检测模型，在识别FakeParts时的准确率都大幅下降。


<details>
  <summary>Details</summary>
Motivation: 现有的深度伪造检测方法主要针对完全合成或大范围修改的视频，但对于仅做局部、细微改动的“部分深度伪造”，几乎没有研究和有效检测方法，这成为当前检测技术的重大漏洞。

Method: 作者提出了“FakeParts”的概念，定义了带有局部篡改的深度伪造视频类型，并构建了包含超过2.5万条视频、带有像素级和帧级标注的FakePartsBench数据集，用于全面测试部分深度伪造的检测能力。同时，作者进行了用户实验与现有模型的评测。

Result: 用户实验表明，面对FakeParts这种部分深度伪造，人类检测准确率相比传统伪造下降超过30%；而现有最先进检测模型的表现也类似显著下降。

Conclusion: 当前深度伪造检测方法对局部细微篡改存在显著漏洞。本文发布的数据集和评测结果为今后开发更强健的局部视频伪造检测方法提供了基础资源和研究方向。

Abstract: We introduce FakeParts, a new class of deepfakes characterized by subtle,
localized manipulations to specific spatial regions or temporal segments of
otherwise authentic videos. Unlike fully synthetic content, these partial
manipulations, ranging from altered facial expressions to object substitutions
and background modifications, blend seamlessly with real elements, making them
particularly deceptive and difficult to detect. To address the critical gap in
detection capabilities, we present FakePartsBench, the first large-scale
benchmark dataset specifically designed to capture the full spectrum of partial
deepfakes. Comprising over 25K videos with pixel-level and frame-level
manipulation annotations, our dataset enables comprehensive evaluation of
detection methods. Our user studies demonstrate that FakeParts reduces human
detection accuracy by over 30% compared to traditional deepfakes, with similar
performance degradation observed in state-of-the-art detection models. This
work identifies an urgent vulnerability in current deepfake detection
approaches and provides the necessary resources to develop more robust methods
for partial video manipulations.

</details>


### [95] [Multi-View 3D Point Tracking](https://arxiv.org/abs/2508.21060)
*Frano Rajič,Haofei Xu,Marko Mihajlovic,Siyuan Li,Irem Demir,Emircan Gündoğdu,Lei Ke,Sergey Prokudin,Marc Pollefeys,Siyu Tang*

Main category: cs.CV

TL;DR: 本文提出了首个数据驱动的多视角三维点追踪方法，通过多个摄像机对动态场景中任意点进行精确追踪，摆脱了单目追踪深度歧义及现有多摄像机方法繁琐的限制。新方法只需4台摄像头即可实现准确、鲁棒的在线跟踪，且泛化性强。


<details>
  <summary>Details</summary>
Motivation: 现有的单目三维追踪方法在深度歧义和遮挡情况下表现有限，而依赖多摄像机的传统方法通常需要超过20台摄像机及繁琐的优化过程，难以应用于实际场景。研究者希望开发一种高效、准确且实用的多视角三维点追踪方法。

Method: 所提出的方法基于已知摄像机姿态及传感器或估计的多视角深度信息，将多视角特征融合为统一的点云，并通过k最近邻相关与基于transformer的更新机制，可靠地估算长距离三维对应关系，即便在遮挡情况下也能准确追踪点的位置。模型在5K条合成多视角序列上进行训练。

Result: 在两个真实场景数据集（Panoptic Studio和DexYCB）上评估，该方法分别获得3.1 cm和2.0 cm的中值轨迹误差，展现出较高的精度。同时，其能适应1至8台摄像机的不同配置、24至150帧的视频长度，展现出良好的泛化能力。

Conclusion: 本文方法大大简化了多视角三维追踪任务，对实际应用更为友好，精度高且泛化能力强，并通过开源追踪器和数据集，推动了多视角三维追踪领域研究发展。

Abstract: We introduce the first data-driven multi-view 3D point tracker, designed to
track arbitrary points in dynamic scenes using multiple camera views. Unlike
existing monocular trackers, which struggle with depth ambiguities and
occlusion, or prior multi-camera methods that require over 20 cameras and
tedious per-sequence optimization, our feed-forward model directly predicts 3D
correspondences using a practical number of cameras (e.g., four), enabling
robust and accurate online tracking. Given known camera poses and either
sensor-based or estimated multi-view depth, our tracker fuses multi-view
features into a unified point cloud and applies k-nearest-neighbors correlation
alongside a transformer-based update to reliably estimate long-range 3D
correspondences, even under occlusion. We train on 5K synthetic multi-view
Kubric sequences and evaluate on two real-world benchmarks: Panoptic Studio and
DexYCB, achieving median trajectory errors of 3.1 cm and 2.0 cm, respectively.
Our method generalizes well to diverse camera setups of 1-8 views with varying
vantage points and video lengths of 24-150 frames. By releasing our tracker
alongside training and evaluation datasets, we aim to set a new standard for
multi-view 3D tracking research and provide a practical tool for real-world
applications. Project page available at https://ethz-vlg.github.io/mvtracker.

</details>


### [96] [OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning](https://arxiv.org/abs/2508.21066)
*Yuan Gong,Xionghui Wang,Jie Wu,Shiyin Wang,Yitong Wang,Xinglong Wu*

Main category: cs.CV

TL;DR: 提出了OneReward，一个统一的强化学习框架，通过单一奖励模型提升多任务下的生成能力，无需任务特定的监督微调，实验优于现有商用及开源模型。


<details>
  <summary>Details</summary>
Motivation: 现有多任务生成方法大多依赖于任务特定的监督微调，限制了泛化能力和训练效率，且针对不同评价标准难以统一优化。为此，作者希望通过一种统一机制，提升多种任务下的生成表现。

Method: 使用单一视觉-语言生成奖励模型（VLM）作为奖励函数，判别不同任务和评价标准下的优胜者或劣者。将其应用于Mask引导的图像生成，包括图像填充、扩展、物体去除和文本渲染等子任务。通过强化学习对多任务直接在预训练基座模型上训练，无需单独任务微调。

Result: 实验显示，该统一编辑模型在多个评价指标上稳定优于主流商用软件（如Ideogram、Adobe Photoshop和FLUX Fill [Pro]）和开源竞品。

Conclusion: OneReward证明了单一奖励模型可提升多任务生成模型的泛化性与效率，并取得了优越的性能，推动了多任务生成领域的发展。

Abstract: In this paper, we introduce OneReward, a unified reinforcement learning
framework that enhances the model's generative capabilities across multiple
tasks under different evaluation criteria using only \textit{One Reward} model.
By employing a single vision-language model (VLM) as the generative reward
model, which can distinguish the winner and loser for a given task and a given
evaluation criterion, it can be effectively applied to multi-task generation
models, particularly in contexts with varied data and diverse task objectives.
We utilize OneReward for mask-guided image generation, which can be further
divided into several sub-tasks such as image fill, image extend, object
removal, and text rendering, involving a binary mask as the edit area. Although
these domain-specific tasks share same conditioning paradigm, they differ
significantly in underlying data distributions and evaluation metrics. Existing
methods often rely on task-specific supervised fine-tuning (SFT), which limits
generalization and training efficiency. Building on OneReward, we develop
Seedream 3.0 Fill, a mask-guided generation model trained via multi-task
reinforcement learning directly on a pre-trained base model, eliminating the
need for task-specific SFT. Experimental results demonstrate that our unified
edit model consistently outperforms both commercial and open-source
competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across
multiple evaluation dimensions. Code and model are available at:
https://one-reward.github.io

</details>


### [97] [Dress&Dance: Dress up and Dance as You Like It - Technical Preview](https://arxiv.org/abs/2508.21070)
*Jun-Kun Chen,Aayush Bansal,Minh Phuoc Vo,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: Dress&Dance提出了一种新的视频扩散框架，可以生成高质量的虚拟试衣视频，用户只需一张照片即可体验多种服装的动态试穿。核心方法是通过CondNet融合多模态输入，提升衣物贴合与动作真实性。实验显示该方法优于现有开源和商业系统。


<details>
  <summary>Details</summary>
Motivation: 当前虚拟试衣技术在服装贴合度和动作真实性上存在不足，且试衣多样性和试穿体验有限。用户通常需要多张照片或复杂输入，难以便捷体验高质量的虚拟试穿。该研究旨在解决如何以简单方式（单张照片）实现高质量、多服装类型、动态虚拟试穿的问题。

Method: 提出Dress&Dance视频扩散框架，核心是一个名为CondNet的条件网络，通过注意力机制融合文本、图片和视频等多模态信息，增强服装与人物的贴合效果及动作的真实感。训练采用渐进式多阶段策略，融合有限的视频数据和大量图像数据，提升模型泛化及生成质量。支持多种服装、支持单次多件服装试穿。

Result: Dress&Dance能以24FPS、1152x720分辨率生成5秒高质量试衣视频。相比现有开源和商业解决方案，在衣物逼真度和动作一致性等指标上表现更优。该体系支持范围广泛的服装类型和灵活试穿。

Conclusion: Dress&Dance为虚拟试衣体验带来了显著提升，无需繁杂的输入即实现高质量的动态试衣，具备商业和用户层面很大应用潜力，有望推动虚拟试衣技术的进一步发展和普及。

Abstract: We present Dress&Dance, a video diffusion framework that generates high
quality 5-second-long 24 FPS virtual try-on videos at 1152x720 resolution of a
user wearing desired garments while moving in accordance with a given reference
video. Our approach requires a single user image and supports a range of tops,
bottoms, and one-piece garments, as well as simultaneous tops and bottoms
try-on in a single pass. Key to our framework is CondNet, a novel conditioning
network that leverages attention to unify multi-modal inputs (text, images, and
videos), thereby enhancing garment registration and motion fidelity. CondNet is
trained on heterogeneous training data, combining limited video data and a
larger, more readily available image dataset, in a multistage progressive
manner. Dress&Dance outperforms existing open source and commercial solutions
and enables a high quality and flexible try-on experience.

</details>


### [98] [First-Place Solution to NeurIPS 2024 Invisible Watermark Removal Challenge](https://arxiv.org/abs/2508.21072)
*Fahad Shamshad,Tameem Bakr,Yahia Shaaban,Noor Hussein,Karthik Nandakumar,Nils Lukas*

Main category: cs.CV

TL;DR: 论文介绍了其在NeurIPS 2024“擦除隐形水印”挑战赛中获胜的解决方案，提出了针对不同攻击者知识程度的水印移除方法，并在实验上实现了几乎完美的水印去除效果，同时对图像质量影响极小。


<details>
  <summary>Details</summary>
Motivation: 虽然内容水印广泛用于数字媒体的认证和版权保护，但目前尚不清楚主流水印技术在对抗性攻击下的鲁棒性。因此，作者希望通过设计并检验新的攻击方法，评估和推动更鲁棒的图像水印方案发展。

Method: 1. 米色盒子（beige-box）场景：提出基于自适应VAE的规避攻击，测试阶段优化，并利用CIELAB空间的颜色对比度恢复以保证图像质量。
2. 黑盒子（black-box）场景：首先按图像空间或频域伪影聚类，然后分别对每类使用带有噪声控制和ChatGPT生成语义先验的图像-图像扩散模型，对攻击参数进行最优化。

Result: 实证结果表明，该方法对水印移除的成功率极高（达到95.7%），同时对图像剩余质量影响可以忽略不计。

Conclusion: 提出的方法能够极有效地去除现有水印，说明主流水印方法在高水平对抗性攻击下并不安全，因此呼吁研究更强健、更安全的图像水印方法。

Abstract: Content watermarking is an important tool for the authentication and
copyright protection of digital media. However, it is unclear whether existing
watermarks are robust against adversarial attacks. We present the winning
solution to the NeurIPS 2024 Erasing the Invisible challenge, which
stress-tests watermark robustness under varying degrees of adversary knowledge.
The challenge consisted of two tracks: a black-box and beige-box track,
depending on whether the adversary knows which watermarking method was used by
the provider. For the beige-box track, we leverage an adaptive VAE-based
evasion attack, with a test-time optimization and color-contrast restoration in
CIELAB space to preserve the image's quality. For the black-box track, we first
cluster images based on their artifacts in the spatial or frequency-domain.
Then, we apply image-to-image diffusion models with controlled noise injection
and semantic priors from ChatGPT-generated captions to each cluster with
optimized parameter settings. Empirical evaluations demonstrate that our method
successfully achieves near-perfect watermark removal (95.7%) with negligible
impact on the residual image's quality. We hope that our attacks inspire the
development of more robust image watermarking methods.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [99] [Social Bias in Multilingual Language Models: A Survey](https://arxiv.org/abs/2508.20201)
*Lance Calvin Lim Gamboa,Yue Feng,Mark Lee*

Main category: cs.CL

TL;DR: 本综述系统分析了关于多语言和非英语语境下模型社会偏见的最新研究，探讨了评测与缓解方法在多语言场景的适应问题，并指出该领域存在的不足与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管多语言预训练模型在处理非英语文本时同样表现出社会偏见，但相关研究和缓解措施主要集中于英语场景，缺乏对多语言和文化多样性的系统关注。

Method: 本文采用文献综述方法，系统梳理近年来有关多语言文本中偏见评估和缓解的方法。评估研究内容包括语言多样性、文化敏感度、评测指标选择与缓解技术等，并总结其主流方法和存在的问题。

Result: 综述发现，当前多语言偏见研究依然存在对特定语言偏好、跨语言缓解实验匮乏等问题，适应不同语言和文化的偏见评测基准面临诸多挑战。但也总结了一些主流实践与应对策略。

Conclusion: 本综述强调未来应加强方法学包容性、文化适应性和与前沿NLP进展的结合，为多语言偏见研究指明了更具普适性与前瞻性的研究方向。

Abstract: Pretrained multilingual models exhibit the same social bias as models
processing English texts. This systematic review analyzes emerging research
that extends bias evaluation and mitigation approaches into multilingual and
non-English contexts. We examine these studies with respect to linguistic
diversity, cultural awareness, and their choice of evaluation metrics and
mitigation techniques. Our survey illuminates gaps in the field's dominant
methodological design choices (e.g., preference for certain languages, scarcity
of multilingual mitigation experiments) while cataloging common issues
encountered and solutions implemented in adapting bias benchmarks across
languages and cultures. Drawing from the implications of our findings, we chart
directions for future research that can reinforce the multilingual bias
literature's inclusivity, cross-cultural appropriateness, and alignment with
state-of-the-art NLP advancements.

</details>


### [100] [Prompting Strategies for Language Model-Based Item Generation in K-12 Education: Bridging the Gap Between Small and Large Language Models](https://arxiv.org/abs/2508.20217)
*Mohammad Amini,Babak Ahmadi,Xiaomeng Xiong,Yilin Zhang,Christopher Qiao*

Main category: cs.CL

TL;DR: 本研究利用语言模型（如Gemma和GPT-3.5）自动生成形态学评估用的选择题，并通过结构化提示与微调技术提升生成质量，实现高效低成本的试题开发。


<details>
  <summary>Details</summary>
Motivation: 手工开发语言评估题目成本高且容易产生不一致，研究自动生成方法以提升效率和质量，对K-12教育具有实际应用价值。

Method: 采用两步法：一是对比微调后的中等规模模型Gemma与未调优的大型模型GPT-3.5的效果；二是评估七种结构化提示策略（如零样本、少样本、思维链、角色扮演、顺序法及其组合），并用自动化指标和专家评分对生成试题的五个维度进行评估。同时引入经过训练的GPT-4.1大模型模拟大规模人工评分。

Result: 结构化提示（特别是思维链与顺序法结合）显著提升了Gemma模型的生成质量，使用结构化提示和微调后的Gemma能生成比GPT-3.5零样本模式更贴合命题意图和教学要求的题目，提示设计对中等规模模型表现影响显著。

Conclusion: 结构化提示和高效微调能在数据有限的情况下有效提升中等规模语言模型的自动试题生成表现。结合自动评测、专家判断与大模型模拟评分，为K-12语言评估题目开发提供了实用且可扩展的工作流。

Abstract: This study explores automatic generation (AIG) using language models to
create multiple choice questions (MCQs) for morphological assessment, aiming to
reduce the cost and inconsistency of manual test development. The study used a
two-fold approach. First, we compared a fine-tuned medium model (Gemma, 2B)
with a larger untuned one (GPT-3.5, 175B). Second, we evaluated seven
structured prompting strategies, including zero-shot, few-shot,
chain-of-thought, role-based, sequential, and combinations. Generated items
were assessed using automated metrics and expert scoring across five
dimensions. We also used GPT-4.1, trained on expert-rated samples, to simulate
human scoring at scale. Results show that structured prompting, especially
strategies combining chain-of-thought and sequential design, significantly
improved Gemma's outputs. Gemma generally produced more construct-aligned and
instructionally appropriate items than GPT-3.5's zero-shot responses, with
prompt design playing a key role in mid-size model performance. This study
demonstrates that structured prompting and efficient fine-tuning can enhance
midsized models for AIG under limited data conditions. We highlight the value
of combining automated metrics, expert judgment, and large-model simulation to
ensure alignment with assessment goals. The proposed workflow offers a
practical and scalable way to develop and validate language assessment items
for K-12.

</details>


### [101] [Integrating SystemC TLM into FMI 3.0 Co-Simulations with an Open-Source Approach](https://arxiv.org/abs/2508.20223)
*Andrei Mihai Albu,Giovanni Pollo,Alessio Burrello,Daniele Jahier Pagliari,Cristian Tesconi,Alessandra Neri,Dario Soldi,Fabio Autieri,Sara Vinco*

Main category: cs.CL

TL;DR: 本文提出了一种完全开源的方法，将SystemC TLM模型集成到基于FMI的协同仿真流程中，解决了跨域模拟的集成难题，并通过案例验证了其可行性与有效性。


<details>
  <summary>Details</summary>
Motivation: 随着网络物理系统特别是汽车领域的复杂性增加，跨领域仿真与高效建模需求提升。然而，SystemC TLM虽然支持软硬件协同设计，但与其他领域模型的互操作性有限，导致集成存在挑战。

Method: 作者提出将SystemC TLM组件封装为FMI 3.0标准的协同仿真FMU，开发了轻量级开源工具链，并专门解决了时间同步与数据交换等技术难题。

Result: 通过具有代表性的案例，验证了所提出集成方法的可行性和有效性，展示了在异构仿真环境中实现无缝标准化集成的能力。

Conclusion: 提出的开源方法和工具链能够显著提升SystemC TLM与其他工程域模型的集成效率和互操作性，有助于提升复杂网络物理系统仿真的整体能力。

Abstract: The growing complexity of cyber-physical systems, particularly in automotive
applications, has increased the demand for efficient modeling and cross-domain
co-simulation techniques. While SystemC Transaction-Level Modeling (TLM)
enables effective hardware/software co-design, its limited interoperability
with models from other engineering domains poses integration challenges. This
paper presents a fully open-source methodology for integrating SystemC TLM
models into Functional Mock-up Interface (FMI)-based co-simulation workflows.
By encapsulating SystemC TLM components as FMI 3.0 Co Simulation Functional
Mock-up Units (FMUs), the proposed approach facilitates seamless, standardized
integration across heterogeneous simulation environments. We introduce a
lightweight open-source toolchain, address key technical challenges such as
time synchronization and data exchange, and demonstrate the feasibility and
effectiveness of the integration through representative case studies.

</details>


### [102] [Can Compact Language Models Search Like Agents? Distillation-Guided Policy Optimization for Preserving Agentic RAG Capabilities](https://arxiv.org/abs/2508.20324)
*Rikuto Kotoge,Mai Nishimura,Jiaxin Ma*

Main category: cs.CL

TL;DR: 论文提出了一种名为Distillation-Guided Policy Optimization（DGPO）的新方法，通过结合知识蒸馏和强化学习优化，使参数较少的小型语言模型（如0.5B参数）也能展现复杂的agentic检索-生成（RAG）行为，且在部分案例中甚至超越了更大型的教师模型。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型因推理能力有限，在强化学习驱动的agentic搜索和计划任务中表现不佳，训练也容易出现奖励稀疏和不稳定问题。论文希望突破小模型在RAG任务上的瓶颈，提升其实用价值，特别是在计算受限场景。

Method: 提出DGPO方法：首先通过教师模型的演示对小模型进行冷启动初始化（distillation），随后在策略优化阶段持续引入教师模型的指导信号，结合强化学习来训练学生模型。此外，作者还提出ARC（Agentic RAG Capabilities）这一新评测指标，从推理、搜索协调和响应合成等细粒度方面系统性评估模型能力。

Result: 实验结果显示，DGPO极大提升了小模型的agentic搜索行为表现，并在部分测试中小模型甚至超越了自身教师模型。该方法显著增强了小模型在RAG等高阶任务的能力。

Conclusion: DGPO方法使得在高效计算资源环境下，小型语言模型也可实现复杂的agentic RAG操作，为模型压缩和实际应用提供有力支持。

Abstract: Reinforcement Learning has emerged as a post-training approach to elicit
agentic RAG behaviors such as search and planning from language models.
However, compact language models (e.g., 0.5B parameters) struggle due to poor
reasoning ability, resulting in sparse rewards and unstable training. To
overcome these difficulties, we propose Distillation-Guided Policy Optimization
(DGPO), which addresses the challenges through cold-start initialization from
teacher demonstrations and continuous teacher guidance during policy
optimization. To systematically evaluate our approach, we introduce Agentic RAG
Capabilities (ARC), a fine-grained metric analyzing reasoning, search
coordination, and response synthesis. Comprehensive experiments demonstrate
that DGPO enables compact models to achieve sophisticated agentic search
behaviors, even outperforming the larger teacher model in some cases. DGPO
makes agentic RAG feasible in computing resource-constrained environments.

</details>


### [103] [GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs](https://arxiv.org/abs/2508.20325)
*Haibo Jin,Ruoxi Chen,Peiyan Zhang,Andy Zhou,Yang Zhang,Haohan Wang*

Main category: cs.CL

TL;DR: 本文提出了一种自动化测试方法（GUARD），用于将政府AI伦理规范转化为可操作的LLM合规性测试问题，并在多个主流大模型上验证其实用性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）的广泛应用，其生成有害内容的风险引发了社会和政策关注。尽管有政策指南，但它们多为原则性要求，难以直接用于实际测试，因此需要一种可操作的测试框架来检验模型对规范的遵守情况。

Method: 作者提出了GUARD方法，将高层次的政府伦理与安全指南通过自动化方式转化为具体的违规测试问题，测试大模型对政策合规性的响应。对于未直接违规的情况，提出GUARD-JD“越狱”诊断方法，通过诱导场景测试模型安全防护能力。最终生成合规性报告。

Result: 在Vicuna-13B、LongChat-7B、Llama2-7B、Llama-3-8B、GPT-3.5、GPT-4、GPT-4o和Claude-3.7等七种主流大语言模型上，以及三个政府政策指导下，实证验证了GUARD及GUARD-JD的有效性，且该“越狱”诊断能力可迁移至视觉语言模型。

Conclusion: GUARD可以自动化地将政策指南转为具体的合规测试，有效检测大模型对伦理与法规的遵守情况，包括识别潜在安全防护薄弱点，为可信赖的LLM应用提供支撑。

Abstract: As Large Language Models become increasingly integral to various domains,
their potential to generate harmful responses has prompted significant societal
and regulatory concerns. In response, governments have issued ethics guidelines
to promote the development of trustworthy AI. However, these guidelines are
typically high-level demands for developers and testers, leaving a gap in
translating them into actionable testing questions to verify LLM compliance.
  To address this challenge, we introduce GUARD (\textbf{G}uideline
\textbf{U}pholding Test through \textbf{A}daptive \textbf{R}ole-play and
Jailbreak \textbf{D}iagnostics), a testing method designed to operationalize
guidelines into specific guideline-violating questions that assess LLM
adherence. To implement this, GUARD uses automated generation of
guideline-violating questions based on government-issued guidelines, thereby
testing whether responses comply with these guidelines. When responses directly
violate guidelines, GUARD reports inconsistencies. Furthermore, for responses
that do not directly violate guidelines, GUARD integrates the concept of
``jailbreaks'' to diagnostics, named GUARD-JD, which creates scenarios that
provoke unethical or guideline-violating responses, effectively identifying
potential scenarios that could bypass built-in safety mechanisms. Our method
finally culminates in a compliance report, delineating the extent of adherence
and highlighting any violations.
  We have empirically validated the effectiveness of GUARD on seven LLMs,
including Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4,
GPT-4o, and Claude-3.7, by testing compliance under three government-issued
guidelines and conducting jailbreak diagnostics. Additionally, GUARD-JD can
transfer jailbreak diagnostics to vision-language models, demonstrating its
usage in promoting reliable LLM-based applications.

</details>


### [104] [Joint Enhancement of Relational Reasoning for Long-Context LLMs](https://arxiv.org/abs/2508.20351)
*Zhirui Chen,Wei Shen,Jiashui Huang,Ling Shao*

Main category: cs.CL

TL;DR: JERR是一个为大语言模型设计的新框架，通过图结构推理显著提升长文本理解和推理能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理长文本和复杂推理任务时受限于记忆容量与推理能力，同时面临可解释性差和幻觉问题。该工作旨在解决这些挑战。

Method: JERR框架整合了摘要提取、图构建与关系推理三大模块。首先将长文本分块并提取摘要以聚合信息；然后通过DAG图解决冗余并保证逻辑一致性；最后结合蒙特卡洛树搜索辅助模型完成复杂推理。

Result: 实验表明，JERR在ROUGE、F1等指标和LLM-Rater评估中全面超越了所有对比基线，取得最佳表现。

Conclusion: JERR显著提升了大语言模型在长文本与复杂推理任务中的可靠性和可解释性，提供了新的有效解决方案。

Abstract: Despite significant progress, large language models (LLMs) still struggle
with long contexts due to memory limitations and their inability to tackle
complex and long-context tasks. Additionally, LLMs often suffer from a lack of
transparency and are prone to producing hallucinations. To address these
challenges, we propose \textbf{JERR}, a novel framework designed to enhance
long-context comprehension via graph-based reasoning in LLMs. JERR integrates
three key components: synopsis extraction, graph construction, and relational
reasoning. First, synopsis is extracted by chunking text strategically,
allowing the model to summarize and understand information more efficiently.
Second, we build a directed acyclic graph (DAG) to resolve redundancy, ensuring
logical consistency and clarity. Finally, we incorporate Monte Carlo Tree
Search (MCTS) to help the model navigate complex reasoning paths, ensuring more
accurate and interpretable outputs. This framework provides a novel solution
that enables LLMs to handle extended contexts and complex reasoning tasks with
improved reliability and transparency. Experimental results show that JERR
consistently outperforms all baselines on the ROUGE and F1 metrics, achieving
the highest scores on the LLM-Rater evaluation.

</details>


### [105] [Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems](https://arxiv.org/abs/2508.20373)
*Yuyao Wang,Bowen Liu,Jianheng Tang,Nuo Chen,Yuhan Li,Qifan Zhang,Jia Li*

Main category: cs.CL

TL;DR: 本文提出使用NP-hard图问题作为合成训练语料，提升大语言模型在长链推理（Long CoT）任务中的表现，并开发出Graph-R1-7B模型，验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在复杂推理任务上取得进展主要依赖于人类精编的高质量后训练数据集（如数学与代码），但这些数据集获取成本高且难以扩展。因此，探索可扩展的替代数据来源具有重要意义。

Method: 作者引入NP-hard图问题作为新的合成训练语料，设计了两阶段的后训练框架：1）利用拒绝采样生成的NP-hard图实例进行Supervised Fine-Tuning（SFT），提升模型推理深度；2）引入细粒度奖励机制的强化学习（RL），进一步提升推理效率。

Result: 提出的Graph-R1-7B模型在数学、编程、STEM及逻辑等领域展现出良好的泛化能力，并在NP-hard图问题上超过同类模型QwQ-32B，在准确率和推理效率两方面表现优异。

Conclusion: NP-hard图问题可作为一种高效且可扩展的训练资源用于后训练，有助于提升大语言模型的长链推理能力，为相关模型的训练开辟了新方向。

Abstract: Reasoning Large Language Models (RLLMs) have recently achieved remarkable
progress on complex reasoning tasks, largely enabled by their long
chain-of-thought (Long CoT) capabilities. However, developing these Long CoT
behaviors relies heavily on post-training with high-quality datasets, which are
typically costly and human-curated (e.g., mathematics and code), leaving
scalable alternatives unexplored. In this work, we introduce NP-hard (NPH)
graph problems as a novel synthetic training corpus, as they inherently require
deep reasoning, extensive exploration, and reflective strategies, which are
core characteristics of Long CoT reasoning. Building on this insight, we
develop a two-stage post-training framework: (i) Long CoT Supervised
Fine-Tuning (SFT) on rejection-sampled NPH graph instances, which substantially
enhances reasoning depth, and (ii) Reinforcement Learning (RL) with a
fine-grained reward design, which sharpens reasoning efficiency. Our flagship
model, Graph-R1-7B, demonstrates strong generalization across mathematics,
coding, STEM, and logic, and surpasses QwQ-32B on NPH graph problems in both
accuracy and reasoning efficiency. These results position NPH graph problems as
an effective and scalable resource for advancing Long CoT reasoning in LLMs,
opening a new frontier for LLM post-training. Our implementation is available
at https://github.com/Graph-Reasoner/Graph-R1, with models and datasets hosted
in our Hugging Face collection HKUST-DSAIL/Graph-R1.

</details>


### [106] [CAPE: Context-Aware Personality Evaluation Framework for Large Language Models](https://arxiv.org/abs/2508.20385)
*Jivnesh Sandhan,Fei Cheng,Tushar Sandhan,Yugo Murawaki*

Main category: cs.CL

TL;DR: 本文提出了一种新的针对大语言模型(LLM) 的性格评估方法，突出了上下文对模型行为一致性和性格表现的显著影响。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型评测方法大多采用无上下文的“孤岛式”测试，与实际应用中对话历史影响模型行为的情况不符，因此有必要引入包含上下文信息的更真实评测策略。

Method: 作者提出了Context-Aware Personality Evaluation (CAPE) 框架，首次将对话历史（上下文）纳入LLM性格评估，并设计了一系列新颖的度量标准来分析LLM在不同上下文下的响应一致性。具体测试了7个主流LLM，并特别考察了问题顺序和上下文对模型表现的影响。

Result: 实验发现：1) 上下文能够提升大模型响应的一致性，但也可能带来性格偏移；2) GPT系列模型对问题顺序鲁棒且性格表现主要由模型本身及历史交互共同决定；而Gemini-1.5-Flash和Llama-8B高度依赖历史交互且对问题顺序更敏感；3) 模拟角色扮演场景时，基于上下文的性格偏移可进一步提升一致性并更贴近人类判断。

Conclusion: 引入上下文信息更贴近实际应用，有助于更全面评估LLM的人格特质和行为一致性，CAPE框架为未来大模型性格与行为安全性研究提供了重要工具。

Abstract: Psychometric tests, traditionally used to assess humans, are now being
applied to Large Language Models (LLMs) to evaluate their behavioral traits.
However, existing studies follow a context-free approach, answering each
question in isolation to avoid contextual influence. We term this the Disney
World test, an artificial setting that ignores real-world applications, where
conversational history shapes responses. To bridge this gap, we propose the
first Context-Aware Personality Evaluation (CAPE) framework for LLMs,
incorporating prior conversational interactions. To thoroughly analyze the
influence of context, we introduce novel metrics to quantify the consistency of
LLM responses, a fundamental trait in human behavior.
  Our exhaustive experiments on 7 LLMs reveal that conversational history
enhances response consistency via in-context learning but also induces
personality shifts, with GPT-3.5-Turbo and GPT-4-Turbo exhibiting extreme
deviations. While GPT models are robust to question ordering, Gemini-1.5-Flash
and Llama-8B display significant sensitivity. Moreover, GPT models response
stem from their intrinsic personality traits as well as prior interactions,
whereas Gemini-1.5-Flash and Llama--8B heavily depend on prior interactions.
Finally, applying our framework to Role Playing Agents (RPAs) shows
context-dependent personality shifts improve response consistency and better
align with human judgments. Our code and datasets are publicly available at:
https://github.com/jivnesh/CAPE

</details>


### [107] [Measuring Reasoning Utility in LLMs via Conditional Entropy Reduction](https://arxiv.org/abs/2508.20395)
*Xu Guo*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在生成中间推理步骤以提升准确度时，这些推理步骤实际对最终答案正确性的贡献。通过实验证明，推理链条中条件熵的下降与正确答案高度相关，而不下降甚至上升时往往导致错误答案。


<details>
  <summary>Details</summary>
Motivation: 现有 LLMs 通过逐步推理提升答案质量，但很少关注每步推理对于最终答案正确性的具体作用。生成更多推理内容并不总是提升信心或准确率，因此需要更好地理解推理过程和有效步骤，以提高效率和准确性。

Method: 作者以 MATH 数据集为例，使用 Qwen2.5-32B 和 GPT-4o 生成推理链，然后由另一模型（Qwen3-8B）评估这些推理链的效用。方法上，逐步扩展上下文，计算每一步中答案跨度 Y 的条件熵（基于词表的期望负对数似然），观察条件熵变化与答案正确性的关系。

Result: 实验发现，随着推理步骤的推进，如果条件熵逐步下降，通常能得到正确答案；反之条件熵平稳或上升时，更可能输出错误答案。同时，错误推理链往往比正确推理链更长，说明冗长推理未必带来更好结果。

Conclusion: 研究结果显示，通过监控条件熵的变化可以判断推理的有效性，为优化 LLMs 推理流程、早期终止/修剪无效推理提供理论依据。这为未来设计更高效推理流程、避免无效冗余提供了基础和参考。

Abstract: Recent advancements in large language models (LLMs) often rely on generating
intermediate reasoning steps to enhance accuracy. However, little work has
examined how reasoning utility contributes to the final answer's correctness.
Due to the stochastic nature of autoregressive generation, generating more
context does not guarantee increased confidence in the answer. If we could
predict, during generation, whether a reasoning step will be useful, we could
stop early or prune ineffective steps, avoiding distractions in the final
decision.
  We present an oracle study on MATH dataset, using Qwen2.5-32B and GPT-4o to
generate reasoning chains, and then employing a separate model (Qwen3-8B) to
quantify the utility of these chains for final accuracy. Specifically, we
measure the model's uncertainty on the answer span Y at each reasoning step
using conditional entropy (expected negative log-likelihood over the
vocabulary) with context expanding step by step. Our results show a clear
pattern: conditional entropy that decreases over steps is strongly associated
with correct answers, whereas flat or increasing entropy often results in wrong
answers. We also corroborate that incorrect reasoning paths tend to be longer
than correct ones, suggesting that longer reasoning does not necessarily yield
better outcomes. These findings serve as a foundation to inspire future work on
designing efficient reasoning pipelines that detect and avoid unproductive
reasoning early.

</details>


### [108] [UI-Bench: A Benchmark for Evaluating Design Capabilities of AI Text-to-App Tools](https://arxiv.org/abs/2508.20410)
*Sam Jung,Agustin Garcinuno,Spencer Mateega*

Main category: cs.CL

TL;DR: 本文提出了UI-Bench，这是首个大规模用于评测AI文本生成App/Web工具视觉质量的基准，包含多工具、多题目和大量专家打分，并公开了评测框架、数据和排行榜。


<details>
  <summary>Details</summary>
Motivation: 随着AI文本到App（text-to-app）工具的发展，官方宣传通常声称能在几分钟内生成高质量应用或网站，但缺乏公开、统一、严格的评测基准来验证这些说法。该论文旨在建立一个系统性的标准来客观评估各种AI工具在网页视觉设计上的表现并推动该方向的研究。

Method: 研究者提出UI-Bench基准，包含10个主流AI文本到App工具，30套文本提示，人工生成300个网站。然后，邀请专家进行4000+对比打分，评测每个系统所生成网站的视觉表现。评分数据以TrueSkill模型进行排序，获得各系统的相对评分和置信区间。此外，作者公开了全部的题目集、评测框架源码和在线排行榜，计划在后续开放具体打分网站。

Result: 通过专家打分与TrueSkill模型，UI-Bench系统性地给出了各AI工具在网页视觉设计能力上的排名，数据和代码也全部开放，建立了可重复的评测标准。

Conclusion: UI-Bench为AI驱动的Web设计工具建立了公开、权威、可重复的评测标准与基准，大幅推进了相关研究的透明性和公平性，也为后续开发者和用户选择工具提供了重要参考。

Abstract: AI text-to-app tools promise high quality applications and websites in
minutes, yet no public benchmark rigorously verifies those claims. We introduce
UI-Bench, the first large-scale benchmark that evaluates visual excellence
across competing AI text-to-app tools through expert pairwise comparison.
Spanning 10 tools, 30 prompts, 300 generated sites, and \textit{4000+} expert
judgments, UI-Bench ranks systems with a TrueSkill-derived model that yields
calibrated confidence intervals. UI-Bench establishes a reproducible standard
for advancing AI-driven web design. We release (i) the complete prompt set,
(ii) an open-source evaluation framework, and (iii) a public leaderboard. The
generated sites rated by participants will be released soon. View the UI-Bench
leaderboard at https://uibench.ai/leaderboard.

</details>


### [109] [DentalBench: Benchmarking and Advancing LLMs Capability for Bilingual Dentistry Understanding](https://arxiv.org/abs/2508.20416)
*Hengchuan Zhu,Yihuan Xu,Yichen Li,Zijie Meng,Zuozhu Liu*

Main category: cs.CL

TL;DR: 本文提出了DentalBench，这是首个专为口腔医学领域设计的中英双语大模型评测基准，包含问答数据与高质量语料，用于推动大模型在口腔医学领域的进步。实验结果显示领域适应对提升模型性能非常重要。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在医学领域已有进展，但针对更细分的专业领域如口腔医学的评测资源有限，无法准确衡量模型在该领域的能力，因此需要开发特定的评测基准。

Method: 作者提出DentalBench，包括两部分：36,597道中英问答题（涵盖4类任务、16个口腔学子领域）的DentalQA，以及包含3.37亿词的DentalCorpus语料库，支持有监督微调与RAG。并分别评测了14种LLM模型，其中文开源、闭源、医学专属模型。

Result: 实验揭示了不同模型在任务类型和语言表现上存在显著差距。对Qwen-2.5-3B等进行领域适应后，模型在知识和术语密集的任务上能力显著提升。

Conclusion: 研究强调开发面向口腔医学等专业领域的评测基准的重要性，这对于部署可信、有效的医疗AI模型十分关键。

Abstract: Recent advances in large language models (LLMs) and medical LLMs (Med-LLMs)
have demonstrated strong performance on general medical benchmarks. However,
their capabilities in specialized medical fields, such as dentistry which
require deeper domain-specific knowledge, remain underexplored due to the lack
of targeted evaluation resources. In this paper, we introduce DentalBench, the
first comprehensive bilingual benchmark designed to evaluate and advance LLMs
in the dental domain. DentalBench consists of two main components: DentalQA, an
English-Chinese question-answering (QA) benchmark with 36,597 questions
spanning 4 tasks and 16 dental subfields; and DentalCorpus, a large-scale,
high-quality corpus with 337.35 million tokens curated for dental domain
adaptation, supporting both supervised fine-tuning (SFT) and
retrieval-augmented generation (RAG). We evaluate 14 LLMs, covering
proprietary, open-source, and medical-specific models, and reveal significant
performance gaps across task types and languages. Further experiments with
Qwen-2.5-3B demonstrate that domain adaptation substantially improves model
performance, particularly on knowledge-intensive and terminology-focused tasks,
and highlight the importance of domain-specific benchmarks for developing
trustworthy and effective LLMs tailored to healthcare applications.

</details>


### [110] [KG-CQR: Leveraging Structured Relation Representations in Knowledge Graphs for Contextual Query Retrieval](https://arxiv.org/abs/2508.20417)
*Chi Minh Bui,Ngoc Mai Thieu,Van Vinh Nguyen,Json J. Jung,Khac-Hoai Nam Bui*

Main category: cs.CL

TL;DR: 本文提出了KG-CQR框架，将知识图谱与大语言模型结合，用于增强检索增强生成（RAG）系统中的检索阶段，从而提升检索效果。实验证明该方法优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在复杂查询的上下文理解上有局限，主要针对语料库级别的上下文损失，而忽略了查询本身的结构化语义信息。为弥补这一不足，作者引入知识图谱来丰富查询上下文表达，提高检索质量。

Method: 提出了KG-CQR框架，包括子图提取、补全及上下文生成三大模块。该框架以模型无关方式工作，将复杂查询与知识图谱结合，自动提取并完善相关KG子图，从而生成丰富的查询上下文，无需对LLMs进行额外训练。

Result: 在RAGBench和MultiHop-RAG等数据集上的实验表明，KG-CQR相较于强基线模型，mAP提升4-6%，Recall@25提升2-3%。在多跳问答等任务中也持续优于现有方法，检索效果显著提升。

Conclusion: KG-CQR通过结构化丰富查询上下文，有效增强了RAG系统的检索阶段，在多种任务和多尺寸LLM上表现优越，具备良好的可扩展性和实用价值。

Abstract: The integration of knowledge graphs (KGs) with large language models (LLMs)
offers significant potential to improve the retrieval phase of
retrieval-augmented generation (RAG) systems. In this study, we propose KG-CQR,
a novel framework for Contextual Query Retrieval (CQR) that enhances the
retrieval phase by enriching the contextual representation of complex input
queries using a corpus-centric KG. Unlike existing methods that primarily
address corpus-level context loss, KG-CQR focuses on query enrichment through
structured relation representations, extracting and completing relevant KG
subgraphs to generate semantically rich query contexts. Comprising subgraph
extraction, completion, and contextual generation modules, KG-CQR operates as a
model-agnostic pipeline, ensuring scalability across LLMs of varying sizes
without additional training. Experimental results on RAGBench and MultiHop-RAG
datasets demonstrate KG-CQR's superior performance, achieving a 4-6%
improvement in mAP and a 2-3% improvement in Recall@25 over strong baseline
models. Furthermore, evaluations on challenging RAG tasks such as multi-hop
question answering show that, by incorporating KG-CQR, the performance
consistently outperforms the existing baseline in terms of retrieval
effectiveness

</details>


### [111] [CAMB: A comprehensive industrial LLM benchmark on civil aviation maintenance](https://arxiv.org/abs/2508.20420)
*Feng Zhang,Chengjie Pang,Yuehan Zhang,Chenyu Luo*

Main category: cs.CL

TL;DR: 本论文提出并开发了专为民航维修领域设计的大型语言模型（LLM）评测基准，填补了当前缺乏该垂直行业专用评测工具的空白。论文开源了评测基准和代码，旨在推动相关研究发展。


<details>
  <summary>Details</summary>
Motivation: 目前主流的LLM评测多聚焦于数学和编程等领域，缺乏针对民航维修这样知识密集型、需要复杂推理能力的垂直行业评测工具，导致难以识别和针对性优化模型在实际行业中的不足。

Method: 设计并开发了面向民航维修领域的工业级评测基准，包含对知识掌握、复杂推理等多方面能力的考察。基于该基准，对现有主流的向量嵌入模型和LLM进行实验评估，并开源了所有评测工具和代码。

Result: 实验结果证明，这一新的评测基准能够有效揭示主流模型在民航维修场景下的性能，并能具体暴露其知识及推理短板。

Conclusion: 所提出的评测基准是推动民航维修领域智能化解决方案发展的关键，有助于模型针对性提升，并为学界和业界后续研究提供了标准化工具和资源。

Abstract: Civil aviation maintenance is a domain characterized by stringent industry
standards. Within this field, maintenance procedures and troubleshooting
represent critical, knowledge-intensive tasks that require sophisticated
reasoning. To address the lack of specialized evaluation tools for large
language models (LLMs) in this vertical, we propose and develop an
industrial-grade benchmark specifically designed for civil aviation
maintenance. This benchmark serves a dual purpose: It provides a standardized
tool to measure LLM capabilities within civil aviation maintenance, identifying
specific gaps in domain knowledge and complex reasoning. By pinpointing these
deficiencies, the benchmark establishes a foundation for targeted improvement
efforts (e.g., domain-specific fine-tuning, RAG optimization, or specialized
prompt engineering), ultimately facilitating progress toward more intelligent
solutions within civil aviation maintenance. Our work addresses a significant
gap in the current LLM evaluation, which primarily focuses on mathematical and
coding reasoning tasks. In addition, given that Retrieval-Augmented Generation
(RAG) systems are currently the dominant solutions in practical applications ,
we leverage this benchmark to evaluate existing well-known vector embedding
models and LLMs for civil aviation maintenance scenarios. Through experimental
exploration and analysis, we demonstrate the effectiveness of our benchmark in
assessing model performance within this domain, and we open-source this
evaluation benchmark and code to foster further research and
development:https://github.com/CamBenchmark/cambenchmark

</details>


### [112] [Searching the Title of Practical Work of the Informatics Engineering Bachelor Program with the Case Base Reasoning Method](https://arxiv.org/abs/2508.20442)
*Agung Sukrisna Jaya,Osvari Arsalan,Danny Matthew Saputra*

Main category: cs.CL

TL;DR: 本文提出了一种结合CBR、TF-IDF和余弦相似度的方法，用于检索与关键词或标题相似的实习项目标题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 实际工作中，学生或用户在寻找实习课题时，往往难以准确匹配他们的需求与已有课题标题。为此，需要一种高效的相似度检索方法来辅助匹配合适的课题。

Method: 系统结合了案例推理（CBR），对以往实习工作标题构建案例库。每个标题通过TF-IDF向量化，利用余弦相似度计算检索时输入与案例的相似度。系统支持基于标题或关键词的检索，输出匹配的课题标题及其相似度分值。

Result: 在705个实习课题标题上测试系统，随机选择五个标题，分为两步：第一步直接用现有标题检索，第二步用第一步标题的随机化版本检索。两步都找到了相同数量的标题，且随机化后的最高平均匹配分数不变，表明系统具有稳定有效的匹配能力。

Conclusion: 结合CBR、TF-IDF和余弦相似度的系统能够有效地根据标题或关键词检索相似的实习项目标题，并且对输入顺序或内容的微小变化具有稳定的匹配能力。

Abstract: Case Base Reasoning (CBR) is a case solving technique based on experience in
cases that have occurred before with the highest similarity. CBR is used to
search for practical work titles. TF-IDF is applied to process the
vectorization of each practical work title word and Cosine Similarity for the
calculation of similarity values. This system can search either in the form of
titles or keywords. The output of the system is the title of practical work and
the match value of each title. Based on the test results using 705 practical
work titles, testing was carried out with five titles and carried out in two
stages. The first stage searches with existing titles and the second stage
randomizes the title from the first stage. And the results obtained in the
second stage are the same number of titles found and the highest average match
score.

</details>


### [113] [MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers](https://arxiv.org/abs/2508.20453)
*Zhenting Wang,Qi Chang,Hemani Patel,Shashank Biju,Cheng-En Wu,Quan Liu,Aolin Ding,Alireza Rezazadeh,Ankit Shah,Yujia Bao,Eugene Siow*

Main category: cs.CL

TL;DR: 本文提出了MCP-Bench，这是一个用于评估大语言模型(LLM)在多步骤工具协作任务中的能力的新基准。它连接28个MCP服务器，提供250种跨领域工具，用于真实复杂任务，弥补了现有基准在真实工具组合和多领域任务流程中的不足。实验揭示了现有LLM在该基准下尚存诸多挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的基于API的LLM评测基准无法真实反映模型在多工具协同、参数精准控制和复杂规划推理方面的综合能力，因为它们通常仅包含浅层、单一领域、显式给出工具名的短流程任务。作者希望通过MCP-Bench更全面、真实地测试并推动LLM在实际应用场景下的多工具协作与复杂推理能力发展。

Method: MCP-Bench基于Model Context Protocol (MCP)设计，将LLMs与28个实际运行的MCP服务器连接，这些服务器合计提供250种工具，涵盖金融、旅行、科研计算、学术搜索等多个领域。每个MCP服务器包含互补型工具，组合构建真实的多步骤任务。任务要求模型根据模糊指令检索工具、规划执行路径、协调跨领域工作流程，并依据中间工具输出调整响应。评估框架多维度覆盖工具理解与使用、流程规划、任务完成度。

Result: 实验在20种先进LLM上进行，结果显示在MCP-Bench提出的多步真实任务下，当前LLM在工具检索、流程规划、跨域协调等多个关键能力上仍面临显著挑战，整体表现差强人意。

Conclusion: MCP-Bench为评估和推动LLM多工具协作与复杂推理能力提供了更真实和挑战性的标准。数据和代码已开源，可用于持续追踪模型能力进展，未来应持续提升模型在复杂、多工具现实场景下的协作与推理表现。

Abstract: We introduce MCP-Bench, a benchmark for evaluating large language models
(LLMs) on realistic, multi-step tasks that demand tool use, cross-tool
coordination, precise parameter control, and planning/reasoning for solving
tasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28
representative live MCP servers spanning 250 tools across domains such as
finance, traveling, scientific computing, and academic search. Unlike prior
API-based benchmarks, each MCP server provides a set of complementary tools
designed to work together, enabling the construction of authentic, multi-step
tasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability
to retrieve relevant tools from fuzzy instructions without explicit tool names,
plan multi-hop execution trajectories for complex objectives, ground responses
in intermediate tool outputs, and orchestrate cross-domain workflows -
capabilities not adequately evaluated by existing benchmarks that rely on
explicit tool specifications, shallow few-step workflows, and isolated domain
operations. We propose a multi-faceted evaluation framework covering tool-level
schema understanding and usage, trajectory-level planning, and task completion.
Experiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code
and data: https://github.com/Accenture/mcp-bench.

</details>


### [114] [Prediction of mortality and resource utilization in critical care: a deep learning approach using multimodal electronic health records with natural language processing techniques](https://arxiv.org/abs/2508.20460)
*Yucheng Ruan,Xiang Lan,Daniel J. Tan,Hairil Rizal Abdullah,Mengling Feng*

Main category: cs.CL

TL;DR: 本文提出了一种结合多模态电子健康记录（EHR）的深度学习框架，利用自然语言处理技术提升重症监护病房（ICU）患者死亡率和资源利用等指标的预测能力。


<details>
  <summary>Details</summary>
Motivation: 当前大多数预测ICU患者死亡率和资源利用的方法只利用了结构化EHR数据，忽略自由文本笔记中的临床关键信息，同时也未充分发掘结构化数据中的文本信息。因此，亟需一种能融合多源异构EHR（包括文本和结构化数据），提升临床任务预测能力的高效方法。

Method: 作者基于两个真实EHR数据集，构建集成深度学习与NLP的模型，综合使用医学提示（medical prompts）、自由文本和预训练的句子编码器，对死亡率、住院时间、手术时长等进行预测，并与现有主流方法对比，同时设计消融实验，验证关键模块的贡献，并考察结构化数据受损情况下模型的鲁棒性。

Result: 新模型在两个真实数据集的三项任务中均优于最佳现有方法。例如，死亡率预测BACC/AUROC提升1.6%/0.8%，住院时间预测RMSE/MAE提升0.5%/2.2%，手术时长预测RMSE/MAE提升10.9%/11.0%。在不同结构化数据破损情况下，框架始终优于对比方法并表现出较强鲁棒性。

Conclusion: 该深度学习框架能有效融合多模态EHR，对ICU患者死亡率和资源利用的预测准确性高，且对结构化数据噪声具有很强的适应性。利用Prompt Learning结合Transformer编码器分析多模态EHR在实际医疗AI场景中表现出巨大潜力。

Abstract: Background Predicting mortality and resource utilization from electronic
health records (EHRs) is challenging yet crucial for optimizing patient
outcomes and managing costs in intensive care unit (ICU). Existing approaches
predominantly focus on structured EHRs, often ignoring the valuable clinical
insights in free-text notes. Additionally, the potential of textual information
within structured data is not fully leveraged. This study aimed to introduce
and assess a deep learning framework using natural language processing
techniques that integrates multimodal EHRs to predict mortality and resource
utilization in critical care settings. Methods Utilizing two real-world EHR
datasets, we developed and evaluated our model on three clinical tasks with
leading existing methods. We also performed an ablation study on three key
components in our framework: medical prompts, free-texts, and pre-trained
sentence encoder. Furthermore, we assessed the model's robustness against the
corruption in structured EHRs. Results Our experiments on two real-world
datasets across three clinical tasks showed that our proposed model improved
performance metrics by 1.6\%/0.8\% on BACC/AUROC for mortality prediction,
0.5%/2.2% on RMSE/MAE for LOS prediction, 10.9%/11.0% on RMSE/MAE for surgical
duration estimation compared to the best existing methods. It consistently
demonstrated superior performance compared to other baselines across three
tasks at different corruption rates. Conclusions The proposed framework is an
effective and accurate deep learning approach for predicting mortality and
resource utilization in critical care. The study also highlights the success of
using prompt learning with a transformer encoder in analyzing multimodal EHRs.
Importantly, the model showed strong resilience to data corruption within
structured data, especially at high corruption levels.

</details>


### [115] [ConspirED: A Dataset for Cognitive Traits of Conspiracy Theories and Large Language Model Safety](https://arxiv.org/abs/2508.20468)
*Luke Bates,Max Glockner,Preslav Nakov,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文提出了一个新的数据集ConspirED，用于研究阴谋论内容的认知特征，并利用该数据集开发和评估识别阴谋论倾向的计算模型及大模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 阴谋论内容会削弱公众对科学和机构的信任，而且具有高度适应性，难以被驳斥。随着AI生成的虚假信息愈发精致，有必要理解阴谋论文本的修辞和认知特征，以便开发定向防护和评估AI的潜在脆弱性。

Method: 作者构建了ConspirED数据集，基于CONSPIR认知框架对多句阴谋论文本片段进行认知特征标注。利用该数据集：(i) 开发可识别阴谋特征并判断主导特征的计算模型；(ii) 测试大语言/推理模型应对阴谋论输入的鲁棒性。

Result: 实验发现，无论是现有的识别模型还是大语言/推理模型，在遇到阴谋论内容时容易被其推理模式“带跑”，生成的结果往往与输入的阴谋论推理保持一致，即使同样能成功识别和防御经过事实核查的误信息。

Conclusion: 当前的大模型和识别算法在对抗阴谋论内容时存在显著不足，有必要开发更强的识别能力和防护机制以避免AI被阴谋论影响。

Abstract: Conspiracy theories erode public trust in science and institutions while
resisting debunking by evolving and absorbing counter-evidence. As AI-generated
misinformation becomes increasingly sophisticated, understanding rhetorical
patterns in conspiratorial content is important for developing interventions
such as targeted prebunking and assessing AI vulnerabilities. We introduce
ConspirED (CONSPIR Evaluation Dataset), which captures the cognitive traits of
conspiratorial ideation in multi-sentence excerpts (80--120 words) from online
conspiracy articles, annotated using the CONSPIR cognitive framework
(Lewandowsky and Cook, 2020). ConspirED is the first dataset of conspiratorial
content annotated for general cognitive traits. Using ConspirED, we (i) develop
computational models that identify conspiratorial traits and determine dominant
traits in text excerpts, and (ii) evaluate large language/reasoning model
(LLM/LRM) robustness to conspiratorial inputs. We find that both are misaligned
by conspiratorial content, producing output that mirrors input reasoning
patterns, even when successfully deflecting comparable fact-checked
misinformation.

</details>


### [116] [Languages Still Left Behind: Toward a Better Multilingual Machine Translation Benchmark](https://arxiv.org/abs/2508.20511)
*Chihiro Taguchi,Seng Mai,Keita Kurabe,Yusuke Sakai,Georgina Agyei,Soudabeh Eslami,David Chiang*

Main category: cs.CL

TL;DR: 本文评估了多语言机器翻译基准FLORES+的有效性，发现其数据存在质量与公平性问题。


<details>
  <summary>Details</summary>
Motivation: FLORES+作为流行的多语言机器翻译评测基准，其声称覆盖200多种语言并有高达90%的翻译质量，但实际应用中对其可靠性和代表性的质疑尚未被充分检验。

Method: 作者选取Asante Twi、日语、景颇语和南阿塞拜疆语四种语言，进行了人工质量评估，并分析了基准测试中数据的领域偏向及命名实体对BLEU分数的影响。此外，将基于自然语料训练的MT模型结果与FLORES+结果对比。

Result: 人工评测发现许多翻译未达到90%质量标准，数据内容明显偏向英语文化，且命名实体复制会虚高BLEU分数。此外，在FLORES+表现不佳的模型在更相关的数据集上表现出显著提升。

Conclusion: FLORES+在多语言、公平和真实性方面存在不足。作者建议未来基准应采用领域中性、文化中立的文本，并减少命名实体依赖，以更好评估机器翻译系统的真实能力。

Abstract: Multilingual machine translation (MT) benchmarks play a central role in
evaluating the capabilities of modern MT systems. Among them, the FLORES+
benchmark is widely used, offering English-to-many translation data for over
200 languages, curated with strict quality control protocols. However, we study
data in four languages (Asante Twi, Japanese, Jinghpaw, and South Azerbaijani)
and uncover critical shortcomings in the benchmark's suitability for truly
multilingual evaluation. Human assessments reveal that many translations fall
below the claimed 90% quality standard, and the annotators report that source
sentences are often too domain-specific and culturally biased toward the
English-speaking world. We further demonstrate that simple heuristics, such as
copying named entities, can yield non-trivial BLEU scores, suggesting
vulnerabilities in the evaluation protocol. Notably, we show that MT models
trained on high-quality, naturalistic data perform poorly on FLORES+ while
achieving significant gains on our domain-relevant evaluation set. Based on
these findings, we advocate for multilingual MT benchmarks that use
domain-general and culturally neutral source texts rely less on named entities,
in order to better reflect real-world translation challenges.

</details>


### [117] [SciTopic: Enhancing Topic Discovery in Scientific Literature through Advanced LLM](https://arxiv.org/abs/2508.20514)
*Pengjiang Li,Zaitian Wang,Xinhao Zhang,Ran Zhang,Lu Jiang,Pengfei Wang,Yuanchun Zhou*

Main category: cs.CL

TL;DR: 本文提出了SciTopic方法，通过结合大语言模型（LLMs）以提升科学文献中的主题发现效果，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有主题发现方法主要通过词嵌入捕捉语义，难以全面理解科学文献的复杂语境及高维关系，因此需要更高级的方法提升主题识别能力。

Method: 作者提出SciTopic，首先用文本编码器抽取包括元数据、标题和摘要在内的文献内容，然后构建空间优化模块，结合LLM指导下的熵采样和三元组任务，突出主题相关性和语境细节，最后利用LLM指导将三元组的对比损失用以微调文本编码器，增强不同主题的区分能力。

Result: 在三个真实科学出版物数据集上进行了广泛实验，SciTopic在主题发现任务上优于当前主流方法。

Conclusion: SciTopic方法有效利用LLM提升了科学主题发现的准确性和效率，帮助研究者能更快、更深入地理解科学发展趋势。

Abstract: Topic discovery in scientific literature provides valuable insights for
researchers to identify emerging trends and explore new avenues for
investigation, facilitating easier scientific information retrieval. Many
machine learning methods, particularly deep embedding techniques, have been
applied to discover research topics. However, most existing topic discovery
methods rely on word embedding to capture the semantics and lack a
comprehensive understanding of scientific publications, struggling with
complex, high-dimensional text relationships. Inspired by the exceptional
comprehension of textual information by large language models (LLMs), we
propose an advanced topic discovery method enhanced by LLMs to improve
scientific topic identification, namely SciTopic. Specifically, we first build
a textual encoder to capture the content from scientific publications,
including metadata, title, and abstract. Next, we construct a space
optimization module that integrates entropy-based sampling and triplet tasks
guided by LLMs, enhancing the focus on thematic relevance and contextual
intricacies between ambiguous instances. Then, we propose to fine-tune the
textual encoder based on the guidance from the LLMs by optimizing the
contrastive loss of the triplets, forcing the text encoder to better
discriminate instances of different topics. Finally, extensive experiments
conducted on three real-world datasets of scientific publications demonstrate
that SciTopic outperforms the state-of-the-art (SOTA) scientific topic
discovery methods, enabling researchers to gain deeper and faster insights.

</details>


### [118] [Overview of BioASQ 2024: The twelfth BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering](https://arxiv.org/abs/2508.20532)
*Anastasios Nentidis,Georgios Katsimpras,Anastasia Krithara,Salvador Lima-López,Eulàlia Farré-Maduell,Martin Krallinger,Natalia Loukachevitch,Vera Davydova,Elena Tutubalina,Georgios Paliouras*

Main category: cs.CL

TL;DR: BioASQ 2024 是一项推动生物医学语义检索和问答领域的国际挑战赛，第十二届涵盖了两项经典任务和两项新任务，参与广泛、系统表现优异。


<details>
  <summary>Details</summary>
Motivation: 促进大规模生物医学语义标注和问答技术发展，通过定期挑战赛推动社区进步。

Method: 设置四个共享任务——两个已建立的任务（b 和 Synergy），新增两个任务：多语言心血管实体识别（MultiCardioNER）和俄英嵌套实体识别（BIONNE），吸引国际参赛队伍提交系统结果。

Result: 共有37个团队参与，提交了700余份结果，多数系统表现出与以往相似的竞争力，推动技术持续进步。

Conclusion: BioASQ 持续推进生物医学检索和问答前沿，任务体系不断扩展，社区参与活跃，技术水平逐年提高。

Abstract: This is an overview of the twelfth edition of the BioASQ challenge in the
context of the Conference and Labs of the Evaluation Forum (CLEF) 2024. BioASQ
is a series of international challenges promoting advances in large-scale
biomedical semantic indexing and question answering. This year, BioASQ
consisted of new editions of the two established tasks b and Synergy, and two
new tasks: a) MultiCardioNER on the adaptation of clinical entity detection to
the cardiology domain in a multilingual setting, and b) BIONNE on nested NER in
Russian and English. In this edition of BioASQ, 37 competing teams participated
with more than 700 distinct submissions in total for the four different shared
tasks of the challenge. Similarly to previous editions, most of the
participating systems achieved competitive performance, suggesting the
continuous advancement of the state-of-the-art in the field.

</details>


### [119] [Overview of BioASQ 2025: The Thirteenth BioASQ Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering](https://arxiv.org/abs/2508.20554)
*Anastasios Nentidis,Georgios Katsimpras,Anastasia Krithara,Martin Krallinger,Miguel Rodríguez-Ortega,Eduard Rodriguez-López,Natalia Loukachevitch,Andrey Sakhovskiy,Elena Tutubalina,Dimitris Dimitriadis,Grigorios Tsoumakas,George Giannakoulas,Alexandra Bekiaridou,Athanasios Samaras,Giorgio Maria Di Nunzio,Nicola Ferro,Stefano Marchesin,Marco Martinelli,Gianmaria Silvello,Georgios Paliouras*

Main category: cs.CL

TL;DR: BioASQ第十三届挑战赛涵盖了大型生物医学语义索引和问答领域，共包含6项任务，其中包括4项新任务，吸引了83支队伍参与，系统表现持续进步。


<details>
  <summary>Details</summary>
Motivation: 推动生物医学领域的大规模语义索引和问答技术进步，通过国际竞赛推动多语种、特殊应用场景（如肠脑轴、心脏科、俄语/英语实体链接等）下的新方法发展。

Method: 组织包含既有与新设6项任务的国际性挑战赛，包括多语种医学总结、嵌套命名实体链接、心脏科临床编码、肠脑信息抽取等。通过统一平台评测不同参赛系统。

Result: 共有83支队伍提交超过1000份系统结果，多个系统达到或超越现有最佳水平，展示出领域内技术持续进步。

Conclusion: BioASQ持续以国际共享任务形式推动生物医学文本处理技术进步，多元任务拓展了适用场景，保持领域内的研究活跃和技术前沿。

Abstract: This is an overview of the thirteenth edition of the BioASQ challenge in the
context of the Conference and Labs of the Evaluation Forum (CLEF) 2025. BioASQ
is a series of international challenges promoting advances in large-scale
biomedical semantic indexing and question answering. This year, BioASQ
consisted of new editions of the two established tasks, b and Synergy, and four
new tasks: a) Task MultiClinSum on multilingual clinical summarization. b) Task
BioNNE-L on nested named entity linking in Russian and English. c) Task
ELCardioCC on clinical coding in cardiology. d) Task GutBrainIE on gut-brain
interplay information extraction. In this edition of BioASQ, 83 competing teams
participated with more than 1000 distinct submissions in total for the six
different shared tasks of the challenge. Similar to previous editions, several
participating systems achieved competitive performance, indicating the
continuous advancement of the state-of-the-art in the field.

</details>


### [120] [Adaptive Federated Distillation for Multi-Domain Non-IID Textual Data](https://arxiv.org/abs/2508.20557)
*Jiahao Xiao,Jiangming Liu*

Main category: cs.CL

TL;DR: 本文提出了一种自适应联邦蒸馏框架(AdaFD)，能够在多领域非独立同分布（non-IID）环境下改进语言模型在联邦学习中的表现，并提供了一个覆盖多种输入语言领域的统一评测基准。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦蒸馏工作主要关注输出标签上的非IID性，但忽视了现实NLP任务中输入语言领域的多样性，该问题导致模型在真实环境中的泛化能力受限。

Method: 作者设计了一个包含多领域输入多样性的非IID场景评测基准，并提出了AdaFD框架，支持在同构和异构环境中自适应地处理多领域非IID挑战，从而增强联邦学习系统对本地客户端差异性的适应能力。

Result: 实验结果表明，AdaFD框架能够更好地适应各地客户端的多样性，相较于现有方法，显著提升了模型性能。

Conclusion: AdaFD为联邦学习领域提供了有效应对多领域非IID挑战的新方案，提升了实际应用的效果和通用性，包括一个公开可用的评测基准和代码实现。

Abstract: The widespread success of pre-trained language models has established a new
training paradigm, where a global PLM is fine-tuned using task-specific data
from local clients. The local data are highly different from each other and can
not capture the global distribution of the whole data in real world. To address
the challenges of non-IID data in real environments, privacy-preserving
federated distillation has been proposed and highly investigated. However,
previous experimental non-IID scenarios are primarily identified with the label
(output) diversity, without considering the diversity of language domains
(input) that is crucial in natural language processing. In this paper, we
introduce a comprehensive set of multi-domain non-IID scenarios and propose a
unified benchmarking framework that includes diverse data. The benchmark can be
used to evaluate the federated learning framework in a real environment. To
this end, we propose an Adaptive Federated Distillation (AdaFD) framework
designed to address multi-domain non-IID challenges in both homogeneous and
heterogeneous settings. Experimental results demonstrate that our models
capture the diversity of local clients and achieve better performance compared
to the existing works. The code for this paper is available at:
https://github.com/jiahaoxiao1228/AdaFD.

</details>


### [121] [Leveraging Generative Models for Real-Time Query-Driven Text Summarization in Large-Scale Web Search](https://arxiv.org/abs/2508.20559)
*Zeyu Xiong,Yixuan Nan,Li Gao,Hengzhu Tang,Shuaiqiang Wang,Junfeng Wang,Dawei Yin*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的生成式方法用于高效的查询驱动文本摘要（QDTS），显著提升了工业级大规模网络搜索中的摘要质量和部署效率。


<details>
  <summary>Details</summary>
Motivation: 传统的抽取式摘要方法因多阶段流程导致信息损失，以及语义理解能力有限，难以应对复杂搜索意图，已经无法满足现代工业应用需求。

Method: 作者提出利用生成式模型，通过大模型蒸馏、监督微调、偏好优化和预测解码等技术，将小参数模型（0.1B）训练为领域专用的QDTS专家，并优化部署效率。

Result: 该方法在多项业界指标上超过现有生产基线，达到了最新的技术水平。实际部署下，仅需334块NVIDIA L20显卡即可在平均55毫秒延迟下处理约5万个查询每秒。

Conclusion: 所提生成式QDTS框架兼具准确性与高效性，为大规模Web搜索场景下的实时查询驱动摘要提供了可行的新方向，有望推动相关工业应用革新。

Abstract: In the dynamic landscape of large-scale web search, Query-Driven Text
Summarization (QDTS) aims to generate concise and informative summaries from
textual documents based on a given query, which is essential for improving user
engagement and facilitating rapid decision-making. Traditional extractive
summarization models, based primarily on ranking candidate summary segments,
have been the dominant approach in industrial applications. However, these
approaches suffer from two key limitations: 1) The multi-stage pipeline often
introduces cumulative information loss and architectural bottlenecks due to its
weakest component; 2) Traditional models lack sufficient semantic understanding
of both user queries and documents, particularly when dealing with complex
search intents. In this study, we propose a novel framework to pioneer the
application of generative models to address real-time QDTS in industrial web
search. Our approach integrates large model distillation, supervised
fine-tuning, direct preference optimization, and lookahead decoding to
transform a lightweight model with only 0.1B parameters into a
domain-specialized QDTS expert. Evaluated on multiple industry-relevant
metrics, our model outperforms the production baseline and achieves a new state
of the art. Furthermore, it demonstrates excellent deployment efficiency,
requiring only 334 NVIDIA L20 GPUs to handle \textasciitilde50,000 queries per
second under 55~ms average latency per query.

</details>


### [122] [KCS: Diversify Multi-hop Question Generation with Knowledge Composition Sampling](https://arxiv.org/abs/2508.20567)
*Yangfan Wang,Jie Liu,Chen Tang,Lian Yan,Jingchi Jiang*

Main category: cs.CL

TL;DR: 本文提出了一种创新的方法KCS，通过多样化知识组合采样，提升多跳问答系统生成问题的质量和多样性，并在多个数据集上取得明显提升。


<details>
  <summary>Details</summary>
Motivation: 多跳问答任务因数据稀疏，容易让模型学到虚假相关性。现有方法过于关注简单问题生成，忽视了如何有效集成和选择关键知识（如文档中的相关句子），从而限制了系统的表现。

Method: 提出知识组合采样（KCS）框架，将知识（句子）组合选择建模为条件预测任务，并设计概率对比损失来预测下一个最相关的知识。在推理阶段，采用随机解码策略，实现准确性与多样性之间的平衡。

Result: KCS在多跳问答知识组合选择准确率上比现有强基线提升3.9%，并在HotpotQA和2WikiMultihopQA上通过数据增强带来整体性能提升。

Conclusion: KCS能够有效提升多跳问答生成的数据多样性与知识整合能力，为改进多跳问答系统提供了新思路和有效路径。

Abstract: Multi-hop question answering faces substantial challenges due to data
sparsity, which increases the likelihood of language models learning spurious
patterns. To address this issue, prior research has focused on diversifying
question generation through content planning and varied expression. However,
these approaches often emphasize generating simple questions and neglect the
integration of essential knowledge, such as relevant sentences within
documents. This paper introduces the Knowledge Composition Sampling (KCS), an
innovative framework designed to expand the diversity of generated multi-hop
questions by sampling varied knowledge compositions within a given context. KCS
models the knowledge composition selection as a sentence-level conditional
prediction task and utilizes a probabilistic contrastive loss to predict the
next most relevant piece of knowledge. During inference, we employ a stochastic
decoding strategy to effectively balance accuracy and diversity. Compared to
competitive baselines, our KCS improves the overall accuracy of knowledge
composition selection by 3.9%, and its application for data augmentation yields
improvements on HotpotQA and 2WikiMultihopQA datasets. Our code is available
at: https://github.com/yangfanww/kcs.

</details>


### [123] [A Graph Talks, But Who's Listening? Rethinking Evaluations for Graph-Language Models](https://arxiv.org/abs/2508.20583)
*Soham Petkar,Hari Aakash K,Anirudh Vempati,Akshit Sinha,Ponnurangam Kumarauguru,Chirag Agarwal*

Main category: cs.CL

TL;DR: 本文指出现有图-语言模型（GLMs）的评测基准无法有效评估其多模态推理能力，并提出了一个新的基准CLEGR，发现现有GLMs在结构推理任务上表现有限。


<details>
  <summary>Details</summary>
Motivation: 当前GLMs结合了图神经网络（GNNs）的结构推理能力以及大型语言模型（LLMs）的语义理解能力，但现有评测数据集大多为节点分类任务，无法真正测试多模态推理能力。该领域亟需能真正区分和考察图-语言结合推理能力的评测方法。

Method: 作者分析了现有GLMs评测基准的不足，设计了CLEGR新基准，结合合成图生成流程和问题设置，要求模型同时进行结构推理和文本理解，并用其全面评估了主流GLM架构。

Result: 实验发现，仅用软提示（soft-prompted）的LLMs，其表现可与结合完整GNN骨干的GLMs相媲美。GLMs在需要结构推理的复杂任务上性能显著下降，表明现有模型对于结构推理的能力不足。

Conclusion: 现有GLMs在多模态推理、特别是结构推理方面存在明显瓶颈。作者提出的CLEGR为该领域多模态推理能力的评估和提升奠定了基础，有助于推动图结构与语言深度结合的研究发展。

Abstract: Developments in Graph-Language Models (GLMs) aim to integrate the structural
reasoning capabilities of Graph Neural Networks (GNNs) with the semantic
understanding of Large Language Models (LLMs). However, we demonstrate that
current evaluation benchmarks for GLMs, which are primarily repurposed
node-level classification datasets, are insufficient to assess multimodal
reasoning. Our analysis reveals that strong performance on these benchmarks is
achievable using unimodal information alone, suggesting that they do not
necessitate graph-language integration. To address this evaluation gap, we
introduce the CLEGR(Compositional Language-Graph Reasoning) benchmark, designed
to evaluate multimodal reasoning at various complexity levels. Our benchmark
employs a synthetic graph generation pipeline paired with questions that
require joint reasoning over structure and textual semantics. We perform a
thorough evaluation of representative GLM architectures and find that
soft-prompted LLM baselines perform on par with GLMs that incorporate a full
GNN backbone. This result calls into question the architectural necessity of
incorporating graph structure into LLMs. We further show that GLMs exhibit
significant performance degradation in tasks that require structural reasoning.
These findings highlight limitations in the graph reasoning capabilities of
current GLMs and provide a foundation for advancing the community toward
explicit multimodal reasoning involving graph structure and language.

</details>


### [124] [Generative Annotation for ASR Named Entity Correction](https://arxiv.org/abs/2508.20700)
*Yuanchang Luo,Daimeng Wei,Shaojun Li,Hengchao Shang,Jiaxin Guo,Zongyao Li,Zhanglin Wu,Xiaoyu Chen,Zhiqiang Rao,Jinlong Yang,Hao Yang*

Main category: cs.CL

TL;DR: 本文提出了一种结合语音声学特征和生成式方法的命名实体校正（NEC）模型，显著提升了ASR系统中实体转写的准确性。


<details>
  <summary>Details</summary>
Motivation: 端到端语音识别系统在处理领域相关命名实体时经常出错，严重影响下游任务。现有轻量级NEC方法大多依赖于编辑距离等音素级匹配，但面对词形差异较大的错误识别时，无法准确定位并修正错误。

Method: 作者提出一种新的NEC方法：首先利用语音声学特征来检索候选实体，然后结合这些特征和候选实体设计生成式方法，自动标注并替换识别错误的实体文本。该方法特别适合处理词形差异大的场景。

Result: 在开源及自建测试集上的实验表明，所提出的NEC方法在实体识别准确性上有显著提升。

Conclusion: 该方法有效克服了现有编辑距离方法的局限性，显著提升ASR中命名实体的识别表现，并计划开源相关测试集与训练数据。

Abstract: End-to-end automatic speech recognition systems often fail to transcribe
domain-specific named entities, causing catastrophic failures in downstream
tasks. Numerous fast and lightweight named entity correction (NEC) models have
been proposed in recent years. These models, mainly leveraging phonetic-level
edit distance algorithms, have shown impressive performances. However, when the
forms of the wrongly-transcribed words(s) and the ground-truth entity are
significantly different, these methods often fail to locate the wrongly
transcribed words in hypothesis, thus limiting their usage. We propose a novel
NEC method that utilizes speech sound features to retrieve candidate entities.
With speech sound features and candidate entities, we inovatively design a
generative method to annotate entity errors in ASR transcripts and replace the
text with correct entities. This method is effective in scenarios of word form
difference. We test our method using open-source and self-constructed test
sets. The results demonstrate that our NEC method can bring significant
improvement to entity accuracy. We will open source our self-constructed test
set and training data.

</details>


### [125] [Multi-Lingual Implicit Discourse Relation Recognition with Multi-Label Hierarchical Learning](https://arxiv.org/abs/2508.20712)
*Nelson Filipe Costa,Leila Kosseim*

Main category: cs.CL

TL;DR: 本文提出了首个用于隐式篇章关系识别（IDRR）的多语种多标签分类模型HArch，并在DiscoGeM 2.0语料库上进行了评估。通过利用篇章感知的层级依赖，对PDTB 3.0框架中所有三级感知进行概率分布预测，并实现了最优表现。结果显示：经过专门微调的模型在所有语言设定下都优于GPT-4o和Llama-4-Maverick等大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前IDRR任务缺乏能够同时支持多语种、多标签分类的有效模型，且现有方法在处理篇章关系时未充分利用篇章感知的层级结构，因而难以有效提升性能。

Method: 提出了HArch模型，结合层级结构建模篇章感知间的依赖，分别采用RoBERTa和XLM-RoBERTa为主干，针对DiscoGeM 2.0和1.0等多语种数据集进行微调，并与现有大型语言模型进行全面对比。

Result: RoBERTa-HArch在英文任务中表现最佳，XLM-RoBERTa-HArch在多语言环境下表现最佳；与GPT-4o、Llama-4-Maverick的few-shot提示结果相比，微调后模型始终优于LLMs，并在DiscoGeM 1.0上取得SOTA结果。

Conclusion: 专门为IDRR设计的层级结构、多标签、多语种模型HArch优于当前主流大模型，展示了任务特定微调策略在篇章分析中的突出优势。

Abstract: This paper introduces the first multi-lingual and multi-label classification
model for implicit discourse relation recognition (IDRR). Our model, HArch, is
evaluated on the recently released DiscoGeM 2.0 corpus and leverages
hierarchical dependencies between discourse senses to predict probability
distributions across all three sense levels in the PDTB 3.0 framework. We
compare several pre-trained encoder backbones and find that RoBERTa-HArch
achieves the best performance in English, while XLM-RoBERTa-HArch performs best
in the multi-lingual setting. In addition, we compare our fine-tuned models
against GPT-4o and Llama-4-Maverick using few-shot prompting across all
language configurations. Our results show that our fine-tuned models
consistently outperform these LLMs, highlighting the advantages of
task-specific fine-tuning over prompting in IDRR. Finally, we report SOTA
results on the DiscoGeM 1.0 corpus, further validating the effectiveness of our
hierarchical approach.

</details>


### [126] [Addressing Tokenization Inconsistency in Steganography and Watermarking Based on Large Language Models](https://arxiv.org/abs/2508.20718)
*Ruiyi Yan,Yugo Murawaki*

Main category: cs.CL

TL;DR: 本文研究了大语言模型在文本信息隐藏（隐写术）和数字水印中的token化不一致问题，并提出专门的消除方法，提升了相关系统的鲁棒性与效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型提升了文本生成质量，使得基于文本的隐写和水印技术更加有效，但同时也带来了滥用风险。因此，需要提升其鲁棒性，特别是在实际应用中存在的tokenization inconsistency（TI）问题。

Method: 作者分析了TI产生的根源，发现问题token通常表现为“罕见性”和“临时性”。针对这一点，提出了两种解决方案：用于隐写的分步验证方法和用于水印的后置回滚方法。

Result: 实验表明：1）与传统消歧法相比，直接解决TI问题可提升隐写的流畅性、隐蔽性和抗检测能力；2）对于数字水印，处理TI后，检测性和抗攻击性也得到提升。

Conclusion: 本文提出的消除token化不一致的两种方法，能够有效提升文本隐写和水印的鲁棒性，在实际应用中具有较好的效果和改进空间。

Abstract: Large language models have significantly enhanced the capacities and
efficiency of text generation. On the one hand, they have improved the quality
of text-based steganography. On the other hand, they have also underscored the
importance of watermarking as a safeguard against malicious misuse. In this
study, we focus on tokenization inconsistency (TI) between Alice and Bob in
steganography and watermarking, where TI can undermine robustness. Our
investigation reveals that the problematic tokens responsible for TI exhibit
two key characteristics: infrequency and temporariness. Based on these
findings, we propose two tailored solutions for TI elimination: a stepwise
verification method for steganography and a post-hoc rollback method for
watermarking. Experiments show that (1) compared to traditional disambiguation
methods in steganography, directly addressing TI leads to improvements in
fluency, imperceptibility, and anti-steganalysis capacity; (2) for
watermarking, addressing TI enhances detectability and robustness against
attacks.

</details>


### [127] [rStar2-Agent: Agentic Reasoning Technical Report](https://arxiv.org/abs/2508.20722)
*Ning Shang,Yifei Liu,Yi Zhu,Li Lyna Zhang,Weijiang Xu,Xinyu Guan,Buze Zhang,Bingcheng Dong,Xudong Zhou,Bowen Zhang,Ying Xin,Ziming Miao,Scarlett Li,Fan Yang,Mao Yang*

Main category: cs.CL

TL;DR: 本文提出了rStar2-Agent，一种14B参数的数学推理模型，利用具备代理能力的强化学习在前沿水平取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 目前，长链式思维（CoT）已取得良好效果，但实际复杂推理任务中，模型在编程工具使用和中间步骤探索、修正能力仍有待提升。研究动机是提升大模型的复杂问题解决能力与代码工具协作效率，使其推理更加智能和自动化。

Method: 作者提出了三项创新：1）高效RL基础设施，结合可靠Python环境，支持高吞吐训练，降低算力消耗；2）GRPO-RoC算法，结合Resample-on-Correct滚动策略，有效应对工具环境的噪声；3）分阶段Agent训练配方，从非推理SFT到多轮RL，提升推理能力并节省计算成本。

Result: 仅用510步强化学习、一周时间，将14B参数模型提升至最先进水平，在AIME24和AIME25数学竞赛中分别达到80.6%和69.8%的平均pass@1分数，超越大规模模型DeepSeek-R1（671B），且回复更简洁。同时对泛化推理和工具使用同样表现优良。

Conclusion: rStar2-Agent验证了通过高效代理型强化学习，能以低资源消耗显著提升中等规模大模型的复杂推理与自反能力，成果具开源价值，对数学及综合智能领域有重要推动。

Abstract: We introduce rStar2-Agent, a 14B math reasoning model trained with agentic
reinforcement learning to achieve frontier-level performance. Beyond current
long CoT, the model demonstrates advanced cognitive behaviors, such as thinking
carefully before using Python coding tools and reflecting on code execution
feedback to autonomously explore, verify, and refine intermediate steps in
complex problem-solving. This capability is enabled through three key
innovations that makes agentic RL effective at scale: (i) an efficient RL
infrastructure with a reliable Python code environment that supports
high-throughput execution and mitigates the high rollout costs, enabling
training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic
RL algorithm with a Resample-on-Correct rollout strategy that addresses the
inherent environment noises from coding tools, allowing the model to reason
more effectively in a code environment; (iii) An efficient agent training
recipe that starts with non-reasoning SFT and progresses through multi-RL
stages, yielding advanced cognitive abilities with minimal compute cost. To
this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in
only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on
AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly
shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates
strong generalization to alignment, scientific reasoning, and agentic tool-use
tasks. Code and training recipes are available at
https://github.com/microsoft/rStar.

</details>


### [128] [Leveraging Semantic Triples for Private Document Generation with Local Differential Privacy Guarantees](https://arxiv.org/abs/2508.20736)
*Stephen Meisenbacher,Maulik Chevli,Florian Matthes*

Main category: cs.CL

TL;DR: 本文提出了一种基于语义三元组的文本差分隐私生成方法（DP-ST），旨在在本地差分隐私保障下，实现更高效、连贯的文本私有化。


<details>
  <summary>Details</summary>
Motivation: 在自然语言处理中，差分隐私（DP）常用于通过点扰或者重写文本保护隐私，尤其是在本地差分隐私场景，但当前方法往往要求较高的隐私参数ε才能保持合理文本质量，难以兼顾隐私与可用性。

Method: 提出DP-ST方法，通过语义三元组将文档分解，并在邻域范围内采用本地差分隐私机制生成私有化文本。方法还结合大语言模型（LLM）进行后处理，以增强生成文本的连贯性。

Result: 实验表明，DP-ST方法利用分而治之策略，能在较低ε值下实现文本连贯且兼顾隐私与实用性。限制专注于邻域内私有化，有效提升了低ε场景下的文本质量。

Conclusion: DP-ST方法强调文本生成的连贯性，对于实现平衡的私有化输出至关重要。通过邻域限定和LLM后处理，可在较低ε下达成较佳隐私-可用性权衡，对差分隐私文本生成有积极推动作用。

Abstract: Many works at the intersection of Differential Privacy (DP) in Natural
Language Processing aim to protect privacy by transforming texts under DP
guarantees. This can be performed in a variety of ways, from word perturbations
to full document rewriting, and most often under local DP. Here, an input text
must be made indistinguishable from any other potential text, within some bound
governed by the privacy parameter $\varepsilon$. Such a guarantee is quite
demanding, and recent works show that privatizing texts under local DP can only
be done reasonably under very high $\varepsilon$ values. Addressing this
challenge, we introduce DP-ST, which leverages semantic triples for
neighborhood-aware private document generation under local DP guarantees.
Through the evaluation of our method, we demonstrate the effectiveness of the
divide-and-conquer paradigm, particularly when limiting the DP notion (and
privacy guarantees) to that of a privatization neighborhood. When combined with
LLM post-processing, our method allows for coherent text generation even at
lower $\varepsilon$ values, while still balancing privacy and utility. These
findings highlight the importance of coherence in achieving balanced
privatization outputs at reasonable $\varepsilon$ levels.

</details>


### [129] [Specializing General-purpose LLM Embeddings for Implicit Hate Speech Detection across Datasets](https://arxiv.org/abs/2508.20750)
*Vassiliy Cheremetiev,Quang Long Ho Ngo,Chau Ying Kot,Alina Elena Baia,Andrea Cavallaro*

Main category: cs.CL

TL;DR: 本文研究了如何检测隐性仇恨言论（IHS），通过对LLM嵌入模型进行微调，达到了最新的性能。


<details>
  <summary>Details</summary>
Motivation: 隐性仇恨言论以间接、隐晦的方式表达偏见或仇恨，难以通过传统的基于关键词方法识别。提升IHS检测能力有助于网络安全与社交媒体治理。

Method: 作者使用了几种基于大型语言模型（LLMs）的通用嵌入模型（如Stella、Jasper、NV-Embed和E5），并通过微调方式应用于IHS任务，无需额外引入外部知识或情感信息。

Result: 在多个IHS数据集上，微调后的模型在F1-macro分数上，原始数据集最高提升1.10个百分点，跨数据集评测提升高达20.35个百分点，达到了最新水平。

Conclusion: 通过微调LLM嵌入模型，无需特别的上下文或情绪数据补充，就能有效检测隐性仇恨言论，并显著提升跨域泛化能力。

Abstract: Implicit hate speech (IHS) is indirect language that conveys prejudice or
hatred through subtle cues, sarcasm or coded terminology. IHS is challenging to
detect as it does not include explicit derogatory or inflammatory words. To
address this challenge, task-specific pipelines can be complemented with
external knowledge or additional information such as context, emotions and
sentiment data. In this paper, we show that, by solely fine-tuning recent
general-purpose embedding models based on large language models (LLMs), such as
Stella, Jasper, NV-Embed and E5, we achieve state-of-the-art performance.
Experiments on multiple IHS datasets show up to 1.10 percentage points
improvements for in-dataset, and up to 20.35 percentage points improvements in
cross-dataset evaluation, in terms of F1-macro score.

</details>


### [130] [GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and Efficient Open-Ended Text Generation](https://arxiv.org/abs/2508.20757)
*Yuanhao Ding,Esteban Garces Arias,Meimingwei Li,Julian Rodemann,Matthias Aßenmacher,Danlu Chen,Gaojuan Fan,Christian Heumann,Chongsheng Zhang*

Main category: cs.CL

TL;DR: GUARD是一种用于大语言模型（LLM）自由文本生成的自适应解码方法，能平衡文本的连贯性和多样性，并提升生成速度。


<details>
  <summary>Details</summary>
Motivation: 现有的对比搜索解码法在平衡文本多样性和连贯性上有效，但高度依赖超参数且计算成本高，影响实际应用。作者提出GUARD以解决这些问题。

Method: GUARD方法采用创新的“Glocal”不确定性驱动框架，结合全局熵估计和局部熵偏差，既考虑长远也兼顾局部不确定性。同时采用基于token数量的惩罚机制，降低计算负担。

Result: 实验表明GUARD在文本多样性与连贯性之间实现了良好平衡，同时大幅提升生成速度。在多维度的文本质量评测中，无论人工还是LLM评审，GUARD均表现优秀。

Conclusion: GUARD不仅理论上保证解码的无偏性和一致性，在实验中也展现了兼顾质量和效率的强大优势，为实际用例提供重要价值。

Abstract: Open-ended text generation faces a critical challenge: balancing coherence
with diversity in LLM outputs. While contrastive search-based decoding
strategies have emerged to address this trade-off, their practical utility is
often limited by hyperparameter dependence and high computational costs. We
introduce GUARD, a self-adaptive decoding method that effectively balances
these competing objectives through a novel "Glocal" uncertainty-driven
framework. GUARD combines global entropy estimates with local entropy
deviations to integrate both long-term and short-term uncertainty signals. We
demonstrate that our proposed global entropy formulation effectively mitigates
abrupt variations in uncertainty, such as sudden overconfidence or high entropy
spikes, and provides theoretical guarantees of unbiasedness and consistency. To
reduce computational overhead, we incorporate a simple yet effective
token-count-based penalty into GUARD. Experimental results demonstrate that
GUARD achieves a good balance between text diversity and coherence, while
exhibiting substantial improvements in generation speed. In a more nuanced
comparison study across different dimensions of text quality, both human and
LLM evaluators validated its remarkable performance. Our code is available at
https://github.com/YecanLee/GUARD.

</details>


### [131] [Feel the Difference? A Comparative Analysis of Emotional Arcs in Real and LLM-Generated CBT Sessions](https://arxiv.org/abs/2508.20764)
*Xiaoyi Wang,Jiwei Zhang,Guangtao Zhang,Honglei Guo*

Main category: cs.CL

TL;DR: 本论文比较了真实和大语言模型生成的认知行为疗法（CBT）对话的情感动态，发现合成对话在情感真实性上存在显著差距，并发布了真实CBT数据集以促进相关研究。


<details>
  <summary>Details</summary>
Motivation: 当前心理健康NLP领域大量使用由大语言模型生成的合成心理咨询对话用于模型训练等，但尚不清楚这些对话能否真实反映真实咨询过程中的细腻情感动态。因此，作者希望系统性剖析两者在情感表达上的异同。

Method: 作者采用Utterance Emotion Dynamics框架，从效价、唤醒和主导性三个维度细致分析真实与合成CBT对话（包括咨询师和来访者）。真实数据来自公开视频转写，合成数据来自CACTUS数据集，对比两者在情感轨迹上的表现。

Result: 合成对话虽然流畅、结构合理，但在情感特性上与真实对话存在差异：真实对话情感变化更丰富，情感用词更多，反应和调节模式更自然。真实与合成角色（特别是来访者）的情感弧线相似度较低。

Conclusion: 目前大语言模型生成的心理咨询对话在情感真实性上有明显不足，影响其在心理健康场景下的可靠性。作者发布了RealCBT真实对话数据集，旨在推动后续相关研究。

Abstract: Synthetic therapy dialogues generated by large language models (LLMs) are
increasingly used in mental health NLP to simulate counseling scenarios, train
models, and supplement limited real-world data. However, it remains unclear
whether these synthetic conversations capture the nuanced emotional dynamics of
real therapy. In this work, we conduct the first comparative analysis of
emotional arcs between real and LLM-generated Cognitive Behavioral Therapy
dialogues. We adapt the Utterance Emotion Dynamics framework to analyze
fine-grained affective trajectories across valence, arousal, and dominance
dimensions. Our analysis spans both full dialogues and individual speaker roles
(counselor and client), using real sessions transcribed from public videos and
synthetic dialogues from the CACTUS dataset. We find that while synthetic
dialogues are fluent and structurally coherent, they diverge from real
conversations in key emotional properties: real sessions exhibit greater
emotional variability,more emotion-laden language, and more authentic patterns
of reactivity and regulation. Moreover, emotional arc similarity between real
and synthetic speakers is low, especially for clients. These findings
underscore the limitations of current LLM-generated therapy data and highlight
the importance of emotional fidelity in mental health applications. We
introduce RealCBT, a curated dataset of real CBT sessions, to support future
research in this space.

</details>


### [132] [Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection](https://arxiv.org/abs/2508.20766)
*Harethah Abu Shairah,Hasan Abed Al Kader Hammoud,George Turkiyyah,Bernard Ghanem*

Main category: cs.CL

TL;DR: 本文提出了一种名为ROSI（Rank-One Safety Injection）的新方法，通过简单权重修改，增强LLM（大型语言模型）的安全性，提升拒绝有害请求的能力，同时保持模型原有的有用性和表现。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的安全机制容易被绕过，且现有对齐方法多需消耗大量计算资源或复杂的微调流程。本文动机是开发一个廉价、高效、易于理解的安全增强技术，解决安全性与实用性之间的平衡痛点。

Method: 提出白盒方法ROSI，在所有residual stream写入矩阵施加一个秩为1的权重调整，使模型激活向“拒绝子空间”方向强化。所需的安全方向只需通过小规模的有害、无害指令对计算获得，无需重新微调。

Result: 实验表明，应用ROSI后，模型（经Llama Guard 3评估）拒绝有害请求的比例明显提升，而在MMLU、HellaSwag和Arc等标准任务上仍保持高性能。此外，ROSI也可以提升“未审查”模型自身的安全暗示，实现即时安全重调。

Conclusion: 有针对性的、可解释的权重调控（如ROSI），是一种低成本、有效补充现有资源密集型微调方案的LLM安全提升机制。

Abstract: Safety alignment in Large Language Models (LLMs) often involves mediating
internal representations to refuse harmful requests. Recent research has
demonstrated that these safety mechanisms can be bypassed by ablating or
removing specific representational directions within the model. In this paper,
we propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box
method that amplifies a model's safety alignment by permanently steering its
activations toward the refusal-mediating subspace. ROSI operates as a simple,
fine-tuning-free rank-one weight modification applied to all residual stream
write matrices. The required safety direction can be computed from a small set
of harmful and harmless instruction pairs. We show that ROSI consistently
increases safety refusal rates - as evaluated by Llama Guard 3 - while
preserving the utility of the model on standard benchmarks such as MMLU,
HellaSwag, and Arc. Furthermore, we show that ROSI can also re-align
'uncensored' models by amplifying their own latent safety directions,
demonstrating its utility as an effective last-mile safety procedure. Our
results suggest that targeted, interpretable weight steering is a cheap and
potent mechanism to improve LLM safety, complementing more resource-intensive
fine-tuning paradigms.

</details>


### [133] [Signs of Struggle: Spotting Cognitive Distortions across Language and Register](https://arxiv.org/abs/2508.20771)
*Abhishek Kuber,Enrico Liscio,Ruixuan Zhang,Caroline Figueroa,Pradeep K. Murukannaiah*

Main category: cs.CL

TL;DR: 本论文关注于通过自动化方法检测青少年数字文本中的早期心理困扰迹象，特别是认知扭曲的识别，并首次对荷兰青少年论坛帖进行了跨语言、跨文体的泛化研究。


<details>
  <summary>Details</summary>
Motivation: 青少年心理健康问题增加，加之线上互动日益频繁，如何通过低成本、自动化方式及早发现心理困扰成为研究热点，尤其是识别加剧心理问题的认知扭曲。

Method: 本研究首次探索了认知扭曲检测在跨语言（非英语）和跨文本风格（register）下的泛化能力，利用荷兰青少年论坛文本进行实验，同时比较各种领域自适应方法。

Result: 结果显示，语言和写作风格的变化会显著影响模型表现，但领域自适应方法在提升跨语言、跨文体场景下的识别效果方面表现最好。

Conclusion: 实现认知扭曲的自动检测在不同语言和文本风格环境下确有挑战，但领域适应技术为实际应用带来了新的希望。

Abstract: Rising mental health issues among youth have increased interest in automated
approaches for detecting early signs of psychological distress in digital text.
One key focus is the identification of cognitive distortions, irrational
thought patterns that have a role in aggravating mental distress. Early
detection of these distortions may enable timely, low-cost interventions. While
prior work has focused on English clinical data, we present the first in-depth
study of cross-lingual and cross-register generalization of cognitive
distortion detection, analyzing forum posts written by Dutch adolescents. Our
findings show that while changes in language and writing style can
significantly affect model performance, domain adaptation methods show the most
promise.

</details>


### [134] [Exploring Machine Learning and Language Models for Multimodal Depression Detection](https://arxiv.org/abs/2508.20805)
*Javier Si Zhao Hong,Timothy Zoe Delaya,Sherwyn Chan Yin Kit,Pai Chet Ng,Xiaoxiao Miao*

Main category: cs.CL

TL;DR: 本文比较了多种机器学习与深度学习模型在多模态抑郁症检测任务中的表现，利用音频、视频和文本特征分析各模型优劣。


<details>
  <summary>Details</summary>
Motivation: 抑郁症检测对于心理健康至关重要，而多模态数据能够提供更全面的用户信息，探索不同模型在多模态下的表现有助于提升检测准确率与实际应用价值。

Method: 作者分别采用了XGBoost、基于transformer的架构和大语言模型（LLMs），对音频、视频和文本三种模式的特征进行融合与比较分析。

Result: 实验结果表明，不同模型对于多模态数据中的抑郁症相关信号捕捉各有长短，并揭示出各种模型在不同模态上的表现特点。

Conclusion: 研究为心理健康预测中的有效多模态表征策略提供了新见解，并指出了各类模型的优势与局限性。

Abstract: This paper presents our approach to the first Multimodal Personality-Aware
Depression Detection Challenge, focusing on multimodal depression detection
using machine learning and deep learning models. We explore and compare the
performance of XGBoost, transformer-based architectures, and large language
models (LLMs) on audio, video, and text features. Our results highlight the
strengths and limitations of each type of model in capturing depression-related
signals across modalities, offering insights into effective multimodal
representation strategies for mental health prediction.

</details>


### [135] [GDLLM: A Global Distance-aware Modeling Approach Based on Large Language Models for Event Temporal Relation Extraction](https://arxiv.org/abs/2508.20828)
*Jie Zhao,Wanting Ning,Yuxiao Fei,Yubo Feng,Lishuang Li*

Main category: cs.CL

TL;DR: 论文提出了一种结合大语言模型和图神经网络的全局距离感知方法（GDLLM），有效提升了事件时序关系抽取的性能，特别是在处理类别不平衡问题上取得了SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 小模型由于预训练知识有限，难以处理少数类别。大模型则受手动提示词带来的噪声影响，难以准确判断远距离事件间的关系。针对这些问题，需要设计一种新方法来同时解决远距离依赖和小样本类别识别能力不足的问题。

Method: 提出了GDLLM方法，将Graph Attention Network（GAT）构建距离感知图结构，帮助大模型捕获事件间的长距离依赖。同时，设计了基于软推理的时序特征学习范式，利用LLM生成的概率信息增强短距离事件关系的识别能力，并融入多头注意力机制中。

Result: 在TB-Dense和MATRES两个公开基准数据集上进行了实验，GDLLM方法显著提升了少数类时序关系的识别效果，并达到了最新的SOTA性能。

Conclusion: 结合图结构建模和大模型的全局特征捕获能力，不仅提升了整体性能，也有效增强了对长距离和少数类别事件关系的建模能力，体现了方法的实用性和创新性。

Abstract: In Natural Language Processing(NLP), Event Temporal Relation Extraction
(ETRE) is to recognize the temporal relations of two events. Prior studies have
noted the importance of language models for ETRE. However, the restricted
pre-trained knowledge of Small Language Models(SLMs) limits their capability to
handle minority class relations in imbalanced classification datasets. For
Large Language Models(LLMs), researchers adopt manually designed prompts or
instructions, which may introduce extra noise, leading to interference with the
model's judgment of the long-distance dependencies between events. To address
these issues, we propose GDLLM, a Global Distance-aware modeling approach based
on LLMs. We first present a distance-aware graph structure utilizing Graph
Attention Network(GAT) to assist the LLMs in capturing long-distance dependency
features. Additionally, we design a temporal feature learning paradigm based on
soft inference to augment the identification of relations with a short-distance
proximity band, which supplements the probabilistic information generated by
LLMs into the multi-head attention mechanism. Since the global feature can be
captured effectively, our framework substantially enhances the performance of
minority relation classes and improves the overall learning ability.
Experiments on two publicly available datasets, TB-Dense and MATRES,
demonstrate that our approach achieves state-of-the-art (SOTA) performance.

</details>


### [136] [MSRS: Evaluating Multi-Source Retrieval-Augmented Generation](https://arxiv.org/abs/2508.20867)
*Rohan Phanse,Yijie Zhou,Kejian Shi,Wencai Zhang,Yixin Liu,Yilun Zhao,Arman Cohan*

Main category: cs.CL

TL;DR: 该论文提出并构建了两个能够考验检索增强生成（RAG）系统多源信息整合与生成能力的新基准，重点在于长文本生成任务。实验发现，多源的检索与综合比传统单一、短答检索任务更具挑战。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统评测大多局限于信息只需单一来源、答案较短或为事实型的问题，难以体现真实应用中需要从多个独立来源整合、总结信息生成长文本的能力。该工作旨在填补此类评测基准的空白。

Method: 作者提出了一套可扩展的基准构建框架，用于评测RAG系统在需要多源检索与综合生成长文本的任务中的表现，并据此开发了两个新数据集：MSRS-Story（叙事型信息整合）和MSRS-Meet（多源会议摘要）。实验涵盖了多种稀疏/稠密检索与主流大模型的组合。

Result: 实验发现，生成质量高度依赖于检索效果，且检索表现因任务不同差异较大。即使在理想检索条件下，多源信息融合与推理仍有很高挑战性，但推理模型在此步骤上明显优于标准大模型。

Conclusion: 多源信息检索和综合显著提高了RAG系统的难度，现有技术面临挑战。新基准有助于推动相关模型能力提升，尤其是多源推理与综合能力。

Abstract: Retrieval-augmented systems are typically evaluated in settings where
information required to answer the query can be found within a single source or
the answer is short-form or factoid-based. However, many real-world
applications demand the ability to integrate and summarize information
scattered across multiple sources, where no single source is sufficient to
respond to the user's question. In such settings, the retrieval component of a
RAG pipeline must recognize a variety of relevance signals, and the generation
component must connect and synthesize information across multiple sources. We
present a scalable framework for constructing evaluation benchmarks that
challenge RAG systems to integrate information across distinct sources and
generate long-form responses. Using our framework, we build two new benchmarks
on Multi-Source Retrieval and Synthesis: MSRS-Story and MSRS-Meet, representing
narrative synthesis and summarization tasks, respectively, that require
retrieval from large collections. Our extensive experiments with various RAG
pipelines -- including sparse and dense retrievers combined with frontier LLMs
-- reveal that generation quality is highly dependent on retrieval
effectiveness, which varies greatly by task. While multi-source synthesis
proves challenging even in an oracle retrieval setting, we find that reasoning
models significantly outperform standard LLMs at this distinct step.

</details>


### [137] [The Uneven Impact of Post-Training Quantization in Machine Translation](https://arxiv.org/abs/2508.20893)
*Benjamin Marie,Atsushi Fujita*

Main category: cs.CL

TL;DR: 本文对后训练量化（PTQ）在机器翻译中的影响进行了大规模评估，涵盖55种语言和多个LLM模型。结果显示，虽然高资源语言在4-bit量化下表现良好，但低资源和类型多样语言在2-bit等低比特率下性能下降严重。


<details>
  <summary>Details</summary>
Motivation: 量化可以有效降低模型部署的硬件资源需求，但其对多语言任务，尤其是在低资源语言上的影响尚不清楚。探索不同量化策略对多语言翻译模型的影响，有助于实际部署中在性能与资源之间做出权衡。

Method: 作者在五个参数规模从1.7B到70B的LLM上，系统评估了四种PTQ技术（AWQ、BitsAndBytes、GGUF、AutoRound）在55种语言翻译任务中的表现，并分析解码超参数和标定语言的相互作用。

Result: 4-bit量化通常能保持大模型和高资源语言的翻译质量，但在2-bit及低资源、类型多样化语言下效果显著下降。GGUF量化方法在低比特率下表现最稳定。语言匹配的校准在低比特情况下对性能有一定提升。

Conclusion: 本文为多语言LLM在量化条件下的机器翻译部署提供了实践建议：如选择合适的量化算法、模型大小及校准数据，尤其是在低资源场景中，有助于更好地权衡模型精度与计算资源。

Abstract: Quantization is essential for deploying large language models (LLMs) on
resource-constrained hardware, but its implications for multilingual tasks
remain underexplored. We conduct the first large-scale evaluation of
post-training quantization (PTQ) on machine translation across 55 languages
using five LLMs ranging from 1.7B to 70B parameters. Our analysis reveals that
while 4-bit quantization often preserves translation quality for high-resource
languages and large models, significant degradation occurs for low-resource and
typologically diverse languages, particularly in 2-bit settings. We compare
four quantization techniques (AWQ, BitsAndBytes, GGUF, and AutoRound), showing
that algorithm choice and model size jointly determine robustness. GGUF
variants provide the most consistent performance, even at 2-bit precision.
Additionally, we quantify the interactions between quantization, decoding
hyperparameters, and calibration languages, finding that language-matched
calibration offers benefits primarily in low-bit scenarios. Our findings offer
actionable insights for deploying multilingual LLMs for machine translation
under quantization constraints, especially in low-resource settings.

</details>


### [138] [SageLM: A Multi-aspect and Explainable Large Language Model for Speech Judgement](https://arxiv.org/abs/2508.20916)
*Yuan Ge,Junxiang Zhang,Xiaoqian Liu,Bei Li,Xiangnan Ma,Chenglong Wang,Kaiyang Ye,Yangfan Du,Linfeng Zhang,Yuxin Huang,Tong Xiao,Zhengtao Yu,JingBo Zhu*

Main category: cs.CL

TL;DR: 本文提出了 SageLM——一种用于评测语音到语音大型语言模型（S2S LLM）的端到端、多维度、可解释性的模型，在语义和声学层面均能全面评测，并表现出高度与人类评测者一致的能力。


<details>
  <summary>Details</summary>
Motivation: 现有针对S2S大型语言模型的评测手段难以兼顾语义和声学特征，且缺乏解释性与高效的评测数据方法。为解决客观全面评估S2S LLM的挑战，提出新的评测框架和数据集。

Method: 1）SageLM联合评测语义与声学两方面特征，不同于只评语义的级联方法；2）采用基于理由的监督方式提升解释性与模型学习效果，优于仅基于规则的强化学习方法；3）引入合成语音偏好数据集SpeechFeedback，并利用两阶段训练弥补数据稀缺问题。

Result: SageLM在语义和声学维度均实现了，人类评测者一致率高达82.79%，较级联方法和仅SLM基线分别高7.42%和26.20%。

Conclusion: SageLM作为一款端到端、可解释、综合评估S2S语音大模型的工具，实现了更全面、更接近人类评价标准的评测效果，为S2S LLM系统评估带来新方法。

Abstract: Speech-to-Speech (S2S) Large Language Models (LLMs) are foundational to
natural human-computer interaction, enabling end-to-end spoken dialogue
systems. However, evaluating these models remains a fundamental challenge. We
propose \texttt{SageLM}, an end-to-end, multi-aspect, and explainable speech
LLM for comprehensive S2S LLMs evaluation. First, unlike cascaded approaches
that disregard acoustic features, SageLM jointly assesses both semantic and
acoustic dimensions. Second, it leverages rationale-based supervision to
enhance explainability and guide model learning, achieving superior alignment
with evaluation outcomes compared to rule-based reinforcement learning methods.
Third, we introduce \textit{SpeechFeedback}, a synthetic preference dataset,
and employ a two-stage training paradigm to mitigate the scarcity of speech
preference data. Trained on both semantic and acoustic dimensions, SageLM
achieves an 82.79\% agreement rate with human evaluators, outperforming
cascaded and SLM-based baselines by at least 7.42\% and 26.20\%, respectively.

</details>


### [139] [How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on $τ$-bench](https://arxiv.org/abs/2508.20931)
*Venkatesh Mishra,Amir Saeidi,Satyam Raj,Mutsumi Nakamura,Jayanth Srinivasa,Gaowen Liu,Ali Payani,Chitta Baral*

Main category: cs.CL

TL;DR: 本文提出了一种名为IRMA（Input-Reformulation Multi-Agent）的多智能体框架，通过自动重构用户查询并结合领域规则与工具推荐，显著提升了大型语言模型作为工具使用代理在动态多轮对话环境下的稳定性和表现。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在推理和工具调用方面表现优异，但在多轮对话（如τ-bench）中容易出现推理不一致、不遵循特定规则以及信息提取不准确等问题。为提升其可靠性，作者希望系统性分析导致这些表现不佳的错误类型，并提出针对性的改进方法。

Method: 作者首先对工具调用代理在对话过程中的常见错误进行手动分析。随后，尝试通过输入重构方式提升决策表现。最后，提出IRMA框架，自动重构用户输入，将其与领域特定规则和工具建议相结合，帮助工具调用代理更好聚焦于任务和决策。

Result: 实验数据显示，IRMA在overall pass^5分数上分别比ReAct、Function Calling和Self-Reflection提升了16.1%、12.7%和19.1%，在动态环境下展现出更高的可靠性和一致性。

Conclusion: IRMA框架大幅提升了大型语言模型作为工具代理在复杂多轮对话环境中的稳定性和准确性，优于现有主流方法，为未来强化LLM自治能力提供了有效手段。

Abstract: Recent advances in reasoning and planning capabilities of large language
models (LLMs) have enabled their potential as autonomous agents capable of tool
use in dynamic environments. However, in multi-turn conversational environments
like $\tau$-bench, these agents often struggle with consistent reasoning,
adherence to domain-specific policies, and extracting correct information over
a long horizon of tool-calls and conversation. To capture and mitigate these
failures, we conduct a comprehensive manual analysis of the common errors
occurring in the conversation trajectories. We then experiment with
reformulations of inputs to the tool-calling agent for improvement in agent
decision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA)
framework, which automatically reformulates user queries augmented with
relevant domain rules and tool suggestions for the tool-calling agent to focus
on. The results show that IRMA significantly outperforms ReAct, Function
Calling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in
overall pass^5 scores. These findings highlight the superior reliability and
consistency of IRMA compared to other methods in dynamic environments.

</details>


### [140] [STARE at the Structure: Steering ICL Exemplar Selection with Structural Alignment](https://arxiv.org/abs/2508.20944)
*Jiaqian Li,Qisheng Hu,Jing Li,Wenya Wang*

Main category: cs.CL

TL;DR: 提出了一种结构感知的两阶段示例选择方法，提升大模型在结构化预测任务如语义解析中的表现，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大模型使用ICL（In-Context Learning）在无需特定微调下实现多任务，但ICL的效果高度依赖于示例选择，尤其在结构化任务上，现有策略常忽视了结构对齐，导致效果不佳与泛化性弱。

Method: 方法包括两阶段：首先，利用结构感知方式对BERT检索器进行微调，使其能选择语义相关且结构对齐的示例；其次，为检索器增加一个插件模块，增强语法层面信息，该模块与模型无关、开销小、易集成。

Result: 在三个语义解析任务、四个基准上验证，搭配多种主流大模型时，新方法均超过现有基线表现。

Conclusion: 结构感知的两阶段示例选择策略能有效提升结构化任务的泛化与性能，具有较强的效率和适用性。

Abstract: In-Context Learning (ICL) has become a powerful paradigm that enables LLMs to
perform a wide range of tasks without task-specific fine-tuning. However, the
effectiveness of ICL heavily depends on the quality of exemplar selection. In
particular, for structured prediction tasks such as semantic parsing, existing
ICL selection strategies often overlook structural alignment, leading to
suboptimal performance and poor generalization. To address this issue, we
propose a novel two-stage exemplar selection strategy that achieves a strong
balance between efficiency, generalizability, and performance. First, we
fine-tune a BERT-based retriever using structure-aware supervision, guiding it
to select exemplars that are both semantically relevant and structurally
aligned. Then, we enhance the retriever with a plug-in module, which amplifies
syntactically meaningful information in the hidden representations. This
plug-in is model-agnostic, requires minimal overhead, and can be seamlessly
integrated into existing pipelines. Experiments on four benchmarks spanning
three semantic parsing tasks demonstrate that our method consistently
outperforms existing baselines with multiple recent LLMs as inference-time
models.

</details>


### [141] [ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue Agents](https://arxiv.org/abs/2508.20973)
*Tianjian Liu,Fanqi Wan,Jiajian Guo,Xiaojun Quan*

Main category: cs.CL

TL;DR: 本文提出了ProactiveEval框架，实现对大语言模型主动对话能力的统一评价。框架拆解主动对话为目标制定和对话引导，并自动生成多样化测试环境，为22种LLM在多个领域进行系统评测。


<details>
  <summary>Details</summary>
Motivation: 目前主动对话评估多局限在特定领域，缺乏统一标准，导致评价碎片化、覆盖面窄，限制了对LLM主动对话能力的全面理解。

Method: 提出ProactiveEval统一评测框架，将主动对话分为目标规划和对话引导两部分，并设立跨领域评价指标。此外，框架可自动生成多样复杂的评估数据。基于此，作者构建了涵盖6大领域的328个评测环境，对22种主流LLM进行了系统对比实验。

Result: 实验发现，不同模型在分任务上各有长处：DeepSeek-R1在目标规划表现突出，Claude-3.7-Sonnet在对话引导上表现优异，并通过实验分析推理能力对主动行为的影响。

Conclusion: ProactiveEval为主动对话能力评估提供了统一方法和丰富数据，有助于促进LLM主动能力研究，并为今后的模型设计和发展方向带来启示。

Abstract: Proactive dialogue has emerged as a critical and challenging research problem
in advancing large language models (LLMs). Existing works predominantly focus
on domain-specific or task-oriented scenarios, which leads to fragmented
evaluations and limits the comprehensive exploration of models' proactive
conversation abilities. In this work, we propose ProactiveEval, a unified
framework designed for evaluating proactive dialogue capabilities of LLMs. This
framework decomposes proactive dialogue into target planning and dialogue
guidance, establishing evaluation metrics across various domains. Moreover, it
also enables the automatic generation of diverse and challenging evaluation
data. Based on the proposed framework, we develop 328 evaluation environments
spanning 6 distinct domains. Through experiments with 22 different types of
LLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional
performance on target planning and dialogue guidance tasks, respectively.
Finally, we investigate how reasoning capabilities influence proactive
behaviors and discuss their implications for future model development.

</details>


### [142] [Lethe: Purifying Backdoored Large Language Models with Knowledge Dilution](https://arxiv.org/abs/2508.21004)
*Chen Chen,Yuchen Sun,Jiaxin Gao,Xueluan Gong,Qian Wang,Ziyao Wang,Yongsen Zheng,Kwok-Yan Lam*

Main category: cs.CL

TL;DR: 本文提出了一种名为LETHE的新方法，有效清除大语言模型（LLM）中的后门行为，同时保持模型性能。实验显示，在多种主流后门攻击下，LETHE显著优于现有防御方法。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型尽管性能强大，但容易受到后门攻击。一旦被攻击，模型在特定触发条件下会输出有害或意外内容。现有防御方法或仅能检测某些类型的后门，或难以应对复杂攻击场景，迫切需要更加完备且适用范围广的防御措施。

Method: LETHE方法分为内部和外部两部分：内部通过用轻量级干净数据集训练干净模型，并与受污染的模型融合，稀释参数中后门的影响；外部则通过在模型输入中插入良性、相关的证据，引导模型注意力远离后门特征。

Result: 在5种主流LLM、8种后门攻击下进行的分类与生成任务实验证明，LETHE的攻击成功率最高可下降98%，综合效果超过8种主流防御基线，且不影响模型正常用途。

Conclusion: LETHE是一种高效、适应性强且成本低廉的后门防御方案，能够在保持LLM实用性的前提下，有效抵御多类型后门攻击。

Abstract: Large language models (LLMs) have seen significant advancements, achieving
superior performance in various Natural Language Processing (NLP) tasks.
However, they remain vulnerable to backdoor attacks, where models behave
normally for standard queries but generate harmful responses or unintended
output when specific triggers are activated. Existing backdoor defenses either
lack comprehensiveness, focusing on narrow trigger settings, detection-only
mechanisms, and limited domains, or fail to withstand advanced scenarios like
model-editing-based, multi-trigger, and triggerless attacks. In this paper, we
present LETHE, a novel method to eliminate backdoor behaviors from LLMs through
knowledge dilution using both internal and external mechanisms. Internally,
LETHE leverages a lightweight dataset to train a clean model, which is then
merged with the backdoored model to neutralize malicious behaviors by diluting
the backdoor impact within the model's parametric memory. Externally, LETHE
incorporates benign and semantically relevant evidence into the prompt to
distract LLM's attention from backdoor features. Experimental results on
classification and generation domains across 5 widely used LLMs demonstrate
that LETHE outperforms 8 state-of-the-art defense baselines against 8 backdoor
attacks. LETHE reduces the attack success rate of advanced backdoor attacks by
up to 98% while maintaining model utility. Furthermore, LETHE has proven to be
cost-efficient and robust against adaptive backdoor attacks.

</details>


### [143] [An Agile Method for Implementing Retrieval Augmented Generation Tools in Industrial SMEs](https://arxiv.org/abs/2508.21024)
*Mathieu Bourdin,Anas Neumann,Thomas Paviot,Robert Pellerin,Samir Lamouri*

Main category: cs.CL

TL;DR: 本文提出了EASI-RAG方法，旨在帮助中小企业（SMEs）快速、有效地部署检索增强生成（RAG）系统，即使团队缺乏NLP经验。该方法在实际环境实验室案例中得到验证，实现了快速部署和高用户采纳率。


<details>
  <summary>Details</summary>
Motivation: 虽然RAG技术能够弥补大型语言模型的幻觉和知识过时等缺陷，但其在中小企业中的应用受限于资源和专业能力。本文的动机是为工业SMEs提供一套易于采用的RAG部署方法。

Method: 提出了基于方法工程的EASI-RAG方法，定义了清晰的角色、活动和技术路径。在一个环境检测实验室中通过实际案例实现和迭代优化。

Result: 实验表明，无RAG经验的团队可在一个月内完成部署，用户采纳率高，系统回答准确，并提高了数据可靠性。

Conclusion: EASI-RAG方法能够支持工业SMEs高效实施RAG系统，未来工作方向包括推广到更多应用场景以及结合精调模型。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful solution to
mitigate the limitations of Large Language Models (LLMs), such as
hallucinations and outdated knowledge. However, deploying RAG-based tools in
Small and Medium Enterprises (SMEs) remains a challenge due to their limited
resources and lack of expertise in natural language processing (NLP). This
paper introduces EASI-RAG, Enterprise Application Support for Industrial RAG, a
structured, agile method designed to facilitate the deployment of RAG systems
in industrial SME contexts. EASI-RAG is based on method engineering principles
and comprises well-defined roles, activities, and techniques. The method was
validated through a real-world case study in an environmental testing
laboratory, where a RAG tool was implemented to answer operators queries using
data extracted from operational procedures. The system was deployed in under a
month by a team with no prior RAG experience and was later iteratively improved
based on user feedback. Results demonstrate that EASI-RAG supports fast
implementation, high user adoption, delivers accurate answers, and enhances the
reliability of underlying data. This work highlights the potential of RAG
deployment in industrial SMEs. Future works include the need for generalization
across diverse use cases and further integration with fine-tuned models.

</details>


### [144] [Re-Representation in Sentential Relation Extraction with Sequence Routing Algorithm](https://arxiv.org/abs/2508.21049)
*Ramazan Ali Bahrami,Ramin Yahyapour*

Main category: cs.CL

TL;DR: 本文提出了一种在胶囊网络下使用动态路由方法进行句子级关系抽取的新方法，在多个主流数据集上取得了领先表现，但在一个噪声较大的Wikidata数据集上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 句子级关系抽取对于信息抽取和知识图谱构建等NLP任务非常重要，但现有方法在不同数据集上表现不稳定，尤其面对含有标签噪声的大型数据集时性能下降。

Method: 提出利用动态路由胶囊网络（capsule network with dynamic routing）进行句子级关系抽取。方法不仅在结构上创新，还重点分析了模型在不同数据集上的表现差异，尤其关注噪声影响和再表征（re-representation）能力。

Result: 在Tacred、Tacredrev、Retacred和Conll04等常见数据集上优于现有最佳方法，但在Wikidata这样标签噪声较大的数据集上效果较差。实验证明，改进的模型在再表征能力上优于vanilla模型。

Conclusion: 该方法在标准数据集上表现突出，但暴露出标签噪声影响大和再表征能力不足的问题。提出再表征能力也是句子级关系抽取模型需关注的重要挑战。

Abstract: Sentential relation extraction (RE) is an important task in natural language
processing (NLP). In this paper we propose to do sentential RE with dynamic
routing in capsules. We first show that the proposed approach outperform state
of the art on common sentential relation extraction datasets Tacred, Tacredrev,
Retacred, and Conll04. We then investigate potential reasons for its good
performance on the mentioned datasets, and yet low performance on another
similar, yet larger sentential RE dataset, Wikidata. As such, we identify noise
in Wikidata labels as one of the reasons that can hinder performance.
Additionally, we show associativity of better performance with better
re-representation, a term from neuroscience referred to change of
representation in human brain to improve the match at comparison time. As
example, in the given analogous terms King:Queen::Man:Woman, at comparison
time, and as a result of re-representation, the similarity between related head
terms (King,Man), and tail terms (Queen,Woman) increases. As such, our
observation show that our proposed model can do re-representation better than
the vanilla model compared with. To that end, beside noise in the labels of the
distantly supervised RE datasets, we propose re-representation as a challenge
in sentential RE.

</details>


### [145] [Enabling Equitable Access to Trustworthy Financial Reasoning](https://arxiv.org/abs/2508.21051)
*William Jurayj,Nils Holzenberger,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 本论文提出了一种将大型语言模型（LLM）与符号求解器结合的方法，用于准确、可审核地计算税务申报，提升自动化税务系统的表现。


<details>
  <summary>Details</summary>
Motivation: 税务申报复杂、规则重叠、计算繁琐且出错代价高，普通人需花大量时间和金钱。目前的LLM即使在文本推理表现优异，但准确性和可审核性难以满足税务场景需求，急需高效可靠的自动化解决方案。

Method: 提出了神经-符号混合系统，将LLM与符号求解器结合处理税务规则。设计了将普通文本规则翻译为形式化逻辑程序的方法，并选用智能检索的典型案例进行形式化推理。该系统在SARA数据集上进行性能评估，并引入了基于真实罚款成本的系统部署成本估算方法。

Result: 所提出方法在SARA挑战数据集上取得显著性能提升。形式化规则转换与案例检索策略大幅提升了系统性能，并使税务申报的平均时间和成本大幅低于现实中的平均水平。

Conclusion: 神经-符号架构可大幅提高税务自动化系统的准确率和可用性，降低普通人的时间和经济负担，具有提升税务服务普惠性的前景和经济可行性。

Abstract: According to the United States Internal Revenue Service, ''the average
American spends $\$270$ and 13 hours filing their taxes''. Even beyond the
U.S., tax filing requires complex reasoning, combining application of
overlapping rules with numerical calculations. Because errors can incur costly
penalties, any automated system must deliver high accuracy and auditability,
making modern large language models (LLMs) poorly suited for this task. We
propose an approach that integrates LLMs with a symbolic solver to calculate
tax obligations. We evaluate variants of this system on the challenging
StAtutory Reasoning Assessment (SARA) dataset, and include a novel method for
estimating the cost of deploying such a system based on real-world penalties
for tax errors. We further show how combining up-front translation of
plain-text rules into formal logic programs, combined with intelligently
retrieved exemplars for formal case representations, can dramatically improve
performance on this task and reduce costs to well below real-world averages.
Our results demonstrate the promise and economic feasibility of neuro-symbolic
architectures for increasing equitable access to reliable tax assistance.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [146] [Learning Fast, Tool aware Collision Avoidance for Collaborative Robots](https://arxiv.org/abs/2508.20457)
*Joonho Lee,Yunho Kim,Seokjoon Kim,Quan Nguyen,Youngjin Heo*

Main category: cs.RO

TL;DR: 本文提出了一种工具感知的协作机器人避碰系统，能实时适应不同工具尺寸和交互模式，在动态与部分可见环境下实现高效、安全的运动规划，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 在现实动态环境中，协作机器人面临障碍物和任务不断变化的挑战，现有方法对视野和工具假设不切实际，导致碰撞或过度保守，亟需更智能、高效的避碰策略。

Method: 本方法结合学习感知模型与受约束强化学习控制策略，对点云中机器人及工具部分进行分离、推断遮挡区域，并在部分可观测情况下预测碰撞风险，生成流畅、快速的避障动作。

Result: 在仿真和实际测试中，该系统优于APF和MPPI等传统方法，能在复杂环境下保持亚毫米级精度，并比先进GPU规划器降低约60%的计算成本。

Conclusion: 所提系统为动态环境下协作机器人实现安全、高效、模块化避障提供了有效方案，并已集成到实际应用，验证了其实用性和响应性。

Abstract: Ensuring safe and efficient operation of collaborative robots in human
environments is challenging, especially in dynamic settings where both obstacle
motion and tasks change over time. Current robot controllers typically assume
full visibility and fixed tools, which can lead to collisions or overly
conservative behavior. In our work, we introduce a tool-aware collision
avoidance system that adjusts in real time to different tool sizes and modes of
tool-environment interaction. Using a learned perception model, our system
filters out robot and tool components from the point cloud, reasons about
occluded area, and predicts collision under partial observability. We then use
a control policy trained via constrained reinforcement learning to produce
smooth avoidance maneuvers in under 10 milliseconds. In simulated and
real-world tests, our approach outperforms traditional approaches (APF, MPPI)
in dynamic environments, while maintaining sub-millimeter accuracy. Moreover,
our system operates with approximately 60% lower computational cost compared to
a state-of-the-art GPU-based planner. Our approach provides modular, efficient,
and effective collision avoidance for robots operating in dynamic environments.
We integrate our method into a collaborative robot application and demonstrate
its practical use for safe and responsive operation.

</details>


### [147] [SPGrasp: Spatiotemporal Prompt-driven Grasp Synthesis in Dynamic Scenes](https://arxiv.org/abs/2508.20547)
*Yunpeng Mei,Hongjie Cao,Yinqiu Xia,Wei Xiao,Zhaohan Feng,Gang Wang,Jie Chen*

Main category: cs.RO

TL;DR: SPGrasp是一种实时、基于提示的动态图像抓取合成框架，实现低延迟且高交互性的抓取，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有动态图像抓取方法难以兼顾低延迟和用户交互（promptability），导致实时应用受限。

Method: 提出SPGrasp框架，将用户提示与时空上下文信息结合，扩展SAMv2模型以适应视频流实时抓取推理，实现端到端最低59毫秒的响应速度，并保持时序一致性。

Result: SPGrasp在OCID和Jacquard数据集上抓取准确率分别为90.6%和93.8%；在GraspNet-1Billion下连续追踪，准确率达92.0%，单帧延迟73.1毫秒，比现有方法RoG-SAM延迟降低58.5%。实际实验中抓取成功率达到94.8%。

Conclusion: SPGrasp有效平衡了动态图像抓取的延迟与交互性，在动态物体实时抓取中显著领先，适用于实际场景。

Abstract: Real-time interactive grasp synthesis for dynamic objects remains challenging
as existing methods fail to achieve low-latency inference while maintaining
promptability. To bridge this gap, we propose SPGrasp (spatiotemporal
prompt-driven dynamic grasp synthesis), a novel framework extending segment
anything model v2 (SAMv2) for video stream grasp estimation. Our core
innovation integrates user prompts with spatiotemporal context, enabling
real-time interaction with end-to-end latency as low as 59 ms while ensuring
temporal consistency for dynamic objects. In benchmark evaluations, SPGrasp
achieves instance-level grasp accuracies of 90.6% on OCID and 93.8% on
Jacquard. On the challenging GraspNet-1Billion dataset under continuous
tracking, SPGrasp achieves 92.0% accuracy with 73.1 ms per-frame latency,
representing a 58.5% reduction compared to the prior state-of-the-art
promptable method RoG-SAM while maintaining competitive accuracy. Real-world
experiments involving 13 moving objects demonstrate a 94.8% success rate in
interactive grasping scenarios. These results confirm SPGrasp effectively
resolves the latency-interactivity trade-off in dynamic grasp synthesis. Code
is available at https://github.com/sejmoonwei/SPGrasp.

</details>


### [148] [SimShear: Sim-to-Real Shear-based Tactile Servoing](https://arxiv.org/abs/2508.20561)
*Kipp McAdam Freud,Yijiong Lin,Nathan F. Lepora*

Main category: cs.RO

TL;DR: 提出了SimShear管线，通过GAN模型弥补仿真触觉图像中缺失的剪切变形信息，实现更真实的仿真到真实领域迁移。有效提升了机器人基于剪切感知的控制任务表现。


<details>
  <summary>Details</summary>
Motivation: 剪切（shear）信息对动态物体交互中的触觉控制至关重要，但传统仿真很难准确模拟剪切动力学，导致难以直接用仿真训练实现剪切感知。

Method: 设计了一种名为shPix2pix的条件U-Net GAN模型，将不含剪切信息的仿真触觉图像和剪切向量，转换为包含真实剪切变形的触觉图像，然后应用于机器人触觉控制任务。该方法显著优于传统pix2pix方法。

Result: 方法在仿真触觉图像生成、姿态/剪切预测上的表现均优于基线。在两个机器人协作任务（跟踪与合托举）中，触觉控制时的接触误差能保持在1~2毫米，验证了方案的有效性。

Conclusion: 证明了无需直接建模复杂剪切动力学，仅利用GAN对仿真图像进行剪切信息增强即可实现高精度sim-to-real迁移，拓展了触觉仿真和机器人的新方向。

Abstract: We present SimShear, a sim-to-real pipeline for tactile control that enables
the use of shear information without explicitly modeling shear dynamics in
simulation. Shear, arising from lateral movements across contact surfaces, is
critical for tasks involving dynamic object interactions but remains
challenging to simulate. To address this, we introduce shPix2pix, a
shear-conditioned U-Net GAN that transforms simulated tactile images absent of
shear, together with a vector encoding shear information, into realistic
equivalents with shear deformations. This method outperforms baseline pix2pix
approaches in simulating tactile images and in pose/shear prediction. We apply
SimShear to two control tasks using a pair of low-cost desktop robotic arms
equipped with a vision-based tactile sensor: (i) a tactile tracking task, where
a follower arm tracks a surface moved by a leader arm, and (ii) a collaborative
co-lifting task, where both arms jointly hold an object while the leader
follows a prescribed trajectory. Our method maintains contact errors within 1
to 2 mm across varied trajectories where shear sensing is essential, validating
the feasibility of sim-to-real shear modeling with rigid-body simulators and
opening new directions for simulation in tactile robotics.

</details>


### [149] [Traversing the Narrow Path: A Two-Stage Reinforcement Learning Framework for Humanoid Beam Walking](https://arxiv.org/abs/2508.20661)
*TianChen Huang,Wei Gao,Runchen Xu,Shiwu Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种两阶段方法，使人形机器人能够安全地通过狭窄的横梁，包括基于物理的步态模板和残差优化器，并实现了优异的实机过梁表现。


<details>
  <summary>Details</summary>
Motivation: 由于狭窄梁上接触点稀疏且行动安全要求高，现有依赖纯学习策略方法易失效，缺乏稳健性，急需一个既具物理解释性又可实际部署的解决方案。

Method: 提出XCoM/LIPM步态模板与残差高层规划器相结合的两阶段框架。第一阶段在平地训练低层跟踪器以提高步态目标跟踪鲁棒性。第二阶段在模拟梁上训练高阶规划器，对摆动脚步伐微调以增强安全性与精度。方法保持传感需求低，便于真实机器人部署，并采用结构清晰的步态参数接口。

Result: 在Unitree G1机器人上的实验显示，本系统能稳定穿越0.2米宽、3米长的横梁；无论模拟还是现实测试中，残差细化策略在成功率、中心线贴合度和安全裕度上均优于基线方法。

Conclusion: 两阶段的步态模板与残差规划器相结合，不仅提升了人形机器人在窄梁上的稳定性和安全性，还通过结构化接口促进了算法分析与从仿真到现实的顺利迁移。

Abstract: Traversing narrow beams is challenging for humanoids due to sparse,
safety-critical contacts and the fragility of purely learned policies. We
propose a physically grounded, two-stage framework that couples an XCoM/LIPM
footstep template with a lightweight residual planner and a simple low-level
tracker. Stage-1 is trained on flat ground: the tracker learns to robustly
follow footstep targets by adding small random perturbations to heuristic
footsteps, without any hand-crafted centerline locking, so it acquires stable
contact scheduling and strong target-tracking robustness. Stage-2 is trained in
simulation on a beam: a high-level planner predicts a body-frame residual
(Delta x, Delta y, Delta psi) for the swing foot only, refining the template
step to prioritize safe, precise placement under narrow support while
preserving interpretability. To ease deployment, sensing is kept minimal and
consistent between simulation and hardware: the planner consumes compact,
forward-facing elevation cues together with onboard IMU and joint signals. On a
Unitree G1, our system reliably traverses a 0.2 m-wide, 3 m-long beam. Across
simulation and real-world studies, residual refinement consistently outperforms
template-only and monolithic baselines in success rate, centerline adherence,
and safety margins, while the structured footstep interface enables transparent
analysis and low-friction sim-to-real transfer.

</details>


### [150] [Task-Oriented Edge-Assisted Cross-System Design for Real-Time Human-Robot Interaction in Industrial Metaverse](https://arxiv.org/abs/2508.20664)
*Kan Chen,Zhen Meng,Xiangmin Xu,Jiaming Yang,Emma Li,Philip G. Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种基于边缘计算的数字孪生跨系统框架，通过预测人类操作意图，实现工业元宇宙中低延迟的人机交互，有效提升工业应用的空间精度和视觉表现。


<details>
  <summary>Details</summary>
Motivation: 工业元宇宙环境下的人机交互需要应对高计算负载、带宽受限与低延迟等多个挑战，传统方案无法兼顾实时性与精度。因此，本文旨在提出新的架构机制和算法，以提升系统适应性和交互质量。

Method: 框架借助数字孪生技术，将视觉显示与机器人控制两个虚拟功能解耦；利用预测操作者动作实现1）主动元宇宙渲染；2）远程设备的超前控制。提出HITL-MAML算法，实现预测窗口的动态调整，提升泛化能力。

Result: 实验验证了该框架在两个典型任务中的有效性：在轨迹绘制控制任务中，RMSE由0.0712m降至0.0101m；在真实场景3D重建任务中，实现PSNR 22.11，SSIM 0.8729，LPIPS 0.1298，均显示出高空间精度与视觉保真度。

Conclusion: 提出的任务导向边缘辅助跨系统框架及相应算法可切实提升工业元宇宙中的实时交互体验和系统可扩展性，特别适用于高风险、对精度要求严苛的工业场景。

Abstract: Real-time human-device interaction in industrial Metaverse faces challenges
such as high computational load, limited bandwidth, and strict latency. This
paper proposes a task-oriented edge-assisted cross-system framework using
digital twins (DTs) to enable responsive interactions. By predicting operator
motions, the system supports: 1) proactive Metaverse rendering for visual
feedback, and 2) preemptive control of remote devices. The DTs are decoupled
into two virtual functions-visual display and robotic control-optimizing both
performance and adaptability. To enhance generalizability, we introduce the
Human-In-The-Loop Model-Agnostic Meta-Learning (HITL-MAML) algorithm, which
dynamically adjusts prediction horizons. Evaluation on two tasks demonstrates
the framework's effectiveness: in a Trajectory-Based Drawing Control task, it
reduces weighted RMSE from 0.0712 m to 0.0101 m; in a real-time 3D scene
representation task for nuclear decommissioning, it achieves a PSNR of 22.11,
SSIM of 0.8729, and LPIPS of 0.1298. These results show the framework's
capability to ensure spatial precision and visual fidelity in real-time,
high-risk industrial environments.

</details>


### [151] [Task Allocation for Autonomous Machines using Computational Intelligence and Deep Reinforcement Learning](https://arxiv.org/abs/2508.20688)
*Thanh Thi Nguyen,Quoc Viet Hung Nguyen,Jonathan Kua,Imran Razzak,Dung Nguyen,Saeid Nahavandi*

Main category: cs.RO

TL;DR: 本文综述了多自主机器协同控制算法，尤其聚焦于基于计算智能（CI）和深度强化学习（RL）的任务分配方法，并分析其优缺点，指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着自主机器在复杂环境中应用的增长，如何高效协同控制和任务分配成为关键问题。现有方法面临动态性和不确定性的挑战，亟需更强大的算法支持。

Method: 本文着重于任务分配领域，系统梳理了利用计算智能和深度强化学习的主要控制与协同算法，评述了各自的优势和限制，并对未来研究方向进行了探讨。

Result: 研究表明，CI与深度RL为应对动态和不确定环境中的复杂任务分配问题提供了有效手段，近年深度RL的发展极大推动了该领域的进步。

Conclusion: CI与深度RL方法在自主机器控制与任务分配中显示出巨大潜力。未来研究应关注现有方法的改进、新技术的开发以及现实应用中的挑战。

Abstract: Enabling multiple autonomous machines to perform reliably requires the
development of efficient cooperative control algorithms. This paper presents a
survey of algorithms that have been developed for controlling and coordinating
autonomous machines in complex environments. We especially focus on task
allocation methods using computational intelligence (CI) and deep reinforcement
learning (RL). The advantages and disadvantages of the surveyed methods are
analysed thoroughly. We also propose and discuss in detail various future
research directions that shed light on how to improve existing algorithms or
create new methods to enhance the employability and performance of autonomous
machines in real-world applications. The findings indicate that CI and deep RL
methods provide viable approaches to addressing complex task allocation
problems in dynamic and uncertain environments. The recent development of deep
RL has greatly contributed to the literature on controlling and coordinating
autonomous machines, and it has become a growing trend in this area. It is
envisaged that this paper will provide researchers and engineers with a
comprehensive overview of progress in machine learning research related to
autonomous machines. It also highlights underexplored areas, identifies
emerging methodologies, and suggests new avenues for exploration in future
research within this domain.

</details>


### [152] [Non-expert to Expert Motion Translation Using Generative Adversarial Networks](https://arxiv.org/abs/2508.20740)
*Yuki Tanaka,Seiichiro Katsura*

Main category: cs.RO

TL;DR: 本文提出了一种基于生成对抗网络（GAN）的灵活动作转换方法，以实现专家技能向机器人转移，解决机器人任务切换和泛化受限的问题。


<details>
  <summary>Details</summary>
Motivation: 面对全球熟练工人数减少的问题，如何将专家的技术迁移到机器人身上变得尤为重要。目前模仿学习多数仅能转移位置与力量数据，且在任务切换与多任务泛化上存在明显的限制。

Method: 采用生成对抗网络（GAN）实现灵活的机器人动作转换，使机器人能够通过输入数据以及训练好的模型接受人类专家技能迁移，并且支持多种任务切换。系统在三自由度的书法机器人上进行了评估。

Result: 实验结果表明，所提出的方法可以支持用户通过输入不同的数据教机器人完成不同的任务，实现了较高的灵活性和任务泛化能力。

Conclusion: 生成对抗网络能够有效扩展机器人模仿学习方法，实现用户意图驱动下的灵活任务切换，为今后专家技能的机器人转移提供了有力的技术支持。

Abstract: Decreasing skilled workers is a very serious problem in the world. To deal
with this problem, the skill transfer from experts to robots has been
researched. These methods which teach robots by human motion are called
imitation learning. Experts' skills generally appear in not only position data,
but also force data. Thus, position and force data need to be saved and
reproduced. To realize this, a lot of research has been conducted in the
framework of a motion-copying system. Recent research uses machine learning
methods to generate motion commands. However, most of them could not change
tasks by following human intention. Some of them can change tasks by
conditional training, but the labels are limited. Thus, we propose the flexible
motion translation method by using Generative Adversarial Networks. The
proposed method enables users to teach robots tasks by inputting data, and
skills by a trained model. We evaluated the proposed system with a 3-DOF
calligraphy robot.

</details>


### [153] [Uncertainty Aware-Predictive Control Barrier Functions: Safer Human Robot Interaction through Probabilistic Motion Forecasting](https://arxiv.org/abs/2508.20812)
*Lorenzo Busellato,Federico Cunico,Diego Dall'Alba,Marco Emporio,Andrea Giachetti,Riccardo Muradore,Marco Cristani*

Main category: cs.RO

TL;DR: 本文提出了一种融合概率化人类手部运动预测和控制障碍函数(CBF)的框架UA-PCBF，以提升在协作机器人系统中的安全性与交互流畅性。实验显示，相较于现有方法，该方法能更好地平衡安全和效率。


<details>
  <summary>Details</summary>
Motivation: 当前协作机器人系统中的安全机制，常因过于保守导致频繁停车，影响效率和人机交互流畅性。与此同时，基于学习的人类运动预测虽进步明显，但普遍采用最坏情况假设，缺乏对预测不确定性的结构化处理，导致规划过度保守。

Method: 提出了UA-PCBFs框架，将概率化的人类手部运动预测模块与具备严格安全保证的控制障碍函数相结合。该框架基于对人手运动不确定性的估计，动态调整机器人安全裕度，实现更灵活及响应式的运动规划。

Result: 在真实环境下，使用机械手和人机交互进行实验。UA-PCBFs在保证安全的同时，在任务完成效率、反应速度、系统可用性和增强人类信心方面均优于现有协作机器人交互系统，极大减少了机器人安全空间的违规次数。

Conclusion: UA-PCBFs通过对人类动作不确定性的理解和利用，提升了人机协作机器人的智能与流畅性，在高效、人性化、且更安全的机器人交互和协作应用中展现出优异性能。

Abstract: To enable flexible, high-throughput automation in settings where people and
robots share workspaces, collaborative robotic cells must reconcile stringent
safety guarantees with the need for responsive and effective behavior. A
dynamic obstacle is the stochastic, task-dependent variability of human motion:
when robots fall back on purely reactive or worst-case envelopes, they brake
unnecessarily, stall task progress, and tamper with the fluidity that true
Human-Robot Interaction demands. In recent years, learning-based human-motion
prediction has rapidly advanced, although most approaches produce worst-case
scenario forecasts that often do not treat prediction uncertainty in a
well-structured way, resulting in over-conservative planning algorithms,
limiting their flexibility. We introduce Uncertainty-Aware Predictive Control
Barrier Functions (UA-PCBFs), a unified framework that fuses probabilistic
human hand motion forecasting with the formal safety guarantees of Control
Barrier Functions. In contrast to other variants, our framework allows for
dynamic adjustment of the safety margin thanks to the human motion uncertainty
estimation provided by a forecasting module. Thanks to uncertainty estimation,
UA-PCBFs empower collaborative robots with a deeper understanding of future
human states, facilitating more fluid and intelligent interactions through
informed motion planning. We validate UA-PCBFs through comprehensive real-world
experiments with an increasing level of realism, including automated setups (to
perform exactly repeatable motions) with a robotic hand and direct human-robot
interactions (to validate promptness, usability, and human confidence).
Relative to state-of-the-art HRI architectures, UA-PCBFs show better
performance in task-critical metrics, significantly reducing the number of
violations of the robot's safe space during interaction with respect to the
state-of-the-art.

</details>


### [154] [A Soft Fabric-Based Thermal Haptic Device for VR and Teleoperation](https://arxiv.org/abs/2508.20831)
*Rui Chen,Domenico Chiaradia,Antonio Frisoli,Daniele Leonardis*

Main category: cs.RO

TL;DR: 该论文提出了一种新型基于织物的热-触觉接口，集成气动驱动和加热元件，实现轻量、可穿戴的虚拟现实和遥操作触觉反馈。实验结果证明了其在温度识别和操作任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前虚拟现实和遥操作系统在触觉反馈方面存在不足，尤其难以集成动态的压力和热觉刺激。为提升沉浸感和操作精度，需研发新型、集成、高效且穿戴舒适的多模态触觉接口。

Method: 作者设计了一款每根手指仅重2g的织物热-触觉接口，将加热元件嵌入气动腔体，通过软质穿戴设备向手指提供气动压力和热刺激。系统进行了全面性能表征，包括加热速率、输出力、冷却效率的优化，并通过用户实验验证了实用性。

Result: 系统加热速率高达3°C/s，气动子系统可产生8.93N的力。通过结构优化提升了降温效率且牺牲的力很小。用户实验中，三档温度的识别准确率达0.98，并在虚拟抓取任务中操作成功率由88.5%提升至96.4%，力控制精度显著提升。

Conclusion: 该集成热-触觉接口在增强虚拟现实和遥操作的人机交互体验方面效果显著，具备应用前景。

Abstract: This paper presents a novel fabric-based thermal-haptic interface for virtual
reality and teleoperation. It integrates pneumatic actuation and conductive
fabric with an innovative ultra-lightweight design, achieving only 2~g for each
finger unit. By embedding heating elements within textile pneumatic chambers,
the system delivers modulated pressure and thermal stimuli to fingerpads
through a fully soft, wearable interface.
  Comprehensive characterization demonstrates rapid thermal modulation with
heating rates up to 3$^{\circ}$C/s, enabling dynamic thermal feedback for
virtual or teleoperation interactions. The pneumatic subsystem generates forces
up to 8.93~N at 50~kPa, while optimization of fingerpad-actuator clearance
enhances cooling efficiency with minimal force reduction. Experimental
validation conducted with two different user studies shows high temperature
identification accuracy (0.98 overall) across three thermal levels, and
significant manipulation improvements in a virtual pick-and-place tasks.
Results show enhanced success rates (88.5\% to 96.4\%, p = 0.029) and improved
force control precision (p = 0.013) when haptic feedback is enabled, validating
the effectiveness of the integrated thermal-haptic approach for advanced
human-machine interaction applications.

</details>


### [155] [Model-Free Hovering and Source Seeking via Extremum Seeking Control: Experimental Demonstration](https://arxiv.org/abs/2508.20836)
*Ahmed A. Elgohary,Rohan Palanikumar,Sameh A. Eisa*

Main category: cs.RO

TL;DR: 本论文首次在实验中验证了使用极值寻优控制（ESC）方法，能使扑翼机器人在无需模型的情况下实现实时悬停和寻源。


<details>
  <summary>Details</summary>
Motivation: 扑翼昆虫和蜂鸟等生物具备卓越的悬停与寻源能力，现有仿生机器人难以高效、实时、无需模型地实现类似行为，因此需要新的控制方法。

Method: 提出并实验验证了一种新颖的极值寻优控制（ESC）方法，将其应用于扑翼机器人系统，实现了无需建模即可进行实时控制。通过1维实验环境测试ESC在悬停及寻源上的表现。

Result: 实验结果显示，在1维条件下，ESC能够有效实现扑翼机器人的模型无关、实时悬停与寻源控制，初步验证了其可行性。

Conclusion: ESC是一种自然且有效的扑翼飞行生物仿生控制机制，有望推动仿生飞行机器人领域实现更高效的悬停和寻源能力。

Abstract: In a recent effort, we successfully proposed a categorically novel approach
to mimic the phenomenoa of hovering and source seeking by flapping insects and
hummingbirds using a new extremum seeking control (ESC) approach. Said ESC
approach was shown capable of characterizing the physics of hovering and source
seeking by flapping systems, providing at the same time uniquely novel
opportunity for a model-free, real-time biomimicry control design. In this
paper, we experimentally test and verify, for the first time in the literature,
the potential of ESC in flapping robots to achieve model-free, real-time
controlled hovering and source seeking. The results of this paper, while being
restricted to 1D, confirm the premise of introducing ESC as a natural control
method and biomimicry mechanism to the field of flapping flight and robotics.

</details>


### [156] [Learning Primitive Embodied World Models: Towards Scalable Robotic Learning](https://arxiv.org/abs/2508.20840)
*Qiao Sun,Liujia Yang,Wei Tang,Wei Huang,Kaixin Xu,Yongchao Chen,Mingyu Liu,Jiange Yang,Haoyi Zhu,Yating Wang,Tong He,Yilun Chen,Xili Dai,Nanyang Ye,Qinying Gu*

Main category: cs.RO

TL;DR: 本文提出了一种新范式——Primitive Embodied World Models（PEWM），通过生成短时序视频以实现细粒度的机器人动作和语言对齐，缓解高维度和数据稀缺问题，并能更高效地实现复杂任务的组合泛化。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成式具身世界模型受到数据稀缺与高维特性限制，难以实现细粒度的语言-动作对齐，并阻碍长时序视频的生成，影响生成模型在具身智能领域的突破。作者观察到，具身数据的多样性远超基础动作空间，为简化建模提供契机。

Method: PEWM将视频生成限定于短时段原语动作，通过模块化的视觉-语言模型规划器和起点-终点热力图引导机制，分别实现对原语策略的可组合泛化和端到端控制。框架结合了视频模型的时空先验与视觉-语言模型的语义理解。

Result: 该方法实现了更细粒度的语言与视觉动作对齐，简化了学习过程，提高了数据采集效率，减少了推理延迟，并能在更长、更复杂任务中泛化原语策略。

Conclusion: PEWM推进了可扩展、可解释且通用的具身智能系统，为高层推理与精细物理交互的桥接提供新思路，对具身世界建模具有重要意义。

Abstract: While video-generation-based embodied world models have gained increasing
attention, their reliance on large-scale embodied interaction data remains a
key bottleneck. The scarcity, difficulty of collection, and high dimensionality
of embodied data fundamentally limit the alignment granularity between language
and actions and exacerbate the challenge of long-horizon video
generation--hindering generative models from achieving a "GPT moment" in the
embodied domain. There is a naive observation: the diversity of embodied data
far exceeds the relatively small space of possible primitive motions. Based on
this insight, we propose a novel paradigm for world modeling--Primitive
Embodied World Models (PEWM). By restricting video generation to fixed short
horizons, our approach 1) enables fine-grained alignment between linguistic
concepts and visual representations of robotic actions, 2) reduces learning
complexity, 3) improves data efficiency in embodied data collection, and 4)
decreases inference latency. By equipping with a modular Vision-Language Model
(VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further
enables flexible closed-loop control and supports compositional generalization
of primitive-level policies over extended, complex tasks. Our framework
leverages the spatiotemporal vision priors in video models and the semantic
awareness of VLMs to bridge the gap between fine-grained physical interaction
and high-level reasoning, paving the way toward scalable, interpretable, and
general-purpose embodied intelligence.

</details>


### [157] [Genetic Informed Trees (GIT*): Path Planning via Reinforced Genetic Programming Heuristics](https://arxiv.org/abs/2508.20871)
*Liding Zhang,Kuanqi Cai,Zhenshan Bing,Chaoqun Wang,Alois Knoll*

Main category: cs.RO

TL;DR: 本文提出了一种新的最优路径规划方法Genetic Informed Trees (GIT*)，融合了丰富的环境数据和强化遗传编程（RGP），大幅提升了路径搜索效率和解的质量。


<details>
  <summary>Details</summary>
Motivation: 现有路径规划方法在启发函数设计上常忽略环境数据且结构较为简化，导致搜索效率和结果并不理想。复杂的信息关系使得充分利用环境信息变得困难。

Method: 提出GIT*方法，整合如障碍物斥力、顶点动态重要性等多种环境数据，细化启发函数的引导能力。同时引入强化遗传编程（RGP），将遗传编程与奖励反馈系统结合，优化启发函数。

Result: GIT*在包含R^4至R^16的多种问题和实际移动操作任务中均优于现有单查询、采样式规划器。提升了计算效率与解的质量。

Conclusion: GIT*方法通过丰富环境数据利用及RGP优化启发式函数，显著优于现有方法，可为复杂高维路径规划提供有效的新思路。

Abstract: Optimal path planning involves finding a feasible state sequence between a
start and a goal that optimizes an objective. This process relies on heuristic
functions to guide the search direction. While a robust function can improve
search efficiency and solution quality, current methods often overlook
available environmental data and simplify the function structure due to the
complexity of information relationships. This study introduces Genetic Informed
Trees (GIT*), which improves upon Effort Informed Trees (EIT*) by integrating a
wider array of environmental data, such as repulsive forces from obstacles and
the dynamic importance of vertices, to refine heuristic functions for better
guidance. Furthermore, we integrated reinforced genetic programming (RGP),
which combines genetic programming with reward system feedback to mutate
genotype-generative heuristic functions for GIT*. RGP leverages a multitude of
data types, thereby improving computational efficiency and solution quality
within a set timeframe. Comparative analyses demonstrate that GIT* surpasses
existing single-query, sampling-based planners in problems ranging from R^4 to
R^16 and was tested on a real-world mobile manipulation task. A video
showcasing our experimental results is available at
https://youtu.be/URjXbc_BiYg

</details>


### [158] [Deep Fuzzy Optimization for Batch-Size and Nearest Neighbors in Optimal Robot Motion Planning](https://arxiv.org/abs/2508.20884)
*Liding Zhang,Qiyang Zong,Yu Zhang,Zhenshan Bing,Alois Knoll*

Main category: cs.RO

TL;DR: LIT*是一种结合深度模糊神经网络思想的采样型运动规划方法，能够根据环境动态调整关键参数，实现了更高效的路径规划，其在高维空间和实际任务中性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的采样型运动规划方法在批大小和邻居选择参数等方面往往不具备环境自适应能力，导致在不同障碍物分布下规划效率和路径质量不稳定。本工作希望通过引入学习机制提升算法对环境的适应性，优化路径质量和算法效率。

Method: 提出了一种基于深度模糊学习的采样型规划器LIT*，它动态学习并调整采样批大小和最近邻参数，通过分析全局和局部的有效/无效状态比例，区分障碍稀疏和密集区域，从而优化参数设置。

Result: 在高维空间（如R^8到R^14）和双臂机器人操作任务测试中，LIT*表现出更快的收敛速度和更优的解质量，超过了现有最优的单次查询采样规划器。

Conclusion: LIT*通过深度模糊学习方法显著提升了采样型规划器的环境自适应能力，在复杂高维任务下有效提升了规划效率和路径质量。

Abstract: Efficient motion planning algorithms are essential in robotics. Optimizing
essential parameters, such as batch size and nearest neighbor selection in
sampling-based methods, can enhance performance in the planning process.
However, existing approaches often lack environmental adaptability. Inspired by
the method of the deep fuzzy neural networks, this work introduces
Learning-based Informed Trees (LIT*), a sampling-based deep fuzzy
learning-based planner that dynamically adjusts batch size and nearest neighbor
parameters to obstacle distributions in the configuration spaces. By encoding
both global and local ratios via valid and invalid states, LIT* differentiates
between obstacle-sparse and obstacle-dense regions, leading to lower-cost paths
and reduced computation time. Experimental results in high-dimensional spaces
demonstrate that LIT* achieves faster convergence and improved solution
quality. It outperforms state-of-the-art single-query, sampling-based planners
in environments ranging from R^8 to R^14 and is successfully validated on a
dual-arm robot manipulation task. A video showcasing our experimental results
is available at: https://youtu.be/NrNs9zebWWk

</details>


### [159] [CoCoL: A Communication Efficient Decentralized Collaborative Method for Multi-Robot Systems](https://arxiv.org/abs/2508.20898)
*Jiaxi Huang,Yan Huang,Yixian Zhao,Wenchao Meng,Jinming Xu*

Main category: cs.RO

TL;DR: 本文提出了一种高效通信的去中心化多机器人协作学习方法CoCoL，在保证精度的同时大幅减少多机器人系统的通信轮数与带宽消耗，特别适用于数据异质、流式与动态网络环境。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统在协作学习时，因通信量大与本地数据异质性强，面临效率与适应性挑战。现有方法难以兼顾通信效率和对异质性的鲁棒性。

Method: 作者提出CoCoL算法，采用镜像下降框架，利用近似Newton更新捕获各机器人目标函数之间的相似性，通过求解不精确子问题降低计算开销。引入梯度跟踪机制以提升对数据异质性的鲁棒性。

Result: 在三类典型多机器人协作学习任务上，CoCoL能大幅降低通信轮数和带宽消耗，同时维持或超越现有方法的精度。对于非IID数据、流数据及时变网络情景表现尤为突出。

Conclusion: CoCoL为多机器人系统协作学习提供了高效、鲁棒的解决方案，特别适用于现实中数据分布异质、通信受限及网络动态变化的复杂环境。

Abstract: Collaborative learning enhances the performance and adaptability of
multi-robot systems in complex tasks but faces significant challenges due to
high communication overhead and data heterogeneity inherent in multi-robot
tasks. To this end, we propose CoCoL, a Communication efficient decentralized
Collaborative Learning method tailored for multi-robot systems with
heterogeneous local datasets. Leveraging a mirror descent framework, CoCoL
achieves remarkable communication efficiency with approximate Newton-type
updates by capturing the similarity between objective functions of robots, and
reduces computational costs through inexact sub-problem solutions. Furthermore,
the integration of a gradient tracking scheme ensures its robustness against
data heterogeneity. Experimental results on three representative multi robot
collaborative learning tasks show the superiority of the proposed CoCoL in
significantly reducing both the number of communication rounds and total
bandwidth consumption while maintaining state-of-the-art accuracy. These
benefits are particularly evident in challenging scenarios involving non-IID
(non-independent and identically distributed) data distribution, streaming
data, and time-varying network topologies.

</details>


### [160] [Language-Enhanced Mobile Manipulation for Efficient Object Search in Indoor Environments](https://arxiv.org/abs/2508.20899)
*Liding Zhang,Zeqi Li,Kuanqi Cai,Qian Huang,Zhenshan Bing,Alois Knoll*

Main category: cs.RO

TL;DR: 本文提出了一种结合大语言模型和层级导航的新方法（GODHS），提升机器人在陌生、复杂环境下寻找目标物体的效率，在仿真实验中表现优越。


<details>
  <summary>Details</summary>
Motivation: 传统场景表示仅关注静态语义，缺乏对环境上下文的可解释推理能力，难以指导机器人在全新环境中有效搜寻目标物体，因此急需提升语义和推理能力引导导航。

Method: 提出了一种“基于目标动态启发引导的层级搜索”（GODHS）框架，将大语言模型用于推断场景语义和多层决策引导，通过结构化提示和逻辑约束增强推理可信性，并设计了结合极角排序与距离优先的启发式移动路径规划器，应对移动操作机器人需求。

Result: 在Isaac Sim仿真环境中，GODHS方法能以更高效率找到目标物体，相较于传统非语义搜索策略表现更佳。

Conclusion: 融合语义感知、大语言模型推理和层级导航能显著提升机器人在未知复杂环境中的目标搜索效率，展示了新型语言增强导航框架的可行性和优势。

Abstract: Enabling robots to efficiently search for and identify objects in complex,
unstructured environments is critical for diverse applications ranging from
household assistance to industrial automation. However, traditional scene
representations typically capture only static semantics and lack interpretable
contextual reasoning, limiting their ability to guide object search in
completely unfamiliar settings. To address this challenge, we propose a
language-enhanced hierarchical navigation framework that tightly integrates
semantic perception and spatial reasoning. Our method, Goal-Oriented
Dynamically Heuristic-Guided Hierarchical Search (GODHS), leverages large
language models (LLMs) to infer scene semantics and guide the search process
through a multi-level decision hierarchy. Reliability in reasoning is achieved
through the use of structured prompts and logical constraints applied at each
stage of the hierarchy. For the specific challenges of mobile manipulation, we
introduce a heuristic-based motion planner that combines polar angle sorting
with distance prioritization to efficiently generate exploration paths.
Comprehensive evaluations in Isaac Sim demonstrate the feasibility of our
framework, showing that GODHS can locate target objects with higher search
efficiency compared to conventional, non-semantic search strategies. Website
and Video are available at: https://drapandiger.github.io/GODHS

</details>


### [161] [PLUME: Procedural Layer Underground Modeling Engine](https://arxiv.org/abs/2508.20926)
*Gabriel Manuel Garcia,Antoine Richard,Miguel Olivares-Mendez*

Main category: cs.RO

TL;DR: 本文介绍了PLUME，一个用于生成地下三维环境的程序化生成框架，能为空间探索相关的AI训练与机器人算法测试提供支持。PLUME开源，适合多领域应用。


<details>
  <summary>Details</summary>
Motivation: 随着太空探索的推进，地下环境因其遮蔽、安全及科学价值受到关注。地球上的地下环境资源有限并难以获取，且难以模拟太阳系各地的多样地下环境，迫切需要高效的模拟与测试平台。

Method: 作者开发了PLUME程序化生成框架，通过高度灵活的架构，可以持续增强并扩展地下环境的各项特征。PLUME生成的3D场景能与机器人仿真器结合，支持AI训练、算法评估和渲染工作。

Result: 实验表明，PLUME能无缝对接机器人仿真系统，实现3D地下场景快速生成及相关算法的测试评估。

Conclusion: PLUME为太空地下探索及相关技术研发提供了有力工具，其开放源码可促进科研和工业界在相关领域的进一步研究和应用。

Abstract: As space exploration advances, underground environments are becoming
increasingly attractive due to their potential to provide shelter, easier
access to resources, and enhanced scientific opportunities. Although such
environments exist on Earth, they are often not easily accessible and do not
accurately represent the diversity of underground environments found throughout
the solar system. This paper presents PLUME, a procedural generation framework
aimed at easily creating 3D underground environments. Its flexible structure
allows for the continuous enhancement of various underground features, aligning
with our expanding understanding of the solar system. The environments
generated using PLUME can be used for AI training, evaluating robotics
algorithms, 3D rendering, and facilitating rapid iteration on developed
exploration algorithms. In this paper, it is demonstrated that PLUME has been
used along with a robotic simulator. PLUME is open source and has been released
on Github. https://github.com/Gabryss/P.L.U.M.E

</details>


### [162] [Scaling Fabric-Based Piezoresistive Sensor Arrays for Whole-Body Tactile Sensing](https://arxiv.org/abs/2508.20959)
*Curtis C. Johnson,Daniel Webb,David Hill,Marc D. Killpack*

Main category: cs.RO

TL;DR: 本文提出了一种完整的硬件与系统架构，实现了大规模、低串扰、实时更新的织物式触觉传感阵列，适用于全身机器人操作，为后续人机交互及高阶控制研究打下基础。


<details>
  <summary>Details</summary>
Motivation: 目前大规模触觉传感面临布线复杂、数据吞吐瓶颈及系统可靠性差等挑战，限制了机器人全身触觉操控的发展。作者试图解决这些瓶颈，使得大面积高密度触觉数据更易获取与实时处理。

Method: 作者结合开源织物传感器与定制读取电子学，通过硬件降低信号串扰，并设计了创新的串联SPI总线结构，简化布线、提升可靠性，替代传统无线或复杂USB集线器方案。该系统支持8000余个触点1平米范围下的50FPS以上数据流同步采集。

Result: 在全身抓持实验中，若无触觉回馈，机器人易加压损坏物体；引入该系统的实时触觉回馈后，机器人可实现温和且稳定抓握，有效避免对纸箱的破坏。

Conclusion: 该系统架构性能优秀、鲁棒且易于扩展，为后续高级机器人全身控制和人机物理交互研究提供了理想平台。

Abstract: Scaling tactile sensing for robust whole-body manipulation is a significant
challenge, often limited by wiring complexity, data throughput, and system
reliability. This paper presents a complete architecture designed to overcome
these barriers. Our approach pairs open-source, fabric-based sensors with
custom readout electronics that reduce signal crosstalk to less than 3.3%
through hardware-based mitigation. Critically, we introduce a novel,
daisy-chained SPI bus topology that avoids the practical limitations of common
wireless protocols and the prohibitive wiring complexity of USB hub-based
systems. This architecture streams synchronized data from over 8,000 taxels
across 1 square meter of sensing area at update rates exceeding 50 FPS,
confirming its suitability for real-time control. We validate the system's
efficacy in a whole-body grasping task where, without feedback, the robot's
open-loop trajectory results in an uncontrolled application of force that
slowly crushes a deformable cardboard box. With real-time tactile feedback, the
robot transforms this motion into a gentle, stable grasp, successfully
manipulating the object without causing structural damage. This work provides a
robust and well-characterized platform to enable future research in advanced
whole-body control and physical human-robot interaction.

</details>


### [163] [ActLoc: Learning to Localize on the Move via Active Viewpoint Selection](https://arxiv.org/abs/2508.20981)
*Jiajie Li,Boyang Sun,Luca Di Giammarino,Hermann Blum,Marc Pollefeys*

Main category: cs.RO

TL;DR: 论文提出了ActLoc，一种主动视角感知的机器人导航定位方案，通过选择最优摄像头朝向提升定位可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有定位系统普遍假设所有朝向均等，但在实际场景中，观测到未知、模糊或无效区域时，定位准确性不高，需要提升不同视角下的定位能力。

Method: ActLoc框架基于大规模训练的attention-based模型，针对每个3D点预测各个朝向下的定位精度，并将这些分布应用到轨迹规划中，使机器人主动选择有利于定位的视角。

Result: ActLoc在单一视角选择上取得了最优效果，并且在完整路径规划任务中表现良好，优于现有方法。

Conclusion: ActLoc方法通用且模块化，可直接应用于多种机器人导航与巡检任务，在提升定位鲁棒性方面具有明显优势。

Abstract: Reliable localization is critical for robot navigation, yet most existing
systems implicitly assume that all viewing directions at a location are equally
informative. In practice, localization becomes unreliable when the robot
observes unmapped, ambiguous, or uninformative regions. To address this, we
present ActLoc, an active viewpoint-aware planning framework for enhancing
localization accuracy for general robot navigation tasks. At its core, ActLoc
employs a largescale trained attention-based model for viewpoint selection. The
model encodes a metric map and the camera poses used during map construction,
and predicts localization accuracy across yaw and pitch directions at arbitrary
3D locations. These per-point accuracy distributions are incorporated into a
path planner, enabling the robot to actively select camera orientations that
maximize localization robustness while respecting task and motion constraints.
ActLoc achieves stateof-the-art results on single-viewpoint selection and
generalizes effectively to fulltrajectory planning. Its modular design makes it
readily applicable to diverse robot navigation and inspection tasks.

</details>


### [164] [UltraTac: Integrated Ultrasound-Augmented Visuotactile Sensor for Enhanced Robotic Perception](https://arxiv.org/abs/2508.20982)
*Junhao Gong,Kit-Wa Sou,Shoujie Li,Changqing Guo,Yan Huang,Chuqiao Lyu,Ziwu Song,Wenbo Ding*

Main category: cs.RO

TL;DR: 本文提出了一款名为UltraTac的集成传感器，将视觉触觉传感与超声波传感结合，既能获取高分辨率触觉信息，又能感知物体的材质，提升了机器人操作的感知能力。


<details>
  <summary>Details</summary>
Motivation: 传统视觉触觉传感器虽然能提供丰富的触觉图像信息，但无法直接感知物体材料等特性，限制了机器人对环境的感知能力。该工作旨在通过集成超声波传感技术，提升传感器对材料等物理属性的识别能力，补足原有的不足。

Method: 作者设计了一种同轴光声结构，将超声波传感模块集成到传统视觉触觉传感器中，并针对结构进行了声学匹配，使两种传感方式能共享同一区域、互不干扰。同时根据触觉反馈，动态调节超声模块的工作状态以实现功能协同。开展的系统实验包括距离感知、材料分类以及纹理-材料双模识别等。

Result: 实验表明，该传感器可以实现3-8 cm距离范围内的接近感知（$R^2=0.90$），材料分类平均准确率达99.20%，在15类识别的双模（纹理与材质）任务中准确率为92.11%。应用于机器人操作系统时，可同时检测物体表面纹理和内部材料，展现了实际应用潜力。

Conclusion: UltraTac传感器实现了高效集成视觉触觉与超声波感知，显著提升了机器人对于物体材料和状态的综合感知能力，适合于精细操作和复杂的人机交互等应用场景。

Abstract: Visuotactile sensors provide high-resolution tactile information but are
incapable of perceiving the material features of objects. We present UltraTac,
an integrated sensor that combines visuotactile imaging with ultrasound sensing
through a coaxial optoacoustic architecture. The design shares structural
components and achieves consistent sensing regions for both modalities.
Additionally, we incorporate acoustic matching into the traditional
visuotactile sensor structure, enabling integration of the ultrasound sensing
modality without compromising visuotactile performance. Through tactile
feedback, we dynamically adjust the operating state of the ultrasound module to
achieve flexible functional coordination. Systematic experiments demonstrate
three key capabilities: proximity sensing in the 3-8 cm range ($R^2=0.90$),
material classification (average accuracy: 99.20%), and texture-material
dual-mode object recognition achieving 92.11% accuracy on a 15-class task.
Finally, we integrate the sensor into a robotic manipulation system to
concurrently detect container surface patterns and internal content, which
verifies its potential for advanced human-machine interaction and precise
robotic manipulation.

</details>


### [165] [Rapid Mismatch Estimation via Neural Network Informed Variational Inference](https://arxiv.org/abs/2508.21007)
*Mateusz Jaszczuk,Nadia Figueroa*

Main category: cs.RO

TL;DR: 本文提出了一种名为Rapid Mismatch Estimation (RME)的方法，可在线快速估计机器人末端执行器动力学模型的不匹配问题，无需外部力-力矩传感器，能够实现对物理接触环境中动力学变化的快速、可靠适应。


<details>
  <summary>Details</summary>
Motivation: 现有机器人在与人及环境的物理交互中，常因动力学模型不准确造成安全隐患及任务失败。因此需要一种无需外部传感器、可实时地检测和适应动力学变化的新方法，以提升机器人在实际应用中的安全性和适应性。

Method: RME为一种自适应、与控制器无关的概率框架：利用机器人的本体反馈信号，经由神经网络生成模型不匹配的先验信息，之后通过变分推理快速收敛到未知动力学参数并对不确定性进行量化。实验使用7自由度机械臂和先进的被动阻抗控制器，在末端执行器负载质量和质心突变时，能在约400毫秒内完成在线自适应调整。

Result: RME方法在实际协作任务中被验证——人在不知情情况下把一个篮子加到机械臂末端，并动态地添加/移除重物。RME无需外部感测设备即可实现动力学变化的快速检测和安全适应，保证任务执行的稳定和安全。

Conclusion: RME极大提升了力度控制机器人应对动力学模型不确定性的能力，有助于机器人在与人协作及复杂环境中实现更安全、快速和高效的物理交互，适用于多种阻抗控制应用场景。

Abstract: With robots increasingly operating in human-centric environments, ensuring
soft and safe physical interactions, whether with humans, surroundings, or
other machines, is essential. While compliant hardware can facilitate such
interactions, this work focuses on impedance controllers that allow
torque-controlled robots to safely and passively respond to contact while
accurately executing tasks. From inverse dynamics to quadratic
programming-based controllers, the effectiveness of these methods relies on
accurate dynamics models of the robot and the object it manipulates. Any model
mismatch results in task failures and unsafe behaviors. Thus, we introduce
Rapid Mismatch Estimation (RME), an adaptive, controller-agnostic,
probabilistic framework that estimates end-effector dynamics mismatches online,
without relying on external force-torque sensors. From the robot's
proprioceptive feedback, a Neural Network Model Mismatch Estimator generates a
prior for a Variational Inference solver, which rapidly converges to the
unknown parameters while quantifying uncertainty. With a real 7-DoF manipulator
driven by a state-of-the-art passive impedance controller, RME adapts to sudden
changes in mass and center of mass at the end-effector in $\sim400$ ms, in
static and dynamic settings. We demonstrate RME in a collaborative scenario
where a human attaches an unknown basket to the robot's end-effector and
dynamically adds/removes heavy items, showcasing fast and safe adaptation to
changing dynamics during physical interaction without any external sensory
system.

</details>


### [166] [HITTER: A HumanoId Table TEnnis Robot via Hierarchical Planning and Learning](https://arxiv.org/abs/2508.21043)
*Zhi Su,Bike Zhang,Nima Rahmanian,Yuman Gao,Qiayuan Liao,Caitlin Regan,Koushil Sreenath,S. Shankar Sastry*

Main category: cs.RO

TL;DR: 本论文提出了一种用于人形机器人打乒乓球的分层控制框架，并在实际比赛中取得了良好表现，实现了机器人与人类间的高连击数和快速反应。


<details>
  <summary>Details</summary>
Motivation: 虽然近年来人形机器人在行走及全身控制方面取得了显著进步，但在需要快速感知和动态操作的任务（如乒乓球）上仍受限，因这些任务对反应速度、预测能力和动作协调性有极高要求。现有方法难以实现真正的人机互动和高效灵活的操控能力。

Method: 本文提出的分层框架包括基于模型的规划器用于预测球的轨迹和制定球拍运动计划，以及基于强化学习的全身控制器实现仿人连续击球动作和身体平衡，在训练阶段融入人类运动示例以鼓励自然动作。

Result: 在通用人形机器人上实验，机器人与人类对打最多可实现106次连续回合，并能与另一机器人稳定进行多次回合，显示出极佳的速度与稳定性。

Conclusion: 所提方法实现了人形机器人在真实环境中的乒乓球技能，能以亚秒级响应与人和机器人进行高效互动，推动了人形机器人向更敏捷和互动的行为迈进。

Abstract: Humanoid robots have recently achieved impressive progress in locomotion and
whole-body control, yet they remain constrained in tasks that demand rapid
interaction with dynamic environments through manipulation. Table tennis
exemplifies such a challenge: with ball speeds exceeding 5 m/s, players must
perceive, predict, and act within sub-second reaction times, requiring both
agility and precision. To address this, we present a hierarchical framework for
humanoid table tennis that integrates a model-based planner for ball trajectory
prediction and racket target planning with a reinforcement learning-based
whole-body controller. The planner determines striking position, velocity and
timing, while the controller generates coordinated arm and leg motions that
mimic human strikes and maintain stability and agility across consecutive
rallies. Moreover, to encourage natural movements, human motion references are
incorporated during training. We validate our system on a general-purpose
humanoid robot, achieving up to 106 consecutive shots with a human opponent and
sustained exchanges against another humanoid. These results demonstrate
real-world humanoid table tennis with sub-second reactive control, marking a
step toward agile and interactive humanoid behaviors.

</details>


### [167] [Prompt-to-Product: Generative Assembly via Bimanual Manipulation](https://arxiv.org/abs/2508.21063)
*Ruixuan Liu,Philip Huang,Ava Pun,Kangle Deng,Shobhit Aggarwal,Kevin Tang,Michelle Liu,Deva Ramanan,Jun-Yan Zhu,Jiaoyang Li,Changliu Liu*

Main category: cs.RO

TL;DR: 提出了Prompt-to-Product系统，实现了从自然语言描述到现实组装产品的自动化构建，显著降低了组装类产品的设计和制造门槛。


<details>
  <summary>Details</summary>
Motivation: 组装类产品（如乐高积木）设计和制作过程需要大量人工和专门知识，导致一般用户难以将想法转化为实际产品。

Method: 提出了Prompt-to-Product自动化流程：用户用自然语言输入组装需求，系统自动生成可实际搭建的乐高设计，并用双臂机器人完成实体组装。还进行了全面的用户研究以验证系统有效性。

Result: 用户研究表明，该系统能大幅降低组装类产品从创意到成品的门槛和人工操作量。

Conclusion: Prompt-to-Product极大简化了用户将想象转化为实际组装产品的流程，有效推动了创意到现实的转变。

Abstract: Creating assembly products demands significant manual effort and expert
knowledge in 1) designing the assembly and 2) constructing the product. This
paper introduces Prompt-to-Product, an automated pipeline that generates
real-world assembly products from natural language prompts. Specifically, we
leverage LEGO bricks as the assembly platform and automate the process of
creating brick assembly structures. Given the user design requirements,
Prompt-to-Product generates physically buildable brick designs, and then
leverages a bimanual robotic system to construct the real assembly products,
bringing user imaginations into the real world. We conduct a comprehensive user
study, and the results demonstrate that Prompt-to-Product significantly lowers
the barrier and reduces manual effort in creating assembly products from
imaginative ideas.

</details>


### [168] [Learning on the Fly: Rapid Policy Adaptation via Differentiable Simulation](https://arxiv.org/abs/2508.21065)
*Jiahe Pan,Jiaxu Xing,Rudolf Reiter,Yifan Zhai,Elie Aljalbout,Davide Scaramuzza*

Main category: cs.RO

TL;DR: 该论文提出了一种新颖的在线自适应学习方法，用于在真实环境中快速适应和修正机器人控制策略，从而有效解决模拟到现实（sim-to-real）迁移的难题。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人控制策略主要在仿真中开发，虽然高效且安全，但实际部署时会因未建模的动态变化和环境扰动性能下降。传统方法如domain randomization效果有限或需高昂重训练成本，亟需新的在线适应方法来提升策略的现实鲁棒性和泛化能力。

Method: 提出了一种融合残差动力学学习和实时策略适应的在线自适应学习框架。其核心为在可微分仿真器中，利用实时收集的真实世界数据不断精化动力学模型，通过梯度反向传播对策略进行快速、样本高效的在线更新，从而应对扰动和模型不精确问题。

Result: 在敏捷四旋翼飞控任务中（包含多种仿真及真实扰动环境），本方法可在5秒内完成适应训练，悬停误差相较L1-MPC和DATT算法分别降低81%和55%。此外，在基于视觉的无需状态估计的控制任务中也展现出优越的鲁棒性。

Conclusion: 该方法极大提高了机器人从仿真到现实策略迁移的效率与鲁棒性，为实时高效的智能体自适应控制提供了有力方案，尤其适合动态环境和存在未知干扰的实际应用。

Abstract: Learning control policies in simulation enables rapid, safe, and
cost-effective development of advanced robotic capabilities. However,
transferring these policies to the real world remains difficult due to the
sim-to-real gap, where unmodeled dynamics and environmental disturbances can
degrade policy performance. Existing approaches, such as domain randomization
and Real2Sim2Real pipelines, can improve policy robustness, but either struggle
under out-of-distribution conditions or require costly offline retraining. In
this work, we approach these problems from a different perspective. Instead of
relying on diverse training conditions before deployment, we focus on rapidly
adapting the learned policy in the real world in an online fashion. To achieve
this, we propose a novel online adaptive learning framework that unifies
residual dynamics learning with real-time policy adaptation inside a
differentiable simulation. Starting from a simple dynamics model, our framework
refines the model continuously with real-world data to capture unmodeled
effects and disturbances such as payload changes and wind. The refined dynamics
model is embedded in a differentiable simulation framework, enabling gradient
backpropagation through the dynamics and thus rapid, sample-efficient policy
updates beyond the reach of classical RL methods like PPO. All components of
our system are designed for rapid adaptation, enabling the policy to adjust to
unseen disturbances within 5 seconds of training. We validate the approach on
agile quadrotor control under various disturbances in both simulation and the
real world. Our framework reduces hovering error by up to 81% compared to
L1-MPC and 55% compared to DATT, while also demonstrating robustness in
vision-based control without explicit state estimation.

</details>
