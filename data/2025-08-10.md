<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 101]
- [cs.CL](#cs.CL) [Total: 41]
- [cs.RO](#cs.RO) [Total: 28]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [RetinexDual: Retinex-based Dual Nature Approach for Generalized Ultra-High-Definition Image Restoration](https://arxiv.org/abs/2508.04797)
*Mohab Kishawy,Ali Abdellatif Hussein,Jun Chen*

Main category: cs.CV

TL;DR: RetinexDual是一个基于Retinex理论的新型超高清图像恢复框架，通过两个互补子网络提升液滴、去模糊、去雾和低照度增强等任务的表现，在结构和细节还原上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统超高清图像恢复方法如极端下采样和纯频域变换各有严重缺陷，下采样导致不可逆信息损失，频域处理对局部退化无效。因此需要一种兼顾空间和频率信息的方法。

Method: 提出RetinexDual框架，包含两个子网络：1）SAMBA用于纠正反射分量，基于粗到细机制降低伪影并恢复细节；2）FIA在频域校正颜色与光照失真，利用全局上下文信息。两个分支协同工作，分别针对细节与色彩校正。

Result: RetinexDual在超高清图像去雨、去模糊、去雾和低照度增强四项任务上，定量和定性都优于先进方法。消融实验显示两分支独立设计及其组件有效。

Conclusion: RetinexDual通过空间和频域信息互补，解决了传统方法信息损失和局部校正不足的问题，在多类图像增强任务中表现突出。

Abstract: Advancements in image sensing have elevated the importance of
Ultra-High-Definition Image Restoration (UHD IR). Traditional methods, such as
extreme downsampling or transformation from the spatial to the frequency
domain, encounter significant drawbacks: downsampling induces irreversible
information loss in UHD images, while our frequency analysis reveals that pure
frequency-domain approaches are ineffective for spatially confined image
artifacts, primarily due to the loss of degradation locality. To overcome these
limitations, we present RetinexDual, a novel Retinex theory-based framework
designed for generalized UHD IR tasks. RetinexDual leverages two complementary
sub-networks: the Scale-Attentive maMBA (SAMBA) and the Frequency Illumination
Adaptor (FIA). SAMBA, responsible for correcting the reflectance component,
utilizes a coarse-to-fine mechanism to overcome the causal modeling of mamba,
which effectively reduces artifacts and restores intricate details. On the
other hand, FIA ensures precise correction of color and illumination
distortions by operating in the frequency domain and leveraging the global
context provided by it. Evaluating RetinexDual on four UHD IR tasks, namely
deraining, deblurring, dehazing, and Low-Light Image Enhancement (LLIE), shows
that it outperforms recent methods qualitatively and quantitatively. Ablation
studies demonstrate the importance of employing distinct designs for each
branch in RetinexDual, as well as the effectiveness of its various components.

</details>


### [2] [ACM Multimedia Grand Challenge on ENT Endoscopy Analysis](https://arxiv.org/abs/2508.04801)
*Trong-Thuan Nguyen,Viet-Tham Huynh,Thao Thi Phuong Dao,Ha Nguyen Thi,Tien To Vu Thuy,Uyen Hanh Tran,Tam V. Nguyen,Thanh Dinh Le,Minh-Triet Tran*

Main category: cs.CV

TL;DR: 本文提出了ENTRep数据集和基准，用于提升耳鼻喉（ENT）内镜图像分析自动化，结合了分类与检索能力，并支持英越双语临床描述。


<details>
  <summary>Details</summary>
Motivation: 当前ENT内镜图像自动分析发展缓慢，主要受限于图像采集和操作者的差异性、病变表现细微且局部、存在声带侧别及状态等细粒度识别难点。此外，临床医生除了需要自动分类，还需能够检索类似病例，包括图像和简洁文本描述，但现有公开基准对此支持较少。

Method: 作者构建了ENTRep数据集，收集并标注了专家认证的ENT内镜图像，标明解剖部位及正常/异常状态，同时配有英越双语文字描述。并设立了三个基准任务，通过标准化提交流程和服务端评分，确保评测统一性。

Result: 基于ENTRep基准，作者公布了不同队伍在三个任务上的表现，并对顶尖团队的成果进行了分析和讨论。

Conclusion: ENTRep数据集和基准填补了ENT内镜图像分析领域的空白，为学界和产业提供了标准化的开放平台，促进ENT图像分类与检索算法的发展与比较，特别是在多语言和细粒度任务方面具有推动作用。

Abstract: Automated analysis of endoscopic imagery is a critical yet underdeveloped
component of ENT (ear, nose, and throat) care, hindered by variability in
devices and operators, subtle and localized findings, and fine-grained
distinctions such as laterality and vocal-fold state. In addition to
classification, clinicians require reliable retrieval of similar cases, both
visually and through concise textual descriptions. These capabilities are
rarely supported by existing public benchmarks. To this end, we introduce
ENTRep, the ACM Multimedia 2025 Grand Challenge on ENT endoscopy analysis,
which integrates fine-grained anatomical classification with image-to-image and
text-to-image retrieval under bilingual (Vietnamese and English) clinical
supervision. Specifically, the dataset comprises expert-annotated images,
labeled for anatomical region and normal or abnormal status, and accompanied by
dual-language narrative descriptions. In addition, we define three benchmark
tasks, standardize the submission protocol, and evaluate performance on public
and private test splits using server-side scoring. Moreover, we report results
from the top-performing teams and provide an insight discussion.

</details>


### [3] [CoMAD: A Multiple-Teacher Self-Supervised Distillation Framework](https://arxiv.org/abs/2508.04816)
*Sriram Mandalika,Lalitha V*

Main category: cs.CV

TL;DR: 提出了一种新的知识蒸馏方法CoMAD，将多种自监督ViT模型的知识融合到紧凑的学生网络中，实现了更高效的小模型训练，并在多项视觉任务上取得了新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习（SSL）方法如对比学习、掩码建模，虽然能捕获丰富特征，但单独预训练，未融合各自优点，且通常得到较大模型，不适合资源受限应用。如何将多种SSL模型的知识在紧凑模型中高效整合，是待解决的问题。

Method: 提出CoMAD框架：1）从三种预训练ViT-Base教师（MAE、MoCo v3、iBOT）蒸馏知识，2）采用非对称掩码，学生仅看25%patch，每个教师用不同轻度掩码，增强推理挑战，3）教师特征通过线性适配及归一化后，用共识门控机制融合（综合余弦相似度和教师间一致性），4）学生目标包括可见token和重建特征的双层KL损失，兼顾局部与全局结构。

Result: 在ImageNet-1K上，CoMAD的小模型ViT-Tiny取得75.4%的Top-1精度，超越此前SOTA 0.4%；在ADE20K上分割mIoU为47.3%，MS-COCO检测/分割分别达44.5%/40.5% AP，均创小模型蒸馏新SOTA。

Conclusion: CoMAD巧妙整合了多种SOTA自监督ViT模型的知识，仅通过轻量学生模型即可达到高精度，证明了“多源共识导向知识蒸馏”框架的有效性，为资源受限场景下的视觉任务提供了新思路。

Abstract: Numerous self-supervised learning paradigms, such as contrastive learning and
masked image modeling, learn powerful representations from unlabeled data but
are typically pretrained in isolation, overlooking complementary insights and
yielding large models that are impractical for resource-constrained deployment.
To overcome these challenges, we introduce Consensus-oriented Masked
Distillation (CoMAD), a lightweight, parameter-free framework that unifies
knowledge from multiple current state-of-the-art self-supervised Vision
Transformers into a compact student network. CoMAD distills from three
pretrained ViT-Base teachers, MAE, MoCo v3, and iBOT, each offering distinct
semantic and contextual priors. Rather than naively averaging teacher outputs,
we apply asymmetric masking: the student sees only 25 percent of patches while
each teacher receives a progressively lighter, unique mask, forcing the student
to interpolate missing features under richer contexts. Teacher embeddings are
aligned to the student's space via a linear adapter and layer normalization,
then fused through our joint consensus gating, which weights each token by
combining cosine affinity with inter-teacher agreement. The student is trained
with dual-level KL divergence on visible tokens and reconstructed feature maps,
capturing both local and global structure. On ImageNet-1K, CoMAD's ViT-Tiny
achieves 75.4 percent Top-1, an increment of 0.4 percent over the previous
state-of-the-art. In dense-prediction transfers, it attains 47.3 percent mIoU
on ADE20K, and 44.5 percent box average precision and 40.5 percent mask average
precision on MS-COCO, establishing a new state-of-the-art in compact SSL
distillation.

</details>


### [4] [Single-Step Reconstruction-Free Anomaly Detection and Segmentation via Diffusion Models](https://arxiv.org/abs/2508.04818)
*Mehrdad Moradi,Marco Grasso,Bianca Maria Colosimo,Kamran Paynabar*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的基于扩散模型、无需重建的异常检测方法RADAR，以解决重建方法在计算效率和准确性上的局限。在多个数据集上，RADAR在准确率、召回率和F1分数等各项指标上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于生成模型的异常检测（如GANs和VAEs）已取得进展，但扩散模型表现更佳。现有扩散模型方法依赖于重建，不仅计算开销大、实时性差，而且对于复杂或微妙异常可能产生与输入无关的正常图像重建。此外，噪声水平参数选择依赖先验知识，不适用于无监督场景。因此，需要新方法克服这些限制。

Method: 作者提出了一种重建自由、基于注意力的扩散模型异常检测方法RADAR。RADAR直接输出异常图，无需重建输入的正常图像，大幅提升推理效率和检测准确性。

Result: 在真实3D打印材料集和MVTec-AD数据集上评估表明，RADAR在各项关键指标上超越了当前扩散模型和传统机器学习统计模型，F1分数分别比次优方法高7%和13%。

Conclusion: RADAR不仅极大提升了扩散模型用于异常检测的效率与准确性，还消除了重建过程中的限制，更适用于实时和无监督环境，具备更广的应用前景。

Abstract: Generative models have demonstrated significant success in anomaly detection
and segmentation over the past decade. Recently, diffusion models have emerged
as a powerful alternative, outperforming previous approaches such as GANs and
VAEs. In typical diffusion-based anomaly detection, a model is trained on
normal data, and during inference, anomalous images are perturbed to a
predefined intermediate step in the forward diffusion process. The
corresponding normal image is then reconstructed through iterative reverse
sampling.
  However, reconstruction-based approaches present three major challenges: (1)
the reconstruction process is computationally expensive due to multiple
sampling steps, making real-time applications impractical; (2) for complex or
subtle patterns, the reconstructed image may correspond to a different normal
pattern rather than the original input; and (3) Choosing an appropriate
intermediate noise level is challenging because it is application-dependent and
often assumes prior knowledge of anomalies, an assumption that does not hold in
unsupervised settings.
  We introduce Reconstruction-free Anomaly Detection with Attention-based
diffusion models in Real-time (RADAR), which overcomes the limitations of
reconstruction-based anomaly detection. Unlike current SOTA methods that
reconstruct the input image, RADAR directly produces anomaly maps from the
diffusion model, improving both detection accuracy and computational
efficiency. We evaluate RADAR on real-world 3D-printed material and the
MVTec-AD dataset. Our approach surpasses state-of-the-art diffusion-based and
statistical machine learning models across all key metrics, including accuracy,
precision, recall, and F1 score. Specifically, RADAR improves F1 score by 7% on
MVTec-AD and 13% on the 3D-printed material dataset compared to the next best
model.
  Code available at: https://github.com/mehrdadmoradi124/RADAR

</details>


### [5] [A deep learning approach to track eye movements based on events](https://arxiv.org/abs/2508.04827)
*Chirag Seth,Divya Naiken,Keyan Lin*

Main category: cs.CV

TL;DR: 本研究利用事件相机并结合深度学习方法（主要是CNN_LSTM模型），开发了一种高效、低成本的眼动追踪算法，以预测人类注意力位置，成果达到了约81%的准确率，对VR/AR等消费电子领域有重要应用价值。


<details>
  <summary>Details</summary>
Motivation: 由于眼球运动极为迅速（最高可达300°/s），传统高精度眼动追踪依赖昂贵的高速相机，限制了其大规模商用应用。研究致力于开发更经济、精确、可解释的眼动追踪算法，满足智能设备与人机交互的需求。

Method: 采用事件相机捕捉眼部运动数据，利用卷积神经网络（CNN）与长短时记忆网络（LSTM）结合的深度学习架构（CNN_LSTM）进行眼中心(x, y)位置估计。此外，提出未来引入分层相关性传播（LRP）方法提升模型可解释性。

Result: 选用CNN_LSTM模型后，能够以约81%的准确率定位眼中心坐标。模型效果优于其他尝试方法，表明该算法具有效率与准确率兼得的优势。

Conclusion: 基于事件相机与深度学习的眼动追踪方案在保持较低硬件成本的前提下，实现了高效的注意力预测，对提升VR/AR等设备用户体验具有现实价值。未来通过采用LRP等方法，有望进一步增强模型解读性和性能。

Abstract: This research project addresses the challenge of accurately tracking eye
movements during specific events by leveraging previous research. Given the
rapid movements of human eyes, which can reach speeds of 300{\deg}/s, precise
eye tracking typically requires expensive and high-speed cameras. Our primary
objective is to locate the eye center position (x, y) using inputs from an
event camera. Eye movement analysis has extensive applications in consumer
electronics, especially in VR and AR product development. Therefore, our
ultimate goal is to develop an interpretable and cost-effective algorithm using
deep learning methods to predict human attention, thereby improving device
comfort and enhancing overall user experience. To achieve this goal, we
explored various approaches, with the CNN\_LSTM model proving most effective,
achieving approximately 81\% accuracy. Additionally, we propose future work
focusing on Layer-wise Relevance Propagation (LRP) to further enhance the
model's interpretability and predictive performance.

</details>


### [6] [LuKAN: A Kolmogorov-Arnold Network Framework for 3D Human Motion Prediction](https://arxiv.org/abs/2508.04847)
*Md Zahidul Hasan,A. Ben Hamza,Nizar Bouguila*

Main category: cs.CV

TL;DR: 本文提出了一种新的人体三维动作预测方法LuKAN，在保证预测准确性的同时，大幅提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有三维人体动作预测方法在精度与计算效率之间存在权衡难题，需要更加高效、准确的建模方式。

Method: 1) 输入动作序列先通过离散小波变换编码时序特征；2) 通过空间投影层建模关节依赖关系，保证身体结构一致性；3) 核心的时序依赖学习器采用以Lucas多项式为激活函数的Kolmogorov-Arnold Network高效建模复杂动态；4) 最终通过逆小波变换重建时域动作序列。

Result: 在三个公开数据集上，LuKAN在定量和定性评测中均优于多数主流基线方法，为预测精度和模型简洁性带来良好效果。

Conclusion: LuKAN通过创新性地结合KAN和Lucas多项式，实现了高效、准确的人体动作预测，兼顾计算与结构优势，为三维人体动作预测提供了新思路。

Abstract: The goal of 3D human motion prediction is to forecast future 3D poses of the
human body based on historical motion data. Existing methods often face
limitations in achieving a balance between prediction accuracy and
computational efficiency. In this paper, we present LuKAN, an effective model
based on Kolmogorov-Arnold Networks (KANs) with Lucas polynomial activations.
Our model first applies the discrete wavelet transform to encode temporal
information in the input motion sequence. Then, a spatial projection layer is
used to capture inter-joint dependencies, ensuring structural consistency of
the human body. At the core of LuKAN is the Temporal Dependency Learner, which
employs a KAN layer parameterized by Lucas polynomials for efficient function
approximation. These polynomials provide computational efficiency and an
enhanced capability to handle oscillatory behaviors. Finally, the inverse
discrete wavelet transform reconstructs motion sequences in the time domain,
generating temporally coherent predictions. Extensive experiments on three
benchmark datasets demonstrate the competitive performance of our model
compared to strong baselines, as evidenced by both quantitative and qualitative
evaluations. Moreover, its compact architecture coupled with the linear
recurrence of Lucas polynomials, ensures computational efficiency.

</details>


### [7] [VER-Bench: Evaluating MLLMs on Reasoning with Fine-Grained Visual Evidence](https://arxiv.org/abs/2508.04852)
*Chenhui Qiang,Zhaoyang Wei,Xumeng Han Zipeng Wang,Siyao Li,Xiangyuan Lan,Jianbin Jiao,Zhenjun Han*

Main category: cs.CV

TL;DR: 本文提出了VER-Bench，一个专门用于评估多模态大模型（MLLMs）在细粒度视觉线索提取与复杂推理能力的新型基准测试。该基准通过设计374个多类别高难度问题，聚焦图像中极小区域的重要细节，系统性揭示现有模型在真实视觉理解上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉基准往往偏重于识别显著的大型目标，缺乏对细致、局部线索及它们与世界知识整合推理能力的有效考察。但人类深层次的视觉理解实际上高度依赖于对这些微小细节的敏锐捕捉和推理。为此，研究者希望推动MLLMs从注重大物体的“看图说话”，进步到能够察觉、分析微小却关键的信息，提升模型在真实、复杂视觉场景的实用价值。

Method: 研究者构建了VER-Bench，一个包含374个精心设计问题的多维度评测集，横跨地理、时间、情境、意图、系统状态和符号推理等六大类别。每道题都提供了结构化证据，包括详细视觉线索和基于线索的推理路径。这些线索平均仅占图像0.25%的面积，极为隐蔽，需模型具备极强的细粒度识别和高阶推理能力。

Result: 通过VER-Bench的测试，现有主流MLLMs在提取和利用细微视觉证据、基于这些证据进行复杂推理方面能力明显不足，表现出了明显限度。这一差距揭示了目前视觉大模型与人类高阶理解之间的重大差距。

Conclusion: VER-Bench为评估和提升MLLMs的细粒度视觉理解和推理能力提供了专业基准与工具，也明确指出了当前模型的短板。未来需着重提升模型对微小视觉线索的感知与整合推理能力，以实现更真实和类人的视觉分析效果。

Abstract: With the rapid development of MLLMs, evaluating their visual capabilities has
become increasingly crucial. Current benchmarks primarily fall into two main
types: basic perception benchmarks, which focus on local details but lack deep
reasoning (e.g., "what is in the image?"), and mainstream reasoning benchmarks,
which concentrate on prominent image elements but may fail to assess subtle
clues requiring intricate analysis. However, profound visual understanding and
complex reasoning depend more on interpreting subtle, inconspicuous local
details than on perceiving salient, macro-level objects. These details, though
occupying minimal image area, often contain richer, more critical information
for robust analysis. To bridge this gap, we introduce the VER-Bench, a novel
framework to evaluate MLLMs' ability to: 1) identify fine-grained visual clues,
often occupying on average just 0.25% of the image area; 2) integrate these
clues with world knowledge for complex reasoning. Comprising 374 carefully
designed questions across Geospatial, Temporal, Situational, Intent, System
State, and Symbolic reasoning, each question in VER-Bench is accompanied by
structured evidence: visual clues and question-related reasoning derived from
them. VER-Bench reveals current models' limitations in extracting subtle visual
evidence and constructing evidence-based arguments, highlighting the need to
enhance models's capabilities in fine-grained visual evidence extraction,
integration, and reasoning for genuine visual understanding and human-like
analysis. Dataset and additional materials are available
https://github.com/verbta/ACMMM-25-Materials.

</details>


### [8] [Dual-Stream Attention with Multi-Modal Queries for Object Detection in Transportation Applications](https://arxiv.org/abs/2508.04868)
*Noreen Anwar,Guillaume-Alexandre Bilodeau,Wassim Bouachir*

Main category: cs.CV

TL;DR: 提出了名为DAMM的目标检测新方法，通过多模态查询和双流注意力，显著提升检测精度和效率，在多个基准上取得了SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的目标检测器在遮挡、精确定位、计算效率等方面存在挑战，主要受限于固定查询和稠密注意力机制。

Method: DAMM框架引入了（1）查询自适应机制，（2）多模态查询（包括基于视觉-语言模型的外观查询，多边形嵌入的位置查询，以及一般性的随机学习查询），以及（3）双流交叉注意力模块，分别精细化语义和空间特征。

Result: 在四个具有挑战性的目标检测基准上评估DAMM，结果显示其在平均精度（AP）和召回率等指标上达到最新的SOTA水平。

Conclusion: 多模态查询自适应和双流注意力机制有效增强了目标检测器在复杂场景下的表现，提升了精度与效率。

Abstract: Transformer-based object detectors often struggle with occlusions,
fine-grained localization, and computational inefficiency caused by fixed
queries and dense attention. We propose DAMM, Dual-stream Attention with
Multi-Modal queries, a novel framework introducing both query adaptation and
structured cross-attention for improved accuracy and efficiency. DAMM
capitalizes on three types of queries: appearance-based queries from
vision-language models, positional queries using polygonal embeddings, and
random learned queries for general scene coverage. Furthermore, a dual-stream
cross-attention module separately refines semantic and spatial features,
boosting localization precision in cluttered scenes. We evaluated DAMM on four
challenging benchmarks, and it achieved state-of-the-art performance in average
precision (AP) and recall, demonstrating the effectiveness of multi-modal query
adaptation and dual-stream attention. Source code is at:
\href{https://github.com/DET-LIP/DAMM}{GitHub}.

</details>


### [9] [Revealing Temporal Label Noise in Multimodal Hateful Video Classification](https://arxiv.org/abs/2508.04900)
*Shuonan Yang,Tailin Chen,Rahul Singh,Jiangbei Yue,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

TL;DR: 当前主流多模态仇恨视频检测依赖粗粒度视频级标注，导致标签噪声和模型准确性降低，本文提出基于时间戳的精细化标注方法，有效区分仇恨与非仇恨片段，证明更细粒度标注能提升检测效果和模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 仇恨言论在网络多媒体中快速扩散，传统利用整段视频级标签的检测方法会引入大量噪声，模型难以精准识别真正的仇恨内容，因此需要更细粒度的标注和分析方法以提升检测性能。

Method: 作者利用HateMM和MultiHateClip两大英语数据集，根据标注时间戳剪辑出明确的仇恨片段，并对这些片段及非仇恨片段进行分布及特征分析。同时，通过对比实验，分析时间戳精度（标注噪声）对模型决策边界与分类置信度的影响。

Result: 通过精细化分割与分析，发现视频级标签带来的标签噪声会显著混淆模型判断，引发决策边界混乱和信心下降；而基于时间戳的精细标注可明显减少误判，提高模型对仇恨内容识别的精度与鲁棒性。

Conclusion: 论文强调了仇恨视频检测需关注语境和时间连续性，主张发展时序感知模型与精细化基准，以提升多模态仇恨检测的可解释性和鲁棒性。

Abstract: The rapid proliferation of online multimedia content has intensified the
spread of hate speech, presenting critical societal and regulatory challenges.
While recent work has advanced multimodal hateful video detection, most
approaches rely on coarse, video-level annotations that overlook the temporal
granularity of hateful content. This introduces substantial label noise, as
videos annotated as hateful often contain long non-hateful segments. In this
paper, we investigate the impact of such label ambiguity through a fine-grained
approach. Specifically, we trim hateful videos from the HateMM and
MultiHateClip English datasets using annotated timestamps to isolate explicitly
hateful segments. We then conduct an exploratory analysis of these trimmed
segments to examine the distribution and characteristics of both hateful and
non-hateful content. This analysis highlights the degree of semantic overlap
and the confusion introduced by coarse, video-level annotations. Finally,
controlled experiments demonstrated that time-stamp noise fundamentally alters
model decision boundaries and weakens classification confidence, highlighting
the inherent context dependency and temporal continuity of hate speech
expression. Our findings provide new insights into the temporal dynamics of
multimodal hateful videos and highlight the need for temporally aware models
and benchmarks for improved robustness and interpretability. Code and data are
available at
https://github.com/Multimodal-Intelligence-Lab-MIL/HatefulVideoLabelNoise.

</details>


### [10] [Test-Time Adaptation for Video Highlight Detection Using Meta-Auxiliary Learning and Cross-Modality Hallucinations](https://arxiv.org/abs/2508.04924)
*Zahidul Islam,Sujoy Paul,Mrigank Rochan*

Main category: cs.CV

TL;DR: 提出了一种名为Highlight-TTA的视频高光时刻检测测试时自适应框架，在测试阶段针对每个视频动态适配模型，有效提升了模型的泛化能力和检测表现。


<details>
  <summary>Details</summary>
Motivation: 现有视频高光检测方法采用通用模型，难以适应各测试视频的独特特征，导致检测性能下降。因此需要一种方法能根据测试视频内容动态调整模型以提升表现。

Method: 提出Highlight-TTA测试时自适应框架，通过联合主任务（高光检测）和辅助任务（跨模态伪造），利用元-辅助训练机制提升适应能力。测试时使用辅助任务对模型进行迁移和自适应，提高检测效果。

Result: 在三个最先进的高光检测模型和三个基准数据集上实验，接入Highlight-TTA均显著提升了这些模型的检测性能。

Conclusion: Highlight-TTA能够针对测试视频自适应提高高光检测的泛化与性能，对现有高光检测模型有良好的兼容性和实际提升效果。

Abstract: Existing video highlight detection methods, although advanced, struggle to
generalize well to all test videos. These methods typically employ a generic
highlight detection model for each test video, which is suboptimal as it fails
to account for the unique characteristics and variations of individual test
videos. Such fixed models do not adapt to the diverse content, styles, or audio
and visual qualities present in new, unseen test videos, leading to reduced
highlight detection performance. In this paper, we propose Highlight-TTA, a
test-time adaptation framework for video highlight detection that addresses
this limitation by dynamically adapting the model during testing to better
align with the specific characteristics of each test video, thereby improving
generalization and highlight detection performance. Highlight-TTA is jointly
optimized with an auxiliary task, cross-modality hallucinations, alongside the
primary highlight detection task. We utilize a meta-auxiliary training scheme
to enable effective adaptation through the auxiliary task while enhancing the
primary task. During testing, we adapt the trained model using the auxiliary
task on the test video to further enhance its highlight detection performance.
Extensive experiments with three state-of-the-art highlight detection models
and three benchmark datasets show that the introduction of Highlight-TTA to
these models improves their performance, yielding superior results.

</details>


### [11] [Extending Foundational Monocular Depth Estimators to Fisheye Cameras with Calibration Tokens](https://arxiv.org/abs/2508.04928)
*Suchisrit Gangopadhyay,Jung-Hee Kim,Xien Chen,Patrick Rim,Hyoungseob Park,Alex Wong*

Main category: cs.CV

TL;DR: 本文提出了一种无须重新训练或微调的轻量级方法，扩展基础单目深度估计器在鱼眼图像上的深度估计能力，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基础单目深度估计器（FMDEs）多数在透视图像上训练，遇到摄像头内参与畸变变化（如鱼眼相机）会因分布偏移导致深度估计失真。如何无需大量鱼眼数据及重新训练，提升其泛化能力，是本文关注的问题。

Method: 作者提出使用轻量级的“Calibration Tokens”，通过调制FMDE的潜在嵌入空间，将鱼眼图像的分布对齐到透视图像分布，实现模型输入适配与泛化。方法自监督训练，仅需使用透视大规模数据集，通过在训练时将透视图像校准为鱼眼，并对其估计结果进行一致性约束，不需要鱼眼图像数据。

Result: 在室内和室外多场景测试了多种FMDE，方法均优于当前最优技术，同时单一组token可兼容多种场景。

Conclusion: 本文方法简单高效，可广泛扩展FMDE至鱼眼相机，具备实际工程意义与较强通用性，无需鱼眼数据和模型重训练。

Abstract: We propose a method to extend foundational monocular depth estimators
(FMDEs), trained on perspective images, to fisheye images. Despite being
trained on tens of millions of images, FMDEs are susceptible to the covariate
shift introduced by changes in camera calibration (intrinsic, distortion)
parameters, leading to erroneous depth estimates. Our method aligns the
distribution of latent embeddings encoding fisheye images to those of
perspective images, enabling the reuse of FMDEs for fisheye cameras without
retraining or finetuning. To this end, we introduce a set of Calibration Tokens
as a light-weight adaptation mechanism that modulates the latent embeddings for
alignment. By exploiting the already expressive latent space of FMDEs, we posit
that modulating their embeddings avoids the negative impact of artifacts and
loss introduced in conventional recalibration or map projection to a canonical
reference frame in the image space. Our method is self-supervised and does not
require fisheye images but leverages publicly available large-scale perspective
image datasets. This is done by recalibrating perspective images to fisheye
images, and enforcing consistency between their estimates during training. We
evaluate our approach with several FMDEs, on both indoors and outdoors, where
we consistently improve over state-of-the-art methods using a single set of
tokens for both. Code available at:
https://github.com/JungHeeKim29/calibration-token.

</details>


### [12] [Toward Errorless Training ImageNet-1k](https://arxiv.org/abs/2508.04941)
*Bo Deng,Levi Heath*

Main category: cs.CV

TL;DR: 本文提出了一种基于前馈神经网络的新方法，在ImageNet 2012数据集上取得了较高的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 作者旨在通过新方法提升神经网络在大规模图像分类任务（如ImageNet 2012）上的准确率，并分析影响模型上限的问题。

Method: 采用了文献[5]的新型训练方法，使用大规模参数的前馈人工神经网络，在ImageNet 2012竞赛数据集上进行训练与测试。

Result: 模型达到了98.3%的总准确率和99.69%的Top-1准确率，10个数据子集平均有285.9个标签被完全正确分类。最佳模型包含322,430,160个参数，精确到小数点后4位。

Conclusion: 模型表现优异，但未能达到100%准确率，作者推测主要原因在于数据集存在重复图片但标签不同的双重标签问题。

Abstract: In this paper, we describe a feedforward artificial neural network trained on
the ImageNet 2012 contest dataset [7] with the new method of [5] to an accuracy
rate of 98.3% with a 99.69 Top-1 rate, and an average of 285.9 labels that are
perfectly classified over the 10 batch partitions of the dataset. The best
performing model uses 322,430,160 parameters, with 4 decimal places precision.
We conjecture that the reason our model does not achieve a 100% accuracy rate
is due to a double-labeling problem, by which there are duplicate images in the
dataset with different labels.

</details>


### [13] [Accelerating Conditional Prompt Learning via Masked Image Modeling for Vision-Language Models](https://arxiv.org/abs/2508.04942)
*Phuoc-Nguyen Bui,Khanh-Binh Nguyen,Hyunseung Choo*

Main category: cs.CV

TL;DR: 提出了ProMIM，一种可插拔的框架，通过引入掩码图像建模（MIM）提升现有视觉-语言模型（VLMs）在新任务上的泛化能力，且无需增加显著的计算成本。


<details>
  <summary>Details</summary>
Motivation: VLM如CLIP在零样本学习中表现出色，但适应新任务时需大量计算资源。现有Prompt学习方法虽然高效，但容易对已知类别过拟合，导致对未知类别泛化能力差。需要一种既高效又能改善泛化的方法。

Method: 提出ProMIM，将掩码图像建模（MIM）整合进条件式Prompt学习，采用对可见图像块进行掩码并利用其特征生成Prompt。该框架可无缝增强CoOp、CoCoOp等现有方法，无需更改其核心架构，且只带来极小计算开销。

Result: 在零样本和小样本分类任务上的大量实验证明，ProMIM可以稳定提升现有方法的泛化能力。

Conclusion: ProMIM是一种实用、轻量级的框架，通过简单但有效的掩码策略增强VLMs泛化能力，为真实场景下的视觉-语言应用提供了新解决方案。

Abstract: Vision-language models (VLMs) like CLIP excel in zero-shot learning but often
require resource-intensive training to adapt to new tasks. Prompt learning
techniques, such as CoOp and CoCoOp, offer efficient adaptation but tend to
overfit to known classes, limiting generalization to unseen categories. We
introduce ProMIM, a plug-and-play framework that enhances conditional prompt
learning by integrating masked image modeling (MIM) into existing VLM
pipelines. ProMIM leverages a simple yet effective masking strategy to generate
robust, instance-conditioned prompts, seamlessly augmenting methods like CoOp
and CoCoOp without altering their core architectures. By masking only visible
image patches and using these representations to guide prompt generation,
ProMIM improves feature robustness and mitigates overfitting, all while
introducing negligible additional computational cost. Extensive experiments
across zero-shot and few-shot classification tasks demonstrate that ProMIM
consistently boosts generalization performance when plugged into existing
approaches, providing a practical, lightweight solution for real-world
vision-language applications.

</details>


### [14] [TRKT: Weakly Supervised Dynamic Scene Graph Generation with Temporal-enhanced Relation-aware Knowledge Transferring](https://arxiv.org/abs/2508.04943)
*Zhu Xu,Ting Lei,Zhimin Li,Guan Wang,Qingchao Chen,Yuxin Peng,Yang liu*

Main category: cs.CV

TL;DR: 本论文提出了一种新方法TRKT，用于动态场景图生成中的弱监督学习，有效提升了视频中对象和关系的检测精度，并在Action Genome数据集上取得了领先的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的弱监督动态场景图生成方法过度依赖外部目标检测器，这些检测器在处理动态关系感知场景时表现不佳，导致伪标签不准确，影响最终场景图生成效果。因此，研究者需要寻找摆脱外部检测器局限的新方法。

Method: 作者提出了Temporal-enhanced Relation-aware Knowledge Transferring（TRKT）方法。首先，设计了'关系感知知识挖掘'，通过对象与关系类的解码器生成类别注意力图，突出对象和交互区域，并通过'帧间注意力增强'策略结合光流信息，提升注意力图对运动与模糊的鲁棒性。其次，引入'双流融合模块'，将上述类别注意力图与外部检测器生成的结果融合，从而优化对象定位和候选置信度。

Result: 大量实验验证了TRKT的有效性，在Action Genome数据集上达到了目前最优的动态场景图生成性能。

Conclusion: TRKT方法能够高效利用弱标签信息、克服外部检测器局限，提升动态视频场景中对象与关系检测的准确性，理论和实验结果均支持其作为WS-DSGG任务的先进解决方案。

Abstract: Dynamic Scene Graph Generation (DSGG) aims to create a scene graph for each
video frame by detecting objects and predicting their relationships. Weakly
Supervised DSGG (WS-DSGG) reduces annotation workload by using an unlocalized
scene graph from a single frame per video for training. Existing WS-DSGG
methods depend on an off-the-shelf external object detector to generate pseudo
labels for subsequent DSGG training. However, detectors trained on static,
object-centric images struggle in dynamic, relation-aware scenarios required
for DSGG, leading to inaccurate localization and low-confidence proposals. To
address the challenges posed by external object detectors in WS-DSGG, we
propose a Temporal-enhanced Relation-aware Knowledge Transferring (TRKT)
method, which leverages knowledge to enhance detection in relation-aware
dynamic scenarios. TRKT is built on two key components:(1)Relation-aware
knowledge mining: we first employ object and relation class decoders that
generate category-specific attention maps to highlight both object regions and
interactive areas. Then we propose an Inter-frame Attention Augmentation
strategy that exploits optical flow for neighboring frames to enhance the
attention maps, making them motion-aware and robust to motion blur. This step
yields relation- and motion-aware knowledge mining for WS-DSGG. (2) we
introduce a Dual-stream Fusion Module that integrates category-specific
attention maps into external detections to refine object localization and boost
confidence scores for object proposals. Extensive experiments demonstrate that
TRKT achieves state-of-the-art performance on Action Genome dataset. Our code
is avaliable at https://github.com/XZPKU/TRKT.git.

</details>


### [15] [AdvDINO: Domain-Adversarial Self-Supervised Representation Learning for Spatial Proteomics](https://arxiv.org/abs/2508.04955)
*Stella Su,Marc Harary,Scott J. Rodig,William Lotter*

Main category: cs.CV

TL;DR: 本文提出了一种名为AdvDINO的自监督学习框架，结合领域对抗机制来提升在存在领域偏移情况下的鲁棒性，并在肺癌多通道免疫荧光图像中验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 当前自监督学习虽无需人工标注，但在领域偏移（如生物医学成像中的批次效应）下表现不稳健，影响其在实际中准确捕捉生物学信号的能力。

Method: 作者将梯度反转层集成到DINOv2架构中，实现领域对抗的自监督特征学习（AdvDINO），从而抑制数据源间的偏移，促使模型学到领域无关且具生物意义的特征。

Result: 在逾546万张非小细胞肺癌mIF图像切片上测试，AdvDINO减弱了切片特异性偏差，相比非对抗方法学到更稳健、与生物学更相关的表示，区分出具有不同蛋白质谱和预后意义的表型簇，并提升了多实例学习下的生存预测表现。

Conclusion: AdvDINO不仅适用于多通道免疫荧光数据，在放射学、遥感、自动驾驶等数据有限且存在领域偏移的成像领域同样有广泛应用前景，可提升模型泛化和解释能力。

Abstract: Self-supervised learning (SSL) has emerged as a powerful approach for
learning visual representations without manual annotations. However, the
robustness of standard SSL methods to domain shift -- systematic differences
across data sources -- remains uncertain, posing an especially critical
challenge in biomedical imaging where batch effects can obscure true biological
signals. We present AdvDINO, a domain-adversarial self-supervised learning
framework that integrates a gradient reversal layer into the DINOv2
architecture to promote domain-invariant feature learning. Applied to a
real-world cohort of six-channel multiplex immunofluorescence (mIF) whole slide
images from non-small cell lung cancer patients, AdvDINO mitigates
slide-specific biases to learn more robust and biologically meaningful
representations than non-adversarial baselines. Across $>5.46$ million mIF
image tiles, the model uncovers phenotype clusters with distinct proteomic
profiles and prognostic significance, and improves survival prediction in
attention-based multiple instance learning. While demonstrated on mIF data,
AdvDINO is broadly applicable to other imaging domains -- including radiology,
remote sensing, and autonomous driving -- where domain shift and limited
annotated data hinder model generalization and interpretability.

</details>


### [16] [Open-world Point Cloud Semantic Segmentation: A Human-in-the-loop Framework](https://arxiv.org/abs/2508.04962)
*Peng Zhang,Songru Yang,Jinsheng Sun,Weiqing Li,Zhiyong Su*

Main category: cs.CV

TL;DR: 该论文提出了一种新的人机协同点云语义分割方法HOW-Seg，用于开放世界场景，实现了对基础类和新类的高效分割，且所需人工标注极少。方法在多个数据集上表现优异，超越现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 现有开放世界点云分割方法依赖耗时的离线增量学习或密集标注，实际应用受限，因此亟需更实用、标注成本更低的方案。

Method: 提出HOW-Seg框架：1）直接在查询数据生成类别原型，减少类内分布偏差影响；2）通过少量人类标注指导原型分割，支持基础类与新类；3）引入层次化原型消歧以细化原型表达；4）在精细原型上利用条件随机场进一步优化标签分配；5）通过人类反馈不断迭代提升分割性能。

Result: 在仅用极少（如“一类一次点击”）人工标注时，HOW-Seg能达到或超越当前SOTA的GFS-Seg（5-shot）表现。采用更强骨干网络和更密集标注时，在S3DIS数据集上取得85.27%的mIoU，在ScanNetv2上取得66.37%的mIoU，显著优于其他方法。

Conclusion: HOW-Seg框架有效降低了开放世界点云分割任务的人力和数据需求，对实际部署具有更高实用性和推广价值。

Abstract: Open-world point cloud semantic segmentation (OW-Seg) aims to predict point
labels of both base and novel classes in real-world scenarios. However,
existing methods rely on resource-intensive offline incremental learning or
densely annotated support data, limiting their practicality. To address these
limitations, we propose HOW-Seg, the first human-in-the-loop framework for
OW-Seg. Specifically, we construct class prototypes, the fundamental
segmentation units, directly on the query data, avoiding the prototype bias
caused by intra-class distribution shifts between the support and query data.
By leveraging sparse human annotations as guidance, HOW-Seg enables
prototype-based segmentation for both base and novel classes. Considering the
lack of granularity of initial prototypes, we introduce a hierarchical
prototype disambiguation mechanism to refine ambiguous prototypes, which
correspond to annotations of different classes. To further enrich contextual
awareness, we employ a dense conditional random field (CRF) upon the refined
prototypes to optimize their label assignments. Through iterative human
feedback, HOW-Seg dynamically improves its predictions, achieving high-quality
segmentation for both base and novel classes. Experiments demonstrate that with
sparse annotations (e.g., one-novel-class-one-click), HOW-Seg matches or
surpasses the state-of-the-art generalized few-shot segmentation (GFS-Seg)
method under the 5-shot setting. When using advanced backbones (e.g.,
Stratified Transformer) and denser annotations (e.g., 10 clicks per sub-scene),
HOW-Seg achieves 85.27% mIoU on S3DIS and 66.37% mIoU on ScanNetv2,
significantly outperforming alternatives.

</details>


### [17] [UGOD: Uncertainty-Guided Differentiable Opacity and Soft Dropout for Enhanced Sparse-View 3DGS](https://arxiv.org/abs/2508.04968)
*Zhihao Guo,Peng Wang,Zidong Chen,Xiangyu Kong,Yan Lyu,Guanyu Gao,Liangxiu Han*

Main category: cs.CV

TL;DR: 本文提出了一种基于自适应权重和不确定性建模的3D高斯渲染方法，有效提升了在稀疏视角下的新视角合成质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯投影渲染方法中，高斯成分通常被赋予相等权重，容易导致过拟合，尤其在稀疏视角条件下，渲染质量难以保障。如何自适应地调整不同高斯的权重，提升渲染效果，是当前亟待解决的问题。

Method: 本研究提出通过学习高斯的不确定性来自适应调整其权重：不确定性一方面引导高斯的不透明度的可微分更新，保持原有管线结构；另一方面融合了软可微dropout正则化，将不确定性转为连续的drop概率，指导最终的高斯投影和混合的渲染过程。

Result: 在多个标准数据集上的实验表明，该方法在稀疏视角新视角合成任务取得了领先效果：重建精度高于主流方法，在大多数数据集上用更少的高斯即能获得更高的渲染质量。例如在MipNeRF 360数据集上，较DropGaussian方法提升3.27%的PSNR。

Conclusion: 通过引入基于不确定性的自适应高斯加权和dropout正则，显著缓解了3DGS方法在稀疏视角下的过拟合问题，提高了视角合成的重建质量，未来有望广泛应用于高效3D重建领域。

Abstract: 3D Gaussian Splatting (3DGS) has become a competitive approach for novel view
synthesis (NVS) due to its advanced rendering efficiency through 3D Gaussian
projection and blending. However, Gaussians are treated equally weighted for
rendering in most 3DGS methods, making them prone to overfitting, which is
particularly the case in sparse-view scenarios. To address this, we investigate
how adaptive weighting of Gaussians affects rendering quality, which is
characterised by learned uncertainties proposed. This learned uncertainty
serves two key purposes: first, it guides the differentiable update of Gaussian
opacity while preserving the 3DGS pipeline integrity; second, the uncertainty
undergoes soft differentiable dropout regularisation, which strategically
transforms the original uncertainty into continuous drop probabilities that
govern the final Gaussian projection and blending process for rendering.
Extensive experimental results over widely adopted datasets demonstrate that
our method outperforms rivals in sparse-view 3D synthesis, achieving higher
quality reconstruction with fewer Gaussians in most datasets compared to
existing sparse-view approaches, e.g., compared to DropGaussian, our method
achieves 3.27\% PSNR improvements on the MipNeRF 360 dataset.

</details>


### [18] [CSRAP: Enhanced Canvas Attention Scheduling for Real-Time Mission Critical Perception](https://arxiv.org/abs/2508.04976)
*Md Iftekharul Islam Sakib,Yigong Hu,Tarek Abdelzaher*

Main category: cs.CV

TL;DR: 本文提出了对canvas-based attention scheduling机制的改进，通过引入可变画布大小和灵活帧率，进一步优化了边缘设备上高分辨率目标检测的性能与资源消耗之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 边缘平台实时感知任务难以在有限计算资源下实现高分辨率目标检测，因此需要更加高效的资源调度机制以应对严格的延迟约束。

Method: 在已有canvas-based attention scheduling方法基础上，本文提出可变大小的canvas帧，并可选择canvas帧率不必与原始数据帧率一致。通过在NVIDIA Jetson Orin Nano上部署YOLOv11并处理Waymo数据集，评估其性能。

Result: 引入更多调度自由度后，可以在保持较高帧率的同时提升检测精度（mAP和recall），实现优于现有方法的质量/成本权衡。

Conclusion: 可变canvas帧大小和灵活帧率为边缘平台下现实感知系统提供了更强的适应性和效能，可显著提升检测表现，推动高效实时感知的发展。

Abstract: Real-time perception on edge platforms faces a core challenge: executing
high-resolution object detection under stringent latency constraints on limited
computing resources. Canvas-based attention scheduling was proposed in earlier
work as a mechanism to reduce the resource demands of perception subsystems. It
consolidates areas of interest in an input data frame onto a smaller area,
called a canvas frame, that can be processed at the requisite frame rate. This
paper extends prior canvas-based attention scheduling literature by (i)
allowing for variable-size canvas frames and (ii) employing selectable canvas
frame rates that may depart from the original data frame rate. We evaluate our
solution by running YOLOv11, as the perception module, on an NVIDIA Jetson Orin
Nano to inspect video frames from the Waymo Open Dataset. Our results show that
the additional degrees of freedom improve the attainable quality/cost
trade-offs, thereby allowing for a consistently higher mean average precision
(mAP) and recall with respect to the state of the art.

</details>


### [19] [Steering One-Step Diffusion Model with Fidelity-Rich Decoder for Fast Image Compression](https://arxiv.org/abs/2508.04979)
*Zheng Chen,Mingde Zhou,Jinpei Guo,Jiale Yuan,Yifei Ji,Yulun Zhang*

Main category: cs.CV

TL;DR: SODEC是一种新颖的单步扩散图像压缩模型，实现了更快的解码速度和更高的图像保真度。


<details>
  <summary>Details</summary>
Motivation: 基于扩散的图像压缩方法在感知质量上表现优异，但存在解码延迟高和保真度差的问题。为了解决这些问题，提出了新的单步扩散解码方法。

Method: 提出SODEC模型，利用预训练的VAE生成包含丰富信息的潜变量，用单步扩散取代多步去噪过程。同时，设计保真度引导模块和码率退火训练策略，以进一步提升还原质量和低码率下的表现。

Result: 实验显示，SODEC在码率-失真-感知平衡上显著优于现有方法，解码速度提升超20倍。

Conclusion: SODEC克服了扩散压缩模型的主要瓶颈，兼具高效率与高质量，具有实际应用前景。

Abstract: Diffusion-based image compression has demonstrated impressive perceptual
performance. However, it suffers from two critical drawbacks: (1) excessive
decoding latency due to multi-step sampling, and (2) poor fidelity resulting
from over-reliance on generative priors. To address these issues, we propose
SODEC, a novel single-step diffusion image compression model. We argue that in
image compression, a sufficiently informative latent renders multi-step
refinement unnecessary. Based on this insight, we leverage a pre-trained
VAE-based model to produce latents with rich information, and replace the
iterative denoising process with a single-step decoding. Meanwhile, to improve
fidelity, we introduce the fidelity guidance module, encouraging output that is
faithful to the original image. Furthermore, we design the rate annealing
training strategy to enable effective training under extremely low bitrates.
Extensive experiments show that SODEC significantly outperforms existing
methods, achieving superior rate-distortion-perception performance. Moreover,
compared to previous diffusion-based compression models, SODEC improves
decoding speed by more than 20$\times$. Code is released at:
https://github.com/zhengchen1999/SODEC.

</details>


### [20] [Propagating Sparse Depth via Depth Foundation Model for Out-of-Distribution Depth Completion](https://arxiv.org/abs/2508.04984)
*Shenglun Chen,Xinzhu Ma,Hong Zhang,Haojie Li,Zhihui Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的深度补全框架，利用深度基础模型和无学习参数的双空间传播策略，有效提升了模型在分布外(OOD)场景下的鲁棒性，并在多个数据集上取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 现有深度补全方法依赖有限且精心准备的数据，在分布外(OOD)场景下性能显著下降。基础模型在大规模单目深度估计任务显示出强鲁棒性，如何结合基础模型提升深度补全鲁棒性成为重要问题。

Method: 框架利用深度基础模型从RGB图像中提取结构和语义线索，引导稀疏深度的传播。设计了无可学习参数的三维与二维双空间传播策略，以保持几何结构和局部一致性。为细化结构，引入了可学习的校正模块以逐步调整预测深度。训练于NYUv2和KITTI，并广泛在16个其他数据集上测试。

Result: 该方法在多个OOD场景和众多基准数据集上性能突出，超越现有最先进的深度补全方法。

Conclusion: 通过结合基础深度模型和创新的传播机制，所提方法显著提升了深度补全的泛化能力和鲁棒性，适用于多样化实际应用场景。

Abstract: Depth completion is a pivotal challenge in computer vision, aiming at
reconstructing the dense depth map from a sparse one, typically with a paired
RGB image. Existing learning based models rely on carefully prepared but
limited data, leading to significant performance degradation in
out-of-distribution (OOD) scenarios. Recent foundation models have demonstrated
exceptional robustness in monocular depth estimation through large-scale
training, and using such models to enhance the robustness of depth completion
models is a promising solution. In this work, we propose a novel depth
completion framework that leverages depth foundation models to attain
remarkable robustness without large-scale training. Specifically, we leverage a
depth foundation model to extract environmental cues, including structural and
semantic context, from RGB images to guide the propagation of sparse depth
information into missing regions. We further design a dual-space propagation
approach, without any learnable parameters, to effectively propagates sparse
depth in both 3D and 2D spaces to maintain geometric structure and local
consistency. To refine the intricate structure, we introduce a learnable
correction module to progressively adjust the depth prediction towards the real
depth. We train our model on the NYUv2 and KITTI datasets as in-distribution
datasets and extensively evaluate the framework on 16 other datasets. Our
framework performs remarkably well in the OOD scenarios and outperforms
existing state-of-the-art depth completion methods. Our models are released in
https://github.com/shenglunch/PSD.

</details>


### [21] [Unified modality separation: A vision-language framework for unsupervised domain adaptation](https://arxiv.org/abs/2508.04987)
*Xinyao Li,Jingjing Li,Zhekai Du,Lei Zhu,Heng Tao Shen*

Main category: cs.CV

TL;DR: 本文提出了一种新的无监督领域自适应（UDA）框架，针对预训练视觉-语言模型（VLM）中的模态差异问题，分离处理模态相关和无关成分，通过自适应集成权重提升目标域表现。


<details>
  <summary>Details</summary>
Motivation: 现有UDA方法在处理视觉-语言模型时，受限于视觉与文本模态间的天然鸿沟（模态差异），直接迁移仅转移模态无关知识，导致在新领域表现不佳。为更好地利用视觉和文本特征，需要提升模态相关信息的转移能力。

Method: 方法分为训练阶段和测试阶段：训练时，通过特征解耦，分别处理模态相关与无关分量；测试时，利用自适应集成机制，自动确定不同分量的权重。此外，提出一种模态差异度量，进行样本归类，并采用提示微调（prompt tuning）技术提升性能。

Result: 所提方法在多种骨干网络、基线方法、数据集与自适应场景上进行了大量实验和分析。结果显示，不仅性能最多提升9%，计算效率也提升了9倍，验证了方法的有效性。

Conclusion: 文中提出的统一模态分离框架，能有效缓解视觉与文本模态间的差异，提升无监督领域自适应的效果，对VLM的泛化能力有显著促进意义。

Abstract: Unsupervised domain adaptation (UDA) enables models trained on a labeled
source domain to handle new unlabeled domains. Recently, pre-trained
vision-language models (VLMs) have demonstrated promising zero-shot performance
by leveraging semantic information to facilitate target tasks. By aligning
vision and text embeddings, VLMs have shown notable success in bridging domain
gaps. However, inherent differences naturally exist between modalities, which
is known as modality gap. Our findings reveal that direct UDA with the presence
of modality gap only transfers modality-invariant knowledge, leading to
suboptimal target performance. To address this limitation, we propose a unified
modality separation framework that accommodates both modality-specific and
modality-invariant components. During training, different modality components
are disentangled from VLM features then handled separately in a unified manner.
At test time, modality-adaptive ensemble weights are automatically determined
to maximize the synergy of different components. To evaluate instance-level
modality characteristics, we design a modality discrepancy metric to categorize
samples into modality-invariant, modality-specific, and uncertain ones. The
modality-invariant samples are exploited to facilitate cross-modal alignment,
while uncertain ones are annotated to enhance model capabilities. Building upon
prompt tuning techniques, our methods achieve up to 9% performance gain with 9
times of computational efficiencies. Extensive experiments and analysis across
various backbones, baselines, datasets and adaptation settings demonstrate the
efficacy of our design.

</details>


### [22] [Modeling Rapid Contextual Learning in the Visual Cortex with Fast-Weight Deep Autoencoder Networks](https://arxiv.org/abs/2508.04988)
*Yue Li,Weifan Wang,Tai Sing Lee*

Main category: cs.CV

TL;DR: 本研究通过使用基于Vision Transformer（ViT）的自编码器，结合LoRA方法，实现了对大脑视觉皮层快速学习全球图像上下文现象的建模。结果显示，熟悉性训练可增强网络早期层对全局上下文的敏感性，这种效应在引入LoRA快速权重后进一步提升。该研究为理解大脑如何快速整合全局视觉信息提供了深度学习模型的支持。


<details>
  <summary>Details</summary>
Motivation: 神经生理学研究发现，视觉皮层对于熟悉的图像背景能快速产生稀疏化、平均活动降低等现象，但具体的计算机制尚待深入探讨。过去主要关注局部反馈连接，而对于神经网络如何实现快速、全局的上下文学习和记忆，理解还较为有限。

Method: 本研究构建了基于Vision Transformer（ViT）的自编码器网络，并通过“熟悉性训练”使网络学习图像的全局上下文。同时，提出并在Transformer每层中引入LoRA低秩快速权重，模拟神经系统中的短时记忆机制，进而探究其对网络表示和注意力机制的影响。

Result: 1）ViT自编码器的自注意力电路实现了与神经模型一致的流形变换，呈现熟悉性效应。2）经过熟悉性训练，网络早期层的潜在表示与包含全局信息的顶层对齐。3）熟悉性训练扩大了自注意力在熟悉上下文中的范围。4）这种全局敏感性的增强在引入LoRA快速权重后明显加剧。

Conclusion: 熟悉性训练能引入全局上下文敏感性至神经网络早期层，并且采用快慢权重混合架构（如LoRA）能更有效地模拟大脑快速、全局上下文学习过程，对理解和建模人脑视觉快速学习机制具有重要意义。

Abstract: Recent neurophysiological studies have revealed that the early visual cortex
can rapidly learn global image context, as evidenced by a sparsification of
population responses and a reduction in mean activity when exposed to familiar
versus novel image contexts. This phenomenon has been attributed primarily to
local recurrent interactions, rather than changes in feedforward or feedback
pathways, supported by both empirical findings and circuit-level modeling.
Recurrent neural circuits capable of simulating these effects have been shown
to reshape the geometry of neural manifolds, enhancing robustness and
invariance to irrelevant variations. In this study, we employ a Vision
Transformer (ViT)-based autoencoder to investigate, from a functional
perspective, how familiarity training can induce sensitivity to global context
in the early layers of a deep neural network. We hypothesize that rapid
learning operates via fast weights, which encode transient or short-term memory
traces, and we explore the use of Low-Rank Adaptation (LoRA) to implement such
fast weights within each Transformer layer. Our results show that (1) The
proposed ViT-based autoencoder's self-attention circuit performs a manifold
transform similar to a neural circuit model of the familiarity effect. (2)
Familiarity training aligns latent representations in early layers with those
in the top layer that contains global context information. (3) Familiarity
training broadens the self-attention scope within the remembered image context.
(4) These effects are significantly amplified by LoRA-based fast weights.
Together, these findings suggest that familiarity training introduces global
sensitivity to earlier layers in a hierarchical network, and that a hybrid
fast-and-slow weight architecture may provide a viable computational model for
studying rapid global context learning in the brain.

</details>


### [23] [Attribute Guidance With Inherent Pseudo-label For Occluded Person Re-identification](https://arxiv.org/abs/2508.04998)
*Rui Zhi,Zhen Yang,Haiyang Zhang*

Main category: cs.CV

TL;DR: 本论文提出了一种名为Attribute-Guide ReID（AG-ReID）的新型行人重识别方法，特别提升了在遮挡和细粒度属性变化场景下的识别效果，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前预训练视觉-语言模型在行人重识别（Re-ID）任务中表现良好，但在行人部分遮挡或细微外观差异时，因过于关注整体语义、忽略细粒度属性，导致表现下降。实际应用中，遮挡和外观相似极为常见，亟需新方法增强模型对细节的识别能力。

Method: AG-ReID利用预训练模型的潜能，提出两阶段框架：第一阶段自动为训练样本生成属性伪标签，提取细致的视觉属性；第二阶段通过全局与属性双重引导机制，结合整体与细粒度属性信息进行特征提取，无需额外人工标注数据。

Result: 实验结果表明，AG-ReID在多个主流Re-ID数据集上取得了最新最优成绩，尤其是在遮挡和区分细微属性方面表现明显提升，同时在无遮挡标准场景下也保持了强竞争力。

Conclusion: AG-ReID成功缓解了传统方法过于依赖整体语义而忽视细粒度属性的问题，在复杂的遮挡及难分样本下具有更强鲁棒性，为Re-ID应用提供了高效先进的解决方案。

Abstract: Person re-identification (Re-ID) aims to match person images across different
camera views, with occluded Re-ID addressing scenarios where pedestrians are
partially visible. While pre-trained vision-language models have shown
effectiveness in Re-ID tasks, they face significant challenges in occluded
scenarios by focusing on holistic image semantics while neglecting fine-grained
attribute information. This limitation becomes particularly evident when
dealing with partially occluded pedestrians or when distinguishing between
individuals with subtle appearance differences. To address this limitation, we
propose Attribute-Guide ReID (AG-ReID), a novel framework that leverages
pre-trained models' inherent capabilities to extract fine-grained semantic
attributes without additional data or annotations. Our framework operates
through a two-stage process: first generating attribute pseudo-labels that
capture subtle visual characteristics, then introducing a dual-guidance
mechanism that combines holistic and fine-grained attribute information to
enhance image feature extraction. Extensive experiments demonstrate that
AG-ReID achieves state-of-the-art results on multiple widely-used Re-ID
datasets, showing significant improvements in handling occlusions and subtle
attribute differences while maintaining competitive performance on standard
Re-ID scenarios.

</details>


### [24] [CRAM: Large-scale Video Continual Learning with Bootstrapped Compression](https://arxiv.org/abs/2508.05001)
*Shivani Mall,Joao F. Henriques*

Main category: cs.CV

TL;DR: 本文提出了一种高效的视频持续学习方法，通过存储压缩后的视频嵌入而非原始输入，显著降低了存储需求，并有效缓解了视频持续学习中常见的遗忘问题，实验表明在有限内存下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 持续学习允许神经网络从连续数据流中学习，而不是依赖随即访问全量数据。然而，视频数据体量大、流式特性使得基于回放的持续学习方法面临巨大的内存压力，尤其在处理长时序的视频流时更加突出。本文旨在解决高存储需求与有限回放缓冲区之间的矛盾。

Method: 本文提出通过“压缩视觉”，即仅在回放缓冲区保存视频编码（嵌入向量）而非原始数据，并采用IID方式采样进行训练。同时，视频压缩器在在线训练过程中也可能遗忘，为此提出了一种刷新机制，通过用旧模型解码再用新模型重新编码的方式保持嵌入有效性。该方法命名为CRAM（Continually Refreshed Amodal Memory）。

Result: 在EpicKitchens-100与Kinetics-700两个大规模视频持续学习基准上进行实验，能够在不到2GB的存储中保存上千个长视频。实验结果显示，所提方法在内存占用大幅降低的前提下，持续学习性能明显优于现有方法。

Conclusion: 本文方法显著降低了视频持续学习任务的内存需求，并通过嵌入刷新机制有效避免了遗忘现象，为构建资源有限情境下高效、鲁棒的视频持续学习系统提供了新思路。

Abstract: Continual learning (CL) promises to allow neural networks to learn from
continuous streams of inputs, instead of IID (independent and identically
distributed) sampling, which requires random access to a full dataset. This
would allow for much smaller storage requirements and self-sufficiency of
deployed systems that cope with natural distribution shifts, similarly to
biological learning. We focus on video CL employing a rehearsal-based approach,
which reinforces past samples from a memory buffer. We posit that part of the
reason why practical video CL is challenging is the high memory requirements of
video, further exacerbated by long-videos and continual streams, which are at
odds with the common rehearsal-buffer size constraints. To address this, we
propose to use compressed vision, i.e. store video codes (embeddings) instead
of raw inputs, and train a video classifier by IID sampling from this rolling
buffer. Training a video compressor online (so not depending on any pre-trained
networks) means that it is also subject to catastrophic forgetting. We propose
a scheme to deal with this forgetting by refreshing video codes, which requires
careful decompression with a previous version of the network and recompression
with a new one. We name our method Continually Refreshed Amodal Memory (CRAM).
We expand current video CL benchmarks to large-scale settings, namely
EpicKitchens-100 and Kinetics-700, storing thousands of relatively long videos
in under 2 GB, and demonstrate empirically that our video CL method outperforms
prior art with a significantly reduced memory footprint.

</details>


### [25] [Multimodal Causal-Driven Representation Learning for Generalizable Medical Image Segmentation](https://arxiv.org/abs/2508.05008)
*Xusheng Liang,Lihua Zhou,Nianxin Li,Miao Xu,Ziyang Song,Dong Yi,Jinlin Wu,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.CV

TL;DR: 本文提出了一种结合因果推断与视觉-语言模型（VLM）的新框架MCDRL，用于提升医学图像分割任务中的泛化能力，通过构建混淆因素字典和因果干预网络，有效克服设备差异等造成的领域迁移问题，实验结果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前VLM（如CLIP）在普通计算机视觉任务中表现出色的零样本能力，但在医学影像领域存在显著困难，主要由于医学影像的多样性和复杂性，以及设备、操作等导致的显著领域差异，使得模型很难泛化到新的数据分布。

Method: MCDRL方法主要包含两步：首先利用CLIP的跨模态能力，通过文本提示语识别候选病灶区域，并构建代表领域差异的混淆因素字典；其次，训练一个因果干预网络，利用此字典识别并消除领域特有的混淆因素影响，同时保留分割任务中至关重要的解剖结构信息。

Result: 大量实验表明，MCDRL在医学图像分割任务中，不仅分割精度优于其它现有方法，而且在领域泛化能力方面也表现得更为稳健和突出。

Conclusion: 将因果推断融入VLM，通过识别并干预领域混淆因素，可以显著提升医学图像分割的准确性及泛化能力，为复杂领域的数据处理提供新思路。

Abstract: Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable
zero-shot capabilities in various computer vision tasks. However, their
application to medical imaging remains challenging due to the high variability
and complexity of medical data. Specifically, medical images often exhibit
significant domain shifts caused by various confounders, including equipment
differences, procedure artifacts, and imaging modes, which can lead to poor
generalization when models are applied to unseen domains. To address this
limitation, we propose Multimodal Causal-Driven Representation Learning
(MCDRL), a novel framework that integrates causal inference with the VLM to
tackle domain generalization in medical image segmentation. MCDRL is
implemented in two steps: first, it leverages CLIP's cross-modal capabilities
to identify candidate lesion regions and construct a confounder dictionary
through text prompts, specifically designed to represent domain-specific
variations; second, it trains a causal intervention network that utilizes this
dictionary to identify and eliminate the influence of these domain-specific
variations while preserving the anatomical structural information critical for
segmentation tasks. Extensive experiments demonstrate that MCDRL consistently
outperforms competing methods, yielding superior segmentation accuracy and
exhibiting robust generalizability.

</details>


### [26] [AU-IQA: A Benchmark Dataset for Perceptual Quality Assessment of AI-Enhanced User-Generated Content](https://arxiv.org/abs/2508.05016)
*Shushi Wang,Chunyi Li,Zicheng Zhang,Han Zhou,Wei Dong,Jun Chen,Guangtao Zhai,Xiaohong Liu*

Main category: cs.CV

TL;DR: 本文提出了一个专门针对AI增强用户生成内容（AI-UGC）图像的质量评价数据集AU-IQA，并系统评测了现有的图像质量评价方法。


<details>
  <summary>Details</summary>
Motivation: 目前虽然AI技术极大提升了用户生成内容（UGC）的视觉质量，但针对AI增强UGC的图像质量评价模型研究不足，现有模型无法有效衡量AI-UGC的感知质量，影响用户体验和后续方法进步。

Method: 构建了包含4800幅AI-UGC图像的数据集AU-IQA，涵盖超分辨率、低光增强、去噪三类典型增强方式；在此数据集上评测传统IQA方法与大规模多模态模型的表现，并进行综合性能分析。

Result: 通过实测，多数现有质量评价模型在AI-UGC图像上的效果有限，不同方法在不同增强类型下存在较大差异，显示AI-UGC具有独特的质量评价挑战。

Conclusion: 当前主流视觉质量评价模型在AI-UGC上的应用效果不理想，需进一步开发专门针对AI-UGC的质量评价工具；AU-IQA数据集为相关研究提供了重要的基准资源。

Abstract: AI-based image enhancement techniques have been widely adopted in various
visual applications, significantly improving the perceptual quality of
user-generated content (UGC). However, the lack of specialized quality
assessment models has become a significant limiting factor in this field,
limiting user experience and hindering the advancement of enhancement methods.
While perceptual quality assessment methods have shown strong performance on
UGC and AIGC individually, their effectiveness on AI-enhanced UGC (AI-UGC)
which blends features from both, remains largely unexplored. To address this
gap, we construct AU-IQA, a benchmark dataset comprising 4,800 AI-UGC images
produced by three representative enhancement types which include
super-resolution, low-light enhancement, and denoising. On this dataset, we
further evaluate a range of existing quality assessment models, including
traditional IQA methods and large multimodal models. Finally, we provide a
comprehensive analysis of how well current approaches perform in assessing the
perceptual quality of AI-UGC. The access link to the AU-IQA is
https://github.com/WNNGGU/AU-IQA-Dataset.

</details>


### [27] [Skin-SOAP: A Weakly Supervised Framework for Generating Structured SOAP Notes](https://arxiv.org/abs/2508.05019)
*Sadia Kamal,Tim Oates,Joy Wan*

Main category: cs.CV

TL;DR: 本文提出了一种弱监督的多模态框架skin-SOAP，可从皮肤病变图像和稀疏临床文本自动生成结构化SOAP病历笔记，减少人工标注和医生负担，性能与主流大模型相当。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌发病率高、医疗开支巨大，早期诊断和规范化文档对于提高生存率至关重要。现有SOAP笔记记录繁琐，人工作业加重医生负担且不可扩展，亟需自动化临床文档生成方法。

Method: 提出skin-SOAP框架，采用弱监督方法，从有限输入（包括图像和稀疏文本）自动生成结构化SOAP笔记。引入两项新指标：MedConceptEval（与医学概念语义对齐）和Clinical Coherence Score（与输入特征一致性）。

Result: 方法在核心临床相关性评价指标上，与GPT-4o、Claude和DeepSeek Janus Pro等主流大模型表现相当，能够有效生成高质量、结构化SOAP笔记。

Conclusion: skin-SOAP能够减少手工标注及医生记录工作量，同时具备临床可用性与可扩展性，是生成结构化医学文档的有效工具。

Abstract: Skin carcinoma is the most prevalent form of cancer globally, accounting for
over $8 billion in annual healthcare expenditures. Early diagnosis, accurate
and timely treatment are critical to improving patient survival rates. In
clinical settings, physicians document patient visits using detailed SOAP
(Subjective, Objective, Assessment, and Plan) notes. However, manually
generating these notes is labor-intensive and contributes to clinician burnout.
In this work, we propose skin-SOAP, a weakly supervised multimodal framework to
generate clinically structured SOAP notes from limited inputs, including lesion
images and sparse clinical text. Our approach reduces reliance on manual
annotations, enabling scalable, clinically grounded documentation while
alleviating clinician burden and reducing the need for large annotated data.
Our method achieves performance comparable to GPT-4o, Claude, and DeepSeek
Janus Pro across key clinical relevance metrics. To evaluate this clinical
relevance, we introduce two novel metrics MedConceptEval and Clinical Coherence
Score (CCS) which assess semantic alignment with expert medical concepts and
input features, respectively.

</details>


### [28] [A Novel Image Similarity Metric for Scene Composition Structure](https://arxiv.org/abs/2508.05037)
*Md Redwanul Haque,Manzur Murshed,Manoranjan Paul,Tsz-Kwan Lee*

Main category: cs.CV

TL;DR: 本文提出了一种名为SCSSIM的新指标，用于衡量生成式AI图像中场景结构的保真性，克服了传统方法对结构变化不敏感的问题。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI模型的进步，图像质量评估亟需突破仅依赖人类感知的局限。尤其生成模型需保证场景结构（对象之间及与背景的几何关系）稳定，而现有指标很难有效衡量结构完整性。

Method: 作者提出了SCSSIM（SCS Similarity Index Measure）这一新颖、训练无关的评估指标。SCSSIM通过对图像进行立方体层次划分，提取统计性结构信息，能够稳健地捕捉对象间（非单独对象）以及与背景的复杂结构关系。该方法不需繁琐的网络训练，避免了现有神经网络法的泛化和计算问题。

Result: 实验表明，SCSSIM对非结构性（非构图相关）失真高度不敏感，能准确反馈场景结构未发生变化。而一旦发生结构性失真，SCSSIM能单调、显著下降，精准标识结构更变优于以往指标。

Conclusion: SCSSIM有力补足了现有生成式图像质量评价手段，能有效辅助生成模型的开发与评测，对保障场景结构真实性和完整性具有重要意义。

Abstract: The rapid advancement of generative AI models necessitates novel methods for
evaluating image quality that extend beyond human perception. A critical
concern for these models is the preservation of an image's underlying Scene
Composition Structure (SCS), which defines the geometric relationships among
objects and the background, their relative positions, sizes, orientations, etc.
Maintaining SCS integrity is paramount for ensuring faithful and structurally
accurate GenAI outputs. Traditional image similarity metrics often fall short
in assessing SCS. Pixel-level approaches are overly sensitive to minor visual
noise, while perception-based metrics prioritize human aesthetic appeal,
neither adequately capturing structural fidelity. Furthermore, recent
neural-network-based metrics introduce training overheads and potential
generalization issues. We introduce the SCS Similarity Index Measure (SCSSIM),
a novel, analytical, and training-free metric that quantifies SCS preservation
by exploiting statistical measures derived from the Cuboidal hierarchical
partitioning of images, robustly capturing non-object-based structural
relationships. Our experiments demonstrate SCSSIM's high invariance to
non-compositional distortions, accurately reflecting unchanged SCS. Conversely,
it shows a strong monotonic decrease for compositional distortions, precisely
indicating when SCS has been altered. Compared to existing metrics, SCSSIM
exhibits superior properties for structural evaluation, making it an invaluable
tool for developing and evaluating generative models, ensuring the integrity of
scene composition.

</details>


### [29] [HAMoBE: Hierarchical and Adaptive Mixture of Biometric Experts for Video-based Person ReID](https://arxiv.org/abs/2508.05038)
*Yiyang Su,Yunping Shi,Feng Liu,Xiaoming Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频行人重识别框架HAMoBE，利用多层大模型特征，并模仿人类识别机制，通过分层独立建模和自适应整合多种生物特征，实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前视频行人重识别方法在匹配过程往往无法有效提取和选择最具区分性的视频特征，影响了识别准确率。

Method: 提出了HAMoBE框架，利用冻结的大模型（如CLIP）的多层特征，分层独立抽取外观、静态体型、动态步态等主要生物特征；引入双输入决策门控网络，根据输入情况动态调整各生物专家的贡献，以实现自适应特征融合。

Result: 在MEVID等基准数据集上进行了大量实验，结果表明该方法取得了显著性能提升，如Rank-1准确率提升13%。

Conclusion: HAMoBE通过自适应和分层的特征整合策略，有效提升了视频行人重识别的性能，为复杂环境下的监控和安防提供了更可靠的技术方案。

Abstract: Recently, research interest in person re-identification (ReID) has
increasingly focused on video-based scenarios, which are essential for robust
surveillance and security in varied and dynamic environments. However, existing
video-based ReID methods often overlook the necessity of identifying and
selecting the most discriminative features from both videos in a query-gallery
pair for effective matching. To address this issue, we propose a novel
Hierarchical and Adaptive Mixture of Biometric Experts (HAMoBE) framework,
which leverages multi-layer features from a pre-trained large model (e.g.,
CLIP) and is designed to mimic human perceptual mechanisms by independently
modeling key biometric features--appearance, static body shape, and dynamic
gait--and adaptively integrating them. Specifically, HAMoBE includes two
levels: the first level extracts low-level features from multi-layer
representations provided by the frozen large model, while the second level
consists of specialized experts focusing on long-term, short-term, and temporal
features. To ensure robust matching, we introduce a new dual-input decision
gating network that dynamically adjusts the contributions of each expert based
on their relevance to the input scenarios. Extensive evaluations on benchmarks
like MEVID demonstrate that our approach yields significant performance
improvements (e.g., +13.0% Rank-1 accuracy).

</details>


### [30] [Finding Needles in Images: Can Multimodal LLMs Locate Fine Details?](https://arxiv.org/abs/2508.05053)
*Parth Thakkar,Ankush Agarwal,Prasad Kasu,Pulkit Bansal,Chaitanya Devaguptapu*

Main category: cs.CV

TL;DR: 本文提出了NiM基准和Spot-IT方法，用于评估并提升多模态大模型在复杂文档中精细细节定位与理解的能力。Spot-IT在需要精确细节提取的场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大模型在文档理解任务中表现优异，但其在复杂文档中定位并推理细粒度细节的能力研究不足。实际需求往往需要模型精准查找和理解重要细节（如菜单营养信息、新闻中的免责声明等），类似于从大图中找“小针”。

Method: 作者提出了NiM这一新基准，涵盖报纸、菜单、讲座图片等多种真实文档，专门用于评估MLLM在细粒度任务上的能力。同时，作者提出Spot-IT方法，通过智能patch选择和类高斯注意机制，让模型模仿人类查找细节时的放大和聚焦行为，从而提升模型性能。

Result: 大规模实验评估了当前MLLM在精细文档理解任务上的能力与不足。Spot-IT方法相较现有方法在复杂布局、精确细节提取任务中有显著提升。

Conclusion: 提出的NiM和Spot-IT为复杂文档中细粒度信息提取提供了有效的评测基准和方法，Spot-IT显著提高了MLLM在相关任务上的表现。

Abstract: While Multi-modal Large Language Models (MLLMs) have shown impressive
capabilities in document understanding tasks, their ability to locate and
reason about fine-grained details within complex documents remains
understudied. Consider searching a restaurant menu for a specific nutritional
detail or identifying a disclaimer in a lengthy newspaper article tasks that
demand careful attention to small but significant details within a broader
narrative, akin to Finding Needles in Images (NiM). To address this gap, we
introduce NiM, a carefully curated benchmark spanning diverse real-world
documents including newspapers, menus, and lecture images, specifically
designed to evaluate MLLMs' capability in these intricate tasks. Building on
this, we further propose Spot-IT, a simple yet effective approach that enhances
MLLMs capability through intelligent patch selection and Gaussian attention,
motivated from how humans zoom and focus when searching documents. Our
extensive experiments reveal both the capabilities and limitations of current
MLLMs in handling fine-grained document understanding tasks, while
demonstrating the effectiveness of our approach. Spot-IT achieves significant
improvements over baseline methods, particularly in scenarios requiring precise
detail extraction from complex layouts.

</details>


### [31] [DualMat: PBR Material Estimation via Coherent Dual-Path Diffusion](https://arxiv.org/abs/2508.05060)
*Yifeng Huang,Zhang Chen,Yi Xu,Minh Hoai,Zhong Li*

Main category: cs.CV

TL;DR: 本文提出DualMat，一种双通道扩散框架，可在复杂光照条件下从单张图像估算PBR材质，大幅提升了反照率和金属-粗糙度的预测精度。


<details>
  <summary>Details</summary>
Motivation: 单张图片下PBR材质估计面临复杂光照干扰及各属性预测难兼顾的问题。现有方法在精度和推理效率上存在局限。

Method: DualMat采用双通道结构：一条路径在RGB潜空间中优化反照率，利用预训练视觉知识；另一条路径在专门的材质潜空间中精准估算金属度和粗糙度。两通道通过特征蒸馏保持一致性，并使用rectified flow提升推理效率。此外，该框架支持高分辨率与多视图输入，通过patch估计与跨视图注意力实现。

Result: 在Objaverse和真实世界数据集上，DualMat的反照率估计性能提升最高可达28%，金属-粗糙度错误降低39%，显著优于现有方法。

Conclusion: DualMat实现了复杂光照下PBR材质的高效高精度估算，可无缝集成至图像到3D管道，推动了材质提取自动化的发展。

Abstract: We present DualMat, a novel dual-path diffusion framework for estimating
Physically Based Rendering (PBR) materials from single images under complex
lighting conditions. Our approach operates in two distinct latent spaces: an
albedo-optimized path leveraging pretrained visual knowledge through RGB latent
space, and a material-specialized path operating in a compact latent space
designed for precise metallic and roughness estimation. To ensure coherent
predictions between the albedo-optimized and material-specialized paths, we
introduce feature distillation during training. We employ rectified flow to
enhance efficiency by reducing inference steps while maintaining quality. Our
framework extends to high-resolution and multi-view inputs through patch-based
estimation and cross-view attention, enabling seamless integration into
image-to-3D pipelines. DualMat achieves state-of-the-art performance on both
Objaverse and real-world data, significantly outperforming existing methods
with up to 28% improvement in albedo estimation and 39% reduction in
metallic-roughness prediction errors.

</details>


### [32] [Decoupling Continual Semantic Segmentation](https://arxiv.org/abs/2508.05065)
*Yifu Guo,Yuquan Lu,Wentao Zhang,Zishan Xu,Dexia Chen,Siyu Zhang,Yizhe Zhang,Ruixuan Wang*

Main category: cs.CV

TL;DR: 提出DecoupleCSS框架，通过解耦类别检测与分割任务，实现更有效的持续语义分割，显著提升旧知识保持和新类别学习能力，取得SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有持续语义分割方法容易在学习新类别时遗忘旧类别知识，且传统单阶段架构中类别与分割耦合加剧了干扰，导致难以均衡保持与适应性。

Method: 提出双阶段DecoupleCSS框架。第一阶段利用预训练文本与图像编码器（通过LoRA微调）进行类别特定特征提取和位置提示生成。第二阶段用Segment Anything Model (SAM)生成精确分割掩码，允许新旧类别共享分割知识。

Result: 在多个挑战性任务上，DecoupleCSS实现了持续语义分割的新SOTA表现，有效增强了模型对旧知识的保持与对新类的适应能力。

Conclusion: 解耦类别检测和分割流程能显著强化持续语义分割的保持与适应平衡。所提方法具备实际可用性，并推动CSS领域发展。

Abstract: Continual Semantic Segmentation (CSS) requires learning new classes without
forgetting previously acquired knowledge, addressing the fundamental challenge
of catastrophic forgetting in dense prediction tasks. However, existing CSS
methods typically employ single-stage encoder-decoder architectures where
segmentation masks and class labels are tightly coupled, leading to
interference between old and new class learning and suboptimal
retention-plasticity balance. We introduce DecoupleCSS, a novel two-stage
framework for CSS. By decoupling class-aware detection from class-agnostic
segmentation, DecoupleCSS enables more effective continual learning, preserving
past knowledge while learning new classes. The first stage leverages
pre-trained text and image encoders, adapted using LoRA, to encode
class-specific information and generate location-aware prompts. In the second
stage, the Segment Anything Model (SAM) is employed to produce precise
segmentation masks, ensuring that segmentation knowledge is shared across both
new and previous classes. This approach improves the balance between retention
and adaptability in CSS, achieving state-of-the-art performance across a
variety of challenging tasks. Our code is publicly available at:
https://github.com/euyis1019/Decoupling-Continual-Semantic-Segmentation.

</details>


### [33] [Automatic Image Colorization with Convolutional Neural Networks and Generative Adversarial Networks](https://arxiv.org/abs/2508.05068)
*Ruiyu Li,Changyuan Qiu,Hangrui Cao,Qihan Ren,Yuqing Qiu*

Main category: cs.CV

TL;DR: 这篇论文探讨了通过分类和对抗学习方法实现自动图像上色，并基于已有方法进行模型构建和改进，最终做出比较。


<details>
  <summary>Details</summary>
Motivation: 灰度图像上色在计算机视觉领域有广泛应用，比如色彩修复和动画自动上色，但其本质是高度不适定的问题，需要依赖场景语义和表面纹理等线索。

Method: 作者将自动图像上色问题从传统的回归任务转向分类和对抗学习范式，结合已有的研究成果，并对模型做出特定调整以适应研究场景，最后对不同方法进行对比。

Result: 论文未在摘要中详细给出实验或对比结果。

Conclusion: 作者尝试通过新范式改进图像上色的效果，利用更丰富的数据先验和学习方法，研究其在不同场景下的应用效果。

Abstract: Image colorization, the task of adding colors to grayscale images, has been
the focus of significant research efforts in computer vision in recent years
for its various application areas such as color restoration and automatic
animation colorization [15, 1]. The colorization problem is challenging as it
is highly ill-posed with two out of three image dimensions lost, resulting in
large degrees of freedom. However, semantics of the scene as well as the
surface texture could provide important cues for colors: the sky is typically
blue, the clouds are typically white and the grass is typically green, and
there are huge amounts of training data available for learning such priors
since any colored image could serve as a training data point [20].
  Colorization is initially formulated as a regression task[5], which ignores
the multi-modal nature of color prediction. In this project, we explore
automatic image colorization via classification and adversarial learning. We
will build our models on prior works, apply modifications for our specific
scenario and make comparisons.

</details>


### [34] [FLUX-Makeup: High-Fidelity, Identity-Consistent, and Robust Makeup Transfer via Diffusion Transformer](https://arxiv.org/abs/2508.05069)
*Jian Zhu,Shanyuan Liu,Liuzhuozheng Li,Yue Gong,He Wang,Bo Cheng,Yuhang Ma,Liebucha Wu,Xiaoyu Wu,Dawei Leng,Yuhui Yin,Yang Xu*

Main category: cs.CV

TL;DR: 本文提出了一种高保真、身份一致且鲁棒的无辅助成分人脸化妆迁移框架FLUX-Makeup，创新性地突破了现有依赖控制模块或损失函数的局限，实验效果优异。


<details>
  <summary>Details</summary>
Motivation: 现有化妆迁移方法依赖于辅助控制模块或特殊损失设计，这些组件不仅增加系统复杂度，还可能引入额外误差，影响迁移质量和身份一致性。简化流程并提升性能是亟待解决的问题。

Method: 提出FLUX-Makeup框架，直接利用源图像和参考图像对作为因条件输入，通过RefLoRAInjector轻量模块解耦和注入化妆特征，无需额外身份控制部件。同时，设计强大且可扩展的数据生成管道，生成高质量成对化妆数据集，用于更准确监督训练。

Result: FLUX-Makeup在多个场景中展现出强鲁棒性，化妆迁移质量及身份一致性超过现有所有方法，且数据集质量优于以往。

Conclusion: FLUX-Makeup为人脸化妆迁移领域带来了新的范式，简化了结构，提高了鲁棒性和迁移效果，有望推动化妆迁移技术在实际应用中的落地。

Abstract: Makeup transfer aims to apply the makeup style from a reference face to a
target face and has been increasingly adopted in practical applications.
Existing GAN-based approaches typically rely on carefully designed loss
functions to balance transfer quality and facial identity consistency, while
diffusion-based methods often depend on additional face-control modules or
algorithms to preserve identity. However, these auxiliary components tend to
introduce extra errors, leading to suboptimal transfer results. To overcome
these limitations, we propose FLUX-Makeup, a high-fidelity,
identity-consistent, and robust makeup transfer framework that eliminates the
need for any auxiliary face-control components. Instead, our method directly
leverages source-reference image pairs to achieve superior transfer
performance. Specifically, we build our framework upon FLUX-Kontext, using the
source image as its native conditional input. Furthermore, we introduce
RefLoRAInjector, a lightweight makeup feature injector that decouples the
reference pathway from the backbone, enabling efficient and comprehensive
extraction of makeup-related information. In parallel, we design a robust and
scalable data generation pipeline to provide more accurate supervision during
training. The paired makeup datasets produced by this pipeline significantly
surpass the quality of all existing datasets. Extensive experiments demonstrate
that FLUX-Makeup achieves state-of-the-art performance, exhibiting strong
robustness across diverse scenarios.

</details>


### [35] [AdaFusion: Prompt-Guided Inference with Adaptive Fusion of Pathology Foundation Models](https://arxiv.org/abs/2508.05084)
*Yuxiang Xiao,Yang Hu,Bin Li,Tianyang Zhang,Zexi Li,Huazhu Fu,Jens Rittscher,Kaixiang Yang*

Main category: cs.CV

TL;DR: 提出AdaFusion框架，通过融合多个病理基础模型（PFMs）的知识，提升下游任务性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前PFMs虽能从大规模未标注病理图像中自监督学习，但因训练数据和模型结构多样且不透明，存在潜在偏置，影响其泛化能力和透明性。

Method: AdaFusion是一种基于prompt引导的推理框架，动态整合来自多个PFMs的知识。具体方法是对各模型提取的切片级特征进行压缩和对齐，并通过轻量的注意力机制根据组织表型语境自适应地融合这些特征。

Result: 在三个真实世界基准（含治疗反应预测、肿瘤分级、空间基因表达推断）上，AdaFusion在分类和回归任务均优于任何单一PFM模型，并能解释各模型的生物语义专长。

Conclusion: AdaFusion能弥合PFMs间的差异，提升性能和可解释性，对于模型的归纳偏置做到更透明、可控。

Abstract: Pathology foundation models (PFMs) have demonstrated strong representational
capabilities through self-supervised pre-training on large-scale, unannotated
histopathology image datasets. However, their diverse yet opaque pretraining
contexts, shaped by both data-related and structural/training factors,
introduce latent biases that hinder generalisability and transparency in
downstream applications. In this paper, we propose AdaFusion, a novel
prompt-guided inference framework that, to our knowledge, is among the very
first to dynamically integrate complementary knowledge from multiple PFMs. Our
method compresses and aligns tile-level features from diverse models and
employs a lightweight attention mechanism to adaptively fuse them based on
tissue phenotype context. We evaluate AdaFusion on three real-world benchmarks
spanning treatment response prediction, tumour grading, and spatial gene
expression inference. Our approach consistently surpasses individual PFMs
across both classification and regression tasks, while offering interpretable
insights into each model's biosemantic specialisation. These results highlight
AdaFusion's ability to bridge heterogeneous PFMs, achieving both enhanced
performance and interpretability of model-specific inductive biases.

</details>


### [36] [PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human Video Generation](https://arxiv.org/abs/2508.05091)
*Jingxuan He,Busheng Su,Finn Wong*

Main category: cs.CV

TL;DR: 本文提出了PoseGen框架，实现了对单一主体、长时序连贯的视频生成，突破现有扩散模型视频长度和身份漂移的瓶颈。创新方法可实现高保真身份和精细动作控制，并拼接生成超长、无瑕疵视频。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在长视频生成时面临身份漂移和视频时长受限的问题，难以精确保持主体身份与动作连续性。作者旨在解决这一难题，提升视频生成质量及时长。

Method: 本方法引入了名为PoseGen的新框架。核心创新在于：1）使用in-context LoRA微调技术，将主体外观信息注入token级别，以实现身份保持；同时在通道级别引入动作（姿态）调控，实现精细动作控制；2）提出交错分段生成（interleaved segment generation）方法，通过共享KV缓存与特定过渡处理机制，实现视频片段无缝拼接，保证背景一致性和时间连续性。

Result: 在仅33小时视频数据集上训练，PoseGen在身份保真度、姿态准确性及生成超长、无伪影视频方面，均远超当前同类领先方法。

Conclusion: PoseGen突破了现有视频扩散模型在身份一致性、动作控制和时长限制方面的瓶颈，为无限时长、高质量主体视频生成提供了有效方案。

Abstract: Generating long, temporally coherent videos with precise control over subject
identity and motion is a formidable challenge for current diffusion models,
which often suffer from identity drift and are limited to short clips. We
introduce PoseGen, a novel framework that generates arbitrarily long videos of
a specific subject from a single reference image and a driving pose sequence.
Our core innovation is an in-context LoRA finetuning strategy that injects
subject appearance at the token level for identity preservation, while
simultaneously conditioning on pose information at the channel level for
fine-grained motion control. To overcome duration limits, PoseGen pioneers an
interleaved segment generation method that seamlessly stitches video clips
together, using a shared KV cache mechanism and a specialized transition
process to ensure background consistency and temporal smoothness. Trained on a
remarkably small 33-hour video dataset, extensive experiments show that PoseGen
significantly outperforms state-of-the-art methods in identity fidelity, pose
accuracy, and its unique ability to produce coherent, artifact-free videos of
unlimited duration.

</details>


### [37] [Sculpting Margin Penalty: Intra-Task Adapter Merging and Classifier Calibration for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2508.05094)
*Liang Bai,Hong Song,Jinfu Li,Yucong Lin,Jingfan Fan,Tianyu Fu,Danni Ai,Deqiang Xiao,Jian Yang*

Main category: cs.CV

TL;DR: 本论文针对小样本类增量学习（FSCIL）中由于数据隐私和获取成本导致训练数据不足，进而带来性能下降的问题，提出了一种新方法SMP，通过在训练过程中引入多阶段margin penalty，有效提升了模型的泛化能力和基类辨别力，并取得了当前最佳实验结果。


<details>
  <summary>Details</summary>
Motivation: 实际应用中数据受限导致FSCIL模型难以兼顾基类辨别力和新类泛化能力，且增量学习阶段原始数据受限，决策边界容易模糊，因此亟需新的方法提升FSCIL的表现。

Method: 提出SMP方法，在参数高效微调范式下，基任务阶段利用Margin-aware Intra-task Adapter Merging (MIAM)机制，训练两个低秩adapter（一个带margin penalty提升基类辨别力，一个无margin提升新类泛化），随后自适应融合。在增量任务阶段，采用Margin Penalty-based Classifier Calibration (MPCC)，通过对所有已见类别特征加margin penalty精细调整分类器边界。

Result: 在CIFAR100、ImageNet-R和CUB200三个数据集上进行实验，所提SMP方法在FSCIL任务上均取得了当前最优性能，并在基类与新类表现间取得了更优平衡。

Conclusion: SMP方法能有效提升FSCIL任务中模型对基类和新类的兼容能力，是实现数据受限场景下高效类增量学习的有力工具。

Abstract: Real-world applications often face data privacy constraints and high
acquisition costs, making the assumption of sufficient training data in
incremental tasks unrealistic and leading to significant performance
degradation in class-incremental learning. Forward-compatible learning, which
prospectively prepares for future tasks during base task training, has emerged
as a promising solution for Few-Shot Class-Incremental Learning (FSCIL).
However, existing methods still struggle to balance base-class discriminability
and new-class generalization. Moreover, limited access to original data during
incremental tasks often results in ambiguous inter-class decision boundaries.
To address these challenges, we propose SMP (Sculpting Margin Penalty), a novel
FSCIL method that strategically integrates margin penalties at different stages
within the parameter-efficient fine-tuning paradigm. Specifically, we introduce
the Margin-aware Intra-task Adapter Merging (MIAM) mechanism for base task
learning. MIAM trains two sets of low-rank adapters with distinct
classification losses: one with a margin penalty to enhance base-class
discriminability, and the other without margin constraints to promote
generalization to future new classes. These adapters are then adaptively merged
to improve forward compatibility. For incremental tasks, we propose a Margin
Penalty-based Classifier Calibration (MPCC) strategy to refine decision
boundaries by fine-tuning classifiers on all seen classes' embeddings with a
margin penalty. Extensive experiments on CIFAR100, ImageNet-R, and CUB200
demonstrate that SMP achieves state-of-the-art performance in FSCIL while
maintaining a better balance between base and new classes.

</details>


### [38] [AHDMIL: Asymmetric Hierarchical Distillation Multi-Instance Learning for Fast and Accurate Whole-Slide Image Classification](https://arxiv.org/abs/2508.05114)
*Jiuyang Dong,Jiahan Li,Junjun Jiang,Kui Jiang,Yongbing Zhang*

Main category: cs.CV

TL;DR: 该论文提出了AHDMIL框架，实现了更快且更准确的病理图像多实例分类，显著提升了推理速度与分类表现。


<details>
  <summary>Details</summary>
Motivation: 多实例学习（MIL）虽然在病理图像分类中取得了成功，但由于每张全景切片（WSI）需处理成千上万个patch，推理成本极高。提升推理效率与保持精度成为迫切需求。

Method: 提出AHDMIL框架，包含两个组件：1）动态多实例网络（DMIN），针对高分辨率WSI分类并用注意力筛出无关patch；2）轻量化的双分支实例预筛选网络（DB-LIPN），对低分辨率patch预测相关性。采用两步训练：先用DMIN生成注意力分数进行自蒸馏，再用这些分数引导DB-LIPN做非对称蒸馏，筛选patch后反哺高分辨率模型精调与推理。额外引入基于Chebyshev多项式的CKA分类器提升性能。

Result: 在四个公开数据集上，AHDMIL在分类性能和推理速度上都优于现有方法。如在Camelyon16数据集上，准确率提升5.3%，推理加速1.2倍。其他指标（AUC、准确率、F1、Brier分数）也均有提升，平均推理速度提升1.2到2.1倍。

Conclusion: AHDMIL显著提高了病理图像多实例分类的效率和准确度，为大规模WSI自动筛查提供更实用的方法，具备广泛应用前景。

Abstract: Although multi-instance learning (MIL) has succeeded in pathological image
classification, it faces the challenge of high inference costs due to the need
to process thousands of patches from each gigapixel whole slide image (WSI). To
address this, we propose AHDMIL, an Asymmetric Hierarchical Distillation
Multi-Instance Learning framework that enables fast and accurate classification
by eliminating irrelevant patches through a two-step training process. AHDMIL
comprises two key components: the Dynamic Multi-Instance Network (DMIN), which
operates on high-resolution WSIs, and the Dual-Branch Lightweight Instance
Pre-screening Network (DB-LIPN), which analyzes corresponding low-resolution
counterparts. In the first step, self-distillation (SD), DMIN is trained for
WSI classification while generating per-instance attention scores to identify
irrelevant patches. These scores guide the second step, asymmetric distillation
(AD), where DB-LIPN learns to predict the relevance of each low-resolution
patch. The relevant patches predicted by DB-LIPN have spatial correspondence
with patches in high-resolution WSIs, which are used for fine-tuning and
efficient inference of DMIN. In addition, we design the first
Chebyshev-polynomial-based Kolmogorov-Arnold (CKA) classifier in computational
pathology, which improves classification performance through learnable
activation layers. Extensive experiments on four public datasets demonstrate
that AHDMIL consistently outperforms previous state-of-the-art methods in both
classification performance and inference speed. For example, on the Camelyon16
dataset, it achieves a relative improvement of 5.3% in accuracy and accelerates
inference by 1.2.times. Across all datasets, area under the curve (AUC),
accuracy, f1 score, and brier score show consistent gains, with average
inference speedups ranging from 1.2 to 2.1 times. The code is available.

</details>


### [39] [Latent Expression Generation for Referring Image Segmentation and Grounding](https://arxiv.org/abs/2508.05123)
*Seonghoon Yu,Joonbeom Hong,Joonseok Lee,Jeany Son*

Main category: cs.CV

TL;DR: 该论文提出针对视觉指代任务的新框架，通过生成多个潜在表达增加文本与图像的匹配能力，从而提升目标定位准确性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉指代方法只利用单一文本输入，难以覆盖丰富视觉信息，导致对相似目标的识别易出错，因此需要增加文本表达的多样性。

Method: 提出通过引入subject distributor和visual concept injector模块，从单一文本输入中生成多种潜在表达，并用正边界对比学习方式对齐这些表达与原始文本，同时保持细微差异。此方法增强了文本对目标独特视觉线索的捕捉能力。

Result: 该方法在RIS、REC等多项主流视觉指代任务基准和广义表达分割（GRES）基准上均优于当前主流方法。

Conclusion: 多潜在表达与视觉概念注入提升了视觉指代任务的准确性和泛化能力，对丰富目标识别线索有积极作用。

Abstract: Visual grounding tasks, such as referring image segmentation (RIS) and
referring expression comprehension (REC), aim to localize a target object based
on a given textual description. The target object in an image can be described
in multiple ways, reflecting diverse attributes such as color, position, and
more. However, most existing methods rely on a single textual input, which
captures only a fraction of the rich information available in the visual
domain. This mismatch between rich visual details and sparse textual cues can
lead to the misidentification of similar objects. To address this, we propose a
novel visual grounding framework that leverages multiple latent expressions
generated from a single textual input by incorporating complementary visual
details absent from the original description. Specifically, we introduce
subject distributor and visual concept injector modules to embed both
shared-subject and distinct-attributes concepts into the latent
representations, thereby capturing unique and target-specific visual cues. We
also propose a positive-margin contrastive learning strategy to align all
latent expressions with the original text while preserving subtle variations.
Experimental results show that our method not only outperforms state-of-the-art
RIS and REC approaches on multiple benchmarks but also achieves outstanding
performance on the generalized referring expression segmentation (GRES)
benchmark.

</details>


### [40] [FedGIN: Federated Learning with Dynamic Global Intensity Non-linear Augmentation for Organ Segmentation using Multi-modal Images](https://arxiv.org/abs/2508.05137)
*Sachin Dudda Nagaraju,Ashkan Moradi,Bendik Skarre Abrahamsen,Mattijs Elschot*

Main category: cs.CV

TL;DR: 本文提出了一种名为FedGIN的联邦学习框架，用于兼顾多模态医学图像分割的准确性及隐私保护。通过引入全局强度非线性（GIN）增强模块，有效提升了模型在不同模态间的泛化能力和分割表现。


<details>
  <summary>Details</summary>
Motivation: 当前医学图像存在多模态（如CT与MRI），但由于数据稀缺、跨模态分布差异、隐私保护等问题，难以训练能泛化到多种模态的统一分割模型。解决这一问题可以简化临床流程，并减少对单独模态模型的依赖。

Method: 提出FedGIN框架，通过联邦学习方式，各参与方不共享原始数据，仅共享模型参数；在本地训练阶段集成GIN增强模块，用于标准化和对齐不同模态下的图像强度分布，从而提升模型对不同模态的泛化能力。

Result: 在有限数据场景下，FedGIN相较于不含GIN的联邦学习基线，MRI测试集3D Dice得分提升12-18%，且优于本地基线；在完整数据场景下，FedGIN分割效果接近中心化训练，较MRI-only基线提升30%，较CT-only基线提升10%。

Conclusion: FedGIN能够在充分保证数据隐私的前提下，实现高效的多模态医学图像分割，具备较强的跨模态泛化能力，为实际临床应用带来了可行的解决方案。

Abstract: Medical image segmentation plays a crucial role in AI-assisted diagnostics,
surgical planning, and treatment monitoring. Accurate and robust segmentation
models are essential for enabling reliable, data-driven clinical decision
making across diverse imaging modalities. Given the inherent variability in
image characteristics across modalities, developing a unified model capable of
generalizing effectively to multiple modalities would be highly beneficial.
This model could streamline clinical workflows and reduce the need for
modality-specific training. However, real-world deployment faces major
challenges, including data scarcity, domain shift between modalities (e.g., CT
vs. MRI), and privacy restrictions that prevent data sharing. To address these
issues, we propose FedGIN, a Federated Learning (FL) framework that enables
multimodal organ segmentation without sharing raw patient data. Our method
integrates a lightweight Global Intensity Non-linear (GIN) augmentation module
that harmonizes modality-specific intensity distributions during local
training. We evaluated FedGIN using two types of datasets: an imputed dataset
and a complete dataset. In the limited dataset scenario, the model was
initially trained using only MRI data, and CT data was added to assess its
performance improvements. In the complete dataset scenario, both MRI and CT
data were fully utilized for training on all clients. In the limited-data
scenario, FedGIN achieved a 12 to 18% improvement in 3D Dice scores on MRI test
cases compared to FL without GIN and consistently outperformed local baselines.
In the complete dataset scenario, FedGIN demonstrated near-centralized
performance, with a 30% Dice score improvement over the MRI-only baseline and a
10% improvement over the CT-only baseline, highlighting its strong
cross-modality generalization under privacy constraints.

</details>


### [41] [Deep Learning-based Animal Behavior Analysis: Insights from Mouse Chronic Pain Models](https://arxiv.org/abs/2508.05138)
*Yu-Hsi Chen,Wei-Hsin Chen,Chien-Yao Wang,Hong-Yuan Mark Liao,James C. Liao,Chien-Chang Chen*

Main category: cs.CV

TL;DR: 本研究提出了一种无需人工行为标签、自动提取慢性疼痛相关行为特征的新方法，并显著优于现有手工和自动化方法。


<details>
  <summary>Details</summary>
Motivation: 现有小鼠慢性疼痛行为评估方法依赖人工手动标签，且对哪些行为能最好地反映慢性疼痛缺乏清晰认识，导致难以捕捉慢性疼痛中潜移默化且持续的行为变化。

Method: 开发了一套自动化框架，借助通用行为空间投影工具，从原始视频自动提取动作特征，避免人为标签带来的偏见。同时，收集了一套涵盖神经性和炎症性疼痛多时间点进展的小鼠疼痛行为数据集。

Result: 在15类疼痛分类任务上新方法准确率为48.41%，高于专家（21.33%）和B-SOiD（30.52%）；三分类情境下达73.1%，远超专家（48%）和B-SOiD（58.43%）。此外在Gabapentin药物零样本测试中也能反映不同疼痛类型的药效差异，且与既往文献一致。

Conclusion: 本方法显著提升了小鼠慢性疼痛自动行为分析的准确性，有助于揭示疼痛机制和药物开发，具备临床应用潜力。

Abstract: Assessing chronic pain behavior in mice is critical for preclinical studies.
However, existing methods mostly rely on manual labeling of behavioral
features, and humans lack a clear understanding of which behaviors best
represent chronic pain. For this reason, existing methods struggle to
accurately capture the insidious and persistent behavioral changes in chronic
pain. This study proposes a framework to automatically discover features
related to chronic pain without relying on human-defined action labels. Our
method uses universal action space projector to automatically extract mouse
action features, and avoids the potential bias of human labeling by retaining
the rich behavioral information in the original video. In this paper, we also
collected a mouse pain behavior dataset that captures the disease progression
of both neuropathic and inflammatory pain across multiple time points. Our
method achieves 48.41\% accuracy in a 15-class pain classification task,
significantly outperforming human experts (21.33\%) and the widely used method
B-SOiD (30.52\%). Furthermore, when the classification is simplified to only
three categories, i.e., neuropathic pain, inflammatory pain, and no pain, then
our method achieves an accuracy of 73.1\%, which is notably higher than that of
human experts (48\%) and B-SOiD (58.43\%). Finally, our method revealed
differences in drug efficacy for different types of pain on zero-shot
Gabapentin drug testing, and the results were consistent with past drug
efficacy literature. This study demonstrates the potential clinical application
of our method, which can provide new insights into pain research and related
drug development.

</details>


### [42] [MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs](https://arxiv.org/abs/2508.05502)
*Yufei Gao,Jiaying Fei,Nuo Chen,Ruirui Chen,Guohang Yan,Yunshi Lan,Botian Shi*

Main category: cs.CV

TL;DR: 本文提出通过双源策略提升多模态大模型（MLLM）在低资源语言下的表现，构建MELLA数据集，并验证其在八种语言上的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大模型在高资源语言下表现优异，但在低资源语言下效果显著下降。现有方法大多局限于文本模态，或单纯依赖机器翻译，缺乏多模态丰富性和文化扎根性，难以服务低资源语言用户。

Method: 提出以语言能力和文化扎根性为双重目标，采用双源策略分别采集本地网页alt-text（关注文化）及MLLM自动生成说明（关注语言能力），并据此构建多模态多语言数据集MELLA。

Result: MELLA数据集微调后，在八种低资源语言和各种MLLM骨干模型上均取得整体性能提升，模型可生成更具信息量与文化底蕴的描述。

Conclusion: 提出的新型数据采集策略和MELLA数据集能够在提升低资源语言多模态大模型语言能力的同时，显著增强其文化相关性和表达质量。

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable performance in
high-resource languages. However, their effectiveness diminishes significantly
in the contexts of low-resource languages. Current multilingual enhancement
methods are often limited to text modality or rely solely on machine
translation. While such approaches help models acquire basic linguistic
capabilities and produce "thin descriptions", they neglect the importance of
multimodal informativeness and cultural groundedness, both of which are crucial
for serving low-resource language users effectively. To bridge this gap, in
this study, we identify two significant objectives for a truly effective MLLM
in low-resource language settings, namely 1) linguistic capability and 2)
cultural groundedness, placing special emphasis on cultural awareness. To
achieve these dual objectives, we propose a dual-source strategy that guides
the collection of data tailored to each goal, sourcing native web alt-text for
culture and MLLM-generated captions for linguistics. As a concrete
implementation, we introduce MELLA, a multimodal, multilingual dataset.
Experiment results show that after fine-tuning on MELLA, there is a general
performance improvement for the eight languages on various MLLM backbones, with
models producing "thick descriptions". We verify that the performance gains are
from both cultural knowledge enhancement and linguistic capability enhancement.
Our dataset can be found at https://opendatalab.com/applyMultilingualCorpus.

</details>


### [43] [Rotation Equivariant Arbitrary-scale Image Super-Resolution](https://arxiv.org/abs/2508.05160)
*Qi Xie,Jiahong Fu,Zongben Xu,Deyu Meng*

Main category: cs.CV

TL;DR: 本文提出了一种具备旋转等变性的任意尺度图像超分辨率（ASISR）方法，通过结构性改进显著提升了高分辨率图像恢复的几何结构保真度。


<details>
  <summary>Details</summary>
Motivation: 当前ASISR方法在低分辨率输入中常因几何图案（如纹理、边缘、形状）的变形，导致高分辨率恢复出现失真和伪影。已有研究表明，嵌入旋转等变性有助于保持原始图案的方向性和结构完整性，因此亟需构建具备旋转等变性的ASISR方法。

Method: 本文重新设计了ASISR网络的编码器和隐式神经表示（INR）模块，使其具备端到端的旋转等变性，并对其等变性误差进行了理论分析。此外，所提方法支持以插件方式集成到现有ASISR框架中。

Result: 通过在模拟和真实数据集上的实验，验证了所提方法在结构还原和细节呈现方面优于现有方法，并成功增强了现有ASISR模型的性能。

Conclusion: 该方法首次实现了端到端的旋转等变ASISR网络，不仅提升了恢复图像的结构与方向保真度，还能灵活应用于现有的任意尺度超分模型，具有良好的实用价值和推广潜力。

Abstract: The arbitrary-scale image super-resolution (ASISR), a recent popular topic in
computer vision, aims to achieve arbitrary-scale high-resolution recoveries
from a low-resolution input image. This task is realized by representing the
image as a continuous implicit function through two fundamental modules, a
deep-network-based encoder and an implicit neural representation (INR) module.
Despite achieving notable progress, a crucial challenge of such a highly
ill-posed setting is that many common geometric patterns, such as repetitive
textures, edges, or shapes, are seriously warped and deformed in the
low-resolution images, naturally leading to unexpected artifacts appearing in
their high-resolution recoveries. Embedding rotation equivariance into the
ASISR network is thus necessary, as it has been widely demonstrated that this
enhancement enables the recovery to faithfully maintain the original
orientations and structural integrity of geometric patterns underlying the
input image. Motivated by this, we make efforts to construct a rotation
equivariant ASISR method in this study. Specifically, we elaborately redesign
the basic architectures of INR and encoder modules, incorporating intrinsic
rotation equivariance capabilities beyond those of conventional ASISR networks.
Through such amelioration, the ASISR network can, for the first time, be
implemented with end-to-end rotational equivariance maintained from input to
output. We also provide a solid theoretical analysis to evaluate its intrinsic
equivariance error, demonstrating its inherent nature of embedding such an
equivariance structure. The superiority of the proposed method is substantiated
by experiments conducted on both simulated and real datasets. We also validate
that the proposed framework can be readily integrated into current ASISR
methods in a plug \& play manner to further enhance their performance.

</details>


### [44] [X-MoGen: Unified Motion Generation across Humans and Animals](https://arxiv.org/abs/2508.05162)
*Xuan Wang,Kai Ruan,Liyang Qian,Zhizhi Guo,Chang Su,Gaoang Wang*

Main category: cs.CV

TL;DR: 本文提出了X-MoGen，这是首个能同时对人类和动物进行基于文本驱动的动作生成统一框架，并发布了覆盖115种物种的大规模数据集UniMo4D。该方法综合了图变分自编码器、统一运动嵌入空间和骨架一致性模块，实现了跨物种的高质量动作生成，实验显示超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的动作生成方法往往只针对单一物种（如人类或动物），缺乏通用于不同物种的统一方法。由于不同物种间形态结构差异大，现有跨物种建模难以保证动作合理性和泛化能力。因此，作者希望提出统一且高效的跨物种动作生成方法，提升动作多样性和应用范围。

Method: X-MoGen框架分为两阶段：第一阶段用条件图变分自编码器学习标准T姿势的先验，同时用另一个自编码器将动作编码进统一的潜在空间，并用形态损失对其正则化；第二阶段进行掩码动作建模，利用文本描述生成潜在动作嵌入。同时引入骨架形态一致性模块，以提升不同物种间动作的合理性。并构建了UniMo4D大数据集作为训练和评测基础。

Result: 在UniMo4D数据集上的大量实验表明，X-MoGen在已见和未见物种上都显著超越了现有同类方法，无论在动作的合理性、可泛化性还是一致性方面均有提升。

Conclusion: X-MoGen实现了面向人类和动物的跨物种文本驱动动作生成，解决了形态差异带来的挑战，并获得了优越于现有方法的性能，为虚拟现实等领域迈向泛化和统一建模提供了新途径。

Abstract: Text-driven motion generation has attracted increasing attention due to its
broad applications in virtual reality, animation, and robotics. While existing
methods typically model human and animal motion separately, a joint
cross-species approach offers key advantages, such as a unified representation
and improved generalization. However, morphological differences across species
remain a key challenge, often compromising motion plausibility. To address
this, we propose \textbf{X-MoGen}, the first unified framework for
cross-species text-driven motion generation covering both humans and animals.
X-MoGen adopts a two-stage architecture. First, a conditional graph variational
autoencoder learns canonical T-pose priors, while an autoencoder encodes motion
into a shared latent space regularized by morphological loss. In the second
stage, we perform masked motion modeling to generate motion embeddings
conditioned on textual descriptions. During training, a morphological
consistency module is employed to promote skeletal plausibility across species.
To support unified modeling, we construct \textbf{UniMo4D}, a large-scale
dataset of 115 species and 119k motion sequences, which integrates human and
animal motions under a shared skeletal topology for joint training. Extensive
experiments on UniMo4D demonstrate that X-MoGen outperforms state-of-the-art
methods on both seen and unseen species.

</details>


### [45] [Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and Vision](https://arxiv.org/abs/2508.05606)
*Luozheng Qin,Jia Gong,Yuqing Sun,Tianjiao Li,Mengping Yang,Xiaomeng Yang,Chao Qu,Zhiyu Tan,Hao Li*

Main category: cs.CV

TL;DR: 本文提出Uni-CoT统一Chain-of-Thought多模态推理框架，有效提升多模态任务的推理能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统Chain-of-Thought（CoT）推理在大语言模型取得成功，但在视觉-语言推理任务上难以建模视觉状态的变化。现有方案受限于视觉转移建模能力或架构碎片化，导致推理连贯性差。

Method: 提出Uni-CoT统一框架，利用单一模型同时支持图像理解和生成，通过两级推理范式（宏观CoT进行任务规划，微观CoT负责子任务执行）降低计算成本。采用结构化训练策略，宏观层采用图文交错监督，微观层联合多任务目标，实现高效训练。

Result: 在推理驱动的图像生成（WISE）与编辑（RISE和KRIS）基准上，Uni-CoT获得了SOTA表现，并展现出较强的泛化能力。全部实验可在8张A100-80GB显卡上高效完成。

Conclusion: Uni-CoT解决了视觉-语言推理中连贯性和高成本难题，是多模态推理任务中的有前景方案。

Abstract: Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large
Language Models (LLMs) by decomposing complex tasks into simpler, sequential
subtasks. However, extending CoT to vision-language reasoning tasks remains
challenging, as it often requires interpreting transitions of visual states to
support reasoning. Existing methods often struggle with this due to limited
capacity of modeling visual state transitions or incoherent visual trajectories
caused by fragmented architectures.
  To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought
framework that enables coherent and grounded multimodal reasoning within a
single unified model. The key idea is to leverage a model capable of both image
understanding and generation to reason over visual content and model evolving
visual states. However, empowering a unified model to achieve that is
non-trivial, given the high computational cost and the burden of training. To
address this, Uni-CoT introduces a novel two-level reasoning paradigm: A
Macro-Level CoT for high-level task planning and A Micro-Level CoT for subtask
execution. This design significantly reduces the computational overhead.
Furthermore, we introduce a structured training paradigm that combines
interleaved image-text supervision for macro-level CoT with multi-task
objectives for micro-level CoT. Together, these innovations allow Uni-CoT to
perform scalable and coherent multi-modal reasoning. Furthermore, thanks to our
design, all experiments can be efficiently completed using only 8 A100 GPUs
with 80GB VRAM each. Experimental results on reasoning-driven image generation
benchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT
demonstrates SOTA performance and strong generalization, establishing Uni-CoT
as a promising solution for multi-modal reasoning. Project Page and Code:
https://sais-fuxi.github.io/projects/uni-cot/

</details>


### [46] [PhysPatch: A Physically Realizable and Transferable Adversarial Patch Attack for Multimodal Large Language Models-based Autonomous Driving Systems](https://arxiv.org/abs/2508.05167)
*Qi Guo,Xiaojun Jia,Shanmin Pang,Simeng Qin,Lin Wang,Ju Jia,Yang Liu,Qing Guo*

Main category: cs.CV

TL;DR: 本文提出了一种针对多模态大模型(Multimodal Large Language Models, MLLMs)在自动驾驶系统中的可迁移物理对抗补丁攻击方法PhysPatch, 显著提升了现有攻击手段在真实场景与MLLM体系下的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大模型因其视觉-语言推理能力在自动驾驶中应用日益广泛，安全性问题随之凸显。已有针对物体检测的对抗补丁方法难以高效攻击复杂MLLM架构，无法应对其推理特性，有必要提出更强大、适用于现实物理场景的对抗攻击框架。

Method: PhysPatch设计了对补丁位置、形状、内容的联合优化流程。方法包括语义感知掩码初始化实现补丁现实合理放置；采用基于SVD的局部对齐损失与补丁引导的裁剪-缩放提升可迁移性；利用基于势场的掩码精细化进一步提高攻击效果。

Result: 通过针对开源、商用和具推理能力的MLLM自动驾驶系统的广泛实验，PhysPatch在误导目标系统感知和规划输出方面，显著优于已有对抗补丁方法，并能稳定将补丁部署于物理可行且现实的自动驾驶场景区域。

Conclusion: PhysPatch证明了在MLLM主导自动驾驶系统中进行高效物理可实现的对抗补丁攻击的可行性与优越性，为未来自动驾驶安全评估、攻防研究提供了新工具和思路。

Abstract: Multimodal Large Language Models (MLLMs) are becoming integral to autonomous
driving (AD) systems due to their strong vision-language reasoning
capabilities. However, MLLMs are vulnerable to adversarial attacks,
particularly adversarial patch attacks, which can pose serious threats in
real-world scenarios. Existing patch-based attack methods are primarily
designed for object detection models and perform poorly when transferred to
MLLM-based systems due to the latter's complex architectures and reasoning
abilities. To address these limitations, we propose PhysPatch, a physically
realizable and transferable adversarial patch framework tailored for MLLM-based
AD systems. PhysPatch jointly optimizes patch location, shape, and content to
enhance attack effectiveness and real-world applicability. It introduces a
semantic-based mask initialization strategy for realistic placement, an
SVD-based local alignment loss with patch-guided crop-resize to improve
transferability, and a potential field-based mask refinement method. Extensive
experiments across open-source, commercial, and reasoning-capable MLLMs
demonstrate that PhysPatch significantly outperforms prior methods in steering
MLLM-based AD systems toward target-aligned perception and planning outputs.
Moreover, PhysPatch consistently places adversarial patches in physically
feasible regions of AD scenes, ensuring strong real-world applicability and
deployability.

</details>


### [47] [Test-Time Reinforcement Learning for GUI Grounding via Region Consistency](https://arxiv.org/abs/2508.05615)
*Yong Du,Yuchen Yan,Fei Tang,Zhengxi Lu,Chang Zong,Weiming Lu,Shengpei Jiang,Yongliang Shen*

Main category: cs.CV

TL;DR: 本文提出了一种新的GUI定位方法（GUI-RC和GUI-RCPO），通过利用模型多次预测的一致性提升定位精度，无需额外标注即可在多种架构上提升准确率。


<details>
  <summary>Details</summary>
Motivation: 现有GUI grounding（界面定位）方法依赖大量有标签数据，像素级注释成本高昂且难以获得，限制了方法的扩展性与普适性。作者观察到模型多次预测的空间重叠信息可作为隐式置信度信号，从而启发了提升无监督定位准确度的新方法。

Method: 1）提出GUI-RC：在测试时对同一目标多次采样预测结果，通过空间投票格寻找预测一致性最高的区域，提高定位置信度。2）提出GUI-RCPO：将预测一致性转换为强化学习奖励，让模型测试时在无标注数据上自我优化预测结果。无需重新训练，无需额外标签。

Result: GUI-RC在多个主流架构和ScreenSpot基准上提升了2-3%的准确率。例如，Qwen2.5-VL-3B-Instruct模型从80.11%提升至83.57%；采用GUI-RCPO继续提升到85.14%。

Conclusion: 基于一致性的测试时标度和无监督优化能高效提升GUI grounding任务性能，减少对有标签数据的依赖，为发展更健壮和高效的数据驱动GUI智能体开辟了新道路。

Abstract: Graphical User Interface (GUI) grounding, the task of mapping natural
language instructions to precise screen coordinates, is fundamental to
autonomous GUI agents. While existing methods achieve strong performance
through extensive supervised training or reinforcement learning with labeled
rewards, they remain constrained by the cost and availability of pixel-level
annotations. We observe that when models generate multiple predictions for the
same GUI element, the spatial overlap patterns reveal implicit confidence
signals that can guide more accurate localization. Leveraging this insight, we
propose GUI-RC (Region Consistency), a test-time scaling method that constructs
spatial voting grids from multiple sampled predictions to identify consensus
regions where models show highest agreement. Without any training, GUI-RC
improves accuracy by 2-3% across various architectures on ScreenSpot
benchmarks. We further introduce GUI-RCPO (Region Consistency Policy
Optimization), which transforms these consistency patterns into rewards for
test-time reinforcement learning. By computing how well each prediction aligns
with the collective consensus, GUI-RCPO enables models to iteratively refine
their outputs on unlabeled data during inference. Extensive experiments
demonstrate the generality of our approach: GUI-RC boosts
Qwen2.5-VL-3B-Instruct from 80.11% to 83.57% on ScreenSpot-v2, while GUI-RCPO
further improves it to 85.14% through self-supervised optimization. Our
approach reveals the untapped potential of test-time scaling and test-time
reinforcement learning for GUI grounding, offering a promising path toward more
robust and data-efficient GUI agents.

</details>


### [48] [Multi-tracklet Tracking for Generic Targets with Adaptive Detection Clustering](https://arxiv.org/abs/2508.05172)
*Zewei Wu,Longhao Wang,Cui Wang,César Teixeira,Wei Ke,Zhang Xiong*

Main category: cs.CV

TL;DR: 本文提出了一种名为Multi-Tracklet Tracking (MTT)的多目标跟踪方法，通过将检测结果灵活生成短时轨迹片段(Tracklet)并进行多线索关联，实现对未知类别目标的稳健追踪，在公共基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的多目标跟踪方法在面对未知类别、低置信度检测、弱动作和外观约束以及长时间遮挡时表现不佳。作者希望解决这些现实场景中的难题，提升跟踪系统的泛化能力和鲁棒性。

Method: 提出了一种Tracklet增强跟踪器——MTT。该方法首先基于时空相关性，将检测结果自适应聚类成短时Tracklet；然后结合位置和外观等信息，进行多轨迹片段关联与划分，从而减轻长时间关联中的误差传播。

Result: 在通用多目标跟踪基准数据集上进行大量实验证明，MTT框架比现有方法具备更强的竞争力。

Conclusion: 所提出的MTT方法能够有效应对未知类别、多目标跟踪中的多种挑战，实现更稳健和准确的目标轨迹管理。

Abstract: Tracking specific targets, such as pedestrians and vehicles, has been the
focus of recent vision-based multitarget tracking studies. However, in some
real-world scenarios, unseen categories often challenge existing methods due to
low-confidence detections, weak motion and appearance constraints, and
long-term occlusions. To address these issues, this article proposes a
tracklet-enhanced tracker called Multi-Tracklet Tracking (MTT) that integrates
flexible tracklet generation into a multi-tracklet association framework. This
framework first adaptively clusters the detection results according to their
short-term spatio-temporal correlation into robust tracklets and then estimates
the best tracklet partitions using multiple clues, such as location and
appearance over time to mitigate error propagation in long-term association.
Finally, extensive experiments on the benchmark for generic multiple object
tracking demonstrate the competitiveness of the proposed framework.

</details>


### [49] [SPA++: Generalized Graph Spectral Alignment for Versatile Domain Adaptation](https://arxiv.org/abs/2508.05182)
*Zhiqing Xiao,Haobo Wang,Xu Lu,Wentao Ye,Gang Chen,Junbo Zhao*

Main category: cs.CV

TL;DR: 本文提出了一个通用的图谱谱对齐框架SPA++，在领域自适应任务中取得显著效果，提升了目标领域的可辨识性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有领域自适应方法多关注于领域间的迁移性，而忽视了领域内的结构信息，这导致分类性能下降。为解决这一问题，作者希望平衡领域间对齐和领域内判别能力。

Method: SPA++：1）将领域自适应问题转化为图结构，通过谱正则化机制对齐源域与目标域的图谱；2）提出邻居感知传播机制，增强目标域判别性；3）结合数据增强与一致性正则，适应复杂分布和多种DA场景。并进行了理论分析和泛化界证明。

Result: 在基准数据集的大量实验中，SPA++在多种具有挑战性的领域自适应场景下均优于现有顶尖方法，表现出更好的鲁棒性和适应能力。

Conclusion: SPA++成功平衡了领域对齐与判别能力，理论与实验均证明其有效性，是领域自适应任务的新颖且鲁棒的解决方案。

Abstract: Domain Adaptation (DA) aims to transfer knowledge from a labeled source
domain to an unlabeled or sparsely labeled target domain under domain shifts.
Most prior works focus on capturing the inter-domain transferability but
largely overlook rich intra-domain structures, which empirically results in
even worse discriminability. To tackle this tradeoff, we propose a generalized
graph SPectral Alignment framework, SPA++. Its core is briefly condensed as
follows: (1)-by casting the DA problem to graph primitives, it composes a
coarse graph alignment mechanism with a novel spectral regularizer toward
aligning the domain graphs in eigenspaces; (2)-we further develop a
fine-grained neighbor-aware propagation mechanism for enhanced discriminability
in the target domain; (3)-by incorporating data augmentation and consistency
regularization, SPA++ can adapt to complex scenarios including most DA settings
and even challenging distribution scenarios. Furthermore, we also provide
theoretical analysis to support our method, including the generalization bound
of graph-based DA and the role of spectral alignment and smoothing consistency.
Extensive experiments on benchmark datasets demonstrate that SPA++ consistently
outperforms existing cutting-edge methods, achieving superior robustness and
adaptability across various challenging adaptation scenarios.

</details>


### [50] [SPEX: A Vision-Language Model for Land Cover Extraction on Spectral Remote Sensing Images](https://arxiv.org/abs/2508.05202)
*Dongchen Si,Di Wang,Erzhong Gao,Xiaolei Qin,Liu Zhao,Jing Zhang,Minqiang Xu,Jianbo Zhan,Jianshe Wang,Lin Liu,Bo Du,Liangpei Zhang*

Main category: cs.CV

TL;DR: 本文提出了SPEX多模态大模型，通过编码光谱信息提升对遥感图像地物的精细化识别，同时具备良好的可解释性和用户交互性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型对遥感图像的像素级解读时，光谱信息利用不足，特别是在多光谱图像场景下表现不佳。

Method: 作者构建了SPIE数据集，将地物的光谱先验编码为可被大模型识别的文本属性；并基于该集提出了SPEX多模态大模型，采用多尺度特征聚合、token上下文压缩、多光谱视觉预训练等策略以提升像素级解释能力。

Result: 在五个公开多光谱数据集上，SPEX在植被、建筑、水体等地物类别的提取任务中，性能超越了当前主流方法。模型还能对预测结果生成文本解释，提升了可解释性和友好性。

Conclusion: SPEX是首个专为遥感光谱图像地物提取设计的多模态视觉-语言模型，在准确性、灵活性和可解释性方面均具备优势，有望推动遥感智能解译的发展。

Abstract: Spectral information has long been recognized as a critical cue in remote
sensing observations. Although numerous vision-language models have been
developed for pixel-level interpretation, spectral information remains
underutilized, resulting in suboptimal performance, particularly in
multispectral scenarios. To address this limitation, we construct a
vision-language instruction-following dataset named SPIE, which encodes
spectral priors of land-cover objects into textual attributes recognizable by
large language models (LLMs), based on classical spectral index computations.
Leveraging this dataset, we propose SPEX, a multimodal LLM designed for
instruction-driven land cover extraction. To this end, we introduce several
carefully designed components and training strategies, including multiscale
feature aggregation, token context condensation, and multispectral visual
pre-training, to achieve precise and flexible pixel-level interpretation. To
the best of our knowledge, SPEX is the first multimodal vision-language model
dedicated to land cover extraction in spectral remote sensing imagery.
Extensive experiments on five public multispectral datasets demonstrate that
SPEX consistently outperforms existing state-of-the-art methods in extracting
typical land cover categories such as vegetation, buildings, and water bodies.
Moreover, SPEX is capable of generating textual explanations for its
predictions, thereby enhancing interpretability and user-friendliness. Code
will be released at: https://github.com/MiliLab/SPEX.

</details>


### [51] [EndoMatcher: Generalizable Endoscopic Image Matcher via Multi-Domain Pre-training for Robot-Assisted Surgery](https://arxiv.org/abs/2508.05205)
*Bingyu Yang,Qingyao Tian,Yimeng Geng,Huai Liao,Xinyan Huang,Jiebo Luo,Hongbin Liu*

Main category: cs.CV

TL;DR: 本文提出EndoMatcher，一种面向内镜图像的可泛化稠密特征匹配方法，通过大规模、多领域的数据预训练实现对弱纹理、大视角变化环境下的关键点匹配显著提升。


<details>
  <summary>Details</summary>
Motivation: 在机器人辅助手术中，如3D重建、导航、场景理解等任务，需要内镜图像的稠密特征匹配。然而，内镜图像存在纹理弱、视角变化大及标注数据稀缺等难题，现有方法难以泛化。为解决这些瓶颈，作者提出新方法提升匹配的稳健性和泛化能力。

Method: 提出EndoMatcher，采用双分支的Vision Transformer提取多尺度特征，并引入双重交互模块增强特征对应关系学习。为缓解标注数据不足和域多样性问题，作者构建了首个多领域内镜匹配数据集Endo-Mix6，涵盖六个领域共约120万对图像，通过SFM和模拟变换获得标注。针对多样性带来的训练不稳定，提出渐进多目标训练策略，促进平衡学习。

Result: EndoMatcher在Hamlyn和Bladder数据集上的有效匹配点数量分别提升了140.69%和201.43%，在Gastro-Matching数据集上MDPA提升9.40%，显著优于最新方法，证实其在挑战性内镜图像下的稠密准确匹配能力。

Conclusion: EndoMatcher利用多领域大规模预训练和创新结构，显著提升了内镜图像匹配的泛化性和准确性，在不同器官与成像条件下实现零样本泛化，为机器人手术等下游任务提供强大基础。

Abstract: Generalizable dense feature matching in endoscopic images is crucial for
robot-assisted tasks, including 3D reconstruction, navigation, and surgical
scene understanding. Yet, it remains a challenge due to difficult visual
conditions (e.g., weak textures, large viewpoint variations) and a scarcity of
annotated data. To address these challenges, we propose EndoMatcher, a
generalizable endoscopic image matcher via large-scale, multi-domain data
pre-training. To address difficult visual conditions, EndoMatcher employs a
two-branch Vision Transformer to extract multi-scale features, enhanced by dual
interaction blocks for robust correspondence learning. To overcome data
scarcity and improve domain diversity, we construct Endo-Mix6, the first
multi-domain dataset for endoscopic matching. Endo-Mix6 consists of
approximately 1.2M real and synthetic image pairs across six domains, with
correspondence labels generated using Structure-from-Motion and simulated
transformations. The diversity and scale of Endo-Mix6 introduce new challenges
in training stability due to significant variations in dataset sizes,
distribution shifts, and error imbalance. To address them, a progressive
multi-objective training strategy is employed to promote balanced learning and
improve representation quality across domains. This enables EndoMatcher to
generalize across unseen organs and imaging conditions in a zero-shot fashion.
Extensive zero-shot matching experiments demonstrate that EndoMatcher increases
the number of inlier matches by 140.69% and 201.43% on the Hamlyn and Bladder
datasets over state-of-the-art methods, respectively, and improves the Matching
Direction Prediction Accuracy (MDPA) by 9.40% on the Gastro-Matching dataset,
achieving dense and accurate matching under challenging endoscopic conditions.
The code is publicly available at https://github.com/Beryl2000/EndoMatcher.

</details>


### [52] [VFlowOpt: A Token Pruning Framework for LMMs with Visual Information Flow-Guided Optimization](https://arxiv.org/abs/2508.05211)
*Sihan Yang,Runsen Xu,Chenhang Cui,Tai Wang,Dahua Lin,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 本论文提出VFlowOpt方法，通过优化视觉token剪枝以提升大规模多模态模型推理效率，在高达90%剪枝率下保持性能基本不变。


<details>
  <summary>Details</summary>
Motivation: 当前大规模多模态模型为提升视觉-语言任务表现，普遍使用大量视觉token，造成推理阶段算力和内存消耗极大。虽然已有研究试图减少推理时视觉token数量，但现有剪枝方法简单，易造成性能下降，需要更高效、灵活的剪枝框架。

Method: VFlowOpt包含三部分：1）基于注意力衍生的上下文相关性与patch级信息熵，计算并生成重要性地图以评估每个视觉token重要性；2）基于重要性地图进行逐步剪枝，并回收（recycle）被剪掉的token以防信息损失；3）提出视觉信息流指导的剪枝参数优化方法，通过最小化剪枝前后关键token表征差异，提高策略自适应性。

Result: 实验表明，VFlowOpt在剪除90%视觉token时，基本不损失模型性能，同时KV-Cache内存消耗降低89%，推理速度提升3.8倍。

Conclusion: VFlowOpt能显著降低大模型视觉推理的计算与内存开销，在保证性能的同时提升推理效率，对实际多模态应用具有重要意义。

Abstract: Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging
numerous visual tokens for fine-grained visual information, but this token
redundancy results in significant computational costs. Previous research aimed
at reducing visual tokens during inference typically leverages importance maps
derived from attention scores among vision-only tokens or vision-language
tokens to prune tokens across one or multiple pruning stages. Despite this
progress, pruning frameworks and strategies remain simplistic and
insufficiently explored, often resulting in substantial performance
degradation. In this paper, we propose VFlowOpt, a token pruning framework that
introduces an importance map derivation process and a progressive pruning
module with a recycling mechanism. The hyperparameters of its pruning strategy
are further optimized by a visual information flow-guided method. Specifically,
we compute an importance map for image tokens based on their attention-derived
context relevance and patch-level information entropy. We then decide which
tokens to retain or prune and aggregate the pruned ones as recycled tokens to
avoid potential information loss. Finally, we apply a visual information
flow-guided method that regards the last token in the LMM as the most
representative signal of text-visual interactions. This method minimizes the
discrepancy between token representations in LMMs with and without pruning,
thereby enabling superior pruning strategies tailored to different LMMs.
Experiments demonstrate that VFlowOpt can prune 90% of visual tokens while
maintaining comparable performance, leading to an 89% reduction in KV-Cache
memory and 3.8 times faster inference.

</details>


### [53] [Textual and Visual Guided Task Adaptation for Source-Free Cross-Domain Few-Shot Segmentation](https://arxiv.org/abs/2508.05213)
*Jianming Liu,Wenlong Qiu,Haitao Wei*

Main category: cs.CV

TL;DR: 本文提出了一种无需源域数据的跨域小样本分割（source-free CD-FSS）方法，通过结合视觉和文本信息，提升目标域分割表现，并超越现有同类方法。


<details>
  <summary>Details</summary>
Motivation: 当前CD-FSS方法要求访问源域数据，但在数据隐私和减少数据传输、训练成本的需求下，发展无需源域数据的CD-FSS方法变得至关重要。

Method: 1. 在预训练主干网络特征金字塔中新增任务特定注意力适配器(TSAA)，自适应调整多层特征。2. 通过视觉-视觉嵌入对齐(VVEA)模块和文本-视觉嵌入对齐(TVEA)模块训练TSAA参数。VVEA用于不同视角下的全局/局部视觉特征对齐，TVEA利用例如CLIP的多模态文本先验辅助跨模态特征自适应。3. 使用稠密比较操作和跳跃连接融合模块输出，生成更精细的分割掩码。

Result: 在四个跨域数据集上，1-shot和5-shot指标分别提升2.18%和4.11%，显著优于现有CD-FSS方法。

Conclusion: 该方法实现了无需源域参与的高效目标域小样本分割，兼顾隐私安全与性能提升，具有良好通用性和实际应用价值。

Abstract: Few-Shot Segmentation(FSS) aims to efficient segmentation of new objects with
few labeled samples. However, its performance significantly degrades when
domain discrepancies exist between training and deployment. Cross-Domain
Few-Shot Segmentation(CD-FSS) is proposed to mitigate such performance
degradation. Current CD-FSS methods primarily sought to develop segmentation
models on a source domain capable of cross-domain generalization. However,
driven by escalating concerns over data privacy and the imperative to minimize
data transfer and training expenses, the development of source-free CD-FSS
approaches has become essential. In this work, we propose a source-free CD-FSS
method that leverages both textual and visual information to facilitate target
domain task adaptation without requiring source domain data. Specifically, we
first append Task-Specific Attention Adapters (TSAA) to the feature pyramid of
a pretrained backbone, which adapt multi-level features extracted from the
shared pre-trained backbone to the target task. Then, the parameters of the
TSAA are trained through a Visual-Visual Embedding Alignment (VVEA) module and
a Text-Visual Embedding Alignment (TVEA) module. The VVEA module utilizes
global-local visual features to align image features across different views,
while the TVEA module leverages textual priors from pre-aligned multi-modal
features (e.g., from CLIP) to guide cross-modal adaptation. By combining the
outputs of these modules through dense comparison operations and subsequent
fusion via skip connections, our method produces refined prediction masks.
Under both 1-shot and 5-shot settings, the proposed approach achieves average
segmentation accuracy improvements of 2.18\% and 4.11\%, respectively, across
four cross-domain datasets, significantly outperforming state-of-the-art CD-FSS
methods. Code are available at https://github.com/ljm198134/TVGTANet.

</details>


### [54] [ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language Tracking](https://arxiv.org/abs/2508.05221)
*Xiao Wang,Liye Jin,Xufeng Lou,Shiao Wang,Lan Chen,Bo Jiang,Zhipeng Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于推理的视觉-语言跟踪新方法ReasoningTrack，并发布了一个新的大规模基准数据集TNLLT，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 目前视觉-语言跟踪中，通过文本指定跟踪目标可以提升灵活性和准确性。而现有方法或简单融合语言与视觉特征、或利用注意力机制，效果有限。此外，新近方法虽用文本生成来适应目标变化，但缺乏对模型推理过程的解释，也未充分利用大模型潜力。

Method: 提出ReasoningTrack框架，基于预训练视觉-语言大模型Qwen2.5-VL，结合有监督微调（SFT）和强化学习（GRPO）优化推理与文本生成。将随时更新的文本描述与视觉特征一同输入统一跟踪主干网络，通过跟踪头输出目标具体位置。并提出包含200段视频的新基准数据集TNLLT，用于全面评测。

Result: 在多个视觉-语言跟踪基准测试上，ReasoningTrack方法取得了优异效果，并系统性地评测了20种视觉跟踪器在新数据集上的表现，验证了方法和数据集的有效性。

Conclusion: 本文提出的基于推理的视觉-语言跟踪方法显著提升了跟踪效果，并推动了领域内基准建设和方法创新。源代码即将开源，有望促进相关研究和应用的发展。

Abstract: Vision-language tracking has received increasing attention in recent years,
as textual information can effectively address the inflexibility and inaccuracy
associated with specifying the target object to be tracked. Existing works
either directly fuse the fixed language with vision features or simply modify
using attention, however, their performance is still limited. Recently, some
researchers have explored using text generation to adapt to the variations in
the target during tracking, however, these works fail to provide insights into
the model's reasoning process and do not fully leverage the advantages of large
models, which further limits their overall performance. To address the
aforementioned issues, this paper proposes a novel reasoning-based
vision-language tracking framework, named ReasoningTrack, based on a
pre-trained vision-language model Qwen2.5-VL. Both SFT (Supervised Fine-Tuning)
and reinforcement learning GRPO are used for the optimization of reasoning and
language generation. We embed the updated language descriptions and feed them
into a unified tracking backbone network together with vision features. Then,
we adopt a tracking head to predict the specific location of the target object.
In addition, we propose a large-scale long-term vision-language tracking
benchmark dataset, termed TNLLT, which contains 200 video sequences. 20
baseline visual trackers are re-trained and evaluated on this dataset, which
builds a solid foundation for the vision-language visual tracking task.
Extensive experiments on multiple vision-language tracking benchmark datasets
fully validated the effectiveness of our proposed reasoning-based natural
language generation strategy. The source code of this paper will be released on
https://github.com/Event-AHU/Open_VLTrack

</details>


### [55] [Segmenting the Complex and Irregular in Two-Phase Flows: A Real-World Empirical Study with SAM2](https://arxiv.org/abs/2508.05227)
*Semanur Küçük,Cosimo Della Santina,Angeliki Laskari*

Main category: cs.CV

TL;DR: 本文通过迁移学习方法，利用微调后的Segment Anything Model（SAM v2.1），实现了对多相流中高度变形及非规则气泡的精确分割，且仅需少量标注图像。


<details>
  <summary>Details</summary>
Motivation: 多相流中气泡的分割在工业领域有重要意义，但传统及现有学习方法普遍假设气泡为接近球形，难以处理真实工况中的扭曲、聚并或破裂等复杂形态，特别是在气体润滑系统中表现尤为突出。

Method: 将气泡分割问题视为迁移学习任务，利用现代视觉基础模型（SAM v2.1），通过微调和极少量标注图像训练模型，以适应高度非凸、形态多变的气泡结构。

Result: 实验证明，仅用100张标注图片即可使微调后的SAM准确分割复杂、非球形的气泡区域。

Conclusion: 该方法首次验证了基于现代视觉基础模型的迁移学习方案，在分割非规则气泡方面的有效性，为工业多相流的图像分析提供了新工具。

Abstract: Segmenting gas bubbles in multiphase flows is a critical yet unsolved
challenge in numerous industrial settings, from metallurgical processing to
maritime drag reduction. Traditional approaches-and most recent learning-based
methods-assume near-spherical shapes, limiting their effectiveness in regimes
where bubbles undergo deformation, coalescence, or breakup. This complexity is
particularly evident in air lubrication systems, where coalesced bubbles form
amorphous and topologically diverse patches. In this work, we revisit the
problem through the lens of modern vision foundation models. We cast the task
as a transfer learning problem and demonstrate, for the first time, that a
fine-tuned Segment Anything Model SAM v2.1 can accurately segment highly
non-convex, irregular bubble structures using as few as 100 annotated images.

</details>


### [56] [ArbiViewGen: Controllable Arbitrary Viewpoint Camera Data Generation for Autonomous Driving via Stable Diffusion Models](https://arxiv.org/abs/2508.05236)
*Yatong Lan,Jingfeng Chen,Yiru Wang,Lei He*

Main category: cs.CV

TL;DR: 本文提出了一种名为Arbiviewgen的新型扩散模型框架，实现了可控的任意视角摄像机图像生成，主要应用于自动驾驶领域，突破了以往因缺乏外推视角真实数据而训练受限的问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要从任意视角感知环境，但缺少对这些未知角度真实画面的数据，导致现有生成模型难以泛化和高保真生成，因此需要一种无需外推真实数据的新方法。

Method: Arbiviewgen框架包含两个核心模块：1）FAVS（特征感知自适应视图拼接），通过摄像头位姿粗配准和改进特征匹配进行细微对齐，并用聚类分析筛选高置信区域；2）CVC-SSL（跨视角一致性自监督学习），采用扩散模型要求重建原始视角图像，实现无监督的一致性优化。训练时仅需多摄像头图像和位姿，无需额外传感器或深度图。

Result: 实验结果表明，Arbiviewgen在多种车辆配置下，首次实现了可控任意视角的相机图像高质量生成，且只需常规训练数据，无需外部依赖。

Conclusion: 提出的方法为自动驾驶等场景中的任意视角感知带来了新的解决思路，突破了传统生成模型对真实外推数据的依赖，对行业具有重要应用前景。

Abstract: Arbitrary viewpoint image generation holds significant potential for
autonomous driving, yet remains a challenging task due to the lack of
ground-truth data for extrapolated views, which hampers the training of
high-fidelity generative models. In this work, we propose Arbiviewgen, a novel
diffusion-based framework for the generation of controllable camera images from
arbitrary points of view. To address the absence of ground-truth data in unseen
views, we introduce two key components: Feature-Aware Adaptive View Stitching
(FAVS) and Cross-View Consistency Self-Supervised Learning (CVC-SSL). FAVS
employs a hierarchical matching strategy that first establishes coarse
geometric correspondences using camera poses, then performs fine-grained
alignment through improved feature matching algorithms, and identifies
high-confidence matching regions via clustering analysis. Building upon this,
CVC-SSL adopts a self-supervised training paradigm where the model reconstructs
the original camera views from the synthesized stitched images using a
diffusion model, enforcing cross-view consistency without requiring supervision
from extrapolated data. Our framework requires only multi-camera images and
their associated poses for training, eliminating the need for additional
sensors or depth maps. To our knowledge, Arbiviewgen is the first method
capable of controllable arbitrary view camera image generation in multiple
vehicle configurations.

</details>


### [57] [Navigating the Trade-off: A Synthesis of Defensive Strategies for Zero-Shot Adversarial Robustness in Vision-Language Models](https://arxiv.org/abs/2508.05237)
*Zane Xu,Jason Sun*

Main category: cs.CV

TL;DR: 本报告综述了8篇关于视觉-语言模型（如CLIP）零样本对抗鲁棒性的核心论文，探讨了提升鲁棒性与维持零样本泛化能力之间的权衡，总结了主要防御范式和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（VLMs）在实际应用中面临着对抗攻击的威胁。如何在增强模型对抗鲁棒性的同时，不牺牲其零样本（zero-shot）泛化能力，是迫切需要解决的问题。本文旨在梳理现有主流防御方法的进展及存在的挑战。

Method: 系统分析了两大防御范式：(1) 对抗微调（AFT），通过修改模型参数以提升鲁棒性；(2) 无需训练/测试时防御手段，通过不改变模型参数的方法实现保护。涵盖了从保留特征对齐（如TeCoA）、嵌入空间重构（LAAT、TIMA）、输入空间启发式（AOM、TTC）到隐空间净化（CLIPure）的进展。

Result: 总结了不同方法提升鲁棒性的效果及其对零样本性能的影响，揭示了各类防御技术在保护VLMs免受对抗攻击以及维持泛化能力上的优劣与权衡，并指出现有方法仍有不少研究空白。

Conclusion: 本文最终指出，现有技术尚未完美解决鲁棒性与零样本泛化能力的平衡，建议未来可关注混合防御策略和对抗式预训练等方向。

Abstract: This report synthesizes eight seminal papers on the zero-shot adversarial
robustness of vision-language models (VLMs) like CLIP. A central challenge in
this domain is the inherent trade-off between enhancing adversarial robustness
and preserving the model's zero-shot generalization capabilities. We analyze
two primary defense paradigms: Adversarial Fine-Tuning (AFT), which modifies
model parameters, and Training-Free/Test-Time Defenses, which preserve them. We
trace the evolution from alignment-preserving methods (TeCoA) to embedding
space re-engineering (LAAT, TIMA), and from input heuristics (AOM, TTC) to
latent-space purification (CLIPure). Finally, we identify key challenges and
future directions including hybrid defense strategies and adversarial
pre-training.

</details>


### [58] [RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning Pre-trained Model for Medical Image Understanding](https://arxiv.org/abs/2508.05244)
*Tianchen Fang,Guiru Liu*

Main category: cs.CV

TL;DR: 作者提出RegionMed-CLIP框架，通过显式融合局部病灶信号与全局语义，实现更精准多模态医学图像理解，并构建了包含大规模区域标注与临床描述的数据集MedRegion-500k，显著提升多项任务表现。


<details>
  <summary>Details</summary>
Motivation: 医学图像理解受高质量标注数据有限和过度依赖全局特征所困，难以精准捕捉临床关键病灶区域，限制了自动化诊断和辅助决策应用。

Method: 提出RegionMed-CLIP多模态对比学习方法，包含创新的ROI处理器，将局部细粒度区域特征自适应融合至全局背景，并采用分层渐进式训练策略。此外，自建包含50万例、细化区域标注和分级临床描述的新数据集MedRegion-500k，用于区域级别特征学习。

Result: 在图文检索、零样本分类、视觉问答等多项任务上，RegionMed-CLIP在准确性和泛化能力上均大幅优于现有主流视觉语言模型。

Conclusion: 区域感知对比预训练对于提升医学多模态图像理解至关重要，RegionMed-CLIP为该领域下游任务提供了坚实基础和显著性能提升。

Abstract: Medical image understanding plays a crucial role in enabling automated
diagnosis and data-driven clinical decision support. However, its progress is
impeded by two primary challenges: the limited availability of high-quality
annotated medical data and an overreliance on global image features, which
often miss subtle but clinically significant pathological regions. To address
these issues, we introduce RegionMed-CLIP, a region-aware multimodal
contrastive learning framework that explicitly incorporates localized
pathological signals along with holistic semantic representations. The core of
our method is an innovative region-of-interest (ROI) processor that adaptively
integrates fine-grained regional features with the global context, supported by
a progressive training strategy that enhances hierarchical multimodal
alignment. To enable large-scale region-level representation learning, we
construct MedRegion-500k, a comprehensive medical image-text corpus that
features extensive regional annotations and multilevel clinical descriptions.
Extensive experiments on image-text retrieval, zero-shot classification, and
visual question answering tasks demonstrate that RegionMed-CLIP consistently
exceeds state-of-the-art vision language models by a wide margin. Our results
highlight the critical importance of region-aware contrastive pre-training and
position RegionMed-CLIP as a robust foundation for advancing multimodal medical
image understanding.

</details>


### [59] [A Study of Gender Classification Techniques Based on Iris Images: A Deep Survey and Analysis](https://arxiv.org/abs/2508.05246)
*Basna Mohammed Salih Hasan,Ramadhan J. Mstafa*

Main category: cs.CV

TL;DR: 本文综述了基于多种生物特征（尤其是虹膜）进行性别分类的方法，回顾了现有研究进展，并对未来的研究方向提出了建议。


<details>
  <summary>Details</summary>
Motivation: 性别分类在安防监控、人机交互等领域应用广泛，能够作为一种辅助身份识别手段。由于虹膜特征在生命周期中相对稳定且非侵入性，成为研究的一个重要方向。

Method: 论文介绍了多种基于身体特征进行性别分类的方法，主要关注于面部和虹膜特征，并总结了现有虹膜图像分割与特征提取技术。还对不同分类步骤中的方法进行分类和回顾。

Result: 通过分析文献与现有技术，总结了性别分类方法的优缺点，指出当前技术的局限及尚存的技术难题。

Conclusion: 本文为研究者提供了性别分类领域的综述，促进对现有技术的理解，并提出未来的研究建议和改进路径，以推动该领域发展。

Abstract: Gender classification is attractive in a range of applications, including
surveillance and monitoring, corporate profiling, and human-computer
interaction. Individuals' identities may be gleaned from information about
their gender, which is a kind of soft biometric.Over the years, several methods
for determining a person's gender have been devised. Some of the most
well-known ones are based on physical characteristics like face, fingerprint,
palmprint, DNA, ears, gait, and iris. On the other hand, facial features
account for the vast majority of gender classification methods. Also, the iris
is a significant biometric trait because the iris, according to research,
remains basically constant during an individual's life. Besides that, the iris
is externally visible and is non-invasive to the user, which is important for
practical applications. Furthermore, there are already high-quality methods for
segmenting and encoding iris images, and the current methods facilitate
selecting and extracting attribute vectors from iris textures. This study
discusses several approaches to determining gender. The previous works of
literature are briefly reviewed. Additionally, there are a variety of
methodologies for different steps of gender classification. This study provides
researchers with knowledge and analysis of the existing gender classification
approaches. Also, it will assist researchers who are interested in this
specific area, as well as highlight the gaps and challenges in the field, and
finally provide suggestions and future paths for improvement.

</details>


### [60] [CF3: Compact and Fast 3D Feature Fields](https://arxiv.org/abs/2508.05254)
*Hyunjoon Lee,Joonkyu Min,Jaesik Park*

Main category: cs.CV

TL;DR: 本论文提出了一种名为CF3的新型3D高斯特征场构建方法，通过自顶向下策略与自适应稀疏化技术，显著减少高斯数量，提高了效率且保持几何细节。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法依赖于自底向上的2D特征优化，不仅计算量大，还将2D特征当作真实标签，提升了成本。因此，需要一种更高效且能减少高斯数量的方法。

Method: 作者提出先使用预训练高斯进行多视角2D特征的加权融合，直接在‘提升’后的特征上训练每个高斯的自编码器，使其更贴合3D特征分布。同时，设计了自适应稀疏化算法，对冗余高斯进行裁剪与合并，最终获得紧凑高效的3D高斯特征场。

Result: 新方法在保持竞品3D特征场表现的同时，仅用Feature-3DGS的5%高斯数量，展现了极高的压缩率和效率。

Conclusion: CF3方法显著提升了3D高斯特征场的构建效率和紧凑性，有效保存了几何细节，对降低存储和计算开销有重要意义。

Abstract: 3D Gaussian Splatting (3DGS) has begun incorporating rich information from 2D
foundation models. However, most approaches rely on a bottom-up optimization
process that treats raw 2D features as ground truth, incurring increased
computational costs. We propose a top-down pipeline for constructing compact
and fast 3D Gaussian feature fields, namely, CF3. We first perform a fast
weighted fusion of multi-view 2D features with pre-trained Gaussians. This
approach enables training a per-Gaussian autoencoder directly on the lifted
features, instead of training autoencoders in the 2D domain. As a result, the
autoencoder better aligns with the feature distribution. More importantly, we
introduce an adaptive sparsification method that optimizes the Gaussian
attributes of the feature field while pruning and merging the redundant
Gaussians, constructing an efficient representation with preserved geometric
details. Our approach achieves a competitive 3D feature field using as little
as 5% of the Gaussians compared to Feature-3DGS.

</details>


### [61] [Robust Tracking with Particle Filtering for Fluorescent Cardiac Imaging](https://arxiv.org/abs/2508.05262)
*Suresh Guttikonda,Maximilian Neidhart,Johanna Sprenger,Johannes Petersen,Christian Detter,Alexander Schlaefer*

Main category: cs.CV

TL;DR: 本文提出了一种基于循环一致性检查的粒子滤波追踪方法，以实现心脏旁路移植术后高效、实时的血管标志点追踪，提高术中灌注质量控制的准确性。


<details>
  <summary>Details</summary>
Motivation: 心脏旁路移植术后需要通过术中荧光成像进行灌注质量控制，现有追踪方法在心脏剧烈运动和图像特性大幅变化下表现不佳，难以准确、实时地获取局部定量指标，因此亟需更稳健的追踪算法。

Method: 作者提出了一种新颖的粒子滤波追踪器，并引入循环一致性检查机制，用于鲁棒地同步追踪多个血管标志点。该方法能够在包含心脏运动和图像噪声等复杂条件下依然实现高效追踪。

Result: 提出的方法能够以每秒25.4帧的速度同步追踪117个目标点，平均追踪误差为5.00±0.22像素，性能明显优于主流的深度学习追踪器（22.3±1.1像素）和传统追踪器（58.1±27.1像素）。

Conclusion: 该方法极大提升了术中追踪的鲁棒性和精度，能够为手术过程实时提供定量评估，有助于提升心脏外科手术的安全性与疗效。

Abstract: Intraoperative fluorescent cardiac imaging enables quality control following
coronary bypass grafting surgery. We can estimate local quantitative
indicators, such as cardiac perfusion, by tracking local feature points.
However, heart motion and significant fluctuations in image characteristics
caused by vessel structural enrichment limit traditional tracking methods. We
propose a particle filtering tracker based on cyclicconsistency checks to
robustly track particles sampled to follow target landmarks. Our method tracks
117 targets simultaneously at 25.4 fps, allowing real-time estimates during
interventions. It achieves a tracking error of (5.00 +/- 0.22 px) and
outperforms other deep learning trackers (22.3 +/- 1.1 px) and conventional
trackers (58.1 +/- 27.1 px).

</details>


### [62] [SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible Image Fusion](https://arxiv.org/abs/2508.05264)
*Xiaoyang Zhang,Zhen Hua,Yakun Ju,Wei Zhou,Jun Liu,Alex C. Kot*

Main category: cs.CV

TL;DR: 本文提出了一种创新的红外与可见光图像融合方法SGDFuse，借助Segement Anything Model (SAM)作为先验，引导扩散模型实现语义感知且高保真的图像融合，显著提升了融合成像质量和下游任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有红外与可见光融合方法由于缺乏对场景的深层语义理解，容易导致关键目标无法保留，同时融合过程易引入伪影与细节损失，影响影像质量和后续任务效果，因此亟需一种能在保留语义和细节同时提升融合质量的方法。

Method: 提出SGDFuse框架：首先利用SAM生成的高质量语义掩码作为显式先验，通过条件扩散模型引导融合过程。具体为两阶段：一是对多模态特征进行初步融合，二是将SAM语义掩码与初融合图像作为条件，引导扩散网络进行由粗到细的去噪重建，以实现高语义指向性和高保真度的最终融合。

Result: SGDFuse在主观、客观评价及下游应用适应性等多方面大幅优于现有方法，达到了融合领域的最新效果。

Conclusion: SGDFuse结合SAM的语义感知能力与扩散模型的高保真生成能力，为图像融合的关键难题提供了创新有效的解决方案，并显著提升了实际应用性能。

Abstract: Infrared and visible image fusion (IVIF) aims to combine the thermal
radiation information from infrared images with the rich texture details from
visible images to enhance perceptual capabilities for downstream visual tasks.
However, existing methods often fail to preserve key targets due to a lack of
deep semantic understanding of the scene, while the fusion process itself can
also introduce artifacts and detail loss, severely compromising both image
quality and task performance. To address these issues, this paper proposes
SGDFuse, a conditional diffusion model guided by the Segment Anything Model
(SAM), to achieve high-fidelity and semantically-aware image fusion. The core
of our method is to utilize high-quality semantic masks generated by SAM as
explicit priors to guide the optimization of the fusion process via a
conditional diffusion model. Specifically, the framework operates in a
two-stage process: it first performs a preliminary fusion of multi-modal
features, and then utilizes the semantic masks from SAM jointly with the
preliminary fused image as a condition to drive the diffusion model's
coarse-to-fine denoising generation. This ensures the fusion process not only
has explicit semantic directionality but also guarantees the high fidelity of
the final result. Extensive experiments demonstrate that SGDFuse achieves
state-of-the-art performance in both subjective and objective evaluations, as
well as in its adaptability to downstream tasks, providing a powerful solution
to the core challenges in image fusion. The code of SGDFuse is available at
https://github.com/boshizhang123/SGDFuse.

</details>


### [63] [B4DL: A Benchmark for 4D LiDAR LLM in Spatio-Temporal Understanding](https://arxiv.org/abs/2508.05269)
*Changho Choi,Youngwoo Shin,Gyojin Han,Dong-Jae Lee,Junmo Kim*

Main category: cs.CV

TL;DR: 论文提出了B4DL基准和数据生成管道，并设计了能直接处理4D LiDAR与语言结合的多模态大模型，以促进动态户外环境的时空推理。


<details>
  <summary>Details</summary>
Motivation: 动态户外环境包含复杂的对象交互，需要时空信息丰富的数据。4D LiDAR点云能精确呈现实景，但目前缺乏针对该模态的高质量标注、数据集和能处理其高维特性的MLLM模型，使其潜能没有被充分挖掘。

Method: 1）构建了B4DL基准数据集，专用于训练和评估处理4D LiDAR的多模态大模型；2）提出可扩展的数据生成流程；3）构建了能直接处理原始4D LiDAR并与语言交互结合的MLLM架构。

Result: 论文模型结合自建数据集与基准，首次实现了对4D LiDAR原始点云与自然语言的直接处理与推理，可在动态场景下完成统一的时空推理任务。

Conclusion: B4DL基准和所提模型为推动4D LiDAR多模态研究提供了基础设施与方法，有望提升对真实动态环境的理解与时空推理能力。

Abstract: Understanding dynamic outdoor environments requires capturing complex object
interactions and their evolution over time. LiDAR-based 4D point clouds provide
precise spatial geometry and rich temporal cues, making them ideal for
representing real-world scenes. However, despite their potential, 4D LiDAR
remains underexplored in the context of Multimodal Large Language Models
(MLLMs) due to the absence of high-quality, modality-specific annotations and
the lack of MLLM architectures capable of processing its high-dimensional
composition. To address these challenges, we introduce B4DL, a new benchmark
specifically designed for training and evaluating MLLMs on 4D LiDAR
understanding. In addition, we propose a scalable data generation pipeline and
an MLLM model that, for the first time, directly processes raw 4D LiDAR by
bridging it with language understanding. Combined with our dataset and
benchmark, our model offers a unified solution for spatio-temporal reasoning in
dynamic outdoor environments. We provide rendered 4D LiDAR videos, generated
dataset, and inference outputs on diverse scenarios at:
https://mmb4dl.github.io/mmb4dl/

</details>


### [64] [Wavelet-Guided Dual-Frequency Encoding for Remote Sensing Change Detection](https://arxiv.org/abs/2508.05271)
*Xiaoyang Zhang,Guodong Fan,Guang-Yong Chen,Zhen Hua,Jinjiang Li,Min Gan,C. L. Philip Chen*

Main category: cs.CV

TL;DR: 本文提出了一种基于小波引导的双频率编码（WGDF）方法，以提升遥感图像变化检测的准确性和鲁棒性，通过结合空间域和频率域特征，有效捕捉细粒度和边缘变化。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在遥感图像变化检测上取得了巨大进展，但大多数现有方法仍主要依赖空间域建模，导致细微变化区域检测能力受限。作者观察到频率域、特别是小波域的特征建模能够更好地表现细粒度的频率成分差异，增强边缘变化的感知能力。

Method: 方法上，首先利用离散小波变换（DWT）将输入图像分解为高频和低频分量。高频分支用于捕捉局部细节，引入双频率特征增强（DFFE）模块和频域交互差分（FDID）模块，突出边缘和细微变化。低频分支利用Transformer建模全局结构，并通过渐进式上下文差分模块（PCDM）细化变化区域。最终融合高低频特征，实现局部灵敏性与全局判别性的统一。

Result: 在多个遥感数据集上的实验表明，所提WGDF方法在缓解边缘模糊和提升检测精度、鲁棒性方面，均显著优于现有主流方法。

Conclusion: 本文方法通过小波域引导的双频率特征建模，有效提升了细粒度变化和边缘检测能力，为遥感变化检测任务提供了新思路。

Abstract: Change detection in remote sensing imagery plays a vital role in various
engineering applications, such as natural disaster monitoring, urban expansion
tracking, and infrastructure management. Despite the remarkable progress of
deep learning in recent years, most existing methods still rely on
spatial-domain modeling, where the limited diversity of feature representations
hinders the detection of subtle change regions. We observe that
frequency-domain feature modeling particularly in the wavelet domain an amplify
fine-grained differences in frequency components, enhancing the perception of
edge changes that are challenging to capture in the spatial domain. Thus, we
propose a method called Wavelet-Guided Dual-Frequency Encoding (WGDF).
Specifically, we first apply Discrete Wavelet Transform (DWT) to decompose the
input images into high-frequency and low-frequency components, which are used
to model local details and global structures, respectively. In the
high-frequency branch, we design a Dual-Frequency Feature Enhancement (DFFE)
module to strengthen edge detail representation and introduce a
Frequency-Domain Interactive Difference (FDID) module to enhance the modeling
of fine-grained changes. In the low-frequency branch, we exploit Transformers
to capture global semantic relationships and employ a Progressive Contextual
Difference Module (PCDM) to progressively refine change regions, enabling
precise structural semantic characterization. Finally, the high- and
low-frequency features are synergistically fused to unify local sensitivity
with global discriminability. Extensive experiments on multiple remote sensing
datasets demonstrate that WGDF significantly alleviates edge ambiguity and
achieves superior detection accuracy and robustness compared to
state-of-the-art methods. The code will be available at
https://github.com/boshizhang123/WGDF.

</details>


### [65] [VS-LLM: Visual-Semantic Depression Assessment based on LLM for Drawing Projection Test](https://arxiv.org/abs/2508.05299)
*Meiqi Wu,Yaxuan Kang,Xuchen Li,Shiyu Hu,Xiaotang Chen,Yunfeng Kang,Weiqiang Wang,Kaiqi Huang*

Main category: cs.CV

TL;DR: 本文提出一种用于心理状态自动评估的PPAT素描自动识别方法，通过视觉-语义大模型有效辅助心理学家开展大规模绘画投射测试（DPT）。方法在PPAT素描抑郁评估中表现优异，较传统心理学家评估有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统的PPAT绘画投射测试依赖心理学家人工解释，费时费力，并且受个人经验影响较大，无法满足大规模自动化需求。作者希望通过自动化方法提高评估效率和客观性，助力心理健康筛查。

Method: 1. 构建了适用于PPAT素描自动分析的实验环境；2. 提出基于大型语言模型（LLM）的视觉-语义抑郁评估方法（VS-LLM）；3. 着重考虑素描的用色和空间利用等整体特征，针对PPAT画作易粗略、细节少的特点进行了专门设计。

Result: 通过实验证明，所提方法在抑郁评估任务中的表现相较于传统心理学家人工评估方法提升了17.6%。

Conclusion: 本研究证明了基于元素识别的PPAT素描自动分析在心理状态评估中的有效性，方法为后续自动化心理健康筛查研究提供了新思路和工具。相关数据集和代码已开源。

Abstract: The Drawing Projection Test (DPT) is an essential tool in art therapy,
allowing psychologists to assess participants' mental states through their
sketches. Specifically, through sketches with the theme of "a person picking an
apple from a tree (PPAT)", it can be revealed whether the participants are in
mental states such as depression. Compared with scales, the DPT can enrich
psychologists' understanding of an individual's mental state. However, the
interpretation of the PPAT is laborious and depends on the experience of the
psychologists. To address this issue, we propose an effective identification
method to support psychologists in conducting a large-scale automatic DPT.
Unlike traditional sketch recognition, DPT more focus on the overall evaluation
of the sketches, such as color usage and space utilization. Moreover, PPAT
imposes a time limit and prohibits verbal reminders, resulting in low drawing
accuracy and a lack of detailed depiction. To address these challenges, we
propose the following efforts: (1) Providing an experimental environment for
automated analysis of PPAT sketches for depression assessment; (2) Offering a
Visual-Semantic depression assessment based on LLM (VS-LLM) method; (3)
Experimental results demonstrate that our method improves by 17.6% compared to
the psychologist assessment method. We anticipate that this work will
contribute to the research in mental state assessment based on PPAT sketches'
elements recognition. Our datasets and codes are available at
https://github.com/wmeiqi/VS-LLM.

</details>


### [66] [CoCAViT: Compact Vision Transformer with Robust Global Coordination](https://arxiv.org/abs/2508.05307)
*Xuyang Wang,Lingjuan Miao,Zhiqiang Zhou*

Main category: cs.CV

TL;DR: 本文提出一种新型视觉主干网络CoCAViT，提升小模型在分布外数据集上的泛化能力，同时保持高效和实时性。


<details>
  <summary>Details</summary>
Motivation: 近年大模型虽然通过大规模预训练取得强大特征表达能力，但小模型虽然在标准测试集上可达大模型表现，在分布外数据上却掉点严重，说明现有效率模型泛化较差。因此，研究如何改进小模型的结构以提升其ODD泛化能力非常迫切。

Method: 作者系统性分析了当前小模型存在的结构瓶颈与设计问题。基于分析，本文提出一种Coordinator-patch Cross Attention (CoCA)机制，引入动态、适应域信息的全局token以增强局部与全局特征耦合，并具备极低计算开销。最终融合该机制与多项结构优化，构建了CoCAViT视觉骨干网络。

Result: 在ImageNet-1K图像分类任务上，CoCAViT-28M在224*224分辨率下取得84.0% Top-1准确率；在多个ODD基准上优于同类模型。在COCO检测和ADE20K分割任务分别取得52.2 mAP和51.3 mIOU，并保持低延迟。

Conclusion: 通过结构分析和创新性CoCA注意力机制，CoCAViT模型显著提升了效率模型在分布外场景的鲁棒性和泛化能力，具备实际落地价值。

Abstract: In recent years, large-scale visual backbones have demonstrated remarkable
capabilities in learning general-purpose features from images via extensive
pre-training. Concurrently, many efficient architectures have emerged that have
performance comparable to that of larger models on in-domain benchmarks.
However, we observe that for smaller models, the performance drop on
out-of-distribution (OOD) data is disproportionately larger, indicating a
deficiency in the generalization performance of existing efficient models. To
address this, we identify key architectural bottlenecks and inappropriate
design choices that contribute to this issue, retaining robustness for smaller
models. To restore the global field of pure window attention, we further
introduce a Coordinator-patch Cross Attention (CoCA) mechanism, featuring
dynamic, domain-aware global tokens that enhance local-global feature modeling
and adaptively capture robust patterns across domains with minimal
computational overhead. Integrating these advancements, we present CoCAViT, a
novel visual backbone designed for robust real-time visual representation.
Extensive experiments empirically validate our design. At a resolution of
224*224, CoCAViT-28M achieves 84.0% top-1 accuracy on ImageNet-1K, with
significant gains on multiple OOD benchmarks, compared to competing models. It
also attains 52.2 mAP on COCO object detection and 51.3 mIOU on ADE20K semantic
segmentation, while maintaining low latency.

</details>


### [67] [mKG-RAG: Multimodal Knowledge Graph-Enhanced RAG for Visual Question Answering](https://arxiv.org/abs/2508.05318)
*Xu Yuan,Liangbo Ning,Wenqi Fan,Qing Li*

Main category: cs.CV

TL;DR: 该论文提出了一种将多模态知识图谱（KG）融入到RAG（检索增强生成）框架的新方法，显著提升了知识型视觉问答（VQA）任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统RAG-VQA方法依赖非结构化文档，忽视知识元素之间的结构关系，导致引入无关或误导内容，影响答案准确性和可靠性。为解决这一问题，作者考虑结合结构化的多模态知识图谱。

Method: 提出mKG-RAG框架，结合MLLM驱动的关键词提取和视觉-文本匹配，从多模态文档中抽取语义一致和模态对齐的实体/关系，构建高质量多模态知识图谱。同时引入两阶段检索策略和问题感知型多模态检索器，提高检索效率和精度。

Result: 综合实验验证该方法在知识型VQA任务中显著优于现有方法，取得新的SOTA结果。

Conclusion: 将多模态知识图谱引入RAG框架，能够有效提升多模态知识密集型视觉问答任务的生成能力和答案准确率。

Abstract: Recently, Retrieval-Augmented Generation (RAG) has been proposed to expand
internal knowledge of Multimodal Large Language Models (MLLMs) by incorporating
external knowledge databases into the generation process, which is widely used
for knowledge-based Visual Question Answering (VQA) tasks. Despite impressive
advancements, vanilla RAG-based VQA methods that rely on unstructured documents
and overlook the structural relationships among knowledge elements frequently
introduce irrelevant or misleading content, reducing answer accuracy and
reliability. To overcome these challenges, a promising solution is to integrate
multimodal knowledge graphs (KGs) into RAG-based VQA frameworks to enhance the
generation by introducing structured multimodal knowledge. Therefore, in this
paper, we propose a novel multimodal knowledge-augmented generation framework
(mKG-RAG) based on multimodal KGs for knowledge-intensive VQA tasks.
Specifically, our approach leverages MLLM-powered keyword extraction and
vision-text matching to distill semantically consistent and modality-aligned
entities/relationships from multimodal documents, constructing high-quality
multimodal KGs as structured knowledge representations. In addition, a
dual-stage retrieval strategy equipped with a question-aware multimodal
retriever is introduced to improve retrieval efficiency while refining
precision. Comprehensive experiments demonstrate that our approach
significantly outperforms existing methods, setting a new state-of-the-art for
knowledge-based VQA.

</details>


### [68] [Textual Inversion for Efficient Adaptation of Open-Vocabulary Object Detectors Without Forgetting](https://arxiv.org/abs/2508.05323)
*Frank Ruis,Gertjan Burghouts,Hugo Kuijf*

Main category: cs.CV

TL;DR: 提出了一种类似Textual Inversion（TI）的方法，实现在不开启VLM主体参数训练的情况下扩展或优化VLM中的词汇，用极少样本实现新类别或细粒度目标检测，并保持原有模型的零样本和自然语言检索能力。


<details>
  <summary>Details</summary>
Motivation: 虽然大规模预训练视觉语言模型（VLM）表现优异且具备强大的零样本能力，但针对具体检测目标仍需微调，传统微调会损失零样本能力。作者希望能在不牺牲原有能力和计算效率的前提下，提高模型对新目标的检测能力。

Method: 借鉴文本反演（Textual Inversion, TI）思想，通过极少数据学习新token或优化原有token，并只在token嵌入空间进行参数更新，冻结主模型参数，从而扩展模型可检测对象的种类，并保留零样本推理能力和原生性能。

Result: 新方法在不同定量和定性实验中表现出色，与常规微调或替代方案相比，不仅避免了能力遗忘，还实现了对新类别或细粒度目标的准确检测，同时大大减少了计算资源消耗。

Conclusion: 所提方法兼顾了模型能力提升与性能稳定，允许以低计算代价扩展VLM在新任务上的表现，无损保留其零样本与语言检索等原有优势。

Abstract: Recent progress in large pre-trained vision language models (VLMs) has
reached state-of-the-art performance on several object detection benchmarks and
boasts strong zero-shot capabilities, but for optimal performance on specific
targets some form of finetuning is still necessary. While the initial VLM
weights allow for great few-shot transfer learning, this usually involves the
loss of the original natural language querying and zero-shot capabilities.
Inspired by the success of Textual Inversion (TI) in personalizing
text-to-image diffusion models, we propose a similar formulation for
open-vocabulary object detection. TI allows extending the VLM vocabulary by
learning new or improving existing tokens to accurately detect novel or
fine-grained objects from as little as three examples. The learned tokens are
completely compatible with the original VLM weights while keeping them frozen,
retaining the original model's benchmark performance, and leveraging its
existing capabilities such as zero-shot domain transfer (e.g., detecting a
sketch of an object after training only on real photos). The storage and
gradient calculations are limited to the token embedding dimension, requiring
significantly less compute than full-model fine-tuning. We evaluated whether
the method matches or outperforms the baseline methods that suffer from
forgetting in a wide variety of quantitative and qualitative experiments.

</details>


### [69] [3DGabSplat: 3D Gabor Splatting for Frequency-adaptive Radiance Field Rendering](https://arxiv.org/abs/2508.05343)
*Junyu Zhou,Yuyang Huang,Wenrui Dai,Junni Zou,Ziyang Zheng,Nuowen Kan,Chenglin Li,Hongkai Xiong*

Main category: cs.CV

TL;DR: 论文提出了一种新方法3DGabSplat，通过引入3D Gabor基元素，提升了3D高斯渲染（3DGS）在新视角合成中的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯Splatting（3DGS）虽然能够实现高保真的实时渲染，但由于高斯核本身的低通特性，导致难以表示高频细节，存在冗余基元、效率和内存消耗高等问题。作者希望突破这些局限，提高细节表现和资源利用率。

Method: 作者提出3D Gabor Splatting（3DGabSplat）方法，设计了结合多方向3D频率响应的Gabor基元，利用3D Gabor滤波器组增强对细节捕获能力。除基元改进外，还开发了高效的CUDA光栅化工具投影这些频率分量，并提出了适应性频率优化机制。该方法兼容现有3DGS框架，支持无缝集成。

Result: 实验证明3DGabSplat相较于3DGS及其他变体，在真实和合成场景上均取得了更高的渲染质量，PSNR提升可达1.35 dB，同时减少了基元数量和内存开销。

Conclusion: 3DGabSplat通过引入新型Gabor基元，有效提升了3D新视角渲染的细节表现、效率和内存占用，具备与现有主流方法兼容和替换的潜力，实现了当前最优渲染效果。

Abstract: Recent prominence in 3D Gaussian Splatting (3DGS) has enabled real-time
rendering while maintaining high-fidelity novel view synthesis. However, 3DGS
resorts to the Gaussian function that is low-pass by nature and is restricted
in representing high-frequency details in 3D scenes. Moreover, it causes
redundant primitives with degraded training and rendering efficiency and
excessive memory overhead. To overcome these limitations, we propose 3D Gabor
Splatting (3DGabSplat) that leverages a novel 3D Gabor-based primitive with
multiple directional 3D frequency responses for radiance field representation
supervised by multi-view images. The proposed 3D Gabor-based primitive forms a
filter bank incorporating multiple 3D Gabor kernels at different frequencies to
enhance flexibility and efficiency in capturing fine 3D details. Furthermore,
to achieve novel view rendering, an efficient CUDA-based rasterizer is
developed to project the multiple directional 3D frequency components
characterized by 3D Gabor-based primitives onto the 2D image plane, and a
frequency-adaptive mechanism is presented for adaptive joint optimization of
primitives. 3DGabSplat is scalable to be a plug-and-play kernel for seamless
integration into existing 3DGS paradigms to enhance both efficiency and quality
of novel view synthesis. Extensive experiments demonstrate that 3DGabSplat
outperforms 3DGS and its variants using alternative primitives, and achieves
state-of-the-art rendering quality across both real-world and synthetic scenes.
Remarkably, we achieve up to 1.35 dB PSNR gain over 3DGS with simultaneously
reduced number of primitives and memory consumption.

</details>


### [70] [PriorRG: Prior-Guided Contrastive Pre-training and Coarse-to-Fine Decoding for Chest X-ray Report Generation](https://arxiv.org/abs/2508.05353)
*Kang Liu,Zhuoqi Ma,Zikang Fang,Yunan Li,Kun Xie,Qiguang Miao*

Main category: cs.CV

TL;DR: 本论文提出PriorRG模型，通过利用患者特定的临床上下文和历史影像信息，提高胸部X光自动生成报告的质量和临床相关性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有胸部X光报告生成方法大多仅基于单张影像，忽视了临床实际中放射科医生会参考患者历史信息和最新影像的诊断流程，导致自动生成报告时难以准确体现诊断意图或疾病进展。因此，充分利用患者特定历史和临床上下文成为迫切需求。

Method: 提出了PriorRG框架，采用两阶段训练管线。第一阶段，通过先验引导的对比预训练机制利用临床上下文，指导时空特征提取，更贴合放射报告的语义结构。第二阶段，提出先验感知的粗到细解码方式，将患者特定的历史信息与视觉编码特征逐步融合，用于生成更符合临床流程和逻辑的报告。

Result: 在MIMIC-CXR和MIMIC-ABN数据集上，PriorRG分别提升了3.6%的BLEU-4和3.8%的F1分数（MIMIC-CXR），以及提升了5.9%的BLEU-1分数（MIMIC-ABN），整体表现优于最新方法。

Conclusion: PriorRG框架通过结合患者历史信息和临床上下文，显著提升了自动化X光报告的准确性和流畅性，对提高临床实际应用价值具有重要意义。代码和模型将在论文接收后公开。

Abstract: Chest X-ray report generation aims to reduce radiologists' workload by
automatically producing high-quality preliminary reports. A critical yet
underexplored aspect of this task is the effective use of patient-specific
prior knowledge -- including clinical context (e.g., symptoms, medical history)
and the most recent prior image -- which radiologists routinely rely on for
diagnostic reasoning. Most existing methods generate reports from single
images, neglecting this essential prior information and thus failing to capture
diagnostic intent or disease progression. To bridge this gap, we propose
PriorRG, a novel chest X-ray report generation framework that emulates
real-world clinical workflows via a two-stage training pipeline. In Stage 1, we
introduce a prior-guided contrastive pre-training scheme that leverages
clinical context to guide spatiotemporal feature extraction, allowing the model
to align more closely with the intrinsic spatiotemporal semantics in radiology
reports. In Stage 2, we present a prior-aware coarse-to-fine decoding for
report generation that progressively integrates patient-specific prior
knowledge with the vision encoder's hidden states. This decoding allows the
model to align with diagnostic focus and track disease progression, thereby
enhancing the clinical accuracy and fluency of the generated reports. Extensive
experiments on MIMIC-CXR and MIMIC-ABN datasets demonstrate that PriorRG
outperforms state-of-the-art methods, achieving a 3.6% BLEU-4 and 3.8% F1 score
improvement on MIMIC-CXR, and a 5.9% BLEU-1 gain on MIMIC-ABN. Code and
checkpoints will be released upon acceptance.

</details>


### [71] [Cross-View Localization via Redundant Sliced Observations and A-Contrario Validation](https://arxiv.org/abs/2508.05369)
*Yongjun Zhang,Mingtao Xiong,Yi Wan,Gui-Song Xia*

Main category: cs.CV

TL;DR: 提出了一种新方法Slice-Loc，通过将地面图片切分成子图独立定位，并利用几何约束和假警报数（NFA）评估可靠性，大幅提升了跨视角定位的准确率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有跨视角定位（CVL）方法只输出单一样本，缺乏冗余观测，无法通过观测数据互相验证定位可靠性。这在实际应用（如智能车辆）中难以检测和排除定位失败或大误差。本论文旨在利用冗余观测提升定位结果的可靠性验证能力。

Method: 提出两阶段方法Slice-Loc：首先将单张地面查询图像切分为多个子图像，并分别估算每个子图的3自由度（3-DoF）相机位姿，得到冗余且相互独立的观测。其次，设计几何刚性公式过滤错误位姿，并融合内点生成最终相机位姿。此外，通过分析切片定位分布，采用NFA模型定量评估定位成果的可靠性，有效检测误定位。

Result: 实验表明，Slice-Loc能够在过滤误定位后，将定位误差大于10米的比例降至3%以下。在DReSS跨城市数据集上，平均定位误差从4.47米降至1.86米，平均朝向误差从3.42°降至1.24°，全面优于现有最佳方法。

Conclusion: Slice-Loc通过冗余观测和可靠性验证机制，显著提升了GNSS拒止环境下的跨视角图像定位准确性与鲁棒性，为实际应用提供更可靠的自定位解决方案。

Abstract: Cross-view localization (CVL) matches ground-level images with aerial
references to determine the geo-position of a camera, enabling smart vehicles
to self-localize offline in GNSS-denied environments. However, most CVL methods
output only a single observation, the camera pose, and lack the redundant
observations required by surveying principles, making it challenging to assess
localization reliability through the mutual validation of observational data.
To tackle this, we introduce Slice-Loc, a two-stage method featuring an
a-contrario reliability validation for CVL. Instead of using the query image as
a single input, Slice-Loc divides it into sub-images and estimates the 3-DoF
pose for each slice, creating redundant and independent observations. Then, a
geometric rigidity formula is proposed to filter out the erroneous 3-DoF poses,
and the inliers are merged to generate the final camera pose. Furthermore, we
propose a model that quantifies the meaningfulness of localization by
estimating the number of false alarms (NFA), according to the distribution of
the locations of the sliced images. By eliminating gross errors, Slice-Loc
boosts localization accuracy and effectively detects failures. After filtering
out mislocalizations, Slice-Loc reduces the proportion of errors exceeding 10 m
to under 3\%. In cross-city tests on the DReSS dataset, Slice-Loc cuts the mean
localization error from 4.47 m to 1.86 m and the mean orientation error from
$\mathbf{3.42^{\circ}}$ to $\mathbf{1.24^{\circ}}$, outperforming
state-of-the-art methods. Code and dataset will be available at:
https://github.com/bnothing/Slice-Loc.

</details>


### [72] [CT-GRAPH: Hierarchical Graph Attention Network for Anatomy-Guided CT Report Generation](https://arxiv.org/abs/2508.05375)
*Hamza Kalisch,Fabian Hörst,Jens Kleesiek,Ken Herrmann,Constantin Seibold*

Main category: cs.CV

TL;DR: 本文提出了一种基于分层图注意力网络（CT-GRAPH）的方法，用于自动生成胸部CT的放射学报告，在结构化解剖区域的基础上融合全局与器官级特征，结合大语言模型生成更为准确详细的报告，在公开任务上大幅提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动生成放射学报告的研究大多只利用全局图像特征，忽略了重要的细粒度器官关系，使得生成报告的准确性受限。由于放射科诊断高度依赖不同解剖结构之间的关系，提升器官级信息建模能力势在必行。

Method: 方法上，作者提出了CT-GRAPH，将医学解剖知识以图结构进行表达，节点表示不同解剖区域，通过引入解剖掩膜获得器官和全局特征，利用图注意力机制对特征进一步融合，然后结合大语言模型生成放射学报告。

Result: 在大规模胸部CT数据集CT-RATE上实验，方法在报告生成任务中，F1分数相比当前最优方法提升了绝对7.9%。并对不同预训练特征编码器进行了深入分析。

Conclusion: CT-GRAPH通过结构化细粒度的解剖信息并结合大模型，显著提升了自动放射学报告生成的质量，有望为临床自动化辅助诊断提供更强的技术支撑。

Abstract: As medical imaging is central to diagnostic processes, automating the
generation of radiology reports has become increasingly relevant to assist
radiologists with their heavy workloads. Most current methods rely solely on
global image features, failing to capture fine-grained organ relationships
crucial for accurate reporting. To this end, we propose CT-GRAPH, a
hierarchical graph attention network that explicitly models radiological
knowledge by structuring anatomical regions into a graph, linking fine-grained
organ features to coarser anatomical systems and a global patient context. Our
method leverages pretrained 3D medical feature encoders to obtain global and
organ-level features by utilizing anatomical masks. These features are further
refined within the graph and then integrated into a large language model to
generate detailed medical reports. We evaluate our approach for the task of
report generation on the large-scale chest CT dataset CT-RATE. We provide an
in-depth analysis of pretrained feature encoders for CT report generation and
show that our method achieves a substantial improvement of absolute 7.9\% in F1
score over current state-of-the-art methods. The code is publicly available at
https://github.com/hakal104/CT-GRAPH.

</details>


### [73] [Deformable Attention Graph Representation Learning for Histopathology Whole Slide Image Analysis](https://arxiv.org/abs/2508.05382)
*Mingxi Fu,Xitong Ling,Yuxuan Chen,Jiawen Li,fanglei fu,Huaitian Yuan,Tian Guan,Yonghong He,Lianghui Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种结合可变形注意力机制的图神经网络（GNN）框架，用于提升全幻灯片图像（WSI）和感兴趣区域（ROI）的分类性能，并在主流数据集上取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 现有的计算病理图像分类方法多采用多示例学习（MIL）或静态图神经网络（GNN），但它们难以有效捕捉组织结构间的空间依赖，且常规注意力机制不能专门聚焦在结构上相关的重要区域。

Method: 作者提出基于patch特征自适应构建动态加权有向图，每个节点通过带有注意力权重的边聚合邻域信息，并引入可学习的空间偏移（依据patch的物理坐标），实现模型对空间结构相关区域的自适应关注。

Result: 所提方法在TCGA-COAD、BRACS、肠上皮化生分级和肠ROI分类四个基准数据集上取得了最先进的分类效果。

Conclusion: 可变形注意力机制增强了上下文信息的获取同时保留空间特异性，有效提升了对WSI及ROI中复杂空间结构的分类能力。

Abstract: Accurate classification of Whole Slide Images (WSIs) and Regions of Interest
(ROIs) is a fundamental challenge in computational pathology. While mainstream
approaches often adopt Multiple Instance Learning (MIL), they struggle to
capture the spatial dependencies among tissue structures. Graph Neural Networks
(GNNs) have emerged as a solution to model inter-instance relationships, yet
most rely on static graph topologies and overlook the physical spatial
positions of tissue patches. Moreover, conventional attention mechanisms lack
specificity, limiting their ability to focus on structurally relevant regions.
In this work, we propose a novel GNN framework with deformable attention for
pathology image analysis. We construct a dynamic weighted directed graph based
on patch features, where each node aggregates contextual information from its
neighbors via attention-weighted edges. Specifically, we incorporate learnable
spatial offsets informed by the real coordinates of each patch, enabling the
model to adaptively attend to morphologically relevant regions across the
slide. This design significantly enhances the contextual field while preserving
spatial specificity. Our framework achieves state-of-the-art performance on
four benchmark datasets (TCGA-COAD, BRACS, gastric intestinal metaplasia
grading, and intestinal ROI classification), demonstrating the power of
deformable attention in capturing complex spatial structures in WSIs and ROIs.

</details>


### [74] [UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation](https://arxiv.org/abs/2508.05399)
*Wonjun Kang,Byeongkeun Ahn,Minjae Lee,Kevin Galim,Seunghyuk Oh,Hyung Il Koo,Nam Ik Cho*

Main category: cs.CV

TL;DR: 该论文提出了一种用于提升文本到图像生成中组合性表现的新方法UNCAGE，能够在无需重新训练模型的情况下有效提升文本与图像的对应与组合属性绑定的准确性。


<details>
  <summary>Details</summary>
Motivation: 文本到图像（T2I）生成模型在将多个属性或对象精确结合到图片时依然存在挑战，现有的扩散模型及Masked Generative Transformers（MGT）都容易出现文本与图像未对齐、属性混淆等问题。尽管扩散模型相关的改进被广泛研究，MGT在组合性方面的局限性及解决方法却鲜有探讨，因此需要新的方法提升其组合表现。

Method: 作者提出一种全新的、无需再训练原模型的方法UNCAGE（Unmasking with Contrastive Attention Guidance），其核心是利用注意力图来优先解码能清晰代表单独物体的token，从而提升文本描述中不同对象和属性在图像生成中的准确映射和组合。

Result: UNCAGE在多个基准测试和评价指标上都带来了显著提升，无论是定量评价还是定性分析均优于现有方法，推理开销几乎可以忽略。

Conclusion: UNCAGE作为一种训练外部的注意力引导解码算法，有效改善了Masked Generative Transformers在文本到图像生成中的组合问题，为更精确多属性、多对象描述的自动图像生成提供了实用方案。

Abstract: Text-to-image (T2I) generation has been actively studied using Diffusion
Models and Autoregressive Models. Recently, Masked Generative Transformers have
gained attention as an alternative to Autoregressive Models to overcome the
inherent limitations of causal attention and autoregressive decoding through
bidirectional attention and parallel decoding, enabling efficient and
high-quality image generation. However, compositional T2I generation remains
challenging, as even state-of-the-art Diffusion Models often fail to accurately
bind attributes and achieve proper text-image alignment. While Diffusion Models
have been extensively studied for this issue, Masked Generative Transformers
exhibit similar limitations but have not been explored in this context. To
address this, we propose Unmasking with Contrastive Attention Guidance
(UNCAGE), a novel training-free method that improves compositional fidelity by
leveraging attention maps to prioritize the unmasking of tokens that clearly
represent individual objects. UNCAGE consistently improves performance in both
quantitative and qualitative evaluations across multiple benchmarks and
metrics, with negligible inference overhead. Our code is available at
https://github.com/furiosa-ai/uncage.

</details>


### [75] [From Detection to Correction: Backdoor-Resilient Face Recognition via Vision-Language Trigger Detection and Noise-Based Neutralization](https://arxiv.org/abs/2508.05409)
*Farah Wahida,M. A. P. Chamikara,Yashothara Shanmugarasa,Mohan Baruwal Chhetri,Thilina Ranbaduge,Ibrahim Khalil*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法TrueBiometric，通过多模型投票检测和修正后门攻击下的人脸识别数据集中的中毒样本，提升了人脸识别系统的安全性。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的人脸识别等生物特征识别系统容易受到后门攻击，已有防御方法在精准识别和消除中毒样本时仍面临数据实用性受损等难题。如何不损失系统性能的前提下，高效准确地检测并修正中毒样本成为亟需解决的问题。

Method: 作者提出TrueBiometric方法，利用多种先进的大型视觉语言模型进行多数投票，精确识别中毒图像。识别后通过有针对性的定量校正噪声方法修复这些样本，从而去除后门攻击影响。

Result: 实验结果显示，TrueBiometric可以100%准确地检测和修正中毒图像，对干净样本的识别准确率无损失，并优于现有最新防御方法。

Conclusion: TrueBiometric提供了一种更实用、准确且有效的人脸识别系统后门攻防解决方案，在不牺牲数据实用性的情况下极大提升了系统的可靠性。

Abstract: Biometric systems, such as face recognition systems powered by deep neural
networks (DNNs), rely on large and highly sensitive datasets. Backdoor attacks
can subvert these systems by manipulating the training process. By inserting a
small trigger, such as a sticker, make-up, or patterned mask, into a few
training images, an adversary can later present the same trigger during
authentication to be falsely recognized as another individual, thereby gaining
unauthorized access. Existing defense mechanisms against backdoor attacks still
face challenges in precisely identifying and mitigating poisoned images without
compromising data utility, which undermines the overall reliability of the
system. We propose a novel and generalizable approach, TrueBiometric:
Trustworthy Biometrics, which accurately detects poisoned images using a
majority voting mechanism leveraging multiple state-of-the-art large vision
language models. Once identified, poisoned samples are corrected using targeted
and calibrated corrective noise. Our extensive empirical results demonstrate
that TrueBiometric detects and corrects poisoned images with 100\% accuracy
without compromising accuracy on clean images. Compared to existing
state-of-the-art approaches, TrueBiometric offers a more practical, accurate,
and effective solution for mitigating backdoor attacks in face recognition
systems.

</details>


### [76] [Physical Adversarial Camouflage through Gradient Calibration and Regularization](https://arxiv.org/abs/2508.05414)
*Jiawei Liang,Siyuan Liang,Jianjie Huang,Chenxi Si,Ming Zhang,Xiaochun Cao*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于梯度优化的对抗伪装框架，有效提升物理环境中对深度目标检测器的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 物理对抗伪装通过改变物体纹理欺骗深度检测器，已对自动驾驶等安全领域造成风险。现有技术难以适应多变物理环境，主要有采样点密度不一致和多角度纹理梯度冲突两大难题，严重影响攻击效果与稳定性。

Method: 提出梯度校准策略，使不同距离下纹理点梯度更新一致。同时，开发梯度去相关方法，依据损失值优先并正交化多角度梯度，提升多角度优化的稳定性和效果。

Result: 大规模实验显示，该方法在不同检测模型、角度和距离下对抗攻击成功率显著提升，ASR平均提升13.46%（距离维度）和11.03%（角度维度）。

Conclusion: 新提出的对抗伪装框架有效提升了物理世界中对深度检测系统的攻击能力，实验还表明现有系统设计在真实环境下仍需加强鲁棒性。

Abstract: The advancement of deep object detectors has greatly affected safety-critical
fields like autonomous driving. However, physical adversarial camouflage poses
a significant security risk by altering object textures to deceive detectors.
Existing techniques struggle with variable physical environments, facing two
main challenges: 1) inconsistent sampling point densities across distances
hinder the gradient optimization from ensuring local continuity, and 2)
updating texture gradients from multiple angles causes conflicts, reducing
optimization stability and attack effectiveness. To address these issues, we
propose a novel adversarial camouflage framework based on gradient
optimization. First, we introduce a gradient calibration strategy, which
ensures consistent gradient updates across distances by propagating gradients
from sparsely to unsampled texture points. Additionally, we develop a gradient
decorrelation method, which prioritizes and orthogonalizes gradients based on
loss values, enhancing stability and effectiveness in multi-angle optimization
by eliminating redundant or conflicting updates. Extensive experimental results
on various detection models, angles and distances show that our method
significantly exceeds the state of the art, with an average increase in attack
success rate (ASR) of 13.46% across distances and 11.03% across angles.
Furthermore, empirical evaluation in real-world scenarios highlights the need
for more robust system design.

</details>


### [77] [Smoothing Slot Attention Iterations and Recurrences](https://arxiv.org/abs/2508.05417)
*Rongzhen Zhao,Wenyan Yang,Juho Kannala,Joni Pajarinen*

Main category: cs.CV

TL;DR: 本文提出了SmoothSA方法，改进Slot Attention在图像和视频中聚合对象时的冷启动问题，通过预加热初始化向量和差异化跨帧变换，提高了对象聚合的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的Slot Attention在处理第一帧的对象聚合时采用统一的冷启动查询向量，缺乏针对具体样本的特征，导致聚合不精准。同时后续帧的查询向量已经包含样本特异性信息，但处理方式与第一帧相同，不够灵活。因此有必要分别优化不同场景下的聚合方式。

Method: 提出SmoothSA方法：一方面通过一个自蒸馏的小模块，用输入特征丰富第一帧冷启动查询向量，实现预加热；另一方面，对第一帧和非第一帧采用差异化的Slot Attention迭代次数（第一帧用完整迭代，非第一帧用单步迭代），以匹配它们不同的需求。

Result: 在对象发现、识别等多种任务和基准测试中，SmoothSA方法取得了更优的实验效果。作者同时开展了分析，展示该方法如何在迭代和递归过程中提升聚合的平滑性和精度。

Conclusion: SmoothSA有效解决了Slot Attention在不同帧对象聚合中的冷启动与递归不足问题，提升了对象中心学习任务的效果。

Abstract: Slot Attention (SA) and its variants lie at the heart of mainstream
Object-Centric Learning (OCL). Objects in an image can be aggregated into
respective slot vectors, by \textit{iteratively} refining cold-start query
vectors, typically three times, via SA on image features. For video, such
aggregation is \textit{recurrently} shared across frames, with queries
cold-started on the first frame while transitioned from the previous frame's
slots on non-first frames. However, the cold-start queries lack sample-specific
cues thus hinder precise aggregation on the image or video's first frame; Also,
non-first frames' queries are already sample-specific thus require transforms
different from the first frame's aggregation. We address these issues for the
first time with our \textit{SmoothSA}: (1) To smooth SA iterations on the image
or video's first frame, we \textit{preheat} the cold-start queries with rich
information of input features, via a tiny module self-distilled inside OCL; (2)
To smooth SA recurrences across all video frames, we \textit{differentiate} the
homogeneous transforms on the first and non-first frames, by using full and
single iterations respectively. Comprehensive experiments on object discovery,
recognition and downstream benchmarks validate our method's effectiveness.
Further analyses intuitively illuminate how our method smooths SA iterations
and recurrences. Our code is available in the supplement.

</details>


### [78] [Explaining Similarity in Vision-Language Encoders with Weighted Banzhaf Interactions](https://arxiv.org/abs/2508.05430)
*Hubert Baniecki,Maximilian Muschalik,Fabian Fumagalli,Barbara Hammer,Eyke Hüllermeier,Przemyslaw Biecek*

Main category: cs.CV

TL;DR: FIxLIP方法提升了语言-图像预训练模型解释的质量，尤其在反映多模态交互方面优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有解释方法主要基于一阶归因，无法捕捉到视觉-语言编码器中复杂的跨模态高阶交互，因此难以全面解释模型对图文输入匹配度的判断机制。

Method: 提出了基于博弈论Banzhaf交互指数的FIxLIP方法，实现对模型相似性输出的二阶（及更高阶）交互归因。算法相比Shapley框架更高效，并扩展了相关的解释评估指标以适用于高阶交互。

Result: 在MS COCO和ImageNet-1k基准测试中，FIxLIP（二阶方法）在解释可靠性和质量上均优于现有一阶归因方法。

Conclusion: FIxLIP不仅可生成更具解释力的多模态模型归因，可用于比较不同视觉-语言模型（如CLIP与SigLIP-2、不同ViT变体）之间的机制差异，对模型理解和改进极具价值。

Abstract: Language-image pre-training (LIP) enables the development of vision-language
models capable of zero-shot classification, localization, multimodal retrieval,
and semantic understanding. Various explanation methods have been proposed to
visualize the importance of input image-text pairs on the model's similarity
outputs. However, popular saliency maps are limited by capturing only
first-order attributions, overlooking the complex cross-modal interactions
intrinsic to such encoders. We introduce faithful interaction explanations of
LIP models (FIxLIP) as a unified approach to decomposing the similarity in
vision-language encoders. FIxLIP is rooted in game theory, where we analyze how
using the weighted Banzhaf interaction index offers greater flexibility and
improves computational efficiency over the Shapley interaction quantification
framework. From a practical perspective, we propose how to naturally extend
explanation evaluation metrics, like the pointing game and area between the
insertion/deletion curves, to second-order interaction explanations.
Experiments on MS COCO and ImageNet-1k benchmarks validate that second-order
methods like FIxLIP outperform first-order attribution methods. Beyond
delivering high-quality explanations, we demonstrate the utility of FIxLIP in
comparing different models like CLIP vs. SigLIP-2 and ViT-B/32 vs. ViT-L/16.

</details>


### [79] [How and Why: Taming Flow Matching for Unsupervised Anomaly Detection and Localization](https://arxiv.org/abs/2508.05461)
*Liangwei Li,Lin Liu,Juanxiu Liu,Jing Zhang,Ruqian Hao,Xiaohui Du*

Main category: cs.CV

TL;DR: 本文提出了一种基于Flow Matching（FM）的全新无监督异常检测与定位方法，通过新理论克服了传统流模型表达能力有限的问题，并首次在MVTec数据集上取得了当前最优结果。


<details>
  <summary>Details</summary>
Motivation: 传统的基于流的异常检测方法受限于模型表达能力，且在高维空间下对异常点的分离能力较弱。本文旨在通过理论分析和新范式提升异常检测与定位的效果。

Method: 作者形式化提出了time-reversed Flow Matching（rFM）作为矢量场回归，将未知分布映射到高斯分布。分析表明线性插值路径不可逆，反向高斯路径在高维下易退化。基于此，作者提出Worst Transport (WT) 位移插值，提出WT-Flow，通过构建动力学控制，增强了正常与异常样本的区分。

Result: WT-Flow首次成功将FM应用于无监督异常检测，并在MVTec数据集上单尺度实现了最先进的性能。

Conclusion: WT-Flow为无监督异常检测提供了理论保障的新分离机制和可扩展的计算框架，有望推动异常检测领域进一步发展。

Abstract: We propose a new paradigm for unsupervised anomaly detection and localization
using Flow Matching (FM), which fundamentally addresses the model expressivity
limitations of conventional flow-based methods. To this end, we formalize the
concept of time-reversed Flow Matching (rFM) as a vector field regression along
a predefined probability path to transform unknown data distributions into
standard Gaussian. We bring two core observations that reshape our
understanding of FM. First, we rigorously prove that FM with linear
interpolation probability paths is inherently non-invertible. Second, our
analysis reveals that employing reversed Gaussian probability paths in
high-dimensional spaces can lead to trivial vector fields. This issue arises
due to the manifold-related constraints. Building on the second observation, we
propose Worst Transport (WT) displacement interpolation to reconstruct a
non-probabilistic evolution path. The proposed WT-Flow enhances dynamical
control over sample trajectories, constructing ''degenerate potential wells''
for anomaly-free samples while allowing anomalous samples to escape. This novel
unsupervised paradigm offers a theoretically grounded separation mechanism for
anomalous samples. Notably, FM provides a computationally tractable framework
that scales to complex data. We present the first successful application of FM
for the unsupervised anomaly detection task, achieving state-of-the-art
performance at a single scale on the MVTec dataset. The reproducible code for
training will be released upon camera-ready submission.

</details>


### [80] [F2PASeg: Feature Fusion for Pituitary Anatomy Segmentation in Endoscopic Surgery](https://arxiv.org/abs/2508.05465)
*Lumin Chen,Zhiying Wu,Tianye Lei,Xuexue Bai,Ming Feng,Yuxi Wang,Gaofeng Meng,Zhen Lei,Hongbin Liu*

Main category: cs.CV

TL;DR: 本文提出了一个新的垂体解剖结构分割数据集（PAS），并开发了结合特征融合模块的分割方法（F2PASeg），用于提升手术场景下关键解剖结构的分割准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 垂体肿瘤常导致邻近重要结构变形或包绕，增加了手术风险。精确的解剖结构分割能为外科医生术中预警风险区域，但现有垂体手术场景下带像素级标注的视频数据极为稀缺。

Method: 作者构建了 PAS 数据集，收集自 120 段手术视频的 7,845 张时序相关图像。为缓解类别不平衡，引入了模拟手术器械的数据增强方法。提出了一种特征融合模块（Feature Fusion），融合高分辨率图像特征与深层语义嵌入，实现 F2PASeg 模型，提升对结构遮挡、摄像头移动和术中出血等干扰的鲁棒性。

Result: 实验证明，所提 F2PASeg 方法在实时分割关键解剖结构方面表现优异，优于常规方法，能适应术中各种变化。

Conclusion: 新公开的 PAS 数据集和 F2PASeg 方法为提高手术安全性和术中规划决策提供了有效的技术支持，并具备实际应用价值。

Abstract: Pituitary tumors often cause deformation or encapsulation of adjacent vital
structures. Anatomical structure segmentation can provide surgeons with early
warnings of regions that pose surgical risks, thereby enhancing the safety of
pituitary surgery. However, pixel-level annotated video stream datasets for
pituitary surgeries are extremely rare. To address this challenge, we introduce
a new dataset for Pituitary Anatomy Segmentation (PAS). PAS comprises 7,845
time-coherent images extracted from 120 videos. To mitigate class imbalance, we
apply data augmentation techniques that simulate the presence of surgical
instruments in the training data. One major challenge in pituitary anatomy
segmentation is the inconsistency in feature representation due to occlusions,
camera motion, and surgical bleeding. By incorporating a Feature Fusion module,
F2PASeg is proposed to refine anatomical structure segmentation by leveraging
both high-resolution image features and deep semantic embeddings, enhancing
robustness against intraoperative variations. Experimental results demonstrate
that F2PASeg consistently segments critical anatomical structures in real time,
providing a reliable solution for intraoperative pituitary surgery planning.
Code: https://github.com/paulili08/F2PASeg.

</details>


### [81] [Keep It Real: Challenges in Attacking Compression-Based Adversarial Purification](https://arxiv.org/abs/2508.05489)
*Samuel Räber,Till Aczel,Andreas Plesner,Roger Wattenhofer*

Main category: cs.CV

TL;DR: 本文研究了通过有损压缩预处理图像防御对抗扰动的有效性，并发现能高保真还原图像的压缩模型，对强白盒与自适应攻击也有显著的鲁棒性。对抗攻击未来的一大挑战在于突破高真实感重建图像的天然防护效果。


<details>
  <summary>Details</summary>
Motivation: 之前的研究表明，有损压缩处理可作为防御对抗攻击的手段，但针对压缩模型的系统性攻击评测不足，因此有必要厘清这类方法实际安全性及面临的核心挑战。

Method: 作者设计并实施了强白盒与自适应攻击，针对多种压缩模型进行对比测试，重点分析高真实感重建对攻击难度的影响，并检验鲁棒性的成因。

Result: 实验发现，能够高保真、真实还原图像分布的压缩模型，相较低真实感模型，能显著抵御包括自适应攻击在内的多种攻击方式。这一鲁棒性并非梯度掩蔽所致。

Conclusion: 高真实感的图像重建为对抗攻击带来了实质性挑战。未来对抗攻击方法的改进必须专注于如何绕过这种天然鲁棒性，这对安全评估提出更高要求。

Abstract: Previous work has suggested that preprocessing images through lossy
compression can defend against adversarial perturbations, but comprehensive
attack evaluations have been lacking. In this paper, we construct strong
white-box and adaptive attacks against various compression models and identify
a critical challenge for attackers: high realism in reconstructed images
significantly increases attack difficulty. Through rigorous evaluation across
multiple attack scenarios, we demonstrate that compression models capable of
producing realistic, high-fidelity reconstructions are substantially more
resistant to our attacks. In contrast, low-realism compression models can be
broken. Our analysis reveals that this is not due to gradient masking. Rather,
realistic reconstructions maintaining distributional alignment with natural
images seem to offer inherent robustness. This work highlights a significant
obstacle for future adversarial attacks and suggests that developing more
effective techniques to overcome realism represents an essential challenge for
comprehensive security evaluation.

</details>


### [82] [SMOL-MapSeg: Show Me One Label](https://arxiv.org/abs/2508.05501)
*Yunshuang Yuan,Frank Thiemann,Thorsten Dahms,Monika Sester*

Main category: cs.CV

TL;DR: 本文提出了一种新方法（SMOL-MapSeg），通过在基础模型中引入“按需声明式知识提示”（OND prompting），显著提升了深度学习模型对历史地图的分割与识别能力。


<details>
  <summary>Details</summary>
Motivation: 当前主流的深度学习分割模型虽然在自动驾驶、医学影像等领域表现出色，但在历史地图中因风格多变、缺乏一致性，预训练模型难以泛化。本研究动机是解决基础模型在历史地图分割时表现不佳的问题。

Method: 本文提出按需声明式知识提示（OND prompting）机制，用户可在推理阶段显式指定目标概念与模式。具体做法为替换基础分割模型（SAM）的提示编码器，结合少样本微调技术，开发出SMOL-MapSeg模型。

Result: 实验结果显示，SMOL-MapSeg可精准分割由OND知识定义的类别，支持对未见类别的少样本适应，并在分割性能上优于UNet基线模型。

Conclusion: 引入声明式知识提示可显著提升历史地图分割的准确率和泛化能力，SMOL-MapSeg为处理异构、变化多端的地图数据提供了有效工具。

Abstract: Historical maps are valuable for studying changes to the Earth's surface.
With the rise of deep learning, models like UNet have been used to extract
information from these maps through semantic segmentation. Recently,
pre-trained foundation models have shown strong performance across domains such
as autonomous driving, medical imaging, and industrial inspection. However,
they struggle with historical maps. These models are trained on modern or
domain-specific images, where patterns can be tied to predefined concepts
through common sense or expert knowledge. Historical maps lack such consistency
-- similar concepts can appear in vastly different shapes and styles. To
address this, we propose On-Need Declarative (OND) knowledge-based prompting,
which introduces explicit prompts to guide the model on what patterns
correspond to which concepts. This allows users to specify the target concept
and pattern during inference (on-need inference). We implement this by
replacing the prompt encoder of the foundation model SAM with our OND prompting
mechanism and fine-tune it on historical maps. The resulting model is called
SMOL-MapSeg (Show Me One Label). Experiments show that SMOL-MapSeg can
accurately segment classes defined by OND knowledge. It can also adapt to
unseen classes through few-shot fine-tuning. Additionally, it outperforms a
UNet-based baseline in average segmentation performance.

</details>


### [83] [AutoIAD: Manager-Driven Multi-Agent Collaboration for Automated Industrial Anomaly Detection](https://arxiv.org/abs/2508.05503)
*Dongwei Ji,Bingzhang Hu,Yi Zhou*

Main category: cs.CV

TL;DR: 提出了一种工业异常检测的自动化多智能体协作框架AutoIAD，可端到端完成制造业视觉异常检测模型的开发，实验显示其优于通用智能体和AutoML框架。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测对制造业质检至关重要，但传统方法常需大量人工，仅适用于特定场景，因此需要更高效、自动化、通用的开发流程。

Method: 本文提出AutoIAD框架，采用以Manager为核心的多智能体系统，包含数据准备、加载、模型设计、训练等子智能体，并引入工业领域知识库，实现从原始图片到模型的全流程自动处理。通过在MVTec AD数据集上，结合不同大模型后端，对框架进行了系统评测。

Result: 实验表明，AutoIAD在任务完成率、模型性能（AUROC）等方面，显著优于现有通用多智能体和AutoML框架。此外，AutoIAD可通过迭代优化有效缓解幻觉等问题。消融实验验证了Manager智能体和知识库对系统稳健性和性能提升的核心作用。

Conclusion: AutoIAD能高效自动化地开发高性能的工业视觉异常检测模型，显著降低了人工参与门槛，有潜力广泛应用于工业质控场景，未来可进一步推动自动化建模发展。

Abstract: Industrial anomaly detection (IAD) is critical for manufacturing quality
control, but conventionally requires significant manual effort for various
application scenarios. This paper introduces AutoIAD, a multi-agent
collaboration framework, specifically designed for end-to-end automated
development of industrial visual anomaly detection. AutoIAD leverages a
Manager-Driven central agent to orchestrate specialized sub-agents (including
Data Preparation, Data Loader, Model Designer, Trainer) and integrates a
domain-specific knowledge base, which intelligently handles the entire pipeline
using raw industrial image data to develop a trained anomaly detection model.
We construct a comprehensive benchmark using MVTec AD datasets to evaluate
AutoIAD across various LLM backends. Extensive experiments demonstrate that
AutoIAD significantly outperforms existing general-purpose agentic
collaboration frameworks and traditional AutoML frameworks in task completion
rate and model performance (AUROC), while effectively mitigating issues like
hallucination through iterative refinement. Ablation studies further confirm
the crucial roles of the Manager central agent and the domain knowledge base
module in producing robust and high-quality IAD solutions.

</details>


### [84] [Symmetry Understanding of 3D Shapes via Chirality Disentanglement](https://arxiv.org/abs/2508.05505)
*Weikang Wang,Tobias Weißberg,Nafie El Amrani,Florian Bernard*

Main category: cs.CV

TL;DR: 本文提出了一种无监督手性（chirality）特征提取方法，用于改善点云和网格等形状分析中的左右区分问题，并验证了其在多项任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然手性信息在图像领域已被广泛研究，但在点云和网格等形状分析中的相关研究还很薄弱。现有的形状描述符对左右对称结构常常不能有效区分，缺乏对手性敏感的特征。因此，开发能够提取手性信息的特征提取器，提升形状分析的精度，具有重要意义。

Method: 本文基于 Diff3F 框架，提出了一套无监督的流程，能够利用2D大型预训练模型的信息为形状的顶点提取和赋予手性特征。这些特征无需监督标注，能自动补充现有描述符在左右区分方面的不足。

Result: 通过在多个公开数据集上的定量和定性实验，本文方法在左右区分、形状匹配、部件分割等下游任务中表现出优秀和实用性，有效提升了形状分析任务的性能。

Conclusion: 本文提出的手性特征提取器为点云和网格带来了区分左右的新能力，能够极大增强下游形状分析任务的表现，为形状描述符带来实用的新特性。

Abstract: Chirality information (i.e. information that allows distinguishing left from
right) is ubiquitous for various data modes in computer vision, including
images, videos, point clouds, and meshes. While chirality has been extensively
studied in the image domain, its exploration in shape analysis (such as point
clouds and meshes) remains underdeveloped. Although many shape vertex
descriptors have shown appealing properties (e.g. robustness to rigid-body
transformations), they are often not able to disambiguate between left and
right symmetric parts. Considering the ubiquity of chirality information in
different shape analysis problems and the lack of chirality-aware features
within current shape descriptors, developing a chirality feature extractor
becomes necessary and urgent. Based on the recent Diff3F framework, we propose
an unsupervised chirality feature extraction pipeline to decorate shape
vertices with chirality-aware information, extracted from 2D foundation models.
We evaluated the extracted chirality features through quantitative and
qualitative experiments across diverse datasets. Results from downstream tasks
including left-right disentanglement, shape matching, and part segmentation
demonstrate their effectiveness and practical utility. Project page:
https://wei-kang-wang.github.io/chirality/

</details>


### [85] [MagicHOI: Leveraging 3D Priors for Accurate Hand-object Reconstruction from Short Monocular Video Clips](https://arxiv.org/abs/2508.05506)
*Shibo Wang,Haonan He,Maria Parelli,Christoph Gebhardt,Zicong Fan,Jie Song*

Main category: cs.CV

TL;DR: 本文提出了MagicHOI方法，实现了在单目交互视频下，即使视角变化有限，也能重建手部和物体，并解决了传统方法在物体部分被遮挡时重建效果不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于RGB的手-物体三维重建方法普遍依赖物体模板或假设物体完全可见，这在现实场景中往往难以满足（如摄像机固定、抓握手势不变导致部分物体被遮挡），从而影响重建的准确性和真实性。

Method: MagicHOI以大规模新视角合成扩散模型为先验，为未见到的物体区域提供监督约束。具体做法是将新视角合成模型集成进手-物体三维重建框架，并引入可见接触点约束，实现对手和物体的空间对齐和更合理的三维重建。

Result: MagicHOI在实验中取得了显著优于当前主流手-物体重建方法的性能，尤其是在视角受限、物体部分不可见的情况下。新视角合成的扩散先验明显提升了未观测区域的三维重建质量。

Conclusion: MagicHOI克服了因遮挡和视角受限带来的重建难题，通过新视角合成扩散先验和手-物体约束实现了更精确、合理的三维重建，对实际场景具有更强的适用性和鲁棒性。

Abstract: Most RGB-based hand-object reconstruction methods rely on object templates,
while template-free methods typically assume full object visibility. This
assumption often breaks in real-world settings, where fixed camera viewpoints
and static grips leave parts of the object unobserved, resulting in implausible
reconstructions. To overcome this, we present MagicHOI, a method for
reconstructing hands and objects from short monocular interaction videos, even
under limited viewpoint variation. Our key insight is that, despite the
scarcity of paired 3D hand-object data, large-scale novel view synthesis
diffusion models offer rich object supervision. This supervision serves as a
prior to regularize unseen object regions during hand interactions. Leveraging
this insight, we integrate a novel view synthesis model into our hand-object
reconstruction framework. We further align hand to object by incorporating
visible contact constraints. Our results demonstrate that MagicHOI
significantly outperforms existing state-of-the-art hand-object reconstruction
methods. We also show that novel view synthesis diffusion priors effectively
regularize unseen object regions, enhancing 3D hand-object reconstruction.

</details>


### [86] [Revealing Latent Information: A Physics-inspired Self-supervised Pre-training Framework for Noisy and Sparse Events](https://arxiv.org/abs/2508.05507)
*Lin Zhu,Ruonan Liu,Xiao Wang,Lizhi Wang,Hua Huang*

Main category: cs.CV

TL;DR: 本文提出了一种针对事件相机数据的自监督预训练框架，通过差分引导的掩码建模、特征迁移和聚焦对比学习，充分挖掘事件数据中的边缘和纹理信息，在物体识别、语义分割与光流估计等任务上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 事件相机虽然拥有高时域分辨率和宽动态范围，在复杂场景下有很大潜力，但其数据稀疏、噪声大且仅反映亮度变化，导致特征提取效果有限。因此，需要专门的特征挖掘和建模方法，以提升事件数据在各种视觉任务中的应用效果。

Method: 提出三阶段自监督预训练方法：（1）差分引导的掩码建模，重建时间强度差分图获取更丰富信息；（2）骨干网络固定的特征迁移，对比事件与图像特征，巩固预训练表示并稳定对比学习；（3）聚焦对比学习，通过关注高价值区域，优化语义判别能力，实现全模型更新。

Result: 广泛实验验证了该框架在物体识别、语义分割、光流估计等下游任务上均明显优于当前最先进方法。

Conclusion: 所提出的事件数据自监督预训练方法有效提升了特征提取能力，具有强鲁棒性和泛化性，为事件相机数据在多种视觉任务中的应用与发展提供了新方向。

Abstract: Event camera, a novel neuromorphic vision sensor, records data with high
temporal resolution and wide dynamic range, offering new possibilities for
accurate visual representation in challenging scenarios. However, event data is
inherently sparse and noisy, mainly reflecting brightness changes, which
complicates effective feature extraction. To address this, we propose a
self-supervised pre-training framework to fully reveal latent information in
event data, including edge information and texture cues. Our framework consists
of three stages: Difference-guided Masked Modeling, inspired by the event
physical sampling process, reconstructs temporal intensity difference maps to
extract enhanced information from raw event data. Backbone-fixed Feature
Transition contrasts event and image features without updating the backbone to
preserve representations learned from masked modeling and stabilizing their
effect on contrastive learning. Focus-aimed Contrastive Learning updates the
entire model to improve semantic discrimination by focusing on high-value
regions. Extensive experiments show our framework is robust and consistently
outperforms state-of-the-art methods on various downstream tasks, including
object recognition, semantic segmentation, and optical flow estimation. The
code and dataset are available at https://github.com/BIT-Vision/EventPretrain.

</details>


### [87] [Head Anchor Enhanced Detection and Association for Crowded Pedestrian Tracking](https://arxiv.org/abs/2508.05514)
*Zewei Wu,César Teixeira,Wei Ke,Zhang Xiong*

Main category: cs.CV

TL;DR: 本文提出了一种改进的多行人视觉跟踪方法，增强了特征表达及运动建模，显著提高了拥挤和遮挡场景下的跟踪鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前行人跟踪领域在实际应用中面临严重的遮挡问题，尤其在多行人交互或重叠时目标特征会丢失，导致跟踪轨迹不稳定。传统方法依赖全身特征和简单运动模型，难以应对严重遮挡。因此需要新的方法以提升遮挡环境下的跟踪性能。

Method: 1. 特征层面，结合了检测器的回归与分类分支特征，将空间和位置信息直接嵌入特征表达。2. 为应对遮挡引入头部关键点检测用于辅助跟踪，因为头部较不易被遮挡。3. 运动模型方面，采用迭代卡尔曼滤波，与现代检测器假设相一致，并融入3D先验信息，更好地补全复杂场景中的运动轨迹。

Result: 经丰富特征与改进运动建模的方案，在多目标、拥挤与高遮挡场景下显著提升了行人跟踪的鲁棒性和性能。

Conclusion: 结合深度特征增强与更鲁棒的运动建模，可以有效提升遮挡环境下的多行人跟踪效果，为智能监控、人机交互等领域提供更可靠的解决方案。

Abstract: Visual pedestrian tracking represents a promising research field, with
extensive applications in intelligent surveillance, behavior analysis, and
human-computer interaction. However, real-world applications face significant
occlusion challenges. When multiple pedestrians interact or overlap, the loss
of target features severely compromises the tracker's ability to maintain
stable trajectories. Traditional tracking methods, which typically rely on
full-body bounding box features extracted from {Re-ID} models and linear
constant-velocity motion assumptions, often struggle in severe occlusion
scenarios. To address these limitations, this work proposes an enhanced
tracking framework that leverages richer feature representations and a more
robust motion model. Specifically, the proposed method incorporates detection
features from both the regression and classification branches of an object
detector, embedding spatial and positional information directly into the
feature representations. To further mitigate occlusion challenges, a head
keypoint detection model is introduced, as the head is less prone to occlusion
compared to the full body. In terms of motion modeling, we propose an iterative
Kalman filtering approach designed to align with modern detector assumptions,
integrating 3D priors to better complete motion trajectories in complex scenes.
By combining these advancements in appearance and motion modeling, the proposed
method offers a more robust solution for multi-object tracking in crowded
environments where occlusions are prevalent.

</details>


### [88] [FS-IQA: Certified Feature Smoothing for Robust Image Quality Assessment](https://arxiv.org/abs/2508.05516)
*Ekaterina Shumitskaya,Dmitriy Vatolin,Anastasia Antsiferova*

Main category: cs.CV

TL;DR: 本论文提出了一种新颖的针对图像质量评估（IQA）模型的认证防御方法，利用在特征空间而非输入空间添加噪声的随机平滑策略，显著提高了鲁棒性且大幅减少了计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统的随机平滑认证方法往往在输入图像上直接添加高斯噪声，导致视觉质量受损，不适合对图像质量敏感的IQA任务。因此，亟需一种能够在保证图像原始质量的前提下，提升鲁棒性并提供理论认证的新方法。

Method: 本文方法在特征空间注入噪声，通过分析主干网络Jacobian的最大奇异值，将特征空间与输入空间的扰动建立理论联系。方法适用于全参考和无参考IQA模型，无需修改原有架构且只需一次前向推理，极大提高了计算效率。

Result: 在两个基准数据集和六个常用IQA模型上实验，结果显示该方法与五种最新认证防御方法相比，推理时间无认证时减少99.5%，认证时减少20.6%，同时与主观质量分数的相关性提升高达30.9%。

Conclusion: 该方法在保持图像质量的同时，有效提升了IQA模型的鲁棒性和推理效率，为实际IQA应用提供了更优的认证防御选择。

Abstract: We propose a novel certified defense method for Image Quality Assessment
(IQA) models based on randomized smoothing with noise applied in the feature
space rather than the input space. Unlike prior approaches that inject Gaussian
noise directly into input images, often degrading visual quality, our method
preserves image fidelity while providing robustness guarantees. To formally
connect noise levels in the feature space with corresponding input-space
perturbations, we analyze the maximum singular value of the backbone network's
Jacobian. Our approach supports both full-reference (FR) and no-reference (NR)
IQA models without requiring any architectural modifications, suitable for
various scenarios. It is also computationally efficient, requiring a single
backbone forward pass per image. Compared to previous methods, it reduces
inference time by 99.5% without certification and by 20.6% when certification
is applied. We validate our method with extensive experiments on two benchmark
datasets, involving six widely-used FR and NR IQA models and comparisons
against five state-of-the-art certified defenses. Our results demonstrate
consistent improvements in correlation with subjective quality scores by up to
30.9%.

</details>


### [89] [Leveraging AI to Accelerate Clinical Data Cleaning: A Comparative Study of AI-Assisted vs. Traditional Methods](https://arxiv.org/abs/2508.05519)
*Matthew Purri,Amit Patel,Erik Deurrell*

Main category: cs.CV

TL;DR: 本文提出了一种结合大型语言模型与领域专用启发式的新型人工智能平台Octozi，能够大幅提升临床试验数据清洗效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 随着临床试验数据量和复杂度的指数级增长，传统手动数据审核方式难以应对，成为药物开发瓶颈，因此需要高效、智能的数据清洗解决方案。

Method: 提出并实验评估Octozi平台，该平台融合大语言模型与专业启发规则，支持有经验的临床评审员进行数据审核。在对照实验中，记录了AI协助下数据清洗速度和准确性的变化。

Result: 有AI辅助的数据清洗效率提升6.03倍，错误率从54.67%降至8.48%（提升6.44倍），同时大幅降低了15.48倍的误报（false positive queries）。这一改进在不同经验层次的审核员中均保持一致。

Conclusion: AI辅助方法可有效解决临床试验操作中的低效与高错问题，有望加速药物开发进程、降低成本，同时保证合规性，建立了AI融入临床安全操作流程的基础。

Abstract: Clinical trial data cleaning represents a critical bottleneck in drug
development, with manual review processes struggling to manage exponentially
increasing data volumes and complexity. This paper presents Octozi, an
artificial intelligence-assisted platform that combines large language models
with domain-specific heuristics to transform clinical data review. In a
controlled experimental study with experienced clinical reviewers (n=10), we
demonstrate that AI assistance increased data cleaning throughput by 6.03-fold
while simultaneously decreasing cleaning errors from 54.67% to 8.48% (a
6.44-fold improvement). Crucially, the system reduced false positive queries by
15.48-fold, minimizing unnecessary site burden. These improvements were
consistent across reviewers regardless of experience level, suggesting broad
applicability. Our findings indicate that AI-assisted approaches can address
fundamental inefficiencies in clinical trial operations, potentially
accelerating drug development timelines and reducing costs while maintaining
regulatory compliance. This work establishes a framework for integrating AI
into safety-critical clinical workflows and demonstrates the transformative
potential of human-AI collaboration in pharmaceutical clinical trials.

</details>


### [90] [Optimal Brain Connection: Towards Efficient Structural Pruning](https://arxiv.org/abs/2508.05521)
*Shaowu Chen,Wei Ma,Binhua Huang,Qingyuan Wang,Guoxin Wang,Weize Sun,Lei Huang,Deepu John*

Main category: cs.CV

TL;DR: 本文提出了一种新的结构剪枝框架Optimal Brain Connection，包括基于雅可比矩阵的评估标准以及等价剪枝机制，以提升压缩神经网络时模型性能的保留能力。


<details>
  <summary>Details</summary>
Motivation: 现有结构剪枝方法忽视了参数间相互联系，导致剪枝后模型性能下降。本文旨在更好地捕捉参数间依赖，提升剪枝有效性。

Method: 提出了两项改进：1）雅可比评判标准，作为评估结构参数重要性的一级度量，能显式捕捉网络内外部的依赖关系；2）等价剪枝机制，利用自编码器，在微调时保存被剪参数的贡献。

Result: 实验表明，雅可比评判标准在模型性能保留方面优于多种已有标准，等价剪枝机制有效减缓了微调后的性能下降。

Conclusion: Optimal Brain Connection结构剪枝框架在压缩神经网络的同时能更好地保留模型性能，对实际神经网络压缩有较高参考价值。

Abstract: Structural pruning has been widely studied for its effectiveness in
compressing neural networks. However, existing methods often neglect the
interconnections among parameters. To address this limitation, this paper
proposes a structural pruning framework termed Optimal Brain Connection. First,
we introduce the Jacobian Criterion, a first-order metric for evaluating the
saliency of structural parameters. Unlike existing first-order methods that
assess parameters in isolation, our criterion explicitly captures both
intra-component interactions and inter-layer dependencies. Second, we propose
the Equivalent Pruning mechanism, which utilizes autoencoders to retain the
contributions of all original connection--including pruned ones--during
fine-tuning. Experimental results demonstrate that the Jacobian Criterion
outperforms several popular metrics in preserving model performance, while the
Equivalent Pruning mechanism effectively mitigates performance degradation
after fine-tuning. Code: https://github.com/ShaowuChen/Optimal_Brain_Connection

</details>


### [91] [When Deepfake Detection Meets Graph Neural Network:a Unified and Lightweight Learning Framework](https://arxiv.org/abs/2508.05526)
*Haoyu Liu,Chaoyu Gong,Mengke He,Jiate Li,Kai Han,Siqiang Luo*

Main category: cs.CV

TL;DR: 本文提出了一种新型轻量化的视频真伪检测模型SSTGNN，该框架能更好地识别多种类型的AI生成与篡改视频，且参数量远低于主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频真伪检测方法通常只关注空间、时间或频谱某一方面，且模型参数庞大，难以适应多类型篡改，无法高效部署于实际场景。需要一种通用、高效且轻量的检测方法。

Method: 作者提出空间-频谱-时间图神经网络（SSTGNN），将视频建模为结构化图，结合可学习频谱滤波器和时间差分建模，有效联合捕捉篡改留下的空间不一致、时间伪影及频谱畸变。

Result: 在多个主流检测基准数据集上，SSTGNN在同域和跨域场景中均表现出色，对未见过的篡改类型也有很强的鲁棒性。同时，参数量比现有先进模型最多少42.4倍，极大提升了模型的轻量化和可扩展性。

Conclusion: SSTGNN在保证检测性能的同时，大幅减少参数量，兼具泛化能力强和应用实现性高，为实际部署视频真伪检测系统提供了有效解决方案。

Abstract: The proliferation of generative video models has made detecting AI-generated
and manipulated videos an urgent challenge. Existing detection approaches often
fail to generalize across diverse manipulation types due to their reliance on
isolated spatial, temporal, or spectral information, and typically require
large models to perform well. This paper introduces SSTGNN, a lightweight
Spatial-Spectral-Temporal Graph Neural Network framework that represents videos
as structured graphs, enabling joint reasoning over spatial inconsistencies,
temporal artifacts, and spectral distortions. SSTGNN incorporates learnable
spectral filters and temporal differential modeling into a graph-based
architecture, capturing subtle manipulation traces more effectively. Extensive
experiments on diverse benchmark datasets demonstrate that SSTGNN not only
achieves superior performance in both in-domain and cross-domain settings, but
also offers strong robustness against unseen manipulations. Remarkably, SSTGNN
accomplishes these results with up to 42.4$\times$ fewer parameters than
state-of-the-art models, making it highly lightweight and scalable for
real-world deployment.

</details>


### [92] [AI vs. Human Moderators: A Comparative Evaluation of Multimodal LLMs in Content Moderation for Brand Safety](https://arxiv.org/abs/2508.05527)
*Adi Levi,Or Levi,Sardhendu Mishra,Jonathan Morra*

Main category: cs.CV

TL;DR: 该论文评估了多模态大模型（MLLMs）在品牌安全审核中的表现，并公开了一个新的多模态多语种数据集。


<details>
  <summary>Details</summary>
Motivation: 随着线上视频内容暴增，人工审核已无法满足安全性审核需求，尤其在广告中的品牌安全领域。因此，迫切需要自动化、准确且高效的内容审核方法。

Method: 作者提出了一个由专业人士标签的多模态、多语种、含多种风险类别的视频数据集。利用该数据集，比较分析了Gemini、GPT和Llama等MLLMs在品牌安全审核任务中的表现，并与人工审核人员在准确性和成本效率上进行了对比。

Result: 实验结果显示，MLLMs在多模态品牌安全审核领域具有较高的准确率与成本效益，但仍存在一定的局限性和失败案例。

Conclusion: MLLMs在视频品牌安全自动化审核方面表现出应用潜力，公开的数据集可推动该领域进一步研究，但还需正视其当前的不足与改进空间。

Abstract: As the volume of video content online grows exponentially, the demand for
moderation of unsafe videos has surpassed human capabilities, posing both
operational and mental health challenges. While recent studies demonstrated the
merits of Multimodal Large Language Models (MLLMs) in various video
understanding tasks, their application to multimodal content moderation, a
domain that requires nuanced understanding of both visual and textual cues,
remains relatively underexplored. In this work, we benchmark the capabilities
of MLLMs in brand safety classification, a critical subset of content
moderation for safe-guarding advertising integrity. To this end, we introduce a
novel, multimodal and multilingual dataset, meticulously labeled by
professional reviewers in a multitude of risk categories. Through a detailed
comparative analysis, we demonstrate the effectiveness of MLLMs such as Gemini,
GPT, and Llama in multimodal brand safety, and evaluate their accuracy and cost
efficiency compared to professional human reviewers. Furthermore, we present an
in-depth discussion shedding light on limitations of MLLMs and failure cases.
We are releasing our dataset alongside this paper to facilitate future research
on effective and responsible brand safety and content moderation.

</details>


### [93] [Looking into the Unknown: Exploring Action Discovery for Segmentation of Known and Unknown Actions](https://arxiv.org/abs/2508.05529)
*Federico Spurio,Emad Bahrami,Olga Zatsarynna,Yazan Abu Farha,Gianpiero Francesca,Juergen Gall*

Main category: cs.CV

TL;DR: 本文提出了一种名为Action Discovery的新任务，在只部分标注的时间序列动作数据中探索已知与未知动作，重点解决标签不完整和含糊问题。该方法通过两步策略，分别识别和归类未知动作，大幅提升了现有基线上的表现。


<details>
  <summary>Details</summary>
Motivation: 现实中，很多领域（如神经科学等）只具备部分明确动作的标注，而一些细微、不常见动作往往被遗漏，此外数据集本身也可能存在标签不完整或含糊的问题。如何在这样的数据下有效识别和区分未知动作，是一个实际但未充分解决的问题。

Method: 方法包括两个主要模块：1）Granularity-Guided Segmentation Module (GGSM)，模仿已标注动作的粒度去划分已知与未知动作的时间段；2）Unknown Action Segment Assignment (UASA)，应用嵌入相似度，将未知动作片段分配为有语义意义的新的类别。

Result: 在Breakfast、50Salads和Desktop Assembly三个具有挑战性的数据集上系统验证了方法，实验结果显示该方法能显著优于现有基线方法。

Conclusion: 本文的方法能够在部分标注的数据中有效发现和区分未知动作，对处理标注不完整及复杂实际数据场景具有广泛应用前景。

Abstract: We introduce Action Discovery, a novel setup within Temporal Action
Segmentation that addresses the challenge of defining and annotating ambiguous
actions and incomplete annotations in partially labeled datasets. In this
setup, only a subset of actions - referred to as known actions - is annotated
in the training data, while other unknown actions remain unlabeled. This
scenario is particularly relevant in domains like neuroscience, where
well-defined behaviors (e.g., walking, eating) coexist with subtle or
infrequent actions that are often overlooked, as well as in applications where
datasets are inherently partially annotated due to ambiguous or missing labels.
To address this problem, we propose a two-step approach that leverages the
known annotations to guide both the temporal and semantic granularity of
unknown action segments. First, we introduce the Granularity-Guided
Segmentation Module (GGSM), which identifies temporal intervals for both known
and unknown actions by mimicking the granularity of annotated actions. Second,
we propose the Unknown Action Segment Assignment (UASA), which identifies
semantically meaningful classes within the unknown actions, based on learned
embedding similarities. We systematically explore the proposed setting of
Action Discovery on three challenging datasets - Breakfast, 50Salads, and
Desktop Assembly - demonstrating that our method considerably improves upon
existing baselines.

</details>


### [94] [Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis](https://arxiv.org/abs/2508.05580)
*Kunyu Feng,Yue Ma,Xinhua Zhang,Boshi Liu,Yikuang Yuluo,Yinhan Zhang,Runtao Liu,Hongyu Liu,Zhiyuan Qin,Shanhui Mo,Qifeng Chen,Zeyu Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种多模态大语言模型驱动的数据自动合成框架Follow-Your-Instruction，能够高效生成高质量2D、3D和4D数据，并显著提升生成模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成内容需求增加，高质量、大规模、多样化数据需求急剧上升，但真实数据收集成本高、效率低，限制了相关下游任务发展。现有方法多依赖手工构建场景，导致扩展性和准确性受限。作者希望建立一种自动化、可扩展、通用的数据生成系统。

Method: 作者提出Follow-Your-Instruction框架：首先使用MLLM-Collector模块，结合多模态输入收集资产及其描述；然后通过MLLM-Generator和MLLM-Optimizer，构建和优化三维布局和多视角场景，利用视觉-语言模型进行语义精炼；最后由MLLM-Planner生成时间一致的未来帧，从而合成2D、3D、4D高质量数据。

Result: 在2D、3D和4D生成任务上进行综合实验，结果表明该方法生成的数据能显著提升现有基线模型的性能。

Conclusion: 本文提出的Follow-Your-Instruction框架是一个高扩展性、高效率的数据引擎，能够解决数据生成瓶颈，为生成式智能提供强力支持。

Abstract: With the growing demands of AI-generated content (AIGC), the need for
high-quality, diverse, and scalable data has become increasingly crucial.
However, collecting large-scale real-world data remains costly and
time-consuming, hindering the development of downstream applications. While
some works attempt to collect task-specific data via a rendering process, most
approaches still rely on manual scene construction, limiting their scalability
and accuracy. To address these challenges, we propose Follow-Your-Instruction,
a Multimodal Large Language Model (MLLM)-driven framework for automatically
synthesizing high-quality 2D, 3D, and 4D data. Our
\textbf{Follow-Your-Instruction} first collects assets and their associated
descriptions through multimodal inputs using the MLLM-Collector. Then it
constructs 3D layouts, and leverages Vision-Language Models (VLMs) for semantic
refinement through multi-view scenes with the MLLM-Generator and
MLLM-Optimizer, respectively. Finally, it uses MLLM-Planner to generate
temporally coherent future frames. We evaluate the quality of the generated
data through comprehensive experiments on the 2D, 3D, and 4D generative tasks.
The results show that our synthetic data significantly boosts the performance
of existing baseline models, demonstrating Follow-Your-Instruction's potential
as a scalable and effective data engine for generative intelligence.

</details>


### [95] [DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label Recognition](https://arxiv.org/abs/2508.05585)
*Haijing Liu,Tao Pu,Hefeng Wu,Keze Wang,Liang Lin*

Main category: cs.CV

TL;DR: 本文提出了DART框架，通过自适应地优化视觉-语言预训练模型，实现对图像中已知和未知多标签类别的精准识别，在细粒度定位和类别关系推理上取得了突破性进展，达成了新的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言预训练模型虽具开放词汇能力，但在弱监督下细粒度目标定位能力弱，且无法充分利用结构化关系知识，尤其在未见类别上表现受限，因此需要创新方法提升多标签识别精度及泛化性。

Method: 提出Dual Adaptive Refinement Transfer (DART) 框架，在冻结的VLP主干网络基础上，引入两大模块：1）自适应细化模块（ARM）结合新颖的弱监督Patch选择损失，在仅有图像级标签下提升细粒度区域区分能力；2）自适应转移模块（ATM）基于以大语言模型挖掘获得的类关系图CRG，通过图注意力机制，在类别表示间自适应传递结构化知识。

Result: 在多个具挑战性的公开多标签识别数据集上，DART框架取得了超越以往方法的最新最优性能。

Conclusion: DART首次实现了将外部LLM关系知识有效融入多标签识别中的类别关系推理，并在弱监督下实现类别内区分性提升，为多标签开放词汇识别领域树立了新标杆。

Abstract: Open-Vocabulary Multi-Label Recognition (OV-MLR) aims to identify multiple
seen and unseen object categories within an image, requiring both precise
intra-class localization to pinpoint objects and effective inter-class
reasoning to model complex category dependencies. While Vision-Language
Pre-training (VLP) models offer a strong open-vocabulary foundation, they often
struggle with fine-grained localization under weak supervision and typically
fail to explicitly leverage structured relational knowledge beyond basic
semantics, limiting performance especially for unseen classes. To overcome
these limitations, we propose the Dual Adaptive Refinement Transfer (DART)
framework. DART enhances a frozen VLP backbone via two synergistic adaptive
modules. For intra-class refinement, an Adaptive Refinement Module (ARM)
refines patch features adaptively, coupled with a novel Weakly Supervised Patch
Selecting (WPS) loss that enables discriminative localization using only
image-level labels. Concurrently, for inter-class transfer, an Adaptive
Transfer Module (ATM) leverages a Class Relationship Graph (CRG), constructed
using structured knowledge mined from a Large Language Model (LLM), and employs
graph attention network to adaptively transfer relational information between
class representations. DART is the first framework, to our knowledge, to
explicitly integrate external LLM-derived relational knowledge for adaptive
inter-class transfer while simultaneously performing adaptive intra-class
refinement under weak supervision for OV-MLR. Extensive experiments on
challenging benchmarks demonstrate that our DART achieves new state-of-the-art
performance, validating its effectiveness.

</details>


### [96] [WeTok: Powerful Discrete Tokenization for High-Fidelity Visual Reconstruction](https://arxiv.org/abs/2508.05599)
*Shaobin Zhuang,Yiwei Guo,Canmiao Fu,Zhipeng Huang,Zeyue Tian,Ying Zhang,Chen Li,Yali Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新型视觉分词器WeTok，通过分组无查找量化（GQ）和生成式解码（GD）两大创新，有效提升了视觉生成系统中信息压缩和重建精度的表现，在主流数据集上刷新了性能记录。


<details>
  <summary>Details</summary>
Motivation: 现有视觉分词器在压缩比与重建保真度间难以兼顾，急需突破性方法提升视觉生成质量并降低资源消耗。

Method: WeTok包括两项创新：(1) 分组无查找量化（GQ）将潜在特征分组后分别进行无查找量化，极大提升了码本规模和效率；(2) 生成式解码（GD）引入额外噪声变量为先验，用概率建模方式提升高压缩率下的细节重建能力。

Result: 在ImageNet 50k等数据集上，WeTok的指标优于当前主流方法，如在零样本rFID上取得0.12（优于FLUX-VAE的0.18和SD-VAE3.5的0.19），同时以高压缩比（768）达到了优于Cosmos（384）的3.49（Cosmos为4.57），展现显著优势。

Conclusion: WeTok在提升视觉生成压缩率和重建精度方面表现突出，为视觉分词器设计提供了新的方向和工具，具有实际应用推广价值。

Abstract: Visual tokenizer is a critical component for vision generation. However, the
existing tokenizers often face unsatisfactory trade-off between compression
ratios and reconstruction fidelity. To fill this gap, we introduce a powerful
and concise WeTok tokenizer, which surpasses the previous leading tokenizers
via two core innovations. (1) Group-wise lookup-free Quantization (GQ). We
partition the latent features into groups, and perform lookup-free quantization
for each group. As a result, GQ can efficiently overcome memory and computation
limitations of prior tokenizers, while achieving a reconstruction breakthrough
with more scalable codebooks. (2) Generative Decoding (GD). Different from
prior tokenizers, we introduce a generative decoder with a prior of extra noise
variable. In this case, GD can probabilistically model the distribution of
visual data conditioned on discrete tokens, allowing WeTok to reconstruct
visual details, especially at high compression ratios. Extensive experiments on
mainstream benchmarks show superior performance of our WeTok. On the ImageNet
50k validation set, WeTok achieves a record-low zero-shot rFID (WeTok: 0.12 vs.
FLUX-VAE: 0.18 vs. SD-VAE 3.5: 0.19). Furthermore, our highest compression
model achieves a zero-shot rFID of 3.49 with a compression ratio of 768,
outperforming Cosmos (384) 4.57 which has only 50% compression rate of ours.
Code and models are available: https://github.com/zhuangshaobin/WeTok.

</details>


### [97] [LLaVA-RE: Binary Image-Text Relevancy Evaluation with Multimodal Large Language Model](https://arxiv.org/abs/2508.05602)
*Tao Sun,Oliver Liu,JinJin Li,Lan Ma*

Main category: cs.CV

TL;DR: 本文提出了一种基于多模态大语言模型（MLLM）的二元图文相关性评价方法LLaVA-RE，并构建了新的相关性数据集，实验结果证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 图文生成任务中，评估生成内容与输入之间的相关性非常重要，尤其是“相关”与“不相关”的二元判别，但实际应用中由于文本多样性和相关性定义不一，该任务具有挑战性。

Method: 作者利用多模态大语言模型（MLLM），结合LLaVA架构，采用详细任务指令和多模态上下文样例实现图文相关性二元判别。此外，作者还构建了覆盖多种任务的全新标注数据集。

Result: 实验结果表明，基于MLLM和LLaVA-RE框架的二元相关性判断效果良好，有效提升了评测准确性。

Conclusion: 本文首次实现了通过多模态大语言模型进行图文相关性二元判别，为多模态生成任务的自动化评价提供了高效新工具。

Abstract: Multimodal generative AI usually involves generating image or text responses
given inputs in another modality. The evaluation of image-text relevancy is
essential for measuring response quality or ranking candidate responses. In
particular, binary relevancy evaluation, i.e., ``Relevant'' vs. ``Not
Relevant'', is a fundamental problem. However, this is a challenging task
considering that texts have diverse formats and the definition of relevancy
varies in different scenarios. We find that Multimodal Large Language Models
(MLLMs) are an ideal choice to build such evaluators, as they can flexibly
handle complex text formats and take in additional task information. In this
paper, we present LLaVA-RE, a first attempt for binary image-text relevancy
evaluation with MLLM. It follows the LLaVA architecture and adopts detailed
task instructions and multimodal in-context samples. In addition, we propose a
novel binary relevancy data set that covers various tasks. Experimental results
validate the effectiveness of our framework.

</details>


### [98] [Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity](https://arxiv.org/abs/2508.05609)
*Yuhan Zhang,Long Zhuo,Ziyang Chu,Tong Wu,Zhibing Li,Liang Pan,Dahua Lin,Ziwei Liu*

Main category: cs.CV

TL;DR: 该论文提出Hi3DEval，一个用于3D生成内容的层次化评价框架，并推出配套的大规模数据集和自动评分系统，实现了比现有图像指标更全面、更贴合人类偏好的3D质量评估。


<details>
  <summary>Details</summary>
Motivation: 现有3D内容生成的质量评估方法依赖图像指标、且只关注整体，难以捕捉空间一致性、材料真实性和细节质量，因此需要更科学、细致的3D评估手段。

Method: 作者设计Hi3DEval框架，结合了对象级和部件级的多维度评估，并扩展对纹理的评价尤其关注材料真实感（如反照率、饱和度、金属度等）。为支持该框架，作者构建了大规模3D资产和高质量标注的数据集Hi3DBench，并提出了基于混合3D表征的自动评分系统，利用视频表示建模对象和材料感知，部件层面用预训练3D特征。

Result: 实验结果表明，该方法在捕捉3D特性、和人类偏好一致性方面，明显优于基于图像的传统指标，可大规模替代人工评估。

Conclusion: Hi3DEval及其配套工具集极大提升了3D资产质量评估的科学性、精细度和自动化能力，为3D内容生成和应用的进一步发展提供了可靠基础。

Abstract: Despite rapid advances in 3D content generation, quality assessment for the
generated 3D assets remains challenging. Existing methods mainly rely on
image-based metrics and operate solely at the object level, limiting their
ability to capture spatial coherence, material authenticity, and high-fidelity
local details. 1) To address these challenges, we introduce Hi3DEval, a
hierarchical evaluation framework tailored for 3D generative content. It
combines both object-level and part-level evaluation, enabling holistic
assessments across multiple dimensions as well as fine-grained quality
analysis. Additionally, we extend texture evaluation beyond aesthetic
appearance by explicitly assessing material realism, focusing on attributes
such as albedo, saturation, and metallicness. 2) To support this framework, we
construct Hi3DBench, a large-scale dataset comprising diverse 3D assets and
high-quality annotations, accompanied by a reliable multi-agent annotation
pipeline. We further propose a 3D-aware automated scoring system based on
hybrid 3D representations. Specifically, we leverage video-based
representations for object-level and material-subject evaluations to enhance
modeling of spatio-temporal consistency and employ pretrained 3D features for
part-level perception. Extensive experiments demonstrate that our approach
outperforms existing image-based metrics in modeling 3D characteristics and
achieves superior alignment with human preference, providing a scalable
alternative to manual evaluations. The project page is available at
https://zyh482.github.io/Hi3DEval/.

</details>


### [99] [MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes](https://arxiv.org/abs/2508.05630)
*Henghui Ding,Kaining Ying,Chang Liu,Shuting He,Xudong Jiang,Yu-Gang Jiang,Philip H. S. Torr,Song Bai*

Main category: cs.CV

TL;DR: 本文提出了一个更具挑战性的复杂视频目标分割数据集MOSEv2，以提升分割模型在真实世界环境下的表现，并对多种现有方法进行评测，结果显示当前模型在复杂场景下性能明显下滑。


<details>
  <summary>Details</summary>
Motivation: 现有的视频目标分割数据集主要包含突出、简单、分离的目标，难以反映真实世界的复杂场景，因此需要更复杂的数据集来推动分割技术实际应用。

Method: 作者构建了MOSEv2数据集，包含5024个视频、70多万高质量掩膜、10074个对象和200类。该数据集设计了更复杂的场景设置，包括遮挡、消失/重现、小目标、恶劣天气、低光照、多镜头、伪像目标、需要外部知识等多种新挑战。同时，针对现有20种VOS方法和9种目标跟踪方法在5种不同设置下进行了基准测试。

Result: 在MOSEv2上，典型分割方法如SAM2的分数从MOSEv1的76.4%下降到50.9%；目标跟踪方法也有类似降幅，表明当前技术在复杂场景下难以维持高性能。

Conclusion: 虽然现有VOS方法在传统数据集上表现优异，但在环境复杂尤其接近真实世界的MOSEv2上性能大降，反映出现有方法难以应对真实应用挑战，MOSEv2将促进该领域进一步发展。

Abstract: Video object segmentation (VOS) aims to segment specified target objects
throughout a video. Although state-of-the-art methods have achieved impressive
performance (e.g., 90+% J&F) on existing benchmarks such as DAVIS and
YouTube-VOS, these datasets primarily contain salient, dominant, and isolated
objects, limiting their generalization to real-world scenarios. To advance VOS
toward more realistic environments, coMplex video Object SEgmentation (MOSEv1)
was introduced to facilitate VOS research in complex scenes. Building on the
strengths and limitations of MOSEv1, we present MOSEv2, a significantly more
challenging dataset designed to further advance VOS methods under real-world
conditions. MOSEv2 consists of 5,024 videos and over 701,976 high-quality masks
for 10,074 objects across 200 categories. Compared to its predecessor, MOSEv2
introduces significantly greater scene complexity, including more frequent
object disappearance and reappearance, severe occlusions and crowding, smaller
objects, as well as a range of new challenges such as adverse weather (e.g.,
rain, snow, fog), low-light scenes (e.g., nighttime, underwater), multi-shot
sequences, camouflaged objects, non-physical targets (e.g., shadows,
reflections), scenarios requiring external knowledge, etc. We benchmark 20
representative VOS methods under 5 different settings and observe consistent
performance drops. For example, SAM2 drops from 76.4% on MOSEv1 to only 50.9%
on MOSEv2. We further evaluate 9 video object tracking methods and find similar
declines, demonstrating that MOSEv2 presents challenges across tasks. These
results highlight that despite high accuracy on existing datasets, current VOS
methods still struggle under real-world complexities. MOSEv2 is publicly
available at https://MOSE.video.

</details>


### [100] [GAP: Gaussianize Any Point Clouds with Text Guidance](https://arxiv.org/abs/2508.05631)
*Weiqi Zhang,Junsheng Zhou,Haotian Geng,Wenyuan Zhang,Yu-Shen Liu*

Main category: cs.CV

TL;DR: 这篇论文提出了GAP方法，将原始无色的点云数据生成高保真的3D高斯分布，并结合文本指导实现高质量渲染。


<details>
  <summary>Details</summary>
Motivation: 点云作为3D表示方式应用广泛，但直接从无色点云生成可渲染的高斯体模型存在挑战。已有工作针对有色点云，但无色点云到高斯的高质量转换问题尚未得到解决。

Method: 作者设计了多视角优化框架，利用深度感知的图像扩散模型在不同视角间合成一致外观，并提出表面锚定机制，使高斯体依附于3D物体表面，同时引入基于扩散的补全策略以填补难观测区域。整个过程可根据文本进行外观指导。

Result: GAP方法在点云转高斯体的各类测试（包括合成、真实及大规模场景）中均展现出优异性能，确保外观与几何结构准确、高保真。

Conclusion: GAP有效解决了无色点云到高斯体的生成问题，提供了高效且高质量的3D可视化方案，可拓展用于真实世界复杂场景。

Abstract: 3D Gaussian Splatting (3DGS) has demonstrated its advantages in achieving
fast and high-quality rendering. As point clouds serve as a widely-used and
easily accessible form of 3D representation, bridging the gap between point
clouds and Gaussians becomes increasingly important. Recent studies have
explored how to convert the colored points into Gaussians, but directly
generating Gaussians from colorless 3D point clouds remains an unsolved
challenge. In this paper, we propose GAP, a novel approach that gaussianizes
raw point clouds into high-fidelity 3D Gaussians with text guidance. Our key
idea is to design a multi-view optimization framework that leverages a
depth-aware image diffusion model to synthesize consistent appearances across
different viewpoints. To ensure geometric accuracy, we introduce a
surface-anchoring mechanism that effectively constrains Gaussians to lie on the
surfaces of 3D shapes during optimization. Furthermore, GAP incorporates a
diffuse-based inpainting strategy that specifically targets at completing
hard-to-observe regions. We evaluate GAP on the Point-to-Gaussian generation
task across varying complexity levels, from synthetic point clouds to
challenging real-world scans, and even large-scale scenes. Project Page:
https://weiqi-zhang.github.io/GAP.

</details>


### [101] [FaceAnonyMixer: Cancelable Faces via Identity Consistent Latent Space Mixing](https://arxiv.org/abs/2508.05636)
*Mohammed Talha Alam,Fahad Shamshad,Fakhri Karray,Karthik Nandakumar*

Main category: cs.CV

TL;DR: 本文提出了一种新的面部匿名化方法FaceAnonyMixer，能保护人脸隐私且保持识别精度，并满足可撤销、不可关联和不可逆的生物特征模板保护需求。该方法混合真实和合成的潜在编码后生成匿名人脸，无需修改现有识别系统，实验结果优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 随着人脸识别技术的发展，隐私泄露问题日益严重。现有匿名方法主要掩盖身份，但难以满足生物模板保护的撤销性、不可关联性、不可逆性需求。

Method: 采用预训练生成模型的潜在空间，将真实人脸的潜在编码与根据可撤销密钥生成的合成编码不可逆混合，并通过多目标损失优化，生成兼顾隐私保护和可识别性的可撤销匿名人脸。

Result: 在多个基准数据集上，FaceAnonyMixer比同类可撤销生物识别方法实现了更好的识别准确率和更强的隐私保护能力，在商业API上性能提升超11%。

Conclusion: FaceAnonyMixer在保护人脸隐私和满足生物模板保护需求的同时，保持了对现有人脸识别系统的兼容性，具有优越的实用性和推广价值。

Abstract: Advancements in face recognition (FR) technologies have amplified privacy
concerns, necessitating methods that protect identity while maintaining
recognition utility. Existing face anonymization methods typically focus on
obscuring identity but fail to meet the requirements of biometric template
protection, including revocability, unlinkability, and irreversibility. We
propose FaceAnonyMixer, a cancelable face generation framework that leverages
the latent space of a pre-trained generative model to synthesize
privacy-preserving face images. The core idea of FaceAnonyMixer is to
irreversibly mix the latent code of a real face image with a synthetic code
derived from a revocable key. The mixed latent code is further refined through
a carefully designed multi-objective loss to satisfy all cancelable biometric
requirements. FaceAnonyMixer is capable of generating high-quality cancelable
faces that can be directly matched using existing FR systems without requiring
any modifications. Extensive experiments on benchmark datasets demonstrate that
FaceAnonyMixer delivers superior recognition accuracy while providing
significantly stronger privacy protection, achieving over an 11% gain on
commercial API compared to recent cancelable biometric methods. Code is
available at: https://github.com/talha-alam/faceanonymixer.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [102] [Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a Frozen LLM](https://arxiv.org/abs/2508.04795)
*Thomas Thebaud,Yen-Ju Lu,Matthew Wiesner,Peter Viechnicki,Najim Dehak*

Main category: cs.CL

TL;DR: 本文提出了一种在对话转录后，利用现有大型语言模型（LLM）和音频基础模型，为转录文本补充说话人元数据（如年龄、性别、情感）的新方法，在保持模型冻结和高效的同时，取得了良好的效果。


<details>
  <summary>Details</summary>
Motivation: 对话转录通常只关注文本内容，缺乏说话人属性信息会限制下游任务的表现。因此，作者希望在不增加显著计算成本和开发负担的前提下，为转录添加有价值的说话人元数据。

Method: 该方法将冻结的音频基础模型（如Whisper或WavLM）与冻结的LLAMA语言模型组合，并通过高效的轻量级连接器桥接二者的信息，无需对这些模型进行专项微调。通过这种方式推断说话人的多种属性，并对模型的说话人嵌入向量进行直接比较。

Result: 在说话人属性（如年龄、性别、情感等）推断任务上取得了有竞争力的性能，同时保持了模型的模块化和速度效率。此外，冻结的LLAMA模型能够直接比较x-vector，在某些场景下等错误率(EER)达到8.8%。

Conclusion: 研究表明，不经过专项微调，通过冻结的音频和语言模型及其高效衔接，同样能够在说话人属性推断和比较方面取得很好的效果，对低成本、高效率的管线有重要意义。

Abstract: In dialogue transcription pipelines, Large Language Models (LLMs) are
frequently employed in post-processing to improve grammar, punctuation, and
readability. We explore a complementary post-processing step: enriching
transcribed dialogues by adding metadata tags for speaker characteristics such
as age, gender, and emotion. Some of the tags are global to the entire
dialogue, while some are time-variant. Our approach couples frozen audio
foundation models, such as Whisper or WavLM, with a frozen LLAMA language model
to infer these speaker attributes, without requiring task-specific fine-tuning
of either model. Using lightweight, efficient connectors to bridge audio and
language representations, we achieve competitive performance on speaker
profiling tasks while preserving modularity and speed. Additionally, we
demonstrate that a frozen LLAMA model can compare x-vectors directly, achieving
an Equal Error Rate of 8.8% in some scenarios.

</details>


### [103] [Parity-Aware Byte-Pair Encoding: Improving Cross-lingual Fairness in Tokenization](https://arxiv.org/abs/2508.04796)
*Negar Foroutan,Clara Meister,Debjit Paul,Joel Niklaus,Sina Ahmadi,Antoine Bosselut,Rico Sennrich*

Main category: cs.CL

TL;DR: 本文提出Parity-aware BPE算法，通过在每步合并过程中优先优化压缩效果最差的语言，实现了跨语言token数量的公平性，几乎不影响整体压缩率与下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有的基于频率的tokenizer算法更偏向于高资源语言，而低资源语言得到的分词更长且可能不符合形态规范，甚至充满<UNK>占位符，导致跨语言之间在计算和资源上的不平等。

Method: 作者提出了Parity-aware BPE，主要思想是在每一步BPE合并时，最大化当前token压缩效果最差语言的压缩收益，以牺牲少量全局压缩率为代价，提升不同语言间分词公平性。

Result: 实验表明，Parity-aware BPE能显著减少不同语言间的token数量不均，有效提升跨语言公平性，并且对整体压缩率的影响可以忽略，在下游任务中的模型表现也没有显著差异。

Conclusion: Parity-aware BPE能够在保持较好压缩率和模型性能的前提下，实现各语言间分词效果的公平分配，有助于缓解NLP系统在多语言环境下的计算和资源不平等问题。

Abstract: Tokenization is the first -- and often least scrutinized -- step of most NLP
pipelines. Standard algorithms for learning tokenizers rely on frequency-based
objectives, which favor languages dominant in the training data and
consequently leave lower-resource languages with tokenizations that are
disproportionately longer, morphologically implausible, or even riddled with
<UNK> placeholders. This phenomenon ultimately amplifies computational and
financial inequalities between users from different language backgrounds. To
remedy this, we introduce Parity-aware Byte Pair Encoding (BPE), a variant of
the widely-used BPE algorithm. At every merge step, Parity-aware BPE maximizes
the compression gain of the currently worst-compressed language, trading a
small amount of global compression for cross-lingual parity. We find
empirically that Parity-aware BPE leads to more equitable token counts across
languages, with negligible impact on global compression rate and no substantial
effect on language-model performance in downstream tasks.

</details>


### [104] [Pitch Accent Detection improves Pretrained Automatic Speech Recognition](https://arxiv.org/abs/2508.04814)
*David Sasu,Natalie Schluter*

Main category: cs.CL

TL;DR: 本论文提出将重音检测模块引入ASR系统，通过联合训练提升自动语音识别和重音检测的性能，在资源有限的情况下显著降低了错误率。


<details>
  <summary>Details</summary>
Motivation: 目前自动语音识别系统在表示语音时往往忽略了韵律信息，如重音，这些信息对识别准确率提升具有潜在价值。

Method: 作者设计了一个联合ASR与重音检测的模型，并将半监督语音表示与重音检测模块结合，采用联合训练方法提升两者性能。

Result: 该模型在重音检测任务上，相较以往方法F1-score提升了41%；在LibriSpeech数据集上，ASR的WER在有限微调资源下减少了28.3%。

Conclusion: 将韵律特征（如重音）与ASR系统结合，可以有效提升识别系统的表现，说明扩展和改进预训练语音模型对韵律信息的感知具有重要意义。

Abstract: We show the performance of Automatic Speech Recognition (ASR) systems that
use semi-supervised speech representations can be boosted by a complimentary
pitch accent detection module, by introducing a joint ASR and pitch accent
detection model. The pitch accent detection component of our model achieves a
significant improvement on the state-of-the-art for the task, closing the gap
in F1-score by 41%. Additionally, the ASR performance in joint training
decreases WER by 28.3% on LibriSpeech, under limited resource fine-tuning. With
these results, we show the importance of extending pretrained speech models to
retain or re-learn important prosodic cues such as pitch accent.

</details>


### [105] [Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History](https://arxiv.org/abs/2508.04826)
*Tommaso Tosato,Saskia Helbling,Yorguin-Jose Mantilla-Ramos,Mahmood Hegazy,Alberto Tosato,David John Lemay,Irina Rish,Guillaume Dumas*

Main category: cs.CL

TL;DR: 本文提出了PERSIST评测框架，系统评估了25个以上大语言模型在一致性和个性稳定性上的表现，发现即便最大模型也未能实现行为上的一致性，这对安全部署提出了挑战。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的个性化和行为一致性状况未知，但模型在安全敏感的应用场景下需要稳定、可预测的行为表现，因此急需系统评测和理解其个性稳定性的实际表现和局限。

Method: 作者开发了PERSIST评测框架，对25+主流开源大模型（覆盖1B-671B参数规模）进行实验，基于传统以及为LLM定制的新型人格测量工具，测试了不同题序、措辞重写、角色设定、推理模式对测评结果的影响，并量化多个干预手段的有效性。

Result: （1）即使是规模最大的模型（400B+参数），回答的一致性依然存在显著波动；（2）单纯调整题目顺序即可让人格评测结果变化高达20%；（3）包括链式推理、角色精细设定、对话历史等“稳健”机制反而会加大模型的不稳定性；（4）为LLM定制的人格测评工具依然和人类量表一样不稳定，说明问题不是量表翻译而是模型架构本身致使。

Conclusion: 当前大语言模型在各类机制和规模下均无法保证个性行为的一致性，说明依赖人格（个性）对齐来保证模型安全性的方法存在根本性缺陷，尤其在对行为一致性有强需求的安全场景下，现有模型尚不适用。

Abstract: Large language models require consistent behavioral patterns for safe
deployment, yet their personality-like traits remain poorly understood. We
present PERSIST (PERsonality Stability in Synthetic Text), a comprehensive
evaluation framework testing 25+ open-source models (1B-671B parameters) across
500,000+ responses. Using traditional (BFI-44, SD3) and novel LLM-adapted
personality instruments, we systematically vary question order, paraphrasing,
personas, and reasoning modes. Our findings challenge fundamental deployment
assumptions: (1) Even 400B+ models exhibit substantial response variability (SD
> 0.4); (2) Minor prompt reordering alone shifts personality measurements by up
to 20%; (3) Interventions expected to stabilize behavior, such as
chain-of-thought reasoning, detailed personas instruction, inclusion of
conversation history, can paradoxically increase variability; (4) LLM-adapted
instruments show equal instability to human-centric versions, confirming
architectural rather than translational limitations. This persistent
instability across scales and mitigation strategies suggests current LLMs lack
the foundations for genuine behavioral consistency. For safety-critical
applications requiring predictable behavior, these findings indicate that
personality-based alignment strategies may be fundamentally inadequate.

</details>


### [106] [RCR-Router: Efficient Role-Aware Context Routing for Multi-Agent LLM Systems with Structured Memory](https://arxiv.org/abs/2508.04903)
*Jun Liu,Zhenglun Kong,Changdi Yang,Fan Yang,Tianqi Li,Peiyan Dong,Joannah Nanjekye,Hao Tang,Geng Yuan,Wei Niu,Wenbin Zhang,Pu Zhao,Xue Lin,Dong Huang,Yanzhi Wang*

Main category: cs.CL

TL;DR: 本论文提出了一种新的多智能体大语言模型（LLM）协作框架RCR-Router，可实现更高效、适应性更强的信息路由，并显著降低token消耗，同时保持甚至提升解答质量。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体大语言模型的上下文路由方案普遍采用静态或全上下文策略，导致token浪费、冗余内存暴露和协作适应性不佳，限制了复杂推理与决策中的应用效果。

Method: 提出RCR-Router框架，基于每个智能体的角色与任务阶段动态选择相关语义记忆子集，并使用轻量级打分策略在严格token预算下引导上下文挑选。每轮智能体输出会迭代纳入共享记忆以渐进优化上下文信息。同时引入Answer Quality Score新指标来捕捉LLM答案解释的质量。

Result: 在HotPotQA、MuSiQue与2WikiMultihop三个多跳QA任务上，RCR-Router在减少高达30% token消耗的同时，能够提升或维持答案质量。

Conclusion: RCR-Router验证了结构化记忆路由和新颖输出质量评估对更大规模、多智能体LLM系统发展的重要价值，为高效协作和更优性能奠定了基础。

Abstract: Multi-agent large language model (LLM) systems have shown strong potential in
complex reasoning and collaborative decision-making tasks. However, most
existing coordination schemes rely on static or full-context routing
strategies, which lead to excessive token consumption, redundant memory
exposure, and limited adaptability across interaction rounds. We introduce
RCR-Router, a modular and role-aware context routing framework designed to
enable efficient, adaptive collaboration in multi-agent LLMs. To our knowledge,
this is the first routing approach that dynamically selects semantically
relevant memory subsets for each agent based on its role and task stage, while
adhering to a strict token budget. A lightweight scoring policy guides memory
selection, and agent outputs are iteratively integrated into a shared memory
store to facilitate progressive context refinement. To better evaluate model
behavior, we further propose an Answer Quality Score metric that captures
LLM-generated explanations beyond standard QA accuracy. Experiments on three
multi-hop QA benchmarks -- HotPotQA, MuSiQue, and 2WikiMultihop -- demonstrate
that RCR-Router reduces token usage (up to 30%) while improving or maintaining
answer quality. These results highlight the importance of structured memory
routing and output-aware evaluation in advancing scalable multi-agent LLM
systems.

</details>


### [107] [I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating Linguistic Shibboleth Detection in LLM Hiring Evaluations](https://arxiv.org/abs/2508.04939)
*Julia Kharchenko,Tanya Roosta,Aman Chadha,Chirag Shah*

Main category: cs.CL

TL;DR: 本文提出了一个用于评估大型语言模型（LLMs）对“语言暗号”（shibboleths）反应的基准测试，揭示模型潜在的群体偏见。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM自动评估方法可能隐藏着对特定语言特征（如性别、社会阶层、地区背景）的偏见，有必要建立能够检测和量化这些偏见的评测框架。

Method: 作者构建了包含100个经验证的问题-回答对的人工访谈模拟，通过精细化控制生成不同的语言变体（如是否使用hedging），保证语义等价，仅改变语言风格，来测试LLM的自动评分系统偏好。多维度语言特征均做了验证。

Result: 结果显示，含有hedging的答案平均得分比直接答案低25.6%，证明了模型系统性惩罚特定语言风格。该基准能有效发现不同模型的偏见特征。

Conclusion: 本研究为AI系统中语言歧视检测和量化提供了基础性框架，对公平性自动化决策等领域有重要应用价值。

Abstract: This paper introduces a comprehensive benchmark for evaluating how Large
Language Models (LLMs) respond to linguistic shibboleths: subtle linguistic
markers that can inadvertently reveal demographic attributes such as gender,
social class, or regional background. Through carefully constructed interview
simulations using 100 validated question-response pairs, we demonstrate how
LLMs systematically penalize certain linguistic patterns, particularly hedging
language, despite equivalent content quality. Our benchmark generates
controlled linguistic variations that isolate specific phenomena while
maintaining semantic equivalence, which enables the precise measurement of
demographic bias in automated evaluation systems. We validate our approach
along multiple linguistic dimensions, showing that hedged responses receive
25.6% lower ratings on average, and demonstrate the benchmark's effectiveness
in identifying model-specific biases. This work establishes a foundational
framework for detecting and measuring linguistic discrimination in AI systems,
with broad applications to fairness in automated decision-making contexts.

</details>


### [108] [Towards Robust Evaluation of Visual Activity Recognition: Resolving Verb Ambiguity with Sense Clustering](https://arxiv.org/abs/2508.04945)
*Louie Hong Yao,Nicholas Jarvis,Tianyu Jiang*

Main category: cs.CL

TL;DR: 本论文针对视觉活动识别系统中动词语义和图像解释的歧义性，提出了一种基于视觉-语言聚类的新型评价方法，能够更准确反映模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的精确匹配评估方法仅依赖于单一标准答案，无法处理描述同一事件的多义动词或不同视角下的合理动词选择，导致模型性能评估不全面。

Method: 作者提出了一种视觉-语言聚类框架：通过构建动词语义簇（verb sense clusters），将图像对应的动作描述划分到多个语义接近的类别中，从而进行更具鲁棒性的模型评估。该方法对imSitu数据集进行了分析，并将多种活动识别模型在聚类基础和标准方法下分别评测。

Result: 实验证明，每张imSitu数据集图片平均对应2.8个动词语义簇，每个簇代表该图片的不同视角。聚类方法评测结果与标准方法进行了对比。

Conclusion: 人类一致性分析表明，基于聚类的评估方法更符合人类判断，可为模型性能提供更细致、更真实的评估。

Abstract: Evaluating visual activity recognition systems is challenging due to inherent
ambiguities in verb semantics and image interpretation. When describing actions
in images, synonymous verbs can refer to the same event (e.g., brushing vs.
grooming), while different perspectives can lead to equally valid but distinct
verb choices (e.g., piloting vs. operating). Standard exact-match evaluation,
which relies on a single gold answer, fails to capture these ambiguities,
resulting in an incomplete assessment of model performance. To address this, we
propose a vision-language clustering framework that constructs verb sense
clusters, providing a more robust evaluation. Our analysis of the imSitu
dataset shows that each image maps to an average of 2.8 sense clusters, with
each cluster representing a distinct perspective of the image. We evaluate
multiple activity recognition models and compare our cluster-based evaluation
with standard evaluation methods. Additionally, our human alignment analysis
suggests that the cluster-based evaluation better aligns with human judgements,
offering a more nuanced assessment of model performance.

</details>


### [109] [A Multi-Stage Large Language Model Framework for Extracting Suicide-Related Social Determinants of Health](https://arxiv.org/abs/2508.05003)
*Song Wang,Yishu Wei,Haotian Ma,Max Lovitt,Kelly Deng,Yuan Meng,Zihan Xu,Jingze Zhang,Yunyu Xiao,Ying Ding,Xuhai Xu,Joydeep Ghosh,Yifan Peng*

Main category: cs.CL

TL;DR: 本文提出了一种多阶段大语言模型框架，用于从非结构化文本中高效提取与自杀相关的健康社会决定因素（SDoH），提高了任务准确性和模型解释性。


<details>
  <summary>Details</summary>
Motivation: 自杀的发生与多种健康社会决定因素（SDoH）密切相关，对这些因素的识别有助于早期发现和预防自杀事件。然而，目前数据驱动方法受限于因子分布不均、关键压力源难以解析以及模型解释性有限等问题。

Method: 作者设计了一个多阶段大语言模型框架，用于从非结构化文本中提取SDoH因子，并与业内领先的语言模型（如BioBERT、GPT-3.5-turbo）及推理模型（DeepSeek-R1）进行了对比评估。此外，还结合自动化评测及用户实验，评估模型解释性对标注效率的提升作用。

Result: 提出的框架在整体SDoH因子提取和细粒度上下文检索任务中均优于对比模型。同时，通过微调小型任务专用模型，在推理成本更低的情况下获得了相当甚至更优的表现。多阶段设计提升了抽取性能及模型解释性。

Conclusion: 所提方法提升了从非结构化文本中提取与自杀相关SDoH的准确率与透明度，有望促进早期识别高风险个体并完善自杀预防策略。

Abstract: Background: Understanding social determinants of health (SDoH) factors
contributing to suicide incidents is crucial for early intervention and
prevention. However, data-driven approaches to this goal face challenges such
as long-tailed factor distributions, analyzing pivotal stressors preceding
suicide incidents, and limited model explainability. Methods: We present a
multi-stage large language model framework to enhance SDoH factor extraction
from unstructured text. Our approach was compared to other state-of-the-art
language models (i.e., pre-trained BioBERT and GPT-3.5-turbo) and reasoning
models (i.e., DeepSeek-R1). We also evaluated how the model's explanations help
people annotate SDoH factors more quickly and accurately. The analysis included
both automated comparisons and a pilot user study. Results: We show that our
proposed framework demonstrated performance boosts in the overarching task of
extracting SDoH factors and in the finer-grained tasks of retrieving relevant
context. Additionally, we show that fine-tuning a smaller, task-specific model
achieves comparable or better performance with reduced inference costs. The
multi-stage design not only enhances extraction but also provides intermediate
explanations, improving model explainability. Conclusions: Our approach
improves both the accuracy and transparency of extracting suicide-related SDoH
from unstructured texts. These advancements have the potential to support early
identification of individuals at risk and inform more effective prevention
strategies.

</details>


### [110] [Dialogues Aspect-based Sentiment Quadruple Extraction via Structural Entropy Minimization Partitioning](https://arxiv.org/abs/2508.05023)
*Kun Peng,Cong Cao,Hao Peng,Zhifeng Hao,Lei Jiang,Kongjing Gu,Yanbing Liu,Philip S. Yu*

Main category: cs.CL

TL;DR: 本文关注对多轮多参与者对话中的目标-方面-观点-情感四元组进行抽取，通过引入结构熵最小化算法对对话进行子对话划分，有效减少了信息噪声，并提出了两步抽取框架，在效率和效果上都达到了新的水平。


<details>
  <summary>Details</summary>
Motivation: 现有方法在抽取对话情感四元组时，将整个对话作为整体来学习词关系，容易引入与语义无关的信息噪声；而实际对话经常包含多个独立的子话题、子对话。如何合理划分并抽取完整的信息，是当前的难题。

Method: 采用结构熵最小化算法将对话划分为语义独立的子对话，达到尽量完整覆盖又最少干扰。然后，提出先在话轮级别抽取情感元素，再把抽取结果在子对话级进行四元组匹配的两步抽取框架。

Result: 大量实验显示，本文方法在DiaASQ任务中取得了目前最优水平，同时计算开销显著降低。

Conclusion: 划分独立子对话、两步抽取的方法兼顾准确性与效率，对对话情感四元组抽取任务具有重要的实际意义和理论参考价值。

Abstract: Dialogues Aspect-based Sentiment Quadruple Extraction (DiaASQ) aims to
extract all target-aspect-opinion-sentiment quadruples from a given
multi-round, multi-participant dialogue. Existing methods typically learn word
relations across entire dialogues, assuming a uniform distribution of sentiment
elements. However, we find that dialogues often contain multiple semantically
independent sub-dialogues without clear dependencies between them. Therefore,
learning word relationships across the entire dialogue inevitably introduces
additional noise into the extraction process. To address this, our method
focuses on partitioning dialogues into semantically independent sub-dialogues.
Achieving completeness while minimizing these sub-dialogues presents a
significant challenge. Simply partitioning based on reply relationships is
ineffective. Instead, we propose utilizing a structural entropy minimization
algorithm to partition the dialogues. This approach aims to preserve relevant
utterances while distinguishing irrelevant ones as much as possible.
Furthermore, we introduce a two-step framework for quadruple extraction: first
extracting individual sentiment elements at the utterance level, then matching
quadruples at the sub-dialogue level. Extensive experiments demonstrate that
our approach achieves state-of-the-art performance in DiaASQ with much lower
computational costs.

</details>


### [111] [Evaluation of LLMs in AMR Parsing](https://arxiv.org/abs/2508.05028)
*Shu Han Ho*

Main category: cs.CL

TL;DR: 本文评估了通过微调四种不同的解码器型大型语言模型（LLM）用于AMR语义解析的表现，结果显示其性能可与复杂的SOTA方法媲美。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力提升，探索其在结构化语义解析任务（如AMR parsing）中，能否以更简单的微调方案实现与现有复杂方法类似的效果。

Method: 选择四种不同的解码器型LLM（Phi 3.5, Gemma 2, LLaMA 3.2, DeepSeek R1 LLaMA Distilled），用LDC2020T02 AMR3.0测试集对其进行直接微调，并与当前SOTA AMR解析模型做详细比较和分析。

Result: 直接微调的解码器LLM能达到与主流SOTA模型（如APT+Silver/Graphene Smatch）相当或接近的性能，LLaMA 3.2 SMATCH F1分数为0.804。LLaMA 3.2在语义表现最佳，Phi 3.5在结构有效性上表现优异。

Conclusion: 直接微调解码器型LLM在AMR解析领域表现优异，足以媲美复杂SOTA系统，为相关任务提供了简洁高效的新范式。

Abstract: Meaning Representation (AMR) is a semantic formalism that encodes sentence
meaning as rooted, directed, acyclic graphs, where nodes represent concepts and
edges denote semantic relations. Finetuning decoder only Large Language Models
(LLMs) represent a promising novel straightfoward direction for AMR parsing.
This paper presents a comprehensive evaluation of finetuning four distinct LLM
architectures, Phi 3.5, Gemma 2, LLaMA 3.2, and DeepSeek R1 LLaMA Distilled
using the LDC2020T02 Gold AMR3.0 test set. Our results have shown that
straightfoward finetuning of decoder only LLMs can achieve comparable
performance to complex State of the Art (SOTA) AMR parsers. Notably, LLaMA 3.2
demonstrates competitive performance against SOTA AMR parsers given a
straightforward finetuning approach. We achieved SMATCH F1: 0.804 on the full
LDC2020T02 test split, on par with APT + Silver (IBM) at 0.804 and approaching
Graphene Smatch (MBSE) at 0.854. Across our analysis, we also observed a
consistent pattern where LLaMA 3.2 leads in semantic performance while Phi 3.5
excels in structural validity.

</details>


### [112] [Align, Don't Divide: Revisiting the LoRA Architecture in Multi-Task Learning](https://arxiv.org/abs/2508.05078)
*Jinda Liu,Bo Cheng,Yi Chang,Yuan Wu*

Main category: cs.CL

TL;DR: 本文研究了在多任务学习场景下，如何高效调整大语言模型（LLM）的参数以适应多任务需求，提出新的方法Align-LoRA，取得了优于现有方法的结果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在实际应用中需要支持多领域、多任务，如何在参数高效微调（PEFT）框架下，既保证多任务表现、又不过度增加复杂度，是当前亟需解决的问题。

Method: 作者首先实验比较了当前主流的LoRA多适配器/多头结构与简化多头结构（各头高度相似），发现在多任务下，后者表现更优。进一步，提升单一LoRA适配器rank后，也可达高水平。由此提出Align-LoRA，在LoRA结构里显式加入任务表示对齐损失，促进任务间表示共享。

Result: Align-LoRA效果显著超越现有复杂多适配器/多头方案，以及普通单LoRA结构，在多任务调整大语言模型时表现优异。

Conclusion: 结构复杂度并非多任务泛化的关键，核心是增强任务间有效共享。Align-LoRA实现了结构简化和性能提升，为多任务高效适配大模型提供了新范式。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) is essential for adapting Large
Language Models (LLMs). In practice, LLMs are often required to handle a
diverse set of tasks from multiple domains, a scenario naturally addressed by
multi-task learning (MTL). Within this MTL context, a prevailing trend involves
LoRA variants with multiple adapters or heads, which advocate for structural
diversity to capture task-specific knowledge. Our findings present a direct
challenge to this paradigm. We first show that a simplified multi-head
architecture with high inter-head similarity substantially outperforms complex
multi-adapter and multi-head systems. This leads us to question the
multi-component paradigm itself, and we further demonstrate that a standard
single-adapter LoRA, with a sufficiently increased rank, also achieves highly
competitive performance. These results lead us to a new hypothesis: effective
MTL generalization hinges on learning robust shared representations, not
isolating task-specific features. To validate this, we propose Align-LoRA,
which incorporates an explicit loss to align task representations within the
shared adapter space. Experiments confirm that Align-LoRA significantly
surpasses all baselines, establishing a simpler yet more effective paradigm for
adapting LLMs to multiple tasks. The code is available at
https://github.com/jinda-liu/Align-LoRA.

</details>


### [113] [Multimodal Fact Checking with Unified Visual, Textual, and Contextual Representations](https://arxiv.org/abs/2508.05097)
*Aditya Kishore,Gaurav Kumar,Jasabanta Patro*

Main category: cs.CL

TL;DR: 本文提出了一种名为MultiCheck的统一多模态事实核查框架，通过同时处理文本和图像信息，实现对复杂事实的高效核查，在Factify 2数据集上取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 由于越来越多的虚假信息同时结合了文本和图像，单纯依赖文本证据的事实核查系统难以应对，因此亟需能够协调多模态信息的自动化方法。

Method: 提出MultiCheck框架，利用专门的文本和图像编码器，并通过融合模块捕捉跨模态关系，同时使用对比学习目标增强语义对齐，最后通过分类器判定事实真伪。

Result: 在Factify 2数据集上，提出的方法取得了0.84的加权F1分数，明显优于现有基线。

Conclusion: MultiCheck有效提升了多模态事实核查的性能，具备良好的可扩展性和可解释性，有助于复杂场景下的自动化虚假信息检测。

Abstract: The growing rate of multimodal misinformation, where claims are supported by
both text and images, poses significant challenges to fact-checking systems
that rely primarily on textual evidence. In this work, we have proposed a
unified framework for fine-grained multimodal fact verification called
"MultiCheck", designed to reason over structured textual and visual signals.
Our architecture combines dedicated encoders for text and images with a fusion
module that captures cross-modal relationships using element-wise interactions.
A classification head then predicts the veracity of a claim, supported by a
contrastive learning objective that encourages semantic alignment between
claim-evidence pairs in a shared latent space. We evaluate our approach on the
Factify 2 dataset, achieving a weighted F1 score of 0.84, substantially
outperforming the baseline. These results highlight the effectiveness of
explicit multimodal reasoning and demonstrate the potential of our approach for
scalable and interpretable fact-checking in complex, real-world scenarios.

</details>


### [114] [BEE-RAG: Balanced Entropy Engineering for Retrieval-Augmented Generation](https://arxiv.org/abs/2508.05100)
*Yuhao Wang,Ruiyang Ren,Yucheng Wang,Jing Liu,Wayne Xin Zhao,Hua Wu,Haifeng Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的RAG框架，称为BEE-RAG，通过熵平衡原则稳定长上下文条件下的信息处理，有效提升了RAG性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）不断发展，RAG成为弥补LLM知识盲区的重要方法。但长文本检索导致熵不受控增长与注意力稀释，影响了RAG效果。如何在长上下文场景下提升RAG表现，成为亟待解决的问题。

Method: 作者提出BEE-RAG框架，基于熵不变性原理，优化上下文熵分布以分隔注意力灵敏度与上下文长度，保持信息处理的稳定性。并引入零样本推理的多重要性估计和参数高效的自适应微调机制，以自动获取最优的熵平衡因子。

Result: 在多个RAG相关任务上的大量实验结果表明，BEE-RAG在处理不同上下文长度时表现出更强的适应性和总体优越的性能。

Conclusion: BEE-RAG有效地解决了传统RAG因长文本检索导致的熵扩张和注意力稀释问题，显著提升了系统的稳定性和任务表现，适用于多种RAG应用场景。

Abstract: With the rapid advancement of large language models (LLMs),
retrieval-augmented generation (RAG) has emerged as a critical approach to
supplement the inherent knowledge limitations of LLMs. However, due to the
typically large volume of retrieved information, RAG tends to operate with long
context lengths. From the perspective of entropy engineering, we identify
unconstrained entropy growth and attention dilution due to long retrieval
context as significant factors affecting RAG performance. In this paper, we
propose the balanced entropy-engineered RAG (BEE-RAG) framework, which improves
the adaptability of RAG systems to varying context lengths through the
principle of entropy invariance. By leveraging balanced context entropy to
reformulate attention dynamics, BEE-RAG separates attention sensitivity from
context length, ensuring a stable entropy level. Building upon this, we
introduce a zero-shot inference strategy for multi-importance estimation and a
parameter-efficient adaptive fine-tuning mechanism to obtain the optimal
balancing factor for different settings. Extensive experiments across multiple
RAG tasks demonstrate the effectiveness of BEE-RAG.

</details>


### [115] [Attention Basin: Why Contextual Position Matters in Large Language Models](https://arxiv.org/abs/2508.05128)
*Zihao Yi,Delong Zeng,Zhenqing Ling,Haohao Luo,Zhe Xu,Wei Liu,Jian Luan,Wanxia Cao,Ying Shen*

Main category: cs.CL

TL;DR: 本文发现大型语言模型（LLMs）对输入信息位置极为敏感：序列开头和结尾的信息受关注度更高，而中间部分被忽略。作者提出了一种无需训练、可直接部署的排序方法AttnRank来提升关键内容关注度，从而显著提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 目前LLMs在输入序列里的信息排列顺序会极大影响其最终表现，但背后机制未明。研究动机是系统揭示并利用这一序列位置偏置，以提升实际任务中的模型效果。

Method: 作者通过大量实验证明了『attention basin』现象，即模型更关注序列两端的信息。基于此，提出了AttnRank框架：首先用少量数据估算模型对不同位置的注意偏好，然后将重要内容调整到模型高关注度位置。这一方法无需训练，可与任意现有模型结合。

Result: 在多跳问答和few-shot学习任务上进行实验，涵盖10种不同结构和规模的LLMs。AttnRank在无需更改模型结构和训练流程的前提下，均带来了显著性能提升。

Conclusion: 位置偏置对LLMs信息利用影响很大。只需巧妙调整关键信息所在序列位置，就能显著提升模型表现，AttnRank方法实用、高效且通用。

Abstract: The performance of Large Language Models (LLMs) is significantly sensitive to
the contextual position of information in the input. To investigate the
mechanism behind this positional bias, our extensive experiments reveal a
consistent phenomenon we term the attention basin: when presented with a
sequence of structured items (e.g., retrieved documents or few-shot examples),
models systematically assign higher attention to the items at the beginning and
end of the sequence, while neglecting those in the middle. Crucially, our
analysis further reveals that allocating higher attention to critical
information is key to enhancing model performance. Based on these insights, we
introduce Attention-Driven Reranking (AttnRank), a two-stage framework that (i)
estimates a model's intrinsic positional attention preferences using a small
calibration set, and (ii) reorders retrieved documents or few-shot examples to
align the most salient content with these high-attention positions. AttnRank is
a model-agnostic, training-free, and plug-and-play method with minimal
computational overhead. Experiments on multi-hop QA and few-shot in-context
learning tasks demonstrate that AttnRank achieves substantial improvements
across 10 large language models of varying architectures and scales, without
modifying model parameters or training procedures.

</details>


### [116] [Towards Assessing Medical Ethics from Knowledge to Practice](https://arxiv.org/abs/2508.05132)
*Chang Hong,Minghao Wu,Qingying Xiao,Yuchi Wang,Xiang Wan,Guangjun Yu,Benyou Wang,Yan Hu*

Main category: cs.CL

TL;DR: 本文提出了PrinciplismQA基准，用于系统性评估大语言模型（LLM）在医疗伦理推理方面的能力。结果显示各模型在将伦理知识应用到现实情境上存在显著差距，尤其在利他性原则上表现较弱。该基准为提升医疗AI的伦理水平提供了工具。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型被广泛集成到医疗场景，其伦理推理能力变得极其重要。然而，以往基准往往忽略对伦理推理的系统性评测，缺乏专门针对医学伦理的评测工具。

Method: 作者提出PrinciplismQA基准，基于医学伦理学四原则（Principlism）理论，收集并构建了3648道涵盖多选与开放性问题的数据集，问题来源于权威教材和案例，且均经医学专家验证。通过系统测评主流LLMs，包括经过医学领域微调与未微调的模型。

Result: 实验表明，当前多数学术与商业LLMs在伦理知识与实际应用之间存在显著差距，尤其是在利他性（Beneficence）原则的权衡上表现欠佳，有时过度强调其他伦理原则。前沿的闭源模型在能力上领先，而医学领域微调则能提升某些伦理推理表现。

Conclusion: PrinciplismQA为诊断模型伦理弱点提供了可扩展、细致的评测工具。医学AI要实现更加平衡和负责任的伦理推理，仍需在知识对齐与实际应用上不断改进。

Abstract: The integration of large language models into healthcare necessitates a
rigorous evaluation of their ethical reasoning, an area current benchmarks
often overlook. We introduce PrinciplismQA, a comprehensive benchmark with
3,648 questions designed to systematically assess LLMs' alignment with core
medical ethics. Grounded in Principlism, our benchmark features a high-quality
dataset. This includes multiple-choice questions curated from authoritative
textbooks and open-ended questions sourced from authoritative medical ethics
case study literature, all validated by medical experts. Our experiments reveal
a significant gap between models' ethical knowledge and their practical
application, especially in dynamically applying ethical principles to
real-world scenarios. Most LLMs struggle with dilemmas concerning Beneficence,
often over-emphasizing other principles. Frontier closed-source models, driven
by strong general capabilities, currently lead the benchmark. Notably, medical
domain fine-tuning can enhance models' overall ethical competence, but further
progress requires better alignment with medical ethical knowledge.
PrinciplismQA offers a scalable framework to diagnose these specific ethical
weaknesses, paving the way for more balanced and responsible medical AI.

</details>


### [117] [ATLANTIS at SemEval-2025 Task 3: Detecting Hallucinated Text Spans in Question Answering](https://arxiv.org/abs/2508.05179)
*Catherine Kobus,François Lancelot,Marion-Cécile Martin,Nawal Ould Amer*

Main category: cs.CL

TL;DR: 本文针对问答系统中的幻觉文本检测，提出了多种基于大语言模型（LLM）的方法，在SemEval-2025大赛中取得佳绩。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在自然语言生成领域取得重大进展，但仍容易产生“幻觉”——生成不准确或误导性内容，因此需要有效的检测与应对策略。

Method: 作者探索了有外部上下文和无外部上下文的检测方法，包括LLM少样本提示、基于token的分类、以及在合成数据上微调LLM等多种手段。

Result: 提出的方法在西班牙语文本中获得了第一名，在英语和德语方面也取得了竞争性成绩。

Conclusion: 集成相关上下文、微调模型和巧妙设计的提示方式对缓解幻觉现象有效，该方向有进一步应用和研究价值。

Abstract: This paper presents the contributions of the ATLANTIS team to SemEval-2025
Task 3, focusing on detecting hallucinated text spans in question answering
systems. Large Language Models (LLMs) have significantly advanced Natural
Language Generation (NLG) but remain susceptible to hallucinations, generating
incorrect or misleading content. To address this, we explored methods both with
and without external context, utilizing few-shot prompting with a LLM,
token-level classification or LLM fine-tuned on synthetic data. Notably, our
approaches achieved top rankings in Spanish and competitive placements in
English and German. This work highlights the importance of integrating relevant
context to mitigate hallucinations and demonstrate the potential of fine-tuned
models and prompt engineering.

</details>


### [118] [Resource-Limited Joint Multimodal Sentiment Reasoning and Classification via Chain-of-Thought Enhancement and Distillation](https://arxiv.org/abs/2508.05234)
*Haonan Shangguan,Xiaocui Yang,Shi Feng,Daling Wang,Yifei Zhang,Ge Yu*

Main category: cs.CL

TL;DR: 本文提出了一种适用于资源受限环境的多模态情感推理与分类模型MulCoT-RD，通过蒸馏方式用轻量级模型同时生成情感推理链和进行情感分类，取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态情感分析依赖参数量大的大模型，难以在资源有限环境中部署，同时忽视了自主进行多模态情感推理生成的能力，因此作者希望开发轻量级模型以实现高效、可解释的情感分析。

Method: 提出JMSRC任务（联合多模态情感推理与分类），设计MulCoT-RD模型，采用“教师-助手-学生”蒸馏范式。先用大多模态语言模型生成推理数据，并用多任务机制训练中等规模助手模型，再训练轻量学生模型，实现有效推理链生成与分类。

Result: 在四个数据集上实验，MulCoT-RD仅用3B参数，在JMSRC任务上取得了强劲性能，表现出良好的泛化性和更好的可解释性。

Conclusion: MulCoT-RD模型兼具轻量、高效和可解释性，为资源受限环境下的多模态情感分析提供了新方法和实践路径。

Abstract: The surge in rich multimodal content on social media platforms has greatly
advanced Multimodal Sentiment Analysis (MSA), with Large Language Models (LLMs)
further accelerating progress in this field. Current approaches primarily
leverage the knowledge and reasoning capabilities of parameter-heavy
(Multimodal) LLMs for sentiment classification, overlooking autonomous
multimodal sentiment reasoning generation in resource-constrained environments.
Therefore, we focus on the Resource-Limited Joint Multimodal Sentiment
Reasoning and Classification task, JMSRC, which simultaneously performs
multimodal sentiment reasoning chain generation and sentiment classification
only with a lightweight model. We propose a Multimodal Chain-of-Thought
Reasoning Distillation model, MulCoT-RD, designed for JMSRC that employs a
"Teacher-Assistant-Student" distillation paradigm to address deployment
constraints in resource-limited environments. We first leverage a
high-performance Multimodal Large Language Model (MLLM) to generate the initial
reasoning dataset and train a medium-sized assistant model with a multi-task
learning mechanism. A lightweight student model is jointly trained to perform
efficient multimodal sentiment reasoning generation and classification.
Extensive experiments on four datasets demonstrate that MulCoT-RD with only 3B
parameters achieves strong performance on JMSRC, while exhibiting robust
generalization and enhanced interpretability.

</details>


### [119] [Pruning Large Language Models by Identifying and Preserving Functional Networks](https://arxiv.org/abs/2508.05239)
*Yiheng Liu,Junhao Ning,Sichen Xia,Xiaohui Gao,Ning Qiang,Bao Ge,Junwei Han,Xintao Hu*

Main category: cs.CL

TL;DR: 本文提出一种新的大语言模型（LLM）结构化剪枝方法，通过识别人造神经网络中的功能网络来进行高效剪枝，保留模型功能的同时大幅减少模型规模。


<details>
  <summary>Details</summary>
Motivation: 现有LLM结构化剪枝方法通常只关注单元重要性，忽略神经元之间的协作关系，容易破坏模型的整体功能结构，导致性能下降。作者希望通过借鉴人脑功能网络研究，改善这一剪枝策略的不足。

Method: 将LLM视为“数字大脑”，采用类似于脑影像学中识别功能网络的方法，将LLM分解为多个功能网络，然后在剪枝时优先保留这些网络中的关键神经元，而不是仅基于单元重要性得分进行裁剪。

Result: 实验证明，该方法能有效识别和定位LLM中的功能网络及关键神经元，实现了性能损失较小的高效模型压缩。

Conclusion: 通过保留LLM中的功能网络和关键神经元，可以有效提高剪枝后的模型性能，为LLM实际应用中的高效部署提供新思路。

Abstract: Structured pruning is one of the representative techniques for compressing
large language models (LLMs) to reduce GPU memory consumption and accelerate
inference speed. It offers significant practical value in improving the
efficiency of LLMs in real-world applications. Current structured pruning
methods typically rely on assessment of the importance of the structure units
and pruning the units with less importance. Most of them overlooks the
interaction and collaboration among artificial neurons that are crucial for the
functionalities of LLMs, leading to a disruption in the macro functional
architecture of LLMs and consequently a pruning performance degradation.
Inspired by the inherent similarities between artificial neural networks and
functional neural networks in the human brain, we alleviate this challenge and
propose to prune LLMs by identifying and preserving functional networks within
LLMs in this study. To achieve this, we treat an LLM as a digital brain and
decompose the LLM into functional networks, analogous to identifying functional
brain networks in neuroimaging data. Afterwards, an LLM is pruned by preserving
the key neurons within these functional networks. Experimental results
demonstrate that the proposed method can successfully identify and locate
functional networks and key neurons in LLMs, enabling efficient model pruning.
Our code is available at https://github.com/WhatAboutMyStar/LLM_ACTIVATION.

</details>


### [120] [CodeBoost: Boosting Code LLMs by Squeezing Knowledge from Code Snippets with RL](https://arxiv.org/abs/2508.05242)
*Sijie Wang,Quanjiang Guo,Kai Zhao,Yawei Zhang,Xin Li,Xiang Li,Siqi Li,Rui She,Shangshu Yu,Wee Peng Tay*

Main category: cs.CL

TL;DR: CodeBoost是一个提升代码大模型能力的后训练框架，无需人工注释的指令，仅通过丰富的代码片段即可显著增强模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有代码大模型大多依赖于人工注释的‘指令-答案’对进行微调，但高质量指令收集费时且难以大规模扩展；而代码片段丰富易得。如何仅利用代码片段进一步训练和提升模型，成为当前研究的一个瓶颈和挑战。

Method: 提出了CodeBoost后训练框架，包括：最大团选取法（筛选代表性多样性代码）、双向预测（前向反向均预测）、错识感知预测（同时利用正误答案信号）、异质性增强（通过多样化方式丰富训练数据）、异质性奖励（多种奖励信号如格式正确性与执行反馈指导模型学习）。全流程不依赖人工指令，仅用代码片段自动后训练。

Result: 通过多种代码大语言模型和基准测试的综合实验，CodeBoost在所有评测中都稳步提升模型性能，表现优异。

Conclusion: CodeBoost是一个无需人工指令、可规模化、高效提升代码大模型能力的新型后训练方案，适合实际应用推广。

Abstract: Code large language models (LLMs) have become indispensable tools for
building efficient and automated coding pipelines. Existing models are
typically post-trained using reinforcement learning (RL) from general-purpose
LLMs using "human instruction-final answer" pairs, where the instructions are
usually from manual annotations. However, collecting high-quality coding
instructions is both labor-intensive and difficult to scale. On the other hand,
code snippets are abundantly available from various sources. This imbalance
presents a major bottleneck in instruction-based post-training. We propose
CodeBoost, a post-training framework that enhances code LLMs purely from code
snippets, without relying on human-annotated instructions. CodeBoost introduces
the following key components: (1) maximum-clique curation, which selects a
representative and diverse training corpus from code; (2) bi-directional
prediction, which enables the model to learn from both forward and backward
prediction objectives; (3) error-aware prediction, which incorporates learning
signals from both correct and incorrect outputs; (4) heterogeneous
augmentation, which diversifies the training distribution to enrich code
semantics; and (5) heterogeneous rewarding, which guides model learning through
multiple reward types including format correctness and execution feedback from
both successes and failures. Extensive experiments across several code LLMs and
benchmarks verify that CodeBoost consistently improves performance,
demonstrating its effectiveness as a scalable and effective training pipeline.

</details>


### [121] [ASCoT: An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs](https://arxiv.org/abs/2508.05282)
*Dongxu Zhang,Ning Yang,Jihua Zhu,Jinnan Yang,Miao Xin,Baoliang Tian*

Main category: cs.CL

TL;DR: 本文发现CoT推理中，后期步骤的错误比早期错误对最终答案影响更大，并提出了ASCoT方法用于自适应修正高风险错误。


<details>
  <summary>Details</summary>
Motivation: 目前主流观点认为，链式思维推理中早期错误最致命（即“级联失败”假设），但对错误在推理链不同位置上的真实影响缺乏系统验证。

Method: 通过系统性地在不同推理链位置注入错误，发现后期错误对结果损害更大，即“后期脆弱性”现象。基于此，提出ASCoT方法：其包括一个自适应验证管理器（AVM），使用位置影响分数函数识别高风险后期步骤，然后由多视角自我纠错引擎（MSCE）进行针对性纠正。

Result: 在GSM8K与MATH等基准测试中，ASCoT方法的推理准确率优于标准CoT及其他强基线。

Conclusion: 应从均一验证转向自适应、关注脆弱环节的纠错机制，这有助于提升大语言模型推理的可靠性。

Abstract: Chain-of-Thought (CoT) prompting has significantly advanced the reasoning
capabilities of Large Language Models (LLMs), yet the reliability of these
reasoning chains remains a critical challenge. A widely held "cascading
failure" hypothesis suggests that errors are most detrimental when they occur
early in the reasoning process. This paper challenges that assumption through
systematic error-injection experiments, revealing a counter-intuitive
phenomenon we term "Late-Stage Fragility": errors introduced in the later
stages of a CoT chain are significantly more likely to corrupt the final answer
than identical errors made at the beginning. To address this specific
vulnerability, we introduce the Adaptive Self-Correction Chain-of-Thought
(ASCoT) method. ASCoT employs a modular pipeline in which an Adaptive
Verification Manager (AVM) operates first, followed by the Multi-Perspective
Self-Correction Engine (MSCE). The AVM leverages a Positional Impact Score
function I(k) that assigns different weights based on the position within the
reasoning chains, addressing the Late-Stage Fragility issue by identifying and
prioritizing high-risk, late-stage steps. Once these critical steps are
identified, the MSCE applies robust, dual-path correction specifically to the
failure parts. Extensive experiments on benchmarks such as GSM8K and MATH
demonstrate that ASCoT achieves outstanding accuracy, outperforming strong
baselines, including standard CoT. Our work underscores the importance of
diagnosing specific failure modes in LLM reasoning and advocates for a shift
from uniform verification strategies to adaptive, vulnerability-aware
correction mechanisms.

</details>


### [122] [Decision-Making with Deliberation: Meta-reviewing as a Document-grounded Dialogue](https://arxiv.org/abs/2508.05283)
*Sukannya Purkayastha,Nils Dycke,Anne Lauscher,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文将元评审过程视为决策过程而非单纯总结，提出用利用大型语言模型（LLM）生成的合成对话数据，训练更有效的对话型元评审助理。结果显示这些专门训练的对话代理在提升元评审效率上优于通用LLM助手。


<details>
  <summary>Details</summary>
Motivation: 以往工作大多聚焦于评审总结，而忽视了元评审作为权衡与决策的复杂过程。当前缺乏能有效辅助元评审者决策的对话智能体，且因数据稀缺，训练此类对话体难度较大。

Method: 提出用大型语言模型配合自我完善（self-refinement）策略，生成高质量的合成元评审对话数据，并用这些数据训练专门的对话代理。将该代理与通用LLM助手在实际元评审任务中进行效果对比。

Result: 所提出的合成数据生成方法能显著提升数据相关性及质量，专门训练的对话代理在元评审辅助任务中性能优于现成通用LLM助手。在真实元评审场景中，表现出明显提升评审效率的效果。

Conclusion: 将元评审环节建模为决策性对话，有助于培养更专业的对话助手。通过高质量合成数据训练出的定制对话代理，能有效提升元评审效率和质量。

Abstract: Meta-reviewing is a pivotal stage in the peer-review process, serving as the
final step in determining whether a paper is recommended for acceptance. Prior
research on meta-reviewing has treated this as a summarization problem over
review reports. However, complementary to this perspective, meta-reviewing is a
decision-making process that requires weighing reviewer arguments and placing
them within a broader context. Prior research has demonstrated that
decision-makers can be effectively assisted in such scenarios via dialogue
agents. In line with this framing, we explore the practical challenges for
realizing dialog agents that can effectively assist meta-reviewers. Concretely,
we first address the issue of data scarcity for training dialogue agents by
generating synthetic data using Large Language Models (LLMs) based on a
self-refinement strategy to improve the relevance of these dialogues to expert
domains. Our experiments demonstrate that this method produces higher-quality
synthetic data and can serve as a valuable resource towards training
meta-reviewing assistants. Subsequently, we utilize this data to train dialogue
agents tailored for meta-reviewing and find that these agents outperform
\emph{off-the-shelf} LLM-based assistants for this task. Finally, we apply our
agents in real-world meta-reviewing scenarios and confirm their effectiveness
in enhancing the efficiency of meta-reviewing.\footnote{Code and Data:
https://github.com/UKPLab/arxiv2025-meta-review-as-dialog

</details>


### [123] [SONAR-LLM: Autoregressive Transformer that Thinks in Sentence Embeddings and Speaks in Tokens](https://arxiv.org/abs/2508.05305)
*Nikita Dragunov,Temurbek Rahmatullaev,Elizaveta Goncharova,Andrey Kuznetsov,Anton Razzhigaev*

Main category: cs.CL

TL;DR: 本文提出了SONAR-LLM，一个在连续SONAR嵌入空间中进行推理的解码器，结合LCM的语义抽象能力和传统概率训练目标，提升了文本生成的表现，并开源了所有代码和模型。


<details>
  <summary>Details</summary>
Motivation: 现有的LCM模型虽然有较强语义抽象能力，但依赖于扩散采样，不易恢复基于似然的训练信号。研究者希望设计一种兼具LCM优势且可以进行高效概率训练的新一代模型。

Method: 提出SONAR-LLM，这是一种仅有解码器的Transformer，基于连续的SONAR嵌入空间推理，通过被冻结的SONAR解码器实现token级交叉熵监督，移除了传统LCM中的扩散采样过程，实现了混合训练目标。

Result: SONAR-LLM在39M到1.3B参数规模的多种模型设置下都取得了有竞争力的文本生成质量，并通过一系列消融实验和基准测试验证了模型有效性。

Conclusion: SONAR-LLM兼具LCM语义抽象和概率监督的优点，消除了原LCM训练中的不足，有潜力提升文本生成领域的研究，并为复现和后续工作提供了完整的开源资源。

Abstract: The recently proposed Large Concept Model (LCM) generates text by predicting
a sequence of sentence-level embeddings and training with either mean-squared
error or diffusion objectives. We present SONAR-LLM, a decoder-only transformer
that "thinks" in the same continuous SONAR embedding space, yet is supervised
through token-level cross-entropy propagated via the frozen SONAR decoder. This
hybrid objective retains the semantic abstraction of LCM while eliminating its
diffusion sampler and restoring a likelihood-based training signal. Across
model sizes from 39M to 1.3B parameters, SONAR-LLM attains competitive
generation quality. We report scaling trends, ablations, benchmark results, and
release the complete training code and all pretrained checkpoints to foster
reproducibility and future research.

</details>


### [124] [Efficient Reasoning for Large Reasoning Language Models via Certainty-Guided Reflection Suppression](https://arxiv.org/abs/2508.05337)
*Jiameng Huang,Baijiong Lin,Guhao Feng,Jierun Chen,Di He,Lu Hou*

Main category: cs.CL

TL;DR: 本文提出了一种用于大型推理语言模型（LRLMs）的高效推理方法，能在不降低推理准确性的前提下，减少冗余推理步骤和计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前大模型通过长链式思考（chain-of-thought）与复杂的反思行为提升推理能力，但这些反思行为会导致“过度思考”，产生冗余的推理步骤，增加推理所需的token数量、推理成本，并减少实际应用价值。

Method: 作者提出了Certainty-Guided Reflection Suppression（CGRS）方法：当模型对当前答案有较高信心时，动态抑制其生成反思触发词（如“Wait”、“Alternatively”），避免不必要的反思回路。该方法无需对模型重新训练或结构变更，可直接集成到现有自回归生成流程，且适用于多种架构和规模。

Result: 在AIME24、AMC23、MATH500 和 GPQA-D 四个推理基准测试中，CGRS能在保持原有准确率的前提下，将token使用量平均减少18.5%-41.9%。与现有SOTA方法相比，在降低token数量和保持性能间取得最佳平衡，该结果对不同架构和不同规模的模型均适用。

Conclusion: CGRS为大模型高效推理提供了实用手段，不仅可大幅降低推理成本，还能保持推理性能，对实际部署具有重要应用价值。

Abstract: Recent Large Reasoning Language Models (LRLMs) employ long chain-of-thought
reasoning with complex reflection behaviors, typically signaled by specific
trigger words (e.g., "Wait" and "Alternatively") to enhance performance.
However, these reflection behaviors can lead to the overthinking problem where
the generation of redundant reasoning steps that unnecessarily increase token
usage, raise inference costs, and reduce practical utility. In this paper, we
propose Certainty-Guided Reflection Suppression (CGRS), a novel method that
mitigates overthinking in LRLMs while maintaining reasoning accuracy. CGRS
operates by dynamically suppressing the model's generation of reflection
triggers when it exhibits high confidence in its current response, thereby
preventing redundant reflection cycles without compromising output quality. Our
approach is model-agnostic, requires no retraining or architectural
modifications, and can be integrated seamlessly with existing autoregressive
generation pipelines. Extensive experiments across four reasoning benchmarks
(i.e., AIME24, AMC23, MATH500, and GPQA-D) demonstrate CGRS's effectiveness: it
reduces token usage by an average of 18.5% to 41.9% while preserving accuracy.
It also achieves the optimal balance between length reduction and performance
compared to state-of-the-art baselines. These results hold consistently across
model architectures (e.g., DeepSeek-R1-Distill series, QwQ-32B, and Qwen3
family) and scales (4B to 32B parameters), highlighting CGRS's practical value
for efficient reasoning.

</details>


### [125] [Evaluation of a Sign Language Avatar on Comprehensibility, User Experience \& Acceptability](https://arxiv.org/abs/2508.05358)
*Fenya Wasserroth,Eleftherios Avramidis,Vera Czehmann,Tanja Kojic,Fabrizio Nunnari,Sebastian Möller*

Main category: cs.CL

TL;DR: 本文研究了在Hololens 2设备上为手语虚拟人添加可调节功能对用户体验与可理解性的影响。尽管用户支持可调节性，但可理解性和用户体验并未提升，问题主要集中在表情、口型等核心手语要素缺失；推荐优化动画与交互。


<details>
  <summary>Details</summary>
Motivation: 随着手语虚拟人技术的发展，提升其可理解性与用户体验成为关键。本研究探索添加可调节功能（如手语虚拟人自定义设置）是否能提升德语手语用户的体验及接受度。

Method: 作者在Hololens 2上实现了可调节与不可调节的手语虚拟人系统，招募专家级德语手语用户进行交互体验，通过用户反馈、问卷和观察，评估不同系统的可理解性、用户体验、可接受性及压力水平。

Result: 尽管用户对可调节设置有偏好，但在用户体验及可理解性上与不可调节版无显著差异，共同问题包括动画不完整（缺乏口型、表情），手势不清晰及交互菜单设计不佳。可调节版反而导致更高压力及挫折感。情感愉悦性评价高于实际功能性。

Conclusion: 仅靠个性化调整不足以提升手语虚拟人体验，虚拟人应首先具备基础的高可理解性。建议增强口型、面部动画表现，改善交互界面，并通过参与式设计持续优化产品。

Abstract: This paper presents an investigation into the impact of adding adjustment
features to an existing sign language (SL) avatar on a Microsoft Hololens 2
device. Through a detailed analysis of interactions of expert German Sign
Language (DGS) users with both adjustable and non-adjustable avatars in a
specific use case, this study identifies the key factors influencing the
comprehensibility, the user experience (UX), and the acceptability of such a
system. Despite user preference for adjustable settings, no significant
improvements in UX or comprehensibility were observed, which remained at low
levels, amid missing SL elements (mouthings and facial expressions) and
implementation issues (indistinct hand shapes, lack of feedback and menu
positioning). Hedonic quality was rated higher than pragmatic quality,
indicating that users found the system more emotionally or aesthetically
pleasing than functionally useful. Stress levels were higher for the adjustable
avatar, reflecting lower performance, greater effort and more frustration.
Additionally, concerns were raised about whether the Hololens adjustment
gestures are intuitive and easy to familiarise oneself with. While
acceptability of the concept of adjustability was generally positive, it was
strongly dependent on usability and animation quality. This study highlights
that personalisation alone is insufficient, and that SL avatars must be
comprehensible by default. Key recommendations include enhancing mouthing and
facial animation, improving interaction interfaces, and applying participatory
design.

</details>


### [126] [Can Language Models Critique Themselves? Investigating Self-Feedback for Retrieval Augmented Generation at BioASQ 2025](https://arxiv.org/abs/2508.05366)
*Samy Ateia,Udo Kruschwitz*

Main category: cs.CL

TL;DR: 本文探讨了在生物医学等专业领域内，利用大模型自反馈机制提升信息检索质量的可行性及其局限性。


<details>
  <summary>Details</summary>
Motivation: 当前RAG和深度研究系统注重自动化，但在专业搜索中（如生物医学研究）往往忽视了用户参与和需求的透明性，因此需要研究如何让LLM在此环境下更有效协作。

Method: 作者在BioASQ CLEF 2025挑战下，针对LLM（如Gemini-Flash 2.0、o3-mini等）引入自反馈机制，即模型自我生成、评估并优化回答（包含查询扩展和多种答案类型），以测试其性能提升效果。

Result: 实验表明，不同模型和任务下自反馈策略效果不一，推理型LLM在生成有用反馈方面有一定优势，但提升并不稳定。

Conclusion: LLM自反馈机制在专业检索任务中存在潜力，但表现差异较大。后续应进一步比较LLM反馈与专家人工反馈的实际差异，为专业领域搜素系统设计提供依据。

Abstract: Agentic Retrieval Augmented Generation (RAG) and 'deep research' systems aim
to enable autonomous search processes where Large Language Models (LLMs)
iteratively refine outputs. However, applying these systems to domain-specific
professional search, such as biomedical research, presents challenges, as
automated systems may reduce user involvement and misalign with expert
information needs. Professional search tasks often demand high levels of user
expertise and transparency. The BioASQ CLEF 2025 challenge, using
expert-formulated questions, can serve as a platform to study these issues. We
explored the performance of current reasoning and nonreasoning LLMs like
Gemini-Flash 2.0, o3-mini, o4-mini and DeepSeek-R1. A key aspect of our
methodology was a self-feedback mechanism where LLMs generated, evaluated, and
then refined their outputs for query expansion and for multiple answer types
(yes/no, factoid, list, ideal). We investigated whether this iterative
self-correction improves performance and if reasoning models are more capable
of generating useful feedback. Preliminary results indicate varied performance
for the self-feedback strategy across models and tasks. This work offers
insights into LLM self-correction and informs future work on comparing the
effectiveness of LLM-generated feedback with direct human expert input in these
search systems.

</details>


### [127] [The TUB Sign Language Corpus Collection](https://arxiv.org/abs/2508.05374)
*Eleftherios Avramidis,Vera Czehmann,Fabian Deckert,Lorenz Hufe,Aljoscha Lipski,Yuni Amaloa Quintero Villalobos,Tae Kwon Rhee,Mengqian Shi,Lennart Stölting,Fabrizio Nunnari,Sebastian Möller*

Main category: cs.CL

TL;DR: 本文构建了包含12种手语及其配套口语字幕的大规模视频平行语料库，涵盖1300小时视频和130万条字幕，覆盖范围和规模均为同类数据集首次。


<details>
  <summary>Details</summary>
Motivation: 手语资源尤其是平行语料稀缺，极大限制了手语识别、翻译等相关研究的发展。作者希望通过大规模多语种手语平行数据，推动该领域的进展。

Method: 从多种线上公开资源（如新闻节目、政府及教育频道）采集12种手语视频，并与相应国家主流口语字幕配对。数据处理流程包括数据采集、与内容创作者沟通寻求授权、网页抓取、视频剪辑与对齐等多个步骤。

Result: 构建了涵盖12种手语的大型平行语料库（4381个视频，总时长超过1300小时，含约130万条字幕和1400万词），其中包括首批8种拉美手语的平行语料，德国语言部分规模比此前大10倍。同时公布了详细的数据统计和收集方法。

Conclusion: 该语料库为多手语处理、机器翻译等任务提供了宝贵数据资源，有助于推动多语言、尤其是拉美手语等低资源手语的处理与研究。

Abstract: We present a collection of parallel corpora of 12 sign languages in video
format, together with subtitles in the dominant spoken languages of the
corresponding countries. The entire collection includes more than 1,300 hours
in 4,381 video files, accompanied by 1,3~M subtitles containing 14~M tokens.
Most notably, it includes the first consistent parallel corpora for 8 Latin
American sign languages, whereas the size of the German Sign Language corpora
is ten times the size of the previously available corpora. The collection was
created by collecting and processing videos of multiple sign languages from
various online sources, mainly broadcast material of news shows, governmental
bodies and educational channels. The preparation involved several stages,
including data collection, informing the content creators and seeking usage
approvals, scraping, and cropping. The paper provides statistics on the
collection and an overview of the methods used to collect the data.

</details>


### [128] [MyCulture: Exploring Malaysia's Diverse Culture under Low-Resource Language Constraints](https://arxiv.org/abs/2508.05429)
*Zhong Ken Hew,Jia Xin Low,Sze Jue Yang,Chee Seng chan*

Main category: cs.CL

TL;DR: 本文提出了MyCulture基准，用于评估大语言模型（LLMs）对马来西亚文化的理解能力，并展示各模型在文化理解上的差异。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs主要依赖于英语和中文等高资源语言的数据，导致对低资源语言及相应文化的理解存在偏差。本文试图解决评测中对多样文化背景尤其是低资源语种文化表征不足的问题。

Method: 作者设计了MyCulture基准，覆盖艺术、服饰、习俗、娱乐、饮食和宗教六个马来西亚文化领域，题目使用马来语并采用无预设选项的开放式多项选择题新格式，减少猜测和格式偏差。通过理论分析和对比实验，评估结构性和语言性偏见，并让多种本地及国际LLMs参评。

Result: 实验显示，不同LLMs在文化理解上存在明显差异；开放式题目结构提升了评测的公平性和辨别能力。

Conclusion: 当前主要LLMs对低资源语言文化掌握存在显著不足，亟需发展与采用更具文化底蕴和语言包容性的评测基准，进一步推动LLMs公平、多元发展。

Abstract: Large Language Models (LLMs) often exhibit cultural biases due to training
data dominated by high-resource languages like English and Chinese. This poses
challenges for accurately representing and evaluating diverse cultural
contexts, particularly in low-resource language settings. To address this, we
introduce MyCulture, a benchmark designed to comprehensively evaluate LLMs on
Malaysian culture across six pillars: arts, attire, customs, entertainment,
food, and religion presented in Bahasa Melayu. Unlike conventional benchmarks,
MyCulture employs a novel open-ended multiple-choice question format without
predefined options, thereby reducing guessing and mitigating format bias. We
provide a theoretical justification for the effectiveness of this open-ended
structure in improving both fairness and discriminative power. Furthermore, we
analyze structural bias by comparing model performance on structured versus
free-form outputs, and assess language bias through multilingual prompt
variations. Our evaluation across a range of regional and international LLMs
reveals significant disparities in cultural comprehension, highlighting the
urgent need for culturally grounded and linguistically inclusive benchmarks in
the development and assessment of LLMs.

</details>


### [129] [LLMEval-3: A Large-Scale Longitudinal Study on Robust and Fair Evaluation of Large Language Models](https://arxiv.org/abs/2508.05452)
*Ming Zhang,Yujiong Shen,Jingyi Deng,Yuhui Wang,Yue Zhang,Junzhe Wang,Shichun Liu,Shihan Dou,Huayu Sha,Qiyuan Peng,Changhao Jiang,Jingqi Tong,Yilong Wu,Zhihao Zhang,Mingqi Wu,Zhiheng Xi,Mingxu Chai,Tao Liang,Zhihui Fei,Zhen Wang,Mingyang Wan,Guojun Ma,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 静态基准测试易受数据污染和过拟合影响，LLMEval-3通过动态抽题和自动评测体系更可靠地评估大模型能力，提升了评测的公正性与可信度。


<details>
  <summary>Details</summary>
Motivation: 当前大模型的静态评测方法存在数据污染和过度优化排行榜的问题，难以反映模型的真实能力，因此作者希望提出新的评测方法以提升评测的准确性和可信度。

Method: LLMEval-3基于22万道研究生级别题库，每次评测动态抽取未见测试集。整体流程自动化，包括抗污染数据筛选、防作弊架构、校准的LLM裁判（与人工专家90%一致）及相对排名体系。

Result: 对近50个主流模型进行了为期20个月的纵向研究，证实静态基准无法检测到的数据污染问题，且揭示了知识记忆能力的上限。动态评测体系在排名稳定性和一致性方面表现出极强的鲁棒性。

Conclusion: LLMEval-3为LLM能力评估提供了更可靠的动态测评方案，有望推动更可信赖的评测标准发展，突破以排行榜分数论英雄的局限。

Abstract: Existing evaluation of Large Language Models (LLMs) on static benchmarks is
vulnerable to data contamination and leaderboard overfitting, critical issues
that obscure true model capabilities. To address this, we introduce LLMEval-3,
a framework for dynamic evaluation of LLMs. LLMEval-3 is built on a proprietary
bank of 220k graduate-level questions, from which it dynamically samples unseen
test sets for each evaluation run. Its automated pipeline ensures integrity via
contamination-resistant data curation, a novel anti-cheating architecture, and
a calibrated LLM-as-a-judge process achieving 90% agreement with human experts,
complemented by a relative ranking system for fair comparison. An 20-month
longitudinal study of nearly 50 leading models reveals a performance ceiling on
knowledge memorization and exposes data contamination vulnerabilities
undetectable by static benchmarks. The framework demonstrates exceptional
robustness in ranking stability and consistency, providing strong empirical
validation for the dynamic evaluation paradigm. LLMEval-3 offers a robust and
credible methodology for assessing the true capabilities of LLMs beyond
leaderboard scores, promoting the development of more trustworthy evaluation
standards.

</details>


### [130] [TASE: Token Awareness and Structured Evaluation for Multilingual Language Models](https://arxiv.org/abs/2508.05468)
*Chenzhuo Zhao,Xinda Wang,Yue Huang,Junting Lu,Ziqian Liu*

Main category: cs.CL

TL;DR: 本文提出了TASE基准，用于全面测试大型语言模型（LLM）在细粒度token级别感知和结构化推理能力，覆盖10项任务和三国语言，评估了30余种主流LLM，结果表明现有模型与人类存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在高层语义任务上表现优越，但其在精确和受控场景中常常缺乏对token级信息的理解和结构化推理能力。因此需要一个系统性基准来揭示和量化这些不足。

Method: 构建TASE基准，分为token感知和结构理解两个核心类别（共10项任务），涵盖中、英、韩三种语言，包含超3.5万条评测实例，并开发了可扩展的合成数据生成流水线。评测超30种主流LLM（包括O3, Claude 4, Gemini 2.5 Pro等）及自行训练的Qwen2.5-14B。

Result: 结果显示人类在token级推理任务上表现显著优于现有LLM，揭示了当前主流模型在底层语言理解和跨语言泛化能力方面的持续短板。

Conclusion: TASE为LLM细粒度、低层级语言理解的局限性提供了诊断工具，能促进更具精准控制和泛化能力的新模型产生。基准和数据集公开，为社区未来改进提供基础。

Abstract: While large language models (LLMs) have demonstrated remarkable performance
on high-level semantic tasks, they often struggle with fine-grained,
token-level understanding and structural reasoning--capabilities that are
essential for applications requiring precision and control. We introduce TASE,
a comprehensive benchmark designed to evaluate LLMs' ability to perceive and
reason about token-level information across languages. TASE covers 10 tasks
under two core categories: token awareness and structural understanding,
spanning Chinese, English, and Korean, with a 35,927-instance evaluation set
and a scalable synthetic data generation pipeline for training. Tasks include
character counting, token alignment, syntactic structure parsing, and length
constraint satisfaction. We evaluate over 30 leading commercial and open-source
LLMs, including O3, Claude 4, Gemini 2.5 Pro, and DeepSeek-R1, and train a
custom Qwen2.5-14B model using the GRPO training method. Results show that
human performance significantly outpaces current LLMs, revealing persistent
weaknesses in token-level reasoning. TASE sheds light on these limitations and
provides a new diagnostic lens for future improvements in low-level language
understanding and cross-lingual generalization. Our code and dataset are
publicly available at https://github.com/cyzcz/Tase .

</details>


### [131] [Rethinking Creativity Evaluation: A Critical Analysis of Existing Creativity Evaluations](https://arxiv.org/abs/2508.05470)
*Li-Chun Lu,Miri Liu,Pin-Chun Lu,Yufei Tian,Shao-Hua Sun,Nanyun Peng*

Main category: cs.CL

TL;DR: 本论文系统对比了多种创意度评估指标，但发现它们各自存在局限性，亟需更全面一致的评价体系。


<details>
  <summary>Details</summary>
Motivation: 当前用于评价生成创意内容的指标（如创造力指数、困惑度等）众多，但缺乏一致性，不能全面准确衡量AI或人的创造力。因此，有必要厘清各指标的优缺点，并探讨更好的评价方法。

Method: 作者系统分析并实证比较了几种主要的创意度评估方法，包括创造力指数、困惑度、句法模板法以及基于大模型评价（LLM-as-a-Judge），覆盖了创意写作、非常规问题解决、科研构思等不同领域。

Result: 研究发现，不同创意指标间一致性较差，各自仅反映创造力的某一维度。例如，创造力指数偏重词汇多样性，困惑度受模型置信度影响大，句法模板难以描述概念创新，大模型判官存在主观性和偏见。

Conclusion: 现有创意性评价指标各有不足，难以完全反映人类对创意的判断。未来亟需开发更稳健、泛化性更强且能更好贴合人类评判的新型评估框架。

Abstract: We systematically examine, analyze, and compare representative creativity
measures--creativity index, perplexity, syntactic templates, and
LLM-as-a-Judge--across diverse creative domains, including creative writing,
unconventional problem-solving, and research ideation. Our analyses reveal that
these metrics exhibit limited consistency, capturing different dimensions of
creativity. We highlight key limitations, including the creativity index's
focus on lexical diversity, perplexity's sensitivity to model confidence, and
syntactic templates' inability to capture conceptual creativity. Additionally,
LLM-as-a-Judge shows instability and bias. Our findings underscore the need for
more robust, generalizable evaluation frameworks that better align with human
judgments of creativity.

</details>


### [132] [LAG: Logic-Augmented Generation from a Cartesian Perspective](https://arxiv.org/abs/2508.05509)
*Yilin Xiao,Chuang Zhou,Qinggang Zhang,Su Dong,Shengyuan Chen,Xiao Huang*

Main category: cs.CL

TL;DR: 本文提出了一种名为逻辑增强生成（LAG）的新范式，通过系统性的问题分解和依赖感知推理，提升大语言模型（LLMs）在知识密集型任务中的表现，显著减少幻觉、提升推理鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在遇到需要专业知识的问题时容易产生幻觉（错误答案）。尽管RAG（检索增强生成）方法能部分缓解此问题，但由于它依赖直接语义检索和缺乏结构化逻辑推理，依然难以应对复杂推理任务。

Method: LAG 方法首先将复杂问题分解为逻辑有依赖关系的原子子问题，并按依赖顺序逐步求解。每解答一个子问题，其答案会用来指导后续相关内容的检索和推理，从而保证推理链路的逻辑性。若在过程中遇到无法解答的子问题，LAG 会及时终止，避免错误传播和资源浪费。最终，将所有子问题的结论综合起来，生成经过验证的最终答案。

Result: 在四个基准数据集实验中，LAG 显著提升了LLM的推理鲁棒性，有效减少了幻觉现象，并且让大模型的推理过程更贴合人类的认知方式。

Conclusion: LAG 为当前RAG体系提供了一个结构化及原则性的替代方案，针对知识密集与复杂推理任务，取得了更优的性能表现。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
a wide range of tasks, yet exhibit critical limitations in knowledge-intensive
tasks, often generating hallucinations when faced with questions requiring
specialized expertise. While retrieval-augmented generation (RAG) mitigates
this by integrating external knowledge, it struggles with complex reasoning
scenarios due to its reliance on direct semantic retrieval and lack of
structured logical organization. Inspired by Cartesian principles from
\textit{Discours de la m\'ethode}, this paper introduces Logic-Augmented
Generation (LAG), a novel paradigm that reframes knowledge augmentation through
systematic question decomposition and dependency-aware reasoning. Specifically,
LAG first decomposes complex questions into atomic sub-questions ordered by
logical dependencies. It then resolves these sequentially, using prior answers
to guide context retrieval for subsequent sub-questions, ensuring stepwise
grounding in logical chain. To prevent error propagation, LAG incorporates a
logical termination mechanism that halts inference upon encountering
unanswerable sub-questions and reduces wasted computation on excessive
reasoning. Finally, it synthesizes all sub-resolutions to generate verified
responses. Experiments on four benchmark datasets demonstrate that LAG
significantly enhances reasoning robustness, reduces hallucination, and aligns
LLM problem-solving with human cognition, offering a principled alternative to
existing RAG systems.

</details>


### [133] [The World According to LLMs: How Geographic Origin Influences LLMs' Entity Deduction Capabilities](https://arxiv.org/abs/2508.05525)
*Harsh Nishant Lalai,Raj Sanjay Shah,Jiaxin Pei,Sashank Varma,Yi-Chia Wang,Ali Emami*

Main category: cs.CL

TL;DR: 本论文提出通过“20个问题”游戏新颖地评估大语言模型（LLMs）中的隐性地理和文化偏见，结果发现模型在推理和归纳时存在对全球北方和西方的显著偏好。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs经过大量微调以减轻显性偏见，但隐性、细微的偏见依然存在。当前评估多依赖人工设置的提示，可能触发模型的“护栏”机制，无法充分暴露潜在偏见。因此，作者希望通过模型主动提问的情境，探索其深层次的推理和归纳偏好。

Method: 作者设计了一个基于“20个问题”游戏的评测框架，构建了新的Geo20Q+数据集，涵盖全球不同地区的人物及文化对象，并在英语、印地语、汉语、日语、法语、西班牙语和土耳其语七种语言下、两种游戏模式（20问和不限次数）测试LLMs的推理表现，量化地理和文化差异。

Result: 实验显示，无论活动在哪种语言下进行，LLMs在推理归纳来自全球北方、西方的实体时成功率大幅高于南方、东方实体。这种差异与实体在维基百科的曝光度及预训练数据中的频率相关但关联有限，语言影响则很小。

Conclusion: 采用“20个问题”这样的自由推理框架，可以有效揭示LLMs中深层、微妙的地理与文化偏见。文章呼吁更多类似创新评估方式，并公开了数据集和代码以促进后续研究。

Abstract: Large Language Models (LLMs) have been extensively tuned to mitigate explicit
biases, yet they often exhibit subtle implicit biases rooted in their
pre-training data. Rather than directly probing LLMs with human-crafted
questions that may trigger guardrails, we propose studying how models behave
when they proactively ask questions themselves. The 20 Questions game, a
multi-turn deduction task, serves as an ideal testbed for this purpose. We
systematically evaluate geographic performance disparities in entity deduction
using a new dataset, Geo20Q+, consisting of both notable people and culturally
significant objects (e.g., foods, landmarks, animals) from diverse regions. We
test popular LLMs across two gameplay configurations (canonical 20-question and
unlimited turns) and in seven languages (English, Hindi, Mandarin, Japanese,
French, Spanish, and Turkish). Our results reveal geographic disparities: LLMs
are substantially more successful at deducing entities from the Global North
than the Global South, and the Global West than the Global East. While
Wikipedia pageviews and pre-training corpus frequency correlate mildly with
performance, they fail to fully explain these disparities. Notably, the
language in which the game is played has minimal impact on performance gaps.
These findings demonstrate the value of creative, free-form evaluation
frameworks for uncovering subtle biases in LLMs that remain hidden in standard
prompting setups. By analyzing how models initiate and pursue reasoning goals
over multiple turns, we find geographic and cultural disparities embedded in
their reasoning processes. We release the dataset (Geo20Q+) and code at
https://sites.google.com/view/llmbias20q/home.

</details>


### [134] [CoCoLex: Confidence-guided Copy-based Decoding for Grounded Legal Text Generation](https://arxiv.org/abs/2508.05534)
*Santosh T. Y. S. S,Youssef Tarek Elkhayat,Oana Ichim,Pranav Shetty,Dongsheng Wang,Zhiqiang Ma,Armineh Nourbakhsh,Xiaomo Liu*

Main category: cs.CL

TL;DR: 本文提出了一种名为CoCoLex的新型解码策略，用于提升大语言模型在法律文本生成中的忠实性和准确性。


<details>
  <summary>Details</summary>
Motivation: 大模型在法律领域应用时，虽然能处理复杂和长语境，但经常输出虚假或不基于事实的内容。虽然检索增强生成（RAG）能够一定程度缓解这一问题，但仍无法保证模型有效融合外部知识，现有的语境感知解码技术也未能显式保障生成内容的忠实度。

Method: 作者提出了基于置信度引导的拷贝式解码方法（CoCoLex），通过将模型自身生成的词分布与从上下文‘拷贝’而得的分布动态结合，提升模型根据置信度直接复制原始上下文内容的能力，从而增强生成结果与事实依据的一致性。

Result: 在五个法律数据集上进行实验，CoCoLex在长文本生成等任务上明显优于现有的语境感知解码方法。

Conclusion: CoCoLex能有效提升法律文本生成任务的忠实性和准确性，有望推动大语言模型在法律行业的应用。

Abstract: Due to their ability to process long and complex contexts, LLMs can offer key
benefits to the Legal domain, but their adoption has been hindered by their
tendency to generate unfaithful, ungrounded, or hallucinatory outputs. While
Retrieval-Augmented Generation offers a promising solution by grounding
generations in external knowledge, it offers no guarantee that the provided
context will be effectively integrated. To address this, context-aware decoding
strategies have been proposed to amplify the influence of relevant context, but
they usually do not explicitly enforce faithfulness to the context. In this
work, we introduce Confidence-guided Copy-based Decoding for Legal Text
Generation (CoCoLex)-a decoding strategy that dynamically interpolates the
model produced vocabulary distribution with a distribution derived based on
copying from the context. CoCoLex encourages direct copying based on the
model's confidence, ensuring greater fidelity to the source. Experimental
results on five legal benchmarks demonstrate that CoCoLex outperforms existing
context-aware decoding methods, particularly in long-form generation tasks.

</details>


### [135] [Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees](https://arxiv.org/abs/2508.05544)
*Guang Yang,Xinyang Liu*

Main category: cs.CL

TL;DR: 本文提出了一种基于频率的不确定性量化方法，有效提升了在选择题任务中大语言模型（LLM）的可靠性，尤其适用于黑盒场景。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在选择题作答中表现优异，但由于其出现幻觉和过度自信等不可靠因素，限制了其在高风险领域的应用，因此需要增强可靠性和量化不确定性的方法。

Method: 作者提出利用共形预测（CP）方法，通过对每一输入执行多次独立采样，统计输出分布出现最频繁的答案，并据此计算预测熵（Predictive Entropy, PE）。该方法仅依赖采样结果，无需访问模型内部得分或概率，适合黑盒模型。

Result: 在六个LLM和四个不同选择题数据集（包括医学和广义任务）上，实验证明基于频率的PE在区分正确与错误答案上（以AUROC衡量）优于传统基于logit的PE，同时能精准控制实际错误覆盖率，符合用户设定的风险水平。

Conclusion: 提出的方法为黑盒LLM提供了分布无关、模型无关的可靠不确定性量化框架，提高了选择题任务中LLM的可信度，推动其在实际高风险应用中的落地。

Abstract: Large Language Models (LLMs) have shown remarkable progress in
multiple-choice question answering (MCQA), but their inherent unreliability,
such as hallucination and overconfidence, limits their application in high-risk
domains. To address this, we propose a frequency-based uncertainty
quantification method under black-box settings, leveraging conformal prediction
(CP) to ensure provable coverage guarantees. Our approach involves multiple
independent samplings of the model's output distribution for each input, with
the most frequent sample serving as a reference to calculate predictive entropy
(PE). Experimental evaluations across six LLMs and four datasets (MedMCQA,
MedQA, MMLU, MMLU-Pro) demonstrate that frequency-based PE outperforms
logit-based PE in distinguishing between correct and incorrect predictions, as
measured by AUROC. Furthermore, the method effectively controls the empirical
miscoverage rate under user-specified risk levels, validating that sampling
frequency can serve as a viable substitute for logit-based probabilities in
black-box scenarios. This work provides a distribution-free model-agnostic
framework for reliable uncertainty quantification in MCQA with guaranteed
coverage, enhancing the trustworthiness of LLMs in practical applications.

</details>


### [136] [Do Political Opinions Transfer Between Western Languages? An Analysis of Unaligned and Aligned Multilingual LLMs](https://arxiv.org/abs/2508.05553)
*Franziska Weeber,Tanise Ceron,Sebastian Padó*

Main category: cs.CL

TL;DR: 本研究探讨了多语言大模型（MLLMs）在不同西方语言中展现的政治观点是否存在差异。结果发现，未对齐的模型在五种语言中几乎没有显著的政治观点差异。通过对齐操作，所有语言的政治倾向几乎一致变化。结论：在西方语言背景下，政治观点能在不同语言间转移，难以实现明确的社会、文化和政治对齐。


<details>
  <summary>Details</summary>
Motivation: 现实中不同文化背景下公众政治观点存在明显差异，但尚不清楚这些差异是否会体现在MLLMs的多语言输出上。理解这一点有助于评估大模型在多语言、多文化环境下的表现和潜在偏见。

Method: 作者选取多种规模的MLLMs，针对五种西方语言，利用政治立场相关的问卷语句，通过prompt方式测试模型的赞同或反对意见。为分析语言间交互，还进行了用英文数据单独左/右政治对齐（preference optimization）对比试验。

Result: 未对齐模型在五种语言间表现出的政治意见差异极少，再经过英文对齐操作后，各语言的政治立场几乎同步转变，显示对齐效果能跨语言一致影响模型输出。

Conclusion: 多语言大模型在西方语言环境中的政治观点能在不同语言间迁移，这突显了在实现真正多语种、多文化背景对齐上的挑战。

Abstract: Public opinion surveys show cross-cultural differences in political opinions
between socio-cultural contexts. However, there is no clear evidence whether
these differences translate to cross-lingual differences in multilingual large
language models (MLLMs). We analyze whether opinions transfer between languages
or whether there are separate opinions for each language in MLLMs of various
sizes across five Western languages. We evaluate MLLMs' opinions by prompting
them to report their (dis)agreement with political statements from voting
advice applications. To better understand the interaction between languages in
the models, we evaluate them both before and after aligning them with more left
or right views using direct preference optimization and English alignment data
only. Our findings reveal that unaligned models show only very few significant
cross-lingual differences in the political opinions they reflect. The political
alignment shifts opinions almost uniformly across all five languages. We
conclude that in Western language contexts, political opinions transfer between
languages, demonstrating the challenges in achieving explicit socio-linguistic,
cultural, and political alignment of MLLMs.

</details>


### [137] [MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging Synthetic Problems with a Reinforced Policy](https://arxiv.org/abs/2508.05592)
*Shaoxiong Zhan,Yanlin Lai,Ziyu Lu,Dahua Lin,Ziqing Yang,Fei Tang*

Main category: cs.CL

TL;DR: MathSmith提出了一种全新框架，从头自动合成高难度数学题目，为大语言模型的数学推理能力训练提供优质数据，并在多个基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在数学推理方面受限于高质量、高难度训练数据的稀缺。现有的方法多依赖于人工题目模板的变换，导致题目类型单一，难以扩展。

Method: MathSmith框架通过随机采样PlanetMath中的概念及解释对，从零构建数学问题，保证数据独立且避免数据污染。框架还设计了九种软约束以提升难度，并利用强化学习联合优化题目的结构有效性、推理复杂度和答案一致性。通过自回归生成推理轨迹的长度来度量认知复杂度，促进长链推理题目的生成。此外，还引入了按弱点生成变体的模块用于针对性提升。

Result: 在五个数学推理基准集（包括易中难类别和高难类别）上，MathSmith在短链和长链推理场景下均超过了现有的合成数据方法。弱点变体模块可实现针对性提升。

Conclusion: MathSmith展示了优秀的可扩展性、泛化能力和可迁移性，说明高难度合成数据有助于提升大语言模型的数学推理能力。

Abstract: Large language models have achieved substantial progress in mathematical
reasoning, yet their advancement is limited by the scarcity of high-quality,
high-difficulty training data. Existing synthesis methods largely rely on
transforming human-written templates, limiting both diversity and scalability.
We propose MathSmith, a novel framework for synthesizing challenging
mathematical problems to enhance LLM reasoning. Rather than modifying existing
problems, MathSmith constructs new ones from scratch by randomly sampling
concept-explanation pairs from PlanetMath, ensuring data independence and
avoiding contamination. To increase difficulty, we design nine predefined
strategies as soft constraints during rationales. We further adopts
reinforcement learning to jointly optimize structural validity, reasoning
complexity, and answer consistency. The length of the reasoning trace generated
under autoregressive prompting is used to reflect cognitive complexity,
encouraging the creation of more demanding problems aligned with
long-chain-of-thought reasoning. Experiments across five benchmarks,
categorized as easy & medium (GSM8K, MATH-500) and hard (AIME2024, AIME2025,
OlympiadBench), show that MathSmith consistently outperforms existing baselines
under both short and long CoT settings. Additionally, a weakness-focused
variant generation module enables targeted improvement on specific concepts.
Overall, MathSmith exhibits strong scalability, generalization, and
transferability, highlighting the promise of high-difficulty synthetic data in
advancing LLM reasoning capabilities.

</details>


### [138] [Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2508.05613)
*Haitao Hong,Yuchen Yan,Xingyu Wu,Guiyang Hou,Wenqi Zhang,Weiming Lu,Yongliang Shen,Jun Xiao*

Main category: cs.CL

TL;DR: 本文提出了一种名为Cooper的RL框架，联合优化策略模型和奖励模型，结合规则和模型奖励的优点，以提升大语言模型的推理能力，显著缓解reward hacking问题并提高RL表现。


<details>
  <summary>Details</summary>
Motivation: 当前用于大语言模型推理能力强化的奖励范式分为基于规则和基于模型两种，但均存在局限性：规则奖励鲁棒性差，模型奖励易被reward hacking。为解决上述问题，需设计兼具两者优点且能动态适应的奖励机制。

Method: 提出Cooper框架，实现策略模型和奖励模型的联合训练。方法融合规则奖励的高精度，动态挑选正负样本对持续优化奖励模型，提升鲁棒性并减少reward hacking风险。同时引入混合标注策略为奖励模型高效生成训练数据，并设计参考答案输入的奖励建模范式。

Result: 基于该方法训练的VerifyRM奖励模型，在VerifyBench上超越同等规模模型；用VerifyRM和Cooper做RL训练，能有效缓解reward hacking，提升RL端到端表现（在Qwen2.5-1.5B-Instruct取得0.54%的平均准确率提升）。

Conclusion: 动态更新的奖励模型能有效对抗reward hacking，验证了联合优化并整合奖励模型进RL的优势，为未来RL范式设计提供新思路。

Abstract: Large language models (LLMs) have demonstrated remarkable performance in
reasoning tasks, where reinforcement learning (RL) serves as a key algorithm
for enhancing their reasoning capabilities. Currently, there are two mainstream
reward paradigms: model-based rewards and rule-based rewards. However, both
approaches suffer from limitations: rule-based rewards lack robustness, while
model-based rewards are vulnerable to reward hacking. To address these issues,
we propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework
that jointly optimizes both the policy model and the reward model. Cooper
leverages the high precision of rule-based rewards when identifying correct
responses, and dynamically constructs and selects positive-negative sample
pairs for continued training the reward model. This design enhances robustness
and mitigates the risk of reward hacking. To further support Cooper, we
introduce a hybrid annotation strategy that efficiently and accurately
generates training data for the reward model. We also propose a reference-based
reward modeling paradigm, where the reward model takes a reference answer as
input. Based on this design, we train a reward model named VerifyRM, which
achieves higher accuracy on VerifyBench compared to other models of the same
size. We conduct reinforcement learning using both VerifyRM and Cooper. Our
experiments show that Cooper not only alleviates reward hacking but also
improves end-to-end RL performance, for instance, achieving a 0.54% gain in
average accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that
dynamically updating reward model is an effective way to combat reward hacking,
providing a reference for better integrating reward models into RL.

</details>


### [139] [OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks](https://arxiv.org/abs/2508.05614)
*Zixuan Wang,Dingming Li,Hongxing Li,Shuo Chen,Yuchen Yan,Wenqi Zhang,Yongliang Shen,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.CL

TL;DR: OmniEAR提出了一个全新的基准框架，全面评估大语言模型在具身任务中的物理交互、工具使用和多智能体协作推理能力。实验显示当前模型在面对具身推理时表现显著下降，特别是在多智能体和需要自主协调场景，说明具身推理对现有架构提出了新的挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在抽象推理方面表现出色，但其在具身智能（如物理世界交互、工具使用、多智能体协作）场景下的推理能力仍未被充分探索和系统评估。目前相关基准主要提供固定工具或显式协作指令，缺乏对模型自主适应和泛化能力的测试。因此，作者希望通过一个更具挑战性和系统性的框架，推动该领域研究发展。

Method: 1. 构建OmniEAR基准，包含1,500个涵盖家居和工业场景的具身推理任务。2. 让语言模型在没有预定义工具/协作指令下，需要自主获取能力、协调与推理。3. 任务涉及连续物理属性、复杂空间关系，环境以文本方式表征。4. 系统评估模型在工具推理、隐式和显式多智能体任务下的表现，并分析最终结果。

Result: 1. 如果提供明确指令，模型成功率可达85-96%；但在工具推理和隐式协作任务中，仅为56-85%、63-85%。2. 复合复杂任务中，模型失败率超过50%。3. 给出完整环境信息反而降低了多智能体的协作表现，表明模型难以筛选关键约束。4. 微调对单智能体任务极大提升性能（0.6%增至76.3%），但对多智能体仅有极小提升（1.5%增至5.5%）。5. 揭示了当前架构在多智能体具身推理上的核心局限。

Conclusion: 具身推理任务对大语言模型提出了全新的理论与工程挑战。与文本抽象推理表现强大形成对比，现有模型在具身、多智能体自主协作方面有显著短板。OmniEAR为评估和推动具身AI系统研究提供了高标准基准，未来需专门发展更适合复杂物理世界协作与推理的AI架构。

Abstract: Large language models excel at abstract reasoning but their capacity for
embodied agent reasoning remains largely unexplored. We present OmniEAR, a
comprehensive framework for evaluating how language models reason about
physical interactions, tool usage, and multi-agent coordination in embodied
tasks. Unlike existing benchmarks that provide predefined tool sets or explicit
collaboration directives, OmniEAR requires agents to dynamically acquire
capabilities and autonomously determine coordination strategies based on task
demands. Through text-based environment representation, we model continuous
physical properties and complex spatial relationships across 1,500 scenarios
spanning household and industrial domains. Our systematic evaluation reveals
severe performance degradation when models must reason from constraints: while
achieving 85-96% success with explicit instructions, performance drops to
56-85% for tool reasoning and 63-85% for implicit collaboration, with compound
tasks showing over 50% failure rates. Surprisingly, complete environmental
information degrades coordination performance, indicating models cannot filter
task-relevant constraints. Fine-tuning improves single-agent tasks dramatically
(0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing
fundamental architectural limitations. These findings demonstrate that embodied
reasoning poses fundamentally different challenges than current models can
address, establishing OmniEAR as a rigorous benchmark for evaluating and
advancing embodied AI systems. Our code and data are included in the
supplementary materials and will be open-sourced upon acceptance.

</details>


### [140] [Learning to Reason for Factuality](https://arxiv.org/abs/2508.05618)
*Xilun Chen,Ilia Kulikov,Vincent-Pierre Berges,Barlas Oğuz,Rulin Shao,Gargi Ghosh,Jason Weston,Wen-tau Yih*

Main category: cs.CL

TL;DR: 本文提出了一种新的奖励函数，结合事实准确性、回答细节性和相关性，解决了现有基于强化学习优化大语言模型事实性推理时出现的奖赏规避问题。该方法在长文本事实性任务上显著降低了幻觉发生率，提高了回答详细程度，并保持整体有用性。


<details>
  <summary>Details</summary>
Motivation: 尽管带推理能力的大语言模型（R-LLMs）在复杂推理任务上表现突出，但其在长文本事实性任务中，生成幻觉现象比不带推理的模型更多，对长期应用构成挑战。当前强化学习方式难以适用于事实性推理，因为缺乏可靠的事实性自动评估工具并存在奖赏滥用（如生成无关或简略回答）问题。

Method: 作者提出了一种新的奖励函数，将事实精度、回答细节和相关性三项因素同时纳入评价，并在在线强化学习框架下训练大模型，旨在抑制奖赏规避行为并提升模型事实性推理表现。

Result: 在六个长文本事实性基准任务上，该模型平均将幻觉率降低了23.1个百分点，回答详细程度提升23%，并且在整体帮助性方面无明显下降。

Conclusion: 通过引入多重指标设计的奖励函数，可有效改善大语言模型长文本事实性推理的幻觉率和回答详细度，证明此方法优于仅用传统自动评测的方式。

Abstract: Reasoning Large Language Models (R-LLMs) have significantly advanced complex
reasoning tasks but often struggle with factuality, generating substantially
more hallucinations than their non-reasoning counterparts on long-form
factuality benchmarks. However, extending online Reinforcement Learning (RL), a
key component in recent R-LLM advancements, to the long-form factuality setting
poses several unique challenges due to the lack of reliable verification
methods. Previous work has utilized automatic factuality evaluation frameworks
such as FActScore to curate preference data in the offline RL setting, yet we
find that directly leveraging such methods as the reward in online RL leads to
reward hacking in multiple ways, such as producing less detailed or relevant
responses. We propose a novel reward function that simultaneously considers the
factual precision, response detail level, and answer relevance, and applies
online RL to learn high quality factual reasoning. Evaluated on six long-form
factuality benchmarks, our factual reasoning model achieves an average
reduction of 23.1 percentage points in hallucination rate, a 23% increase in
answer detail level, and no degradation in the overall response helpfulness.

</details>


### [141] [How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations](https://arxiv.org/abs/2508.05625)
*Brandon Jaipersaud,David Krueger,Ekdeep Singh Lubana*

Main category: cs.CL

TL;DR: 本文通过使用线性探针（linear probes）分析大语言模型（LLM）在多轮自然对话中的说服能力，并发现这种方法高效且在某些方面优于传统的提示方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型展现了说服人的能力，但我们对其背后动态机制的理解有限。过去的工作表明线性探针可以有效分析模型的某些技能，因此本文希望利用这种方法深入理解LLM的说服机制。

Method: 本文借鉴认知科学的洞见，针对说服成功、被说服者个性、说服策略等不同维度训练线性探针，并在大规模、多轮对话数据集上进行分析，比较与提示方法在效率和表现上的差异。

Result: 实验显示，线性探针不仅可以有效定位对话中被说服的关键点，也能在策略分析等任务上与高耗算力的提示方法表现相当甚至更优。

Conclusion: 线性探针在多轮对话和大数据量下提供了一种高效且有力的分析工具，有望应用于欺骗、操控等复杂行为的研究，在需要高效率分析的场景尤其具备优势。

Abstract: Large Language Models (LLMs) have started to demonstrate the ability to
persuade humans, yet our understanding of how this dynamic transpires is
limited. Recent work has used linear probes, lightweight tools for analyzing
model representations, to study various LLM skills such as the ability to model
user sentiment and political perspective. Motivated by this, we apply probes to
study persuasion dynamics in natural, multi-turn conversations. We leverage
insights from cognitive science to train probes on distinct aspects of
persuasion: persuasion success, persuadee personality, and persuasion strategy.
Despite their simplicity, we show that they capture various aspects of
persuasion at both the sample and dataset levels. For instance, probes can
identify the point in a conversation where the persuadee was persuaded or where
persuasive success generally occurs across the entire dataset. We also show
that in addition to being faster than expensive prompting-based approaches,
probes can do just as well and even outperform prompting in some settings, such
as when uncovering persuasion strategy. This suggests probes as a plausible
avenue for studying other complex behaviours such as deception and
manipulation, especially in multi-turn settings and large-scale dataset
analysis where prompting-based methods would be computationally inefficient.

</details>


### [142] [H-Net++: Hierarchical Dynamic Chunking for Tokenizer-Free Language Modelling in Morphologically-Rich Languages](https://arxiv.org/abs/2508.05628)
*Mehrdad Zakershahrak,Samira Ghodratnama*

Main category: cs.CL

TL;DR: 本文提出了H-NET++模型，通过层次动态分块学习，实现无需分词器的字节级语言建模，有效提升了对形态丰富语言（如波斯语）的处理能力，在多个基准上取得了最佳表现。


<details>
  <summary>Details</summary>
Motivation: 字节级语言模型虽然消除了对脆弱分词器的依赖，但在处理词形变化丰富的语言时，容易因字节序列过长带来计算负担。该问题亟需更高效的建模方法。

Method: 提出H-NET++模型，引入轻量级Transformer上下文混合模块、两级隐式超先验保持文档一致性、特殊处理正字法符号（如波斯语的ZWNJ），并采用分阶段的课程学习训练不同序列长度。

Result: 在1.4B波斯语数据集上，H-NET++在压缩率、泛化能力、正字法鲁棒性及词形边界检测上均大幅优于BPE-GPT-2。具体表现包括BPB减少0.159，ParsGLUE提升5.4个百分点，ZWNJ鲁棒性提升53%，词形边界检测F1为73.8%。

Conclusion: H-NET++模型能在无需显式分词与人工标注的情况下，通过端到端学习对形态丰富语言实现高效、精准的建模，为此类语言提供了更优的字节级解决方案。

Abstract: Byte-level language models eliminate fragile tokenizers but face
computational challenges in morphologically-rich languages (MRLs), where words
span many bytes. We propose H-NET++, a hierarchical dynamic-chunking model that
learns linguistically-informed segmentation through end-to-end training. Key
innovations include: (1) a lightweight Transformer context-mixer (1.9M
parameters) for cross-chunk attention, (2) a two-level latent hyper-prior for
document-level consistency, (3) specialized handling of orthographic artifacts
(e.g. Persian ZWNJ), and (4) curriculum-based training with staged sequence
lengths. On a 1.4B-token Persian corpus, H-NET++ achieves state-of-the-art
results: 0.159 BPB reduction versus BPE-based GPT-2-fa (12% better
compression), 5.4pp gain on ParsGLUE, 53% improved robustness to ZWNJ
corruption, and 73.8% F1 on gold morphological boundaries. Our learned chunks
align with Persian morphology without explicit supervision, demonstrating that
hierarchical dynamic chunking provides an effective tokenizer-free solution for
MRLs while maintaining computational efficiency.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [143] [On the causality between affective impact and coordinated human-robot reactions](https://arxiv.org/abs/2508.04834)
*Morten Roed Frederiksen,Kasper Støy*

Main category: cs.RO

TL;DR: 本文研究了社交情境中机器人与人类共享反应对人类感知机器人的情感影响的作用，发现机器人的快速、与人类同步的反应有助于提升人类对机器人情感表现的认可。


<details>
  <summary>Details</summary>
Motivation: 为了提升机器人在与人类互动时的社交能力，使机器人在与人类共同经历事件时能通过情感化反应改变人类对机器人的情感认知。

Method: 设计了两组实验。第一组分别让实验组和对照组各自与机器人互动，探究机器人向人类共享反应时的影响。第二组让110名参与者经历逐渐递增反应延迟的机器人，60组共测试不同延迟反应效果。通过观察不同设置下人类对机器人的情感感知差异，结合统计显著性分析。

Result: 结果显示，当机器人在与人类共享事件时做出同步反应时，人类对机器人的情感感知显著增强（p<0.05）。对于物理互动，机器人接近人类的反应时长最能被接受。200ms左右的延时能带来最强情感冲击，100ms左右的延时让人类最有影响机器人的感觉。

Conclusion: 小型非人形机器人在情感表达时，反应延迟约200ms能带来最大人类感知冲击；如果目标是让人类感觉自己影响最大，则约100ms的反应延迟最有效。

Abstract: In an effort to improve how robots function in social contexts, this paper
investigates if a robot that actively shares a reaction to an event with a
human alters how the human perceives the robot's affective impact. To verify
this, we created two different test setups. One to highlight and isolate the
reaction element of affective robot expressions, and one to investigate the
effects of applying specific timing delays to a robot reacting to a physical
encounter with a human. The first test was conducted with two different groups
(n=84) of human observers, a test group and a control group both interacting
with the robot. The second test was performed with 110 participants using
increasingly longer reaction delays for the robot with every ten participants.
The results show a statistically significant change (p$<$.05) in perceived
affective impact for the robots when they react to an event shared with a human
observer rather than reacting at random. The result also shows for shared
physical interaction, the near-human reaction times from the robot are most
appropriate for the scenario. The paper concludes that a delay time around
200ms may render the biggest impact on human observers for small-sized
non-humanoid robots. It further concludes that a slightly shorter reaction time
around 100ms is most effective when the goal is to make the human observers
feel they made the biggest impact on the robot.

</details>


### [144] [INTENTION: Inferring Tendencies of Humanoid Robot Motion Through Interactive Intuition and Grounded VLM](https://arxiv.org/abs/2508.04931)
*Jin Wang,Weijie Wang,Boyuan Deng,Heng Zhang,Rui Dai,Nikos Tsagarakis*

Main category: cs.RO

TL;DR: 该论文提出了一个名为INTENTION的新型机器人操作框架，通过结合视觉-语言模型的场景推理与交互驱动记忆，实现机器人在多样场景下的自主操控能力。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作方法高度依赖精确的物理建模和预设动作序列，虽然在结构化环境下效果良好，但在真实世界场景中容易因模型不准确而失效，也难以推广到全新任务。相比之下，人类能凭直觉处理复杂环境，展现出高度适应性和决策效率。

Method: 提出了INTENTION框架，将视觉-语言模型（VLM）进行场景推理与交互式记忆相结合。具体方法包括：设计Memory Graph结构，记录以往任务中的场景与交互，模拟人类对任务的理解与决策；开发Intuitive Perceptor，从视觉场景中提取物理关系及可操作性（affordance）。

Result: INTENTION框架使机器人在无须重复指令的情况下，能在新场景中自主推理并采取合适的交互行为，展现出更强的泛化能力和操作灵活性。

Conclusion: 结合视觉-语言推理与交互记忆能显著提升机器人在真实世界复杂任务中的自主操作能力，其人类直觉式的处理与决策方式为机器人通用智能发展提供了新思路。

Abstract: Traditional control and planning for robotic manipulation heavily rely on
precise physical models and predefined action sequences. While effective in
structured environments, such approaches often fail in real-world scenarios due
to modeling inaccuracies and struggle to generalize to novel tasks. In
contrast, humans intuitively interact with their surroundings, demonstrating
remarkable adaptability, making efficient decisions through implicit physical
understanding. In this work, we propose INTENTION, a novel framework enabling
robots with learned interactive intuition and autonomous manipulation in
diverse scenarios, by integrating Vision-Language Models (VLMs) based scene
reasoning with interaction-driven memory. We introduce Memory Graph to record
scenes from previous task interactions which embodies human-like understanding
and decision-making about different tasks in real world. Meanwhile, we design
an Intuitive Perceptor that extracts physical relations and affordances from
visual scenes. Together, these components empower robots to infer appropriate
interaction behaviors in new scenes without relying on repetitive instructions.
Videos: https://robo-intention.github.io

</details>


### [145] [Optimal Planning for Multi-Robot Simultaneous Area and Line Coverage Using Hierarchical Cyclic Merging Regulation](https://arxiv.org/abs/2508.04981)
*Tianyuan Zheng,Jingang Yi,Kaiyan Yu*

Main category: cs.RO

TL;DR: 本文提出了一种针对多机器人同时进行线性特征覆盖和区域探索的优化路径规划算法（HCMR），有效提升了路径规划效率、降低了任务耗时，并实现了无冲突作业。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统常用于表面裂缝检测、道路巡检等任务，需同时实现对线性特征和整个区域的覆盖。由于服务任务时间成本高、足迹小，如何在已知环境下高效协调多机器人无冲突且高效地完成双重覆盖任务具有挑战性。

Method: 提出了基于分层循环合并调控（HCMR）的最优路径规划算法。方法包括在图遍历中利用Morse理论分析流形连接过程，借助循环合并搜索和边序列反向传播调控机器人路径。此外结合平衡分区，选取最优序列为每个机器人生成具体路径。

Result: 仿真多机器人任务，HCMR算法在确保无冲突的情况下，相较先进规划算法，提升路径规划效率不少于10%，整体任务时间降低至少16.9%。

Conclusion: HCMR算法在固定清扫方向下能实现最优路径分配，大幅优于现有多机器人路径规划技术，并有效确保机器人无碰撞、高效作业。

Abstract: The double coverage problem focuses on determining efficient, collision-free
routes for multiple robots to simultaneously cover linear features (e.g.,
surface cracks or road routes) and survey areas (e.g., parking lots or local
regions) in known environments. In these problems, each robot carries two
functional roles: service (linear feature footprint coverage) and exploration
(complete area coverage). Service has a smaller operational footprint but
incurs higher costs (e.g., time) compared to exploration. We present optimal
planning algorithms for the double coverage problems using hierarchical cyclic
merging regulation (HCMR). To reduce the complexity for optimal planning
solutions, we analyze the manifold attachment process during graph traversal
from a Morse theory perspective. We show that solutions satisfying minimum path
length and collision-free constraints must belong to a Morse-bounded
collection. To identify this collection, we introduce the HCMR algorithm. In
HCMR, cyclic merging search regulates traversal behavior, while edge sequence
back propagation converts these regulations into graph edge traversal
sequences. Incorporating balanced partitioning, the optimal sequence is
selected to generate routes for each robot. We prove the optimality of the HCMR
algorithm under a fixed sweep direction. The multi-robot simulation results
demonstrate that the HCMR algorithm significantly improves planned path length
by at least 10.0%, reduces task time by at least 16.9% in average, and ensures
conflict-free operation compared to other state-of-the-art planning methods.

</details>


### [146] [Hierarchical Deep Deterministic Policy Gradient for Autonomous Maze Navigation of Mobile Robots](https://arxiv.org/abs/2508.04994)
*Wenjie Hu,Ye Zhou,Hann Woei Ho*

Main category: cs.RO

TL;DR: 本文提出了一种高效的分层深度确定性策略梯度（HDDPG）算法，通过高低层协同提升机器人在迷宫导航任务中的表现，显著优于传统DDPG和其变体。


<details>
  <summary>Details</summary>
Motivation: 传统的DDPG算法在迷宫导航任务中受限于稀疏奖励、低效探索和长时规划能力不足，导致导航成功率低、平均奖励差，甚至无法有效完成导航。亟需通过改进探索能力和分层决策机制解决上述难题。

Method: 提出分层DDPG框架，包括高层策略（产生子目标、长远决策）和低层策略（执行原始动作、跟随子目标），均采用改进的DDPG算法。引入off-policy修正、经验重标、参数噪声、自适应奖励函数、梯度裁剪与Xavier初始化，增强探索、学习效率和算法鲁棒性。

Result: 在ROS和Gazebo平台上的自主迷宫导航仿真实验显示：与标准DDPG及其变体相比，该算法在三种目标场景中，成功率提升至少56.59%，平均奖励提升519.03分以上。

Conclusion: 分层DDPG在迷宫导航任务上大幅优于标准DDPG，方法能有效解决稀疏奖励和复杂长时导航的问题，为智能机器人高效自主导航提供了有力方案。

Abstract: Maze navigation is a fundamental challenge in robotics, requiring agents to
traverse complex environments efficiently. While the Deep Deterministic Policy
Gradient (DDPG) algorithm excels in control tasks, its performance in maze
navigation suffers from sparse rewards, inefficient exploration, and
long-horizon planning difficulties, often leading to low success rates and
average rewards, sometimes even failing to achieve effective navigation. To
address these limitations, this paper proposes an efficient Hierarchical DDPG
(HDDPG) algorithm, which includes high-level and low-level policies. The
high-level policy employs an advanced DDPG framework to generate intermediate
subgoals from a long-term perspective and on a higher temporal scale. The
low-level policy, also powered by the improved DDPG algorithm, generates
primitive actions by observing current states and following the subgoal
assigned by the high-level policy. The proposed method enhances stability with
off-policy correction, refining subgoal assignments by relabeling historical
experiences. Additionally, adaptive parameter space noise is utilized to
improve exploration, and a reshaped intrinsic-extrinsic reward function is
employed to boost learning efficiency. Further optimizations, including
gradient clipping and Xavier initialization, are employed to improve
robustness. The proposed algorithm is rigorously evaluated through numerical
simulation experiments executed using the Robot Operating System (ROS) and
Gazebo. Regarding the three distinct final targets in autonomous maze
navigation tasks, HDDPG significantly overcomes the limitations of standard
DDPG and its variants, improving the success rate by at least 56.59% and
boosting the average reward by a minimum of 519.03 compared to baseline
algorithms.

</details>


### [147] [MAG-Nav: Language-Driven Object Navigation Leveraging Memory-Reserved Active Grounding](https://arxiv.org/abs/2508.05021)
*Weifan Zhang,Tingguang Li,Yuzhen Liu*

Main category: cs.RO

TL;DR: 本研究提出了一种基于视觉语言模型（VLM）的导航框架，通过主动感知和记忆机制，使机器人能够仅凭自然语言描述在未知环境中实现高效导航。


<details>
  <summary>Details</summary>
Motivation: 当前智能机器人在未知环境中依赖自然语言进行导航时，常常仅被动接受视觉输入，缺乏主动优化感知和利用历史观测的能力，导致在复杂场景下的语言-视觉对齐和导航表现有限。

Method: 该方法基于现成的视觉语言模型（VLM），并结合两个人类启发机制：一是基于视角的主动对齐，动态调整机器人视角以获取更好的视觉信息；二是历史记忆回溯，令系统能够保留并动态重评不确定的观测。整个流程无需数据标注或模型微调，实现零样本泛化。

Result: 在Habitat-Matterport 3D（HM3D）数据集上的实验表明，新框架在语言驱动的目标导航任务上超越了当前主流方法。此外，该方法还成功应用于真实四足机器人实测，表现出强健且高效的导航能力。

Conclusion: 论文表明，结合主动感知和历史记忆机制的视觉语言模型能够大幅提升机器人在复杂未知环境下基于自然语言的导航能力，且具备良好的泛化性与实际应用价值。

Abstract: Visual navigation in unknown environments based solely on natural language
descriptions is a key capability for intelligent robots. In this work, we
propose a navigation framework built upon off-the-shelf Visual Language Models
(VLMs), enhanced with two human-inspired mechanisms: perspective-based active
grounding, which dynamically adjusts the robot's viewpoint for improved visual
inspection, and historical memory backtracking, which enables the system to
retain and re-evaluate uncertain observations over time. Unlike existing
approaches that passively rely on incidental visual inputs, our method actively
optimizes perception and leverages memory to resolve ambiguity, significantly
improving vision-language grounding in complex, unseen environments. Our
framework operates in a zero-shot manner, achieving strong generalization to
diverse and open-ended language descriptions without requiring labeled data or
model fine-tuning. Experimental results on Habitat-Matterport 3D (HM3D) show
that our method outperforms state-of-the-art approaches in language-driven
object navigation. We further demonstrate its practicality through real-world
deployment on a quadruped robot, achieving robust and effective navigation
performance.

</details>


### [148] [Benchmarking Shortcutting Techniques for Multi-Robot-Arm Motion Planning](https://arxiv.org/abs/2508.05027)
*Philip Huang,Yorai Shaoul,Jiaoyang Li*

Main category: cs.RO

TL;DR: 论文对多机械臂运动轨迹中的常用后处理优化方法进行了全面、定量的对比研究，并提出了两种更有效的组合策略。


<details>
  <summary>Details</summary>
Motivation: 多机械臂系统下，传统轨迹规划方法难以兼顾高维空间的复杂性和机械臂间的避障，又往往生成不够平滑、耗时较长的轨迹，现有的轨迹优化手段在多臂避障上的实际效果也缺乏系统量化分析。

Method: 作者系统评测了多种现有的轨迹捷径（shortcutting）方法在典型多臂任务仿真中的表现，分析优缺点，并提出了两种组合策略以兼顾最优性能与计算效率。

Result: 实验结果表明，不同捷径方法针对不同场景各有优势，提出的组合策略在保持高效的同时可生成更优质的运动轨迹。

Conclusion: 论文为多机械臂系统中轨迹优化方法的选择与实际应用提供了定量参考，组合捷径策略兼具执行效率和轨迹质量，有利于提升多机器人协作系统的整体表现。

Abstract: Generating high-quality motion plans for multiple robot arms is challenging
due to the high dimensionality of the system and the potential for inter-arm
collisions. Traditional motion planning methods often produce motions that are
suboptimal in terms of smoothness and execution time for multi-arm systems.
Post-processing via shortcutting is a common approach to improve motion quality
for efficient and smooth execution. However, in multi-arm scenarios, optimizing
one arm's motion must not introduce collisions with other arms. Although
existing multi-arm planning works often use some form of shortcutting
techniques, their exact methodology and impact on performance are often vaguely
described. In this work, we present a comprehensive study quantitatively
comparing existing shortcutting methods for multi-arm trajectories across
diverse simulated scenarios. We carefully analyze the pros and cons of each
shortcutting method and propose two simple strategies for combining these
methods to achieve the best performance-runtime tradeoff. Video, code, and
dataset are available at https://philip-huang.github.io/mr-shortcut/.

</details>


### [149] [A Vision-Based Collision Sensing Method for Stable Circular Object Grasping with A Soft Gripper System](https://arxiv.org/abs/2508.05040)
*Boyang Zhang,Jiahui Zuo,Zeyu Duan,Fumin Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种基于视觉的传感模块，可用于检测并应对软体抓手在抓取环状物体时遭遇的外部碰撞，提高抓取的稳定性和安全性。通过实验验证了系统的快速响应和对碰撞方向与强度的准确检测能力。


<details>
  <summary>Details</summary>
Motivation: 机械臂在实际应用中常遭遇外部碰撞，尤其在抓取环状或圆形物体时易导致抓取失败或物体损坏，因此亟需提升碰撞检测与响应能力以保证抓取过程的稳定性和安全性。

Method: 该系统通过在软体抓手上集成拥有大视野的“掌内相机”，实时监控手指与被抓物体状态，并结合碰撞丰富的抓取策略实现对碰撞的探测和快速响应。利用实体软体抓手在协作机器人上进行实验，检测碰撞后的响应时间及碰撞方向和强度识别能力。

Result: 实验结果显示，系统能够瞬时识别碰撞并作出响应，并且能够准确检测外部碰撞的方向和强度，确保抓取过程的稳定性。

Conclusion: 实验证实该基于视觉的碰撞检测方案有效提升了软体抓手在动态抓取过程中应对外部冲击的能力，为机器人安全与稳定抓取提供了新的解决思路。

Abstract: External collisions to robot actuators typically pose risks to grasping
circular objects. This work presents a vision-based sensing module capable of
detecting collisions to maintain stable grasping with a soft gripper system.
The system employs an eye-in-palm camera with a broad field of view to
simultaneously monitor the motion of fingers and the grasped object.
Furthermore, we have developed a collision-rich grasping strategy to ensure the
stability and security of the entire dynamic grasping process. A physical soft
gripper was manufactured and affixed to a collaborative robotic arm to evaluate
the performance of the collision detection mechanism. An experiment regarding
testing the response time of the mechanism confirmed the system has the
capability to react to the collision instantaneously. A dodging test was
conducted to demonstrate the gripper can detect the direction and scale of
external collisions precisely.

</details>


### [150] [Examining the legibility of humanoid robot arm movements in a pointing task](https://arxiv.org/abs/2508.05104)
*Andrej Lúčny,Matilde Antonj,Carlo Mazzola,Hana Hornáčková,Ana Farić,Kristína Malinovská,Michal Vavrecka,Igor Farkaš*

Main category: cs.RO

TL;DR: 本研究通过实验分析了人类如何根据仿人机器人部分完成的指向动作和身体线索预测其意图，结果支持多模态优势和眼动主导的假设。


<details>
  <summary>Details</summary>
Motivation: 在人机交互过程中，机器人动作的可辨识性（legibility）对人类的理解与安全感至关重要。因此需要研究哪些动作和线索能够帮助人类更好地预测机器人行为。

Method: 设计了以NICO仿人机器人为主体的实验，让参与者观察机器人手臂指向触摸屏目标的动作。机器人提供了不同的视觉线索（凝视、指向、指向+一致/不一致的凝视），手臂动作被提前终止（行进至60%或80%），由被试预测最终目标。

Result: 实验支持了多模态优势（multimodal superiority）和眼动主导（ocular primacy）假说，即通过多种感知线索以及眼动线索能更好地传达机器人意图。

Conclusion: 多种身体及感知线索的结合有助于提升机器人动作的可辨识性，对于设计更易于理解和协作的人机交互系统具有指导意义。

Abstract: Human--robot interaction requires robots whose actions are legible, allowing
humans to interpret, predict, and feel safe around them. This study
investigates the legibility of humanoid robot arm movements in a pointing task,
aiming to understand how humans predict robot intentions from truncated
movements and bodily cues. We designed an experiment using the NICO humanoid
robot, where participants observed its arm movements towards targets on a
touchscreen. Robot cues varied across conditions: gaze, pointing, and pointing
with congruent or incongruent gaze. Arm trajectories were stopped at 60\% or
80\% of their full length, and participants predicted the final target. We
tested the multimodal superiority and ocular primacy hypotheses, both of which
were supported by the experiment.

</details>


### [151] [From Canada to Japan: How 10,000 km Affect User Perception in Robot Teleoperation](https://arxiv.org/abs/2508.05143)
*Siméon Capy,Thomas M. Kwok,Kevin Joseph,Yuichiro Kawasumi,Koichi Nagashima,Tomoya Sasaki,Yue Hu,Eiichi Yoshida*

Main category: cs.RO

TL;DR: 本文研究了在机器人远程操作（RTo）中，用户对长距离操作感知的变化，并比较了本地和远程操作对用户感知的影响。结果发现两者感知无显著差异，表明远程机器人可作为本地操作的可行替代。


<details>
  <summary>Details</summary>
Motivation: 随着机器人在远程医疗和老年人照护等领域应用增多，远程操作为弥补本地操作不足、提升安全性和覆盖范围提供了新途径。本文动机在于探讨远距离操作是否影响用户对机器人的感知，验证其在敏感应用场景（如老年人照护）的可行性。

Method: 作者设计了一套包括多份问卷的评估协议，以及基于ROS和Unity开发的软件架构，对非专业用户进行本地和远程机器人操作体验对比，记录其操作前后的感知变化。

Result: 实验结果表明，在本地与远程操作条件下，用户的感知无统计学显著差异。

Conclusion: 机器人远程操作在用户感知上与本地操作类似，对用户体验影响不大，因此在需要远程介入的场景下，远程操作机器人可以作为本地操作的可行替代方案。

Abstract: Robot teleoperation (RTo) has emerged as a viable alternative to local
control, particularly when human intervention is still necessary. This research
aims to study the distance effect on user perception in RTo, exploring the
potential of teleoperated robots for older adult care. We propose an evaluation
of non-expert users' perception of long-distance RTo, examining how their
perception changes before and after interaction, as well as comparing it to
that of locally operated robots. We have designed a specific protocol
consisting of multiple questionnaires, along with a dedicated software
architecture using the Robotics Operating System (ROS) and Unity. The results
revealed no statistically significant differences between the local and remote
robot conditions, suggesting that robots may be a viable alternative to
traditional local control.

</details>


### [152] [Chemist Eye: A Visual Language Model-Powered System for Safety Monitoring and Robot Decision-Making in Self-Driving Laboratories](https://arxiv.org/abs/2508.05148)
*Francisco Munguia-Galeano,Zhengxue Zhou,Satheeshkumar Veeramani,Hatem Fakhruldeen,Louis Longley,Rob Clowes,Andrew I. Cooper*

Main category: cs.RO

TL;DR: 本文提出了一种名为Chemist Eye的分布式安全监测系统，专为自驾实验室（SDLs）设计，可实时识别安全隐患、PPE合规和火灾风险，通过多种相机监控和视觉语言模型辅助决策，支持即时预警和与机器人互动，实际测试表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着机器人和自动化技术融入自驾实验室，实验室面临新的复杂安全挑战，特别是在涉及移动机器人和易燃锂电池时。当前安全手段难以满足SDLs的新需求，因此需开发更智能和集成度更高的监控系统来保障人员与设备安全。

Method: 设计并实现了Chemist Eye系统，该系统采用分布多站点布置，配备RGB、深度及红外相机。利用视觉-语言模型（VLM）分析摄像头数据，实时检测实验室内的安全事件、工作人员PPE佩戴情况及火灾隐患。系统可与机器人实时通讯，根据VLM建议采取避险行动，并通过消息平台及时通知实验室人员。

Result: 在一个配备三台移动机器人的自驾实验室中真实数据测试，Chemist Eye在识别潜在安全隐患和辅助决策上的准确率分别达到97%和95%。

Conclusion: Chemist Eye系统显著提升了智能实验室的安全水平，可高效检测并预防多种安全事故，同时与机器人和通讯平台的无缝集成进一步增强了其实用性和响应速度。

Abstract: The integration of robotics and automation into self-driving laboratories
(SDLs) can introduce additional safety complexities, in addition to those that
already apply to conventional research laboratories. Personal protective
equipment (PPE) is an essential requirement for ensuring the safety and
well-being of workers in laboratories, self-driving or otherwise. Fires are
another important risk factor in chemical laboratories. In SDLs, fires that
occur close to mobile robots, which use flammable lithium batteries, could have
increased severity. Here, we present Chemist Eye, a distributed safety
monitoring system designed to enhance situational awareness in SDLs. The system
integrates multiple stations equipped with RGB, depth, and infrared cameras,
designed to monitor incidents in SDLs. Chemist Eye is also designed to spot
workers who have suffered a potential accident or medical emergency, PPE
compliance and fire hazards. To do this, Chemist Eye uses decision-making
driven by a vision-language model (VLM). Chemist Eye is designed for seamless
integration, enabling real-time communication with robots. Based on the VLM
recommendations, the system attempts to drive mobile robots away from potential
fire locations, exits, or individuals not wearing PPE, and issues audible
warnings where necessary. It also integrates with third-party messaging
platforms to provide instant notifications to lab personnel. We tested Chemist
Eye with real-world data from an SDL equipped with three mobile robots and
found that the spotting of possible safety hazards and decision-making
performances reached 97 % and 95 %, respectively.

</details>


### [153] [FCBV-Net: Category-Level Robotic Garment Smoothing via Feature-Conditioned Bimanual Value Prediction](https://arxiv.org/abs/2508.05153)
*Mohammed Daba,Jing Qiu*

Main category: cs.RO

TL;DR: 本文提出了一种新的方法（FCBV-Net），能使机器人在处理新类别衣物时实现更好的泛化性，尤其是在双手衣物抚平任务中，实验显示该方法显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前机器人衣物操作（如双手衣物抚平）受高维度、动态复杂性和类别内部多样性的限制，现有方法容易过拟合或无法准确估计双手协同动作的价值，难以在新衣物类别上泛化。

Method: 作者提出Feature-Conditioned Bimanual Value Network（FCBV-Net），利用预训练且冻结的稠密几何特征作为输入，增强对衣物类别内部差异的鲁棒性。只在下游任务策略学习中进行训练，从而将几何理解与动作价值学习解耦。该方法主要在3D点云上运行。

Result: 在CLOTH3D数据集模拟实验中，FCBV-Net在未见过的新衣物上只表现出11.5%的效率下降，而基于2D图像的方法下降达96.2%；最终覆盖率达89%，也超过了采用相同几何特征但固定动作的3D基线（83%）。

Conclusion: 通过将几何特征与动作价值学习分离，FCBV-Net能大幅提升机器人在于衣物类别泛化上的表现，优于以往的基线方法。

Abstract: Category-level generalization for robotic garment manipulation, such as
bimanual smoothing, remains a significant hurdle due to high dimensionality,
complex dynamics, and intra-category variations. Current approaches often
struggle, either overfitting with concurrently learned visual features for a
specific instance or, despite category-level perceptual generalization, failing
to predict the value of synergistic bimanual actions. We propose the
Feature-Conditioned Bimanual Value Network (FCBV-Net), operating on 3D point
clouds to specifically enhance category-level policy generalization for garment
smoothing. FCBV-Net conditions bimanual action value prediction on pre-trained,
frozen dense geometric features, ensuring robustness to intra-category garment
variations. Trainable downstream components then learn a task-specific policy
using these static features. In simulated GarmentLab experiments with the
CLOTH3D dataset, FCBV-Net demonstrated superior category-level generalization.
It exhibited only an 11.5% efficiency drop (Steps80) on unseen garments
compared to 96.2% for a 2D image-based baseline, and achieved 89% final
coverage, outperforming an 83% coverage from a 3D correspondence-based baseline
that uses identical per-point geometric features but a fixed primitive. These
results highlight that the decoupling of geometric understanding from bimanual
action value learning enables better category-level generalization.

</details>


### [154] [Learning to See and Act: Task-Aware View Planning for Robotic Manipulation](https://arxiv.org/abs/2508.05186)
*Yongjie Bai,Zhouxia Wang,Yang Liu,Weixing Chen,Ziliang Chen,Mingtong Dai,Yongsen Zheng,Lingbo Liu,Guanbin Li,Liang Lin*

Main category: cs.RO

TL;DR: 本文提出了一个用于多任务机器人操作的新框架——任务感知视角规划（TAVP），通过主动选择视角和任务特定的视觉表示提升任务泛化与鲁棒性，在多个操纵任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作（VLA）模型通常依赖固定视角与共享视觉编码器，导致机器人在3D感知受限、任务干扰严重，影响模型在复杂多任务场景下的泛化与鲁棒性。

Method: 提出了TAVP框架：通过主动视角规划策略（由创新的伪环境加速），为不同任务动态获取最有信息量的视觉观察。同时，融入Mixture-of-Experts视觉编码器，将不同任务特征解耦，提高表示质量与泛化能力。

Result: 在RLBench多项机器人操作任务中，TAVP相较于最优的固定视角方法，展现出显著更高的动作预测准确率与泛化能力，实验充分验证了方法有效性。

Conclusion: TAVP通过任务感知的主动视角获得与特征解耦，有效提升了多任务机器人的3D感知能力与鲁棒性，对多任务视觉操控领域具有推动作用。

Abstract: Recent vision-language-action (VLA) models for multi-task robotic
manipulation commonly rely on static viewpoints and shared visual encoders,
which limit 3D perception and cause task interference, hindering robustness and
generalization. In this work, we propose Task-Aware View Planning (TAVP), a
framework designed to overcome these challenges by integrating active view
planning with task-specific representation learning. TAVP employs an efficient
exploration policy, accelerated by a novel pseudo-environment, to actively
acquire informative views. Furthermore, we introduce a Mixture-of-Experts (MoE)
visual encoder to disentangle features across different tasks, boosting both
representation fidelity and task generalization. By learning to see the world
in a task-aware way, TAVP generates more complete and discriminative visual
representations, demonstrating significantly enhanced action prediction across
a wide array of manipulation challenges. Extensive experiments on RLBench tasks
show that our proposed TAVP model achieves superior performance over
state-of-the-art fixed-view approaches. Visual results and code are provided
at: https://hcplab-sysu.github.io/TAVP.

</details>


### [155] [Dancing with a Robot: An Experimental Study of Child-Robot Interaction in a Performative Art Setting](https://arxiv.org/abs/2508.05208)
*Victor Ngo,Rachel,Ramchurn,Roma Patel,Alan Chamberlain,Ayse Kucukyilmaz*

Main category: cs.RO

TL;DR: 本文评估了18名儿童与自主机器人手臂表演者NED在艺术装置中的真实交互体验。通过观察发现儿童乐于互动，但也存在保持参与、机器人表达力不足和期望落空三大挑战。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究儿童与机器人手臂表演者在艺术装置中的交互体验，并找出提升这类人机交互系统的关键要素，以增强艺术表演中的互动性和吸引力。

Method: 作者对NED的设计（包括服装、行为和与人的互动）进行了详细描述，并通过实地观察18名儿童与机器人在展览中的实际互动进行分析。

Result: 结果表明，儿童对机器人表演者充满好奇并善于互动。但在实际交互中存在三个主要挑战：1）如何激发和维持儿童的参与；2）机器人缺乏足够的表达性和互动性；3）儿童对机器人表演的实际体验未达到预期。

Conclusion: 研究强调，为了让年轻观众获得有意义且富有吸引力的体验，应在机器人表演艺术交互设计中充分考虑观众的能力、认知和预期，优化人机交互系统。

Abstract: This paper presents an evaluation of 18 children's in-the-wild experiences
with the autonomous robot arm performer NED (Never-Ending Dancer) within the
Thingamabobas installation, showcased across the UK. We detail NED's design,
including costume, behaviour, and human interactions, all integral to the
installation. Our observational analysis revealed three key challenges in
child-robot interactions: 1) Initiating and maintaining engagement, 2) Lack of
robot expressivity and reciprocity, and 3) Unmet expectations. Our findings
show that children are naturally curious, and adept at interacting with a
robotic art performer. However, our observations emphasise the critical need to
optimise human-robot interaction (HRI) systems through careful consideration of
audience's capabilities, perceptions, and expectations, within the performative
arts context, to enable engaging and meaningful experiences, especially for
young audiences.

</details>


### [156] [Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction](https://arxiv.org/abs/2508.05294)
*Sahar Salimpour,Lei Fu,Farhad Keramat,Leonardo Militano,Giovanni Toffetti,Harry Edelman,Jorge Peña Queralta*

Main category: cs.RO

TL;DR: 本文综述了基础模型（如大语言模型和视觉-语言模型）在推动机器人自主性和人机交互方面的最新进展，重点关注了朝向智能体（agentic）架构演化的相关工作，包括技术分类和行业趋势分析。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型强大能力的不断涌现，它们为机器人带来了全新的认知和决策方式，但学界与工业中的相关进展非常迅速，现有文献亟需一次系统总结和分类，为研究者与开发者提供参考。

Method: 作者对最新同行评审论文、社区项目、ROS包及工业框架进行了调研和整理，提出了针对模型集成方式的分类方法，并对不同智能体架构在各方案中的作用做了对比分析。

Result: 文中总结了基础模型驱动下各类机器人智能体应用的最新进展，梳理了相关模型与架构的分类标准，分析了智能体在工具接口、任务规划、感知与操控等具体应用中的关键角色。

Conclusion: 基础模型为机器人赋能已成为趋势，智能体架构能有效提升机器人理解、规划与互动能力。该领域正快速发展，社区和产业的创新频繁，本文提出的分类体系和对前沿方案的对比分析为后续研究及实际部署提供了重要参考。

Abstract: Foundation models, including large language models (LLMs) and vision-language
models (VLMs), have recently enabled novel approaches to robot autonomy and
human-robot interfaces. In parallel, vision-language-action models (VLAs) or
large behavior models (BLMs) are increasing the dexterity and capabilities of
robotic systems. This survey paper focuses on those words advancing towards
agentic applications and architectures. This includes initial efforts exploring
GPT-style interfaces to tooling, as well as more complex system where AI agents
are coordinators, planners, perception actors, or generalist interfaces. Such
agentic architectures allow robots to reason over natural language
instructions, invoke APIs, plan task sequences, or assist in operations and
diagnostics. In addition to peer-reviewed research, due to the fast-evolving
nature of the field, we highlight and include community-driven projects, ROS
packages, and industrial frameworks that show emerging trends. We propose a
taxonomy for classifying model integration approaches and present a comparative
analysis of the role that agents play in different solutions in today's
literature.

</details>


### [157] [GhostShell: Streaming LLM Function Calls for Concurrent Embodied Programming](https://arxiv.org/abs/2508.05298)
*Jian Gong,Youwei Huang,Bo Yuan,Ming Zhu,Juncheng Zhan,Jinke Wang,Hang Shu,Mingyue Xiong,Yanjun Ye,Yufan Zu,Yang Zhou,Yihan Ding,Xuannian Chen,Xingyu Lu,Runjie Ban,Bingchao Huang,Fusen Liu*

Main category: cs.RO

TL;DR: GhostShell是一种结合大语言模型（LLM）以实现流式并发行为编程的机器人系统框架，支持多通道并行调度和接口映射，显著提升机器人任务的响应速度和行为正确性。


<details>
  <summary>Details</summary>
Motivation: 当前机器人行为编程常依赖预设的行为树或动作序列，缺乏灵活性和实时性，难以适应多变且复杂的实际场景。作者希望借助LLM的生成能力，实现机器人系统行为的动态流式编排，提高机器人在真实任务中的表现和泛化能力。

Method: GhostShell利用LLM按令牌流式输出行为，通过XML解析和动态函数映射，将LLM的指令实时转换为具体的机器功能调用，并设计了多通道调度器，实现不同组件间串行或并行的行为协调。实验部分以COCO机器人为平台，覆盖34项现实交互任务，并测试多个LLM模型，量化了行为正确率与响应速度。

Result: GhostShell在B​​ehavioral Correctness Metric上，使用Claude-4 Sonnet模型达到0.85的业界领先表现，响应速度比传统LLM函数调用接口快了最多66倍。同时，该方法在复杂、长时段多模态任务中表现出较强的鲁棒性和泛化能力。

Conclusion: GhostShell有效提升了机器人系统的行为灵活性、并发效率及正确率，是实现复杂 embodied 系统实时、多任务控制的有力工具，在未来多机器人协作与多模态任务控制场景下具有广阔应用前景。

Abstract: We present GhostShell, a novel approach that leverages Large Language Models
(LLMs) to enable streaming and concurrent behavioral programming for embodied
systems. In contrast to conventional methods that rely on pre-scheduled action
sequences or behavior trees, GhostShell drives embodied systems to act
on-the-fly by issuing function calls incrementally as tokens are streamed from
the LLM. GhostShell features a streaming XML function token parser, a dynamic
function interface mapper, and a multi-channel scheduler that orchestrates
intra-channel synchronous and inter-channel asynchronous function calls,
thereby coordinating serial-parallel embodied actions across multiple robotic
components as directed by the LLM. We evaluate GhostShell on our robot
prototype COCO through comprehensive grounded experiments across 34 real-world
interaction tasks and multiple LLMs. The results demonstrate that our approach
achieves state-of-the-art Behavioral Correctness Metric of 0.85 with Claude-4
Sonnet and up to 66X faster response times compared to LLM native function
calling APIs. GhostShell also proves effective in long-horizon multimodal
tasks, demonstrating strong robustness and generalization.

</details>


### [158] [Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control](https://arxiv.org/abs/2508.05342)
*Shunlei Li,Longsen Gao,Jin Wang,Chang Che,Xi Xiao,Jiuwen Cao,Yingbai Hu,Hamid Reza Karimi*

Main category: cs.RO

TL;DR: 本文提出了一种新方法，使机器人能够从人类视频中直接学习和执行灵巧的双臂操作任务，具有很强的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖于底层轨迹模仿，难以适应不同物体类型、空间布局和机械臂结构，导致泛化能力不足。

Method: 提出Graph-Fused Vision-Language-Action（GF-VLA）框架：先通过信息量提取相关的手和物体，构建时序场景图，捕捉交互关系，并融合语言条件变换器生成层级行为树及可解释运动指令；同时通过双手选择策略提升执行效率。

Result: 在双臂积木组装等四项任务中，场景图准确率超95%，子任务分割93%，机器人执行抓取成功率94%、放置准确率89%、总体任务成功率90%，表现出优秀的泛化和稳健性。

Conclusion: GF-VLA框架能高效泛化，不依赖低级动作模仿，在多样空间和语义变化中依然可靠，推动了机器人灵巧操作技能的进步。

Abstract: Teaching robots dexterous skills from human videos remains challenging due to
the reliance on low-level trajectory imitation, which fails to generalize
across object types, spatial layouts, and manipulator configurations. We
propose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables
dual-arm robotic systems to perform task-level reasoning and execution directly
from RGB and Depth human demonstrations. GF-VLA first extracts
Shannon-information-based cues to identify hands and objects with the highest
task relevance, then encodes these cues into temporally ordered scene graphs
that capture both hand-object and object-object interactions. These graphs are
fused with a language-conditioned transformer that generates hierarchical
behavior trees and interpretable Cartesian motion commands. To improve
execution efficiency in bimanual settings, we further introduce a cross-hand
selection policy that infers optimal gripper assignment without explicit
geometric reasoning. We evaluate GF-VLA on four structured dual-arm block
assembly tasks involving symbolic shape construction and spatial
generalization. Experimental results show that the information-theoretic scene
representation achieves over 95 percent graph accuracy and 93 percent subtask
segmentation, supporting the LLM planner in generating reliable and
human-readable task policies. When executed by the dual-arm robot, these
policies yield 94 percent grasp success, 89 percent placement accuracy, and 90
percent overall task success across stacking, letter-building, and geometric
reconfiguration scenarios, demonstrating strong generalization and robustness
across diverse spatial and semantic variations.

</details>


### [159] [Affecta-Context: The Context-Guided Behavior Adaptation Framework](https://arxiv.org/abs/2508.05359)
*Morten Roed Frederiksen,Kasper Støy*

Main category: cs.RO

TL;DR: 本文提出了Affecta-context框架，使社交机器人能根据物理环境灵活调整行为，并通过人机交互学习行为优先级。实验证明其能泛化并应对新环境。


<details>
  <summary>Details</summary>
Motivation: 当前社交机器人在多变物理环境下行为调整能力有限，且难以根据用户及环境偏好自适应行为，缺乏泛化能力。作者希望解决机器人行为适应和优先级学习难题。

Method: Affecta-context框架包含：1）对物理环境进行特征聚类表示；2）通过与用户的互动在不同环境下学习行为优先级。实验中，在两种物理环境下、6名用户共72次交互中训练机器人，检测其在新环境中的泛化能力。

Result: 实验结果表明，Affecta-context能让机器人自主学会在不同物理环境下的行为优先级，并能将所学推广到未见过的新环境。

Conclusion: Affecta-context为社交机器人在复杂与变化物理环境中自适应和泛化行为提供了一种有效的通用框架，增进了机器人的人机交互适应性。

Abstract: This paper presents Affecta-context, a general framework to facilitate
behavior adaptation for social robots. The framework uses information about the
physical context to guide its behaviors in human-robot interactions. It
consists of two parts: one that represents encountered contexts and one that
learns to prioritize between behaviors through human-robot interactions. As
physical contexts are encountered the framework clusters them by their measured
physical properties. In each context, the framework learns to prioritize
between behaviors to optimize the physical attributes of the robot's behavior
in line with its current environment and the preferences of the users it
interacts with. This paper illlustrates the abilities of the Affecta-context
framework by enabling a robot to autonomously learn the prioritization of
discrete behaviors. This was achieved by training across 72 interactions in two
different physical contexts with 6 different human test participants. The paper
demonstrates the trained Affecta-context framework by verifying the robot's
ability to generalize over the input and to match its behaviors to a previously
unvisited physical context.

</details>


### [160] [A Multi-view Landmark Representation Approach with Application to GNSS-Visual-Inertial Odometry](https://arxiv.org/abs/2508.05368)
*Tong Hua,Jiale Han,Wei Ouyang*

Main category: cs.RO

TL;DR: 本文提出了一种高效的多视图仅位姿估计方法，并将其应用于GNSS-视觉-惯性里程计（GVIO），显著提升了IEKF在视觉辅助多传感器融合中的效率和精度。


<details>
  <summary>Details</summary>
Motivation: IEKF在视觉辅助传感器融合中效果显著，但当联合优化相机位姿与特征点时计算量大，影响其实用性与实时性，因此亟需提升IEKF在多传感器融合中的效率。

Method: 本文设计了一种仅位姿的视觉观测模型，可将特征点直接与多个相机位姿和观测关联，实现仅位姿的紧耦合测量，同时保持理想的零空间特性。该方法结合创新的特征管理策略，应用于基于滤波的GVIO系统中。

Result: 通过仿真和真实世界实验，验证了所提方法相比传统方法在效率和精度方面存在显著优势。

Conclusion: 提出的多视图仅位姿估计方法有效缓解了IEKF在多传感器融合时的高计算负担，提升了GVIO系统的效率和精度，具有较高的实际应用价值。

Abstract: Invariant Extended Kalman Filter (IEKF) has been a significant technique in
vision-aided sensor fusion. However, it usually suffers from high computational
burden when jointly optimizing camera poses and the landmarks. To improve its
efficiency and applicability for multi-sensor fusion, we present a multi-view
pose-only estimation approach with its application to GNSS-Visual-Inertial
Odometry (GVIO) in this paper. Our main contribution is deriving a visual
measurement model which directly associates landmark representation with
multiple camera poses and observations. Such a pose-only measurement is proven
to be tightly-coupled between landmarks and poses, and maintain a perfect null
space that is independent of estimated poses. Finally, we apply the proposed
approach to a filter based GVIO with a novel feature management strategy. Both
simulation tests and real-world experiments are conducted to demonstrate the
superiority of the proposed method in terms of efficiency and accuracy.

</details>


### [161] [Robots can defuse high-intensity conflict situations](https://arxiv.org/abs/2508.05373)
*Morten Roed Frederiksen,Kasper Støy*

Main category: cs.RO

TL;DR: 该论文研究了在人与机器人高强度对抗情境下，机器人如何通过情感表达来化解冲突。实验发现，无论采用哪种情感表达方式，冲突都能得到有效缓解，但针对当前情境的社会感知和反应更为重要。


<details>
  <summary>Details</summary>
Motivation: 随着机器人在生活中的普及，人与机器人之间的负面互动难以避免。理解如何化解此类冲突，有助于提升机器人在人类社会中的适应性和接受度。

Method: 作者设计了一种自定义情感机器人，并设置模拟的冲突情境，邀请105名受试者参与。机器人通过五种不同的情感表达方式（模态）来回应冲突，观察哪一类表达最有效于化解人的敌意。

Result: 所有的情感表达方式都在一定程度上有效缓解了对抗情境，参与者对不同表达方式给予了相似的正面评价，但运动模态与其他方式呈现出显著不同。受试者对机器人受到冲突影响的程度感知也非常接近。

Conclusion: 不同情感表达方式都有助于化解机器人和人的冲突，相比表达手段本身，机器人的社会感知能力和针对情境做出合适反应的能力更为关键。

Abstract: This paper investigates the specific scenario of high-intensity
confrontations between humans and robots, to understand how robots can defuse
the conflict. It focuses on the effectiveness of using five different affective
expression modalities as main drivers for defusing the conflict. The aim is to
discover any strengths or weaknesses in using each modality to mitigate the
hostility that people feel towards a poorly performing robot. The defusing of
the situation is accomplished by making the robot better at acknowledging the
conflict and by letting it express remorse. To facilitate the tests, we used a
custom affective robot in a simulated conflict situation with 105 test
participants. The results show that all tested expression modalities can
successfully be used to defuse the situation and convey an acknowledgment of
the confrontation. The ratings were remarkably similar, but the movement
modality was different (ANON p$<$.05) than the other modalities. The test
participants also had similar affective interpretations on how impacted the
robot was of the confrontation across all expression modalities. This indicates
that defusing a high-intensity interaction may not demand special attention to
the expression abilities of the robot, but rather require attention to the
abilities of being socially aware of the situation and reacting in accordance
with it.

</details>


### [162] [Real-Time Iteration Scheme for Diffusion Policy](https://arxiv.org/abs/2508.05396)
*Yufei Duan,Hang Yin,Danica Kragic*

Main category: cs.RO

TL;DR: 本文提出了一种借鉴最优控制领域的实时迭代（RTI）方案的新方法，以加速扩散策略（Diffusion Policy）在机器人操作任务中的推理过程，从而大幅减少推理时间且无需额外蒸馏或模型重设计。


<details>
  <summary>Details</summary>
Motivation: 传统的Diffusion Policy虽然在机器人控制领域表现出色，但推理延迟较高、需耗时多步去噪且执行chunk动作，占用周期长，不适合要求低延迟或简单快速循环任务；加速的既有方法如蒸馏等又需耗费大量再训练资源。

Method: 本文引入了借鉴于最优控制RTI方案的推理方法，即用前一步的解作为初值，减少去噪步骤，并设计了一种基于缩放的方法以有效支持机器人操作中的离散动作（如抓取）。不要求对原模型做蒸馏或结构修改，可直接集成于多种预训练大模型中。

Result: 实验表明，在仿真环境下该方法大幅降低了推理时间，并且在整体性能上与使用全流程去噪的传统Diffusion Policy基本持平。

Conclusion: 新方法显著降低了Diffusion Policy的推理延时和计算消耗，适合资源敏感与大模型场景，为扩散类决策模型实用化带来助力。

Abstract: Diffusion Policies have demonstrated impressive performance in robotic
manipulation tasks. However, their long inference time, resulting from an
extensive iterative denoising process, and the need to execute an action chunk
before the next prediction to maintain consistent actions limit their
applicability to latency-critical tasks or simple tasks with a short cycle
time. While recent methods explored distillation or alternative policy
structures to accelerate inference, these often demand additional training,
which can be resource-intensive for large robotic models. In this paper, we
introduce a novel approach inspired by the Real-Time Iteration (RTI) Scheme, a
method from optimal control that accelerates optimization by leveraging
solutions from previous time steps as initial guesses for subsequent
iterations. We explore the application of this scheme in diffusion inference
and propose a scaling-based method to effectively handle discrete actions, such
as grasping, in robotic manipulation. The proposed scheme significantly reduces
runtime computational costs without the need for distillation or policy
redesign. This enables a seamless integration into many pre-trained
diffusion-based models, in particular, to resource-demanding large models. We
also provide theoretical conditions for the contractivity which could be useful
for estimating the initial denoising step. Quantitative results from extensive
simulation experiments show a substantial reduction in inference time, with
comparable overall performance compared with Diffusion Policy using full-step
denoising. Our project page with additional resources is available at:
https://rti-dp.github.io/.

</details>


### [163] [DistillDrive: End-to-End Multi-Mode Autonomous Driving Distillation by Isomorphic Hetero-Source Planning Model](https://arxiv.org/abs/2508.05402)
*Rui Yu,Xianghang Zhang,Runkai Zhao,Huaicheng Yan,Meng Wang*

Main category: cs.RO

TL;DR: 提出了一种新的端到端自动驾驶模型DistillDrive，通过多样化知识蒸馏与生成建模提升自动驾驶鲁棒性和规划能力，实验上相较基础模型取得了优越表现。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法过分关注自车状态，缺乏以规划为导向的理解，导致决策鲁棒性不足。该研究旨在引入多目标、规划导向的学习目标，解决现有模型的局限性。

Method: 采用基于结构化场景表征的规划模型作为教师模型，对端到端模型进行多样化知识蒸馏，结合强化学习优化状态到决策的映射，并用生成建模制造规划相关的训练实例，增强模型对多模态运动特征的学习能力。

Result: 在nuScenes和NAVSIM数据集上，DistillDrive模型将碰撞率减少了50%，闭环性能提升3分，显著优于基线方法。

Conclusion: 通过结合规划模型知识蒸馏、生成建模与强化学习，可有效提升端到端自动驾驶模型的鲁棒性和规划能力，为自动驾驶决策系统提供了新思路。

Abstract: End-to-end autonomous driving has been recently seen rapid development,
exerting a profound influence on both industry and academia. However, the
existing work places excessive focus on ego-vehicle status as their sole
learning objectives and lacks of planning-oriented understanding, which limits
the robustness of the overall decision-making prcocess. In this work, we
introduce DistillDrive, an end-to-end knowledge distillation-based autonomous
driving model that leverages diversified instance imitation to enhance
multi-mode motion feature learning. Specifically, we employ a planning model
based on structured scene representations as the teacher model, leveraging its
diversified planning instances as multi-objective learning targets for the
end-to-end model. Moreover, we incorporate reinforcement learning to enhance
the optimization of state-to-decision mappings, while utilizing generative
modeling to construct planning-oriented instances, fostering intricate
interactions within the latent space. We validate our model on the nuScenes and
NAVSIM datasets, achieving a 50\% reduction in collision rate and a 3-point
improvement in closed-loop performance compared to the baseline model. Code and
model are publicly available at https://github.com/YuruiAI/DistillDrive

</details>


### [164] [Computational Design and Fabrication of Modular Robots with Untethered Control](https://arxiv.org/abs/2508.05410)
*Manas Bhargava,Takefumi Hiraki,Malina Strugaru,Michal Piovarci,Chiara Daraio,Daisuke Iwai,Bernd Bickel*

Main category: cs.RO

TL;DR: 该论文提出了一种全新的基于分布式驱动的模块化肌肉骨骼机器人架构，实现了类似自然生物的形态变换和多样化适应能力。


<details>
  <summary>Details</summary>
Motivation: 自然生物能通过肌肉骨骼系统完成复杂动作和形态变化，而现有软体机器人通常只能满足单一功能、形态不可变、或需外部庞大控制系统。论文旨在弥补这一差距，研发出更接近生物能力的机器人。

Method: 提出利用3D打印骨骼与液晶弹性体（LCE）肌肉结合的积木式机器人单元，LCE棒可通过红外辐射实现局部收缩，进而使机器人整体形变。开发了两个计算工具：一用于优化骨架结构以实现多种形变目标，另一则协同优化结构与步态以达到运动目标。

Result: 通过实验，实际制造了多种机器人，展示了复杂形变、不同控制方式及对环境的适应性，验证了所提框架的可行性和多功能性。

Conclusion: 该系统在材料、驱动与计算设计上的集成创新，为开发具有生物体能力的新一代机器人奠定了基础。

Abstract: Natural organisms use distributed actuation via their musculoskeletal systems
to adapt their gait for traversing diverse terrains or to morph their bodies to
perform varied tasks. A longstanding challenge in the field of robotics is to
mimic this extensive adaptability and range of motion. This has led humans to
develop various soft robotic systems that emulate natural organisms. However,
such systems are generally optimized for a single functionality, lack the
ability to change form or function on demand, or are often tethered to bulky
control systems. To address these challenges, we present our framework for
designing and controlling robots that mimic nature's blueprint by utilizing
distributed actuation. We propose a novel building block that combines
3D-printed bones with liquid crystal elastomer (LCE) muscles as lightweight
actuators and enables the modular assembly of musculoskeletal robots. We
developed LCE rods that contract in response to infrared radiation, thereby
achieving local and untethered control over the distributed network of bones,
which in turn results in global deformation of the robot. Furthermore, to
capitalize on the extensive design space, we develop two computational tools:
one to optimize the robot's skeletal graph, enabling multiple target
deformations, and another to co-optimize the skeletal designs and control gaits
to achieve target locomotion. We validate our system by building several robots
that show complex shape morphing, varying control schemes, and adaptability to
their environment. Our system integrates advances in modular material building,
untethered and distributed control, and computational design to introduce a new
generation of robots that brings us closer to the capabilities of living
organisms.

</details>


### [165] [Do Robots Really Need Anthropomorphic Hands?](https://arxiv.org/abs/2508.05415)
*Alexander Fabisch,Wadhah Zai El Amri,Chandandeep Singh,Nicolás Navarro-Guerrero*

Main category: cs.RO

TL;DR: 本文综述了类人手在机器人操作中是否必要，发现并非所有任务都需要复杂的五指手，三指手设计更具性价比，而非类人手型甚至可能更优。


<details>
  <summary>Details</summary>
Motivation: 人类手部灵巧操作技能极高，但机器人是否需要模仿人类手的复杂结构？研究现有手型，寻找机器人机械手在复杂度与实际操作能力间的平衡点。

Method: 文献调研和系统性综述，并对比了人手、商用及假肢手，分析了不同手型的机械结构与可实现的操作技能，提炼出实现主要操作所需的最低结构和传感能力要求。

Result: 结果显示，腕部灵活性和手指的外展/内收动作是提升操作能力的关键，单纯增加手指数/自由度意义不大。三指手达到较高操作性和简单性的平衡，部分非类人手型（如六指手或双对向手指）可实现比人手更高的灵巧度。

Conclusion: 类人五指机械手不是万能目标，实际应用中应结合任务要求选择更具性价比的手型。部分非类人方案有望超过人手操作水平，人手并非极限。

Abstract: Human manipulation skills represent a pinnacle of their voluntary motor
functions, requiring the coordination of many degrees of freedom and processing
of high-dimensional sensor input to achieve such a high level of dexterity.
Thus, we set out to answer whether the human hand, with its associated
biomechanical properties, sensors, and control mechanisms, is an ideal that we
should strive for in robotics-do we really need anthropomorphic robotic hands?
  This survey can help practitioners to make the trade-off between hand
complexity and potential manipulation skills. We provide an overview of the
human hand, a comparison of commercially available robotic and prosthetic
hands, and a systematic review of hand mechanisms and skills that they are
capable of. This leads to follow-up questions. What is the minimum requirement
for mechanisms and sensors to implement most skills that a robot needs? What is
missing to reach human-level dexterity? Can we improve upon human dexterity?
  Although complex five-fingered hands are often used as the ultimate goal for
robotic manipulators, they are not necessary for all tasks. We found that wrist
flexibility and finger abduction/adduction are important for manipulation
capabilities. On the contrary, increasing the number of fingers, actuators, or
degrees of freedom is often not necessary. Three fingers are a good compromise
between simplicity and dexterity. Non-anthropomorphic hand designs with two
opposing pairs of fingers or human hands with six fingers can further increase
dexterity, suggesting that the human hand may not be the optimum.

</details>


### [166] [Mixed-Initiative Dialog for Human-Robot Collaborative Manipulation](https://arxiv.org/abs/2508.05535)
*Albert Yu,Chengshu Li,Luca Macesanu,Arnav Balaji,Ruchira Ray,Raymond Mooney,Roberto Martín-Martín*

Main category: cs.RO

TL;DR: MICoBot是一个混合主动性的人机协作系统，利用自然语言对话，让人和机器人双方都可以主动提出、接受或拒绝协作任务分配，实现高效协作。


<details>
  <summary>Details</summary>
Motivation: 现有长周期的人机协作系统难以适应不同人类伙伴的变化行为和需求，缺乏兼顾灵活沟通和高效分工的机制。

Method: 采用Mixed-Initiative（混合主动）对话范式，提出MICoBot系统。系统在三层级决策：（1）元规划器根据对话制定高层协作策略；（2）规划器基于任务需求、机器人能力和人类可用性分配子任务；（3）执行器决定机器人具体行动或生成对话。系统通过自然语言处理实现人与机器人协同制定分工和任务流。

Result: 在仿真和现实测试（18名参与者，27小时）下，MICoBot较LLM和传统分配模型取得更高的任务成功率和更好的用户体验。

Conclusion: MICoBot能够适应多样化的人机协作场景，有效促进人与机器人共同完成复杂任务，实现更高效、用户体验更佳的协作。

Abstract: Effective robotic systems for long-horizon human-robot collaboration must
adapt to a wide range of human partners, whose physical behavior, willingness
to assist, and understanding of the robot's capabilities may change over time.
This demands a tightly coupled communication loop that grants both agents the
flexibility to propose, accept, or decline requests as they coordinate toward
completing the task effectively. We apply a Mixed-Initiative dialog paradigm to
Collaborative human-roBot teaming and propose MICoBot, a system that handles
the common scenario where both agents, using natural language, take initiative
in formulating, accepting, or rejecting proposals on who can best complete
different steps of a task. To handle diverse, task-directed dialog, and find
successful collaborative strategies that minimize human effort, MICoBot makes
decisions at three levels: (1) a meta-planner considers human dialog to
formulate and code a high-level collaboration strategy, (2) a planner optimally
allocates the remaining steps to either agent based on the robot's capabilities
(measured by a simulation-pretrained affordance model) and the human's
estimated availability to help, and (3) an action executor decides the
low-level actions to perform or words to say to the human. Our extensive
evaluations in simulation and real-world -- on a physical robot with 18 unique
human participants over 27 hours -- demonstrate the ability of our method to
effectively collaborate with diverse human users, yielding significantly
improved task success and user experience than a pure LLM baseline and other
agent allocation models. See additional videos and materials at
https://robin-lab.cs.utexas.edu/MicoBot/.

</details>


### [167] [CleanUpBench: Embodied Sweeping and Grasping Benchmark](https://arxiv.org/abs/2508.05543)
*Wenbo Li,Guanting Chen,Tao Zhao,Jiyao Wang,Tianxin Hu,Yuwen Liao,Weixiang Guo,Shenghai Yuan*

Main category: cs.RO

TL;DR: 提出了CleanUpBench，这是一个用于评估具身智能体在室内清洁场景中表现的基准测试。该平台基于NVIDIA Isaac Sim，能模拟包含扫地和抓取能力的服务机器人，并支持不同环境和任务的全面评估。


<details>
  <summary>Details</summary>
Motivation: 当前的具身智能基准多聚焦于复杂的类人机器人或大规模仿真环境，距离真实应用较远。而现实中如扫地与抓取双模式的清洁机器人快速发展，缺乏相应系统评测基准，存在学术与实际应用的脱节。

Method: 构建CleanUpBench基准，利用仿真平台NVIDIA Isaac Sim，设置既有手工设计也有自动生成的室内环境，配置配备扫地装置和六自由度机械臂的移动机器人。评测涵盖任务完成度、空间效率、运动质量和控制性能，并提供基于启发式和地图规划的基线方法。

Result: CleanUpBench可系统性评估清洁机器人在多目标任务和多类型物品交互中的能力，并通过丰富的评价指标客观反映智能体表现。支持环境泛化能力评估，通过设定多种场景考查算法通用性。

Conclusion: CleanUpBench为现实场景下的服务机器人智能评测提供标准化、可扩展的测试平台，促进学术模型向实际产品的转化，并有助于推进具身智能算法的实际应用落地。

Abstract: Embodied AI benchmarks have advanced navigation, manipulation, and reasoning,
but most target complex humanoid agents or large-scale simulations that are far
from real-world deployment. In contrast, mobile cleaning robots with dual mode
capabilities, such as sweeping and grasping, are rapidly emerging as realistic
and commercially viable platforms. However, no benchmark currently exists that
systematically evaluates these agents in structured, multi-target cleaning
tasks, revealing a critical gap between academic research and real-world
applications. We introduce CleanUpBench, a reproducible and extensible
benchmark for evaluating embodied agents in realistic indoor cleaning
scenarios. Built on NVIDIA Isaac Sim, CleanUpBench simulates a mobile service
robot equipped with a sweeping mechanism and a six-degree-of-freedom robotic
arm, enabling interaction with heterogeneous objects. The benchmark includes
manually designed environments and one procedurally generated layout to assess
generalization, along with a comprehensive evaluation suite covering task
completion, spatial efficiency, motion quality, and control performance. To
support comparative studies, we provide baseline agents based on heuristic
strategies and map-based planning. CleanUpBench bridges the gap between
low-level skill evaluation and full-scene testing, offering a scalable testbed
for grounded, embodied intelligence in everyday settings.

</details>


### [168] [Robust adaptive fuzzy sliding mode control for trajectory tracking for of cylindrical manipulator](https://arxiv.org/abs/2508.05584)
*Van Cuong Pham,Minh Hai Tran,Phuc Anh Nguyen,Ngoc Son Vu,Nga Nguyen Thi*

Main category: cs.RO

TL;DR: 本研究提出了一种自适应模糊滑模控制（AFSMC）方法，用于提升圆柱型机器人操作手的轨迹跟踪性能，并通过仿真验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统工业机器人在实际应用中，如CNC加工和3D打印，对轨迹跟踪的精确性和鲁棒性有较高要求，但复杂动力学和外部扰动常导致性能受限。针对这些挑战，亟需一种既能适应不确定性又具有鲁棒性的控制方法。

Method: 方法将模糊逻辑和滑模控制（SMC）相结合。模糊逻辑用于逼近系统的不确定动力学，滑模控制保证系统的强健性能。通过MATLAB/Simulink对所提控制器进行了仿真测试。

Result: 仿真结果显示，AFSMC在轨迹跟踪精度、系统稳定性以及抗扰动能力方面，均较传统方法有显著提升。

Conclusion: 研究证实了AFSMC对工业机器人控制的有效性，可提升机器人轨迹跟踪的精度，有助于提升工业制造的质量和效率。

Abstract: This research proposes a robust adaptive fuzzy sliding mode control (AFSMC)
approach to enhance the trajectory tracking performance of cylindrical robotic
manipulators, extensively utilized in applications such as CNC and 3D printing.
The proposed approach integrates fuzzy logic with sliding mode control (SMC) to
bolster adaptability and robustness, with fuzzy logic approximating the
uncertain dynamics of the system, while SMC ensures strong performance.
Simulation results in MATLAB/Simulink demonstrate that AFSMC significantly
improves trajectory tracking accuracy, stability, and disturbance rejection
compared to traditional methods. This research underscores the effectiveness of
AFSMC in controlling robotic manipulators, contributing to enhanced precision
in industrial robotic applications.

</details>


### [169] [Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling](https://arxiv.org/abs/2508.05634)
*Jianpeng Yao,Xiaopan Zhang,Yu Xia,Zejin Wang,Amit K. Roy-Chowdhury,Jiachen Li*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习的不确定性感知移动机器人群体导航方法，显著提升了机器人在分布外场景下的导航安全性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的移动机器人在人群环境中导航时，容易在遇到与训练数据分布不同的场景（分布外）时性能降低，增加碰撞和安全隐患。为提升机器人在实际复杂环境下的安全性和鲁棒性，需要有效处理人群动态变化的不确定性。

Method: 作者通过引入自适应保序推断（adaptive conformal inference）产生对行人行为的不确定性估计，并将其作为增强观测输入，结合约束强化学习机制用于指导机器人行为决策，从而在导航过程中更好处理环境不确定性。

Result: 在分布内场景，该方法成功率达到96.93%，比现有最佳方法提升8.8%，碰撞数和入侵真实行人轨迹次数分别减少3.72倍和2.43倍。在三类分布外场景（速度变化、行为策略变化及个体向群体动态转变）下展现出更强鲁棒性。实机测试中，机器人在稀疏和密集人群中均表现出良好的安全性和决策鲁棒性。

Conclusion: 通过合理建模行人不确定性并在强化学习中加以利用，可以显著增强移动机器人在实际复杂人群环境中的导航安全性与鲁棒性。该方法具备较强的泛化能力，有望促进机器人群体导航的现实部署应用。

Abstract: Mobile robots navigating in crowds trained using reinforcement learning are
known to suffer performance degradation when faced with out-of-distribution
scenarios. We propose that by properly accounting for the uncertainties of
pedestrians, a robot can learn safe navigation policies that are robust to
distribution shifts. Our method augments agent observations with prediction
uncertainty estimates generated by adaptive conformal inference, and it uses
these estimates to guide the agent's behavior through constrained reinforcement
learning. The system helps regulate the agent's actions and enables it to adapt
to distribution shifts. In the in-distribution setting, our approach achieves a
96.93% success rate, which is over 8.80% higher than the previous
state-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times
fewer intrusions into ground-truth human future trajectories. In three
out-of-distribution scenarios, our method shows much stronger robustness when
facing distribution shifts in velocity variations, policy changes, and
transitions from individual to group dynamics. We deploy our method on a real
robot, and experiments show that the robot makes safe and robust decisions when
interacting with both sparse and dense crowds. Our code and videos are
available on https://gen-safe-nav.github.io/.

</details>


### [170] [Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation](https://arxiv.org/abs/2508.05635)
*Yue Liao,Pengfei Zhou,Siyuan Huang,Donglin Yang,Shengcong Chen,Yuxin Jiang,Yue Hu,Jingbin Cai,Si Liu,Jianlan Luo,Liliang Chen,Shuicheng Yan,Maoqing Yao,Guanghui Ren*

Main category: cs.RO

TL;DR: 本文提出了Genie Envisioner（GE）平台，将机器人操作中的策略学习、评估与仿真整合到单一的视频生成框架内，具备统一与可扩展性的特点。


<details>
  <summary>Details</summary>
Motivation: 当前机器人操作领域缺乏集成学习、评估和仿真的统一平台，现有方法难以高效泛化和扩展到多种场景，且监督代价高、评估不标准化。作者希望通过统一的平台支持可扩展和通用的人机指令驱动智能。

Method: GE平台包含：1）GE-Base，大规模、指令条件的视频扩散模型，获取机器人交互的时空和语义动态；2）GE-Act，从潜变量解码至可执行动作轨迹；3）GE-Sim，基于神经网络的行动条件仿真器，用于高保真策略训练与评估；4）EWMBench，统一的基准，用于评测视觉效果、物理一致性和指令与动作对齐程度。

Result: GE实现了对不同机器人平台的广泛适用性，在无需大量监督下实现了高精度、强泛化性的动作策略生成，并通过高保真的仿真支持可扩展的训练和评估。

Conclusion: Genie Envisioner建立了一个实用且可扩展的平台，推动了面向指令驱动、通用型实体智能的发展，对机器人操作的学习与测试具有重要基础意义。

Abstract: We introduce Genie Envisioner (GE), a unified world foundation platform for
robotic manipulation that integrates policy learning, evaluation, and
simulation within a single video-generative framework. At its core, GE-Base is
a large-scale, instruction-conditioned video diffusion model that captures the
spatial, temporal, and semantic dynamics of real-world robotic interactions in
a structured latent space. Built upon this foundation, GE-Act maps latent
representations to executable action trajectories through a lightweight,
flow-matching decoder, enabling precise and generalizable policy inference
across diverse embodiments with minimal supervision. To support scalable
evaluation and training, GE-Sim serves as an action-conditioned neural
simulator, producing high-fidelity rollouts for closed-loop policy development.
The platform is further equipped with EWMBench, a standardized benchmark suite
measuring visual fidelity, physical consistency, and instruction-action
alignment. Together, these components establish Genie Envisioner as a scalable
and practical foundation for instruction-driven, general-purpose embodied
intelligence. All code, models, and benchmarks will be released publicly.

</details>
