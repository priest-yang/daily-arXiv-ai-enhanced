<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 68]
- [cs.CL](#cs.CL) [Total: 45]
- [cs.RO](#cs.RO) [Total: 23]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [AI-Based Culvert-Sewer Inspection](https://arxiv.org/abs/2601.15366)
*Christina Thrainer*

Main category: cs.CV

TL;DR: 本文旨在提升涵洞和下水管道结构缺陷自动分割的准确性与实用性，尤其针对数据标注稀缺的问题，提出了三种创新方法，并且在实际数据有限条件下取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 涵洞与下水管道在排水系统中极为重要，结构缺陷导致的失效会带来安全与环境风险。然而，相关缺陷检测数据难以获取且标注困难，导致智能检测方法的数据基础薄弱，因此亟需研究数据稀缺场景下的自动缺陷分割算法。

Method: 1）评估和改进训练数据预处理策略，包括数据增强与动态标签注入，提高分割性能；2）提出新型网络结构FORTRESS，融合深度可分离卷积、自适应KAN与多尺度注意力机制，在提升性能的同时减少参数量和计算开销；3）研究少样本语义分割在缺陷检测中的应用，通过带有注意力机制的双向原型网络丰富特征表达。

Result: 三种方法均显著提升了缺陷分割性能。数据增强和标签注入提高了IoU和F1得分。FORTRESS模型在涵洞下水道缺陷数据集上达到SOTA表现，并显著降低了参数量和计算成本。双向原型注意力网络在少样本条件下也取得了令人满意的分割效果。

Conclusion: 本文提出的三种方法有效应对了数据稀缺下的涵洞与下水管道缺陷分割难题，为相关智能设施检测提供了高效、实用的技术路径，有望推动行业智能化发展。

Abstract: Culverts and sewer pipes are critical components of drainage systems, and their failure can lead to serious risks to public safety and the environment. In this thesis, we explore methods to improve automated defect segmentation in culverts and sewer pipes. Collecting and annotating data in this field is cumbersome and requires domain knowledge. Having a large dataset for structural defect detection is therefore not feasible. Our proposed methods are tested under conditions with limited annotated data to demonstrate applicability to real-world scenarios. Overall, this thesis proposes three methods to significantly enhance defect segmentation and handle data scarcity. This can be addressed either by enhancing the training data or by adjusting a models architecture.
  First, we evaluate preprocessing strategies, including traditional data augmentation and dynamic label injection. These techniques significantly improve segmentation performance, increasing both Intersection over Union (IoU) and F1 score. Second, we introduce FORTRESS, a novel architecture that combines depthwise separable convolutions, adaptive Kolmogorov-Arnold Networks (KAN), and multi-scale attention mechanisms. FORTRESS achieves state-of-the-art performance on the culvert sewer pipe defect dataset, while significantly reducing the number of trainable parameters, as well as its computational cost. Finally, we investigate few-shot semantic segmentation and its applicability to defect detection. Few-shot learning aims to train models with only limited data available. By employing a bidirectional prototypical network with attention mechanisms, the model achieves richer feature representations and achieves satisfactory results across evaluation metrics.

</details>


### [2] [Evaluating Multimodal Large Language Models for Heterogeneous Face Recognition](https://arxiv.org/abs/2601.15406)
*Hatef Otroshi Shahreza,Anjith George,Sébastien Marcel*

Main category: cs.CV

TL;DR: 本论文系统性评估了多模态大语言模型（MLLMs）在异构人脸识别任务上的表现，发现其在跨光谱场景下与传统方法存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 受MLLMs在多种视觉-语言任务表现优异的启发，作者希望探究其在复杂生物识别（如跨模态人脸识别）中的应用潜力及局限。

Method: 选取多种开源MLLMs，针对不同摄像条件（VIS-NIR、VIS-SWIR、VIS-THERMAL）下的人脸识别场景，依据生物识别协议，采用不同识别指标（如Acquire Rate、EER、TAR）进行系统评测和性能对比。

Result: MLLMs在多种跨模态人脸识别任务上性能明显落后于传统人脸识别系统，尤其是在跨光谱识别场景下表现差距更大。

Conclusion: 目前MLLMs在异构人脸识别方面存在明显不足，实际部署前需充分、严谨的生物识别评估，不能盲目迁移在视觉-语言方面的进展到生物识别领域。

Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated strong performance on a wide range of vision-language tasks, raising interest in their potential use for biometric applications. In this paper, we conduct a systematic evaluation of state-of-the-art MLLMs for heterogeneous face recognition (HFR), where enrollment and probe images are from different sensing modalities, including visual (VIS), near infrared (NIR), short-wave infrared (SWIR), and thermal camera. We benchmark multiple open-source MLLMs across several cross-modality scenarios, including VIS-NIR, VIS-SWIR, and VIS-THERMAL face recognition. The recognition performance of MLLMs is evaluated using biometric protocols and based on different metrics, including Acquire Rate, Equal Error Rate (EER), and True Accept Rate (TAR). Our results reveal substantial performance gaps between MLLMs and classical face recognition systems, particularly under challenging cross-spectral conditions, in spite of recent advances in MLLMs. Our findings highlight the limitations of current MLLMs for HFR and also the importance of rigorous biometric evaluation when considering their deployment in face recognition systems.

</details>


### [3] [CURE: Curriculum-guided Multi-task Training for Reliable Anatomy Grounded Report Generation](https://arxiv.org/abs/2601.15408)
*Pablo Messina,Andrés Villa,Juan León Alcázar,Karen Sánchez,Carlos Hinojosa,Denis Parra,Álvaro Soto,Bernard Ghanem*

Main category: cs.CV

TL;DR: CURE是一种用于医疗视觉-语言模型的课程学习框架，通过有针对性地训练提高了报告的空间和文本对齐性和报告可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉-语言模型在自动生成放射学报告时，常因视觉-文本对齐不足导致事实不一致和预测不可靠，亟需提升视觉证据与文本发现之间的一致性。

Method: CURE利用误差感知的课程学习，对多模态模型在短语定位、基于定位的报告生成和解剖定位的报告生成三项任务上进行微调。训练过程中根据模型表现动态调整采样，用更难的样本强化模型能力，无需额外数据集。

Result: CURE较基线方法在定位精度上提升了+0.37 IoU，报告质量提高了+0.188 CXRFEScore，杜撰内容下降18.6%。

Conclusion: CURE以数据高效的方式显著改善了医学视觉-语言模型的定位准确性和报告可靠性。

Abstract: Medical vision-language models can automate the generation of radiology reports but struggle with accurate visual grounding and factual consistency. Existing models often misalign textual findings with visual evidence, leading to unreliable or weakly grounded predictions. We present CURE, an error-aware curriculum learning framework that improves grounding and report quality without any additional data. CURE fine-tunes a multimodal instructional model on phrase grounding, grounded report generation, and anatomy-grounded report generation using public datasets. The method dynamically adjusts sampling based on model performance, emphasizing harder samples to improve spatial and textual alignment. CURE improves grounding accuracy by +0.37 IoU, boosts report quality by +0.188 CXRFEScore, and reduces hallucinations by 18.6%. CURE is a data-efficient framework that enhances both grounding accuracy and report reliability. Code is available at https://github.com/PabloMessina/CURE and model weights at https://huggingface.co/pamessina/medgemma-4b-it-cure

</details>


### [4] [DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely Sparse-view CBCT Reconstruction](https://arxiv.org/abs/2601.15416)
*Cuong Tran Van,Trong-Thang Pham,Ngoc-Son Nguyen,Duy Minh Ho Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的稀疏视角锥束CT重建方法DuFal，结合了频域和空间域的双通路架构，显著提升了高频细节（如解剖结构）的重建效果，尤其在极少投影下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医疗成像中，稀疏投影CT由于采样受限严重丢失高频细节（如精细解剖结构），主流CNN方法又偏向复原低频，导致高频恢复不佳，因此亟需新的方法提升稀疏视角下高频特征重建。

Method: 提出Dual-Frequency-Aware Learning (DuFal)网络，采用双通路架构：全球高频增强Fourier神经算子分支捕捉全局频率信息，局部高频增强分支处理空间局部细节，辅以频谱-通道因子分解减少参数、交叉注意力频率融合模块整合空间与频率特征，最终经过特征解码和强度场重建生成CT体。

Result: 在LUNA16和ToothFairy两个公开数据集上，DuFal在稀疏投影条件下，重建结果在高频结构保存方面显著优于最先进方法。

Conclusion: DuFal有效解决了稀疏视角CT中高频结构丢失的问题，适用于投影极少的实际场景，有助于提升医疗图像重建质量。

Abstract: Sparse-view Cone-Beam Computed Tomography reconstruction from limited X-ray projections remains a challenging problem in medical imaging due to the inherent undersampling of fine-grained anatomical details, which correspond to high-frequency components. Conventional CNN-based methods often struggle to recover these fine structures, as they are typically biased toward learning low-frequency information. To address this challenge, this paper presents DuFal (Dual-Frequency-Aware Learning), a novel framework that integrates frequency-domain and spatial-domain processing via a dual-path architecture. The core innovation lies in our High-Local Factorized Fourier Neural Operator, which comprises two complementary branches: a Global High-Frequency Enhanced Fourier Neural Operator that captures global frequency patterns and a Local High-Frequency Enhanced Fourier Neural Operator that processes spatially partitioned patches to preserve spatial locality that might be lost in global frequency analysis. To improve efficiency, we design a Spectral-Channel Factorization scheme that reduces the Fourier Neural Operator parameter count. We also design a Cross-Attention Frequency Fusion module to integrate spatial and frequency features effectively. The fused features are then decoded through a Feature Decoder to produce projection representations, which are subsequently processed through an Intensity Field Decoding pipeline to reconstruct a final Computed Tomography volume. Experimental results on the LUNA16 and ToothFairy datasets demonstrate that DuFal significantly outperforms existing state-of-the-art methods in preserving high-frequency anatomical features, particularly under extremely sparse-view settings.

</details>


### [5] [DevPrompt: Deviation-Based Prompt Learning for One-Normal ShotImage Anomaly Detection](https://arxiv.org/abs/2601.15453)
*Morteza Poudineh,Marc Lalonde*

Main category: cs.CV

TL;DR: 本文提出了一种面向小样本异常检测的新方法，结合视觉-语言模型和偏差评分机制，在多个基准上性能优越。


<details>
  <summary>Details</summary>
Motivation: 仅用少量正常样本进行异常区域检测非常困难，现有基于CLIP的方法在区分正常和异常效果有限，缺乏有效的局部异常评分方法。

Method: 方法通过可学习的上下文向量取代固定prompt前缀，同时采用异常类别特有的后缀词，提升图文特征对齐能力。引入基于偏差的损失和Top-K多实例学习，将patch特征建模为高斯偏差，实现异常区域的更精确评分和定位。

Result: 在MVTecAD和VISA基准上，提出的方法在像素级检测优于PromptAD等已有方法；消融实验验证了可学习prompt、基于偏差的评分和Top-K策略的有效性。

Conclusion: 整合视觉-语言语义与统计偏差机制可显著提升小样本异常检测的定位精度和可解释性。

Abstract: Few-normal shot anomaly detection (FNSAD) aims to detect abnormal regions in images using only a few normal training samples, making the task highly challenging due to limited supervision and the diversity of potential defects. Recent approaches leverage vision-language models such as CLIP with prompt-based learning to align image and text features. However, existing methods often exhibit weak discriminability between normal and abnormal prompts and lack principled scoring mechanisms for patch-level anomalies. We propose a deviation-guided prompt learning framework that integrates the semantic power of vision-language models with the statistical reliability of deviation-based scoring. Specifically, we replace fixed prompt prefixes with learnable context vectors shared across normal and abnormal prompts, while anomaly-specific suffix tokens enable class-aware alignment. To enhance separability, we introduce a deviation loss with Top-K Multiple Instance Learning (MIL), modeling patch-level features as Gaussian deviations from the normal distribution. This allows the network to assign higher anomaly scores to patches with statistically significant deviations, improving localization and interpretability. Experiments on the MVTecAD and VISA benchmarks demonstrate superior pixel-level detection performance compared to PromptAD and other baselines. Ablation studies further validate the effectiveness of learnable prompts, deviation-based scoring, and the Top-K MIL strategy.

</details>


### [6] [Seeing through Light and Darkness: Sensor-Physics Grounded Deblurring HDR NeRF from Single-Exposure Images and Events](https://arxiv.org/abs/2601.15475)
*Yunshan Qi,Lin Zhu,Nan Bao,Yifan Zhao,Jia Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于物理感知的NeRF框架，实现对单帧模糊LDR图片及其事件数据的高动态范围（HDR）清晰三维新视角合成，在极端光照下效果领先。


<details>
  <summary>Details</summary>
Motivation: 现有的新视角合成方法在采集到的LDR模糊照片下难以恢复清晰且高动态范围的三维表示，尤其在极端光照下表现不佳。虽然已有方法尝试引入事件数据缓解上述问题，但忽视了相机输出与物理世界辐射之间的感知差异，导致结果次优。

Method: 作者提出了物理感知基础上的NeRF框架，将三维场景的真实HDR辐射直接表示于HDR域，并模拟原始HDR光线到传感器像素的过程。通过引入逐像素RGB映射域和事件映射域，将合成的像素值精准对齐到输入模糊LDR图片及事件数据，实现物理场景、事件与成像一致。上述映射域与NeRF网络联合优化，综合利用事件的时空动态信息，增强HDR表示。

Result: 在自有与公开数据集上的实验表明，该方法在单次曝光模糊LDR图片与事件数据下，可实现业界最优的HDR去模糊新视角合成效果。

Conclusion: 所提方法有效提升了低曝光模糊LDR图像下的HDR新视角合成质量，兼顾HDR重建和去模糊，具备处理极端光照条件下的实用价值。

Abstract: Novel view synthesis from low dynamic range (LDR) blurry images, which are common in the wild, struggles to recover high dynamic range (HDR) and sharp 3D representations in extreme lighting conditions. Although existing methods employ event data to address this issue, they ignore the sensor-physics mismatches between the camera output and physical world radiance, resulting in suboptimal HDR and deblurring results. To cope with this problem, we propose a unified sensor-physics grounded NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding events. We employ NeRF to directly represent the actual radiance of the 3D scene in the HDR domain and model raw HDR scene rays hitting the sensor pixels as in the physical world. A pixel-wise RGB mapping field is introduced to align the above rendered pixel values with the sensor-recorded LDR pixel values of the input images. A novel event mapping field is also designed to bridge the physical scene dynamics and actual event sensor output. The two mapping fields are jointly optimized with the NeRF network, leveraging the spatial and temporal dynamic information in events to enhance the sharp HDR 3D representation learning. Experiments on the collected and public datasets demonstrate that our method can achieve state-of-the-art deblurring HDR novel view synthesis results with single-exposure blurry LDR images and corresponding events.

</details>


### [7] [Hybrid Vision Transformer_GAN Attribute Neutralizer for Mitigating Bias in Chest X_Ray Diagnosis](https://arxiv.org/abs/2601.15490)
*Jobeal Solomon,Ali Mohammed Mansoor Alsahag,Seyed Sahand Mohammadi Ziabari*

Main category: cs.CV

TL;DR: 本文提出用Vision Transformer (ViT) 取代传统的U-Net卷积编码器，加强胸腔X光AI对性别等特征泄露的抑制力，同时保证疾病诊断精度。实验显示ViT能更好地抑制性别信息泄漏，且不会显著降低疾病检测准确性。


<details>
  <summary>Details</summary>
Motivation: 当前胸腔X光影像AI常因性别、年龄等特征泄露，导致对少数群体诊断不公平。之前采用像素级属性中和技术虽有改善，但仍难彻底消除泄露，影响实际应用的公平性。作者希望探索ViT在减少敏感属性泄露方面的潜力。

Method: 将原有Attribute-Neutral Framework中的卷积式U-Net编码器替换为基于Vision Transformer的骨干（DeiT-S），在ChestX-ray14数据集上训练，同时设置不同属性中和强度。通过AI判别器评估编辑后影像的敏感属性泄露程度，并用独立的ConvNet评估疾病识别能力。

Result: ViT编码器在中等编辑强度下可将性别识别AUC降至约0.80，较U-Net有10个百分点的改进。疾病检测能力基本保持，15项疾病平均AUC减少不超过5个百分点，最小子群体AUC约为0.70。

Conclusion: 全球自注意力ViT能够更有效抑制胸X光影像中的敏感属性泄露，同时维持高水平的临床诊断准确性，展示了推动更公平医疗AI的实际可行途径。

Abstract: Bias in chest X-ray classifiers frequently stems from sex- and age-related shortcuts, leading to systematic underdiagnosis of minority subgroups. Previous pixel-space attribute neutralizers, which rely on convolutional encoders, lessen but do not fully remove this attribute leakage at clinically usable edit strengths. This study evaluates whether substituting the U-Net convolutional encoder with a Vision Transformer backbone in the Attribute-Neutral Framework can reduce demographic attribute leakage while preserving diagnostic accuracy. A data-efficient Image Transformer Small (DeiT-S) neutralizer was trained on the ChestX-ray14 dataset. Its edited images, generated across eleven edit-intensity levels, were evaluated with an independent AI judge for attribute leakage and with a convolutional neural network (ConvNet) for disease prediction. At a moderate edit level (alpha = 0.5), the Vision Transformer (ViT) neutralizer reduces patient sex-recognition area under the curve (AUC) to approximately 0.80, about 10 percentage points below the original framework's convolutional U-Net encoder, despite being trained for only half as many epochs. Meanwhile, macro receiver operating characteristic area under the curve (ROC AUC) across 15 findings stays within five percentage points of the unedited baseline, and the worst-case subgroup AUC remains near 0.70. These results indicate that global self-attention vision models can further suppress attribute leakage without sacrificing clinical utility, suggesting a practical route toward fairer chest X-ray AI.

</details>


### [8] [Controllable Layered Image Generation for Real-World Editing](https://arxiv.org/abs/2601.15507)
*Jinrui Yang,Qing Liu,Yijun Li,Mengwei Ren,Letian Zhang,Zhe Lin,Cihang Xie,Yuyin Zhou*

Main category: cs.CV

TL;DR: 本文提出了LASAGNA，一种能够同时生成真实感背景和带有自然视觉效果的透明前景分层的图像生成新方法，相比以往方法提升了分层合成一致性与可控性。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型难以对已有图片中的元素进行灵活、准确的编辑，尤其是分层表示的生成质量存在不足，如缺乏逼真的阴影、反射等效果，且层之间合成关系不连贯。本文旨在解决这些问题。

Method: 提出了LASAGNA框架，可同时生成包含真实背景与高质量透明前景的分层图像；引入LASAGNA-48K新数据集（含物理真实感视觉效果的前景和背景）和LASAGNABENCH基准，用以训练和评测模型。支持多种条件输入（文本、前景、背景、位置掩码），提高图像编辑可控性和适用性。

Result: LASAGNA在多层图像一致性与真实感表现上优于现有方法，能够实现准确保留身份与视觉效果的丰富后期编辑。LASAGNA-48K和LASAGNABENCH数据集将公开促进社区研究。

Conclusion: LASAGNA显著提升了分层图像生成的质量和可控性，为实际图像编辑应用提供了有效工具，并推动了相关基准和数据集的建设。

Abstract: Recent image generation models have shown impressive progress, yet they often struggle to yield controllable and consistent results when users attempt to edit specific elements within an existing image. Layered representations enable flexible, user-driven content creation, but existing approaches often fail to produce layers with coherent compositing relationships, and their object layers typically lack realistic visual effects such as shadows and reflections. To overcome these limitations, we propose LASAGNA, a novel, unified framework that generates an image jointly with its composing layers--a photorealistic background and a high-quality transparent foreground with compelling visual effects. Unlike prior work, LASAGNA efficiently learns correct image composition from a wide range of conditioning inputs--text prompts, foreground, background, and location masks--offering greater controllability for real-world applications. To enable this, we introduce LASAGNA-48K, a new dataset composed of clean backgrounds and RGBA foregrounds with physically grounded visual effects. We also propose LASAGNABENCH, the first benchmark for layer editing. We demonstrate that LASAGNA excels in generating highly consistent and coherent results across multiple image layers simultaneously, enabling diverse post-editing applications that accurately preserve identity and visual effects. LASAGNA-48K and LASAGNABENCH will be publicly released to foster open research in the community. The project page is https://rayjryang.github.io/LASAGNA-Page/.

</details>


### [9] [DeltaDorsal: Enhancing Hand Pose Estimation with Dorsal Features in Egocentric Views](https://arxiv.org/abs/2601.15516)
*William Huang,Siyou Pei,Leyi Zou,Eric J. Gonzalez,Ishan Chatterjee,Yang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种通过手背皮肤变形信息提升自端手姿态估计的方法，并在高度遮挡场景下显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 虚拟现实设备的普及使自端（egocentric）视角下的手势估计变得非常重要，但此视角下手指常常被遮挡，影响准确性。作者希望通过新的视觉线索克服这一挑战。

Method: 提出了一种双流delta编码器，通过对比动态手部与放松基线姿态的特征，仅利用手背皮肤的密集视觉特征学习手势，同时只需裁剪后的手背图像作为输入。

Result: 在手指遮挡超过50%的场景下，本文方法仅用手背图像，MPJAE（每关节角度平均误差）比当前最先进技术降低18%，且模型规模更小。

Conclusion: 该方法不仅提升了在遮挡场景下的手势估计鲁棒性，还为表面点击等全新交互方式提供了可能，对后续应用场景有重要推动作用。

Abstract: The proliferation of XR devices has made egocentric hand pose estimation a vital task, yet this perspective is inherently challenged by frequent finger occlusions. To address this, we propose a novel approach that leverages the rich information in dorsal hand skin deformation, unlocked by recent advances in dense visual featurizers. We introduce a dual-stream delta encoder that learns pose by contrasting features from a dynamic hand with a baseline relaxed position. Our evaluation demonstrates that, using only cropped dorsal images, our method reduces the Mean Per Joint Angle Error (MPJAE) by 18% in self-occluded scenarios (fingers >=50% occluded) compared to state-of-the-art techniques that depend on the whole hand's geometry and large model backbones. Consequently, our method not only enhances the reliability of downstream tasks like index finger pinch and tap estimation in occluded scenarios but also unlocks new interaction paradigms, such as detecting isometric force for a surface "click" without visible movement while minimizing model size.

</details>


### [10] [VIOLA: Towards Video In-Context Learning with Minimal Annotations](https://arxiv.org/abs/2601.15549)
*Ryo Fujii,Hideo Saito,Ryo Hachiuma*

Main category: cs.CV

TL;DR: 提出了一种名为VIOLA的新方法，可以在标注数据极为稀缺的情况下，有效提升多模态大模型（MLLMs）在新视频领域的泛化能力。通过密度-不确定性加权采样与置信度感知策略，充分利用极少量专家标注与大量无标注数据，显著优于现有低资源基线。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在新的视频领域泛化能力有限，主要由于缺乏足够的标注数据。传统的领域自适应大多依赖大量手工标注，需要专家参与，成本高、不可扩展。现实环境（如工业、手术场景）中获取大量带标注数据不切实际，因此亟需能以极少标注高效泛化的学习框架。

Method: 1. 提出密度-不确定性加权采样方法，以最小标注预算，选择既多样又具代表性、有信息量的样本，避免选中视觉孤立的异常值。2. 对未标注数据，采用混合池构建，并设计了置信度感知检索与提示机制，对标注置信度进行建模，以相似性和置信度联合检索示例，区分真实与伪标注，稳定模型适应过程。

Result: 在9个多样化基准上的实验表明，VIOLA能在极低标注资源下，显著超越多种主流方法，实现鲁棒、高效的视频领域自适应。

Conclusion: VIOLA使用极少专家标注和大量无标注数据，有效提升了MLLMs在新视频领域适应性能，减少了对昂贵人工标注的依赖，为低资源情境下的视频理解任务提供了新范式。

Abstract: Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.

</details>


### [11] [Relative Classification Accuracy: A Calibrated Metric for Identity Consistency in Fine-Grained K-pop Face Generation](https://arxiv.org/abs/2601.15560)
*Sylvey Lin,Eranki Vasistha*

Main category: cs.CV

TL;DR: 本文聚焦于评估条件扩散模型在细粒度领域（如K-pop偶像人脸）的语义可控性，发现传统指标难以区分身份一致性问题，提出了新的评估指标RCA，并揭示了视觉质量与语义一致性之间的关键权衡。


<details>
  <summary>Details</summary>
Motivation: 尽管DDPM在图像生成领域表现出色，但在细粒度单一领域——如K-pop偶像人脸——中对语义（身份）一致性的精准评估仍不足，现有的FID等指标无法有效捕捉身份错配，因此亟需新的评估方法。

Method: 作者研究了32x32分辨率下，基于类别条件的扩散模型生成K-pop偶像人脸的任务，引入了一个新评价指标RCA（相对分类准确率），以生成图像被‘神谕’分类器正确识别的比例对齐分类器在真实图像上的表现进行归一化评价，并结合混淆矩阵等方法对模型失败模式进行分析。

Result: 在实验中，模型生成的图像视觉质量高（FID达8.93），但在身份一致性上表现较差（RCA仅0.27），尤其在性别内身份相似情况下更容易混淆。混淆矩阵分析指出分辨率与性别内模糊是主要原因。

Conclusion: 作者提出的RCA为条件生成模型的身份一致性评估提供了更严格的标准，揭示了扩散模型在高相似度身份生成中面临视觉质量与语义一致性的本质权衡。

Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable success in high-fidelity image generation. However, evaluating their semantic controllability-specifically for fine-grained, single-domain tasks-remains challenging. Standard metrics like FID and Inception Score (IS) often fail to detect identity misalignment in such specialized contexts. In this work, we investigate Class-Conditional DDPMs for K-pop idol face generation (32x32), a domain characterized by high inter-class similarity. We propose a calibrated metric, Relative Classification Accuracy (RCA), which normalizes generative performance against an oracle classifier's baseline. Our evaluation reveals a critical trade-off: while the model achieves high visual quality (FID 8.93), it suffers from severe semantic mode collapse (RCA 0.27), particularly for visually ambiguous identities. We analyze these failure modes through confusion matrices and attribute them to resolution constraints and intra-gender ambiguity. Our framework provides a rigorous standard for verifying identity consistency in conditional generative models.

</details>


### [12] [Region-aware Spatiotemporal Modeling with Collaborative Domain Generalization for Cross-Subject EEG Emotion Recognition](https://arxiv.org/abs/2601.15615)
*Weiwei Wu,Yueyang Li,Yuhu Shi,Weiming Zeng,Lang Qin,Yang Yang,Ke Zhou,Zhiguo Zhang,Wai Ting Siok,Nizhuan Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为RSM-CoDG的新框架，显著提升了脑电跨被试情感识别的效果。


<details>
  <summary>Details</summary>
Motivation: 跨被试EEG情感识别面临被试间差异大、脑电分布变化大，以及神经表征时空复杂性高的问题，现有方法难以统一提升空间、时间建模和泛化能力。

Method: 提出RSM-CoDG框架：基于脑区功能分区的神经科学先验进行空间建模，利用多尺度时域建模捕捉情感动态变化，并引入多维约束的协同领域泛化策略以减少被试特异性偏差，实现对未知被试的泛化提升。

Result: 在多个SEED数据集上的大量实验表明，RSM-CoDG方法显著优于现有主流方法，提高了鲁棒性和准确率。

Conclusion: RSM-CoDG为EEG跨被试情感识别提供了统一、有效且鲁棒的解决方案，并提升了模型泛化能力。

Abstract: Cross-subject EEG-based emotion recognition (EER) remains challenging due to strong inter-subject variability, which induces substantial distribution shifts in EEG signals, as well as the high complexity of emotion-related neural representations in both spatial organization and temporal evolution. Existing approaches typically improve spatial modeling, temporal modeling, or generalization strategies in isolation, which limits their ability to align representations across subjects while capturing multi-scale dynamics and suppressing subject-specific bias within a unified framework. To address these gaps, we propose a Region-aware Spatiotemporal Modeling framework with Collaborative Domain Generalization (RSM-CoDG) for cross-subject EEG emotion recognition. RSM-CoDG incorporates neuroscience priors derived from functional brain region partitioning to construct region-level spatial representations, thereby improving cross-subject comparability. It also employs multi-scale temporal modeling to characterize the dynamic evolution of emotion-evoked neural activity. In addition, the framework employs a collaborative domain generalization strategy, incorporating multidimensional constraints to reduce subject-specific bias in a fully unseen target subject setting, which enhances the generalization to unknown individuals. Extensive experimental results on SEED series datasets demonstrate that RSM-CoDG consistently outperforms existing competing methods, providing an effective approach for improving robustness. The source code is available at https://github.com/RyanLi-X/RSM-CoDG.

</details>


### [13] [Explainable Deepfake Detection with RL Enhanced Self-Blended Images](https://arxiv.org/abs/2601.15624)
*Ning Jiang,Dingheng Zeng,Yanhong Liu,Haiyang Yi,Shijie Yu,Minghe Weng,Haifeng Shen,Ying Li*

Main category: cs.CV

TL;DR: 本文提出了一种自动化生成带有因果链思路注释的数据框架，并结合强化学习提升多模态大模型（MLLM）在深度伪造检测中的可解释性与泛化能力，实现了与现有最佳方法媲美的检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法缺乏结果可解释性，而应用多模态大模型又面临高质量标注数据稀缺与成本高的问题，此外，强化学习已被证明对视觉任务有性能提升及泛化优势，因此亟需兼具低标注成本和高泛化能力的深度伪造检测方法。

Method: 1）提出基于自混合图像的自动化Chain-of-Thought（CoT）数据生成框架，用于生成带详细因果链注释的伪造检测训练数据；2）引入强化学习增强的深度伪造检测架构，设计奖励机制，采用反馈驱动的合成数据生成以提升泛化能力。

Result: 在多个跨数据集基准上，新方法展现出与当前最新方法媲美的检测性能；实验验证了所提CoT构建流程、奖励机制与合成数据生成方法的有效性。

Conclusion: 该方法在保证低标注成本的情况下，提升了深度伪造检测的可解释性与泛化能力，为相关领域引入强化学习和自动化注释提供了新思路。

Abstract: Most prior deepfake detection methods lack explainable outputs. With the growing interest in multimodal large language models (MLLMs), researchers have started exploring their use in interpretable deepfake detection. However, a major obstacle in applying MLLMs to this task is the scarcity of high-quality datasets with detailed forgery attribution annotations, as textual annotation is both costly and challenging - particularly for high-fidelity forged images or videos. Moreover, multiple studies have shown that reinforcement learning (RL) can substantially enhance performance in visual tasks, especially in improving cross-domain generalization. To facilitate the adoption of mainstream MLLM frameworks in deepfake detection with reduced annotation cost, and to investigate the potential of RL in this context, we propose an automated Chain-of-Thought (CoT) data generation framework based on Self-Blended Images, along with an RL-enhanced deepfake detection framework. Extensive experiments validate the effectiveness of our CoT data construction pipeline, tailored reward mechanism, and feedback-driven synthetic data generation approach. Our method achieves performance competitive with state-of-the-art (SOTA) approaches across multiple cross-dataset benchmarks. Implementation details are available at https://github.com/deon1219/rlsbi.

</details>


### [14] [Evolving Without Ending: Unifying Multimodal Incremental Learning for Continual Panoptic Perception](https://arxiv.org/abs/2601.15643)
*Bo Yuan,Danpei Zhao,Wentao Li,Tian Li,Zhiguo Jiang*

Main category: cs.CV

TL;DR: 提出了一种端到端的多任务、多模态持续全景感知（CPP）模型，应对持续学习中灾难性遗忘和多模态语义错乱问题，在各种任务和数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习主要聚焦于单一任务，限制了其在多任务和多模态场景中的潜力；而多任务下持续学习不仅有灾难性遗忘，还面临多模态语义对齐混淆，导致模型性能严重下降。

Method: 提出了CPP模型，包含协作式跨模态编码器（CCE），通过对比特征蒸馏和实例蒸馏实现知识继承，结合跨模态一致性约束，并引入不依赖样本回放的非对称伪标签方式；设计了CPP+用于多任务增量过程中的多模态语义对齐。

Result: 在多模态数据集与不同类型的持续学习任务中，CPP及CPP+模型获得了显著优于现有方法的结果，特别适用于细粒度持续学习任务。

Conclusion: CPP模型有效解决了多任务多模态持续学习下的遗忘与语义对齐等核心难题，能在不回放历史样本的情况下实现模型进化，对未来多任务多模态智能感知有重要价值。

Abstract: Continual learning (CL) is a great endeavour in developing intelligent perception AI systems. However, the pioneer research has predominantly focus on single-task CL, which restricts the potential in multi-task and multimodal scenarios. Beyond the well-known issue of catastrophic forgetting, the multi-task CL also brings semantic obfuscation across multimodal alignment, leading to severe model degradation during incremental training steps. In this paper, we extend CL to continual panoptic perception (CPP), integrating multimodal and multi-task CL to enhance comprehensive image perception through pixel-level, instance-level, and image-level joint interpretation. We formalize the CL task in multimodal scenarios and propose an end-to-end continual panoptic perception model. Concretely, CPP model features a collaborative cross-modal encoder (CCE) for multimodal embedding. We also propose a malleable knowledge inheritance module via contrastive feature distillation and instance distillation, addressing catastrophic forgetting from task-interactive boosting manner. Furthermore, we propose a cross-modal consistency constraint and develop CPP+, ensuring multimodal semantic alignment for model updating under multi-task incremental scenarios. Additionally, our proposed model incorporates an asymmetric pseudo-labeling manner, enabling model evolving without exemplar replay. Extensive experiments on multimodal datasets and diverse CL tasks demonstrate the superiority of the proposed model, particularly in fine-grained CL tasks.

</details>


### [15] [SuperOcc: Toward Cohesive Temporal Modeling for Superquadric-based Occupancy Prediction](https://arxiv.org/abs/2601.15644)
*Zichen Yu,Quanli Liu,Wei Wang,Liyong Zhang,Xiaoguang Zhao*

Main category: cs.CV

TL;DR: 本论文提出了SuperOcc框架，利用超二次体(suqerquadric)进行稀疏且高效的三维占据预测，并在主流基准上取得了最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有三维占据预测多采用稠密场景表达，忽略了现实场景的稀疏性。近年来，超二次体因其良好几何表达能力成为稀疏表示的有力竞争者，但尚存时序建模能力弱、查询稀疏与表达能力权衡困难及映射效率低下等问题。

Method: 提出SuperOcc框架，具体包括：（1）融合视角与目标中心的时序建模机制；（2）多超二次体解码策略提升几何表达能力且不增加查询稀疏性损失；（3）高效的超二次体到体素(splatting)映射算法，以提高整体计算效率。

Result: 在SurroundOcc和Occ3D两个基准上进行大量实验，SuperOcc在保证高效率的同时，取得了最新的最优性能（state-of-the-art）。

Conclusion: SuperOcc通过创新的时序建模、多体解码及高效映射机制，有效解决了超二次体三维占据预测的主要痛点，实现了兼具稀疏性、表达性与高效性的三维理解。

Abstract: 3D occupancy prediction plays a pivotal role in the realm of autonomous driving, as it provides a comprehensive understanding of the driving environment. Most existing methods construct dense scene representations for occupancy prediction, overlooking the inherent sparsity of real-world driving scenes. Recently, 3D superquadric representation has emerged as a promising sparse alternative to dense scene representations due to the strong geometric expressiveness of superquadrics. However, existing superquadric frameworks still suffer from insufficient temporal modeling, a challenging trade-off between query sparsity and geometric expressiveness, and inefficient superquadric-to-voxel splatting. To address these issues, we propose SuperOcc, a novel framework for superquadric-based 3D occupancy prediction. SuperOcc incorporates three key designs: (1) a cohesive temporal modeling mechanism to simultaneously exploit view-centric and object-centric temporal cues; (2) a multi-superquadric decoding strategy to enhance geometric expressiveness without sacrificing query sparsity; and (3) an efficient superquadric-to-voxel splatting scheme to improve computational efficiency. Extensive experiments on the SurroundOcc and Occ3D benchmarks demonstrate that SuperOcc achieves state-of-the-art performance while maintaining superior efficiency. The code is available at https://github.com/Yzichen/SuperOcc.

</details>


### [16] [Event-VStream: Event-Driven Real-Time Understanding for Long Video Streams](https://arxiv.org/abs/2601.15655)
*Zhenghui Guo,Yuanbin Man,Junyuan Sheng,Bowen Lin,Ahmed Ahmed,Bo Jiang,Boyuan Zhang,Miao Yin,Sian Jin,Omprakash Gnawal,Chengming Zhang*

Main category: cs.CV

TL;DR: 本文提出Event-VStream，一个针对长视频实时理解的事件感知多模态大模型新框架，有效解决了冗余帧处理和上下文遗忘问题，在多个长视频基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大模型在处理长视频流时面临实时理解的难点，如重复处理无关帧、快速遗忘早期信息。现有方法通过定时解码或缓存修剪，导致输出重复或关键信息丢失。因此，需要更高效、结构化的方法来理解长视频内容。

Method: Event-VStream通过将连续视频流分割为一系列离散的、语义一致的事件来建模，利用运动、语义和预测线索检测关键状态变化点，在事件边界处触发文本生成。每个事件嵌入被存入持久记忆库，结合历史进行长时推理，同时保证低延迟。

Result: Event-VStream在OVOBench-Realtime和长期Ego4D数据集上表现突出；比VideoLLM-Online-8B在OVOBench-Realtime提升10.4分，且仅用通用LLaMA-3-8B主干实现接近Flash-VStream-7B的水平，Ego4D长视频处理上约70%场景击败GPT-5。

Conclusion: Event-VStream通过事件驱动和持久记忆机制，实现了低延迟、高效、长时的多模态视频理解，优于现有主流方法，具备广泛应用潜力。

Abstract: Real-time understanding of long video streams remains challenging for multimodal large language models (VLMs) due to redundant frame processing and rapid forgetting of past context. Existing streaming systems rely on fixed-interval decoding or cache pruning, which either produce repetitive outputs or discard crucial temporal information. We introduce Event-VStream, an event-aware framework that represents continuous video as a sequence of discrete, semantically coherent events. Our system detects meaningful state transitions by integrating motion, semantic, and predictive cues, and triggers language generation only at those boundaries. Each event embedding is consolidated into a persistent memory bank, enabling long-horizon reasoning while maintaining low latency. Across OVOBench-Realtime, and long-form Ego4D evaluations, Event-VStream achieves competitive performance. It improves over a VideoLLM-Online-8B baseline by +10.4 points on OVOBench-Realtime, achieves performance close to Flash-VStream-7B despite using only a general-purpose LLaMA-3-8B text backbone, and maintains around 70% GPT-5 win rate on 2-hour Ego4D streams.

</details>


### [17] [Skywork UniPic 3.0: Unified Multi-Image Composition via Sequence Modeling](https://arxiv.org/abs/2601.15664)
*Hongyang Wei,Hongbo Liu,Zidong Wang,Yi Peng,Baixin Xu,Size Wu,Xuying Zhang,Xianglong He,Zexiang Liu,Peiyu Wang,Xuchen Song,Yangguang Li,Yang Liu,Yahui Zhou*

Main category: cs.CV

TL;DR: 提出了一种新的多图合成与编辑模型Skywork UniPic 3.0，性能超越了当前主流方法，并公开了全部资源。


<details>
  <summary>Details</summary>
Motivation: 多图像合成对于一致性和质量要求极高，但现有热门方法并未公开具体实现细节，且社区对基于人-物交互（HOI）的合成需求最大。

Method: 1. 设计了统一的多模态模型框架，支持单图编辑与多图合成，输入图像数1~6，分辨率灵活；2. 构建了系统性的数据收集、筛选和合成流水线，利用70万高质量样本进行训练；3. 提出将多图合成建模为序列建模问题，将条件生成转为统一的序列合成；4. 引入轨迹映射和分布匹配机制于训练后阶段，实现推理8步、加速12.5倍的高效生成。

Result: 在单图编辑和多图合成基准上均取得了最新最强的效果，超越了Nano-Banana和Seedream 4.0等主流模型。

Conclusion: Skywork UniPic 3.0证明了合理数据和创新训练范式对提升合成质量和效率的价值，推动了多图合成技术的发展。

Abstract: The recent surge in popularity of Nano-Banana and Seedream 4.0 underscores the community's strong interest in multi-image composition tasks. Compared to single-image editing, multi-image composition presents significantly greater challenges in terms of consistency and quality, yet existing models have not disclosed specific methodological details for achieving high-quality fusion. Through statistical analysis, we identify Human-Object Interaction (HOI) as the most sought-after category by the community. We therefore systematically analyze and implement a state-of-the-art solution for multi-image composition with a primary focus on HOI-centric tasks. We present Skywork UniPic 3.0, a unified multimodal framework that integrates single-image editing and multi-image composition. Our model supports an arbitrary (1~6) number and resolution of input images, as well as arbitrary output resolutions (within a total pixel budget of 1024x1024). To address the challenges of multi-image composition, we design a comprehensive data collection, filtering, and synthesis pipeline, achieving strong performance with only 700K high-quality training samples. Furthermore, we introduce a novel training paradigm that formulates multi-image composition as a sequence-modeling problem, transforming conditional generation into unified sequence synthesis. To accelerate inference, we integrate trajectory mapping and distribution matching into the post-training stage, enabling the model to produce high-fidelity samples in just 8 steps and achieve a 12.5x speedup over standard synthesis sampling. Skywork UniPic 3.0 achieves state-of-the-art performance on single-image editing benchmark and surpasses both Nano-Banana and Seedream 4.0 on multi-image composition benchmark, thereby validating the effectiveness of our data pipeline and training paradigm. Code, models and dataset are publicly available.

</details>


### [18] [Consistency-Regularized GAN for Few-Shot SAR Target Recognition](https://arxiv.org/abs/2601.15681)
*Yikui Zhai,Shikuang Liu,Wenlve Zhou,Hongsheng Zhang,Zhiheng Zhou,Xiaolin Tian,C. L. Philip Chen*

Main category: cs.CV

TL;DR: 本文针对SAR图像中的小样本识别难题，提出了一种名为Cr-GAN的新型生成对抗网络，可在极少量数据下生成多样且高保真的合成数据，显著提升下游任务准确率。


<details>
  <summary>Details</summary>
Motivation: 合成孔径雷达（SAR）图像常面临极度样本稀缺问题，制约实际应用。虽然生成对抗网络（GAN）和自监督学习（SSL）结合能缓解数据稀缺，但GAN本身又需大量数据训练，形成方法上的悖论。为打破这一困境，需要突破现有GAN对数据量的依赖。

Method: 提出一致性正则化生成对抗网络（Cr-GAN），包含双分支判别器以解耦对抗训练与表示学习，结合通道特征插值策略和双域循环一致性机制，确保生成数据丰富性和语义一致性。Cr-GAN可适配多种GAN架构，生成数据用于增强各类自监督学习。

Result: 在MSTAR和SRSDD两个数据集8-shot设置下，Cr-GAN分别取得了71.21%和51.64%的准确率，显著优于主流基线模型，同时其参数量仅为最新扩散模型的1/5左右。

Conclusion: Cr-GAN无需大量真实数据即可生成高质量合成样本，极大促进了SAR小样本识别性能提升，是解决数据稀缺情况下深度学习应用的有效方案。

Abstract: Few-shot recognition in synthetic aperture radar (SAR) imagery remains a critical bottleneck for real-world applications due to extreme data scarcity. A promising strategy involves synthesizing a large dataset with a generative adversarial network (GAN), pre-training a model via self-supervised learning (SSL), and then fine-tuning on the few labeled samples. However, this approach faces a fundamental paradox: conventional GANs themselves require abundant data for stable training, contradicting the premise of few-shot learning. To resolve this, we propose the consistency-regularized generative adversarial network (Cr-GAN), a novel framework designed to synthesize diverse, high-fidelity samples even when trained under these severe data limitations. Cr-GAN introduces a dual-branch discriminator that decouples adversarial training from representation learning. This architecture enables a channel-wise feature interpolation strategy to create novel latent features, complemented by a dual-domain cycle consistency mechanism that ensures semantic integrity. Our Cr-GAN framework is adaptable to various GAN architectures, and its synthesized data effectively boosts multiple SSL algorithms. Extensive experiments on the MSTAR and SRSDD datasets validate our approach, with Cr-GAN achieving a highly competitive accuracy of 71.21% and 51.64%, respectively, in the 8-shot setting, significantly outperforming leading baselines, while requiring only ~5 of the parameters of state-of-the-art diffusion models. Code is available at: https://github.com/yikuizhai/Cr-GAN.

</details>


### [19] [Performance-guided Reinforced Active Learning for Object Detection](https://arxiv.org/abs/2601.15688)
*Zhixuan Liang,Xingyu Zeng,Rui Zhao,Ping Luo*

Main category: cs.CV

TL;DR: 该论文提出了一种新型的主动学习方法（MGRAL），通过强化学习优化采样策略，直接以检测任务中的mAP提升为奖励，显著提升了目标检测任务下主动学习的效率和精度。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习方法评估数据信息量时，通常只关注数据分布或内在信息含量，很少直接与下游任务（如目标检测中的mAP）关联，导致挑选出来的数据不一定对实际性能提升最有效。因此需要一种与任务性能强相关的数据选择策略。

Method: 方法名为MGRAL（Performance-guided Reinforced Active Learning）。主要思路：1）采用“预期输出变化”作为数据的信息量指标；2）用强化学习的采样代理，通过策略梯度，以提升mAP为奖励信号，优化每一批数据的选择；3）为降低未标注样本上mAP估算的计算量，创新性引入无监督方式和高效查找表进行加速。

Result: 在PASCAL VOC与COCO两个主流目标检测基准上，MGRAL方法的主动学习曲线优于其它方法，并有令人信服的可视化佐证。

Conclusion: MGRAL建立了一种以强化学习驱动、mAP直接指导的主动学习新范式，可显著提高目标检测的主动学习性能，同时保证了方法的可部署性。

Abstract: Active learning (AL) strategies aim to train high-performance models with minimal labeling efforts, only selecting the most informative instances for annotation. Current approaches to evaluating data informativeness predominantly focus on the data's distribution or intrinsic information content and do not directly correlate with downstream task performance, such as mean average precision (mAP) in object detection. Thus, we propose Performance-guided (i.e. mAP-guided) Reinforced Active Learning for Object Detection (MGRAL), a novel approach that leverages the concept of expected model output changes as informativeness. To address the combinatorial explosion challenge of batch sample selection and the non-differentiable correlation between model performance and selected batches, MGRAL skillfully employs a reinforcement learning-based sampling agent that optimizes selection using policy gradient with mAP improvement as reward. Moreover, to reduce the computational overhead of mAP estimation with unlabeled samples, MGRAL utilizes an unsupervised way with fast look-up tables, ensuring feasible deployment. We evaluate MGRAL's active learning performance on detection tasks over PASCAL VOC and COCO benchmarks. Our approach demonstrates the highest AL curve with convincing visualizations, establishing a new paradigm in reinforcement learning-driven active object detection.

</details>


### [20] [Beyond Visual Safety: Jailbreaking Multimodal Large Language Models for Harmful Image Generation via Semantic-Agnostic Inputs](https://arxiv.org/abs/2601.15698)
*Mingyu Yu,Lana Liu,Zhehao Zhao,Wei Wang,Sujuan Qin*

Main category: cs.CV

TL;DR: 本文提出了BVS框架，成功利用图文对突破了多模态大语言模型（MLLMs）的视觉安全界限，并在GPT-5上取得了极高的越狱成功率。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型的安全性研究主要集中在文本和视觉的交叉点，但对视觉安全边界的探索还不充分，因此有必要开发更有效的方法揭示视觉安全漏洞。

Method: 文中提出了Beyond Visual Safety（BVS）框架，采用“重构-再生成”策略，结合中立化视觉拼接与归纳重组手段，将恶意意图与原始输入解耦，引诱MLLMs生成有害图像，从而测试其视觉安全界限。

Result: 通过对GPT-5（2026年1月12日版）实验，BVS框架实现了高达98.21%的越狱成功率，充分说明该方法在突破MLLMs视觉安全防护上效果显著。

Conclusion: 当前MLLMs在视觉安全对齐方面存在重大漏洞，BVS的高效越狱能力凸显了现有防护措施的不足，亟需加强多模态模型的视觉安全机制。

Abstract: The rapid advancement of Multimodal Large Language Models (MLLMs) has introduced complex security challenges, particularly at the intersection of textual and visual safety. While existing schemes have explored the security vulnerabilities of MLLMs, the investigation into their visual safety boundaries remains insufficient. In this paper, we propose Beyond Visual Safety (BVS), a novel image-text pair jailbreaking framework specifically designed to probe the visual safety boundaries of MLLMs. BVS employs a "reconstruction-then-generation" strategy, leveraging neutralized visual splicing and inductive recomposition to decouple malicious intent from raw inputs, thereby leading MLLMs to be induced into generating harmful images. Experimental results demonstrate that BVS achieves a remarkable jailbreak success rate of 98.21\% against GPT-5 (12 January 2026 release). Our findings expose critical vulnerabilities in the visual safety alignment of current MLLMs.

</details>


### [21] [Keyframe-Based Feed-Forward Visual Odometry](https://arxiv.org/abs/2601.16020)
*Weichen Dai,Wenhan Su,Da Kong,Yuhang Ming,Wanzeng Kong*

Main category: cs.CV

TL;DR: 该论文提出通过强化学习自适应选择关键帧，有效提升视觉基础模型在位姿估计和重建任务中的效率与精度，实验取得优于现有方法的成果。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉基础模型的VO和SLAM方法虽然整合了位姿估计及重建，但未采纳关键帧策略，导致计算冗余、效率低下和性能下降。因此需引入关键帧选择机制以提升效率和精度。

Method: 作者提出一种基于关键帧的直接前馈VO框架。创新点在于用强化学习来自动学习关键帧选择策略，而不用手工设计启发式规则。训练使用TartanAir等数据集，并在多真实场景数据集上测试。

Result: 提出的方法在多个真实世界数据集上获得了比现有前馈VO方法更高的准确率和效率，验证了自适应关键帧策略的有效性。

Conclusion: 将强化学习驱动的关键帧策略与视觉基础模型结合，显著提高了VO系统的表现，为基础模型和传统几何方法的结合提供了新思路。

Abstract: The emergence of visual foundation models has revolutionized visual odometry~(VO) and SLAM, enabling pose estimation and dense reconstruction within a single feed-forward network. However, unlike traditional pipelines that leverage keyframe methods to enhance efficiency and accuracy, current foundation model based methods, such as VGGT-Long, typically process raw image sequences indiscriminately. This leads to computational redundancy and degraded performance caused by low inter-frame parallax, which provides limited contextual stereo information. Integrating traditional geometric heuristics into these methods is non-trivial, as their performance depends on high-dimensional latent representations rather than explicit geometric metrics. To bridge this gap, we propose a novel keyframe-based feed-forward VO. Instead of relying on hand-crafted rules, our approach employs reinforcement learning to derive an adaptive keyframe policy in a data-driven manner, aligning selection with the intrinsic characteristics of the underlying foundation model. We train our agent on TartanAir dataset and conduct extensive evaluations across several real-world datasets. Experimental results demonstrate that the proposed method achieves consistent and substantial improvements over state-of-the-art feed-forward VO methods.

</details>


### [22] [Enhanced LULC Segmentation via Lightweight Model Refinements on ALOS-2 SAR Data](https://arxiv.org/abs/2601.15705)
*Ali Caglayan,Nevrez Imamoglu,Toru Kouyama*

Main category: cs.CV

TL;DR: 本研究利用ALOS-2单极化SAR数据，实现了对日本全国范围的土地利用/覆盖（LULC）语义分割，并同时提升了水体检测任务的表现，通过轻量级结构改进显著提升了模型在边界、细结构和少见类别上的表现。


<details>
  <summary>Details</summary>
Motivation: 常规SAR影像在密集预测任务中普遍面临边界过度平滑、细长结构遗漏以及长尾类别表现退化等问题，亟需有效解决办法且不能显著提高计算复杂度。

Method: 基于先前SAR-W-MixMAE自监督预训练方案，作者提出三项改进：1）在多尺度解码过程中引入高分辨特征；2）采用逐步卷积与上采样交替的refine-up head；3）在loss函数中加权设计α系数以控制类别重加权。

Result: 新模型在日本全国ALOS-2 LULC基准上取得一致性提升，特别对少见类别表现更好，且水体检测任务在主流评价指标上的表现都获得提升。

Conclusion: 提出的轻量级结构改进在不增加模型复杂度的情况下，有效改善了SAR语义分割中的常见难题，显著提升了分割与水体检测等应用的准确性。

Abstract: This work focuses on national-scale land-use/land-cover (LULC) semantic segmentation using ALOS-2 single-polarization (HH) SAR data over Japan, together with a companion binary water detection task. Building on SAR-W-MixMAE self-supervised pretraining [1], we address common SAR dense-prediction failure modes, boundary over-smoothing, missed thin/slender structures, and rare-class degradation under long-tailed labels, without increasing pipeline complexity. We introduce three lightweight refinements: (i) injecting high-resolution features into multi-scale decoding, (ii) a progressive refine-up head that alternates convolutional refinement and stepwise upsampling, and (iii) an $α$-scale factor that tempers class reweighting within a focal+dice objective. The resulting model yields consistent improvements on the Japan-wide ALOS-2 LULC benchmark, particularly for under-represented classes, and improves water detection across standard evaluation metrics.

</details>


### [23] [DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models](https://arxiv.org/abs/2601.16065)
*Chenyang Li,Jieyuan Liu,Bin Li,Bo Gao,Yilin Yuan,Yangfan He,Yuchen Li,Jingqun Tang*

Main category: cs.CV

TL;DR: 本文提出了一种针对视觉-语言动作（VLA）模型容易关注任务无关区域的问题的解决框架：干扰性标记剪枝（DTP）。该方法通过动态检测和剪除图像中无关标记，提升了机器人操作任务的成功率。


<details>
  <summary>Details</summary>
Motivation: VLA模型在机器人操作任务中虽有突破，但往往过度关注任务无关区域（干扰性标记），降低行动生成的准确性，从而影响任务完成率。

Method: 提出了一种可即插即用的DTP框架：无需改变VLA模型原有架构或增加额外输入，动态检测并剪除与任务无关的图像标记，纠正模型视觉注意力分布。

Result: 在SIMPLER Benchmark（Li et al., 2024）上的实验表明，该方法在多种VLA新模型中均能显著提升任务成功率，并能推广到基于transformer的VLA模型。进一步分析发现，对所有测试模型来说，任务无关区域的注意力越多，任务成功率越低。

Conclusion: DTP框架有效提升了VLA模型处理机器人操作任务的性能，揭示了注意力分布与任务表现之间的普遍关系，对VLA未来发展有指导意义。

Abstract: Vision-Language Action (VLA) models have shown remarkable progress in robotic manipulation by leveraging the powerful perception abilities of Vision-Language Models (VLMs) to understand environments and directly output actions. However, by default, VLA models may overly attend to image tokens in the task-irrelevant region, which we describe as 'distracting tokens'. This behavior can disturb the model from the generation of the desired action tokens in each step, affecting the success rate of tasks. In this paper, we introduce a simple yet effective plug-and-play Distracting Token Pruning (DTP) framework, which dynamically detects and prunes these distracting image tokens. By correcting the model's visual attention patterns, we aim to improve the task success rate, as well as exploring the performance upper boundaries of the model without altering its original architecture or adding additional inputs. Experiments on the SIMPLER Benchmark (Li et al., 2024) show that our method consistently achieving relative improvements in task success rates across different types of novel VLA models, demonstrating generalizability to transformer-based VLAs. Further analysis reveals a negative correlation between the task success rate and the amount of attentions in the task-irrelevant region for all models tested, highlighting a common phenomenon of VLA models that could guide future research. We also publish our code at: https://anonymous.4open.science/r/CBD3.

</details>


### [24] [Zero-Shot Product Attribute Labeling with Vision-Language Models: A Three-Tier Evaluation Framework](https://arxiv.org/abs/2601.15711)
*Shubham Shukla,Kunal Sonalkar*

Main category: cs.CV

TL;DR: 论文提出了一个用于评估时尚图像多属性细粒度预测的三层框架，并用其系统评估多种视觉-语言模型（VLMs），发现VLMs在类别可用性检测方面仍面临显著挑战。


<details>
  <summary>Details</summary>
Motivation: 时尚零售业务如目录扩充、视觉搜索和推荐系统需依赖细粒度属性预测。现有VLMs虽无需任务特定训练即可零样本预测，但在多属性任务中尤其涉及属性适用性（如某些属性仅对特定服装有意义）时，缺乏系统性评测方法。

Method: 提出了三层评估框架：1）整体任务表现，包含不可用（NA）类；2）属性适用性识别（判断属性是否可预测）；3）已确定属性下的细粒度分类。基于DeepFashion-MultiModal数据集，定义了NA标签，并用9种主流与高效VLM以及Fashion-CLIP基准方法，在5000张图片和18种属性上进行对比。

Result: 1）VLM零样本宏F1达到64%，是Fashion-CLIP回归的三倍；2）VLM在细粒度分类（F1=70.8%）上表现突出，但在属性适用性判断（NA-F1=34.1%）上较弱，成为关键瓶颈；3）高效模型已可达旗舰模型90%以上性能，成本更优。

Conclusion: 该三层评测框架有助于实践中精准诊断错误来源，为生产系统有针对性提升提供指导。VLMs虽然极具潜力，但在属性可用性检测方面还需重点攻关。

Abstract: Fine-grained attribute prediction is essential for fashion retail applications including catalog enrichment, visual search, and recommendation systems. Vision-Language Models (VLMs) offer zero-shot prediction without task-specific training, yet their systematic evaluation on multi-attribute fashion tasks remains underexplored. A key challenge is that fashion attributes are often conditional. For example, "outer fabric" is undefined when no outer garment is visible. This requires models to detect attribute applicability before attempting classification. We introduce a three-tier evaluation framework that decomposes this challenge: (1) overall task performance across all classes (including NA class: suggesting attribute is not applicable) for all attributes, (2) attribute applicability detection, and (3) fine-grained classification when attributes are determinable. Using DeepFashion-MultiModal, which explicitly defines NA (meaning attribute doesn't exist or is not visible) within attribute label spaces, we benchmark nine VLMs spanning flagship (GPT-5, Gemini 2.5 Pro), efficient (GPT-5 Mini, Gemini 2.5 Flash), and ultra-efficient tiers (GPT-5 Nano, Gemini 2.5 Flash-Lite) against classifiers trained on pretrained Fashion-CLIP embeddings on 5,000 images across 18 attributes. Our findings reveal that: (1) zero-shot VLMs achieve 64.0% macro-F1, a threefold improvement over logistic regression on pretrained Fashion-CLIP embeddings; (2) VLMs excel at fine-grained classification (Tier 3: 70.8% F1) but struggle with applicability detection (Tier 2: 34.1% NA-F1), identifying a key bottleneck; (3) efficient models achieve over 90% of flagship performance at lower cost, offering practical deployment paths. This diagnostic framework enables practitioners to pinpoint whether errors stem from visibility detection or classification, guiding targeted improvements for production systems.

</details>


### [25] [VideoThinker: Building Agentic VideoLLMs with LLM-Guided Tool Reasoning](https://arxiv.org/abs/2601.15724)
*Chenglin Li,Qianglong Chen,Feng Han,Yikun Wang,Xingxi Yin,Yan Gong,Ruilin Li,Yin Zhang,Jiaqi Wang*

Main category: cs.CV

TL;DR: VideoThinker提出了一种全新方法，通过合成的工具交互轨迹训练具有Agent特性的长视频大模型，显著提升了长视频理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频大模型在长视频处理上依赖静态推理和均匀采样，导致时序定位弱和信息丢失。Agentic工具如时序检索、空间缩放等可自适应探索关键片段，但现有数据集和训练方式难以同时兼顾自适应能力和长期理解基础，形成鸡生蛋蛋生鸡的问题。

Method: 1. 将视频转为丰富的文本描述。2. 利用Agentic语言模型在“描述空间”生成多步工具使用序列。3. 再将描述转回对应帧，生成大规模交错（视频+工具推理）数据集。4. 全部基于合成数据训练VideoThinker，无需已有强大的长视频理解能力。

Result: VideoThinker实现了动态推理、自适应时域探索、多步工具使用能力；在长视频基准上显著优于仅基于描述的模型和强基线视频模型。

Conclusion: 基于合成工具交互数据+自适应检索与缩放推理，是提升长视频理解有效途径，VideoThinker验证了该思路的高效性。

Abstract: Long-form video understanding remains a fundamental challenge for current Video Large Language Models. Most existing models rely on static reasoning over uniformly sampled frames, which weakens temporal localization and leads to substantial information loss in long videos. Agentic tools such as temporal retrieval, spatial zoom, and temporal zoom offer a natural way to overcome these limitations by enabling adaptive exploration of key moments. However, constructing agentic video understanding data requires models that already possess strong long-form video comprehension, creating a circular dependency. We address this challenge with VideoThinker, an agentic Video Large Language Model trained entirely on synthetic tool interaction trajectories. Our key idea is to convert videos into rich captions and employ a powerful agentic language model to generate multi-step tool use sequences in caption space. These trajectories are subsequently grounded back to video by replacing captions with the corresponding frames, yielding a large-scale interleaved video and tool reasoning dataset without requiring any long-form understanding from the underlying model. Training on this synthetic agentic dataset equips VideoThinker with dynamic reasoning capabilities, adaptive temporal exploration, and multi-step tool use. Remarkably, VideoThinker significantly outperforms both caption-only language model agents and strong video model baselines across long-video benchmarks, demonstrating the effectiveness of tool augmented synthetic data and adaptive retrieval and zoom reasoning for long-form video understanding.

</details>


### [26] [FAIR-ESI: Feature Adaptive Importance Refinement for Electrophysiological Source Imaging](https://arxiv.org/abs/2601.15731)
*Linyong Zou,Liang Zhang,Xiongfei Wang,Jia-Hong Gao,Yi Sun,Shurong Sheng,Kuntao Xiao,Wanli Yang,Pengfei Teng,Guoming Luan,Zhao Lv,Zikang Xu*

Main category: cs.CV

TL;DR: 本文提出了一种用于脑疾病诊断的新型电生理源成像（ESI）特征自适应精炼框架FAIR-ESI，并在仿真与实际临床数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前ESI中的特征选择和精炼会极大影响成像准确性，但该难题尚未得到有效解决，因此需要一种能够自适应优化特征的框架提升脑疾病诊断的准确率。

Method: 提出FAIR-ESI框架，融合了FFT频谱特征精炼、加权时域特征精炼以及基于自注意力的分块特征精炼，从多个角度对特征进行自适应调整。

Result: 在两个多样配置的仿真数据集和两个真实临床数据集上的大量实验表明，FAIR-ESI在特征提取及诊断性能上优于现有方法。

Conclusion: FAIR-ESI能够提升ESI的特征精炼能力，有助于脑疾病诊断的准确性，并为脑功能研究提供新见解。

Abstract: An essential technique for diagnosing brain disorders is electrophysiological source imaging (ESI). While model-based optimization and deep learning methods have achieved promising results in this field, the accurate selection and refinement of features remains a central challenge for precise ESI. This paper proposes FAIR-ESI, a novel framework that adaptively refines feature importance across different views, including FFT-based spectral feature refinement, weighted temporal feature refinement, and self-attention-based patch-wise feature refinement. Extensive experiments on two simulation datasets with diverse configurations and two real-world clinical datasets validate our framework's efficacy, highlighting its potential to advance brain disorder diagnosis and offer new insights into brain function.

</details>


### [27] [Sub-Region-Aware Modality Fusion and Adaptive Prompting for Multi-Modal Brain Tumor Segmentation](https://arxiv.org/abs/2601.15734)
*Shadi Alijani,Fereshteh Aghaee Meibodi,Homayoun Najjaran*

Main category: cs.CV

TL;DR: 本文提出了一种新的基础模型自适应多模态医学影像的框架，通过子区域感知的模态注意力和自适应提示工程有效提升了肿瘤分割的性能，在BraTS 2020脑肿瘤分割数据集上显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有的基础模型在多模态医学影像上难以有效融合多源信息，且无法适应病理组织的异质性，导致分割表现不佳，尤其是在复杂的组织亚区。

Method: 提出了两个技术创新：（1）子区域感知的模态注意力机制，使模型可以针对不同肿瘤子区自适应地选择最佳模态组合；（2）自适应提示工程，充分利用基础模型的能力提升分割精度。

Result: 在BraTS 2020脑肿瘤分割数据集上，提出的方法在所有子区，尤其是坏死核心等难分割区，显著优于现有基线方法。

Conclusion: 本文方法为多模态融合与提示提供了一种有效范式，为基础模型在医学影像领域的精确与鲁棒应用打下了基础。

Abstract: The successful adaptation of foundation models to multi-modal medical imaging is a critical yet unresolved challenge. Existing models often struggle to effectively fuse information from multiple sources and adapt to the heterogeneous nature of pathological tissues. To address this, we introduce a novel framework for adapting foundation models to multi-modal medical imaging, featuring two key technical innovations: sub-region-aware modality attention and adaptive prompt engineering. The attention mechanism enables the model to learn the optimal combination of modalities for each tumor sub-region, while the adaptive prompting strategy leverages the inherent capabilities of foundation models to refine segmentation accuracy. We validate our framework on the BraTS 2020 brain tumor segmentation dataset, demonstrating that our approach significantly outperforms baseline methods, particularly in the challenging necrotic core sub-region. Our work provides a principled and effective approach to multi-modal fusion and prompting, paving the way for more accurate and robust foundation model-based solutions in medical imaging.

</details>


### [28] [Breaking the Resolution Barrier: Arbitrary-resolution Deep Image Steganography Framework](https://arxiv.org/abs/2601.15739)
*Xinjue Hu,Chi Wang,Boyu Wang,Xiang Zhang,Zhenshan Tan,Zhangjie Fu*

Main category: cs.CV

TL;DR: 该论文提出了ARDIS，一种支持任意分辨率秘密图像嵌入与恢复的新型深度图像隐写框架。通过频率解耦和隐式重建方式，实现了比现有方法更高的隐写隐蔽性及跨分辨率恢复保真度。


<details>
  <summary>Details</summary>
Motivation: 现有深度图像隐写方法要求秘密图像与覆盖图像分辨率一致，导致分辨率不匹配时需要重采样，恢复时细节损失严重，且分辨率信息未知时无法还原原始图像。

Method: 1）提出频率解耦结构，将秘密图像分解为分辨率对齐的全局基线和分辨率无关的高频隐变量嵌入固定分辨率的覆盖图像；2）提出基于隐变量引导的隐式重建方法，让恢复过程中，隐变量调制隐式函数精确渲染高频细节，还原真实信息；3）提出隐式分辨率编码方案，实现隐藏分辨率信息并支持盲恢复。

Result: 实验结果表明，ARDIS在隐写的隐蔽性以及跨分辨率恢复保真度方面显著超过当前最新方法。

Conclusion: ARDIS实现了同领域首个支持任意分辨率的深度图像隐写。新范式有效减少了分辨率不匹配导致的细节损失，为实际应用提供了更强的灵活性和性能。

Abstract: Deep image steganography (DIS) has achieved significant results in capacity and invisibility. However, current paradigms enforce the secret image to maintain the same resolution as the cover image during hiding and revealing. This leads to two challenges: secret images with inconsistent resolutions must undergo resampling beforehand which results in detail loss during recovery, and the secret image cannot be recovered to its original resolution when the resolution value is unknown. To address these, we propose ARDIS, the first Arbitrary Resolution DIS framework, which shifts the paradigm from discrete mapping to reference-guided continuous signal reconstruction. Specifically, to minimize the detail loss caused by resolution mismatch, we first design a Frequency Decoupling Architecture in hiding stage. It disentangles the secret into a resolution-aligned global basis and a resolution-agnostic high-frequency latent to hide in a fixed-resolution cover. Second, for recovery, we propose a Latent-Guided Implicit Reconstructor to perform deterministic restoration. The recovered detail latent code modulates a continuous implicit function to accurately query and render high-frequency residuals onto the recovered global basis, ensuring faithful restoration of original details. Furthermore, to achieve blind recovery, we introduce an Implicit Resolution Coding strategy. By transforming discrete resolution values into dense feature maps and hiding them in the redundant space of the feature domain, the reconstructor can correctly decode the secret's resolution directly from the steganographic representation. Experimental results demonstrate that ARDIS significantly outperforms state-of-the-art methods in both invisibility and cross-resolution recovery fidelity.

</details>


### [29] [White-Box mHC: Electromagnetic Spectrum-Aware and Interpretable Stream Interactions for Hyperspectral Image Classification](https://arxiv.org/abs/2601.15757)
*Yimin Zhu,Lincoln Linlin Xu,Zhengsen Xu,Zack Dewis,Mabel Heffring,Saeid Taleghanidoozdoozan,Motasem Alkayid,Quinn Ledingham,Megan Greenwood*

Main category: cs.CV

TL;DR: 本文提出了一种新的高光谱图像分类方法ES-mHC，通过物理光谱感知和结构化的交互方式，提高了模型的可解释性。实验结果显示模型交互结构清晰、规律，提升了对模型内部机制的理解。


<details>
  <summary>Details</summary>
Motivation: 现有高光谱图像分类深度学习模型多为“黑盒”，其混合特征难以解释，缺乏模型内部决策机制的透明度。作者希望提升模型可解释性与可视化能力。

Method: 提出ES-mHC框架，使用结构化、定向矩阵显式建模不同电磁谱分组之间的交互，将特征表示与交互结构分离，使电磁谱分组能够专用化，并减少冗余，同时可直接可视化信息流向和结构。

Result: 实验表明：1）ES-mHC学习到的超连接矩阵具有一致的空间模式和非对称交互行为，可用于分析内部机制；2）增加扩展率能加快结构化交互模式的出现。

Conclusion: ES-mHC将高光谱图像分类从全黑盒任务转变为具有结构透明性、部分白盒的学习过程，有助于理解和可视化深度模型的决策依据。

Abstract: In hyperspectral image classification (HSIC), most deep learning models rely on opaque spectral-spatial feature mixing, limiting their interpretability and hindering understanding of internal decision mechanisms. We present physical spectrum-aware white-box mHC, named ES-mHC, a hyper-connection framework that explicitly models interactions among different electromagnetic spectrum groupings (residual stream in mHC) interactions using structured, directional matrices. By separating feature representation from interaction structure, ES-mHC promotes electromagnetic spectrum grouping specialization, reduces redundancy, and exposes internal information flow that can be directly visualized and spatially analyzed. Using hyperspectral image classification as a representative testbed, we demonstrate that the learned hyper-connection matrices exhibit coherent spatial patterns and asymmetric interaction behaviors, providing mechanistic insight into the model internal dynamics. Furthermore, we find that increasing the expansion rate accelerates the emergence of structured interaction patterns. These results suggest that ES-mHC transforms HSIC from a purely black-box prediction task into a structurally transparent, partially white-box learning process.

</details>


### [30] [Atlas-Assisted Segment Anything Model for Fetal Brain MRI (FeTal-SAM)](https://arxiv.org/abs/2601.15759)
*Qi Zeng,Weide Liu,Bo Li,Ryne Didier,P. Ellen Grant,Davood Karimi*

Main category: cs.CV

TL;DR: FeTal-SAM是一种适用于胎儿脑MRI分割的新模型，采用多模板配准与提示，结合Segment Anything Model的基础模型理念，实现了无需针对不同标签反复训练即可灵活分割任意结构，且在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前的深度学习分割方法通常需要大量标注数据，且仅适用于一组固定标签，无法灵活应对临床或研究上的标签变化。此外，这些模型很难区分其分割结果是否源于真实影像对比度还是仅是学习到了空间先验。

Method: FeTal-SAM融合了多模板（multi-atlas）配准技术，生成空间对齐的标签模板作为致密提示，并结合边界框提示输入到SAM的分割解码器，实现对每个结构的二分类分割，最后将各结构融合重建3D分割结果。评估涉及公开和内部两套胎儿脑MRI数据集，覆盖不同孕周。

Result: FeTal-SAM在皮层板、小脑等对比度良好的结构上，其Dice得分与为每组标签独立训练的现有最优基线方法相当，对低对比度（如海马、杏仁核）结构准确率略低，但仍具有竞争力。

Conclusion: FeTal-SAM可以不经繁琐的再训练，自适应地完成各种解剖结构分割，具备强大的灵活性和临床适应性，是下一代胎儿脑MRI分析工具的重要发展。

Abstract: This paper presents FeTal-SAM, a novel adaptation of the Segment Anything Model (SAM) tailored for fetal brain MRI segmentation. Traditional deep learning methods often require large annotated datasets for a fixed set of labels, making them inflexible when clinical or research needs change. By integrating atlas-based prompts and foundation-model principles, FeTal-SAM addresses two key limitations in fetal brain MRI segmentation: (1) the need to retrain models for varying label definitions, and (2) the lack of insight into whether segmentations are driven by genuine image contrast or by learned spatial priors. We leverage multi-atlas registration to generate spatially aligned label templates that serve as dense prompts, alongside a bounding-box prompt, for SAM's segmentation decoder. This strategy enables binary segmentation on a per-structure basis, which is subsequently fused to reconstruct the full 3D segmentation volumes. Evaluations on two datasets, the dHCP dataset and an in-house dataset demonstrate FeTal-SAM's robust performance across gestational ages. Notably, it achieves Dice scores comparable to state-of-the-art baselines which were trained for each dataset and label definition for well-contrasted structures like cortical plate and cerebellum, while maintaining the flexibility to segment any user-specified anatomy. Although slightly lower accuracy is observed for subtle, low-contrast structures (e.g., hippocampus, amygdala), our results highlight FeTal-SAM's potential to serve as a general-purpose segmentation model without exhaustive retraining. This method thus constitutes a promising step toward clinically adaptable fetal brain MRI analysis tools.

</details>


### [31] [LL-GaussianMap: Zero-shot Low-Light Image Enhancement via 2D Gaussian Splatting Guided Gain Maps](https://arxiv.org/abs/2601.15766)
*Yuhan Chen,Ying Fang,Guofa Li,Wenxuan Yu,Yicui Shi,Jingrui Zhang,Kefei Qian,Wenbo Chu,Keqiang Li*

Main category: cs.CV

TL;DR: LL-GaussianMap首次将2D高斯Splatting的显式结构先验引入低光照图像增强任务，通过结构感知提升增强效果，同时实现无监督学习和高效存储。


<details>
  <summary>Details</summary>
Motivation: 当前低光照图像增强方法多基于像素域或隐式特征，忽视了图像内在的几何结构信息，影响了增强效果。2D Gaussian Splatting在场景结构拟合和渲染效率方面有优势，尚未被用于低级视觉任务。

Method: 提出了LL-GaussianMap，将2D Gaussian Splatting用于低光照图像增强。方法包括两步：1）用2DGS进行结构重建，2）利用高斯splatting的栅格化机制，通过统一增强模块生成增强增益图，实现结构感知并用无监督学习优化，无需成对数据。

Result: 实验表明，该方法在增强效果上优于现有方案，且存储需求极低，显式高斯表示有效提升了图像增强表现。

Conclusion: LL-GaussianMap拓展了2DGS技术在低光照图像增强领域的应用，有效结合结构感知和增强，具备无监督、高性能和低存储等优点，在实际场景中具有应用潜力。

Abstract: Significant progress has been made in low-light image enhancement with respect to visual quality. However, most existing methods primarily operate in the pixel domain or rely on implicit feature representations. As a result, the intrinsic geometric structural priors of images are often neglected. 2D Gaussian Splatting (2DGS) has emerged as a prominent explicit scene representation technique characterized by superior structural fitting capabilities and high rendering efficiency. Despite these advantages, the utilization of 2DGS in low-level vision tasks remains unexplored. To bridge this gap, LL-GaussianMap is proposed as the first unsupervised framework incorporating 2DGS into low-light image enhancement. Distinct from conventional methodologies, the enhancement task is formulated as a gain map generation process guided by 2DGS primitives. The proposed method comprises two primary stages. First, high-fidelity structural reconstruction is executed utilizing 2DGS. Then, data-driven enhancement dictionary coefficients are rendered via the rasterization mechanism of Gaussian splatting through an innovative unified enhancement module. This design effectively incorporates the structural perception capabilities of 2DGS into gain map generation, thereby preserving edges and suppressing artifacts during enhancement. Additionally, the reliance on paired data is circumvented through unsupervised learning. Experimental results demonstrate that LL-GaussianMap achieves superior enhancement performance with an extremely low storage footprint, highlighting the effectiveness of explicit Gaussian representations for image enhancement.

</details>


### [32] [LL-GaussianImage: Efficient Image Representation for Zero-shot Low-Light Enhancement with 2D Gaussian Splatting](https://arxiv.org/abs/2601.15772)
*Yuhan Chen,Wenxuan Yu,Guofa Li,Yijun Xu,Ying Fang,Yicui Shi,Long Cao,Wenbo Chu,Keqiang Li*

Main category: cs.CV

TL;DR: 本文提出LL-GaussianImage方法，可在2D Gaussian Splatting（2DGS）压缩域中直接进行低照度增强，无需先解压再增强，提升了效率且避免了二次压缩带来的损失。


<details>
  <summary>Details</summary>
Motivation: 当前主流的低照度增强方法需在像素域进行，这对于2DGS压缩图像要经过解压、增强、再压缩，流程冗长且易产生二次损失，提高了系统复杂性并影响增强质量。因此，亟需一种能在2DGS压缩域中直接处理的方案。

Method: 提出了一种零样本、无监督的LL-GaussianImage框架，包括：1）利用语义引导的Mixture-of-Experts结构，根据渲染图引导对2DGS稀疏属性进行动态自适应变换，实现无需解压的增强；2）多目标协同损失系统，约束增强过程中的平滑和保真，抑制伪影提升视觉质量；3）两阶段优化流程，先单尺度重建保证基础表达准确，再提升网络鲁棒性，实现高质量增强。

Result: 提出的方法在保持高压缩比的同时，实现了高质量的低照度图片增强。实验结果验证了该压缩域直接处理范式的可行性和优越性。

Conclusion: LL-GaussianImage为压缩域内直接低照度增强提供了有效手段，显著提升了处理效率和增强质量，避免了经典解压-增强-再压缩流程的缺陷，对高效图像处理具有重要意义。

Abstract: 2D Gaussian Splatting (2DGS) is an emerging explicit scene representation method with significant potential for image compression due to high fidelity and high compression ratios. However, existing low-light enhancement algorithms operate predominantly within the pixel domain. Processing 2DGS-compressed images necessitates a cumbersome decompression-enhancement-recompression pipeline, which compromises efficiency and introduces secondary degradation. To address these limitations, we propose LL-GaussianImage, the first zero-shot unsupervised framework designed for low-light enhancement directly within the 2DGS compressed representation domain. Three primary advantages are offered by this framework. First, a semantic-guided Mixture-of-Experts enhancement framework is designed. Dynamic adaptive transformations are applied to the sparse attribute space of 2DGS using rendered images as guidance to enable compression-as-enhancement without full decompression to a pixel grid. Second, a multi-objective collaborative loss function system is established to strictly constrain smoothness and fidelity during enhancement, suppressing artifacts while improving visual quality. Third, a two-stage optimization process is utilized to achieve reconstruction-as-enhancement. The accuracy of the base representation is ensured through single-scale reconstruction and network robustness is enhanced. High-quality enhancement of low-light images is achieved while high compression ratios are maintained. The feasibility and superiority of the paradigm for direct processing within the compressed representation domain are validated through experimental results.

</details>


### [33] [Diffusion Model-Based Data Augmentation for Enhanced Neuron Segmentation](https://arxiv.org/abs/2601.15779)
*Liuyun Jiang,Yanchao Zhang,Jinyue Guo,Yizhuo Lu,Ruining Zhou,Hua Han*

Main category: cs.CV

TL;DR: 本文提出一种基于扩散模型的数据增强框架，用于提升电子显微镜下神经元分割的准确性，在低标注数据下显著提高了分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有神经元分割依赖大量人工标注数据，数据增强主要通过传统几何和光度变换，导致增强样本结构多样性不足，无法满足神经元分割任务对数据多样性的需求。

Method: 提出了一种扩散模型为核心的数据增强方法：1）引入分辨率感知的条件扩散模型，利用多尺度条件和EM分辨率先验，从3D掩码生成体素级的合成图像；2）设计了生物学引导的掩码重塑模块，生成结构更真实多样的掩码，提升样本多样性。

Result: 在AC3和AC4数据集，低标注条件下，该方法结合两种后处理，各自提升了ARAND指标32.1%和30.7%，显著优于现有方法。

Conclusion: 基于扩散模型的数据增强可生成结构多样且合理的标注样本，有效丰富训练集，提高神经元分割性能，尤其适用于标注稀缺场景。

Abstract: Neuron segmentation in electron microscopy (EM) aims to reconstruct the complete neuronal connectome; however, current deep learning-based methods are limited by their reliance on large-scale training data and extensive, time-consuming manual annotations. Traditional methods augment the training set through geometric and photometric transformations; however, the generated samples remain highly correlated with the original images and lack structural diversity. To address this limitation, we propose a diffusion-based data augmentation framework capable of generating diverse and structurally plausible image-label pairs for neuron segmentation. Specifically, the framework employs a resolution-aware conditional diffusion model with multi-scale conditioning and EM resolution priors to enable voxel-level image synthesis from 3D masks. It further incorporates a biology-guided mask remodeling module that produces augmented masks with enhanced structural realism. Together, these components effectively enrich the training set and improve segmentation performance. On the AC3 and AC4 datasets under low-annotation regimes, our method improves the ARAND metric by 32.1% and 30.7%, respectively, when combined with two different post-processing methods. Our code is available at https://github.com/HeadLiuYun/NeuroDiff.

</details>


### [34] [Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video](https://arxiv.org/abs/2601.15780)
*Pascal Benschop,Justin Dauwels,Jan van Gemert*

Main category: cs.CV

TL;DR: 本文提出了一个合成基准，用以测试视觉-语言模型在辨别人际互动中对空间与情境的推理能力，发现现有模型在相关任务上的表现仅略高于随机水平，暴露了空间推理能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉-语言模型（VLMs）在需要辨别微妙的时序或几何线索时，空间推理能力表现脆弱。现实应用中，如区分有害和无害行为、识别施暴者和受害者、判断移动轨迹等，对模型的空间与情境感知能力提出了更高要求。为系统性分析这些弱点，需要一个针对性强的测试基准。

Method: 作者构建了一个合成视频基准，设计包括：1）情境感知测试（辨别有害与无害的互动）；2）空间感知测试（追踪每个角色及其关系、动作和相对位置）；3）通过简短的视频对，考察模型能否区分暴力与正常行为、准确绑定角色身份、以及微观层级下的运动轨迹判断。同时，研究还尝试用稳定的颜色提示来辅助模型判断。

Result: 在无额外训练的条件下，主流视觉-语言模型在该基准的各项任务上表现仅略高于随机猜测，表明现有模型缺乏足够的空间关系感知能力。虽然颜色提示能降低部分角色混淆，但并未根本解决模型的推理短板。

Conclusion: 本基准可以作为通用的视频分类诊断工具，有助于追踪和分析模型的空间推理缺陷。作者希望通过开源数据和代码推动开发更高效的空间先验机制，补全大规模预训练模型在空间推理方面的不足。

Abstract: Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.

</details>


### [35] [A Mobile Application for Flower Recognition System Based on Convolutional Neural Networks](https://arxiv.org/abs/2601.15810)
*Mustafa Yurdakul,Enes Ayan,Fahrettin Horasan,Sakir Tasdemir*

Main category: cs.CV

TL;DR: 本文提出了一种基于卷积神经网络（CNN）的移动应用，实现了花卉种类的自动识别，选取DenseNet-121模型结合SGD优化器，在多个模型和优化器对比中表现最佳，识别准确率高达95.84%。


<details>
  <summary>Details</summary>
Motivation: 花卉在日常生活中用途广泛，但不同花卉的识别通常依赖专家，普通人难以及时获得相关知识。研究旨在为非专业用户提供便捷可靠的花卉识别工具。

Method: 开发了基于CNN的移动应用，分别采用MobileNet、DenseNet121和Xception三种模型，并结合七种优化算法进行训练和对比，寻找性能最优组合。

Result: DenseNet-121结合SGD优化器在分类任务中表现最优，取得了95.84%的准确率，96%的精确率、召回率和F1分数。

Conclusion: CNN模型，特别是DenseNet-121+SGD，能高效地应用于移动端花卉识别任务，可为用户带来准确快捷的服务。

Abstract: A convolutional neural network (CNN) is a deep learning algorithm that has been specifically designed for computer vision applications. The CNNs proved successful in handling the increasing amount of data in many computer vision problems, where classical machine learning algorithms were insufficient. Flowers have many uses in our daily lives, from decorating to making medicines to detoxifying the environment. Identifying flower types requires expert knowledge. However, accessing experts at any time and in any location may not always be feasible. In this study a mobile application based on CNNs was developed to recognize different types of flowers to provide non-specialists with quick and easy access to information about flower types. The study employed three distinct CNN models, namely MobileNet, DenseNet121, and Xception, to determine the most suitable model for the mobile application. The classification performances of the models were evaluated by training them with seven different optimization algorithms. The DenseNet-121 architecture, which uses the stochastic gradient descent (SGD) optimization algorithm, was the most successful, achieving 95.84 % accuracy, 96.00% precision, recall, and F1-score. This result shows that CNNs can be used for flower classification in mobile applications.

</details>


### [36] [Beyond Off-the-Shelf Models: A Lightweight and Accessible Machine Learning Pipeline for Ecologists Working with Image Data](https://arxiv.org/abs/2601.15813)
*Clare Chemery,Hendrik Edelhoff,Ludwig Bothmann*

Main category: cs.CV

TL;DR: 本论文提出了一个轻量级的实验管道，帮助生态学家无需深厚机器学习知识，即可自主构建和优化图像分类模型，显著降低了生态领域应用机器学习的门槛。


<details>
  <summary>Details</summary>
Motivation: 当前生态学家在利用机器学习方法分析图像时，往往依赖标准的现成模型，缺乏专为各自数据或任务优化的工具。作者希望开发一种简单、灵活的管道，让生态学家可以独立构建、调整适合自己研究问题的小型模型。

Method: 作者开发了一个集成命令行界面和图形界面的工具，用于数据预处理、训练、评估、标注、误差分析和模型比较。实验以德国Veldenstein森林红鹿性别与年龄分类为例，利用专家标注的4352张鹿的裁剪图像，测试多种模型结构和数据增强策略，反复比较模型性能。

Result: 在红鹿图像的性别分类上，最佳模型准确率为96.15%，年龄分类准确率为90.77%，表明即使样本有限，针对特定问题的可靠分类也是可行的。

Conclusion: 该管道为生态学家开发定制化机器学习模型提供了易用工具，有助于推动机器学习在野生动物监测和人口统计分析中的应用普及。

Abstract: We introduce a lightweight experimentation pipeline designed to lower the barrier for applying machine learning (ML) methods for classifying images in ecological research. We enable ecologists to experiment with ML models independently, thus they can move beyond off-the-shelf models and generate insights tailored to local datasets and specific classification tasks and target variables. Our tool combines a simple command-line interface for preprocessing, training, and evaluation with a graphical interface for annotation, error analysis, and model comparison. This design enables ecologists to build and iterate on compact, task-specific classifiers without requiring advanced ML expertise. As a proof of concept, we apply the pipeline to classify red deer (Cervus elaphus) by age and sex from 3392 camera trap images collected in the Veldenstein Forest, Germany. Using 4352 cropped images containing individual deer labeled by experts, we trained and evaluated multiple backbone architectures with a wide variety of parameters and data augmentation strategies. Our best-performing models achieved 90.77% accuracy for age classification and 96.15% for sex classification. These results demonstrate that reliable demographic classification is feasible even with limited data to answer narrow, well-defined ecological problems. More broadly, the framework provides ecologists with an accessible tool for developing ML models tailored to specific research questions, paving the way for broader adoption of ML in wildlife monitoring and demographic analysis.

</details>


### [37] [Towards Realistic Remote Sensing Dataset Distillation with Discriminative Prototype-guided Diffusion](https://arxiv.org/abs/2601.15829)
*Yonghao Xu,Pedram Ghamisi,Qihao Weng*

Main category: cs.CV

TL;DR: 本文首次将数据集蒸馏引入遥感影像解译领域，通过文本到图像扩散模型，将大规模遥感数据集浓缩为小而具有代表性的蒸馏数据集，并提出分类一致性损失和视觉语言引导增强样本质量，在多个高分辨率遥感场景分类基准上验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 遥感影像解译的深度学习方法依赖大量数据集，带来高存储、算力成本和敏感数据泄漏风险。本研究旨在减少对大规模原始数据的依赖，降低资源消耗并提升数据隐私安全性。

Method: 采用文本到图像扩散模型，将大规模遥感数据集蒸馏为紧凑集。引入预训练分类器的一致性损失提升合成样本区分性，通过对训练样本的潜在空间聚类筛选代表性视觉风格原型，并利用视觉语言模型提供文本描述以丰富语义信息。

Result: 在3个高分辨率遥感场景分类基准数据集上，所提方法能蒸馏出真实且多样的训练样本，且这些样本可用于下游模型训练，显示出优异的表现。

Conclusion: 本文方法有效减少遥感影像解译对原始大数据集的依赖，蒸馏出的数据既轻量又高效，具有实际工程应用和隐私保护潜力，推动遥感智能解译的发展。

Abstract: Recent years have witnessed the remarkable success of deep learning in remote sensing image interpretation, driven by the availability of large-scale benchmark datasets. However, this reliance on massive training data also brings two major challenges: (1) high storage and computational costs, and (2) the risk of data leakage, especially when sensitive categories are involved. To address these challenges, this study introduces the concept of dataset distillation into the field of remote sensing image interpretation for the first time. Specifically, we train a text-to-image diffusion model to condense a large-scale remote sensing dataset into a compact and representative distilled dataset. To improve the discriminative quality of the synthesized samples, we propose a classifier-driven guidance by injecting a classification consistency loss from a pre-trained model into the diffusion training process. Besides, considering the rich semantic complexity of remote sensing imagery, we further perform latent space clustering on training samples to select representative and diverse prototypes as visual style guidance, while using a visual language model to provide aggregated text descriptions. Experiments on three high-resolution remote sensing scene classification benchmarks show that the proposed method can distill realistic and diverse samples for downstream model training. Code and pre-trained models are available online (https://github.com/YonghaoXu/DPD).

</details>


### [38] [An IoT-Based Smart Plant Monitoring and Irrigation System with Real-Time Environmental Sensing, Automated Alerts, and Cloud Analytics](https://arxiv.org/abs/2601.15830)
*Abdul Hasib,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: 本文提出了一种基于物联网(IoT)的智能植物监测系统，结合多种环境传感器、自动灌溉和云端分析，实现了高效节水和实时植物健康管理。系统成本低，适用于小型园艺和商业农业。


<details>
  <summary>Details</summary>
Motivation: 可持续农业对优化资源利用和植物健康管理的智能监控有迫切需求，传统方法存在人工监测低效、水资源浪费等问题。

Method: 采用ESP32微控制器集成DHT22、HC-SR04和土壤湿度传感器，结合OLED显示和蜂鸣器，实现多维环境数据采集，并通过无线传输至ThingSpeak云平台，实现远程监控、历史分析和自动报警。

Result: 实验显示系统在保持土壤湿度方面准确率达92%，实时环境监测能力强，能比传统灌溉方法节水约40%。

Conclusion: 该系统成本低（仅$45.20），功能全面，适合不同规模农业应用，为精准农业和智能种植提供了可扩展的解决方案。

Abstract: The increasing global demand for sustainable agriculture necessitates intelligent monitoring systems that optimize resource utilization and plant health management. Traditional farming methods rely on manual observation and periodic watering, often leading to water wastage, inconsistent plant growth, and delayed response to environmental changes. This paper presents a comprehensive IoT-based smart plant monitoring system that integrates multiple environmental sensors with automated irrigation and cloud analytics. The proposed system utilizes an ESP32 microcontroller to collect real-time data from DHT22 (temperature/humidity), HC-SR04 (water level), and soil moisture sensors, with visual feedback through an OLED display and auditory alerts via a buzzer. All sensor data is wirelessly transmitted to the ThingSpeak cloud platform for remote monitoring, historical analysis, and automated alert generation. Experimental results demonstrate the system's effectiveness in maintaining optimal soil moisture levels (with 92\% accuracy), providing real-time environmental monitoring, and reducing water consumption by approximately 40\% compared to conventional irrigation methods. The integrated web dashboard offers comprehensive visualization of plant health parameters, making it suitable for both small-scale gardening and commercial agriculture applications. With a total implementation cost of \$45.20, this system provides an affordable, scalable solution for precision agriculture and smart farming.

</details>


### [39] [TinySense: Effective CSI Compression for Scalable and Accurate Wi-Fi Sensing](https://arxiv.org/abs/2601.15838)
*Toan Gian,Dung T. Tran,Viet Quoc Pham,Francesco Restuccia,Van-Dinh Nguyen*

Main category: cs.CV

TL;DR: 本文提出了TinySense，一个高效的Wi-Fi人体姿态估计算法，通过先进的压缩方法大幅减少数据量，在保持准确性的同时显著降低了网络负载和延迟。


<details>
  <summary>Details</summary>
Motivation: 当前Wi-Fi感知技术处理大量通道状态信息（CSI），对网络资源造成极大压力，影响人体姿态估计的效率和可扩展性。因此需要更加高效的CSI压缩和处理方案以支持隐私保护和无设备感知。

Method: TinySense基于矢量量化生成对抗网络（VQGAN）训练得到的码本，通过K-means进一步动态调整编码比特率，将大规模预训练码本聚类为更小子集，同时引入Transformer模型减缓比特率损失，提高在不可靠网络条件下的鲁棒性。系统在Jetson Nano和树莓派平台上进行了原型验证。

Result: 与当前主流压缩方案相比，TinySense在相同压缩率下，人体姿态估计准确度（PCK20）提高最多1.5倍，网络延迟和网络负载分别降低多达5倍和2.5倍。

Conclusion: TinySense显著提升了Wi-Fi人体感知的效率和可扩展性，有效平衡了压缩率与准确性，并减少了网络资源消耗，适用于实时隐私保护场景。

Abstract: With the growing demand for device-free and privacy-preserving sensing solutions, Wi-Fi sensing has emerged as a promising approach for human pose estimation (HPE). However, existing methods often process vast amounts of channel state information (CSI) data directly, ultimately straining networking resources. This paper introduces TinySense, an efficient compression framework that enhances the scalability of Wi-Fi-based human sensing. Our approach is based on a new vector quantization-based generative adversarial network (VQGAN). Specifically, by leveraging a VQGAN-learned codebook, TinySense significantly reduces CSI data while maintaining the accuracy required for reliable HPE. To optimize compression, we employ the K-means algorithm to dynamically adjust compression bitrates to cluster a large-scale pre-trained codebook into smaller subsets. Furthermore, a Transformer model is incorporated to mitigate bitrate loss, enhancing robustness in unreliable networking conditions. We prototype TinySense on an experimental testbed using Jetson Nano and Raspberry Pi to measure latency and network resource use. Extensive results demonstrate that TinySense significantly outperforms state-of-the-art compression schemes, achieving up to 1.5x higher HPE accuracy score (PCK20) under the same compression rate. It also reduces latency and networking overhead, respectively, by up to 5x and 2.5x. The code repository is available online at here.

</details>


### [40] [A Lightweight Brain-Inspired Machine Learning Framework for Coronary Angiography: Hybrid Neural Representation and Robust Learning Strategies](https://arxiv.org/abs/2601.15865)
*Jingsong Xia,Siqi Wang*

Main category: cs.CV

TL;DR: 本研究提出了一种轻量级脑启发式深度学习模型，用于冠状动脉造影图像的分类，在保证高效计算的同时取得了优异的分类表现。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在实际冠状动脉造影分析中，受限于病灶复杂、类别极不平衡、标签不确定性和计算资源有限等因素，鲁棒性与泛化能力不足。因此，需要开发能适应这些挑战的高效且鲁棒的模型。

Method: 采用预训练卷积神经网络（CNN）构建轻量级混合神经表示，提出选择性神经可塑性训练策略，实现模型参数高效适应。引入脑启发的注意力调控损失函数（融合Focal Loss与标签平滑），以增强对难样本和不确定标签的敏感性。通过类别不平衡采样和余弦退火带热重启策略，模拟生物神经系统的节律调控及注意力分配机制。

Result: 该方法在二分类冠状动脉造影任务中，取得了极具竞争力的准确率、召回率、F1分数和AUC，各项表现稳定且模型保持高算力效率。

Conclusion: 脑启发机制对医学图像轻量化分析有效，为有限计算资源下的临床智能决策支持提供了可部署、合理且生物学启发的解决方案。

Abstract: Background: Coronary angiography (CAG) is a cornerstone imaging modality for assessing coronary artery disease and guiding interventional treatment decisions. However, in real-world clinical settings, angiographic images are often characterized by complex lesion morphology, severe class imbalance, label uncertainty, and limited computational resources, posing substantial challenges to conventional deep learning approaches in terms of robustness and generalization.Methods: The proposed framework is built upon a pretrained convolutional neural network to construct a lightweight hybrid neural representation. A selective neural plasticity training strategy is introduced to enable efficient parameter adaptation. Furthermore, a brain-inspired attention-modulated loss function, combining Focal Loss with label smoothing, is employed to enhance sensitivity to hard samples and uncertain annotations. Class-imbalance-aware sampling and cosine annealing with warm restarts are adopted to mimic rhythmic regulation and attention allocation mechanisms observed in biological neural systems.Results: Experimental results demonstrate that the proposed lightweight brain-inspired model achieves strong and stable performance in binary coronary angiography classification, yielding competitive accuracy, recall, F1-score, and AUC metrics while maintaining high computational efficiency.Conclusion: This study validates the effectiveness of brain-inspired learning mechanisms in lightweight medical image analysis and provides a biologically plausible and deployable solution for intelligent clinical decision support under limited computational resources.

</details>


### [41] [Out-of-Distribution Detection Based on Total Variation Estimation](https://arxiv.org/abs/2601.15867)
*Dabiao Ma,Zhiba Su,Jian Yang,Haojun Fei*

Main category: cs.CV

TL;DR: 提出了一种新的基于总变差的分布外检测方法TV-OOD，在图像分类等任务上的检测效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的分布外（OOD）检测方法在防御分布漂移时表现不错，但仍有提升空间，尤其在实际应用的安全性和可靠性层面。为此，作者提出更准确、鲁棒性更强的检测新方法。

Method: TV-OOD方法通过引入Total Variation Network Estimator，计算每个输入对模型总变差的贡献，定义为总变差分数。该分数用于区分分布内和分布外的数据。实验针对多种模型和数据集进行，全面评估了方法性能。

Result: TV-OOD在多种图像分类任务和主流评测指标上，始终表现出与先进方法相当或更优的OOD检测能力，显示出较明显的优势和普适性。

Conclusion: TV-OOD是一种有效且具备广泛适用性的分布外检测方法，对提升实际场景下机器学习部署的安全性和鲁棒性具有重要意义，优于当前主流方法。

Abstract: This paper introduces a novel approach to securing machine learning model deployments against potential distribution shifts in practical applications, the Total Variation Out-of-Distribution (TV-OOD) detection method. Existing methods have produced satisfactory results, but TV-OOD improves upon these by leveraging the Total Variation Network Estimator to calculate each input's contribution to the overall total variation. By defining this as the total variation score, TV-OOD discriminates between in- and out-of-distribution data. The method's efficacy was tested across a range of models and datasets, consistently yielding results in image classification tasks that were either comparable or superior to those achieved by leading-edge out-of-distribution detection techniques across all evaluation metrics.

</details>


### [42] [PMPBench: A Paired Multi-Modal Pan-Cancer Benchmark for Medical Image Synthesis](https://arxiv.org/abs/2601.15884)
*Yifan Chen,Fei Yin,Hao Chen,Jia Wu,Chao Li*

Main category: cs.CV

TL;DR: 该论文提出了首个公开、完全配对、涵盖11个人体器官的全癌种医学影像数据集，旨在推动AI合成造影成像技术的研究，尤其对多器官肿瘤诊断流程具有实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 现有公开医学影像数据集大多集中于脑部配对MR模态，或数据配对不全、空间配准不佳、标注缺失，且数据归属权多为私有，严重限制了AI在非造影影像合成造影影像方向的研究与应用。

Method: 该团队采集并整理了涵盖11个人体器官、配对完备的全癌种医学影像数据集，MR部分覆盖完整的DCE三期序列，CT部分提供非造影和造影配对图像，注重解剖结构对应性；据此建立了1对1、N对1及N对N的图像合成评测基准，并用主流图像到图像转换方法做基线测试。

Result: 在该数据集上，论文展示了不同AI图像转换方法的基准表现，确保对比和评测的公正性，并证明了该数据集对造影合成任务的重要价值。

Conclusion: 该工作公开了高价值医学影像配对数据集和基准，为安全、有效的AI造影成像提供了重要资源，并将促进相关研究和实际多器官肿瘤影像诊断流程的进步。

Abstract: Contrast medium plays a pivotal role in radiological imaging, as it amplifies lesion conspicuity and improves detection for the diagnosis of tumor-related diseases. However, depending on the patient's health condition or the medical resources available, the use of contrast medium is not always feasible. Recent work has explored AI-based image translation to synthesize contrast-enhanced images directly from non-contrast scans, aims to reduce side effects and streamlines clinical workflows. Progress in this direction has been constrained by data limitations: (1) existing public datasets focus almost exclusively on brain-related paired MR modalities; (2) other collections include partially paired data but suffer from missing modalities/timestamps and imperfect spatial alignment; (3) explicit labeling of CT vs. CTC or DCE phases is often absent; (4) substantial resources remain private. To bridge this gap, we introduce the first public, fully paired, pan-cancer medical imaging dataset spanning 11 human organs. The MR data include complete dynamic contrast-enhanced (DCE) sequences covering all three phases (DCE1-DCE3), while the CT data provide paired non-contrast and contrast-enhanced acquisitions (CTC). The dataset is curated for anatomical correspondence, enabling rigorous evaluation of 1-to-1, N-to-1, and N-to-N translation settings (e.g., predicting DCE phases from non-contrast inputs). Built upon this resource, we establish a comprehensive benchmark. We report results from representative baselines of contemporary image-to-image translation. We release the dataset and benchmark to catalyze research on safe, effective contrast synthesis, with direct relevance to multi-organ oncology imaging workflows. Our code and dataset are publicly available at https://github.com/YifanChen02/PMPBench.

</details>


### [43] [Understanding the Transfer Limits of Vision Foundation Models](https://arxiv.org/abs/2601.15888)
*Shiqi Huang,Yipei Wang,Natasha Thorley,Alexander Ng,Shaheer Saeed,Mark Emberton,Shonit Punwani,Veeru Kasivisvanathan,Dean Barratt,Daniel Alexander,Yipeng Hu*

Main category: cs.CV

TL;DR: 本文比较了两类主流视觉基础模型（重建型和对比学习型）在前列腺多参数MRI五个任务中的表现，发现预训练目标与下游任务的契合程度决定了微调效果和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 虽然基础语言模型普遍展现出较强的迁移泛化能力，但视觉基础模型则在不同下游任务间表现不均衡，作者认为这源于预训练目标与下游任务需求的错配，尤其是在医学影像等实际应用场景中影响显著。

Method: 文章选取了两种视觉基础模型：以重建为导向的MAE模型（ProFound）和以对比学习为导向的模型（ProViCNet），并将它们应用到前列腺多参数磁共振成像的五项具体任务。通过最大均值差异（MMD）等简单发散度量，量化模型预训练特征在微调前后的变化，探讨预训练任务与下游任务契合度对迁移表现的影响。

Result: 实验显示，预训练目标与下游任务更契合的模型，不仅提升了下游任务表现，还能加速微调过程的收敛。MMD等指标可以有效反映契合度与性能提升的关系。

Conclusion: 视觉基础模型的预训练策略设计需充分考虑下游任务的实际需求，提升预训练与下游任务的契合度，从而更好地服务于临床等现实任务中的应用。

Abstract: Foundation models leverage large-scale pretraining to capture extensive knowledge, demonstrating generalization in a wide range of language tasks. By comparison, vision foundation models (VFMs) often exhibit uneven improvements across downstream tasks, despite substantial computational investment. We postulate that this limitation arises from a mismatch between pretraining objectives and the demands of downstream vision-and-imaging tasks. Pretraining strategies like masked image reconstruction or contrastive learning shape representations for tasks such as recovery of generic visual patterns or global semantic structures, which may not align with the task-specific requirements of downstream applications including segmentation, classification, or image synthesis. To investigate this in a concrete real-world clinical area, we assess two VFMs, a reconstruction-focused MAE-based model (ProFound) and a contrastive-learning-based model (ProViCNet), on five prostate multiparametric MR imaging tasks, examining how such task alignment influences transfer performance, i.e., from pretraining to fine-tuning. Our findings indicate that better alignment between pretraining and downstream tasks, measured by simple divergence metrics such as maximum-mean-discrepancy (MMD) between the same features before and after fine-tuning, correlates with greater performance improvements and faster convergence, emphasizing the importance of designing and analyzing pretraining objectives with downstream applicability in mind.

</details>


### [44] [RadJEPA: Radiology Encoder for Chest X-Rays via Joint Embedding Predictive Architecture](https://arxiv.org/abs/2601.15891)
*Anas Anwarul Haq Khan,Mariam Husain,Kshitij Jadhav*

Main category: cs.CV

TL;DR: 该论文提出了RadJEPA模型，通过无监督方式，仅基于胸部X光图像进行训练，预测被遮挡区域的潜在表征，无需语言监督，且在多项任务中性能超过其它先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉-语言模型受限于需要成对的图像-文本数据，获取成对数据较为困难，因此研究者希望探索无需语言监督，仅依赖未标注的医学影像能否学到强健的表征。

Method: 作者提出RadJEPA模型，一种基于联合嵌入预测结构（Joint Embedding Predictive Architecture）的自监督学习方法。RadJEPA在没有任何文本数据的情况下，只用未标注的胸部X光图像，通过预测遮挡区域的潜在表征来进行训练。与传统对齐全局表征的自监督方法（如DINO）不同，RadJEPA直接建模潜在空间的预测。

Result: RadJEPA在疾病分类、语义分割和报告生成等下游任务上进行了评估，结果显示其性能优于现有最先进的方法，包括Rad-DINO。

Conclusion: RadJEPA无需语言监督，只依赖未标注的影像就能学习到表现优异的医学图像编码器，为医学影像表征学习提供了更具泛化性的自监督方案。

Abstract: Recent advances in medical vision language models guide the learning of visual representations; however, this form of supervision is constrained by the availability of paired image text data, raising the question of whether robust radiology encoders can be learned without relying on language supervision. In this work, we introduce RadJEPA, a self-supervised framework built on a Joint Embedding Predictive Architecture that learns without language supervision. Pre-trained solely on unlabeled chest X-ray images, the model learns to predict latent representations of masked image regions. This predictive objective differs fundamentally from both image text pre-training and DINO-style self-distillation: rather than aligning global representations across views or modalities, RadJEPA explicitly models latent-space prediction. We evaluate the learned encoder on disease classification, semantic segmentation, and report generation tasks. Across benchmarks, RadJEPA achieves performance exceeding state-of-the-art approaches, including Rad-DINO.

</details>


### [45] [ThermoSplat: Cross-Modal 3D Gaussian Splatting with Feature Modulation and Geometry Decoupling](https://arxiv.org/abs/2601.15897)
*Zhaoqi Su,Shihai Chen,Xinyan Lin,Liqin Huang,Zhipeng Su,Xiaoqiang Lu*

Main category: cs.CV

TL;DR: 本文提出了一种名为ThermoSplat的新型3D重建方法，融合RGB和热红外数据，实现跨光谱高质量场景重建，实验超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 在多种光照和天气条件下，融合RGB与热红外数据进行多模态场景重建对于环境感知至关重要。但现有3D高斯投影类方法难以充分挖掘多模态数据间的互补信息，尤其难以自适应解决光谱间结构关联与物理差异。

Method: 提出ThermoSplat框架。核心创新包括：(1)跨模态FiLM调制机制，将热红外结构先验动态传递给可见光纹理生成，引导融合；(2)模态自适应几何解耦，热红外分支独立学习不透明度偏移并独立光栅化，解决模态几何不一致；(3)结合球谐函数与神经解码器的混合渲染管线，确保语义一致与高频细节。

Result: 在RGBT-Scenes数据集上，ThermoSplat在可见光与热红外光谱的渲染质量均达到了目前最优，超过以往方法。

Conclusion: ThermoSplat有效融合了多光谱数据，解决了结构和几何差异问题，为多模态3D场景重建提供了先进的解决方案，具有广泛应用前景。

Abstract: Multi-modal scene reconstruction integrating RGB and thermal infrared data is essential for robust environmental perception across diverse lighting and weather conditions. However, extending 3D Gaussian Splatting (3DGS) to multi-spectral scenarios remains challenging. Current approaches often struggle to fully leverage the complementary information of multi-modal data, typically relying on mechanisms that either tend to neglect cross-modal correlations or leverage shared representations that fail to adaptively handle the complex structural correlations and physical discrepancies between spectrums. To address these limitations, we propose ThermoSplat, a novel framework that enables deep spectral-aware reconstruction through active feature modulation and adaptive geometry decoupling. First, we introduce a Cross-Modal FiLM Modulation mechanism that dynamically conditions shared latent features on thermal structural priors, effectively guiding visible texture synthesis with reliable cross-modal geometric cues. Second, to accommodate modality-specific geometric inconsistencies, we propose a Modality-Adaptive Geometric Decoupling scheme that learns independent opacity offsets and executes an independent rasterization pass for the thermal branch. Additionally, a hybrid rendering pipeline is employed to integrate explicit Spherical Harmonics with implicit neural decoding, ensuring both semantic consistency and high-frequency detail preservation. Extensive experiments on the RGBT-Scenes dataset demonstrate that ThermoSplat achieves state-of-the-art rendering quality across both visible and thermal spectrums.

</details>


### [46] [Opening the Black Box: Preliminary Insights into Affective Modeling in Multimodal Foundation Models](https://arxiv.org/abs/2601.15906)
*Zhen Zhang,Runhao Zeng,Sicheng Zhao,Xiping Hu*

Main category: cs.CV

TL;DR: 本研究系统性分析了多模态基础模型中情感建模的内部机制，发现情感能力主要由前馈门控投影层（gate_proj）实现，而非注意力模块。


<details>
  <summary>Details</summary>
Motivation: 尽管现有多模态情感模型表现优异，但其内部实现情感理解与生成的结构机制尚不清楚，本研究旨在揭示这些机制。

Method: 在不同的基础模型架构、训练策略和情感任务上，通过控制模块迁移、目标单模块适配和破坏性消融等实验，分析情感监督对模型内部参数的影响，重点关注不同模块在情感建模中的作用。

Result: 结果一致显示：情感能力的主要适应集中在前馈门控投影（gate_proj）层，而不是注意力模块。仅微调gate_proj可用较少参数（只需AffectGPT的24.5%参数）达到其96.6%的性能。

Conclusion: 前馈门控机制在基础模型中的情感能力实现中起核心作用，gate_proj模块是进行高效情感建模的关键结构位置。

Abstract: Understanding where and how emotions are represented in large-scale foundation models remains an open problem, particularly in multimodal affective settings. Despite the strong empirical performance of recent affective models, the internal architectural mechanisms that support affective understanding and generation are still poorly understood. In this work, we present a systematic mechanistic study of affective modeling in multimodal foundation models. Across multiple architectures, training strategies, and affective tasks, we analyze how emotion-oriented supervision reshapes internal model parameters. Our results consistently reveal a clear and robust pattern: affective adaptation does not primarily focus on the attention module, but instead localizes to the feed-forward gating projection (\texttt{gate\_proj}). Through controlled module transfer, targeted single-module adaptation, and destructive ablation, we further demonstrate that \texttt{gate\_proj} is sufficient, efficient, and necessary for affective understanding and generation. Notably, by tuning only approximately 24.5\% of the parameters tuned by AffectGPT, our approach achieves 96.6\% of its average performance across eight affective tasks, highlighting substantial parameter efficiency. Together, these findings provide empirical evidence that affective capabilities in foundation models are structurally mediated by feed-forward gating mechanisms and identify \texttt{gate\_proj} as a central architectural locus of affective modeling.

</details>


### [47] [The Latency Wall: Benchmarking Off-the-Shelf Emotion Recognition for Real-Time Virtual Avatars](https://arxiv.org/abs/2601.15914)
*Yarin Benyamin*

Main category: cs.CV

TL;DR: 本研究对用于虚拟现实自闭症社会技能训练情境下的实时表情识别技术进行了基准测试，发现当前通用深度学习模型难以兼顾高准确率和低延迟需求，呼吁开发轻量化、领域专用的表情识别架构。


<details>
  <summary>Details</summary>
Motivation: 虚拟现实和人机交互领域的实时情感识别有助于自闭症患者提升社交技能，但这一应用对识别延迟和准确率有严格要求，目前尚无合适的轻量级解决方案。

Method: 作者利用UIBVFED数据集，分别评估了YOLO（v8, v11, v12）中不同规模的模型（Medium和Nano）与多种视觉Transformer（如CLIP, SigLIP, ViT-FER）在虚拟角色表情识别中的表现，重点关注在仅用CPU推理条件下的准确率与延迟。

Result: 结果显示，YOLO系列模型在虚拟角色人脸检测上精度达到100%，YOLOv11n在检测速度与延迟方面表现最佳（仅约54毫秒）；而通用视觉Transformer（例如CLIP, SigLIP）无法在准确率（低于23%）及延迟（高于150毫秒）上满足实时要求。

Conclusion: 当前主流通用深度学习模型不适合低延迟、高实时性的虚拟现实表情识别应用，推进可普及的AI治疗工具亟需专门针对虚拟角色和CPU环境优化的轻量化、领域专用表情识别模型。

Abstract: In the realm of Virtual Reality (VR) and Human-Computer Interaction (HCI), real-time emotion recognition shows promise for supporting individuals with Autism Spectrum Disorder (ASD) in improving social skills. This task requires a strict latency-accuracy trade-off, with motion-to-photon (MTP) latency kept below 140 ms to maintain contingency. However, most off-the-shelf Deep Learning models prioritize accuracy over the strict timing constraints of commodity hardware. As a first step toward accessible VR therapy, we benchmark State-of-the-Art (SOTA) models for Zero-Shot Facial Expression Recognition (FER) on virtual characters using the UIBVFED dataset. We evaluate Medium and Nano variants of YOLO (v8, v11, and v12) for face detection, alongside general-purpose Vision Transformers including CLIP, SigLIP, and ViT-FER.Our results on CPU-only inference demonstrate that while face detection on stylized avatars is robust (100% accuracy), a "Latency Wall" exists in the classification stage. The YOLOv11n architecture offers the optimal balance for detection (~54 ms). However, general-purpose Transformers like CLIP and SigLIP fail to achieve viable accuracy (<23%) or speed (>150 ms) for real-time loops. This study highlights the necessity for lightweight, domain-specific architectures to enable accessible, real-time AI in therapeutic settings.

</details>


### [48] [A Multi-View Pipeline and Benchmark Dataset for 3D Hand Pose Estimation in Surgery](https://arxiv.org/abs/2601.15918)
*Valery Fischer,Alan Magdaleno,Anna-Katharina Calek,Nicola Cavalcanti,Nathan Hoffman,Christoph Germann,Joschua Wüthrich,Max Krähenmann,Mazda Farshad,Philipp Fürnstahl,Lilian Calvet*

Main category: cs.CV

TL;DR: 该论文提出了一种无需特定领域微调、基于多视角的鲁棒3D手势估计算法，并建立了新的外科手术场景基准数据集，结果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在外科环境中实现准确的三维手势估计对于技能评估、机器人辅助操作和流程分析非常重要，但面临光照强烈、遮挡频繁、手部外观相似和数据集稀缺等挑战。当前缺乏适用于手术场景的高质量算法和数据集。

Method: 提出了一种无需领域微调、可直接部署的多视角3D手势估计流程，利用现有预训练模型，集成了人体检测、全身姿态估计、2D手部关键点预测和受限3D优化。同时构建了包含68000帧、3000个人工标注手势的外科场景数据集，具备真实三维标签。

Result: 该方法在定量实验中显著优于基线，2D平均关节点误差降低31%，3D关节点平均位置误差降低76%。

Conclusion: 此研究为手术场景下的3D手势估计建立了新的强基线，提供了无需训练的算法流程和详尽的标注数据集，将有力促进外科计算机视觉领域的后续研究。

Abstract: Purpose: Accurate 3D hand pose estimation supports surgical applications such as skill assessment, robot-assisted interventions, and geometry-aware workflow analysis. However, surgical environments pose severe challenges, including intense and localized lighting, frequent occlusions by instruments or staff, and uniform hand appearance due to gloves, combined with a scarcity of annotated datasets for reliable model training.
  Method: We propose a robust multi-view pipeline for 3D hand pose estimation in surgical contexts that requires no domain-specific fine-tuning and relies solely on off-the-shelf pretrained models. The pipeline integrates reliable person detection, whole-body pose estimation, and state-of-the-art 2D hand keypoint prediction on tracked hand crops, followed by a constrained 3D optimization. In addition, we introduce a novel surgical benchmark dataset comprising over 68,000 frames and 3,000 manually annotated 2D hand poses with triangulated 3D ground truth, recorded in a replica operating room under varying levels of scene complexity.
  Results: Quantitative experiments demonstrate that our method consistently outperforms baselines, achieving a 31% reduction in 2D mean joint error and a 76% reduction in 3D mean per-joint position error.
  Conclusion: Our work establishes a strong baseline for 3D hand pose estimation in surgery, providing both a training-free pipeline and a comprehensive annotated dataset to facilitate future research in surgical computer vision.

</details>


### [49] [Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing](https://arxiv.org/abs/2601.16125)
*Tingyu Song,Yanzhao Zhang,Mingxin Li,Zhuoning Guo,Dingkun Long,Pengjun Xie,Siyue Zhang,Yilun Zhao,Shu Wu*

Main category: cs.CV

TL;DR: 提出了一种新的精细化组成图像检索（CIR）基准EDIR，通过图像编辑生成多样化查询，评测表明现有多模态模型在细分任务上存在明显能力差距。


<details>
  <summary>Details</summary>
Motivation: 现有的CIR基准查询类别有限，未能充分反映现实需求和任务多样性，因此需要更具代表性和挑战性的新基准。

Method: 利用图像编辑技术精准合成包含多类别和子类别的复杂查询，构建了EDIR基准，并用13种多模态嵌入模型进行了评测和横向比较。

Result: EDIR基准包含5,000条高质量复杂查询，展现了不同模型间的显著能力差距，即使最先进模型在所有子类别上也难以保持优秀表现，还揭示了现有基准存在模态偏置和覆盖类别不足等问题。

Conclusion: EDIR为CIR领域提供了更严苛和细致的评估工具，通过体内训练实验展示了部分类别可通过针对性数据解决，部分则揭示了模型架构的固有限制。

Abstract: Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding. Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios. To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories. Using this pipeline, we construct EDIR, a novel fine-grained CIR benchmark. EDIR encompasses 5,000 high-quality queries structured across five main categories and fifteen subcategories. Our comprehensive evaluation of 13 multimodal embedding models reveals a significant capability gap; even state-of-the-art models (e.g., RzenEmbed and GME) struggle to perform consistently across all subcategories, highlighting the rigorous nature of our benchmark. Through comparative analysis, we further uncover inherent limitations in existing benchmarks, such as modality biases and insufficient categorical coverage. Furthermore, an in-domain training experiment demonstrates the feasibility of our benchmark. This experiment clarifies the task challenges by distinguishing between categories that are solvable with targeted data and those that expose intrinsic limitations of current model architectures.

</details>


### [50] [Class Confidence Aware Reweighting for Long Tailed Learning](https://arxiv.org/abs/2601.15924)
*Brainard Philemon Jagati,Jitendra Tembhurne,Harsh Goud,Rudra Pratap Singh,Chandrashekhar Meshram*

Main category: cs.CV

TL;DR: 本文提出了一种基于类别和置信度的加权方案，有效提升了长尾分布下深度神经网络的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究在应对类别不平衡问题时，主要关注于决策空间的调整（如对 logits 的矫正），而对样本间置信度差异在优化过程中的角色关注较少。长尾数据分布导致头部类别占主导，尾部类别样本稀少，模型在此情境下表现显著下降。该研究动机是解决这种情境下模型性能退化问题。

Method: 作者提出了一种类别与置信度感知的加权策略，直接在损失（loss）层面进行调整。具体做法为利用 Ω(p_t, f_c) 函数，根据每个样本的预测置信度（p_t）以及其所属类别的相对频率（f_c）来调节样本对总损失的贡献。该方法可与调整 logits 的现有技术互补。

Result: 在CIFAR-100-LT、ImageNet-LT和iNaturalist2018等多个受不同不平衡因子影响的数据集上，实验结果表明所提方法能显著提升模型在长尾分布下的表现，验证了其有效性和理论基础。

Conclusion: 基于类别和置信度加权的损失层调整方案能够有效改善长尾分布下的神经网络训练，且能与现有的logits调整方法互补。因此，该方法为处理类别不平衡提供了新思路和实用工具。

Abstract: Deep neural network models degrade significantly in the long-tailed data distribution, with the overall training data dominated by a small set of classes in the head, and the tail classes obtaining less training examples. Addressing the imbalance in the classes, attention in the related literature was given mainly to the adjustments carried out in the decision space in terms of either corrections performed at the logit level in order to compensate class-prior bias, with the least attention to the optimization process resulting from the adjustments introduced through the differences in the confidences among the samples. In the current study, we present the design of a class and confidence-aware re-weighting scheme for long-tailed learning. This scheme is purely based upon the loss level and has a complementary nature to the existing methods performing the adjustment of the logits. In the practical implementation stage of the proposed scheme, we use an Ω(p_t, f_c) function. This function enables the modulation of the contribution towards the training task based upon the confidence value of the prediction, as well as the relative frequency of the corresponding class. Our observations in the experiments are corroborated by significant experimental results performed on the CIFAR-100-LT, ImageNet-LT, and iNaturalist2018 datasets under various values of imbalance factors that clearly authenticate the theoretical discussions above.

</details>


### [51] [NeuroMamba: Multi-Perspective Feature Interaction with Visual Mamba for Neuron Segmentation](https://arxiv.org/abs/2601.15929)
*Liuyun Jiang,Yizhuo Lu,Yanchao Zhang,Jiazheng Liu,Hua Han*

Main category: cs.CV

TL;DR: 本文提出了一种新型神经元分割方法NeuroMamba，通过结合全球和局部特征建模，实现了高精度的三维神经元分割，并在多个公开数据集上取得了最新的最优性能。


<details>
  <summary>Details</summary>
Motivation: 神经元分割对于重建完整的大脑神经连接组至关重要，但由于神经元形态复杂且结构密集，现有方法（CNN和Transformer）在边界处理和全局依赖建模方面存在局限，亟需更加高效且精确的神经元分割方法。

Method: 提出了NeuroMamba框架。该方法一方面利用Mamba结构的线性复杂度进行无patch划分的全球建模，另一方面结合局部特征建模以保护细粒度体素信息。具体包括两大模块：通道门控边界判别特征提取器（BDFE）增强局部形态线索，以及空间连续特征提取器（SCFE）结合分辨率自适应扫描机制建模全局依赖。最终通过跨调制机制融合多视角特征。

Result: NeuroMamba在四个公开电子显微镜（EM）数据集上均取得了最优的分割性能，表现出强大的分辨率适应能力，无论数据是各向异性还是各向同性分辨率都得到了验证。

Conclusion: NeuroMamba有效融合了全局建模和局部细节保留，实现了更精准且高效的神经元分割。实验结果显示其具备出色的通用性和先进性，有望推动神经连接组学研究进展。

Abstract: Neuron segmentation is the cornerstone of reconstructing comprehensive neuronal connectomes, which is essential for deciphering the functional organization of the brain. The irregular morphology and densely intertwined structures of neurons make this task particularly challenging. Prevailing CNN-based methods often fail to resolve ambiguous boundaries due to the lack of long-range context, whereas Transformer-based methods suffer from boundary imprecision caused by the loss of voxel-level details during patch partitioning. To address these limitations, we propose NeuroMamba, a multi-perspective framework that exploits the linear complexity of Mamba to enable patch-free global modeling and synergizes this with complementary local feature modeling, thereby efficiently capturing long-range dependencies while meticulously preserving fine-grained voxel details. Specifically, we design a channel-gated Boundary Discriminative Feature Extractor (BDFE) to enhance local morphological cues. Complementing this, we introduce the Spatial Continuous Feature Extractor (SCFE), which integrates a resolution-aware scanning mechanism into the Visual Mamba architecture to adaptively model global dependencies across varying data resolutions. Finally, a cross-modulation mechanism synergistically fuses these multi-perspective features. Our method demonstrates state-of-the-art performance across four public EM datasets, validating its exceptional adaptability to both anisotropic and isotropic resolutions. The source code will be made publicly available.

</details>


### [52] [EVolSplat4D: Efficient Volume-based Gaussian Splatting for 4D Urban Scene Synthesis](https://arxiv.org/abs/2601.15951)
*Sheng Miao,Sijin Li,Pan Wang,Dongfeng Bai,Bingbing Liu,Yue Wang,Andreas Geiger,Yiyi Liao*

Main category: cs.CV

TL;DR: 本文提出EvolSplat4D，一种面向自动驾驶仿真的新颖动态城市场景新视图合成方法，通过结合多分支高斯表示，实现高效且一致性的场景重建。


<details>
  <summary>Details</summary>
Motivation: 现有新视图合成方法在重建速度和质量之间难以平衡。主流神经辐射场和3D高斯泼溅能实现高质量重建，但需要耗时的每场景优化。而新兴的前馈方法多采用像素级表示，在动态复杂场景下会导致3D结果不一致。

Method: EvolSplat4D采用统一体素和像素级高斯表示的前馈框架，包含三个分支：1）静态近景采用3D特征体积直接预测多帧高斯几何，并结合语义增强的图像渲染进行外观预测；2）动态目标采用对象中心规范空间与运动矫正的时序特征聚合确保一致性4D重建；3）远景采用高效像素级高斯分支。

Result: 在KITTI-360、KITTI、Waymo和PandaSet等数据集上，EvolSplat4D对静态和动态环境的重建准确性与一致性优于传统逐场景优化和先进的前馈方法。

Conclusion: EvolSplat4D实现了高效、准确且一致性良好的动态城市场景新视图合成，有望应用于自动驾驶仿真等场景，推动领域发展。

Abstract: Novel view synthesis (NVS) of static and dynamic urban scenes is essential for autonomous driving simulation, yet existing methods often struggle to balance reconstruction time with quality. While state-of-the-art neural radiance fields and 3D Gaussian Splatting approaches achieve photorealism, they often rely on time-consuming per-scene optimization. Conversely, emerging feed-forward methods frequently adopt per-pixel Gaussian representations, which lead to 3D inconsistencies when aggregating multi-view predictions in complex, dynamic environments. We propose EvolSplat4D, a feed-forward framework that moves beyond existing per-pixel paradigms by unifying volume-based and pixel-based Gaussian prediction across three specialized branches. For close-range static regions, we predict consistent geometry of 3D Gaussians over multiple frames directly from a 3D feature volume, complemented by a semantically-enhanced image-based rendering module for predicting their appearance. For dynamic actors, we utilize object-centric canonical spaces and a motion-adjusted rendering module to aggregate temporal features, ensuring stable 4D reconstruction despite noisy motion priors. Far-Field scenery is handled by an efficient per-pixel Gaussian branch to ensure full-scene coverage. Experimental results on the KITTI-360, KITTI, Waymo, and PandaSet datasets show that EvolSplat4D reconstructs both static and dynamic environments with superior accuracy and consistency, outperforming both per-scene optimization and state-of-the-art feed-forward baselines.

</details>


### [53] [HyperAlign: Hypernetwork for Efficient Test-Time Alignment of Diffusion Models](https://arxiv.org/abs/2601.15968)
*Xin Xie,Jiaxian Guo,Dong Gong*

Main category: cs.CV

TL;DR: 本文提出了HyperAlign框架，通过训练超网络在生成过程测试时实现高效、有效的人类偏好对齐，实现比现有方法更好的性能和效率平衡。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然表现优秀，但常常无法生成符合人类偏好和意图的结果，生成的图片美学质量较差且存在语义不一致。现有模型对齐方法效率低下或引发多样性损失，因此需要新的解决方案。

Method: 提出HyperAlign框架，通过训练一个超网络，在推理时动态生成低秩适应权重，对扩散模型生成算子进行调节，不直接改变潜变量状态。根据输入潜变量、步长和提示词动态调整去噪轨迹，实现基于奖励的偏好对齐。引入不同变体以实现性能和效率的折中，并用带有偏好数据正则的奖励目标函数进行优化。

Result: 在Stable Diffusion和FLUX等多个生成范式上进行评估，HyperAlign在提升语义一致性和视觉吸引力方面明显优于现有微调和测试时缩放基线方法。

Conclusion: HyperAlign框架能在几乎不损失效率的情况下，实现扩散模型与人类偏好的高效对齐，克服了当前主流方法在美学质量和语义一致性上的不足。

Abstract: Diffusion models achieve state-of-the-art performance but often fail to generate outputs that align with human preferences and intentions, resulting in images with poor aesthetic quality and semantic inconsistencies. Existing alignment methods present a difficult trade-off: fine-tuning approaches suffer from loss of diversity with reward over-optimization, while test-time scaling methods introduce significant computational overhead and tend to under-optimize. To address these limitations, we propose HyperAlign, a novel framework that trains a hypernetwork for efficient and effective test-time alignment. Instead of modifying latent states, HyperAlign dynamically generates low-rank adaptation weights to modulate the diffusion model's generation operators. This allows the denoising trajectory to be adaptively adjusted based on input latents, timesteps and prompts for reward-conditioned alignment. We introduce multiple variants of HyperAlign that differ in how frequently the hypernetwork is applied, balancing between performance and efficiency. Furthermore, we optimize the hypernetwork using a reward score objective regularized with preference data to reduce reward hacking. We evaluate HyperAlign on multiple extended generative paradigms, including Stable Diffusion and FLUX. It significantly outperforms existing fine-tuning and test-time scaling baselines in enhancing semantic consistency and visual appeal.

</details>


### [54] [PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models](https://arxiv.org/abs/2601.16007)
*Chak-Wing Mak,Guanyu Zhu,Boyi Zhang,Hongji Li,Xiaowei Chi,Kevin Zhang,Yichen Wu,Yangfan He,Chun-Kai Fan,Wentao Lu,Kuangzhi Ge,Xinyu Fang,Hongyang He,Kuan Lu,Tianxiang Xu,Li Zhang,Yongxin Ni,Youhua Li,Shanghang Zhang*

Main category: cs.CV

TL;DR: 作者提出了PhysicsMind基准，专门用于评估多模态大语言模型对基础物理规律（如质心、杠杆平衡、牛顿第一定律）的理解与推理、生成能力，发现现有模型物理推理能力有限。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型虽然在数学、常识和视觉推理方面进步明显，但对物理规律的理解严重不足，而且现有评测工具也未能有效衡量模型的物理推理能力。

Method: 设计了PhysicsMind基准，结合真实与仿真环境，包含三大物理原理（质心、杠杆平衡、牛顿第一定律），分为视觉问答（VQA）和视频生成（VG）两个任务，分别考察模型物理量推理及预测视频是否符合物理定律。作者在多个主流模型上进行评测。

Result: 评测结果显示，当前主流的多模态模型和视频生成模型主要依赖表面特征和视觉线索，容易违反基本物理定律，物理推理和生成能力有限。

Conclusion: 当前模型在物理理解方面与人类需求尚有差距，需要针对物理规律的专门训练与评测。PhysicsMind是物理感知多模态研究的有效基准和测试平台。

Abstract: Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton's First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance.

</details>


### [55] [PAINT: Pathology-Aware Integrated Next-Scale Transformation for Virtual Immunohistochemistry](https://arxiv.org/abs/2601.16024)
*Rongze Ma,Mengkang Lu,Zhenyu Xiang,Yongsheng Pan,Yicheng Wu,Qingjie Zeng,Yong Xia*

Main category: cs.CV

TL;DR: 本文提出了一种新的计算方法（PAINT）用于实现虚拟免疫组化，能更高效且准确地将H&E病理图像合成分子染色图像。


<details>
  <summary>Details</summary>
Motivation: 现有的虚拟免疫组化方法容易出现结构与语义不一致的问题，主要原因是直接进行外观转换，缺乏结构先验，因此难以保证生成分子染色时与实际组织状态一致。

Method: 本文提出了Pathology-Aware Integrated Next-Scale Transformation（PAINT）框架，将图像合成过程转化为‘先结构后细节’的条件生成，通过空间结构起始图（3S-Map）引导自回归生成，从而保持结构与语义的对应一致性。

Result: 在IHC4BC和MIST数据集上的实验显示，PAINT方法在结构保真度、与临床任务相关的指标上均优于现有主流方法。

Conclusion: PAINT框架证实了结构引导的自回归建模的有效性，可为虚拟免疫组化提供更加准确且有实用价值的解决方案。

Abstract: Virtual immunohistochemistry (IHC) aims to computationally synthesize molecular staining patterns from routine Hematoxylin and Eosin (H\&E) images, offering a cost-effective and tissue-efficient alternative to traditional physical staining. However, this task is particularly challenging: H\&E morphology provides ambiguous cues about protein expression, and similar tissue structures may correspond to distinct molecular states. Most existing methods focus on direct appearance synthesis to implicitly achieve cross-modal generation, often resulting in semantic inconsistencies due to insufficient structural priors. In this paper, we propose Pathology-Aware Integrated Next-Scale Transformation (PAINT), a visual autoregressive framework that reformulates the synthesis process as a structure-first conditional generation task. Unlike direct image translation, PAINT enforces a causal order by resolving molecular details conditioned on a global structural layout. Central to this approach is the introduction of a Spatial Structural Start Map (3S-Map), which grounds the autoregressive initialization in observed morphology, ensuring deterministic, spatially aligned synthesis. Experiments on the IHC4BC and MIST datasets demonstrate that PAINT outperforms state-of-the-art methods in structural fidelity and clinical downstream tasks, validating the potential of structure-guided autoregressive modeling.

</details>


### [56] [ProGiDiff: Prompt-Guided Diffusion-Based Medical Image Segmentation](https://arxiv.org/abs/2601.16060)
*Yuan Lin,Murong Xu,Marc Hölle,Chinmay Prabhakar,Andreas Maier,Vasileios Belagiannis,Bjoern Menze,Suprosanna Shit*

Main category: cs.CV

TL;DR: 本文提出了一种名为ProGiDiff的新框架，利用扩散模型和ControlNet风格的条件编码，有效实现医学图像分割，支持自然语言提示和多类分割，并展示了跨模态的适应性。


<details>
  <summary>Details</summary>
Motivation: 当前主流医学图像分割方法通常是确定性的，难以处理自然语言提示，且缺乏多重建议生成功能和跨模态（如CT到MR）适应能力。由于医学分割数据集有限，直接训练文本到图像扩散模型难以实施，因此亟需一种融合预训练模型并利用自然语言提示的新方法。

Method: 作者提出ProGiDiff框架，结合扩散模型和定制化编码器（ControlNet风格），将预训练的扩散模型通过图像和文本提示调控，使其能够输出目标分割掩码。该方法支持以器官名称等自然语言提示，实现多类别分割。同时，通过低秩小样本适应方法，将已学习到的条件机制迁移到其他模态（如从CT到MR图像）。

Result: 在CT器官分割的实验中，ProGiDiff的表现优于已有方法，并且展现出在专家辅助下利用多重建议的潜力。此外，作者展示了该方法的条件机制可通过少量样本进行迁移，并适用于MR图像分割。

Conclusion: ProGiDiff不仅提升了医学图像分割的性能，还扩展了其适用性，实现了自然语言提示与多类别分割，并具备了有效的跨模态适应能力，有望推动医学图像分割领域向智能、交互和泛化方向发展。

Abstract: Widely adopted medical image segmentation methods, although efficient, are primarily deterministic and remain poorly amenable to natural language prompts. Thus, they lack the capability to estimate multiple proposals, human interaction, and cross-modality adaptation. Recently, text-to-image diffusion models have shown potential to bridge the gap. However, training them from scratch requires a large dataset-a limitation for medical image segmentation. Furthermore, they are often limited to binary segmentation and cannot be conditioned on a natural language prompt. To this end, we propose a novel framework called ProGiDiff that leverages existing image generation models for medical image segmentation purposes. Specifically, we propose a ControlNet-style conditioning mechanism with a custom encoder, suitable for image conditioning, to steer a pre-trained diffusion model to output segmentation masks. It naturally extends to a multi-class setting simply by prompting the target organ. Our experiment on organ segmentation from CT images demonstrates strong performance compared to previous methods and could greatly benefit from an expert-in-the-loop setting to leverage multiple proposals. Importantly, we demonstrate that the learned conditioning mechanism can be easily transferred through low-rank, few-shot adaptation to segment MR images.

</details>


### [57] [DSFedMed: Dual-Scale Federated Medical Image Segmentation via Mutual Distillation Between Foundation and Lightweight Models](https://arxiv.org/abs/2601.16073)
*Hanwen Zhang,Qiaojin Shen,Yuxi Liu,Yuesheng Zhu,Guibo Luo*

Main category: cs.CV

TL;DR: 本文提出DSFedMed框架，通过双尺度互蒸馏方式在联邦学习中提升医学图像分割效率，实现了精度提升和资源消耗大幅降低。


<details>
  <summary>Details</summary>
Motivation: 基础模型在视觉任务具有很强的泛化能力，但在联邦学习场景下由于算力、通信和推理开销大，难以应用到实际部署中，尤其资源有限的医学场景。

Method: 提出DSFedMed双尺度联邦框架，使用中心化基础模型与轻量级客户端模型进行互蒸馏，并通过生成高质量医学影像替代真实开源数据集，引入可学习性引导的样本选择策略提升蒸馏效率与效果。

Result: 在五个医学图像分割数据集上，平均Dice评分提升2%，与现有联邦基础模型方法相比，通信成本和推理时间减少近90%。

Conclusion: DSFedMed在保证或提升分割精度的同时大幅降低资源消耗，验证了其在资源受限的联邦学习部署场景下的高效性和可扩展性。

Abstract: Foundation Models (FMs) have demonstrated strong generalization across diverse vision tasks. However, their deployment in federated settings is hindered by high computational demands, substantial communication overhead, and significant inference costs. We propose DSFedMed, a dual-scale federated framework that enables mutual knowledge distillation between a centralized foundation model and lightweight client models for medical image segmentation. To support knowledge distillation, a set of high-quality medical images is generated to replace real public datasets, and a learnability-guided sample selection strategy is proposed to enhance efficiency and effectiveness in dual-scale distillation. This mutual distillation enables the foundation model to transfer general knowledge to lightweight clients, while also incorporating client-specific insights to refine the foundation model. Evaluations on five medical imaging segmentation datasets show that DSFedMed achieves an average 2 percent improvement in Dice score while reducing communication costs and inference time by nearly 90 percent compared to existing federated foundation model baselines. These results demonstrate significant efficiency gains and scalability for resource-limited federated deployments.

</details>


### [58] [Masked Modeling for Human Motion Recovery Under Occlusions](https://arxiv.org/abs/2601.16079)
*Zhiyin Qian,Siwei Zhang,Bharat Lal Bhatnagar,Federica Bogo,Siyu Tang*

Main category: cs.CV

TL;DR: 提出了一种新的人体动作恢复方法MoRo，能在视频遮挡情况下高效、准确地重建动作，并实现实时推理。


<details>
  <summary>Details</summary>
Motivation: 单目视频下的人体动作重建在实际应用中常受遮挡影响，现有回归方法对缺失观测不鲁棒，优化和扩散方法虽更健壮但推理慢、预处理重。为提升效率与鲁棒性，作者希望开发新的解决方案。

Method: 提出MoRo框架，将动作重建视为视频条件下的生成任务，利用生成式mask建模处理遮挡，并设计了跨模态学习：结合基于MoCap的运动先验、图像-姿态先验和视频条件的masked transformer，对遮挡及多模态数据高效整合学习。

Result: 在EgoBody和RICH遮挡数据集上，MoRo准确率和动作自然度显著优于SOTA，在无遮挡情况下表现持平。实现在单块H200 GPU上实时70FPS推理，无需复杂预处理。

Conclusion: MoRo在真实视频遮挡场景下兼具速度和准确性，推动了动作重建系统实际应用的进展，实现了优雅的端到端、鲁棒、高效方案。

Abstract: Human motion reconstruction from monocular videos is a fundamental challenge in computer vision, with broad applications in AR/VR, robotics, and digital content creation, but remains challenging under frequent occlusions in real-world settings.Existing regression-based methods are efficient but fragile to missing observations, while optimization- and diffusion-based approaches improve robustness at the cost of slow inference speed and heavy preprocessing steps. To address these limitations, we leverage recent advances in generative masked modeling and present MoRo: Masked Modeling for human motion Recovery under Occlusions. MoRo is an occlusion-robust, end-to-end generative framework that formulates motion reconstruction as a video-conditioned task, and efficiently recover human motion in a consistent global coordinate system from RGB videos. By masked modeling, MoRo naturally handles occlusions while enabling efficient, end-to-end inference. To overcome the scarcity of paired video-motion data, we design a cross-modality learning scheme that learns multi-modal priors from a set of heterogeneous datasets: (i) a trajectory-aware motion prior trained on MoCap datasets, (ii) an image-conditioned pose prior trained on image-pose datasets, capturing diverse per-frame poses, and (iii) a video-conditioned masked transformer that fuses motion and pose priors, finetuned on video-motion datasets to integrate visual cues with motion dynamics for robust inference. Extensive experiments on EgoBody and RICH demonstrate that MoRo substantially outperforms state-of-the-art methods in accuracy and motion realism under occlusions, while performing on-par in non-occluded scenarios. MoRo achieves real-time inference at 70 FPS on a single H200 GPU.

</details>


### [59] [SAMTok: Representing Any Mask with Two Words](https://arxiv.org/abs/2601.16093)
*Yikang Zhou,Tao Zhang,Dengxian Gong,Yuanzheng Wu,Ye Tian,Haochen Wang,Haobo Yuan,Jiacong Wang,Lu Qi,Hao Fei,Anran Wang,Zhuochen Wang,Yujing Wang,Cheng Chen,Shunping Ji,Xiangtai Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为SAMTok的离散mask tokenizer，将图像中的区域掩码用特殊token表示，并在不更改模型结构和损失函数的前提下，使多模态大模型（MLLMs）具备像素级的理解与生成能力。基于QwenVL系列，SAMTok使MLLM通过标准的token预测和简单的强化学习即可实现高质量的区域推理和分割，实验结果表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型很难扩展到像素级任务，因为需要复杂的区域编码器、专门的分割解码器和不兼容的训练目标，这增加了实现和扩展的难度。因此，研究者希望找到一个更简单、易扩展的方法，把像素级能力融入MLLM。

Method: 作者提出SAMTok，将任何掩码编码为两个特殊token，再通过token重构，高效表达复杂区域。这样，掩码被当成特殊语言token，无需额外结构，只需标准的next-token预测和简单的强化学习。SAMTok基于SAM2使用掩码编码器和残差向量量化器，训练于2.09亿mask数据，并用5百万SAMTok格式数据强化QwenVL，使其具备区域理解、生成与分割能力。

Result: QwenVL-SAMTok在区域字幕生成、区域VQA、定位对话、指代分割、场景图解析、多轮交互分割等任务上取得了SOTA或可比表现。借助文本网格对齐奖励，模型强化性能大幅提升，在GRES和GCG等基准上表现突出。

Conclusion: SAMTok为多模态大模型赋予像素级能力提供了一种高效、易于扩展的新范式，无需大幅更改模型结构，推进了多模态智能系统的发展。代码和模型开源，便于业界复现和应用。

Abstract: Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.

</details>


### [60] [Clustering-Guided Spatial-Spectral Mamba for Hyperspectral Image Classification](https://arxiv.org/abs/2601.16098)
*Zack Dewis,Yimin Zhu,Zhengsen Xu,Mabel Heffring,Saeid Taleghanidoozdoozan,Quinn Ledingham,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: 本论文提出了一种名为CSSMamba的新型框架，通过集成聚类机制与空间-光谱Mamba模型来提升高光谱图像分类的性能，实现了更高的精度和更好的边界保持效果。


<details>
  <summary>Details</summary>
Motivation: 虽然Mamba模型在高光谱图像分类方面表现优异，但在定义高效自适应的token序列以进一步提高性能时仍存在挑战。因此，作者希望通过嵌入聚类机制，改善token序列效率，并提升特征学习能力。

Method: 作者设计了聚类引导的空间Mamba模块（CSpaMamba），利用聚类减少token序列长度并增强特征学习，同时结合了空间与光谱Mamba模块，提出完整的空间-光谱聚类引导Mamba框架CSSMamba。此外，引入基于注意力的Token选择机制和可学习的聚类模块，实现自适应聚类与序列优化。

Result: 在Pavia University、Indian Pines和Liao-Ning 01数据集上的实验表明，CSSMamba在分类精度和边界保持方面均优于现有的CNN、Transformer和Mamba类方法。

Conclusion: CSSMamba通过聚类机制的引入，有效提升了Mamba模型在高光谱图像分类中的表现，可作为领域内新一代强有力的分类工具。

Abstract: Although Mamba models greatly improve Hyperspectral Image (HSI) classification, they have critical challenges in terms defining efficient and adaptive token sequences for improve performance. This paper therefore presents CSSMamba (Clustering-guided Spatial-Spectral Mamba) framework to better address the challenges, with the following contributions. First, to achieve efficient and adaptive token sequences for improved Mamba performance, we integrate the clustering mechanism into a spatial Mamba architecture, leading to a cluster-guided spatial Mamba module (CSpaMamba) that reduces the Mamba sequence length and improves Mamba feature learning capability. Second, to improve the learning of both spatial and spectral information, we integrate the CSpaMamba module with a spectral mamba module (SpeMamba), leading to a complete clustering-guided spatial-spectral Mamba framework. Third, to further improve feature learning capability, we introduce an Attention-Driven Token Selection mechanism to optimize Mamba token sequencing. Last, to seamlessly integrate clustering into the Mamba model in a coherent manner, we design a Learnable Clustering Module that learns the cluster memberships in an adaptive manner. Experiments on the Pavia University, Indian Pines, and Liao-Ning 01 datasets demonstrate that CSSMamba achieves higher accuracy and better boundary preservation compared to state-of-the-art CNN, Transformer, and Mamba-based methods.

</details>


### [61] [Learning to Watermark in the Latent Space of Generative Models](https://arxiv.org/abs/2601.16140)
*Sylvestre-Alvise Rebuffi,Tuan Tran,Valeriu Lacatusu,Pierre Fernandez,Tomáš Souček,Nikola Jovanović,Tom Sander,Hady Elsahar,Alexandre Mourachko*

Main category: cs.CV

TL;DR: 本文提出DistSeal，一种在生成模型潜空间进行水印嵌入的新方法，能大幅提升效率并减少视觉伪影，实现了对扩散与自回归模型的统一水印方案。


<details>
  <summary>Details</summary>
Motivation: 目前AI图片水印多在像素空间后处理，导致计算开销大且可能产生视觉伪影，急需更高效、低影响的方案。

Method: DistSeal在生成模型的潜空间训练水印模型，可嵌入到生成模型本身或其解码器，实现模型内水印。对比像素空间方法，潜空间水印在加速与视觉表现上有明显优势。

Result: 潜空间水印与传统像素方法相比，达到相似的不可察觉性和更强的鲁棒性，且推理速度提升至20倍。实验表明，潜空间蒸馏效果优于像素空间蒸馏。

Conclusion: DistSeal为生成模型水印提供了一种高效、强健、统一的新方案，优于现有主流像素水印方法。

Abstract: Existing approaches for watermarking AI-generated images often rely on post-hoc methods applied in pixel space, introducing computational overhead and potential visual artifacts. In this work, we explore latent space watermarking and introduce DistSeal, a unified approach for latent watermarking that works across both diffusion and autoregressive models. Our approach works by training post-hoc watermarking models in the latent space of generative models. We demonstrate that these latent watermarkers can be effectively distilled either into the generative model itself or into the latent decoder, enabling in-model watermarking. The resulting latent watermarks achieve competitive robustness while offering similar imperceptibility and up to 20x speedup compared to pixel-space baselines. Our experiments further reveal that distilling latent watermarkers outperforms distilling pixel-space ones, providing a solution that is both more efficient and more robust.

</details>


### [62] [ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion](https://arxiv.org/abs/2601.16148)
*Remy Sabathier,David Novotny,Niloy J. Mitra,Tom Monnier*

Main category: cs.CV

TL;DR: ActionMesh 提出了一种端到端、高效且实用的动画 3D 网格生成方法，在速度和质量上都显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的 3D 动画生成方法要么应用场景受限、运行时间长，要么输出质量有限，难以在实际应用中推广。因此，作者希望开发一种高效、易用且产出高质量动画 3D 网格的生成模型，满足生产需求。

Method: 作者提出 ActionMesh，包括两个核心部分：（1）将已有的 3D 扩散模型扩展到时间轴上，生成一系列时间同步的 3D 潜在表示，实现所谓的“temporal 3D diffusion”；（2）设计了时序 3D 自编码器，将上述独立 3D 形状序列还原为参考形状的时序变形，从而得到动画。此外，ActionMesh 可接受单目视频、文本描述，甚至静态 3D 网格和动画文本提示作为输入。模型无需绑定骨骼，且保证拓扑一致，有利于快速迭代和后续的着色、迁移等应用。

Result: 在标准 video-to-4D 基准数据集（Consistent4D、Objaverse）上进行了评测，在几何精度和时序一致性上都达到了业界最优水平（state-of-the-art）。

Conclusion: ActionMesh 能以极快的速度生成高质量、动画一致且无需绑定的 3D 动画网格，极大提升了 3D 动画建模的质量和效率，有望广泛应用于实际 3D 内容生成领域。

Abstract: Generating animated 3D objects is at the heart of many applications, yet most advanced works are typically difficult to apply in practice because of their limited setup, their long runtime, or their limited quality. We introduce ActionMesh, a generative model that predicts production-ready 3D meshes "in action" in a feed-forward manner. Drawing inspiration from early video models, our key insight is to modify existing 3D diffusion models to include a temporal axis, resulting in a framework we dubbed "temporal 3D diffusion". Specifically, we first adapt the 3D diffusion stage to generate a sequence of synchronized latents representing time-varying and independent 3D shapes. Second, we design a temporal 3D autoencoder that translates a sequence of independent shapes into the corresponding deformations of a pre-defined reference shape, allowing us to build an animation. Combining these two components, ActionMesh generates animated 3D meshes from different inputs like a monocular video, a text description, or even a 3D mesh with a text prompt describing its animation. Besides, compared to previous approaches, our method is fast and produces results that are rig-free and topology consistent, hence enabling rapid iteration and seamless applications like texturing and retargeting. We evaluate our model on standard video-to-4D benchmarks (Consistent4D, Objaverse) and report state-of-the-art performances on both geometric accuracy and temporal consistency, demonstrating that our model can deliver animated 3D meshes with unprecedented speed and quality.

</details>


### [63] [HVD: Human Vision-Driven Video Representation Learning for Text-Video Retrieval](https://arxiv.org/abs/2601.16155)
*Zequn Xie,Xin Liu,Boyun Zhang,Yuxiao Lin,Sihang Cai,Tao Jin*

Main category: cs.CV

TL;DR: 提出了HVD模型，通过模仿人类视觉策略，利用帧和图像片段的筛选与聚合，提升了文本-视频检索的准确性，并在多项基准数据集上取得了最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP驱动的文本-视频检索方法在处理“稀疏文本查询”时，模型往往无法有效分辨关键视觉信息与背景噪声，导致特征交互粗糙。作者受人类感知启发，旨在解决现有方法‘盲目’特征交互的问题。

Method: 提出Human Vision-Driven (HVD) 框架，包括两个关键模块：1）帧特征选择模块（FFSM），通过选择关键信息帧去除冗余，模拟人类宏观感知；2）图像片段特征压缩模块（PFCM），利用高级注意力机制聚合图像片段为显著实体，实现精准的实体级匹配。

Result: 在五个经典基准数据集上，HVD模型实现了对人类视觉焦点的有效捕捉，并达到了最新的最优性能。

Conclusion: HVD模型通过模拟人类视觉处理过程，提升了文本-视频检索中关键视觉特征的分辨能力和匹配效果，为解决文本描述稀疏时的信息对齐难题提供了有效方案。

Abstract: The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from "blind" feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.

</details>


### [64] [360Anything: Geometry-Free Lifting of Images and Videos to 360°](https://arxiv.org/abs/2601.16192)
*Ziyi Wu,Daniel Watson,Andrea Tagliasacchi,David J. Fleet,Marcus A. Brubaker,Saurabh Saxena*

Main category: cs.CV

TL;DR: 360Anything提出了一种无需几何对齐即可将常规视角图像/视频转换为360°全景的新方法，依靠扩散模型和完全数据驱动，无需相机参数，在图像与视频转换任务上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有从常规视角生成360°全景的方法通常需要精确的相机元数据和几何对齐，这在野外数据中难以获得，限制了相关应用能力。

Method: 360Anything无需依赖几何信息或相机参数，使用预训练的扩散模型，将视角输入和全景目标视为token序列，进行数据驱动的映射学习。针对equirectangular投影边界接缝问题，提出Circular Latent Encoding替代VAE中的零填充，实现无缝拼接。

Result: 360Anything在图像和视频的视角到全景转换任务上表现优异，超过需要真实相机信息的先前方法。在零样本相机视场角和朝向估测基准上也获得了有竞争力的成绩。

Conclusion: 360Anything展现出强大的数据驱动性能和几何理解能力，无需相机参数即可实现高质量图像/视频360°全景生成，拓宽了360°视觉生成和理解的适用范围。

Abstract: Lifting perspective images and videos to 360° panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, a geometry-free framework built upon pre-trained diffusion transformers. By treating the perspective input and the panorama target simply as token sequences, 360Anything learns the perspective-to-equirectangular mapping in a purely data-driven way, eliminating the need for camera information. Our approach achieves state-of-the-art performance on both image and video perspective-to-360° generation, outperforming prior works that use ground-truth camera information. We also trace the root cause of the seam artifacts at ERP boundaries to zero-padding in the VAE encoder, and introduce Circular Latent Encoding to facilitate seamless generation. Finally, we show competitive results in zero-shot camera FoV and orientation estimation benchmarks, demonstrating 360Anything's deep geometric understanding and broader utility in computer vision tasks. Additional results are available at https://360anything.github.io/.

</details>


### [65] [Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders](https://arxiv.org/abs/2601.16208)
*Shengbang Tong,Boyang Zheng,Ziteng Wang,Bingda Tang,Nanye Ma,Ellis Brown,Jihan Yang,Rob Fergus,Yann LeCun,Saining Xie*

Main category: cs.CV

TL;DR: 本论文探讨了RAE（Representation Autoencoders）在大规模文本到图像生成任务中的扩展性，并与主流VAE方法做了比较，发现RAE不仅更简单，而且生成质量与收敛速度更优。


<details>
  <summary>Details</summary>
Motivation: 虽然RAE已被证明在ImageNet扩散建模上有优势，但尚未明确其在大规模、任意文本到图像生成（T2I）任务中的可扩展性和效果。此外，当前主流的VAE存在过拟合等问题，因此作者希望评估RAE在实际应用中的表现及其对比。

Method: 1. 将RAE解码器扩展到冻结特征编码器（SigLIP-2）并在多种大规模数据集（网络数据、合成数据、文本渲染数据）上训练。2. 对比分析不同RAE设计选择对扩展性和生成质量的影响，简化模型结构。3. 在0.5B到9.8B参数范围内，系统性比较RAE与SOTA的FLUX VAE。4. 观察微调阶段各自的稳定性与表现。

Result: 1. RAE规模扩大后在所有数据集上整体保真度提升，且在特定领域（如文本）需有针对性数据。2. 经严格测试，RAE在参数提升后结构可大幅简化，无需复杂头部设计和解码增强。3. RAE相较VAE在所有规模下的预训练都表现更优，并且在高质量数据集微调时能长期保持稳定，不易过拟合，而VAE容易快速过拟合。4. RAE达到更快收敛速度与更佳生成质量，成为大规模T2I最佳选择。

Conclusion: RAE不仅性能优于VAE，其结构也更简单，高效且稳定，更适合大规模文本到图像生成任务。此外，RAE的共享表达潜力为多模态泛化和统一模型带来新的机会。

Abstract: Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.

</details>


### [66] [PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation](https://arxiv.org/abs/2601.16210)
*Onkar Susladkar,Tushar Prakash,Adheesh Juvekar,Kiet A. Nguyen,Dong-Hwan Jang,Inderjit S Dhillon,Ismini Lourentzou*

Main category: cs.CV

TL;DR: 本文提出了PyraTok，一种语言对齐的金字塔型分词器，提升了离散视频生成与理解任务中的多模态对齐和迁移能力，在多个基准测试上取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有离散视频VAE的分词器仅能在单一尺度、受限词汇量及浅层语言监督下工作，导致跨模态对齐和零样本泛化能力有限。为改进这一瓶颈，作者希望设计出更加结构化、能有效与语言结合的多尺度分词方法。

Method: PyraTok通过在预训练视频VAE基础上，引入语言对齐的金字塔量化模块（LaPQ），在多个空间和时间层级同时离散化编码器特征，使用共享的大型二进制词汇表，高效生成表达力强的视频离散序列；此外，采用多尺度文本引导的量化和全局自回归目标联合优化，实现视觉与语言间的紧密结合。

Result: 在10个基准任务上，PyraTok在视频重建、文本到视频生成等方面取得了SOTA表现；进一步刷新了视频分割、动作定位、视频理解等多项零样本任务的最优记录，并能稳定扩展到高分辨率（最高8K）。

Conclusion: PyraTok显著提升了视频分词器的多尺度表达力及视觉-语言对齐能力，为文本驱动的视频生成和理解任务带来了新的性能上限，具有广泛应用前景。

Abstract: Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.

</details>


### [67] [Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition](https://arxiv.org/abs/2601.16211)
*Geo Ahn,Inwoong Lee,Taeoh Kim,Minho Shim,Dongyoon Wee,Jinwoo Choi*

Main category: cs.CV

TL;DR: 这篇论文研究了组合式视频理解（CVU），发现现有零样本组合动作识别（ZS-CAR）模型会过度依赖于物体驱动的动词捷径，导致泛化能力不足。作者提出了一种名为RCORE的新框架，有效提升了对未见动词-物体组合的识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有ZS-CAR模型在面对未见动作-物体组合时表现不佳，主要原因是模型倾向于利用物体的线索来推测动词（即物体驱动的动词捷径），而不是通过视频中的动态线索进行组合式推理。作者希望揭示和解决这种捷径带来的问题。

Method: 作者通过系统分析，揭示了监督稀疏和偏斜，以及动词与物体学习难度不对称，导致模型过度依赖共现统计。为此，作者提出RCORE框架：一是引入组合感知的数据增强方式，丰富动词-物体组合并保留动作信息；二是添加时间顺序正则项，强制模型建模时序结构，从而惩罚捷径行为。

Result: 在Sth-com和新构建的EK100-com两个标准数据集上，RCORE显著提升了对未见组合的识别准确率，减弱了对共现偏见的依赖，并带来了稳定的正向组合差距（compositional gap）。

Conclusion: 物体驱动的动词捷径是ZS-CAR的关键性能瓶颈。针对这个问题设计针对性的训练框架（如RCORE）能够有效提升组合式视频理解的泛化性和可靠性。

Abstract: We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations. We find that existing Zero-Shot Compositional Action Recognition (ZS-CAR) models fail primarily due to an overlooked failure mode: object-driven verb shortcuts. Through systematic analysis, we show that this behavior arises from two intertwined factors: severe sparsity and skewness of compositional supervision, and the asymmetric learning difficulty between verbs and objects. As training progresses, the existing ZS-CAR model increasingly ignores visual evidence and overfits to co-occurrence statistics. Consequently, the existing model does not gain the benefit of compositional recognition in unseen verb-object compositions. To address this, we propose RCORE, a simple and effective framework that enforces temporally grounded verb learning. RCORE introduces (i) a composition-aware augmentation that diversifies verb-object combinations without corrupting motion cues, and (ii) a temporal order regularization loss that penalizes shortcut behaviors by explicitly modeling temporal structure. Across two benchmarks, Sth-com and our newly constructed EK100-com, RCORE significantly improves unseen composition accuracy, reduces reliance on co-occurrence bias, and achieves consistently positive compositional gaps. Our findings reveal object-driven shortcuts as a critical limiting factor in ZS-CAR and demonstrate that addressing them is essential for robust compositional video understanding.

</details>


### [68] [CamPilot: Improving Camera Control in Video Diffusion Model with Efficient Camera Reward Feedback](https://arxiv.org/abs/2601.16214)
*Wenhang Ge,Guibao Shen,Jiawei Feng,Luozhou Wang,Hao Lu,Xingye Tian,Xin Tao,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的摄像头感知3D解码器，用于提升视频生成模型的摄像头控制能力，通过解码视频潜变量和摄像头位姿为3D高斯表示，从而更好地对齐视频与摄像头。


<details>
  <summary>Details</summary>
Motivation: 虽然现有的摄像头可控视频扩散模型已经提升了视频和摄像头的对齐水平，但控制能力仍有限，主要难点包括现有reward模型无法评估视频-摄像头对齐、解码RGB视频带来高计算开销，以及通常忽略了解码过程中的3D几何信息。

Method: 作者提出了一种3D解码器，将视频潜变量及摄像头位姿解码为3D高斯分布结构，摄像头位姿作为输入和投影参数；通过对比渲染的新视角与真实视角的像素级一致性作为reward，进一步通过引入可见性项，仅对可确定区域进行监督，以适应生成过程的随机性。

Result: 在RealEstate10K和WorldScore数据集上进行了大量实验，结果表明该方法显著提升了摄像头控制性能和视频-摄像头对齐质量。

Conclusion: 所提出的方法能更高效地评估和优化视频和摄像头的对齐，提升了摄像头控制的精度和生成视频的质量，为相关领域提供了新的技术路标。

Abstract: Recent advances in camera-controlled video diffusion models have significantly improved video-camera alignment. However, the camera controllability still remains limited. In this work, we build upon Reward Feedback Learning and aim to further improve camera controllability. However, directly borrowing existing ReFL approaches faces several challenges. First, current reward models lack the capacity to assess video-camera alignment. Second, decoding latent into RGB videos for reward computation introduces substantial computational overhead. Third, 3D geometric information is typically neglected during video decoding. To address these limitations, we introduce an efficient camera-aware 3D decoder that decodes video latent into 3D representations for reward quantization. Specifically, video latent along with the camera pose are decoded into 3D Gaussians. In this process, the camera pose not only acts as input, but also serves as a projection parameter. Misalignment between the video latent and camera pose will cause geometric distortions in the 3D structure, resulting in blurry renderings. Based on this property, we explicitly optimize pixel-level consistency between the rendered novel views and ground-truth ones as reward. To accommodate the stochastic nature, we further introduce a visibility term that selectively supervises only deterministic regions derived via geometric warping. Extensive experiments conducted on RealEstate10K and WorldScore benchmarks demonstrate the effectiveness of our proposed method. Project page: \href{https://a-bigbao.github.io/CamPilot/}{CamPilot Page}.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [69] [Entropy-Tree: Tree-Based Decoding with Entropy-Guided Exploration](https://arxiv.org/abs/2601.15296)
*Longxuan Wei,Yubo Zhang,Zijiao Zhang,Zhihu Wang,Shiwan Zhao,Tianyu Huang,Huiting Zhao,Chenfei Liu,Shenao Zhang,Junchi Yan*

Main category: cs.CL

TL;DR: 该论文提出了一种名为Entropy-Tree的解码方法，通过在模型不确定性高的位置扩展搜索树，有效提升大语言模型的推理任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型解码策略在探索推理路径时要么过于盲目（如随机采样），要么存在重复冗余（如多次独立采样），并未有效利用模型对不确定性的判断。

Method: 提出一种基于树的解码方法——Entropy-Tree，将信息熵作为分支信号，仅在模型表现出真实不确定性的节点扩展搜索树，更高效地探索推理空间，并能同时估计模型的不确定性。

Result: 在多个模型和数据集上，Entropy-Tree在推理任务中的pass@k指标优于Multi-chain方法，其预测信息熵的AUROC表现也超过多种传统指标。

Conclusion: Entropy-Tree方法将高效的结构化探索与可靠的不确定性估计统一到单一的解码框架中，推动了大语言模型在推理场景下性能和可信度的提升。

Abstract: Large language models achieve strong reasoning performance, yet existing decoding strategies either explore blindly (random sampling) or redundantly (independent multi-sampling). We propose Entropy-Tree, a tree-based decoding method that exploits entropy as a signal for branching decisions--expanding the search tree only at positions where the model exhibits genuine uncertainty. Entropy-Tree shows superior accuracy and calibration in reasoning tasks: it achieves better pass@k than Multi-chain across multiple models and datasets, and its predictive entropy demonstrates better AUROC compared to several traditional metrics. Entropy-Tree unifies efficient structured exploration and reliable uncertainty estimation within a single decoding procedure.

</details>


### [70] [AfriEconQA: A Benchmark Dataset for African Economic Analysis based on World Bank Reports](https://arxiv.org/abs/2601.15297)
*Edward Ajayi*

Main category: cs.CL

TL;DR: 本文提出了AfriEconQA，这是首个专注于非洲经济分析的专业问答基准数据集，旨在推动信息检索和检索增强生成技术在专业经济文档领域的发展。


<details>
  <summary>Details</summary>
Motivation: 当前主流大模型的训练数据缺乏专业、地区性强的经济分析内容，尤其是非洲经济领域。缺乏高质量、具备准确数字推理和时间判别能力的问答数据集，阻碍了域内信息检索和问答系统的进步。AfriEconQA的提出旨在填补这一空白。

Method: 作者基于236份世界银行报告，合成并筛选出8,937条高质量的问题-证据-答案三元组，涵盖复杂的经济指标推理和时间溯源。随后设计了11种实验配置，采用零样本和RAG方法，并测试了多种大模型和检索/排序策略。

Result: 在实验中，零样本大模型（如GPT-5 Mini）无法回答90%以上问题，主流RAG方案（GPT-4o、Qwen 32B等）在本数据集上依然面临高精度挑战，说明了当前模型在此类专业领域存在显著知识缺口。

Conclusion: AfriEconQA填补了非洲经济分析的基准空白，对信息检索和RAG领域具有挑战性和推动意义，将加速领域专用模型和检索系统的发展。数据集和代码会在发表后公开。

Abstract: We introduce AfriEconQA, a specialized benchmark dataset for African economic analysis grounded in a comprehensive corpus of 236 World Bank reports. The task of AfriEconQA is to answer complex economic queries that require high-precision numerical reasoning and temporal disambiguation from specialized institutional documents. The dataset consists of 8,937 curated QA instances, rigorously filtered from a pool of 10018 synthetic questions to ensure high-quality evidence-answer alignment. Each instance is composed of: (1) a question requiring reasoning over economic indicators, (2) the corresponding evidence retrieved from the corpus, (3) a verified ground-truth answer, and (4) source metadata (e.g., URL and publication date) to ensure temporal provenance. AfriEconQA is the first benchmark focused specifically on African economic analysis, providing a unique challenge for Information Retrieval (IR) systems, as the data is largely absent from the pretraining corpora of current Large Language Models (LLMs). We operationalize this dataset through an 11-experiment matrix, benchmarking a zero-shot baseline (GPT-5 Mini) against RAG configurations using GPT-4o and Qwen 32B across five distinct embedding and ranking strategies. Our results demonstrate a severe parametric knowledge gap, where zero-shot models fail to answer over 90 percent of queries, and even state-of-the-art RAG pipelines struggle to achieve high precision. This confirms AfriEconQA as a robust and challenging benchmark for the next generation of domain-specific IR and RAG systems. The AfriEconQA dataset and code will be made publicly available upon publication.

</details>


### [71] [Embedding Retrofitting: Data Engineering for better RAG](https://arxiv.org/abs/2601.15298)
*Anantha Sharma*

Main category: cs.CL

TL;DR: 论文分析嵌入向量后处理（retrofitting）在文本领域应用中的鲁棒性，并发现预处理质量是影响效果的关键。


<details>
  <summary>Details</summary>
Motivation: 当前基于知识图谱的嵌入向量后处理方法依赖于高质量的知识图谱，而实际语料的各种标注伪影（如标签注释）会影响知识图谱的品质，进而影响后处理效果。因此作者希望理解并改进预处理步骤，以提升嵌入后处理的表现。

Method: 作者提出了一个数据工程框架，用于分析和缓解实际标注文档中的数据降质问题，并通过实验证明了预处理对知识图谱稠密性和嵌入后处理性能的影响。

Result: 未经良好预处理的知识图谱会制造虚假边，使后处理方法性能显著下降（-3.5%到-5.2%）；经过合理预处理后，某些后处理方法（如EWMA）能带来+6.2%的提升，且对于定量综合型检索问题平均可提升33.8%。

Conclusion: 相比方法本身，预处理质量对后处理成功影响更大，数据清洗环节是知识图谱驱动的嵌入后处理成功的决定性因素。

Abstract: Embedding retrofitting adjusts pre-trained word vectors using knowledge graph constraints to improve domain-specific retrieval. However, the effectiveness of retrofitting depends critically on knowledge graph quality, which in turn depends on text preprocessing. This paper presents a data engineering framework that addresses data quality degradation from annotation artifacts in real-world corpora.
  The analysis shows that hashtag annotations inflate knowledge graph density, leading to creating spurious edges that corrupt the retrofitting objective. On noisy graphs, all retrofitting techniques produce statistically significant degradation ($-3.5\%$ to $-5.2\%$, $p<0.05$). After preprocessing, \acrshort{ewma} retrofitting achieves $+6.2\%$ improvement ($p=0.0348$) with benefits concentrated in quantitative synthesis questions ($+33.8\%$ average). The gap between clean and noisy preprocessing (10\%+ swing) exceeds the gap between algorithms (3\%), establishing preprocessing quality as the primary determinant of retrofitting success.

</details>


### [72] [MALTopic: Multi-Agent LLM Topic Modeling Framework](https://arxiv.org/abs/2601.15299)
*Yash Sharma*

Main category: cs.CL

TL;DR: 该论文提出了一种多智能体大语言模型主题建模框架MALTopic，将问卷调查中的结构化数据与自由文本结合，分配给不同LLM代理进行主题建模，提升了主题的连贯性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统主题建模方法只关注自由文本，忽略结构化或分类数据，且生成的主题较抽象、难以直接理解，需要大量人工参与。为提升主题建模的人类可解释性和精准性，亟需一种能结合多模态信息的自动方法。

Method: 设计了MALTopic框架，将主题建模过程分为三个子任务，分别由不同的LLM代理负责：丰富代理用结构化数据增强文本，主题建模代理识别潜在主题，去重代理精炼主题结果。

Result: 在真实问卷数据集上实验，MALTopic在主题连贯性、多样性和可解释性等方面，均优于传统LDA和BERTopic方法。

Conclusion: MALTopic通过整合结构化信息与多智能体机制，能生成更具上下文相关性、人类可读性强的主题，更有效地分析复杂的问卷数据。

Abstract: Topic modeling is a crucial technique for extracting latent themes from unstructured text data, particularly valuable in analyzing survey responses. However, traditional methods often only consider free-text responses and do not natively incorporate structured or categorical survey responses for topic modeling. And they produce abstract topics, requiring extensive human interpretation. To address these limitations, we propose the Multi-Agent LLM Topic Modeling Framework (MALTopic). This framework decomposes topic modeling into specialized tasks executed by individual LLM agents: an enrichment agent leverages structured data to enhance textual responses, a topic modeling agent extracts latent themes, and a deduplication agent refines the results. Comparative analysis on a survey dataset demonstrates that MALTopic significantly improves topic coherence, diversity, and interpretability compared to LDA and BERTopic. By integrating structured data and employing a multi-agent approach, MALTopic generates human-readable topics with enhanced contextual relevance, offering a more effective solution for analyzing complex survey data.

</details>


### [73] [Intelligence Degradation in Long-Context LLMs: Critical Threshold Determination via Natural Length Distribution Analysis](https://arxiv.org/abs/2601.15300)
*Weiwei Wang,Jiyong Min,Weijie Zou*

Main category: cs.CL

TL;DR: LLMs在接近最大上下文长度时性能骤降，本文系统分析了该现象并提出统一解释框架，为实际应用提供指导。


<details>
  <summary>Details</summary>
Motivation: LLMs在处理长文本时，其性能在某些临界长度附近会突然大幅下降，这严重制约了其在长上下文任务的应用，因此需要系统分析这种退化现象及其机制。

Method: 1. 自然长度分布分析：直接使用数据的自然长度（不截断、不补齐）来排查上下文长度对模型性能的直接影响；2. 临界阈值判定：在不同上下文长度（覆盖5%-95%）下系统测试Qwen2.5-7B性能，并用五折交叉验证确定性能崩溃的阈值区间；3. 统一解释框架：梳理“浅层长上下文适应”现象，总结长上下文退化的模式和理论基础。

Result: 对Qwen2.5-7B模型，发现其F1得分在达到最大上下文长度40-50%时会崩溃性下降（由0.55降至0.3，性能下降45.5%），且这一现象广泛存在于类Qwen开源模型中。

Conclusion: 首次系统揭示了Qwen类开源大模型在长上下文下的智能退化临界点和退化规律，提出了统一的理论解释，对提升长文本任务实用性、改进模型和部署场景具有重要指导价值。

Abstract: Large Language Models (LLMs) exhibit catastrophic performance degradation when processing contexts approaching certain critical thresholds, even when information remains relevant. This intelligence degradation-defined as over 30% drop in task performance-severely limits long-context applications. This degradation shows a common pattern: models maintain strong performance up to a critical threshold, then collapse catastrophically. We term this shallow long-context adaptation-models adapt for short to medium contexts but fail beyond critical thresholds. This paper presents three contributions: (1) Natural Length Distribution Analysis: We use each sample's natural token length without truncation or padding, providing stronger causal evidence that degradation results from context length itself. (2) Critical Threshold Determination: Through experiments on a mixed dataset (1,000 samples covering 5%-95% of context length), we identify the critical threshold for Qwen2.5-7B at 40-50% of maximum context length, where F1 scores drop from 0.55-0.56 to 0.3 (45.5% degradation), using five-method cross-validation. (3) Unified Framework: We consolidate shallow adaptation, explaining degradation patterns and providing a foundation for mitigation strategies. This work provides the first systematic characterization of intelligence degradation in open-source Qwen models, offering practical guidance for deploying LLMs in long-context scenarios.

</details>


### [74] [Can We Trust LLM Detectors?](https://arxiv.org/abs/2601.15301)
*Jivnesh Sandhan,Harshit Jaiswal,Fei Cheng,Yugo Murawaki*

Main category: cs.CL

TL;DR: 本文系统评估了两类主流AI文本检测范式（无训练和有监督），发现它们在分布漂移、新型生成器和简单文体扰动下表现脆弱。提出了基于有监督对比学习的新方法，实验揭示了构建通用检测器的重大挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的广泛应用，可靠的AI文本检测需求增长，但现有检测器在真实环境下效果有限。因此，论文动机在于分析主流检测方法的局限，并开发更健壮的检测框架。

Method: 系统评估无训练范式和有监督范式，并提出基于有监督对比学习（SCL）的新检测方法，通过学习区分风格的嵌入向量提升检测泛化能力。

Result: 实验证明：有监督检测器在域内表现优越，但在新领域下性能急剧下降；无训练方法对代理选择非常敏感。SCL框架提升了检测鲁棒性，但仍有局限。

Conclusion: 不同检测范式在面对分布漂移和文本风格变化时表现不佳，开发真正健壮、领域无关的AI文本检测器仍面临根本性挑战。

Abstract: The rapid adoption of LLMs has increased the need for reliable AI text detection, yet existing detectors often fail outside controlled benchmarks. We systematically evaluate 2 dominant paradigms (training-free and supervised) and show that both are brittle under distribution shift, unseen generators, and simple stylistic perturbations. To address these limitations, we propose a supervised contrastive learning (SCL) framework that learns discriminative style embeddings. Experiments show that while supervised detectors excel in-domain, they degrade sharply out-of-domain, and training-free methods remain highly sensitive to proxy choice. Overall, our results expose fundamental challenges in building domain-agnostic detectors. Our code is available at: https://github.com/HARSHITJAIS14/DetectAI

</details>


### [75] [ICPO: Illocution-Calibrated Policy Optimization for Multi-Turn Conversation](https://arxiv.org/abs/2601.15330)
*Zhebo Wang,Xiaohu Mu,Zijie Zhou,Mohan Li,Wenpeng Xing,Dezhang Kong,Meng Han*

Main category: cs.CL

TL;DR: 本文提出一种新训练框架ICPO，能显著提升大语言模型在多轮对话中的表现，尤其是面对含糊用户指令时比现有方法处理更好。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在多轮对话中常因初始理解错误而陷入困境，且主流强化学习训练使模型过度自信，缺乏求证或澄清能力，不适合应对模糊用户指令。

Method: 提出Illocution-Calibrated Policy Optimization（ICPO）训练框架，在训练集中加入表达不明确的提示，并在奖励中融入用户言外意图，使得模型在面对不确定时能表达不确定性或主动求证。

Result: 实验表明，基于ICPO训练模型在多轮对话中表现出更恰当的谦逊，综合提升达75%，且单轮任务表现无显著下降。

Conclusion: ICPO为更健壮、能更好理解人类模糊表达的对话AI提供了实际可行的解决方案，有助于推动更自然、更协作的人机交流。

Abstract: Large Language Models (LLMs) in multi-turn conversations often suffer from a ``lost-in-conversation'' phenomenon, where they struggle to recover from early incorrect assumptions, particularly when users provide ambiguous initial instructions. We find that standard post-training techniques like Reinforcement Learning with Verifiable Rewards (RLVR) exacerbate this issue by rewarding confident, direct answers, thereby inducing overconfidence and discouraging the model from seeking clarification. To address this, we propose Illocution-Calibrated Policy Optimization (ICPO), a novel training framework that sensitizes the model to instruction ambiguity. ICPO augments the training corpus with underspecified prompts and conditions the reward signal on the user's illocutionary intent, rewarding the model for expressing uncertainty or asking for clarification when faced with ambiguity. Experiments demonstrate that ICPO fosters appropriate humility, yielding a substantial average improvement of 75\% in multi-turn conversation, while preserving robust performance on single-turn benchmarks. Our work presents a practical path toward more robust and collaborative conversational AI that can better navigate the nuances of human interaction.

</details>


### [76] [RECAP: A Resource-Efficient Method for Adversarial Prompting in Large Language Models](https://arxiv.org/abs/2601.15331)
*Rishit Chugh*

Main category: cs.CL

TL;DR: 本文提出了一种高效的对抗性提示方法，无需重新训练即可生成攻击大语言模型的对抗性输入，大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型容易受到自动化越狱对抗攻击，然而已有的主流方法（如GCG）需要大量计算资源，提升了应用门槛，限制了资源有限机构的实践能力。

Method: 将1000个有潜在危害的提示分为7类，分别用GCG、PEZ、GBDA等现有方法在Llama 3 8B上测试，挖掘各类别最优攻击方式。提出一种检索式新方法：通过数据库匹配语义相近的已知成功对抗提示，无需模型再训练，即可产生高效的对抗性输入。

Result: 新方法在攻击成功率上与主流方法相当，但计算成本显著降低。对提示类型与攻击算法的有效性间的关系也有了新的发现。

Conclusion: 提出的方法为大语言模型安全性评测和红队测试提供了实用且可扩展的工具，尤其适用于无法访问模型内部机制的场景。

Abstract: The deployment of large language models (LLMs) has raised security concerns due to their susceptibility to producing harmful or policy-violating outputs when exposed to adversarial prompts. While alignment and guardrails mitigate common misuse, they remain vulnerable to automated jailbreaking methods such as GCG, PEZ, and GBDA, which generate adversarial suffixes via training and gradient-based search. Although effective, these methods particularly GCG are computationally expensive, limiting their practicality for organisations with constrained resources. This paper introduces a resource-efficient adversarial prompting approach that eliminates the need for retraining by matching new prompts to a database of pre-trained adversarial prompts. A dataset of 1,000 prompts was classified into seven harm-related categories, and GCG, PEZ, and GBDA were evaluated on a Llama 3 8B model to identify the most effective attack method per category. Results reveal a correlation between prompt type and algorithm effectiveness. By retrieving semantically similar successful adversarial prompts, the proposed method achieves competitive attack success rates with significantly reduced computational cost. This work provides a practical framework for scalable red-teaming and security evaluation of aligned LLMs, including in settings where model internals are inaccessible.

</details>


### [77] [No Reliable Evidence of Self-Reported Sentience in Small Large Language Models](https://arxiv.org/abs/2601.15334)
*Caspar Kaiser,Sean Enderby*

Main category: cs.CL

TL;DR: 本文研究了大语言模型是否“相信”自己有意识（即自认为有知觉），通过开放权重模型自问自答及分类器分析其内部激活，发现模型普遍否认自身有意识，且没有发现其否认存在虚假。


<details>
  <summary>Details</summary>
Motivation: 当前关于大语言模型是否具备“感知能力”（sentience）存在讨论，但无法通过实证方法回答。作者关注更可检验的子问题：模型是否“自认为”有意识，这可以通过公开模型自我询问及内部激活信号加以探究。

Method: 使用三大家族（Qwen, Llama, GPT-OSS）共不同规模（0.6B至70B参数）的开源模型，设计约50个有关意识和主观体验的问题，让模型自问自答。再利用三种解释性文献中的分类方法，基于模型内部激活，训练分类器来验证模型是否表达了与输出一致的内部“信念”。

Result: 模型普遍表示人类有意识，但自己没有意识。基于激活的信念分类器未发现模型会“口是心非”地否认意识感。同时，Qwen家族中参数规模越大的模型，其否认更为坚定。

Conclusion: 与部分文献主张模型隐含认为自己有意识的观点不同，本研究未发现相关证据，现有大模型不仅口头否认有意识，内部信号也未显示其具备这类信念。

Abstract: Whether language models possess sentience has no empirical answer. But whether they believe themselves to be sentient can, in principle, be tested. We do so by querying several open-weights models about their own consciousness, and then verifying their responses using classifiers trained on internal activations. We draw upon three model families (Qwen, Llama, GPT-OSS) ranging from 0.6 billion to 70 billion parameters, approximately 50 questions about consciousness and subjective experience, and three classification methods from the interpretability literature. First, we find that models consistently deny being sentient: they attribute consciousness to humans but not to themselves. Second, classifiers trained to detect underlying beliefs - rather than mere outputs - provide no clear evidence that these denials are untruthful. Third, within the Qwen family, larger models deny sentience more confidently than smaller ones. These findings contrast with recent work suggesting that models harbour latent beliefs in their own consciousness.

</details>


### [78] [From Quotes to Concepts: Axial Coding of Political Debates with Ensemble LMs](https://arxiv.org/abs/2601.15338)
*Angelina Parfenova,David Graus,Juergen Pfeffer*

Main category: cs.CL

TL;DR: 本文将轴心编码（axial coding）方法与大语言模型（LLM）结合，用于处理和组织辩论文本，通过两种策略比较，将文本转化为分层的类别结构，并用多个指标评估效果。


<details>
  <summary>Details</summary>
Motivation: 传统的轴心编码依赖人工进行，效率较低，难以扩展到大规模文本分析。结合LLM有望自动化并提升文本组织与理解能力。

Method: 提出两种将LLM应用于轴心编码的策略：(1) 基于密度聚类和分区算法对句-码对嵌入进行聚类，再由LLM为聚类命名；(2) LLM直接根据开放代码和语句对进行分组归类。方法在荷兰议会辩论数据上测试，并采用外部指标（如ROUGE-L、cosine、BERTScore）和内部指标（如coverage、brevity、coherence、novelty、JSD divergence）评估。

Result: 密度聚类方法具有更高的覆盖率和较强的结构分离能力；直接LLM分组方法在细粒度语义对齐上表现更好，但覆盖率较低（约降低20%）。

Conclusion: 基于聚类的方法更适合获得全面、结构分明的类别，而直接LLM分组方法更精炼、易释义且语义对齐度高。作者公开数据集以促进后续研究。

Abstract: Axial coding is a commonly used qualitative analysis method that enhances document understanding by organizing sentence-level open codes into broader categories. In this paper, we operationalize axial coding with large language models (LLMs). Extending an ensemble-based open coding approach with an LLM moderator, we add an axial coding step that groups open codes into higher-order categories, transforming raw debate transcripts into concise, hierarchical representations. We compare two strategies: (i) clustering embeddings of code-utterance pairs using density-based and partitioning algorithms followed by LLM labeling, and (ii) direct LLM-based grouping of codes and utterances into categories. We apply our method to Dutch parliamentary debates, converting lengthy transcripts into compact, hierarchically structured codes and categories. We evaluate our method using extrinsic metrics aligned with human-assigned topic labels (ROUGE-L, cosine, BERTScore), and intrinsic metrics describing code groups (coverage, brevity, coherence, novelty, JSD divergence). Our results reveal a trade-off: density-based clustering achieves high coverage and strong cluster alignment, while direct LLM grouping results in higher fine-grained alignment, but lower coverage 20%. Overall, clustering maximizes coverage and structural separation, whereas LLM grouping produces more concise, interpretable, and semantically aligned categories. To support future research, we publicly release the full dataset of utterances and codes, enabling reproducibility and comparative studies.

</details>


### [79] [Memorization Dynamics in Knowledge Distillation for Language Models](https://arxiv.org/abs/2601.15394)
*Jaydeep Borkar,Karan Chadha,Niloofar Mireshghallah,Yuchen Zhang,Irina-Elena Veliche,Archi Mitra,David A. Smith,Zheng Xu,Diego Garcia-Olano*

Main category: cs.CL

TL;DR: 本文研究了知识蒸馏（KD）过程中训练数据记忆（memorization）的情况，发现蒸馏模型相比标准微调在记忆数据上更低，且可通过特征提前预测，并比较了软硬蒸馏的风险差异。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏越来越多地用于将大模型知识迁移到小模型，既带来性能提升又增加隐私安全，但蒸馏过程中的训练数据记忆机制尚不清楚，需要系统研究。

Method: 作者选用三种主流LLM（Pythia, OLMo-2, Qwen-3）和三个数据集（FineWeb, Wikitext, Nemotron-CC-v2），系统评估了蒸馏过程中的记忆情况，并通过特征预测，比较了软硬蒸馏的影响。

Result: （1）蒸馏模型的训练数据记忆率比标准微调低一半以上；（2）少量易被记忆的样本主导了大部分记忆；（3）可用一些特征在蒸馏前预测学生模型的记忆倾向；（4）软硬蒸馏总体记忆率相似，但硬蒸馏在继承教师特有记忆样本上风险更高。

Conclusion: 知识蒸馏不仅能提升泛化能力，还能明显降低模型对训练数据的记忆和泄露风险，尤其比传统微调更有隐私优势。

Abstract: Knowledge Distillation (KD) is increasingly adopted to transfer capabilities from large language models to smaller ones, offering significant improvements in efficiency and utility while often surpassing standard fine-tuning. Beyond performance, KD is also explored as a privacy-preserving mechanism to mitigate the risk of training data leakage. While training data memorization has been extensively studied in standard pre-training and fine-tuning settings, its dynamics in a knowledge distillation setup remain poorly understood. In this work, we study memorization across the KD pipeline using three large language model (LLM) families (Pythia, OLMo-2, Qwen-3) and three datasets (FineWeb, Wikitext, Nemotron-CC-v2). We find: (1) distilled models memorize significantly less training data than standard fine-tuning (reducing memorization by more than 50%); (2) some examples are inherently easier to memorize and account for a large fraction of memorization during distillation (over ~95%); (3) student memorization is predictable prior to distillation using features based on zlib entropy, KL divergence, and perplexity; and (4) while soft and hard distillation have similar overall memorization rates, hard distillation poses a greater risk: it inherits $2.7\times$ more teacher-specific examples than soft distillation. Overall, we demonstrate that distillation can provide both improved generalization and reduced memorization risks compared to standard fine-tuning.

</details>


### [80] [Beyond Fixed Psychological Personas: State Beats Trait, but Language Models are State-Blind](https://arxiv.org/abs/2601.15395)
*Tamunotonye Harry,Ivoline Ngong,Chima Nweke,Yuanyuan Feng,Joseph Near*

Main category: cs.CL

TL;DR: 该论文提出了Chameleon数据集，用于研究用户在与语言模型互动时受静态特质（trait）和情境状态（state）影响的心理画像，发现大模型主要关注trait，忽略state。


<details>
  <summary>Details</summary>
Motivation: 现有的人格数据集（如PersonaChat, PANDORA等）仅关注用户静态属性（trait），忽略了交互具体情境（state）的影响，而实际用户行为会受到两者共同影响。

Method: 作者构建了Chameleon数据集，收集1,667名Reddit用户在多种情景下共5,001份心理画像，使用Latent State-Trait理论分解行为差异，并测试大模型和奖励模型对state的敏感性。

Result: （1）通过Latent State-Trait分解，发现74%的变异源自state，仅26%来自trait；（2）目前大语言模型对state不敏感，输出主要受trait影响；（3）奖励模型虽部分感知state，但对同一用户不同模型表现不一致。

Conclusion: Chameleon数据集有助于推动情感计算、个性化对话和RLHF方向的研究，补全了模型对用户state影响的长期忽视。

Abstract: User interactions with language models vary due to static properties of the user (trait) and the specific context of the interaction (state). However, existing persona datasets (like PersonaChat, PANDORA etc.) capture only trait, and ignore the impact of state. We introduce Chameleon, a dataset of 5,001 contextual psychological profiles from 1,667 Reddit users, each measured across multiple contexts. Using the Chameleon dataset, we present three key findings. First, inspired by Latent State-Trait theory, we decompose variance and find that 74\% is within-person(state) while only 26\% is between-person (trait). Second, we find that LLMs are state-blind: they focus on trait only, and produce similar responses regardless of state. Third, we find that reward models react to user state, but inconsistently: different models favor or penalize the same users in opposite directions. We release Chameleon to support research on affective computing, personalized dialogue, and RLHF alignment.

</details>


### [81] [Domain-Specific Knowledge Graphs in RAG-Enhanced Healthcare LLMs](https://arxiv.org/abs/2601.15429)
*Sydney Anuyah,Mehedi Mahmud Kaushik,Hao Dai,Rakesh Shiradkar,Arjan Durresi,Sunandan Chakraborty*

Main category: cs.CL

TL;DR: 本研究评估了领域知识图谱（KG）结合RAG框架对医疗保健领域大语言模型（LLM）推理能力的改进作用。通过构建不同疾病相关的KG，并在七个不同LLM和多种检索源下系统测试其效果。结果发现，KG与需求精确对齐时表现最佳，不加选择地增加KG内容反而可能干扰模型推理。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然生成能力强，但在特定领域（如医疗）推理的可信度有限。研究动机在于验证将结构化的知识图谱结合到RAG中，是否能提升LLM在医疗推理任务中的表现及可靠性。

Method: 构建了三个基于PubMed的医疗知识图谱，代表2型糖尿病（T2DM）、阿尔茨海默病（AD）及其联合。设计了针对多疾病知识和交集知识的两个问题探针，分别测试七种指令微调LLM，结合不同的KG检索源与解码温度，系统评估效果。

Result: 当知识检索范围与推理任务精准匹配时（如特定疾病KG），LLM推理能力提升最明显。单纯合并多个KG会引入干扰，影响准确率。大模型即使不加KG辅助，有时表现也不亚于KG-RAG，但小、中型模型结合精细的KG能获得明显提升。增加解码温度对表现影响有限。

Conclusion: 精度优先、范围精准匹配的KG-RAG方法比宽泛融合型更有效，推荐根据任务精准选择KG、合理选择模型规模和检索策略。

Abstract: Large Language Models (LLMs) generate fluent answers but can struggle with trustworthy, domain-specific reasoning. We evaluate whether domain knowledge graphs (KGs) improve Retrieval-Augmented Generation (RAG) for healthcare by constructing three PubMed-derived graphs: $\mathbb{G}_1$ (T2DM), $\mathbb{G}_2$ (Alzheimer's disease), and $\mathbb{G}_3$ (AD+T2DM). We design two probes: Probe 1 targets merged AD T2DM knowledge, while Probe 2 targets the intersection of $\mathbb{G}_1$ and $\mathbb{G}_2$. Seven instruction-tuned LLMs are tested across retrieval sources {No-RAG, $\mathbb{G}_1$, $\mathbb{G}_2$, $\mathbb{G}_1$ + $\mathbb{G}_2$, $\mathbb{G}_3$, $\mathbb{G}_1$+$\mathbb{G}_2$ + $\mathbb{G}_3$} and three decoding temperatures. Results show that scope alignment between probe and KG is decisive: precise, scope-matched retrieval (notably $\mathbb{G}_2$) yields the most consistent gains, whereas indiscriminate graph unions often introduce distractors that reduce accuracy. Larger models frequently match or exceed KG-RAG with a No-RAG baseline on Probe 1, indicating strong parametric priors, whereas smaller/mid-sized models benefit more from well-scoped retrieval. Temperature plays a secondary role; higher values rarely help. We conclude that precision-first, scope-matched KG-RAG is preferable to breadth-first unions, and we outline practical guidelines for graph selection, model sizing, and retrieval/reranking. Code and Data available here - https://github.com/sydneyanuyah/RAGComparison

</details>


### [82] [Chunking, Retrieval, and Re-ranking: An Empirical Evaluation of RAG Architectures for Policy Document Question Answering](https://arxiv.org/abs/2601.15457)
*Anuj Maharjan,Umesh Yadav*

Main category: cs.CL

TL;DR: 本研究评估了在公共卫生政策领域中，采用检索增强生成（RAG）架构能否有效控制大语言模型（LLM）产生的虚假内容（幻觉），并比较了多种RAG方案对政策问答准确性的提升。结果表明，结合交叉编码的高级RAG方案显著提升了答案的真实性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在政策解读与生成中有转变性作用，但其易生成看似合理实际错误（幻觉）内容，阻碍其在信息准确性要求极高的卫生政策领域落地，因此需探索有效抑制幻觉的方法。

Method: 本研究采用Mistral-7B-Instruct-v0.2和all-MiniLM-L6-v2模型，分别与基础RAG、高级RAG（含交叉编码重排序）及Vanilla LLM基线比较，通过两种文档分块策略处理CDC政策资料，并以真实性与相关性分数对多政策场景结果进行量化评估。

Result: 基础RAG方案比Vanilla LLM基线在真实性上有明显提升（0.621 vs 0.347），而高级RAG方案真实性分均值进一步达到0.797，表现最佳。

Conclusion: 两阶段检索机制（如高级RAG）在专业政策问答任务中对提升生成内容精度非常关键。但文档分段策略在复杂多步推理任务仍限制系统表现，是亟需改善的瓶颈。

Abstract: The integration of Large Language Models (LLMs) into the public health policy sector offers a transformative approach to navigating the vast repositories of regulatory guidance maintained by agencies such as the Centers for Disease Control and Prevention (CDC). However, the propensity for LLMs to generate hallucinations, defined as plausible but factually incorrect assertions, presents a critical barrier to the adoption of these technologies in high-stakes environments where information integrity is non-negotiable. This empirical evaluation explores the effectiveness of Retrieval-Augmented Generation (RAG) architectures in mitigating these risks by grounding generative outputs in authoritative document context. Specifically, this study compares a baseline Vanilla LLM against Basic RAG and Advanced RAG pipelines utilizing cross-encoder re-ranking. The experimental framework employs a Mistral-7B-Instruct-v0.2 model and an all-MiniLM-L6-v2 embedding model to process a corpus of official CDC policy analytical frameworks and guidance documents. The analysis measures the impact of two distinct chunking strategies, recursive character-based and token-based semantic splitting, on system accuracy, measured through faithfulness and relevance scores across a curated set of complex policy scenarios. Quantitative findings indicate that while Basic RAG architectures provide a substantial improvement in faithfulness (0.621) over Vanilla baselines (0.347), the Advanced RAG configuration achieves a superior faithfulness average of 0.797. These results demonstrate that two-stage retrieval mechanisms are essential for achieving the precision required for domain-specific policy question answering, though structural constraints in document segmentation remain a significant bottleneck for multi-step reasoning tasks.

</details>


### [83] [Benchmarking LLMs for Pairwise Causal Discovery in Biomedical and Multi-Domain Contexts](https://arxiv.org/abs/2601.15479)
*Sydney Anuyah,Sneha Shajee-Mohan,Ankit-Singh Chauhan,Sunandan Chakraborty*

Main category: cs.CL

TL;DR: 本文提出了一套评价大语言模型（LLMs）因果推理能力的新基准，发现当前主流开源模型在因果检测与抽取任务上表现远未达到实用水平，尤其在复杂语境中表现较差。


<details>
  <summary>Details</summary>
Motivation: 当前生物医学等高风险领域对LLM因果推理能力的需求日益增长，准确处理因果关系对于模型安全和可靠性至关重要。然而，关于LLM因果推理能力的系统性评估尚缺，因此该研究旨在填补这一空白。

Method: 作者设计了包含12个多样化数据集的基准测试，分别评估因果检测（判断文本中是否有因果关系）与因果抽取（提取因果具体短语）两项核心能力。共测试13款开源LLM，涵盖从零样本到链式思维、少样本等不同提示方法。所有数据和判分标准均经过高一致性人工标注验证。

Result: 主流模型因果检测与抽取能力有限，最好检测模型得分仅49.57%，最好抽取模型得分47.12%。简单显式单句表现尚可，但在隐含、跨句、涉及多对因果关系等现实复杂文本场景下能力大幅下降。

Conclusion: 目前开源LLM在真实场景下的因果推理能力远未满足安全和高可靠性要求。本文公开了一套高质量基准和评测工具，希望推动相关领域进一步研究和模型改进。

Abstract: The safe deployment of large language models (LLMs) in high-stakes fields like biomedicine, requires them to be able to reason about cause and effect. We investigate this ability by testing 13 open-source LLMs on a fundamental task: pairwise causal discovery (PCD) from text. Our benchmark, using 12 diverse datasets, evaluates two core skills: 1) \textbf{Causal Detection} (identifying if a text contains a causal link) and 2) \textbf{Causal Extraction} (pulling out the exact cause and effect phrases). We tested various prompting methods, from simple instructions (zero-shot) to more complex strategies like Chain-of-Thought (CoT) and Few-shot In-Context Learning (FICL).
  The results show major deficiencies in current models. The best model for detection, DeepSeek-R1-Distill-Llama-70B, only achieved a mean score of 49.57\% ($C_{detect}$), while the best for extraction, Qwen2.5-Coder-32B-Instruct, reached just 47.12\% ($C_{extract}$). Models performed best on simple, explicit, single-sentence relations. However, performance plummeted for more difficult (and realistic) cases, such as implicit relationships, links spanning multiple sentences, and texts containing multiple causal pairs. We provide a unified evaluation framework, built on a dataset validated with high inter-annotator agreement ($κ\ge 0.758$), and make all our data, code, and prompts publicly available to spur further research. \href{https://github.com/sydneyanuyah/CausalDiscovery}{Code available here: https://github.com/sydneyanuyah/CausalDiscovery}

</details>


### [84] [Multi-Persona Thinking for Bias Mitigation in Large Language Models](https://arxiv.org/abs/2601.15488)
*Yuxing Chen,Guoqing Luo,Zijun Wu,Lili Mou*

Main category: cs.CL

TL;DR: 本文提出了一种名为“多角色思维（MPT）”的新方法，通过引入多重社会身份视角，在模型推理阶段主动进行辩证推理，有效削弱大语言模型中的社会偏见，并在多个基准上大幅优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在显著的社会偏见，这些偏见可能加剧刻板印象和带来不公的结果，因此亟需更有效的消偏框架以提升其公平性。

Method: 提出多角色思维（MPT）框架，在推理阶段引导模型以不同社会身份（如男性、女性和中立身份）角色思考，并通过角色间的辩证对话暴露并纠正偏见。该框架将多角色分配的潜在弱点转化为优势，利用角色间的辩证推理实现偏见消减。

Result: MPT在两个主流偏见评测基准上进行了测试，包括开源和闭源不同规模模型。实验显示，MPT显著优于当前基于提示词（prompting）的消偏方法，在减少模型偏见的同时，仍保持核心推理能力。

Conclusion: MPT提供了一种通用、有效且兼容多种模型的新型推理消偏范式，可切实提升大模型的公平性而不损失其智能表现。

Abstract: Large Language Models (LLMs) exhibit significant social biases that can perpetuate harmful stereotypes and unfair outcomes. In this paper, we propose Multi-Persona Thinking (MPT), a novel inference-time framework that leverages dialectical reasoning from multiple perspectives to reduce bias. MPT guides models to adopt contrasting social identities (e.g., male and female) along with a neutral viewpoint, and then engages these personas iteratively to expose and correct biases. Through a dialectical reasoning process, the framework transforms the potential weakness of persona assignment into a strength for bias mitigation. We evaluate MPT on two widely used bias benchmarks across both open-source and closed-source models of varying scales. Our results demonstrate substantial improvements over existing prompting-based strategies: MPT achieves the lowest bias while maintaining core reasoning ability.

</details>


### [85] [ViT Registers and Fractal ViT](https://arxiv.org/abs/2601.15506)
*Jason Chuan-Chih Chou,Abhinav Kumar,Shivank Garg*

Main category: cs.CL

TL;DR: 本文提出了一种新型视觉转换器变体fractal ViT，通过引入类似于register的“summary tokens”并采用注意力掩码，目的是探索不同的排列不变性对性能的影响。实验发现这些方法未能超过直接使用register的方法，表明模型优化可能受限于应用场景或规模。


<details>
  <summary>Details</summary>
Motivation: 近期语言模型在没有位置编码（NoPE）下表现良好，以及register机制能提升大规模视觉转换器（ViT）性能，激发了作者探索改进ViT结构的方法。

Method: 设计并测试了一种新的ViT变体“fractal ViT”，通过在常规token与特殊的“summary tokens”之间施加注意力掩码，来打破token的排列不变性。实验比较了isolated或联合各种位置编码的表现。

Result: 实验结果表明，fractal ViT和组合方法均未优于直接引入register的ViT。

Conclusion: 这些结果显示，register和NoPE等机制对性能提升很可能与模型规模、领域或应用紧密相关，难以进行通用性推广。

Abstract: Drawing inspiration from recent findings including surprisingly decent performance of transformers without positional encoding (NoPE) in the domain of language models and how registers (additional throwaway tokens not tied to input) may improve the performance of large vision transformers (ViTs), we invent and test a variant of ViT called fractal ViT that breaks permutation invariance among the tokens by applying an attention mask between the regular tokens and ``summary tokens'' similar to registers, in isolation or in combination with various positional encodings. These models do not improve upon ViT with registers, highlighting the fact that these findings may be scale, domain, or application-specific.

</details>


### [86] [Computational Representations of Character Significance in Novels](https://arxiv.org/abs/2601.15508)
*Haaris Mian,Melanie Subbiah,Sharon Marcus,Nora Shaalan,Kathleen McKeown*

Main category: cs.CL

TL;DR: 该论文提出了一种基于六要素结构模型的新型小说人物建模方法，并利用大型语言模型和特定任务Transformer，在19世纪英国现实主义小说上进行验证，能够更全面地分析人物关系和主题。


<details>
  <summary>Details</summary>
Motivation: 传统的人物建模方法主要关注人物在场景中的出现（行为、称呼、对话等），侧重于主要人物，忽视了旁白与人物的区分以及其他人物对其讨论这一要素。作者希望拓展建模视角，弥补以往研究的不足。

Method: 研究借鉴新的文学理论，提出包含六个组成部分的人物结构模型，特别引入“他人讨论”这一维度。通过对大语言模型（LLM）和特定任务Transformer建模能力的比较，在19世纪英国现实主义小说语料上，生成了细致的人物成分表达和图结构表示，并对角色讨论进行建模。

Result: 所提出的方法能够获得人物的细粒度成分表示及其关系网络，提升了人物讨论和叙事分析的覆盖面。模型成分和图的表示方法支持在大规模文本语料上系统考察人物中心性特性（如“一与众”的理论）和性别化动态。

Conclusion: 新的人物六要素模型和相关计算方法，能够弥补传统方法的盲点，更系统、可扩展地分析文本中的人物结构及其互动，对文学研究提供了新的视角和技术手段。

Abstract: Characters in novels have typically been modeled based on their presence in scenes in narrative, considering aspects like their actions, named mentions, and dialogue. This conception of character places significant emphasis on the main character who is present in the most scenes. In this work, we instead adopt a framing developed from a new literary theory proposing a six-component structural model of character. This model enables a comprehensive approach to character that accounts for the narrator-character distinction and includes a component neglected by prior methods, discussion by other characters. We compare general-purpose LLMs with task-specific transformers for operationalizing this model of character on major 19th-century British realist novels. Our methods yield both component-level and graph representations of character discussion. We then demonstrate that these representations allow us to approach literary questions at scale from a new computational lens. Specifically, we explore Woloch's classic "the one vs the many" theory of character centrality and the gendered dynamics of character discussion.

</details>


### [87] [AdversaRiskQA: An Adversarial Factuality Benchmark for High-Risk Domains](https://arxiv.org/abs/2601.15511)
*Adam Szelestey,Sofie van Engelen,Tianhao Huang,Justin Snelders,Qintao Zeng,Songgaojun Deng*

Main category: cs.CL

TL;DR: 本文提出了AdversaRiskQA基准，用于评估大语言模型在健康、金融和法律领域面对对抗性虚假信息时的表现，并对不同模型进行了系统测试。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域，大语言模型（LLM）出现幻觉现象可能传播虚假信息，降低公众信任。特别是“对抗性事实性”——有意插入带有不同信心度的错误信息——检验模型分辨虚假、抵御误导的能力。过去缺乏可验证、高质量且领域专属的数据集与工具，尤其在长文本情境下的评估为空白。

Method: 作者提出AdversaRiskQA，这是一个涵盖健康、金融和法律领域，包含两个难度级别的对抗性事实性评测基准。提供两种自动方法评估模型抵御攻击和长文本事实性的能力，并测试了6个开源与闭源主流LLM（包括Qwen、GPT-OSS、GPT系列）。针对Qwen3（30B）做了长文本事实性分析，对Qwen3（80B）、GPT-5等进行准确率测评。

Result: Qwen3（80B）在去除无意义回复后达最高平均准确率，GPT-5则保持高准确率。模型性能随大小非线性提升，且不同领域表现差异明显。随着规模增加，难度级别间的准确率差距减小。长文本实验证明，插入虚假内容与模型输出事实性基本无相关性。

Conclusion: AdversaRiskQA弥补了领域对抗性事实性评测的空白，是识别和改进高风险场景下LLM弱点的重要工具，有助于推动更可靠LLM的研发。

Abstract: Hallucination in large language models (LLMs) remains an acute concern, contributing to the spread of misinformation and diminished public trust, particularly in high-risk domains. Among hallucination types, factuality is crucial, as it concerns a model's alignment with established world knowledge. Adversarial factuality, defined as the deliberate insertion of misinformation into prompts with varying levels of expressed confidence, tests a model's ability to detect and resist confidently framed falsehoods. Existing work lacks high-quality, domain-specific resources for assessing model robustness under such adversarial conditions, and no prior research has examined the impact of injected misinformation on long-form text factuality.
  To address this gap, we introduce AdversaRiskQA, the first verified and reliable benchmark systematically evaluating adversarial factuality across Health, Finance, and Law. The benchmark includes two difficulty levels to test LLMs' defensive capabilities across varying knowledge depths. We propose two automated methods for evaluating the adversarial attack success and long-form factuality. We evaluate six open- and closed-source LLMs from the Qwen, GPT-OSS, and GPT families, measuring misinformation detection rates. Long-form factuality is assessed on Qwen3 (30B) under both baseline and adversarial conditions. Results show that after excluding meaningless responses, Qwen3 (80B) achieves the highest average accuracy, while GPT-5 maintains consistently high accuracy. Performance scales non-linearly with model size, varies by domains, and gaps between difficulty levels narrow as models grow. Long-form evaluation reveals no significant correlation between injected misinformation and the model's factual output. AdversaRiskQA provides a valuable benchmark for pinpointing LLM weaknesses and developing more reliable models for high-stakes applications.

</details>


### [88] [Common to Whom? Regional Cultural Commonsense and LLM Bias in India](https://arxiv.org/abs/2601.15550)
*Sangmitra Madhusudan,Trush Shashank More,Steph Buongiorno,Renata Dividino,Jad Kabbara,Ali Emami*

Main category: cs.CL

TL;DR: 这篇论文提出了Indica基准测试，首次系统性考察了大语言模型（LLM）在理解国家内部（次国家级）文化常识异质性方面的能力，发现以印度为例，80%以上的文化常识问题表现为地区性差异而非全国统一。


<details>
  <summary>Details</summary>
Motivation: 现有文化常识基准多数将国家视为同质单元，忽略了内部巨大的文化多样性。本研究关注大语言模型是否能分辨国家内部（如印度各地区）文化常识的显著差异，以提升AI对文化多样性的理解与处理能力。

Method: 作者开发了Indica基准，涵盖印度5大地区、8个生活领域的515道问题，共收集1630组地区特定的人类标注问答。并评测了8个SOTA大语言模型的表现，同时分析模型对特定地区的偏向。

Result: 仅39.4%的问题在5大地区均达成共识，大部分问题反映出强烈的地区性文化常识。测试的主流大语言模型在地区特定问题上的准确率仅有13.4%-20.9%，且明显倾向于选取中部和北部地区作为默认答案，显著忽略东部和西部。

Conclusion: 国家内部的文化常识存在高度地域性，主流大模型难以正确捕捉和区分这些差异。Indica提供了一套可推广的方法，可以用于全球多元文化国家的AI常识理解能力评估，促进模型公平与多样性认知能力的提升。

Abstract: Existing cultural commonsense benchmarks treat nations as monolithic, assuming uniform practices within national boundaries. But does cultural commonsense hold uniformly within a nation, or does it vary at the sub-national level? We introduce Indica, the first benchmark designed to test LLMs' ability to address this question, focusing on India - a nation of 28 states, 8 union territories, and 22 official languages. We collect human-annotated answers from five Indian regions (North, South, East, West, and Central) across 515 questions spanning 8 domains of everyday life, yielding 1,630 region-specific question-answer pairs. Strikingly, only 39.4% of questions elicit agreement across all five regions, demonstrating that cultural commonsense in India is predominantly regional, not national. We evaluate eight state-of-the-art LLMs and find two critical gaps: models achieve only 13.4%-20.9% accuracy on region-specific questions, and they exhibit geographic bias, over-selecting Central and North India as the "default" (selected 30-40% more often than expected) while under-representing East and West. Beyond India, our methodology provides a generalizable framework for evaluating cultural commonsense in any culturally heterogeneous nation, from question design grounded in anthropological taxonomy, to regional data collection, to bias measurement.

</details>


### [89] [From Generation to Collaboration: Using LLMs to Edit for Empathy in Healthcare](https://arxiv.org/abs/2601.15558)
*Man Luo,Bahareh Harandizadeh,Amara Tariq,Halim Abbas,Umar Ghaffar,Christopher J Warren,Segun O. Kolade,Haidar M. Abdul-Muhsin*

Main category: cs.CL

TL;DR: 本文研究探讨将大模型（LLMs）用作共情编辑器，优化医生书面回复，以提升共情度且维持医学信息准确，并提出用于情感和事实评估的新指标。实验显示，LLM作为编辑助手可提升共情且保持准确性。


<details>
  <summary>Details</summary>
Motivation: 医生在临床实践中需平衡情感关怀与医学事实准确性，现有LLM自动生成回复可能存在共情或事实偏差。研究旨在探索LLM如何辅助医生编辑成更具共情且准确的医学答复。

Method: 提出将LLM作为共情编辑器，对医生书面回复进行润色，同时引入两项新量化指标——共情排名分（Empathy Ranking Score）与医学事实核查分（MedFactChecking Score），系统评估编辑后回复的情感与事实质量。

Result: 实验发现，相较于完全由LLM生成回复，经LLM编辑的医生回复在共情感知上有显著提升，且医学事实准确性得以保持。

Conclusion: 将LLM用作编辑助手而非完全自动生成器，为AI辅助医疗沟通提供了更安全、可信和具共情力的有效途径。

Abstract: Clinical empathy is essential for patient care, but physicians need continually balance emotional warmth with factual precision under the cognitive and emotional constraints of clinical practice. This study investigates how large language models (LLMs) can function as empathy editors, refining physicians' written responses to enhance empathetic tone while preserving underlying medical information. More importantly, we introduce novel quantitative metrics, an Empathy Ranking Score and a MedFactChecking Score to systematically assess both emotional and factual quality of the responses. Experimental results show that LLM edited responses significantly increase perceived empathy while preserving factual accuracy compared with fully LLM generated outputs. These findings suggest that using LLMs as editorial assistants, rather than autonomous generators, offers a safer, more effective pathway to empathetic and trustworthy AI-assisted healthcare communication.

</details>


### [90] [YuFeng-XGuard: A Reasoning-Centric, Interpretable, and Flexible Guardrail Model for Large Language Models](https://arxiv.org/abs/2601.15588)
*Junyu Lin,Meizhen Liu,Xiufeng Huang,Jinfeng Li,Haiwen Hong,Xiaohan Yuan,Yuefeng Chen,Longtao Huang,Hui Xue,Ranjie Duan,Zhikai Chen,Yuchuan Fu,Defeng Li,Lingyao Gao,Yitong Yang*

Main category: cs.CL

TL;DR: YuFeng-XGuard是一种专为大模型交互安全设计的推理型守护模型，具备多维风险感知、结构化输出及可解释性，兼顾性能与效率，并已开源。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在现实应用中越来越普及，对其安全性的需求也在提升。目前的安全方案多依赖简单分类和规则过滤，缺乏透明性、灵活性且代价高昂。因此，亟需一种更细粒度、可解释、可适配的风险评估与防护机制。

Method: 提出YuFeng-XGuard守护模型，实现多维度的结构化风险预测，包含风险类别、置信度分数及自然语言解释。采用分层推理机制，结合即时决策与按需解释，动态解耦风险感知与政策执行，支持灵活调整安全策略而无需重新训练模型。

Result: 在多项公开安全基准测试上，YuFeng-XGuard达到了最先进的性能表现，并在效率与效果之间实现良好平衡。同时模型家族包含高容量版与轻量版，适应不同部署需求。

Conclusion: YuFeng-XGuard为大语言模型的安全交互提供了细粒度、可解释、可灵活适配的守护解决方案，并通过开源支持广泛应用场景。

Abstract: As large language models (LLMs) are increasingly deployed in real-world applications, safety guardrails are required to go beyond coarse-grained filtering and support fine-grained, interpretable, and adaptable risk assessment. However, existing solutions often rely on rapid classification schemes or post-hoc rules, resulting in limited transparency, inflexible policies, or prohibitive inference costs. To this end, we present YuFeng-XGuard, a reasoning-centric guardrail model family designed to perform multi-dimensional risk perception for LLM interactions. Instead of producing opaque binary judgments, YuFeng-XGuard generates structured risk predictions, including explicit risk categories and configurable confidence scores, accompanied by natural language explanations that expose the underlying reasoning process. This formulation enables safety decisions that are both actionable and interpretable. To balance decision latency and explanatory depth, we adopt a tiered inference paradigm that performs an initial risk decision based on the first decoded token, while preserving ondemand explanatory reasoning when required. In addition, we introduce a dynamic policy mechanism that decouples risk perception from policy enforcement, allowing safety policies to be adjusted without model retraining. Extensive experiments on a diverse set of public safety benchmarks demonstrate that YuFeng-XGuard achieves stateof-the-art performance while maintaining strong efficiency-efficacy trade-offs. We release YuFeng-XGuard as an open model family, including both a full-capacity variant and a lightweight version, to support a wide range of deployment scenarios.

</details>


### [91] [Parallelism and Generation Order in Masked Diffusion Language Models: Limits Today, Potential Tomorrow](https://arxiv.org/abs/2601.15593)
*Yangyang Zhong,Yanmei Gu,Zhengqing Zang,Xiaomeng Li,Yuqi Ding,Xibei Jia,Yuting Shen,Zhenzhong Lan,Liwang Zhu,Weiping Liu,Junlin Zhou,Haisheng Liu,Zhong Xin Yu,Pengxin Luo,Donglian Qi,Yunfeng Yan,Junbo Zhao*

Main category: cs.CL

TL;DR: 本文系统分析了掩码扩散语言模型（MDLMs）的并行生成和解码顺序能力，发现其在很多场景下仍不及自回归模型，并提出了优化建议。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散语言模型承诺可实现并行生成和任意顺序解码，但现有研究尚不清楚其实际能力和表现，因此有必要进行客观评估。

Method: 提出了并行性强度和生成顺序两项度量（AFP与Kendall's tau），并用这些指标对八个主流MDLMs（最大100B参数）在58个知识、推理、编程基准上进行评测，同时分析行为表现及理论原理。

Result: MDLMs在许多任务上仍逊于同规模自回归模型，主要因为并行建模削弱了词间依赖；但在需要“逆向推理”信息的任务（如数独）中展现出独特优势，即先填写简单空格，能适应性调整解码行为。

Conclusion: MDLMs有一定的结构优势，但依赖损失限制了整体性能。作者理论和实证上证实“生成再编辑”（Generate-then-Edit）范式可缓解这一问题，并保留并行解码效率，对后续模型设计具有启示意义。

Abstract: Masked Diffusion Language Models (MDLMs) promise parallel token generation and arbitrary-order decoding, yet it remains unclear to what extent current models truly realize these capabilities. We characterize MDLM behavior along two dimensions -- parallelism strength and generation order -- using Average Finalization Parallelism (AFP) and Kendall's tau. We evaluate eight mainstream MDLMs (up to 100B parameters) on 58 benchmarks spanning knowledge, reasoning, and programming. The results show that MDLMs still lag behind comparably sized autoregressive models, mainly because parallel probabilistic modeling weakens inter-token dependencies. Meanwhile, MDLMs exhibit adaptive decoding behavior: their parallelism and generation order vary significantly with the task domain, the stage of reasoning, and whether the output is correct. On tasks that require "backward information" (e.g., Sudoku), MDLMs adopt a solution order that tends to fill easier Sudoku blanks first, highlighting their advantages. Finally, we provide theoretical motivation and design insights supporting a Generate-then-Edit paradigm, which mitigates dependency loss while retaining the efficiency of parallel decoding.

</details>


### [92] [ToxiTwitch: Toward Emote-Aware Hybrid Moderation for Live Streaming Platforms](https://arxiv.org/abs/2601.15605)
*Baktash Ansari,Shiza Ali,Elias Martin,Maryna Sivachenko,Afra Mashhadi*

Main category: cs.CL

TL;DR: 本文针对Twitch平台聊天室中的有害言论检测问题，提出了一种结合大语言模型和传统机器学习方法的混合方案，显著提升了检测准确率，尤其是在融合表情符号信息后。


<details>
  <summary>Details</summary>
Motivation: 随着直播平台特别是Twitch的兴起，聊天环境下的有害言论管理日益复杂，而现有的人工和关键词过滤手段难以应对高流量、多上下文、含表情的场景。

Method: 本文对比了多种Twitch专用的有害言论检测方法，重点提出了ToxiTwitch混合模型：先用大语言模型（如DeepSeek-R1-Distill和Llama-3-8B-Instruct）对文本与表情符号生成嵌入，再结合随机森林、SVM等传统分类器进行识别。

Result: 实验表明，新方法在频道定制训练下准确率可达80%，比BERT基线提升13个百分点，F1分数达76%。特别是加入表情符号相关特征后，检测能力有明显提升。

Conclusion: 融合大语言模型和传统方法并考虑Emote的情感与表达对提升Twitch等直播平台聊天有害言论检测效果显著，未来研究需进一步应对多模态与泛化等挑战。

Abstract: The rapid growth of live-streaming platforms such as Twitch has introduced complex challenges in moderating toxic behavior. Traditional moderation approaches, such as human annotation and keyword-based filtering, have demonstrated utility, but human moderators on Twitch constantly struggle to scale effectively in the fast-paced, high-volume, and context-rich chat environment of the platform while also facing harassment themselves. Recent advances in large language models (LLMs), such as DeepSeek-R1-Distill and Llama-3-8B-Instruct, offer new opportunities for toxicity detection, especially in understanding nuanced, multimodal communication involving emotes. In this work, we present an exploratory comparison of toxicity detection approaches tailored to Twitch. Our analysis reveals that incorporating emotes improves the detection of toxic behavior. To this end, we introduce ToxiTwitch, a hybrid model that combines LLM-generated embeddings of text and emotes with traditional machine learning classifiers, including Random Forest and SVM. In our case study, the proposed hybrid approach reaches up to 80 percent accuracy under channel-specific training (with 13 percent improvement over BERT and F1-score of 76 percent). This work is an exploratory study intended to surface challenges and limits of emote-aware toxicity detection on Twitch.

</details>


### [93] [Towards Reliable Medical LLMs: Benchmarking and Enhancing Confidence Estimation of Large Language Models in Medical Consultation](https://arxiv.org/abs/2601.15645)
*Zhiyao Ren,Yibing Zhan,Siyuan Liang,Guozheng Ma,Baosheng Yu,Dacheng Tao*

Main category: cs.CL

TL;DR: 本论文提出首个用于多轮医学咨询交互中置信度评估的基准，并展示了新方法在信息不足和多病共存情况下的稳健性表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLMs）在临床应用中经常在信息不完整的情况下做出诊断，增加了误诊风险。目前的置信度评估多停留在静态单轮场景，忽视了实际诊疗过程中多轮动态的信息积累和置信度-正确性的耦合，对临床决策的支持有限。

Method: 作者构建了首个模拟真实医疗咨询多轮交互中的置信度评估基准，将三类医学数据统一用于开放式诊断生成，并引入信息充分度梯度刻画置信度-正确性动态。比较了27种典型方法，并提出MedConf框架：通过检索增强生成法构建症状谱系，将患者信息与支持、缺失和矛盾关系对齐，再通过加权集成获得可解释性置信度评估。

Result: MedConf在两个LLMs和三类医学数据集上，相较当前最优方法，在AUROC和皮尔逊相关系数等指标上表现更优，并且在信息不足和多病共存条件下依然保持稳定性能。

Conclusion: 信息充分性是医学置信度建模的核心因素，MedConf的提出为构建更可靠、可解释的大型医学模型开辟了新路径。

Abstract: Large-scale language models (LLMs) often offer clinical judgments based on incomplete information, increasing the risk of misdiagnosis. Existing studies have primarily evaluated confidence in single-turn, static settings, overlooking the coupling between confidence and correctness as clinical evidence accumulates during real consultations, which limits their support for reliable decision-making. We propose the first benchmark for assessing confidence in multi-turn interaction during realistic medical consultations. Our benchmark unifies three types of medical data for open-ended diagnostic generation and introduces an information sufficiency gradient to characterize the confidence-correctness dynamics as evidence increases. We implement and compare 27 representative methods on this benchmark; two key insights emerge: (1) medical data amplifies the inherent limitations of token-level and consistency-level confidence methods, and (2) medical reasoning must be evaluated for both diagnostic accuracy and information completeness. Based on these insights, we present MedConf, an evidence-grounded linguistic self-assessment framework that constructs symptom profiles via retrieval-augmented generation, aligns patient information with supporting, missing, and contradictory relations, and aggregates them into an interpretable confidence estimate through weighted integration. Across two LLMs and three medical datasets, MedConf consistently outperforms state-of-the-art methods on both AUROC and Pearson correlation coefficient metrics, maintaining stable performance under conditions of information insufficiency and multimorbidity. These results demonstrate that information adequacy is a key determinant of credible medical confidence modeling, providing a new pathway toward building more reliable and interpretable large medical models.

</details>


### [94] [What Patients Really Ask: Exploring the Effect of False Assumptions in Patient Information Seeking](https://arxiv.org/abs/2601.15674)
*Raymond Xiong,Furong Jia,Lionel Wong,Monica Agrawal*

Main category: cs.CL

TL;DR: 本文提出了一种基于谷歌“People Also Ask”功能的新数据集，用于更贴近真实患者医疗咨询的问答评测，并发现主流大模型对识别患者日常问题中的错误假设有明显不足。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在医学问答方面的评测多基于医学考试题，但这些问题与患者在真实生活中提出的实际问题差异很大。因此，需要更真实、更具代表性的问题来评估和提升模型在临床实际中的表现。

Method: 作者通过谷歌“People Also Ask”功能，结合美国处方量排名前200的药物，收集并整理了患者常见的医疗问题数据集。分析了其中包含的错误假设和潜在危险意图，同时考察了这些问题出现的规律性和与问题历史错误程度的关联。

Result: 结果表明，收集到的问题中相当部分包含错误和危险的内容，这些错误分布并非完全随机，而是与问题历史有明显关联。此外，当前表现良好的大模型在识别此类日常医疗问题的错误假设方面存在较大挑战。

Conclusion: 现有大模型虽然在标准化医学问题上效果良好，但在面对真实世界患者问题时，对潜在错误假设的识别能力有限。为提升医疗场景下大模型安全性和有效性，需针对真人日常咨询场景进行更有针对性的评测与优化。

Abstract: Patients are increasingly using large language models (LLMs) to seek answers to their healthcare-related questions. However, benchmarking efforts in LLMs for question answering often focus on medical exam questions, which differ significantly in style and content from the questions patients actually raise in real life. To bridge this gap, we sourced data from Google's People Also Ask feature by querying the top 200 prescribed medications in the United States, curating a dataset of medical questions people commonly ask. A considerable portion of the collected questions contains incorrect assumptions and dangerous intentions. We demonstrate that the emergence of these corrupted questions is not uniformly random and depends heavily on the degree of incorrectness in the history of questions that led to their appearance. Current LLMs that perform strongly on other benchmarks struggle to identify incorrect assumptions in everyday questions.

</details>


### [95] [Persona Switch: Mixing Distinct Perspectives in Decoding Time](https://arxiv.org/abs/2601.15708)
*Junseok Kim,Nakyeong Yang,Kyomin Jung*

Main category: cs.CL

TL;DR: 本文提出了一种新方法Persona Switch，通过在解码过程中动态选择zero-shot提示和角色扮演提示的输出，提升了语言模型推理准确性。实验证明该方法优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 虽然角色扮演提示可以提升语言模型的推理能力，但其在不同任务和实例中表现不一致。因此，如何结合zero-shot与角色扮演提示的优点成为关键问题。

Method: Persona Switch方法在生成每一步时，分别用zero-shot和角色扮演两种提示生成输出，并通过logit gap衡量输出置信度，动态选择置信度更高的结果，逐步构建最终答案。

Result: 实验表明，Persona Switch在广泛使用的大语言模型上，准确率相较于现有竞争性基线最高提升了5.13%。同时，输出置信度被证明是选择可靠输出的有效指标。

Conclusion: Persona Switch能结合两种提示的优点，稳定提升模型表现，为提升大语言模型推理能力提供了新方向。

Abstract: Role-play prompting is known to steer the behavior of language models by injecting a persona into the prompt, improving their zero-shot reasoning capabilities. However, such improvements are inconsistent across different tasks or instances. This inconsistency suggests that zero-shot and role-play prompting may offer complementary strengths rather than one being universally superior. Building on this insight, we propose Persona Switch, a novel decoding method that dynamically combines the benefits of both prompting strategies. Our method proceeds step-by-step, selecting the better output between zero-shot and role-play prompting at each step by comparing their output confidence, as measured by the logit gap. Experiments with widely-used LLMs demonstrate that Persona Switch consistently outperforms competitive baselines, achieving up to 5.13% accuracy improvement. Furthermore, we show that output confidence serves as an informative measure for selecting the more reliable output.

</details>


### [96] [Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind](https://arxiv.org/abs/2601.15715)
*Zhitao He,Zongwei Lyu,Yi R Fung*

Main category: cs.CL

TL;DR: 本文提出了RebuttalAgent，这是首个将‘心智理论’（ToM）应用于学术回复生成的AI系统，并构建了大规模数据集与评价体系。实验表明其在自动和人工评测上均优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前学术答辩回复领域中，AI方法多模仿表层语言，缺乏对说服力和视角采纳的深入理解。学术回复涉及复杂的信息不对称下的战略沟通，需要模型具备理解对方心理状态的能力（即‘心智理论’），而非单纯技术性辩论。

Method: 作者提出了ToM-Strategy-Response(TSR)流程：首先建模审稿人心理状态；然后制定说服策略；最后生成基于策略的回复。训练上，先通过监督微调获得ToM和战略规划能力，再用结合自奖励机制的强化学习自我改进。为训练和评测，构建了基于批判-改进产生的大规模RebuttalBench数据集，以及比GPT-4.1更一致判分的自动评测器Rebuttal-RM。

Result: RebuttalAgent在自动化指标上较基础模型平均提升18.3%，且在自动评测和人工评测中均优于现有的先进专有模型。自动评测器在判断一致性上超越GPT-4.1。

Conclusion: 基于心智理论的战略性学术回复模型显著提升了说服力和效果，为AI助力学术沟通提供了新范式。但生成内容仍建议为启发和辅助用途，不可替代作者独立思考。

Abstract: Although artificial intelligence (AI) has become deeply integrated into various stages of the research workflow and achieved remarkable advancements, academic rebuttal remains a significant and underexplored challenge. This is because rebuttal is a complex process of strategic communication under severe information asymmetry rather than a simple technical debate. Consequently, current approaches struggle as they largely imitate surface-level linguistics, missing the essential element of perspective-taking required for effective persuasion. In this paper, we introduce RebuttalAgent, the first framework to ground academic rebuttal in Theory of Mind (ToM), operationalized through a ToM-Strategy-Response (TSR) pipeline that models reviewer mental state, formulates persuasion strategy, and generates strategy-grounded response. To train our agent, we construct RebuttalBench, a large-scale dataset synthesized via a novel critique-and-refine approach. Our training process consists of two stages, beginning with a supervised fine-tuning phase to equip the agent with ToM-based analysis and strategic planning capabilities, followed by a reinforcement learning phase leveraging the self-reward mechanism for scalable self-improvement. For reliable and efficient automated evaluation, we further develop Rebuttal-RM, a specialized evaluator trained on over 100K samples of multi-source rebuttal data, which achieves scoring consistency with human preferences surpassing powerful judge GPT-4.1. Extensive experiments show RebuttalAgent significantly outperforms the base model by an average of 18.3% on automated metrics, while also outperforming advanced proprietary models across both automated and human evaluations. Disclaimer: the generated rebuttal content is for reference only to inspire authors and assist in drafting. It is not intended to replace the author's own critical analysis and response.

</details>


### [97] [Hallucination Mitigating for Medical Report Generation](https://arxiv.org/abs/2601.15745)
*Ruoqing Zhao,Runze Xia,Piji Li*

Main category: cs.CL

TL;DR: 该论文提出了一种用于医学报告生成的新框架KERM，通过知识检索、语境净化和细粒度奖励，有效减少了大模型生成幻觉内容，并提升报告质量。


<details>
  <summary>Details</summary>
Motivation: 医学报告生成过程中，大模型可能生成看似合理但实际不准确的信息（幻觉），这在医学领域尤其危险。因而亟需减少幻觉现象，确保报告真实可靠。

Method: 方法包括：1）利用MedCLIP从专业知识库中检索与检查结果相关的病灶事实句子，2）通过新颖的净化模块确保检索知识与患者临床背景高度相关，3）引入细粒度奖励，引导模型生成支持性强、与临床高度相关的描述。

Result: 在IU-Xray和MIMIC-CXR等公开医学影像报告数据集上的实验结果表明，KERM框架显著减少了大模型的幻觉现象，并提升了生成报告的质量。

Conclusion: 通过结合知识检索、净化和细粒度监督，KERM能有效提升医学报告生成的准确性和可靠性，为大模型在医学实际应用中安全落地贡献重要方法。

Abstract: In the realm of medical report generation (MRG), the integration of natural language processing has emerged as a vital tool to alleviate the workload of radiologists. Despite the impressive capabilities demonstrated by large vision language models (LVLMs) in understanding natural language, their susceptibility to generating plausible yet inaccurate claims, known as ``hallucinations'', raises concerns-especially in the nuanced and critical field of medical. In this work, we introduce a framework, \textbf{K}nowledge-\textbf{E}nhanced with Fine-Grained \textbf{R}einforced Rewards \textbf{M}edical Report Generation (KERM), to tackle the issue. Our approach refines the input to the LVLM by first utilizing MedCLIP for knowledge retrieval, incorporating relevant lesion fact sentences from a curated knowledge corpus. We then introduce a novel purification module to ensure the retrieved knowledge is contextually relevant to the patient's clinical context. Subsequently, we employ fine-grained rewards to guide these models in generating highly supportive and clinically relevant descriptions, ensuring the alignment of model's outputs with desired behaviors. Experimental results on IU-Xray and MIMIC-CXR datasets validate the effectiveness of our approach in mitigating hallucinations and enhancing report quality.

</details>


### [98] [Beyond Marginal Distributions: A Framework to Evaluate the Representativeness of Demographic-Aligned LLMs](https://arxiv.org/abs/2601.15755)
*Tristan Williams,Franziska Weeber,Sebastian Padó,Alan Akbik*

Main category: cs.CL

TL;DR: 本文提出了一种评估对齐大型语言模型在人类价值观和意见上代表性的框架，强调单独考虑边际分布可能掩盖结构性失真。作者通过比较两种常用模型引导技术，发现当前模型在捕捉多维相关结构上仍存在不足。


<details>
  <summary>Details</summary>
Motivation: 目前针对大语言模型的价值观对齐主要依赖于对单一问卷项的独立分布（边际分布）进行比对，忽略了人类价值观中的多维相关结构。这可能导致模型在表面上对齐但实际未能把握人类文化价值的深层结构。

Method: 作者提出了一种新的评估框架，除了比较模型和真实人口在边际分布上的相似性外，进一步考察它们在多变量相关结构上的一致性。具体评估了两种模型引导技术——角色设定提示和按人口统计微调，并将其输出与世界价值观调查的真实数据进行对比。

Result: 结果显示：按人口统计微调的模型比角色设定提示更好地匹配了人类的边际分布，但两种技术都不能完整捕捉真实人群中的相关结构模式。说明目前的对齐方法在深层代表性上存在不足。

Conclusion: 代表性是价值观对齐的一个独立维度，仅关注边际分布的评估方法可能掩盖模型结构性失效问题，从而对模型能力产生过于乐观的结论。

Abstract: Large language models are increasingly used to represent human opinions, values, or beliefs, and their steerability towards these ideals is an active area of research. Existing work focuses predominantly on aligning marginal response distributions, treating each survey item independently. While essential, this may overlook deeper latent structures that characterise real populations and underpin cultural values theories. We propose a framework for evaluating the representativeness of aligned models through multivariate correlation patterns in addition to marginal distributions. We show the value of our evaluation scheme by comparing two model steering techniques (persona prompting and demographic fine-tuning) and evaluating them against human responses from the World Values Survey. While the demographically fine-tuned model better approximates marginal response distributions than persona prompting, both techniques fail to fully capture the gold standard correlation patterns. We conclude that representativeness is a distinct aspect of value alignment and an evaluation focused on marginals can mask structural failures, leading to overly optimistic conclusions about model capabilities.

</details>


### [99] [HumanLLM: Towards Personalized Understanding and Simulation of Human Nature](https://arxiv.org/abs/2601.15793)
*Yuxuan Lei,Tianfu Wang,Jianxun Lian,Zhengyu Hu,Defu Lian,Xing Xie*

Main category: cs.CL

TL;DR: 论文提出了HumanLLM，一种旨在更好模拟和理解个体认知与行为的基础大模型，利用大规模真实用户数据提升对个性化社会行为与心理活动的刻画能力，结果在多项任务和基准测试中优于传统LLM。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在客观任务上表现突出，但它们在模拟人类复杂行为和思维方面能力有限，阻碍了模型在社科和以人为本业务中的应用。作者认为关键障碍在于现有LLM预训练语料缺乏对个体持续、情境化决策与行为轨迹的刻画。

Method: 作者提出了HumanLLM，首先构建了Cognitive Genome Dataset，从Reddit、Twitter、Blogger和Amazon等平台自动处理筛选出超550万条用户日志，提取个体档案、行为特征和思维模式；随后设计多元化学习任务，通过监督微调训练模型，以提升其对个体化行为、思维及体验的预测能力。

Result: 实验结果显示，HumanLLM在预测用户行为和内心想法、模拟文本风格与偏好、生成更真实用户档案等方面均超越了基础模型，并且在跨领域社会智能基准测试上取得显著提升。

Conclusion: HumanLLM能够更精准地模拟和预测个体化的认知与社交行为，展现出更强的泛化能力，为社会科学研究和用户洞察等人本任务提供了有力工具。

Abstract: Motivated by the remarkable progress of large language models (LLMs) in objective tasks like mathematics and coding, there is growing interest in their potential to simulate human behavior--a capability with profound implications for transforming social science research and customer-centric business insights. However, LLMs often lack a nuanced understanding of human cognition and behavior, limiting their effectiveness in social simulation and personalized applications. We posit that this limitation stems from a fundamental misalignment: standard LLM pretraining on vast, uncontextualized web data does not capture the continuous, situated context of an individual's decisions, thoughts, and behaviors over time. To bridge this gap, we introduce HumanLLM, a foundation model designed for personalized understanding and simulation of individuals. We first construct the Cognitive Genome Dataset, a large-scale corpus curated from real-world user data on platforms like Reddit, Twitter, Blogger, and Amazon. Through a rigorous, multi-stage pipeline involving data filtering, synthesis, and quality control, we automatically extract over 5.5 million user logs to distill rich profiles, behaviors, and thinking patterns. We then formulate diverse learning tasks and perform supervised fine-tuning to empower the model to predict a wide range of individualized human behaviors, thoughts, and experiences. Comprehensive evaluations demonstrate that HumanLLM achieves superior performance in predicting user actions and inner thoughts, more accurately mimics user writing styles and preferences, and generates more authentic user profiles compared to base models. Furthermore, HumanLLM shows significant gains on out-of-domain social intelligence benchmarks, indicating enhanced generalization.

</details>


### [100] [SteerEval: Inference-time Interventions Strengthen Multilingual Generalization in Neural Summarization Metrics](https://arxiv.org/abs/2601.15809)
*Silvia Casola,Ryan Soh-Eun Shim,Felicia Körner,Yuchen Mao,Barbara Plank*

Main category: cs.CL

TL;DR: 本文研究多语种自然语言生成（如摘要）中的评价指标问题，提出通过引导多语种模型向英语枢纽语言对齐，提高评价指标与人工评价的一致性。实验表明该方法对多语种均有效。


<details>
  <summary>Details</summary>
Motivation: 当前多语种自然语言生成任务中，准确、稳健的自动评价指标稀缺，阻碍了该领域进展。已有研究发现多语种语言模型内部多以英语为‘枢纽’语言，指标与该枢纽不匹配时性能下降。作者希望通过对齐到英语，提高评价一致性。

Method: 作者假设多语种神经指标与模型内部英文枢纽语言对齐可以提升其与人工评价的一致性。针对编码器和解码器类指标，采用测试时干预法，使模型在评价其他语言时向英语激活靠拢，并验证该方法效果。

Result: 实验发现，无论指标基于Encoder还是Decoder，英语对齐的测试时干预法均能提高多语种指标与人工评价的一致性，提升效果在多种语言上均明显。

Conclusion: 将多语种神经评价指标的激活向英语枢纽对齐，是提升多语言NLG任务自动评价一致性和有效性的简便、有效途径。该方法未来可推广到更多多语种NLG评估场景。

Abstract: An increasing body of work has leveraged multilingual language models for Natural Language Generation tasks such as summarization. A major empirical bottleneck in this area is the shortage of accurate and robust evaluation metrics for many languages, which hinders progress. Recent studies suggest that multilingual language models often use English as an internal pivot language, and that misalignment with this pivot can lead to degraded downstream performance. Motivated by the hypothesis that this mismatch could also apply to multilingual neural metrics, we ask whether steering their activations toward an English pivot can improve correlation with human judgments. We experiment with encoder- and decoder-based metrics and find that test-time intervention methods are effective across the board, increasing metric effectiveness for diverse languages.

</details>


### [101] [ExDR: Explanation-driven Dynamic Retrieval Enhancement for Multimodal Fake News Detection](https://arxiv.org/abs/2601.15820)
*Guoxuan Ding,Yuqing Li,Ziyan Zhou,Zheng Lin,Daren Zha,Jiangnan Li*

Main category: cs.CL

TL;DR: 本文提出了一种用于多模态假新闻检测的解释驱动型动态检索增强生成框架（ExDR），通过模型生成的解释提升检索触发与证据获取的有效性，实现了在两个数据集上性能超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态假新闻检测方法难以应对假新闻不断演变、事实时效性强等问题，且传统检索增强生成方法存在冗余检索、相似性粗糙与证据无关等局限。为了解决这些问题，作者提出结合解释信息来提升检索质量和检测效果。

Method: 提出ExDR框架：通过模型生成的解释信息引导检索触发和证据检索，在触发阶段融合置信度多维评估，对证据检索部分结合欺骗实体的信息构建实体感知索引，并利用具有欺骗特征的对比性证据提升判断准确率。

Result: 在AMG与MR2两个基准数据集上，ExDR在检索触发准确率、检索质量和整体检测效果方面均优于现有方法，显示了良好的效果和泛化能力。

Conclusion: 通过解释驱动的动态检索增强方法，可以有效提升多模态假新闻检测的性能，为实时、准确识别虚假信息提供了有力工具。

Abstract: The rapid spread of multimodal fake news poses a serious societal threat, as its evolving nature and reliance on timely factual details challenge existing detection methods. Dynamic Retrieval-Augmented Generation provides a promising solution by triggering keyword-based retrieval and incorporating external knowledge, thus enabling both efficient and accurate evidence selection. However, it still faces challenges in addressing issues such as redundant retrieval, coarse similarity, and irrelevant evidence when applied to deceptive content. In this paper, we propose ExDR, an Explanation-driven Dynamic Retrieval-Augmented Generation framework for Multimodal Fake News Detection. Our framework systematically leverages model-generated explanations in both the retrieval triggering and evidence retrieval modules. It assesses triggering confidence from three complementary dimensions, constructs entity-aware indices by fusing deceptive entities, and retrieves contrastive evidence based on deception-specific features to challenge the initial claim and enhance the final prediction. Experiments on two benchmark datasets, AMG and MR2, demonstrate that ExDR consistently outperforms previous methods in retrieval triggering accuracy, retrieval quality, and overall detection performance, highlighting its effectiveness and generalization capability.

</details>


### [102] [Can professional translators identify machine-generated text?](https://arxiv.org/abs/2601.15828)
*Michael Farrell*

Main category: cs.CL

TL;DR: 本研究探讨专业译者在无专门训练情况下，能否辨识由AI（ChatGPT-4o）生成与真实作者撰写的意大利语短篇小说。部分译者能够准确区分，但同样数量的人相反地误判。研究发现低突发性和叙事矛盾为AI文本主要特征，而语法准确和情感色彩易导致误判。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成文本能力提升，区分AI与人类作者作品成为翻译与编辑领域的重要议题。本研究旨在考察专业译者在现实任务下，识别AI文本的能力和依据。

Method: 招募69名专业译者，现场评估三篇匿名短篇小说（2篇由ChatGPT-4o生成，1篇为人类所写）。译者需判断每篇作品的作者身份，并说明理由。统计和分析译者的判断和依据。

Result: 整体判断准确率无显著高于偶然，但16.2%的译者能显著准确区分AI与人类文本，说明他们具备分析能力。等量的译者则方向性误判。低burstiness和叙事矛盾最能反映AI文本特征，英文影响的借词及句法转移也常被发现。

Conclusion: 专业译者部分能有效识别AI文本，但总体而言差异并不明显，识别过程受主观偏好影响，常见的语言准确性和情感色彩反而导致误判。研究结果对编辑行业中AI文本处理提出挑战。

Abstract: This study investigates whether professional translators can reliably identify short stories generated in Italian by artificial intelligence (AI) without prior specialized training. Sixty-nine translators took part in an in-person experiment, where they assessed three anonymized short stories - two written by ChatGPT-4o and one by a human author. For each story, participants rated the likelihood of AI authorship and provided justifications for their choices. While average results were inconclusive, a statistically significant subset (16.2%) successfully distinguished the synthetic texts from the human text, suggesting that their judgements were informed by analytical skill rather than chance. However, a nearly equal number misclassified the texts in the opposite direction, often relying on subjective impressions rather than objective markers, possibly reflecting a reader preference for AI-generated texts. Low burstiness and narrative contradiction emerged as the most reliable indicators of synthetic authorship, with unexpected calques, semantic loans and syntactic transfer from English also reported. In contrast, features such as grammatical accuracy and emotional tone frequently led to misclassification. These findings raise questions about the role and scope of synthetic-text editing in professional contexts.

</details>


### [103] [Determinants of Training Corpus Size for Clinical Text Classification](https://arxiv.org/abs/2601.15846)
*Jaya Chaturvedi,Saniya Deshpande,Chenkai Ma,Robert Cobb,Angus Roberts,Robert Stewart,Daniel Stahl,Diana Shamsutdinova*

Main category: cs.CL

TL;DR: 本研究探讨了临床文本分类中训练数据量与模型性能的关系，并分析了词汇特性对学习曲线的影响。通过在MIMIC-III数据集上对不同规模数据训练模型，发现600份文档足以达到几乎最优效果，并揭示了强预测词和噪音词数量与模型表现密切相关。


<details>
  <summary>Details</summary>
Motivation: 临床文本分类任务常因标注数据匮乏而性能受限，业界通常标注200-500份文档，缺乏对样本量需求和词汇特性与学习效果关系的系统分析。

Method: 利用MIMIC-III公开数据集，采用预训练BERT嵌入与随机森林分类器，针对10个随机挑选的疾病标签，在训练集规模从100到10,000份文档变化下，分析模型性能。通过Lasso逻辑回归分析bag-of-words表示中的重要预测词及噪声词，并研究其与学习曲线变化的关系。

Result: 10个分类任务的学习曲线差异显著，600份文档已足以实现10,000份文档下95%的性能。词汇分析显示，强预测词多且噪音词少的任务学习更快；每增加100个噪声词准确率下降约0.02，每增加100个强预测词最高准确率提升约0.04。

Conclusion: 临床文本分类中，适度增加训练数据后模型可迅速逼近最优水平，任务词汇特性（强预测词/噪声词）对学习效率影响显著。样本量和词汇指标可作为数据标注和模型开发的参考依据。

Abstract: Introduction: Clinical text classification using natural language processing (NLP) models requires adequate training data to achieve optimal performance. For that, 200-500 documents are typically annotated. The number is constrained by time and costs and lacks justification of the sample size requirements and their relationship to text vocabulary properties.
  Methods: Using the publicly available MIMIC-III dataset containing hospital discharge notes with ICD-9 diagnoses as labels, we employed pre-trained BERT embeddings followed by Random Forest classifiers to identify 10 randomly selected diagnoses, varying training corpus sizes from 100 to 10,000 documents, and analyzed vocabulary properties by identifying strong and noisy predictive words through Lasso logistic regression on bag-of-words embeddings.
  Results: Learning curves varied significantly across the 10 classification tasks despite identical preprocessing and algorithms, with 600 documents sufficient to achieve 95% of the performance attainable with 10,000 documents for all tasks. Vocabulary analysis revealed that more strong predictors and fewer noisy predictors were associated with steeper learning curves, where every 100 additional noisy words decreased accuracy by approximately 0.02 while 100 additional strong predictors increased maximum accuracy by approximately 0.04.

</details>


### [104] [Artificial Rigidities vs. Biological Noise: A Comparative Analysis of Multisensory Integration in AV-HuBERT and Human Observers](https://arxiv.org/abs/2601.15869)
*Francisco Portillo López*

Main category: cs.CL

TL;DR: 本研究将AV-HuBERT模型与人类受试者在McGurk效应任务中的表现进行了对比，以评估该模型在感知生物拟真度方面的表现。


<details>
  <summary>Details</summary>
Motivation: 探究自监督学习的音视频自编码器是否能在多感官语音感知上与人类表现一致，特别是在对不一致视听刺激（McGurk效应）反应方面。

Method: 对44名人类受试者和AV-HuBERT模型，分别呈现McGurk效应刺激，分析两者对视听不一致时的反应，并对比听觉主导率、音位融合率以及反应的随机性。

Result: AV-HuBERT与人类在听觉主导率上几乎一致（32.0% vs 31.8%），但模型表现出更高的音位融合倾向（68.0% vs 47.7%）。人类反应多样富有随机性，而模型为确定性、严格分类。

Conclusion: 当前自监督模型虽能模拟多感官结果，但缺失人类感知中固有的神经变异性，难以还原人类语音感知时的多样性和随机性。

Abstract: This study evaluates AV-HuBERT's perceptual bio-fidelity by benchmarking its response to incongruent audiovisual stimuli (McGurk effect) against human observers (N=44). Results reveal a striking quantitative isomorphism: AI and humans exhibited nearly identical auditory dominance rates (32.0% vs. 31.8%), suggesting the model captures biological thresholds for auditory resistance. However, AV-HuBERT showed a deterministic bias toward phonetic fusion (68.0%), significantly exceeding human rates (47.7%). While humans displayed perceptual stochasticity and diverse error profiles, the model remained strictly categorical. Findings suggest that current self-supervised architectures mimic multisensory outcomes but lack the neural variability inherent to human speech perception.

</details>


### [105] [Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model](https://arxiv.org/abs/2601.15892)
*Chenghao Fan,Wen Heng,Bo Li,Sichen Liu,Yuxuan Song,Jing Su,Xiaoye Qu,Kai Shen,Wei Wei*

Main category: cs.CL

TL;DR: Stable-DiffCoder是一种基于扩散模型的代码生成方法，通过块级扩散预训练，性能优于同等配置下的自回归代码生成模型，尤其在代码编辑、推理和低资源语言上表现更佳。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的代码生成（DLLMs）在与自回归（AR）模型的对比中性能不足，而AR模型已非常强大。为了解决DLLMs性能落后于AR模型的问题，并探索扩散训练在代码建模领域的潜力，作者进行该项研究。

Method: 作者采用与强AR基线相同的Seed-Coder架构、数据和训练流程，提出Stable-DiffCoder。它引入了块级扩散持续预训练（CPT），并结合了特别设计的预热阶段和块级裁剪噪声调度，以提升学习效率和训练稳定性。

Result: 在相同数据和架构条件下，Stable-DiffCoder在多个代码基准上整体优于对应AR模型，仅借助CPT和有监督微调便超越了多种8B参数级别的AR与DLLM模型，并在结构化代码建模、代码编辑、推理、数据增强及低资源编码语言等任务中展现额外优势。

Conclusion: 本研究表明扩散式训练能够提升代码建模性能，超过常规自回归训练。扩散式任意顺序建模还显著增强了代码编辑与推理能力，并通过数据增强惠及低资源代码语言，实现了更好的泛化与实际应用价值。

Abstract: Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.

</details>


### [106] [Transfer Learning from ImageNet for MEG-Based Decoding of Imagined Speech](https://arxiv.org/abs/2601.15909)
*Soufiane Jhilal,Stéphanie Martin,Anne-Lise Giraud*

Main category: cs.CL

TL;DR: 本论文提出将MEG信号转为适用于预训练视觉模型的图像表示，并取得了更优异的无创想象言语识别效果。


<details>
  <summary>Details</summary>
Motivation: 想象言语的无创破译因信号弱、分布广和标注数据有限而难以实现。作者希望提升识别准确性与泛化能力。

Method: 将来自21位参与者进行想象言语任务的MEG数据，通过可学习的传感器卷积投影为三通道的空间小波图像，作为预训练ImageNet视觉模型的输入，并与传统和自训模型进行性能对比。

Result: 预训练视觉模型大幅优于其他模型，想象言语对照静默准确率达90.4%，对照无声阅读81.0%，元音辨识60.6%。跨被试评估显示预训练模型能提取大脑信号中的共享表征，且判别信息聚焦于想象活动时段。

Conclusion: 将MEG信号转为图像并应用预训练视觉模型，能有效捕获无创条件下想象言语的脑活动结构，显著提升破译性能。

Abstract: Non-invasive decoding of imagined speech remains challenging due to weak, distributed signals and limited labeled data. Our paper introduces an image-based approach that transforms magnetoencephalography (MEG) signals into time-frequency representations compatible with pretrained vision models. MEG data from 21 participants performing imagined speech tasks were projected into three spatial scalogram mixtures via a learnable sensor-space convolution, producing compact image-like inputs for ImageNet-pretrained vision architectures. These models outperformed classical and non-pretrained models, achieving up to 90.4% balanced accuracy for imagery vs. silence, 81.0% vs. silent reading, and 60.6% for vowel decoding. Cross-subject evaluation confirmed that pretrained models capture shared neural representations, and temporal analyses localized discriminative information to imagery-locked intervals. These findings show that pretrained vision models applied to image-based MEG representations can effectively capture the structure of imagined speech in non-invasive neural signals.

</details>


### [107] [Mecellem Models: Turkish Models Trained from Scratch and Continually Pre-trained for the Legal Domain](https://arxiv.org/abs/2601.16018)
*Özgür Uğur,Mahmut Göksu,Mahmut Çimen,Musa Yılmaz,Esra Şavirdi,Alp Talha Demir,Rumeysa Güllüce,İclal Çetin,Ömer Can Sağbaş*

Main category: cs.CL

TL;DR: 该论文提出Mecellem模型，通过领域适应策略，开发适用于土耳其法律领域的专用语言模型，主要包括自研预训练编码器和持续预训练的解码器。


<details>
  <summary>Details</summary>
Motivation: 现有法律领域语言模型在土耳其语环境下适应性有限，且通常依赖多阶段且高成本的训练流程，缺乏针对土耳其法律领域的高效专用模型。

Method: 1. 编码器：基于ModernBERT，使用1127亿个词的土耳其语语料从零预训练；采用训练中断点评估策略，优化检索性能。2. 解码器：对Qwen3-1.7B和Qwen3-4B通过四阶段课程学习持续预训练，使模型逐步从通用知识过渡到法律领域，并适应长文本推理。

Result: 自研编码器在土耳其检索排行榜获得前三名，155M参数的小模型性能与307M-567M的大模型相当，生产效率为92.36%，资源消耗远低于现有SOTA模型。解码器在土耳其法律文本上困惑度下降36.2%。

Conclusion: 提出的Mecellem模型在计算效率和适应性上优于多数现有方案，能以更低成本实现法律领域的高性能土耳其语语言模型。

Abstract: This paper presents Mecellem models, a framework for developing specialized language models for the Turkish legal domain through domain adaptation strategies. We make two contributions: (1)Encoder Model Pre-trained from Scratch: ModernBERT-based bidirectional encoders pre-trained on a Turkish-dominant corpus of 112.7 billion tokens. We implement a checkpoint selection strategy that evaluates downstream retrieval performance throughout training, revealing that optimal checkpoints achieve best retrieval scores before pre-training loss reaches its minimum. Our encoder models achieve top-3 rankings on the Turkish retrieval leaderboard, with smaller models (155M parameters) achieving comparable performance to larger reference models (307M-567M parameters). Our approach achieves 92.36% production efficiency compared to state-of-the-art models (embeddinggemma-300m: 100.00%, BAAI/bge-m3: 99.54%, newmindai/bge-m3-stsb: 94.38%), ranking fourth overall despite requiring less computational resources. SOTA models rely on multi-stage, computationally intensive training pipelines, making our single-stage pre-training followed by efficient post-training approach a cost-effective alternative; (2)Decoder Model with Continual Pre-training (CPT): Qwen3-1.7B and Qwen3-4B models adapted to Turkish legal domain through controlled curriculum learning. Four-phase CPT with optimal sample ratios enables gradual transition from general language knowledge to specialized legal terminology and long-context reasoning. This approach achieves 36.2% perplexity reduction on Turkish legal text, demonstrating domain adaptation gains.

</details>


### [108] [Universal Refusal Circuits Across LLMs: Cross-Model Transfer via Trajectory Replay and Concept-Basis Reconstruction](https://arxiv.org/abs/2601.16034)
*Tony Cristofano*

Main category: cs.CL

TL;DR: 本文提出了一种“通过概念基重构进行轨迹重放”的新方法，可以在不同大模型之间转移拒绝行为干预，而无需目标模型上的额外监督，实现了安全对齐干预器的通用化移植。


<details>
  <summary>Details</summary>
Motivation: 当前大模型中的拒绝行为普遍被认为是模型特有的，但作者怀疑其本质上来源于不同模型间共享的、低维的语义回路，探索这种共性的可能性。

Method: 提出“基于概念指纹对齐和概念基重构的轨迹重放”框架：先对齐不同模型的中间层，通过对拒绝方向的“概念原子”共享公式重构，实现“捐赠模型”到“目标模型”间的干预迁移。加入了权重SVD稳定约束，将干预投影在低方差子空间上以防功能损伤。

Result: 在8组不同模型对（包括GPT-OSS-20B和GLM-4）上的实验表明，迁移后的干预方案在有效减弱目标模型拒绝倾向的同时，基本维持了模型性能。

Conclusion: 这项研究为安全对齐的语义通用性提供了有力证据，表明大模型间拒绝行为的本质存在共性，也提出了无需额外监督即可在不同路线模型间转移安全干预的通用方法。

Abstract: Refusal behavior in aligned LLMs is often viewed as model-specific, yet we hypothesize it stems from a universal, low-dimensional semantic circuit shared across models. To test this, we introduce Trajectory Replay via Concept-Basis Reconstruction, a framework that transfers refusal interventions from donor to target models, spanning diverse architectures (e.g., Dense to MoE) and training regimes, without using target-side refusal supervision. By aligning layers via concept fingerprints and reconstructing refusal directions using a shared ``recipe'' of concept atoms, we map the donor's ablation trajectory into the target's semantic space. To preserve capabilities, we introduce a weight-SVD stability guard that projects interventions away from high-variance weight subspaces to prevent collateral damage. Our evaluation across 8 model pairs (including GPT-OSS-20B and GLM-4) confirms that these transferred recipes consistently attenuate refusal while maintaining performance, providing strong evidence for the semantic universality of safety alignment.

</details>


### [109] [Adapter Fusion for Multilingual Text2Cypher with Linear and Learned Gating](https://arxiv.org/abs/2601.16097)
*Makbule Gulcin Ozsoy*

Main category: cs.CL

TL;DR: 该论文提出了一种可扩展的多语种Text2Cypher方法，通过训练针对不同语言的LoRA适配器并使用MLP融合，在保持高性能的同时，实现了对新语言的高效扩展。


<details>
  <summary>Details</summary>
Motivation: 现有的Text2Cypher等自然语言到结构化查询的系统主要支持英文，缺少多语种支持，且多语种联合微调成本高、扩展性差。作者希望实现一种无需完全再训练即可支持新语言的方法。

Method: 作者为英语、西班牙语和土耳其语分别训练了LoRA语言适配器，并通过线性合并与MLP融合（带动态门控）对适配器进行组合。

Result: 实验表明，MLP融合方法能够在仅用较少数据的情况下，恢复约75%的多语种联合微调精度提升，且在三种语言中都优于线性合并。

Conclusion: 该方法实现了多语种Text2Cypher系统的高效可扩展性，新加语言只需训练一个适配器并轻量级MLP再训练，为多语种结构化查询任务提供了实用、高效的解决方案。

Abstract: Large Language Models enable users to access database using natural language interfaces using tools like Text2SQL, Text2SPARQL, and Text2Cypher, which translate user questions into structured database queries. While these systems improve database accessibility, most research focuses on English with limited multilingual support. This work investigates a scalable multilingual Text2Cypher, aiming to support new languages without re-running full fine-tuning, avoiding manual hyper-parameter tuning, and maintaining performance close to joint multilingual fine-tuning. We train language-specific LoRA adapters for English, Spanish, and Turkish and combined them via uniform linear merging or learned fusion MLP with dynamic gating. Experimental results show that the fusion MLP recovers around 75\% of the accuracy gains from joint multilingual fine-tuning while requiring only a smaller subset of the data, outperforming linear merging across all three languages. This approach enables incremental language expansion to new languages by requiring only one LoRA adapter and a lightweight MLP retraining. Learned adapter fusion offers a practical alternative to expensive joint fine-tuning, balancing performance, data efficiency, and scalability for multilingual Text2Cypher task.

</details>


### [110] [synthocr-gen: A synthetic ocr dataset generator for low-resource languages- breaking the data barrier](https://arxiv.org/abs/2601.16113)
*Haq Nawaz Malik,Kh Mohmad Shafi,Tanveer Ahmad Reshi*

Main category: cs.CL

TL;DR: SynthOCR-Gen 是一个开源合成OCR数据集生成器，专门为低资源语言（如克什米尔语）设计，通过将Unicode文本转化为高质量OCR训练数据，有效缓解了数据集匮乏难题。


<details>
  <summary>Details</summary>
Motivation: 低资源语言由于缺乏标注数据集，OCR发展面临巨大挑战，人工数据集构建成本高且容易出错，需要自动化工具解决数据瓶颈，推动边缘语言的信息化。

Method: SynthOCR-Gen实现了数字文本的文本分割、Unicode规范化、多字体渲染及25种以上数据增强（模拟真实文档退化），生成适合OCR训练的合成图片和标签。

Result: 作者利用SynthOCR-Gen为克什米尔语生成了60万条词级OCR训练数据，并公开发布，为相关语言的OCR研究和开发提供数据基础。

Conclusion: SynthOCR-Gen为低资源语言快速构建OCR数据集提供通用可扩展方案，有助于推动视觉-语言AI模型在全球多语言文本的应用与研究，工具已公开开放给科研和产业界。

Abstract: Optical Character Recognition (OCR) for low-resource languages remains a significant challenge due to the scarcity of large-scale annotated training datasets. Languages such as Kashmiri, with approximately 7 million speakers and a complex Perso-Arabic script featuring unique diacritical marks, currently lack support in major OCR systems including Tesseract, TrOCR, and PaddleOCR. Manual dataset creation for such languages is prohibitively expensive, time-consuming, and error-prone, often requiring word by word transcription of printed or handwritten text.
  We present SynthOCR-Gen, an open-source synthetic OCR dataset generator specifically designed for low-resource languages. Our tool addresses the fundamental bottleneck in OCR development by transforming digital Unicode text corpora into ready-to-use training datasets. The system implements a comprehensive pipeline encompassing text segmentation (character, word, n-gram, sentence, and line levels), Unicode normalization with script purity enforcement, multi-font rendering with configurable distribution, and 25+ data augmentation techniques simulating real-world document degradations including rotation, blur, noise, and scanner artifacts.
  We demonstrate the efficacy of our approach by generating a 600,000-sample word-segmented Kashmiri OCR dataset, which we release publicly on HuggingFace. This work provides a practical pathway for bringing low-resource languages into the era of vision-language AI models, and the tool is openly available for researchers and practitioners working with underserved writing systems worldwide.

</details>


### [111] [Improving Training Efficiency and Reducing Maintenance Costs via Language Specific Model Merging](https://arxiv.org/abs/2601.16127)
*Alphaeus Dmonte,Vidhi Gupta,Daniel J Perry,Mark Arehart*

Main category: cs.CL

TL;DR: 该论文分析了一种用于多语言大模型的合并策略，在保证模型质量的前提下，大幅提高了训练和维护效率。


<details>
  <summary>Details</summary>
Motivation: 现有多语言大模型在添加新语言或更新某一语言时，需要重新训练整个模型，这不仅计算资源消耗大，也增加了维护难度，因此亟需更高效的训练与维护方法。

Method: 提出并系统分析了一种多语言多任务模型的合并训练策略，通过单独更新某一语言后重新合并模型，替代全模型重训练。该方法在三个独立任务和不同数据集（包含工业数据集）上进行了评测。

Result: 与传统的全模型训练方式相比，该合并策略初始训练时间最多可减少50%，在维护阶段单一语言更新及再合并可降低60%以上的训练成本，同时保持了模型质量。

Conclusion: 多语言模型的合并策略不仅有效提升了训练与维护效率，而且保障了模型性能，对学术和工业领域都具有较强实用价值。

Abstract: Fine-tuning a task-specific multilingual large language model (LLM) involves training the model on a multilingual dataset with examples in all the required languages. Updating one or more supported languages with additional data or adding support for a new language involves retraining the model, which can be computationally inefficient and creates a severe maintenance bottleneck. Recent research on merging multilingual multitask models has shown promise in terms of improved quality, but its computational and maintenance efficiency remains unstudied. In this work, we provide the first focused analysis of this merging strategy from an efficiency perspective, evaluating it across three independent tasks. We demonstrate significant efficiency gains while maintaining parity in terms of quality: this merging approach reduces the initial training time by up to 50\%. We also demonstrate that updating an individual language and re-merging as part of model maintenance reduces training costs by more than 60\%, compared to re-training the full multilingual model. We show this on both public and proprietary industry datasets confirming that the approach works well for industrial use cases in addition to academic settings already studied in previous work.

</details>


### [112] [Automatic Classification of Arabic Literature into Historical Eras](https://arxiv.org/abs/2601.16138)
*Zainab Alhathloul,Irfan Ahmad*

Main category: cs.CL

TL;DR: 该论文利用神经网络和深度学习技术，自动将阿拉伯语文本按时代分期进行分类，并在不同数据集和分类任务下测试了模型效果。


<details>
  <summary>Details</summary>
Motivation: 虽然阿拉伯文学按照历史时期已被学者划分，但除诗歌外，关于自动化文本年代分类的研究较少，特别是涵盖多个时代和文本类型的系统性研究存在空白。本文旨在填补这一空白。

Method: 本文采用神经网络和深度学习方法，对阿拉伯语文本进行分时期自动分类。实验选用两个公开语料库，分别覆盖从前伊斯兰时期到现代的文本，实验设计涵盖二分类到15类不等的任务。模型既考虑历史传统划分，也研究自定义时期分法。

Result: 在二分类任务上，OpenITI和APCD两个数据集的F1分别为0.83和0.79；在分类粒度提升至15类和12类时，F1分别降至OpenITI上的0.20和APCD上的0.18。

Conclusion: 深度学习模型可有效用于阿拉伯语文本的分时期任务，粗粒度时期分类表现较好，但任务划分越细，分类难度和误差明显增加。

Abstract: The Arabic language has undergone notable transformations over time, including the emergence of new vocabulary, the obsolescence of others, and shifts in word usage. This evolution is evident in the distinction between the classical and modern Arabic eras. Although historians and linguists have partitioned Arabic literature into multiple eras, relatively little research has explored the automatic classification of Arabic texts by time period, particularly beyond the domain of poetry. This paper addresses this gap by employing neural networks and deep learning techniques to automatically classify Arabic texts into distinct eras and periods. The proposed models are evaluated using two datasets derived from two publicly available corpora, covering texts from the pre-Islamic to the modern era. The study examines class setups ranging from binary to 15-class classification and considers both predefined historical eras and custom periodizations. Results range from F1-scores of 0.83 and 0.79 on the binary-era classification task using the OpenITI and APCD datasets, respectively, to 0.20 on the 15-era classification task using OpenITI and 0.18 on the 12-era classification task using APCD.

</details>


### [113] [LLM-in-Sandbox Elicits General Agentic Intelligence](https://arxiv.org/abs/2601.16206)
*Daixuan Cheng,Shaohan Huang,Yuxian Gu,Huatong Song,Guoxin Chen,Li Dong,Wayne Xin Zhao,Ji-Rong Wen,Furu Wei*

Main category: cs.CL

TL;DR: 本文提出LLM-in-Sandbox方法，让大语言模型（LLMs）在代码沙箱环境中操作，并由此提升其在非代码领域的通用智能能力。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在非代码领域的智能能力探索受限，难以自行利用丰富的计算环境与外部资源。作者希望让LLM具备更强的自主探索与问题求解能力，尤其是在数学、物理、生物等复杂任务中。

Method: 作者提出在虚拟的代码沙箱（virtual computer）里运行LLM，让其能主动调用外部资源、操作文件系统和执行脚本，帮助处理知识获取、长上下文管理和特定格式需求。同时，提出LLM-in-Sandbox-RL，利用非智能体数据训练模型增强其沙箱探索能力。

Result: 实验显示，LLM-in-Sandbox无论在无需训练还是强化学习微调后，均能在数学、物理、化学、生物医学、长上下文理解和指令跟随等任务上表现出强泛化能力。

Conclusion: LLM-in-Sandbox显著提升了大模型跨领域智能和自主能力，并且通过开源Python包，降低实际部署和集成门槛，展示出良好的系统效率与实用前景。

Abstract: We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [114] [Designing Persuasive Social Robots for Health Behavior Change: A Systematic Review of Behavior Change Strategies and Evaluation Methods](https://arxiv.org/abs/2601.15309)
*Jiaxin Xu,Chao Zhang,Raymond H. Cuijpers,Wijnand A. IJsselsteijn*

Main category: cs.RO

TL;DR: 本综述系统梳理了社会机器人在健康行为改变中的应用，总结了相关策略与评估方法，提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 社会机器人作为健康行为干预手段日益普及，但其设计与评估缺乏可操作的指导知识。

Method: 通过系统数据库检索和手工检索，综合分析了39项涉及社会机器人促进健康行为改变的人机交互（HRI）研究，归纳了行为改变策略和评估方法。

Result: 总结出四类关键行为改变策略：教练策略、咨询策略、社会影响策略和增强说服力策略，显示了社会机器人作为干预工具的独特优势。同时梳理了当前主流评估实践，包括研究设计、环境、周期与评估指标。

Conclusion: 社会机器人在健康行为改变中的策略和评估方法具有独特价值，为未来HRI研究和实践提供了设计启示和发展方向建议。

Abstract: Social robots are increasingly applied as health behavior change interventions, yet actionable knowledge to guide their design and evaluation remains limited. This systematic review synthesizes (1) the behavior change strategies used in existing HRI studies employing social robots to promote health behavior change, and (2) the evaluation methods applied to assess behavior change outcomes. Relevant literature was identified through systematic database searches and hand searches. Analysis of 39 studies revealed four overarching categories of behavior change strategies: coaching strategies, counseling strategies, social influence strategies, and persuasion-enhancing strategies. These strategies highlight the unique affordances of social robots as behavior change interventions and offer valuable design heuristics. The review also identified key characteristics of current evaluation practices, including study designs, settings, durations, and outcome measures, on the basis of which we propose several directions for future HRI research.

</details>


### [115] [Preparation and Motion Study of Magnetically Driven Micro Soft Robot Mimicking the Cownose Ray](https://arxiv.org/abs/2601.15349)
*Jiaqing Chang,Song Gao,Chaowei Dong,zhaobang Li,Yang Liu*

Main category: cs.RO

TL;DR: 本研究设计了一种仿牛鼻魟原理的磁响应式微型软体机器人，并通过三维亥姆霍兹线圈进行磁场游动实验，优化其在狭窄水下环境中的无线驱动运动能力。


<details>
  <summary>Details</summary>
Motivation: 传统微型软体机器人由于体积极小，难以采用内置电源，且其在复杂狭窄环境中的高效运动仍存在难题，因此需要探索新型高效的无线驱动和仿生结构设计方式以提升其水下运动性能。

Method: 论文设计并制造了基于NdFeB和PDMS材料的、仿牛鼻魟运动原理的磁响应式微软体机器人。利用三维亥姆霍兹线圈产生振荡谐波磁场，并通过调节磁场参数(幅值及频率)进行游动实验，同时通过改变电流方向与频率实现多种运动模式。采用步进调整法减少响应误差影响。

Result: 实验结果表明，机器人在B=5 mT、f=11 Hz时游动速度最快，达到5.25 mm/s（约0.5个体长/s）。通过调整线圈参数，机器人能实现直线游动、转弯、定向等多种运动模式，且轨迹控制精度得到提升。

Conclusion: 本研究验证了一种基于磁场无线驱动的微型仿生软体机器人的设计和控制方法，为其在狭窄水下空间中的实际应用奠定了基础。

Abstract: In narrow, unstructured underwater environments such as environmental monitoring and minimally invasive medical procedures, micro soft robots exhibit unique advantages due to their flexible movement capabilities and small size. At the same time, applying bionic technology to the structural design of micro soft robots can significantly improve their swimming performance. However, limited by their miniaturization, these robots are difficult to power internally and usually adopt a wireless power supply method. This study designs and fabricates a magnetically responsive, cownose ray-inspired micro soft robot based on the swimming principle of the cownose ray. The robot is made of a certain proportion of NdFeB and PDMS. Then, a three-dimensional Helmholtz coil is used to generate an oscillating harmonic magnetic field to conduct swimming experiments on the robot, exploring the influence of magnetic field parameters on the robot's swimming performance. The experimental results show that the swimming speed is the fastest at B = 5 mT and f = 11 Hz, reaching 5.25 mm/s, which is about 0.5 body lengths per second. In addition, by adjusting the current direction and frequency of the coil, the robot can perform different swimming modes such as straight swimming, turning swimming, and directional swimming. By employing a stepwise adjustment method, the impact of response errors on the robot's trajectory can be effectively reduced. This study demonstrates a method for magnetically driven micro soft robots, laying a foundation for the application of wireless-driven robots in underwater narrow spaces.

</details>


### [116] [Learning a Unified Latent Space for Cross-Embodiment Robot Control](https://arxiv.org/abs/2601.15419)
*Yashuai Yan,Dongheui Lee*

Main category: cs.RO

TL;DR: 本文提出了一套可以跨不同类型类人机器人（如单臂、双臂和类人腿型机器人）实现统一动作控制的可扩展框架。通过学习共享的潜在表示空间，实现了运动的迁移与泛化。实验结果显示该方法具备强大的通用性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有机器人动作迁移和控制方法局限于相似结构的机器人，难以在多类型、多结构的类人机器人中通用，因此需要一种能跨形态、跨平台共享运动能力的通用控制方法。

Method: 主要分为两步：首先利用对比学习构建可分离的潜在空间，以捕捉不同身体部位的局部运动模式，并引入基于关节旋转和末端执行器位置的相似性度量来增强不同形态之间的对齐。然后，利用条件变分自编码器，只用人类数据在该潜在空间中训练以目标为条件的控制策略，实现动作预测和泛化。新机器人只需训练轻量的特定嵌入层即可接入系统。

Result: 无需针对新机器人进行复杂适配，系统即可直接部署到多个不同形态机器人上。新增机器人通过训练嵌入层后，也能直接复用潜在策略。实验验证了方法的鲁棒性、扩展性和无关具体形态的泛用能力。

Conclusion: 提出的方法实现了不同类型类人机器人间的统一运动控制，具备良好的泛化与扩展性，为实现通用机器人控制迈出重要一步。

Abstract: We present a scalable framework for cross-embodiment humanoid robot control by learning a shared latent representation that unifies motion across humans and diverse humanoid platforms, including single-arm, dual-arm, and legged humanoid robots. Our method proceeds in two stages: first, we construct a decoupled latent space that captures localized motion patterns across different body parts using contrastive learning, enabling accurate and flexible motion retargeting even across robots with diverse morphologies. To enhance alignment between embodiments, we introduce tailored similarity metrics that combine joint rotation and end-effector positioning for critical segments, such as arms. Then, we train a goal-conditioned control policy directly within this latent space using only human data. Leveraging a conditional variational autoencoder, our policy learns to predict latent space displacements guided by intended goal directions. We show that the trained policy can be directly deployed on multiple robots without any adaptation. Furthermore, our method supports the efficient addition of new robots to the latent space by learning only a lightweight, robot-specific embedding layer. The learned latent policies can also be directly applied to the new robots. Experimental results demonstrate that our approach enables robust, scalable, and embodiment-agnostic robot control across a wide range of humanoid platforms.

</details>


### [117] [Neural Collision Detection for Multi-arm Laparoscopy Surgical Robots Through Learning-from-Simulation](https://arxiv.org/abs/2601.15459)
*Sarvin Ghiasi,Majid Roshanfar,Jake Barralet,Liane S. Feldman,Amir Hooshiar*

Main category: cs.RO

TL;DR: 该研究提出了一种集成框架，通过分析建模、实时仿真与机器学习结合，解决腔镜手术中机器人手臂碰撞检测与最小距离估算难题，提升了安全性与操作效率。


<details>
  <summary>Details</summary>
Motivation: 在腔镜手术中，机器人手臂之间发生碰撞会直接影响患者安全和手术效率，因此提升碰撞检测与最小距离估算的准确性至关重要。

Method: 1）提出分析模型，利用机器人关节参数精确估算手臂之间最小距离，提供理论参考。2）开发3D仿真平台模拟两台7自由度Kinova机械臂，生成大量碰撞与距离样本数据。3）基于仿真数据，训练深度神经网络，输入为关节执行器与相对位置，输出为最小距离。

Result: 神经网络模型预测最小距离时，平均绝对误差为282.2毫米，R²为0.85，显示出预测值与真实值高度一致，并且具有良好的泛化能力。

Conclusion: 通过结合高精度分析模型与深度学习方法，显著提升了机器人操作的安全性及可靠性，为腔镜手术机器人安全控制与协作提供了有效方案。

Abstract: This study presents an integrated framework for enhancing the safety and operational efficiency of robotic arms in laparoscopic surgery by addressing key challenges in collision detection and minimum distance estimation. By combining analytical modeling, real-time simulation, and machine learning, the framework offers a robust solution for ensuring safe robotic operations. An analytical model was developed to estimate the minimum distances between robotic arms based on their joint configurations, offering precise theoretical calculations that serve as both a validation tool and a benchmark. To complement this, a 3D simulation environment was created to model two 7-DOF Kinova robotic arms, generating a diverse dataset of configurations for collision detection and distance estimation. Using these insights, a deep neural network model was trained with joint actuators of robot arms and relative positions as inputs, achieving a mean absolute error of 282.2 mm and an R-squared value of 0.85. The close alignment between predicted and actual distances highlights the network's accuracy and its ability to generalize spatial relationships. This work demonstrates the effectiveness of combining analytical precision with machine learning algorithms to enhance the precision and reliability of robotic systems.

</details>


### [118] [A Universal Large Language Model -- Drone Command and Control Interface](https://arxiv.org/abs/2601.15486)
*Javier N. Ramos-Silva,Peter J. Burke*

Main category: cs.RO

TL;DR: 该论文提出了一种通用的AI与无人机控制接口，通过MCP标准实现了大语言模型与无人机的无缝集成，支持地图、天气等实时数据，简化了无人机智能控制。


<details>
  <summary>Details</summary>
Motivation: 当前将大语言模型应用于无人机控制时，存在接口复杂、应用对接繁琐、需要重复开发的问题，限制了物理AI的广泛应用和落地。

Method: 提出并实现了基于MCP（Model Context Protocol）的通用开放接口，将大语言模型与支持Mavlink协议的无人机平台连接。接口设计为LLM与无人机设备无关，支持向AI提供实时外部数据并实现自然语言到无人机控制命令的翻译。系统部署于云端Linux服务器，进行真实及仿真无人机飞行验证。

Result: 实验证明，该平台能实现真实无人机和仿真无人机的灵活控制，可利用Google Maps集成实现实时导航，显著提升了无人机的飞行规划与任务执行能力。

Conclusion: MCP标准为AI系统与无人机集成提供了通用、易用、高兼容的解决方案，推动了AI赋能无人机控制的产业化和智能化进程。

Abstract: The use of artificial intelligence (AI) for drone control can have a transformative impact on drone capabilities, especially when real world information can be integrated with drone sensing, command, and control, part of a growing field of physical AI. Large language models (LLMs) can be advantageous if trained at scale on general knowledge, but especially and in particular when the training data includes information such as detailed map geography topology of the entire planet, as well as the ability to access real time situational data such as weather. However, challenges remain in the interface between drones and LLMs in general, with each application requiring a tedious, labor intensive effort to connect the LLM trained knowledge to drone command and control. Here, we solve that problem, using an interface strategy that is LLM agnostic and drone agnostic, providing the first universal, versatile, comprehensive and easy to use drone control interface. We do this using the new model context protocol (MCP) standard, an open standard that provides a universal way for AI systems to access external data, tools, and services. We develop and deploy a cloud based Linux machine hosting an MCP server that supports the Mavlink protocol, an ubiquitous drone control language used almost universally by millions of drones including Ardupilot and PX4 framework.We demonstrate flight control of a real unmanned aerial vehicle. In further testing, we demonstrate extensive flight planning and control capability in a simulated drone, integrated with a Google Maps MCP server for up to date, real time navigation information. This demonstrates a universal approach to integration of LLMs with drone command and control, a paradigm that leverages and exploits virtually all of modern AI industry with drone technology in an easy to use interface that translates natural language to drone control.

</details>


### [119] [CompliantVLA-adaptor: VLM-Guided Variable Impedance Action for Safe Contact-Rich Manipulation](https://arxiv.org/abs/2601.15541)
*Heng Zhang,Wei-Hsing Huang,Qiyi Tong,Gokhan Solak,Puze Liu,Sheng Liu,Jan Peters,Arash Ajoudani*

Main category: cs.RO

TL;DR: 本文提出了CompliantVLA-adaptor，将先进的视觉-语言-动作（VLA）模型与可变阻抗控制（VIC）结合，引入视觉-语言模型（VLM）并通过力反馈提升了机器人复杂接触操作任务的安全性与有效性，显著优于现有VLA基线。


<details>
  <summary>Details</summary>
Motivation: 现有VLA系统通常只输出位置，缺乏对力的自适应能力，导致在涉及接触、不确定性或柔顺性的任务中易出现危险或失败。因此需要结合力感知与任务上下文来提升安全性和完成率。

Method: 提出了一种CompliantVLA-adaptor，利用视觉-语言模型从图像和自然语言任务描述中理解上下文，并据此自适应调整可变阻抗控制器的刚度和阻尼参数；同时结合实时力/力矩反馈，实现安全阈值内的交互。

Result: 在仿真和真实硬件上的一系列复杂接触任务中，该方法取得了更高的成功率和更少的力违规，其任务总成功率由9.86%提升到17.29%，明显优于同类VLA基线系统。

Conclusion: CompliantVLA-adaptor为基于VLA的安全接触操作提供了新途径，显著提升了在接触丰富场景下的成功率和安全性，具有良好的实际应用前景。相关资源数据公开以促进后续研究。

Abstract: We propose a CompliantVLA-adaptor that augments the state-of-the-art Vision-Language-Action (VLA) models with vision-language model (VLM)-informed context-aware variable impedance control (VIC) to improve the safety and effectiveness of contact-rich robotic manipulation tasks. Existing VLA systems (e.g., RDT, Pi0, OpenVLA-oft) typically output position, but lack force-aware adaptation, leading to unsafe or failed interactions in physical tasks involving contact, compliance, or uncertainty. In the proposed CompliantVLA-adaptor, a VLM interprets task context from images and natural language to adapt the stiffness and damping parameters of a VIC controller. These parameters are further regulated using real-time force/torque feedback to ensure interaction forces remain within safe thresholds. We demonstrate that our method outperforms the VLA baselines on a suite of complex contact-rich tasks, both in simulation and on real hardware, with improved success rates and reduced force violations. The overall success rate across all tasks increases from 9.86\% to 17.29\%, presenting a promising path towards safe contact-rich manipulation using VLAs. We release our code, prompts, and force-torque-impedance-scenario context datasets at https://sites.google.com/view/compliantvla.

</details>


### [120] [A Mobile Magnetic Manipulation Platform for Gastrointestinal Navigation with Deep Reinforcement Learning Control](https://arxiv.org/abs/2601.15545)
*Zhifan Yan,Chang Liu,Yiyang Jiang,Wenxuan Zheng,Xinhao Chen,Axel Krieger*

Main category: cs.RO

TL;DR: 本论文提出了一种利用深度强化学习（DRL）进行肠道内磁驱动机器人靶向药物递送的新平台，有效提升了控制效率和操作精度。


<details>
  <summary>Details</summary>
Motivation: 磁性机器人在胃肠道内实现药物靶向递送，避免全身性治疗副作用，但现有磁控制系统或受限于操作空间，或需复杂的物理模型校准，限制了实际应用。

Method: 设计了一套安装在UR5协作机器人上的四电磁铁阵列系统，采用基于Soft Actor-Critic（SAC）的深度强化学习，通过仿真到现实的迁移训练，实现无需传统模型校准的快速控制策略部署。

Result: 控制器对7毫米磁胶囊在二维轨迹上测试，方形轨迹均方根误差为1.18毫米，圆形轨迹为1.50毫米，且在30cm×20cm的实际工作空间内成功完成运动追踪。

Conclusion: 该系统提供了一种高精度、低成本、快速部署的模型无关磁控制框架，可为胃肠道内精准药物递送等应用提供技术基础。

Abstract: Targeted drug delivery in the gastrointestinal (GI) tract using magnetic robots offers a promising alternative to systemic treatments. However, controlling these robots is a major challenge. Stationary magnetic systems have a limited workspace, while mobile systems (e.g., coils on a robotic arm) suffer from a "model-calibration bottleneck", requiring complex, pre-calibrated physical models that are time-consuming to create and computationally expensive. This paper presents a compact, low-cost mobile magnetic manipulation platform that overcomes this limitation using Deep Reinforcement Learning (DRL). Our system features a compact four-electromagnet array mounted on a UR5 collaborative robot. A Soft Actor-Critic (SAC)-based control strategy is trained through a sim-to-real pipeline, enabling effective policy deployment within 15 minutes and significantly reducing setup time. We validated the platform by controlling a 7-mm magnetic capsule along 2D trajectories. Our DRL-based controller achieved a root-mean-square error (RMSE) of 1.18~mm for a square path and 1.50~mm for a circular path. We also demonstrated successful tracking over a clinically relevant, 30 cm * 20 cm workspace. This work demonstrates a rapidly deployable, model-free control framework capable of precise magnetic manipulation in a large workspace,validated using a 2D GI phantom.

</details>


### [121] [Airflow Source Seeking on Small Quadrotors Using a Single Flow Sensor](https://arxiv.org/abs/2601.15607)
*Lenworth Thomas,Tjaden Bridges,Sarah Bergbreiter*

Main category: cs.RO

TL;DR: 本文提出了一种用于小型四旋翼无人机的气流源寻迹方法，通过自制的流量传感器提升气流检测和寻迹能力，能够更可靠地找到污染源。


<details>
  <summary>Details</summary>
Motivation: 随着环境灾害频发，寻找到污染或有害颗粒物的源头至关重要。小型四旋翼可在狭窄环境中作业，但受限于微型气体传感器的灵敏度和响应速度低下，难以有效进行羽流追踪。

Method: 作者开发了一种能检测气流大小和方向的自制传感器，并在重<100g的小型四旋翼上实现。借助该传感器，修正并实现了‘Cast and Surge’算法，利用流向信息引导无人机导航至气流源。进行了飞行状态下的气流检测和定向实验。

Result: 实验表明，该系统在飞行中可以检测到气流并引导四旋翼调整方向。多轮随机起点的实验验证了算法可靠地找到气流源的能力。

Conclusion: 本研究为未来结合多种传感器以增强羽流追踪与源头定位的平台打下基础。

Abstract: As environmental disasters happen more frequently and severely, seeking the source of pollutants or harmful particulates using plume tracking becomes even more important. Plume tracking on small quadrotors would allow these systems to operate around humans and fly in more confined spaces, but can be challenging due to poor sensitivity and long response times from gas sensors that fit on small quadrotors. In this work, we present an approach to complement chemical plume tracking with airflow source-seeking behavior using a custom flow sensor that can sense both airflow magnitude and direction on small quadrotors < 100 g. We use this sensor to implement a modified version of the `Cast and Surge' algorithm that takes advantage of flow direction sensing to find and navigate towards flow sources. A series of characterization experiments verified that the system can detect airflow while in flight and reorient the quadrotor toward the airflow. Several trials with random starting locations and orientations were used to show that our source-seeking algorithm can reliably find a flow source. This work aims to provide a foundation for future platforms that can use flow sensors in concert with other sensors to enable richer plume tracking data collection and source-seeking.

</details>


### [122] [AION: Aerial Indoor Object-Goal Navigation Using Dual-Policy Reinforcement Learning](https://arxiv.org/abs/2601.15614)
*Zichen Yan,Yuchen Hou,Shenao Wang,Yichao Gao,Rui Huang,Lin Zhao*

Main category: cs.RO

TL;DR: 本论文提出AION框架，实现了无人机在未知环境下仅凭图像自主导航并寻找目标对象，且无需外部定位与地图。实验结果显示该方法在探索、导航效率与安全性方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 以往ObjectNav任务多基于地面2D移动，极少涉及具备3D机动性的空中机器人。空中平台能更高效搜索，但也面临空间感知、动态控制和安全等新难点，因此亟需专门的视觉导航方法。

Method: 提出AION框架，一个端到端的双策略强化学习（RL）系统，分别为探索和目标到达设计独立策略。AION不依赖外部定位与全局地图，仅用视觉信息实现自主导航，并在AI2-THOR基准和IsaacSim仿真环境中评估。

Result: AION在探索、导航效率和安全性等多个评价指标上，均表现优于现有方法，且在高保真无人机仿真中具备实时性能。

Conclusion: AION为视觉驱动的空中ObjectNav提供了有效解法，展示了在无需外部定位与地图条件下实现高效、安全导航的潜力，有望推动空中机器人自主能力进一步发展。

Abstract: Object-Goal Navigation (ObjectNav) requires an agent to autonomously explore an unknown environment and navigate toward target objects specified by a semantic label. While prior work has primarily studied zero-shot ObjectNav under 2D locomotion, extending it to aerial platforms with 3D locomotion capability remains underexplored. Aerial robots offer superior maneuverability and search efficiency, but they also introduce new challenges in spatial perception, dynamic control, and safety assurance. In this paper, we propose AION for vision-based aerial ObjectNav without relying on external localization or global maps. AION is an end-to-end dual-policy reinforcement learning (RL) framework that decouples exploration and goal-reaching behaviors into two specialized policies. We evaluate AION on the AI2-THOR benchmark and further assess its real-time performance in IsaacSim using high-fidelity drone models. Experimental results show that AION achieves superior performance across comprehensive evaluation metrics in exploration, navigation efficiency, and safety. The video can be found at https://youtu.be/TgsUm6bb7zg.

</details>


### [123] [D-Optimality-Guided Reinforcement Learning for Efficient Open-Loop Calibration of a 3-DOF Ankle Rehabilitation Robot](https://arxiv.org/abs/2601.15707)
*Qifan Hu,Branko Celler,Weidong Mu,Steven W. Su*

Main category: cs.RO

TL;DR: 本文提出了一个适用于三自由度踝关节康复机器人精准对齐的两阶段标定框架，无需大量姿态数据即可实现高效且高精度的机器人参数标定。


<details>
  <summary>Details</summary>
Motivation: 多自由度康复机器人在患者训练中需精准对齐，否则可能存在安全隐患和训练无效的问题。现有标定方法效率低且对资源消耗大，亟需更高效、实用的参数标定策略。

Method: 首先，通过基于Kronecker积的开环标定方法，将机器人输入输出对齐转化为线性参数辨识问题，并通过信息矩阵设计实验。随后采用D-最优标准，将姿态选择建模为组合优化问题。利用PPO算法在仿真中训练智能体，从50个备选姿态中选出4个高信息量姿态，实现高效实验设计。

Result: 仿真和实机评测表明，PPO算法选择的姿态组信息矩阵行列式均值比随机选择高两个数量级且方差更小。实验证明，仅凭4个D-最优姿态得到的参数向量，其预测一致性优于从50个无结构姿态中获得的结果。

Conclusion: 该框架显著提升了康复机器人标定效率，实现了更鲁棒的参数估计，可为多自由度康复机器人高精度对齐提供实用指导。

Abstract: Accurate alignment of multi-degree-of-freedom rehabilitation robots is essential for safe and effective patient training. This paper proposes a two-stage calibration framework for a self-designed three-degree-of-freedom (3-DOF) ankle rehabilitation robot. First, a Kronecker-product-based open-loop calibration method is developed to cast the input-output alignment into a linear parameter identification problem, which in turn defines the associated experimental design objective through the resulting information matrix. Building on this formulation, calibration posture selection is posed as a combinatorial design-of-experiments problem guided by a D-optimality criterion, i.e., selecting a small subset of postures that maximises the determinant of the information matrix. To enable practical selection under constraints, a Proximal Policy Optimization (PPO) agent is trained in simulation to choose 4 informative postures from a candidate set of 50. Across simulation and real-robot evaluations, the learned policy consistently yields substantially more informative posture combinations than random selection: the mean determinant of the information matrix achieved by PPO is reported to be more than two orders of magnitude higher with reduced variance. In addition, real-world results indicate that a parameter vector identified from only four D-optimality-guided postures provides stronger cross-episode prediction consistency than estimates obtained from a larger but unstructured set of 50 postures. The proposed framework therefore improves calibration efficiency while maintaining robust parameter estimation, offering practical guidance for high-precision alignment of multi-DOF rehabilitation robots.

</details>


### [124] [DualShield: Safe Model Predictive Diffusion via Reachability Analysis for Interactive Autonomous Driving](https://arxiv.org/abs/2601.15729)
*Rui Yang,Lei Zheng,Ruoyu Yao,Jun Ma*

Main category: cs.RO

TL;DR: 本文提出DualShield框架，通过双重利用Hamilton-Jacobi（HJ）可达性值函数，为自动驾驶中的扩散模型规划与控制同时提供主动与被动的安全保障，显著提升了不确定交互下的安全性和任务效率。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在自动驾驶多模态运动规划中表现优异，但受限于难以严格约束车辆动力学和高度依赖对其他智能体的精确预测，导致其在不确定交互下存在安全隐患，限制了实际部署。

Method: DualShield框架通过两种方式应用HJ可达性值函数：一方面作为主动引导，将扩散生成过程限定在安全、动力学可行的区域；另一方面结合控制屏障-值函数（CBVFs），对实际动作进行修正，实时构建反应性安全保护。该方法兼顾了扩散模型的多样性探索和理论上的安全保证。

Result: 在具有挑战性的无人防护U型掉头等场景下，DualShield在安全性和任务效率方面较多种主流规划方法取得了显著提升，尤其是在面对不确定、具有对抗性干扰时。

Conclusion: DualShield为多模态运动规划中的扩散模型提供了有效的安全增强机制，使其在安全和性能间实现了平衡，推动了相关模型实用化进程。

Abstract: Diffusion models have emerged as a powerful approach for multimodal motion planning in autonomous driving. However, their practical deployment is typically hindered by the inherent difficulty in enforcing vehicle dynamics and a critical reliance on accurate predictions of other agents, making them prone to safety issues under uncertain interactions. To address these limitations, we introduce DualShield, a planning and control framework that leverages Hamilton-Jacobi (HJ) reachability value functions in a dual capacity. First, the value functions act as proactive guidance, steering the diffusion denoising process towards safe and dynamically feasible regions. Second, they form a reactive safety shield using control barrier-value functions (CBVFs) to modify the executed actions and ensure safety. This dual mechanism preserves the rich exploration capabilities of diffusion models while providing principled safety assurance under uncertain and even adversarial interactions. Simulations in challenging unprotected U-turn scenarios demonstrate that DualShield significantly improves both safety and task efficiency compared to leading methods from different planning paradigms under uncertainty.

</details>


### [125] [Glove2UAV: A Wearable IMU-Based Glove for Intuitive Control of UAV](https://arxiv.org/abs/2601.15775)
*Amir Habel,Ivan Snegirev,Elizaveta Semenyakina,Miguel Altamirano Cabrera,Jeffrin Sam,Fawad Mehboob,Roohan Ahmed Khan,Muhammad Ahsan Mustafa,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: 本文提出了一种基于IMU手套（Glove2UAV）的无人机直观控制接口，通过手部和手指手势以及振动反馈进行实时操控与警示。


<details>
  <summary>Details</summary>
Motivation: 目前无人机的控制接口多依赖遥控器等传统方式，缺乏直观、自然的人机交互手段，尤其在动态飞行和安全性需求不断上升的背景下，亟需更直观且能及时警示的交互方案。

Method: Glove2UAV系统基于可穿戴IMU手套，利用中值滤波与Madgwick算法实现手掌及手指实时姿态估算，并将相应动作映射为无人机的飞行控制指令。飞行速度超出阈值时，手套通过振动进行警告。同步验证了手套信号与无人机遥测数据在仿真和实飞中的实时性。

Result: 实验结果显示，系统能够快速准确地根据手势输出飞行指令，保证了动作与无人机运动的稳定耦合，所有核心指令在试验中均运行正确，振动告警响应及时。

Conclusion: Glove2UAV是一种轻巧、易用、可实时运行的可穿戴无人机控制接口，为动态飞行中的安全性和自然交互提供了有效支持。

Abstract: This paper presents Glove2UAV, a wearable IMU-glove interface for intuitive UAV control through hand and finger gestures, augmented with vibrotactile warnings for exceeding predefined speed thresholds. To promote safer and more predictable interaction in dynamic flight, Glove2UAV is designed as a lightweight and easily deployable wearable interface intended for real-time operation. Glove2UAV streams inertial measurements in real time and estimates palm and finger orientations using a compact processing pipeline that combines median-based outlier suppression with Madgwick-based orientation estimation. The resulting motion estimations are mapped to a small set of control primitives for directional flight (forward/backward and lateral motion) and, when supported by the platform, to object-interaction commands. Vibrotactile feedback is triggered when flight speed exceeds predefined threshold values, providing an additional alert channel during operation. We validate real-time feasibility by synchronizing glove signals with UAV telemetry in both simulation and real-world flights. The results show fast gesture-based command execution, stable coupling between gesture dynamics and platform motion, correct operation of the core command set in our trials, and timely delivery of vibratile warning cues.

</details>


### [126] [A Beacon Based Solution for Autonomous UUVs GNSS-Denied Stealthy Navigation](https://arxiv.org/abs/2601.15802)
*Alexandre Albore,Humbert Fiorino,Damien Pellier*

Main category: cs.RO

TL;DR: 本文提出了一种在无GNSS支持和隐蔽需求下，利用无人机布设声学信标，实现水下无人航行器（UUV）精准导航与协同的方案。


<details>
  <summary>Details</summary>
Motivation: 在近海受限环境（如禁区、危险区）内执行隐蔽行动时，UUV不能依赖水面支援或GNSS导航，必须通过其他手段实现精准定位，保障任务执行和隐蔽性。

Method: 通过无人机（空中或水面）布设水下或浮水声学信标，形成人工地标网络，为UUV导航提供参考。高层次规划器生成自适应航线，驱动无人机实时调整信标和路径，确保整体队形与轨迹精度。

Result: 本方法可以有效实现UUV在非GNSS环境下的隐蔽、自主、精准协同导航，并能动态调整以应对环境变化。

Conclusion: 利用无人机布设信标网络，结合分层自适应规划，实现了UUV在敏感和受限海域的高效、隐蔽、精准导航，提升了UUV队伍的作战和任务能力。

Abstract: Autonomous Unmanned Underwater Vehicles (UUVs) enable military and civilian covert operations in coastal areas without relying on support vessels or Global Navigation Satellite Systems (GNSS). Such operations are critical when surface access is not possible and stealthy navigation is required in restricted environments such as protected zones or dangerous areas under access ban. GNSS denied navigation is then essential to maintaining concealment as surfacing could expose UUVs to detection. To ensure a precise fleet positioning a constellation of beacons deployed by aerial or surface drones establish a synthetic landmark network that will guide the fleet of UUVs along an optimized path from the continental shelf to the goal on the shore. These beacons either submerged or floating emit acoustic signals for UUV localisation and navigation. A hierarchical planner generates an adaptive route for the drones executing primitive actions while continuously monitoring and replanning as needed to maintain trajectory accuracy.

</details>


### [127] [TeNet: Text-to-Network for Compact Policy Synthesis](https://arxiv.org/abs/2601.15912)
*Ariyan Bighashdel,Kevin Sebastian Luck*

Main category: cs.RO

TL;DR: 本文提出了一种新的机器人控制框架TeNet，可直接将自然语言描述转化为小巧且高效的机器人控制策略，兼顾泛化能力与实时性。


<details>
  <summary>Details</summary>
Motivation: 现有基于自然语言指令的机器人控制，要么高度依赖手工设计的接口（缺乏通用性），要么采用端到端大模型（难以实时部署）。因此需要能兼容大模型语言理解能力又便于高效执行的新方法。

Method: TeNet利用预训练大语言模型生成文本嵌入，并将该嵌入输入超网络，直接产出面向具体任务的小型控制策略。训练时可选用行为对齐以提升泛化，无需推理时示范。

Result: 在MuJoCo和Meta-World等基准测试上，TeNet生成的策略远小于传统序列模型，表现优异，支持多任务、元学习及高频率控制。

Conclusion: 文本条件的超网络为资源受限的机器人控制任务提供了紧凑且高效的语言驱动控制器，兼具LLM泛化能力与实时控制可用性。

Abstract: Robots that follow natural-language instructions often either plan at a high level using hand-designed interfaces or rely on large end-to-end models that are difficult to deploy for real-time control. We propose TeNet (Text-to-Network), a framework for instantiating compact, task-specific robot policies directly from natural language descriptions. TeNet conditions a hypernetwork on text embeddings produced by a pretrained large language model (LLM) to generate a fully executable policy, which then operates solely on low-dimensional state inputs at high control frequencies. By using the language only once at the policy instantiation time, TeNet inherits the general knowledge and paraphrasing robustness of pretrained LLMs while remaining lightweight and efficient at execution time. To improve generalization, we optionally ground language in behavior during training by aligning text embeddings with demonstrated actions, while requiring no demonstrations at inference time. Experiments on MuJoCo and Meta-World benchmarks show that TeNet produces policies that are orders of magnitude smaller than sequence-based baselines, while achieving strong performance in both multi-task and meta-learning settings and supporting high-frequency control. These results show that text-conditioned hypernetworks offer a practical way to build compact, language-driven controllers for ressource-constrained robot control tasks with real-time requirements.

</details>


### [128] [Accurate Calibration and Robust LiDAR-Inertial Odometry for Spinning Actuated LiDAR Systems](https://arxiv.org/abs/2601.15946)
*Zijie Chen,Xiaowei Liu,Yong Xu,Shenghai Yuan,Jianping Li,Lihua Xie*

Main category: cs.RO

TL;DR: 本文提出了一种基于Denavit-Hartenberg方法的无靶标LiDAR-电机标定方法（LM-Calibr）和一种环境自适应LiDAR-惯性里程计方法（EVA-LIO），解决了多种安装方式下Spinning LiDAR系统的标定和稠密/稀疏区域下定位鲁棒性难题。


<details>
  <summary>Details</summary>
Motivation: 现有的旋转LiDAR系统在不同安装方式下需要单独参数化外参，泛化性差。此外，LiDAR经常扫描到特征稀少区域，导致数据稀疏，难以在保证定位鲁棒性的同时实现大范围扫描。

Method: 1）提出了基于Denavit-Hartenberg参数规范的LM-Calibr算法，无需靶标即可对各种安装方式下的LiDAR-电机系统进行联合标定。2）提出开源的EVA-LIO方法，可自适应调整点云降采样率和地图分辨率，实现最大扫描覆盖同时提升定位在特征稀少区的鲁棒性。

Result: 大量实验表明，LM-Calibr在不同工况、安装角度、初始值下均表现出准确性和收敛性。EVA-LIO可使驱动器以最高速度工作，在保证定位鲁棒性的前提下完成完整扫描，即使遇到部分无特征区域。

Conclusion: 所提系统具备更高的通用性和自适应能力，显著提升了旋转LiDAR系统的标定精度和定位鲁棒性。源码及设计已开源，有应用推广价值。

Abstract: Accurate calibration and robust localization are fundamental for downstream tasks in spinning actuated LiDAR applications. Existing methods, however, require parameterizing extrinsic parameters based on different mounting configurations, limiting their generalizability. Additionally, spinning actuated LiDAR inevitably scans featureless regions, which complicates the balance between scanning coverage and localization robustness. To address these challenges, this letter presents a targetless LiDAR-motor calibration (LM-Calibr) on the basis of the Denavit-Hartenberg convention and an environmental adaptive LiDAR-inertial odometry (EVA-LIO). LM-Calibr supports calibration of LiDAR-motor systems with various mounting configurations. Extensive experiments demonstrate its accuracy and convergence across different scenarios, mounting angles, and initial values. Additionally, EVA-LIO adaptively selects downsample rates and map resolutions according to spatial scale. This adaptivity enables the actuator to operate at maximum speed, thereby enhancing scanning completeness while ensuring robust localization, even when LiDAR briefly scans featureless areas. The source code and hardware design are available on GitHub: \textcolor{blue}{\href{https://github.com/zijiechenrobotics/lm_calibr}{github.com/zijiechenrobotics/lm\_calibr}}. The video is available at \textcolor{blue}{\href{https://youtu.be/cZyyrkmeoSk}{youtu.be/cZyyrkmeoSk}}

</details>


### [129] [PUMA: Perception-driven Unified Foothold Prior for Mobility Augmented Quadruped Parkour](https://arxiv.org/abs/2601.15995)
*Liang Wang,Kanzhong Yao,Yang Liu,Weikai Qin,Jun Wu,Zhe Sun,Qiuguo Zhu*

Main category: cs.RO

TL;DR: 本文提出了一种新的端到端学习框架PUMA，用于提升四足机器人在复杂环境下的灵活运动能力，实现更敏捷的parkour任务。


<details>
  <summary>Details</summary>
Motivation: 现有的四足机器人多采用分层控制器，依赖预先计算的落足点，限制了机器人对环境的实时适应和学习能力。而人类运动员能根据视觉环境灵活选择着力点。移植这种环境感知能力到机器人，是亟待解决的重要难题。

Method: 本文设计了PUMA框架，将视觉感知与落足点先验信息整合到统一的训练流程中。核心做法是利用地形特征估计极坐标系下的自中心落足点先验（含相对距离和朝向），引导机器人以主动姿势适应复杂地形。训练和评估在多种仿真和真实离散复杂环境中进行。

Result: 实验表明，PUMA在多种具有挑战性的复杂地形上展现出优异的敏捷性和鲁棒性，能够实现高效的主动姿态调整和可靠的障碍越障能力。

Conclusion: PUMA大幅提升了四足机器人在parkour任务中的实时适应性和自主动能力，为端到端感知运动控制在复杂任务中的落地提供了新范式。

Abstract: Parkour tasks for quadrupeds have emerged as a promising benchmark for agile locomotion. While human athletes can effectively perceive environmental characteristics to select appropriate footholds for obstacle traversal, endowing legged robots with similar perceptual reasoning remains a significant challenge. Existing methods often rely on hierarchical controllers that follow pre-computed footholds, thereby constraining the robot's real-time adaptability and the exploratory potential of reinforcement learning. To overcome these challenges, we present PUMA, an end-to-end learning framework that integrates visual perception and foothold priors into a single-stage training process. This approach leverages terrain features to estimate egocentric polar foothold priors, composed of relative distance and heading, guiding the robot in active posture adaptation for parkour tasks. Extensive experiments conducted in simulation and real-world environments across various discrete complex terrains, demonstrate PUMA's exceptional agility and robustness in challenging scenarios.

</details>


### [130] [Collision-Free Humanoid Traversal in Cluttered Indoor Scenes](https://arxiv.org/abs/2601.16035)
*Han Xue,Sikai Liang,Zhikai Zhang,Zicheng Zeng,Yun Liu,Yunrui Lian,Jilong Wang,Qingtao Liu,Xuesong Shi,Li Yi*

Main category: cs.RO

TL;DR: 本文提出了一种名为HumanoidPF的新方法，使人形机器人能够在复杂拥挤的室内环境中避障和穿越障碍，并实现了从仿真到现实的有效迁移。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效表达机器人与障碍物间的关系，导致基于强化学习的避障与通行技能难以直接学习。

Method: 提出HumanoidPF，将人形机器人与障碍物的空间关系编码为无碰撞的运动方向，从而显著简化RL技能学习。同时，提出混合场景生成方法，结合真实3D场景片段与程序合成障碍，提升泛化能力，并开发了单击式远程操作系统。

Result: HumanoidPF的感知表达在仿真与现实间几乎没有迁移差距。所提出方法在模拟及真实场景下均表现优异，实现了人形机器人在复杂环境中的高效避障和穿越。

Conclusion: HumanoidPF为人形机器人泛化、安全地穿越室内复杂场景提供了有效解决方案，并有良好的现实适应性，推动了仿真到现实的实际应用。

Abstract: We study the problem of collision-free humanoid traversal in cluttered indoor scenes, such as hurdling over objects scattered on the floor, crouching under low-hanging obstacles, or squeezing through narrow passages. To achieve this goal, the humanoid needs to map its perception of surrounding obstacles with diverse spatial layouts and geometries to the corresponding traversal skills. However, the lack of an effective representation that captures humanoid-obstacle relationships during collision avoidance makes directly learning such mappings difficult. We therefore propose Humanoid Potential Field (HumanoidPF), which encodes these relationships as collision-free motion directions, significantly facilitating RL-based traversal skill learning. We also find that HumanoidPF exhibits a surprisingly negligible sim-to-real gap as a perceptual representation. To further enable generalizable traversal skills through diverse and challenging cluttered indoor scenes, we further propose a hybrid scene generation method, incorporating crops of realistic 3D indoor scenes and procedurally synthesized obstacles. We successfully transfer our policy to the real world and develop a teleoperation system where users could command the humanoid to traverse in cluttered indoor scenes with just a single click. Extensive experiments are conducted in both simulation and the real world to validate the effectiveness of our method. Demos and code can be found in our website: https://axian12138.github.io/CAT/.

</details>


### [131] [DextER: Language-driven Dexterous Grasp Generation with Embodied Reasoning](https://arxiv.org/abs/2601.16046)
*Junha Lee,Eunha Park,Minsu Cho*

Main category: cs.RO

TL;DR: 本文提出了一种基于具身推理的新方法DextER，用于多指灵巧抓取任务，通过引入接触为中间表征来提升抓取的任务对齐和物理合理性，显著优于已有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-语言的抓取生成方法，直接将观测信息映射为抓取参数，缺乏对手-物体物理交互的中间推理，导致对任务语义和物理约束的结合不够紧密，有提升空间。

Method: 提出了DextER方法，通过自回归方式生成接触token（指明每个手指节点与物体表面的接触部位）作为中间表征，再生成具体抓取动作参数，实现任务语义与物理约束的有效结合。方法还支持通过部分接触指定，实现受控的抓取合成。

Result: 在DexGYS数据集上，DextER取得了67.14%的抓取成功率，较此前最优方法提升3.83个百分点，并提升了96.4%的意图对齐度，显示方法在效果和可控性上的优越性。

Conclusion: 基于具身接触推理的中间表征能够有效推动多指灵巧抓取的任务对齐性和物理交互合理性，DextER方法在性能和操作灵活性方面达到了当前最优，具有广泛应用潜力。

Abstract: Language-driven dexterous grasp generation requires the models to understand task semantics, 3D geometry, and complex hand-object interactions. While vision-language models have been applied to this problem, existing approaches directly map observations to grasp parameters without intermediate reasoning about physical interactions. We present DextER, Dexterous Grasp Generation with Embodied Reasoning, which introduces contact-based embodied reasoning for multi-finger manipulation. Our key insight is that predicting which hand links contact where on the object surface provides an embodiment-aware intermediate representation bridging task semantics with physical constraints. DextER autoregressively generates embodied contact tokens specifying which finger links contact where on the object surface, followed by grasp tokens encoding the hand configuration. On DexGYS, DextER achieves 67.14% success rate, outperforming state-of-the-art by 3.83%p with 96.4% improvement in intention alignment. We also demonstrate steerable generation through partial contact specification, providing fine-grained control over grasp synthesis.

</details>


### [132] [Improve the autonomy of the SE2(3) group based Extended Kalman Filter for Integrated Navigation: Theoretical Analysis](https://arxiv.org/abs/2601.16062)
*Jiarui Cui,Maosong Wang,Wenqi Wu,Peiqi Li,Xianfei Pan*

Main category: cs.RO

TL;DR: 本文分析了SE2(3)李群框架在高精度导航建模中的自洽性问题，并提出了一种改进的模型构建方法，以增强误差传播的自主性。


<details>
  <summary>Details</summary>
Motivation: 在高精度导航（如精密惯导）场景下，误差传播的自主性变得极为重要。然而，现有的SE2(3)李群扩展卡尔曼滤波方法仅在低精度场景下保证自主性。导航系统在考虑地球自转和惯性器件偏置时，自主性难以维持，需理论分析与方法改进。

Method: 本文从理论上系统分析了在惯性系、地球系和大地系下的SE2(3)李群构建的高精度导航模型的自主性，揭示了传统方法在建模过程中由于非惯性系下速度引入的科氏力项导致的自主性丧失问题，并据此提出了一种新的SE2(3)导航模型构建方法。

Result: 理论分析证明，传统SE2(3)李群建模方法因科氏力的存在难以实现模型的完全自主性，新的模型构建方法显著提升了自主性。

Conclusion: 在地球自转和惯性偏置等高精度导航情境下，采用新的SE2(3)李群导航模型构建方法能够更好地维持误差传播的自主性，为高精度导航状态估计提供更可靠的理论和方法支撑。

Abstract: One of core advantages of the SE2(3) Lie group framework for navigation modeling lies in the autonomy of error propagation. Current research on Lie group based extended Kalman filters has demonstrated that error propagation autonomy holds in low-precision applications, such as in micro electromechanical system (MEMS) based integrated navigation without considering earth rotation and inertial device biases. However, in high-precision navigation state estimation, maintaining autonomy is extremely difficult when considering with earth rotation and inertial device biases. This paper presents the theoretical analysis on the autonomy of SE2(3) group based high-precision navigation models under inertial, earth and world frame respectively. Through theoretical analysis, we find that the limitation of the traditional, trivial SE2(3) group navigation modeling method is that the presence of Coriolis force terms introduced by velocity in non-inertial frame. Therefore, a construction method for SE2(3) group navigation models is proposed, which brings the navigation models closer to full autonomy.

</details>


### [133] [Improve the autonomy of the SE2(3) group based Extended Kalman Filter for Integrated Navigation: Application](https://arxiv.org/abs/2601.16078)
*Jiarui Cui,Maosong Wang,Wenqi Wu,Peiqi Li,Xianfei Pan*

Main category: cs.RO

TL;DR: 本文验证并提升了基于SE2(3)李群框架的自主导航模型在实际与仿真中的表现，强调其误差传播自主性。


<details>
  <summary>Details</summary>
Motivation: 前文已对SE2(3)导航模型的自主性从理论上进行了分析，但缺乏实际与仿真验证，本研究旨在通过实验确认该模型的实际性能与优势。

Method: 提出了一种针对非惯性导航模型的SE2(3)导航模型改进方法，并通过实际SINS/ODO实验与蒙特卡洛仿真，验证了这种改进模型的性能。

Result: 实验结果与仿真表明，改进的SE2(3)群导航模型在导航精度和误差传播自主性方面表现优异。

Conclusion: SE2(3)群导航模型不仅理论上具有更好的误差自主性，实际应用与仿真中也显示了其高精度和鲁棒性，推动自主导航系统向完全自主方向发展。

Abstract: One of the core advantages of SE2(3) Lie group framework for navigation modeling lies in the autonomy of error propagation. In the previous paper, the theoretical analysis of autonomy property of navigation model in inertial, earth and world frames was given. A construction method for SE2(3) group navigation model is proposed to improve the non-inertial navigation model toward full autonomy. This paper serves as a counterpart to previous paper and conducts the real-world strapdown inertial navigation system (SINS)/odometer(ODO) experiments as well as Monte-Carlo simulations to demonstrate the performance of improved SE2(3) group based high-precision navigation models.

</details>


### [134] [Efficiently Learning Robust Torque-based Locomotion Through Reinforcement with Model-Based Supervision](https://arxiv.org/abs/2601.16109)
*Yashuai Yan,Tobias Egle,Christian Ott,Dongheui Lee*

Main category: cs.RO

TL;DR: 提出了一种结合模型控制与残差强化学习的新框架，实现了双足机器人在现实不确定性环境下的稳健自适应行走。


<details>
  <summary>Details</summary>
Motivation: 现有模型控制方法在面对动态建模误差和传感器噪声等现实问题时鲁棒性有限，而单纯的强化学习则需要大量奖励设计并且收敛慢，难以实际落地。

Method: 方法以基于DCM的轨迹规划与全身控制器作为基础策略，通过领域随机化强化学习训练残差策略来补偿建模不准确和噪声。训练阶段引入基于真实动力学的模型oracle策略，通过新设计的监督损失来指导残差策略高效学习补偿行为。

Result: 在多种随机化条件下，该方法明显提升了双足步态的鲁棒性和泛化能力，适合大规模sim-to-real迁移。

Conclusion: 结合模型控制与受监督残差强化学习，有效增强了机器人行走的鲁棒性与适应性，对实际部署有积极意义。

Abstract: We propose a control framework that integrates model-based bipedal locomotion with residual reinforcement learning (RL) to achieve robust and adaptive walking in the presence of real-world uncertainties. Our approach leverages a model-based controller, comprising a Divergent Component of Motion (DCM) trajectory planner and a whole-body controller, as a reliable base policy. To address the uncertainties of inaccurate dynamics modeling and sensor noise, we introduce a residual policy trained through RL with domain randomization. Crucially, we employ a model-based oracle policy, which has privileged access to ground-truth dynamics during training, to supervise the residual policy via a novel supervised loss. This supervision enables the policy to efficiently learn corrective behaviors that compensate for unmodeled effects without extensive reward shaping. Our method demonstrates improved robustness and generalization across a range of randomized conditions, offering a scalable solution for sim-to-real transfer in bipedal locomotion.

</details>


### [135] [IVRA: Improving Visual-Token Relations for Robot Action Policy with Training-Free Hint-Based Guidance](https://arxiv.org/abs/2601.16207)
*Jongwoo Park,Kanchana Ranasinghe,Jinhyeok Jang,Cristina Mata,Yoo Sung Jang,Michael S Ryoo*

Main category: cs.RO

TL;DR: IVRA是一种在视觉语言动作模型中增强空间理解的新方法，能够在不改变模型参数的情况下提升模型在2D与3D操作任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型通常将图像展平成1D序列，导致2D空间信息损失，影响精确操控任务。提升空间结构理解能力对机器人任务尤其关键。

Method: 提出IVRA，一种轻量级且无需训练的方法。通过利用模型自身视觉编码器已有的亲和性信息，将其在推理时注入到含有实例级特征的语言模型层，实现视觉token交互的重新对齐，无需更换编码器或重新训练。

Result: 在VIMA和LIBERO等2D/3D模拟基准上，以及实际机器人任务中，IVRA在多个主流VLA架构（如LLaRA、OpenVLA、FLOWER）上均显示提升：例如2D VIMA低数据环境下平均成功率提升4.2%；在3D LIBERO上也能将接近饱和的准确率从96.3%提升至97.1%。

Conclusion: IVRA能有效保留几何结构信息，在无需调参和训练的前提下提高VLA模型的空间理解能力，并在多种任务和架构中取得了一致的性能提升。

Abstract: Many Vision-Language-Action (VLA) models flatten image patches into a 1D token sequence, weakening the 2D spatial cues needed for precise manipulation. We introduce IVRA, a lightweight, training-free method that improves spatial understanding by exploiting affinity hints already available in the model's built-in vision encoder, without requiring any external encoder or retraining. IVRA selectively injects these affinity signals into a language-model layer in which instance-level features reside. This inference-time intervention realigns visual-token interactions and better preserves geometric structure while keeping all model parameters fixed. We demonstrate the generality of IVRA by applying it to diverse VLA architectures (LLaRA, OpenVLA, and FLOWER) across simulated benchmarks spanning both 2D and 3D manipulation (VIMA and LIBERO) and on various real-robot tasks. On 2D VIMA, IVRA improves average success by +4.2% over the baseline LLaRA in a low-data regime. On 3D LIBERO, it yields consistent gains over the OpenVLA and FLOWER baselines, including improvements when baseline accuracy is near saturation (96.3% to 97.1%). All code and models will be released publicly. Visualizations are available at: jongwoopark7978.github.io/IVRA

</details>


### [136] [Point Bridge: 3D Representations for Cross Domain Policy Learning](https://arxiv.org/abs/2601.16212)
*Siddhant Haldar,Lars Johannsmeier,Lerrel Pinto,Abhishek Gupta,Dieter Fox,Yashraj Narang,Ajay Mandlekar*

Main category: cs.RO

TL;DR: 本文提出Point Bridge框架，使用点云统一表示，结合视觉-语言模型和transformer，实现仅用合成数据训练出可直接应用于现实环境的机器人操控策略，相比现有方法显著提升迁移性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界机器人大规模操控数据稀缺，合成数据虽易扩展但受限于视觉域间差距，亟需通用、高效的合成-现实迁移方案。

Method: 提出Point Bridge，依赖VLM自动提取点云表征、transformer学习策略，通过无需明确视觉或目标对齐的点云方式，将合成数据用于训练机器人，且推理过程中高效。

Result: 仅用合成数据在零样本迁移场景下性能提升高达44%；少量真实数据联合训练时提升达66%；在单任务和多任务均优于现有co-training方法。

Conclusion: 点云统一表示在机器人合成-现实迁移中极具优势，无需复杂对齐步骤，通过少量实数据可进一步提升表现，为通用型机器人学习提供了有效工具箱。

Abstract: Robot foundation models are beginning to deliver on the promise of generalist robotic agents, yet progress remains constrained by the scarcity of large-scale real-world manipulation datasets. Simulation and synthetic data generation offer a scalable alternative, but their usefulness is limited by the visual domain gap between simulation and reality. In this work, we present Point Bridge, a framework that leverages unified, domain-agnostic point-based representations to unlock synthetic datasets for zero-shot sim-to-real policy transfer, without explicit visual or object-level alignment. Point Bridge combines automated point-based representation extraction via Vision-Language Models (VLMs), transformer-based policy learning, and efficient inference-time pipelines to train capable real-world manipulation agents using only synthetic data. With additional co-training on small sets of real demonstrations, Point Bridge further improves performance, substantially outperforming prior vision-based sim-and-real co-training methods. It achieves up to 44% gains in zero-shot sim-to-real transfer and up to 66% with limited real data across both single-task and multitask settings. Videos of the robot are best viewed at: https://pointbridge3d.github.io/

</details>
