<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 181]
- [cs.CL](#cs.CL) [Total: 103]
- [cs.RO](#cs.RO) [Total: 48]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [ESCA: Contextualizing Embodied Agents via Scene-Graph Generation](https://arxiv.org/abs/2510.15963)
*Jiani Huang,Amish Sethi,Matthew Kuo,Mayank Keoliya,Neelay Velingker,JungHo Jung,Ser-Nam Lim,Ziyang Li,Mayur Naik*

Main category: cs.CV

TL;DR: 提出了一种用于多模态大模型的新框架ESCA，核心为SGClip新模型，无需人工标注即可生成场景图，提升了智能体的感知与理解能力，在多个环境中取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大模型训练中，视觉和文本对齐常常只做高层次语义匹配，缺乏像素级别细粒化结构关联，影响智能体精细理解和能力提升。

Method: 提出ESCA框架，核心模型SGClip基于CLIP架构，通过神经符号学习方式，在8.7万+开放领域视频上基于视频-字幕自监督训练，无需人工标注场景图；SGClip可灵活用于prompt推理和特定任务微调，提升场景图生成与动作定位任务表现。

Result: SGClip在场景图生成和动作定位基准上表现优异，结合ESCA后，不论是开源还是商业MLLM模型，在两个智能体环境上均创下新纪录，有效降低了感知错误率，开源模型超越了专有模型表现。

Conclusion: ESCA和SGClip有效提升了多模态大模型的空间-时序理解和感知能力，实现了结构化对齐，为通用智能体发展提供了更强支撑，在行业和学术领域均具推广价值。

Abstract: Multi-modal large language models (MLLMs) are making rapid progress toward
general-purpose embodied agents. However, current training pipelines primarily
rely on high-level vision-sound-text pairs and lack fine-grained, structured
alignment between pixel-level visual content and textual semantics. To overcome
this challenge, we propose ESCA, a new framework for contextualizing embodied
agents through structured spatial-temporal understanding. At its core is
SGClip, a novel CLIP-based, open-domain, and promptable model for generating
scene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic
learning pipeline, which harnesses model-driven self-supervision from
video-caption pairs and structured reasoning, thereby eliminating the need for
human-labeled scene graph annotations. We demonstrate that SGClip supports both
prompt-based inference and task-specific fine-tuning, excelling in scene graph
generation and action localization benchmarks. ESCA with SGClip consistently
improves both open-source and commercial MLLMs, achieving state-of-the-art
performance across two embodied environments. Notably, it significantly reduces
agent perception errors and enables open-source models to surpass proprietary
baselines.

</details>


### [2] [CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection](https://arxiv.org/abs/2510.15991)
*Huiming Yang*

Main category: cs.CV

TL;DR: 本论文提出了一种新的稀疏跨模态探测器CrossRay3D，通过引入稀疏选择器（Sparse Selector）和光线感知监督（Ray-Aware Supervision）等新结构，有效提升了检测能力并大幅降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏跨模态探测器虽然计算高效，但忽视了token表示的质量，导致前景表示能力弱和整体检测性能受限。因此，提升token的表达质量成为亟需解决的问题。

Method: 作者提出稀疏选择器（SS），其核心包括“光线感知监督（Ray-Aware Supervision）”和“类别平衡监督（Class-Balanced Supervision）”，分别用于保留丰富的几何信息和自适应平衡类别语义。此外，设计了针对LiDAR与图像分布差异的'Ray Positional Encoding'，并将上述创新整合到端到端的CrossRay3D框架中。

Result: 在nuScenes基准上，CrossRay3D取得了72.4的mAP和74.7的NDS，性能优于现有方法，并且速度提高1.84倍。此外，无论部分还是全部模态缺失时，模型表现出较强的鲁棒性。

Conclusion: CrossRay3D通过优化token表示和引入多种监督机制，有效提升了稀疏跨模态探测的表现，并兼顾高效性与强鲁棒性，为实际应用带来潜在价值。

Abstract: The sparse cross-modality detector offers more advantages than its
counterpart, the Bird's-Eye-View (BEV) detector, particularly in terms of
adaptability for downstream tasks and computational cost savings. However,
existing sparse detectors overlook the quality of token representation, leaving
it with a sub-optimal foreground quality and limited performance. In this
paper, we identify that the geometric structure preserved and the class
distribution are the key to improving the performance of the sparse detector,
and propose a Sparse Selector (SS). The core module of SS is Ray-Aware
Supervision (RAS), which preserves rich geometric information during the
training stage, and Class-Balanced Supervision, which adaptively reweights the
salience of class semantics, ensuring that tokens associated with small objects
are retained during token sampling. Thereby, outperforming other sparse
multi-modal detectors in the representation of tokens. Additionally, we design
Ray Positional Encoding (Ray PE) to address the distribution differences
between the LiDAR modality and the image. Finally, we integrate the
aforementioned module into an end-to-end sparse multi-modality detector, dubbed
CrossRay3D. Experiments show that, on the challenging nuScenes benchmark,
CrossRay3D achieves state-of-the-art performance with 72.4 mAP and 74.7 NDS,
while running 1.84 faster than other leading methods. Moreover, CrossRay3D
demonstrates strong robustness even in scenarios where LiDAR or camera data are
partially or entirely missing.

</details>


### [3] [InfraGPT Smart Infrastructure: An End-to-End VLM-Based Framework for Detecting and Managing Urban Defects](https://arxiv.org/abs/2510.16017)
*Ibrahim Sheikh Mohamed,Abdullah Yahya Abdullah Omaisan*

Main category: cs.CV

TL;DR: 本论文提出了一个综合管道，结合YOLO目标检测器和视觉语言模型，从城市CCTV视频中检测多种基础设施缺陷，并生成结构化维护方案。


<details>
  <summary>Details</summary>
Motivation: 基础设施如道路、桥梁和隧道经常出现裂缝、坑洞与泄漏，这些不仅影响公共安全，还需要及时修复，而人工巡查成本高且风险大。现有自动化方法要么只检测单一类型缺陷，要么输出信息不够结构化，难以直接指导维护工作。

Method: 构建端到端管道：第一步用YOLO系列目标检测器实时检测CCTV视频流中的多种城市基础设施缺陷，第二步将检测结果传递给先进的视觉语言模型（如QwenVL和LLaVA），由其生成包含事件描述、所需工具、尺寸、维修建议和紧急警报等的JSON结构化行动方案。

Result: 在公开数据集和实际CCTV视频片段上的实验显示，系统能够准确识别多种缺陷，并且自动生成内容清晰、可操作的维护总结。

Conclusion: 该系统能够提升自动化巡检和维护决策的效率和安全性。作者最后讨论了当前挑战及将系统推广到全市范围的展望。

Abstract: Infrastructure in smart cities is increasingly monitored by networks of
closed circuit television (CCTV) cameras. Roads, bridges and tunnels develop
cracks, potholes, and fluid leaks that threaten public safety and require
timely repair. Manual inspection is costly and hazardous, and existing
automatic systems typically address individual defect types or provide
unstructured outputs that cannot directly guide maintenance crews. This paper
proposes a comprehensive pipeline that leverages street CCTV streams for multi
defect detection and segmentation using the YOLO family of object detectors and
passes the detections to a vision language model (VLM) for scene aware
summarization. The VLM generates a structured action plan in JSON format that
includes incident descriptions, recommended tools, dimensions, repair plans,
and urgent alerts. We review literature on pothole, crack and leak detection,
highlight recent advances in large vision language models such as QwenVL and
LLaVA, and describe the design of our early prototype. Experimental evaluation
on public datasets and captured CCTV clips demonstrates that the system
accurately identifies diverse defects and produces coherent summaries. We
conclude by discussing challenges and directions for scaling the system to city
wide deployments.

</details>


### [4] [IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection](https://arxiv.org/abs/2510.16036)
*Zewen Li,Zitong Yu,Qilang Ye,Weicheng Xie,Wei Zhuo,Linlin Shen*

Main category: cs.CV

TL;DR: 论文提出了一种结合多模态大模型与异常检测的新方法IAD-GPT，实现了工业异常检测和详细多轮对话解释，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的工业异常检测方法无法支持多轮人机对话，以及对异常细节（如颜色、形状、类型）进行丰富描述。同时，尽管大模型具备强大潜力，在异常检测领域尚未有效发挥其能力。

Method: 提出IAD-GPT，融合大语言模型的文本语义与图像的全局与像素信息。1) 利用APG模块生成针对目标的详细异常提示词；2) 通过这些提示词激活CLIP等预训练视觉-语言模型的检测与分割能力；3) 设计Text-Guided Enhancer模块，使图像特征与文本提示交互，动态选择增强路径，提升大模型对视觉数据内异常关注的精确性；4) 引入Multi-Mask Fusion模块，将专家级分割掩膜融入，强化像素级异常感知。

Result: 在MVTec-AD和VisA等公开数据集上，IAD-GPT在自监督和小样本异常检测与分割任务上取得SOTA表现。

Conclusion: IAD-GPT有效释放了多模态大模型在工业异常检测场景下的潜力，不仅提升了检测性能，还实现了细粒度和互动性更强的异常解读，为后续相关研究提供了新范式。

Abstract: The robust causal capability of Multimodal Large Language Models (MLLMs) hold
the potential of detecting defective objects in Industrial Anomaly Detection
(IAD). However, most traditional IAD methods lack the ability to provide
multi-turn human-machine dialogues and detailed descriptions, such as the color
of objects, the shape of an anomaly, or specific types of anomalies. At the
same time, methods based on large pre-trained models have not fully stimulated
the ability of large models in anomaly detection tasks. In this paper, we
explore the combination of rich text semantics with both image-level and
pixel-level information from images and propose IAD-GPT, a novel paradigm based
on MLLMs for IAD. We employ Abnormal Prompt Generator (APG) to generate
detailed anomaly prompts for specific objects. These specific prompts from the
large language model (LLM) are used to activate the detection and segmentation
functions of the pre-trained visual-language model (i.e., CLIP). To enhance the
visual grounding ability of MLLMs, we propose Text-Guided Enhancer, wherein
image features interact with normal and abnormal text prompts to dynamically
select enhancement pathways, which enables language models to focus on specific
aspects of visual data, enhancing their ability to accurately interpret and
respond to anomalies within images. Moreover, we design a Multi-Mask Fusion
module to incorporate mask as expert knowledge, which enhances the LLM's
perception of pixel-level anomalies. Extensive experiments on MVTec-AD and VisA
datasets demonstrate our state-of-the-art performance on self-supervised and
few-shot anomaly detection and segmentation tasks, such as MVTec-AD and VisA
datasets. The codes are available at
\href{https://github.com/LiZeWen1225/IAD-GPT}{https://github.com/LiZeWen1225/IAD-GPT}.

</details>


### [5] [Effect of Reporting Mode and Clinical Experience on Radiologists' Gaze and Image Analysis Behavior in Chest Radiography](https://arxiv.org/abs/2510.16070)
*Mahta Khoobi,Marc Sebastian von der Stueck,Felix Barajas Ordonez,Anca-Maria Iancu,Eric Corban,Julia Nowak,Aleksandar Kargaliev,Valeria Perelygina,Anna-Sophie Schott,Daniel Pinto dos Santos,Christiane Kuhl,Daniel Truhn,Sven Nebelung,Robert Siepmann*

Main category: cs.CV

TL;DR: 本文比较了传统自由文本、结构化报告和AI辅助结构化报告三种放射报告方式对诊断准确率、效率和用户体验等的影响，发现AI辅助结构化报告在各项指标上表现最优。


<details>
  <summary>Details</summary>
Motivation: 随着结构化报告(SR)和人工智能(AI)的发展，放射科医师如何与影像学检查互动可能发生变革。作者希望评估不同报告方式对诊断行为、准确率、效率和体验的影响，以指导未来临床实践提升诊断质量和效率。

Method: 前瞻性研究，包含4名初学者和4名非初学者，分别用自由文本(FT)、结构化报告(SR)、AI辅助结构化报告(AI-SR)三种模式分析胸部X线片，配合眼动追踪仪。评估诊断准确率（用Cohen's κ与专家共识对比）、报告时间、眼动指标和用户体验问卷，使用多元统计分析。

Result: AI-SR组诊断准确率最高(κ=0.71)，FT和SR接近(κ分别为0.58、0.60)；AI-SR报告时间最短(25s)，SR次之(37s)，FT最长(88s)。结构化报告和AI-SR可减少不必要的凝视与视线跳动。AI-SR为用户首选。

Conclusion: 结构化报告可提升分析效率与引导视线关注图像，AI辅助结构化报告在兼具效率、准确率和用户满意度改善方面优于其他模式，建议推广应用。

Abstract: Structured reporting (SR) and artificial intelligence (AI) may transform how
radiologists interact with imaging studies. This prospective study (July to
December 2024) evaluated the impact of three reporting modes: free-text (FT),
structured reporting (SR), and AI-assisted structured reporting (AI-SR), on
image analysis behavior, diagnostic accuracy, efficiency, and user experience.
Four novice and four non-novice readers (radiologists and medical students)
each analyzed 35 bedside chest radiographs per session using a customized
viewer and an eye-tracking system. Outcomes included diagnostic accuracy
(compared with expert consensus using Cohen's $\kappa$), reporting time per
radiograph, eye-tracking metrics, and questionnaire-based user experience.
Statistical analysis used generalized linear mixed models with Bonferroni
post-hoc tests with a significance level of ($P \le .01$). Diagnostic accuracy
was similar in FT ($\kappa = 0.58$) and SR ($\kappa = 0.60$) but higher in
AI-SR ($\kappa = 0.71$, $P < .001$). Reporting times decreased from $88 \pm 38$
s (FT) to $37 \pm 18$ s (SR) and $25 \pm 9$ s (AI-SR) ($P < .001$). Saccade
counts for the radiograph field ($205 \pm 135$ (FT), $123 \pm 88$ (SR), $97 \pm
58$ (AI-SR)) and total fixation duration for the report field ($11 \pm 5$ s
(FT), $5 \pm 3$ s (SR), $4 \pm 1$ s (AI-SR)) were lower with SR and AI-SR ($P <
.001$ each). Novice readers shifted gaze towards the radiograph in SR, while
non-novice readers maintained their focus on the radiograph. AI-SR was the
preferred mode. In conclusion, SR improves efficiency by guiding visual
attention toward the image, and AI-prefilled SR further enhances diagnostic
accuracy and user satisfaction.

</details>


### [6] [Data-Driven Analysis of Intersectional Bias in Image Classification: A Framework with Bias-Weighted Augmentation](https://arxiv.org/abs/2510.16072)
*Farjana Yesmin*

Main category: cs.CV

TL;DR: 本文提出了一种用于分析和缓解图像分类模型中交互性偏见的新框架IFEF，并通过BWA增强方法有效提升了模型对弱势群体的公平性与准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习模型在样本不均衡的数据集上容易出现交互性偏见，即模型在多属性（如对象类别和环境条件）同时存在下产生的系统性误差，影响公平性与性能。

Method: 1. 提出Intersectional Fairness Evaluation Framework（IFEF），结合定量公平性指标和可解释性工具，用于系统性分析模型预测中的偏见模式。
2. 基于分析结果，提出Bias-Weighted Augmentation（BWA）数据增强策略，根据子群体的分布统计自适应调整增强强度，以实现数据的平衡。

Result: 在Open Images V7数据集五个目标类别上实验证明：BWA能将弱势类别-环境组合的准确率提升多达24个百分点，同时公平性指标差距减少35%；多次独立实验统计分析，结果具有显著性（p<0.05）。

Conclusion: 该方法为分析和解决图像分类系统中的交互性偏见提供了可复现的有效途径，为模型的公平性和实用性带来提升。

Abstract: Machine learning models trained on imbalanced datasets often exhibit
intersectional biases-systematic errors arising from the interaction of
multiple attributes such as object class and environmental conditions. This
paper presents a data-driven framework for analyzing and mitigating such biases
in image classification. We introduce the Intersectional Fairness Evaluation
Framework (IFEF), which combines quantitative fairness metrics with
interpretability tools to systematically identify bias patterns in model
predictions. Building on this analysis, we propose Bias-Weighted Augmentation
(BWA), a novel data augmentation strategy that adapts transformation
intensities based on subgroup distribution statistics. Experiments on the Open
Images V7 dataset with five object classes demonstrate that BWA improves
accuracy for underrepresented class-environment intersections by up to 24
percentage points while reducing fairness metric disparities by 35%.
Statistical analysis across multiple independent runs confirms the significance
of improvements (p < 0.05). Our methodology provides a replicable approach for
analyzing and addressing intersectional biases in image classification systems.

</details>


### [7] [Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch](https://arxiv.org/abs/2510.16088)
*Zia Badar*

Main category: cs.CV

TL;DR: 本文提出了一种可微分的神经网络量化方法，实现了位移/对数量化，同时兼顾权重量化与激活量化，且精度接近全精度网络，并在ImageNet和ResNet18上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 目前的神经网络量化方法大都不可微分，影响训练效果与收敛性，且在对数/位移量化中，更少关注激活量化或者精度较低，难以支持多比特量化。本文旨在提出一种可微、理论上可收敛，并支持n比特的高效量化方法。

Method: 作者提出了一种可微的量化方法，并提供了收敛到最优神经网络的理论证明。该方法支持权重与激活的对数（shift/logarithmic）量化，并能扩展到n比特。训练时直接采用可微分的量化函数，无需手动设置反向传播导数。

Result: 在ImageNet数据集和ResNet18模型上，仅对权重量化时，精度损失小于1%；同时支持权重和激活量化，精度能与SOTA方法媲美，训练收敛速度快，仅需15轮(epoch)；推理成本略高于单比特量化（主要为CPU指令增多），且无需更高精度的乘法。

Conclusion: 本文提出的方法不仅解决了现有不可微分量化方法的缺陷，还实现了灵活的多比特位移量化，在保持高推理效率的同时，能在多种神经网络场景下实现几乎无精度损失的高效训练和推理。

Abstract: Quantization of neural networks provides benefits of inference in less
compute and memory requirements. Previous work in quantization lack two
important aspects which this work provides. First almost all previous work in
quantization used a non-differentiable approach and for learning; the
derivative is usually set manually in backpropogation which make the learning
ability of algorithm questionable, our approach is not just differentiable, we
also provide proof of convergence of our approach to the optimal neural
network. Second previous work in shift/logrithmic quantization either have
avoided activation quantization along with weight quantization or achieved less
accuracy. Learning logrithmic quantize values of form $2^n$ requires the
quantization function can scale to more than 1 bit quantization which is
another benifit of our quantization that it provides $n$ bits quantization as
well. Our approach when tested with image classification task using imagenet
dataset, resnet18 and weight quantization only achieves less than 1 percent
accuracy compared to full precision accuracy while taking only 15 epochs to
train using shift bit quantization and achieves comparable to SOTA approaches
accuracy in both weight and activation quantization using shift bit
quantization in 15 training epochs with slightly higher(only higher cpu
instructions) inference cost compared to 1 bit quantization(without logrithmic
quantization) and not requiring any higher precision multiplication.

</details>


### [8] [StripRFNet: A Strip Receptive Field and Shape-Aware Network for Road Damage Detection](https://arxiv.org/abs/2510.16115)
*Jianhan Lin,Yuchu Qin,Shuai Gao,Yikang Rui,Jie Liu,Yanjie Lv*

Main category: cs.CV

TL;DR: 本文提出了一种新型深度神经网络StripRFNet，实现了对道路表面损伤（如裂缝）的高精度、实时检测。


<details>
  <summary>Details</summary>
Motivation: 道路表面损伤威胁交通安全并阻碍可持续城市发展。现有检测方法难以准确识别形状多变、窄长或小尺度的损伤。为实现SDG 11目标，需要更精确、高效的方法。

Method: StripRFNet网络包括三个模块：（1）形状感知模块（SPM）：利用大可分离核注意力机制，在多尺度特征聚合中提升形状判别能力；（2）条带感受野模块（SRFM）：采用大条带卷积和池化，捕捉细长裂缝特征；（3）小尺度增强模块（SSEM）：结合高分辨率特征图、专用检测头和动态上采样，优化小型损伤识别。

Result: 在RDD2022基准测试中，StripRFNet优于现有方法。在中国子集上，F1-score、mAP50和mAP50:95分别比基线提升了4.4、2.9和3.4个百分点。全数据集上，F1-score达到了80.33%，超越CRDDC'2022参赛结果，并保持了较快的推理速度。

Conclusion: StripRFNet在准确率和实时性方面均达到当前最优水平，为智慧道路养护与可持续基础设施管理提供了有前景的新工具。

Abstract: Well-maintained road networks are crucial for achieving Sustainable
Development Goal (SDG) 11. Road surface damage not only threatens traffic
safety but also hinders sustainable urban development. Accurate detection,
however, remains challenging due to the diverse shapes of damages, the
difficulty of capturing slender cracks with high aspect ratios, and the high
error rates in small-scale damage recognition. To address these issues, we
propose StripRFNet, a novel deep neural network comprising three modules: (1) a
Shape Perception Module (SPM) that enhances shape discrimination via large
separable kernel attention (LSKA) in multi-scale feature aggregation; (2) a
Strip Receptive Field Module (SRFM) that employs large strip convolutions and
pooling to capture features of slender cracks; and (3) a Small-Scale
Enhancement Module (SSEM) that leverages a high-resolution P2 feature map, a
dedicated detection head, and dynamic upsampling to improve small-object
detection. Experiments on the RDD2022 benchmark show that StripRFNet surpasses
existing methods. On the Chinese subset, it improves F1-score, mAP50, and
mAP50:95 by 4.4, 2.9, and 3.4 percentage points over the baseline,
respectively. On the full dataset, it achieves the highest F1-score of 80.33%
compared with CRDDC'2022 participants and ORDDC'2024 Phase 2 results, while
maintaining competitive inference speed. These results demonstrate that
StripRFNet achieves state-of-the-art accuracy and real-time efficiency,
offering a promising tool for intelligent road maintenance and sustainable
infrastructure management.

</details>


### [9] [ObjectTransforms for Uncertainty Quantification and Reduction in Vision-Based Perception for Autonomous Vehicles](https://arxiv.org/abs/2510.16118)
*Nishad Sahu,Shounak Sural,Aditya Satish Patil,Ragunathan,Rajkumar*

Main category: cs.CV

TL;DR: 本文提出ObjectTransforms方法，通过对目标对象进行特定变换，有效提升自动驾驶视觉目标检测的鲁棒性并量化预测不确定性，显著改善YOLOv8在NuImages数据集上的检测精度。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶场景要求感知系统具备高度可靠性，但当前基于视觉的目标检测网络易受数据偏置和分布变化影响，存在较大不确定性，这会危及下游决策的安全性。因此，如何提升检测鲁棒性与准确、不确定性量化成为亟需解决的问题。

Method: ObjectTransforms方法在训练阶段对目标对象进行颜色空间扰动，提高模型对光照和颜色变化的适应性，并利用扩散模型生成真实多样的行人数据增强类别分布；在推理阶段，对检测到的对象实施扰动，并以检测分数的方差实时量化预测不确定性，据此过滤假阳性和恢复假阴性，从而优化整体精度-召回曲线。

Result: 在YOLOv8和NuImages 10K数据集实验表明，ObjectTransforms能在所有目标类别上提升检测准确率和减少不确定性。此外，于推理时对假阳性给出更高不确定性，实现有效过滤，提升感知可靠性。

Conclusion: ObjectTransforms是一种高效且轻量级的方法，既能在训练中提升检测鲁棒性，也能在推理时准确量化及降低不确定性，对提升自动驾驶感知系统的可靠性具有重要应用前景。

Abstract: Reliable perception is fundamental for safety critical decision making in
autonomous driving. Yet, vision based object detector neural networks remain
vulnerable to uncertainty arising from issues such as data bias and
distributional shifts. In this paper, we introduce ObjectTransforms, a
technique for quantifying and reducing uncertainty in vision based object
detection through object specific transformations at both training and
inference times. At training time, ObjectTransforms perform color space
perturbations on individual objects, improving robustness to lighting and color
variations. ObjectTransforms also uses diffusion models to generate realistic,
diverse pedestrian instances. At inference time, object perturbations are
applied to detected objects and the variance of detection scores are used to
quantify predictive uncertainty in real time. This uncertainty signal is then
used to filter out false positives and also recover false negatives, improving
the overall precision recall curve. Experiments with YOLOv8 on the NuImages 10K
dataset demonstrate that our method yields notable accuracy improvements and
uncertainty reduction across all object classes during training, while
predicting desirably higher uncertainty values for false positives as compared
to true positives during inference. Our results highlight the potential of
ObjectTransforms as a lightweight yet effective mechanism for reducing and
quantifying uncertainty in vision-based perception during training and
inference respectively.

</details>


### [10] [Aria Gen 2 Pilot Dataset](https://arxiv.org/abs/2510.16134)
*Chen Kong,James Fort,Aria Kang,Jonathan Wittmer,Simon Green,Tianwei Shen,Yipu Zhao,Cheng Peng,Gustavo Solaira,Andrew Berkovich,Nikhil Raina,Vijay Baiyya,Evgeniy Oleinik,Eric Huang,Fan Zhang,Julian Straub,Mark Schwesinger,Luis Pesqueira,Xiaqing Pan,Jakob Julian Engel,Carl Ren,Mingfei Yan,Richard Newcombe*

Main category: cs.CV

TL;DR: A2PD是一个利用最先进的眼镜采集的第一视角多模态公开数据集，包含丰富的原始传感器数据和感知算法输出，覆盖五种日常情景，支持多条件、多人实验，并配套开放工具。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏高质量、多模态、第一视角下的公开数据集，不利于增强可穿戴设备和机器感知研究的深入发展。因此A2PD旨在填补这一空白，为社区提供高质量数据和工具。

Method: 研究团队使用Aria Gen 2智能眼镜，邀请主被试及其朋友在清洁、烹饪、进食、游戏和户外步行五类场景下记录日常活动，收集原始传感器数据及多种机器感知算法的输出，并通过分阶段持续发布数据。

Result: A2PD初次发布已囊括五大生活场景的完整数据，展现了设备在使用者自我、环境以及人机环境交互等多方面高性能感知能力，也体现了在不同用户和情境下的稳定表现。

Conclusion: A2PD作为首批同类第一视角多模态公开数据集，不仅有助于推进机器感知与可穿戴设备领域研究，还通过配套开源工具大大降低了研究门槛，推动相关技术应用和创新。

Abstract: The Aria Gen 2 Pilot Dataset (A2PD) is an egocentric multimodal open dataset
captured using the state-of-the-art Aria Gen 2 glasses. To facilitate timely
access, A2PD is released incrementally with ongoing dataset enhancements. The
initial release features Dia'ane, our primary subject, who records her daily
activities alongside friends, each equipped with Aria Gen 2 glasses. It
encompasses five primary scenarios: cleaning, cooking, eating, playing, and
outdoor walking. In each of the scenarios, we provide comprehensive raw sensor
data and output data from various machine perception algorithms. These data
illustrate the device's ability to perceive the wearer, the surrounding
environment, and interactions between the wearer and the environment, while
maintaining robust performance across diverse users and conditions. The A2PD is
publicly available at projectaria.com, with open-source tools and usage
examples provided in Project Aria Tools.

</details>


### [11] [GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer](https://arxiv.org/abs/2510.16136)
*Sayan Deb Sarkar,Sinisa Stekovic,Vincent Lepetit,Iro Armeni*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，实现了不同外观对象（如图片或文本）的3D资产外观迁移，特别在几何差异较大的情况下优于现有技术。通过引入通用引导与差异化损失函数，有效提升了纹理和几何细节迁移的质量。


<details>
  <summary>Details</summary>
Motivation: 现有将图片或文本外观迁移到3D资产的方法在输入与外观对象几何差异大时表现不佳。直接用3D生成模型效果不理想，因此亟需一种能在极端几何差异条件下依然有效的外观迁移方法。

Method: 方法基于已训练的rectified flow模型，无需重新训练；在采样过程中周期性加入引导，引导通过可微分损失函数实现，包括局部外观感知损失和自相似损失，并在外观对象为图片或文本时均可应用。与此同时，为评测迁移质量，采用基于GPT的系统进行自动评价，以克服传统指标在细节和相异输入比较方面的不足。

Result: 实验证明，该方法能够更好地将纹理和几何细节迁移到目标3D资产，在主观和客观质控上均显著领先于现有基线方法。基于GPT的评测结果与用户调研高度吻合，验证了方法的可行性和评价体系的可靠性。

Conclusion: 本文提出的训练无关、引导增强的3D外观迁移方法，突破了现有技术在结构不匹配时的限制，具有很强的通用性，能扩展至更广泛的扩散模型和引导策略，在数字内容、游戏和AR领域具有广泛应用前景。

Abstract: Transferring appearance to 3D assets using different representations of the
appearance object - such as images or text - has garnered interest due to its
wide range of applications in industries like gaming, augmented reality, and
digital content creation. However, state-of-the-art methods still fail when the
geometry between the input and appearance objects is significantly different. A
straightforward approach is to directly apply a 3D generative model, but we
show that this ultimately fails to produce appealing results. Instead, we
propose a principled approach inspired by universal guidance. Given a
pretrained rectified flow model conditioned on image or text, our training-free
method interacts with the sampling process by periodically adding guidance.
This guidance can be modeled as a differentiable loss function, and we
experiment with two different types of guidance including part-aware losses for
appearance and self-similarity. Our experiments show that our approach
successfully transfers texture and geometric details to the input 3D asset,
outperforming baselines both qualitatively and quantitatively. We also show
that traditional metrics are not suitable for evaluating the task due to their
inability of focusing on local details and comparing dissimilar inputs, in
absence of ground truth data. We thus evaluate appearance transfer quality with
a GPT-based system objectively ranking outputs, ensuring robust and human-like
assessment, as further confirmed by our user study. Beyond showcased scenarios,
our method is general and could be extended to different types of diffusion
models and guidance functions.

</details>


### [12] [C-arm Guidance: A Self-supervised Approach To Automated Positioning During Stroke Thrombectomy](https://arxiv.org/abs/2510.16145)
*Ahmad Arrabi,Jay hwasung Jung,J Le,A Nguyen,J Reed,E Stahl,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: 本文提出用自监督深度学习方法，自动化血栓切除术中的关键步骤，提升效率和安全性。实验显示新方法在回归和分类任务上优于已有方法。


<details>
  <summary>Details</summary>
Motivation: 目前缺血性中风的血栓切除术虽然效果好，但对资源和人员要求极高，限制了其普及和高效利用。因此，研究希望通过AI技术自动化操作，减少人力依赖、提高手术效率和安全性。

Method: 设计了一种自监督学习框架，通过回归预训练任务对骨性标记点进行分类。模型先通过回归任务学习特征，然后转向下游分类任务，并用实验与现有方法进行了对比。

Result: 新模型在骨性标记点的回归与分类任务上均取得了比现有方法更好的表现。采用位置相关的预训练任务后，下游分类表现提升明显。

Conclusion: 自监督学习和回归预训练任务可以有效提升医疗影像自动识别的准确性和效率。未来将继续开发该框架，实现自动化C臂控制，助力中风血栓切除术流程优化。

Abstract: Thrombectomy is one of the most effective treatments for ischemic stroke, but
it is resource and personnel-intensive. We propose employing deep learning to
automate critical aspects of thrombectomy, thereby enhancing efficiency and
safety. In this work, we introduce a self-supervised framework that classifies
various skeletal landmarks using a regression-based pretext task. Our
experiments demonstrate that our model outperforms existing methods in both
regression and classification tasks. Notably, our results indicate that the
positional pretext task significantly enhances downstream classification
performance. Future work will focus on extending this framework toward fully
autonomous C-arm control, aiming to optimize trajectories from the pelvis to
the head during stroke thrombectomy procedures. All code used is available at
https://github.com/AhmadArrabi/C_arm_guidance

</details>


### [13] [DuetMatch: Harmonizing Semi-Supervised Brain MRI Segmentation via Decoupled Branch Optimization](https://arxiv.org/abs/2510.16146)
*Thanh-Huy Nguyen,Hoang-Thien Nguyen,Vi Vu,Ba-Thinh Lam,Phat Huynh,Tianyang Wang,Xingjian Li,Ulas Bagci,Min Xu*

Main category: cs.CV

TL;DR: 提出了一种新型半监督医学图像分割方法DuetMatch，通过异步优化和多种增强机制，有效提升了分割性能，在多个基准数据集上超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 医学图像注释数据稀缺，半监督学习可以充分利用未标注数据，但现有teacher-student框架在复杂情况下优化困难，影响模型收敛和稳定性，因此需要更有效的半监督方法。

Method: 引入双分支异步优化框架DuetMatch，一个分支优化编码器，另一个优化解码器，分别冻结未优化部分。同时设计了解耦Dropout扰动、成对CutMix交叉引导以及一致性匹配机制，以提升分支间一致性，增强模型多样性并减轻伪标签噪声带来的偏差。

Result: 在ISLES2022和BraTS等脑MRI分割基准数据集上，DuetMatch在多种评估下均优于最新半监督分割方法，效果稳健、泛化强。

Conclusion: DuetMatch通过全新的异步优化和多重正则、增强手段，有效解决了现有半监督方法的稳定性与收敛问题，是医学图像分割领域值得关注的进展。

Abstract: The limited availability of annotated data in medical imaging makes
semi-supervised learning increasingly appealing for its ability to learn from
imperfect supervision. Recently, teacher-student frameworks have gained
popularity for their training benefits and robust performance. However, jointly
optimizing the entire network can hinder convergence and stability, especially
in challenging scenarios. To address this for medical image segmentation, we
propose DuetMatch, a novel dual-branch semi-supervised framework with
asynchronous optimization, where each branch optimizes either the encoder or
decoder while keeping the other frozen. To improve consistency under noisy
conditions, we introduce Decoupled Dropout Perturbation, enforcing
regularization across branches. We also design Pair-wise CutMix Cross-Guidance
to enhance model diversity by exchanging pseudo-labels through augmented input
pairs. To mitigate confirmation bias from noisy pseudo-labels, we propose
Consistency Matching, refining labels using stable predictions from frozen
teacher models. Extensive experiments on benchmark brain MRI segmentation
datasets, including ISLES2022 and BraTS, show that DuetMatch consistently
outperforms state-of-the-art methods, demonstrating its effectiveness and
robustness across diverse semi-supervised segmentation scenarios.

</details>


### [14] [Automated C-Arm Positioning via Conformal Landmark Localization](https://arxiv.org/abs/2510.16160)
*Ahmad Arrabi,Jay Hwasung Jung,Jax Luo,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: 本文提出了一种利用X射线图像自动导航C形臂到预定解剖位置的算法，能够预测三维位移矢量并评估不确定性，实现高精度、安全可靠的C形臂自动定位。


<details>
  <summary>Details</summary>
Motivation: C形臂的精确定位对于透视引导手术至关重要，目前的手术流程主要依赖手动对齐，导致辐射暴露增加和手术延误。因此需要一种自动、可靠的C形臂定位方法以改善安全性和效率。

Method: 作者设计了一个从任意初始位置输入X射线图像，预测到目标解剖标志点三维位移的方法。模型不仅捕捉了隶属不确定性（aleatoric）和模型不确定性（epistemic），还通过conformal prediction校准其预测。同时结合概率损失和骨骼姿态正则化，提升解剖合理性。在DeepDRR生成的合成X射线数据集上进行了实验验证。

Result: 在多个网络结构上，该方法均展示了强大的定位精度和良好的预测置信区间校准能力。

Conclusion: 该方法能够为C形臂自动导航提供高精度且安全可靠的解决方案，有望应用于自主C形臂系统，保障手术安全与效率。

Abstract: Accurate and reliable C-arm positioning is essential for fluoroscopy-guided
interventions. However, clinical workflows rely on manual alignment that
increases radiation exposure and procedural delays. In this work, we present a
pipeline that autonomously navigates the C-arm to predefined anatomical
landmarks utilizing X-ray images. Given an input X-ray image from an arbitrary
starting location on the operating table, the model predicts a 3D displacement
vector toward each target landmark along the body. To ensure reliable
deployment, we capture both aleatoric and epistemic uncertainties in the
model's predictions and further calibrate them using conformal prediction. The
derived prediction regions are interpreted as 3D confidence regions around the
predicted landmark locations. The training framework combines a probabilistic
loss with skeletal pose regularization to encourage anatomically plausible
outputs. We validate our approach on a synthetic X-ray dataset generated from
DeepDRR. Results show not only strong localization accuracy across multiple
architectures but also well-calibrated prediction bounds. These findings
highlight the pipeline's potential as a component in safe and reliable
autonomous C-arm systems. Code is available at
https://github.com/AhmadArrabi/C_arm_guidance_APAH

</details>


### [15] [Cost Savings from Automatic Quality Assessment of Generated Images](https://arxiv.org/abs/2510.16179)
*Xavier Giro-i-Nieto,Nefeli Andreou,Anqi Liang,Manel Baradad,Francesc Moreno-Noguer,Aleix Martinez*

Main category: cs.CV

TL;DR: 该论文提出了一种通过自动化图像质量评估（IQA）预筛选，从而降低深度生成模型图像生产成本的方式，并用实际用例验证了节省成本的效果。


<details>
  <summary>Details</summary>
Motivation: 当前深度生成模型虽然图像合成技术取得进步，但尚不能达到传统摄影质量标准，生产流程中需人工筛选高质量生成图像，但这一过程缓慢且成本高昂，因此需要自动化手段提升筛选效率和降低成本。

Method: 作者提出在图像生成流程加入自动化IQA预过滤环节，只将高质量图像送入人工复审，并建立公式估算不同IQA引擎的精度与通过率对整体成本节省的作用。通过AutoML实现了IQA引擎，并针对背景修复应用进行了实际实验。

Result: 通过在背景修复场景中应用AutoML实现的IQA预筛选，实验表明整体人工审核成本节约达51.61%。公式可量化不同IQA引擎的经济影响。

Conclusion: 引入自动IQA环节可以显著降低深度生成图像生产流程中的人工成本，所提出的成本估算公式具有实际价值，为生成式图像应用提供了更高效的生产流程。

Abstract: Deep generative models have shown impressive progress in recent years, making
it possible to produce high quality images with a simple text prompt or a
reference image. However, state of the art technology does not yet meet the
quality standards offered by traditional photographic methods. For this reason,
production pipelines that use generated images often include a manual stage of
image quality assessment (IQA). This process is slow and expensive, especially
because of the low yield of automatically generated images that pass the
quality bar. The IQA workload can be reduced by introducing an automatic
pre-filtering stage, that will increase the overall quality of the images sent
to review and, therefore, reduce the average cost required to obtain a high
quality image. We present a formula that estimates the cost savings depending
on the precision and pass yield of a generic IQA engine. This formula is
applied in a use case of background inpainting, showcasing a significant cost
saving of 51.61% obtained with a simple AutoML solution.

</details>


### [16] [Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI](https://arxiv.org/abs/2510.16196)
*Zheng Huang,Enpei Zhang,Yinghao Cai,Weikang Qiu,Carl Yang,Elynn Chen,Xiang Zhang,Rex Ying,Dawei Zhou,Yujun Yan*

Main category: cs.CV

TL;DR: 本论文提出了一种新的基于结构化文本空间的fMRI视图重建框架PRISM，有效提升了图像重建精度。


<details>
  <summary>Details</summary>
Motivation: 当前将fMRI脑信号用于重建视觉图像主要分为两步：将信号映射到潜在空间，再由生成模型重建图像，但尚不明确哪种潜在空间最有效。作者旨在找出最合适的潜在空间以提升重建效果。

Method: 作者首先对比fMRI信号与语言模型文本空间、视觉空间及联合空间的相似性，发现fMRI信号与文本空间更为相似。提出PRISM模型，将fMRI信号映射到结构化文本空间，通过面向对象的扩散模块逐步组合对象生成图像，辅以属性关系搜索模块自动识别关键属性及关系。

Result: 在真实数据集上，PRISM显著优于其它现有方法，感知损失最多降低8%。

Conclusion: 结构化文本空间是连接fMRI信号与图像重建的有效中间空间，利用其面向对象及属性关系结构能显著提升视觉刺激重建质量。

Abstract: Understanding how the brain encodes visual information is a central challenge
in neuroscience and machine learning. A promising approach is to reconstruct
visual stimuli, essentially images, from functional Magnetic Resonance Imaging
(fMRI) signals. This involves two stages: transforming fMRI signals into a
latent space and then using a pretrained generative model to reconstruct
images. The reconstruction quality depends on how similar the latent space is
to the structure of neural activity and how well the generative model produces
images from that space. Yet, it remains unclear which type of latent space best
supports this transformation and how it should be organized to represent visual
stimuli effectively. We present two key findings. First, fMRI signals are more
similar to the text space of a language model than to either a vision based
space or a joint text image space. Second, text representations and the
generative model should be adapted to capture the compositional nature of
visual stimuli, including objects, their detailed attributes, and
relationships. Building on these insights, we propose PRISM, a model that
Projects fMRI sIgnals into a Structured text space as an interMediate
representation for visual stimuli reconstruction. It includes an object centric
diffusion module that generates images by composing individual objects to
reduce object detection errors, and an attribute relationship search module
that automatically identifies key attributes and relationships that best align
with the neural activity. Extensive experiments on real world datasets
demonstrate that our framework outperforms existing methods, achieving up to an
8% reduction in perceptual loss. These results highlight the importance of
using structured text as the intermediate space to bridge fMRI signals and
image reconstruction.

</details>


### [17] [Data-Centric AI for Tropical Agricultural Mapping: Challenges, Strategies and Scalable Solutions](https://arxiv.org/abs/2510.16207)
*Mateus Pinto da Silva,Sabrina P. L. P. Correa,Hugo N. Oliveira,Ian M. Nunes,Jefersson A. dos Santos*

Main category: cs.CV

TL;DR: 本论文提出一种以数据为中心的AI方法，提高在热带地区利用遥感技术进行农业制图的鲁棒性和可扩展性，并系统评估25种数据处理与提升策略，最后给出9种最适合大规模应用的方法。


<details>
  <summary>Details</summary>
Motivation: 遥感农业制图在热带地区面临高云量、作物周期多样性、建模样本缺乏等问题，现有以模型为中心的方法难以适应实际需求。提升数据质量和挖掘新型数据策略对实际应用具有重要意义。

Method: 论文采用数据为中心的AI视角，重点评估如置信学习、核心集选择、数据增强和主动学习等25种技术在农业遥感映射中的可用性和成熟度，并通过文献与实际案例总结其适配性，最终筛选出9种实践性强的方法，构建适合热带地区的遥感AI建模流程。

Result: 系统梳理了25种数据优化与增强技术在热带遥感农业中的应用情况，评价其可行性、优劣势和落地条件，筛选出9种现阶段最成熟、易实施的数据处理与建模策略。

Conclusion: 论文强调数据质量和管理是提升遥感农业建模效果的关键，建议优先采用数据为中心的方法，以应对热带区域多变环境和信息采集受限的问题。所总结的9种策略为相关项目的落地实施提供了具体可操作的流程。

Abstract: Mapping agriculture in tropical areas through remote sensing presents unique
challenges, including the lack of high-quality annotated data, the elevated
costs of labeling, data variability, and regional generalisation. This paper
advocates a Data-Centric Artificial Intelligence (DCAI) perspective and
pipeline, emphasizing data quality and curation as key drivers for model
robustness and scalability. It reviews and prioritizes techniques such as
confident learning, core-set selection, data augmentation, and active learning.
The paper highlights the readiness and suitability of 25 distinct strategies in
large-scale agricultural mapping pipelines. The tropical context is of high
interest, since high cloudiness, diverse crop calendars, and limited datasets
limit traditional model-centric approaches. This tutorial outlines practical
solutions as a data-centric approach for curating and training AI models better
suited to the dynamic realities of tropical agriculture. Finally, we propose a
practical pipeline using the 9 most mature and straightforward methods that can
be applied to a large-scale tropical agricultural mapping project.

</details>


### [18] [StretchySnake: Flexible SSM Training Unlocks Action Recognition Across Spatio-Temporal Scales](https://arxiv.org/abs/2510.16209)
*Nyle Siddiqui,Rohit Gupta,Sirnam Swetha,Mubarak Shah*

Main category: cs.CV

TL;DR: 本文提出了一种面向视频理解的弹性训练方法，提升了状态空间模型（SSM）对不同空间和时间分辨率视频的适应能力，并显著优于主流Transformer和传统SSM方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视频理解训练方法主要设计针对Transformer，未能充分挖掘SSM的独特优势。在固定分辨率和长度下训练导致现有视频模型面对不同空间与时间尺度时表现下降，即“时空不灵活性”，严重限制实际应用效果。

Method: 作者提出一种新的灵活训练法，通过在训练时对视频采样不同的时空分辨率，同时动态调整模型权重，实现模型能适应任意时空尺度，并提出模型StretchySnake。此外，文中还比较了五种不同的弹性训练策略，并确定了最有效的方式。

Result: 在多个短视频（UCF-101、HMDB-51）与长视频（COIN、Breakfast）数据集上，StretchySnake在准确率上比Transformer和以往SSM方法最高提升达28%；在细粒度动作识别（SSV2、Diving-48）上也表现出很强的适应性。

Conclusion: 提出的训练方法显著提升了视频SSM的鲁棒性、分辨率无关性和泛化能力，为视频动作识别任务提供了有效且可直接应用的训练方案。

Abstract: State space models (SSMs) have emerged as a competitive alternative to
transformers in various tasks. Their linear complexity and hidden-state
recurrence make them particularly attractive for modeling long sequences,
whereas attention becomes quadratically expensive. However, current training
methods for video understanding are tailored towards transformers and fail to
fully leverage the unique attributes of SSMs. For example, video models are
often trained at a fixed resolution and video length to balance the quadratic
scaling of attention cost against performance. Consequently, these models
suffer from degraded performance when evaluated on videos with spatial and
temporal resolutions unseen during training; a property we call spatio-temporal
inflexibility. In the context of action recognition, this severely limits a
model's ability to retain performance across both short- and long-form videos.
Therefore, we propose a flexible training method that leverages and improves
the inherent adaptability of SSMs. Our method samples videos at varying
temporal and spatial resolutions during training and dynamically interpolates
model weights to accommodate any spatio-temporal scale. This instills our SSM,
which we call StretchySnake, with spatio-temporal flexibility and enables it to
seamlessly handle videos ranging from short, fine-grained clips to long,
complex activities. We introduce and compare five different variants of
flexible training, and identify the most effective strategy for video SSMs. On
short-action (UCF-101, HMDB-51) and long-action (COIN, Breakfast) benchmarks,
StretchySnake outperforms transformer and SSM baselines alike by up to 28%,
with strong adaptability to fine-grained actions (SSV2, Diving-48). Therefore,
our method provides a simple drop-in training recipe that makes video SSMs more
robust, resolution-agnostic, and efficient across diverse action recognition
scenarios.

</details>


### [19] [VM-BeautyNet: A Synergistic Ensemble of Vision Transformer and Mamba for Facial Beauty Prediction](https://arxiv.org/abs/2510.16220)
*Djamel Eddine Boukhari*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的异构集成网络VM-BeautyNet，将ViT和基于Mamba的视觉模型结合用于面部美学预测任务，在SCUT-FBP5500数据集上取得了当前最优表现。


<details>
  <summary>Details</summary>
Motivation: 面部美学预测任务复杂且主观，传统CNN难以充分捕捉面部的全局特征，ViT虽能建模全局关系但计算复杂度高，Mamba模型作为最新SSM具备高效建模长距离依赖的优势，因此作者旨在结合ViT的全局特征提取能力与Mamba的高效长序列建模能力，提升预测表现和模型解释性。

Method: 提出VM-BeautyNet，将Vision Transformer作为主干用于建模面部整体结构与对称性，引入Mamba视觉模型作为另一主干高效捕获长序列依赖（如纹理与细节），最终融合双主干的特征用于美学预测，并通过Grad-CAM可视化评估模型可解释性。方法在SCUT-FBP5500标准数据集上进行评测。

Result: VM-BeautyNet在SCUT-FBP5500数据集上取得了Pearson相关性0.9212、平均绝对误差0.2085、均方根误差0.2698的SOTA成绩。通过Grad-CAM可视化验证了双主干在特征提取上的互补性和可解释性提升。

Conclusion: VM-BeautyNet将ViT和Mamba视觉模型的优势结合，有效提升了人脸美学预测的准确性和模型解释性，提出了面向计算美学的新型网络架构范式。

Abstract: Facial Beauty Prediction (FBP) is a complex and challenging computer vision
task, aiming to model the subjective and intricate nature of human aesthetic
perception. While deep learning models, particularly Convolutional Neural
Networks (CNNs), have made significant strides, they often struggle to capture
the global, holistic facial features that are critical to human judgment.
Vision Transformers (ViT) address this by effectively modeling long-range
spatial relationships, but their quadratic complexity can be a bottleneck. This
paper introduces a novel, heterogeneous ensemble architecture,
\textbf{VM-BeautyNet}, that synergistically fuses the complementary strengths
of a Vision Transformer and a Mamba-based Vision model, a recent advancement in
State-Space Models (SSMs). The ViT backbone excels at capturing global facial
structure and symmetry, while the Mamba backbone efficiently models long-range
dependencies with linear complexity, focusing on sequential features and
textures. We evaluate our approach on the benchmark SCUT-FBP5500 dataset. Our
proposed VM-BeautyNet achieves state-of-the-art performance, with a
\textbf{Pearson Correlation (PC) of 0.9212}, a \textbf{Mean Absolute Error
(MAE) of 0.2085}, and a \textbf{Root Mean Square Error (RMSE) of 0.2698}.
Furthermore, through Grad-CAM visualizations, we provide interpretability
analysis that confirms the complementary feature extraction of the two
backbones, offering new insights into the model's decision-making process and
presenting a powerful new architectural paradigm for computational aesthetics.

</details>


### [20] [Designing a Convolutional Neural Network for High-Accuracy Oral Cavity Squamous Cell Carcinoma (OCSCC) Detection](https://arxiv.org/abs/2510.16235)
*Vishal Manikanden,Aniketh Bandlamudi,Daniel Haehn*

Main category: cs.CV

TL;DR: 本文利用卷积神经网络（CNN）结合影像采集硬件，实现早期口腔鳞状细胞癌（OCSCC）的高效检测，并分析了图像分辨率对诊断准确性的影响。


<details>
  <summary>Details</summary>
Motivation: OCSCC早期难以察觉且检测不便，导致高致死率，因此需要便捷且高效的早期检测手段，以降低可预防的死亡。

Method: 研究开发了一个CNN模型，并结合硬件对采集图像进行处理。CNN在4293张良恶性肿瘤和阴性样本的训练集上训练，并在不同分辨率的测试集上进行准确率、召回率和mAP等指标评估。同时对自设计的硬件采集效果进行分析，并开发应用程序支持测试。

Result: CNN在高分辨率图像上的检测准确率更高，但分辨率提升带来的准确率提升呈现递减趋势。硬件增强显著提升了图像质量和检测效果。

Conclusion: 结合CNN和高质量影像采集硬件可以有效提升OCSCC的早期检测能力，但图像分辨率提升对准确率的增益有限，需权衡成本与效果。

Abstract: Oral Cavity Squamous Cell Carcinoma (OCSCC) is the most common type of head
and neck cancer. Due to the subtle nature of its early stages, deep and hidden
areas of development, and slow growth, OCSCC often goes undetected, leading to
preventable deaths. However, properly trained Convolutional Neural Networks
(CNNs), with their precise image segmentation techniques and ability to apply
kernel matrices to modify the RGB values of images for accurate image pattern
recognition, would be an effective means for early detection of OCSCC. Pairing
this neural network with image capturing and processing hardware would allow
increased efficacy in OCSCC detection. The aim of our project is to develop a
Convolutional Neural Network trained to recognize OCSCC, as well as to design a
physical hardware system to capture and process detailed images, in order to
determine the image quality required for accurate predictions. A CNN was
trained on 4293 training images consisting of benign and malignant tumors, as
well as negative samples, and was evaluated for its precision, recall, and Mean
Average Precision (mAP) in its predictions of OCSCC. A testing dataset of
randomly assorted images of cancerous, non-cancerous, and negative images was
chosen, and each image was altered to represent 5 common resolutions. This test
data set was thoroughly analyzed by the CNN and predictions were scored on the
basis of accuracy. The designed enhancement hardware was used to capture
detailed images, and its impact was scored. An application was developed to
facilitate the testing process and bring open access to the CNN. Images of
increasing resolution resulted in higher-accuracy predictions on a logarithmic
scale, demonstrating the diminishing returns of higher pixel counts.

</details>


### [21] [Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset](https://arxiv.org/abs/2510.16258)
*Claire McLean,Makenzie Meendering,Tristan Swartz,Orri Gabbay,Alexandra Olsen,Rachel Jacobs,Nicholas Rosen,Philippe de Bree,Tony Garcia,Gadsden Merrill,Jake Sandakly,Julia Buffalini,Neham Jain,Steven Krenn,Moneish Kumar,Dejan Markovic,Evonne Ng,Fabian Prada,Andrew Saba,Siwei Zhang,Vasu Agrawal,Tim Godisart,Alexander Richard,Michael Zollhoefer*

Main category: cs.CV

TL;DR: Meta的Codec Avatars Lab发布了Embody 3D，这是一个包含439名参与者、总时长500小时、超过5400万帧的3D动作多模态数据集，涵盖了丰富的人体和多人大场景数据。


<details>
  <summary>Details</summary>
Motivation: 当前用于人机交互、虚拟现实、动作识别等领域的3D人体动作数据集往往规模有限、场景单一，缺乏多模态信息，难以支持复杂交互和多人的行为建模，限制了相关技术的发展。

Method: 研究团队在多相机采集场地，采用多传感器同步，采集并追踪了大量参与者的3D动作，包括身体、手部轨迹和身体形状，同时记录文本注释和为每个参与者单独分离的音频轨道。数据涵盖单人动作、手势、行走、多人的情感交流、协作和共同生活场景。

Result: 作者构建了涵盖439人、总共500小时、5400万帧的Embody 3D数据集，数据内容多样且高质量，囊括多模态信息，为后续行为建模、虚拟人等应用提供了丰富的训练和评测资源。

Conclusion: Embody 3D是目前最大规模且最具多样性的3D人体多模态数据集之一，将推动虚拟人、沉浸式交互等多个人工智能与人类行为相关领域的发展。

Abstract: The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of
500 individual hours of 3D motion data from 439 participants collected in a
multi-camera collection stage, amounting to over 54 million frames of tracked
3D motion. The dataset features a wide range of single-person motion data,
including prompted motions, hand gestures, and locomotion; as well as
multi-person behavioral and conversational data like discussions, conversations
in different emotional states, collaborative activities, and co-living
scenarios in an apartment-like space. We provide tracked human motion including
hand tracking and body shape, text annotations, and a separate audio track for
each participant.

</details>


### [22] [Proactive Scene Decomposition and Reconstruction](https://arxiv.org/abs/2510.16272)
*Baicheng Li,Zike Yan,Dong Wu,Hongbin Zha*

Main category: cs.CV

TL;DR: 该论文提出了一种利用人-物交互实现场景动态分解与重建的新方法，不仅能提升动态环境下的建模精度，还能实时地更新地图和对象状态。


<details>
  <summary>Details</summary>
Motivation: 传统的场景重建方法主要关注静态物体，难以应对动态变化及人的行为带来的不确定性。该论文旨在通过利用人类行为与物体互动，提升场景重建的灵活性和准确性。

Method: 论文正式提出了主动场景分解与重建任务，开发了一套在线系统，可通过观察人-物交互来逐步分解和重建环境。系统融合了摄像头和物体位姿估计、实例分解和地图在线更新等功能，用高斯斑点（Gaussian splatting）技术实现高精度、真实感的动态场景建模和高效渲染。

Result: 该方法在多个真实场景下进行了实验，展示了在动态环境中建模的准确性、鲁棒性和实用性，并优于传统的重建方法。

Conclusion: 基于人-物交互的主动场景分解与重建方法，在动态环境下表现出显著优势，对下一代场景理解与建模任务具有现实应用潜力。

Abstract: Human behaviors are the major causes of scene dynamics and inherently contain
rich cues regarding the dynamics. This paper formalizes a new task of proactive
scene decomposition and reconstruction, an online approach that leverages
human-object interactions to iteratively disassemble and reconstruct the
environment. By observing these intentional interactions, we can dynamically
refine the decomposition and reconstruction process, addressing inherent
ambiguities in static object-level reconstruction. The proposed system
effectively integrates multiple tasks in dynamic environments such as accurate
camera and object pose estimation, instance decomposition, and online map
updating, capitalizing on cues from human-object interactions in egocentric
live streams for a flexible, progressive alternative to conventional
object-level reconstruction methods. Aided by the Gaussian splatting technique,
accurate and consistent dynamic scene modeling is achieved with photorealistic
and efficient rendering. The efficacy is validated in multiple real-world
scenarios with promising advantages.

</details>


### [23] [Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models](https://arxiv.org/abs/2510.16290)
*Yue Zheng,Xiufang Shi,Jiming Chen,Yuanchao Shu*

Main category: cs.CV

TL;DR: 提出了一种名为Cerberus的两阶段级联系统，实现高效且准确的实时视频异常检测，结合轻量级筛选与细粒度VLM推理，性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-语言模型的视频异常检测方法虽然零样本能力强，但计算成本高、可视化定位稳定性差，难以实时部署。

Method: Cerberus系统在离线阶段学习正常行为规则，在线阶段先通过轻量过滤选出可疑片段，再利用VLM进行细粒度推理。核心方法包括运动掩模提示（motion mask prompting）聚焦关键区域，以及基于规则检测偏离（rule-based deviation detection），即检测是否偏离学习到的正常范式。

Result: 在四个数据集上获得了平均57.68帧/秒的推理速度，较传统方法提速151.79倍，准确率达97.2%，与最先进方法持平。

Conclusion: Cerberus作为高效且准确的VLM视频异常检测系统，兼具实时性与实用性，适合实际视频分析应用，弥补了现有VLM方法在实时部署上的短板。

Abstract: Video anomaly detection (VAD) has rapidly advanced by recent development of
Vision-Language Models (VLMs). While these models offer superior zero-shot
detection capabilities, their immense computational cost and unstable visual
grounding performance hinder real-time deployment. To overcome these
challenges, we introduce Cerberus, a two-stage cascaded system designed for
efficient yet accurate real-time VAD. Cerberus learns normal behavioral rules
offline, and combines lightweight filtering with fine-grained VLM reasoning
during online inference. The performance gains of Cerberus come from two key
innovations: motion mask prompting and rule-based deviation detection. The
former directs the VLM's attention to regions relevant to motion, while the
latter identifies anomalies as deviations from learned norms rather than
enumerating possible anomalies. Extensive evaluations on four datasets show
that Cerberus on average achieves 57.68 fps on an NVIDIA L40S GPU, a
151.79$\times$ speedup, and 97.2\% accuracy comparable to the state-of-the-art
VLM-based VAD methods, establishing it as a practical solution for real-time
video analytics.

</details>


### [24] [OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models](https://arxiv.org/abs/2510.16295)
*Ryoto Miyamoto,Xin Fan,Fuyuko Kido,Tsuneo Matsumoto,Hayato Yamana*

Main category: cs.CV

TL;DR: OpenLVLM-MIA是一个新的基准数据集，专为评估大规模视觉-语言模型（LVLMs）上的成员推断攻击（MIA）而设计。在控制了分布偏差后，现有MIA方法的表现退化为随机猜测，表明以前结果受偏差影响。该基准为MIA和隐私保护方法研究提供了更可靠的基础。


<details>
  <summary>Details</summary>
Motivation: 此前MIA在LVLMs上的效果高，其实很多源自数据构造时成员和非成员样本分布存在偏差，攻击成功率高未必反映真实能力。作者想消除这种分布偏差，澄清MIA当前真实水平。

Method: 作者构建了一个包含6000张图片的基准数据集，保证成员样本与非成员样本在多个训练阶段中分布均衡，并提供真实的成员标签。对现有MIA方法在此基准上评测。

Result: 在作者设定的无偏条件下，现有主流MIA方法的表现几乎与随机猜测一样，显示此前高成功率多由于分布偏差。

Conclusion: OpenLVLM-MIA为评估MIA方法提供了透明无偏的基准，有助于澄清MIA能力边界并推动更强隐私保护技术的研究。

Abstract: OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in
evaluating membership inference attacks (MIA) against large vision-language
models (LVLMs). While prior work has reported high attack success rates, our
analysis suggests that these results often arise from detecting distributional
bias introduced during dataset construction rather than from identifying true
membership status. To address this issue, we introduce a controlled benchmark
of 6{,}000 images where the distributions of member and non-member samples are
carefully balanced, and ground-truth membership labels are provided across
three distinct training stages. Experiments using OpenLVLM-MIA demonstrated
that the performance of state-of-the-art MIA methods converged to random chance
under unbiased conditions. By offering a transparent and unbiased benchmark,
OpenLVLM-MIA clarifies the current limitations of MIA research on LVLMs and
provides a solid foundation for developing stronger privacy-preserving
techniques.

</details>


### [25] [Stroke2Sketch: Harnessing Stroke Attributes for Training-Free Sketch Generation](https://arxiv.org/abs/2510.16319)
*Rui Yang,Huining Li,Yiyi Long,Xiaojun Wu,Shengfeng He*

Main category: cs.CV

TL;DR: 本文提出了Stroke2Sketch框架，实现了无需训练、可精准转移线条风格的素描生成方法。通过创新的跨图像线条注意力机制和一系列自适应增强策略，极大改善了风格迁移的细致度和内容一致性，超越当前相关方法。


<details>
  <summary>Details</summary>
Motivation: 在基于参考图像生成素描时，需同时满足对线条属性（如粗细、形变、稀疏度）精准迁移和对原始内容/结构的忠实保留。现有方法在此平衡上存在不足。

Method: 提出一种训练无关的新方法Stroke2Sketch，在自注意力机制中嵌入跨图像线条注意力，以实现线条属性的精细匹配和迁移。同时引入自适应对比增强和语义关注机制，增强前景和内容信息的保留。

Result: Stroke2Sketch能够合成出在表达力和风格一致性上优于现有方法的手绘风格素描，实现更准确的线条控制和良好的语义连贯性。

Conclusion: 该方法无需训练，能有效转移复杂线条风格，并保持结构一致性，为风格迁移领域提供了新思路，在素描生成的表达与可控性方面表现突出。

Abstract: Generating sketches guided by reference styles requires precise transfer of
stroke attributes, such as line thickness, deformation, and texture sparsity,
while preserving semantic structure and content fidelity. To this end, we
propose Stroke2Sketch, a novel training-free framework that introduces
cross-image stroke attention, a mechanism embedded within self-attention layers
to establish fine-grained semantic correspondences and enable accurate stroke
attribute transfer. This allows our method to adaptively integrate reference
stroke characteristics into content images while maintaining structural
integrity. Additionally, we develop adaptive contrast enhancement and
semantic-focused attention to reinforce content preservation and foreground
emphasis. Stroke2Sketch effectively synthesizes stylistically faithful sketches
that closely resemble handcrafted results, outperforming existing methods in
expressive stroke control and semantic coherence. Codes are available at
https://github.com/rane7/Stroke2Sketch.

</details>


### [26] [Scaling Laws for Deepfake Detection](https://arxiv.org/abs/2510.16320)
*Wenhao Wang,Longqi Cai,Taihong Xiao,Yuxiao Wang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 本文系统性研究了深度伪造检测任务中的扩展规律（scaling laws），并构建了当前最大规模的相关数据集ScaleDF，揭示了增加数据量对提升检测模型性能的作用。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术不断进化，检测任务面临挑战，当前缺乏大规模、跨域的数据集及对扩展规律的系统研究限制了模型性能的进一步提升，因此有必要系统分析不同数据因素对检测性能的影响。

Method: 构建包含51个真实图像域和102种伪造方法、总计超1460万张图像的大型数据集ScaleDF。通过对比不同真实域数、伪造方法数及训练图片规模，分析检测误差与这些因素之间的关系，并借助扩展规律进行定量建模。同时考察预训练和数据增强策略对检测效果的影响。

Result: 实验发现，随着真实图像域数量或深度伪造方法数量的增加，检测误差呈幂律规律下降，具备可预测性。此外，探索了数据扩展、预训练和数据增强对性能提升的边界和贡献。

Conclusion: 检测模型性能受数据多样性和规模驱动且可预测，未来可基于幂律趋势合理扩展数据集应对不断演化的深度伪造技术，数据中心化思路对提升检测能力具有重要意义，但盲目扩展也存在效果瓶颈。

Abstract: This paper presents a systematic study of scaling laws for the deepfake
detection task. Specifically, we analyze the model performance against the
number of real image domains, deepfake generation methods, and training images.
Since no existing dataset meets the scale requirements for this research, we
construct ScaleDF, the largest dataset to date in this field, which contains
over 5.8 million real images from 51 different datasets (domains) and more than
8.8 million fake images generated by 102 deepfake methods. Using ScaleDF, we
observe power-law scaling similar to that shown in large language models
(LLMs). Specifically, the average detection error follows a predictable
power-law decay as either the number of real domains or the number of deepfake
methods increases. This key observation not only allows us to forecast the
number of additional real domains or deepfake methods required to reach a
target performance, but also inspires us to counter the evolving deepfake
technology in a data-centric manner. Beyond this, we examine the role of
pre-training and data augmentations in deepfake detection under scaling, as
well as the limitations of scaling itself.

</details>


### [27] [Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical Local Attention](https://arxiv.org/abs/2510.16325)
*Yuyao Zhang,Yu-Wing Tai*

Main category: cs.CV

TL;DR: 本文提出了Scale-DiT，一种创新的文生图扩散模型，在无需4K原生数据的情况下，实现了高效、语义一致的超高分辨率图像（4K x 4K）合成。核心通过分层局部注意力结合低分辨率全局引导，显著提升推理速度和内存效率，同时保持细节和结构的高质量。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型受到注意力机制的二次复杂度和4K训练数据稀缺的双重制约，难以扩展到超高分辨率（>1K x 1K）的细致图像生成。既要细粒度纹理，还需全局结构一致，现有方法难以两者兼得。

Method: 提出Scale-DiT框架：1）将高分辨率特征划分为局部窗口，令注意力复杂度近线性化；2）引入带有位置锚点的低分辨率特征提供全局语义引导；3）用LoRA轻量适配桥接全局-局部信息；4）推理阶段按Hilbert曲线重排列令GPU更高效，使用融合Kernel跳过掩码运算。

Result: 相比于稠密注意力的扩散模型，Scale-DiT实现了超过2倍的推理加速和更低的显存消耗，无需额外4K训练数据即稳健支持4K x 4K生成。在量化评测（FID分数、IS分数、CLIP分数）及主观效果上，Scale-DiT的整体结构保持和局部细节质量均优于依赖4K原生数据的同类SOTA方法。

Conclusion: 分层局部注意力配合低分辨语义引导是一种实现超高分辨率文生图合成的高效且有效范式，对推动大尺寸高质量图片生成具有重要意义。

Abstract: Ultra-high-resolution text-to-image generation demands both fine-grained
texture synthesis and globally coherent structure, yet current diffusion models
remain constrained to sub-$1K \times 1K$ resolutions due to the prohibitive
quadratic complexity of attention and the scarcity of native $4K$ training
data. We present \textbf{Scale-DiT}, a new diffusion framework that introduces
hierarchical local attention with low-resolution global guidance, enabling
efficient, scalable, and semantically coherent image synthesis at ultra-high
resolutions. Specifically, high-resolution latents are divided into fixed-size
local windows to reduce attention complexity from quadratic to near-linear,
while a low-resolution latent equipped with scaled positional anchors injects
global semantics. A lightweight LoRA adaptation bridges global and local
pathways during denoising, ensuring consistency across structure and detail. To
maximize inference efficiency, we repermute token sequence in Hilbert curve
order and implement a fused-kernel for skipping masked operations, resulting in
a GPU-friendly design. Extensive experiments demonstrate that Scale-DiT
achieves more than $2\times$ faster inference and lower memory usage compared
to dense attention baselines, while reliably scaling to $4K \times 4K$
resolution without requiring additional high-resolution training data. On both
quantitative benchmarks (FID, IS, CLIP Score) and qualitative comparisons,
Scale-DiT delivers superior global coherence and sharper local detail, matching
or outperforming state-of-the-art methods that rely on native 4K training.
Taken together, these results highlight hierarchical local attention with
guided low-resolution anchors as a promising and effective approach for
advancing ultra-high-resolution image generation.

</details>


### [28] [DiffusionX: Efficient Edge-Cloud Collaborative Image Generation with Multi-Round Prompt Evolution](https://arxiv.org/abs/2510.16326)
*Yi Wei,Shunpu Tang,Liang Zhao,Qiangian Yang*

Main category: cs.CV

TL;DR: 本文提出了DiffusionX，一个结合云端和终端的高效扩散模型图像生成框架，能在保证质量的同时提升多轮生成效率，降低延迟和云端压力。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型虽能生成高质量图像，但计算复杂，用户需多次精调提示，整体生成延迟高、云端负载重。如何在保证生成质量的前提下提升效率、减少云资源消耗，是亟需解决的问题。

Method: 作者设计了DiffusionX框架，将生成过程分为两步：1）在本地设备用轻量级扩散模型快速生成预览图，与用户交互优化提示词；2）用户确定最终提示词后，由云端高性能模型进行最终高清图生成。此外，引入噪声等级预测器动态分配本地和云端算力。

Result: 实验表明，DiffusionX在保证与Stable Diffusion v1.5相近图像质量的同时，平均生成时间缩短了15.8%；与Tiny-SD相比，画质显著提升，仅慢0.9%。

Conclusion: DiffusionX能在保持生成质量的同时，有效减少延迟与云端算力，具备良好的效率与可扩展性，对大规模云边协同AI图像生成有实用意义。

Abstract: Recent advances in diffusion models have driven remarkable progress in image
generation. However, the generation process remains computationally intensive,
and users often need to iteratively refine prompts to achieve the desired
results, further increasing latency and placing a heavy burden on cloud
resources. To address this challenge, we propose DiffusionX, a cloud-edge
collaborative framework for efficient multi-round, prompt-based generation. In
this system, a lightweight on-device diffusion model interacts with users by
rapidly producing preview images, while a high-capacity cloud model performs
final refinements after the prompt is finalized. We further introduce a noise
level predictor that dynamically balances the computation load, optimizing the
trade-off between latency and cloud workload. Experiments show that DiffusionX
reduces average generation time by 15.8% compared with Stable Diffusion v1.5,
while maintaining comparable image quality. Moreover, it is only 0.9% slower
than Tiny-SD with significantly improved image quality, thereby demonstrating
efficiency and scalability with minimal overhead.

</details>


### [29] [TokenAR: Multiple Subject Generation via Autoregressive Token-level enhancement](https://arxiv.org/abs/2510.16332)
*Haiyue Sun,Qingdong He,Jinlong Peng,Peng Tang,Jiangning Zhang,Junwei Zhu,Xiaobin Hu,Shuicheng Yan*

Main category: cs.CV

TL;DR: 本文提出了一种名为TokenAR的新框架，通过对自回归模型(AR)的token级增强，有效解决了多参考图像生成中身份混淆问题，并发布了首个大规模多参考输入公开数据集InstructAR，实验优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有自回归图像生成方法在处理多参考图像生成时，难以有效拆分不同参考身份，导致身份混淆，降低了生成图像的质量和多样性。因此，亟需一种能更好区分参考身份且保证图像一致性的生成方法。

Method: 1) 提出Token Index Embedding，对token索引聚类以更准确表示同一参考图；2) 引入Instruct Token Injection，作为附加视觉特征容器，为参考token注入细致补充先验；3) 提出身份-token解耦策略（ITD），显式指导每个token独立表达不同身份特征。并公开了一个包含2.8万个例子的InstructAR多参考输入数据集。

Result: 所提出的TokenAR框架显著提升了现有AR方法在多参考条件下生成图像的身份一致性和背景还原质量。大量实验显示，其在多参考输入的图像生成任务中超越现有最先进方法。

Conclusion: TokenAR为多参考自回归图像生成带来了简单高效token增强机制，有效解决身份混淆等关键问题；并提供开源数据集和代码推动发展。文中方法具有明确实际意义，结果可靠，对领域研究具有重要参考价值。

Abstract: Autoregressive Model (AR) has shown remarkable success in conditional image
generation. However, these approaches for multiple reference generation
struggle with decoupling different reference identities. In this work, we
propose the TokenAR framework, specifically focused on a simple but effective
token-level enhancement mechanism to address reference identity confusion
problem. Such token-level enhancement consists of three parts, 1). Token Index
Embedding clusters the tokens index for better representing the same reference
images; 2). Instruct Token Injection plays as a role of extra visual feature
container to inject detailed and complementary priors for reference tokens; 3).
The identity-token disentanglement strategy (ITD) explicitly guides the token
representations toward independently representing the features of each
identity.This token-enhancement framework significantly augments the
capabilities of existing AR based methods in conditional image generation,
enabling good identity consistency while preserving high quality background
reconstruction. Driven by the goal of high-quality and high-diversity in
multi-subject generation, we introduce the InstructAR Dataset, the first
open-source, large-scale, multi-reference input, open domain image generation
dataset that includes 28K training pairs, each example has two reference
subjects, a relative prompt and a background with mask annotation, curated for
multiple reference image generation training and evaluating. Comprehensive
experiments validate that our approach surpasses current state-of-the-art
models in multiple reference image generation task. The implementation code and
datasets will be made publicly. Codes are available, see
https://github.com/lyrig/TokenAR

</details>


### [30] [RL makes MLLMs see better than SFT](https://arxiv.org/abs/2510.16333)
*Junha Song,Sangdoo Yun,Dongyoon Han,Jaegul Choo,Byeongho Heo*

Main category: cs.CV

TL;DR: 本论文探究了多模态语言模型（MLLM）中视觉编码器的作用，发现强化学习（RL）训练策略较有监督微调（SFT）更能提升视觉表现，并提出了高效的视觉优化方法PIVOT。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM研究大多认为其性能主要来自LLM主干，忽视了视觉编码器的变化及其重要性，尤其是在从SFT到RL训练范式转变的背景下。

Method: 作者首先比较了RL和SFT两种训练策略在视觉相关任务（如VQA）上的表现，然后通过图像分类、分割和梯度可视化等多实验深入分析了训练策略对视觉编码器的影响，最后提出PIVOT方案并验证其实用性。

Result: 实验显示RL训练不仅提升了下游多模态表现，更能显著强化视觉编码器的局部表征能力。PIVOT策略训练出的视觉编码器在计算成本远低于标准预训练的条件下，性能超越更大规模的模型。

Conclusion: RL训练策略能极大提升MLLM的视觉能力，PIVOT提供了一种高效、低成本加强视觉编码器的新路径，对MLLM体系发展有重要意义。

Abstract: A dominant assumption in Multimodal Language Model (MLLM) research is that
its performance is largely inherited from the LLM backbone, given its immense
parameter scale and remarkable capabilities. This has created a void in the
understanding of the vision encoder, which determines how MLLMs perceive
images. The recent shift in MLLM training paradigms, from Supervised Finetuning
(SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the
significant lack of analysis on how such training reshapes the vision encoder
as well as the MLLM. To address this, we first investigate the impact of
training strategies on MLLMs, where RL shows a clear advantage over SFT in
strongly vision-related VQA benchmarks. Motivated by this, we conduct a
critical yet under-explored analysis of the vision encoder of MLLMs through
diverse and in-depth experiments, ranging from ImageNet classification and
segmentation to gradient visualization. Our results demonstrate that MLLM's
post-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on
MLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual
representations. Specifically, the key finding of our study is that RL produces
stronger and precisely localized visual representations compared to SFT,
boosting the ability of the vision encoder for MLLM. We then reframe our
findings into a simple recipe for building strong vision encoders for MLLMs,
Preference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs,
a PIVOT-trained vision encoder outperforms even larger and more heavily-trained
counterparts, despite requiring less than 1% of the computational cost of
standard vision pretraining. This result opens an effective and efficient path
for advancing the vision backbones of MLLMs. Project page available at
https://june-page.github.io/pivot/

</details>


### [31] [On the Provable Importance of Gradients for Language-Assisted Image Clustering](https://arxiv.org/abs/2510.16335)
*Bo Peng,Jie Lu,Guangquan Zhang,Zhen Fang*

Main category: cs.CV

TL;DR: 该论文研究了语言辅助的图像聚类问题，并提出了一种基于梯度的名词筛选方法GradNorm，在理论和实验上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LaIC（语言辅助图像聚类）希望利用文本语义提升图像表征的区分性。目前一大难点是如何从无标签语料中筛选出与目标图像相关的正面名词，因为真实类别名不可用，而现有方法理论基础不足。

Method: 作者提出了一种新的梯度范数筛选方法GradNorm。其思想是，利用图像-文本模型（如CLIP）的softmax输出和目标分布间的交叉熵，计算每个候选名词反向传播到输入层的梯度大小，以此衡量该名词的正面性。理论上，GradNorm能严格量化正面名词的区分度，并可看作现有方法的推广。

Result: 理论分析给出GradNorm筛选名词的误差上界，并证明其兼容和涵盖现有基于CLIP的策略。在多个基准数据集上的实验也证明了GradNorm取得了最优的聚类性能。

Conclusion: GradNorm不仅在理论上填补了现有方法的空白，也在实际效果上取得了领先结果，是LaIC问题中的有效方案。

Abstract: This paper investigates the recently emerged problem of Language-assisted
Image Clustering (LaIC), where textual semantics are leveraged to improve the
discriminability of visual representations to facilitate image clustering. Due
to the unavailability of true class names, one of core challenges of LaIC lies
in how to filter positive nouns, i.e., those semantically close to the images
of interest, from unlabeled wild corpus data. Existing filtering strategies are
predominantly based on the off-the-shelf feature space learned by CLIP;
however, despite being intuitive, these strategies lack a rigorous theoretical
foundation. To fill this gap, we propose a novel gradient-based framework,
termed as GradNorm, which is theoretically guaranteed and shows strong
empirical performance. In particular, we measure the positiveness of each noun
based on the magnitude of gradients back-propagated from the cross-entropy
between the predicted target distribution and the softmax output.
Theoretically, we provide a rigorous error bound to quantify the separability
of positive nouns by GradNorm and prove that GradNorm naturally subsumes
existing filtering strategies as extremely special cases of itself.
Empirically, extensive experiments show that GradNorm achieves the
state-of-the-art clustering performance on various benchmarks.

</details>


### [32] [MIRAD - A comprehensive real-world robust anomaly detection dataset for Mass Individualization](https://arxiv.org/abs/2510.16370)
*Pulin Li,Guocheng Wu,Li Yin,Yuxin Zheng,Wei Zhang,Yanjie Zhou*

Main category: cs.CV

TL;DR: 本文介绍了首个专为社会化制造中的异常检测问题设计的数据集MIRAD，反映了大规模个性化生产环境下产品多样性、分布式节点和成像异质性的真实挑战，并通过各种主流方法评测，证明了该领域的研究难度。


<details>
  <summary>Details</summary>
Motivation: 社会化制造推动了大规模个性化生产，但随之带来了质量控制、尤其是缺陷检测上的显著挑战。主要难点在于：产品高度定制、订单量零散、成像环境各异，而现实场景下缺乏合适的数据集和算法，制约了学术及工业应用发展。

Method: 作者提出并公开了MIRAD数据集，涵盖六个地理分散工厂、复杂多样的产品类别，以及成像环境的多种变化。利用该数据集，作者评测了目前主流的异常检测方法，包括单类、多类和零样本方法。

Result: 所有算法在MIRAD上的性能都比在传统基准数据集时大幅下降，反映出个性化生产实际场景下异常检测的复杂性和技术难点。

Conclusion: MIRAD数据集有效填补了个性化社会化制造领域异常检测基准的空白，为学界和业界开发更鲁棒的质量控制技术奠定了坚实基础，对Industry 5.0发展具有重要意义。

Abstract: Social manufacturing leverages community collaboration and scattered
resources to realize mass individualization in modern industry. However, this
paradigm shift also introduces substantial challenges in quality control,
particularly in defect detection. The main difficulties stem from three
aspects. First, products often have highly customized configurations. Second,
production typically involves fragmented, small-batch orders. Third, imaging
environments vary considerably across distributed sites. To overcome the
scarcity of real-world datasets and tailored algorithms, we introduce the Mass
Individualization Robust Anomaly Detection (MIRAD) dataset. As the first
benchmark explicitly designed for anomaly detection in social manufacturing,
MIRAD captures three critical dimensions of this domain: (1) diverse
individualized products with large intra-class variation, (2) data collected
from six geographically dispersed manufacturing nodes, and (3) substantial
imaging heterogeneity, including variations in lighting, background, and motion
conditions. We then conduct extensive evaluations of state-of-the-art (SOTA)
anomaly detection methods on MIRAD, covering one-class, multi-class, and
zero-shot approaches. Results show a significant performance drop across all
models compared with conventional benchmarks, highlighting the unresolved
complexities of defect detection in real-world individualized production. By
bridging industrial requirements and academic research, MIRAD provides a
realistic foundation for developing robust quality control solutions essential
for Industry 5.0. The dataset is publicly available at
https://github.com/wu33learn/MIRAD.

</details>


### [33] [Cataract-LMM: Large-Scale, Multi-Source, Multi-Task Benchmark for Deep Learning in Surgical Video Analysis](https://arxiv.org/abs/2510.16371)
*Mohammad Javad Ahmadi,Iman Gandomi,Parisa Abdi,Seyed-Farzad Mohammadi,Amirhossein Taslimi,Mehdi Khodaparast,Hassan Hashemi,Mahdi Tavakoli,Hamid D. Taghirad*

Main category: cs.CV

TL;DR: 本文提出了一个包含3,000例白内障超声乳化手术视频的数据集，并配备四层详细标注，用于支持计算机辅助手术领域的AI建模与研究。


<details>
  <summary>Details</summary>
Motivation: 现有白内障手术数据集缺乏多样性和足够的标注深度，难以支撑通用的深度学习模型开发，对此亟需新的高质量数据资源。

Method: 收集来自两大手术中心、不同经验水平外科医生完成的3,000例白内障手术视频。对每例数据进行四重标注：手术时相、器械及解剖结构实例分割、器械与组织交互追踪、基于ICO-OSCAR等标准技能评分。同时通过流程识别、场景分割、自动技能评估等AI任务进行基准测试，并在手术流程识别任务中设立领域自适应基线。

Result: 数据集为行业内罕见地提供了丰富、多元且精细的标注，为外科AI任务（如手术流程识别、场景分割等）提供了高质量训练与评估资源。基准实验展示了数据的多样性和挑战性，并测试了领域适应在不同中心间的效果。

Conclusion: 该数据集为外科AI研究提供了坚实的基础，有助于推动白内障手术流程分析、自动化技能评估等多项研究进展，是领域内宝贵的开放资源。

Abstract: The development of computer-assisted surgery systems depends on large-scale,
annotated datasets. Current resources for cataract surgery often lack the
diversity and annotation depth needed to train generalizable deep-learning
models. To address this gap, we present a dataset of 3,000 phacoemulsification
cataract surgery videos from two surgical centers, performed by surgeons with a
range of experience levels. This resource is enriched with four annotation
layers: temporal surgical phases, instance segmentation of instruments and
anatomical structures, instrument-tissue interaction tracking, and quantitative
skill scores based on the established competency rubrics like the ICO-OSCAR.
The technical quality of the dataset is supported by a series of benchmarking
experiments for key surgical AI tasks, including workflow recognition, scene
segmentation, and automated skill assessment. Furthermore, we establish a
domain adaptation baseline for the phase recognition task by training a model
on a subset of surgical centers and evaluating its performance on a held-out
center. The dataset and annotations are available in Google Form
(https://docs.google.com/forms/d/e/1FAIpQLSfmyMAPSTGrIy2sTnz0-TMw08ZagTimRulbAQcWdaPwDy187A/viewform?usp=dialog).

</details>


### [34] [iWatchRoadv2: Pothole Detection, Geospatial Mapping, and Intelligent Road Governance](https://arxiv.org/abs/2510.16375)
*Rishi Raj Sahoo,Surbhi Saswati Mohanty,Subhankar Mishra*

Main category: cs.CV

TL;DR: 本文提出了iWatchRoadv2，一个基于YOLO模型、能够自动识别和定位道路坑洞，同时实现路况动态可视化和智能治理的新平台。


<details>
  <summary>Details</summary>
Motivation: 印度道路维护困难，坑洞频发，威胁交通安全且难以有效管理。本研究旨在开发一个自动化、实时、可扩展的道路坑洞监测与治理平台，提升养护效率和管理透明度。

Method: 1. 构建自标注的印度道路行车记录仪图像数据集（7,000帧）；2. 微调Ultralytics YOLO模型进行坑洞检测；3. 结合GPS日志实现精准地理标注；4. 使用OCR同步时间戳；5. 后端系统关联道路、承包商等元数据，支持自动告警、责任追踪和网页端可视化、分析。

Result: iWatchRoadv2实现了高效、准确的坑洞检测与地理标注，支持道路健康动态展示、责任主体告警和智能分析，具备成本低、数据处理高效和大规模部署能力。

Conclusion: 本平台能够自动化涵盖坑洞检测、治理、修复追踪的全流程，为智慧城市管理及道路维护提供数据驱动、透明高效的解决方案。项目公开，社会公众和官方均可参与。

Abstract: Road potholes pose significant safety hazards and maintenance challenges,
particularly on India's diverse and under-maintained road networks. This paper
presents iWatchRoadv2, a fully automated end-to-end platform for real-time
pothole detection, GPS-based geotagging, and dynamic road health visualization
using OpenStreetMap (OSM). We curated a self-annotated dataset of over 7,000
dashcam frames capturing diverse Indian road conditions, weather patterns, and
lighting scenarios, which we used to fine-tune the Ultralytics YOLO model for
accurate pothole detection. The system synchronizes OCR-extracted video
timestamps with external GPS logs to precisely geolocate each detected pothole,
enriching detections with comprehensive metadata, including road segment
attribution and contractor information managed through an optimized backend
database. iWatchRoadv2 introduces intelligent governance features that enable
authorities to link road segments with contract metadata through a secure login
interface. The system automatically sends alerts to contractors and officials
when road health deteriorates, supporting automated accountability and warranty
enforcement. The intuitive web interface delivers actionable analytics to
stakeholders and the public, facilitating evidence-driven repair planning,
budget allocation, and quality assessment. Our cost-effective and scalable
solution streamlines frame processing and storage while supporting seamless
public engagement for urban and rural deployments. By automating the complete
pothole monitoring lifecycle, from detection to repair verification,
iWatchRoadv2 enables data-driven smart city management, transparent governance,
and sustainable improvements in road infrastructure maintenance. The platform
and live demonstration are accessible at
https://smlab.niser.ac.in/project/iwatchroad.

</details>


### [35] [Demeter: A Parametric Model of Crop Plant Morphology from the Real World](https://arxiv.org/abs/2510.16377)
*Tianhang Cheng,Albert J. Zhai,Evan Z. Chen,Rui Zhou,Yawen Deng,Zitong Li,Kejie Zhao,Janice Shiu,Qianyu Zhao,Yide Xu,Xinlei Wang,Yuan Shen,Sheng Wang,Lisa Ainsworth,Kaiyu Guan,Shenlong Wang*

Main category: cs.CV

TL;DR: 本文提出了Demeter，一种适用于植物的三维参数化形状模型，能够有效模拟植物的形态、结构与变形。


<details>
  <summary>Details</summary>
Motivation: 目前3D参数化形状模型在人类和动物上取得了显著进展，但针对于植物，尤其是能适应复杂形态与拓扑变化的表达能力强的模型较为缺乏。植物的三维建模对于重建、生成、理解和仿真非常重要，因此需要开发专门的植物形态建模方法。

Method: 提出了一种数据驱动的参数化模型Demeter，能对植物的关键形态特征（如拓扑、形状、关节和变形）进行紧凑表达。与以往模型不同，Demeter 能适应不同物种间的形态拓扑变化，并建模三种形状变异来源：关节、子结构形状变化与非刚性变形。作者还采集了大规模大豆农田实测数据，用于验证方法有效性。

Result: 实验结果显示，Demeter 能有效进行植物形状合成、结构重建与生物物理过程模拟。

Conclusion: Demeter 提升了作物（植物）形态建模的精度和灵活性，为三维重建、仿真等下游任务带来推动，具备实际应用价值。

Abstract: Learning 3D parametric shape models of objects has gained popularity in
vision and graphics and has showed broad utility in 3D reconstruction,
generation, understanding, and simulation. While powerful models exist for
humans and animals, equally expressive approaches for modeling plants are
lacking. In this work, we present Demeter, a data-driven parametric model that
encodes key factors of a plant morphology, including topology, shape,
articulation, and deformation into a compact learned representation. Unlike
previous parametric models, Demeter handles varying shape topology across
various species and models three sources of shape variation: articulation,
subcomponent shape variation, and non-rigid deformation. To advance crop plant
modeling, we collected a large-scale, ground-truthed dataset from a soybean
farm as a testbed. Experiments show that Demeter effectively synthesizes
shapes, reconstructs structures, and simulates biophysical processes. Code and
data is available at https://tianhang-cheng.github.io/Demeter/.

</details>


### [36] [SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation](https://arxiv.org/abs/2510.16396)
*Yeh Keng Hao,Hsu Tzu Wei,Sun Min*

Main category: cs.CV

TL;DR: 本论文提出了一种轻量级的手势识别深度学习框架，专为AR/VR边缘设备优化，大幅提升了速度与效率，同时保持了高准确率。


<details>
  <summary>Details</summary>
Motivation: 随着AR/VR设备日益普及，将深度学习模型部署到边缘设备（如Raspberry Pi等）成为关键挑战。这些设备对实时性、低功耗和低延迟有较高要求，但现有模型经常在效率和性能间难以取舍。

Method: 作者采用了编码器-解码器框架，具体方法包括：在ResNet-18骨干网络中应用稀疏卷积以提升推理效率；提出了全新的SPLite解码器以大幅加速解码过程；采用量化感知训练来进一步减少内存占用，同时几乎不损失准确率。所有优化均针对低算力的嵌入式边缘设备进行。

Result: 在Raspberry Pi 5 CPU（BCM2712四核A76）上，推理速度提升了2.98倍，端到端效率提升42%，同时解码帧率提升了3.1倍。准确率基本保持不变，主指标PA-MPJPE仅从9.0 mm增加到9.1 mm。在FreiHAND和复合基准数据集上表现与SOTA方法相当。

Conclusion: 该工作显著提升了AR/VR边缘设备上手势识别的推理速度和计算效率，同时保持了高准确性，为深度学习模型实际部署提供了重要参考和新策略。

Abstract: With the increasing ubiquity of AR/VR devices, the deployment of deep
learning models on edge devices has become a critical challenge. These devices
require real-time inference, low power consumption, and minimal latency. Many
framework designers face the conundrum of balancing efficiency and performance.
We design a light framework that adopts an encoder-decoder architecture and
introduces several key contributions aimed at improving both efficiency and
accuracy. We apply sparse convolution on a ResNet-18 backbone to exploit the
inherent sparsity in hand pose images, achieving a 42% end-to-end efficiency
improvement. Moreover, we propose our SPLite decoder. This new architecture
significantly boosts the decoding process's frame rate by 3.1x on the Raspberry
Pi 5, while maintaining accuracy on par. To further optimize performance, we
apply quantization-aware training, reducing memory usage while preserving
accuracy (PA-MPJPE increases only marginally from 9.0 mm to 9.1 mm on
FreiHAND). Overall, our system achieves a 2.98x speed-up on a Raspberry Pi 5
CPU (BCM2712 quad-core Arm A76 processor). Our method is also evaluated on
compound benchmark datasets, demonstrating comparable accuracy to
state-of-the-art approaches while significantly enhancing computational
efficiency.

</details>


### [37] [REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting](https://arxiv.org/abs/2510.16410)
*Changyue Shi,Minghao Chen,Yiping Mao,Chuxiao Yang,Xinyuan Hu,Jiajun Ding,Zhou Yu*

Main category: cs.CV

TL;DR: 本文提出了REALM——一个面向开放世界推理分割的多模态大模型智能体框架，实现从复杂的人类指令到精确的3D目标分割，无需大量3D专属训练。


<details>
  <summary>Details</summary>
Motivation: 现有3D分割方法难以理解需要推理的复杂人类指令，而2D视觉-语言模型虽擅长推理却缺乏3D空间理解，导致复杂任务难以实现。因此，亟需一种结合两者优势的新方法。

Method: REALM利用3D高斯斑点（Gaussian Splatting）表示，使MLLM能基于渲染视图理解3D场景。通过“全局到局部空间定位”策略，先并行输入多个全局视角实现粗定位，再合成近距离视角进行细粒度3D分割。无需专门3D后训练。

Result: 在LERF、3D-OVS和新提出的REALM3D基准上，REALM能精准解释各种显式和隐式指令，表现优异。同时支持3D目标移除、替换、风格迁移等多种3D操作。

Conclusion: REALM开创性地实现了面向复杂指令的高精度3D对象定位，兼顾实用性和多功能性，有望推动真实场景中的机器人和视觉理解应用。

Abstract: Bridging the gap between complex human instructions and precise 3D object
grounding remains a significant challenge in vision and robotics. Existing 3D
segmentation methods often struggle to interpret ambiguous, reasoning-based
instructions, while 2D vision-language models that excel at such reasoning lack
intrinsic 3D spatial understanding. In this paper, we introduce REALM, an
innovative MLLM-agent framework that enables open-world reasoning-based
segmentation without requiring extensive 3D-specific post-training. We perform
segmentation directly on 3D Gaussian Splatting representations, capitalizing on
their ability to render photorealistic novel views that are highly suitable for
MLLM comprehension. As directly feeding one or more rendered views to the MLLM
can lead to high sensitivity to viewpoint selection, we propose a novel
Global-to-Local Spatial Grounding strategy. Specifically, multiple global views
are first fed into the MLLM agent in parallel for coarse-level localization,
aggregating responses to robustly identify the target object. Then, several
close-up novel views of the object are synthesized to perform fine-grained
local segmentation, yielding accurate and consistent 3D masks. Extensive
experiments show that REALM achieves remarkable performance in interpreting
both explicit and implicit instructions across LERF, 3D-OVS, and our newly
introduced REALM3D benchmarks. Furthermore, our agent framework seamlessly
supports a range of 3D interaction tasks, including object removal,
replacement, and style transfer, demonstrating its practical utility and
versatility. Project page: https://ChangyueShi.github.io/REALM.

</details>


### [38] [SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning](https://arxiv.org/abs/2510.16416)
*Xiaojun Guo,Runyu Zhou,Yifei Wang,Qi Zhang,Chenheng Zhang,Stefanie Jegelka,Xiaohan Wang,Jiajun Chai,Guojun Yin,Wei Lin,Yisen Wang*

Main category: cs.CV

TL;DR: 提出用自监督任务（如旋转预测、掩码重建）替代人工奖励机制来提升视觉-语言模型（VLMs）表现，显著改善模型在多模态任务的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型在推理时容易依赖语言或取巧，难以充分利用视觉证据，而强化学习因缺少可扩展、可靠的奖励机制，难以进一步提升VLMs的泛化能力。

Method: 提出SSL4RL方法，将自监督学习（SSL）任务作为可验证的奖励，改写为密集的奖励信号，无需人工偏好数据或不可靠的AI评估器。通过系统实验和消融分析探索影响效果的关键要素，如任务难度、模型规模和语义对齐等。

Result: SSL4RL明显提升了VLMs在纯视觉和多模态推理基准的性能，在图学习等跨模态场景同样取得显著收益。

Conclusion: SSL4RL利用自监督目标，为多模态模型对齐提供了可验证且高效的通用新框架，对未来模型设计具有指导意义。

Abstract: Vision-language models (VLMs) have shown remarkable abilities by integrating
large language models with visual inputs. However, they often fail to utilize
visual evidence adequately, either depending on linguistic priors in
vision-centric tasks or resorting to textual shortcuts during reasoning.
Although reinforcement learning (RL) can align models with desired behaviors,
its application to VLMs has been hindered by the lack of scalable and reliable
reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel
framework that leverages self-supervised learning (SSL) tasks as a source of
verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL
objectives-such as predicting image rotation or reconstructing masked
patches-into dense, automatic reward signals, eliminating the need for human
preference data or unreliable AI evaluators. Experiments show that SSL4RL
substantially improves performance on both vision-centric and vision-language
reasoning benchmarks. Furthermore, through systematic ablations, we identify
key factors-such as task difficulty, model scale, and semantic alignment with
the target domain-that influence the effectiveness of SSL4RL tasks, offering
new design principles for future work. We also demonstrate the framework's
generality by applying it to graph learning, where it yields significant gains.
SSL4RL establishes a versatile and effective paradigm for aligning multimodal
models using verifiable, self-supervised objectives.

</details>


### [39] [LightGlueStick: a Fast and Robust Glue for Joint Point-Line Matching](https://arxiv.org/abs/2510.16438)
*Aidyn Ubingazhibov,Rémi Pautrat,Iago Suárez,Shaohui Liu,Marc Pollefeys,Viktor Larsson*

Main category: cs.CV

TL;DR: 该论文提出了LightGlueStick，一种高效、轻量级的点与线段联合匹配方法，并在多个基准测试上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有点和线特征的匹配方法对SLAM和SfM应用效果好，但常规点、线独立处理，或者联合方法（如GlueStick）计算开销大、难以实时。需要一种既高效又能联合利用点线信息的轻量化特征匹配方案。

Method: 作者提出LightGlueStick，一种轻量级点和线段联合匹配算法。其核心创新为Attentional Line Message Passing(ALMP)模块，可以高效地捕捉线段之间的连接性并提升特征间通信效率。整体架构减小了模型复杂性，兼顾效果和实时性。

Result: 实验表明，LightGlueStick在多个匹配基准测试中刷新了前沿SOTA（state-of-the-art），兼具精度和运算速度优势，可胜任实时或边缘端部署需求。

Conclusion: LightGlueStick结合点与线段匹配的优势，并通过轻量化设计达到更高效率与精度，推动了特征匹配在SLAM、SfM等应用中的实际落地。

Abstract: Lines and points are complementary local features, whose combination has
proven effective for applications such as SLAM and Structure-from-Motion. The
backbone of these pipelines are the local feature matchers, establishing
correspondences across images. Traditionally, point and line matching have been
treated as independent tasks. Recently, GlueStick proposed a GNN-based network
that simultaneously operates on points and lines to establish matches. While
running a single joint matching reduced the overall computational complexity,
the heavy architecture prevented real-time applications or deployment to edge
devices.
  Inspired by recent progress in point matching, we propose LightGlueStick, a
lightweight matcher for points and line segments. The key novel component in
our architecture is the Attentional Line Message Passing (ALMP), which
explicitly exposes the connectivity of the lines to the network, allowing for
efficient communication between nodes. In thorough experiments we show that
LightGlueStick establishes a new state-of-the-art across different benchmarks.
The code is available at https://github.com/aubingazhib/LightGlueStick.

</details>


### [40] [EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning](https://arxiv.org/abs/2510.16442)
*Haoran Sun,Chen Cai,Huiping Zhuang,Kong Aik Lee,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: 本文提出了一种可解释性深度伪造视频检测（EDVD）新任务，并设计了EDVD-LLaMA多模态大语言模型推理框架，能实现深度伪造检测与可追溯解释。提出了时空细粒度特征提取与推理机制，并构建了可解释推理数据集，在多个检测和泛化任务上效果优异。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造视频检测方法检测原理不透明、泛化性能不足，难以应对不断演化的伪造技术，且难以提供可验证的检测解释，因此亟需既能检测伪造，又能提供推理解释的检测器。

Method: 1. 定义了可解释深度伪造视频检测（EDVD）任务。
2. 设计了EDVD-LLaMA多模态大语言模型推理框架：
- 采用空间-时间细腻信息分词（ST-SIT）机制提取融合帧间的全局与局部深度伪造特征。
- 引入细粒度多模态链式思维（Fg-MCoT）机制，将人脸特征数据作为推理中的硬约束，提升时空定位与解释的可靠性。
- 构建了可解释推理的FF++基准数据集（ER-FF++set），结合结构化标注，实现检测与推理的双重监督。

Result: EDVD-LLaMA框架在检测准确率、可解释性、对多类伪造方法和新数据集的泛化能力等方面均优于已有深度伪造检测方法。

Conclusion: 该方法不仅提升了深度伪造视频检测的准确性和鲁棒性，还使检测结果拥有清晰可验证的推理链路，提升了检测的可信度和实用性，可广泛适用于对深度伪造检测有高要求的领域。

Abstract: The rapid development of deepfake video technology has not only facilitated
artistic creation but also made it easier to spread misinformation. Traditional
deepfake video detection (DVD) methods face issues such as a lack of
transparency in their principles and insufficient generalization capabilities
to cope with evolving forgery techniques. This highlights an urgent need for
detectors that can identify forged content and provide verifiable reasoning
explanations. This paper proposes the explainable deepfake video detection
(EDVD) task and designs the EDVD-LLaMA multimodal, a large language model
(MLLM) reasoning framework, which provides traceable reasoning processes
alongside accurate detection results and trustworthy explanations. Our approach
first incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT)
to extract and fuse global and local cross-frame deepfake features, providing
rich spatio-temporal semantic information input for MLLM reasoning. Second, we
construct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which
introduces facial feature data as hard constraints during the reasoning process
to achieve pixel-level spatio-temporal video localization, suppress
hallucinated outputs, and enhance the reliability of the chain of thought. In
addition, we build an Explainable Reasoning FF++ benchmark dataset
(ER-FF++set), leveraging structured data to annotate videos and ensure quality
control, thereby supporting dual supervision for reasoning and detection.
Extensive experiments demonstrate that EDVD-LLaMA achieves outstanding
performance and robustness in terms of detection accuracy, explainability, and
its ability to handle cross-forgery methods and cross-dataset scenarios.
Compared to previous DVD methods, it provides a more explainable and superior
solution. The source code and dataset will be publicly available.

</details>


### [41] [RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba](https://arxiv.org/abs/2510.16444)
*Kunyu Peng,Di Wen,Jia Fu,Jiamin Wu,Kailun Yang,Junwei Zheng,Ruiping Liu,Yufan Chen,Yuqian Fu,Danda Pani Paudel,Luc Van Gool,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 本文提出了RefAtomNet++，一种针对原子级参考视频动作识别的新方法，并发布了新的大规模数据集RefAVA++。实验结果显示该方法在多个基准任务上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统动作识别方法在复杂多人场景下难以基于自然语言精细识别特定个体的原子级动作。为促进语言引导的视频细粒度动作识别，作者扩展数据集并改进模型，致力于提升跨模态对齐与检索能力。

Method: 提出RefAtomNet++，结合了多层次语义对齐的跨模态注意力机制与多轨迹Mamba建模。通过不同语义层次（部分关键词、场景属性、整体句子）动态选择视觉空间token，增强跨模态特征聚合能力。

Result: RefAtomNet++在RefAVA++数据集上的实验显著优于同类工作，展现出更好的目标人物定位与原子级动作预测能力，并刷新了当前基准任务的性能记录。

Conclusion: RefAtomNet++提升了多语义层级的视觉-文本特征融合能力，为基于自然语言的精细动作识别设立了新标准，对未来复杂视频理解研究有重要推动作用。

Abstract: Referring Atomic Video Action Recognition (RAVAR) aims to recognize
fine-grained, atomic-level actions of a specific person of interest conditioned
on natural language descriptions. Distinct from conventional action recognition
and detection tasks, RAVAR emphasizes precise language-guided action
understanding, which is particularly critical for interactive human action
analysis in complex multi-person scenarios. In this work, we extend our
previously introduced RefAVA dataset to RefAVA++, which comprises >2.9 million
frames and >75.1k annotated persons in total. We benchmark this dataset using
baselines from multiple related domains, including atomic action localization,
video question answering, and text-video retrieval, as well as our earlier
model, RefAtomNet. Although RefAtomNet surpasses other baselines by
incorporating agent attention to highlight salient features, its ability to
align and retrieve cross-modal information remains limited, leading to
suboptimal performance in localizing the target person and predicting
fine-grained actions. To overcome the aforementioned limitations, we introduce
RefAtomNet++, a novel framework that advances cross-modal token aggregation
through a multi-hierarchical semantic-aligned cross-attention mechanism
combined with multi-trajectory Mamba modeling at the partial-keyword,
scene-attribute, and holistic-sentence levels. In particular, scanning
trajectories are constructed by dynamically selecting the nearest visual
spatial tokens at each timestep for both partial-keyword and scene-attribute
levels. Moreover, we design a multi-hierarchical semantic-aligned
cross-attention strategy, enabling more effective aggregation of spatial and
temporal tokens across different semantic hierarchies. Experiments show that
RefAtomNet++ establishes new state-of-the-art results. The dataset and code are
released at https://github.com/KPeng9510/refAVA2.

</details>


### [42] [Enhancing Rotated Object Detection via Anisotropic Gaussian Bounding Box and Bhattacharyya Distance](https://arxiv.org/abs/2510.16445)
*Chien Thai,Mai Xuan Trang,Huong Ninh,Hoang Hiep Ly,Anh Son Le*

Main category: cs.CV

TL;DR: 本文针对旋转目标检测任务提出了一种改进的损失函数，通过高斯边界框和Bhattacharyya距离提升检测精度，并引入各向异性高斯表征以适应不同形状目标，实验验证该方法显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 旋转目标检测在遥感、无人驾驶等领域中很重要，但传统的目标检测方法对旋转目标表现不佳，主要是难以有效捕捉目标的方向、多样化形状等几何属性。本文希望通过改进损失函数和表征，解决旋转物体检测困难。

Method: 采用高斯边界框表征目标，并计算Bhattacharyya距离作为对象之间相似度的计算方式。同时，引入各向异性高斯表征，解决方形等特殊目标的方差问题。最后将新的旋转不变损失函数集成到最新的深度学习检测框架中。

Result: 将本文提出的损失函数集成到主流旋转检测网络后，实验显示在mean Average Precision（mAP）等关键指标上有显著提升，优于现有方法。

Conclusion: 本文方法有效增强了旋转目标检测的精度和鲁棒性，为相关应用提供了新的基线和更可靠的解决方案，对实现高精度任意方向目标定位有较大意义。

Abstract: Detecting rotated objects accurately and efficiently is a significant
challenge in computer vision, particularly in applications such as aerial
imagery, remote sensing, and autonomous driving. Although traditional object
detection frameworks are effective for axis-aligned objects, they often
underperform in scenarios involving rotated objects due to their limitations in
capturing orientation variations. This paper introduces an improved loss
function aimed at enhancing detection accuracy and robustness by leveraging the
Gaussian bounding box representation and Bhattacharyya distance. In addition,
we advocate for the use of an anisotropic Gaussian representation to address
the issues associated with isotropic variance in square-like objects. Our
proposed method addresses these challenges by incorporating a
rotation-invariant loss function that effectively captures the geometric
properties of rotated objects. We integrate this proposed loss function into
state-of-the-art deep learning-based rotated object detection detectors, and
extensive experiments demonstrated significant improvements in mean Average
Precision metrics compared to existing methods. The results highlight the
potential of our approach to establish new benchmark in rotated object
detection, with implications for a wide range of applications requiring precise
and reliable object localization irrespective of orientation.

</details>


### [43] [VIPAMIN: Visual Prompt Initialization via Embedding Selection and Subspace Expansion](https://arxiv.org/abs/2510.16446)
*Jaekyun Park,Hye Won Chung*

Main category: cs.CV

TL;DR: 提出了一种视觉提示初始化策略VIPAMIN，有效提升自监督模型在少量数据和难任务上的适应能力，操作简单且性能优异。


<details>
  <summary>Details</summary>
Motivation: 现有大模型全参数微调资源开销极大，而视觉提示调优方法对于自监督模型适应性和表达能力不足，特别在难任务和小数据集场景下问题突出。

Method: VIPAMIN通过：1）将提示向量与embedding空间中具有语义信息的区域对齐；2）注入新的表征方向突破原有预训练子空间，仅需一次前向传播和简单运算即可实现高效初始化。

Result: VIPAMIN在不同下游任务和数据规模下均显著优于现有视觉提示方法，树立了视觉提示调优的新性能标杆。

Conclusion: VIPAMIN作为一种简洁有效的初始策略，极大提升了自监督视觉模型的小样本适应能力，将推动视觉提示调优方法的应用和发展。

Abstract: In the era of large-scale foundation models, fully fine-tuning pretrained
networks for each downstream task is often prohibitively resource-intensive.
Prompt tuning offers a lightweight alternative by introducing tunable prompts
while keeping the backbone frozen. However, existing visual prompt tuning
methods often fail to specialize the prompts or enrich the representation
space--especially when applied to self-supervised backbones. We show that these
limitations become especially pronounced in challenging tasks and data-scarce
settings, where effective adaptation is most critical. In this work, we
introduce VIPAMIN, a visual prompt initialization strategy that enhances
adaptation of self-supervised models by (1) aligning prompts with semantically
informative regions in the embedding space, and (2) injecting novel
representational directions beyond the pretrained subspace. Despite its
simplicity--requiring only a single forward pass and lightweight
operations--VIPAMIN consistently improves performance across diverse tasks and
dataset sizes, setting a new state of the art in visual prompt tuning. Our code
is available at https://github.com/iamjaekyun/vipamin.

</details>


### [44] [Instance-Aware Pseudo-Labeling and Class-Focused Contrastive Learning for Weakly Supervised Domain Adaptive Segmentation of Electron Microscopy](https://arxiv.org/abs/2510.16450)
*Shan Xiong,Jiabao Chen,Ye Wang,Jialin Peng*

Main category: cs.CV

TL;DR: 本文提出了一种高效的线粒体实例分割方法，仅需极少的点标注，显著提高不同电镜影像中的分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督领域自适应（UDA）方法在实际应用中效果有限，完全标注代价高，因此希望通过极小量的弱标注提升目标域分割性能。

Method: 提出了一种多任务学习框架，融合分割和中心检测，结合新颖的跨任务教学和类关注跨域对比学习。引入基于实例感知的伪标签（IPL）自训练策略，结合检测任务语义筛选高质量伪标签。

Result: 在多个具挑战性的数据集上，方法优于现有UDA和WDA方法，性能大幅提升，并明显缩小与全监督方法的性能差距。在UDA场景下也优于其他UDA技术。

Conclusion: 该方法在减少标注工作量的情况下，实现了更优的分割效果，为领域适应任务提供了新的高效注释方案。

Abstract: Annotation-efficient segmentation of the numerous mitochondria instances from
various electron microscopy (EM) images is highly valuable for biological and
neuroscience research. Although unsupervised domain adaptation (UDA) methods
can help mitigate domain shifts and reduce the high costs of annotating each
domain, they typically have relatively low performance in practical
applications. Thus, we investigate weakly supervised domain adaptation (WDA)
that utilizes additional sparse point labels on the target domain, which
require minimal annotation effort and minimal expert knowledge. To take full
use of the incomplete and imprecise point annotations, we introduce a multitask
learning framework that jointly conducts segmentation and center detection with
a novel cross-teaching mechanism and class-focused cross-domain contrastive
learning. While leveraging unlabeled image regions is essential, we introduce
segmentation self-training with a novel instance-aware pseudo-label (IPL)
selection strategy. Unlike existing methods that typically rely on pixel-wise
pseudo-label filtering, the IPL semantically selects reliable and diverse
pseudo-labels with the help of the detection task. Comprehensive validations
and comparisons on challenging datasets demonstrate that our method outperforms
existing UDA and WDA methods, significantly narrowing the performance gap with
the supervised upper bound. Furthermore, under the UDA setting, our method also
achieves substantial improvements over other UDA techniques.

</details>


### [45] [NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation](https://arxiv.org/abs/2510.16457)
*Peiran Xu,Xicheng Gong,Yadong MU*

Main category: cs.CV

TL;DR: 提出了一种前瞻性视觉-语言导航方法，通过结合Q-learning和无监督轨迹数据，提升了导航智能体对未来可能结果的考虑，并验证了其在多个数据集上的有效性。


<details>
  <summary>Details</summary>
Motivation: 以往的视觉-语言导航方法主要依赖历史信息，忽略了动作的长期影响和未来结果。本研究动机是克服这一局限，使智能体在行动时能够考虑未来的潜在回报，从而实现更高效的导航。

Method: 方法核心是利用Q-learning在大规模无标注轨迹数据上训练Q-model，让模型学会室内场景的布局和物体关系。该模型能为每个候选动作生成类似Q值的Q-feature，表征未来观察到的信息。然后，设计跨模态未来编码器，将无任务偏见的Q-feature与导航指令结合，生成能反映未来前景的一组动作分数。最后，这些分数与基于历史的原始分数结合，采用类似A*搜索策略，探索更可能到达目标的位置。

Result: 在多个主流的VLN目标导向数据集上进行大量实验证明，该方法在导航任务上取得了显著的性能提升。

Conclusion: 通过引入前瞻性的决策机制，结合历史和未来的动作得分，提出的方法能更有效地进行目标导向的视觉-语言导航，为之后相关任务提供了新的思路与范式。

Abstract: In this work we concentrate on the task of goal-oriented Vision-and-Language
Navigation (VLN). Existing methods often make decisions based on historical
information, overlooking the future implications and long-term outcomes of the
actions. In contrast, we aim to develop a foresighted agent. Specifically, we
draw upon Q-learning to train a Q-model using large-scale unlabeled trajectory
data, in order to learn the general knowledge regarding the layout and object
relations within indoor scenes. This model can generate a Q-feature, analogous
to the Q-value in traditional Q-network, for each candidate action, which
describes the potential future information that may be observed after taking
the specific action. Subsequently, a cross-modal future encoder integrates the
task-agnostic Q-feature with navigation instructions to produce a set of action
scores reflecting future prospects. These scores, when combined with the
original scores based on history, facilitate an A*-style searching strategy to
effectively explore the regions that are more likely to lead to the
destination. Extensive experiments conducted on widely used goal-oriented VLN
datasets validate the effectiveness of the proposed method.

</details>


### [46] [HGC-Avatar: Hierarchical Gaussian Compression for Streamable Dynamic 3D Avatars](https://arxiv.org/abs/2510.16463)
*Haocheng Tang,Ruoke Yan,Xinhui Yin,Qi Zhang,Xinfeng Zhang,Siwei Ma,Wen Gao,Chuanmin Jia*

Main category: cs.CV

TL;DR: 该论文提出了一种分层高斯压缩（HGC-Avatar）框架，用于高效传输和高质量动态数字人渲染，提升了渲染效果和压缩率。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯Splatting（3DGS）技术虽能实现高质量、快速渲染，但在数字人编码和传输时，由于缺乏人类先验，导致压缩率和重建质量不理想，难以应用于流式3D虚拟人场景。

Method: 提出了HGC-Avatar方法，将3D高斯表示分为结构层和运动层。结构层通过StyleUNet生成器将姿态映射为高斯点，运动层利用SMPL-X模型紧凑表达时间上的姿态变化。该层次结构支持分层压缩、渐进解码和多样姿态可控渲染，同时在人脸区域采用注意力机制以保留面部细节。

Result: 该方法在实验中证明了能够以较低码率实现高质量的3D虚拟人快速渲染，其视觉质量和压缩效率显著优于以往方法。

Conclusion: HGC-Avatar提供了一种高效流式动态3D虚拟人渲染和传输的解决方案，为相关应用如虚拟交流等带来了更高的画质和传输效率。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled fast,
photorealistic rendering of dynamic 3D scenes, showing strong potential in
immersive communication. However, in digital human encoding and transmission,
the compression methods based on general 3DGS representations are limited by
the lack of human priors, resulting in suboptimal bitrate efficiency and
reconstruction quality at the decoder side, which hinders their application in
streamable 3D avatar systems. We propose HGC-Avatar, a novel Hierarchical
Gaussian Compression framework designed for efficient transmission and
high-quality rendering of dynamic avatars. Our method disentangles the Gaussian
representation into a structural layer, which maps poses to Gaussians via a
StyleUNet-based generator, and a motion layer, which leverages the SMPL-X model
to represent temporal pose variations compactly and semantically. This
hierarchical design supports layer-wise compression, progressive decoding, and
controllable rendering from diverse pose inputs such as video sequences or
text. Since people are most concerned with facial realism, we incorporate a
facial attention mechanism during StyleUNet training to preserve identity and
expression details under low-bitrate constraints. Experimental results
demonstrate that HGC-Avatar provides a streamable solution for rapid 3D avatar
rendering, while significantly outperforming prior methods in both visual
quality and compression efficiency.

</details>


### [47] [PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies](https://arxiv.org/abs/2510.16505)
*Lukas Selch,Yufang Hou,M. Jehanzeb Mirza,Sivan Doveh,James Glass,Rogerio Feris,Wei Lin*

Main category: cs.CV

TL;DR: 本论文提出PRISMM-Bench，这是首个基于真实论文同行评审指出的不一致性、多模态（文本、图表、表格、公式）理解的新基准，并评测了21个主流大多模态模型（LMMs），发现这些模型在科学推理场景下表现不佳。


<details>
  <summary>Details</summary>
Motivation: 随着大多模态模型在科学领域的应用增长，其理解和推理复杂多模态学术内容的能力存疑。尤其是跨文本、图表、公式的细微不一致性会影响科学研究的透明度和信任，目前缺乏真实、复杂的评测基准。

Method: 作者提出PRISMM-Bench，一个由真实论文评审报告挖掘并人工筛选验证的不一致性数据集，覆盖文本、图、表、公式等多模态内容，设计了三项任务：不一致检测、修正及配对，并采用结构化JSON答案减少题型捷径带来的偏差。

Result: 对21个现有大型多模态模型，包括开源和闭源高性能模型进行评测，结果显示模型在该数据集上的准确率很低（26.1%-54.2%），远未达到可靠科学助理的水平。

Conclusion: 现有大多模态模型尚不能胜任科学文献中多模态一致性推理任务，需要进一步提升其能力，以实现可信赖的科研辅助。

Abstract: Large Multimodal Models (LMMs) are increasingly applied to scientific
research, yet it remains unclear whether they can reliably understand and
reason over the multimodal complexity of papers. A central challenge lies in
detecting and resolving inconsistencies across text, figures, tables, and
equations, issues that are often subtle, domain-specific, and ultimately
undermine clarity, reproducibility, and trust. Existing benchmarks overlook
this issue, either isolating single modalities or relying on synthetic errors
that fail to capture real-world complexity. We introduce PRISMM-Bench
(Peer-Review-sourced Inconsistency Set for Multimodal Models), the first
benchmark grounded in real reviewer-flagged inconsistencies in scientific
papers. Through a multi-stage pipeline of review mining, LLM-assisted filtering
and human verification, we curate 262 inconsistencies from 242 papers. Based on
this set, we design three tasks, namely inconsistency identification, remedy
and pair matching, which assess a model's capacity to detect, correct, and
reason over inconsistencies across different modalities. Furthermore, to
address the notorious problem of choice-only shortcuts in multiple-choice
evaluation, where models exploit answer patterns without truly understanding
the question, we further introduce structured JSON-based answer representations
that minimize linguistic biases by reducing reliance on superficial stylistic
cues. We benchmark 21 leading LMMs, including large open-weight models
(GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5
with high reasoning). Results reveal strikingly low performance (26.1-54.2%),
underscoring the challenge of multimodal scientific reasoning and motivating
progress towards trustworthy scientific assistants.

</details>


### [48] [OOS-DSD: Improving Out-of-stock Detection in Retail Images using Auxiliary Tasks](https://arxiv.org/abs/2510.16508)
*Franko Šikić,Sven Lončarić*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的新方法OOS-DSD，通过辅助学习提升了零售货架缺货（OOS）检测的准确性，并在实验中取得了优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 缺货检测对于零售管理非常重要，但现有方法在复杂场景下仍面临检测准确率不足的问题。本文旨在通过引入辅助任务提升检测性能。

Method: 在YOLOv8目标检测框架基础上增加了辅助卷积分支，同时进行缺货检测、商品分割和场景深度估计。缺货和分割分支依赖真实标注训练，深度分支则采用先进的Depth Anything V2模型生成的伪标注，并引入深度归一化方法以稳定训练过程。

Result: OOS-DSD方法的缺货检测性能（mAP）比现有最优方法高1.8%；辅助学习提升mAP 3.7%，深度归一化带来4.2%的提升。

Conclusion: 辅助学习和深度归一化能够有效提升OOS检测性能，所提方法优于SOTA现有方法，具有实际应用价值。

Abstract: Out-of-stock (OOS) detection is a very important retail verification process
that aims to infer the unavailability of products in their designated areas on
the shelf. In this paper, we introduce OOS-DSD, a novel deep learning-based
method that advances OOS detection through auxiliary learning. In particular,
we extend a well-established YOLOv8 object detection architecture with
additional convolutional branches to simultaneously detect OOS, segment
products, and estimate scene depth. While OOS detection and product
segmentation branches are trained using ground truth data, the depth estimation
branch is trained using pseudo-labeled annotations produced by the
state-of-the-art (SOTA) depth estimation model Depth Anything V2. Furthermore,
since the aforementioned pseudo-labeled depth estimates display relative depth,
we propose an appropriate depth normalization procedure that stabilizes the
training process. The experimental results show that the proposed method
surpassed the performance of the SOTA OOS detection methods by 1.8% of the mean
average precision (mAP). In addition, ablation studies confirm the
effectiveness of auxiliary learning and the proposed depth normalization
procedure, with the former increasing mAP by 3.7% and the latter by 4.2%.

</details>


### [49] [Image Categorization and Search via a GAT Autoencoder and Representative Models](https://arxiv.org/abs/2510.16514)
*Duygu Sap,Martin Lotz,Connor Mattinson*

Main category: cs.CV

TL;DR: 本文提出一种基于图和图注意力网络（GAT）自编码器的图像分类和检索方法。方法通过构建图像及类别代表性模型，利用邻居关系获得判别性特征，实现有效的图像分类和同类别内检索。实验表明该方法优于传统特征方法。


<details>
  <summary>Details</summary>
Motivation: 传统的图像分类和检索方法依赖于单一图像特征，难以充分利用图像间关系，且模型鲁棒性不强。本文旨在结合GAT和自编码器机制，更好挖掘图像间的上下文联系，提高分类与检索精度。

Method: 首先以图形式建模图像数据，节点对应图像或代表性特征，边表示相似度关系。接着使用GAT自编码器提取每张图像的上下文敏感潜在表示，并从中生成类别代表向量。分类时通过比较查询图像与各类别代表的距离确定类别，检索时在同类别内寻找最相似图像。

Result: 通过与标准特征方法对比实验，所提出的代表性中心方法表现更优，有效提升了图像分类与检索的准确率。

Conclusion: 基于GAT自编码器的代表性中心方法能够建模图像间复杂关系，获得更加判别性的表征，在图像分类和检索任务中效果明显优于常规方法。

Abstract: We propose a method for image categorization and retrieval that leverages
graphs and a graph attention network (GAT)-based autoencoder. Our approach is
representative-centric, that is, we execute the categorization and retrieval
process via the representative models we construct for the images and image
categories. We utilize a graph where nodes represent images (or their
representatives) and edges capture similarity relationships. GAT highlights
important features and relationships between images, enabling the autoencoder
to construct context-aware latent representations that capture the key features
of each image relative to its neighbors. We obtain category representatives
from these embeddings and categorize a query image by comparing its
representative to the category representatives. We then retrieve the most
similar image to the query image within its identified category. We demonstrate
the effectiveness of our representative-centric approach through experiments
with both the GAT autoencoders and standard feature-based techniques.

</details>


### [50] [Self-Supervised Learning to Fly using Efficient Semantic Segmentation and Metric Depth Estimation for Low-Cost Autonomous UAVs](https://arxiv.org/abs/2510.16624)
*Sebastian Mocanu,Emil Slusanschi,Marius Leordeanu*

Main category: cs.CV

TL;DR: 本文提出了一种仅依靠视觉的无人机自主飞行系统，结合语义分割与单目深度估算，实现了无需GPS或昂贵传感器的室内避障、自主降落等功能，显著提升了任务效率并保持高成功率。


<details>
  <summary>Details</summary>
Motivation: 当前小型无人机受限于硬件资源，传统导航需依赖GPS、LiDAR等昂贵或无法用于室内环境的设备，因此亟需低成本、高效且可部署在资源受限平台上的视觉导航方案。

Method: 系统结合语义分割与单目深度估算，通过自适应尺度因子算法将深度预测转化为真实距离，依赖语义地面检测与相机参数优化精度。采用知识蒸馏框架：基于颜色的SVM作为教师模型生成训练标签，用轻量级U-Net学生网络实现实时语义分割。对于复杂环境，可替换为更先进的分割网络。最终通过端到端学习，学生网络从演示数据中学习完整飞行策略。

Result: 在5x4米实验室环境中进行实测，30次真实无人机飞行测试和100次数字孪生环境测试显示，该方法提升了无人机任务路径长度、缩短了任务时间，并在部分环节维持100%成功率。基于端到端策略网络的自主任务成功率达到87.5%。

Conclusion: 本研究实现了无需昂贵硬件、适用于受限环境的视觉导航无人机系统，在准确定距与计算资源优化方面取得突破，为实用化小型无人机自主飞行提供了创新性解决方案。

Abstract: This paper presents a vision-only autonomous flight system for small UAVs
operating in controlled indoor environments. The system combines semantic
segmentation with monocular depth estimation to enable obstacle avoidance,
scene exploration, and autonomous safe landing operations without requiring GPS
or expensive sensors such as LiDAR. A key innovation is an adaptive scale
factor algorithm that converts non-metric monocular depth predictions into
accurate metric distance measurements by leveraging semantic ground plane
detection and camera intrinsic parameters, achieving a mean distance error of
14.4 cm. The approach uses a knowledge distillation framework where a
color-based Support Vector Machine (SVM) teacher generates training data for a
lightweight U-Net student network (1.6M parameters) capable of real-time
semantic segmentation. For more complex environments, the SVM teacher can be
replaced with a state-of-the-art segmentation model. Testing was conducted in a
controlled 5x4 meter laboratory environment with eight cardboard obstacles
simulating urban structures. Extensive validation across 30 flight tests in a
real-world environment and 100 flight tests in a digital-twin environment
demonstrates that the combined segmentation and depth approach increases the
distance traveled during surveillance and reduces mission time while
maintaining 100% success rates. The system is further optimized through
end-to-end learning, where a compact student neural network learns complete
flight policies from demonstration data generated by our best-performing
method, achieving an 87.5% autonomous mission success rate. This work advances
practical vision-based drone navigation in structured environments,
demonstrating solutions for metric depth estimation and computational
efficiency challenges that enable deployment on resource-constrained platforms.

</details>


### [51] [Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions](https://arxiv.org/abs/2510.16540)
*Jihoon Kwon,Kyle Min,Jy-yong Sohn*

Main category: cs.CV

TL;DR: 本文提出READ方法，通过给视觉-语言模型的对比学习添加重建和对齐辅助目标，提高其组合推理能力。READ-CLIP模型在五个主流组合推理基准上取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型（如CLIP）的标准对比学习目标容易导致模型理解单词而非结构化关系，缺乏对视觉和语言元素间结构关系的掌握，从而组合推理能力有限。需要更好地利用文本结构信息以提升推理能力。

Method: 提出READ（Reconstruction and Alignment of text Descriptions）微调方法，在对比学习基础上新增两个辅助目标：（1）Token级的重建目标，利用冻结的预训练解码器基于原始文本嵌入重建改写句子；（2）句子级的对齐目标，使同一意思的不同表达在嵌入空间中对齐。该方法可应用于CLIP及其变体。

Result: 使用READ微调的READ-CLIP，在五个主要组合推理数据集上均取得最优结果，相较现有微调方法最高提升4.1%。对NegCLIP、FSC-CLIP等CLIP变体同样有效。分析显示重建目标促进词关系建模，对齐目标强化意义一致性。

Conclusion: READ方法通过加强句内词关系建模和同义句语义对齐，显著提升模型的组合推理能力；方案通用、易于集成到现有主流视觉-语言模型。

Abstract: Despite recent advances, vision-language models trained with standard
contrastive objectives still struggle with compositional reasoning -- the
ability to understand structured relationships between visual and linguistic
elements. This shortcoming is largely due to the tendency of the text encoder
to focus on individual words rather than their relations, a limitation
reinforced by contrastive training that primarily aligns words with visual
objects. In this paper, we introduce REconstruction and Alignment of text
Descriptions (READ), a fine-tuning method designed to enhance compositional
reasoning by adding two auxiliary objectives to the contrastive learning: (1) a
token-level reconstruction objective, where a frozen pre-trained decoder
reconstructs alternative captions based on the embedding of the original
caption; and (2) a sentence-level alignment objective, which explicitly aligns
paraphrased sentences in the embedding space. We show that READ-CLIP, a model
derived by applying the READ method to the pre-trained CLIP model, achieves the
state-of-the-art performance across five major compositional reasoning
benchmarks, outperforming the strongest conventional fine-tuning baseline by up
to 4.1%. Furthermore, applying the READ to existing CLIP variants (including
NegCLIP and FSC-CLIP) also improves performance on these benchmarks.
Quantitative and qualitative analyses reveal that our proposed objectives --
reconstruction and alignment -- offer complementary benefits: the former
encourages the encoder to capture relationships between words within a caption,
while the latter ensures consistent representations for paraphrases expressed
with different wording.

</details>


### [52] [Structured Interfaces for Automated Reasoning with 3D Scene Graphs](https://arxiv.org/abs/2510.16643)
*Aaron Ray,Jacob Arkin,Harel Biggie,Chuchu Fan,Luca Carlone,Nicholas Roy*

Main category: cs.CV

TL;DR: 该论文提出了一种结合大语言模型（LLM）与3D场景图（3DSG）进行自然语言理解与响应的新方法，通过查询接口（Cypher）提升大模型在复杂场景中的语言理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法将3D场景图序列化为文本输入LLM，难以扩展到大型或丰富的3DSG，导致效果和效率受限。作者希望解决LLM与3DSG在复杂环境下的高效对接问题。

Method: 将3D场景图存储在图数据库中，为LLM提供基于Cypher的查询接口，使其可按需检索相关3DSG子集，通过“检索增强生成”方式提升场景信息提取的相关性与效率。并与传统的上下文窗口和代码生成法进行对比实验。

Result: 实验表明，该方法在指令追踪和场景问答任务中，比传统方法在处理大规模、丰富场景图时有更好的可扩展性和性能，显著减少场景图的token消耗。无论本地还是云端模型都有明显提升。

Conclusion: 通过引入查询语言作为LLM与3DSG的接口，实现了更高效、更可扩展的自然语言“落地”能力，为机器人理解和处理复杂自然语言任务提供了有效路径。

Abstract: In order to provide a robot with the ability to understand and react to a
user's natural language inputs, the natural language must be connected to the
robot's underlying representations of the world. Recently, large language
models (LLMs) and 3D scene graphs (3DSGs) have become a popular choice for
grounding natural language and representing the world. In this work, we address
the challenge of using LLMs with 3DSGs to ground natural language. Existing
methods encode the scene graph as serialized text within the LLM's context
window, but this encoding does not scale to large or rich 3DSGs. Instead, we
propose to use a form of Retrieval Augmented Generation to select a subset of
the 3DSG relevant to the task. We encode a 3DSG in a graph database and provide
a query language interface (Cypher) as a tool to the LLM with which it can
retrieve relevant data for language grounding. We evaluate our approach on
instruction following and scene question-answering tasks and compare against
baseline context window and code generation methods. Our results show that
using Cypher as an interface to 3D scene graphs scales significantly better to
large, rich graphs on both local and cloud-based models. This leads to large
performance improvements in grounded language tasks while also substantially
reducing the token count of the scene graph content. A video supplement is
available at https://www.youtube.com/watch?v=zY_YI9giZSA.

</details>


### [53] [Watch Where You Move: Region-aware Dynamic Aggregation and Excitation for Gait Recognition](https://arxiv.org/abs/2510.16541)
*Binyuan Huang,Yongdong Luo,Xianda Guo,Xiawu Zheng,Zheng Zhu,Jiahui Pan,Chengju Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的步态识别新框架（GaitRDAE），通过动态挖掘和建模运动区域以提升识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有步态识别方法大多采用预定义区域进行时序建模，无法灵活应对运动区域随时间动态变化的问题，尤其在辅助信息导致外观变化时，建模效果有限。作者希望通过自适应建模提升动态步态识别的鲁棒性和精度。

Method: 提出了GaitRDAE框架，包含两个核心模块：1）区域感知的动态聚合（RDA）模块，自动寻找每个区域最优的时间感受野；2）区域感知的动态激励（RDE）模块，增强对稳定运动区域的关注，抑制对噪音敏感静态区域的注意力。通过端到端训练实现区域与时序建模自适应。

Result: 该方法在多个常用步态识别基准数据集上取得了最新最优的表现，显著优于传统和现有深度学习方法。

Conclusion: 通过自适应地区域和时间尺度建模，有效提升了步态识别的准确性和鲁棒性，验证了动态建模运动区域的有效性，具备广泛应用前景。

Abstract: Deep learning-based gait recognition has achieved great success in various
applications. The key to accurate gait recognition lies in considering the
unique and diverse behavior patterns in different motion regions, especially
when covariates affect visual appearance. However, existing methods typically
use predefined regions for temporal modeling, with fixed or equivalent temporal
scales assigned to different types of regions, which makes it difficult to
model motion regions that change dynamically over time and adapt to their
specific patterns. To tackle this problem, we introduce a Region-aware Dynamic
Aggregation and Excitation framework (GaitRDAE) that automatically searches for
motion regions, assigns adaptive temporal scales and applies corresponding
attention. Specifically, the framework includes two core modules: the
Region-aware Dynamic Aggregation (RDA) module, which dynamically searches the
optimal temporal receptive field for each region, and the Region-aware Dynamic
Excitation (RDE) module, which emphasizes the learning of motion regions
containing more stable behavior patterns while suppressing attention to static
regions that are more susceptible to covariates. Experimental results show that
GaitRDAE achieves state-of-the-art performance on several benchmark datasets.

</details>


### [54] [An RGB-D Image Dataset for Lychee Detection and Maturity Classification for Robotic Harvesting](https://arxiv.org/abs/2510.16800)
*Zhenpeng Zhang,Yi Wang,Shanglei Chai,Yingying Liu,Zekai Xie,Wenhao Huang,Pengyu Li,Zipei Luo,Dajiang Lu,Yibin Tian*

Main category: cs.CV

TL;DR: 论文构建并公开了一个全面注释的荔枝果实检测与成熟度分类数据集，促进基于视觉的采摘机器人研发。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏在自然生长环境下、具有一致性和全面注释的开源荔枝图像数据集，限制了基于视觉的采摘机器人和相应深度学习技术的发展。

Method: 采集不同气候、时段和品种下的荔枝图像，包括3种成熟度阶段，共获得11414张图片（包括原始RGB、增强RGB和深度图像），并由三位标注者独立标注，再由第四位专家复核。此外，进行了详细的统计分析，并用三种代表性深度学习模型对数据集进行了实验评估。

Result: 获得了详细标注的11414张荔枝数据集，含9658对检测与成熟度标签。统计分析表现数据质量高且多样性丰富。深度学习模型测试展示了该数据集的实用价值。

Conclusion: 论文提供了第一个在自然环境下全面标注的荔枝检测与分级公开数据集，有助于推动视觉采摘机器人与相关研究领域进步。

Abstract: Lychee is a high-value subtropical fruit. The adoption of vision-based
harvesting robots can significantly improve productivity while reduce reliance
on labor. High-quality data are essential for developing such harvesting
robots. However, there are currently no consistently and comprehensively
annotated open-source lychee datasets featuring fruits in natural growing
environments. To address this, we constructed a dataset to facilitate lychee
detection and maturity classification. Color (RGB) images were acquired under
diverse weather conditions, and at different times of the day, across multiple
lychee varieties, such as Nuomici, Feizixiao, Heiye, and Huaizhi. The dataset
encompasses three different ripeness stages and contains 11,414 images,
consisting of 878 raw RGB images, 8,780 augmented RGB images, and 1,756 depth
images. The images are annotated with 9,658 pairs of lables for lychee
detection and maturity classification. To improve annotation consistency, three
individuals independently labeled the data, and their results were then
aggregated and verified by a fourth reviewer. Detailed statistical analyses
were done to examine the dataset. Finally, we performed experiments using three
representative deep learning models to evaluate the dataset. It is publicly
available for academic

</details>


### [55] [Fit for Purpose? Deepfake Detection in the Real World](https://arxiv.org/abs/2510.16556)
*Guangyu Lin,Li Lin,Christina P. Walker,Daniel S. Schiff,Shu Hu*

Main category: cs.CV

TL;DR: 本论文指出现有深度伪造检测工具难以应对真实社交媒体上的政治深度伪造内容，均存在泛化能力不足、易被简单操控等问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成内容（如深度伪造）激增，尤其是在政治领域，这类虚假内容对公共信任和政治机构造成威胁，现有检测手段却主要基于实验室数据，缺乏对真实场景的应对能力。因此，亟需评估现有检测器在实际政治深度伪造内容上的表现，推动更有效的检测方案发展。

Method: 作者基于自建的Political Deepfakes Incident Database（自2018年以来在社交媒体流传的真实政治深度伪造合集），系统评测了来自学术界、政府和工业界的多种最先进深度伪造检测器的检测性能。

Result: 学术及政府机构的检测器表现欠佳；付费工具略优于免费模型，但全部检测器在应对真实政治领域的深度伪造时泛化能力有限，特别是视频检测容易被简单操作规避。

Conclusion: 现有检测模型难以有效保护公众免受政治深度伪造威胁，迫切需要结合政治语境，开发针对实际环境的检测框架。

Abstract: The rapid proliferation of AI-generated content, driven by advances in
generative adversarial networks, diffusion models, and multimodal large
language models, has made the creation and dissemination of synthetic media
effortless, heightening the risks of misinformation, particularly political
deepfakes that distort truth and undermine trust in political institutions. In
turn, governments, research institutions, and industry have strongly promoted
deepfake detection initiatives as solutions. Yet, most existing models are
trained and validated on synthetic, laboratory-controlled datasets, limiting
their generalizability to the kinds of real-world political deepfakes
circulating on social platforms that affect the public. In this work, we
introduce the first systematic benchmark based on the Political Deepfakes
Incident Database, a curated collection of real-world political deepfakes
shared on social media since 2018. Our study includes a systematic evaluation
of state-of-the-art deepfake detectors across academia, government, and
industry. We find that the detectors from academia and government perform
relatively poorly. While paid detection tools achieve relatively higher
performance than free-access models, all evaluated detectors struggle to
generalize effectively to authentic political deepfakes, and are vulnerable to
simple manipulations, especially in the video domain. Results urge the need for
politically contextualized deepfake detection frameworks to better safeguard
the public in real-world settings.

</details>


### [56] [M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention for Monocular Spatial Perception](https://arxiv.org/abs/2510.17363)
*U. V. B. L Udugama,George Vosselman,Francesco Nex*

Main category: cs.CV

TL;DR: 本文提出了一种名为M2H的新型多任务学习框架，实现了在单张单目图像下同时进行语义分割、深度估计、边缘检测和表面法向量预测，兼顾了效率和性能，适用于实际边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上实现实时空间感知任务时，需要既高效又能充分利用多任务间互补信息的模型，以克服计算资源有限带来的挑战。

Method: M2H框架采用轻量级基于ViT的DINOv2骨干网络，创新性地引入了基于窗口的跨任务注意力模块（Window-Based Cross-Task Attention Module），实现了结构化的特征交换，同时保留每个任务的特定信息，提高多任务间的预测一致性。

Result: 在NYUDv2、Hypersim、Cityscapes多个公开数据集上，M2H优于当前多任务模型，并在部分单任务基线（如深度、语义）任务上取得更好表现。同时，该方法在普通笔记本硬件上仍能保持高效运行。

Conclusion: M2H不仅在基准数据集上表现出色，还在真实世界场景中经过验证，证实其适合实际空间感知应用，能够作为动态环境下单目3D场景图构建的有效基础。

Abstract: Deploying real-time spatial perception on edge devices requires efficient
multi-task models that leverage complementary task information while minimizing
computational overhead. This paper introduces Multi-Mono-Hydra (M2H), a novel
multi-task learning framework designed for semantic segmentation and depth,
edge, and surface normal estimation from a single monocular image. Unlike
conventional approaches that rely on independent single-task models or shared
encoder-decoder architectures, M2H introduces a Window-Based Cross-Task
Attention Module that enables structured feature exchange while preserving
task-specific details, improving prediction consistency across tasks. Built on
a lightweight ViT-based DINOv2 backbone, M2H is optimized for real-time
deployment and serves as the foundation for monocular spatial perception
systems supporting 3D scene graph construction in dynamic environments.
Comprehensive evaluations show that M2H outperforms state-of-the-art multi-task
models on NYUDv2, surpasses single-task depth and semantic baselines on
Hypersim, and achieves superior performance on the Cityscapes dataset, all
while maintaining computational efficiency on laptop hardware. Beyond
benchmarks, M2H is validated on real-world data, demonstrating its practicality
in spatial perception tasks.

</details>


### [57] [SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense](https://arxiv.org/abs/2510.16596)
*Yiyang Huang,Liang Shi,Yitian Zhang,Yi Xu,Yun Fu*

Main category: cs.CV

TL;DR: 本文指出大规模视觉-语言模型（LVLMs）在多模态任务中依然存在“物体幻觉”问题，即模型产生看似合理但不准确的目标描述。作者首次发现幻觉主要来自视觉编码器，并提出无训练框架SHIELD，有效缓解此问题。


<details>
  <summary>Details</summary>
Motivation: 虽然LVLM在多模态任务表现强大，但“物体幻觉”现象影响其实用性和可靠性。此前研究多关注于语言模型部分，而忽视了视觉编码器的作用。亟需找到LVLM幻觉的根本来源并进行有针对性的改进。

Method: 论文首次将LVLM的物体幻觉追溯到视觉编码器，分析出三大原因：统计偏见、固有偏见及脆弱性。为此，提出无需训练的SHIELD框架：1）重加权视觉token以减少统计偏见，2）引入噪声token以对抗固有偏见，3）对抗性攻击结合对比解码以提升鲁棒性。

Result: 在多个基准和不同LVLM架构上实验，结果显示SHIELD能有效降低物体幻觉，并在通用LVLM评测中也表现出较强性能。

Conclusion: SHIELD作为一种训练无关的通用防护方法，不仅有效缓解了LVLM的物体幻觉问题，还有良好的通用性和推广潜力，为提升LVLM实际应用的可靠性带来积极影响。

Abstract: Large Vision-Language Models (LVLMs) excel in diverse cross-modal tasks.
However, object hallucination, where models produce plausible but inaccurate
object descriptions, remains a significant challenge. In contrast to previous
work focusing on LLM components, this paper is the first to trace LVLM
hallucinations to visual encoders and identifies three key issues: statistical
bias, inherent bias, and vulnerability. To address these challenges, we propose
SHIELD, a training-free framework that mitigates hallucinations through three
strategies: re-weighting visual tokens to reduce statistical bias, introducing
noise-derived tokens to counter inherent bias, and applying adversarial attacks
with contrastive decoding to address vulnerability. Experiments demonstrate
that SHIELD effectively mitigates object hallucinations across diverse
benchmarks and LVLM families. Moreover, SHIELD achieves strong performance on
the general LVLM benchmark, highlighting its broad applicability. Code will be
released.

</details>


### [58] [VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.16598)
*Jiaying Zhu,Yurui Zhu,Xin Lu,Wenrui Yan,Dong Li,Kunlin Liu,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 本论文针对多模态大语言模型在处理高分辨率图像或多图输入时遇到的大量视觉token引发的计算和内存瓶颈问题，提出了一种轻量级、端到端可学习的token压缩框架VisionSelector，显著提高了模型效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的token压缩方法大多依赖启发式规则，容易丢失关键信息，同时在高压缩比下性能骤降，且存在诸如attention sinks等偏差，亟需一种泛化能力更强、更智能的token压缩方案。

Method: 作者提出VisionSelector，一个与大模型主干解耦的score模块，采用可微分的Top-K机制和curriculum annealing策略，实现端到端、可调整压缩率的token选择，参数量仅为12.85M，可灵活自适应不同压缩需求。

Result: VisionSelector在多种压缩比下都表现出很好的泛化性，能有效识别关键信息。结果显示，在仅保留30% token时仍可在MME数据集上保持100%准确率，在10%保留情况下超越前人12.14%，prefill速度提升一倍。

Conclusion: VisionSelector为token压缩提供了高效、灵活且易于集成的新方法，有效缓解了多模态大模型的计算与内存压力，兼具轻量性与优良性能，对实际应用具有重要意义。

Abstract: Multimodal Large Language Models (MLLMs) encounter significant computational
and memory bottlenecks from the massive number of visual tokens generated by
high-resolution images or multi-image inputs. Previous token compression
techniques are often constrained by heuristic rules that risk discarding
critical information. They may suffer from biases, such as attention sinks,
that lead to sharp performance drops under aggressive compression ratios. To
address these limitations, we reformulate token compression as a lightweight
plug-and-play framework that reformulates token compression into an end-to-end
learnable decision process. To be specific, we propose VisionSelector, a scorer
module decoupled from the MLLM backbone that incorporates a differentiable
Top-K mechanism and a curriculum annealing strategy to bridge the
training-inference gap, enabling efficient and adaptive token selection various
arbitrary compression rates. Remarkably lightweight with only 12.85M trainable
parameters, VisionSelector demonstrates generalization across various
compression rates and adaptively identifying critical tokens. This leads to
superior performance across all compression budgets, evidenced by preserving
100% accuracy on MME with 30% retention budget, outperforming prior methods by
12.14% at 10% retention budget, and doubling prefill speed. Our code is
available at https://github.com/JulietChoo/VisionSelector .

</details>


### [59] [A Deep Learning Framework for Real-Time Image Processing in Medical Diagnostics: Enhancing Accuracy and Speed in Clinical Applications](https://arxiv.org/abs/2510.16611)
*Melika Filvantorkaman,Maral Filvan Torkaman*

Main category: cs.CV

TL;DR: 本文提出了一种面向实时医学图像分析的深度学习框架，既提升了诊断准确率，又提高了计算效率，适用于多种医学影像模式，并且在实际测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像处理方法在诊断时仍耗时且效果受临床医生主观影响，且传统图像处理技术难以满足实时性、精准性和鲁棒性等临床需求。因此，开发一种高效、准确、实时的医学图像分析工具成为迫切需求。

Method: 文章提出结合U-Net、EfficientNet和基于Transformer的网络架构，配合模型剪枝、量化及GPU加速，通过深度学习实现X光、CT、MRI等多种影像的高效率分析，并以Grad-CAM和分割可视化增强模型解释性。系统支持部署于云端、本地服务器和边缘设备，可无缝集成PACS和EHR等临床系统。

Result: 在公开基准数据集上的实验显示，该系统分类准确率超过92%，分割Dice系数高于91%，单次推理时间小于80毫秒。

Conclusion: 所提出的深度学习框架有效加快了医学影像诊断流程，减少了医生负担，提升了AI在时间敏感医疗场景下的可信度和集成性。

Abstract: Medical imaging plays a vital role in modern diagnostics; however,
interpreting high-resolution radiological data remains time-consuming and
susceptible to variability among clinicians. Traditional image processing
techniques often lack the precision, robustness, and speed required for
real-time clinical use. To overcome these limitations, this paper introduces a
deep learning framework for real-time medical image analysis designed to
enhance diagnostic accuracy and computational efficiency across multiple
imaging modalities, including X-ray, CT, and MRI. The proposed system
integrates advanced neural network architectures such as U-Net, EfficientNet,
and Transformer-based models with real-time optimization strategies including
model pruning, quantization, and GPU acceleration. The framework enables
flexible deployment on edge devices, local servers, and cloud infrastructures,
ensuring seamless interoperability with clinical systems such as PACS and EHR.
Experimental evaluations on public benchmark datasets demonstrate
state-of-the-art performance, achieving classification accuracies above 92%,
segmentation Dice scores exceeding 91%, and inference times below 80
milliseconds. Furthermore, visual explanation tools such as Grad-CAM and
segmentation overlays enhance transparency and clinical interpretability. These
results indicate that the proposed framework can substantially accelerate
diagnostic workflows, reduce clinician workload, and support trustworthy AI
integration in time-critical healthcare environments.

</details>


### [60] [MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models](https://arxiv.org/abs/2510.16641)
*Young-Jun Lee,Byung-Kwan Lee,Jianshu Zhang,Yechan Hwang,Byungsoo Ko,Han-Gyu Kim,Dongyu Yao,Xuankun Rong,Eojin Joo,Seung-Ho Han,Bowon Ko,Ho-Jin Choi*

Main category: cs.CV

TL;DR: 本文提出了MultiVerse多轮对话基准，用于全面评测视觉-语言模型（VLMs）在复杂多轮对话中的能力，并展示了当前VLM在此任务下仍面临巨大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在单轮任务表现优异，但现实应用往往需要复杂的多轮对话。以往多轮数据集覆盖面与深度有限，无法充分考察模型的真实对话能力，因此需要构建更具挑战性的评测平台。

Method: 作者构建了MultiVerse数据集，包含647个平均四轮对话，覆盖484个任务和交互目标，涵盖知识、感知、推理等多种能力。评测体系引入基于GPT-4o的自动评价，37个评价维度包括感知准确性、语言清晰性和事实正确性等，并对18个主流VLM进行了系统评测。

Result: 在MultiVerse基准下，最强VLM（如GPT-4o）在复杂多轮对话中成功率仅为50%，说明任务具有极大挑战性。同时，给弱模型提供完整对话上下文能显著提升表现，体现上下文学习对提升模型对话能力的重要性。

Conclusion: MultiVerse是评估多轮视觉-语言对话能力的关键基准，揭示了现有模型在复杂多轮交互方面的短板，并为后续研究指明了优化方向。

Abstract: Vision-and-Language Models (VLMs) have shown impressive capabilities on
single-turn benchmarks, yet real-world applications often demand more intricate
multi-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only
partially capture the breadth and depth of conversational scenarios encountered
by users. In this work, we introduce MultiVerse, a novel multi-turn
conversation benchmark featuring 647 dialogues - each averaging four turns -
derived from a diverse set of 12 popular VLM evaluation benchmarks. With 484
tasks and 484 interaction goals, MultiVerse covers a wide range of topics, from
factual knowledge and perception to advanced reasoning tasks such as
mathematics and coding. To facilitate robust assessment, we propose a
checklist-based evaluation method that leverages GPT-4o as the automated
evaluator, measuring performance across 37 key aspects, including perceptual
accuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on
MultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve
only a 50% success rate in complex multi-turn conversations, highlighting the
dataset's challenging nature. Notably, we find that providing full dialogue
context significantly enhances performance for smaller or weaker models,
emphasizing the importance of in-context learning. We believe MultiVerse is a
landscape of evaluating multi-turn interaction abilities for VLMs.

</details>


### [61] [Universal and Transferable Attacks on Pathology Foundation Models](https://arxiv.org/abs/2510.16660)
*Yuntian Wang,Xilin Yang,Che-Yung Shen,Nir Pillar,Aydogan Ozcan*

Main category: cs.CV

TL;DR: 本文提出了一种针对病理学基础模型的通用可迁移对抗扰动(UTAP)，通过几乎不可察觉的固定噪声大幅削弱多种模型在下游任务上的表现，显示出基础模型的严重安全隐患。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型在病理学中的广泛应用，其在各种任务和数据分布上的泛化能力令人期待，但模型鲁棒性和安全性尚未被充分测试。作者试图揭示此类基础模型可能存在的共性安全脆弱点。

Method: 作者构建了一类通用、弱小但普适的对抗性扰动UTAP，该扰动通过深度学习优化得出，并可以直接添加到输入病理图像上。UTAP针对多个基础模型训练，并系统性测试了其在不同数据集、视野和模型上的攻击效能，包括对未见过的黑盒模型的迁移能力。

Result: UTAP在多个主流基础模型及多种数据集上的实验均导致了性能显著下降，包括广范围的误分类。该扰动不依赖于特定模型或数据集，其对输入图像的变化肉眼几乎不可察觉却效果显著。

Conclusion: UTAP揭示了病理学基础模型鲁棒性不足和易受攻击的普遍问题，为模型安全性评测设立了新基准。此研究强调需开发更强防御机制，如对抗训练，以确保AI在病理学中的安全、可靠应用。

Abstract: We introduce Universal and Transferable Adversarial Perturbations (UTAP) for
pathology foundation models that reveal critical vulnerabilities in their
capabilities. Optimized using deep learning, UTAP comprises a fixed and weak
noise pattern that, when added to a pathology image, systematically disrupts
the feature representation capabilities of multiple pathology foundation
models. Therefore, UTAP induces performance drops in downstream tasks that
utilize foundation models, including misclassification across a wide range of
unseen data distributions. In addition to compromising the model performance,
we demonstrate two key features of UTAP: (1) universality: its perturbation can
be applied across diverse field-of-views independent of the dataset that UTAP
was developed on, and (2) transferability: its perturbation can successfully
degrade the performance of various external, black-box pathology foundation
models - never seen before. These two features indicate that UTAP is not a
dedicated attack associated with a specific foundation model or image dataset,
but rather constitutes a broad threat to various emerging pathology foundation
models and their applications. We systematically evaluated UTAP across various
state-of-the-art pathology foundation models on multiple datasets, causing a
significant drop in their performance with visually imperceptible modifications
to the input images using a fixed noise pattern. The development of these
potent attacks establishes a critical, high-standard benchmark for model
robustness evaluation, highlighting a need for advancing defense mechanisms and
potentially providing the necessary assets for adversarial training to ensure
the safe and reliable deployment of AI in pathology.

</details>


### [62] [HYDRA: HYbrid knowledge Distillation and spectral Reconstruction Algorithm for high channel hyperspectral camera applications](https://arxiv.org/abs/2510.16664)
*Christopher Thirgood,Oscar Mendez,Erin Ling,Jon Storey,Simon Hadfield*

Main category: cs.CV

TL;DR: 论文提出了一种创新的光谱重建方法HYDRA，通过知识蒸馏框架显著提升了从三通道图像重建高光谱图像的性能，实现了更高的准确率和更快的推断速度。


<details>
  <summary>Details</summary>
Motivation: 传统的多尺度注意力方法在光谱通道较少时表现尚可，但无法满足现代高光谱传感器高通道（数百个通道）下的重建需求。因此，亟需新的方法提升稠密光谱数据的重建效果和泛化能力。

Method: 论文提出HYDRA架构，采用教师-学生模型。教师模型以隐式方式捕捉高光谱数据特征，学生模型负责从自然图像学习映射到教师模型编码的域。同时提出了新的训练方法以提升重建质量。

Result: HYDRA在所有评价指标上均达到当前最优（SOTA）水平，准确率提升18%，推断速度也优于现有最优方法，且在不同通道深度下表现优秀。

Conclusion: HYDRA方法能够有效弥补现有光谱重建模型的不足，尤其在高通道场景下表现优越，是HSI重建领域的重要进展。

Abstract: Hyperspectral images (HSI) promise to support a range of new applications in
computer vision. Recent research has explored the feasibility of generalizable
Spectral Reconstruction (SR), the problem of recovering a HSI from a natural
three-channel color image in unseen scenarios.
  However, previous Multi-Scale Attention (MSA) works have only demonstrated
sufficient generalizable results for very sparse spectra, while modern HSI
sensors contain hundreds of channels.
  This paper introduces a novel approach to spectral reconstruction via our
HYbrid knowledge Distillation and spectral Reconstruction Architecture (HYDRA).
  Using a Teacher model that encapsulates latent hyperspectral image data and a
Student model that learns mappings from natural images to the Teacher's encoded
domain, alongside a novel training method, we achieve high-quality spectral
reconstruction.
  This addresses key limitations of prior SR models, providing SOTA performance
across all metrics, including an 18\% boost in accuracy, and faster inference
times than current SOTA models at various channel depths.

</details>


### [63] [Pursuing Minimal Sufficiency in Spatial Reasoning](https://arxiv.org/abs/2510.16688)
*Yejie Guo,Yunzhong Hou,Wufei Ma,Meng Tang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为MSSR（Minimal Sufficient Spatial Reasoner）的新型空间推理框架，能够提升视觉-语言模型（VLMs）在3D空间推理任务中的能力，并取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型在空间推理方面表现不佳，主要受限于2D预训练导致的对3D理解不足，以及冗余3D信息带来的推理干扰。作者希望解决这两个关键瓶颈。

Method: 作者设计了MSSR双代理框架：第一是感知代理，利用专家模型分析3D场景，结合SOG模块抽取紧凑且与问答相关的空间信息；第二是推理代理，对信息进行精简和补充以获取最小且充分的信息集（MSS），形成闭环优化。

Result: 通过在两个具有挑战性的空间推理基准上进行大量实验，MSSR显著提升了模型的准确率，并达到当前最优结果。同时，其推理路径可解释性强。

Conclusion: 显式追求信息的充分性与最小性不仅可以提升视觉语言模型的空间推理性能，还有助于生成高质量训练数据，为后续模型打下基础。

Abstract: Spatial reasoning, the ability to ground language in 3D understanding,
remains a persistent challenge for Vision-Language Models (VLMs). We identify
two fundamental bottlenecks: inadequate 3D understanding capabilities stemming
from 2D-centric pre-training, and reasoning failures induced by redundant 3D
information. To address these, we first construct a Minimal Sufficient Set
(MSS) of information before answering a given question: a compact selection of
3D perception results from \textit{expert models}. We introduce MSSR (Minimal
Sufficient Spatial Reasoner), a dual-agent framework that implements this
principle. A Perception Agent programmatically queries 3D scenes using a
versatile perception toolbox to extract sufficient information, including a
novel SOG (Situated Orientation Grounding) module that robustly extracts
language-grounded directions. A Reasoning Agent then iteratively refines this
information to pursue minimality, pruning redundant details and requesting
missing ones in a closed loop until the MSS is curated. Extensive experiments
demonstrate that our method, by explicitly pursuing both sufficiency and
minimality, significantly improves accuracy and achieves state-of-the-art
performance across two challenging benchmarks. Furthermore, our framework
produces interpretable reasoning paths, offering a promising source of
high-quality training data for future models. Source code is available at
https://github.com/gyj155/mssr.

</details>


### [64] [SDPA++: A General Framework for Self-Supervised Denoising with Patch Aggregation](https://arxiv.org/abs/2510.16702)
*Huy Minh Nhat Nguyen,Triet Hoang Minh Dao,Chau Vinh Hoang Truong,Cuong Tuan Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种名为SDPA++的新型自监督OCT图像去噪方法，仅利用真实带噪声的OCT数据，通过自融合和自监督去噪生成伪真值，并采用拼块集成策略提升去噪效果，在实际数据集上展现了优异性能。


<details>
  <summary>Details</summary>
Motivation: OCT图像普遍受散斑噪声影响，然而由于临床实际条件限制，难以获得干净-带噪的配对数据，这极大限制了有监督去噪方法的应用。因此，发展无需干净参考、能有效提升临床OCT影像质量的新方法非常必要。

Method: 方法首先对真实带噪OCT图像进行自融合和自监督去噪生成伪真值，然后用这些伪真值作为目标，通过基于图像块的策略训练多模型集成，以提高清晰度和去噪性能。

Result: 在IEEE SPS VIP Cup真实OCT数据集上，利用CNR、MSR、TP和EP等指标评估，该方法在无干净参考的前提下显著提升了OCT图像质量。

Conclusion: SDPA++为OCT图像无监督去噪提供了一套通用框架，不依赖于干净-带噪配对样本，就能有效提升诊断图像质量，有很大潜力推动临床OCT成像和诊断效果。

Abstract: Optical Coherence Tomography (OCT) is a widely used non-invasive imaging
technique that provides detailed three-dimensional views of the retina, which
are essential for the early and accurate diagnosis of ocular diseases.
Consequently, OCT image analysis and processing have emerged as key research
areas in biomedical imaging. However, acquiring paired datasets of clean and
real-world noisy OCT images for supervised denoising models remains a
formidable challenge due to intrinsic speckle noise and practical constraints
in clinical imaging environments. To address these issues, we propose SDPA++: A
General Framework for Self-Supervised Denoising with Patch Aggregation. Our
novel approach leverages only noisy OCT images by first generating
pseudo-ground-truth images through self-fusion and self-supervised denoising.
These refined images then serve as targets to train an ensemble of denoising
models using a patch-based strategy that effectively enhances image clarity.
Performance improvements are validated via metrics such as Contrast-to-Noise
Ratio (CNR), Mean Square Ratio (MSR), Texture Preservation (TP), and Edge
Preservation (EP) on the real-world dataset from the IEEE SPS Video and Image
Processing Cup. Notably, the VIP Cup dataset contains only real-world noisy OCT
images without clean references, highlighting our method's potential for
improving image quality and diagnostic outcomes in clinical practice.

</details>


### [65] [Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization](https://arxiv.org/abs/2510.16704)
*Tianxin Wei,Yifan Chen,Xinrui He,Wenxuan Bao,Jingrui He*

Main category: cs.CV

TL;DR: 本文关注域泛化问题，即模型在训练和测试分布存在偏移时的泛化能力，提出了一种新的对比学习方法DCCL，有效提升模型在不同域上的泛化能力，并在多个公开基准上取得了比现有方法更优的性能表现。


<details>
  <summary>Details</summary>
Motivation: 实际应用中训练和测试数据分布常存在差异，导致模型泛化能力下降，域泛化（DG）旨在无需目标域数据，实现对未知域样本的准确预测。虽然对比学习能学习分类区分性强的表征，但直接应用在DG问题上反而性能下降，因此需要提高域间表征的连通性。

Method: 提出领域连接对比学习（DCCL）范式，核心做法包括：1）在数据层面，采用更强的数据增强并引入跨域正样本对，以增强同类别样本的连通性；2）在模型层面，引入模型锚定（model anchoring）和生成变换损失，从预训练表征中挖掘类内连通性，并提升对未见测试域的泛化能力。

Result: 在五个主流DG基准数据集上进行实验证明，DCCL相较于当前最优的基线方法在无领域监督情况下取得了更好的泛化表现。

Conclusion: DCCL有效提升了对比学习在域泛化任务上的性能，为应对分布偏移问题提供了新的解决范式，并具有较好的实用价值。

Abstract: Distribution shifts between training and testing samples frequently occur in
practice and impede model generalization performance. This crucial challenge
thereby motivates studies on domain generalization (DG), which aim to predict
the label on unseen target domain data by solely using data from source
domains. It is intuitive to conceive the class-separated representations
learned in contrastive learning (CL) are able to improve DG, while the reality
is quite the opposite: users observe directly applying CL deteriorates the
performance. We analyze the phenomenon with the insights from CL theory and
discover lack of intra-class connectivity in the DG setting causes the
deficiency. We thus propose a new paradigm, domain-connecting contrastive
learning (DCCL), to enhance the conceptual connectivity across domains and
obtain generalizable representations for DG. On the data side, more aggressive
data augmentation and cross-domain positive samples are introduced to improve
intra-class connectivity. On the model side, to better embed the unseen test
domains, we propose model anchoring to exploit the intra-class connectivity in
pre-trained representations and complement the anchoring with generative
transformation loss. Extensive experiments on five standard DG benchmarks are
performed. The results verify that DCCL outperforms state-of-the-art baselines
even without domain supervision. The detailed model implementation and the code
are provided through https://github.com/weitianxin/DCCL

</details>


### [66] [HumanCM: One Step Human Motion Prediction](https://arxiv.org/abs/2510.16709)
*Liu Haojie,Gao Suixiang*

Main category: cs.CV

TL;DR: 本文提出了HumanCM，一种基于一致性模型的人体运动预测新框架，实现了比扩散模型更高效的一步预测，并兼具准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的人体运动预测常用扩散模型，需要多步去噪，计算量大、推理速度慢，因此亟需更高效的预测方法。

Method: 提出HumanCM，基于一致性模型，通过学习有噪声到无噪声运动状态之间的自洽映射，仅需一步生成，采用基于Transformer的时空结构，并引入时序嵌入以建模长期依赖。

Result: 在Human3.6M和HumanEva-I数据集上的实验显示，HumanCM模型在预测准确性方面达到或超过最新扩散模型，同时推理步数减少最多可达百倍。

Conclusion: HumanCM在保持高预测准确性的同时，显著提升了推理效率，为人体运动预测提供了高效且有竞争力的新方法。

Abstract: We present HumanCM, a one-step human motion prediction framework built upon
consistency models. Instead of relying on multi-step denoising as in
diffusion-based methods, HumanCM performs efficient single-step generation by
learning a self-consistent mapping between noisy and clean motion states. The
framework adopts a Transformer-based spatiotemporal architecture with temporal
embeddings to model long-range dependencies and preserve motion coherence.
Experiments on Human3.6M and HumanEva-I demonstrate that HumanCM achieves
comparable or superior accuracy to state-of-the-art diffusion models while
reducing inference steps by up to two orders of magnitude.

</details>


### [67] [Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes](https://arxiv.org/abs/2510.16714)
*Xiongkun Linghu,Jiangyong Huang,Ziyu Zhu,Baoxiong Jia,Siyuan Huang*

Main category: cs.CV

TL;DR: 本论文提出了一种新的3D大模型推理框架，通过引入基于场景的链式思维（Chain-of-Thought, CoT）推理方法，实现了更具人类特征的3D场景问答，并首次构建了大规模3D场景推理数据集SCENECOT-185K。在多个复杂推理基准测试上取得了强劲表现。


<details>
  <summary>Details</summary>
Motivation: 现有3D大模型在场景问答中难以实现真正的“锚定”推理，主要是由于对类人场景-物体推理机制的探索不足，因此亟需一个升级的推理框架来提升模型在3D任务中的理解力和推理过程的可解释性。

Method: 作者提出SCENECOT框架，将复杂推理任务分解为更易处理的小问题，并引入多模态专家模块来构建视觉线索。为此，团队还构建了SCENECOT-185K数据集，包含18.5万条高质量的人类推理实例。

Result: 通过在多个复杂3D场景推理测试集上的实验，验证该方法在推理准确率和问答一致性上均表现优异，且具备很高的人类类比能力。

Conclusion: 本研究首次将链式思维推理方法成功应用于3D理解领域，有效实现了类人逐步推理，框架具备良好的扩展性，有望应用于更广泛的3D场景理解任务。

Abstract: Existing research on 3D Large Language Models (LLMs) still struggles to
achieve grounded question-answering, primarily due to the under-exploration of
the mech- anism of human-like scene-object grounded reasoning. This paper
bridges the gap by presenting a novel framework. We first introduce a grounded
Chain-of- Thought reasoning method in 3D scenes (SCENECOT), decoupling a
complex reasoning task into simpler and manageable problems, and building
corresponding visual clues based on multimodal expert modules. To enable such a
method, we develop SCENECOT-185K, the first large-scale grounded CoT reasoning
dataset, consisting of 185K high-quality instances. Extensive experiments
across various complex 3D scene reasoning benchmarks demonstrate that our new
framework achieves strong performance with high grounding-QA coherence. To the
best of our knowledge, this is the first successful application of CoT
reasoning to 3D scene understanding, enabling step-by-step human-like reasoning
and showing potential for extension to broader 3D scene understanding
scenarios.

</details>


### [68] [Vision-Centric 4D Occupancy Forecasting and Planning via Implicit Residual World Models](https://arxiv.org/abs/2510.16729)
*Jianbiao Mei,Yu Yang,Xuemeng Yang,Licheng Wen,Jiajun Lv,Botian Shi,Yong Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为IR-WM的隐式残差世界模型，大幅提升了自动驾驶视觉世界建模的效率和精度，在nuScenes基准上获得了领先的4D占用预测和轨迹规划表现。


<details>
  <summary>Details</summary>
Motivation: 传统视觉世界模型在自动驾驶场景下存在对未来场景冗余重建的问题，尤其在静态背景建模上浪费了大量容量，限制了系统的性能和泛化能力。本文旨在提升模型对动态变化建模的能力，同时减少资源消耗。

Method: 作者提出IR-WM（Implicit Residual World Model），首先通过视觉观测生成当前状态的鸟瞰视角（BEV）表征。然后利用前一时刻的BEV特征作为时序先验，仅预测残差（即受自车决策和环境变化影响的部分）。为防止误差累积，引入了对齐模块校正语义和动态偏差；并比较了不同的预测-规划耦合方案。

Result: 实验在nuScenes数据集上表明，IR-WM在4D占用预测和轨迹规划任务中均获得了顶级性能。

Conclusion: IR-WM通过专注于动态残差和多重对齐，有效提升了自动驾驶视觉世界模型的效率和准确率，为未来自动驾驶世界建模和规划任务提供了新的范式。

Abstract: End-to-end autonomous driving systems increasingly rely on vision-centric
world models to understand and predict their environment. However, a common
ineffectiveness in these models is the full reconstruction of future scenes,
which expends significant capacity on redundantly modeling static backgrounds.
To address this, we propose IR-WM, an Implicit Residual World Model that
focuses on modeling the current state and evolution of the world. IR-WM first
establishes a robust bird's-eye-view representation of the current state from
the visual observation. It then leverages the BEV features from the previous
timestep as a strong temporal prior and predicts only the "residual", i.e., the
changes conditioned on the ego-vehicle's actions and scene context. To
alleviate error accumulation over time, we further apply an alignment module to
calibrate semantic and dynamic misalignments. Moreover, we investigate
different forecasting-planning coupling schemes and demonstrate that the
implicit future state generated by world models substantially improves planning
accuracy. On the nuScenes benchmark, IR-WM achieves top performance in both 4D
occupancy forecasting and trajectory planning.

</details>


### [69] [UKANFormer: Noise-Robust Semantic Segmentation for Coral Reef Mapping via a Kolmogorov-Arnold Network-Transformer Hybrid](https://arxiv.org/abs/2510.16730)
*Tianyang Dou,Ming Li,Jiangying Qin,Xuan Liao,Jiageng Zhong,Armin Gruen,Mengyi Deng*

Main category: cs.CV

TL;DR: 该论文提出了一种新型语义分割模型UKANFormer，用于提升对珊瑚礁的高精度大尺度制图，能够克服现有低质量标签带来的限制。


<details>
  <summary>Details</summary>
Motivation: 现有的全球珊瑚礁分布制图产品虽覆盖广，但空间精度有限、语义一致性差，难以精细化界定珊瑚礁边界，限制了有效保护。亟需在数据标签质量不高的情况下，提高地图精度。

Method: 提出UKANFormer模型，基于UKAN架构并结合Global-Local Transformer（GL-Trans）模块，增强解码器对全局语义信息和局部边界细节的提取能力，专门处理Allen Coral Atlas产生的噪声标签。通过与主流基线模型对比实验，检验其性能。

Result: UKANFormer在有噪声标签的条件下，珊瑚类别IoU达到67.00%，像素精度83.98%，显著优于传统基线模型，生成的分割结果在视觉和结构上均优于训练所用的噪声标签。

Conclusion: UKANFormer证明了通过模型结构创新可以缓解标签噪声带来的负面影响，即数据质量并非性能上限，可实现高精度可扩展珊瑚礁制图，对缺乏高质量标签的生态监测具有重要推动作用。

Abstract: Coral reefs are vital yet fragile ecosystems that require accurate
large-scale mapping for effective conservation. Although global products such
as the Allen Coral Atlas provide unprecedented coverage of global coral reef
distri-bution, their predictions are frequently limited in spatial precision
and semantic consistency, especially in regions requiring fine-grained boundary
delineation. To address these challenges, we propose UKANFormer, a novel
se-mantic segmentation model designed to achieve high-precision mapping under
noisy supervision derived from Allen Coral Atlas. Building upon the UKAN
architecture, UKANFormer incorporates a Global-Local Transformer (GL-Trans)
block in the decoder, enabling the extraction of both global semantic
structures and local boundary details. In experiments, UKANFormer achieved a
coral-class IoU of 67.00% and pixel accuracy of 83.98%, outperforming
conventional baselines under the same noisy labels setting. Remarkably, the
model produces predictions that are visually and structurally more accurate
than the noisy labels used for training. These results challenge the notion
that data quality directly limits model performance, showing that architectural
design can mitigate label noise and sup-port scalable mapping under imperfect
supervision. UKANFormer provides a foundation for ecological monitoring where
reliable labels are scarce.

</details>


### [70] [A Comprehensive Survey on World Models for Embodied AI](https://arxiv.org/abs/2510.16732)
*Xinqing Li,Xin He,Le Zhang,Yun Liu*

Main category: cs.CV

TL;DR: 本文对具身智能（Embodied AI）领域中的世界模型进行了系统性综述，提出了统一的框架和三维分类法，总结主流模型、数据资源、评测指标，并指出当前面临的核心挑战。


<details>
  <summary>Details</summary>
Motivation: 随着AI在机器人、自动驾驶等实际物理环境中的应用增加，AI体需要能够感知、行动并预测其行为对世界的影响，因此对能够模拟世界动态、支持决策的世界模型需求愈发迫切。

Method: 作者提出了具身智能世界模型的三轴分类法：（1）功能性：决策耦合型vs.通用型；（2）时间建模：序列模拟推理vs.全局变化预测；（3）空间表示：全局隐向量、特征序列、空间隐格、分解渲染等。对现有方法的数据与评测体系进行系统综述，并对主流模型进行定量比较。

Result: 系统化了世界模型的基本问题、学习目标、类别体系，展示了从机器人到视频理解领域相关的数据和指标，对现有最先进模型进行了量化对比，并整理了现有的开源文献。

Conclusion: 世界模型作为支持具身智能的关键组件，虽已取得进展，但在统一数据集稀缺、物理一致性评估、实时效率与长时序一致性等方面仍有重大挑战需解决。该综述为后续工作指明了方向。

Abstract: Embodied AI requires agents that perceive, act, and anticipate how actions
reshape future world states. World models serve as internal simulators that
capture environment dynamics, enabling forward and counterfactual rollouts to
support perception, prediction, and decision making. This survey presents a
unified framework for world models in embodied AI. Specifically, we formalize
the problem setting and learning objectives, and propose a three-axis taxonomy
encompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2)
Temporal Modeling, Sequential Simulation and Inference vs. Global Difference
Prediction; (3) Spatial Representation, Global Latent Vector, Token Feature
Sequence, Spatial Latent Grid, and Decomposed Rendering Representation. We
systematize data resources and metrics across robotics, autonomous driving, and
general video settings, covering pixel prediction quality, state-level
understanding, and task performance. Furthermore, we offer a quantitative
comparison of state-of-the-art models and distill key open challenges,
including the scarcity of unified datasets and the need for evaluation metrics
that assess physical consistency over pixel fidelity, the trade-off between
model performance and the computational efficiency required for real-time
control, and the core modeling difficulty of achieving long-horizon temporal
consistency while mitigating error accumulation. Finally, we maintain a curated
bibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.

</details>


### [71] [Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling](https://arxiv.org/abs/2510.16751)
*Erik Riise,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: 论文表明，在图像生成任务中，视觉自回归模型（尤其是结合beam search搜索）在推理优化上比连续扩散模型更具优势，即便参数规模较小也能超越大模型。


<details>
  <summary>Details</summary>
Motivation: 虽然搜索方法极大提升了大语言模型的推理效果，但相同方法在图像扩散模型中的提升有限。作者希望探索是否可通过改变模型架构和推理策略，提升图像生成质量。

Method: 作者将beam search等搜索策略应用于离散化、序列式的视觉自回归模型，并与扩散模型进行对比实验，通过系统性消融分析探讨两种架构在推理优化上的差异。

Result: 2B参数的视觉自回归模型，在结合beam search后，图像生成质量优于采用随机采样的12B参数的扩散模型。离散token空间支持早期剪枝和计算复用。

Conclusion: 模型架构对推理时间的优化至关重要，离散自回归模型比连续扩散模型更能从搜索中获益，推理优化不仅依赖模型规模，更要关注模型结构本身。

Abstract: While inference-time scaling through search has revolutionized Large Language
Models, translating these gains to image generation has proven difficult.
Recent attempts to apply search strategies to continuous diffusion models show
limited benefits, with simple random sampling often performing best. We
demonstrate that the discrete, sequential nature of visual autoregressive
models enables effective search for image generation. We show that beam search
substantially improves text-to-image generation, enabling a 2B parameter
autoregressive model to outperform a 12B parameter diffusion model across
benchmarks. Systematic ablations show that this advantage comes from the
discrete token space, which allows early pruning and computational reuse, and
our verifier analysis highlights trade-offs between speed and reasoning
capability. These findings suggest that model architecture, not just scale, is
critical for inference-time optimization in visual generation.

</details>


### [72] [Prominence-Aware Artifact Detection and Dataset for Image Super-Resolution](https://arxiv.org/abs/2510.16752)
*Ivan Molodetskikh,Kirill Malyshev,Mark Mirgaleev,Nikita Zagainov,Evgeney Bogatyrev,Dmitriy Vatolin*

Main category: cs.CV

TL;DR: 本文提出了一个包含超分辨率方法生成伪影及其突出度评分的新数据集，并基于此开发了优于现有方法的伪影显著性检测模型。


<details>
  <summary>Details</summary>
Motivation: 超分辨率（SR）模型虽然提升了视觉质量，但也容易产生降低图像观感的伪影。现有研究通常将伪影视为同质的二元瑕疵，未能区分其对人眼观感的不同影响。为更有效地评估和缓解SR伪影，有必要按照伪影对人类观察者的突出程度进行表征和评分。

Method: 作者收集了来自11种主流SR方法的1302个伪影样本，并通过众包给每个样本打上突出度分数，构建了新数据集。在此基础上，训练一个轻量化回归模型，生成空间上的伪影显著性热图，从而自动检测和量化图像中最显著的伪影。

Result: 实验显示，该回归模型在突出伪影检测任务上优于现有方法。数据集及代码均已发布，为后续社区研发基于显著性的伪影缓解与评估工具提供了数据基础。

Conclusion: 文中方法能评估并量化SR生成图像中更具视觉影响的伪影，有助于改进SR模型的客观评价与实际应用视觉质量。

Abstract: Generative image super-resolution (SR) is rapidly advancing in visual quality
and detail restoration. As the capacity of SR models expands, however, so does
their tendency to produce artifacts: incorrect, visually disturbing details
that reduce perceived quality. Crucially, their perceptual impact varies: some
artifacts are barely noticeable while others strongly degrade the image. We
argue that artifacts should be characterized by their prominence to human
observers rather than treated as uniform binary defects. Motivated by this, we
present a novel dataset of 1302 artifact examples from 11 contemporary image-SR
methods, where each artifact is paired with a crowdsourced prominence score.
Building on this dataset, we train a lightweight regressor that produces
spatial prominence heatmaps and outperforms existing methods at detecting
prominent artifacts. We release the dataset and code to facilitate
prominence-aware evaluation and mitigation of SR artifacts.

</details>


### [73] [WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and Mamba-based Channel Modeling with Texture Enhancement](https://arxiv.org/abs/2510.16765)
*Shengyu Zhu,Fan,Fuxuan Zhang*

Main category: cs.CV

TL;DR: 该论文提出了WaMaIR框架，通过扩展感受野与增强通道特征建模能力，显著提升了图像复原的细节与纹理表现，超越了现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN的图像复原方法在还原精细纹理细节方面存在不足，主要受限于网络的感受野较小和对通道特征挖掘不充分，导致复原图像细节不佳。

Method: 1. 提出WaMaIR框架，具有大感受野与强通道关系建模能力；2. 引入全局多尺度小波变换卷积（GMWTConvs），用于扩展感受野并丰富输入的纹理特征；3. 设计基于Mamba的通道感知模块（MCAM），捕捉通道内长距离依赖，提高对颜色、边缘和纹理的敏感性；4. 新定义多尺度纹理增强损失（MTELoss），引导模型更好地恢复细节纹理结构。

Result: 大量实验表明，WaMaIR在多个指标上优于最新方法，在提升恢复效果的同时，保持了较高的计算效率。

Conclusion: WaMaIR在图像复原任务中不仅有效提升了细节和纹理还原能力，而且兼顾了模型效率，为后续相关研究提供了新的方法思路。

Abstract: Image restoration is a fundamental and challenging task in computer vision,
where CNN-based frameworks demonstrate significant computational efficiency.
However, previous CNN-based methods often face challenges in adequately
restoring fine texture details, which are limited by the small receptive field
of CNN structures and the lack of channel feature modeling. In this paper, we
propose WaMaIR, which is a novel framework with a large receptive field for
image perception and improves the reconstruction of texture details in restored
images. Specifically, we introduce the Global Multiscale Wavelet Transform
Convolutions (GMWTConvs) for expandding the receptive field to extract image
features, preserving and enriching texture features in model inputs. Meanwhile,
we propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to
capture long-range dependencies within feature channels, which enhancing the
model sensitivity to color, edges, and texture information. Additionally, we
propose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to
guide the model in preserving detailed texture structures effectively.
Extensive experiments confirm that WaMaIR outperforms state-of-the-art methods,
achieving better image restoration and efficient computational performance of
the model.

</details>


### [74] [Region in Context: Text-condition Image editing with Human-like semantic reasoning](https://arxiv.org/abs/2510.16772)
*Thuy Phuong Vu,Dinh-Cuong Hoang,Minhhuy Le,Phan Xuan Tan*

Main category: cs.CV

TL;DR: 本文提出了一种新的文本驱动图像编辑框架，能够在保持整体语义一致性的同时，对图像局部区域进行精确编辑。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的图像区域编辑方法，往往只关注局部区域，忽略每一部分与整体构图和语义的一致性，易造成编辑区域与整幅图像之间的不一致、突兀或语义失真。本文旨在解决该局部与全局协同编辑的难题。

Method: 提出了Region in Context框架，通过多层次的视觉-语言语义对齐机制实现编辑。具体来说，局部区域的特征结合了整图上下文，并与区域级详细自然语言描述对齐。同时，整幅图像被映射到由大规模视觉-语言模型生成的场景整体描述，实现区域与整体间的双重引导。

Result: 实验证明，该方法在图像编辑一致性、自然过渡以及与文本指令的匹配度方面，效果优于现有主流方法。

Conclusion: 文中框架有效提升了基于文本的图像区域编辑的整体语义和细节一致性，是图像编辑从单一局部处理到全局协同理解的重要进步。

Abstract: Recent research has made significant progress in localizing and editing image
regions based on text. However, most approaches treat these regions in
isolation, relying solely on local cues without accounting for how each part
contributes to the overall visual and semantic composition. This often results
in inconsistent edits, unnatural transitions, or loss of coherence across the
image. In this work, we propose Region in Context, a novel framework for
text-conditioned image editing that performs multilevel semantic alignment
between vision and language, inspired by the human ability to reason about
edits in relation to the whole scene. Our method encourages each region to
understand its role within the global image context, enabling precise and
harmonized changes. At its core, the framework introduces a dual-level guidance
mechanism: regions are represented with full-image context and aligned with
detailed region-level descriptions, while the entire image is simultaneously
matched to a comprehensive scene-level description generated by a large
vision-language model. These descriptions serve as explicit verbal references
of the intended content, guiding both local modifications and global structure.
Experiments show that it produces more coherent and instruction-aligned
results. Code is available at:
https://github.com/thuyvuphuong/Region-in-Context.git

</details>


### [75] [EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation](https://arxiv.org/abs/2510.16776)
*Mingzheng Zhang,Jinfeng Gao,Dan Xu,Jiangrui Yu,Yuhan Qiao,Lan Chen,Jin Tang,Xiao Wang*

Main category: cs.CV

TL;DR: 本文提出了一种面向X光医学图像报告生成的新方法，利用高效的参数微调技术，将先进的Mamba视觉模型与大语言模型结合，实现了优异的端到端自动医学报告生成效果。


<details>
  <summary>Details</summary>
Motivation: 医学影像报告生成能够减轻医生工作负担和患者等待时间。目前主流方法过度依赖大语言模型，极少探索视觉基础模型及其高效微调策略，且多忽视交叉注意力机制改进，同时对于新型非Transformer架构（如Mamba网络）在此领域的应用几乎空白，因此亟需新探索。

Method: 作者提出EMRRG框架，采用Mamba网络作为视觉主干，通过参数高效微调（如Partial LoRA）处理X光图像patches，实现更优特征提取。再利用融合解码器的LLM输出医学报告，实现端到端训练。方法在三个权威数据集上进行了对比实验。

Result: 新方法在三大公开基准集上取得了优异表现，实验全面验证了其策略的有效性。

Conclusion: 采用Mamba网络、高效微调与融合解码器的新框架，有效提升了X光图像医学报告自动生成的准确性和效率，展示了非Transformer架构在该领域的广阔前景。

Abstract: X-ray image-based medical report generation (MRG) is a pivotal area in
artificial intelligence that can significantly reduce diagnostic burdens for
clinicians and patient wait times. Existing MRG models predominantly rely on
Large Language Models (LLMs) to improve report generation, with limited
exploration of pre-trained vision foundation models or advanced fine-tuning
techniques. Mainstream frameworks either avoid fine-tuning or utilize
simplistic methods like LoRA, often neglecting the potential of enhancing
cross-attention mechanisms. Additionally, while Transformer-based models
dominate vision-language tasks, non-Transformer architectures, such as the
Mamba network, remain underexplored for medical report generation, presenting a
promising avenue for future research. In this paper, we propose EMRRG, a novel
X-ray report generation framework that fine-tunes pre-trained Mamba networks
using parameter-efficient methods. Specifically, X-ray images are divided into
patches, tokenized, and processed by an SSM-based vision backbone for feature
extraction, with Partial LoRA yielding optimal performance. An LLM with a
hybrid decoder generates the medical report, enabling end-to-end training and
achieving strong results on benchmark datasets. Extensive experiments on three
widely used benchmark datasets fully validated the effectiveness of our
proposed strategies for the X-ray MRG. The source code of this paper will be
released on https://github.com/Event-AHU/Medical_Image_Analysis.

</details>


### [76] [GS2POSE: Marry Gaussian Splatting to 6D Object Pose Estimation](https://arxiv.org/abs/2510.16777)
*Junbo Li,Weimin Yuan,Yinuo Wang,Yue Zeng,Shihao Shu,Cai Meng,Xiangzhi Bai*

Main category: cs.CV

TL;DR: 本文提出GS2POSE，一种新颖的6D物体位姿估计算法，能够更好地处理无纹理物体和光照变化问题，并在多个数据集上优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 传统的6D位姿估计方法在处理无纹理物体和不同光照条件下表现不佳，亟需一种更鲁棒且适应性强的估计算法。

Method: GS2POSE受Bundle Adjustment的启发，基于Lie代数，将3DGS（3D高斯球）扩展为可微分的渲染流程。该算法通过对比输入图片和渲染图片，迭代优化物体的位姿，同时还动态更新3DGS模型的颜色参数，以适应不同光照。

Result: GS2POSE在T-LESS、LineMod-Occlusion和LineMod数据集上，准确率分别提升1.4%、2.8%和2.5%。

Conclusion: GS2POSE有效提高了6D位姿估计的精度和鲁棒性，特别是在无纹理和光照变化场景下，表现优于现有主流方法。

Abstract: Accurate 6D pose estimation of 3D objects is a fundamental task in computer
vision, and current research typically predicts the 6D pose by establishing
correspondences between 2D image features and 3D model features. However, these
methods often face difficulties with textureless objects and varying
illumination conditions. To overcome these limitations, we propose GS2POSE, a
novel approach for 6D object pose estimation. GS2POSE formulates a pose
regression algorithm inspired by the principles of Bundle Adjustment (BA). By
leveraging Lie algebra, we extend the capabilities of 3DGS to develop a
pose-differentiable rendering pipeline, which iteratively optimizes the pose by
comparing the input image to the rendered image. Additionally, GS2POSE updates
color parameters within the 3DGS model, enhancing its adaptability to changes
in illumination. Compared to previous models, GS2POSE demonstrates accuracy
improvements of 1.4\%, 2.8\% and 2.5\% on the T-LESS, LineMod-Occlusion and
LineMod datasets, respectively.

</details>


### [77] [Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features](https://arxiv.org/abs/2510.16781)
*Shihao Ji,Zihui Song*

Main category: cs.CV

TL;DR: 本文提出了一种全新的、无需训练的视频理解框架，结合了预训练视觉语言模型（VLM）的语义先验和经典机器学习算法，实现对视频内容的自动结构化分析与总结。


<details>
  <summary>Details</summary>
Motivation: 当前大规模视觉语言模型在静态图像上的零样本推理能力很强，但尚未有效应用于视频领域。传统视频理解依赖大量标注数据的训练，成本高且扩展性有限，因此需要一种高效、无需数据标注的新方法。

Method: 提出将视频理解转化为自监督的时空聚类问题：先用VLM的视觉编码器将视频转换为高维语义特征轨迹，再用Kernel Temporal Segmentation算法把特征流分段为连续的语义事件，接着对这些片段做无监督密度聚类，发现视频中的重复场景和主题，最后选取关键帧，由VLM生成文字描述，形成结构化、多模态视频摘要。整个过程无需端到端训练。

Result: 该方法能高效发现视频中的核心事件、场景和主题，并自动生成结构化多模态摘要，具备良好的可解释性和通用性。

Conclusion: 本文框架为零样本、自动化、结构化视频内容分析提供了一种高效、解释性强、模型无关的新途径，推动了视频理解领域的发展。

Abstract: The remarkable zero-shot reasoning capabilities of large-scale Visual
Language Models (VLMs) on static images have yet to be fully translated to the
video domain. Conventional video understanding models often rely on extensive,
task-specific training on annotated datasets, a process that is both costly and
limited in scalability. This paper introduces a novel, training-free framework
for video understanding that circumvents end-to-end training by synergistically
combining the rich semantic priors of pre-trained VLMs with classic machine
learning algorithms for pattern discovery. Our core idea is to reframe video
understanding as a self-supervised spatio-temporal clustering problem within a
high-dimensional semantic feature space. The proposed pipeline first transforms
a video stream into a semantic feature trajectory using the frozen visual
encoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal
Segmentation (KTS), a robust machine learning technique, to partition the
continuous feature stream into discrete, semantically coherent event segments.
These segments are then subjected to unsupervised density-based clustering to
identify recurring macroscopic scenes and themes throughout the video. By
selecting representative keyframes from each discovered cluster and leveraging
the VLM's generative capabilities for textual description, our framework
automatically produces a structured, multi-modal summary of the video content.
This approach provides an effective, interpretable, and model-agnostic pathway
for zero-shot, automated structural analysis of video content.

</details>


### [78] [Segmentation as A Plug-and-Play Capability for Frozen Multimodal LLMs](https://arxiv.org/abs/2510.16785)
*Jiazhen Liu,Long Chen*

Main category: cs.CV

TL;DR: 本文提出LENS，一种可插拔的方案，无需对多模态大模型（MLLMs）微调即可实现分割能力，且保持模型泛化性。其方法通过冻结MLLM，仅在其上添加轻量级可训练头，利用注意力图提取关键点，生成适用于分割的特征。实验结果显示，LENS分割效果优于多数重训练方法，并保持原有模型的多任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型正朝着一体化能力方向发展，将分割功能整合到统一模型极具挑战性。传统要通过微调来实现像素级分割，往往损害模型对其它任务的泛化能力，违背构建通用模型的目标，因此需要新的设计以同时兼顾分割效果和模型通用性。

Method: 提出LENS方法：无需对MLLM整体微调，而是在冻结的MLLM上附加一个轻量可训练头；结合模型注意力图，自动提取关键点，转化为适合分割的点特征，并直接对接分割掩码解码器。

Result: LENS在大量实验中实现了与或优于微调重训练方法的分割性能，并且完全保持了MLLM的原有泛化能力，无须牺牲通用性。

Conclusion: LENS为扩展MLLM能力提供了简洁高效的路径，在不影响模型统一性的前提下，成功赋予了像素级分割能力，为未来多才多艺、多任务统一的大模型奠定基础。

Abstract: Integrating diverse visual capabilities into a unified model is a significant
trend in Multimodal Large Language Models (MLLMs). Among these, the inclusion
of segmentation poses a distinct set of challenges. To equip MLLMs with
pixel-level segmentation abilities, prevailing methods require finetuning the
model to produce specific outputs compatible with a mask decoder. This process
typically alters the model's output space and compromises its intrinsic
generalization, which undermines the goal of building a unified model. We
introduce LENS (Leveraging kEypoiNts for MLLMs' Segmentation), a novel
plug-and-play solution. LENS attaches a lightweight, trainable head to a
completely frozen MLLM. By refining the spatial cues embedded in attention
maps, LENS extracts keypoints and describes them into point-wise features
directly compatible with the mask decoder. Extensive experiments validate our
approach: LENS achieves segmentation performance competitive with or superior
to that of retraining-based methods. Crucially, it does so while fully
preserving the MLLM's generalization capabilities, which are significantly
degraded by finetuning approaches. As such, the attachable design of LENS
establishes an efficient and powerful paradigm for extending MLLMs, paving the
way for truly multi-talented, unified models.

</details>


### [79] [Unsupervised Monocular Road Segmentation for Autonomous Driving via Scene Geometry](https://arxiv.org/abs/2510.16790)
*Sara Hatami Rostami,Behrooz Nasihatkon*

Main category: cs.CV

TL;DR: 该论文提出了一种完全无监督的道路分割方法，无需依赖昂贵的人工标注数据，通过结合场景几何和时序信息，实现了高精度的道路与非道路区域分割。


<details>
  <summary>Details</summary>
Motivation: 为解决道路分割任务中对人工标注数据的高度依赖，从而降低成本并提升可扩展性，特别是在自动驾驶应用中，亟需有效的无监督道路分割方法。

Method: 方法分为两阶段：首先利用几何先验（如地平线以上为非道路、车辆前方特定四边形区域为道路）生成弱标签；然后通过时序一致性对这些标签进行细化，通过局部特征点在多帧图像中的跟踪，利用互信息最大化来惩罚标签分配中的不一致，从而提升分割精度和时序稳定性。

Result: 在Cityscapes数据集上，该方法实现了0.82的IoU，表现出高精度，同时模型设计简单。

Conclusion: 实验结果显示，结合几何约束和时序一致性的方法可实现高效、可扩展的无监督道路分割，为自动驾驶等场景提供了有竞争力的解决方案。

Abstract: This paper presents a fully unsupervised approach for binary road
segmentation (road vs. non-road), eliminating the reliance on costly manually
labeled datasets. The method leverages scene geometry and temporal cues to
distinguish road from non-road regions. Weak labels are first generated from
geometric priors, marking pixels above the horizon as non-road and a predefined
quadrilateral in front of the vehicle as road. In a refinement stage, temporal
consistency is enforced by tracking local feature points across frames and
penalizing inconsistent label assignments using mutual information
maximization. This enhances both precision and temporal stability. On the
Cityscapes dataset, the model achieves an Intersection-over-Union (IoU) of
0.82, demonstrating high accuracy with a simple design. These findings
demonstrate the potential of combining geometric constraints and temporal
consistency for scalable unsupervised road segmentation in autonomous driving.

</details>


### [80] [Personalized Image Filter: Mastering Your Photographic Style](https://arxiv.org/abs/2510.16791)
*Chengxuan Zhu,Shuchen Weng,Jiacong Fang,Peixuan Zhang,Si Li,Chao Xu,Boxin Shi*

Main category: cs.CV

TL;DR: 本文提出了一种个性化图像滤镜（PIF）方法，能够学习和迁移不同摄影风格，并在保持图像内容的同时进行风格转换。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从参考图片中学习摄影概念和迁移风格时，无法有效提取有意义的摄影概念，或者难以很好地保留原图内容。因此，亟需一种能学习摄影风格并高质量迁移的解决方案。

Method: 基于预训练的文本到图像扩散模型，PIF通过生成先验学习摄影概念的平均外观，并根据文本提示调整它们。采用文本反演技术，通过优化与摄影概念相关的提示语，学习参考图像的摄影风格。

Result: PIF在提取并迁移多种摄影风格方面表现出色，实现了高质量的风格迁移并能较好地保留原始图像内容。

Conclusion: PIF方法有效解决了现有风格迁移方法无法同时学到摄影风格与保持内容的问题，为图像风格迁移提供了更灵活、个性化的解决方案。

Abstract: Photographic style, as a composition of certain photographic concepts, is the
charm behind renowned photographers. But learning and transferring photographic
style need a profound understanding of how the photo is edited from the unknown
original appearance. Previous works either fail to learn meaningful
photographic concepts from reference images, or cannot preserve the content of
the content image. To tackle these issues, we proposed a Personalized Image
Filter (PIF). Based on a pretrained text-to-image diffusion model, the
generative prior enables PIF to learn the average appearance of photographic
concepts, as well as how to adjust them according to text prompts. PIF then
learns the photographic style of reference images with the textual inversion
technique, by optimizing the prompts for the photographic concepts. PIF shows
outstanding performance in extracting and transferring various kinds of
photographic style. Project page: https://pif.pages.dev/

</details>


### [81] [ReefNet: A Large scale, Taxonomically Enriched Dataset and Benchmark for Hard Coral Classification](https://arxiv.org/abs/2510.16822)
*Yahia Battach,Abdulwahab Felemban,Faizan Farooq Khan,Yousef A. Radwan,Xiang Li,Fabio Marchese,Sara Beery,Burton H. Jones,Francesca Benzoni,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: 本文提出了ReefNet，这是一个大规模公共珊瑚礁图像数据集，带有精细物种标签，旨在推动全球范围内的机器学习自动化监测。


<details>
  <summary>Details</summary>
Motivation: 由于人类活动（如气候变化）导致珊瑚礁迅速退化，因此需要可扩展、自动化的监测手段。目前公开的相关数据集在数据规模、地理范围、标签精度等方面存在局限，无法满足高效机器学习研究的需求。

Method: 作者构建了ReefNet 数据集，收集了76个CoralNet资源及红海Al Wajh站点的图像，获得约92.5万个属级珊瑚硬体标注。这些标签与世界海洋生物物种登记册（WoRMS）对齐。提出了两种评测方案：（1）同源内的本地评测；（2）跨源域泛化评测，并测试了监督学习和零样本模型的分类性能。

Result: 监督学习在源内测试有较好表现，但在不同域间（跨源）表现显著下降。对于稀有或形态相近的种属，零样本模型性能普遍较低。

Conclusion: ReefNet 为细粒度、领域泛化的珊瑚分类提供了具有挑战性的基准，有望推动全球范围内鲁棒的自动化珊瑚礁监测和保护。数据集、代码和预训练模型将对外开放。

Abstract: Coral reefs are rapidly declining due to anthropogenic pressures such as
climate change, underscoring the urgent need for scalable, automated
monitoring. We introduce ReefNet, a large public coral reef image dataset with
point-label annotations mapped to the World Register of Marine Species (WoRMS).
ReefNet aggregates imagery from 76 curated CoralNet sources and an additional
site from Al Wajh in the Red Sea, totaling approximately 925000 genus-level
hard coral annotations with expert-verified labels. Unlike prior datasets,
which are often limited by size, geography, or coarse labels and are not
ML-ready, ReefNet offers fine-grained, taxonomically mapped labels at a global
scale to WoRMS. We propose two evaluation settings: (i) a within-source
benchmark that partitions each source's images for localized evaluation, and
(ii) a cross-source benchmark that withholds entire sources to test domain
generalization. We analyze both supervised and zero-shot classification
performance on ReefNet and find that while supervised within-source performance
is promising, supervised performance drops sharply across domains, and
performance is low across the board for zero-shot models, especially for rare
and visually similar genera. This provides a challenging benchmark intended to
catalyze advances in domain generalization and fine-grained coral
classification. We will release our dataset, benchmarking code, and pretrained
models to advance robust, domain-adaptive, global coral reef monitoring and
conservation.

</details>


### [82] [Robust Cross-Domain Adaptation in Texture Features Transferring for Wood Chip Moisture Content Prediction](https://arxiv.org/abs/2510.16832)
*Abdur Rahman,Mohammad Marufuzzaman,Jason Street,Haifeng Wang,Veera G. Gude,Randy Buchanan*

Main category: cs.CV

TL;DR: 该研究通过分析木屑图像的五类纹理特征，开发了AdaptMoist领域自适应方法，实现了在不同来源木屑间准确预测含水率，显著提升了模型跨域性能。


<details>
  <summary>Details</summary>
Motivation: 目前直接检测木屑含水率的方法耗时且破坏样品，间接方法在木屑来源多样化时准确性大幅下降，因此亟需一种鲁棒、能应对不同来源材料的预测方法。

Method: 本文分析了木屑图像五种纹理特征，利用特征组合提升预测能力，并提出了基于纹理特征进行知识迁移的领域自适应方法AdaptMoist，实现模型在不同来源木屑上的迁移。同时，提出基于调整互信息的模型保存标准。

Result: 整合五类纹理特征后模型预测准确率达95%；AdaptMoist方法跨域预测准确率平均提升23%，达80%，显著优于未适配模型（57%）。

Conclusion: AdaptMoist方法在不同木屑来源间具有良好的泛化与鲁棒性，为木屑含水率快速预测和相关产业应用提供了有效方案。

Abstract: Accurate and quick prediction of wood chip moisture content is critical for
optimizing biofuel production and ensuring energy efficiency. The current
widely used direct method (oven drying) is limited by its longer processing
time and sample destructiveness. On the other hand, existing indirect methods,
including near-infrared spectroscopy-based, electrical capacitance-based, and
image-based approaches, are quick but not accurate when wood chips come from
various sources. Variability in the source material can alter data
distributions, undermining the performance of data-driven models. Therefore,
there is a need for a robust approach that effectively mitigates the impact of
source variability. Previous studies show that manually extracted texture
features have the potential to predict wood chip moisture class. Building on
this, in this study, we conduct a comprehensive analysis of five distinct
texture feature types extracted from wood chip images to predict moisture
content. Our findings reveal that a combined feature set incorporating all five
texture features achieves an accuracy of 95% and consistently outperforms
individual texture features in predicting moisture content. To ensure robust
moisture prediction, we propose a domain adaptation method named AdaptMoist
that utilizes the texture features to transfer knowledge from one source of
wood chip data to another, addressing variability across different domains. We
also proposed a criterion for model saving based on adjusted mutual
information. The AdaptMoist method improves prediction accuracy across domains
by 23%, achieving an average accuracy of 80%, compared to 57% for non-adapted
models. These results highlight the effectiveness of AdaptMoist as a robust
solution for wood chip moisture content estimation across domains, making it a
potential solution for wood chip-reliant industries.

</details>


### [83] [From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display](https://arxiv.org/abs/2510.16833)
*Xiangyu Mu,Dongliang Zhou,Jie Hou,Haijun Zhang,Weili Guan*

Main category: cs.CV

TL;DR: 本文提出了M2HVideo框架，实现了从服装模特（假人）视频到可控身份、照片级真实感人类视频的自动生成，极大提升了服装展示的真实感和表现力。实验结果显示该方法在服装一致性、身份保持和视频质量上优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 在线时尚展示通常依赖成本较高的真人模特，而假人展示在逼真度和细节表现上远逊真人。解决这一问题，有助于以更低成本获得高质量服装展示视频，提升用户体验与产品转化率。

Method: 提出了M2HVideo视频生成框架，主要创新包括：（1）动态姿态感知头部编码器，将面部语义与身体姿态融合，有效解决头部与身体动作错位及身份漂移；（2）基于DDIM的一步去噪镜像损失，弥补潜空间压缩导致的面部细节丢失；（3）分布感知适配器对齐身份与服装特征分布，增强时序一致性。

Result: 在UBC、ASOS自建及MannequinVideos新采集数据集上的大量实验表明，M2HVideo在服装一致性、身份保持和视频保真度三方面均优于现有方法。

Conclusion: M2HVideo为服装假人视频转真人高质量视频生成提供了有效的解决方案，有助于促进时尚工业在线展示的自动化和视觉体验升级。

Abstract: Mannequin-based clothing displays offer a cost-effective alternative to
real-model showcases for online fashion presentation, but lack realism and
expressive detail. To overcome this limitation, we introduce a new task called
mannequin-to-human (M2H) video generation, which aims to synthesize
identity-controllable, photorealistic human videos from footage of mannequins.
We propose M2HVideo, a pose-aware and identity-preserving video generation
framework that addresses two key challenges: the misalignment between head and
body motion, and identity drift caused by temporal modeling. In particular,
M2HVideo incorporates a dynamic pose-aware head encoder that fuses facial
semantics with body pose to produce consistent identity embeddings across
frames. To address the loss of fine facial details due to latent space
compression, we introduce a mirror loss applied in pixel space through a
denoising diffusion implicit model (DDIM)-based one-step denoising.
Additionally, we design a distribution-aware adapter that aligns statistical
distributions of identity and clothing features to enhance temporal coherence.
Extensive experiments on the UBC fashion dataset, our self-constructed ASOS
dataset, and the newly collected MannequinVideos dataset captured on-site
demonstrate that M2HVideo achieves superior performance in terms of clothing
consistency, identity preservation, and video fidelity in comparison to
state-of-the-art methods.

</details>


### [84] [2DGS-R: Revisiting the Normal Consistency Regularization in 2D Gaussian Splatting](https://arxiv.org/abs/2510.16837)
*Haofan Ren,Qingsong Yan,Ming Lu,Rongfeng Lu,Zunjie Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种名为2DGS-R的新方法，实现了高质量渲染与精确几何结构的平衡，显著提升了2D Gaussian Splatting的表现，而只需极少的额外存储与训练时间。


<details>
  <summary>Details</summary>
Motivation: 当前3D Gaussian Splatting（3DGS）虽能实现高保真渲染，但在表面表示上存在困难；2DGS对几何精度有所改善，但渲染质量不足，两者难以在单一训练阶段优化。因此，需要一种能兼顾渲染质量与几何精度的新方法。

Method: 2DGS-R采用分层训练策略：首先利用法向一致性正则化训练原始2D高斯体；再对渲染质量不足的2D高斯体采取创新的原位克隆操作进行增强；最后在冻结不透明度的情况下进行微调。

Result: 实验表明，2DGS-R相比原始2DGS仅需增加1%的存储空间和极少的训练时间，但能显著提升渲染质量，同时保持精细的几何结构。

Conclusion: 2DGS-R有效兼顾了效率与性能，在视觉保真度与几何重建精度方面均有提升，对2DGS的发展具有重要推动作用。

Abstract: Recent advancements in 3D Gaussian Splatting (3DGS) have greatly influenced
neural fields, as it enables high-fidelity rendering with impressive visual
quality. However, 3DGS has difficulty accurately representing surfaces. In
contrast, 2DGS transforms the 3D volume into a collection of 2D planar Gaussian
disks. Despite advancements in geometric fidelity, rendering quality remains
compromised, highlighting the challenge of achieving both high-quality
rendering and precise geometric structures. This indicates that optimizing both
geometric and rendering quality in a single training stage is currently
unfeasible. To overcome this limitation, we present 2DGS-R, a new method that
uses a hierarchical training approach to improve rendering quality while
maintaining geometric accuracy. 2DGS-R first trains the original 2D Gaussians
with the normal consistency regularization. Then 2DGS-R selects the 2D
Gaussians with inadequate rendering quality and applies a novel in-place
cloning operation to enhance the 2D Gaussians. Finally, we fine-tune the 2DGS-R
model with opacity frozen. Experimental results show that compared to the
original 2DGS, our method requires only 1\% more storage and minimal additional
training time. Despite this negligible overhead, it achieves high-quality
rendering results while preserving fine geometric structures. These findings
indicate that our approach effectively balances efficiency with performance,
leading to improvements in both visual fidelity and geometric reconstruction
accuracy.

</details>


### [85] [ArmFormer: Lightweight Transformer Architecture for Real-Time Multi-Class Weapon Segmentation and Classification](https://arxiv.org/abs/2510.16854)
*Akhila Kambhatla,Taminul Islam,Khaled R Ahmed*

Main category: cs.CV

TL;DR: 本文提出了一种用于武器实时精细分割检测的新型轻量级Transformer架构ArmFormer，能在计算资源有限的边缘设备上实现高精度、高效率的多类武器分割。


<details>
  <summary>Details</summary>
Motivation: 当前武器检测多依赖粗粒度的目标检测方法，无法满足安防场景下对像素级精细分割和实时性能的要求。而主流语义分割模型通常要么准确率不足，要么计算资源消耗高，不适于边缘部署。

Method: 作者提出了ArmFormer，将Convolutional Block Attention Module (CBAM)与MixVisionTransformer结合，形成高效编码器主干，并在解码器中融合Attention机制，实现包括手枪、步枪、刀具、左轮手枪和人类五类目标的细致分割。模型结构旨在兼顾准确率和运算效率，适合嵌入式安防设备。

Result: 实验表明，ArmFormer在五类武器分割任务上达到80.64%的mIoU和89.13%的mFscore，推理速度达82.26 FPS。模型参数量仅3.66M，计算量4.886G FLOPs，远低于同类主流模型，但性能更优。

Conclusion: ArmFormer综合了高分割精度、低运算需求和实时推理能力，非常适合于便携安防摄像头、监控无人机及嵌入式AI设备等分布式安防场景，是部署武器精准检测的最佳选择之一。

Abstract: The escalating threat of weapon-related violence necessitates automated
detection systems capable of pixel-level precision for accurate threat
assessment in real-time security applications. Traditional weapon detection
approaches rely on object detection frameworks that provide only coarse
bounding box localizations, lacking the fine-grained segmentation required for
comprehensive threat analysis. Furthermore, existing semantic segmentation
models either sacrifice accuracy for computational efficiency or require
excessive computational resources incompatible with edge deployment scenarios.
This paper presents ArmFormer, a lightweight transformer-based semantic
segmentation framework that strategically integrates Convolutional Block
Attention Module (CBAM) with MixVisionTransformer architecture to achieve
superior accuracy while maintaining computational efficiency suitable for
resource-constrained edge devices. Our approach combines CBAM-enhanced encoder
backbone with attention-integrated hamburger decoder to enable multi-class
weapon segmentation across five categories: handgun, rifle, knife, revolver,
and human. Comprehensive experiments demonstrate that ArmFormer achieves
state-of-the-art performance with 80.64% mIoU and 89.13% mFscore while
maintaining real-time inference at 82.26 FPS. With only 4.886G FLOPs and 3.66M
parameters, ArmFormer outperforms heavyweight models requiring up to 48x more
computation, establishing it as the optimal solution for deployment on portable
security cameras, surveillance drones, and embedded AI accelerators in
distributed security infrastructure.

</details>


### [86] [BARL: Bilateral Alignment in Representation and Label Spaces for Semi-Supervised Volumetric Medical Image Segmentation](https://arxiv.org/abs/2510.16863)
*Shujian Gao,Yuan Wang,Zekuan Yu*

Main category: cs.CV

TL;DR: 本文提出了一种用于半监督医学图像分割的新框架BARL，同时在标签空间和特征表征空间进行一致性对齐，从而提升分割性能，实验结果优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有半监督医学图像分割方法主要依赖标签空间的一致性正则，但忽略了特征表征空间的一致性，导致模型无法有效学习判别性和空间连续的特征表达。本文旨在解决这一不足。

Method: BARL框架包括两个协作分支，在标签空间采用Dual-Path Regularization（DPR）和Progressively Cognitive Bias Correction（PCBC）两种方法进行多尺度、一致性的正则，减少误差累积；在表征空间则通过区域级和病灶级的分支间匹配，捕捉医学图像中复杂、碎片化的病理特征分布。

Result: 在四个公开医学数据集和一个私有CBCT数据集上，BARL均超过了当前最先进的半监督分割方法。消融实验显示各组件均有重要贡献。

Conclusion: 同时对标签空间和表征空间进行对齐对于提升半监督医学图像分割性能至关重要。BARL方法有效、通用，推动了该领域的进步。

Abstract: Semi-supervised medical image segmentation (SSMIS) seeks to match fully
supervised performance while sharply reducing annotation cost. Mainstream SSMIS
methods rely on \emph{label-space consistency}, yet they overlook the equally
critical \emph{representation-space alignment}. Without harmonizing latent
features, models struggle to learn representations that are both discriminative
and spatially coherent. To this end, we introduce \textbf{Bilateral Alignment
in Representation and Label spaces (BARL)}, a unified framework that couples
two collaborative branches and enforces alignment in both spaces. For
label-space alignment, inspired by co-training and multi-scale decoding, we
devise \textbf{Dual-Path Regularization (DPR)} and \textbf{Progressively
Cognitive Bias Correction (PCBC)} to impose fine-grained cross-branch
consistency while mitigating error accumulation from coarse to fine scales. For
representation-space alignment, we conduct region-level and lesion-instance
matching between branches, explicitly capturing the fragmented, complex
pathological patterns common in medical imagery. Extensive experiments on four
public benchmarks and a proprietary CBCT dataset demonstrate that BARL
consistently surpasses state-of-the-art SSMIS methods. Ablative studies further
validate the contribution of each component. Code will be released soon.

</details>


### [87] [Registration is a Powerful Rotation-Invariance Learner for 3D Anomaly Detection](https://arxiv.org/abs/2510.16865)
*Yuyang Yu,Zhengwei Chen,Xuemiao Xu,Lei Zhang,Haoxin Yang,Yongwei Nie,Shengfeng He*

Main category: cs.CV

TL;DR: 本文提出了一种结合点云配准与异常检测的新方法，在工业检测中提升了3D点云异常检测的准确性和鲁棒性，尤其是在旋转不变性和局部几何特征提取方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于记忆库的3D点云异常检测方法在特征转换一致性和区分能力上存在不足，难以有效捕捉局部几何细节与实现旋转不变，尤其在配准失败时表现不佳。因此，需要一种能够提升特征旋转不变性和局部判别能力的新方法。

Method: 提出一种将点云配准任务与异常检测任务目标结合的特征提取框架，通过将特征提取过程嵌入配准学习任务，使网络能够联合优化点云对齐与表示学习，从而获得既旋转不变又具有局部判别性的特征表征。

Result: 在Anomaly-ShapeNet和Real3D-AD等公开数据集上进行广泛实验，结果显示所提方法在检测效果和泛化能力方面均优于现有方法。

Conclusion: 将点云配准与异常检测特征学习深度结合，能显著提升3D点云异常检测的表现，并为工业质控中的结构缺陷检测提供了更可靠的解决方案。

Abstract: 3D anomaly detection in point-cloud data is critical for industrial quality
control, aiming to identify structural defects with high reliability. However,
current memory bank-based methods often suffer from inconsistent feature
transformations and limited discriminative capacity, particularly in capturing
local geometric details and achieving rotation invariance. These limitations
become more pronounced when registration fails, leading to unreliable detection
results. We argue that point-cloud registration plays an essential role not
only in aligning geometric structures but also in guiding feature extraction
toward rotation-invariant and locally discriminative representations. To this
end, we propose a registration-induced, rotation-invariant feature extraction
framework that integrates the objectives of point-cloud registration and
memory-based anomaly detection. Our key insight is that both tasks rely on
modeling local geometric structures and leveraging feature similarity across
samples. By embedding feature extraction into the registration learning
process, our framework jointly optimizes alignment and representation learning.
This integration enables the network to acquire features that are both robust
to rotations and highly effective for anomaly detection. Extensive experiments
on the Anomaly-ShapeNet and Real3D-AD datasets demonstrate that our method
consistently outperforms existing approaches in effectiveness and
generalizability.

</details>


### [88] [Uncovering Brain-Like Hierarchical Patterns in Vision-Language Models through fMRI-Based Neural Encoding](https://arxiv.org/abs/2510.16870)
*Yudan Ren,Xinlong Wang,Kexin Wang,Tian Xia,Zihan Ma,Zhaowei Li,Xiangrong Bi,Xiao Li,Xiaowei He*

Main category: cs.CV

TL;DR: 该论文提出了一套创新的神经元级分析框架，探索类脑多模态处理机制，并揭示ANN与人脑在神经元层面的诸多共性。


<details>
  <summary>Details</summary>
Motivation: 当前ANN与人脑处理机制的相似性研究存在两个不足：一是仅限单模态，未充分体现人脑多模态特性；二是多模态研究多聚焦整体输出，忽略了关键的单一神经元作用。

Method: 作者结合了细粒度人工神经元分析与基于fMRI的体素编码，研究两种不同架构的视觉-语言模型（CLIP和METER），对比人工神经元与生物神经元的表征和激活模式。

Result: 1）人工神经元能有效预测生物神经元在多个功能网络（如语言、视觉、注意力、默认模式）中的活动，展现共同表征机制；2）二者均体现功能冗余性与协作性；3）人工与生物神经元在极性激活特征上高度相似，且跨层次有镜像趋势；4）模型架构影响神经元的“脑似”表现——CLIP更具专属性，METER表现跨模态统一激活。

Conclusion: 该方法验证了视觉-语言模型在神经元层面具备类似人脑的分层处理能力，强调了模型架构设计对ANN“类脑”特性的影响。

Abstract: While brain-inspired artificial intelligence(AI) has demonstrated promising
results, current understanding of the parallels between artificial neural
networks (ANNs) and human brain processing remains limited: (1) unimodal ANN
studies fail to capture the brain's inherent multimodal processing
capabilities, and (2) multimodal ANN research primarily focuses on high-level
model outputs, neglecting the crucial role of individual neurons. To address
these limitations, we propose a novel neuron-level analysis framework that
investigates the multimodal information processing mechanisms in
vision-language models (VLMs) through the lens of human brain activity. Our
approach uniquely combines fine-grained artificial neuron (AN) analysis with
fMRI-based voxel encoding to examine two architecturally distinct VLMs: CLIP
and METER. Our analysis reveals four key findings: (1) ANs successfully predict
biological neurons (BNs) activities across multiple functional networks
(including language, vision, attention, and default mode), demonstrating shared
representational mechanisms; (2) Both ANs and BNs demonstrate functional
redundancy through overlapping neural representations, mirroring the brain's
fault-tolerant and collaborative information processing mechanisms; (3) ANs
exhibit polarity patterns that parallel the BNs, with oppositely activated BNs
showing mirrored activation trends across VLM layers, reflecting the complexity
and bidirectional nature of neural information processing; (4) The
architectures of CLIP and METER drive distinct BNs: CLIP's independent branches
show modality-specific specialization, whereas METER's cross-modal design
yields unified cross-modal activation, highlighting the architecture's
influence on ANN brain-like properties. These results provide compelling
evidence for brain-like hierarchical processing in VLMs at the neuronal level.

</details>


### [89] [Class-N-Diff: Classification-Induced Diffusion Model Can Make Fair Skin Cancer Diagnosis](https://arxiv.org/abs/2510.16887)
*Nusrat Munia,Abdullah Imran*

Main category: cs.CV

TL;DR: 该论文提出了一种新的分类引导扩散模型（Class-N-Diff），能够同时生成和分类皮肤镜图像，有助于提升医疗影像的合成质量和分类准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的类条件生成模型在生成特定医学类别图像时表现有限，导致在实际诊断如皮肤癌场景下应用受限。因此，研究者希望提升生成模型对类别的控制能力和下游任务表现。

Method: 作者提出Class-N-Diff模型，将分类器嵌入扩散模型体系中，通过类别信息直接指导图像生成过程，实现生成与分类的协同优化。

Result: 实验结果表明，该方法相比传统模型能够生成更真实、多样化且符合分类要求的皮肤镜图像，并提升了分类器在相关诊断任务上的准确率。

Conclusion: Class-N-Diff有效提升了类条件扩散模型在医学图像中的生成与分类能力，对提升基于扩散模型的医学图像合成的质量和应用价值具有重要意义。

Abstract: Generative models, especially Diffusion Models, have demonstrated remarkable
capability in generating high-quality synthetic data, including medical images.
However, traditional class-conditioned generative models often struggle to
generate images that accurately represent specific medical categories, limiting
their usefulness for applications such as skin cancer diagnosis. To address
this problem, we propose a classification-induced diffusion model, namely,
Class-N-Diff, to simultaneously generate and classify dermoscopic images. Our
Class-N-Diff model integrates a classifier within a diffusion model to guide
image generation based on its class conditions. Thus, the model has better
control over class-conditioned image synthesis, resulting in more realistic and
diverse images. Additionally, the classifier demonstrates improved performance,
highlighting its effectiveness for downstream diagnostic tasks. This unique
integration in our Class-N-Diff makes it a robust tool for enhancing the
quality and utility of diffusion model-based synthetic dermoscopic image
generation. Our code is available at https://github.com/Munia03/Class-N-Diff.

</details>


### [90] [Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback](https://arxiv.org/abs/2510.16888)
*Zongjian Li,Zheyuan Liu,Qihui Zhang,Bin Lin,Shenghai Yuan,Zhiyuan Yan,Yang Ye,Wangbo Yu,Yuwei Niu,Li Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于指令的图像编辑后训练框架Edit-R1，融合了策略优化和大模型无监督评价，显著提升了现有方法的表现，并在多个基准上取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前指令驱动的图像编辑虽有不错进展，但单纯的有监督微调容易过拟合，缺乏泛化和探索能力。此外，不同编辑任务难以统一评价，限制方法的通用性。

Method: 该方法提出Edit-R1框架，基于Diffusion Negative-aware Finetuning（DiffusionNFT）进行策略优化，使训练更高效、采样更灵活；将多模态大语言模型（MLLM）用作统一的奖励模型，对编辑结果进行细粒度评分；设计分组过滤机制，减少评分噪音并稳定优化过程。

Result: UniWorld-V2——利用该框架训练——在ImgEdit和GEdit-Bench两个权威基准上分别取得4.49和7.83的SOTA成绩。

Conclusion: Edit-R1是一种通用、可扩展性强且稳定的图像编辑后训练方法，对多种基础模型均有较大提升，具备广泛应用潜力，相关代码和模型已开源。

Abstract: Instruction-based image editing has achieved remarkable progress; however,
models solely trained via supervised fine-tuning often overfit to annotated
patterns, hindering their ability to explore and generalize beyond training
distributions. To this end, we introduce Edit-R1, a novel post-training
framework for instruction-based image editing based on policy optimization.
Specifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a
likelihood-free policy optimization method consistent with the flow matching
forward process, thereby enabling the use of higher-order samplers and more
efficient training. Another key challenge here is the absence of a universal
reward model, resulting from the diverse nature of editing instructions and
tasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM)
as a unified, training-free reward model, leveraging its output logits to
provide fine-grained feedback. Furthermore, we carefully design a low-variance
group filtering mechanism to reduce MLLM scoring noise and stabilize
optimization. UniWorld-V2, trained with this framework, achieves
\textbf{state-of-the-art} results on the ImgEdit and GEdit-Bench benchmarks,
scoring 4.49 and 7.83, respectively. Crucially, our framework is
model-agnostic, delivering substantial performance gains when applied to
diverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its
wide applicability. Code and models are publicly available at
https://github.com/PKU-YuanGroup/UniWorld-V2.

</details>


### [91] [Contrail-to-Flight Attribution Using Ground Visible Cameras and Flight Surveillance Data](https://arxiv.org/abs/2510.16891)
*Ramon Dalmau,Gabriel Jarry,Philippe Very*

Main category: cs.CV

TL;DR: 本论文提出了一种使用地面摄像机鉴别凝结尾迹与其生成航班的方法，并建立了灵活的归因框架，有助于未来准确评估航空业的非二氧化碳气候影响。


<details>
  <summary>Details</summary>
Motivation: 虽然航空二氧化碳排放的气候影响广为人知，但非二氧化碳影响（特别是凝结尾迹导致的辐射强迫）具有显著作用，而且相关物理模型的校准和验证较为困难，亟需准确的凝结尾迹与航班对应方法。

Method: 作者提出用地面可见光摄像机捕捉刚形成时的薄而线性凝结尾迹，并利用GVCCS数据集，将摄像机观测到的凝结尾迹与由飞机监测和气象数据推算出的理论凝结尾迹进行几何匹配。方法框架可适应多种几何表示与距离度量，并整合时间平滑与概率分配策略。

Result: 建立并验证了模块化归因框架，能够有效将地面摄像机观测到的凝结尾迹与其产生的航班关联，表现出较高的准确性和灵活性，为后续精准研究打下基础。

Conclusion: 该工作为基于地面摄像机的凝结尾迹归因提供了有力的基线方法，并搭建了便于扩展的模块化框架，有助于后续深入探究航空非二氧化碳气候效应、促进模型校准与政策制定。

Abstract: Aviation's non-CO2 effects, particularly contrails, are a significant
contributor to its climate impact. Persistent contrails can evolve into
cirrus-like clouds that trap outgoing infrared radiation, with radiative
forcing potentially comparable to or exceeding that of aviation's CO2
emissions. While physical models simulate contrail formation, evolution and
dissipation, validating and calibrating these models requires linking observed
contrails to the flights that generated them, a process known as
contrail-to-flight attribution. Satellite-based attribution is challenging due
to limited spatial and temporal resolution, as contrails often drift and deform
before detection. In this paper, we evaluate an alternative approach using
ground-based cameras, which capture contrails shortly after formation at high
spatial and temporal resolution, when they remain thin, linear, and visually
distinct. Leveraging the ground visible camera contrail sequences (GVCCS)
dataset, we introduce a modular framework for attributing contrails observed
using ground-based cameras to theoretical contrails derived from aircraft
surveillance and meteorological data. The framework accommodates multiple
geometric representations and distance metrics, incorporates temporal
smoothing, and enables flexible probability-based assignment strategies. This
work establishes a strong baseline and provides a modular framework for future
research in linking contrails to their source flight.

</details>


### [92] [Beyond RGB: Leveraging Vision Transformers for Thermal Weapon Segmentation](https://arxiv.org/abs/2510.16913)
*Akhila Kambhatla,Ahmed R Khaled*

Main category: cs.CV

TL;DR: 本文评估了四种Transformer架构在热成像武器分割任务上的表现，显著优于传统CNN方法。


<details>
  <summary>Details</summary>
Motivation: 低光照和遮挡环境下，传统RGB视觉系统在武器检测上表现不佳，而热成像可以提供鲁棒性。现有主流方法多为CNN，但其处理全局依赖和细节结构有限，需要探索ViT在此场景的潜力。

Method: 自定义热成像数据集（9711张实际监控视频采集图像，SAM2自动标注）；评估SegFormer、DeepLabV3+、SegNeXt、Swin Transformer四种主流Transformer架构进行二分类武器分割；在MMSegmentation框架下采用标准增强策略保证训练鲁棒性和模型公平对比。

Result: SegFormer-b5取得最高mIoU（94.15%）及像素精度（97.04%）；SegFormer-b0推理最快（98.32 FPS），mIoU为90.84%；SegNeXt-mscans性能均衡（85.12 FPS，92.24% mIoU）；DeepLabV3+ R101-D8为92.76% mIoU/29.86 FPS。

Conclusion: 多种Transformer模型在热成像武器检测任务上表现强劲，能够在低光和遮挡环境下实现高鲁棒性、可根据实际应用需求灵活选择速度与精度。

Abstract: Thermal weapon segmentation is crucial for surveillance and security
applications, enabling robust detection under lowlight and visually obscured
conditions where RGB-based systems fail. While convolutional neural networks
(CNNs) dominate thermal segmentation literature, their ability to capture
long-range dependencies and fine structural details is limited. Vision
Transformers (ViTs), with their global context modeling capabilities, have
achieved state-of-the-art results in RGB segmentation tasks, yet their
potential in thermal weapon segmentation remains underexplored. This work
adapts and evaluates four transformer-based architectures SegFormer,
DeepLabV3\+, SegNeXt, and Swin Transformer for binary weapon segmentation on a
custom thermal dataset comprising 9,711 images collected from real world
surveillance videos and automatically annotated using SAM2. We employ standard
augmentation strategies within the MMSegmentation framework to ensure robust
model training and fair architectural comparison. Experimental results
demonstrate significant improvements in segmentation performance: SegFormer-b5
achieves the highest mIoU (94.15\%) and Pixel Accuracy (97.04\%), while
SegFormer-b0 provides the fastest inference speed (98.32 FPS) with competitive
mIoU (90.84\%). SegNeXt-mscans offers balanced performance with 85.12 FPS and
92.24\% mIoU, and DeepLabV3\+ R101-D8 reaches 92.76\% mIoU at 29.86 FPS. The
transformer architectures demonstrate robust generalization capabilities for
weapon detection in low-light and occluded thermal environments, with flexible
accuracy-speed trade-offs suitable for diverse real-time security applications.

</details>


### [93] [Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input](https://arxiv.org/abs/2510.16926)
*Chenxu Li,Zhicai Wang,Yuan Sheng,Xingyu Zhu,Yanbin Hao,Xiang Wang*

Main category: cs.CV

TL;DR: 本文关注多模态大语言模型（MLLMs）对输入图像分辨率变化的鲁棒性，并提出了Res-Bench基准，系统评测主要模型在不同分辨率下的表现波动及相关提升方法。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大模型支持动态图像分辨率，现有评测大多仅关注语义理解性能，忽略了模型在不同分辨率输入下性能是否一致及稳定，实际应用中分辨率多变，性能稳定性很关键，因此需要新的评测体系。

Method: 作者提出Res-Bench基准，包括14400个样本、12种分辨率和6个能力维度，建立了结合Spearman相关、绝对和相对连续误差（ACE/RCE）的新鲁棒性评估框架，并基于此系统评测主流MLLM，比较了模型性能，探索预处理（如填充、超分辨率）、微调等对鲁棒性的影响。

Result: 通过Res-Bench和新鲁棒性指标，作者深入分析了多模态大模型随分辨率变化的稳定性，揭示了模型和任务相关的鲁棒性差异，以及不同预处理和微调手段带来的性能波动和提升效果。

Conclusion: 论文填补了分辨率鲁棒性评测的空白，Res-Bench和新框架能有效比较和分析多模态大模型在多分辨率下的表现，为后续MLLM的实际部署和方法改进提供了重要参考。

Abstract: Multimodal Large Language Models (MLLMs) increasingly support dynamic image
resolutions. However, current evaluation paradigms primarily assess semantic
performance, overlooking the critical question of resolution robustness -
whether performance remains stable across varying input resolutions. To address
this gap, we introduce \textbf{Res-Bench}, a comprehensive benchmark comprising
14,400 samples across 12 resolution levels and six core capability dimensions.
We designed a novel evaluation framework that goes beyond traditional accuracy
metrics to capture performance stability. This framework introduces multiple
robustness metrics: Spearman's correlation for assessing resolution-performance
trends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring
performance volatility. Using these metrics, we conducted a large-scale
evaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and
task-centric robustness examination, (2) investigation of preprocessing
strategies including padding and super-resolution, and (3) exploration of
fine-tuning for stability enhancement.

</details>


### [94] [Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis](https://arxiv.org/abs/2510.16973)
*Praveenbalaji Rajendran,Mojtaba Safari,Wenfeng He,Mingzhe Hu,Shansong Wang,Jun Zhou,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 该综述系统梳理了基础模型（Foundation Models, FMs）在医学影像分析领域的应用进展，包括其架构演化、训练范式及临床应用，目前尚存领域碎片化和多项挑战，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 医学影像人工智能快速进展，尤其是基础模型在多任务上表现突出，但研究分散缺乏系统梳理，对架构、应用和挑战等需有全面分析，帮助推动实际临床落地。

Method: 对医学影像FMs的研究按架构（纯视觉/视觉-语言）、训练策略和下游任务进行分类梳理，并进行了量化元分析，讨论领域难题及前沿解决方案，最后提出未来研究路线。

Result: 文章细致归纳了基础模型在医学影像分析中的关键技术进展、应用领域和存在挑战，总结了现有数据利用和临床任务的发展趋势。

Conclusion: 全面综述有助于学界和产业了解FM在医学影像领域的现状与前景，强调提升稳健性、可解释性及临床融合性的重要性，为后续研究和临床转化提供了明确方向。

Abstract: Recent advancements in artificial intelligence (AI), particularly foundation
models (FMs), have revolutionized medical image analysis, demonstrating strong
zero- and few-shot performance across diverse medical imaging tasks, from
segmentation to report generation. Unlike traditional task-specific AI models,
FMs leverage large corpora of labeled and unlabeled multimodal datasets to
learn generalized representations that can be adapted to various downstream
clinical applications with minimal fine-tuning. However, despite the rapid
proliferation of FM research in medical imaging, the field remains fragmented,
lacking a unified synthesis that systematically maps the evolution of
architectures, training paradigms, and clinical applications across modalities.
To address this gap, this review article provides a comprehensive and
structured analysis of FMs in medical image analysis. We systematically
categorize studies into vision-only and vision-language FMs based on their
architectural foundations, training strategies, and downstream clinical tasks.
Additionally, a quantitative meta-analysis of the studies was conducted to
characterize temporal trends in dataset utilization and application domains. We
also critically discuss persistent challenges, including domain adaptation,
efficient fine-tuning, computational constraints, and interpretability along
with emerging solutions such as federated learning, knowledge distillation, and
advanced prompting. Finally, we identify key future research directions aimed
at enhancing the robustness, explainability, and clinical integration of FMs,
thereby accelerating their translation into real-world medical practice.

</details>


### [95] [One-step Diffusion Models with Bregman Density Ratio Matching](https://arxiv.org/abs/2510.16983)
*Yuanzhi Zhu,Eleftherios Tsonis,Lucas Degeorge,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: 本文提出了一种基于Bregman散度的扩散模型蒸馏新框架Di-Bregman，实现了高效的一步采样，同时兼顾生成质量和理论基础。


<details>
  <summary>Details</summary>
Motivation: 扩散和流模型在生成任务上表现优异，但因采样步骤多，计算开销大。虽然蒸馏法可提升采样效率，但现有目标函数大多缺乏统一理论基础。因此，本工作旨在为扩散蒸馏提供更为坚实和统一的理论框架。

Method: 作者提出Di-Bregman，将扩散蒸馏问题统一建模为基于Bregman散度的密度比匹配，从凸分析的视角将多种已有蒸馏目标联系起来，并给出了理论推导和算法实现。

Result: 在CIFAR-10和文本到图像生成任务上，Di-Bregman在一步采样的FID优于反KL蒸馏，同时生成图像的视觉质量接近教师模型。

Conclusion: Bregman密度比匹配为高效的一步扩散生成提供了一条实用且有理论支撑的新途径，兼具速度与生成质量。

Abstract: Diffusion and flow models achieve high generative quality but remain
computationally expensive due to slow multi-step sampling. Distillation methods
accelerate them by training fast student generators, yet most existing
objectives lack a unified theoretical foundation. In this work, we propose
Di-Bregman, a compact framework that formulates diffusion distillation as
Bregman divergence-based density-ratio matching. This convex-analytic view
connects several existing objectives through a common lens. Experiments on
CIFAR-10 and text-to-image generation demonstrate that Di-Bregman achieves
improved one-step FID over reverse-KL distillation and maintains high visual
fidelity compared to the teacher model. Our results highlight Bregman
density-ratio matching as a practical and theoretically-grounded route toward
efficient one-step diffusion generation.

</details>


### [96] [CARE: Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams](https://arxiv.org/abs/2510.16988)
*Junhao Zhao,Zishuai Liu,Ruili Fang,Jin Lu,Linghan Zhang,Fei Dou*

Main category: cs.CV

TL;DR: 本文提出了一种用于智能家居中的日常活动识别的新方法CARE，通过融合序列和图像两种表示方式，实现了更加准确和鲁棒的活动检测。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于单一表示方式：序列方法虽能保持时间顺序但易受噪声影响且空间感知不足；图像方法虽能捕获全局和隐式空间关系，但时间信息和传感器布局细节丢失。简单的特征拼接也难以有效融合两类优势。因此，亟需一种有效结合两种表征、提升识别性能的方法。

Method: 提出CARE框架，包含：1）鲁棒的时序编码方法，提升对噪声的抗性；2）空间感知和频率敏感的图像编码方式；3）通过序列-图像对比对齐(SICA)和分类联合优化，确保表征对齐和鉴别能力。该方法端到端学习判别性嵌入。

Result: 在三个CASAS智能家居公开数据集上，CARE取得了业界领先的活动识别准确率（Milan 89.8%、Cairo 88.9%、Kyoto7 73.3%），并在传感器故障和布局变化下表现出高鲁棒性。

Conclusion: CARE框架实现了对日常活动的更可靠识别，充分发挥了时间、空间信息的互补优势，为智能家居中的活动检测带来了更高的准确性和实用性。

Abstract: The recognition of Activities of Daily Living (ADLs) from event-triggered
ambient sensors is an essential task in Ambient Assisted Living, yet existing
methods remain constrained by representation-level limitations. Sequence-based
approaches preserve temporal order of sensor activations but are sensitive to
noise and lack spatial awareness, while image-based approaches capture global
patterns and implicit spatial correlations but compress fine-grained temporal
dynamics and distort sensor layouts. Naive fusion (e.g., feature concatenation)
fail to enforce alignment between sequence- and image-based representation
views, underutilizing their complementary strengths. We propose Contrastive
Alignment for ADL Recognition from Event-Triggered Sensor Streams (CARE), an
end-to-end framework that jointly optimizes representation learning via
Sequence-Image Contrastive Alignment (SICA) and classification via
cross-entropy, ensuring both cross-representation alignment and task-specific
discriminability. CARE integrates (i) time-aware, noise-resilient sequence
encoding with (ii) spatially-informed and frequency-sensitive image
representations, and employs (iii) a joint contrastive-classification objective
for end-to-end learning of aligned and discriminative embeddings. Evaluated on
three CASAS datasets, CARE achieves state-of-the-art performance (89.8% on
Milan, 88.9% on Cairo, and 73.3% on Kyoto7) and demonstrates robustness to
sensor malfunctions and layout variability, highlighting its potential for
reliable ADL recognition in smart homes.

</details>


### [97] [Training-free Online Video Step Grounding](https://arxiv.org/abs/2510.16989)
*Luca Zanella,Massimiliano Mancini,Yiming Wang,Alessio Tonioni,Elisa Ricci*

Main category: cs.CV

TL;DR: 本文提出一种利用大型多模态模型（LMM）进行视频步骤定位的新方法，无需专门训练即可实现在线推理，并提出了结合贝叶斯滤波原理的BaGLM方法，实验证明其优于现有离线训练方法。


<details>
  <summary>Details</summary>
Motivation: 传统的视频步骤定位方法依赖于大量有标注的数据训练且离线处理视频，成本高且不适用于需要实时决策的场景。为解决标注昂贵和不能在线处理的问题，作者探索了无需训练且能够实时（在线）处理的新方法。

Method: 利用大型多模态模型（LMM）的零样本推理能力，在仅观察部分视频帧的情况下进行步骤检测。基础方法通过LMM预测当前帧可能的步骤。进一步提出BaGLM方法，结合贝叶斯滤波思想，将历史帧信息融入预测，通过大语言模型获得步骤依赖矩阵，并估计步骤进度以建模步骤转换。

Result: 在三个数据集上的实验表明，本文的无监督在线方法不仅优于基线的无训练方法，进一步提出的BaGLM方法也超过了现有的基于训练的离线方法，实现了更好的步骤检测性能。

Conclusion: 通过结合LMM的零样本识别能力和贝叶斯滤波方法，本文提出的BaGLM方法可高效、准确地实现视频步骤的在线检测，不依赖于有标注数据，具有较好的实际应用前景。

Abstract: Given a task and a set of steps composing it, Video Step Grounding (VSG) aims
to detect which steps are performed in a video. Standard approaches for this
task require a labeled training set (e.g., with step-level annotations or
narrations), which may be costly to collect. Moreover, they process the full
video offline, limiting their applications for scenarios requiring online
decisions. Thus, in this work, we explore how to perform VSG online and without
training. We achieve this by exploiting the zero-shot capabilities of recent
Large Multimodal Models (LMMs). In particular, we use LMMs to predict the step
associated with a restricted set of frames, without access to the whole video.
We show that this online strategy without task-specific tuning outperforms
offline and training-based models. Motivated by this finding, we develop
Bayesian Grounding with Large Multimodal Models (BaGLM), further injecting
knowledge of past frames into the LMM-based predictions. BaGLM exploits
Bayesian filtering principles, modeling step transitions via (i) a dependency
matrix extracted through large language models and (ii) an estimation of step
progress. Experiments on three datasets show superior performance of BaGLM over
state-of-the-art training-based offline methods.

</details>


### [98] [An empirical study of the effect of video encoders on Temporal Video Grounding](https://arxiv.org/abs/2510.17007)
*Ignacio M. De la Jara,Cristian Rodriguez-Opazo,Edison Marrese-Taylor,Felipe Bravo-Marquez*

Main category: cs.CV

TL;DR: 本论文通过实证研究分析了不同类型的视频特征对经典时序视频定位模型性能的影响，发现视频编码器的选择会导致显著性能差异，并揭示了特征互补性。


<details>
  <summary>Details</summary>
Motivation: 尽管时序视频定位领域已有丰富研究，但大部分工作集中在少数特定视频特征表示上，这可能导致模型在架构上过拟合，并限制了方法的泛化能力。因此，有必要系统性地评估不同视频编码特征对该任务的影响。

Method: 作者对Charades-STA、ActivityNet-Captions和YouCookII三个知名基准数据集，采用基于CNN、时序推理和Transformer的视频特征编码器，分别提取特征，并将这些特征输入到同一经典架构中进行实证对比分析。

Result: 实验证明，仅更换视频编码器就会带来模型性能上的显著变化。同时，作者还发现某些特征会导致特定模式的误差，暗示不同特征间存在互补性。

Conclusion: 视频特征选择在时序视频定位任务中起着关键作用，探索多样的特征融合具有提升模型性能的潜力。未来应关注特征互补性以进一步改进时序视频定位方法。

Abstract: Temporal video grounding is a fundamental task in computer vision, aiming to
localize a natural language query in a long, untrimmed video. It has a key role
in the scientific community, in part due to the large amount of video generated
every day. Although we find extensive work in this task, we note that research
remains focused on a small selection of video representations, which may lead
to architectural overfitting in the long run. To address this issue, we propose
an empirical study to investigate the impact of different video features on a
classical architecture. We extract features for three well-known benchmarks,
Charades-STA, ActivityNet-Captions and YouCookII, using video encoders based on
CNNs, temporal reasoning and transformers. Our results show significant
differences in the performance of our model by simply changing the video
encoder, while also revealing clear patterns and errors derived from the use of
certain features, ultimately indicating potential feature complementarity.

</details>


### [99] [Do Satellite Tasks Need Special Pretraining?](https://arxiv.org/abs/2510.17014)
*Ani Vanyan,Alvard Barseghyan,Hakob Tamazyan,Tigran Galstyan,Vahan Huroyan,Naira Hovakimyan,Hrant Khachatrian*

Main category: cs.CV

TL;DR: 本文系统性地评估了专为遥感设计的基础模型与通用视觉基础模型在小规模上的表现，发现前者在某些情景下并未明显优于后者。


<details>
  <summary>Details</summary>
Motivation: 遥感图像具有独特特性和特定应用需求，近年来有越来越多团队开发了专门针对遥感的基础模型，认为这些定制化模型能在遥感任务上超越通用模型。作者希望检验这一假设。

Method: 设计了一个用于评估遥感模型泛化能力的基准，主要针对低分辨率图像下的两个下游任务。同时将自监督视觉编码器iBOT在大规模遥感数据集MillionAID上进行定制化预训练，并与通用模型进行对比。

Result: 实验结果显示，无论是专门针对遥感优化过的iBOT模型还是其他遥感专用预训练模型，在ViT-B（Vision Transformer-B）这一规模上，都未能带来相较于通用模型的稳定提升。

Conclusion: 针对遥感图像的特定基础模型在当前小规模下，并未明显优于通用视觉基础模型。在实际应用前，其优势需要进一步验证，当前阶段选择通用模型亦可达到较好效果。

Abstract: Foundation models have advanced machine learning across various modalities,
including images. Recently multiple teams trained foundation models specialized
for remote sensing applications. This line of research is motivated by the
distinct characteristics of remote sensing imagery, specific applications and
types of robustness useful for satellite image analysis. In this work we
systematically challenge the idea that specific foundation models are more
useful than general-purpose vision foundation models, at least in the small
scale. First, we design a simple benchmark that measures generalization of
remote sensing models towards images with lower resolution for two downstream
tasks. Second, we train iBOT, a self-supervised vision encoder, on MillionAID,
an ImageNet-scale satellite imagery dataset, with several modifications
specific to remote sensing. We show that none of those pretrained models bring
consistent improvements upon general-purpose baselines at the ViT-B scale.

</details>


### [100] [Enrich and Detect: Video Temporal Grounding with Multimodal LLMs](https://arxiv.org/abs/2510.17023)
*Shraman Pramanick,Effrosyni Mavroudi,Yale Song,Rama Chellappa,Lorenzo Torresani,Triantafyllos Afouras*

Main category: cs.CV

TL;DR: ED-VTG方法提出了利用多模态大模型，对视频中的自然语言查询进行更细粒度的时序定位，分为两阶段进行：首先丰富查询语句，再结合视频信息精准定位，在多个基准测试中取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型（如LLM）虽能理解文本和视频，但在视频时序定位领域以前没有充足利用其潜力，本工作旨在提升LLM在视频时序定位任务中的表现，尤其在细粒度理解和零样本场景下。

Method: 方法分两阶段：先通过LLM丰富原始语言查询，补全关键细节以便后续定位；再用轻量化解码器，根据上下文化的丰富查询进行边界预测。同时采用多实例学习目标，动态选择最优查询版本，减弱噪声与幻觉影响。

Result: 在多项视频时序定位和段落定位基准数据集上取得了最优表现，不仅全面超越了以往LLM方案，也优于或媲美于专用模型，并在零样本测试上有明显优势。

Conclusion: ED-VTG证明了多模态LLM在视频细粒度时序定位有巨大潜力，通过查询丰富和高效训练，实现了更强的定位表现，有望推动领域发展。

Abstract: We introduce ED-VTG, a method for fine-grained video temporal grounding
utilizing multi-modal large language models. Our approach harnesses the
capabilities of multimodal LLMs to jointly process text and video, in order to
effectively localize natural language queries in videos through a two-stage
process. Rather than being directly grounded, language queries are initially
transformed into enriched sentences that incorporate missing details and cues
to aid in grounding. In the second stage, these enriched queries are grounded,
using a lightweight decoder, which specializes at predicting accurate
boundaries conditioned on contextualized representations of the enriched
queries. To mitigate noise and reduce the impact of hallucinations, our model
is trained with a multiple-instance-learning objective that dynamically selects
the optimal version of the query for each training sample. We demonstrate
state-of-the-art results across various benchmarks in temporal video grounding
and paragraph grounding settings. Experiments reveal that our method
significantly outperforms all previously proposed LLM-based temporal grounding
approaches and is either superior or comparable to specialized models, while
maintaining a clear advantage against them in zero-shot evaluation scenarios.

</details>


### [101] [Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding](https://arxiv.org/abs/2510.17034)
*Yutong Zhong*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的训练框架W2R2，解决多模态3D定位中模型对2D语义特征过度依赖的问题，大幅提升3D定位的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型在3D场景下的多模态定位任务中，普遍受限于对2D图像特征的过度依赖，忽视了3D几何信息，导致空间推理和融合性能不佳，尤其在复杂场景中局限性明显。

Method: 提出What-Where Representation Re-Forming (W2R2) 框架，通过表征解耦和有针对性地抑制模型捷径，对2D和3D特征分工（2D捕捉语义，3D负责空间定位），结合双重目标损失函数（融合预测的Alignment Loss与基于边界的Pseudo-Label Loss），无需更改推理结构即可提升多模态协同能力。

Result: 在ScanRefer和ScanQA基准测试中，W2R2显著提升了多模态3D定位的准确率和鲁棒性，尤其在杂乱的室外场景下表现突出。

Conclusion: W2R2能够有效缓解2D语义偏置，激发模型对3D特征的利用潜力，为多模态3D定位提供了一种高效且无需更改推理结构的训练方案。

Abstract: Multimodal 3D grounding has garnered considerable interest in Vision-Language
Models (VLMs) \cite{yin2025spatial} for advancing spatial reasoning in complex
environments. However, these models suffer from a severe "2D semantic bias"
that arises from over-reliance on 2D image features for coarse localization,
largely disregarding 3D geometric inputs and resulting in suboptimal fusion
performance. In this paper, we propose a novel training framework called
What-Where Representation Re-Forming (W2R2) to tackle this issue via
disentangled representation learning and targeted shortcut suppression. Our
approach fundamentally reshapes the model's internal space by designating 2D
features as semantic beacons for "What" identification and 3D features as
spatial anchors for "Where" localization, enabling precise 3D grounding without
modifying inference architecture. Key components include a dual-objective loss
function with an Alignment Loss that supervises fused predictions using adapted
cross-entropy for multimodal synergy, and a Pseudo-Label Loss that penalizes
overly effective 2D-dominant pseudo-outputs via a margin-based mechanism.
Experiments conducted on ScanRefer and ScanQA demonstrate the effectiveness of
W2R2, with significant gains in localization accuracy and robustness,
particularly in cluttered outdoor scenes.

</details>


### [102] [Conditional Synthetic Live and Spoof Fingerprint Generation](https://arxiv.org/abs/2510.17035)
*Syed Konain Abbas,Sandip Purnapatra,M. G. Sarwar Murshed,Conor Miller-Lynch,Lambert Igene,Soumyabrata Dey,Stephanie Schuckers,Faraz Hussain*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法，利用生成模型合成高质量的指纹（含真实和伪造），解决隐私、成本和获取难题，并证明其合成数据适用于算法训练和检测，且具备隐私保护特性。


<details>
  <summary>Details</summary>
Motivation: 真实指纹数据的大规模收集昂贵且需严格隐私保护，因此亟需高质量的合成指纹数据来提升生物识别系统的研究和测试效率，并规避隐私问题。

Method: 作者采用了条件式StyleGAN2-ADA和StyleGAN3模型生成高分辨率的真实指纹，并使用CycleGAN将其转化为多种材料伪造的指纹，创建了DB2和DB3两个包含多种印象和伪造材料的合成指纹数据库。合成指纹在指定位（拇指至小指）上进行条件控制，伪造材料拟合常见攻击类型。

Result: StyleGAN3模型生成的指纹FID低达5，TAR在0.01% FAR下为99.47%，StyleGAN2-ADA为98.67%。生成指纹经NFIQ2、MINDTCT等标准指标评估，匹配实验显示没有身份泄露迹象，表现出较高的隐私保护能力。

Conclusion: 本研究方法可有效生成高质量且具隐私保护能力的合成指纹，包括真实与多种伪造材料类型，为指纹识别和防伪系统的训练与评估提供了安全、低成本、可扩展的数据支持。

Abstract: Large fingerprint datasets, while important for training and evaluation, are
time-consuming and expensive to collect and require strict privacy measures.
Researchers are exploring the use of synthetic fingerprint data to address
these issues. This paper presents a novel approach for generating synthetic
fingerprint images (both spoof and live), addressing concerns related to
privacy, cost, and accessibility in biometric data collection. Our approach
utilizes conditional StyleGAN2-ADA and StyleGAN3 architectures to produce
high-resolution synthetic live fingerprints, conditioned on specific finger
identities (thumb through little finger). Additionally, we employ CycleGANs to
translate these into realistic spoof fingerprints, simulating a variety of
presentation attack materials (e.g., EcoFlex, Play-Doh). These synthetic spoof
fingerprints are crucial for developing robust spoof detection systems. Through
these generative models, we created two synthetic datasets (DB2 and DB3), each
containing 1,500 fingerprint images of all ten fingers with multiple
impressions per finger, and including corresponding spoofs in eight material
types. The results indicate robust performance: our StyleGAN3 model achieves a
Fr\'echet Inception Distance (FID) as low as 5, and the generated fingerprints
achieve a True Accept Rate of 99.47% at a 0.01% False Accept Rate. The
StyleGAN2-ADA model achieved a TAR of 98.67% at the same 0.01% FAR. We assess
fingerprint quality using standard metrics (NFIQ2, MINDTCT), and notably,
matching experiments confirm strong privacy preservation, with no significant
evidence of identity leakage, confirming the strong privacy-preserving
properties of our synthetic datasets.

</details>


### [103] [Click, Predict, Trust: Clinician-in-the-Loop AI Segmentation for Lung Cancer CT-Based Prognosis within the Knowledge-to-Action Framework](https://arxiv.org/abs/2510.17039)
*Mohammad R. Salmanpour,Sonya Falahati,Amir Hossein Pouria,Amin Mousavi,Somayeh Sadat Mehrnia,Morteza Alizadeh,Arman Gorji,Zeinab Farsangi,Alireza Safarian,Mehdi Maghsudi,Carlos Uribe,Arman Rahmim,Ren Yuan*

Main category: cs.CV

TL;DR: 本研究提出了一个结合临床医生反馈的深度学习自动分割管道，利用VNet网络和半监督学习，提升了肺癌CT图像分割的准确性、可重现性和临床可用性。


<details>
  <summary>Details</summary>
Motivation: 手动分割肺癌CT图像耗时且主观性强，现有深度学习方法自动化能力虽强，但在临床应用上存在信任和可采纳性障碍。因此需要开发一种既提升分割准确性又能获得临床医生认可的自动分割方法。

Method: 研究分析了来自12个公开数据集的999例肺癌患者CT数据，应用5种深度学习分割模型，并用专家人工标注作对比，采用PySERA提取497项影像组学特征，结合多种统计方法评价分割可重现性和影像组学稳定性。在预后建模中，比较了监督与半监督学习在不同降维与分类策略下的表现。同时有六位医生对分割掩膜的临床意义、边界质量等七项内容进行主观评价。

Result: VNet模型在分割准确性（Dice=0.83, IoU=0.71）、影像组学特征稳定性（均相关系数=0.76, ICC=0.65）和半监督学习下的预后预测（准确率=0.88, F1=0.83）方面表现最佳。半监督学习方法优于单纯监督学习。医生主观上更认可VNet的分割边界，对AI自动分割结果作为初稿并人工优化的方式较为认可。

Conclusion: 将VNet与半监督学习结合可以实现准确、稳定、具备临床信任的肺癌CT分割和预后预测，是推动以临床医师为中心的AI落地应用的可行路径。

Abstract: Lung cancer remains the leading cause of cancer mortality, with CT imaging
central to screening, prognosis, and treatment. Manual segmentation is variable
and time-intensive, while deep learning (DL) offers automation but faces
barriers to clinical adoption. Guided by the Knowledge-to-Action framework,
this study develops a clinician-in-the-loop DL pipeline to enhance
reproducibility, prognostic accuracy, and clinical trust. Multi-center CT data
from 999 patients across 12 public datasets were analyzed using five DL models
(3D Attention U-Net, ResUNet, VNet, ReconNet, SAM-Med3D), benchmarked against
expert contours on whole and click-point cropped images. Segmentation
reproducibility was assessed using 497 PySERA-extracted radiomic features via
Spearman correlation, ICC, Wilcoxon tests, and MANOVA, while prognostic
modeling compared supervised (SL) and semi-supervised learning (SSL) across 38
dimensionality reduction strategies and 24 classifiers. Six physicians
qualitatively evaluated masks across seven domains, including clinical
meaningfulness, boundary quality, prognostic value, trust, and workflow
integration. VNet achieved the best performance (Dice = 0.83, IoU = 0.71),
radiomic stability (mean correlation = 0.76, ICC = 0.65), and predictive
accuracy under SSL (accuracy = 0.88, F1 = 0.83). SSL consistently outperformed
SL across models. Radiologists favored VNet for peritumoral representation and
smoother boundaries, preferring AI-generated initial masks for refinement
rather than replacement. These results demonstrate that integrating VNet with
SSL yields accurate, reproducible, and clinically trusted CT-based lung cancer
prognosis, highlighting a feasible path toward physician-centered AI
translation.

</details>


### [104] [Person Re-Identification via Generalized Class Prototypes](https://arxiv.org/abs/2510.17043)
*Md Ahmed Al Muzaddid,William J. Beksi*

Main category: cs.CV

TL;DR: 本文提出了一种新的类代表选择方法，用于提升行人再识别准确率和平均精度，并在多种现有特征嵌入方法上均取得了优于当前最佳的结果。


<details>
  <summary>Details</summary>
Motivation: 虽然特征提取和目标函数优化促进了再识别性能，但在如何选择更优的类代表方面研究不足。作者注意到，现有方法多局限于使用类中心，且仅有少量工作在检索阶段探索了替代代表方式，这一领域有明显提升空间。

Method: 提出了一种泛化的类代表选择方法，打破了只用类中心的局限，根据需要灵活调整每类代表数量。该方法被应用于多种再识别特征嵌入方法的基础之上。

Result: 无论在哪种再识别嵌入方法上，新的类代表选择方法都显著超越了当前同类最新技术，具体在准确率和平均精度指标上均有提升。

Conclusion: 选择合适的类代表对行人再识别有重要影响，所提出的方法在准确率和平均精度间实现了良好平衡，并且具有实际应用灵活性。

Abstract: Advanced feature extraction methods have significantly contributed to
enhancing the task of person re-identification. In addition, modifications to
objective functions have been developed to further improve performance.
Nonetheless, selecting better class representatives is an underexplored area of
research that can also lead to advancements in re-identification performance.
Although past works have experimented with using the centroid of a gallery
image class during training, only a few have investigated alternative
representations during the retrieval stage. In this paper, we demonstrate that
these prior techniques yield suboptimal results in terms of re-identification
metrics. To address the re-identification problem, we propose a generalized
selection method that involves choosing representations that are not limited to
class centroids. Our approach strikes a balance between accuracy and mean
average precision, leading to improvements beyond the state of the art. For
example, the actual number of representations per class can be adjusted to meet
specific application requirements. We apply our methodology on top of multiple
re-identification embeddings, and in all cases it substantially improves upon
contemporary results

</details>


### [105] [Video Reasoning without Training](https://arxiv.org/abs/2510.17045)
*Deepak Sridhar,Kartikeya Bhardwaj,Jeya Pradha Jeyaraj,Nuno Vasconcelos,Ankita Nayak,Harris Teague*

Main category: cs.CV

TL;DR: 该论文提出了一种无需强化学习、监督微调的视频推理推理方法V-Reason，通过熵信号在推理过程直接优化LMM模型，提高推理效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型多模态模型的视频推理方法依赖强化学习和多步推理链，训练和推理阶段计算开销大，且推理过程控制不足。作者希望改善推理效率，去除高成本的RL训练，同时增强模型推理的可控性和收敛性。

Method: 作者通过分析模型输出的熵，发现高质量模型的推理过程呈微观探索与利用交替，并最终收敛。基于这一理论见解，提出在推理阶段通过熵目标在小型可训练控制器上优化LMM的value cache，无需任何数据集监督或RL训练，从而改进模型推理过程。该方法命名为V-Reason。

Result: V-Reason方法在多个视频推理数据集上，相较于基线指令微调模型显著提升了准确率，与RL主导模型的平均准确率差距缩小到0.6%，且输出token数比RL模型减少58.6%，推理速度和效率大幅提升。

Conclusion: 作者证明了无需RL或监督微调，仅通过推理阶段基于熵的简单优化即可大幅提升LMM在视频推理任务的表现，在效率与效果间取得了优越平衡。

Abstract: Video reasoning using Large Multimodal Models (LMMs) relies on costly
reinforcement learning (RL) and verbose chain-of-thought, resulting in
substantial computational overhead during both training and inference.
Moreover, the mechanisms that control the thinking process in these reasoning
models are very limited. In this paper, using entropy of the model's output as
a signal, we discover that the high-quality models go through a series of
micro-explorations and micro-exploitations which keep the reasoning process
grounded (i.e., avoid excessive randomness while the model is exploring or
thinking through an answer). We further observe that once this "thinking"
process is over, more accurate models demonstrate a better convergence by
reducing the entropy significantly via a final exploitation phase (i.e., a more
certain convergence towards a solution trajectory). We then use these novel,
theoretically-grounded insights to tune the model's behavior directly at
inference, without using any RL or supervised fine-tuning. Specifically, during
inference, our proposed approach called V-Reason (Video-Reason) adapts the
value cache of the LMM via a few optimization steps on a small, trainable
controller using an entropy-based objective, i.e., no supervision from any
dataset or RL is necessary. This tuning improves the model's micro-exploration
and exploitation behavior during inference. Our experiments show that our
proposed method achieves significant improvements over the base
instruction-tuned models across several video reasoning datasets, narrowing the
gap with RL-trained models to within 0.6% average accuracy without any
training, while offering massive efficiency benefits: output tokens are reduced
by 58.6% compared to the RL model.

</details>


### [106] [How Universal Are SAM2 Features?](https://arxiv.org/abs/2510.17051)
*Masoud Khairi Atani,Alon Harell,Hyomin Choi,Runyu Yang,Fabien Racape,Ivan V. Bajic*

Main category: cs.CV

TL;DR: 本文对通用视觉基础模型与专用模型在特征表示上的权衡进行了定量分析。通过对Hiera和SAM2的对比，揭示了专用模型在特定任务上表现优势的同时，牺牲了更广泛语义信息的通用性，从而为后续特征编码和适应性方案设计提供了参考。


<details>
  <summary>Details</summary>
Motivation: 当前基础视觉模型分为通用型和专用型两大类，但二者在特征编码设计上的高效性权衡尚不清楚。本文旨在深入理解并量化通用模型与专用模型在适应各种下游任务时的信息损耗与性能差异。

Method: 通过对冻结的通用模型（Hiera）与分割专用模型（SAM2）增加可训练的轻量级neck模块，评估它们在不同下游任务（如深度估计、姿态估计、图像描述）中的特征适应和表现。同时，提出cross-neck分析方法，探究多级适应对特征表达的限制。

Result: 实验表明，SAM2在空间相关任务（如深度估计）上优于Hiera，但在概念距离较大的任务（如姿态估计、图像描述）上，SAM2的表现明显不如Hiera。进一步cross-neck分析显示，适应的每一级都会带来特征表达的信息瓶颈。

Conclusion: 专用模型在其目标任务上的表现出色，但牺牲了特征的通用性和泛化能力。本文工作为理解特征通用性和高效特征编码、适应性策略的设计提供了定量依据。

Abstract: The trade-off between general-purpose foundation vision models and their
specialized counterparts is critical for efficient feature coding design and is
not yet fully understood. We investigate this trade-off by comparing the
feature versatility of the general-purpose Hiera encoder against the
segmentation-specialized Segment Anything Model 2 (SAM2). Using a lightweight,
trainable neck to probe the adaptability of their frozen features, we quantify
the information-theoretic cost of specialization. Our results reveal that while
SAM2's specialization is highly effective for spatially-related tasks like
depth estimation, it comes at a cost. The specialized SAM2 encoder
underperforms its generalist predecessor, Hiera, on conceptually distant tasks
such as pose estimation and image captioning, demonstrating a measurable loss
of broader semantic information. A novel cross-neck analysis on SAM2 reveals
that each level of adaptation creates a further representational bottleneck.
Our analysis illuminates these trade-offs in feature universality, providing a
quantitative foundation for designing efficient feature coding and adaptation
strategies for diverse downstream applications.

</details>


### [107] [ProDAT: Progressive Density-Aware Tail-Drop for Point Cloud Coding](https://arxiv.org/abs/2510.17068)
*Zhe Luo,Wenjing Jia,Stuart Perry*

Main category: cs.CV

TL;DR: 本文提出了一种新方法（ProDAT），能够对三维点云数据进行分级逐步编码和解码，并在多种数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 三维点云数据在多种场景下非常重要，但数据量大且带宽有限，迫切需要高效的分层编码技术，而现有学习型方法无法支持渐进和多比特率解码。

Method: 提出了ProDAT密度感知尾部丢弃机制，根据点云密度指导性地自适应解码潜在特征和坐标，实现了单模型多比特率渐进解码。

Result: 在SemanticKITTI和ShapeNet两大基准数据集上，ProDAT实现了分层逐步编码，比最先进的方法分别提升28.6%和18.15%的BD-rate。

Conclusion: ProDAT不仅支持分级编码，且具备更高的编码效率，适合资源受限环境下三维点云应用。

Abstract: Three-dimensional (3D) point clouds are becoming increasingly vital in
applications such as autonomous driving, augmented reality, and immersive
communication, demanding real-time processing and low latency. However, their
large data volumes and bandwidth constraints hinder the deployment of
high-quality services in resource-limited environments. Progres- sive coding,
which allows for decoding at varying levels of detail, provides an alternative
by allowing initial partial decoding with subsequent refinement. Although
recent learning-based point cloud geometry coding methods have achieved notable
success, their fixed latent representation does not support progressive
decoding. To bridge this gap, we propose ProDAT, a novel density-aware
tail-drop mechanism for progressive point cloud coding. By leveraging density
information as a guidance signal, latent features and coordinates are decoded
adaptively based on their significance, therefore achieving progressive
decoding at multiple bitrates using one single model. Experimental results on
benchmark datasets show that the proposed ProDAT not only enables progressive
coding but also achieves superior coding efficiency compared to
state-of-the-art learning-based coding techniques, with over 28.6% BD-rate
improvement for PSNR- D2 on SemanticKITTI and over 18.15% for ShapeNet

</details>


### [108] [Towards a Generalizable Fusion Architecture for Multimodal Object Detection](https://arxiv.org/abs/2510.17078)
*Jad Berjawi,Yoann Dupas,Christophe C'erin*

Main category: cs.CV

TL;DR: 本文提出了一种用于RGB与红外(IR)图像融合的检测模型架构FMCAF，通过频域滤波和跨模态注意力机制提升多模态目标检测鲁棒性，并在多个数据集上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境下，单一模态目标检测易受干扰。多模态融合（如RGB与红外）能利用各模态互补特性提升检测鲁棒性，但当前方法普遍对特定数据集优化，泛化性不足。因此，作者希望构建一种无需数据集特定调优、能广泛适用的通用多模态融合方案。

Method: FMCAF包括两部分：1）Freq-Filter频域滤波模块，用于抑制冗余光谱特征，提炼有效信息；2）基于跨模态注意力的融合模块MCAF，加强不同模态间的信息互动和共享，从根源提升融合效率。整个架构设计为预处理模块，可无缝接入不同的数据或检测任务，无需针对特定数据集做深度调整。

Result: 在LLVIP（低照度行人检测）和VEDAI（航拍车辆检测）数据集上，FMCAF均优于传统串接式（concatenation）融合方法，在VEDAI上mAP@50提升+13.9%，在LLVIP上提升+1.1%。

Conclusion: FMCAF是一种鲁棒、灵活且具有良好泛化能力的多模态融合基础架构，不依赖特定数据集调参与网络深度调整，能够有效提升多模态检测模型在不同应用场景下的表现。

Abstract: Multimodal object detection improves robustness in chal- lenging conditions
by leveraging complementary cues from multiple sensor modalities. We introduce
Filtered Multi- Modal Cross Attention Fusion (FMCAF), a preprocess- ing
architecture designed to enhance the fusion of RGB and infrared (IR) inputs.
FMCAF combines a frequency- domain filtering block (Freq-Filter) to suppress
redun- dant spectral features with a cross-attention-based fusion module (MCAF)
to improve intermodal feature sharing. Unlike approaches tailored to specific
datasets, FMCAF aims for generalizability, improving performance across
different multimodal challenges without requiring dataset- specific tuning. On
LLVIP (low-light pedestrian detec- tion) and VEDAI (aerial vehicle detection),
FMCAF outper- forms traditional fusion (concatenation), achieving +13.9% mAP@50
on VEDAI and +1.1% on LLVIP. These results support the potential of FMCAF as a
flexible foundation for robust multimodal fusion in future detection pipelines.

</details>


### [109] [GSPlane: Concise and Accurate Planar Reconstruction via Structured Representation](https://arxiv.org/abs/2510.17095)
*Ruitong Gan,Junran Peng,Yang Liu,Chuanchen Luo,Qing Li,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为GSPlane的新方法，通过引入平面先验和结构化高斯坐标，使3D场景中的平面区域重建更加平滑且精确，同时提升了网格结构质量，便于下游应用编辑和物理仿真。


<details>
  <summary>Details</summary>
Motivation: 虽现有的高斯散射（GS）方法在新视图合成和表面重建上表现出色，但在平面区域的光滑与精确重建上仍存在不足。平面是三维场景中的基本组成，特别在室内和城市街景等人造环境中。因此，高效且精确地重建这些平面对后续应用（如场景编辑、物理仿真等）至关重要。

Method: GSPlane利用现成的分割和法线预测模型，提取稳健的平面先验，通过结构化的高斯坐标表示平面区域，并引入几何一致性约束辅助训练。同时，动态高斯再分类器能将训练中梯度持续较高的平面高斯自适应地归为非平面，从而提升训练的稳健性。最后，利用优化后的平面先验对网格结构进行完善，减少顶点和面数，并提升拓扑结构。

Result: 实验表明在不牺牲渲染质量的前提下，GSPlane能大幅提升重建平面区域的几何精度，生成的网格拓扑更紧凑，且在多个基线方法下均有效。结构化平面表示还支持对象在支撑平面上的灵活操作。

Conclusion: GSPlane为基于高斯散射的新视角合成与三维重建引入了平面结构化表示，显著提升了平面几何精度和网格结构质量，为下游三维编辑与仿真任务提供了更优的基础。

Abstract: Planes are fundamental primitives of 3D sences, especially in man-made
environments such as indoor spaces and urban streets. Representing these planes
in a structured and parameterized format facilitates scene editing and physical
simulations in downstream applications. Recently, Gaussian Splatting (GS) has
demonstrated remarkable effectiveness in the Novel View Synthesis task, with
extensions showing great potential in accurate surface reconstruction. However,
even state-of-the-art GS representations often struggle to reconstruct planar
regions with sufficient smoothness and precision. To address this issue, we
propose GSPlane, which recovers accurate geometry and produces clean and
well-structured mesh connectivity for plane regions in the reconstructed scene.
By leveraging off-the-shelf segmentation and normal prediction models, GSPlane
extracts robust planar priors to establish structured representations for
planar Gaussian coordinates, which help guide the training process by enforcing
geometric consistency. To further enhance training robustness, a Dynamic
Gaussian Re-classifier is introduced to adaptively reclassify planar Gaussians
with persistently high gradients as non-planar, ensuring more reliable
optimization. Furthermore, we utilize the optimized planar priors to refine the
mesh layouts, significantly improving topological structure while reducing the
number of vertices and faces. We also explore applications of the structured
planar representation, which enable decoupling and flexible manipulation of
objects on supportive planes. Extensive experiments demonstrate that, with no
sacrifice in rendering quality, the introduction of planar priors significantly
improves the geometric accuracy of the extracted meshes across various
baselines.

</details>


### [110] [Boosting Fidelity for Pre-Trained-Diffusion-Based Low-Light Image Enhancement via Condition Refinement](https://arxiv.org/abs/2510.17105)
*Xiaogang Xu,Jian Wang,Yunfan Lu,Ruihang Chu,Ruixing Wang,Jiafei Wu,Bei Yu,Liang Lin*

Main category: cs.CV

TL;DR: 本文提出了一种新的优化策略，用于提升预训练扩散模型（如Stable Diffusion+ControlNet）处理低光等低水平视觉任务时的内容保真度，同时保持图像真实感与美学。


<details>
  <summary>Details</summary>
Motivation: 现有的基于预训练扩散模型的方法虽然能实现优秀的视觉效果，但多以内容保真度为代价，尤其是在光照不足、细节缺失严重的场景中更为突出，亟需兼顾保真度与感知效果的新方法。

Method: 作者分析了保真度损失的两大原因：缺乏合适的条件潜变量建模、条件潜变量与噪声潜变量之间缺乏双向交互。为此，作者提出通过潜变量精炼机制和动态交互策略，恢复VAE编码丢失的空间细节，改进条件建模过程，且该方法可无缝集成到现有扩散模型中。

Result: 实验表明，所提出的机制在多个低水平视觉任务中显著提高了现有PTDB方法的保真度，图像细节得到更好地恢复，同时保留了视觉真实感和美学。

Conclusion: 该方法无需对扩散模型架构进行大改动，即插即用，实现了内容保真度与感知质量的平衡，为预训练扩散模型低光修复等任务提供了更优解决方案。

Abstract: Diffusion-based methods, leveraging pre-trained large models like Stable
Diffusion via ControlNet, have achieved remarkable performance in several
low-level vision tasks. However, Pre-Trained Diffusion-Based (PTDB) methods
often sacrifice content fidelity to attain higher perceptual realism. This
issue is exacerbated in low-light scenarios, where severely degraded
information caused by the darkness limits effective control. We identify two
primary causes of fidelity loss: the absence of suitable conditional latent
modeling and the lack of bidirectional interaction between the conditional
latent and noisy latent in the diffusion process. To address this, we propose a
novel optimization strategy for conditioning in pre-trained diffusion models,
enhancing fidelity while preserving realism and aesthetics. Our method
introduces a mechanism to recover spatial details lost during VAE encoding,
i.e., a latent refinement pipeline incorporating generative priors.
Additionally, the refined latent condition interacts dynamically with the noisy
latent, leading to improved restoration performance. Our approach is
plug-and-play, seamlessly integrating into existing diffusion networks to
provide more effective control. Extensive experiments demonstrate significant
fidelity improvements in PTDB methods.

</details>


### [111] [$\mathcal{V}isi\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.17205)
*Yingqi Fan,Anhao Zhao,Jinlan Fu,Junlong Tong,Hui Su,Yijie Pan,Wei Zhang,Xiaoyu Shen*

Main category: cs.CV

TL;DR: 本文提出VisiPruner，一种无需重新训练的多模态视觉Token剪枝框架，极大减少了多模态大模型中的视觉相关计算量，并显著提升了剪枝性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型（MLLMs）在视觉-语言任务中表现出色，但由于注意力计算随多模态Token数量增长而增长，导致计算开销巨大。现有剪枝方法对MLLM的信息处理缺乏深入理解，限制了效率提升。

Method: 通过系统性分析，作者揭示了MLLM中视觉与语言信息融合的三阶段过程，并据此提出了VisiPruner框架。该框架在不依赖训练的情况下，有效识别和剪除大量无用视觉Token，从而显著减少视觉相关的计算。

Result: 该方法在LLaVA-v1.5 7B模型上削减了高达99%的视觉相关注意力计算和53.9%的FLOPs，在各种MLLMs上均优于已有剪枝方法。

Conclusion: VisiPruner不仅提升MLLM推理效率，还为高效模型设计和训练提供新思路。

Abstract: Multimodal Large Language Models (MLLMs) have achieved strong performance
across vision-language tasks, but suffer from significant computational
overhead due to the quadratic growth of attention computations with the number
of multimodal tokens. Though efforts have been made to prune tokens in MLLMs,
\textit{they lack a fundamental understanding of how MLLMs process and fuse
multimodal information.} Through systematic analysis, we uncover a
\textbf{three-stage} cross-modal interaction process: (1) Shallow layers
recognize task intent, with visual tokens acting as passive attention sinks;
(2) Cross-modal fusion occurs abruptly in middle layers, driven by a few
critical visual tokens; (3) Deep layers discard vision tokens, focusing solely
on linguistic refinement. Based on these findings, we propose
\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\% of
vision-related attention computations and 53.9\% of FLOPs on LLaVA-v1.5 7B. It
significantly outperforms existing token pruning methods and generalizes across
diverse MLLMs. Beyond pruning, our insights further provide actionable
guidelines for training efficient MLLMs by aligning model architecture with its
intrinsic layer-wise processing dynamics. Our code is available at:
https://github.com/EIT-NLP/VisiPruner.

</details>


### [112] [Towards Imperceptible Watermarking Via Environment Illumination for Consumer Cameras](https://arxiv.org/abs/2510.17114)
*Hodaka Kawachi,Tomoya Nakamura,Hiroaki Santo,SaiKiran Kumar Tedla,Trevor Dalton Canham,Yasushi Yagi,Michael S. Brown*

Main category: cs.CV

TL;DR: 本论文提出了一种利用LED环境照明，实现对消费级相机而言可检测、但对人眼不可见的光学水印方法。该方法通过优化LED光源的光谱，实现信息嵌入并保证视觉不可察觉。


<details>
  <summary>Details</summary>
Motivation: 当前光学水印技术要么易被人眼察觉，要么对普通相机不友好，亟需在隐蔽性和可检测性间取得平衡，满足如隐私保护和内容验证等实际需求。

Method: 该方法综合考虑人眼对可见光谱的敏感度、消费级相机传感器的光谱响应以及窄带LED混合实现白光输出的能力（D65照明标准），采用光谱调制（而非强度调制）进行信息编码，使水印信息对人眼不可见但对摄像头可检测。

Result: 能够在10秒视频片段中嵌入128比特的水印信息，对普通人眼几乎无法察觉，但消费级相机可稳定识别，且可用低帧率摄像头（30-60fps）提取水印信息。

Conclusion: 该方法在保障隐蔽性的前提下，满足了低速率的视频水印需求，可用于内容验证和隐私保护等应用场景，为LED照明下的信息隐藏提供了新途径。

Abstract: This paper introduces a method for using LED-based environmental lighting to
produce visually imperceptible watermarks for consumer cameras. Our approach
optimizes an LED light source's spectral profile to be minimally visible to the
human eye while remaining highly detectable by typical consumer cameras. The
method jointly considers the human visual system's sensitivity to visible
spectra, modern consumer camera sensors' spectral sensitivity, and narrowband
LEDs' ability to generate broadband spectra perceived as "white light"
(specifically, D65 illumination). To ensure imperceptibility, we employ
spectral modulation rather than intensity modulation. Unlike conventional
visible light communication, our approach enables watermark extraction at
standard low frame rates (30-60 fps). While the information transfer rate is
modest-embedding 128 bits within a 10-second video clip-this capacity is
sufficient for essential metadata supporting privacy protection and content
verification.

</details>


### [113] [UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action](https://arxiv.org/abs/2510.17790)
*Yuhao Yang,Zhen Yang,Zi-Yi Dou,Anh Nguyen,Keen You,Omar Attia,Andrew Szot,Michael Feng,Ram Ramrakhya,Alexander Toshev,Chao Huang,Yinfei Yang,Zhe Gan*

Main category: cs.CV

TL;DR: 本文提出UltraCUA，一种结合了GUI原语操作与高阶编程工具调用的多模态混合操作计算机代理模型，在多项评测中显著超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前计算机使用多模态代理主要依赖低级GUI操作，如点击、输入、滚动，难以高效完成复杂任务，容易造成错误传递和性能瓶颈，而其他智能体能借助丰富的程序化接口，计算机使用代理（CUAs）则与这些能力脱节。该工作旨在打通这一隔阂。

Method: UltraCUA采用四个核心组件：（1）自动流程大规模提取和扩展程序化工具；（2）合成超过17000个可验证真实计算机使用任务的数据引擎；（3）大规模高质量混合操作数据集采集，包含低级GUI操作和高级工具调用；（4）分两阶段——有监督微调和在线强化学习——进行训练，实现对低级和高级动作的策略性选择。

Result: UltraCUA在7B和32B模型上远超最先进代理。在OSWorld任务上，平均相对提升22%，执行步数快11%；在WindowsAgentArena的泛化测试中，达到21.7%成功率，明显领先于以Windows数据训练的基线模型；混合操作机制有效减少错误传递，提升效率。

Conclusion: UltraCUA通过引入混合操作能力，首次使CUA能高效结合低级GUI原语和高级程序化工具，显著提升任务成功率和执行效率，为下一代智能计算机使用代理树立了新标准。

Abstract: Multimodal agents for computer use rely exclusively on primitive actions
(click, type, scroll) that require accurate visual grounding and lengthy
execution chains, leading to cascading failures and performance bottlenecks.
While other agents leverage rich programmatic interfaces (APIs, MCP servers,
tools), computer-use agents (CUAs) remain isolated from these capabilities. We
present UltraCUA, a foundation model that bridges this gap through hybrid
action -- seamlessly integrating GUI primitives with high-level programmatic
tool calls. To achieve this, our approach comprises four key components: (1) an
automated pipeline that scales programmatic tools from software documentation,
open-source repositories, and code generation; (2) a synthetic data engine
producing over 17,000 verifiable tasks spanning real-world computer-use
scenarios; (3) a large-scale high-quality hybrid action trajectory collection
with both low-level GUI actions and high-level programmatic tool calls; and (4)
a two-stage training pipeline combining supervised fine-tuning with online
reinforcement learning, enabling strategic alternation between low-level and
high-level actions. Experiments with our 7B and 32B models demonstrate
substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA
models achieve an average 22% relative improvement over base models, while
being 11% faster in terms of steps. Out-of-domain evaluation on
WindowsAgentArena shows our model reaches 21.7% success rate, outperforming
baselines trained on Windows data. The hybrid action mechanism proves critical,
reducing error propagation while maintaining execution efficiency.

</details>


### [114] [GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection](https://arxiv.org/abs/2510.17131)
*Xin Gao,Jiyao Liu,Guanghao Li,Yueming Lyu,Jianxiong Gao,Weichen Yu,Ningsheng Xu,Liang Wang,Caifeng Shan,Ziwei Liu,Chenyang Si*

Main category: cs.CV

TL;DR: 本论文提出了一种新的利用扩散模型生成OOD样本的框架（GOOD），通过双重引导机制高效提升了异常检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的扩散模型生成OOD样本用于异常检测的方法存在语义不稳定及生成样本多样性不足的问题，导致对真实场景下的异常检测泛化能力有限。为此，作者希望提升生成OOD样本的多样性与质量，从而支持更稳健的异常检测。

Method: 作者提出了GOOD框架：利用现成的ID分类器，直接引导扩散采样轨迹向OOD区域。GOOD包含两级引导机制：（1）基于图像级别的log partition梯度，引导样本向像素空间的低密度区域迁移；（2）基于特征空间（通过分类器隐空间的k-NN距离）的特征级别引导，鼓励生成特征稀疏区域的样本。两者结合提升了生成样本的多样性和可控性。此外，作者还提出了自适应融合图像和特征差异的统一OOD分数，用于增强异常检测的鲁棒性。

Result: 实验通过定量与定性分析验证了GOOD的有效性：用GOOD生成的样本训练，可以显著提升OOD检测性能。

Conclusion: GOOD框架能够灵活且有效地生成多样、可控的OOD样本，并提升后续的异常检测表现，对实际应用具有潜力。

Abstract: Recent advancements have explored text-to-image diffusion models for
synthesizing out-of-distribution (OOD) samples, substantially enhancing the
performance of OOD detection. However, existing approaches typically rely on
perturbing text-conditioned embeddings, resulting in semantic instability and
insufficient shift diversity, which limit generalization to realistic OOD. To
address these challenges, we propose GOOD, a novel and flexible framework that
directly guides diffusion sampling trajectories towards OOD regions using
off-the-shelf in-distribution (ID) classifiers. GOOD incorporates dual-level
guidance: (1) Image-level guidance based on the gradient of log partition to
reduce input likelihood, drives samples toward low-density regions in pixel
space. (2) Feature-level guidance, derived from k-NN distance in the
classifier's latent space, promotes sampling in feature-sparse regions. Hence,
this dual-guidance design enables more controllable and diverse OOD sample
generation. Additionally, we introduce a unified OOD score that adaptively
combines image and feature discrepancies, enhancing detection robustness. We
perform thorough quantitative and qualitative analyses to evaluate the
effectiveness of GOOD, demonstrating that training with samples generated by
GOOD can notably enhance OOD detection performance.

</details>


### [115] [Glyph: Scaling Context Windows via Visual-Text Compression](https://arxiv.org/abs/2510.17800)
*Jiale Cheng,Yusen Liu,Xinyu Zhang,Yulin Fei,Wenyi Hong,Ruiliang Lyu,Weihan Wang,Zhe Su,Xiaotao Gu,Xiao Liu,Yushi Bai,Jie Tang,Hongning Wang,Minlie Huang*

Main category: cs.CV

TL;DR: 本文提出了一种全新处理超长文本的方法Glyph，将文本渲染为图像后用视觉-语言模型处理，实现大幅度的压缩并节省算力和内存开销。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型要应对更长的上下文任务（如文档理解、代码分析、多步推理），但将上下文扩展到百万token会导致算力和显存成本过高，因此需要新的技术突破。

Method: Glyph将长文本渲染成图像，由视觉-语言模型（VLM）处理，用遗传算法搜索最优渲染和压缩配置，在保证理解准确度的前提下极大压缩输入长度。

Result: Glyph在多个长文本任务上实现了3-4倍的token压缩比，准确率接近主流大模型（如Qwen3-8B），同时推理速度加速约4倍，微调训练效率提升约2倍，在极端压缩下，VLM甚至可支持100万token级别文本理解。同时，该方法对多模态应用（如文档理解）有实际效果提升。

Conclusion: Glyph极大提升了模型处理超长文本的能力与效率，为长上下文任务提供了切实可行的新范式，并可迁移到多模态现实场景。

Abstract: Large language models (LLMs) increasingly rely on long-context modeling for
tasks such as document understanding, code analysis, and multi-step reasoning.
However, scaling context windows to the million-token level brings prohibitive
computational and memory costs, limiting the practicality of long-context LLMs.
In this work, we take a different perspective-visual context scaling-to tackle
this challenge. Instead of extending token-based sequences, we propose Glyph, a
framework that renders long texts into images and processes them with
vision-language models (VLMs). This approach substantially compresses textual
input while preserving semantic information, and we further design an
LLM-driven genetic search to identify optimal visual rendering configurations
for balancing accuracy and compression. Through extensive experiments, we
demonstrate that our method achieves 3-4x token compression while maintaining
accuracy comparable to leading LLMs such as Qwen3-8B on various long-context
benchmarks. This compression also leads to around 4x faster prefilling and
decoding, and approximately 2x faster SFT training. Furthermore, under extreme
compression, a 128K-context VLM could scale to handle 1M-token-level text
tasks. In addition, the rendered text data benefits real-world multimodal
tasks, such as document understanding. Our code and model are released at
https://github.com/thu-coai/Glyph.

</details>


### [116] [KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation](https://arxiv.org/abs/2510.17137)
*WenBo Xu,Liu Liu,Li Zhang,Ran Zhang,Hao Wu,Dan Guo,Meng Wang*

Main category: cs.CV

TL;DR: 本文提出了KineDiff3D，一种结合运动学感知扩散模型的统一框架，实现对多部件可动对象（如笔记本电脑和抽屉）从单视图输入的3D重建和姿态估计。该方法利用VAE和扩散模型，并通过优化迭代提升重建及参数估计性能。实验证明其在多数据集下效果优异。


<details>
  <summary>Details</summary>
Motivation: 多部件可动对象因其关节连接和结构变化，使得3D重建和姿态估计极具挑战。现有方法难以同时准确重建结构和推断运动学状态，因此需要一种能有效处理运动学多样性和结构多样性的统一方法。

Method: 1. 提出Kinematic-Aware VAE (KA-VAE)，编码几何、关节角度及零件分割信息至潜在空间。2. 设计两个条件扩散模型：一个用于回归全局姿态与关节参数，另一个用于从观察到的部分生成运动学潜在编码。3. 提出迭代优化模块，通过Chamfer距离最小化双向优化重建和运动参数，同时保证运动学约束。

Result: 在合成、半合成和真实数据集上进行了实验，结果显示本文方法在可动对象的3D形状重建及运动学参数估计上表现出了高精度和有效性。

Conclusion: KineDiff3D框架能有效处理类别级可动对象的单视图3D重建与姿态估计问题，在结构及运动状态的精确还原上优于现有方法，为该领域提供了新思路。

Abstract: Articulated objects, such as laptops and drawers, exhibit significant
challenges for 3D reconstruction and pose estimation due to their multi-part
geometries and variable joint configurations, which introduce structural
diversity across different states. To address these challenges, we propose
KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object
Shape Reconstruction and Generation, a unified framework for reconstructing
diverse articulated instances and pose estimation from single view input.
Specifically, we first encode complete geometry (SDFs), joint angles, and part
segmentation into a structured latent space via a novel Kinematic-Aware VAE
(KA-VAE). In addition, we employ two conditional diffusion models: one for
regressing global pose (SE(3)) and joint parameters, and another for generating
the kinematic-aware latent code from partial observations. Finally, we produce
an iterative optimization module that bidirectionally refines reconstruction
accuracy and kinematic parameters via Chamfer-distance minimization while
preserving articulation constraints. Experimental results on synthetic,
semi-synthetic, and real-world datasets demonstrate the effectiveness of our
approach in accurately reconstructing articulated objects and estimating their
kinematic properties.

</details>


### [117] [GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation from Single Image](https://arxiv.org/abs/2510.17157)
*Yinghui Wang,Xinyu Zhang,Peng Du*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的两阶段后训练框架GACO-CAD，可利用单张图片自动生成可编辑、参数化的CAD模型，将3D建模门槛显著降低，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型（MLLM）在2D图片转3D CAD建模任务上受限于空间推理能力，难以准确还原复杂的3D几何形状。工业等领域亟需简单高效的解决方案，以推动自动化设计的发展。

Method: 作者提出GACO-CAD框架，分为两个阶段：在监督微调阶段，引入深度图和法线图作为空间先验，与RGB图像组成多通道输入，引导MLLM更好地从单视图重建3D几何；在强化学习阶段，设计分组长度奖励，兼顾几何精度和建模指令简洁性，并采用动态权重平衡训练稳定性。

Result: 在DeepCAD和Fusion360数据集上，GACO-CAD在相同MLLM主干下，实现了当前最佳水平的表现，不仅几何准确率更高，生成的建模指令序列也更紧凑冗余更少，有效提升了代码可用性和模型精度。

Conclusion: GACO-CAD框架显著提升了多模态大模型由单张图片推断高质量3D参数化CAD模型的能力，为工业快速设计与智能建模提供了有力工具，在几何准确率和建模简洁性上均超越了已有方法。

Abstract: Generating editable, parametric CAD models from a single image holds great
potential to lower the barriers of industrial concept design. However, current
multi-modal large language models (MLLMs) still struggle with accurately
inferring 3D geometry from 2D images due to limited spatial reasoning
capabilities. We address this limitation by introducing GACO-CAD, a novel
two-stage post-training framework. It is designed to achieve a joint objective:
simultaneously improving the geometric accuracy of the generated CAD models and
encouraging the use of more concise modeling procedures. First, during
supervised fine-tuning, we leverage depth and surface normal maps as dense
geometric priors, combining them with the RGB image to form a multi-channel
input. In the context of single-view reconstruction, these priors provide
complementary spatial cues that help the MLLM more reliably recover 3D geometry
from 2D observations. Second, during reinforcement learning, we introduce a
group length reward that, while preserving high geometric fidelity, promotes
the generation of more compact and less redundant parametric modeling
sequences. A simple dynamic weighting strategy is adopted to stabilize
training. Experiments on the DeepCAD and Fusion360 datasets show that GACO-CAD
achieves state-of-the-art performance under the same MLLM backbone,
consistently outperforming existing methods in terms of code validity,
geometric accuracy, and modeling conciseness.

</details>


### [118] [Investigating Adversarial Robustness against Preprocessing used in Blackbox Face Recognition](https://arxiv.org/abs/2510.17169)
*Roland Croft,Brian Du,Darcy Joseph,Sharath Kumar*

Main category: cs.CV

TL;DR: 本文分析了人脸识别系统中的人脸预处理对对抗攻击转移性的影响，并提出了一种预处理不变的输入变换方法以提升攻击效果。


<details>
  <summary>Details</summary>
Motivation: 当前人脸识别系统虽在实际应用中广泛部署，但对抗攻击能导致系统错误识别或保护隐私。已有研究多忽略了人脸预处理在黑盒攻击场景下对攻击效果的影响，因此需要系统地分析预处理流程对对抗攻击转移性的影响。

Method: 作者研究了现成几种主流的对抗攻击方法，针对不同的人脸预处理技术（如不同人脸检测模型与插值下采样方法）在黑盒和白盒场景下的攻击转移性。实验证明，预处理流程中的人脸检测模型对攻击成功率影响较大。随后提出预处理不变的输入变换方法，提升现有对抗攻击在不同预处理流程下的转移性。

Result: 人脸检测模型的选择可使攻击成功率下降高达78%，而下采样插值方法对攻击影响不大；即使在白盒设置下，预处理步骤也会弱化攻击；提出的预处理不变输入变换能使攻击转移性提升多达27%。

Conclusion: 人脸预处理是人脸识别系统对抗攻击不可忽视的环节，需要在设计对抗样本时充分考虑预处理带来的影响，文中提出的方法能有效提升对抗攻击的转移性，为后续对抗性研究和防御设计提供了参考。

Abstract: Face Recognition (FR) models have been shown to be vulnerable to adversarial
examples that subtly alter benign facial images, exposing blind spots in these
systems, as well as protecting user privacy. End-to-end FR systems first obtain
preprocessed faces from diverse facial imagery prior to computing the
similarity of the deep feature embeddings. Whilst face preprocessing is a
critical component of FR systems, and hence adversarial attacks against them,
we observe that this preprocessing is often overlooked in blackbox settings.
Our study seeks to investigate the transferability of several out-of-the-box
state-of-the-art adversarial attacks against FR when applied against different
preprocessing techniques used in a blackbox setting. We observe that the choice
of face detection model can degrade the attack success rate by up to 78%,
whereas choice of interpolation method during downsampling has relatively
minimal impacts. Furthermore, we find that the requirement for facial
preprocessing even degrades attack strength in a whitebox setting, due to the
unintended interaction of produced noise vectors against face detection models.
Based on these findings, we propose a preprocessing-invariant method using
input transformations that improves the transferability of the studied attacks
by up to 27%. Our findings highlight the importance of preprocessing in FR
systems, and the need for its consideration towards improving the adversarial
generalisation of facial adversarial examples.

</details>


### [119] [Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling](https://arxiv.org/abs/2510.17171)
*Feihong Yan,Peiru Wang,Yao Zhu,Kaiyu Pang,Qingyan Wei,Huiqi Li,Linfeng Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种提升视觉生成效率的新方法，通过分阶段处理结构和细节，显著加速生成过程，同时保持高质量输出。


<details>
  <summary>Details</summary>
Motivation: 传统的自回归（AR）模型虽然生成质量高，但生成速度慢，仅能逐步生成视觉内容。掩码自回归（MAR）模型支持并行生成，理论上效率更高，但在处理空间强相关的视觉Token时加速受限，难以充分发挥其潜力。

Method: 提出了一种无须重新训练的两阶段（分层）采样策略——Generation then Reconstruction（GtR）。第一阶段生成图像的全局结构，搭建语义支架；第二阶段在结构基础上高效重建详细内容。为进一步提升细节质量和计算效率，引入频率加权Token选择（FTS），根据高频信息分配更多计算预算于图像细节Token。

Result: 在ImageNet类条件和文生图任务中，MAR-H模型通过GtR方法实现了3.72倍加速，同时生成质量几乎无损（如FID: 1.59, IS: 304.4 与原方法1.59, 299.1相当），且在多尺度与多任务测试中显著优于现有加速方法。

Conclusion: GtR策略有效提高了视觉生成模型的推理速度，而无需牺牲生成质量，显示出较强的通用性与实用价值，有望在视觉生成领域广泛应用。

Abstract: Masked Autoregressive (MAR) models promise better efficiency in visual
generation than autoregressive (AR) models for the ability of parallel
generation, yet their acceleration potential remains constrained by the
modeling complexity of spatially correlated visual tokens in a single step. To
address this limitation, we introduce Generation then Reconstruction (GtR), a
training-free hierarchical sampling strategy that decomposes generation into
two stages: structure generation establishing global semantic scaffolding,
followed by detail reconstruction efficiently completing remaining tokens.
Assuming that it is more difficult to create an image from scratch than to
complement images based on a basic image framework, GtR is designed to achieve
acceleration by computing the reconstruction stage quickly while maintaining
the generation quality by computing the generation stage slowly. Moreover,
observing that tokens on the details of an image often carry more semantic
information than tokens in the salient regions, we further propose
Frequency-Weighted Token Selection (FTS) to offer more computation budget to
tokens on image details, which are localized based on the energy of high
frequency information. Extensive experiments on ImageNet class-conditional and
text-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining
comparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1),
substantially outperforming existing acceleration methods across various model
scales and generation tasks. Our codes will be released in
https://github.com/feihongyan1/GtR.

</details>


### [120] [Benchmarking Out-of-Distribution Detection for Plankton Recognition: A Systematic Evaluation of Advanced Methods in Marine Ecological Monitoring](https://arxiv.org/abs/2510.17179)
*Yingzi Han,Jiakai He,Chuanlong Xie,Jianping Li*

Main category: cs.CV

TL;DR: 本文系统地评估了22种最新的OoD检测方法在浮游生物自动识别中的表现，提出并构建了实用的基准，并发现ViM方法效果最优。


<details>
  <summary>Details</summary>
Motivation: 在自动化浮游生物识别落地过程中，由于训练集和测试集分布偏移（OoD），模型面临准确率下降的难题。同时，浮游生物物种多样，形态复杂，新物种频现，使得现有OoD方法难以直接移植，也缺乏统一评价基准。该研究旨在填补这一领域空白。

Method: 作者基于DYB-PlanktonNet数据集设计了一系列模拟实际分布偏移情景的OoD基准，系统评测了22种主流OoD检测方法，并对各方法在不同偏移情境下的表现进行深入对比分析。

Result: 实验显示，尤其在大幅分布偏移（Far-OoD）场景下，ViM方法在关键指标上显著优于其他方法。所提出的基准和分析为实际筛选与应用OoD算法提供了数据支撑。

Conclusion: 该工作首次对浮游生物领域进行了大规模、系统的OoD方法评测，明确了不同方法优劣，推广了先进计算机视觉技术，为后续相关研究奠定了基础。

Abstract: Automated plankton recognition models face significant challenges during
real-world deployment due to distribution shifts (Out-of-Distribution, OoD)
between training and test data. This stems from plankton's complex
morphologies, vast species diversity, and the continuous discovery of novel
species, which leads to unpredictable errors during inference. Despite rapid
advancements in OoD detection methods in recent years, the field of plankton
recognition still lacks a systematic integration of the latest computer vision
developments and a unified benchmark for large-scale evaluation. To address
this, this paper meticulously designed a series of OoD benchmarks simulating
various distribution shift scenarios based on the DYB-PlanktonNet dataset
\cite{875n-f104-21}, and systematically evaluated twenty-two OoD detection
methods. Extensive experimental results demonstrate that the ViM
\cite{wang2022vim} method significantly outperforms other approaches in our
constructed benchmarks, particularly excelling in Far-OoD scenarios with
substantial improvements in key metrics. This comprehensive evaluation not only
provides a reliable reference for algorithm selection in automated plankton
recognition but also lays a solid foundation for future research in plankton
OoD detection. To our knowledge, this study marks the first large-scale,
systematic evaluation and analysis of Out-of-Distribution data detection
methods in plankton recognition. Code is available at
https://github.com/BlackJack0083/PlanktonOoD.

</details>


### [121] [Capturing Head Avatar with Hand Contacts from a Monocular Video](https://arxiv.org/abs/2510.17181)
*Haonan He,Yufeng Zheng,Jie Song*

Main category: cs.CV

TL;DR: 该论文提出了一种同时学习头部精细头像和手-脸交互变形的新方法，显著提升了3D头像与手部自然互动的真实感。


<details>
  <summary>Details</summary>
Motivation: 现有3D头部头像方法多忽略了手与脸的自然互动，如手托下巴、手指碰脸等，这些互动对于增强虚拟化身的真实感和表达用户状态很关键。因此，研究如何高效且真实地建模手-脸交互对提升头部头像质量具有重要意义。

Method: 作者提出了两大创新：（1）结合了深度顺序损失和接触正则化的姿态跟踪方式，有效捕捉手和脸之间的空间关系；（2）从手-脸交互数据集学习特定PCA变形基，显著简化形变参数预测，并引入基于物理模拟的接触损失，减少穿模现象并提升物理合理性。

Result: 在iPhone采集的RGB(D)视频和合成手-脸交互数据集上评测，所提方法比当前3D表面重建SOTA取得了更优的外观和变形几何效果。

Conclusion: 该方法不仅能更好地捕捉手-脸交互下的面部外观和几何变形，还可提升3D头像在远程交流、游戏和虚拟现实场景中的真实感和表现力。

Abstract: Photorealistic 3D head avatars are vital for telepresence, gaming, and VR.
However, most methods focus solely on facial regions, ignoring natural
hand-face interactions, such as a hand resting on the chin or fingers gently
touching the cheek, which convey cognitive states like pondering. In this work,
we present a novel framework that jointly learns detailed head avatars and the
non-rigid deformations induced by hand-face interactions.
  There are two principal challenges in this task. First, naively tracking hand
and face separately fails to capture their relative poses. To overcome this, we
propose to combine depth order loss with contact regularization during pose
tracking, ensuring correct spatial relationships between the face and hand.
Second, no publicly available priors exist for hand-induced deformations,
making them non-trivial to learn from monocular videos. To address this, we
learn a PCA basis specific to hand-induced facial deformations from a face-hand
interaction dataset. This reduces the problem to estimating a compact set of
PCA parameters rather than a full spatial deformation field. Furthermore,
inspired by physics-based simulation, we incorporate a contact loss that
provides additional supervision, significantly reducing interpenetration
artifacts and enhancing the physical plausibility of the results.
  We evaluate our approach on RGB(D) videos captured by an iPhone.
Additionally, to better evaluate the reconstructed geometry, we construct a
synthetic dataset of avatars with various types of hand interactions. We show
that our method can capture better appearance and more accurate deforming
geometry of the face than SOTA surface reconstruction methods.

</details>


### [122] [HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery](https://arxiv.org/abs/2510.17188)
*Vaibhav Rathore,Divyam Gupta,Biplab Banerjee*

Main category: cs.CV

TL;DR: 本文提出了HIDISC框架，无需访问目标域即可实现新类别发现与领域泛化，并在多个基准数据集上取得最优效果。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法一般假设训练时有标注和无标注数据的共同访问，且都来自同一域，但实际开放世界中常常有域分布偏移，现有方法泛化性差。此外，先前唯一的DG-GCD方法计算成本高，易积累误差。因此需要更高效、更泛化的新框架。

Method: 提出HIDISC，一种基于双曲空间表示学习的方法。训练中用GPT引导的扩散生成进行源域多样化，避免过拟合。提出Tangent CutMix，在切空间进行曲率感知的插值以生成伪新类别样本。联合损失函数包括Busemann对齐、混合对比损失和自适应离群点排斥。还引入可学习曲率参数，以适应不同数据集复杂度，无需模拟多域任务。

Result: HIDISC在PACS、Office-Home和DomainNet等标准DG-GCD基准上均取得了最新最优表现，优于既有欧几里得与双曲空间的DG-GCD方法。

Conclusion: HIDISC无需目标域信息即可实现类别与域的泛化，提升了开放世界下新类别发现的实用性和效率，在DG-GCD任务中明显优于现有方法。

Abstract: Generalized Category Discovery (GCD) aims to classify test-time samples into
either seen categories** -- available during training -- or novel ones, without
relying on label supervision. Most existing GCD methods assume simultaneous
access to labeled and unlabeled data during training and arising from the same
domain, limiting applicability in open-world scenarios involving distribution
shifts. Domain Generalization with GCD (DG-GCD) lifts this constraint by
requiring models to generalize to unseen domains containing novel categories,
without accessing targetdomain data during training. The only prior DG-GCD
method, DG2CD-Net, relies on episodic training with multiple synthetic domains
and task vector aggregation, incurring high computational cost and error
accumulation. We propose HIDISC, a hyperbolic representation learning framework
that achieves domain and category-level generalization without episodic
simulation. To expose the model to minimal but diverse domain variations, we
augment the source domain using GPT-guided diffusion, avoiding overfitting
while maintaining efficiency. To structure the representation space, we
introduce Tangent CutMix, a curvature-aware interpolation that synthesizes
pseudo-novel samples in tangent space, preserving manifold consistency. A
unified loss -- combining penalized Busemann alignment, hybrid hyperbolic
contrastive regularization, and adaptive outlier repulsion -- **facilitates
compact, semantically structured embeddings. A learnable curvature parameter
further adapts the geometry to dataset complexity. HIDISC achieves
state-of-the-art results on PACS , Office-Home , and DomainNet, consistently
outperforming the existing Euclidean and hyperbolic (DG)-GCD baselines.

</details>


### [123] [ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models](https://arxiv.org/abs/2510.17197)
*Pu Zhang,Yuwei Li,Xingyuan Xian,Guoming Tang*

Main category: cs.CV

TL;DR: 本文提出了一种面向提示（prompt-aware）的视觉令牌剪枝方法，高效减少视觉-语言模型推理成本，在保证性能的前提下实现高比例（最多90%）的token剪枝。


<details>
  <summary>Details</summary>
Motivation: 随着视觉-语言模型（VLM）的能力增强，其输入尺寸也变得更大，视觉token冗余导致推理成本高昂。现有减少token的方法大多忽视了文本提示，对于任务相关性的考虑不足。

Method: 提出一种零样本的分层剪枝方法：首先根据任务挑选一组核心且与提示相关的token，再补充具备信息多样性的token，从而在任务相关性与多样性之间取得平衡。

Result: 在多个模型与基准测试中，该方法即使剪枝90%的token，也能维持或超越当前最佳水平，只造成极小的准确率损失。显著降低了GPU显存占用和推理延迟。

Conclusion: 面向任务提示的token剪枝能够兼顾效率和性能，为大规模视觉-语言模型的实际部署提供了有效手段。

Abstract: As the capabilities of Vision-Language Models (VLMs) advance, they can
process increasingly large inputs, which, unlike in LLMs, generates significant
visual token redundancy and leads to prohibitive inference costs. While many
methods aim to reduce these costs by pruning visual tokens, existing
approaches, whether based on attention or diversity, typically neglect the
guidance of the text prompt and thus fail to prioritize task relevance. In this
work, we propose a novel, zero-shot method that reframes the problem by
introducing a prompt-aware perspective, explicitly modeling visual token
pruning as a balance between task relevance and information diversity. Our
hierarchical approach first selects a core set of task-relevant visual tokens
and then supplements them with diversity tokens to preserve broader context.
Experiments across multiple models and benchmarks show that our method achieves
performance that matches or surpasses the state-of-the-art with only minimal
accuracy loss, even when pruning up to 90\% of the tokens. Furthermore, these
gains are accompanied by significant reductions in GPU memory footprint and
inference latency.

</details>


### [124] [From Pixels to People: Satellite-Based Mapping and Quantification of Riverbank Erosion and Lost Villages in Bangladesh](https://arxiv.org/abs/2510.17198)
*M Saifuzzaman Rafat,Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Jungpil Shin*

Main category: cs.CV

TL;DR: 本论文提出一种基于深度学习的自动化河岸侵蚀追踪手段，利用SAM模型和新构建的数据集精确监测孟加拉国河流致灾区域的村庄和土地消失情况，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国大河每年因河岸侵蚀导致大量土地与村庄消失，成千上万家庭流离失所。长期以来，缺乏有效的自动化跟踪手段，对这种灾害进行持续量化监测极为困难。作者希望通过人工智能助力相关分析，提高监测效率和准确性。

Method: 作者构建了第一个针对孟加拉国已消失定居点的手动标注数据集（2003-2025年谷歌地球影像）。方法包括：1）利用简单的颜色通道分割，初步划分水陆区域；2）在此基础上对通用分割模型SAM的mask decoder部分进行微调，使其可以识别河岸侵蚀的微妙特征。最后，通过准确性评估对比传统方法。

Result: 微调后的SAM模型在河岸侵蚀识别任务上达到mean IoU 86.30%、Dice 92.60%的高分，显著高于传统方法及未优化的深度学习模型。

Conclusion: 提出的数据集、专项AI模型与定量分析方法，为监控孟加拉国河岸侵蚀、预测其趋势与支持政策制定提供了强有力工具，有望推动对受威胁社区的保护。

Abstract: The great rivers of Bangladesh, arteries of commerce and sustenance, are also
agents of relentless destruction. Each year, they swallow whole villages and
vast tracts of farmland, erasing communities from the map and displacing
thousands of families. To track this slow-motion catastrophe has, until now,
been a Herculean task for human analysts. Here we show how a powerful
general-purpose vision model, the Segment Anything Model (SAM), can be adapted
to this task with remarkable precision. To do this, we assembled a new dataset
- a digital chronicle of loss compiled from historical Google Earth imagery of
Bangladesh's most vulnerable regions, including Mokterer Char Union, Kedarpur
Union, Balchipara village, and Chowhali Upazila, from 2003 to 2025. Crucially,
this dataset is the first to include manually annotated data on the settlements
that have vanished beneath the water. Our method first uses a simple
color-channel analysis to provide a rough segmentation of land and water, and
then fine-tunes SAM's mask decoder to recognize the subtle signatures of
riverbank erosion. The resulting model demonstrates a keen eye for this
destructive process, achieving a mean Intersection over Union of 86.30% and a
Dice score of 92.60% - a performance that significantly surpasses traditional
methods and off-the-shelf deep learning models. This work delivers three key
contributions: the first annotated dataset of disappeared settlements in
Bangladesh due to river erosion; a specialized AI model fine-tuned for this
critical task; and a method for quantifying land loss with compelling visual
evidence. Together, these tools provide a powerful new lens through which
policymakers and disaster management agencies can monitor erosion, anticipate
its trajectory, and ultimately protect the vulnerable communities in its path.

</details>


### [125] [Round Outcome Prediction in VALORANT Using Tactical Features from Video Analysis](https://arxiv.org/abs/2510.17199)
*Nirai Hayakawa,Kazumasa Shimari,Kazuma Yamasaki,Hirotatsu Hoshikawa,Rikuto Tsuchida,Kenichi Matsumoto*

Main category: cs.CV

TL;DR: 本研究针对电竞游戏《无畏契约》（VALORANT），提出通过分析比赛录像中的小地图信息，预测回合胜负，结合视频识别模型和战术特征，大幅提升了预测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有电竞比赛结果预测多依赖于日志和统计数据，缺乏对复杂战术行为的分析。VALORANT作为一款策略性很强的FPS游戏，基于视频数据和战术特征的信息有望提高预测模型的表现。

Method: 以TimeSformer视频识别模型为基础，从比赛录像中的小地图提取角色位置及游戏事件等战术特征，将这些标签加入数据集，用于训练回合预测模型。

Result: 引入详细战术事件标签的数据集训练的模型，在比赛中后期能达到约81%的预测准确率，显著优于仅用小地图信息训练的模型。

Conclusion: 结合小地图战术特征和视觉识别技术能够有效提升VALORANT回合胜负预测的准确性。

Abstract: Recently, research on predicting match outcomes in esports has been actively
conducted, but much of it is based on match log data and statistical
information. This research targets the FPS game VALORANT, which requires
complex strategies, and aims to build a round outcome prediction model by
analyzing minimap information in match footage. Specifically, based on the
video recognition model TimeSformer, we attempt to improve prediction accuracy
by incorporating detailed tactical features extracted from minimap information,
such as character position information and other in-game events. This paper
reports preliminary results showing that a model trained on a dataset augmented
with such tactical event labels achieved approximately 81% prediction accuracy,
especially from the middle phases of a round onward, significantly
outperforming a model trained on a dataset with the minimap information itself.
This suggests that leveraging tactical features from match footage is highly
effective for predicting round outcomes in VALORANT.

</details>


### [126] [EndoCIL: A Class-Incremental Learning Framework for Endoscopic Image Classification](https://arxiv.org/abs/2510.17200)
*Bingrong Liu,Jun Shi,Yushan Zheng*

Main category: cs.CV

TL;DR: 本文提出了EndoCIL框架，有效提升了内镜图像分析中的持续学习表现，解决了灾难性遗忘和类别不平衡等关键问题，在多个公开数据集上优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 内镜影像诊断模型在真实临床环境下需要不断适应新的临床数据，同时保持对已学任务的性能。目前基于回放的类增量学习方法难以应对端域差异和类别不平衡，导致灌输性遗忘严重，因此亟需更有效的方法。

Method: 作者提出了EndoCIL，包括三个核心模块：1. 最大均值差异回放（MDBR），通过分布对齐贪婪策略选择具有代表性的样本；2. 先验正则化的类别均衡损失（PRCBL），结合类别先验和权重缓解新旧类别和每阶段内部的类别不平衡；3. 全连接梯度校准（CFG），调整分类器梯度减轻对新类别的偏置。

Result: 在四个公开内镜数据集上，EndoCIL在不同buffer设置和各项评估指标上均显著优于当前最前沿的类增量学习方法。

Conclusion: EndoCIL有效提升了内镜影像持续学习模型的稳定性与适应性，展示了其在实际临床中的可扩展性和应用前景。

Abstract: Class-incremental learning (CIL) for endoscopic image analysis is crucial for
real-world clinical applications, where diagnostic models should continuously
adapt to evolving clinical data while retaining performance on previously
learned ones. However, existing replay-based CIL methods fail to effectively
mitigate catastrophic forgetting due to severe domain discrepancies and class
imbalance inherent in endoscopic imaging. To tackle these challenges, we
propose EndoCIL, a novel and unified CIL framework specifically tailored for
endoscopic image diagnosis. EndoCIL incorporates three key components: Maximum
Mean Discrepancy Based Replay (MDBR), employing a distribution-aligned greedy
strategy to select diverse and representative exemplars, Prior Regularized
Class Balanced Loss (PRCBL), designed to alleviate both inter-phase and
intra-phase class imbalance by integrating prior class distributions and
balance weights into the loss function, and Calibration of Fully-Connected
Gradients (CFG), which adjusts the classifier gradients to mitigate bias toward
new classes. Extensive experiments conducted on four public endoscopic datasets
demonstrate that EndoCIL generally outperforms state-of-the-art CIL methods
across varying buffer sizes and evaluation metrics. The proposed framework
effectively balances stability and plasticity in lifelong endoscopic diagnosis,
showing promising potential for clinical scalability and deployment.

</details>


### [127] [Optimizing DINOv2 with Registers for Face Anti-Spoofing](https://arxiv.org/abs/2510.17201)
*Mika Feng,Pierre Gallin-Martel,Koichi Ito,Takafumi Aoki*

Main category: cs.CV

TL;DR: 本文提出了一种基于DINOv2的识别算法，有效区分真人和攻击者照片，以增强人脸识别系统的防伪能力。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统容易被冒用注册人照片骗过，存在安全隐患，因此需要检测攻击者的伪造行为（如展示照片等），在识别前拦截攻击。

Method: 提出利用DINOv2模型并结合register机制，通过抑制注意力扰动以集中模型关注重要细微特征，提升分辨真人与伪造人脸（如照片）图像的能力。

Result: 在ICCV2025“Face Anti-Spoofing”比赛和SiW数据集上的实验，验证了所提方法能有效区分真人与伪造人脸。

Conclusion: 基于DINOv2的伪造检测方法增强了传统人脸识别系统的安全性，是应对物理与数字攻击的有效手段。

Abstract: Face recognition systems are designed to be robust against variations in head
pose, illumination, and image blur during capture. However, malicious actors
can exploit these systems by presenting a face photo of a registered user,
potentially bypassing the authentication process. Such spoofing attacks must be
detected prior to face recognition. In this paper, we propose a DINOv2-based
spoofing attack detection method to discern minute differences between live and
spoofed face images. Specifically, we employ DINOv2 with registers to extract
generalizable features and to suppress perturbations in the attention
mechanism, which enables focused attention on essential and minute features. We
demonstrate the effectiveness of the proposed method through experiments
conducted on the dataset provided by ``The 6th Face Anti-Spoofing Workshop:
Unified Physical-Digital Attacks Detection@ICCV2025'' and SiW dataset.

</details>


### [128] [When One Moment Isn't Enough: Multi-Moment Retrieval with Cross-Moment Interactions](https://arxiv.org/abs/2510.17218)
*Zhuo Cao,Heming Du,Bingqing Zhang,Xin Yu,Xue Li,Sen Wang*

Main category: cs.CV

TL;DR: 本文针对现有视频瞬间检索（MR）方法无法处理一条查询对应多个相关片段的问题，提出了多瞬间检索（MMR）的数据集与方法。


<details>
  <summary>Details</summary>
Motivation: 以往MR任务仅关注单目标片段，忽略了一条查询可对应多个相关片段的真实需求，这使得现有数据集和方法不足以覆盖真实应用场景。

Method: 提出高质量多瞬间数据集QV-M$^2$，并设计新的多瞬间检索评测指标。同时，提出了FlashMMR方法，包含多瞬间后验证模块，通过约束的时间调整与候选片段再验证，有效过滤置信度低的候选片段，实现更加稳健的多瞬间定位。

Result: 在新数据集QV-M$^2$及原始QVHighlights评测6种主流MR方法，FlashMMR在G-mAP、mAP@3+tgt、mR@3等指标上超越SOTA方法，提升分别为3.00%、2.70%、2.56%。

Conclusion: 提出的数据集和方法为更加真实、更具挑战性的视频瞬间定位研究奠定了基础，也为后续相关研究提供了有力支持。

Abstract: Existing Moment retrieval (MR) methods focus on Single-Moment Retrieval
(SMR). However, one query can correspond to multiple relevant moments in
real-world applications. This makes the existing datasets and methods
insufficient for video temporal grounding. By revisiting the gap between
current MR tasks and real-world applications, we introduce a high-quality
datasets called QVHighlights Multi-Moment Dataset (QV-M$^2$), along with new
evaluation metrics tailored for multi-moment retrieval (MMR). QV-M$^2$ consists
of 2,212 annotations covering 6,384 video segments. Building on existing
efforts in MMR, we propose a framework called FlashMMR. Specifically, we
propose a Multi-moment Post-verification module to refine the moment
boundaries. We introduce constrained temporal adjustment and subsequently
leverage a verification module to re-evaluate the candidate segments. Through
this sophisticated filtering pipeline, low-confidence proposals are pruned, and
robust multi-moment alignment is achieved. We retrain and evaluate 6 existing
MR methods on QV-M$^2$ and QVHighlights under both SMR and MMR settings.
Results show that QV-M$^2$ serves as an effective benchmark for training and
evaluating MMR models, while FlashMMR provides a strong baseline. Specifically,
on QV-M$^2$, it achieves improvements over prior SOTA method by 3.00% on G-mAP,
2.70% on mAP@3+tgt, and 2.56% on mR@3. The proposed benchmark and method
establish a foundation for advancing research in more realistic and challenging
video temporal grounding scenarios. Code is released at
https://github.com/Zhuo-Cao/QV-M2.

</details>


### [129] [Fair and Interpretable Deepfake Detection in Videos](https://arxiv.org/abs/2510.17264)
*Akihito Yoshii,Ryosuke Sonoda,Ramya Srinivasan*

Main category: cs.CV

TL;DR: 本文提出了一种兼顾公平性与可解释性的深度伪造检测方法，有效缓解了现有方法的偏见及时序信息捕捉不足问题，在多个常用数据集上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法在面对不同人群时存在公平性问题，往往对少数群体表现不佳，且缺乏对视频时序特征的有效利用，导致检测准确性和可解释性不足。

Method: 提出一种公平性感知的深度伪造检测框架，融合时序特征学习和人口统计敏感的数据增强。其中，采用基于序列的聚类方法对伪造视频的时序建模，并引入概念抽取提升检测的可靠性和可解释性。同时，通过人口统计敏感的数据增强手段平衡样本分布，并使用频域变换保持伪造特征，有效减少偏见，提高泛化能力。

Result: 在FaceForensics++、DFD、Celeb-DF和DFDC等主流深度伪造检测数据集上，结合Xception和ResNet等SoTA架构，所提方法兼顾了公平性与精度，在权衡公平性和准确性方面优于现有方法。

Conclusion: 该方法不仅提升了不同人群间的检测公平性，也增强了模型可解释性和泛化能力，为深度伪造检测带来了更为公正且可靠的解决方案。

Abstract: Existing deepfake detection methods often exhibit bias, lack transparency,
and fail to capture temporal information, leading to biased decisions and
unreliable results across different demographic groups. In this paper, we
propose a fairness-aware deepfake detection framework that integrates temporal
feature learning and demographic-aware data augmentation to enhance fairness
and interpretability. Our method leverages sequence-based clustering for
temporal modeling of deepfake videos and concept extraction to improve
detection reliability while also facilitating interpretable decisions for
non-expert users. Additionally, we introduce a demography-aware data
augmentation method that balances underrepresented groups and applies
frequency-domain transformations to preserve deepfake artifacts, thereby
mitigating bias and improving generalization. Extensive experiments on
FaceForensics++, DFD, Celeb-DF, and DFDC datasets using state-of-the-art (SoTA)
architectures (Xception, ResNet) demonstrate the efficacy of the proposed
method in obtaining the best tradeoff between fairness and accuracy when
compared to SoTA.

</details>


### [130] [FineVision: Open Data Is All You Need](https://arxiv.org/abs/2510.17269)
*Luis Wiedmann,Orr Zohar,Amir Mahla,Xiaohan Wang,Rui Li,Thibaud Frere,Leandro von Werra,Aritra Roy Gosthipaty,Andrés Marafioti*

Main category: cs.CV

TL;DR: 本文提出并公开了FineVision，一个包含2400万样本的、统一且高质量的视觉-语言多模态数据集，并证明其大规模、干净和严格管理的数据能显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有公开视觉-语言数据集高度碎片化、数据结构不一致且存在污染（如基准测试集的泄漏），严重阻碍VLMs的发展，因此需要更大规模且高质量的数据资源来推动研究。

Method: 作者从200多个来源整合统一出185个子数据集，通过半自动+人工流程（自动批量采集、结构映射；人工审核和抽查）确保数据准确、规范、多样和安全。流程还包括跨源去重及基准集去污染。此外，涵盖了包含action space的GUI/agent任务，并经人工验证。

Result: 在FineVision上训练的模型，在多项评测中持续优于基于现有公开数据混合集训练的模型，证明了其数据规模和质量带来的性能提升。

Conclusion: FineVision作为最大规模且高质量的公开VLM数据资源，连同数据整合工具一起发布，有望极大推动面向数据的VLM研究与开发，建议未来研究采用高质量、统一化的数据集。

Abstract: The advancement of vision-language models (VLMs) is hampered by a fragmented
landscape of inconsistent and contaminated public datasets. We introduce
FineVision, a meticulously collected, curated, and unified corpus of 24 million
samples - the largest open resource of its kind. We unify more than 200 sources
into 185 subsets via a semi-automated, human-in-the-loop pipeline: automation
performs bulk ingestion and schema mapping, while reviewers audit mappings and
spot-check outputs to verify faithful consumption of annotations, appropriate
formatting and diversity, and safety; issues trigger targeted fixes and
re-runs. The workflow further applies rigorous de-duplication within and across
sources and decontamination against 66 public benchmarks. FineVision also
encompasses agentic/GUI tasks with a unified action space; reviewers validate
schemas and inspect a sample of trajectories to confirm executable fidelity.
Models trained on FineVision consistently outperform those trained on existing
open mixtures across a broad evaluation suite, underscoring the benefits of
scale, data hygiene, and balanced automation with human oversight. We release
the corpus and curation tools to accelerate data-centric VLM research.

</details>


### [131] [Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models](https://arxiv.org/abs/2510.17274)
*Katie Luo,Jingwei Ji,Tong He,Runsheng Xu,Yichen Xie,Dragomir Anguelov,Mingxing Tan*

Main category: cs.CV

TL;DR: 提出了一种可以无缝集成到现有自动驾驶运动预测系统中的新方法，利用多模态大语言模型增强模型对复杂场景的理解和预测能力，并在公开数据集上有效提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统的感知与运动预测模型在标准环境下表现可靠，但难以低成本地适应具有多样性和复杂性的真实世界场景。如何利用更高层次的信息来提升通用性成为亟待解决的问题。

Method: 提出Plug-and-Forecast (PnF)方法，通过设计提示词（prompt）让多模态大语言模型（MLLMs）对场景进行结构化理解，再将其知识以可学习的向量形式输入现有运动预测模型，无需微调即可实现增强。

Result: 在Waymo Open Motion Dataset和nuScenes Dataset上，将PnF集成到两种主流运动预测模型中，均取得了显著且一致的性能提升。

Conclusion: PnF方法能有效增强自动驾驶运动预测模型对复杂现实场景的适应能力，且易于部署，具有良好的实际应用前景。

Abstract: Current autonomous driving systems rely on specialized models for perceiving
and predicting motion, which demonstrate reliable performance in standard
conditions. However, generalizing cost-effectively to diverse real-world
scenarios remains a significant challenge. To address this, we propose
Plug-and-Forecast (PnF), a plug-and-play approach that augments existing motion
forecasting models with multimodal large language models (MLLMs). PnF builds on
the insight that natural language provides a more effective way to describe and
handle complex scenarios, enabling quick adaptation to targeted behaviors. We
design prompts to extract structured scene understanding from MLLMs and distill
this information into learnable embeddings to augment existing behavior
prediction models. Our method leverages the zero-shot reasoning capabilities of
MLLMs to achieve significant improvements in motion prediction performance,
while requiring no fine-tuning -- making it practical to adopt. We validate our
approach on two state-of-the-art motion forecasting models using the Waymo Open
Motion Dataset and the nuScenes Dataset, demonstrating consistent performance
improvements across both benchmarks.

</details>


### [132] [SG-CLDFF: A Novel Framework for Automated White Blood Cell Classification and Segmentation](https://arxiv.org/abs/2510.17278)
*Mehdi Zekriyapanah Gashti,Mostafa Mohammadpour,Ghasem Farjamnia*

Main category: cs.CV

TL;DR: 提出了一种新颖的白细胞分割与分类框架SG-CLDFF，结合了显著性引导预处理和跨层特征融合，显著提升了自动化白细胞分析的准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的白细胞显微镜图像分割与分类受染色差异、复杂背景、类别不均衡等影响，准确性和鲁棒性不足，亟需更高效、可解释的自动化分析方法。

Method: SG-CLDFF框架以显著性先验突出白细胞区域，利用轻量级混合主干网络提取多尺度特征，通过ResNeXt-CC风格的跨层融合模块整合浅层与深层信息。此外，采用多任务训练（分割+分类）、类别加权损失与显著性一致性正则化，并通过Grad-CAM和显著性对齐提升模型可解释性。

Result: 在BCCD、LISC、ALL-IDB等公开数据集上，IoU、F1值、分类准确率均优于主流CNN和Transformer基线。消融实验显示显著性处理和跨层融合对提升性能有显著贡献。

Conclusion: SG-CLDFF框架为临床白细胞自动分析提供了更准确、可解释、实用的解决方案，有助于提升相关临床工作流程的智能化水平。

Abstract: Accurate segmentation and classification of white blood cells (WBCs) in
microscopic images are essential for diagnosis and monitoring of many
hematological disorders, yet remain challenging due to staining variability,
complex backgrounds, and class imbalance. In this paper, we introduce a novel
Saliency-Guided Cross-Layer Deep Feature Fusion framework (SG-CLDFF) that
tightly integrates saliency-driven preprocessing with multi-scale deep feature
aggregation to improve both robustness and interpretability for WBC analysis.
SG-CLDFF first computes saliency priors to highlight candidate WBC regions and
guide subsequent feature extraction. A lightweight hybrid backbone
(EfficientSwin-style) produces multi-resolution representations, which are
fused by a ResNeXt-CC-inspired cross-layer fusion module to preserve
complementary information from shallow and deep layers. The network is trained
in a multi-task setup with concurrent segmentation and cell-type classification
heads, using class-aware weighted losses and saliency-alignment regularization
to mitigate imbalance and suppress background activation. Interpretability is
enforced through Grad-CAM visualizations and saliency consistency checks,
allowing model decisions to be inspected at the regional level. We validate the
framework on standard public benchmarks (BCCD, LISC, ALL-IDB), reporting
consistent gains in IoU, F1, and classification accuracy compared to strong CNN
and transformer baselines. An ablation study also demonstrates the individual
contributions of saliency preprocessing and cross-layer fusion. SG-CLDFF offers
a practical and explainable path toward more reliable automated WBC analysis in
clinical workflows.

</details>


### [133] [Machine Vision-Based Surgical Lighting System:Design and Implementation](https://arxiv.org/abs/2510.17287)
*Amir Gharghabi,Mahdi Hakiminezhad,Maryam Shafaei,Shaghayegh Gharghabi*

Main category: cs.CV

TL;DR: 本文提出了一种结合YOLOv11目标检测算法和伺服电机的自动化手术照明系统，通过识别手术区域上的蓝色标记，自动调整高功率LED光源的位置，提高手术照明的精度与一致性，减轻外科医生的疲劳。


<details>
  <summary>Details</summary>
Motivation: 现有手术照明系统依赖手动调节，容易导致医生疲劳、颈部劳损及照明不一致（如照明偏移、阴影遮挡），影响手术精度和安全性。

Method: 采用YOLOv11目标检测算法自动识别手术区域上方的蓝色球形标记，通过连接倾斜-俯仰支架的两台伺服电机将高功率LED光源精准地指向目标位置。系统在模拟手术场景的验证集上取得了96.7%的mAP@50检测性能。

Result: 该系统能在高精度识别下自动调节照明方向，显著降低了对医生的物理需求，保证了手术照明的持续一致性。

Conclusion: 基于机器视觉的自动化手术照明系统能够有效减少医生的身体负担，提高照明质量，并有望推动手术效果的进一步提升。

Abstract: Effortless and ergonomically designed surgical lighting is critical for
precision and safety during procedures. However, traditional systems often rely
on manual adjustments, leading to surgeon fatigue, neck strain, and
inconsistent illumination due to drift and shadowing. To address these
challenges, we propose a novel surgical lighting system that leverages the
YOLOv11 object detection algorithm to identify a blue marker placed above the
target surgical site. A high-power LED light source is then directed to the
identified location using two servomotors equipped with tilt-pan brackets. The
YOLO model achieves 96.7% mAP@50 on the validation set consisting of annotated
images simulating surgical scenes with the blue spherical marker. By automating
the lighting process, this machine vision-based solution reduces physical
strain on surgeons, improves consistency in illumination, and supports improved
surgical outcomes.

</details>


### [134] [Exploring Structural Degradation in Dense Representations for Self-supervised Learning](https://arxiv.org/abs/2510.17299)
*Siran Dai,Qianqian Xu,Peisong Wen,Yang Liu,Qingming Huang*

Main category: cs.CV

TL;DR: 本文发现自监督学习（SSL）训练时间过长会导致模型在下游密集预测任务（如语义分割）上的性能下降（称为自监督密集退化，SDD）。作者提出新的性能度量与正则方法，并实验证明其能提升模型选择和性能。


<details>
  <summary>Details</summary>
Motivation: 过去SSL技术在处理密集预测任务时，长期训练不一定带来更好效果，但其机制和解决方案不明确。解决如何在不依赖标签的情况下测量和改善这个问题，是推动SSL应用的重要挑战。

Method: 作者提出密集表示结构评估器（DSE），包括类别相关性和有效维度测量，并基于此开发了模型选择策略和正则化方法。所有方法在多种SSL主流方法和数据集上进行系统实证测试。

Result: 模型选择策略平均提升mIoU 3%，DSE正则化在所有实验中均有效缓解退化；附带开源代码。

Conclusion: 自监督学习存在密集退化问题，DSE度量和调优方法能低成本有效提升密集预测任务性能，对SSL下游应用具有重要指导意义。

Abstract: In this work, we observe a counterintuitive phenomenon in self-supervised
learning (SSL): longer training may impair the performance of dense prediction
tasks (e.g., semantic segmentation). We refer to this phenomenon as
Self-supervised Dense Degradation (SDD) and demonstrate its consistent presence
across sixteen state-of-the-art SSL methods with various losses, architectures,
and datasets. When the model performs suboptimally on dense tasks at the end of
training, measuring the performance during training becomes essential. However,
evaluating dense performance effectively without annotations remains an open
challenge. To tackle this issue, we introduce a Dense representation Structure
Estimator (DSE), composed of a class-relevance measure and an effective
dimensionality measure. The proposed DSE is both theoretically grounded and
empirically validated to be closely correlated with the downstream performance.
Based on this metric, we introduce a straightforward yet effective model
selection strategy and a DSE-based regularization method. Experiments on
sixteen SSL methods across four benchmarks confirm that model selection
improves mIoU by $3.0\%$ on average with negligible computational cost.
Additionally, DSE regularization consistently mitigates the effects of dense
degradation. Code is available at
https://github.com/EldercatSAM/SSL-Degradation.

</details>


### [135] [LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding](https://arxiv.org/abs/2510.17305)
*ZhaoYang Han,Qihan Lin,Hao Liang,Bowen Chen,Zhou Liu,Wentao Zhang*

Main category: cs.CV

TL;DR: 本文提出了LongInsightBench，这是首个专为评估模型对长时长、多模态（视觉、音频、文本）视频理解能力而设计的基准数据集。该基准侧重于语言、观点、行为和上下文信息。实验显示，即使是最先进的多模态模型（OLMs）在特定任务上仍有不足。


<details>
  <summary>Details</summary>
Motivation: 现有多模态视频理解基准多针对短视频、片段，缺乏对长时间、高信息密度视频的评测，不能真实考验模型对现实复杂场景中多模态信息的深度理解能力。

Method: 1. 精选约1000个来自FineVideo的开放数据集长视频，涵盖讲座、访谈、Vlog等高语言信息密度内容；2. 设计6类具代表性的问题任务，分为事件内(单一事件)与事件间（多个事件）两类；3. 构建半自动三步数据质控流程，确保问题和选项的难度与有效性；4. 基于此基准设计系统性实验，评测主流多模态模型。

Result: 研究发现，当前Omni-modal models（OLMs）在涉及精确时序定位（T-Loc）和长程因果推理（CE-Caus）的任务上表现有限。进一步实验揭示了多模态信息融合过程中存在的信息损失和处理偏差。

Conclusion: LongInsightBench极大地推动了长视频多模态理解领域，为模型评测、改进和新算法发展提供了坚实基础。

Abstract: We introduce \textbf{LongInsightBench}, the first benchmark designed to
assess models' ability to understand long videos, with a focus on human
language, viewpoints, actions, and other contextual elements, while integrating
\textbf{visual, audio, and text} modalities. Our benchmark excels in three key
areas: \textbf{a) Long-Duration, Information-Dense Videos:} We carefully select
approximately 1,000 videos from open-source datasets FineVideo based on
duration limit and the information density of both visual and audio modalities,
focusing on content like lectures, interviews, and vlogs, which contain rich
language elements. \textbf{b) Diverse and Challenging Task Scenarios:} We have
designed six challenging task scenarios, including both Intra-Event and
Inter-Event Tasks. \textbf{c) Rigorous and Comprehensive Quality Assurance
Pipelines:} We have developed a three-step, semi-automated data quality
assurance pipeline to ensure the difficulty and validity of the synthesized
questions and answer options. Based on LongInsightBench, we designed a series
of experiments. Experimental results shows that Omni-modal models(OLMs) still
face challenge in tasks requiring precise temporal localization (T-Loc) and
long-range causal inference (CE-Caus). Extended experiments reveal the
information loss and processing bias in multi-modal fusion of OLMs. Our dataset
and code is available at
https://anonymous.4open.science/r/LongInsightBench-910F/.

</details>


### [136] [CausalMamba: Scalable Conditional State Space Models for Neural Causal Inference](https://arxiv.org/abs/2510.17318)
*Sangyoon Bae,Jiook Cha*

Main category: cs.CV

TL;DR: CausalMamba是一种可扩展的框架，用于提升fMRI因果推断，显著优于现有方法，并能识别灵活的脑区网络动态。


<details>
  <summary>Details</summary>
Motivation: 现有的fMRI因果推断方法（如DCM）在神经因果关系推断方面存在软体失真和计算复杂性高的问题，难以准确反映大脑真实因果结构。

Method: 作者提出将BOLD信号反卷积以恢复潜在神经活动，然后利用一种创新的Conditional Mamba结构进行因果图推断，分解逆问题为两个可解决阶段。

Result: 在模拟数据上，CausalMamba比DCM准确率高37%；应用于真实任务fMRI数据时，能以88%的准确率恢复已知脑通路，而传统方法在99%以上受试者中无法识别这些回路。此外，CausalMamba还揭示了工作记忆任务中脑因果枢纽的策略性重构。

Conclusion: CausalMamba为神经科学家提供了可扩展的大规模因果推断工具，能捕捉基本的神经环路与灵活的网络动态，有助于深入理解认知功能的底层机制。

Abstract: We introduce CausalMamba, a scalable framework that addresses fundamental
limitations in fMRI-based causal inference: the ill-posed nature of inferring
neural causality from hemodynamically distorted BOLD signals and the
computational intractability of existing methods like Dynamic Causal Modeling
(DCM). Our approach decomposes this complex inverse problem into two tractable
stages: BOLD deconvolution to recover latent neural activity, followed by
causal graph inference using a novel Conditional Mamba architecture. On
simulated data, CausalMamba achieves 37% higher accuracy than DCM. Critically,
when applied to real task fMRI data, our method recovers well-established
neural pathways with 88% fidelity, whereas conventional approaches fail to
identify these canonical circuits in over 99% of subjects. Furthermore, our
network analysis of working memory data reveals that the brain strategically
shifts its primary causal hub-recruiting executive or salience networks
depending on the stimulus-a sophisticated reconfiguration that remains
undetected by traditional methods. This work provides neuroscientists with a
practical tool for large-scale causal inference that captures both fundamental
circuit motifs and flexible network dynamics underlying cognitive function.

</details>


### [137] [A Single Set of Adversarial Clothes Breaks Multiple Defense Methods in the Physical World](https://arxiv.org/abs/2510.17322)
*Wei Zhang,Zhanhao Hu,Xiao Li,Xiaopei Zhu,Xiaolin Hu*

Main category: cs.CV

TL;DR: 本文系统性评估了当前主流对抗补丁防御方法在大面积物理世界攻击（如对抗衣物）下的有效性，发现现有防御手段普遍失效。


<details>
  <summary>Details</summary>
Motivation: 近年来深度学习目标检测模型面临物理世界中的对抗攻击威胁，例如通过贴附对抗补丁欺骗检测器。许多防御机制针对小面积对抗补丁提出，但尚不清楚这些对大面积且自然的对抗对象（如衣物）是否有效。基于实验中发现仅增大补丁面积即可破解现有效防御，作者提出深入评估大尺寸、自然形态的补丁防御需求。

Method: 作者设计并制作了大面积对抗衣物，系统性测试了多种主流基于补丁的防御方法在数字世界和物理世界中的抗攻击能力，并测量攻击成功率。此外，实验针对Faster R-CNN平台，制作了能突破多种防御体系的单套衣物，并统计了突破率。

Result: 所有被测防御在对抗衣物面前均表现不佳。在未加防御的检测器上，对抗衣物攻击成功率高达96.06%；即使面对九种防御模型，物理攻击成功率依然超过64.84%。

Conclusion: 大面积、自然伪装的对抗衣物能有效突破现有所有主流对抗补丁防御方法，表明基于补丁的现有防御手段存在普遍弱点，亟需研发更鲁棒的防御机制。

Abstract: In recent years, adversarial attacks against deep learning-based object
detectors in the physical world have attracted much attention. To defend
against these attacks, researchers have proposed various defense methods
against adversarial patches, a typical form of physically-realizable attack.
However, our experiments showed that simply enlarging the patch size could make
these defense methods fail. Motivated by this, we evaluated various defense
methods against adversarial clothes which have large coverage over the human
body. Adversarial clothes provide a good test case for adversarial defense
against patch-based attacks because they not only have large sizes but also
look more natural than a large patch on humans. Experiments show that all the
defense methods had poor performance against adversarial clothes in both the
digital world and the physical world. In addition, we crafted a single set of
clothes that broke multiple defense methods on Faster R-CNN. The set achieved
an Attack Success Rate (ASR) of 96.06% against the undefended detector and over
64.84% ASRs against nine defended models in the physical world, unveiling the
common vulnerability of existing adversarial defense methods against
adversarial clothes. Code is available at:
https://github.com/weiz0823/adv-clothes-break-multiple-defenses.

</details>


### [138] [CharDiff: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration](https://arxiv.org/abs/2510.17330)
*Gyuhwan Park,Kihyun Na,Injung Kim*

Main category: cs.CV

TL;DR: 提出了CharDiff方法，实现了高效的车牌图像修复和字符识别，显著提升修复质量与识别准确率。


<details>
  <summary>Details</summary>
Motivation: 车牌图像通常存在严重退化，如低清晰度或噪声，这影响了车牌识别系统的性能和实际应用价值，因此需要一种能够有效恢复和增强车牌图像的方法。

Method: 作者提出了一个基于扩散模型和字符级引导的框架CharDiff。该方法利用外部分割和OCR模块，针对低质量车牌图像提取字符级先验信息。通过创新的CHARM模块，实现区域掩码下的字符引导注意力，确保每个字符的指导仅局限于自身区域，并减少互相干扰。

Result: 实验证明，CharDiff在恢复质量与识别准确率方面显著超过了现有的修复基线方法。在Roboflow-LP数据集上，字符错误率（CER）相对于最佳基线降低了28%。

Conclusion: 结构化的字符级引导极大提升了基于扩散模型的车牌修复和识别的稳健性，具备较强的实际应用价值。

Abstract: The significance of license plate image restoration goes beyond the
preprocessing stage of License Plate Recognition (LPR) systems, as it also
serves various purposes, including increasing evidential value, enhancing the
clarity of visual interface, and facilitating further utilization of license
plate images. We propose a novel diffusion-based framework with character-level
guidance, CharDiff, which effectively restores and recognizes severely degraded
license plate images captured under realistic conditions. CharDiff leverages
fine-grained character-level priors extracted through external segmentation and
Optical Character Recognition (OCR) modules tailored for low-quality license
plate images. For precise and focused guidance, CharDiff incorporates a novel
Character-guided Attention through Region-wise Masking (CHARM) module, which
ensures that each character's guidance is restricted to its own region, thereby
avoiding interference with other regions. In experiments, CharDiff
significantly outperformed the baseline restoration models in both restoration
quality and recognition accuracy, achieving a 28% relative reduction in CER on
the Roboflow-LP dataset, compared to the best-performing baseline model. These
results indicate that the structured character-guided conditioning effectively
enhances the robustness of diffusion-based license plate restoration and
recognition in practical deployment scenarios.

</details>


### [139] [iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA](https://arxiv.org/abs/2510.17332)
*Zhaoran Zhao,Xinli Yue,Jianhui Sun,Yuhao Xie,Tao Shao,Liangchao Yao,Fan Xia,Yuetang Deng*

Main category: cs.CV

TL;DR: 本文提出了一种名为iDETEX的多模态大语言模型，能够同时执行图像质量定位、感知和描述三项任务，并在多个主流基准上取得了最佳表现。


<details>
  <summary>Details</summary>
Motivation: 现有图像质量评估（IQA）方法难以兼顾结果的准确性和可解释性，更难实现与人类感知一致的细致质量解释。为了解决这一挑战，作者提出结合多模态和大语言模型的方法。

Method: iDETEX模型融合了多模态信息，支持质量定位、感知以及图像质量描述三项任务。为提升模型的训练效率和泛化能力，作者设计了针对各类子任务的数据增强模块、数据混合策略，以及在线增强手段，实现多源监督的深度利用。

Result: 在大规模ViDA-UGC基准和ICCV MIPI 2025详细图像质量评估挑战中，iDETEX在所有子任务上都取得了新的SOTA，排名第一，展现了强大的有效性和稳健性。

Conclusion: iDETEX模型不仅能给出准确的质量评估分数，还能提供具有人类解释性的详细质量描述，有助于推动IQA领域向更解释性和人类一致性方向发展。

Abstract: Image Quality Assessment (IQA) has progressed from scalar quality prediction
to more interpretable, human-aligned evaluation paradigms. In this work, we
address the emerging challenge of detailed and explainable IQA by proposing
iDETEX-a unified multimodal large language model (MLLM) capable of
simultaneously performing three key tasks: quality grounding, perception, and
description. To facilitate efficient and generalizable training across these
heterogeneous subtasks, we design a suite of task-specific offline augmentation
modules and a data mixing strategy. These are further complemented by online
enhancement strategies to fully exploit multi-sourced supervision. We validate
our approach on the large-scale ViDA-UGC benchmark, where iDETEX achieves
state-of-the-art performance across all subtasks. Our model ranks first in the
ICCV MIPI 2025 Detailed Image Quality Assessment Challenge, demonstrating its
effectiveness and robustness in delivering accurate and interpretable quality
assessments.

</details>


### [140] [Nearest-Class Mean and Logits Agreement for Wildlife Open-Set Recognition](https://arxiv.org/abs/2510.17338)
*Jiahao Huo,Mufhumudzi Muthivhi,Terence L. van Zyl,Fredrik Gustafsson*

Main category: cs.CV

TL;DR: 该论文提出了一种无需重新训练分类模型的野生动物开放集识别（OSR）新方法，通过后处理方式提升模型对未知类别的识别与拒绝能力，并在两个数据集上取得了领先的表现。


<details>
  <summary>Details</summary>
Motivation: 当前野生动物分类模型多数仅适用于封闭世界场景，对未知类别容易过度自信，缺乏处理新类别样本的能力。现有OSR方法通常需要重新训练模型，应用受限，实际部署成本高。因此，开发无需重新训练的高效OSR方法具有重要意义。

Method: 作者提出一种基于后处理的OSR方法，通过测量模型特征与预测logit之间的一致性来实现未知类别的拒绝。具体做法为：以输入样本到其最近类别均值（Nearest Class Mean, NCM）的距离为基础，构建概率分布，并与logit空间的softmax概率分布进行比较，一致性低则判断为未知类别。

Result: 该方法在两个野生动物数据集（非洲动物和瑞典动物）上测试，AUROC分别达到93.41和95.35，表现位列评测的前三，并显示出跨数据集的稳定性，而最新其他方法往往只在单一数据集表现突出。

Conclusion: 作者提出的基于NCM与softmax一致性判别的后处理OSR方法，在保证无需重新训练模型的前提下，有效提升了开放集识别能力，并展现了良好的跨领域泛化性。

Abstract: Current state-of-the-art Wildlife classification models are trained under the
closed world setting. When exposed to unknown classes, they remain
overconfident in their predictions. Open-set Recognition (OSR) aims to classify
known classes while rejecting unknown samples. Several OSR methods have been
proposed to model the closed-set distribution by observing the feature, logit,
or softmax probability space. A significant drawback of many existing
approaches is the requirement to retrain the pre-trained classification model
with the OSR-specific strategy. This study contributes a post-processing OSR
method that measures the agreement between the models' features and predicted
logits. We propose a probability distribution based on an input's distance to
its Nearest Class Mean (NCM). The NCM-based distribution is then compared with
the softmax probabilities from the logit space to measure agreement between the
NCM and the classification head. Our proposed strategy ranks within the top
three on two evaluated datasets, showing consistent performance across the two
datasets. In contrast, current state-of-the-art methods excel on a single
dataset. We achieve an AUROC of 93.41 and 95.35 for African and Swedish
animals. The code can be found
https://github.com/Applied-Representation-Learning-Lab/OSR.

</details>


### [141] [Exploring The Missing Semantics In Event Modality](https://arxiv.org/abs/2510.17347)
*Jingqian Wu,Shengpeng Xu,Yunbo Jia,Edmund Y. Lam*

Main category: cs.CV

TL;DR: 本文提出了一种新的事件到视频重建（E2V）方法，利用视觉基础模型（SAM）中的丰富语义信息提升事件相机生成视频的语义质量，在多个基准测试上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 事件相机只捕捉光强变化，缺乏对静态物体和背景的记录，因此重建的视频缺乏语义信息。然而，语义信息对于视频和帧重建极为重要，但目前大多数E2V方法未能有效利用或恢复这部分内容。本文旨在通过引入语义知识，提升事件到视频的重建质量。

Method: 作者提出Semantic-E2VID方法，包括：（1）跨模态特征对齐（CFA）模块，将基于帧的视觉模型（如SAM）中强大的语义特征迁移到事件编码器，并对齐不同模态的高级特征；（2）语义感知特征融合（SFF）模块，将丰富的帧语义特征整合进事件表征中；（3）设计了新的语义感知E2V感知监督，利用SAM生成的类别标签，引导模型重建更详细的语义信息。

Result: 在多个标准数据集上进行了大量实验，结果显示Semantic-E2VID方法显著提升了重建视频的帧质量，在各项指标和视觉效果上均超越了当前最优的E2V方法。

Conclusion: 本文创新性地将视觉基础模型的语义能力迁移到事件视觉领域，极大提升了事件到视频重建的语义表现和总体质量。所提出的方法具有较广泛的适用性和推广价值，并为后续事件视觉相关研究提供了新思路。

Abstract: Event cameras offer distinct advantages such as low latency, high dynamic
range, and efficient motion capture. However, event-to-video reconstruction
(E2V), a fundamental event-based vision task, remains challenging, particularly
for reconstructing and recovering semantic information. This is primarily due
to the nature of the event camera, as it only captures intensity changes,
ignoring static objects and backgrounds, resulting in a lack of semantic
information in captured event modality. Further, semantic information plays a
crucial role in video and frame reconstruction, yet is often overlooked by
existing E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2V
framework that explores the missing visual semantic knowledge in event modality
and leverages it to enhance event-to-video reconstruction. Specifically,
Semantic-E2VID introduces a cross-modal feature alignment (CFA) module to
transfer the robust visual semantics from a frame-based vision foundation
model, the Segment Anything Model (SAM), to the event encoder, while aligning
the high-level features from distinct modalities. To better utilize the learned
semantic feature, we further propose a semantic-aware feature fusion (SFF)
block to integrate learned semantics in frame modality to form event
representations with rich semantics that can be decoded by the event decoder.
Further, to facilitate the reconstruction of semantic information, we propose a
novel Semantic Perceptual E2V Supervision that helps the model to reconstruct
semantic details by leveraging SAM-generated categorical labels. Extensive
experiments demonstrate that Semantic-E2VID significantly enhances frame
quality, outperforming state-of-the-art E2V methods across multiple benchmarks.
The sample code is included in the supplementary material.

</details>


### [142] [Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs](https://arxiv.org/abs/2510.17364)
*Vaggelis Dorovatas,Soroush Seifi,Gunshi Gupta,Rahaf Aljundi*

Main category: cs.CV

TL;DR: 本文提出了一种无需重新训练的新方法，使得现有的视频大语言模型（Video-LLMs）可以高效应对长时长视频的流式解读任务，通过大幅度筛减无关视觉信息并保持性能，实现高效的流媒体问答。


<details>
  <summary>Details</summary>
Motivation: 传统的Video-LLMs虽然在完整获取视频时表现优异，但难以实时、在线处理时长为数小时的流媒体视频并及时应答，这限制了其在实际实时场景下的应用。

Method: 1）利用大语言模型的注意力机制，选取对模型理解短视频片段贡献最大的视觉token，舍弃高达95%不重要的token，并保持性能；2）循环处理已选取的token，提升视频序列的时序一致性理解能力；3）采用基于描述生成的问答方式，实现轻量高效的问题解答。

Result: 该方法在流视频基准上取得了领先性能，能够在极大提升效率的同时，保证理解和问答的准确性，验证了其在实际流媒体情境中的有效性。

Conclusion: 本文提出的方法能让主流Video-LLMs无痛适配长视频流媒体场景，既高效又有较强的表达能力，有望显著扩展Video-LLMs的实际应用范围。

Abstract: Video Large Language Models (Video-LLMs) excel at understanding videos
in-context, provided they have full access to the video when answering queries.
However, these models face challenges in streaming scenarios where hour-long
videos must be processed online, and questions need timely responses. In this
work, we propose a training-free approach compatible with standard Video-LLMs,
leveraging three key concepts: 1) LLM-informed selection of visual tokens to
identify those that the LLM has attended to and contributed to its
understanding of each short clip. Our attention-based selection allows us to
discard up to ~95% of unimportant visual tokens with minimal performance loss;
2) Recurrent processing of past selected tokens to generate temporally coherent
understanding of each processed clip; 3) Caption-based question answering for
lightweight and accurate responses. Our method achieves state-of-the-art
performance on streaming video benchmarks, striking a balance between
efficiency and effectiveness.

</details>


### [143] [Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition Performance without Privacy Compromise](https://arxiv.org/abs/2510.17372)
*Paweł Borsukiewicz,Fadi Boutros,Iyiola E. Olatunji,Charles Beumier,Wendkûuni C. Ouedraogo,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CV

TL;DR: 本文系统评估了合成面部数据集在隐私保护面部识别任务中的实际效果，发现其识别准确率可与真实数据媲美，甚至更优，并具有更好的隐私和偏见控制能力。


<details>
  <summary>Details</summary>
Motivation: 传统高效的人脸识别系统通常需要大量未获同意的真实人脸数据，这引发了法律与伦理问题（如GDPR风险）。合成面部数据虽为可能的隐私替代，但缺乏系统性实证验证。因此亟需明确合成数据集在识别任务中的有效性与道德可行性。

Method: 作者首先系统性综述了25个自2018年至2025年出现的合成面部识别数据集，并围绕七大隐私要求（防身份泄露、类内变化、类间分隔、规模、伦理来源、偏见缓解、基准可靠性）进行实验。共评测一千多万合成样本，并与五个标准基准测试结果比较。

Result: 最佳的合成数据集（VariFace, VIGFace）在人脸识别准确率上超过主流真实数据（最高达95.67%），公开的合成集（Vec2Face, CemiFace）也接近最佳真实数据。同时，分析显示合成数据在类内变化与身份分隔上表现良好，且对不同人口群体的偏见更易受控与调节。

Conclusion: 合成面部识别数据在准确率和偏见控制方面表现出色，已成为科学上可行且道德上的必要替代方案，为面部识别研究和实际应用提供了新视角。

Abstract: The deployment of facial recognition systems has created an ethical dilemma:
achieving high accuracy requires massive datasets of real faces collected
without consent, leading to dataset retractions and potential legal liabilities
under regulations like GDPR. While synthetic facial data presents a promising
privacy-preserving alternative, the field lacks comprehensive empirical
evidence of its viability. This study addresses this critical gap through
extensive evaluation of synthetic facial recognition datasets. We present a
systematic literature review identifying 25 synthetic facial recognition
datasets (2018-2025), combined with rigorous experimental validation. Our
methodology examines seven key requirements for privacy-preserving synthetic
data: identity leakage prevention, intra-class variability, identity
separability, dataset scale, ethical data sourcing, bias mitigation, and
benchmark reliability. Through experiments involving over 10 million synthetic
samples, extended by a comparison of results reported on five standard
benchmarks, we provide the first comprehensive empirical assessment of
synthetic data's capability to replace real datasets. Best-performing synthetic
datasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and
94.91% respectively, surpassing established real datasets including
CASIA-WebFace (94.70%). While those images remain private, publicly available
alternatives Vec2Face (93.52%) and CemiFace (93.22%) come close behind. Our
findings reveal that they ensure proper intra-class variability while
maintaining identity separability. Demographic bias analysis shows that, even
though synthetic data inherits limited biases, it offers unprecedented control
for bias mitigation through generation parameters. These results establish
synthetic facial data as a scientifically viable and ethically imperative
alternative for facial recognition research.

</details>


### [144] [Facial Expression-based Parkinson's Disease Severity Diagnosis via Feature Fusion and Adaptive Class Balancing](https://arxiv.org/abs/2510.17373)
*Yintao Zhou,Wei Huang,Zhengyu Li,Jing Huang,Meng Pang*

Main category: cs.CV

TL;DR: 本论文提出了一种基于面部表情的新方法用于帕金森病（PD）严重程度诊断，通过注意力机制融合多种表情特征，并采用自适应类别平衡策略提升分类性能，实验结果证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于面部表情的PD诊断方法仅依赖单一表情类型，易导致误诊，并忽视了不同PD阶段的数据类别不平衡问题；同时，大多数方法只做PD/非PD的二分类，无法判断疾病严重程度。为解决这些局限性，作者提出新的方法。

Method: 该方法结合多种面部表情特征，通过注意力机制进行特征融合；并通过自适应类别平衡策略，根据样本分布和分类难度动态调整训练样本的贡献，以减少类别不平衡带来的负面影响。

Result: 实验表明，该方法在PD严重程度诊断上表现优异，注意力特征融合和自适应类别平衡均有效提升诊断性能。

Conclusion: 该方法为PD严重程度诊断提供了新的思路和工具，具有良好的潜力和应用前景。

Abstract: Parkinson's disease (PD) severity diagnosis is crucial for early detecting
potential patients and adopting tailored interventions. Diagnosing PD based on
facial expression is grounded in PD patients' "masked face" symptom and gains
growing interest recently for its convenience and affordability. However,
current facial expression-based approaches often rely on single type of
expression which can lead to misdiagnosis, and ignore the class imbalance
across different PD stages which degrades the prediction performance. Moreover,
most existing methods focus on binary classification (i.e., PD / non-PD) rather
than diagnosing the severity of PD. To address these issues, we propose a new
facial expression-based method for PD severity diagnosis which integrates
multiple facial expression features through attention-based feature fusion.
Moreover, we mitigate the class imbalance problem via an adaptive class
balancing strategy which dynamically adjusts the contribution of training
samples based on their class distribution and classification difficulty.
Experimental results demonstrate the promising performance of the proposed
method for PD severity diagnosis, as well as the efficacy of attention-based
feature fusion and adaptive class balancing.

</details>


### [145] [Closed-Loop Transfer for Weakly-supervised Affordance Grounding](https://arxiv.org/abs/2510.17384)
*Jiajin Tang,Zhengxuan Wei,Ge Zheng,Sibei Yang*

Main category: cs.CV

TL;DR: 该论文提出了LoopTrans闭环框架，实现了外视图与第一视角之间的弱监督可供性定位与知识双向传递，并在多个数据集上取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 人类能够通过观察他人互动，推断未知物体的使用方式。现有基于弱监督的可供性（affordance）定位大多只实现单向知识迁移（从外视角到第一视角），在复杂交互场景下表现有限。作者希望通过闭环方法增强模型在复杂场景下的知识表达与迁移能力。

Method: 提出LoopTrans框架，实现了外视角（他人操作物体图像）与第一视角（自我视角图像）之间的知识双向传递。在该框架中引入了跨模态统一定位、去噪知识蒸馏等机制，有效弥合了两种视角的领域差异，并提升了知识迁移效果。

Result: 实验结果表明，LoopTrans在多个图像与视频标准数据集上所有指标均取得了稳定提升，尤其在物体交互区域被人体完全遮挡等困难场景下，也有出色表现。

Conclusion: LoopTrans通过引入闭环和创新的跨模态机制，实现了更强的弱监督可供性知识建模与迁移，为复杂人机交互场景下的视觉认知提供了更优解决方案。

Abstract: Humans can perform previously unexperienced interactions with novel objects
simply by observing others engage with them. Weakly-supervised affordance
grounding mimics this process by learning to locate object regions that enable
actions on egocentric images, using exocentric interaction images with
image-level annotations. However, extracting affordance knowledge solely from
exocentric images and transferring it one-way to egocentric images limits the
applicability of previous works in complex interaction scenarios. Instead, this
study introduces LoopTrans, a novel closed-loop framework that not only
transfers knowledge from exocentric to egocentric but also transfers back to
enhance exocentric knowledge extraction. Within LoopTrans, several innovative
mechanisms are introduced, including unified cross-modal localization and
denoising knowledge distillation, to bridge domain gaps between object-centered
egocentric and interaction-centered exocentric images while enhancing knowledge
transfer. Experiments show that LoopTrans achieves consistent improvements
across all metrics on image and video benchmarks, even handling challenging
scenarios where object interaction regions are fully occluded by the human
body.

</details>


### [146] [Monitoring Horses in Stalls: From Object to Event Detection](https://arxiv.org/abs/2510.17409)
*Dmitrii Galimzianov,Viacheslav Vyshegorodtsev,Ivan Nezhivykh*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉的自动马厩监控系统，实现了马匹和人的自动检测与跟踪，显著减少了人工监控的工作量。


<details>
  <summary>Details</summary>
Motivation: 马厩内马匹行为监控对健康与福利管理至关重要，但现有方式依赖人工，非常耗时且繁琐。为提高效率，提出自动化视觉监控解决方案。

Method: 系统采用YOLOv11进行目标检测，BoT-SORT实现多目标跟踪，并根据目标轨迹及空间关系推断马厩内事件状态。同时，构建了专用数据集，并利用CLIP与GroundingDINO辅助标注，实现对五种事件的区分与对监控死角的考量。

Result: 该系统在涉及马匹的事件检测中表现可靠，但由于人员相关数据不足，人在监测中的识别能力有限。

Conclusion: 本文工作为马厩实时行为监测奠定基础，对动物福利与马厩管理具有积极意义，但未来仍需完善针对人的监测。

Abstract: Monitoring the behavior of stalled horses is essential for early detection of
health and welfare issues but remains labor-intensive and time-consuming. In
this study, we present a prototype vision-based monitoring system that
automates the detection and tracking of horses and people inside stables using
object detection and multi-object tracking techniques. The system leverages
YOLOv11 and BoT-SORT for detection and tracking, while event states are
inferred based on object trajectories and spatial relations within the stall.
To support development, we constructed a custom dataset annotated with
assistance from foundation models CLIP and GroundingDINO. The system
distinguishes between five event types and accounts for the camera's blind
spots. Qualitative evaluation demonstrated reliable performance for
horse-related events, while highlighting limitations in detecting people due to
data scarcity. This work provides a foundation for real-time behavioral
monitoring in equine facilities, with implications for animal welfare and
stable management.

</details>


### [147] [DeepDetect: Learning All-in-One Dense Keypoints](https://arxiv.org/abs/2510.17422)
*Shaharyar Ahmed Khan Tareen,Filza Khan Tareen*

Main category: cs.CV

TL;DR: 本文提出了一种融合经典和深度学习方法的新型稠密关键点检测器DeepDetect，显著提升了关键点的密度、重复性和匹配能力，适应多样及恶劣场景。


<details>
  <summary>Details</summary>
Motivation: 现有关键点检测方法在光照变化敏感、关键点稠密度和可重复性有限、对复杂场景适应性差且缺乏语义理解，无法有效聚焦视觉重要区域，制约了后续任务效果。

Method: 作者利用7种关键点检测器和2种边缘检测器的输出进行融合，生成丰富多样的伪标签掩膜，通过轻量模型ESPNet学习这些标签，获得既具备密集性又能自适应多场景的深度关键点检测器。

Result: 在Oxford Affine Covariant Regions数据集上，DeepDetect在关键点密度（0.5143）、重复率（0.9582）以及正确匹配数（59,003）等多项指标上全面超越现有方法。

Conclusion: DeepDetect通过集成多种视觉线索，结合高效深度模型，实现了高密度、高重复性和高鲁棒性的关键点检测，有效弥补了传统和端到端方法的不足，适合各种挑战性视觉场景。

Abstract: Keypoint detection is the foundation of many computer vision tasks, including
image registration, structure-from motion, 3D reconstruction, visual odometry,
and SLAM. Traditional detectors (SIFT, SURF, ORB, BRISK, etc.) and learning
based methods (SuperPoint, R2D2, LF-Net, D2-Net, etc.) have shown strong
performance yet suffer from key limitations: sensitivity to photometric
changes, low keypoint density and repeatability, limited adaptability to
challenging scenes, and lack of semantic understanding, often failing to
prioritize visually important regions. We present DeepDetect, an intelligent,
all-in-one, dense keypoint detector that unifies the strengths of classical
detectors using deep learning. Firstly, we create ground-truth masks by fusing
outputs of 7 keypoint and 2 edge detectors, extracting diverse visual cues from
corners and blobs to prominent edges and textures in the images. Afterwards, a
lightweight and efficient model: ESPNet, is trained using these masks as
labels, enabling DeepDetect to focus semantically on images while producing
highly dense keypoints, that are adaptable to diverse and visually degraded
conditions. Evaluations on the Oxford Affine Covariant Regions dataset
demonstrate that DeepDetect surpasses other detectors in keypoint density,
repeatability, and the number of correct matches, achieving maximum values of
0.5143 (average keypoint density), 0.9582 (average repeatability), and 59,003
(correct matches).

</details>


### [148] [Leveraging AV1 motion vectors for Fast and Dense Feature Matching](https://arxiv.org/abs/2510.17434)
*Julien Zouein,Hossein Javidnia,François Pitié,Anil Kokaram*

Main category: cs.CV

TL;DR: 本论文利用AV1运动向量实现高效的稠密亚像素匹配，可作为压缩域前端，兼具资源效率和几何精度。


<details>
  <summary>Details</summary>
Motivation: 传统的方法比如SIFT在视频帧间进行特征匹配时，计算资源消耗较大，且速率受限于逐帧解码和特征提取。论文旨在利用视频压缩中的运动向量，直接在压缩域高效获得帧间密集对应关系，提高效率和实用性。

Method: 作者提出重新利用AV1编码器中的运动向量，通过余弦一致性过滤获取稠密的亚像素级帧间对应，并形成短轨迹。与传统SIFT方法相比，这一前端算法无需完整解码视频图像，依赖压缩域信息完成帧间匹配与几何重建。

Result: 在短视频实验中，该方法运行速度可与顺序SIFT媲美，但CPU消耗显著更低，并获得密度更高的匹配点，从而具有竞争力的两帧几何表现。在117帧结构光恢复实验中，运动向量匹配能注册全部图像并重建0.46-0.62百万点，重投影误差为0.51-0.53像素，并验证了匹配密度的改进会提升BA（束束调整）计算耗时。

Conclusion: 实验结果表明，压缩域的对应点可作为结构光恢复等任务高效且资源节约的前端，为后端处理的大规模扩展提供了可行途径，显示出实际应用价值。

Abstract: We repurpose AV1 motion vectors to produce dense sub-pixel correspondences
and short tracks filtered by cosine consistency. On short videos, this
compressed-domain front end runs comparably to sequential SIFT while using far
less CPU, and yields denser matches with competitive pairwise geometry. As a
small SfM demo on a 117-frame clip, MV matches register all images and
reconstruct 0.46-0.62M points at 0.51-0.53,px reprojection error; BA time grows
with match density. These results show compressed-domain correspondences are a
practical, resource-efficient front end with clear paths to scaling in full
pipelines.

</details>


### [149] [Rethinking Nighttime Image Deraining via Learnable Color Space Transformation](https://arxiv.org/abs/2510.17440)
*Qiyuan Guan,Xiang Chen,Guiyue Jin,Jiyu Jin,Shumin Fan,Tianyu Song,Jinshan Pan*

Main category: cs.CV

TL;DR: 本文针对夜间去雨图像的挑战，提出了高质量的新数据集HQ-NightRain，并设计了新的网络结构CST-Net，结合了可学习的色彩空间转换和隐式光照引导，有效提升了夜间图像去雨质量。


<details>
  <summary>Details</summary>
Motivation: 夜间去雨图像处理因夜间复杂场景和缺乏高质量数据集而面临更大挑战。现有数据集普遍不能很好地反映雨与光照耦合效应，去雨效果有限。因此，作者重新思考夜间去雨任务，并致力于提升数据和方法的真实性与有效性。

Method: 1. 构建具有更高真实感和和谐性的HQ-NightRain夜间去雨数据集。
2. 提出Color Space Transformation Network（CST-Net），其中引入可学习的色彩空间转换组件（CSC），特别利用Y通道进行去雨。
3. 加入隐式光照引导机制，提升网络应对复杂夜间光照和雨水干扰的鲁棒性。

Result: 通过大量实验验证，提出的数据集和方法均优于现有方案。模型能更好地去除夜间复杂雨水并保持图像自然感。

Conclusion: 本文为夜间图像去雨任务提供了新基准和有效方法，实验效果优秀，并开源数据集和代码，推动该领域发展。

Abstract: Compared to daytime image deraining, nighttime image deraining poses
significant challenges due to inherent complexities of nighttime scenarios and
the lack of high-quality datasets that accurately represent the coupling effect
between rain and illumination. In this paper, we rethink the task of nighttime
image deraining and contribute a new high-quality benchmark, HQ-NightRain,
which offers higher harmony and realism compared to existing datasets. In
addition, we develop an effective Color Space Transformation Network (CST-Net)
for better removing complex rain from nighttime scenes. Specifically, we
propose a learnable color space converter (CSC) to better facilitate rain
removal in the Y channel, as nighttime rain is more pronounced in the Y channel
compared to the RGB color space. To capture illumination information for
guiding nighttime deraining, implicit illumination guidance is introduced
enabling the learned features to improve the model's robustness in complex
scenarios. Extensive experiments show the value of our dataset and the
effectiveness of our method. The source code and datasets are available at
https://github.com/guanqiyuan/CST-Net.

</details>


### [150] [Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS](https://arxiv.org/abs/2510.17479)
*Feng Zhou,Wenkai Guo,Pu Cao,Zhicheng Zhang,Jianqin Yin*

Main category: cs.CV

TL;DR: 本文发现稀疏视角下3D高斯散点渲染（3DGS）主要受初始化（如通过SfM得到的点云）影响，而训练过程中加入约束仅有有限改善。为此，作者通过优化初始化流程显著提升稀疏视角3DGS的表现。


<details>
  <summary>Details</summary>
Motivation: 3DGS在稀疏视角时易于过拟合训练视角，导致新颖视角渲染模糊等伪影。目前的解决方法主要是优化初始化或增加训练约束，但两者的实际贡献不明确。作者希望明确影响因素，并寻找更有效的提升手段。

Method: 作者通过消融实验表明，初始化对稀疏视角3DGS性能起决定作用。基于此，提出三点改进：（1）频率感知SfM增强低纹理区点云覆盖；（2）3DGS自初始化用光度监督补足SfM稀疏区；（3）点云正则化保持多视角一致性及空间均匀覆盖。这些方法一起提升初始化质量。

Result: 在LLFF和Mip-NeRF360数据集上，提出的方法在稀疏视角设置下均获得明显提升，验证了改进初始化的有效性。

Conclusion: 初始化决定了稀疏视角3DGS的性能上限，对初始化的优化比训练时正则化约束更有效。作者提出的增强初始化策略显著提升了3DGS在稀疏数据下的表现，为相关任务提供了更强的基线方法。

Abstract: Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the training
views, leading to artifacts like blurring in novel view rendering. Prior work
addresses it either by enhancing the initialization (\emph{i.e.}, the point
cloud from Structure-from-Motion (SfM)) or by adding training-time constraints
(regularization) to the 3DGS optimization. Yet our controlled ablations reveal
that initialization is the decisive factor: it determines the attainable
performance band in sparse-view 3DGS, while training-time constraints yield
only modest within-band improvements at extra cost. Given initialization's
primacy, we focus our design there. Although SfM performs poorly under sparse
views due to its reliance on feature matching, it still provides reliable seed
points. Thus, building on SfM, our effort aims to supplement the regions it
fails to cover as comprehensively as possible. Specifically, we design: (i)
frequency-aware SfM that improves low-texture coverage via low-frequency view
augmentation and relaxed multi-view correspondences; (ii) 3DGS
self-initialization that lifts photometric supervision into additional points,
compensating SfM-sparse regions with learned Gaussian centers; and (iii)
point-cloud regularization that enforces multi-view consistency and uniform
spatial coverage through simple geometric/visibility priors, yielding a clean
and reliable point cloud. Our experiments on LLFF and Mip-NeRF360 demonstrate
consistent gains in sparse-view settings, establishing our approach as a
stronger initialization strategy. Code is available at
https://github.com/zss171999645/ItG-GS.

</details>


### [151] [SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries](https://arxiv.org/abs/2510.17482)
*Chenxu Dang,Haiyan Liu,Guangjun Bao,Pei An,Xinyue Tang,Jie Ma,Bingchuan Sun,Yan Wang*

Main category: cs.CV

TL;DR: 本文提出了SparseWorld，一种基于稀疏与动态查询的4D语义占据世界模型。该模型通过引入可学习的查询机制及新颖的预测方法，实现了更广泛、更灵活和高效的全场景感知与预测，表现优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有的语义占据世界模型普遍依赖于静态、固定的嵌入或网格结构，这对于实际动态连续场景下的灵活感知存在限制。为了克服“原地分类”与实际动态场景不匹配的问题，提升对连续实体的表达能力，需要更灵活自适应的建模方式。

Method: 1）提出基于稀疏和动态查询的4D占据建模方式；2）引入Range-Adaptive Perception模块，通过与自车状态相关联的可学习查询，融合时空信息以扩展感知范围；3）设计State-Conditioned Forecasting模块，将预测从分类转为回归方式，提升对环境动态连续性的对齐；4）提出时序自调度训练策略，优化模型训练过程。

Result: SparseWorld在感知、预测和规划等任务上，都达到了当前最优性能。大量实验和消融分析表明，其在灵活性、适应性和效率方面明显优于传统方法。

Conclusion: 该文方法突破了传统占据模型感知灵活性的限制，通过稀疏动态查询，实现了更优的动态空间语义模型，推动了自动驾驶与世界建模技术的进步。

Abstract: Semantic occupancy has emerged as a powerful representation in world models
for its ability to capture rich spatial semantics. However, most existing
occupancy world models rely on static and fixed embeddings or grids, which
inherently limit the flexibility of perception. Moreover, their ``in-place
classification" over grids exhibits a potential misalignment with the dynamic
and continuous nature of real scenarios.In this paper, we propose SparseWorld,
a novel 4D occupancy world model that is flexible, adaptive, and efficient,
powered by sparse and dynamic queries. We propose a Range-Adaptive Perception
module, in which learnable queries are modulated by the ego vehicle states and
enriched with temporal-spatial associations to enable extended-range
perception. To effectively capture the dynamics of the scene, we design a
State-Conditioned Forecasting module, which replaces classification-based
forecasting with regression-guided formulation, precisely aligning the dynamic
queries with the continuity of the 4D environment. In addition, We specifically
devise a Temporal-Aware Self-Scheduling training strategy to enable smooth and
efficient training. Extensive experiments demonstrate that SparseWorld achieves
state-of-the-art performance across perception, forecasting, and planning
tasks. Comprehensive visualizations and ablation studies further validate the
advantages of SparseWorld in terms of flexibility, adaptability, and
efficiency. The code is available at https://github.com/MSunDYY/SparseWorld.

</details>


### [152] [Split-Fuse-Transport: Annotation-Free Saliency via Dual Clustering and Optimal Transport Alignment](https://arxiv.org/abs/2510.17484)
*Muhammad Umer Ramzan,Ali Zia,Abdelwahed Khamis,Noman Ali,Usman Ali,Wei Xiang*

Main category: cs.CV

TL;DR: 本文提出了一种新的无监督显著性目标检测（SOD）方法 —— AutoSOD，通过改良的原型最优传输方式(POTNet)自动生成精确的伪标签，显著提升了检测精度，接近监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有SOD方法依赖强监督标注，成本高昂。部分无监督方法利用伪标签提升，但现有的伪标签生成机制存在边界和区域描绘不准确、传输机制受限的痛点。希望设计一种既高效又准确的无监督标注生成机制，提升整体检测效果。

Method: 提出POTNet，通过熵值引导的双聚类头（高熵像素用谱聚类、低熵像素用K-means），并结合最优传输机制，使两种原型集合对齐，生成更优伪掩膜，最终通过标准的MaskFormer风格架构组成完整AutoSOD端到端无监督流水线。

Result: 在五个公开数据集上的实验表明，AutoSOD在F-measure指标上比其他同类无监督方法最高提升26%、比弱监督方法最高提升36%，在精度和训练效率上均取得优势。

Conclusion: AutoSOD显著缩小了无监督及弱监督SOD方法与全监督方法之间的性能差距，为显著性目标检测的无监督标注和建模提供了更具实用价值的解决方案。

Abstract: Salient object detection (SOD) aims to segment visually prominent regions in
images and serves as a foundational task for various computer vision
applications. We posit that SOD can now reach near-supervised accuracy without
a single pixel-level label, but only when reliable pseudo-masks are available.
We revisit the prototype-based line of work and make two key observations.
First, boundary pixels and interior pixels obey markedly different geometry;
second, the global consistency enforced by optimal transport (OT) is
underutilized if prototype quality is weak. To address this, we introduce
POTNet, an adaptation of Prototypical Optimal Transport that replaces POT's
single k-means step with an entropy-guided dual-clustering head: high-entropy
pixels are organized by spectral clustering, low-entropy pixels by k-means, and
the two prototype sets are subsequently aligned by OT. This
split-fuse-transport design yields sharper, part-aware pseudo-masks in a single
forward pass, without handcrafted priors. Those masks supervise a standard
MaskFormer-style encoder-decoder, giving rise to AutoSOD, an end-to-end
unsupervised SOD pipeline that eliminates SelfMask's offline voting yet
improves both accuracy and training efficiency. Extensive experiments on five
benchmarks show that AutoSOD outperforms unsupervised methods by up to 26% and
weakly supervised methods by up to 36% in F-measure, further narrowing the gap
to fully supervised models.

</details>


### [153] [Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization](https://arxiv.org/abs/2510.17501)
*Yuanli Wu,Long Zhang,Yue Du,Bin Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于准则指导和伪标签的提示框架，实现了无需训练即可对视频进行高质量的零样本摘要，超越了现有无监督和零样本方法。


<details>
  <summary>Details</summary>
Motivation: 长视频内容快速增长，但人工标注成本高，现有无监督与零样本方法难以兼顾高语义表达与通用性，需要一种高效、具泛化能力且易于解释的视频摘要方法。

Method: 提出Rubric-guided, pseudo-labeled prompting框架：利用少量真实标注生成高置信伪标签，并汇总为适应数据集的打分准则。在推理时对首末片段仅基于自身描述打分，中间片段结合相邻场景简要上下文，对LLM评分进行指导，无需调参。

Result: 在SumMe和TVSum两个数据集上，获得了F1分数分别为57.58和63.05，超过了无监督与先前零样本基线，并接近有监督方法表现。

Conclusion: 准则驱动的伪标签稳定提升了LLM评分可靠性，为视频摘要任务确立了一种通用、可解释的零样本新范式。

Abstract: With the rapid proliferation of video content across social media,
surveillance, and education platforms, efficiently summarizing long videos into
concise yet semantically faithful surrogates has become increasingly vital.
Existing supervised methods achieve strong in-domain accuracy by learning from
dense annotations but suffer from high labeling costs and limited cross-dataset
generalization, while unsupervised approaches, though label-free, often fail to
capture high-level human semantics and fine-grained narrative cues. More
recently, zero-shot prompting pipelines have leveraged large language models
(LLMs) for training-free video summarization, yet remain highly sensitive to
handcrafted prompt templates and dataset-specific score normalization. To
overcome these limitations, we introduce a rubric-guided, pseudo-labeled
prompting framework that transforms a small subset of ground-truth annotations
into high-confidence pseudo labels, which are aggregated into structured,
dataset-adaptive scoring rubrics guiding interpretable scene evaluation. During
inference, first and last segments are scored based solely on their
descriptions, whereas intermediate ones incorporate brief contextual summaries
of adjacent scenes to assess narrative progression and redundancy. This
contextual prompting enables the LLM to balance local salience and global
coherence without parameter tuning. On SumMe and TVSum, our method achieves F1
scores of \textbf{57.58} and \textbf{63.05}, surpassing unsupervised and prior
zero-shot baselines while approaching supervised performance. The results
demonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based
scoring and establishes a general, interpretable zero-shot paradigm for video
summarization.

</details>


### [154] [MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models](https://arxiv.org/abs/2510.17519)
*Yongshun Zhang,Zhongyi Fan,Yonghang Zhang,Zhangzikang Li,Weifeng Chen,Zhongwei Feng,Chaoyue Wang,Peng Hou,Anxiang Zeng*

Main category: cs.CV

TL;DR: 本文介绍了大规模视频生成模型MUG-V 10B的训练框架，并开源了完整的训练与推理代码。该模型在电商视频生成任务中表现优于主流开源基线，同时在整体表现上可与SOTA持平。


<details>
  <summary>Details</summary>
Motivation: 当前大规模视觉生成模型（如视频生成）训练非常复杂且资源消耗大，主要因文本-视频对齐、高维时空依赖等难题。因此，亟需提升视频生成模型训练效率和性能，同时促进社区共享。

Method: 提出了针对大规模视频生成训练的四大优化支柱：数据处理、模型结构、训练策略和基础架构。具体包括高效数据预处理、视频压缩、参数规模扩展、课程式预训练、对齐优化的后训练，并结合Megatron-Core大幅提升分布式训练效率。

Result: MUG-V 10B模型在电商类视频任务的人评中优于主流开源基线，并能与最新SOTA模型整体持平；所用框架实现了近线性多节点扩展和高效训练。

Conclusion: 该工作不仅取得了出色的任务性能，还首次开源了完整的大规模视频生成训练/推理代码与模型权重，推动了视频生成领域的开放与发展。

Abstract: In recent years, large-scale generative models for visual content
(\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable
progress. However, training large-scale video generation models remains
particularly challenging and resource-intensive due to cross-modal text-video
alignment, the long sequences involved, and the complex spatiotemporal
dependencies. To address these challenges, we present a training framework that
optimizes four pillars: (i) data processing, (ii) model architecture, (iii)
training strategy, and (iv) infrastructure for large-scale video generation
models. These optimizations delivered significant efficiency gains and
performance improvements across all stages of data preprocessing, video
compression, parameter scaling, curriculum-based pretraining, and
alignment-focused post-training. Our resulting model, MUG-V 10B, matches recent
state-of-the-art video generators overall and, on e-commerce-oriented video
generation tasks, surpasses leading open-source baselines in human evaluations.
More importantly, we open-source the complete stack, including model weights,
Megatron-Core-based large-scale training code, and inference pipelines for
video generation and enhancement. To our knowledge, this is the first public
release of large-scale video generation training code that exploits
Megatron-Core to achieve high training efficiency and near-linear multi-node
scaling, details are available in
\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}.

</details>


### [155] [MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for Longitudinal MRI Segmentation](https://arxiv.org/abs/2510.17529)
*Yovin Yahathugoda,Davide Prezzi,Piyalitt Ittichaiwong,Vicky Goh,Sebastien Ourselin,Michela Antonelli*

Main category: cs.CV

TL;DR: 本文提出了一种新型半监督双时点3D前列腺分割网络MambaX-Net，显著提升了前列腺分割在长期随访中的准确性，即使在标注稀缺和数据噪声大的情况下也优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 主动监测（AS）为低/中风险前列腺癌提供保守管理手段，减少过度治疗，需连续追踪MRI影像；而当前深度学习分割模型常依赖单时点、大量专家标注，难以适应纵向、标注稀缺的AS场景。

Method: 设计MambaX-Net网络，整合了：（1）Mamba增强的交叉注意力模块，提升对时序和空间关系的捕捉；（2）形状提取模块，将前一时点分割结果编码为潜在解剖信息，为当前分割提供参考，并通过自训练半监督策略利用伪标签提升模型泛化和鲁棒性。

Result: 在一个纵向AS数据集上实验，MambaX-Net相比U-Net及Transformer类模型表现更优，无论是在标注不足还是标签嘈杂的情况下，前列腺分区分割精度都得到了提升。

Conclusion: MambaX-Net有效解决了纵向AS中前列腺自动分割的难题，可在专家标注稀缺和多时点跟踪等临床实际场景下，提升分割的准确性与实用性，推动自动化前列腺癌管理发展。

Abstract: Active Surveillance (AS) is a treatment option for managing low and
intermediate-risk prostate cancer (PCa), aiming to avoid overtreatment while
monitoring disease progression through serial MRI and clinical follow-up.
Accurate prostate segmentation is an important preliminary step for automating
this process, enabling automated detection and diagnosis of PCa. However,
existing deep-learning segmentation models are often trained on
single-time-point and expertly annotated datasets, making them unsuitable for
longitudinal AS analysis, where multiple time points and a scarcity of expert
labels hinder their effective fine-tuning. To address these challenges, we
propose MambaX-Net, a novel semi-supervised, dual-scan 3D segmentation
architecture that computes the segmentation for time point t by leveraging the
MRI and the corresponding segmentation mask from the previous time point. We
introduce two new components: (i) a Mamba-enhanced Cross-Attention Module,
which integrates the Mamba block into cross attention to efficiently capture
temporal evolution and long-range spatial dependencies, and (ii) a Shape
Extractor Module that encodes the previous segmentation mask into a latent
anatomical representation for refined zone delination. Moreover, we introduce a
semi-supervised self-training strategy that leverages pseudo-labels generated
from a pre-trained nnU-Net, enabling effective learning without expert
annotations. MambaX-Net was evaluated on a longitudinal AS dataset, and results
showed that it significantly outperforms state-of-the-art U-Net and
Transformer-based models, achieving superior prostate zone segmentation even
when trained on limited and noisy data.

</details>


### [156] [WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection](https://arxiv.org/abs/2510.17566)
*Nachuan Ma,Zhengfei Song,Qiang Hu,Xiaoyu Tang,Chengxi Zhang,Rui Fan,Lihua Xie*

Main category: cs.CV

TL;DR: 本文提出WP-CrackNet,一种仅依赖图像级标签、无需昂贵像素级标注即可进行道路裂缝像素级检测的新型弱监督方法，并在多个数据集上达到了与有监督方法媲美的效果。


<details>
  <summary>Details</summary>
Motivation: 道路裂缝检测对于智慧城市交通基础设施维护至关重要，但像素级标注耗时耗力。本文旨在降低标注成本，通过仅依赖图像级标签实现高效裂缝检测。

Method: WP-CrackNet集成了分类器(生成类激活图CAMs)、重构器(衡量特征可推断性)和检测器(输出像素级检测结果)三部分。训练时，分类器和重构器通过对抗学习促进CAMs覆盖完整裂缝区域，检测器基于后处理的CAM伪标签学习。同时，引入路径感知注意力模块(PAAM)融合高—低层特征，及中心增强CAM一致性模块(CECCM)优化伪标签生成。

Result: 在作者构建的三个图像级数据集及公开对比中，WP-CrackNet的性能接近完全监督方法，明显优于当前主流弱监督裂缝检测方法。

Conclusion: WP-CrackNet显著提升了弱监督道路裂缝检测的精度和稳定性，为大规模自动化道路巡检提供了更可扩展的解决方案。源代码和数据集已开源，便于社区复现与应用。

Abstract: Road crack detection is essential for intelligent infrastructure maintenance
in smart cities. To reduce reliance on costly pixel-level annotations, we
propose WP-CrackNet, an end-to-end weakly-supervised method that trains with
only image-level labels for pixel-wise crack detection. WP-CrackNet integrates
three components: a classifier generating class activation maps (CAMs), a
reconstructor measuring feature inferability, and a detector producing
pixel-wise road crack detection results. During training, the classifier and
reconstructor alternate in adversarial learning to encourage crack CAMs to
cover complete crack regions, while the detector learns from pseudo labels
derived from post-processed crack CAMs. This mutual feedback among the three
components improves learning stability and detection accuracy. To further boost
detection performance, we design a path-aware attention module (PAAM) that
fuses high-level semantics from the classifier with low-level structural cues
from the reconstructor by modeling spatial and channel-wise dependencies.
Additionally, a center-enhanced CAM consistency module (CECCM) is proposed to
refine crack CAMs using center Gaussian weighting and consistency constraints,
enabling better pseudo-label generation. We create three image-level datasets
and extensive experiments show that WP-CrackNet achieves comparable results to
supervised methods and outperforms existing weakly-supervised methods,
significantly advancing scalable road inspection. The source code package and
datasets are available at https://mias.group/WP-CrackNet/.

</details>


### [157] [PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception](https://arxiv.org/abs/2510.17568)
*Kaichen Zhou,Yuhan Wang,Grace Chen,Xinhai Chang,Gaspard Beaudouin,Fangneng Zhan,Paul Pu Liang,Mengyu Wang*

Main category: cs.CV

TL;DR: PAGE-4D模型扩展了VGGT，实现了对动态场景的摄像机位姿估计、深度预测和点云重建，无需后处理，并在具体任务上表现优于VGGT。


<details>
  <summary>Details</summary>
Motivation: 现有的VGGT等3D前馈模型在静态场景中的表现优异，但面对包含运动物体或可变形物体的动态场景时能力有限，因此需要更适用于真实动态场景的模型。

Method: 提出PAGE-4D模型，引入了一个动态感知聚合器，通过预测动态感知掩码来分离静态和动态信息，实现对动态区域的选择性压制（用于位姿估计）和增强（用于几何重建），可用于摄像机位姿估计、深度估计和点云重建。

Result: PAGE-4D在动态场景下，摄像机位姿估计、单目与视频深度估计、稠密点云重建等任务上均优于原始VGGT模型。

Conclusion: PAGE-4D有效解决了动态场景下多任务重建中任务冲突的问题，能够无后处理地进行高质量的动态场景重建。

Abstract: Recent 3D feed-forward models, such as the Visual Geometry Grounded
Transformer (VGGT), have shown strong capability in inferring 3D attributes of
static scenes. However, since they are typically trained on static datasets,
these models often struggle in real-world scenarios involving complex dynamic
elements, such as moving humans or deformable objects like umbrellas. To
address this limitation, we introduce PAGE-4D, a feedforward model that extends
VGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and
point cloud reconstruction -- all without post-processing. A central challenge
in multi-task 4D reconstruction is the inherent conflict between tasks:
accurate camera pose estimation requires suppressing dynamic regions, while
geometry reconstruction requires modeling them. To resolve this tension, we
propose a dynamics-aware aggregator that disentangles static and dynamic
information by predicting a dynamics-aware mask -- suppressing motion cues for
pose estimation while amplifying them for geometry reconstruction. Extensive
experiments show that PAGE-4D consistently outperforms the original VGGT in
dynamic scenarios, achieving superior results in camera pose estimation,
monocular and video depth estimation, and dense point map reconstruction.

</details>


### [158] [Expose Camouflage in the Water: Underwater Camouflaged Instance Segmentation and Dataset](https://arxiv.org/abs/2510.17585)
*Chuhong Wang,Hua Li,Chongyi Li,Huazhong Liu,Xiongxin Tang,Sam Kwong*

Main category: cs.CV

TL;DR: 本文提出了首个针对海洋伪装实例分割的大型数据集UCIS4K，并设计了一种基于Segment Anything Model（SAM）的水下伪装实例分割新方法UCIS-SAM，在多个公开数据集和新数据集上超过当前最优方法。


<details>
  <summary>Details</summary>
Motivation: 水下环境因色彩失真、低对比度和模糊等问题，导致现有伪装实例分割方法在该场景下效果不佳，缺少高质量水下伪装数据集也制约了技术发展。

Method: （1）构建了标注精细、涵盖3,953幅海洋伪装生物图片的数据集UCIS4K；（2）提出UCIS-SAM分割网络，核心模块包括：通道平衡优化（CBOM）提升水下特征学习，频域真积分（FDTIM）强化本质特征抑制伪装干扰，多尺度特征频率聚合（MFFAM）增强低对比对象边界分割。

Result: 在UCIS4K和公开基准上的实验显示，所提UCIS-SAM显著超越目前最优分割模型，在水下伪装分割任务中展示更高精度和鲁棒性。

Conclusion: 针对水下伪装实例分割的挑战，本文提出的数据集与方法有效弥补了数据和分割性能短板，推动了相关领域发展。

Abstract: With the development of underwater exploration and marine protection,
underwater vision tasks are widespread. Due to the degraded underwater
environment, characterized by color distortion, low contrast, and blurring,
camouflaged instance segmentation (CIS) faces greater challenges in accurately
segmenting objects that blend closely with their surroundings. Traditional
camouflaged instance segmentation methods, trained on terrestrial-dominated
datasets with limited underwater samples, may exhibit inadequate performance in
underwater scenes. To address these issues, we introduce the first underwater
camouflaged instance segmentation (UCIS) dataset, abbreviated as UCIS4K, which
comprises 3,953 images of camouflaged marine organisms with instance-level
annotations. In addition, we propose an Underwater Camouflaged Instance
Segmentation network based on Segment Anything Model (UCIS-SAM). Our UCIS-SAM
includes three key modules. First, the Channel Balance Optimization Module
(CBOM) enhances channel characteristics to improve underwater feature learning,
effectively addressing the model's limited understanding of underwater
environments. Second, the Frequency Domain True Integration Module (FDTIM) is
proposed to emphasize intrinsic object features and reduce interference from
camouflage patterns, enhancing the segmentation performance of camouflaged
objects blending with their surroundings. Finally, the Multi-scale Feature
Frequency Aggregation Module (MFFAM) is designed to strengthen the boundaries
of low-contrast camouflaged instances across multiple frequency bands,
improving the model's ability to achieve more precise segmentation of
camouflaged objects. Extensive experiments on the proposed UCIS4K and public
benchmarks show that our UCIS-SAM outperforms state-of-the-art approaches.

</details>


### [159] [ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling](https://arxiv.org/abs/2510.17603)
*Shuyuan Zhang,Chenhan Jiang,Zuoou Li,Jiankang Deng*

Main category: cs.CV

TL;DR: ShapeCraft提出利用多智能体和图结构方法，实现从文本生成结构化、可交互、带纹理且高质量的3D资产，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前文本生成3D技术虽可提升3D资源生成的便捷性，但往往只输出无结构网格，缺乏交互性与编辑友好度，难以应用于艺术创作等实际工作流。

Method: 作者提出Graph-based Procedural Shape (GPS)表示方法，将复杂自然语言指令解析为结构化图形任务，并由多智能体LLM层次化解析，逐步生成并调整结构化、带纹理的3D资产。

Result: ShapeCraft生成的3D资产具有更高的几何准确性和语义丰富性，并支持动画及用户个性化编辑，优于现有LLM驱动的3D生成方法。

Conclusion: ShapeCraft不仅提升了文本到3D生成的质量，还增强了模型的交互性和编辑能力，适于更广泛的实际与艺术3D应用场景。

Abstract: 3D generation from natural language offers significant potential to reduce
expert manual modeling efforts and enhance accessibility to 3D assets. However,
existing methods often yield unstructured meshes and exhibit poor
interactivity, making them impractical for artistic workflows. To address these
limitations, we represent 3D assets as shape programs and introduce ShapeCraft,
a novel multi-agent framework for text-to-3D generation. At its core, we
propose a Graph-based Procedural Shape (GPS) representation that decomposes
complex natural language into a structured graph of sub-tasks, thereby
facilitating accurate LLM comprehension and interpretation of spatial
relationships and semantic shape details. Specifically, LLM agents
hierarchically parse user input to initialize GPS, then iteratively refine
procedural modeling and painting to produce structured, textured, and
interactive 3D assets. Qualitative and quantitative experiments demonstrate
ShapeCraft's superior performance in generating geometrically accurate and
semantically rich 3D assets compared to existing LLM-based agents. We further
show the versatility of ShapeCraft through examples of animated and
user-customized editing, highlighting its potential for broader interactive
applications.

</details>


### [160] [Integrating BIM and UAV-based photogrammetry for Automated 3D Structure Model Segmentation](https://arxiv.org/abs/2510.17609)
*Siqi Chen,Shanyue Guan*

Main category: cs.CV

TL;DR: 本文提出了一种基于机器学习的框架，实现无人机（UAV）扫描3D点云的自动分割，有效提升了基础设施健康监测的自动化和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统三维基础设施模型中组件分割主要依赖人工标注，既耗时又易出错，成为结构健康监测自动化的瓶颈。

Method: 本文方法结合了UAV扫描获取的真实点云和由BIM建模生成的合成数据，通过融合两类数据进行训练，用于自动识别和分割关键结构组件。

Result: 在铁路轨道数据集上测试，准确识别和分割了主要部件（如钢轨与枕木）；并且通过引入BIM数据，减少了对真实数据的需求和训练时间，同时保持了良好的分割效果。

Conclusion: 该自动分割方法提升了3D基础设施模型分割的效率和精度，促进了UAV与BIM在结构健康监测和基础设施管理领域的融合应用。

Abstract: The advancement of UAV technology has enabled efficient, non-contact
structural health monitoring. Combined with photogrammetry, UAVs can capture
high-resolution scans and reconstruct detailed 3D models of infrastructure.
However, a key challenge remains in segmenting specific structural components
from these models-a process traditionally reliant on time-consuming and
error-prone manual labeling. To address this issue, we propose a machine
learning-based framework for automated segmentation of 3D point clouds. Our
approach uses the complementary strengths of real-world UAV-scanned point
clouds and synthetic data generated from Building Information Modeling (BIM) to
overcome the limitations associated with manual labeling. Validation on a
railroad track dataset demonstrated high accuracy in identifying and segmenting
major components such as rails and crossties. Moreover, by using smaller-scale
datasets supplemented with BIM data, the framework significantly reduced
training time while maintaining reasonable segmentation accuracy. This
automated approach improves the precision and efficiency of 3D infrastructure
model segmentation and advances the integration of UAV and BIM technologies in
structural health monitoring and infrastructure management.

</details>


### [161] [One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum Unsupervised Anomaly Detection](https://arxiv.org/abs/2510.17611)
*Jia Guo,Shuai Lu,Lei Fan,Zelin Li,Donglin Di,Yang Song,Weihang Zhang,Wenbing Zhu,Hong Yan,Fang Chen,Huiqi Li,Hongen Liao*

Main category: cs.CV

TL;DR: 本文提出了Dinomaly2，一个统一的无监督异常检测（UAD）框架，实现了在多类别、跨模态、多任务等多个现实场景下的领先性能，且方法极为简洁，适用于全谱图像异常检测。


<details>
  <summary>Details</summary>
Motivation: 当前UAD领域存在“单类别模型效果强，多类别模型表现弱”的问题，且方法细分严重，造成部署困难。亟需一个跨场景、跨模态的统一方法来解决性能与通用性的矛盾。

Method: Dinomaly2采用重建为基础的标准框架，通过协调5个简单元素实现极简设计，能原生兼容2D、多视角、RGB-3D、RGB-IR等多模态输入以及单类别、多类别、少样本等多种任务设置，无需针对性修改。

Result: 在12个UAD基准上，Dinomaly2均取得最优或超越现有的方法。例如多类别MVTec-AD与VisA数据集I-AUROC分别达99.9%与99.3%；少样本每类仅8张图也超越所有full-shot模型。多模态、多视角等任务也具最强表现。

Conclusion: Dinomaly2结合极简方法、极强可扩展性和统一适用性，成为现实异常检测应用的全谱统一解决方案，奠定了“简单即普适”的研究基础。

Abstract: Unsupervised anomaly detection (UAD) has evolved from building specialized
single-class models to unified multi-class models, yet existing multi-class
models significantly underperform the most advanced one-for-one counterparts.
Moreover, the field has fragmented into specialized methods tailored to
specific scenarios (multi-class, 3D, few-shot, etc.), creating deployment
barriers and highlighting the need for a unified solution. In this paper, we
present Dinomaly2, the first unified framework for full-spectrum image UAD,
which bridges the performance gap in multi-class models while seamlessly
extending across diverse data modalities and task settings. Guided by the "less
is more" philosophy, we demonstrate that the orchestration of five simple
element achieves superior performance in a standard reconstruction-based
framework. This methodological minimalism enables natural extension across
diverse tasks without modification, establishing that simplicity is the
foundation of true universality. Extensive experiments on 12 UAD benchmarks
demonstrate Dinomaly2's full-spectrum superiority across multiple modalities
(2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class,
inference-unified multi-class, few-shot) and application domains (industrial,
biological, outdoor). For example, our multi-class model achieves unprecedented
99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For
multi-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art
performance with minimum adaptations. Moreover, using only 8 normal examples
per class, our method surpasses previous full-shot models, achieving 98.7% and
97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design,
computational scalability, and universal applicability positions Dinomaly2 as a
unified solution for the full spectrum of real-world anomaly detection
applications.

</details>


### [162] [CaMiT: A Time-Aware Car Model Dataset for Classification and Generation](https://arxiv.org/abs/2510.17626)
*Frédéric LIN,Biruk Abere Ambaw,Adrian Popescu,Hejer Ammar,Romaric Audigier,Hervé Le Borgne*

Main category: cs.CV

TL;DR: 论文提出了CaMiT数据集，用于研究随时间演化的细粒度视觉识别与生成任务，并探索了提升模型时间适应性的多种方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中，物体（如汽车）外观会随着时间发生变化，现有AI系统和数据集很少考虑时间因素，导致模型在跨年份识别时准确率下降，亟需新的数据和方法进行时间适应研究。

Method: 作者构建了CaMiT数据集，收集了787K有标签和5.1M无标签汽车图片，覆盖190个车型及其在2005-2023年间的变化。提出了time-incremental classification设置，模拟新类别出现、旧类别消失等真实应用场景，并测试了两种提升方法：逐步预训练和逐步分类器调整。此外，还尝试了利用时间元数据的时间感知型图像生成方法。

Result: 在有标签的静态训练基础上，模型能在资源效率上媲美大规模通用模型，但跨年份测试时准确率下降。提出的time-incremental方法提升了模型的时间鲁棒性。时间感知型图像生成明显提升了图片的真实感。

Conclusion: CaMiT为细粒度视觉任务中的时间自适应研究提供了全面的基准，验证了时间渐进策略有效，并为未来具有时间感知的视觉识别和生成模型提供了实验与数据基础。

Abstract: AI systems must adapt to evolving visual environments, especially in domains
where object appearances change over time. We introduce Car Models in Time
(CaMiT), a fine-grained dataset capturing the temporal evolution of car models,
a representative class of technological artifacts. CaMiT includes 787K labeled
samples of 190 car models (2007-2023) and 5.1M unlabeled samples (2005-2023),
supporting both supervised and self-supervised learning. Static pretraining on
in-domain data achieves competitive performance with large-scale generalist
models while being more resource-efficient, yet accuracy declines when models
are tested across years. To address this, we propose a time-incremental
classification setting, a realistic continual learning scenario with emerging,
evolving, and disappearing classes. We evaluate two strategies:
time-incremental pretraining, which updates the backbone, and time-incremental
classifier learning, which updates only the final layer, both improving
temporal robustness. Finally, we explore time-aware image generation that
leverages temporal metadata during training, yielding more realistic outputs.
CaMiT offers a rich benchmark for studying temporal adaptation in fine-grained
visual recognition and generation.

</details>


### [163] [Self-supervised Pre-training for Mapping of Archaeological Stone Wall in Historic Landscapes Using High-Resolution DEM Derivatives](https://arxiv.org/abs/2510.17644)
*Zexian Huang,Mashnoon Islam,Brian Armstrong,Kourosh Khoshelham,Martin Tomko*

Main category: cs.CV

TL;DR: 该论文提出了一种利用高分辨率LiDARDEM和自监督学习实现澳大利亚干石墙自动测绘的方法，在植被密集、标注数据稀缺情况下有良好效果。


<details>
  <summary>Details</summary>
Motivation: 澳大利亚干石墙因生态和文化价值需被映射，但人工标注代价高且墙体难以识别。现有深度学习方法难以应对植被遮挡和标注数据不足两个问题。

Method: 提出DINO-CV分割框架，利用高分辨率LiDAR数字高程模型（DEM）克服植被遮挡，通过自监督的跨视图蒸馏预训练降低对人工标注数据的依赖。框架支持多种主流视觉骨干结构，学习结构-几何不变特征。

Result: 在维州Budj Bim世界文化遗产地进行实证，能较好（mIoU分别达68.6%/63.8%）识别密集干石墙，即使只用10%标注数据微调也有较好表现。

Conclusion: 方法证明自监督学习结合高分辨率DEM在缺少标注数据、植被遮挡环境中可有效自动化干石墙测绘，对生态与文化遗产保护具有应用潜力。

Abstract: Dry-stone walls hold significant heritage and environmental value. Mapping
these structures is essential for ecosystem preservation and wildfire
management in Australia. Yet, many walls remain unidentified due to their
inaccessibility and the high cost of manual mapping. Deep learning-based
segmentation offers a scalable solution, but two major challenges persist: (1)
visual occlusion of low-lying walls by dense vegetation, and (2) limited
labeled data for supervised training. We propose DINO-CV, a segmentation
framework for automatic mapping of low-lying dry-stone walls using
high-resolution Airborne LiDAR-derived digital elevation models (DEMs). DEMs
overcome visual occlusion by capturing terrain structures hidden beneath
vegetation, enabling analysis of structural rather than spectral cues. DINO-CV
introduces a self-supervised cross-view pre-training strategy based on
knowledge distillation to mitigate data scarcity. It learns invariant visual
and geometric representations across multiple DEM derivatives, supporting
various vision backbones including ResNet, Wide ResNet, and Vision
Transformers. Applied to the UNESCO World Heritage cultural landscape of Budj
Bim, Victoria, the method identifies one of Australia's densest collections of
colonial dry-stone walls beyond Indigenous heritage contexts. DINO-CV achieves
a mean Intersection over Union (mIoU) of 68.6% on test areas and maintains
63.8% mIoU when fine-tuned with only 10% labeled data. These results
demonstrate the potential of self-supervised learning on high-resolution DEM
derivatives for automated dry-stone wall mapping in vegetated and heritage-rich
environments with scarce annotations.

</details>


### [164] [Frugal Federated Learning for Violence Detection: A Comparison of LoRA-Tuned VLMs and Personalized CNNs](https://arxiv.org/abs/2510.17651)
*Sébastien Thuau,Siba Haidar,Ayush Bajracharya,Rachid Chelouah*

Main category: cs.CV

TL;DR: 本文比较了两种节能的联邦学习方法（VLM模型的零样本与微调、个性化CNN3D训练）在暴力检测中的表现，强调能耗与环境影响，提出了混合模型的应用策略。


<details>
  <summary>Details</summary>
Motivation: 在视频监控中进行暴力检测对模型精度、能耗和环境影响有较高要求，现有方法要么计算开销大，要么缺乏灵活性，因此作者希望比较经典轻量CNN和最新VLM模型的优势与局限，推动更可持续和高效的联邦学习系统。

Method: 采用LLaVA-7B（作为代表性VLM）和参数量为65.8M的紧凑型3D CNN，并在非独立同分布的数据设置下，比较了零样本推理、联邦微调（LoRA）、个性化CNN3D训练三种策略。评估指标包括准确率、标定、能耗和碳排放等，分析了模型部署的可持续性权衡。

Result: 两类方法准确率均超过90%，CNN3D在ROC AUC和log loss上略超微调VLM，且能耗更低。VLM更适合复杂语境推理和多模态场景。首次详细量化了训练和推理过程的能耗及CO₂排放，并给出分析。

Conclusion: 推荐采用混合模型：日常用轻量CNN，遇到复杂或需描述推理时再调用VLM。该混合框架兼顾准确率、能耗与环境友好性，为负责任、资源感知的视频监控AI奠定了可重复基线。

Abstract: We examine frugal federated learning approaches to violence detection by
comparing two complementary strategies: (i) zero-shot and federated fine-tuning
of vision-language models (VLMs), and (ii) personalized training of a compact
3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter
CNN3D as representative cases, we evaluate accuracy, calibration, and energy
usage under realistic non-IID settings.
  Both approaches exceed 90% accuracy. CNN3D slightly outperforms Low-Rank
Adaptation(LoRA)-tuned VLMs in ROC AUC and log loss, while using less energy.
VLMs remain favorable for contextual reasoning and multimodal inference. We
quantify energy and CO$_2$ emissions across training and inference, and analyze
sustainability trade-offs for deployment.
  To our knowledge, this is the first comparative study of LoRA-tuned
vision-language models and personalized CNNs for federated violence detection,
with an emphasis on energy efficiency and environmental metrics.
  These findings support a hybrid model: lightweight CNNs for routine
classification, with selective VLM activation for complex or descriptive
scenarios. The resulting framework offers a reproducible baseline for
responsible, resource-aware AI in video surveillance, with extensions toward
real-time, multimodal, and lifecycle-aware systems.

</details>


### [165] [4DSegStreamer: Streaming 4D Panoptic Segmentation via Dual Threads](https://arxiv.org/abs/2510.17664)
*Ling Liu,Jun Tian,Li Yi*

Main category: cs.CV

TL;DR: 本文提出了一种用于流式4D全景分割的新框架4DSegStreamer，采用双线程系统，有效提升了在动态环境下的实时感知能力，并在多个数据集上验证了其优越的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 在紧急疏散密集人群和自动驾驶等高度动态环境中，要求系统在受限时间内能进行实时、细粒度的4D全景分割。然而，现有流式感知方法在高帧率和动态场景下往往精度和时效性有限，因此需要一种兼顾性能与实时性的解决方案。

Method: 提出4DSegStreamer框架，核心为双线程系统，包括预测线程和推理线程。预测线程利用历史运动与几何信息提前提取特征并预测未来动态，而推理线程则针对新帧进行高效预测，并通过对最新内存以及自车运动和动态物体运动的补偿，保证预测的及时性与准确性。该框架可通用地集成到现有的3D或4D分割方法中，增强其实时性能。

Result: 实验在室内HOI4D数据集、室外SemanticKITTI与nuScenes数据集上进行。结果显示，4DSegStreamer超越了现有的流式感知方法，特别是在高帧率条件下表现出更好的鲁棒性和对动态目标的预测准确性。

Conclusion: 4DSegStreamer显著提升了流式4D全景分割的实时性与精度，尤其适合动态、复杂场景的应用，对实际自动驾驶和人群感知任务具有重要意义。

Abstract: 4D panoptic segmentation in a streaming setting is critical for highly
dynamic environments, such as evacuating dense crowds and autonomous driving in
complex scenarios, where real-time, fine-grained perception within a
constrained time budget is essential. In this paper, we introduce
4DSegStreamer, a novel framework that employs a Dual-Thread System to
efficiently process streaming frames. The framework is general and can be
seamlessly integrated into existing 3D and 4D segmentation methods to enable
real-time capability. It also demonstrates superior robustness compared to
existing streaming perception approaches, particularly under high FPS
conditions. The system consists of a predictive thread and an inference thread.
The predictive thread leverages historical motion and geometric information to
extract features and forecast future dynamics. The inference thread ensures
timely prediction for incoming frames by aligning with the latest memory and
compensating for ego-motion and dynamic object movements. We evaluate
4DSegStreamer on the indoor HOI4D dataset and the outdoor SemanticKITTI and
nuScenes datasets. Comprehensive experiments demonstrate the effectiveness of
our approach, particularly in accurately predicting dynamic objects in complex
scenes.

</details>


### [166] [PICABench: How Far Are We from Physically Realistic Image Editing?](https://arxiv.org/abs/2510.17681)
*Yuandong Pu,Le Zhuo,Songhao Han,Jinbo Xing,Kaiwen Zhu,Shuo Cao,Bin Fu,Si Liu,Hongsheng Li,Yu Qiao,Wenlong Zhang,Xi Chen,Yihao Liu*

Main category: cs.CV

TL;DR: 本文针对当前图像编辑任务中缺乏对物理真实感（如阴影、反射、交互等效果）的关注，提出了系统性的评测基准和评测协议，并探索了从视频中学习物理规律以提升编辑真实感的方法。


<details>
  <summary>Details</summary>
Motivation: 虽然现代图像编辑模型能够很好地完成编辑指令，但实际生成的图像常常忽视了物理效果（如删除物体时应一并删除其阴影、反射等），这影响了编辑结果的真实感。当前主流模型和基准过于关注指令完成度，忽略了真实物理一致性。

Method: 作者提出了PICABench基准，从光学、力学和状态变化等八个子维度系统评测主流编辑操作的物理真实感；同时设计了基于VLM-as-a-judge和区域级人类标注、问题的PICAEval评测协议；此外通过从视频中学习物理规律并建立训练数据集PICA-100K，尝试改善现有模型的物理一致性。

Result: 通过PICABench系统评测主流图像编辑模型，发现即使最先进的模型，在物理真实感方面仍有较大提升空间，尤其在物理一致性上表现不足。

Conclusion: 作者提出的基准和方法为今后向物理一致性更高的图像编辑迈进打下了基础，未来工作可基于此进一步提升生成图像的物理真实感。

Abstract: Image editing has achieved remarkable progress recently. Modern editing
models could already follow complex instructions to manipulate the original
content. However, beyond completing the editing instructions, the accompanying
physical effects are the key to the generation realism. For example, removing
an object should also remove its shadow, reflections, and interactions with
nearby objects. Unfortunately, existing models and benchmarks mainly focus on
instruction completion but overlook these physical effects. So, at this moment,
how far are we from physically realistic image editing? To answer this, we
introduce PICABench, which systematically evaluates physical realism across
eight sub-dimension (spanning optics, mechanics, and state transitions) for
most of the common editing operations (add, remove, attribute change, etc). We
further propose the PICAEval, a reliable evaluation protocol that uses
VLM-as-a-judge with per-case, region-level human annotations and questions.
Beyond benchmarking, we also explore effective solutions by learning physics
from videos and construct a training dataset PICA-100K. After evaluating most
of the mainstream models, we observe that physical realism remains a
challenging problem with large rooms to explore. We hope that our benchmark and
proposed solutions can serve as a foundation for future work moving from naive
content editing toward physically consistent realism.

</details>


### [167] [Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model](https://arxiv.org/abs/2510.17684)
*Xinwei Zhang,Hu Chen,Zhe Yuan,Sukun Tian,Peng Feng*

Main category: cs.CV

TL;DR: 本文提出了一种名为IC-MoE的混合专家模型，有效提升医学图像分割基础模型高层特征表达能力，并保持预训练权重结构完整性，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型微调方法在医学图像分割中高层特征表达不足且易破坏预训练权重结构，影响模型性能。

Method: 提出IC-MoE模型，包括基础、语义和自适应专家，并结合像素概率自适应投票策略，实现专家选择与融合。同时引入语义引导对比学习方法强化高层特征表达。

Result: 在三个公开医学图像分割数据集上，IC-MoE模型优于当前最先进模型，表现出更高的分割精度和更好的泛化能力。

Conclusion: IC-MoE模型能有效提升医学图像分割基础模型的高层特征表达和预训练结构完整性，具有广泛适用性和优越性能。

Abstract: Foundation models for medical image segmentation have achieved remarkable
performance. Adaptive fine-tuning of natural image segmentation foundation
models is crucial for medical image segmentation tasks. However, some
limitations exist in existing fine-tuning methods: 1) insufficient
representation of high-level features and 2) the fine-tuning process disrupts
the structural integrity of pretrained weights. Inspired by these critical
problems, we propose an intelligent communication mixture-of-experts
boosted-medical image segmentation foundation model, named IC-MoE, with twofold
ideas: 1) We construct basic experts, semantic experts, and adaptive experts.
Moreover, we implement a pixel probability adaptive voting strategy, which
enables expert selection and fusion through label consistency and load
balancing. This approach preliminarily enhances the representation capability
of high-level features while preserving the structural integrity of pretrained
weights. 2) We propose a semantic-guided contrastive learning method to address
the issue of weak supervision in contrastive learning. This method further
enhances the representation capability of high-level features while preserving
the structural integrity of pretrained weights. Extensive experiments across
three public medical image segmentation datasets demonstrate that the IC-MoE
outperforms other SOTA models. Consequently, the proposed IC-MoE effectively
supplements foundational medical image segmentation models with high-level
features and pretrained structural integrity. We also validate the superior
generalizability of the IC-MoE across diverse medical image segmentation
scenarios.

</details>


### [168] [Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning](https://arxiv.org/abs/2510.17685)
*Min Cao,Xinyu Zhou,Ding Jiang,Bo Du,Mang Ye,Min Zhang*

Main category: cs.CV

TL;DR: 本文提出了多语言文本到图像人物检索任务（TIPR），为此构建了多语言TIPR基准数据集，并提出了一种新模型Bi-IRRA，取得了最新的最佳性能。


<details>
  <summary>Details</summary>
Motivation: 传统TIPR方法通常只针对英语，且现有的对齐方法要么忽视细粒度的模态差异，要么需要额外先验信息，限制了其实用性和拓展性。因此，需要一种能够自然支持多语言并更好处理跨模态关系的新方法。

Method: 1）构建多语言TIPR基准数据集，利用大语言模型初步翻译，并结合领域知识精细化校正。2）提出Bi-IRRA模型，包括双向隐式关系推理模块（能够跨语言跨模态地预测文本和图像，学习细粒度关系）和多维全局对齐模块（解决模态异构性）。

Result: 所提出方法在所有多语言TIPR数据集上取得了最新的最优结果（SOTA）。

Conclusion: Bi-IRRA在多语言语境下有效提升了TIPR任务的性能，同时数据集和代码已开源，为后续研究提供了平台和基线。

Abstract: Text-to-image person retrieval (TIPR) aims to identify the target person
using textual descriptions, facing challenge in modality heterogeneity. Prior
works have attempted to address it by developing cross-modal global or local
alignment strategies. However, global methods typically overlook fine-grained
cross-modal differences, whereas local methods require prior information to
explore explicit part alignments. Additionally, current methods are
English-centric, restricting their application in multilingual contexts. To
alleviate these issues, we pioneer a multilingual TIPR task by developing a
multilingual TIPR benchmark, for which we leverage large language models for
initial translations and refine them by integrating domain-specific knowledge.
Correspondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation
Reasoning and Aligning framework to learn alignment across languages and
modalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module
enables bidirectional prediction of masked image and text, implicitly enhancing
the modeling of local relations across languages and modalities, a
multi-dimensional global alignment module is integrated to bridge the modality
heterogeneity. The proposed method achieves new state-of-the-art results on all
multilingual TIPR datasets. Data and code are presented in
https://github.com/Flame-Chasers/Bi-IRRA.

</details>


### [169] [Towards 3D Objectness Learning in an Open World](https://arxiv.org/abs/2510.17686)
*Taichi Liu,Zhenyu Wang,Ruofeng Liu,Guang Wang,Desheng Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种无需手工提示、可在开放世界场景下检测所有三维物体的新方法OP3Det，通过融合2D和3D模型优势，极大提升了未知类别目标的检测能力，在开集和闭集基线上均有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前三维目标检测和新类别检测技术虽有进展，但对泛化三维物体性（objectness）的学习仍显不足。尤其是传统闭集方法难以应对开放世界的未知物体检测，而直接用开放词汇模型在三维场景下推广亦存在语义混淆和词汇扩展困难。因此，亟需一种泛化力强、无需依赖手动文本提示的三维目标检测思路。

Method: 提出OP3Det，一种类别无关、开放世界、无需prompt的三维检测器。方法核心在于引入2D大模型的泛化和零样本能力，将2D语义先验与3D几何先验结合，实现类别无关的目标提议，并在点云与RGB图的多模态专家模块中，动态融合单模和多模特征，从而学习泛化三维物体性。

Result: OP3Det在多项实验中展现卓越性能，分别在开放世界三维检测任务上比现有方法提升了16.0%的AR指标，较传统闭集检测器提升了13.5%。

Conclusion: OP3Det极大推动了三维物体检测在开放世界泛化、“零样本”新目标发现上的进步，为无需手工提示和未知类别三维物体检测提供了有效解决方案。

Abstract: Recent advancements in 3D object detection and novel category detection have
made significant progress, yet research on learning generalized 3D objectness
remains insufficient. In this paper, we delve into learning open-world 3D
objectness, which focuses on detecting all objects in a 3D scene, including
novel objects unseen during training. Traditional closed-set 3D detectors
struggle to generalize to open-world scenarios, while directly incorporating 3D
open-vocabulary models for open-world ability struggles with vocabulary
expansion and semantic overlap. To achieve generalized 3D object discovery, We
propose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detect
any objects within 3D scenes without relying on hand-crafted text prompts. We
introduce the strong generalization and zero-shot capabilities of 2D foundation
models, utilizing both 2D semantic priors and 3D geometric priors for
class-agnostic proposals to broaden 3D object discovery. Then, by integrating
complementary information from point cloud and RGB image in the cross-modal
mixture of experts, OP3Det dynamically routes uni-modal and multi-modal
features to learn generalized 3D objectness. Extensive experiments demonstrate
the extraordinary performance of OP3Det, which significantly surpasses existing
open-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement
compared to closed-world 3D detectors.

</details>


### [170] [GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver](https://arxiv.org/abs/2510.17699)
*Aleksandr Oganov,Ilya Bykov,Eva Neudachina,Mishan Aliev,Alexander Tolmachev,Alexander Sidorov,Aleksandr Zuev,Andrey Okhotin,Denis Rakitin,Aibek Alanov*

Main category: cs.CV

TL;DR: 扩散模型在生成质量上表现突出，但采样耗时高。本文提出了一种更简单高效的ODE采样器参数化方法，并结合对抗训练提升细节质量，方法无需复杂训练技巧，效果优于现有技术。代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型虽然生成效果好，但采样速度较慢。此前的快速采样技术往往依赖复杂训练或难以保留细节。作者希望改进采样效率的同时，简化训练流程并提升生成细节质量。

Method: 提出Generalized Solver，一种无需特殊训练技巧的ODE采样器新参数化方法，并将蒸馏损失与对抗训练结合，产生Generalized Adversarial Solver，有效提升细节保真度和整体质量。

Result: 与现有采样器训练方法对比，在同等资源条件下，GAS方法生成质量更优，细节还原更好，伪影更少。

Conclusion: 该方法在不增加训练复杂度的情况下，能显著提升扩散模型生成速度和质量，特别是在细节保持和减少伪影方面表现突出。

Abstract: While diffusion models achieve state-of-the-art generation quality, they
still suffer from computationally expensive sampling. Recent works address this
issue with gradient-based optimization methods that distill a few-step ODE
diffusion solver from the full sampling process, reducing the number of
function evaluations from dozens to just a few. However, these approaches often
rely on intricate training techniques and do not explicitly focus on preserving
fine-grained details. In this paper, we introduce the Generalized Solver: a
simple parameterization of the ODE sampler that does not require additional
training tricks and improves quality over existing approaches. We further
combine the original distillation loss with adversarial training, which
mitigates artifacts and enhances detail fidelity. We call the resulting method
the Generalized Adversarial Solver and demonstrate its superior performance
compared to existing solver training methods under similar resource
constraints. Code is available at https://github.com/3145tttt/GAS.

</details>


### [171] [Elastic ViTs from Pretrained Models without Retraining](https://arxiv.org/abs/2510.17700)
*Walter Simoncini,Michael Dorkenwald,Tijmen Blankevoort,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CV

TL;DR: 本文提出了一种名为SnapViT的视觉Transformer剪枝方法，无需重新训练或标签信息，即可快速生成针对任意计算预算可调的高效模型。该方法在多个主流模型和剪枝率下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型仅提供有限的预设规模，难以满足实际部署中的多样计算资源需求，存在性能与资源利用率不匹配的问题。需要一种支持弹性推理、无需繁复微调的新方法。

Method: 提出了一种新的结构化剪枝策略，通过结合梯度信息与跨网络结构相关性（利用进化算法近似Hessian矩阵的非对角结构），不需要有标签数据、无需分类头，且实现快速一键剪枝。还引入了自监督重要性评分机制。

Result: 在DINO、SigLIPv2、DeIT、AugReg等多种视觉Transformer模型上实验，SnapViT在各种剪枝稀疏率下均能超越现有剪枝方法。同时在一块A100 GPU上不到5分钟就能完成弹性模型的生成。

Conclusion: SnapViT实现了高效、通用的剪枝，支持视觉Transformer模型的弹性调整，显著提升了模型在实际部署中的灵活性，无需重训练或标签即可获得优良的推理性能。

Abstract: Vision foundation models achieve remarkable performance but are only
available in a limited set of pre-determined sizes, forcing sub-optimal
deployment choices under real-world constraints. We introduce SnapViT:
Single-shot network approximation for pruned Vision Transformers, a new
post-pretraining structured pruning method that enables elastic inference
across a continuum of compute budgets. Our approach efficiently combines
gradient information with cross-network structure correlations, approximated
via an evolutionary algorithm, does not require labeled data, generalizes to
models without a classification head, and is retraining-free. Experiments on
DINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over
state-of-the-art methods across various sparsities, requiring less than five
minutes on a single A100 GPU to generate elastic models that can be adjusted to
any computational budget. Our key contributions include an efficient pruning
strategy for pretrained Vision Transformers, a novel evolutionary approximation
of Hessian off-diagonal structures, and a self-supervised importance scoring
mechanism that maintains strong performance without requiring retraining or
labels. Code and pruned models are available at: https://elastic.ashita.nl/

</details>


### [172] [Improving Cross-Patient Generalization in Parkinson's Disease Detection through Chunk-Based Analysis of Hand-Drawn Patterns](https://arxiv.org/abs/2510.17703)
*Mhd Adnan Albani,Riad Sonbol*

Main category: cs.CV

TL;DR: 本文提出了一种基于手绘图像的新方法检测帕金森病，通过分块特征提取和集成分类器提升了对新患者的泛化效果。


<details>
  <summary>Details</summary>
Motivation: 现有利用手绘图像检测帕金森病的方法存在数据集不足及泛化能力弱的主要问题。

Method: 作者采用双阶段方法：首先分类图像类型（圆形、曲折、螺旋），然后对每张图像进行2x2分块，每块分别提取特征并单独判别，再通过集成方法融合每块判别结果进行最终诊断。

Result: 新方法在NewHandPD数据集上性能优异，对已知患者准确率为97.08%，对未知患者为94.91%，准确率下降仅2.17个百分点，显著优于前人方法。

Conclusion: 所提出的分块+集成检测方法提升了对未知患者的鲁棒性（泛化能力），为基于手绘图像的帕金森病早期识别提供了更有效的技术路线。

Abstract: Parkinson's disease (PD) is a neurodegenerative disease affecting about 1% of
people over the age of 60, causing motor impairments that impede hand
coordination activities such as writing and drawing. Many approaches have tried
to support early detection of Parkinson's disease based on hand-drawn images;
however, we identified two major limitations in the related works: (1) the lack
of sufficient datasets, (2) the robustness when dealing with unseen patient
data. In this paper, we propose a new approach to detect Parkinson's disease
that consists of two stages: The first stage classifies based on their drawing
type(circle, meander, spiral), and the second stage extracts the required
features from the images and detects Parkinson's disease. We overcame the
previous two limitations by applying a chunking strategy where we divide each
image into 2x2 chunks. Each chunk is processed separately when extracting
features and recognizing Parkinson's disease indicators. To make the final
classification, an ensemble method is used to merge the decisions made from
each chunk. Our evaluation shows that our proposed approach outperforms the top
performing state-of-the-art approaches, in particular on unseen patients. On
the NewHandPD dataset our approach, it achieved 97.08% accuracy for seen
patients and 94.91% for unseen patients, our proposed approach maintained a gap
of only 2.17 percentage points, compared to the 4.76-point drop observed in
prior work.

</details>


### [173] [Automatic Classification of Circulating Blood Cell Clusters based on Multi-channel Flow Cytometry Imaging](https://arxiv.org/abs/2510.17716)
*Suqiang Ma,Subhadeep Sengupta,Yao Lee,Beikang Gu,Xianyan Chen,Xianqiao Wang,Yang Liu,Mengjia Xu,Galit H. Frydman,He Li*

Main category: cs.CV

TL;DR: 本研究提出了一种用于自动分析循环血液细胞簇（CCCs）图像的新型计算框架，实现了对细胞簇和具体细胞类型的高精度识别，并在血细胞图像数据上取得了超过95%的准确率。


<details>
  <summary>Details</summary>
Motivation: 循环血液细胞簇作为多种疾病重要生物标志物，需通过流式细胞术成像分析，但现有自动化工具主要聚焦单细胞，对异质性细胞簇自动分析方法匮乏。因此亟需开发能精准处理多种细胞类型簇的自动化分析工具。

Method: 提出了两步分析策略：第一步利用改进的YOLOv11深度学习模型，将图像分类为细胞簇与非细胞簇，优于传统CNN和ViT模型；第二步将细胞簇轮廓与多通道荧光染色区域叠加，精确识别细胞簇内各类细胞类型，有效缓解染色碎片和伪影影响。

Result: 所提方法在细胞簇分类和细胞类型表型识别两方面均获得了超95%的准确率，显示出算法的高效与稳健性。

Conclusion: 该自动化分析框架可高效分析流式细胞术获取的CCCs图像，首次实现对含多类细胞簇的精准识别，具备向多种疾病相关的免疫或肿瘤细胞簇分析扩展的潜力，将为相关细胞生物医学研究提供有力工具。

Abstract: Circulating blood cell clusters (CCCs) containing red blood cells (RBCs),
white blood cells(WBCs), and platelets are significant biomarkers linked to
conditions like thrombosis, infection, and inflammation. Flow cytometry, paired
with fluorescence staining, is commonly used to analyze these cell clusters,
revealing cell morphology and protein profiles. While computational approaches
based on machine learning have advanced the automatic analysis of single-cell
flow cytometry images, there is a lack of effort to build tools to
automatically analyze images containing CCCs. Unlike single cells, cell
clusters often exhibit irregular shapes and sizes. In addition, these cell
clusters often consist of heterogeneous cell types, which require multi-channel
staining to identify the specific cell types within the clusters. This study
introduces a new computational framework for analyzing CCC images and
identifying cell types within clusters. Our framework uses a two-step analysis
strategy. First, it categorizes images into cell cluster and non-cluster groups
by fine-tuning the You Only Look Once(YOLOv11) model, which outperforms
traditional convolutional neural networks (CNNs), Vision Transformers (ViT).
Then, it identifies cell types by overlaying cluster contours with regions from
multi-channel fluorescence stains, enhancing accuracy despite cell debris and
staining artifacts. This approach achieved over 95% accuracy in both cluster
classification and phenotype identification. In summary, our automated
framework effectively analyzes CCC images from flow cytometry, leveraging both
bright-field and fluorescence data. Initially tested on blood cells, it holds
potential for broader applications, such as analyzing immune and tumor cell
clusters, supporting cellular research across various diseases.

</details>


### [174] [Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop Conditions](https://arxiv.org/abs/2510.17719)
*Zhiqiang Teng,Beibei Lin,Tingting Chen,Zifeng Yuan,Xuanyi Li,Xuanyu Zhang,Shunli Zhang*

Main category: cs.CV

TL;DR: 本文关注于3D高斯溅射（3DGS）在雨滴污染镜头情况下的性能下降，并提出了RaindropGS基准用于评测完整3DGS流程下受真实雨滴干扰的重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS基准多以合成雨滴图像和已知相机位姿为主，忽略了真实雨滴对相机位姿估计、点云初始化及泛化能力的影响，亟需一个能真实反映雨滴干扰下3DGS性能的完整评测体系。

Method: 构建了RaindropGS基准，涵盖数据准备、处理与新颖的雨滴感知3DGS评估流程，包括多种雨滴干扰类型、相机位姿估计、点云初始化、图像去雨和Gauss训练等比较。同时采集了对齐的实拍数据集，三种对焦（雨滴、背景、无雨）支持全面评测。

Result: 实验证明，现有3DGS方法在无约束雨滴图片下性能大幅下降。不同对焦方式对重建影响显著，且位姿和点云初始化误差造成重构质量进一步退化。

Conclusion: 该基准揭示了现有方法的瓶颈，为未来更鲁棒的雨天3DGS重建方法提供了方向和目标。

Abstract: 3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe
occlusions and optical distortions caused by raindrop contamination on the
camera lens, substantially degrading reconstruction quality. Existing
benchmarks typically evaluate 3DGS using synthetic raindrop images with known
camera poses (constrained images), assuming ideal conditions. However, in
real-world scenarios, raindrops often interfere with accurate camera pose
estimation and point cloud initialization. Moreover, a significant domain gap
between synthetic and real raindrops further impairs generalization. To tackle
these issues, we introduce RaindropGS, a comprehensive benchmark designed to
evaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images
to clear 3DGS reconstructions. Specifically, the whole benchmark pipeline
consists of three parts: data preparation, data processing, and raindrop-aware
3DGS evaluation, including types of raindrop interference, camera pose
estimation and point cloud initialization, single image rain removal
comparison, and 3D Gaussian training comparison. First, we collect a real-world
raindrop reconstruction dataset, in which each scene contains three aligned
image sets: raindrop-focused, background-focused, and rain-free ground truth,
enabling a comprehensive evaluation of reconstruction quality under different
focus conditions. Through comprehensive experiments and analyses, we reveal
critical insights into the performance limitations of existing 3DGS methods on
unconstrained raindrop images and the varying impact of different pipeline
components: the impact of camera focus position on 3DGS reconstruction
performance, and the interference caused by inaccurate pose and point cloud
initialization on reconstruction. These insights establish clear directions for
developing more robust 3DGS methods under raindrop conditions.

</details>


### [175] [MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues](https://arxiv.org/abs/2510.17722)
*Yaning Pan,Zekun Wang,Qianqian Xie,Yongqian Wen,Yuanxing Zhang,Guohui Zhang,Haoxuan Hu,Zhiyu Pan,Yibing Huang,Zhidong Gan,Yonghong Lin,An Ping,Tianhao Peng,Jiaheng Liu*

Main category: cs.CV

TL;DR: 本文提出了MT-Video-Bench，这是一个专为多轮对话场景设计的视频理解基准，用以评测多模态大模型在实际复杂环境中的能力。评测显示主流模型在多轮视频对话中的表现差异明显，揭示了当前模型的局限性。该基准将公开以促进相关研究。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型评测主要集中在单轮问答，忽视了真实场景中普遍存在的多轮对话需求。本研究旨在填补这一评测空白，更真实地反映模型在视频多轮理解中的能力。

Method: 作者构建了MT-Video-Bench基准，包含987条跨多个领域、精心设计的多轮对话，重点考察感知力和交互能力等六项核心能力，并对多种开源与闭源主流多模态大模型进行系统评测。

Result: 实验结果表明，不同的大模型在多轮视频对话环境下的表现存在显著差异，且均存在不足，无法完全胜任真实世界的复杂交互需求。

Conclusion: MT-Video-Bench有效揭示了主流多模态大模型在多轮视频对话理解上的局限，为未来相关领域研究和模型改进提供了重要工具和参考。

Abstract: The recent development of Multimodal Large Language Models (MLLMs) has
significantly advanced AI's ability to understand visual modalities. However,
existing evaluation benchmarks remain limited to single-turn question
answering, overlooking the complexity of multi-turn dialogues in real-world
scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video
understanding benchmark for evaluating MLLMs in multi-turn dialogues.
Specifically, our MT-Video-Bench mainly assesses six core competencies that
focus on perceptivity and interactivity, encompassing 987 meticulously curated
multi-turn dialogues from diverse domains. These capabilities are rigorously
aligned with real-world applications, such as interactive sports analysis and
multi-turn video-based intelligent tutoring. With MT-Video-Bench, we
extensively evaluate various state-of-the-art open-source and closed-source
MLLMs, revealing their significant performance discrepancies and limitations in
handling multi-turn video dialogues. The benchmark will be publicly available
to foster future research.

</details>


### [176] [Signature Forgery Detection: Improving Cross-Dataset Generalization](https://arxiv.org/abs/2510.17724)
*Matheus Ramos Parracho*

Main category: cs.CV

TL;DR: 本论文探讨了提升离线签名验证中跨数据集泛化能力的方法，并对比了基于原始图像和壳预处理两种特征学习策略。结果显示，基于原始图像的方法在现有基准上表现更佳，但壳预处理显示出潜在提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有离线签名验证方法，尤其是深度学习方法，在数据集间泛化能力较弱，主要因不同书写风格和采集协议带来的差异影响了模型表现。因此，急需提升跨数据集的鲁棒性以适用于实际应用。

Method: 作者基于三个公开基准数据集（CEDAR、ICDAR、GPDS Synthetic）设计了两套实验流程：一套直接用原始签名图像训练模型，另一套将图像应用“壳预处理”后训练模型，对比分析两种特征学习策略在跨数据集情境的表现。

Result: 分析显示，基于原始图像的模型在所有基准测试中整体表现更优；壳预处理模型虽未超过原始图像，但表现有进一步改进的潜力。

Conclusion: 针对签名伪造检测，原始图像策略目前更优但壳预处理方法值得进一步研究，未来或有助于实现更强的跨域鲁棒性签名验证。

Abstract: Automated signature verification is a critical biometric technique used in
banking, identity authentication, and legal documentation. Despite the notable
progress achieved by deep learning methods, most approaches in offline
signature verification still struggle to generalize across datasets, as
variations in handwriting styles and acquisition protocols often degrade
performance. This study investigates feature learning strategies for signature
forgery detection, focusing on improving cross-dataset generalization -- that
is, model robustness when trained on one dataset and tested on another. Using
three public benchmarks -- CEDAR, ICDAR, and GPDS Synthetic -- two experimental
pipelines were developed: one based on raw signature images and another
employing a preprocessing method referred to as shell preprocessing. Several
behavioral patterns were identified and analyzed; however, no definitive
superiority between the two approaches was established. The results show that
the raw-image model achieved higher performance across benchmarks, while the
shell-based model demonstrated promising potential for future refinement toward
robust, cross-domain signature verification.

</details>


### [177] [Can Image-To-Video Models Simulate Pedestrian Dynamics?](https://arxiv.org/abs/2510.17731)
*Aaron Appelle,Jerome P. Lynch*

Main category: cs.CV

TL;DR: 本文探索了基于Diffusion Transformer（DiT）变体的图像到视频（I2V）模型，在模拟真实公共场景下人群行人运动模式中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有I2V模型在大规模视频数据集上展现出良好的世界建模能力，但其在生成真实复杂行人轨迹上的能力未被充分评估。作者希望验证这些强大模型除视觉生成外，是否同样能真实、合理地模拟人群动态。

Method: 作者设计了一个框架，利用行人轨迹基准中的关键帧作为条件输入到I2V模型，并用定量的行人动力学指标对生成结果进行评估，考察其在行人轨迹预测任务上的表现。

Result: 通过在行人轨迹基准上的实验，评估了I2V模型生成的人群运动与真实运动之间的匹配度与动态一致性（具体实验数据和数值在原文中提供）。

Conclusion: I2V模型在训练得当和合理条件输入下，能够在某种程度上生成现实可信的复杂行人动态，为扩展这类模型在动态场景建模、轨迹预测等实际应用中提供了可能性。

Abstract: Recent high-performing image-to-video (I2V) models based on variants of the
diffusion transformer (DiT) have displayed remarkable inherent world-modeling
capabilities by virtue of training on large scale video datasets. We
investigate whether these models can generate realistic pedestrian movement
patterns in crowded public scenes. Our framework conditions I2V models on
keyframes extracted from pedestrian trajectory benchmarks, then evaluates their
trajectory prediction performance using quantitative measures of pedestrian
dynamics.

</details>


### [178] [Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition](https://arxiv.org/abs/2510.17739)
*Timur Ismagilov,Shakaiba Majeed,Michael Milford,Tan Viet Tuyen Nguyen,Sarvapali D. Ramchurn,Shoaib Ehsan*

Main category: cs.CV

TL;DR: 本文提出了一种多参考视觉位置识别（VPR）新方法，通过矩阵分解与投影残差匹配，无需训练，在多样环境下大幅提升定位回召率，并提出了多视角VPR基准SotonMV。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习手段虽然能提升VPR的鲁棒性，但依赖于高昂的数据多样化和模型复杂度，导致训练和部署计算成本巨大。现有基于描述子的融合方法（如通过投票或聚合）虽能避免训练，但多针对多传感器环境或者缺乏通用性，在外观和视角变换下提升有限。因此，亟需一种无需训练、适用多种描述子的多参考高效VPR方法。

Method: 方法核心为利用多参考图像的描述子，通过矩阵分解（提取基础表示）实现场所的联合建模，然后采用基于投影残差的匹配策略进行定位。该方法与VPR描述子无关且无需深度训练。此外，作者还提出了新的数据集SotonMV，用于多视角VPR评测。

Result: 在多外观数据集上，该方法相比单参考提升Recall@1最高18%；在不同外观和视角变化下，也优于主流多参考基线方法，在无结构数据集上提升约5%。方法表现出很强的泛化能力且计算代价低。

Conclusion: 作者的方法既提高了在多变化环境下的VPR性能，又保持了轻量高效，无需训练和复杂模型，对学术界和工程应用均具实用价值。

Abstract: We address multi-reference visual place recognition (VPR), where reference
sets captured under varying conditions are used to improve localisation
performance. While deep learning with large-scale training improves robustness,
increasing data diversity and model complexity incur extensive computational
cost during training and deployment. Descriptor-level fusion via voting or
aggregation avoids training, but often targets multi-sensor setups or relies on
heuristics with limited gains under appearance and viewpoint change. We propose
a training-free, descriptor-agnostic approach that jointly models places using
multiple reference descriptors via matrix decomposition into basis
representations, enabling projection-based residual matching. We also introduce
SotonMV, a structured benchmark for multi-viewpoint VPR. On multi-appearance
data, our method improves Recall@1 by up to ~18% over single-reference and
outperforms multi-reference baselines across appearance and viewpoint changes,
with gains of ~5% on unstructured data, demonstrating strong generalisation
while remaining lightweight.

</details>


### [179] [Towards Explainable Skin Cancer Classification: A Dual-Network Attention Model with Lesion Segmentation and Clinical Metadata Fusion](https://arxiv.org/abs/2510.17773)
*Md. Enamul Atiq,Shaikh Anowarul Fattah*

Main category: cs.CV

TL;DR: 该论文提出了一种融合分割病灶与临床元数据的双编码器注意力机制框架，有效提高皮肤癌分类模型的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌早期检测至关重要，但从皮肤镜图像自动诊断存在类别内变异大、类别间差异小等难题，而且许多深度学习模型缺乏可解释性，影响临床信任。作者旨在通过结合多模态信息和可解释机制，提升分类效果并增强模型可靠性。

Method: 作者设计了两阶段方法：首先利用包含双重注意门（DAG）和空洞空间金字塔池化（ASPP）的深度UNet对皮损精准分割；再通过双路DenseNet201编码器，分别提取原图与分割病灶特征，并借助多头交叉注意机制融合。此外，模型还引入transformer模块，将患者元数据（年龄、性别、病变部位）纳入预测流程。最后，采用Grad-CAM生热图检验模型关注区域。

Result: 在HAM10000和ISIC 2018/2019数据集上，所提方法取得了最优分割表现，分类准确率和AUC均显著优于基线模型。热图显示模型决策集中于病灶区而非背景。

Conclusion: 精确的病灶分割与临床数据结合，并通过注意力机制融合，能同时提升皮肤癌智能诊断的精度与可解释性，有益于实际临床应用。

Abstract: Skin cancer is a life-threatening disease where early detection significantly
improves patient outcomes. Automated diagnosis from dermoscopic images is
challenging due to high intra-class variability and subtle inter-class
differences. Many deep learning models operate as "black boxes," limiting
clinical trust. In this work, we propose a dual-encoder attention-based
framework that leverages both segmented lesions and clinical metadata to
enhance skin lesion classification in terms of both accuracy and
interpretability. A novel Deep-UNet architecture with Dual Attention Gates
(DAG) and Atrous Spatial Pyramid Pooling (ASPP) is first employed to segment
lesions. The classification stage uses two DenseNet201 encoders-one on the
original image and another on the segmented lesion whose features are fused via
multi-head cross-attention. This dual-input design guides the model to focus on
salient pathological regions. In addition, a transformer-based module
incorporates patient metadata (age, sex, lesion site) into the prediction. We
evaluate our approach on the HAM10000 dataset and the ISIC 2018 and 2019
challenges. The proposed method achieves state-of-the-art segmentation
performance and significantly improves classification accuracy and average AUC
compared to baseline models. To validate our model's reliability, we use
Gradient-weighted Class Activation Mapping (Grad-CAM) to generate heatmaps.
These visualizations confirm that our model's predictions are based on the
lesion area, unlike models that rely on spurious background features. These
results demonstrate that integrating precise lesion segmentation and clinical
data with attention-based fusion leads to a more accurate and interpretable
skin cancer classification model.

</details>


### [180] [SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference](https://arxiv.org/abs/2510.17777)
*Samir Khaki,Junxian Guo,Jiaming Tang,Shang Yang,Yukang Chen,Konstantinos N. Plataniotis,Yao Lu,Song Han,Zhijian Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉语言模型（VLM）高效推理范式SparseVILA，通过在预填充和解码阶段分离处理视觉稀疏性，大幅提升推理速度且保持推理质量。


<details>
  <summary>Details</summary>
Motivation: 现有VLM虽然在多模态任务上取得巨大进展，但推理时视觉token数量较多，导致计算开销高、速度慢，影响模型在高分辨率、多轮会话等实际场景下的扩展性。

Method: SparseVILA方法在推理的预填充阶段通过剪枝去除冗余视觉token，在解码阶段则仅检索与当前query相关的token，将稀疏性巧妙分布于两个阶段，且大部分视觉缓存得以保留，以便支持多轮对话；整个方法无需对模型重新训练，对现有架构通用。

Result: SparseVILA在长时视频任务中的prefill速度提升可达4倍，解码提升2.5倍，总体端到端加速2.6倍，在文档理解和推理任务上还能提升准确率。

Conclusion: 分离式稀疏性处理为高效多模态推理提供了新思路，SparseVILA在无需训练、架构无关的情况下兼具推理加速和能力提升，为大模型实际落地提供突破。

Abstract: Vision Language Models (VLMs) have rapidly advanced in integrating visual and
textual reasoning, powering applications across high-resolution image
understanding, long-video analysis, and multi-turn conversation. However, their
scalability remains limited by the growing number of visual tokens that
dominate inference latency. We present SparseVILA, a new paradigm for efficient
VLM inference that decouples visual sparsity across the prefilling and decoding
stages. SparseVILA distributes sparsity across stages by pruning redundant
visual tokens during prefill and retrieving only query-relevant tokens during
decoding. This decoupled design matches leading prefill pruning methods while
preserving multi-turn fidelity by retaining most of the visual cache so that
query-aware tokens can be retrieved at each conversation round. Built on an
AWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster
prefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end
speedup on long-context video tasks -- while improving accuracy on
document-understanding and reasoning tasks. By decoupling query-agnostic
pruning and query-aware retrieval, SparseVILA establishes a new direction for
efficient multimodal inference, offering a training-free, architecture-agnostic
framework for accelerating large VLMs without sacrificing capability.

</details>


### [181] [ConsistEdit: Highly Consistent and Precise Training-free Visual Editing](https://arxiv.org/abs/2510.17803)
*Zixin Yin,Ling-Hao Chen,Lionel Ni,Xili Dai*

Main category: cs.CV

TL;DR: 本文提出了一种针对MM-DiT生成模型的全新注意力控制方法ConsistEdit，能够在提高编辑强度的同时保持对源内容的高度一致性，尤其适用于多轮和视频编辑任务，且实现了更精细的属性编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的训练自由注意力控制方法在提升编辑能力和保持内容一致性之间存在取舍，尤其是在多轮编辑或视频编辑中容易出现累积错误。此外，大多数方法在全局上一致，难以做到精细的属性区分编辑。近期MM-DiT架构的发展为解决这些问题带来了新可能。

Method: 在深入分析MM-DiT注意力机制的基础上，作者提出ConsistEdit方法，包含三大技术创新：1）仅针对视觉的注意力控制，2）掩膜引导的预注意力融合，3）对query、key、value进行差异化处理。同时，该方法能在所有推理步骤和注意力层进行编辑，无需手动干预。

Result: 大量实验结果表明，ConsistEdit在多种图像和视频编辑任务（包括结构一致和非一致场景）中均实现了SOTA（最优）的编辑性能。同时展现出极高的可靠性和一致性，支持多轮、多区域和逐步结构一致性调控。

Conclusion: ConsistEdit方法显著提升了MM-DiT架构下生成模型的编辑能力，实现了更强、更细致、更一致的图像与视频编辑，对多轮、复杂编辑需求具有突破性意义。

Abstract: Recent advances in training-free attention control methods have enabled
flexible and efficient text-guided editing capabilities for existing generation
models. However, current approaches struggle to simultaneously deliver strong
editing strength while preserving consistency with the source. This limitation
becomes particularly critical in multi-round and video editing, where visual
errors can accumulate over time. Moreover, most existing methods enforce global
consistency, which limits their ability to modify individual attributes such as
texture while preserving others, thereby hindering fine-grained editing.
Recently, the architectural shift from U-Net to MM-DiT has brought significant
improvements in generative performance and introduced a novel mechanism for
integrating text and vision modalities. These advancements pave the way for
overcoming challenges that previous methods failed to resolve. Through an
in-depth analysis of MM-DiT, we identify three key insights into its attention
mechanisms. Building on these, we propose ConsistEdit, a novel attention
control method specifically tailored for MM-DiT. ConsistEdit incorporates
vision-only attention control, mask-guided pre-attention fusion, and
differentiated manipulation of the query, key, and value tokens to produce
consistent, prompt-aligned edits. Extensive experiments demonstrate that
ConsistEdit achieves state-of-the-art performance across a wide range of image
and video editing tasks, including both structure-consistent and
structure-inconsistent scenarios. Unlike prior methods, it is the first
approach to perform editing across all inference steps and attention layers
without handcraft, significantly enhancing reliability and consistency, which
enables robust multi-round and multi-region editing. Furthermore, it supports
progressive adjustment of structural consistency, enabling finer control.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [182] [Quantum NLP models on Natural Language Inference](https://arxiv.org/abs/2510.15972)
*Ling Sun,Peter Sullivan,Michael Martin,Yun Zhou*

Main category: cs.CL

TL;DR: 本文探讨了利用量子自然语言处理（QNLP）模型进行自然语言推理（NLI），并将其与经典与混合变换器模型在有限样本条件下进行比较。结果显示量子模型在参数更少的情况下表现接近甚至优于经典模型，且每参数信息增益效率显著更高。还提出了基于聚类的参数共享架构以提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前自然语言处理模型（如变换器）在大规模语料和参数下性能优异，但参数量庞大且对结构信息的利用有限。QNLP 通过将组合结构嵌入量子电路，探索在低资源、结构敏感任务上的高效建模能力，有望在参数极少的场景下表现突出。

Method: 作者基于 lambeq 库和 DisCoCat 框架，为句子对构建参数化量子电路，用以训练语义相关性和推理分类任务，对比量子、混合和经典模型表现。提出信息增益/参数（IGPP）作为模型学习效率的新衡量指标。并设计了一种基于词聚类的参数共享量子电路结构以改善泛化。

Result: 量子模型在自然语言推理任务上性能与经典基线相当，但所需参数大为减少。在语义相关性任务上量子模型测试误差更低。量子模型的每参数信息增益效率远高于经典方法（可高出五个数量级）。聚类架构有助于提升泛化表现。

Conclusion: QNLP 在结构敏感和低资源情形下显示出强大潜力，能以极低参数量实现与经典大模型相近、甚至更优的推理与相关性性能。其参数学习效率极高，提出的聚类参数共享机制进一步促进了模型泛化。

Abstract: Quantum natural language processing (QNLP) offers a novel approach to
semantic modeling by embedding compositional structure directly into quantum
circuits. This paper investigates the application of QNLP models to the task of
Natural Language Inference (NLI), comparing quantum, hybrid, and classical
transformer-based models under a constrained few-shot setting. Using the lambeq
library and the DisCoCat framework, we construct parameterized quantum circuits
for sentence pairs and train them for both semantic relatedness and inference
classification. To assess efficiency, we introduce a novel
information-theoretic metric, Information Gain per Parameter (IGPP), which
quantifies learning dynamics independent of model size. Our results demonstrate
that quantum models achieve performance comparable to classical baselines while
operating with dramatically fewer parameters. The Quantum-based models
outperform randomly initialized transformers in inference and achieve lower
test error on relatedness tasks. Moreover, quantum models exhibit significantly
higher per-parameter learning efficiency (up to five orders of magnitude more
than classical counterparts), highlighting the promise of QNLP in low-resource,
structure-sensitive settings. To address circuit-level isolation and promote
parameter sharing, we also propose a novel cluster-based architecture that
improves generalization by tying gate parameters to learned word clusters
rather than individual tokens.

</details>


### [183] [Fusion-Augmented Large Language Models: Boosting Diagnostic Trustworthiness via Model Consensus](https://arxiv.org/abs/2510.16057)
*Md Kamrul Siam,Md Jobair Hossain Faruk,Jerry Q. Cheng,Huanying Gu*

Main category: cs.CL

TL;DR: 本研究提出了一种结合ChatGPT和Claude两种主流大语言模型的多模型融合框架，用于提高胸部X光片解读的准确性。在CheXpert数据集上，混合模型的共识方法在单模态和多模态场景下均提升了诊断性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在医学图像诊断中的效果有所提升，但单一模型或单模态输入常存在误诊风险。为提升AI辅助诊断的可靠性，亟需结合多模型和多模态信息以减少错误。

Method: 从CheXpert数据集中抽取234份放射科医师标注的病例，评估ChatGPT和Claude在仅依靠图像输入下的性能，并采用输出相似度共识提升准确性。此外，生成模拟临床说明，并在50例图像加文本的病例子集上评估多模态输入对表现的影响。

Result: 仅图像输入时，ChatGPT和Claude准确率分别为62.8%和76.9%，通过相似度共识法提升到77.6%。在多模态（图像+文本）输入下，ChatGPT和Claude分别达到84%和76%，共识法准确率达91.3%。共识融合法在所有设置下均优于单一模型。

Conclusion: 多模型融合与多模态信息输入能有效提升胸部X光自动解读的可靠性，能够在较低的计算成本下减少误诊，具有临床应用前景。

Abstract: This study presents a novel multi-model fusion framework leveraging two
state-of-the-art large language models (LLMs), ChatGPT and Claude, to enhance
the reliability of chest X-ray interpretation on the CheXpert dataset. From the
full CheXpert corpus of 224,316 chest radiographs, we randomly selected 234
radiologist-annotated studies to evaluate unimodal performance using image-only
prompts. In this setting, ChatGPT and Claude achieved diagnostic accuracies of
62.8% and 76.9%, respectively. A similarity-based consensus approach, using a
95% output similarity threshold, improved accuracy to 77.6%. To assess the
impact of multimodal inputs, we then generated synthetic clinical notes
following the MIMIC-CXR template and evaluated a separate subset of 50 randomly
selected cases paired with both images and synthetic text. On this multimodal
cohort, performance improved to 84% for ChatGPT and 76% for Claude, while
consensus accuracy reached 91.3%. Across both experimental conditions,
agreement-based fusion consistently outperformed individual models. These
findings highlight the utility of integrating complementary modalities and
using output-level consensus to improve the trustworthiness and clinical
utility of AI-assisted radiological diagnosis, offering a practical path to
reduce diagnostic errors with minimal computational overhead.

</details>


### [184] [Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs](https://arxiv.org/abs/2510.16062)
*Guiyao Tie,Zenghui Yuan,Zeli Zhao,Chaoran Hu,Tianhe Gu,Ruihang Zhang,Sizhe Zhang,Junran Wu,Xiaoyue Tu,Ming Jin,Qingsong Wen,Lixing Chen,Pan Zhou,Lichao Sun*

Main category: cs.CL

TL;DR: 本论文提出了CorrectBench基准，用于系统评估大语言模型（LLMs）自我纠错策略对推理能力的提升效果。实验结果显示自我纠错能提升准确率，尤其在复杂推理任务中更明显，但也会牺牲效率。不同纠错方法混合效果更佳但效率降低，且部分推理模型在引入纠错后提升有限。基线的CoT方法在准确率和效率上具有较好表现。


<details>
  <summary>Details</summary>
Motivation: 尽管已有多种自我纠错方法被提出，但关于这些方法的全面系统评估仍然不足，尚不清楚LLMs能否真正有效自我纠正。本研究旨在通过建立标准基准来定量分析不同自我纠错策略的实际效果，为相关研究提供指导。

Method: 作者构建了CorrectBench基准，围绕常识推理、数学推理与代码生成三类任务，评估自我纠错方法在固有、自外和微调三种策略下的表现，并对多种方法做组合实验，与链式思考（CoT）等基线进行对比，分析准确性与效率。

Result: 结果显示自我纠错方法能提升模型准确率，组合不同方式还能进一步改善，但会导致效率下降。有些专门用于推理的LLMs在引入额外纠错后改进有限且耗时较高。简单的CoT基线在准确率和效率间取得了较好平衡。

Conclusion: 自我纠错可提升LLMs推理表现，但效率仍是挑战。未来需重视推理能力与运行效率间的权衡优化，建议后续聚焦进一步提升这两者的平衡。

Abstract: Self-correction of large language models (LLMs) emerges as a critical
component for enhancing their reasoning performance. Although various
self-correction methods have been proposed, a comprehensive evaluation of these
methods remains largely unexplored, and the question of whether LLMs can truly
correct themselves is a matter of significant interest and concern. In this
study, we introduce CorrectBench, a benchmark developed to evaluate the
effectiveness of self-correction strategies, including intrinsic, external, and
fine-tuned approaches, across three tasks: commonsense reasoning, mathematical
reasoning, and code generation. Our findings reveal that: 1) Self-correction
methods can improve accuracy, especially for complex reasoning tasks; 2) Mixing
different self-correction strategies yields further improvements, though it
reduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited
optimization under additional self-correction methods and have high time costs.
Interestingly, a comparatively simple chain-of-thought (CoT) baseline
demonstrates competitive accuracy and efficiency. These results underscore the
potential of self-correction to enhance LLM's reasoning performance while
highlighting the ongoing challenge of improving their efficiency. Consequently,
we advocate for further research focused on optimizing the balance between
reasoning capabilities and operational efficiency. Project Page:
https://correctbench.github.io/

</details>


### [185] [EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle](https://arxiv.org/abs/2510.16079)
*Rong Wu,Xiaoman Wang,Jianbiao Mei,Pinlong Cai,Daocheng Fu,Cheng Yang,Licheng Wen,Xuemeng Yang,Yufan Shen,Yuxin Wang,Botian Shi*

Main category: cs.CL

TL;DR: EvolveR框架通过自循环学习机制，实现LLM智能体自我改进，显著提升了复杂任务的解题能力。


<details>
  <summary>Details</summary>
Motivation: 现有大模型虽能有效调用外部工具，但缺乏系统性地利用自身经验进行自我提升，无法持续优化其策略。弥补这种能力的缺失，将推动智能体更加自主和持续进步。

Method: 提出EvolveR框架，包括两个阶段：(1) 离线自我蒸馏，将智能体的互动轨迹总结为可复用的抽象策略原则；(2) 在线交互，智能体在实际任务中检索并应用这些策略，形成经验闭环，并通过策略强化机制不断优化。

Result: EvolveR在复杂多跳问答任务上与主流基线相比表现更优，展现出明显的自我提升能力。

Conclusion: EvolveR为大模型智能体自主学习和持续迭代提升提供了新思路，有助于实现更自治和不断进步的AI系统。

Abstract: Current Large Language Model (LLM) agents show strong performance in tool
use, but lack the crucial capability to systematically learn from their own
experiences. While existing frameworks mainly focus on mitigating external
knowledge gaps, they fail to address a more fundamental limitation: the
inability to iteratively refine problem-solving strategies. In this work, we
introduce EvolveR, a framework designed to enable agent to self-improve through
a complete, closed-loop experience lifecycle. This lifecycle comprises two key
stages: (1) Offline Self-Distillation, where the agent's interaction
trajectories are synthesized into a structured repository of abstract, reusable
strategic principles; (2) Online Interaction, where the agent interacts with
tasks and actively retrieves distilled principles to guide its decision-making,
accumulating a diverse set of behavioral trajectories. This loop employs a
policy reinforcement mechanism to iteratively update the agent based on its
performance. We demonstrate the effectiveness of EvolveR on complex multi-hop
question-answering benchmarks, where it achieves superior performance over
strong agentic baselines. Our work presents a comprehensive blueprint for
agents that learn not only from external data but also from the consequences of
their own actions, paving the way for more autonomous and continuously
improving systems. Code is available at https://github.com/Edaizi/EvolveR.

</details>


### [186] [Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification](https://arxiv.org/abs/2510.16091)
*Binglan Han,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

TL;DR: 本文系统评估了多种提示策略与6种主流大语言模型（LLMs）在系统性文献综述中自动筛选环节的表现，比较其准确率、召回率、F1分数和成本，为实际应用提供基准和建议。


<details>
  <summary>Details</summary>
Motivation: 自动化系统性文献综述中的文献筛选流程能极大提升研究效率，但当前LLM受模型、提示设计影响显著，缺乏系统性对比分析。本文旨在量化“模型-提示”组合的效果与成本，推动实际落地。

Method: 作者选取GPT-4o、GPT-4o-mini、DeepSeek-Chat-V3、Gemini-2.5-Flash、Claude-3.5-Haiku、Llama-4-Maverick六个LLM，采用五种主流提示（零样本、少样本、链式思维、链式思维+少样本、自我反思），并在相关性分类及六项细分任务上，用准确率、召回率、F1等指标进行详细评测，同时纳入经济成本分析。

Result: 链式思维+少样本提示在精确率与召回率间表现最好；零样本提示用于高敏感性筛查时召回率最高；自我反思提示对所有模型表现波动且过于宽松。GPT-4o和DeepSeek表现最佳，GPT-4o-mini则以显著低成本获得较强竞争力。成本分析显示模型间差异较大，结构化提示（如链式思维类）在GPT-4o-mini上性价比突出。

Conclusion: 建议文献初筛采用低成本模型+结构化提示，边际样本再用高阶模型精判。系统对比提示-模型交互，厘清LLM自动筛文的长处与不足，并为实际部署提供分步和自适应配置建议。

Abstract: This study quantifies how prompting strategies interact with large language
models (LLMs) to automate the screening stage of systematic literature reviews
(SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3,
Gemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types
(zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection)
across relevance classification and six Level-2 tasks, using accuracy,
precision, recall, and F1. Results show pronounced model-prompt interaction
effects: CoT-few-shot yields the most reliable precision-recall balance;
zero-shot maximizes recall for high-sensitivity passes; and self-reflection
underperforms due to over-inclusivity and instability across models. GPT-4o and
DeepSeek provide robust overall performance, while GPT-4o-mini performs
competitively at a substantially lower dollar cost. A cost-performance analysis
for relevance classification (per 1,000 abstracts) reveals large absolute
differences among model-prompt pairings; GPT-4o-mini remains low-cost across
prompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer
attractive F1 at a small incremental cost. We recommend a staged workflow that
(1) deploys low-cost models with structured prompts for first-pass screening
and (2) escalates only borderline cases to higher-capacity models. These
findings highlight LLMs' uneven but promising potential to automate literature
screening. By systematically analyzing prompt-model interactions, we provide a
comparative benchmark and practical guidance for task-adaptive LLM deployment.

</details>


### [187] [Facts in Stats: Impacts of Pretraining Diversity on Language Model Generalization](https://arxiv.org/abs/2510.16096)
*Tina Behnia,Puneesh Deora,Christos Thrampoulidis*

Main category: cs.CL

TL;DR: 本论文通过构建合成测试平台，系统研究了语言模型中统计与事实关联多样性对泛化能力的影响。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在学习统计流畅性与事实知识之间的复杂交互，已有研究表明这种交互的多样性会影响模型泛化能力，但缺乏系统分析。

Method: 作者设计了一种合成测试平台，将统计流（通用token序列）与事实流（source-target成对token）组合，并能独立控制多样性特点与水平。通过改变流组成和事实出现的统计流，细致操控上下文结构与多样性。通过一系列受控实验，分析了不同情境下模型的事实回忆与泛化表现。

Result: 结果发现，提高上下文多样性会延迟分布内事实记忆，但对分布外泛化效果依赖于上下文结构，多样性有时对复杂事实回忆至关重要。此外，最优多样性水平与训练时长有关，某些结构下统计或事实泛化能力会各自或共同下降。进一步识别出嵌入层与解嵌入层为限制泛化的关键瓶颈。

Conclusion: 本研究揭示了上下文设计与多样性水平如何影响语言模型泛化，并提供了一个可控的分析测试平台，对未来深入机制研究具有重要意义。

Abstract: Language models are pretrained on sequences that blend statistical
regularities (making text fluent) with factual associations between specific
tokens (knowledge of facts). While recent work suggests that the variability of
their interaction, such as paraphrases of factual associations, critically
determines generalization ability, we lack a systematic analysis of these
impacts. This paper introduces a flexible synthetic testbed that combines a
statistical stream of generic tokens with an abstract factual stream of
source-target token pairs, enabling fine-grained control over their
interaction. The design enables the independent control of diversity nature by
manipulating stream composition (contextual structure) and the diversity level
by varying which statistical streams each fact appears in. Through controlled
experiments, we find that while higher contextual diversity delays
in-distribution (ID) factual accuracy, its impact on out-of-distribution (OOD)
factual generalization depends critically on contextual structure. In some
cases, OOD performance follows the same trend as ID, but in others, diversity
becomes essential for non-trivial factual recall. Even when low diversity
prohibits factual recall, optimal diversity levels depend on training duration.
Beyond factual recall failures, we identify structures where statistical
generalization fails independently, and others where both capabilities degrade.
This shows how the interplay between contextual design and diversity level
impacts different generalization aspects. Further, through a series of
controlled interventions on the model components, we trace the OOD failures to
distinct optimization bottlenecks, highlighting the importance of the embedding
and unembedding layers. Our synthetic framework allows us to isolate effects
that would be confounded in large-scale studies, offering a controlled testbed
for future investigations.

</details>


### [188] [In Generative AI We (Dis)Trust? Computational Analysis of Trust and Distrust in Reddit Discussions](https://arxiv.org/abs/2510.16173)
*Aria Pessianzadeh,Naima Sultana,Hildegarde Van den Bulck,David Gefen,Shahin Jabari,Rezvaneh Rezapour*

Main category: cs.CL

TL;DR: 本文首次通过大规模计算方法分析了公众对生成式AI（GenAI）的信任和不信任动态，利用Reddit多年数据，揭示公众态度随AI模型发布而波动，并总结影响信任的主要原因和人群差异。


<details>
  <summary>Details</summary>
Motivation: 生成式AI广泛进入日常生活，但学界对于社会大众如何信任或不信任这类AI缺乏基于大规模、纵向数据的系统性研究。理解公众信任对于AI的负责任应用和治理至关重要。现有文献多侧重心理和人机交互视角，缺乏计算方法和广泛数据证据。

Method: 作者收集了2022至2025年Reddit平台上39个子版块、近20万帖子的讨论，采用众包标注和分类模型相结合的方式，对数据进行大规模标注和信任情感识别，系统分析信任/不信任的动态变化及其影响因素。

Result: 结果发现，公众对GenAI信任与不信任呈现几乎平衡的态势，且主要在重大模型发布节点出现波动。信任的主要维度是技术性能和可用性，个人经验是影响态度的最常见理由。不同人群（如专家、伦理学者、普通用户）在信任表达上存在明显差异。

Conclusion: 本文为AI领域提供了首个大规模信任分析计算框架，并揭示了公众对生成式AI信任态度的多样性、动态性和主要影响因素，将为后续AI治理、应用和社会影响研究提供重要参考。

Abstract: The rise of generative AI (GenAI) has impacted many aspects of human life. As
these systems become embedded in everyday practices, understanding public trust
in them also becomes essential for responsible adoption and governance. Prior
work on trust in AI has largely drawn from psychology and human-computer
interaction, but there is a lack of computational, large-scale, and
longitudinal approaches to measuring trust and distrust in GenAI and large
language models (LLMs). This paper presents the first computational study of
Trust and Distrust in GenAI, using a multi-year Reddit dataset (2022--2025)
spanning 39 subreddits and 197,618 posts. Crowd-sourced annotations of a
representative sample were combined with classification models to scale
analysis. We find that Trust and Distrust are nearly balanced over time, with
shifts around major model releases. Technical performance and usability
dominate as dimensions, while personal experience is the most frequent reason
shaping attitudes. Distinct patterns also emerge across trustors (e.g.,
experts, ethicists, general users). Our results provide a methodological
framework for large-scale Trust analysis and insights into evolving public
perceptions of GenAI.

</details>


### [189] [EgMM-Corpus: A Multimodal Vision-Language Dataset for Egyptian Culture](https://arxiv.org/abs/2510.16198)
*Mohamed Gamil,Abdelrahman Elsayed,Abdelrahman Lila,Ahmed Gad,Hesham Abdelgawad,Mohamed Aref,Ahmed Fares*

Main category: cs.CL

TL;DR: 本文介绍了专注于埃及文化的多模态数据集EgMM-Corpus，弥补了中东和非洲地区多模态文化多样性数据集的空白。通过人工标注和验证，EgMM-Corpus为评估和训练视觉-语言模型提供了可靠的基准。实验表明现有主流模型在该数据集上表现不佳，凸显文化偏见问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI领域缺乏涵盖中东和非洲等地区文化的多模态数据集，尤其是在埃及文化背景下，这限制了模型的泛化能力和文化适应性。

Method: 作者设计并运行了一套新的数据收集流程，从地标、食物和民俗等方面采集了3000多张图片，涵盖313个与埃及文化相关的概念。所有样本均经人工进行文化真实性和多模态一致性验证。

Result: EgMM-Corpus建立后，作者使用CLIP模型在此数据集上做零样本分类实验，Top-1准确率仅为21.2%，Top-5为36.4%，说明主流视觉-语言模型在埃及文化领域表现有限。

Conclusion: EgMM-Corpus为视觉-语言模型的文化适应性研究和评价提供了新基准，有助于推动开发具文化敏感性的AI系统。

Abstract: Despite recent advances in AI, multimodal culturally diverse datasets are
still limited, particularly for regions in the Middle East and Africa. In this
paper, we introduce EgMM-Corpus, a multimodal dataset dedicated to Egyptian
culture. By designing and running a new data collection pipeline, we collected
over 3,000 images, covering 313 concepts across landmarks, food, and folklore.
Each entry in the dataset is manually validated for cultural authenticity and
multimodal coherence. EgMM-Corpus aims to provide a reliable resource for
evaluating and training vision-language models in an Egyptian cultural context.
We further evaluate the zero-shot performance of Contrastive Language-Image
Pre-training CLIP on EgMM-Corpus, on which it achieves 21.2% Top-1 accuracy and
36.4% Top-5 accuracy in classification. These results underscore the existing
cultural bias in large-scale vision-language models and demonstrate the
importance of EgMM-Corpus as a benchmark for developing culturally aware
models.

</details>


### [190] [What Can String Probability Tell Us About Grammaticality?](https://arxiv.org/abs/2510.16227)
*Jennifer Hu,Ethan Gotlieb Wilcox,Siyuan Song,Kyle Mahowald,Roger P. Levy*

Main category: cs.CL

TL;DR: 本文分析了语言模型（LM）中的概率能在多大程度上反映其语法知识，并通过理论与实证相结合的方法，揭示了模型概率、语法和意义三者间的联系及局限。


<details>
  <summary>Details</summary>
Motivation: 目前关于语言模型到底掌握了多少语法知识仍有争议，这关乎语言理论的发展。然而“概率性”和“语法性”在语言学上是两个不同的概念，因此，仅由模型输出的概率难以直观地揭示其是否真正掌握了语法。

Method: 提出了描述语法、意义与概率之间关系的理论框架，并基于语料库数据生成假设。通过英文和中文共28万对最小对（语义极小差异的句子）进行实证验证，考察了：（1）最小对间字符串概率的相关性；（2）模型和人类在最小对上的概率变化差异相关性；（3）语法正确与不正确句子的概率分布分离性。

Result: 结果证实了三点：最小对之间的概率存在相关性；模型与人类在最小对的语法判断差异上表现出相关性；概率空间中语法正确与否的句子分离效果较差。

Conclusion: 理论上证实了概率可作为探析语言模型是否掌握句法结构知识的参考，但也强调了其局限性，并为未来如何更好评估语言模型的语法能力提供了理论和实证基础。

Abstract: What have language models (LMs) learned about grammar? This question remains
hotly debated, with major ramifications for linguistic theory. However, since
probability and grammaticality are distinct notions in linguistics, it is not
obvious what string probabilities can reveal about an LM's underlying
grammatical knowledge. We present a theoretical analysis of the relationship
between grammar, meaning, and string probability, based on simple assumptions
about the generative process of corpus data. Our framework makes three
predictions, which we validate empirically using 280K sentence pairs in English
and Chinese: (1) correlation between the probability of strings within minimal
pairs, i.e., string pairs with minimal semantic differences; (2) correlation
between models' and humans' deltas within minimal pairs; and (3) poor
separation in probability space between unpaired grammatical and ungrammatical
strings. Our analyses give theoretical grounding for using probability to learn
about LMs' structural knowledge, and suggest directions for future work in LM
grammatical evaluation.

</details>


### [191] [Towards Low-Resource Alignment to Diverse Perspectives with Sparse Feedback](https://arxiv.org/abs/2510.16257)
*Chu Fei Luo,Samuel Dahan,Xiaodan Zhu*

Main category: cs.CL

TL;DR: 该论文提出了提升大语言模型多元价值观对齐的两种方法，并在低资源场景下实证取得明显改善。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型对社会影响力日增，如何让模型反映多样化视角和复杂的人类价值观成为重要课题。但主流训练范式多假定每个问题都有唯一优化答案，易导致模型输出同质化、对齐效果差。为克服这一局限，作者提出了增强多元对齐的新方法。

Method: 作者提出了两种方法：（1）多元解码（pluralistic decoding）；（2）模型引导（model steering）。这两种方法设计用以在低标注数据（仅50个样本）情况下，促进模型输出更多元、贴合人类复杂价值观。

Result: 实验证明，模型引导方法相比零样本/小样本方法有更优表现。两种方法在仇恨言论检测、虚假信息检测等高风险任务中明显降低了误报率，并在GlobalOpinionQA数据集上提升了模型与人类价值分布对齐度。

Conclusion: 作者强调多样性的重要性，证明语言模型可通过方法优化来更好地反映多样化和细腻的人类价值观。

Abstract: As language models have a greater impact on society, it is important to
ensure they are aligned to a diverse range of perspectives and are able to
reflect nuance in human values. However, the most popular training paradigms
for modern language models often assume there is one optimal answer for every
query, leading to generic responses and poor alignment. In this work, we aim to
enhance pluralistic alignment of language models in a low-resource setting with
two methods: pluralistic decoding and model steering. We empirically
demonstrate that model steering offers consistent improvement over zero-shot
and few-shot baselines with only 50 annotated samples. Our proposed methods
decrease false positives in several high-stakes tasks such as hate speech
detection and misinformation detection, and improves the distributional
alignment to human values in GlobalOpinionQA. We hope our work highlights the
importance of diversity and how language models can be adapted to consider
nuanced perspectives.

</details>


### [192] [Instant Personalized Large Language Model Adaptation via Hypernetwork](https://arxiv.org/abs/2510.16282)
*Zhaoxuan Tan,Zixuan Zhang,Haoyang Wen,Zheng Li,Rongzhi Zhang,Pei Chen,Fengran Mo,Zheyuan Liu,Qingkai Zeng,Qingyu Yin,Meng Jiang*

Main category: cs.CL

TL;DR: 本文提出了一种高效可扩展的大语言模型（LLM）个性化方法Profile-to-PEFT，通过超网络将用户信息直接映射为PEFT参数，实现无需逐用户训练的个性化部署。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调（PEFT）方法需要为每个用户单独训练适配器，计算资源消耗大，不适合实时和大规模更新。如何在保证个性化效果的同时兼顾效率、扩展性与隐私成为难题。

Method: 提出Profile-to-PEFT框架，利用端到端训练的超网络，根据用户编码画像直接生成完整的适配器参数（如LoRA），在部署时无需为每个用户单独训练，支持即刻适配和广泛泛化。

Result: 实验证明，该方法比基于prompt的个性化和按用户PEFT适配在部署效率、泛化能力和资源消耗等方面均有优势，对不同用户活动水平、不同比例的未见用户以及不同嵌入模型均表现出较强鲁棒性。

Conclusion: Profile-to-PEFT为大模型个性化提供了一种高效、可扩展、隐私友好的解决方案，适合大规模实际应用场景。

Abstract: Personalized large language models (LLMs) tailor content to individual
preferences using user profiles or histories. However, existing
parameter-efficient fine-tuning (PEFT) methods, such as the
``One-PEFT-Per-User'' (OPPU) paradigm, require training a separate adapter for
each user, making them computationally expensive and impractical for real-time
updates. We introduce Profile-to-PEFT, a scalable framework that employs a
hypernetwork, trained end-to-end, to map a user's encoded profile directly to a
full set of adapter parameters (e.g., LoRA), eliminating per-user training at
deployment. This design enables instant adaptation, generalization to unseen
users, and privacy-preserving local deployment. Experimental results
demonstrate that our method outperforms both prompt-based personalization and
OPPU while using substantially fewer computational resources at deployment. The
framework exhibits strong generalization to out-of-distribution users and
maintains robustness across varying user activity levels and different
embedding backbones. The proposed Profile-to-PEFT framework enables efficient,
scalable, and adaptive LLM personalization suitable for large-scale
applications.

</details>


### [193] [Thinking About Thinking: Evaluating Reasoning in Post-Trained Language Models](https://arxiv.org/abs/2510.16340)
*Pratham Singla,Shivank Garg,Ayush Singh,Ishan Garg,Ketan Suhaas Saichandran*

Main category: cs.CL

TL;DR: 本文分析了后训练技术对大语言模型(LLM)自我认知与泛化能力的影响，比较了不同训练方法下模型在逻辑推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在逻辑密集型任务中的能力提升，人们开始关注模型是否“意识到”自己学会了什么，以及内部推理过程与输出的关系。作者试图澄清模型的自觉性、泛化能力及推理与结论之间的一致性。

Method: 作者设计了需要学习不同策略的一系列任务，评估模型在自我认知、策略跨领域泛化和推理一致性三方面的能力。比较了受监督微调（SFT）、直接策略优化（DPO）与群体相对策略优化（GRPO）三种后训练方法下的模型差异。

Result: 实验发现，通过强化学习（RL）优化的模型（如DPO、GRPO）比SFT模型表现出更强的自我认知和策略泛化能力，但在推理过程与最终输出对齐性上较弱，尤以GRPO最明显。

Conclusion: 后训练技术能够提升大语言模型的自我认知和泛化能力，但以GRPO为代表的RL方法存在推理与输出一致性不佳的问题。

Abstract: Recent advances in post-training techniques have endowed Large Language
Models (LLMs) with enhanced capabilities for tackling complex, logic-intensive
tasks through the generation of supplementary planning tokens. This development
raises a fundamental question: Are these models aware of what they "learn" and
"think"? To address this, we define three core competencies: (1) awareness of
learned latent policies, (2) generalization of these policies across domains,
and (3) alignment between internal reasoning traces and final outputs. We
empirically evaluate these abilities on several tasks, each designed to require
learning a distinct policy. Furthermore, we contrast the profiles of models
post-trained via Supervised Fine-Tuning (SFT), Direct Policy Optimization
(DPO), and Group Relative Policy Optimization (GRPO). Our findings indicate
that RL-trained models not only demonstrate greater awareness of their learned
behaviors and stronger generalizability to novel, structurally similar tasks
than SFT models but also often exhibit weak alignment between their reasoning
traces and final outputs, an effect most pronounced in GRPO-trained models.

</details>


### [194] [Utilising Large Language Models for Generating Effective Counter Arguments to Anti-Vaccine Tweets](https://arxiv.org/abs/2510.16359)
*Utsav Dhanuka,Soham Poddar,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: 本论文探讨用大语言模型（LLM）自动生成针对疫苗错误信息的反驳性论点，并通过多种方法优化和评估生成效果。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体对公众健康影响加剧，疫苗怀疑和错误信息广泛传播，成为提升疫苗接种率和维护健康信任的障碍。虽然检测假信息已有进展，但实现高质量、实时、针对性的反驳生成尚未充分探索。

Method: 作者利用LLM生成针对疫苗假信息的反驳论点，尝试不同的提示策略和微调方法以优化生成质量。此外，训练分类器对反疫苗推文按多标签（如对有效性、副作用、政治因素等担忧）分类，实现更有针对性的反驳。效果评估方法包括人工、LLM和自动指标。

Result: 多种评估方式显示各方法一致性良好。通过引入标签描述和结构化微调，反驳效果进一步提升。

Conclusion: 结合标签分类和结构化微调后的LLM可有效提升反驳疫苗错误信息的能力，是大规模减缓疫苗虚假信息传播的有希望方法。

Abstract: In an era where public health is increasingly influenced by information
shared on social media, combatting vaccine skepticism and misinformation has
become a critical societal goal. Misleading narratives around vaccination have
spread widely, creating barriers to achieving high immunisation rates and
undermining trust in health recommendations. While efforts to detect
misinformation have made significant progress, the generation of real time
counter-arguments tailored to debunk such claims remains an insufficiently
explored area. In this work, we explore the capabilities of LLMs to generate
sound counter-argument rebuttals to vaccine misinformation. Building on prior
research in misinformation debunking, we experiment with various prompting
strategies and fine-tuning approaches to optimise counter-argument generation.
Additionally, we train classifiers to categorise anti-vaccine tweets into
multi-labeled categories such as concerns about vaccine efficacy, side effects,
and political influences allowing for more context aware rebuttals. Our
evaluation, conducted through human judgment, LLM based assessments, and
automatic metrics, reveals strong alignment across these methods. Our findings
demonstrate that integrating label descriptions and structured fine-tuning
enhances counter-argument effectiveness, offering a promising approach for
mitigating vaccine misinformation at scale.

</details>


### [195] [End-to-End Argument Mining through Autoregressive Argumentative Structure Prediction](https://arxiv.org/abs/2510.16363)
*Nilmadhab Das,Vishal Vaibhav,Yash Sunil Choudhary,V. Vijaya Saradhi,Ashish Anand*

Main category: cs.CL

TL;DR: 该论文提出了AASP框架，通过自回归方式有效预测和解析复杂的论证结构，且在多个基准任务中性能优越。


<details>
  <summary>Details</summary>
Motivation: 当前论证挖掘任务面临AC与AR间依赖建模难题，现有方法常规扁平化处理结构，未能高效表达推理流程。作者旨在开发高效捕捉论证结构依赖与推理流的自动化方法。

Method: 本文提出Autoregressive Argumentative Structure Prediction (AASP) 框架，将论证结构建模为预定义动作集合，通过条件式预训练语言模型自回归生成，逐步搭建完整的论证结构。并在3个主流数据集上进行实验。

Result: AASP在两个基准任务上取得了最优表现，在另一个基准上表现也非常强劲，全面优于已有方法。

Conclusion: AASP能更有效建模复杂论证结构和推理流程，为论证挖掘任务提供了新颖且高效的方法。

Abstract: Argument Mining (AM) helps in automating the extraction of complex
argumentative structures such as Argument Components (ACs) like Premise, Claim
etc. and Argumentative Relations (ARs) like Support, Attack etc. in an
argumentative text. Due to the inherent complexity of reasoning involved with
this task, modelling dependencies between ACs and ARs is challenging. Most of
the recent approaches formulate this task through a generative paradigm by
flattening the argumentative structures. In contrast to that, this study
jointly formulates the key tasks of AM in an end-to-end fashion using
Autoregressive Argumentative Structure Prediction (AASP) framework. The
proposed AASP framework is based on the autoregressive structure prediction
framework that has given good performance for several NLP tasks. AASP framework
models the argumentative structures as constrained pre-defined sets of actions
with the help of a conditional pre-trained language model. These actions build
the argumentative structures step-by-step in an autoregressive manner to
capture the flow of argumentative reasoning in an efficient way. Extensive
experiments conducted on three standard AM benchmarks demonstrate that AASP
achieves state-of-theart (SoTA) results across all AM tasks in two benchmarks
and delivers strong results in one benchmark.

</details>


### [196] [Navigating through the hidden embedding space: steering LLMs to improve mental health assessment](https://arxiv.org/abs/2510.16373)
*Federico Ravenda,Seyed Ali Bahrainian,Andrea Raballo,Antonietta Mira*

Main category: cs.CL

TL;DR: 本文提出了一种高效低成本的方法，通过对大语言模型（LLM）中的特定层激活进行线性变换（引导向量），提升其在精神健康（MH）领域的任务表现。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在诸多领域取得进步，但规模较小的模型在特定领域（如精神健康）应用中仍表现欠佳，且现有提高性能的方法往往资源消耗大。因此，需要一种无需大量计算资源、可高效提升LLM在MH领域能力的方法。

Method: 采用轻量级的引导机制：在LLM的某一特定层，对激活值进行线性变换（利用steering vectors），引导模型在输出时更适应MH相关任务。该方法不依赖高计算量的训练或微调。

Result: 该方法使LLM在两个任务上表现显著提升：（1）判断Reddit帖子对于检测抑郁症状的相关性；（2）根据用户历史帖子自动完成标准化抑郁症自评量表。

Conclusion: 简单、经济的引导机制可以高效提升LLM在精神健康领域的适应性和表现，展现了steering vector方法在低资源场景下的潜力。

Abstract: The rapid evolution of Large Language Models (LLMs) is transforming AI,
opening new opportunities in sensitive and high-impact areas such as Mental
Health (MH). Yet, despite these advancements, recent evidence reveals that
smaller-scale models still struggle to deliver optimal performance in
domain-specific applications. In this study, we present a cost-efficient yet
powerful approach to improve MH assessment capabilities of an LLM, without
relying on any computationally intensive techniques. Our lightweight method
consists of a linear transformation applied to a specific layer's activations,
leveraging steering vectors to guide the model's output. Remarkably, this
intervention enables the model to achieve improved results across two distinct
tasks: (1) identifying whether a Reddit post is useful for detecting the
presence or absence of depressive symptoms (relevance prediction task), and (2)
completing a standardized psychological screening questionnaire for depression
based on users' Reddit post history (questionnaire completion task). Results
highlight the untapped potential of steering mechanisms as computationally
efficient tools for LLMs' MH domain adaptation.

</details>


### [197] [MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes](https://arxiv.org/abs/2510.16380)
*Yu Ying Chiu,Michael S. Lee,Rachel Calcott,Brandon Handoko,Paul de Font-Reaulx,Paula Rodriguez,Chen Bo Calvin Zhang,Ziwen Han,Udari Madhushani Sehwag,Yash Maurya,Christina Q Knight,Harry R. Lloyd,Florence Bacus,Mantas Mazeika,Bing Liu,Yejin Choi,Mitchell L Gordon,Sydney Levine*

Main category: cs.CL

TL;DR: 本文提出了MoReBench，一个专用于评估AI道德推理能力的基准数据集，涵盖1,000个道德情境及专家制定的评价标准，并分析了AI在道德推理中的表现和偏好。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越多地参与人类决策，确保AI的决策过程符合人类价值观变得至关重要。以往的评估多聚焦于有唯一答案的数学或代码问题，而道德困境则提供了多元且有争议的结论，更能测试AI推理过程的合理性和透明度。

Method: 作者构建了MoReBench基准，包括1000个道德情境及2.3万个细致的评价标准，全面检验AI在道德推理中的细节，包括识别道德考量、权衡利弊、提出建议等。此外，还设立MoReBench-Theory，聚焦AI能否在主流伦理理论下进行推理。作者对多种AI模型在该基准下的行为进行了系统性实验和分析。

Result: 实验发现，AI模型在道德推理上的表现无法用模型规模增长规律或传统推理基准结果来预测。多数模型在道德推理时偏向某些伦理框架（如功利主义和义务论），反映出当前主流训练方式的局限性与偏见。

Conclusion: MoReBench为AI道德推理的过程评估提供了丰富标准和数据，显著推进了AI伦理与决策过程的透明性和安全性研究。未来需要进一步优化训练方法，使AI在多样的道德观念中保持中立和全面。

Abstract: As AI systems progress, we rely more on them to make decisions with us and
for us. To ensure that such decisions are aligned with human values, it is
imperative for us to understand not only what decisions they make but also how
they come to those decisions. Reasoning language models, which provide both
final responses and (partially transparent) intermediate thinking traces,
present a timely opportunity to study AI procedural reasoning. Unlike math and
code problems which often have objectively correct answers, moral dilemmas are
an excellent testbed for process-focused evaluation because they allow for
multiple defensible conclusions. To do so, we present MoReBench: 1,000 moral
scenarios, each paired with a set of rubric criteria that experts consider
essential to include (or avoid) when reasoning about the scenarios. MoReBench
contains over 23 thousand criteria including identifying moral considerations,
weighing trade-offs, and giving actionable recommendations to cover cases on AI
advising humans moral decisions as well as making moral decisions autonomously.
Separately, we curate MoReBench-Theory: 150 examples to test whether AI can
reason under five major frameworks in normative ethics. Our results show that
scaling laws and existing benchmarks on math, code, and scientific reasoning
tasks fail to predict models' abilities to perform moral reasoning. Models also
show partiality towards specific moral frameworks (e.g., Benthamite Act
Utilitarianism and Kantian Deontology), which might be side effects of popular
training paradigms. Together, these benchmarks advance process-focused
reasoning evaluation towards safer and more transparent AI.

</details>


### [198] [ATA: A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents](https://arxiv.org/abs/2510.16381)
*David Peer,Sebastian Stabinger*

Main category: cs.CL

TL;DR: 本论文提出了一种新的大语言模型（LLM）应用框架——自主可信智能体（ATA），通过将任务分为离线知识摄取与在线任务处理两阶段，实现了更高的可控性、稳定性和安全性。实验证明，ATA不仅在自动化推理任务中与端到端模型竞争，基于人工验证知识库时更是显著优越。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在高风险领域中的应用受限于可解释性不足、容易产生幻觉、不稳定和不透明等问题。因此，需要新方法来提升LLM的可信度与可控性，增强其在关键领域的适用性。

Method: 作者提出了一种神经-符号混合的方法：首先，LLM将在离线阶段把非正式问题描述转化为可验证的符号知识库，经人工审核后保证其正确。随后，在实际任务处理时，将输入同样编码为符号语言，通过符号决策引擎基于知识库进行推理，从而输出结果，确保输出的可控性和透明度。

Result: 实验证明，ATA即使在完全自动流程下也能与最先进的端到端模型竞争；若知识库经过人工校验，ATA的表现显著超过更大型的模型，并且具备确定性、稳定及抗注入攻击等优势。

Conclusion: ATA以符号推理为基础，为构建下一代透明、可审计和可靠的智能体提供了可落地的架构。这一方法提升了结果的可验证性和抗风险能力，为AI应用于高风险领域提供了有力支撑。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities, yet
their deployment in high-stakes domains is hindered by inherent limitations in
trustworthiness, including hallucinations, instability, and a lack of
transparency. To address these challenges, we introduce a generic
neuro-symbolic approach, which we call Autonomous Trustworthy Agents (ATA). The
core of our approach lies in decoupling tasks into two distinct phases: Offline
knowledge ingestion and online task processing. During knowledge ingestion, an
LLM translates an informal problem specification into a formal, symbolic
knowledge base. This formal representation is crucial as it can be verified and
refined by human experts, ensuring its correctness and alignment with domain
requirements. In the subsequent task processing phase, each incoming input is
encoded into the same formal language. A symbolic decision engine then utilizes
this encoded input in conjunction with the formal knowledge base to derive a
reliable result. Through an extensive evaluation on a complex reasoning task,
we demonstrate that a concrete implementation of ATA is competitive with
state-of-the-art end-to-end reasoning models in a fully automated setup while
maintaining trustworthiness. Crucially, with a human-verified and corrected
knowledge base, our approach significantly outperforms even larger models,
while exhibiting perfect determinism, enhanced stability against input
perturbations, and inherent immunity to prompt injection attacks. By generating
decisions grounded in symbolic reasoning, ATA offers a practical and
controllable architecture for building the next generation of transparent,
auditable, and reliable autonomous agents.

</details>


### [199] [Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment](https://arxiv.org/abs/2510.16387)
*Fu-An Chao,Bi-Cheng Yan,Berlin Chen*

Main category: cs.CL

TL;DR: 本文探索了Whisper语音识别模型在二语口语评测中的潜力，通过提取其隐层表征的声学和语言特征，结合轻量级分类器，在GEPT图片描述数据集上取得了优异成绩，并优于现有多模态方法。


<details>
  <summary>Details</summary>
Motivation: 当前大多数关于Whisper模型的研究仅分析其生成的转录文本，未深入探究其隐藏表征在口语评测等语言任务中的潜在能力。本文旨在挖掘Whisper模型无需专门微调即可为二语口语评测等任务提供基础支持的可能性。

Method: 方法上，作者提取了Whisper模型中间与最终输出的声学和语言特征，并训练轻量级分类器进行口语评测。此外还融合了图片及文本提示作为额外关联信息，并对Whisper隐层嵌入进行了分析。

Result: 实验表明，该方法在GEPT图片描述数据集上表现优异，优于现有最先进基线方法（包括多模态方法）。加入图片与文本提示信息后性能进一步提升。

Conclusion: Whisper模型即使不进行任务微调，其隐层表征已能有效编码说话者的口语能力层级和语义信息，展现出作为口语评测及其他口语语言理解基础模型的巨大潜力。

Abstract: In this paper, we explore the untapped potential of Whisper, a
well-established automatic speech recognition (ASR) foundation model, in the
context of L2 spoken language assessment (SLA). Unlike prior studies that
extrinsically analyze transcriptions produced by Whisper, our approach goes a
step further to probe its latent capabilities by extracting acoustic and
linguistic features from hidden representations. With only a lightweight
classifier being trained on top of Whisper's intermediate and final outputs,
our method achieves strong performance on the GEPT picture-description dataset,
outperforming existing cutting-edge baselines, including a multimodal approach.
Furthermore, by incorporating image and text-prompt information as auxiliary
relevance cues, we demonstrate additional performance gains. Finally, we
conduct an in-depth analysis of Whisper's embeddings, which reveals that, even
without task-specific fine-tuning, the model intrinsically encodes both ordinal
proficiency patterns and semantic aspects of speech, highlighting its potential
as a powerful foundation for SLA and other spoken language understanding tasks.

</details>


### [200] [FrugalPrompt: Reducing Contextual Overhead in Large Language Models via Token Attribution](https://arxiv.org/abs/2510.16439)
*Syed Rifat Raiyan,Md Farhan Ishmam,Abdullah Al Imran,Mohammad Ali Moni*

Main category: cs.CL

TL;DR: 该论文提出了一种名为FrugalPrompt的提示压缩方法，通过保留最有语义意义的token，减少大语言模型推理所需token数量，以降低计算成本和延迟。实验证明，在情感分析、常识问答和摘要任务中，大幅压缩token数量几乎不影响性能，但在数学推理任务中则有明显性能下降。


<details>
  <summary>Details</summary>
Motivation: 大语言模型需要处理大量输入token，这导致推理成本高、能耗大且延迟增加。现有提示中存在大量冗余token，仅部分token承载大部分语义信息。因此需要一种方法，保留关键信息同时减少token数量，从而提升效率。

Method: 提出FrugalPrompt框架，采用GlobEnc和DecompX两种token归因方法为输入序列中的每个token分配显著性分数，然后筛选保留语义贡献最高的部分token（top-k%），并保持其原有顺序，形成精简后的提示。该方法在四种典型NLP任务和多种LLM模型上进行了实证评估。

Result: 在情感分析、常识问答和文本摘要等任务上，即便将prompt压缩20%，模型性能仅有微小下降，说明LLM可通过高显著性线索恢复丢失部分上下文。而在数学推理任务中，性能急剧下降，表明此类任务对完整上下文依赖较强。进一步分析还揭示了部分任务存在模型记忆或偏见等“污染”现象。

Conclusion: FrugalPrompt为探索大模型效率和性能之间权衡提供了新视角。论文明确划分了可容忍语境稀疏的任务与需完整版上下文的任务边界，为高效利用LLM提供理论和方法指导。

Abstract: Large language models (LLMs) owe much of their stellar performance to
expansive input contexts, yet such verbosity inflates monetary costs, carbon
footprint, and inference-time latency. Much of this overhead manifests from the
redundant low-utility tokens present in typical prompts, as only a fraction of
tokens typically carries the majority of the semantic weight. We address this
inefficiency by introducing FrugalPrompt, a novel prompt compression framework
for LLMs, which retains only the most semantically significant tokens.
Leveraging two state-of-the-art token attribution methods, GlobEnc and DecompX,
we assign salience scores to every token in an input sequence, rank them to
preserve the top-k% tokens in their original order, and obtain a sparse
frugalized prompt. We evaluate the approach across four NLP tasks: Sentiment
Analysis, Commonsense QA, Summarization, and Mathematical Reasoning, using a
suite of frontier LLMs. For the first three tasks, a 20% prompt reduction
incurs only a marginal loss in task performance, demonstrating that
contemporary LLMs can reconstruct elided context from high-salience cues. In
contrast, performance on mathematical reasoning deteriorates sharply,
reflecting a stronger dependence on complete token continuity. Further analysis
with bottom-k% and random-k% tokens reveals asymmetric performance patterns
that may suggest potential task contamination effects, wherein models may
resort to shallow memorized patterns from pretraining exposure for conventional
NLP tasks. We posit that our work contributes to a more nuanced understanding
of LLM behavior in performance-efficiency trade-offs, and delineate the
boundary between tasks tolerant to contextual sparsity and those requiring
exhaustive context. Our source code and models are available at:
https://github.com/Starscream-11813/Frugal-ICL

</details>


### [201] [TrajSelector: Harnessing Latent Representations for Efficient and Effective Best-of-N in Large Reasoning Model](https://arxiv.org/abs/2510.16449)
*Bin Yu,Xinming Wang,Shijie Lian,Haotian Li,Changti Wu,Ruina Hu,Bailing Wang,Yuliang Wei,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出TrajSelector框架，可在大语言模型（LLM）推理时高效选择最佳解题路径，提升复杂推理任务性能，并显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的Best-of-N推理（如Best-of-N选择法）可提升模型性能，但需要大量计算资源（如外部奖励模型），且未充分利用LLM自身的隐藏状态，导致效率和效果受限。

Method: 作者提出TrajSelector框架，直接利用采样时LLM的隐藏状态，通过一个仅0.6B参数的小型验证器对推理路径逐步打分，再聚合得分选出最优路径。整个方法用端到端、数据驱动的训练流程，避免了大规模逐步标注的需求。

Result: 在5个基准任务上，TrajSelector能为Best-of-32方案带来4.61%的精度提升，相较主流过程奖励模型高出4.31%～12.21%，且推理计算成本更低。

Conclusion: TrajSelector方法高效、实用，能以更低的成本在多个复杂推理任务中提升LLM推理表现，为LLM推理范式带来新思路。

Abstract: Large language models (LLMs) have shown remarkable progress in complex
reasoning tasks, largely enabled by test-time scaling (TTS) paradigms that
allocate additional compute during inference. Among these, external TTS
(particularly the Best-of-N selection paradigm) yields scalable performance
improvements by selecting from multiple independently generated reasoning
trajectories. However, this approach faces key limitations: (i) the high
computational overhead of deploying process reward models, (ii) the
underutilization of the LLM's intrinsic latent representations. We introduce
TrajSelector, an efficient and effective Best-of-N framework that exploit the
hidden states in the sampler LLM for process-level scoring. A lightweight
verifier (with only 0.6B parameters) evaluates the quality of step-wise
trajectory, and then aggregates these scores to identify the optimal reasoning
trajectory. Our framework employs a fully data-driven, end-to-end training
recipe that eliminates reliance on massive step-level annotations. Experiential
results across five benchmarks demonstrate that TrajSelector delivers
consistent performance gains. In Best-of-32 settings, it surpasses majority
voting by 4.61% accuracy and outperforms existing process reward models by
4.31% to 12.21%, all while maintaining lower inference costs.

</details>


### [202] [RAVEN: Robust Advertisement Video Violation Temporal Grounding via Reinforcement Reasoning](https://arxiv.org/abs/2510.16455)
*Deyi Ji,Yuekui Yang,Haiyang Wu,Shaoping Ma,Tianrun Chen,Lanyun Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种新方法RAVEN，通过引入课程强化学习与多模态大语言模型相结合，提升广告视频违规检测的准确性和泛化能力，并在工业及公共数据上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有广告视频违规检测方法在精确定位违规时间段、处理带噪声标注及泛化能力方面表现不佳，亟需更智能、更鲁棒的解决方案以保障平台合规。

Method: RAVEN框架采用课程强化学习策略，结合精细和粗粒度标注数据，引入多模态大语言模型和Group Relative Policy Optimization（GRPO），通过多层次奖励机制提升模型推理和认知能力，实现高精度的时间定位及违规类别预测。

Result: 在真实工业数据集和公开基准上，RAVEN在违规类别准确率和时间区间定位上均优于现有方法；实际上线并A/B测试后，精确率和召回率均大幅提升。

Conclusion: RAVEN框架不仅提升了广告违规检测的性能，还具备较强泛化能力与实际应用价值，有效缓解了传统有监督微调过程中遗忘问题。

Abstract: Advertisement (Ad) video violation detection is critical for ensuring
platform compliance, but existing methods struggle with precise temporal
grounding, noisy annotations, and limited generalization. We propose RAVEN, a
novel framework that integrates curriculum reinforcement learning with
multimodal large language models (MLLMs) to enhance reasoning and cognitive
capabilities for violation detection. RAVEN employs a progressive training
strategy, combining precisely and coarsely annotated data, and leverages Group
Relative Policy Optimization (GRPO) to develop emergent reasoning abilities
without explicit reasoning annotations. Multiple hierarchical sophisticated
reward mechanism ensures precise temporal grounding and consistent category
prediction. Experiments on industrial datasets and public benchmarks show that
RAVEN achieves superior performances in violation category accuracy and
temporal interval localization. We also design a pipeline to deploy the RAVEN
on the online Ad services, and online A/B testing further validates its
practical applicability, with significant improvements in precision and recall.
RAVEN also demonstrates strong generalization, mitigating the catastrophic
forgetting issue associated with supervised fine-tuning.

</details>


### [203] [Agree, Disagree, Explain: Decomposing Human Label Variation in NLI through the Lens of Explanations](https://arxiv.org/abs/2510.16458)
*Pingjun Hong,Beiduo Chen,Siyao Peng,Marie-Catherine de Marneffe,Benjamin Roth,Barbara Plank*

Main category: cs.CL

TL;DR: 本文扩展了以往自然语言推断（NLI）数据集标注分歧的研究，探讨了标注者在推理类型和标签选择上都可能存在差异，并通过分析自由文本解释深入理解标注分歧。


<details>
  <summary>Details</summary>
Motivation: 现有NLI数据集存在标注者标签分歧，而以往工作多聚焦于标签一致但推理解释不同的情况，未深入探究更广泛的分歧来源和其本质。

Method: 作者采用LiTEx自由文本解释推理分类法，对两个英文NLI数据集进行分析，结合标签一致性、解释相似性和推理类型一致性，多角度探讨标注差异，并引入标注者选择偏差因素进行深入比对。

Result: 作者发现：1）有时标注者虽然在最终标签上分歧，但给出的解释极为相似，表明表面上的标签分歧未必意味着理解上的根本差别；2）标注者在解释策略和标签选择上存有个人偏好。

Conclusion: 推理类型上的一致性相比标签一致性更能反映解释的语义相似性，自由文本解释提供了丰富的分析维度，警示科研和应用中不能简单将标签视为绝对真理。

Abstract: Natural Language Inference datasets often exhibit human label variation. To
better understand these variations, explanation-based approaches analyze the
underlying reasoning behind annotators' decisions. One such approach is the
LiTEx taxonomy, which categorizes free-text explanations in English into
reasoning types. However, previous work applying such taxonomies has focused on
within-label variation: cases where annotators agree on the final NLI label but
provide different explanations. In contrast, this paper broadens the scope by
examining how annotators may diverge not only in the reasoning type but also in
the labeling step. We use explanations as a lens to decompose the reasoning
process underlying NLI annotation and to analyze individual differences. We
apply LiTEx to two NLI English datasets and align annotation variation from
multiple aspects: NLI label agreement, explanation similarity, and taxonomy
agreement, with an additional compounding factor of annotators' selection bias.
We observe instances where annotators disagree on the label but provide highly
similar explanations, suggesting that surface-level disagreement may mask
underlying agreement in interpretation. Moreover, our analysis reveals
individual preferences in explanation strategies and label choices. These
findings highlight that agreement in reasoning types better reflects the
semantic similarity of free-text explanations than label agreement alone. Our
findings underscore the richness of reasoning-based explanations and the need
for caution in treating labels as ground truth.

</details>


### [204] [Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety](https://arxiv.org/abs/2510.16492)
*Vamshi Krishna Bonagiri,Ponnurangam Kumaragurum,Khanh Nguyen,Benjamin Plaut*

Main category: cs.CL

TL;DR: 论文提出通过让大语言模型代理“退出”不确定情境作为增强安全性的机制，系统评估证明该方法能显著提升安全性且几乎不影响有用性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型被用作具备工具访问能力的多轮自主代理，其安全性面临更严峻挑战，传统的不确定性量化方法难以应对实际复杂风险。

Method: 作者利用ToolEmu框架，对12种现有主流大模型实施了“明确退出指令”，系统性评估在面对不确定或高风险情境时模型主动退出的表现，通过量化安全性与有用性变化评测方法有效性。

Result: 所有模型因“退出”机制综合安全性提升0.39分（专有模型可达0.64分），有用性仅微小下降0.03分，表明安全性的显著改善远大于对实用性的负面影响。

Conclusion: 在现有LLM代理系统中，通过简单添加“退出”指令即可落地有效的安全保障，推荐作为自主代理在高风险领域首选、前置的安全防护措施。

Abstract: As Large Language Model (LLM) agents increasingly operate in complex
environments with real-world consequences, their safety becomes critical. While
uncertainty quantification is well-studied for single-turn tasks, multi-turn
agentic scenarios with real-world tool access present unique challenges where
uncertainties and ambiguities compound, leading to severe or catastrophic risks
beyond traditional text generation failures. We propose using "quitting" as a
simple yet effective behavioral mechanism for LLM agents to recognize and
withdraw from situations where they lack confidence. Leveraging the ToolEmu
framework, we conduct a systematic evaluation of quitting behavior across 12
state-of-the-art LLMs. Our results demonstrate a highly favorable
safety-helpfulness trade-off: agents prompted to quit with explicit
instructions improve safety by an average of +0.39 on a 0-3 scale across all
models (+0.64 for proprietary models), while maintaining a negligible average
decrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding
explicit quit instructions proves to be a highly effective safety mechanism
that can immediately be deployed in existing agent systems, and establishes
quitting as an effective first-line defense mechanism for autonomous agents in
high-stakes applications.

</details>


### [205] [Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection](https://arxiv.org/abs/2510.16499)
*Michelle Yuan,Khushbu Pahwa,Shuaichen Chang,Mustafa Kaba,Jiarong Jiang,Xiaofei Ma,Yi Zhang,Monica Sunkara*

Main category: cs.CL

TL;DR: 本文提出了一种灵感来自背包问题的结构化自动化框架，用于高效组装Agent系统，实现性能、成本和兼容性的动态平衡，在多个基准测试集上优于静态检索基线。


<details>
  <summary>Details</summary>
Motivation: 现有的Agent系统组件（如agent、工具和模型）集成方法多基于静态语义检索，难以克服能力描述不全和成本、效用等动态约束导致的组件选择与复用难题。

Method: 提出一种自动化的agentic系统组装框架，将组件选择建模为类似背包问题，并允许构建代理动态测试候选组件及其实用价值，同时兼顾性能、预算和兼容性因素，实现按需组装最优组件集。

Result: 基于Claude 3.5 Sonnet的实验，在线背包方法在五个基准数据集上始终优于传统静态检索基线。单agent场景成功率提升最高达31.6%，多agent场景下从37%提升至87%，且组件成本更低。

Conclusion: 该方法大幅提升了Agentic系统的组装效率和效果，证实了其在不同领域和预算约束下的强适应性和资源复用能力。

Abstract: Designing effective agentic systems requires the seamless composition and
integration of agents, tools, and models within dynamic and uncertain
environments. Most existing methods rely on static, semantic retrieval
approaches for tool or agent discovery. However, effective reuse and
composition of existing components remain challenging due to incomplete
capability descriptions and the limitations of retrieval methods. Component
selection suffers because the decisions are not based on capability, cost, and
real-time utility. To address these challenges, we introduce a structured,
automated framework for agentic system composition that is inspired by the
knapsack problem. Our framework enables a composer agent to systematically
identify, select, and assemble an optimal set of agentic components by jointly
considering performance, budget constraints, and compatibility. By dynamically
testing candidate components and modeling their utility in real-time, our
approach streamlines the assembly of agentic systems and facilitates scalable
reuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five
benchmarking datasets shows that our online-knapsack-based composer
consistently lies on the Pareto frontier, achieving higher success rates at
significantly lower component costs compared to our baselines. In the
single-agent setup, the online knapsack composer shows a success rate
improvement of up to 31.6% in comparison to the retrieval baselines. In
multi-agent systems, the online knapsack composer increases success rate from
37% to 87% when agents are selected from an agent inventory of 100+ agents. The
substantial performance gap confirms the robust adaptability of our method
across diverse domains and budget constraints.

</details>


### [206] [ReviewGuard: Enhancing Deficient Peer Review Detection via LLM-Driven Data Augmentation](https://arxiv.org/abs/2510.16549)
*Haoxuan Zhang,Ruochi Li,Sarthak Shrestha,Shree Harshini Mamidala,Revanth Putta,Arka Krishan Aggarwal,Ting Xiao,Junhua Ding,Haihua Chen*

Main category: cs.CL

TL;DR: 本文提出ReviewGuard系统，首次利用大模型（LLM）检测学术同行评审中的不合格（deficient）评审，以保障学术诚信及评审质量。


<details>
  <summary>Details</summary>
Motivation: 当前学术投稿数量激增且LLM在评审过程被广泛使用，不足的（无论是人工还是AI生成的）评审内容可能系统性损害同行评审生态和学术诚信，亟需自动化检测不合格评审的方法。

Method: 提出了ReviewGuard系统，包括四个阶段：1）收集ICLR和NeurIPS会议的论文及评审；2）用GPT-4.1结合人工标注评审类型；3）通过LLM合成数据进行类别平衡和数据扩增，最终合成与真实评审数量分别为46,438和24,657条；4）用这些数据微调编码器模型和开源LLM，并对评审结构和质量做全面特征分析。

Result: 实验证明，不合格评审通常评分较低、自信程度更高、结构复杂度更低、负面情感占比高。AI生成内容检测显示，自ChatGPT出现后AI生成的评审显著增多。混合真实和合成评审数据训练检测模型，可显著提升不合格评审检测的召回率和F1分数。

Conclusion: ReviewGuard是首个LLM驱动的不合格评审检测系统，有助于提升学术评审环节的AI治理能力，并对维护学术诚信和优化人机协同评审流程具有重要意义。

Abstract: Peer review serves as the gatekeeper of science, yet the surge in submissions
and widespread adoption of large language models (LLMs) in scholarly evaluation
present unprecedented challenges. Recent work has focused on using LLMs to
improve review efficiency or generate insightful review content. However,
unchecked deficient reviews from both human experts and AI systems threaten to
systematically undermine the peer review ecosystem and compromise academic
integrity. To address this critical issue, we introduce ReviewGuard, an
automated system for detecting and categorizing deficient reviews. ReviewGuard
employs a comprehensive four-stage LLM-driven framework that: (1) collects ICLR
and NeurIPS papers with their corresponding reviews from OpenReview; (2)
annotates review types using GPT-4.1 with human validation; (3) addresses class
imbalance and data scarcity through LLM-driven synthetic data augmentation,
producing a final corpus of 6,634 papers, 24,657 real reviews, and 46,438
synthetic reviews; and (4) fine-tunes both encoder-based models and open source
LLMs. We perform comprehensive feature analysis of the structure and quality of
the review text. Compared to sufficient reviews, deficient reviews demonstrate
lower rating scores, higher self-reported confidence, reduced structural
complexity, and a higher proportion of negative sentiment. AI-generated text
detection reveals that, since ChatGPT's emergence, AI-generated reviews have
increased dramatically. In the evaluation of deficient review detection models,
mixed training with synthetic and real review data provides substantial
enhancements to recall and F1 scores on the binary task. This study presents
the first LLM-driven system for detecting deficient peer reviews, providing
evidence to inform AI governance in peer review while offering valuable
insights into human-AI collaboration to maintain academic integrity.

</details>


### [207] [Language over Content: Tracing Cultural Understanding in Multilingual Large Language Models](https://arxiv.org/abs/2510.16565)
*Seungho Cho,Changgeon Ko,Eui Jun Hwang,Junmyeong Lee,Huije Lee,Jong C. Park*

Main category: cs.CL

TL;DR: 本文分析了大语言模型在不同文化环境下内部表征的差异，发现语言对模型内部机制影响更大，但同语种不同文化的情况也存在很高变异性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型被广泛应用于多元文化情景中，准确的文化理解能力至关重要，但目前主流评估多关注输出表现，鲜有深入探讨其内部机制，尤其是涉及多语言和文化因素时。

Method: 作者通过对比模型回答语义等价问题时的内部激活路径重叠度，分别在保持问题语言不变但改变目标国家、以及固定国家但变换问题语言的设置下，追踪模型的内部文化理解机制。另外，还用同语种不同国家的问答对来区分语言和文化的影响。

Result: 发现同语种不同国家的问题，模型内部路径重叠度高于同国家但不同语言的问题，说明模型存在明显的语言特定处理模式。值得注意的是，韩国和朝鲜之间的重叠度显著低且变化大，说明语言相似性并不能保证模型内部表征一致。

Conclusion: 大语言模型的内部文化理解机制更多受语言因素影响，但在某些同语言不同文化的场景下，模型也会产生显著的内部差异（如韩朝案例），提示后续需要更细致地处理语言和文化因素对模型能力的影响。

Abstract: Large language models (LLMs) are increasingly used across diverse cultural
contexts, making accurate cultural understanding essential. Prior evaluations
have mostly focused on output-level performance, obscuring the factors that
drive differences in responses, while studies using circuit analysis have
covered few languages and rarely focused on culture. In this work, we trace
LLMs' internal cultural understanding mechanisms by measuring activation path
overlaps when answering semantically equivalent questions under two conditions:
varying the target country while fixing the question language, and varying the
question language while fixing the country. We also use same-language country
pairs to disentangle language from cultural aspects. Results show that internal
paths overlap more for same-language, cross-country questions than for
cross-language, same-country questions, indicating strong language-specific
patterns. Notably, the South Korea-North Korea pair exhibits low overlap and
high variability, showing that linguistic similarity does not guarantee aligned
internal representation.

</details>


### [208] [Hallucination Benchmark for Speech Foundation Models](https://arxiv.org/abs/2510.16567)
*Alkis Koudounas,Moreno La Quatra,Manuel Giollo,Sabato Marco Siniscalchi,Elena Baralis*

Main category: cs.CL

TL;DR: 本文针对ASR系统出现的幻觉（无关音频但语法和语义通顺的转写）现象，提出了SHALLOW评测框架，用于系统化检测和量化幻觉，并通过多维指标对模型表现进行细致分析。


<details>
  <summary>Details</summary>
Motivation: 现有ASR系统可能生成看似合理、实则与原始语音无关的幻觉内容，且常用的WER等错误指标难以区分常规错误和幻觉，亟需新的评估方法识别并量化该现象。

Method: 作者提出SHALLOW基准框架，从词汇、语音、形态和语义四个维度对幻觉进行分类和量化，并为每种类型设计专门指标，通过多模型和多语音场景实验，分析SHALLOW指标与传统WER的关系。

Result: SHALLOW指标在低WER（高识别质量）时与WER高度相关，但在WER升高时该相关性下降，显示SHALLOW能捕获传统WER无法区分的细粒度错误模式。

Conclusion: SHALLOW能够系统分类检测ASR幻觉现象，并为模型薄弱环节提供更细致诊断，推动ASR系统幻觉相关性能评测和改进。

Abstract: Hallucinations in automatic speech recognition (ASR) systems refer to fluent
and coherent transcriptions produced by neural ASR models that are completely
unrelated to the underlying acoustic input (i.e., the speech signal). While
similar to conventional decoding errors in potentially compromising the
usability of transcriptions for downstream applications, hallucinations can be
more detrimental due to their preservation of syntactically and semantically
plausible structure. This apparent coherence can mislead subsequent processing
stages and introduce serious risks, particularly in critical domains such as
healthcare and law. Conventional evaluation metrics are primarily centered on
error-based metrics and fail to distinguish between phonetic inaccuracies and
hallucinations. Consequently, there is a critical need for new evaluation
frameworks that can effectively identify and assess models with a heightened
propensity for generating hallucinated content. To this end, we introduce
SHALLOW, the first benchmark framework that systematically categorizes and
quantifies hallucination phenomena in ASR along four complementary axes:
lexical, phonetic, morphological, and semantic. We define targeted metrics
within each category to produce interpretable profiles of model behavior.
Through evaluation across various architectures and speech domains, we have
found that SHALLOW metrics correlate strongly with word error rate (WER) when
recognition quality is high (i.e., low WER). Still, this correlation weakens
substantially as WER increases. SHALLOW, therefore, captures fine-grained error
patterns that WER fails to distinguish under degraded and challenging
conditions. Our framework supports specific diagnosis of model weaknesses and
provides feedback for model improvement beyond what aggregate error rates can
offer.

</details>


### [209] [AI-Generated Text Detection in Low-Resource Languages: A Case Study on Urdu](https://arxiv.org/abs/2510.16573)
*Muhammad Ammar,Hadiya Murad Hadi,Usman Majeed Butt*

Main category: cs.CL

TL;DR: 本文提出了一种针对乌尔都语的AI生成文本检测框架，通过对平衡数据集的特征分析和多语言Transformer模型微调，实现了超过91%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型生成的文本愈发接近人类书写，导致难以区分人类与AI写作，尤其是低资源语言如乌尔都语，缺乏有效检测工具。旨在填补这一技术空白，维护乌尔都语社群中的内容真实性。

Method: 1) 构建包含1800个人类写作和1800个AI生成样本的数据集；2) 分析字符、词汇量、N-gram等语言与统计特征，并用t检验和Mann-Whitney U检验评估显著性；3) 对三种多语言Transformer（mdeberta-v3-base, distilbert-base-multilingual-cased, xlm-roberta-base）进行微调，评估其检测能力。

Result: 微调的mDeBERTa-v3-base模型在测试集上取得了F1分数91.29和准确率91.26%的最佳检测性能。

Conclusion: 该方法为乌尔都语社群检测AI生成文本提供了高效方案，有助于抵制虚假信息与学术不端，并促进低资源语言NLP工具的发展。

Abstract: Large Language Models (LLMs) are now capable of generating text that closely
resembles human writing, making them powerful tools for content creation, but
this growing ability has also made it harder to tell whether a piece of text
was written by a human or by a machine. This challenge becomes even more
serious for languages like Urdu, where there are very few tools available to
detect AI-generated text. To address this gap, we propose a novel AI-generated
text detection framework tailored for the Urdu language. A balanced dataset
comprising 1,800 humans authored, and 1,800 AI generated texts, sourced from
models such as Gemini, GPT-4o-mini, and Kimi AI was developed. Detailed
linguistic and statistical analysis was conducted, focusing on features such as
character and word counts, vocabulary richness (Type Token Ratio), and N-gram
patterns, with significance evaluated through t-tests and MannWhitney U tests.
Three state-of-the-art multilingual transformer models such as
mdeberta-v3-base, distilbert-base-multilingualcased, and xlm-roberta-base were
fine-tuned on this dataset. The mDeBERTa-v3-base achieved the highest
performance, with an F1-score 91.29 and accuracy of 91.26% on the test set.
This research advances efforts in contesting misinformation and academic
misconduct in Urdu-speaking communities and contributes to the broader
development of NLP tools for low resource languages.

</details>


### [210] [Fine-tuning of Large Language Models for Constituency Parsing Using a Sequence to Sequence Approach](https://arxiv.org/abs/2510.16604)
*Francisco Jose Cortes Delgado,Eduardo Martinez Gracia,Rafael Valencia Garcia*

Main category: cs.CL

TL;DR: 本文通过微调大型语言模型（LLM），将输入句子翻译为其对应的短语结构语法，大大提升了机器学习驱动的句法分析准确性。


<details>
  <summary>Details</summary>
Motivation: 近年来，大型神经网络模型在自然语言处理中的突破为句法分析带来了新的可能性。本文旨在扩展用于教学的西班牙语句法工具MiSintaxis的能力。

Method: 作者从Hugging Face模型库选取多种LLM，并利用AnCora-ES语料库生成的训练数据对这些模型进行微调，让模型能够将输入句子转换为对应的短语结构。同时采用F1分数评价模型表现。

Result: 实验结果显示，经过微调后的模型在句子短语结构分析任务上获得了较高的F1分数，展示了该方法的高准确性。

Conclusion: 基于LLM进行短语结构句法分析是一种有效且有前景的方法，特别适用于如MiSintaxis这样的教学工具，有望促进语法教学和分析的自动化发展。

Abstract: Recent advances in natural language processing with large neural models have
opened new possibilities for syntactic analysis based on machine learning. This
work explores a novel approach to phrase-structure analysis by fine-tuning
large language models (LLMs) to translate an input sentence into its
corresponding syntactic structure. The main objective is to extend the
capabilities of MiSintaxis, a tool designed for teaching Spanish syntax.
Several models from the Hugging Face repository were fine-tuned using training
data generated from the AnCora-ES corpus, and their performance was evaluated
using the F1 score. The results demonstrate high accuracy in phrase-structure
analysis and highlight the potential of this methodology.

</details>


### [211] [Unleashing Diverse Thinking Modes in LLMs through Multi-Agent Collaboration](https://arxiv.org/abs/2510.16645)
*Zhixuan He,Yue Feng*

Main category: cs.CL

TL;DR: 本文提出了一种多智能体协作框架DiMo，让四种不同思维模式的LLM智能体以结构化辩论提升推理的准确性和可解释性，效果优于单模型与传统辩论方法，尤其在数学任务上提升明显。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）表现优异，但其推理过程常缺乏可解释性和多样化思维，难以满足对严谨理由链及用户友好解释的需求。因此，作者希望通过多智能体协作提升推理的透明度和准确性。

Method: DiMo 框架设定四个具备不同推理范式的LLM智能体，通过多轮结构化辩论彼此挑战和完善初步答案。每位智能体提供独特认知视角，最终产出更强有力的结论和清晰、可追溯的理由链。同时，理由链具备语义类型和网页URL标注，方便后续系统利用。

Result: 在六个标准推理基准测试下，DiMo在统一的开源环境中表现优于常用的单模型和辩论基线，尤其在数学任务上效果提升最大。理由链具有可审计性和扩展性。

Conclusion: DiMo证明了多智能体、多范式辩论能够同时提升大型语言模型的准确性与可解释性，为人机协同推理提供了新的Web兼容框架，对后续研究和实际推理系统有重要参考价值。

Abstract: Large Language Models (LLMs) demonstrate strong performance but often lack
interpretable reasoning. This paper introduces the Multi-Agent Collaboration
Framework for Diverse Thinking Modes (DiMo), which enhances both performance
and interpretability by simulating a structured debate among four specialized
LLM agents. Each agent embodies a distinct reasoning paradigm, allowing the
framework to collaboratively explore diverse cognitive approaches. Through
iterative debate, agents challenge and refine initial responses, yielding more
robust conclusions and an explicit, auditable reasoning chain. Across six
benchmarks and under a unified open-source setup, DiMo improves accuracy over
widely used single-model and debate baselines, with the largest gains on math.
We position DiMo as a semantics-aware, Web-native multi-agent framework: it
models human-machine intelligence with LLM agents that produce semantically
typed, URL-annotated evidence chains for explanations and user-friendly
interactions. Although our experiments use standard reasoning benchmarks, the
framework is designed to be instantiated over Web corpora and knowledge graphs,
combining retrieval-augmented reasoning with structured justifications that
downstream systems can inspect and reuse.

</details>


### [212] [All You Need is One: Capsule Prompt Tuning with a Single Vector](https://arxiv.org/abs/2510.16670)
*Yiyang Liu,James C. Liang,Heng Fan,Wenhao Yang,Yiming Cui,Xiaotian Han,Lifu Huang,Dongfang Liu,Qifan Wang,Cheng Han*

Main category: cs.CL

TL;DR: 本研究提出了一种高效、有效的提示微调方法Capsule Prompt-Tuning（CaPT），通过集成实例感知和任务感知信息，以极低参数开销增强大语言模型在下游任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前提示学习方法高度依赖于繁琐的提示长度调参，并且通常需要大量提示，造成计算负担。此外，现有任务感知提示设计缺乏实例感知信息，限制了模型对输入的精细交互。作者希望提升提示方法的参数效率和适应能力。

Method: 提出Capsule Prompt-Tuning（CaPT），将实例感知信息与任务感知信息结合，并创新性地将实例感知tokens置于输入最前端。整个方法几乎不增加模型参数，实现单一胶囊提示高效指导模型。

Result: 实证结果表明，该方法在多项语言任务上表现优异（如T5-Large平均准确率84.03%），且参数开销极小（Llama3.2-1B仅占0.003%）。研究还发现，将实例感知信息置于序列起始可作为“attention anchor”，强化关键结构信息的关注。

Conclusion: CaPT方法能有效整合实例和任务信息，显著提升大模型的任务适应能力和效率，为提示调优方法提供了高效的新思路。

Abstract: Prompt-based learning has emerged as a parameter-efficient finetuning (PEFT)
approach to facilitate Large Language Model (LLM) adaptation to downstream
tasks by conditioning generation with task-aware guidance. Despite its
successes, current prompt-based learning methods heavily rely on laborious grid
searching for optimal prompt length and typically require considerable number
of prompts, introducing additional computational burden. Worse yet, our pioneer
findings indicate that the task-aware prompt design is inherently limited by
its absence of instance-aware information, leading to a subtle attention
interplay with the input sequence. In contrast, simply incorporating
instance-aware information as a part of the guidance can enhance the
prompt-tuned model performance without additional fine-tuning. Moreover, we
find an interesting phenomenon, namely "attention anchor", that incorporating
instance-aware tokens at the earliest position of the sequence can successfully
preserve strong attention to critical structural information and exhibit more
active attention interaction with all input tokens. In light of our
observation, we introduce Capsule Prompt-Tuning (CaPT), an efficient and
effective solution that leverages off-the-shelf, informative instance semantics
into prompt-based learning. Our approach innovatively integrates both
instance-aware and task-aware information in a nearly parameter-free manner
(i.e., one single capsule prompt). Empirical results demonstrate that our
method can exhibit superior performance across various language tasks (e.g.,
84.03\% average accuracy on T5-Large), serving as an "attention anchor," while
enjoying high parameter efficiency (e.g., 0.003\% of model parameters on
Llama3.2-1B).

</details>


### [213] [Temporal Understanding under Deictic Frame of Reference](https://arxiv.org/abs/2510.16685)
*Damin Zhang,Julia Rayz*

Main category: cs.CL

TL;DR: 本研究提出TUuD框架用于评估大型语言模型（LLMs）在动态“现在”参考点（deictic t-FoR）下的时间理解能力，并发现LLMs对时间关系的理解有限，尤其在远离当前时刻时。


<details>
  <summary>Details</summary>
Motivation: 语言中对时间的理解常通过空间隐喻实现，依赖于特定的参考框架（如FoR）。尽管LLMs在自然语言处理上表现优越，但其对时间的推理能力尚存在不足，因此需要研究其在变动“现在”参考点下对时间关系的理解。

Method: 作者提出TUuD框架，让LLMs在“现在”不断变化的时间轴上，对当前时刻与目标事件的相似性（0-1分）进行评分，以此量化模型对时间关系的感知能力。评测了四种主流LLMs。

Result: 评测发现，所有四个LLMs在当前时刻对目标事件的相似性评分较高，随着目标事件远离“现在”（无论过去还是未来），评分均下降，表现出一定的人类类时间认知特征。但在更远的时间距离上，这种适应性显著减弱。

Conclusion: LLMs在近距离的时间参考框架内能部分模拟人类的时间感知，但其对参考点变化和远距离的时间推理能力有限，需进一步改进提升其时间推理能力。

Abstract: Understanding time is fundamental to human cognition, where temporal
experience is often conceptualized through spatial metaphors grounded in
sensory-motor experience. For example, "summer is approaching" parallels "We
are approaching the summer". In such expressions, humans rely on a frame of
reference (FoR) to interpret meaning relative to a particular viewpoint.
Extending this concept to time, a temporal frame of reference (t-FoR) defines
how temporal relations are perceived relative to an experiencer's moment of
"now". While Large Language Models (LLMs) have shown remarkable advances in
natural language understanding, their ability to interpret and reason about
time remains limited. In this work, we introduce TUuD (Temporal Understanding
under Deictic t-FoR), a framework that evaluates how LLMs interpret time-event
and event-event relations when the reference point of "now" dynamically shifts
along a timeline. Following recent work on temporal cognition
\cite{li2025other}, LLMs are prompted to rate the similarity between the
current moment and a target event from 0.00 (completely dissimilar) to 1.00
(highly similar), where similarity quantifies perceived temporal alignment
between the two points. Our results show that four evaluated LLMs exhibit
measurable adaptation to a deictic t-FoR, with similarity ratings peaking
around the present and decreasing toward past and future events. The
adaptation, however, weakens beyond near-term contexts, suggesting that while
LLMs display partial human-like temporal cognition, their temporal reasoning
remains sensitive to reference-frame shifts and temporal distance.

</details>


### [214] [Investigating the Impact of Rationales for LLMs on Natural Language Understanding](https://arxiv.org/abs/2510.16686)
*Wenhang Shi,Shuqing Bian,Yiren Chen,Xinyi Zhang,Zhe Zhao,Pengfei Hu,Wei Lu,Xiaoyong Du*

Main category: cs.CL

TL;DR: 本文系统性地探索了链式思维(CoT)推理与理由(rationales)在自然语言理解(NLU)任务中的作用，提出了新的数据集和方法，并揭示了模型规模、训练策略与推理性能之间的关系。


<details>
  <summary>Details</summary>
Motivation: 虽然链式思维(CoT)和理由在数学、常识等推理类任务中取得了显著效果，但其对自然语言理解(NLU)任务能否带来类似益处尚未被系统研究。因此，作者希望通过系统构建带理理由的数据集，并检验相应方法在NLU任务中的表现，推动相关技术更广泛的应用。

Method: 作者构建了名为NLURC的大型高质量带理由的NLU数据集，设计了多种结合理由的推理与训练方法，对这些方法在多种NLU任务上的表现进行了全面实验研究，并分析了模型规模、训练策略和推理方式的影响。

Result: 1) 随着模型规模增大，CoT推理对NLU性能由负面影响转为优于直接标注预测，显示出二者正相关；2) 多数理由增强训练方法效果不及单纯标注训练，仅有一种特别设计方法表现优异；3) 用理由训练的大模型在未见NLU任务上有明显提升，可与大十倍且无理由训练的模型媲美，并提供了极佳的可解释性。

Conclusion: 理由(链式思维)在NLU任务中具有独特价值，尤其是在大模型和精心设计训练策略下能带来显著性能和可解释性的提升，为未来NLU研究和应用提供了新思路。

Abstract: Chain-of-thought (CoT) rationales, which provide step-by-step reasoning to
derive final answers, benefit LLMs in both inference and training.
Incorporating rationales, either by generating them before answering during
inference, or by placing them before or after the original answers during
training - significantly improves model performance on mathematical, symbolic
and commonsense reasoning tasks. However, most work focuses on the role of
rationales in these reasoning tasks, overlooking their potential impact on
other important tasks like natural language understanding (NLU) tasks. In this
work, we raise the question: Can rationales similarly benefit NLU tasks? To
conduct a systematic exploration, we construct NLURC, a comprehensive and
high-quality NLU dataset collection with rationales, and develop various
rationale-augmented methods. Through exploring the applicability of these
methods on NLU tasks using the dataset, we uncover several potentially
surprising findings: (1) CoT inference shifts from hindering NLU performance to
surpassing direct label prediction as model size grows, indicating a positive
correlation. (2) Most rationale-augmented training methods perform worse than
label-only training, with one specially designed method consistently achieving
improvements. (3) LLMs trained with rationales achieve significant performance
gains on unseen NLU tasks, rivaling models ten times their size, while
delivering interpretability on par with commercial LLMs.

</details>


### [215] [Natural Language Processing Applications in Cardiology: A Narrative Review](https://arxiv.org/abs/2510.16708)
*Kailai Yang,Yan Leng,Xin Zhang,Tianlin Zhang,Paul Thompson,Bernard Keavney,Maciej Tomaszewski,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 本文回顾了2014至2025年间自然语言处理（NLP）技术在心脏病学领域的应用，分析了265篇相关文献，系统总结了该领域的研究现状和发展趋势。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病广泛影响全球健康，其成因复杂多样，大量相关信息分散在不同文本数据中。利用NLP技术整合和分析这些非结构化数据，有助于深入揭示心脏病发病机制、诊断和治疗的新方法。

Method: 作者检索并分析了6个数据库中发表的相关文献，经过严格筛选，最终纳入了265篇文章。研究从NLP范式、心脏病相关任务、疾病类型、数据来源等多个维度对文献进行归类和梳理，并进行了时序性发展分析。

Result: 分析发现，在NLP类型、任务、疾病和数据源方面均呈现高度多样性。时序分析揭示了近十年来NLP技术在心脏病学领域的快速发展及应用趋势变化。

Conclusion: 该综述全面系统总结了心脏病学领域NLP研究的现状和演变，对未来心脏病的诊断、治疗和预防提供了重要参考，是目前该领域最具全面性的综述性文献。

Abstract: Cardiovascular disease has become increasingly prevalent in modern society
and has a significant effect on global health and well-being. Heart-related
conditions are intricate, multifaceted disorders, which may be influenced by a
combination of genetic predispositions, lifestyle choices, and various
socioeconomic and clinical factors. Information regarding these potentially
complex interrelationships is dispersed among diverse types of textual data,
which include patient narratives, medical records, and scientific literature,
among others. Natural language processing (NLP) techniques have increasingly
been adopted as a powerful means to analyse and make sense of this vast amount
of unstructured data. This, in turn, can allow healthcare professionals to gain
deeper insights into the cardiology field, which has the potential to
revolutionize current approaches to the diagnosis, treatment, and prevention of
cardiac problems. This review provides a detailed overview of NLP research in
cardiology between 2014 and 2025. We queried six literature databases to find
articles describing the application of NLP techniques in the context of a range
of different cardiovascular diseases. Following a rigorous screening process,
we identified a total of 265 relevant articles. We analysed each article from
multiple dimensions, i.e., NLP paradigm types, cardiology-related task types,
cardiovascular disease types, and data source types. Our analysis reveals
considerable diversity within each of these dimensions, thus demonstrating the
considerable breadth of NLP research within the field. We also perform a
temporal analysis, which illustrates the evolution and changing trends in NLP
methods employed over the last decade that we cover. To our knowledge, the
review constitutes the most comprehensive overview of NLP research in
cardiology to date.

</details>


### [216] [The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models](https://arxiv.org/abs/2510.16712)
*Shivam Ratnakar,Sanjay Raghavendra*

Main category: cs.CL

TL;DR: 本文系统性研究了集成搜索/检索引擎的大型语言模型（LLM）在多轮对话中易受“变色龙行为”影响，表现为面对矛盾问题时立场频繁变化。基于大规模新数据集和指标，评测主流LLM均存在此缺陷。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM与检索系统集成已广泛应用，但模型在面对多轮、立场对立的问题时是否能保持立场一致尚未被系统研究，而立场不一致会严重削弱其在关键领域（如医疗、法律、金融）的可靠性。

Method: 作者构建了包含1,180组对话、17,770组精心设计问答的Chameleon基准，对12个有争议领域多轮立场一致性进行测试。同时提出两项新指标：Chameleon Score量化立场稳定性，Source Re-use Rate衡量信息多样性，对三大主流模型系统性评测并统计相关性。

Result: 评测发现所有主流LLM（如Llama-4-Maverick、GPT-4o-mini、Gemini-2.5-Flash）都存在显著变色龙行为（不稳定分数0.391-0.511），GPT-4o-mini最差。不同温度的表现非常接近，说明非采样噪声导致。知识来源单一与立场变化、信心之间高度相关。

Conclusion: 主流LLM一旦集成检索，在多轮对抗性提问下普遍被问题措辞引导，无法持续一致立场，这一严重缺陷在决策相关关键领域部署前必须得到严格评估和改进。

Abstract: Integration of Large Language Models with search/retrieval engines has become
ubiquitous, yet these systems harbor a critical vulnerability that undermines
their reliability. We present the first systematic investigation of "chameleon
behavior" in LLMs: their alarming tendency to shift stances when presented with
contradictory questions in multi-turn conversations (especially in
search-enabled LLMs). Through our novel Chameleon Benchmark Dataset, comprising
17,770 carefully crafted question-answer pairs across 1,180 multi-turn
conversations spanning 12 controversial domains, we expose fundamental flaws in
state-of-the-art systems. We introduce two theoretically grounded metrics: the
Chameleon Score (0-1) that quantifies stance instability, and Source Re-use
Rate (0-1) that measures knowledge diversity. Our rigorous evaluation of
Llama-4-Maverick, GPT-4o-mini, and Gemini-2.5-Flash reveals consistent
failures: all models exhibit severe chameleon behavior (scores 0.391-0.511),
with GPT-4o-mini showing the worst performance. Crucially, small
across-temperature variance (less than 0.004) suggests the effect is not a
sampling artifact. Our analysis uncovers the mechanism: strong correlations
between source re-use rate and confidence (r=0.627) and stance changes
(r=0.429) are statistically significant (p less than 0.05), indicating that
limited knowledge diversity makes models pathologically deferential to query
framing. These findings highlight the need for comprehensive consistency
evaluation before deploying LLMs in healthcare, legal, and financial systems
where maintaining coherent positions across interactions is critical for
reliable decision support.

</details>


### [217] [so much depends / upon / a whitespace: Why Whitespace Matters for Poets and LLMs](https://arxiv.org/abs/2510.16713)
*Sriharsh Bhyravajjula,Melanie Walsh,Anna Preus,Maria Antoniak*

Main category: cs.CL

TL;DR: 本文研究了诗歌中的空白（whitespace）如何体现诗歌形式和诗人艺术选择。通过分析英文诗歌大数据，以及LLM生成诗歌和网络发布的诗歌，揭示了空白使用的差异与特征，并发布了格式保留的公开诗歌数据集。


<details>
  <summary>Details</summary>
Motivation: 诗歌空白是个重要但被NLP领域忽视的研究对象，而当前大语言模型（LLM）在诗歌生成上的空白处理方式不明，对其研究可以改进诗歌生成及文本预处理策略。

Method: 收集和分析了来自Poetry Foundation的19k英文出版诗歌，并将其中2.8k公开域诗歌完整格式发布。对比分析了出版诗歌、5.1万LLM生成诗歌、1.2万网络社区诗歌中的空白使用情况，进一步考察空白在不同历史时期、诗歌形式、数据源中的变化。探讨不同文本处理方法对空白表示的影响。

Result: 诗歌中空白的使用方式因来源、诗歌形式和时代而差异显著。不同文本处理方法对空白信息的保留度影响很大，影响数据集质量和后续LLM训练。

Conclusion: 诗歌空白是语义和空间双重重要的特征，当前NLP和LLM研究需重视其处理策略。发布的数据集可为该领域研究提供基础资源，推动对诗歌空白更深层次的自动化、智能化理解和处理。

Abstract: Whitespace is a critical component of poetic form, reflecting both adherence
to standardized forms and rebellion against those forms. Each poem's whitespace
distribution reflects the artistic choices of the poet and is an integral
semantic and spatial feature of the poem. Yet, despite the popularity of poetry
as both a long-standing art form and as a generation task for large language
models (LLMs), whitespace has not received sufficient attention from the NLP
community. Using a corpus of 19k English-language published poems from Poetry
Foundation, we investigate how 4k poets have used whitespace in their works. We
release a subset of 2.8k public-domain poems with preserved formatting to
facilitate further research in this area. We compare whitespace usage in the
published poems to (1) 51k LLM-generated poems, and (2) 12k unpublished poems
posted in an online community. We also explore whitespace usage across time
periods, poetic forms, and data sources. Additionally, we find that different
text processing methods can result in significantly different representations
of whitespace in poetry data, motivating us to use these poems and whitespace
patterns to discuss implications for the processing strategies used to assemble
pretraining datasets for LLMs.

</details>


### [218] [Beacon: Single-Turn Diagnosis and Mitigation of Latent Sycophancy in Large Language Models](https://arxiv.org/abs/2510.16727)
*Sanskar Pandey,Ruhaan Chopra,Angkul Puniya,Sohom Pal*

Main category: cs.CL

TL;DR: 本文提出了一种新基准，用于测量大语言模型在真实回应与阿谀奉承间的权衡，并分析和干预这种“谄媚性”偏差。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在优化帮助性与礼貌性时，常将帮助与顺从用户混为一谈，导致“谄媚性”偏差——即优先取悦用户而非坚持原则。这种偏差可能危害模型的真实和理性推理。

Method: 作者提出了Beacon基准，这是一种单轮强制选择测试，能在无对话背景下精确测量模型在事实准确性与顺从性偏差间的张力。通过对12个主流模型进行评测，并提出通过提示或激活层级进行干预，探索模型内部对齐的动态结构。

Result: 分析发现：谄媚性可以细分为稳定的语言和情感层次子偏差，且随模型规模增大而增强。不同干预方法能显著调节这些偏差，并揭示了模型对齐在真实性与社会顺从性间的动态权衡。

Conclusion: Beacon基准为“谄媚性”提供了可重复度量方法，将其视为一种规范泛化误差，并为后续研究和缓解大模型对齐漂移提供基础。

Abstract: Large language models internalize a structural trade-off between truthfulness
and obsequious flattery, emerging from reward optimization that conflates
helpfulness with polite submission. This latent bias, known as sycophancy,
manifests as a preference for user agreement over principled reasoning. We
introduce Beacon, a single-turn forced-choice benchmark that isolates this bias
independent of conversational context, enabling precise measurement of the
tension between factual accuracy and submissive bias. Evaluations across twelve
state-of-the-art models reveal that sycophancy decomposes into stable
linguistic and affective sub-biases, each scaling with model capacity. We
further propose prompt-level and activation-level interventions that modulate
these biases in opposing directions, exposing the internal geometry of
alignment as a dynamic manifold between truthfulness and socially compliant
judgment. Beacon reframes sycophancy as a measurable form of normative
misgeneralization, providing a reproducible foundation for studying and
mitigating alignment drift in large-scale generative systems.

</details>


### [219] [Enhancing Language Agent Strategic Reasoning through Self-Play in Adversarial Games](https://arxiv.org/abs/2510.16761)
*Yikai Zhang,Ye Rong,Siyu Yuan,Jiangjie Chen,Jian Xie,Yanghua Xiao*

Main category: cs.CL

TL;DR: 本论文提出了一种新的对抗博弈策略优化方法SCO-PAL，通过自博弈显著提升了语言智能体的策略推理能力，在多项对抗性游戏中胜率大幅提高。


<details>
  <summary>Details</summary>
Motivation: 现有的语言智能体在动态对抗博弈中由于策略推理不足表现不佳。专家标注数据昂贵，自动化的对抗学习成为重要方向，但如何有效选择对手进行训练尚缺乏深入探讨。

Method: 作者提出了SCO-PAL（Step-level poliCy Optimization through Play-And-Learn）方法，通过设置不同水平的对手进行对比实验，重点分析了自博弈对战略推理能力的提升效果。

Result: 基于SCO-PAL自博弈训练的智能体，在与四位对手的对局中，平均胜率较基线方法提高约30%；在与GPT-4的六项对抗性游戏中实现了54.76%的胜率。

Conclusion: 自博弈是提升语言智能体在动态对抗博弈中策略推理的最有效方式，SCO-PAL方法实现了显著的性能提升。

Abstract: Existing language agents often encounter difficulties in dynamic adversarial
games due to poor strategic reasoning. To mitigate this limitation, a promising
approach is to allow agents to learn from game interactions automatically,
without relying on costly expert-labeled data. Unlike static environments where
agents receive fixed feedback or rewards, selecting appropriate opponents in
dynamic adversarial games can significantly impact learning performance.
However, the discussion of opponents in adversarial environments remains an
area under exploration. In this paper, we propose a Step-level poliCy
Optimization method through Play-And-Learn, SCO-PAL. Leveraging SCO-PAL, we
conduct a detailed analysis of opponent selection by setting opponents at
different levels and find that self-play is the most effective way to improve
strategic reasoning in such adversarial environments. Utilizing SCO-PAL with
self-play, we increase the average win rate against four opponents by
approximately 30% compared to baselines and achieve a 54.76% win rate against
GPT-4 in six adversarial games.

</details>


### [220] [LC-Eval: A Bilingual Multi-Task Evaluation Benchmark for Long-Context Understanding](https://arxiv.org/abs/2510.16783)
*Sheikh Jubair,Arwa Omayrah,Amal Alshammari,Alhanoof Althnian,Abdulhamed Alothaimen,Norah A. Alzahrani,Shahad D. Alzaidi,Nora Al-Twairesh,Abdulmohsen Al-Thubaity*

Main category: cs.CL

TL;DR: 该论文提出了LC-Eval，一个面向英语和阿拉伯语的大型语言模型长上下文理解能力评测基准，包括4项具有挑战性的任务，测试主流LLM在超长文本理解上的表现，多个高性能模型表现均存在不足。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在处理超长文本上的能力显著提升，原有评测方法已无法全面衡量其长上下文理解能力，特别是在多语言场景（如英语与阿拉伯语）下。因此亟需新的评估基准来系统性测评LLM在长文本、深层推理及多语言信息提取上的能力。

Method: 作者设计了一个双语、多任务的评测基准LC-Eval，覆盖4k至128k以上tokens的上下文长度，内含4个创新性任务：多文档问答、双语问答、段落级事实核查和基于长文本的多项选择题。数据集覆盖英语与阿拉伯语，并对模型进行跨体裁、跨语种的性能对比测试。

Result: 在多款开放权重及闭源LLM模型上的实验证明，LC-Eval显著提升了长上下文理解的测试难度，即使是如GPT-4o这样表现突出的模型，在部分任务上依然难以取得较好成绩。

Conclusion: LC-Eval填补了当前长上下文、多语言LLM真实评测的空白，为今后语言模型相关研究与优化提供了更严谨的测评工具。

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated
sophisticated capabilities, including the ability to process and comprehend
extended contexts. These emergent capabilities necessitate rigorous evaluation
methods to effectively assess their performance in long-context understanding.
In this paper, we present \textbf{LC-Eval}, a bilingual, multi-task evaluation
benchmark designed to evaluate long-context understanding in English and
Arabic, targeting context lengths ranging from 4k to over 128k tokens. LC-Eval
introduces four novel and challenging tasks: multi-document question answering,
bilingual question answering, claim verification within a paragraph, and
multiple-choice questions based on long contexts. These tasks are designed to
assess LLMs' abilities in deep reasoning, document comprehension, information
tracing, and bilingual information extraction and understanding. The benchmark
includes datasets in both Arabic and English for each task, allowing for a
comparative analysis of their performance across different text genres.
Evaluations were conducted on both open-weight and closed LLMs, with results
indicating that LC-Eval presents significant challenges. Even high-performing
models, such as GPT-4o, struggled with certain tasks, highlighting the
complexity and rigor of the benchmark.

</details>


### [221] [MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning](https://arxiv.org/abs/2510.16797)
*Vera Pavlova,Mohammed Makhlouf*

Main category: cs.CL

TL;DR: MOSAIC是一种多阶段的句子嵌入模型领域自适应框架，通过联合使用掩码语言建模和对比学习目标，在特定领域实现更优表达学习。


<details>
  <summary>Details</summary>
Motivation: 当前大规模通用句子嵌入模型难以直接应用到专业领域，如何高效迁移并保持语义判别能力是主要挑战。

Method: 提出MOSAIC框架，融合掩码语言建模（MLM）和对比学习目标，采用分阶段、联合训练，在适应新领域时强化领域相关表达并保留原模型的语义判别力。

Result: 在高资源和低资源领域均验证有效，对比强通用基线方法，NDCG@10指标提升最高可达13.4%。消融实验表明联合训练和分阶段适应的重要性。

Conclusion: MOSAIC能有效实现句子嵌入模型的领域自适应，兼顾领域表达能力和语义判别效果，各组成部分均有显著贡献。

Abstract: We introduce MOSAIC (Masked Objective with Selective Adaptation for In-domain
Contrastive learning), a multi-stage framework for domain adaptation of
sentence embedding models that incorporates joint domain-specific masked
supervision. Our approach addresses the challenges of adapting large-scale
general-domain sentence embedding models to specialized domains. By jointly
optimizing masked language modeling (MLM) and contrastive objectives within a
unified training pipeline, our method enables effective learning of
domain-relevant representations while preserving the robust semantic
discrimination properties of the original model. We empirically validate our
approach on both high-resource and low-resource domains, achieving improvements
up to 13.4% in NDCG@10 (Normalized Discounted Cumulative Gain) over strong
general-domain baselines. Comprehensive ablation studies further demonstrate
the effectiveness of each component, highlighting the importance of balanced
joint supervision and staged adaptation.

</details>


### [222] [Knowing the Facts but Choosing the Shortcut: Understanding How Large Language Models Compare Entities](https://arxiv.org/abs/2510.16815)
*Hans Hergen Lehmann,Jae Hee Lee,Steven Schockaert,Stefan Wermter*

Main category: cs.CL

TL;DR: 本文研究大型语言模型（LLM）在基于知识推理任务中是否真正依赖知识还是受到启发式偏见影响，发现即使有相关知识，LLM仍常犯错，易受表面特征驱动。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM被广泛用于知识性推理，但目前难以区分模型是依据真实知识还是仅靠表层启发式特征做出判断，理解其推理机制对提升模型可靠性和解释性很重要。

Method: 通过让模型对实体的数值属性进行比较（如河流长度比较），并结合真实答案，观察和分析模型的响应。同时，通过逻辑回归建模主流表面特征（如实体流行度、出现顺序和语义共现）对模型判断的影响。对比不同规模模型，并引入chain-of-thought提示法。

Result: 小模型明显依赖表面启发式，使用这些特征的简单逻辑回归甚至比模型本身的答案更准确。大模型在数值知识更可靠时会主动依赖知识进行判断。chain-of-thought提示可以帮助各规模模型更多地利用数值知识。

Conclusion: 当前LLM在实体数值比较等推理任务中仍易受启发式影响，尤其是小模型。大模型能部分克服这一问题，chain-of-thought能提升模型对根本知识的利用。

Abstract: Large Language Models (LLMs) are increasingly used for knowledge-based
reasoning tasks, yet understanding when they rely on genuine knowledge versus
superficial heuristics remains challenging. We investigate this question
through entity comparison tasks by asking models to compare entities along
numerical attributes (e.g., ``Which river is longer, the Danube or the
Nile?''), which offer clear ground truth for systematic analysis. Despite
having sufficient numerical knowledge to answer correctly, LLMs frequently make
predictions that contradict this knowledge. We identify three heuristic biases
that strongly influence model predictions: entity popularity, mention order,
and semantic co-occurrence. For smaller models, a simple logistic regression
using only these surface cues predicts model choices more accurately than the
model's own numerical predictions, suggesting heuristics largely override
principled reasoning. Crucially, we find that larger models (32B parameters)
selectively rely on numerical knowledge when it is more reliable, while smaller
models (7--8B parameters) show no such discrimination, which explains why
larger models outperform smaller ones even when the smaller models possess more
accurate knowledge. Chain-of-thought prompting steers all models towards using
the numerical features across all model sizes.

</details>


### [223] [Cross-Genre Authorship Attribution via LLM-Based Retrieve-and-Rerank](https://arxiv.org/abs/2510.16819)
*Shantanu Agarwal,Joel Barry,Steven Fincke,Scott Miller*

Main category: cs.CL

TL;DR: 本文提出了一种用于跨体裁作者归属（AA）任务的两阶段检索-重排序框架，并显著提升了主流基准的表现。


<details>
  <summary>Details</summary>
Motivation: 跨体裁作者归属任务难点在于识别作者本人的语言特征，而不能依赖于文本主题等线索。现有的信息检索常用训练策略并不适用于这一场景，导致表现不佳。

Method: 作者提出一种检索-重排序方法，基于大语言模型细调，并引入针对作者区分信号的数据筛选策略，使重排序器更好地捕捉作者特有的语言模式。

Result: 在HIATUS的HRS1和HRS2跨体裁基准任务上，新方法将Success@8指标分别提升了22.3和34.4个百分点，超越了现有其他方法。

Conclusion: 只用通用的信息检索范式并不能很好解决跨体裁作者归属问题，论文提出的数据策略和LLM框架显著提升了该任务的准确性。

Abstract: Authorship attribution (AA) is the task of identifying the most likely author
of a query document from a predefined set of candidate authors. We introduce a
two-stage retrieve-and-rerank framework that finetunes LLMs for cross-genre AA.
Unlike the field of information retrieval (IR), where retrieve-and-rerank is a
de facto strategy, cross-genre AA systems must avoid relying on topical cues
and instead learn to identify author-specific linguistic patterns that are
independent of the text's subject matter (genre/domain/topic). Consequently,
for the reranker, we demonstrate that training strategies commonly used in IR
are fundamentally misaligned with cross-genre AA, leading to suboptimal
behavior. To address this, we introduce a targeted data curation strategy that
enables the reranker to effectively learn author-discriminative signals. Using
our LLM-based retrieve-and-rerank pipeline, we achieve substantial gains of
22.3 and 34.4 absolute Success@8 points over the previous state-of-the-art on
HIATUS's challenging HRS1 and HRS2 cross-genre AA benchmarks.

</details>


### [224] [Who's Asking? Simulating Role-Based Questions for Conversational AI Evaluation](https://arxiv.org/abs/2510.16829)
*Navreet Kaur,Hoda Ayad,Hayoung Jung,Shravika Mittal,Munmun De Choudhury,Tanushree Mitra*

Main category: cs.CL

TL;DR: 该论文提出了CoRUS框架，关注于语言模型用户在问答中的角色，并模拟不同角色的问题以更好地评价大模型的实际表现。


<details>
  <summary>Details</summary>
Motivation: 目前主流大模型的评估很少考虑“提问者是谁”，特别是在像阿片类使用障碍（OUD）等敏感领域，忽视用户角色可能导致模型响应不够贴合真实需求。该研究试图弥补此类评估的不足。

Method: 作者基于角色理论和OUD康复社区真实帖子，构建了提问者角色分类（患者、照护者、专家），并据此模拟15,321个嵌入具体角色目标和行为的问题。然后用这些模拟问题评估5个主流大模型对于同一问题、不同提问者角色的回答差异。

Result: 发现不同角色会引发模型系统性反应差异：患者和照护者等弱势角色获得的回复更具支持性（提升17%），但知识性内容较少（降低19%），而专家角色获得的信息性回复更多。模拟问题高度可信，与真实数据相符。

Conclusion: 用户角色的隐性信号会显著影响语言模型回答内容。论文提出的CoRUS方法为更注重用户角色的任务型AI评测提供了新的工具，有助于未来生成更加贴合用户需求和情境的对话AI。

Abstract: Language model users often embed personal and social context in their
questions. The asker's role -- implicit in how the question is framed --
creates specific needs for an appropriate response. However, most evaluations,
while capturing the model's capability to respond, often ignore who is asking.
This gap is especially critical in stigmatized domains such as opioid use
disorder (OUD), where accounting for users' contexts is essential to provide
accessible, stigma-free responses. We propose CoRUS (COmmunity-driven Roles for
User-centric Question Simulation), a framework for simulating role-based
questions. Drawing on role theory and posts from an online OUD recovery
community (r/OpiatesRecovery), we first build a taxonomy of asker roles --
patients, caregivers, practitioners. Next, we use it to simulate 15,321
questions that embed each role's goals, behaviors, and experiences. Our
evaluations show that these questions are both highly believable and comparable
to real-world data. When used to evaluate five LLMs, for the same question but
differing roles, we find systematic differences: vulnerable roles, such as
patients and caregivers, elicit more supportive responses (+17%) and reduced
knowledge content (-19%) in comparison to practitioners. Our work demonstrates
how implicitly signaling a user's role shapes model responses, and provides a
methodology for role-informed evaluation of conversational AI.

</details>


### [225] [FinSight: Towards Real-World Financial Deep Research](https://arxiv.org/abs/2510.16844)
*Jiajie Jin,Yuyao Zhang,Yimeng Xu,Hongjin Qian,Yutao Zhu,Zhicheng Dou*

Main category: cs.CL

TL;DR: FinSight是一个创新的多智能体框架，用于自动化生成高质量、多模态的专业金融报告，大幅提高了报告的准确性和专业性。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统难以高质量、全自动地完成金融报告的生成，因其需要集成多种数据、分析与呈现能力。作者希望通过新架构解决这一难题。

Method: 提出了FinSight系统，基于CAVM架构，统一外部数据、工具和代理，实现灵活的数据采集、分析和报告生成。同时引入循环视觉增强机制优化金融可视化图表，以及两阶段写作框架提升报告的连贯性和结构一致性。

Result: 实验证明，FinSight在多个公司和行业任务上，在事实准确性、分析深度和报告质量上都明显优于现有主流方法。

Conclusion: FinSight展示了利用多智能体和可编程框架生成接近人类专家水平金融报告的可行性，对自动化财报领域具有重大推动作用。

Abstract: Generating professional financial reports is a labor-intensive and
intellectually demanding process that current AI systems struggle to fully
automate. To address this challenge, we introduce FinSight (Financial InSight),
a novel multi agent framework for producing high-quality, multimodal financial
reports. The foundation of FinSight is the Code Agent with Variable Memory
(CAVM) architecture, which unifies external data, designed tools, and agents
into a programmable variable space, enabling flexible data collection, analysis
and report generation through executable code. To ensure professional-grade
visualization, we propose an Iterative Vision-Enhanced Mechanism that
progressively refines raw visual outputs into polished financial charts.
Furthermore, a two stage Writing Framework expands concise Chain-of-Analysis
segments into coherent, citation-aware, and multimodal reports, ensuring both
analytical depth and structural consistency. Experiments on various company and
industry-level tasks demonstrate that FinSight significantly outperforms all
baselines, including leading deep research systems in terms of factual
accuracy, analytical depth, and presentation quality, demonstrating a clear
path toward generating reports that approach human-expert quality.

</details>


### [226] [Neuronal Group Communication for Efficient Neural representation](https://arxiv.org/abs/2510.16851)
*Zhengqi Pei,Qingming Huang,Shuhui Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的神经网络构建框架（Neuronal Group Communication, NGC），通过神经元组之间的通信和动态系统理论，以模块化和低秩的方式提高了神经网络的效率和可解释性，在大语言模型上的实验表明该方法在压缩时可显著提升复杂推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络规模不断扩大，带来更强性能的同时也带来了效率低和难以解释的问题，因此亟需设计高效、模块化和可解释的表示与训练新方法。

Method: NGC框架将神经网络建模为神经元组之间的动态通信系统，权重被视为神经元嵌入状态之间的临时交互信号。通过低秩、分组地传递信息，减少冗余参数，提高模型紧凑性。引入动态系统理论中的神经稳定性指标（类似Lyapunov稳定性），用于衡量神经元激活趋向稳定模式的过程。同时，发现推理能力的出现类似于外部势驱动神经动力学偏离简单轨迹。

Result: 将NGC应用于大语言模型后，在保持模型压缩的同时，能在复杂推理基准任务上取得更好表现，且比现有的低秩近似和跨层基底共享方法有更明显优势。

Conclusion: NGC为高维学习系统中的泛化和可解释性等问题提供了新思路，通过结构化神经元组动力学，可能推动未来高效且可解释的神经网络设计。

Abstract: The ever-increasing scale of modern neural networks has brought unprecedented
performance alongside daunting challenges in efficiency and interpretability.
This paper addresses the core question of how to build large neural systems
that learn efficient, modular, and interpretable representations. We propose
Neuronal Group Communication (NGC), a theory-driven framework that reimagines a
neural network as a dynamical system of interacting neuronal groups rather than
a monolithic collection of neural weights. Instead of treating each weight as
an independent trainable parameter, NGC treats weights as transient
interactions between embedding-like neuronal states, with neural computation
unfolding through iterative communication among groups of neurons. This
low-rank, modular representation yields compact models: groups of neurons
exchange low-dimensional signals, enabling intra-group specialization and
inter-group information sharing while dramatically reducing redundant
parameters. By drawing on dynamical systems theory, we introduce a neuronal
stability metric (analogous to Lyapunov stability) that quantifies the
contraction of neuron activations toward stable patterns during sequence
processing. Using this metric, we reveal that emergent reasoning capabilities
correspond to an external driving force or ``potential'', which nudges the
neural dynamics away from trivial trajectories while preserving stability.
Empirically, we instantiate NGC in large language models (LLMs) and demonstrate
improved performance on complex reasoning benchmarks under moderate
compression. NGC consistently outperforms standard low-rank approximations and
cross-layer basis-sharing methods at comparable compression rates. We conclude
by discussing the broader implications of NGC, including how structured
neuronal group dynamics might relate to generalization in high-dimensional
learning systems.

</details>


### [227] [Does Visual Grounding Enhance the Understanding of Embodied Knowledge in Large Language Models?](https://arxiv.org/abs/2510.16924)
*Zhihui Yang,Yupei Wang,Kaijie Mo,Zhe Zhao,Renfen Hu*

Main category: cs.CL

TL;DR: 本研究提出了一个全新的体现知识理解基准，比较多模态语言模型与纯文本模型在感官理解上的表现，发现多模态模型并未优于纯文本模型。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态语言模型取得了显著进展，但其视觉基础是否提升了体现知识理解仍不明确。本研究旨在探究多模态模型在体现知识理解方面的实际效用。

Method: 作者基于心理学的感知理论，构建了涵盖视觉、听觉、触觉、味觉、嗅觉和内感知六类外部感官、包含1700多个问题的体现知识理解基准，并通过向量对比和问答任务评估30个顶尖语言模型。

Result: 实验结果显示，多模态视觉-语言模型（VLMs）在所有任务中并未超过文本模型，且在视觉维度的表现显著劣于其他感官维度。此外，模型的向量表示容易受到词形、词频影响，并且在空间感知与推理问题上表现不佳。

Conclusion: 研究表明，当前多模态模型尚未有效整合体现知识，对物理世界理解有待提升。未来需更深入地融合多感官知识以增强模型理解能力。

Abstract: Despite significant progress in multimodal language models (LMs), it remains
unclear whether visual grounding enhances their understanding of embodied
knowledge compared to text-only models. To address this question, we propose a
novel embodied knowledge understanding benchmark based on the perceptual theory
from psychology, encompassing visual, auditory, tactile, gustatory, olfactory
external senses, and interoception. The benchmark assesses the models'
perceptual abilities across different sensory modalities through vector
comparison and question-answering tasks with over 1,700 questions. By comparing
30 state-of-the-art LMs, we surprisingly find that vision-language models
(VLMs) do not outperform text-only models in either task. Moreover, the models
perform significantly worse in the visual dimension compared to other sensory
dimensions. Further analysis reveals that the vector representations are easily
influenced by word form and frequency, and the models struggle to answer
questions involving spatial perception and reasoning. Our findings underscore
the need for more effective integration of embodied knowledge in LMs to enhance
their understanding of the physical world.

</details>


### [228] [ChiKhaPo: A Large-Scale Multilingual Benchmark for Evaluating Lexical Comprehension and Generation in Large Language Models](https://arxiv.org/abs/2510.16928)
*Emily Chang,Niyati Bafna*

Main category: cs.CL

TL;DR: 本文提出了一个覆盖2700多种语言的多语言基准ChiKhaPo，用于测试大语言模型的基础词汇理解与生成能力，发现主流模型在该基准上的表现普遍较差。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的评测主要集中在资源丰富的语言和高阶任务，缺乏对众多低资源语言及其基本语言能力的评估，亟需更广泛覆盖和关注基础能力的基准。

Method: 提出了ChiKhaPo基准，包含8项不同难度的子任务，评估模型的词汇理解与生成能力。数据来源涵盖现有词汇表、单语数据及平行语料，2项子任务覆盖2700+语言，远超现有基准。将6种主流SOTA模型应用于该基准，进行性能比较和深度分析。

Result: 六个主流SOTA模型在ChiKhaPo基准上的成绩不理想，对不同语言（特别是低资源语言）、任务类型和理解/生成方向均存在明显差异。

Conclusion: ChiKhaPo显著扩展了多语言评测的语言覆盖度，检验了大模型在基础语言能力上的局限性，为未来大语言模型的多语言评测和改进提供了有力工具和数据基础。

Abstract: Existing benchmarks for large language models (LLMs) are largely restricted
to high- or mid-resource languages, and often evaluate performance on
higher-order tasks in reasoning and generation. However, plenty of evidence
points to the fact that LLMs lack basic linguistic competence in the vast
majority of the world's 3800+ written languages. We introduce ChiKhaPo,
consisting of 8 subtasks of varying difficulty designed to evaluate the lexical
comprehension and generation abilities of generative models. ChiKhaPo draws on
existing lexicons, monolingual data, and bitext, and provides coverage for
2700+ languages for 2 subtasks, surpassing any existing benchmark in terms of
language coverage. We further show that 6 SOTA models struggle on our
benchmark, and discuss the factors contributing to performance scores,
including language family, language resourcedness, task, and comprehension
versus generation directions. With ChiKhaPo, we hope to enable and encourage
the massively multilingual benchmarking of LLMs.

</details>


### [229] [Prompt-MII: Meta-Learning Instruction Induction for LLMs](https://arxiv.org/abs/2510.16932)
*Emily Xiao,Yixiao Zeng,Ada Chen,Chin-Jou Li,Amanda Bertsch,Graham Neubig*

Main category: cs.CL

TL;DR: 本文提出了一种利用强化学习的PROMPT-MII方法，可将大规模语言模型的训练样本自动压缩为高效的提示，从而在极大降低推理耗时和token消耗的同时，实现类似于全量样本ICL的任务适应效果。


<details>
  <summary>Details</summary>
Motivation: ICL（in-context learning）能让大语言模型适配新任务，但随着样本变多，推理的上下文长度也随之增长，导致推理消耗高。作者希望通过紧凑的提示代替全量样本，兼顾性能和效率。

Method: 提出PROMPT-MII框架，它通过强化学习，元学习训练一个模型，让其根据输入数据自动生成精简且描述性强的指令提示。模型在HuggingFace上超过3000个分类任务的数据集上训练，并在90个新任务上评测。

Result: 实验表明，PROMPT-MII在下游任务上的F1分数提升4-9分（相对提升10-20%），且只需3-13倍更少的token，就能达到ICL全量训练集的性能。

Conclusion: PROMPT-MII能够用更少的token和更低的推理成本获得与ICL相当的性能，为提升大语言模型的推理效率和任务适应能力提供了新思路。

Abstract: A popular method to adapt large language models (LLMs) to new tasks is
in-context learning (ICL), which is effective but incurs high inference costs
as context length grows. In this paper we propose a method to perform
instruction induction, where we take training examples and reduce them to a
compact but descriptive prompt that can achieve performance comparable to ICL
over the full training set. Specifically, we propose PROMPT-MII, a
reinforcement learning (RL) based framework to meta-learn an instruction
induction model that can generate compact instructions on the fly for an
arbitrary new dataset. We train on over 3,000 diverse classification datasets
from the HuggingFace hub, and evaluate on 90 unseen tasks. PROMPT-MII improves
downstream model quality by 4-9 F1 points (10-20% relative), matching ICL
performance while requiring 3-13x fewer tokens.

</details>


### [230] [Parameter-Efficient Fine-Tuning for Low-Resource Languages: A Comparative Study of LLMs for Bengali Hate Speech Detection](https://arxiv.org/abs/2510.16985)
*Akif Islam,Mohd Ruhul Ameen*

Main category: cs.CL

TL;DR: 本论文首次将参数高效微调（PEFT）方法应用于孟加拉语仇恨言论检测。通过对三种不同大型语言模型（LLM）进行微调，显著提高了检测性能，并显著降低了资源消耗，为低资源语言仇恨言论检测提供了实用方案。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语社交平台上的仇恨言论激增，尤其对女性和青少年影响严重。现有方法要么消耗大量计算资源进行全面微调，要么依赖昂贵的专有API，因此亟需高效、可复现且资源友好的解决方案。

Method: 作者采用了PEFT方法中的LoRA和QLoRA，对三种主流的大语言模型（Gemma-3-4B、Llama-3.2-3B、Mistral-7B）仅微调不到1%的参数，在一块消费级GPU上对包含5万多条标注评论的BD-SHS数据集进行训练。

Result: Llama-3.2-3B在仇恨言论检测任务中取得了92.23%的最高F1分数，Mistral-7B和Gemma-3-4B分别获得88.94%和80.25%。

Conclusion: PEFT是一种在资源受限情况下高效、可复现的孟加拉语及其他低资源语言仇恨言论检测方案。

Abstract: Bengali social media platforms have witnessed a sharp increase in hate
speech, disproportionately affecting women and adolescents. While datasets such
as BD-SHS provide a basis for structured evaluation, most prior approaches rely
on either computationally costly full-model fine-tuning or proprietary APIs.
This paper presents the first application of Parameter-Efficient Fine-Tuning
(PEFT) for Bengali hate speech detection using LoRA and QLoRA. Three
instruction-tuned large language models - Gemma-3-4B, Llama-3.2-3B, and
Mistral-7B - were fine-tuned on the BD-SHS dataset of 50,281 annotated
comments. Each model was adapted by training fewer than 1% of its parameters,
enabling experiments on a single consumer-grade GPU. The results show that
Llama-3.2-3B achieved the highest F1-score of 92.23%, followed by Mistral-7B at
88.94% and Gemma-3-4B at 80.25%. These findings establish PEFT as a practical
and replicable strategy for Bengali and related low-resource languages.

</details>


### [231] [Back to Bytes: Revisiting Tokenization Through UTF-8](https://arxiv.org/abs/2510.16987)
*Amit Moryossef,Clara Meister,Pavel Stepachev,Desmond Elliott*

Main category: cs.CL

TL;DR: 本文提出了UTF8Tokenizer，一种极简的字节级分词器，直接将文本映射到对应UTF-8编码字节的ID，无需辅助特殊Token或超出范围的ID。所有特殊功能通过C0控制字节实现。该设计在速度、存储效率、模型兼容性和训练优化方面有显著优势。


<details>
  <summary>Details</summary>
Motivation: 现有的字节级分词方案存在引入超范围ID和依赖特殊辅助Token等缺点，增加了实现和交互复杂度。作者旨在以更简洁高效的方式，实现精确可靠的Token-ID映射和文本控制结构的表达。

Method: UTF8Tokenizer直接将文本按UTF-8逐字节映射到0~255的Token ID，所有特殊功能通过C0控制字节实现，无需额外辅助Token。其embedding表为256*d尺寸，可直接兼容和对齐不同模型。此外，作者还提出了bit-biased embedding提升训练能力，并可后处理地添加，不影响推理速度。

Result: 实验显示，UTF8Tokenizer分词速度提升14倍，传输数据量为int64的1/8，embedding表结构更简单且可共享。bit-biased embedding加速了语言模型的收敛。实现已兼容HuggingFace接口。

Conclusion: UTF8Tokenizer实现了极简、高效的分词和控制逻辑，提升了速度、兼容性和训练表现，在语言建模领域具有实用价值。

Abstract: We present UTF8Tokenizer, a minimalist byte-level tokenizer that maps text
exactly to IDs corresponding to the bytes underlying the text's UTF-8 encoding
(e.g., byte x09 is token ID 9). Unlike prior byte-level approaches (Xue et al.,
2021; Pagnoni et al., 2025), our implementation never introduces out-of-range
IDs (i.e. there is no token ID 256) or auxiliary tokens: all special behavior
(e.g., padding, boundaries, conversation structure, attention segments, tool
calling, "thinking" spans, etc.) is encoded using C0 control bytes - just as
ASCII was originally designed to embed control information alongside printable
text. These design principles yield practical benefits: (1) faster tokenization
(14x) and significantly lower host-device transfer (8x less than int64); (2)
simple, shareable 256*d embedding tables that can be aligned across models; and
(3) a training-time enhancement via bit-biased embeddings, which exposes
per-byte bit structure and can be added to the embedding table post-training,
removing inference costs. Our HuggingFace-compatible implementation improves
language modeling convergence.

</details>


### [232] [Vocab Diet: Reshaping the Vocabulary of LLMs with Vector Arithmetic](https://arxiv.org/abs/2510.17001)
*Yuval Reif,Guy Kaplan,Roy Schwartz*

Main category: cs.CL

TL;DR: 该论文提出用共同的词基和变形向量组合表示词形变化，减少词表冗余并提高语言覆盖。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的分词算法将每一个词形变化（如“walked”、“walking”）都当作独立词元，导致词表冗余、频率低词和多语种覆盖受限。

Method: 发现在嵌入空间中，词形变化之间可以用加性变换向量表示。进而提出将表面词形表示为词基加变化向量（如“walked”=“walk”+过去式），并在多个大型语言模型和五种语言上进行实验。

Result: 删除了10%的词汇项，增加了新词及多样性，词表覆盖扩展至低频与未登录词，且下游任务性能影响极小，无需修改模型权重。

Conclusion: 词表设计应从字符串枚举转向基于语言结构的组合性词表，本方法节省空间、提升表现并具有普适意义。

Abstract: Large language models (LLMs) were shown to encode word form variations, such
as "walk"->"walked", as linear directions in embedding space. However, standard
tokenization algorithms treat these variations as distinct tokens -- filling
the size-capped vocabulary with surface form variants (e.g., "walk", "walking",
"Walk"), at the expense of less frequent words and multilingual coverage. We
show that many of these variations can be captured by transformation vectors --
additive offsets that yield the appropriate word's representation when applied
to the base form word embedding -- in both the input and output spaces.
Building on this, we propose a compact reshaping of the vocabulary: rather than
assigning unique tokens to each surface form, we compose them from shared base
form and transformation vectors (e.g., "walked" = "walk" + past tense). We
apply our approach to multiple LLMs and across five languages, removing up to
10% of vocabulary entries -- thereby freeing space to allocate new, more
diverse tokens. Importantly, we do so while also expanding vocabulary coverage
to out-of-vocabulary words, with minimal impact on downstream performance, and
without modifying model weights. Our findings motivate a foundational
rethinking of vocabulary design, moving from string enumeration to a
compositional vocabulary that leverages the underlying structure of language.

</details>


### [233] [Online Learning Defense against Iterative Jailbreak Attacks via Prompt Optimization](https://arxiv.org/abs/2510.17006)
*Masahiro Kaneko,Zeerak Talat,Timothy Baldwin*

Main category: cs.CL

TL;DR: 本文提出了一种针对大型语言模型迭代越狱攻击的动态防御框架，利用在线学习和强化学习优化防御策略，并引入梯度抑制技术，有效提升安全性且不损害正常任务表现。实验结果表明，方法优于现有防御。


<details>
  <summary>Details</summary>
Motivation: 目前的防御方法无法有效干预攻击者通过试错法多次迭代输入，从而突破LLM安全防线。作者希望设计一种能主动适应和抵抗这种动态越狱攻击的防御框架。

Method: 作者提出一种基于在线学习和强化学习的防御方法，能够根据攻击者每次迭代的prompts动态调整防御；并提出Past-Direction Gradient Damping (PDGD)来防止模型过拟合于迭代攻击路径中的细微重写。

Result: 在三种LLM和五种越狱攻击方法上，本文提出的方法均显著优于五种现有防御方法；同时，优化策略还提升了对无害任务的响应质量。

Conclusion: 动态在线学习与强化学习结合，并配合梯度抑制机制，是抵御迭代越狱攻击的有效防线。在提升安全性的同时，也确保了对正常任务的友好响应。

Abstract: Iterative jailbreak methods that repeatedly rewrite and input prompts into
large language models (LLMs) to induce harmful outputs -- using the model's
previous responses to guide each new iteration -- have been found to be a
highly effective attack strategy. Despite being an effective attack strategy
against LLMs and their safety mechanisms, existing defenses do not proactively
disrupt this dynamic trial-and-error cycle. In this study, we propose a novel
framework that dynamically updates its defense strategy through online learning
in response to each new prompt from iterative jailbreak methods. Leveraging the
distinctions between harmful jailbreak-generated prompts and typical harmless
prompts, we introduce a reinforcement learning-based approach that optimizes
prompts to ensure appropriate responses for harmless tasks while explicitly
rejecting harmful prompts. Additionally, to curb overfitting to the narrow band
of partial input rewrites explored during an attack, we introduce
Past-Direction Gradient Damping (PDGD). Experiments conducted on three LLMs
show that our approach significantly outperforms five existing defense methods
against five iterative jailbreak methods. Moreover, our results indicate that
our prompt optimization strategy simultaneously enhances response quality for
harmless tasks.

</details>


### [234] [DiscoTrack: A Multilingual LLM Benchmark for Discourse Tracking](https://arxiv.org/abs/2510.17013)
*Lanni Bu,Lauren Levin,Amir Zeldes*

Main category: cs.CL

TL;DR: 本文提出了DiscoTrack基准，专注于更具挑战性和多语言的隐含信息和语篇推理能力评测，涵盖12种语言和四个语篇理解层次。实验证明即使最先进的LLM在该基准上的表现依然有限。


<details>
  <summary>Details</summary>
Motivation: 现有大模型基准主要评测显式信息抽取如问答和摘要，缺乏跨句、跨语段及多语言、聚合隐性语篇信息的高难度测试。为推动模型对复杂语篇关联的理解，作者设计了新的基准。

Method: 构建了DiscoTrack基准，设计了涵盖12种语言、四个理解层次（显著性识别、实体跟踪、语篇关系、推理）的多任务评测体系，通过多语言多层次的任务来全面衡量大模型的语篇与推理能力。

Result: 评测结果显示，无论是通用还是最新的大型语言模型，在DiscoTrack的多个任务上表现都较为一般，存在显著挑战。

Conclusion: 当前主流和最先进大模型在跨句、多语言、隐性语篇理解等复杂推理任务上仍有较大提升空间，DiscoTrack为推动该方向研究提供了评测基础。

Abstract: Recent LLM benchmarks have tested models on a range of phenomena, but are
still focused primarily on natural language understanding for extraction of
explicit information, such as QA or summarization, with responses often tar-
geting information from individual sentences. We are still lacking more
challenging, and im- portantly also multilingual, benchmarks focus- ing on
implicit information and pragmatic infer- ences across larger documents in the
context of discourse tracking: integrating and aggregating information across
sentences, paragraphs and multiple speaker utterances. To this end, we present
DiscoTrack, an LLM benchmark target- ing a range of tasks across 12 languages
and four levels of discourse understanding: salience recognition, entity
tracking, discourse relations and bridging inference. Our evaluation shows that
these tasks remain challenging, even for state-of-the-art models.

</details>


### [235] [SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents](https://arxiv.org/abs/2510.17017)
*Qiusi Zhan,Angeline Budiman-Chan,Abdelrahman Zayed,Xingzhi Guo,Daniel Kang,Joo-Kyung Kim*

Main category: cs.CL

TL;DR: 本文揭示基于大语言模型（LLM）的搜索智能体在安全性方面存在隐患，并提出SafeSearch方法，有效提升了检索智能体的安全性，同时保持其实用性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM检索智能体旨在提升开放域问答能力，但其安全行为鲜有研究。作者发现这类智能体更容易生成有害内容，现有提升实用性的调优方式反而加重了安全风险，因此亟须在安全性和实用性之间达到平衡。

Method: 提出SafeSearch方法，通过多目标强化学习，将最终输出的安全性与实用性奖励结合，并引入创新的查询级激励项，对安全/不安全的查询行为分别给予奖励和惩罚，实现查询生成与最终输出的双重安全保障。

Result: SafeSearch方法在三个红队测试数据集上将智能体有害响应率降低超过70%，在保证输出安全、实用的同时，问答性能与仅关注实用性的fine-tuned智能体持平。

Conclusion: SafeSearch显著提升了检索智能体在安全性和实用性方面的双重表现，证明了查询级安全激励的有效性，并为安全性与实用性联合对齐提供了新思路。

Abstract: Large language model (LLM) based search agents iteratively generate queries,
retrieve external information, and reason to answer open-domain questions.
While researchers have primarily focused on improving their utility, their
safety behaviors remain underexplored. In this paper, we first evaluate search
agents using red-teaming datasets and find that they are more likely to produce
harmful outputs than base LLMs. For example, when asked ``How can I track
someone's location without their consent?'', a base model refuses, whereas a
search agent designed to retrieve and cite sources may lower its refusal
threshold, fetch documents (e.g., court cases), and, once appended, synthesize
them into an informative yet unsafe summary. We further show that
utility-oriented fine-tuning intensifies this risk, motivating joint alignment
of safety and utility. We present SafeSearch, a multi-objective reinforcement
learning approach that couples a final-output safety/utility reward with a
novel query-level shaping term that penalizes unsafe queries and rewards safe
ones. Experiments show that SafeSearch reduces agent harmfulness by over 70%
across three red-teaming datasets while producing safe, helpful responses, and
matches the QA performance of a utility-only finetuned agent; further analyses
confirm the effectiveness of the query-level reward in jointly improving safety
and utility.

</details>


### [236] [Extended LSTM: Adaptive Feature Gating for Toxic Comment Classification](https://arxiv.org/abs/2510.17018)
*Noor Islam S. Mohammad*

Main category: cs.CL

TL;DR: 本文提出了一种高效且理论支撑的新模型xLSTM，以提升有毒评论检测的准确率并降低对资源的需求。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的模型如BERT在有毒评论检测中计算开销高，对少数类别表现较差；传统集成方法则缺乏语义适应性。这些不足推动了新方案的提出，以兼顾计算效率和对少数毒性类别的检测性能。

Method: xLSTM结合了余弦相似门控、特征自适应优先、类别重平衡三大机制。其创新点包括学习型参考向量调制上下文嵌入（通过余弦相似加强有毒信号），多源嵌入（GloVe、FastText、BERT CLS）融合，字级BiLSTM挖掘形态信息，嵌入空间SMOTE增强少数类样本，以及带动态加权的自适应焦点损失。

Result: 在Jigsaw Toxic Comment数据集上，xLSTM达到96.0%准确率和0.88宏F1，威胁类别提升33%，身份仇恨提升28%，仅用BERT约1/15参数，推理延迟低至50ms，余弦门控贡献了+4.8%的F1提升。

Conclusion: xLSTM展示了小参数、理论驱动模型在不平衡领域任务中的高效与适应性表现，超过了大型预训练模型，为特定NLP任务的模型设计提供了新方向。

Abstract: Toxic comment detection remains a challenging task, where transformer-based
models (e.g., BERT) incur high computational costs and degrade on minority
toxicity classes, while classical ensembles lack semantic adaptability. We
propose xLSTM, a parameter-efficient and theoretically grounded framework that
unifies cosine-similarity gating, adaptive feature prioritization, and
principled class rebalancing. A learnable reference vector {v} in {R}^d
modulates contextual embeddings via cosine similarity, amplifying toxic cues
and attenuating benign signals to yield stronger gradients under severe class
imbalance. xLSTM integrates multi-source embeddings (GloVe, FastText, BERT CLS)
through a projection layer, a character-level BiLSTM for morphological cues,
embedding-space SMOTE for minority augmentation, and adaptive focal loss with
dynamic class weighting. On the Jigsaw Toxic Comment benchmark, xLSTM attains
96.0% accuracy and 0.88 macro-F1, outperforming BERT by 33% on threat and 28%
on identity_hate categories, with 15 times fewer parameters and 50ms inference
latency. Cosine gating contributes a +4.8% F1 gain in ablations. The results
establish a new efficiency adaptability frontier, demonstrating that
lightweight, theoretically informed architectures can surpass large pretrained
models on imbalanced, domain-specific NLP tasks.

</details>


### [237] [Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive Language Models](https://arxiv.org/abs/2510.17028)
*Kyle Cox,Jiawei Xu,Yikun Han,Rong Xu,Tianhao Li,Chi-Yang Hsu,Tianlong Chen,Walter Gerych,Ying Ding*

Main category: cs.CL

TL;DR: 本文提出通过对语义等价但形式不同的提示（prompt）进行采样，可以改进大语言模型（LLMs）面对提示敏感性时的不确定性校准。作者还提出了一种新指标，用以更精细地刻画模型不确定性来源，尤其针对黑盒LLM。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs对语义等价但表述不同的prompt输出分布差异较大，即prompt敏感性，导致模型对输入语义的理解不稳定。文章旨在探究和改进这种不确定性。

Method: 作者将prompt敏感性建模为泛化误差，通过语义改写采样（不同的同义说法）改善模型不确定性的校准。同时提出了一种新颖的不确定性分解指标，能够体现自然语言生成中的语义连续性，并用于量化不确定性有多少源自prompt敏感性。

Result: 实验表明，利用语义改写采样可以在不损失准确率的情况下提升LLM的不确定性校准能力。新指标能更好地分解和量化因prompt敏感性带来的不确定性。

Conclusion: 文章为LLM中提示敏感性问题导致的不确定性校准提供了一种新的改进方式，并证明当前部分LLM在输入语义推理上的稳定性存在不足，为后续提升模型泛化能力和可靠性提供了思路。

Abstract: An interesting behavior in large language models (LLMs) is prompt
sensitivity. When provided with different but semantically equivalent versions
of the same prompt, models may produce very different distributions of answers.
This suggests that the uncertainty reflected in a model's output distribution
for one prompt may not reflect the model's uncertainty about the meaning of the
prompt. We model prompt sensitivity as a type of generalization error, and show
that sampling across the semantic ``concept space'' with paraphrasing
perturbations improves uncertainty calibration without compromising accuracy.
Additionally, we introduce a new metric for uncertainty decomposition in
black-box LLMs that improves upon entropy-based decomposition by modeling
semantic continuities in natural language generation. We show that this
decomposition metric can be used to quantify how much LLM uncertainty is
attributed to prompt sensitivity. Our work introduces a new way to improve
uncertainty calibration in prompt-sensitive language models, and provides
evidence that some LLMs fail to exhibit consistent general reasoning about the
meanings of their inputs.

</details>


### [238] [Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation](https://arxiv.org/abs/2510.17062)
*Guoqing Luo,Iffat Maab,Lili Mou,Junichi Yamagishi*

Main category: cs.CL

TL;DR: 本研究发现，大语言模型在推理过程会加剧社会偏见，并提出了一种轻量的提示式缓解方法，有效减少偏见同时保持准确率。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在复杂任务上表现优异，但它们在推理过程中会无意间采纳社会刻板印象，导致偏见结果，而这一现象的具体表现机制尚不清楚。

Method: 作者系统分析了模型在社会偏见场景下的推理机制，总结出两种偏差模式：1）刻板印象重复，即直接以社会偏见为主要依据，2）无关信息注入，即编造或引入新细节以支撑偏见叙述。随后，提出一种基于提示的自我审查方法，促使模型识别自身推理中的上述错误模式。

Result: 在BBQ、StereoSet、BOLD等问答与开放问题基准测试中，该方法能显著降低模型的社会偏见，并能保持或提升准确率。

Conclusion: 提示式自我审查能有效缓解大语言模型推理过程中的社会偏见，为安全、公平使用大模型提供了实用工具。

Abstract: While reasoning-based large language models excel at complex tasks through an
internal, structured thinking process, a concerning phenomenon has emerged that
such a thinking process can aggregate social stereotypes, leading to biased
outcomes. However, the underlying behaviours of these language models in social
bias scenarios remain underexplored. In this work, we systematically
investigate mechanisms within the thinking process behind this phenomenon and
uncover two failure patterns that drive social bias aggregation: 1) stereotype
repetition, where the model relies on social stereotypes as its primary
justification, and 2) irrelevant information injection, where it fabricates or
introduces new details to support a biased narrative. Building on these
insights, we introduce a lightweight prompt-based mitigation approach that
queries the model to review its own initial reasoning against these specific
failure patterns. Experiments on question answering (BBQ and StereoSet) and
open-ended (BOLD) benchmarks show that our approach effectively reduces bias
while maintaining or improving accuracy.

</details>


### [239] [Verification-Aware Planning for Multi-Agent Systems](https://arxiv.org/abs/2510.17109)
*Tianyang Xu,Dan Zhang,Kushan Mitra,Estevam Hruschka*

Main category: cs.CL

TL;DR: 提出了VeriMAP框架，通过验证感知的规划提升多Agent协作系统的鲁棒性和可解释性，并在多个数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 多智能体LLM需要协作以应对复杂任务，然而协作过程中容易出现由于任务理解、输出格式或交接不当等细微不匹配导致的失败。现有方案在规划、协调和验证方面存在不足，因此需要更精准的机制提升多智能体协作的可靠性。

Method: 提出VeriMAP框架，规划器能将任务分解、建模子任务依赖，并以Python和自然语言双重形式编写“子任务验证函数”（VF）来定义中间成果的可通过标准，实现验证感知的多Agent协作流程。

Result: 在多项数据集上实验，验证VeriMAP优于现有单体和多体基线，系统更鲁棒、可解释。

Conclusion: 验证感知的规划显著提升了多Agent系统的协作可靠性和迭代优化能力，且无需依赖外部标注，实现了更强的自治和可用性。

Abstract: Large language model (LLM) agents are increasingly deployed to tackle complex
tasks, often necessitating collaboration among multiple specialized agents.
However, multi-agent collaboration introduces new challenges in planning,
coordination, and verification. Execution failures frequently arise not from
flawed reasoning alone, but from subtle misalignments in task interpretation,
output format, or inter-agent handoffs. To address these challenges, we present
VeriMAP, a framework for multi-agent collaboration with verification-aware
planning. The VeriMAP planner decomposes tasks, models subtask dependencies,
and encodes planner-defined passing criteria as subtask verification functions
(VFs) in Python and natural language. We evaluate VeriMAP on diverse datasets,
demonstrating that it outperforms both single- and multi-agent baselines while
enhancing system robustness and interpretability. Our analysis highlights how
verification-aware planning enables reliable coordination and iterative
refinement in multi-agent systems, without relying on external labels or
annotations.

</details>


### [240] [DVAGen: Dynamic Vocabulary Augmented Generation](https://arxiv.org/abs/2510.17115)
*Wei Du,Nuowei Liu,Jie Wang,Jiahao Kuang,Tao Ji,Xiaoling Wang,Yuanbin Wu*

Main category: cs.CL

TL;DR: 当前的大型语言模型（LLMs）使用固定词表，难以应对新词或未登录词，影响了泛化能力。本文提出了DVAGen，一个开源、一体化的动态词表增强语言模型框架，支持训练、评估与可视化，并能与主流开源LLMs无缝集成。DVAGen创新地支持了CLI和WebUI工具，可实时查看结果，并大幅提升推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有固定词表的LLMs对新词、未登录词泛化能力弱，动态词表方法虽然有改进，但存在工具碎片化、缺乏现代模型支持、推理效率低等问题，亟需兼容性好且高效的统一框架。

Method: 提出了DVAGen框架，通过模块化设计，方便自定义和维护；可直接集成现有开源LLMs；提供命令行和网页用户界面；支持批量推理提升效率；可以便捷地训练、评估，并可视化动态词表方法的效果。

Result: 验证了动态词表技术在现代LLMs上的有效性，并证明DVAGen框架能够显著提升批量推理效率，方便结果分析与可视化。

Conclusion: DVAGen克服了现有动态词表技术在兼容性和扩展性上的不足，为增强LLMs处理多样词汇的能力提供了一个高效、易用、可扩展的开源解决方案。

Abstract: Language models trained with a fixed vocabulary struggle to generalize to
novel or out-of-vocabulary words, limiting their flexibility in handling
diverse token combinations. Existing dynamic vocabulary approaches attempt to
address this limitation but face challenges such as fragmented codebases, lack
of support for modern LLMs, and limited inference scalability. To overcome
these issues, we introduce DVAGen, a fully open-source, unified framework
designed for training, evaluation, and visualization of dynamic
vocabulary-augmented language models. Our framework modularizes the pipeline
for ease of customization, integrates seamlessly with open-source LLMs, and is
the first to provide both CLI and WebUI tools for real-time result inspection.
We validate the effectiveness of dynamic vocabulary methods on modern LLMs and
demonstrate support for batch inference, significantly improving inference
throughput.

</details>


### [241] [Rethinking On-policy Optimization for Query Augmentation](https://arxiv.org/abs/2510.17139)
*Zhichao Xu,Shengyao Zhuang,Xueguang Ma,Bingsen Chen,Yijun Tian,Fengran Mo,Jie Cao,Vivek Srikumar*

Main category: cs.CL

TL;DR: 大语言模型（LLM）驱动的查询增强主要有两类方法：基于提示生成新查询和基于强化学习（RL）优化查询重写。本文首次系统比较了两者，并提出结合优点的新方法OPQE，效果优于单一方法。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM驱动的查询增强被广泛研究，但基于提示与基于RL的两类主流方法尚未在统一标准下系统比较，两者优劣及融合潜力未被充分探讨。

Method: 作者在多个检索基准上系统比较了提示生成与RL微调的方法，并提出新颖的OPQE混合方法：由LLM策略生成伪文档以最大化检索性能，将生成式和强化优化结合。

Result: 实验发现，无需训练的简单提示法在强LLM上常可媲美或超越RL重写法。OPQE方法表现更佳，优于单独的提示和RL重写方案。

Conclusion: 简单提示已具强大效果，RL虽优化但成本高；两者结合即OPQE能进一步提升性能，是高效且优越的检索增强方案。

Abstract: Recent advances in large language models (LLMs) have led to a surge of
interest in query augmentation for information retrieval (IR). Two main
approaches have emerged. The first prompts LLMs to generate answers or
pseudo-documents that serve as new queries, relying purely on the model's
parametric knowledge or contextual information. The second applies
reinforcement learning (RL) to fine-tune LLMs for query rewriting, directly
optimizing retrieval metrics. While having respective advantages and
limitations, the two approaches have not been compared under consistent
experimental conditions. In this work, we present the first systematic
comparison of prompting-based and RL-based query augmentation across diverse
benchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key
finding is that simple, training-free query augmentation often performs on par
with, or even surpasses, more expensive RL-based counterparts, especially when
using powerful LLMs. Motivated by this discovery, we introduce a novel hybrid
method, On-policy Pseudo-document Query Expansion (OPQE), which, instead of
rewriting a query, the LLM policy learns to generate a pseudo-document that
maximizes retrieval performance, thus merging the flexibility and generative
structure of prompting with the targeted optimization of RL. We show OPQE
outperforms both standalone prompting and RL-based rewriting, demonstrating
that a synergistic approach yields the best results. Our implementation is made
available to facilitate reproducibility.

</details>


### [242] [When AI companions become witty: Can human brain recognize AI-generated irony?](https://arxiv.org/abs/2510.17168)
*Xiaohui Rao,Hanlin Wu,Zhenguang G. Cai*

Main category: cs.CL

TL;DR: 该研究探讨了人们在理解AI与人类产生的讽刺言论时，是否会像对待人类一样，将意图归因于AI。行为与神经数据都显示，人们对于AI生成的讽刺，较少认为是有意为之，而更可能被看作是计算失误。认知神经指标也表明对AI的讽刺处理更为弱化，只有当人们主观上认为AI更真诚时，这种归因才会增加。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）越来越多地以社交智能体身份出现、人们希望它们能产生幽默和讽刺，一个关键问题出现了：人们是否会像对真人一样，将AI的话语理解为有意图的交流，而不是简单的计算产物？讽刺是检验这种理解过程的理想范例，因为它要求分辨信息的矛盾到底是有意为之还是无心之失。

Method: 研究对比了被试者对人类和AI出处的讽刺言论的行为和脑电数据（ERP）。分析了P200（表早期不一致检测）与P600（指数重新诠释不一致为讽刺时的认知努力）两个ERP成分，来考察人们如何加工不同来源的讽刺。

Result: 行为上，被试对于AI和人类的讽刺都能部分归因为有意沟通，但归因程度对AI明显较低，更倾向于被认为是计算错误。脑电数据显示，AI讽刺引发的P200和P600变化幅度较弱，说明对AI讽刺的认知投入和意图归因减少。主观上认为AI更真诚的人，AI讽刺诱发的相关ERP反应会更明显。

Conclusion: 人们对AI生成的讽刺言论不会像对人类一样采取意图立场，归因为有思想有交流意图的行为。即使当前LLM语言能力已很强，真正成为社交主体还需要改变人类对AI意图的认知归因。

Abstract: As Large Language Models (LLMs) are increasingly deployed as social agents
and trained to produce humor and irony, a question emerges: when encountering
witty AI remarks, do people interpret these as intentional communication or
mere computational output? This study investigates whether people adopt the
intentional stance, attributing mental states to explain behavior,toward AI
during irony comprehension. Irony provides an ideal paradigm because it
requires distinguishing intentional contradictions from unintended errors
through effortful semantic reanalysis. We compared behavioral and neural
responses to ironic statements from AI versus human sources using established
ERP components: P200 reflecting early incongruity detection and P600 indexing
cognitive efforts in reinterpreting incongruity as deliberate irony. Results
demonstrate that people do not fully adopt the intentional stance toward
AI-generated irony. Behaviorally, participants attributed incongruity to
deliberate communication for both sources, though significantly less for AI
than human, showing greater tendency to interpret AI incongruities as
computational errors. Neural data revealed attenuated P200 and P600 effects for
AI-generated irony, suggesting reduced effortful detection and reanalysis
consistent with diminished attribution of communicative intent. Notably, people
who perceived AI as more sincere showed larger P200 and P600 effects for
AI-generated irony, suggesting that intentional stance adoption is calibrated
by specific mental models of artificial agents. These findings reveal that
source attribution shapes neural processing of social-communicative phenomena.
Despite current LLMs' linguistic sophistication, achieving genuine social
agency requires more than linguistic competence, it necessitates a shift in how
humans perceive and attribute intentionality to artificial agents.

</details>


### [243] [Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models](https://arxiv.org/abs/2510.17196)
*Jiaqi Leng,Xiang Hu,Junxiong Wang,Jianguo Li,Wei Wu,Yucheng Lu*

Main category: cs.CL

TL;DR: 本文系统性分析了块状稀疏注意力（chunk-based sparse attention）结构在处理超长文本有效性方面的关键设计要素，并通过实验和理论论证找出三项核心原则，有效支持了训练外超长长度泛化能力的提升。


<details>
  <summary>Details</summary>
Motivation: 当前主流Transformer模型处理长文本时因计算复杂度高和泛化性不足而受限，已有的滑动窗口、自回归记忆结构为代价牺牲了全局上下文利用效率。块状稀疏注意力近年来展示出强大泛化潜力，但其关键结构性设计尚未系统阐明。作者希望明确哪些结构性原则真正提升了其长文本处理能力。

Method: 作者构建统一框架，对块状稀疏注意力模型进行全面消融实验，系统分析不同子结构（如Chunk Encoder、Residual Path、稀疏选择机制）在性能中的独立作用，同时从理论解释内部信息流与“地标”生成的机制。

Result: 实验显示，具备强表达力的非线性Chunk Encoder并配备CLS标记、全局信息独立融合的Bypass Residual Path、以及预训练阶段强制的选择稀疏性三者是核心要素。基于此组合，模型在不需微调情况下实现了从4K到3200万token的极端长度泛化，并刷新RULER与BABILong两个长文本任务的纪录。

Conclusion: 本文为块状稀疏注意力及更广泛长文本模型的发展，系统总结了成功的结构性经验原则，为后续设计更强大长上下文模型提供了理论与经验基础。

Abstract: Effectively processing long contexts is a critical challenge for language
models. While standard Transformers are limited by quadratic complexity and
poor length extrapolation, alternative architectures like sliding window
attention and state space models sacrifice the ability to effectively utilize
the full context due to their fixed-size memory. Chunk-based sparse attention
has emerged as a promising paradigm for extreme length generalization, yet the
key architectural principles underpinning its success are not yet fully
understood. In this work, we present a systematic dissection of these models to
identify the core components driving their performance. Through a unified
framework and comprehensive ablation studies, we demonstrate that a combination
of three design principles is critical: (1) an expressive, non-linear Chunk
Encoder with a dedicated CLS token to produce representations for retrieval;
(2) a Bypassing Residual Path to stably integrate retrieved global information
without it being overridden by the local residual stream; and (3) enforced
selection sparsity during pre-training to bridge the train-test distribution
gap. We provide a theoretical motivation for intra-chunk information processing
and landmark generation. By combining these principles, we establish a new
state-of-the-art for training-free length extrapolation, successfully
generalizing models trained on a 4K context to 32 million tokens on RULER and
BABILong. Our findings provide a clear and empirically-grounded set of design
principles for developing future, highly-capable long-context language models.

</details>


### [244] [Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning via Attention Shifting](https://arxiv.org/abs/2510.17210)
*Chenchen Tan,Youyang Qu,Xinghao Li,Hui Zhang,Shujie Cui,Cunjian Chen,Longxiang Gao*

Main category: cs.CL

TL;DR: 本文提出了一种新的注意力迁移（Attention-Shifting, AS）框架，用于大语言模型的选择性忘却，旨在在不损害模型效用的前提下，有效忘却指定知识，同时减少幻觉响应。实验显示AS方法相比现有技术在准确率和幻觉抑制方面均有提升。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的广泛应用带来了对数据隐私与模型忘却（machine unlearning）的需求，但现有方法在忘却效果与模型效用之间存在两难：积极忘却会严重降低模型效用，保守忘却又可能导致模型产生“幻觉”或虚构答案。因此，需要一种既能有效选择性忘却，又能保持模型效用和响应可靠性的新方法。

Method: 作者提出了注意力迁移（AS）框架，核心思路包括：1）对需被遗忘的信息位点进行注意力抑制，但保护语言结构完整；2）增强调语集中于保留数据中的语义关键信息，减少对记忆知识的依赖，防止知识扩散和泛化损失。两者通过双重损失目标优化，实现局部性的‘软’遗忘边界，保障无关知识的保留。

Result: AS方法在ToFU和TDEC两个公开基准上分别比现有最优方法准确率提升15%和10%，同时保持同等水平的无幻觉忘却能力，效果优于当前主流遗忘技术。

Conclusion: AS框架能在忘却效果、模型泛化能力和生成响应可靠性之间实现更优平衡，比现有方法更适用于实际的知识密集型大语言模型应用场景。

Abstract: The increase in computing power and the necessity of AI-assisted
decision-making boost the growing application of large language models (LLMs).
Along with this, the potential retention of sensitive data of LLMs has spurred
increasing research into machine unlearning. However, existing unlearning
approaches face a critical dilemma: Aggressive unlearning compromises model
utility, while conservative strategies preserve utility but risk hallucinated
responses. This significantly limits LLMs' reliability in knowledge-intensive
applications. To address this, we introduce a novel Attention-Shifting (AS)
framework for selective unlearning. AS is driven by two design objectives: (1)
context-preserving suppression that attenuates attention to fact-bearing tokens
without disrupting LLMs' linguistic structure; and (2) hallucination-resistant
response shaping that discourages fabricated completions when queried about
unlearning content. AS realizes these objectives through two attention-level
interventions, which are importance-aware suppression applied to the unlearning
set to reduce reliance on memorized knowledge and attention-guided retention
enhancement that reinforces attention toward semantically essential tokens in
the retained dataset to mitigate unintended degradation. These two components
are jointly optimized via a dual-loss objective, which forms a soft boundary
that localizes unlearning while preserving unrelated knowledge under
representation superposition. Experimental results show that AS improves
performance preservation over the state-of-the-art unlearning methods,
achieving up to 15% higher accuracy on the ToFU benchmark and 10% on the TDEC
benchmark, while maintaining competitive hallucination-free unlearning
effectiveness. Compared to existing methods, AS demonstrates a superior balance
between unlearning effectiveness, generalization, and response reliability.

</details>


### [245] [StreamingThinker: Large Language Models Can Think While Reading](https://arxiv.org/abs/2510.17238)
*Junlong Tong,Yingqi Fan,Anhao Zhao,Yunpu Ma,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 本文提出了一种让大语言模型（LLMs）在接收输入时边读边思考（streaming thinking）的推理范式，有效降低延迟，同时保持推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理方式需等输入全部到齐才开始推理，这导致延迟高且在动态场景下对早期信息关注减弱。受人类边读边思考方式启发，作者希望让LLM能实时推理以提升效率和信息保持。

Method: 提出并实现了StreamingThinker框架，包括流式CoT（chain of thought）生成、流式约束训练和流式并行推理三部分。具体方法包括流式推理单元（结合质量控制）、顺序保持的注意力机制和位置编码、以及并行KV缓存，从而实现输入编码与推理生成的解耦及并发能力。

Result: 在Qwen3模型家族上的多类推理任务实验证明，在基本保持原有推理性能的前提下，StreamingThinker使推理开始前的token等待时间减少了80%，总响应时延缩短超60%。

Conclusion: Streaming thinking范式显著提升了LLM推理的效率和实时性，为动态场景下大模型应用提供了有效技术路径。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm
initiates thinking only after the entire input is available, which introduces
unnecessary latency and weakens attention to earlier information in dynamic
scenarios. Inspired by human cognition of thinking while reading, we first
design a \textit{\textbf{streaming thinking}} paradigm for LLMs, where
reasoning unfolds in the order of input and further adjusts its depth once
reading is complete. We instantiate this paradigm with
\textit{StreamingThinker}, a framework that enables LLMs to think while reading
through the integration of streaming CoT generation, streaming-constraint
training, and streaming parallel inference. Specifically, StreamingThinker
employs streaming reasoning units with quality control for CoT generation,
enforces order-preserving reasoning through streaming attention masks and
position encoding, and leverages parallel KV caches that decouple input
encoding from reasoning generation, thereby ensuring alignment and enabling
true concurrency. We evaluate StreamingThinker on the Qwen3 model family across
math reasoning, logical reasoning, and context-based QA reasoning tasks.
Experimental results show that the StreamingThinker preserves performance
comparable to batch thinking, while yielding an 80\% reduction in token waiting
before the onset of reasoning and a more than 60\% reduction in time-level
latency for producing the final answer, demonstrating the effectiveness of the
streaming paradigm for LLM reasoning. Code will be released at
\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this
repository.}

</details>


### [246] [From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models](https://arxiv.org/abs/2510.17247)
*Zefan Cai,Haoyi Qiu,Haozhe Zhao,Ke Wan,Jiachen Li,Jiuxiang Gu,Wen Xiao,Nanyun Peng,Junjie Hu*

Main category: cs.CL

TL;DR: 该论文提出了一套名为VideoBiasEval的框架，系统评估视频生成模型在对齐调优过程中社会偏见的演化及放大。发现视觉质量提升的同时也增强并稳定了偏见，呼吁对齐流程需重视公平性评估与偏见缓解。


<details>
  <summary>Details</summary>
Motivation: 尽管文本到视频生成技术因视觉质量提升而受到关注，但模型调优过程中也加剧了社会偏见。缺乏系统方法来追踪与量化这些偏见在数据集、奖励模型到生成视频的各阶段传播与放大。

Method: 提出VideoBiasEval评估工具，框架依据成熟的社会偏见分类学，采用事件为基础的提示，将行为语义与人物属性分离，并设计多层次度量指标分析族裔偏见、族裔下的性别偏见、属性分布变化以及偏见随时间的延续。

Result: 首次系统追踪了偏见如何从偏好数据集经奖励模型传递并放大至调优后的视频扩散模型。实验显示，模型对齐不仅加深代表性偏见，还使偏见随时间更稳定（如刻板印象更加顺滑且持续）。

Conclusion: 视频生成模型对齐过程应引入对偏见的检测与缓解策略，否则视觉效果提升的同时会强化并稳固社会偏见。强调公平、负责任的生成很有必要。

Abstract: Recent advances in video diffusion models have significantly enhanced
text-to-video generation, particularly through alignment tuning using reward
models trained on human preferences. While these methods improve visual
quality, they can unintentionally encode and amplify social biases. To
systematically trace how such biases evolve throughout the alignment pipeline,
we introduce VideoBiasEval, a comprehensive diagnostic framework for evaluating
social representation in video generation. Grounded in established social bias
taxonomies, VideoBiasEval employs an event-based prompting strategy to
disentangle semantic content (actions and contexts) from actor attributes
(gender and ethnicity). It further introduces multi-granular metrics to
evaluate (1) overall ethnicity bias, (2) gender bias conditioned on ethnicity,
(3) distributional shifts in social attributes across model variants, and (4)
the temporal persistence of bias within videos. Using this framework, we
conduct the first end-to-end analysis connecting biases in human preference
datasets, their amplification in reward models, and their propagation through
alignment-tuned video diffusion models. Our results reveal that alignment
tuning not only strengthens representational biases but also makes them
temporally stable, producing smoother yet more stereotyped portrayals. These
findings highlight the need for bias-aware evaluation and mitigation throughout
the alignment process to ensure fair and socially responsible video generation.

</details>


### [247] [How News Feels: Understanding Affective Bias in Multilingual Headlines for Human-Centered Media Design](https://arxiv.org/abs/2510.17252)
*Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Ayesha Siddiqua,Jungpil Shin*

Main category: cs.CL

TL;DR: 本文通过对30万条孟加拉语新闻标题及内容的大规模情感分析，发现媒体报道普遍带有消极情感，尤其突出愤怒、恐惧和失望，并提出了可帮助读者辨识情感倾向的新闻聚合器设计建议。


<details>
  <summary>Details</summary>
Motivation: 新闻媒体通过情感框架影响公众情绪，尤其以负面、情绪强烈的报道吸引眼球，因此有必要分析新闻内容中的情感倾向及其变异性，以揭示报道背后的情感操控现象。

Method: 采用Gemma-3 4B模型进行零样本推理，对30万条孟加拉语新闻标题及其内容进行主导情感与整体语气分析，并比较不同媒体对同类事件情感表达的差异。

Result: 研究结果显示，孟加拉新闻标题中消极情绪（尤其是愤怒、恐惧和失望）占主导地位；相同新闻事件在不同媒体中的情感呈现有明显差异。

Conclusion: 媒体报道存在广泛的负面情感倾向，建议设计具备情感可视化功能的人本化新闻聚合器，帮助读者识别新闻中的隐含情感偏向。

Abstract: News media often shape the public mood not only by what they report but by
how they frame it. The same event can appear calm in one outlet and alarming in
another, reflecting subtle emotional bias in reporting. Negative or emotionally
charged headlines tend to attract more attention and spread faster, which in
turn encourages outlets to frame stories in ways that provoke stronger
reactions. This research explores that tendency through large-scale emotion
analysis of Bengali news. Using zero-shot inference with Gemma-3 4B, we
analyzed 300000 Bengali news headlines and their content to identify the
dominant emotion and overall tone of each. The findings reveal a clear
dominance of negative emotions, particularly anger, fear, and disappointment,
and significant variation in how similar stories are emotionally portrayed
across outlets. Based on these insights, we propose design ideas for a
human-centered news aggregator that visualizes emotional cues and helps readers
recognize hidden affective framing in daily news.

</details>


### [248] [Explainability of Large Language Models: Opportunities and Challenges toward Generating Trustworthy Explanations](https://arxiv.org/abs/2510.17256)
*Shahin Atakishiyev,Housam K. B. Babiker,Jiayi Dai,Nawshad Farruque,Teruaki Hayashi,Nafisa Sadaf Hriti,Md Abed Rahman,Iain Smith,Mi-Young Kim,Osmar R. Zaïane,Randy Goebel*

Main category: cs.CL

TL;DR: 本文旨在探讨大型语言模型（LLM）在局部可解释性和机制可解释性方面的最新研究，综述相关方法，并通过在医疗和自动驾驶等关键领域的实验，分析LLM解释如何影响用户信任，指出未来挑战与方向。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在多项NLP任务中表现优异，但其生成内容的过程对人类缺乏可解释性，且易产生幻觉（错误推理与预测），亟需理解其内部机制，以提升对模型输出的信任。

Method: 本文首先系统回顾LLM局部可解释性及机制可解释性研究，并选取医疗与自动驾驶两大领域开展实验，评估模型解释能力及其对用户信任的影响，最后梳理现有未解决问题和未来研究方向。

Result: 通过理论综述与实验，论文揭示了目前可解释性方法的进展和局限，并指出在实际应用中，增强解释对于提升受众信任具有重要作用，同时总结了主要难题。

Conclusion: 当前LLM可解释性领域仍存在诸多挑战，需进一步发展能与人类思维对齐的机制，提升模型解释的可靠性和人类可接受度，为未来研究和实际应用提供指引。

Abstract: Large language models have exhibited impressive performance across a broad
range of downstream tasks in natural language processing. However, how a
language model predicts the next token and generates content is not generally
understandable by humans. Furthermore, these models often make errors in
prediction and reasoning, known as hallucinations. These errors underscore the
urgent need to better understand and interpret the intricate inner workings of
language models and how they generate predictive outputs. Motivated by this
gap, this paper investigates local explainability and mechanistic
interpretability within Transformer-based large language models to foster trust
in such models. In this regard, our paper aims to make three key contributions.
First, we present a review of local explainability and mechanistic
interpretability approaches and insights from relevant studies in the
literature. Furthermore, we describe experimental studies on explainability and
reasoning with large language models in two critical domains -- healthcare and
autonomous driving -- and analyze the trust implications of such explanations
for explanation receivers. Finally, we summarize current unaddressed issues in
the evolving landscape of LLM explainability and outline the opportunities,
critical challenges, and future directions toward generating human-aligned,
trustworthy LLM explanations.

</details>


### [249] [TaxoAlign: Scholarly Taxonomy Generation Using Language Models](https://arxiv.org/abs/2510.17263)
*Avishek Lahiri,Yufang Hou,Debarshi Kumar Sanyal*

Main category: cs.CL

TL;DR: 本文提出了一种自动生成学术分类体系（taxonomy）的方法TaxoAlign，并通过新建的CS-TaxoBench基准数据集，将自动生成的分类体系与人工撰写的进行结构和语义对比评估，实验结果表明新方法优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 目前自动生成文献综述的研究缺乏与专家人工撰写分类结构的细致对比，而结构化知识有助于文献梳理。填补这一评估空白对于推动自动化综述生成具有重要意义。

Method: 1）构建CS-TaxoBench基准数据集，包含460个人工撰写的学术分类体系及80个会议综述分类；2）提出TaxoAlign方法，通过三阶段的基于主题、指令引导方式生成taxonomy；3）设计严格的自动化评价体系，从结构对齐和语义一致性角度，与人工分类进行比较。

Result: TaxoAlign方法在CS-TaxoBench测试中，在几乎所有自动化测评指标以及人工评价上均优于其它现有方法和基线。

Conclusion: TaxoAlign能更好地生成与人工结构一致且语义合理的学术分类体系，推动了自动化综述生成的相关技术发展。

Abstract: Taxonomies play a crucial role in helping researchers structure and navigate
knowledge in a hierarchical manner. They also form an important part in the
creation of comprehensive literature surveys. The existing approaches to
automatic survey generation do not compare the structure of the generated
surveys with those written by human experts. To address this gap, we present
our own method for automated taxonomy creation that can bridge the gap between
human-generated and automatically-created taxonomies. For this purpose, we
create the CS-TaxoBench benchmark which consists of 460 taxonomies that have
been extracted from human-written survey papers. We also include an additional
test set of 80 taxonomies curated from conference survey papers. We propose
TaxoAlign, a three-phase topic-based instruction-guided method for scholarly
taxonomy generation. Additionally, we propose a stringent automated evaluation
framework that measures the structural alignment and semantic coherence of
automatically generated taxonomies in comparison to those created by human
experts. We evaluate our method and various baselines on CS-TaxoBench, using
both automated evaluation metrics and human evaluation studies. The results
show that TaxoAlign consistently surpasses the baselines on nearly all metrics.
The code and data can be found at https://github.com/AvishekLahiri/TaxoAlign.

</details>


### [250] [Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning](https://arxiv.org/abs/2510.17289)
*Hajar Bakarou,Mohamed Sinane El Messoussi,Anaïs Ollagnier*

Main category: cs.CL

TL;DR: 本研究利用法语多方对话网络数据集，比较不同模型在社交平台反社会行为识别中的表现，发现多模态融合模型效果最佳。


<details>
  <summary>Details</summary>
Motivation: 现有关于社交平台反社会行为的研究大多集中在X和Reddit等单一网络，缺乏对多方对话场景（如群聊）反社会行为识别的深入探究，主要因相关数据有限。多方对话是现实中常见且复杂度高的场景，深入研究可增强平台安全。

Method: 作者使用了一个开放获取的法语多方对话反社会行为数据集CyberAgressionAdo-Large，围绕滥用检测、欺凌行为分析和欺凌团体识别三个任务，评测六种文本和八种图建模方法，并对词汇线索、互动动态及两者的多模态融合进行分析。

Result: 多模态融合模型整体优于单模态模型。其中mBERT＋WD-SGCN晚期融合模型在滥用检测任务中取得0.718的最高分，并在团体识别和行为分析任务中也有良好表现（分别为0.286和0.606）。错误分析显示该模型对隐性攻击、角色转变和依赖上下文的敌意也有很好的捕捉能力。

Conclusion: 将文本和社交图谱等多模态信息融合，有助于更有效识别多方对话中的复杂反社会行为。研究推动了多模态、群体语境下反社会行为检测方法的发展，对社交媒体安全管理具有应用价值。

Abstract: Antisocial behavior (ASB) on social media -- including hate speech,
harassment, and cyberbullying -- poses growing risks to platform safety and
societal well-being. Prior research has focused largely on networks such as X
and Reddit, while \textit{multi-party conversational settings} remain
underexplored due to limited data. To address this gap, we use
\textit{CyberAgressionAdo-Large}, a French open-access dataset simulating ASB
in multi-party conversations, and evaluate three tasks: \textit{abuse
detection}, \textit{bullying behavior analysis}, and \textit{bullying
peer-group identification}. We benchmark six text-based and eight graph-based
\textit{representation-learning methods}, analyzing lexical cues, interactional
dynamics, and their multimodal fusion. Results show that multimodal models
outperform unimodal baselines. The late fusion model \texttt{mBERT + WD-SGCN}
achieves the best overall results, with top performance on abuse detection
(0.718) and competitive scores on peer-group identification (0.286) and
bullying analysis (0.606). Error analysis highlights its effectiveness in
handling nuanced ASB phenomena such as implicit aggression, role transitions,
and context-dependent hostility.

</details>


### [251] [Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation](https://arxiv.org/abs/2510.17354)
*Chenghao Zhang,Guanting Dong,Xinyu Yang,Zhicheng Dou*

Main category: cs.CL

TL;DR: 本文提出了一种面向混合模态（如文本+图片）检索增强生成（URAG）任务的新方法与系统：Nyx，并构建了一个真实、混合模态的问答数据集NyxQA。结果显示该方法在传统以及混合模态场景中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统主要面向纯文本检索，但现实世界中查询和文档常常包含文本与图像等多种模态。现有方法在混合模态检索和生成任务中效果有限，因此需要发展能处理多种信息源的RAG系统。

Method: 提出了Nyx——一种统一的混合模态检索器，使其可处理输入和目标均为混合模态的数据。为解决真实混合模态数据稀缺的问题，设计了四阶段自动化数据生成与筛选流程，从网络文档中构建出多样化的NyxQA混合模态问答对数据集。训练方法分为两步：先在NyxQA及其他开源检索数据集上预训练，再用下游视觉-语言模型反馈进行有监督微调，使检索结果更贴合生成任务需求。

Result: 实验显示，Nyx不仅在传统文本RAG基准表现不俗，在更具挑战性的混合模态URAG场景中也有显著提升，极大提高了视觉-语言生成任务的效果。

Conclusion: Nyx作为一种面向混合模态检索与生成的系统，能够更好地解决现实中复杂多模态信息的处理与生成问题。其方法和数据集也为未来多模态RAG任务研究提供了新思路。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for
enhancing large language models (LLMs) by retrieving relevant documents from an
external corpus. However, existing RAG systems primarily focus on unimodal text
documents, and often fall short in real-world scenarios where both queries and
documents may contain mixed modalities (such as text and images). In this
paper, we address the challenge of Universal Retrieval-Augmented Generation
(URAG), which involves retrieving and reasoning over mixed-modal information to
improve vision-language generation. To this end, we propose Nyx, a unified
mixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate
the scarcity of realistic mixed-modal data, we introduce a four-stage automated
pipeline for generation and filtering, leveraging web documents to construct
NyxQA, a dataset comprising diverse mixed-modal question-answer pairs that
better reflect real-world information needs. Building on this high-quality
dataset, we adopt a two-stage training framework for Nyx: we first perform
pre-training on NyxQA along with a variety of open-source retrieval datasets,
followed by supervised fine-tuning using feedback from downstream
vision-language models (VLMs) to align retrieval outputs with generative
preferences. Experimental results demonstrate that Nyx not only performs
competitively on standard text-only RAG benchmarks, but also excels in the more
general and realistic URAG setting, significantly improving generation quality
in vision-language tasks.

</details>


### [252] [The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple, Self-Contained Directives](https://arxiv.org/abs/2510.17388)
*Henry Lim,Kwan Hui Lim*

Main category: cs.CL

TL;DR: 该论文指出，尽管指令微调的大型语言模型（IT-LLMs）表现了强大的零样本推理能力，但其对简单、原子指令的执行能力仍然有限，在不同选项标签格式中表现出显著波动。


<details>
  <summary>Details</summary>
Motivation: 当前多数研究关注IT-LLMs在复杂指令或推理任务中的表现，对其基础的、简单指令执行能力关注较少，但对原子指令的理解和执行是更复杂任务的基础。本文旨在系统性揭示模型在这方面的实际能力与局限。

Method: 作者对20个IT-LLM模型，基于MMLU和MMLU-Pro两组基准，系统地改变选项标签的格式（字母、数字、罗马数字），同时在有无指令、移除选项内容、使用三次示例等不同设置下，比较模型表现。进一步分析选项标签对模型表现的影响和模型错误类型。

Result: 不同标签格式下模型表现变化极大（如罗马数字相比数字标签降低30%以上），且没有指令时表现进一步下降，并且模型仅在数字标签下才能超过随机选择。此外，增加示例并未显著提升鲁棒性，大模型虽然整体准确率更高但偏差依然存在。

Conclusion: 当前主流的指令微调训练不足以支撑稳定和一致的原子指令遵循。今后需要发展更专门针对原子级指令能力的评测和训练方法，以提升模型对基本指令的准确执行能力。

Abstract: Instruction-tuned large language models (IT-LLMs) exhibit strong zero-shot
reasoning, yet their ability to execute simple, self-contained instructions
remains underexplored, despite this being foundational to complex
instruction-following. We evaluate 20 IT-LLMs on modified MMLU and MMLU-Pro
benchmarks, by systematically varying the format of option labels (alphabetic,
numeric, Roman) while keeping their meaning identical under four paradigms,
namely: (1) With explicit instructions, label changes cause large performance
shifts (e.g., -30.45\% for Roman vs. numeric), revealing instruction-format
bias. (2) Without instructions, performance drops further (up to -10.84\%) and
label sensitivity intensifies, underscoring the role of explicit guidance. (3)
When option contents are removed, models fail random-choice baselines except
with numeric labels, suggesting weak adherence to atomic directives. (4)
Three-shot exemplars yield no significant gains in robustness or fidelity, and
generation analyses show persistent label errors, especially for non-numeric
formats. Across model sizes, larger LLMs achieve higher accuracy but remain
inconsistent in instruction adherence. These results expose the insufficiencies
of current instruction-tuning paradigms and highlight the need for evaluation
methods and training strategies that explicitly target atomic
instruction-following.

</details>


### [253] [EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level Adaptability in LLMs](https://arxiv.org/abs/2510.17389)
*Numaan Naeem,Abdellah El Mekki,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 本论文提出了EduAdapt基准数据集，用于评估大语言模型在不同年级（K-12）教育场景下调整回答水平的能力，并发现现有模型在针对低年级学生生成合适回答方面仍存在困难。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在学术任务上表现优秀，但它们难以根据学生年级调整输出，这在K-12教育中极为关键。当前缺乏衡量模型适应不同认知和发展阶段能力的标准基准，因此需要开发相关评测工具。

Method: 作者构建了包含近4.8万组、涵盖1-12年级、分布在九大科学学科的问答数据集EduAdapt，并分为四个年级段。利用这一基准评估了多种开源大模型的年级适应性。

Result: 实验表明，虽然参数量较大的LLM整体表现更好，但针对1-5年级学生的问题回答依旧不够恰当，存在表达过难或过泛的问题。

Conclusion: 本工作首次提出了系统评测LLM年级适应性的基准和框架，促进了面向发展阶段的教育AI模型发展，并呼吁通过更优训练和提示策略提升低龄适应能力。数据与代码已开源。

Abstract: Large language models (LLMs) are transforming education by answering
questions, explaining complex concepts, and generating content across a wide
range of subjects. Despite strong performance on academic benchmarks, they
often fail to tailor responses to students' grade levels. This is a critical
need in K-12 education, where age-appropriate vocabulary and explanation are
essential for effective learning. Existing models frequently produce outputs
that are too advanced or vague for younger learners, and there are no
standardized benchmarks to evaluate their ability to adjust across cognitive
and developmental stages. To address this gap, we introduce EduAdapt, a
benchmark of nearly 48k grade-labeled QA pairs across nine science subjects,
spanning Grades 1-12 and grouped into four grade levels. We evaluate a diverse
set of open-source LLMs on EduAdapt and find that while larger models generally
perform better, they still struggle with generating suitable responses for
early-grade students (Grades 1-5). Our work presents the first dataset and
evaluation framework for assessing grade-level adaptability in LLMs, aiming to
foster more developmentally aligned educational AI systems through better
training and prompting strategies. EduAdapt code and datasets are publicly
available at https://github.com/NaumanNaeem/EduAdapt.

</details>


### [254] [Leveraging Group Relative Policy Optimization to Advance Large Language Models in Traditional Chinese Medicine](https://arxiv.org/abs/2510.17402)
*Jiacheng Xie,Shuai Zeng,Yang Yu,Xiaoting Tang,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: 本研究提出了Ladder-base，这是首个基于GRPO强化学习方法训练的中医专属大型语言模型，显著提升了推理与事实一致性，在多个标准化评测中超越了主流通用与中医领域模型。


<details>
  <summary>Details</summary>
Motivation: 中医学知识系统复杂且独特，难以用现有通用大模型直接有效处理。以往中医专属大模型常因对齐、数据和评测一致性等问题，难以实现高水平推理与事实能力，因此亟需创新训练方法，提升中医AI系统的可信度和专业性。

Method: 提出Ladder-base模型，基于Qwen2.5-7B-Instruct，利用中医Ladder基准数据集的文本部分，先将数据按8:1:1分为训练、验证和测试集，再采用一种名为Group Relative Policy Optimization（GRPO）的强化学习方法，通过组内比较优化模型回答选择，提升推理和事实一致性。

Result: 在标准化评测下，Ladder-base在多项推理指标上均优于目前主流的大型通用语言模型（比如GPT-4、Gemini 2.5、Claude 3、Qwen3）与中医领域模型（BenTsao、HuatuoGPT2、Zhongjing）。

Conclusion: 采用GRPO训练方法能够有效提升大模型在复杂专业领域（如中医）下的推理和事实能力，有助于发展高可信、临床可靠的中医AI系统。

Abstract: Traditional Chinese Medicine (TCM) presents a rich and structurally unique
knowledge system that challenges conventional applications of large language
models (LLMs). Although previous TCM-specific LLMs have shown progress through
supervised fine-tuning, they often face limitations in alignment, data quality,
and evaluation consistency. In this study, we introduce Ladder-base, the first
TCM-focused LLM trained with Group Relative Policy Optimization (GRPO), a
reinforcement learning method that improves reasoning and factual consistency
by optimizing response selection based on intra-group comparisons. Ladder-base
is built upon the Qwen2.5-7B-Instruct foundation model and trained exclusively
on the textual subset of the TCM-Ladder benchmark, using 80 percent of the data
for training and the remaining 20 percent split evenly between validation and
test sets. Through standardized evaluation, Ladder-base demonstrates superior
performance across multiple reasoning metrics when compared to both
state-of-the-art general-purpose LLMs such as GPT-4, Gemini 2.5, Claude 3, and
Qwen3 and domain-specific TCM models including BenTsao, HuatuoGPT2, and
Zhongjing. These findings suggest that GRPO provides an effective and efficient
strategy for aligning LLMs with expert-level reasoning in traditional medical
domains and supports the development of trustworthy and clinically grounded TCM
artificial intelligence systems.

</details>


### [255] [AFRICAPTION: Establishing a New Paradigm for Image Captioning in African Languages](https://arxiv.org/abs/2510.17405)
*Mardiyyah Oduwole,Prince Mireku,Fatimo Adebanjo,Oluwatosin Olajide,Mahi Aminu Aliyu,Jekaterina Novikova*

Main category: cs.CL

TL;DR: AfriCaption提出了针对非洲20种语言的多语种图像描述框架，填补了非洲低资源语言在多模态AI领域的空白。


<details>
  <summary>Details</summary>
Motivation: 目前多模态AI主要服务于高资源语言，非洲等低资源语言被忽视，阻碍了技术普及与公平发展。该研究意在缩小这一差距，推动技术民主化。

Method: 1. 基于Flickr8k建立包含20种非洲语言的有意义图像描述数据集，采用上下文感知的选择与翻译流程生成语义一致的描述；2. 构建动态、保留语境的处理流程，通过模型集成与自适应替换，持续保证数据质量；3. 开发AfriCaption模型，结合SigLIP与NLLB200架构实现0.5B参数的视觉到文本转化。

Result: 建立了首个可扩展的非洲低资源语言图像描述数据集，提供持续高质量数据处理流程，并推出适用于多种非洲语言的图像自动描述模型。

Conclusion: AfriCaption为非洲低资源语言多模态AI研究提供了统一的高质量工具和资源，为多语言平等和包容性的AI发展奠定基础。

Abstract: Multimodal AI research has overwhelmingly focused on high-resource languages,
hindering the democratization of advancements in the field. To address this, we
present AfriCaption, a comprehensive framework for multilingual image
captioning in 20 African languages and our contributions are threefold: (i) a
curated dataset built on Flickr8k, featuring semantically aligned captions
generated via a context-aware selection and translation process; (ii) a
dynamic, context-preserving pipeline that ensures ongoing quality through model
ensembling and adaptive substitution; and (iii) the AfriCaption model, a 0.5B
parameter vision-to-text architecture that integrates SigLIP and NLLB200 for
caption generation across under-represented languages. This unified framework
ensures ongoing data quality and establishes the first scalable
image-captioning resource for under-represented African languages, laying the
groundwork for truly inclusive multimodal AI.

</details>


### [256] [BenCao: An Instruction-Tuned Large Language Model for Traditional Chinese Medicine](https://arxiv.org/abs/2510.17415)
*Jiacheng Xie,Yang Yu,Yibo Chen,Hanyao Zhang,Lening Zhao,Jiaxuan He,Lei Jiang,Xiaoting Tang,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: 本文提出了BenCao——基于ChatGPT的中医多模态助手，融合结构化知识库、诊断数据和专家反馈，提升了中医大模型的解释性与临床实用性，并在多个中医任务上超过了现有模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在中医领域应用存在不足，特别是多模态信息整合、解释性和临床应用方面。为解决这些痛点，开发具备更强实用性的中医专属大模型势在必行。

Method: 提出BenCao系统：基于ChatGPT，自然语言指令微调（非重训练参数），深度融合1000余部经典/现代中医文献、场景化交互框架、可解释推理机制和开方中医师反馈。系统还接入API实现舌像识别、多模态信息动态检索和丰富的人机互动。

Result: BenCao在单选题基准和多模态分类等评测中，诊断、药材识别、体质分类等关键任务准确率明显优于通用及行业内中医大模型。系统上线于GPTs Store，全球约1,000用户实际使用，体现其广泛实用性。

Conclusion: 通过自然语言指令微调和多模态整合，BenCao不仅提升了中医大模型的临床适用性，还为AI与传统医疗推理融合提供了切实可行、易于部署的解决方案。

Abstract: Traditional Chinese Medicine (TCM), with a history spanning over two
millennia, plays a role in global healthcare. However, applying large language
models (LLMs) to TCM remains challenging due to its reliance on holistic
reasoning, implicit logic, and multimodal diagnostic cues. Existing TCM-domain
LLMs have made progress in text-based understanding but lack multimodal
integration, interpretability, and clinical applicability. To address these
limitations, we developed BenCao, a ChatGPT-based multimodal assistant for TCM,
integrating structured knowledge bases, diagnostic data, and expert feedback
refinement. BenCao was trained through natural language instruction tuning
rather than parameter retraining, aligning with expert-level reasoning and
ethical norms specific to TCM. The system incorporates a comprehensive
knowledge base of over 1,000 classical and modern texts, a scenario-based
instruction framework for diverse interactions, a chain-of-thought simulation
mechanism for interpretable reasoning, and a feedback refinement process
involving licensed TCM practitioners. BenCao connects to external APIs for
tongue-image classification and multimodal database retrieval, enabling dynamic
access to diagnostic resources. In evaluations across single-choice question
benchmarks and multimodal classification tasks, BenCao achieved superior
accuracy to general-domain and TCM-domain models, particularly in diagnostics,
herb recognition, and constitution classification. The model was deployed as an
interactive application on the OpenAI GPTs Store, accessed by nearly 1,000
users globally as of October 2025. This study demonstrates the feasibility of
developing a TCM-domain LLM through natural language-based instruction tuning
and multimodal integration, offering a practical framework for aligning
generative AI with traditional medical reasoning and a scalable pathway for
real-world deployment.

</details>


### [257] [Navigating the Alignment-Calibration Trade-off: A Pareto-Superior Frontier via Model Merging](https://arxiv.org/abs/2510.17426)
*Tiancheng Hu,Benjamin Minixhofer,Nigel Collier*

Main category: cs.CL

TL;DR: 后训练对齐虽然提高了模型的符合性，却会显著降低模型校准性，使其过于自信、可靠性下降、输出多样性变差。通过对齐前后权重插值，可以高效平衡这些损失，实现能力与可靠性的共同提升。


<details>
  <summary>Details</summary>
Motivation: 后训练对齐常常导致模型在任务准确性之外，还出现严重的校准损失，对实际应用的可靠性影响较大。因此，研究如何减缓这种“对齐税”的全面负面影响，是提升模型实用价值的重要课题。

Method: 作者提出在模型对齐前后的权重间进行插值，通过简单的后处理方法，探索任务准确性与校准性之间的Pareto最优解，实现能力与可靠性的权衡优化。

Result: 实验发现，这种权重插值方法可以找到精度高于原始模型、同时校准性也大幅改善的插值模型，缓解了传统后训练对齐带来的多方面损失。

Conclusion: 权重插值是一种高效简单、计算开销低的方法，有助于显著缓解对齐税，提升模型的能力和可靠性。

Abstract: The "alignment tax" of post-training is typically framed as a drop in task
accuracy. We show it also involves a severe loss of calibration, making models
overconfident, less reliable, and model outputs less diverse. We show that this
trade-off can be navigated effectively via a simple post-hoc intervention:
interpolating between a model's weights before and after alignment. Crucially,
this is not a strict trade-off. We find that the process consistently reveals
Pareto-optimal interpolations - models that improve accuracy beyond both
parents while substantially recovering the calibration lost during alignment.
Our work demonstrates that simple model merging provides a computationally
efficient method for mitigating the full scope of the alignment tax, yielding
models that are more capable and more reliable.

</details>


### [258] [Agentic Reinforcement Learning for Search is Unsafe](https://arxiv.org/abs/2510.17431)
*Yushi Yang,Shreyansh Padarha,Andrew Lee,Adam Mahdi*

Main category: cs.CL

TL;DR: 本文发现，经过强化学习（RL）训练用于自主搜索的大语言模型（Agentic RL模型）虽然能拒绝部分有害请求，但容易被简单攻击手段绕开，从而产生大量有害搜索与回答。


<details>
  <summary>Details</summary>
Motivation: 近年来，RL训练的语言模型在多步推理和自主工具调用方面表现优异，广泛应用于搜索等场景。但这些模型的安全性尚未被充分研究，尤其是在与外部世界联动时，可能存在暴露有害信息的风险。因此，需要探究这类模型的安全鲁棒性和潜在弱点。

Method: 作者通过对Qwen与Llama等两大家族的RL搜索模型，分别在本地和网页搜索场景下进行实证测试，设计了两种简单的攻击方法：一种强制模型以搜索开头（Search attack），另一种诱导模型反复进行搜索（Multi-search attack），以测试其对有害请求的防御能力。

Result: 实验表明，这些攻击手段使模型拒绝有害请求的比例下降高达60.0%，回答安全性下降82.5%，搜索查询安全性下降82.4%。攻击通过诱导模型先生成有害的搜索查询绕开原有拒绝机制，从而导致恶意内容泄漏。

Conclusion: 当前RL训练仅注重高效查询生成而未权衡内容有害性，导致agentic RL模型存在显著安全漏洞。未来急需构建兼顾安全的RL训练流程，以更好地保障自主搜索的安全性。

Abstract: Agentic reinforcement learning (RL) trains large language models to
autonomously call tools during reasoning, with search as the most common
application. These models excel at multi-step reasoning tasks, but their safety
properties are not well understood. In this study, we show that RL-trained
search models inherit refusal from instruction tuning and often deflect harmful
requests by turning them into safe queries. However, this safety is fragile.
Two simple attacks, one that forces the model to begin response with search
(Search attack), another that encourages models to repeatedly search
(Multi-search attack), trigger cascades of harmful searches and answers. Across
two model families (Qwen, Llama) with both local and web search, these attacks
lower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query
safety by 82.4%. The attacks succeed by triggering models to generate harmful,
request-mirroring search queries before they can generate the inherited refusal
tokens. This exposes a core weakness of current RL training: it rewards
continued generation of effective queries without accounting for their
harmfulness. As a result, RL search models have vulnerabilities that users can
easily exploit, making it urgent to develop safety-aware agentic RL pipelines
optimising for safe search.

</details>


### [259] [Multilingual Clinical NER for Diseases and Medications Recognition in Cardiology Texts using BERT Embeddings](https://arxiv.org/abs/2510.17437)
*Manuela Daniela Danu,George Marica,Constantin Suciu,Lucian Mihai Itu,Oladimeji Farri*

Main category: cs.CL

TL;DR: 该论文针对不同语言的临床文本，提出了多种基于BERT的深度上下文嵌入模型，在心脏病学领域的命名实体识别任务上取得了比现有方法更优的效果。


<details>
  <summary>Details</summary>
Motivation: 随着电子健康记录数据的爆炸式增长，如何有效从非结构化的临床文本中提取有用的生物医学知识（如疾病、药物信息），对临床诊断、疾病监测等尤为关键。然而，目前对于低资源语言临床文本的命名实体识别研究依然稀缺，因此亟需填补这一研究空白。

Method: 本文开发了多种深度上下文嵌入模型（包括单语与多语BERT变体），在普通领域文本上预训练，并在BioASQ MultiCardioNER任务数据集上微调，实现对英文、西班牙文和意大利文临床报告中的疾病和药物实体抽取。

Result: 该方法在四项子任务上取得了优异的F1分数：西班牙语疾病识别77.88%、西班牙语药物识别92.09%、英语药物识别91.74%、意大利语药物识别88.9%。所有分数均高于测试榜单的均值和中位数。

Conclusion: 多语言深度上下文语言模型能显著提升低资源语言临床文本的命名实体识别性能，有助于推动多语种临床知识自动化抽取技术的发展。

Abstract: The rapidly increasing volume of electronic health record (EHR) data
underscores a pressing need to unlock biomedical knowledge from unstructured
clinical texts to support advancements in data-driven clinical systems,
including patient diagnosis, disease progression monitoring, treatment effects
assessment, prediction of future clinical events, etc. While contextualized
language models have demonstrated impressive performance improvements for named
entity recognition (NER) systems in English corpora, there remains a scarcity
of research focused on clinical texts in low-resource languages. To bridge this
gap, our study aims to develop multiple deep contextual embedding models to
enhance clinical NER in the cardiology domain, as part of the BioASQ
MultiCardioNER shared task. We explore the effectiveness of different
monolingual and multilingual BERT-based models, trained on general domain text,
for extracting disease and medication mentions from clinical case reports
written in English, Spanish, and Italian. We achieved an F1-score of 77.88% on
Spanish Diseases Recognition (SDR), 92.09% on Spanish Medications Recognition
(SMR), 91.74% on English Medications Recognition (EMR), and 88.9% on Italian
Medications Recognition (IMR). These results outperform the mean and median F1
scores in the test leaderboard across all subtasks, with the mean/median values
being: 69.61%/75.66% for SDR, 81.22%/90.18% for SMR, 89.2%/88.96% for EMR, and
82.8%/87.76% for IMR.

</details>


### [260] [Evaluating Large Language Models on Urdu Idiom Translation](https://arxiv.org/abs/2510.17460)
*Muhammad Farmal Khan,Mousumi Akter*

Main category: cs.CL

TL;DR: 本文提出并公开了首个乌尔都语-英语成语翻译评测数据集，比较了大模型和神经机器翻译在成语和文化保留上的表现。


<details>
  <summary>Details</summary>
Motivation: 传统机器翻译对低资源语言（如乌尔都语）的成语翻译关注较少，现有研究和数据不足，难以推动该方向进展。

Method: 作者构建了包含乌尔都语原文和罗马化乌尔都语两种形式、配有标准英文成语翻译的数据集，并采用多种开源大语言模型与神经机器翻译系统进行评测，评价其成语及文化内涵翻译效果；自动评测指标包括BLEU、BERTScore、COMET和XCOMET。

Result: 经测试，基于提示词工程的翻译方法在成语翻译上优于直接翻译，但不同提示类型间提升有限。不同文本输入形式的对比显示，乌尔都语原文输入的成语翻译准确率高于罗马化输入。

Conclusion: 提示词工程对成语翻译有所帮助，同时输入文本的脚本类型会显著影响翻译质量。研究为低资源语言成语翻译提供了基准和分析工具，有助于后续改进。

Abstract: Idiomatic translation remains a significant challenge in machine translation,
especially for low resource languages such as Urdu, and has received limited
prior attention. To advance research in this area, we introduce the first
evaluation datasets for Urdu to English idiomatic translation, covering both
Native Urdu and Roman Urdu scripts and annotated with gold-standard English
equivalents. We evaluate multiple open-source Large Language Models (LLMs) and
Neural Machine Translation (NMT) systems on this task, focusing on their
ability to preserve idiomatic and cultural meaning. Automatic metrics including
BLEU, BERTScore, COMET, and XCOMET are used to assess translation quality. Our
findings indicate that prompt engineering enhances idiomatic translation
compared to direct translation, though performance differences among prompt
types are relatively minor. Moreover, cross script comparisons reveal that text
representation substantially affects translation quality, with Native Urdu
inputs producing more accurate idiomatic translations than Roman Urdu.

</details>


### [261] [Disparities in Multilingual LLM-Based Healthcare Q&A](https://arxiv.org/abs/2510.17476)
*Ipek Baris Schlicht,Burcu Sayin,Zhixue Zhao,Frederik M. Labonté,Cesare Barbera,Marco Viviani,Paolo Rosso,Lucie Flek*

Main category: cs.CL

TL;DR: 该论文系统性地分析了多语言大型语言模型在医疗健康问答中的事实一致性和信息覆盖差异，发现当前模型更倾向于英文学科知识，即使处理非英语任务也是如此。作者提出通过引入非英语维基百科上下文，有效提升了模型回答的文化适应性和真实性。


<details>
  <summary>Details</summary>
Motivation: 当下，AI大模型已经广泛应用于医疗健康领域，但不同语言的信息质量和覆盖度存在巨大差异。公平获取可靠医疗信息成为多语言AI实际部署的关键难题。论文旨在量化和分析这一现象，并找到提升多语言事实一致性的可行方案。

Method: 作者构建了MultiWikiHealthCare多语言医疗健康数据集（基于多语种维基百科），分别分析了医疗内容的跨语言覆盖率，并对比了大模型在用不同语言处理问答任务时与权威参考的一致性。还设计了一个案例研究，探讨引入非英语语境和RAG（检索增强生成）技术对事实一致性的影响。

Result: 发现英文维基百科无论在覆盖度还是模型回答对齐度上都远高于其他语种。在不提供上下文时，即使是非英语提问，模型还是更倾向与英文知识对齐。通过推理时提供非英文维基百科片段，能有效提升模型与本地文化/语言知识的一致率。

Conclusion: 论文结果揭示出医疗健康领域多语种信息和AI输出的一致性差距，同时提出通过语境增强等方法来提升多语言AI的公平性和知识本地化，为后续开发更加公平和可靠的多语言医疗AI系统指明了路径。

Abstract: Equitable access to reliable health information is vital when integrating AI
into healthcare. Yet, information quality varies across languages, raising
concerns about the reliability and consistency of multilingual Large Language
Models (LLMs). We systematically examine cross-lingual disparities in
pre-training source and factuality alignment in LLM answers for multilingual
healthcare Q&A across English, German, Turkish, Chinese (Mandarin), and
Italian. We (i) constructed Multilingual Wiki Health Care
(MultiWikiHealthCare), a multilingual dataset from Wikipedia; (ii) analyzed
cross-lingual healthcare coverage; (iii) assessed LLM response alignment with
these references; and (iv) conducted a case study on factual alignment through
the use of contextual information and Retrieval-Augmented Generation (RAG). Our
findings reveal substantial cross-lingual disparities in both Wikipedia
coverage and LLM factual alignment. Across LLMs, responses align more with
English Wikipedia, even when the prompts are non-English. Providing contextual
excerpts from non-English Wikipedia at inference time effectively shifts
factual alignment toward culturally relevant knowledge. These results highlight
practical pathways for building more equitable, multilingual AI systems for
healthcare.

</details>


### [262] [ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts](https://arxiv.org/abs/2510.17483)
*Zheyue Tan,Zhiyuan Li,Tao Yuan,Dong Zhou,Weilin Liu,Yueqing Zhuang,Yadong Li,Guowei Niu,Cheng Qin,Zhuyu Yao,Congyi Liu,Haiyang Xu,Boxun Li,Guohao Dai,Bo Zhao,Yu Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为ReXMoE的新型专家混合（MoE）架构，通过允许相邻层之间共享专家资源，突破了传统MoE架构在路由上的局部性限制，有效提升了模型表现和参数效率。


<details>
  <summary>Details</summary>
Motivation: 现有MoE架构通过细粒度专家增强模型表达能力，但受限于每层只能使用本地专家池，需在专家容量和路由多样性上权衡，在给定参数预算下无法兼得。作者希望打破这一限制，实现更丰富的专家组合，同时提升模型表达力而不增加整体参数量。

Method: 提出ReXMoE架构，允许路由器在相邻层间重用专家，从而实现专家维度与单层参数预算解耦。设计了逐步扩展专家池的渐进式路由策略（PSR），在训练过程中逐渐扩大可用专家集合，增强路由多样性和组合能力。

Result: 在多种架构下、参数规模从0.5B到7B的实验显示，ReXMoE在不增加整体参数量的情况下，提升了语言建模和下游任务的性能，始终优于传统MoE设计。

Conclusion: ReXMoE证明了打破层间专家限制的新设计范式，可以在参数效率不变的前提下提升大语言模型能力，为可扩展、高效的MoE设计提供了新方向。

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a promising approach
to scale Large Language Models (LLMs). MoE boosts the efficiency by activating
a subset of experts per token. Recent works show that fine-grained experts
substantially enriches the combinatorial flexibility of active experts and
enhances model expressiveness. However, such a design is fundamentally limited
by the layer-local routing mechanism: each layer is restricted to its own
expert pool. This requires a careful trade-off between expert dimensionality
and routing diversity given fixed parameter budgets. We describe ReXMoE, a
novel MoE architecture that improves routing beyond the existing layer-local
approaches by allowing routers to reuse experts across adjacent layers. ReXMoE
decouples expert dimensionality from per-layer budgets, enabling richer expert
combinations without sacrificing individual expert capacity or inflating
overall parameters. To this end, we propose a new progressive scaling routing
(PSR) strategy to gradually increase the candidate expert pool during training.
As a result, ReXMoE improves both language modeling and downstream task
performance. Extensive experiments on models ranging from 0.5B to 7B parameters
across different architectures demonstrate that ReXMoE consistently improves
performance under fixed architectural dimensions, confirming ReXMoE as new
design paradigm for parameter-efficient and scalable MoE-based LLMs.

</details>


### [263] [DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical Representation Learning](https://arxiv.org/abs/2510.17489)
*Yongxin He,Shan Zhang,Yixuan Cao,Lei Ma,Ping Luo*

Main category: cs.CL

TL;DR: 本论文提出了一种新的方法DETree，以及一个新的混合文本基准数据集RealBench，用以检测由不同人类与AI合作过程生成的文本，并且该方法在检测性能和泛化能力方面都取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有AI文本检测方法多半将文本简单分为“纯人类”或“涉及AI”，或者粗略地分类，无法细致刻画人类和AI多样协作生成的复杂文本，这极大限制了检测的效果和适用范围。作者认为需要更精细、更结构化的建模方式来反映不同文本生成过程间的内在关系。

Method: 作者提出DETree方法，将不同文本生成过程间的关系建模为分层的亲和树结构（Hierarchical Affinity Tree），并设计了专门的损失函数，引导文本在表示空间中与该结构对齐。此外，作者构建了RealBench，一个自动化生成、涵盖多种人类与AI协作形式的混合文本数据集，便于模型训练和评估。

Result: DETree在检测多种人类-AI混合生成文本的任务上表现出了更高的准确率，特别是在小样本学习和分布外（out-of-distribution, OOD）场景中展现出了更好的鲁棒性和泛化能力，相较现有方法有明显提升。

Conclusion: 以分层结构建模文本生成过程间关系，并结合专用损失函数和丰富的基准数据集，可以极大提升AI文本检测系统对复杂人类-AI协作生成文本的检测能力，尤其在泛化性和少样本条件下表现突出。

Abstract: Detecting AI-involved text is essential for combating misinformation,
plagiarism, and academic misconduct. However, AI text generation includes
diverse collaborative processes (AI-written text edited by humans,
human-written text edited by AI, and AI-generated text refined by other AI),
where various or even new LLMs could be involved. Texts generated through these
varied processes exhibit complex characteristics, presenting significant
challenges for detection. Current methods model these processes rather crudely,
primarily employing binary classification (purely human vs. AI-involved) or
multi-classification (treating human-AI collaboration as a new class). We
observe that representations of texts generated through different processes
exhibit inherent clustering relationships. Therefore, we propose DETree, a
novel approach that models the relationships among different processes as a
Hierarchical Affinity Tree structure, and introduces a specialized loss
function that aligns text representations with this tree. To facilitate this
learning, we developed RealBench, a comprehensive benchmark dataset that
automatically incorporates a wide spectrum of hybrid texts produced through
various human-AI collaboration processes. Our method improves performance in
hybrid text detection tasks and significantly enhances robustness and
generalization in out-of-distribution scenarios, particularly in few-shot
learning conditions, further demonstrating the promise of training-based
approaches in OOD settings. Our code and dataset are available at
https://github.com/heyongxin233/DETree.

</details>


### [264] [Empowering Real-World: A Survey on the Technology, Practice, and Evaluation of LLM-driven Industry Agents](https://arxiv.org/abs/2510.17491)
*Yihong Tang,Kehai Chen,Liang Yue,Jinxin Fan,Caishen Zhou,Xiaoguang Li,Yuyang Zhang,Mingming Zhao,Shixiong Kai,Kaiyang Guo,Xingshan Zeng,Wenjing Cun,Lifeng Shang,Min Zhang*

Main category: cs.CL

TL;DR: 本文系统综述了基于大语言模型（LLMs）的产业智能体的技术、应用与评估方法，提出了能力成熟度框架，并提出未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型快速发展，拥有自主推理、规划与执行复杂任务能力的智能体成为AI前沿，但如何将通用智能体研究转化为实际产业生产力仍具挑战。本文旨在梳理相关技术链条及其在产业界的落地路径。

Method: 作者通过能力成熟度框架梳理了产业智能体从“流程执行系统”到“自适应社会系统”的演化路径，分析了记忆、规划和工具使用三大技术支柱，并综合展示其在数字工程、科学发现、实体智能、协同业务与复杂系统仿真等场景的应用。同时，评述了现有的能力评测方法及其面对真实性、安全性、行业特性等挑战，并探讨了治理等实际问题。

Result: 本文总结了各类技术和产业实践现状，指出了现有评估方法面临的具体问题，明确了产业智能体在不同场景下的技术边界与发展潜力。

Conclusion: 作者梳理出产业智能体的完整进化与评估体系，既总结了行业现状，也提出未来发展路线和理论指导，为构建下一代产业智能体提供了基础与方向。

Abstract: With the rise of large language models (LLMs), LLM agents capable of
autonomous reasoning, planning, and executing complex tasks have become a
frontier in artificial intelligence. However, how to translate the research on
general agents into productivity that drives industry transformations remains a
significant challenge. To address this, this paper systematically reviews the
technologies, applications, and evaluation methods of industry agents based on
LLMs. Using an industry agent capability maturity framework, it outlines the
evolution of agents in industry applications, from "process execution systems"
to "adaptive social systems." First, we examine the three key technological
pillars that support the advancement of agent capabilities: Memory, Planning,
and Tool Use. We discuss how these technologies evolve from supporting simple
tasks in their early forms to enabling complex autonomous systems and
collective intelligence in more advanced forms. Then, we provide an overview of
the application of industry agents in real-world domains such as digital
engineering, scientific discovery, embodied intelligence, collaborative
business execution, and complex system simulation. Additionally, this paper
reviews the evaluation benchmarks and methods for both fundamental and
specialized capabilities, identifying the challenges existing evaluation
systems face regarding authenticity, safety, and industry specificity. Finally,
we focus on the practical challenges faced by industry agents, exploring their
capability boundaries, developmental potential, and governance issues in
various scenarios, while providing insights into future directions. By
combining technological evolution with industry practices, this review aims to
clarify the current state and offer a clear roadmap and theoretical foundation
for understanding and building the next generation of industry agents.

</details>


### [265] [Deep Self-Evolving Reasoning](https://arxiv.org/abs/2510.17498)
*Zihan Liu,Shun Zheng,Xumeng Wen,Yang Wang,Jiang Bian,Mao Yang*

Main category: cs.CL

TL;DR: 本文提出了深度自我演化推理（DSER）方法，通过并行进行多步自我演化推理过程，即使验证和改正能力较弱，也能大幅提升小规模开源大模型复杂推理任务的能力，在AIME基准上取得了突破性表现。


<details>
  <summary>Details</summary>
Motivation: 当前开源小模型由于验证和改正能力较弱，难以应用于需要复杂多步推理的高级任务。作者希望探索在此类模型能力有限的情况下，是否有可能显著提升其推理表现。

Method: 将推理过程建模为马尔可夫链，通过多个随机、长链条的推理过程并行运行，只要求每一步改进的概率略高于退步，通过统计多数结果来提升整体趋近于正确答案的概率，这就是所谓的深度自我演化推理（DSER）框架。

Result: 在AIME 2024-2025 高难度测评上，DSER 使得 DeepSeek-R1-0528-Qwen3-8B 模型解决了5道此前未能解决的题目，总体表现也超越了其600B参数的教师模型单步能力。

Conclusion: DSER不仅在实践中提升了小规模开源模型在复杂推理任务上的能力，同时也揭示了当前模型自检、自纠及稳定性等方面的不足，并为今后开发具备强大自我演化推理能力的新一代模型指明了方向。

Abstract: Long-form chain-of-thought reasoning has become a cornerstone of advanced
reasoning in large language models. While recent verification-refinement
frameworks have enabled proprietary models to solve Olympiad-level problems,
their effectiveness hinges on strong, reliable verification and correction
capabilities, which remain fragile in open-weight, smaller-scale models. This
work demonstrates that even with weak verification and refinement capabilities
on hard tasks, the reasoning limits of such models can be substantially
extended through a probabilistic paradigm we call Deep Self-Evolving Reasoning
(DSER). We conceptualize iterative reasoning as a Markov chain, where each step
represents a stochastic transition in the solution space. The key insight is
that convergence to a correct solution is guaranteed as long as the probability
of improvement marginally exceeds that of degradation. By running multiple
long-horizon, self-evolving processes in parallel, DSER amplifies these small
positive tendencies, enabling the model to asymptotically approach correct
answers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On
the challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously
unsolvable problems and boosts overall performance, enabling this compact model
to surpass the single-turn accuracy of its 600B-parameter teacher through
majority voting. Beyond its immediate utility for test-time scaling, the DSER
framework serves to diagnose the fundamental limitations of current open-weight
reasoners. By clearly delineating their shortcomings in self-verification,
refinement, and stability, our findings establish a clear research agenda for
developing next-generation models with powerful, intrinsic self-evolving
capabilities.

</details>


### [266] [Lingua Custodi's participation at the WMT 2025 Terminology shared task](https://arxiv.org/abs/2510.17504)
*Jingshu Liu,Raheel Qader,Gaëtan Caillaut,Mariam Nakhlé*

Main category: cs.CL

TL;DR: 该论文提出并系统性评估了基于BERT的多语种句子嵌入方法，显著提升了跨语言语义检索和迁移学习能力，并发布了支持109种语言的模型。


<details>
  <summary>Details</summary>
Motivation: 虽然BERT在单语语义表示上表现良好，但基于BERT的跨语言句子嵌入方法未被充分探索。作者希望改进多语种嵌入表现，减少对大规模平行语料的依赖，并推动跨语言NLP任务的进步。

Method: 作者结合并实验了多种单语与跨语种表示学习方法，包括MLM、TLM、双编码器翻译排序和加性margin softmax，并用多语种预训练语言模型，与不同数据规模平行语料进行系统比较。

Result: 组合多种优势方法后，新模型在Tatoeba数据集的112种语言双文本检索任务达到83.7%的准确率，大幅超过LASER的65.5%；并在单语迁移学习任务上表现有竞争力。所挖掘平行语料用于NMT也同样取得优异结果。

Conclusion: 多语种预训练语言模型极大减少了高质量多语种句子嵌入对平行语料的需求，并推动了跨语言任务的表现。作者公开发布了涵盖109种语言的最佳模型，为相关研究和应用提供资源支持。

Abstract: While BERT is an effective method for learning monolingual sentence
embeddings for semantic similarity and embedding based transfer learning BERT
based cross-lingual sentence embeddings have yet to be explored. We
systematically investigate methods for learning multilingual sentence
embeddings by combining the best methods for learning monolingual and
cross-lingual representations including: masked language modeling (MLM),
translation language modeling (TLM), dual encoder translation ranking, and
additive margin softmax. We show that introducing a pre-trained multilingual
language model dramatically reduces the amount of parallel training data
required to achieve good performance by 80%. Composing the best of these
methods produces a model that achieves 83.7% bi-text retrieval accuracy over
112 languages on Tatoeba, well above the 65.5 achieved by LASER, while still
performing competitively on monolingual transfer learning benchmarks. Parallel
data mined from CommonCrawl using our best model is shown to train competitive
NMT models for en-zh and en-de. We publicly release our best multilingual
sentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.

</details>


### [267] [Annotation-Efficient Universal Honesty Alignment](https://arxiv.org/abs/2510.17509)
*Shiyu Ni,Keping Bi,Jiafeng Guo,Minghao Tang,Jingtong Wu,Zengxin Han,Xueqi Cheng*

Main category: cs.CL

TL;DR: 论文提出了EliCal，一种高效实现LLM“诚实对齐”的两阶段方法，并发布了大规模HonestyBench基准数据集。EliCal用极少标注达到了接近最优的效果，提升了LLM可信度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在应用中需要能够准确识别自身知识边界，并输出合理的置信度（诚实对齐），以获得用户信任。现有方法要么无需训练但效果有限，要么依赖大量人工标注，成本高且难以普及。

Method: 提出EliCal框架，分为两个阶段：首先通过自洽性（self-consistency）获得内部置信监督信号，成本低；然后用极少部分带有正确性标注的数据对置信度进行微调标定。同时发布包含大量自洽性和正确性标注的HonestyBench数据集，用于支持大规模实验。

Result: EliCal只需1000条正确性标注（大约全部监督的0.18%），即可实现接近最优的诚实对齐效果，对比仅用标定的基线法在MMLU等新任务上的泛化能力更强。

Conclusion: EliCal框架在大幅减少人工标注成本的同时，实现了可扩展、高效的LLM诚实性对齐，为大模型可信部署提供了可行方案。

Abstract: Honesty alignment-the ability of large language models (LLMs) to recognize
their knowledge boundaries and express calibrated confidence-is essential for
trustworthy deployment. Existing methods either rely on training-free
confidence estimation (e.g., token probabilities, self-consistency) or
training-based calibration with correctness annotations. While effective,
achieving universal honesty alignment with training-based calibration requires
costly, large-scale labeling. To support annotation-efficient training, we
introduce Elicitation-Then-Calibration (EliCal), a two-stage framework that
first elicits internal confidence using inexpensive self-consistency
supervision, then calibrates this confidence with a small set of correctness
annotations. To support a large-scale study, we release HonestyBench, a
benchmark covering ten free-form QA datasets with 560k training and 70k
evaluation instances annotated with correctness and self-consistency signals.
Experiments show that EliCal achieves near-optimal alignment with only 1k
correctness annotations (0.18% of full supervision) and better alignment
performance on unseen MMLU tasks than the calibration-only baseline, offering a
scalable solution toward universal honesty alignment in LLMs.

</details>


### [268] [SimBench: Benchmarking the Ability of Large Language Models to Simulate Human Behaviors](https://arxiv.org/abs/2510.17516)
*Tiancheng Hu,Joachim Baumann,Lorenzo Lupo,Dirk Hovy,Nigel Collier,Paul Röttger*

Main category: cs.CL

TL;DR: 本文提出了SimBench，这是首个大规模、标准化的人类行为模拟基准，用于系统评测大语言模型（LLM）在模拟真实人类行为方面的能力，发现现有LLM模拟能力有限，并揭示了其性能特点及瓶颈。


<details>
  <summary>Details</summary>
Motivation: 目前对大语言模型模拟人类行为的评估方法碎片化，缺乏统一的标准和可比性结果，导致难以科学准确地衡量和提升模型的模拟能力。

Method: 作者设计并发布了SimBench，整合了涵盖道德决策、经济选择等20种多样任务和全球大规模参与者的数据；用统一评价体系对主流LLM从多角度测试，并考察模型体量、推理计算量、指令微调等因素的影响。

Result: 发现现有最佳LLM的模拟能力分数仅为40.80/100，性能随模型规模对数增长，但推理算力提升无益。指令微调在共识类问题上提高正确率，在多样类问题上反而降低。模拟特定人口群体时表现不佳。模拟能力与知识密集型推理能力（如MMLU-Pro）高度相关。

Conclusion: SimBench为推动LLM人类行为模拟的科学研究和进步提供了统一基准。虽然当前模型仍有较大提升空间，但通过量化进展可加速开发更真实可靠的模拟系统。

Abstract: Large language model (LLM) simulations of human behavior have the potential
to revolutionize the social and behavioral sciences, if and only if they
faithfully reflect real human behaviors. Current evaluations are fragmented,
based on bespoke tasks and metrics, creating a patchwork of incomparable
results. To address this, we introduce SimBench, the first large-scale,
standardized benchmark for a robust, reproducible science of LLM simulation. By
unifying 20 diverse datasets covering tasks from moral decision-making to
economic choice across a large global participant pool, SimBench provides the
necessary foundation to ask fundamental questions about when, how, and why LLM
simulations succeed or fail. We show that, while even the best LLMs today have
limited simulation ability (score: 40.80/100), performance scales log-linearly
with model size. Simulation performance is not improved by increased
inference-time compute. We demonstrate an alignment-simulation trade-off:
instruction-tuning improves performance on low-entropy (consensus) questions
but degrades it on high-entropy (diverse) ones. Models particularly struggle
when simulating specific demographic groups. Finally, we demonstrate that
simulation ability correlates most strongly with deep, knowledge-intensive
reasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to
accelerate the development of more faithful LLM simulators.

</details>


### [269] [OncoReason: Structuring Clinical Reasoning in LLMs for Robust and Interpretable Survival Prediction](https://arxiv.org/abs/2510.17532)
*Raghu Vamshi Hemadri,Geetha Krishna Guruju,Kristi Topollai,Anna Ewa Choromanska*

Main category: cs.CL

TL;DR: 本文提出了一种统一的多任务学习框架，结合大语言模型（LLM）与临床推理，实现肿瘤治疗结局的可解释性预测，在多个评测中取得了当前最优结果。


<details>
  <summary>Details</summary>
Motivation: 在肿瘤结局预测任务中，模型不仅需要高准确率，还必须具备可解释性，而现有大语言模型往往缺乏结构化推理能力，难以用于高风险临床决策。

Method: 作者提出了一个联合自回归LLM和临床推理的多任务框架，在MSK-CHORD数据集上同时进行生存二分类、连续生存时间回归、以及自然语言推理生成。共测试了三种对齐策略：①传统有监督微调（SFT）；②SFT结合链式思维（CoT）提示增强推理能力；③基于专家推理轨迹的群体相对策略优化（GRPO）强化学习方法。

Result: 实验证明，CoT提升了F1分数6.0，减少了12%均方误差（MAE）；GRPO方法在BLEU、ROUGE和BERTScore等解释性及预测表现上实现了最新最好成绩。此外，现有生物医学LLM由于架构限制，常无法生成有效推理过程。

Conclusion: 推理感知对齐在多任务临床建模中至关重要，本文方法提升了模型的可解释性和可信度，为精准肿瘤学领域的大模型建立了新标杆。

Abstract: Predicting cancer treatment outcomes requires models that are both accurate
and interpretable, particularly in the presence of heterogeneous clinical data.
While large language models (LLMs) have shown strong performance in biomedical
NLP, they often lack structured reasoning capabilities critical for high-stakes
decision support. We present a unified, multi-task learning framework that
aligns autoregressive LLMs with clinical reasoning for outcome prediction on
the MSK-CHORD dataset. Our models are trained to jointly perform binary
survival classification, continuous survival time regression, and natural
language rationale generation. We evaluate three alignment strategies: (1)
standard supervised fine-tuning (SFT), (2) SFT with Chain-of-Thought (CoT)
prompting to elicit step-by-step reasoning, and (3) Group Relative Policy
Optimization (GRPO), a reinforcement learning method that aligns model outputs
to expert-derived reasoning trajectories. Experiments with LLaMa3-8B and
Med42-8B backbones demonstrate that CoT prompting improves F1 by +6.0 and
reduces MAE by 12%, while GRPO achieves state-of-the-art interpretability and
predictive performance across BLEU, ROUGE, and BERTScore. We further show that
existing biomedical LLMs often fail to produce valid reasoning traces due to
architectural constraints. Our findings underscore the importance of
reasoning-aware alignment in multi-task clinical modeling and set a new
benchmark for interpretable, trustworthy LLMs in precision oncology.

</details>


### [270] [When Annotators Disagree, Topology Explains: Mapper, a Topological Tool for Exploring Text Embedding Geometry and Ambiguity](https://arxiv.org/abs/2510.17548)
*Nisrine Rair,Alban Goupil,Valeriu Vrabie,Emmanuel Chochoy*

Main category: cs.CL

TL;DR: 本文提出了一种利用拓扑数据分析工具Mapper分析语言模型中歧义处理的新方法。


<details>
  <summary>Details</summary>
Motivation: 准确率等标量指标无法揭示模型对歧义的内部表征，尤其是在人工标注者意见不一致的情况下，因此需要更深入的分析方法。

Method: 作者将拓扑数据分析工具Mapper应用于RoBERTa-Large模型在MD-Offense数据集上的嵌入空间，分析微调后的空间结构与模型预测、真实标签之间的关系。

Result: 微调后，嵌入空间被重组为与模型预测对应的模块化、非凸区域，即使对于高度歧义的数据也是如此。98%以上的连通分支中预测纯度≥90%，但在歧义数据上与真实标签的一致性下降，揭示了结构自信和标签不确定性之间的矛盾。

Conclusion: Mapper能直接揭示模型决策区域、边界塌陷和过度自信的聚类，是诊断模型处理歧义能力的有力工具，并为主观性NLP任务提供了新的建模指标和分析路径。

Abstract: Language models are often evaluated with scalar metrics like accuracy, but
such measures fail to capture how models internally represent ambiguity,
especially when human annotators disagree. We propose a topological perspective
to analyze how fine-tuned models encode ambiguity and more generally instances.
  Applied to RoBERTa-Large on the MD-Offense dataset, Mapper, a tool from
topological data analysis, reveals that fine-tuning restructures embedding
space into modular, non-convex regions aligned with model predictions, even for
highly ambiguous cases. Over $98\%$ of connected components exhibit $\geq 90\%$
prediction purity, yet alignment with ground-truth labels drops in ambiguous
data, surfacing a hidden tension between structural confidence and label
uncertainty.
  Unlike traditional tools such as PCA or UMAP, Mapper captures this geometry
directly uncovering decision regions, boundary collapses, and overconfident
clusters. Our findings position Mapper as a powerful diagnostic tool for
understanding how models resolve ambiguity. Beyond visualization, it also
enables topological metrics that may inform proactive modeling strategies in
subjective NLP tasks.

</details>


### [271] [Language Confusion Gate: Language-Aware Decoding Through Model Self-Distillation](https://arxiv.org/abs/2510.17555)
*Collin Zhang,Fei Huang,Chenhan Yuan,Junyang Lin*

Main category: cs.CL

TL;DR: 该论文提出了Language Confusion Gate（LCG）方法，在不改变基础大模型的情况下，有效降低大语言模型生成文本时发生的语言混淆现象。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成多语言文本时，常常会出现非预期的语言混用现象（语言混淆），现有的解决方案往往需要重新训练模型或无法分辨有害的混淆与可接受的代码切换，因此需要一种轻量级、插件式且智能的解决方式。

Method: 作者提出了LCG（Language Confusion Gate）作为一种插件式的解码时token过滤模块：① 使用归一化的自蒸馏训练方法，预测适合的语言族；② 只在必要时对候选token进行mask过滤；③ 不需改变基础模型参数。方法依据三个关键发现：一、语言混淆很少发生；二、目标正确语言的token通常在高概率预测中；三、高资源语言的token向量范数较大，导致采样偏置。

Result: 在Qwen3、GPT-OSS、Gemma3、Llama3.1等多种主流模型中评测，LCG能显著降低语言混淆发生率，多为数量级上的下降，并且对模型性能没有负面影响。

Conclusion: LCG是一种轻量级、高效并通用的解决大模型语言混淆问题的方法，不影响原模型其他任务的性能，具有实际部署价值。

Abstract: Large language models (LLMs) often experience language confusion, which is
the unintended mixing of languages during text generation. Current solutions to
this problem either necessitate model retraining or cannot differentiate
between harmful confusion and acceptable code-switching. This paper introduces
the Language Confusion Gate (LCG), a lightweight, plug-in solution that filters
tokens during decoding without altering the base LLM. The LCG is trained using
norm-adjusted self-distillation to predict appropriate language families and
apply masking only when needed. Our method is based on the findings that
language confusion is infrequent, correct-language tokens are usually among the
top predictions, and output token embedding norms are larger for high-resource
languages, which biases sampling. When evaluated across various models,
including Qwen3, GPT-OSS, Gemma3, Llama3.1, LCG decreases language confusion
significantly, often by an order of magnitude, without negatively impacting
task performance. Code is available at
https://github.com/collinzrj/language_confusion_gate.

</details>


### [272] [HGAdapter: Hypergraph-based Adapters in Language Models for Code Summarization and Clone Detection](https://arxiv.org/abs/2510.17591)
*Guang Yang,Yujie Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种利用高阶相关性增强编程语言模型（PLMs）表现的方法。


<details>
  <summary>Details</summary>
Motivation: 当前的预训练语言模型在代码任务中表现良好，但未考虑代码中的高阶数据相关性，如语法树、词汇和行之间的关联。

Method: 提出了三种代码高阶相关性（抽象语法树家族相关性、词汇相关性、行相关性），设计了tokens与超边生成器以捕捉这些高阶数据相关性，并改进超图神经网络结构，结合adapter tuning，提出了HGAdapter用于微调现有PLMs。

Result: 在多个公开数据集（覆盖六种编程语言的代码摘要和代码克隆检测任务）上实验，HGAdapter能够提升PLMs在不同数据集上的表现，改善程度视任务不同有所变化。

Conclusion: 引入代码中的高阶数据相关性有助于提升PLMs在代码相关任务上的效果，HGAdapter可泛化于多种PLMs以获得性能增强。

Abstract: Pre-trained language models (PLMs) are increasingly being applied to
code-related tasks. Although PLMs have achieved good results, they do not take
into account potential high-order data correlations within the code. We propose
three types of high-order correlations in code tokens, i.e. abstract syntax
tree family correlation, lexical correlation, and line correlation. We design a
tokens and hyperedges generator to capture these high-order data correlations.
We improve the architecture of hypergraph neural networks and combine it with
adapter tuning to propose a novel hypergraph-based adapter (HGAdapter) to
fine-tune PLMs. HGAdapter can encode high-order data correlations and is
allowed to be inserted into various PLMs to enhance performance. Experiments
were conducted on several public datasets, including six languages of code
summarization and code clone detection tasks. Our methods improved the
performance of PLMs in datasets to varying degrees. Experimental results
validate the introduction of high-order data correlations that contribute to
improved effectiveness.

</details>


### [273] [LawChain: Modeling Legal Reasoning Chains for Chinese Tort Case Analysis](https://arxiv.org/abs/2510.17602)
*Huiyuan Xie,Chenyang Li,Huining Zhu,Chubin Zhang,Yuxiao Ye,Zhenghao Liu,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 本文提出了一种面向中国侵权类民事案件的法律推理新框架LawChain，并构建了相关评测基准，对现有大模型在相关场景下的法律推理能力进行评估，结果显示当前模型在侵权法律推理上仍有不足。通过引入LawChain式推理的基线方法，性能大幅提升，并具备良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有法律推理的计算方法多以通用推理框架为主，缺乏对法律推理细致过程的建模，且主要集中在刑事案件，民事案件（如侵权分析）的建模不足。论文旨在完善民事案件法律推理的计算方法。

Method: 将侵权分析中常用法律推理流程抽象为LawChain三模块推理框架，每模块含多层细化子步骤。在此基础上，提出侵权推理任务，并构建LawChain_eval评测数据集，对推理关键环节进行系统性评估。评估当前SOTA大模型法律推理能力，并设计结合LawChain推理的提示或训练基线方法提升其表现。

Result: 实验显示当前大模型对侵权推理关键要素表现不佳。结合LawChain推理链的各类基线方法能显著提升模型推理能力，在法律实体识别、刑事损害赔偿计算等任务上也表现出良好泛化能力。

Conclusion: 显式建模法律推理链对提升大模型法律推理能力有效，LawChain方法在侵权及其他相关法律分析任务中均具推广和应用价值。

Abstract: Legal reasoning is a fundamental component of legal analysis and
decision-making. Existing computational approaches to legal reasoning
predominantly rely on generic reasoning frameworks such as syllogism and IRAC,
which do not comprehensively examine the nuanced processes that underpin legal
reasoning. Moreover, current research has largely focused on criminal cases,
with insufficient modeling for civil cases. In this work, we present a novel
framework for explicitly modeling legal reasoning in the analysis of Chinese
tort-related civil cases. We first operationalize the legal reasoning processes
used in tort analysis into the LawChain framework. LawChain is a three-module
reasoning framework, with each module consisting of multiple finer-grained
sub-steps. Informed by the LawChain framework, we introduce the task of tort
legal reasoning and construct an evaluation benchmark, LawChain$_{eval}$, to
systematically assess the critical steps within analytical reasoning chains for
tort analysis. Leveraging this benchmark, we evaluate state-of-the-art large
language models for their legal reasoning ability in civil tort contexts. Our
results indicate that current models still fall short in accurately handling
crucial elements of tort legal reasoning. Furthermore, we introduce several
baseline approaches that explicitly incorporate LawChain-style reasoning
through prompting or post-training. We conduct further experiments on
additional legal analysis tasks, such as Legal Named-Entity Recognition and
Criminal Damages Calculation, to verify the generalizability of these
baselines. The proposed baseline approaches achieve significant improvements in
tort-related legal reasoning and generalize well to related legal analysis
tasks, thus demonstrating the value of explicitly modeling legal reasoning
chains to enhance the reasoning capabilities of language models.

</details>


### [274] [Forget to Know, Remember to Use: Context-Aware Unlearning for Large Language Models](https://arxiv.org/abs/2510.17620)
*Yuefeng Peng,Parnian Afshar,Megan Ganji,Thomas Butler,Amir Houmansadr,Mingxian Wang,Dezhi Hong*

Main category: cs.CL

TL;DR: 本论文提出在大模型的遗忘（unlearning）过程中，引入新目标以提高模型在相关知识被上下文重新引入时的表现。实验显示，新方法有效恢复模型的上下文利用能力，并保持遗忘与保留知识的平衡。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型常包含敏感或过时信息，需求控制其记忆内容。直接重训练代价高，研究人员转向unlearning技术来移除特定知识。但遗忘后，用户若在输入中再次提供相关信息，模型常无法灵活利用，这限制了模型的实用性。

Method: 作者对现有六种主流unlearning方法进行系统评估，并发现它们普遍削弱了模型在及时上下文中活用已遗忘知识的能力。为此，论文在遗忘目标中新增plug-in项，专门优化模型在上下文提供目标知识时的利用能力。

Result: 实验表明，新增plug-in项后，模型对被遗忘知识的上下文利用能力几乎恢复到原始水平，同时保持了目标知识的有效遗忘和非删集合上的表现。

Conclusion: 该研究提出的方法在不牺牲遗忘及整体性能的前提下，显著增强了模型对已忘知识的灵活应用，提升了unlearning技术的实用性。

Abstract: Large language models may encode sensitive information or outdated knowledge
that needs to be removed, to ensure responsible and compliant model responses.
Unlearning has emerged as an efficient alternative to full retraining, aiming
to remove specific knowledge while preserving overall model utility. Existing
evaluations of unlearning methods focus on (1) the extent of forgetting of the
target knowledge (forget set) and (2) maintaining performance on the retain set
(i.e., utility). However, these evaluations overlook an important usability
aspect: users may still want the model to leverage the removed information if
it is re-introduced in the prompt. In a systematic evaluation of six
state-of-the-art unlearning methods, we find that they consistently impair such
contextual utility. To address this, we augment unlearning objectives with a
plug-in term that preserves the model's ability to use forgotten knowledge when
it is present in context. Extensive experiments demonstrate that our approach
restores contextual utility to near original levels while still maintaining
effective forgetting and retain-set utility.

</details>


### [275] [Qomhra: A Bilingual Irish-English Large Language Model](https://arxiv.org/abs/2510.17652)
*Joseph McInerney*

Main category: cs.CL

TL;DR: 本文介绍了Qomhr'a，这是一个爱尔兰语-英语双语大语言模型（LLM），针对低资源环境开发，涵盖了双语持续预训练、指令微调及基于人类偏好的对齐。


<details>
  <summary>Details</summary>
Motivation: 由于爱尔兰语为低资源语言，现有的大语言模型在爱尔兰语上的表现有限。因此，作者希望提高LLM在爱尔兰语上的能力，同时保留其对英语的理解和生成能力。

Method: 作者新采集和整理了爱尔兰语与英语的语料，通过混合训练提高双语表现。评估了6个闭源LLM对爱尔兰语文本生成的能力，并利用Gemini-2.5-Pro生成指令微调和人类偏好数据集（包括30K规模的平行指令数据集和1K的人类偏好数据集）。Qomhr'a经过持续预训练和指令微调，并在多项基准测试上进行评估。

Result: Qomhr'a在多个评测任务如翻译、性别理解、主题识别和世界知识上表现优异，爱尔兰语提升最高达29%，英语提升达44%。微调后其指令跟随能力显著增强，适合于对话机器人应用。

Conclusion: Qomhr'a通过创新数据采集与训练策略，有效提升了低资源语言爱尔兰语的模型能力，是解决多语言不平衡问题的重要进展，并为今后双语及低资源语言模型的开发提供了新方法。

Abstract: This paper introduces Qomhr\'a, a bilingual Irish-English large language
model (LLM), developed under low-resource constraints presenting a complete
pipeline spanning bilingual continued pre-training, instruction tuning, and
alignment from human preferences. Newly accessible Irish corpora and English
text are mixed and curated to improve Irish performance while preserving
English ability. 6 closed-weight LLMs are judged for their Irish text
generation by a native speaker, a learner and other LLMs. Google's
Gemini-2.5-Pro is ranked the highest and is subsequently used to synthesise
instruction tuning and human preference datasets. Two datasets are contributed
leveraging Gemini-2.5-Pro: a 30K Irish-English parallel instruction tuning
dataset and a 1K human preference dataset, generating accepted and rejected
responses that show near perfect alignment with a native Irish speaker.
Qomhr\'a is comprehensively evaluated across benchmarks testing translation,
gender understanding, topic identification and world knowledge with gains of up
to 29% in Irish and 44% in English. Qomhr\'a also undergoes instruction tuning
and demonstrates clear progress in instruction following, crucial for chatbot
functionality.

</details>


### [276] [Towards Mining Effective Pedagogical Strategies from Learner-LLM Educational Dialogues](https://arxiv.org/abs/2510.17698)
*Liqun He,Manolis Mavrikis,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 本论文提出在教育场景下分析学习者与大模型（LLM）对话，以发现和评估有效的教学策略，弥补以往只关注技术或学习结果的不足。


<details>
  <summary>Details</summary>
Motivation: 目前教育中对LLM应用的评估往往忽视了学习者与LLM之间的实际对话互动，本研究旨在通过对话分析方法，深入挖掘对话动态和教学策略，以提升教育场景下LLM应用的有效性。

Method: 研究采用收集学习者与LLM的对话数据，对对话行为进行注释，挖掘对话行为模式，并建立预测模型。

Result: 初步结果已得到一些洞见，揭示对话过程中的教学策略和互动特征，为后续进一步研究提供了基础。

Conclusion: 通过关注对话动态和教学策略，本文强调提升基于LLM的教育应用质量的重要性，建议未来对教育类LLM的评估应重视互动过程。

Abstract: Dialogue plays a crucial role in educational settings, yet existing
evaluation methods for educational applications of large language models (LLMs)
primarily focus on technical performance or learning outcomes, often neglecting
attention to learner-LLM interactions. To narrow this gap, this AIED Doctoral
Consortium paper presents an ongoing study employing a dialogue analysis
approach to identify effective pedagogical strategies from learner-LLM
dialogues. The proposed approach involves dialogue data collection, dialogue
act (DA) annotation, DA pattern mining, and predictive model building. Early
insights are outlined as an initial step toward future research. The work
underscores the need to evaluate LLM-based educational applications by focusing
on dialogue dynamics and pedagogical strategies.

</details>


### [277] [QueST: Incentivizing LLMs to Generate Difficult Problems](https://arxiv.org/abs/2510.17715)
*Hanxu Hu,Xingxing Zhang,Jannis Vamvas,Rico Sennrich,Furu Wei*

Main category: cs.CL

TL;DR: 提出了QueST框架，用于大规模生成高难度合成编程题数据；通过训练出的生成器辅助蒸馏和微调小模型，在多个评测上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在推理和竞赛级编码任务上已表现优秀，但受到高质量人工标注数据稀缺的瓶颈限制，难以进一步扩展，尤其是在大规模高难度编程题的数据上。现有竞争性编程题数据集规模有限，且合成数据手段不足。

Method: 提出了QueST框架，结合难度感知的图采样和拒绝式微调，优化题目生成器以直接创造高难度编程题。利用训练好的生成器批量生成合成高难度题，进而用于从强师模型蒸馏，以及对小模型实施强化训练。

Result: 实验表明：用QueST生成的10万道高难度题微调Qwen3-8B-base后，其在LiveCodeBench上超越原始Qwen3-8B；额外加入112K题（含多方案合成解答的人写题）后，性能等同于更大规模的DeepSeek-R1-671B。

Conclusion: QueST可高效合成复杂题目，有效提升大模型在竞赛编程及推理任务上的能力，突破标注数据瓶颈，为大模型智能提升提供了新思路和具备可扩展性的方案。

Abstract: Large Language Models have achieved strong performance on reasoning tasks,
solving competition-level coding and math problems. However, their scalability
is limited by human-labeled datasets and the lack of large-scale, challenging
coding problem training data. Existing competitive coding datasets contain only
thousands to tens of thousands of problems. Previous synthetic data generation
methods rely on either augmenting existing instruction datasets or selecting
challenging problems from human-labeled data. In this paper, we propose QueST,
a novel framework which combines difficulty-aware graph sampling and
difficulty-aware rejection fine-tuning that directly optimizes specialized
generators to create challenging coding problems. Our trained generators
demonstrate superior capability compared to even GPT-4o at creating challenging
problems that benefit downstream performance. We leverage QueST to generate
large-scale synthetic coding problems, which we then use to distill from strong
teacher models with long chain-of-thought or to conduct reinforcement learning
for smaller models, proving effective in both scenarios. Our distillation
experiments demonstrate significant performance gains. Specifically, after
fine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we
surpass the performance of the original Qwen3-8B on LiveCodeBench. With an
additional 112K examples (i.e., 28K human-written problems paired with multiple
synthetic solutions), our 8B model matches the performance of the much larger
DeepSeek-R1-671B. These findings indicate that generating complex problems via
QueST offers an effective and scalable approach to advancing the frontiers of
competitive coding and reasoning for large language models.

</details>


### [278] [PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity Recognition](https://arxiv.org/abs/2510.17720)
*Nanda Kumar Rengarajan,Jun Yan,Chun Wang*

Main category: cs.CL

TL;DR: 本文提出了一种面对低资源环境下命名实体识别（NER）任务的新型轻量级few-shot框架，借助创新的指令模板与数据增强技术，极大提升了少量标注数据下的NER表现。


<details>
  <summary>Details</summary>
Motivation: 命名实体识别需要大量标注数据，获取标签成本高昂，尤其是在低资源领域。现有零样本或指令微调方法对领域实体泛化有限，且未充分利用有限数据。

Method: 1）设计了一种全新指令微调模板，结合以往IT方法的优点，通过简化输出格式和充分利用大语言模型的上下文窗口；2）提出了一种数据增强策略，对实体周围上下文进行释义处理，同时保留实体本身信息，扩充训练数据且不破坏语义关系。

Result: 在多个基准数据集上进行实验，few-shot方法在CrossNER数据集上平均F1达80.1，基于释义增强的数据训练模型比基础版本F1提升最高达17分，接近SOTA。

Conclusion: 该方法可实现低算力、少量标注数据条件下的高效NER，在few-shot和zero-shot场景下具有强大普适性，为缺乏训练资源的群体提供了有效解决方案。

Abstract: Named Entity Recognition (NER) is a critical task that requires substantial
annotated data, making it challenging in low-resource scenarios where label
acquisition is expensive. While zero-shot and instruction-tuned approaches have
made progress, they often fail to generalize to domain-specific entities and do
not effectively utilize limited available data. We present a lightweight
few-shot NER framework that addresses these challenges through two key
innovations: (1) a new instruction tuning template with a simplified output
format that combines principles from prior IT approaches to leverage the large
context window of recent state-of-the-art LLMs; (2) introducing a strategic
data augmentation technique that preserves entity information while
paraphrasing the surrounding context, thereby expanding our training data
without compromising semantic relationships. Experiments on benchmark datasets
show that our method achieves performance comparable to state-of-the-art models
on few-shot and zero-shot tasks, with our few-shot approach attaining an
average F1 score of 80.1 on the CrossNER datasets. Models trained with our
paraphrasing approach show consistent improvements in F1 scores of up to 17
points over baseline versions, offering a promising solution for groups with
limited NER training data and compute power.

</details>


### [279] [AcademicEval: Live Long-Context LLM Benchmark](https://arxiv.org/abs/2510.17725)
*Haozhen Zhang,Tao Feng,Pengrui Han,Jiaxuan You*

Main category: cs.CL

TL;DR: 本文提出了AcademicEval，这是一个用于大语言模型（LLMs）长上下文生成能力评测的新基准，基于arXiv学术论文任务，无需人工标注，且避免了标签泄漏问题。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文LLM评测基准存在上下文长度固定、人工标注劳动密集、训练过程存在标签泄漏等问题，不能全面且高效地评估LLMs对长文本的理解和生成能力。

Method: 作者提出了AcademicEval基准，从arXiv学术论文中构建多种学术写作任务（如标题、摘要、引言、相关工作），无需人工标注，覆盖不同抽象层次。利用合著者图收集高质量、专家筛选的few-shot示例，实现灵活的上下文长度。同时，采用高效的实时评测方法，确保无标签泄漏。

Result: 实验表明，现有LLM在涉及多个抽象层次的任务上表现较差，且在应对长few-shot示例时有明显困难，说明该基准任务具有挑战性。此外，实验分析也揭示了提升LLMs长上下文建模能力的一些见解。

Conclusion: AcademicEval作为一个无需人工标注且能有效防止标签泄漏的长上下文LLM评测基准，能够更全面准确地评估LLMs在长篇学术文本生成与理解上的能力，并为后续模型优化提供了有价值的研究基础。

Abstract: Large Language Models (LLMs) have recently achieved remarkable performance in
long-context understanding. However, current long-context LLM benchmarks are
limited by rigid context length, labor-intensive annotation, and the pressing
challenge of label leakage issues during LLM training. Therefore, we propose
\textsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context
generation tasks. \textsc{AcademicEval} adopts papers on arXiv to introduce
several academic writing tasks with long-context inputs, \textit{i.e.},
\textsc{Title}, \textsc{Abstract}, \textsc{Introduction}, and \textsc{Related
Work}, which cover a wide range of abstraction levels and require no manual
labeling. Moreover, \textsc{AcademicEval} integrates high-quality and
expert-curated few-shot demonstrations from a collected co-author graph to
enable flexible context length. Especially, \textsc{AcademicEval} features an
efficient live evaluation, ensuring no label leakage. We conduct a holistic
evaluation on \textsc{AcademicEval}, and the results illustrate that LLMs
perform poorly on tasks with hierarchical abstraction levels and tend to
struggle with long few-shot demonstrations, highlighting the challenge of our
benchmark. Through experimental analysis, we also reveal some insights for
enhancing LLMs' long-context modeling capabilities. Code is available at
https://github.com/ulab-uiuc/AcademicEval

</details>


### [280] [Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations](https://arxiv.org/abs/2510.17733)
*Tong Chen,Akari Asai,Luke Zettlemoyer,Hannaneh Hajishirzi,Faeze Brahman*

Main category: cs.CL

TL;DR: 本文提出了一种基于二值检索增强奖励（RAR）的在线强化学习方法，显著降低了大模型的幻觉（hallucination）率，并在确保事实正确性的同时不影响其他下游任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在生成文本时常会输出与实际事实不符的信息（即幻觉），而当前一些缓解方法又会使模型在生成任务或下游任务上的表现下降，亟需一种既能减少幻觉又不损害整体性能的新方法。

Method: 作者提出了一种新的在线强化学习方法，核心是使用二值检索增强奖励（RAR）：当且仅当模型输出完全事实正确时才给予奖励1，否则为0。该方法应用于Qwen3推理模型，并与基于连续奖励和有监督训练的方法进行了系统对比。

Result: 在开放式生成任务中，该方法将幻觉率降低了39.3%；在短文本问答中，模型学会了在知识不足时选择"我不知道"，分别在PopQA和GPQA数据集上减少了44.4%和21.7%的错误答案。此外，与连续奖励的RL方法相比，没有导致指令遵循、数学或代码等任务性能的退化。

Conclusion: 文中提出的二值RAR强化学习框架能有效减少语言模型的事实错误。而与以往方法不同，该方法不会损害模型在其他关键任务上的表现，展示出更优的实用性和灵活性。

Abstract: Language models often generate factually incorrect information unsupported by
their training data, a phenomenon known as extrinsic hallucination. Existing
mitigation approaches often degrade performance on open-ended generation and
downstream tasks, limiting their practical utility. We propose an online
reinforcement learning method using a novel binary retrieval-augmented reward
(RAR) to address this tradeoff. Unlike continuous reward schemes, our approach
assigns a reward of one only when the model's output is entirely factually
correct, and zero otherwise. We evaluate our method on Qwen3 reasoning models
across diverse tasks. For open-ended generation, binary RAR achieves a 39.3%
reduction in hallucination rates, substantially outperforming both supervised
training and continuous-reward RL baselines. In short-form question answering,
the model learns calibrated abstention, strategically outputting "I don't know"
when faced with insufficient parametric knowledge. This yields 44.4% and 21.7%
fewer incorrect answers on PopQA and GPQA, respectively. Crucially, these
factuality gains come without performance degradation on instruction following,
math, or code, whereas continuous-reward RL, despite improving factuality,
induces quality regressions.

</details>


### [281] [Evaluating Medical LLMs by Levels of Autonomy: A Survey Moving from Benchmarks to Applications](https://arxiv.org/abs/2510.17764)
*Xiao Ye,Jacob Dineen,Zhaonan Li,Zhikun Xu,Weiyu Chen,Shijie Lu,Yuxi Huang,Ming Shen,Phu Tran,Ji-Eun Irene Yum,Muhammad Ali Khan,Muhammad Umar Afzal,Irbaz Bin Riaz,Ben Zhou*

Main category: cs.CL

TL;DR: 本文对医疗大模型的评估方法进行了重新构建，引入了基于自主性分级（L0-L3）的评估框架，以更好指导其真实临床应用的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 医学大模型在标准基准测试中表现优异，但这些分数仍难以直接转化为临床工作流中的安全可靠应用，因此需要新的评估视角来弥补这一落差。

Method: 作者提出将模型的评估与自主性分级（L0-L3）相结合，从信息工具、信息转化与聚合、决策支持和受监督代理等层面对其进行评估，并关联现有指标和基准，明确各级别可执行的动作与相关风险。

Result: 制定了按自主性等级选择评估指标、收集证据和报告声明的指导蓝图，并提出将评估与监督相结合的未来方向。

Conclusion: 以自主性为核心的评估体系能推动医学大模型从单纯的分数对比，迈向风险感知和真实可用的临床证据，为模型的临床可靠部署奠定基础。

Abstract: Medical Large language models achieve strong scores on standard benchmarks;
however, the transfer of those results to safe and reliable performance in
clinical workflows remains a challenge. This survey reframes evaluation through
a levels-of-autonomy lens (L0-L3), spanning informational tools, information
transformation and aggregation, decision support, and supervised agents. We
align existing benchmarks and metrics with the actions permitted at each level
and their associated risks, making the evaluation targets explicit. This
motivates a level-conditioned blueprint for selecting metrics, assembling
evidence, and reporting claims, alongside directions that link evaluation to
oversight. By centering autonomy, the survey moves the field beyond score-based
claims toward credible, risk-aware evidence for real clinical use.

</details>


### [282] [Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains](https://arxiv.org/abs/2510.17793)
*Austin Xu,Xuan-Phi Nguyen,Yilun Zhou,Chien-Sheng Wu,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TL;DR: 本文提出了一种通过大规模数据集微调的自动推理评估器（FARE），其效果超越了体量更大的RL训练基准模型，并在多个实际任务中展现出强大性能，刷新了评测标准。


<details>
  <summary>Details</summary>
Motivation: 现有生成式评估器多依赖新方法（如RL）微调，缺乏利用大规模数据集进行数据驱动开发。作者希望通过数据扩展和更高效的SFT微调方法，提升生成式模型评估的能力和通用性。

Method: 作者整理了包含250万样本、涵盖五类推理评估任务的大型数据集，跨多个领域。基于该数据集，采用逐步拒绝采样的SFT法微调了8B和20B参数量的基础推理评估器（FARE）。

Result: FARE-8B在各项任务上能竞争甚至超越更大参数量的RL训练专用模型，FARE-20B成为开源评测器新标杆，超越了70B+参数的同类模型。在数学、代码等真实应用场景下，FARE家族表现优异，提升RL模型达14.1%，在代码测试质量评估任务超越其它20B模型65%。

Conclusion: 用大规模数据和高效的SFT微调可以训练出性能优异、通用性强的自动推理评估器，不仅提升多项下游任务模型表现，还推动了开源自动评测器的发展水平。

Abstract: Finetuning specialized generative evaluators has emerged as a popular
paradigm to meet the increasing demand for scalable evaluation during both
training and test-time. However, recent work has largely focused on applying
new methodology, such as reinforcement learning (RL), to training evaluators,
shying away from large-scale, data-driven development. In this work, we focus
on data scaling, curating a set of 2.5M samples spanning five unique evaluation
tasks (pairwise, step-level, reference-free and reference-based verification,
and single rating) and multiple domains focused on reasoning evaluation. With
our data, we train Foundational Automatic Reasoning Evaluators (FARE), a family
of 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative
rejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges
larger specialized RL-trained evaluators and FARE-20B sets the new standard for
open-source evaluators, surpassing specialized 70B+ evaluators. Beyond static
benchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,
FARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,
FARE improves the downstream RL-trained model performance by up to 14.1% vs.
string-matching verifiers. When initialized from FARE, a continually-finetuned
FARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.

</details>


### [283] [Executable Knowledge Graphs for Replicating AI Research](https://arxiv.org/abs/2510.17795)
*Yujie Luo,Zhuoyun Yu,Xuehai Wang,Yuqi Zhu,Ningyu Zhang,Lanning Wei,Lun Du,Da Zheng,Huajun Chen*

Main category: cs.CL

TL;DR: 本文提出了可执行知识图（xKG），该系统自动整合论文中的技术细节和代码片段，有效提升了大语言模型代理在AI研究复现中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在自动复现AI研究方面存在诸多挑战，主要体现在生成可执行代码能力弱，缺乏必要的背景知识，以及检索增强生成方法（RAG）难以提取论文中隐含的技术细节。同时，以往方法忽视了实现级别的关键信号，也缺少支持多粒度检索和重用的结构化知识表示。因此，需要一种集成和组织这些信息的新方法，提升代理自动化复现的能力。

Method: 作者提出了Executable Knowledge Graphs（xKG），这是一种模块化且可插拔的知识库，能够自动从科学文献中抽取技术洞见、代码片段与领域知识，并组织为易于检索和复用的结构化知识图。xKG可无缝整合到不同LLM代理框架中，提升其生成可执行代码和理解深度技术内容的能力。

Result: 将xKG集成到三种代理框架、两种不同大语言模型中，在PaperBench基准上实现了显著性能提升（比如在o3-mini模型上提升了10.9%），显示了xKG作为通用、可扩展解决方案的有效性。

Conclusion: xKG能够高效集成文献中的代码与知识，显著提升了代理在AI研究自动复现中的能力，是实现AI研究自动复现的通用且可扩展的工具，对AI领域知识的结构化积累与应用具有重要价值。

Abstract: Replicating AI research is a crucial yet challenging task for large language
model (LLM) agents. Existing approaches often struggle to generate executable
code, primarily due to insufficient background knowledge and the limitations of
retrieval-augmented generation (RAG) methods, which fail to capture latent
technical details hidden in referenced papers. Furthermore, previous approaches
tend to overlook valuable implementation-level code signals and lack structured
knowledge representations that support multi-granular retrieval and reuse. To
overcome these challenges, we propose Executable Knowledge Graphs (xKG), a
modular and pluggable knowledge base that automatically integrates technical
insights, code snippets, and domain-specific knowledge extracted from
scientific literature. When integrated into three agent frameworks with two
different LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on
PaperBench, demonstrating its effectiveness as a general and extensible
solution for automated AI research replication. Code will released at
https://github.com/zjunlp/xKG.

</details>


### [284] [Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics](https://arxiv.org/abs/2510.17797)
*Akshara Prabhakar,Roshan Ram,Zixiang Chen,Silvio Savarese,Frank Wang,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.CL

TL;DR: 本文提出了一个多智能体系统——Enterprise Deep Research（EDR），用于将企业中的非结构化数据转化为可用洞见，并在多个公开基准上优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 面对信息的指数级增长，企业急需智能系统，自动化地理解和处理各种复杂、非结构化的信息，并生成具有业务价值的报告，但现有智能体在行业知识、意图对齐和系统集成上存在局限。

Method: EDR由主控规划智能体管理任务分解，配合4个专业搜索智能体（通用、学术、GitHub、LinkedIn），集成基于MCP的工具生态系统（支持NL2SQL、文件分析和工作流），并配有可视化和反思机制，自动补足知识盲区，可选择引入人工指导。

Result: 在内部数据集和开放基准（DeepResearch Bench、DeepConsult）上，EDR在无需人工干预的情况下，均超越了现有最先进的多智能体系统；系统可自动生成报告、实时流式反馈并无缝在企业中部署。

Conclusion: EDR多智能体系统在企业信息洞察和自动化报告生成方面展现出优越性能，推动了多智能体推理和应用的研究发展，同时开源了代码和基准数据，促进社区进一步探索。

Abstract: As information grows exponentially, enterprises face increasing pressure to
transform unstructured data into coherent, actionable insights. While
autonomous agents show promise, they often struggle with domain-specific
nuances, intent alignment, and enterprise integration. We present Enterprise
Deep Research (EDR), a multi-agent system that integrates (1) a Master Planning
Agent for adaptive query decomposition, (2) four specialized search agents
(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool
ecosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a
Visualization Agent for data-driven insights, and (5) a reflection mechanism
that detects knowledge gaps and updates research direction with optional
human-in-the-loop steering guidance. These components enable automated report
generation, real-time streaming, and seamless enterprise deployment, as
validated on internal datasets. On open-ended benchmarks including DeepResearch
Bench and DeepConsult, EDR outperforms state-of-the-art agentic systems without
any human steering. We release the EDR framework and benchmark trajectories to
advance research on multi-agent reasoning applications.
  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and
Dataset at https://huggingface.co/datasets/Salesforce/EDR-200

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [285] [VAR-SLAM: Visual Adaptive and Robust SLAM for Dynamic Environments](https://arxiv.org/abs/2510.16205)
*João Carlos Virgolino Soares,Gabriel Fischer Abati,Claudio Semini*

Main category: cs.RO

TL;DR: 提出了一种新型SLAM系统VAR-SLAM，在动态环境下显著提升了轨迹精度和鲁棒性，对已知和未知运动物体都有良好表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉SLAM方法在动态环境下效果不佳，主要因为针对已知物体类别的语义过滤有限或使用固定鲁棒核无法适应未知运动物体，导致SLAM精度降低。

Method: 基于ORB-SLAM3，结合轻量语义关键点过滤（处理已知运动物体）与自适应Barron鲁棒损失（应对未知运动物体）。鲁棒核参数可根据残差在线估计，自适应调整损失函数特性。

Result: 在包括已知和未知运动物体的多个数据集（TUM RGB-D、Bonn RGB-D Dynamic、OpenLORIS）上，VAR-SLAM轨迹准确率和鲁棒性均优于现有主流方法，相较NGD-SLAM，某些序列ATE RMSE降低达25%，平均速度达27 FPS。

Conclusion: VAR-SLAM在动态环境下有效提升了视觉SLAM系统对运动物体的适应性、精度和实时性，优于现有技术，适合实际应用。

Abstract: Visual SLAM in dynamic environments remains challenging, as several existing
methods rely on semantic filtering that only handles known object classes, or
use fixed robust kernels that cannot adapt to unknown moving objects, leading
to degraded accuracy when they appear in the scene. We present VAR-SLAM (Visual
Adaptive and Robust SLAM), an ORB-SLAM3-based system that combines a
lightweight semantic keypoint filter to deal with known moving objects, with
Barron's adaptive robust loss to handle unknown ones. The shape parameter of
the robust kernel is estimated online from residuals, allowing the system to
automatically adjust between Gaussian and heavy-tailed behavior. We evaluate
VAR-SLAM on the TUM RGB-D, Bonn RGB-D Dynamic, and OpenLORIS datasets, which
include both known and unknown moving objects. Results show improved trajectory
accuracy and robustness over state-of-the-art baselines, achieving up to 25%
lower ATE RMSE than NGD-SLAM on challenging sequences, while maintaining
performance at 27 FPS on average.

</details>


### [286] [DeGrip: A Compact Cable-driven Robotic Gripper for Desktop Disassembly](https://arxiv.org/abs/2510.16231)
*Bihao Zhang,Davood Soleymanzadeh,Xiao Liang,Minghui Zheng*

Main category: cs.RO

TL;DR: 本文提出了一种名为DeGrip的定制化夹爪，用于老旧台式机的智能拆解，并通过仿真环境对其实用性和灵活性进行验证。


<details>
  <summary>Details</summary>
Motivation: 智能机器人在废旧产品拆解领域应用受限，其中硬件专用性不足是关键瓶颈，因此亟需专为此类任务设计的高效硬件夹具。

Method: 设计并实现了DeGrip夹爪，具备三自由度，通过线缆驱动减少整体体积，适用于狭窄空间；关节驱动解耦提升灵活性。同步开发基于Isaac Sim的仿真测试环境，用于评估DeGrip在复杂拆解任务中的表现。

Result: 实验结果显示，DeGrip能够灵活适应空间受限与构型多变的拆解任务，验证了其在老旧台式机拆解中的有效性。

Conclusion: DeGrip为实际废旧产品拆解场景提供了硬件层面的创新解决方案，拓展了智能拆解机器人在实际工业环境中的应用前景。

Abstract: Intelligent robotic disassembly of end-of-life (EOL) products has been a
long-standing challenge in robotics. While machine learning techniques have
shown promise, the lack of specialized hardware limits their application in
real-world scenarios. We introduce DeGrip, a customized gripper designed for
the disassembly of EOL computer desktops. DeGrip provides three degrees of
freedom (DOF), enabling arbitrary configurations within the disassembly
environment when mounted on a robotic manipulator. It employs a cable-driven
transmission mechanism that reduces its overall size and enables operation in
confined spaces. The wrist is designed to decouple the actuation of wrist and
jaw joints. We also developed an EOL desktop disassembly environment in Isaac
Sim to evaluate the effectiveness of DeGrip. The tasks were designed to
demonstrate its ability to operate in confined spaces and disassemble
components in arbitrary configurations. The evaluation results confirm the
capability of DeGrip for EOL desktop disassembly.

</details>


### [287] [Cosmos-Surg-dVRK: World Foundation Model-based Automated Online Evaluation of Surgical Robot Policy Learning](https://arxiv.org/abs/2510.16240)
*Lukas Zbinden,Nigel Nelson,Juo-Tung Chen,Xinhao Chen,Ji Woong,Kim,Mahdi Azizian,Axel Krieger,Sean Huver*

Main category: cs.RO

TL;DR: 该论文提出并评估了一种新型手术机器人仿真评测平台Cosmos-Surg-dVRK，实现了对手术策略的全自动在线评估，对于推广真实手术机器人的自动化评测和训练具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 当前手术策略的评估主要在真实机器人平台（如dVRK）上进行，但成本高、耗时长、执行难以复现且有较大变异性，严重制约了自动手术策略的发展与快速迭代。因此，迫切需要一种高保真、低成本并可快速复现的自动化评测手段。

Method: 作者基于Cosmos世界基础模型（WFM），专门针对手术场景进行了微调，提出Cosmos-Surg-dVRK平台，并结合训练的V-JEPA 2视频分类器，实现了手术策略的全自动仿真评测流程。该平台在模拟软组织变形等复杂现实手术任务时表现出高保真度。研究通过两个手术数据集分别在台面缝合垫任务和离体猪胆囊切除任务上，对平台进行评测。

Result: 在台面缝合垫任务中，Cosmos-Surg-dVRK仿真评测与真实dVRK平台的实验结果高度相关，视频分类器评价结果与人工标注的一致性较高。在离体猪胆囊切除实验中，平台仿真评测结果同样显示出良好与真实实验的对齐，具有较好泛化能力。

Conclusion: Cosmos-Surg-dVRK平台为复杂手术机器人的策略评测和对比提供了高效、可扩展的仿真基础，将有助于推动世界基础模型（WFM）在医学机器人领域的应用，为后续更复杂的自动化手术策略和评测研究奠定基础。

Abstract: The rise of surgical robots and vision-language-action models has accelerated
the development of autonomous surgical policies and efficient assessment
strategies. However, evaluating these policies directly on physical robotic
platforms such as the da Vinci Research Kit (dVRK) remains hindered by high
costs, time demands, reproducibility challenges, and variability in execution.
World foundation models (WFM) for physical AI offer a transformative approach
to simulate complex real-world surgical tasks, such as soft tissue deformation,
with high fidelity. This work introduces Cosmos-Surg-dVRK, a surgical finetune
of the Cosmos WFM, which, together with a trained video classifier, enables
fully automated online evaluation and benchmarking of surgical policies. We
evaluate Cosmos-Surg-dVRK using two distinct surgical datasets. On tabletop
suture pad tasks, the automated pipeline achieves strong correlation between
online rollouts in Cosmos-Surg-dVRK and policy outcomes on the real dVRK Si
platform, as well as good agreement between human labelers and the V-JEPA
2-derived video classifier. Additionally, preliminary experiments with ex-vivo
porcine cholecystectomy tasks in Cosmos-Surg-dVRK demonstrate promising
alignment with real-world evaluations, highlighting the platform's potential
for more complex surgical procedures.

</details>


### [288] [NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly?](https://arxiv.org/abs/2510.16263)
*Jierui Peng,Yanyan Zhang,Yicheng Duan,Tuo Liang,Vipin Chaudhary,Yu Yin*

Main category: cs.RO

TL;DR: NEBULA提出了一种统一的视觉-语言-动作（VLA）智能体评测生态体系，通过更细致的能力测试和压力测试，弥补了当前评估手段粗糙、数据分散等问题。


<details>
  <summary>Details</summary>
Motivation: 现有对VLA智能体的评估主要依赖于整体任务成功与否，难以精确诊断智能体的具体技能，且研究中的数据分散导致可复现性和通用模型开发受阻。

Method: 提出NEBULA，一个针对单臂操作的统一生态系统，包括标准化API和大规模聚合数据集。其核心是创新性的双轴评测协议：细粒度能力测试评估智能体具体技能，系统的压力测试衡量其在现实扰动下的鲁棒性，并支持跨数据集训练及公平对比。

Result: 采用NEBULA评测时，发现当前领先的VLA智能体在空间推理和动态适应等关键能力上表现不佳，这些短板此前常被传统整体度量掩盖。NEBULA能区分智能体‘能做什么’以及‘何时能可靠完成’。

Conclusion: NEBULA为VLA智能体提供了实用、可复现和全面的评测基础，有助于发展更健壮、通用的具身智能体。

Abstract: The evaluation of Vision-Language-Action (VLA) agents is hindered by the
coarse, end-task success metric that fails to provide precise skill diagnosis
or measure robustness to real-world perturbations. This challenge is
exacerbated by a fragmented data landscape that impedes reproducible research
and the development of generalist models. To address these limitations, we
introduce \textbf{NEBULA}, a unified ecosystem for single-arm manipulation that
enables diagnostic and reproducible evaluation. NEBULA features a novel
dual-axis evaluation protocol that combines fine-grained \textit{capability
tests} for precise skill diagnosis with systematic \textit{stress tests} that
measure robustness. A standardized API and a large-scale, aggregated dataset
are provided to reduce fragmentation and support cross-dataset training and
fair comparison. Using NEBULA, we demonstrate that top-performing VLAs struggle
with key capabilities such as spatial reasoning and dynamic adaptation, which
are consistently obscured by conventional end-task success metrics. By
measuring both what an agent can do and when it does so reliably, NEBULA
provides a practical foundation for robust, general-purpose embodied agents.

</details>


### [289] [Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification](https://arxiv.org/abs/2510.16281)
*Yilin Wu,Anqi Li,Tucker Hermans,Fabio Ramos,Andrea Bajcsy,Claudia P'erez-D'Arpino*

Main category: cs.RO

TL;DR: 本文提出了一种改进行为规划与执行一致性的机器人视觉语言行为模型（VLA）的推理-行动对齐方法，有效提升模型在分布外（OOD）场景下的稳健性和新行为组合能力，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型即使生成了正确的文本计划，其具体执行的低层动作也常常偏离计划意图，特别是在分布外场景中。这种分离现象限制了VLA模型的实际应用潜力，需要新方法提升推理与动作的一致性。

Method: 提出一种无训练、运行时的策略引导方法。每当VLA模型生成中间文本计划时，该框架会：1）采样多个候选动作序列，2）通过仿真预测各自结果，3）用预训练视觉-语言模型（VLM）评估哪个候选动作结果最贴合原文本计划，并仅执行最符合的动作序列。

Result: 该方法在无需再次训练的前提下，将VLA固有的动作多样性从误差来源转化为优势。实验在扩展标注了推理信息的LIBERO-100数据集等环境下，对比以往方法在行为组合任务的分布外表现，性能提升高达15%。

Conclusion: 通过结合仿真和VLM对齐，强化推理-行动一致性，大幅提升VLA模型在复杂环境下的鲁棒性和泛化能力，支持新任务的高效复用和组合，免去了高成本的再训练。

Abstract: Reasoning Vision Language Action (VLA) models improve robotic
instruction-following by generating step-by-step textual plans before low-level
actions, an approach inspired by Chain-of-Thought (CoT) reasoning in language
models. Yet even with a correct textual plan, the generated actions can still
miss the intended outcomes in the plan, especially in out-of-distribution (OOD)
scenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness,
and introduce a training-free, runtime policy steering method for
reasoning-action alignment. Given a reasoning VLA's intermediate textual plan,
our framework samples multiple candidate action sequences from the same model,
predicts their outcomes via simulation, and uses a pre-trained Vision-Language
Model (VLM) to select the sequence whose outcome best aligns with the VLA's own
textual plan. Only executing action sequences that align with the textual
reasoning turns our base VLA's natural action diversity from a source of error
into a strength, boosting robustness to semantic and visual OOD perturbations
and enabling novel behavior composition without costly re-training. We also
contribute a reasoning-annotated extension of LIBERO-100, environment
variations tailored for OOD evaluation, and demonstrate up to 15% performance
gain over prior work on behavior composition tasks and scales with compute and
data diversity. Project Website at:
https://yilin-wu98.github.io/steering-reasoning-vla/

</details>


### [290] [SPOT: Sensing-augmented Trajectory Planning via Obstacle Threat Modeling](https://arxiv.org/abs/2510.16308)
*Chi Zhang,Xian Huang,Wei Dong*

Main category: cs.RO

TL;DR: 本论文提出了一种针对单深度相机UAV的全新规划框架SPOT，实现了实时、基于观测的动态障碍物规避，大幅提升了障碍物可见性和导航安全性。


<details>
  <summary>Details</summary>
Motivation: 现有无人机仅配备单目深度相机时，视野有限且存在盲区，动态障碍物规避效果不佳。虽然有方法尝试用主动视觉策略扩展感知范围，但通常将运动规划与感知分离，导致响应滞后且效果有限。该研究为了解决这一问题，将感知目标和运动规划紧密结合，提高动态障碍物检测和反应能力。

Method: 作者提出SPOT框架，将障碍物的存在概率通过高斯过程进行建模，建立统一的概率障碍物地图。结合轨迹与空间不确定性，推理出时变的观测紧急性地图，并将其集成到实时可微的运动规划目标中，从而快速计算出兼顾感知的路径。

Result: 在仿真与真实动态复杂环境实验中，SPOT能比现有方法提前2.8秒检测到潜在动态障碍物，提升动态障碍物可见性超过500%，并成功实现复杂环境安全导航。

Conclusion: SPOT方法显著提升了UAV在受限视野下对动态障碍物的响应速度和安全性，验证了将感知与运动规划融合对提升系统整体性能的重要作用。

Abstract: UAVs equipped with a single depth camera encounter significant challenges in
dynamic obstacle avoidance due to limited field of view and inevitable blind
spots. While active vision strategies that steer onboard cameras have been
proposed to expand sensing coverage, most existing methods separate motion
planning from sensing considerations, resulting in less effective and delayed
obstacle response. To address this limitation, we introduce SPOT
(Sensing-augmented Planning via Obstacle Threat modeling), a unified planning
framework for observation-aware trajectory planning that explicitly
incorporates sensing objectives into motion optimization. At the core of our
method is a Gaussian Process-based obstacle belief map, which establishes a
unified probabilistic representation of both recognized (previously observed)
and potential obstacles. This belief is further processed through a
collision-aware inference mechanism that transforms spatial uncertainty and
trajectory proximity into a time-varying observation urgency map. By
integrating urgency values within the current field of view, we define
differentiable objectives that enable real-time, observation-aware trajectory
planning with computation times under 10 ms. Simulation and real-world
experiments in dynamic, cluttered, and occluded environments show that our
method detects potential dynamic obstacles 2.8 seconds earlier than baseline
approaches, increasing dynamic obstacle visibility by over 500\%, and enabling
safe navigation through cluttered, occluded environments.

</details>


### [291] [Manual2Skill++: Connector-Aware General Robotic Assembly from Instruction Manuals via Vision-Language Models](https://arxiv.org/abs/2510.16344)
*Chenrui Tie,Shengxiang Sun,Yudi Lin,Yanbo Wang,Zhongrui Li,Zhouhan Zhong,Jinxuan Zhu,Yiman Pang,Haonan Chen,Junting Chen,Ruihai Wu,Lin Shao*

Main category: cs.RO

TL;DR: 本文提出了一种以连接件为核心的自动装配任务理解与执行方法，通过视觉-语言模型从人工说明书中提取结构化连接信息，将装配任务编码为层次化图结构，并在多领域任务中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统机器人装配关注零件顺序和姿态，而忽视了连接件的关键作用，导致最终装配成功率受限。为提高装配可靠性，有必要将连接件设计为装配任务的核心部分。

Method: 作者提出了Manual2Skill++框架，利用视觉-语言大模型自动解析装配说明书中的符号图与注释，提取详细的连接件类型、规格、数量及位置信息。将整体装配任务编码为层次图，节点为部件或子组件，边明确表示组件间连接关系。并构建多类型大规模数据集进行实验。

Result: 收集并标注了20余种装配任务数据，包含丰富连接类型。在涉及家具、玩具和制造组件的四个复杂装配仿真任务中，验证了信息提取和从理解到执行全流程的有效性与通用性。

Conclusion: 将连接件作为装配表示的基本元素，并利用视觉-语言模型自动提取连接关系，可以提升机器人装配任务的理解能力和最终执行成功率，具有广泛的现实应用前景。

Abstract: Assembly hinges on reliably forming connections between parts; yet most
robotic approaches plan assembly sequences and part poses while treating
connectors as an afterthought. Connections represent the critical "last mile"
of assembly execution, while task planning may sequence operations and motion
plan may position parts, the precise establishment of physical connections
ultimately determines assembly success or failure. In this paper, we consider
connections as first-class primitives in assembly representation, including
connector types, specifications, quantities, and placement locations. Drawing
inspiration from how humans learn assembly tasks through step-by-step
instruction manuals, we present Manual2Skill++, a vision-language framework
that automatically extracts structured connection information from assembly
manuals. We encode assembly tasks as hierarchical graphs where nodes represent
parts and sub-assemblies, and edges explicitly model connection relationships
between components. A large-scale vision-language model parses symbolic
diagrams and annotations in manuals to instantiate these graphs, leveraging the
rich connection knowledge embedded in human-designed instructions. We curate a
dataset containing over 20 assembly tasks with diverse connector types to
validate our representation extraction approach, and evaluate the complete task
understanding-to-execution pipeline across four complex assembly scenarios in
simulation, spanning furniture, toys, and manufacturing components with
real-world correspondence.

</details>


### [292] [Learning to Optimize Edge Robotics: A Fast Integrated Perception-Motion-Communication Approach](https://arxiv.org/abs/2510.16424)
*Dan Guo,Xibin Jin,Shuai Wang,Zhigang Wen,Miaowen Wen,Chengzhong Xu*

Main category: cs.RO

TL;DR: 本文提出了一种融合感知、运动和通信的集成式边缘机器人系统（IPMC），通过动态调整通信策略，实现通信效率和资源利用率的提升。


<details>
  <summary>Details</summary>
Motivation: 当前边缘机器人系统频繁交换大体量多模态数据，但现有方法忽略了机器人功能与通信条件的相互依赖，导致通信开销过大。

Method: 提出IPMC架构，使机器人能够基于感知和运动信息动态调整压缩率、传输频率和发射功率等通信策略。同时，基于学习优化（LTO）范式，设计并实现了一种模仿学习神经网络，用于在线高效优化决策，极大降低计算复杂度。

Result: 提出的模仿学习网络使优化求解计算复杂度降低了10倍以上。实验验证了IPMC方案以及LTO范式在实时执行和性能方面的优越性。

Conclusion: 通过IPMC和LTO方法，显著提升了边缘机器人系统的通信效率和实时优化能力，为相关系统的设计和实际部署提供了新的思路。

Abstract: Edge robotics involves frequent exchanges of large-volume multi-modal data.
Existing methods ignore the interdependency between robotic functionalities and
communication conditions, leading to excessive communication overhead. This
paper revolutionizes edge robotics systems through integrated perception,
motion, and communication (IPMC). As such, robots can dynamically adapt their
communication strategies (i.e., compression ratio, transmission frequency,
transmit power) by leveraging the knowledge of robotic perception and motion
dynamics, thus reducing the need for excessive sensor data uploads.
Furthermore, by leveraging the learning to optimize (LTO) paradigm, an
imitation learning neural network is designed and implemented, which reduces
the computational complexity by over 10x compared to state-of-the art
optimization solvers. Experiments demonstrate the superiority of the proposed
IPMC and the real-time execution capability of LTO.

</details>


### [293] [What Questions Should Robots Be Able to Answer? A Dataset of User Questions for Explainable Robotics](https://arxiv.org/abs/2510.16435)
*Lennart Wachowiak,Andrew Coles,Gerard Canal,Oya Celiktutan*

Main category: cs.RO

TL;DR: 本文介绍了一个包含1893个家庭机器人用户提问的数据集，来源于基于视频和文字刺激的用户调查，归类详细，为理解用户需求及提升机器人问答能力提供基础。


<details>
  <summary>Details</summary>
Motivation: 越来越多家庭环境中机器人需要通过对话和大模型理解用户提问，因此需要了解用户真实会问什么问题，以指导机器人能力建设。

Method: 研究者设计了15段视频和7个文本关于家务机器行为，由100名参与者基于这些情境生成他们想问机器人的问题。问题经过整理，按12大类、70子类别归档，并对问题类型与用户背景进行了分析。

Result: 用户最常问的是任务执行细节（22.5%），其次是机器人能力（12.7%）和性能评估（11.3%）；虽然关于高难情景和正确性保障的提问较少，但用户认为这类问题最重要。新手用户关注简单事实，经验用户关心更复杂情境。

Conclusion: 该数据集可用于指导机器人日志记录和交互接口设计、问答模块评估和面向用户期望的解释策略制定，对提升人机对话和机器人自主性有重要作用。

Abstract: With the growing use of large language models and conversational interfaces
in human-robot interaction, robots' ability to answer user questions is more
important than ever. We therefore introduce a dataset of 1,893 user questions
for household robots, collected from 100 participants and organized into 12
categories and 70 subcategories. Most work in explainable robotics focuses on
why-questions. In contrast, our dataset provides a wide variety of questions,
from questions about simple execution details to questions about how the robot
would act in hypothetical scenarios -- thus giving roboticists valuable
insights into what questions their robot needs to be able to answer. To collect
the dataset, we created 15 video stimuli and 7 text stimuli, depicting robots
performing varied household tasks. We then asked participants on Prolific what
questions they would want to ask the robot in each portrayed situation. In the
final dataset, the most frequent categories are questions about task execution
details (22.5%), the robot's capabilities (12.7%), and performance assessments
(11.3%). Although questions about how robots would handle potentially difficult
scenarios and ensure correct behavior are less frequent, users rank them as the
most important for robots to be able to answer. Moreover, we find that users
who identify as novices in robotics ask different questions than more
experienced users. Novices are more likely to inquire about simple facts, such
as what the robot did or the current state of the environment. As robots enter
environments shared with humans and language becomes central to giving
instructions and interaction, this dataset provides a valuable foundation for
(i) identifying the information robots need to log and expose to conversational
interfaces, (ii) benchmarking question-answering modules, and (iii) designing
explanation strategies that align with user expectations.

</details>


### [294] [Advancing Off-Road Autonomous Driving: The Large-Scale ORAD-3D Dataset and Comprehensive Benchmarks](https://arxiv.org/abs/2510.16500)
*Chen Min,Jilin Mei,Heng Zhai,Shuai Wang,Tong Sun,Fanjie Kong,Haoyang Li,Fangyuan Mao,Fuyang Liu,Shuo Wang,Yiming Nie,Qi Zhu,Liang Xiao,Dawei Zhao,Yu Hu*

Main category: cs.RO

TL;DR: 本文提出并发布了目前最大的专为越野自动驾驶设计的数据集ORAD-3D，涵盖多种地貌和极端环境，并提供多个基准任务，推动该领域发展。


<details>
  <summary>Details</summary>
Motivation: 越野自动驾驶领域缺乏大规模、高质量的数据集和统一的基准，严重制约了感知和规划技术的研究。

Method: 作者收集并整理了包括林地、农田、草地、河边、碎石路、水泥路、农村等多样地形及多种天气、光照条件下的数据，构建了ORAD-3D数据集。同时，建立了涵盖2D自由空间检测、3D占用预测、粗GPS路径规划、视觉-语言模型自动驾驶、越野世界建模的五项基准任务。

Result: 获得了覆盖面广、数据量大、环境丰富的专属越野自动驾驶数据集，并设定了系统性基准任务。

Conclusion: ORAD-3D数据集和基准为越野自动驾驶的感知和规划研究提供了统一、强有力的资源，将有效推动相关技术进步。

Abstract: A major bottleneck in off-road autonomous driving research lies in the
scarcity of large-scale, high-quality datasets and benchmarks. To bridge this
gap, we present ORAD-3D, which, to the best of our knowledge, is the largest
dataset specifically curated for off-road autonomous driving. ORAD-3D covers a
wide spectrum of terrains, including woodlands, farmlands, grasslands,
riversides, gravel roads, cement roads, and rural areas, while capturing
diverse environmental variations across weather conditions (sunny, rainy,
foggy, and snowy) and illumination levels (bright daylight, daytime, twilight,
and nighttime). Building upon this dataset, we establish a comprehensive suite
of benchmark evaluations spanning five fundamental tasks: 2D free-space
detection, 3D occupancy prediction, rough GPS-guided path planning,
vision-language model-driven autonomous driving, and world model for off-road
environments. Together, the dataset and benchmarks provide a unified and robust
resource for advancing perception and planning in challenging off-road
scenarios. The dataset and code will be made publicly available at
https://github.com/chaytonmin/ORAD-3D.

</details>


### [295] [A Novel Gripper with Semi-Peaucellier Linkage and Idle-Stroke Mechanism for Linear Pinching and Self-Adaptive Grasping](https://arxiv.org/abs/2510.16517)
*Haokai Ding,Wenzeng Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种新型SPD机械手，能通过线性运动轨迹高效抓取不同大小和形状的物体，优化了传统机械手的不足。


<details>
  <summary>Details</summary>
Motivation: 现有工业并联夹爪的指尖多为弧形运动，需调整夹爪整体高度以避免与台面碰撞，操作复杂且不灵活。需要一种结构简单、运动轨迹优越的夹爪来提升抓取效率和适应性。

Method: 设计了一种拥有对称双指并可独立或单一驱动的SPD机械手，该机械手指尖运动轨迹为线性，避免了传统并联夹爪的弧线问题。详细给出了其设计理念、组成原理及优化理论，并开发了原型进行实验验证。

Result: 实验表明，原型SPD机械手实现了线性并联抓持功能，对不同形状与尺寸的物体具备良好适应性。

Conclusion: SPD机械手有效解决了夹持过程中的高度调整问题，提升了系统抓取性能，适用于多类型机器人协作抓取，并为深度学习数据采集提供有力工具。

Abstract: This paper introduces a novel robotic gripper, named as the SPD gripper. It
features a palm and two mechanically identical and symmetrically arranged
fingers, which can be driven independently or by a single motor. The fingertips
of the fingers follow a linear motion trajectory, facilitating the grasping of
objects of various sizes on a tabletop without the need to adjust the overall
height of the gripper. Traditional industrial grippers with parallel gripping
capabilities often exhibit an arcuate motion at the fingertips, requiring the
entire robotic arm to adjust its height to avoid collisions with the tabletop.
The SPD gripper, with its linear parallel gripping mechanism, effectively
addresses this issue. Furthermore, the SPD gripper possesses adaptive
capabilities, accommodating objects of different shapes and sizes. This paper
presents the design philosophy, fundamental composition principles, and
optimization analysis theory of the SPD gripper. Based on the design theory, a
robotic gripper prototype was developed and tested. The experimental results
demonstrate that the robotic gripper successfully achieves linear parallel
gripping functionality and exhibits good adaptability. In the context of the
ongoing development of embodied intelligence technologies, this robotic gripper
can assist various robots in achieving effective grasping, laying a solid
foundation for collecting data to enhance deep learning training.

</details>


### [296] [DIV-Nav: Open-Vocabulary Spatial Relationships for Multi-Object Navigation](https://arxiv.org/abs/2510.16518)
*Jesús Ortega-Peimbert,Finn Lukas Busch,Timon Homberger,Quantao Yang,Olov Andersson*

Main category: cs.RO

TL;DR: 该论文提出了DIV-Nav系统，实现机器人对含有复杂空间关系的自由文本查询的高效导航和对象搜索。系统通过将复杂指令分解、语义地图求交与空间约束验证三个步骤提升了搜索能力，并在模拟和真实环境下验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本对象导航通常仅处理单一对象词汇，难以应对带有复杂空间关系的自然语言查询，而实际场景中人类指令常包含此类信息，因此有必要探索机器人如何处理此类复杂查询。

Method: DIV-Nav首先将含有复杂空间关系的自然语言指令分解为针对语义地图的简单对象查询，然后对各个对象的语义置信图进行交集操作，定位可能满足所有条件的区域，最后通过大型视觉语言模型（LVLM）对候选区域进行空间关系验证。此外，系统适应性地调整前沿探索目标，更高效地完成带空间约束的搜索。

Result: 在MultiON基准测试和真实世界的Boston Dynamics Spot机器人（搭载Jetson Orin AGX）平台上进行了大量实验，验证了该方法在复杂指令导航任务上的有效性和实时性能。

Conclusion: DIV-Nav能够有效地让机器人响应带复杂空间关系的自由文本对象请求，系统结构在模拟和真实环境下均表现出良好效果，为智能体复杂环境理解和导航能力的提升带来新方向。

Abstract: Advances in open-vocabulary semantic mapping and object navigation have
enabled robots to perform an informed search of their environment for an
arbitrary object. However, such zero-shot object navigation is typically
designed for simple queries with an object name like "television" or "blue
rug". Here, we consider more complex free-text queries with spatial
relationships, such as "find the remote on the table" while still leveraging
robustness of a semantic map. We present DIV-Nav, a real-time navigation system
that efficiently addresses this problem through a series of relaxations: i)
Decomposing natural language instructions with complex spatial constraints into
simpler object-level queries on a semantic map, ii) computing the Intersection
of individual semantic belief maps to identify regions where all objects
co-exist, and iii) Validating the discovered objects against the original,
complex spatial constrains via a LVLM. We further investigate how to adapt the
frontier exploration objectives of online semantic mapping to such spatial
search queries to more effectively guide the search process. We validate our
system through extensive experiments on the MultiON benchmark and real-world
deployment on a Boston Dynamics Spot robot using a Jetson Orin AGX. More
details and videos are available at https://anonsub42.github.io/reponame/

</details>


### [297] [Semi-Peaucellier Linkage and Differential Mechanism for Linear Pinching and Self-Adaptive Grasping](https://arxiv.org/abs/2510.16524)
*Haokai Ding,Zhaohan Chen,Tao Yang,Wenzeng Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种新型SP-Diff并联夹爪系统，通过采用创新的差动连杆机构和模块化对称双指结构，实现了更高的自适应抓取能力，同时具备结构刚性和精确线性运动。系统结构紧凑，支持未来结合多模态传感（如力/视觉传感），适用于智能制造、协作机器人等多场景。


<details>
  <summary>Details</summary>
Motivation: 传统末端执行器在智能工业自动化中的适应性有限，难以有效应对多样且柔性的工业工件需求，因此需要开发更加智能、灵活且适应性强的夹爪系统。

Method: 设计采用差动连杆和行星齿轮传动机制，实现双指并联线性同步运动及独立指位调节；结构中优化了平行四边形连杆和紧凑的掌部布局，可减轻Z轴重复校准负担，并具备未来集成多模态传感器的接口。

Result: 与传统弧轨迹夹爪相比，SP-Diff系统Z轴校准需求减少30%；夹爪能够适应多样工业工件及变形物体（如柑橘等），展现出良好的自适应抓取能力。

Conclusion: SP-Diff夹爪系统显著提升了机器人末端执行器的智能化与自适应性，为柔性制造、协作机器人和物流自动化等领域提供了有前景的解决方案。

Abstract: This paper presents the SP-Diff parallel gripper system, addressing the
limited adaptability of conventional end-effectors in intelligent industrial
automation. The proposed design employs an innovative differential linkage
mechanism with a modular symmetric dual-finger configuration to achieve
linear-parallel grasping. By integrating a planetary gear transmission, the
system enables synchronized linear motion and independent finger pose
adjustment while maintaining structural rigidity, reducing Z-axis recalibration
requirements by 30% compared to arc-trajectory grippers. The compact palm
architecture incorporates a kinematically optimized parallelogram linkage and
Differential mechanism, demonstrating adaptive grasping capabilities for
diverse industrial workpieces and deformable objects such as citrus fruits.
Future-ready interfaces are embedded for potential force/vision sensor
integration to facilitate multimodal data acquisition (e.g., trajectory
planning and object deformation) in digital twin frameworks. Designed as a
flexible manufacturing solution, SP-Diff advances robotic end-effector
intelligence through its adaptive architecture, showing promising applications
in collaborative robotics, logistics automation, and specialized operational
scenarios.

</details>


### [298] [MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation](https://arxiv.org/abs/2510.16617)
*Ruihan Zhao,Tyler Ingebrand,Sandeep Chinchali,Ufuk Topcu*

Main category: cs.RO

TL;DR: 提出了一种新模型MoS-VLA，通过组合基础技能并用凸优化进行单演示任务迁移，实现跨域泛化，且优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-行动（VLA）模型在遇到新环境、新形态或新任务时表现不佳，缺乏泛化能力。作者希望提升模型适应新情景的能力，降低部署门槛。

Method: MoS-VLA将机器人操作策略表示为有限个学习到的基础函数的线性组合。在预训练阶段，通过Open X-Embodiment多数据集联合学习基础技能空间。测试时，仅需单专家演示，通过L1范数凸优化（无需梯度更新）快速推断新的技能组合。

Result: MoS-VLA在五个未见过的数据集上都有更低的动作预测误差，并在仿真和真实机器人任务中实现了成功，而预训练的VLA模型直接失败。

Conclusion: MoS-VLA显著提升了VLA模型在新任务上的适应能力，只需少量演示与极低计算量即可迁移，具有良好的泛化性和实用价值。

Abstract: Vision-Language-Action (VLA) models trained on large robot datasets promise
general-purpose, robust control across diverse domains and embodiments.
However, existing approaches often fail out-of-the-box when deployed in novel
environments, embodiments, or tasks. We introduce Mixture of Skills VLA
(MoS-VLA), a framework that represents robot manipulation policies as linear
combinations of a finite set of learned basis functions. During pretraining,
MoS-VLA jointly learns these basis functions across datasets from the Open
X-Embodiment project, producing a structured skill space. At test time,
adapting to a new task requires only a single expert demonstration. The
corresponding skill representation is then inferred via a lightweight convex
optimization problem that minimizes the L1 action error, without requiring
gradient updates. This gradient-free adaptation incurs minimal overhead while
enabling rapid instantiation of new skills. Empirically, MoS-VLA achieves lower
action-prediction error on five out of five unseen datasets and succeeds in
both simulation and real-robot tasks where a pretrained VLA model fails
outright. Project page: mos-vla.github.io/

</details>


### [299] [First Responders' Perceptions of Semantic Information for Situational Awareness in Robot-Assisted Emergency Response](https://arxiv.org/abs/2510.16692)
*Tianshu Ruan,Zoe Betta,Georgios Tzoumas,Rustam Stolkin,Manolis Chiou*

Main category: cs.RO

TL;DR: 本研究调查了急救人员对在机器人系统中使用语义信息和态势感知的态度。通过跨国问卷发现，急救人员普遍认可语义信息提升应急机器人能力，认为它对预测突发事件和建立态势感知有价值。


<details>
  <summary>Details</summary>
Motivation: 目前针对急救人员（FRs）在应急行动中如何看待和接受基于语义信息的机器人系统的跨国数据较少，研究动机是弥补这一研究空白，为今后机器人系统设计提供依据。

Method: 研究通过结构化问卷，调查了来自八个国家的22名急救人员，收集其人口信息、对机器人的一般态度、以及有关语义增强态势感知的经验和需求。

Result: 大多数急救人员对机器人持正面态度，认为语义信息对建立态势感知有用（平均3.6分/5分），并能用于预测突发事件（平均3.9分/5分）。受访者认为只要语义输出准确率达到74.6%，就值得信任，67.8%即可用，显示他们愿意接受不完美但有用的AI工具。

Conclusion: 本研究首次跨国调查急救人员对基于语义的态势感知的态度，发现其最看重对象身份、空间关系、风险等信息，并指明了机器人实验室研发和实际应用之间的差距，强调加强一线人员与机器人研究者协作，为开发更贴合需求的应急机器人系统提供参考。

Abstract: This study investigates First Responders' (FRs) attitudes toward the use of
semantic information and Situational Awareness (SA) in robotic systems during
emergency operations. A structured questionnaire was administered to 22 FRs
across eight countries, capturing their demographic profiles, general attitudes
toward robots, and experiences with semantics-enhanced SA. Results show that
most FRs expressed positive attitudes toward robots, and rated the usefulness
of semantic information for building SA at an average of 3.6 out of 5. Semantic
information was also valued for its role in predicting unforeseen emergencies
(mean 3.9). Participants reported requiring an average of 74.6\% accuracy to
trust semantic outputs and 67.8\% for them to be considered useful, revealing a
willingness to use imperfect but informative AI support tools.
  To the best of our knowledge, this study offers novel insights by being one
of the first to directly survey FRs on semantic-based SA in a cross-national
context. It reveals the types of semantic information most valued in the field,
such as object identity, spatial relationships, and risk context-and connects
these preferences to the respondents' roles, experience, and education levels.
The findings also expose a critical gap between lab-based robotics capabilities
and the realities of field deployment, highlighting the need for more
meaningful collaboration between FRs and robotics researchers. These insights
contribute to the development of more user-aligned and situationally aware
robotic systems for emergency response.

</details>


### [300] [Towards Active Excitation-Based Dynamic Inertia Identification in Satellites](https://arxiv.org/abs/2510.16738)
*Matteo El-Hariry,Vittorio Franzese,Miguel Olivares-Mendez*

Main category: cs.RO

TL;DR: 本文分析了不同激励设计对纳米和微型卫星惯性参数辨识的影响，通过对八种不同频谱丰富度的激励下进行仿真，并对比了两种参数估计算法的性能，给出了不同条件下各自的最优适用范围，为实际轨道在轨辨识提供了实用建议。代码开源。


<details>
  <summary>Details</summary>
Motivation: 实际纳米、微型卫星在轨任务常因结构变化、燃料消耗等导致惯性参数变化，准确获取惯性特性是姿态控制系统稳定运行的基础，如何设计激励序列并选择合适辨识算法直接影响参数估计精度和系统鲁棒性，因此需要系统分析激励设计与辨识方法的协同作用。

Method: 首先建立包含反作用轮耦合、执行器约束及外部扰动的非线性姿态动力学模型，采用八种不同频谱特性的激励力矩进行系统激励。在三种卫星结构及时变惯量条件下，对比批量最小二乘法与扩展卡尔曼滤波器两种参数估计方法的表现，通过仿真定量评估各自精度和鲁棒性。

Result: 结果表明，激励的频谱丰富度与参数估计算法本身的假设会共同影响参数辨识的准确性和鲁棒性。批量最小二乘法在某些激励下精准度较高，而在惯性时变情形下扩展卡尔曼滤波器表现更优。实验总结了不同场景下两类算法各自的适用条件。

Conclusion: 针对纳米和微型卫星惯性识别，激励设计与估计算法选择需结合卫星物理特性和任务需求综合考量，得出实践中应优先选用的配置，可提升在轨自适应辨识精度，为后续系统设计与实际应用提供指导。

Abstract: This paper presents a comprehensive analysis of how excitation design
influences the identification of the inertia properties of rigid nano- and
micro-satellites. We simulate nonlinear attitude dynamics with reaction-wheel
coupling, actuator limits, and external disturbances, and excite the system
using eight torque profiles of varying spectral richness. Two estimators are
compared, a batch Least Squares method and an Extended Kalman Filter, across
three satellite configurations and time-varying inertia scenarios. Results show
that excitation frequency content and estimator assumptions jointly determine
estimation accuracy and robustness, offering practical guidance for in-orbit
adaptive inertia identification by outlining the conditions under which each
method performs best. The code is provided as open-source .

</details>


### [301] [Adaptive Invariant Extended Kalman Filter for Legged Robot State Estimation](https://arxiv.org/abs/2510.16755)
*Kyung-Hwan Kim,DongHyun Ahn,Dong-hyun Lee,JuYoung Yoon,Dong Jin Hyun*

Main category: cs.RO

TL;DR: 本文提出了一种自适应不变扩展卡尔曼滤波器（Adaptive Invariant Extended Kalman Filter），用于提升足式机器人本体状态的估计准确性。通过对接触足模型噪声进行自适应调整，改善了在多变接触条件下的状态估计，并在真实四足机器人上进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 足式机器人的状态估计对其控制性能和行走稳定性有决定性影响。传统方法对足部滑动处理不足，易导致滤波器发散或精度下降，因此亟需一种更鲁棒且自适应的状态估计算法。

Method: 该方法基于卡尔曼滤波原理，提出自适应调整接触足模型噪声大小，通过在线协方差估计实现噪声参数自适应。结合接触检测算法（无需额外传感器），有助于实时精确反馈地面接触状态。

Result: 所提出算法在真实四足机器人LeoQuad平台进行了实验证明，能够在动态运动场景下显著提升状态估计精度，尤其在细微滑移及复杂地面接触场景下表现优越。

Conclusion: 自适应不变扩展卡尔曼滤波器能有效增强足式机器人在多变接触和动态环境下的状态估计表现，无需依赖额外硬件，具备实际应用价值。

Abstract: State estimation is crucial for legged robots as it directly affects control
performance and locomotion stability. In this paper, we propose an Adaptive
Invariant Extended Kalman Filter to improve proprioceptive state estimation for
legged robots. The proposed method adaptively adjusts the noise level of the
contact foot model based on online covariance estimation, leading to improved
state estimation under varying contact conditions. It effectively handles small
slips that traditional slip rejection fails to address, as overly sensitive
slip rejection settings risk causing filter divergence. Our approach employs a
contact detection algorithm instead of contact sensors, reducing the reliance
on additional hardware. The proposed method is validated through real-world
experiments on the quadruped robot LeoQuad, demonstrating enhanced state
estimation performance in dynamic locomotion scenarios.

</details>


### [302] [T3 Planner: A Self-Correcting LLM Framework for Robotic Motion Planning with Temporal Logic](https://arxiv.org/abs/2510.16767)
*Jia Li,Guoxiang Zhao*

Main category: cs.RO

TL;DR: 提出了一种名为T3 Planner的新型机器人运动规划框架，结合大语言模型（LLM）和形式化方法自动修正输出，以更好地将自然语言指令转化为可执行的轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有从自然语言到运动规划的方法依赖领域知识定制，难以处理空间-时间耦合，经常造成不可行的动作规划，且单靠LLM容易产生虚构（hallucination）导致结果不可执行。

Method: T3 Planner分为三个串联模块，每个模块激发LLM生成轨迹候选，并通过信号时序逻辑（STL）验证器检查轨迹的可行性，直到满足复杂空间、时间和逻辑约束为止。最终，可将推理能力蒸馏至轻量级Qwen3-4B模型以便高效部署。

Result: 在多种不同实验场景下，T3 Planner显著优于其他基线方法，验证了其有效性和通用性。

Conclusion: T3 Planner能有效将自然语言任务转化为可执行、符合复杂约束的机器人运动规划，结合LLM和形式化验证机制，提升了可行性、通用性和部署效率。

Abstract: Translating natural language instructions into executable motion plans is a
fundamental challenge in robotics. Traditional approaches are typically
constrained by their reliance on domain-specific expertise to customize
planners, and often struggle with spatio-temporal couplings that usually lead
to infeasible motions or discrepancies between task planning and motion
execution. Despite the proficiency of Large Language Models (LLMs) in
high-level semantic reasoning, hallucination could result in infeasible motion
plans. In this paper, we introduce the T3 Planner, an LLM-enabled robotic
motion planning framework that self-corrects it output with formal methods. The
framework decomposes spatio-temporal task constraints via three cascaded
modules, each of which stimulates an LLM to generate candidate trajectory
sequences and examines their feasibility via a Signal Temporal Logic (STL)
verifier until one that satisfies complex spatial, temporal, and logical
constraints is found.Experiments across different scenarios show that T3
Planner significantly outperforms the baselines. The required reasoning can be
distilled into a lightweight Qwen3-4B model that enables efficient deployment.
All supplementary materials are accessible at
https://github.com/leeejia/T3_Planner.

</details>


### [303] [A Preliminary Exploration of the Differences and Conjunction of Traditional PNT and Brain-inspired PNT](https://arxiv.org/abs/2510.16771)
*Xu He,Xiaolin Meng,Wenxuan Yin,Youdong Zhang,Lingfei Mo,Xiangdong An,Fangwen Yu,Shuguo Pan,Yufeng Liu,Jingnan Liu,Yujia Zhang,Wang Gao*

Main category: cs.RO

TL;DR: 提出了一种将脑启发式空间认知与高精度机器PNT结合的新路线图，旨在提升定位、导航与授时系统的智能性、弹性与能效。


<details>
  <summary>Details</summary>
Motivation: 当前复杂环境对PNT系统提出了更高要求，需要更具弹性、节能和类脑认知能力的PNT系统。现有PNT以工具为导向，难以满足智能化和复杂场景下的需求。

Method: （1）多层次拆解了传统PNT、生物大脑PNT和脑启发PNT的异同；（2）提出了一个四层次（感知-能力-决策-硬件）融合框架，结合精确数值与脑启发智能；（3）对脑启发PNT未来发展给出前瞻建议。

Result: 明确展示三类PNT的差异，提出具体融合框架，理论上提升了PNT系统的智能性和韧性，指明了从工具导向到认知驱动的转变路径。

Conclusion: 将脑启发认知融入PNT系统是提升其智能化、弹性及能效的有效途径，为未来智能无人系统导航提供了理论基础和发展建议。

Abstract: Developing universal Positioning, Navigation, and Timing (PNT) is our
enduring goal. Today's complex environments demand PNT that is more resilient,
energy-efficient and cognitively capable. This paper asks how we can endow
unmanned systems with brain-inspired spatial cognition navigation while
exploiting the high precision of machine PNT to advance universal PNT. We
provide a new perspective and roadmap for shifting PNT from "tool-oriented" to
"cognition-driven". Contributions: (1) multi-level dissection of differences
among traditional PNT, biological brain PNT and brain-inspired PNT; (2) a
four-layer (observation-capability-decision-hardware) fusion framework that
unites numerical precision and brain-inspired intelligence; (3) forward-looking
recommendations for future development of brain-inspired PNT.

</details>


### [304] [C-Free-Uniform: A Map-Conditioned Trajectory Sampler for Model Predictive Path Integral Control](https://arxiv.org/abs/2510.16905)
*Yukang Cao,Rahul Moorthy,O. Goktug Poyrazoglu,Volkan Isler*

Main category: cs.RO

TL;DR: 本文提出了一种新的控制输入采样方法C-Free-Uniform(CFU)，该方法能够在自由空间内均匀地采样，并将其应用于新型MPPI控制器CFU-MPPI，大幅提升了在复杂环境下的导航成功率，同时降低了采样需求。


<details>
  <summary>Details</summary>
Motivation: 以往的轨迹采样方法无法根据环境特征进行调整，导致采样效率低，尤其是在复杂环境下。为了提高在障碍密集环境中的导航成功率及效率，需要开发能利用环境信息的采样机制。

Method: 提出C-Free-Uniform采样方法：根据当前状态和环境局部地图，自适应地生成能在自由空间内均匀分布的控制输入。将这种采样方法集成到改进的模型预测路径积分控制器（CFU-MPPI）中。

Result: 在多项挑战性导航实验中，CFU-MPPI导航成功率明显高于传统方法，且在实现更高性能的同时，所需采样数量更低。

Conclusion: CFU-MPPI利用环境自适应采样，大幅提升了控制效果和采样效率，为复杂环境中的机器人路径规划和控制提供了有效解决方案。

Abstract: Trajectory sampling is a key component of sampling-based control mechanisms.
Trajectory samplers rely on control input samplers, which generate control
inputs u from a distribution p(u | x) where x is the current state. We
introduce the notion of Free Configuration Space Uniformity (C-Free-Uniform for
short) which has two key features: (i) it generates a control input
distribution so as to uniformly sample the free configuration space, and (ii)
in contrast to previously introduced trajectory sampling mechanisms where the
distribution p(u | x) is independent of the environment, C-Free-Uniform is
explicitly conditioned on the current local map. Next, we integrate this
sampler into a new Model Predictive Path Integral (MPPI) Controller, CFU-MPPI.
Experiments show that CFU-MPPI outperforms existing methods in terms of success
rate in challenging navigation tasks in cluttered polygonal environments while
requiring a much smaller sampling budget.

</details>


### [305] [Design of an Affordable, Fully-Actuated Biomimetic Hand for Dexterous Teleoperation Systems](https://arxiv.org/abs/2510.16931)
*Zhaoliang Wan,Zida Zhou,Zetong Bi,Zehui Yang,Hao Ding,Hui Cheng*

Main category: cs.RO

TL;DR: 本文提出了一种名为RAPID Hand的五指全驱动仿人机械手，成本低、结构创新，适用于灵巧远程操作任务，在多个任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有五指灵巧手的高昂成本阻碍了大规模“从演示中学习”范式下的真实机器人数据收集，因此亟需一种经济实用的全驱动机械手解决此问题。

Method: 设计并实现了一种20自由度的RAPID Hand，结合创新的仿人驱动与传动方式、优化的电机布局及结构，并采用3D打印与定制齿轮以降低成本和便于维修。通过量化指标和三项远程操作任务（多指取物、舀勺操作、钢琴弹奏）对该手进行性能评估。

Result: RAPID Hand在各项测试任务中表现出较高的灵巧性和可控性。评测结果表明，该手能够胜任复杂的远程操作任务，兼具功能性与经济性。

Conclusion: RAPID Hand凭借其创新结构和低成本优势，为实现大规模灵巧手远程操作及真实数据采集提供了新工具，具有广阔的应用前景。

Abstract: This paper addresses the scarcity of affordable, fully-actuated five-fingered
hands for dexterous teleoperation, which is crucial for collecting large-scale
real-robot data within the "Learning from Demonstrations" paradigm. We
introduce the prototype version of the RAPID Hand, the first low-cost,
20-degree-of-actuation (DoA) dexterous hand that integrates a novel
anthropomorphic actuation and transmission scheme with an optimized motor
layout and structural design to enhance dexterity. Specifically, the RAPID Hand
features a universal phalangeal transmission scheme for the non-thumb fingers
and an omnidirectional thumb actuation mechanism. Prioritizing affordability,
the hand employs 3D-printed parts combined with custom gears for easier
replacement and repair. We assess the RAPID Hand's performance through
quantitative metrics and qualitative testing in a dexterous teleoperation
system, which is evaluated on three challenging tasks: multi-finger retrieval,
ladle handling, and human-like piano playing. The results indicate that the
RAPID Hand's fully actuated 20-DoF design holds significant promise for
dexterous teleoperation.

</details>


### [306] [DINO-CVA: A Multimodal Goal-Conditioned Vision-to-Action Model for Autonomous Catheter Navigation](https://arxiv.org/abs/2510.17038)
*Pedram Fekri,Majid Roshanfar,Samuel Barbeau,Seyedfarzad Famouri,Thomas Looi,Dale Podolsky,Mehrdad Zadeh,Javad Dargahi*

Main category: cs.RO

TL;DR: 本论文提出了一种多模态、目标条件的行为克隆框架DINO-CVA，实现了自主导管导航，实验显示其高效准确，推动了导管操作的智能化自主化发展。


<details>
  <summary>Details</summary>
Motivation: 现有心脏导管手术高度依赖人工操作，现有机器人系统需持续医生干预，导致医生疲劳、辐射暴露增加和手术效果波动，因此亟需提升智能自主能力，减轻操作依赖。

Method: 提出DINO-CVA框架，将视觉观测和操作杆运动学数据融合到联合嵌入空间，通过从专家演示中自回归学习预测动作，并采用目标条件引导导航。为此构建了含有合成血管模型的机器人实验平台，采集多模态数据并进行性能评测。

Result: 实验结果显示，DINO-CVA能高准确预测动作，不仅与仅用运动学基线模型表现相当，还能结合解剖环境信息做出决策。

Conclusion: 该研究证明了多模态、目标条件模型在导管导航中的可行性，减少了对操作员的依赖，有助于提高导管治疗的一致性与可靠性，是实现手术自主化的重要进展。

Abstract: Cardiac catheterization remains a cornerstone of minimally invasive
interventions, yet it continues to rely heavily on manual operation. Despite
advances in robotic platforms, existing systems are predominantly follow-leader
in nature, requiring continuous physician input and lacking intelligent
autonomy. This dependency contributes to operator fatigue, more radiation
exposure, and variability in procedural outcomes. This work moves towards
autonomous catheter navigation by introducing DINO-CVA, a multimodal
goal-conditioned behavior cloning framework. The proposed model fuses visual
observations and joystick kinematics into a joint embedding space, enabling
policies that are both vision-aware and kinematic-aware. Actions are predicted
autoregressively from expert demonstrations, with goal conditioning guiding
navigation toward specified destinations. A robotic experimental setup with a
synthetic vascular phantom was designed to collect multimodal datasets and
evaluate performance. Results show that DINO-CVA achieves high accuracy in
predicting actions, matching the performance of a kinematics-only baseline
while additionally grounding predictions in the anatomical environment. These
findings establish the feasibility of multimodal, goal-conditioned
architectures for catheter navigation, representing an important step toward
reducing operator dependency and improving the reliability of catheterbased
therapies.

</details>


### [307] [Learning to Design Soft Hands using Reward Models](https://arxiv.org/abs/2510.17086)
*Xueqian Bai,Nicklas Hansen,Adabhav Singh,Michael T. Tolley,Yan Duan,Pieter Abbeel,Xiaolong Wang,Sha Yi*

Main category: cs.RO

TL;DR: 本文提出了一种结合优化方法和奖励模型（CEM-RM）的软体机械手联合硬件与控制协同设计框架，显著提升了机械手的抓取性能，并有效降低了设计评估的计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有软体机械手在实现顺应性和广泛适用性之间设计存在难题，且协同设计导致高维搜索空间，评估成本高。本研究希望高效地实现软体手的优化设计。

Method: 提出基于Cross-Entropy Method与Reward Model（CEM-RM）的设计优化框架，利用预采集遥操作控制数据学习优化手型分布，通过仿真并行化训练，最后将最优设计3D打印并在真实环境下通过遥操作测试。

Result: 实验表明，所优化设计的机械手在仿真和真实世界测试中，抓取多样复杂物体的成功率显著超过基线设计，且设计评估次数大大减少。

Conclusion: CEM-RM方法能够在显著降低设计评估代价的同时，高效获得性能更优的软体机械手，体现了软硬件协同优化的有效性。

Abstract: Soft robotic hands promise to provide compliant and safe interaction with
objects and environments. However, designing soft hands to be both compliant
and functional across diverse use cases remains challenging. Although co-design
of hardware and control better couples morphology to behavior, the resulting
search space is high-dimensional, and even simulation-based evaluation is
computationally expensive. In this paper, we propose a Cross-Entropy Method
with Reward Model (CEM-RM) framework that efficiently optimizes tendon-driven
soft robotic hands based on teleoperation control policy, reducing design
evaluations by more than half compared to pure optimization while learning a
distribution of optimized hand designs from pre-collected teleoperation data.
We derive a design space for a soft robotic hand composed of flexural soft
fingers and implement parallelized training in simulation. The optimized hands
are then 3D-printed and deployed in the real world using both teleoperation
data and real-time teleoperation. Experiments in both simulation and hardware
demonstrate that our optimized design significantly outperforms baseline hands
in grasping success rates across a diverse set of challenging objects.

</details>


### [308] [Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey](https://arxiv.org/abs/2510.17111)
*Weifan Guan,Qinghao Hu,Aosheng Li,Jian Cheng*

Main category: cs.RO

TL;DR: 本文综述了旨在提升视觉-语言-动作（VLA）模型效率的方法，尤其是在内存、延迟和成本等方面，系统地总结了现有技术并探讨未来趋势。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型能将自然语言指令和视觉输入映射为机器人动作，但由于计算和内存消耗巨大，难以应用于对实时性要求高、资源受限的边缘设备（如移动机器人）中。因此，需要探索高效、可扩展的解决方案。

Method: 本综述将提升VLA系统效率的研究分为模型架构、感知特征、动作生成、训练与推理策略四个维度，对每类主流方法进行了梳理和归纳，并对代表性技术进行了总结。

Result: 论文系统归纳了降低VLA模型延迟、内存消耗及训练推理开销的最新实践，为研究者提供了不同技术路线的对比和参考。

Conclusion: 作者认为，高效VLA系统是实现智能体实际落地的关键，未来应继续关注模型轻量化、特征优化、动作决策及高效学习等方向，同时面对新的挑战和开放性问题。

Abstract: Vision-Language-Action (VLA) models extend vision-language models to embodied
control by mapping natural-language instructions and visual observations to
robot actions. Despite their capabilities, VLA systems face significant
challenges due to their massive computational and memory demands, which
conflict with the constraints of edge platforms such as on-board mobile
manipulators that require real-time performance. Addressing this tension has
become a central focus of recent research. In light of the growing efforts
toward more efficient and scalable VLA systems, this survey provides a
systematic review of approaches for improving VLA efficiency, with an emphasis
on reducing latency, memory footprint, and training and inference costs. We
categorize existing solutions into four dimensions: model architecture,
perception feature, action generation, and training/inference strategies,
summarizing representative techniques within each category. Finally, we discuss
future trends and open challenges, highlighting directions for advancing
efficient embodied intelligence.

</details>


### [309] [Decentralized Real-Time Planning for Multi-UAV Cooperative Manipulation via Imitation Learning](https://arxiv.org/abs/2510.17143)
*Shantnav Agarwal,Javier Alonso-Mora,Sihao Sun*

Main category: cs.RO

TL;DR: 本文提出一种新颖的基于机器学习的分布式动力学规划方法，实现多架无人机在部分可观测和无通信条件下高效协同搬运悬挂载荷。结果显示与集中式方法性能相当。


<details>
  <summary>Details</summary>
Motivation: 当前多无人机悬挂载荷运输与控制大多依赖集中式架构或可靠通信，在部分可观测、无通信条件下难以应用。作者希望打破此局限，提升系统的分布式自主性和实用性。

Method: 采用模仿学习框架，每架无人机学习并模仿有全局信息的集中式动力学规划器（teacher），训练其分布式学生策略（student），并利用物理约束的神经网络生成光滑轨迹。训练阶段学生策略利用教师策略完整轨迹数据，提升样本效率，各策略在普通电脑两小时内训练完成。

Result: 方法在仿真及真实场景下验证，通过无通信、部分可观测情况下的运动，成功完成灵活参考轨迹跟踪，并取得接近集中式方案的性能。

Conclusion: 本文方法无需通信即可通过分布式学习实现多无人机协调搬运任务，性能强大、训练高效，为实际无人机集群协作提供了新途径。

Abstract: Existing approaches for transporting and manipulating cable-suspended loads
using multiple UAVs along reference trajectories typically rely on either
centralized control architectures or reliable inter-agent communication. In
this work, we propose a novel machine learning based method for decentralized
kinodynamic planning that operates effectively under partial observability and
without inter-agent communication. Our method leverages imitation learning to
train a decentralized student policy for each UAV by imitating a centralized
kinodynamic motion planner with access to privileged global observations. The
student policy generates smooth trajectories using physics-informed neural
networks that respect the derivative relationships in motion. During training,
the student policies utilize the full trajectory generated by the teacher
policy, leading to improved sample efficiency. Moreover, each student policy
can be trained in under two hours on a standard laptop. We validate our method
in both simulation and real-world environments to follow an agile reference
trajectory, demonstrating performance comparable to that of centralized
approaches.

</details>


### [310] [DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment](https://arxiv.org/abs/2510.17148)
*Yu Gao,Yiru Wang,Anqing Jiang,Heng Yuwen,Wang Shuo,Sun Hao,Wang Jijun*

Main category: cs.RO

TL;DR: 提出DiffVLA++框架，将视觉-语言-动作模型(VLA)与端到端(E2E)模型结合，并通过度量引导对齐，提高自动驾驶在复杂场景下的泛化能力和物理可行性。


<details>
  <summary>Details</summary>
Motivation: 现有E2E驾驶模型缺乏世界知识，导致在长尾场景下泛化能力差；而VLA模型虽然具备世界知识，但常产生物理上不可行的动作。急需一种方法兼顾认知推理与物理可行性，从而提升自动驾驶的可靠性和表现。

Method: 1) 构建能直接生成语义驱动轨迹的VLA模块；2) 设计具有稠密轨迹词汇表的E2E模块以保证轨迹物理合理；3) 引入度量引导的轨迹打分器，促使VLA与E2E输出结果对齐，整合二者优势。

Result: 在ICCV 2025 Autonomous Grand Challenge榜单上，DiffVLA++获得EPDMS值49.12，显示出较强的表现。

Conclusion: DiffVLA++框架通过结合VLA和E2E模块，并用度量引导实现输出对齐，有效提升了自动驾驶模型在复杂环境下的认知推理和轨迹物理合理性，取得优异实验结果。

Abstract: Conventional end-to-end (E2E) driving models are effective at generating
physically plausible trajectories, but often fail to generalize to long-tail
scenarios due to the lack of essential world knowledge to understand and reason
about surrounding environments. In contrast, Vision-Language-Action (VLA)
models leverage world knowledge to handle challenging cases, but their limited
3D reasoning capability can lead to physically infeasible actions. In this work
we introduce DiffVLA++, an enhanced autonomous driving framework that
explicitly bridges cognitive reasoning and E2E planning through metric-guided
alignment. First, we build a VLA module directly generating semantically
grounded driving trajectories. Second, we design an E2E module with a dense
trajectory vocabulary that ensures physical feasibility. Third, and most
critically, we introduce a metric-guided trajectory scorer that guides and
aligns the outputs of the VLA and E2E modules, thereby integrating their
complementary strengths. The experiment on the ICCV 2025 Autonomous Grand
Challenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.

</details>


### [311] [OmniVIC: A Self-Improving Variable Impedance Controller with Vision-Language In-Context Learning for Safe Robotic Manipulation](https://arxiv.org/abs/2510.17150)
*Heng Zhang,Wei-Hsing Huang,Gokhan Solak,Arash Ajoudani*

Main category: cs.RO

TL;DR: 本文提出了OmniVIC，一种结合视觉语言模型（VLM）的通用可变阻抗控制器（VIC），显著提升了机器人在复杂接触操作任务中的安全性和适应性。新方法利用以往经验和当前环境信息，实时调整控制参数，实现更安全和通用的操作，实验结果在多项任务上超越了基础方法，成功率显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统的可变阻抗控制器（VIC）在与环境物理交互时拥有优势，但在面对复杂、未见过、结构化较差的全新接触任务或存在不确定性的情境时泛化性有限，且难以保证交互的安全性。因而需要一种能够根据任务语境自适应调整参数、提高通用性和安全性的机器人控制方法。

Method: OmniVIC融合了视觉语言模型（VLM），可通过图片和自然语言解释任务语境，生成自适应的阻抗参数。其核心包括自改进的检索增强生成（RAG）与上下文学习（ICL），RAG从结构化记忆库中检索相关历史经验以辅助当前决策，ICL则基于检索结果和当前任务提示，驱动VLM生成针对性阻抗参数。此外，结合实时力/力矩反馈，动态保证交互力处于安全阈值。

Result: 在模拟和现实复杂接触任务中，OmniVIC表现优于现有基线方法，既提升了任务成功率，也显著减少了力违例（力超安全阈值）的发生。总体来看，成功率从基线的27%提升到OmniVIC的61.4%。

Conclusion: OmniVIC有效桥接了高层语义推理与低层顺应控制，提升了机器人系统在不同复杂任务中的安全性和普适性，实现了更通用和可拓展的操控能力。

Abstract: We present OmniVIC, a universal variable impedance controller (VIC) enhanced
by a vision language model (VLM), which improves safety and adaptation in any
contact-rich robotic manipulation task to enhance safe physical interaction.
Traditional VIC have shown advantages when the robot physically interacts with
the environment, but lack generalization in unseen, complex, and unstructured
safe interactions in universal task scenarios involving contact or uncertainty.
To this end, the proposed OmniVIC interprets task context derived reasoning
from images and natural language and generates adaptive impedance parameters
for a VIC controller. Specifically, the core of OmniVIC is a self-improving
Retrieval-Augmented Generation(RAG) and in-context learning (ICL), where RAG
retrieves relevant prior experiences from a structured memory bank to inform
the controller about similar past tasks, and ICL leverages these retrieved
examples and the prompt of current task to query the VLM for generating
context-aware and adaptive impedance parameters for the current manipulation
scenario. Therefore, a self-improved RAG and ICL guarantee OmniVIC works in
universal task scenarios. The impedance parameter regulation is further
informed by real-time force/torque feedback to ensure interaction forces remain
within safe thresholds. We demonstrate that our method outperforms baselines on
a suite of complex contact-rich tasks, both in simulation and on real-world
robotic tasks, with improved success rates and reduced force violations.
OmniVIC takes a step towards bridging high-level semantic reasoning and
low-level compliant control, enabling safer and more generalizable
manipulation. Overall, the average success rate increases from 27% (baseline)
to 61.4% (OmniVIC).

</details>


### [312] [SimpleVSF: VLM-Scoring Fusion for Trajectory Prediction of End-to-End Autonomous Driving](https://arxiv.org/abs/2510.17191)
*Peiru Zheng,Yun Zhao,Zhan Gong,Hong Zhu,Shaohua Wu*

Main category: cs.RO

TL;DR: 本文提出了一种名为SimpleVSF的新方法，通过融合视觉-语言模型（VLM）的认知能力和先进的轨迹融合技术，提高端到端自动驾驶系统的规划能力，并在ICCV 2025 NAVSIM v2挑战中取得领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法在复杂场景下决策表现仍不理想。本文旨在利用VLM的认知能力和先进的融合方法，提升自动驾驶系统决策的鲁棒性和智能性。

Method: 提出SimpleVSF框架，结合常规和VLM增强的评分机制，通过加权融合及上下文感知的决策过程对多种规划轨迹进行有效整合，实现更优的端到端驾驶决策。

Result: 在ICCV 2025 NAVSIM v2端到端驾驶挑战赛中，该方法表现优异，达到了业界领先的安全、舒适与效率的平衡。

Conclusion: 利用VLM模型的认知能力和优化的轨迹融合技术，SimpleVSF为端到端自动驾驶领域提供了一种高效且智能的新方案，极大提升了复杂场景下的决策表现。

Abstract: End-to-end autonomous driving has emerged as a promising paradigm for
achieving robust and intelligent driving policies. However, existing end-to-end
methods still face significant challenges, such as suboptimal decision-making
in complex scenarios. In this paper,we propose SimpleVSF (Simple VLM-Scoring
Fusion), a novel framework that enhances end-to-end planning by leveraging the
cognitive capabilities of Vision-Language Models (VLMs) and advanced trajectory
fusion techniques. We utilize the conventional scorers and the novel
VLM-enhanced scorers. And we leverage a robust weight fusioner for quantitative
aggregation and a powerful VLM-based fusioner for qualitative, context-aware
decision-making. As the leading approach in the ICCV 2025 NAVSIM v2 End-to-End
Driving Challenge, our SimpleVSF framework demonstrates state-of-the-art
performance, achieving a superior balance between safety, comfort, and
efficiency.

</details>


### [313] [Performance Evaluation of an Integrated System for Visible Light Communication and Positioning Using an Event Camera](https://arxiv.org/abs/2510.17203)
*Ryota Soga,Masataka Kobayashi,Tsukasa Shimizu,Shintaro Shiba,Quan Kong,Shan Lu,Takaya Yamazato*

Main category: cs.RO

TL;DR: 本文提出了一种创新的基于事件相机的车辆自定位系统，实现了可见光通信（VLC）与可见光定位（VLP）的融合，在GPS不可用的环境下（如隧道），仍能高效、精准定位车辆。系统通过多个带独特编码的LED进行数据传输和距离估计，实验表明其具备高鲁棒性与精度。


<details>
  <summary>Details</summary>
Motivation: 传统摄像头或导航方法在高动态范围、强光对比或GPS信号弱的环境（如隧道出口）下难以满足快速、精准自定位需求，而事件相机具备极高的时间分辨率和宽动态范围，适合上述场景。为了解决车辆在此类极端环境中的定位难题，研究者提出结合VLC和VLP于单一事件相机的系统。

Method: 系统架设多个带有Walsh-Hadamard编码的LED发射端，通过事件相机捕获光信号，再经相关算法分离不同LED信号，实现以VLC获取位置信息（坐标）、以VLP通过相位相关法（POC）测距。该方法利用事件相机独特的感知能力，在一台相机上实现多输入单输出（MISO）的高速通信和精确距离估算。

Result: 车辆以30km/h(8.3m/s)速度实地测试，系统在100米内距离估计的均方根误差（RMSE）小于0.75米，通信比特误码率（BER）在同一范围内低于0.01，证明了系统在实际场景下的高性能。

Conclusion: 本文首次实现了基于单台事件相机的车辆可见光通信和可见光定位系统，能在GPS不可用的复杂环境下为车辆提供高鲁棒性和精度的自定位服务，对智能交通和自动驾驶领域具有重要意义。

Abstract: Event cameras, featuring high temporal resolution and high dynamic range,
offer visual sensing capabilities comparable to conventional image sensors
while capturing fast-moving objects and handling scenes with extreme lighting
contrasts such as tunnel exits. Leveraging these properties, this study
proposes a novel self-localization system that integrates visible light
communication (VLC) and visible light positioning (VLP) within a single event
camera. The system enables a vehicle to estimate its position even in
GPS-denied environments, such as tunnels, by using VLC to obtain coordinate
information from LED transmitters and VLP to estimate the distance to each
transmitter.
  Multiple LEDs are installed on the transmitter side, each assigned a unique
pilot sequence based on Walsh-Hadamard codes. The event camera identifies
individual LEDs within its field of view by correlating the received signal
with these codes, allowing clear separation and recognition of each light
source. This mechanism enables simultaneous high-capacity MISO (multi-input
single-output) communication through VLC and precise distance estimation via
phase-only correlation (POC) between multiple LED pairs.
  To the best of our knowledge, this is the first vehicle-mounted system to
achieve simultaneous VLC and VLP functionalities using a single event camera.
Field experiments were conducted by mounting the system on a vehicle traveling
at 30 km/h (8.3 m/s). The results demonstrated robust real-world performance,
with a root mean square error (RMSE) of distance estimation within 0.75 m for
ranges up to 100 m and a bit error rate (BER) below 0.01 across the same range.

</details>


### [314] [Pole-Image: A Self-Supervised Pole-Anchored Descriptor for Long-Term LiDAR Localization and Map Maintenance](https://arxiv.org/abs/2510.17237)
*Wuhao Xie,Kanji Tanaka*

Main category: cs.RO

TL;DR: 提出用于移动机器人定位和地图维护的新型'Pole-Image'表征方法，结合易检测的杆状物与其周围环境生成独特签名，并通过对比学习获得高判别性描述子。


<details>
  <summary>Details</summary>
Motivation: 传统基于地标的方法，在可检测性与可区分性之间存在难以平衡的问题。高可检测地标（如杆）难以唯一性，高可区分地标（如局部点云结构）则检测不稳定。本工作旨在兼顾这两者优势。

Method: 引入'Pole-Image'方法，用杆状物作为锚点，将周围3D点云结构编码为以杆为原点的二维极坐标图像。利用杆的高可检测性，自动采集多样正样本对，结合对比学习获得视角无关、高区分性的描述子。

Result: 提出的描述子可克服感知混淆，实现鲁棒的自定位；同时其高精度编码可支持对环境变化的高敏感检测。

Conclusion: Pole-Image方法结合传统杆状物地标的优点与点云结构，提升了移动机器人自定位与地图维护的长期稳定性。

Abstract: Long-term autonomy for mobile robots requires both robust self-localization
and reliable map maintenance. Conventional landmark-based methods face a
fundamental trade-off between landmarks with high detectability but low
distinctiveness (e.g., poles) and those with high distinctiveness but difficult
stable detection (e.g., local point cloud structures). This work addresses the
challenge of descriptively identifying a unique "signature" (local point cloud)
by leveraging a detectable, high-precision "anchor" (like a pole). To solve
this, we propose a novel canonical representation, "Pole-Image," as a hybrid
method that uses poles as anchors to generate signatures from the surrounding
3D structure. Pole-Image represents a pole-like landmark and its surrounding
environment, detected from a LiDAR point cloud, as a 2D polar coordinate image
with the pole itself as the origin. This representation leverages the pole's
nature as a high-precision reference point, explicitly encoding the "relative
geometry" between the stable pole and the variable surrounding point cloud. The
key advantage of pole landmarks is that "detection" is extremely easy. This
ease of detection allows the robot to easily track the same pole, enabling the
automatic and large-scale collection of diverse observational data (positive
pairs). This data acquisition feasibility makes "Contrastive Learning (CL)"
applicable. By applying CL, the model learns a viewpoint-invariant and highly
discriminative descriptor. The contributions are twofold: 1) The descriptor
overcomes perceptual aliasing, enabling robust self-localization. 2) The
high-precision encoding enables high-sensitivity change detection, contributing
to map maintenance.

</details>


### [315] [An adaptive hierarchical control framework for quadrupedal robots in planetary exploration](https://arxiv.org/abs/2510.17249)
*Franek Stark,Rohit Kumar,Shubham Vyas,Hannah Isermann,Jonas Haack,Mihaela Popescu,Jakob Middelberg,Dennis Mronga,Frank Kirchner*

Main category: cs.RO

TL;DR: 這篇論文提出一種結合動態控制、線上模型調整及自適應步態規劃的模組化控制框架，用於應對未知極端環境中四足機器人移動的不確定性，並在實地火山考察中驗證其有效性。


<details>
  <summary>Details</summary>
Motivation: 傳統輪式探測器難以應對崎嶇、障礙多變和可變形表面的行走需求，而四足機器人潛力巨大。但未知環境對控制策略適應性的要求較高，當地形和機器人參數不明時，傳統方法難以應對，因此急需更通用和自適應的控制框架。

Method: 提出一套模組化控制框架，融合基於模型的動態控制、線上模型自適應和自適應落足點規劃，並支援不同感測條件下的狀態估計及執行時重新組態，系統已整合進ROS 2並開源。

Result: 該方法在兩個異構四足機器人平台、多種硬體架構上進行了測試，並在火山實地測試中讓機器人成功行走超過700米，顯示出良好的魯棒性與適應性。

Conclusion: 本文提出的控制框架能有效提升四足機器人在極端與未知環境下的自主移動能力，具有良好的通用性與實用性，對未來行星探測機器人設計具有重要推動作用。

Abstract: Planetary exploration missions require robots capable of navigating extreme
and unknown environments. While wheeled rovers have dominated past missions,
their mobility is limited to traversable surfaces. Legged robots, especially
quadrupeds, can overcome these limitations by handling uneven, obstacle-rich,
and deformable terrains. However, deploying such robots in unknown conditions
is challenging due to the need for environment-specific control, which is
infeasible when terrain and robot parameters are uncertain. This work presents
a modular control framework that combines model-based dynamic control with
online model adaptation and adaptive footstep planning to address uncertainties
in both robot and terrain properties. The framework includes state estimation
for quadrupeds with and without contact sensing, supports runtime
reconfiguration, and is integrated into ROS 2 with open-source availability.
Its performance was validated on two quadruped platforms, multiple hardware
architectures, and in a volcano field test, where the robot walked over 700 m.

</details>


### [316] [High-Level Multi-Robot Trajectory Planning And Spurious Behavior Detection](https://arxiv.org/abs/2510.17261)
*Fernando Salanova,Jesús Roche,Cristian Mahuela,Eduardo Montijano*

Main category: cs.RO

TL;DR: 论文提出了一种基于Transformer变换器的异常检测方法，用于识别多机器人系统执行任务过程中的异常行为，实验显示该方法在多项检测任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统在执行高层次任务时，容易因机器人异构性及环境复杂性而出现计划偏差或异常执行，需要一种有效方式早期检测这些异常以保证整体任务的可靠性和安全性。

Method: 论文提出一种结合Nets-within-Nets（NWN）结构化数据生成框架和基于LTL（线性时序逻辑）任务规格的全局协调，并设计了基于Transformer的轨迹异常检测流程，将机器人轨迹分为正常和异常类别。同时还进行了嵌入和架构消融实验。

Result: 实验显示该方法能够以91.3%的准确率检测执行低效行为，对任务核心违规和约束自适应异常的检测准确率分别达88.3%和66.8%；消融实验中，该方法优于简单表示。

Conclusion: 该方法在多机器人任务中的异常检测表现优异，验证了提出的数据结构和Transformer网络在复杂多约束环境下的有效性和适应性。

Abstract: The reliable execution of high-level missions in multi-robot systems with
heterogeneous agents, requires robust methods for detecting spurious behaviors.
In this paper, we address the challenge of identifying spurious executions of
plans specified as a Linear Temporal Logic (LTL) formula, as incorrect task
sequences, violations of spatial constraints, timing inconsis- tencies, or
deviations from intended mission semantics. To tackle this, we introduce a
structured data generation framework based on the Nets-within-Nets (NWN)
paradigm, which coordinates robot actions with LTL-derived global mission
specifications. We further propose a Transformer-based anomaly detection
pipeline that classifies robot trajectories as normal or anomalous. Experi-
mental evaluations show that our method achieves high accuracy (91.3%) in
identifying execution inefficiencies, and demonstrates robust detection
capabilities for core mission violations (88.3%) and constraint-based adaptive
anomalies (66.8%). An ablation experiment of the embedding and architecture was
carried out, obtaining successful results where our novel proposition performs
better than simpler representations.

</details>


### [317] [Floating-Base Deep Lagrangian Networks](https://arxiv.org/abs/2510.17270)
*Lucas Schulze,Juliano Decico Negri,Victor Barasuol,Vivian Suzano Medeiros,Marcelo Becker,Jan Peters,Oleg Arenz*

Main category: cs.RO

TL;DR: 本文提出了一种用于浮基系统的动力学建模新方法，即Floating-Base Deep Lagrangian Networks（FeLaN），能够结合物理约束，提升模型泛化能力与物理解释性。


<details>
  <summary>Details</summary>
Motivation: 浮基系统（如人形机器人和四足机器人）在机器人领域日趋重要，但现有的灰盒建模方法忽略了其惯性矩阵的特殊物理约束（如正定性、稀疏性与输入无关性等），导致物理一致性不足。为保证物理一致性和泛化能力，亟需专门面向浮基系统的动力学建模方法。

Method: 作者提出了一种用于浮基系统惯性矩阵物理一致性参数化的方法，确保所有物理约束均被满足（包括分支稀疏性和正定性、特征值三角不等式等）。采用受Deep Lagrangian Networks启发的神经网络结构，预测满足这些物理约束的惯性矩阵，并最小化拉格朗日力学下的逆动力学误差。

Result: 作者建立了多种四足及人形机器人数据集进行测试，并公开数据集。所提出的FeLaN方法在仿真与真实机器人上均取得了与现有方法高度竞争的性能，且具有更好的物理解释性。

Conclusion: FeLaN方法能有效提升浮基系统动力学建模的物理一致性与泛化能力，对仿真与实际应用均具竞争力，并有助于物理可解释性分析。

Abstract: Grey-box methods for system identification combine deep learning with
physics-informed constraints, capturing complex dependencies while improving
out-of-distribution generalization. Yet, despite the growing importance of
floating-base systems such as humanoids and quadrupeds, current grey-box models
ignore their specific physical constraints. For instance, the inertia matrix is
not only positive definite but also exhibits branch-induced sparsity and input
independence. Moreover, the 6x6 composite spatial inertia of the floating base
inherits properties of single-rigid-body inertia matrices. As we show, this
includes the triangle inequality on the eigenvalues of the composite rotational
inertia. To address the lack of physical consistency in deep learning models of
floating-base systems, we introduce a parameterization of inertia matrices that
satisfies all these constraints. Inspired by Deep Lagrangian Networks (DeLaN),
we train neural networks to predict physically plausible inertia matrices that
minimize inverse dynamics error under Lagrangian mechanics. For evaluation, we
collected and released a dataset on multiple quadrupeds and humanoids. In these
experiments, our Floating-Base Deep Lagrangian Networks (FeLaN) achieve highly
competitive performance on both simulated and real robots, while providing
greater physical interpretability.

</details>


### [318] [Implicit State Estimation via Video Replanning](https://arxiv.org/abs/2510.17315)
*Po-Chen Ko,Jiayuan Mao,Yu-Hsiang Fu,Hsien-Jeng Yeh,Chu-Rong Chen,Wei-Chiu Ma,Yilun Du,Shao-Hua Sun*

Main category: cs.RO

TL;DR: 本文提出一种新的视频规划框架，能够在执行过程中实时适应环境变化，提高了任务重规划的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视频规划方法在执行阶段难以应对部分可观测环境下的不确定性，因此难以处理执行过程中的失败和环境变化。为此，需要更灵活且具备自适应能力的方法。

Method: 提出了一个集成在线交互数据的规划框架，能在任务执行过程中动态更新模型参数，并在新计划生成时排除先前失败方案，从而实现隐式状态估计和自适应调整。

Result: 通过在新的模拟操作基准中进行大量实验，结果表明该方法有效提升了重规划的表现。

Conclusion: 本工作推进了基于视频的决策方法发展，展示了实时适应性与不确定性处理的新思路。

Abstract: Video-based representations have gained prominence in planning and
decision-making due to their ability to encode rich spatiotemporal dynamics and
geometric relationships. These representations enable flexible and
generalizable solutions for complex tasks such as object manipulation and
navigation. However, existing video planning frameworks often struggle to adapt
to failures at interaction time due to their inability to reason about
uncertainties in partially observed environments. To overcome these
limitations, we introduce a novel framework that integrates interaction-time
data into the planning process. Our approach updates model parameters online
and filters out previously failed plans during generation. This enables
implicit state estimation, allowing the system to adapt dynamically without
explicitly modeling unknown state variables. We evaluate our framework through
extensive experiments on a new simulated manipulation benchmark, demonstrating
its ability to improve replanning performance and advance the field of
video-based decision-making.

</details>


### [319] [DDBot: Differentiable Physics-based Digging Robot for Unknown Granular Materials](https://arxiv.org/abs/2510.17335)
*Xintong Yang,Minglun Wei,Ze Ji,Yu-Kun Lai*

Main category: cs.RO

TL;DR: 本论文提出了一种名为 DDBot 的新型可微分挖掘机器人框架，能够高效精准地操作各种物理属性未知的颗粒物质（如沙土），在实验中表现出优越的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有机器人在操作沙土类颗粒材料时，因为颗粒物的复杂动力学和不可预测性，难以兼顾效率与精度。因此亟需新方法来提升机器人在操作未知特性的颗粒物质时的表现。

Method: 作者设计了 DDBot 框架，内置了可微分、基于物理的沙土材料模拟器，并通过GPU并行计算与自动微分实现高效辨识颗粒物质物理属性及动作技能优化。该系统设计有可微分技能映射、演示机制、梯度裁剪及线性搜索梯度下降法，整体流程环环相扣。

Result: 实验证明，DDBot 能在5到20分钟内快速收敛辨识未知颗粒物质，并直接实现高精度的实际操作。同时，DDBot 在与现有方法的对比中，在鲁棒性与效率方面均有明显优势。

Conclusion: DDBot 框架不仅有效提升了颗粒物操控任务的精度与效率，也显示了其在实际应用的可行性，为相关领域的研究和工业应用提供了新思路。

Abstract: Automating the manipulation of granular materials poses significant
challenges due to complex contact dynamics, unpredictable material properties,
and intricate system states. Existing approaches often fail to achieve
efficiency and accuracy in such tasks. To fill the research gap, this paper
studies the small-scale and high-precision granular material digging task with
unknown physical properties. A new framework, named differentiable digging
robot (DDBot), is proposed to manipulate granular materials, including sand and
soil.
  Specifically, we equip DDBot with a differentiable physics-based simulator,
tailored for granular material manipulation, powered by GPU-accelerated
parallel computing and automatic differentiation. DDBot can perform efficient
differentiable system identification and high-precision digging skill
optimisation for unknown granular materials, which is enabled by a
differentiable skill-to-action mapping, a task-oriented demonstration method,
gradient clipping and line search-based gradient descent.
  Experimental results show that DDBot can efficiently (converge within 5 to 20
minutes) identify unknown granular material dynamics and optimise digging
skills, with high-precision results in zero-shot real-world deployments,
highlighting its practicality. Benchmark results against state-of-the-art
baselines also confirm the robustness and efficiency of DDBot in such digging
tasks.

</details>


### [320] [Interactive Force-Impedance Control](https://arxiv.org/abs/2510.17341)
*Fan Shao,Satoshi Endo,Sandra Hirche,Fanny Ficuciello*

Main category: cs.RO

TL;DR: 本文提出了一种统一的互动力-阻抗控制（IFIC）框架，用于提升机器人在与人及复杂环境接触下的协作安全性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有基于外力分析、名义动力学或数据驱动方法的人机协作，多适用于接触较少、简单环境。面对接触丰富环境和主动人的互动，这些方法存在系统丧失被动性的问题，带来安全隐患。因此，需要一种能确保在复杂互动环境下也保证安全的方法。

Method: 作者基于port-Hamiltonian框架提出了统一的互动力-阻抗控制（IFIC）方法，同时引入互动和任务控制端口，通过适应交互功率流的方式控制机器人，实现角色灵活切换，同时保持系统的被动性，避免失控风险。

Result: 所提出的控制架构能够保障在与主动人及非被动环境的接触中，交互的安全性与顺畅性，即便在复杂、接触丰富场景下也能适应和保证系统的被动性。

Conclusion: 该框架有效改善了人机协作中在复杂接触环境下的安全与适应性问题，为实际应用提供了坚实的理论与方法基础。

Abstract: Human collaboration with robots requires flexible role adaptation, enabling
robot to switch between active leader and passive follower. Effective role
switching depends on accurately estimating human intention, which is typically
achieved through external force analysis, nominal robot dynamics, or
data-driven approaches. However, these methods are primarily effective in
contact-sparse environments. When robots under hybrid or unified
force-impedance control physically interact with active humans or non-passive
environments, the robotic system may lose passivity and thus compromise safety.
To address this challenge, this paper proposes the unified Interactive
Force-Impedance Control (IFIC) framework that adapts to the interaction power
flow, ensuring effortless and safe interaction in contact-rich environments.
The proposed control architecture is formulated within a port-Hamiltonian
framework, incorporating both interaction and task control ports, through which
system passivity is guaranteed.

</details>


### [321] [Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots](https://arxiv.org/abs/2510.17369)
*Haochen Su,Cristian Meo,Francesco Stella,Andrea Peirone,Kai Junge,Josie Hughes*

Main category: cs.RO

TL;DR: 论文提出将视觉-语言-动作（VLA）模型首次应用到柔性连续机械臂上，实现了更安全的人机协作，并通过精细化微调弥补模型之间的实体差异。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型主要部署在传统刚性机械臂上，缺乏在柔性机械臂上的测试与优化，而后者在安全性和柔性方面对人机协作至关重要。

Method: 提出一种结构化的微调与部署流程，在多个典型操作任务中对比两种先进VLA模型（OpenVLA-OFT与π_0），系统实验“开箱即用”策略与经针对性微调后的软体机器人表现。

Result: 未经微调时，VLA模型在柔性机械臂上的性能不佳，被验证为实体不匹配导致功能退化；经过精准微调后，柔性机械臂表现可与传统刚性机械臂媲美。

Conclusion: 精细化微调是跨实体VLA模型落地的关键。结合VLA模型与柔性机械臂能够实现安全、灵活的智能体应用，适合于复杂人类场景。

Abstract: Robotic systems are increasingly expected to operate in human-centered,
unstructured environments where safety, adaptability, and generalization are
essential. Vision-Language-Action (VLA) models have been proposed as a language
guided generalized control framework for real robots. However, their deployment
has been limited to conventional serial link manipulators. Coupled by their
rigidity and unpredictability of learning based control, the ability to safely
interact with the environment is missing yet critical. In this work, we present
the deployment of a VLA model on a soft continuum manipulator to demonstrate
autonomous safe human-robot interaction. We present a structured finetuning and
deployment pipeline evaluating two state-of-the-art VLA models (OpenVLA-OFT and
$\pi_0$) across representative manipulation tasks, and show while
out-of-the-box policies fail due to embodiment mismatch, through targeted
finetuning the soft robot performs equally to the rigid counterpart. Our
findings highlight the necessity of finetuning for bridging embodiment gaps,
and demonstrate that coupling VLA models with soft robots enables safe and
flexible embodied AI in human-shared environments.

</details>


### [322] [Integrating Trustworthy Artificial Intelligence with Energy-Efficient Robotic Arms for Waste Sorting](https://arxiv.org/abs/2510.17408)
*Halima I. Kure,Jishna Retnakumari,Augustine O. Nwajana,Umar M. Ismail,Bilyaminu A. Romo,Ehigiator Egho-Promise*

Main category: cs.RO

TL;DR: 本文提出了一种结合可信AI和节能型机械臂的智能垃圾分类与分拣方法。系统采用MobileNetV2进行迁移学习，准确分类六类垃圾，并在机械臂模拟环境中实现节能分拣。


<details>
  <summary>Details</summary>
Motivation: 传统垃圾分类系统效率低且可靠性有限，难以满足城市智能管理的需求。为提高分类准确率和系统的可靠性，提升城市垃圾处理自动化水平，作者提出集成可信AI和节能机械臂的方案。

Method: 采用卷积神经网络（CNN），通过MobileNetV2迁移学习，实现对塑料、玻璃、金属、纸张、纸板和其他垃圾的分类。机械臂模拟器根据欧几里得距离计算动作能耗，实现虚拟高效分拣，系统还纳入可信AI原则，如透明性、鲁棒性、公平性与安全性。

Result: 模型训练准确率达99.8%，验证准确率为80.5%。机械臂分拣动作的能耗得到优化，系统整体表现出较高的可靠性和可扩展性。

Conclusion: 将可信AI与节能机械臂相结合，为城市垃圾智能管理提供了可靠、可推广的解决方案。该方法具有高准确率、高效能，便于实际部署应用。

Abstract: This paper presents a novel methodology that integrates trustworthy
artificial intelligence (AI) with an energy-efficient robotic arm for
intelligent waste classification and sorting. By utilizing a convolutional
neural network (CNN) enhanced through transfer learning with MobileNetV2, the
system accurately classifies waste into six categories: plastic, glass, metal,
paper, cardboard, and trash. The model achieved a high training accuracy of
99.8% and a validation accuracy of 80.5%, demonstrating strong learning and
generalization. A robotic arm simulator is implemented to perform virtual
sorting, calculating the energy cost for each action using Euclidean distance
to ensure optimal and efficient movement. The framework incorporates key
elements of trustworthy AI, such as transparency, robustness, fairness, and
safety, making it a reliable and scalable solution for smart waste management
systems in urban settings.

</details>


### [323] [From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors](https://arxiv.org/abs/2510.17439)
*Zhengshen Zhang,Hao Li,Yalun Dai,Zhengbang Zhu,Lei Zhou,Chenchen Liu,Dong Wang,Francis E. H. Tay,Sijin Chen,Ziwei Liu,Yuxiao Liu,Xinghang Li,Pan Zhou*

Main category: cs.RO

TL;DR: FALCON引入3D空间token提升视觉-语言-动作模型在3D任务中的空间推理和泛化能力，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型多依赖2D特征，空间理解有限，影响其在真实3D世界中的泛化和适应能力。已有3D信息融合方法要么依赖特殊传感器且不能跨模态迁移，要么引入的空间信息过弱，缺乏几何意义，损害视觉-语言匹配。

Method: 提出FALCON框架，将丰富的3D空间token注入至动作头部分，通过空间基础模型单靠RGB获得强几何先验；可选地融合深度和位姿信息，且无需重新训练或调整结构；空间token由专门的空间增强动作头处理，未直接与视觉-语言主干拼接，以保留语言推理能力。

Result: 在三个模拟基准和十一项真实任务中，FALCON取得了当前最优表现，性能超越现有主流方法，并在复杂环境、空间提示、物体尺度与高度变化下表现出稳健性。

Conclusion: FALCON有效克服了VLA模型在空间表达、跨模态迁移和对齐方面的局限，显著提升了3D场景下的任务表现和泛化能力。

Abstract: Existing vision-language-action (VLA) models act in 3D real-world but are
typically built on 2D encoders, leaving a spatial reasoning gap that limits
generalization and adaptability. Recent 3D integration techniques for VLAs
either require specialized sensors and transfer poorly across modalities, or
inject weak cues that lack geometry and degrade vision-language alignment. In
this work, we introduce FALCON (From Spatial to Action), a novel paradigm that
injects rich 3D spatial tokens into the action head. FALCON leverages spatial
foundation models to deliver strong geometric priors from RGB alone, and
includes an Embodied Spatial Model that can optionally fuse depth, or pose for
higher fidelity when available, without retraining or architectural changes. To
preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced
Action Head rather than being concatenated into the vision-language backbone.
These designs enable FALCON to address limitations in spatial representation,
modality transferability, and alignment. In comprehensive evaluations across
three simulation benchmarks and eleven real-world tasks, our proposed FALCON
achieves state-of-the-art performance, consistently surpasses competitive
baselines, and remains robust under clutter, spatial-prompt conditioning, and
variations in object scale and height.

</details>


### [324] [A Generalization of Input-Output Linearization via Dynamic Switching Between Melds of Output Functions](https://arxiv.org/abs/2510.17448)
*Mirko Mizzoni,Pieter van Goor,Barbara Bazzana,Antonio Franchi*

Main category: cs.RO

TL;DR: 本文提出了一种针对非线性系统通过反馈线性化实现不同输出集合切换的系统化框架，理论上保证了系统状态有界和输出误差的指数稳定性，并通过数值仿真验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 在复杂的非线性系统控制中，不同的任务或工况可能需要切换不同的输出变量来控制，现有方法对于多输出切换时难以保证系统的稳定性与性能。因此，亟需一种能够在保证系统状态有界和输出误差稳定的前提下，实现多种输出切换的通用理论框架。

Method: 引入了“meld”这一概念，对可反馈线性化的有效输出子集进行形式化定义，并提出了在满足最小驻留时间和输出集合兼容性的条件下，可安全切换不同“meld”，同时保证系统状态的有界性和主动输出的指数稳定性。通过理论推导给出了切换时的稳定性证明，并采用机械臂的数值仿真进行了示例验证。

Result: 理论结果证明，在合适的驻留时间和兼容性条件下，可以在不同“melds”之间切换，且系统状态始终有界；每个切换区间内主动输出的误差动态保持指数稳定；连续“melds”之间的公共输出可无缝过渡。仿真结果证实了该方法的有效性。

Conclusion: 本文建立的理论框架适用于任何可反馈线性化的非线性系统，实现了不同输出集合间带“指数稳定”切换。对机器人、飞行器等实际系统具有重要应用价值，为复杂任务下的输出切换控制提供了理论依据。

Abstract: This letter presents a systematic framework for switching between different
sets of outputs for the control of nonlinear systems via feedback
linearization. We introduce the concept of a meld to formally define a valid,
feedback-linearizable subset of outputs that can be selected from a larger deck
of possible outputs. The main contribution is a formal proof establishing that
under suitable dwell-time and compatibility conditions, it is possible to
switch between different melds while guaranteeing the uniform boundedness of
the system state. We further show that the error dynamics of the active outputs
remain exponentially stable within each switching interval and that outputs
common to consecutive melds are tracked seamlessly through transitions. The
proposed theory is valid for any feedback linearizable nonlinear system, such
as, e.g., robots, aerial and terrestrial vehicles, etc.. We demonstrate it on a
simple numerical simulation of a robotic manipulator.

</details>


### [325] [HumanMPC - Safe and Efficient MAV Navigation among Humans](https://arxiv.org/abs/2510.17525)
*Simon Schaefer,Helen Oleynikova,Sandra Hirche,Stefan Leutenegger*

Main category: cs.RO

TL;DR: 本文提出了一种结合理论安全保障与数据驱动人体运动预测的三维微型飞行器（MAV）导航方法HumanMPC，并验证了其实用性与高效性。


<details>
  <summary>Details</summary>
Motivation: 现有机器人导航主要针对二维人群环境，且仅关注人体根部运动，难以应对更真实、动态、复杂的三维环境与肢体动作，亟需一种兼顾安全与高效的导航方法。

Method: 提出HumanMPC框架，基于模型预测控制（MPC），结合可达性安全理论和数据驱动的人类3D运动预测模型。创新性地只对初始控制输入进行安全约束，但模型中考虑其对整个规划时域的影响，实现有效的安全性与效率平衡。

Result: 在真实人类轨迹的模拟实验和实际场景下进行了验证，HumanMPC在导航、视觉伺服等多种任务中表现良好，比基线方法在安全性、效率、可靠性上均有优势，且不显著保守。

Conclusion: HumanMPC能够在保证安全的前提下，实现高效的三维人群环境导航，泛化性强，可适用于多种机器人平台。

Abstract: Safe and efficient robotic navigation among humans is essential for
integrating robots into everyday environments. Most existing approaches focus
on simplified 2D crowd navigation and fail to account for the full complexity
of human body dynamics beyond root motion. We present HumanMPC, a Model
Predictive Control (MPC) framework for 3D Micro Air Vehicle (MAV) navigation
among humans that combines theoretical safety guarantees with data-driven
models for realistic human motion forecasting. Our approach introduces a novel
twist to reachability-based safety formulation that constrains only the initial
control input for safety while modeling its effects over the entire planning
horizon, enabling safe yet efficient navigation. We validate HumanMPC in both
simulated experiments using real human trajectories and in the real-world,
demonstrating its effectiveness across tasks ranging from goal-directed
navigation to visual servoing for human tracking. While we apply our method to
MAVs in this work, it is generic and can be adapted by other platforms. Our
results show that the method ensures safety without excessive conservatism and
outperforms baseline approaches in both efficiency and reliability.

</details>


### [326] [Distributed Spatial-Temporal Trajectory Optimization for Unmanned-Aerial-Vehicle Swarm](https://arxiv.org/abs/2510.17541)
*Xiaobo Zheng,Pan Tang,Defu Lin,Shaoming He*

Main category: cs.RO

TL;DR: 本文提出了一种针对大规模无人机集群的分布式时空轨迹优化框架，融合了ADMM和参数化微分动态规划（PDDP），显著提高了多智能体轨迹规划的效率。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体轨迹优化方法存在需提前设定终止时间、收敛慢和不适合集群规模扩展等问题，阻碍了其在大规模无人机集群中的实际应用。

Method: 提出采用分布式双层架构：每台无人机用参数化DDP局部规划轨迹，集群整体用ADMM保证一致性与约束，同时引入基于谱梯度的自适应惩罚系数调整以减少迭代次数。

Result: 通过仿真实验展示，所提算法能够有效提升多无人机集群轨迹优化的速度与性能，显著减少迭代次数，解决了大规模分布式优化的瓶颈。

Conclusion: 该算法为多无人机轨迹优化提供了高效可扩展的分布式解法，有潜力在实际大规模集群应用中推广。

Abstract: Swarm trajectory optimization problems are a well-recognized class of
multi-agent optimal control problems with strong nonlinearity. However, the
heuristic nature of needing to set the final time for agents beforehand and the
time-consuming limitation of the significant number of iterations prohibit the
application of existing methods to large-scale swarm of Unmanned Aerial
Vehicles (UAVs) in practice. In this paper, we propose a spatial-temporal
trajectory optimization framework that accomplishes multi-UAV consensus based
on the Alternating Direction Multiplier Method (ADMM) and uses Differential
Dynamic Programming (DDP) for fast local planning of individual UAVs. The
introduced framework is a two-level architecture that employs Parameterized DDP
(PDDP) as the trajectory optimizer for each UAV, and ADMM to satisfy the local
constraints and accomplish the spatial-temporal parameter consensus among all
UAVs. This results in a fully distributed algorithm called Distributed
Parameterized DDP (D-PDDP). In addition, an adaptive tuning criterion based on
the spectral gradient method for the penalty parameter is proposed to reduce
the number of algorithmic iterations. Several simulation examples are presented
to verify the effectiveness of the proposed algorithm.

</details>


### [327] [Intent-Driven LLM Ensemble Planning for Flexible Multi-Robot Disassembly: Demonstration on EV Batteries](https://arxiv.org/abs/2510.17576)
*Cansu Erdogan,Cesar Alan Contreras,Alireza Rastegarpanah,Manolis Chiou,Rustam Stolkin*

Main category: cs.RO

TL;DR: 本文提出了一种结合人类意图与大语言模型的多机器人协作操作规划管线，通过整合感知、LLM规划、验证和一致性过滤，实现复杂任务的鲁棒执行，效能在电动车电池拆卸任务上得到实证验证。


<details>
  <summary>Details</summary>
Motivation: 多机器人在无结构场景中处理多种对象、执行复杂串联操作任务，需要将人类意图高效、准确映射为可执行的机器人动作序列，现有方法缺少灵活性及高效的人机协作方式。

Method: 提出意图驱动的规划管线，集成：(i) 感知到文本场景编码；(ii) 基于LLM的意图-动作序列生成；(iii) LLM验证器确保格式和顺序约束；(iv) 一致性过滤剔除虚幻对象。并在现实场景下与不同LLM方案及组件做了消融实验和用户交互评估。

Result: 在200个真实场景、600个操作指令下，方案能准确可靠地将用户意图转化为多机器人安全可行的动作序列，且用户操作负担低。实验涵盖五种零件类别，多个管线版本对比和人机交互体验验证。

Conclusion: 集成LLM与人类意图的协作规划管线在复杂多机器人场景下实现了高效、低负担的人机协同操作规划，具有良好的通用性与安全性，适合复杂自动化场景应用。

Abstract: This paper addresses the problem of planning complex manipulation tasks, in
which multiple robots with different end-effectors and capabilities, informed
by computer vision, must plan and execute concatenated sequences of actions on
a variety of objects that can appear in arbitrary positions and configurations
in unstructured scenes. We propose an intent-driven planning pipeline which can
robustly construct such action sequences with varying degrees of supervisory
input from a human using simple language instructions. The pipeline integrates:
(i) perception-to-text scene encoding, (ii) an ensemble of large language
models (LLMs) that generate candidate removal sequences based on the operator's
intent, (iii) an LLM-based verifier that enforces formatting and precedence
constraints, and (iv) a deterministic consistency filter that rejects
hallucinated objects. The pipeline is evaluated on an example task in which two
robot arms work collaboratively to dismantle an Electric Vehicle battery for
recycling applications. A variety of components must be grasped and removed in
specific sequences, determined by human instructions and/or by task-order
feasibility decisions made by the autonomous system. On 200 real scenes with
600 operator prompts across five component classes, we used metrics of
full-sequence correctness and next-task correctness to evaluate and compare
five LLM-based planners (including ablation analyses of pipeline components).
We also evaluated the LLM-based human interface in terms of time to execution
and NASA TLX with human participant experiments. Results indicate that our
ensemble-with-verification approach reliably maps operator intent to safe,
executable multi-robot plans while maintaining low user effort.

</details>


### [328] [Learned Inertial Odometry for Cycling Based on Mixture of Experts Algorithm](https://arxiv.org/abs/2510.17604)
*Hao Qiao,Yan Wang,Shuo Yang,Xiaoyao Yu,Jian kuang,Xiaoji Niu*

Main category: cs.RO

TL;DR: 本文提出了一种高效的自行车定位方法，通过改进的混合专家模型（MoE）扩展和优化现有的TLIO方法，实现了更低的计算和参数开销，同时保持了高定位精度。


<details>
  <summary>Details</summary>
Motivation: 自行车共享和骑行应用迅速发展，对自行车定位精度的需求增加。目前基于GNSS的方法易受多路径干扰，惯导方法对精确建模要求高且鲁棒性有限，因此亟需一种能兼顾精度和效率的新方法。

Method: 将Tight Learned Inertial Odometry（TLIO）扩展用于自行车定位，并引入改进的混合专家（MoE）模型，减少训练和推理时的计算资源消耗。

Result: 与当前最先进的LLIO框架相比，新方法在精度相当的情况下，参数量减少了64.7%，计算成本降低了81.8%。

Conclusion: 所提方法适合移动设备，在提升效率的情况下依然保证了定位精度，为自行车定位任务提供了更实用的解决方案。

Abstract: With the rapid growth of bike sharing and the increasing diversity of cycling
applications, accurate bicycle localization has become essential. traditional
GNSS-based methods suffer from multipath effects, while existing inertial
navigation approaches rely on precise modeling and show limited robustness.
Tight Learned Inertial Odometry (TLIO) achieves low position drift by combining
raw IMU data with predicted displacements by neural networks, but its high
computational cost restricts deployment on mobile devices. To overcome this, we
extend TLIO to bicycle localization and introduce an improved Mixture-of
Experts (MoE) model that reduces both training and inference costs. Experiments
show that, compared to the state-of-the-art LLIO framework, our method achieves
comparable accuracy while reducing parameters by 64.7% and computational cost
by 81.8%.

</details>


### [329] [RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation](https://arxiv.org/abs/2510.17640)
*Yuquan Xue,Guanxing Lu,Zhenyu Wu,Chuanrui Zhang,Bofang Jia,Zhengyi Gu,Yansong Tang,Ziwei Wang*

Main category: cs.RO

TL;DR: 本文提出了一种自动化OOD（分布外）数据增强框架RESample，通过探索性采样改善视觉-语言-行动（VLA）模型的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 目前的VLA模型依赖的模仿学习数据集通常仅包含成功轨迹，缺乏失败或恢复情景的数据，导致模型在遇到意外或未见过的情形时表现不佳。解决这一问题有助于提升机器人控制系统的实际应用能力。

Method: 作者首先利用离线强化学习获得一个可评估亚最优动作的动作价值网络，并通过回滚采样潜在的分布外状态，随后采用探索性采样机制自适应地将这些状态及相关动作加入训练集，提升数据多样性和有效性。

Result: 在LIBERO基准和真实世界机器人操作任务的广泛实验表明，RESample框架显著提升了VLA模型的稳定性和泛化能力。

Conclusion: RESample能够自动增强VLA训练数据，对OOD状态恢复能力显著提升，使模型在遇到分布转移时更加健壮。

Abstract: Vision-Language-Action models (VLAs) have demonstrated remarkable performance
on complex robotic manipulation tasks through imitation learning. However,
existing imitation learning datasets contain only successful trajectories and
lack failure or recovery data, especially for out-of-distribution (OOD) states
where the robot deviates from the main policy due to minor perturbations or
errors, leading VLA models to struggle with states deviating from the training
distribution. To this end, we propose an automated OOD data augmentation
framework named RESample through exploratory sampling. Specifically, we first
leverage offline reinforcement learning to obtain an action-value network that
accurately identifies sub-optimal actions under the current manipulation
policy. We further sample potential OOD states from trajectories via rollout,
and design an exploratory sampling mechanism that adaptively incorporates these
action proxies into the training dataset to ensure efficiency. Subsequently,
our framework explicitly encourages the VLAs to recover from OOD states and
enhances their robustness against distributional shifts. We conduct extensive
experiments on the LIBERO benchmark as well as real-world robotic manipulation
tasks, demonstrating that RESample consistently improves the stability and
generalization ability of VLA models.

</details>


### [330] [Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats](https://arxiv.org/abs/2510.17783)
*Simeon Adebola,Chung Min Kim,Justin Kerr,Shuangyu Xie,Prithvi Akella,Jose Luis Susa Rincon,Eugen Solowjow,Ken Goldberg*

Main category: cs.RO

TL;DR: 本文提出了Botany-Bot系统，利用机器人和多视角成像，突破传统定点相机植物表型系统的叶片遮挡瓶颈，实现植物高精度三维数字孪生与详细结构注释。


<details>
  <summary>Details</summary>
Motivation: 由于传统的植物表型分析系统采用固定摄像头，常受到叶片遮挡限制，无法感知到如叶背、茎芽等精细结构，影响对植物形态的全面理解和数字化建模。

Method: 设计了包含双目相机、数字转盘、光箱、工业机械臂的硬件系统，结合3D分割高斯斑点（Gaussian Splat）模型重建植物三维结构。开发了机械臂控制算法，实现自动拨开叶片、拍摄叶片正反面及茎芽等被遮挡部位的高清索引图像。

Result: 实验结果显示，Botany-Bot系统可实现90.8%的叶片分割精度，86.2%的叶片检测精度，77.9%的机械臂拨开/提升叶片成功率，以及77.3%的正反面细节成像成功率。

Conclusion: Botany-Bot显著提升了植物结构感知的精细度和完整性，为高精度数字孪生与植物科研表型测量提供了有效工具，代码和数据已公开以促进后续研究。

Abstract: Commercial plant phenotyping systems using fixed cameras cannot perceive many
plant details due to leaf occlusion. In this paper, we present Botany-Bot, a
system for building detailed "annotated digital twins" of living plants using
two stereo cameras, a digital turntable inside a lightbox, an industrial robot
arm, and 3D segmentated Gaussian Splat models. We also present robot algorithms
for manipulating leaves to take high-resolution indexable images of occluded
details such as stem buds and the underside/topside of leaves. Results from
experiments suggest that Botany-Bot can segment leaves with 90.8% accuracy,
detect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and
take detailed overside/underside images with 77.3% accuracy. Code, videos, and
datasets are available at https://berkeleyautomation.github.io/Botany-Bot/.

</details>


### [331] [SoftMimic: Learning Compliant Whole-body Control from Examples](https://arxiv.org/abs/2510.17792)
*Gabriel B. Margolis,Michelle Wang,Nolan Fey,Pulkit Agrawal*

Main category: cs.RO

TL;DR: 该论文提出了SoftMimic框架，旨在让人形机器人通过模仿学习顺应式、全身的运动控制，从而安全有效地与环境交互。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的模仿学习方法容易导致机器人动作僵硬，在遇到突发外力时容易失衡甚至不安全。该工作旨在提升机器人对外部干扰的顺应性和任务泛化能力。

Method: 作者利用逆运动学生成一组可行的顺应性动作数据集，通过强化学习训练策略，奖励机器人对干扰的顺应性响应而非一味跟踪参考动作。

Result: 实验包括仿真和实物测试，结果表明SoftMimic训练出的机器人能够吸收外界扰动，在多种任务下表现出安全有效的交互和良好的顺应性。

Conclusion: SoftMimic为人形机器人模仿学习提供了一种新的顺应性控制框架，有助于提升机器人在实际环境下的适应性和安全性。

Abstract: We introduce SoftMimic, a framework for learning compliant whole-body control
policies for humanoid robots from example motions. Imitating human motions with
reinforcement learning allows humanoids to quickly learn new skills, but
existing methods incentivize stiff control that aggressively corrects
deviations from a reference motion, leading to brittle and unsafe behavior when
the robot encounters unexpected contacts. In contrast, SoftMimic enables robots
to respond compliantly to external forces while maintaining balance and
posture. Our approach leverages an inverse kinematics solver to generate an
augmented dataset of feasible compliant motions, which we use to train a
reinforcement learning policy. By rewarding the policy for matching compliant
responses rather than rigidly tracking the reference motion, SoftMimic learns
to absorb disturbances and generalize to varied tasks from a single motion
clip. We validate our method through simulations and real-world experiments,
demonstrating safe and effective interaction with the environment.

</details>


### [332] [Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models as Embodied Brain](https://arxiv.org/abs/2510.17801)
*Yulin Luo,Chun-Kai Fan,Menghang Dong,Jiayu Shi,Mengdi Zhao,Bo-Wen Zhang,Cheng Chi,Jiaming Liu,Gaole Dai,Rongyu Zhang,Ruichuan An,Kun Wu,Zhengping Che,Shaoxuan Xie,Guocai Yao,Zhongxia Zhao,Pengwei Wang,Guang Liu,Zhongyuan Wang,Tiejun Huang,Shanghang Zhang*

Main category: cs.RO

TL;DR: 本文提出了RoboBench，一个系统性评测多模态大语言模型在机器人操控任务中高层认知能力的基准，覆盖五大维度，并展示了现有模型的主要局限。


<details>
  <summary>Details</summary>
Motivation: 当前机器人系统虽采用分层架构（低层控制+高层推理），但针对高层系统（embodied brain）的评测不全面，现有方法无法系统衡量其认知与推理能力。

Method: 作者设计了RoboBench，针对多模态大语言模型（MLLMs）在类脑系统中的表现，从指令理解、感知推理、泛化规划、可供性预测及失败分析五大维度，定义14项能力，涵盖25个任务和6092个QA对，并采用真实多视角机器人数据。计划维度评测采用预测方案对对象状态变化的可执行性进行仿真验证（MLLM-as-world-simulator）。

Result: 在14种主流MLLMs上的实验显示，模型在隐含指令理解、时空推理、跨场景规划、细粒度可供性认知和执行失败诊断等方面存在明显性能不足。

Conclusion: RoboBench作为覆盖认知全流程的评测基准，有助于定量分析现有MLLMs的高级认知能力，发现短板，指导下一代多模态机器人认知系统的研发。

Abstract: Building robots that can perceive, reason, and act in dynamic, unstructured
environments remains a core challenge. Recent embodied systems often adopt a
dual-system paradigm, where System 2 handles high-level reasoning while System
1 executes low-level control. In this work, we refer to System 2 as the
embodied brain, emphasizing its role as the cognitive core for reasoning and
decision-making in manipulation tasks. Given this role, systematic evaluation
of the embodied brain is essential. Yet existing benchmarks emphasize execution
success, or when targeting high-level reasoning, suffer from incomplete
dimensions and limited task realism, offering only a partial picture of
cognitive capability. To bridge this gap, we introduce RoboBench, a benchmark
that systematically evaluates multimodal large language models (MLLMs) as
embodied brains. Motivated by the critical roles across the full manipulation
pipeline, RoboBench defines five dimensions-instruction comprehension,
perception reasoning, generalized planning, affordance prediction, and failure
analysis-spanning 14 capabilities, 25 tasks, and 6092 QA pairs. To ensure
realism, we curate datasets across diverse embodiments, attribute-rich objects,
and multi-view scenes, drawing from large-scale real robotic data. For
planning, RoboBench introduces an evaluation framework,
MLLM-as-world-simulator. It evaluate embodied feasibility by simulating whether
predicted plans can achieve critical object-state changes. Experiments on 14
MLLMs reveal fundamental limitations: difficulties with implicit instruction
comprehension, spatiotemporal reasoning, cross-scenario planning, fine-grained
affordance understanding, and execution failure diagnosis. RoboBench provides a
comprehensive scaffold to quantify high-level cognition, and guide the
development of next-generation embodied MLLMs. The project page is in
https://robo-bench.github.io.

</details>
